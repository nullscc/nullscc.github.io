
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-08-24 123:00:00 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Sparks of Large Audio Models: A Survey and Outlook paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.12792 repo_url: None paper_authors: Siddique Latif, Moazzam Shoukat, Fahad Shamshad, Muhammad Usama, Heriberto C">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-08-24 123:00:00">
<meta property="og:url" content="http://example.com/2023/08/24/cs.SD_2023_08_24/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Sparks of Large Audio Models: A Survey and Outlook paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.12792 repo_url: None paper_authors: Siddique Latif, Moazzam Shoukat, Fahad Shamshad, Muhammad Usama, Heriberto C">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-23T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:52.359Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_08_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/24/cs.SD_2023_08_24/" class="article-date">
  <time datetime="2023-08-23T16:00:00.000Z" itemprop="datePublished">2023-08-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-08-24 123:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Sparks-of-Large-Audio-Models-A-Survey-and-Outlook"><a href="#Sparks-of-Large-Audio-Models-A-Survey-and-Outlook" class="headerlink" title="Sparks of Large Audio Models: A Survey and Outlook"></a>Sparks of Large Audio Models: A Survey and Outlook</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12792">http://arxiv.org/abs/2308.12792</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddique Latif, Moazzam Shoukat, Fahad Shamshad, Muhammad Usama, Heriberto Cuayáhuitl, Björn W. Schuller</li>
<li>for: 这篇论文提供了大语音模型在音频处理领域的最新进展和挑战。</li>
<li>methods: 这篇论文涵盖了最新的大语音模型方法，包括基于转换器的架构，以及这些模型在各种音频任务上的表现。</li>
<li>results: 这些大语音模型在自动语音识别、文本读取和音乐生成等多种音频任务上都有显著的表现，甚至可以作为通用翻译器，支持多种语言的多种speech任务无需单独的任务特定系统。<details>
<summary>Abstract</summary>
This survey paper provides a comprehensive overview of the recent advancements and challenges in applying large language models to the field of audio signal processing. Audio processing, with its diverse signal representations and a wide range of sources--from human voices to musical instruments and environmental sounds--poses challenges distinct from those found in traditional Natural Language Processing scenarios. Nevertheless, \textit{Large Audio Models}, epitomized by transformer-based architectures, have shown marked efficacy in this sphere. By leveraging massive amount of data, these models have demonstrated prowess in a variety of audio tasks, spanning from Automatic Speech Recognition and Text-To-Speech to Music Generation, among others. Notably, recently these Foundational Audio Models, like SeamlessM4T, have started showing abilities to act as universal translators, supporting multiple speech tasks for up to 100 languages without any reliance on separate task-specific systems. This paper presents an in-depth analysis of state-of-the-art methodologies regarding \textit{Foundational Large Audio Models}, their performance benchmarks, and their applicability to real-world scenarios. We also highlight current limitations and provide insights into potential future research directions in the realm of \textit{Large Audio Models} with the intent to spark further discussion, thereby fostering innovation in the next generation of audio-processing systems. Furthermore, to cope with the rapid development in this area, we will consistently update the relevant repository with relevant recent articles and their open-source implementations at https://github.com/EmulationAI/awesome-large-audio-models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Whombat-An-open-source-annotation-tool-for-machine-learning-development-in-bioacoustics"><a href="#Whombat-An-open-source-annotation-tool-for-machine-learning-development-in-bioacoustics" class="headerlink" title="Whombat: An open-source annotation tool for machine learning development in bioacoustics"></a>Whombat: An open-source annotation tool for machine learning development in bioacoustics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12688">http://arxiv.org/abs/2308.12688</a></li>
<li>repo_url: None</li>
<li>paper_authors: Santiago Martinez Balvanera, Oisin Mac Aodha, Matthew J. Weldy, Holly Pringle, Ella Browning, Kate E. Jones</li>
<li>for: 这种机器学习方法可以帮助生物声学记录的自动分析，从而提高生物多样性监测的效率。</li>
<li>methods: 使用用户友好的浏览器界面管理音频记录和注释项目，提供了多种视觉化、探索和注释工具，使用者可以快速注释、审查和分享注释，以及视觉化和评估数据集中机器学习预测的性能。</li>
<li>results: 我们通过两个不同的应用场景来展示Whombat的灵活性：一是英国蝙蝠保护协会（BCT）的自动蝙蝠呼声识别项目，另一是美国农业部和奥REGON州立大学合作的生物声学应用研究。Whombat是一个高效地解决生物声学注释挑战的工具，可以用于个人和团队工作，可以在共享服务器上主机或远程访问，也可以在本地电脑上运行而无需编程技能。<details>
<summary>Abstract</summary></li>
</ul>
<ol>
<li>Automated analysis of bioacoustic recordings using machine learning (ML) methods has the potential to greatly scale biodiversity monitoring efforts. The use of ML for high-stakes applications, such as conservation research, demands a data-centric approach with a focus on utilizing carefully annotated and curated evaluation and training data that is relevant and representative. Creating annotated datasets of sound recordings presents a number of challenges, such as managing large collections of recordings with associated metadata, developing flexible annotation tools that can accommodate the diverse range of vocalization profiles of different organisms, and addressing the scarcity of expert annotators.   2. We present Whombat a user-friendly, browser-based interface for managing audio recordings and annotation projects, with several visualization, exploration, and annotation tools. It enables users to quickly annotate, review, and share annotations, as well as visualize and evaluate a set of machine learning predictions on a dataset. The tool facilitates an iterative workflow where user annotations and machine learning predictions feedback to enhance model performance and annotation quality.   3. We demonstrate the flexibility of Whombat by showcasing two distinct use cases: an project aimed at enhancing automated UK bat call identification at the Bat Conservation Trust (BCT), and a collaborative effort among the USDA Forest Service and Oregon State University researchers exploring bioacoustic applications and extending automated avian classification models in the Pacific Northwest, USA.   4. Whombat is a flexible tool that can effectively address the challenges of annotation for bioacoustic research. It can be used for individual and collaborative work, hosted on a shared server or accessed remotely, or run on a personal computer without the need for coding skills.</details>
<details>
<summary>摘要</summary></li>
<li>机器学习（ML）技术可以大幅提高生物声学记录的自动分析，从而推动生物多样性监测的扩大。在高度重要的应用领域，如保护研究，需要采用数据驱动的方法，强调使用严格标注和抽象的评估和训练数据，以确保模型的可靠性和可重复性。创建声音记录的标注数据集存在许多挑战，例如管理大量的声音记录和相关的元数据，开发flexible的标注工具，以满足不同生物种的声音oprofile的多样性。2. 我们提出了一个用户友好的、浏览器基本的界面，用于管理声音记录和标注项目。该工具具有许多视觉化、探索和标注工具，允许用户快速标注、复审和分享标注，以及可视化和评估数据集上的机器学习预测结果。工具支持迭代式的工作流程，在用户标注和机器学习预测之间进行反馈，以提高标注质量和模型性能。3. 我们示例了Whombat的灵活性，通过两个不同的应用场景：一是英国蝙蝠保护基金会（BCT）的自动蝙蝠呼声识别项目，二是美国森林服务和奥REGON州立大学合作的生物声学应用研究，推进了自动鸟类分类模型的扩展和改进。4. Whombat是一款适用于生物声学研究的灵活工具，可以有效地解决声音标注的挑战。它可以用于个人和团队的合作工作，可以在共享服务器上Host或远程访问，或者在个人电脑上运行，无需编程技能。</details></li>
</ol>
<hr>
<h2 id="Naaloss-Rethinking-the-objective-of-speech-enhancement"><a href="#Naaloss-Rethinking-the-objective-of-speech-enhancement" class="headerlink" title="Naaloss: Rethinking the objective of speech enhancement"></a>Naaloss: Rethinking the objective of speech enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12615">http://arxiv.org/abs/2308.12615</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kuan-Hsun Ho, En-Lun Yu, Jeih-weih Hung, Berlin Chen</li>
<li>for: 提高自动语音识别（ASR）系统在实际场景中的性能，即减少噪声污染的影响。</li>
<li>methods: 提出一种听力和artefacts-aware损失函数（NAaLoss），以便SE模型能够分别模型语音、artefacts和噪声。</li>
<li>results: 对于大多数设置，NAaLoss可以显著提高ASR性能，同时保持SE模型的质量，并且通过波形和spectrogram来视觉化artefacts的影响。<details>
<summary>Abstract</summary>
Reducing noise interference is crucial for automatic speech recognition (ASR) in a real-world scenario. However, most single-channel speech enhancement (SE) generates "processing artifacts" that negatively affect ASR performance. Hence, in this study, we suggest a Noise- and Artifacts-aware loss function, NAaLoss, to ameliorate the influence of artifacts from a novel perspective. NAaLoss considers the loss of estimation, de-artifact, and noise ignorance, enabling the learned SE to individually model speech, artifacts, and noise. We examine two SE models (simple/advanced) learned with NAaLoss under various input scenarios (clean/noisy) using two configurations of the ASR system (with/without noise robustness). Experiments reveal that NAaLoss significantly improves the ASR performance of most setups while preserving the quality of SE toward perception and intelligibility. Furthermore, we visualize artifacts through waveforms and spectrograms, and explain their impact on ASR.
</details>
<details>
<summary>摘要</summary>
减少干扰是自动语音识别（ASR）在实际场景中的关键。然而，大多数单通道语音增强（SE）生成“处理残留”，这些残留对ASR性能产生负面影响。因此，在本研究中，我们建议一种听力和痕迹感知损失函数（NAaLoss），以改善处理残留的影响。NAaLoss考虑语音估算损失、去痕迹和干扰无关的损失，使得学习的SE能够分别模型语音、痕迹和干扰。我们使用两种SE模型（简单/高级），在不同的输入场景（干扰/不干扰）下，使用两种ASR系统（含/不含干扰鲁然）进行了两种配置。实验表明，NAaLoss可以提高大多数配置中ASR性能的质量，同时保持SE的质量与人类听力和可读性的关系。此外，我们使用波形和spectrogram来视觉化痕迹，并解释其对ASR的影响。
</details></li>
</ul>
<hr>
<h2 id="Emotion-Aligned-Contrastive-Learning-Between-Images-and-Music"><a href="#Emotion-Aligned-Contrastive-Learning-Between-Images-and-Music" class="headerlink" title="Emotion-Aligned Contrastive Learning Between Images and Music"></a>Emotion-Aligned Contrastive Learning Between Images and Music</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12610">http://arxiv.org/abs/2308.12610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shanti Stewart, Tiantian Feng, Kleanthis Avramidis, Shrikanth Narayanan</li>
<li>for: 这个研究旨在 Retrieving emotionally-relevant music from image queries by proposing a framework for learning an affective alignment between images and music audio.</li>
<li>methods: 方法包括 learning an emotion-aligned joint embedding space between images and music, using emotion-supervised contrastive learning, and directly evaluating the joint embeddings with cross-modal retrieval tasks based on emotion labels.</li>
<li>results: 实验结果显示，我们的方法成功将图像和音乐联系起来，并且学习的 embedding space 具有跨模式 Retrieval 应用的效果。<details>
<summary>Abstract</summary>
Traditional music search engines rely on retrieval methods that match natural language queries with music metadata. There have been increasing efforts to expand retrieval methods to consider the audio characteristics of music itself, using queries of various modalities including text, video, and speech. Most approaches aim to match general music semantics to the input queries, while only a few focus on affective qualities. We address the task of retrieving emotionally-relevant music from image queries by proposing a framework for learning an affective alignment between images and music audio. Our approach focuses on learning an emotion-aligned joint embedding space between images and music. This joint embedding space is learned via emotion-supervised contrastive learning, using an adapted cross-modal version of the SupCon loss. We directly evaluate the joint embeddings with cross-modal retrieval tasks (image-to-music and music-to-image) based on emotion labels. In addition, we investigate the generalizability of the learned music embeddings with automatic music tagging as a downstream task. Our experiments show that our approach successfully aligns images and music, and that the learned embedding space is effective for cross-modal retrieval applications.
</details>
<details>
<summary>摘要</summary>
传统音乐搜索引擎通常使用自然语言查询匹配音乐元数据。随着扩展到考虑音乐自身特征的搜索方法的努力，包括文本、视频和语音等多种模式的查询。大多数方法都是尝试将通用音乐Semantics匹配到输入查询，只有一些关注情感特征。我们解决通过图像查询获取情感相关的音乐的任务，我们提出了一个框架，通过学习图像和音乐的情感相对适应来学习图像和音乐的共同嵌入空间。我们的方法通过情感supervised contrastive learning，使用适应的跨模态版本的SupCon损失函数来学习图像和音乐的共同嵌入空间。我们直接测试了图像和音乐的 JOINT 嵌入空间，基于情感标签来进行跨模态检索任务（图像到音乐和音乐到图像）。此外，我们还 investigate了学习音乐嵌入空间的通用性，用于下游自动音乐标签任务。我们的实验表明，我们的方法成功地将图像和音乐进行了对应，并且学习的嵌入空间是跨模态检索应用中有效。
</details></li>
</ul>
<hr>
<h2 id="Hybrid-noise-shaping-for-audio-coding-using-perfectly-overlapped-window"><a href="#Hybrid-noise-shaping-for-audio-coding-using-perfectly-overlapped-window" class="headerlink" title="Hybrid noise shaping for audio coding using perfectly overlapped window"></a>Hybrid noise shaping for audio coding using perfectly overlapped window</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12566">http://arxiv.org/abs/2308.12566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Byeongho Jo, Seungkwon Beack</li>
<li>for: 低比特率音频编码</li>
<li>methods: 模拟幂等幂Transform基于TCX和复杂LPC-基于CTNS的编码框架，使用50%重叠窗口和切换方案提高编码效率，并使用频谱板面信息基于逐档LPC减法来计算目标位数。</li>
<li>results: 对比对metric和主观听测试，提出的编码框架显示出优于对比的表现，达到了low bit-rate音频编码。<details>
<summary>Abstract</summary>
In recent years, audio coding technology has been standardized based on several frameworks that incorporate linear predictive coding (LPC). However, coding the transient signal using frequency-domain LP residual signals remains a challenge. To address this, temporal noise shaping (TNS) can be adapted, although it cannot be effectively operated since the estimated temporal envelope in the modified discrete cosine transform (MDCT) domain is accompanied by the time-domain aliasing (TDA) terms. In this study, we propose the modulated complex lapped transform-based coding framework integrated with transform coded excitation (TCX) and complex LPC-based TNS (CTNS). Our approach uses a 50\% overlap window and switching scheme for the CTNS to improve the coding efficiency. Additionally, an adaptive calculation of the target bits for the sub-bands using the frequency envelope information based on the quantized LPC coefficients is proposed. To minimize the quantization mismatch between both modes, an integrated quantization for real and complex values and a TDA augmentation method that compensates for the artificially generated TDA components during switching operations are proposed. The proposed coding framework shows a superior performance in both objective metrics and subjective listening tests, thereby demonstrating its low bit-rate audio coding.
</details>
<details>
<summary>摘要</summary>
近年来，音频编码技术基于多个框架实现标准化，其中包括线性预测编码（LPC）。然而，使用频域LP残差信号编码传递信号仍然是一个挑战。为此，可以采用时间噪声扭曲（TNS）技术，但是在MDCT频域中估计的时间板声信号会受到时间域抖音（TDA）的影响。在这项研究中，我们提出了基于模拟复杂延迟变换（MCLT）的编码框架，并与转换编码刺激（TCX）和复杂LPC-基于TNS（CTNS）相结合。我们的方法使用50%的重叠窗口和切换方案来提高编码效率。此外，我们还提出了基于频率封频信息来计算子带目标位数的可适应算法。为了减少两种模式之间的量化差异，我们提出了整合量化和TDA扩充方法。该编码框架在对象和主观听测中显示出优秀的低比特率音频编码性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/08/24/cs.SD_2023_08_24/" data-id="cllskiwc9005fid88by8cdwb1" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/24/cs.LG_2023_08_24/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-08-24 18:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/24/eess.AS_2023_08_24/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.AS - 2023-08-24</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'', root:''}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
