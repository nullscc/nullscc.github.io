
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-08-24 123:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Sparks of Large Audio Models: A Survey and Outlook paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.12792 repo_url: None paper_authors: Siddique Latif, Moazzam Shoukat, Fahad Shamshad, Muhammad Usama, Heriberto C">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-08-24 123:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/24/cs.SD_2023_08_24/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Sparks of Large Audio Models: A Survey and Outlook paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.12792 repo_url: None paper_authors: Siddique Latif, Moazzam Shoukat, Fahad Shamshad, Muhammad Usama, Heriberto C">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-23T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:37.017Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_08_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/24/cs.SD_2023_08_24/" class="article-date">
  <time datetime="2023-08-23T16:00:00.000Z" itemprop="datePublished">2023-08-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-08-24 123:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Sparks-of-Large-Audio-Models-A-Survey-and-Outlook"><a href="#Sparks-of-Large-Audio-Models-A-Survey-and-Outlook" class="headerlink" title="Sparks of Large Audio Models: A Survey and Outlook"></a>Sparks of Large Audio Models: A Survey and Outlook</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12792">http://arxiv.org/abs/2308.12792</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddique Latif, Moazzam Shoukat, Fahad Shamshad, Muhammad Usama, Heriberto Cuayáhuitl, Björn W. Schuller</li>
<li>for: 这份论文提供了最近的进展和挑战在应用大型语言模型于音频处理领域。</li>
<li>methods: 这份论文探讨了现代大型语言模型的应用方法，包括基于传播器架构的模型，以及它们在不同的音频任务上的表现。</li>
<li>results: 这份论文总结了现有的大型语言模型在音频处理领域的表现，包括自动语音识别、文本读取和音乐生成等多个任务。此外，这些模型还在不同的语言上展示了多语言支持和可转换性。<details>
<summary>Abstract</summary>
This survey paper provides a comprehensive overview of the recent advancements and challenges in applying large language models to the field of audio signal processing. Audio processing, with its diverse signal representations and a wide range of sources--from human voices to musical instruments and environmental sounds--poses challenges distinct from those found in traditional Natural Language Processing scenarios. Nevertheless, \textit{Large Audio Models}, epitomized by transformer-based architectures, have shown marked efficacy in this sphere. By leveraging massive amount of data, these models have demonstrated prowess in a variety of audio tasks, spanning from Automatic Speech Recognition and Text-To-Speech to Music Generation, among others. Notably, recently these Foundational Audio Models, like SeamlessM4T, have started showing abilities to act as universal translators, supporting multiple speech tasks for up to 100 languages without any reliance on separate task-specific systems. This paper presents an in-depth analysis of state-of-the-art methodologies regarding \textit{Foundational Large Audio Models}, their performance benchmarks, and their applicability to real-world scenarios. We also highlight current limitations and provide insights into potential future research directions in the realm of \textit{Large Audio Models} with the intent to spark further discussion, thereby fostering innovation in the next generation of audio-processing systems. Furthermore, to cope with the rapid development in this area, we will consistently update the relevant repository with relevant recent articles and their open-source implementations at https://github.com/EmulationAI/awesome-large-audio-models.
</details>
<details>
<summary>摘要</summary>
这份调查论文提供了大语音模型在听音信号处理领域的最新进展和挑战。听音信号处理领域的多样化信号表示和广泛的来源，从人声到乐器和环境声音，与传统自然语言处理场景不同，但是大语音模型，如基于转换器的架构，在这个领域表现出了突出的能力。通过吞吐大量数据，这些模型在各种听音任务中表现出了多样化的能力，从自动语音识别和文本转语音到音乐生成等。尤其是最近，这些基础Audio模型，如SeamlessM4T，已经开始展示了多语言支持的能力，无需单独的任务特定系统。这篇论文对现状的方法ologies进行了深入分析，包括基础大语音模型的性能标准和其在实际场景中的应用性。我们还提出了当前的限制和未来研究方向，以便在下一代听音处理系统中促进更多的创新。此外，为了应对这一领域的快速发展，我们将在https://github.com/EmulationAI/awesome-large-audio-models中不断更新相关的文章和其开源实现。
</details></li>
</ul>
<hr>
<h2 id="WavMark-Watermarking-for-Audio-Generation"><a href="#WavMark-Watermarking-for-Audio-Generation" class="headerlink" title="WavMark: Watermarking for Audio Generation"></a>WavMark: Watermarking for Audio Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12770">http://arxiv.org/abs/2308.12770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangyu Chen, Yu Wu, Shujie Liu, Tao Liu, Xiaoyong Du, Furu Wei</li>
<li>for: 这篇论文旨在探讨一种新的声音杀除技术，可以在几秒钟的音频记录上模仿说话者的声音，同时保持高度的真实性。</li>
<li>methods: 该技术使用了一种新的音频杀除框架，可以在1秒钟的音频记录上编码32位的水印，并且这个水印是不可见的，具有强大的鲁棒性，可以抵御多种攻击。</li>
<li>results: 该技术可以具有高度的鲁棒性和扩展性，可以在10-20秒的音频上实现0.48%的比特错误率，与现有技术相比，可以提高2800%的比特错误率。<details>
<summary>Abstract</summary>
Recent breakthroughs in zero-shot voice synthesis have enabled imitating a speaker's voice using just a few seconds of recording while maintaining a high level of realism. Alongside its potential benefits, this powerful technology introduces notable risks, including voice fraud and speaker impersonation. Unlike the conventional approach of solely relying on passive methods for detecting synthetic data, watermarking presents a proactive and robust defence mechanism against these looming risks. This paper introduces an innovative audio watermarking framework that encodes up to 32 bits of watermark within a mere 1-second audio snippet. The watermark is imperceptible to human senses and exhibits strong resilience against various attacks. It can serve as an effective identifier for synthesized voices and holds potential for broader applications in audio copyright protection. Moreover, this framework boasts high flexibility, allowing for the combination of multiple watermark segments to achieve heightened robustness and expanded capacity. Utilizing 10 to 20-second audio as the host, our approach demonstrates an average Bit Error Rate (BER) of 0.48\% across ten common attacks, a remarkable reduction of over 2800\% in BER compared to the state-of-the-art watermarking tool. See https://aka.ms/wavmark for demos of our work.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)最近的零批训练语音合成技术突破有了可以使用只需几秒钟的录音来实现高度真实的语音imitating的能力。这些技术具有可观的可能性，但同时也 introduce了一些风险，如语音fraud和speaker impersonation。与传统方法所取得的只通过静止方法检测合成数据的approach不同， watermarking 提供了一种积极和强大的防御机制。这篇论文介绍了一种创新的音频 watermarking 框架，可以在1秒钟的音频片断中编码 Up to 32 bits的水印。这个水印是人类感知不到的，并且在不同的攻击下显示出强大的抗性。它可以作为合成voice的标识符，并且有可能在更广泛的音频版权保护领域得到应用。此外，这个框架具有高度的灵活性，可以将多个水印段组合以实现更高的强度和扩展性。使用10-20秒的音频作为主体，我们的方法在10种常见的攻击下demonstrate了 average Bit Error Rate（BER）为0.48%，相比之下，现有的 watermarking 工具的BER下降了2800%以上。请参考https://aka.ms/wavmark 查看我们的工作示例。
</details></li>
</ul>
<hr>
<h2 id="Whombat-An-open-source-annotation-tool-for-machine-learning-development-in-bioacoustics"><a href="#Whombat-An-open-source-annotation-tool-for-machine-learning-development-in-bioacoustics" class="headerlink" title="Whombat: An open-source annotation tool for machine learning development in bioacoustics"></a>Whombat: An open-source annotation tool for machine learning development in bioacoustics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12688">http://arxiv.org/abs/2308.12688</a></li>
<li>repo_url: None</li>
<li>paper_authors: Santiago Martinez Balvanera, Oisin Mac Aodha, Matthew J. Weldy, Holly Pringle, Ella Browning, Kate E. Jones</li>
<li>for: 这个论文旨在提高生物声学记录的自动分析，使用机器学习方法可以大幅提高生物多样性监测的规模。</li>
<li>methods: 这篇论文使用了一种名为Whombat的用户友好的浏览器基本的界面，用于管理音频记录和注释项目，并提供了许多视觉化、探索和注释工具。</li>
<li>results: 作者通过使用Whombat工具，成功地解决了音频记录注释的一些挑战，如管理大量的记录和相关metadata，开发 flexible的注释工具，并解决专家注释员的缺乏问题。<details>
<summary>Abstract</summary></li>
</ul>
<ol>
<li>Automated analysis of bioacoustic recordings using machine learning (ML) methods has the potential to greatly scale biodiversity monitoring efforts. The use of ML for high-stakes applications, such as conservation research, demands a data-centric approach with a focus on utilizing carefully annotated and curated evaluation and training data that is relevant and representative. Creating annotated datasets of sound recordings presents a number of challenges, such as managing large collections of recordings with associated metadata, developing flexible annotation tools that can accommodate the diverse range of vocalization profiles of different organisms, and addressing the scarcity of expert annotators.   2. We present Whombat a user-friendly, browser-based interface for managing audio recordings and annotation projects, with several visualization, exploration, and annotation tools. It enables users to quickly annotate, review, and share annotations, as well as visualize and evaluate a set of machine learning predictions on a dataset. The tool facilitates an iterative workflow where user annotations and machine learning predictions feedback to enhance model performance and annotation quality.   3. We demonstrate the flexibility of Whombat by showcasing two distinct use cases: an project aimed at enhancing automated UK bat call identification at the Bat Conservation Trust (BCT), and a collaborative effort among the USDA Forest Service and Oregon State University researchers exploring bioacoustic applications and extending automated avian classification models in the Pacific Northwest, USA.   4. Whombat is a flexible tool that can effectively address the challenges of annotation for bioacoustic research. It can be used for individual and collaborative work, hosted on a shared server or accessed remotely, or run on a personal computer without the need for coding skills.</details>
<details>
<summary>摘要</summary></li>
<li>机器学习（ML）技术可以帮助自动分析生物声音记录，大大提高生物多样性监测的规模。在保护研究等高度重要应用领域使用ML时，需要一种数据驱动的方法，强调使用仔细标注和整理的评估和培训数据，该数据是有 relevance 和 representativeness 的。创建声音记录的标注数据集存在许多挑战，如管理大量记录和相关 metadata，开发可扩展的标注工具，以便 Accommodate 不同生物体种的声音profile。2. 我们提供了一个用户友好的浏览器基本的界面，用于管理声音记录和标注项目，包括视觉化、探索和标注工具。它允许用户快速标注、复审和分享标注，以及可视化和评估一个数据集上的机器学习预测结果。工具支持迭代式的工作流程，在用户标注和机器学习预测之间进行反馈，以提高标注质量和模型性能。3. 我们在两个不同的应用场景中展示了 Whombat 的灵活性：一个是英国蝙蝠保护协会（BCT）的自动蝙蝠叫出识别项目，另一个是美国农业部和奥REGON州立大学合作的生物声音应用研究，探索生物声音应用和自动鸟类分类模型在太平洋北西部的扩展。4. Whombat 是一种灵活的工具，可以有效地解决生物声音研究中的标注挑战。它可以用于个人和团队的工作，可以在共享服务器上hosts或远程访问，或者在个人电脑上运行，无需编程技能。</details></li>
</ol>
<hr>
<h2 id="Naaloss-Rethinking-the-objective-of-speech-enhancement"><a href="#Naaloss-Rethinking-the-objective-of-speech-enhancement" class="headerlink" title="Naaloss: Rethinking the objective of speech enhancement"></a>Naaloss: Rethinking the objective of speech enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12615">http://arxiv.org/abs/2308.12615</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kuan-Hsun Ho, En-Lun Yu, Jeih-weih Hung, Berlin Chen</li>
<li>for: 提高自动语音识别（ASR）系统在实际场景中的性能，减少干扰声音的影响。</li>
<li>methods: 提出了一种针对干扰声音的损失函数NAaLoss，通过考虑损失估计、去噪和干扰声音无关的三个方面，使SE模型能够准确地模型干扰声音、降低误差。</li>
<li>results: 对两种SE模型（简单&#x2F;高级）在不同的输入enario（干扰&#x2F;不干扰）和ASR系统的两种配置（含&#x2F;不含噪音鲁棒性）进行了测试，结果表明NAaLoss可以显著提高ASR性能，同时保持SE模型的质量。此外，通过波形和spectrogram的视觉化，解释了干扰声音对ASR的影响。<details>
<summary>Abstract</summary>
Reducing noise interference is crucial for automatic speech recognition (ASR) in a real-world scenario. However, most single-channel speech enhancement (SE) generates "processing artifacts" that negatively affect ASR performance. Hence, in this study, we suggest a Noise- and Artifacts-aware loss function, NAaLoss, to ameliorate the influence of artifacts from a novel perspective. NAaLoss considers the loss of estimation, de-artifact, and noise ignorance, enabling the learned SE to individually model speech, artifacts, and noise. We examine two SE models (simple/advanced) learned with NAaLoss under various input scenarios (clean/noisy) using two configurations of the ASR system (with/without noise robustness). Experiments reveal that NAaLoss significantly improves the ASR performance of most setups while preserving the quality of SE toward perception and intelligibility. Furthermore, we visualize artifacts through waveforms and spectrograms, and explain their impact on ASR.
</details>
<details>
<summary>摘要</summary>
红外干扰减少是自动语音识别（ASR）实际场景中的关键因素。然而，大多数单通道speech增强（SE）生成“处理 artifacts”，这些artifacts negatively affect ASR性能。因此，在本研究中，我们提出了一种听力和 artifacts 意识的损失函数（NAaLoss），从新的角度来缓解 artifacts 的影响。NAaLoss 考虑了损失估计、去artifact、和干扰无关的损失，使得学习的 SE 可以单独模型speech、artifacts 和干扰。我们在不同的输入场景（干净/噪音）和 ASR 系统的两种配置（带/没有噪音鲁�能）中，使用两种 SE 模型（简单/高级），并对其进行了实验。实验结果表明，NAaLoss 可以明显提高 ASR 性能的大多数设置，同时保持 SE 的质量。此外，我们通过波形和spectrogram来可视化artefacts，并解释它们对 ASR 的影响。
</details></li>
</ul>
<hr>
<h2 id="Emotion-Aligned-Contrastive-Learning-Between-Images-and-Music"><a href="#Emotion-Aligned-Contrastive-Learning-Between-Images-and-Music" class="headerlink" title="Emotion-Aligned Contrastive Learning Between Images and Music"></a>Emotion-Aligned Contrastive Learning Between Images and Music</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12610">http://arxiv.org/abs/2308.12610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shanti Stewart, Tiantian Feng, Kleanthis Avramidis, Shrikanth Narayanan</li>
<li>for: 这篇论文主要针对的是如何通过图像查询检索具有情感特征的音乐。</li>
<li>methods: 本文提出了一种基于感受协调的图像-音乐 JOINT embedding 框架，通过情感调教的对照学习来学习图像和音乐之间的共同embedding空间。</li>
<li>results: 实验结果表明，该方法可以成功对图像和音乐进行对应的匹配，并且学习出的 embedding 空间有效地应用于跨模态检索任务。<details>
<summary>Abstract</summary>
Traditional music search engines rely on retrieval methods that match natural language queries with music metadata. There have been increasing efforts to expand retrieval methods to consider the audio characteristics of music itself, using queries of various modalities including text, video, and speech. Most approaches aim to match general music semantics to the input queries, while only a few focus on affective qualities. We address the task of retrieving emotionally-relevant music from image queries by proposing a framework for learning an affective alignment between images and music audio. Our approach focuses on learning an emotion-aligned joint embedding space between images and music. This joint embedding space is learned via emotion-supervised contrastive learning, using an adapted cross-modal version of the SupCon loss. We directly evaluate the joint embeddings with cross-modal retrieval tasks (image-to-music and music-to-image) based on emotion labels. In addition, we investigate the generalizability of the learned music embeddings with automatic music tagging as a downstream task. Our experiments show that our approach successfully aligns images and music, and that the learned embedding space is effective for cross-modal retrieval applications.
</details>
<details>
<summary>摘要</summary>
传统音乐搜索引擎通常采用匹配自然语言查询与音乐元数据的方法。随着扩展到考虑音乐自身特征的搜索方法的努力，包括文本、视频和speech等多种模式的查询。大多数方法尝试匹配通用音乐 semantics 与输入查询，只有一些关注情感质量。我们解决通过图像查询获取情感相关的音乐的任务，我们提议一种学习图像和音乐audio之间的情感匹配。我们的方法是学习图像和音乐之间的情感对齐的共同嵌入空间。我们使用基于情感标签的cross-modal SupCon损失函数进行情感激活的contrastive学习，以学习图像和音乐之间的情感匹配。我们直接测试共同嵌入的表现，使用图像到音乐和音乐到图像的跨模态检索任务，并评估下游自动音乐标签的可行性。我们的实验表明，我们的方法成功地将图像和音乐进行了对齐，并且学习的嵌入空间对跨模态检索应用有效。
</details></li>
</ul>
<hr>
<h2 id="Hybrid-noise-shaping-for-audio-coding-using-perfectly-overlapped-window"><a href="#Hybrid-noise-shaping-for-audio-coding-using-perfectly-overlapped-window" class="headerlink" title="Hybrid noise shaping for audio coding using perfectly overlapped window"></a>Hybrid noise shaping for audio coding using perfectly overlapped window</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12566">http://arxiv.org/abs/2308.12566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Byeongho Jo, Seungkwon Beack</li>
<li>for: 低比特率音频编码</li>
<li>methods: 模拟复复数扩展 transform-based coding框架、transformcoded excitation（TCX）和复复数 Laplace transform-based noise shaping（CTNS）</li>
<li>results: 提高编码效率和聆听评价表现<details>
<summary>Abstract</summary>
In recent years, audio coding technology has been standardized based on several frameworks that incorporate linear predictive coding (LPC). However, coding the transient signal using frequency-domain LP residual signals remains a challenge. To address this, temporal noise shaping (TNS) can be adapted, although it cannot be effectively operated since the estimated temporal envelope in the modified discrete cosine transform (MDCT) domain is accompanied by the time-domain aliasing (TDA) terms. In this study, we propose the modulated complex lapped transform-based coding framework integrated with transform coded excitation (TCX) and complex LPC-based TNS (CTNS). Our approach uses a 50\% overlap window and switching scheme for the CTNS to improve the coding efficiency. Additionally, an adaptive calculation of the target bits for the sub-bands using the frequency envelope information based on the quantized LPC coefficients is proposed. To minimize the quantization mismatch between both modes, an integrated quantization for real and complex values and a TDA augmentation method that compensates for the artificially generated TDA components during switching operations are proposed. The proposed coding framework shows a superior performance in both objective metrics and subjective listening tests, thereby demonstrating its low bit-rate audio coding.
</details>
<details>
<summary>摘要</summary>
(Note: The text has been translated into Simplified Chinese, which is the standardized form of Chinese used in mainland China. However, the translation may not be perfect and may not capture all the nuances of the original text.)
</details></li>
</ul>
<hr>
<h2 id="MultiPA-a-multi-task-speech-pronunciation-assessment-system-for-a-closed-and-open-response-scenario"><a href="#MultiPA-a-multi-task-speech-pronunciation-assessment-system-for-a-closed-and-open-response-scenario" class="headerlink" title="MultiPA: a multi-task speech pronunciation assessment system for a closed and open response scenario"></a>MultiPA: a multi-task speech pronunciation assessment system for a closed and open response scenario</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12490">http://arxiv.org/abs/2308.12490</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu-Wen Chen, Zhou Yu, Julia Hirschberg</li>
<li>for: 这个研究旨在开发一种多任务语音发音评估模型，以提供更加准确和全面的发音技巧评估。</li>
<li>methods: 该模型使用了多任务学习方法，并与其他神经网络模型相结合，以提高发音评估的准确性和可靠性。</li>
<li>results: 实验结果表明，该模型在关闭响应场景下的性能与以往的Kaldi-based系统相当，而在开放响应场景下的性能更为稳定和可靠。<details>
<summary>Abstract</summary>
The design of automatic speech pronunciation assessment can be categorized into closed and open response scenarios, each with strengths and limitations. A system with the ability to function in both scenarios can cater to diverse learning needs and provide a more precise and holistic assessment of pronunciation skills. In this study, we propose a Multi-task Pronunciation Assessment model called MultiPA. MultiPA provides an alternative to Kaldi-based systems in that it has simpler format requirements and better compatibility with other neural network models. Compared with previous open response systems, MultiPA provides a wider range of evaluations, encompassing assessments at both the sentence and word-level. Our experimental results show that MultiPA achieves comparable performance when working in closed response scenarios and maintains more robust performance when directly used for open responses.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Attention-Based-Acoustic-Feature-Fusion-Network-for-Depression-Detection"><a href="#Attention-Based-Acoustic-Feature-Fusion-Network-for-Depression-Detection" class="headerlink" title="Attention-Based Acoustic Feature Fusion Network for Depression Detection"></a>Attention-Based Acoustic Feature Fusion Network for Depression Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12478">http://arxiv.org/abs/2308.12478</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xuxiaoooo/abafnet">https://github.com/xuxiaoooo/abafnet</a></li>
<li>paper_authors: Xiao Xu, Yang Wang, Xinru Wei, Fei Wang, Xizhe Zhang</li>
<li>for: 这篇论文的目的是提出一个新的语音特征融合网络，以提高对抑郁症的检测。</li>
<li>methods: 这篇论文使用了一种新的注意力网络，将四种不同的语音特征融合到一个深度学习模型中，以实现多维度特征的有效融合。</li>
<li>results: 这篇论文的实验结果显示，这个新的方法可以在两个临床语音数据库中进行优化，并在抑郁症检测和亚型分类中表现出色，比前一些方法更好。<details>
<summary>Abstract</summary>
Depression, a common mental disorder, significantly influences individuals and imposes considerable societal impacts. The complexity and heterogeneity of the disorder necessitate prompt and effective detection, which nonetheless, poses a difficult challenge. This situation highlights an urgent requirement for improved detection methods. Exploiting auditory data through advanced machine learning paradigms presents promising research directions. Yet, existing techniques mainly rely on single-dimensional feature models, potentially neglecting the abundance of information hidden in various speech characteristics. To rectify this, we present the novel Attention-Based Acoustic Feature Fusion Network (ABAFnet) for depression detection. ABAFnet combines four different acoustic features into a comprehensive deep learning model, thereby effectively integrating and blending multi-tiered features. We present a novel weight adjustment module for late fusion that boosts performance by efficaciously synthesizing these features. The effectiveness of our approach is confirmed via extensive validation on two clinical speech databases, CNRAC and CS-NRAC, thereby outperforming previous methods in depression detection and subtype classification. Further in-depth analysis confirms the key role of each feature and highlights the importance of MFCCrelated features in speech-based depression detection.
</details>
<details>
<summary>摘要</summary>
抑郁症，一种常见的心理疾病，对个人和社会产生了深远的影响。由于这种疾病的复杂性和多样性，早期检测成为了一项紧迫的需求。然而，现有的方法主要基于单一的特征模型，可能会忽略潜在的语音特征信息。为了解决这个问题，我们提出了一种新的注意力基于的听音特征融合网络（ABAFnet），用于抑郁检测。ABAFnet组合了四种不同的听音特征，并将其集成到深度学习模型中，以有效地融合多维特征。我们还提出了一种新的权重调整模块，用于补做末级融合，从而提高表现。我们的方法在两个临床speech数据库中进行了广泛验证，并在抑郁检测和亚型分类方面表现出了超过前方法的优异性。进一步的深入分析表明，每种特征在抑郁检测中扮演着重要的角色，并且MFCC相关特征在语音基于的抑郁检测中具有重要性。
</details></li>
</ul>
<hr>
<h2 id="An-Initial-Exploration-Learning-to-Generate-Realistic-Audio-for-Silent-Video"><a href="#An-Initial-Exploration-Learning-to-Generate-Realistic-Audio-for-Silent-Video" class="headerlink" title="An Initial Exploration: Learning to Generate Realistic Audio for Silent Video"></a>An Initial Exploration: Learning to Generate Realistic Audio for Silent Video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12408">http://arxiv.org/abs/2308.12408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew Martel, Jackson Wagner</li>
<li>for: 这篇论文旨在开发一个基于深度学习的框架，用于创建电影和其他媒体中的吸引人声音效。</li>
<li>methods: 这篇论文使用了深度学习技术，将影像和声音融合为一体，并使用了多种模型架构，包括深度融合CNN、增压Wavenet CNN和transformer架构。</li>
<li>results: 研究发现，使用transformer架构可以将低频声音与视觉模式匹配得非常好，但是尚未能够生成更加细腻的波形。<details>
<summary>Abstract</summary>
Generating realistic audio effects for movies and other media is a challenging task that is accomplished today primarily through physical techniques known as Foley art. Foley artists create sounds with common objects (e.g., boxing gloves, broken glass) in time with video as it is playing to generate captivating audio tracks. In this work, we aim to develop a deep-learning based framework that does much the same - observes video in it's natural sequence and generates realistic audio to accompany it. Notably, we have reason to believe this is achievable due to advancements in realistic audio generation techniques conditioned on other inputs (e.g., Wavenet conditioned on text). We explore several different model architectures to accomplish this task that process both previously-generated audio and video context. These include deep-fusion CNN, dilated Wavenet CNN with visual context, and transformer-based architectures. We find that the transformer-based architecture yields the most promising results, matching low-frequencies to visual patterns effectively, but failing to generate more nuanced waveforms.
</details>
<details>
<summary>摘要</summary>
生成电影和其他媒体中的真实音效是一项复杂的任务，主要通过物理方法实现，称为FOLEY艺术。FOLEY艺术家使用常见物品（例如拳击手套、碎玻璃）与视频同步生成吸引人的音轨。在这种工作中，我们希望通过深度学习框架来实现类似的目标——观察视频的自然序列，并生成真实的音效。我们认为这是可能的，因为现有的真实音效生成技术（例如文本conditioned Wavenet）的进步。我们检 explore了多种不同的模型架构来完成这个任务，包括深度融合CNN、扩展Wavenet CNN与视觉上下文、转换器基 Architecture。我们发现Transformer-based architecture最有前途，能够准确地匹配视觉模式下的低频谱，但是无法生成更复杂的波形。
</details></li>
</ul>
<hr>
<h2 id="AdVerb-Visually-Guided-Audio-Dereverberation"><a href="#AdVerb-Visually-Guided-Audio-Dereverberation" class="headerlink" title="AdVerb: Visually Guided Audio Dereverberation"></a>AdVerb: Visually Guided Audio Dereverberation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12370">http://arxiv.org/abs/2308.12370</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanjoy Chowdhury, Sreyan Ghosh, Subhrajyoti Dasgupta, Anton Ratnarajah, Utkarsh Tyagi, Dinesh Manocha</li>
<li>for: 增强杂音音频识别和人脸识别</li>
<li>methods: 利用视觉特征和听频特征共同学习计算清晰音频</li>
<li>results: 在三个下游任务中，包括语音提升、语音识别和人脸识别，与传统音频只和音频视频基线相比，提高了18%-82%的相对提升率，并在AVSpeech数据集上达到了非常满意的RT60错误值。<details>
<summary>Abstract</summary>
We present AdVerb, a novel audio-visual dereverberation framework that uses visual cues in addition to the reverberant sound to estimate clean audio. Although audio-only dereverberation is a well-studied problem, our approach incorporates the complementary visual modality to perform audio dereverberation. Given an image of the environment where the reverberated sound signal has been recorded, AdVerb employs a novel geometry-aware cross-modal transformer architecture that captures scene geometry and audio-visual cross-modal relationship to generate a complex ideal ratio mask, which, when applied to the reverberant audio predicts the clean sound. The effectiveness of our method is demonstrated through extensive quantitative and qualitative evaluations. Our approach significantly outperforms traditional audio-only and audio-visual baselines on three downstream tasks: speech enhancement, speech recognition, and speaker verification, with relative improvements in the range of 18% - 82% on the LibriSpeech test-clean set. We also achieve highly satisfactory RT60 error scores on the AVSpeech dataset.
</details>
<details>
<summary>摘要</summary>
我们介绍了 AdVerb，一种新的听视频去扩散框架，该框架使用视觉准确信息以及扩散声音来估算清晰声音。虽然听音只的去扩散是已经广泛研究的问题，但我们的方法将视觉modalitat incorporated into the framework，以实现听视频去扩散。给出了 recording environment中的图像，AdVerb使用了一种新的场景意识geometry-aware cross-modal transformer架构，该架构能够捕捉场景准确性和听视频cross-modal关系，生成一个复杂的理想比率幕，当应用于扩散声音时，可以预测清晰声音。我们的方法的效果被通过广泛的量化和质量评估证明。我们的方法在三个下游任务中表现出了显著的改善，即speech enhancement、speech recognition和speaker verification，相对于传统的听音只和听视频基线，在LibriSpeech测试集上的改善范围为18%-82%。我们还在AVSpeech数据集上获得了非常满意的RT60错误分数。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/24/cs.SD_2023_08_24/" data-id="cllurrpbe009psw88gp8jdt70" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/24/cs.LG_2023_08_24/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-08-24 18:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/24/eess.AS_2023_08_24/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.AS - 2023-08-24</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
