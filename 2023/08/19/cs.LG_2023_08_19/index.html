
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-08-19 18:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Efficient Representation Learning for Healthcare with Cross-Architectural Self-Supervision paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.10064 repo_url: https:&#x2F;&#x2F;github.com&#x2F;pranavsinghps1&#x2F;CASS paper_authors: Pr">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-08-19 18:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/19/cs.LG_2023_08_19/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Efficient Representation Learning for Healthcare with Cross-Architectural Self-Supervision paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.10064 repo_url: https:&#x2F;&#x2F;github.com&#x2F;pranavsinghps1&#x2F;CASS paper_authors: Pr">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-18T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:33.548Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_08_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/19/cs.LG_2023_08_19/" class="article-date">
  <time datetime="2023-08-18T16:00:00.000Z" itemprop="datePublished">2023-08-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-08-19 18:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Efficient-Representation-Learning-for-Healthcare-with-Cross-Architectural-Self-Supervision"><a href="#Efficient-Representation-Learning-for-Healthcare-with-Cross-Architectural-Self-Supervision" class="headerlink" title="Efficient Representation Learning for Healthcare with Cross-Architectural Self-Supervision"></a>Efficient Representation Learning for Healthcare with Cross-Architectural Self-Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10064">http://arxiv.org/abs/2308.10064</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pranavsinghps1/CASS">https://github.com/pranavsinghps1/CASS</a></li>
<li>paper_authors: Pranav Singh, Jacopo Cirrone</li>
<li>for: 这篇论文目的是提出一种新的自我超vised学习方法，以增强深度学习架构在医疗应用中的表现。</li>
<li>methods: 本文使用了一种新的siamese自我超vised学习方法，Synergistically leveraging Transformer和Convolutional Neural Networks (CNN) для高效学习。</li>
<li>results: 在四个不同的医疗数据集上，CASS-trained CNNs和Transformers已经超越了现有的自我超vised学习方法，仅使用1%的标签数据进行调整，协助得到了3.8%的平均提升。<details>
<summary>Abstract</summary>
In healthcare and biomedical applications, extreme computational requirements pose a significant barrier to adopting representation learning. Representation learning can enhance the performance of deep learning architectures by learning useful priors from limited medical data. However, state-of-the-art self-supervised techniques suffer from reduced performance when using smaller batch sizes or shorter pretraining epochs, which are more practical in clinical settings. We present Cross Architectural - Self Supervision (CASS) in response to this challenge. This novel siamese self-supervised learning approach synergistically leverages Transformer and Convolutional Neural Networks (CNN) for efficient learning. Our empirical evaluation demonstrates that CASS-trained CNNs and Transformers outperform existing self-supervised learning methods across four diverse healthcare datasets. With only 1% labeled data for finetuning, CASS achieves a 3.8% average improvement; with 10% labeled data, it gains 5.9%; and with 100% labeled data, it reaches a remarkable 10.13% enhancement. Notably, CASS reduces pretraining time by 69% compared to state-of-the-art methods, making it more amenable to clinical implementation. We also demonstrate that CASS is considerably more robust to variations in batch size and pretraining epochs, making it a suitable candidate for machine learning in healthcare applications.
</details>
<details>
<summary>摘要</summary>
在医疗和生物医学应用中，极高的计算需求成为了采用表示学习的重大障碍。表示学习可以提高深度学习架构的性能，但是现有的自我超vised学习技术在使用小批量或短时间预训练时表现不佳。为回应这个挑战，我们提出了跨建筑自我超vised学习（CASS）方法。这种新的siamese自我超vised学习方法可以有效地利用转换器和卷积神经网络（CNN）进行学习。我们的实验证明，CASS-trained CNNs和转换器在四个不同的医疗数据集上都能够超越现有的自我超vised学习方法。只有1%的标注数据进行微调，CASS可以实现3.8%的平均提高; 使用10%的标注数据，它可以获得5.9%的提高; 使用100%的标注数据，它可以达到10.13%的提高。另外，CASS可以降低预训练时间，比现有的方法减少69%，使其更适合在临床实施。我们还证明了CASS在批量大小和预训练轮次上的变化对其性能的影响较小，使其成为医学机器学习中适用的方法。
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Exact-Combinatorial-Optimization-via-RL-based-Initialization-–-A-Case-Study-in-Scheduling"><a href="#Accelerating-Exact-Combinatorial-Optimization-via-RL-based-Initialization-–-A-Case-Study-in-Scheduling" class="headerlink" title="Accelerating Exact Combinatorial Optimization via RL-based Initialization – A Case Study in Scheduling"></a>Accelerating Exact Combinatorial Optimization via RL-based Initialization – A Case Study in Scheduling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11652">http://arxiv.org/abs/2308.11652</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaqi Yin, Cunxi Yu</li>
<li>for: 本研究旨在提出一种创新的方法，使用机器学习（ML）解决 combinatorial 优化问题，用调度为例子。目标是提供优化和推理的 garanties，同时保持优化算法的运行时间成本。</li>
<li>methods: 我们提出了一种两阶段 RL-to-ILP 调度框架，包括以下三个步骤：1）RL 算法 acts as coarse-grain scheduler，2）解决 relaxation 和 3）精确的解决方案 via ILP。</li>
<li>results: 我们的框架能够与使用精确调度方法具有同样的调度性能，而且可以达到 128 倍的速度提升。这些结果基于实际的 EdgeTPU 平台，使用 ImageNet DNN 计算图为输入。此外，我们的框架还可以提供更好的在芯片上的执行时间和加速。<details>
<summary>Abstract</summary>
Scheduling on dataflow graphs (also known as computation graphs) is an NP-hard problem. The traditional exact methods are limited by runtime complexity, while reinforcement learning (RL) and heuristic-based approaches struggle with determinism and solution quality. This research aims to develop an innovative approach that employs machine learning (ML) for addressing combinatorial optimization problems, using scheduling as a case study. The goal is to provide guarantees in optimality and determinism while maintaining the runtime cost of heuristic methods. Specifically, we introduce a novel two-phase RL-to-ILP scheduling framework, which includes three steps: 1) RL solver acts as coarse-grain scheduler, 2) solution relaxation and 3) exact solving via ILP. Our framework demonstrates the same scheduling performance compared with using exact scheduling methods while achieving up to 128 $\times$ speed improvements. This was conducted on actual EdgeTPU platforms, utilizing ImageNet DNN computation graphs as input. Additionally, the framework offers improved on-chip inference runtime and acceleration compared to the commercially available EdgeTPU compiler.
</details>
<details>
<summary>摘要</summary>
“计划在数据流图（也称作计算图）是一个NP困难问题。传统的精确方法受到运行时复杂性的限制，而循环学习（RL）和规则基本方法受到束缚和解决质量的影响。这项研究旨在开发一种创新的方法，使用机器学习（ML）来解决 combinatorial 优化问题，使用计划为 caso study。目标是提供优化和决定性的保证，同时保持规则基本方法的运行时成本。具体来说，我们提出了一种新的两阶段RL-to-ILP 计划框架，包括以下三个步骤：1）RL 算法作为粗粒度调度器，2）解决缓和3）使用 ILP 进行精确解决。我们的框架实现了与使用精确调度方法相同的计划性表现，同时实现了Up to 128倍的速度提升。这些实验在实际的 EdgeTPU 平台上进行，使用 ImageNet DNN 计算图作为输入。此外，我们的框架也提供了比商业可用的 EdgeTPU 编译器更好的在处理器上的执行时间和加速。”
</details></li>
</ul>
<hr>
<h2 id="The-Snowflake-Hypothesis-Training-Deep-GNN-with-One-Node-One-Receptive-field"><a href="#The-Snowflake-Hypothesis-Training-Deep-GNN-with-One-Node-One-Receptive-field" class="headerlink" title="The Snowflake Hypothesis: Training Deep GNN with One Node One Receptive field"></a>The Snowflake Hypothesis: Training Deep GNN with One Node One Receptive field</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10051">http://arxiv.org/abs/2308.10051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kun Wang, Guohao Li, Shilong Wang, Guibin Zhang, Kai Wang, Yang You, Xiaojiang Peng, Yuxuan Liang, Yang Wang</li>
<li>for: 本研究的目的是解释深度Graph Neural Networks（GNN）在图像领域的成功，并提出一种新的概念——雪花假设，以提高GNN的可读性和通用性。</li>
<li>methods: 本研究使用了多种方法，包括不同的训练方案、不同的束深度和层数、以及不同的层数和训练集。同时，本研究还使用了 simplest gradient和node-level cosine distance来规则节点聚合的深度。</li>
<li>results: 本研究的结果表明，雪花假设可以成为一种通用的操作，可以应用于多种GNN框架，提高其效果并提供可读性和可 generalized的选择网络深度。同时，本研究还发现，采用雪花假设可以减少GNN的过拟合和过熔化问题。<details>
<summary>Abstract</summary>
Despite Graph Neural Networks demonstrating considerable promise in graph representation learning tasks, GNNs predominantly face significant issues with over-fitting and over-smoothing as they go deeper as models of computer vision realm. In this work, we conduct a systematic study of deeper GNN research trajectories. Our findings indicate that the current success of deep GNNs primarily stems from (I) the adoption of innovations from CNNs, such as residual/skip connections, or (II) the tailor-made aggregation algorithms like DropEdge. However, these algorithms often lack intrinsic interpretability and indiscriminately treat all nodes within a given layer in a similar manner, thereby failing to capture the nuanced differences among various nodes. To this end, we introduce the Snowflake Hypothesis -- a novel paradigm underpinning the concept of ``one node, one receptive field''. The hypothesis draws inspiration from the unique and individualistic patterns of each snowflake, proposing a corresponding uniqueness in the receptive fields of nodes in the GNNs.   We employ the simplest gradient and node-level cosine distance as guiding principles to regulate the aggregation depth for each node, and conduct comprehensive experiments including: (1) different training schemes; (2) various shallow and deep GNN backbones, and (3) various numbers of layers (8, 16, 32, 64) on multiple benchmarks (six graphs including dense graphs with millions of nodes); (4) compare with different aggregation strategies. The observational results demonstrate that our hypothesis can serve as a universal operator for a range of tasks, and it displays tremendous potential on deep GNNs. It can be applied to various GNN frameworks, enhancing its effectiveness when operating in-depth, and guiding the selection of the optimal network depth in an explainable and generalizable way.
</details>
<details>
<summary>摘要</summary>
尽管图像神经网络（Graph Neural Networks，GNNs）在图表学习任务上表现出了诸多承诺，但它们在深度上遇到了许多问题，如预测和泛化。在这项工作中，我们进行了系统的深度GNN研究轨迹。我们的发现表明，当前深度GNN的成功主要归功于（I） adopting innovations from CNNs，如径规/跳过连接，或（II）适应性的聚合算法，如DropEdge。但这些算法通常缺乏内在解释性，并且不具备对不同节点的细化了的认知。因此，我们提出了雪花假设——一种新的思维方式，它提出了每个节点都有独特的感受领域。这种假设 Draws inspiration from the unique and individualistic patterns of each snowflake，并提出了对节点的聚合深度进行调控的一种新的方法。我们采用 simplest gradient 和 node-level cosine distance 作为导引原则，并进行了广泛的实验，包括：（1）不同的训练方案；（2）不同的束缚和深度的 GNN 基础架构；（3）不同层数（8, 16, 32, 64）在多个 Benchmark 上进行了多种实验。我们的观察结果表明，我们的假设可以作为一种通用的操作，并且在深度GNNs中表现出了巨大的潜力。它可以应用于不同的 GNN 框架，提高其在深度下的效果，并且可以在可解释和普适的方式下选择网络的最佳层数。
</details></li>
</ul>
<hr>
<h2 id="Computing-the-Vapnik-Chervonenkis-Dimension-for-Non-Discrete-Settings"><a href="#Computing-the-Vapnik-Chervonenkis-Dimension-for-Non-Discrete-Settings" class="headerlink" title="Computing the Vapnik Chervonenkis Dimension for Non-Discrete Settings"></a>Computing the Vapnik Chervonenkis Dimension for Non-Discrete Settings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10041">http://arxiv.org/abs/2308.10041</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammed Nechba, Mouhajir Mohamed, Sedjari Yassine</li>
<li>for: 本文旨在研究一种不受概念类或其域集的约束而 compute the VC dimension的方法。</li>
<li>methods: 本文使用了Empirical Risk Minimization（ERM）学习模型，以characterize the shattering property of a concept class。</li>
<li>results: 本文提出了一种可以approximately compute the VC dimension的方法，不受概念类或其域集的约束。<details>
<summary>Abstract</summary>
In 1984, Valiant [ 7 ] introduced the Probably Approximately Correct (PAC) learning framework for boolean function classes. Blumer et al. [ 2] extended this model in 1989 by introducing the VC dimension as a tool to characterize the learnability of PAC. The VC dimension was based on the work of Vapnik and Chervonenkis in 1971 [8 ], who introduced a tool called the growth function to characterize the shattering property. Researchers have since determined the VC dimension for specific classes, and efforts have been made to develop an algorithm that can calculate the VC dimension for any concept class. In 1991, Linial, Mansour, and Rivest [4] presented an algorithm for computing the VC dimension in the discrete setting, assuming that both the concept class and domain set were finite. However, no attempts had been made to design an algorithm that could compute the VC dimension in the general setting.Therefore, our work focuses on developing a method to approximately compute the VC dimension without constraints on the concept classes or their domain set. Our approach is based on our finding that the Empirical Risk Minimization (ERM) learning paradigm can be used as a new tool to characterize the shattering property of a concept class.
</details>
<details>
<summary>摘要</summary>
在1984年，Valiant（7）提出了一种名为“Probably Approximately Correct”（PAC）学习框架，用于 boolean 函数类型。Blumer等人（2）在1989年Extension this model by introducing the VC dimension as a tool to characterize the learnability of PAC.VC dimension是基于Vapnik和Chervonenkis（8）在1971年引入的一种工具，用于 caracterize the shattering property。研究人员已经确定了特定类型的VC dimension，并且有努力开发一个可以计算VC dimension for any concept class的算法。在1991年，Linial、Mansour和Rivest（4）提出了一种算法来计算VC dimension在离散设定下，假设概念类和域集都是finite。然而，没有任何尝试过开发一个可以计算VC dimension在通用设定下的算法。因此，我们的工作是关注开发一种可以约approximately compute VC dimension的方法，不受概念类或其域集的限制。我们的方法基于我们发现，Empirical Risk Minimization（ERM）学习模式可以用作一种新的工具来characterize the shattering property of a concept class。
</details></li>
</ul>
<hr>
<h2 id="Physics-guided-training-of-GAN-to-improve-accuracy-in-airfoil-design-synthesis"><a href="#Physics-guided-training-of-GAN-to-improve-accuracy-in-airfoil-design-synthesis" class="headerlink" title="Physics-guided training of GAN to improve accuracy in airfoil design synthesis"></a>Physics-guided training of GAN to improve accuracy in airfoil design synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10038">http://arxiv.org/abs/2308.10038</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kazunari Wada, Katsuyuki Suzuki, Kazuo Yonekura</li>
<li>for: 本研究使用生成对抗网络（GAN）进行机械形状的设计合成，但GANometimes输出物理不合理的形状。例如，当GAN模型被训练以输出符合需要的 aerodynamic表现的气流形状时，会出现显著的错误。这是因为GAN模型只考虑数据，不考虑下面的物理方程。本研究提出了通过physics-guided Training来导引GAN模型学习物理有效性。</li>
<li>methods: 本研究使用physics-guided Training来导引GAN模型学习物理有效性。通过general-purpose software locate outside the neural network model来计算物理有效性。</li>
<li>results: 数字实验显示，提议的模型可以减少错误，并且输出的形状与训练集数据不同，但仍满足物理有效性，超越现有的GAN模型的限制。<details>
<summary>Abstract</summary>
Generative adversarial networks (GAN) have recently been used for a design synthesis of mechanical shapes. A GAN sometimes outputs physically unreasonable shapes. For example, when a GAN model is trained to output airfoil shapes that indicate required aerodynamic performance, significant errors occur in the performance values. This is because the GAN model only considers data but does not consider the aerodynamic equations that lie under the data. This paper proposes the physics-guided training of the GAN model to guide the model to learn physical validity. Physical validity is computed using general-purpose software located outside the neural network model. Such general-purpose software cannot be used in physics-informed neural network frameworks, because physical equations must be implemented inside the neural network models. Additionally, a limitation of generative models is that the output data are similar to the training data and cannot generate completely new shapes. However, because the proposed model is guided by a physical model and does not use a training dataset, it can generate completely new shapes. Numerical experiments show that the proposed model drastically improves the accuracy. Moreover, the output shapes differ from those of the training dataset but still satisfy the physical validity, overcoming the limitations of existing GAN models.
</details>
<details>
<summary>摘要</summary>
《生成对抗网络（GAN）在机械形状设计中的应用》，Recently, GAN has been used for mechanical shape design. However, sometimes the output shapes of GAN are physically unreasonable. For example, when training a GAN model to output airfoil shapes that meet certain aerodynamic performance requirements, significant errors can occur in the performance values. This is because the GAN model only considers the data but does not consider the underlying aerodynamic equations. This paper proposes the physics-guided training of the GAN model to ensure physical validity. Physical validity is computed using general-purpose software located outside the neural network model. Unfortunately, such general-purpose software cannot be used in physics-informed neural network frameworks, because physical equations must be implemented inside the neural network models. Additionally, a limitation of generative models is that the output data are similar to the training data and cannot generate completely new shapes. However, because the proposed model is guided by a physical model and does not use a training dataset, it can generate completely new shapes. Numerical experiments show that the proposed model significantly improves accuracy. Moreover, the output shapes differ from those of the training dataset but still satisfy physical validity, overcoming the limitations of existing GAN models.
</details></li>
</ul>
<hr>
<h2 id="High-Performance-Computing-Applied-to-Logistic-Regression-A-CPU-and-GPU-Implementation-Comparison"><a href="#High-Performance-Computing-Applied-to-Logistic-Regression-A-CPU-and-GPU-Implementation-Comparison" class="headerlink" title="High Performance Computing Applied to Logistic Regression: A CPU and GPU Implementation Comparison"></a>High Performance Computing Applied to Logistic Regression: A CPU and GPU Implementation Comparison</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10037">http://arxiv.org/abs/2308.10037</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nechbamohammed/swiftlogisticreg">https://github.com/nechbamohammed/swiftlogisticreg</a></li>
<li>paper_authors: Nechba Mohammed, Mouhajir Mohamed, Sedjari Yassine</li>
<li>for: 这个研究是为了提高大数据集的binary classification的速度，使用GPU进行并行运算。</li>
<li>methods: 这个研究使用了X. Zou等人提出的平行Gradient Descent Logistic Regression算法，并将其Directly translate到GPU上。</li>
<li>results: 实验结果显示，我们的GPU-based LR比CPU-based实现更快，具有相似的f1分数。这种加速处理大数据集的能力使得我们的方法特别有利于实时预测应用，如影像识别、垃圾邮件检测和诈欺检测。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
We present a versatile GPU-based parallel version of Logistic Regression (LR), aiming to address the increasing demand for faster algorithms in binary classification due to large data sets. Our implementation is a direct translation of the parallel Gradient Descent Logistic Regression algorithm proposed by X. Zou et al. [12]. Our experiments demonstrate that our GPU-based LR outperforms existing CPU-based implementations in terms of execution time while maintaining comparable f1 score. The significant acceleration of processing large datasets makes our method particularly advantageous for real-time prediction applications like image recognition, spam detection, and fraud detection. Our algorithm is implemented in a ready-to-use Python library available at : https://github.com/NechbaMohammed/SwiftLogisticReg
</details>
<details>
<summary>摘要</summary>
我们提出了一种高性能的GPU基于的Logistic Regression（LR）版本，用于解决由大数据集引起的快速算法需求增加。我们的实现是基于平行梯度下降Logistic Regression算法的直译，由X. Zou等人提出。我们的实验表明，我们的GPU基于LR在执行时间方面与CPU基于实现相比具有明显的优势，同时保持相似的准确率。这种加速处理大数据集的能力使我们的方法在实时预测应用，如图像识别、垃圾邮件检测和诈骗检测等方面特别有优势。我们的算法在Python中实现，可以在：https://github.com/NechbaMohammed/SwiftLogisticReg 中获取。
</details></li>
</ul>
<hr>
<h2 id="Semi-Supervised-Anomaly-Detection-for-the-Determination-of-Vehicle-Hijacking-Tweets"><a href="#Semi-Supervised-Anomaly-Detection-for-the-Determination-of-Vehicle-Hijacking-Tweets" class="headerlink" title="Semi-Supervised Anomaly Detection for the Determination of Vehicle Hijacking Tweets"></a>Semi-Supervised Anomaly Detection for the Determination of Vehicle Hijacking Tweets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10036">http://arxiv.org/abs/2308.10036</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taahir Aiyoob Patel, Clement N. Nyirenda</li>
<li>for: 这个研究旨在使用推文来识别车质事件，以帮助旅行者免受车质事件的威胁。</li>
<li>methods: 这个研究使用了两种无监督侦错算法，包括K-Nearest Neighbour (KNN) 和 Cluster Based Outlier Factor (CBLOF)，以检测推文中的偏差。</li>
<li>results: 比较研究表明，CBLOF 方法的精度为 90%，而 KNN 方法的精度为 89%。CBLOF 方法也得到了 F1-Score 的 0.8，而 KNN 方法得到了 0.78。因此，CBLOF 方法 slightly outperformed KNN 方法，并被选为车质事件推文的侦错方法。<details>
<summary>Abstract</summary>
In South Africa, there is an ever-growing issue of vehicle hijackings. This leads to travellers constantly being in fear of becoming a victim to such an incident. This work presents a new semi-supervised approach to using tweets to identify hijacking incidents by using unsupervised anomaly detection algorithms. Tweets consisting of the keyword "hijacking" are obtained, stored, and processed using the term frequency-inverse document frequency (TF-IDF) and further analyzed by using two anomaly detection algorithms: 1) K-Nearest Neighbour (KNN); 2) Cluster Based Outlier Factor (CBLOF). The comparative evaluation showed that the KNN method produced an accuracy of 89%, whereas the CBLOF produced an accuracy of 90%. The CBLOF method was also able to obtain a F1-Score of 0.8, whereas the KNN produced a 0.78. Therefore, there is a slight difference between the two approaches, in favour of CBLOF, which has been selected as a preferred unsupervised method for the determination of relevant hijacking tweets. In future, a comparison will be done between supervised learning methods and the unsupervised methods presented in this work on larger dataset. Optimisation mechanisms will also be employed in order to increase the overall performance.
</details>
<details>
<summary>摘要</summary>
在南非， vehicular hijacking 问题日益严重。这使得旅行者们constantemente在担忧成为受害者的风险。本工作提出了一种新的半监督方法，使用 Twitter 上的异常检测算法来识别劫持事件。利用 TF-IDF 加以处理的 tweets 中包含 "劫持" 关键词，并使用 KNN 和 CBLOF 两种异常检测算法进行分析。对比评估表明，KNN 方法的准确率为 89%，CBLOF 方法的准确率为 90%，CBLOF 方法还可以获得 F1-Score 0.8，而 KNN 方法的 F1-Score 为 0.78。因此，CBLOF 方法在劫持 tweets 的识别中赢得了一定的优势，因此被选为半监督方法。未来，将对大型数据集进行比较，以及使用优化机制提高总性能。
</details></li>
</ul>
<hr>
<h2 id="Effects-of-Convolutional-Autoencoder-Bottleneck-Width-on-StarGAN-based-Singing-Technique-Conversion"><a href="#Effects-of-Convolutional-Autoencoder-Bottleneck-Width-on-StarGAN-based-Singing-Technique-Conversion" class="headerlink" title="Effects of Convolutional Autoencoder Bottleneck Width on StarGAN-based Singing Technique Conversion"></a>Effects of Convolutional Autoencoder Bottleneck Width on StarGAN-based Singing Technique Conversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10021">http://arxiv.org/abs/2308.10021</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tung-Cheng Su, Yung-Chuan Chang, Yi-Wen Liu</li>
<li>for: 将一种声音技巧转换为另一种声音技巧，保持原始歌手身份、旋律和语言元素不变</li>
<li>methods: 使用 Generative Adversarial Networks (GANs) 和 Convolutional Autoencoders (CAEs) 进行转换</li>
<li>results: 研究发现，宽度更大的瓶颈对转换质量有着良好的影响，但并不一定导致更高的类似性于目标技巧。其中，折唇声是转换最容易的目标，而其他三种声音技巧作为源则更能够生成更加吸引人的转换结果。<details>
<summary>Abstract</summary>
Singing technique conversion (STC) refers to the task of converting from one voice technique to another while leaving the original singer identity, melody, and linguistic components intact. Previous STC studies, as well as singing voice conversion research in general, have utilized convolutional autoencoders (CAEs) for conversion, but how the bottleneck width of the CAE affects the synthesis quality has not been thoroughly evaluated. To this end, we constructed a GAN-based multi-domain STC system which took advantage of the WORLD vocoder representation and the CAE architecture. We varied the bottleneck width of the CAE, and evaluated the conversion results subjectively. The model was trained on a Mandarin dataset which features four singers and four singing techniques: the chest voice, the falsetto, the raspy voice, and the whistle voice. The results show that a wider bottleneck corresponds to better articulation clarity but does not necessarily lead to higher likeness to the target technique. Among the four techniques, we also found that the whistle voice is the easiest target for conversion, while the other three techniques as a source produce more convincing conversion results than the whistle.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Semi-Implicit-Variational-Inference-via-Score-Matching"><a href="#Semi-Implicit-Variational-Inference-via-Score-Matching" class="headerlink" title="Semi-Implicit Variational Inference via Score Matching"></a>Semi-Implicit Variational Inference via Score Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10014">http://arxiv.org/abs/2308.10014</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/longinyu/sivism">https://github.com/longinyu/sivism</a></li>
<li>paper_authors: Longlin Yu, Cheng Zhang</li>
<li>for: 提高变量家族表达力，使其能够更好地捕捉复杂的 bayesian 推理问题。</li>
<li>methods: 基于代理证明对象的得分匹配方法，利用层次结构来自然地处理不可读取的变量分布。</li>
<li>results: 与 MCMC 相当精准，并且超过 ELBO 基于的 SIVI 方法在多种 bayesian 推理任务中表现出色。<details>
<summary>Abstract</summary>
Semi-implicit variational inference (SIVI) greatly enriches the expressiveness of variational families by considering implicit variational distributions defined in a hierarchical manner. However, due to the intractable densities of variational distributions, current SIVI approaches often use surrogate evidence lower bounds (ELBOs) or employ expensive inner-loop MCMC runs for unbiased ELBOs for training. In this paper, we propose SIVI-SM, a new method for SIVI based on an alternative training objective via score matching. Leveraging the hierarchical structure of semi-implicit variational families, the score matching objective allows a minimax formulation where the intractable variational densities can be naturally handled with denoising score matching. We show that SIVI-SM closely matches the accuracy of MCMC and outperforms ELBO-based SIVI methods in a variety of Bayesian inference tasks.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate into Simplified Chinese semi-implicit variational inference (SIVI)  greatly enriches the expressiveness of variational families by considering implicit variational distributions defined in a hierarchical manner. However, due to the intractable densities of variational distributions, current SIVI approaches often use surrogate evidence lower bounds (ELBOs) or employ expensive inner-loop MCMC runs for unbiased ELBOs for training. In this paper, we propose SIVI-SM, a new method for SIVI based on an alternative training objective via score matching. Leveraging the hierarchical structure of semi-implicit variational families, the score matching objective allows a minimax formulation where the intractable variational densities can be naturally handled with denoising score matching. We show that SIVI-SM closely matches the accuracy of MCMC and outperforms ELBO-based SIVI methods in a variety of Bayesian inference tasks.Translate completed.
</details></li>
</ul>
<hr>
<h2 id="Distributionally-Robust-Cross-Subject-EEG-Decoding"><a href="#Distributionally-Robust-Cross-Subject-EEG-Decoding" class="headerlink" title="Distributionally Robust Cross Subject EEG Decoding"></a>Distributionally Robust Cross Subject EEG Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11651">http://arxiv.org/abs/2308.11651</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiehang Duan, Zhenyi Wang, Gianfranco Doretto, Fang Li, Cui Tao, Donald Adjeroh</li>
<li>for: 提高 Electroencephalography (EEG) 解码任务的性能</li>
<li>methods: 使用分布robust优化和 Wasserstein gradient flow (WGF) 进行数据演化</li>
<li>results: 比基eline方法 significanly 提高解码性能，特别是在具有各种损害的 EEG 信号下<details>
<summary>Abstract</summary>
Recently, deep learning has shown to be effective for Electroencephalography (EEG) decoding tasks. Yet, its performance can be negatively influenced by two key factors: 1) the high variance and different types of corruption that are inherent in the signal, 2) the EEG datasets are usually relatively small given the acquisition cost, annotation cost and amount of effort needed. Data augmentation approaches for alleviation of this problem have been empirically studied, with augmentation operations on spatial domain, time domain or frequency domain handcrafted based on expertise of domain knowledge. In this work, we propose a principled approach to perform dynamic evolution on the data for improvement of decoding robustness. The approach is based on distributionally robust optimization and achieves robustness by optimizing on a family of evolved data distributions instead of the single training data distribution. We derived a general data evolution framework based on Wasserstein gradient flow (WGF) and provides two different forms of evolution within the framework. Intuitively, the evolution process helps the EEG decoder to learn more robust and diverse features. It is worth mentioning that the proposed approach can be readily integrated with other data augmentation approaches for further improvements. We performed extensive experiments on the proposed approach and tested its performance on different types of corrupted EEG signals. The model significantly outperforms competitive baselines on challenging decoding scenarios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Disposable-Transfer-Learning-for-Selective-Source-Task-Unlearning"><a href="#Disposable-Transfer-Learning-for-Selective-Source-Task-Unlearning" class="headerlink" title="Disposable Transfer Learning for Selective Source Task Unlearning"></a>Disposable Transfer Learning for Selective Source Task Unlearning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09971">http://arxiv.org/abs/2308.09971</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seunghee Koh, Hyounguk Shon, Janghyeon Lee, Hyeong Gwon Hong, Junmo Kim</li>
<li>for: 本研究旨在提出一种新的转移学习方法，即可 dispose 的转移学习（DTL），以便在转移学习过程中保留目标任务的表现。</li>
<li>methods: 本研究提出了一种新的损失函数名为 Gradient Collision loss (GC loss)，用于 selectively 忘记源任务知识。 GC loss 使得梯度向量在不同批处理中的方向不同，以便减少知识泄露。</li>
<li>results: 研究表明，GC loss 是一种有效的方法来解决转移学习问题，可以保留目标任务表现，同时减少知识泄露。<details>
<summary>Abstract</summary>
Transfer learning is widely used for training deep neural networks (DNN) for building a powerful representation. Even after the pre-trained model is adapted for the target task, the representation performance of the feature extractor is retained to some extent. As the performance of the pre-trained model can be considered the private property of the owner, it is natural to seek the exclusive right of the generalized performance of the pre-trained weight. To address this issue, we suggest a new paradigm of transfer learning called disposable transfer learning (DTL), which disposes of only the source task without degrading the performance of the target task. To achieve knowledge disposal, we propose a novel loss named Gradient Collision loss (GC loss). GC loss selectively unlearns the source knowledge by leading the gradient vectors of mini-batches in different directions. Whether the model successfully unlearns the source task is measured by piggyback learning accuracy (PL accuracy). PL accuracy estimates the vulnerability of knowledge leakage by retraining the scrubbed model on a subset of source data or new downstream data. We demonstrate that GC loss is an effective approach to the DTL problem by showing that the model trained with GC loss retains the performance on the target task with a significantly reduced PL accuracy.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXTTransfer learning 广泛用于训练深度神经网络（DNN）以建立强大的表示。即使预训练模型被适应目标任务，表示性性能 OF 特征提取器也会保持一定程度的遗传。由于预训练模型的性能可以视为专有财产，因此是自然的寻求专利权的通用性表现。 To address this issue, we propose a new paradigm of transfer learning called disposable transfer learning (DTL), which discards only the source task without degrading the performance of the target task. To achieve knowledge disposal, we propose a novel loss function named Gradient Collision loss (GC loss). GC loss selectively unlearns the source knowledge by leading the gradient vectors of mini-batches in different directions. Whether the model successfully unlearns the source task is measured by piggyback learning accuracy (PL accuracy). PL accuracy estimates the vulnerability of knowledge leakage by retraining the scrubbed model on a subset of source data or new downstream data. We demonstrate that GC loss is an effective approach to the DTL problem by showing that the model trained with GC loss retains the performance on the target task with a significantly reduced PL accuracy.Note: The translation is done using a machine translation tool, and may not be perfect or idiomatic.
</details></li>
</ul>
<hr>
<h2 id="Tackling-Vision-Language-Tasks-Through-Learning-Inner-Monologues"><a href="#Tackling-Vision-Language-Tasks-Through-Learning-Inner-Monologues" class="headerlink" title="Tackling Vision Language Tasks Through Learning Inner Monologues"></a>Tackling Vision Language Tasks Through Learning Inner Monologues</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09970">http://arxiv.org/abs/2308.09970</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diji Yang, Kezhen Chen, Jinmeng Rao, Xiaoyuan Guo, Yawen Zhang, Jie Yang, Yi Zhang</li>
<li>for: 解决复杂的视觉语言问题，通过内部对话机制来协调语言模型和视觉模型。</li>
<li>methods: 提出了一种新的 Inner Monologue Multi-Modal Optimization（IMMO）方法，通过自然语言对话来促进语言模型和视觉模型之间的交互，并采用两个阶段训练方式来学习内部对话过程。</li>
<li>results: 对两个popular任务进行评估，结果表明，通过模拟内部对话机制，IMMO可以提高语义解释能力和推理能力，从而更有效地融合视觉和语言模型。此外，IMMO不需要人工定制的对话，可以在多个AI问题中广泛应用。<details>
<summary>Abstract</summary>
Visual language tasks require AI models to comprehend and reason with both visual and textual content. Driven by the power of Large Language Models (LLMs), two prominent methods have emerged: (1) the hybrid integration between LLMs and Vision-Language Models (VLMs), where visual inputs are firstly converted into language descriptions by VLMs, serving as inputs for LLMs to generate final answer(s); (2) visual feature alignment in language space, where visual inputs are encoded as embeddings and projected to LLMs' language space via further supervised fine-tuning. The first approach provides light training costs and interpretability but is hard to be optimized in an end-to-end fashion. The second approach presents decent performance, but feature alignment usually requires large amounts of training data and lacks interpretability. To tackle this dilemma, we propose a novel approach, Inner Monologue Multi-Modal Optimization (IMMO), to solve complex vision language problems by simulating inner monologue processes, a cognitive process in which an individual engages in silent verbal communication with themselves. We enable LLMs and VLMs to interact through natural language conversation and propose to use a two-stage training process to learn how to do the inner monologue (self-asking questions and answering questions). IMMO is evaluated on two popular tasks and the results suggest by emulating the cognitive phenomenon of internal dialogue, our approach can enhance reasoning and explanation abilities, contributing to the more effective fusion of vision and language models. More importantly, instead of using predefined human-crafted monologues, IMMO learns this process within the deep learning models, promising wider applicability to many different AI problems beyond vision language tasks.
</details>
<details>
<summary>摘要</summary>
“视觉语言任务需要人工智能模型理解和处理视觉和文本内容。受大语言模型（LLM）的能力驱动，两种主要方法得到推广：（1）将视觉输入转换为语言描述，并将其作为LLM的输入进行生成答案（2）在语言空间中对视觉输入进行可视化特征对齐，通过进一步的超vision fine-tuning来实现。首种方法可以减少训练成本和提高可读性，但实际上困难以在端到端方式进行优化。第二种方法可以达到可 Acceptable performance，但特征对齐通常需要大量的训练数据，并且缺乏可读性。为了解决这个困境，我们提出了一种新的方法——内部对话多模态优化（IMMO），用于解决复杂的视觉语言问题。我们通过模拟内部对话过程，让LLM和VLM之间进行自然语言交流，并提出了一个两阶段训练过程，以学习如何进行内部对话（自我问答和回答问题）。IMMO在两个流行任务上进行评估，结果表明，通过模拟内部对话，我们的方法可以提高理解和解释能力，为视觉语言模型的融合做出更有效的贡献。更重要的是，IMMO不使用预先定义的人类编写的对话，而是在深度学习模型中学习这个过程，这意味着我们的方法可以在许多不同的AI问题中应用。”
</details></li>
</ul>
<hr>
<h2 id="Anomaly-Aware-Semantic-Segmentation-via-Style-Aligned-OoD-Augmentation"><a href="#Anomaly-Aware-Semantic-Segmentation-via-Style-Aligned-OoD-Augmentation" class="headerlink" title="Anomaly-Aware Semantic Segmentation via Style-Aligned OoD Augmentation"></a>Anomaly-Aware Semantic Segmentation via Style-Aligned OoD Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09965">http://arxiv.org/abs/2308.09965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dan Zhang, Kaspar Sakmann, William Beluch, Robin Hutmacher, Yumeng Li</li>
<li>for: 提高自动驾驶中的未知物体检测精度</li>
<li>methods: 使用精度更高的OoD数据生成，并提出一种简单的精度调整方法</li>
<li>results: 通过少量的微调，可以使用预训练模型进行异常检测，并保持原始任务的性能In English, this translates to:</li>
<li>for: Improving the accuracy of unknown object detection in autonomous driving</li>
<li>methods: Using higher-quality OoD data generation, and proposing a simple fine-tuning method</li>
<li>results: By minimally fine-tuning a pre-trained model, we can use it for anomaly detection while maintaining the performance on the original task.<details>
<summary>Abstract</summary>
Within the context of autonomous driving, encountering unknown objects becomes inevitable during deployment in the open world. Therefore, it is crucial to equip standard semantic segmentation models with anomaly awareness. Many previous approaches have utilized synthetic out-of-distribution (OoD) data augmentation to tackle this problem. In this work, we advance the OoD synthesis process by reducing the domain gap between the OoD data and driving scenes, effectively mitigating the style difference that might otherwise act as an obvious shortcut during training. Additionally, we propose a simple fine-tuning loss that effectively induces a pre-trained semantic segmentation model to generate a ``none of the given classes" prediction, leveraging per-pixel OoD scores for anomaly segmentation. With minimal fine-tuning effort, our pipeline enables the use of pre-trained models for anomaly segmentation while maintaining the performance on the original task.
</details>
<details>
<summary>摘要</summary>
在自动驾驶中，遇到未知对象是不可避免的，因此需要在开放世界中部署标准 semantic segmentation 模型时提供异常意识。许多先前的方法使用了 synthetic out-of-distribution（OoD）数据增强来解决这个问题。在这种工作中，我们将 OoD 数据增强过程进行了改进，将驱动场景和 OoD 数据之间的领域差减少到最小化，从而减轻了训练中可能会作为短cut的样式差异。此外，我们提议一种简单的精通化损失函数，使得先验性学习的 semantic segmentation 模型在训练中能够生成“无任何给定类”的预测，利用每个像素的 OoD 分数进行异常分 segmentation。与 minimal fine-tuning 努力相比，我们的管道可以使用先验性学习的模型进行异常分 segmentation，同时保持原始任务的性能。
</details></li>
</ul>
<hr>
<h2 id="Towards-Self-Adaptive-Machine-Learning-Enabled-Systems-Through-QoS-Aware-Model-Switching"><a href="#Towards-Self-Adaptive-Machine-Learning-Enabled-Systems-Through-QoS-Aware-Model-Switching" class="headerlink" title="Towards Self-Adaptive Machine Learning-Enabled Systems Through QoS-Aware Model Switching"></a>Towards Self-Adaptive Machine Learning-Enabled Systems Through QoS-Aware Model Switching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09960">http://arxiv.org/abs/2308.09960</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sa4s-serc/adamls">https://github.com/sa4s-serc/adamls</a></li>
<li>paper_authors: Shubham Kulkarni, Arya Marda, Karthik Vaidhyanathan</li>
<li>For: 本研究旨在提出一种机器学习模型均衡器，以管理运行时uncertainty，提高机器学习生成系统（MLS）的可靠性和性能。* Methods: 本研究提出了一种基于多模型自适应的机器学习模型均衡器（AdaMLS），通过动态模型交换来维护系统和模型的性能平衡。* Results: 通过一个基于物体检测的自适应对象检测系统的证明，研究人员发现 AdaMLS 可以在不可预测的环境下提供优化的QoS保证，比Naive和单个最佳方案更高。<details>
<summary>Abstract</summary>
Machine Learning (ML), particularly deep learning, has seen vast advancements, leading to the rise of Machine Learning-Enabled Systems (MLS). However, numerous software engineering challenges persist in propelling these MLS into production, largely due to various run-time uncertainties that impact the overall Quality of Service (QoS). These uncertainties emanate from ML models, software components, and environmental factors. Self-adaptation techniques present potential in managing run-time uncertainties, but their application in MLS remains largely unexplored. As a solution, we propose the concept of a Machine Learning Model Balancer, focusing on managing uncertainties related to ML models by using multiple models. Subsequently, we introduce AdaMLS, a novel self-adaptation approach that leverages this concept and extends the traditional MAPE-K loop for continuous MLS adaptation. AdaMLS employs lightweight unsupervised learning for dynamic model switching, thereby ensuring consistent QoS. Through a self-adaptive object detection system prototype, we demonstrate AdaMLS's effectiveness in balancing system and model performance. Preliminary results suggest AdaMLS surpasses naive and single state-of-the-art models in QoS guarantees, heralding the advancement towards self-adaptive MLS with optimal QoS in dynamic environments.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Comparison-of-Adversarial-Learning-Techniques-for-Malware-Detection"><a href="#A-Comparison-of-Adversarial-Learning-Techniques-for-Malware-Detection" class="headerlink" title="A Comparison of Adversarial Learning Techniques for Malware Detection"></a>A Comparison of Adversarial Learning Techniques for Malware Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09958">http://arxiv.org/abs/2308.09958</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pavla Louthánová, Matouš Kozák, Martin Jureček, Mark Stamp</li>
<li>for: 本文 addresses the problem of generating adversarial malware samples, specifically malicious Windows Portable Executable files, to evaluate the effectiveness of different methods in evading machine learning-based malware detection.</li>
<li>methods: 本文使用 gradient-based, evolutionary algorithm-based, 和 reinforcement-based methods to generate adversarial samples, and then tests the generated samples against selected antivirus products.</li>
<li>results: 结果显示，使用优化后的恶意软件amples可以导致 incorrectly classify the file as benign, 并且生成的恶意软件amples可以成功用于其他检测模型。使用多个生成器可以创建新的恶意软件amples，并且使用 Gym-malware generator 可以 achieve the highest practical potential.<details>
<summary>Abstract</summary>
Machine learning has proven to be a useful tool for automated malware detection, but machine learning models have also been shown to be vulnerable to adversarial attacks. This article addresses the problem of generating adversarial malware samples, specifically malicious Windows Portable Executable files. We summarize and compare work that has focused on adversarial machine learning for malware detection. We use gradient-based, evolutionary algorithm-based, and reinforcement-based methods to generate adversarial samples, and then test the generated samples against selected antivirus products. We compare the selected methods in terms of accuracy and practical applicability. The results show that applying optimized modifications to previously detected malware can lead to incorrect classification of the file as benign. It is also known that generated malware samples can be successfully used against detection models other than those used to generate them and that using combinations of generators can create new samples that evade detection. Experiments show that the Gym-malware generator, which uses a reinforcement learning approach, has the greatest practical potential. This generator achieved an average sample generation time of 5.73 seconds and the highest average evasion rate of 44.11%. Using the Gym-malware generator in combination with itself improved the evasion rate to 58.35%.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="To-prune-or-not-to-prune-A-chaos-causality-approach-to-principled-pruning-of-dense-neural-networks"><a href="#To-prune-or-not-to-prune-A-chaos-causality-approach-to-principled-pruning-of-dense-neural-networks" class="headerlink" title="To prune or not to prune : A chaos-causality approach to principled pruning of dense neural networks"></a>To prune or not to prune : A chaos-causality approach to principled pruning of dense neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09955">http://arxiv.org/abs/2308.09955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rajan Sahu, Shivam Chadha, Nithin Nagaraj, Archana Mathur, Snehanshu Saha</li>
<li>for: 这篇论文的目的是提出一种基于 chaos in learning 的 neural network 缩小方法，以维持网络性能并且保留特征解释性。</li>
<li>methods: 本篇论文使用了 weight update 的 chaos in learning 来定义适当的缩小策略，并且通过 causality 来特定引起错分类的几个重要权重。</li>
<li>results: 根据实验结果显示，这种缩小策略可以将网络大小缩小到原来的一半，而且网络性能仍然保持在原本水准。同时，这种缩小策略仍然可以保留网络的特征解释性。<details>
<summary>Abstract</summary>
Reducing the size of a neural network (pruning) by removing weights without impacting its performance is an important problem for resource-constrained devices. In the past, pruning was typically accomplished by ranking or penalizing weights based on criteria like magnitude and removing low-ranked weights before retraining the remaining ones. Pruning strategies may also involve removing neurons from the network in order to achieve the desired reduction in network size. We formulate pruning as an optimization problem with the objective of minimizing misclassifications by selecting specific weights. To accomplish this, we have introduced the concept of chaos in learning (Lyapunov exponents) via weight updates and exploiting causality to identify the causal weights responsible for misclassification. Such a pruned network maintains the original performance and retains feature explainability.
</details>
<details>
<summary>摘要</summary>
将神经网络的大小缩小（裁剪），无需影响其表现，是资源有限设备上的重要问题。在过去，裁剪通常通过按照字段大小或字段排名来选择丢弃重要性较低的字段，然后重新训练剩下的字段。裁剪策略可能还包括从网络中移除神经元，以达到预期的网络大小增加。我们将裁剪视为一个优化问题，并通过选择特定的字段来实现最小化错分。为此，我们引入了学习中的混乱（ Lyapunov 数据）via 字段更新，并利用因果关系来识别导致错分的字段。这样的裁剪网络保持原始表现，并保留特征解释性。
</details></li>
</ul>
<hr>
<h2 id="Finding-emergence-in-data-causal-emergence-inspired-dynamics-learning"><a href="#Finding-emergence-in-data-causal-emergence-inspired-dynamics-learning" class="headerlink" title="Finding emergence in data: causal emergence inspired dynamics learning"></a>Finding emergence in data: causal emergence inspired dynamics learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09952">http://arxiv.org/abs/2308.09952</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingzhe Yang, Zhipeng Wang, Kaiwei Liu, Yingqi Rong, Bing Yuan, Jiang Zhang</li>
<li>for: 这篇论文旨在开发一种基于数据驱动的模型，用于模拟复杂的动力系统，并可以有效地捕捉出 emergent 性质。</li>
<li>methods: 该论文提出了一种基于机器学习的框架，通过最大化有效信息（EI）来学习 macro-dinamics 模型，并且可以量化 emergence 在数据中。</li>
<li>results: 实验结果表明，该框架可以成功地捕捉出 emergent 模式，并且可以学习 coarse-graining 策略和量化数据中的 causal emergence 度。此外，对于不同于训练数据集的环境进行了测试，结果表明该模型具有出色的泛化能力。<details>
<summary>Abstract</summary>
Modelling complex dynamical systems in a data-driven manner is challenging due to the presence of emergent behaviors and properties that cannot be directly captured by micro-level observational data. Therefore, it is crucial to develop a model that can effectively capture emergent dynamics at the macro-level and quantify emergence based on the available data. Drawing inspiration from the theory of causal emergence, this paper introduces a machine learning framework aimed at learning macro-dynamics within an emergent latent space. The framework achieves this by maximizing the effective information (EI) to obtain a macro-dynamics model with stronger causal effects. Experimental results on both simulated and real data demonstrate the effectiveness of the proposed framework. Not only does it successfully capture emergent patterns, but it also learns the coarse-graining strategy and quantifies the degree of causal emergence in the data. Furthermore, experiments conducted on environments different from the training dataset highlight the superior generalization ability of our model.
</details>
<details>
<summary>摘要</summary>
模拟复杂动力系统的数据驱动方法是具有挑战性的，因为存在不可直接捕捉的emergent行为和性质。因此，需要开发一个能够有效捕捉emergent dynamics的 macro-级模型，并且量化emergence基于可用的数据。 drawing inspiration from the theory of causal emergence，本文提出了一种基于机器学习的框架，用于在emergent latent space中学习macro-dynamics。该框架通过最大化有效信息（EI）来获得一个具有更强的 causal effect的macro-dynamics模型。实验结果表明，该模型不仅能成功捕捉emergent pattern，还能学习coarse-graining strategy和量化数据中的causal emergence度。此外，在不同于训练数据的环境下进行的实验还表明了我们的模型具有更高的泛化能力。
</details></li>
</ul>
<hr>
<h2 id="Study-on-the-effectiveness-of-AutoML-in-detecting-cardiovascular-disease"><a href="#Study-on-the-effectiveness-of-AutoML-in-detecting-cardiovascular-disease" class="headerlink" title="Study on the effectiveness of AutoML in detecting cardiovascular disease"></a>Study on the effectiveness of AutoML in detecting cardiovascular disease</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09947">http://arxiv.org/abs/2308.09947</a></li>
<li>repo_url: None</li>
<li>paper_authors: T. V. Afanasieva, A. P. Kuzlyakin, A. V. Komolov</li>
<li>for: 这个研究旨在开发和应用patient-oriented系统，以帮助患有 chronic noncommunicable diseases 的患者早发现和预测心血管疾病。</li>
<li>methods: 该研究使用了自动机器学习（AutoML）技术，可以简化和加速AI&#x2F;ML应用的开发过程，使得医疗专业人员可以更容易地使用这些应用。</li>
<li>results: 研究发现，自动机器学习模型在检测心血管疾病方面的准确率在87.41%至92.3%之间，最高准确率达92.3%，并且发现数据Normalization技术对模型的准确率有较大影响。<details>
<summary>Abstract</summary>
Cardiovascular diseases are widespread among patients with chronic noncommunicable diseases and are one of the leading causes of death, including in the working age. The article presents the relevance of the development and application of patient-oriented systems, in which machine learning (ML) is a promising technology that allows predicting cardiovascular diseases. Automated machine learning (AutoML) makes it possible to simplify and speed up the process of developing AI/ML applications, which is key in the development of patient-oriented systems by application users, in particular medical specialists. The authors propose a framework for the application of automatic machine learning and three scenarios that allowed for data combining five data sets of cardiovascular disease indicators from the UCI Machine Learning Repository to investigate the effectiveness in detecting this class of diseases. The study investigated one AutoML model that used and optimized the hyperparameters of thirteen basic ML models (KNeighborsUnif, KNeighborsDist, LightGBMXT, LightGBM, RandomForestGini, RandomForestEntr, CatBoost, ExtraTreesGini, ExtraTreesEntr, NeuralNetFastA, XGBoost, NeuralNetTorch, LightGBMLarge) and included the most accurate models in the weighted ensemble. The results of the study showed that the structure of the AutoML model for detecting cardiovascular diseases depends not only on the efficiency and accuracy of the basic models used, but also on the scenarios for preprocessing the initial data, in particular, on the technique of data normalization. The comparative analysis showed that the accuracy of the AutoML model in detecting cardiovascular disease varied in the range from 87.41% to 92.3%, and the maximum accuracy was obtained when normalizing the source data into binary values, and the minimum was obtained when using the built-in AutoML technique.
</details>
<details>
<summary>摘要</summary>
心血管疾病非常普遍 среди慢性非传染疾病患者，是死亡的主要原因之一，包括在工作年龄。这篇文章介绍了开发和应用 patient-oriented 系统的重要性，其中机器学习（ML）是一种承诺的技术，可以预测心血管疾病。自动机器学习（AutoML）使得开发 AI/ML 应用的过程可以简化和加速，这对医疗专业人员特别重要。作者提出了一个框架，并在五个数据集中组合了心血管疾病指标数据，以调查这类疾病的检测效果。研究中使用了一个 AutoML 模型，该模型使用和优化了十三种基本 ML 模型（KNeighborsUnif、KNeighborsDist、LightGBMXT、LightGBM、RandomForestGini、RandomForestEntr、CatBoost、ExtraTreesGini、ExtraTreesEntr、NeuralNetFastA、XGBoost、NeuralNetTorch、LightGBMLarge），并包括最佳模型在权重ensemble中。研究结果表明，AutoML 模型的结构不仅受到基本模型的效率和准确度影响，还受到数据预处理方法的选择，特别是数据normalization技术。比较分析表明，AutoML 模型在检测心血管疾病方面的准确率在87.41%到92.3%之间，最高准确率为对源数据进行二分化normalization，最低准确率为使用自动 ML 技术。
</details></li>
</ul>
<hr>
<h2 id="Dual-Branch-Deep-Learning-Network-for-Detection-and-Stage-Grading-of-Diabetic-Retinopathy"><a href="#Dual-Branch-Deep-Learning-Network-for-Detection-and-Stage-Grading-of-Diabetic-Retinopathy" class="headerlink" title="Dual Branch Deep Learning Network for Detection and Stage Grading of Diabetic Retinopathy"></a>Dual Branch Deep Learning Network for Detection and Stage Grading of Diabetic Retinopathy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09945">http://arxiv.org/abs/2308.09945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hossein Shakibania, Sina Raoufi, Behnam Pourafkham, Hassan Khotanlou, Muharram Mansoorizadeh</li>
<li>for: 这个论文的目的是为了检测和分级糖尿病性视网膜病变，使用单一的视网膜图像。</li>
<li>methods: 这个模型使用了转移学习，使用两个现有的顶尖预训模型作为特征提取器，并对新的数据集进行精确化。</li>
<li>results: 这个模型在APTOS 2019数据集上实现了优异的表现，在糖尿病性视网膜检测和分级中，实现了98.50%的准确率、99.46%的感度和97.51%的特异度。<details>
<summary>Abstract</summary>
Diabetic retinopathy is a severe complication of diabetes that can lead to permanent blindness if not treated promptly. Early and accurate diagnosis of the disease is essential for successful treatment. This paper introduces a deep learning method for the detection and stage grading of diabetic retinopathy, using a single fundus retinal image. Our model utilizes transfer learning, employing two state-of-the-art pre-trained models as feature extractors and fine-tuning them on a new dataset. The proposed model is trained on a large multi-center dataset, including the APTOS 2019 dataset, obtained from publicly available sources. It achieves remarkable performance in diabetic retinopathy detection and stage classification on the APTOS 2019, outperforming the established literature. For binary classification, the proposed approach achieves an accuracy of 98.50%, a sensitivity of 99.46%, and a specificity of 97.51%. In stage grading, it achieves a quadratic weighted kappa of 93.00%, an accuracy of 89.60%, a sensitivity of 89.60%, and a specificity of 97.72%. The proposed approach serves as a reliable screening and stage grading tool for diabetic retinopathy, offering significant potential to enhance clinical decision-making and patient care.
</details>
<details>
<summary>摘要</summary>
糖尿病肠病是糖尿病的严重并发症，如果不及时治疗，可能会导致永久潦积。早期和准确的诊断是成功治疗的关键。本文提出了一种深度学习方法，用于检测和评分糖尿病肠病，只需一张背部照片。我们的模型使用了传输学习，使用两个国际先进的预训练模型，并对其进行精细调整。我们的模型在APTOS 2019数据集上训练，并在这个数据集上实现了糖尿病肠病检测和评分的优异表现，比Literature中的已知方法更出色。为二分类问题，我们的方法实现了98.50%的准确率，99.46%的感知率和97.51%的特异性。在评分问题上，我们的方法实现了93.00%的卷积权重κ值，89.60%的准确率，89.60%的感知率和97.72%的特异性。我们的方法可以作为糖尿病肠病检测和评分工具，为临床决策和患者护理带来了重要的可能性。
</details></li>
</ul>
<hr>
<h2 id="On-the-Robustness-of-Open-World-Test-Time-Training-Self-Training-with-Dynamic-Prototype-Expansion"><a href="#On-the-Robustness-of-Open-World-Test-Time-Training-Self-Training-with-Dynamic-Prototype-Expansion" class="headerlink" title="On the Robustness of Open-World Test-Time Training: Self-Training with Dynamic Prototype Expansion"></a>On the Robustness of Open-World Test-Time Training: Self-Training with Dynamic Prototype Expansion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09942">http://arxiv.org/abs/2308.09942</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yushu-li/owttt">https://github.com/yushu-li/owttt</a></li>
<li>paper_authors: Yushu Li, Xun Xu, Yongyi Su, Kui Jia</li>
<li>for: 该论文旨在提高 unknown 目标频谱分布下的深度学习模型的泛化性，并且具有低延迟。</li>
<li>methods: 该论文提出了一种基于 test-time training&#x2F;adaptation (TTT&#x2F;TTA) 的方法，并且对存在强度外部数据的情况进行了研究。</li>
<li>results: 该论文在 5 个 open-world test-time training (OWTTT)  benchmark 上达到了 state-of-the-art 性能，并且提出了一种 adaptive strong OOD pruning 和动态扩展 prototype 的方法来提高模型的 robustness。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Generalizing deep learning models to unknown target domain distribution with low latency has motivated research into test-time training/adaptation (TTT/TTA). Existing approaches often focus on improving test-time training performance under well-curated target domain data. As figured out in this work, many state-of-the-art methods fail to maintain the performance when the target domain is contaminated with strong out-of-distribution (OOD) data, a.k.a. open-world test-time training (OWTTT). The failure is mainly due to the inability to distinguish strong OOD samples from regular weak OOD samples. To improve the robustness of OWTTT we first develop an adaptive strong OOD pruning which improves the efficacy of the self-training TTT method. We further propose a way to dynamically expand the prototypes to represent strong OOD samples for an improved weak/strong OOD data separation. Finally, we regularize self-training with distribution alignment and the combination yields the state-of-the-art performance on 5 OWTTT benchmarks. The code is available at https://github.com/Yushu-Li/OWTTT.
</details>
<details>
<summary>摘要</summary>
通过快速适应Unknown目标分布，深度学习模型的普及化已成为研究焦点。现有方法通常是在Well-curated目标分布数据下提高测试时训练性能。然而，这些state-of-the-art方法在Open-world测试时训练（OWTTT）中表现不佳，主要是因为无法分辨强OOD样本（out-of-distribution）和弱OOD样本（weak out-of-distribution）之间的差异。为了改进OWTTT的Robustness，我们首先开发了自适应强OOD��ppring，以提高自学习TTT方法的效果。然后，我们提议在运行时动态扩展表例，以更好地分离弱OOD样本和强OOD样本。最后，我们添加了分布对齐的REG regularization，这种组合得到了5个OWTTT标准测试 benchmarks的state-of-the-art性能。代码可以在https://github.com/Yushu-Li/OWTTT中找到。
</details></li>
</ul>
<hr>
<h2 id="Practical-Anomaly-Detection-over-Multivariate-Monitoring-Metrics-for-Online-Services"><a href="#Practical-Anomaly-Detection-over-Multivariate-Monitoring-Metrics-for-Online-Services" class="headerlink" title="Practical Anomaly Detection over Multivariate Monitoring Metrics for Online Services"></a>Practical Anomaly Detection over Multivariate Monitoring Metrics for Online Services</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09937">http://arxiv.org/abs/2308.09937</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/OpsPAI/CMAnomaly">https://github.com/OpsPAI/CMAnomaly</a></li>
<li>paper_authors: Jinyang Liu, Tianyi Yang, Zhuangbin Chen, Yuxin Su, Cong Feng, Zengyin Yang, Michael R. Lyu</li>
<li>for: 这个研究的目的是为了提出一个基于协同机器学习的异常探测框架，以便更好地探测现代软件系统中的异常情况。</li>
<li>methods: 这个研究使用了一种叫做协同机器学习的方法，以捕捉多元监控指标之间的相互依存关系，并且可以在线时间复杂度下进行效率地探测。</li>
<li>results: 实验结果显示，与现有基eline模型相比，CMAnomaly可以提高异常探测的精度，并且可以在10X到20X的速度上进行探测。此外，这个框架也在Huawei Cloud中进行了部署。<details>
<summary>Abstract</summary>
As modern software systems continue to grow in terms of complexity and volume, anomaly detection on multivariate monitoring metrics, which profile systems' health status, becomes more and more critical and challenging. In particular, the dependency between different metrics and their historical patterns plays a critical role in pursuing prompt and accurate anomaly detection. Existing approaches fall short of industrial needs for being unable to capture such information efficiently. To fill this significant gap, in this paper, we propose CMAnomaly, an anomaly detection framework on multivariate monitoring metrics based on collaborative machine. The proposed collaborative machine is a mechanism to capture the pairwise interactions along with feature and temporal dimensions with linear time complexity. Cost-effective models can then be employed to leverage both the dependency between monitoring metrics and their historical patterns for anomaly detection. The proposed framework is extensively evaluated with both public data and industrial data collected from a large-scale online service system of Huawei Cloud. The experimental results demonstrate that compared with state-of-the-art baseline models, CMAnomaly achieves an average F1 score of 0.9494, outperforming baselines by 6.77% to 10.68%, and runs 10X to 20X faster. Furthermore, we also share our experience of deploying CMAnomaly in Huawei Cloud.
</details>
<details>
<summary>摘要</summary>
现代软件系统在复杂性和规模上不断增长，异常检测在多变量监控指标上成为更加重要和挑战性的。特别是在不同指标之间的依赖关系以及历史 patrern 的情况下，异常检测变得更加重要。现有的方法无法有效地捕捉这些信息，因此在这篇论文中，我们提出了 CMAnomaly 异常检测框架，基于协同机器学习。我们的提议的协同机器是一种可以有效地捕捉多变量监控指标之间的对价关系，以及特征和时间维度的机制，具有线性时间复杂度。这使得可以使用便宜的模型来利用异常检测。我们的框架在大规模在线服务系统中进行了广泛的评估，结果显示，相比状态之前的基准模型，CMAnomaly 的平均 F1 分数为 0.9494，高于基准模型的 6.77% 到 10.68%，并且运行速度比基准模型快 10 倍到 20 倍。此外，我们还分享了在华为云上部署 CMAnomaly 的经验。
</details></li>
</ul>
<hr>
<h2 id="BLIVA-A-Simple-Multimodal-LLM-for-Better-Handling-of-Text-Rich-Visual-Questions"><a href="#BLIVA-A-Simple-Multimodal-LLM-for-Better-Handling-of-Text-Rich-Visual-Questions" class="headerlink" title="BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions"></a>BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09936">http://arxiv.org/abs/2308.09936</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mlpc-ucsd/bliva">https://github.com/mlpc-ucsd/bliva</a></li>
<li>paper_authors: Wenbo Hu, Yifan Xu, Yi Li, Weiyue Li, Zeyuan Chen, Zhuowen Tu</li>
<li>For: The paper aims to improve the ability of Vision Language Models (VLMs) to interpret images with text-rich context, which is a common occurrence in real-world scenarios.* Methods: The proposed method, called BLIVA, incorporates query embeddings from InstructBLIP and directly projects encoded patch embeddings into the LLM, a technique inspired by LLaVA. This approach helps the model capture intricate details potentially missed during the query decoding process.* Results: The proposed BLIVA model significantly enhances performance in processing text-rich VQA benchmarks (up to 17.76% in OCR-VQA benchmark) and typical VQA benchmarks (up to 7.9% in Visual Spatial Reasoning benchmark), compared to the baseline InstructBLIP. Additionally, BLIVA demonstrates significant capability in decoding real-world images, regardless of text presence.<details>
<summary>Abstract</summary>
Vision Language Models (VLMs), which extend Large Language Models (LLM) by incorporating visual understanding capability, have demonstrated significant advancements in addressing open-ended visual question-answering (VQA) tasks. However, these models cannot accurately interpret images infused with text, a common occurrence in real-world scenarios. Standard procedures for extracting information from images often involve learning a fixed set of query embeddings. These embeddings are designed to encapsulate image contexts and are later used as soft prompt inputs in LLMs. Yet, this process is limited to the token count, potentially curtailing the recognition of scenes with text-rich context. To improve upon them, the present study introduces BLIVA: an augmented version of InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings from InstructBLIP and also directly projects encoded patch embeddings into the LLM, a technique inspired by LLaVA. This approach assists the model to capture intricate details potentially missed during the query decoding process. Empirical evidence demonstrates that our model, BLIVA, significantly enhances performance in processing text-rich VQA benchmarks (up to 17.76\% in OCR-VQA benchmark) and in undertaking typical VQA benchmarks (up to 7.9\% in Visual Spatial Reasoning benchmark), comparing to our baseline InstructBLIP. BLIVA demonstrates significant capability in decoding real-world images, irrespective of text presence. To demonstrate the broad industry applications enabled by BLIVA, we evaluate the model using a new dataset comprising YouTube thumbnails paired with question-answer sets across 13 diverse categories. For researchers interested in further exploration, our code and models are freely accessible at https://github.com/mlpc-ucsd/BLIVA.git
</details>
<details>
<summary>摘要</summary>
《视觉语言模型（VLM）》，它们将大型语言模型（LLM）扩展以包含视觉理解能力，在开放式视觉问答任务（VQA）中显示出了重要的进步。然而，这些模型无法正确地解释含有文本的图像，这是现实世界中常见的情况。标准的图像信息提取方法通常包括学习固定的查询嵌入。这些嵌入用于在LLM中作为软提问输入，然而这种过程受到固定的token计数的限制，可能会遗弃场景中的文本背景。为此，本研究提出了BLIVA：一个基于InstructBLIP的增强版，它在LLM中直接将编码的质心嵌入投影到LLaVA的技术。这种方法帮助模型捕捉文本背景中的细节，可能在查询解码过程中被遗弃。empirical evidence表明，我们的模型BLIVA在处理含有文本的VQAbenchmark上（最高提升17.76%）和 Typical VQA benchmark上（最高提升7.9%）表现出色，相比基eline InstructBLIP。BLIVA在实际图像中解码表现出色，不管文本存在或不存在。为了展示BLIVA在广泛的产业应用中的应用前景，我们使用YouTube预览图片和相应的问答集来评估模型。对研究人员来说，我们在GitHub上提供了代码和模型，可以免费下载：https://github.com/mlpc-ucsd/BLIVA.git。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Quantization-in-TVM"><a href="#Analyzing-Quantization-in-TVM" class="headerlink" title="Analyzing Quantization in TVM"></a>Analyzing Quantization in TVM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10905">http://arxiv.org/abs/2308.10905</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingfei Guo</li>
<li>For: The paper aims to investigate the underperformance of 8-bit quantization in Tensor Virtual Machine (TVM) and to optimize the quantization process for deep learning models.* Methods: The paper uses TVM to perform 8-bit quantization on deep learning models and compares the performance with the non-quantized version. The authors also discuss various optimization techniques in TVM, such as graph building and memory access optimization, to improve the performance of quantized models.* Results: The paper achieves a 163.88% improvement in inference time for compute-bound tasks and a 194.98% improvement for memory-bound tasks compared to the TVM compiled baseline after addressing a bug in graph building and applying optimization strategies.<details>
<summary>Abstract</summary>
There has been many papers in academic literature on quantizing weight tensors in deep learning models to reduce inference latency and memory footprint. TVM also has the ability to quantize weights and support low-bit computations. Although quantization is typically expected to improve inference time, in TVM, the performance of 8-bit quantization does not meet the expectations. Typically, when applying 8-bit quantization to a deep learning model, it is usually expected to achieve around 50% of the full-precision inference time. However, in this particular case, not only does the quantized version fail to achieve the desired performance boost, but it actually performs worse, resulting in an inference time that is about 2 times as slow as the non-quantized version. In this project, we thoroughly investigate the reasons behind the underperformance and assess the compatibility and optimization opportunities of 8-bit quantization in TVM. We discuss the optimization of two different types of tasks: computation-bound and memory-bound, and provide a detailed comparison of various optimization techniques in TVM. Through the identification of performance issues, we have successfully improved quantization by addressing a bug in graph building. Furthermore, we analyze multiple optimization strategies to achieve the optimal quantization result. The best experiment achieves 163.88% improvement compared with the TVM compiled baseline in inference time for the compute-bound task and 194.98% for the memory-bound task.
</details>
<details>
<summary>摘要</summary>
有很多学术论文提出了深度学习模型中量化权重矩阵以降低推理时间和内存占用的想法。TVM也具有量化权重和低位计算的能力。although quantization 通常预计会改善推理时间，在 TVM 中，8 位量化的表现不符合预期。通常在应用 8 位量化深度学习模型时，预计可以达到约 50% 的全精度推理时间。但在这个特定情况下，量化版本并不只是不符合预期，而且实际更慢，导致推理时间约 double 非量化版本。在这个项目中，我们进行了深入的调查和分析，探讨 TVM 中 8 位量化的Compatibility和优化机会。我们分析了两种不同的任务类型：计算约束和内存约束，并对 TVM 中不同优化技术进行了详细的比较。通过发现性能问题，我们成功地修复了图像建立的漏洞，并分析了多种优化策略以实现最佳量化结果。最佳实验结果显示，与 TVM 编译基线相比，compute-bound 任务的推理时间提高了 163.88%，而 memory-bound 任务的推理时间提高了 194.98%。
</details></li>
</ul>
<hr>
<h2 id="East-Efficient-and-Accurate-Secure-Transformer-Framework-for-Inference"><a href="#East-Efficient-and-Accurate-Secure-Transformer-Framework-for-Inference" class="headerlink" title="East: Efficient and Accurate Secure Transformer Framework for Inference"></a>East: Efficient and Accurate Secure Transformer Framework for Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09923">http://arxiv.org/abs/2308.09923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanchao Ding, Hua Guo, Yewei Guan, Weixin Liu, Jiarong Huo, Zhenyu Guan, Xiyong Zhang</li>
<li>for: 该论文旨在提供一个可靠和准确的安全Transformer推理框架，以保护用户的隐私。</li>
<li>methods: 该论文提出了一种新的幂等分割多项式评估算法，用于活动函数的评估，从而降低了GELU的运行时间和通信量。此外，该论文还为软max和层normalization的安全协议进行了仔细的设计，以保持所需的功能。</li>
<li>results: 该论文应用于BERT，并证明了在不需要精心调整的情况下，推理精度与明文推理相同。相比 iron，我们的方法具有1.8倍lower的通信量和1.2倍lower的运行时间。<details>
<summary>Abstract</summary>
Transformer has been successfully used in practical applications, such as ChatGPT, due to its powerful advantages. However, users' input is leaked to the model provider during the service. With people's attention to privacy, privacy-preserving Transformer inference is on the demand of such services. Secure protocols for non-linear functions are crucial in privacy-preserving Transformer inference, which are not well studied. Thus, designing practical secure protocols for non-linear functions is hard but significant to model performance. In this work, we propose a framework \emph{East} to enable efficient and accurate secure Transformer inference. Firstly, we propose a new oblivious piecewise polynomial evaluation algorithm and apply it to the activation functions, which reduces the runtime and communication of GELU by over 1.5$\times$ and 2.5$\times$, compared to prior arts. Secondly, the secure protocols for softmax and layer normalization are carefully designed to faithfully maintain the desired functionality. Thirdly, several optimizations are conducted in detail to enhance the overall efficiency. We applied \emph{East} to BERT and the results show that the inference accuracy remains consistent with the plaintext inference without fine-tuning. Compared to Iron, we achieve about 1.8$\times$ lower communication within 1.2$\times$ lower runtime.
</details>
<details>
<summary>摘要</summary>
“transformer”已经成功应用在实际应用中，例如ChatGPT，因为它具有强大的优势。然而，用户的输入会被提供者 During the service 泄露。随着人们对隐私的关注，隐私保护的transformer推理是实际的服务中的请求。对非线性函数的安全协议是这些服务中的重要课题，但是尚未得到充分的研究。因此，设计实用的安全协议 для非线性函数是具有挑战性和重要性的。在这个工作中，我们提出了一个名为“East”的框架，以实现有效和准确的隐私保护transformer推理。首先，我们提出了一个新的隐私 polynomial evaluation algorithm，并将其应用到活动函数中，这减少了GELU的runtime和通信量，相比于先前的艺术，则是1.5倍以上和2.5倍以上。其次，我们对于softmax和层Normalization进行了详细的设计，以确保所需的功能faithfully maintained。最后，我们在细节上进行了多个优化，以提高整体的效率。我们将“East”应用到BERT，结果显示，在不需要精确调整的情况下，推理精度与纯文本推理相同。相比于Iron，我们在1.2倍的runtime和1.8倍的通信量下可以 дости到相同的推理精度。”
</details></li>
</ul>
<hr>
<h2 id="EGANS-Evolutionary-Generative-Adversarial-Network-Search-for-Zero-Shot-Learning"><a href="#EGANS-Evolutionary-Generative-Adversarial-Network-Search-for-Zero-Shot-Learning" class="headerlink" title="EGANS: Evolutionary Generative Adversarial Network Search for Zero-Shot Learning"></a>EGANS: Evolutionary Generative Adversarial Network Search for Zero-Shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09915">http://arxiv.org/abs/2308.09915</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiming Chen, Shihuang Chen, Wenjin Hou, Weiping Ding, Xinge You</li>
<li>for: 这篇论文目的是提出了一种基于进化的对抗性探索方法（EGANS），用于实现零目标学习（ZSL）中的视觉标本生成。</li>
<li>methods: 这篇论文使用了对抗性探索（EGANS）来自动设计视觉生成网络，以提高ZSL中的视觉标本生成精度。</li>
<li>results: 实验结果显示，EGANS可以成功地对抗现有的对抗性探索方法，并在标准的CUB、SUN、AWA2和FLO datasets上实现了视觉标本生成中的改进。<details>
<summary>Abstract</summary>
Zero-shot learning (ZSL) aims to recognize the novel classes which cannot be collected for training a prediction model. Accordingly, generative models (e.g., generative adversarial network (GAN)) are typically used to synthesize the visual samples conditioned by the class semantic vectors and achieve remarkable progress for ZSL. However, existing GAN-based generative ZSL methods are based on hand-crafted models, which cannot adapt to various datasets/scenarios and fails to model instability. To alleviate these challenges, we propose evolutionary generative adversarial network search (termed EGANS) to automatically design the generative network with good adaptation and stability, enabling reliable visual feature sample synthesis for advancing ZSL. Specifically, we adopt cooperative dual evolution to conduct a neural architecture search for both generator and discriminator under a unified evolutionary adversarial framework. EGANS is learned by two stages: evolution generator architecture search and evolution discriminator architecture search. During the evolution generator architecture search, we adopt a many-to-one adversarial training strategy to evolutionarily search for the optimal generator. Then the optimal generator is further applied to search for the optimal discriminator in the evolution discriminator architecture search with a similar evolution search algorithm. Once the optimal generator and discriminator are searched, we entail them into various generative ZSL baselines for ZSL classification. Extensive experiments show that EGANS consistently improve existing generative ZSL methods on the standard CUB, SUN, AWA2 and FLO datasets. The significant performance gains indicate that the evolutionary neural architecture search explores a virgin field in ZSL.
</details>
<details>
<summary>摘要</summary>
zero-shot learning (ZSL) targets recognizing novel classes that cannot be collected for training a prediction model. Therefore, generative models (e.g., generative adversarial network (GAN)) are typically used to synthesize visual samples conditioned by the class semantic vectors and achieve remarkable progress for ZSL. However, existing GAN-based generative ZSL methods are based on hand-crafted models, which cannot adapt to various datasets/scenarios and fail to model instability. To address these challenges, we propose evolutionary generative adversarial network search (termed EGANS) to automatically design the generative network with good adaptation and stability, enabling reliable visual feature sample synthesis for advancing ZSL. Specifically, we adopt cooperative dual evolution to conduct a neural architecture search for both generator and discriminator under a unified evolutionary adversarial framework. EGANS is learned by two stages: evolution generator architecture search and evolution discriminator architecture search. During the evolution generator architecture search, we adopt a many-to-one adversarial training strategy to evolutionarily search for the optimal generator. Then the optimal generator is further applied to search for the optimal discriminator in the evolution discriminator architecture search with a similar evolution search algorithm. Once the optimal generator and discriminator are searched, we entail them into various generative ZSL baselines for ZSL classification. Extensive experiments show that EGANS consistently improve existing generative ZSL methods on the standard CUB, SUN, AWA2, and FLO datasets. The significant performance gains indicate that the evolutionary neural architecture search explores a virgin field in ZSL.
</details></li>
</ul>
<hr>
<h2 id="Never-Explore-Repeatedly-in-Multi-Agent-Reinforcement-Learning"><a href="#Never-Explore-Repeatedly-in-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Never Explore Repeatedly in Multi-Agent Reinforcement Learning"></a>Never Explore Repeatedly in Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09909">http://arxiv.org/abs/2308.09909</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenghao Li, Tonghan Wang, Chongjie Zhang, Qianchuan Zhao</li>
<li>for: 增强多智能体强化学习中的探索性能</li>
<li>methods: 提出了动态奖励缩放方法，用于稳定前期探索区域的奖励变化，促进更广泛的探索</li>
<li>results: 实验结果表明，该方法能够在Google研究足球和StarCraft II微管理任务中提高性能，特别在罕见奖励设定下<details>
<summary>Abstract</summary>
In the realm of multi-agent reinforcement learning, intrinsic motivations have emerged as a pivotal tool for exploration. While the computation of many intrinsic rewards relies on estimating variational posteriors using neural network approximators, a notable challenge has surfaced due to the limited expressive capability of these neural statistics approximators. We pinpoint this challenge as the "revisitation" issue, where agents recurrently explore confined areas of the task space. To combat this, we propose a dynamic reward scaling approach. This method is crafted to stabilize the significant fluctuations in intrinsic rewards in previously explored areas and promote broader exploration, effectively curbing the revisitation phenomenon. Our experimental findings underscore the efficacy of our approach, showcasing enhanced performance in demanding environments like Google Research Football and StarCraft II micromanagement tasks, especially in sparse reward settings.
</details>
<details>
<summary>摘要</summary>
在多代理激励学习领域，内在动机已经出现为探索的重要工具。而计算许多内在奖励的计算则依赖于使用神经网络近似器来估算变量 posterior。然而，由于神经统计近似器的表达能力有限，这导致了一种“再次访问”问题，Agent会重复探索任务空间中的封闭区域。为解决这个问题，我们提议一种动态奖励缩放方法。这种方法可以稳定在已经探索过的区域中的内在奖励的大幅波动，并促进更广泛的探索，从而控制“再次访问”现象。我们的实验发现，我们的方法在Google研究足球和星际争霸II微管理任务中表现出色，特别在罕见奖励设定下。
</details></li>
</ul>
<hr>
<h2 id="Imputing-Brain-Measurements-Across-Data-Sets-via-Graph-Neural-Networks"><a href="#Imputing-Brain-Measurements-Across-Data-Sets-via-Graph-Neural-Networks" class="headerlink" title="Imputing Brain Measurements Across Data Sets via Graph Neural Networks"></a>Imputing Brain Measurements Across Data Sets via Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09907">http://arxiv.org/abs/2308.09907</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yixin Wang, Wei Peng, Susan F. Tapert, Qingyu Zhao, Kilian M. Pohl</li>
<li>for: 这个论文的目的是为了填充公共可用的数据集中缺失的 brain Region of Interest (ROI) 测量值。</li>
<li>methods: 这个论文使用了深度学习的方法来预测缺失的测量值，包括使用图 neural network (GNN) 模型来模拟 ROI 测量值之间的依赖关系，并考虑到不同性别的脑测量值差异。</li>
<li>results: 该论文的结果表明，使用 DAGI 算法可以准确地预测缺失的 Freesurfer 测量值，并且可以考虑到不同性别的脑测量值差异。<details>
<summary>Abstract</summary>
Publicly available data sets of structural MRIs might not contain specific measurements of brain Regions of Interests (ROIs) that are important for training machine learning models. For example, the curvature scores computed by Freesurfer are not released by the Adolescent Brain Cognitive Development (ABCD) Study. One can address this issue by simply reapplying Freesurfer to the data set. However, this approach is generally computationally and labor intensive (e.g., requiring quality control). An alternative is to impute the missing measurements via a deep learning approach. However, the state-of-the-art is designed to estimate randomly missing values rather than entire measurements. We therefore propose to re-frame the imputation problem as a prediction task on another (public) data set that contains the missing measurements and shares some ROI measurements with the data sets of interest. A deep learning model is then trained to predict the missing measurements from the shared ones and afterwards is applied to the other data sets. Our proposed algorithm models the dependencies between ROI measurements via a graph neural network (GNN) and accounts for demographic differences in brain measurements (e.g. sex) by feeding the graph encoding into a parallel architecture. The architecture simultaneously optimizes a graph decoder to impute values and a classifier in predicting demographic factors. We test the approach, called Demographic Aware Graph-based Imputation (DAGI), on imputing those missing Freesurfer measurements of ABCD (N=3760) by training the predictor on those publicly released by the National Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA, N=540)...
</details>
<details>
<summary>摘要</summary>
公共可用数据集的结构MRI数据可能不包含特定的脑区域关注点（ROIs）的准确测量。例如，ABCDFreesurfer的曲线分数不会由ABCDFreesurfer发布。可以解决这个问题通过简单地重新应用Freessurfer来处理数据集。然而，这种方法通常是计算机和人工劳动（例如质量控制）的。另一种方法是使用深度学习方法进行填充。然而，现状的深度学习方法是随机缺失值的估计而不是整个测量。我们因此提议将填充问题重新定义为一个预测任务，使用另一个（公共）数据集来计算缺失测量，该数据集与数据集之间存在ROI测量的相似性。然后，我们使用深度学习模型来预测缺失测量，并将其应用于其他数据集。我们提出的算法模拟了ROI测量之间的依赖关系，使用图神经网络（GNN）来模型这些关系，同时考虑了脑测量中的人口差异（如性别），通过将图编码 feed 到平行架构中来进行考虑。该架构同时优化了图解码器来填充值，以及一个分类器来预测人口因素。我们测试了我们的方法，称为人口意识图像基于填充（DAGI），在ABCDFreesurfer中缺失的测量（N=3760）中进行填充，通过在NCANDA（N=540）中公共发布的数据集进行训练。
</details></li>
</ul>
<hr>
<h2 id="DPMAC-Differentially-Private-Communication-for-Cooperative-Multi-Agent-Reinforcement-Learning"><a href="#DPMAC-Differentially-Private-Communication-for-Cooperative-Multi-Agent-Reinforcement-Learning" class="headerlink" title="DPMAC: Differentially Private Communication for Cooperative Multi-Agent Reinforcement Learning"></a>DPMAC: Differentially Private Communication for Cooperative Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09902">http://arxiv.org/abs/2308.09902</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/CANVOLCANO/DPMAC">https://github.com/CANVOLCANO/DPMAC</a></li>
<li>paper_authors: Canzhe Zhao, Yanjie Ze, Jing Dong, Baoxiang Wang, Shuai Li</li>
<li>for: 保护多体智能学习（MARL）中每个代理的敏感信息，以确保人工智能（AI）系统的隐私。</li>
<li>methods: 提议了一种基于（ε，δ）敏感数据隐私（DP）的差分性多体通信算法（DPMAC），每个代理都有一个本地消息发送器，并自动调整学习的消息分布，以缓解DP噪声所引起的不稳定性。</li>
<li>results: 证明了在保护隐私的情况下，协作MARL存在纳什平衡，这表示这个问题是游戏理论上可学习的。实验证明DPMAC在隐私保护场景下表现明显优于基eline方法。<details>
<summary>Abstract</summary>
Communication lays the foundation for cooperation in human society and in multi-agent reinforcement learning (MARL). Humans also desire to maintain their privacy when communicating with others, yet such privacy concern has not been considered in existing works in MARL. To this end, we propose the \textit{differentially private multi-agent communication} (DPMAC) algorithm, which protects the sensitive information of individual agents by equipping each agent with a local message sender with rigorous $(\epsilon, \delta)$-differential privacy (DP) guarantee. In contrast to directly perturbing the messages with predefined DP noise as commonly done in privacy-preserving scenarios, we adopt a stochastic message sender for each agent respectively and incorporate the DP requirement into the sender, which automatically adjusts the learned message distribution to alleviate the instability caused by DP noise. Further, we prove the existence of a Nash equilibrium in cooperative MARL with privacy-preserving communication, which suggests that this problem is game-theoretically learnable. Extensive experiments demonstrate a clear advantage of DPMAC over baseline methods in privacy-preserving scenarios.
</details>
<details>
<summary>摘要</summary>
通信layfoundationforthecooperationinhuman社会和多代理权威学习（MARL）。人们也渴望保持与他人通信时的隐私，但这一问题在现有的MARL工作中未得到考虑。为此，我们提出了《差分隐私多代理通信算法》（DPMAC），该算法保护每个代理的敏感信息，并在每个代理机器人上实现了严格（ε、δ）差分隐私（DP）保证。与直接在消息上添加固定DP噪声的常见方法不同，我们采用了每个代理机器人的本地消息发送器，并将DP要求直接 интеグinto sender，这 автоматичеamente调整了学习的消息分布，以解决由DP噪声所引起的不稳定性。此外，我们证明了在保持隐私的情况下，多代理MARL проблеma是可学习的游戏理论问题。广泛的实验表明，DPMAC在隐私保护场景下具有明显的优势。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Learning-based-Imputation-Prediction-Networks-for-In-hospital-Mortality-Risk-Modeling-using-EHRs"><a href="#Contrastive-Learning-based-Imputation-Prediction-Networks-for-In-hospital-Mortality-Risk-Modeling-using-EHRs" class="headerlink" title="Contrastive Learning-based Imputation-Prediction Networks for In-hospital Mortality Risk Modeling using EHRs"></a>Contrastive Learning-based Imputation-Prediction Networks for In-hospital Mortality Risk Modeling using EHRs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09896">http://arxiv.org/abs/2308.09896</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liulab1356/CL-ImpPreNet">https://github.com/liulab1356/CL-ImpPreNet</a></li>
<li>paper_authors: Yuxi Liu, Zhenhao Zhang, Shaowen Qin, Flora D. Salim, Antonio Jimeno Yepes</li>
<li>for: 预测医院内死亡风险基于电子医疗记录 (EHRs) 已经受到了广泛关注，以提供早期警示患者的健康状况，以便医疗专业人员能够在时间上采取措施。</li>
<li>methods: 我们的方法包括使用图分析模型来划分病人，以便只使用相似病人的信息进行缺失值填充。此外，我们还将对比学习 integrate 到我们的网络架构中，以提高病人表示学习和预测性能。</li>
<li>results: 我们的方法在两个真实的 EHR 数据集上进行实验，与当前状态的方法相比，在缺失值填充和预测任务中均有较高的性能。<details>
<summary>Abstract</summary>
Predicting the risk of in-hospital mortality from electronic health records (EHRs) has received considerable attention. Such predictions will provide early warning of a patient's health condition to healthcare professionals so that timely interventions can be taken. This prediction task is challenging since EHR data are intrinsically irregular, with not only many missing values but also varying time intervals between medical records. Existing approaches focus on exploiting the variable correlations in patient medical records to impute missing values and establishing time-decay mechanisms to deal with such irregularity. This paper presents a novel contrastive learning-based imputation-prediction network for predicting in-hospital mortality risks using EHR data. Our approach introduces graph analysis-based patient stratification modeling in the imputation process to group similar patients. This allows information of similar patients only to be used, in addition to personal contextual information, for missing value imputation. Moreover, our approach can integrate contrastive learning into the proposed network architecture to enhance patient representation learning and predictive performance on the classification task. Experiments on two real-world EHR datasets show that our approach outperforms the state-of-the-art approaches in both imputation and prediction tasks.
</details>
<details>
<summary>摘要</summary>
预测医院内死亡风险从电子医疗记录（EHR）获得了广泛关注。这种预测可以提供早期诊断病人健康状况的警示，以便医疗专业人员在时间上采取措施。这个预测任务是挑战性的，因为EHR数据本身是不规则的，有许多缺失的值和不同的时间间隔between medical records。现有的方法是利用病人医疗记录中的变量相关性来填充缺失值，并设置时间衰退机制来处理这种不规则性。本文提出了一种新的对比学习基于抽象的插值预测网络，用于预测医院内死亡风险。我们的方法包括基于图分析的病人划分模型，以组合相似病人的信息。这使得只有相似病人的信息，以及个人上下文信息，用于缺失值填充。此外，我们的方法还可以将对比学习integrated到提议的网络架构中，以提高病人表征学习和预测性能。实验结果表明，我们的方法在两个实际的EHR数据集上比状态机制方法更高。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Transfer-from-High-Resource-to-Low-Resource-Programming-Languages-for-Code-LLMs"><a href="#Knowledge-Transfer-from-High-Resource-to-Low-Resource-Programming-Languages-for-Code-LLMs" class="headerlink" title="Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs"></a>Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09895">http://arxiv.org/abs/2308.09895</a></li>
<li>repo_url: None</li>
<li>paper_authors: Federico Cassano, John Gouwar, Francesca Lucchetti, Claire Schlesinger, Carolyn Jane Anderson, Michael Greenberg, Abhinav Jangda, Arjun Guha</li>
<li>for: 本文旨在提高Code LLMP的表现在低资源语言上，使其能够更好地支持低资源语言的编程。</li>
<li>methods: 本文提出了一种效果的方法，即使用半人工生成的数据来提高Code LLMP的表现。这种方法可以将高资源语言的训练数据翻译成低资源语言的训练数据，以便使用任何预训练的Code LLMP进行精度。</li>
<li>results: 本文使用MultiPL-T生成了大量的新训练数据，并对这些数据进行了验证。 results表明，通过使用MultiPL-T生成的数据，可以在Racket、OCaml和Lua等低资源语言上达到类似于高资源语言的性能。<details>
<summary>Abstract</summary>
Over the past few years, Large Language Models of Code (Code LLMs) have started to have a significant impact on programming practice. Code LLMs are also emerging as a building block for research in programming languages and software engineering. However, the quality of code produced by a Code LLM varies significantly by programming languages. Code LLMs produce impressive results on programming languages that are well represented in their training data (e.g., Java, Python, or JavaScript), but struggle with low-resource languages, like OCaml and Racket.   This paper presents an effective approach for boosting the performance of Code LLMs on low-resource languages using semi-synthetic data. Our approach generates high-quality datasets for low-resource languages, which can then be used to fine-tune any pretrained Code LLM. Our approach, called MultiPL-T, translates training data from high-resource languages into training data for low-resource languages. We apply our approach to generate tens of thousands of new, validated training items for Racket, OCaml, and Lua from Python. Moreover, we use an open dataset (The Stack) and model (StarCoderBase), which allow us to decontaminate benchmarks and train models on this data without violating the model license.   With MultiPL-T generated data, we present fine-tuned versions of StarCoderBase that achieve state-of-the-art performance for Racket, OCaml, and Lua on benchmark problems. For Lua, our fine-tuned model achieves the same performance as StarCoderBase as Python -- a very high-resource language -- on the MultiPL-E benchmarks. For Racket and OCaml, we double their performance on MultiPL-E, bringing their performance close to higher-resource languages such as Ruby and C#.
</details>
<details>
<summary>摘要</summary>
在过去几年，大型代码语言模型（Code LLM）已经开始对程序设计产生重要的影响。 Code LLM 也在程序语言和软件工程研究中出现为建筑块。然而，由 Code LLM 生成的代码质量受到程序语言的影响，高Resource语言（如 Java、Python 或 JavaScript）的代码生成印象良好，而低Resource语言（如 OCaml 和 Racket）的代码生成却受到限制。本文提出一种有效的方法，使 Code LLM 在低Resource语言上表现更好。我们的方法通过生成高质量的低Resource语言数据集来提高 Code LLM 的表现。我们的方法被称为 MultiPL-T，它将高Resource语言的训练数据翻译成低Resource语言的训练数据。我们使用 Python 等高Resource语言生成了数以千计的新的有效训练项目，并使用开放数据集（The Stack）和模型（StarCoderBase），以便在这些数据上训练模型，而不违反模型的许可证。使用 MultiPL-T 生成的数据，我们提出了一些精心调整的 StarCoderBase 模型，以实现 Racket、OCaml 和 Lua 在 benchmark 问题上的状态的表现。对 Lua，我们的调整模型与 Python 在 MultiPL-E  benchmark 上达到了同等水平的表现。对 Racket 和 OCaml，我们将其表现提高至 Ruby 和 C# 等高Resource语言水平的两倍。
</details></li>
</ul>
<hr>
<h2 id="Utilizing-Semantic-Textual-Similarity-for-Clinical-Survey-Data-Feature-Selection"><a href="#Utilizing-Semantic-Textual-Similarity-for-Clinical-Survey-Data-Feature-Selection" class="headerlink" title="Utilizing Semantic Textual Similarity for Clinical Survey Data Feature Selection"></a>Utilizing Semantic Textual Similarity for Clinical Survey Data Feature Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09892">http://arxiv.org/abs/2308.09892</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bcwarner/sts-select">https://github.com/bcwarner/sts-select</a></li>
<li>paper_authors: Benjamin C. Warner, Ziqi Xu, Simon Haroutounian, Thomas Kannampallil, Chenyang Lu</li>
<li>for: 这篇论文的目的是提出一种基于文本名称的Feature选择方法，以提高预测结果的普遍性。</li>
<li>methods: 这篇论文使用语言模型（LM）评估文本名称之间的 semantic textual similarity（STS）分数，以选择最佳的特征集。</li>
<li>results: 研究发现，使用 STS 选择特征可以导致更高的模型性能，比较传统的特征选择算法。<details>
<summary>Abstract</summary>
Survey data can contain a high number of features while having a comparatively low quantity of examples. Machine learning models that attempt to predict outcomes from survey data under these conditions can overfit and result in poor generalizability. One remedy to this issue is feature selection, which attempts to select an optimal subset of features to learn upon. A relatively unexplored source of information in the feature selection process is the usage of textual names of features, which may be semantically indicative of which features are relevant to a target outcome. The relationships between feature names and target names can be evaluated using language models (LMs) to produce semantic textual similarity (STS) scores, which can then be used to select features. We examine the performance using STS to select features directly and in the minimal-redundancy-maximal-relevance (mRMR) algorithm. The performance of STS as a feature selection metric is evaluated against preliminary survey data collected as a part of a clinical study on persistent post-surgical pain (PPSP). The results suggest that features selected with STS can result in higher performance models compared to traditional feature selection algorithms.
</details>
<details>
<summary>摘要</summary>
survey data 可以包含大量特征，但同时只有一小部分例子。机器学习模型在这些条件下预测结果时可能会过拟合，导致泛化性差。一种解决方案是特征选择，它尝试选择最佳的特征子来学习。文本特征名称可能是semantic indicative的特征相关性的一种不常 Investigated sources of information in the feature selection process。我们使用语言模型（LM）评估特征名称和目标名称之间的语义文本相似性（STS）分数，并用这些分数选择特征。我们对直接使用STS作为特征选择度量的性能进行了评估，并与传统特征选择算法进行比较。结果表明，使用STS选择特征可以对比传统特征选择算法获得更高性能的模型。
</details></li>
</ul>
<hr>
<h2 id="Inductive-bias-Learning-Generating-Code-Models-with-Large-Language-Model"><a href="#Inductive-bias-Learning-Generating-Code-Models-with-Large-Language-Model" class="headerlink" title="Inductive-bias Learning: Generating Code Models with Large Language Model"></a>Inductive-bias Learning: Generating Code Models with Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09890">http://arxiv.org/abs/2308.09890</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fuyu-quant/iblm">https://github.com/fuyu-quant/iblm</a></li>
<li>paper_authors: Toma Tanaka, Naofumi Emoto, Tsukasa Yumibayashi</li>
<li>For: The paper proposes a novel method called Inductive-Bias Learning (IBL) that combines the techniques of In-Context Learning (ICL) and code generation.* Methods: The paper uses a contextual understanding to generate a code with a necessary structure for inference, leveraging the property of inference without explicit inductive bias inherent in ICL and the readability and explainability of code generation.* Results: The generated Code Models have been found to achieve predictive accuracy comparable to, and in some cases surpassing, ICL and representative machine learning models.<details>
<summary>Abstract</summary>
Large Language Models(LLMs) have been attracting attention due to a ability called in-context learning(ICL). ICL, without updating the parameters of a LLM, it is possible to achieve highly accurate inference based on rules ``in the context'' by merely inputting a training data into the prompt. Although ICL is a developing field with many unanswered questions, LLMs themselves serves as a inference model, seemingly realizing inference without explicitly indicate ``inductive bias''. On the other hand, a code generation is also a highlighted application of LLMs. The accuracy of code generation has dramatically improved, enabling even non-engineers to generate code to perform the desired tasks by crafting appropriate prompts. In this paper, we propose a novel ``learning'' method called an ``Inductive-Bias Learning (IBL)'', which combines the techniques of ICL and code generation. An idea of IBL is straightforward. Like ICL, IBL inputs a training data into the prompt and outputs a code with a necessary structure for inference (we referred to as ``Code Model'') from a ``contextual understanding''. Despite being a seemingly simple approach, IBL encompasses both a ``property of inference without explicit inductive bias'' inherent in ICL and a ``readability and explainability'' of the code generation. Surprisingly, generated Code Models have been found to achieve predictive accuracy comparable to, and in some cases surpassing, ICL and representative machine learning models. Our IBL code is open source: https://github.com/fuyu-quant/IBLM
</details>
<details>
<summary>摘要</summary>
大型语言模型(LLMs) 在 latest 时期引起了关注，主要是因为它具有一种能力 называ为 "在上下文中学习" (ICL)。 ICL 可以在不更新 LLM 参数的情况下，通过输入训练数据来实现高度准确的推理，只需要在提问中输入训练数据。虽然 ICL 是一个还未解决的问题，但 LLMS 本身就是一种推理模型，似乎不需要显式地指定 "推理偏好"。此外，代码生成也是 LLMS 的突出应用。代码生成的准确率已经提高到了非常高的水平，使得even non-engineers可以通过制定合适的提问来生成代码以执行所需的任务。在这篇论文中，我们提出了一种新的 "学习" 方法，称为 "推理偏好学习" (IBL)。 IBL 结合了 ICL 和代码生成的技术。IBL 的想法是 straightforward。与 ICL 类似，IBL 通过输入训练数据来提问，并从上下文理解中生成一个代码模型（我们称之为 "代码模型"），以实现推理。尽管看起来很简单，但 IBL 包含了 ICL 中 "推理无需显式偏好" 的性质和代码生成 "可读性和解释性"。 surprisingly，生成的代码模型已经被发现可以达到与 ICL 和代表性机器学习模型相同或更高的预测精度。我们的 IBL 代码开源在 GitHub：https://github.com/fuyu-quant/IBLM
</details></li>
</ul>
<hr>
<h2 id="DUAW-Data-free-Universal-Adversarial-Watermark-against-Stable-Diffusion-Customization"><a href="#DUAW-Data-free-Universal-Adversarial-Watermark-against-Stable-Diffusion-Customization" class="headerlink" title="DUAW: Data-free Universal Adversarial Watermark against Stable Diffusion Customization"></a>DUAW: Data-free Universal Adversarial Watermark against Stable Diffusion Customization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09889">http://arxiv.org/abs/2308.09889</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoyu Ye, Hao Huang, Jiaqi An, Yongtao Wang</li>
<li>For: The paper aims to protect a myriad of copyrighted images from different customization approaches across various versions of SD models.* Methods: The proposed approach, called invisible data-free universal adversarial watermark (DUAW), is designed to disrupt the variational autoencoder during SD customization. It operates in a data-free context, where it is trained on synthetic images produced by a Large Language Model (LLM) and a pretrained SD model.* Results: Experimental results demonstrate that DUAW can effectively distort the outputs of fine-tuned SD models, rendering them discernible to both human observers and a simple classifier.Here’s the same information in Simplified Chinese text:* For: 该研究旨在保护不同版本的SD模型自定义的数字艺术作品，以防止抄袭和侵犯版权。* Methods: 提议的方法是透明无数据自由对抗水印（DUAW），通过在自适应变换器中打乱VAR的工作方式来保护权利图像。该方法在无需直接处理版权图像的情况下进行训练，通过大自然语言模型（LLM）和预训练SD模型生成的 sintetic图像来实现。* Results: 实验结果表明，DUAW可以有效地打乱定制后的SD模型输出，使其对人类观察员和简单的分类器都可见。<details>
<summary>Abstract</summary>
Stable Diffusion (SD) customization approaches enable users to personalize SD model outputs, greatly enhancing the flexibility and diversity of AI art. However, they also allow individuals to plagiarize specific styles or subjects from copyrighted images, which raises significant concerns about potential copyright infringement. To address this issue, we propose an invisible data-free universal adversarial watermark (DUAW), aiming to protect a myriad of copyrighted images from different customization approaches across various versions of SD models. First, DUAW is designed to disrupt the variational autoencoder during SD customization. Second, DUAW operates in a data-free context, where it is trained on synthetic images produced by a Large Language Model (LLM) and a pretrained SD model. This approach circumvents the necessity of directly handling copyrighted images, thereby preserving their confidentiality. Once crafted, DUAW can be imperceptibly integrated into massive copyrighted images, serving as a protective measure by inducing significant distortions in the images generated by customized SD models. Experimental results demonstrate that DUAW can effectively distort the outputs of fine-tuned SD models, rendering them discernible to both human observers and a simple classifier.
</details>
<details>
<summary>摘要</summary>
stable diffusion（SD）自定义方法可以让用户个性化SD模型输出，大大提高AI艺术的灵活性和多样性。然而，这些自定义方法也使得个人可以复制特定风格或主题的版权图像，这引发了对可能的版权侵犯的重大担忧。为解决这个问题，我们提出了隐形数据自由 universial adversarial watermark（DUAW），以保护不同版本的SD模型在不同自定义方法下生成的多种版权图像。首先，DUAW是在SD自定义过程中打乱变量自动encoder的。其次，DUAW在无数据上下文中进行训练，使用一个大型自然语言模型（LLM）和一个预训练的SD模型生成的 sintetic图像。这种方法可以避免直接处理版权图像，从而保持其 конфиденциальность。一旦创制完成，DUAW可以隐藏地将入prise到大量版权图像中，作为一种保护措施，使得生成的图像被自定义SD模型输出的 Distortion 可见 both to human observers and a simple classifier。实验结果表明，DUAW可以有效地对精度调整后的SD模型输出进行 Distortion，使其可见 both to human observers and a simple classifier。
</details></li>
</ul>
<hr>
<h2 id="On-Estimating-the-Gradient-of-the-Expected-Information-Gain-in-Bayesian-Experimental-Design"><a href="#On-Estimating-the-Gradient-of-the-Expected-Information-Gain-in-Bayesian-Experimental-Design" class="headerlink" title="On Estimating the Gradient of the Expected Information Gain in Bayesian Experimental Design"></a>On Estimating the Gradient of the Expected Information Gain in Bayesian Experimental Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09888">http://arxiv.org/abs/2308.09888</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ziq-ao/GradEIG">https://github.com/ziq-ao/GradEIG</a></li>
<li>paper_authors: Ziqiao Ao, Jinglai Li</li>
<li>for: 本研究的目的是为bayesian inference中的实验设计优化预期信息增强(EIG)的优化问题提供方法。</li>
<li>methods: 本研究提出了两种方法来估计EIG的梯度，分别是UEEG-MCMC和BEEG-AP。UEEG-MCMC通过MCMC生成 posterior samples来估计EIG梯度，而BEEG-AP则是通过重复使用参数样本来实现高效的 simulations。</li>
<li>results: 理论分析和数值实验表明，UEEG-MCMC是具有robust性的，而BEEG-AP在EIG值小时具有更高的效率。此外，两种方法在我们的数值实验中表现了更好的性能，比如popular benchmarks。<details>
<summary>Abstract</summary>
Bayesian Experimental Design (BED), which aims to find the optimal experimental conditions for Bayesian inference, is usually posed as to optimize the expected information gain (EIG). The gradient information is often needed for efficient EIG optimization, and as a result the ability to estimate the gradient of EIG is essential for BED problems. The primary goal of this work is to develop methods for estimating the gradient of EIG, which, combined with the stochastic gradient descent algorithms, result in efficient optimization of EIG. Specifically, we first introduce a posterior expected representation of the EIG gradient with respect to the design variables. Based on this, we propose two methods for estimating the EIG gradient, UEEG-MCMC that leverages posterior samples generated through Markov Chain Monte Carlo (MCMC) to estimate the EIG gradient, and BEEG-AP that focuses on achieving high simulation efficiency by repeatedly using parameter samples. Theoretical analysis and numerical studies illustrate that UEEG-MCMC is robust agains the actual EIG value, while BEEG-AP is more efficient when the EIG value to be optimized is small. Moreover, both methods show superior performance compared to several popular benchmarks in our numerical experiments.
</details>
<details>
<summary>摘要</summary>
bayesian 实验设计（BED），旨在找到bayesian 推理中的最佳实验条件，通常是要最大化预期信息增加（EIG）的。在这种情况下，梯度信息是非常重要的，因此能够估算EIG梯度的能力是BED问题的关键。本工作的主要目标是开发一些估算EIG梯度的方法，这些方法可以与梯度下降法相结合，从而实现EIG的有效优化。首先，我们引入了 posterior 预期表示EIG梯度的关系，并对这个表示进行了分析。然后，我们提出了两种估算EIG梯度的方法：UEEG-MCMC，利用MCMC生成的 posterior 样本来估算EIG梯度，和BEEG-AP，强调在实验中实现高效率，通过重复使用参数样本来实现。我们的理论分析和数值研究表明，UEEG-MCMC 对实际的EIG值具有较高的稳定性，而BEEG-AP 在EIG值小于一定程度时具有更高的效率。此外，两种方法在我们的数值实验中都表现出了较好的性能，比如几种流行的参考方法。
</details></li>
</ul>
<hr>
<h2 id="Calibrating-Uncertainty-for-Semi-Supervised-Crowd-Counting"><a href="#Calibrating-Uncertainty-for-Semi-Supervised-Crowd-Counting" class="headerlink" title="Calibrating Uncertainty for Semi-Supervised Crowd Counting"></a>Calibrating Uncertainty for Semi-Supervised Crowd Counting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09887">http://arxiv.org/abs/2308.09887</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Li, Xiaoling Hu, Shahira Abousamra, Chao Chen</li>
<li>for: 这篇论文的目的是提出一种新的半指导人数推断方法，以提高半指导人数推断 task 的性能。</li>
<li>methods: 本篇论文使用了一种基于 uncertainty 的 iterative pseudo-labeling 方法，通过一个 surrogate function 来训练模型，以控制模型的 uncertainty。</li>
<li>results: 本篇论文的结果显示，使用 proposed 方法可以生成可靠的 uncertainty estimation，高品质的 pseudo-labels，并 achieved state-of-the-art performance in semisupervised crowd counting task。<details>
<summary>Abstract</summary>
Semi-supervised crowd counting is an important yet challenging task. A popular approach is to iteratively generate pseudo-labels for unlabeled data and add them to the training set. The key is to use uncertainty to select reliable pseudo-labels. In this paper, we propose a novel method to calibrate model uncertainty for crowd counting. Our method takes a supervised uncertainty estimation strategy to train the model through a surrogate function. This ensures the uncertainty is well controlled throughout the training. We propose a matching-based patch-wise surrogate function to better approximate uncertainty for crowd counting tasks. The proposed method pays a sufficient amount of attention to details, while maintaining a proper granularity. Altogether our method is able to generate reliable uncertainty estimation, high quality pseudolabels, and achieve state-of-the-art performance in semisupervised crowd counting.
</details>
<details>
<summary>摘要</summary>
半指导的人群计数是一项重要但又具有挑战性的任务。一种受欢迎的方法是通过逐步生成 pseudo-标签 для无标签数据并将其添加到训练集中。关键在于使用uncertainty来选择可靠的 pseudo-标签。在这篇论文中，我们提出了一种新的方法来准确控制模型的uncertainty。我们使用一种监督型 uncertainty estimation 策略来训练模型，并使用一个 matching-based patch-wise 替换函数来更好地估计uncertainty。我们的方法具有着充分的注意力于细节，同时保持合理的粒度。总的来说，我们的方法可以生成可靠的 uncertainty estimation，高质量的 pseudo-标签，并实现semisupervised人群计数中的顶峰性能。
</details></li>
</ul>
<hr>
<h2 id="A-Transformer-based-Framework-For-Multi-variate-Time-Series-A-Remaining-Useful-Life-Prediction-Use-Case"><a href="#A-Transformer-based-Framework-For-Multi-variate-Time-Series-A-Remaining-Useful-Life-Prediction-Use-Case" class="headerlink" title="A Transformer-based Framework For Multi-variate Time Series: A Remaining Useful Life Prediction Use Case"></a>A Transformer-based Framework For Multi-variate Time Series: A Remaining Useful Life Prediction Use Case</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09884">http://arxiv.org/abs/2308.09884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oluwaseyi Ogunfowora, Homayoun Najjaran</li>
<li>for: 这个研究旨在提出一个基于encoder-transformer架构的多变量时间序列预测框架，用于预测机器的剩下有用生命时间（RUL）。</li>
<li>methods: 本研究使用了transformer模型，并进行了三个模型特有的实验，以将transformer模型从自然语言领域转移到时间序列领域。此外，本研究还提出了一个新的扩展窗口方法，以帮助模型识别机器的初期阶段和衰退路径。</li>
<li>results: 根据所有C-MAPPSbenchmark dataset上的四个集，这个提案的encoder-transformer模型的预测性能大幅提高，与13个现有的州际之最（SOTA）模型相比，平均提高137.65%。<details>
<summary>Abstract</summary>
In recent times, Large Language Models (LLMs) have captured a global spotlight and revolutionized the field of Natural Language Processing. One of the factors attributed to the effectiveness of LLMs is the model architecture used for training, transformers. Transformer models excel at capturing contextual features in sequential data since time series data are sequential, transformer models can be leveraged for more efficient time series data prediction. The field of prognostics is vital to system health management and proper maintenance planning. A reliable estimation of the remaining useful life (RUL) of machines holds the potential for substantial cost savings. This includes avoiding abrupt machine failures, maximizing equipment usage, and serving as a decision support system (DSS). This work proposed an encoder-transformer architecture-based framework for multivariate time series prediction for a prognostics use case. We validated the effectiveness of the proposed framework on all four sets of the C-MAPPS benchmark dataset for the remaining useful life prediction task. To effectively transfer the knowledge and application of transformers from the natural language domain to time series, three model-specific experiments were conducted. Also, to enable the model awareness of the initial stages of the machine life and its degradation path, a novel expanding window method was proposed for the first time in this work, it was compared with the sliding window method, and it led to a large improvement in the performance of the encoder transformer model. Finally, the performance of the proposed encoder-transformer model was evaluated on the test dataset and compared with the results from 13 other state-of-the-art (SOTA) models in the literature and it outperformed them all with an average performance increase of 137.65% over the next best model across all the datasets.
</details>
<details>
<summary>摘要</summary>
To address this challenge, this work proposes an encoder-transformer architecture-based framework for multivariate time series prediction in a prognostics use case. The proposed framework was validated on four datasets from the C-MAPPS benchmark, and three model-specific experiments were conducted to transfer knowledge from the natural language domain to time series. Additionally, a novel expanding window method was proposed to improve the model's awareness of the initial stages of machine life and its degradation path.The proposed encoder-transformer model outperformed 13 other state-of-the-art (SOTA) models in the literature with an average performance increase of 137.65% over the next best model across all datasets. This demonstrates the effectiveness of the proposed framework and the potential of transformer models for time series prediction in prognostics.
</details></li>
</ul>
<hr>
<h2 id="Flamingo-Multi-Round-Single-Server-Secure-Aggregation-with-Applications-to-Private-Federated-Learning"><a href="#Flamingo-Multi-Round-Single-Server-Secure-Aggregation-with-Applications-to-Private-Federated-Learning" class="headerlink" title="Flamingo: Multi-Round Single-Server Secure Aggregation with Applications to Private Federated Learning"></a>Flamingo: Multi-Round Single-Server Secure Aggregation with Applications to Private Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09883">http://arxiv.org/abs/2308.09883</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/eniac/flamingo">https://github.com/eniac/flamingo</a></li>
<li>paper_authors: Yiping Ma, Jess Woods, Sebastian Angel, Antigoni Polychroniadou, Tal Rabin</li>
<li>for: 这篇论文介绍了一种用于安全聚合数据的系统，以便在大量客户端上进行训练。</li>
<li>methods: 该系统使用了一种新的轻量级Dropout鲁棒性协议，以确保如果客户端在聚合过程中离开，服务器仍然可以获得有意义的结果。此外，它还引入了一种新的客户端 neighboorhood选择方法。</li>
<li>results: 作者们实现并评估了Flamingo系统，并证明了它可以安全地训练基于MNIST和CIFAR-100数据集的神经网络模型，并且模型的学习结果与非私有 Federated Learning 系统相同。<details>
<summary>Abstract</summary>
This paper introduces Flamingo, a system for secure aggregation of data across a large set of clients. In secure aggregation, a server sums up the private inputs of clients and obtains the result without learning anything about the individual inputs beyond what is implied by the final sum. Flamingo focuses on the multi-round setting found in federated learning in which many consecutive summations (averages) of model weights are performed to derive a good model. Previous protocols, such as Bell et al. (CCS '20), have been designed for a single round and are adapted to the federated learning setting by repeating the protocol multiple times. Flamingo eliminates the need for the per-round setup of previous protocols, and has a new lightweight dropout resilience protocol to ensure that if clients leave in the middle of a sum the server can still obtain a meaningful result. Furthermore, Flamingo introduces a new way to locally choose the so-called client neighborhood introduced by Bell et al. These techniques help Flamingo reduce the number of interactions between clients and the server, resulting in a significant reduction in the end-to-end runtime for a full training session over prior work. We implement and evaluate Flamingo and show that it can securely train a neural network on the (Extended) MNIST and CIFAR-100 datasets, and the model converges without a loss in accuracy, compared to a non-private federated learning system.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:这篇论文介绍了Flamingo系统，用于在多个客户端上安全地汇集数据。在安全汇集中，服务器将客户端的私有输入汇集起来，而不会学习到每个输入的细节，只知道最终结果的含义。Flamingo专注于联合学习中的多轮设定，在多个汇集（平均）后 derivate 出一个好的模型。先前的协议，如Bell et al. (CCS '20)，已经为单轮设定而设计，并在联合学习设定中重复协议多次。Flamingo消除了先前协议的每轮设定需求，并 introduce 了一种轻量级的dropout鲁棒性协议，以保证如果客户端在汇集过程中离开，服务器仍然可以获得有意义的结果。此外，Flamingo引入了一种新的客户端选择方法，以及 Bell et al. 所引入的客户端社区。这些技术帮助Flamingo减少客户端和服务器之间的交互数量，从而实现了与先前工作相比的很大减少。我们实现和评估了Flamingo，并证明它可以安全地训练（扩展）MNIST和CIFAR-100数据集上的神经网络模型，模型也可以在私有化联合学习环境中减少准确性损失。
</details></li>
</ul>
<hr>
<h2 id="Generative-Adversarial-Networks-Unlearning"><a href="#Generative-Adversarial-Networks-Unlearning" class="headerlink" title="Generative Adversarial Networks Unlearning"></a>Generative Adversarial Networks Unlearning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09881">http://arxiv.org/abs/2308.09881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hui Sun, Tianqing Zhu, Wenhan Chang, Wanlei Zhou<br>for: This paper focuses on the issue of unlearning in Generative Adversarial Networks (GANs), specifically addressing the challenges of generator unlearning and defining a criterion for the discriminator.methods: The authors propose a cascaded unlearning approach that utilizes a substitution mechanism and fake label to mitigate the challenges of generator unlearning.results: The proposed approach achieves significantly improved item and class unlearning efficiency, reducing the required time by up to 185x and 284x for the MNIST and CIFAR-10 datasets, respectively, compared to retraining from scratch. Additionally, the model’s performance experiences only minor degradation after unlearning, and has no adverse effects on downstream tasks such as classification.Here is the result in Simplified Chinese text:for: 这篇论文关注在生成 adversarial 网络（GANs）中的快速学习问题，特别是生成器快速学习的挑战和定义验证器的标准。methods: 作者提出了一种叠加快速学习方法，利用替换机制和假标签来解决生成器快速学习的挑战。results: 提议的方法在MNIST和CIFAR-10 datasets上实现了明显的项目和类快速学习效率提高，比重新训练从头开始的时间减少了185倍和284倍。此外，模型之后学习后的性能只受到了微小的降低，与只需要64个图像相比，无法对下游任务如分类造成负面影响。<details>
<summary>Abstract</summary>
As machine learning continues to develop, and data misuse scandals become more prevalent, individuals are becoming increasingly concerned about their personal information and are advocating for the right to remove their data. Machine unlearning has emerged as a solution to erase training data from trained machine learning models. Despite its success in classifiers, research on Generative Adversarial Networks (GANs) is limited due to their unique architecture, including a generator and a discriminator. One challenge pertains to generator unlearning, as the process could potentially disrupt the continuity and completeness of the latent space. This disruption might consequently diminish the model's effectiveness after unlearning. Another challenge is how to define a criterion that the discriminator should perform for the unlearning images. In this paper, we introduce a substitution mechanism and define a fake label to effectively mitigate these challenges. Based on the substitution mechanism and fake label, we propose a cascaded unlearning approach for both item and class unlearning within GAN models, in which the unlearning and learning processes run in a cascaded manner. We conducted a comprehensive evaluation of the cascaded unlearning technique using the MNIST and CIFAR-10 datasets. Experimental results demonstrate that this approach achieves significantly improved item and class unlearning efficiency, reducing the required time by up to 185x and 284x for the MNIST and CIFAR-10 datasets, respectively, in comparison to retraining from scratch. Notably, although the model's performance experiences minor degradation after unlearning, this reduction is negligible when dealing with a minimal number of images (e.g., 64) and has no adverse effects on downstream tasks such as classification.
</details>
<details>
<summary>摘要</summary>
随着机器学习技术的不断发展，个人数据泄露事件的出现也越来越普遍，人们对自己的个人信息越来越关注，并且呼吁保护自己的数据权利。机器学习模型中的数据解启（unlearning）技术已成为一种解决方案，可以将训练数据从已经训练过的机器学习模型中除去。然而，对于生成器（generator）和判别器（discriminator）的特殊架构，研究生成对抗网络（GANs）的unlearning却受到了限制。一个挑战在生成器unlearning中，即可能导致生成器的维度空间中断和不连续，从而影响模型的效果。另一个挑战是如何定义判别器对unlearning图像的标准。在本文中，我们提出了替换机制和假标签，以解决这些挑战。基于替换机制和假标签，我们提议一种叠加式unlearning方法，在GAN模型中进行项和类unlearning。我们在MNIST和CIFAR-10 datasets上进行了广泛的评估，结果表明，这种方法可以大幅提高item和类unlearning效率，比 retraining from scratch 需要的时间减少至多达185倍和284倍。尤其是，模型性能减少后仍然保持可观，只有当处理少量图像（例如64）时，这种减少才会导致轻微的性能下降，无法影响下游任务 such as classification。
</details></li>
</ul>
<hr>
<h2 id="DatasetEquity-Are-All-Samples-Created-Equal-In-The-Quest-For-Equity-Within-Datasets"><a href="#DatasetEquity-Are-All-Samples-Created-Equal-In-The-Quest-For-Equity-Within-Datasets" class="headerlink" title="DatasetEquity: Are All Samples Created Equal? In The Quest For Equity Within Datasets"></a>DatasetEquity: Are All Samples Created Equal? In The Quest For Equity Within Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09878">http://arxiv.org/abs/2308.09878</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/towardsautonomy/datasetequity">https://github.com/towardsautonomy/datasetequity</a></li>
<li>paper_authors: Shubham Shrivastava, Xianling Zhang, Sushruth Nagesh, Armin Parchami</li>
<li>for: Addressing data imbalance in machine learning, particularly in computer vision tasks.</li>
<li>methods: Using deep perceptual embeddings and clustering to compute sample likelihoods, and proposing a novel $\textbf{Generalized Focal Loss}$ function to weigh samples differently during training.</li>
<li>results: Achieving over $200%$ AP gains on under-represented classes (Cyclist) in the KITTI dataset, and demonstrating the method’s effectiveness across autonomous driving vision datasets including nuScenes.Here’s the full summary in Simplified Chinese:</li>
<li>for: 本研究旨在解决机器学习中的数据不均衡问题，特别是计算机视觉任务中的数据不均衡问题。</li>
<li>methods: 使用深度感知嵌入和聚类计算样本可能性，并提出一种新的$\textbf{扩展Focus损失函数}$来在训练中不同样本的权重。</li>
<li>results: 在KITTI数据集中，对于不足 Represented 类（自行车手）的AP得分提高了超过200%，并在自动驾驶视觉数据集（包括nuScenes）中证明了方法的一致性和通用性。<details>
<summary>Abstract</summary>
Data imbalance is a well-known issue in the field of machine learning, attributable to the cost of data collection, the difficulty of labeling, and the geographical distribution of the data. In computer vision, bias in data distribution caused by image appearance remains highly unexplored. Compared to categorical distributions using class labels, image appearance reveals complex relationships between objects beyond what class labels provide. Clustering deep perceptual features extracted from raw pixels gives a richer representation of the data. This paper presents a novel method for addressing data imbalance in machine learning. The method computes sample likelihoods based on image appearance using deep perceptual embeddings and clustering. It then uses these likelihoods to weigh samples differently during training with a proposed $\textbf{Generalized Focal Loss}$ function. This loss can be easily integrated with deep learning algorithms. Experiments validate the method's effectiveness across autonomous driving vision datasets including KITTI and nuScenes. The loss function improves state-of-the-art 3D object detection methods, achieving over $200\%$ AP gains on under-represented classes (Cyclist) in the KITTI dataset. The results demonstrate the method is generalizable, complements existing techniques, and is particularly beneficial for smaller datasets and rare classes. Code is available at: https://github.com/towardsautonomy/DatasetEquity
</details>
<details>
<summary>摘要</summary>
“数据不匹配是机器学习领域的一个公认问题，这可以归因于数据收集的成本、标签的困难以及数据的地域分布。在计算机视觉领域，图像的外观偏见对数据分布的偏见尚未得到充分的探讨。相比于使用类别标签的分布，图像的外观 revelas了对象之间复杂的关系，这些关系超出了类别标签所提供的信息。使用深度感知特征提取自原始像素的归一化可以为数据提供更加富有的表示。本文提出了一种 novel 的数据不匹配解决方法，该方法使用图像外观的深度感知嵌入和归一化计算样本的可能性。然后，使用这些可能性来调整样本的权重，并使用提议的 $\textbf{通用强调损失}$ 函数进行训练。这个损失函数可以轻松地与深度学习算法结合使用。实验证明了该方法在自动驾驶视觉 datasets 中的效果，包括 KITTI 和 nuScenes。该损失函数可以提高 state-of-the-art 3D 物体检测方法的性能，在 KITTI dataset 中Cyclist 类型的下 Represented 类型中获得了更 than 200% AP 提升。结果表明该方法是通用的，可以补充现有的技术，特别是对小型 datasets 和罕见类型的支持。代码可以在：https://github.com/towardsautonomy/DatasetEquity 中找到。”Note that Simplified Chinese is used in the translation, as it is more widely used in mainland China and is the standard language used in most online platforms and publications. If you prefer Traditional Chinese, I can provide that version as well.
</details></li>
</ul>
<hr>
<h2 id="Skill-Transformer-A-Monolithic-Policy-for-Mobile-Manipulation"><a href="#Skill-Transformer-A-Monolithic-Policy-for-Mobile-Manipulation" class="headerlink" title="Skill Transformer: A Monolithic Policy for Mobile Manipulation"></a>Skill Transformer: A Monolithic Policy for Mobile Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09873">http://arxiv.org/abs/2308.09873</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoyu Huang, Dhruv Batra, Akshara Rai, Andrew Szot</li>
<li>for: 解决长期机器人任务， combining conditional sequence modeling 和技能归一化。</li>
<li>methods: 使用 transformer 架构，通过示例轨迹进行端到端训练，预测高级技能和全身低级动作。</li>
<li>results: 在embodied rearrangement benchmark上测试， Skill Transformer 可以实现 robust 任务规划和低级控制，成功率高于基eline 2.5倍。<details>
<summary>Abstract</summary>
We present Skill Transformer, an approach for solving long-horizon robotic tasks by combining conditional sequence modeling and skill modularity. Conditioned on egocentric and proprioceptive observations of a robot, Skill Transformer is trained end-to-end to predict both a high-level skill (e.g., navigation, picking, placing), and a whole-body low-level action (e.g., base and arm motion), using a transformer architecture and demonstration trajectories that solve the full task. It retains the composability and modularity of the overall task through a skill predictor module while reasoning about low-level actions and avoiding hand-off errors, common in modular approaches. We test Skill Transformer on an embodied rearrangement benchmark and find it performs robust task planning and low-level control in new scenarios, achieving a 2.5x higher success rate than baselines in hard rearrangement problems.
</details>
<details>
<summary>摘要</summary>
我们提出了技能变换器，一种解决长期机器人任务的方法，结合条件序列模型和技能分解性。基于机器人 egocentric 和 proprioceptive 观察，技能变换器通过 transformer 架构进行端到端训练，预测高级技能（例如导航、捕捉、放置）和整体低级动作（例如基底和臂动作），使用示例轨迹解决整个任务。它保留了总任务的可组合性和分解性，通过技能预测模块进行低级动作的编制和避免手动错误，常见于模块化方法。我们在embodied重新排序测试上测试了技能变换器，发现它在新的情况下能够做出坚固的任务规划和低级控制，比基eline高2.5倍成功率。
</details></li>
</ul>
<hr>
<h2 id="Tensor-Compressed-Back-Propagation-Free-Training-for-Physics-Informed-Neural-Networks"><a href="#Tensor-Compressed-Back-Propagation-Free-Training-for-Physics-Informed-Neural-Networks" class="headerlink" title="Tensor-Compressed Back-Propagation-Free Training for (Physics-Informed) Neural Networks"></a>Tensor-Compressed Back-Propagation-Free Training for (Physics-Informed) Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09858">http://arxiv.org/abs/2308.09858</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yequan Zhao, Xinling Yu, Zhixiong Chen, Ziyue Liu, Sijia Liu, Zheng Zhang</li>
<li>for:  This paper aims to present a completely backward propagation (BP)-free framework for training realistic neural networks on edge devices, which can greatly improve the design complexity and time-to-market of on-device training accelerators.</li>
<li>methods:  The paper proposes a three-fold technical contribution to achieve BP-free training: (1) a tensor-compressed variance reduction approach to improve the scalability of zeroth-order (ZO) optimization, (2) a hybrid gradient evaluation approach to improve the efficiency of ZO training, and (3) an extension of the BP-free training framework to physics-informed neural networks (PINNs) using a sparse-grid approach to estimate derivatives without BP.</li>
<li>results:  The paper shows that the proposed BP-free training framework only loses little accuracy on the MNIST dataset compared with standard first-order training, and successfully trains a PINN for solving a 20-dim Hamiltonian-Jacobi-Bellman PDE. The memory-efficient and BP-free approach may serve as a foundation for the near-future on-device training on many resource-constraint platforms (e.g., FPGA, ASIC, micro-controllers, and photonic chips).<details>
<summary>Abstract</summary>
Backward propagation (BP) is widely used to compute the gradients in neural network training. However, it is hard to implement BP on edge devices due to the lack of hardware and software resources to support automatic differentiation. This has tremendously increased the design complexity and time-to-market of on-device training accelerators. This paper presents a completely BP-free framework that only requires forward propagation to train realistic neural networks. Our technical contributions are three-fold. Firstly, we present a tensor-compressed variance reduction approach to greatly improve the scalability of zeroth-order (ZO) optimization, making it feasible to handle a network size that is beyond the capability of previous ZO approaches. Secondly, we present a hybrid gradient evaluation approach to improve the efficiency of ZO training. Finally, we extend our BP-free training framework to physics-informed neural networks (PINNs) by proposing a sparse-grid approach to estimate the derivatives in the loss function without using BP. Our BP-free training only loses little accuracy on the MNIST dataset compared with standard first-order training. We also demonstrate successful results in training a PINN for solving a 20-dim Hamiltonian-Jacobi-Bellman PDE. This memory-efficient and BP-free approach may serve as a foundation for the near-future on-device training on many resource-constraint platforms (e.g., FPGA, ASIC, micro-controllers, and photonic chips).
</details>
<details>
<summary>摘要</summary>
<<SYS>>用简化中文表示大多数 neural network 训练使用倒推 propagation (BP) 计算梯度，但在边缘设备上实现 BP 具有硬件和软件资源的限制，导致训练减速器的设计复杂度和时间到市场增加。这篇论文提出了一个完全无需 BP 的框架，只需要前向传播来训练真实的 neural network。我们的技术贡献包括以下三个方面：1. 我们提出了一种紧凑变量 reduction 技术，以提高 zero-order (ZO) 优化的扩展性，使得可以处理的网络大小超出了先前 ZO 方法的能力。2. 我们提出了一种混合式梯度评估方法，以提高 ZO 训练的效率。3. 我们将我们的 BP-free 训练框架应用到物理学 informed neural networks (PINNs) 中，提出了一种稀疏网格方法，以无需 BP 来估算损失函数中的导数。我们的 BP-free 训练只在 MNIST 数据集上减少了一些精度，与标准首次训练相比。我们还成功地训练了一个 PINN 来解决一个 20 维 Hamiltonian-Jacobi-Bellman PDE。这种内存有效并且 BP-free 的方法可能将成为未来资源限制的平台（如 FPGA、ASIC、微控制器和光子Integrated Circuits）上的训练基础。
</details></li>
</ul>
<hr>
<h2 id="Backdoor-Mitigation-by-Correcting-the-Distribution-of-Neural-Activations"><a href="#Backdoor-Mitigation-by-Correcting-the-Distribution-of-Neural-Activations" class="headerlink" title="Backdoor Mitigation by Correcting the Distribution of Neural Activations"></a>Backdoor Mitigation by Correcting the Distribution of Neural Activations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09850">http://arxiv.org/abs/2308.09850</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xi Li, Zhen Xiang, David J. Miller, George Kesidis</li>
<li>for: 本研究旨在揭示和分析对深度神经网络（DNN）的后门（Trojan）攻击的一种重要性，即在攻击成功后，攻击者的后门触发器会导致内层活动的分布变化，并且如果这种变化得到修复，则可以正确地将攻击者的目标类归类。</li>
<li>methods: 本研究使用了反工程化的触发器来修复后门攻击所导致的分布变化，并不需要改变DNN的任何可调参数。</li>
<li>results: 对比 existed 方法，本研究的方法可以更好地 mitigate 后门攻击，并且可以有效地检测测试实例中是否存在触发器。<details>
<summary>Abstract</summary>
Backdoor (Trojan) attacks are an important type of adversarial exploit against deep neural networks (DNNs), wherein a test instance is (mis)classified to the attacker's target class whenever the attacker's backdoor trigger is present. In this paper, we reveal and analyze an important property of backdoor attacks: a successful attack causes an alteration in the distribution of internal layer activations for backdoor-trigger instances, compared to that for clean instances. Even more importantly, we find that instances with the backdoor trigger will be correctly classified to their original source classes if this distribution alteration is corrected. Based on our observations, we propose an efficient and effective method that achieves post-training backdoor mitigation by correcting the distribution alteration using reverse-engineered triggers. Notably, our method does not change any trainable parameters of the DNN, but achieves generally better mitigation performance than existing methods that do require intensive DNN parameter tuning. It also efficiently detects test instances with the trigger, which may help to catch adversarial entities in the act of exploiting the backdoor.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本翻译成简化中文：<</SYS>>深度神经网络（DNN）面临着重要的反对抗攻击，即后门（Trojan）攻击，其中测试实例在攻击者的目标类上被识别为攻击者的后门触发器存在时。在这篇论文中，我们揭示了和分析了后门攻击的一个重要性特征：成功攻击会导致后门触发器实例的内部层活动分布变化，相比于干净实例。更重要的是，我们发现了一个关键的现象：如果这种分布变化得到了修正，那么带有后门触发器的实例将被正确地分类到其原始的类别。基于我们的观察，我们提出了一种高效和有效的后门恢复方法，通过修正分布变化来实现。这种方法不改变了DNN的任何可变参数，但它在恢复性能方面比既有的方法更好，并且可以有效地检测测试实例中的触发器。这可能帮助捕捉了利用后门攻击的恶意实体。
</details></li>
</ul>
<hr>
<h2 id="Enumerating-Safe-Regions-in-Deep-Neural-Networks-with-Provable-Probabilistic-Guarantees"><a href="#Enumerating-Safe-Regions-in-Deep-Neural-Networks-with-Provable-Probabilistic-Guarantees" class="headerlink" title="Enumerating Safe Regions in Deep Neural Networks with Provable Probabilistic Guarantees"></a>Enumerating Safe Regions in Deep Neural Networks with Provable Probabilistic Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09842">http://arxiv.org/abs/2308.09842</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Marzari, Davide Corsi, Enrico Marchesini, Alessandro Farinelli, Ferdinando Cicalese</li>
<li>for:  Ensuring trust in Deep Neural Networks (DNNs) by identifying safe areas.</li>
<li>methods:  Proposed an efficient approximation method called epsilon-ProVe, which exploits statistical prediction of tolerance limits to provide a tight lower estimate of the safe areas.</li>
<li>results:  Scalable and effective method that offers provable probabilistic guarantees, evaluated on standard benchmarks.<details>
<summary>Abstract</summary>
Identifying safe areas is a key point to guarantee trust for systems that are based on Deep Neural Networks (DNNs). To this end, we introduce the AllDNN-Verification problem: given a safety property and a DNN, enumerate the set of all the regions of the property input domain which are safe, i.e., where the property does hold. Due to the #P-hardness of the problem, we propose an efficient approximation method called epsilon-ProVe. Our approach exploits a controllable underestimation of the output reachable sets obtained via statistical prediction of tolerance limits, and can provide a tight (with provable probabilistic guarantees) lower estimate of the safe areas. Our empirical evaluation on different standard benchmarks shows the scalability and effectiveness of our method, offering valuable insights for this new type of verification of DNNs.
</details>
<details>
<summary>摘要</summary>
安全区域的标识是深度神经网络（DNN）系统的关键安全保证点。为此，我们提出了AllDNN-Verification问题：给定一个安全性质和一个DNN，列出安全区域的输入领域中的所有区域，即where the property does hold。由于这个问题的#P-hardness，我们提出了一种高效的近似方法called epsilon-ProVe。我们的方法利用通过统计预测容差范围来控制输出可达集的下界，并可以提供一个紧靠的（具有可证明的概率保证）下界。我们的实验表明我们的方法可以扩展到不同的标准准比，并且有效地验证DNNs。这些结果提供了对这种新类型的验证方法的有价值的理解。
</details></li>
</ul>
<hr>
<h2 id="Microscopy-Image-Segmentation-via-Point-and-Shape-Regularized-Data-Synthesis"><a href="#Microscopy-Image-Segmentation-via-Point-and-Shape-Regularized-Data-Synthesis" class="headerlink" title="Microscopy Image Segmentation via Point and Shape Regularized Data Synthesis"></a>Microscopy Image Segmentation via Point and Shape Regularized Data Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09835">http://arxiv.org/abs/2308.09835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shijie Li, Mengwei Ren, Thomas Ach, Guido Gerig<br>for:* 这个论文主要针对的是用深度学习方法进行微scopia图像分割，但是现有的方法几乎都需要大量的训练数据，包括完整的对象边界信息，这会非常困难和成本高昂。methods:* 作者提出了一个整体框架，用于将点注释转换为 Synthetically generated training data，包括三个阶段：	1. 使用点注释生成一个 Pseudo dense segmentation mask，并使用 shape priors 来约束其生成;	2. 使用一个未经 paired 训练的图像生成模型，将 Pseudo mask 翻译成一个真实的 microscopy 图像，并使用 object level consistency 来补做;	3. 将 Pseudo masks 和生成的图像组成一个 pairwise 数据集，用于训练适应的分割模型。results:* 作者在公共的 MoNuSeg 数据集上测试了自己的生成框架，并发现其生成的图像比基eline模型更加多样化和真实，同时保持了输入mask和生成图像之间的高协调性;* 当使用同一个分割后果的模型，使用作者的生成数据集进行训练，与使用 pseudo-labels 或基eline生成的图像进行训练相比，模型的性能明显提高;* 作者的框架可以与 dense 标注数据进行比较，并且在 authentic microscopy 图像上达到相同的性能，这表明其可以作为 dense 标注数据的可靠和高效的替代方案。<details>
<summary>Abstract</summary>
Current deep learning-based approaches for the segmentation of microscopy images heavily rely on large amount of training data with dense annotation, which is highly costly and laborious in practice. Compared to full annotation where the complete contour of objects is depicted, point annotations, specifically object centroids, are much easier to acquire and still provide crucial information about the objects for subsequent segmentation. In this paper, we assume access to point annotations only during training and develop a unified pipeline for microscopy image segmentation using synthetically generated training data. Our framework includes three stages: (1) it takes point annotations and samples a pseudo dense segmentation mask constrained with shape priors; (2) with an image generative model trained in an unpaired manner, it translates the mask to a realistic microscopy image regularized by object level consistency; (3) the pseudo masks along with the synthetic images then constitute a pairwise dataset for training an ad-hoc segmentation model. On the public MoNuSeg dataset, our synthesis pipeline produces more diverse and realistic images than baseline models while maintaining high coherence between input masks and generated images. When using the identical segmentation backbones, the models trained on our synthetic dataset significantly outperform those trained with pseudo-labels or baseline-generated images. Moreover, our framework achieves comparable results to models trained on authentic microscopy images with dense labels, demonstrating its potential as a reliable and highly efficient alternative to labor-intensive manual pixel-wise annotations in microscopy image segmentation. The code is available.
</details>
<details>
<summary>摘要</summary>
当前的深度学习基本方法 для微scopic图像分割具有大量的训练数据和密集的标注，但在实际应用中是非常成本高昂和劳动密集的。相比于全部标注，其中包括对象的完整边框，点标注，特别是对象的中心点，更加容易获取并且仍然提供了对象分割的关键信息。在这篇论文中，我们假设在训练时有点标注可用，并开发了一个整体框架，包括以下三个阶段：1. 使用点标注生成一个 Pseudo 稠密分割mask，并将其约束于形状假设；2. 使用一个在无对应方式下训练的图像生成模型，将 Pseudo 分割mask 翻译成一个真实的微scopic图像，并对其进行对象水平的准确性 regularization；3. Pseudo 分割mask 和生成的图像组成一个对应的数据集，用于训练适应性的分割模型。在公共的 MoNuSeg 数据集上，我们的生成框架生成了更加多样和真实的图像，同时保持了输入掩模的高准确性。使用同样的分割背包，我们在我们的 sintetic 数据集上训练的模型比使用 Pseudo 标签或基eline-生成的图像训练得更高效，并且与 dense 标注的模型相当。此外，我们的框架可以实现 dense 标注的微scopic图像分割任务中的高效和可靠的替代方案。代码可用。
</details></li>
</ul>
<hr>
<h2 id="Learning-from-A-Single-Graph-is-All-You-Need-for-Near-Shortest-Path-Routing-in-Wireless-Networks"><a href="#Learning-from-A-Single-Graph-is-All-You-Need-for-Near-Shortest-Path-Routing-in-Wireless-Networks" class="headerlink" title="Learning from A Single Graph is All You Need for Near-Shortest Path Routing in Wireless Networks"></a>Learning from A Single Graph is All You Need for Near-Shortest Path Routing in Wireless Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09829">http://arxiv.org/abs/2308.09829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yung-Fu Chen, Sen Lin, Anish Arora</li>
<li>for: 本研究旨在开发一种可以在无需大量数据样本的情况下，适应所有随机网络模型的本地路由策略学习算法。</li>
<li>methods: 本研究使用深度神经网络（DNNs）来学习本地路由策略，该策略只考虑当前节点和邻居节点的状态。研究者们在选择输入特征和选择“种子图”和子样本的方面借鉴了网络领域知识，以提供理论上的解释性。</li>
<li>results: 研究结果表明，使用生成于一些路由路径的抽样从一个较小的种子图就能够快速学习一个普适的路由策略，该策略可以在大多数随机网络模型中适用。此外，研究者们还发现了一种神经网络，可以准确地模仿扩散前方路由策略的性能。<details>
<summary>Abstract</summary>
We propose a learning algorithm for local routing policies that needs only a few data samples obtained from a single graph while generalizing to all random graphs in a standard model of wireless networks. We thus solve the all-pairs near-shortest path problem by training deep neural networks (DNNs) that efficiently and scalably learn routing policies that are local, i.e., they only consider node states and the states of neighboring nodes. Remarkably, one of these DNNs we train learns a policy that exactly matches the performance of greedy forwarding; another generally outperforms greedy forwarding. Our algorithm design exploits network domain knowledge in several ways: First, in the selection of input features and, second, in the selection of a ``seed graph'' and subsamples from its shortest paths. The leverage of domain knowledge provides theoretical explainability of why the seed graph and node subsampling suffice for learning that is efficient, scalable, and generalizable. Simulation-based results on uniform random graphs with diverse sizes and densities empirically corroborate that using samples generated from a few routing paths in a modest-sized seed graph quickly learns a model that is generalizable across (almost) all random graphs in the wireless network model.
</details>
<details>
<summary>摘要</summary>
Our algorithm leverages network domain knowledge in two ways:1. Selection of input features: We carefully select the input features to ensure that the learned policy is efficient and scalable.2. Selection of a "seed graph" and subsamples from its shortest paths: We use a small seed graph and subsamples from its shortest paths to train the DNNs, which provides theoretical explainability of why the seed graph and node subsampling suffice for learning.Our simulation results on uniform random graphs with diverse sizes and densities show that using samples generated from a few routing paths in a modest-sized seed graph can quickly learn a model that is generalizable across (almost) all random graphs in the wireless network model. This means that our algorithm can learn a routing policy that is effective and efficient, even in a large and complex wireless network.
</details></li>
</ul>
<hr>
<h2 id="VL-PET-Vision-and-Language-Parameter-Efficient-Tuning-via-Granularity-Control"><a href="#VL-PET-Vision-and-Language-Parameter-Efficient-Tuning-via-Granularity-Control" class="headerlink" title="VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity Control"></a>VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09804">http://arxiv.org/abs/2308.09804</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/henryhzy/vl-pet">https://github.com/henryhzy/vl-pet</a></li>
<li>paper_authors: Zi-Yuan Hu, Yanyang Li, Michael R. Lyu, Liwei Wang</li>
<li>for: 这个研究旨在提出一个可控的vision-and-language参数效率训练（VL-PET）框架，以提高现有的参数效率训练技术的精确度和效率。</li>
<li>methods: 本研究使用了一个新的粒度控制机制，允许在不同的粒度控制矩阵上进行模块化修改，从而产生多种模型独立的VL-PET模组。此外，我们还提出了一些轻量级PET模组的设计，以提高预料和文本生成的整合。</li>
<li>results: 我们在四个图像数据项目和四个影片数据项目上进行了广泛的实验，结果显示了我们的VL-PET框架在效率、有效性和转移性方面具有优秀的表现。尤其是，我们的VL-PET-大模组，配备了轻量级PET模组，与BART-base和T5-base相比，在图像数据项目上提高了2.92%（3.41%）和3.37%（7.03%）。<details>
<summary>Abstract</summary>
As the model size of pre-trained language models (PLMs) grows rapidly, full fine-tuning becomes prohibitively expensive for model training and storage. In vision-and-language (VL), parameter-efficient tuning (PET) techniques are proposed to integrate modular modifications (e.g., Adapter and LoRA) into encoder-decoder PLMs. By tuning a small set of trainable parameters, these techniques perform on par with full fine-tuning. However, excessive modular modifications and neglecting the functionality gap between the encoders and decoders can lead to performance degradation, while existing PET techniques (e.g., VL-Adapter) overlook these critical issues. In this paper, we propose a Vision-and-Language Parameter-Efficient Tuning (VL-PET) framework to impose effective control over modular modifications via a novel granularity-controlled mechanism. Considering different granularity-controlled matrices generated by this mechanism, a variety of model-agnostic VL-PET modules can be instantiated from our framework for better efficiency and effectiveness trade-offs. We further propose lightweight PET module designs to enhance VL alignment and modeling for the encoders and maintain text generation for the decoders. Extensive experiments conducted on four image-text tasks and four video-text tasks demonstrate the efficiency, effectiveness and transferability of our VL-PET framework. In particular, our VL-PET-large with lightweight PET module designs significantly outperforms VL-Adapter by 2.92% (3.41%) and LoRA by 3.37% (7.03%) with BART-base (T5-base) on image-text tasks. Furthermore, we validate the enhanced effect of employing our VL-PET designs on existing PET techniques, enabling them to achieve significant performance improvements. Our code is available at https://github.com/HenryHZY/VL-PET.
</details>
<details>
<summary>摘要</summary>
As the size of pre-trained language models (PLMs) continues to grow rapidly, full fine-tuning becomes increasingly expensive for model training and storage. In the field of vision-and-language (VL), parameter-efficient tuning (PET) techniques have been proposed to integrate modular modifications (e.g., Adapter and LoRA) into encoder-decoder PLMs. By tuning a small set of trainable parameters, these techniques can achieve performance on par with full fine-tuning. However, excessive modular modifications and neglecting the functionality gap between the encoders and decoders can lead to performance degradation. Additionally, existing PET techniques (e.g., VL-Adapter) overlook these critical issues.In this paper, we propose a Vision-and-Language Parameter-Efficient Tuning (VL-PET) framework to effectively control modular modifications through a novel granularity-controlled mechanism. By considering different granularity-controlled matrices generated by this mechanism, a variety of model-agnostic VL-PET modules can be instantiated from our framework for better efficiency and effectiveness trade-offs. Furthermore, we propose lightweight PET module designs to enhance VL alignment and modeling for the encoders and maintain text generation for the decoders.Extensive experiments conducted on four image-text tasks and four video-text tasks demonstrate the efficiency, effectiveness, and transferability of our VL-PET framework. In particular, our VL-PET-large with lightweight PET module designs significantly outperforms VL-Adapter by 2.92% (3.41%) and LoRA by 3.37% (7.03%) with BART-base (T5-base) on image-text tasks. Additionally, we validate the enhanced effect of employing our VL-PET designs on existing PET techniques, enabling them to achieve significant performance improvements. Our code is available at https://github.com/HenryHZY/VL-PET.
</details></li>
</ul>
<hr>
<h2 id="An-Efficient-High-Dimensional-Gene-Selection-Approach-based-on-Binary-Horse-Herd-Optimization-Algorithm-for-Biological-Data-Classification"><a href="#An-Efficient-High-Dimensional-Gene-Selection-Approach-based-on-Binary-Horse-Herd-Optimization-Algorithm-for-Biological-Data-Classification" class="headerlink" title="An Efficient High-Dimensional Gene Selection Approach based on Binary Horse Herd Optimization Algorithm for Biological Data Classification"></a>An Efficient High-Dimensional Gene Selection Approach based on Binary Horse Herd Optimization Algorithm for Biological Data Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09791">http://arxiv.org/abs/2308.09791</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niloufar Mehrabi, Sayed Pedram Haeri Boroujeni, Elnaz Pashaei</li>
<li>for: 解决复杂和高维度问题</li>
<li>methods: 使用新的Horse Herd Optimization Algorithm（HOA）和一种新的 Transfer Function（TF），以及一种hybrid feature selection方法 combining HOA和MRMR筛选法。</li>
<li>results: 比较其他状态的精准率和最少选择的特征数，MRMR-BHOA方法表现出色，并且实验结果表明X-形BHOA方法超过其他方法。<details>
<summary>Abstract</summary>
The Horse Herd Optimization Algorithm (HOA) is a new meta-heuristic algorithm based on the behaviors of horses at different ages. The HOA was introduced recently to solve complex and high-dimensional problems. This paper proposes a binary version of the Horse Herd Optimization Algorithm (BHOA) in order to solve discrete problems and select prominent feature subsets. Moreover, this study provides a novel hybrid feature selection framework based on the BHOA and a minimum Redundancy Maximum Relevance (MRMR) filter method. This hybrid feature selection, which is more computationally efficient, produces a beneficial subset of relevant and informative features. Since feature selection is a binary problem, we have applied a new Transfer Function (TF), called X-shape TF, which transforms continuous problems into binary search spaces. Furthermore, the Support Vector Machine (SVM) is utilized to examine the efficiency of the proposed method on ten microarray datasets, namely Lymphoma, Prostate, Brain-1, DLBCL, SRBCT, Leukemia, Ovarian, Colon, Lung, and MLL. In comparison to other state-of-the-art, such as the Gray Wolf (GW), Particle Swarm Optimization (PSO), and Genetic Algorithm (GA), the proposed hybrid method (MRMR-BHOA) demonstrates superior performance in terms of accuracy and minimum selected features. Also, experimental results prove that the X-Shaped BHOA approach outperforms others methods.
</details>
<details>
<summary>摘要</summary>
《马群优化算法（HOA）是一种新的meta-heuristic算法，基于马匹不同年龄的行为。HOA最近被引入以解决复杂高维问题。本文提出了一种二进制版本的马群优化算法（BHOA），用于解 discrete问题并选择出色特征子集。此外，本研究还提出了一种 hybrid 特征选择框架，基于 BHOA 和最小重复度最大相关性（MRMR）筛选法。这种 hybrid 特征选择更加 computationally efficient，生成了有利的特征子集。由于特征选择是一个二进制问题，我们采用了一个新的转移函数（TF），称为 X-形 TF，将连续问题转换成二进制搜索空间。此外，我们使用了支持向量机（SVM）来评估提案方法在十个 microarray 数据集上的效率，即 Limphoma、Prostate、Brain-1、DLBCL、SRBCT、Leukemia、Ovarian、Colon、Lung 和 MLL。与其他现有的state-of-the-art，如灰狼（GW）、PARTICLE SWARM OPTIMIZATION（PSO）和遗传算法（GA）相比，我们的 hybrid 方法（MRMR-BHOA）在准确率和选择的最小特征数上显示出超越性。此外，实验结果也证明了 X-Shape BHOA 方法的优越性。
</details></li>
</ul>
<hr>
<h2 id="A-Two-Part-Machine-Learning-Approach-to-Characterizing-Network-Interference-in-A-B-Testing"><a href="#A-Two-Part-Machine-Learning-Approach-to-Characterizing-Network-Interference-in-A-B-Testing" class="headerlink" title="A Two-Part Machine Learning Approach to Characterizing Network Interference in A&#x2F;B Testing"></a>A Two-Part Machine Learning Approach to Characterizing Network Interference in A&#x2F;B Testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09790">http://arxiv.org/abs/2308.09790</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Yuan, Kristen M. Altenburger</li>
<li>for: 提高控制实验的可靠性，解决网络干扰问题</li>
<li>methods: 使用机器学习算法自动识别和特征化不同类型的网络干扰，并使用透明的机器学习模型确定最佳曝光映射</li>
<li>results: 通过验证两个synthetic实验和一个实际的大规模Instagram用户测试，与传统方法相比，提高了A&#x2F;B测试结果的精度和可靠性<details>
<summary>Abstract</summary>
The reliability of controlled experiments, or "A/B tests," can often be compromised due to the phenomenon of network interference, wherein the outcome for one unit is influenced by other units. To tackle this challenge, we propose a machine learning-based method to identify and characterize heterogeneous network interference. Our approach accounts for latent complex network structures and automates the task of "exposure mapping'' determination, which addresses the two major limitations in the existing literature. We introduce "causal network motifs'' and employ transparent machine learning models to establish the most suitable exposure mapping that reflects underlying network interference patterns. Our method's efficacy has been validated through simulations on two synthetic experiments and a real-world, large-scale test involving 1-2 million Instagram users, outperforming conventional methods such as design-based cluster randomization and analysis-based neighborhood exposure mapping. Overall, our approach not only offers a comprehensive, automated solution for managing network interference and improving the precision of A/B testing results, but it also sheds light on users' mutual influence and aids in the refinement of marketing strategies.
</details>
<details>
<summary>摘要</summary>
控制实验的可靠性，或“A/B测试”，经常受到网络干扰的影响，其中一个单元的结果会受到其他单元的影响。为解决这个挑战，我们提议一种基于机器学习的方法，用于识别和特征化不同类型的网络干扰。我们的方法考虑了隐藏的复杂网络结构，并自动确定“曝光 mapping”，解决了现有文献中的两大限制。我们引入“ causal 网络模式”并使用透明的机器学习模型，以确定最佳的曝光 mapping，它反映了下面网络干扰模式。我们的方法在两个人工实验和一个实际的大规模实验中，与传统的设计基于块随机分配和分析基于邻居曝光 mapping相比，表现出了更高的效果。总之，我们的方法不仅提供了一种完整、自动化的网络干扰管理和A/B测试结果的精度提高的解决方案，还可以揭示用户之间的互动关系，帮助改进营销策略。
</details></li>
</ul>
<hr>
<h2 id="Towards-Grounded-Visual-Spatial-Reasoning-in-Multi-Modal-Vision-Language-Models"><a href="#Towards-Grounded-Visual-Spatial-Reasoning-in-Multi-Modal-Vision-Language-Models" class="headerlink" title="Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models"></a>Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09778">http://arxiv.org/abs/2308.09778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navid Rajabi, Jana Kosecka</li>
<li>for: 研究大规模视言模型（VLM）在视觉理解任务中表现，特别是识别空间关系的能力。</li>
<li>methods: 提出细化的 композиitional 顺序排序方法，结合图像文本匹配或视觉问答任务，以评估视觉关系理解能力。</li>
<li>results: 通过对象和其位置的识别，计算最终排名的空间 clause，并在多种视觉语言模型（Tan和Bansal 2019; Gupta等 2022; Kamath等 2021）上进行评估和比较，以 highlight 它们在理解空间关系方面的能力。<details>
<summary>Abstract</summary>
With the advances in large scale vision-and-language models (VLMs) it is of interest to assess their performance on various visual reasoning tasks such as counting, referring expressions and general visual question answering. The focus of this work is to study the ability of these models to understanding spatial relations. Previously, this has been tackled using image-text matching (Liu, Emerson, and Collier 2022) or visual question answering task, both showing poor performance and a large gap compared to human performance. To better understand the gap, we present fine-grained compositional grounding of spatial relationships and propose a bottom up approach for ranking spatial clauses and evaluating the performance of spatial relationship reasoning task. We propose to combine the evidence from grounding noun phrases corresponding to objects and their locations to compute the final rank of the spatial clause. We demonstrate the approach on representative vision-language models (Tan and Bansal 2019; Gupta et al. 2022; Kamath et al. 2021) and compare and highlight their abilities to reason about spatial relationships.
</details>
<details>
<summary>摘要</summary>
通过大规模视力语言模型（VLM）的发展，我们关心其在不同的视觉逻辑任务中的表现，如数字、引用表达和通用视觉问答。我们的研究着点在于检测这些模型对空间关系的理解能力。既前面的研究通过图像文本匹配（Liu、Emerson和Collier 2022）或视觉问答任务来评估这些模型的表现，都显示了较差的性能和人类表现之间的大差。为了更好地理解这个差距，我们提出了细化的 композиitional 顺序排序方法，并提出了一种底向方法来评估视觉关系逻辑任务的表现。我们将基于对物体和其位置的语言表达grounding的证据来计算最终排名的空间条款。我们在代表性的视力语言模型（Tan和Bansal 2019; Gupta等 2022; Kamath等 2021）上进行了实验，并对这些模型在视觉关系逻辑任务中的表现进行了比较和高亮。
</details></li>
</ul>
<hr>
<h2 id="Time-Series-Predictions-in-Unmonitored-Sites-A-Survey-of-Machine-Learning-Techniques-in-Water-Resources"><a href="#Time-Series-Predictions-in-Unmonitored-Sites-A-Survey-of-Machine-Learning-Techniques-in-Water-Resources" class="headerlink" title="Time Series Predictions in Unmonitored Sites: A Survey of Machine Learning Techniques in Water Resources"></a>Time Series Predictions in Unmonitored Sites: A Survey of Machine Learning Techniques in Water Resources</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09766">http://arxiv.org/abs/2308.09766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jared D. Willard, Charuleka Varadharajan, Xiaowei Jia, Vipin Kumar</li>
<li>For: The paper is written for water resources science, specifically for predicting dynamic environmental variables in unmonitored sites.* Methods: The paper uses modern machine learning methods, such as deep learning frameworks, to predict hydrological variables like river flow and water quality.* Results: The paper reviews state-of-the-art applications of machine learning for streamflow, water quality, and other water resources prediction, and identifies open questions for time series predictions in unmonitored sites, including incorporating dynamic inputs and site characteristics, mechanistic understanding and spatial context, and explainable AI techniques.Here is the same information in Simplified Chinese text:* For: 论文写于水资源科学领域，特点是预测未监测站点的动态环境变量。* Methods: 论文使用现代机器学习方法，如深度学习框架，预测河流流量和水质等ydrological变量。* Results: 论文对水资源预测进行了国际评估，并确定了未监测站点预测时间序列的开放问题，包括包括动态输入和站点特点、机制理解和空间上下文等。<details>
<summary>Abstract</summary>
Prediction of dynamic environmental variables in unmonitored sites remains a long-standing challenge for water resources science. The majority of the world's freshwater resources have inadequate monitoring of critical environmental variables needed for management. Yet, the need to have widespread predictions of hydrological variables such as river flow and water quality has become increasingly urgent due to climate and land use change over the past decades, and their associated impacts on water resources. Modern machine learning methods increasingly outperform their process-based and empirical model counterparts for hydrologic time series prediction with their ability to extract information from large, diverse data sets. We review relevant state-of-the art applications of machine learning for streamflow, water quality, and other water resources prediction and discuss opportunities to improve the use of machine learning with emerging methods for incorporating watershed characteristics into deep learning models, transfer learning, and incorporating process knowledge into machine learning models. The analysis here suggests most prior efforts have been focused on deep learning learning frameworks built on many sites for predictions at daily time scales in the United States, but that comparisons between different classes of machine learning methods are few and inadequate. We identify several open questions for time series predictions in unmonitored sites that include incorporating dynamic inputs and site characteristics, mechanistic understanding and spatial context, and explainable AI techniques in modern machine learning frameworks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>水资源科学中预测无监测站的动态环境变量是一项长期挑战。全球大多数新鲜水资源尚未充分监测关键环境变量，尤其是气候和土地使用变化过去几十年所带来的影响。然而，由于气候和土地使用变化，水资源预测的需求日益增加。现代机器学习方法在水文时序预测方面升级表现，可以从大量多样数据集中提取信息。我们评估了相关的现代应用，包括流量和水质预测，并讨论了将水 shed特征 incorporated into deep learning模型、传输学习和机器学习模型中的进程知识。分析表明，大多数先前努力都集中在了基于多地点的深度学习框架上，但对不同类型机器学习方法的比较 remains limited。我们标识了一些未解决的问题，包括 incorporating 动态输入和站点特征、机制理解和空间上下文，以及现代机器学习框架中的可解释AI技术。
</details></li>
</ul>
<hr>
<h2 id="Taken-by-Surprise-Contrast-effect-for-Similarity-Scores"><a href="#Taken-by-Surprise-Contrast-effect-for-Similarity-Scores" class="headerlink" title="Taken by Surprise: Contrast effect for Similarity Scores"></a>Taken by Surprise: Contrast effect for Similarity Scores</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09765">http://arxiv.org/abs/2308.09765</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/meetelise/surprise-similarity">https://github.com/meetelise/surprise-similarity</a></li>
<li>paper_authors: Thomas C. Bachlechner, Mario Martone, Marjorie Schillo</li>
<li>for: 本研究旨在提出一种基于人类听觉效应的对象向量表示的相似性评价方法，以提高自然语言处理、信息检索和分类任务的性能。</li>
<li>methods: 本研究使用了一种 называ为“surprise score”的ensemble-normalized相似性指标，该指标基于对象向量的分布来衡量对象之间的相似性。</li>
<li>results: 研究发现，使用“surprise score”指标可以在零&#x2F;几shot文档分类任务中提高性能， Typically 10-15% better than raw cosine similarity。<details>
<summary>Abstract</summary>
Accurately evaluating the similarity of object vector embeddings is of critical importance for natural language processing, information retrieval and classification tasks. Popular similarity scores (e.g cosine similarity) are based on pairs of embedding vectors and disregard the distribution of the ensemble from which objects are drawn. Human perception of object similarity significantly depends on the context in which the objects appear. In this work we propose the $\textit{surprise score}$, an ensemble-normalized similarity metric that encapsulates the contrast effect of human perception and significantly improves the classification performance on zero- and few-shot document classification tasks. This score quantifies the surprise to find a given similarity between two elements relative to the pairwise ensemble similarities. We evaluate this metric on zero/few shot classification and clustering tasks and typically find 10-15 % better performance compared to raw cosine similarity. Our code is available at https://github.com/MeetElise/surprise-similarity.
</details>
<details>
<summary>摘要</summary>
精准评估对象vector编码的相似性对自然语言处理、信息检索和分类任务是极为重要的。受欢迎的相似性分数（例如归一化相似性）基于对 embedding vector的对应对，而忽略了对象 ensemble 的分布。人类对对象相似性的识别受到对象出现的 Context 的影响。在这种工作中，我们提议使用 $\textit{surprise score}$，一种ensemble-normalized相似度度量，它体现了人类对对象相似性的冲击效应，并在零/几个shot文档分类任务中显著提高分类性能， Typically 10-15%。我们在这些任务中评估了这个分数，并发现它 Typically 10-15% 比raw cosine similarity better。我们的代码可以在https://github.com/MeetElise/surprise-similarity上找到。
</details></li>
</ul>
<hr>
<h2 id="The-Impact-of-Background-Removal-on-Performance-of-Neural-Networks-for-Fashion-Image-Classification-and-Segmentation"><a href="#The-Impact-of-Background-Removal-on-Performance-of-Neural-Networks-for-Fashion-Image-Classification-and-Segmentation" class="headerlink" title="The Impact of Background Removal on Performance of Neural Networks for Fashion Image Classification and Segmentation"></a>The Impact of Background Removal on Performance of Neural Networks for Fashion Image Classification and Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09764">http://arxiv.org/abs/2308.09764</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junhui Liang, Ying Liu, Vladimir Vlassov</li>
<li>for: 提高时尚图像数据质量和模型性能</li>
<li>methods: 使用突出对象检测来移除背景</li>
<li>results: 可以提高模型准确率达5%，但是深度网络不适合使用Background Removal因为与其他正则化技术不兼容<details>
<summary>Abstract</summary>
Fashion understanding is a hot topic in computer vision, with many applications having great business value in the market. Fashion understanding remains a difficult challenge for computer vision due to the immense diversity of garments and various scenes and backgrounds. In this work, we try removing the background from fashion images to boost data quality and increase model performance. Having fashion images of evident persons in fully visible garments, we can utilize Salient Object Detection to achieve the background removal of fashion data to our expectations. A fashion image with the background removed is claimed as the "rembg" image, contrasting with the original one in the fashion dataset. We conducted extensive comparative experiments with these two types of images on multiple aspects of model training, including model architectures, model initialization, compatibility with other training tricks and data augmentations, and target task types. Our experiments show that background removal can effectively work for fashion data in simple and shallow networks that are not susceptible to overfitting. It can improve model accuracy by up to 5% in the classification on the FashionStyle14 dataset when training models from scratch. However, background removal does not perform well in deep neural networks due to incompatibility with other regularization techniques like batch normalization, pre-trained initialization, and data augmentations introducing randomness. The loss of background pixels invalidates many existing training tricks in the model training, adding the risk of overfitting for deep models.
</details>
<details>
<summary>摘要</summary>
现代时尚理解是计算机视觉领域的热门话题，具有广泛的商业价值。然而，时尚理解仍然是计算机视觉中的挑战，因为衣服的多样性和不同的场景和背景。在这项工作中，我们尝试将时尚图像的背景移除，以提高数据质量并提高模型性能。通过使用突出对象检测来实现背景移除，我们提出了“rembg”图像的概念，与原始时尚数据集的图像进行对比。我们进行了多种比较实验，包括模型架构、模型初始化、与其他训练技巧和数据扩展相容性等方面。我们的实验结果表明，背景移除可以有效地提高时尚数据的模型准确率，但是深度网络中的背景移除不太好，因为它们与批处理常规化、预先初始化和数据扩展引入随机性不兼容。失去背景像素会让许多现有的训练技巧在模型训练中添加随机性，增加深度模型的风险过拟合。
</details></li>
</ul>
<hr>
<h2 id="Data-Compression-and-Inference-in-Cosmology-with-Self-Supervised-Machine-Learning"><a href="#Data-Compression-and-Inference-in-Cosmology-with-Self-Supervised-Machine-Learning" class="headerlink" title="Data Compression and Inference in Cosmology with Self-Supervised Machine Learning"></a>Data Compression and Inference in Cosmology with Self-Supervised Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09751">http://arxiv.org/abs/2308.09751</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aizhanaakhmet/data-compression-inference-in-cosmology-with-ssl">https://github.com/aizhanaakhmet/data-compression-inference-in-cosmology-with-ssl</a></li>
<li>paper_authors: Aizhan Akhmetzhanova, Siddharth Mishra-Sharma, Cora Dvorkin</li>
<li>for:  cosmological surveys 的数据压缩</li>
<li>methods: 使用自动augmentation的自然语言处理技术</li>
<li>results: 可以生成高度信息含量的摘要，用于各种下游任务，如精确参数推导Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written for the purpose of exploring a new method for compressing and analyzing large cosmological datasets.</li>
<li>methods: The paper uses a self-supervised machine learning approach called simulation-based augmentations to construct representative summaries of the data.</li>
<li>results: The method is shown to deliver highly informative summaries that can be used for a variety of downstream tasks, such as precise and accurate parameter inference, and is insensitive to prescribed systematic effects like the influence of baryonic physics.<details>
<summary>Abstract</summary>
The influx of massive amounts of data from current and upcoming cosmological surveys necessitates compression schemes that can efficiently summarize the data with minimal loss of information. We introduce a method that leverages the paradigm of self-supervised machine learning in a novel manner to construct representative summaries of massive datasets using simulation-based augmentations. Deploying the method on hydrodynamical cosmological simulations, we show that it can deliver highly informative summaries, which can be used for a variety of downstream tasks, including precise and accurate parameter inference. We demonstrate how this paradigm can be used to construct summary representations that are insensitive to prescribed systematic effects, such as the influence of baryonic physics. Our results indicate that self-supervised machine learning techniques offer a promising new approach for compression of cosmological data as well its analysis.
</details>
<details>
<summary>摘要</summary>
Current and upcoming cosmological surveys will produce vast amounts of data, making it essential to develop compression schemes that can efficiently summarize the data with minimal loss of information. We propose a method that leverages self-supervised machine learning in a novel way to create representative summaries of massive datasets using simulation-based augmentations. Applying the method to hydrodynamical cosmological simulations, we show that it can produce highly informative summaries that can be used for a variety of downstream tasks, such as precise and accurate parameter inference. Our results suggest that self-supervised machine learning techniques offer a promising new approach for compressing and analyzing cosmological data.Here's the text in Traditional Chinese for comparison:现有和未来的 cosmological surveys 将生成巨量数据，因此需要发展压缩方案，以实现最小化信息损失的数据概要。我们提出了一种方法，利用自动化学习的 Paradigma 在一种新的方式中，创建大量数据的代表概要，使用 simulations 的增强。将方法应用到 hydrodynamical cosmological simulations，我们展示了它可以生成高度有用的概要，可以用于多种下游任务，例如精确和准确的参数推断。我们的结果表明，自动化学习技术对 cosmological data 的压缩和分析提供了一个有前途的新方法。
</details></li>
</ul>
<hr>
<h2 id="Robust-Monocular-Depth-Estimation-under-Challenging-Conditions"><a href="#Robust-Monocular-Depth-Estimation-under-Challenging-Conditions" class="headerlink" title="Robust Monocular Depth Estimation under Challenging Conditions"></a>Robust Monocular Depth Estimation under Challenging Conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09711">http://arxiv.org/abs/2308.09711</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/md4all/md4all">https://github.com/md4all/md4all</a></li>
<li>paper_authors: Stefano Gasperini, Nils Morbitzer, HyunJun Jung, Nassir Navab, Federico Tombari</li>
<li>for: 提高单目深度估计的可靠性，并能在不同的环境和气候条件下工作，包括夜间和雨天等。</li>
<li>methods: 利用现有方法的有效性，生成复杂的样本，并通过自我或全量监督训练，使模型能够在不同的condition下恢复信息。</li>
<li>results: 在nuScenes和Oxford RobotCar等两个公共数据集上进行了广泛的实验，比 Priors works 大幅提高了效果，在标准和挑战性条件下都有出色的表现。<details>
<summary>Abstract</summary>
While state-of-the-art monocular depth estimation approaches achieve impressive results in ideal settings, they are highly unreliable under challenging illumination and weather conditions, such as at nighttime or in the presence of rain. In this paper, we uncover these safety-critical issues and tackle them with md4all: a simple and effective solution that works reliably under both adverse and ideal conditions, as well as for different types of learning supervision. We achieve this by exploiting the efficacy of existing methods under perfect settings. Therefore, we provide valid training signals independently of what is in the input. First, we generate a set of complex samples corresponding to the normal training ones. Then, we train the model by guiding its self- or full-supervision by feeding the generated samples and computing the standard losses on the corresponding original images. Doing so enables a single model to recover information across diverse conditions without modifications at inference time. Extensive experiments on two challenging public datasets, namely nuScenes and Oxford RobotCar, demonstrate the effectiveness of our techniques, outperforming prior works by a large margin in both standard and challenging conditions. Source code and data are available at: https://md4all.github.io.
</details>
<details>
<summary>摘要</summary>
当前的单目深度估计方法在理想的设置下可以获得很好的结果，但在具有挑战性的照明和天气条件下（如夜晚或雨天），这些方法却很不可靠。在这篇论文中，我们揭示了这些安全关键问题并解决了它们，使用md4all：一个简单有效的解决方案，在不同的条件下都可靠地工作，包括理想和挑战性的条件，以及不同类型的学习监督。我们实现了这一点通过利用现有方法在完美的设置下的效果。因此，我们可以在训练时提供有效的训练信号，不受输入内容的限制。首先，我们生成一个包含复杂样本的集合，与常见的训练样本相对应。然后，我们使用这些生成的样本和对原始图像进行标准损失计算来引导模型的自我或全自监督。这样做的原因是，我们可以在推理时不需要对模型进行修改，以便在多种条件下进行推理。我们的技术在nuScenes和Oxford RobotCar两个公共数据集上进行了广泛的实验，并且与之前的工作相比，在标准和挑战性条件下都有很大的进步。代码和数据可以在https://md4all.github.io获取。
</details></li>
</ul>
<hr>
<h2 id="Neural-network-quantum-state-study-of-the-long-range-antiferromagnetic-Ising-chain"><a href="#Neural-network-quantum-state-study-of-the-long-range-antiferromagnetic-Ising-chain" class="headerlink" title="Neural-network quantum state study of the long-range antiferromagnetic Ising chain"></a>Neural-network quantum state study of the long-range antiferromagnetic Ising chain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09709">http://arxiv.org/abs/2308.09709</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jicheol Kim, Dongkyu Kim, Dong-Hee Kim</li>
<li>for:  investigate quantum phase transitions in the transverse field Ising chain with algebraically decaying long-range antiferromagnetic interactions</li>
<li>methods:  use variational Monte Carlo method with restricted Boltzmann machine as trial wave function ansatz</li>
<li>results:  find that central charge deviates from 1&#x2F;2 at small decay exponent $\alpha_\mathrm{LR}$, and identify threshold of Ising universality and conformal symmetry deviation from SR Ising class at $\alpha_\mathrm{LR} &lt; 2$<details>
<summary>Abstract</summary>
We investigate quantum phase transitions in the transverse field Ising chain with algebraically decaying long-range antiferromagnetic interactions by using the variational Monte Carlo method with the restricted Boltzmann machine being employed as a trial wave function ansatz. In the finite-size scaling analysis with the order parameter and the second R\'enyi entropy, we find that the central charge deviates from 1/2 at a small decay exponent $\alpha_\mathrm{LR}$ in contrast to the critical exponents staying very close to the short-range (SR) Ising values regardless of $\alpha_\mathrm{LR}$ examined, supporting the previously proposed scenario of conformal invariance breakdown. To identify the threshold of the Ising universality and the conformal symmetry, we perform two additional tests for the universal Binder ratio and the conformal field theory (CFT) description of the correlation function. It turns out that both indicate a noticeable deviation from the SR Ising class at $\alpha_\mathrm{LR} < 2$. However, a closer look at the scaled correlation function for $\alpha_\mathrm{LR} \ge 2$ shows a gradual change from the asymptotic line of the CFT verified at $\alpha_\mathrm{LR} = 3$, providing a rough estimate of the threshold being in the range of $2 \lesssim \alpha_\mathrm{LR} < 3$.
</details>
<details>
<summary>摘要</summary>
我们研究量子阶段转变在横向Isings链中的数学态度，使用variational Monte Carlo方法和受限 Boltzmann机作为实验波函数构想。在finite-size扩展分析中，我们发现中心 charge在小数字 $\alpha_\text{LR}$ 下异 від 1/2，与短距离Isings值不同，支持之前提出的对称性破坏enario。为了识别Isings universality和对称性的阈值，我们进行了两个额外测试： universal Binder 比率和对称场论 (CFT) 描述的联系函数。结果显示，两个测试都显示了 SR Isings 类型的明显偏离，但是在 $\alpha_\text{LR} \ge 2$ 时，扩展联系函数的涨落趋势逐渐变化为CFT预测的极限线，提供了约 estimate的阈值在 $2 \lesssim \alpha_\text{LR} < 3$ 之间。
</details></li>
</ul>
<hr>
<h2 id="Do-you-know-what-q-means"><a href="#Do-you-know-what-q-means" class="headerlink" title="Do you know what q-means?"></a>Do you know what q-means?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09701">http://arxiv.org/abs/2308.09701</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sfedfcv/redesigned-pancake">https://github.com/Sfedfcv/redesigned-pancake</a></li>
<li>paper_authors: João F. Doriguello, Alessandro Luongo, Ewin Tang</li>
<li>for: 本研究旨在提出一种可以实现高效的分类算法，具体来说是$k$-means算法的一种近似版本。</li>
<li>methods: 本研究使用的方法包括：Lloyd’s iteration algorithm和一种基于QRAM的近似$k$-means算法。</li>
<li>results: 本研究的结果包括：提出了一种基于QRAM的$k$-means算法，该算法可以在$O\big(\frac{k^{2}}{\varepsilon^2}(\sqrt{k}d + \log(Nd))\big)$时间内完成分类任务，同时保持了对$N$的多项幂逻辑依赖。此外，还提出了一种类比的分类算法，即”dequantized”算法，其时间复杂度为$O\big(\frac{k^{2}}{\varepsilon^2}(kd + \log(Nd))\big)$。<details>
<summary>Abstract</summary>
Clustering is one of the most important tools for analysis of large datasets, and perhaps the most popular clustering algorithm is Lloyd's iteration for $k$-means. This iteration takes $N$ vectors $v_1,\dots,v_N\in\mathbb{R}^d$ and outputs $k$ centroids $c_1,\dots,c_k\in\mathbb{R}^d$; these partition the vectors into clusters based on which centroid is closest to a particular vector. We present an overall improved version of the "$q$-means" algorithm, the quantum algorithm originally proposed by Kerenidis, Landman, Luongo, and Prakash (2019) which performs $\varepsilon$-$k$-means, an approximate version of $k$-means clustering. This algorithm does not rely on the quantum linear algebra primitives of prior work, instead only using its QRAM to prepare and measure simple states based on the current iteration's clusters. The time complexity is $O\big(\frac{k^{2}}{\varepsilon^2}(\sqrt{k}d + \log(Nd))\big)$ and maintains the polylogarithmic dependence on $N$ while improving the dependence on most of the other parameters. We also present a "dequantized" algorithm for $\varepsilon$-$k$-means which runs in $O\big(\frac{k^{2}}{\varepsilon^2}(kd + \log(Nd))\big)$ time. Notably, this classical algorithm matches the polylogarithmic dependence on $N$ attained by the quantum algorithms.
</details>
<details>
<summary>摘要</summary>
clustering 是大规模数据分析中最重要的工具之一，而可能最受欢迎的归一化算法之一就是沛降的迭代法（Lloyd's iteration）。这个迭代法会将 N 个坐标vector $v_1, \ldots, v_N \in \mathbb{R}^d$ 转化为 K 个中心点 $c_1, \ldots, c_K \in \mathbb{R}^d$，这些中心点将坐标分成 clusters 基于哪个中心点最近。我们提出了一个全面改进的 "$q$-means" 算法，它是 Kerenidis et al. (2019) 提出的量子算法的应用，用于实现 $\varepsilon$-$k$-means，这是 $k$-means 归一化 clustering 的一个 Approximate 版本。这个算法不依赖于量子线性代数基本操作，而是只使用其 QRAM 来准备和测量基于当前迭代的 clusters 的简单状态。时间复杂度为 $O\big(\frac{k^{2}}{\varepsilon^2}(\sqrt{k}d + \log(Nd))\big)$，这保持了对 N 的多项式依赖性，而改善了大多数其他参数的依赖性。我们还提出了一个 "dequantized" 算法，它在 $O\big(\frac{k^{2}}{\varepsilon^2}(kd + \log(Nd))\big)$ 时间内运行，并且与量子算法具有相同的多项式依赖性。它的时间复杂度与量子算法具有相同的多项式依赖性。
</details></li>
</ul>
<hr>
<h2 id="A-Lightweight-Transformer-for-Faster-and-Robust-EBSD-Data-Collection"><a href="#A-Lightweight-Transformer-for-Faster-and-Robust-EBSD-Data-Collection" class="headerlink" title="A Lightweight Transformer for Faster and Robust EBSD Data Collection"></a>A Lightweight Transformer for Faster and Robust EBSD Data Collection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09693">http://arxiv.org/abs/2308.09693</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hdong920/ebsd_slice_recovery">https://github.com/hdong920/ebsd_slice_recovery</a></li>
<li>paper_authors: Harry Dong, Sean Donegan, Megna Shah, Yuejie Chi</li>
<li>for: 提高三维电子背散射束Diffraction（EBSD）微scopic的数据质量，以便在材料科学中应用。</li>
<li>methods: 使用深度学习模型和投影算法来修复3D EBSD数据中缺失的层。</li>
<li>results: 使用只有自我超vision的synthetic 3D EBSD数据进行训练，在真实3D EBSD数据上获得了比现有方法更高的恢复精度。<details>
<summary>Abstract</summary>
Three dimensional electron back-scattered diffraction (EBSD) microscopy is a critical tool in many applications in materials science, yet its data quality can fluctuate greatly during the arduous collection process, particularly via serial-sectioning. Fortunately, 3D EBSD data is inherently sequential, opening up the opportunity to use transformers, state-of-the-art deep learning architectures that have made breakthroughs in a plethora of domains, for data processing and recovery. To be more robust to errors and accelerate this 3D EBSD data collection, we introduce a two step method that recovers missing slices in an 3D EBSD volume, using an efficient transformer model and a projection algorithm to process the transformer's outputs. Overcoming the computational and practical hurdles of deep learning with scarce high dimensional data, we train this model using only synthetic 3D EBSD data with self-supervision and obtain superior recovery accuracy on real 3D EBSD data, compared to existing methods.
</details>
<details>
<summary>摘要</summary>
三维电子后射扩散Diffraction（EBSD） Mikroskopi是物料科学中多种应用中的重要工具，但它的数据质量可能会在收集过程中变化很大，特别是通过串行sectioning。幸运的是，3D EBSD数据是顺序的，这为使用transformer，当前领域的最先进深度学习架构，进行数据处理和恢复提供了机会。为了更加鲁棒地处理错误和加速3D EBSD数据收集，我们提出了一种两步方法，使用高效的transformer模型和投影算法来处理transformer的输出。通过超越深度学习中的计算和实践障碍，我们使用只有自我超vision的synthetic 3D EBSD数据进行训练，并在实际3D EBSD数据上获得了比现有方法更高的恢复精度。
</details></li>
</ul>
<hr>
<h2 id="Reduced-Order-Modeling-of-a-MOOSE-based-Advanced-Manufacturing-Model-with-Operator-Learning"><a href="#Reduced-Order-Modeling-of-a-MOOSE-based-Advanced-Manufacturing-Model-with-Operator-Learning" class="headerlink" title="Reduced Order Modeling of a MOOSE-based Advanced Manufacturing Model with Operator Learning"></a>Reduced Order Modeling of a MOOSE-based Advanced Manufacturing Model with Operator Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09691">http://arxiv.org/abs/2308.09691</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahmoud Yaseen, Dewen Yushu, Peter German, Xu Wu</li>
<li>for: 这个论文的目的是开发一种快速运行的减少维度模型（ROM），用于在深度学习控制和优化方法中使用。</li>
<li>methods: 这个论文使用了一种基于操作学习（OL）的方法，使用 fourier neural operator 来构建ROM。</li>
<li>results: 这个论文的研究结果表明，OL-based ROM 可以具有更高的准确性和更快的运行速度，相比于传统的深度神经网络基于 ROM。<details>
<summary>Abstract</summary>
Advanced Manufacturing (AM) has gained significant interest in the nuclear community for its potential application on nuclear materials. One challenge is to obtain desired material properties via controlling the manufacturing process during runtime. Intelligent AM based on deep reinforcement learning (DRL) relies on an automated process-level control mechanism to generate optimal design variables and adaptive system settings for improved end-product properties. A high-fidelity thermo-mechanical model for direct energy deposition has recently been developed within the MOOSE framework at the Idaho National Laboratory (INL). The goal of this work is to develop an accurate and fast-running reduced order model (ROM) for this MOOSE-based AM model that can be used in a DRL-based process control and optimization method. Operator learning (OL)-based methods will be employed due to their capability to learn a family of differential equations, in this work, produced by changing process variables in the Gaussian point heat source for the laser. We will develop OL-based ROM using Fourier neural operator, and perform a benchmark comparison of its performance with a conventional deep neural network-based ROM.
</details>
<details>
<summary>摘要</summary>
高等制造（AM）在核子社区中已引起了广泛的关注，因为它在核子材料方面的应用有很大的潜力。一个挑战是通过控制生产过程的 runtime 来实现所需的材料性能。基于深度强化学习（DRL）的智能制造利用了自动化的过程级别控制机制，以生成优化的设计变量和适应系统设置，以提高终产品的性能。在蒙大学（INL）的MOOSE框架中，最近开发了一个高精度的热力学-机械模型，用于直接能量沟入。本工作的目标是开发一个准确和快速运行的减少阶段模型（ROM），用于这个 MOOSE-based AM 模型，以便在 DRL-based 过程控制和优化方法中使用。我们将使用操作学（OL）-based 方法，因为它们可以学习一个家族的差分方程，在这里，由变量改变在泊松点热源中的激光。我们将开发 OL-based ROM 使用整数 ней网络，并对它的性能进行了比较。
</details></li>
</ul>
<hr>
<h2 id="Graph-of-Thoughts-Solving-Elaborate-Problems-with-Large-Language-Models"><a href="#Graph-of-Thoughts-Solving-Elaborate-Problems-with-Large-Language-Models" class="headerlink" title="Graph of Thoughts: Solving Elaborate Problems with Large Language Models"></a>Graph of Thoughts: Solving Elaborate Problems with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09687">http://arxiv.org/abs/2308.09687</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/spcl/graph-of-thoughts">https://github.com/spcl/graph-of-thoughts</a></li>
<li>paper_authors: Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, Torsten Hoefler</li>
<li>for: 本研究旨在提高大语言模型（LLM）的提示能力，并超越链条思维或树思维（ToT）的限制。</li>
<li>methods: 本研究提出了Graph of Thoughts（GoT）框架，可以模型LLM生成的信息为自由图，其中单元为“LLM思维”，带有两个端点的边表示了这些单元之间的依赖关系。这种方法可以将不同的LLM思维组合成衍生新的结果，捕捉整个网络的核心意思，或通过反馈循环进行增强。</li>
<li>results: 研究表明，GoT可以与现状下的task比较，例如排序任务上提高质量62%，同时降低成本&gt;31%。此外，GoT还可以扩展到新的思维变换，因此可以用于开拓新的提示方案。这项工作使得LLM的思维更加接近人类思维或脑内的复杂网络机制。<details>
<summary>Abstract</summary>
We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ("LLM thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62% over ToT, while simultaneously reducing costs by >31%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks.
</details>
<details>
<summary>摘要</summary>
我团队介绍 Graph of Thoughts（GoT）框架：可以超越Chain-of-Thought或Tree of Thoughts（ToT）概念的推导能力。GoT的关键思想和优势在于将LLM生成的信息视为一个任意图，其中单元信息（LLM思想）是顶点，而各个顶点之间的边表示这些思想之间的依赖关系。这种方法可以将不同的LLM思想结合成补做出新的结果，浓缩整个网络的核心意义，或者通过反馈循环进行增强。我们证明GoT在不同任务上比ToT高质量，同时Costs下降了>31%。此外，GoT可以扩展新的思想转换，因此可以用于推导新的思路。这项工作使LLM推导更接近人类思维或脑机制，如 recursivity，两者都形成复杂的网络。
</details></li>
</ul>
<hr>
<h2 id="Audiovisual-Moments-in-Time-A-Large-Scale-Annotated-Dataset-of-Audiovisual-Actions"><a href="#Audiovisual-Moments-in-Time-A-Large-Scale-Annotated-Dataset-of-Audiovisual-Actions" class="headerlink" title="Audiovisual Moments in Time: A Large-Scale Annotated Dataset of Audiovisual Actions"></a>Audiovisual Moments in Time: A Large-Scale Annotated Dataset of Audiovisual Actions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09685">http://arxiv.org/abs/2308.09685</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mjoannou/audiovisual-moments-in-time">https://github.com/mjoannou/audiovisual-moments-in-time</a></li>
<li>paper_authors: Michael Joannou, Pia Rotshtein, Uta Noppeney</li>
<li>for: 本研究创造了一个大规模的 audiovisual moments in time（AVMIT）数据集，用于识别 audiovisual 动作事件。</li>
<li>methods: 研究人员使用了11名参与者进行了一项广泛的标注任务，对 Mit 数据集中的 3 秒 audiovisual 视频进行标注。每个试验都需要参与者确定 audiovisual 动作事件是否存在，以及该事件是视频中最为突出的特征。</li>
<li>results: 研究人员从 inicial 收集的 57,177 个 audiovisual 视频中选择了 16 种 distinct 动作类，并生成了 60 个视频测试集（960 个视频）。此外，研究人员还提供了 2 个预计算 audiovisual 特征嵌入，使用 VGGish&#x2F;YamNet 和 VGG16&#x2F;EfficientNetB0 进行音频和视频数据的预处理。研究人员发现，使用 AVMIT 的标注和特征嵌入可以提高 audiovisual 事件识别的性能。<details>
<summary>Abstract</summary>
We present Audiovisual Moments in Time (AVMIT), a large-scale dataset of audiovisual action events. In an extensive annotation task 11 participants labelled a subset of 3-second audiovisual videos from the Moments in Time dataset (MIT). For each trial, participants assessed whether the labelled audiovisual action event was present and whether it was the most prominent feature of the video. The dataset includes the annotation of 57,177 audiovisual videos, each independently evaluated by 3 of 11 trained participants. From this initial collection, we created a curated test set of 16 distinct action classes, with 60 videos each (960 videos). We also offer 2 sets of pre-computed audiovisual feature embeddings, using VGGish/YamNet for audio data and VGG16/EfficientNetB0 for visual data, thereby lowering the barrier to entry for audiovisual DNN research. We explored the advantages of AVMIT annotations and feature embeddings to improve performance on audiovisual event recognition. A series of 6 Recurrent Neural Networks (RNNs) were trained on either AVMIT-filtered audiovisual events or modality-agnostic events from MIT, and then tested on our audiovisual test set. In all RNNs, top 1 accuracy was increased by 2.71-5.94\% by training exclusively on audiovisual events, even outweighing a three-fold increase in training data. We anticipate that the newly annotated AVMIT dataset will serve as a valuable resource for research and comparative experiments involving computational models and human participants, specifically when addressing research questions where audiovisual correspondence is of critical importance.
</details>
<details>
<summary>摘要</summary>
我们介绍了听视频时刻（AVMIT）数据集，这是一个大规模的听视频动作事件数据集。在一项广泛的注释任务中，11名参与者标注了MIT数据集中的3秒听视频示例。每个试验中，参与者评估了听视频动作事件是否存在，以及它是视频中最为突出的特征。该数据集包括57,177个听视频示例，每个示例由3名训练参与者独立地评估。从这些初始集合中，我们创建了一个精心挑选的测试集，包含16种动作类别，每个类别有60个视频示例（共960个视频）。我们还提供了2个预计算的听视频特征嵌入，使用VGGish/YamNet для音频数据和VGG16/EfficientNetB0 для视频数据，从而降低了听视频DNN研究的门槛。我们利用AVMIT注释和特征嵌入的优势来提高听视频事件认识性能。我们使用6个回归神经网络（RNN）在AVMIT听视频事件或MIT多模态无关事件上训练，然后测试在我们的听视频测试集上。在所有RNN中，测试准确率上升了2.71-5.94%，即使训练数据量只有多样化三倍。我们预计AVMIT数据集会成为研究计算模型和人类参与者之间的重要资源，特别是在研究问题中，听视频协调是关键性的。
</details></li>
</ul>
<hr>
<h2 id="Variational-optimization-of-the-amplitude-of-neural-network-quantum-many-body-ground-states"><a href="#Variational-optimization-of-the-amplitude-of-neural-network-quantum-many-body-ground-states" class="headerlink" title="Variational optimization of the amplitude of neural-network quantum many-body ground states"></a>Variational optimization of the amplitude of neural-network quantum many-body ground states</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09664">http://arxiv.org/abs/2308.09664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jia-Qi Wang, Rong-Qiang He, Zhong-Yi Lu</li>
<li>for: 这个论文目的是找到量子多体系统的凝固态基态，并使用深度学习技术来优化。</li>
<li>methods: 这个论文使用了一种新的方法，即将量子多体系统的变量分解成一个实数valued amplitude神经网络和一个固定的Sign结构，然后优化 amplitude网络。</li>
<li>results: 这个论文在三个典型的量子多体系统上进行了测试，并 obtainted了较低的基态能量，与传统的变量 Monte Carlo（VMC）方法和density matrix renormalization group（DMRG）方法相比。而对于受挫的Heisenberg $J_1$-$J_2$模型，这个论文的结果比文献中的复数valued CNN更好，这表明了 sign structure of complex-valued NQS 难以优化。<details>
<summary>Abstract</summary>
Neural-network quantum states (NQSs), variationally optimized by combining traditional methods and deep learning techniques, is a new way to find quantum many-body ground states and gradually becomes a competitor of traditional variational methods. However, there are still some difficulties in the optimization of NQSs, such as local minima, slow convergence, and sign structure optimization. Here, we split a quantum many-body variational wave function into a multiplication of a real-valued amplitude neural network and a sign structure, and focus on the optimization of the amplitude network while keeping the sign structure fixed. The amplitude network is a convolutional neural network (CNN) with residual blocks, namely a ResNet. Our method is tested on three typical quantum many-body systems. The obtained ground state energies are lower than or comparable to those from traditional variational Monte Carlo (VMC) methods and density matrix renormalization group (DMRG). Surprisingly, for the frustrated Heisenberg $J_1$-$J_2$ model, our results are better than those of the complex-valued CNN in the literature, implying that the sign structure of the complex-valued NQS is difficult to be optimized. We will study the optimization of the sign structure of NQSs in the future.
</details>
<details>
<summary>摘要</summary>
新型神经网络量子状态（NQS），通过结合传统方法和深度学习技术进行变分优化，成为了找寻量子多体底态的新方法，但是仍有一些优化困难，如本地最小值、慢速收敛和正负结构优化。我们将量子多体变量波函数分解为一个实值神经网络幂与一个固定的正负结构，并将焦点放在幂网络的优化上，保持正负结构不变。我们的方法使用了卷积神经网络（CNN） WITH residual块，即ResNet。我们在三个典型量子多体系统上进行测试，获得的底态能量比或等于传统变量 Monte Carlo（VMC）和density matrix renormalization group（DMRG）方法所获得的值更低。 Surprisingly, 对于受挫的Heisenberg-$J_1$-$J_2$模型，我们的结果比文献中的复数值神经网络更好，这 imply That the sign structure of the complex-valued NQS is difficult to be optimized. 我们将在未来研究NQSs的正负结构优化。
</details></li>
</ul>
<hr>
<h2 id="GiGaMAE-Generalizable-Graph-Masked-Autoencoder-via-Collaborative-Latent-Space-Reconstruction"><a href="#GiGaMAE-Generalizable-Graph-Masked-Autoencoder-via-Collaborative-Latent-Space-Reconstruction" class="headerlink" title="GiGaMAE: Generalizable Graph Masked Autoencoder via Collaborative Latent Space Reconstruction"></a>GiGaMAE: Generalizable Graph Masked Autoencoder via Collaborative Latent Space Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09663">http://arxiv.org/abs/2308.09663</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sycny/gigamae">https://github.com/sycny/gigamae</a></li>
<li>paper_authors: Yucheng Shi, Yushun Dong, Qiaoyu Tan, Jundong Li, Ninghao Liu</li>
<li>for: 提高自适应学习模型对图数据的泛化能力</li>
<li>methods: 提出一种图自适应马斯克感知器框架 GiGaMAE，不同于现有的马斯克感知器，我们在这 paper 中提议将共同重建有用和集成的封闭嵌入作为重建目标，以捕捉更广泛的知识。</li>
<li>results: 对三个下游任务和七个数据集进行了广泛的实验，证明 GiGaMAE 在比较难的任务上表现出优于现有基elines。<details>
<summary>Abstract</summary>
Self-supervised learning with masked autoencoders has recently gained popularity for its ability to produce effective image or textual representations, which can be applied to various downstream tasks without retraining. However, we observe that the current masked autoencoder models lack good generalization ability on graph data. To tackle this issue, we propose a novel graph masked autoencoder framework called GiGaMAE. Different from existing masked autoencoders that learn node presentations by explicitly reconstructing the original graph components (e.g., features or edges), in this paper, we propose to collaboratively reconstruct informative and integrated latent embeddings. By considering embeddings encompassing graph topology and attribute information as reconstruction targets, our model could capture more generalized and comprehensive knowledge. Furthermore, we introduce a mutual information based reconstruction loss that enables the effective reconstruction of multiple targets. This learning objective allows us to differentiate between the exclusive knowledge learned from a single target and common knowledge shared by multiple targets. We evaluate our method on three downstream tasks with seven datasets as benchmarks. Extensive experiments demonstrate the superiority of GiGaMAE against state-of-the-art baselines. We hope our results will shed light on the design of foundation models on graph-structured data. Our code is available at: https://github.com/sycny/GiGaMAE.
</details>
<details>
<summary>摘要</summary>
自顾学学习掌握到掩码自适应器的能力，可以生成有效的图像或文本表示，可以应用于多个下游任务无需重新训练。然而，我们发现当前的掩码自适应器模型在图数据上的泛化能力不佳。为解决这个问题，我们提出了一个新的图自适应器框架 called GiGaMAE。与现有的掩码自适应器模型不同，我们在这篇论文中提议使用共同重构有用和完整的嵌入码。我们考虑嵌入码包括图格结构和属性信息作为重构目标，这使我们的模型可以捕捉更加普遍和全面的知识。此外，我们引入了互信息基于的重建损失，这使得我们的模型可以有效地重建多个目标。我们在三个下游任务上使用七个数据集进行评估。广泛的实验表明GiGaMAE比 estado-of-the-art 基线模型更高效。我们希望我们的结果可以指导基本模型的设计在图结构数据上。我们的代码可以在 GitHub 上找到：https://github.com/sycny/GiGaMAE。
</details></li>
</ul>
<hr>
<h2 id="Robust-Uncertainty-Quantification-using-Conformalised-Monte-Carlo-Prediction"><a href="#Robust-Uncertainty-Quantification-using-Conformalised-Monte-Carlo-Prediction" class="headerlink" title="Robust Uncertainty Quantification using Conformalised Monte Carlo Prediction"></a>Robust Uncertainty Quantification using Conformalised Monte Carlo Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09647">http://arxiv.org/abs/2308.09647</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/team-daniel/mc-cp">https://github.com/team-daniel/mc-cp</a></li>
<li>paper_authors: Daniel Bethell, Simos Gerasimou, Radu Calinescu</li>
<li>for: 这篇研究旨在提供一个新的深度学习模型评估方法，以提高深度学习模型在安全敏感应用中的可靠性。</li>
<li>methods: 这篇研究使用了一种新的混合MC-CP方法，它结合了适应MC dropout方法和整构预测方法，以提高深度学习模型的不确定性评估。</li>
<li>results: 经过了严格的实验评估，这篇研究发现MC-CP方法可以实现深度学习模型的更好的可靠性和精度，并且可以轻松地添加到现有的模型中，使其更加简单地应用。<details>
<summary>Abstract</summary>
Deploying deep learning models in safety-critical applications remains a very challenging task, mandating the provision of assurances for the dependable operation of these models. Uncertainty quantification (UQ) methods estimate the model's confidence per prediction, informing decision-making by considering the effect of randomness and model misspecification. Despite the advances of state-of-the-art UQ methods, they are computationally expensive or produce conservative prediction sets/intervals. We introduce MC-CP, a novel hybrid UQ method that combines a new adaptive Monte Carlo (MC) dropout method with conformal prediction (CP). MC-CP adaptively modulates the traditional MC dropout at runtime to save memory and computation resources, enabling predictions to be consumed by CP, yielding robust prediction sets/intervals. Throughout comprehensive experiments, we show that MC-CP delivers significant improvements over advanced UQ methods, like MC dropout, RAPS and CQR, both in classification and regression benchmarks. MC-CP can be easily added to existing models, making its deployment simple.
</details>
<details>
<summary>摘要</summary>
部署深度学习模型在安全关键应用中仍然是一项非常具有挑战性的任务，需要提供对模型可靠性的保证。不约束量评估（UQ）方法可以估算模型每个预测的可信度，为决策做出考虑，考虑随机性和模型误差的效果。despite the advances of state-of-the-art UQ methods, they are computationally expensive or produce conservative prediction sets/intervals. 我们介绍MC-CP，一种新的混合MC dropout和CP（确定预测）方法。MC-CP在运行时动态调整传统MC dropout，以避免过度使用内存和计算资源，使预测可以被CP所使用，生成Robust预测集/区间。通过广泛的实验，我们表明MC-CP在分类和回归 benchmark上都能够实现显著改进，比如MC dropout、RAPS和CQR。MC-CP可以轻松地添加到现有模型中，使其部署简单。
</details></li>
</ul>
<hr>
<h2 id="biquality-learn-a-Python-library-for-Biquality-Learning"><a href="#biquality-learn-a-Python-library-for-Biquality-Learning" class="headerlink" title="biquality-learn: a Python library for Biquality Learning"></a>biquality-learn: a Python library for Biquality Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09643">http://arxiv.org/abs/2308.09643</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/biquality-learn/biquality-learn">https://github.com/biquality-learn/biquality-learn</a></li>
<li>paper_authors: Pierre Nodet, Vincent Lemaire, Alexis Bondu, Antoine Cornuéjols</li>
<li>For: The paper is written for practitioners and researchers who need to handle weak supervision and dataset shifts in machine learning, and who want to use a Python library for Biquality Learning.* Methods: The paper proposes a machine learning framework called Biquality Learning, which can handle multiple weaknesses of supervision and dataset shifts without assumptions on their nature and level. The proposed library, biquality-learn, provides an intuitive and consistent API for learning machine learning models from biquality data.* Results: The paper does not present specific results, but rather proposes a new machine learning framework and library for handling weak supervision and dataset shifts. The proposed library, biquality-learn, is designed to be accessible and easy to use for everyone, and enables researchers to experiment in a reproducible way on biquality data.<details>
<summary>Abstract</summary>
The democratization of Data Mining has been widely successful thanks in part to powerful and easy-to-use Machine Learning libraries. These libraries have been particularly tailored to tackle Supervised Learning. However, strong supervision signals are scarce in practice, and practitioners must resort to weak supervision. In addition to weaknesses of supervision, dataset shifts are another kind of phenomenon that occurs when deploying machine learning models in the real world. That is why Biquality Learning has been proposed as a machine learning framework to design algorithms capable of handling multiple weaknesses of supervision and dataset shifts without assumptions on their nature and level by relying on the availability of a small trusted dataset composed of cleanly labeled and representative samples. Thus we propose biquality-learn: a Python library for Biquality Learning with an intuitive and consistent API to learn machine learning models from biquality data, with well-proven algorithms, accessible and easy to use for everyone, and enabling researchers to experiment in a reproducible way on biquality data.
</details>
<details>
<summary>摘要</summary>
“数据挖掘的民主化得到了广泛的成功，很大的准确是归功于强大且易于使用的机器学习库。这些库具有特别地针对超级vised学习。然而，实际中强制监督信号强度很弱，实践者必须采用弱监督。此外，在部署机器学习模型时，数据Shift是一种常见的现象。因此，我们提出了Biquality学习框架，设计用于处理多种弱监督和数据Shift的算法，不假设其性质和水平。Biquality学习库提供了一个小型可信的数据集，包含清晰标注和代表性样本，并提供了一个intuitive和一致的API，让人们轻松地学习机器学习模型从biquality数据，并且具有证明的算法、易于使用和可重复地进行研究。”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know and I can provide that as well.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/19/cs.LG_2023_08_19/" data-id="clluro5k60069q9884qlxhr2z" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/20/eess.IV_2023_08_20/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.IV - 2023-08-20 17:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/19/cs.SD_2023_08_19/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-08-19 123:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
