
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-08-03 17:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Maximum-likelihood estimation in ptychography in the presence of Poisson-Gaussian noise statistics paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.02436 repo_url: None paper_authors: Jacob Seifert, Yifeng Shao,">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-08-03 17:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/03/eess.IV_2023_08_03/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Maximum-likelihood estimation in ptychography in the presence of Poisson-Gaussian noise statistics paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.02436 repo_url: None paper_authors: Jacob Seifert, Yifeng Shao,">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-02T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:43.354Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_08_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/03/eess.IV_2023_08_03/" class="article-date">
  <time datetime="2023-08-02T16:00:00.000Z" itemprop="datePublished">2023-08-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-08-03 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Maximum-likelihood-estimation-in-ptychography-in-the-presence-of-Poisson-Gaussian-noise-statistics"><a href="#Maximum-likelihood-estimation-in-ptychography-in-the-presence-of-Poisson-Gaussian-noise-statistics" class="headerlink" title="Maximum-likelihood estimation in ptychography in the presence of Poisson-Gaussian noise statistics"></a>Maximum-likelihood estimation in ptychography in the presence of Poisson-Gaussian noise statistics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02436">http://arxiv.org/abs/2308.02436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacob Seifert, Yifeng Shao, Rens van Dam, Dorian Bouchet, Tristan van Leeuwen, Allard P. Mosk</li>
<li>for: 提高图像质量，特别是在低信号强度（SNR）下。</li>
<li>methods: 利用最大可能性估计，在梯度基于ptychography优化中考虑camera读取噪声。</li>
<li>results: 通过实验和数值数据，我们的方法比传统方法有更高的图像重建质量，并且可以在低SNR下提高图像质量。<details>
<summary>Abstract</summary>
Optical measurements often exhibit mixed Poisson-Gaussian noise statistics, which hampers image quality, particularly under low signal-to-noise ratio (SNR) conditions. Computational imaging falls short in such situations when solely Poissonian noise statistics are assumed. In response to this challenge, we define a loss function that explicitly incorporates this mixed noise nature. By using maximum-likelihood estimation, we devise a practical method to account for camera readout noise in gradient-based ptychography optimization. Our results, based on both experimental and numerical data, demonstrate that this approach outperforms the conventional one, enabling enhanced image reconstruction quality under challenging noise conditions through a straightforward methodological adjustment.
</details>
<details>
<summary>摘要</summary>
光学测量经常表现出杂合波尔兹-加布拉斯噪声统计，这会影响图像质量，特别是在低信号噪声比（SNR）条件下。计算成像在这些情况下失败，因为它假设了单一的波尔兹噪声统计。为解决这个挑战，我们定义了一个损失函数，其直接表达混合噪声的 NATURE。通过最大likelihood估计，我们开发了一种实用的方法，用于在梯度基本ptychography优化中考虑相机读取噪声。我们的结果，基于实验和数值数据，表明这种方法在具有挑战性的噪声条件下表现出色，可以通过简单的方法调整来提高图像重建质量。
</details></li>
</ul>
<hr>
<h2 id="Focus-on-Content-not-Noise-Improving-Image-Generation-for-Nuclei-Segmentation-by-Suppressing-Steganography-in-CycleGAN"><a href="#Focus-on-Content-not-Noise-Improving-Image-Generation-for-Nuclei-Segmentation-by-Suppressing-Steganography-in-CycleGAN" class="headerlink" title="Focus on Content not Noise: Improving Image Generation for Nuclei Segmentation by Suppressing Steganography in CycleGAN"></a>Focus on Content not Noise: Improving Image Generation for Nuclei Segmentation by Suppressing Steganography in CycleGAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01769">http://arxiv.org/abs/2308.01769</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonas Utz, Tobias Weise, Maja Schlereth, Fabian Wagner, Mareike Thies, Mingxuan Gu, Stefan Uderhardt, Katharina Breininger</li>
<li>for: 这个论文是为了提高投影机微scopy图像中的核体分割任务而准备的。</li>
<li>methods: 这个论文使用的方法是使用CycleGAN生成器 inverse microscopy图像，并使用低通过滤波器基于DCT来移除生成图像中的隐藏快捷信息。</li>
<li>results: 这个论文的结果显示，通过移除隐藏快捷信息，可以提高生成图像和цикли图像之间的准确性，并提高核体分割任务的F1分数。<details>
<summary>Abstract</summary>
Annotating nuclei in microscopy images for the training of neural networks is a laborious task that requires expert knowledge and suffers from inter- and intra-rater variability, especially in fluorescence microscopy. Generative networks such as CycleGAN can inverse the process and generate synthetic microscopy images for a given mask, thereby building a synthetic dataset. However, past works report content inconsistencies between the mask and generated image, partially due to CycleGAN minimizing its loss by hiding shortcut information for the image reconstruction in high frequencies rather than encoding the desired image content and learning the target task. In this work, we propose to remove the hidden shortcut information, called steganography, from generated images by employing a low pass filtering based on the DCT. We show that this increases coherence between generated images and cycled masks and evaluate synthetic datasets on a downstream nuclei segmentation task. Here we achieve an improvement of 5.4 percentage points in the F1-score compared to a vanilla CycleGAN. Integrating advanced regularization techniques into the CycleGAN architecture may help mitigate steganography-related issues and produce more accurate synthetic datasets for nuclei segmentation.
</details>
<details>
<summary>摘要</summary>
描述核在微scopic图像中标注是一项劳动密集的任务，需要专业知识和受人类评估的变化，特别是在染色微scopic图像中。生成网络如CycleGAN可以反转过程，生成基于给定的mask的假图像，并建立一个假数据集。然而，过去的工作发现，生成图像中的内容与mask之间存在不一致，这部分是因为CycleGAN寻找短cut减少高频信息，而不是编码愿意图像内容和学习目标任务。在这项工作中，我们提议从生成图像中除掉隐藏的短cut信息（即steganography），使用基于DCT的低通滤波器。我们发现，这会提高生成图像和cycled mask之间的协调性，并评估假数据集在下游核 segmentation任务上的性能。在这里，我们实现了与vanilla CycleGAN相比的5.4个百分点的F1分数提升。将进降常规正则化技术integrated into CycleGAN architecture可能可以减少steganography相关的问题，生成更准确的假数据集 для核 segmentation。
</details></li>
</ul>
<hr>
<h2 id="NuInsSeg-A-Fully-Annotated-Dataset-for-Nuclei-Instance-Segmentation-in-H-E-Stained-Histological-Images"><a href="#NuInsSeg-A-Fully-Annotated-Dataset-for-Nuclei-Instance-Segmentation-in-H-E-Stained-Histological-Images" class="headerlink" title="NuInsSeg: A Fully Annotated Dataset for Nuclei Instance Segmentation in H&amp;E-Stained Histological Images"></a>NuInsSeg: A Fully Annotated Dataset for Nuclei Instance Segmentation in H&amp;E-Stained Histological Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01760">http://arxiv.org/abs/2308.01760</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/masih4/nuinsseg">https://github.com/masih4/nuinsseg</a></li>
<li>paper_authors: Amirreza Mahbod, Christine Polak, Katharina Feldmann, Rumsha Khan, Katharina Gelles, Georg Dorffner, Ramona Woitek, Sepideh Hatamikia, Isabella Ellinger</li>
<li>for:  automatic nuclei instance segmentation in whole slide image analysis</li>
<li>methods:  supervised deep learning (DL) methods with fully manually annotated datasets</li>
<li>results:  release of one of the biggest fully manually annotated datasets of nuclei in Hematoxylin and Eosin (H&amp;E)-stained histological images, called NuInsSeg, with additional ambiguous area masks for the entire dataset.<details>
<summary>Abstract</summary>
In computational pathology, automatic nuclei instance segmentation plays an essential role in whole slide image analysis. While many computerized approaches have been proposed for this task, supervised deep learning (DL) methods have shown superior segmentation performances compared to classical machine learning and image processing techniques. However, these models need fully annotated datasets for training which is challenging to acquire, especially in the medical domain. In this work, we release one of the biggest fully manually annotated datasets of nuclei in Hematoxylin and Eosin (H&E)-stained histological images, called NuInsSeg. This dataset contains 665 image patches with more than 30,000 manually segmented nuclei from 31 human and mouse organs. Moreover, for the first time, we provide additional ambiguous area masks for the entire dataset. These vague areas represent the parts of the images where precise and deterministic manual annotations are impossible, even for human experts. The dataset and detailed step-by-step instructions to generate related segmentation masks are publicly available at https://www.kaggle.com/datasets/ipateam/nuinsseg and https://github.com/masih4/NuInsSeg, respectively.
</details>
<details>
<summary>摘要</summary>
Computational pathology 中，自动核体实例分割扮演着重要的角色，整体扫描图像分析中。虽然许多计算机化的方法已经被提出，但是超级深度学习（DL）方法在这个任务中显示出了更高的分割性能，相比于经典机器学习和图像处理技术。然而，这些模型需要完全注释的数据集进行训练，特别是在医疗领域中，这是一个具有挑战性的任务。在这项工作中，我们释放了一个包含665个图像块，共计超过30,000个手动注释的核体的大数据集，称为NuInsSeg。这个数据集包括31种人类和小鼠的组织，以及每个组织的多个图像块。此外，我们还提供了整个数据集的抽象区域mask，这些抽象区域表示图像中具有不确定性的部分，即人类专家无法在这些部分提供准确的手动注释。该数据集和相关的分割步骤说明可以在https://www.kaggle.com/datasets/ipateam/nuinsseg和https://github.com/masih4/NuInsSeg中下载。
</details></li>
</ul>
<hr>
<h2 id="Reference-Free-Isotropic-3D-EM-Reconstruction-using-Diffusion-Models"><a href="#Reference-Free-Isotropic-3D-EM-Reconstruction-using-Diffusion-Models" class="headerlink" title="Reference-Free Isotropic 3D EM Reconstruction using Diffusion Models"></a>Reference-Free Isotropic 3D EM Reconstruction using Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01594">http://arxiv.org/abs/2308.01594</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kyungryun Lee, Won-Ki Jeong</li>
<li>for: 这个论文是为了解决电子顾微显微镜像中的扩散率差异问题，提高分析和下游任务的精度和效率。</li>
<li>methods: 该方法基于二维扩散模型，可以一直重建3D体积，并且适用于高下采样的数据。</li>
<li>results: 对两个公共数据集进行了广泛的实验，并证明了利用生成模型比超级vised学习方法更加稳定和有效。此外，还示出了该方法的自动化重建可能性，可以无需任何训练数据来重建单个不同的体积。<details>
<summary>Abstract</summary>
Electron microscopy (EM) images exhibit anisotropic axial resolution due to the characteristics inherent to the imaging modality, presenting challenges in analysis and downstream tasks.In this paper, we propose a diffusion-model-based framework that overcomes the limitations of requiring reference data or prior knowledge about the degradation process. Our approach utilizes 2D diffusion models to consistently reconstruct 3D volumes and is well-suited for highly downsampled data. Extensive experiments conducted on two public datasets demonstrate the robustness and superiority of leveraging the generative prior compared to supervised learning methods. Additionally, we demonstrate our method's feasibility for self-supervised reconstruction, which can restore a single anisotropic volume without any training data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DMDC-Dynamic-mask-based-dual-camera-design-for-snapshot-Hyperspectral-Imaging"><a href="#DMDC-Dynamic-mask-based-dual-camera-design-for-snapshot-Hyperspectral-Imaging" class="headerlink" title="DMDC: Dynamic-mask-based dual camera design for snapshot Hyperspectral Imaging"></a>DMDC: Dynamic-mask-based dual camera design for snapshot Hyperspectral Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01541">http://arxiv.org/abs/2308.01541</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/caizeyu1992/dmdc">https://github.com/caizeyu1992/dmdc</a></li>
<li>paper_authors: Zeyu Cai, Chengqian Jin, Feipeng Da</li>
<li>for: 提高coded aperture snapshot spectral imaging（CASSI）中深度学习方法的性能。</li>
<li>methods: 使用动态Mask基于双摄像头系统，首先从RGB图像中学习场景的空间特征分布，然后使用SLM编码场景，最后将RGB和CASSI图像传给网络进行重建。设计了DMDC-net，包括一个小规模的CNN基于动态Mask网络和多模式重建网络。</li>
<li>results: 对多个数据集进行了广泛的实验，结果表明我们的方法可以与最佳状态提高超过9dB的PSNR。<details>
<summary>Abstract</summary>
Deep learning methods are developing rapidly in coded aperture snapshot spectral imaging (CASSI). The number of parameters and FLOPs of existing state-of-the-art methods (SOTA) continues to increase, but the reconstruction accuracy improves slowly. Current methods still face two problems: 1) The performance of the spatial light modulator (SLM) is not fully developed due to the limitation of fixed Mask coding. 2) The single input limits the network performance. In this paper we present a dynamic-mask-based dual camera system, which consists of an RGB camera and a CASSI system running in parallel. First, the system learns the spatial feature distribution of the scene based on the RGB images, then instructs the SLM to encode each scene, and finally sends both RGB and CASSI images to the network for reconstruction. We further designed the DMDC-net, which consists of two separate networks, a small-scale CNN-based dynamic mask network for dynamic adjustment of the mask and a multimodal reconstruction network for reconstruction using RGB and CASSI measurements. Extensive experiments on multiple datasets show that our method achieves more than 9 dB improvement in PSNR over the SOTA. (https://github.com/caizeyu1992/DMDC)
</details>
<details>
<summary>摘要</summary>
深度学习方法在干扰窗口Snapshot спектраль成像（CASSI）领域不断发展。现有State-of-the-art方法（SOTA）中参数和FLOPs的数量继续增加，但重建精度变化缓慢。现有方法仍面临两个问题：1）SLM的性能尚未得到完全发挥，归因于固定的Mask编码限制。2）单输入限制网络性能。在本文中，我们提出了一种基于动态面罩的双相机系统，该系统由一个RGB相机和一个CASSI系统在并行运行。首先，系统根据RGB图像学习场景的空间特征分布，然后通过SLM编码场景，并将RGB和CASSI图像传递给网络进行重建。我们还设计了DMDC-net，它由两个独立网络组成：一个小规模的CNN基于动态面罩网络用于动态调整面罩，以及一个多模式重建网络用于使用RGB和CASSI测量进行重建。我们对多个数据集进行了广泛的实验，结果显示，我们的方法可以与SOTA比进行9dB以上的PSNR提升。（https://github.com/caizeyu1992/DMDC）
</details></li>
</ul>
<hr>
<h2 id="Numerical-Uncertainty-of-Convolutional-Neural-Networks-Inference-for-Structural-Brain-MRI-Analysis"><a href="#Numerical-Uncertainty-of-Convolutional-Neural-Networks-Inference-for-Structural-Brain-MRI-Analysis" class="headerlink" title="Numerical Uncertainty of Convolutional Neural Networks Inference for Structural Brain MRI Analysis"></a>Numerical Uncertainty of Convolutional Neural Networks Inference for Structural Brain MRI Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01939">http://arxiv.org/abs/2308.01939</a></li>
<li>repo_url: None</li>
<li>paper_authors: Inés Gonzalez Pepe, Vinuyan Sivakolunthu, Hae Lang Park, Yohan Chatelain, Tristan Glatard</li>
<li>for: 这 paper 探讨了 Convolutional Neural Networks (CNNs) 推理过程中的数字不确定性，用于Structural Brain MRI 分析。</li>
<li>methods: 这 paper 使用 Random Rounding 方法，对 CNN 模型在非线性调整 (SynthMorph) 和整个大脑 segmentation (FastSurfer) 中使用。</li>
<li>results: 结果表明，CNN 预测比传统图像处理结果更加精确（非线性调整：19 vs 13 重要位数的平均值；整个大脑 segmentation：0.99 vs 0.92 S{\o}rensen-Dice 分数的平均值），这表明 CNN 结果在执行环境中更加重复性。<details>
<summary>Abstract</summary>
This paper investigates the numerical uncertainty of Convolutional Neural Networks (CNNs) inference for structural brain MRI analysis. It applies Random Rounding -- a stochastic arithmetic technique -- to CNN models employed in non-linear registration (SynthMorph) and whole-brain segmentation (FastSurfer), and compares the resulting numerical uncertainty to the one measured in a reference image-processing pipeline (FreeSurfer recon-all). Results obtained on 32 representative subjects show that CNN predictions are substantially more accurate numerically than traditional image-processing results (non-linear registration: 19 vs 13 significant bits on average; whole-brain segmentation: 0.99 vs 0.92 S{\o}rensen-Dice score on average), which suggests a better reproducibility of CNN results across execution environments.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="TDMD-A-Database-for-Dynamic-Color-Mesh-Subjective-and-Objective-Quality-Explorations"><a href="#TDMD-A-Database-for-Dynamic-Color-Mesh-Subjective-and-Objective-Quality-Explorations" class="headerlink" title="TDMD: A Database for Dynamic Color Mesh Subjective and Objective Quality Explorations"></a>TDMD: A Database for Dynamic Color Mesh Subjective and Objective Quality Explorations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01499">http://arxiv.org/abs/2308.01499</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qi Yang, Joel Jung, Timon Deschamps, Xiaozhong Xu, Shan Liu<br>for:The paper aims to create a database of distorted dynamic colored meshes (DCMs) to study the impact of distortions on human perception and to develop objective metrics for evaluating the quality of DCMs.methods:The authors use a large-scale subjective experiment to collect mean opinion scores for 303 distorted DCM samples, and they evaluate three types of state-of-the-art objective metrics on the database.results:The authors find that different types of distortion have varying effects on human perception, and they provide recommendations for selecting objective metrics in practical DCM applications. They also make the TDMD database publicly available for future research.<details>
<summary>Abstract</summary>
Dynamic colored meshes (DCM) are widely used in various applications; however, these meshes may undergo different processes, such as compression or transmission, which can distort them and degrade their quality. To facilitate the development of objective metrics for DCMs and study the influence of typical distortions on their perception, we create the Tencent - dynamic colored mesh database (TDMD) containing eight reference DCM objects with six typical distortions. Using processed video sequences (PVS) derived from the DCM, we have conducted a large-scale subjective experiment that resulted in 303 distorted DCM samples with mean opinion scores, making the TDMD the largest available DCM database to our knowledge. This database enabled us to study the impact of different types of distortion on human perception and offer recommendations for DCM compression and related tasks. Additionally, we have evaluated three types of state-of-the-art objective metrics on the TDMD, including image-based, point-based, and video-based metrics, on the TDMD. Our experimental results highlight the strengths and weaknesses of each metric, and we provide suggestions about the selection of metrics in practical DCM applications. The TDMD will be made publicly available at the following location: https://multimedia.tencent.com/resources/tdmd.
</details>
<details>
<summary>摘要</summary>
“弹性颜色网 mesh”（DCM）在多个应用中广泛使用，但这些网可能会经历不同的处理程序，如压缩或传输，这可能会使其变形和降低质量。为了促进DCM的开发和研究，我们创建了“天聪”- dynamic colored mesh database（TDMD），包含8个参考DCM物件，以6种常见的变形作为试验材料。我们透过从DCM derivated的处理影像序列（PVS）进行了大规模的主观实验，产生了303个扭曲DCM样品，并记录了它们的主观评分，从而使TDMD成为我们所知道的最大的DCM数据库。这个数据库允许我们研究不同类型的变形对人类感知的影响，并提供了适用于DCM压缩和相关任务的建议。此外，我们也评估了三种现有的州Of-the-art物理指标，包括影像基于、点基于和影像基于的指标，在TDMD上。我们的实验结果显示了这些指标的优点和缺点，并提供了实际应用中选择指标的建议。TDMD将在以下位置公开：https://multimedia.tencent.com/resources/tdmd。”
</details></li>
</ul>
<hr>
<h2 id="Estimation-of-motion-blur-kernel-parameters-using-regression-convolutional-neural-networks"><a href="#Estimation-of-motion-blur-kernel-parameters-using-regression-convolutional-neural-networks" class="headerlink" title="Estimation of motion blur kernel parameters using regression convolutional neural networks"></a>Estimation of motion blur kernel parameters using regression convolutional neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01381">http://arxiv.org/abs/2308.01381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luis G. Varela, Laura E. Boucheron, Steven Sandoval, David Voelz, Abu Bucker Siddik</li>
<li>for: 本文针对 Linear Motion Blur 预测方法进行提出，将 Linear Motion Blur 视为一种条件对称噪声，并使用对应的 Neural Network 进行预测。</li>
<li>methods: 本文使用的方法是基于 Neural Network 的回推预测方法，将 Linear Motion Blur 的长度和方向转换为 Parameters，并使用这些 Parameters 来预测 Linear Motion Blur 预测器。</li>
<li>results: 本文透过分析 Linear Motion Blur 的关系性，实现了对 Uniformed Motion Blur 图像的预测。这些预测结果可以帮助对 Linear Motion Blur 进行更好的处理和修复。<details>
<summary>Abstract</summary>
Many deblurring and blur kernel estimation methods use MAP or classification deep learning techniques to sharpen an image and predict the blur kernel. We propose a regression approach using neural networks to predict the parameters of linear motion blur kernels. These kernels can be parameterized by its length of blur and the orientation of the blur.This paper will analyze the relationship between length and angle of linear motion blur. This analysis will help establish a foundation to using regression prediction in uniformed motion blur images.
</details>
<details>
<summary>摘要</summary>
很多锐化和抖镜元数估计方法使用MAP或分类深度学习技术来锐化图像并预测抖镜元数。我们提议使用回归方法使用神经网络预测线性运动抖镜元数的参数。这些参数可以 Parameterized by its length of blur and the orientation of the blur。这篇论文将分析线性运动抖镜的关系，以建立基础 для使用回归预测在均匀运动抖镜图像中。
</details></li>
</ul>
<hr>
<h2 id="ELIXR-Towards-a-general-purpose-X-ray-artificial-intelligence-system-through-alignment-of-large-language-models-and-radiology-vision-encoders"><a href="#ELIXR-Towards-a-general-purpose-X-ray-artificial-intelligence-system-through-alignment-of-large-language-models-and-radiology-vision-encoders" class="headerlink" title="ELIXR: Towards a general purpose X-ray artificial intelligence system through alignment of large language models and radiology vision encoders"></a>ELIXR: Towards a general purpose X-ray artificial intelligence system through alignment of large language models and radiology vision encoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01317">http://arxiv.org/abs/2308.01317</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shawn Xu, Lin Yang, Christopher Kelly, Marcin Sieniek, Timo Kohlberger, Martin Ma, Wei-Hung Weng, Attila Kiraly, Sahar Kazemzadeh, Zakkai Melamed, Jungyeon Park, Patricia Strachan, Yun Liu, Chuck Lau, Preeti Singh, Christina Chen, Mozziyar Etemadi, Sreenivasa Raju Kalidindi, Yossi Matias, Katherine Chou, Greg S. Corrado, Shravya Shetty, Daniel Tse, Shruthi Prabhakara, Daniel Golden, Rory Pilgrim, Krish Eswaran, Andrew Sellergren</li>
<li>for: 这个研究旨在开发一个名为ELIXR的语言&#x2F;图像测合模型，用于进行各种医疗影像处理任务。</li>
<li>methods: 这个模型使用了一个语言aligned图像Encoder，与固定的LLM PaLM 2结合，并通过对MIMIC-CXR dataset中的图像和相应的自由文本医疗报告进行训练。</li>
<li>results: ELIXR在零扩展骨臓X射线（CXR）分类（平均AUC为0.850）、数据效率CXR分类（平均AUCs为0.893和0.898）以及semantic搜寻（NDCG为0.76）等方面均 achieve state-of-the-art performance，并且在医疗影像语言任务中也表现良好。<details>
<summary>Abstract</summary>
Our approach, which we call Embeddings for Language/Image-aligned X-Rays, or ELIXR, leverages a language-aligned image encoder combined or grafted onto a fixed LLM, PaLM 2, to perform a broad range of tasks. We train this lightweight adapter architecture using images paired with corresponding free-text radiology reports from the MIMIC-CXR dataset. ELIXR achieved state-of-the-art performance on zero-shot chest X-ray (CXR) classification (mean AUC of 0.850 across 13 findings), data-efficient CXR classification (mean AUCs of 0.893 and 0.898 across five findings (atelectasis, cardiomegaly, consolidation, pleural effusion, and pulmonary edema) for 1% (~2,200 images) and 10% (~22,000 images) training data), and semantic search (0.76 normalized discounted cumulative gain (NDCG) across nineteen queries, including perfect retrieval on twelve of them). Compared to existing data-efficient methods including supervised contrastive learning (SupCon), ELIXR required two orders of magnitude less data to reach similar performance. ELIXR also showed promise on CXR vision-language tasks, demonstrating overall accuracies of 58.7% and 62.5% on visual question answering and report quality assurance tasks, respectively. These results suggest that ELIXR is a robust and versatile approach to CXR AI.
</details>
<details>
<summary>摘要</summary>
我们的方法，我们称之为语言/图像aligned X-射线（ELIXR），利用一个语言对应的图像编码器与固定的LLM（PaLM 2）结合，以实现广泛的任务。我们在使用图像和相应的自由文本 radiology report 从 MIMIC-CXR 数据集进行训练这个轻量级适配器建筑。ELIXR 在零 shot 胸部 X-射线（CXR）分类中获得了状态机器人表现（平均 AUC 为 0.850  across 13 个发现），数据效率 CXR 分类（平均 AUCs 为 0.893 和 0.898  across 五 个发现（肿瘤、心脏 hypertrophy、填充、肿水和肺水肿） for 1% （约 2,200 张图像）和 10% （约 22,000 张图像） 训练数据），以及semantic search（NDCG 为 0.76）。相比现有的数据效率方法，包括 supervised contrastive learning （SupCon），ELIXR 需要两个数据效率下来到达相似的性能。ELIXR 还在 CXR vision-language 任务上显示了抢器，得到了 58.7% 和 62.5% 的总准确率在视觉问答和报告质量答案任务中。这些结果表明 ELIXR 是一种强大和多功能的 CXR AI 方法。
</details></li>
</ul>
<hr>
<h2 id="A-vision-transformer-based-framework-for-knowledge-transfer-from-multi-modal-to-mono-modal-lymphoma-subtyping-models"><a href="#A-vision-transformer-based-framework-for-knowledge-transfer-from-multi-modal-to-mono-modal-lymphoma-subtyping-models" class="headerlink" title="A vision transformer-based framework for knowledge transfer from multi-modal to mono-modal lymphoma subtyping models"></a>A vision transformer-based framework for knowledge transfer from multi-modal to mono-modal lymphoma subtyping models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01328">http://arxiv.org/abs/2308.01328</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bilel Guetarni, Feryal Windal, Halim Benhabiles, Marianne Petit, Romain Dubois, Emmanuelle Leteurtre, Dominique Collard</li>
<li>for: 这个论文的目的是为了提出一种基于整体板准的DLBCL悬浮性诊断方法，以提高癌症患者的生存机会。</li>
<li>methods: 该方法使用了整体板准图像分析技术，并利用深度学习模型来预测DLBCL悬浮性诊断结果。</li>
<li>results: 实验研究表明，该方法可以高效地识别DLBCL悬浮性诊断结果，并且比现有的六种方法更高效。此外，研究还发现，为了达到同等诊断精度，该方法只需要训练一个单模态分类器。<details>
<summary>Abstract</summary>
Determining lymphoma subtypes is a crucial step for better patients treatment targeting to potentially increase their survival chances. In this context, the existing gold standard diagnosis method, which is based on gene expression technology, is highly expensive and time-consuming making difficult its accessibility. Although alternative diagnosis methods based on IHC (immunohistochemistry) technologies exist (recommended by the WHO), they still suffer from similar limitations and are less accurate. WSI (Whole Slide Image) analysis by deep learning models showed promising new directions for cancer diagnosis that would be cheaper and faster than existing alternative methods. In this work, we propose a vision transformer-based framework for distinguishing DLBCL (Diffuse Large B-Cell Lymphoma) cancer subtypes from high-resolution WSIs. To this end, we propose a multi-modal architecture to train a classifier model from various WSI modalities. We then exploit this model through a knowledge distillation mechanism for efficiently driving the learning of a mono-modal classifier. Our experimental study conducted on a dataset of 157 patients shows the promising performance of our mono-modal classification model, outperforming six recent methods from the state-of-the-art dedicated for cancer classification. Moreover, the power-law curve, estimated on our experimental data, shows that our classification model requires a reasonable number of additional patients for its training to potentially reach identical diagnosis accuracy as IHC technologies.
</details>
<details>
<summary>摘要</summary>
确定淋巴瘤Subtype是诊断患者的关键Step，以提高治疗效果和存活率。然而，现有的金标准诊断方法，基于基因表达技术，是非常昂贵和耗时的，使得它的可accessibility受限。尽管现有的诊断方法基于IHC（免疫组织化学）技术存在，但它们仍然受到相同的限制，并且较为不准确。WSI（整个报告图像）分析方法，基于深入学习模型，显示了新的可能性 Directions for cancer diagnosis，这些方法比现有的替代方法更加便宜和快速。在这项工作中，我们提出了一个视Transformer基于的框架，用于从高分辨率WSIs中分类Diffuse Large B-Cell Lymphoma（淋巴瘤）亚型。为此，我们提出了一种多模态建立的建议模型，用于训练分类模型。然后，我们利用这个模型的知识塑化机制，以有效地驱动模型的学习。我们的实验研究在157名病人的数据集上进行，显示我们的单模态分类模型在诊断方面表现出色，超过了过去六个最新的投入于cancer分类的方法。此外，我们对实验数据进行了力级曲线的估计，显示我们的分类模型需要一个合理的数量的更多patients的训练，以达到与IHC技术相同的诊断准确率。
</details></li>
</ul>
<hr>
<h2 id="Incorporating-Season-and-Solar-Specificity-into-Renderings-made-by-a-NeRF-Architecture-using-Satellite-Images"><a href="#Incorporating-Season-and-Solar-Specificity-into-Renderings-made-by-a-NeRF-Architecture-using-Satellite-Images" class="headerlink" title="Incorporating Season and Solar Specificity into Renderings made by a NeRF Architecture using Satellite Images"></a>Incorporating Season and Solar Specificity into Renderings made by a NeRF Architecture using Satellite Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01262">http://arxiv.org/abs/2308.01262</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/enterprisecv-6/season-nerf">https://github.com/enterprisecv-6/season-nerf</a></li>
<li>paper_authors: Michael Gableman, Avinash Kak</li>
<li>for: 可以使用卫星图像进行训练，在基于NeRF的框架中，根据季节 angle 渲染场景从不同的视角。</li>
<li>methods: 我们的工作extend了Shadow NeRF和Sat-NeRF的贡献，并显示了如何使得渲染得到季节特征。我们引入了一个时间变量，以教育我们的网络渲染季节特征，并且仍然能够渲染阴影。但是，卫星图像的小训练集可能会导致在同一个季节中的阴影存在相同的位置，这会引入歧义。我们添加了额外的损失函数项，以避免网络使用季节特征来补做阴影。</li>
<li>results: 我们在八个Area of Interest中进行了评估，包括测试框架能够准确渲染新视角、生成高度图、预测阴影、独立地考虑季节特征。我们的ablation study表明了我们的网络设计参数的选择是合理的。<details>
<summary>Abstract</summary>
As a result of Shadow NeRF and Sat-NeRF, it is possible to take the solar angle into account in a NeRF-based framework for rendering a scene from a novel viewpoint using satellite images for training. Our work extends those contributions and shows how one can make the renderings season-specific. Our main challenge was creating a Neural Radiance Field (NeRF) that could render seasonal features independently of viewing angle and solar angle while still being able to render shadows. We teach our network to render seasonal features by introducing one more input variable -- time of the year. However, the small training datasets typical of satellite imagery can introduce ambiguities in cases where shadows are present in the same location for every image of a particular season. We add additional terms to the loss function to discourage the network from using seasonal features for accounting for shadows. We show the performance of our network on eight Areas of Interest containing images captured by the Maxar WorldView-3 satellite. This evaluation includes tests measuring the ability of our framework to accurately render novel views, generate height maps, predict shadows, and specify seasonal features independently from shadows. Our ablation studies justify the choices made for network design parameters.
</details>
<details>
<summary>摘要</summary>
因为阴影NeRF和Sat-NeRF，可以在基于NeRF框架中考虑太阳角度，使用卫星图像进行训练，并在不同的视角和太阳角度下渲染场景。我们的工作扩展了这些贡献，并显示了如何使渲染季节特征独立于视角和太阳角度。我们教育我们的神经辐射场（NeRF）如何独立地渲染季节特征，而不是通过视角和太阳角度来覆盖阴影。我们添加了额外的损失函数项来防止神经网络使用季节特征来覆盖阴影。我们在八个区域 интерес中测试了我们的框架，包括测试框架能够准确渲染新视图、生成高程图像、预测阴影和独立地渲染季节特征。我们的剥削研究证明了我们的网络设计参数的选择。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/03/eess.IV_2023_08_03/" data-id="cllsjm79e008xbp8809drdypg" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/03/eess.AS_2023_08_03/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.AS - 2023-08-03 22:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/02/cs.LG_2023_08_02/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-08-02 18:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      // $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
