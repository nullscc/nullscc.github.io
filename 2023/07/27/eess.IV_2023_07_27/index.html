
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-07-27 17:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Weakly Supervised AI for Efficient Analysis of 3D Pathology Samples paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.14907 repo_url: https:&#x2F;&#x2F;github.com&#x2F;mahmoodlab&#x2F;mamba paper_authors: Andrew H. Song, Mane William">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-07-27 17:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/07/27/eess.IV_2023_07_27/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Weakly Supervised AI for Efficient Analysis of 3D Pathology Samples paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.14907 repo_url: https:&#x2F;&#x2F;github.com&#x2F;mahmoodlab&#x2F;mamba paper_authors: Andrew H. Song, Mane William">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-26T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:39.438Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_07_27" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/27/eess.IV_2023_07_27/" class="article-date">
  <time datetime="2023-07-26T16:00:00.000Z" itemprop="datePublished">2023-07-27</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-07-27 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Weakly-Supervised-AI-for-Efficient-Analysis-of-3D-Pathology-Samples"><a href="#Weakly-Supervised-AI-for-Efficient-Analysis-of-3D-Pathology-Samples" class="headerlink" title="Weakly Supervised AI for Efficient Analysis of 3D Pathology Samples"></a>Weakly Supervised AI for Efficient Analysis of 3D Pathology Samples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14907">http://arxiv.org/abs/2307.14907</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mahmoodlab/mamba">https://github.com/mahmoodlab/mamba</a></li>
<li>paper_authors: Andrew H. Song, Mane Williams, Drew F. K. Williamson, Guillaume Jaume, Andrew Zhang, Bowen Chen, Robert Serafin, Jonathan T. C. Liu, Alex Baras, Anil V. Parwani, Faisal Mahmood</li>
<li>for: 这个研究旨在开发一种基于深度学习的平台，用于处理多种成像模式的3D组织图像，预测病人结果。</li>
<li>methods: 该平台使用多样化的3D块分析方法，基于5年生化复诊结果进行风险分化网络训练。</li>
<li>results: 研究发现，使用3D块方法可以提高预测性能，并且可以减少采样偏误的风险，建议在临床实践中使用3D成像技术进行诊断和预测。<details>
<summary>Abstract</summary>
Human tissue and its constituent cells form a microenvironment that is fundamentally three-dimensional (3D). However, the standard-of-care in pathologic diagnosis involves selecting a few two-dimensional (2D) sections for microscopic evaluation, risking sampling bias and misdiagnosis. Diverse methods for capturing 3D tissue morphologies have been developed, but they have yet had little translation to clinical practice; manual and computational evaluations of such large 3D data have so far been impractical and/or unable to provide patient-level clinical insights. Here we present Modality-Agnostic Multiple instance learning for volumetric Block Analysis (MAMBA), a deep-learning-based platform for processing 3D tissue images from diverse imaging modalities and predicting patient outcomes. Archived prostate cancer specimens were imaged with open-top light-sheet microscopy or microcomputed tomography and the resulting 3D datasets were used to train risk-stratification networks based on 5-year biochemical recurrence outcomes via MAMBA. With the 3D block-based approach, MAMBA achieves an area under the receiver operating characteristic curve (AUC) of 0.86 and 0.74, superior to 2D traditional single-slice-based prognostication (AUC of 0.79 and 0.57), suggesting superior prognostication with 3D morphological features. Further analyses reveal that the incorporation of greater tissue volume improves prognostic performance and mitigates risk prediction variability from sampling bias, suggesting the value of capturing larger extents of heterogeneous 3D morphology. With the rapid growth and adoption of 3D spatial biology and pathology techniques by researchers and clinicians, MAMBA provides a general and efficient framework for 3D weakly supervised learning for clinical decision support and can help to reveal novel 3D morphological biomarkers for prognosis and therapeutic response.
</details>
<details>
<summary>摘要</summary>
人类组织和其内部细胞形成一个基本三维（3D）的微环境。然而，现行标准的病理诊断方法仅选择一些二维（2D）的section进行微scopic评估，可能存在采样偏见和诊断错误。各种用于捕捉3D组织形态的方法已经发展出来，但它们在临床实践中尚未得到广泛应用。我们现在介绍了模态无关多例学习 для块分析（MAMBA），一种基于深度学习的平台，用于处理不同成像模式的3D组织图像并预测病人结果。我们使用了扫描镜开式光sheet微scopy或微计算tomography扫描患有前列腺癌的肉瘤样本，并使用MAMBA进行风险分级网络的训练，以达到5年生物化学回报的 outcome。与传统的2D单片 slice-based预测相比，MAMBA的3D块基本approach在 receiver operating characteristic曲线（AUC）中得分0.86和0.74，表明3D形态特征可以提供更好的预测性。进一步分析表明，包含更大的组织体积可以提高预测性并减少采样偏见导致的预测变化，这表明3D morphological特征的捕捉是重要的。随着研究人员和临床医生对3D空间生物学和病理学技术的快速成长和采用，MAMBA提供了一个通用和高效的3D弱监学习框架，可以帮助揭示新的3D形态生物标志物和预测病人response。
</details></li>
</ul>
<hr>
<h2 id="A-full-resolution-training-framework-for-Sentinel-2-image-fusion"><a href="#A-full-resolution-training-framework-for-Sentinel-2-image-fusion" class="headerlink" title="A full-resolution training framework for Sentinel-2 image fusion"></a>A full-resolution training framework for Sentinel-2 image fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14864">http://arxiv.org/abs/2307.14864</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/matciotola/FR-FUSE">https://github.com/matciotola/FR-FUSE</a></li>
<li>paper_authors: Matteo Ciotola, Mario Ragosta, Giovanni Poggi, Giuseppe Scarpa</li>
<li>for: 这篇论文旨在提出一种新的无监督框架，用于深度学习模型的决 Height 采集 Sentinel-2 图像的超分辨率处理。</li>
<li>methods: 该方案使用 Sentinel-2 图像的 10-m 和 20-m 频道进行融合，而不需要降解分辨率生成训练数据。同时，提出了一种适合的损失函数，以确保网络预测和输入组件之间的循环一致性。</li>
<li>results: 在我们的初步实验中，提出的方案已经显示出了与监督方法相比的扩展性。此外，由于构造的损失函数，得到的训练网络可以归类为多分辨率分析方法。<details>
<summary>Abstract</summary>
This work presents a new unsupervised framework for training deep learning models for super-resolution of Sentinel-2 images by fusion of its 10-m and 20-m bands. The proposed scheme avoids the resolution downgrade process needed to generate training data in the supervised case. On the other hand, a proper loss that accounts for cycle-consistency between the network prediction and the input components to be fused is proposed. Despite its unsupervised nature, in our preliminary experiments the proposed scheme has shown promising results in comparison to the supervised approach. Besides, by construction of the proposed loss, the resulting trained network can be ascribed to the class of multi-resolution analysis methods.
</details>
<details>
<summary>摘要</summary>
这个工作提出了一种新的无监督框架，用于深度学习模型的超分辨率准备，基于报告20米和10米频道的融合。提议的方案不需要生成训练数据的分辨率下降过程，同时提出了一种适当的损失函数，该函数考虑了网络预测和输入组件的循环一致性。在我们的初步实验中，提议的方案已经达到了与监督方法相比的承诺性。此外，由于构造的损失函数，得到的训练网络可以被归类为多分辨率分析方法。
</details></li>
</ul>
<hr>
<h2 id="Seeing-through-the-Brain-Image-Reconstruction-of-Visual-Perception-from-Human-Brain-Signals"><a href="#Seeing-through-the-Brain-Image-Reconstruction-of-Visual-Perception-from-Human-Brain-Signals" class="headerlink" title="Seeing through the Brain: Image Reconstruction of Visual Perception from Human Brain Signals"></a>Seeing through the Brain: Image Reconstruction of Visual Perception from Human Brain Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02510">http://arxiv.org/abs/2308.02510</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu-Ting Lan, Kan Ren, Yansen Wang, Wei-Long Zheng, Dongsheng Li, Bao-Liang Lu, Lili Qiu</li>
<li>for: 这篇论文的目的是重建视觉刺激图像基于电энцеphalography（EEG）数据。</li>
<li>methods: 这篇论文提出了一个全面的执行管道，名为NeuroImagen，用于从EEG数据中重建视觉刺激图像。该管道包括一种新的多级感知信息解码器，用于从EEG数据中提取多级输出。然后，一种潜在扩散模型将利用提取的信息来重建高分辨率的视觉刺激图像。</li>
<li>results: 实验结果表明，该方法可以有效地重建视觉刺激图像，并且对比于现有方法有较高的量化性表现。<details>
<summary>Abstract</summary>
Seeing is believing, however, the underlying mechanism of how human visual perceptions are intertwined with our cognitions is still a mystery. Thanks to the recent advances in both neuroscience and artificial intelligence, we have been able to record the visually evoked brain activities and mimic the visual perception ability through computational approaches. In this paper, we pay attention to visual stimuli reconstruction by reconstructing the observed images based on portably accessible brain signals, i.e., electroencephalography (EEG) data. Since EEG signals are dynamic in the time-series format and are notorious to be noisy, processing and extracting useful information requires more dedicated efforts; In this paper, we propose a comprehensive pipeline, named NeuroImagen, for reconstructing visual stimuli images from EEG signals. Specifically, we incorporate a novel multi-level perceptual information decoding to draw multi-grained outputs from the given EEG data. A latent diffusion model will then leverage the extracted information to reconstruct the high-resolution visual stimuli images. The experimental results have illustrated the effectiveness of image reconstruction and superior quantitative performance of our proposed method.
</details>
<details>
<summary>摘要</summary>
视觉是信任的来源，但是人类视觉与认知之间的内部机制仍然是一个谜。随着 neuroscience 和人工智能的最近进步，我们可以记录人类视觉活动和模拟视觉能力通过计算方法。在这篇论文中，我们关注于基于可 portable 脑电声信号（EEG）的视觉刺激重建。因为EEG信号是时间序列格式的动态信号，容易受到干扰，因此处理和提取有用信息需要更多的努力。为解决这个问题，我们提出了一个完整的推管道，名为NeuroImagen，可以从EEG信号中提取多层次的视觉信息，并使用扩散模型重建高分辨率的视觉刺激图像。实验结果表明我们的方法可以有效地重建图像，并且在量化性能方面表现出色。
</details></li>
</ul>
<hr>
<h2 id="Test-Time-Adaptation-for-Blind-Image-Quality-Assessment"><a href="#Test-Time-Adaptation-for-Blind-Image-Quality-Assessment" class="headerlink" title="Test Time Adaptation for Blind Image Quality Assessment"></a>Test Time Adaptation for Blind Image Quality Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14735">http://arxiv.org/abs/2307.14735</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shankhanil006/tta-iqa">https://github.com/shankhanil006/tta-iqa</a></li>
<li>paper_authors: Subhadeep Roy, Shankhanil Mitra, Soma Biswas, Rajiv Soundararajan</li>
<li>for: 提高隐藏图像质量评估（IQA）算法的执行时间性能。</li>
<li>methods: 使用两个新的质量相关的辅助任务：批处理级别的群集对比损失和样本级别的相对排名损失，以使模型更加质量相关，适应目标数据。</li>
<li>results: 使用一小批测试分布中的图像更新批处理平均值可以实现显著提高性能。<details>
<summary>Abstract</summary>
While the design of blind image quality assessment (IQA) algorithms has improved significantly, the distribution shift between the training and testing scenarios often leads to a poor performance of these methods at inference time. This motivates the study of test time adaptation (TTA) techniques to improve their performance at inference time. Existing auxiliary tasks and loss functions used for TTA may not be relevant for quality-aware adaptation of the pre-trained model. In this work, we introduce two novel quality-relevant auxiliary tasks at the batch and sample levels to enable TTA for blind IQA. In particular, we introduce a group contrastive loss at the batch level and a relative rank loss at the sample level to make the model quality aware and adapt to the target data. Our experiments reveal that even using a small batch of images from the test distribution helps achieve significant improvement in performance by updating the batch normalization statistics of the source model.
</details>
<details>
<summary>摘要</summary>
尽管干燥图像质量评估（IQA）算法的设计已经得到了显著改进，但在执行时，测试场景和训练场景之间的分布偏移 frequently leads to poor performance of these methods. This motivates the study of test time adaptation（TTA）techniques to improve their performance at inference time. Existing auxiliary tasks and loss functions used for TTA may not be relevant for quality-aware adaptation of the pre-trained model. In this work, we introduce two novel quality-relevant auxiliary tasks at the batch and sample levels to enable TTA for blind IQA. In particular, we introduce a group contrastive loss at the batch level and a relative rank loss at the sample level to make the model quality aware and adapt to the target data. Our experiments reveal that even using a small batch of images from the test distribution helps achieve significant improvement in performance by updating the batch normalization statistics of the source model.
</details></li>
</ul>
<hr>
<h2 id="A-Multimodal-Supervised-Machine-Learning-Approach-for-Satellite-based-Wildfire-Identification-in-Europe"><a href="#A-Multimodal-Supervised-Machine-Learning-Approach-for-Satellite-based-Wildfire-Identification-in-Europe" class="headerlink" title="A Multimodal Supervised Machine Learning Approach for Satellite-based Wildfire Identification in Europe"></a>A Multimodal Supervised Machine Learning Approach for Satellite-based Wildfire Identification in Europe</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02508">http://arxiv.org/abs/2308.02508</a></li>
<li>repo_url: None</li>
<li>paper_authors: Angelica Urbanelli, Luca Barco, Edoardo Arnaudo, Claudio Rossi</li>
<li>for: 提高自动化卫星热点检测系统的准确性，以适应自然灾害的加频。</li>
<li>methods: 跨参照模式热点检测服务（MODIS和VIIRS）和欧洲森林火灾信息系统（EFFIS）数据库，建立大规模热点数据集，用于森林火灾相关研究。提出一种多模式指导学习方法，利用多种数据源，如ERSI年度土地用途土地覆盖（LULC）和 Copernicus Sentinel-3 数据，准确地分类热点检测结果。</li>
<li>results: 实验结果表明，我们的方法在森林火灾标识任务中具有效果。<details>
<summary>Abstract</summary>
The increasing frequency of catastrophic natural events, such as wildfires, calls for the development of rapid and automated wildfire detection systems. In this paper, we propose a wildfire identification solution to improve the accuracy of automated satellite-based hotspot detection systems by leveraging multiple information sources. We cross-reference the thermal anomalies detected by the Moderate-resolution Imaging Spectroradiometer (MODIS) and the Visible Infrared Imaging Radiometer Suite (VIIRS) hotspot services with the European Forest Fire Information System (EFFIS) database to construct a large-scale hotspot dataset for wildfire-related studies in Europe. Then, we propose a novel multimodal supervised machine learning approach to disambiguate hotspot detections, distinguishing between wildfires and other events. Our methodology includes the use of multimodal data sources, such as the ERSI annual Land Use Land Cover (LULC) and the Copernicus Sentinel-3 data. Experimental results demonstrate the effectiveness of our approach in the task of wildfire identification.
</details>
<details>
<summary>摘要</summary>
随着自然灾害的频繁发生，如野火，需要开发高速自动化野火检测系统。在这篇论文中，我们提出了一种野火标识解决方案，以提高自动遥感系统中热点检测的准确性。我们将模拟高分辨率 спектро镜谱仪(MODIS)和可见红外成像雷达仪(VIIRS)的热点服务与欧洲森林火灾信息系统(EFFIS)数据库进行交叉引用，以构建欧洲大规模热点数据集用于野火相关研究。然后，我们提出了一种新的多模式超vised机器学习方法，用于细分热点检测，将野火和其他事件分开。我们的方法包括使用多模式数据源，如地理信息系统(ERSI)年度土地用途土地覆盖(LULC)和科学技术卫星(Copernicus)三号卫星数据。实验结果表明，我们的方法在野火标识任务中具有效果。
</details></li>
</ul>
<hr>
<h2 id="LLDiffusion-Learning-Degradation-Representations-in-Diffusion-Models-for-Low-Light-Image-Enhancement"><a href="#LLDiffusion-Learning-Degradation-Representations-in-Diffusion-Models-for-Low-Light-Image-Enhancement" class="headerlink" title="LLDiffusion: Learning Degradation Representations in Diffusion Models for Low-Light Image Enhancement"></a>LLDiffusion: Learning Degradation Representations in Diffusion Models for Low-Light Image Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14659">http://arxiv.org/abs/2307.14659</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/taowangzj/lldiffusion">https://github.com/taowangzj/lldiffusion</a></li>
<li>paper_authors: Tao Wang, Kaihao Zhang, Ziqian Shao, Wenhan Luo, Bjorn Stenger, Tae-Kyun Kim, Wei Liu, Hongdong Li</li>
<li>for: 提高低光照图像的增强（LLIE）</li>
<li>methods: 使用扩散模型，并 integrates 衰变和图像约束，以提高图像增强效果。</li>
<li>results: 比对多个 benchmark 数据集，实验结果表明，提案的 LLDiffusion 方法可以 Quantitatively 和 Qualitatively 超过当前的 LLIE 方法。Here’s a breakdown of each point:</li>
<li>for: The paper is written for low-light image enhancement (LLIE), specifically addressing the limitation of current deep learning methods that overlook the importance of considering degradation representations.</li>
<li>methods: The proposed method uses a degradation-aware learning scheme based on diffusion models, which integrates degradation and image priors into the diffusion process. The method includes a joint learning framework for image generation and enhancement, as well as a well-designed dynamic diffusion module that takes into account both the color map and the latent degradation representations.</li>
<li>results: The proposed method is evaluated on several well-known benchmark datasets, including synthetic and real-world unpaired datasets. The results demonstrate that LLDiffusion outperforms state-of-the-art LLIE methods both quantitatively and qualitatively.<details>
<summary>Abstract</summary>
Current deep learning methods for low-light image enhancement (LLIE) typically rely on pixel-wise mapping learned from paired data. However, these methods often overlook the importance of considering degradation representations, which can lead to sub-optimal outcomes. In this paper, we address this limitation by proposing a degradation-aware learning scheme for LLIE using diffusion models, which effectively integrates degradation and image priors into the diffusion process, resulting in improved image enhancement. Our proposed degradation-aware learning scheme is based on the understanding that degradation representations play a crucial role in accurately modeling and capturing the specific degradation patterns present in low-light images. To this end, First, a joint learning framework for both image generation and image enhancement is presented to learn the degradation representations. Second, to leverage the learned degradation representations, we develop a Low-Light Diffusion model (LLDiffusion) with a well-designed dynamic diffusion module. This module takes into account both the color map and the latent degradation representations to guide the diffusion process. By incorporating these conditioning factors, the proposed LLDiffusion can effectively enhance low-light images, considering both the inherent degradation patterns and the desired color fidelity. Finally, we evaluate our proposed method on several well-known benchmark datasets, including synthetic and real-world unpaired datasets. Extensive experiments on public benchmarks demonstrate that our LLDiffusion outperforms state-of-the-art LLIE methods both quantitatively and qualitatively. The source code and pre-trained models are available at https://github.com/TaoWangzj/LLDiffusion.
</details>
<details>
<summary>摘要</summary>
当前的深度学习方法 для低光照图像提升（LLIE）通常是基于像素级映射学习的，但这些方法经常忽视了质量的衰减表示。在这篇论文中，我们解决这个限制，提出了一种考虑衰减表示的学习方案，使得图像提升更加稳定。我们的提议的衰减意识学习方案基于衰减表示在低光照图像中的重要作用。为了实现这一点，我们首先提出了一种共同学习框架，用于学习图像生成和图像提升。其次，我们开发了一种基于扩散模型的低光照扩散模型（LLDiffusion），该模型具有一个有效地考虑了颜色图和背景衰减表示的动态扩散模块。通过将这些条件因素纳入考虑，我们的LLDiffusion可以更好地提升低光照图像，考虑到了图像的自然衰减模式以及颜色准确性。最后，我们对一些公共 benchmark 上进行了广泛的实验，证明了我们的LLDiffusion在量和质量上都超过了现有的LLIE方法。代码和预训练模型可以在 <https://github.com/TaoWangzj/LLDiffusion> 上获取。
</details></li>
</ul>
<hr>
<h2 id="A-Weakly-Supervised-Segmentation-Network-Embedding-Cross-scale-Attention-Guidance-and-Noise-sensitive-Constraint-for-Detecting-Tertiary-Lymphoid-Structures-of-Pancreatic-Tumors"><a href="#A-Weakly-Supervised-Segmentation-Network-Embedding-Cross-scale-Attention-Guidance-and-Noise-sensitive-Constraint-for-Detecting-Tertiary-Lymphoid-Structures-of-Pancreatic-Tumors" class="headerlink" title="A Weakly Supervised Segmentation Network Embedding Cross-scale Attention Guidance and Noise-sensitive Constraint for Detecting Tertiary Lymphoid Structures of Pancreatic Tumors"></a>A Weakly Supervised Segmentation Network Embedding Cross-scale Attention Guidance and Noise-sensitive Constraint for Detecting Tertiary Lymphoid Structures of Pancreatic Tumors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14603">http://arxiv.org/abs/2307.14603</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bingxue Wang, Liwen Zou, Jun Chen, Yingying Cao, Zhenghua Cai, Yudong Qiu, Liang Mao, Zhongqiu Wang, Jingya Chen, Luying Gui, Xiaoping Yang</li>
<li>For: 这篇研究旨在探讨一种几何学习的方法，用于检测胰脏病变中的次次性林肺结构（TLS）。* Methods: 我们提出了一个弱监督分类网络，来检测TLS。我们首先使用预训练的模型进行核lei分 segmentation，然后将精度检测过滤到我们设计的 linfocyte density 对应。我们还实现了一个跨度对应 Mechanism，将粗细度特征学习自原始胰脏病变图像，以及细节度特征学习自我设计的 linfocyte density 对应。* Results: 我们将这个方法应用于两个收集的数据集，结果显示，我们的提议方法在TLS检测精度方面与现有的分类型检测方法相比，有 statistically significant 的优化。此外，我们还将这个方法应用于研究胰脏病变中TLS的density和周围血管侵入之间的相互关系，并获得了一些临床有用的结果。<details>
<summary>Abstract</summary>
The presence of tertiary lymphoid structures (TLSs) on pancreatic pathological images is an important prognostic indicator of pancreatic tumors. Therefore, TLSs detection on pancreatic pathological images plays a crucial role in diagnosis and treatment for patients with pancreatic tumors. However, fully supervised detection algorithms based on deep learning usually require a large number of manual annotations, which is time-consuming and labor-intensive. In this paper, we aim to detect the TLSs in a manner of few-shot learning by proposing a weakly supervised segmentation network. We firstly obtain the lymphocyte density maps by combining a pretrained model for nuclei segmentation and a domain adversarial network for lymphocyte nuclei recognition. Then, we establish a cross-scale attention guidance mechanism by jointly learning the coarse-scale features from the original histopathology images and fine-scale features from our designed lymphocyte density attention. A noise-sensitive constraint is introduced by an embedding signed distance function loss in the training procedure to reduce tiny prediction errors. Experimental results on two collected datasets demonstrate that our proposed method significantly outperforms the state-of-the-art segmentation-based algorithms in terms of TLSs detection accuracy. Additionally, we apply our method to study the congruent relationship between the density of TLSs and peripancreatic vascular invasion and obtain some clinically statistical results.
</details>
<details>
<summary>摘要</summary>
pancreatic tumors中的次级血液结构（TLSs）的存在是诊断和治疗中非常重要的诊断指标。因此，TLSs的检测在pancreatic tumors中扮演着关键的角色。然而，通常需要大量的手动标注，这是时间consuming和劳动密集的。在这篇论文中，我们想要通过几shot学习来检测TLSs，我们提出了一种弱型指导网络。首先，我们获得了lymphocyte density map，通过结合预训练的核体分割模型和域 adversarial network来识别lymphocyte的核体。然后，我们建立了跨度级别的注意力引导机制，通过同时学习原始的 histopathology 图像的粗级特征和我们设计的lymphocyte density注意力来实现。在训练过程中，我们引入了一个嵌入签名距离函数损失，以降低微小预测错误。实验结果表明，我们的提议方法在pancreatic tumors中的TLSs检测精度上明显超过了现状的segmentation-based算法。此外，我们通过应用我们的方法来研究peripancreatic vascular invasion和TLSs的密切关系，并获得了一些临床 statistically significant的结果。
</details></li>
</ul>
<hr>
<h2 id="FocalErrorNet-Uncertainty-aware-focal-modulation-network-for-inter-modal-registration-error-estimation-in-ultrasound-guided-neurosurgery"><a href="#FocalErrorNet-Uncertainty-aware-focal-modulation-network-for-inter-modal-registration-error-estimation-in-ultrasound-guided-neurosurgery" class="headerlink" title="FocalErrorNet: Uncertainty-aware focal modulation network for inter-modal registration error estimation in ultrasound-guided neurosurgery"></a>FocalErrorNet: Uncertainty-aware focal modulation network for inter-modal registration error estimation in ultrasound-guided neurosurgery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14520">http://arxiv.org/abs/2307.14520</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soorena Salari, Amirhossein Rasoulian, Hassan Rivaz, Yiming Xiao</li>
<li>for: brain tumor resection, accurate removal of cancerous tissues while preserving eloquent regions is crucial to the safety and outcomes of the treatment.</li>
<li>methods: intra-operative ultrasound (iUS) has been adopted to provide real-time images to track brain shift, and inter-modal (i.e., MRI-iUS) registration is often required to update the pre-surgical plan.</li>
<li>results: a novel deep learning technique based on 3D focal modulation in conjunction with uncertainty estimation to accurately assess MRI-iUS registration errors for brain tumor surgery, with an estimation error of 0.59+-0.57 mm.Here is the result in Simplified Chinese text:</li>
<li>for: 脑肿瘤镜下手术，准确除除肿瘤组织，保留语言功能区域是治疗安全和结果的关键。</li>
<li>methods: 使用实时图像跟踪脑shift的术前评估，并通过多Modal（i.e., MRI-iUS）注册更新预后方案。</li>
<li>results: 一种基于3D焦点调制的深度学习技术，以及不确定度估计，为脑肿瘤手术中MRI-iUS注册错误的准确评估，错误估计值为0.59+-0.57 mm。<details>
<summary>Abstract</summary>
In brain tumor resection, accurate removal of cancerous tissues while preserving eloquent regions is crucial to the safety and outcomes of the treatment. However, intra-operative tissue deformation (called brain shift) can move the surgical target and render the pre-surgical plan invalid. Intra-operative ultrasound (iUS) has been adopted to provide real-time images to track brain shift, and inter-modal (i.e., MRI-iUS) registration is often required to update the pre-surgical plan. Quality control for the registration results during surgery is important to avoid adverse outcomes, but manual verification faces great challenges due to difficult 3D visualization and the low contrast of iUS. Automatic algorithms are urgently needed to address this issue, but the problem was rarely attempted. Therefore, we propose a novel deep learning technique based on 3D focal modulation in conjunction with uncertainty estimation to accurately assess MRI-iUS registration errors for brain tumor surgery. Developed and validated with the public RESECT clinical database, the resulting algorithm can achieve an estimation error of 0.59+-0.57 mm.
</details>
<details>
<summary>摘要</summary>
在脑肿瘤切除手术中，准确地移除癌细胞组织，同时保留eloquent区域的安全和效果是致命的。然而，操作期间脑部塑形（brain shift）可能导致手术目标移动，使原先的预期计划无效。为了提供实时图像，脑部ultrasound（iUS）已被采用，而inter-modal（i.e., MRI-iUS） регистрация经常需要更新预期计划。在手术中质量控制注册结果的重要性，由于3D视化和iUS的低对比度，导致手动验证困难。因此，我们提出了一种基于3D焦点修饰的深度学习技术，以确定MRI-iUS注册错误的准确评估方法。与公共RESECT临床数据库的开发和验证结果显示，这种算法可以实现注册错误的估计误差为0.59±0.57毫米。
</details></li>
</ul>
<hr>
<h2 id="Phenotype-preserving-metric-design-for-high-content-image-reconstruction-by-generative-inpainting"><a href="#Phenotype-preserving-metric-design-for-high-content-image-reconstruction-by-generative-inpainting" class="headerlink" title="Phenotype-preserving metric design for high-content image reconstruction by generative inpainting"></a>Phenotype-preserving metric design for high-content image reconstruction by generative inpainting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14436">http://arxiv.org/abs/2307.14436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vaibhav Sharma, Artur Yakimovich</li>
<li>for: 这个论文主要用于研究高通量微scopic影像数据的自动化处理和修复技术，以提高生物系统学和药物层次creening应用。</li>
<li>methods: 这个论文使用了现有的image inpainting技术，如DeepFill V2和Edge Connect，对高通量fluorescence microscopy图像进行修复和Restoration。</li>
<li>results: 研究发现，通过精心调整和少量数据，这些技术可以准确修复微scopic影像，并且Restoration质量与图像区域大小相关。此外，提出了一种新的phenotype-preserving度量设计策略，以控制修复质量并避免不良修复。<details>
<summary>Abstract</summary>
In the past decades, automated high-content microscopy demonstrated its ability to deliver large quantities of image-based data powering the versatility of phenotypic drug screening and systems biology applications. However, as the sizes of image-based datasets grew, it became infeasible for humans to control, avoid and overcome the presence of imaging and sample preparation artefacts in the images. While novel techniques like machine learning and deep learning may address these shortcomings through generative image inpainting, when applied to sensitive research data this may come at the cost of undesired image manipulation. Undesired manipulation may be caused by phenomena such as neural hallucinations, to which some artificial neural networks are prone. To address this, here we evaluate the state-of-the-art inpainting methods for image restoration in a high-content fluorescence microscopy dataset of cultured cells with labelled nuclei. We show that architectures like DeepFill V2 and Edge Connect can faithfully restore microscopy images upon fine-tuning with relatively little data. Our results demonstrate that the area of the region to be restored is of higher importance than shape. Furthermore, to control for the quality of restoration, we propose a novel phenotype-preserving metric design strategy. In this strategy, the size and count of the restored biological phenotypes like cell nuclei are quantified to penalise undesirable manipulation. We argue that the design principles of our approach may also generalise to other applications.
</details>
<details>
<summary>摘要</summary>
在过去的几十年中，自动化高内容微scopia技术已经证明了它可以提供大量的图像数据，为phenotypic drug screening和系统生物学应用提供了多样化的能力。然而，随着图像数据的大小的增长，人类控制、避免和消除图像和样本准备 artifacts在图像中的成本变得不可持续。 novels技术如机器学习和深度学习可能会解决这些缺陷通过生成图像填充，但是当应用于敏感研究数据时，这可能会导致不желатель的图像修饰。这些修饰可能会由人工神经网络的神经抑制引起，导致图像修饰。为了解决这个问题，我们在高内容染料微scopiadataset中评估了state-of-the-art填充方法的图像修复能力。我们发现，Architecture如DeepFill V2和Edge Connect可以在高度微scopiadataset中 faithful restore microscopy images，并且只需要少量的数据进行微调。我们的结果表明，图像修复的区域大小比较重要于形态。此外，为了控制修复质量，我们提出了一种新的phenotype-preserving度量设计策略。在这种策略中，修复的生物fenotypes，如细胞核的大小和数量，被量化以 penalize不желатель的修饰。我们认为，我们的设计原则可能也会总结到其他应用中。
</details></li>
</ul>
<hr>
<h2 id="Optimization-of-Image-Acquisition-for-Earth-Observation-Satellites-via-Quantum-Computing"><a href="#Optimization-of-Image-Acquisition-for-Earth-Observation-Satellites-via-Quantum-Computing" class="headerlink" title="Optimization of Image Acquisition for Earth Observation Satellites via Quantum Computing"></a>Optimization of Image Acquisition for Earth Observation Satellites via Quantum Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14419">http://arxiv.org/abs/2307.14419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antón Makarov, Márcio M. Taddei, Eneko Osaba, Giacomo Franceschetto, Esther Villar-Rodriguez, Izaskun Oregi</li>
<li>for: 这篇论文的目的是对于卫星图像获取时间调度问题进行优化，以找到在给定轨道通过时间下最佳的图像子集。</li>
<li>methods: 这篇论文使用了两种QUBO表述方法来解决这个问题，并且使用不同的缓存处理技术来处理非易式约束。</li>
<li>results: 实验结果显示，不同的表述方法和缓存处理技术对于解决这个问题有很大的影响，而且Current quantum computers可以解决的问题型例子限制在一定的大小上。<details>
<summary>Abstract</summary>
Satellite image acquisition scheduling is a problem that is omnipresent in the earth observation field; its goal is to find the optimal subset of images to be taken during a given orbit pass under a set of constraints. This problem, which can be modeled via combinatorial optimization, has been dealt with many times by the artificial intelligence and operations research communities. However, despite its inherent interest, it has been scarcely studied through the quantum computing paradigm. Taking this situation as motivation, we present in this paper two QUBO formulations for the problem, using different approaches to handle the non-trivial constraints. We compare the formulations experimentally over 20 problem instances using three quantum annealers currently available from D-Wave, as well as one of its hybrid solvers. Fourteen of the tested instances have been obtained from the well-known SPOT5 benchmark, while the remaining six have been generated ad-hoc for this study. Our results show that the formulation and the ancilla handling technique is crucial to solve the problem successfully. Finally, we also provide practical guidelines on the size limits of problem instances that can be realistically solved on current quantum computers.
</details>
<details>
<summary>摘要</summary>
卫星图像获取计划是地球观测领域中一个普遍存在的问题，其目标是在给定的轨道过程中选择最佳的图像子集，以满足一系列约束。这个问题可以被模型为 combinatorial optimization 问题，在人工智能和运筹学社区中已经得到了广泛的研究。然而，即使它具有潜在的兴趣，它在量子计算理解中却很少被研究。在这种情况下，我们在这篇论文中提出了两种 QUBO 表示方法，使用不同的方法来处理非质量约束。我们通过实验测试了这些表示方法，使用 D-Wave 提供的三个量子泵浸器和一个混合解决方案。我们测试的问题实例数量为 20，其中 14 个来自 SPOT5 标准准则，另外 6 个是为本研究而生成的。我们的结果表明，表示方法和卵处理技术是解决问题的关键。此外，我们还提供了现有量子计算机的实际问题大小限制。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/27/eess.IV_2023_07_27/" data-id="cllta0ljg008hny885lmu5ore" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/27/eess.AS_2023_07_27/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.AS - 2023-07-27 22:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/26/cs.LG_2023_07_26/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-07-26 18:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
