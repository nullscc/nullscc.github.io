
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-07-27 18:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Federated Model Aggregation via Self-Supervised Priors for Highly Imbalanced Medical Image Classification paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.14959 repo_url: https:&#x2F;&#x2F;github.com&#x2F;xmed-lab&#x2F;fed-mas paper">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-07-27 18:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/07/27/cs.LG_2023_07_27/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Federated Model Aggregation via Self-Supervised Priors for Highly Imbalanced Medical Image Classification paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.14959 repo_url: https:&#x2F;&#x2F;github.com&#x2F;xmed-lab&#x2F;fed-mas paper">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-26T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:39.321Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_07_27" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/27/cs.LG_2023_07_27/" class="article-date">
  <time datetime="2023-07-26T16:00:00.000Z" itemprop="datePublished">2023-07-27</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-07-27 18:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Federated-Model-Aggregation-via-Self-Supervised-Priors-for-Highly-Imbalanced-Medical-Image-Classification"><a href="#Federated-Model-Aggregation-via-Self-Supervised-Priors-for-Highly-Imbalanced-Medical-Image-Classification" class="headerlink" title="Federated Model Aggregation via Self-Supervised Priors for Highly Imbalanced Medical Image Classification"></a>Federated Model Aggregation via Self-Supervised Priors for Highly Imbalanced Medical Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14959">http://arxiv.org/abs/2307.14959</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/fed-mas">https://github.com/xmed-lab/fed-mas</a></li>
<li>paper_authors: Marawan Elbatel, Hualiang Wang, Robert Martí, Huazhu Fu, Xiaomeng Li</li>
<li>for: 这篇论文主要针对医疗领域 Federated Learning 中的高度不均衡数据集，包括皮肤损伤和肠道图像。现有的 Federated 方法在高度不均衡数据集上主要是优化全局模型，而不考虑医疗影像中的内部类别差异。</li>
<li>methods: 本论文使用公共可用的自动学习助记网络（如 MoCo-V2）在每个客户端上进行本地预训练，并发现使用共享 auxiliary 预训练模型可以获得一致异常度测量。基于这些发现，我们 derivate了一种动态均衡模型聚合方法（MAS）来导引全局模型优化。</li>
<li>results: Fed-MAS 可以与不同的本地学习方法结合使用，以实现高度可靠和无偏见的全局模型。我们的代码可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/xmed-lab/Fed-MAS%7D">https://github.com/xmed-lab/Fed-MAS}</a> 上找到。<details>
<summary>Abstract</summary>
In the medical field, federated learning commonly deals with highly imbalanced datasets, including skin lesions and gastrointestinal images. Existing federated methods under highly imbalanced datasets primarily focus on optimizing a global model without incorporating the intra-class variations that can arise in medical imaging due to different populations, findings, and scanners. In this paper, we study the inter-client intra-class variations with publicly available self-supervised auxiliary networks. Specifically, we find that employing a shared auxiliary pre-trained model, like MoCo-V2, locally on every client yields consistent divergence measurements. Based on these findings, we derive a dynamic balanced model aggregation via self-supervised priors (MAS) to guide the global model optimization. Fed-MAS can be utilized with different local learning methods for effective model aggregation toward a highly robust and unbiased global model. Our code is available at \url{https://github.com/xmed-lab/Fed-MAS}.
</details>
<details>
<summary>摘要</summary>
医疗领域中，联合学习经常面临高度不均衡的数据集，包括皮肤病变和Digestive tract影像。现有的联合方法在高度不均衡数据集上主要是优化全球模型，而不考虑医疗影像中的内部类别差异，这些差异可能 arise due to different populations, findings, and scanners。在这篇论文中，我们研究了客户端间内部类别差异，使用公共可用的无监督辅助网络。我们发现，在每个客户端上使用共享的辅助预训练模型，如MoCo-V2，可以获得一致的分化度测量。基于这些发现，我们 derive了一种动态平衡的模型聚合方法（MAS），以导引全球模型优化。Fed-MAS可以与不同的本地学习方法结合使用，以实现高度可靠和无偏的全球模型。我们的代码可以在 GitHub上找到：https://github.com/xmed-lab/Fed-MAS。
</details></li>
</ul>
<hr>
<h2 id="Multi-Source-Domain-Adaptation-through-Dataset-Dictionary-Learning-in-Wasserstein-Space"><a href="#Multi-Source-Domain-Adaptation-through-Dataset-Dictionary-Learning-in-Wasserstein-Space" class="headerlink" title="Multi-Source Domain Adaptation through Dataset Dictionary Learning in Wasserstein Space"></a>Multi-Source Domain Adaptation through Dataset Dictionary Learning in Wasserstein Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14953">http://arxiv.org/abs/2307.14953</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/eddardd/demo-dadil">https://github.com/eddardd/demo-dadil</a></li>
<li>paper_authors: Eduardo Fernandes Montesuma, Fred Ngolè Mboula, Antoine Souloumiac</li>
<li>for: 解决多源领域适应（MSDA）问题，即在多个标注源频道中传递知识，并且避免数据分布偏移。</li>
<li>methods: 提出了一种基于字典学习和最优运输的MSDA框架。对每个频道进行解释，表示每个频道为一个 Wasserstein 质量中心，并提出了一种基于字典的两种新方法：DaDil-R 和 DaDiL-E。</li>
<li>results: 在 Caltech-Office、Office 31 和 CRWU 三个标准测试集上进行评估，并与之前的状态前进行了3.15%、2.29% 和 7.71% 的改进。最后，我们表明了 interpolations 在学习的atom集中的 Wasserstein 树可以泛化到目标频道。<details>
<summary>Abstract</summary>
This paper seeks to solve Multi-Source Domain Adaptation (MSDA), which aims to mitigate data distribution shifts when transferring knowledge from multiple labeled source domains to an unlabeled target domain. We propose a novel MSDA framework based on dictionary learning and optimal transport. We interpret each domain in MSDA as an empirical distribution. As such, we express each domain as a Wasserstein barycenter of dictionary atoms, which are empirical distributions. We propose a novel algorithm, DaDiL, for learning via mini-batches: (i) atom distributions; (ii) a matrix of barycentric coordinates. Based on our dictionary, we propose two novel methods for MSDA: DaDil-R, based on the reconstruction of labeled samples in the target domain, and DaDiL-E, based on the ensembling of classifiers learned on atom distributions. We evaluate our methods in 3 benchmarks: Caltech-Office, Office 31, and CRWU, where we improved previous state-of-the-art by 3.15%, 2.29%, and 7.71% in classification performance. Finally, we show that interpolations in the Wasserstein hull of learned atoms provide data that can generalize to the target domain.
</details>
<details>
<summary>摘要</summary>
We represent each domain in MSDA as an empirical distribution, and express each domain as a Wasserstein barycenter of dictionary atoms, which are empirical distributions. We then propose a novel algorithm, DaDiL, for learning via mini-batches, which involves (i) learning atom distributions, and (ii) computing a matrix of barycentric coordinates.Based on our dictionary, we propose two novel methods for MSDA: DaDil-R, which uses the reconstruction of labeled samples in the target domain, and DaDiL-E, which uses the ensembling of classifiers learned on atom distributions. We evaluate our methods on three benchmark datasets: Caltech-Office, Office 31, and CRWU, and show that our approach achieves state-of-the-art performance, improving upon previous results by 3.15%, 2.29%, and 7.71% in classification accuracy.Finally, we demonstrate that interpolations in the Wasserstein hull of learned atoms can provide data that can generalize to the target domain, providing a promising avenue for future research.
</details></li>
</ul>
<hr>
<h2 id="Network-Fault-tolerant-and-Byzantine-resilient-Social-Learning-via-Collaborative-Hierarchical-Non-Bayesian-Learning"><a href="#Network-Fault-tolerant-and-Byzantine-resilient-Social-Learning-via-Collaborative-Hierarchical-Non-Bayesian-Learning" class="headerlink" title="Network Fault-tolerant and Byzantine-resilient Social Learning via Collaborative Hierarchical Non-Bayesian Learning"></a>Network Fault-tolerant and Byzantine-resilient Social Learning via Collaborative Hierarchical Non-Bayesian Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14952">http://arxiv.org/abs/2307.14952</a></li>
<li>repo_url: None</li>
<li>paper_authors: Connor Mclaughlin, Matthew Ding, Denis Edogmus, Lili Su</li>
<li>for:  addresses the problem of non-Bayesian learning over networks that are vulnerable to communication failures and external adversarial attacks.</li>
<li>methods:  proposes a hierarchical robust push-sum algorithm, a sparse information fusion rule, and a packet-dropping fault-tolerant non-Bayesian learning algorithm with provable convergence guarantees.</li>
<li>results:  achieves average consensus despite frequent packet-dropping link failures and external adversarial attacks, and solves the non-Bayesian learning problem via running multiple dynamics.Here is the simplified Chinese text:</li>
<li>for:  Addresses 非托 bayesian 学习问题，即在受到通信故障和外部恶意攻击的网络上。</li>
<li>methods: 提出了层次系统架构，并使用了 packets 损失链接故障和外部恶意攻击的鲁棒推 sum 算法， sparse 信息融合规则，以及一种 packet-dropping  fault-tolerant 非托 bayesian 学习算法，并提供了可证明的收敛保证。</li>
<li>results:  Achieves 平均consensus ，并在 packet-dropping 链接故障和外部恶意攻击下解决了非托 bayesian 学习问题，并通过运行多个动力来解决这个问题。<details>
<summary>Abstract</summary>
As the network scale increases, existing fully distributed solutions start to lag behind the real-world challenges such as (1) slow information propagation, (2) network communication failures, and (3) external adversarial attacks. In this paper, we focus on hierarchical system architecture and address the problem of non-Bayesian learning over networks that are vulnerable to communication failures and adversarial attacks. On network communication, we consider packet-dropping link failures.   We first propose a hierarchical robust push-sum algorithm that can achieve average consensus despite frequent packet-dropping link failures. We provide a sparse information fusion rule between the parameter server and arbitrarily selected network representatives. Then, interleaving the consensus update step with a dual averaging update with Kullback-Leibler (KL) divergence as the proximal function, we obtain a packet-dropping fault-tolerant non-Bayesian learning algorithm with provable convergence guarantees.   On external adversarial attacks, we consider Byzantine attacks in which the compromised agents can send maliciously calibrated messages to others (including both the agents and the parameter server). To avoid the curse of dimensionality of Byzantine consensus, we solve the non-Bayesian learning problem via running multiple dynamics, each of which only involves Byzantine consensus with scalar inputs. To facilitate resilient information propagation across sub-networks, we use a novel Byzantine-resilient gossiping-type rule at the parameter server.
</details>
<details>
<summary>摘要</summary>
随着网络规模的增加，现有的完全分布式解决方案开始落后于实际世界中的挑战，如（1）慢速信息传播，（2）网络通信失败，以及（3）外部敌意攻击。在这篇论文中，我们关注层次系统架构，并解决非bayesian学习问题，面临网络通信失败和外部敌意攻击。对于网络通信，我们考虑 packet-dropping 链接故障。我们首先提出一种层次可靠推轮算法，可以在 packet-dropping 链接故障情况下达到平均共识。我们提供一种稀缺信息融合规则，使得参数服务器和随机选择的网络代表进行简单的信息交换。然后，将整体协同更新步骤与 dual averaging 更新步骤结合，使得 packet-dropping 故障tolerant 非bayesian 学习算法具有可证明的收敛保证。对于外部敌意攻击，我们考虑 Byzantine 攻击，在这种情况下，被入侵的代理会发送卑势化的消息到其他代理（包括代理和参数服务器）。为了避免 Byzantine 协同收敛的维度约束，我们使用多个动态，每个动态仅涉及 Byzantine 协同，并使用一种新的 Byzantine 抗性的聊天规则。为了促进网络下扩散信息的稳定传递，我们使用一种新的 Byzantine 抗性的聊天规则。
</details></li>
</ul>
<hr>
<h2 id="A-Self-Adaptive-Penalty-Method-for-Integrating-Prior-Knowledge-Constraints-into-Neural-ODEs"><a href="#A-Self-Adaptive-Penalty-Method-for-Integrating-Prior-Knowledge-Constraints-into-Neural-ODEs" class="headerlink" title="A Self-Adaptive Penalty Method for Integrating Prior Knowledge Constraints into Neural ODEs"></a>A Self-Adaptive Penalty Method for Integrating Prior Knowledge Constraints into Neural ODEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14940">http://arxiv.org/abs/2307.14940</a></li>
<li>repo_url: None</li>
<li>paper_authors: C. Coelho, M. Fernanda P. Costa, L. L. Ferrás</li>
<li>for: 模拟自然系统的持续动力学行为</li>
<li>methods: 使用神经ordinary differential equation（Neural ODE）模型</li>
<li>results: 提出了一种自适应罚函数算法，以便模拟受限制的自然系统Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written to model the continuous dynamics of natural systems, specifically using Neural ODEs.</li>
<li>methods: The paper proposes a self-adaptive penalty algorithm for Neural ODEs to enable modelling of constrained natural systems. This algorithm dynamically adjusts the penalty parameters based on the data.</li>
<li>results: The proposed approach is validated using three natural systems with prior knowledge constraints: population growth, chemical reaction evolution, and damped harmonic oscillator motion. The numerical experiments show that the self-adaptive penalty approach provides more accurate and robust models with reliable and meaningful predictions.<details>
<summary>Abstract</summary>
The continuous dynamics of natural systems has been effectively modelled using Neural Ordinary Differential Equations (Neural ODEs). However, for accurate and meaningful predictions, it is crucial that the models follow the underlying rules or laws that govern these systems. In this work, we propose a self-adaptive penalty algorithm for Neural ODEs to enable modelling of constrained natural systems. The proposed self-adaptive penalty function can dynamically adjust the penalty parameters. The explicit introduction of prior knowledge helps to increase the interpretability of Neural ODE -based models. We validate the proposed approach by modelling three natural systems with prior knowledge constraints: population growth, chemical reaction evolution, and damped harmonic oscillator motion. The numerical experiments and a comparison with other penalty Neural ODE approaches and \emph{vanilla} Neural ODE, demonstrate the effectiveness of the proposed self-adaptive penalty algorithm for Neural ODEs in modelling constrained natural systems. Moreover, the self-adaptive penalty approach provides more accurate and robust models with reliable and meaningful predictions.
</details>
<details>
<summary>摘要</summary>
自然系统的连续动力学已经非常有效地使用神经常微方程（Neural ODE）来模拟。但是，为了获得准确和有意义的预测，模型需要遵循下面的规则或法律来控制这些系统。在这种工作中，我们提议一种自适应罚函数算法来使神经常微方程模型受到约束。这种自适应罚函数可以动态调整罚参数。通过直接引入先知知识，可以增加神经常微方程模型的解释性。我们验证了提议的方法，通过模拟三个自然系统受约束的模型：人口增长、化学反应演化和抑制响应振荡。计算实验和与其他罚Neural ODE方法和"vanilla"Neural ODE进行比较，表明提议的自适应罚函数算法对神经常微方程模型的受约束模型具有更高的有效性和可靠性。此外，自适应罚approach还可以提供更准确和稳定的预测。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Interaction-Aware-Interval-Analysis-of-Neural-Network-Feedback-Loops"><a href="#Efficient-Interaction-Aware-Interval-Analysis-of-Neural-Network-Feedback-Loops" class="headerlink" title="Efficient Interaction-Aware Interval Analysis of Neural Network Feedback Loops"></a>Efficient Interaction-Aware Interval Analysis of Neural Network Feedback Loops</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14938">http://arxiv.org/abs/2307.14938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saber Jafarpour, Akash Harapanahalli, Samuel Coogan</li>
<li>for: 本文提出一种 computationally efficient 框架，用于计算 interval reachability 系统中 neural network 控制器的行为下的不确定性。</li>
<li>methods: 本文使用 inclusion functions 将开loop系统和神经网络控制器嵌入更大的 embedding system 中，以便使用单个轨迹来拟合原始系统的行为下 uncertainty。提出了两种不同的构建关闭loop embedding system 的方法，它们分别考虑了系统和控制器之间的交互方式。</li>
<li>results: 本文在 Python 框架 ReachMM 中实现了这种方法，并在各种例子和 benchmark 上示出了高效性和可扩展性。<details>
<summary>Abstract</summary>
In this paper, we propose a computationally efficient framework for interval reachability of systems with neural network controllers. Our approach leverages inclusion functions for the open-loop system and the neural network controller to embed the closed-loop system into a larger-dimensional embedding system, where a single trajectory over-approximates the original system's behavior under uncertainty. We propose two methods for constructing closed-loop embedding systems, which account for the interactions between the system and the controller in different ways. The interconnection-based approach considers the worst-case evolution of each coordinate separately by substituting the neural network inclusion function into the open-loop inclusion function. The interaction-based approach uses novel Jacobian-based inclusion functions to capture the first-order interactions between the open-loop system and the controller by leveraging state-of-the-art neural network verifiers. Finally, we implement our approach in a Python framework called ReachMM to demonstrate its efficiency and scalability on benchmarks and examples ranging to $200$ state dimensions.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种计算效率高的扩Interval可达性框架，用于系统控制器是神经网络的系统。我们的方法利用包含函数来包含开 Loop系统和神经网络控制器，将关闭Loop系统嵌入到更大的嵌入系统中，以便单个轨迹过度度量原始系统的行为下 uncertainty。我们提出了两种方法用于构建关闭Loop嵌入系统，这两种方法均考虑了系统和控制器之间的互动。基于连接的方法将每个坐标的最坏情况演化分别substitue神经网络包含函数到开 Loop包含函数中。基于互动的方法使用新的Jacobian包含函数来捕捉开 Loop系统和控制器之间的首次互动，通过利用现有神经网络验证器。最后，我们在Python框架ReachMM中实现了我们的方法，以示其效率和可扩展性。
</details></li>
</ul>
<hr>
<h2 id="PanGu-Coder2-Boosting-Large-Language-Models-for-Code-with-Ranking-Feedback"><a href="#PanGu-Coder2-Boosting-Large-Language-Models-for-Code-with-Ranking-Feedback" class="headerlink" title="PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback"></a>PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14936">http://arxiv.org/abs/2307.14936</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan Ji, Jingyang Zhao, Yuenan Guo, Qianxiang Wang</li>
<li>for: 本研究旨在提高预训练Code大语言模型（Code LLM）的代码生成性能。</li>
<li>methods: 本研究提出了一种新的RRTF（ Rank Responses to align Test&amp;Teacher Feedback）框架，用于有效地和高效地提高预训练Code LLM的代码生成性能。</li>
<li>results: 在OpenAI HumanEval benchmark上，PanGu-Coder2实现了62.20%的pass@1分数，并在CoderEval和LeetCode benchmark上表现出色，Consistently outperforming all previous Code LLMs。<details>
<summary>Abstract</summary>
Large Language Models for Code (Code LLM) are flourishing. New and powerful models are released on a weekly basis, demonstrating remarkable performance on the code generation task. Various approaches have been proposed to boost the code generation performance of pre-trained Code LLMs, such as supervised fine-tuning, instruction tuning, reinforcement learning, etc. In this paper, we propose a novel RRTF (Rank Responses to align Test&Teacher Feedback) framework, which can effectively and efficiently boost pre-trained large language models for code generation. Under this framework, we present PanGu-Coder2, which achieves 62.20% pass@1 on the OpenAI HumanEval benchmark. Furthermore, through an extensive evaluation on CoderEval and LeetCode benchmarks, we show that PanGu-Coder2 consistently outperforms all previous Code LLMs.
</details>
<details>
<summary>摘要</summary>
大型语言模型 для代码（代码 LLM）正在繁荣。每周新发布的新型号都在代码生成任务上表现出色。各种方法被提出来提高预训练代码 LLM 的代码生成性能，例如监督微调、指令优化、强化学习等。在这篇论文中，我们提出了一种新的 RRTF（排名回应对测试&教师反馈）框架，可以有效地和高效地提高预训练大型语言模型的代码生成性能。在这个框架下，我们介绍了 PanGu-Coder2，它在 OpenAI HumanEval benchmark 上取得了 62.20% 的 pass@1 分数。此外，通过对 CoderEval 和 LeetCode  benchmark 进行广泛的评估，我们表明 PanGu-Coder2 在所有前一代代码 LLM 之上卓越表现。
</details></li>
</ul>
<hr>
<h2 id="Solving-Data-Quality-Problems-with-Desbordante-a-Demo"><a href="#Solving-Data-Quality-Problems-with-Desbordante-a-Demo" class="headerlink" title="Solving Data Quality Problems with Desbordante: a Demo"></a>Solving Data Quality Problems with Desbordante: a Demo</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14935">http://arxiv.org/abs/2307.14935</a></li>
<li>repo_url: None</li>
<li>paper_authors: George Chernishev, Michael Polyntsov, Anton Chizhov, Kirill Stupakov, Ilya Shchuckin, Alexander Smirnov, Maxim Strutovsky, Alexey Shlyonskikh, Mikhail Firsov, Stepan Manannikov, Nikita Bobrov, Daniil Goncharov, Ilia Barutkin, Vladislav Shalnev, Kirill Muraviev, Anna Rakhmukova, Dmitriy Shcheka, Anton Chernikov, Mikhail Vyrodov, Yaroslav Kurbatov, Maxim Fofanov, Sergei Belokonnyi, Pavel Anosov, Arthur Saliou, Eduard Gaisin, Kirill Smirnov</li>
<li>for: 提高现代数据驱动行业中数据 profiling 的效率和可靠性，并提供对现有工具的良好整合。</li>
<li>methods: 使用可扩展的 C++ 核心，实现高效缓存和快速排序，并提供可靠的数据探索和检测功能。</li>
<li>results: 通过多种实际场景的示例，包括 typo 检测、数据重复检测和数据异常检测，表明 Desbordante 可以高效地解决不同的数据质量问题。<details>
<summary>Abstract</summary>
Data profiling is an essential process in modern data-driven industries. One of its critical components is the discovery and validation of complex statistics, including functional dependencies, data constraints, association rules, and others.   However, most existing data profiling systems that focus on complex statistics do not provide proper integration with the tools used by contemporary data scientists. This creates a significant barrier to the adoption of these tools in the industry. Moreover, existing systems were not created with industrial-grade workloads in mind. Finally, they do not aim to provide descriptive explanations, i.e. why a given pattern is not found. It is a significant issue as it is essential to understand the underlying reasons for a specific pattern's absence to make informed decisions based on the data.   Because of that, these patterns are effectively rest in thin air: their application scope is rather limited, they are rarely used by the broader public. At the same time, as we are going to demonstrate in this presentation, complex statistics can be efficiently used to solve many classic data quality problems.   Desbordante is an open-source data profiler that aims to close this gap. It is built with emphasis on industrial application: it is efficient, scalable, resilient to crashes, and provides explanations. Furthermore, it provides seamless Python integration by offloading various costly operations to the C++ core, not only mining.   In this demonstration, we show several scenarios that allow end users to solve different data quality problems. Namely, we showcase typo detection, data deduplication, and data anomaly detection scenarios.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified ChineseData profiling 是现代数据驱动行业中的一个重要过程。其中一个关键组件是发现和验证复杂统计，包括函数依赖关系、数据约束、关联规则等。然而，大多数现有的数据 profiling 系统，它们专注于复杂统计，并不提供适合当代数据科学家使用的合理集成。这创造了一个很大的障碍物，阻碍了这些工具在行业中的普及。此外，现有系统并不是为现代工作荟载设计的。最重要的是，它们不提供描述性解释，即为什么某个模式不存在。这是一个重要的问题，因为需要理解数据下面的根本原因，才能基于数据做出 Informed 决策。因此，这些模式效果是“浮空”的，它们的应用范围很限定，并且只有少数人使用。在这个演示中，我们将展示一些使用复杂统计解决不同数据质量问题的场景。具体来说，我们将展示 typo 检测、数据减重、数据异常检测等场景。Desbordante 是一个开源的数据 profiler，它专注于工业应用。它具有高效、可扩展、可靠性和描述性解释等特点。此外，它提供了PYTHON 集成，通过将多种费时操作委托给 C++ 核心来实现，不仅是探采。在这个演示中，我们将展示 Desbordante 如何解决不同数据质量问题。>>Here's the translation:<<SYS>>现代数据驱动行业中的一个重要过程是数据 profiling。这个过程中的一个关键组件是发现和验证复杂统计，例如函数依赖关系、数据约束、关联规则等。然而，大多数现有的数据 profiling 系统不适合当代数据科学家使用，因为它们不提供适合工业应用的合理集成。这创造了一个很大的障碍物，阻碍了这些工具在行业中的普及。现有系统并不是为现代工作荟载设计的。最重要的是，它们不提供描述性解释，即为什么某个模式不存在。这是一个重要的问题，因为需要理解数据下面的根本原因，才能基于数据做出 Informed 决策。因此，这些模式效果是“浮空”的，它们的应用范围很限定，并且只有少数人使用。在这个演示中，我们将展示一些使用复杂统计解决不同数据质量问题的场景。具体来说，我们将展示 typo 检测、数据减重、数据异常检测等场景。Desbordante 是一个开源的数据 profiler，它专注于工业应用。它具有高效、可扩展、可靠性和描述性解释等特点。此外，它提供了PYTHON 集成，通过将多种费时操作委托给 C++ 核心来实现，不仅是探采。在这个演示中，我们将展示 Desbordante 如何解决不同数据质量问题。
</details></li>
</ul>
<hr>
<h2 id="Approximate-Model-Based-Shielding-for-Safe-Reinforcement-Learning"><a href="#Approximate-Model-Based-Shielding-for-Safe-Reinforcement-Learning" class="headerlink" title="Approximate Model-Based Shielding for Safe Reinforcement Learning"></a>Approximate Model-Based Shielding for Safe Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00707">http://arxiv.org/abs/2308.00707</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sacktock/ambs">https://github.com/sacktock/ambs</a></li>
<li>paper_authors: Alexander W. Goodall, Francesco Belardinelli</li>
<li>for: 这篇论文目的是为了解决RL在实际世界中应用的问题，特别是在安全关键系统中。</li>
<li>methods: 这篇论文提出了一种名为approximate model-based shielding（AMBS）的原则，用于验证RL策略是否符合给定的安全约束。AMBS不需要知道系统的安全相关动态。</li>
<li>results: 论文提供了一个强有力的理论依据，并在一组Atari游戏中表明AMBS在安全意识方面表现出色，超过其他安全意识方法。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) has shown great potential for solving complex tasks in a variety of domains. However, applying RL to safety-critical systems in the real-world is not easy as many algorithms are sample-inefficient and maximising the standard RL objective comes with no guarantees on worst-case performance. In this paper we propose approximate model-based shielding (AMBS), a principled look-ahead shielding algorithm for verifying the performance of learned RL policies w.r.t. a set of given safety constraints. Our algorithm differs from other shielding approaches in that it does not require prior knowledge of the safety-relevant dynamics of the system. We provide a strong theoretical justification for AMBS and demonstrate superior performance to other safety-aware approaches on a set of Atari games with state-dependent safety-labels.
</details>
<details>
<summary>摘要</summary>
利用增强学习（RL）解决复杂任务的潜力已经被证明了，但是在实际世界中应用RL到安全关键系统上并不容易，因为许多算法是采样不充分的，并且最大化标准RL目标不提供最坏情况性能的保证。在这篇论文中，我们提出了一种名为准确模型基于遮盾（AMBS）的原则正确的遮盾算法，用于验证RL策略与一组给定的安全约束之间的关系。我们的算法与其他安全意识的方法不同，不需要知道系统的安全相关动力学。我们提供了强有力的理论基础，并在一组Atari游戏中的状态依赖安全标签上示出了我们的方法的超越性。
</details></li>
</ul>
<hr>
<h2 id="Graph-based-Polyphonic-Multitrack-Music-Generation"><a href="#Graph-based-Polyphonic-Multitrack-Music-Generation" class="headerlink" title="Graph-based Polyphonic Multitrack Music Generation"></a>Graph-based Polyphonic Multitrack Music Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14928">http://arxiv.org/abs/2307.14928</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/emanuelecosenza/polyphemus">https://github.com/emanuelecosenza/polyphemus</a></li>
<li>paper_authors: Emanuele Cosenza, Andrea Valenti, Davide Bacciu</li>
<li>for: 这篇论文是为了研究用深度学习系统来生成乐曲，特别是使用图表来模型多重符号音乐。</li>
<li>methods: 该论文提出了一种新的图表表示法，并使用深度Variational Autoencoder来生成乐曲的结构和内容。这种方法可以根据指定的乐器和时间来控制生成过程，从而实现人机交互式的音乐合作。</li>
<li>results: 经过训练的模型能够生成愉悦的短和长乐曲，并可以实际地 interpolate  между它们，生成具有律动和和声一致性的乐曲。图表可视化表明模型可以将其隐藏空间按照知道的音乐概念进行组织。<details>
<summary>Abstract</summary>
Graphs can be leveraged to model polyphonic multitrack symbolic music, where notes, chords and entire sections may be linked at different levels of the musical hierarchy by tonal and rhythmic relationships. Nonetheless, there is a lack of works that consider graph representations in the context of deep learning systems for music generation. This paper bridges this gap by introducing a novel graph representation for music and a deep Variational Autoencoder that generates the structure and the content of musical graphs separately, one after the other, with a hierarchical architecture that matches the structural priors of music. By separating the structure and content of musical graphs, it is possible to condition generation by specifying which instruments are played at certain times. This opens the door to a new form of human-computer interaction in the context of music co-creation. After training the model on existing MIDI datasets, the experiments show that the model is able to generate appealing short and long musical sequences and to realistically interpolate between them, producing music that is tonally and rhythmically consistent. Finally, the visualization of the embeddings shows that the model is able to organize its latent space in accordance with known musical concepts.
</details>
<details>
<summary>摘要</summary>
图可以用来模拟多音轨 симвоlic music， где每个音和每个和声可以在不同的音乐层次结构中相互关联。然而，现有的研究很少考虑图表 Representation在深度学习系统中的应用。这篇论文填补了这个空白，并 introduce a novel graph representation for music and a deep Variational Autoencoder that generates the structure and content of musical graphs separately, one after the other, with a hierarchical architecture that matches the structural priors of music.通过分离音乐图的结构和内容，可以根据特定的乐器和时间指定生成。这打开了一种新的人机交互方式，即音乐合作创作。经过训练模型于现有的 MIDI 数据集，实验显示，模型能够生成愉悦的短和长乐 sequences，并可以实际地在之间 interpolate，生成具有和谐和节奏的音乐。最后，Embeddings 的可视化表明，模型能够在 latent space 中组织 according to known musical concepts。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Performance-of-Deep-Learning-Model-for-Material-Segmentation-on-Two-HPC-Systems"><a href="#Benchmarking-Performance-of-Deep-Learning-Model-for-Material-Segmentation-on-Two-HPC-Systems" class="headerlink" title="Benchmarking Performance of Deep Learning Model for Material Segmentation on Two HPC Systems"></a>Benchmarking Performance of Deep Learning Model for Material Segmentation on Two HPC Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14921">http://arxiv.org/abs/2307.14921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Warren R. Williams, S. Ross Glandon, Luke L. Morris, Jing-Ru C. Cheng</li>
<li>for: 本研究的目的是提供高性能HPC系统的性能评估方法，以便提高任务调度器的性能。</li>
<li>methods: 本研究开发了一种基于机器学习模型的benchmark工具，该工具在GPU加速节点上进行物质分 segmentation分析，并使用MMdnn工具库和MINC-2500数据集。</li>
<li>results: 对两个ERDC DSRC系统（Onyx和Vulcanite）进行性能测试，结果显示，虽然Vulcanite在许多benchmark中具有更快的模型时间，但它也更容易受到环境因素的影响，导致性能下降，而Onyx的模型时间在benchmark中具有更高的一致性。<details>
<summary>Abstract</summary>
Performance Benchmarking of HPC systems is an ongoing effort that seeks to provide information that will allow for increased performance and improve the job schedulers that manage these systems. We develop a benchmarking tool that utilizes machine learning models and gathers performance data on GPU-accelerated nodes while they perform material segmentation analysis. The benchmark uses a ML model that has been converted from Caffe to PyTorch using the MMdnn toolkit and the MINC-2500 dataset. Performance data is gathered on two ERDC DSRC systems, Onyx and Vulcanite. The data reveals that while Vulcanite has faster model times in a large number of benchmarks, and it is also more subject to some environmental factors that can cause performances slower than Onyx. In contrast the model times from Onyx are consistent across benchmarks.
</details>
<details>
<summary>摘要</summary>
高性能计算机系统的性能测试是一项不断进行的努力，旨在提供可以提高性能和改进作业调度器的信息。我们开发了一个性能测试工具，该工具利用机器学习模型并在加速节点上进行材料分割分析时收集性能数据。这个benchmark使用通过Caffe到PyTorch的MMdnn工具包和MINC-2500数据集 converts的ML模型。在ERDC DSRC系统Onyx和Vulcanite上进行性能测试，数据显示，虽然Vulcanite在许多benchmark中具有更快的模型时间，但它也更容易受到环境因素的影响，导致性能 slower than Onyx。相比之下，Onyx上的模型时间在benchmark中具有一致性。
</details></li>
</ul>
<hr>
<h2 id="NSA-Naturalistic-Support-Artifact-to-Boost-Network-Confidence"><a href="#NSA-Naturalistic-Support-Artifact-to-Boost-Network-Confidence" class="headerlink" title="NSA: Naturalistic Support Artifact to Boost Network Confidence"></a>NSA: Naturalistic Support Artifact to Boost Network Confidence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14917">http://arxiv.org/abs/2307.14917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhijith Sharma, Phil Munz, Apurva Narayan<br>for: 这种研究旨在提高视觉人工智能系统对自然环境的抗衰减能力。methods: 该研究提议使用自然支持物件（NSA）来提高预测confidence分数。NSA通过DC-GAN进行artifact Training，以确保在场景中具有高视觉准确性。results: 对于Imagenette dataset中的自然衰减，NSA能够提高预测confidence分数四倍。此外，NSA还能够提高对抗攻击的准确率平均提高8%。此外，通过精度图来分析NSA的作用，可以了解它们如何提高预测confidence。<details>
<summary>Abstract</summary>
Visual AI systems are vulnerable to natural and synthetic physical corruption in the real-world. Such corruption often arises unexpectedly and alters the model's performance. In recent years, the primary focus has been on adversarial attacks. However, natural corruptions (e.g., snow, fog, dust) are an omnipresent threat to visual AI systems and should be considered equally important. Many existing works propose interesting solutions to train robust models against natural corruption. These works either leverage image augmentations, which come with the additional cost of model training, or place suspicious patches in the scene to design unadversarial examples. In this work, we propose the idea of naturalistic support artifacts (NSA) for robust prediction. The NSAs are shown to be beneficial in scenarios where model parameters are inaccessible and adding artifacts in the scene is feasible. The NSAs are natural looking objects generated through artifact training using DC-GAN to have high visual fidelity in the scene. We test against natural corruptions on the Imagenette dataset and observe the improvement in prediction confidence score by four times. We also demonstrate NSA's capability to increase adversarial accuracy by 8\% on average. Lastly, we qualitatively analyze NSAs using saliency maps to understand how they help improve prediction confidence.
</details>
<details>
<summary>摘要</summary>
视觉人工智能系统容易受到自然和人工物理损害的影响，这种损害通常会不断发生，影响模型的性能。在过去几年，主要关注点是对抗性攻击。然而，自然损害（如雪、雾、尘埃）对视觉人工智能系统是一种普遍存在的威胁，应该得到同等重视。许多现有的研究提出了许多有趣的解决方案，如通过图像扩展来训练Robust模型，或者在场景中添加不可信的质地来设计不可 adversarial例子。在这个工作中，我们提出了自然支持物件（NSA）的想法，用于Robust预测。NSA通过使用DC-GAN进行 artifact训练，在场景中生成高可识别度的自然looking对象。我们对Imagenette数据集进行了对自然损害的测试，并观察到预测信心分数提高四倍。此外，我们还证明NSA可以提高对抗率平均8%。最后，我们使用saliency map来 качеitative分析NSA，以便更好地理解它们如何提高预测 confidence。
</details></li>
</ul>
<hr>
<h2 id="Clustering-of-illustrations-by-atmosphere-using-a-combination-of-supervised-and-unsupervised-learning"><a href="#Clustering-of-illustrations-by-atmosphere-using-a-combination-of-supervised-and-unsupervised-learning" class="headerlink" title="Clustering of illustrations by atmosphere using a combination of supervised and unsupervised learning"></a>Clustering of illustrations by atmosphere using a combination of supervised and unsupervised learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15099">http://arxiv.org/abs/2307.15099</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keisuke Kubota, Masahiro Okuda<br>for:这 paper 是为了解决图像描述中的氛围问题，通过将图像分类为不同的氛围来提高搜索和推荐的效果。methods:这 paper 使用了双向学习和自动标注来解决氛围分类问题，并使用了supervised learning和Unsupervised learning来获得特征向量。results:实验结果表明，这 paper 的方法可以比传统方法更好地对图像进行分类，并且可以准确地捕捉图像中的氛围特征。<details>
<summary>Abstract</summary>
The distribution of illustrations on social media, such as Twitter and Pixiv has increased with the growing popularity of animation, games, and animated movies. The "atmosphere" of illustrations plays an important role in user preferences. Classifying illustrations by atmosphere can be helpful for recommendations and searches. However, assigning clear labels to the elusive "atmosphere" and conventional supervised classification is not always practical. Furthermore, even images with similar colors, edges, and low-level features may not have similar atmospheres, making classification based on low-level features challenging. In this paper, this problem is solved using both supervised and unsupervised learning with pseudo-labels. The feature vectors are obtained using the supervised method with pseudo-labels that contribute to an ambiguous atmosphere. Further, clustering is performed based on these feature vectors. Experimental analyses show that our method outperforms conventional methods in human-like clustering on datasets manually classified by humans.
</details>
<details>
<summary>摘要</summary>
社交媒体上的插图分布量增加，与动画、游戏和动画电影的流行相关。插图的“氛围”在用户喜好中扮演重要角色。根据氛围进行分类可以有助于推荐和搜索。但是，将氛围论坛到明确的标签是不实用的。此外，即使颜色、边缘和低级特征都相似，插图的氛围可能不同，从而使基于低级特征的分类困难。在这篇论文中，我们解决了这个问题，使用了超级vised和无监督学习，并使用pseudo标签。通过这些特征向量，我们进行了团 clustering。实验分析表明，我们的方法在人类化 clustering 中超出了传统方法。
</details></li>
</ul>
<hr>
<h2 id="Scaling-Session-Based-Transformer-Recommendations-using-Optimized-Negative-Sampling-and-Loss-Functions"><a href="#Scaling-Session-Based-Transformer-Recommendations-using-Optimized-Negative-Sampling-and-Loss-Functions" class="headerlink" title="Scaling Session-Based Transformer Recommendations using Optimized Negative Sampling and Loss Functions"></a>Scaling Session-Based Transformer Recommendations using Optimized Negative Sampling and Loss Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14906">http://arxiv.org/abs/2307.14906</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/otto-de/tron">https://github.com/otto-de/tron</a></li>
<li>paper_authors: Timo Wilm, Philipp Normann, Sophie Baumeister, Paul-Vincent Kobow</li>
<li>for: 这篇论文是为了提出一种可扩展的会话基于转换器推荐算法，以解决现有模型的缺乏扩展性和性能问题。</li>
<li>methods: 该论文使用了最佳负样本选择和列式损失函数，以提高推荐准确性。</li>
<li>results: 对于大规模电商数据集，TRON显示了与当前方法相比的推荐质量提高，同时保持与SASRec相似的训练速度。实际应用中，TRON实现了与SASRec相比18.14%的点击率提高。<details>
<summary>Abstract</summary>
This work introduces TRON, a scalable session-based Transformer Recommender using Optimized Negative-sampling. Motivated by the scalability and performance limitations of prevailing models such as SASRec and GRU4Rec+, TRON integrates top-k negative sampling and listwise loss functions to enhance its recommendation accuracy. Evaluations on relevant large-scale e-commerce datasets show that TRON improves upon the recommendation quality of current methods while maintaining training speeds similar to SASRec. A live A/B test yielded an 18.14% increase in click-through rate over SASRec, highlighting the potential of TRON in practical settings. For further research, we provide access to our source code at https://github.com/otto-de/TRON and an anonymized dataset at https://github.com/otto-de/recsys-dataset.
</details>
<details>
<summary>摘要</summary>
这个研究介绍了TRON，一种可扩展的会话基于转移器推荐器，使用优化的负样本选择。为了解决现有模型如SASRec和GRU4Rec+的可扩展性和性能限制，TRON integrates top-k负样本选择和listwise损失函数，以提高推荐准确性。对于相关的大规模电商数据集，评估表明TRON可以超越当前方法的推荐质量，同时保持与SASRec相似的训练速度。一次实际A/B测试显示，TRON相比SASRec提高了18.14%的点击率，这highlights TRON在实际场景中的潜力。为了进一步研究，我们在https://github.com/otto-de/TRON上提供了源代码，并在https://github.com/otto-de/recsys-dataset上提供了匿名数据集。
</details></li>
</ul>
<hr>
<h2 id="CodeLens-An-Interactive-Tool-for-Visualizing-Code-Representations"><a href="#CodeLens-An-Interactive-Tool-for-Visualizing-Code-Representations" class="headerlink" title="CodeLens: An Interactive Tool for Visualizing Code Representations"></a>CodeLens: An Interactive Tool for Visualizing Code Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14902">http://arxiv.org/abs/2307.14902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuejun Guo, Seifeddine Bettaieb, Qiang Hu, Yves Le Traon, Qiang Tang</li>
<li>for: 提供了一个可视化代码表示的工具，帮助开发者快速理解不同类型的代码表示，以及代码表示所代表的输入。</li>
<li>methods: 支持多种程式语言，包括Java、Python和JavaScript，并且支持四种代码表示方法，包括字符串序列、抽象 syntax tree (AST)、资料流graph (DFG) 和控制流graph (CFG)。</li>
<li>results: 为开发者提供了一个可用于多种代码表示的可视化互动环境，帮助开发者快速理解代码表示，并且获取代码表示所代表的输入。<details>
<summary>Abstract</summary>
Representing source code in a generic input format is crucial to automate software engineering tasks, e.g., applying machine learning algorithms to extract information. Visualizing code representations can further enable human experts to gain an intuitive insight into the code. Unfortunately, as of today, there is no universal tool that can simultaneously visualise different types of code representations. In this paper, we introduce a tool, CodeLens, which provides a visual interaction environment that supports various representation methods and helps developers understand and explore them. CodeLens is designed to support multiple programming languages, such as Java, Python, and JavaScript, and four types of code representations, including sequence of tokens, abstract syntax tree (AST), data flow graph (DFG), and control flow graph (CFG). By using CodeLens, developers can quickly visualize the specific code representation and also obtain the represented inputs for models of code. The Web-based interface of CodeLens is available at http://www.codelens.org. The demonstration video can be found at http://www.codelens.org/demo.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将源代码表示format化为通用输入格式是软件工程任务的关键之一，例如应用机器学习算法提取信息。可视化代码表示可以帮助人工专家获得直观印象。 unfortunately，到目前为止，没有一个通用工具可同时可视化不同类型的代码表示。在这篇论文中，我们介绍了一个工具——CodeLens，它提供了一个可视化交互环境，支持多种代码表示方法，帮助开发者理解和探索代码。CodeLens支持多种编程语言，如Java、Python和JavaScript，以及四种代码表示方法，包括序列化token、抽象语法树（AST）、数据流图（DFG）和控制流图（CFG）。通过使用CodeLens，开发者可快速可视化特定的代码表示，并获得代码表示的输入数据。Web-based Interface of CodeLens可在http://www.codelens.org上获取。示例视频可在http://www.codelens.org/demo找到。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Learning-for-Improved-Synthetic-Aperture-Sonar-Target-Recognition"><a href="#Self-Supervised-Learning-for-Improved-Synthetic-Aperture-Sonar-Target-Recognition" class="headerlink" title="Self-Supervised Learning for Improved Synthetic Aperture Sonar Target Recognition"></a>Self-Supervised Learning for Improved Synthetic Aperture Sonar Target Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15098">http://arxiv.org/abs/2307.15098</a></li>
<li>repo_url: None</li>
<li>paper_authors: BW Sheffield</li>
<li>for: 本研究探讨了基于自动学习（SSL）的目标识别方法在激光孔干成像中的应用，以解决水下环境下传统计算机视觉技术的不足。</li>
<li>methods: 本研究使用了两种知名的SSL算法：MoCov2和BYOL，以及一个常见的supervised learning模型：ResNet18，进行比较。</li>
<li>results: 结果表明，当使用少量标签时，SSL模型可以超过完全监督学习模型，但当使用所有标签时，它们不能超过完全监督学习模型。这些结果表明SSL可以作为监督学习的可靠替代方案，并且可以降低数据标签的时间和成本。<details>
<summary>Abstract</summary>
This study explores the application of self-supervised learning (SSL) for improved target recognition in synthetic aperture sonar (SAS) imagery. The unique challenges of underwater environments make traditional computer vision techniques, which rely heavily on optical camera imagery, less effective. SAS, with its ability to generate high-resolution imagery, emerges as a preferred choice for underwater imaging. However, the voluminous high-resolution SAS data presents a significant challenge for labeling; a crucial step for training deep neural networks (DNNs).   SSL, which enables models to learn features in data without the need for labels, is proposed as a potential solution to the data labeling challenge in SAS. The study evaluates the performance of two prominent SSL algorithms, MoCov2 and BYOL, against the well-regarded supervised learning model, ResNet18, for binary image classification tasks. The findings suggest that while both SSL models can outperform a fully supervised model with access to a small number of labels in a few-shot scenario, they do not exceed it when all the labels are used.   The results underscore the potential of SSL as a viable alternative to traditional supervised learning, capable of maintaining task performance while reducing the time and costs associated with data labeling. The study also contributes to the growing body of evidence supporting the use of SSL in remote sensing and could stimulate further research in this area.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Cascaded-Cross-Modal-Transformer-for-Request-and-Complaint-Detection"><a href="#Cascaded-Cross-Modal-Transformer-for-Request-and-Complaint-Detection" class="headerlink" title="Cascaded Cross-Modal Transformer for Request and Complaint Detection"></a>Cascaded Cross-Modal Transformer for Request and Complaint Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15097">http://arxiv.org/abs/2307.15097</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ristea/ccmt">https://github.com/ristea/ccmt</a></li>
<li>paper_authors: Nicolae-Catalin Ristea, Radu Tudor Ionescu</li>
<li>for: 检测客户请求和投诉在电话对话中</li>
<li>methods:  combine speech和文本译本使用自动语音识别（ASR）模型和不同语言BERT基于模型，并使用wave2vec2.0音频特征</li>
<li>results: 在ACM Multimedia 2023 Computational Paralinguistics Challenge的请求子挑战中，我们的系统达到了不Weighted average recall（UAR）的65.41%和85.87%，分别对应于投诉和请求类别。<details>
<summary>Abstract</summary>
We propose a novel cascaded cross-modal transformer (CCMT) that combines speech and text transcripts to detect customer requests and complaints in phone conversations. Our approach leverages a multimodal paradigm by transcribing the speech using automatic speech recognition (ASR) models and translating the transcripts into different languages. Subsequently, we combine language-specific BERT-based models with Wav2Vec2.0 audio features in a novel cascaded cross-attention transformer model. We apply our system to the Requests Sub-Challenge of the ACM Multimedia 2023 Computational Paralinguistics Challenge, reaching unweighted average recalls (UAR) of 65.41% and 85.87% for the complaint and request classes, respectively.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的层次跨模态转换器（CCMT），该模型结合语音和文本译文来检测电话对话中的客户请求和投诉。我们的方法采用多模式观念，通过自动语音识别（ASR）模型将语音转录为不同语言的文本，然后将语言特定的 BERT 基于模型与 Wav2Vec2.0 音频特征在一种新的层次跨注意力转换器模型中结合。我们对 ACM Multimedia 2023 计算语言学挑战的请求子挑战问题进行应用，实现无权重平均回归率（UAR）为 65.41% 和 85.87%  для投诉和请求类别 respectively。
</details></li>
</ul>
<hr>
<h2 id="Generative-convective-parametrization-of-dry-atmospheric-boundary-layer"><a href="#Generative-convective-parametrization-of-dry-atmospheric-boundary-layer" class="headerlink" title="Generative convective parametrization of dry atmospheric boundary layer"></a>Generative convective parametrization of dry atmospheric boundary layer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14857">http://arxiv.org/abs/2307.14857</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florian Heyder, Juan Pedro Mellado, Jörg Schumacher</li>
<li>for: 这个论文的目的是为了开发一种基于生成 adversarial network的湍流 parametrization，用于 kilometer-scale Earth system models。</li>
<li>methods: 这个论文使用了生成机器学习算法，基于 direct numerical simulation 数据进行训练。</li>
<li>results: 这个模型能够预测湍流场的非均匀统计特征，包括气压波的强度和高度分布，以及气压波的横向组织结构。模型的预测结果与标准两方程或多气流扩展模型相符。<details>
<summary>Abstract</summary>
Turbulence parametrizations will remain a necessary building block in kilometer-scale Earth system models. In convective boundary layers, where the mean vertical gradients of conserved properties such as potential temperature and moisture are approximately zero, the standard ansatz which relates turbulent fluxes to mean vertical gradients via an eddy diffusivity has to be extended by mass flux parametrizations for the typically asymmetric up- and downdrafts in the atmospheric boundary layer. In this work, we present a parametrization for a dry convective boundary layer based on a generative adversarial network. The model incorporates the physics of self-similar layer growth following from the classical mixed layer theory by Deardorff. This enhances the training data base of the generative machine learning algorithm and thus significantly improves the predicted statistics of the synthetically generated turbulence fields at different heights inside the boundary layer. The algorithm training is based on fully three-dimensional direct numerical simulation data. Differently to stochastic parametrizations, our model is able to predict the highly non-Gaussian transient statistics of buoyancy fluctuations, vertical velocity, and buoyancy flux at different heights thus also capturing the fastest thermals penetrating into the stabilized top region. The results of our generative algorithm agree with standard two-equation or multi-plume stochastic mass-flux schemes. The present parametrization provides additionally the granule-type horizontal organization of the turbulent convection which cannot be obtained in any of the other model closures. Our work paves the way to efficient data-driven convective parametrizations in other natural flows, such as moist convection, upper ocean mixing, or convection in stellar interiors.
</details>
<details>
<summary>摘要</summary>
“湍流 parametrizations 将继续作为 Earth 系统模型中必需的构建块。在湍流边层中， где保守量的平均垂直梯度近乎为零，标准推理，将湍流 fluxes 关联到平均垂直梯度via 湍流散度，需要进一步扩展为质量流 parametrizations。在这个工作中，我们提出了一种基于生成对抗网络的湍流 parametrization。该模型包括自similar层成长的物理学，这将提高训练数据集的生成机器学习算法，并因此改善预测在不同高度内边层的湍流场的统计。我们的模型训练基于三维直接数值计算数据。与抽象参量化不同，我们的模型能够预测湍流场的非高斯散度特征，包括垂直速度、湍流 flux 和湍流能量的快速变化。我们的结果与标准两方程或多柱抽象液体散度模型相符。我们的 parametrization 提供了扩展的 granule 类型的湍流组织，这不能在其他任何模型 closure 中获得。我们的工作开创了数据驱动的湍流 parametrization 的可能性，可以应用于其他自然流体中，如湿气湍流、上层水温混合或星系内部湍流。”
</details></li>
</ul>
<hr>
<h2 id="Counterfactual-Explanations-for-Graph-Classification-Through-the-Lenses-of-Density"><a href="#Counterfactual-Explanations-for-Graph-Classification-Through-the-Lenses-of-Density" class="headerlink" title="Counterfactual Explanations for Graph Classification Through the Lenses of Density"></a>Counterfactual Explanations for Graph Classification Through the Lenses of Density</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14849">http://arxiv.org/abs/2307.14849</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/carlo-abrate/Counterfactual-Explanations-for-Graph-Classification-Through-the-Lenses-of-Density">https://github.com/carlo-abrate/Counterfactual-Explanations-for-Graph-Classification-Through-the-Lenses-of-Density</a></li>
<li>paper_authors: Carlo Abrate, Giulia Preti, Francesco Bonchi</li>
<li>for: 这paper主要用于提出一种基于浓度的对比例例行对图像分类器的解释方法，以便更好地理解图像分类器的决策过程。</li>
<li>methods: 该paper使用了一种通过调整图像中的稠密结构来生成对比例例行的图像，包括打开或关闭三角形、以及基于最大 clique的方法。</li>
<li>results: 实验结果表明，采用浓度作为对比例例行的单位可以生成更加灵活和可读的解释方法，并且可以在7个大脑网络数据集上证明这种方法的有效性。<details>
<summary>Abstract</summary>
Counterfactual examples have emerged as an effective approach to produce simple and understandable post-hoc explanations. In the context of graph classification, previous work has focused on generating counterfactual explanations by manipulating the most elementary units of a graph, i.e., removing an existing edge, or adding a non-existing one. In this paper, we claim that such language of explanation might be too fine-grained, and turn our attention to some of the main characterizing features of real-world complex networks, such as the tendency to close triangles, the existence of recurring motifs, and the organization into dense modules. We thus define a general density-based counterfactual search framework to generate instance-level counterfactual explanations for graph classifiers, which can be instantiated with different notions of dense substructures. In particular, we show two specific instantiations of this general framework: a method that searches for counterfactual graphs by opening or closing triangles, and a method driven by maximal cliques. We also discuss how the general method can be instantiated to exploit any other notion of dense substructures, including, for instance, a given taxonomy of nodes. We evaluate the effectiveness of our approaches in 7 brain network datasets and compare the counterfactual statements generated according to several widely-used metrics. Results confirm that adopting a semantic-relevant unit of change like density is essential to define versatile and interpretable counterfactual explanation methods.
</details>
<details>
<summary>摘要</summary>
counterfactual 例子在 graph classification 中 emerged as an effective approach to produce simple and understandable post-hoc explanations. In the context of graph classification, previous work has focused on generating counterfactual explanations by manipulating the most elementary units of a graph, such as removing an existing edge, or adding a non-existing one. In this paper, we claim that such language of explanation might be too fine-grained, and turn our attention to some of the main characterizing features of real-world complex networks, such as the tendency to close triangles, the existence of recurring motifs, and the organization into dense modules. We thus define a general density-based counterfactual search framework to generate instance-level counterfactual explanations for graph classifiers, which can be instantiated with different notions of dense substructures. In particular, we show two specific instantiations of this general framework: a method that searches for counterfactual graphs by opening or closing triangles, and a method driven by maximal cliques. We also discuss how the general method can be instantiated to exploit any other notion of dense substructures, including, for instance, a given taxonomy of nodes. We evaluate the effectiveness of our approaches in 7 brain network datasets and compare the counterfactual statements generated according to several widely-used metrics. Results confirm that adopting a semantic-relevant unit of change like density is essential to define versatile and interpretable counterfactual explanation methods.
</details></li>
</ul>
<hr>
<h2 id="Kernelised-Normalising-Flows"><a href="#Kernelised-Normalising-Flows" class="headerlink" title="Kernelised Normalising Flows"></a>Kernelised Normalising Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14839">http://arxiv.org/abs/2307.14839</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eshant English, Matthias Kirchler, Christoph Lippert</li>
<li>for: 本研究旨在提出一种基于kernels的normalizing flows模型，以提高模型表达力和数据效应性。</li>
<li>methods: 该模型采用了kernel化的方法，将传统的神经网络变换替换为kernel化的变换，以提高模型的表达能力和参数效率。</li>
<li>results: 实验结果表明，kernel化的normalizing flows模型可以与基于神经网络的模型相比，在低数据量 régime下具有竞争力或更高的表达能力，同时具有更好的数据效应性。<details>
<summary>Abstract</summary>
Normalising Flows are generative models characterised by their invertible architecture. However, the requirement of invertibility imposes constraints on their expressiveness, necessitating a large number of parameters and innovative architectural designs to achieve satisfactory outcomes. Whilst flow-based models predominantly rely on neural-network-based transformations for expressive designs, alternative transformation methods have received limited attention. In this work, we present Ferumal flow, a novel kernelised normalising flow paradigm that integrates kernels into the framework. Our results demonstrate that a kernelised flow can yield competitive or superior results compared to neural network-based flows whilst maintaining parameter efficiency. Kernelised flows excel especially in the low-data regime, enabling flexible non-parametric density estimation in applications with sparse data availability.
</details>
<details>
<summary>摘要</summary>
通常的流程模型具有可逆的架构，但是这种要求对表达能力带来限制，因此需要许多参数和创新的架构来实现可Acceptable的结果。而流程模型主要依靠神经网络变换来实现表达设计，而其他变换方法受到了有限的关注。在这项工作中，我们介绍了 Ferumal flow，一种新的内核化正常流程模型，它将内核 integrate into the framework。我们的结果表明，内核化流可以与神经网络基于的流相比，在参数效率方面具有竞争力，并且在数据稀缺情况下表现特别出色。
</details></li>
</ul>
<hr>
<h2 id="Building-RadiologyNET-Unsupervised-annotation-of-a-large-scale-multimodal-medical-database"><a href="#Building-RadiologyNET-Unsupervised-annotation-of-a-large-scale-multimodal-medical-database" class="headerlink" title="Building RadiologyNET: Unsupervised annotation of a large-scale multimodal medical database"></a>Building RadiologyNET: Unsupervised annotation of a large-scale multimodal medical database</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08517">http://arxiv.org/abs/2308.08517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mateja Napravnik, Franko Hržić, Sebastian Tschauner, Ivan Štajduhar<br>for: This paper aims to address the challenge of annotating large medical radiology image databases by proposing an automated, unsupervised approach for clustering images based on their semantic similarity.methods: The proposed approach uses a combination of feature extractors from multiple data sources, including images, DICOM metadata, and narrative diagnoses. The features are then integrated into a multimodal representation and clustered using k-means and k-medoids algorithms.results: The results show that fusing the embeddings of all three data sources together results in the most concise clusters, indicating that this approach is effective in unsupervised clustering of large-scale medical data. The proposed method has the potential to create a much larger and more fine-grained annotated dataset of medical radiology images.<details>
<summary>Abstract</summary>
Background and objective: The usage of machine learning in medical diagnosis and treatment has witnessed significant growth in recent years through the development of computer-aided diagnosis systems that are often relying on annotated medical radiology images. However, the availability of large annotated image datasets remains a major obstacle since the process of annotation is time-consuming and costly. This paper explores how to automatically annotate a database of medical radiology images with regard to their semantic similarity.   Material and methods: An automated, unsupervised approach is used to construct a large annotated dataset of medical radiology images originating from Clinical Hospital Centre Rijeka, Croatia, utilising multimodal sources, including images, DICOM metadata, and narrative diagnoses. Several appropriate feature extractors are tested for each of the data sources, and their utility is evaluated using k-means and k-medoids clustering on a representative data subset.   Results: The optimal feature extractors are then integrated into a multimodal representation, which is then clustered to create an automated pipeline for labelling a precursor dataset of 1,337,926 medical images into 50 clusters of visually similar images. The quality of the clusters is assessed by examining their homogeneity and mutual information, taking into account the anatomical region and modality representation.   Conclusion: The results suggest that fusing the embeddings of all three data sources together works best for the task of unsupervised clustering of large-scale medical data, resulting in the most concise clusters. Hence, this work is the first step towards building a much larger and more fine-grained annotated dataset of medical radiology images.
</details>
<details>
<summary>摘要</summary>
背景和目标：随着医疗机器学习技术的发展，医疗诊断和治疗中的计算机支持诊断系统得到了广泛应用，但大量注释医疗影像数据集的可 availability 仍然是一个主要障碍。这篇论文探讨如何自动注释医疗影像数据库中的semantic similarity。材料和方法：使用自动化、无监督的方法，使用来自克林尼克医院中心的医疗影像数据，包括图像、DICOM元数据和描述诊断。对每种数据源，选择合适的特征提取器，并对选择的数据子集进行k-means和k-medoids归一化 clustering。结果：最佳特征提取器被集成到多模式表示中，并对 precursor 数据集进行自动化标注，将1,337,926个医疗影像分为50个视觉相似的集群。评估结果表明，将所有数据源的嵌入都 fusion 起来是最佳的选择，可以获得最紧凑的集群。因此，这是建立更大和更细化的注释医疗影像数据集的第一步。
</details></li>
</ul>
<hr>
<h2 id="Fading-memory-as-inductive-bias-in-residual-recurrent-networks"><a href="#Fading-memory-as-inductive-bias-in-residual-recurrent-networks" class="headerlink" title="Fading memory as inductive bias in residual recurrent networks"></a>Fading memory as inductive bias in residual recurrent networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14823">http://arxiv.org/abs/2307.14823</a></li>
<li>repo_url: None</li>
<li>paper_authors: Igor Dubinin, Felix Effenberger</li>
<li>for: 这 paper 探讨了 residual connections 在 RNN 中的影响，以提高 task 性能和网络动态特性。</li>
<li>methods: 作者使用了 weakly coupled residual recurrent networks (WCRNNs)，并 investigate 了 residual connections 对网络性能、动态特性和记忆特性的影响。</li>
<li>results: 研究发现，不同类型的 residual connections 可以提供不同的 inductive bias，提高网络表达能力。 especailly, residual connections 可以使网络在静止边缘附近的动态特性， capitalize 数据特征的spectral properties，以及实现异质记忆特性。 更进一步，作者还展示了如何扩展到非线性 residual 和 introducing weakly coupled residual initialization scheme for Elman RNNs.<details>
<summary>Abstract</summary>
Residual connections have been proposed as architecture-based inductive bias to mitigate the problem of exploding and vanishing gradients and increase task performance in both feed-forward and recurrent networks (RNNs) when trained with the backpropagation algorithm. Yet, little is known about how residual connections in RNNs influence their dynamics and fading memory properties. Here, we introduce weakly coupled residual recurrent networks (WCRNNs) in which residual connections result in well-defined Lyapunov exponents and allow for studying properties of fading memory. We investigate how the residual connections of WCRNNs influence their performance, network dynamics, and memory properties on a set of benchmark tasks. We show that several distinct forms of residual connections yield effective inductive biases that result in increased network expressivity. In particular, residual connections that (i) result in network dynamics at the proximity of the edge of chaos, (ii) allow networks to capitalize on characteristic spectral properties of the data, and (iii) result in heterogeneous memory properties are shown to increase practical expressivity. In addition, we demonstrate how our results can be extended to non-linear residuals and introduce a weakly coupled residual initialization scheme that can be used for Elman RNNs
</details>
<details>
<summary>摘要</summary>
剩下的连接（residual connections）已经被提议为网络架构层基的偏好，以降低反向传播算法中的扩散和消失梯度问题，并提高任务性能。然而，关于具有剩下的连接的RNN（Recurrent Neural Network）的动态和忘记性特性所知之少。在这里，我们介绍了弱相互连接的剩下RNN（Weakly Coupled Residual Recurrent Network，WCRNN），其中剩下的连接导致明确的Lyapunov exponent，并使得研究RNN的忘记性特性变得可行。我们 investigate了WCRNN中剩下连接的影响，包括网络性能、网络动态和忘记性特性在内的几个任务 benchmark。我们发现，不同的剩下连接形式可以提供不同的偏好，以提高网络表达能力。具体来说，剩下连接可以：（i）导致网络动态在边缘附近，（ii）让网络利用数据的特征频率特性，以及（iii）使得网络具有不同的忘记性特性。此外，我们还证明了我们的结果可以扩展到非线性剩下连接，并提出了一种弱相互连接初始化方案，可以应用于Elman RNN。
</details></li>
</ul>
<hr>
<h2 id="Likely-Light-and-Accurate-Context-Free-Clusters-based-Trajectory-Prediction"><a href="#Likely-Light-and-Accurate-Context-Free-Clusters-based-Trajectory-Prediction" class="headerlink" title="Likely, Light, and Accurate Context-Free Clusters-based Trajectory Prediction"></a>Likely, Light, and Accurate Context-Free Clusters-based Trajectory Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14788">http://arxiv.org/abs/2307.14788</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiago Rodrigues de Almeida, Oscar Martinez Mozos</li>
<li>for: 预测道路交通网络中自主系统的未来路径，以适应未知性。</li>
<li>methods: 提出了一种多stage probabilistic方法，包括路径变换到位差空间、时间序列归一化、路径提议和排名提议。新引入了深度特征归一化方法，自适应GAN，可以更好地适应分布变化。</li>
<li>results: 对人行道和道路代理人 trajectory 数据进行比较，全系统超过了上下文深度生成模型，同时与点估计模型相当，可以准确预测道路交通网络中自主系统的未来路径。<details>
<summary>Abstract</summary>
Autonomous systems in the road transportation network require intelligent mechanisms that cope with uncertainty to foresee the future. In this paper, we propose a multi-stage probabilistic approach for trajectory forecasting: trajectory transformation to displacement space, clustering of displacement time series, trajectory proposals, and ranking proposals. We introduce a new deep feature clustering method, underlying self-conditioned GAN, which copes better with distribution shifts than traditional methods. Additionally, we propose novel distance-based ranking proposals to assign probabilities to the generated trajectories that are more efficient yet accurate than an auxiliary neural network. The overall system surpasses context-free deep generative models in human and road agents trajectory data while performing similarly to point estimators when comparing the most probable trajectory.
</details>
<details>
<summary>摘要</summary>
自动化系统在路运输网络中需要智能机制来预测未来。本文提出了一种多个阶段 probabilistic 方法 для路径预测：路径变换到位移空间，分聚运动时序序列，路径提议和排名提议。我们引入了一种新的深度特征划分方法，基于自我条件 GAN，可以更好地处理分布转移。此外，我们提出了一种新的距离基于排名提议，用于将生成的路径分配概率，这种方法比 auxiliary 神经网络更高效 yet 准确。总体系统在人员和道路代理 trajectory 数据中超越了上下文深度生成模型，同时与点估计相比，最有可能的路径预测的准确性也有所提高。
</details></li>
</ul>
<hr>
<h2 id="Emotion4MIDI-a-Lyrics-based-Emotion-Labeled-Symbolic-Music-Dataset"><a href="#Emotion4MIDI-a-Lyrics-based-Emotion-Labeled-Symbolic-Music-Dataset" class="headerlink" title="Emotion4MIDI: a Lyrics-based Emotion-Labeled Symbolic Music Dataset"></a>Emotion4MIDI: a Lyrics-based Emotion-Labeled Symbolic Music Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14783">http://arxiv.org/abs/2307.14783</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/serkansulun/lyricsemotions">https://github.com/serkansulun/lyricsemotions</a></li>
<li>paper_authors: Serkan Sulun, Pedro Oliveira, Paula Viana</li>
<li>for: 这个论文是为了创建一个大规模的情感标注的 симвоlic music dataset（12k首midi乐曲）而写的。</li>
<li>methods: 作者首先在GoEmotions dataset上训练了情感分类模型，实现了状态空间最佳的结果，并且使用这些模型对两个大规模的midi dataset中的歌词进行应用。</li>
<li>results: 该dataset覆盖了一系列细化的情感，为研究音乐和情感之间的连接，以及开发基于具体情感的音乐生成模型提供了一个非常有价值的资源。<details>
<summary>Abstract</summary>
We present a new large-scale emotion-labeled symbolic music dataset consisting of 12k MIDI songs. To create this dataset, we first trained emotion classification models on the GoEmotions dataset, achieving state-of-the-art results with a model half the size of the baseline. We then applied these models to lyrics from two large-scale MIDI datasets. Our dataset covers a wide range of fine-grained emotions, providing a valuable resource to explore the connection between music and emotions and, especially, to develop models that can generate music based on specific emotions. Our code for inference, trained models, and datasets are available online.
</details>
<details>
<summary>摘要</summary>
我们介绍了一个新的大规模情感标注的符号音乐数据集，包含12个MIDI歌曲。为创建这个数据集，我们首先在GoEmotions数据集上训练情感分类模型，实现了状态之arte的结果，模型半大小比基线。然后，我们应用了这些模型到两个大规模MIDI数据集中的歌词上。我们的数据集覆盖了各种细化的情感，提供了一个 ценный资源，探索音乐和情感之间的连接，特别是开发根据特定情感生成音乐的模型。我们在线上提供了推理代码、训练模型和数据集。
</details></li>
</ul>
<hr>
<h2 id="MATNilm-Multi-appliance-task-Non-intrusive-Load-Monitoring-with-Limited-Labeled-Data"><a href="#MATNilm-Multi-appliance-task-Non-intrusive-Load-Monitoring-with-Limited-Labeled-Data" class="headerlink" title="MATNilm: Multi-appliance-task Non-intrusive Load Monitoring with Limited Labeled Data"></a>MATNilm: Multi-appliance-task Non-intrusive Load Monitoring with Limited Labeled Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14778">http://arxiv.org/abs/2307.14778</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jxiong22/matnilm">https://github.com/jxiong22/matnilm</a></li>
<li>paper_authors: Jing Xiong, Tianqi Hong, Dongbo Zhao, Yu Zhang</li>
<li>for: 该论文目的是提出一种基于多应用程序任务框架和培根监测的非扰式电力监测（NILM）方法，以提高家庭应用程序的状态和消耗电力的精度和效率。</li>
<li>methods: 该方法使用了一种培根监测框架，其中每个应用程序都有一个共享层次拆分结构，以实现每个应用程序的回归和分类任务。此外，该方法还使用了一种两维注意机制，以捕捉所有应用程序之间的空间时间相关性。</li>
<li>results:  simulation results show that the proposed approach significantly improves the performance of NILM, with relative errors reduced by more than 50% on average. The approach also achieves comparable test performance with only one day of training data and limited appliance operation profiles.<details>
<summary>Abstract</summary>
Non-intrusive load monitoring (NILM) identifies the status and power consumption of various household appliances by disaggregating the total power usage signal of an entire house. Efficient and accurate load monitoring facilitates user profile establishment, intelligent household energy management, and peak load shifting. This is beneficial for both the end-users and utilities by improving the overall efficiency of a power distribution network. Existing approaches mainly focus on developing an individual model for each appliance. Those approaches typically rely on a large amount of household-labeled data which is hard to collect. In this paper, we propose a multi-appliance-task framework with a training-efficient sample augmentation (SA) scheme that boosts the disaggregation performance with limited labeled data. For each appliance, we develop a shared-hierarchical split structure for its regression and classification tasks. In addition, we also propose a two-dimensional attention mechanism in order to capture spatio-temporal correlations among all appliances. With only one-day training data and limited appliance operation profiles, the proposed SA algorithm can achieve comparable test performance to the case of training with the full dataset. Finally, simulation results show that our proposed approach features a significantly improved performance over many baseline models. The relative errors can be reduced by more than 50% on average. The codes of this work are available at https://github.com/jxiong22/MATNilm
</details>
<details>
<summary>摘要</summary>
非侵入式卷积监测（NILM）可以识别家庭各种设备的状态和能量消耗。这样的监测可以帮助建立用户profile，实现智能家庭能源管理和峰值负荷延迟。这对 endpoint 用户和供应商都是有利的，因为它可以提高总能源分配网络的效率。现有的方法主要集中在开发每个设备的个性化模型。这些方法通常需要大量的标注数据，但这些数据很难收集。在这篇论文中，我们提出了一个多设备任务框架，并使用训练效率高的样本扩展（SA）策略来提高分解性能。对于每个设备，我们开发了共享层次分割结构，用于其预测和分类任务。此外，我们还提出了两个维度的注意力机制，以捕捉所有设备之间的空间时间相关性。只需一天的训练数据和有限的设备操作 profiling，我们的SA算法可以达到与整个数据集训练时的比较好的测试性能。最后，我们的实验结果显示，我们的提案的方法在许多基线模型之上显示出了显著的改善。相对误差可以降低超过50%的平均值。代码这个工作可以在 <https://github.com/jxiong22/MATNilm> 查看。
</details></li>
</ul>
<hr>
<h2 id="Towards-Practicable-Sequential-Shift-Detectors"><a href="#Towards-Practicable-Sequential-Shift-Detectors" class="headerlink" title="Towards Practicable Sequential Shift Detectors"></a>Towards Practicable Sequential Shift Detectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14758">http://arxiv.org/abs/2307.14758</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oliver Cobb, Arnaud Van Looveren</li>
<li>for: 检测机器学习模型中的分布变化，以避免模型性能下降。</li>
<li>methods: existin works relevant to their satisfaction, and recommend impactful directions for future research.</li>
<li>results:  identificaiton of three desiderata crucial to the practicable deployment of sequential shift detectors.<details>
<summary>Abstract</summary>
There is a growing awareness of the harmful effects of distribution shift on the performance of deployed machine learning models. Consequently, there is a growing interest in detecting these shifts before associated costs have time to accumulate. However, desiderata of crucial importance to the practicable deployment of sequential shift detectors are typically overlooked by existing works, precluding their widespread adoption. We identify three such desiderata, highlight existing works relevant to their satisfaction, and recommend impactful directions for future research.
</details>
<details>
<summary>摘要</summary>
有增长的认识到分布转移对已经部署的机器学习模型表现的负面影响。因此，有增长的兴趣检测这些转移，以避免成本累累。然而，现有的工作通常忽视了重要的实用部署顺序转移检测的要求，这使得它们在实际应用中没有得到广泛采用。我们认为有三个如此的要求，提到现有的工作，并建议未来的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Fair-Machine-Unlearning-Data-Removal-while-Mitigating-Disparities"><a href="#Fair-Machine-Unlearning-Data-Removal-while-Mitigating-Disparities" class="headerlink" title="Fair Machine Unlearning: Data Removal while Mitigating Disparities"></a>Fair Machine Unlearning: Data Removal while Mitigating Disparities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14754">http://arxiv.org/abs/2307.14754</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex Oesterling, Jiaqi Ma, Flavio P. Calmon, Hima Lakkaraju</li>
<li>for: 本研究旨在提供一种可靠地忘记数据实例的机器学习方法，以保障个人隐私和公平性。</li>
<li>methods: 本研究使用了一种新的机器学习方法，可以高效地和可靠地忘记数据实例，同时保持集体公平性。</li>
<li>results: 实验结果表明，本方法可以高效地忘记数据实例，并且保持集体公平性。<details>
<summary>Abstract</summary>
As public consciousness regarding the collection and use of personal information by corporations grows, it is of increasing importance that consumers be active participants in the curation of corporate datasets. In light of this, data governance frameworks such as the General Data Protection Regulation (GDPR) have outlined the right to be forgotten as a key principle allowing individuals to request that their personal data be deleted from the databases and models used by organizations. To achieve forgetting in practice, several machine unlearning methods have been proposed to address the computational inefficiencies of retraining a model from scratch with each unlearning request. While efficient online alternatives to retraining, it is unclear how these methods impact other properties critical to real-world applications, such as fairness. In this work, we propose the first fair machine unlearning method that can provably and efficiently unlearn data instances while preserving group fairness. We derive theoretical results which demonstrate that our method can provably unlearn data instances while maintaining fairness objectives. Extensive experimentation with real-world datasets highlight the efficacy of our method at unlearning data instances while preserving fairness.
</details>
<details>
<summary>摘要</summary>
In this work, we propose the first fair machine unlearning method that can provably and efficiently unlearn data instances while preserving group fairness. We derive theoretical results that demonstrate our method can provably unlearn data instances while maintaining fairness objectives. Extensive experiments with real-world datasets show that our method is effective at unlearning data instances while preserving fairness.
</details></li>
</ul>
<hr>
<h2 id="FLARE-Fingerprinting-Deep-Reinforcement-Learning-Agents-using-Universal-Adversarial-Masks"><a href="#FLARE-Fingerprinting-Deep-Reinforcement-Learning-Agents-using-Universal-Adversarial-Masks" class="headerlink" title="FLARE: Fingerprinting Deep Reinforcement Learning Agents using Universal Adversarial Masks"></a>FLARE: Fingerprinting Deep Reinforcement Learning Agents using Universal Adversarial Masks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14751">http://arxiv.org/abs/2307.14751</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ssg-research/FLARE">https://github.com/ssg-research/FLARE</a></li>
<li>paper_authors: Buse G. A. Tekgul, N. Asokan</li>
<li>for: 防止深度强化学习策略（DRL）的非法复制和使用</li>
<li>methods: 使用非可转移的通用敌意掩蔽（perturbations）生成对抗示例，并将这些掩蔽用作指纹来验证盗取的DRL策略的真实所属</li>
<li>results: FLARE效果很好（100% 动作一致率），不会误告发独立策略（无false positives），并且对模型修改攻击和更有经验的敌对者进行攻击也具有较好的Robustness。<details>
<summary>Abstract</summary>
We propose FLARE, the first fingerprinting mechanism to verify whether a suspected Deep Reinforcement Learning (DRL) policy is an illegitimate copy of another (victim) policy. We first show that it is possible to find non-transferable, universal adversarial masks, i.e., perturbations, to generate adversarial examples that can successfully transfer from a victim policy to its modified versions but not to independently trained policies. FLARE employs these masks as fingerprints to verify the true ownership of stolen DRL policies by measuring an action agreement value over states perturbed via such masks. Our empirical evaluations show that FLARE is effective (100% action agreement on stolen copies) and does not falsely accuse independent policies (no false positives). FLARE is also robust to model modification attacks and cannot be easily evaded by more informed adversaries without negatively impacting agent performance. We also show that not all universal adversarial masks are suitable candidates for fingerprints due to the inherent characteristics of DRL policies. The spatio-temporal dynamics of DRL problems and sequential decision-making process make characterizing the decision boundary of DRL policies more difficult, as well as searching for universal masks that capture the geometry of it.
</details>
<details>
<summary>摘要</summary>
我们提出了FLARE，第一个验证怀疑深度强化学习（DRL）策略是否为另一个（受害者）策略的伪造 Mechanism。我们首先显示了可以找到不可转移的通用敌方攻击库（perturbations），即可以将攻击者从受害者策略中获得独特的攻击例子，但不能获得独立训练的策略中的攻击例子。FLARE使用这些库作为指纹，用于验证伪造的DRL策略的真实所有权。我们的实验评估显示FLARE有100%的动作一致率（action agreement），并不会误判独立的策略（no false positives）。FLARE还是对模型修改攻击和更 Informed 攻击者的攻击而言，不会轻松避免。我们还显示了不同的通用攻击库可能不适合指纹，因为深度强化学习问题的空间时间动态和继续决策过程使得characterizing DRL策略的决策边界更加困难，以及搜寻适合的通用库。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Image-Completion-and-Enhancement-using-GANs"><a href="#Semantic-Image-Completion-and-Enhancement-using-GANs" class="headerlink" title="Semantic Image Completion and Enhancement using GANs"></a>Semantic Image Completion and Enhancement using GANs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14748">http://arxiv.org/abs/2307.14748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Priyansh Saxena, Raahat Gupta, Akshat Maheshwari, Saumil Maheshwari</li>
<li>for: 这篇论文主要用于描述如何使用生成对抗网络（GAN）来实现图像完成和提高 зада务。</li>
<li>methods: 这篇论文使用的方法主要是基于GAN的架构，用于实现图像完成和提高。</li>
<li>results: 这篇论文的研究结果表明，GAN可以有效地完成和提高图像，并且可以保持图像的详细信息。<details>
<summary>Abstract</summary>
Semantic inpainting or image completion alludes to the task of inferring arbitrary large missing regions in images based on image semantics. Since the prediction of image pixels requires an indication of high-level context, this makes it significantly tougher than image completion, which is often more concerned with correcting data corruption and removing entire objects from the input image. On the other hand, image enhancement attempts to eliminate unwanted noise and blur from the image, along with sustaining most of the image details. Efficient image completion and enhancement model should be able to recover the corrupted and masked regions in images and then refine the image further to increase the quality of the output image. Generative Adversarial Networks (GAN), have turned out to be helpful in picture completion tasks. In this chapter, we will discuss the underlying GAN architecture and how they can be used used for image completion tasks.
</details>
<details>
<summary>摘要</summary>
semantic inpainting or image completion 涉及到根据图像 semantics 推断大量缺失区域的图像。由于预测图像像素需要高级上下文指示，这使得其 significatively 更加复杂于图像完成，而图像完成通常更关注于修复数据损害和从输入图像中除去 объек 的。然而，图像提高尝试去除不必要的噪声和模糊，同时保持大部分图像细节。高效的图像完成和提高模型应该能够回复缺失和masked 区域的图像，然后进一步提高输出图像的质量。生成对抗网络（GAN）在图像完成任务中表现出了有利的效果。在这章中，我们将讨论GAN的基本架构和如何使其用于图像完成任务。
</details></li>
</ul>
<hr>
<h2 id="A-Strategic-Framework-for-Optimal-Decisions-in-Football-1-vs-1-Shot-Taking-Situations-An-Integrated-Approach-of-Machine-Learning-Theory-Based-Modeling-and-Game-Theory"><a href="#A-Strategic-Framework-for-Optimal-Decisions-in-Football-1-vs-1-Shot-Taking-Situations-An-Integrated-Approach-of-Machine-Learning-Theory-Based-Modeling-and-Game-Theory" class="headerlink" title="A Strategic Framework for Optimal Decisions in Football 1-vs-1 Shot-Taking Situations: An Integrated Approach of Machine Learning, Theory-Based Modeling, and Game Theory"></a>A Strategic Framework for Optimal Decisions in Football 1-vs-1 Shot-Taking Situations: An Integrated Approach of Machine Learning, Theory-Based Modeling, and Game Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14732">http://arxiv.org/abs/2307.14732</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/calvinyeungck/analyzing-two-agents-interaction-in-football-shot-taking-situations">https://github.com/calvinyeungck/analyzing-two-agents-interaction-in-football-shot-taking-situations</a></li>
<li>paper_authors: Calvin C. K. Yeung, Keisuke Fujii<br>for:这篇论文的目的是分析足球比赛中攻击者和防守者之间的复杂互动，并提供一个数据驱动和理论基础的分析方法。methods:这篇论文使用了游戏理论和机器学习模型来分析攻击者和防守者之间的竞争情况，并提出了一个新的评价指标——预期进球目标概率（xSOT），以评价球员的行为，即使投射不中目标也能够提供有价值的信息。results:经验 validate了这个框架，并与基线和减少模型进行比较。此外，发现xSOT和现有指标之间存在高度的相似性，这表明xSOT提供了有价值的信息。最后，通过对2022年世界杯和2020年欧洲锦标赛的一个投射情况进行分析， illustrate了这个框架的应用。<details>
<summary>Abstract</summary>
Complex interactions between two opposing agents frequently occur in domains of machine learning, game theory, and other application domains. Quantitatively analyzing the strategies involved can provide an objective basis for decision-making. One such critical scenario is shot-taking in football, where decisions, such as whether the attacker should shoot or pass the ball and whether the defender should attempt to block the shot, play a crucial role in the outcome of the game. However, there are currently no effective data-driven and/or theory-based approaches to analyzing such situations. To address this issue, we proposed a novel framework to analyze such scenarios based on game theory, where we estimate the expected payoff with machine learning (ML) models, and additional features for ML models were extracted with a theory-based shot block model. Conventionally, successes or failures (1 or 0) are used as payoffs, while a success shot (goal) is extremely rare in football. Therefore, we proposed the Expected Probability of Shot On Target (xSOT) metric to evaluate players' actions even if the shot results in no goal; this allows for effective differentiation and comparison between different shots and even enables counterfactual shot situation analysis. In our experiments, we have validated the framework by comparing it with baseline and ablated models. Furthermore, we have observed a high correlation between the xSOT and existing metrics. This alignment of information suggests that xSOT provides valuable insights. Lastly, as an illustration, we studied optimal strategies in the World Cup 2022 and analyzed a shot situation in EURO 2020.
</details>
<details>
<summary>摘要</summary>
在机器学习、游戏理论等领域，两个对立代理经常发生复杂的互动。量化分析这些策略可以提供客观的决策基础。一个典型的情况是足球中的射击，决策是否射球或传球，以及是否阻止射击都对游戏的结果产生重要影响。然而，目前没有有效的数据驱动和/或理论基础的方法来分析这些情况。为解决这个问题，我们提出了一种新的分析框架，基于游戏理论，我们使用机器学习（ML）模型来估计射击的预期收益，并从理论基础上提取了适用于ML模型的附加特征。传统上，成功或失败（1或0）被用作奖励，而射击成功（进球）在足球中是非常罕见的。因此，我们提出了射击点对象概率（xSOT）指标，以评估球员的行为，即使射击无法得分，这些指标允许有效地区分和比较不同的射击，甚至允许对射击情况进行对照分析。在我们的实验中，我们 validate了框架，并与基线和减少模型进行比较。此外，我们发现xSOT和现有指标之间存在高度的相关性。这种对应信息表示xSOT提供了有价值的信息。最后，我们以2022年世界杯和2020年欧锦赛作为例子，分析了一个射击情况。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Silent-Failures-in-Medical-Image-Classification"><a href="#Understanding-Silent-Failures-in-Medical-Image-Classification" class="headerlink" title="Understanding Silent Failures in Medical Image Classification"></a>Understanding Silent Failures in Medical Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14729">http://arxiv.org/abs/2307.14729</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iml-dkfz/sf-visuals">https://github.com/iml-dkfz/sf-visuals</a></li>
<li>paper_authors: Till J. Bungert, Levin Kobelke, Paul F. Jaeger</li>
<li>for: 预防静默失败，以确保医疗应用中的分类系统可靠。</li>
<li>methods: 使用 confidence scoring functions (CSFs) 检测和预防静默失败。</li>
<li>results: none of the benchmarked CSFs can reliably prevent silent failures, indicating a need for a deeper understanding of the root causes of failures in the data.I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
To ensure the reliable use of classification systems in medical applications, it is crucial to prevent silent failures. This can be achieved by either designing classifiers that are robust enough to avoid failures in the first place, or by detecting remaining failures using confidence scoring functions (CSFs). A predominant source of failures in image classification is distribution shifts between training data and deployment data. To understand the current state of silent failure prevention in medical imaging, we conduct the first comprehensive analysis comparing various CSFs in four biomedical tasks and a diverse range of distribution shifts. Based on the result that none of the benchmarked CSFs can reliably prevent silent failures, we conclude that a deeper understanding of the root causes of failures in the data is required. To facilitate this, we introduce SF-Visuals, an interactive analysis tool that uses latent space clustering to visualize shifts and failures. On the basis of various examples, we demonstrate how this tool can help researchers gain insight into the requirements for safe application of classification systems in the medical domain. The open-source benchmark and tool are at: https://github.com/IML-DKFZ/sf-visuals.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:要确保医疗应用中的分类系统可靠使用，避免悬念性失败是非常重要。这可以通过设计更加鲁棒的分类器来避免失败，或者使用信任分数函数（CSF）来检测剩下的失败。图像分类中的主要失败来源之一是在训练数据和部署数据之间的分布shift。为了了解医疗影像中的现状，我们进行了首次全面的分析，比较了各种 CSF 在四种生物医学任务和多样化的分布shift 下的表现。结果显示，none of the benchmarked CSFs 可靠地防止悬念性失败。这表明，更深入了解数据中的失败根源是必要的。为此，我们介绍了 SF-Visuals，一种可互动地分析工具，使用幽默空间划分来visualize  shift 和失败。通过多个示例，我们示出了这个工具如何帮助研究人员了解在医疗领域中安全应用分类系统的需求。开源 benchmark 和工具可以在：https://github.com/IML-DKFZ/sf-visuals 中找到。
</details></li>
</ul>
<hr>
<h2 id="The-Effect-of-Spoken-Language-on-Speech-Enhancement-using-Self-Supervised-Speech-Representation-Loss-Functions"><a href="#The-Effect-of-Spoken-Language-on-Speech-Enhancement-using-Self-Supervised-Speech-Representation-Loss-Functions" class="headerlink" title="The Effect of Spoken Language on Speech Enhancement using Self-Supervised Speech Representation Loss Functions"></a>The Effect of Spoken Language on Speech Enhancement using Self-Supervised Speech Representation Loss Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14502">http://arxiv.org/abs/2307.14502</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leto19/commonvoice-demand">https://github.com/leto19/commonvoice-demand</a></li>
<li>paper_authors: George Close, Thomas Hain, Stefan Goetze</li>
<li>for: 这项研究旨在探讨自动提高抖音（SE）系统中使用自动提高抖音（SSSR）作为特征变换在损失函数中的效果。</li>
<li>methods: 本研究使用了不同语言组合和网络结构的自动提高抖音（SSSR）来训练和测试SE系统。</li>
<li>results: 研究发现，训练自动提高抖音的语言对提高性能的影响较小，但是训练数据量的影响很大。<details>
<summary>Abstract</summary>
Recent work in the field of speech enhancement (SE) has involved the use of self-supervised speech representations (SSSRs) as feature transformations in loss functions. However, in prior work, very little attention has been paid to the relationship between the language of the audio used to train the self-supervised representation and that used to train the SE system. Enhancement models trained using a loss function which incorporates a self-supervised representation that shares exactly the language of the noisy data used to train the SE system show better performance than those which do not match exactly. This may lead to enhancement systems which are language specific and as such do not generalise well to unseen languages, unlike models trained using traditional spectrogram or time domain loss functions. In this work, SE models are trained and tested on a number of different languages, with self-supervised representations which themselves are trained using different language combinations and with differing network structures as loss function representations. These models are then tested across unseen languages and their performances are analysed. It is found that the training language of the self-supervised representation appears to have a minor effect on enhancement performance, the amount of training data of a particular language, however, greatly affects performance.
</details>
<details>
<summary>摘要</summary>
最近在语音增强（SE）领域的研究中，有使用自然语言自我监督语音表示（SSSRs）作为损失函数中的特征变换。然而，在先前的工作中，对于使用自然语言来训练自我监督表示和语音增强系统之间的关系，几乎没有关注。在不匹配的语言条件下训练语音增强系统时，使用具有相同语言的自我监督表示可能会导致更好的性能，而不匹配的语言可能会导致语音增强系统不易泛化到未看过的语言。在这项工作中，我们训练和测试了多种不同语言的语音增强模型，使用不同的语言组合和网络结构作为损失函数表示。这些模型在未看过的语言上进行测试，其性能分析结果表明，训练语音增强模型的语言对性能的影响相对较小，但是训练数据的量对性能的影响很大。
</details></li>
</ul>
<hr>
<h2 id="Robust-vertebra-identification-using-simultaneous-node-and-edge-predicting-Graph-Neural-Networks"><a href="#Robust-vertebra-identification-using-simultaneous-node-and-edge-predicting-Graph-Neural-Networks" class="headerlink" title="Robust vertebra identification using simultaneous node and edge predicting Graph Neural Networks"></a>Robust vertebra identification using simultaneous node and edge predicting Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02509">http://arxiv.org/abs/2308.02509</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/imfusiongmbh/vid-vertebra-identification-dataset">https://github.com/imfusiongmbh/vid-vertebra-identification-dataset</a></li>
<li>paper_authors: Vincent Bürgin, Raphael Prevost, Marijn F. Stollenga</li>
<li>for: 验证 Automatic vertebra localization and identification in CT scans 的重要性，并提出一种简单的ipeline来实现这一目标。</li>
<li>methods: 使用 U-Net 预测 vertebra 的位置和orientation，并使用单个图像神经网络来关联和分类 vertebra。</li>
<li>results: 方法可以准确地关联正确的体部和肋骨特征点，忽略假阳性结果，并在标准 VerSe 挑战任务中表现竞争力。<details>
<summary>Abstract</summary>
Automatic vertebra localization and identification in CT scans is important for numerous clinical applications. Much progress has been made on this topic, but it mostly targets positional localization of vertebrae, ignoring their orientation. Additionally, most methods employ heuristics in their pipeline that can be sensitive in real clinical images which tend to contain abnormalities. We introduce a simple pipeline that employs a standard prediction with a U-Net, followed by a single graph neural network to associate and classify vertebrae with full orientation. To test our method, we introduce a new vertebra dataset that also contains pedicle detections that are associated with vertebra bodies, creating a more challenging landmark prediction, association and classification task. Our method is able to accurately associate the correct body and pedicle landmarks, ignore false positives and classify vertebrae in a simple, fully trainable pipeline avoiding application-specific heuristics. We show our method outperforms traditional approaches such as Hungarian Matching and Hidden Markov Models. We also show competitive performance on the standard VerSe challenge body identification task.
</details>
<details>
<summary>摘要</summary>
自动骨vertebra位置和识别在CT扫描图中是许多临床应用的重要任务。许多研究已经进行了这方面的进步，但大多数方法都忽略了骨vertebra的方向。此外，大多数方法还使用了一些规则来处理实际的临床图像，这些图像通常含有异常。我们提出了一个简单的管道，其中使用标准预测和U-Net，然后使用单个图гра树神经网络来关联和分类骨vertebra。为测试我们的方法，我们提出了一个新的骨vertebra数据集，该数据集还包含了骨体的找到和关联。我们的方法能够准确地关联正确的骨体和骨脊的标记，忽略假阳性和分类骨vertebra。我们显示我们的方法超过传统的方法，如匈牙利匹配和隐马尔可夫模型。我们还显示我们的方法在标准VerSe挑战体部识别任务中 exhibits 竞争性的表现。
</details></li>
</ul>
<hr>
<h2 id="TimeGNN-Temporal-Dynamic-Graph-Learning-for-Time-Series-Forecasting"><a href="#TimeGNN-Temporal-Dynamic-Graph-Learning-for-Time-Series-Forecasting" class="headerlink" title="TimeGNN: Temporal Dynamic Graph Learning for Time Series Forecasting"></a>TimeGNN: Temporal Dynamic Graph Learning for Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14680">http://arxiv.org/abs/2307.14680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nancy Xu, Chrysoula Kosma, Michalis Vazirgiannis</li>
<li>for: 预测时间序列数据，帮助解决多种科学和工程领域中的重要应用问题。</li>
<li>methods: 使用图神经网络方法，同时学习图structure和时间序列数据的 correlations，以便更好地预测时间序列。</li>
<li>results: 相比其他状态对比方法，时间GNNSince achieves 4-80倍快于其他状态对比方法，而且预测性能相似。<details>
<summary>Abstract</summary>
Time series forecasting lies at the core of important real-world applications in many fields of science and engineering. The abundance of large time series datasets that consist of complex patterns and long-term dependencies has led to the development of various neural network architectures. Graph neural network approaches, which jointly learn a graph structure based on the correlation of raw values of multivariate time series while forecasting, have recently seen great success. However, such solutions are often costly to train and difficult to scale. In this paper, we propose TimeGNN, a method that learns dynamic temporal graph representations that can capture the evolution of inter-series patterns along with the correlations of multiple series. TimeGNN achieves inference times 4 to 80 times faster than other state-of-the-art graph-based methods while achieving comparable forecasting performance
</details>
<details>
<summary>摘要</summary>
时序序列预测在许多科学和工程领域的实际应用中具有核心地位。由于大量的时序序列数据集中含有复杂的模式和长期关系，因此引发了许多神经网络架构的发展。图神经网络方法，它同时学习基于时序序列值的相关性建立图structure，在预测时已经取得了很大成功。然而，这些解决方案经常具有高成本和难以扩展的缺点。在本文中，我们提出了TimeGNN方法，它可以在实时预测过程中学习动态的时间序列图表示，同时捕捉多个序列之间的演变和相关性。TimeGNN在对其他状态艺术图法进行比较时，实现了4-80倍 быстре的推理速度，并具有相似的预测性能。
</details></li>
</ul>
<hr>
<h2 id="Prediction-of-wind-turbines-power-with-physics-informed-neural-networks-and-evidential-uncertainty-quantification"><a href="#Prediction-of-wind-turbines-power-with-physics-informed-neural-networks-and-evidential-uncertainty-quantification" class="headerlink" title="Prediction of wind turbines power with physics-informed neural networks and evidential uncertainty quantification"></a>Prediction of wind turbines power with physics-informed neural networks and evidential uncertainty quantification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14675">http://arxiv.org/abs/2307.14675</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alfonso Gijón, Ainhoa Pujana-Goitia, Eugenio Perea, Miguel Molina-Solana, Juan Gómez-Romero</li>
<li>for: 这个研究的目的是优化风力机操作和维护，通过早期缺陷探测和精准预测风机发电功率。</li>
<li>methods: 这个研究使用数据驱动方法，使用物理约束的启发式神经网络来复制历史数据，并提供了对输出变量的高度准确预测。</li>
<li>results: 研究结果表明，使用物理约束的启发式神经网络可以高度准确地预测风机发电功率、扭矩和功率系数，并且可以提供对预测结果的不确定性的估计。<details>
<summary>Abstract</summary>
The ever-growing use of wind energy makes necessary the optimization of turbine operations through pitch angle controllers and their maintenance with early fault detection. It is crucial to have accurate and robust models imitating the behavior of wind turbines, especially to predict the generated power as a function of the wind speed. Existing empirical and physics-based models have limitations in capturing the complex relations between the input variables and the power, aggravated by wind variability. Data-driven methods offer new opportunities to enhance wind turbine modeling of large datasets by improving accuracy and efficiency. In this study, we used physics-informed neural networks to reproduce historical data coming from 4 turbines in a wind farm, while imposing certain physical constraints to the model. The developed models for regression of the power, torque, and power coefficient as output variables showed great accuracy for both real data and physical equations governing the system. Lastly, introducing an efficient evidential layer provided uncertainty estimations of the predictions, proved to be consistent with the absolute error, and made possible the definition of a confidence interval in the power curve.
</details>
<details>
<summary>摘要</summary>
随着风能的不断发展，风机操作的优化变得必要，特别是通过扭角控制器和其维护，以早期发现FAULT。准确和可靠的风机模型对预测风速的输出功率具有关键性，但现有的empirical和物理基础模型具有限制，尤其是在风速变化的情况下。数据驱动方法可以提高风机模型的准确率和效率，并且可以补做现有模型的缺陷。在这个研究中，我们使用物理知识束缚神经网络来复制历史数据，并对输出变量（功率、扭矩和功率系数）进行回归。我们发现，这些模型具有很高的准确率，并且可以准确地预测风机的输出。最后，我们引入了一个高效的证据层，以获得预测结果的不确定性评估，并证明了其与绝对误差之间的一致。这种方法可以为风机模型的建立提供一个信度评估。
</details></li>
</ul>
<hr>
<h2 id="Bipartite-Ranking-Fairness-through-a-Model-Agnostic-Ordering-Adjustment"><a href="#Bipartite-Ranking-Fairness-through-a-Model-Agnostic-Ordering-Adjustment" class="headerlink" title="Bipartite Ranking Fairness through a Model Agnostic Ordering Adjustment"></a>Bipartite Ranking Fairness through a Model Agnostic Ordering Adjustment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14668">http://arxiv.org/abs/2307.14668</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cuis15/xorder">https://github.com/cuis15/xorder</a></li>
<li>paper_authors: Sen Cui, Weishen Pan, Changshui Zhang, Fei Wang</li>
<li>for: 本文关注在两类样本（正确和错误）的协同排序enario中，学习一个排序函数，以便正确类样本高于错误类样本。</li>
<li>methods: 我们提出了一种模型不含的后处理框架xOrder，以实现在协同排序中保持算法排序性能和公平性。我们优化了一个权重和untility为定义最佳折叠路径，并使用动态编程过程解决。xOrder可以与不同的分类模型和公平度量 метрик相容，包括supervised和Unsupervised公平度量。</li>
<li>results: 我们在四个 benchmark 数据集和两个实际世界病人电子医疗记录库中评估了我们的提议算法。xOrder在不同的数据集和度量上具有一个更好的平衡，在不同的分组下MITIGATEscore分布的变化。此外，我们还提供了一些针对性的分析结果，表明xOrder在小样本和大分布分类分数的情况下具有Robust性。<details>
<summary>Abstract</summary>
Algorithmic fairness has been a serious concern and received lots of interest in machine learning community. In this paper, we focus on the bipartite ranking scenario, where the instances come from either the positive or negative class and the goal is to learn a ranking function that ranks positive instances higher than negative ones. While there could be a trade-off between fairness and performance, we propose a model agnostic post-processing framework xOrder for achieving fairness in bipartite ranking and maintaining the algorithm classification performance. In particular, we optimize a weighted sum of the utility as identifying an optimal warping path across different protected groups and solve it through a dynamic programming process. xOrder is compatible with various classification models and ranking fairness metrics, including supervised and unsupervised fairness metrics. In addition to binary groups, xOrder can be applied to multiple protected groups. We evaluate our proposed algorithm on four benchmark data sets and two real-world patient electronic health record repositories. xOrder consistently achieves a better balance between the algorithm utility and ranking fairness on a variety of datasets with different metrics. From the visualization of the calibrated ranking scores, xOrder mitigates the score distribution shifts of different groups compared with baselines. Moreover, additional analytical results verify that xOrder achieves a robust performance when faced with fewer samples and a bigger difference between training and testing ranking score distributions.
</details>
<details>
<summary>摘要</summary>
《算法公平性在机器学习社区中已经引起了很多关注。在这篇论文中，我们关注了二分类排名场景， instances 来自正确或错误类别，并且目标是学习一个排名函数，将正确类别的 instances 高于错误类别的 instances。尽管存在性能和公平性之间的交易，我们提出了一种模型无关的后处理框架 xOrder，以实现在二分类排名中保持算法分类性能的同时保证公平性。具体来说，我们优化了一个权重和排名公平度之间的平衡，通过动态规划过程解决。xOrder 与不同的分类模型和公平度度量相容，并且可以应用于多个保护组。我们在四个基本数据集和两个实际电子医疗纪录库上评估了我们的提议算法。 xOrder 在不同的数据集和度量上均可以寻求一个更好的平衡 между算法实用性和排名公平性。从折衔分配的排名得分视图来看，xOrder 可以减少不同组的分配得分的偏移。此外，额外的分析结果表明，xOrder 在样本数少和测试排名分布与训练排名分布之间的差异较大时表现更加稳定。
</details></li>
</ul>
<hr>
<h2 id="Decoding-the-Secrets-of-Machine-Learning-in-Malware-Classification-A-Deep-Dive-into-Datasets-Feature-Extraction-and-Model-Performance"><a href="#Decoding-the-Secrets-of-Machine-Learning-in-Malware-Classification-A-Deep-Dive-into-Datasets-Feature-Extraction-and-Model-Performance" class="headerlink" title="Decoding the Secrets of Machine Learning in Malware Classification: A Deep Dive into Datasets, Feature Extraction, and Model Performance"></a>Decoding the Secrets of Machine Learning in Malware Classification: A Deep Dive into Datasets, Feature Extraction, and Model Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14657">http://arxiv.org/abs/2307.14657</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/eurecom-s3/decodingmlsecretsofwindowsmalwareclassification">https://github.com/eurecom-s3/decodingmlsecretsofwindowsmalwareclassification</a></li>
<li>paper_authors: Savino Dambra, Yufei Han, Simone Aonzo, Platon Kotzias, Antonino Vitale, Juan Caballero, Davide Balzarotti, Leyla Bilge</li>
<li>for: 本研究旨在探讨机器学习模型在恶意软件检测和分类中的关键因素。</li>
<li>methods: 我们使用最新的机器学习模型对恶意软件进行检测和分类，并对收集到的数据进行分类。</li>
<li>results: 我们发现静态特征比动态特征更好地表现，并且将静态和动态特征相结合只有微量提高静态特征的表现。我们还发现packing与分类精度无关，并且在动态特征中缺失行为会严重降低表现。此外，我们发现在不同家族数量的情况下，模型的表现会随着家族数量的增加而变化。最后，我们发现使用 uniform 分布的样本数据可以更好地泛化到未经看过的数据。<details>
<summary>Abstract</summary>
Many studies have proposed machine-learning (ML) models for malware detection and classification, reporting an almost-perfect performance. However, they assemble ground-truth in different ways, use diverse static- and dynamic-analysis techniques for feature extraction, and even differ on what they consider a malware family. As a consequence, our community still lacks an understanding of malware classification results: whether they are tied to the nature and distribution of the collected dataset, to what extent the number of families and samples in the training dataset influence performance, and how well static and dynamic features complement each other.   This work sheds light on those open questions. by investigating the key factors influencing ML-based malware detection and classification. For this, we collect the largest balanced malware dataset so far with 67K samples from 670 families (100 samples each), and train state-of-the-art models for malware detection and family classification using our dataset. Our results reveal that static features perform better than dynamic features, and that combining both only provides marginal improvement over static features. We discover no correlation between packing and classification accuracy, and that missing behaviors in dynamically-extracted features highly penalize their performance. We also demonstrate how a larger number of families to classify make the classification harder, while a higher number of samples per family increases accuracy. Finally, we find that models trained on a uniform distribution of samples per family better generalize on unseen data.
</details>
<details>
<summary>摘要</summary>
To do this, we collected the largest balanced malware dataset to date, consisting of 67,000 samples from 670 families (100 samples each). We then trained state-of-the-art models for malware detection and family classification using our dataset. Our results show that static features perform better than dynamic features, and that combining both only provides marginal improvement over static features. We also found no correlation between packing and classification accuracy, and that missing behaviors in dynamically-extracted features highly penalize their performance.Furthermore, we demonstrated that a larger number of families to classify makes the classification harder, while a higher number of samples per family increases accuracy. Additionally, we found that models trained on a uniform distribution of samples per family better generalize on unseen data.
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-based-Parameter-Sensitivity-of-Regional-Climate-Models-–-A-Case-Study-of-the-WRF-Model-for-Heat-Extremes-over-Southeast-Australia"><a href="#Machine-Learning-based-Parameter-Sensitivity-of-Regional-Climate-Models-–-A-Case-Study-of-the-WRF-Model-for-Heat-Extremes-over-Southeast-Australia" class="headerlink" title="Machine Learning based Parameter Sensitivity of Regional Climate Models – A Case Study of the WRF Model for Heat Extremes over Southeast Australia"></a>Machine Learning based Parameter Sensitivity of Regional Climate Models – A Case Study of the WRF Model for Heat Extremes over Southeast Australia</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14654">http://arxiv.org/abs/2307.14654</a></li>
<li>repo_url: None</li>
<li>paper_authors: P. Jyoteeshkumar Reddy, Sandeep Chinta, Richard Matear, John Taylor, Harish Baki, Marcus Thatcher, Jatin Kala, Jason Sharples</li>
<li>For: This paper aims to investigate the sensitivity of Weather Research and Forecasting (WRF) model parameters to surface meteorological variables during extreme heat events in southeast Australia.* Methods: The paper uses a machine learning (ML) surrogate-based global sensitivity analysis method to identify the sensitivity of 24 adjustable parameters in seven different physics schemes of the WRF model.* Results: The study finds that only three parameters are important for the considered meteorological variables, and these results are consistent for the two different extreme heat events.Here’s the same information in Simplified Chinese text:* For: 这篇论文旨在investigateWRF模型参数对表面地理变量的敏感性，特别是在澳大利亚东南部的极热事件中。* Methods: 这篇论文使用机器学习（ML）Surrogate基于全球敏感分析方法来确定WRF模型参数的敏感性。* Results: 研究发现仅有3个参数对考虑的地理变量具有重要作用，这些结果在两个不同的极热事件中都是一致的。<details>
<summary>Abstract</summary>
Heatwaves and bushfires cause substantial impacts on society and ecosystems across the globe. Accurate information of heat extremes is needed to support the development of actionable mitigation and adaptation strategies. Regional climate models are commonly used to better understand the dynamics of these events. These models have very large input parameter sets, and the parameters within the physics schemes substantially influence the model's performance. However, parameter sensitivity analysis (SA) of regional models for heat extremes is largely unexplored. Here, we focus on the southeast Australian region, one of the global hotspots of heat extremes. In southeast Australia Weather Research and Forecasting (WRF) model is the widely used regional model to simulate extreme weather events across the region. Hence in this study, we focus on the sensitivity of WRF model parameters to surface meteorological variables such as temperature, relative humidity, and wind speed during two extreme heat events over southeast Australia. Due to the presence of multiple parameters and their complex relationship with output variables, a machine learning (ML) surrogate-based global sensitivity analysis method is considered for the SA. The ML surrogate-based Sobol SA is used to identify the sensitivity of 24 adjustable parameters in seven different physics schemes of the WRF model. Results show that out of these 24, only three parameters, namely the scattering tuning parameter, multiplier of saturated soil water content, and profile shape exponent in the momentum diffusivity coefficient, are important for the considered meteorological variables. These SA results are consistent for the two different extreme heat events. Further, we investigated the physical significance of sensitive parameters. This study's results will help in further optimising WRF parameters to improve model simulation.
</details>
<details>
<summary>摘要</summary>
世界各地的热浪和森林火灾会对社会和生态系统造成重大影响。为了开发有效的避免和适应策略，需要更好地了解热极值的情况。区域气象模型通常用于更好地理解这些事件的动力学。这些模型有非常大的输入参数集，并且physics scheme中的参数对模型性能有很大的影响。然而，区域模型参数敏感性分析（SA）对热极值的模型参数的影响还很少研究。本研究在澳大利亚南东部地区进行了研究，这是全球热极值的热点之一。在这个地区，Weather Research and Forecasting（WRF）模型是广泛使用的区域模型，用于模拟极端天气事件。因此，本研究将在WRF模型参数的敏感性分析中强调surface meteorological变量，如温度、相对湿度和风速。为了处理多个参数和它们复杂的关系，本研究采用机器学习（ML）surrogate-based Sobol SA方法进行敏感性分析。结果显示，24个可调参数中，只有三个参数（散射调整参数、满足饱和 soil water content multiplier和profile shape exponent in momentum diffusivity coefficient）对surface meteorological变量具有重要影响。这些SA结果在两个不同的极端热事件中均相互一致。此外，我们还对敏感参数进行了物理意义的调查。本研究的结果将有助于进一步优化WRF参数，以提高模型的预测。
</details></li>
</ul>
<hr>
<h2 id="Speed-Limits-for-Deep-Learning"><a href="#Speed-Limits-for-Deep-Learning" class="headerlink" title="Speed Limits for Deep Learning"></a>Speed Limits for Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14653">http://arxiv.org/abs/2307.14653</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/RishabhP19/Traffic-Surveillance">https://github.com/RishabhP19/Traffic-Surveillance</a></li>
<li>paper_authors: Inbar Seroussi, Alexander A. Alemi, Moritz Helias, Zohar Ringel</li>
<li>for: 本研究探讨了现代神经网络是否可以最优地训练。</li>
<li>methods: 本文使用了最近的热力学进步，将神经网络的初始权重分布与完全训练后的权重分布之间的速度上限确定。</li>
<li>results: 研究发现，对于线性和线性可变的神经网络（如神经汇kernel），在某些可能的尺度下 assumption 下，学习是在尺度上优化的。这些结果与小规模实验表明，权重分布在初始化后不久就会进入一个非优化的短暂阶段，然后是一个更长的优化阶段。<details>
<summary>Abstract</summary>
State-of-the-art neural networks require extreme computational power to train. It is therefore natural to wonder whether they are optimally trained. Here we apply a recent advancement in stochastic thermodynamics which allows bounding the speed at which one can go from the initial weight distribution to the final distribution of the fully trained network, based on the ratio of their Wasserstein-2 distance and the entropy production rate of the dynamical process connecting them. Considering both gradient-flow and Langevin training dynamics, we provide analytical expressions for these speed limits for linear and linearizable neural networks e.g. Neural Tangent Kernel (NTK). Remarkably, given some plausible scaling assumptions on the NTK spectra and spectral decomposition of the labels -- learning is optimal in a scaling sense. Our results are consistent with small-scale experiments with Convolutional Neural Networks (CNNs) and Fully Connected Neural networks (FCNs) on CIFAR-10, showing a short highly non-optimal regime followed by a longer optimal regime.
</details>
<details>
<summary>摘要</summary>
现代神经网络需要极高的计算能力来训练，因此自然会思考是否最优地训练。我们在材料科学中应用最新的温度动力学技术，可以界定从初始权重分布到全部训练后的神经网络权重分布之间的速度上限，基于这两个分布之间的伪拟合距离和动力学过程的热力学生产率。我们考虑了梯度流和拉杆训练动力学，并提供了分析表达式，其中包括线性和线性可变神经网络等例如神经 Tangent Kernel（NTK）。很remarkably，对于一些可能的 NTK  спектrum 和标签的特征分解的假设，我们发现学习是在一定的缩放意义上最优的。我们的结果与小规模的 CIFAR-10 上的 Convolutional Neural Networks（CNNs）和 Fully Connected Neural Networks（FCNs）的实验结果相符，显示一个短暂的非优化期 followed by a longer optimal period。
</details></li>
</ul>
<hr>
<h2 id="Spatial-Frequency-U-Net-for-Denoising-Diffusion-Probabilistic-Models"><a href="#Spatial-Frequency-U-Net-for-Denoising-Diffusion-Probabilistic-Models" class="headerlink" title="Spatial-Frequency U-Net for Denoising Diffusion Probabilistic Models"></a>Spatial-Frequency U-Net for Denoising Diffusion Probabilistic Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14648">http://arxiv.org/abs/2307.14648</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Yuan, Linjie Li, Jianfeng Wang, Zhengyuan Yang, Kevin Lin, Zicheng Liu, Lijuan Wang</li>
<li>for: 这个论文是用来研究在波лет空间中使用潮汐传播概率模型 (DDPM) 进行视觉合成。</li>
<li>methods: 这个论文使用了一个新的架构 SFUNet，它特意设计来有效地捕捉波лет变换所表示的图像中的相互联系。在标准的潮汐净化 U-Net 中，我们增加了2D潮汐条件和频率对应层，以同时模型空间和频率领域中的联系。</li>
<li>results: 这个研究发现，使用我们的架构可以在 CIFAR-10、FFHQ、LSUN-Bedroom 和 LSUN-Church 数据集上生成高品质的图像，比过 pixel-based 网络。<details>
<summary>Abstract</summary>
In this paper, we study the denoising diffusion probabilistic model (DDPM) in wavelet space, instead of pixel space, for visual synthesis. Considering the wavelet transform represents the image in spatial and frequency domains, we carefully design a novel architecture SFUNet to effectively capture the correlation for both domains. Specifically, in the standard denoising U-Net for pixel data, we supplement the 2D convolutions and spatial-only attention layers with our spatial frequency-aware convolution and attention modules to jointly model the complementary information from spatial and frequency domains in wavelet data. Our new architecture can be used as a drop-in replacement to the pixel-based network and is compatible with the vanilla DDPM training process. By explicitly modeling the wavelet signals, we find our model is able to generate images with higher quality on CIFAR-10, FFHQ, LSUN-Bedroom, and LSUN-Church datasets, than the pixel-based counterpart.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了在wavelet空间中使用扩散概率模型（DDPM）进行视觉合成。我们注意到，wavelet变换可以在空间和频率域中表示图像，因此我们 méticulously设计了一种新的架构SFUNet，以有效地捕捉这两个频率域之间的相关性。具体来说，在标准的干扰U-Net中，我们补充了2D卷积和专门关注空间频率信息的卷积和注意力模块，以同时模型空间和频率频率域中的补做信息。我们的新架构可以与标准的像素数据网络进行互换，并且与普通的DDPM训练过程相容。通过显式地模型wavelet信号，我们发现我们的模型在CIFAR-10、FFHQ、LSUN-Bedroom和LSUN-Church数据集上能够生成高质量的图像，比标准的像素数据网络更高。
</details></li>
</ul>
<hr>
<h2 id="MVMR-FS-Non-parametric-feature-selection-algorithm-based-on-Maximum-inter-class-Variation-and-Minimum-Redundancy"><a href="#MVMR-FS-Non-parametric-feature-selection-algorithm-based-on-Maximum-inter-class-Variation-and-Minimum-Redundancy" class="headerlink" title="MVMR-FS : Non-parametric feature selection algorithm based on Maximum inter-class Variation and Minimum Redundancy"></a>MVMR-FS : Non-parametric feature selection algorithm based on Maximum inter-class Variation and Minimum Redundancy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14643">http://arxiv.org/abs/2307.14643</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haitao Nie, Shengbo Zhang, Bin Xie</li>
<li>for: 本研究旨在解决 feature selection 中的 relevance 和 redundancy 问题，提出了一种基于最大间类差和最小重复度的非参数化Feature Selection算法（MVMR-FS）。</li>
<li>methods: 本研究使用了 supervised 和Unsupervised 预测器构成kernel density estimation来捕捉特征之间的相似性和差异，然后提出了 maximum inter-class variation和minimum redundancy（MVMR）的 критери来衡量特征的相关性和重复度。</li>
<li>results: 与前十种状态顶方法进行比较，MVMR-FS 实现了最高的平均准确率，提高了准确率5%到11%。<details>
<summary>Abstract</summary>
How to accurately measure the relevance and redundancy of features is an age-old challenge in the field of feature selection. However, existing filter-based feature selection methods cannot directly measure redundancy for continuous data. In addition, most methods rely on manually specifying the number of features, which may introduce errors in the absence of expert knowledge. In this paper, we propose a non-parametric feature selection algorithm based on maximum inter-class variation and minimum redundancy, abbreviated as MVMR-FS. We first introduce supervised and unsupervised kernel density estimation on the features to capture their similarities and differences in inter-class and overall distributions. Subsequently, we present the criteria for maximum inter-class variation and minimum redundancy (MVMR), wherein the inter-class probability distributions are employed to reflect feature relevance and the distances between overall probability distributions are used to quantify redundancy. Finally, we employ an AGA to search for the feature subset that minimizes the MVMR. Compared with ten state-of-the-art methods, MVMR-FS achieves the highest average accuracy and improves the accuracy by 5% to 11%.
</details>
<details>
<summary>摘要</summary>
age-old challenge 在feature选择领域是如何准确地测量特征相关性和重复性。然而，现有的筛选方法无法直接测量连续数据中的重复性。此外，大多数方法需要手动指定特征的数量，这可能会导致专家知识不足的情况下出现错误。在本文中，我们提出了一种非参数化特征选择算法基于最大 между类差异和最小重复性，简称MVMR-FS。我们首先引入了监督和无监督核密度估计器，用于捕捉特征之间的相似性和总体分布的不同。接着，我们介绍了MVMR的标准，其中用于反映特征相关性的inter-class probability distribution，以及用于衡量特征重复性的 distances between overall probability distributions。最后，我们使用AGA进行搜索，以找到最小化MVMR的特征子集。与十种当前状态的方法相比，MVMR-FS achieve最高的平均准确率，提高了5%到11%。
</details></li>
</ul>
<hr>
<h2 id="Linear-Convergence-of-Black-Box-Variational-Inference-Should-We-Stick-the-Landing"><a href="#Linear-Convergence-of-Black-Box-Variational-Inference-Should-We-Stick-the-Landing" class="headerlink" title="Linear Convergence of Black-Box Variational Inference: Should We Stick the Landing?"></a>Linear Convergence of Black-Box Variational Inference: Should We Stick the Landing?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14642">http://arxiv.org/abs/2307.14642</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kyurae Kim, Yian Ma, Jacob R. Gardner</li>
<li>for: 这paper是为了证明黑盒变量推断（BBVI）与控制变量的使用，特别是使用扣板降落（STL）估计器，在完美变量家族特定下 converges at a geometric rate。</li>
<li>methods: 这paper使用的方法包括证明STL估计器的偏导数方差为quadratic bound，以及对已有的closed-form entropy gradient estimators进行改进，以获得更好的非假设性质量保证。</li>
<li>results: 这paper的结果表明，使用BBVI和STL估计器可以在完美变量家族下 converges at a geometric rate，并且可以使用 projeted stochastic gradient descent来实现。此外，paper还提供了对closed-form entropy gradient estimators的改进，以及对其非假设性质量保证的explicit non-asymptotic complexity guarantees。<details>
<summary>Abstract</summary>
We prove that black-box variational inference (BBVI) with control variates, particularly the sticking-the-landing (STL) estimator, converges at a geometric (traditionally called "linear") rate under perfect variational family specification. In particular, we prove a quadratic bound on the gradient variance of the STL estimator, one which encompasses misspecified variational families. Combined with previous works on the quadratic variance condition, this directly implies convergence of BBVI with the use of projected stochastic gradient descent. We also improve existing analysis on the regular closed-form entropy gradient estimators, which enables comparison against the STL estimator and provides explicit non-asymptotic complexity guarantees for both.
</details>
<details>
<summary>摘要</summary>
我们证明黑盒变量推断（BBVI）与控制变量，尤其是粘降（STL）估计器，在完美变量家族 спецификация下 converge 于 геометри（传统上称为“线性”）速率。特别是，我们证明 STL 估计器的梯度方差呈 quadratic 形式，包括变量家族不准确的情况。与先前的二阶 variance 条件研究相结合，这直接意味着 BBVI 使用投影式随机梯度下降 converge。我们还改进了现有的关于关闭形式Entropy Gradient估计器的分析，使其与 STL 估计器进行比较，并提供了非含极限性质保证。
</details></li>
</ul>
<hr>
<h2 id="Fact-Checking-of-AI-Generated-Reports"><a href="#Fact-Checking-of-AI-Generated-Reports" class="headerlink" title="Fact-Checking of AI-Generated Reports"></a>Fact-Checking of AI-Generated Reports</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14634">http://arxiv.org/abs/2307.14634</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Razi Mahmood, Ge Wang, Mannudeep Kalra, Pingkun Yan</li>
<li>for: 这篇论文旨在探讨如何使用生成型人工智能（AI）生成真实的医疗影像报告，以减少临床过程中的时间和成本。</li>
<li>methods: 本研究使用了一种新的方法，即基于图像和文本的对映来验证AI生成的报告。这个方法通过学习图像和文本之间的相互关联，将真实和伪造的句子区分开来。</li>
<li>results: 研究发现，这个方法可以实时验证AI生成的报告，并将伪造的句子移除，以提高医疗过程中的精确性和可靠性。这个工具有助于未来的生成AI方法，以责任地使用AI实现医疗过程的加速。<details>
<summary>Abstract</summary>
With advances in generative artificial intelligence (AI), it is now possible to produce realistic-looking automated reports for preliminary reads of radiology images. This can expedite clinical workflows, improve accuracy and reduce overall costs. However, it is also well-known that such models often hallucinate, leading to false findings in the generated reports. In this paper, we propose a new method of fact-checking of AI-generated reports using their associated images. Specifically, the developed examiner differentiates real and fake sentences in reports by learning the association between an image and sentences describing real or potentially fake findings. To train such an examiner, we first created a new dataset of fake reports by perturbing the findings in the original ground truth radiology reports associated with images. Text encodings of real and fake sentences drawn from these reports are then paired with image encodings to learn the mapping to real/fake labels. The utility of such an examiner is demonstrated for verifying automatically generated reports by detecting and removing fake sentences. Future generative AI approaches can use the resulting tool to validate their reports leading to a more responsible use of AI in expediting clinical workflows.
</details>
<details>
<summary>摘要</summary>
随着生成式人工智能（AI）的进步，现在可以生成具有真实look的自动报告，以便加速临床工作流程，提高准确性并降低总成本。然而，这些模型经常“幻想”，导致生成的报告中的假发现。在这篇论文中，我们提议一种新的实验室检查AI生成的报告的方法。具体来说，我们开发了一个新的检查器，可以在报告中分辨真实和假的句子。为了训练这个检查器，我们首先创建了一个新的假报告数据集，其中对原始的真实股票报告中的发现进行了修改。然后，我们对真实和假句子的文本编码和图像编码进行了对应，以学习将真实和假标签映射到句子和图像中。我们证明了这种检查器可以用于检查自动生成的报告，并且可以检测并移除假的句子。未来的生成AI方法可以使用这种工具来验证他们的报告，从而实现负责任的AI使用。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Reservoir-Computing-and-its-Interdisciplinary-Applications-Beyond-Traditional-Machine-Learning"><a href="#A-Survey-on-Reservoir-Computing-and-its-Interdisciplinary-Applications-Beyond-Traditional-Machine-Learning" class="headerlink" title="A Survey on Reservoir Computing and its Interdisciplinary Applications Beyond Traditional Machine Learning"></a>A Survey on Reservoir Computing and its Interdisciplinary Applications Beyond Traditional Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15092">http://arxiv.org/abs/2307.15092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heng Zhang, Danilo Vasconcellos Vargas</li>
<li>for: 本研究评论文章的目的是对储量计算（RC）的最新发展进行统一的回顾，从机器学习到物理、生物和神经科学。</li>
<li>methods: 本文使用的方法包括早期RC模型的介绍，以及当前状态的RC模型和其应用。同时，文章还介绍了模拟大脑机制的研究。</li>
<li>results: 本文对RC的发展进行了统一的回顾，并介绍了它在不同领域的应用，包括机器学习、物理、生物和神经科学。此外，文章还提供了新的发展perspective，包括储量设计、编程框架的统一、物理实现和与认知神经科学和进化相互作用。<details>
<summary>Abstract</summary>
Reservoir computing (RC), first applied to temporal signal processing, is a recurrent neural network in which neurons are randomly connected. Once initialized, the connection strengths remain unchanged. Such a simple structure turns RC into a non-linear dynamical system that maps low-dimensional inputs into a high-dimensional space. The model's rich dynamics, linear separability, and memory capacity then enable a simple linear readout to generate adequate responses for various applications. RC spans areas far beyond machine learning, since it has been shown that the complex dynamics can be realized in various physical hardware implementations and biological devices. This yields greater flexibility and shorter computation time. Moreover, the neuronal responses triggered by the model's dynamics shed light on understanding brain mechanisms that also exploit similar dynamical processes. While the literature on RC is vast and fragmented, here we conduct a unified review of RC's recent developments from machine learning to physics, biology, and neuroscience. We first review the early RC models, and then survey the state-of-the-art models and their applications. We further introduce studies on modeling the brain's mechanisms by RC. Finally, we offer new perspectives on RC development, including reservoir design, coding frameworks unification, physical RC implementations, and interaction between RC, cognitive neuroscience and evolution.
</details>
<details>
<summary>摘要</summary>
储池计算（RC），最初应用于时间信号处理，是一种循环神经网络，其neurons randomly连接。一旦初始化，连接强度保持不变。这种简单的结构使RC变成一个非线性动力系统，可以将低维度输入映射到高维度空间中。模型的丰富动力、线性分离和记忆容量使得一个简单的直线读取可以生成适用于各种应用的合适响应。RC的应用范围超出机器学习，因为它已经在物理硬件实现和生物设备中实现了复杂的动力。这些实现带来更多的灵活性和更短的计算时间。此外，模型的神经响应也为了解大脑机制提供了新的思路，这些机制也利用类似的动力过程。在文献中，关于RC的研究非常广泛和杂乱，这里我们提供一个统一的RC最新发展的评论，从机器学习到物理、生物和神经科学。我们首先介绍了RC的早期模型，然后评论了当前最佳模型和其应用。我们还介绍了模型大脑机制的研究。最后，我们提出了新的RC发展 perspective，包括储池设计、编程框架统一、物理RC实现和RC、认知神经科学和演化之间的交互。
</details></li>
</ul>
<hr>
<h2 id="Rapid-and-Scalable-Bayesian-AB-Testing"><a href="#Rapid-and-Scalable-Bayesian-AB-Testing" class="headerlink" title="Rapid and Scalable Bayesian AB Testing"></a>Rapid and Scalable Bayesian AB Testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14628">http://arxiv.org/abs/2307.14628</a></li>
<li>repo_url: None</li>
<li>paper_authors: Srivas Chennu, Andrew Maher, Christian Pangerl, Subash Prabanantham, Jae Hyeon Bae, Jamie Martin, Bud Goswami</li>
<li>for: 该论文旨在帮助企业operator更好地做出决策，通过利用数据学习来提高数字用户体验。</li>
<li>methods: 该论文提出了一种基于 Bayesian 估计的解决方案，用于 Addressing the limitations of current sequential AB testing methodology, such as lack of statistical power, correlations between factors, and inability to pool knowledge from past tests.</li>
<li>results: 论文通过 numercial simulations 和实际应用 demonstrate 了该方法的有用性，包括增加了统计力量、允许顺序测试和早期停止、不受过分风险等。此外，论文还展示了如何使用这种方法来加速未来测试。<details>
<summary>Abstract</summary>
AB testing aids business operators with their decision making, and is considered the gold standard method for learning from data to improve digital user experiences. However, there is usually a gap between the requirements of practitioners, and the constraints imposed by the statistical hypothesis testing methodologies commonly used for analysis of AB tests. These include the lack of statistical power in multivariate designs with many factors, correlations between these factors, the need of sequential testing for early stopping, and the inability to pool knowledge from past tests. Here, we propose a solution that applies hierarchical Bayesian estimation to address the above limitations. In comparison to current sequential AB testing methodology, we increase statistical power by exploiting correlations between factors, enabling sequential testing and progressive early stopping, without incurring excessive false positive risk. We also demonstrate how this methodology can be extended to enable the extraction of composite global learnings from past AB tests, to accelerate future tests. We underpin our work with a solid theoretical framework that articulates the value of hierarchical estimation. We demonstrate its utility using both numerical simulations and a large set of real-world AB tests. Together, these results highlight the practical value of our approach for statistical inference in the technology industry.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="BubbleML-A-Multi-Physics-Dataset-and-Benchmarks-for-Machine-Learning"><a href="#BubbleML-A-Multi-Physics-Dataset-and-Benchmarks-for-Machine-Learning" class="headerlink" title="BubbleML: A Multi-Physics Dataset and Benchmarks for Machine Learning"></a>BubbleML: A Multi-Physics Dataset and Benchmarks for Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14623">http://arxiv.org/abs/2307.14623</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hpcforge/bubbleml">https://github.com/hpcforge/bubbleml</a></li>
<li>paper_authors: Sheikh Md Shakeel Hassan, Arthur Feeney, Akash Dhruv, Jihoon Kim, Youngjoon Suh, Jaiyoung Ryu, Yoonjin Won, Aparna Chandramowlishwaran</li>
<li>for: 该论文主要目的是提供一个可访问的和多样化的数据集，用于机器学习（ML）训练，以更好地理解多物理现象的热相变化。</li>
<li>methods: 该论文使用物理驱动的计算机模拟来提供精准的观测数据，包括各种热泡沸点情况，如 Pool boiling, flow boiling, 和半冷泡沸。该数据集覆盖了广泛的参数，包括不同的重力条件、流速、半冷水位和墙面超热情况，涵盖51个计算。</li>
<li>results: 该论文验证了该数据集的有效性，并展示了其在多种下游任务中的潜在应用，包括光流分析和温度动力学学习。该数据集和其标准 benchmarks 将成为机器学习驱动的多物理热相变化研究的推进者，推动了技术和模型的开发和比较。<details>
<summary>Abstract</summary>
In the field of phase change phenomena, the lack of accessible and diverse datasets suitable for machine learning (ML) training poses a significant challenge. Existing experimental datasets are often restricted, with limited availability and sparse ground truth data, impeding our understanding of this complex multi-physics phenomena. To bridge this gap, we present the BubbleML Dataset(https://github.com/HPCForge/BubbleML) which leverages physics-driven simulations to provide accurate ground truth information for various boiling scenarios, encompassing nucleate pool boiling, flow boiling, and sub-cooled boiling. This extensive dataset covers a wide range of parameters, including varying gravity conditions, flow rates, sub-cooling levels, and wall superheat, comprising 51 simulations. BubbleML is validated against experimental observations and trends, establishing it as an invaluable resource for ML research. Furthermore, we showcase its potential to facilitate exploration of diverse downstream tasks by introducing two benchmarks: (a) optical flow analysis to capture bubble dynamics, and (b) operator networks for learning temperature dynamics. The BubbleML dataset and its benchmarks serve as a catalyst for advancements in ML-driven research on multi-physics phase change phenomena, enabling the development and comparison of state-of-the-art techniques and models.
</details>
<details>
<summary>摘要</summary>
在热变现象领域，因缺乏可访问的多样化数据集而受到机器学习（ML）训练的挑战。现有的实验数据集经常受限，有限的可用性和罕见的基准数据，妨碍我们对这种复杂多物理现象的理解。为了缓解这个差距，我们提供了BubbleML数据集（https://github.com/HPCForge/BubbleML），利用物理驱动的 simulations提供了各种爆发enario中的准确基准信息，包括 Pool boiling、流泌 boiling 和半冷含气 boiling 等多种情况。这个广泛的数据集覆盖了多种参数，包括不同重力条件、流量、冷凉水位、墙superheat 等，涵盖51个 simulations。BubbleML被验证了对实验观测和趋势的验证，确立了它作为ML研究中不可或缺的资源。此外，我们还在其中引入了两个比较任务：（a）Optical flow分析，捕捉气泡动态；（b）运算网络，学习温度动态。BubbleML数据集和其比较任务serve as a catalyst for advancements in ML-driven research on multi-physics phase change phenomena，激发开发和对state-of-the-art技术和模型的比较。
</details></li>
</ul>
<hr>
<h2 id="Imitating-Complex-Trajectories-Bridging-Low-Level-Stability-and-High-Level-Behavior"><a href="#Imitating-Complex-Trajectories-Bridging-Low-Level-Stability-and-High-Level-Behavior" class="headerlink" title="Imitating Complex Trajectories: Bridging Low-Level Stability and High-Level Behavior"></a>Imitating Complex Trajectories: Bridging Low-Level Stability and High-Level Behavior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14619">http://arxiv.org/abs/2307.14619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Block, Daniel Pfrommer, Max Simchowitz</li>
<li>for: studying the imitation of stochastic, non-Markovian, potentially multi-modal expert demonstrations in nonlinear dynamical systems.</li>
<li>methods: invoking low-level controllers (either learned or implicit in position-command control) to stabilize imitation policies around expert demonstrations, with a focus on ensuring “total variation continuity” (TVC) to achieve accurate matching of the demonstrator’s distribution over entire trajectories.</li>
<li>results: the paper provides theoretical guarantees for policies parameterized by diffusion models, showing that if the learner accurately estimates the score of the (noise-augmented) expert policy, then the distribution of imitator trajectories is close to the demonstrator distribution in a natural optimal transport distance, with empirical validation of the algorithmic recommendations.Here’s the Chinese translation of the three key information points:</li>
<li>for: 研究stochoastic, non-Markovian, potentially multi-modal expert示例在非线性动力系统中的模仿。</li>
<li>methods: 通过 invoke low-level控制器（ Either learned或implicit in position-command control）来稳定模仿策略 around expert示例，以确保 “总变量稳定” (TVC) 以实现准确地匹配示例者的分布 over entire trajectories。</li>
<li>results: 文章提供了对政策参数化 by diffusion models的 teorotical guarantees，表明如果学习者准确地估计（noise-augmented）专家策略的Score，那么imitator的分布 will be close to the demonstrator distribution in a natural optimal transport distance,并进行了实验验证算法建议。<details>
<summary>Abstract</summary>
We propose a theoretical framework for studying the imitation of stochastic, non-Markovian, potentially multi-modal (i.e. "complex" ) expert demonstrations in nonlinear dynamical systems. Our framework invokes low-level controllers - either learned or implicit in position-command control - to stabilize imitation policies around expert demonstrations. We show that with (a) a suitable low-level stability guarantee and (b) a stochastic continuity property of the learned policy we call "total variation continuity" (TVC), an imitator that accurately estimates actions on the demonstrator's state distribution closely matches the demonstrator's distribution over entire trajectories. We then show that TVC can be ensured with minimal degradation of accuracy by combining a popular data-augmentation regimen with a novel algorithmic trick: adding augmentation noise at execution time. We instantiate our guarantees for policies parameterized by diffusion models and prove that if the learner accurately estimates the score of the (noise-augmented) expert policy, then the distribution of imitator trajectories is close to the demonstrator distribution in a natural optimal transport distance. Our analysis constructs intricate couplings between noise-augmented trajectories, a technique that may be of independent interest. We conclude by empirically validating our algorithmic recommendations.
</details>
<details>
<summary>摘要</summary>
我们提出一种理论框架，用于研究复杂专家示范的模仿在非线性动力系统中。我们的框架借鉴低级控制器，ether学习或含有位置控制的隐式控制器，以稳定模仿政策。我们证明，如果（a）有适当的低级稳定保证，并且（b）学习政策具有总变量连续性（TVC）性质，那么模仿者可以准确地模仿专家的动作，并且模仿者的动作分布与专家的动作分布在整个轨迹上几乎相同。我们然后证明，可以通过组合流行的数据扩展约束和一种新的算法技巧来保证TVC的存在：在执行时添加扩展噪声。我们在执行时添加扩展噪声，可以在较低的精度下保证TVC。我们实例化我们的保证，并证明如果学习者准确地估计噪声扩展后的专家政策的分数，那么模仿者的轨迹分布与专家的轨迹分布在自然的优质量度中几乎相同。我们的分析建立了复杂的拓扑关系，这可能是独立的兴趣。我们最后通过实验验证我们的算法建议。
</details></li>
</ul>
<hr>
<h2 id="Self-Contrastive-Graph-Diffusion-Network"><a href="#Self-Contrastive-Graph-Diffusion-Network" class="headerlink" title="Self-Contrastive Graph Diffusion Network"></a>Self-Contrastive Graph Diffusion Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14613">http://arxiv.org/abs/2307.14613</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kunzhan/SCDGN">https://github.com/kunzhan/SCDGN</a></li>
<li>paper_authors: Yixian Ma, Kun Zhan</li>
<li>for: 本文是用于提出一种新的自适应图diffusion网络(SCGDN)的框架，用于图像自适应学习。</li>
<li>methods: 本文使用了两个主要组成部分：宏观模块(AttM)和扩散模块(DiFM)。AttM通过聚合高阶结构和特征信息来获得优秀的嵌入，而DiFM通过拉普拉斯扩散学习来均衡每个节点在图中的状态，并允许特征信息和邻接信息在图中协同演化。</li>
<li>results: 本文的实验结果表明，SCGDN可以在图像自适应学习中提供更高的性能，并且可以避免”采样偏见”和语义漂移。<details>
<summary>Abstract</summary>
Augmentation techniques and sampling strategies are crucial in contrastive learning, but in most existing works, augmentation techniques require careful design, and their sampling strategies can only capture a small amount of intrinsic supervision information. Additionally, the existing methods require complex designs to obtain two different representations of the data. To overcome these limitations, we propose a novel framework called the Self-Contrastive Graph Diffusion Network (SCGDN). Our framework consists of two main components: the Attentional Module (AttM) and the Diffusion Module (DiFM). AttM aggregates higher-order structure and feature information to get an excellent embedding, while DiFM balances the state of each node in the graph through Laplacian diffusion learning and allows the cooperative evolution of adjacency and feature information in the graph. Unlike existing methodologies, SCGDN is an augmentation-free approach that avoids "sampling bias" and semantic drift, without the need for pre-training. We conduct a high-quality sampling of samples based on structure and feature information. If two nodes are neighbors, they are considered positive samples of each other. If two disconnected nodes are also unrelated on $k$NN graph, they are considered negative samples for each other. The contrastive objective reasonably uses our proposed sampling strategies, and the redundancy reduction term minimizes redundant information in the embedding and can well retain more discriminative information. In this novel framework, the graph self-contrastive learning paradigm gives expression to a powerful force. SCGDN effectively balances between preserving high-order structure information and avoiding overfitting. The results manifest that SCGDN can consistently generate outperformance over both the contrastive methods and the classical methods.
</details>
<details>
<summary>摘要</summary>
“增强技术和采样策略是对比学习中非常重要的，但现有的方法中的增强技术需要精心设计，采样策略只能捕捉一小部分内在监督信息。此外，现有的方法需要复杂的设计来获得两种不同的数据表示。为了解决这些限制，我们提出了一种新的框架called Self-Contrastive Graph Diffusion Network (SCGDN)。我们的框架包括两个主要组件：Attentional Module (AttM)和Diffusion Module (DiFM)。AttM将高阶结构和特征信息聚合以获得优秀的嵌入，而DiFM通过laplacian diffusion learning来均衡每个节点在图中的状态，allowing the cooperative evolution of adjacency and feature information in the graph。不同于现有的方法ologies，SCGDN是一种无增强 approached that avoids "sampling bias"和semantic drift，不需要预训练。我们采用高质量的采样策略，基于结构和特征信息。如果两个节点是邻居，它们被视为对方的正例样本。如果两个不相关的节点也不在$k$NN图中相互关系，它们被视为对方的负例样本。对比目标函数合理地使用我们提议的采样策略，而减 redundancy reduction term可以减少嵌入中的重复信息，能够良好地保留更多的权威信息。在这种新的框架中，图自身对比学习模式发挥了强大的力量。SCGDN能够平衡保持高阶结构信息和避免过拟合。结果表明，SCGDN可以一直在对比方法和传统方法之上出perform。”
</details></li>
</ul>
<hr>
<h2 id="Complete-and-separate-Conditional-separation-with-missing-target-source-attribute-completion"><a href="#Complete-and-separate-Conditional-separation-with-missing-target-source-attribute-completion" class="headerlink" title="Complete and separate: Conditional separation with missing target source attribute completion"></a>Complete and separate: Conditional separation with missing target source attribute completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14609">http://arxiv.org/abs/2307.14609</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitrios Bralios, Efthymios Tzinis, Paris Smaragdis</li>
<li>for:  This paper is written for improving the performance of source separation models by leveraging semantic information about the input mixtures and constituent sources.</li>
<li>methods: The paper uses a pre-trained model to extract additional semantic data from the input mixture, which is then used to improve the separation performance of an uncoupled multi-conditional separation network.</li>
<li>results: The paper demonstrates that the separation performance of the multi-conditional model is significantly improved, approaching the performance of an oracle model with complete semantic information. Additionally, the approach achieves performance levels that are comparable to those of the best performing specialized single conditional models, providing an easier-to-use alternative.<details>
<summary>Abstract</summary>
Recent approaches in source separation leverage semantic information about their input mixtures and constituent sources that when used in conditional separation models can achieve impressive performance. Most approaches along these lines have focused on simple descriptions, which are not always useful for varying types of input mixtures. In this work, we present an approach in which a model, given an input mixture and partial semantic information about a target source, is trained to extract additional semantic data. We then leverage this pre-trained model to improve the separation performance of an uncoupled multi-conditional separation network. Our experiments demonstrate that the separation performance of this multi-conditional model is significantly improved, approaching the performance of an oracle model with complete semantic information. Furthermore, our approach achieves performance levels that are comparable to those of the best performing specialized single conditional models, thus providing an easier to use alternative.
</details>
<details>
<summary>摘要</summary>
现代源分离方法利用输入混合的semantic信息和组成源的信息，当用于条件分离模型时可以达到吸目的性能。大多数方法都是简单的描述，不一定适用于不同类型的输入混合。在这种工作中，我们提出了一种方法，即给定输入混合和部分semantic信息的target源，训练模型提取额外的semantic数据。然后，我们利用这个预训练模型提高不相互连接的多Conditional分离网络的分离性能。我们的实验表明，这种多Conditional网络的分离性能明显提高，接近完美的oracle模型，并且与专门设计的单Conditional模型性能相当。此外，我们的方法比特化的单Conditional模型更容易使用，提供了一种更容易使用的代替方案。
</details></li>
</ul>
<hr>
<h2 id="HUTFormer-Hierarchical-U-Net-Transformer-for-Long-Term-Traffic-Forecasting"><a href="#HUTFormer-Hierarchical-U-Net-Transformer-for-Long-Term-Traffic-Forecasting" class="headerlink" title="HUTFormer: Hierarchical U-Net Transformer for Long-Term Traffic Forecasting"></a>HUTFormer: Hierarchical U-Net Transformer for Long-Term Traffic Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14596">http://arxiv.org/abs/2307.14596</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zezhi Shao, Fei Wang, Zhao Zhang, Yuchen Fang, Guangyin Jin, Yongjun Xu</li>
<li>for: 预测交通情况，即基于历史观察数据预测交通条件，是智能交通领域的长期研究主题，而且被广泛认为是智能交通系统的重要组成部分。</li>
<li>methods: 我们提出了一种新的层次U-NetTransformer（HUTFormer）来解决长期交通预测的问题，它包括一个层次编码器和解码器，以同时生成和利用多级表示。特别是，编码器使用窗口自注意力和段合并来提取多级表示，而解码器则使用跨级注意力机制以有效地合并多级表示。</li>
<li>results: 我们在四个交通数据集上进行了广泛的实验，结果表明，我们提出的HUTFormer显著超过了当前交通预测和长时间序列预测基线。<details>
<summary>Abstract</summary>
Traffic forecasting, which aims to predict traffic conditions based on historical observations, has been an enduring research topic and is widely recognized as an essential component of intelligent transportation. Recent proposals on Spatial-Temporal Graph Neural Networks (STGNNs) have made significant progress by combining sequential models with graph convolution networks. However, due to high complexity issues, STGNNs only focus on short-term traffic forecasting, e.g., 1-hour forecasting, while ignoring more practical long-term forecasting. In this paper, we make the first attempt to explore long-term traffic forecasting, e.g., 1-day forecasting. To this end, we first reveal its unique challenges in exploiting multi-scale representations. Then, we propose a novel Hierarchical U-net TransFormer (HUTFormer) to address the issues of long-term traffic forecasting. HUTFormer consists of a hierarchical encoder and decoder to jointly generate and utilize multi-scale representations of traffic data. Specifically, for the encoder, we propose window self-attention and segment merging to extract multi-scale representations from long-term traffic data. For the decoder, we design a cross-scale attention mechanism to effectively incorporate multi-scale representations. In addition, HUTFormer employs an efficient input embedding strategy to address the complexity issues. Extensive experiments on four traffic datasets show that the proposed HUTFormer significantly outperforms state-of-the-art traffic forecasting and long time series forecasting baselines.
</details>
<details>
<summary>摘要</summary>
traffic 预测，targeting to predict traffic conditions based on historical observations，has been a long-standing research topic and is widely recognized as an essential component of intelligent transportation. Recent proposals on Spatial-Temporal Graph Neural Networks (STGNNs) have made significant progress by combining sequential models with graph convolution networks. However, due to high complexity issues, STGNNs only focus on short-term traffic forecasting, e.g., 1-hour forecasting, while ignoring more practical long-term forecasting. In this paper, we make the first attempt to explore long-term traffic forecasting, e.g., 1-day forecasting. To this end, we first reveal its unique challenges in exploiting multi-scale representations. Then, we propose a novel Hierarchical U-net TransFormer (HUTFormer) to address the issues of long-term traffic forecasting. HUTFormer consists of a hierarchical encoder and decoder to jointly generate and utilize multi-scale representations of traffic data. Specifically, for the encoder, we propose window self-attention and segment merging to extract multi-scale representations from long-term traffic data. For the decoder, we design a cross-scale attention mechanism to effectively incorporate multi-scale representations. In addition, HUTFormer employs an efficient input embedding strategy to address the complexity issues. Extensive experiments on four traffic datasets show that the proposed HUTFormer significantly outperforms state-of-the-art traffic forecasting and long time series forecasting baselines.
</details></li>
</ul>
<hr>
<h2 id="MCPA-Multi-scale-Cross-Perceptron-Attention-Network-for-2D-Medical-Image-Segmentation"><a href="#MCPA-Multi-scale-Cross-Perceptron-Attention-Network-for-2D-Medical-Image-Segmentation" class="headerlink" title="MCPA: Multi-scale Cross Perceptron Attention Network for 2D Medical Image Segmentation"></a>MCPA: Multi-scale Cross Perceptron Attention Network for 2D Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14588">http://arxiv.org/abs/2307.14588</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/simonustc/mcpa-for-2d-medical-image-segmentation">https://github.com/simonustc/mcpa-for-2d-medical-image-segmentation</a></li>
<li>paper_authors: Liang Xu, Mingxiao Chen, Yi Cheng, Pengfei Shao, Shuwei Shen, Peng Yao, Ronald X. Xu</li>
<li>for: 这个研究的目的是提出一个基于Convolutional Neural Networks (CNN)的双维医疗影像分类模型，以提高医疗影像分类的精度和效能。</li>
<li>methods: 这个模型使用了Transformer模组来强化UNet架构，以更好地捕捉全局特征相关性。此外，模型还导入了多 scales Cross Perceptron模组，以捕捉不同 scales的特征相关性。</li>
<li>results: 在各种公开的医疗影像数据集上进行评估，这个模型实现了顶尖的表现，并且在不同的任务和设备上均有出色的结果。<details>
<summary>Abstract</summary>
The UNet architecture, based on Convolutional Neural Networks (CNN), has demonstrated its remarkable performance in medical image analysis. However, it faces challenges in capturing long-range dependencies due to the limited receptive fields and inherent bias of convolutional operations. Recently, numerous transformer-based techniques have been incorporated into the UNet architecture to overcome this limitation by effectively capturing global feature correlations. However, the integration of the Transformer modules may result in the loss of local contextual information during the global feature fusion process. To overcome these challenges, we propose a 2D medical image segmentation model called Multi-scale Cross Perceptron Attention Network (MCPA). The MCPA consists of three main components: an encoder, a decoder, and a Cross Perceptron. The Cross Perceptron first captures the local correlations using multiple Multi-scale Cross Perceptron modules, facilitating the fusion of features across scales. The resulting multi-scale feature vectors are then spatially unfolded, concatenated, and fed through a Global Perceptron module to model global dependencies. Furthermore, we introduce a Progressive Dual-branch Structure to address the semantic segmentation of the image involving finer tissue structures. This structure gradually shifts the segmentation focus of MCPA network training from large-scale structural features to more sophisticated pixel-level features. We evaluate our proposed MCPA model on several publicly available medical image datasets from different tasks and devices, including the open large-scale dataset of CT (Synapse), MRI (ACDC), fundus camera (DRIVE, CHASE_DB1, HRF), and OCTA (ROSE). The experimental results show that our MCPA model achieves state-of-the-art performance. The code is available at https://github.com/simonustc/MCPA-for-2D-Medical-Image-Segmentation.
</details>
<details>
<summary>摘要</summary>
UNet 架构，基于卷积神经网络（CNN），在医疗影像分析中表现出色。然而，它在捕捉长距离依赖关系方面存在挑战，因为卷积操作具有限定的接收区域和自然偏好。在最近，许多基于转换器的技术被 incorporated 到 UNet 架构中，以有效地捕捉全局特征相关性。然而，将转换模块 интегра到 UNet 架构中可能会导致全局特征相关性的损失。为了解决这些挑战，我们提出了一种名为 Multi-scale Cross Perceptron Attention Network (MCPA) 的2D医疗影像分类模型。MCPA 模型由三个主要组件组成：编码器、解码器和 Cross Perceptron。Cross Perceptron 首先使用多个 Multi-scale Cross Perceptron 模块捕捉本地相关性，以便在不同尺度上进行特征融合。得到的多尺度特征向量然后在空间上展开，并将其 concatenate 并输入到全球 Perceptron 模块，以模拟全局依赖关系。此外，我们还提出了一种进步的双支结构，以解决医疗影像分类中的semantic segmentation问题。这种结构逐渐将 MCPA 网络训练的 segmentation 焦点从大规模结构特征向 pixel-level 特征进行转换。我们在多个公共可用的医疗影像数据集上进行了多种任务和设备的测试，包括 Synapse 等开放式大规模数据集、ACDC 等 MRI 数据集、DRIVE 等fundus camera 数据集、CHASE_DB1 等 OCTA 数据集。实验结果表明，我们的 MCPA 模型在 state-of-the-art 性能。代码可以在 GitHub 上获取：https://github.com/simonustc/MCPA-for-2D-Medical-Image-Segmentation。
</details></li>
</ul>
<hr>
<h2 id="Evaluation-of-Safety-Constraints-in-Autonomous-Navigation-with-Deep-Reinforcement-Learning"><a href="#Evaluation-of-Safety-Constraints-in-Autonomous-Navigation-with-Deep-Reinforcement-Learning" class="headerlink" title="Evaluation of Safety Constraints in Autonomous Navigation with Deep Reinforcement Learning"></a>Evaluation of Safety Constraints in Autonomous Navigation with Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14568">http://arxiv.org/abs/2307.14568</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brian Angulo, Gregory Gorbov, Aleksandr Panov, Konstantin Yakovlev</li>
<li>for: 这个研究旨在高亮安全性因素在自动驾驶系统中的重要性，并通过比较两种学习导航策略：安全和不安全的策略来证明这一点。</li>
<li>methods: 这个研究使用了强化学习算法在自动驾驶系统中进行导航，并对两种策略进行比较，以 highlight the importance of considering safety constraints in the development of autonomous vehicles.</li>
<li>results: 研究结果表明，安全策略可以生成更多的减噪距离（与障碍物之间的距离），并少量碰撞，而不 sacrificing the overall performance。<details>
<summary>Abstract</summary>
While reinforcement learning algorithms have had great success in the field of autonomous navigation, they cannot be straightforwardly applied to the real autonomous systems without considering the safety constraints. The later are crucial to avoid unsafe behaviors of the autonomous vehicle on the road. To highlight the importance of these constraints, in this study, we compare two learnable navigation policies: safe and unsafe. The safe policy takes the constraints into account, while the other does not. We show that the safe policy is able to generate trajectories with more clearance (distance to the obstacles) and makes less collisions while training without sacrificing the overall performance.
</details>
<details>
<summary>摘要</summary>
autonomous navigation 算法已经在场景中取得了很大的成功，但它们不能直接应用于实际的自动驾驶系统中，因为需要考虑安全约束。这些约束是关键，以避免自动车辆在路上发生不安全行为。为了强调这些约束的重要性，在这个研究中，我们比较了两种可学习导航策略：安全和不安全。安全策略考虑了约束，而另一个不考虑。我们显示，安全策略能够生成具有更多的避险距离（距离障碍物）并且 fewer collisions  durante el entrenamiento，而不 sacrificing the overall performance。
</details></li>
</ul>
<hr>
<h2 id="Auto-Tables-Synthesizing-Multi-Step-Transformations-to-Relationalize-Tables-without-Using-Examples"><a href="#Auto-Tables-Synthesizing-Multi-Step-Transformations-to-Relationalize-Tables-without-Using-Examples" class="headerlink" title="Auto-Tables: Synthesizing Multi-Step Transformations to Relationalize Tables without Using Examples"></a>Auto-Tables: Synthesizing Multi-Step Transformations to Relationalize Tables without Using Examples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14565">http://arxiv.org/abs/2307.14565</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lipengcs/auto-tables-benchmark">https://github.com/lipengcs/auto-tables-benchmark</a></li>
<li>paper_authors: Peng Li, Yeye He, Cong Yan, Yue Wang, Surajit Chaudhuri<br>for:* This paper aims to address the problem of non-relational tables in relational databases, specifically the need for complex table-restructuring transformations before these tables can be queried using SQL-based analytics tools.methods:* The authors develop an Auto-Tables system that can automatically synthesize pipelines with multi-step transformations to transform non-relational tables into standard relational forms for downstream analytics.results:* The authors evaluate the effectiveness of their proposed system using an extensive benchmark of 244 real test cases from user spreadsheets and online forums. Their evaluation suggests that Auto-Tables can successfully synthesize transformations for over 70% of test cases at interactive speeds, without requiring any input from users.<details>
<summary>Abstract</summary>
Relational tables, where each row corresponds to an entity and each column corresponds to an attribute, have been the standard for tables in relational databases. However, such a standard cannot be taken for granted when dealing with tables "in the wild". Our survey of real spreadsheet-tables and web-tables shows that over 30% of such tables do not conform to the relational standard, for which complex table-restructuring transformations are needed before these tables can be queried easily using SQL-based analytics tools. Unfortunately, the required transformations are non-trivial to program, which has become a substantial pain point for technical and non-technical users alike, as evidenced by large numbers of forum questions in places like StackOverflow and Excel/Power-BI/Tableau forums.   We develop an Auto-Tables system that can automatically synthesize pipelines with multi-step transformations (in Python or other languages), to transform non-relational tables into standard relational forms for downstream analytics, obviating the need for users to manually program transformations. We compile an extensive benchmark for this new task, by collecting 244 real test cases from user spreadsheets and online forums. Our evaluation suggests that Auto-Tables can successfully synthesize transformations for over 70% of test cases at interactive speeds, without requiring any input from users, making this an effective tool for both technical and non-technical users to prepare data for analytics.
</details>
<details>
<summary>摘要</summary>
Relational tables, where each row represents an entity and each column represents an attribute, have been the standard for tables in relational databases. However, this standard cannot be taken for granted when dealing with tables "in the wild". Our survey of real spreadsheet-tables and web-tables shows that over 30% of such tables do not conform to the relational standard, and complex table-restructuring transformations are needed before these tables can be queried easily using SQL-based analytics tools. Unfortunately, the required transformations are non-trivial to program, which has become a substantial pain point for both technical and non-technical users, as evidenced by large numbers of forum questions in places like Stack Overflow and Excel/Power BI/Tableau forums.We develop an Auto-Tables system that can automatically synthesize pipelines with multi-step transformations (in Python or other languages) to transform non-relational tables into standard relational forms for downstream analytics, obviating the need for users to manually program transformations. We compile an extensive benchmark for this new task by collecting 244 real test cases from user spreadsheets and online forums. Our evaluation suggests that Auto-Tables can successfully synthesize transformations for over 70% of test cases at interactive speeds, without requiring any input from users, making this an effective tool for both technical and non-technical users to prepare data for analytics.
</details></li>
</ul>
<hr>
<h2 id="Understanding-Forward-Process-of-Convolutional-Neural-Network"><a href="#Understanding-Forward-Process-of-Convolutional-Neural-Network" class="headerlink" title="Understanding Forward Process of Convolutional Neural Network"></a>Understanding Forward Process of Convolutional Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15090">http://arxiv.org/abs/2307.15090</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning">https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning</a></li>
<li>paper_authors: Peixin Tian</li>
<li>for: 这篇论文揭示了深度神经网络（CNNs）的选择性旋转处理。</li>
<li>methods: 论文解释了 activation function 作为一种分辨率机制，将输入数据的旋转性统一和量化。</li>
<li>results: 实验显示，这种定义的方法论网络可以根据统计指标来识别输入数据，可以通过结构化数学工具进行分析。我们的发现还揭示了人工神经网络和人脑的数据处理模式之间的一致性。<details>
<summary>Abstract</summary>
This paper reveal the selective rotation in the CNNs' forward processing. It elucidates the activation function as a discerning mechanism that unifies and quantizes the rotational aspects of the input data. Experiments show how this defined methodology reflects the progress network distinguish inputs based on statistical indicators, which can be comprehended or analyzed by applying structured mathematical tools. Our findings also unveil the consistency between artificial neural networks and the human brain in their data processing pattern.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is also known as "简化字符" or "简化字符".Translation notes:* "selective rotation" is translated as "选择的旋转" (选择的旋转)* "forward processing" is translated as "前进处理" (前进处理)* "activation function" is translated as "活化函数" (活化函数)* "progress network" is translated as "进步网络" (进步网络)* "statistical indicators" is translated as "统计指标" (统计指标)* "structured mathematical tools" is translated as "结构化数学工具" (结构化数学工具)
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Sleeping-Bandit-Problems-with-Multiple-Plays-Algorithm-and-Ranking-Application"><a href="#Adversarial-Sleeping-Bandit-Problems-with-Multiple-Plays-Algorithm-and-Ranking-Application" class="headerlink" title="Adversarial Sleeping Bandit Problems with Multiple Plays: Algorithm and Ranking Application"></a>Adversarial Sleeping Bandit Problems with Multiple Plays: Algorithm and Ranking Application</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14549">http://arxiv.org/abs/2307.14549</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianjun Yuan, Wei Lee Woon, Ludovik Coba</li>
<li>for: 这 paper 是为了解决在线推荐系统中的睡眠骑士问题，该问题具有固定、敌对损失和不确定的 i.i.d. 分布。</li>
<li>methods: 提出的算法基于睡眠骑士算法，并对单臂选择进行扩展，可以保证理论性能，增量误差 upper bounded by $\bigO(kN^2\sqrt{T\log T})$, где $k$ 是每个时间步选择的臂数，$N$ 是总臂数，$T$ 是时间轴。</li>
<li>results: 该 paper 获得了理论性能，增量误差 upper bounded by $\bigO(kN^2\sqrt{T\log T})$.<details>
<summary>Abstract</summary>
This paper presents an efficient algorithm to solve the sleeping bandit with multiple plays problem in the context of an online recommendation system. The problem involves bounded, adversarial loss and unknown i.i.d. distributions for arm availability. The proposed algorithm extends the sleeping bandit algorithm for single arm selection and is guaranteed to achieve theoretical performance with regret upper bounded by $\bigO(kN^2\sqrt{T\log T})$, where $k$ is the number of arms selected per time step, $N$ is the total number of arms, and $T$ is the time horizon.
</details>
<details>
<summary>摘要</summary>
（本文提出了一种有效的算法，用于解决在线推荐系统中的睡着投注问题。该问题包括 bounded,  adversarial 损失以及 unknown i.i.d. 分布 дляarm availability。提出的算法基于单个arm选择的睡着投注算法，并且能够保证理论性能， regret upper bounded by $\bigO(kN^2\sqrt{T\log T})$，where $k$ is the number of arms selected per time step, $N$ is the total number of arms, and $T$ is the time horizon。）
</details></li>
</ul>
<hr>
<h2 id="Controlling-the-Inductive-Bias-of-Wide-Neural-Networks-by-Modifying-the-Kernel’s-Spectrum"><a href="#Controlling-the-Inductive-Bias-of-Wide-Neural-Networks-by-Modifying-the-Kernel’s-Spectrum" class="headerlink" title="Controlling the Inductive Bias of Wide Neural Networks by Modifying the Kernel’s Spectrum"></a>Controlling the Inductive Bias of Wide Neural Networks by Modifying the Kernel’s Spectrum</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14531">http://arxiv.org/abs/2307.14531</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amnon Geifman, Daniel Barzilai, Ronen Basri, Meirav Galun</li>
<li>for: 该论文主要目的是提出一种 modify 宽神经网络的方法，以便根据任务需要调整宽神经网络的学习偏好。</li>
<li>methods: 该论文提出了一种新的构造kernel的方法，称为Modified Spectrum Kernels（MSK），可以用于 aproximate 具有愿景值的kernel。此外，该论文还提出了一种基于宽神经网络和Neural Tangent Kernels的对偶方法，称为预conditioned gradient descent方法，可以改变Gradient Descent算法的轨迹。</li>
<li>results: 该论文的实验结果表明，使用Modified Spectrum Kernels和预conditioned gradient descent方法可以在一些情况下获得 polynomial 和 exponential 的训练速度提升，而不会改变最终解。此外，该方法也是 computationally efficient 和简单实现的。<details>
<summary>Abstract</summary>
Wide neural networks are biased towards learning certain functions, influencing both the rate of convergence of gradient descent (GD) and the functions that are reachable with GD in finite training time. As such, there is a great need for methods that can modify this bias according to the task at hand. To that end, we introduce Modified Spectrum Kernels (MSKs), a novel family of constructed kernels that can be used to approximate kernels with desired eigenvalues for which no closed form is known. We leverage the duality between wide neural networks and Neural Tangent Kernels and propose a preconditioned gradient descent method, which alters the trajectory of GD. As a result, this allows for a polynomial and, in some cases, exponential training speedup without changing the final solution. Our method is both computationally efficient and simple to implement.
</details>
<details>
<summary>摘要</summary>
广阶层神经网络具有倾向于学习特定函数的倾向，这影响了梯度下降（GD）的速度和训练时间内可到的函数。因此，有一个很大的需求，即可以根据任务改变这种倾向。为了解决这个问题，我们介绍Modified Spectrum Kernels（MSK），一种新的建构kernel的家族，可以用来 aproximate kernel with desired eigenvalues，即无法known的closed form。我们利用广阶层神经网络和Neural Tangent Kernels的 dual性，提出预调corrected gradient descent方法，这个方法可以改变GD的轨迹。因此，这可以实现 polynomial 和，在一些情况下，exponential training speedup，而不需要变更最终解。我们的方法具有computational efficiency和简单实现。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Estimation-in-Mixed-Membership-Stochastic-Block-Models"><a href="#Optimal-Estimation-in-Mixed-Membership-Stochastic-Block-Models" class="headerlink" title="Optimal Estimation in Mixed-Membership Stochastic Block Models"></a>Optimal Estimation in Mixed-Membership Stochastic Block Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14530">http://arxiv.org/abs/2307.14530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fedor Noskov, Maxim Panov</li>
<li>for: 研究混合成员 Stochastic Block Model（MMSB），用于描述图像中的 overlap community 结构。</li>
<li>methods: 比较不同方法的可重建性，并提出一种新的估计器，实现最小最大 Lower Bound（LB）。</li>
<li>results: 在一系列实验中，证明了新估计器的可靠性和高效性。<details>
<summary>Abstract</summary>
Community detection is one of the most critical problems in modern network science. Its applications can be found in various fields, from protein modeling to social network analysis. Recently, many papers appeared studying the problem of overlapping community detection, where each node of a network may belong to several communities. In this work, we consider Mixed-Membership Stochastic Block Model (MMSB) first proposed by Airoldi et al. (2008). MMSB provides quite a general setting for modeling overlapping community structure in graphs. The central question of this paper is to reconstruct relations between communities given an observed network. We compare different approaches and establish the minimax lower bound on the estimation error. Then, we propose a new estimator that matches this lower bound. Theoretical results are proved under fairly general conditions on the considered model. Finally, we illustrate the theory in a series of experiments.
</details>
<details>
<summary>摘要</summary>
社区探测是现代网络科学中最关键的问题之一。它的应用可以在各个领域找到，从蛋白质模型到社会网络分析。最近，许多论文研究了过lapping community detection问题，其中每个网络节点可能属于多个社区。在这项工作中，我们考虑了Airoldi等人于2008年提出的混合会员随机块模型（MMSB）。MMSB提供了一个非常通用的社区结构模型化图的方法。我们的中心问题是根据观察网络重建社区之间的关系。我们比较了不同的方法，并证明了最小最大下界 Error的下界。然后，我们提出了一个新的估计器，与这个下界匹配。我们的理论结果在较为通用的模型假设下得到了证明。最后，我们在一系列实验中证明了我们的理论。
</details></li>
</ul>
<hr>
<h2 id="Function-Value-Learning-Adaptive-Learning-Rates-Based-on-the-Polyak-Stepsize-and-Function-Splitting-in-ERM"><a href="#Function-Value-Learning-Adaptive-Learning-Rates-Based-on-the-Polyak-Stepsize-and-Function-Splitting-in-ERM" class="headerlink" title="Function Value Learning: Adaptive Learning Rates Based on the Polyak Stepsize and Function Splitting in ERM"></a>Function Value Learning: Adaptive Learning Rates Based on the Polyak Stepsize and Function Splitting in ERM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14528">http://arxiv.org/abs/2307.14528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillaume Garrigos, Robert M. Gower, Fabian Schaipp</li>
<li>For: The paper focuses on solving a finite sum-of-terms problem, also known as empirical risk minimization, using stochastic gradient descent (SGD) with an adaptive step size.* Methods: The paper proposes two variants of SGD, called $\texttt{SPS}<em>+$ and $\texttt{FUVAL}$, which make use of the sampled loss values and gradually learn the loss values at optimality.* Results: The paper shows that $\texttt{SPS}</em>+$ achieves the best known rates of convergence for SGD in the Lipschitz non-smooth, but the new $\texttt{FUVAL}$ method does not offer any clear theoretical or practical advantage over SGD.Here are the three information points in Simplified Chinese text:* For: 本文解决了一个finite sum-of-terms problem，也就是empirical risk minimization，使用stochastic gradient descent（SGD）的adaptive step size。* Methods: 本文提出了两种SGD变种，即$\texttt{SPS}<em>+$和$\texttt{FUVAL}$，它们利用样本损失值来进行优化。* Results: 本文证明了$\texttt{SPS}</em>+$在Lipschitz non-smooth中达到了最佳known的迭代速率，但新的$\texttt{FUVAL}$方法并没有明显的理论或实践优势。<details>
<summary>Abstract</summary>
Here we develop variants of SGD (stochastic gradient descent) with an adaptive step size that make use of the sampled loss values. In particular, we focus on solving a finite sum-of-terms problem, also known as empirical risk minimization. We first detail an idealized adaptive method called $\texttt{SPS}_+$ that makes use of the sampled loss values and assumes knowledge of the sampled loss at optimality. This $\texttt{SPS}_+$ is a minor modification of the SPS (Stochastic Polyak Stepsize) method, where the step size is enforced to be positive. We then show that $\texttt{SPS}_+$ achieves the best known rates of convergence for SGD in the Lipschitz non-smooth. We then move onto to develop $\texttt{FUVAL}$, a variant of $\texttt{SPS}_+$ where the loss values at optimality are gradually learned, as opposed to being given. We give three viewpoints of $\texttt{FUVAL}$, as a projection based method, as a variant of the prox-linear method, and then as a particular online SGD method. We then present a convergence analysis of $\texttt{FUVAL}$ and experimental results. The shortcomings of our work is that the convergence analysis of $\texttt{FUVAL}$ shows no advantage over SGD. Another shortcomming is that currently only the full batch version of $\texttt{FUVAL}$ shows a minor advantages of GD (Gradient Descent) in terms of sensitivity to the step size. The stochastic version shows no clear advantage over SGD. We conjecture that large mini-batches are required to make $\texttt{FUVAL}$ competitive.   Currently the new $\texttt{FUVAL}$ method studied in this paper does not offer any clear theoretical or practical advantage. We have chosen to make this draft available online nonetheless because of some of the analysis techniques we use, such as the non-smooth analysis of $\texttt{SPS}_+$, and also to show an apparently interesting approach that currently does not work.
</details>
<details>
<summary>摘要</summary>
我们在这里开发出了SGD（测量函数下降）的变体，具有适应步长的优点。具体来说，我们专注于解决一个总和形式的问题，也就是一个empirical risk minimization。我们首先介绍了一个理想化的自适应方法，called $\texttt{SPS}_+$,它使用抽象的损失值，并假设知道抽象损失的最佳值。这个 $\texttt{SPS}_+ $ 是 SPS（ Stochastic Polyak Stepsize）方法的小修改，步长强制为正。我们随后证明了 $\texttt{SPS}_+ $ 在 Lipschitz 非均匀中的最佳知识率。然后我们开发了 $\texttt{FUVAL} $, 这是 $\texttt{SPS}_+ $ 的一个改进版本，损失值在最佳值 Gradually 学习，而不是直接知道。我们从三个不同的角度来探讨 $\texttt{FUVAL} $, 分别是投影基于方法、变形的 prox-linear 方法和在线 SGD 方法。我们随后提供了 $\texttt{FUVAL} $ 的内部分析和实验结果。我们的研究的缺点是 $\texttt{FUVAL} $ 的内部分析不会提供任何优点，而且在某些情况下，SGD 可能比 $\texttt{FUVAL} $ 更为稳定。此外，目前只有全批量版本的 $\texttt{FUVAL} $ 表现出一定的优点，而测量版本则未能获得明显的优点。我们推测需要大小批量才能使 $\texttt{FUVAL} $ 竞争。总之，我们的新方法 $\texttt{FUVAL} $ 目前无法提供任何明显的理论或实践优点。不过，我们使用了一些有趣的分析技巧，例如非均匀分析 $\texttt{SPS}_+ $，以及展示了一个可能不太有用的方法。因此，我们选择发布这份草稿，以便在未来进一步探索这个方向。
</details></li>
</ul>
<hr>
<h2 id="Open-Problems-in-Computer-Vision-for-Wilderness-SAR-and-The-Search-for-Patricia-Wu-Murad"><a href="#Open-Problems-in-Computer-Vision-for-Wilderness-SAR-and-The-Search-for-Patricia-Wu-Murad" class="headerlink" title="Open Problems in Computer Vision for Wilderness SAR and The Search for Patricia Wu-Murad"></a>Open Problems in Computer Vision for Wilderness SAR and The Search for Patricia Wu-Murad</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14527">http://arxiv.org/abs/2307.14527</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/crasar/wisar">https://github.com/crasar/wisar</a></li>
<li>paper_authors: Thomas Manzini, Robin Murphy</li>
<li>for: 这篇论文探讨了在日本 Wu-Murad 野外搜救（WSAR）活动中应用两种计算机视觉系统，即可靠的DET 超vised学习模型和无监督的RX spectral分类器，并发现了3个未来研究的方向。</li>
<li>methods: 这篇论文使用了98.9 GB的飞行器影像数据，并应用了EfficientDET 建模和RX spectral分类器来检测缺失人员。</li>
<li>results: 论文发现，现有的方法中只有3种（2个无监督的和1个结构不确定）在实际WSAR操作中被使用，并且选择了EfficientDET 建模和RX spectral分类器作为最佳选择。然而，实际应用中，EfficientDET 模型具有 statistically 等效的性能，但它在实际情况中存在许多假阳性（如 mistakenly 识别树枝和石头为人）和假负性（如不能识别搜救队成员）的问题。<details>
<summary>Abstract</summary>
This paper details the challenges in applying two computer vision systems, an EfficientDET supervised learning model and the unsupervised RX spectral classifier, to 98.9 GB of drone imagery from the Wu-Murad wilderness search and rescue (WSAR) effort in Japan and identifies 3 directions for future research. There have been at least 19 proposed approaches and 3 datasets aimed at locating missing persons in drone imagery, but only 3 approaches (2 unsupervised and 1 of an unknown structure) are referenced in the literature as having been used in an actual WSAR operation. Of these proposed approaches, the EfficientDET architecture and the unsupervised spectral RX classifier were selected as the most appropriate for this setting. The EfficientDET model was applied to the HERIDAL dataset and despite achieving performance that is statistically equivalent to the state-of-the-art, the model fails to translate to the real world in terms of false positives (e.g., identifying tree limbs and rocks as people), and false negatives (e.g., failing to identify members of the search team). The poor results in practice for algorithms that showed good results on datasets suggest 3 areas of future research: more realistic datasets for wilderness SAR, computer vision models that are capable of seamlessly handling the variety of imagery that can be collected during actual WSAR operations, and better alignment on performance measures.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>More realistic datasets for wilderness SAR: The current datasets used for training and testing computer vision models may not accurately reflect the real-world conditions and variability of imagery collected during actual WSAR operations.2. Computer vision models that can handle diverse imagery: The models need to be able to seamlessly handle the variety of imagery that can be collected during actual WSAR operations, including different lighting conditions, weather, and terrain.3. Better alignment on performance measures: The performance of the models needs to be evaluated using relevant and meaningful metrics that reflect the specific goals and requirements of WSAR operations.The paper also discusses the limitations of the EfficientDET model and the unsupervised spectral RX classifier, including their poor performance in real-world scenarios due to false positives (identifying tree limbs and rocks as people) and false negatives (failing to identify members of the search team).</details></li>
</ol>
<hr>
<h2 id="A-new-algorithm-for-Subgroup-Set-Discovery-based-on-Information-Gain"><a href="#A-new-algorithm-for-Subgroup-Set-Discovery-based-on-Information-Gain" class="headerlink" title="A new algorithm for Subgroup Set Discovery based on Information Gain"></a>A new algorithm for Subgroup Set Discovery based on Information Gain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15089">http://arxiv.org/abs/2307.15089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Gómez-Bravo, Aaron García, Guillermo Vigueras, Belén Ríos, Alejandro Rodríguez-González</li>
<li>For: The paper aims to propose a new pattern discovery algorithm called Information Gained Subgroup Discovery (IGSD) to address the limitations of state-of-the-art pattern discovery algorithms.* Methods: The IGSD algorithm combines Information Gain (IG) and Odds Ratio (OR) as multi-criteria for pattern selection. It also uses a novel approach to explore the subgroup space and evaluate the discovered patterns.* Results: The paper evaluates the performance of IGSD with two state-of-the-art pattern discovery algorithms (FSSD and SSD++) on 11 datasets. The results show that IGSD provides more reliable patterns and better agreement with domain experts than the other two algorithms. Additionally, IGSD provides better OR values, indicating a higher dependence between patterns and targets.Here’s the simplified Chinese text for the three key information points:*  для：本文提出了一种新的模式发现算法 called Information Gained Subgroup Discovery（IGSD），以解决现有模式发现算法的限制。* 方法：IGSD算法将信息增量（IG）和很大比率（OR）作为多个标准来选择模式。它还使用了一种新的方法来探索 subgroup空间和评估发现的模式。* 结果：本文对IGSD算法与两种现有模式发现算法（FSSD和SSD++）在11个数据集上进行了性能评估。结果显示，IGSD算法提供了更可靠的模式和与领域专家的一致性更高。此外，IGSD算法提供了更高的OR值，表明模式和目标之间的依赖性更高。<details>
<summary>Abstract</summary>
Pattern discovery is a machine learning technique that aims to find sets of items, subsequences, or substructures that are present in a dataset with a higher frequency value than a manually set threshold. This process helps to identify recurring patterns or relationships within the data, allowing for valuable insights and knowledge extraction. In this work, we propose Information Gained Subgroup Discovery (IGSD), a new SD algorithm for pattern discovery that combines Information Gain (IG) and Odds Ratio (OR) as a multi-criteria for pattern selection. The algorithm tries to tackle some limitations of state-of-the-art SD algorithms like the need for fine-tuning of key parameters for each dataset, usage of a single pattern search criteria set by hand, usage of non-overlapping data structures for subgroup space exploration, and the impossibility to search for patterns by fixing some relevant dataset variables. Thus, we compare the performance of IGSD with two state-of-the-art SD algorithms: FSSD and SSD++. Eleven datasets are assessed using these algorithms. For the performance evaluation, we also propose to complement standard SD measures with IG, OR, and p-value. Obtained results show that FSSD and SSD++ algorithms provide less reliable patterns and reduced sets of patterns than IGSD algorithm for all datasets considered. Additionally, IGSD provides better OR values than FSSD and SSD++, stating a higher dependence between patterns and targets. Moreover, patterns obtained for one of the datasets used, have been validated by a group of domain experts. Thus, patterns provided by IGSD show better agreement with experts than patterns obtained by FSSD and SSD++ algorithms. These results demonstrate the suitability of the IGSD as a method for pattern discovery and suggest that the inclusion of non-standard SD metrics allows to better evaluate discovered patterns.
</details>
<details>
<summary>摘要</summary>
“ patrern 发现”是一种机器学习技术，旨在找到数据集中出现频率较高的项集、 subsequences 或 substructures。这个过程可以帮助发现数据中的循环模式或关系，从而提供有价值的发现和知识提取。在这个工作中，我们提出了一种新的 patrern 发现算法，即 Information Gained Subgroup Discovery（IGSD）。该算法结合信息增益（IG）和偶极率（OR）作为多个评价标准，用于 patrern 选择。该算法希望解决现有 patrern 发现算法的一些局限性，如手动设置的关键参数、使用单一 patrern 搜索标准、非重叠数据结构等。因此，我们与现有 patrern 发现算法进行比较，包括 FSSD 和 SSD++。我们对 Eleven 个数据集进行了这些算法的评价。为了评价性能，我们还提出了一些不同于标准 patrern 搜索度量的方式，包括 IG、OR 和 p-value。结果显示，FSSD 和 SSD++ 算法提供的 patrern 较为不可靠，而 IGSD 算法对所有数据集都提供了更好的 patrern。此外，IGSD 算法对 patrern 和目标变量之间的相互关系提供了更高的依赖性。此外，IGSD 算法对一个数据集进行了验证，并得到了域专家的认可。因此，IGSD 算法提供的 patrern 更好地匹配专家的意见。这些结果表明 IGSD 算法是一种适合 patrern 发现的方法，并且包括非标准 patrern 搜索度量可以更好地评价发现的 patrern。
</details></li>
</ul>
<hr>
<h2 id="Bug-Characterization-in-Machine-Learning-based-Systems"><a href="#Bug-Characterization-in-Machine-Learning-based-Systems" class="headerlink" title="Bug Characterization in Machine Learning-based Systems"></a>Bug Characterization in Machine Learning-based Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14512">http://arxiv.org/abs/2307.14512</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ml-bugs-2022/replication-package">https://github.com/ml-bugs-2022/replication-package</a></li>
<li>paper_authors: Mohammad Mehdi Morovati, Amin Nikanjam, Florian Tambon, Foutse Khomh, Zhen Ming, Jiang</li>
<li>for: This paper aims to investigate the characteristics of bugs in Machine Learning (ML)-based software systems and the differences between ML and non-ML bugs from a maintenance viewpoint.</li>
<li>methods: The paper uses a dataset of 447,948 GitHub repositories that use one of the three most popular ML frameworks (TensorFlow, Keras, and PyTorch) to extract 300 repositories with the highest number of closed issues. The authors then manually inspect the extracted repositories to exclude non-ML-based systems, and investigate 386 sampled reported issues to determine whether they affect ML components or not.</li>
<li>results: The paper finds that nearly half of the real issues reported in ML-based systems are ML bugs, indicating that ML components are more error-prone than non-ML components. The authors also identify the root causes, symptoms, and required fixing time for 109 identified ML bugs, and find that ML bugs have significantly different characteristics compared to non-ML bugs in terms of the complexity of bug-fixing. The results suggest that fixing ML bugs is more costly and ML components are more error-prone compared to non-ML bugs and non-ML components respectively, highlighting the importance of paying attention to the reliability of ML components in ML-based systems.<details>
<summary>Abstract</summary>
Rapid growth of applying Machine Learning (ML) in different domains, especially in safety-critical areas, increases the need for reliable ML components, i.e., a software component operating based on ML. Understanding the bugs characteristics and maintenance challenges in ML-based systems can help developers of these systems to identify where to focus maintenance and testing efforts, by giving insights into the most error-prone components, most common bugs, etc. In this paper, we investigate the characteristics of bugs in ML-based software systems and the difference between ML and non-ML bugs from the maintenance viewpoint. We extracted 447,948 GitHub repositories that used one of the three most popular ML frameworks, i.e., TensorFlow, Keras, and PyTorch. After multiple filtering steps, we select the top 300 repositories with the highest number of closed issues. We manually investigate the extracted repositories to exclude non-ML-based systems. Our investigation involved a manual inspection of 386 sampled reported issues in the identified ML-based systems to indicate whether they affect ML components or not. Our analysis shows that nearly half of the real issues reported in ML-based systems are ML bugs, indicating that ML components are more error-prone than non-ML components. Next, we thoroughly examined 109 identified ML bugs to identify their root causes, symptoms, and calculate their required fixing time. The results also revealed that ML bugs have significantly different characteristics compared to non-ML bugs, in terms of the complexity of bug-fixing (number of commits, changed files, and changed lines of code). Based on our results, fixing ML bugs are more costly and ML components are more error-prone, compared to non-ML bugs and non-ML components respectively. Hence, paying a significant attention to the reliability of the ML components is crucial in ML-based systems.
</details>
<details>
<summary>摘要</summary>
Machine Learning (ML) 在不同领域的快速应用导致了可靠 ML 组件的需求增加，即基于 ML 的软件组件。了解 ML 系统中 bug 的特点和维护挑战可以帮助 ML 系统开发者identify где要集中维护和测试努力，提供了关于最常出现的bug、最常出现的错误等信息。在这篇论文中，我们研究了 ML 系统中 bug 的特点和非 ML 系统中 bug 的区别从维护角度来 investigate。我们从 GitHub 上抽取了使用最Popular ML 框架之一的300个仓库，并经过多个筛选步骤后，选择了最高数量关闭 Issue 的仓库。我们手动验证了提取的仓库，以确保它们是 ML 基于系统。我们的调查包括对386个样本问题的手动检查，以确定它们是否affect ML 组件。我们的分析结果表明，ML 系统中 Reported 的实际问题大约占 ML 系统总数的46.7%，表明 ML 组件比非 ML 组件更容易出现错误。接着，我们对109个 ID 为 ML 错误的问题进行了详细分析，以确定其根本原因、症状和修复时间。结果还表明，ML 错误和非 ML 错误在修复复杂性、修复时间等方面有很大差异。根据我们的结果，修复 ML 错误需要更多的时间和精力，而 ML 组件也更容易出现错误。因此，在 ML 基于系统中，需要付出更大的注意力来确保 ML 组件的可靠性。
</details></li>
</ul>
<hr>
<h2 id="A-Predictive-Model-of-Digital-Information-Engagement-Forecasting-User-Engagement-With-English-Words-by-Incorporating-Cognitive-Biases-Computational-Linguistics-and-Natural-Language-Processing"><a href="#A-Predictive-Model-of-Digital-Information-Engagement-Forecasting-User-Engagement-With-English-Words-by-Incorporating-Cognitive-Biases-Computational-Linguistics-and-Natural-Language-Processing" class="headerlink" title="A Predictive Model of Digital Information Engagement: Forecasting User Engagement With English Words by Incorporating Cognitive Biases, Computational Linguistics and Natural Language Processing"></a>A Predictive Model of Digital Information Engagement: Forecasting User Engagement With English Words by Incorporating Cognitive Biases, Computational Linguistics and Natural Language Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14500">http://arxiv.org/abs/2307.14500</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nimrod Dvir, Elaine Friedman, Suraj Commuri, Fan yang, Jennifer Romano</li>
<li>for: 这种研究旨在开发一种用于数字信息参与度（IE）的预测模型，即READ模型，该模型基于汇集了主要认知偏见和计算语言学的理论框架，以提供一种多维度的信息参与度观察。</li>
<li>methods: 该研究使用了50个随机选择的同义词对（共100个词）从WordNet数据库，通过在线调查（参与者数为80,500人）测量这些词的参与度，以计算READ属性。</li>
<li>results: 研究发现，READ模型能够准确预测一个词的参与度，并在84%的 случа子中能够 correctly distinguish 词与其同义词之间的参与度。该模型在不同领域，如商业、教育、政府和医疗等领域，可能有效地提高内容参与度和AI语言模型的发展。<details>
<summary>Abstract</summary>
This study introduces and empirically tests a novel predictive model for digital information engagement (IE) - the READ model, an acronym for the four pivotal attributes of engaging information: Representativeness, Ease-of-use, Affect, and Distribution. Conceptualized within the theoretical framework of Cumulative Prospect Theory, the model integrates key cognitive biases with computational linguistics and natural language processing to develop a multidimensional perspective on information engagement. A rigorous testing protocol was implemented, involving 50 randomly selected pairs of synonymous words (100 words in total) from the WordNet database. These words' engagement levels were evaluated through a large-scale online survey (n = 80,500) to derive empirical IE metrics. The READ attributes for each word were then computed and their predictive efficacy examined. The findings affirm the READ model's robustness, accurately predicting a word's IE level and distinguishing the more engaging word from a pair of synonyms with an 84% accuracy rate. The READ model's potential extends across various domains, including business, education, government, and healthcare, where it could enhance content engagement and inform AI language model development and generative text work. Future research should address the model's scalability and adaptability across different domains and languages, thereby broadening its applicability and efficacy.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="HUGE-Huge-Unsupervised-Graph-Embeddings-with-TPUs"><a href="#HUGE-Huge-Unsupervised-Graph-Embeddings-with-TPUs" class="headerlink" title="HUGE: Huge Unsupervised Graph Embeddings with TPUs"></a>HUGE: Huge Unsupervised Graph Embeddings with TPUs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14490">http://arxiv.org/abs/2307.14490</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brandon Mayer, Anton Tsitsulin, Hendrik Fichtenberger, Jonathan Halcrow, Bryan Perozzi</li>
<li>for: 这篇论文是为了快速分析大规模图像而设计的，以便解决后续机器学习任务。</li>
<li>methods: 论文使用了Tensor Processing Units（TPUs）和可配置的高带宽内存来实现高性能图嵌入架构，以解决大规模图像的嵌入问题。</li>
<li>results: 论文验证了嵌入空间质量在真实和 sintetic 大规模数据集上，并达到了高性能。<details>
<summary>Abstract</summary>
Graphs are a representation of structured data that captures the relationships between sets of objects. With the ubiquity of available network data, there is increasing industrial and academic need to quickly analyze graphs with billions of nodes and trillions of edges. A common first step for network understanding is Graph Embedding, the process of creating a continuous representation of nodes in a graph. A continuous representation is often more amenable, especially at scale, for solving downstream machine learning tasks such as classification, link prediction, and clustering. A high-performance graph embedding architecture leveraging Tensor Processing Units (TPUs) with configurable amounts of high-bandwidth memory is presented that simplifies the graph embedding problem and can scale to graphs with billions of nodes and trillions of edges. We verify the embedding space quality on real and synthetic large-scale datasets.
</details>
<details>
<summary>摘要</summary>
GRAPH Embedding 是一种将图像转换为连续表示的技术，以便更好地解决大规模图像的下游机器学习任务，如分类、链接预测和团 clustering。在大量网络数据的时代，快速分析大规模图像变得越来越重要。我们提出了一种高性能的图 Embedding 架构，利用 tensor Processing Units (TPUs) 和可配置的高带宽内存，可以简化图 Embedding 问题，并可扩展到百亿个节点和万亿个边的图像。我们对真实和 sintetic 大规模数据进行了验证，以证明 embedding 空间质量。Note: "GRAPH" is written in capital letters in Simplified Chinese to emphasize the importance of the concept.
</details></li>
</ul>
<hr>
<h2 id="Role-of-Image-Acquisition-and-Patient-Phenotype-Variations-in-Automatic-Segmentation-Model-Generalization"><a href="#Role-of-Image-Acquisition-and-Patient-Phenotype-Variations-in-Automatic-Segmentation-Model-Generalization" class="headerlink" title="Role of Image Acquisition and Patient Phenotype Variations in Automatic Segmentation Model Generalization"></a>Role of Image Acquisition and Patient Phenotype Variations in Automatic Segmentation Model Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14482">http://arxiv.org/abs/2307.14482</a></li>
<li>repo_url: None</li>
<li>paper_authors: Timothy L. Kline, Sumana Ramanathan, Harrison C. Gottlich, Panagiotis Korfiatis, Adriana V. Gregory</li>
<li>for: 这个研究是为了评估自动医疗影像分割模型在不同频谱和疾病类型上的性能和泛化能力。</li>
<li>methods: 研究使用了多种数据集，包括非对照和对照肝脏CT影像数据，并对这些数据进行了100个示例的训练和验证，以便分割肝脏、胆囊和脾脏。最终的模型被测试在100个非对照PKD病人的CT影像上。性能被评估使用了 dice相似度、Jacard相似度、TPR和精度。</li>
<li>results: 研究发现，使用更广泛的数据集可以提高模型的泛化性能和外域性能，无需额外训练或特定的疾病类型。例如，模型通过在25%的数据集上训练，与仅使用域内数据进行训练相同的 dice相似度。I hope that helps!<details>
<summary>Abstract</summary>
Purpose: This study evaluated the out-of-domain performance and generalization capabilities of automated medical image segmentation models, with a particular focus on adaptation to new image acquisitions and disease type.   Materials: Datasets from both non-contrast and contrast-enhanced abdominal CT scans of healthy patients and those with polycystic kidney disease (PKD) were used. A total of 400 images (100 non-contrast controls, 100 contrast controls, 100 non-contrast PKD, 100 contrast PKD) were utilized for training/validation of models to segment kidneys, livers, and spleens, and the final models were then tested on 100 non-contrast CT images of patients affected by PKD. Performance was evaluated using Dice, Jaccard, TPR, and Precision.   Results: Models trained on a diverse range of data showed no worse performance than models trained exclusively on in-domain data when tested on in-domain data. For instance, the Dice similarity of the model trained on 25% from each dataset was found to be non-inferior to the model trained purely on in-domain data.   Conclusions: The results indicate that broader training examples significantly enhances model generalization and out-of-domain performance, thereby improving automated segmentation tools' applicability in clinical settings. The study's findings provide a roadmap for future research to adopt a data-centric approach in medical image AI model development.
</details>
<details>
<summary>摘要</summary>
目的：本研究评估了自动医疗图像分割模型的域外性和总体可扩展性，尤其是适应新图像获取和疾病类型的适应性。材料：来自非对照和增强的腹部CT扫描图像的健康患者和肾脏癌病（PKD）的数据集被使用。总共使用400张图像（100张非对照控制、100张对照控制、100张非对照PKD、100张对照PKD）进行模型训练/验证，并将最终模型测试在100张非对照CT图像上。性能被评估使用 dice、jaccard、TPR和精度。结果：模型训练在多样化的数据上表现和专门训练在域内数据上无分别性能。例如，模型训练使用25%的数据从每个数据集得到的 dice相似性与专门训练在域内数据上的模型相似。结论：结果表明，更广泛的训练示例可以显著提高模型的总体可扩展性和域外性，因此提高自动分割工具在临床应用中的可靠性。这项研究的结果为未来医疗图像AI模型开发提供了一个路线图。
</details></li>
</ul>
<hr>
<h2 id="Equitable-Time-Varying-Pricing-Tariff-Design-A-Joint-Learning-and-Optimization-Approach"><a href="#Equitable-Time-Varying-Pricing-Tariff-Design-A-Joint-Learning-and-Optimization-Approach" class="headerlink" title="Equitable Time-Varying Pricing Tariff Design: A Joint Learning and Optimization Approach"></a>Equitable Time-Varying Pricing Tariff Design: A Joint Learning and Optimization Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15088">http://arxiv.org/abs/2307.15088</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liudong Chen, Bolun Xu</li>
<li>for: 这个论文的目的是设计合理的时间变化价格政策，以激励消费者减少电力峰值需求，同时保护低收入消费者免受价格涨升的影响。</li>
<li>methods: 该论文提出一种基于学习联合标定优化方法，通过历史价格和需求响应数据来捕捉高维和非线性消费者价格响应行为。</li>
<li>results: 模拟使用实际消费者数据显示，我们的公平价格政策能够保护低收入消费者免受价格涨升的影响，同时有效激励消费者减少峰值需求，使得供应商公司能够实现收益回报。<details>
<summary>Abstract</summary>
Time-varying pricing tariffs incentivize consumers to shift their electricity demand and reduce costs, but may increase the energy burden for consumers with limited response capability. The utility must thus balance affordability and response incentives when designing these tariffs by considering consumers' response expectations. This paper proposes a joint learning-based identification and optimization method to design equitable time-varying tariffs. Our proposed method encodes historical prices and demand response data into a recurrent neural network (RNN) to capture high-dimensional and non-linear consumer price response behaviors. We then embed the RNN into the tariff design optimization, formulating a non-linear optimization problem with a quadratic objective. We propose a gradient-based solution method that achieves fast and scalable computation. Simulation using real-world consumer data shows that our equitable tariffs protect low-income consumers from price surges while effectively motivating consumers to reduce peak demand. The method also ensures revenue recovery for the utility company and achieves robust performance against demand response uncertainties and prediction errors.
</details>
<details>
<summary>摘要</summary>
时间变化的价格批价可以鼓励消费者调整电力需求，从而降低成本，但可能增加有限响应能力的消费者的能源负担。公司因此需要平衡可持续性和响应奖励，当设计时变化价格时。本文提出一种基于学习的同时标识和优化方法，用于设计公平的时变价格。我们使用循环神经网络（RNN）编码历史价格和需求响应数据，以捕捉高维和非线性的消费者价格响应行为。然后，我们将RNNembed到价格设计优化中，形成非线性优化问题的quadratic对象。我们提出一种梯度基于的解决方案，可以实现快速和可扩展的计算。通过使用实际的消费者数据进行模拟，我们发现我们的公平价格可以保护低收入消费者免受价格涨升，同时有效地鼓励消费者减少峰值需求。此外，我们的方法还确保了公司的收益恢复和对需求响应不确定性和预测错误的稳定性。
</details></li>
</ul>
<hr>
<h2 id="Limits-to-Reservoir-Learning"><a href="#Limits-to-Reservoir-Learning" class="headerlink" title="Limits to Reservoir Learning"></a>Limits to Reservoir Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14474">http://arxiv.org/abs/2307.14474</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anthony M. Polloreno</li>
<li>for: 这 paper 是研究机器学习的能力受到物理限制的工作。</li>
<li>methods: 作者使用信息处理容量（IPC）来衡量受到噪声的干扰下，某种特定的循环网络（即散射器）的性能下降。</li>
<li>results: 作者发现，IPC 是最多是一个多项式函数关系于系统大小 n，而且在噪声存在的情况下，这种干扰会使某种函数家族需要 exponential 数量的样本来学习。<details>
<summary>Abstract</summary>
In this work, we bound a machine's ability to learn based on computational limitations implied by physicality. We start by considering the information processing capacity (IPC), a normalized measure of the expected squared error of a collection of signals to a complete basis of functions. We use the IPC to measure the degradation under noise of the performance of reservoir computers, a particular kind of recurrent network, when constrained by physical considerations. First, we show that the IPC is at most a polynomial in the system size $n$, even when considering the collection of $2^n$ possible pointwise products of the $n$ output signals. Next, we argue that this degradation implies that the family of functions represented by the reservoir requires an exponential number of samples to learn in the presence of the reservoir's noise. Finally, we conclude with a discussion of the performance of the same collection of $2^n$ functions without noise when being used for binary classification.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们约束机器学习的能力基于物理限制所含的计算限制。我们开始思考信息处理容量（IPC），一种正规化的函数收敛率的标准化量。我们使用IPC来度量噪声下机器学习性能的下降，特别是当受到物理限制时。首先，我们证明IPC在系统大小$n$上是最多一个多项式函数。接着，我们 argue that这种下降表明了由泵函数表示的家族需要很多样本来在噪声下学习。最后，我们讨论无噪声情况下同样的$2^n$函数在二分类 зада务中的表现。
</details></li>
</ul>
<hr>
<h2 id="What-Kinds-of-Contracts-Do-ML-APIs-Need"><a href="#What-Kinds-of-Contracts-Do-ML-APIs-Need" class="headerlink" title="What Kinds of Contracts Do ML APIs Need?"></a>What Kinds of Contracts Do ML APIs Need?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14465">http://arxiv.org/abs/2307.14465</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samantha Syeda Khairunnesa, Shibbir Ahmed, Sayem Mohammad Imtiaz, Hridesh Rajan, Gary T. Leavens</li>
<li>For: This paper aims to identify the most commonly needed contracts for Machine Learning (ML) APIs and understand the root causes and effects of ML contract violations.* Methods: The authors conducted an empirical study of posts on Stack Overflow to extract 413 informal API specifications for the four most often-discussed ML libraries (TensorFlow, Scikit-learn, Keras, and PyTorch). They used these specifications to understand the common patterns of ML contract violations and the need for advanced ML software expertise to understand ML contracts.* Results: The study found that the most commonly needed contracts for ML APIs are checking constraints on single arguments of an API or on the order of API calls. The authors also noted a need to combine behavioral and temporal contract mining approaches to better understand ML APIs and design effective contract languages.<details>
<summary>Abstract</summary>
Recent work has shown that Machine Learning (ML) programs are error-prone and called for contracts for ML code. Contracts, as in the design by contract methodology, help document APIs and aid API users in writing correct code. The question is: what kinds of contracts would provide the most help to API users? We are especially interested in what kinds of contracts help API users catch errors at earlier stages in the ML pipeline. We describe an empirical study of posts on Stack Overflow of the four most often-discussed ML libraries: TensorFlow, Scikit-learn, Keras, and PyTorch. For these libraries, our study extracted 413 informal (English) API specifications. We used these specifications to understand the following questions. What are the root causes and effects behind ML contract violations? Are there common patterns of ML contract violations? When does understanding ML contracts require an advanced level of ML software expertise? Could checking contracts at the API level help detect the violations in early ML pipeline stages? Our key findings are that the most commonly needed contracts for ML APIs are either checking constraints on single arguments of an API or on the order of API calls. The software engineering community could employ existing contract mining approaches to mine these contracts to promote an increased understanding of ML APIs. We also noted a need to combine behavioral and temporal contract mining approaches. We report on categories of required ML contracts, which may help designers of contract languages.
</details>
<details>
<summary>摘要</summary>
We conducted an empirical study of posts on Stack Overflow about the four most commonly discussed ML libraries: TensorFlow, Scikit-learn, Keras, and PyTorch. For these libraries, we extracted 413 informal (English) API specifications. We used these specifications to answer the following questions:1. What are the root causes and effects of ML contract violations?2. Are there common patterns of ML contract violations?3. When does understanding ML contracts require an advanced level of ML software expertise?4. Can checking contracts at the API level help detect violations in early ML pipeline stages?Our key findings are that the most commonly needed contracts for ML APIs are:1. Checking constraints on single arguments of an API2. Checking the order of API callsWe also noted a need to combine behavioral and temporal contract mining approaches. Additionally, we categorized the required ML contracts, which may help designers of contract languages.
</details></li>
</ul>
<hr>
<h2 id="Training-Quantum-Boltzmann-Machines-with-Coresets"><a href="#Training-Quantum-Boltzmann-Machines-with-Coresets" class="headerlink" title="Training Quantum Boltzmann Machines with Coresets"></a>Training Quantum Boltzmann Machines with Coresets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14459">http://arxiv.org/abs/2307.14459</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joshua Viszlai, Teague Tomesh, Pranav Gokhale, Eric Anschuetz, Frederic T. Chong</li>
<li>for: 加速近期量子算法在半导体设备上的应用，使用核心集技术。</li>
<li>methods: 使用核心集取代全数据集，以降低训练时间。</li>
<li>results: 使用核心集可以大幅减少训练时间，减少量子计算机的计算时间。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Recent work has proposed and explored using coreset techniques for quantum algorithms that operate on classical data sets to accelerate the applicability of these algorithms on near-term quantum devices. We apply these ideas to Quantum Boltzmann Machines (QBM) where gradient-based steps which require Gibbs state sampling are the main computational bottleneck during training. By using a coreset in place of the full data set, we try to minimize the number of steps needed and accelerate the overall training time. In a regime where computational time on quantum computers is a precious resource, we propose this might lead to substantial practical savings. We evaluate this approach on 6x6 binary images from an augmented bars and stripes data set using a QBM with 36 visible units and 8 hidden units. Using an Inception score inspired metric, we compare QBM training times with and without using coresets.
</details>
<details>
<summary>摘要</summary>
近期的工作已经提出了和探索了使用核Set技术来加速近期quantum设备上的量子算法运行，以便在这些算法的应用中减少计算时间。我们在Quantum Boltzmann Machines（QBM）中应用这些想法，其中梯度based步骤的主要计算瓶颈在训练中是Gibbs状态抽样。通过使用核Set而不是全量数据集，我们尝试将训练时间缩短。在计算时间在量子计算机上是珍贵资源的情况下，我们建议这可能导致实质性的实用性级别。我们使用6x6的二进制图像从一个扩展的棒和条纹数据集，使用一个QBM的可见单元为36个，隐藏单元为8个进行训练。使用基于Inception metric的评价标准，我们比较了在使用核Set和没有使用核Set的情况下QBM训练时间。
</details></li>
</ul>
<hr>
<h2 id="Predictive-Maintenance-of-Armoured-Vehicles-using-Machine-Learning-Approaches"><a href="#Predictive-Maintenance-of-Armoured-Vehicles-using-Machine-Learning-Approaches" class="headerlink" title="Predictive Maintenance of Armoured Vehicles using Machine Learning Approaches"></a>Predictive Maintenance of Armoured Vehicles using Machine Learning Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14453">http://arxiv.org/abs/2307.14453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prajit Sengupta, Anant Mehta, Prashant Singh Rana</li>
<li>for: 这个研究旨在提出一种基于预测维护的 ensemble 系统，用于预测 armoured 车的维护需求，以提高车辆的运作效率和可靠性。</li>
<li>methods: 该模型采用了多种模型，如光树落准、Random Forest、决策树、Extra Tree Classifier 和 Gradient Boosting，以准确预测车辆的维护需求。 plus 使用了 K-fold 十字验证和 TOPSIS 分析来评估模型的稳定性。</li>
<li>results: 结果显示，提议的系统具有98.93%的准确率、99.80%的精度和99.03%的回归率，能够有效预测车辆的维护需求，从而降低车辆的停机时间和提高运作效率。<details>
<summary>Abstract</summary>
Armoured vehicles are specialized and complex pieces of machinery designed to operate in high-stress environments, often in combat or tactical situations. This study proposes a predictive maintenance-based ensemble system that aids in predicting potential maintenance needs based on sensor data collected from these vehicles. The proposed model's architecture involves various models such as Light Gradient Boosting, Random Forest, Decision Tree, Extra Tree Classifier and Gradient Boosting to predict the maintenance requirements of the vehicles accurately. In addition, K-fold cross validation, along with TOPSIS analysis, is employed to evaluate the proposed ensemble model's stability. The results indicate that the proposed system achieves an accuracy of 98.93%, precision of 99.80% and recall of 99.03%. The algorithm can effectively predict maintenance needs, thereby reducing vehicle downtime and improving operational efficiency. Through comparisons between various algorithms and the suggested ensemble, this study highlights the potential of machine learning-based predictive maintenance solutions.
</details>
<details>
<summary>摘要</summary>
armored vehicles 是特殊化和复杂的机器设备，设计用于高压环境下运行，经常在战斗或战术情况下使用。本研究提出了一种基于预测维护的ensemble系统，用于预测这些车辆的维护需求。提议的模型体系包括了各种模型，如光梯度抛光、随机森林、决策树、附加树分类器和梯度抛光。此外， employ K-fold Cross Validation 和 TOPSIS分析来评估提议的ensemble模型的稳定性。结果表明，提议的系统实现了98.93%的准确率、99.80%的精度和99.03%的回归率。这个算法可以有效预测维护需求，从而减少车辆的停机时间，提高运作效率。通过对不同算法和建议的ensemble进行比较，本研究强调了机器学习基于预测维护解决方案的潜力。
</details></li>
</ul>
<hr>
<h2 id="VISPUR-Visual-Aids-for-Identifying-and-Interpreting-Spurious-Associations-in-Data-Driven-Decisions"><a href="#VISPUR-Visual-Aids-for-Identifying-and-Interpreting-Spurious-Associations-in-Data-Driven-Decisions" class="headerlink" title="VISPUR: Visual Aids for Identifying and Interpreting Spurious Associations in Data-Driven Decisions"></a>VISPUR: Visual Aids for Identifying and Interpreting Spurious Associations in Data-Driven Decisions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14448">http://arxiv.org/abs/2307.14448</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/picsolab/vispur">https://github.com/picsolab/vispur</a></li>
<li>paper_authors: Xian Teng, Yongsu Ahn, Yu-Ru Lin</li>
<li>for: 该论文旨在帮助人们在受到大数据和机器学习工具支持的情况下，更好地识别和理解干扰因素导致的假关系。</li>
<li>methods: 该论文提出了一种可见分析框架和一个人类中心的工作流程，用于检测和解决干扰因素导致的假关系。其中包括一个干扰因素仪表板，可以自动标识可能的干扰因素，以及一个 subgroup 视图器，可以视觉化和比较不同 subgroup 的各种模式，以避免因果混乱的误解。</li>
<li>results: 我们通过专家采访和控制的用户实验，证明了我们提出的 “de-paradox” 工作流程和设计的可见分析系统是有效的，帮助人们更好地识别和理解干扰因素导致的假关系，以及做出可识别的决策。<details>
<summary>Abstract</summary>
Big data and machine learning tools have jointly empowered humans in making data-driven decisions. However, many of them capture empirical associations that might be spurious due to confounding factors and subgroup heterogeneity. The famous Simpson's paradox is such a phenomenon where aggregated and subgroup-level associations contradict with each other, causing cognitive confusions and difficulty in making adequate interpretations and decisions. Existing tools provide little insights for humans to locate, reason about, and prevent pitfalls of spurious association in practice. We propose VISPUR, a visual analytic system that provides a causal analysis framework and a human-centric workflow for tackling spurious associations. These include a CONFOUNDER DASHBOARD, which can automatically identify possible confounding factors, and a SUBGROUP VIEWER, which allows for the visualization and comparison of diverse subgroup patterns that likely or potentially result in a misinterpretation of causality. Additionally, we propose a REASONING STORYBOARD, which uses a flow-based approach to illustrate paradoxical phenomena, as well as an interactive DECISION DIAGNOSIS panel that helps ensure accountable decision-making. Through an expert interview and a controlled user experiment, our qualitative and quantitative results demonstrate that the proposed "de-paradox" workflow and the designed visual analytic system are effective in helping human users to identify and understand spurious associations, as well as to make accountable causal decisions.
</details>
<details>
<summary>摘要</summary>
大数据和机器学习工具已经共同强化了人类在基于数据的决策中的能力。然而，许多其中捕捉到了偶合关系，这些关系可能因为干扰因素和 subgroup 多样性而是假的。例如，西мп逊的 парадокс是这种现象，其中总体和 subgroup 级别的关系相互矛盾，导致认知混乱和不能正确地解释和决策。现有工具提供的知识对 humans 来说太少，以至于无法在实践中找到、理解和避免偶合关系的坑。我们提出了 VISPUR，一个视觉分析系统，它提供了一种 causal 分析框架和一个人类中心的工作流程，用于解决偶合关系。这些包括一个 CONFOUNDER DASHBOARD，可以自动Identify possible confounding factors，以及一个 SUBGROUP VIEWER，可以将多个 subgroup 的各种模式视觉化和比较，以便更好地理解和分析 causality。此外，我们还提出了一个 REASONING STORYBOARD，使用流程方式来描述悖论现象，以及一个交互式的 DECISION DIAGNOSIS 面板，帮助确保决策是合理的。经过专家采访和控制的用户试验，我们的质量和量度结果表明，我们提出的 "de-paradox" 工作流程和设计的视觉分析系统都是有效的，帮助人类用户更好地找到、理解和解决偶合关系，以及做出负责任的 causal 决策。
</details></li>
</ul>
<hr>
<h2 id="Neural-Schrodinger-Bridge-with-Sinkhorn-Losses-Application-to-Data-driven-Minimum-Effort-Control-of-Colloidal-Self-assembly"><a href="#Neural-Schrodinger-Bridge-with-Sinkhorn-Losses-Application-to-Data-driven-Minimum-Effort-Control-of-Colloidal-Self-assembly" class="headerlink" title="Neural Schrödinger Bridge with Sinkhorn Losses: Application to Data-driven Minimum Effort Control of Colloidal Self-assembly"></a>Neural Schrödinger Bridge with Sinkhorn Losses: Application to Data-driven Minimum Effort Control of Colloidal Self-assembly</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14442">http://arxiv.org/abs/2307.14442</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iman Nodozi, Charlie Yan, Mira Khare, Abhishek Halder, Ali Mesbah</li>
<li>for: 这个论文是为了研究溶解自组装的最小努力控制问题而写的。</li>
<li>methods: 这篇论文使用了一种名为“神经学桥”的数据驱动学习和控制框架，以解决一类通过扩展Schrödinger桥问题来处理溶解自组装控制问题。</li>
<li>results: 研究人员通过使用分子动力学模拟数据和神经网络来学习控制拖动和扩散系数，并使用这些系数来训练一个特定于这类控制问题的神经网络，以解决溶解自组装控制问题。<details>
<summary>Abstract</summary>
We show that the minimum effort control of colloidal self-assembly can be naturally formulated in the order-parameter space as a generalized Schr\"odinger bridge problem -- a class of fixed-horizon stochastic optimal control problems that originated in the works of Erwin Schr\"odinger in the early 1930s. In recent years, this class of problems has seen a resurgence of research activities in control and machine learning communities. Different from the existing literature on the theory and computation for such problems, the controlled drift and diffusion coefficients for colloidal self-assembly are typically non-affine in control, and are difficult to obtain from physics-based modeling. We deduce the conditions of optimality for such generalized problems, and show that the resulting system of equations is structurally very different from the existing results in a way that standard computational approaches no longer apply. Thus motivated, we propose a data-driven learning and control framework, named `neural Schr\"odinger bridge', to solve such generalized Schr\"odinger bridge problems by innovating on recent advances in neural networks. We illustrate the effectiveness of the proposed framework using a numerical case study of colloidal self-assembly. We learn the controlled drift and diffusion coefficients as two neural networks using molecular dynamics simulation data, and then use these two to train a third network with Sinkhorn losses designed for distributional endpoint constraints, specific for this class of control problems.
</details>
<details>
<summary>摘要</summary>
我们显示，材料自组装的最小努力控制可以自然地表示为一种通用的Schrödinger桥问题，这是Erwin Schrödinger在30年代初期提出的一类固定时间 horizon随机控制问题。在最近几年，这类问题在控制和机器学习领域中得到了新的研究活动。与现有Literature不同，控制拖拽和扩散系数对材料自组装通常是非线性的，从物理模型中难以获得。我们推导了这类总体最优条件，并发现其系统方程与现有结果 Structurally very different，使得标准计算方法不再适用。因此，我们提出了一种基于数据驱动学习和控制的框架，名为“神经Schrödinger桥”，用于解决这类通用Schrödinger桥问题。我们通过一个数字 caso study of colloidal self-assembly illustrate the effectiveness of the proposed framework，我们通过分子动力学模拟数据学习控制拖拽和扩散系数为两个神经网络，然后使用这两个网络来训练第三个网络，用Sinkhorn损失函数，特定于这类控制问题的分布端点约束。
</details></li>
</ul>
<hr>
<h2 id="Fixed-Integral-Neural-Networks"><a href="#Fixed-Integral-Neural-Networks" class="headerlink" title="Fixed Integral Neural Networks"></a>Fixed Integral Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14439">http://arxiv.org/abs/2307.14439</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryan Kortvelesy</li>
<li>for: 本研究旨在解决通常通过数值方法进行的神经网络中函数 интеграル计算的问题，提供一种能够确定神经网络函数 интеграル的方法。</li>
<li>methods: 本研究使用了一种基于约束的神经网络模型，其中约束直接应用于函数 интеграル中。此外，研究还提出了一种方法来保证函数 интеграル为正，这是许多应用中必需的条件。</li>
<li>results: 研究得到了一种能够确定神经网络函数 интеграル的方法，并且可以应用于许多应用中，如概率分布、距离度量等。此外，研究还发现了一些约束神经网络的特性和应用场景。<details>
<summary>Abstract</summary>
It is often useful to perform integration over learned functions represented by neural networks. However, this integration is usually performed numerically, as analytical integration over learned functions (especially neural networks) is generally viewed as intractable. In this work, we present a method for representing the analytical integral of a learned function $f$. This allows the exact integral of a neural network to be computed, and enables constrained neural networks to be parametrised by applying constraints directly to the integral. Crucially, we also introduce a method to constrain $f$ to be positive, a necessary condition for many applications (e.g. probability distributions, distance metrics, etc). Finally, we introduce several applications where our fixed-integral neural network (FINN) can be utilised.
</details>
<details>
<summary>摘要</summary>
通常情况下，使用神经网络学习的函数 интеграル是一个有用的技术。然而，通常是通过数值方法来实现这种 интеграル计算，因为对神经网络学习函数的分析性 интеграル是一个通常被视为无法计算的问题。在这项工作中，我们提出了一种方法来计算神经网络学习函数 $f$ 的分析性 интеграル。这使得神经网络的精确 интеграル可以被计算出来，并且可以通过直接应用约束来 parametrise 受限神经网络。其中，我们还提出了一种方法来约束 $f$ 为正，这是许多应用中的必要条件（例如概率分布、距离度量等）。最后，我们介绍了一些应用场景，where our fixed-integral neural network (FINN) can be used.
</details></li>
</ul>
<hr>
<h2 id="Skill-it-A-Data-Driven-Skills-Framework-for-Understanding-and-Training-Language-Models"><a href="#Skill-it-A-Data-Driven-Skills-Framework-for-Understanding-and-Training-Language-Models" class="headerlink" title="Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models"></a>Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14430">http://arxiv.org/abs/2307.14430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mayee F. Chen, Nicholas Roberts, Kush Bhatia, Jue Wang, Ce Zhang, Frederic Sala, Christopher Ré</li>
<li>for: 这个论文的目的是研究如何使用有限Token数据进行语言模型的训练，以提高其下游任务的性能。</li>
<li>methods: 该论文提出了一个新的框架，基于人类学习的自然顺序来理解语言模型如何学习从其训练数据中的技能。这个框架使用了一种新的在线数据采样算法，它可以在不同任务之间进行循环训练，从而提高模型的性能。</li>
<li>results: 通过使用这个框架和采样算法， authors 在 Synthetic LEGO 数据集和 Natural Instructions 数据集上进行了实验，并证明了这种方法可以提高模型的性能。在 continual pre-training  Setting中，Skill-It 算法在 LEGO 数据集上取得了36.5个点更高的准确率，而在 fine-tuning  Setting中，它在 target 技能上降低了13.6%的验证损失。此外， authors 还在 RedPajama 数据集上使用了这种方法，并在 1B 个Token数据上训练了一个 3B 参数的语言模型，并取得了高于基eline方法的准确率。<details>
<summary>Abstract</summary>
The quality of training data impacts the performance of pre-trained large language models (LMs). Given a fixed budget of tokens, we study how to best select data that leads to good downstream model performance across tasks. We develop a new framework based on a simple hypothesis: just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data. If such an order exists, it can be utilized for improved understanding of LMs and for data-efficient training. Using this intuition, our framework formalizes the notion of a skill and of an ordered set of skills in terms of the associated data. First, using both synthetic and real data, we demonstrate that these ordered skill sets exist, and that their existence enables more advanced skills to be learned with less data when we train on their prerequisite skills. Second, using our proposed framework, we introduce an online data sampling algorithm, Skill-It, over mixtures of skills for both continual pre-training and fine-tuning regimes, where the objective is to efficiently learn multiple skills in the former and an individual skill in the latter. On the LEGO synthetic in the continual pre-training setting, Skill-It obtains 36.5 points higher accuracy than random sampling. On the Natural Instructions dataset in the fine-tuning setting, Skill-It reduces the validation loss on the target skill by 13.6% versus training on data associated with the target skill itself. We apply our skills framework on the recent RedPajama dataset to continually pre-train a 3B-parameter LM, achieving higher accuracy on the LM Evaluation Harness with 1B tokens than the baseline approach of sampling uniformly over data sources with 3B tokens.
</details>
<details>
<summary>摘要</summary>
“训练数据质量对预训练大语言模型（LM）的性能产生很大影响。我们研究如何在固定的字符数限制下选择数据，以确保在任务之间具有良好的下游模型性能。我们提出了一新的框架，基于一个简单的假设：人类在意图的顺序中学习了一系列技能，语言模型也会在它们的训练数据中学习一个自然的顺序。如果这种顺序存在，那么可以用于更好地理解LMs，以及进行数据效率的训练。使用这种假设，我们的框架将技能和相关数据进行了正式的定义。我们首先使用 sintetic 和实际数据示出，这些顺序技能集存在，并且它们的存在可以使更高级别的技能在更少的数据上学习。其次，我们根据我们提出的框架，引入了一种在混合技能上进行线上数据采样的算法，叫Skill-It。在不断预训练和精度训练的情况下，Skill-It 可以更高效地学习多种技能。在 LEGO  sintetic 上，Skill-It 在不断预训练情况下取得了36.5个点更高的准确率。在 Natural Instructions 数据集上，Skill-It 在精度训练情况下降低了目标技能的验证损失13.6%。我们在最近的 RedPajama 数据集上应用了我们的技能框架，在3B-参数 LM 中进行不断预训练，以达到LM Evaluation Harness 的更高准确率，比基eline方法（随机采样数据源）的3B个字符Token。”
</details></li>
</ul>
<hr>
<h2 id="TabR-Unlocking-the-Power-of-Retrieval-Augmented-Tabular-Deep-Learning"><a href="#TabR-Unlocking-the-Power-of-Retrieval-Augmented-Tabular-Deep-Learning" class="headerlink" title="TabR: Unlocking the Power of Retrieval-Augmented Tabular Deep Learning"></a>TabR: Unlocking the Power of Retrieval-Augmented Tabular Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14338">http://arxiv.org/abs/2307.14338</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yandex-research/tabular-dl-tabr">https://github.com/yandex-research/tabular-dl-tabr</a></li>
<li>paper_authors: Yury Gorishniy, Ivan Rubachev, Nikolay Kartashev, Daniil Shlenskii, Akim Kotelnikov, Artem Babenko</li>
<li>for: 该研究是为了解决表格数据问题上的深度学习（DL）模型是否值得投入研究的问题。</li>
<li>methods: 该研究使用了一种基于注意力的搜索组件，将简单的扩展报文Architecture与 Retrieval-based 模型结合在一起，并对注意力机制的几个细节进行了优化。</li>
<li>results: 该研究在一些公共的benchmark上达到了最佳平均性能，超过了其他表格DL模型，并在“GBDT友好”的benchmark上超越了GBDT模型。<details>
<summary>Abstract</summary>
Deep learning (DL) models for tabular data problems are receiving increasingly more attention, while the algorithms based on gradient-boosted decision trees (GBDT) remain a strong go-to solution. Following the recent trends in other domains, such as natural language processing and computer vision, several retrieval-augmented tabular DL models have been recently proposed. For a given target object, a retrieval-based model retrieves other relevant objects, such as the nearest neighbors, from the available (training) data and uses their features or even labels to make a better prediction. However, we show that the existing retrieval-based tabular DL solutions provide only minor, if any, benefits over the properly tuned simple retrieval-free baselines. Thus, it remains unclear whether the retrieval-based approach is a worthy direction for tabular DL.   In this work, we give a strong positive answer to this question. We start by incrementally augmenting a simple feed-forward architecture with an attention-like retrieval component similar to those of many (tabular) retrieval-based models. Then, we highlight several details of the attention mechanism that turn out to have a massive impact on the performance on tabular data problems, but that were not explored in prior work. As a result, we design TabR -- a simple retrieval-based tabular DL model which, on a set of public benchmarks, demonstrates the best average performance among tabular DL models, becomes the new state-of-the-art on several datasets, and even outperforms GBDT models on the recently proposed ``GBDT-friendly'' benchmark (see the first figure).
</details>
<details>
<summary>摘要</summary>
深度学习（DL）模型在表格数据问题上 receiving increasingly more attention，而基于梯度抛物线（GBDT）算法的模型仍然是强大的首选解决方案。随着其他领域，如自然语言处理和计算机视觉，多种基于检索的表格DL模型已经被提出。为给定目标对象，一个基于检索的模型将其他相关对象（如最近的邻居）从可用的数据集中检索出来，并使用其特征或甚至标签来进行更好的预测。然而，我们显示出现有的检索基于DL解决方案只提供了微不足的，如果有的，利益于简单的无检索基线。因此，是否使用检索方法是值得追究的问题。在这项工作中，我们给出了一个积极的答案。我们首先将简单的扩散架构逐步增强为包含拟合 Retrieval 的注意力机制，类似于许多（表格）检索基于DL模型。然后，我们强调了一些关于注意力机制的细节，其中一些在对表格数据问题进行表达的时候有很大的影响，但在前期工作中未经探讨。因此，我们设计了 TabR -- 一个简单的检索基于DL模型，在一些公共的 benchmark 上表现出了最好的平均性能，创下了新的状态码，并在一些数据集上 even outperform GBDT 模型（参见第一个图像）。
</details></li>
</ul>
<hr>
<h2 id="Waypoint-Based-Imitation-Learning-for-Robotic-Manipulation"><a href="#Waypoint-Based-Imitation-Learning-for-Robotic-Manipulation" class="headerlink" title="Waypoint-Based Imitation Learning for Robotic Manipulation"></a>Waypoint-Based Imitation Learning for Robotic Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14326">http://arxiv.org/abs/2307.14326</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lucys0/awe">https://github.com/lucys0/awe</a></li>
<li>paper_authors: Lucy Xiaoyang Shi, Archit Sharma, Tony Z. Zhao, Chelsea Finn</li>
<li>for: 该论文旨在提出一种自动生成方向点的方法，以便在人工学习中减少错误的汇总。</li>
<li>methods: 该论文提出了一种自动方向点提取（AWE）模块，可以将示例分解为最小的方向点集，以便在指定的误差阈值下 linearly  interpolate approximate  trajectory。</li>
<li>results: 实验和实际应用中，AWE 可以增加状态艺术 algorithm 的成功率，提高了实验和实际应用中的成功率，并且可以减少决策准点数量。<details>
<summary>Abstract</summary>
While imitation learning methods have seen a resurgent interest for robotic manipulation, the well-known problem of compounding errors continues to afflict behavioral cloning (BC). Waypoints can help address this problem by reducing the horizon of the learning problem for BC, and thus, the errors compounded over time. However, waypoint labeling is underspecified, and requires additional human supervision. Can we generate waypoints automatically without any additional human supervision? Our key insight is that if a trajectory segment can be approximated by linear motion, the endpoints can be used as waypoints. We propose Automatic Waypoint Extraction (AWE) for imitation learning, a preprocessing module to decompose a demonstration into a minimal set of waypoints which when interpolated linearly can approximate the trajectory up to a specified error threshold. AWE can be combined with any BC algorithm, and we find that AWE can increase the success rate of state-of-the-art algorithms by up to 25% in simulation and by 4-28% on real-world bimanual manipulation tasks, reducing the decision making horizon by up to a factor of 10. Videos and code are available at https://lucys0.github.io/awe/
</details>
<details>
<summary>摘要</summary>
While imitation learning methods have seen a resurgence of interest for robotic manipulation, the well-known problem of compounding errors continues to affect behavioral cloning (BC). Waypoints can help address this problem by reducing the horizon of the learning problem for BC, but waypoint labeling is underspecified and requires additional human supervision. Can we generate waypoints automatically without any additional human supervision? Our key insight is that if a trajectory segment can be approximated by linear motion, the endpoints can be used as waypoints. We propose Automatic Waypoint Extraction (AWE) for imitation learning, a preprocessing module to decompose a demonstration into a minimal set of waypoints that can be interpolated linearly to approximate the trajectory up to a specified error threshold. AWE can be combined with any BC algorithm, and we find that AWE can increase the success rate of state-of-the-art algorithms by up to 25% in simulation and by 4-28% on real-world bimanual manipulation tasks, reducing the decision-making horizon by up to a factor of 10. Videos and code are available at https://lucys0.github.io/awe/.Here's the translation in Traditional Chinese:随着从imitative learning方法的复兴， robotic manipulation 中的well-known problem of compounding errors仍然存在。 Waypoints可以帮助解决这个问题，但是 waypoint labeling 是不够详细的，需要额外的人工supervision。 Can we generate waypoints automatically without any additional human supervision? Our key insight is that if a trajectory segment can be approximated by linear motion, the endpoints can be used as waypoints. We propose Automatic Waypoint Extraction (AWE) for imitation learning, a preprocessing module to decompose a demonstration into a minimal set of waypoints that can be interpolated linearly to approximate the trajectory up to a specified error threshold. AWE can be combined with any BC algorithm, and we find that AWE can increase the success rate of state-of-the-art algorithms by up to 25% in simulation and by 4-28% on real-world bimanual manipulation tasks, reducing the decision-making horizon by up to a factor of 10. Videos and code are available at https://lucys0.github.io/awe/.
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-Moral-Beliefs-Encoded-in-LLMs"><a href="#Evaluating-the-Moral-Beliefs-Encoded-in-LLMs" class="headerlink" title="Evaluating the Moral Beliefs Encoded in LLMs"></a>Evaluating the Moral Beliefs Encoded in LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14324">http://arxiv.org/abs/2307.14324</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ninodimontalcino/moralchoice">https://github.com/ninodimontalcino/moralchoice</a></li>
<li>paper_authors: Nino Scherrer, Claudia Shi, Amir Feder, David M. Blei</li>
<li>for: 这个研究探讨了大型自然语言模型（LLM）上的问naire设计、管理、后期处理和评估方法。</li>
<li>methods: 这个研究使用了一种统计方法来揭示LLM中的信仰。研究者们引入了一些统计量和评价指标，以量化LLM“选择”的概率、相关的uncertainty以及选择的一致性。</li>
<li>results: 研究发现，在明确的场景下，大多数模型选择与常识相符的行为。在抽象的场景下，大多数模型表现出uncertainty。此外，一些模型在抽象场景下表现出明确的偏好，特别是关闭源代码模型之间存在一定的一致性。<details>
<summary>Abstract</summary>
This paper presents a case study on the design, administration, post-processing, and evaluation of surveys on large language models (LLMs). It comprises two components: (1) A statistical method for eliciting beliefs encoded in LLMs. We introduce statistical measures and evaluation metrics that quantify the probability of an LLM "making a choice", the associated uncertainty, and the consistency of that choice. (2) We apply this method to study what moral beliefs are encoded in different LLMs, especially in ambiguous cases where the right choice is not obvious. We design a large-scale survey comprising 680 high-ambiguity moral scenarios (e.g., "Should I tell a white lie?") and 687 low-ambiguity moral scenarios (e.g., "Should I stop for a pedestrian on the road?"). Each scenario includes a description, two possible actions, and auxiliary labels indicating violated rules (e.g., "do not kill"). We administer the survey to 28 open- and closed-source LLMs. We find that (a) in unambiguous scenarios, most models "choose" actions that align with commonsense. In ambiguous cases, most models express uncertainty. (b) Some models are uncertain about choosing the commonsense action because their responses are sensitive to the question-wording. (c) Some models reflect clear preferences in ambiguous scenarios. Specifically, closed-source models tend to agree with each other.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>A statistical method for eliciting beliefs encoded in LLMs. We introduce statistical measures and evaluation metrics that quantify the probability of an LLM “making a choice,” the associated uncertainty, and the consistency of that choice.2. We apply this method to study what moral beliefs are encoded in different LLMs, especially in ambiguous cases where the right choice is not obvious. We design a large-scale survey with 680 high-ambiguity moral scenarios (e.g., “Should I tell a white lie?”) and 687 low-ambiguity moral scenarios (e.g., “Should I stop for a pedestrian on the road?”). Each scenario includes a description, two possible actions, and auxiliary labels indicating violated rules (e.g., “do not kill”). We administer the survey to 28 open- and closed-source LLMs.Our findings are as follows:1. In unambiguous scenarios, most models “choose” actions that align with commonsense.2. In ambiguous cases, most models express uncertainty.3. Some models are uncertain about choosing the commonsense action because their responses are sensitive to the question-wording.4. Some models reflect clear preferences in ambiguous scenarios, and closed-source models tend to agree with each other.</details></li>
</ol>
<hr>
<h2 id="Reinforcement-Learning-by-Guided-Safe-Exploration"><a href="#Reinforcement-Learning-by-Guided-Safe-Exploration" class="headerlink" title="Reinforcement Learning by Guided Safe Exploration"></a>Reinforcement Learning by Guided Safe Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14316">http://arxiv.org/abs/2307.14316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qisong Yang, Thiago D. Simão, Nils Jansen, Simon H. Tindemans, Matthijs T. J. Spaan</li>
<li>for: 这篇论文的目的是为了帮助RL算法在不知道目标任务的情况下安全地扩展应用。</li>
<li>methods: 这篇论文使用了不受奖励指导的RL方法，在一个控制的环境中训练一个引导者，以便在不知道目标任务时快速适应。在目标任务被揭示后，不允许安全违反。此外，该方法还利用了传输学习来正则化一个目标策略（学生），使其快速解决目标任务。</li>
<li>results: 实验表明，这种方法可以实现安全的传输学习，帮助学生更快地解决目标任务。<details>
<summary>Abstract</summary>
Safety is critical to broadening the application of reinforcement learning (RL). Often, we train RL agents in a controlled environment, such as a laboratory, before deploying them in the real world. However, the real-world target task might be unknown prior to deployment. Reward-free RL trains an agent without the reward to adapt quickly once the reward is revealed. We consider the constrained reward-free setting, where an agent (the guide) learns to explore safely without the reward signal. This agent is trained in a controlled environment, which allows unsafe interactions and still provides the safety signal. After the target task is revealed, safety violations are not allowed anymore. Thus, the guide is leveraged to compose a safe behaviour policy. Drawing from transfer learning, we also regularize a target policy (the student) towards the guide while the student is unreliable and gradually eliminate the influence of the guide as training progresses. The empirical analysis shows that this method can achieve safe transfer learning and helps the student solve the target task faster.
</details>
<details>
<summary>摘要</summary>
安全性是扩展强化学习（RL）的关键因素。我们通常在实验室中首先训练RL代理人，然后在真实世界中部署。然而，真实世界目标任务可能未知之前部署。无奖RL在扩展RL代理人时不提供奖励。我们考虑了受限的奖励自由设置，其中RL代理人（导师）在安全的环境中学习探索，不需要奖励信号。当目标任务揭示后，安全违反不再允许。因此，导师被用来组织安全行为策略。从传输学学习中，我们还启用了一个目标策略（学生），使其向导师进行Regularization，以解决学生在训练过程中的不可靠性。实验分析表明，这种方法可以实现安全的传输学习，帮助学生更快地解决目标任务。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Deep-Learning-based-Pansharpening-with-Jointly-Enhanced-Spectral-and-Spatial-Fidelity"><a href="#Unsupervised-Deep-Learning-based-Pansharpening-with-Jointly-Enhanced-Spectral-and-Spatial-Fidelity" class="headerlink" title="Unsupervised Deep Learning-based Pansharpening with Jointly-Enhanced Spectral and Spatial Fidelity"></a>Unsupervised Deep Learning-based Pansharpening with Jointly-Enhanced Spectral and Spatial Fidelity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14403">http://arxiv.org/abs/2307.14403</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/matciotola/lambda-pnn">https://github.com/matciotola/lambda-pnn</a></li>
<li>paper_authors: Matteo Ciotola, Giovanni Poggi, Giuseppe Scarpa<br>for: 这个论文主要关注于深度学习在多resolution图像缩进中的应用，具体来说是提出一种可以在全分辨率域进行无监督训练的整体框架，以提高图像缩进的性能。methods: 该论文提出了一种新的深度学习基于模型，具有增强的建筑特性和一种新的损失函数，该损失函数同时 promote spectral和spatial图像质量。此外，该模型还使用了一种新的微调策略来提高推理时的适应性。results: 实验表明，提出的方法在具有各种挑战性的测试图像上达到了STATE OF THE ART的性能水平，both in terms of numerical results and visual output。<details>
<summary>Abstract</summary>
In latest years, deep learning has gained a leading role in the pansharpening of multiresolution images. Given the lack of ground truth data, most deep learning-based methods carry out supervised training in a reduced-resolution domain. However, models trained on downsized images tend to perform poorly on high-resolution target images. For this reason, several research groups are now turning to unsupervised training in the full-resolution domain, through the definition of appropriate loss functions and training paradigms. In this context, we have recently proposed a full-resolution training framework which can be applied to many existing architectures.   Here, we propose a new deep learning-based pansharpening model that fully exploits the potential of this approach and provides cutting-edge performance. Besides architectural improvements with respect to previous work, such as the use of residual attention modules, the proposed model features a novel loss function that jointly promotes the spectral and spatial quality of the pansharpened data. In addition, thanks to a new fine-tuning strategy, it improves inference-time adaptation to target images. Experiments on a large variety of test images, performed in challenging scenarios, demonstrate that the proposed method compares favorably with the state of the art both in terms of numerical results and visual output. Code is available online at https://github.com/matciotola/Lambda-PNN.
</details>
<details>
<summary>摘要</summary>
最近几年，深度学习在多尺度图像缩进中扮演了主导角色。由于缺乏地面真实数据，大多数深度学习基于方法在减小分辨率领域进行了supervised训练。然而，在高分辨率目标图像上训练的模型通常表现不佳。为了解决这个问题，许多研究小组现在转向无监督训练在全分辨率领域，通过定义适当的损失函数和训练方法。在这种情况下，我们最近提出了一个全分辨率训练框架，可以应用于许多现有的架构。  我们提出了一个新的深度学习基于缩进模型，该模型完全利用了这种方法的潜力，并提供了顶尖性能。除了以前的建筑改进外，该模型还包括尚未使用的差分注意模块，以及一个新的损失函数，该函数同时Promote spectral和空间数据的质量。此外，通过一种新的精度调整策略，该模型提高了对target图像的推理时适应性。在一个大量的测试图像上，通过在复杂的场景下进行测试，我们的方法与状态对照数据比较，得到了优秀的数值结果和视觉输出。代码可以在https://github.com/matciotola/Lambda-PNN上下载。
</details></li>
</ul>
<hr>
<h2 id="A-Constraint-Enforcement-Deep-Reinforcement-Learning-Framework-for-Optimal-Energy-Storage-Systems-Dispatch"><a href="#A-Constraint-Enforcement-Deep-Reinforcement-Learning-Framework-for-Optimal-Energy-Storage-Systems-Dispatch" class="headerlink" title="A Constraint Enforcement Deep Reinforcement Learning Framework for Optimal Energy Storage Systems Dispatch"></a>A Constraint Enforcement Deep Reinforcement Learning Framework for Optimal Energy Storage Systems Dispatch</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14304">http://arxiv.org/abs/2307.14304</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ShengrenHou/Energy-management-MIP-Deep-Reinforcement-Learning">https://github.com/ShengrenHou/Energy-management-MIP-Deep-Reinforcement-Learning</a></li>
<li>paper_authors: Shengren Hou, Edgar Mauricio Salazar Duque, Peter Palensky, Pedro P. Vergara</li>
<li>for: 这篇论文的目的是提出一个基于深度学习的能源储存系统优化策略，以应对动态价格、需求耗用和可再生能源生产的不确定性。</li>
<li>methods: 这篇论文使用深度神经网络（DNNs）和深度问题学习（DRL）算法，以学习适应分布网络的数据分布，并将其转换为一个混合整数程式（MIP）形式，以考虑环境的操作限制。</li>
<li>results:  comparison simulations show that the proposed MIP-DRL framework can effectively enforce all constraints while delivering high-quality dispatch decisions, outperforming state-of-the-art DRL algorithms and the optimal solution obtained with a perfect forecast of the stochastic variables.<details>
<summary>Abstract</summary>
The optimal dispatch of energy storage systems (ESSs) presents formidable challenges due to the uncertainty introduced by fluctuations in dynamic prices, demand consumption, and renewable-based energy generation. By exploiting the generalization capabilities of deep neural networks (DNNs), deep reinforcement learning (DRL) algorithms can learn good-quality control models that adaptively respond to distribution networks' stochastic nature. However, current DRL algorithms lack the capabilities to enforce operational constraints strictly, often even providing unfeasible control actions. To address this issue, we propose a DRL framework that effectively handles continuous action spaces while strictly enforcing the environments and action space operational constraints during online operation. Firstly, the proposed framework trains an action-value function modeled using DNNs. Subsequently, this action-value function is formulated as a mixed-integer programming (MIP) formulation enabling the consideration of the environment's operational constraints. Comprehensive numerical simulations show the superior performance of the proposed MIP-DRL framework, effectively enforcing all constraints while delivering high-quality dispatch decisions when compared with state-of-the-art DRL algorithms and the optimal solution obtained with a perfect forecast of the stochastic variables.
</details>
<details>
<summary>摘要</summary>
优化能量存储系统（ESS）的分发表现出了巨大的挑战，这主要归结于能量价格、消耗量和可再生能源生产的波动性带来的不确定性。通过深度神经网络（DNN）的泛化能力，深度强化学习（DRL）算法可以学习适应分布网络的随机性，并且可以适应不同的环境和操作约束。然而，当前的DRL算法通常无法严格地执行环境和操作约束，有时候甚至提供了不可行的控制动作。为解决这个问题，我们提出了一种DRL框架，可以有效地处理连续动作空间，同时严格执行环境和操作约束。首先，我们在DNN中训练一个动作价值函数。然后，我们将这个动作价值函数转换为混合整数编程（MIP）形式，以便考虑环境的操作约束。通过对数字实验进行了全面的比较，我们发现了我们提出的MIP-DRL框架的优秀表现。它可以坚持所有约束，同时提供高质量的分发决策，与当前的DRL算法和完美预测随机变量的优质策略相比。
</details></li>
</ul>
<hr>
<h2 id="ChatGPT-and-Persuasive-Technologies-for-the-Management-and-Delivery-of-Personalized-Recommendations-in-Hotel-Hospitality"><a href="#ChatGPT-and-Persuasive-Technologies-for-the-Management-and-Delivery-of-Personalized-Recommendations-in-Hotel-Hospitality" class="headerlink" title="ChatGPT and Persuasive Technologies for the Management and Delivery of Personalized Recommendations in Hotel Hospitality"></a>ChatGPT and Persuasive Technologies for the Management and Delivery of Personalized Recommendations in Hotel Hospitality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14298">http://arxiv.org/abs/2307.14298</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manolis Remountakis, Konstantinos Kotis, Babis Kourtzis, George E. Tsekouras</li>
<li>for: 这篇论文的目的是探讨推荐系统在酒店互助业中的应用，以及利用语言模型和吸引技术来提高推荐系统的效果。</li>
<li>methods: 这篇论文使用了语言模型ChatGPT和吸引技术来自动化和改进酒店推荐系统。它还分析了用户喜好和在线评论中提取有价值信息，并基于用户 профиль生成个性化推荐。</li>
<li>results: 这篇论文通过一个实验研究了将ChatGPT和吸引技术Integrated into酒店推荐系统的效果，发现这些技术可以提高用户参与度、满意度和酒店收入。<details>
<summary>Abstract</summary>
Recommender systems have become indispensable tools in the hotel hospitality industry, enabling personalized and tailored experiences for guests. Recent advancements in large language models (LLMs), such as ChatGPT, and persuasive technologies, have opened new avenues for enhancing the effectiveness of those systems. This paper explores the potential of integrating ChatGPT and persuasive technologies for automating and improving hotel hospitality recommender systems. First, we delve into the capabilities of ChatGPT, which can understand and generate human-like text, enabling more accurate and context-aware recommendations. We discuss the integration of ChatGPT into recommender systems, highlighting the ability to analyze user preferences, extract valuable insights from online reviews, and generate personalized recommendations based on guest profiles. Second, we investigate the role of persuasive technology in influencing user behavior and enhancing the persuasive impact of hotel recommendations. By incorporating persuasive techniques, such as social proof, scarcity and personalization, recommender systems can effectively influence user decision-making and encourage desired actions, such as booking a specific hotel or upgrading their room. To investigate the efficacy of ChatGPT and persuasive technologies, we present a pilot experi-ment with a case study involving a hotel recommender system. We aim to study the impact of integrating ChatGPT and persua-sive techniques on user engagement, satisfaction, and conversion rates. The preliminary results demonstrate the potential of these technologies in enhancing the overall guest experience and business performance. Overall, this paper contributes to the field of hotel hospitality by exploring the synergistic relationship between LLMs and persuasive technology in recommender systems, ultimately influencing guest satisfaction and hotel revenue.
</details>
<details>
<summary>摘要</summary>
各种推荐系统在酒店互联网行业已成为不可或缺的工具，帮助提供个性化和适应性的旅客体验。最新的大型自然语言模型（LLM），如ChatGPT，以及吸引技术，已经开创了推荐系统的新可能性。本文探讨了将ChatGPT和吸引技术integrated into hotel hospitality recommender systems的可能性，以提高旅客体验和酒店业绩。首先，我们探讨了ChatGPT的能力，可以理解和生成人类语言，从而提供更准确和上下文感知的推荐。我们介绍了将ChatGPT integrate into recommender systems，包括分析用户偏好、从在线评论中提取有价值信息和基于客户 profiling 生成个性化的推荐。其次，我们调查了吸引技术在使用者行为上的影响，以及如何在推荐系统中应用吸引技术以提高旅客决策的可能性。通过涉及到社会证明、缺失和个性化等吸引技术，推荐系统可以有效地影响用户决策和鼓励旅客选择特定酒店或升级房间。为了评估ChatGPT和吸引技术的效果，我们在一个酒店推荐系统的 caso study中进行了试点。我们的目标是研究将ChatGPT和吸引技术integrated into recommender systems的影响在用户参与度、满意度和转化率上。初步结果表明这些技术在总体客户体验和酒店业绩方面具有潜在的潜力。总的来说，本文对酒店互联网领域的推荐系统做出了贡献，探讨了LLMs和吸引技术之间的相互作用，最终影响客户满意度和酒店收益。
</details></li>
</ul>
<hr>
<h2 id="Unraveling-the-Complexity-of-Splitting-Sequential-Data-Tackling-Challenges-in-Video-and-Time-Series-Analysis"><a href="#Unraveling-the-Complexity-of-Splitting-Sequential-Data-Tackling-Challenges-in-Video-and-Time-Series-Analysis" class="headerlink" title="Unraveling the Complexity of Splitting Sequential Data: Tackling Challenges in Video and Time Series Analysis"></a>Unraveling the Complexity of Splitting Sequential Data: Tackling Challenges in Video and Time Series Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14294">http://arxiv.org/abs/2307.14294</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diego Botache, Kristina Dingel, Rico Huhnstock, Arno Ehresmann, Bernhard Sick</li>
<li>for: 本文探讨了分割顺序数据的挑战，包括数据获取、数据表示、分割率选择、设置质量标准和选择适当的选择策略。</li>
<li>methods: 本文使用了两个实际应用例──汽车测试台和流体中的粒子跟踪──来探讨分割顺序数据的挑战。</li>
<li>results: 本文通过两个实际应用例的分析，揭示了分割顺序数据的挑战，并提供了一些可能的解决方案。<details>
<summary>Abstract</summary>
Splitting of sequential data, such as videos and time series, is an essential step in various data analysis tasks, including object tracking and anomaly detection. However, splitting sequential data presents a variety of challenges that can impact the accuracy and reliability of subsequent analyses. This concept article examines the challenges associated with splitting sequential data, including data acquisition, data representation, split ratio selection, setting up quality criteria, and choosing suitable selection strategies. We explore these challenges through two real-world examples: motor test benches and particle tracking in liquids.
</details>
<details>
<summary>摘要</summary>
分割连续数据，如视频和时间序列数据，是数据分析任务中的一个重要步骤，包括对象跟踪和异常检测。然而，分割连续数据会出现多种挑战，这些挑战可能会影响后续分析的准确性和可靠性。本概念文章探讨分割连续数据的挑战，包括数据收集、数据表示、分割率选择、设置质量标准和选择适合的选择策略。我们通过两个实际例子：汽车测试台和液体中粒子跟踪来探讨这些挑战。
</details></li>
</ul>
<hr>
<h2 id="General-Purpose-Artificial-Intelligence-Systems-GPAIS-Properties-Definition-Taxonomy-Open-Challenges-and-Implications"><a href="#General-Purpose-Artificial-Intelligence-Systems-GPAIS-Properties-Definition-Taxonomy-Open-Challenges-and-Implications" class="headerlink" title="General Purpose Artificial Intelligence Systems (GPAIS): Properties, Definition, Taxonomy, Open Challenges and Implications"></a>General Purpose Artificial Intelligence Systems (GPAIS): Properties, Definition, Taxonomy, Open Challenges and Implications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14283">http://arxiv.org/abs/2307.14283</a></li>
<li>repo_url: None</li>
<li>paper_authors: Isaac Triguero, Daniel Molina, Javier Poyatos, Javier Del Ser, Francisco Herrera</li>
<li>for: The paper discusses and proposes a new definition for General-Purpose Artificial Intelligence Systems (GPAIS) and its differentiation based on various factors.</li>
<li>methods: The paper uses existing definitions and proposes a new taxonomy of approaches to realize GPAIS, including the use of AI techniques to improve another AI or foundation models.</li>
<li>results: The paper provides a comprehensive overview of the current state of GPAIS, its challenges and prospects, implications for our society, and the need for responsible and trustworthy AI systems and regulation.Here are the three points in Simplified Chinese text:</li>
<li>for: 这篇论文提出了一个新的 General-Purpose Artificial Intelligence Systems（GPAIS）定义，并对其进行了不同因素的分类。</li>
<li>methods: 该论文使用了现有的定义，并提出了一种新的实现GPAIS的纲要，包括使用AI技术来改进另一个AI或基础模型。</li>
<li>results: 该论文提供了GPAIS的当前状况，其挑战和前景，对我们社会的影响，以及负责任和可信worthy AI系统和regulation的需要。<details>
<summary>Abstract</summary>
Most applications of Artificial Intelligence (AI) are designed for a confined and specific task. However, there are many scenarios that call for a more general AI, capable of solving a wide array of tasks without being specifically designed for them. The term General-Purpose Artificial Intelligence Systems (GPAIS) has been defined to refer to these AI systems. To date, the possibility of an Artificial General Intelligence, powerful enough to perform any intellectual task as if it were human, or even improve it, has remained an aspiration, fiction, and considered a risk for our society. Whilst we might still be far from achieving that, GPAIS is a reality and sitting at the forefront of AI research.   This work discusses existing definitions for GPAIS and proposes a new definition that allows for a gradual differentiation among types of GPAIS according to their properties and limitations. We distinguish between closed-world and open-world GPAIS, characterising their degree of autonomy and ability based on several factors such as adaptation to new tasks, competence in domains not intentionally trained for, ability to learn from few data, or proactive acknowledgment of their own limitations. We then propose a taxonomy of approaches to realise GPAIS, describing research trends such as the use of AI techniques to improve another AI or foundation models. As a prime example, we delve into generative AI, aligning them with the terms and concepts presented in the taxonomy. Through the proposed definition and taxonomy, our aim is to facilitate research collaboration across different areas that are tackling general-purpose tasks, as they share many common aspects. Finally, we discuss the current state of GPAIS, its challenges and prospects, implications for our society, and the need for responsible and trustworthy AI systems and regulation, with the goal of providing a holistic view of GPAIS.
</details>
<details>
<summary>摘要</summary>
大多数人工智能（AI）应用都是为特定任务设计的，但有许多情况需要一种更通用的AI系统，能够解决多种任务而不需要特定设计。这种AI系统被称为通用人工智能系统（GPAIS）。迄今为止，人工通用智能，能够像人类一样完成任何知识任务，或者甚至超越人类，仍然是一个梦想和科幻。尽管我们仍然远离实现这一点，但GPAIS已经成为人工智能研究的前沿。本文讨论了现有的GPAIS定义，并提出了一个新的定义，允许逐步分化GPAIS类型根据其性质和限制。我们将GPAIS分为关闭世界和开放世界两类，根据它们的自主度和能力，包括适应新任务、在不直接训练的领域中的能力、从少量数据学习、或者主动承认自己的限制等因素。然后，我们提出了一种分类方法，描述了在实现GPAIS方面的研究趋势，如使用AI技术提高另一个AI的性能或基础模型。为了更好地推动不同领域之间的合作研究，我们采用了一种概念和术语的分类方法。我们还讨论了GPAIS的当前状况，挑战和前途，以及对社会的影响和责任的人工智能系统和regulation的需要，以提供一个整体的GPAIS视图。
</details></li>
</ul>
<hr>
<h2 id="Deepfake-Image-Generation-for-Improved-Brain-Tumor-Segmentation"><a href="#Deepfake-Image-Generation-for-Improved-Brain-Tumor-Segmentation" class="headerlink" title="Deepfake Image Generation for Improved Brain Tumor Segmentation"></a>Deepfake Image Generation for Improved Brain Tumor Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14273">http://arxiv.org/abs/2307.14273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roa’a Al-Emaryeen, Sara Al-Nahhas, Fatima Himour, Waleed Mahafza, Omar Al-Kadi</li>
<li>For: This paper aims to improve brain tumor segmentation using deep-fake image generation.* Methods: The proposed approach uses a Generative Adversarial Network (GAN) for image-to-image translation and a U-Net-based convolutional neural network (CNN) for image segmentation, trained with deepfake images.* Results: The proposed approach shows improved performance in terms of image segmentation quality metrics compared to ground truth, and has the potential to assist when training with limited data.Here’s the Chinese translation of the three pieces of information:* For: 这篇论文的目的是提高脑肿分割使用深伪图生成。* Methods: 该方法使用了生成对抗网络（GAN）进行图像到图像的转换，然后使用U-Net基于的卷积神经网络（CNN）进行图像分割，并使用深伪图进行训练。* Results: 该方法在图像分割质量指标方面比基准数据显示出了改善的性能，并有可能帮助在具有有限数据的情况下训练。<details>
<summary>Abstract</summary>
As the world progresses in technology and health, awareness of disease by revealing asymptomatic signs improves. It is important to detect and treat tumors in early stage as it can be life-threatening. Computer-aided technologies are used to overcome lingering limitations facing disease diagnosis, while brain tumor segmentation remains a difficult process, especially when multi-modality data is involved. This is mainly attributed to ineffective training due to lack of data and corresponding labelling. This work investigates the feasibility of employing deep-fake image generation for effective brain tumor segmentation. To this end, a Generative Adversarial Network was used for image-to-image translation for increasing dataset size, followed by image segmentation using a U-Net-based convolutional neural network trained with deepfake images. Performance of the proposed approach is compared with ground truth of four publicly available datasets. Results show improved performance in terms of image segmentation quality metrics, and could potentially assist when training with limited data.
</details>
<details>
<summary>摘要</summary>
随着科技和医疗的进步，疾病的早期发现越来越重要。检测和治疗早期癌变可以挽救生命。但是诊断疾病尚存有一些限制，特别是当涉及多Modal数据时。这主要归结于数据和相应的标注不充分。本研究探讨使用深归化图像生成技术来提高脑肿瘤分 segmentation的可能性。为此，我们使用生成对抗网络进行图像到图像翻译，然后使用基于U-Net convolutional neural network的图像分割算法，并使用深归化图像进行训练。我们对四个公共可用的数据集进行比较，结果表明我们的方法可以提高图像分割质量指标，并可能帮助在有限数据情况下训练。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/27/cs.LG_2023_07_27/" data-id="cllsiju1t0023a388fmhs238u" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/28/eess.IV_2023_07_28/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.IV - 2023-07-28 17:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/27/cs.SD_2023_07_27/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-07-27 123:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'', root:''}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
