
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-07-27 123:00:00 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Mitigating Cross-Database Differences for Learning Unified HRTF Representation paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.14547 repo_url: https:&#x2F;&#x2F;github.com&#x2F;yutongwen&#x2F;hrtf_field_norm paper_authors: Yutong W">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-07-27 123:00:00">
<meta property="og:url" content="http://example.com/2023/07/27/cs.SD_2023_07_27/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Mitigating Cross-Database Differences for Learning Unified HRTF Representation paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.14547 repo_url: https:&#x2F;&#x2F;github.com&#x2F;yutongwen&#x2F;hrtf_field_norm paper_authors: Yutong W">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-27T00:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:39.391Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_07_27" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/27/cs.SD_2023_07_27/" class="article-date">
  <time datetime="2023-07-27T00:00:00.000Z" itemprop="datePublished">2023-07-27</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-07-27 123:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Mitigating-Cross-Database-Differences-for-Learning-Unified-HRTF-Representation"><a href="#Mitigating-Cross-Database-Differences-for-Learning-Unified-HRTF-Representation" class="headerlink" title="Mitigating Cross-Database Differences for Learning Unified HRTF Representation"></a>Mitigating Cross-Database Differences for Learning Unified HRTF Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14547">http://arxiv.org/abs/2307.14547</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yutongwen/hrtf_field_norm">https://github.com/yutongwen/hrtf_field_norm</a></li>
<li>paper_authors: Yutong Wen, You Zhang, Zhiyao Duan</li>
<li>for: 这篇论文的目的是提出一种用于实现个人化头部相关转换函数（HRTF）的预测方法，以便在虚拟听觉显示中精确地 пози规音频。</li>
<li>methods: 这篇论文使用机器学习模型来预测个人化HRTF，并使用跨 Databases 的HRTF表现来训练这些模型。</li>
<li>results: 这篇论文的结果显示，透过调整HRTF的频谱响应，可以将不同数据库中的HRTF转换为一个更加共同的表现，并且这些转换后的HRTF无法根据数据库的不同区分。<details>
<summary>Abstract</summary>
Individualized head-related transfer functions (HRTFs) are crucial for accurate sound positioning in virtual auditory displays. As the acoustic measurement of HRTFs is resource-intensive, predicting individualized HRTFs using machine learning models is a promising approach at scale. Training such models require a unified HRTF representation across multiple databases to utilize their respectively limited samples. However, in addition to differences on the spatial sampling locations, recent studies have shown that, even for the common location, HRTFs across databases manifest consistent differences that make it trivial to tell which databases they come from. This poses a significant challenge for learning a unified HRTF representation across databases. In this work, we first identify the possible causes of these cross-database differences, attributing them to variations in the measurement setup. Then, we propose a novel approach to normalize the frequency responses of HRTFs across databases. We show that HRTFs from different databases cannot be classified by their database after normalization. We further show that these normalized HRTFs can be used to learn a more unified HRTF representation across databases than the prior art. We believe that this normalization approach paves the road to many data-intensive tasks on HRTF modeling.
</details>
<details>
<summary>摘要</summary>
个人化的头顶相关转换函数（HRTF）是虚拟听觉显示的精确 зву讯定位的重要因素。由于实际量测HRTF的成本高昂，使用机器学习模型预测个人化HRTF是一个具有潜力的方法。但是，训练这些模型需要一个统一的HRTF表示方式，可以利用不同数据库的有限样本。然而，过去的研究显示，即使在共同的位置上，不同数据库的HRTF之间仍存在明显的差异，这使得很难将HRTF表示统一化。在这个工作中，我们首先识别了差异的可能原因，并将其归因于测量设置的变化。然后，我们提出了一个新的方法来对不同数据库的HRTF进行频谱均衡。我们发现，对不同数据库的HRTF进行均衡后，它们不能被分类到哪怕库。此外，我们还证明了这些均衡后的HRTF可以用来学习一个更统一的HRTF表示方式，比对于先前的方法更好。我们认为，这个均衡方法将开启许多数据密集的HRTF模型任务。
</details></li>
</ul>
<hr>
<h2 id="Modality-Agnostic-Audio-Visual-Deepfake-Detection"><a href="#Modality-Agnostic-Audio-Visual-Deepfake-Detection" class="headerlink" title="Modality-Agnostic Audio-Visual Deepfake Detection"></a>Modality-Agnostic Audio-Visual Deepfake Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14491">http://arxiv.org/abs/2307.14491</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cai Yu, Peng Chen, Jiahe Tian, Jin Liu, Jiao Dai, Xi Wang, Yesheng Chai, Jizhong Han</li>
<li>for: 这个研究旨在开发一个能够探测多模式深圳诈骗的数据类型不对称检测方法，并能够处理缺失模式的情况。</li>
<li>methods: 本研究使用了一个统一的诈骗模式框架，可以探测多模式深圳诈骗并处理缺失模式的情况。另外，我们还提出了一个双标签检测方法，可以独立检测每个模式。</li>
<li>results: 实验结果显示，我们的方法不仅在所有三个音频视觉数据集上都超过了现有的州务检测方法，而且在缺失模式情况下也可以 дости持 satisfying的性能。此外，我们的方法甚至在 JOINT 使用两个单模式方法时超过了它们的性能。<details>
<summary>Abstract</summary>
As AI-generated content (AIGC) thrives, Deepfakes have expanded from single-modality falsification to cross-modal fake content creation, where either audio or visual components can be manipulated. While using two unimodal detectors can detect audio-visual deepfakes, cross-modal forgery clues could be overlooked. Existing multimodal deepfake detection methods typically establish correspondence between the audio and visual modalities for binary real/fake classification, and require the co-occurrence of both modalities. However, in real-world multi-modal applications, missing modality scenarios may occur where either modality is unavailable. In such cases, audio-visual detection methods are less practical than two independent unimodal methods. Consequently, the detector can not always obtain the number or type of manipulated modalities beforehand, necessitating a fake-modality-agnostic audio-visual detector. In this work, we propose a unified fake-modality-agnostic scenarios framework that enables the detection of multimodal deepfakes and handles missing modalities cases, no matter the manipulation hidden in audio, video, or even cross-modal forms. To enhance the modeling of cross-modal forgery clues, we choose audio-visual speech recognition (AVSR) as a preceding task, which effectively extracts speech correlation across modalities, which is difficult for deepfakes to reproduce. Additionally, we propose a dual-label detection approach that follows the structure of AVSR to support the independent detection of each modality. Extensive experiments show that our scheme not only outperforms other state-of-the-art binary detection methods across all three audio-visual datasets but also achieves satisfying performance on detection modality-agnostic audio/video fakes. Moreover, it even surpasses the joint use of two unimodal methods in the presence of missing modality cases.
</details>
<details>
<summary>摘要</summary>
“深刻掌握AI生成内容（AIGC）的发展，深伪（Deepfakes）已经从单模态伪造扩展到跨模态伪造，其中可以操作音频或视觉组件。使用两个单模态检测器可以检测音频视频深伪，但跨模态伪造证据可能会被忽略。现有的多模态深伪检测方法通常在音频和视觉modalities之间建立对应关系，并需要两个模式同时存在。但在实际的多模式应用中，缺失模式场景可能会发生，其中一个或多个模式都不可用。在这种情况下，音频视频检测方法不太实用，因为检测器无法在事先获知哪一个模式被修改。因此，我们需要一种不关心模式的多模态深伪检测方法。在这种方案中，我们提出一种统一的多模态深伪检测框架，可以检测多模态深伪并处理缺失模式场景，无论哪一个模式被修改。为了更好地模型跨模式伪造证据，我们选择了音频视频语音识别（AVSR）作为先前任务，它可以有效提取modalities之间的语音相关性，这是深伪很难复制的。此外，我们还提出了一种双标签检测方法，根据AVSR的结构来支持每个模式独立的检测。我们的方案不仅在所有三个音频视频数据集上超过了其他状态对抗方法的性能，还在缺失模式场景下达到了满意的性能。此外，它 même surpassed the joint use of two unimodal methods in the presence of missing modality cases.”
</details></li>
</ul>
<hr>
<h2 id="Single-Channel-Speech-Enhancement-Using-U-Net-Spiking-Neural-Networks"><a href="#Single-Channel-Speech-Enhancement-Using-U-Net-Spiking-Neural-Networks" class="headerlink" title="Single Channel Speech Enhancement Using U-Net Spiking Neural Networks"></a>Single Channel Speech Enhancement Using U-Net Spiking Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14464">http://arxiv.org/abs/2307.14464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abir Riahi, Éric Plourde</li>
<li>for: 提高沟通设备和可靠的语音识别系统的信噪比</li>
<li>methods: 使用基于U-Net架构的脉冲神经网络（SNN）进行沟通增强</li>
<li>results: 比预先达到的状态艺术神经网络（ANN）模型表现更好，并在不同的信号噪比和真实噪音条件下达到了可接受的表现。<details>
<summary>Abstract</summary>
Speech enhancement (SE) is crucial for reliable communication devices or robust speech recognition systems. Although conventional artificial neural networks (ANN) have demonstrated remarkable performance in SE, they require significant computational power, along with high energy costs. In this paper, we propose a novel approach to SE using a spiking neural network (SNN) based on a U-Net architecture. SNNs are suitable for processing data with a temporal dimension, such as speech, and are known for their energy-efficient implementation on neuromorphic hardware. As such, SNNs are thus interesting candidates for real-time applications on devices with limited resources. The primary objective of the current work is to develop an SNN-based model with comparable performance to a state-of-the-art ANN model for SE. We train a deep SNN using surrogate-gradient-based optimization and evaluate its performance using perceptual objective tests under different signal-to-noise ratios and real-world noise conditions. Our results demonstrate that the proposed energy-efficient SNN model outperforms the Intel Neuromorphic Deep Noise Suppression Challenge (Intel N-DNS Challenge) baseline solution and achieves acceptable performance compared to an equivalent ANN model.
</details>
<details>
<summary>摘要</summary>
声音提升（SE）是重要的通信设备或可靠的语音识别系统的关键。虽然传统的人工神经网络（ANN）已经表现出了惊人的表现在SE中，但它们需要显著的计算能力，同时也需要高度的能源成本。在这篇论文中，我们提出了一种使用快速神经网络（SNN）基于U-Net架构的新的方法 дляSE。SNN适用于处理具有时间维度的数据，如语音，并且被认为是能效地实现在神经机器硬件上的。因此，SNN是在有限资源的设备上实现实时应用的优秀选择。目标是在现有的ANN模型性能水平上实现相似的SE模型。我们使用代理梯度基于优化方法来训练深度SNN，并使用感知目标测试来评估其性能。我们的结果表明，我们提出的能效的SNN模型在不同的信号噪比和实际噪声条件下都能够达到适当的性能，并且超越了Intel neuromorphic Deep Noise Suppression Challenge（Intel N-DNS Challenge）基准解决方案。
</details></li>
</ul>
<hr>
<h2 id="WavJourney-Compositional-Audio-Creation-with-Large-Language-Models"><a href="#WavJourney-Compositional-Audio-Creation-with-Large-Language-Models" class="headerlink" title="WavJourney: Compositional Audio Creation with Large Language Models"></a>WavJourney: Compositional Audio Creation with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14335">http://arxiv.org/abs/2307.14335</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/audio-agi/wavjourney">https://github.com/audio-agi/wavjourney</a></li>
<li>paper_authors: Xubo Liu, Zhongkai Zhu, Haohe Liu, Yi Yuan, Meng Cui, Qiushi Huang, Jinhua Liang, Yin Cao, Qiuqiang Kong, Mark D. Plumbley, Wenwu Wang</li>
<li>for: 这 paper 旨在开发一种基于 Large Language Models (LLMs) 的音频内容生成系统，以便在语言和视觉任务中提高人工智能生成内容的能力。</li>
<li>methods: 这 paper 使用了 LLMs 连接多种音频模型，以生成包含演讲、音乐和音效的听写内容。它首先使用 LLMs 生成一份适用于音频storytelling的结构化脚本，然后使用这份脚本生成一个计算机程序，并将每行程序转换为一个特定的音频生成模型或计算操作函数。</li>
<li>results: 这 paper 在多个实际场景中展示了 WavJourney 的实用性，包括科幻、教育和广播剧等。WavJourney 的可靠和可交互的设计，使得人机共创在多轮对话中得到了进一步的创作控制和适应性。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have shown great promise in integrating diverse expert models to tackle intricate language and vision tasks. Despite their significance in advancing the field of Artificial Intelligence Generated Content (AIGC), their potential in intelligent audio content creation remains unexplored. In this work, we tackle the problem of creating audio content with storylines encompassing speech, music, and sound effects, guided by text instructions. We present WavJourney, a system that leverages LLMs to connect various audio models for audio content generation. Given a text description of an auditory scene, WavJourney first prompts LLMs to generate a structured script dedicated to audio storytelling. The audio script incorporates diverse audio elements, organized based on their spatio-temporal relationships. As a conceptual representation of audio, the audio script provides an interactive and interpretable rationale for human engagement. Afterward, the audio script is fed into a script compiler, converting it into a computer program. Each line of the program calls a task-specific audio generation model or computational operation function (e.g., concatenate, mix). The computer program is then executed to obtain an explainable solution for audio generation. We demonstrate the practicality of WavJourney across diverse real-world scenarios, including science fiction, education, and radio play. The explainable and interactive design of WavJourney fosters human-machine co-creation in multi-round dialogues, enhancing creative control and adaptability in audio production. WavJourney audiolizes the human imagination, opening up new avenues for creativity in multimedia content creation.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经显示出很大的损征在融合多种专家模型来解决复杂的语言和视觉任务上。尽管它们在人工智能生成内容（AIGC）领域中的潜力仍然未获探索，但它们在智能音频内容创建方面的潜力仍然未获开发。在这个工作中，我们对于创建包含话语、音乐和效果的音频内容进行了探索。我们提出了WavJourney系统，这个系统利用LLM来连接不同的音频模型，以实现音频内容创建。当我们给出了文本描述一个听频场景时，WavJourney首先透过LLM生成一个关于音频故事的结构化脚本。这个音频脚本包括多种音频元素，并且根据它们的空间时间关系进行了组织。作为一个概念表现的音频，这个音频脚本提供了互动和可解释的理由，以便人类参与。接下来，这个音频脚本会被转换为一个计算机程序，每行代码都会调用一个任务特定的音频生成模型或计算操作函数（例如， concatenate、mix）。计算机程序的执行将获得一个可解释的音频生成解决方案。我们在多个实际应用场景中证明了WavJourney的实用性，包括科幻、教育和广播剧。WavJourney的可说明和互动设计增强了人机共创的多轮 діало格，提高了创作控制和适应性。WavJourney声音化了人类的想像力，开启了新的创作可能性在多媒体内容创建领域。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/07/27/cs.SD_2023_07_27/" data-id="cllt9prwn004eol884wv2hsgo" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/27/cs.LG_2023_07_27/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-07-27 18:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/27/eess.AS_2023_07_27/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.AS - 2023-07-27 22:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
