
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.AS - 2023-07-27 22:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Audio Inputs for Active Speaker Detection and Localization via Microphone Array paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.14739 repo_url: None paper_authors: Davide Berghi, Philip J. B. Jackson for: 本研究探讨了">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.AS - 2023-07-27 22:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/07/27/eess.AS_2023_07_27/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Audio Inputs for Active Speaker Detection and Localization via Microphone Array paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.14739 repo_url: None paper_authors: Davide Berghi, Philip J. B. Jackson for: 本研究探讨了">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-26T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:39.409Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.AS_2023_07_27" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/27/eess.AS_2023_07_27/" class="article-date">
  <time datetime="2023-07-26T16:00:00.000Z" itemprop="datePublished">2023-07-27</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.AS - 2023-07-27 22:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Audio-Inputs-for-Active-Speaker-Detection-and-Localization-via-Microphone-Array"><a href="#Audio-Inputs-for-Active-Speaker-Detection-and-Localization-via-Microphone-Array" class="headerlink" title="Audio Inputs for Active Speaker Detection and Localization via Microphone Array"></a>Audio Inputs for Active Speaker Detection and Localization via Microphone Array</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14739">http://arxiv.org/abs/2307.14739</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davide Berghi, Philip J. B. Jackson</li>
<li>for: 本研究探讨了基于多通道音频的 aktive speaker detection 和位置测定（ASDL）问题。</li>
<li>methods: 该研究使用了 convolutional recurrent neural network（CRNN），使用了多通道音频中的空间声学特征，并对噪声的影响进行了测试。</li>
<li>results: 研究发现，使用GCC-PHAT和SALSA特征可以减少噪声的影响，而 beamforming 方法可以提高ASDL的性能。  Additionally, the study found that the number of channels and the sampling density of the microphone array have a significant impact on the performance of ASDL.<details>
<summary>Abstract</summary>
This study considers the problem of detecting and locating an active talker's horizontal position from multichannel audio captured by a microphone array. We refer to this as active speaker detection and localization (ASDL). Our goal was to investigate the performance of spatial acoustic features extracted from the multichannel audio as the input of a convolutional recurrent neural network (CRNN), in relation to the number of channels employed and additive noise. To this end, experiments were conducted to compare the generalized cross-correlation with phase transform (GCC-PHAT), the spatial cue-augmented log-spectrogram (SALSA) features, and a recently-proposed beamforming method, evaluating their robustness to various noise intensities. The array aperture and sampling density were tested by taking subsets from the 16-microphone array. Results and tests of statistical significance demonstrate the microphones' contribution to performance on the TragicTalkers dataset, which offers opportunities to investigate audio-visual approaches in the future.
</details>
<details>
<summary>摘要</summary>
Here's the simplified Chinese translation:这个研究关注的是从多通道音频记录的扬声器的活动位置探测和定位（ASDL）问题。我们的目标是 investigate CRNN（卷积隐estamp）的输入为多通道音频中的空间音学特征（GCC-PHAT、SALSA）的性能，与通道数和附加噪声之间的关系。为此，我们进行了比较减 correlated with phase transform（GCC-PHAT）、空间cue-augmented log-spectrogram（SALSA）特征和一种最近提出的扬声器方法的性能，对不同噪声强度进行评估。我们还测试了数组的开口和采样密度，使用16个 Microphonearray中的子集。结果和统计测试表明每个 Microphone的贡献，并提供了未来 investigate audio-visualapproaches的机会。
</details></li>
</ul>
<hr>
<h2 id="Physics-Informed-Neural-Network-for-Head-Related-Transfer-Function-Upsampling"><a href="#Physics-Informed-Neural-Network-for-Head-Related-Transfer-Function-Upsampling" class="headerlink" title="Physics Informed Neural Network for Head-Related Transfer Function Upsampling"></a>Physics Informed Neural Network for Head-Related Transfer Function Upsampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14650">http://arxiv.org/abs/2307.14650</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/feima0011/physics-informed-neural-network-for-head-related-transfer-function-upsampling">https://github.com/feima0011/physics-informed-neural-network-for-head-related-transfer-function-upsampling</a></li>
<li>paper_authors: Fei Ma, Thushara D. Abhayapala, Prasanga N. Samarasinghe, Xingyu Chen</li>
<li>for: 提高虚拟听觉体验的准确性，使用physics-informed neural network（PINN）方法进行HRTF上扩。</li>
<li>methods: 利用Helmholtz方程作为更多信息来约束上扩过程，使得生成的增强后HRTF具有物理准确性，并且采用SH分解来控制PINN网络的宽度和深度。</li>
<li>results: 对多个数据集进行比较，PINN方法在 interpolate 和 extrapolate 两种情况下表现出色，较SH方法有更好的性能。<details>
<summary>Abstract</summary>
Head-related transfer functions (HRTFs) capture the spatial and spectral features that a person uses to localize sound sources in space and thus are vital for creating an authentic virtual acoustic experience. However, practical HRTF measurement systems can only provide an incomplete measurement of a person's HRTFs, and this necessitates HRTF upsampling. This paper proposes a physics-informed neural network (PINN) method for HRTF upsampling. Unlike other upsampling methods which are based on the measured HRTFs only, the PINN method exploits the Helmholtz equation as additional information for constraining the upsampling process. This helps the PINN method to generate physically amiable upsamplings which generalize beyond the measured HRTFs. Furthermore, the width and the depth of the PINN are set according to the dimensionality of HRTFs under spherical harmonic (SH) decomposition and the Helmholtz equation. This makes the PINN have an appropriate level of expressiveness and thus does not suffer from under-fitting and over-fitting problems. Numerical experiments confirm the superior performance of the PINN method for HRTF upsampling in both interpolation and extrapolation scenarios over several datasets in comparison with the SH methods.
</details>
<details>
<summary>摘要</summary>
人头相关传函数（HRTF）捕捉声音源在空间中的特征和频谱特征，因此是创建真实的虚拟声学体验的关键。然而，实际的HRTF测量系统只能提供HRTF的不完全测量，因此需要HRTF upsampling。这篇论文提出了基于物理学习神经网络（PINN）方法的HRTF upsampling方法。与其他upsampling方法不同，PINN方法利用Helmholtz方程作为更多的约束来控制upsampling过程。这使得PINN方法能够生成符合物理规则的upsampling，并且在扩展测量HRTF的场景中表现出超过SH方法的优势。此外，PINN的宽度和深度设置与HRTF在圆柱幂分解中的维度和Helmholtz方程相关。这使得PINN具有相应的表达能力，从而不会出现过拟合和下降问题。数值实验证明PINN方法在 interpolación和 extrapolation 场景中对多个数据集表现出了superior performance  contrasted with SH方法。
</details></li>
</ul>
<hr>
<h2 id="NeuroHeed-Neuro-Steered-Speaker-Extraction-using-EEG-Signals"><a href="#NeuroHeed-Neuro-Steered-Speaker-Extraction-using-EEG-Signals" class="headerlink" title="NeuroHeed: Neuro-Steered Speaker Extraction using EEG Signals"></a>NeuroHeed: Neuro-Steered Speaker Extraction using EEG Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14303">http://arxiv.org/abs/2307.14303</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zexu Pan, Marvin Borsdorf, Siqi Cai, Tanja Schultz, Haizhou Li</li>
<li>for: 本研究旨在开发一种基于EEG信号的选择性听说模型，以便在听说场景中提取主要听众的语音信号。</li>
<li>methods: 本研究使用EEG信号来建立一个神经元吸引器，该吸引器与听众的注意力时间相关，以便提取主要听众的语音信号。</li>
<li>results: 实验结果表明，NeuroHeed模型可以有效地提取主要听众的语音信号，并达到高质量、优良 восприятие和 inteligibilty 在两个说话者场景中。<details>
<summary>Abstract</summary>
Humans possess the remarkable ability to selectively attend to a single speaker amidst competing voices and background noise, known as selective auditory attention. Recent studies in auditory neuroscience indicate a strong correlation between the attended speech signal and the corresponding brain's elicited neuronal activities, which the latter can be measured using affordable and non-intrusive electroencephalography (EEG) devices. In this study, we present NeuroHeed, a speaker extraction model that leverages EEG signals to establish a neuronal attractor which is temporally associated with the speech stimulus, facilitating the extraction of the attended speech signal in a cocktail party scenario. We propose both an offline and an online NeuroHeed, with the latter designed for real-time inference. In the online NeuroHeed, we additionally propose an autoregressive speaker encoder, which accumulates past extracted speech signals for self-enrollment of the attended speaker information into an auditory attractor, that retains the attentional momentum over time. Online NeuroHeed extracts the current window of the speech signals with guidance from both attractors. Experimental results demonstrate that NeuroHeed effectively extracts brain-attended speech signals, achieving high signal quality, excellent perceptual quality, and intelligibility in a two-speaker scenario.
</details>
<details>
<summary>摘要</summary>
人类具有选择性听取Single speaker amidst competing voices and background noise的能力，称为选择性听取。最近的听auditory neuroscience研究表明， attended speech signal和对应的大脑活动之间存在强相关关系，可以使用可得性和不侵入性的电enzephalography（EEG）设备来测量。在这个研究中，我们提出了NeuroHeed模型，利用EEG信号来建立一个neuronal attractor，该attractor在时间方面与语音刺激相关。这使得可以在庆酒party scenario中提取获得了注意力的speech signal。我们提出了两种NeuroHeed，一种是Offline NeuroHeed，另一种是在线NeuroHeed。在线NeuroHeed还包括一个自适应 speaker encoder，该encoder在过去提取的speech signal基础上积累 past extracted speech signals，以便在注意力保持的情况下，将注意力集中在获得了注意力的speaker上。在线NeuroHeed在当前窗口中提取speech signal，并且受到两个attractor的引导。实验结果表明，NeuroHeed能够有效地提取大脑注意力的speech signal，实现高质量的信号、优秀的感知质量和智能性在两个speaker scenario中。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/27/eess.AS_2023_07_27/" data-id="cllsiju36006ha388126x4dz8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/27/cs.SD_2023_07_27/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.SD - 2023-07-27 123:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/27/eess.IV_2023_07_27/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-07-27 17:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'', root:''}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
