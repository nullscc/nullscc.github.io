
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.AS - 2023-07-27 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Audio Inputs for Active Speaker Detection and Localization via Microphone Array paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.14739 repo_url: None paper_authors: Davide Berghi, Philip J. B. Jackson for: 本研究探讨了">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.AS - 2023-07-27">
<meta property="og:url" content="https://nullscc.github.io/2023/07/27/eess.AS_2023_07_27/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Audio Inputs for Active Speaker Detection and Localization via Microphone Array paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.14739 repo_url: None paper_authors: Davide Berghi, Philip J. B. Jackson for: 本研究探讨了">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-27T14:00:00.000Z">
<meta property="article:modified_time" content="2023-09-23T08:41:27.069Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.AS_2023_07_27" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/27/eess.AS_2023_07_27/" class="article-date">
  <time datetime="2023-07-27T14:00:00.000Z" itemprop="datePublished">2023-07-27</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.AS - 2023-07-27
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Audio-Inputs-for-Active-Speaker-Detection-and-Localization-via-Microphone-Array"><a href="#Audio-Inputs-for-Active-Speaker-Detection-and-Localization-via-Microphone-Array" class="headerlink" title="Audio Inputs for Active Speaker Detection and Localization via Microphone Array"></a>Audio Inputs for Active Speaker Detection and Localization via Microphone Array</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14739">http://arxiv.org/abs/2307.14739</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davide Berghi, Philip J. B. Jackson</li>
<li>for: 本研究探讨了基于多道声音捕获的活聊检测和定位问题，即活聊检测和定位（ASDL）。</li>
<li>methods: 本研究使用了一种卷积Recurrent Neural Network（CRNN），使用了多道声音中的空间音学特征作为输入，并对不同的渠道数和干扰噪音进行比较。</li>
<li>results: 实验结果表明，使用GCC-PHAT、SALSA特征和新的扩权报知方法可以减轻不同噪音水平下的表达性能下降，并且可以根据降噪性和渠道数进行优化。<details>
<summary>Abstract</summary>
This study considers the problem of detecting and locating an active talker's horizontal position from multichannel audio captured by a microphone array. We refer to this as active speaker detection and localization (ASDL). Our goal was to investigate the performance of spatial acoustic features extracted from the multichannel audio as the input of a convolutional recurrent neural network (CRNN), in relation to the number of channels employed and additive noise. To this end, experiments were conducted to compare the generalized cross-correlation with phase transform (GCC-PHAT), the spatial cue-augmented log-spectrogram (SALSA) features, and a recently-proposed beamforming method, evaluating their robustness to various noise intensities. The array aperture and sampling density were tested by taking subsets from the 16-microphone array. Results and tests of statistical significance demonstrate the microphones' contribution to performance on the TragicTalkers dataset, which offers opportunities to investigate audio-visual approaches in the future.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Physics-Informed-Neural-Network-for-Head-Related-Transfer-Function-Upsampling"><a href="#Physics-Informed-Neural-Network-for-Head-Related-Transfer-Function-Upsampling" class="headerlink" title="Physics Informed Neural Network for Head-Related Transfer Function Upsampling"></a>Physics Informed Neural Network for Head-Related Transfer Function Upsampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14650">http://arxiv.org/abs/2307.14650</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/feima0011/physics-informed-neural-network-for-head-related-transfer-function-upsampling">https://github.com/feima0011/physics-informed-neural-network-for-head-related-transfer-function-upsampling</a></li>
<li>paper_authors: Fei Ma, Thushara D. Abhayapala, Prasanga N. Samarasinghe, Xingyu Chen</li>
<li>for: 提高虚拟听觉体验的真实性，使用physics-informed neural network（PINN）方法进行HRTF upsampling。</li>
<li>methods: 基于Helmholtz方程的PINN方法，利用HRTF的物理特性来做upsampling，避免基于测量数据的局限性。</li>
<li>results: 对多个数据集进行比较，PINN方法在 interpolate 和 extrapolate 两种enario中具有更高的性能，不受under-fitting和over-fitting问题的影响。<details>
<summary>Abstract</summary>
Head-related transfer functions (HRTFs) capture the spatial and spectral features that a person uses to localize sound sources in space and thus are vital for creating an authentic virtual acoustic experience. However, practical HRTF measurement systems can only provide an incomplete measurement of a person's HRTFs, and this necessitates HRTF upsampling. This paper proposes a physics-informed neural network (PINN) method for HRTF upsampling. Unlike other upsampling methods which are based on the measured HRTFs only, the PINN method exploits the Helmholtz equation as additional information for constraining the upsampling process. This helps the PINN method to generate physically amiable upsamplings which generalize beyond the measured HRTFs. Furthermore, the width and the depth of the PINN are set according to the dimensionality of HRTFs under spherical harmonic (SH) decomposition and the Helmholtz equation. This makes the PINN have an appropriate level of expressiveness and thus does not suffer from under-fitting and over-fitting problems. Numerical experiments confirm the superior performance of the PINN method for HRTF upsampling in both interpolation and extrapolation scenarios over several datasets in comparison with the SH methods.
</details>
<details>
<summary>摘要</summary>
人头相关传函数（HRTF）捕捉了声音源的空间和频率特征，因此是创建真实虚拟听音场的关键。然而，实际测量HRTF系统只能提供 incomplete HRTF 测量，这需要HRTF 采样。这篇论文提出了一种基于物理学习神经网络（PINN）方法的 HRTF 采样方法。与其他采样方法不同，PINN 方法利用 Helmholtz 方程作为额外信息，以制约采样过程。这帮助 PINN 方法生成physically amiable的采样，并且这些采样可以扩展到测量 HRTF 之 beyond。此外，PINN 方法的宽度和深度是根据 HRTF 的维度下圆函数（SH）划分和 Helmholtz 方程来设置。这使得 PINN 方法具有合适的表达能力，并且不会出现过拟合和下拟合问题。 numerical experiments 表明，PINN 方法在 interpolate 和 extrapolate  scenarios 中对多个数据集的 HRTF 采样性能较 SH 方法更高。
</details></li>
</ul>
<hr>
<h2 id="NeuroHeed-Neuro-Steered-Speaker-Extraction-using-EEG-Signals"><a href="#NeuroHeed-Neuro-Steered-Speaker-Extraction-using-EEG-Signals" class="headerlink" title="NeuroHeed: Neuro-Steered Speaker Extraction using EEG Signals"></a>NeuroHeed: Neuro-Steered Speaker Extraction using EEG Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14303">http://arxiv.org/abs/2307.14303</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zexu Pan, Marvin Borsdorf, Siqi Cai, Tanja Schultz, Haizhou Li</li>
<li>for: 本研究旨在开发一种基于EEG信号的选择性听力模型，以实现在听到干扰的多人对话中提取主要的说话人信号。</li>
<li>methods: 该模型使用EEG信号来建立一个neuronal attractor，其与听到的语音刺激相关，并通过在线和离线两种方式实现实时和非实时的抽取。在线NeuroHeed还包括一个自适应核心编码器，以积累过去抽取的语音信号，以便在下一个时间窗口中帮助抽取当前说话人信号。</li>
<li>results: 实验结果表明，NeuroHeed能够有效地提取主要的说话人信号，并达到高质量、出色的 восприятия质量和语音可理解性。<details>
<summary>Abstract</summary>
Humans possess the remarkable ability to selectively attend to a single speaker amidst competing voices and background noise, known as selective auditory attention. Recent studies in auditory neuroscience indicate a strong correlation between the attended speech signal and the corresponding brain's elicited neuronal activities, which the latter can be measured using affordable and non-intrusive electroencephalography (EEG) devices. In this study, we present NeuroHeed, a speaker extraction model that leverages EEG signals to establish a neuronal attractor which is temporally associated with the speech stimulus, facilitating the extraction of the attended speech signal in a cocktail party scenario. We propose both an offline and an online NeuroHeed, with the latter designed for real-time inference. In the online NeuroHeed, we additionally propose an autoregressive speaker encoder, which accumulates past extracted speech signals for self-enrollment of the attended speaker information into an auditory attractor, that retains the attentional momentum over time. Online NeuroHeed extracts the current window of the speech signals with guidance from both attractors. Experimental results demonstrate that NeuroHeed effectively extracts brain-attended speech signals, achieving high signal quality, excellent perceptual quality, and intelligibility in a two-speaker scenario.
</details>
<details>
<summary>摘要</summary>
人类具有选择性听觉能力，能够在多个声音和背景噪声中选择一个声音，这种能力被称为选择性听觉注意力。最近的听觉神经科学研究表明，在听觉过程中选择的语音信号和大脑发生的神经活动之间存在强相关性，这些神经活动可以使用便宜和不侵入的电enzephalography（EEG）设备测量。在这个研究中，我们介绍了NeuroHeed，一种基于EEG信号的语音抽取模型，可以在听觉场景中提取选择的语音信号。我们提出了两种NeuroHeed，一个是OFFLINE版本，另一个是ONLINE版本。在ONLINE版本中，我们还提出了自适应语音编码器，该编码器将过去提取的语音信号accumulate为自我投入的听觉招引器，以保持注意力的积累。ONLINE NeuroHeed在当前窗口中提取语音信号，受到两个招引器的引导。实验结果表明，NeuroHeed可以有效地提取大脑注意力的语音信号，实现高质量的语音信号、优美的听觉质量和语音清晰度在两个说话者场景中。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/27/eess.AS_2023_07_27/" data-id="clopawnyz0102ag88dzn065fs" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/27/cs.SD_2023_07_27/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.SD - 2023-07-27
        
      </div>
    </a>
  
  
    <a href="/2023/07/27/cs.CV_2023_07_27/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.CV - 2023-07-27</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">120</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">57</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">67</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
