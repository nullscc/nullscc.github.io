
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-07-21 17:00:00 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.11638 repo_url: None paper_authors: Charlie Budd, Jianrong Qiu, Oscar">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-07-21 17:00:00">
<meta property="og:url" content="http://example.com/2023/07/21/eess.IV_2023_07_21/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.11638 repo_url: None paper_authors: Charlie Budd, Jianrong Qiu, Oscar">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-21T00:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:36.013Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_07_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/21/eess.IV_2023_07_21/" class="article-date">
  <time datetime="2023-07-21T00:00:00.000Z" itemprop="datePublished">2023-07-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-07-21 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Deep-Reinforcement-Learning-Based-System-for-Intraoperative-Hyperspectral-Video-Autofocusing"><a href="#Deep-Reinforcement-Learning-Based-System-for-Intraoperative-Hyperspectral-Video-Autofocusing" class="headerlink" title="Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing"></a>Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11638">http://arxiv.org/abs/2307.11638</a></li>
<li>repo_url: None</li>
<li>paper_authors: Charlie Budd, Jianrong Qiu, Oscar MacCormac, Martin Huber, Christopher Mower, Mirek Janatka, Théo Trotouin, Jonathan Shapey, Mads S. Bergholt, Tom Vercauteren</li>
<li>for:  This paper aims to improve the usability of handheld real-time video hyperspectral imaging (HSI) in the operating room by integrating a focus-tunable liquid lens and proposing novel video autofocusing methods based on deep reinforcement learning.</li>
<li>methods:  The proposed autofocus algorithm uses deep reinforcement learning to improve the accuracy and speed of focusing in HSI. A robotic focal-time scan was used to create a realistic and reproducible testing dataset.</li>
<li>results:  The proposed autofocus algorithm significantly outperformed traditional techniques ($0.070\pm.098$ mean absolute focal error compared to $0.146\pm.148$, $p&lt;0.05$). Additionally, two neurosurgeons preferred the novel approach in a blinded usability trial.<details>
<summary>Abstract</summary>
Hyperspectral imaging (HSI) captures a greater level of spectral detail than traditional optical imaging, making it a potentially valuable intraoperative tool when precise tissue differentiation is essential. Hardware limitations of current optical systems used for handheld real-time video HSI result in a limited focal depth, thereby posing usability issues for integration of the technology into the operating room. This work integrates a focus-tunable liquid lens into a video HSI exoscope, and proposes novel video autofocusing methods based on deep reinforcement learning. A first-of-its-kind robotic focal-time scan was performed to create a realistic and reproducible testing dataset. We benchmarked our proposed autofocus algorithm against traditional policies, and found our novel approach to perform significantly ($p<0.05$) better than traditional techniques ($0.070\pm.098$ mean absolute focal error compared to $0.146\pm.148$). In addition, we performed a blinded usability trial by having two neurosurgeons compare the system with different autofocus policies, and found our novel approach to be the most favourable, making our system a desirable addition for intraoperative HSI.
</details>
<details>
<summary>摘要</summary>
hyperspectral imaging (HSI) 可以捕捉更高水平的spectral detail than traditional optical imaging，这使得它在精确的组织分类是必要时成为了一种有价值的内部操作工具。现有的光学系统的硬件限制使得实时视频HSI中的聚焦深度受限，这导致了技术的使用问题。这个工作将带liquid lens into a video HSI exoscope中，并提出了基于深度学习的视频自动对焦方法。我们首次在实时视频HSI中实现了Robotic focal-time scan，并创建了一个真实和可重复的测试集。我们对我们提议的自动对焦算法与传统策略进行了比较，发现我们的新方法在($p<0.05$) 下表现得更好，其中($0.070\pm.098$ mean absolute focal error compared to $0.146\pm.148$)。此外，我们进行了一次隐藏的用户评估试验，让两名神经外科医生对不同的自动对焦策略进行比较，发现我们的新方法最受欢迎，使得我们的系统成为了内部HSI中的一种愿望。
</details></li>
</ul>
<hr>
<h2 id="Computational-Image-Formation"><a href="#Computational-Image-Formation" class="headerlink" title="Computational Image Formation"></a>Computational Image Formation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11635">http://arxiv.org/abs/2307.11635</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AnirudhaRamesh/15663-Computational-Photography-Assignment1">https://github.com/AnirudhaRamesh/15663-Computational-Photography-Assignment1</a></li>
<li>paper_authors: Stanley H. Chan</li>
<li>for: computational image formation (CIF) is a new approach that shifts the focus from recovering the latent image to designing an approximate mapping that gives a better image reconstruction result.</li>
<li>methods: the paper introduces the concept of CIF and discusses its four attributes: accuracy, speed, well-posedness, and differentiability.</li>
<li>results: the paper provides a detailed case study on imaging through atmospheric turbulence and collects other examples that fall into the category of CIF.Here’s the simplified Chinese text for the three information points:</li>
<li>for: 这篇论文提出了一种新的计算成像方法（CIF），它的目的是设计一个更好地重建图像的数学模型。</li>
<li>methods: 这篇论文提出了CIF的四个特征：准确地模拟天然的映射、快速地与重建算法集成、提供一个准确的反向问题，以及可微的特征。</li>
<li>results: 这篇论文提供了一个具体的案例研究，探讨了通过大气扰动进行成像，并收集了其他符合CIF的例子。<details>
<summary>Abstract</summary>
At the pinnacle of computational imaging is the co-optimization of camera and algorithm. This, however, is not the only form of computational imaging. In problems such as imaging through adverse weather, the bigger challenge is how to accurately simulate the forward degradation process so that we can synthesize data to train reconstruction models and/or integrating the forward model as part of the reconstruction algorithm. This article introduces the concept of computational image formation (CIF). Compared to the standard inverse problems where the goal is to recover the latent image $\mathbf{x}$ from the observation $\mathbf{y} = \mathcal{G}(\mathbf{x})$, CIF shifts the focus to designing an approximate mapping $\mathcal{H}_{\theta}$ such that $\mathcal{H}_{\theta} \approx \mathcal{G}$ while giving a better image reconstruction result. The word ``computational'' highlights the fact that the image formation is now replaced by a numerical simulator. While matching nature remains an important goal, CIF pays even greater attention on strategically choosing an $\mathcal{H}_{\theta}$ so that the reconstruction performance is maximized.   The goal of this article is to conceptualize the idea of CIF by elaborating on its meaning and implications. The first part of the article is a discussion on the four attributes of a CIF simulator: accurate enough to mimic $\mathcal{G}$, fast enough to be integrated as part of the reconstruction, providing a well-posed inverse problem when plugged into the reconstruction, and differentiable in the backpropagation sense. The second part of the article is a detailed case study based on imaging through atmospheric turbulence. The third part of the article is a collection of other examples that fall into the category of CIF. Finally, thoughts about the future direction and recommendations to the community are shared.
</details>
<details>
<summary>摘要</summary>
At the pinnacle of computational imaging is the co-optimization of camera and algorithm. However, this is not the only form of computational imaging. In problems such as imaging through adverse weather, the bigger challenge is how to accurately simulate the forward degradation process so that we can synthesize data to train reconstruction models and/or integrating the forward model as part of the reconstruction algorithm. This article introduces the concept of computational image formation (CIF). Compared to the standard inverse problems where the goal is to recover the latent image $\mathbf{x}$ from the observation $\mathbf{y} = \mathcal{G}(\mathbf{x})$, CIF shifts the focus to designing an approximate mapping $\mathcal{H}_{\theta}$ such that $\mathcal{H}_{\theta} \approx \mathcal{G}$ while giving a better image reconstruction result. The word "computational" highlights the fact that the image formation is now replaced by a numerical simulator. While matching nature remains an important goal, CIF pays even greater attention on strategically choosing an $\mathcal{H}_{\theta}$ so that the reconstruction performance is maximized.  The goal of this article is to conceptualize the idea of CIF by elaborating on its meaning and implications. The first part of the article is a discussion on the four attributes of a CIF simulator: accurate enough to mimic $\mathcal{G}$, fast enough to be integrated as part of the reconstruction, providing a well-posed inverse problem when plugged into the reconstruction, and differentiable in the backpropagation sense. The second part of the article is a detailed case study based on imaging through atmospheric turbulence. The third part of the article is a collection of other examples that fall into the category of CIF. Finally, thoughts about the future direction and recommendations to the community are shared.
</details></li>
</ul>
<hr>
<h2 id="Cascaded-multitask-U-Net-using-topological-loss-for-vessel-segmentation-and-centerline-extraction"><a href="#Cascaded-multitask-U-Net-using-topological-loss-for-vessel-segmentation-and-centerline-extraction" class="headerlink" title="Cascaded multitask U-Net using topological loss for vessel segmentation and centerline extraction"></a>Cascaded multitask U-Net using topological loss for vessel segmentation and centerline extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11603">http://arxiv.org/abs/2307.11603</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Rougé, Nicolas Passat, Odyssée Merveille</li>
<li>for: 这个论文主要针对的是血管分割和中心线抽象两个预期任务，这些任务是许多计算机辅助诊断工具的关键阶段，用于诊断血液疾病。</li>
<li>methods: 这个论文提出了一种基于深度学习的方法，使用U-Net计算出血管skeleton，并通过clDice损失函数嵌入topologicstraineduring segmentation。</li>
<li>results: 研究人员表明，使用这种方法可以获得更加准确的血管skeleton，并且可以提高血管分割和中心线抽象的准确性。<details>
<summary>Abstract</summary>
Vessel segmentation and centerline extraction are two crucial preliminary tasks for many computer-aided diagnosis tools dealing with vascular diseases. Recently, deep-learning based methods have been widely applied to these tasks. However, classic deep-learning approaches struggle to capture the complex geometry and specific topology of vascular networks, which is of the utmost importance in most applications. To overcome these limitations, the clDice loss, a topological loss that focuses on the vessel centerlines, has been recently proposed. This loss requires computing, with a proposed soft-skeleton algorithm, the skeletons of both the ground truth and the predicted segmentation. However, the soft-skeleton algorithm provides suboptimal results on 3D images, which makes the clDice hardly suitable on 3D images. In this paper, we propose to replace the soft-skeleton algorithm by a U-Net which computes the vascular skeleton directly from the segmentation. We show that our method provides more accurate skeletons than the soft-skeleton algorithm. We then build upon this network a cascaded U-Net trained with the clDice loss to embed topological constraints during the segmentation. The resulting model is able to predict both the vessel segmentation and centerlines with a more accurate topology.
</details>
<details>
<summary>摘要</summary>
船体分割和中心线提取是许多计算机助动诊断工具处理血管疾病的两个关键前期任务。最近，深度学习基于方法在这些任务上广泛应用。然而，经典深度学习方法很难捕捉血管网络的复杂几何和特定的拓扑结构，这在大多数应用中是非常重要的。为了解决这些限制，最近提出了clDice损失，这是一种专门关注血管中心线的拓扑损失。这个损失需要，通过我们提议的软skeleton算法，计算两个参考数据和预测分割的skeleton。然而，软skeleton算法在3D图像上提供的结果不够佳，使clDice难以在3D图像上使用。在这篇论文中，我们提议将软skeleton算法 replaced by U-Net，该算法直接从分割中计算血管skeleton。我们显示了我们的方法可以提供更加准确的skeleton。然后，我们在这个网络上建立了一个嵌入拓扑约束的加性U-Net，用clDice损失来训练。结果是，我们的模型可以更加准确地预测船体分割和中心线。
</details></li>
</ul>
<hr>
<h2 id="CortexMorph-fast-cortical-thickness-estimation-via-diffeomorphic-registration-using-VoxelMorph"><a href="#CortexMorph-fast-cortical-thickness-estimation-via-diffeomorphic-registration-using-VoxelMorph" class="headerlink" title="CortexMorph: fast cortical thickness estimation via diffeomorphic registration using VoxelMorph"></a>CortexMorph: fast cortical thickness estimation via diffeomorphic registration using VoxelMorph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11567">http://arxiv.org/abs/2307.11567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Richard McKinley, Christian Rummel</li>
<li>for: 这项研究的目的是提出一种新的 cortical thickness 估计方法，以便在短时间内从 T1 束缚图像中 estimate 区域性厚度。</li>
<li>methods: 该方法使用了一种基于深度学习的不定型变换方法，直接将 gray-white matter 界面 diffeomorphic 变换到 pia 表面，以便计算 cortical thickness。</li>
<li>results: 该方法可以在秒钟内将 anatomical segmentation 完成，并且可以保持 cortical atrophy 的检测能力。 并且，与传统的 image registration 方法相比，学习型的 image registration 方法可以更快地完成 registration 任务。<details>
<summary>Abstract</summary>
The thickness of the cortical band is linked to various neurological and psychiatric conditions, and is often estimated through surface-based methods such as Freesurfer in MRI studies. The DiReCT method, which calculates cortical thickness using a diffeomorphic deformation of the gray-white matter interface towards the pial surface, offers an alternative to surface-based methods. Recent studies using a synthetic cortical thickness phantom have demonstrated that the combination of DiReCT and deep-learning-based segmentation is more sensitive to subvoxel cortical thinning than Freesurfer.   While anatomical segmentation of a T1-weighted image now takes seconds, existing implementations of DiReCT rely on iterative image registration methods which can take up to an hour per volume. On the other hand, learning-based deformable image registration methods like VoxelMorph have been shown to be faster than classical methods while improving registration accuracy. This paper proposes CortexMorph, a new method that employs unsupervised deep learning to directly regress the deformation field needed for DiReCT. By combining CortexMorph with a deep-learning-based segmentation model, it is possible to estimate region-wise thickness in seconds from a T1-weighted image, while maintaining the ability to detect cortical atrophy. We validate this claim on the OASIS-3 dataset and the synthetic cortical thickness phantom of Rusak et al.
</details>
<details>
<summary>摘要</summary>
cortical band 的厚度与多种神经内科和精神病有关，通常通过surface-based方法such as Freesurfer在MRI研究中估算。DiReCT方法，它通过Diffomorphic deformation of the gray-white matter interface towards the pial surface来计算 cortical thickness，提供了一种alternative to surface-based方法。 recent studies using a synthetic cortical thickness phantom have shown that the combination of DiReCT and deep-learning-based segmentation is more sensitive to subvoxel cortical thinning than Freesurfer。 现有的DiReCT实现方法 rely on iterative image registration methods，which can take up to an hour per volume。On the other hand, learning-based deformable image registration methods like VoxelMorph have been shown to be faster than classical methods while improving registration accuracy。this paper proposes CortexMorph，a new method that employs unsupervised deep learning to directly regress the deformation field needed for DiReCT。by combining CortexMorph with a deep-learning-based segmentation model, it is possible to estimate region-wise thickness in seconds from a T1-weighted image，while maintaining the ability to detect cortical atrophy。we validate this claim on the OASIS-3 dataset and the synthetic cortical thickness phantom of Rusak et al。
</details></li>
</ul>
<hr>
<h2 id="FedAutoMRI-Federated-Neural-Architecture-Search-for-MR-Image-Reconstruction"><a href="#FedAutoMRI-Federated-Neural-Architecture-Search-for-MR-Image-Reconstruction" class="headerlink" title="FedAutoMRI: Federated Neural Architecture Search for MR Image Reconstruction"></a>FedAutoMRI: Federated Neural Architecture Search for MR Image Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11538">http://arxiv.org/abs/2307.11538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruoyou Wu, Cheng Li, Juan Zou, Shanshan Wang</li>
<li>For: This paper proposes a novel approach for federated learning-based MR image reconstruction, addressing the privacy concerns and performance degradation issues of existing methods.* Methods: The proposed method, called FederAted neUral archiTecture search approach fOr MR Image reconstruction (FedAutoMRI), uses differentiable architecture search to automatically find the optimal network architecture, and an exponential moving average method to improve the robustness of the client model to address data heterogeneity.* Results: The proposed FedAutoMRI achieves promising performances while utilizing a lightweight model with only a small number of parameters, outperforming classical federated learning methods.Here’s the simplified Chinese text:* 用途: 本文提出了一种基于联合学习的MR图像重建方法，以解决现有方法的隐私问题和性能下降问题。* 方法: 提议的方法使用可导的建筑搜索自动找到最佳网络建筑，并使用指数移动平均方法提高客户端模型的数据不一致问题的 Robustness。* 结果: 提议的 FedAutoMRI 可以实现出色的性能，同时使用轻量级模型，与传统联合学习方法相比，具有较少的参数数量。<details>
<summary>Abstract</summary>
Centralized training methods have shown promising results in MR image reconstruction, but privacy concerns arise when gathering data from multiple institutions. Federated learning, a distributed collaborative training scheme, can utilize multi-center data without the need to transfer data between institutions. However, existing federated learning MR image reconstruction methods rely on manually designed models which have extensive parameters and suffer from performance degradation when facing heterogeneous data distributions. To this end, this paper proposes a novel FederAted neUral archiTecture search approach fOr MR Image reconstruction (FedAutoMRI). The proposed method utilizes differentiable architecture search to automatically find the optimal network architecture. In addition, an exponential moving average method is introduced to improve the robustness of the client model to address the data heterogeneity issue. To the best of our knowledge, this is the first work to use federated neural architecture search for MR image reconstruction. Experimental results demonstrate that our proposed FedAutoMRI can achieve promising performances while utilizing a lightweight model with only a small number of model parameters compared to the classical federated learning methods.
</details>
<details>
<summary>摘要</summary>
中央化训练方法在MR图像重建方面已经显示出了有前途的结果，但是收集数据从多个机构时，隐私问题产生。基于分布式合作训练的联合学习方法可以利用多中心数据无需传输数据 между机构。然而，现有的联合学习MR图像重建方法仍然依赖于手动设计的模型，这些模型具有丰富的参数和面临不一致数据分布时性能下降。为此，本文提出了一种新的 FederAted neUral archiTecture search approach fOr MR Image reconstruction（FedAutoMRI）。提议的方法利用梯度下降搜索自动找到最佳网络架构。此外，我们还引入了一种加权移动平均方法，以提高客户端模型对数据不一致问题的Robustness。我们知道，这是首次使用联合 neural architecture search来进行MR图像重建。实验结果表明，我们的提议的FedAutoMRI可以在使用轻量级模型，只有少数参数的情况下实现出色的性能。
</details></li>
</ul>
<hr>
<h2 id="UWAT-GAN-Fundus-Fluorescein-Angiography-Synthesis-via-Ultra-wide-angle-Transformation-Multi-scale-GAN"><a href="#UWAT-GAN-Fundus-Fluorescein-Angiography-Synthesis-via-Ultra-wide-angle-Transformation-Multi-scale-GAN" class="headerlink" title="UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-wide-angle Transformation Multi-scale GAN"></a>UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-wide-angle Transformation Multi-scale GAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11530">http://arxiv.org/abs/2307.11530</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Tinysqua/UWAT-GAN">https://github.com/Tinysqua/UWAT-GAN</a></li>
<li>paper_authors: Zhaojie Fang, Zhanghao Chen, Pengxue Wei, Wangting Li, Shaochong Zhang, Ahmed Elazab, Gangyong Jia, Ruiquan Ge, Changmiao Wang</li>
<li>for: 这个论文的目的是提出一种新的 conditional generative adversarial network (UWAT-GAN)，用于从 Ultra-Wide-angle Fundus Scanning Laser Ophthalmoscopy (UWF-SLO) 图像中生成 Ultra-Wide-angle Fundus Fluorescein Angiography (UWF-FA) 图像，以避免使用抗生素药物的副作用。</li>
<li>methods: 该论文使用了一种 conditional generative adversarial network (UWAT-GAN)，包括多尺度生成器和融合模块负担，以及一种注意传输模块，以更好地提取全像和局部信息。同时，使用了多种新的质量损失函数进行超参数调整。</li>
<li>results: 实验结果表明，UWAT-GAN 比现有方法更高效，可以生成高分辨率的 UWF-FA 图像，并且可以更好地捕捉小型血管病变区域。<details>
<summary>Abstract</summary>
Fundus photography is an essential examination for clinical and differential diagnosis of fundus diseases. Recently, Ultra-Wide-angle Fundus (UWF) techniques, UWF Fluorescein Angiography (UWF-FA) and UWF Scanning Laser Ophthalmoscopy (UWF-SLO) have been gradually put into use. However, Fluorescein Angiography (FA) and UWF-FA require injecting sodium fluorescein which may have detrimental influences. To avoid negative impacts, cross-modality medical image generation algorithms have been proposed. Nevertheless, current methods in fundus imaging could not produce high-resolution images and are unable to capture tiny vascular lesion areas. This paper proposes a novel conditional generative adversarial network (UWAT-GAN) to synthesize UWF-FA from UWF-SLO. Using multi-scale generators and a fusion module patch to better extract global and local information, our model can generate high-resolution images. Moreover, an attention transmit module is proposed to help the decoder learn effectively. Besides, a supervised approach is used to train the network using multiple new weighted losses on different scales of data. Experiments on an in-house UWF image dataset demonstrate the superiority of the UWAT-GAN over the state-of-the-art methods. The source code is available at: https://github.com/Tinysqua/UWAT-GAN.
</details>
<details>
<summary>摘要</summary>
腔室摄影是临床和不同诊断fundus疾病的重要检查。最近，ultra-wide-angle fundus（UWF）技术、UWF fluorescein angiography（UWF-FA）和UWF scanning laser ophthalmoscopy（UWF-SLO）逐渐投入使用。然而，fluorescein angiography（FA）和UWF-FA需要注射Na fluorescein，可能会有不良影响。为了避免这些影响，cross-modality医学图像生成算法被提议。然而，当前的fundus摄影技术无法生成高分辨率图像，并且无法捕捉微型血管病变区域。本文提出了一种新的conditional generative adversarial network（UWAT-GAN），用于从UWF-SLO中生成UWF-FA。使用多尺度生成器和一个拼接模块，我们的模型可以生成高分辨率图像。此外，我们还提出了一种注意力传输模块，以帮助解码器更好地学习。此外，我们使用多种新的权重损失来训练网络。实验表明，UWAT-GAN在医学图像生成领域的表现较为出色，超过了当前的状态体系。源代码可以在https://github.com/Tinysqua/UWAT-GAN中下载。
</details></li>
</ul>
<hr>
<h2 id="Bone-mineral-density-estimation-from-a-plain-X-ray-image-by-learning-decomposition-into-projections-of-bone-segmented-computed-tomography"><a href="#Bone-mineral-density-estimation-from-a-plain-X-ray-image-by-learning-decomposition-into-projections-of-bone-segmented-computed-tomography" class="headerlink" title="Bone mineral density estimation from a plain X-ray image by learning decomposition into projections of bone-segmented computed tomography"></a>Bone mineral density estimation from a plain X-ray image by learning decomposition into projections of bone-segmented computed tomography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11513">http://arxiv.org/abs/2307.11513</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Gu, Yoshito Otake, Keisuke Uemura, Mazen Soufi, Masaki Takao, Hugues Talbot, Seiji Okada, Nobuhiko Sugano, Yoshinobu Sato</li>
<li>For: The paper aims to estimate bone mineral density (BMD) from plain X-ray images for opportunistic screening of osteoporosis.* Methods: The proposed method uses a deep learning approach to learn decomposition into projections of bone-segmented quantitative computed tomography (QCT) for BMD estimation under limited datasets.* Results: The proposed method achieved high accuracy in BMD estimation, with Pearson correlation coefficients of 0.880 and 0.920 observed for DXA-measured BMD and QCT-measured BMD estimation tasks, respectively. The root mean square of the coefficient of variation values were 3.27 to 3.79% for four measurements with different poses.<details>
<summary>Abstract</summary>
Osteoporosis is a prevalent bone disease that causes fractures in fragile bones, leading to a decline in daily living activities. Dual-energy X-ray absorptiometry (DXA) and quantitative computed tomography (QCT) are highly accurate for diagnosing osteoporosis; however, these modalities require special equipment and scan protocols. To frequently monitor bone health, low-cost, low-dose, and ubiquitously available diagnostic methods are highly anticipated. In this study, we aim to perform bone mineral density (BMD) estimation from a plain X-ray image for opportunistic screening, which is potentially useful for early diagnosis. Existing methods have used multi-stage approaches consisting of extraction of the region of interest and simple regression to estimate BMD, which require a large amount of training data. Therefore, we propose an efficient method that learns decomposition into projections of bone-segmented QCT for BMD estimation under limited datasets. The proposed method achieved high accuracy in BMD estimation, where Pearson correlation coefficients of 0.880 and 0.920 were observed for DXA-measured BMD and QCT-measured BMD estimation tasks, respectively, and the root mean square of the coefficient of variation values were 3.27 to 3.79% for four measurements with different poses. Furthermore, we conducted extensive validation experiments, including multi-pose, uncalibrated-CT, and compression experiments toward actual application in routine clinical practice.
</details>
<details>
<summary>摘要</summary>
骨质疾病（osteoporosis）是一种常见的骨疾病，导致脆弱骨头的破坏，从而导致日常生活活动下降。双能X射线吸收率（DXA）和量子 computed tomography（QCT）是骨质疾病的诊断非常准确，但这些modalities需要特殊设备和扫描协议。为了经常监控骨健康，低成本、低剂量、普遍可用的诊断方法是非常需要。在这个研究中，我们想要从平面X射线像中估算骨骼矫正（BMD），以便早期诊断。现有的方法通常使用多阶段方法，包括类别领域的提取和简单的回归来估算BMD，需要很多训练数据。因此，我们提出了一个效率的方法，可以从骨质疾病患者的平面X射线像中学习分解为骨质疾病患者的骨 segmentation，以便BMD估算。我们的方法实现了高精度的BMD估算，其中DXA-测量BMD和QCT-测量BMD估算任务之间的对应关系系数为0.880和0.920，分别为3.27%到3.79%。此外，我们还进行了广泛的验证实验，包括多姿、无测量CT和压缩实验，以验证它们的可行性和可靠性。
</details></li>
</ul>
<hr>
<h2 id="MatSpectNet-Material-Segmentation-Network-with-Domain-Aware-and-Physically-Constrained-Hyperspectral-Reconstruction"><a href="#MatSpectNet-Material-Segmentation-Network-with-Domain-Aware-and-Physically-Constrained-Hyperspectral-Reconstruction" class="headerlink" title="MatSpectNet: Material Segmentation Network with Domain-Aware and Physically-Constrained Hyperspectral Reconstruction"></a>MatSpectNet: Material Segmentation Network with Domain-Aware and Physically-Constrained Hyperspectral Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11466">http://arxiv.org/abs/2307.11466</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/heng-yuwen/matspectnet">https://github.com/heng-yuwen/matspectnet</a></li>
<li>paper_authors: Yuwen Heng, Yihong Wu, Jiawen Chen, Srinandan Dasmahapatra, Hansung Kim</li>
<li>for: 这 paper 的目的是提出一种新的物质分 segmentation 模型，以便从 RGB 图像中恢复 hyperspectral 图像，并使用这些恢复的 hyperspectral 图像进行物质分 segmentation。</li>
<li>methods: 这 paper 使用了一种名为 MatSpectNet 的新模型，该模型利用了现代摄像头的色彩感知原理来干涉恢复的 hyperspectral 图像，并使用域 adaptive 方法来普适化 hyperspectral 恢复能力从 spectral recovery 数据集到物质分 segmentation 数据集。</li>
<li>results: 根据 authors 的实验，MatSpectNet 可以在 LMD 数据集和 OpenSurfaces 数据集上达到一个 1.60% 的平均像素准确率和一个 3.42% 的平均类准确率，相比之前的最新发表文章。<details>
<summary>Abstract</summary>
Achieving accurate material segmentation for 3-channel RGB images is challenging due to the considerable variation in a material's appearance. Hyperspectral images, which are sets of spectral measurements sampled at multiple wavelengths, theoretically offer distinct information for material identification, as variations in intensity of electromagnetic radiation reflected by a surface depend on the material composition of a scene. However, existing hyperspectral datasets are impoverished regarding the number of images and material categories for the dense material segmentation task, and collecting and annotating hyperspectral images with a spectral camera is prohibitively expensive. To address this, we propose a new model, the MatSpectNet to segment materials with recovered hyperspectral images from RGB images. The network leverages the principles of colour perception in modern cameras to constrain the reconstructed hyperspectral images and employs the domain adaptation method to generalise the hyperspectral reconstruction capability from a spectral recovery dataset to material segmentation datasets. The reconstructed hyperspectral images are further filtered using learned response curves and enhanced with human perception. The performance of MatSpectNet is evaluated on the LMD dataset as well as the OpenSurfaces dataset. Our experiments demonstrate that MatSpectNet attains a 1.60% increase in average pixel accuracy and a 3.42% improvement in mean class accuracy compared with the most recent publication. The project code is attached to the supplementary material and will be published on GitHub.
</details>
<details>
<summary>摘要</summary>
实现精确物品分类 для 3-channel RGB 图像是具有挑战性的，因为物品的外观可能会有很大的变化。对此，可以利用几何pectral 图像，它们是场景中物品的化学成分所决定的几何spectral 测量的集合。然而，现有的几何pectral 数据集不够多，而且还没有充分的标注。为了解决这个问题，我们提出了一个新的模型，即 MatSpectNet，用于从 RGB 图像中恢复几何pectral 图像，并将其用于物品分类任务中。MatSpectNet 模型运用了现代相机中颜色感知的原理，将恢复的几何pectral 图像进行约束，并使用领域适应方法来将几何pectral 恢复能力从几何spectral 恢复数据集扩展到物品分类数据集。再次，恢复的几何pectral 图像会被使用学习的回应曲线进行筛选和人类感知进行增强。我们在 LMD 数据集和 OpenSurfaces 数据集上进行了实验，结果显示 MatSpectNet 比最近的出版物高出 1.60% 的平均像素精度和 3.42% 的 Mean Class 精度。请注意，这里的 Simplified Chinese 是使用的 Traditional Chinese 的字体。如果您需要使用简化字体，请将字体设置为简化字体。
</details></li>
</ul>
<hr>
<h2 id="BLISS-Interplanetary-Exploration-with-Swarms-of-Low-Cost-Spacecraft"><a href="#BLISS-Interplanetary-Exploration-with-Swarms-of-Low-Cost-Spacecraft" class="headerlink" title="BLISS: Interplanetary Exploration with Swarms of Low-Cost Spacecraft"></a>BLISS: Interplanetary Exploration with Swarms of Low-Cost Spacecraft</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11226">http://arxiv.org/abs/2307.11226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander N. Alvara, Lydia Lee, Emmanuel Sin, Nathan Lambert, Andrew J. Westphal, Kristofer S. J. Pister</li>
<li>for: 这个研究是为了设计一个低成本、自动化的小型太阳帆系统，用于探索地球系统内部的太阳系天体。</li>
<li>methods: 这个研究使用了微型技术，包括微型电子机械系统（MEMS）inchworm actuators，来控制一个约10g的太阳帆空间船。</li>
<li>results: 这个研究详细介绍了对近地小行星101955尼杜（Bennu）的轨道控制，以及太阳帆的低阶控制和船上的通信和计算器的规格。 该研究还简单介绍了对数十个黄道族彗星的样本返回和星际彗星会面和摄影的两个应用。<details>
<summary>Abstract</summary>
Leveraging advancements in micro-scale technology, we propose a fleet of autonomous, low-cost, small solar sails for interplanetary exploration. The Berkeley Low-cost Interplanetary Solar Sail (BLISS) project aims to utilize small-scale technologies to create a fleet of tiny interplanetary femto-spacecraft for rapid, low-cost exploration of the inner solar system. This paper describes the hardware required to build a nearly 10 g spacecraft using a 1 m$^2$ solar sail steered by micro-electromechanical systems (MEMS) inchworm actuators. The trajectory control to a NEO, here 101955 Bennu, is detailed along with the low-level actuation control of the solar sail and the specifications of proposed onboard communication and computation. Two other applications are also shortly considered: sample return from dozens of Jupiter-family comets and interstellar comet rendezvous and imaging. The paper concludes by discussing the fundamental scaling limits and future directions for steerable autonomous miniature solar sails with onboard custom computers and sensors.
</details>
<details>
<summary>摘要</summary>
利用微型技术的进步，我们提议使用自动化、低成本、小型太阳帆进行行星探测。本 projet 称为BERKELEY低成本内太阳系太阳帆（BLISS）项目，旨在使用小规模技术创造一支小型内太阳系 femto-空间船，用于快速、低成本地探索内太阳系。本文描述了建立一个约10g空间船使用1米²太阳帆，由微型电子机械系统（MEMS）彩虹舌 actuators控制的硬件。另外，文章还详细介绍了对 Near-Earth Objects（NEO）101955 Benedeto的轨迹控制，以及太阳帆的低级控制和提议的 bordo 通信和计算机器。文章还 briefly consideration两个其他应用：从多个Jupiter家族彗星中返回样本，以及遥感和拍摄间隔彗星。文章结束时，讨论了普适缩放限制和未来方向，以及随时间变化的自适应控制和感知技术。
</details></li>
</ul>
<hr>
<h2 id="Sparse-electrophysiological-source-imaging-predicts-aging-related-gait-speed-slowing"><a href="#Sparse-electrophysiological-source-imaging-predicts-aging-related-gait-speed-slowing" class="headerlink" title="Sparse electrophysiological source imaging predicts aging-related gait speed slowing"></a>Sparse electrophysiological source imaging predicts aging-related gait speed slowing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11273">http://arxiv.org/abs/2307.11273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vega-Hernandez, Mayrim, Galan-Garcia, Lidice, Perez-Hidalgo-Gato, Jhoanna, Ontivero-Ortega, Marlis, Garcia-Agustin, Daysi, Garcia-Reyes, Ronaldo, Bosch-Bayard, Jorge, Marinazzo, Daniele, Martinez-Montes, Eduardo, Valdes-Sosa, Pedro A<br>for:This paper aims to identify stable electrophysiological source imaging (ESI) biomarkers that are associated with gait speed (GS) decline in aging individuals.methods:The authors use flexible sparse&#x2F;smooth&#x2F;non-negative models (NN-SLASSO) to estimate ESI, and select activation ESI (aESI) and connectivity ESI (cESI) features using the Stable Sparse Classifier method.results:The authors find that novel sparse aESI models outperform traditional methods such as the LORETA family, and that combining aESI and cESI features improves the predictability of GS changes. The selected biomarkers are localized to orbitofrontal and temporal cortical regions.<details>
<summary>Abstract</summary>
Objective: We seek stable Electrophysiological Source Imaging (ESI) biomarkers associated with Gait Speed (GS) as a measure of functional decline. Towards this end we determine the predictive value of ESI activation and connectivity patterns of resting-state EEG Theta rhythm on physical performance decline measured by a slowing GS in aging individuals. Methods: As potential biomarkers related to GS changes, we estimate ESI using flexible sparse/smooth/non-negative models (NN-SLASSO), from which activation ESI (aESI) and connectivity ESI (cESI) features are selected using the Stable Sparse Classifier method. Results and Conclusions: Novel sparse aESI models outperformed traditional methods such as the LORETA family. The models combining aESI and cESI features improved the predictability of GS changes. Selected biomarkers from activation/connectivity patterns were localized to orbitofrontal and temporal cortical regions. Significance: The proposed methodology contributes to understanding the activation and connectivity of ESI complex patterns related to GS, providing potential biomarker features for GS slowing. Given the known relationship between GS decline and cognitive impairment, this preliminary work suggests it might be applied to other, more complex measures of healthy and pathological aging. Importantly, it might allow an ESI-based evaluation of rehabilitation programs.
</details>
<details>
<summary>摘要</summary>
Methods: 作为可能的GS变化的征象，我们使用自适应的简单/平滑/非负模型（NN-SLASSO）估计ESI，从中选择 activation ESI（aESI）和连接 ESI（cESI）特征使用稳定的简单类别方法（Stable Sparse Classifier）。Results and Conclusions: 我们发现了一些新的简单aESI模型，比LORETA家族更高度预测GS变化。将aESI和cESI特征组合使用可以提高GS变化的预测性。选择的征象从活化/连接组合中的localized to orbitofrontal和temporal cortical region。Significance: 我们的方法可以帮助理解ESI复杂的活化和连接组合，并提供了可能的GS slowing征象。由于知道GS slowing和认知障碍之间的关系，这个初步的工作可能可以应用于其他更复杂的健康和疾病年轻人群。此外，这种方法可能可以评估rehabilitation程序的效果。
</details></li>
</ul>
<hr>
<h2 id="Treatment-And-Follow-Up-Guidelines-For-Multiple-Brain-Metastases-A-Systematic-Review"><a href="#Treatment-And-Follow-Up-Guidelines-For-Multiple-Brain-Metastases-A-Systematic-Review" class="headerlink" title="Treatment And Follow-Up Guidelines For Multiple Brain Metastases: A Systematic Review"></a>Treatment And Follow-Up Guidelines For Multiple Brain Metastases: A Systematic Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11016">http://arxiv.org/abs/2307.11016</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ana Sofia Santos, Victor Alves, José Soares, Matheus Silva, Crystian Saraiva</li>
<li>for: 这篇论文主要是为了解决多个脑 метаstatic tumor的管理问题，尤其是在考虑各种个人因素时。</li>
<li>methods: 该论文使用了stereotactic radiosurgery（SRS）来管理多个脑 метаstatic tumor，以保持质量生活和神经保存。</li>
<li>results: 该论文提出了一种使用人工智能（AI）预测新的脑 метаstatic tumor的出现方法，以帮助医生在后期治疗中做出最佳决策。<details>
<summary>Abstract</summary>
Brain metastases are a complication of primary cancer, representing the most common type of brain tumor in adults. The management of multiple brain metastases represents a clinical challenge worldwide in finding the optimal treatment for patients considering various individual aspects. Managing multiple metastases with stereotactic radiosurgery (SRS) is being increasingly used because of quality of life and neurocognitive preservation, which do not present such good outcomes when dealt with whole brain radiation therapy (WBRT). After treatment, analyzing the progression of the disease still represents a clinical issue, since it is difficult to determine a standard schedule for image acquisition. A solution could be the applying artificial intelligence, namely predictive models to forecast the incidence of new metastases in post-treatment images. Although there aren't many works on this subject, this could potentially bennefit medical professionals in early decision of the best treatment approaches.
</details>
<details>
<summary>摘要</summary>
脑部 метаstatic 是primary cancer的一种合并症状， represents the most common type of brain tumor in adults。Managing multiple metastases with stereotactic radiosurgery (SRS) is being increasingly used because of quality of life and neurocognitive preservation, which do not present such good outcomes when dealt with whole brain radiation therapy (WBRT)。After treatment, analyzing the progression of the disease still represents a clinical issue, since it is difficult to determine a standard schedule for image acquisition。A solution could be the applying artificial intelligence, namely predictive models to forecast the incidence of new metastases in post-treatment images。Although there aren't many works on this subject, this could potentially benefit medical professionals in early decision of the best treatment approaches。Note: Please keep in mind that the translation is in Simplified Chinese, which is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="Frequency-aware-optical-coherence-tomography-image-super-resolution-via-conditional-generative-adversarial-neural-network"><a href="#Frequency-aware-optical-coherence-tomography-image-super-resolution-via-conditional-generative-adversarial-neural-network" class="headerlink" title="Frequency-aware optical coherence tomography image super-resolution via conditional generative adversarial neural network"></a>Frequency-aware optical coherence tomography image super-resolution via conditional generative adversarial neural network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11130">http://arxiv.org/abs/2307.11130</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueshen Li, Zhenxing Dong, Hongshan Liu, Jennifer J. Kang-Mieler, Yuye Ling, Yu Gan</li>
<li>for: 用于提高医学图像诊断和治疗的能力，特别是在cardiology和ophthalmology领域。</li>
<li>methods:  integrate three critical frequency-based modules (i.e., frequency transformation, frequency skip connection, and frequency alignment) and frequency-based loss function into a conditional generative adversarial network (cGAN).</li>
<li>results: 超过现有深度学习框架，能够更好地重构形态结构。并且通过应用于鱼眼和小鼠眼图像，证明其可以在眼科图像重构中提高形态细节。<details>
<summary>Abstract</summary>
Optical coherence tomography (OCT) has stimulated a wide range of medical image-based diagnosis and treatment in fields such as cardiology and ophthalmology. Such applications can be further facilitated by deep learning-based super-resolution technology, which improves the capability of resolving morphological structures. However, existing deep learning-based method only focuses on spatial distribution and disregard frequency fidelity in image reconstruction, leading to a frequency bias. To overcome this limitation, we propose a frequency-aware super-resolution framework that integrates three critical frequency-based modules (i.e., frequency transformation, frequency skip connection, and frequency alignment) and frequency-based loss function into a conditional generative adversarial network (cGAN). We conducted a large-scale quantitative study from an existing coronary OCT dataset to demonstrate the superiority of our proposed framework over existing deep learning frameworks. In addition, we confirmed the generalizability of our framework by applying it to fish corneal images and rat retinal images, demonstrating its capability to super-resolve morphological details in eye imaging.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Deep-Spiking-UNet-for-Image-Processing"><a href="#Deep-Spiking-UNet-for-Image-Processing" class="headerlink" title="Deep Spiking-UNet for Image Processing"></a>Deep Spiking-UNet for Image Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10974">http://arxiv.org/abs/2307.10974</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/snnresearch/spiking-unet">https://github.com/snnresearch/spiking-unet</a></li>
<li>paper_authors: Hebei Li, Yueyi Zhang, Zhiwei Xiong, Zheng-jun Zha, Xiaoyan Sun</li>
<li>for: 这个论文旨在探讨使用神经网络（SNN）进行图像处理任务，特别是在 neuromorphic 芯片上部署。</li>
<li>methods: 该论文提出了一种新的 Spiking-UNet 模型，将 U-Net 架构与神经网络（SNN）结合，以提高图像处理任务的效能。</li>
<li>results: 实验结果显示，Compared with non-spiking counterpart, our Spiking-UNet achieves comparable performance on image segmentation and denoising tasks, and reduces inference time by approximately 90%.<details>
<summary>Abstract</summary>
U-Net, known for its simple yet efficient architecture, is widely utilized for image processing tasks and is particularly suitable for deployment on neuromorphic chips. This paper introduces the novel concept of Spiking-UNet for image processing, which combines the power of Spiking Neural Networks (SNNs) with the U-Net architecture. To achieve an efficient Spiking-UNet, we face two primary challenges: ensuring high-fidelity information propagation through the network via spikes and formulating an effective training strategy. To address the issue of information loss, we introduce multi-threshold spiking neurons, which improve the efficiency of information transmission within the Spiking-UNet. For the training strategy, we adopt a conversion and fine-tuning pipeline that leverage pre-trained U-Net models. During the conversion process, significant variability in data distribution across different parts is observed when utilizing skip connections. Therefore, we propose a connection-wise normalization method to prevent inaccurate firing rates. Furthermore, we adopt a flow-based training method to fine-tune the converted models, reducing time steps while preserving performance. Experimental results show that, on image segmentation and denoising, our Spiking-UNet achieves comparable performance to its non-spiking counterpart, surpassing existing SNN methods. Compared with the converted Spiking-UNet without fine-tuning, our Spiking-UNet reduces inference time by approximately 90\%. This research broadens the application scope of SNNs in image processing and is expected to inspire further exploration in the field of neuromorphic engineering. The code for our Spiking-UNet implementation is available at https://github.com/SNNresearch/Spiking-UNet.
</details>
<details>
<summary>摘要</summary>
优等网络（U-Net），知名的简单 yet efficient 架构，广泛应用于图像处理任务，特别适合部署在 neuromorphic 芯片上。本文介绍了一种新的激发式 U-Net 图像处理方法，将激发式神经网络（SNNs）与 U-Net 架构结合。为实现高效的激发式 U-Net，我们面临两个主要挑战：确保信息在网络中传递高精度信息和开发有效的训练策略。为了解决信息损失问题，我们提出了多reshold spiking neurons，可以提高激发式 U-Net 中信息传输的效率。在训练策略方面，我们采用了一个转换和细化pipeline，利用预训练的 U-Net 模型。在转换过程中，我们发现了不同部分的数据分布具有显著的变化，因此我们提出了一种 Connection-wise 正规化方法，以避免假正常的发射率。此外，我们采用了一种流式训练方法，以便在减少时间步价 while preserving performance。实验结果表明，我们的激发式 U-Net 在图像分割和减震方面具有相当的表现，比其非激发式对应器更高。与 conversed Spiking-UNet  без fine-tuning 相比，我们的 Spiking-UNet 减少了推理时间约 90%。这种研究扩展了 SNNs 在图像处理领域的应用范围，并预计会鼓励更多关于 neuromorphic 工程的探索。Spiking-UNet 实现代码可以在 GitHub 上找到：https://github.com/SNNresearch/Spiking-UNet。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/07/21/eess.IV_2023_07_21/" data-id="cllt9pry0008gol884tfped6w" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/21/eess.AS_2023_07_21/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.AS - 2023-07-21 22:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/20/cs.LG_2023_07_20/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-07-20 18:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
