
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-07-22 18:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="A Revolution of Personalized Healthcare: Enabling Human Digital Twin with Mobile AIGC paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.12115 repo_url: None paper_authors: Jiayuan Chen, Changyan Yi, Hongyang Du, D">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-07-22 18:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/07/22/cs.LG_2023_07_22/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="A Revolution of Personalized Healthcare: Enabling Human Digital Twin with Mobile AIGC paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.12115 repo_url: None paper_authors: Jiayuan Chen, Changyan Yi, Hongyang Du, D">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-21T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:19.187Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_07_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/22/cs.LG_2023_07_22/" class="article-date">
  <time datetime="2023-07-21T16:00:00.000Z" itemprop="datePublished">2023-07-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-07-22 18:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Revolution-of-Personalized-Healthcare-Enabling-Human-Digital-Twin-with-Mobile-AIGC"><a href="#A-Revolution-of-Personalized-Healthcare-Enabling-Human-Digital-Twin-with-Mobile-AIGC" class="headerlink" title="A Revolution of Personalized Healthcare: Enabling Human Digital Twin with Mobile AIGC"></a>A Revolution of Personalized Healthcare: Enabling Human Digital Twin with Mobile AIGC</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12115">http://arxiv.org/abs/2307.12115</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayuan Chen, Changyan Yi, Hongyang Du, Dusit Niyato, Jiawen Kang, Jun Cai, Xuemin, Shen</li>
<li>for: 本研究旨在探讨移动 искусственный智能生成内容（AIGC）技术在人工智能驱动人工对应（HDT）领域的应用。</li>
<li>methods: 本文提出了移动AIGC驱动HDT的系统架构，并详细介绍了相关的设计要求和挑战。</li>
<li>results: 经过实验研究，移动AIGC驱动HDT解决方案显示出在虚拟物理治疗教学平台中的效果。<details>
<summary>Abstract</summary>
Mobile Artificial Intelligence-Generated Content (AIGC) technology refers to the adoption of AI algorithms deployed at mobile edge networks to automate the information creation process while fulfilling the requirements of end users. Mobile AIGC has recently attracted phenomenal attentions and can be a key enabling technology for an emerging application, called human digital twin (HDT). HDT empowered by the mobile AIGC is expected to revolutionize the personalized healthcare by generating rare disease data, modeling high-fidelity digital twin, building versatile testbeds, and providing 24/7 customized medical services. To promote the development of this new breed of paradigm, in this article, we propose a system architecture of mobile AIGC-driven HDT and highlight the corresponding design requirements and challenges. Moreover, we illustrate two use cases, i.e., mobile AIGC-driven HDT in customized surgery planning and personalized medication. In addition, we conduct an experimental study to prove the effectiveness of the proposed mobile AIGC-driven HDT solution, which shows a particular application in a virtual physical therapy teaching platform. Finally, we conclude this article by briefly discussing several open issues and future directions.
</details>
<details>
<summary>摘要</summary>
移动人工智能生成内容（AIGC）技术指的是在移动边缘网络上部署AI算法，以自动生成信息，同时满足用户需求。移动AIGC在最近吸引了非常多的关注，可以成为人类数字双（HDT）的关键启用技术。HDT通过移动AIGC得到强化，预计将改变个性化医疗，生成罕见疾病数据，建立高准确度数字双，创建多样化测试床，提供24/7个性化医疗服务。为推动这种新的 paradigma 的发展，本文提出了移动AIGC驱动HDT的系统架构，并 highlight了相应的设计要求和挑战。此外，我们还提出了两个使用情景，即移动AIGC驱动HDT在自定义手术规划和个性化药物。此外，我们进行了实验研究，证明了我们提议的移动AIGC驱动HDT解决方案的效果，其在虚拟物理治疗教学平台中有特定的应用。最后，我们 briefly discuss了一些开放问题和未来方向。
</details></li>
</ul>
<hr>
<h2 id="A-Zero-shot-and-Few-shot-Study-of-Instruction-Finetuned-Large-Language-Models-Applied-to-Clinical-and-Biomedical-Tasks"><a href="#A-Zero-shot-and-Few-shot-Study-of-Instruction-Finetuned-Large-Language-Models-Applied-to-Clinical-and-Biomedical-Tasks" class="headerlink" title="A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks"></a>A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12114">http://arxiv.org/abs/2307.12114</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanis Labrak, Mickael Rouvier, Richard Dufour</li>
<li>for: 这些论文是为了评估四种现状最佳大语言模型（LLMs）在临床和生物医学自然语言处理（NLP）任务中的性能。</li>
<li>methods: 这些论文使用了四种实际应用中最佳的 instruction-tuned LLMs，包括 ChatGPT、Flan-T5 UL2、Tk-Instruct 和 Alpaca，在英语的13种实际临床和生物医学 NLP 任务上进行了评估。</li>
<li>results: 结果表明，评估的 LLMs 在零和几个采样enario下对大多数任务的性能已经接近了状态之前模型的性能，尤其是在问答任务上表现非常出色，即使它们没有在这些任务上看到过示例。但是，我们发现了分类和关系抽取任务的性能较低，相比特别训练的医疗领域模型，如 PubMedBERT。此外，我们注意到没有任何 LLM 在所有任务上超过其他模型的性能，一些模型更适合某些任务。<details>
<summary>Abstract</summary>
We evaluate four state-of-the-art instruction-tuned large language models (LLMs) -- ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca -- on a set of 13 real-world clinical and biomedical natural language processing (NLP) tasks in English, such as named-entity recognition (NER), question-answering (QA), relation extraction (RE), etc. Our overall results demonstrate that the evaluated LLMs begin to approach performance of state-of-the-art models in zero- and few-shot scenarios for most tasks, and particularly well for the QA task, even though they have never seen examples from these tasks before. However, we observed that the classification and RE tasks perform below what can be achieved with a specifically trained model for the medical field, such as PubMedBERT. Finally, we noted that no LLM outperforms all the others on all the studied tasks, with some models being better suited for certain tasks than others.
</details>
<details>
<summary>摘要</summary>
我们评估了四种现场最佳大语言模型（LLM）——ChatGPT、Flan-T5 UL2、Tk-Instruct和Alpaca——在英语的13种实际医疗和生物医学自然语言处理（NLP）任务上，如命名实体识别（NER）、问答（QA）、关系提取（RE）等。我们的总结结果表明，评估的LLMs在零或几次示例enario下对大多数任务的性能逐渐接近了现场最佳模型的水平，尤其是在QA任务上表现特别好，即使它们没有在这些任务上看到过示例。然而，我们发现了分类和RE任务的性能比专门为医学领域训练的模型，如PubMedBERT，还下。最后，我们注意到没有LLM能够在所有任务上表现更好，一些模型更适合某些任务。
</details></li>
</ul>
<hr>
<h2 id="Active-Control-of-Flow-over-Rotating-Cylinder-by-Multiple-Jets-using-Deep-Reinforcement-Learning"><a href="#Active-Control-of-Flow-over-Rotating-Cylinder-by-Multiple-Jets-using-Deep-Reinforcement-Learning" class="headerlink" title="Active Control of Flow over Rotating Cylinder by Multiple Jets using Deep Reinforcement Learning"></a>Active Control of Flow over Rotating Cylinder by Multiple Jets using Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12083">http://arxiv.org/abs/2307.12083</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kamyar Dobakhti, Jafar Ghazanfarian</li>
<li>For: This paper aims to optimize the use of rotation and deep reinforcement learning (DRL) for drag reduction on blunt bodies.* Methods: The paper uses multiple controlled jets and a DRL algorithm to reach maximum possible drag suppression. The paper also explores the optimization of the number and positions of the jets, sensor location, and maximum allowed flow rate.* Results: The combination of rotation and DRL reduces the drag coefficient by up to 49.75%, and the agent can keep the lift coefficient at a value near zero or stabilize it at a smaller number. Additionally, the paper finds that having more sensors at more locations is not always beneficial and should be determined based on the need of the user and corresponding configuration.<details>
<summary>Abstract</summary>
The real power of artificial intelligence appears in reinforcement learning, which is computationally and physically more sophisticated due to its dynamic nature. Rotation and injection are some of the proven ways in active flow control for drag reduction on blunt bodies. In this paper, rotation will be added to the cylinder alongside the deep reinforcement learning (DRL) algorithm, which uses multiple controlled jets to reach the maximum possible drag suppression. Characteristics of the DRL code, including controlling parameters, their limitations, and optimization of the DRL network for use with rotation will be presented. This work will focus on optimizing the number and positions of the jets, the sensors location, and the maximum allowed flow rate to jets in the form of the maximum allowed flow rate of each actuation and the total number of them per episode. It is found that combining the rotation and DRL is promising since it suppresses the vortex shedding, stabilizes the Karman vortex street, and reduces the drag coefficient by up to 49.75%. Also, it will be shown that having more sensors at more locations is not always a good choice and the sensor number and location should be determined based on the need of the user and corresponding configuration. Also, allowing the agent to have access to higher flow rates, mostly reduces the performance, except when the cylinder rotates. In all cases, the agent can keep the lift coefficient at a value near zero, or stabilize it at a smaller number.
</details>
<details>
<summary>摘要</summary>
真正的人工智能能量在回归学习中表现出真正的力量，因为它的动态性使得计算和物理上更加复杂。在活流控制中，旋转和注射是已知的方法来降低碰撞体的阻力。在这篇论文中，我们将在圆柱体上添加旋转，并使用深度回归学习（DRL）算法，使用多个控制的气流来达到最大可能的阻力减少。我们将介绍DRL代码中控制参数的限制和优化，以及在旋转下使用DRL网络的优化。我们发现，将旋转和DRL相结合，能够有效地降低阻力系数，最高达49.75%。此外，我们还发现，在不同的配置下，有更多的感测器并不总是有利，感测器的数量和位置应该根据用户的需求进行定制。同时，允许代理人访问更高的流速，通常不会提高性能，除非圆柱体在旋转。在所有情况下，代理人都能保持升力系数在零附近，或者稳定其为较小的数字。
</details></li>
</ul>
<hr>
<h2 id="Spectral-Normalized-Cut-Graph-Partitioning-with-Fairness-Constraints"><a href="#Spectral-Normalized-Cut-Graph-Partitioning-with-Fairness-Constraints" class="headerlink" title="Spectral Normalized-Cut Graph Partitioning with Fairness Constraints"></a>Spectral Normalized-Cut Graph Partitioning with Fairness Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12065">http://arxiv.org/abs/2307.12065</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiali2000/fnm">https://github.com/jiali2000/fnm</a></li>
<li>paper_authors: Jia Li, Yanhao Wang, Arpit Merchant</li>
<li>for: 这个论文的目的是提出一种基于分类敏感特征的图分区算法，以确保图中每个群组中的分布相对均衡，同时最小化正规化距离值。</li>
<li>methods: 本文提出的方法包括一个两阶段spectral算法，首先添加了一个基于公平性 criterion的扩展拉格朗日函数，然后在第二阶段使用一种轮换方案来从公平embedding中生成$k$个群组。</li>
<li>results: 经过对九个 benchmark dataset的实验表明，FNM方法与三个基eline方法相比，在公平性和分区质量之间能够更好地平衡。<details>
<summary>Abstract</summary>
Normalized-cut graph partitioning aims to divide the set of nodes in a graph into $k$ disjoint clusters to minimize the fraction of the total edges between any cluster and all other clusters. In this paper, we consider a fair variant of the partitioning problem wherein nodes are characterized by a categorical sensitive attribute (e.g., gender or race) indicating membership to different demographic groups. Our goal is to ensure that each group is approximately proportionally represented in each cluster while minimizing the normalized cut value. To resolve this problem, we propose a two-phase spectral algorithm called FNM. In the first phase, we add an augmented Lagrangian term based on our fairness criteria to the objective function for obtaining a fairer spectral node embedding. Then, in the second phase, we design a rounding scheme to produce $k$ clusters from the fair embedding that effectively trades off fairness and partition quality. Through comprehensive experiments on nine benchmark datasets, we demonstrate the superior performance of FNM compared with three baseline methods.
</details>
<details>
<summary>摘要</summary>
通常化 cut 图分割目标是将图中的节点集分成 $k$ 个不 overlap 的集群，以最小化任何两个集群之间的总边数。在这篇论文中，我们考虑了一种公平的变种图分割问题，其中节点被 categorical 敏感特征（例如性别或种族）分类为不同的社会组别。我们的目标是确保每个组别在每个集群中都有约等的表现，同时最小化 normalized cut 值。为解决这个问题，我们提出了一种两个阶段的spectral算法 called FNM。在第一阶段，我们添加了基于我们的公平准则的扩展拉格朗日函数到目标函数中，以获得更公平的spectral节点嵌入。然后，在第二阶段，我们设计了一种轮换方案，以生成 $k$ 个集群从公平嵌入中，并且有效地考虑了公平和分割质量之间的衡量。通过对九个基准数据集进行了广泛的实验，我们展示了 FNM 的超过基准方法的性能。
</details></li>
</ul>
<hr>
<h2 id="Balancing-Exploration-and-Exploitation-in-Hierarchical-Reinforcement-Learning-via-Latent-Landmark-Graphs"><a href="#Balancing-Exploration-and-Exploitation-in-Hierarchical-Reinforcement-Learning-via-Latent-Landmark-Graphs" class="headerlink" title="Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs"></a>Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12063">http://arxiv.org/abs/2307.12063</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/papercode2022/hill">https://github.com/papercode2022/hill</a></li>
<li>paper_authors: Qingyang Zhang, Yiming Yang, Jingqing Ruan, Xuantang Xiong, Dengpeng Xing, Bo Xu</li>
<li>for: This paper aims to address the exploration-exploitation dilemma in reinforcement learning by proposing a hierarchical reinforcement learning method called HILL, which learns latent subgoal representations that satisfy temporal coherence and dynamically builds latent landmark graphs to balance exploration and exploitation.</li>
<li>methods: The HILL method uses a contrastive representation learning objective to learn latent subgoal representations, and employs a novelty measure on nodes and a utility measure on edges to balance exploration and exploitation.</li>
<li>results: The experimental results show that HILL outperforms state-of-the-art baselines on continuous control tasks with sparse rewards in sample efficiency and asymptotic performance.Here’s the Chinese translation of the three key information points:</li>
<li>for: 这篇论文目标是解决 reinforcement learning 中的冒险奖励矛盾，提出一种层次游戏学习方法 called HILL，该方法学习具有时间准确性的隐藏目标表示。</li>
<li>methods: HILL 方法使用对比表示学习目标来学习隐藏目标表示，并使用节点新鲜度和边利用度来均衡冒险和利用。</li>
<li>results: 实验结果显示，HILL 方法在缺乏奖励的连续控制任务上超过了现状权限的基线。<details>
<summary>Abstract</summary>
Goal-Conditioned Hierarchical Reinforcement Learning (GCHRL) is a promising paradigm to address the exploration-exploitation dilemma in reinforcement learning. It decomposes the source task into subgoal conditional subtasks and conducts exploration and exploitation in the subgoal space. The effectiveness of GCHRL heavily relies on subgoal representation functions and subgoal selection strategy. However, existing works often overlook the temporal coherence in GCHRL when learning latent subgoal representations and lack an efficient subgoal selection strategy that balances exploration and exploitation. This paper proposes HIerarchical reinforcement learning via dynamically building Latent Landmark graphs (HILL) to overcome these limitations. HILL learns latent subgoal representations that satisfy temporal coherence using a contrastive representation learning objective. Based on these representations, HILL dynamically builds latent landmark graphs and employs a novelty measure on nodes and a utility measure on edges. Finally, HILL develops a subgoal selection strategy that balances exploration and exploitation by jointly considering both measures. Experimental results demonstrate that HILL outperforms state-of-the-art baselines on continuous control tasks with sparse rewards in sample efficiency and asymptotic performance. Our code is available at https://github.com/papercode2022/HILL.
</details>
<details>
<summary>摘要</summary>
“对于受挑战的问题，Goal-Conditioned Hierarchical Reinforcement Learning（GCHRL）是一种有前途的方法。它将源任务分解为子目标对应的子任务，并在子任务空间进行探索和实现。然而，现有的作品往往忽略GCHRL中的时间一致性，对于学习隐藏的子目标表示而言，没有有效的时间一致性评估方法。这篇论文提出了HIerarchical reinforcement learning via dynamically building Latent Landmark graphs（HILL），以解决这些限制。HILL使用了一个对称的描述学习目标，以learn隐藏的子目标表示，并且 dynamically builds latent landmark graphs，使用了一个新的novelty measure和一个utility measure。最后，HILL发展了一个具有平衡探索和实现的子目标选择策略。实验结果显示，HILL在缺乏对象奖励的连续控制任务上，在样本效率和长期性方面，与现有的基eline相比，表现出色。我们的代码可以在https://github.com/papercode2022/HILL上获取。”
</details></li>
</ul>
<hr>
<h2 id="Game-Theoretic-Robust-Reinforcement-Learning-Handles-Temporally-Coupled-Perturbations"><a href="#Game-Theoretic-Robust-Reinforcement-Learning-Handles-Temporally-Coupled-Perturbations" class="headerlink" title="Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations"></a>Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12062">http://arxiv.org/abs/2307.12062</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongyuan Liang, Yanchao Sun, Ruijie Zheng, Xiangyu Liu, Tuomas Sandholm, Furong Huang, Stephen McAleer</li>
<li>for:  Robust reinforcement learning (RL) seeks to train policies that can perform well under environment perturbations or adversarial attacks.</li>
<li>methods: GRAD, a novel game-theoretic approach that treats the temporally-coupled robust RL problem as a partially-observable two-player zero-sum game.</li>
<li>results: The proposed approach exhibits significant robustness advantages compared to baselines against both standard and temporally-coupled attacks, in both state and action spaces.Here’s the text in Simplified Chinese:</li>
<li>for:  Robust reinforcement learning (RL) 实现训练策略，能够在环境干扰或敌方攻击下表现良好。</li>
<li>methods: GRAD，一种新的游戏理论方法，将当前时间步骤中的干扰视为一个部分可观察的零余游戏。</li>
<li>results: 提案的方法在多个连续控制任务上展现了与基准相比的显著强健优势，包括标准和时间相依的干扰攻击。<details>
<summary>Abstract</summary>
Robust reinforcement learning (RL) seeks to train policies that can perform well under environment perturbations or adversarial attacks. Existing approaches typically assume that the space of possible perturbations remains the same across timesteps. However, in many settings, the space of possible perturbations at a given timestep depends on past perturbations. We formally introduce temporally-coupled perturbations, presenting a novel challenge for existing robust RL methods. To tackle this challenge, we propose GRAD, a novel game-theoretic approach that treats the temporally-coupled robust RL problem as a partially-observable two-player zero-sum game. By finding an approximate equilibrium in this game, GRAD ensures the agent's robustness against temporally-coupled perturbations. Empirical experiments on a variety of continuous control tasks demonstrate that our proposed approach exhibits significant robustness advantages compared to baselines against both standard and temporally-coupled attacks, in both state and action spaces.
</details>
<details>
<summary>摘要</summary>
Robust reinforcement learning（RL）寻求训练策略，以便在环境干扰或敌意攻击时表现出色。现有方法通常假设每个时间步骤的可能的干扰空间保持不变。然而，在许多情况下，每个时间步骤的可能的干扰空间取决于过去的干扰。我们正式引入了时间相关的干扰，并提出了一种新的挑战 для现有的robust RL方法。为解决这个挑战，我们提议GRAD，一种基于游戏理论的新方法，将时间相关的Robust RL问题视为一个部分可见的两个玩家零加游戏。通过在这个游戏中找到一个近似平衡，GRAD确保了机器人的对时间相关干扰的 Robustness。empirical experiments表明，我们的提议方法在许多连续控制任务上具有显著的Robustness优势，比基准方法更能抵抗标准和时间相关的攻击，包括状态和动作空间。
</details></li>
</ul>
<hr>
<h2 id="Fast-Knowledge-Graph-Completion-using-Graphics-Processing-Units"><a href="#Fast-Knowledge-Graph-Completion-using-Graphics-Processing-Units" class="headerlink" title="Fast Knowledge Graph Completion using Graphics Processing Units"></a>Fast Knowledge Graph Completion using Graphics Processing Units</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12059">http://arxiv.org/abs/2307.12059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chun-Hee Lee, Dong-oh Kang, Hwa Jeon Song</li>
<li>for: 提高知识图的完善性，增加新的关系</li>
<li>methods: 使用GPU加速知识图完善框架，将知识图完善问题转化为相似Join问题，并使用度量空间的性质 derivate关键公式，实现快速的知识图完善</li>
<li>results: 实验表明，提出的框架可以高效处理知识图完善问题<details>
<summary>Abstract</summary>
Knowledge graphs can be used in many areas related to data semantics such as question-answering systems, knowledge based systems. However, the currently constructed knowledge graphs need to be complemented for better knowledge in terms of relations. It is called knowledge graph completion. To add new relations to the existing knowledge graph by using knowledge graph embedding models, we have to evaluate $N\times N \times R$ vector operations, where $N$ is the number of entities and $R$ is the number of relation types. It is very costly.   In this paper, we provide an efficient knowledge graph completion framework on GPUs to get new relations using knowledge graph embedding vectors. In the proposed framework, we first define "transformable to a metric space" and then provide a method to transform the knowledge graph completion problem into the similarity join problem for a model which is "transformable to a metric space". After that, to efficiently process the similarity join problem, we derive formulas using the properties of a metric space. Based on the formulas, we develop a fast knowledge graph completion algorithm. Finally, we experimentally show that our framework can efficiently process the knowledge graph completion problem.
</details>
<details>
<summary>摘要</summary>
知识图可以在数据 semantics 中的多个领域应用，如问答系统、知识基于系统。然而，目前构建的知识图需要进一步补充以获得更好的知识，这被称为知识图完成。为在现有知识图中添加新关系，我们需要评估 $N\times N \times R$ 矩阵操作，其中 $N$ 是实体的数量，$R$ 是关系类型的数量。这是非常昂贵的。在这篇论文中，我们提供了一种高效的知识图完成框架，使用 GPU 进行加速。我们首先定义 "可转换到度量空间"，然后将知识图完成问题转换成度量空间中的相似Join问题。然后，我们使用度量空间的性质 derive  formulas，并根据这些方程开发了一种快速的知识图完成算法。最后，我们通过实验证明了我们的框架可以高效处理知识图完成问题。
</details></li>
</ul>
<hr>
<h2 id="Exploring-MLOps-Dynamics-An-Experimental-Analysis-in-a-Real-World-Machine-Learning-Project"><a href="#Exploring-MLOps-Dynamics-An-Experimental-Analysis-in-a-Real-World-Machine-Learning-Project" class="headerlink" title="Exploring MLOps Dynamics: An Experimental Analysis in a Real-World Machine Learning Project"></a>Exploring MLOps Dynamics: An Experimental Analysis in a Real-World Machine Learning Project</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13473">http://arxiv.org/abs/2307.13473</a></li>
<li>repo_url: None</li>
<li>paper_authors: Awadelrahman M. A. Ahmed</li>
<li>For: This paper aims to optimize the MLOps process to enhance the efficiency and effectiveness of machine learning projects.* Methods: The paper employs a comprehensive MLOps workflow that covers essential phases like problem definition, data acquisition, data preparation, model development, model deployment, monitoring, management, scalability, and governance and compliance. The study also utilizes a systematic tracking approach to document revisits to specific phases and constructs a matrix to quantify the degree of overlap between phases.* Results: The paper provides valuable insights into the dynamic and iterative nature of the MLOps workflow, offering practical tips and recommendations for optimizing the workflow. The resulting data provides visual representations of the interdependencies and iterative characteristics of the MLOps process, contributing to enhancing the efficiency and effectiveness of machine learning projects.Here’s the simplified Chinese text for the three key points:* For: 这篇论文目标是优化机器学习操作（MLOps）过程，以提高机器学习项目的效率和效果。* Methods: 这篇论文采用了一个全面的 MLOps 工作流程，覆盖了重要的阶段，如问题定义、数据获取、数据准备、模型开发、模型部署、监控、管理、扩展性和合规性。研究还采用了系统的跟踪方法来记录特定阶段的重新访问，并构建了一个矩阵来衡量阶段之间的重叠度。* Results: 这篇论文提供了有价值的实践建议和技术指南，以便优化 MLOps 过程。研究还提供了可见的数据表示MLOps 过程的相互关系和迭代特性，为实际应用提供了有价值的指导。<details>
<summary>Abstract</summary>
This article presents an experiment focused on optimizing the MLOps (Machine Learning Operations) process, a crucial aspect of efficiently implementing machine learning projects. The objective is to identify patterns and insights to enhance the MLOps workflow, considering its iterative and interdependent nature in real-world model development scenarios.   The experiment involves a comprehensive MLOps workflow, covering essential phases like problem definition, data acquisition, data preparation, model development, model deployment, monitoring, management, scalability, and governance and compliance. Practical tips and recommendations are derived from the results, emphasizing proactive planning and continuous improvement for the MLOps workflow.   The experimental investigation was strategically integrated within a real-world ML project which followed essential phases of the MLOps process in a production environment, handling large-scale structured data. A systematic tracking approach was employed to document revisits to specific phases from a main phase under focus, capturing the reasons for such revisits. By constructing a matrix to quantify the degree of overlap between phases, the study unveils the dynamic and iterative nature of the MLOps workflow.   The resulting data provides visual representations of the MLOps process's interdependencies and iterative characteristics within the experimental framework, offering valuable insights for optimizing the workflow and making informed decisions in real-world scenarios. This analysis contributes to enhancing the efficiency and effectiveness of machine learning projects through an improved MLOps process.   Keywords: MLOps, Machine Learning Operations, Optimization, Experimental Analysis, Iterative Process, Pattern Identification.
</details>
<details>
<summary>摘要</summary>
本文描述了一项实验，旨在优化机器学习操作（MLOps）过程，这是实现机器学习项目的关键一环。实验的目标是识别MLOps工作流程中的模式和洞察，以提高实际场景中模型开发的MLOps工作流程。实验包括了MLOps工作流程的全面覆盖，从问题定义、数据收集、数据准备、模型开发、模型部署、监测、管理、可扩展性和合规性等主要阶段。实验结果提供了实践的建议和推荐，强调了积极的规划和不断改进，以提高MLOps工作流程的效率和效iveness。实验中使用了一个真实的机器学习项目，该项目遵循了MLOps工作流程的主要阶段，处理大规模结构化数据。通过系统地跟踪 revisits 到特定阶段，记录了 revisits 的原因，并由此构建了度量MLOps工作流程中各阶段之间的重叠度的矩阵。这种方法显示了MLOps工作流程的动态和融合特性。实验结果提供了 ML Ops 过程中各阶段之间的相互关联和迭代特性的可见表示，为实际场景中优化 ML Ops 过程提供了有价值的洞察。这种分析对于提高机器学习项目的效率和效iveness有很大的贡献。关键词：MLOps, 机器学习操作, 优化, 实验分析, 迭代过程, 模式识别.
</details></li>
</ul>
<hr>
<h2 id="Extracting-Molecular-Properties-from-Natural-Language-with-Multimodal-Contrastive-Learning"><a href="#Extracting-Molecular-Properties-from-Natural-Language-with-Multimodal-Contrastive-Learning" class="headerlink" title="Extracting Molecular Properties from Natural Language with Multimodal Contrastive Learning"></a>Extracting Molecular Properties from Natural Language with Multimodal Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12996">http://arxiv.org/abs/2307.12996</a></li>
<li>repo_url: None</li>
<li>paper_authors: Romain Lacombe, Andrew Gaut, Jeff He, David Lüdeke, Kateryna Pistunova</li>
<li>for: 本研究旨在将科学知识从文本中提取到分子图表示中。</li>
<li>methods: 研究使用对比学习将神经图表示与文本描述的特征进行对应，并使用神经相关性分配策略提高文本检索。此外，我们还提出了一种基于有机反应的新型分子图生成策略。</li>
<li>results: 研究表明，在下游分子网络Property Classification任务中，我们的模型表现出了+4.26% AUROC的提升和+1.54%的提升 compared to MoMu模型（Su et al. 2022）。<details>
<summary>Abstract</summary>
Deep learning in computational biochemistry has traditionally focused on molecular graphs neural representations; however, recent advances in language models highlight how much scientific knowledge is encoded in text. To bridge these two modalities, we investigate how molecular property information can be transferred from natural language to graph representations. We study property prediction performance gains after using contrastive learning to align neural graph representations with representations of textual descriptions of their characteristics. We implement neural relevance scoring strategies to improve text retrieval, introduce a novel chemically-valid molecular graph augmentation strategy inspired by organic reactions, and demonstrate improved performance on downstream MoleculeNet property classification tasks. We achieve a +4.26% AUROC gain versus models pre-trained on the graph modality alone, and a +1.54% gain compared to recently proposed molecular graph/text contrastively trained MoMu model (Su et al. 2022).
</details>
<details>
<summary>摘要</summary>
深度学习在计算生物化学中传统上专注于分子图神经表示;然而，最近的语言模型发展显示了科学知识如何被编码在文本中。为了融合这两种模式，我们研究如何从自然语言中提取分子性质信息，并将其转换为图表示。我们使用对比学习对神经图表示和文本描述的特征进行对齐，并使用神经相关性分配策略来提高文本检索。我们还开发了一种基于有机反应的新型化学图像增强策略，并在下游的分子网络性质分类任务上实现了+4.26%的AUROC提升和+1.54%的提升，相比之前的图模式和文本对照训练的MoMu模型（Su et al. 2022）。
</details></li>
</ul>
<hr>
<h2 id="Flight-Contrail-Segmentation-via-Augmented-Transfer-Learning-with-Novel-SR-Loss-Function-in-Hough-Space"><a href="#Flight-Contrail-Segmentation-via-Augmented-Transfer-Learning-with-Novel-SR-Loss-Function-in-Hough-Space" class="headerlink" title="Flight Contrail Segmentation via Augmented Transfer Learning with Novel SR Loss Function in Hough Space"></a>Flight Contrail Segmentation via Augmented Transfer Learning with Novel SR Loss Function in Hough Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12032">http://arxiv.org/abs/2307.12032</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/junzis/contrail-net">https://github.com/junzis/contrail-net</a></li>
<li>paper_authors: Junzi Sun, Esther Roosenbrand</li>
<li>for: 检测飞行 contrails 的卫星图像中的 contrails，减少环境影响和气候变化。</li>
<li>methods: 使用加强转移学习模型和 SR Loss 函数，从 varying 的图像条件下准确地检测 contrails。</li>
<li>results: 提供了一种新的 contrail 检测模型，可以在 minimal 数据下准确地检测 contrails，并且开辟了新的机器学习在航空研究中的应用前景。<details>
<summary>Abstract</summary>
Air transport poses significant environmental challenges, particularly the contribution of flight contrails to climate change due to their potential global warming impact. Detecting contrails from satellite images has been a long-standing challenge. Traditional computer vision techniques have limitations under varying image conditions, and machine learning approaches using typical convolutional neural networks are hindered by the scarcity of hand-labeled contrail datasets and contrail-tailored learning processes. In this paper, we introduce an innovative model based on augmented transfer learning that accurately detects contrails with minimal data. We also propose a novel loss function, SR Loss, which improves contrail line detection by transforming the image space into Hough space. Our research opens new avenues for machine learning-based contrail detection in aviation research, offering solutions to the lack of large hand-labeled datasets, and significantly enhancing contrail detection models.
</details>
<details>
<summary>摘要</summary>
飞行交通对环境造成重要挑战，特别是飞机烟尘对气候变化的贡献，这些烟尘可能对全球暖化产生影响。传统的计算机视觉技术在不同的图像条件下有限制，机器学习方法使用Typical Convolutional Neural Networks (CNNs) 受到手动标注烟尘数据的罕见性和烟尘特化学习过程的限制。在这篇论文中，我们介绍了一种创新的模型，基于增强转移学习，可以准确地检测烟尘。我们还提出了一个新的损失函数，SR损失，该函数在图像空间转换到抽象空间，从而提高烟尘线检测。我们的研究开启了新的机器学习基于烟尘检测的可能性，为航空研究提供了新的解决方案，并对烟尘检测模型进行了重要改进。
</details></li>
</ul>
<hr>
<h2 id="FinPT-Financial-Risk-Prediction-with-Profile-Tuning-on-Pretrained-Foundation-Models"><a href="#FinPT-Financial-Risk-Prediction-with-Profile-Tuning-on-Pretrained-Foundation-Models" class="headerlink" title="FinPT: Financial Risk Prediction with Profile Tuning on Pretrained Foundation Models"></a>FinPT: Financial Risk Prediction with Profile Tuning on Pretrained Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00065">http://arxiv.org/abs/2308.00065</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuweiyin/finpt">https://github.com/yuweiyin/finpt</a></li>
<li>paper_authors: Yuwei Yin, Yazheng Yang, Jian Yang, Qi Liu</li>
<li>for: 该论文旨在提出一种新的金融风险预测方法，以及一个开源的金融标准benchmark。</li>
<li>methods: 该方法使用Profile Tuning技术，将大型预训模型与自然语言客户 profiling 结合，以便更好地预测金融风险。</li>
<li>results: 实验表明，该方法可以与一系列代表性的强基线进行比较，并且可以更好地预测金融风险。further analysis也深入了解了LLMs在金融风险预测方面的性能。<details>
<summary>Abstract</summary>
Financial risk prediction plays a crucial role in the financial sector. Machine learning methods have been widely applied for automatically detecting potential risks and thus saving the cost of labor. However, the development in this field is lagging behind in recent years by the following two facts: 1) the algorithms used are somewhat outdated, especially in the context of the fast advance of generative AI and large language models (LLMs); 2) the lack of a unified and open-sourced financial benchmark has impeded the related research for years. To tackle these issues, we propose FinPT and FinBench: the former is a novel approach for financial risk prediction that conduct Profile Tuning on large pretrained foundation models, and the latter is a set of high-quality datasets on financial risks such as default, fraud, and churn. In FinPT, we fill the financial tabular data into the pre-defined instruction template, obtain natural-language customer profiles by prompting LLMs, and fine-tune large foundation models with the profile text to make predictions. We demonstrate the effectiveness of the proposed FinPT by experimenting with a range of representative strong baselines on FinBench. The analytical studies further deepen the understanding of LLMs for financial risk prediction.
</details>
<details>
<summary>摘要</summary>
金融风险预测在金融领域扮演着关键的角色。机器学习技术已广泛应用于自动检测potential risks，从而节省劳动成本。然而，在这一领域的发展受到了两个因素的压缩：1）使用的算法相对落后，尤其是在生成AI和大语言模型（LLM）的快速发展的背景下；2）缺乏一个统一的开源金融标准 benchmark，对相关研究造成了年月的妨碍。为解决这些问题，我们提出了FinPT和FinBench：前者是一种新的金融风险预测方法，通过在大型预训模型上进行 Profile Tuning，并使用自然语言客户 profiling 来提高预测精度；后者是一个高质量的金融风险数据集，包括落后、诈骗和迁移等风险。在FinPT中，我们将金融表格数据填充到预定的指示模板中，通过LLM提取自然语言客户 profiling，并使用这些 profiling 来练化大型基础模型以进行预测。我们通过对一系列代表强基eline进行实验，证明了FinPT的效果。分析研究还深入了解LLMs在金融风险预测方面的性能。
</details></li>
</ul>
<hr>
<h2 id="A-Flexible-Framework-for-Incorporating-Patient-Preferences-Into-Q-Learning"><a href="#A-Flexible-Framework-for-Incorporating-Patient-Preferences-Into-Q-Learning" class="headerlink" title="A Flexible Framework for Incorporating Patient Preferences Into Q-Learning"></a>A Flexible Framework for Incorporating Patient Preferences Into Q-Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12022">http://arxiv.org/abs/2307.12022</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joshua P. Zitovsky, Leslie Wilson, Michael R. Kosorok</li>
<li>for: 这篇研究旨在解决现实医疗问题中的多重竞争结果（例如治疗效果和副作用严重程度），现有的统计方法 для预测动态治疗方案（DTR）通常假设单一结果的 interess，而几 Methods 则受到重要的限制。</li>
<li>methods: 我们提出了一新的方法，即潜在价值Q学习（LUQ-Learning），它使用潜在模型方法来自然地扩展Q学习到多元结果设定下，并遵循每名病人的理想交易。不同于先前的方法，我们的框架允许无限多个时间点和结果，并将自愿报告的病人偏好纳入考虑。</li>
<li>results: 我们在基于低背痛试验和已经完成的治疗试验的 simulated experiments 中，与多个替代基准点进行比较，结果显示我们的方法在调节多元结果下具有高度竞争的实验性表现。<details>
<summary>Abstract</summary>
In real-world healthcare problems, there are often multiple competing outcomes of interest, such as treatment efficacy and side effect severity. However, statistical methods for estimating dynamic treatment regimes (DTRs) usually assume a single outcome of interest, and the few methods that deal with composite outcomes suffer from important limitations. This includes restrictions to a single time point and two outcomes, the inability to incorporate self-reported patient preferences and limited theoretical guarantees. To this end, we propose a new method to address these limitations, which we dub Latent Utility Q-Learning (LUQ-Learning). LUQ-Learning uses a latent model approach to naturally extend Q-learning to the composite outcome setting and adopt the ideal trade-off between outcomes to each patient. Unlike previous approaches, our framework allows for an arbitrary number of time points and outcomes, incorporates stated preferences and achieves strong asymptotic performance with realistic assumptions on the data. We conduct simulation experiments based on an ongoing trial for low back pain as well as a well-known completed trial for schizophrenia. In all experiments, our method achieves highly competitive empirical performance compared to several alternative baselines.
</details>
<details>
<summary>摘要</summary>
在现实医疗问题中，经常存在多个竞争的目标结果，如治疗效果和生化效果的严重程度。然而，统计方法用于估计动态治疗方案（DTR）通常假设单一的目标结果，而其中几种方法都受到重要的限制。这包括单一时间点和两个结果的限制，无法 интеグ勋自报告病人偏好以及有限的理论保证。为此，我们提出了一种新的方法，我们称之为潜在用户Q学习（LUQ-Learning）。LUQ-Learning使用潜在模型方法来自然地扩展Q学习到复合结果设定下，采取每个患者的理想的质量平衡。与之前的方法不同，我们的框架允许任意的时间点和结果数量，并包括自报告偏好，实现了实际数据下有理的假设下的强 asymptotic performance。我们在一个低背疼患者试验中进行了 simulate实验，以及一个已经完成的试验数据集。在所有实验中，我们的方法与多个基准方法进行了高度竞争性的实验性表现。
</details></li>
</ul>
<hr>
<h2 id="Model-Predictive-Control-MPC-of-an-Artificial-Pancreas-with-Data-Driven-Learning-of-Multi-Step-Ahead-Blood-Glucose-Predictors"><a href="#Model-Predictive-Control-MPC-of-an-Artificial-Pancreas-with-Data-Driven-Learning-of-Multi-Step-Ahead-Blood-Glucose-Predictors" class="headerlink" title="Model Predictive Control (MPC) of an Artificial Pancreas with Data-Driven Learning of Multi-Step-Ahead Blood Glucose Predictors"></a>Model Predictive Control (MPC) of an Artificial Pancreas with Data-Driven Learning of Multi-Step-Ahead Blood Glucose Predictors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12015">http://arxiv.org/abs/2307.12015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eleonora Maria Aiello, Mehrad Jaloli, Marzia Cescon</li>
<li>For: The paper is written for treating type 1 diabetes (T1D) using a closed-loop insulin delivery algorithm that incorporates a data-driven multi-step-ahead blood glucose (BG) predictor and a Linear Time-Varying (LTV) Model Predictive Control (MPC) framework.* Methods: The paper proposes a nonlinear function of past input-output data and an affine function of future insulin control inputs to predict future BG concentrations. Specifically, a Long Short-Term Memory (LSTM) network is used for the nonlinear part, and a linear regression model is used for the affine component.* Results: The proposed LSTM-MPC controller outperformed a traditional linear MPC based on an auto-regressive with exogenous (ARX) input model in terms of accuracy of future glucose concentrations and closed-loop performance. Specifically, the LSTM-MPC controller achieved a mean $\pm$ standard deviation percent time in the range 70-180 [mg&#x2F;dL] of 74.99 $\pm$ 7.09, compared to 54.15 $\pm$ 14.89 for the ARX-MPC controller. Additionally, the LSTM-MPC controller had less severe hypoglycemia, with a mean $\pm$ standard deviation percent time in the range 70-140 [mg&#x2F;dL] of 47.78$\pm$8.55, compared to 34.62 $\pm$9.04 for the ARX-MPC controller.<details>
<summary>Abstract</summary>
We present the design and \textit{in-silico} evaluation of a closed-loop insulin delivery algorithm to treat type 1 diabetes (T1D) consisting in a data-driven multi-step-ahead blood glucose (BG) predictor integrated into a Linear Time-Varying (LTV) Model Predictive Control (MPC) framework. Instead of identifying an open-loop model of the glucoregulatory system from available data, we propose to directly fit the entire BG prediction over a predefined prediction horizon to be used in the MPC, as a nonlinear function of past input-ouput data and an affine function of future insulin control inputs. For the nonlinear part, a Long Short-Term Memory (LSTM) network is proposed, while for the affine component a linear regression model is chosen. To assess benefits and drawbacks when compared to a traditional linear MPC based on an auto-regressive with exogenous (ARX) input model identified from data, we evaluated the proposed LSTM-MPC controller in three simulation scenarios: a nominal case with 3 meals per day, a random meal disturbances case where meals were generated with a recently published meal generator, and a case with 25$\%$ decrease in the insulin sensitivity. Further, in all the scenarios, no feedforward meal bolus was administered. For the more challenging random meal generation scenario, the mean $\pm$ standard deviation percent time in the range 70-180 [mg/dL] was 74.99 $\pm$ 7.09 vs. 54.15 $\pm$ 14.89, the mean $\pm$ standard deviation percent time in the tighter range 70-140 [mg/dL] was 47.78$\pm$8.55 vs. 34.62 $\pm$9.04, while the mean $\pm$ standard deviation percent time in sever hypoglycemia, i.e., $<$ 54 [mg/dl] was 1.00$\pm$3.18 vs. 9.45$\pm$11.71, for our proposed LSTM-MPC controller and the traditional ARX-MPC, respectively. Our approach provided accurate predictions of future glucose concentrations and good closed-loop performances of the overall MPC controller.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种闭环式血糖采集算法，用于治疗一型糖尿病（T1D），该算法包括一个基于数据驱动的多步预测血糖（BG）预测器，并与线性时间变化（LTV）模型预测控制（MPC）框架集成。而不是直接从可用数据中Identify一个开环模型，我们提议直接使用前一个预测 horizon的整个BG预测作为一个非线性函数，并用未来药物控制输入作为一个线性函数。为非线性部分，我们提议使用一个Long Short-Term Memory（LSTM）网络，而为线性部分，我们选择了一个线性回归模型。为了评估我们的LSTM-MPC控制器和传统的ARX-MPC控制器之间的优劣点，我们在三个模拟场景中进行了评估：一个标准情况下，每天有3顿饭，一个随机饭物干扰情况，饭物是通过最近发布的饭物生成器生成的，以及一个25%的药物敏感度下降情况。此外，在所有场景下，没有前向补偿饭物背包。在更加具有挑战性的随机饭物生成场景下， mean ± 标准差%时间在70-180[mg/dL]范围内为74.99 ± 7.09 vs. 54.15 ± 14.89， mean ± 标准差%时间在70-140[mg/dL]范围内为47.78 ± 8.55 vs. 34.62 ± 9.04，而mean ± 标准差%时间在严重低血糖（<54[mg/dL]）下为1.00 ± 3.18 vs. 9.45 ± 11.71。我们的方法提供了准确的未来血糖浓度预测和闭环式MPC控制器的全面性能。
</details></li>
</ul>
<hr>
<h2 id="NLCUnet-Single-Image-Super-Resolution-Network-with-Hairline-Details"><a href="#NLCUnet-Single-Image-Super-Resolution-Network-with-Hairline-Details" class="headerlink" title="NLCUnet: Single-Image Super-Resolution Network with Hairline Details"></a>NLCUnet: Single-Image Super-Resolution Network with Hairline Details</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12014">http://arxiv.org/abs/2307.12014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiancong Feng, Yuan-Gen Wang, Fengchuang Xing</li>
<li>For: 这篇论文旨在提高单图超解像的精度。* Methods: 论文提出了三个核心设计：首先，引入非本地注意力机制，以恢复图像地方的局部 Piece;然后，发现现有工作中的模糊核心可以忽略，因此提出了一种新的网络架构， integrate depth-wise convolution 和通道注意力，而不需要模糊核心估计。最后，为了让剪辑区域尽可能包含semantic信息，提出了随机64×64剪辑在中心512×512剪辑中 instead of direct random crop inside the whole image of 2K size。* Results: 对DF2K数据集进行了许多实验，并证明了我们的NLCUnet在PSNR和SSIM指标上比 estado-of-the-art 性能更好，并且在视觉上也更有优势。<details>
<summary>Abstract</summary>
Pursuing the precise details of super-resolution images is challenging for single-image super-resolution tasks. This paper presents a single-image super-resolution network with hairline details (termed NLCUnet), including three core designs. Specifically, a non-local attention mechanism is first introduced to restore local pieces by learning from the whole image region. Then, we find that the blur kernel trained by the existing work is unnecessary. Based on this finding, we create a new network architecture by integrating depth-wise convolution with channel attention without the blur kernel estimation, resulting in a performance improvement instead. Finally, to make the cropped region contain as much semantic information as possible, we propose a random 64$\times$64 crop inside the central 512$\times$512 crop instead of a direct random crop inside the whole image of 2K size. Numerous experiments conducted on the benchmark DF2K dataset demonstrate that our NLCUnet performs better than the state-of-the-art in terms of the PSNR and SSIM metrics and yields visually favorable hairline details.
</details>
<details>
<summary>摘要</summary>
追求超解像的细节精度在单图超解像 задании中是一项挑战。这篇论文提出了一种单图超解像网络（NLCUnet），包括三个核心设计。具体来说，我们首先引入非本地注意力机制，用于在整个图像区域学习并复原本地精度。然后，我们发现现有工作中用于训练模糊核的权重是不必要的，因此我们创建了一个新的网络架构，将深度wise核论并 Channel Attention 结合使用，从而实现性能提升。最后，我们提议在中心 512x512 像素区域内随机选择 64x64 像素的区域，以便尽可能多地保留图像中的semantic信息。经过在 DF2K 数据集上进行了多次实验，我们发现我们的 NLCUnet 在 PSNR 和 SSIM 指标上表现出色，并且可以提供更加有吸引力的毛细膨胀细节。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Self-Supervised-Learning-Based-Approach-for-Patient-Similarity-A-Case-Study-on-Atrial-Fibrillation-Detection-from-PPG-Signal"><a href="#Contrastive-Self-Supervised-Learning-Based-Approach-for-Patient-Similarity-A-Case-Study-on-Atrial-Fibrillation-Detection-from-PPG-Signal" class="headerlink" title="Contrastive Self-Supervised Learning Based Approach for Patient Similarity: A Case Study on Atrial Fibrillation Detection from PPG Signal"></a>Contrastive Self-Supervised Learning Based Approach for Patient Similarity: A Case Study on Atrial Fibrillation Detection from PPG Signal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02433">http://arxiv.org/abs/2308.02433</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/subangkar/simsig">https://github.com/subangkar/simsig</a></li>
<li>paper_authors: Subangkar Karmaker Shanto, Shoumik Saha, Atif Hasan Rahman, Mohammad Mehedy Masud, Mohammed Eunus Ali</li>
<li>for: 用于搜索类似病人，使用生物信号physiological signal进行深度学习搜索。</li>
<li>methods: 使用异构学习方法学习类似病人的生物信号数据，并提出了一些邻居选择算法来确定最相似的病人。</li>
<li>results: 对于检测心脏病Atrial Fibrillation（AF），通过光敏电生物学（PPG）信号来评估病人类似性，并在大量实验中证明了其效果。<details>
<summary>Abstract</summary>
In this paper, we propose a novel contrastive learning based deep learning framework for patient similarity search using physiological signals. We use a contrastive learning based approach to learn similar embeddings of patients with similar physiological signal data. We also introduce a number of neighbor selection algorithms to determine the patients with the highest similarity on the generated embeddings. To validate the effectiveness of our framework for measuring patient similarity, we select the detection of Atrial Fibrillation (AF) through photoplethysmography (PPG) signals obtained from smartwatch devices as our case study. We present extensive experimentation of our framework on a dataset of over 170 individuals and compare the performance of our framework with other baseline methods on this dataset.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种基于对比学习的深度学习框架，用于基于生物physiological signal的患者相似性搜索。我们使用对比学习方法来学习患者的生物信号数据中的相似性 embedding。我们还介绍了一些邻居选择算法，用于确定生成的 embedding 中的最高相似度患者。为了证明我们的框架对患者相似性的评估效果，我们选择了基于 photoplethysmography (PPG) 信号的 Atrial Fibrillation (AF) 检测作为我们的案例研究。我们对一个包含超过 170 人的数据集进行了广泛的实验，并与其他基线方法进行比较。
</details></li>
</ul>
<hr>
<h2 id="Expert-Knowledge-Aware-Image-Difference-Graph-Representation-Learning-for-Difference-Aware-Medical-Visual-Question-Answering"><a href="#Expert-Knowledge-Aware-Image-Difference-Graph-Representation-Learning-for-Difference-Aware-Medical-Visual-Question-Answering" class="headerlink" title="Expert Knowledge-Aware Image Difference Graph Representation Learning for Difference-Aware Medical Visual Question Answering"></a>Expert Knowledge-Aware Image Difference Graph Representation Learning for Difference-Aware Medical Visual Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11986">http://arxiv.org/abs/2307.11986</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/holipori/mimic-diff-vqa">https://github.com/holipori/mimic-diff-vqa</a></li>
<li>paper_authors: Xinyue Hu, Lin Gu, Qiyuan An, Mengliang Zhang, Liangchen Liu, Kazuma Kobayashi, Tatsuya Harada, Ronald M. Summers, Yingying Zhu</li>
<li>for: 提高医疗机器人视觉语言模型的自动化水平，并且更好地识别医疗图像的差异。</li>
<li>methods: 提出了一种新的胸部X射线图像差异视觉问答任务，并收集了700,703个问答对，来评估医疗图像的差异。</li>
<li>results: 通过利用专家知识来构建多关系图表，提高了医疗图像差异问答任务的性能。<details>
<summary>Abstract</summary>
To contribute to automating the medical vision-language model, we propose a novel Chest-Xray Difference Visual Question Answering (VQA) task. Given a pair of main and reference images, this task attempts to answer several questions on both diseases and, more importantly, the differences between them. This is consistent with the radiologist's diagnosis practice that compares the current image with the reference before concluding the report. We collect a new dataset, namely MIMIC-Diff-VQA, including 700,703 QA pairs from 164,324 pairs of main and reference images. Compared to existing medical VQA datasets, our questions are tailored to the Assessment-Diagnosis-Intervention-Evaluation treatment procedure used by clinical professionals. Meanwhile, we also propose a novel expert knowledge-aware graph representation learning model to address this task. The proposed baseline model leverages expert knowledge such as anatomical structure prior, semantic, and spatial knowledge to construct a multi-relationship graph, representing the image differences between two images for the image difference VQA task. The dataset and code can be found at https://github.com/Holipori/MIMIC-Diff-VQA. We believe this work would further push forward the medical vision language model.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:为推动医疗视力语言模型的自动化，我们提出了一个新的胸部X射影异常视觉问答任务（VQA）。给定两个主要和参考图像，这个任务试图回答几个疾病和两个图像之间的差异问题。这与诊断医生在比较当前图像和参考图像后作出报告的实践相符。我们收集了一个新的数据集，名为MIMIC-Diff-VQA，包含700,703个问题对从164,324对主要和参考图像中。相比现有的医学VQA数据集，我们的问题更加适应诊断、诊断、治疗和评估过程中的临床专业人员的做法。同时，我们还提出了一种基于专家知识的图像差异图表学习模型来解决这个任务。我们的基eline模型利用了专家知识，如生物结构先天知识、semantic知识和空间知识，构建多关系图，表示两个图像之间的差异。数据集和代码可以在https://github.com/Holipori/MIMIC-Diff-VQA中找到。我们认为这项工作将会驱动医疗视力语言模型的进一步发展。
</details></li>
</ul>
<hr>
<h2 id="Collaborative-Graph-Neural-Networks-for-Attributed-Network-Embedding"><a href="#Collaborative-Graph-Neural-Networks-for-Attributed-Network-Embedding" class="headerlink" title="Collaborative Graph Neural Networks for Attributed Network Embedding"></a>Collaborative Graph Neural Networks for Attributed Network Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11981">http://arxiv.org/abs/2307.11981</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qiaoyut/conn">https://github.com/qiaoyut/conn</a></li>
<li>paper_authors: Qiaoyu Tan, Xin Zhang, Xiao Huang, Hao Chen, Jundong Li, Xia Hu</li>
<li>for: 这篇论文的目的是提出一种适应属性网络嵌入的Graph Neural Networks（GNN）架构，以提高模型的容量和表现。</li>
<li>methods: 这篇论文使用的方法包括：1) 选择性地传递邻近节点和受到属性类别的消息，2) 同时重建节点到节点和节点到属性类别的互动。</li>
<li>results: 实验结果表明，这篇论文提出的CONN架构在真实的网络上表现出了明显的优势，与现有的 embedding 算法相比，具有较大的margin。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have shown prominent performance on attributed network embedding. However, existing efforts mainly focus on exploiting network structures, while the exploitation of node attributes is rather limited as they only serve as node features at the initial layer. This simple strategy impedes the potential of node attributes in augmenting node connections, leading to limited receptive field for inactive nodes with few or even no neighbors. Furthermore, the training objectives (i.e., reconstructing network structures) of most GNNs also do not include node attributes, although studies have shown that reconstructing node attributes is beneficial. Thus, it is encouraging to deeply involve node attributes in the key components of GNNs, including graph convolution operations and training objectives. However, this is a nontrivial task since an appropriate way of integration is required to maintain the merits of GNNs. To bridge the gap, in this paper, we propose COllaborative graph Neural Networks--CONN, a tailored GNN architecture for attribute network embedding. It improves model capacity by 1) selectively diffusing messages from neighboring nodes and involved attribute categories, and 2) jointly reconstructing node-to-node and node-to-attribute-category interactions via cross-correlation. Experiments on real-world networks demonstrate that CONN excels state-of-the-art embedding algorithms with a great margin.
</details>
<details>
<summary>摘要</summary>
Graph Neural Networks (GNNs) have shown prominent performance on attributed network embedding. However, existing efforts mainly focus on exploiting network structures, while the exploitation of node attributes is rather limited as they only serve as node features at the initial layer. This simple strategy impedes the potential of node attributes in augmenting node connections, leading to limited receptive field for inactive nodes with few or even no neighbors. Furthermore, the training objectives (i.e., reconstructing network structures) of most GNNs also do not include node attributes, although studies have shown that reconstructing node attributes is beneficial. Thus, it is encouraging to deeply involve node attributes in the key components of GNNs, including graph convolution operations and training objectives. However, this is a nontrivial task since an appropriate way of integration is required to maintain the merits of GNNs. To bridge the gap, in this paper, we propose COllaborative graph Neural Networks--CONN, a tailored GNN architecture for attribute network embedding. It improves model capacity by 1) selectively diffusing messages from neighboring nodes and involved attribute categories, and 2) jointly reconstructing node-to-node and node-to-attribute-category interactions via cross-correlation. Experiments on real-world networks demonstrate that CONN excels state-of-the-art embedding algorithms with a great margin.
</details></li>
</ul>
<hr>
<h2 id="Simulation-of-Arbitrary-Level-Contrast-Dose-in-MRI-Using-an-Iterative-Global-Transformer-Model"><a href="#Simulation-of-Arbitrary-Level-Contrast-Dose-in-MRI-Using-an-Iterative-Global-Transformer-Model" class="headerlink" title="Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model"></a>Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11980">http://arxiv.org/abs/2307.11980</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dayang Wang, Srivathsa Pasumarthi, Greg Zaharchuk, Ryan Chamberlain</li>
<li>for: 本研究旨在提出一种基于 transformer 的迭代模型方法，用于Synthesizing 具有不同增强程度的图像，以满足不同类型的 GBCAs 和疾病的需求。</li>
<li>methods: 该方法基于 sub-sampling based attention 机制和旋转 shift 模块，能够 capture 不同增强相关特征。</li>
<li>results: 比较测试表明，提出的模型表现比其他当前状态的方法更好，并在下游任务 such as 剂量减少和肿瘤分 segmentation 中表现出优异的临床实用性。<details>
<summary>Abstract</summary>
Deep learning (DL) based contrast dose reduction and elimination in MRI imaging is gaining traction, given the detrimental effects of Gadolinium-based Contrast Agents (GBCAs). These DL algorithms are however limited by the availability of high quality low dose datasets. Additionally, different types of GBCAs and pathologies require different dose levels for the DL algorithms to work reliably. In this work, we formulate a novel transformer (Gformer) based iterative modelling approach for the synthesis of images with arbitrary contrast enhancement that corresponds to different dose levels. The proposed Gformer incorporates a sub-sampling based attention mechanism and a rotational shift module that captures the various contrast related features. Quantitative evaluation indicates that the proposed model performs better than other state-of-the-art methods. We further perform quantitative evaluation on downstream tasks such as dose reduction and tumor segmentation to demonstrate the clinical utility.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）基于对比剂减少和消除在MRI成像中得到了进展，由于质子剂（GBCAs）的负面影响。这些DL算法然而受到高质量低剂量数据的有效性的限制。此外，不同类型的GBCAs和疾病需要不同的剂量水平以确保DL算法的可靠性。在这种工作中，我们提出了一种基于变换器（Gformer）的迭代模型方法，用于synthesize图像的任意对比强化，与不同的剂量水平相对应。提案中的Gformer包括一种子抽样基于注意机制和一种旋转shift模块，以捕捉不同的对比相关特征。量化评估表明，提案的模型比其他现状顶尖方法性能更好。我们进一步进行了下游任务 such as 剂量减少和肿瘤分 segmentation，以证明临床实用性。
</details></li>
</ul>
<hr>
<h2 id="Why-Is-Prompt-Tuning-for-Vision-Language-Models-Robust-to-Noisy-Labels"><a href="#Why-Is-Prompt-Tuning-for-Vision-Language-Models-Robust-to-Noisy-Labels" class="headerlink" title="Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?"></a>Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11978">http://arxiv.org/abs/2307.11978</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cewu/ptnl">https://github.com/cewu/ptnl</a></li>
<li>paper_authors: Cheng-En Wu, Yu Tian, Haichao Yu, Heng Wang, Pedro Morgado, Yu Hen Hu, Linjie Yang</li>
<li>for: 这个论文主要是研究如何使用CLIP进行几何预处理，以提高预测的准确率。</li>
<li>methods: 这个论文使用了CLIP来学习一个通用的文本-图像嵌入空间，然后通过几何预处理来适应新的分类任务。</li>
<li>results: 这个论文发现，使用几何预处理可以很好地鲁应对标签噪音，并且可以使用不精确的预测来更新预处理。<details>
<summary>Abstract</summary>
Vision-language models such as CLIP learn a generic text-image embedding from large-scale training data. A vision-language model can be adapted to a new classification task through few-shot prompt tuning. We find that such a prompt tuning process is highly robust to label noises. This intrigues us to study the key reasons contributing to the robustness of the prompt tuning paradigm. We conducted extensive experiments to explore this property and find the key factors are: 1) the fixed classname tokens provide a strong regularization to the optimization of the model, reducing gradients induced by the noisy samples; 2) the powerful pre-trained image-text embedding that is learned from diverse and generic web data provides strong prior knowledge for image classification. Further, we demonstrate that noisy zero-shot predictions from CLIP can be used to tune its own prompt, significantly enhancing prediction accuracy in the unsupervised setting. The code is available at https://github.com/CEWu/PTNL.
</details>
<details>
<summary>摘要</summary>
视觉语模型如CLIP通过大规模训练学习一个通用的文本图像嵌入。一个视觉语模型可以通过几张图片推理来适应新的分类任务。我们发现这种推理过程具有很高的鲁棒性，这使我们感兴趣研究这种鲁棒性的关键原因。我们进行了广泛的实验研究，并发现关键因素有：1）固定的类名token提供了模型优化的强大正则化，减少了噪声样本引起的梯度; 2）通过多样化和通用的网络数据学习的强大预训练图像文本嵌入，为图像分类提供了强大的先验知识。此外，我们示出了CLIP的噪声零shot预测可以用来调整其自己的提示，显著提高了无监督设定下的预测精度。代码可以在https://github.com/CEWu/PTNL中找到。
</details></li>
</ul>
<hr>
<h2 id="Out-of-Distribution-Optimality-of-Invariant-Risk-Minimization"><a href="#Out-of-Distribution-Optimality-of-Invariant-Risk-Minimization" class="headerlink" title="Out-of-Distribution Optimality of Invariant Risk Minimization"></a>Out-of-Distribution Optimality of Invariant Risk Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11972">http://arxiv.org/abs/2307.11972</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shoji Toyota, Kenji Fukumizu</li>
<li>for: 这篇论文是为了解释 invariant risk minimization (IRM) 方法的理论基础，并提供了一个准确的证明，证明 IRM 方法可以减少 out-of-distribution (o.o.d.) 风险。</li>
<li>methods: 这篇论文使用了 bi-level 优化问题来解释 IRM 方法，并提供了一个准确的证明，证明 bi-level 优化问题的解决方案可以减少 o.o.d. 风险。</li>
<li>results: 这篇论文提供了一个准确的证明，证明 IRM 方法可以减少 o.o.d. 风险，并提供了一些条件，以确保 bi-level 优化问题的解决方案可以减少 o.o.d. 风险。<details>
<summary>Abstract</summary>
Deep Neural Networks often inherit spurious correlations embedded in training data and hence may fail to generalize to unseen domains, which have different distributions from the domain to provide training data. M. Arjovsky et al. (2019) introduced the concept out-of-distribution (o.o.d.) risk, which is the maximum risk among all domains, and formulated the issue caused by spurious correlations as a minimization problem of the o.o.d. risk. Invariant Risk Minimization (IRM) is considered to be a promising approach to minimize the o.o.d. risk: IRM estimates a minimum of the o.o.d. risk by solving a bi-level optimization problem. While IRM has attracted considerable attention with empirical success, it comes with few theoretical guarantees. Especially, a solid theoretical guarantee that the bi-level optimization problem gives the minimum of the o.o.d. risk has not yet been established. Aiming at providing a theoretical justification for IRM, this paper rigorously proves that a solution to the bi-level optimization problem minimizes the o.o.d. risk under certain conditions. The result also provides sufficient conditions on distributions providing training data and on a dimension of feature space for the bi-leveled optimization problem to minimize the o.o.d. risk.
</details>
<details>
<summary>摘要</summary>
Note:* "Deep Neural Networks" is 深度神经网络 (shēn dì shén zhì wǎng) in Simplified Chinese.* "out-of-distribution" is 外部风险 (wài bù fēng xióng) in Simplified Chinese.* "Invariant Risk Minimization" is 风险规范化 (fēng xióng guī fāng huà) in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="DHC-Dual-debiased-Heterogeneous-Co-training-Framework-for-Class-imbalanced-Semi-supervised-Medical-Image-Segmentation"><a href="#DHC-Dual-debiased-Heterogeneous-Co-training-Framework-for-Class-imbalanced-Semi-supervised-Medical-Image-Segmentation" class="headerlink" title="DHC: Dual-debiased Heterogeneous Co-training Framework for Class-imbalanced Semi-supervised Medical Image Segmentation"></a>DHC: Dual-debiased Heterogeneous Co-training Framework for Class-imbalanced Semi-supervised Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11960">http://arxiv.org/abs/2307.11960</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/dhc">https://github.com/xmed-lab/dhc</a></li>
<li>paper_authors: Haonan Wang, Xiaomeng Li<br>for:这篇论文主要targets semi-supervised learning (SSL) for 3D medical image segmentation, with a focus on addressing the class imbalance problem.methods:The proposed method, called Dual-debiased Heterogeneous Co-training (DHC), leverages two sub-models with different strengths to improve the segmentation accuracy. The method also introduces two loss weighting strategies, DistDW和DiffDW, to dynamically adjust the pseudo labels and alleviate the class imbalance issue.results:Experiments show that the proposed DHC framework significantly improves the segmentation accuracy compared to the state-of-the-art SSL methods, demonstrating its potential for more challenging SSL settings. Additionally, the proposed method outperforms other class-imbalance designs, providing a more robust solution for real-world applications.<details>
<summary>Abstract</summary>
The volume-wise labeling of 3D medical images is expertise-demanded and time-consuming; hence semi-supervised learning (SSL) is highly desirable for training with limited labeled data. Imbalanced class distribution is a severe problem that bottlenecks the real-world application of these methods but was not addressed much. Aiming to solve this issue, we present a novel Dual-debiased Heterogeneous Co-training (DHC) framework for semi-supervised 3D medical image segmentation. Specifically, we propose two loss weighting strategies, namely Distribution-aware Debiased Weighting (DistDW) and Difficulty-aware Debiased Weighting (DiffDW), which leverage the pseudo labels dynamically to guide the model to solve data and learning biases. The framework improves significantly by co-training these two diverse and accurate sub-models. We also introduce more representative benchmarks for class-imbalanced semi-supervised medical image segmentation, which can fully demonstrate the efficacy of the class-imbalance designs. Experiments show that our proposed framework brings significant improvements by using pseudo labels for debiasing and alleviating the class imbalance problem. More importantly, our method outperforms the state-of-the-art SSL methods, demonstrating the potential of our framework for the more challenging SSL setting. Code and models are available at: https://github.com/xmed-lab/DHC.
</details>
<details>
<summary>摘要</summary>
《三维医疗图像分割预测中的卷积批处理是专业需求占用时间，因此半supervised learning（SSL）是非常有优点的。然而，实际应用中存在很严重的分类不均衡问题，这限制了这些方法的应用。为解决这个问题，我们提出了一个新的双向偏置多样化合成（DHC）框架，用于 semi-supervised 三维医疗图像分割。我们提出了两种损失补偿策略，namely Distribution-aware Debiased Weighting（DistDW）和 Difficulty-aware Debiased Weighting（DiffDW），这些策略在运动pseudo标签来导引模型解决数据和学习偏见。该框架在多个子模型之间协同训练，从而提高了性能。我们还引入了更加代表的 semi-supervised 医疗图像分割benchmark，可以完全展示class-imbalance设计的效果。实验表明，我们提出的方法可以通过使用pseudo标签进行偏置和平衡分类不均衡问题，并且超过了状态当前的SSL方法，示出了我们的框架在更加挑战的SSL设置中的潜在能力。代码和模型可以在https://github.com/xmed-lab/DHC上获取。》
</details></li>
</ul>
<hr>
<h2 id="Multi-representations-Space-Separation-based-Graph-level-Anomaly-aware-Detection"><a href="#Multi-representations-Space-Separation-based-Graph-level-Anomaly-aware-Detection" class="headerlink" title="Multi-representations Space Separation based Graph-level Anomaly-aware Detection"></a>Multi-representations Space Separation based Graph-level Anomaly-aware Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12994">http://arxiv.org/abs/2307.12994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fu Lin, Haonan Gong, Mingkang Li, Zitong Wang, Yue Zhang, Xuexiong Luo</li>
<li>for: 本研究的目的是检测图像中的异常图。</li>
<li>methods: 我们提出了一种基于多 Representation Space 的异常检测方法，包括一个异常感知模块，用于评估异常图的重要性。我们还使用四种不同的加权图表示来学习正常和异常图的表示空间。</li>
<li>results: 我们对基线方法进行了广泛的比较，并在十个公共图像dataset上进行了广泛的测试。结果表明，我们的方法具有高效性。<details>
<summary>Abstract</summary>
Graph structure patterns are widely used to model different area data recently. How to detect anomalous graph information on these graph data has become a popular research problem. The objective of this research is centered on the particular issue that how to detect abnormal graphs within a graph set. The previous works have observed that abnormal graphs mainly show node-level and graph-level anomalies, but these methods equally treat two anomaly forms above in the evaluation of abnormal graphs, which is contrary to the fact that different types of abnormal graph data have different degrees in terms of node-level and graph-level anomalies. Furthermore, abnormal graphs that have subtle differences from normal graphs are easily escaped detection by the existing methods. Thus, we propose a multi-representations space separation based graph-level anomaly-aware detection framework in this paper. To consider the different importance of node-level and graph-level anomalies, we design an anomaly-aware module to learn the specific weight between them in the abnormal graph evaluation process. In addition, we learn strictly separate normal and abnormal graph representation spaces by four types of weighted graph representations against each other including anchor normal graphs, anchor abnormal graphs, training normal graphs, and training abnormal graphs. Based on the distance error between the graph representations of the test graph and both normal and abnormal graph representation spaces, we can accurately determine whether the test graph is anomalous. Our approach has been extensively evaluated against baseline methods using ten public graph datasets, and the results demonstrate its effectiveness.
</details>
<details>
<summary>摘要</summary>
近些年来，图structure模式广泛应用于不同领域数据模型中。但是如何检测图数据中的异常信息已成为一个热门研究问题。我们的研究目标是在图集中检测异常图。前期研究发现，异常图主要表现为节点级别和图级别异常，但是现有方法均视这两种异常形态为一样，这与实际情况不符。此外，异常图具有微妙异常特征，容易被现有方法排除。因此，我们提出了一个基于多个表示空间分离的图级别异常检测框架。为了考虑节点级别和图级别异常的不同重要性，我们设计了一个异常感知模块，用于在异常图评估过程中学习特定权重。此外，我们学习严格分别的正常和异常图表示空间，通过四种权重图表示对彼此进行比较。基于测试图与正常和异常图表示空间之间的距离错误，我们可以准确地判断测试图是否异常。我们的方法在基于基线方法的比较下进行了广泛的评估，结果表明其效果高效。
</details></li>
</ul>
<hr>
<h2 id="High-performance-real-world-optical-computing-trained-by-in-situ-model-free-optimization"><a href="#High-performance-real-world-optical-computing-trained-by-in-situ-model-free-optimization" class="headerlink" title="High-performance real-world optical computing trained by in situ model-free optimization"></a>High-performance real-world optical computing trained by in situ model-free optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11957">http://arxiv.org/abs/2307.11957</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangyuan Zhao, Xin Shu, Renjie Zhou</li>
<li>for: 提高光学计算系统的高速和低能耗数据处理能力，但面临 computationally demanding 训练和实际 gap。</li>
<li>methods: 提出了一种无模型解决方案，基于分布式扩散算法进行光学计算系统的增量优化。这种方法不需要 computation-heavy 和偏见的系统模拟，直接借鉴系统的黑盒特性，将损失反射到光学权重的概率分布中。</li>
<li>results: 通过在单层折射光学计算系统上进行实验，达到了在 MNIST 和 FMNIST 数据集上的高精度分类result。此外，还展示了其在无图像和高速细胞分析中的潜在应用 potential。<details>
<summary>Abstract</summary>
Optical computing systems can provide high-speed and low-energy data processing but face deficiencies in computationally demanding training and simulation-to-reality gap. We propose a model-free solution for lightweight in situ optimization of optical computing systems based on the score gradient estimation algorithm. This approach treats the system as a black box and back-propagates loss directly to the optical weights' probabilistic distributions, hence circumventing the need for computation-heavy and biased system simulation. We demonstrate a superior classification accuracy on the MNIST and FMNIST datasets through experiments on a single-layer diffractive optical computing system. Furthermore, we show its potential for image-free and high-speed cell analysis. The inherent simplicity of our proposed method, combined with its low demand for computational resources, expedites the transition of optical computing from laboratory demonstrations to real-world applications.
</details>
<details>
<summary>摘要</summary>
OPTICAL计算系统可提供高速和低能耗数据处理，但面临计算挑战和实际和模拟之间的差距。我们提出了一种无模型解决方案，基于分数跑分布预测算法，用于轻量级在处理器上进行位置优化。这种方法将系统看作黑盒，直接倒退损失到光学权重的概率分布上，因此不需要 computation-intensive和偏见的系统模拟。我们通过实验示出，在单层折射光计算系统上实现了MNIST和FMNIST数据集上的高精度分类。此外，我们还展示了它的潜在应用于无图像和高速细胞分析。我们的提议的简单性和计算资源的低需求，使得光学计算从实验室示范到实际应用的过渡变得更加容易。
</details></li>
</ul>
<hr>
<h2 id="Puioio-On-device-Real-Time-Smartphone-Based-Automated-Exercise-Repetition-Counting-System"><a href="#Puioio-On-device-Real-Time-Smartphone-Based-Automated-Exercise-Repetition-Counting-System" class="headerlink" title="Pūioio: On-device Real-Time Smartphone-Based Automated Exercise Repetition Counting System"></a>Pūioio: On-device Real-Time Smartphone-Based Automated Exercise Repetition Counting System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02420">http://arxiv.org/abs/2308.02420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Sinclair, Kayla Kautai, Seyed Reza Shahamiri<br>for: 这个研究的目的是为了开发一种能够在智能手机上实时计数运动 répétitions的系统，以便提高人们的 физи健身和rehabilitation 效果。methods: 这个研究使用了深度学习技术，包括pose estimation、thresholding、optical flow和状态机制，以实现智能手机上的运动 répétitions计数。results: 研究表明，这个系统在真实的测试中达到了98.89%的准确率，而在预录的视频数据集上测试时的准确率为98.85%。这表明这个系统是一种有效、低成本、便捷的解决方案，不需要特殊的硬件或敏感器，也不需要网络连接。<details>
<summary>Abstract</summary>
Automated exercise repetition counting has applications across the physical fitness realm, from personal health to rehabilitation. Motivated by the ubiquity of mobile phones and the benefits of tracking physical activity, this study explored the feasibility of counting exercise repetitions in real-time, using only on-device inference, on smartphones. In this work, after providing an extensive overview of the state-of-the-art automatic exercise repetition counting methods, we introduce a deep learning based exercise repetition counting system for smartphones consisting of five components: (1) Pose estimation, (2) Thresholding, (3) Optical flow, (4) State machine, and (5) Counter. The system is then implemented via a cross-platform mobile application named P\=uioio that uses only the smartphone camera to track repetitions in real time for three standard exercises: Squats, Push-ups, and Pull-ups. The proposed system was evaluated via a dataset of pre-recorded videos of individuals exercising as well as testing by subjects exercising in real time. Evaluation results indicated the system was 98.89% accurate in real-world tests and up to 98.85% when evaluated via the pre-recorded dataset. This makes it an effective, low-cost, and convenient alternative to existing solutions since the proposed system has minimal hardware requirements without requiring any wearable or specific sensors or network connectivity.
</details>
<details>
<summary>摘要</summary>
自动化运动重复计数有应用于身体健康和重建领域，由于移动电话的普遍和跟踪物理活动的好处，这项研究探索了使用只有移动设备逻辑进行实时计数的可能性。本文首先提供了 automatic exercise repetition counting方法的现状报告，然后引入了基于深度学习的运动重复计数系统，包括五个组件：（1）姿势估计，（2）阈值处理，（3）光流计算，（4）状态机制，（5）计数器。该系统然后通过一款跨平台移动应用程序名为P\=uioio实现，该应用程序使用移动设备摄像头实时跟踪重复进行三种标准运动：蹲squats、推手push-ups和拔手pull-ups。进行测试时，提出的系统在实际测试中达到98.89%的准确率，并且在预录视频中测试时达到98.85%的准确率。这使得该系统成为一种有效、低成本、便捷的替代方案，因为它具有最低硬件要求，没有需要佩戴式或特殊的感应器或网络连接。
</details></li>
</ul>
<hr>
<h2 id="Implicit-Interpretation-of-Importance-Weight-Aware-Updates"><a href="#Implicit-Interpretation-of-Importance-Weight-Aware-Updates" class="headerlink" title="Implicit Interpretation of Importance Weight Aware Updates"></a>Implicit Interpretation of Importance Weight Aware Updates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11955">http://arxiv.org/abs/2307.11955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keyi Chen, Francesco Orabona</li>
<li>for: 这篇论文主要应用于 convex machine learning 中的优化问题，尤其是对于 subgradient descent 的学习率调整。</li>
<li>methods: 这篇论文使用了 Importance Weight Aware (IWA) 更新方法，这是一种基于 infinitely many infinitesimal updates 的方法，可以对于每个损失函数进行无限多次微更新。</li>
<li>results: 这篇论文首次显示了 IWA 更新方法在 online learning  Setting 中的 strictly better regret upper bound，与 plain gradient updates 相比，这是由新的框架 generalized implicit Follow-the-Regularized-Leader (FTRL) 所支持。<details>
<summary>Abstract</summary>
Due to its speed and simplicity, subgradient descent is one of the most used optimization algorithms in convex machine learning algorithms. However, tuning its learning rate is probably its most severe bottleneck to achieve consistent good performance. A common way to reduce the dependency on the learning rate is to use implicit/proximal updates. One such variant is the Importance Weight Aware (IWA) updates, which consist of infinitely many infinitesimal updates on each loss function. However, IWA updates' empirical success is not completely explained by their theory. In this paper, we show for the first time that IWA updates have a strictly better regret upper bound than plain gradient updates in the online learning setting. Our analysis is based on the new framework, generalized implicit Follow-the-Regularized-Leader (FTRL) (Chen and Orabona, 2023), to analyze generalized implicit updates using a dual formulation. In particular, our results imply that IWA updates can be considered as approximate implicit/proximal updates.
</details>
<details>
<summary>摘要</summary>
由于它的速度和简洁性，梯度下降是 convex 机器学习算法中最常用的优化算法之一。然而，调整它的学习率是它最大的瓶颈，以实现一致良好的性能。一种常见的减少学习率依赖性的方法是使用隐式/ proximal 更新。一种such variant是强迫权重 aware（IWA）更新，它包括无限多个infinitesimal更新。然而， IWA 更新的实际成功并不完全由其理论 explain。在这篇论文中，我们表明，IWA 更新在在线学习设定下具有 strictly better regret upper bound than plain gradient updates。我们的分析基于新的框架，Generalized Implicit Follow-the-Regularized-Leader（FTRL）（Chen and Orabona, 2023），用于分析通用隐式更新。具体来说，我们的结果表明，IWA 更新可以被视为approximate隐式/ proximal 更新。
</details></li>
</ul>
<hr>
<h2 id="On-Robot-Bayesian-Reinforcement-Learning-for-POMDPs"><a href="#On-Robot-Bayesian-Reinforcement-Learning-for-POMDPs" class="headerlink" title="On-Robot Bayesian Reinforcement Learning for POMDPs"></a>On-Robot Bayesian Reinforcement Learning for POMDPs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11954">http://arxiv.org/abs/2307.11954</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hai Nguyen, Sammie Katt, Yuchen Xiao, Christopher Amato</li>
<li>for: 这个论文的目的是提出一种专门针对物理系统的 bayesian 强化学习方法，以解决人工智能学习 robotics 中的数据成本问题。</li>
<li>methods: 该方法使用了一种专门的 factored 表示方法，将专家知识captured，然后证明 posterior 会factorize 成相似的形式，并最终将其формализова为 bayesian 框架。它还提出了一种基于 Monte-Carlo tree search 和 particle filtering 的在线解决方法，可以利用typical low-level robot simulators 和处理未知环境的不确定性。</li>
<li>results: 该方法在两个人机交互任务中实现了near-optimal 性能，只需要一些实际世界的 episodenumbers。一个视频 displaying learned policies 可以在 <a target="_blank" rel="noopener" href="https://youtu.be/H9xp60ngOes">https://youtu.be/H9xp60ngOes</a> 找到。<details>
<summary>Abstract</summary>
Robot learning is often difficult due to the expense of gathering data. The need for large amounts of data can, and should, be tackled with effective algorithms and leveraging expert information on robot dynamics. Bayesian reinforcement learning (BRL), thanks to its sample efficiency and ability to exploit prior knowledge, is uniquely positioned as such a solution method. Unfortunately, the application of BRL has been limited due to the difficulties of representing expert knowledge as well as solving the subsequent inference problem. This paper advances BRL for robotics by proposing a specialized framework for physical systems. In particular, we capture this knowledge in a factored representation, then demonstrate the posterior factorizes in a similar shape, and ultimately formalize the model in a Bayesian framework. We then introduce a sample-based online solution method, based on Monte-Carlo tree search and particle filtering, specialized to solve the resulting model. This approach can, for example, utilize typical low-level robot simulators and handle uncertainty over unknown dynamics of the environment. We empirically demonstrate its efficiency by performing on-robot learning in two human-robot interaction tasks with uncertainty about human behavior, achieving near-optimal performance after only a handful of real-world episodes. A video of learned policies is at https://youtu.be/H9xp60ngOes.
</details>
<details>
<summary>摘要</summary>
瑞博特学习（BRL）通常受到数据成本的限制，因为需要大量数据来训练。然而，BRL因其效率和利用专家知识的能力而成为一个有效的解决方案。然而，BRL的应用受到专家知识表达和后续推理问题的限制。这篇论文推广BRL在 робо特领域，通过特殊的框架来捕捉专家知识。具体来说，我们将知识表示为一个分解表示，并证明 posterior 会分解成类似的形式。最后，我们将模型归纳到 Bayesian 框架中。然后，我们引入一种基于 Monte-Carlo 搜索和粒子筛选的在线解决方法，特化用于解决 resulting 模型。这种方法可以利用常见的低级 robot 模拟器，并处理不确定的环境动力学的不确定性。我们在两个人机交互任务中进行了实验，并达到了几乎最佳性能，只需要几个真实世界的回合。视频展示学习政策的详细信息请参考 <https://youtu.be/H9xp60ngOes>。
</details></li>
</ul>
<hr>
<h2 id="HIQL-Offline-Goal-Conditioned-RL-with-Latent-States-as-Actions"><a href="#HIQL-Offline-Goal-Conditioned-RL-with-Latent-States-as-Actions" class="headerlink" title="HIQL: Offline Goal-Conditioned RL with Latent States as Actions"></a>HIQL: Offline Goal-Conditioned RL with Latent States as Actions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11949">http://arxiv.org/abs/2307.11949</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/seohongpark/hiql">https://github.com/seohongpark/hiql</a></li>
<li>paper_authors: Seohong Park, Dibya Ghosh, Benjamin Eysenbach, Sergey Levine</li>
<li>for: 该论文旨在提出一种基于未经过supervision的自适应RL算法，可以直接从大量无标签数据中学习。</li>
<li>methods: 该算法使用一个不含操作的价值函数，通过分层的 decomposición 学习两个策略：一个高级策略使用状态作为动作，预测子目标，以及一个低级策略预测达到子目标所需的操作。</li>
<li>results: 通过分析和示例，该方法可以减少估计价值函数中的噪声，并应用于Offline goal-reaching benchmark，解决远程目标的问题，可以扩展到高维图像观察数据，并可以使用无操作数据进行学习。<details>
<summary>Abstract</summary>
Unsupervised pre-training has recently become the bedrock for computer vision and natural language processing. In reinforcement learning (RL), goal-conditioned RL can potentially provide an analogous self-supervised approach for making use of large quantities of unlabeled (reward-free) data. However, building effective algorithms for goal-conditioned RL that can learn directly from diverse offline data is challenging, because it is hard to accurately estimate the exact value function for faraway goals. Nonetheless, goal-reaching problems exhibit structure, such that reaching distant goals entails first passing through closer subgoals. This structure can be very useful, as assessing the quality of actions for nearby goals is typically easier than for more distant goals. Based on this idea, we propose a hierarchical algorithm for goal-conditioned RL from offline data. Using one action-free value function, we learn two policies that allow us to exploit this structure: a high-level policy that treats states as actions and predicts (a latent representation of) a subgoal and a low-level policy that predicts the action for reaching this subgoal. Through analysis and didactic examples, we show how this hierarchical decomposition makes our method robust to noise in the estimated value function. We then apply our method to offline goal-reaching benchmarks, showing that our method can solve long-horizon tasks that stymie prior methods, can scale to high-dimensional image observations, and can readily make use of action-free data. Our code is available at https://seohong.me/projects/hiql/
</details>
<details>
<summary>摘要</summary>
Recently, unsupervised pre-training has become the foundation for computer vision and natural language processing. In reinforcement learning (RL), goal-conditioned RL has the potential to provide a self-supervised approach that can utilize large amounts of unlabeled (reward-free) data. However, building effective algorithms for goal-conditioned RL that can learn directly from diverse offline data is challenging, as it is difficult to accurately estimate the exact value function for faraway goals. Nonetheless, goal-reaching problems exhibit structure, such that reaching distant goals involves first passing through closer subgoals. This structure can be very useful, as assessing the quality of actions for nearby goals is typically easier than for more distant goals. Based on this idea, we propose a hierarchical algorithm for goal-conditioned RL from offline data. Using one action-free value function, we learn two policies: a high-level policy that treats states as actions and predicts (a latent representation of) a subgoal, and a low-level policy that predicts the action for reaching this subgoal. Through analysis and didactic examples, we show how this hierarchical decomposition makes our method robust to noise in the estimated value function. We then apply our method to offline goal-reaching benchmarks, showing that our method can solve long-horizon tasks that stymie prior methods, can scale to high-dimensional image observations, and can readily make use of action-free data. Our code is available at <https://seohong.me/projects/hiql/>.Here's the translation in Traditional Chinese:最近，无监督预训学（pre-training）已经成为计算机视觉和自然语言处理的基础。在征募学习（RL）中，目标受控RL有可能提供一个自我指导的方法，可以对大量的无条件数据进行学习。然而，建立有效的目标受控RL算法，可以从多元的过去数据中直接学习，是一个挑战。这是因为，评估远方目标的价值函数是很难精确地估计的。然而，目标实现问题会展示结构，例如，要到达更远的目标时，通常需要先通过更近的子目标。这种结构可以非常有用，因为评估近距离的目标是比较容易的。基于这个想法，我们提出了一个层次化的算法，从过去数据中学习目标受控RL。使用一个无动作的值函数，我们学习了两个政策：一个高层政策，将状态视为动作，预测（一个隐藏表示）子目标，以及一个低层政策，预测用于到达子目标的动作。通过分析和示例，我们显示了这个层次化分解的优点，使我们的方法更准确地处理错误估计的值函数。然后，我们将我们的方法应用到过去目标实现的参考数据上，展示了我们的方法可以解决长时间任务，超越先前的方法，可以扩展到高维影像观察，并且可以快速地使用无动作数据。我们的代码可以在 <https://seohong.me/projects/hiql/> 获取。
</details></li>
</ul>
<hr>
<h2 id="The-instabilities-of-large-learning-rate-training-a-loss-landscape-view"><a href="#The-instabilities-of-large-learning-rate-training-a-loss-landscape-view" class="headerlink" title="The instabilities of large learning rate training: a loss landscape view"></a>The instabilities of large learning rate training: a loss landscape view</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11948">http://arxiv.org/abs/2307.11948</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lawrence Wang, Stephen Roberts</li>
<li>for: 研究深度学习网络训练中大学习率较大的稳定性问题。</li>
<li>methods: 使用Hessian矩阵来描述损失函数的梯度场，描述梯度搅拌的不稳定性。</li>
<li>results: 发现训练过程中存在“梯度场平整”和“梯度场转移”两种现象，这两种现象与训练不稳定性有直接关系。<details>
<summary>Abstract</summary>
Modern neural networks are undeniably successful. Numerous works study how the curvature of loss landscapes can affect the quality of solutions. In this work we study the loss landscape by considering the Hessian matrix during network training with large learning rates - an attractive regime that is (in)famously unstable. We characterise the instabilities of gradient descent, and we observe the striking phenomena of \textit{landscape flattening} and \textit{landscape shift}, both of which are intimately connected to the instabilities of training.
</details>
<details>
<summary>摘要</summary>
现代神经网络确实非常成功。许多研究证明损失函数的曲率对解决方案质量有很大影响。在这个工作中，我们研究损失函数的 landscape，通过考虑大学习率时的希尔бер特矩阵。我们描述了梯度下降的不稳定性，并观察了“ landscape flattening”和“ landscape shift”这两种phenomena，它们与训练不稳定性有着密切的关系。
</details></li>
</ul>
<hr>
<h2 id="Collaboratively-Learning-Linear-Models-with-Structured-Missing-Data"><a href="#Collaboratively-Learning-Linear-Models-with-Structured-Missing-Data" class="headerlink" title="Collaboratively Learning Linear Models with Structured Missing Data"></a>Collaboratively Learning Linear Models with Structured Missing Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11947">http://arxiv.org/abs/2307.11947</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Cheng, Gary Cheng, John Duchi</li>
<li>for: 本文研究了多个Agent共同学习最小二乘估计问题，每个Agent观察不同的特征子数据（例如感知器的分解）。目标是为每个Agent生成最佳估计器。</li>
<li>methods: 我们提出了一种分布式、半监督的算法Collab，包括三个步骤：本地训练、聚合和分布。我们的方法不需要传输标注数据，因此是通信效率高的。</li>
<li>results: 我们的方法可以在实际数据和synthetic数据上测试，并且nearly asymptotically local minimax优秀，即在标注数据不可用的情况下，我们的方法可以与可以通信标注数据的估计器相比。<details>
<summary>Abstract</summary>
We study the problem of collaboratively learning least squares estimates for $m$ agents. Each agent observes a different subset of the features$\unicode{x2013}$e.g., containing data collected from sensors of varying resolution. Our goal is to determine how to coordinate the agents in order to produce the best estimator for each agent. We propose a distributed, semi-supervised algorithm Collab, consisting of three steps: local training, aggregation, and distribution. Our procedure does not require communicating the labeled data, making it communication efficient and useful in settings where the labeled data is inaccessible. Despite this handicap, our procedure is nearly asymptotically local minimax optimal$\unicode{x2013}$even among estimators allowed to communicate the labeled data such as imputation methods. We test our method on real and synthetic data.
</details>
<details>
<summary>摘要</summary>
我们研究多个代理人共同学习最小二乘估计问题。每个代理人都观察不同的特征子集合，例如感应器的分解度不同。我们的目标是让代理人们如何协调，以生成每个代理人最好的估计器。我们提出了分布式、半监督的算法Collab，包括三个步骤：本地训练、聚合和分布。我们的程序不需要通过实际资料进行通信，因此它具有通信效率的优点，特别在资料权限严格的情况下。尽管如此，我们的程序仍然几乎在极限情况下对局部最小最佳，甚至在允许交流实际资料的情况下，如填充方法。我们在实际数据和 sintetic 数据上进行测试。
</details></li>
</ul>
<hr>
<h2 id="Batch-Clipping-and-Adaptive-Layerwise-Clipping-for-Differential-Private-Stochastic-Gradient-Descent"><a href="#Batch-Clipping-and-Adaptive-Layerwise-Clipping-for-Differential-Private-Stochastic-Gradient-Descent" class="headerlink" title="Batch Clipping and Adaptive Layerwise Clipping for Differential Private Stochastic Gradient Descent"></a>Batch Clipping and Adaptive Layerwise Clipping for Differential Private Stochastic Gradient Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11939">http://arxiv.org/abs/2307.11939</a></li>
<li>repo_url: None</li>
<li>paper_authors: Toan N. Nguyen, Phuong Ha Nguyen, Lam M. Nguyen, Marten Van Dijk</li>
<li>for: 这 paper 是为了研究 differential private stochastic gradient descent (DPSGD) 的改进方法，以及其在深度神经网络中的应用。</li>
<li>methods: 这 paper 使用了一种称为批量剪裁 (Batch Clipping, BC) 的方法，以及一种称为层次剪裁 (Adaptive Layerwise Clipping, ALC) 的方法。这两种方法都是为了使得 DPSGD 可以在深度神经网络中使用批处理 (batch normalization layers, BNL)。</li>
<li>results: 实验表明，使用这 two 种方法可以使得 DPSGD 在 CIFAR-$10 $ 上的 resnet-$18 $ 模型进行训练，而原始 DPSGD 则不能进行训练。<details>
<summary>Abstract</summary>
Each round in Differential Private Stochastic Gradient Descent (DPSGD) transmits a sum of clipped gradients obfuscated with Gaussian noise to a central server which uses this to update a global model which often represents a deep neural network. Since the clipped gradients are computed separately, which we call Individual Clipping (IC), deep neural networks like resnet-18 cannot use Batch Normalization Layers (BNL) which is a crucial component in deep neural networks for achieving a high accuracy. To utilize BNL, we introduce Batch Clipping (BC) where, instead of clipping single gradients as in the orginal DPSGD, we average and clip batches of gradients. Moreover, the model entries of different layers have different sensitivities to the added Gaussian noise. Therefore, Adaptive Layerwise Clipping methods (ALC), where each layer has its own adaptively finetuned clipping constant, have been introduced and studied, but so far without rigorous DP proofs. In this paper, we propose {\em a new ALC and provide rigorous DP proofs for both BC and ALC}. Experiments show that our modified DPSGD with BC and ALC for CIFAR-$10$ with resnet-$18$ converges while DPSGD with IC and ALC does not.
</details>
<details>
<summary>摘要</summary>
每回轮在差分私人随机梯度下降（DPSGD）中都会将折叠后的梯度进行干扰加速，并将其发送到中央服务器，以更新一个全球模型，通常是深度神经网络。由于折叠后的梯度在不同层中 computing  separately，因此深度神经网络如 ResNet-18 不能使用批 normalization layer（BNL），这是深度神经网络实现高精度的一个关键组件。为了使用 BNL，我们引入批 clipping（BC），其中，而不是单个梯度的 clipping，我们将批量梯度进行平均和折叠。此外，不同层的模型元素对添加的 Gaussian 噪声有不同的感itivity。因此，我们引入 adaptive layerwise clipping 方法（ALC），其中每个层有自适应地调整的 clipping 常量。在这篇论文中，我们提出了一种新的 ALC，并提供了准确的 DP 证明。实验表明，我们修改后的 DPSGD  WITH BC 和 ALC 可以在 CIFAR-10 上 WITH ResNet-18  converges，而 DPSGD  WITH IC 和 ALC 则不可以。
</details></li>
</ul>
<hr>
<h2 id="Mercer-Large-Scale-Kernel-Machines-from-Ridge-Function-Perspective"><a href="#Mercer-Large-Scale-Kernel-Machines-from-Ridge-Function-Perspective" class="headerlink" title="Mercer Large-Scale Kernel Machines from Ridge Function Perspective"></a>Mercer Large-Scale Kernel Machines from Ridge Function Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11925">http://arxiv.org/abs/2307.11925</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karol Dziedziul, Sergey Kryzhevich</li>
<li>for: 本文提出了一种基于ridge函数的大规模kernel机器学习方法，并通过林-毕肖斯 theorem的推广来解释其效果。</li>
<li>methods: 本文使用了Random features方法，并通过近似理论来研究可以通过权重的积分来approxmiate各种kernel函数。</li>
<li>results: 本文的结果表明，可以通过权重积分来approxmiate各种kernel函数，但是存在一些障碍，如果要在大规模数据集上使用这种方法。这些结果可能在深度学习中有各种应用，特别是在图像处理领域。<details>
<summary>Abstract</summary>
To present Mercer large-scale kernel machines from a ridge function perspective, we recall the results by Lin and Pinkus from Fundamentality of ridge functions. We consider the main theorem of the recent paper by Rachimi and Recht, 2008, Random features for large-scale kernel machines in terms of the Approximation Theory. We study which kernels can be approximated by a sum of cosine function products with arguments depending on $x$ and $y$ and present the obstacles of such an approach. The results of this article may have various applications in Deep Learning, especially in problems related to Image Processing.
</details>
<details>
<summary>摘要</summary>
将梅瑞较大批量机器学习从ridge函数角度提出，我们回忆了林和毕普斯的诸果。我们考虑了2008年rachimi和recht的论文《随机特征 для大规模kernel机器学习》中的主定理，我们研究了可以通过cosine函数乘积来近似哪些kernels，并描述了这种方法的障碍。这些结果可能在深度学习中有各种应用，特别是在图像处理问题中。
</details></li>
</ul>
<hr>
<h2 id="Selective-Perception-Optimizing-State-Descriptions-with-Reinforcement-Learning-for-Language-Model-Actors"><a href="#Selective-Perception-Optimizing-State-Descriptions-with-Reinforcement-Learning-for-Language-Model-Actors" class="headerlink" title="Selective Perception: Optimizing State Descriptions with Reinforcement Learning for Language Model Actors"></a>Selective Perception: Optimizing State Descriptions with Reinforcement Learning for Language Model Actors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11922">http://arxiv.org/abs/2307.11922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kolby Nottingham, Yasaman Razeghi, Kyungmin Kim, JB Lanier, Pierre Baldi, Roy Fox, Sameer Singh</li>
<li>for: 这个论文主要针对的是使用大语言模型（LLM）进行序列决策任务，特别是在 робо控和游戏等领域。</li>
<li>methods: 这篇论文提出了一种自动选择简洁状态描述的方法，名叫“简洁语言输入 для决策响应”（BLINDER）。该方法通过学习任务条件下的状态描述价值函数来自动选择简洁的状态描述。</li>
<li>results: 在 NetHack 游戏和 robotic manipulation 任务中，BLINDER 方法能够提高任务成功率，降低输入大小和计算成本，并在不同的 LLM actor 之间进行泛化。<details>
<summary>Abstract</summary>
Large language models (LLMs) are being applied as actors for sequential decision making tasks in domains such as robotics and games, utilizing their general world knowledge and planning abilities. However, previous work does little to explore what environment state information is provided to LLM actors via language. Exhaustively describing high-dimensional states can impair performance and raise inference costs for LLM actors. Previous LLM actors avoid the issue by relying on hand-engineered, task-specific protocols to determine which features to communicate about a state and which to leave out. In this work, we propose Brief Language INputs for DEcision-making Responses (BLINDER), a method for automatically selecting concise state descriptions by learning a value function for task-conditioned state descriptions. We evaluate BLINDER on the challenging video game NetHack and a robotic manipulation task. Our method improves task success rate, reduces input size and compute costs, and generalizes between LLM actors.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Poverty-rate-prediction-using-multi-modal-survey-and-earth-observation-data"><a href="#Poverty-rate-prediction-using-multi-modal-survey-and-earth-observation-data" class="headerlink" title="Poverty rate prediction using multi-modal survey and earth observation data"></a>Poverty rate prediction using multi-modal survey and earth observation data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11921">http://arxiv.org/abs/2307.11921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simone Fobi, Manuel Cardona, Elliott Collins, Caleb Robinson, Anthony Ortiz, Tina Sederholm, Rahul Dodhia, Juan Lavista Ferres</li>
<li>For: 这种方法用于将家庭民调数据和卫星影像特征结合以预测地区贫困率。* Methods: 该方法使用单步特征提取方法生成10m&#x2F;px Sentinal-2表面反射卫星影像的视觉特征，然后与民调问题相结合以实现贫困率的估计。* Results: 包含卫星影像特征的代表测试集上的贫困率估计 error 从4.09%降低至3.88%，而包含小问题的选择方法可以further reduce error to 3.71%。这些方法还显示出卫星影像特征具有地区和城市化差异的特征。<details>
<summary>Abstract</summary>
This work presents an approach for combining household demographic and living standards survey questions with features derived from satellite imagery to predict the poverty rate of a region. Our approach utilizes visual features obtained from a single-step featurization method applied to freely available 10m/px Sentinel-2 surface reflectance satellite imagery. These visual features are combined with ten survey questions in a proxy means test (PMT) to estimate whether a household is below the poverty line. We show that the inclusion of visual features reduces the mean error in poverty rate estimates from 4.09% to 3.88% over a nationally representative out-of-sample test set. In addition to including satellite imagery features in proxy means tests, we propose an approach for selecting a subset of survey questions that are complementary to the visual features extracted from satellite imagery. Specifically, we design a survey variable selection approach guided by the full survey and image features and use the approach to determine the most relevant set of small survey questions to include in a PMT. We validate the choice of small survey questions in a downstream task of predicting the poverty rate using the small set of questions. This approach results in the best performance -- errors in poverty rate decrease from 4.09% to 3.71%. We show that extracted visual features encode geographic and urbanization differences between regions.
</details>
<details>
<summary>摘要</summary>
In addition to including satellite imagery features in the PMT, the approach proposes a method for selecting a subset of survey questions that are complementary to the visual features extracted from satellite imagery. The approach uses a survey variable selection method guided by the full survey and image features to determine the most relevant set of small survey questions to include in the PMT. The choice of small survey questions is validated in a downstream task of predicting the poverty rate, resulting in the best performance with errors in poverty rate decreasing from 4.09% to 3.71%.The approach shows that the extracted visual features encode geographic and urbanization differences between regions, providing a more accurate estimate of poverty rates. By combining survey questions with satellite imagery features, the approach provides a more comprehensive and accurate assessment of poverty rates, particularly in areas with limited survey data.
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Vulnerabilities-in-Interpretable-Deep-Learning-Systems-with-Query-Efficient-Black-box-Attacks"><a href="#Unveiling-Vulnerabilities-in-Interpretable-Deep-Learning-Systems-with-Query-Efficient-Black-box-Attacks" class="headerlink" title="Unveiling Vulnerabilities in Interpretable Deep Learning Systems with Query-Efficient Black-box Attacks"></a>Unveiling Vulnerabilities in Interpretable Deep Learning Systems with Query-Efficient Black-box Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11906">http://arxiv.org/abs/2307.11906</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eldor Abdukhamidov, Mohammed Abuhamad, Simon S. Woo, Eric Chan-Tin, Tamer Abuhmed</li>
<li>For: This paper aims to demonstrate a novel black-box attack against Interpretable Deep Learning Systems (IDLSes), which can compromise the integrity and reliability of these systems.* Methods: The proposed attack uses a microbial genetic algorithm-based approach that combines transfer-based and score-based methods, and requires no prior knowledge of the target model or its interpretation model.* Results: The proposed attack achieves high attack success rates using adversarial examples with attribution maps that are highly similar to those of benign samples, making it difficult to detect even by human analysts. The results highlight the need for improved IDLS security to ensure their practical reliability.Here’s the summary in Traditional Chinese:* For: 这篇论文旨在展示一种黑盒攻击 against Interpretable Deep Learning Systems (IDLSes)，以确保这些系统的可靠性和可信度。* Methods: 提案的攻击使用了微生物遗传学算法基本的方法，融合了转移基本和分数基本方法，并不需要对目标模型和其解释模型的内容有任何专业知识。* Results: 提案的攻击得到了高的攻击成功率，使用了对抗例显示对应图的高相似性，让人类分析师难以探测。结果显示了IDLS安全性的需求，以确保它们在实际应用中的可靠性。<details>
<summary>Abstract</summary>
Deep learning has been rapidly employed in many applications revolutionizing many industries, but it is known to be vulnerable to adversarial attacks. Such attacks pose a serious threat to deep learning-based systems compromising their integrity, reliability, and trust. Interpretable Deep Learning Systems (IDLSes) are designed to make the system more transparent and explainable, but they are also shown to be susceptible to attacks. In this work, we propose a novel microbial genetic algorithm-based black-box attack against IDLSes that requires no prior knowledge of the target model and its interpretation model. The proposed attack is a query-efficient approach that combines transfer-based and score-based methods, making it a powerful tool to unveil IDLS vulnerabilities. Our experiments of the attack show high attack success rates using adversarial examples with attribution maps that are highly similar to those of benign samples which makes it difficult to detect even by human analysts. Our results highlight the need for improved IDLS security to ensure their practical reliability.
</details>
<details>
<summary>摘要</summary>
深度学习已经广泛应用在许多领域，但它们知道易受到敌意攻击。这些攻击会对深度学习基于系统造成严重的威胁，打乱它们的完整性、可靠性和信任worth。可读性深度学习系统（IDLS）是为了让系统更加透明和可解释的，但它们也被示出可能受到攻击。在这项工作中，我们提出了一种基于微生物遗传算法的黑盒攻击方法，不需要攻击目标模型和其解释模型的先前知识。我们的攻击方法是一种高效的查询方法，它结合了传输基于方法和分数基于方法，使其成为对 IDLS 的攻击工具。我们的实验表明，使用挑战性示例和归属地图，可以得到高攻击成功率，而且这些示例与正常样本具有高度相似性，使其具有极具潜在攻击力。我们的结果表明，为了确保 IDLS 的实际可靠性，需要进一步加强 IDLS 的安全性。
</details></li>
</ul>
<hr>
<h2 id="Model-Compression-Methods-for-YOLOv5-A-Review"><a href="#Model-Compression-Methods-for-YOLOv5-A-Review" class="headerlink" title="Model Compression Methods for YOLOv5: A Review"></a>Model Compression Methods for YOLOv5: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11904">http://arxiv.org/abs/2307.11904</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Jani, Jamil Fayyad, Younes Al-Younes, Homayoun Najjaran</li>
<li>for: 本文主要针对Resource-constrained edge devices上部署YOLOv5对象检测器，以提高其精度和效率。</li>
<li>methods: 本文主要采用Network pruning和Quantization两种方法来压缩YOLOv5模型，以降低内存使用量和执行时间。</li>
<li>results: 经过实践和分析，我们发现在YOLOv5上应用Network pruning和Quantization方法可以降低内存使用量和执行时间，但是还存在一些问题需要进一步解决。<details>
<summary>Abstract</summary>
Over the past few years, extensive research has been devoted to enhancing YOLO object detectors. Since its introduction, eight major versions of YOLO have been introduced with the purpose of improving its accuracy and efficiency. While the evident merits of YOLO have yielded to its extensive use in many areas, deploying it on resource-limited devices poses challenges. To address this issue, various neural network compression methods have been developed, which fall under three main categories, namely network pruning, quantization, and knowledge distillation. The fruitful outcomes of utilizing model compression methods, such as lowering memory usage and inference time, make them favorable, if not necessary, for deploying large neural networks on hardware-constrained edge devices. In this review paper, our focus is on pruning and quantization due to their comparative modularity. We categorize them and analyze the practical results of applying those methods to YOLOv5. By doing so, we identify gaps in adapting pruning and quantization for compressing YOLOv5, and provide future directions in this area for further exploration. Among several versions of YOLO, we specifically choose YOLOv5 for its excellent trade-off between recency and popularity in literature. This is the first specific review paper that surveys pruning and quantization methods from an implementation point of view on YOLOv5. Our study is also extendable to newer versions of YOLO as implementing them on resource-limited devices poses the same challenges that persist even today. This paper targets those interested in the practical deployment of model compression methods on YOLOv5, and in exploring different compression techniques that can be used for subsequent versions of YOLO.
</details>
<details>
<summary>摘要</summary>
过去几年，对 YOLO 对象检测器进行了广泛的研究，以提高其精度和效率。自其引入以来，共有八个主要版本的 YOLO 发布，以提高其精度和效率。虽然 YOLO 的优点已经得到了广泛的应用，但在资源有限的设备上部署它存在挑战。为解决这个问题，多种神经网络压缩方法被开发出来，这些方法可以分为三个主要类别：网络剪辑、量化和知识传递。使用这些压缩方法可以降低内存使用量和执行时间，因此在硬件限制的边缘设备上部署大型神经网络变得可能。在本文中，我们将关注剪辑和量化，因为它们在模型压缩方面具有相对的可模块性。我们将这些方法进行分类和分析，并通过应用这些方法于 YOLOv5 来确定它们的实际效果。通过这些研究，我们可以了解剪辑和量化在 YOLOv5 上的应用存在哪些挑战，并提供未来的探索方向。在多个 YOLO 版本中，我们选择了 YOLOv5，因为它在文献中的最新和最受欢迎性兼有。这是对 YOLOv5 模型压缩方法的实现点视图的第一篇评论文。我们的研究也可以扩展到 newer 版本的 YOLO，因为在资源有限的设备上部署它们也存在同样的挑战。本文针对那些关注实际部署模型压缩方法的人，以及愿意探索不同的压缩技术，以便应用于未来的 YOLO 版本。
</details></li>
</ul>
<hr>
<h2 id="Project-Florida-Federated-Learning-Made-Easy"><a href="#Project-Florida-Federated-Learning-Made-Easy" class="headerlink" title="Project Florida: Federated Learning Made Easy"></a>Project Florida: Federated Learning Made Easy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11899">http://arxiv.org/abs/2307.11899</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Madrigal Diaz, Andre Manoel, Jialei Chen, Nalin Singal, Robert Sim</li>
<li>for: 该论文目的是推介一种基于 Federated Learning（FL）的大规模分布式学习解决方案，以便在不同设备和存储系统之间实现数据privacy和安全性。</li>
<li>methods: 该论文使用了分布式学习的技术，包括模型分布式训练、客户端代码更新和中央整理器的集成，以实现在不同设备和存储系统之间进行大规模分布式学习。</li>
<li>results: 该论文通过提供云主机的基础设施和相关任务管理界面，以及支持多种编程语言的多平台SDK，实现了跨设备的分布式学习解决方案，并在多种操作系统和硬件特性下进行了示例实验，以证明系统的能力。<details>
<summary>Abstract</summary>
We present Project Florida, a system architecture and software development kit (SDK) enabling deployment of large-scale Federated Learning (FL) solutions across a heterogeneous device ecosystem. Federated learning is an approach to machine learning based on a strong data sovereignty principle, i.e., that privacy and security of data is best enabled by storing it at its origin, whether on end-user devices or in segregated cloud storage silos. Federated learning enables model training across devices and silos while the training data remains within its security boundary, by distributing a model snapshot to a client running inside the boundary, running client code to update the model, and then aggregating updated snapshots across many clients in a central orchestrator. Deploying a FL solution requires implementation of complex privacy and security mechanisms as well as scalable orchestration infrastructure. Scale and performance is a paramount concern, as the model training process benefits from full participation of many client devices, which may have a wide variety of performance characteristics. Project Florida aims to simplify the task of deploying cross-device FL solutions by providing cloud-hosted infrastructure and accompanying task management interfaces, as well as a multi-platform SDK supporting most major programming languages including C++, Java, and Python, enabling FL training across a wide range of operating system (OS) and hardware specifications. The architecture decouples service management from the FL workflow, enabling a cloud service provider to deliver FL-as-a-service (FLaaS) to ML engineers and application developers. We present an overview of Florida, including a description of the architecture, sample code, and illustrative experiments demonstrating system capabilities.
</details>
<details>
<summary>摘要</summary>
我们现在介绍Project Florida，一个系统架构和软件开发工具包（SDK），允许在多种设备生态系统上部署大规模联合学习（FL）解决方案。联合学习是一种基于强大数据主权原则的机器学习方法，即数据的隐私和安全性最好是在数据的原始位置（在客户端设备或分隔的云存储囊中）保持。联合学习允许在客户端设备和存储囊之间进行模型训练，而不需要将数据传输到外部，只需将模型快照分发到客户端上，让客户端运行代码来更新模型，然后将更新后的快照集中在中央抽象器中。实施联合学习解决方案需要实施复杂的隐私和安全机制，以及可扩展的集成基础设施。因为模型训练过程具有全面参与的客户端设备，这些设备可能具有多种性能特点，因此缩放和性能是一个关键问题。Project Florida goal is to simplify the task of deploying cross-device FL solutions by providing cloud-hosted infrastructure and accompanying task management interfaces, as well as a multi-platform SDK supporting most major programming languages including C++, Java, and Python, enabling FL training across a wide range of operating system (OS) and hardware specifications. The architecture decouples service management from the FL workflow, enabling a cloud service provider to deliver FL-as-a-service (FLaaS) to ML engineers and application developers. We present an overview of Florida, including a description of the architecture, sample code, and illustrative experiments demonstrating system capabilities.
</details></li>
</ul>
<hr>
<h2 id="Hindsight-DICE-Stable-Credit-Assignment-for-Deep-Reinforcement-Learning"><a href="#Hindsight-DICE-Stable-Credit-Assignment-for-Deep-Reinforcement-Learning" class="headerlink" title="Hindsight-DICE: Stable Credit Assignment for Deep Reinforcement Learning"></a>Hindsight-DICE: Stable Credit Assignment for Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11897">http://arxiv.org/abs/2307.11897</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/skandavaidyanath/credit-assignment">https://github.com/skandavaidyanath/credit-assignment</a></li>
<li>paper_authors: Akash Velu, Skanda Vaidyanath, Dilip Arumugam</li>
<li>for: 解决缺乏评价反馈的环境下的决策问题</li>
<li>methods: 使用已有的重要性评估比例估计技术来稳定化和改善基eline方法</li>
<li>results: 提高了稳定性和效率，适用于各种困难的环境中Here’s a breakdown of each point:</li>
<li>for: The paper is written to solve the problem of sequential decision-making in environments with limited evaluative feedback, specifically the challenge of credit assignment.</li>
<li>methods: The paper uses existing importance-sampling ratio estimation techniques to improve the stability and efficiency of hindsight policy methods.</li>
<li>results: The paper shows that the proposed method improves the stability and efficiency of hindsight policy methods, and is applicable to a broad range of environments.<details>
<summary>Abstract</summary>
Oftentimes, environments for sequential decision-making problems can be quite sparse in the provision of evaluative feedback to guide reinforcement-learning agents. In the extreme case, long trajectories of behavior are merely punctuated with a single terminal feedback signal, leading to a significant temporal delay between the observation of a non-trivial reward and the individual steps of behavior culpable for achieving said reward. Coping with such a credit assignment challenge is one of the hallmark characteristics of reinforcement learning. While prior work has introduced the concept of hindsight policies to develop a theoretically moxtivated method for reweighting on-policy data by impact on achieving the observed trajectory return, we show that these methods experience instabilities which lead to inefficient learning in complex environments. In this work, we adapt existing importance-sampling ratio estimation techniques for off-policy evaluation to drastically improve the stability and efficiency of these so-called hindsight policy methods. Our hindsight distribution correction facilitates stable, efficient learning across a broad range of environments where credit assignment plagues baseline methods.
</details>
<details>
<summary>摘要</summary>
frequently, decision-making environments lack evaluative feedback to guide reinforcement-learning agents. In extreme cases, long behavior trajectories are only punctuated by a single terminal feedback signal, resulting in a significant delay between observing a non-trivial reward and identifying the specific actions responsible for earning said reward. This credit assignment challenge is a hallmark of reinforcement learning. Prior work has introduced hindsight policies to develop a theoretically motivated method for reweighting on-policy data based on its impact on achieving the observed trajectory return. However, these methods experience instabilities that lead to inefficient learning in complex environments. In this work, we adapt existing importance-sampling ratio estimation techniques for off-policy evaluation to significantly improve the stability and efficiency of hindsight policy methods. Our hindsight distribution correction enables stable, efficient learning across a broad range of environments where credit assignment plagues baseline methods.
</details></li>
</ul>
<hr>
<h2 id="On-the-Vulnerability-of-Fairness-Constrained-Learning-to-Malicious-Noise"><a href="#On-the-Vulnerability-of-Fairness-Constrained-Learning-to-Malicious-Noise" class="headerlink" title="On the Vulnerability of Fairness Constrained Learning to Malicious Noise"></a>On the Vulnerability of Fairness Constrained Learning to Malicious Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11892">http://arxiv.org/abs/2307.11892</a></li>
<li>repo_url: None</li>
<li>paper_authors: Avrim Blum, Princewill Okoroafor, Aadirupa Saha, Kevin Stangl</li>
<li>for: 这些研究探讨了对小量恶意噪声的敏感性，具体来说是对具有公平约束的学习系统的抗性。</li>
<li>methods: 这篇论文使用了随机分类器，以减少恶意噪声的影响。</li>
<li>results: 研究发现，允许随机分类器时，敏感性可以降低到$\Theta(\alpha)$，$\alpha$是噪声率。此外，研究还发现，对于等机会和平衡约束，敏感性可以降低到$O(\sqrt{\alpha})$和$O(1)$。这些结果提供了对敏感性噪声的 более细致的视图。<details>
<summary>Abstract</summary>
We consider the vulnerability of fairness-constrained learning to small amounts of malicious noise in the training data. Konstantinov and Lampert (2021) initiated the study of this question and presented negative results showing there exist data distributions where for several fairness constraints, any proper learner will exhibit high vulnerability when group sizes are imbalanced. Here, we present a more optimistic view, showing that if we allow randomized classifiers, then the landscape is much more nuanced. For example, for Demographic Parity we show we can incur only a $\Theta(\alpha)$ loss in accuracy, where $\alpha$ is the malicious noise rate, matching the best possible even without fairness constraints. For Equal Opportunity, we show we can incur an $O(\sqrt{\alpha})$ loss, and give a matching $\Omega(\sqrt{\alpha})$lower bound. In contrast, Konstantinov and Lampert (2021) showed for proper learners the loss in accuracy for both notions is $\Omega(1)$. The key technical novelty of our work is how randomization can bypass simple "tricks" an adversary can use to amplify his power. We also consider additional fairness notions including Equalized Odds and Calibration. For these fairness notions, the excess accuracy clusters into three natural regimes $O(\alpha)$,$O(\sqrt{\alpha})$ and $O(1)$. These results provide a more fine-grained view of the sensitivity of fairness-constrained learning to adversarial noise in training data.
</details>
<details>
<summary>摘要</summary>
我们考虑了对公平性限制学习的不可靠性问题，特别是对小量邪恶噪声的影响。 konstantinov和Lampert（2021）开始了这个研究，并提出了负结果，证明了在某些公平性限制下，任何合法的学习者将具有高度的不可靠性，尤其是当群体大小不均匀时。在这里，我们提供了一个更optimistic的见解，表明如果允许随机分类器，则这个景象会变得非常复杂。例如，对于人口均值的公平性，我们显示了仅具有$\Theta(\alpha)$的损失率，与不具有公平性限制的情况相同。对于平等机会的公平性，我们显示了仅具有$O(\sqrt{\alpha})$的损失率，并提供了匹配的$\Omega(\sqrt{\alpha})$下界。与 Konstantinov和Lampert（2021）的结果不同，我们显示在合法学习者中，两者的损失率皆为$\Omega(1)$。我们还考虑了其他的公平性条件，包括平等机会和准确。这些公平性条件下，损失率分布在三个自然的范围之间：$O(\alpha)$, $O(\sqrt{\alpha})$ 和 $O(1)$。这些结果给出了对公平性限制学习对噪声训练数据的敏感度的更细节的观察。
</details></li>
</ul>
<hr>
<h2 id="On-the-Universality-of-Linear-Recurrences-Followed-by-Nonlinear-Projections"><a href="#On-the-Universality-of-Linear-Recurrences-Followed-by-Nonlinear-Projections" class="headerlink" title="On the Universality of Linear Recurrences Followed by Nonlinear Projections"></a>On the Universality of Linear Recurrences Followed by Nonlinear Projections</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11888">http://arxiv.org/abs/2307.11888</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antonio Orvieto, Soham De, Caglar Gulcehre, Razvan Pascanu, Samuel L. Smith</li>
<li>for: 这篇论文目的是提出一种基于回归线性层的序列模型，包括S4、S5和LRU等，可以将任何足够 régulier sequence-to-sequence 映射精确地模拟。</li>
<li>methods: 该论文使用了回归层和位置层的多层感知器（MLP）组合来实现序列模型，并将回归层看作是压缩算法，可以准确地存储输入序列的信息在内部状态中，然后由高度表达的 MLP 进行处理。</li>
<li>results: 该论文的结果表明，这种基于回归线性层的序列模型可以将任何足够 régulier sequence-to-sequence 映射精确地模拟，无需额外的训练或特殊设计。<details>
<summary>Abstract</summary>
In this note (work in progress towards a full-length paper) we show that a family of sequence models based on recurrent linear layers~(including S4, S5, and the LRU) interleaved with position-wise multi-layer perceptrons~(MLPs) can approximate arbitrarily well any sufficiently regular non-linear sequence-to-sequence map. The main idea behind our result is to see recurrent layers as compression algorithms that can faithfully store information about the input sequence into an inner state, before it is processed by the highly expressive MLP.
</details>
<details>
<summary>摘要</summary>
在这个笔记（ towards a full-length paper 的工作进展）中，我们显示了一家序列模型，基于回归线性层（包括 S4、S5 和 LRU）和位置层叠多层感知器（MLP），可以将任何充分 régulié sequence-to-sequence 映射逼近到任何很好的非线性映射。我们的主要想法是看待回归层为压缩算法，可以准确地将输入序列存储在内部状态中，然后由高度表达的 MLP 处理。
</details></li>
</ul>
<hr>
<h2 id="MORE-Measurement-and-Correlation-Based-Variational-Quantum-Circuit-for-Multi-classification"><a href="#MORE-Measurement-and-Correlation-Based-Variational-Quantum-Circuit-for-Multi-classification" class="headerlink" title="MORE: Measurement and Correlation Based Variational Quantum Circuit for Multi-classification"></a>MORE: Measurement and Correlation Based Variational Quantum Circuit for Multi-classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11875">http://arxiv.org/abs/2307.11875</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jindi0/more">https://github.com/jindi0/more</a></li>
<li>paper_authors: Jindi Wu, Tianjie Hu, Qun Li</li>
<li>for: 这 paper 的目的是提出一种高效的量子多类分类器，即 measurement and correlation based variational quantum multi-classifier (MORE)。</li>
<li>methods: 该方法使用了同 binary 分类器一样的变量 ansatz，并在完全利用单个读取 qubit 的量子信息上进行多类分类。具体来说，选择了三个观察量来形成二维希尔бер特空间的基态，然后使用量子状态探测技术来重建读取态。接着，通过量子变量 clustering 方法来确定类别之间的相关性，并使用量子标签 based 监督学习来确定输入数据与其相应的量子标签之间的映射。</li>
<li>results: 我们通过使用 Qiskit Python 库进行实现并对干扰量子系统和干扰系统进行广泛的实验评估，发现 MORE  DESPITE 使用简单的 ansatz 和有限的量子资源，可以达到先进的性能。<details>
<summary>Abstract</summary>
Quantum computing has shown considerable promise for compute-intensive tasks in recent years. For instance, classification tasks based on quantum neural networks (QNN) have garnered significant interest from researchers and have been evaluated in various scenarios. However, the majority of quantum classifiers are currently limited to binary classification tasks due to either constrained quantum computing resources or the need for intensive classical post-processing. In this paper, we propose an efficient quantum multi-classifier called MORE, which stands for measurement and correlation based variational quantum multi-classifier. MORE adopts the same variational ansatz as binary classifiers while performing multi-classification by fully utilizing the quantum information of a single readout qubit. To extract the complete information from the readout qubit, we select three observables that form the basis of a two-dimensional Hilbert space. We then use the quantum state tomography technique to reconstruct the readout state from the measurement results. Afterward, we explore the correlation between classes to determine the quantum labels for classes using the variational quantum clustering approach. Next, quantum label-based supervised learning is performed to identify the mapping between the input data and their corresponding quantum labels. Finally, the predicted label is determined by its closest quantum label when using the classifier. We implement this approach using the Qiskit Python library and evaluate it through extensive experiments on both noise-free and noisy quantum systems. Our evaluation results demonstrate that MORE, despite using a simple ansatz and limited quantum resources, achieves advanced performance.
</details>
<details>
<summary>摘要</summary>
量子计算在最近几年内已经表现出了较大的搭配能力。例如，基于量子神经网络（QNN）的分类任务已经吸引了研究者的广泛关注并在各种场景中进行了评估。然而，现在大多数量子分类器都受到了限制的量子计算资源或需要大量的经典后处理。在这篇论文中，我们提出了一种高效的量子多分类器，称为MORE，它 stands for measurement and correlation based variational quantum multi-classifier。MORE采用了同 binary 分类器一样的变量 ansatz，而在完全利用单个读取量子 bits 的量子信息进行多分类。为了从读取量子 bits 中提取完整的信息，我们选择了三个观测量，它们构成了一个二维希尔бер特空间的基。然后，我们使用量子状态探测技术来重建读取状态从测量结果中。接着，我们研究分类关系来确定类别的量子标签使用变量量子 clustering 方法。然后，我们使用量子标签基于的超vised 学习方法来确定输入数据和其对应的量子标签之间的映射。最后，我们使用类ifier 来预测输入数据的标签。我们使用 Qiskit Python 库实现这种方法，并通过对噪声量子系统和噪声free 量子系统进行了广泛的实验来评估其性能。我们的评估结果表明，MORE，即使使用简单的 ansatz 和有限的量子资源，仍然可以达到先进的性能。
</details></li>
</ul>
<hr>
<h2 id="The-Looming-Threat-of-Fake-and-LLM-generated-LinkedIn-Profiles-Challenges-and-Opportunities-for-Detection-and-Prevention"><a href="#The-Looming-Threat-of-Fake-and-LLM-generated-LinkedIn-Profiles-Challenges-and-Opportunities-for-Detection-and-Prevention" class="headerlink" title="The Looming Threat of Fake and LLM-generated LinkedIn Profiles: Challenges and Opportunities for Detection and Prevention"></a>The Looming Threat of Fake and LLM-generated LinkedIn Profiles: Challenges and Opportunities for Detection and Prevention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11864">http://arxiv.org/abs/2307.11864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navid Ayoobi, Sadat Shahriar, Arjun Mukherjee<br>for: 这个研究旨在检测 LinkedIn 在注册时 already 生成的假 profiles，以保持平台的完整性，防止假用户从获取真正用户的个人信息和敏感信息，并防止假用户在未来的骗财和骗取活动中增加假信望。methods: 这个研究使用 LinkedIn  Profil 中提供的文本信息，并引入 Section and Subsection Tag Embedding (SSTE) 方法，以增强这些数据的分类特征，从而分辨真实 profil 和假 profil。results: 研究表明，使用 static 和 contextualized word embeddings，包括 GloVe、Flair、BERT 和 RoBERTa，可以将 legitimate 和假 profil 分辨出来，准确率大约为 95%。此外，研究还表明，SSTE 在训练集中没有使用 LLM 生成的 profil 时，可以达到约 90% 的准确率。这是一个重要的发现，因为未来几年内，多种 LLM 将在普及，设计一个可以分辨多种 LLM 生成的 profil 的系统会变得非常困难。<details>
<summary>Abstract</summary>
In this paper, we present a novel method for detecting fake and Large Language Model (LLM)-generated profiles in the LinkedIn Online Social Network immediately upon registration and before establishing connections. Early fake profile identification is crucial to maintaining the platform's integrity since it prevents imposters from acquiring the private and sensitive information of legitimate users and from gaining an opportunity to increase their credibility for future phishing and scamming activities. This work uses textual information provided in LinkedIn profiles and introduces the Section and Subsection Tag Embedding (SSTE) method to enhance the discriminative characteristics of these data for distinguishing between legitimate profiles and those created by imposters manually or by using an LLM. Additionally, the dearth of a large publicly available LinkedIn dataset motivated us to collect 3600 LinkedIn profiles for our research. We will release our dataset publicly for research purposes. This is, to the best of our knowledge, the first large publicly available LinkedIn dataset for fake LinkedIn account detection. Within our paradigm, we assess static and contextualized word embeddings, including GloVe, Flair, BERT, and RoBERTa. We show that the suggested method can distinguish between legitimate and fake profiles with an accuracy of about 95% across all word embeddings. In addition, we show that SSTE has a promising accuracy for identifying LLM-generated profiles, despite the fact that no LLM-generated profiles were employed during the training phase, and can achieve an accuracy of approximately 90% when only 20 LLM-generated profiles are added to the training set. It is a significant finding since the proliferation of several LLMs in the near future makes it extremely challenging to design a single system that can identify profiles created with various LLMs.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的方法，用于在 LinkedIn 在线社交平台上立即 региSTRATION 后识别假 profil和大语言模型（LLM）生成的 profil。 Early 识别假 profil是维护平台的完整性非常重要，因为它防止了假者从获得真实用户的私人和敏感信息，并从获得诈骗和骗子活动的机会。这种工作使用 LinkedIn  profil 中提供的文本信息，并引入了分段和子分段标签嵌入（SSTE）方法，以增强这些数据的区分性，用于分辨真实 profil 和由假者或 LLM 手动或机器生成的 profil。此外，由于没有大规模公开可用的 LinkedIn 数据集，我们自己收集了 3600 个 LinkedIn profil 进行研究。我们将在研究中发布我们的数据集。这是，我们知道的情况下，首个大规模公开 LinkedIn 数据集，用于假 LinkedIn 账户检测。在我们的 paradigm 中，我们评估了静态和 contextualized 单词嵌入，包括 GloVe、Flair、BERT 和 RoBERTa。我们发现，建议的方法可以在所有单词嵌入上分辨真实 profil 和假 profil，准确率大约为 95%。此外，我们发现，SSTE 在 LLM 生成 profil 上有承诺的准确率，即使在训练阶段没有使用 LLM 生成 profil，可以达到约 90% 的准确率，只需要将 20 个 LLM 生成 profil 添加到训练集中。这是一项重要发现，因为未来几年，许多 LLM 将在未来普及，设计一个系统可以identify 由多种 LLM 生成的 profil 是极其困难的。
</details></li>
</ul>
<hr>
<h2 id="Data-Induced-Interactions-of-Sparse-Sensors"><a href="#Data-Induced-Interactions-of-Sparse-Sensors" class="headerlink" title="Data-Induced Interactions of Sparse Sensors"></a>Data-Induced Interactions of Sparse Sensors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11838">http://arxiv.org/abs/2307.11838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrei A. Klishin, J. Nathan Kutz, Krithika Manohar</li>
<li>for: 这篇论文是为了探讨大型实验数据在科学和工程中的低维结构，以及如何使用少量的感知器来重建完整的系统状态。</li>
<li>methods: 论文使用了受限 interpolation 和 QR 分解算法来优化感知器的布局。</li>
<li>results: 论文通过统计物理的热力学视角计算出感知器与训练数据之间的互动场景，并可以结合外部选择 критери估算感知器更换的影响。<details>
<summary>Abstract</summary>
Large-dimensional empirical data in science and engineering frequently has low-rank structure and can be represented as a combination of just a few eigenmodes. Because of this structure, we can use just a few spatially localized sensor measurements to reconstruct the full state of a complex system. The quality of this reconstruction, especially in the presence of sensor noise, depends significantly on the spatial configuration of the sensors. Multiple algorithms based on gappy interpolation and QR factorization have been proposed to optimize sensor placement. Here, instead of an algorithm that outputs a singular "optimal" sensor configuration, we take a thermodynamic view to compute the full landscape of sensor interactions induced by the training data. The landscape takes the form of the Ising model in statistical physics, and accounts for both the data variance captured at each sensor location and the crosstalk between sensors. Mapping out these data-induced sensor interactions allows combining them with external selection criteria and anticipating sensor replacement impacts.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="eXplainable-Artificial-Intelligence-XAI-in-age-prediction-A-systematic-review"><a href="#eXplainable-Artificial-Intelligence-XAI-in-age-prediction-A-systematic-review" class="headerlink" title="eXplainable Artificial Intelligence (XAI) in age prediction: A systematic review"></a>eXplainable Artificial Intelligence (XAI) in age prediction: A systematic review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13704">http://arxiv.org/abs/2307.13704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alena Kalyakulina, Igor Yusipov</li>
<li>for: 这项研究旨在探讨使用可解释人工智能（XAI）方法进行年龄预测任务的应用。</li>
<li>methods: 该研究使用了多种XAI方法，包括体系层次分析、可读性分析和模型解释等方法，对年龄预测任务进行了系统性的梳理和分析。</li>
<li>results: 研究发现，XAI方法可以帮助提高年龄预测的准确率和可解释性，并且可以帮助降低年龄预测模型中的风险和不确定性。<details>
<summary>Abstract</summary>
eXplainable Artificial Intelligence (XAI) is now an important and essential part of machine learning, allowing to explain the predictions of complex models. XAI is especially required in risky applications, particularly in health care, where human lives depend on the decisions of AI systems. One area of medical research is age prediction and identification of biomarkers of aging and age-related diseases. However, the role of XAI in the age prediction task has not previously been explored directly. In this review, we discuss the application of XAI approaches to age prediction tasks. We give a systematic review of the works organized by body systems, and discuss the benefits of XAI in medical applications and, in particular, in the age prediction domain.
</details>
<details>
<summary>摘要</summary>
互解的人工智能（XAI）现在成为机器学习中重要和必需的一部分，允许解释模型预测的结果。XAI特别在危险应用中需要，如医疗领域，人工智能系统的决策直接影响人们的生命。一个医学研究领域是年龄预测和年龄相关疾病的识别 биомarkers。然而，XAI在年龄预测任务中的角色尚未直接探讨。在这篇评论中，我们讨论了XAI方法在年龄预测任务中的应用。我们按照身体系统进行了系统性的回顾，并讨论了医疗应用中XAI的优点，特别是在年龄预测领域。
</details></li>
</ul>
<hr>
<h2 id="PINNsFormer-A-Transformer-Based-Framework-For-Physics-Informed-Neural-Networks"><a href="#PINNsFormer-A-Transformer-Based-Framework-For-Physics-Informed-Neural-Networks" class="headerlink" title="PINNsFormer: A Transformer-Based Framework For Physics-Informed Neural Networks"></a>PINNsFormer: A Transformer-Based Framework For Physics-Informed Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11833">http://arxiv.org/abs/2307.11833</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adityalab/pinnsformer">https://github.com/adityalab/pinnsformer</a></li>
<li>paper_authors: Leo Zhiyuan Zhao, Xueying Ding, B. Aditya Prakash</li>
<li>for: 这个论文是为了解决深度学习框架中的数学方程问题，特别是解决偏微分方程（PDEs）的数值解。</li>
<li>methods: 该论文提出了一种基于Transformer的新框架，称为PINNsFormer，它可以准确地 aproximate PDEs的解决方案，通过捕捉时间相关性使用多头注意力机制。</li>
<li>results: 该论文通过实验表明，PINNsFormer可以在多种场景中更好地学习PDEs的解决方案，比如传统PINNs无法学习的场景。此外，PINNsFormer还可以在计算和存储成本下降时，与传统PINNs相比，减少约10%的计算和存储成本。<details>
<summary>Abstract</summary>
Physics-Informed Neural Networks (PINNs) have emerged as a promising deep learning framework for approximating numerical solutions for partial differential equations (PDEs). While conventional PINNs and most related studies adopt fully-connected multilayer perceptrons (MLP) as the backbone structure, they have neglected the temporal relations in PDEs and failed to approximate the true solution. In this paper, we propose a novel Transformer-based framework, namely PINNsFormer, that accurately approximates PDEs' solutions by capturing the temporal dependencies with multi-head attention mechanisms in Transformer-based models. Instead of approximating point predictions, PINNsFormer adapts input vectors to pseudo sequences and point-wise PINNs loss to a sequential PINNs loss. In addition, PINNsFormer is equipped with a novel activation function, namely Wavelet, which anticipates the Fourier decomposition through deep neural networks. We empirically demonstrate PINNsFormer's ability to capture the PDE solutions for various scenarios, in which conventional PINNs have failed to learn. We also show that PINNsFormer achieves superior approximation accuracy on such problems than conventional PINNs with non-sensitive hyperparameters, in trade of marginal computational and memory costs, with extensive experiments.
</details>
<details>
<summary>摘要</summary>
物理 Informed Neural Networks (PINNs) 已经出现为解析数学方程 partial differential equations (PDEs) 的深度学习框架。  although conventional PINNs and most related studies adopt fully-connected multilayer perceptrons (MLP) as the backbone structure, they have neglected the temporal relations in PDEs and failed to approximate the true solution. 在这篇论文中，我们提出了一种新的 Transformer 基于的框架，即 PINNsFormer，可以准确地 approximates PDEs 的解决方案，通过捕捉 Transformer 中的多头注意机制来捕捉 PDEs 中的时间相关性。 而不是通过点预测来 Approximation，PINNsFormer 将输入向量映射到 pseudo 序列上，并将点级 PINNs 损失转换为顺序 PINNs 损失。 此外，PINNsFormer 还配备了一种新的活动函数，即 Wavelet，可以预测 Fourier 分解在深度神经网络中。 我们在实验中证明了 PINNsFormer 可以在各种情况下 capture PDEs 的解决方案，而 conventional PINNs 无法学习。 此外，PINNsFormer 在这些问题上的 Approximation 精度高于 conventional PINNs ，尽管计算和内存成本增加了一些，通过广泛的实验证明。
</details></li>
</ul>
<hr>
<h2 id="Differentially-Private-Heavy-Hitter-Detection-using-Federated-Analytics"><a href="#Differentially-Private-Heavy-Hitter-Detection-using-Federated-Analytics" class="headerlink" title="Differentially Private Heavy Hitter Detection using Federated Analytics"></a>Differentially Private Heavy Hitter Detection using Federated Analytics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11749">http://arxiv.org/abs/2307.11749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karan Chadha, Junye Chen, John Duchi, Vitaly Feldman, Hanieh Hashemi, Omid Javidbakht, Audra McMillan, Kunal Talwar</li>
<li>for: 本文提出了一种基于前缀树的差分private大量访问检测算法，以实现每个用户的多个数据点的学习和差分隐私保护。</li>
<li>methods: 本文提出了一种自适应hyperparameter调整算法，以提高算法性能，同时满足计算、通信和隐私约束。此外，本文还 explore了不同的数据选择方案以及引入拒绝列表的影响。</li>
<li>results: 经过EXTENSIVE EXPERIMENTATION ON THE REDDIT DATASET，本文发现这些改进都能够提高算法性能，同时满足差分隐私和计算、通信约束。<details>
<summary>Abstract</summary>
In this work, we study practical heuristics to improve the performance of prefix-tree based algorithms for differentially private heavy hitter detection. Our model assumes each user has multiple data points and the goal is to learn as many of the most frequent data points as possible across all users' data with aggregate and local differential privacy. We propose an adaptive hyperparameter tuning algorithm that improves the performance of the algorithm while satisfying computational, communication and privacy constraints. We explore the impact of different data-selection schemes as well as the impact of introducing deny lists during multiple runs of the algorithm. We test these improvements using extensive experimentation on the Reddit dataset~\cite{caldas2018leaf} on the task of learning the most frequent words.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们研究了可行的启发式法则，以提高基于 prefix-tree 算法的差分隐私极大热点检测性能。我们的模型假设每名用户有多个数据点，目标是通过聚合和本地差分隐私来学习所有用户数据中的最多热点数据点。我们提议一种适应性hyperparameter调整算法，可以提高算法的性能，同时满足计算、通信和隐私约束。我们还研究了不同的数据选择方案以及在多次运行算法时引入拒绝列表的影响。我们通过对 Reddit 数据集（Caldas et al., 2018）的 Word 学习任务进行了广泛的实验来评估这些改进。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Ad-Auction-Realism-Practical-Insights-Modeling-Implications"><a href="#Advancing-Ad-Auction-Realism-Practical-Insights-Modeling-Implications" class="headerlink" title="Advancing Ad Auction Realism: Practical Insights &amp; Modeling Implications"></a>Advancing Ad Auction Realism: Practical Insights &amp; Modeling Implications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11732">http://arxiv.org/abs/2307.11732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ming Chen, Sareh Nabi, Marciano Siniscalchi</li>
<li>for: 本文提出了一种在网络广告拍卖中学习模型，以允许现代网络拍卖中的四个关键现实特征：（1）广告插槽的价值和点击率因用户搜索词而变化，（2）投标商的数量和身份在每次拍卖中是不确定的，（3）广告主只收到部分、汇总的反馈，（4）支付规则只有部分指定。</li>
<li>methods: 作者们使用了一种对抗强制算法来模型广告主的行为，不受拍卖机制细节的影响。</li>
<li>results: 研究发现，在更加复杂的环境下，“软底”可以提高关键性能指标，即使投标商来自同一个人口。此外，研究还证明了如何从观察拍卖价格中推断广告主价值分布，从而证明了这种方法在更真实的拍卖 Setting 中的实际可行性。<details>
<summary>Abstract</summary>
This paper proposes a learning model of online ad auctions that allows for the following four key realistic characteristics of contemporary online auctions: (1) ad slots can have different values and click-through rates depending on users' search queries, (2) the number and identity of competing advertisers are unobserved and change with each auction, (3) advertisers only receive partial, aggregated feedback, and (4) payment rules are only partially specified. We model advertisers as agents governed by an adversarial bandit algorithm, independent of auction mechanism intricacies. Our objective is to simulate the behavior of advertisers for counterfactual analysis, prediction, and inference purposes. Our findings reveal that, in such richer environments, "soft floors" can enhance key performance metrics even when bidders are drawn from the same population. We further demonstrate how to infer advertiser value distributions from observed bids, thereby affirming the practical efficacy of our approach even in a more realistic auction setting.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Ad slots can have different values and click-through rates depending on users’ search queries.2. The number and identity of competing advertisers are unobserved and change with each auction.3. Advertisers only receive partial, aggregated feedback.4. Payment rules are only partially specified.We model advertisers as agents governed by an adversarial bandit algorithm, independent of auction mechanism intricacies. Our objective is to simulate the behavior of advertisers for counterfactual analysis, prediction, and inference purposes.Our findings show that, in such richer environments, “soft floors” can enhance key performance metrics even when bidders are drawn from the same population. We also demonstrate how to infer advertiser value distributions from observed bids, thereby confirming the practical efficacy of our approach in a more realistic auction setting.</details></li>
</ol>
<hr>
<h2 id="Mitigating-Communications-Threats-in-Decentralized-Federated-Learning-through-Moving-Target-Defense"><a href="#Mitigating-Communications-Threats-in-Decentralized-Federated-Learning-through-Moving-Target-Defense" class="headerlink" title="Mitigating Communications Threats in Decentralized Federated Learning through Moving Target Defense"></a>Mitigating Communications Threats in Decentralized Federated Learning through Moving Target Defense</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11730">http://arxiv.org/abs/2307.11730</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/enriquetomasmb/fedstellar">https://github.com/enriquetomasmb/fedstellar</a></li>
<li>paper_authors: Enrique Tomás Martínez Beltrán, Pedro Miguel Sánchez Sánchez, Sergio López Bernal, Gérôme Bovet, Manuel Gil Pérez, Gregorio Martínez Pérez, Alberto Huertas Celdrán</li>
<li>for: This paper focuses on addressing communication security challenges in Decentralized Federated Learning (DFL) to ensure the privacy and integrity of data during model aggregation.</li>
<li>methods: The paper introduces a security module that combines symmetric and asymmetric encryption with Moving Target Defense (MTD) techniques, including random neighbor selection and IP&#x2F;port switching, to protect DFL communications.</li>
<li>results: The security module is validated through experiments with the MNIST dataset and eclipse attacks, showing an average F1 score of 95% with moderate increases in CPU usage (up to 63.2% +-3.5%) and network traffic (230 MB +-15 MB) under the most secure configuration, mitigating the risks posed by eavesdropping or eclipse attacks.Here’s the Chinese version of the three key points:</li>
<li>for: 这篇论文关注Decentralized Federated Learning (DFL) 中通信安全问题，以确保数据隐私和完整性 durante 模型聚合。</li>
<li>methods: 这篇论文提出了一个安全模块，该模块结合 симметриック和非对称加密技术，以及移动目标防御 (MTD) 技术，包括随机邻居选择和 IP&#x2F;端口 switching，以保护 DFL 通信。</li>
<li>results: 安全模块通过 MNIST 数据集和 Eclipse 攻击进行了验证，结果显示，在最安全配置下，平均 F1 分数达到 95%，CPU 使用率最高达 63.2% +-3.5%，网络流量最高达 230 MB +-15 MB，成功 mitigate 隐私攻击和 Eclipse 攻击的风险。<details>
<summary>Abstract</summary>
The rise of Decentralized Federated Learning (DFL) has enabled the training of machine learning models across federated participants, fostering decentralized model aggregation and reducing dependence on a server. However, this approach introduces unique communication security challenges that have yet to be thoroughly addressed in the literature. These challenges primarily originate from the decentralized nature of the aggregation process, the varied roles and responsibilities of the participants, and the absence of a central authority to oversee and mitigate threats. Addressing these challenges, this paper first delineates a comprehensive threat model, highlighting the potential risks of DFL communications. In response to these identified risks, this work introduces a security module designed for DFL platforms to counter communication-based attacks. The module combines security techniques such as symmetric and asymmetric encryption with Moving Target Defense (MTD) techniques, including random neighbor selection and IP/port switching. The security module is implemented in a DFL platform called Fedstellar, allowing the deployment and monitoring of the federation. A DFL scenario has been deployed, involving eight physical devices implementing three security configurations: (i) a baseline with no security, (ii) an encrypted configuration, and (iii) a configuration integrating both encryption and MTD techniques. The effectiveness of the security module is validated through experiments with the MNIST dataset and eclipse attacks. The results indicated an average F1 score of 95%, with moderate increases in CPU usage (up to 63.2% +-3.5%) and network traffic (230 MB +-15 MB) under the most secure configuration, mitigating the risks posed by eavesdropping or eclipse attacks.
</details>
<details>
<summary>摘要</summary>
德中 federated learning (DFL) 的出现已经允许机器学习模型在联合参与者之间训练，从而实现了分布式模型集成和减少服务器依赖。然而，这种方法引入了一些独特的通信安全挑战，在文献中尚未得到充分研究。这些挑战主要来自分布式集成过程中的各种角色和责任，以及缺乏中央权威来监督和 Mitigate 威胁。为了解决这些挑战，本文首先提出了一个完整的威胁模型，描述了 DFL 通信中的潜在风险。作为回应，本工作提出了一种针对 DFL 平台的安全模块，该模块结合了Symmetric and Asymmetric Encryption 技术以及 Move Target Defense (MTD) 技术，包括随机邻居选择和 IP/port 转换。该安全模块在 Fedstellar 平台上实现，allowing the deployment and monitoring of the federation。在一个 DFL 场景中，涉及到八个物理设备，实现了三种安全配置：（i）无安全配置，（ii）加密配置，和（iii）集成加密和 MTD 技术的配置。通过使用 MNIST 数据集和 Eclipse 攻击进行实验， validate 了安全模块的效iveness。结果表明，在最安全配置下，模型的 F1 分数平均为 95%，CPU 使用率（最高）为 63.2% ± 3.5%，网络流量（最高）为 230 MB ± 15 MB。这些结果表明，通过加密和 MTD 技术，可以有效地 mitigate 防止窃听或 Eclipse 攻击。
</details></li>
</ul>
<hr>
<h2 id="Local-Kernel-Renormalization-as-a-mechanism-for-feature-learning-in-overparametrized-Convolutional-Neural-Networks"><a href="#Local-Kernel-Renormalization-as-a-mechanism-for-feature-learning-in-overparametrized-Convolutional-Neural-Networks" class="headerlink" title="Local Kernel Renormalization as a mechanism for feature learning in overparametrized Convolutional Neural Networks"></a>Local Kernel Renormalization as a mechanism for feature learning in overparametrized Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11807">http://arxiv.org/abs/2307.11807</a></li>
<li>repo_url: None</li>
<li>paper_authors: R. Aiudi, R. Pacelli, A. Vezzani, R. Burioni, P. Rotondo<br>for:This paper explores the differences in feature learning between fully-connected (FC) and convolutional architectures (CNNs) in deep neural networks.methods:The paper uses a simple theoretical framework to provide a rationale for the differences in performance between FC and CNN architectures. The authors derive a finite-width effective action for an architecture with one convolutional hidden layer and compare it with the result available for FC networks.results:The paper shows that the kernel of the CNN architecture undergoes a local renormalization, meaning that the network can select the local components that will contribute to the final prediction in a data-dependent way. This finding highlights a simple mechanism for feature learning that can take place in overparametrized shallow CNNs, but not in shallow FC architectures or in locally connected neural networks without weight sharing.<details>
<summary>Abstract</summary>
Feature learning, or the ability of deep neural networks to automatically learn relevant features from raw data, underlies their exceptional capability to solve complex tasks. However, feature learning seems to be realized in different ways in fully-connected (FC) or convolutional architectures (CNNs). Empirical evidence shows that FC neural networks in the infinite-width limit eventually outperform their finite-width counterparts. Since the kernel that describes infinite-width networks does not evolve during training, whatever form of feature learning occurs in deep FC architectures is not very helpful in improving generalization. On the other hand, state-of-the-art architectures with convolutional layers achieve optimal performances in the finite-width regime, suggesting that an effective form of feature learning emerges in this case. In this work, we present a simple theoretical framework that provides a rationale for these differences, in one hidden layer networks. First, we show that the generalization performance of a finite-width FC network can be obtained by an infinite-width network, with a suitable choice of the Gaussian priors. Second, we derive a finite-width effective action for an architecture with one convolutional hidden layer and compare it with the result available for FC networks. Remarkably, we identify a completely different form of kernel renormalization: whereas the kernel of the FC architecture is just globally renormalized by a single scalar parameter, the CNN kernel undergoes a local renormalization, meaning that the network can select the local components that will contribute to the final prediction in a data-dependent way. This finding highlights a simple mechanism for feature learning that can take place in overparametrized shallow CNNs, but not in shallow FC architectures or in locally connected neural networks without weight sharing.
</details>
<details>
<summary>摘要</summary>
FC neural networks 和 CNNs 都有不同的方式来实现特征学习。empirical evidence 表明，在无穷宽度限制下，FC neural networks 的性能会超过其有限宽度对应的性能。然而，由于无穷宽度网络的核函数在训练过程中不会发展，因此FC neural networks 中的特征学习不会对泛化性能产生帮助。相反，当前的状态机器学习架构中的 convolutional layers 可以在有限宽度限制下达到最佳性能，这 Suggests 在这种情况下可以发现有效的特征学习方式。在这项工作中，我们提出了一个简单的理论框架，以解释这些差异。首先，我们表明了一个有限宽度 FC 网络的泛化性能可以通过无穷宽度网络来实现，并且可以通过适当的 Gaussian priors 来选择。其次，我们 derivated 一个有限宽度效果动作，并与 FC 网络的结果进行比较。意外地，我们发现了一种完全不同的kernel renormalization：FC 网络的核函数只是全局地 renormalized 一个整数参数，而 CNN 的核函数则会在数据依存的方式下 renormalized，这意味着网络可以在数据依存的情况下选择当地组件，以便在最终预测中做出贡献。这一发现高光了一种简单的特征学习机制，可以在过 parametrization 的 shallow CNN 中发生，但不可以在 shallow FC 网络或者 Without weight sharing 的本地连接神经网络中发生。
</details></li>
</ul>
<hr>
<h2 id="Convergence-of-SGD-for-Training-Neural-Networks-with-Sliced-Wasserstein-Losses"><a href="#Convergence-of-SGD-for-Training-Neural-Networks-with-Sliced-Wasserstein-Losses" class="headerlink" title="Convergence of SGD for Training Neural Networks with Sliced Wasserstein Losses"></a>Convergence of SGD for Training Neural Networks with Sliced Wasserstein Losses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11714">http://arxiv.org/abs/2307.11714</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eloi Tanguy</li>
<li>for: 这 paper 是关于 Optimal Transport 和 Neural Networks 的研究，具体来说是关于 Stochastic Gradient Descent (SGD) 在 Non-smooth 和 Non-convex 函数上的收敛性。</li>
<li>methods: 这 paper 使用了 Sliced Wasserstein (SW) 距离作为比较概率分布的方法，并使用了 Bianchi et al. (2022) 所提出的一些新的结果来证明 SGD 的收敛性。</li>
<li>results: 这 paper 显示了 fixed-step SGD 轨迹在 NN 参数上的收敛性，并且在更严格的假设下，显示了 noised 和 projected SGD 的收敛性。 Specifically, the paper shows that the trajectories of fixed-step SGD approach the set of (sub)-gradient flow equations as the step decreases, and under stricter assumptions, the long-run limits of the trajectories approach a set of generalised critical points of the loss function.<details>
<summary>Abstract</summary>
Optimal Transport has sparked vivid interest in recent years, in particular thanks to the Wasserstein distance, which provides a geometrically sensible and intuitive way of comparing probability measures. For computational reasons, the Sliced Wasserstein (SW) distance was introduced as an alternative to the Wasserstein distance, and has seen uses for training generative Neural Networks (NNs). While convergence of Stochastic Gradient Descent (SGD) has been observed practically in such a setting, there is to our knowledge no theoretical guarantee for this observation. Leveraging recent works on convergence of SGD on non-smooth and non-convex functions by Bianchi et al. (2022), we aim to bridge that knowledge gap, and provide a realistic context under which fixed-step SGD trajectories for the SW loss on NN parameters converge. More precisely, we show that the trajectories approach the set of (sub)-gradient flow equations as the step decreases. Under stricter assumptions, we show a much stronger convergence result for noised and projected SGD schemes, namely that the long-run limits of the trajectories approach a set of generalised critical points of the loss function.
</details>
<details>
<summary>摘要</summary>
最近年来，最优运输（Optimal Transport）已经引起了广泛的关注，尤其是通过瓦asserstein距离（Wasserstein distance）来比较概率分布的 geometrically sensible 和直观的方法。由于计算原因，人工智能神经网络（Neural Networks，NNs）的训练中使用了截面瓦asserstein（SW）距离，并且在实践中观察到了SGD的收敛。然而，我们知道的理论保证是不够。基于 Bianchi et al. (2022) 的最近研究，我们想要填补这个知识隔口，并在NN参数的 SW 损失下实现了SGD的收敛。更加准确地说，我们证明了SGD的流体动向逐步逼近（sub-gradient flow）的设置，并在更加严格的假设下证明了SGD的杂化和投影后的收敛结果。
</details></li>
</ul>
<hr>
<h2 id="JoinGym-An-Efficient-Query-Optimization-Environment-for-Reinforcement-Learning"><a href="#JoinGym-An-Efficient-Query-Optimization-Environment-for-Reinforcement-Learning" class="headerlink" title="JoinGym: An Efficient Query Optimization Environment for Reinforcement Learning"></a>JoinGym: An Efficient Query Optimization Environment for Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11704">http://arxiv.org/abs/2307.11704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaiwen Wang, Junxiong Wang, Yueying Li, Nathan Kallus, Immanuel Trummer, Wen Sun</li>
<li>For: 这篇论文旨在提供一个高效和轻量级的查询优化环境，用于应用强化学习（RL）在数据管理问题中。* Methods: 论文使用Markov决策过程（MDP）形式表述了左深和树状变体的Join order selection（JOS）问题，并提供了遵循标准Gymnasium API的实现。* Results: 论文发现，使用RL算法可以在培训集查询中near-优秀表现，但是在测试集查询中表现下降多个数量级。这个差距驱动了进一步研究RL算法在多任务 combinatorial optimization问题中的泛化能力。<details>
<summary>Abstract</summary>
In this paper, we present \textsc{JoinGym}, an efficient and lightweight query optimization environment for reinforcement learning (RL). Join order selection (JOS) is a classic NP-hard combinatorial optimization problem from database query optimization and can serve as a practical testbed for the generalization capabilities of RL algorithms. We describe how to formulate each of the left-deep and bushy variants of the JOS problem as a Markov Decision Process (MDP), and we provide an implementation adhering to the standard Gymnasium API. We highlight that our implementation \textsc{JoinGym} is completely based on offline traces of all possible joins, which enables RL practitioners to easily and quickly test their methods on a realistic data management problem without needing to setup any systems. Moreover, we also provide all possible join traces on $3300$ novel SQL queries generated from the IMDB dataset. Upon benchmarking popular RL algorithms, we find that at least one method can obtain near-optimal performance on train-set queries but their performance degrades by several orders of magnitude on test-set queries. This gap motivates further research for RL algorithms that generalize well in multi-task combinatorial optimization problems.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一个名为\textsc{JoinGym}的高效和轻量级查询优化环境，用于应用束缚学习（RL）。Join顺序选择（JOS）是一个经典的NP困难的 combinatorial optimization问题，来自数据库查询优化，可以作为RL算法的通用化能力的实验室。我们介绍了如何将左深和叠缩 variant of JOS问题转换为Markov决策过程（MDP），并提供了遵循标准Gymnasium API的实现。我们指出，我们的实现\textsc{JoinGym}完全基于所有可能的 joins 的离线轨迹，这使得RL实践者可以很容易地和快速地在真实的数据管理问题上测试他们的方法，无需设置任何系统。此外，我们还提供了 $3300$ 个新的 SQL 查询，生成自 IMDB 数据集。在 benchmarking 流行的 RL 算法时，我们发现至少有一种方法可以在训练集查询上达到 Near-optimal 性能，但它们在测试集查询上表现下降多个ORDERS。这个差距激励我们进一步研究 RL 算法在多任务 combinatorial optimization 问题中的泛化能力。
</details></li>
</ul>
<hr>
<h2 id="Using-simulation-to-calibrate-real-data-acquisition-in-veterinary-medicine"><a href="#Using-simulation-to-calibrate-real-data-acquisition-in-veterinary-medicine" class="headerlink" title="Using simulation to calibrate real data acquisition in veterinary medicine"></a>Using simulation to calibrate real data acquisition in veterinary medicine</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11695">http://arxiv.org/abs/2307.11695</a></li>
<li>repo_url: None</li>
<li>paper_authors: Krystian Strzałka, Szymon Mazurek, Maciej Wielgosz, Paweł Russek, Jakub Caputa, Daria Łukasik, Jan Krupiński, Jakub Grzeszczyk, Michał Karwatowski, Rafał Frączek, Ernest Jamro, Marcin Pietroń, Sebastian Koryciak, Agnieszka Dąbrowska-Boruch, Kazimierz Wiatr</li>
<li>for: 这paper探讨了使用模拟环境增强兽医数据获取和诊断的创新方法, 专注于狗的步态分析。</li>
<li>methods: 这paper使用Blender和Blenderproc库生成 simulate diverse anatomical, environmental, and behavioral conditions的数据集, 并标准化 Graph形式进行优化分析。</li>
<li>results: 初步结果表明, 这种基于模拟的方法可能会提高兽医诊断的精度和效率, 通过结合实际和synthetic数据来提高整体效果。<details>
<summary>Abstract</summary>
This paper explores the innovative use of simulation environments to enhance data acquisition and diagnostics in veterinary medicine, focusing specifically on gait analysis in dogs. The study harnesses the power of Blender and the Blenderproc library to generate synthetic datasets that reflect diverse anatomical, environmental, and behavioral conditions. The generated data, represented in graph form and standardized for optimal analysis, is utilized to train machine learning algorithms for identifying normal and abnormal gaits. Two distinct datasets with varying degrees of camera angle granularity are created to further investigate the influence of camera perspective on model accuracy. Preliminary results suggest that this simulation-based approach holds promise for advancing veterinary diagnostics by enabling more precise data acquisition and more effective machine learning models. By integrating synthetic and real-world patient data, the study lays a robust foundation for improving overall effectiveness and efficiency in veterinary medicine.
</details>
<details>
<summary>摘要</summary>
这篇论文探讨了使用模拟环境增强畜牧医学数据收集和诊断的创新方法，特地关注了狗的步态分析。研究使用Blender和Blenderproc库生成了具有多样化 анатомиче、环境和行为条件的 sintetic数据集。生成的数据表示为图形形式标准化，用于训练机器学习算法来识别正常和异常的步态。研究创建了两个不同的摄像头角度精细度的数据集，以更好地调查摄像头角度对模型准确性的影响。初步结果表明，这种基于模拟的方法可能对畜牧医学诊断带来进步，使得数据收集更加精准，机器学习模型更加有效。通过将 sintetic和实际病人数据集 integrate，研究为畜牧医学的整体效果和效率提供了一个坚实的基础。
</details></li>
</ul>
<hr>
<h2 id="Fast-Adaptive-Test-Time-Defense-with-Robust-Features"><a href="#Fast-Adaptive-Test-Time-Defense-with-Robust-Features" class="headerlink" title="Fast Adaptive Test-Time Defense with Robust Features"></a>Fast Adaptive Test-Time Defense with Robust Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11672">http://arxiv.org/abs/2307.11672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anurag Singh, Mahalakshmi Sabanayagam, Krikamol Muandet, Debarghya Ghoshdastidar</li>
<li>for: 本研究旨在提高深度神经网络的对抗性能，并且提出了一种新的适应测试时防御策略，可以轻松地与任何现有的鲁棒训练过程结合使用，无需额外的测试时计算。</li>
<li>methods: 本研究使用了一种基于特征feature的稳定性概念，将训练过程中的模型参数 проекed到最稳定的特征空间中，从而减少了对抗性攻击的脆弱性。我们 theoretically 展示了一般加法模型中的顶层特征空间更加稳定，并且通过NTK相等性证明了这一点。</li>
<li>results: 我们在CIFAR-10和CIFAR-100数据集上进行了广泛的实验，包括RobustBench中的州际方法，并观察到了提出的方法在计算成本下较低时即使能够超越现有的适应测试时防御策略。<details>
<summary>Abstract</summary>
Adaptive test-time defenses are used to improve the robustness of deep neural networks to adversarial examples. However, existing methods significantly increase the inference time due to additional optimization on the model parameters or the input at test time. In this work, we propose a novel adaptive test-time defense strategy that is easy to integrate with any existing (robust) training procedure without additional test-time computation. Based on the notion of robustness of features that we present, the key idea is to project the trained models to the most robust feature space, thereby reducing the vulnerability to adversarial attacks in non-robust directions. We theoretically show that the top eigenspace of the feature matrix are more robust for a generalized additive model and support our argument for a large width neural network with the Neural Tangent Kernel (NTK) equivalence. We conduct extensive experiments on CIFAR-10 and CIFAR-100 datasets for several robustness benchmarks, including the state-of-the-art methods in RobustBench, and observe that the proposed method outperforms existing adaptive test-time defenses at much lower computation costs.
</details>
<details>
<summary>摘要</summary>
使用可靠测试时防御技术提高深度神经网络对攻击性例子的Robustness。然而，现有方法会增加测试时间，因为它们需要在测试时进行额外的优化模型参数或输入。在这种工作中，我们提出了一种新的可靠测试时防御策略，可以轻松地与现有的可靠训练方法结合使用，无需额外的测试时间计算。基于我们提出的特征Robustness的概念，我们的关键思想是将训练模型投影到最Robust特征空间中，以降低不Robust方向的攻击。我们理论上显示，通过一般加法模型，特征矩阵的Top eigenvector更Robust，并且支持我们对大宽神经网络的NTK相等性。我们在CIFAR-10和CIFAR-100数据集上进行了广泛的实验，包括RobustBench状态OF艺术方法，并发现提议方法在计算成本下较低的情况下，超过现有的可靠测试时防御方法。
</details></li>
</ul>
<hr>
<h2 id="An-Efficient-Interior-Point-Method-for-Online-Convex-Optimization"><a href="#An-Efficient-Interior-Point-Method-for-Online-Convex-Optimization" class="headerlink" title="An Efficient Interior-Point Method for Online Convex Optimization"></a>An Efficient Interior-Point Method for Online Convex Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11668">http://arxiv.org/abs/2307.11668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elad Hazan, Nimrod Megiddo</li>
<li>for: 这 paper 的目的是减少在线几何优化中的遗弃。</li>
<li>methods: 该 paper 使用了一种新的 adaptive 算法，可以减少遗弃的规模。</li>
<li>results: 该 paper 证明了该算法可以在 $O(\sqrt{T \log T})$ 的时间内减少遗弃，这是最小可能的，只有一个 logarithmic 项。<details>
<summary>Abstract</summary>
A new algorithm for regret minimization in online convex optimization is described. The regret of the algorithm after $T$ time periods is $O(\sqrt{T \log T})$ - which is the minimum possible up to a logarithmic term. In addition, the new algorithm is adaptive, in the sense that the regret bounds hold not only for the time periods $1,\ldots,T$ but also for every sub-interval $s,s+1,\ldots,t$. The running time of the algorithm matches that of newly introduced interior point algorithms for regret minimization: in $n$-dimensional space, during each iteration the new algorithm essentially solves a system of linear equations of order $n$, rather than solving some constrained convex optimization problem in $n$ dimensions and possibly many constraints.
</details>
<details>
<summary>摘要</summary>
新算法可以减少 regret 在在线凸优化中。该算法在 $T$ 个时间段后的 regret 是 $O(\sqrt{T \log T})$，这是最小可能的，即使带有对数项。此外，该算法是可适应的，即在每个子时间段 $s,s+1,\ldots,t$ 中， regret 约束也成立。算法的运行时间与新引入的内部点算法一样，即在 $n$ 维空间中每次迭代时解决一个线性方程组件，而不是解决一个凸优化问题并可能有多个约束。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/22/cs.LG_2023_07_22/" data-id="clmjn91l3005m0j88ftraemur" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/23/eess.IV_2023_07_23/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.IV - 2023-07-23 17:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/22/eess.IV_2023_07_22/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-07-22 17:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
