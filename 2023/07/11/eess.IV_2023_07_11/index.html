
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-07-11 17:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="APRF: Anti-Aliasing Projection Representation Field for Inverse Problem in Imaging paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.05270 repo_url: None paper_authors: Zixuan Chen, Lingxiao Yang, Jianhuang Lai, X">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-07-11 17:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/07/11/eess.IV_2023_07_11/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="APRF: Anti-Aliasing Projection Representation Field for Inverse Problem in Imaging paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.05270 repo_url: None paper_authors: Zixuan Chen, Lingxiao Yang, Jianhuang Lai, X">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-10T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:12.823Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_07_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/11/eess.IV_2023_07_11/" class="article-date">
  <time datetime="2023-07-10T16:00:00.000Z" itemprop="datePublished">2023-07-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-07-11 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="APRF-Anti-Aliasing-Projection-Representation-Field-for-Inverse-Problem-in-Imaging"><a href="#APRF-Anti-Aliasing-Projection-Representation-Field-for-Inverse-Problem-in-Imaging" class="headerlink" title="APRF: Anti-Aliasing Projection Representation Field for Inverse Problem in Imaging"></a>APRF: Anti-Aliasing Projection Representation Field for Inverse Problem in Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05270">http://arxiv.org/abs/2307.05270</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixuan Chen, Lingxiao Yang, Jianhuang Lai, Xiaohua Xie</li>
<li>for: 高品质 Computed Tomography (CT) 影像重建</li>
<li>methods: 使用 Implicit Neural Representations (INRs) 建立坐标基于的映射，以及自我监督 Anti-Aliasing Projection Representation Field (APRF) 方法</li>
<li>results: 比前方法更高效，能够提供更高品质的 CT 影像，并且具有更少的噪音和杂音Here’s a more detailed explanation of each point:</li>
<li>for: The paper is focused on developing a novel method for reconstructing high-quality CT images from sparse-view measurements.</li>
<li>methods: The proposed method uses Implicit Neural Representations (INRs) to establish a coordinate-based mapping between sinograms and CT images. Additionally, a self-supervised method called Anti-Aliasing Projection Representation Field (APRF) is proposed to improve the quality of the reconstructed images.</li>
<li>results: The proposed method outperforms state-of-the-art methods in terms of image quality, with fewer artifacts and more accurate details. The code for the proposed method will be publicly available soon.<details>
<summary>Abstract</summary>
Sparse-view Computed Tomography (SVCT) reconstruction is an ill-posed inverse problem in imaging that aims to acquire high-quality CT images based on sparsely-sampled measurements. Recent works use Implicit Neural Representations (INRs) to build the coordinate-based mapping between sinograms and CT images. However, these methods have not considered the correlation between adjacent projection views, resulting in aliasing artifacts on SV sinograms. To address this issue, we propose a self-supervised SVCT reconstruction method -- Anti-Aliasing Projection Representation Field (APRF), which can build the continuous representation between adjacent projection views via the spatial constraints. Specifically, APRF only needs SV sinograms for training, which first employs a line-segment sampling module to estimate the distribution of projection views in a local region, and then synthesizes the corresponding sinogram values using center-based line integral module. After training APRF on a single SV sinogram itself, it can synthesize the corresponding dense-view (DV) sinogram with consistent continuity. High-quality CT images can be obtained by applying re-projection techniques on the predicted DV sinograms. Extensive experiments on CT images demonstrate that APRF outperforms state-of-the-art methods, yielding more accurate details and fewer artifacts. Our code will be publicly available soon.
</details>
<details>
<summary>摘要</summary>
sparse-view 计算机断层成像（SVCT）重建是一个不定问题在成像中，旨在基于稀疏样本获取高质量 CT 图像。 current works 使用隐式神经表示（INRs）建立坐标基于的映射 между sinograms 和 CT 图像。 however, these methods have not considered the correlation between adjacent projection views, resulting in aliasing artifacts on SV sinograms. to address this issue, we propose a self-supervised SVCT reconstruction method -- Anti-Aliasing Projection Representation Field (APRF), which can build the continuous representation between adjacent projection views via the spatial constraints. specifically, APRF only needs SV sinograms for training, which first employs a line-segment sampling module to estimate the distribution of projection views in a local region, and then synthesizes the corresponding sinogram values using center-based line integral module. after training APRF on a single SV sinogram itself, it can synthesize the corresponding dense-view (DV) sinogram with consistent continuity. high-quality CT images can be obtained by applying re-projection techniques on the predicted DV sinograms. extensive experiments on CT images demonstrate that APRF outperforms state-of-the-art methods, yielding more accurate details and fewer artifacts. our code will be publicly available soon.Note: The translation is in Simplified Chinese, which is one of the two standard versions of Chinese. The other version is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Does-pre-training-on-brain-related-tasks-results-in-better-deep-learning-based-brain-age-biomarkers"><a href="#Does-pre-training-on-brain-related-tasks-results-in-better-deep-learning-based-brain-age-biomarkers" class="headerlink" title="Does pre-training on brain-related tasks results in better deep-learning-based brain age biomarkers?"></a>Does pre-training on brain-related tasks results in better deep-learning-based brain age biomarkers?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05241">http://arxiv.org/abs/2307.05241</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gama-ufsc/brain-age">https://github.com/gama-ufsc/brain-age</a></li>
<li>paper_authors: Bruno Machado Pacheco, Victor Hugo Rocha de Oliveira, Augusto Braga Fernandes Antunes, Saulo Domingos de Souza Pedro, Danilo Silva</li>
<li>for: 预测健康人脑年龄，作为脑健康指标和成功老化指标，以及疾病生物标志。</li>
<li>methods: 使用深度学习模型预测健康人脑年龄，并在训练过程中使用脑相关任务进行预处理。</li>
<li>results: 研究发现，使用脑相关任务预处理的深度学习模型可以在ADNI数据集上达到状态之作的结果，而且可以验证这些模型的脑年龄生物标志在轻度认知障碍和阿尔茨heimer病患者的图像上的效果。<details>
<summary>Abstract</summary>
Brain age prediction using neuroimaging data has shown great potential as an indicator of overall brain health and successful aging, as well as a disease biomarker. Deep learning models have been established as reliable and efficient brain age estimators, being trained to predict the chronological age of healthy subjects. In this paper, we investigate the impact of a pre-training step on deep learning models for brain age prediction. More precisely, instead of the common approach of pre-training on natural imaging classification, we propose pre-training the models on brain-related tasks, which led to state-of-the-art results in our experiments on ADNI data. Furthermore, we validate the resulting brain age biomarker on images of patients with mild cognitive impairment and Alzheimer's disease. Interestingly, our results indicate that better-performing deep learning models in terms of brain age prediction on healthy patients do not result in more reliable biomarkers.
</details>
<details>
<summary>摘要</summary>
��colon cancer prediction using neuroimaging data has shown great potential as an indicator of overall brain health and successful aging, as well as a disease biomarker. Deep learning models have been established as reliable and efficient brain age estimators, being trained to predict the chronological age of healthy subjects. In this paper, we investigate the impact of a pre-training step on deep learning models for brain age prediction. More precisely, instead of the common approach of pre-training on natural imaging classification, we propose pre-training the models on brain-related tasks, which led to state-of-the-art results in our experiments on ADNI data. Furthermore, we validate the resulting brain age biomarker on images of patients with mild cognitive impairment and Alzheimer's disease. Interestingly, our results indicate that better-performing deep learning models in terms of brain age prediction on healthy patients do not result in more reliable biomarkers.Here's the word-for-word translation:��colon cancer prediction using neuroimaging data has shown great potential as an indicator of overall brain health and successful aging, as well as a disease biomarker. Deep learning models have been established as reliable and efficient brain age estimators, being trained to predict the chronological age of healthy subjects. In this paper, we investigate the impact of a pre-training step on deep learning models for brain age prediction. More precisely, instead of the common approach of pre-training on natural imaging classification, we propose pre-training the models on brain-related tasks, which led to state-of-the-art results in our experiments on ADNI data. Furthermore, we validate the resulting brain age biomarker on images of patients with mild cognitive impairment and Alzheimer's disease. Interestingly, our results indicate that better-performing deep learning models in terms of brain age prediction on healthy patients do not result in more reliable biomarkers.
</details></li>
</ul>
<hr>
<h2 id="Encoder-Complexity-Control-in-SVT-AV1-by-Speed-Adaptive-Preset-Switching"><a href="#Encoder-Complexity-Control-in-SVT-AV1-by-Speed-Adaptive-Preset-Switching" class="headerlink" title="Encoder Complexity Control in SVT-AV1 by Speed-Adaptive Preset Switching"></a>Encoder Complexity Control in SVT-AV1 by Speed-Adaptive Preset Switching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05208">http://arxiv.org/abs/2307.05208</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lena Eichermüller, Gaurang Chaudhari, Ioannis Katsavounidis, Zhijun Lei, Hassene Tmar, André Kaup, Christian Herglotz</li>
<li>for: 提高视频编码器的复杂度控制机制，以满足在线视频流量的增长和相关的编码需求。</li>
<li>methods: 使用速度自适应预设切换机制，在完整预设范围内实现用户定义的时间限制，准确率为8.9%。</li>
<li>results: 可以在编码过程中实现用户定义的时间限制，而无需添加额外延迟。<details>
<summary>Abstract</summary>
Current developments in video encoding technology lead to continuously improving compression performance but at the expense of increasingly higher computational demands. Regarding the online video traffic increases during the last years and the concomitant need for video encoding, encoder complexity control mechanisms are required to restrict the processing time to a sufficient extent in order to find a reasonable trade-off between performance and complexity. We present a complexity control mechanism in SVT-AV1 by using speed-adaptive preset switching to comply with the remaining time budget. This method enables encoding with a user-defined time constraint within the complete preset range with an average precision of 8.9 \% without introducing any additional latencies.
</details>
<details>
<summary>摘要</summary>
当前的视频编码技术发展趋势导致无限扩大压缩性能，但是同时也增加了计算负担。随着上传视频流量的增长和相关的编码需求，需要控制编码器复杂性来限制处理时间，以达到一个合理的性能和复杂性交互。我们在SVT-AV1中提出了一种复杂性控制机制，通过速度适应预设切换来遵守剩余时间预算。这种方法可以在完整的预设范围内进行编码，并且保证用户定义的时间约束，平均准确率为8.9%，不增加任何延迟。
</details></li>
</ul>
<hr>
<h2 id="HistoColAi-An-Open-Source-Web-Platform-for-Collaborative-Digital-Histology-Image-Annotation-with-AI-Driven-Predictive-Integration"><a href="#HistoColAi-An-Open-Source-Web-Platform-for-Collaborative-Digital-Histology-Image-Annotation-with-AI-Driven-Predictive-Integration" class="headerlink" title="HistoColAi: An Open-Source Web Platform for Collaborative Digital Histology Image Annotation with AI-Driven Predictive Integration"></a>HistoColAi: An Open-Source Web Platform for Collaborative Digital Histology Image Annotation with AI-Driven Predictive Integration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07525">http://arxiv.org/abs/2307.07525</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cristian Camilo Pulgarín-Ospina, Rocío del Amor, Adrián Colomera, Julio Silva-Rodríguez, Valery Naranjo</li>
<li>for: 这篇论文是为了提供一种高效的在线工具，用于可见化和注释数字 histological 图像的开发而写的。</li>
<li>methods: 本论文使用了深度学习基于方法，用于图像分析，以帮助在数字patology中提高诊断精度。</li>
<li>results: 本论文包含了一个关于细胞皮肤肿瘤诊断的用例，并进行了多个注释者的可用性研究，以验证开发的工具的可行性。<details>
<summary>Abstract</summary>
Digital pathology has become a standard in the pathology workflow due to its many benefits. These include the level of detail of the whole slide images generated and the potential immediate sharing of cases between hospitals. Recent advances in deep learning-based methods for image analysis make them of potential aid in digital pathology. However, a major limitation in developing computer-aided diagnostic systems for pathology is the lack of an intuitive and open web application for data annotation. This paper proposes a web service that efficiently provides a tool to visualize and annotate digitized histological images. In addition, to show and validate the tool, in this paper we include a use case centered on the diagnosis of spindle cell skin neoplasm for multiple annotators. A usability study of the tool is also presented, showing the feasibility of the developed tool.
</details>
<details>
<summary>摘要</summary>
数字病理学已成为病理学工作流程中的标准，这主要归功于它的多种优点。这些优点包括整个报告图像的细节水平和可以立即将案例传递给医院。Recent advances in deep learning-based methods for image analysis make them of potential aid in digital pathology. However, a major limitation in developing computer-aided diagnostic systems for pathology is the lack of an intuitive and open web application for data annotation. This paper proposes a web service that efficiently provides a tool to visualize and annotate digitized histological images. In addition, to show and validate the tool, in this paper we include a use case centered on the diagnosis of spindle cell skin neoplasm for multiple annotators. A usability study of the tool is also presented, showing the feasibility of the developed tool.Here's the translation in Traditional Chinese: digitale pathology 已成为病理学工作流程中的标准，这主要从其许多优点中来。这些优点包括整个报告图像的细节水平和可以立即将案例传递到医院。Recent advances in deep learning-based methods for image analysis make them of potential aid in digital pathology. However, a major limitation in developing computer-aided diagnostic systems for pathology is the lack of an intuitive and open web application for data annotation. This paper proposes a web service that efficiently provides a tool to visualize and annotate digitized histological images. In addition, to show and validate the tool, in this paper we include a use case centered on the diagnosis of spindle cell skin neoplasm for multiple annotators. A usability study of the tool is also presented, showing the feasibility of the developed tool.
</details></li>
</ul>
<hr>
<h2 id="Super-resolution-imaging-through-a-multimode-fiber-the-physical-upsampling-of-speckle-driven"><a href="#Super-resolution-imaging-through-a-multimode-fiber-the-physical-upsampling-of-speckle-driven" class="headerlink" title="Super-resolution imaging through a multimode fiber: the physical upsampling of speckle-driven"></a>Super-resolution imaging through a multimode fiber: the physical upsampling of speckle-driven</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05138">http://arxiv.org/abs/2307.05138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuncheng Zhang, Tingting Liu, Zhihua Xie, Yu Wang, Tong Liu, Qian Chen, Xiubao Sui</li>
<li>for: 这个论文的目的是为了提高endooscopic imaging的精度和质量，以便更好地进行微创外科手术。</li>
<li>methods: 这个论文使用了一种基于深度学习的超分解影像方法，通过physically realizing the upsampling of low-resolution images来提高模型的感知能力。</li>
<li>results: 实验表明，这种方法可以有效地提高endooscopic imaging的精度和质量，并且可以补偿数据驱动方法中缺失的信息。<details>
<summary>Abstract</summary>
Following recent advancements in multimode fiber (MMF), miniaturization of imaging endoscopes has proven crucial for minimally invasive surgery in vivo. Recent progress enabled by super-resolution imaging methods with a data-driven deep learning (DL) framework has balanced the relationship between the core size and resolution. However, most of the DL approaches lack attention to the physical properties of the speckle, which is crucial for reconciling the relationship between the magnification of super-resolution imaging and the quality of reconstruction quality. In the paper, we find that the interferometric process of speckle formation is an essential basis for creating DL models with super-resolution imaging. It physically realizes the upsampling of low-resolution (LR) images and enhances the perceptual capabilities of the models. The finding experimentally validates the role played by the physical upsampling of speckle-driven, effectively complementing the lack of information in data-driven. Experimentally, we break the restriction of the poor reconstruction quality at great magnification by inputting the same size of the speckle with the size of the high-resolution (HR) image to the model. The guidance of our research for endoscopic imaging may accelerate the further development of minimally invasive surgery.
</details>
<details>
<summary>摘要</summary>
Recent advancements in multimode fiber (MMF) have made it crucial to miniaturize imaging endoscopes for minimally invasive surgery in vivo. Recent progress in super-resolution imaging methods with a data-driven deep learning (DL) framework has balanced the relationship between the core size and resolution. However, most DL approaches overlook the physical properties of speckle, which is essential for reconciling the relationship between the magnification of super-resolution imaging and the reconstruction quality. In our paper, we find that the interferometric process of speckle formation is a crucial basis for creating DL models with super-resolution imaging. It physically realizes the upsampling of low-resolution (LR) images and enhances the perceptual capabilities of the models. Our findings experimentally validate the role played by the physical upsampling of speckle-driven, effectively complementing the lack of information in data-driven. Experimentally, we break the restriction of poor reconstruction quality at great magnification by inputting the same size of the speckle with the size of the high-resolution (HR) image to the model. Our research guidance may accelerate the further development of minimally invasive surgery.
</details></li>
</ul>
<hr>
<h2 id="Offline-and-Online-Optical-Flow-Enhancement-for-Deep-Video-Compression"><a href="#Offline-and-Online-Optical-Flow-Enhancement-for-Deep-Video-Compression" class="headerlink" title="Offline and Online Optical Flow Enhancement for Deep Video Compression"></a>Offline and Online Optical Flow Enhancement for Deep Video Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05092">http://arxiv.org/abs/2307.05092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuanbo Tang, Xihua Sheng, Zhuoyuan Li, Haotian Zhang, Li Li, Dong Liu</li>
<li>for: 提高深度视频压缩网络的效率，使其更加适应实际视频数据。</li>
<li>methods: 提出了两 stage 优化方法：离线阶段使用已训练的 optical flow 估计网络进行优化，在线阶段使用梯度下降算法对视频进行适应优化。</li>
<li>results: 实验结果显示，该方法可以在 tested 视频上实现平均 12.8% 的比特率减少，而无需增加解码器的模型或计算复杂度。<details>
<summary>Abstract</summary>
Video compression relies heavily on exploiting the temporal redundancy between video frames, which is usually achieved by estimating and using the motion information. The motion information is represented as optical flows in most of the existing deep video compression networks. Indeed, these networks often adopt pre-trained optical flow estimation networks for motion estimation. The optical flows, however, may be less suitable for video compression due to the following two factors. First, the optical flow estimation networks were trained to perform inter-frame prediction as accurately as possible, but the optical flows themselves may cost too many bits to encode. Second, the optical flow estimation networks were trained on synthetic data, and may not generalize well enough to real-world videos. We address the twofold limitations by enhancing the optical flows in two stages: offline and online. In the offline stage, we fine-tune a trained optical flow estimation network with the motion information provided by a traditional (non-deep) video compression scheme, e.g. H.266/VVC, as we believe the motion information of H.266/VVC achieves a better rate-distortion trade-off. In the online stage, we further optimize the latent features of the optical flows with a gradient descent-based algorithm for the video to be compressed, so as to enhance the adaptivity of the optical flows. We conduct experiments on a state-of-the-art deep video compression scheme, DCVC. Experimental results demonstrate that the proposed offline and online enhancement together achieves on average 12.8% bitrate saving on the tested videos, without increasing the model or computational complexity of the decoder side.
</details>
<details>
<summary>摘要</summary>
视频压缩听说很多地利用视频帧之间的时间重复性，通常通过估计和使用运动信息来实现。运动信息通常被表示为光流在大多数现有的深度视频压缩网络中。实际上，这些网络 часто采用预训练的光流估计网络进行运动估计。然而，光流可能对视频压缩不适用，因为以下两个因素：一是光流估计网络在尽可能准确地进行间帧预测，但光流本身可能cost太多比特来编码。二是光流估计网络在synthetic数据上进行训练，可能无法在实际视频中generalize好 enough。我们通过两个阶段进行优化来解决这两个限制：离线阶段和在线阶段。在离线阶段，我们使用一个已经训练过的光流估计网络，并在H.266/VVC中提供的运动信息的基础上进行微调。在在线阶段，我们使用一种基于梯度下降算法的优化方法，以适应视频压缩。我们在DCVC中进行实验，实际结果表明，我们的离线和在线优化结合使用，在测试视频上平均实现12.8%的比特率折损，无需增加解码器的模型或计算复杂度。
</details></li>
</ul>
<hr>
<h2 id="SAR-NeRF-Neural-Radiance-Fields-for-Synthetic-Aperture-Radar-Multi-View-Representation"><a href="#SAR-NeRF-Neural-Radiance-Fields-for-Synthetic-Aperture-Radar-Multi-View-Representation" class="headerlink" title="SAR-NeRF: Neural Radiance Fields for Synthetic Aperture Radar Multi-View Representation"></a>SAR-NeRF: Neural Radiance Fields for Synthetic Aperture Radar Multi-View Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05087">http://arxiv.org/abs/2307.05087</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengxin Lei, Feng Xu, Jiangtao Wei, Feng Cai, Feng Wang, Ya-Qiu Jin</li>
<li>for: 本研究旨在提出一种基于NeRF的SAR图像生成模型，以便更好地利用SAR图像的特点和特殊性。</li>
<li>methods: 本研究使用了SAR图像机理和神经网络结合，提出了一种基于NeRF的SAR图像生成模型，通过映射和投影原理，将SAR图像模型为3D图像空间中的吸收率和散射强度的函数。</li>
<li>results: 通过多种数据集的量化实验，研究者发现SAR-NeRF模型能够具有良好的多视角表示和泛化能力，并且可以在几何学学习设置下提高SAR目标分类性能。<details>
<summary>Abstract</summary>
SAR images are highly sensitive to observation configurations, and they exhibit significant variations across different viewing angles, making it challenging to represent and learn their anisotropic features. As a result, deep learning methods often generalize poorly across different view angles. Inspired by the concept of neural radiance fields (NeRF), this study combines SAR imaging mechanisms with neural networks to propose a novel NeRF model for SAR image generation. Following the mapping and projection pinciples, a set of SAR images is modeled implicitly as a function of attenuation coefficients and scattering intensities in the 3D imaging space through a differentiable rendering equation. SAR-NeRF is then constructed to learn the distribution of attenuation coefficients and scattering intensities of voxels, where the vectorized form of 3D voxel SAR rendering equation and the sampling relationship between the 3D space voxels and the 2D view ray grids are analytically derived. Through quantitative experiments on various datasets, we thoroughly assess the multi-view representation and generalization capabilities of SAR-NeRF. Additionally, it is found that SAR-NeRF augumented dataset can significantly improve SAR target classification performance under few-shot learning setup, where a 10-type classification accuracy of 91.6\% can be achieved by using only 12 images per class.
</details>
<details>
<summary>摘要</summary>
SAR图像受观测配置影响很大，并且在不同视角下显示出显著的变化，这使得深度学习方法很难通过不同视角来泛化。以NeRF原理为灵感，本研究将SAR探测机制与神经网络结合，提出了一种新的NeRF模型 дляSAR图像生成。通过 mapping和projection原理，SAR-NeRF模型将被用来学习积分吸收率和散射强度的分布，其中每个小体积的SAR渲染公式和视角网格之间的样本关系被分析 derivation。通过多种数据集的量化实验，我们全面评估了SAR-NeRF模型在多视角表示和泛化能力方面。此外，我们发现SAR-NeRF数据集可以大幅提高SAR目标分类性能，特别是在少量学习setup下，只需使用12个图像per类可以 дости得10类分类精度达91.6%。
</details></li>
</ul>
<hr>
<h2 id="Towards-Anytime-Optical-Flow-Estimation-with-Event-Cameras"><a href="#Towards-Anytime-Optical-Flow-Estimation-with-Event-Cameras" class="headerlink" title="Towards Anytime Optical Flow Estimation with Event Cameras"></a>Towards Anytime Optical Flow Estimation with Event Cameras</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05033">http://arxiv.org/abs/2307.05033</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yaozhuwa/eva-flow">https://github.com/yaozhuwa/eva-flow</a></li>
<li>paper_authors: Yaozu Ye, Hao Shi, Kailun Yang, Ze Wang, Xiaoting Yin, Yaonan Wang, Kaiwei Wang</li>
<li>For: The paper is focused on developing a high-frame-rate, low-latency event representation for optical flow estimation using event cameras.* Methods: The proposed method, called EVA-Flow, uses a unified voxel grid to represent events and a stacked Spatiotemporal Motion Refinement (SMR) module to predict temporally-dense optical flow. The method also utilizes a Rectified Flow Warp Loss (RFWL) for unsupervised evaluation of intermediate optical flow.* Results: The proposed method achieves competitive performance, with super-low latency (5ms), fastest inference (9.2ms), time-dense motion estimation (200Hz), and strong generalization.Here is the information in Simplified Chinese text:</li>
<li>for: 本文主要针对使用事件摄像机进行光流估计。</li>
<li>methods: 提议的方法使用团结矩阵表示事件，并使用堆叠的空间时间运动级化模块预测时间密集的光流。方法还利用Rectified Flow Warp Loss（RFWL）进行不监督的中间光流评估。</li>
<li>results: 提议的方法实现了竞争性的性能，具有超低延迟（5ms）、最快的推理（9.2ms）、时间密集的运动估计（200Hz）和强大的泛化性。<details>
<summary>Abstract</summary>
Event cameras are capable of responding to log-brightness changes in microseconds. Its characteristic of producing responses only to the changing region is particularly suitable for optical flow estimation. In contrast to the super low-latency response speed of event cameras, existing datasets collected via event cameras, however, only provide limited frame rate optical flow ground truth, (e.g., at 10Hz), greatly restricting the potential of event-driven optical flow. To address this challenge, we put forward a high-frame-rate, low-latency event representation Unified Voxel Grid, sequentially fed into the network bin by bin. We then propose EVA-Flow, an EVent-based Anytime Flow estimation network to produce high-frame-rate event optical flow with only low-frame-rate optical flow ground truth for supervision. The key component of our EVA-Flow is the stacked Spatiotemporal Motion Refinement (SMR) module, which predicts temporally-dense optical flow and enhances the accuracy via spatial-temporal motion refinement. The time-dense feature warping utilized in the SMR module provides implicit supervision for the intermediate optical flow. Additionally, we introduce the Rectified Flow Warp Loss (RFWL) for the unsupervised evaluation of intermediate optical flow in the absence of ground truth. This is, to the best of our knowledge, the first work focusing on anytime optical flow estimation via event cameras. A comprehensive variety of experiments on MVSEC, DESC, and our EVA-FlowSet demonstrates that EVA-Flow achieves competitive performance, super-low-latency (5ms), fastest inference (9.2ms), time-dense motion estimation (200Hz), and strong generalization. Our code will be available at https://github.com/Yaozhuwa/EVA-Flow.
</details>
<details>
<summary>摘要</summary>
Event 摄像头可以在微秒级别响应日志亮度变化。它的特点是仅响应变化区域，特别适合光流估算。然而，现有的事件摄像头数据集仅提供有限制的帧率光流真实值（例如10Hz），大大限制了事件驱动的光流潜力。为解决这个挑战，我们提出了高帧率、低延迟事件表示 Unified Voxel Grid，顺序Feed into网络bin by bin。然后，我们提出了EVENT-based Anytime Flow estimation Network（EVA-Flow），以生成高帧率事件光流，只需低帧率光流真实值作为超vision。EVA-Flow的关键组件是堆叠的空间时间运动级化（SMR）模块，预测时间密集的光流并通过空间时间运动级化提高准确性。SMR模块使用的时间密集特征扭曲提供了隐式超vision для中间光流。此外，我们引入了Rectified Flow Warp Loss（RFWL），用于无监督评估中间光流。这是我们所知道的首个关注在事件摄像头上的任何时间光流估算工作。我们在MVSEC、DESC和我们自己的EVA-FlowSet上进行了广泛的实验，并证明了EVA-Flow可以实现竞争性表现，超低延迟（5ms），最快执行（9.2ms），时间密集运动估计（200Hz）和强大总体化。我们的代码将在https://github.com/Yaozhuwa/EVA-Flow中提供。
</details></li>
</ul>
<hr>
<h2 id="Count-Free-Single-Photon-3D-Imaging-with-Race-Logic"><a href="#Count-Free-Single-Photon-3D-Imaging-with-Race-Logic" class="headerlink" title="Count-Free Single-Photon 3D Imaging with Race Logic"></a>Count-Free Single-Photon 3D Imaging with Race Logic</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04924">http://arxiv.org/abs/2307.04924</a></li>
<li>repo_url: None</li>
<li>paper_authors: Atul Ingle, David Maier</li>
<li>for: The paper is written for developing an online approach for distance estimation in single-photon cameras (SPCs) without explicitly storing photon counts.</li>
<li>methods: The paper uses race logic to process photon streams in the time-delay domain and constructs count-free equi-depth histograms using a binner element to represent the distribution of photons.</li>
<li>results: The paper shows that the proposed method can provide an order of magnitude reduction in bandwidth and power consumption while maintaining similar distance reconstruction accuracy as conventional processing methods.Here’s the Chinese version of the three points:</li>
<li>for: 这篇论文是为了开发一种在单 photon 相机（SPC）中实现距离估计而不需要直接存储光子计数的在线方法。</li>
<li>methods: 该论文使用了竞速逻辑处理单 photon 流水，并使用一个归一化元素构建无计数的等深分布图。</li>
<li>results: 论文表明，该方法可以减少带宽和功耗消耗量，同时保持与传统处理方法相同的距离重建精度。<details>
<summary>Abstract</summary>
Single-photon cameras (SPCs) have emerged as a promising technology for high-resolution 3D imaging. A single-photon 3D camera determines the round-trip time of a laser pulse by capturing the arrival of individual photons at each camera pixel. Constructing photon-timestamp histograms is a fundamental operation for a single-photon 3D camera. However, in-pixel histogram processing is computationally expensive and requires large amount of memory per pixel. Digitizing and transferring photon timestamps to an off-sensor histogramming module is bandwidth and power hungry. Here we present an online approach for distance estimation without explicitly storing photon counts. The two key ingredients of our approach are (a) processing photon streams using race logic, which maintains photon data in the time-delay domain, and (b) constructing count-free equi-depth histograms. Equi-depth histograms are a succinct representation for ``peaky'' distributions, such as those obtained by an SPC pixel from a laser pulse reflected by a surface. Our approach uses a binner element that converges on the median (or, more generally, to another quantile) of a distribution. We cascade multiple binners to form an equi-depth histogrammer that produces multi-bin histograms. Our evaluation shows that this method can provide an order of magnitude reduction in bandwidth and power consumption while maintaining similar distance reconstruction accuracy as conventional processing methods.
</details>
<details>
<summary>摘要</summary>
单 photon 摄像机（SPC）已成为高分辨率 3D 成像技术的承诺。单 photon 3D 摄像机通过记录每个像素的各个 фотоン的到达时间来确定激光脉冲的圆涂时间。构建 photon 时间频谱 Histogram 是单 photon 3D 摄像机的基本操作之一。然而，在每个像素中进行 Histogram 处理是计算昂贵的，需要大量的内存。将 photon 时间频谱数据转移到外部 Histogramming 模块进行处理也是带宽和功耗浪费。我们现在提出了一种在线方法，不需要直接存储 photon 计数。我们的方法包括以下两个关键组成部分：1. 使用竞赛逻辑处理 photon 流，以保持 photon 数据在时延频域中。2. 使用 equi-depth histogram 构建器，通过 converges onto the median （或更一般地，另一个量化）的方式，来生成具有 "peaky" 分布的 histogram。equi-depth histogram 是一种简洁的表示方式，用于描述由 SPC 像素反射激光脉冲后得到的分布。我们的方法使用一个 binner 元素，该元素 converge 到分布的中值（或更一般地，另一个量化）。我们将多个 binner 元素串接起来，形成一个 equi-depth histogrammer，该机制可以生成多个 histogram。我们的评估表明，这种方法可以在带宽和功耗上减少一个数量级，而保持与传统处理方法相似的距离重建精度。
</details></li>
</ul>
<hr>
<h2 id="Kinematically-Decoupled-Impedance-Control-for-Fast-Object-Visual-Servoing-and-Grasping-on-Quadruped-Manipulators"><a href="#Kinematically-Decoupled-Impedance-Control-for-Fast-Object-Visual-Servoing-and-Grasping-on-Quadruped-Manipulators" class="headerlink" title="Kinematically-Decoupled Impedance Control for Fast Object Visual Servoing and Grasping on Quadruped Manipulators"></a>Kinematically-Decoupled Impedance Control for Fast Object Visual Servoing and Grasping on Quadruped Manipulators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04918">http://arxiv.org/abs/2307.04918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Riccardo Parosi, Mattia Risiglione, Darwin G. Caldwell, Claudio Semini, Victor Barasuol</li>
<li>for: 这 paper 是为了提出一个控制管道，用于对物体进行搜索、接近和抓取（SAG），基于分离的机械臂链和阻力控制，并 integrate 图像基于视能服务（IBVS）。</li>
<li>methods: 该管道使用了分离的机械臂链和阻力控制，并 integrate 图像基于视能服务（IBVS）。</li>
<li>results: 经过各种实验，提出的方法能够在动态移动的四足机器人上实现稳定的搜索、接近和抓取操作，并且能够抗抵抗外部干扰。<details>
<summary>Abstract</summary>
We propose a control pipeline for SAG (Searching, Approaching, and Grasping) of objects, based on a decoupled arm kinematic chain and impedance control, which integrates image-based visual servoing (IBVS). The kinematic decoupling allows for fast end-effector motions and recovery that leads to robust visual servoing. The whole approach and pipeline can be generalized for any mobile platform (wheeled or tracked vehicles), but is most suitable for dynamically moving quadruped manipulators thanks to their reactivity against disturbances. The compliance of the impedance controller makes the robot safer for interactions with humans and the environment. We demonstrate the performance and robustness of the proposed approach with various experiments on our 140 kg HyQReal quadruped robot equipped with a 7-DoF manipulator arm. The experiments consider dynamic locomotion, tracking under external disturbances, and fast motions of the target object.
</details>
<details>
<summary>摘要</summary>
我们提出了一个SAG（搜索、接近和抓取）控制管道，基于分离式机械臂链和弹簧控制，并 integrates 图像基于视服务（IBVS）。机械链的分离使得结束器速度快，并且可以快速恢复，从而实现了可靠的视服务。整个方法和管道可以应用于任何移动平台（轮式或轨道车辆），但是最适合动态移动四足机械人，因为它们对干扰的反应更强。弹簧控制器的灵活性使得机器人在与人类和环境的互动中更安全。我们通过对我们7度自由度 manipulate 机械臂的HyQReal四足机械人进行多种实验，证明了我们的方法的性能和稳定性。实验包括动态移动、外部干扰追踪和目标物体快速移动。
</details></li>
</ul>
<hr>
<h2 id="Rapid-Deforestation-and-Burned-Area-Detection-using-Deep-Multimodal-Learning-on-Satellite-Imagery"><a href="#Rapid-Deforestation-and-Burned-Area-Detection-using-Deep-Multimodal-Learning-on-Satellite-Imagery" class="headerlink" title="Rapid Deforestation and Burned Area Detection using Deep Multimodal Learning on Satellite Imagery"></a>Rapid Deforestation and Burned Area Detection using Deep Multimodal Learning on Satellite Imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04916">http://arxiv.org/abs/2307.04916</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/h2oai/cvpr-multiearth-deforestation-segmentation">https://github.com/h2oai/cvpr-multiearth-deforestation-segmentation</a></li>
<li>paper_authors: Gabor Fodor, Marcos V. Conde</li>
<li>for: 这个研究报告的目的是提出一种基于多模态卫星影像和远程感知技术的方法，用于估计亚马逊热带雨林中的Deforestation和野火检测。</li>
<li>methods: 该研究使用了 convolutional neural networks (CNNs) 和完整的数据处理技术来解决这些问题。 dataset 包括从 Sentinel、Landsat、VIIRS 和 MODIS 卫星获取的 curaed 图像和多个通道频率。</li>
<li>results: 该方法在未见图像上达到了高精度的Deforestation 估计和烧毁地带检测。Here’s the same information in English:</li>
<li>for: The purpose of this research paper is to propose a method based on multimodal satellite imagery and remote sensing technology for estimating deforestation and detecting wildfire in the Amazon region.</li>
<li>methods: The research uses convolutional neural networks (CNNs) and comprehensive data processing techniques to solve these problems. The dataset includes curated images and diverse channel bands from Sentinel, Landsat, VIIRS, and MODIS satellites.</li>
<li>results: The method achieves high-precision deforestation estimation and burned area detection on unseen images from the region.<details>
<summary>Abstract</summary>
Deforestation estimation and fire detection in the Amazon forest poses a significant challenge due to the vast size of the area and the limited accessibility. However, these are crucial problems that lead to severe environmental consequences, including climate change, global warming, and biodiversity loss. To effectively address this problem, multimodal satellite imagery and remote sensing offer a promising solution for estimating deforestation and detecting wildfire in the Amazonia region. This research paper introduces a new curated dataset and a deep learning-based approach to solve these problems using convolutional neural networks (CNNs) and comprehensive data processing techniques. Our dataset includes curated images and diverse channel bands from Sentinel, Landsat, VIIRS, and MODIS satellites. We design the dataset considering different spatial and temporal resolution requirements. Our method successfully achieves high-precision deforestation estimation and burned area detection on unseen images from the region. Our code, models and dataset are open source: https://github.com/h2oai/cvpr-multiearth-deforestation-segmentation
</details>
<details>
<summary>摘要</summary>
亚马逊森林的排除和野火检测 pose 一个 significative 挑战，因为该区域的面积很大，而访问也很困难。然而，这些问题会导致严重的环境后果，包括气候变化、全球变暖和生物多样性损失。为了有效地解决这个问题，多模态卫星成像和远程感知提供了一个有希望的解决方案，通过 convolutional neural networks (CNNs) 和全面的数据处理技术。我们的数据集包括手动准备的图像和多种通道频谱的卫星图像，包括 Sentinel、Landsat、VIIRS 和 MODIS 卫星。我们设计数据集，考虑不同的空间和时间分辨率要求。我们的方法在未见图像上达到了高精度的排除和烧毁地带检测。我们的代码、模型和数据集都是开源的，可以在 GitHub 上找到：https://github.com/h2oai/cvpr-multiearth-deforestation-segmentation。
</details></li>
</ul>
<hr>
<h2 id="KU-DMIS-MSRA-at-RadSum23-Pre-trained-Vision-Language-Model-for-Radiology-Report-Summarization"><a href="#KU-DMIS-MSRA-at-RadSum23-Pre-trained-Vision-Language-Model-for-Radiology-Report-Summarization" class="headerlink" title="KU-DMIS-MSRA at RadSum23: Pre-trained Vision-Language Model for Radiology Report Summarization"></a>KU-DMIS-MSRA at RadSum23: Pre-trained Vision-Language Model for Radiology Report Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07409">http://arxiv.org/abs/2307.07409</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gangwoo Kim, Hajung Kim, Lei Ji, Seongsu Bae, Chanhwi Kim, Mujeen Sung, Hyunjae Kim, Kun Yan, Eric Chang, Jaewoo Kang</li>
<li>for: 这个研究是为了开发一个新的预训掌握视觉语言模型（VLM），用于胸部X射影领域。</li>
<li>methods: 这个模型首先在多个多元领域的资料上进行预训，然后转移到胸部X射影领域。我们将不同的领域对应 задачу聚合为一个简单的序列转换架构，让模型从有限资源中获得所需的知识和技能。</li>
<li>results: 我们的模型在 BioNLP 共享任务的评分数据上显示出了超过其他模型的表现，并在 RadSum23 隐藏测试集的领导排行板上获得了第一名。<details>
<summary>Abstract</summary>
In this paper, we introduce CheXOFA, a new pre-trained vision-language model (VLM) for the chest X-ray domain. Our model is initially pre-trained on various multimodal datasets within the general domain before being transferred to the chest X-ray domain. Following a prominent VLM, we unify various domain-specific tasks into a simple sequence-to-sequence schema. It enables the model to effectively learn the required knowledge and skills from limited resources in the domain. Demonstrating superior performance on the benchmark datasets provided by the BioNLP shared task, our model benefits from its training across multiple tasks and domains. With subtle techniques including ensemble and factual calibration, our system achieves first place on the RadSum23 leaderboard for the hidden test set.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们引入CheXOFA，一种新的预训练视语言模型（VLM），用于胸X光领域。我们的模型首先在通用领域中预训练于多种多样的数据集，然后转移到胸X光领域。借鉴一些知名的VLM，我们将各种领域特定任务整合成简单的序列到sequence schema。这使得模型可以很好地从有限资源中学习需要的知识和技能。在BioNLP共享任务提供的标准 datasets上表现出色，我们的模型受益于跨任务和领域的训练。通过微妙的技巧，包括ensemble和事实抽象，我们的系统在RadSum23隐藏测试集上达到了第一名。
</details></li>
</ul>
<hr>
<h2 id="CVPR-MultiEarth-2023-Deforestation-Estimation-Challenge-SpaceVision4Amazon"><a href="#CVPR-MultiEarth-2023-Deforestation-Estimation-Challenge-SpaceVision4Amazon" class="headerlink" title="CVPR MultiEarth 2023 Deforestation Estimation Challenge:SpaceVision4Amazon"></a>CVPR MultiEarth 2023 Deforestation Estimation Challenge:SpaceVision4Amazon</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04715">http://arxiv.org/abs/2307.04715</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sunita Arya, S Manthira Moorthi, Debajyoti Dhar</li>
<li>for: 本研究提出了一种基于注意力指导UNet架构的森林砍伐估计方法，使用电Optical（EO）和Synthetic Aperture Radar（SAR）卫星影像。</li>
<li>methods: 用Landsat-8和Sentinel-1卫星数据进行训练和验证提议的模型，由于数据的时空尺度不可用，因此每架次训练一个模型。</li>
<li>results: 训练时LandSat-8模型达到了93.45%的训练和验证像素精度，Sentinel-2模型达到了83.87%的像素精度。测试集评估中，模型达到了84.70%的像素精度，F1-Score为0.79，IOU为0.69。<details>
<summary>Abstract</summary>
In this paper, we present a deforestation estimation method based on attention guided UNet architecture using Electro-Optical (EO) and Synthetic Aperture Radar (SAR) satellite imagery. For optical images, Landsat-8 and for SAR imagery, Sentinel-1 data have been used to train and validate the proposed model. Due to the unavailability of temporally and spatially collocated data, individual model has been trained for each sensor. During training time Landsat-8 model achieved training and validation pixel accuracy of 93.45% and Sentinel-2 model achieved 83.87% pixel accuracy. During the test set evaluation, the model achieved pixel accuracy of 84.70% with F1-Score of 0.79 and IoU of 0.69.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种基于注意力引导的UNet架构的森林伐木估计方法，使用电Optical（EO）和Synthetic Aperture Radar（SAR）卫星图像。对于光学图像，我们使用了Landstat-8数据进行训练和验证；对于SAR图像，我们使用了Sentinel-1数据。由于数据的 temporally和spatially不同，我们需要单独训练每款感知器。在训练时，Landstat-8模型在训练和验证像素精度方面达到了93.45%，而Sentinel-2模型在验证像素精度方面达到了83.87%。在测试集评估中，模型在像素精度方面达到了84.70%，F1-Score为0.79，IOU为0.69。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/11/eess.IV_2023_07_11/" data-id="clly4xtfn00d9vl886lyddcon" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/11/eess.AS_2023_07_11/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.AS - 2023-07-11 22:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/10/cs.AI_2023_07_10/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.AI - 2023-07-10 20:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">59</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">55</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">108</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
