
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-07-24 18:00:00 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="QAmplifyNet: Pushing the Boundaries of Supply Chain Backorder Prediction Using Interpretable Hybrid Quantum - Classical Neural Network paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.12906 repo_url: None paper_a">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-07-24 18:00:00">
<meta property="og:url" content="http://example.com/2023/07/24/cs.LG_2023_07_24/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="QAmplifyNet: Pushing the Boundaries of Supply Chain Backorder Prediction Using Interpretable Hybrid Quantum - Classical Neural Network paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.12906 repo_url: None paper_a">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-23T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:37.404Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_07_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/24/cs.LG_2023_07_24/" class="article-date">
  <time datetime="2023-07-23T16:00:00.000Z" itemprop="datePublished">2023-07-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-07-24 18:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="QAmplifyNet-Pushing-the-Boundaries-of-Supply-Chain-Backorder-Prediction-Using-Interpretable-Hybrid-Quantum-Classical-Neural-Network"><a href="#QAmplifyNet-Pushing-the-Boundaries-of-Supply-Chain-Backorder-Prediction-Using-Interpretable-Hybrid-Quantum-Classical-Neural-Network" class="headerlink" title="QAmplifyNet: Pushing the Boundaries of Supply Chain Backorder Prediction Using Interpretable Hybrid Quantum - Classical Neural Network"></a>QAmplifyNet: Pushing the Boundaries of Supply Chain Backorder Prediction Using Interpretable Hybrid Quantum - Classical Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12906">http://arxiv.org/abs/2307.12906</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Abrar Jahin, Md Sakib Hossain Shovon, Md. Saiful Islam, Jungpil Shin, M. F. Mridha, Yuichi Okuyama<br>for: 这篇论文是用于优化供应链管理，提高存储控制、减少费用和提高顾客满意度。methods: 本研究提出了一个新的方法框架，使用量子构思的技术来预测背负货，并且可以处理短时间和不均匀的数据。results: 实验结果显示，QAmplifyNet模型在短时间和不均匀的数据上预测背负货比照 classical models、量子ensemble、量子神经网和深度强化学习模型更加有效。此外，QAmplifyNet模型可以增加存储控制、减少背负货和提高操作效率。<details>
<summary>Abstract</summary>
Supply chain management relies on accurate backorder prediction for optimizing inventory control, reducing costs, and enhancing customer satisfaction. However, traditional machine-learning models struggle with large-scale datasets and complex relationships, hindering real-world data collection. This research introduces a novel methodological framework for supply chain backorder prediction, addressing the challenge of handling large datasets. Our proposed model, QAmplifyNet, employs quantum-inspired techniques within a quantum-classical neural network to predict backorders effectively on short and imbalanced datasets. Experimental evaluations on a benchmark dataset demonstrate QAmplifyNet's superiority over classical models, quantum ensembles, quantum neural networks, and deep reinforcement learning. Its proficiency in handling short, imbalanced datasets makes it an ideal solution for supply chain management. To enhance model interpretability, we use Explainable Artificial Intelligence techniques. Practical implications include improved inventory control, reduced backorders, and enhanced operational efficiency. QAmplifyNet seamlessly integrates into real-world supply chain management systems, enabling proactive decision-making and efficient resource allocation. Future work involves exploring additional quantum-inspired techniques, expanding the dataset, and investigating other supply chain applications. This research unlocks the potential of quantum computing in supply chain optimization and paves the way for further exploration of quantum-inspired machine learning models in supply chain management. Our framework and QAmplifyNet model offer a breakthrough approach to supply chain backorder prediction, providing superior performance and opening new avenues for leveraging quantum-inspired techniques in supply chain management.
</details>
<details>
<summary>摘要</summary>
供应链管理需要准确预测后备订的能力，以优化存储控制、降低成本和提高客户满意度。然而，传统的机器学习模型在大规模数据集和复杂关系下受到限制，阻碍了实际数据收集。这项研究提出了一个新的方法ológical framework for supply chain backorder prediction，解决了处理大规模数据集的挑战。我们提出的模型，QAmplifyNet，采用了量子启发的技术在量子-классиical neural network中预测后备订，在短和不均衡的数据集上显示出了优于经典模型、量子ensemble、量子神经网络和深度强化学习的性能。实验评估表明，QAmplifyNet可以有效地处理短和不均衡的数据集，使其成为供应链管理中的理想解决方案。为提高模型可读性，我们使用了可解释人工智能技术。实际应用包括改善存储控制、减少后备订和提高运营效率。QAmplifyNet可以轻松地与实际供应链管理系统集成，允许您采取激活的决策和有效地分配资源。未来的工作包括进一步探索量子启发技术、扩大数据集和探索其他供应链应用。本研究开阔了量子计算在供应链优化中的潜在可能性，并为Quantum-inspired机器学习模型在供应链管理中铺平了道路。我们的框架和QAmplifyNet模型提供了一个重要的突破在供应链后备订预测方面，为供应链管理提供了超越性能，并开启了新的可能性 для利用量子启发技术在供应链管理中。
</details></li>
</ul>
<hr>
<h2 id="Universal-Approximation-Theorem-and-error-bounds-for-quantum-neural-networks-and-quantum-reservoirs"><a href="#Universal-Approximation-Theorem-and-error-bounds-for-quantum-neural-networks-and-quantum-reservoirs" class="headerlink" title="Universal Approximation Theorem and error bounds for quantum neural networks and quantum reservoirs"></a>Universal Approximation Theorem and error bounds for quantum neural networks and quantum reservoirs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12904">http://arxiv.org/abs/2307.12904</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Gonon, Antoine Jacquier</li>
<li>for: 这个论文是为了研究量子神经网络如何实现类似于类传统神经网络的功能 approximating 函数。</li>
<li>methods: 这个论文使用了 parameterised 量子Circuit 来实现这种函数的近似。</li>
<li>results: 这个论文提供了准确的误差上限 bounds  для特定的函数类型，并将这些结果扩展到随机量子Circuit 中，模拟类传统神经网络。结果表明，一个量子神经网络 WITH $\mathcal{O}(\varepsilon^{-2})$ 权重和 $\mathcal{O} (\lceil \log_2(\varepsilon^{-1}) \rceil)$ 量子 bits 可以达到精度 $\varepsilon&gt;0$ 当近似函数。<details>
<summary>Abstract</summary>
Universal approximation theorems are the foundations of classical neural networks, providing theoretical guarantees that the latter are able to approximate maps of interest. Recent results have shown that this can also be achieved in a quantum setting, whereby classical functions can be approximated by parameterised quantum circuits. We provide here precise error bounds for specific classes of functions and extend these results to the interesting new setup of randomised quantum circuits, mimicking classical reservoir neural networks. Our results show in particular that a quantum neural network with $\mathcal{O}(\varepsilon^{-2})$ weights and $\mathcal{O} (\lceil \log_2(\varepsilon^{-1}) \rceil)$ qubits suffices to achieve accuracy $\varepsilon>0$ when approximating functions with integrable Fourier transform.
</details>
<details>
<summary>摘要</summary>
“通用近似定理是经典神经网络的基础，提供了理论保证，这些神经网络能够近似 Interesting 的函数。现在，我们可以在量子设置下实现这一点，其中经典函数可以被参数化的量子Circuit 近似。我们在这里提供了具体的误差上限，并扩展到随机量子Circuit 的新设置，模拟类似于经典激发神经网络。我们的结果表明，一个具有 $\mathcal{O}(\varepsilon^{-2})$ 的权重和 $\mathcal{O} (\lceil \log_2(\varepsilon^{-1}) \rceil)$ 的量子比特数据可以实现 $\varepsilon>0$ 的准确性，当近似函数的傅ри埃transform 是可积分的。”Note: "通用" (pinyin: "gòngyòu") is translated as "universal" in Simplified Chinese, but it is a more literal translation of "common" or "shared".
</details></li>
</ul>
<hr>
<h2 id="Anytime-Model-Selection-in-Linear-Bandits"><a href="#Anytime-Model-Selection-in-Linear-Bandits" class="headerlink" title="Anytime Model Selection in Linear Bandits"></a>Anytime Model Selection in Linear Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12897">http://arxiv.org/abs/2307.12897</a></li>
<li>repo_url: None</li>
<li>paper_authors: Parnian Kassraie, Aldo Pacchiano, Nicolas Emmenegger, Andreas Krause</li>
<li>for: 该论文主要针对bandit优化中的模型选择问题，即在搜索和利用之间寻求平衡。</li>
<li>methods: 该论文提出了一种自然的方法，即使用在线学习算法，将不同的模型当做专家进行选择。</li>
<li>results: 论文提出的ALEXP方法可以在linear bandit中实现$\log M$的依赖性，从而提高 regret的性能。ALEXP方法也具有任何时间的保证，不需要知道Horizon $n$ 的值，也不需要初始阶段做出纯粹的探索。<details>
<summary>Abstract</summary>
Model selection in the context of bandit optimization is a challenging problem, as it requires balancing exploration and exploitation not only for action selection, but also for model selection. One natural approach is to rely on online learning algorithms that treat different models as experts. Existing methods, however, scale poorly ($\text{poly}M$) with the number of models $M$ in terms of their regret. Our key insight is that, for model selection in linear bandits, we can emulate full-information feedback to the online learner with a favorable bias-variance trade-off. This allows us to develop ALEXP, which has an exponentially improved ($\log M$) dependence on $M$ for its regret. ALEXP has anytime guarantees on its regret, and neither requires knowledge of the horizon $n$, nor relies on an initial purely exploratory stage. Our approach utilizes a novel time-uniform analysis of the Lasso, establishing a new connection between online learning and high-dimensional statistics.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本为 Traditional Chinese.<</SYS>>在带刺探索优化中，选择模型是一项具有挑战性的问题，因为它需要在执行选择和模型选择之间进行平衡探索和利用。一种自然的方法是通过在线学习算法来处理不同的模型。现有方法，然而，对数模型 $M$ 的规模不良 ($\text{poly}M$)。我们的关键发现是，在线选择器中，对于 linear 带刺探索，我们可以通过利用完整的信息反馈来让在线学习者具有有利的偏差-变量规则。这使得我们可以开发 ALEXP，它在 $M$ 上有 exponential 提高的（log $M$）的 regret。ALEXP 具有任何时间的保证，并不需要知道预测时间 $n$，也不需要早期纯探索阶段。我们的方法利用了一种新的时间均分析方法，确定了在线学习和高维统计之间的新连接。
</details></li>
</ul>
<hr>
<h2 id="A-Statistical-View-of-Column-Subset-Selection"><a href="#A-Statistical-View-of-Column-Subset-Selection" class="headerlink" title="A Statistical View of Column Subset Selection"></a>A Statistical View of Column Subset Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12892">http://arxiv.org/abs/2307.12892</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anavsood/css">https://github.com/anavsood/css</a></li>
<li>paper_authors: Anav Sood, Trevor Hastie</li>
<li>for: 本研究解决了选择小型表示变量集的问题，即简化数据集中变量的维度减少问题。</li>
<li>methods: 本研究使用了列子集选择（CSS）和主要变量选择（PCA）两种方法，并证明了这两种方法是等价的。</li>
<li>results: 本研究提出了一种基于摘要统计量的高效CSS方法，以及在缺失和&#x2F;或截断数据的情况下CSS方法的应用。此外，本研究还提出了一种在假设检测框架下选择CSS子集的方法。<details>
<summary>Abstract</summary>
We consider the problem of selecting a small subset of representative variables from a large dataset. In the computer science literature, this dimensionality reduction problem is typically formalized as Column Subset Selection (CSS). Meanwhile, the typical statistical formalization is to find an information-maximizing set of Principal Variables. This paper shows that these two approaches are equivalent, and moreover, both can be viewed as maximum likelihood estimation within a certain semi-parametric model. Using these connections, we show how to efficiently (1) perform CSS using only summary statistics from the original dataset; (2) perform CSS in the presence of missing and/or censored data; and (3) select the subset size for CSS in a hypothesis testing framework.
</details>
<details>
<summary>摘要</summary>
我们考虑一个选择小型表示变量的问题，从大量数据集中选择一小部分表示变量。在计算机科学文献中，这个维度减少问题通常被称为列子集选择（CSS）。而在统计学文献中，通常是找到信息最大化的主要变量集。这篇论文表明了这两种方法是等价的，并且它们都可以视为一种最大化概率的最大化问题，在某种半parametric模型下进行解释。使用这些连接，我们展示了如何：1. 使用原始数据集的摘要统计来高效地执行CSS;2. 在缺失和/或截止数据的情况下执行CSS;3. 在假设测试框架下选择CSS中的子集大小。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Stereotype-Identification-through-Reasoning"><a href="#Interpretable-Stereotype-Identification-through-Reasoning" class="headerlink" title="Interpretable Stereotype Identification through Reasoning"></a>Interpretable Stereotype Identification through Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00071">http://arxiv.org/abs/2308.00071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacob-Junqi Tian, Omkar Dige, David Emerson, Faiza Khan Khattak</li>
<li>for: 避免不必要地启发系统性歧视，语言模型的开发中需要考虑公平性。</li>
<li>methods: 使用 Vicuna-13B-v1.3 进行零shot种族标识 tasks，并分析 scaling 和 reasoning 的效果。</li>
<li>results: 结果显示，理解能力在 zero-shot 种族标识任务中发挥了重要作用，并且 reasoning 的性能提升超过了 scaling 的提升。<details>
<summary>Abstract</summary>
Given that language models are trained on vast datasets that may contain inherent biases, there is a potential danger of inadvertently perpetuating systemic discrimination. Consequently, it becomes essential to examine and address biases in language models, integrating fairness into their development to ensure these models are equitable and free from bias. In this work, we demonstrate the importance of reasoning in zero-shot stereotype identification based on Vicuna-13B-v1.3. While we do observe improved accuracy by scaling from 13B to 33B, we show that the performance gain from reasoning significantly exceeds the gain from scaling up. Our findings suggest that reasoning could be a key factor that enables LLMs to trescend the scaling law on out-of-domain tasks such as stereotype identification. Additionally, through a qualitative analysis of select reasoning traces, we highlight how reasoning enhances not just accuracy but also the interpretability of the decision.
</details>
<details>
<summary>摘要</summary>
language models 可能会遗传恶习，因此必须评估和修正这些模型中的偏见，以确保它们是公平的和无偏见的。在这项工作中，我们示出了逻辑的重要性在零shot刻板刻结定位中，并证明了逻辑的性能提升超过了缩放的提升。我们的发现表明，逻辑可能是LLMs在不同领域任务中的关键因素，帮助它们突破域外任务的缩放法则。此外，通过选择的逻辑轨迹分析，我们强调了逻辑不仅提高了准确率，还提高了决策的可读性。
</details></li>
</ul>
<hr>
<h2 id="Data-free-Black-box-Attack-based-on-Diffusion-Model"><a href="#Data-free-Black-box-Attack-based-on-Diffusion-Model" class="headerlink" title="Data-free Black-box Attack based on Diffusion Model"></a>Data-free Black-box Attack based on Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12872">http://arxiv.org/abs/2307.12872</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingwen Shao, Lingzhuang Meng, Yuanjian Qiao, Lixu Zhang, Wangmeng Zuo</li>
<li>for: 增强数据隐身攻击的效率和准确性，使用扩散模型生成数据并提出一种基于扩散模型的数据隐身攻击方案。</li>
<li>methods: 使用扩散模型生成数据，并提出一种基于扩散模型的数据隐身攻击方案。该方法使用了秘密代码改进（LCA）方法来引导扩散模型生成数据，以确保数据符合目标模型的批判标准。</li>
<li>results: 通过实验表明，使用扩散模型生成数据和LCA方法可以增强数据隐身攻击的效率和准确性，并且可以在不同的目标模型上实现更高的攻击成功率和更低的查询预算。<details>
<summary>Abstract</summary>
Since the training data for the target model in a data-free black-box attack is not available, most recent schemes utilize GANs to generate data for training substitute model. However, these GANs-based schemes suffer from low training efficiency as the generator needs to be retrained for each target model during the substitute training process, as well as low generation quality. To overcome these limitations, we consider utilizing the diffusion model to generate data, and propose a data-free black-box attack scheme based on diffusion model to improve the efficiency and accuracy of substitute training. Despite the data generated by the diffusion model exhibits high quality, it presents diverse domain distributions and contains many samples that do not meet the discriminative criteria of the target model. To further facilitate the diffusion model to generate data suitable for the target model, we propose a Latent Code Augmentation (LCA) method to guide the diffusion model in generating data. With the guidance of LCA, the data generated by the diffusion model not only meets the discriminative criteria of the target model but also exhibits high diversity. By utilizing this data, it is possible to train substitute model that closely resemble the target model more efficiently. Extensive experiments demonstrate that our LCA achieves higher attack success rates and requires fewer query budgets compared to GANs-based schemes for different target models.
</details>
<details>
<summary>摘要</summary>
因为目标模型的训练数据不可用，现今大多数方案使用GANs生成数据来训练代理模型。然而，这些GANs基本方案受到低效率和低生成质量的限制。为了超越这些限制，我们考虑使用扩散模型生成数据，并提出了基于扩散模型的数据 свобо黑盒攻击方案，以提高代理训练的效率和准确性。尽管扩散模型生成的数据具有高质量，但它们具有多样的领域分布和含有许多不符合目标模型的判别标准的样本。为了使扩散模型更好地生成适合目标模型的数据，我们提议使用秘密代码增强（LCA）方法来导向扩散模型。通过LCA的导向，扩散模型生成的数据不仅满足目标模型的判别标准，而且具有高多样性。通过利用这些数据，可以更高效地训练代理模型，并且需要更少的查询预算。我们的LCA在不同的目标模型上实现了更高的攻击成功率和更低的查询预算。
</details></li>
</ul>
<hr>
<h2 id="Stochastic-Step-wise-Feature-Selection-for-Exponential-Random-Graph-Models-ERGMs"><a href="#Stochastic-Step-wise-Feature-Selection-for-Exponential-Random-Graph-Models-ERGMs" class="headerlink" title="Stochastic Step-wise Feature Selection for Exponential Random Graph Models (ERGMs)"></a>Stochastic Step-wise Feature Selection for Exponential Random Graph Models (ERGMs)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12862">http://arxiv.org/abs/2307.12862</a></li>
<li>repo_url: None</li>
<li>paper_authors: Helal El-Zaatari, Fei Yu, Michael R Kosorok</li>
<li>for: 这种研究旨在提高社会网络模型中的准确性，以便更好地理解不同科学领域中的网络现象。</li>
<li>methods: 这种方法使用生成杂化图模型（ERGMs）来捕捉网络依赖关系，并通过内生变量选择来避免模型缺失和准确性问题。</li>
<li>results: 研究人员通过实验和严格分析，发现这种方法可以减轻计算负担，提高网络模型的准确性，并为不同科学领域的网络分析提供实际的应用。<details>
<summary>Abstract</summary>
Statistical analysis of social networks provides valuable insights into complex network interactions across various scientific disciplines. However, accurate modeling of networks remains challenging due to the heavy computational burden and the need to account for observed network dependencies. Exponential Random Graph Models (ERGMs) have emerged as a promising technique used in social network modeling to capture network dependencies by incorporating endogenous variables. Nevertheless, using ERGMs poses multiple challenges, including the occurrence of ERGM degeneracy, which generates unrealistic and meaningless network structures. To address these challenges and enhance the modeling of collaboration networks, we propose and test a novel approach that focuses on endogenous variable selection within ERGMs. Our method aims to overcome the computational burden and improve the accommodation of observed network dependencies, thereby facilitating more accurate and meaningful interpretations of network phenomena in various scientific fields. We conduct empirical testing and rigorous analysis to contribute to the advancement of statistical techniques and offer practical insights for network analysis.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:社交网络统计分析提供了许多有价值的信息，用于各个科学领域的复杂网络互动。然而，准确地模型网络仍然是一项挑战，因为计算负担很大，并且需要考虑观察到的网络依赖关系。泛化随机图模型（ERGMs）在社交网络模型中得到应用，以捕捉网络依赖关系，但使用ERGMs会遇到多种挑战，包括ERGM衰竭现象，这会生成无意义和不可预测的网络结构。为了解决这些挑战并提高社交网络模型的准确性，我们提出了一种新的方法，即内生变量选择在ERGMs中。我们的方法 aim to overcome the computational burden and improve the accommodation of observed network dependencies, thereby facilitating more accurate and meaningful interpretations of network phenomena in various scientific fields.我们进行了实证测试和严格分析，以贡献到统计技术的发展和实际的网络分析。
</details></li>
</ul>
<hr>
<h2 id="A-Real-World-WebAgent-with-Planning-Long-Context-Understanding-and-Program-Synthesis"><a href="#A-Real-World-WebAgent-with-Planning-Long-Context-Understanding-and-Program-Synthesis" class="headerlink" title="A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis"></a>A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12856">http://arxiv.org/abs/2307.12856</a></li>
<li>repo_url: None</li>
<li>paper_authors: Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, Aleksandra Faust</li>
<li>for: 本研究的目的是提高自主浏览器的性能，使其能够根据自然语言指令完成实际网站上的任务。</li>
<li>methods: 本研究使用了大型自然语言模型（LLM）驱动的 WebAgent 代理人，通过分解指令为可能的子指令，摘要长HTML文档为任务相关的摘要，并通过生成的Python程序进行网站上的操作。</li>
<li>results: 实验表明，我们的方法可以提高实际网站上的成功率高于50%，并且HTML-T5模型在解决HTML相关任务方面的性能高于 Priors 的SoTA，在MiniWoB网站浏览 benchmark 上达到14.9%更高的成功率和更高的准确率在离线任务规划评估中。<details>
<summary>Abstract</summary>
Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web navigation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML. We introduce WebAgent, an LLM-driven agent that can complete the tasks on real websites following natural language instructions. WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via generated Python programs from those. We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization. We empirically demonstrate that our recipe improves the success on a real website by over 50%, and that HTML-T5 is the best model to solve HTML-based tasks; achieving 14.9% higher success rate than prior SoTA on the MiniWoB web navigation benchmark and better accuracy on offline task planning evaluation.
</details>
<details>
<summary>摘要</summary>
受投入的大型自然语言模型（LLM）最近在自动化网络浏览中实现了更好的泛化和样本效率。然而，实际网站上的表现仍然受到了开放领域、有限的上下文长度和HTML的适应性的影响。我们介绍了WebAgent，一个基于LLM的代理人，可以根据自然语言指令完成实际网站上的任务。WebAgent在规划和摘要方面使用了Flan-U-PaLM和HTML-T5两种新的预训练大型自然语言模型，以提高代理人的准备和执行能力。我们在实验中证明了我们的方法可以在真实网站上提高成功率超过50%，并且HTML-T5模型在解决HTML基本任务方面表现出色，比之前的SoTA在MiniWoB网络浏览benchmark上的成功率提高14.9%。
</details></li>
</ul>
<hr>
<h2 id="Early-Neuron-Alignment-in-Two-layer-ReLU-Networks-with-Small-Initialization"><a href="#Early-Neuron-Alignment-in-Two-layer-ReLU-Networks-with-Small-Initialization" class="headerlink" title="Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization"></a>Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12851">http://arxiv.org/abs/2307.12851</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hancheng Min, René Vidal, Enrique Mallada</li>
<li>for: 这paper研究了使用梯度流的两层ReLU网络进行二分类训练，并对小初始值进行研究。</li>
<li>methods: 我们使用了一个训练集，其中每个输入数据点的标签是相关的，而不同标签的输入数据点之间是反相关的。我们通过分析神经元的方向动态来提供一个$\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$的上限，表示在训练的早期阶段，神经元在第一层尝试与输入数据点进行Alignment，这个过程中，神经元的方向会随着时间的推移而发生变化。</li>
<li>results: 我们的分析表明，在训练的早期阶段，所有神经元都可以在$\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$时间内达到良好的Alignment状态，然后loss函数会在$\mathcal{O}(\frac{1}{t})$的速度下降到零，并且第一层的weight矩阵会变得相对低矩。数字实验在MNIST数据集上 validate了我们的理论发现。<details>
<summary>Abstract</summary>
This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons' directional dynamics allows us to provide an $\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$ upper bound on the time it takes for all neurons to achieve good alignment with the input data, where $n$ is the number of data points and $\mu$ measures how well the data are separated. After the early alignment phase, the loss converges to zero at a $\mathcal{O}(\frac{1}{t})$ rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Efficiently-Learning-One-Hidden-Layer-ReLU-Networks-via-Schur-Polynomials"><a href="#Efficiently-Learning-One-Hidden-Layer-ReLU-Networks-via-Schur-Polynomials" class="headerlink" title="Efficiently Learning One-Hidden-Layer ReLU Networks via Schur Polynomials"></a>Efficiently Learning One-Hidden-Layer ReLU Networks via Schur Polynomials</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12840">http://arxiv.org/abs/2307.12840</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilias Diakonikolas, Daniel M. Kane</li>
<li>for: 学习一个线性组合的ReLU活化器 under Gaussian Distribution</li>
<li>methods: 使用tensor decomposition和Schur polynomials的方法</li>
<li>results: 实现$(dk&#x2F;\epsilon)^{O(k)}$的样本和计算复杂度，比 Priori工作更高效<details>
<summary>Abstract</summary>
We study the problem of PAC learning a linear combination of $k$ ReLU activations under the standard Gaussian distribution on $\mathbb{R}^d$ with respect to the square loss. Our main result is an efficient algorithm for this learning task with sample and computational complexity $(dk/\epsilon)^{O(k)}$, where $\epsilon>0$ is the target accuracy. Prior work had given an algorithm for this problem with complexity $(dk/\epsilon)^{h(k)}$, where the function $h(k)$ scales super-polynomially in $k$. Interestingly, the complexity of our algorithm is near-optimal within the class of Correlational Statistical Query algorithms. At a high-level, our algorithm uses tensor decomposition to identify a subspace such that all the $O(k)$-order moments are small in the orthogonal directions. Its analysis makes essential use of the theory of Schur polynomials to show that the higher-moment error tensors are small given that the lower-order ones are.
</details>
<details>
<summary>摘要</summary>
我们研究一个PAC学习问题，即在标准Normal分布上的 $\mathbb{R}^d$ 上学习一个线性结构，其中每个neuron activation是 $k$ 个ReLU活化的线性 комбіна��。我们的主要结果是一个高效的学习算法，其时间和空间复杂度为 $(dk/\epsilon)^{O(k)}$，其中 $\epsilon>0$ 是目标精度。先前的作业中的算法则有 $(dk/\epsilon)^{h(k)}$ 的时间和空间复杂度，其中函数 $h(k)$ 随 $k$ 的数量呈斜率增长。值得注意的是，我们的算法具有近乎最佳的内推优化特性，即在Correlational Statistical Query数据类中的内推优化。在高阶概念上，我们的算法使用了维度分解来找到一个子空间，使得所有 $O(k)$-阶幂都小于余弦方向上。它的分析具有重要的Schur多项式理论基础，以示出高阶幂error tensors 是小于余弦方向上的，只要低阶幂error tensors 是小于余弦方向上。
</details></li>
</ul>
<hr>
<h2 id="Learning-Provably-Robust-Estimators-for-Inverse-Problems-via-Jittering"><a href="#Learning-Provably-Robust-Estimators-for-Inverse-Problems-via-Jittering" class="headerlink" title="Learning Provably Robust Estimators for Inverse Problems via Jittering"></a>Learning Provably Robust Estimators for Inverse Problems via Jittering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12822">http://arxiv.org/abs/2307.12822</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mli-lab/robust_reconstructors_via_jittering">https://github.com/mli-lab/robust_reconstructors_via_jittering</a></li>
<li>paper_authors: Anselm Krainovic, Mahdi Soltanolkotabi, Reinhard Heckel</li>
<li>for: 这篇论文研究了 Whether jittering, a simple regularization technique, can be used to train deep neural networks to be worst-case robust for inverse problems.</li>
<li>methods: The paper uses a novel analytical characterization of the optimal $\ell_2$-worst-case robust estimator for linear denoising and trains deep neural networks (U-nets) for natural image denoising, deconvolution, and accelerated magnetic resonance imaging (MRI) with jittering.</li>
<li>results: Jittering significantly enhances the worst-case robustness, but can be suboptimal for inverse problems beyond denoising. Additionally, training on real data which often contains slight noise is somewhat robustness enhancing.<details>
<summary>Abstract</summary>
Deep neural networks provide excellent performance for inverse problems such as denoising. However, neural networks can be sensitive to adversarial or worst-case perturbations. This raises the question of whether such networks can be trained efficiently to be worst-case robust. In this paper, we investigate whether jittering, a simple regularization technique that adds isotropic Gaussian noise during training, is effective for learning worst-case robust estimators for inverse problems. While well studied for prediction in classification tasks, the effectiveness of jittering for inverse problems has not been systematically investigated. In this paper, we present a novel analytical characterization of the optimal $\ell_2$-worst-case robust estimator for linear denoising and show that jittering yields optimal robust denoisers. Furthermore, we examine jittering empirically via training deep neural networks (U-nets) for natural image denoising, deconvolution, and accelerated magnetic resonance imaging (MRI). The results show that jittering significantly enhances the worst-case robustness, but can be suboptimal for inverse problems beyond denoising. Moreover, our results imply that training on real data which often contains slight noise is somewhat robustness enhancing.
</details>
<details>
<summary>摘要</summary>
深度神经网络在逆问题中表现出色，但神经网络可能对抗性或最坏情况的扰动敏感。这引起了我们是否可以有效地在训练中使神经网络具有最坏情况Robustness的问题。在这篇论文中，我们调查了是否可以使用缓冲（jittering）这种简单的规范技术来学习逆问题中的最坏情况Robustness。虽然对于预测的分类任务有广泛的研究，但对于逆问题的效果尚未系统地调查。在这篇论文中，我们提出了一种新的分析Characterization of the optimal $\ell_2$-worst-case robust estimator for linear denoising,并证明了在训练深度神经网络（U-net）时，缓冲可以获得最佳的Robustness。此外，我们通过实验来评估缓冲的效果，并发现缓冲可以增强逆问题的最坏情况Robustness，但在逆问题超出杂噪的情况下可能不太优化。此外，我们的结果表明，训练在真实数据上，通常含有些许噪声，可以提高一些Robustness。
</details></li>
</ul>
<hr>
<h2 id="Maximal-Independent-Sets-for-Pooling-in-Graph-Neural-Networks"><a href="#Maximal-Independent-Sets-for-Pooling-in-Graph-Neural-Networks" class="headerlink" title="Maximal Independent Sets for Pooling in Graph Neural Networks"></a>Maximal Independent Sets for Pooling in Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13011">http://arxiv.org/abs/2307.13011</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stevan Stanovic, Benoit Gaüzère, Luc Brun</li>
<li>for:  This paper aims to address the problem of graph pooling, which is the process of reducing the size of a graph while preserving its important features.</li>
<li>methods:  The authors propose three pooling methods based on the notion of maximal independent sets, which avoid the drawbacks of traditional graph pooling methods such as disconnection, overconnection, and deletion of large parts of the graph.</li>
<li>results:  The authors’ experimental results confirm the relevance of maximal independent set constraints for graph pooling, and demonstrate the effectiveness of their proposed methods.<details>
<summary>Abstract</summary>
Convolutional Neural Networks (CNNs) have enabled major advances in image classification through convolution and pooling. In particular, image pooling transforms a connected discrete lattice into a reduced lattice with the same connectivity and allows reduction functions to consider all pixels in an image. However, there is no pooling that satisfies these properties for graphs. In fact, traditional graph pooling methods suffer from at least one of the following drawbacks: Graph disconnection or overconnection, low decimation ratio, and deletion of large parts of graphs. In this paper, we present three pooling methods based on the notion of maximal independent sets that avoid these pitfalls. Our experimental results confirm the relevance of maximal independent set constraints for graph pooling.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（CNNs）已经为图像分类带来了重大进步，通过卷积和聚合。特别是图像聚合将连接的离散网络转换为减小的网络，让减少函数可以考虑整个图像中的所有像素。然而， для图像不存在一种聚合方法，这些性质。实际上，传统的图像聚合方法受到至少一种以下缺点：图像分解或过连接，低减少比率，以及大量图像的删除。在这篇论文中，我们提出了三种基于最大独立集的聚合方法，避免这些缺点。我们的实验结果证明了独立集约束对图像聚合的重要性。
</details></li>
</ul>
<hr>
<h2 id="Causal-Fair-Machine-Learning-via-Rank-Preserving-Interventional-Distributions"><a href="#Causal-Fair-Machine-Learning-via-Rank-Preserving-Interventional-Distributions" class="headerlink" title="Causal Fair Machine Learning via Rank-Preserving Interventional Distributions"></a>Causal Fair Machine Learning via Rank-Preserving Interventional Distributions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12797">http://arxiv.org/abs/2307.12797</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/slds-lmu/paper_2023_cfml">https://github.com/slds-lmu/paper_2023_cfml</a></li>
<li>paper_authors: Ludwig Bothmann, Susanne Dandl, Michael Schomaker</li>
<li>for: 该论文旨在提出一种Machine Learning模型，用于 Mitigating unfairness in automated decision-making systems。</li>
<li>methods: 该论文使用了 causal thinking 和 rank-preserving interventional distributions 来定义一个FiND世界，并提出了一种 warping method  для估算这个FiND世界。</li>
<li>results: 该论文通过 simulations 和实际数据 validate了其方法和模型的评价标准，并显示了其方法可以准确地 indentify 不公正的偏见和 Mitigating unfairness。<details>
<summary>Abstract</summary>
A decision can be defined as fair if equal individuals are treated equally and unequals unequally. Adopting this definition, the task of designing machine learning models that mitigate unfairness in automated decision-making systems must include causal thinking when introducing protected attributes. Following a recent proposal, we define individuals as being normatively equal if they are equal in a fictitious, normatively desired (FiND) world, where the protected attribute has no (direct or indirect) causal effect on the target. We propose rank-preserving interventional distributions to define an estimand of this FiND world and a warping method for estimation. Evaluation criteria for both the method and resulting model are presented and validated through simulations and empirical data. With this, we show that our warping approach effectively identifies the most discriminated individuals and mitigates unfairness.
</details>
<details>
<summary>摘要</summary>
(Note: Simplified Chinese is also known as "Mandarin" or "Guoyu".)Here's the translation in Traditional Chinese:一个决策可以被定义为公平的，如果平等的个体被平等地对待，而不平等的个体则不平等地对待。遵循最近的建议，我们定义个体为在一个虚拟的、normatively desired（FiND）世界中平等，这个世界中保护 attribute 没有直接或间接的 causal effect  на target。我们提议使用排名保持分布来定义这个 FiND 世界的 estimand，并使用扭曲方法来估计。我们提出了评估这方法和结果模型的评估准则，并透过实验和实际数据进行验证。我们显示，我们的扭曲方法可以妥善地识别最受歧视的个体，并对不公做出适当的处理。
</details></li>
</ul>
<hr>
<h2 id="Compact-Capable-Harnessing-Graph-Neural-Networks-and-Edge-Convolution-for-Medical-Image-Classification"><a href="#Compact-Capable-Harnessing-Graph-Neural-Networks-and-Edge-Convolution-for-Medical-Image-Classification" class="headerlink" title="Compact &amp; Capable: Harnessing Graph Neural Networks and Edge Convolution for Medical Image Classification"></a>Compact &amp; Capable: Harnessing Graph Neural Networks and Edge Convolution for Medical Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12790">http://arxiv.org/abs/2307.12790</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anonrepo-keeper/gcnn-ec">https://github.com/anonrepo-keeper/gcnn-ec</a></li>
<li>paper_authors: Aryan Singh, Pepijn Van de Ven, Ciarán Eising, Patrick Denny</li>
<li>for: 这个研究探讨了使用图形神经网络（Graph Neural Networks，GNNs）进行医疗图像分类。</li>
<li>methods: 我们提出了一个新的模型，它结合了GNNs和边弹散，利用RGB通道特征值之间的连接来强大地表示关键图像之间的关系。</li>
<li>results: 我们的提案模型不仅与现有的深度神经网络（Deep Neural Networks，DNNs）性能相似，而且仅需1000个参数，比DNNs少了90%的训练时间和数据需求。我们与预训练的DNNs进行比较，发现GNNs在医疗图像分类任务中表现良好，并鼓励进一步探索更进阶的图形基于模型，例如图形注意力网络（Graph Attention Networks，GAT）和图形自动编码器（Graph Auto-Encoders）在医疗影像领域的应用。<details>
<summary>Abstract</summary>
Graph-based neural network models are gaining traction in the field of representation learning due to their ability to uncover latent topological relationships between entities that are otherwise challenging to identify. These models have been employed across a diverse range of domains, encompassing drug discovery, protein interactions, semantic segmentation, and fluid dynamics research. In this study, we investigate the potential of Graph Neural Networks (GNNs) for medical image classification. We introduce a novel model that combines GNNs and edge convolution, leveraging the interconnectedness of RGB channel feature values to strongly represent connections between crucial graph nodes. Our proposed model not only performs on par with state-of-the-art Deep Neural Networks (DNNs) but does so with 1000 times fewer parameters, resulting in reduced training time and data requirements. We compare our Graph Convolutional Neural Network (GCNN) to pre-trained DNNs for classifying MedMNIST dataset classes, revealing promising prospects for GNNs in medical image analysis. Our results also encourage further exploration of advanced graph-based models such as Graph Attention Networks (GAT) and Graph Auto-Encoders in the medical imaging domain. The proposed model yields more reliable, interpretable, and accurate outcomes for tasks like semantic segmentation and image classification compared to simpler GCNNs
</details>
<details>
<summary>摘要</summary>
“几何基于神经网络模型在知识学习中受到推广，因为它们可以捕捉隐藏在Entities之间的数据关系，很难通过其他方法检测。这些模型在多个领域中使用，包括药物发现、蛋白质互作、semantic segmentation和流体动力学研究。在这个研究中，我们 investigate the potential of Graph Neural Networks (GNNs) for medical image classification。我们提出了一个新的模型，它结合GNNs和边 convolution，通过RGB通道特征值之间的连接来强大地表示关键几何节点之间的连接。我们的提案模型不仅与状态预设神经网络（DNNs）相似，并且仅需1000倍少的参数，从而降低训练时间和数据需求。我们与预训DNNs进行比较，发现GCNN在MedMNIST数据集中的类别分类任务中表现出色，这点鼓励我们进一步探索高级几何基于模型，如Graph Attention Networks (GAT)和Graph Auto-Encoders在医学影像领域。我们的模型具有更可靠、可解释和精准的结果，比如semantic segmentation和图像分类任务，相比于简单的GCNN”
</details></li>
</ul>
<hr>
<h2 id="Deep-neural-network-improves-the-estimation-of-polygenic-risk-scores-for-breast-cancer"><a href="#Deep-neural-network-improves-the-estimation-of-polygenic-risk-scores-for-breast-cancer" class="headerlink" title="Deep neural network improves the estimation of polygenic risk scores for breast cancer"></a>Deep neural network improves the estimation of polygenic risk scores for breast cancer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13010">http://arxiv.org/abs/2307.13010</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrien Badré, Li Zhang, Wellington Muchero, Justin C. Reynolds, Chongle Pan</li>
<li>for: 这个研究旨在比较计算复杂疾病的遗传风险分数（PRS）的不同计算模型。</li>
<li>methods: 这个研究使用了深度神经网络（DNN）和其他机器学习技术以及统计学算法来计算PRS。</li>
<li>results: DNN模型在测试组中的AUC值为67.4%，高于其他模型的AUC值（64.2%、64.5%和62.4%）。此外，DNN模型还能够将 случа组分为高遗传风险子组和常见遗传风险子组，并且这两个子组的PRS分布不同。这使得DNN模型在18.8%的抓取率下达到90%的准确率。<details>
<summary>Abstract</summary>
Polygenic risk scores (PRS) estimate the genetic risk of an individual for a complex disease based on many genetic variants across the whole genome. In this study, we compared a series of computational models for estimation of breast cancer PRS. A deep neural network (DNN) was found to outperform alternative machine learning techniques and established statistical algorithms, including BLUP, BayesA and LDpred. In the test cohort with 50% prevalence, the Area Under the receiver operating characteristic Curve (AUC) were 67.4% for DNN, 64.2% for BLUP, 64.5% for BayesA, and 62.4% for LDpred. BLUP, BayesA, and LPpred all generated PRS that followed a normal distribution in the case population. However, the PRS generated by DNN in the case population followed a bi-modal distribution composed of two normal distributions with distinctly different means. This suggests that DNN was able to separate the case population into a high-genetic-risk case sub-population with an average PRS significantly higher than the control population and a normal-genetic-risk case sub-population with an average PRS similar to the control population. This allowed DNN to achieve 18.8% recall at 90% precision in the test cohort with 50% prevalence, which can be extrapolated to 65.4% recall at 20% precision in a general population with 12% prevalence. Interpretation of the DNN model identified salient variants that were assigned insignificant p-values by association studies, but were important for DNN prediction. These variants may be associated with the phenotype through non-linear relationships.
</details>
<details>
<summary>摘要</summary>
多因素风险分数（PRS）用于估计个体复杂疾病的遗传风险，基于整个 genomic 中的多个遗传变异。本研究比较了多种计算机模型用于PRS的估计，其中深度神经网络（DNN）被发现表现出色，超过了传统机器学习技术和已知统计方法，包括BLUP、BayesA和LDpred。在测试样本中，DNN 的 AUC 分数为 67.4%，而 BLUP、BayesA 和 LDpred 的 AUC 分数分别为 64.2%、64.5% 和 62.4%。BLUP、BayesA 和 LPpred 都生成的 PRS 遵循了正态分布，而 DNN 在 случа Population 中生成的 PRS 遵循了二元分布，由两个正态分布组成，其中一个分布的mean 明显高于控制人口的 mean。这表明 DNN 可以将 случа Population 分为高遗传风险子 популяции和正常遗传风险子 популяции两个组成部分，其中高遗传风险子 популяции的 PRS 平均值明显高于控制人口，而正常遗传风险子 популяции的 PRS 与控制人口的平均值相似。这使得 DNN 在 50% 患病率的测试样本中 achieve 18.8% 的回归率，在 20% 患病率的总人口中可以预测 65.4% 的回归率。DN 模型的解释发现，DNN 使用了一些在 association 研究中被评估为无关的变异，但这些变异对 DNN 预测有重要作用。这些变异可能与现象之间存在非线性关系。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-the-Strategy-of-Propaganda-using-Inverse-Reinforcement-Learning-Evidence-from-the-2022-Russian-Invasion-of-Ukraine"><a href="#Analyzing-the-Strategy-of-Propaganda-using-Inverse-Reinforcement-Learning-Evidence-from-the-2022-Russian-Invasion-of-Ukraine" class="headerlink" title="Analyzing the Strategy of Propaganda using Inverse Reinforcement Learning: Evidence from the 2022 Russian Invasion of Ukraine"></a>Analyzing the Strategy of Propaganda using Inverse Reinforcement Learning: Evidence from the 2022 Russian Invasion of Ukraine</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12788">http://arxiv.org/abs/2307.12788</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominique Geissler, Stefan Feuerriegel</li>
<li>For: 本研究使用 inverse reinforcement learning (IRL) 方法分析了2022年俄罗斯入侵乌克兰的社交媒体宣传战略。* Methods: 研究者使用了 IRL 方法来模型在线行为，并从推特上收集了349,455条帖子和132,131名用户的数据。* Results: 研究发现，机器人和人类用户采取不同策略：机器人主要回应了支持入侵的消息，而人类用户则主要回应了反对消息，这表明机器人寻求带virality，而人类用户更倾向于进行批评讨论。<details>
<summary>Abstract</summary>
The 2022 Russian invasion of Ukraine was accompanied by a large-scale, pro-Russian propaganda campaign on social media. However, the strategy behind the dissemination of propaganda has remained unclear, particularly how the online discourse was strategically shaped by the propagandists' community. Here, we analyze the strategy of the Twitter community using an inverse reinforcement learning (IRL) approach. Specifically, IRL allows us to model online behavior as a Markov decision process, where the goal is to infer the underlying reward structure that guides propagandists when interacting with users with a supporting or opposing stance toward the invasion. Thereby, we aim to understand empirically whether and how between-user interactions are strategically used to promote the proliferation of Russian propaganda. For this, we leverage a large-scale dataset with 349,455 posts with pro-Russian propaganda from 132,131 users. We show that bots and humans follow a different strategy: bots respond predominantly to pro-invasion messages, suggesting that they seek to drive virality; while messages indicating opposition primarily elicit responses from humans, suggesting that they tend to engage in critical discussions. To the best of our knowledge, this is the first study analyzing the strategy behind propaganda from the 2022 Russian invasion of Ukraine through the lens of IRL.
</details>
<details>
<summary>摘要</summary>
俄罗斯入侵乌克兰的2022年社交媒体宣传活动旁 accompaniment by a large-scale, pro-Russian propaganda campaign on social media. However, the strategy behind the dissemination of propaganda has remained unclear, particularly how the online discourse was strategically shaped by the propagandists' community. Here, we analyze the strategy of the Twitter community using an inverse reinforcement learning (IRL) approach. Specifically, IRL allows us to model online behavior as a Markov decision process, where the goal is to infer the underlying reward structure that guides propagandists when interacting with users with a supporting or opposing stance toward the invasion. Thereby, we aim to understand empirically whether and how between-user interactions are strategically used to promote the proliferation of Russian propaganda. For this, we leverage a large-scale dataset with 349,455 posts with pro-Russian propaganda from 132,131 users. We show that bots and humans follow a different strategy: bots respond predominantly to pro-invasion messages, suggesting that they seek to drive virality; while messages indicating opposition primarily elicit responses from humans, suggesting that they tend to engage in critical discussions. To the best of our knowledge, this is the first study analyzing the strategy behind propaganda from the 2022 Russian invasion of Ukraine through the lens of IRL.
</details></li>
</ul>
<hr>
<h2 id="Is-attention-all-you-need-in-medical-image-analysis-A-review"><a href="#Is-attention-all-you-need-in-medical-image-analysis-A-review" class="headerlink" title="Is attention all you need in medical image analysis? A review"></a>Is attention all you need in medical image analysis? A review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12775">http://arxiv.org/abs/2307.12775</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giorgos Papanastasiou, Nikolaos Dikaios, Jiahao Huang, Chengjia Wang, Guang Yang</li>
<li>for: This paper reviews and analyzes existing hybrid CNN-Transf&#x2F;Attention models for medical image analysis, with a focus on their architectural designs, breakthroughs, and opportunities for future research.</li>
<li>methods: The paper uses a systematic review approach to survey existing hybrid models, and introduces a comprehensive analysis framework for evaluating their generalization ability.</li>
<li>results: The paper discusses the breakthroughs and opportunities in hybrid CNN-Transf&#x2F;Attention models for medical image analysis, and highlights the need for further research in this area to improve their generalization ability and scientific and clinical impact.<details>
<summary>Abstract</summary>
Medical imaging is a key component in clinical diagnosis, treatment planning and clinical trial design, accounting for almost 90% of all healthcare data. CNNs achieved performance gains in medical image analysis (MIA) over the last years. CNNs can efficiently model local pixel interactions and be trained on small-scale MI data. The main disadvantage of typical CNN models is that they ignore global pixel relationships within images, which limits their generalisation ability to understand out-of-distribution data with different 'global' information. The recent progress of Artificial Intelligence gave rise to Transformers, which can learn global relationships from data. However, full Transformer models need to be trained on large-scale data and involve tremendous computational complexity. Attention and Transformer compartments (Transf/Attention) which can well maintain properties for modelling global relationships, have been proposed as lighter alternatives of full Transformers. Recently, there is an increasing trend to co-pollinate complementary local-global properties from CNN and Transf/Attention architectures, which led to a new era of hybrid models. The past years have witnessed substantial growth in hybrid CNN-Transf/Attention models across diverse MIA problems. In this systematic review, we survey existing hybrid CNN-Transf/Attention models, review and unravel key architectural designs, analyse breakthroughs, and evaluate current and future opportunities as well as challenges. We also introduced a comprehensive analysis framework on generalisation opportunities of scientific and clinical impact, based on which new data-driven domain generalisation and adaptation methods can be stimulated.
</details>
<details>
<summary>摘要</summary>
医疗成像是诊断、治疗规划和临床试验设计中关键的一部分，贡献约90%的医疗数据。过去几年，深度学习模型（CNN）在医学成像分析（MIA）中表现出了性能提升。CNN可以有效地模型本地像素间的互动，并可以在小规模的MI数据上训练。然而，典型的CNN模型忽略了图像中的全局像素关系，这限制了它们的泛化能力，无法理解不同的'全局'信息。在人工智能的进步下，变换器（Transformer） emerged，它可以从数据中学习全局关系。然而，完整的Transformer模型需要大规模的数据训练和巨大的计算复杂性。为了维护全局关系的模elling，各种Attention和Transformer分支（Transf/Attention）被提出，它们可以具有较低的计算复杂性。最近，有一种增长的趋势，即将 complementary local-global properties from CNN和Transf/Attention架构相结合，这导致了一个新的时代的半卷积模型。过去几年， hybrid CNN-Transf/Attention模型在多个 MIA 问题上呈现出了明显的增长。在这种系统性评估中，我们survey了现有的半卷积 CNN-Transf/Attention模型，检查和描述了关键的建筑设计，分析了突破点，以及评估了当前和未来的机会和挑战。我们还提出了一种总体分析框架，以便根据这种框架，开发出新的数据驱动的领域泛化和适应方法。
</details></li>
</ul>
<hr>
<h2 id="Detecting-disturbances-in-network-coupled-dynamical-systems-with-machine-learning"><a href="#Detecting-disturbances-in-network-coupled-dynamical-systems-with-machine-learning" class="headerlink" title="Detecting disturbances in network-coupled dynamical systems with machine learning"></a>Detecting disturbances in network-coupled dynamical systems with machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12771">http://arxiv.org/abs/2307.12771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Per Sebastian Skardal, Juan G. Restrepo</li>
<li>for: Identifying disturbances in network-coupled dynamical systems without knowledge of the disturbances or underlying dynamics.</li>
<li>methods: Model-free method based on machine learning, using prior observations of the system when forced by a known training function.</li>
<li>results: Able to identify the locations and properties of many different types of unknown disturbances using a variety of known forcing functions, both with linear and nonlinear disturbances.Here’s the full translation of the abstract in Simplified Chinese:</li>
<li>for: 本研究旨在无需知道外围干扰或系统动力学的情况下，对网络相互作用的动力系统中的干扰进行识别。</li>
<li>methods: 基于机器学习的模型自由方法，使用知道的训练函数来驱动系统的优先观察。</li>
<li>results: 通过使用多种知道的干扰函数，成功地识别了许多不同类型的未知干扰，包括线性和非线性干扰，并在食物网和神经活动模型中进行了应用。I hope this helps!<details>
<summary>Abstract</summary>
Identifying disturbances in network-coupled dynamical systems without knowledge of the disturbances or underlying dynamics is a problem with a wide range of applications. For example, one might want to know which nodes in the network are being disturbed and identify the type of disturbance. Here we present a model-free method based on machine learning to identify such unknown disturbances based only on prior observations of the system when forced by a known training function. We find that this method is able to identify the locations and properties of many different types of unknown disturbances using a variety of known forcing functions. We illustrate our results both with linear and nonlinear disturbances using food web and neuronal activity models. Finally, we discuss how to scale our method to large networks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>对于无知的网络相互作用系统中的干扰，无需知道干扰或系统内部动力学的情况，是一个广泛的应用问题。例如，我们可能想知道哪些节点在网络中受到干扰，并确定干扰的类型。在这里，我们提出了一种无模型方法，基于机器学习，可以基于先前观察到的系统响应，无知地预测未知干扰的位置和性质。我们发现这种方法可以预测多种不同类型的未知干扰，使用多种已知的刺激函数。我们使用食物网和神经活动模型来说明我们的结果，最后讨论如何扩展我们的方法到大型网络。Note: Simplified Chinese is a standardized form of Chinese that is used in mainland China and Singapore. Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="Nonparametric-Linear-Feature-Learning-in-Regression-Through-Regularisation"><a href="#Nonparametric-Linear-Feature-Learning-in-Regression-Through-Regularisation" class="headerlink" title="Nonparametric Linear Feature Learning in Regression Through Regularisation"></a>Nonparametric Linear Feature Learning in Regression Through Regularisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12754">http://arxiv.org/abs/2307.12754</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bertillefollain/regfeal">https://github.com/bertillefollain/regfeal</a></li>
<li>paper_authors: Bertille Follain, Umut Simsekli, Francis Bach</li>
<li>for: 本研究旨在提高自动Feature选择的效果，特别是在高维数据上，非Parametric方法 often struggle.</li>
<li>methods: 我们提出了一种新的方法 для线性特征学习，同时估计预测函数和线性子空间。我们的方法使用Empirical risk minimization，加以函数 Derivatives penalty，以 Ensure versatility.</li>
<li>results: 我们的方法可以得到一个Consistent estimator of the prediction function with explicit rates，并且在各种实验中表现出色。<details>
<summary>Abstract</summary>
Representation learning plays a crucial role in automated feature selection, particularly in the context of high-dimensional data, where non-parametric methods often struggle. In this study, we focus on supervised learning scenarios where the pertinent information resides within a lower-dimensional linear subspace of the data, namely the multi-index model. If this subspace were known, it would greatly enhance prediction, computation, and interpretation. To address this challenge, we propose a novel method for linear feature learning with non-parametric prediction, which simultaneously estimates the prediction function and the linear subspace. Our approach employs empirical risk minimisation, augmented with a penalty on function derivatives, ensuring versatility. Leveraging the orthogonality and rotation invariance properties of Hermite polynomials, we introduce our estimator, named RegFeaL. By utilising alternative minimisation, we iteratively rotate the data to improve alignment with leading directions and accurately estimate the relevant dimension in practical settings. We establish that our method yields a consistent estimator of the prediction function with explicit rates. Additionally, we provide empirical results demonstrating the performance of RegFeaL in various experiments.
</details>
<details>
<summary>摘要</summary>
“特征学习在自动选择特征方面扮演着关键角色，特别是在高维数据的场景下，非 Parametric 方法通常难以处理。在这种研究中，我们关注到supervised 学习场景下，关键信息归于数据中的一个低维线性子空间，即多index 模型。如果这个子空间知道，它会大幅提高预测、计算和解释。为解决这个挑战，我们提出了一种新的方法 для线性特征学习和非 Parametric 预测，该方法同时估算预测函数和线性子空间。我们的方法使用empirical risk minimization，并添加了预测函数导数的罚 penalty，以确保方法的 universality。通过 Hermite 波干的特性，我们引入了我们的估计器，名为RegFeaL。我们使用 alternate minimization 来Iteratively 旋转数据，以便更好地与主要方向相互Alignment，并准确地估算实际场景中的相关维度。我们证明了我们的方法可以提供一个Consistent 的预测函数估算器，并提供了explicit 的速率。此外，我们还提供了多个实验结果，证明 RegFeaL 在各种场景中的表现。”
</details></li>
</ul>
<hr>
<h2 id="Concept-based-explainability-for-an-EEG-transformer-model"><a href="#Concept-based-explainability-for-an-EEG-transformer-model" class="headerlink" title="Concept-based explainability for an EEG transformer model"></a>Concept-based explainability for an EEG transformer model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12745">http://arxiv.org/abs/2307.12745</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/andersgmadsen/tcav-bendr">https://github.com/andersgmadsen/tcav-bendr</a></li>
<li>paper_authors: Anders Gjølbye Madsen, William Theodor Lehn-Schiøler, Áshildur Jónsdóttir, Bergdís Arnardóttir, Lars Kai Hansen</li>
<li>for: 本研究旨在应用于电энце法ogram（EEG）数据，以提高深度学习模型的可解释性。</li>
<li>methods: 本研究使用了Kim et al. (2018)所提出的概念活化向量（CAVs）方法，以理解深度模型内部的状态。这种方法首先应用于图像分类领域，后来扩展到其他领域，包括自然语言处理。</li>
<li>results: 本研究表明，使用外部标注的EEG数据和应用颌定义的概念可以帮助理解深度EEG模型的表示。两种方法均提供了有价值的信息，帮助理解深度模型学习的表示。<details>
<summary>Abstract</summary>
Deep learning models are complex due to their size, structure, and inherent randomness in training procedures. Additional complexity arises from the selection of datasets and inductive biases. Addressing these challenges for explainability, Kim et al. (2018) introduced Concept Activation Vectors (CAVs), which aim to understand deep models' internal states in terms of human-aligned concepts. These concepts correspond to directions in latent space, identified using linear discriminants. Although this method was first applied to image classification, it was later adapted to other domains, including natural language processing. In this work, we attempt to apply the method to electroencephalogram (EEG) data for explainability in Kostas et al.'s BENDR (2021), a large-scale transformer model. A crucial part of this endeavor involves defining the explanatory concepts and selecting relevant datasets to ground concepts in the latent space. Our focus is on two mechanisms for EEG concept formation: the use of externally labeled EEG datasets, and the application of anatomically defined concepts. The former approach is a straightforward generalization of methods used in image classification, while the latter is novel and specific to EEG. We present evidence that both approaches to concept formation yield valuable insights into the representations learned by deep EEG models.
</details>
<details>
<summary>摘要</summary>
深度学习模型因其大小、结构和训练过程中的随机性而变得复杂。此外，选择数据集和 inductive bias 也会增加复杂性。为了解释这些复杂性，金等人（2018）提出了概念活动 вектор (CAV)，它们目的是通过人类对应的概念来理解深度模型的内部状态。这些概念与 latent space 中的方向相对应，使用线性分解器来定义。这种方法最初应用于图像分类领域，后来被应用到其他领域，包括自然语言处理。在这个工作中，我们尝试将这种方法应用到 Kostas 等人（2021）的 BENDR 模型，以便在 electroencephalogram (EEG) 数据上提供解释。我们的关注点在于两种 EEG 概念形成机制：使用外部标注的 EEG 数据，以及基于 анатомически定义的概念。前者是一种直观的推广，而后者是特有的 EEG 概念形成方法。我们展示了这两种方法可以为深度 EEG 模型学习的表示提供有价值的启示。
</details></li>
</ul>
<hr>
<h2 id="Sparse-firing-regularization-methods-for-spiking-neural-networks-with-time-to-first-spike-coding"><a href="#Sparse-firing-regularization-methods-for-spiking-neural-networks-with-time-to-first-spike-coding" class="headerlink" title="Sparse-firing regularization methods for spiking neural networks with time-to-first spike coding"></a>Sparse-firing regularization methods for spiking neural networks with time-to-first spike coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13007">http://arxiv.org/abs/2307.13007</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yusuke Sakemi, Kakei Yamamoto, Takeo Hosomi, Kazuyuki Aihara</li>
<li>for: This paper aims to improve the energy efficiency of information processing in spiking neural networks (SNNs) by reducing the firing frequency of time-to-first spike (TTFS)-coded SNNs.</li>
<li>methods: The paper proposes two spike timing-based sparse-firing (SSR) regularization methods, namely membrane potential-aware SSR (M-SSR) and firing condition-aware SSR (F-SSR), to further reduce the firing frequency of TTFS-coded SNNs.</li>
<li>results: The paper investigates the effects of these regularization methods on the MNIST, Fashion-MNIST, and CIFAR-10 datasets using multilayer perceptron networks and convolutional neural network structures.<details>
<summary>Abstract</summary>
The training of multilayer spiking neural networks (SNNs) using the error backpropagation algorithm has made significant progress in recent years. Among the various training schemes, the error backpropagation method that directly uses the firing time of neurons has attracted considerable attention because it can realize ideal temporal coding. This method uses time-to-first spike (TTFS) coding, in which each neuron fires at most once, and this restriction on the number of firings enables information to be processed at a very low firing frequency. This low firing frequency increases the energy efficiency of information processing in SNNs, which is important not only because of its similarity with information processing in the brain, but also from an engineering point of view. However, only an upper limit has been provided for TTFS-coded SNNs, and the information-processing capability of SNNs at lower firing frequencies has not been fully investigated. In this paper, we propose two spike timing-based sparse-firing (SSR) regularization methods to further reduce the firing frequency of TTFS-coded SNNs. The first is the membrane potential-aware SSR (M-SSR) method, which has been derived as an extreme form of the loss function of the membrane potential value. The second is the firing condition-aware SSR (F-SSR) method, which is a regularization function obtained from the firing conditions. Both methods are characterized by the fact that they only require information about the firing timing and associated weights. The effects of these regularization methods were investigated on the MNIST, Fashion-MNIST, and CIFAR-10 datasets using multilayer perceptron networks and convolutional neural network structures.
</details>
<details>
<summary>摘要</summary>
“多层脉冲神经网络（SNN）的训练使用错误反射算法在最近几年内做出了重要进展。其中许多训练方案中，使用神经元发射时间的错误反射方法吸引了较大的注意力，因为它可以实现理想的时间编码。这种方法使用时刻到首次发射（TTFS）编码，每个神经元最多只能发射一次，这种限制神经元发射数量使得信息处理可以在非常低的发射频率下进行。这种低发射频率提高了SNNs中信息处理的能源效率，这对不仅与脑中信息处理的相似性，还从工程角度来说是非常重要。然而，只提供了TTFS-coded SNNs的最大限制，尚未充分研究SNNs在更低的发射频率下的信息处理能力。在这篇论文中，我们提出了两种基于发射时间的稀发（SSR）规范，以进一步减少TTFS-coded SNNs的发射频率。第一个是Membrame potential-aware SSR（M-SSR）方法，它是脑电压值损失函数的极端形式。第二个是发射条件-aware SSR（F-SSR）方法，它是基于发射条件获得的规范函数。两种方法都具有只需要知道发射时间和相关权重的特点。我们对MNIST、Fashion-MNIST和CIFAR-10数据集使用多层感知网络和卷积神经网络结构进行了实验研究。”
</details></li>
</ul>
<hr>
<h2 id="Safety-Performance-of-Neural-Networks-in-the-Presence-of-Covariate-Shift"><a href="#Safety-Performance-of-Neural-Networks-in-the-Presence-of-Covariate-Shift" class="headerlink" title="Safety Performance of Neural Networks in the Presence of Covariate Shift"></a>Safety Performance of Neural Networks in the Presence of Covariate Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12716">http://arxiv.org/abs/2307.12716</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chih-Hong Cheng, Harald Ruess, Konstantinos Theodorou</li>
<li>for: 这个论文是为了解决 neural network 在不同条件下的安全性性能问题。</li>
<li>methods: 论文提出了一种基于activation pattern的分布approximation的方法，以重新评估 neural network 的安全性性能在 covariate shift 下。</li>
<li>results: 论文提出的方法可以construct a reshaped test set that reflects the distribution of neuron activation values observed during operation, and may therefore be used for re-evaluating safety performance in the presence of covariate shift.<details>
<summary>Abstract</summary>
Covariate shift may impact the operational safety performance of neural networks. A re-evaluation of the safety performance, however, requires collecting new operational data and creating corresponding ground truth labels, which often is not possible during operation. We are therefore proposing to reshape the initial test set, as used for the safety performance evaluation prior to deployment, based on an approximation of the operational data. This approximation is obtained by observing and learning the distribution of activation patterns of neurons in the network during operation. The reshaped test set reflects the distribution of neuron activation values as observed during operation, and may therefore be used for re-evaluating safety performance in the presence of covariate shift. First, we derive conservative bounds on the values of neurons by applying finite binning and static dataflow analysis. Second, we formulate a mixed integer linear programming (MILP) constraint for constructing the minimum set of data points to be removed in the test set, such that the difference between the discretized test and operational distributions is bounded. We discuss potential benefits and limitations of this constraint-based approach based on our initial experience with an implemented research prototype.
</details>
<details>
<summary>摘要</summary>
covariate shift可能会影响神经网络的运行安全性表现。但是重新评估安全性表现时需要收集新的运行数据和创建相应的真实标签，却经常在运行时不可能完成。我们因此提议将初始测试集重新组织，基于神经网络在运行时活动pattern的 Distribution 的 aproximation。这种approximation 通过观察和学习神经网络在运行时 neuron 的活动值分布来获得。重新组织的测试集反映了神经网络在运行时 neuron 的活动值分布，可以用于重新评估安全性表现在 covariate shift 的情况下。首先，我们通过finite binning和静态数据流分析来 derive conservative bounds  на neuron 的值。其次，我们将 mixed integer linear programming (MILP) 约束 formation for constructing the minimum set of data points to be removed in the test set，以使得测试和运行分布之间的差异bounded。我们介绍了这种约束基于的approach 的可能的优点和限制，基于我们对实际研究版本的初步经验。
</details></li>
</ul>
<hr>
<h2 id="Policy-Gradient-Optimal-Correlation-Search-for-Variance-Reduction-in-Monte-Carlo-simulation-and-Maximum-Optimal-Transport"><a href="#Policy-Gradient-Optimal-Correlation-Search-for-Variance-Reduction-in-Monte-Carlo-simulation-and-Maximum-Optimal-Transport" class="headerlink" title="Policy Gradient Optimal Correlation Search for Variance Reduction in Monte Carlo simulation and Maximum Optimal Transport"></a>Policy Gradient Optimal Correlation Search for Variance Reduction in Monte Carlo simulation and Maximum Optimal Transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12703">http://arxiv.org/abs/2307.12703</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Bras, Gilles Pagès</li>
<li>for: 估计 $f(X_T)$ 的变iance减少</li>
<li>methods: 使用 $(f(X^1_T) + f(X^2_T))&#x2F;2$ 新算法，其中 $X^1$ 和 $X^2$ 具有相同的分布，但是具有相对的路径相关性，以降低变iance</li>
<li>results: 使用深度神经网络来approximate优化的相关函数 $\rho$，并使用策略梯度和奖励学习技术来calibrate $\rho$  along the trajectories of $(X^1, X^2)$<details>
<summary>Abstract</summary>
We propose a new algorithm for variance reduction when estimating $f(X_T)$ where $X$ is the solution to some stochastic differential equation and $f$ is a test function. The new estimator is $(f(X^1_T) + f(X^2_T))/2$, where $X^1$ and $X^2$ have same marginal law as $X$ but are pathwise correlated so that to reduce the variance. The optimal correlation function $\rho$ is approximated by a deep neural network and is calibrated along the trajectories of $(X^1, X^2)$ by policy gradient and reinforcement learning techniques. Finding an optimal coupling given marginal laws has links with maximum optimal transport.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的算法来减少 $f(X_T)$ 的方差估计，其中 $X$ 是一个随机 differential equation 的解，$f$ 是一个测试函数。这个新估计器是 $(f(X^1_T) + f(X^2_T))/2$, 其中 $X^1$ 和 $X^2$ 具有同一个分布，但是它们在路径上具有相关性，以降低方差。我们使用深度神经网络来近似优化函数 $\rho$，并通过策略梯度和强化学习技术来调整 $\rho$。在给定分布下找到最佳匹配有关于最大优化交通的链接。
</details></li>
</ul>
<hr>
<h2 id="MC-JEPA-A-Joint-Embedding-Predictive-Architecture-for-Self-Supervised-Learning-of-Motion-and-Content-Features"><a href="#MC-JEPA-A-Joint-Embedding-Predictive-Architecture-for-Self-Supervised-Learning-of-Motion-and-Content-Features" class="headerlink" title="MC-JEPA: A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features"></a>MC-JEPA: A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12698">http://arxiv.org/abs/2307.12698</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrien Bardes, Jean Ponce, Yann LeCun</li>
<li>for: 这篇论文主要针对的是如何通过自动学习来学习视觉表示，以便更好地捕捉视频中的物体运动和位置信息。</li>
<li>methods: 这篇论文提出了一种将自动学习和运动估算相结合的方法，即MC-JEPA，它利用共享编码器来同时学习运动估算和内容特征，从而将运动信息纳入视觉表示中。</li>
<li>results: 实验结果表明，MC-JEPA可以与现有的无监督光流估算 benchmarks相比，以及与常见的自动学习方法在图像和视频 semantic segmentation 任务上表现相当。<details>
<summary>Abstract</summary>
Self-supervised learning of visual representations has been focusing on learning content features, which do not capture object motion or location, and focus on identifying and differentiating objects in images and videos. On the other hand, optical flow estimation is a task that does not involve understanding the content of the images on which it is estimated. We unify the two approaches and introduce MC-JEPA, a joint-embedding predictive architecture and self-supervised learning approach to jointly learn optical flow and content features within a shared encoder, demonstrating that the two associated objectives; the optical flow estimation objective and the self-supervised learning objective; benefit from each other and thus learn content features that incorporate motion information. The proposed approach achieves performance on-par with existing unsupervised optical flow benchmarks, as well as with common self-supervised learning approaches on downstream tasks such as semantic segmentation of images and videos.
</details>
<details>
<summary>摘要</summary>
自顾学学习视觉表示法已经集中着注意力于学习内容特征，这些特征不包括物体运动或位置信息，而是专注于图像和视频中对象的识别和区分。然而，光流估计是一个不需要理解图像内容的任务。我们将这两种方法统一，并引入 MC-JEPA，一种共同预测 Architecture和自顾学学习方法，以共同学习光流和内容特征在共同Encoder中。我们示出了这两个相关的目标；光流估计目标和自顾学学习目标之间存在互助关系，因此学习的内容特征具有运动信息。我们的方法可以与现有的无监督光流标准做比较，以及常见的自顾学学习方法在图像和视频semantic segmentation任务中的表现。
</details></li>
</ul>
<hr>
<h2 id="Addressing-the-Impact-of-Localized-Training-Data-in-Graph-Neural-Networks"><a href="#Addressing-the-Impact-of-Localized-Training-Data-in-Graph-Neural-Networks" class="headerlink" title="Addressing the Impact of Localized Training Data in Graph Neural Networks"></a>Addressing the Impact of Localized Training Data in Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12689">http://arxiv.org/abs/2307.12689</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/akanshaaga/reg_appnp">https://github.com/akanshaaga/reg_appnp</a></li>
<li>paper_authors: Singh Akansha<br>for: 本研究的目的是评估 graphs 上 GNN 模型的性能，并研究如何在受限制的训练数据下提高模型的适应能力。methods: 本研究使用了一种常见的 GNN 模型，并提出了一种基于偏移分布的规范方法来降低模型与数据的偏移。results: 对于三个常用的 цитирование GNN 测试集，研究发现该规范方法可以有效地提高模型在不同数据下的适应能力和泛化能力。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have achieved notable success in learning from graph-structured data, owing to their ability to capture intricate dependencies and relationships between nodes. They excel in various applications, including semi-supervised node classification, link prediction, and graph generation. However, it is important to acknowledge that the majority of state-of-the-art GNN models are built upon the assumption of an in-distribution setting, which hinders their performance on real-world graphs with dynamic structures. In this article, we aim to assess the impact of training GNNs on localized subsets of the graph. Such restricted training data may lead to a model that performs well in the specific region it was trained on but fails to generalize and make accurate predictions for the entire graph. In the context of graph-based semi-supervised learning (SSL), resource constraints often lead to scenarios where the dataset is large, but only a portion of it can be labeled, affecting the model's performance. This limitation affects tasks like anomaly detection or spam detection when labeling processes are biased or influenced by human subjectivity. To tackle the challenges posed by localized training data, we approach the problem as an out-of-distribution (OOD) data issue by by aligning the distributions between the training data, which represents a small portion of labeled data, and the graph inference process that involves making predictions for the entire graph. We propose a regularization method to minimize distributional discrepancies between localized training data and graph inference, improving model performance on OOD data. Extensive tests on popular GNN models show significant performance improvement on three citation GNN benchmark datasets. The regularization approach effectively enhances model adaptation and generalization, overcoming challenges posed by OOD data.
</details>
<details>
<summary>摘要</summary>
图 neural network (GNN) 在处理图结构数据方面取得了显著成功，它可以捕捉图中节点之间的复杂关系和依赖关系。它在半监督节点分类、链接预测和图生成等应用中表现出色。然而，需要注意的是，现代GNN模型大多基于内部分布式 Setting  assumptions，这会限制它们在实际世界图中的性能。在这篇文章中，我们想要评估在本地化 subsets 上训练 GNN 的影响。这种受限的训练数据可能会导致模型在特定区域中表现良好，但是无法总结和对整个图进行准确预测。在图基于半监督学习 (SSL) 中，资源限制经常导致数据集很大，但只有一部分可以标记，这会影响模型的性能。这种限制对tasks like anomaly detection 或者垃圾邮件检测来说是一个问题，因为标注过程可能受到人类主观性的影响。为了解决本地化训练数据所带来的挑战，我们将这问题定义为异常数据 (OOD) 问题，并通过对 training data 和图推理过程之间的分布偏好进行对齐，来降低分布偏好的影响。我们提出了一种 regularization 方法，以降低分布偏好的影响，从而提高模型对 OOD 数据的性能。我们在Popular GNN 模型上进行了广泛的测试，并得到了显著的性能提高 results 在三个引用 GNN 数据集上。我们的 regularization 方法有效地提高了模型的适应和泛化性，超越了 OOD 数据的挑战。
</details></li>
</ul>
<hr>
<h2 id="An-Estimator-for-the-Sensitivity-to-Perturbations-of-Deep-Neural-Networks"><a href="#An-Estimator-for-the-Sensitivity-to-Perturbations-of-Deep-Neural-Networks" class="headerlink" title="An Estimator for the Sensitivity to Perturbations of Deep Neural Networks"></a>An Estimator for the Sensitivity to Perturbations of Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12679">http://arxiv.org/abs/2307.12679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Naman Maheshwari, Nicholas Malaya, Scott Moe, Jaydeep P. Kulkarni, Sudhanva Gurumurthi</li>
<li>for: 这个论文的目的是为了评估深度神经网络（DNNs）在安全关键应用中的稳定性，如自动驾驶车和疾病诊断。</li>
<li>methods: 这篇论文使用了一种估计器来预测DNNs对输入和模型参数的敏感性。该估计器基于不等式和矩阵范数，可以roughly对整个神经网络的condition number进行估计。</li>
<li>results: 对AlexNet和VGG-19两种卷积神经网络，使用ImageNet数据集进行测试，并通过随机干扰和攻击测试了估计器的紧eness。<details>
<summary>Abstract</summary>
For Deep Neural Networks (DNNs) to become useful in safety-critical applications, such as self-driving cars and disease diagnosis, they must be stable to perturbations in input and model parameters. Characterizing the sensitivity of a DNN to perturbations is necessary to determine minimal bit-width precision that may be used to safely represent the network. However, no general result exists that is capable of predicting the sensitivity of a given DNN to round-off error, noise, or other perturbations in input. This paper derives an estimator that can predict such quantities. The estimator is derived via inequalities and matrix norms, and the resulting quantity is roughly analogous to a condition number for the entire neural network. An approximation of the estimator is tested on two Convolutional Neural Networks, AlexNet and VGG-19, using the ImageNet dataset. For each of these networks, the tightness of the estimator is explored via random perturbations and adversarial attacks.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNNs）在安全关键应用中，如自动驾驶车和疾病诊断，必须具有稳定性。为了确定MINIMAL的位数精度可以安全地表示网络，需要量化神经网络对输入和模型参数的敏感性。然而，没有一个通用的结果可以预测给定DNN对圆满误差、噪声或其他输入参数的敏感性。这篇论文提出了一个估计器，可以预测这些量。这个估计器基于不等式和矩阵范数，其结果类似于神经网络的整体condition数。这个估计器的精度被测试在AlexNet和VGG-19Convolutional Neural Networks上，使用ImageNet dataset。对于每个网络，估计器的紧张性被探索 mediante具有随机误差和攻击性的杂音误差。
</details></li>
</ul>
<hr>
<h2 id="Global-k-Space-Interpolation-for-Dynamic-MRI-Reconstruction-using-Masked-Image-Modeling"><a href="#Global-k-Space-Interpolation-for-Dynamic-MRI-Reconstruction-using-Masked-Image-Modeling" class="headerlink" title="Global k-Space Interpolation for Dynamic MRI Reconstruction using Masked Image Modeling"></a>Global k-Space Interpolation for Dynamic MRI Reconstruction using Masked Image Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12672">http://arxiv.org/abs/2307.12672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiazhen Pan, Suprosanna Shit, Özgün Turgut, Wenqi Huang, Hongwei Bran Li, Nil Stolt-Ansó, Thomas Küstner, Kerstin Hammernik, Daniel Rueckert</li>
<li>for: This paper focuses on improving dynamic Magnetic Resonance Imaging (MRI) reconstruction by interpolating undersampled k-space data before obtaining images with Fourier transform.</li>
<li>methods: The proposed approach uses a Transformer-based k-space Global Interpolation Network (k-GIN) to learn global dependencies among low- and high-frequency components of 2D+t k-space, and a novel k-space Iterative Refinement Module (k-IRM) to enhance high-frequency components learning.</li>
<li>results: The proposed method outperforms baseline methods in terms of both quantitative and qualitative measures, and achieves higher robustness and generalizability in highly-undersampled MR data.Here’s the simplified Chinese text for the three points:</li>
<li>for: 这篇论文关注改进动态磁共振成像重建，通过 interpolating 未探测的 k-space 数据 перед使用傅ри transform 获得图像。</li>
<li>methods: 该方法使用 Transformer 基于 k-space 全球 interpolating 网络（k-GIN），学习 k-space 中低频和高频 ком成的全球依赖关系，并使用 novel k-space Iterative Refinement Module (k-IRM) 进一步提高高频 ком成的学习。</li>
<li>results: 该方法在量化和质量上都超过基线方法，并在高度不探测的 MR 数据中具有更高的Robustness 和普适性。<details>
<summary>Abstract</summary>
In dynamic Magnetic Resonance Imaging (MRI), k-space is typically undersampled due to limited scan time, resulting in aliasing artifacts in the image domain. Hence, dynamic MR reconstruction requires not only modeling spatial frequency components in the x and y directions of k-space but also considering temporal redundancy. Most previous works rely on image-domain regularizers (priors) to conduct MR reconstruction. In contrast, we focus on interpolating the undersampled k-space before obtaining images with Fourier transform. In this work, we connect masked image modeling with k-space interpolation and propose a novel Transformer-based k-space Global Interpolation Network, termed k-GIN. Our k-GIN learns global dependencies among low- and high-frequency components of 2D+t k-space and uses it to interpolate unsampled data. Further, we propose a novel k-space Iterative Refinement Module (k-IRM) to enhance the high-frequency components learning. We evaluate our approach on 92 in-house 2D+t cardiac MR subjects and compare it to MR reconstruction methods with image-domain regularizers. Experiments show that our proposed k-space interpolation method quantitatively and qualitatively outperforms baseline methods. Importantly, the proposed approach achieves substantially higher robustness and generalizability in cases of highly-undersampled MR data.
</details>
<details>
<summary>摘要</summary>
在动态磁共振成像（MRI）中，k-空间通常会被受限的扫描时间所压缩，导致图像频率域中的抽象 artifacts。因此，动态MR重建需要不仅考虑x和y方向的空间频率分布，还需要考虑时间重复性。大多数前一些工作都是通过图像领域的正则化（priors）来进行MR重建。与之不同的是，我们将集中于探空 undersampled k-space，并使用傅里叶变换来转换成图像。在这项工作中，我们将掩码图像模型与k-空间插值联系起来，并提出了一种基于Transformer的新型k-空间全局插值网络，称为k-GIN。k-GIN学习了2D+t k-空间中低频和高频组件之间的全球依赖关系，并使用其来插值未探空的数据。此外，我们还提出了一种新的k-空间迭代优化模块（k-IRM），用于进一步优化高频组件的学习。我们对92个自有2D+t心脏MR图像进行评估，并与基于图像领域的正则化方法进行比较。实验表明，我们提出的探空插值方法在量化和质量上都超过了基线方法。重要的是，我们的方法在高度受探空的MR数据情况下具有显著更高的Robustness和普适性。
</details></li>
</ul>
<hr>
<h2 id="Control-and-Monitoring-of-Artificial-Intelligence-Algorithms"><a href="#Control-and-Monitoring-of-Artificial-Intelligence-Algorithms" class="headerlink" title="Control and Monitoring of Artificial Intelligence Algorithms"></a>Control and Monitoring of Artificial Intelligence Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13705">http://arxiv.org/abs/2307.13705</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Carlos Mario Braga Ortuño, Blanza Martinez Donoso, Belén Muñiz Villanueva</li>
<li>for: 这篇论文强调在部署人工智能模型后，需要进行监管和监测数据分布的变化。</li>
<li>methods: 本论文介绍了数据漂移和概念漂移的概念，以及他们的基础分布。此外，它还提出了一些可用来评估模型性能对于可能的时间变化的指标。</li>
<li>results: 该论文的研究结果表明，在不同的时间点上，模型的性能可能会受到数据分布的变化的影响。<details>
<summary>Abstract</summary>
This paper elucidates the importance of governing an artificial intelligence model post-deployment and overseeing potential fluctuations in the distribution of present data in contrast to the training data. The concepts of data drift and concept drift are explicated, along with their respective foundational distributions. Furthermore, a range of metrics is introduced, which can be utilized to scrutinize the model's performance concerning potential temporal variations.
</details>
<details>
<summary>摘要</summary>
这篇论文强调在人工智能模型部署后进行管理和监测数据的变化。文章解释了数据漂移和概念漂移的概念，以及它们在不同的基础分布上的基础分布。此外，文章还介绍了一些可用于评估模型在可能的时间变化下的性能的指标。Here's a breakdown of the translation:* 这篇论文 (zhè běn tiǎo wén) - This paper* 强调 (qiáng zhòng) - Emphasize* 在人工智能模型部署后 (zài rénsheng zhìyì módel bùzhè hòu) - After deploying the artificial intelligence model* 进行管理 (jìnxíng guǎn lí) - To manage* 和监测数据的变化 (hé jiān cháng xìng xiàng yǐ) - And monitoring the data's changes* 数据漂移 (xùnxíng) - Data drift* 概念漂移 (guī jiàn) - Concept drift* 在不同的基础分布上 (zài bù tiān de jī chǎi fēn xiàng) - On different foundational distributions* 基础分布 (jī chǎi fēn xiàng) - Foundational distribution* 以及 (yǐ jí) - And* 可用于 (kě yòu yú) - Can be used for* 评估模型在可能的时间变化下的性能 (píng jǐ model zài kě néng de shí huan xìang yǐ) - Evaluating the model's performance under possible temporal variationsI hope this helps! Let me know if you have any further questions or if you'd like me to translate anything else.
</details></li>
</ul>
<hr>
<h2 id="TransFusion-Generating-Long-High-Fidelity-Time-Series-using-Diffusion-Models-with-Transformers"><a href="#TransFusion-Generating-Long-High-Fidelity-Time-Series-using-Diffusion-Models-with-Transformers" class="headerlink" title="TransFusion: Generating Long, High Fidelity Time Series using Diffusion Models with Transformers"></a>TransFusion: Generating Long, High Fidelity Time Series using Diffusion Models with Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12667">http://arxiv.org/abs/2307.12667</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fahim-sikder/TransFusion">https://github.com/fahim-sikder/TransFusion</a></li>
<li>paper_authors: Md Fahim Sikder, Resmi Ramachandranpillai, Fredrik Heintz</li>
<li>for: 生成高质量、长序时间序数据的需求广泛，因为它们在各种应用领域有广泛的应用。</li>
<li>methods: 我们提出了一种基于扩散和变换器的生成模型，即TransFusion，以生成高质量长序时间序数据。</li>
<li>results: 我们在384个时间序列长度上进行了扩展，并生成了高质量的 sintetic数据。此外，我们还引入了两种评估指标来评估生成的数据质量和预测特性。与之前的最佳状态比较，TransFusion在各种视觉和实际指标上表现出优异。<details>
<summary>Abstract</summary>
The generation of high-quality, long-sequenced time-series data is essential due to its wide range of applications. In the past, standalone Recurrent and Convolutional Neural Network-based Generative Adversarial Networks (GAN) were used to synthesize time-series data. However, they are inadequate for generating long sequences of time-series data due to limitations in the architecture. Furthermore, GANs are well known for their training instability and mode collapse problem. To address this, we propose TransFusion, a diffusion, and transformers-based generative model to generate high-quality long-sequence time-series data. We have stretched the sequence length to 384, and generated high-quality synthetic data. To the best of our knowledge, this is the first study that has been done with this long-sequence length. Also, we introduce two evaluation metrics to evaluate the quality of the synthetic data as well as its predictive characteristics. We evaluate TransFusion with a wide variety of visual and empirical metrics, and TransFusion outperforms the previous state-of-the-art by a significant margin.
</details>
<details>
<summary>摘要</summary>
“高质量、长序列时间序列数据的生成是非常重要，因为它具有广泛的应用领域。在过去，单独的循环和卷积神经网基于的生成反对策略（GAN）被用来合成时间序列数据。然而，它们在生成长序列时间序列数据方面存在限制，并且GAN在训练中的稳定性和模式崩溃问题很严重。为了解决这个问题，我们提出了TransFusion，一个扩散和传播器基于的生成模型，用于生成高质量的长序列时间序列数据。我们已经将序列长度延长到384，并成功生成了高质量的 sintetic 数据。根据我们所知，这是首次在这个长序列长度下进行的研究。此外，我们也引入了两个评估评价指标，用于评估生成的数据质量以及预测特征。我们将TransFusion评估使用广泛的视觉和实验指标，并表明TransFusion在前一代的State-of-the-art之上出现了明显的进步。”
</details></li>
</ul>
<hr>
<h2 id="Online-Continual-Learning-in-Keyword-Spotting-for-Low-Resource-Devices-via-Pooling-High-Order-Temporal-Statistics"><a href="#Online-Continual-Learning-in-Keyword-Spotting-for-Low-Resource-Devices-via-Pooling-High-Order-Temporal-Statistics" class="headerlink" title="Online Continual Learning in Keyword Spotting for Low-Resource Devices via Pooling High-Order Temporal Statistics"></a>Online Continual Learning in Keyword Spotting for Low-Resource Devices via Pooling High-Order Temporal Statistics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12660">http://arxiv.org/abs/2307.12660</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/umbertomichieli/tap-slda">https://github.com/umbertomichieli/tap-slda</a></li>
<li>paper_authors: Umberto Michieli, Pablo Peso Parada, Mete Ozay</li>
<li>for: 这个研究旨在提高附加到嵌入式设备上的语音识别模型的整合学习能力，让模型能够快速适应用户定义的新词语，而不会忘记之前的词语。</li>
<li>methods: 本研究使用了增量学习的方法，将具有冻结的背景的语音识别模型训练为逐一处理新的词语，并使用了高阶统计量 Computing 的 Temporal Aware Pooling（TAP）技术来建立具有优化的特征空间。</li>
<li>results: 实验结果显示，TAP-SLDA 方法在多种设置、背景和基础上比竞争者提供了11.3%的相对平均提升，在 GSC 数据集上。<details>
<summary>Abstract</summary>
Keyword Spotting (KWS) models on embedded devices should adapt fast to new user-defined words without forgetting previous ones. Embedded devices have limited storage and computational resources, thus, they cannot save samples or update large models. We consider the setup of embedded online continual learning (EOCL), where KWS models with frozen backbone are trained to incrementally recognize new words from a non-repeated stream of samples, seen one at a time. To this end, we propose Temporal Aware Pooling (TAP) which constructs an enriched feature space computing high-order moments of speech features extracted by a pre-trained backbone. Our method, TAP-SLDA, updates a Gaussian model for each class on the enriched feature space to effectively use audio representations. In experimental analyses, TAP-SLDA outperforms competitors on several setups, backbones, and baselines, bringing a relative average gain of 11.3% on the GSC dataset.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Remote-Bio-Sensing-Open-Source-Benchmark-Framework-for-Fair-Evaluation-of-rPPG"><a href="#Remote-Bio-Sensing-Open-Source-Benchmark-Framework-for-Fair-Evaluation-of-rPPG" class="headerlink" title="Remote Bio-Sensing: Open Source Benchmark Framework for Fair Evaluation of rPPG"></a>Remote Bio-Sensing: Open Source Benchmark Framework for Fair Evaluation of rPPG</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12644">http://arxiv.org/abs/2307.12644</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/remotebiosensing/rppg">https://github.com/remotebiosensing/rppg</a></li>
<li>paper_authors: Dae-Yeol Kim, Eunsu Goh, KwangKee Lee, JongEui Chae, JongHyeon Mun, Junyeong Na, Chae-bong Sohn, Do-Yup Kim</li>
<li>for: 本研究旨在提供一个 benchmarking 框架，以评估不同的 rPPG 技术在各种数据集上的性能，并提供一个可读性好、可重复性高的评估方法。</li>
<li>methods: 本研究使用了各种 rPPG 技术，包括非深度神经网络 (non-DNN) 和深度神经网络 (DNN) 方法，并使用了各种数据集进行训练、测试和验证。</li>
<li>results: 本研究希望通过提供一个可读性好、可重复性高的 benchmarking 框架，帮助研究人员和工业界人员更好地评估和比较不同的 rPPG 技术，以解决现有的挑战和获得更好的性能。<details>
<summary>Abstract</summary>
rPPG (Remote photoplethysmography) is a technology that measures and analyzes BVP (Blood Volume Pulse) by using the light absorption characteristics of hemoglobin captured through a camera. Analyzing the measured BVP can derive various physiological signals such as heart rate, stress level, and blood pressure, which can be applied to various applications such as telemedicine, remote patient monitoring, and early prediction of cardiovascular disease. rPPG is rapidly evolving and attracting great attention from both academia and industry by providing great usability and convenience as it can measure biosignals using a camera-equipped device without medical or wearable devices. Despite extensive efforts and advances in this field, serious challenges remain, including issues related to skin color, camera characteristics, ambient lighting, and other sources of noise and artifacts, which degrade accuracy performance. We argue that fair and evaluable benchmarking is urgently required to overcome these challenges and make meaningful progress from both academic and commercial perspectives. In most existing work, models are trained, tested, and validated only on limited datasets. Even worse, some studies lack available code or reproducibility, making it difficult to fairly evaluate and compare performance. Therefore, the purpose of this study is to provide a benchmarking framework to evaluate various rPPG techniques across a wide range of datasets for fair evaluation and comparison, including both conventional non-deep neural network (non-DNN) and deep neural network (DNN) methods. GitHub URL: https://github.com/remotebiosensing/rppg
</details>
<details>
<summary>摘要</summary>
远程血液气膜测量技术（rPPG）可以测量和分析血液量脉冲（BVP），通过摄像头捕捉血液中吸收光的特点。分析测量后的BVP可以获得各种生理信号，如心率、压力等，可以应用于远程医疗、患者监测等领域。rPPG在学术和产业界引起了广泛的关注，因为它提供了高度可用性和便捷性，可以通过摄像头设备测量生物信号，无需医疗设备或戴上式设备。然而，该领域仍存在严重的挑战，包括皮肤颜色、摄像头特性、 ambient 照明等因素，这些因素会导致准确性下降。我们认为，需要迅速提供公平可评估的基准框架，以超越这些挑战和实现学术和商业上的进步。现有大多数研究都是在有限的数据集上训练、测试和验证模型，甚至有些研究缺乏可用代码或重现性，使得公平评估和比较表现困难。因此，本研究的目的是提供一个公平可评估的基准框架，用于评估不同的rPPG技术，包括非深度神经网络（non-DNN）和深度神经网络（DNN）方法。GitHub URL：https://github.com/remotebiosensing/rppg
</details></li>
</ul>
<hr>
<h2 id="Fake-News-Detection-Through-Graph-based-Neural-Networks-A-Survey"><a href="#Fake-News-Detection-Through-Graph-based-Neural-Networks-A-Survey" class="headerlink" title="Fake News Detection Through Graph-based Neural Networks: A Survey"></a>Fake News Detection Through Graph-based Neural Networks: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12639">http://arxiv.org/abs/2307.12639</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuzhi Gong, Richard O. Sinnott, Jianzhong Qi, Cecile Paris</li>
<li>for: 本研究目的是对网络媒体中的假新闻检测方法进行系统性的审查，尤其是基于图structured和深度学习方法的方法。</li>
<li>methods: 该研究分类了现有的图structured方法为知识驱动方法、媒体传播基于方法和多元社交背景基于方法，根据如何构建图结构来模型新闻相关信息的流动。</li>
<li>results: 研究发现，基于图structured和深度学习方法的方法在假新闻检测中具有强大的表现，因为它们可以准确地模型社交媒体上信息的传播和扩散过程。<details>
<summary>Abstract</summary>
The popularity of online social networks has enabled rapid dissemination of information. People now can share and consume information much more rapidly than ever before. However, low-quality and/or accidentally/deliberately fake information can also spread rapidly. This can lead to considerable and negative impacts on society. Identifying, labelling and debunking online misinformation as early as possible has become an increasingly urgent problem. Many methods have been proposed to detect fake news including many deep learning and graph-based approaches. In recent years, graph-based methods have yielded strong results, as they can closely model the social context and propagation process of online news. In this paper, we present a systematic review of fake news detection studies based on graph-based and deep learning-based techniques. We classify existing graph-based methods into knowledge-driven methods, propagation-based methods, and heterogeneous social context-based methods, depending on how a graph structure is constructed to model news related information flows. We further discuss the challenges and open problems in graph-based fake news detection and identify future research directions.
</details>
<details>
<summary>摘要</summary>
“在线社交网络的广泛普及使得信息传播速度加快，人们现在可以更快地分享和消耗信息。然而，低品质和/或意外或故意伪造的信息也可以快速传播，对社会造成重大和负面的影响。识别、标识和驳斥网络诈新的问题已成为对社会的一个急迫的问题。许多方法已经被提出来检测伪新闻，包括多个深度学习和图形基于的方法。在过去的几年中，图形基于的方法已经获得了强大的成果，因为它们可以对网络新闻的社会传播过程进行精确的模拟。在这篇文章中，我们提供了一种系统性的伪新闻检测研究，涵盖基于图形和深度学习的技术。我们将现有的图形基于方法分为知识驱动方法、传播基于方法和多元社交内容基于方法，根据将新闻相关信息流建立的图形结构。我们进一步讨论了伪新闻检测中的挑战和未来研究方向。”
</details></li>
</ul>
<hr>
<h2 id="Identifying-drivers-and-mitigators-for-congestion-and-redispatch-in-the-German-electric-power-system-with-explainable-AI"><a href="#Identifying-drivers-and-mitigators-for-congestion-and-redispatch-in-the-German-electric-power-system-with-explainable-AI" class="headerlink" title="Identifying drivers and mitigators for congestion and redispatch in the German electric power system with explainable AI"></a>Identifying drivers and mitigators for congestion and redispatch in the German electric power system with explainable AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12636">http://arxiv.org/abs/2307.12636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maurizio Titz, Sebastian Pütz, Dirk Witthaut</li>
<li>for: 这篇论文旨在分析德国传输网络中的压力峰值问题，以及随之而来的负荷重新分配和对冲贸易的影响。</li>
<li>methods: 该论文采用可解释的机器学习模型来预测每小时的负荷重新分配和对冲贸易量。该模型可以揭示各因素对网络稠密度的影响，并评估这些因素的影响力。</li>
<li>results: 研究发现，风力发电是主要驱动因素，而水力发电和跨国电力贸易也扮演着重要角色。然而，太阳能发电没有减轻网络压力的效果。研究结果表明，改变市场设计可以缓解压力峰值问题。<details>
<summary>Abstract</summary>
The transition to a sustainable energy supply challenges the operation of electric power systems in manifold ways. Transmission grid loads increase as wind and solar power are often installed far away from the consumers. In extreme cases, system operators must intervene via countertrading or redispatch to ensure grid stability. In this article, we provide a data-driven analysis of congestion in the German transmission grid. We develop an explainable machine learning model to predict the volume of redispatch and countertrade on an hourly basis. The model reveals factors that drive or mitigate grid congestion and quantifies their impact. We show that, as expected, wind power generation is the main driver, but hydropower and cross-border electricity trading also play an essential role. Solar power, on the other hand, has no mitigating effect. Our results suggest that a change to the market design would alleviate congestion.
</details>
<details>
<summary>摘要</summary>
逐渐转移到可再生能源供应的过程对电力系统的运行带来多种挑战。许多风电和太阳能install在消费者之 away，增加了传输网络荷车。在极端情况下，系统运维人员需要通过对贸易或重新分配来维护网络稳定。在这篇文章中，我们提供了一个可解释的机器学习模型，用于预测每小时的重新分配和对贸易量。这个模型揭示了导致或减轻网络拥堵的因素，并衡量它们的影响。我们发现，预期的风力发电是主要的推动因素，而水力发电和跨国电力贸易也扮演着重要的角色。然而，太阳能发电没有缓解效果。我们的结果表明，修改市场设计可以缓解拥堵。
</details></li>
</ul>
<hr>
<h2 id="De-confounding-Representation-Learning-for-Counterfactual-Inference-on-Continuous-Treatment-via-Generative-Adversarial-Network"><a href="#De-confounding-Representation-Learning-for-Counterfactual-Inference-on-Continuous-Treatment-via-Generative-Adversarial-Network" class="headerlink" title="De-confounding Representation Learning for Counterfactual Inference on Continuous Treatment via Generative Adversarial Network"></a>De-confounding Representation Learning for Counterfactual Inference on Continuous Treatment via Generative Adversarial Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12625">http://arxiv.org/abs/2307.12625</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yonghe Zhao, Qiang Huang, Haolong Zeng, Yun Pen, Huiyan Sun</li>
<li>for: This paper focuses on developing a de-confounding representation learning (DRL) framework for counterfactual outcome estimation of continuous treatment variables.</li>
<li>methods: The DRL framework uses a non-parametric model to eliminate both linear and nonlinear dependence between treatment and covariates, and it trains the correlations between the de-confounded representations and the treatment variables against the correlations between the covariate representations and the treatment variables.</li>
<li>results: The DRL model outperforms state-of-the-art counterfactual inference models for continuous treatment variables in learning de-confounding representations, and it demonstrates a detailed causal relationship between red cell width distribution and mortality in a real-world medical dataset MIMIC.Here’s the simplified Chinese text:</li>
<li>for: 这篇论文关注于开发一种基于代表学习（DRL）框架，用于连续治疗变量的可靠 outcome 估计。</li>
<li>methods: DRL 框架使用非 paramedic 模型，消除治疗和 covariate 变量之间的线性和非线性相关性。它在 covariate 表示和治疗变量之间的相关性上进行了训练。</li>
<li>results: DRL 模型在连续治疗变量上的可靠 outcome 估计方面表现出色，并在实际医疗数据集 MIMIC 中显示出了详细的 causal 关系 между红细胞宽度分布和死亡率。<details>
<summary>Abstract</summary>
Counterfactual inference for continuous rather than binary treatment variables is more common in real-world causal inference tasks. While there are already some sample reweighting methods based on Marginal Structural Model for eliminating the confounding bias, they generally focus on removing the treatment's linear dependence on confounders and rely on the accuracy of the assumed parametric models, which are usually unverifiable. In this paper, we propose a de-confounding representation learning (DRL) framework for counterfactual outcome estimation of continuous treatment by generating the representations of covariates disentangled with the treatment variables. The DRL is a non-parametric model that eliminates both linear and nonlinear dependence between treatment and covariates. Specifically, we train the correlations between the de-confounded representations and the treatment variables against the correlations between the covariate representations and the treatment variables to eliminate confounding bias. Further, a counterfactual inference network is embedded into the framework to make the learned representations serve both de-confounding and trusted inference. Extensive experiments on synthetic datasets show that the DRL model performs superiorly in learning de-confounding representations and outperforms state-of-the-art counterfactual inference models for continuous treatment variables. In addition, we apply the DRL model to a real-world medical dataset MIMIC and demonstrate a detailed causal relationship between red cell width distribution and mortality.
</details>
<details>
<summary>摘要</summary>
常见的假设推理任务中，对于连续型对策变量的Counterfactual推理更加普遍。虽然现有一些基于Marginal Structural Model的样本重新排重方法，可以删除对于随机变量的随机依赖，但这些方法通常假设对于随机变量的 parametric 模型，这些模型通常是不可验证的。在这篇论文中，我们提出了一个对策变量Counterfactual推理的De-confounding representation learning（DRL）框架。DRL 是一个非Parametric 模型，可以删除对于随机变量和对策变量的 both linear 和非线性依赖。具体来说，我们在DRL框架中训练了对策变量和随机变量之间的相互 correlations 和随机变量和对策变量之间的相互 correlations，以删除随机变量的影响。此外，我们还将Counterfactual推理网络 embed 到DRL框架中，以使得学习的表示可以用于 both de-confounding 和可靠的推理。实际上，我们在 synthetic 数据上进行了广泛的实验，结果显示DRL模型在学习删除随机变量的表示方面表现出色，并且超过了现有的Counterfactual推理模型。此外，我们还应用了DRL模型到了一个真实世界医疗数据集MIMIC，并展示了红细胞幅度分布和死亡的明确 causal 关系。
</details></li>
</ul>
<hr>
<h2 id="Predicting-Ordinary-Differential-Equations-with-Transformers"><a href="#Predicting-Ordinary-Differential-Equations-with-Transformers" class="headerlink" title="Predicting Ordinary Differential Equations with Transformers"></a>Predicting Ordinary Differential Equations with Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12617">http://arxiv.org/abs/2307.12617</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sören Becker, Michal Klein, Alexander Neitz, Giambattista Parascandolo, Niki Kilbertus</li>
<li>for:  recuperate scalar ordinary differential equations (ODEs) in symbolic form from irregularly sampled and noisy observations of a single solution trajectory</li>
<li>methods: 使用 transformer-based sequence-to-sequence model</li>
<li>results: 在各种设定下，模型表现更好或与现有方法相当，并且可以快速扩展到新观察的 governing law。<details>
<summary>Abstract</summary>
We develop a transformer-based sequence-to-sequence model that recovers scalar ordinary differential equations (ODEs) in symbolic form from irregularly sampled and noisy observations of a single solution trajectory. We demonstrate in extensive empirical evaluations that our model performs better or on par with existing methods in terms of accurate recovery across various settings. Moreover, our method is efficiently scalable: after one-time pretraining on a large set of ODEs, we can infer the governing law of a new observed solution in a few forward passes of the model.
</details>
<details>
<summary>摘要</summary>
我们开发了基于变换器的序列到序列模型，可以从不规则采样和噪声捕获的解析方面恢复普通微分方程（ODEs）的符号形式。我们在广泛的实验中表明，我们的模型在不同设定下比现有方法更高精度地恢复解。此外，我们的方法可以高效扩展：只需一次预训练在大量ODEs上，我们就可以在新观察解的几个前进推进中快速恢复规则。
</details></li>
</ul>
<hr>
<h2 id="ExWarp-Extrapolation-and-Warping-based-Temporal-Supersampling-for-High-frequency-Displays"><a href="#ExWarp-Extrapolation-and-Warping-based-Temporal-Supersampling-for-High-frequency-Displays" class="headerlink" title="ExWarp: Extrapolation and Warping-based Temporal Supersampling for High-frequency Displays"></a>ExWarp: Extrapolation and Warping-based Temporal Supersampling for High-frequency Displays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12607">http://arxiv.org/abs/2307.12607</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akanksha Dixit, Yashashwee Chakrabarty, Smruti R. Sarangi</li>
<li>for: 提高高频显示器的响应速度和平滑度，使得用户在游戏和虚拟现实应用程序中享受更加流畅和无杂的视觉体验。</li>
<li>methods: 使用再归折（extrapolation）算法和深度神经网络（DNN）来预测下一帧图像，并通过RL算法选择最佳的预测方法以提高帧率并保持图像质量。</li>
<li>results: 与传统方法相比，提出的Exwarp方法可以提高帧率四倍，并且几乎不带有图像质量的下降。<details>
<summary>Abstract</summary>
High-frequency displays are gaining immense popularity because of their increasing use in video games and virtual reality applications. However, the issue is that the underlying GPUs cannot continuously generate frames at this high rate -- this results in a less smooth and responsive experience. Furthermore, if the frame rate is not synchronized with the refresh rate, the user may experience screen tearing and stuttering. Previous works propose increasing the frame rate to provide a smooth experience on modern displays by predicting new frames based on past or future frames. Interpolation and extrapolation are two widely used algorithms that predict new frames. Interpolation requires waiting for the future frame to make a prediction, which adds additional latency. On the other hand, extrapolation provides a better quality of experience because it relies solely on past frames -- it does not incur any additional latency. The simplest method to extrapolate a frame is to warp the previous frame using motion vectors; however, the warped frame may contain improperly rendered visual artifacts due to dynamic objects -- this makes it very challenging to design such a scheme. Past work has used DNNs to get good accuracy, however, these approaches are slow. This paper proposes Exwarp -- an approach based on reinforcement learning (RL) to intelligently choose between the slower DNN-based extrapolation and faster warping-based methods to increase the frame rate by 4x with an almost negligible reduction in the perceived image quality.
</details>
<details>
<summary>摘要</summary>
高频显示器在游戏和虚拟现实应用中的使用越来越普遍，但问题在于下面的GPU无法连续生成这高的帧率，导致用户体验不平滑和不响应。如果帧率与刷新率不同步，用户可能会经历屏掉和停顿。以前的工作建议提高帧率，以提供现代显示器上的平滑体验。 interpolate和extrapolate是两种广泛使用的预测新帧的算法。 interpolate需要等待未来的帧来做预测，这添加了额外的延迟。 extrapolation提供了更好的用户体验，因为它仅基于过去的帧，不增加额外的延迟。 simplest method to extrapolate a frame is to warp the previous frame using motion vectors; however, the warped frame may contain improperly rendered visual artifacts due to dynamic objects -- this makes it very challenging to design such a scheme。过去的工作使用DNN来获得好的准确性，但这些方法慢。这篇论文提出了Exwarp -- 基于强化学习（RL）的方法，智能地选择 slower DNN-based extrapolation和faster warping-based方法，以提高帧率4倍，而且几乎无法感受到图像质量的减少。
</details></li>
</ul>
<hr>
<h2 id="Concept-backpropagation-An-Explainable-AI-approach-for-visualising-learned-concepts-in-neural-network-models"><a href="#Concept-backpropagation-An-Explainable-AI-approach-for-visualising-learned-concepts-in-neural-network-models" class="headerlink" title="Concept backpropagation: An Explainable AI approach for visualising learned concepts in neural network models"></a>Concept backpropagation: An Explainable AI approach for visualising learned concepts in neural network models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12601">http://arxiv.org/abs/2307.12601</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/patrik-ha/concept-backpropagation">https://github.com/patrik-ha/concept-backpropagation</a></li>
<li>paper_authors: Patrik Hammersborg, Inga Strümke</li>
<li>for: 这个论文的目的是发展一种可解释的人工智能技术，以便更好地理解神经网络模型如何完成任务。</li>
<li>methods: 这篇论文使用了一种名为“概念探测”的方法，用于寻找神经网络模型在训练过程中internal化的概念。具体来说，这种方法使用一个已经训练好的概念探测器，将模型输入数据库中的某些特征进行干扰，以便使概念在模型中得到最大化。这种方法可以在模型的输入空间中直观地显示探测到的概念，从而了解神经网络模型如何依赖于这些概念来完成任务。</li>
<li>results: 这篇论文的结果表明，使用了这种方法可以在不同的输入模式下 visualize 神经网络模型内部的概念表示，并且可以看出这些概念在模型中的排列和相互关系。此外，这种方法还可以用于评估神经网络模型中的概念表示是否受到模型的影响，以及概念之间的相互关系是否存在。<details>
<summary>Abstract</summary>
Neural network models are widely used in a variety of domains, often as black-box solutions, since they are not directly interpretable for humans. The field of explainable artificial intelligence aims at developing explanation methods to address this challenge, and several approaches have been developed over the recent years, including methods for investigating what type of knowledge these models internalise during the training process. Among these, the method of concept detection, investigates which \emph{concepts} neural network models learn to represent in order to complete their tasks. In this work, we present an extension to the method of concept detection, named \emph{concept backpropagation}, which provides a way of analysing how the information representing a given concept is internalised in a given neural network model. In this approach, the model input is perturbed in a manner guided by a trained concept probe for the described model, such that the concept of interest is maximised. This allows for the visualisation of the detected concept directly in the input space of the model, which in turn makes it possible to see what information the model depends on for representing the described concept. We present results for this method applied to a various set of input modalities, and discuss how our proposed method can be used to visualise what information trained concept probes use, and the degree as to which the representation of the probed concept is entangled within the neural network model itself.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Optimized-data-collection-and-analysis-process-for-studying-solar-thermal-desalination-by-machine-learning"><a href="#Optimized-data-collection-and-analysis-process-for-studying-solar-thermal-desalination-by-machine-learning" class="headerlink" title="Optimized data collection and analysis process for studying solar-thermal desalination by machine learning"></a>Optimized data collection and analysis process for studying solar-thermal desalination by machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12594">http://arxiv.org/abs/2307.12594</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guilong Peng, Senshan Sun, Yangjun Qin, Zhenwei Xu, Juxin Du, Swellam W. sharshir, A. W. Kandel, A. E. Kabeel, Nuo Yang</li>
<li>for:  This paper aims to develop a modified dataset collection and analysis process for studying solar-thermal desalination using machine learning.</li>
<li>methods: The paper uses a modified dataset collection and analysis process that includes accelerating data collection and reducing the time by 83.3%. The study also employs three different algorithms, including artificial neural networks, multiple linear regressions, and random forests, to investigate the effects of dataset features on prediction accuracy, factor importance ranking, and model generalization ability.</li>
<li>results: The study finds that a larger dataset can significantly improve prediction accuracy when using artificial neural networks and random forests. Additionally, the study highlights the significant impact of dataset size and range on ranking the importance of influence factors, and reveals that the extrapolation data range significantly affects the extrapolation accuracy of artificial neural networks.<details>
<summary>Abstract</summary>
An effective interdisciplinary study between machine learning and solar-thermal desalination requires a sufficiently large and well-analyzed experimental datasets. This study develops a modified dataset collection and analysis process for studying solar-thermal desalination by machine learning. Based on the optimized water condensation and collection process, the proposed experimental method collects over one thousand datasets, which is ten times more than the average number of datasets in previous works, by accelerating data collection and reducing the time by 83.3%. On the other hand, the effects of dataset features are investigated by using three different algorithms, including artificial neural networks, multiple linear regressions, and random forests. The investigation focuses on the effects of dataset size and range on prediction accuracy, factor importance ranking, and the model's generalization ability. The results demonstrate that a larger dataset can significantly improve prediction accuracy when using artificial neural networks and random forests. Additionally, the study highlights the significant impact of dataset size and range on ranking the importance of influence factors. Furthermore, the study reveals that the extrapolation data range significantly affects the extrapolation accuracy of artificial neural networks. Based on the results, massive dataset collection and analysis of dataset feature effects are important steps in an effective and consistent machine learning process flow for solar-thermal desalination, which can promote machine learning as a more general tool in the field of solar-thermal desalination.
</details>
<details>
<summary>摘要</summary>
需要一个具有足够规模和优化分析的多学科研究，以使机器学习技术在太阳聚变洗涤中得到应用。本研究提出了一种修改后的数据采集和分析过程，以便通过机器学习技术研究太阳聚变洗涤。基于优化的水蒸发和收集过程，提出的实验方法收集了超过一千个数据集，比前一个研究的均值数据集多出了10倍，并且通过加速数据采集和减少时间83.3%。同时，研究对数据集特征的影响进行了三种算法的调查，包括人工神经网络、多linear回归和随机森林。调查关注数据集大小和范围对预测精度、因素重要排名和模型泛化能力的影响。结果显示，大数据集可以在人工神经网络和随机森林中显著提高预测精度。此外，研究还发现数据集大小和范围对因素重要排名产生了深见影响。此外，研究还发现人工神经网络抽象数据范围对抽象预测精度产生了显著影响。根据结果，大规模的数据采集和分析数据集特征的影响是多学科研究中efficient和可靠的机器学习过程流程的重要步骤。这可以推动机器学习在太阳聚变洗涤领域的应用。
</details></li>
</ul>
<hr>
<h2 id="InVAErt-networks-a-data-driven-framework-for-emulation-inference-and-identifiability-analysis"><a href="#InVAErt-networks-a-data-driven-framework-for-emulation-inference-and-identifiability-analysis" class="headerlink" title="InVAErt networks: a data-driven framework for emulation, inference and identifiability analysis"></a>InVAErt networks: a data-driven framework for emulation, inference and identifiability analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12586">http://arxiv.org/abs/2307.12586</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guoxiang Grayson Tong, Carlos A. Sing Long, Daniele E. Schiavazzi</li>
<li>for: 这个论文主要是为了提出一种数据驱动的分析和synthesis方法来处理物理系统。</li>
<li>methods: 这个方法使用了deterministic encoder和decoder来表示前向和反向解决Map，以及normalizing flow来捕捉系统输出的概率分布。还有一个variational encoder来学习一个具有缺失射影的简洁表示。</li>
<li>results: 作者通过大量的数字例子进行了广泛的验证，包括简单的线性、非线性和周期地图，动力系统以及空间-时间PDEs。<details>
<summary>Abstract</summary>
Use of generative models and deep learning for physics-based systems is currently dominated by the task of emulation. However, the remarkable flexibility offered by data-driven architectures would suggest to extend this representation to other aspects of system synthesis including model inversion and identifiability. We introduce inVAErt (pronounced \emph{invert}) networks, a comprehensive framework for data-driven analysis and synthesis of parametric physical systems which uses a deterministic encoder and decoder to represent the forward and inverse solution maps, normalizing flow to capture the probabilistic distribution of system outputs, and a variational encoder designed to learn a compact latent representation for the lack of bijectivity between inputs and outputs. We formally investigate the selection of penalty coefficients in the loss function and strategies for latent space sampling, since we find that these significantly affect both training and testing performance. We validate our framework through extensive numerical examples, including simple linear, nonlinear, and periodic maps, dynamical systems, and spatio-temporal PDEs.
</details>
<details>
<summary>摘要</summary>
使用生成模型和深度学习来处理物理系统的应用现在主要是模拟。然而，数据驱动建筑的灵活性表示可以扩展到其他系统设计方面，包括模型反转和可识别性。我们介绍inVAErt（缩写为“反转”）网络，一个涵盖数据驱动分析和设计 Parametric Physical Systems 的完整框架，使用权重矩阵来表示前向和反向解决Map，使用正则流来捕捉系统输出的概率分布，并使用变量编码器来学习具有缺失射影的短表示。我们正式调查损失函数中的罚 coefficient选择和秘钥空间采样策略，因为这些对训练和测试性能产生了重要影响。我们通过广泛的数字例子进行验证，包括简单的线性、非线性和周期地图，动力系统和空间时间PDEs。
</details></li>
</ul>
<hr>
<h2 id="Self-refining-of-Pseudo-Labels-for-Music-Source-Separation-with-Noisy-Labeled-Data"><a href="#Self-refining-of-Pseudo-Labels-for-Music-Source-Separation-with-Noisy-Labeled-Data" class="headerlink" title="Self-refining of Pseudo Labels for Music Source Separation with Noisy Labeled Data"></a>Self-refining of Pseudo Labels for Music Source Separation with Noisy Labeled Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12576">http://arxiv.org/abs/2307.12576</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junghyun Koo, Yunkee Chae, Chang-Bin Jeon, Kyogu Lee</li>
<li>for: 提高音乐源分离（MSS）性能，增加大数据集来改善MSS模型的训练</li>
<li>methods: 自动修正受损标签的数据集，使用含噪标签数据集训练MSS模型</li>
<li>results: 使用修正后的受损数据集训练MSS模型，与使用干净标签数据集训练MSS模型相当，甚至在仅使用受损数据集情况下，MSS模型表现更佳。<details>
<summary>Abstract</summary>
Music source separation (MSS) faces challenges due to the limited availability of correctly-labeled individual instrument tracks. With the push to acquire larger datasets to improve MSS performance, the inevitability of encountering mislabeled individual instrument tracks becomes a significant challenge to address. This paper introduces an automated technique for refining the labels in a partially mislabeled dataset. Our proposed self-refining technique, employed with a noisy-labeled dataset, results in only a 1% accuracy degradation in multi-label instrument recognition compared to a classifier trained on a clean-labeled dataset. The study demonstrates the importance of refining noisy-labeled data in MSS model training and shows that utilizing the refined dataset leads to comparable results derived from a clean-labeled dataset. Notably, upon only access to a noisy dataset, MSS models trained on a self-refined dataset even outperform those trained on a dataset refined with a classifier trained on clean labels.
</details>
<details>
<summary>摘要</summary>
音乐源分离（MSS）面临受限于有限 Correctly-labeled 个人乐器轨迹的可用性。随着提高 MSS 性能的推动，遇到含有错误标签的个人乐器轨迹变得具有重要性。这篇文章介绍了一种自动化的标签修正技术，用于改善含有错误标签的 dataset。我们的提议的自我修复技术，与含有噪音标签的 dataset 结合使用，对多标签乐器识别器的准确率产生了只有1% 的下降。这个研究表明了 MSS 模型训练时需要对含有噪音标签的数据进行修正，并且表明使用修正后的 dataset 可以达到与使用干净标签的 dataset 相同的结果。进一步地，只有在有限的噪音标签 dataset 上，MSS 模型训练在自我修复 dataset 上表现出了与使用干净标签 dataset 上的表现相当。
</details></li>
</ul>
<hr>
<h2 id="Towards-Generalising-Neural-Topical-Representations"><a href="#Towards-Generalising-Neural-Topical-Representations" class="headerlink" title="Towards Generalising Neural Topical Representations"></a>Towards Generalising Neural Topical Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12564">http://arxiv.org/abs/2307.12564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaohao Yang, He Zhao, Dinh Phung, Lan Du</li>
<li>for: 提高神经话题模型（NTM）的普适性和任务泛化能力。</li>
<li>methods: 使用数据扩充和 hierarchical topic transport distance（HOTT）计算优化交通距离来模型相似文档，从而提高NTM的普适性和任务泛化能力。</li>
<li>results: 对多个corpus和任务进行了广泛的实验，证明了我们的框架可以帮助NTMs在不同的corpus和任务中提高神经话题表示性的普适性和任务泛化能力。<details>
<summary>Abstract</summary>
Topic models have evolved from conventional Bayesian probabilistic models to Neural Topic Models (NTMs) over the last two decays. Although NTMs have achieved promising performance when trained and tested on a specific corpus, their generalisation ability across corpora is rarely studied. In practice, we often expect that an NTM trained on a source corpus can still produce quality topical representation for documents in a different target corpus without retraining. In this work, we aim to improve NTMs further so that their benefits generalise reliably across corpora and tasks. To do so, we propose to model similar documents by minimising their semantical distance when training NTMs. Specifically, similar documents are created by data augmentation during training; The semantical distance between documents is measured by the Hierarchical Topic Transport Distance (HOTT), which computes the Optimal Transport (OT) distance between the topical representations. Our framework can be readily applied to most NTMs as a plug-and-play module. Extensive experiments show that our framework significantly improves the generalisation ability regarding neural topical representation across corpora.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DeepGATGO-A-Hierarchical-Pretraining-Based-Graph-Attention-Model-for-Automatic-Protein-Function-Prediction"><a href="#DeepGATGO-A-Hierarchical-Pretraining-Based-Graph-Attention-Model-for-Automatic-Protein-Function-Prediction" class="headerlink" title="DeepGATGO: A Hierarchical Pretraining-Based Graph-Attention Model for Automatic Protein Function Prediction"></a>DeepGATGO: A Hierarchical Pretraining-Based Graph-Attention Model for Automatic Protein Function Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13004">http://arxiv.org/abs/2307.13004</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Li, Changkun Jiang, Jianqiang Li</li>
<li>for: 这 paper 的目的是提出一种基于 protein sequence 的自动蛋白功能预测方法，以解决当前依赖劳动力密集的湿法方法的问题。</li>
<li>methods: 这 paper 使用的方法包括使用 protein sequence 和 Gene Ontology (GO)  термина来生成最终的功能预测结果。具体来说， protein sequence 、结构信息和 protein-protein 交互网络被 integrate 为 priori 知识，并与 GO term 的嵌入 fusion 生成最终的预测结果。</li>
<li>results:  эксперименталь结果表明，我们提出的模型可以更好地扩展 GO TERM 批量分析中的蛋白功能预测。<details>
<summary>Abstract</summary>
Automatic protein function prediction (AFP) is classified as a large-scale multi-label classification problem aimed at automating protein enrichment analysis to eliminate the current reliance on labor-intensive wet-lab methods. Currently, popular methods primarily combine protein-related information and Gene Ontology (GO) terms to generate final functional predictions. For example, protein sequences, structural information, and protein-protein interaction networks are integrated as prior knowledge to fuse with GO term embeddings and generate the ultimate prediction results. However, these methods are limited by the difficulty in obtaining structural information or network topology information, as well as the accuracy of such data. Therefore, more and more methods that only use protein sequences for protein function prediction have been proposed, which is a more reliable and computationally cheaper approach. However, the existing methods fail to fully extract feature information from protein sequences or label data because they do not adequately consider the intrinsic characteristics of the data itself. Therefore, we propose a sequence-based hierarchical prediction method, DeepGATGO, which processes protein sequences and GO term labels hierarchically, and utilizes graph attention networks (GATs) and contrastive learning for protein function prediction. Specifically, we compute embeddings of the sequence and label data using pre-trained models to reduce computational costs and improve the embedding accuracy. Then, we use GATs to dynamically extract the structural information of non-Euclidean data, and learn general features of the label dataset with contrastive learning by constructing positive and negative example samples. Experimental results demonstrate that our proposed model exhibits better scalability in GO term enrichment analysis on large-scale datasets.
</details>
<details>
<summary>摘要</summary>
自动蛋白功能预测（AFP）被分类为大规模多标签分类问题，旨在自动化蛋白聚集分析，以消除现有的人工劳动密集方法。现有的方法主要将蛋白质相关信息和生物学功能 ontology（GO）概念结合，生成最终功能预测结果。例如，蛋白质序列、结构信息和蛋白质-蛋白质交互网络被 integrate 为先知信息，并与 GO 概念嵌入结合，生成最终预测结果。然而，这些方法受到结构信息或网络拓扑信息困难获取，以及这些数据的准确性限制。因此，更多的方法仅使用蛋白质序列进行蛋白质功能预测，这是一种更可靠和计算成本更低的方法。然而，现有方法未能充分提取蛋白质序列或标签数据中的特征信息，因为它们未能充分考虑数据本身的内在特征。因此，我们提出了一种序列基于的层次预测方法，深度GATGO，它处理蛋白质序列和 GO 标签 Label  hierarchically，并使用图注意网络（GATs）和对比学习来进行蛋白质功能预测。具体来说，我们使用预训练模型计算序列和标签数据的嵌入，以降低计算成本并提高嵌入精度。然后，我们使用 GATs 动态提取非euclid 数据的结构信息，并使用对比学习学习标签数据的通用特征。实验结果表明，我们提出的模型在 GO 概念增加分析中实现了更好的扩展性。
</details></li>
</ul>
<hr>
<h2 id="Homophily-Driven-Sanitation-View-for-Robust-Graph-Contrastive-Learning"><a href="#Homophily-Driven-Sanitation-View-for-Robust-Graph-Contrastive-Learning" class="headerlink" title="Homophily-Driven Sanitation View for Robust Graph Contrastive Learning"></a>Homophily-Driven Sanitation View for Robust Graph Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12555">http://arxiv.org/abs/2307.12555</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/htytewx/softcam">https://github.com/htytewx/softcam</a></li>
<li>paper_authors: Yulin Zhu, Xing Ai, Yevgeniy Vorobeychik, Kai Zhou</li>
<li>for: 本文 investigate GCL 对于结构性攻击的鲁棒性。</li>
<li>methods: 本文提出了一种基于 homophily 的净化视图，可以与对比学习一起学习。在实现这种方法时，需要解决非对数 differentiable 问题。为此，本文提出了一系列技术来实现梯度下降的结构 GCL。</li>
<li>results: 对于两种 state of the art 结构性攻击，GCHS  consistently 超过所有基eline。在生成节点嵌入质量和两个重要下游任务中，GCHS 表现更好。<details>
<summary>Abstract</summary>
We investigate adversarial robustness of unsupervised Graph Contrastive Learning (GCL) against structural attacks. First, we provide a comprehensive empirical and theoretical analysis of existing attacks, revealing how and why they downgrade the performance of GCL. Inspired by our analytic results, we present a robust GCL framework that integrates a homophily-driven sanitation view, which can be learned jointly with contrastive learning. A key challenge this poses, however, is the non-differentiable nature of the sanitation objective. To address this challenge, we propose a series of techniques to enable gradient-based end-to-end robust GCL. Moreover, we develop a fully unsupervised hyperparameter tuning method which, unlike prior approaches, does not require knowledge of node labels. We conduct extensive experiments to evaluate the performance of our proposed model, GCHS (Graph Contrastive Learning with Homophily-driven Sanitation View), against two state of the art structural attacks on GCL. Our results demonstrate that GCHS consistently outperforms all state of the art baselines in terms of the quality of generated node embeddings as well as performance on two important downstream tasks.
</details>
<details>
<summary>摘要</summary>
我们研究对不监督图像对冲学（GCL）的抗性攻击。首先，我们提供了对现有攻击的广泛的实验和理论分析，揭示了这些攻击如何下降GCL的性能。 drawing inspiration from our analytical results, we propose a robust GCL framework that integrates a homophily-driven sanitation view, which can be learned jointly with contrastive learning. However, the non-differentiable nature of the sanitation objective poses a challenge. To address this challenge, we propose a series of techniques to enable gradient-based end-to-end robust GCL. Furthermore, we develop a fully unsupervised hyperparameter tuning method that does not require knowledge of node labels. We conduct extensive experiments to evaluate the performance of our proposed model, GCHS (Graph Contrastive Learning with Homophily-driven Sanitation View), against two state-of-the-art structural attacks on GCL. Our results demonstrate that GCHS consistently outperforms all state-of-the-art baselines in terms of the quality of generated node embeddings as well as performance on two important downstream tasks.
</details></li>
</ul>
<hr>
<h2 id="Continuation-Path-Learning-for-Homotopy-Optimization"><a href="#Continuation-Path-Learning-for-Homotopy-Optimization" class="headerlink" title="Continuation Path Learning for Homotopy Optimization"></a>Continuation Path Learning for Homotopy Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12551">http://arxiv.org/abs/2307.12551</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xi-l/cpl">https://github.com/xi-l/cpl</a></li>
<li>paper_authors: Xi Lin, Zhiyuan Yang, Xiaoyuan Zhang, Qingfu Zhang</li>
<li>for: 提高homotopy优化的性能和提供更多有用信息 для决策</li>
<li>methods: 基于模型的方法，同时优化原问题和所有伪函数问题，并在实时生成任意中间解</li>
<li>results: 实验结果表明，该方法可以大幅提高homotopy优化的性能，并提供更多有用信息支持更好的决策<details>
<summary>Abstract</summary>
Homotopy optimization is a traditional method to deal with a complicated optimization problem by solving a sequence of easy-to-hard surrogate subproblems. However, this method can be very sensitive to the continuation schedule design and might lead to a suboptimal solution to the original problem. In addition, the intermediate solutions, often ignored by classic homotopy optimization, could be useful for many real-world applications. In this work, we propose a novel model-based approach to learn the whole continuation path for homotopy optimization, which contains infinite intermediate solutions for any surrogate subproblems. Rather than the classic unidirectional easy-to-hard optimization, our method can simultaneously optimize the original problem and all surrogate subproblems in a collaborative manner. The proposed model also supports real-time generation of any intermediate solution, which could be desirable for many applications. Experimental studies on different problems show that our proposed method can significantly improve the performance of homotopy optimization and provide extra helpful information to support better decision-making.
</details>
<details>
<summary>摘要</summary>
“几何优化”是一种传统的优化方法，通过解决一系列易于Difficulty Surrogate subproblems来处理复杂的优化问题。然而，这种方法可能对继续计划设计敏感，导致优化问题的解不够优化。此外，经典几何优化中的中间解，常常被忽略，但这些中间解在许多实际应用中可能很有用。在这种工作中，我们提出了一种基于模型的方法，用于学习整个继续路径，这个路径包含任意surrogate subproblems的无限中间解。而不是 классиic的一次易到Difficulty的优化，我们的方法可以同时优化原始问题和所有surrogate subproblems，以一种协同的方式进行优化。提出的模型还支持实时生成任意中间解，这可能对许多应用有用。实验研究表明，我们的提议方法可以大幅提高几何优化的性能，并提供更多有用的信息，以支持更好的决策。
</details></li>
</ul>
<hr>
<h2 id="On-the-Connection-between-Pre-training-Data-Diversity-and-Fine-tuning-Robustness"><a href="#On-the-Connection-between-Pre-training-Data-Diversity-and-Fine-tuning-Robustness" class="headerlink" title="On the Connection between Pre-training Data Diversity and Fine-tuning Robustness"></a>On the Connection between Pre-training Data Diversity and Fine-tuning Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12532">http://arxiv.org/abs/2307.12532</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vivek Ramanujan, Thao Nguyen, Sewoong Oh, Ludwig Schmidt, Ali Farhadi</li>
<li>for: 了解预训练策略对下游模型的泛化性质的影响。</li>
<li>methods: 研究预训练分布中属性的影响，包括标签空间、标签 semantics、图像多样性、数据领域和数据量。</li>
<li>results: 发现预训练分布中数据量的影响是下游模型的有效Robustness的主要因素，其他因素对下游模型的Robustness有限制的影响。<details>
<summary>Abstract</summary>
Pre-training has been widely adopted in deep learning to improve model performance, especially when the training data for a target task is limited. In our work, we seek to understand the implications of this training strategy on the generalization properties of downstream models. More specifically, we ask the following question: how do properties of the pre-training distribution affect the robustness of a fine-tuned model? The properties we explore include the label space, label semantics, image diversity, data domains, and data quantity of the pre-training distribution. We find that the primary factor influencing downstream effective robustness (Taori et al., 2020) is data quantity, while other factors have limited significance. For example, reducing the number of ImageNet pre-training classes by 4x while increasing the number of images per class by 4x (that is, keeping total data quantity fixed) does not impact the robustness of fine-tuned models. We demonstrate our findings on pre-training distributions drawn from various natural and synthetic data sources, primarily using the iWildCam-WILDS distribution shift as a test for downstream robustness.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese)预训练在深度学习中广泛应用，特别是当目标任务的训练数据匮乏时。在我们的工作中，我们想要理解预训练方法的训练策略对下游模型的泛化性能产生的影响。更 Specifically，我们问的问题是：预训练分布中的特性如何影响下游模型的可靠性？我们考虑的特性包括预训练分布中的标签空间、标签 semantics、图像多样性、数据领域和数据量。我们发现，预训练分布中数据量的影响是下游模型的可靠性的主要因素，而其他因素具有有限的意义。例如，将 ImageNet 预训练分布中的类别数量减少到 4 倍，同时将每个类别的图像数量增加到 4 倍（即保持总数据量不变），对下游模型的可靠性没有影响。我们在不同的自然和 sintetic 数据源中测试了我们的发现，主要使用 iWildCam-WILDS 分布shift作为下游模型的可靠性测试。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Medical-Report-Generation-Disease-Revealing-Enhancement-with-Knowledge-Graph"><a href="#Rethinking-Medical-Report-Generation-Disease-Revealing-Enhancement-with-Knowledge-Graph" class="headerlink" title="Rethinking Medical Report Generation: Disease Revealing Enhancement with Knowledge Graph"></a>Rethinking Medical Report Generation: Disease Revealing Enhancement with Knowledge Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12526">http://arxiv.org/abs/2307.12526</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangyixinxin/mrg-kg">https://github.com/wangyixinxin/mrg-kg</a></li>
<li>paper_authors: Yixin Wang, Zihao Lin, Haoyu Dong<br>for:这个研究的目的是提高医疗报告生成（MRG）过程中的 Knowledge Graph（KG）的完整性和应用。methods:这个研究使用了一个完整的 KG，包括137种疾病和异常性，并对现有的 MRG 数据集进行了分析，发现现有的数据集存在长尾问题。为了解决这个问题，该研究提出了一种新的增强策略，并设计了一个两阶段的 MRG 方法。results:该研究的结果表明，提出的两阶段生成方法和增强策略可以大幅提高了生成的报告的准确性和多样性。特别是，该研究提出了一种新的评价指标，即多样性敏感度（DS），可以衡量生成的疾病是否匹配实际的ground truth，并且可以衡量所有生成的疾病的多样性。<details>
<summary>Abstract</summary>
Knowledge Graph (KG) plays a crucial role in Medical Report Generation (MRG) because it reveals the relations among diseases and thus can be utilized to guide the generation process. However, constructing a comprehensive KG is labor-intensive and its applications on the MRG process are under-explored. In this study, we establish a complete KG on chest X-ray imaging that includes 137 types of diseases and abnormalities. Based on this KG, we find that the current MRG data sets exhibit a long-tailed problem in disease distribution. To mitigate this problem, we introduce a novel augmentation strategy that enhances the representation of disease types in the tail-end of the distribution. We further design a two-stage MRG approach, where a classifier is first trained to detect whether the input images exhibit any abnormalities. The classified images are then independently fed into two transformer-based generators, namely, ``disease-specific generator" and ``disease-free generator" to generate the corresponding reports. To enhance the clinical evaluation of whether the generated reports correctly describe the diseases appearing in the input image, we propose diverse sensitivity (DS), a new metric that checks whether generated diseases match ground truth and measures the diversity of all generated diseases. Results show that the proposed two-stage generation framework and augmentation strategies improve DS by a considerable margin, indicating a notable reduction in the long-tailed problem associated with under-represented diseases.
</details>
<details>
<summary>摘要</summary>
医学报告生成（MRG）中知识图（KG）发挥关键作用，因为它揭示疾病之间的关系，可以用于导向生成过程。然而，建立完整的KG是劳动密集的，其在MRG过程中的应用还未得到充分探讨。在这种研究中，我们建立了包括137种疾病和异常的完整KG，基于这个KG，我们发现现有MRG数据集具有长尾问题，即疾病分布不均。为了解决这个问题，我们提出了一种新的扩充策略，该策略可以增强疾病类型在分布尾部的表达。此外，我们设计了两stage的MRG方法，其中首先训练一个分类器来检测输入图像是否具有任何异常。分类结果是否为true，则独立将输入图像 feed into两个基于变换器的生成器，即“疾病特定生成器”和“疾病无异常生成器”，以生成相应的报告。为了提高生成报告的临床评估，我们提出了多样性敏感度（DS），一种新的指标，该指标检查生成疾病与基准数据是否匹配，并且度量所有生成疾病的多样性。结果表明我们的两stage生成框架和扩充策略可以提高DS的较大幅度，这表明我们成功地减少了基于输入图像中的疾病出现的长尾问题。
</details></li>
</ul>
<hr>
<h2 id="Landslide-Surface-Displacement-Prediction-Based-on-VSXC-LSTM-Algorithm"><a href="#Landslide-Surface-Displacement-Prediction-Based-on-VSXC-LSTM-Algorithm" class="headerlink" title="Landslide Surface Displacement Prediction Based on VSXC-LSTM Algorithm"></a>Landslide Surface Displacement Prediction Based on VSXC-LSTM Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12524">http://arxiv.org/abs/2307.12524</a></li>
<li>repo_url: None</li>
<li>paper_authors: Menglin Kong, Ruichen Li, Fan Liu, Xingquan Li, Juan Cheng, Muzhou Hou, Cong Cao</li>
<li>for: 这 paper 是研究 recent 的滑坡自然灾害，提出一种基于变分幂分析的时间序列预测框架 VMD-SegSigmoid-XGBoost-ClusterLSTM (VSXC-LSTM)，可以更准确地预测滑坡表面移动。</li>
<li>methods: 该 paper 使用了变分幂分析、SegSigmoid 函数、XGBoost 算法和嵌入LSTM神经网络，对实际的一向表面变化数据进行模型化和预测。</li>
<li>results: 对于测试集，模型表现良好，除了随机项子序列外，RMSE 和 MAPE 值均小于 0.1，其中 periodic item 预测模块基于 XGBoost 的 RMSE 为 0.006。<details>
<summary>Abstract</summary>
Landslide is a natural disaster that can easily threaten local ecology, people's lives and property. In this paper, we conduct modelling research on real unidirectional surface displacement data of recent landslides in the research area and propose a time series prediction framework named VMD-SegSigmoid-XGBoost-ClusterLSTM (VSXC-LSTM) based on variational mode decomposition, which can predict the landslide surface displacement more accurately. The model performs well on the test set. Except for the random item subsequence that is hard to fit, the root mean square error (RMSE) and the mean absolute percentage error (MAPE) of the trend item subsequence and the periodic item subsequence are both less than 0.1, and the RMSE is as low as 0.006 for the periodic item prediction module based on XGBoost\footnote{Accepted in ICANN2023}.
</details>
<details>
<summary>摘要</summary>
地陷是一种自然灾害，可以轻易威胁当地生态环境、人们的生命和财产。在这篇论文中，我们使用实际数据进行模拟研究，并提出一种基于变分模式分解的时间序列预测框架 named VMD-SegSigmoid-XGBoost-ClusterLSTM (VSXC-LSTM)，以更准确地预测山崩表面变位。模型在测试集上表现良好，除了随机项子序列外，RMSE和MAPE值均小于0.1，而 periodic item prediction module based on XGBoost的RMSE值甚至为0.006。Note: The translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Lost-In-Translation-Generating-Adversarial-Examples-Robust-to-Round-Trip-Translation"><a href="#Lost-In-Translation-Generating-Adversarial-Examples-Robust-to-Round-Trip-Translation" class="headerlink" title="Lost In Translation: Generating Adversarial Examples Robust to Round-Trip Translation"></a>Lost In Translation: Generating Adversarial Examples Robust to Round-Trip Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12520">http://arxiv.org/abs/2307.12520</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neelbhandari6/nmt_text_attack">https://github.com/neelbhandari6/nmt_text_attack</a></li>
<li>paper_authors: Neel Bhandari, Pin-Yu Chen</li>
<li>for: 研究表单文本攻击的稳定性，特别是对于round-trip translation。</li>
<li>methods: 使用6种state-of-the-art文本基于攻击，并对其进行了round-trip translation。</li>
<li>results: 发现6种文本攻击不具有稳定性 послеround-trip translation，并提出了一种基于机器翻译的解决方案，以提高攻击的稳定性。<details>
<summary>Abstract</summary>
Language Models today provide a high accuracy across a large number of downstream tasks. However, they remain susceptible to adversarial attacks, particularly against those where the adversarial examples maintain considerable similarity to the original text. Given the multilingual nature of text, the effectiveness of adversarial examples across translations and how machine translations can improve the robustness of adversarial examples remain largely unexplored. In this paper, we present a comprehensive study on the robustness of current text adversarial attacks to round-trip translation. We demonstrate that 6 state-of-the-art text-based adversarial attacks do not maintain their efficacy after round-trip translation. Furthermore, we introduce an intervention-based solution to this problem, by integrating Machine Translation into the process of adversarial example generation and demonstrating increased robustness to round-trip translation. Our results indicate that finding adversarial examples robust to translation can help identify the insufficiency of language models that is common across languages, and motivate further research into multilingual adversarial attacks.
</details>
<details>
<summary>摘要</summary>
现代语言模型在许多下游任务上具有高精度，但它们仍然容易受到恶意攻击，特别是对于那些保持原文相似性的恶意示例。由于文本的多语言性，文本攻击的效iveness在翻译过程中和机器翻译如何提高攻击示例的Robustness remain largely unexplored。在这篇论文中，我们进行了现有文本攻击的综合研究，证明了6种state-of-the-art文本基于攻击不同语言翻译后仍然有效。此外，我们还提出了一种 intervención-based解决方案，通过将机器翻译 integrate into攻击示例生成过程，并证明了 Round-trip translation 中的Robustness提高。我们的结果表明，找到可以在翻译过程中保持效力的攻击示例可以帮助发现语言模型的共同不足，并促进多语言攻击的研究。
</details></li>
</ul>
<hr>
<h2 id="DEPHN-Different-Expression-Parallel-Heterogeneous-Network-using-virtual-gradient-optimization-for-Multi-task-Learning"><a href="#DEPHN-Different-Expression-Parallel-Heterogeneous-Network-using-virtual-gradient-optimization-for-Multi-task-Learning" class="headerlink" title="DEPHN: Different Expression Parallel Heterogeneous Network using virtual gradient optimization for Multi-task Learning"></a>DEPHN: Different Expression Parallel Heterogeneous Network using virtual gradient optimization for Multi-task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12519">http://arxiv.org/abs/2307.12519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Menglin Kong, Ri Su, Shaojie Zhao, Muzhou Hou</li>
<li>for: 这 paper 的目的是提出一种基于多任务学习（MTL）的推荐系统算法，以便互联网运营商更好地理解用户和预测用户在多种行为场景中的行为。</li>
<li>methods: 这 paper 使用了一种名为 Different Expression Parallel Heterogeneous Network（DEPHN）来模型多个任务同时。DEPHN 通过不同的特征交互方法来提高共享信息流的通用能力。在训练过程中，DEPHN 使用特征显式映射和虚拟Gradient Coefficient来实现专家闭合和任务 corr relation的处理。</li>
<li>results: EXTENSIVE experiments 表明，我们提出的方法可以在复杂的实际场景中捕捉任务 corr relation，并在基eline 模型上达到更好的性能。<details>
<summary>Abstract</summary>
Recommendation system algorithm based on multi-task learning (MTL) is the major method for Internet operators to understand users and predict their behaviors in the multi-behavior scenario of platform. Task correlation is an important consideration of MTL goals, traditional models use shared-bottom models and gating experts to realize shared representation learning and information differentiation. However, The relationship between real-world tasks is often more complex than existing methods do not handle properly sharing information. In this paper, we propose an Different Expression Parallel Heterogeneous Network (DEPHN) to model multiple tasks simultaneously. DEPHN constructs the experts at the bottom of the model by using different feature interaction methods to improve the generalization ability of the shared information flow. In view of the model's differentiating ability for different task information flows, DEPHN uses feature explicit mapping and virtual gradient coefficient for expert gating during the training process, and adaptively adjusts the learning intensity of the gated unit by considering the difference of gating values and task correlation. Extensive experiments on artificial and real-world datasets demonstrate that our proposed method can capture task correlation in complex situations and achieve better performance than baseline models\footnote{Accepted in IJCNN2023}.
</details>
<details>
<summary>摘要</summary>
优化推荐算法基于多任务学习（MTL）是互联网运营商理解用户和预测多种场景下的行为的主要方法。任务相关性是MTL目标的重要考虑因素，传统模型使用共享底模型和阻止专家来实现共享表示学习和信息差异化。然而，现实世界中任务之间的关系经常更加复杂，现有方法无法正确地分享信息。在这篇论文中，我们提议一种不同表达平行多态网络（DEPHN），以同时模型多个任务。DEPHN在底层模型中使用不同的特征互动方法来提高共享信息流的泛化能力。视情况不同，DEPHN使用特征显式映射和虚拟梯度系数进行专家闭合 durante 训练过程中，并通过考虑闭合值和任务相关性来适应性地调整学习强度。广泛的人工和实际数据测试表明，我们提出的方法可以在复杂情况下捕捉任务相关性并实现比基eline模型更好的性能。
</details></li>
</ul>
<hr>
<h2 id="FaFCNN-A-General-Disease-Classification-Framework-Based-on-Feature-Fusion-Neural-Networks"><a href="#FaFCNN-A-General-Disease-Classification-Framework-Based-on-Feature-Fusion-Neural-Networks" class="headerlink" title="FaFCNN: A General Disease Classification Framework Based on Feature Fusion Neural Networks"></a>FaFCNN: A General Disease Classification Framework Based on Feature Fusion Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12518">http://arxiv.org/abs/2307.12518</a></li>
<li>repo_url: None</li>
<li>paper_authors: Menglin Kong, Shaojie Zhao, Juan Cheng, Xingquan Li, Ri Su, Muzhou Hou, Cong Cao</li>
<li>for: 该文章目的是提出一种基于人类学习过程的feature-aware协同相关神经网络（FaFCNN），用于疾病分类任务。</li>
<li>methods: 该方法基于域外挑战学习和协同相关模型，引入了特征感知互动模块和特征对齐模块。</li>
<li>results: 实验结果表明，使用增强样本的权重学习树训练的FaFCNN在低质量数据集上表现最优，并且在多种竞争基线模型中表现最佳。此外，广泛的实验还证明了该方法的稳定性和每个组件的效果。<details>
<summary>Abstract</summary>
There are two fundamental problems in applying deep learning/machine learning methods to disease classification tasks, one is the insufficient number and poor quality of training samples; another one is how to effectively fuse multiple source features and thus train robust classification models. To address these problems, inspired by the process of human learning knowledge, we propose the Feature-aware Fusion Correlation Neural Network (FaFCNN), which introduces a feature-aware interaction module and a feature alignment module based on domain adversarial learning. This is a general framework for disease classification, and FaFCNN improves the way existing methods obtain sample correlation features. The experimental results show that training using augmented features obtained by pre-training gradient boosting decision tree yields more performance gains than random-forest based methods. On the low-quality dataset with a large amount of missing data in our setup, FaFCNN obtains a consistently optimal performance compared to competitive baselines. In addition, extensive experiments demonstrate the robustness of the proposed method and the effectiveness of each component of the model\footnote{Accepted in IEEE SMC2023}.
</details>
<details>
<summary>摘要</summary>
“there are two fundamental problems in applying deep learning/machine learning methods to disease classification tasks, one is the insufficient number and poor quality of training samples; another one is how to effectively fuse multiple source features and thus train robust classification models. to address these problems, inspired by the process of human learning knowledge, we propose the Feature-aware Fusion Correlation Neural Network (FaFCNN), which introduces a feature-aware interaction module and a feature alignment module based on domain adversarial learning. this is a general framework for disease classification, and FaFCNN improves the way existing methods obtain sample correlation features. the experimental results show that training using augmented features obtained by pre-training gradient boosting decision tree yields more performance gains than random-forest based methods. on the low-quality dataset with a large amount of missing data in our setup, FaFCNN obtains a consistently optimal performance compared to competitive baselines. in addition, extensive experiments demonstrate the robustness of the proposed method and the effectiveness of each component of the model。”Here's the breakdown of the translation:* "insufficient number and poor quality of training samples" becomes "训练样本数量和质量不足"* "feature-aware fusion" becomes "特征意识的融合"* "domain adversarial learning" becomes "领域对抗学习"* "sample correlation features" becomes "样本相关特征"* "gradient boosting decision tree" becomes "梯度提升决策树"* "random-forest based methods" becomes "随机森林基于方法"* "low-quality dataset" becomes "低质量数据集"* "consistently optimal performance" becomes "一致性优化性能"Note that some terms may have been translated differently based on their context and the specific requirements of the translation.
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Evaluation-of-Temporal-Graph-Benchmark"><a href="#An-Empirical-Evaluation-of-Temporal-Graph-Benchmark" class="headerlink" title="An Empirical Evaluation of Temporal Graph Benchmark"></a>An Empirical Evaluation of Temporal Graph Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12510">http://arxiv.org/abs/2307.12510</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yule-BUAA/DyGLib_TGB">https://github.com/yule-BUAA/DyGLib_TGB</a></li>
<li>paper_authors: Le Yu</li>
<li>for: 本研究是一个empirical evaluation of Temporal Graph Benchmark (TGB)，通过扩展我们的Dynamic Graph Library (DyGLib)来对TGB进行评估。</li>
<li>methods: 本研究使用了11种流行的动态图学习方法进行对比，以便更全面地评估TGB。</li>
<li>results: 实验结果显示，不同的模型在不同的数据集上表现出了不同的性能，与之前的观察一致；同时，一些基eline的性能可以通过使用DyGLib进行改进，以提高其性能。<details>
<summary>Abstract</summary>
In this paper, we conduct an empirical evaluation of Temporal Graph Benchmark (TGB) by extending our Dynamic Graph Library (DyGLib) to TGB. Compared with TGB, we include eleven popular dynamic graph learning methods for more exhaustive comparisons. Through the experiments, we find that (1) different models depict varying performance across various datasets, which is in line with previous observations; (2) the performance of some baselines can be significantly improved over the reported results in TGB when using DyGLib. This work aims to ease the researchers' efforts in evaluating various dynamic graph learning methods on TGB and attempts to offer results that can be directly referenced in the follow-up research. All the used resources in this project are publicly available at https://github.com/yule-BUAA/DyGLib_TGB. This work is in progress, and feedback from the community is welcomed for improvements.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们进行了emporical评估Temporal Graph Benchmark（TGB），通过扩展我们的动态图库（DyGLib）到TGB。与TGB相比，我们包括了十一种流行的动态图学习方法，以便更加广泛的比较。通过实验，我们发现了以下两点：（1）不同的模型在不同的数据集上表现出了不同的性能，与之前的观察一致；（2）使用DyGLib时，一些基eline的性能可以得到明显的提高，超过了TGB中所报道的结果。本工作的目标是为研究者提供一个简单便捷的方式来评估不同的动态图学习方法，并提供可直接参考的结果，以便在后续研究中进行建立。所有使用的资源都公开可用于https://github.com/yule-BUAA/DyGLib_TGB。此工作处于进行中，欢迎社区提供反馈以便改进。
</details></li>
</ul>
<hr>
<h2 id="AdvDiff-Generating-Unrestricted-Adversarial-Examples-using-Diffusion-Models"><a href="#AdvDiff-Generating-Unrestricted-Adversarial-Examples-using-Diffusion-Models" class="headerlink" title="AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion Models"></a>AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12499">http://arxiv.org/abs/2307.12499</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuelong Dai, Kaisheng Liang, Bin Xiao</li>
<li>For: The paper aims to generate unrestricted adversarial examples for deep learning models, which can effectively bypass existing defense mechanisms and pose a serious threat to their security.* Methods: The paper proposes a new method called AdvDiff, which uses diffusion models to generate adversarial examples. Two novel adversarial guidance techniques are designed to conduct adversarial sampling in the reverse generation process of diffusion models, making the generated examples high-quality and realistic.* Results: The paper demonstrates the effectiveness of AdvDiff on MNIST and ImageNet datasets, outperforming GAN-based methods in terms of attack performance and generation quality. The generated adversarial examples are of high quality and are able to effectively bypass existing defense mechanisms.<details>
<summary>Abstract</summary>
Unrestricted adversarial attacks present a serious threat to deep learning models and adversarial defense techniques. They pose severe security problems for deep learning applications because they can effectively bypass defense mechanisms. However, previous attack methods often utilize Generative Adversarial Networks (GANs), which are not theoretically provable and thus generate unrealistic examples by incorporating adversarial objectives, especially for large-scale datasets like ImageNet. In this paper, we propose a new method, called AdvDiff, to generate unrestricted adversarial examples with diffusion models. We design two novel adversarial guidance techniques to conduct adversarial sampling in the reverse generation process of diffusion models. These two techniques are effective and stable to generate high-quality, realistic adversarial examples by integrating gradients of the target classifier interpretably. Experimental results on MNIST and ImageNet datasets demonstrate that AdvDiff is effective to generate unrestricted adversarial examples, which outperforms GAN-based methods in terms of attack performance and generation quality.
</details>
<details>
<summary>摘要</summary>
深度学习模型面临无限制的敌对攻击呈现了严重的安全问题。这些攻击可以有效绕过防御机制，对深度学习应用程序的安全造成严重问题。然而，之前的攻击方法经常使用生成式对抗网络（GAN），这些网络不是理论可证明的，因此在大规模数据集如ImageNet中生成不真实的例子。在这篇论文中，我们提出了一种新的方法，称为AdvDiff，用于生成无限制的敌对示例。我们设计了两种新的对抗导航技术来进行对抗采样在扩散模型的反生成过程中。这两种技术是可靠和有效的，可以具有高质量和真实的敌对示例，通过可 интеpretability的梯度来整合目标分类器。实验结果表明，AdvDiff在MNIST和ImageNet数据集上效果地生成了无限制的敌对示例，其性能高于基于GAN的方法。
</details></li>
</ul>
<hr>
<h2 id="A-faster-and-simpler-algorithm-for-learning-shallow-networks"><a href="#A-faster-and-simpler-algorithm-for-learning-shallow-networks" class="headerlink" title="A faster and simpler algorithm for learning shallow networks"></a>A faster and simpler algorithm for learning shallow networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12496">http://arxiv.org/abs/2307.12496</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Sitan Chen, Shyam Narayanan</li>
<li>for: 学习一个线性组合的 $k$ 个 ReLU 活化器，给出标注的示例来自标准 $d$-维高斯分布。</li>
<li>methods: 使用 Chen et al. [CDG+23] 提出的算法，该算法可以在 $\text{poly}(d,1&#x2F;\varepsilon)$ 时间内运行，当 $k &#x3D; O(1)$ 时。</li>
<li>results: 我们表明，使用一个简单的一阶版本的该算法即可，其运行时间为 $(d&#x2F;\varepsilon)^{O(k^2)} $。<details>
<summary>Abstract</summary>
We revisit the well-studied problem of learning a linear combination of $k$ ReLU activations given labeled examples drawn from the standard $d$-dimensional Gaussian measure. Chen et al. [CDG+23] recently gave the first algorithm for this problem to run in $\text{poly}(d,1/\varepsilon)$ time when $k = O(1)$, where $\varepsilon$ is the target error. More precisely, their algorithm runs in time $(d/\varepsilon)^{\mathrm{quasipoly}(k)}$ and learns over multiple stages. Here we show that a much simpler one-stage version of their algorithm suffices, and moreover its runtime is only $(d/\varepsilon)^{O(k^2)}$.
</details>
<details>
<summary>摘要</summary>
我们回到了已经很好地研究过的问题：学习一个线性结构中的 $k$ 个 ReLU 活化器，使用标示的例子从标准 $d$-维 Gaussian 分布中获取。陈等人 [CDG+23] 最近提出了首个可以在 $\text{poly}(d,1/\varepsilon)$ 时间内运行的算法，其中 $k = O(1)$，并且目标错误为 $\varepsilon$。更精确地说，他们的算法在多个阶段中运行，并且其时间长度为 $(d/\varepsilon)^{\mathrm{quasipoly}(k)}$。在这里，我们表明了一个更简单的一阶版本的他们的算法足够，并且其时间长度仅为 $(d/\varepsilon)^{O(k^2)}$。
</details></li>
</ul>
<hr>
<h2 id="Learning-Universal-and-Robust-3D-Molecular-Representations-with-Graph-Convolutional-Networks"><a href="#Learning-Universal-and-Robust-3D-Molecular-Representations-with-Graph-Convolutional-Networks" class="headerlink" title="Learning Universal and Robust 3D Molecular Representations with Graph Convolutional Networks"></a>Learning Universal and Robust 3D Molecular Representations with Graph Convolutional Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12491">http://arxiv.org/abs/2307.12491</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuo Zhang, Yang Liu, Li Xie, Lei Xie</li>
<li>for: 用于学习分子准确表示</li>
<li>methods: 使用方向节点对（DNP）描述器和分子图 convolutional neural network（RoM-GCN）</li>
<li>results: 超过所有参考基eline表现， validate the superiority of DNP descriptor in incorporating 3D geometric information of molecules.<details>
<summary>Abstract</summary>
To learn accurate representations of molecules, it is essential to consider both chemical and geometric features. To encode geometric information, many descriptors have been proposed in constrained circumstances for specific types of molecules and do not have the properties to be ``robust": 1. Invariant to rotations and translations; 2. Injective when embedding molecular structures. In this work, we propose a universal and robust Directional Node Pair (DNP) descriptor based on the graph representations of 3D molecules. Our DNP descriptor is robust compared to previous ones and can be applied to multiple molecular types. To combine the DNP descriptor and chemical features in molecules, we construct the Robust Molecular Graph Convolutional Network (RoM-GCN) which is capable to take both node and edge features into consideration when generating molecule representations. We evaluate our model on protein and small molecule datasets. Our results validate the superiority of the DNP descriptor in incorporating 3D geometric information of molecules. RoM-GCN outperforms all compared baselines.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-Resource-Allocation-Policy-Vertex-GNN-or-Edge-GNN"><a href="#Learning-Resource-Allocation-Policy-Vertex-GNN-or-Edge-GNN" class="headerlink" title="Learning Resource Allocation Policy: Vertex-GNN or Edge-GNN?"></a>Learning Resource Allocation Policy: Vertex-GNN or Edge-GNN?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12480">http://arxiv.org/abs/2307.12480</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Peng, Jia Guo, Chenyang Yang</li>
<li>for: 这篇论文探讨了用Graph Neural Networks（GNNs）学习无线策略的问题，具体来说是链接调度、功率控制和 precoding 策略。</li>
<li>methods: 这篇论文使用了Vertex-GNNs和Edge-GNNs两种不同的GNNs来学习无线策略。它们都是通过处理和 pooling 邻居链接和边的信息来更新隐藏表示的。</li>
<li>results: 研究发现，GNNs的表达力取决于处理和组合函数的线性和输出维度。当使用线性处理器时，链接-GNNs 无法分辨所有的通道矩阵，而 Edge-GNNs 可以。在学习 precoding 策略时，即使使用非线性处理器，链接-GNNs 的表达力仍然可能不够强大，因为通道矩阵的维度压缩。研究还提供了 necessary conditions 以确保 GNNs 能够好好地学习 precoding 策略。实验结果证明了这些分析结论，并表明 Edge-GNNs 可以在训练和推理时间更低的情况下达到同样的性能。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) update the hidden representations of vertices (called Vertex-GNNs) or hidden representations of edges (called Edge-GNNs) by processing and pooling the information of neighboring vertices and edges and combining to incorporate graph topology. When learning resource allocation policies, GNNs cannot perform well if their expressive power are weak, i.e., if they cannot differentiate all input features such as channel matrices. In this paper, we analyze the expressive power of the Vertex-GNNs and Edge-GNNs for learning three representative wireless policies: link scheduling, power control, and precoding policies. We find that the expressive power of the GNNs depend on the linearity and output dimensions of the processing and combination functions. When linear processors are used, the Vertex-GNNs cannot differentiate all channel matrices due to the loss of channel information, while the Edge-GNNs can. When learning the precoding policy, even the Vertex-GNNs with non-linear processors may not be with strong expressive ability due to the dimension compression. We proceed to provide necessary conditions for the GNNs to well learn the precoding policy. Simulation results validate the analyses and show that the Edge-GNNs can achieve the same performance as the Vertex-GNNs with much lower training and inference time.
</details>
<details>
<summary>摘要</summary>
Graph neural networks (GNNs) 更新隐藏表示的顶点（称为顶点GNNs）或边的隐藏表示（称为边GNNs），通过处理和汇聚邻居顶点和边的信息，并将其结合以利用图 topology。在学习资源分配策略时，如果GNNs的表达力弱，即无法分辨输入特征，如通道矩阵。在这篇论文中，我们分析顶点GNNs和边GNNs在学习三种代表性无线策略：链接调度、功率控制和预处理策略。我们发现GNNs的表达力取决于处理和组合函数的线性和输出维度。当使用线性处理器时，顶点GNNs无法分辨所有通道矩阵，而边GNNs可以。在学习预处理策略时，即使使用非线性处理器，顶点GNNs的表达力仍然不强，这是因为维度压缩。我们随后提供了必要的条件，使GNNs能够好好地学习预处理策略。实验结果证明了我们的分析，并显示了边GNNs可以与顶点GNNs相比，在训练和推理时间上减少很多。
</details></li>
</ul>
<hr>
<h2 id="Model-free-generalized-fiducial-inference"><a href="#Model-free-generalized-fiducial-inference" class="headerlink" title="Model-free generalized fiducial inference"></a>Model-free generalized fiducial inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12472">http://arxiv.org/abs/2307.12472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan P Williams</li>
<li>for:  This paper aims to develop a model-free statistical framework for uncertainty quantification in machine learning, with a focus on imprecise probabilistic prediction inference.</li>
<li>methods:  The paper proposes and develops a new approach to uncertainty quantification that facilitates finite sample control of type 1 errors and offers more versatile tools for imprecise probabilistic reasoning.</li>
<li>results:  The paper presents a precise probabilistic approximation to the model-free imprecise framework, which is critical for the broader adoption of imprecise probabilistic approaches to inference in the statistical and machine learning communities.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目标是为机器学习领域开发一种无模型的统计框架，用于不确定性评估。</li>
<li>methods: 论文提出了一种新的不确定性评估方法，它可以实现finite sample控制类型1错误，并且提供更多的不确定性推理工具。</li>
<li>results: 论文提出了一种precise的统计approximation，用于无模型不确定性推理框架。这种approximation是用于在统计和机器学习领域更广泛地采用不确定性推理方法的标准。<details>
<summary>Abstract</summary>
Motivated by the need for the development of safe and reliable methods for uncertainty quantification in machine learning, I propose and develop ideas for a model-free statistical framework for imprecise probabilistic prediction inference. This framework facilitates uncertainty quantification in the form of prediction sets that offer finite sample control of type 1 errors, a property shared with conformal prediction sets, but this new approach also offers more versatile tools for imprecise probabilistic reasoning. Furthermore, I propose and consider the theoretical and empirical properties of a precise probabilistic approximation to the model-free imprecise framework. Approximating a belief/plausibility measure pair by an [optimal in some sense] probability measure in the credal set is a critical resolution needed for the broader adoption of imprecise probabilistic approaches to inference in statistical and machine learning communities. It is largely undetermined in the statistical and machine learning literatures, more generally, how to properly quantify uncertainty in that there is no generally accepted standard of accountability of stated uncertainties. The research I present in this manuscript is aimed at motivating a framework for statistical inference with reliability and accountability as the guiding principles.
</details>
<details>
<summary>摘要</summary>
Motivated by the need for safe and reliable methods for uncertainty quantification in machine learning, I propose and develop ideas for a model-free statistical framework for imprecise probabilistic prediction inference. This framework enables uncertainty quantification in the form of prediction sets that offer finite sample control of type 1 errors, a property shared with conformal prediction sets, but this new approach also offers more versatile tools for imprecise probabilistic reasoning. Furthermore, I propose and consider the theoretical and empirical properties of a precise probabilistic approximation to the model-free imprecise framework. Approximating a belief/plausibility measure pair by an optimal probability measure in the credal set is a critical resolution needed for the broader adoption of imprecise probabilistic approaches to inference in the statistical and machine learning communities. It is largely undetermined in the statistical and machine learning literatures how to properly quantify uncertainty, and the research I present in this manuscript is aimed at motivating a framework for statistical inference with reliability and accountability as the guiding principles.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Data-Distillation-Do-Not-Overlook-Calibration"><a href="#Rethinking-Data-Distillation-Do-Not-Overlook-Calibration" class="headerlink" title="Rethinking Data Distillation: Do Not Overlook Calibration"></a>Rethinking Data Distillation: Do Not Overlook Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12463">http://arxiv.org/abs/2307.12463</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dongyaozhu/calibrate-networks-trained-on-distilled-datasets">https://github.com/dongyaozhu/calibrate-networks-trained-on-distilled-datasets</a></li>
<li>paper_authors: Dongyao Zhu, Bowen Lei, Jie Zhang, Yanbo Fang, Ruqi Zhang, Yiqun Xie, Dongkuan Xu</li>
<li>for: 这 paper 的目的是解决基于压缩数据的神经网络模型具有过度自信的问题，通过calibration方法来改善模型的准确性。</li>
<li>methods: 该 paper 使用的方法包括temperature scaling和mixup，但发现这些方法无法calibrate基于压缩数据的神经网络模型。为了解决这问题， authors 提出了Masked Temperature Scaling (MTS)和Masked Distillation Training (MDT)两种方法，以改善模型的准确性并保持数据压缩的效率。</li>
<li>results: Authors 通过实验表明，MTS 和 MDT 可以减轻基于压缩数据的神经网络模型的过度自信问题，同时保持模型的准确性和效率。<details>
<summary>Abstract</summary>
Neural networks trained on distilled data often produce over-confident output and require correction by calibration methods. Existing calibration methods such as temperature scaling and mixup work well for networks trained on original large-scale data. However, we find that these methods fail to calibrate networks trained on data distilled from large source datasets. In this paper, we show that distilled data lead to networks that are not calibratable due to (i) a more concentrated distribution of the maximum logits and (ii) the loss of information that is semantically meaningful but unrelated to classification tasks. To address this problem, we propose Masked Temperature Scaling (MTS) and Masked Distillation Training (MDT) which mitigate the limitations of distilled data and achieve better calibration results while maintaining the efficiency of dataset distillation.
</details>
<details>
<summary>摘要</summary>
neural networks 经过精炼数据训练后通常会生成过自信的输出，需要进行减调方法来修正。现有的减调方法包括温度Scaling和mixup，这些方法在原始大规模数据上训练的网络上工作良好。然而，我们发现这些方法无法calibrate来自大源数据集的数据精炼后的网络。在这篇论文中，我们显示出精炼数据导致的网络不可calibrate，因为（i）精炼数据集的最大лог值的分布更加集中，以及（ii） Semantically meaningful but unrelated to classification tasks的信息被丢失。为解决这个问题，我们提议使用Masked Temperature Scaling（MTS）和Masked Distillation Training（MDT），这些方法可以减少精炼数据的限制，实现更好的减调结果，同时保持数据精炼的效率。
</details></li>
</ul>
<hr>
<h2 id="Rates-of-Approximation-by-ReLU-Shallow-Neural-Networks"><a href="#Rates-of-Approximation-by-ReLU-Shallow-Neural-Networks" class="headerlink" title="Rates of Approximation by ReLU Shallow Neural Networks"></a>Rates of Approximation by ReLU Shallow Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12461">http://arxiv.org/abs/2307.12461</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tong Mao, Ding-Xuan Zhou</li>
<li>for: 这 paper  investigate  shallow neural network  approximating H&quot;older space 函数。</li>
<li>methods:  paper 使用 ReLU  activation function 和一层隐藏层来 aproximate H&quot;older space 函数。</li>
<li>results: paper 得到 uniform approximation 率 O((\log m)^{1&#x2F;2 + d} m^{-r&#x2F;d (d+2)&#x2F;(d+4)}) when r &lt; d&#x2F;2 + 2, 这个率几乎与最佳率 O(m^{-r&#x2F;d}) 相近。<details>
<summary>Abstract</summary>
Neural networks activated by the rectified linear unit (ReLU) play a central role in the recent development of deep learning. The topic of approximating functions from H\"older spaces by these networks is crucial for understanding the efficiency of the induced learning algorithms. Although the topic has been well investigated in the setting of deep neural networks with many layers of hidden neurons, it is still open for shallow networks having only one hidden layer. In this paper, we provide rates of uniform approximation by these networks. We show that ReLU shallow neural networks with $m$ hidden neurons can uniformly approximate functions from the H\"older space $W_\infty^r([-1, 1]^d)$ with rates $O((\log m)^{\frac{1}{2} +d}m^{-\frac{r}{d}\frac{d+2}{d+4}})$ when $r<d/2 +2$. Such rates are very close to the optimal one $O(m^{-\frac{r}{d}})$ in the sense that $\frac{d+2}{d+4}$ is close to $1$, when the dimension $d$ is large.
</details>
<details>
<summary>摘要</summary>
“神经网络activated by rectified linear unit（ReLU）在深度学习的发展中扮演了中心作用。关于使用这些网络来近似Holder空间中函数的问题，是深度学习算法的效率的关键。虽然这个问题在多层神经网络的设置下已经得到了广泛的研究，但是尚未解决了单层神经网络的情况。在这篇论文中，我们提供了uniform approximation的速率。我们证明了ReLU单层神经网络可以 uniform approximation W∞^r([-1, 1]^d)中函数的速率为O((\log m)^\(\frac{1}{2} + d\)m^(\frac{r}{d}\*\(\frac{d+2}{d+4}\))），当r<d/2+2时。这些速率与优化的一个 O(m^(-r/d)) 很相似，当维度d很大时，\(\frac{d+2}{d+4}\)几乎等于1。”
</details></li>
</ul>
<hr>
<h2 id="Information-theoretic-Analysis-of-Test-Data-Sensitivity-in-Uncertainty"><a href="#Information-theoretic-Analysis-of-Test-Data-Sensitivity-in-Uncertainty" class="headerlink" title="Information-theoretic Analysis of Test Data Sensitivity in Uncertainty"></a>Information-theoretic Analysis of Test Data Sensitivity in Uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12456">http://arxiv.org/abs/2307.12456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Futoshi Futami, Tomoharu Iwata</li>
<li>for: 本研究旨在更好地理解bayesian inference中的uncertainty量化任务中的不确定性。</li>
<li>methods: 本文使用了xu和raginsky（2022）的新方法，即将预测不确定性 decomposed into two kinds of uncertainty：aleatoric uncertainty和epistemic uncertainty。这两种不确定性分别表示数据生成过程中的自然随机性和数据不够的多样性。</li>
<li>results: 本研究成功地定义了这种不确定性敏感性，并将其与信息理论量化的方式相关联。此外，本文还扩展了bayesian meta-learning中的现有分析，并首次显示了任务之间的新的敏感性关系。<details>
<summary>Abstract</summary>
Bayesian inference is often utilized for uncertainty quantification tasks. A recent analysis by Xu and Raginsky 2022 rigorously decomposed the predictive uncertainty in Bayesian inference into two uncertainties, called aleatoric and epistemic uncertainties, which represent the inherent randomness in the data-generating process and the variability due to insufficient data, respectively. They analyzed those uncertainties in an information-theoretic way, assuming that the model is well-specified and treating the model's parameters as latent variables. However, the existing information-theoretic analysis of uncertainty cannot explain the widely believed property of uncertainty, known as the sensitivity between the test and training data. It implies that when test data are similar to training data in some sense, the epistemic uncertainty should become small. In this work, we study such uncertainty sensitivity using our novel decomposition method for the predictive uncertainty. Our analysis successfully defines such sensitivity using information-theoretic quantities. Furthermore, we extend the existing analysis of Bayesian meta-learning and show the novel sensitivities among tasks for the first time.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DiAMoNDBack-Diffusion-denoising-Autoregressive-Model-for-Non-Deterministic-Backmapping-of-Cα-Protein-Traces"><a href="#DiAMoNDBack-Diffusion-denoising-Autoregressive-Model-for-Non-Deterministic-Backmapping-of-Cα-Protein-Traces" class="headerlink" title="DiAMoNDBack: Diffusion-denoising Autoregressive Model for Non-Deterministic Backmapping of Cα Protein Traces"></a>DiAMoNDBack: Diffusion-denoising Autoregressive Model for Non-Deterministic Backmapping of Cα Protein Traces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12451">http://arxiv.org/abs/2307.12451</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ferg-lab/diamondback">https://github.com/ferg-lab/diamondback</a></li>
<li>paper_authors: Michael S. Jones, Kirill Shmilovich, Andrew L. Ferguson</li>
<li>for: 这个论文的目的是开发一种基于概率模型的方法，用于从粗细化分子模型中恢复到原子分子模型，以便在长时间尺度上模拟蛋白质的杂化和折叠过程。</li>
<li>methods: 这个方法基于推 diffusion-denoising autoregressive model，使用推 diffusion过程来恢复原子分子模型，并且使用了当地的概率模型来保证生成的结果是可靠的。</li>
<li>results: 这个方法在65000多个蛋白质结构数据上进行训练，并在一个独立的蛋白质结构数据集上进行验证，以及在分子动力学 simulated annealing 和 coarse-grained simulation 数据上进行应用。结果表明，这个方法可以高效地恢复原子分子模型，并且可以保证生成的结果是可靠的和多样的。<details>
<summary>Abstract</summary>
Coarse-grained molecular models of proteins permit access to length and time scales unattainable by all-atom models and the simulation of processes that occur on long-time scales such as aggregation and folding. The reduced resolution realizes computational accelerations but an atomistic representation can be vital for a complete understanding of mechanistic details. Backmapping is the process of restoring all-atom resolution to coarse-grained molecular models. In this work, we report DiAMoNDBack (Diffusion-denoising Autoregressive Model for Non-Deterministic Backmapping) as an autoregressive denoising diffusion probability model to restore all-atom details to coarse-grained protein representations retaining only C{\alpha} coordinates. The autoregressive generation process proceeds from the protein N-terminus to C-terminus in a residue-by-residue fashion conditioned on the C{\alpha} trace and previously backmapped backbone and side chain atoms within the local neighborhood. The local and autoregressive nature of our model makes it transferable between proteins. The stochastic nature of the denoising diffusion process means that the model generates a realistic ensemble of backbone and side chain all-atom configurations consistent with the coarse-grained C{\alpha} trace. We train DiAMoNDBack over 65k+ structures from Protein Data Bank (PDB) and validate it in applications to a hold-out PDB test set, intrinsically-disordered protein structures from the Protein Ensemble Database (PED), molecular dynamics simulations of fast-folding mini-proteins from DE Shaw Research, and coarse-grained simulation data. We achieve state-of-the-art reconstruction performance in terms of correct bond formation, avoidance of side chain clashes, and diversity of the generated side chain configurational states. We make DiAMoNDBack model publicly available as a free and open source Python package.
</details>
<details>
<summary>摘要</summary>
高级分辨率蛋白质模型允许访问不可达的长度和时间尺度，如聚集和折叠过程。减少分辨率实现计算加速，但全原子表示是完全理解机制细节的必要条件。在这种工作中，我们报道了一种推 diffusion-denoising autoregressive model for non-deterministic backmapping（DiAMoNDBack），用于在高级分辨率蛋白质模型中恢复全原子细节，保留只有Cα坐标。这种推 diffusion进程从蛋白质N端到C端的推进程，在每个残基上进行推进，conditioned on the Cα trace和已经backmapping的蛋白质脊梁和副链原子。本地和自适应的我们模型使其可以在不同蛋白质上传递。由于推 diffusion过程的随机性，模型会生成一个真实的ensemble of backbone和副链全原子配置，与高级分辨率Cα轨迹相符。我们在65000+结构中训练DiAMoNDBack，并对其进行了验证。我们在应用于保留PDB测试集、蛋白质结构数据库（PED）中的自发性蛋白质结构、DE Shaw Research的分秒级分子动力学 simulations和含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂
</details></li>
</ul>
<hr>
<h2 id="ProtoFL-Unsupervised-Federated-Learning-via-Prototypical-Distillation"><a href="#ProtoFL-Unsupervised-Federated-Learning-via-Prototypical-Distillation" class="headerlink" title="ProtoFL: Unsupervised Federated Learning via Prototypical Distillation"></a>ProtoFL: Unsupervised Federated Learning via Prototypical Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12450">http://arxiv.org/abs/2307.12450</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hansol Kim, Youngjun Kwak, Minyoung Jung, Jinho Shin, Youngsung Kim, Changick Kim</li>
<li>for: 提高数据隐私保护和授权系统性能</li>
<li>methods: 使用Prototypical Representation Distillation和不监控学习来增强全球模型的表示力和减少通信成本</li>
<li>results: 在五个广泛使用的标准数据集上进行了广泛的实验，证明了我们提出的框架在先前的文献中表现更出色，并且在一类分类任务中提高了表现。<details>
<summary>Abstract</summary>
Federated learning (FL) is a promising approach for enhancing data privacy preservation, particularly for authentication systems. However, limited round communications, scarce representation, and scalability pose significant challenges to its deployment, hindering its full potential. In this paper, we propose 'ProtoFL', Prototypical Representation Distillation based unsupervised Federated Learning to enhance the representation power of a global model and reduce round communication costs. Additionally, we introduce a local one-class classifier based on normalizing flows to improve performance with limited data. Our study represents the first investigation of using FL to improve one-class classification performance. We conduct extensive experiments on five widely used benchmarks, namely MNIST, CIFAR-10, CIFAR-100, ImageNet-30, and Keystroke-Dynamics, to demonstrate the superior performance of our proposed framework over previous methods in the literature.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种有前途的方法，尤其是 для 验证系统的数据隐私保护。然而，有限的回合通信、罕见的表现和扩展性问题，对其部署带来很大的挑战，阻碍其完整的潜力。在这篇论文中，我们提出了“ProtoFL”，基于无监督式聚合学习的构造塑性数据分布来增强全球模型的表现力和减少回合通信成本。此外，我们还引入了基于正规函数的本地一类分类器，以提高具有有限数据的表现。我们的研究是文献中第一次使用 federated learning 提高一类分类性能的调查。我们对五种通用测试集进行了广泛的实验，包括 MNIST、CIFAR-10、CIFAR-100、ImageNet-30 和 Keystroke-Dynamics，以示出我们的提案架构在先前的方法上表现出色。
</details></li>
</ul>
<hr>
<h2 id="WEPRO-Weight-Prediction-for-Efficient-Optimization-of-Hybrid-Quantum-Classical-Algorithms"><a href="#WEPRO-Weight-Prediction-for-Efficient-Optimization-of-Hybrid-Quantum-Classical-Algorithms" class="headerlink" title="WEPRO: Weight Prediction for Efficient Optimization of Hybrid Quantum-Classical Algorithms"></a>WEPRO: Weight Prediction for Efficient Optimization of Hybrid Quantum-Classical Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12449">http://arxiv.org/abs/2307.12449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Satwik Kundu, Debarshi Kundu, Swaroop Ghosh</li>
<li>for: 提高量子 simulate 的效率，以便更好地训练量子 neural network、量子归一化算法和量子优化算法。</li>
<li>methods: 提出一种新的方法——WEPRO，通过利用参数 weights 的常见趋势来加速量子 simulate 的 converge。并 introduce 两种优化预测性能的技术——Naive Prediction 和 Adaptive Prediction。</li>
<li>results: 通过对多个量子 neural network 模型的训练和多种数据集的实验，显示 WEPRO 可以提供约 $2.25\times$ 的速度提升，同时也提供更高的准确率（最高提升 $2.3%$）和更低的损失（最低降低 $6.1%$），并具有低的存储和计算负担。此外，对 VQE 和 QAOA 进行了评估，结果表明 WEPRO 在这些应用中也可以提供速度提升（最高提升 $3.1\times$ 和 $2.91\times$），同时使用更少的射击数（最多降低 $3.3\times$）。<details>
<summary>Abstract</summary>
The exponential run time of quantum simulators on classical machines and long queue depths and high costs of real quantum devices present significant challenges in the effective training of Variational Quantum Algorithms (VQAs) like Quantum Neural Networks (QNNs), Variational Quantum Eigensolver (VQE) and Quantum Approximate Optimization Algorithm (QAOA). To address these limitations, we propose a new approach, WEPRO (Weight Prediction), which accelerates the convergence of VQAs by exploiting regular trends in the parameter weights. We introduce two techniques for optimal prediction performance namely, Naive Prediction (NaP) and Adaptive Prediction (AdaP). Through extensive experimentation and training of multiple QNN models on various datasets, we demonstrate that WEPRO offers a speedup of approximately $2.25\times$ compared to standard training methods, while also providing improved accuracy (up to $2.3\%$ higher) and loss (up to $6.1\%$ lower) with low storage and computational overheads. We also evaluate WEPRO's effectiveness in VQE for molecular ground-state energy estimation and in QAOA for graph MaxCut. Our results show that WEPRO leads to speed improvements of up to $3.1\times$ for VQE and $2.91\times$ for QAOA, compared to traditional optimization techniques, while using up to $3.3\times$ less number of shots (i.e., repeated circuit executions) per training iteration.
</details>
<details>
<summary>摘要</summary>
“量子 simulate 的时间复杂度和磁盘队列深度以及真正量子设备的成本高，对于量子神经网络（QNN）、量子准确矩阵（VQE）和量子近似优化算法（QAOA）的有效训练带来了 significante 挑战。为了解决这些限制，我们提出了一种新的方法，WEPRO（参数预测），它通过利用参数权重的固有趋势来加速 VQAs 的快速整合。我们 introduce 了两种优化性能的技术， namely，Naive Prediction（NaP）和 Adaptive Prediction（AdaP）。通过对多个 QNN 模型在不同的数据集进行广泛的实验和训练，我们证明了 WEPRO 可以提供约 $2.25\times$ 的速度提升，同时也提供了更高的准确率（最高 $2.3\%$）和损失（最低 $6.1\%$），并且具有低的存储和计算负担。我们还评估了 WEPRO 在 VQE 中的粒子能量估计和 QAOA 中的图像 MaxCut 性能。我们的结果表明，WEPRO 可以提供 Up to $3.1\times$ 的速度提升在 VQE 中，并且使用 Up to $3.3\times$  fewer number of shots per training iteration。”
</details></li>
</ul>
<hr>
<h2 id="Multifidelity-Covariance-Estimation-via-Regression-on-the-Manifold-of-Symmetric-Positive-Definite-Matrices"><a href="#Multifidelity-Covariance-Estimation-via-Regression-on-the-Manifold-of-Symmetric-Positive-Definite-Matrices" class="headerlink" title="Multifidelity Covariance Estimation via Regression on the Manifold of Symmetric Positive Definite Matrices"></a>Multifidelity Covariance Estimation via Regression on the Manifold of Symmetric Positive Definite Matrices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12438">http://arxiv.org/abs/2307.12438</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aimee Maurais, Terrence Alsup, Benjamin Peherstorfer, Youssef Marzouk</li>
<li>for: 这个论文的目的是提出一种多 fidelties  covariance矩阵估计器，用于恰当地估计 covariance矩阵的特征。</li>
<li>methods: 这个估计器是基于投影问题的 manifold 上的 regression 问题，使用 manifold 上的 Mahalanobis 距离来最小化。</li>
<li>results:  compared to single-fidelity 和其他多 fidelties  covariance估计器，这个估计器可以提供更好的估计结果，减少平方差误差高达一个数量级。此外，估计器保持了正定性，使其可以在下游任务中使用，如数据充背和度量学习。<details>
<summary>Abstract</summary>
We introduce a multifidelity estimator of covariance matrices formulated as the solution to a regression problem on the manifold of symmetric positive definite matrices. The estimator is positive definite by construction, and the Mahalanobis distance minimized to obtain it possesses properties which enable practical computation. We show that our manifold regression multifidelity (MRMF) covariance estimator is a maximum likelihood estimator under a certain error model on manifold tangent space. More broadly, we show that our Riemannian regression framework encompasses existing multifidelity covariance estimators constructed from control variates. We demonstrate via numerical examples that our estimator can provide significant decreases, up to one order of magnitude, in squared estimation error relative to both single-fidelity and other multifidelity covariance estimators. Furthermore, preservation of positive definiteness ensures that our estimator is compatible with downstream tasks, such as data assimilation and metric learning, in which this property is essential.
</details>
<details>
<summary>摘要</summary>
我们介绍一个多信度估计器，用于构造对称正定矩阵的covariance矩阵。这个估计器是由条件最小化 Mahalanobis 距离来定义的，并且具有可行 computation 的属性。我们显示了我们的数据 regression 多信度估计器（MRMF）是一个最大 LIKELIHOOD 估计器，在某些错误模型上的拓扑向量空间上。更加广泛地说，我们的 Riemannian  regression 框架包含了其他多信度 covariance 估计器，它们是由控制预测项所构成的。我们透过 numerics 例子展示了我们的估计器可以提供相对于单信度和其他多信度 covariance 估计器的一个多倍减少，甚至是一个阶层减少。此外，保持对称正定性的保证，使得我们的估计器适合进行下游任务，如数据吸收和度量学习，在这些任务中，这个属性是必要的。
</details></li>
</ul>
<hr>
<h2 id="A-Generalized-Schwarz-type-Non-overlapping-Domain-Decomposition-Method-using-Physics-constrained-Neural-Networks"><a href="#A-Generalized-Schwarz-type-Non-overlapping-Domain-Decomposition-Method-using-Physics-constrained-Neural-Networks" class="headerlink" title="A Generalized Schwarz-type Non-overlapping Domain Decomposition Method using Physics-constrained Neural Networks"></a>A Generalized Schwarz-type Non-overlapping Domain Decomposition Method using Physics-constrained Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12435">http://arxiv.org/abs/2307.12435</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hipersimlab/pecann">https://github.com/hipersimlab/pecann</a></li>
<li>paper_authors: Shamsulhaq Basir, Inanc Senocak</li>
<li>for: 解决部分� differential 方程（PDEs）的前向和 inverse 问题</li>
<li>methods: 使用人工神经网络和非重叠领域划分法，并采用一般化的Robin条件来保证邻居子域的一致性</li>
<li>results: 对一种 Laplace 和 Helmholtz 方程的前向和 inverse 问题进行了广泛的实验，并证明了该方法的灵活性和性能<details>
<summary>Abstract</summary>
We present a meshless Schwarz-type non-overlapping domain decomposition method based on artificial neural networks for solving forward and inverse problems involving partial differential equations (PDEs). To ensure the consistency of solutions across neighboring subdomains, we adopt a generalized Robin-type interface condition, assigning unique Robin parameters to each subdomain. These subdomain-specific Robin parameters are learned to minimize the mismatch on the Robin interface condition, facilitating efficient information exchange during training. Our method is applicable to both the Laplace's and Helmholtz equations. It represents local solutions by an independent neural network model which is trained to minimize the loss on the governing PDE while strictly enforcing boundary and interface conditions through an augmented Lagrangian formalism. A key strength of our method lies in its ability to learn a Robin parameter for each subdomain, thereby enhancing information exchange with its neighboring subdomains. We observe that the learned Robin parameters adapt to the local behavior of the solution, domain partitioning and subdomain location relative to the overall domain. Extensive experiments on forward and inverse problems, including one-way and two-way decompositions with crosspoints, demonstrate the versatility and performance of our proposed approach.
</details>
<details>
<summary>摘要</summary>
我们提出了一种无缝 Schwarz-类非重叠域分解方法，基于人工神经网络来解决部分� differential 方程（PDEs）的前向和反向问题。为确保邻居子域解的一致性，我们采用了一种泛化 Robin-类界面条件，将每个子域分配特定的 Robin 参数。这些子域特定的 Robin 参数通过训练来最小化 Robin 界面条件的差异，从而促进信息交换的有效进行。我们的方法适用于勒拉契和赫尔姆霍兹方程。它通过独立的神经网络模型来表示本地解，该模型在权重 Lagrange  formalism 下 strict 执行边界和界面条件，以最小化 governing PDE 中的损失。我们的方法的一个关键优点在于可以学习每个子域的 Robin 参数，从而提高信息交换的效率。我们发现，学习的 Robin 参数适应本地解的行为、域分割和子域的位置相对于总域。我们的实验表明，我们提出的方法在前向和反向问题中展现出了广泛的应用和高效性。
</details></li>
</ul>
<hr>
<h2 id="Augmented-Box-Replay-Overcoming-Foreground-Shift-for-Incremental-Object-Detection"><a href="#Augmented-Box-Replay-Overcoming-Foreground-Shift-for-Incremental-Object-Detection" class="headerlink" title="Augmented Box Replay: Overcoming Foreground Shift for Incremental Object Detection"></a>Augmented Box Replay: Overcoming Foreground Shift for Incremental Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12427">http://arxiv.org/abs/2307.12427</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/YuyangSunshine/ABR_IOD">https://github.com/YuyangSunshine/ABR_IOD</a></li>
<li>paper_authors: Liu Yuyang, Cong Yang, Goswami Dipam, Liu Xialei, Joost van de Weijer</li>
<li>for: Addressing catastrophic forgetting in incremental object detection (IOD) by replaying stored samples from previous tasks.</li>
<li>methods: Novel Augmented Box Replay (ABR) method that only stores and replays foreground objects, and an innovative Attentive RoI Distillation loss that uses spatial attention from region-of-interest (RoI) features to constrain current model.</li>
<li>results: Significant reduction of forgetting of previous classes while maintaining high plasticity in current classes, with considerably reduced storage requirements compared to standard image replay. State-of-the-art performance on Pascal-VOC and COCO datasets.Here’s the full summary in Simplified Chinese:</li>
<li>for: 这个论文目的是解决对象检测 tasks 的 catastrophic forgetting 问题，通过在前一 tasks 中储存和重新播放当前 tasks 的标本。</li>
<li>methods: 提出了一种名为 Augmented Box Replay (ABR) 的新方法，它只储存和重新播放前一 tasks 中的前景物体，以避免 background 中的前景物体对 current tasks 的影响。此外，还提出了一个创新的 Attentive RoI Distillation loss，它使用了 Region-of-Interest (RoI) 特征的空间注意力来给 current model 强制关注前一 tasks 中最重要的信息。</li>
<li>results: ABR 方法能够有效降低 previous classes 的忘却，同时保持高度的柔软性，并且与标准的 image replay 相比，储存需求有所减少。实验结果表明，模型在 Pascal-VOC 和 COCO 数据集上具有国际一流的表现。<details>
<summary>Abstract</summary>
In incremental learning, replaying stored samples from previous tasks together with current task samples is one of the most efficient approaches to address catastrophic forgetting. However, unlike incremental classification, image replay has not been successfully applied to incremental object detection (IOD). In this paper, we identify the overlooked problem of foreground shift as the main reason for this. Foreground shift only occurs when replaying images of previous tasks and refers to the fact that their background might contain foreground objects of the current task. To overcome this problem, a novel and efficient Augmented Box Replay (ABR) method is developed that only stores and replays foreground objects and thereby circumvents the foreground shift problem. In addition, we propose an innovative Attentive RoI Distillation loss that uses spatial attention from region-of-interest (RoI) features to constrain current model to focus on the most important information from old model. ABR significantly reduces forgetting of previous classes while maintaining high plasticity in current classes. Moreover, it considerably reduces the storage requirements when compared to standard image replay. Comprehensive experiments on Pascal-VOC and COCO datasets support the state-of-the-art performance of our model.
</details>
<details>
<summary>摘要</summary>
增量学习中，重复先前任务中的样本与当前任务的样本一起是解决快速忘却最有效的方法之一。然而，与增量分类不同，增量物体检测（IOD）中的图像重复还没有得到成功应用。在这篇论文中，我们认为背景变化导致的前景偏移是主要的问题。只有在重复先前任务的图像时，前景中可能包含当前任务的前景对象。为解决这个问题，我们开发了一种新的和高效的增强框架重复（ABR）方法，该方法只存储和重复前景对象，从而缺省前景偏移问题。此外，我们提出了一种创新的关注点损失，使用区域特征的空间注意力来约束当前模型关注到最重要的信息。ABR显著降低了先前类型的忘却，同时保持当前类型的高柔性。此外，它与标准图像重复相比有较大的存储需求减少。我们在 Pascal-VOC 和 COCO 数据集上进行了广泛的实验，并支持我们的模型达到了顶尖性能。
</details></li>
</ul>
<hr>
<h2 id="Practical-Commercial-5G-Standalone-SA-Uplink-Throughput-Prediction"><a href="#Practical-Commercial-5G-Standalone-SA-Uplink-Throughput-Prediction" class="headerlink" title="Practical Commercial 5G Standalone (SA) Uplink Throughput Prediction"></a>Practical Commercial 5G Standalone (SA) Uplink Throughput Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12417">http://arxiv.org/abs/2307.12417</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kasidis Arunruangsirilert, Jiro Katto</li>
<li>for: This paper aims to improve uplink throughput prediction in 5G New Radio (NR) networks, specifically in the high-frequency millimeter wave (mmWave) band, to enhance the quality of experience (QoE) for uplink-intensive smartphone applications such as real-time transmission of UHD 4K&#x2F;8K videos and Virtual Reality (VR)&#x2F;Augmented Reality (AR) contents.</li>
<li>methods: The authors propose using a ConvLSTM-based neural network to predict future uplink throughput based on past uplink throughput and RF parameters. The network is trained using real-world drive test data from commercial 5G SA networks while riding commuter trains, and is limited to only use information available via Android API to make it practical for implementation.</li>
<li>results: The authors achieve an average prediction accuracy of 98.9% with an average RMSE of 1.80 Mbps across all unseen evaluation scenarios, demonstrating the effectiveness of their proposed method in predicting uplink throughput in 5G NR networks.Here’s the simplified Chinese text for the three key points:</li>
<li>for: 这篇论文目的是提高5G新 Radio（NR）网络中下行通道吞吐量预测，尤其是在高频毫米波（mmWave）频率带中，以提高智能手机应用程序如实时传输UHD 4K&#x2F;8K视频和虚拟现实（VR）&#x2F;增强现实（AR）内容的质量经验（QoE）。</li>
<li>methods: 作者们提议使用ConvLSTM基于神经网络来预测未来下行通道吞吐量，基于过去下行通道吞吐量和RF参数。网络通过商业5G SA网络的实际驱动测试数据进行训练，并限制模型只使用Android API中可得到的信息，以确保实用性。</li>
<li>results: 作者们在所有未seen评估场景中达到了98.9%的预测精度，RMSE值为1.80 Mbps，这显示了他们提议的方法在5G NR网络中预测下行通道吞吐量的效果。<details>
<summary>Abstract</summary>
While the 5G New Radio (NR) network promises a huge uplift of the uplink throughput, the improvement can only be seen when the User Equipment (UE) is connected to the high-frequency millimeter wave (mmWave) band. With the rise of uplink-intensive smartphone applications such as the real-time transmission of UHD 4K/8K videos, and Virtual Reality (VR)/Augmented Reality (AR) contents, uplink throughput prediction plays a huge role in maximizing the users' quality of experience (QoE). In this paper, we propose using a ConvLSTM-based neural network to predict the future uplink throughput based on past uplink throughput and RF parameters. The network is trained using the data from real-world drive tests on commercial 5G SA networks while riding commuter trains, which accounted for various frequency bands, handover, and blind spots. To make sure our model can be practically implemented, we then limited our model to only use the information available via Android API, then evaluate our model using the data from both commuter trains and other methods of transportation. The results show that our model reaches an average prediction accuracy of 98.9\% with an average RMSE of 1.80 Mbps across all unseen evaluation scenarios.
</details>
<details>
<summary>摘要</summary>
“5G新Radio（NR）网络 promise a huge uplift of the uplink throughput, but the improvement can only be seen when the User Equipment（UE） is connected to the high-frequency millimeter wave（mmWave）band. With the rise of uplink-intensive smartphone applications such as real-time transmission of UHD 4K/8K videos and Virtual Reality（VR）/Augmented Reality（AR）contents, uplink throughput prediction plays a huge role in maximizing the users' quality of experience（QoE）. In this paper, we propose using a ConvLSTM-based neural network to predict the future uplink throughput based on past uplink throughput and RF parameters. The network is trained using the data from real-world drive tests on commercial 5G SA networks while riding commuter trains, which accounted for various frequency bands, handover, and blind spots. To make sure our model can be practically implemented, we then limited our model to only use the information available via Android API, and evaluate our model using the data from both commuter trains and other methods of transportation. The results show that our model reaches an average prediction accuracy of 98.9% with an average RMSE of 1.80 Mbps across all unseen evaluation scenarios.”Here's the translation in Traditional Chinese:“5G新Radio（NR）网络 promised a huge uplift of the uplink throughput, but the improvement can only be seen when the User Equipment（UE） is connected to the high-frequency millimeter wave（mmWave）band. With the rise of uplink-intensive smartphone applications such as real-time transmission of UHD 4K/8K videos and Virtual Reality（VR）/Augmented Reality（AR）contents, uplink throughput prediction plays a huge role in maximizing the users' quality of experience（QoE）. In this paper, we propose using a ConvLSTM-based neural network to predict the future uplink throughput based on past uplink throughput and RF parameters. The network is trained using the data from real-world drive tests on commercial 5G SA networks while riding commuter trains, which accounted for various frequency bands, handover, and blind spots. To make sure our model can be practically implemented, we then limited our model to only use the information available via Android API, and evaluate our model using the data from both commuter trains and other methods of transportation. The results show that our model reaches an average prediction accuracy of 98.9% with an average RMSE of 1.80 Mbps across all unseen evaluation scenarios.”
</details></li>
</ul>
<hr>
<h2 id="A-Machine-Learning-Approach-to-Two-Stage-Adaptive-Robust-Optimization"><a href="#A-Machine-Learning-Approach-to-Two-Stage-Adaptive-Robust-Optimization" class="headerlink" title="A Machine Learning Approach to Two-Stage Adaptive Robust Optimization"></a>A Machine Learning Approach to Two-Stage Adaptive Robust Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12409">http://arxiv.org/abs/2307.12409</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/molyswu/hand_detection">https://github.com/molyswu/hand_detection</a></li>
<li>paper_authors: Dimitris Bertsimas, Cheol Woo Kim</li>
<li>for: 解决两阶段线性适应 robust 优化（ARO）问题，其中变量是二进制的“现在”决策和多面体不确定集。</li>
<li>methods: 基于机器学习的方法，包括编码优化“现在”决策、最差情况相关的优化“现在”决策和等待决策，以及使用列和约束生成算法提取优化策略。</li>
<li>results: 对具有多个相似ARO实例的问题进行预处理，并使用机器学习模型预测高质量策略，解决ARO问题比州先进技术更快、准确。<details>
<summary>Abstract</summary>
We propose an approach based on machine learning to solve two-stage linear adaptive robust optimization (ARO) problems with binary here-and-now variables and polyhedral uncertainty sets. We encode the optimal here-and-now decisions, the worst-case scenarios associated with the optimal here-and-now decisions, and the optimal wait-and-see decisions into what we denote as the strategy. We solve multiple similar ARO instances in advance using the column and constraint generation algorithm and extract the optimal strategies to generate a training set. We train a machine learning model that predicts high-quality strategies for the here-and-now decisions, the worst-case scenarios associated with the optimal here-and-now decisions, and the wait-and-see decisions. We also introduce an algorithm to reduce the number of different target classes the machine learning algorithm needs to be trained on. We apply the proposed approach to the facility location, the multi-item inventory control and the unit commitment problems. Our approach solves ARO problems drastically faster than the state-of-the-art algorithms with high accuracy.
</details>
<details>
<summary>摘要</summary>
We solve multiple similar ARO instances in advance using the column and constraint generation algorithm and extract the optimal strategies to generate a training set. We train a machine learning model that predicts high-quality strategies for the here-and-now decisions, the worst-case scenarios associated with the optimal here-and-now decisions, and the wait-and-see decisions.To reduce the number of different target classes the machine learning algorithm needs to be trained on, we introduce an algorithm. We apply the proposed approach to the facility location, the multi-item inventory control, and the unit commitment problems. Our approach solves ARO problems much faster than the state-of-the-art algorithms with high accuracy.Translated into Simplified Chinese:我们提出一种基于机器学习的方法来解决两阶段线性适应Robust优化（ARO）问题，其中变量为二进制的当下决策和不确定集为多面体。我们将优化的当下决策、最差情况相关的优化当下决策和等待决策编码为策略。我们使用列和约束生成算法解决多个相似的ARO实例，并提取优化策略来生成训练集。我们使用机器学习模型预测高质量的当下决策、最差情况相关的优化当下决策和等待决策。我们还提出了一种算法，以减少机器学习算法需要训练的目标类型数量。我们应用我们的方法解决了设备位置、多项存储控制和生产规划问题。我们的方法可以快速解决ARO问题，高度准确。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Control-of-Multiclass-Fluid-Queueing-Networks-A-Machine-Learning-Approach"><a href="#Optimal-Control-of-Multiclass-Fluid-Queueing-Networks-A-Machine-Learning-Approach" class="headerlink" title="Optimal Control of Multiclass Fluid Queueing Networks: A Machine Learning Approach"></a>Optimal Control of Multiclass Fluid Queueing Networks: A Machine Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12405">http://arxiv.org/abs/2307.12405</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitris Bertsimas, Cheol Woo Kim</li>
<li>for: 本研究旨在提出一种机器学习方法来优化多类流体队列网络（MFQNET）的控制问题，以获得明确和深入的控制策略。</li>
<li>methods: 本研究使用了优化分类树（OCT）和折线拟合（H）来学习MFQNET的控制策略。数据集是通过数值解决MFQNET控制问题而得到的，并通过OCT-H来学习明确的控制策略。</li>
<li>results: 实验结果表明，使用OCT-H学习的控制策略可以在大规模网络中实时应用，并且在33个服务器和99个类型的数据集上达到100%的准确率。尽管在大规模网络中的线上训练可能需要几天的时间，但在线应用仅需毫秒钟。<details>
<summary>Abstract</summary>
We propose a machine learning approach to the optimal control of multiclass fluid queueing networks (MFQNETs) that provides explicit and insightful control policies. We prove that a threshold type optimal policy exists for MFQNET control problems, where the threshold curves are hyperplanes passing through the origin. We use Optimal Classification Trees with hyperplane splits (OCT-H) to learn an optimal control policy for MFQNETs. We use numerical solutions of MFQNET control problems as a training set and apply OCT-H to learn explicit control policies. We report experimental results with up to 33 servers and 99 classes that demonstrate that the learned policies achieve 100\% accuracy on the test set. While the offline training of OCT-H can take days in large networks, the online application takes milliseconds.
</details>
<details>
<summary>摘要</summary>
我们提出一种机器学习方法来优化多类流体队列网络（MFQNET）的控制问题，该方法提供了明确和深入的控制策略。我们证明了MFQNET控制问题中存在一种阈值型优化策略，其阈值曲线为通过原点的hyperplane。我们使用Optimal Classification Trees with hyperplane splits（OCT-H）来学习MFQNET的控制策略。我们使用MFQNET控制问题的数学解为训练集，并应用OCT-H来学习明确的控制策略。我们对33个服务器和99个类的实验结果表明，学习的策略可以在测试集上达到100%的准确率。虽然大规模网络的离线训练可能需要几天的时间，但在线应用仅需毫秒钟。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-aware-Grounded-Action-Transformation-towards-Sim-to-Real-Transfer-for-Traffic-Signal-Control"><a href="#Uncertainty-aware-Grounded-Action-Transformation-towards-Sim-to-Real-Transfer-for-Traffic-Signal-Control" class="headerlink" title="Uncertainty-aware Grounded Action Transformation towards Sim-to-Real Transfer for Traffic Signal Control"></a>Uncertainty-aware Grounded Action Transformation towards Sim-to-Real Transfer for Traffic Signal Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12388">http://arxiv.org/abs/2307.12388</a></li>
<li>repo_url: None</li>
<li>paper_authors: Longchao Da, Hao Mei, Romir Sharma, Hua Wei</li>
<li>for: 提高交通信号控制（TSC）的效率和可靠性，使用人工智能技术来优化交通管理。</li>
<li>methods: 提出了一种基于强化学习（RL）的实际到真实世界（sim-to-real）传输方法，通过动态转换在模拟环境中学习的策略来减少域之间差距。</li>
<li>results: 在模拟交通环境中评估了该方法，并显示其在真实世界中可以大幅提高RL策略的表现。<details>
<summary>Abstract</summary>
Traffic signal control (TSC) is a complex and important task that affects the daily lives of millions of people. Reinforcement Learning (RL) has shown promising results in optimizing traffic signal control, but current RL-based TSC methods are mainly trained in simulation and suffer from the performance gap between simulation and the real world. In this paper, we propose a simulation-to-real-world (sim-to-real) transfer approach called UGAT, which transfers a learned policy trained from a simulated environment to a real-world environment by dynamically transforming actions in the simulation with uncertainty to mitigate the domain gap of transition dynamics. We evaluate our method on a simulated traffic environment and show that it significantly improves the performance of the transferred RL policy in the real world.
</details>
<details>
<summary>摘要</summary>
交通信号控制（TSC）是一项复杂且重要的任务，影响了百万人的日常生活。强化学习（RL）已经显示出优秀的结果，但现有RL基于TSC方法主要在模拟环境中训练，它们受到模拟和真实世界之间的性能差距的影响。在这篇论文中，我们提出了一种从模拟环境到真实世界（sim-to-real）传递方法，称为UGAT，该方法可以将模拟环境中学习的策略在真实世界中转移，并通过动态将模拟环境中的动作转换为不确定性来减少模拟和真实世界之间的领域差距。我们在模拟交通环境中评估了我们的方法，并显示了在真实世界中转移的RL策略的显著性能提高。
</details></li>
</ul>
<hr>
<h2 id="In-Context-Learning-in-Large-Language-Models-Learns-Label-Relationships-but-Is-Not-Conventional-Learning"><a href="#In-Context-Learning-in-Large-Language-Models-Learns-Label-Relationships-but-Is-Not-Conventional-Learning" class="headerlink" title="In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning"></a>In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12375">http://arxiv.org/abs/2307.12375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jannik Kossen, Tom Rainforth, Yarin Gal</li>
<li>for: 本研究旨在探讨Language Model（LLM）在下游任务中的启示学习（ICL）能力如何工作，以及ICL如何影响LLM的预测结果。</li>
<li>methods: 本研究使用了一些现有的LLM模型，并在这些模型中添加了启示示例，以研究ICL的影响。研究还使用了一些不同的预训练任务和启示示例来检验ICL的作用。</li>
<li>results: 研究发现，LLMs通常会在启示示例中包含标签信息时，对输入的预测结果产生改善。然而，研究还发现，ICL的影响并不一定是通过直接学习标签关系来实现的，而是通过在预训练和启示示例之间的交互来实现。此外，研究还发现ICL不一定会考虑所有的启示信息，而是偏好某些特定的启示示例。<details>
<summary>Abstract</summary>
The performance of Large Language Models (LLMs) on downstream tasks often improves significantly when including examples of the input-label relationship in the context. However, there is currently no consensus about how this in-context learning (ICL) ability of LLMs works: for example, while Xie et al. (2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022b) argue ICL does not even learn label relationships from in-context examples. In this paper, we study (1) how labels of in-context examples affect predictions, (2) how label relationships learned during pre-training interact with input-label examples provided in-context, and (3) how ICL aggregates label information across in-context examples. Our findings suggests LLMs usually incorporate information from in-context labels, but that pre-training and in-context label relationships are treated differently, and that the model does not consider all in-context information equally. Our results give insights into understanding and aligning LLM behavior.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在下游任务中的性能通常会显著提高，当包含输入-标签关系的示例在内的上下文中。然而，目前没有一致的观点关于如何在上下文中学习（ICL）能力的LLM工作：例如，XYZ等人（2021）认为ICL类似于通用学习算法，而MIN等人（2022b）则认为ICL不会从内容中学习标签关系。在这篇论文中，我们研究了以下几个问题：（1）如何影响预测的标签，（2）在预训练中学习的标签关系如何与输入-标签示例在内部交互，（3）ICL如何聚合内容中的标签信息。我们发现LLM通常会在内容中包含标签信息，但是预训练和内容中的标签关系是不同的，而且模型不会对所有内容中的信息进行平等考虑。我们的发现可以帮助理解和调整LLM的行为。
</details></li>
</ul>
<hr>
<h2 id="Assessing-Intra-class-Diversity-and-Quality-of-Synthetically-Generated-Images-in-a-Biomedical-and-Non-biomedical-Setting"><a href="#Assessing-Intra-class-Diversity-and-Quality-of-Synthetically-Generated-Images-in-a-Biomedical-and-Non-biomedical-Setting" class="headerlink" title="Assessing Intra-class Diversity and Quality of Synthetically Generated Images in a Biomedical and Non-biomedical Setting"></a>Assessing Intra-class Diversity and Quality of Synthetically Generated Images in a Biomedical and Non-biomedical Setting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02505">http://arxiv.org/abs/2308.02505</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Muneeb Saad, Mubashir Husain Rehmani, Ruairi O’Reilly<br>for: This paper aims to evaluate the effectiveness of using Generative Adversarial Networks (GANs) for data augmentation in biomedical image analysis, specifically in addressing data imbalance issues.methods: The paper uses Multi-scale Structural Similarity Index Measure and Cosine Distance to evaluate intra-class diversity, and Frechet Inception Distance to evaluate the quality of synthetic images.results: The results show that the metrics scores for diversity and quality vary significantly across biomedical-to-biomedical and biomedical-to-non-biomedical imaging modalities, highlighting the importance of considering the specific imaging modality when evaluating the effectiveness of data augmentation methods.<details>
<summary>Abstract</summary>
In biomedical image analysis, data imbalance is common across several imaging modalities. Data augmentation is one of the key solutions in addressing this limitation. Generative Adversarial Networks (GANs) are increasingly being relied upon for data augmentation tasks. Biomedical image features are sensitive to evaluating the efficacy of synthetic images. These features can have a significant impact on metric scores when evaluating synthetic images across different biomedical imaging modalities. Synthetically generated images can be evaluated by comparing the diversity and quality of real images. Multi-scale Structural Similarity Index Measure and Cosine Distance are used to evaluate intra-class diversity, while Frechet Inception Distance is used to evaluate the quality of synthetic images. Assessing these metrics for biomedical and non-biomedical imaging is important to investigate an informed strategy in evaluating the diversity and quality of synthetic images. In this work, an empirical assessment of these metrics is conducted for the Deep Convolutional GAN in a biomedical and non-biomedical setting. The diversity and quality of synthetic images are evaluated using different sample sizes. This research intends to investigate the variance in diversity and quality across biomedical and non-biomedical imaging modalities. Results demonstrate that the metrics scores for diversity and quality vary significantly across biomedical-to-biomedical and biomedical-to-non-biomedical imaging modalities.
</details>
<details>
<summary>摘要</summary>
在生物医学图像分析中，数据偏好是常见的问题，影响着图像分类和识别的精度。生成对抗网络（GANs）在数据增强任务中具有重要作用。生物医学图像特征对评估生成图像的效果非常敏感。这些特征可以对 metric 分数产生很大的影响，并且在不同的生物医学成像Modalities中评估生成图像的时候非常重要。生成的图像可以通过比较真实图像的多样性和质量来评估。用 Multi-scale Structural Similarity Index Measure 和 Cosine Distance 评估内部多样性，而 Frechet Inception Distance 用于评估生成图像的质量。在生物医学和非生物医学成像中评估这些指标非常重要，以了解一种有 informed 的策略来评估生成图像的多样性和质量。本研究通过对 Deep Convolutional GAN 在生物医学和非生物医学 Setting 中进行实验来评估这些指标。结果显示，在生物医学到生物医学和生物医学到非生物医学的交互modalities 中，指标分数差异很大。Note: Please note that the translation is in Simplified Chinese, and the grammar and sentence structure may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="Early-Prediction-of-Alzheimers-Disease-Leveraging-Symptom-Occurrences-from-Longitudinal-Electronic-Health-Records-of-US-Military-Veterans"><a href="#Early-Prediction-of-Alzheimers-Disease-Leveraging-Symptom-Occurrences-from-Longitudinal-Electronic-Health-Records-of-US-Military-Veterans" class="headerlink" title="Early Prediction of Alzheimers Disease Leveraging Symptom Occurrences from Longitudinal Electronic Health Records of US Military Veterans"></a>Early Prediction of Alzheimers Disease Leveraging Symptom Occurrences from Longitudinal Electronic Health Records of US Military Veterans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12369">http://arxiv.org/abs/2307.12369</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rumeng Li, Xun Wang, Dan Berlowitz, Brian Silver, Wen Hu, Heather Keating, Raelene Goodwin, Weisong Liu, Honghuang Lin, Hong Yu<br>for: 这个研究的目的是使用机器学习方法来分析长期电子医疗记录（EHR），以预测阿尔ц海默病（AD）的发病。methods: 这个研究使用了一种 caso-control 设计，使用了从2004年到2021年的美国卫生部VA卫生管理局（VHA）的长期EHR数据。研究使用了一组AD相关的关键词，并对这些关键词的时间发展进行分析，以预测AD发病。results: 研究发现，在AD诊断后，病例的AD相关关键词的发展速度加剧，从约10个到超40个。而控制组的关键词数量则保持在10个。最佳模型在使用数据少于10年的情况下，达到了高度的分类准确率（ROCAUC 0.997）。模型也是良好地准确（Hosmer-Lemeshow好准确性值&#x3D;0.99），并在不同的年龄、性别和种族&#x2F;民族 subgroup 中具有一致性，除了年龄少于65岁的患者（ROCAUC 0.746）。<details>
<summary>Abstract</summary>
Early prediction of Alzheimer's disease (AD) is crucial for timely intervention and treatment. This study aims to use machine learning approaches to analyze longitudinal electronic health records (EHRs) of patients with AD and identify signs and symptoms that can predict AD onset earlier. We used a case-control design with longitudinal EHRs from the U.S. Department of Veterans Affairs Veterans Health Administration (VHA) from 2004 to 2021. Cases were VHA patients with AD diagnosed after 1/1/2016 based on ICD-10-CM codes, matched 1:9 with controls by age, sex and clinical utilization with replacement. We used a panel of AD-related keywords and their occurrences over time in a patient's longitudinal EHRs as predictors for AD prediction with four machine learning models. We performed subgroup analyses by age, sex, and race/ethnicity, and validated the model in a hold-out and "unseen" VHA stations group. Model discrimination, calibration, and other relevant metrics were reported for predictions up to ten years before ICD-based diagnosis. The study population included 16,701 cases and 39,097 matched controls. The average number of AD-related keywords (e.g., "concentration", "speaking") per year increased rapidly for cases as diagnosis approached, from around 10 to over 40, while remaining flat at 10 for controls. The best model achieved high discriminative accuracy (ROCAUC 0.997) for predictions using data from at least ten years before ICD-based diagnoses. The model was well-calibrated (Hosmer-Lemeshow goodness-of-fit p-value = 0.99) and consistent across subgroups of age, sex and race/ethnicity, except for patients younger than 65 (ROCAUC 0.746). Machine learning models using AD-related keywords identified from EHR notes can predict future AD diagnoses, suggesting its potential use for identifying AD risk using EHR notes, offering an affordable way for early screening on large population.
</details>
<details>
<summary>摘要</summary>
The study population included 16,701 cases and 39,097 matched controls, and the average number of AD-related keywords (such as "concentration" and "speaking") per year increased rapidly for cases as diagnosis approached, from around 10 to over 40, while remaining flat at 10 for controls. The best model achieved high discriminative accuracy (ROCAUC 0.997) for predictions using data from at least ten years before ICD-based diagnoses. The model was well-calibrated (Hosmer-Lemeshow goodness-of-fit p-value = 0.99) and consistent across subgroups of age, sex, and race/ethnicity, except for patients younger than 65 (ROCAUC 0.746).Machine learning models using AD-related keywords identified from EHR notes can predict future AD diagnoses, suggesting its potential use for identifying AD risk using EHR notes, offering an affordable way for early screening on a large population.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/07/24/cs.LG_2023_07_24/" data-id="cllsjvzc2001zf588h88he9ex" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/25/eess.IV_2023_07_25/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.IV - 2023-07-25 17:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/24/cs.SD_2023_07_24/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-07-24 123:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'', root:''}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
