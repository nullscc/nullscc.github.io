
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-07-24 17:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Conditional Residual Coding: A Remedy for Bottleneck Problems in Conditional Inter Frame Coding paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.12864 repo_url: None paper_authors: Fabian Brand, Jürgen Seiler, An">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-07-24 17:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/07/24/eess.IV_2023_07_24/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Conditional Residual Coding: A Remedy for Bottleneck Problems in Conditional Inter Frame Coding paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.12864 repo_url: None paper_authors: Fabian Brand, Jürgen Seiler, An">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-23T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:37.502Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_07_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/24/eess.IV_2023_07_24/" class="article-date">
  <time datetime="2023-07-23T16:00:00.000Z" itemprop="datePublished">2023-07-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-07-24 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Conditional-Residual-Coding-A-Remedy-for-Bottleneck-Problems-in-Conditional-Inter-Frame-Coding"><a href="#Conditional-Residual-Coding-A-Remedy-for-Bottleneck-Problems-in-Conditional-Inter-Frame-Coding" class="headerlink" title="Conditional Residual Coding: A Remedy for Bottleneck Problems in Conditional Inter Frame Coding"></a>Conditional Residual Coding: A Remedy for Bottleneck Problems in Conditional Inter Frame Coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12864">http://arxiv.org/abs/2307.12864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabian Brand, Jürgen Seiler, André Kaup</li>
<li>for: 这篇论文是关于新型的视频编码方法，即基于神经网络的压缩方法。</li>
<li>methods: 这篇论文使用的方法是基于神经网络的 conditional coding，它可以在理论上比传统的剩余编码（如 HEVC 或 VVC 等标准）更好。</li>
<li>results: 但是， conditional coding 可能会受到预测信号处理的数据瓶颈，即不能将所有预测信号中的信息传递到重建信号中，从而降低编码器性能。这篇论文提出了 conditional residual coding 概念，它是基于 conditional coding 的信息学性质，可以减少瓶颈的影响，同时保持 conditional coding 的理论性能。<details>
<summary>Abstract</summary>
Conditional coding is a new video coding paradigm enabled by neural-network-based compression. It can be shown that conditional coding is in theory better than the traditional residual coding, which is widely used in video compression standards like HEVC or VVC. However, on closer inspection, it becomes clear that conditional coders can suffer from information bottlenecks in the prediction path, i.e., that due to the data processing inequality not all information from the prediction signal can be passed to the reconstructed signal, thereby impairing the coder performance. In this paper we propose the conditional residual coding concept, which we derive from information theoretical properties of the conditional coder. This coder significantly reduces the influence of bottlenecks, while maintaining the theoretical performance of the conditional coder. We provide a theoretical analysis of the coding paradigm and demonstrate the performance of the conditional residual coder in a practical example. We show that conditional residual coders alleviate the disadvantages of conditional coders while being able to maintain their advantages over residual coders. In the spectrum of residual and conditional coding, we can therefore consider them as ``the best from both worlds''.
</details>
<details>
<summary>摘要</summary>
conditional coding是一种新的视频编码方法，它基于神经网络压缩而实现。理论上来说，conditional coding比传统的差分编码（如HEVC或VVC中的差分编码）更好。然而，在更加仔细的分析中，我们发现conditional coders可能会在预测路径中遇到信息瓶颈，即由数据处理不均衡而导致的信息不能够从预测信号传递到重建信号中，从而影响编码器性能。在这篇论文中，我们提出了conditional residual coding概念，它基于condition coding的信息理论性质。这种编码器可以减少预测路径中的瓶颈影响，同时保持condition coding的理论性能。我们对 coding  парадиг进行了理论分析，并在实践中证明了conditional residual coder的性能。我们发现，conditional residual coders可以消除condition coding的缺点，同时保持它们在差分编码中的优势。因此，在差分和condition coding之间的spectrum中，我们可以视conditional residual coders为“最佳之选”。
</details></li>
</ul>
<hr>
<h2 id="Spatiotemporal-Modeling-Encounters-3D-Medical-Image-Analysis-Slice-Shift-UNet-with-Multi-View-Fusion"><a href="#Spatiotemporal-Modeling-Encounters-3D-Medical-Image-Analysis-Slice-Shift-UNet-with-Multi-View-Fusion" class="headerlink" title="Spatiotemporal Modeling Encounters 3D Medical Image Analysis: Slice-Shift UNet with Multi-View Fusion"></a>Spatiotemporal Modeling Encounters 3D Medical Image Analysis: Slice-Shift UNet with Multi-View Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12853">http://arxiv.org/abs/2307.12853</a></li>
<li>repo_url: None</li>
<li>paper_authors: C. I. Ugwu, S. Casarin, O. Lanz</li>
<li>for:  This paper aims to develop a new 2D-based model for 3D image analysis, which can extract three-dimensional features at the complexity of 2D CNNs.</li>
<li>methods:  The proposed model, called Slice SHift UNet (SSH-UNet), uses multi-view features that are collaboratively learned by performing 2D convolutions along the three orthogonal planes of a volume, and imposing a weights-sharing mechanism. The third dimension is reincorporated by shifting a portion of the feature maps along the slices’ axis.</li>
<li>results:  The effectiveness of the SSH-UNet model is validated in two datasets, Multi-Modality Abdominal Multi-Organ Segmentation (AMOS) and Multi-Atlas Labeling Beyond the Cranial Vault (BTCV), showing that it is more efficient while on par in performance with state-of-the-art architectures.<details>
<summary>Abstract</summary>
As a fundamental part of computational healthcare, Computer Tomography (CT) and Magnetic Resonance Imaging (MRI) provide volumetric data, making the development of algorithms for 3D image analysis a necessity. Despite being computationally cheap, 2D Convolutional Neural Networks can only extract spatial information. In contrast, 3D CNNs can extract three-dimensional features, but they have higher computational costs and latency, which is a limitation for clinical practice that requires fast and efficient models. Inspired by the field of video action recognition we propose a new 2D-based model dubbed Slice SHift UNet (SSH-UNet) which encodes three-dimensional features at 2D CNN's complexity. More precisely multi-view features are collaboratively learned by performing 2D convolutions along the three orthogonal planes of a volume and imposing a weights-sharing mechanism. The third dimension, which is neglected by the 2D convolution, is reincorporated by shifting a portion of the feature maps along the slices' axis. The effectiveness of our approach is validated in Multi-Modality Abdominal Multi-Organ Segmentation (AMOS) and Multi-Atlas Labeling Beyond the Cranial Vault (BTCV) datasets, showing that SSH-UNet is more efficient while on par in performance with state-of-the-art architectures.
</details>
<details>
<summary>摘要</summary>
computational healthcare的基础部分是计算机成像（CT）和核磁共振成像（MRI），它们提供了三维数据，因此开发三维图像分析算法是必要的。然而，两维卷积神经网络（2D CNN）只能提取空间信息，而三维 CNN 可以提取三维特征，但它们具有更高的计算成本和延迟，这限制了临床实践中需要快速和高效的模型。 Drawing inspiration from the field of video action recognition, we propose a new 2D-based model called Slice SHift UNet (SSH-UNet) that encodes three-dimensional features at the complexity of 2D CNNs. Specifically, we perform 2D convolutions along the three orthogonal planes of a volume and share weights to collaboratively learn multi-view features. The third dimension, which is neglected by the 2D convolution, is reincorporated by shifting a portion of the feature maps along the slices' axis. Our approach is validated on the Multi-Modality Abdominal Multi-Organ Segmentation (AMOS) and Multi-Atlas Labeling Beyond the Cranial Vault (BTCV) datasets, showing that SSH-UNet is more efficient while on par in performance with state-of-the-art architectures.
</details></li>
</ul>
<hr>
<h2 id="Multi-View-Vertebra-Localization-and-Identification-from-CT-Images"><a href="#Multi-View-Vertebra-Localization-and-Identification-from-CT-Images" class="headerlink" title="Multi-View Vertebra Localization and Identification from CT Images"></a>Multi-View Vertebra Localization and Identification from CT Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12845">http://arxiv.org/abs/2307.12845</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shanghaitech-impact/multi-view-vertebra-localization-and-identification-from-ct-images">https://github.com/shanghaitech-impact/multi-view-vertebra-localization-and-identification-from-ct-images</a></li>
<li>paper_authors: Han Wu, Jiadong Zhang, Yu Fang, Zhentao Liu, Nizhuan Wang, Zhiming Cui, Dinggang Shen</li>
<li>for: 验证CT图像中vertebra的准确位置和识别</li>
<li>methods: 多视图vertebra本地化和识别，转化3D问题为2D本地化和识别任务，不受3D裁剪覆盖的限制，能够自然地学习多视图全局信息</li>
<li>results: 使用仅两个2D网络，可以准确地位置和识别CT图像中的vertebra，并且在比较状态方法上占据优势<details>
<summary>Abstract</summary>
Accurately localizing and identifying vertebrae from CT images is crucial for various clinical applications. However, most existing efforts are performed on 3D with cropping patch operation, suffering from the large computation costs and limited global information. In this paper, we propose a multi-view vertebra localization and identification from CT images, converting the 3D problem into a 2D localization and identification task on different views. Without the limitation of the 3D cropped patch, our method can learn the multi-view global information naturally. Moreover, to better capture the anatomical structure information from different view perspectives, a multi-view contrastive learning strategy is developed to pre-train the backbone. Additionally, we further propose a Sequence Loss to maintain the sequential structure embedded along the vertebrae. Evaluation results demonstrate that, with only two 2D networks, our method can localize and identify vertebrae in CT images accurately, and outperforms the state-of-the-art methods consistently. Our code is available at https://github.com/ShanghaiTech-IMPACT/Multi-View-Vertebra-Localization-and-Identification-from-CT-Images.
</details>
<details>
<summary>摘要</summary>
精准地在CT图像上本地化和识别脊椎是各种临床应用的关键。然而，现有的大多数努力都是基于3D的割裁补丁操作，它们受到大量计算成本和局部信息的限制。在这篇论文中，我们提出了基于多视图的脊椎本地化和识别方法，将3D问题转化为2D本地化和识别任务。不同于局部补丁的3D方法，我们的方法可以自然地学习多视图的全局信息。此外，为了更好地捕捉不同视角的解剖结构信息，我们还提出了一种多视图对比学习策略来预训练后iony。此外，我们还提出了一种序列损失来维护脊椎中的顺序结构。评估结果表明，只需要两个2D网络，我们的方法可以在CT图像上准确地本地化和识别脊椎，并在比较前一些方法中占据领先地位。我们的代码可以在<https://github.com/ShanghaiTech-IMPACT/Multi-View-Vertebra-Localization-and-Identification-from-CT-Images>中找到。
</details></li>
</ul>
<hr>
<h2 id="Deep-Homography-Prediction-for-Endoscopic-Camera-Motion-Imitation-Learning"><a href="#Deep-Homography-Prediction-for-Endoscopic-Camera-Motion-Imitation-Learning" class="headerlink" title="Deep Homography Prediction for Endoscopic Camera Motion Imitation Learning"></a>Deep Homography Prediction for Endoscopic Camera Motion Imitation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12792">http://arxiv.org/abs/2307.12792</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Huber, Sebastien Ourselin, Christos Bergeles, Tom Vercauteren</li>
<li>for: 这 paper  investigate laparoscopic camera motion automation through imitation learning from retrospective videos of laparoscopic interventions.</li>
<li>methods: 该 paper  introduce a novel method that learns to augment a surgeon’s behavior in image space through object motion invariant image registration via homographies, without making any geometric assumptions or requiring depth information.</li>
<li>results:  compared to two baselines, the proposed method demonstrates significant improvements on the Cholec80 and HeiChole datasets, with an improvement of 47% over camera motion continuation. Additionally, the method is shown to correctly predict camera motion on the public motion classification labels of the AutoLaparo dataset.<details>
<summary>Abstract</summary>
In this work, we investigate laparoscopic camera motion automation through imitation learning from retrospective videos of laparoscopic interventions. A novel method is introduced that learns to augment a surgeon's behavior in image space through object motion invariant image registration via homographies. Contrary to existing approaches, no geometric assumptions are made and no depth information is necessary, enabling immediate translation to a robotic setup. Deviating from the dominant approach in the literature which consist of following a surgical tool, we do not handcraft the objective and no priors are imposed on the surgical scene, allowing the method to discover unbiased policies. In this new research field, significant improvements are demonstrated over two baselines on the Cholec80 and HeiChole datasets, showcasing an improvement of 47% over camera motion continuation. The method is further shown to indeed predict camera motion correctly on the public motion classification labels of the AutoLaparo dataset. All code is made accessible on GitHub.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们 investigate Laparoscopic camera motion automation through imitation learning from retrospective videos of laparoscopic interventions. 我们引入了一种新的方法，该方法可以在图像空间增强外科医生的行为，不需要任何几何假设和深度信息，因此可以直接应用于机器人设置。相比于文献中主流的方法，我们不会手工设置目标，也不会假设外科场景，因此方法可以自由发现不受干扰的策略。在新的研究领域中，我们展示了对Cholec80和HeiChole数据集的显著改进，提高了相对Camera Motion Continuation的47%。此外，我们还证明了该方法可以正确预测 Camera Motion 的公共运动分类标签AutoLaparo数据集。所有代码都可以在GitHub上获取。
</details></li>
</ul>
<hr>
<h2 id="Synthetic-white-balancing-for-intra-operative-hyperspectral-imaging"><a href="#Synthetic-white-balancing-for-intra-operative-hyperspectral-imaging" class="headerlink" title="Synthetic white balancing for intra-operative hyperspectral imaging"></a>Synthetic white balancing for intra-operative hyperspectral imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12791">http://arxiv.org/abs/2307.12791</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anisha Bahl, Conor C. Horgan, Mirek Janatka, Oscar J. MacCormac, Philip Noonan, Yijing Xie, Jianrong Qiu, Nicola Cavalcanti, Philipp Fürnstahl, Michael Ebner, Mads S. Bergholt, Jonathan Shapey, Tom Vercauteren</li>
<li>for: 该研究旨在开发一种可靠的、抗菌的白色参照图像，用于医学应用中的非侵入式规则分辨。</li>
<li>methods: 该研究提出了一种基于视频中的标准静止测量的新参照图像生成算法，使用了纯净的 sintetic reference construction 方法。</li>
<li>results: 研究表明，使用该算法可以实现高度准确的 spectral and color reconstruction，并且在实验中表现良好， median pixel-by-pixel errors 低于 6.5%。<details>
<summary>Abstract</summary>
Hyperspectral imaging shows promise for surgical applications to non-invasively provide spatially-resolved, spectral information. For calibration purposes, a white reference image of a highly-reflective Lambertian surface should be obtained under the same imaging conditions. Standard white references are not sterilizable, and so are unsuitable for surgical environments. We demonstrate the necessity for in situ white references and address this by proposing a novel, sterile, synthetic reference construction algorithm. The use of references obtained at different distances and lighting conditions to the subject were examined. Spectral and color reconstructions were compared with standard measurements qualitatively and quantitatively, using $\Delta E$ and normalised RMSE respectively. The algorithm forms a composite image from a video of a standard sterile ruler, whose imperfect reflectivity is compensated for. The reference is modelled as the product of independent spatial and spectral components, and a scalar factor accounting for gain, exposure, and light intensity. Evaluation of synthetic references against ideal but non-sterile references is performed using the same metrics alongside pixel-by-pixel errors. Finally, intraoperative integration is assessed though cadaveric experiments. Improper white balancing leads to increases in all quantitative and qualitative errors. Synthetic references achieve median pixel-by-pixel errors lower than 6.5% and produce similar reconstructions and errors to an ideal reference. The algorithm integrated well into surgical workflow, achieving median pixel-by-pixel errors of 4.77%, while maintaining good spectral and color reconstruction.
</details>
<details>
<summary>摘要</summary>
高spectral成像显示在手术应用中的推荐作用，可以不侵入性地提供空间分解的spectral信息。为了 calibration purposes，需要获得一张白色参照图像，该图像是一个高度反射的 Lambertian 表面在同一个捕集条件下获得的。标准的白色参照图像不可 sterilizable，因此不适用于手术环境。我们证明了需要场景中的白色参照图像，并提出了一种新的、可靠、Synthetic 参照图像生成算法。我们 исследова了不同距离和照明条件下的参照图像的使用情况，并对spectral和色彩重建进行了质量和量度的比较，使用 $\Delta E$ 和 normalized RMSE 两种指标。该算法生成了一张从标准 sterile 尺Rule 视频中获得的复合图像，其中偏差的反射率被补做。参照图像被模型为独立的空间和spectral 分量的乘积，以及一个映射矩阵 accounting for gain, exposure, and light intensity。我们对Synthetic 参照图像与理想 pero 非 sterile 参照图像进行了比较，并使用相同的指标进行评价。最后，我们通过 cadaveric 实验评估了 intraoperative 集成。不当的白平衡会导致所有量化和质量错误的增加。Synthetic 参照图像实现了 median 像素误差低于 6.5%，并且生成了与理想参照图像相似的重建和误差。该算法在手术工作流中集成 well，实现了 median 像素误差为 4.77%，并保持了好的spectral和色彩重建。
</details></li>
</ul>
<hr>
<h2 id="ICF-SRSR-Invertible-scale-Conditional-Function-for-Self-Supervised-Real-world-Single-Image-Super-Resolution"><a href="#ICF-SRSR-Invertible-scale-Conditional-Function-for-Self-Supervised-Real-world-Single-Image-Super-Resolution" class="headerlink" title="ICF-SRSR: Invertible scale-Conditional Function for Self-Supervised Real-world Single Image Super-Resolution"></a>ICF-SRSR: Invertible scale-Conditional Function for Self-Supervised Real-world Single Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12751">http://arxiv.org/abs/2307.12751</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reyhaneh Neshatavar, Mohsen Yavartanoo, Sanghyun Son, Kyoung Mu Lee</li>
<li>for: 提高单图超分解问题中的精度和实用性，无需使用任何对&#x2F;无对训练数据。</li>
<li>methods: 提出一种归一化可逆函数（ICF），可以扩大输入图像的比例，然后将原始输入图像还原到不同比例条件下。通过这种ICF，我们构建了一种新的自助学习SR框架（ICF-SRSR）。</li>
<li>results: 经验表明，我们的ICF-SRSR可以在无需使用任何对&#x2F;无对训练数据的情况下处理SR任务，并且在实际世界 scenarios中表现出色，与现有的超级vised&#x2F;无对训练方法相当。<details>
<summary>Abstract</summary>
Single image super-resolution (SISR) is a challenging ill-posed problem that aims to up-sample a given low-resolution (LR) image to a high-resolution (HR) counterpart. Due to the difficulty in obtaining real LR-HR training pairs, recent approaches are trained on simulated LR images degraded by simplified down-sampling operators, e.g., bicubic. Such an approach can be problematic in practice because of the large gap between the synthesized and real-world LR images. To alleviate the issue, we propose a novel Invertible scale-Conditional Function (ICF), which can scale an input image and then restore the original input with different scale conditions. By leveraging the proposed ICF, we construct a novel self-supervised SISR framework (ICF-SRSR) to handle the real-world SR task without using any paired/unpaired training data. Furthermore, our ICF-SRSR can generate realistic and feasible LR-HR pairs, which can make existing supervised SISR networks more robust. Extensive experiments demonstrate the effectiveness of the proposed method in handling SISR in a fully self-supervised manner. Our ICF-SRSR demonstrates superior performance compared to the existing methods trained on synthetic paired images in real-world scenarios and exhibits comparable performance compared to state-of-the-art supervised/unsupervised methods on public benchmark datasets.
</details>
<details>
<summary>摘要</summary>
<<SYS>>单张图像超分辨 (SISR) 是一个具有挑战性的缺失定理问题，旨在将给定的低分辨 (LR) 图像提升到高分辨 (HR) 对应的图像。由于获得实际LR-HR训练对的困难，现有的方法通常是通过简化的下采样操作生成模拟LR图像进行训练，如比立方体抠杆。这种方法在实际应用中存在一定的问题，因为生成的模拟LR图像与实际世界中的LR图像之间存在很大的差异。为了解决这问题，我们提出了一种新的归一化Scale-Conditional函数 (ICF)，可以将输入图像扩大，然后使用不同的扩展级别来恢复原始输入图像。通过利用我们提出的ICF，我们建立了一种新的无监督SISR框架 (ICF-SRSR)，可以在实际世界中进行SR任务无需使用任何配偶/非配偶训练数据。此外，我们的ICF-SRSR可以生成真实可行的LR-HR对，这可以使现有的监督SISR网络更加Robust。我们进行了广泛的实验， demonstarted the effectiveness of the proposed method in handling SISR in a fully self-supervised manner. Our ICF-SRSR demonstrates superior performance compared to existing methods trained on synthetic paired images in real-world scenarios and exhibits comparable performance compared to state-of-the-art supervised/unsupervised methods on public benchmark datasets.Note: The translation is done using Google Translate, and some minor adjustments may be needed to make it more idiomatic.
</details></li>
</ul>
<hr>
<h2 id="Dense-Transformer-based-Enhanced-Coding-Network-for-Unsupervised-Metal-Artifact-Reduction"><a href="#Dense-Transformer-based-Enhanced-Coding-Network-for-Unsupervised-Metal-Artifact-Reduction" class="headerlink" title="Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction"></a>Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12717">http://arxiv.org/abs/2307.12717</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wangduo Xie, Matthew B. Blaschko</li>
<li>for: 针对 CT 图像受到金属残余的抑制，提高临床诊断的精度。</li>
<li>methods: 基于 dense transformer 的增强编码网络 (DTEC-Net)，包括层次分解编码器和高级密度处理，以获取长距离匹配的紧密编码序列。然后，提出第二阶段分离方法，改进密集序列的解码过程。</li>
<li>results: 对一个标准测试集进行了广泛的实验和模型讨论，证明 DTEC-Net 的有效性，在对比前一个状态艺术方法时，大幅减少了金属残余，同时保留了更多的纹理细节。<details>
<summary>Abstract</summary>
CT images corrupted by metal artifacts have serious negative effects on clinical diagnosis. Considering the difficulty of collecting paired data with ground truth in clinical settings, unsupervised methods for metal artifact reduction are of high interest. However, it is difficult for previous unsupervised methods to retain structural information from CT images while handling the non-local characteristics of metal artifacts. To address these challenges, we proposed a novel Dense Transformer based Enhanced Coding Network (DTEC-Net) for unsupervised metal artifact reduction. Specifically, we introduce a Hierarchical Disentangling Encoder, supported by the high-order dense process, and transformer to obtain densely encoded sequences with long-range correspondence. Then, we present a second-order disentanglement method to improve the dense sequence's decoding process. Extensive experiments and model discussions illustrate DTEC-Net's effectiveness, which outperforms the previous state-of-the-art methods on a benchmark dataset, and greatly reduces metal artifacts while restoring richer texture details.
</details>
<details>
<summary>摘要</summary>
《CT图像损坏了金属artefacts会产生严重的负面影响于临床诊断。由于在临床设置中采集对照数据的困难，不supervised方法 для降低金属artefacts的抗 corr 感兴趣。然而，previous unsupervised方法难以保留CT图像的结构信息while处理金属artefacts的非本地特性。为了解决这些挑战，我们提出了一种基于Dense Transformer的增强编码网络（DTEC-Net）。specifically，我们提出了一种层次分解编码器，得到高级密度过程支持的密集编码序列，并使用trasnformer来获得长距离匹配。然后，我们提出了第二个分解方法，以改进密集序列的解码过程。广泛的实验和模型讨论表明DTEC-Net的效果，superior于前一个状态的艺术方法，在一个标准数据集上具有更好的降低金属artefacts的能力，同时保留了更加丰富的текxture细节。》Note: Simplified Chinese is used here, as it is the most widely used variety of Chinese in mainland China. However, if you prefer Traditional Chinese, I can provide that version as well.
</details></li>
</ul>
<hr>
<h2 id="Low-complexity-Overfitted-Neural-Image-Codec"><a href="#Low-complexity-Overfitted-Neural-Image-Codec" class="headerlink" title="Low-complexity Overfitted Neural Image Codec"></a>Low-complexity Overfitted Neural Image Codec</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12706">http://arxiv.org/abs/2307.12706</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Orange-OpenSource/Cool-Chic">https://github.com/Orange-OpenSource/Cool-Chic</a></li>
<li>paper_authors: Thomas Leguay, Théo Ladune, Pierrick Philippe, Gordon Clare, Félix Henry</li>
<li>for: 这个论文是为了提出一种减少复杂度的神经网络图像编码器，使得每个输入图像的解码器参数可以被过度适应。</li>
<li>methods: 这种方法使用了过度适应的方法，并且使用了低复杂度的模块和改进的训练过程，以实现更高的编码效率。</li>
<li>results: 这种方法可以与Autoencoder的性能竞争，并且在不同的编码条件下超过HEVC的性能，同时具有较低的复杂度。<details>
<summary>Abstract</summary>
We propose a neural image codec at reduced complexity which overfits the decoder parameters to each input image. While autoencoders perform up to a million multiplications per decoded pixel, the proposed approach only requires 2300 multiplications per pixel. Albeit low-complexity, the method rivals autoencoder performance and surpasses HEVC performance under various coding conditions. Additional lightweight modules and an improved training process provide a 14% rate reduction with respect to previous overfitted codecs, while offering a similar complexity. This work is made open-source at https://orange-opensource.github.io/Cool-Chic/
</details>
<details>
<summary>摘要</summary>
我们提出了一种带有减少复杂度的神经网络图像编码器，其中编码器参数被每个输入图像进行过拟合。而传统的 autoencoder 可能需要多达一百万次乘法运算每个解码像素，我们的方法只需要每个像素进行 2300 次乘法运算。尽管具有低复杂度，我们的方法可以与 autoencoder 的性能竞争，并在不同的编码条件下超越 HEVC 的性能。此外，我们还提出了一些轻量级的模块和改进的训练过程，使得对于先前的过拟合编码器而言，可以实现14%的比例减少，同时保持相似的复杂度。这个工作将在 <https://orange-opensource.github.io/Cool-Chic/> 上发布为开源项目。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Based-Unrolling-for-Reconstruction-and-Super-resolution-of-Single-Photon-Lidar-Systems"><a href="#Bayesian-Based-Unrolling-for-Reconstruction-and-Super-resolution-of-Single-Photon-Lidar-Systems" class="headerlink" title="Bayesian Based Unrolling for Reconstruction and Super-resolution of Single-Photon Lidar Systems"></a>Bayesian Based Unrolling for Reconstruction and Super-resolution of Single-Photon Lidar Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12700">http://arxiv.org/abs/2307.12700</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abderrahim Halimi, Jakeoung Koo, Stephen McLaughlin</li>
<li>for: 本文旨在应对实际应用中3D单 photon Lidar成像遇到的噪声环境和探测器有限分辨率的挑战，通过deep learning算法来进行重建和超分辨。</li>
<li>methods: 本文提出了基于 Bayesian 模型的深度学习算法，可以利用统计和学习两种不同的框架之优点，提供最佳估计和改善网络可读性。与现有学习算法相比，提议的架构具有更少的可训练参数、更高的噪声和系统响应函数模型化缺陷的Robustness，并提供了更详细的估计信息，包括uncertainty度量。</li>
<li>results: 对于 synthetic 和实际数据，提议的算法可以与现有算法相比，提供类似的推断质量和计算复杂度，同时具有更好的网络解释性。<details>
<summary>Abstract</summary>
Deploying 3D single-photon Lidar imaging in real world applications faces several challenges due to imaging in high noise environments and with sensors having limited resolution. This paper presents a deep learning algorithm based on unrolling a Bayesian model for the reconstruction and super-resolution of 3D single-photon Lidar. The resulting algorithm benefits from the advantages of both statistical and learning based frameworks, providing best estimates with improved network interpretability. Compared to existing learning-based solutions, the proposed architecture requires a reduced number of trainable parameters, is more robust to noise and mismodelling of the system impulse response function, and provides richer information about the estimates including uncertainty measures. Results on synthetic and real data show competitive results regarding the quality of the inference and computational complexity when compared to state-of-the-art algorithms. This short paper is based on contributions published in [1] and [2].
</details>
<details>
<summary>摘要</summary>
在实际应用中部署3D单 photon探测影像遇到许多挑战，主要是因为探测环境噪声高，探测器分辨率有限。本文提出了基于深度学习算法的bayesian模型卷积 reconstruction和超分辨3D单 photon探测。该算法利用了统计和学习框架的优点，提供了最佳估计，同时具有改进的网络解释性。与现有的学习基于算法相比，提议的架构需要较少的可训练参数，更加鲁棒对噪和系统响应函数的误差模型，并提供了更多的估计信息，包括不确定度测量。synthetic和实际数据的结果表明，与当前状态的算法相比，提议的算法可以达到竞争力的结果质量和计算复杂度。这篇短文基于[1]和[2]的贡献。
</details></li>
</ul>
<hr>
<h2 id="Automatic-lobe-segmentation-using-attentive-cross-entropy-and-end-to-end-fissure-generation"><a href="#Automatic-lobe-segmentation-using-attentive-cross-entropy-and-end-to-end-fissure-generation" class="headerlink" title="Automatic lobe segmentation using attentive cross entropy and end-to-end fissure generation"></a>Automatic lobe segmentation using attentive cross entropy and end-to-end fissure generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12634">http://arxiv.org/abs/2307.12634</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/htytewx/softcam">https://github.com/htytewx/softcam</a></li>
<li>paper_authors: Qi Su, Na Wang, Jiawen Xie, Yinan Chen, Xiaofan Zhang</li>
<li>for:  automatic lung lobe segmentation algorithm for the diagnosis and treatment of lung diseases</li>
<li>methods:  task-specific loss function to pay attention to the area around the pulmonary fissure, end-to-end pulmonary fissure generation method, registration-based loss function to alleviate convergence difficulty</li>
<li>results:  achieved 97.83% and 94.75% dice scores on private dataset STLB and public LUNA16 dataset respectively<details>
<summary>Abstract</summary>
The automatic lung lobe segmentation algorithm is of great significance for the diagnosis and treatment of lung diseases, however, which has great challenges due to the incompleteness of pulmonary fissures in lung CT images and the large variability of pathological features. Therefore, we propose a new automatic lung lobe segmentation framework, in which we urge the model to pay attention to the area around the pulmonary fissure during the training process, which is realized by a task-specific loss function. In addition, we introduce an end-to-end pulmonary fissure generation method in the auxiliary pulmonary fissure segmentation task, without any additional network branch. Finally, we propose a registration-based loss function to alleviate the convergence difficulty of the Dice loss supervised pulmonary fissure segmentation task. We achieve 97.83% and 94.75% dice scores on our private dataset STLB and public LUNA16 dataset respectively.
</details>
<details>
<summary>摘要</summary>
“自动lung lobe分 segmentation算法具有诊断和治疗肺病的重要意义，但受到肺CT图像中肺裂的不完整性和病理特征的大幅度变化所困扰。因此，我们提出了一个新的自动lung lobe分 segmentation框架，其中我们强调模型在训练过程中对肺裂附近区域的注意。此外，我们引入了一个终端到终端的肺裂生成方法，不需要额外的网络分支。最后，我们提出了一个注册基准的损失函数，以解决由Dice损失函数监督的肺裂分 segmentation任务中的对准问题。我们在私人数据集STLB和公开的LUNA16数据集上实现了97.83%和94.75%的Dice分数。”Note that Simplified Chinese is used here, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Sparse-annotation-strategies-for-segmentation-of-short-axis-cardiac-MRI"><a href="#Sparse-annotation-strategies-for-segmentation-of-short-axis-cardiac-MRI" class="headerlink" title="Sparse annotation strategies for segmentation of short axis cardiac MRI"></a>Sparse annotation strategies for segmentation of short axis cardiac MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12619">http://arxiv.org/abs/2307.12619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Josh Stein, Maxime Di Folco, Julia Schnabel<br>for:这种研究旨在调查使用少量标注数据（48个标注量）和少量slice标注数据（每个案例）来达到高精度心脏MRI分割结果。methods:研究使用了state-of-the-art nnU-Net模型，并运用了减少数据量和减少slice标注数据的方法来降低标注量。results:研究发现，使用更少的slice标注数据可以达到高精度分割结果，而使用更多的volume标注数据并不一定能够提高分割结果。此外，中部区域的slice标注更有价值，而腹部区域的slice标注最差。因此，对于心脏MRI分割任务，更好的方法是尽量标注多个slice而不是标注更多的volume。<details>
<summary>Abstract</summary>
Short axis cardiac MRI segmentation is a well-researched topic, with excellent results achieved by state-of-the-art models in a supervised setting. However, annotating MRI volumes is time-consuming and expensive. Many different approaches (e.g. transfer learning, data augmentation, few-shot learning, etc.) have emerged in an effort to use fewer annotated data and still achieve similar performance as a fully supervised model. Nevertheless, to the best of our knowledge, none of these works focus on which slices of MRI volumes are most important to annotate for yielding the best segmentation results. In this paper, we investigate the effects of training with sparse volumes, i.e. reducing the number of cases annotated, and sparse annotations, i.e. reducing the number of slices annotated per case. We evaluate the segmentation performance using the state-of-the-art nnU-Net model on two public datasets to identify which slices are the most important to annotate. We have shown that training on a significantly reduced dataset (48 annotated volumes) can give a Dice score greater than 0.85 and results comparable to using the full dataset (160 and 240 volumes for each dataset respectively). In general, training on more slice annotations provides more valuable information compared to training on more volumes. Further, annotating slices from the middle of volumes yields the most beneficial results in terms of segmentation performance, and the apical region the worst. When evaluating the trade-off between annotating volumes against slices, annotating as many slices as possible instead of annotating more volumes is a better strategy.
</details>
<details>
<summary>摘要</summary>
短轴心脏MRI分割是已经有很多研究的话题，现有的最新模型在监督环境下已经 дости得了极佳的结果。然而，annotating MRI卷积是时间consuming和昂贵的。许多不同的方法（例如传播学习、数据增强、几据学习等）在努力使用更少的annotated data并 still achieve相似的性能，但是，到目前为止，none of these works focus on which slices of MRI卷积是最重要的annotate，以获得最好的分割结果。在这篇论文中，我们investigate the effects of training with sparse volumes和sparse annotations，specifically, reducing the number of cases annotated and the number of slices annotated per case。我们使用了state-of-the-art nnU-Net模型在两个公共数据集上评估分割性能，以找出最重要的slice要annotate。我们发现，使用仅具有48个annotated volume的dataset可以获得Dice分数大于0.85，并且和使用完整的dataset（160和240个volume for each dataset）的结果相似。总体而言，training on more slice annotations provides more valuable information compared to training on more volumes。此外， annotating slices from the middle of volumes yields the most beneficial results in terms of segmentation performance, while the apical region is the worst. When evaluating the trade-off between annotating volumes against slices, annotating as many slices as possible instead of annotating more volumes is a better strategy.
</details></li>
</ul>
<hr>
<h2 id="Attribute-Regularized-Soft-Introspective-VAE-Towards-Cardiac-Attribute-Regularization-Through-MRI-Domains"><a href="#Attribute-Regularized-Soft-Introspective-VAE-Towards-Cardiac-Attribute-Regularization-Through-MRI-Domains" class="headerlink" title="Attribute Regularized Soft Introspective VAE: Towards Cardiac Attribute Regularization Through MRI Domains"></a>Attribute Regularized Soft Introspective VAE: Towards Cardiac Attribute Regularization Through MRI Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12618">http://arxiv.org/abs/2307.12618</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maxime Di Folco, Cosmin Bercea, Julia A. Schnabel<br>for:* 这个论文主要目标是提高深度生成模型的控制性，使其能够更好地捕捉隐藏的特征。methods:* 该论文提出了一种叫做Attributed Soft Introspective VAE（Attri-SIVAE）的新方法，它将 attribute  regularized loss  incorporated into Soft-Intro VAE 框架中。results:* 实验表明，该方法在不同的 MRI 数据集上都能够达到类似的重建和Regularization性能，与 state-of-the-art Attributed regularized VAE 相比，该方法还能够在不同的数据集上保持同样的Regularization水平。<details>
<summary>Abstract</summary>
Deep generative models have emerged as influential instruments for data generation and manipulation. Enhancing the controllability of these models by selectively modifying data attributes has been a recent focus. Variational Autoencoders (VAEs) have shown promise in capturing hidden attributes but often produce blurry reconstructions. Controlling these attributes through different imaging domains is difficult in medical imaging. Recently, Soft Introspective VAE leverage the benefits of both VAEs and Generative Adversarial Networks (GANs), which have demonstrated impressive image synthesis capabilities, by incorporating an adversarial loss into VAE training. In this work, we propose the Attributed Soft Introspective VAE (Attri-SIVAE) by incorporating an attribute regularized loss, into the Soft-Intro VAE framework. We evaluate experimentally the proposed method on cardiac MRI data from different domains, such as various scanner vendors and acquisition centers. The proposed method achieves similar performance in terms of reconstruction and regularization compared to the state-of-the-art Attributed regularized VAE but additionally also succeeds in keeping the same regularization level when tested on a different dataset, unlike the compared method.
</details>
<details>
<summary>摘要</summary>
深度生成模型已成为数据生成和修改的重要工具。提高这些模型的可控性，通过选择性地修改数据属性，是最近的焦点。变量自动编码器（VAEs）表现出在捕捉隐藏属性的潜力，但经常生成模糊的重建。在医学成像中，控制这些属性通过不同的成像频谱是困难的。最近，软 introspective VAE（Soft-Intro VAE）利用了 VAEs 和生成对抗网络（GANs）的优点，通过在 VAE 训练中添加对抗损失来产生出色的图像 sintesis 能力。在这项工作中，我们提出了具有属性规范损失的 Attributed Soft Introspective VAE（Attri-SIVAE）。我们通过实验评估了提议的方法在不同频谱的各种心脏 MRI 数据上。提议的方法与 state-of-the-art 带有属性规范损失的 VAE 相当，但同时也能够在不同的数据集上保持同样的规范化水平，与比较方法不同。
</details></li>
</ul>
<hr>
<h2 id="AMaizeD-An-End-to-End-Pipeline-for-Automatic-Maize-Disease-Detection"><a href="#AMaizeD-An-End-to-End-Pipeline-for-Automatic-Maize-Disease-Detection" class="headerlink" title="AMaizeD: An End to End Pipeline for Automatic Maize Disease Detection"></a>AMaizeD: An End to End Pipeline for Automatic Maize Disease Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03766">http://arxiv.org/abs/2308.03766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anish Mall, Sanchit Kabra, Ankur Lhila, Pawan Ajmera</li>
<li>for: 这个研究论文旨在提供一个自动化的丰盈麦病识别框架，用于早期发现农场麦谷植物病变。</li>
<li>methods: 本研究使用多spectral imaging技术，combined with deep learning algorithms和segmenation techniques，以提取植物病变特征。</li>
<li>results: 实验结果显示，本框架能够实时检测多种麦病变，包括粉刺菌、anthracnose和叶萎病。<details>
<summary>Abstract</summary>
This research paper presents AMaizeD: An End to End Pipeline for Automatic Maize Disease Detection, an automated framework for early detection of diseases in maize crops using multispectral imagery obtained from drones. A custom hand-collected dataset focusing specifically on maize crops was meticulously gathered by expert researchers and agronomists. The dataset encompasses a diverse range of maize varieties, cultivation practices, and environmental conditions, capturing various stages of maize growth and disease progression. By leveraging multispectral imagery, the framework benefits from improved spectral resolution and increased sensitivity to subtle changes in plant health. The proposed framework employs a combination of convolutional neural networks (CNNs) as feature extractors and segmentation techniques to identify both the maize plants and their associated diseases. Experimental results demonstrate the effectiveness of the framework in detecting a range of maize diseases, including powdery mildew, anthracnose, and leaf blight. The framework achieves state-of-the-art performance on the custom hand-collected dataset and contributes to the field of automated disease detection in agriculture, offering a practical solution for early identification of diseases in maize crops advanced machine learning techniques and deep learning architectures.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Development-Of-Automated-Cardiac-Arrhythmia-Detection-Methods-Using-Single-Channel-ECG-Signal"><a href="#Development-Of-Automated-Cardiac-Arrhythmia-Detection-Methods-Using-Single-Channel-ECG-Signal" class="headerlink" title="Development Of Automated Cardiac Arrhythmia Detection Methods Using Single Channel ECG Signal"></a>Development Of Automated Cardiac Arrhythmia Detection Methods Using Single Channel ECG Signal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02405">http://arxiv.org/abs/2308.02405</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arpita Paul, Avik Kumar Das, Manas Rakshit, Ankita Ray Chowdhury, Susmita Saha, Hrishin Roy, Sajal Sarkar, Dongiri Prasanth, Eravelli Saicharan</li>
<li>For: 这个研究旨在开发一个基于单通道电子心脏ogram (ECG) 信号的多类型过节损检测算法，以减少对心脏疾病的死亡。* Methods: 本研究使用心跳时间径特征、形态特征和wavelet几何特征，联合使用Statistical、Entropy和能量基于的特征，并应用机器学习基于Random Forest的分类器。* Results: 使用HRV和时域形态特征，取得85.11%的精度、85.11%的敏感性、85.07%的精确性和85.00%的F1分数，使用HRV和wavelet几何特征，取得90.91%的精度、90.91%的敏感性、90.96%的精确性和90.87%的F1分数。<details>
<summary>Abstract</summary>
Arrhythmia, an abnormal cardiac rhythm, is one of the most common types of cardiac disease. Automatic detection and classification of arrhythmia can be significant in reducing deaths due to cardiac diseases. This work proposes a multi-class arrhythmia detection algorithm using single channel electrocardiogram (ECG) signal. In this work, heart rate variability (HRV) along with morphological features and wavelet coefficient features are utilized for detection of 9 classes of arrhythmia. Statistical, entropy and energy-based features are extracted and applied to machine learning based random forest classifiers. Data used in both works is taken from 4 broad databases (CPSC and CPSC extra, PTB-XL, G12EC and Chapman-Shaoxing and Ningbo Database) made available by Physionet. With HRV and time domain morphological features, an average accuracy of 85.11%, sensitivity of 85.11%, precision of 85.07% and F1 score of 85.00% is obtained whereas with HRV and wavelet coefficient features, the performance obtained is 90.91% accuracy, 90.91% sensitivity, 90.96% precision and 90.87% F1 score. The detailed analysis of simulation results affirms that the presented scheme effectively detects broad categories of arrhythmia from single-channel ECG records. In the last part of the work, the proposed classification schemes are implemented on hardware using Raspberry Pi for real time ECG signal classification.
</details>
<details>
<summary>摘要</summary>
心Rate变化（HRV）和形态特征以及wavelet幂特征的组合使用，可以有效地检测单通道ECG信号中的9类cardiac arrhythmia。在这种方法中，使用Physionet提供的4个广泛数据库（CPSC和CPSC extra、PTB-XL、G12EC和Chapman-Shaoxing和Ningbo数据库）的数据，通过机器学习基于Random Forest的分类器进行检测。在实验结果的详细分析中，我们发现，提出的方案可以准确地从单通道ECG记录中检测出广泛的cardiac arrhythmia类型。最后，我们在实时ECG信号分类中实现了提出的分类方案，使用Raspberry Pi硬件实现。Here's the translation of the text into Traditional Chinese:心率变化（HRV）和形态特征以及wavelet幂特征的组合使用，可以有效地检测单通道ECG信号中的9类cardiac arrhythmia。在这种方法中，使用Physionet提供的4个广泛数据库（CPSC和CPSC extra、PTB-XL、G12EC和Chapman-Shaoxing和Ningbo数据库）的数据，通过机器学习基于Random Forest的分类器进行检测。在实验结果的详细分析中，我们发现，提出的方案可以准确地从单通道ECG记录中检测出广泛的cardiac arrhythmia类型。最后，我们在实时ECG信号分类中实现了提出的分类方案，使用Raspberry Pi硬件实现。
</details></li>
</ul>
<hr>
<h2 id="4D-Feet-Registering-Walking-Foot-Shapes-Using-Attention-Enhanced-Dynamic-Synchronized-Graph-Convolutional-LSTM-Network"><a href="#4D-Feet-Registering-Walking-Foot-Shapes-Using-Attention-Enhanced-Dynamic-Synchronized-Graph-Convolutional-LSTM-Network" class="headerlink" title="4D Feet: Registering Walking Foot Shapes Using Attention Enhanced Dynamic-Synchronized Graph Convolutional LSTM Network"></a>4D Feet: Registering Walking Foot Shapes Using Attention Enhanced Dynamic-Synchronized Graph Convolutional LSTM Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12377">http://arxiv.org/abs/2307.12377</a></li>
<li>repo_url: None</li>
<li>paper_authors: Farzam Tajdari, Toon Huysmans, Xinhe Yao, Jun Xu, Yu Song<br>for:这个论文的目的是为了研究4D扫描技术，帮助研究人员更好地理解人体动态变形的特征。methods:这个论文使用的方法包括非固定iterative closest-farthest points算法来找到和对准不同摄像头捕捉的动态特征，以及一种新型的ADGC-LSTM网络来同步不同摄像头捕捉的扫描结果。另外，论文还使用非固定注册方法将同步的扫描结果与时间线相匹配。results:论文的结果表明，提出的框架能够有效地同步不同摄像头捕捉的4D扫描结果，并且能够生成高质量的3D矫正模型。在使用新开发的4D脚部扫描仪时，论文创造了首个开放数据集，名为4D脚部（15 fps），包括58名参与者的右和左脚的4D形状（共116个脚，包括5147帧3D帧），覆盖了走姿周期中重要的阶段。<details>
<summary>Abstract</summary>
4D scans of dynamic deformable human body parts help researchers have a better understanding of spatiotemporal features. However, reconstructing 4D scans based on multiple asynchronous cameras encounters two main challenges: 1) finding the dynamic correspondences among different frames captured by each camera at the timestamps of the camera in terms of dynamic feature recognition, and 2) reconstructing 3D shapes from the combined point clouds captured by different cameras at asynchronous timestamps in terms of multi-view fusion. In this paper, we introduce a generic framework that is able to 1) find and align dynamic features in the 3D scans captured by each camera using the nonrigid iterative closest-farthest points algorithm; 2) synchronize scans captured by asynchronous cameras through a novel ADGC-LSTM-based network, which is capable of aligning 3D scans captured by different cameras to the timeline of a specific camera; and 3) register a high-quality template to synchronized scans at each timestamp to form a high-quality 3D mesh model using a non-rigid registration method. With a newly developed 4D foot scanner, we validate the framework and create the first open-access data-set, namely the 4D feet. It includes 4D shapes (15 fps) of the right and left feet of 58 participants (116 feet in total, including 5147 3D frames), covering significant phases of the gait cycle. The results demonstrate the effectiveness of the proposed framework, especially in synchronizing asynchronous 4D scans using the proposed ADGC-LSTM network.
</details>
<details>
<summary>摘要</summary>
4D扫描技术为研究人体动态可变部分带来了更好的理解空间特征。然而，基于多个异步相机拍摄的4D扫描重建又存在两大挑战：1）在不同相机的时间戳上找到动态相关性，并在不同相机的时间戳上匹配动态特征；2）通过多视图融合来重建3D形状。在本文中，我们提出了一个通用的框架，可以1）使用非定剑迭代最近最远点对算法来找到和对准3D扫描中的动态特征；2）使用一种新型的ADGC-LSTM网络将不同相机拍摄的4D扫描同步到特定相机的时间轴上；3）使用非定剑注册方法将同步化后的4D扫描与高质量模板进行匹配，以生成高质量3D mesh模型。我们采用了一种新开发的4D脚部扫描仪，并验证了该框架。我们创建了首个开放数据集，即4D脚。该数据集包括15帧/秒的4D形状（包括右脚和左脚），共计116个脚，包括5147帧的3D扫描数据，覆盖了人 gaits 周期中的重要阶段。结果表明提出的框架具有较高的效果，特别是在同步异步4D扫描中使用我们提出的ADGC-LSTM网络。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/24/eess.IV_2023_07_24/" data-id="cllsjm799008fbp888wupd3xp" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/24/eess.AS_2023_07_24/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.AS - 2023-07-24 22:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/23/cs.LG_2023_07_23/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-07-23 18:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      // $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
