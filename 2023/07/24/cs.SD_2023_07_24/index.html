
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-07-24 123:00:00 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="An objective evaluation of Hearing Aids and DNN-based speech enhancement in complex acoustic scenes paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.12888 repo_url: https:&#x2F;&#x2F;github.com&#x2F;enricguso&#x2F;guso_waspaa23 pape">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-07-24 123:00:00">
<meta property="og:url" content="http://example.com/2023/07/24/cs.SD_2023_07_24/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="An objective evaluation of Hearing Aids and DNN-based speech enhancement in complex acoustic scenes paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.12888 repo_url: https:&#x2F;&#x2F;github.com&#x2F;enricguso&#x2F;guso_waspaa23 pape">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-23T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:37.443Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_07_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/24/cs.SD_2023_07_24/" class="article-date">
  <time datetime="2023-07-23T16:00:00.000Z" itemprop="datePublished">2023-07-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-07-24 123:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="An-objective-evaluation-of-Hearing-Aids-and-DNN-based-speech-enhancement-in-complex-acoustic-scenes"><a href="#An-objective-evaluation-of-Hearing-Aids-and-DNN-based-speech-enhancement-in-complex-acoustic-scenes" class="headerlink" title="An objective evaluation of Hearing Aids and DNN-based speech enhancement in complex acoustic scenes"></a>An objective evaluation of Hearing Aids and DNN-based speech enhancement in complex acoustic scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12888">http://arxiv.org/abs/2307.12888</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/enricguso/guso_waspaa23">https://github.com/enricguso/guso_waspaa23</a></li>
<li>paper_authors: Enric Gusó, Joanna Luberadzka, Martí Baig, Umut Sayin Saraç, Xavier Serra</li>
<li>for: 本研究 Comparing the objective performance of five high-end commercially available Hearing Aid (HA) devices to DNN-based speech enhancement algorithms in complex acoustic environments.</li>
<li>methods: 我们使用了一种单个HA设备的HRTF测量来 sinthezier a binaural dataset for training two state-of-the-art causal and non-causal DNN enhancement models. We then generate an evaluation set of realistic speech-in-noise situations using an Ambisonics loudspeaker setup and record with a KU100 dummy head wearing each of the HA devices, both with and without the conventional HA algorithms, applying the DNN enhancers to the latter.</li>
<li>results: 我们发现，使用DNN增强 Algorithm outperforms the HA algorithms in terms of noise suppression and objective intelligibility metrics.<details>
<summary>Abstract</summary>
We investigate the objective performance of five high-end commercially available Hearing Aid (HA) devices compared to DNN-based speech enhancement algorithms in complex acoustic environments. To this end, we measure the HRTFs of a single HA device to synthesize a binaural dataset for training two state-of-the-art causal and non-causal DNN enhancement models. We then generate an evaluation set of realistic speech-in-noise situations using an Ambisonics loudspeaker setup and record with a KU100 dummy head wearing each of the HA devices, both with and without the conventional HA algorithms, applying the DNN enhancers to the latter. We find that the DNN-based enhancement outperforms the HA algorithms in terms of noise suppression and objective intelligibility metrics.
</details>
<details>
<summary>摘要</summary>
我团队 investigate了五种高级商业听力器（HA）设备的目标性能，与基于深度学习（DNN）的语音提高算法在复杂的噪音环境中进行比较。为此，我们测量了一个单个HA设备的Head-Related Transfer Functions（HRTFs），以生成一个听力器数据集，用于训练两种当前领先的 causal 和 non-causal DNN 提高模型。然后，我们使用 Ambisonics 喇叭设置生成一个评估集，并通过一个 KU100 假人头 wear 每个HA设备，包括不使用常见HA算法和使用 DNN 提高算法。我们发现，DNN 基于的提高算法在噪音抑制和对话智能度指标方面都超过HA算法。
</details></li>
</ul>
<hr>
<h2 id="Joint-speech-and-overlap-detection-a-benchmark-over-multiple-audio-setup-and-speech-domains"><a href="#Joint-speech-and-overlap-detection-a-benchmark-over-multiple-audio-setup-and-speech-domains" class="headerlink" title="Joint speech and overlap detection: a benchmark over multiple audio setup and speech domains"></a>Joint speech and overlap detection: a benchmark over multiple audio setup and speech domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13012">http://arxiv.org/abs/2307.13012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Lebourdais, Théo Mariotte, Marie Tahon, Anthony Larcher, Antoine Laurent, Silvio Montresor, Sylvain Meignier, Jean-Hugh Thomas</li>
<li>for: 本研究旨在提供一个完整的综合 benchmark，以评估不同的语音分类和过lapped speech检测（VAD和OSD）模型在不同的音频设定和语音领域中的表现。</li>
<li>methods: 本研究使用了一个2&#x2F;3-class模型，融合了时间卷积网络和适应到设定的语音表现，实现了VAD和OSD的 JOINT 训练。</li>
<li>results: 研究结果显示，这个新的架构可以在不同的语音领域中实现高度的精度和稳定性，并且比预先train的两个专门的 VAD 和 OSD 系统更好。此外，这个模型还可以在单道和多道音频处理中进行应用。<details>
<summary>Abstract</summary>
Voice activity and overlapped speech detection (respectively VAD and OSD) are key pre-processing tasks for speaker diarization. The final segmentation performance highly relies on the robustness of these sub-tasks. Recent studies have shown VAD and OSD can be trained jointly using a multi-class classification model. However, these works are often restricted to a specific speech domain, lacking information about the generalization capacities of the systems. This paper proposes a complete and new benchmark of different VAD and OSD models, on multiple audio setups (single/multi-channel) and speech domains (e.g. media, meeting...). Our 2/3-class systems, which combine a Temporal Convolutional Network with speech representations adapted to the setup, outperform state-of-the-art results. We show that the joint training of these two tasks offers similar performances in terms of F1-score to two dedicated VAD and OSD systems while reducing the training cost. This unique architecture can also be used for single and multichannel speech processing.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用简化中文语音活动和重叠说话检测（简称VAD和OSD）是 speaker 分类前置处理的关键任务。最终 segmentation 性能强度取决于这两个子任务的稳定性。 recent studies 表明 VAD 和 OSD 可以通过多类分类模型进行同时训练。然而，这些研究通常受到特定的语音频道的限制，lacking 信息 about the generalization capacities of the systems。 this paper proposes a complete and new benchmark of different VAD and OSD models, on multiple audio setups (single/multi-channel) and speech domains (e.g. media, meeting...). our 2/3-class systems, which combine a Temporal Convolutional Network with speech representations adapted to the setup, outperform state-of-the-art results. we show that the joint training of these two tasks offers similar performances in terms of F1-score to two dedicated VAD and OSD systems while reducing the training cost. this unique architecture can also be used for single and multichannel speech processing.Note: "<<SYS>>" is used to indicate that the text is in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Integration-of-Frame-and-Label-synchronous-Beam-Search-for-Streaming-Encoder-decoder-Speech-Recognition"><a href="#Integration-of-Frame-and-Label-synchronous-Beam-Search-for-Streaming-Encoder-decoder-Speech-Recognition" class="headerlink" title="Integration of Frame- and Label-synchronous Beam Search for Streaming Encoder-decoder Speech Recognition"></a>Integration of Frame- and Label-synchronous Beam Search for Streaming Encoder-decoder Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12767">http://arxiv.org/abs/2307.12767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emiru Tsunoo, Hayato Futami, Yosuke Kashiwagi, Siddhant Arora, Shinji Watanabe</li>
<li>for: 这个论文是为了提高自动语音识别的效率和稳定性而写的。</li>
<li>methods: 这个论文使用了帧基模型（如CTC和泛化器）和标签基模型（如标签注意力编码器）的组合，以实现同时使用帧和标签的同步decoding。</li>
<li>results: 实验结果显示，提出的搜索算法可以比其他搜索方法优化error rate，并且具有对于非预期情况的稳定性。<details>
<summary>Abstract</summary>
Although frame-based models, such as CTC and transducers, have an affinity for streaming automatic speech recognition, their decoding uses no future knowledge, which could lead to incorrect pruning. Conversely, label-based attention encoder-decoder mitigates this issue using soft attention to the input, while it tends to overestimate labels biased towards its training domain, unlike CTC. We exploit these complementary attributes and propose to integrate the frame- and label-synchronous (F-/L-Sync) decoding alternately performed within a single beam-search scheme. F-Sync decoding leads the decoding for block-wise processing, while L-Sync decoding provides the prioritized hypotheses using look-ahead future frames within a block. We maintain the hypotheses from both decoding methods to perform effective pruning. Experiments demonstrate that the proposed search algorithm achieves lower error rates compared to the other search methods, while being robust against out-of-domain situations.
</details>
<details>
<summary>摘要</summary>
尽管框架基模型，如 CTC 和泛音器，与流动自动语音识别有着天生的联系，但它们的解码使用无Future知识，这可能会导致错误的剪裁。相反，标签基于注意力Encoder-Decoder 可以解决这个问题，使用软注意力来输入，但它可能会偏向其训练领域的标签，不同于 CTC。我们利用这些 complementary attributes，并提议将帧和标签同步（F-/L-Sync）解码 alternately 在单个搜索算法中进行。F-Sync 解码在块级处理中领先，而 L-Sync 解码在块内提供了以后的未来帧的优先假设。我们保留了两种解码方法的假设，以实现有效的剪裁。实验表明，我们提议的搜索算法可以比其他搜索方法低于错误率，并对于Out-of-domain 情况 Displaying  robust。
</details></li>
</ul>
<hr>
<h2 id="Code-Switched-Urdu-ASR-for-Noisy-Telephonic-Environment-using-Data-Centric-Approach-with-Hybrid-HMM-and-CNN-TDNN"><a href="#Code-Switched-Urdu-ASR-for-Noisy-Telephonic-Environment-using-Data-Centric-Approach-with-Hybrid-HMM-and-CNN-TDNN" class="headerlink" title="Code-Switched Urdu ASR for Noisy Telephonic Environment using Data Centric Approach with Hybrid HMM and CNN-TDNN"></a>Code-Switched Urdu ASR for Noisy Telephonic Environment using Data Centric Approach with Hybrid HMM and CNN-TDNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12759">http://arxiv.org/abs/2307.12759</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sage-khan/code-switched-noisy-urdu-asr">https://github.com/sage-khan/code-switched-noisy-urdu-asr</a></li>
<li>paper_authors: Muhammad Danyal Khan, Raheem Ali, Arshad Aziz</li>
<li>for: 本研究旨在开发一个资源有效的自动语音识别（ASR）系统，用于快速搜索特定上下文和内容的电话记录，提高服务质量（QoS）和感知分析。</li>
<li>methods: 本研究使用链式混合HMM和CNN-TDNN模型来解决code-switched Urdu语言的ASR问题。利用混合HMM-DNN方法可以利用神经网络的优点，而无需大量标注数据。另外，通过添加CNN和TDNN，可以更好地处理噪音环境，提高准确率。</li>
<li>results: 研究结果表明，使用链式混合HMM和CNN-TDNN模型可以在噪音环境下实现5.2%的Word Error Rate（WER），包括隔离单词或数字、以及连续自由语言。<details>
<summary>Abstract</summary>
Call Centers have huge amount of audio data which can be used for achieving valuable business insights and transcription of phone calls is manually tedious task. An effective Automated Speech Recognition system can accurately transcribe these calls for easy search through call history for specific context and content allowing automatic call monitoring, improving QoS through keyword search and sentiment analysis. ASR for Call Center requires more robustness as telephonic environment are generally noisy. Moreover, there are many low-resourced languages that are on verge of extinction which can be preserved with help of Automatic Speech Recognition Technology. Urdu is the $10^{th}$ most widely spoken language in the world, with 231,295,440 worldwide still remains a resource constrained language in ASR. Regional call-center conversations operate in local language, with a mix of English numbers and technical terms generally causing a "code-switching" problem. Hence, this paper describes an implementation framework of a resource efficient Automatic Speech Recognition/ Speech to Text System in a noisy call-center environment using Chain Hybrid HMM and CNN-TDNN for Code-Switched Urdu Language. Using Hybrid HMM-DNN approach allowed us to utilize the advantages of Neural Network with less labelled data. Adding CNN with TDNN has shown to work better in noisy environment due to CNN's additional frequency dimension which captures extra information from noisy speech, thus improving accuracy. We collected data from various open sources and labelled some of the unlabelled data after analysing its general context and content from Urdu language as well as from commonly used words from other languages, primarily English and were able to achieve WER of 5.2% with noisy as well as clean environment in isolated words or numbers as well as in continuous spontaneous speech.
</details>
<details>
<summary>摘要</summary>
营业厅有庞大的音频数据，可以用于获得有价值的商业智能和电话会议的手动识别是一项耗时的任务。一个有效的自动语音识别系统可以准确地译本会议，以便搜索通过电话历史记录的特定上下文和内容，实现自动监控电话，提高客户满意度（QoS）通过关键词搜索和情感分析。为营业厅的自动语音识别系统，需要更加鲁棒，因为电话环境通常吵闹。此外，有许多资源受限的语言在濒临灭绝，可以通过自动语音识别技术来保存。约旦语是全球第10大常用语言，有231295440名世界各地的人使用，但它仍然是资源受限的语言在自动语音识别领域。地方客服对话通常在本地语言中进行，混合英语数字和技术术语，引起了"代码交换"问题。因此，本文描述了一个资源有效的自动语音识别/语音到文本系统在吵闹营业厅环境中使用链式混合HMM和CNN-TDNN来解决Code-Switched约旦语言的问题。通过混合HMM-DNN方法，我们可以利用神经网络的优势，不需要大量的标注数据。将CNN与TDNN结合使用，在吵闹环境中提高了准确性，因为CNN的额外频率维度捕捉了吵闹speech中的额外信息。我们从多个开源资源中收集数据，并对一些未标注的数据进行分析，以确定其总体上下文和内容的通用语言和英语等其他语言中的通用词汇，并达到了5.2%的WER，包括干扰和清晰环境下的隔离单词或数字以及连续自然语言。
</details></li>
</ul>
<hr>
<h2 id="IteraTTA-An-interface-for-exploring-both-text-prompts-and-audio-priors-in-generating-music-with-text-to-audio-models"><a href="#IteraTTA-An-interface-for-exploring-both-text-prompts-and-audio-priors-in-generating-music-with-text-to-audio-models" class="headerlink" title="IteraTTA: An interface for exploring both text prompts and audio priors in generating music with text-to-audio models"></a>IteraTTA: An interface for exploring both text prompts and audio priors in generating music with text-to-audio models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13005">http://arxiv.org/abs/2307.13005</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hiromu Yakura, Masataka Goto</li>
<li>for: 帮助用户自由生成音乐声音，即使他们没有音乐知识，可以通过不同的文本提示来尝试生成声音。</li>
<li>methods: 我们使用文本到声音生成技术，并提供了一个特制的界面IteraTTA，以帮助用户逐渐实现他们的抽象目标，同时理解和探索可能的结果空间。</li>
<li>results: 我们的实现和讨论探讨了对文本到声音模型的交互技术的特殊设计需求，以及如何通过交互技术提高模型的效iveness。<details>
<summary>Abstract</summary>
Recent text-to-audio generation techniques have the potential to allow novice users to freely generate music audio. Even if they do not have musical knowledge, such as about chord progressions and instruments, users can try various text prompts to generate audio. However, compared to the image domain, gaining a clear understanding of the space of possible music audios is difficult because users cannot listen to the variations of the generated audios simultaneously. We therefore facilitate users in exploring not only text prompts but also audio priors that constrain the text-to-audio music generation process. This dual-sided exploration enables users to discern the impact of different text prompts and audio priors on the generation results through iterative comparison of them. Our developed interface, IteraTTA, is specifically designed to aid users in refining text prompts and selecting favorable audio priors from the generated audios. With this, users can progressively reach their loosely-specified goals while understanding and exploring the space of possible results. Our implementation and discussions highlight design considerations that are specifically required for text-to-audio models and how interaction techniques can contribute to their effectiveness.
</details>
<details>
<summary>摘要</summary>
现代文本到音频生成技术允许新手 Users可以自由生成音频。即使他们没有音乐知识，如和声进程和乐器，用户可以尝试不同的文本提示来生成音频。然而，与图像领域相比，了解音乐频谱中可能的音频空间是困难的，因为用户无法同时听到生成的音频变化。为了帮助用户探索不同的文本提示和音频先前，我们提供了一个特有的界面，IteraTTA，帮助用户细化文本提示和选择生成音频中的有利先前。通过这种双重探索，用户可以逐步实现自己的模糊化目标，同时理解和探索可能的结果空间。我们的实现和讨论描述了特定于文本到音频模型的设计考虑事项，以及如何通过互动技术来提高其效iveness。
</details></li>
</ul>
<hr>
<h2 id="A-Model-for-Every-User-and-Budget-Label-Free-and-Personalized-Mixed-Precision-Quantization"><a href="#A-Model-for-Every-User-and-Budget-Label-Free-and-Personalized-Mixed-Precision-Quantization" class="headerlink" title="A Model for Every User and Budget: Label-Free and Personalized Mixed-Precision Quantization"></a>A Model for Every User and Budget: Label-Free and Personalized Mixed-Precision Quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12659">http://arxiv.org/abs/2307.12659</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edward Fish, Umberto Michieli, Mete Ozay</li>
<li>for: 这个研究是为了提高自动语音识别（ASR）系统的可行性和效率，并且降低模型的大小以适应移动设备。</li>
<li>methods: 这个研究使用了一种名为myQASR的混合精度优化方法，这个方法可以根据不同的用户和任务需求，生成特定的混合精度优化方案，以适应不同的内存预算。</li>
<li>results: 研究结果显示，myQASR可以对大规模的ASR模型进行优化，并且可以根据特定的用户和语言等因素，生成对应的优化方案，以提高特定用户群的识别率。<details>
<summary>Abstract</summary>
Recent advancement in Automatic Speech Recognition (ASR) has produced large AI models, which become impractical for deployment in mobile devices. Model quantization is effective to produce compressed general-purpose models, however such models may only be deployed to a restricted sub-domain of interest. We show that ASR models can be personalized during quantization while relying on just a small set of unlabelled samples from the target domain. To this end, we propose myQASR, a mixed-precision quantization method that generates tailored quantization schemes for diverse users under any memory requirement with no fine-tuning. myQASR automatically evaluates the quantization sensitivity of network layers by analysing the full-precision activation values. We are then able to generate a personalised mixed-precision quantization scheme for any pre-determined memory budget. Results for large-scale ASR models show how myQASR improves performance for specific genders, languages, and speakers.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Robust-Automatic-Speech-Recognition-via-WavAugment-Guided-Phoneme-Adversarial-Training"><a href="#Robust-Automatic-Speech-Recognition-via-WavAugment-Guided-Phoneme-Adversarial-Training" class="headerlink" title="Robust Automatic Speech Recognition via WavAugment Guided Phoneme Adversarial Training"></a>Robust Automatic Speech Recognition via WavAugment Guided Phoneme Adversarial Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12498">http://arxiv.org/abs/2307.12498</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/WAPATASR/WAPAT">https://github.com/WAPATASR/WAPAT</a></li>
<li>paper_authors: Gege Qi, Yuefeng Chen, Xiaofeng Mao, Xiaojun Jia, Ranjie Duan, Rong Zhang, Hui Xue</li>
<li>for: 提高自动语音识别（ASR）模型的实际 robustness，使其不仅维持清晰样本的性能，而且在小量干扰和大域转移下保持一致的效果。</li>
<li>methods: 提出一种新的WavAugment导向phoneme adversarial Training（wapat）方法，使用phoneme空间中的对抗示例作为增强元素，使模型具有较小的phoneme表示变化的敏感性，并且通过对增强样本的phoneme表示进行导航，找到更加稳定和多样化的梯度方向，从而提高模型的泛化能力。</li>
<li>results: 在End-to-end Speech Challenge Benchmark（ESB）上进行了广泛的实验，结果表明，使用wapat方法可以提高SpeechLM-wapat模型的性能，相比原始模型，WER减少6.28%，达到新的州态艺。<details>
<summary>Abstract</summary>
Developing a practically-robust automatic speech recognition (ASR) is challenging since the model should not only maintain the original performance on clean samples, but also achieve consistent efficacy under small volume perturbations and large domain shifts. To address this problem, we propose a novel WavAugment Guided Phoneme Adversarial Training (wapat). wapat use adversarial examples in phoneme space as augmentation to make the model invariant to minor fluctuations in phoneme representation and preserve the performance on clean samples. In addition, wapat utilizes the phoneme representation of augmented samples to guide the generation of adversaries, which helps to find more stable and diverse gradient-directions, resulting in improved generalization. Extensive experiments demonstrate the effectiveness of wapat on End-to-end Speech Challenge Benchmark (ESB). Notably, SpeechLM-wapat outperforms the original model by 6.28% WER reduction on ESB, achieving the new state-of-the-art.
</details>
<details>
<summary>摘要</summary>
开发一个实际上Robust自动语音识别（ASR）模型是挑战，因为模型不仅需要保持干净样本的原始性能，还需要在小量干扰和大频率变化下保持一致性。为解决这问题，我们提出了一种新的WavAugment引导的phoneme adversarialtraining（wapat）方法。wapat使用phoneme空间中的对抗示例作为增强项，使模型在phoneme表示下变得不敏感，并保持干净样本的性能。此外，wapat使用增强后的phoneme表示来引导对抗生成，从而找到更稳定和多元的梯度方向，从而提高泛化。经验表明，wapat在End-to-end Speech Challenge Benchmark（ESB）上具有显著的效果，SpeechLM-wapat比原始模型减少了6.28%的WER，达到新的领先地位。
</details></li>
</ul>
<hr>
<h2 id="SCRAPS-Speech-Contrastive-Representations-of-Acoustic-and-Phonetic-Spaces"><a href="#SCRAPS-Speech-Contrastive-Representations-of-Acoustic-and-Phonetic-Spaces" class="headerlink" title="SCRAPS: Speech Contrastive Representations of Acoustic and Phonetic Spaces"></a>SCRAPS: Speech Contrastive Representations of Acoustic and Phonetic Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12445">http://arxiv.org/abs/2307.12445</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ivan Vallés-Pérez, Grzegorz Beringer, Piotr Bilinski, Gary Cook, Roberto Barra-Chicote</li>
<li>for: 这篇论文旨在探讨CLIP模型在语音领域中学习共同表征空间的可能性，以提高语音识别和生成的性能。</li>
<li>methods: 作者使用CLIP模型，通过将图像和文本描述相关的共同表征空间学习，实现了静止和动态特征之间的共同表征。</li>
<li>results: 研究发现，当替换20%的音节时，模型的分数下降91%，而在混合75%的高斯噪声时，模型的性能下降只有10%。此外，研究还证明了这些嵌入是用于识别和生成语音的下游应用中非常有用。<details>
<summary>Abstract</summary>
Numerous examples in the literature proved that deep learning models have the ability to work well with multimodal data. Recently, CLIP has enabled deep learning systems to learn shared latent spaces between images and text descriptions, with outstanding zero- or few-shot results in downstream tasks. In this paper we explore the same idea proposed by CLIP but applied to the speech domain, where the phonetic and acoustic spaces usually coexist. We train a CLIP-based model with the aim to learn shared representations of phonetic and acoustic spaces. The results show that the proposed model is sensible to phonetic changes, with a 91% of score drops when replacing 20% of the phonemes at random, while providing substantial robustness against different kinds of noise, with a 10% performance drop when mixing the audio with 75% of Gaussian noise. We also provide empirical evidence showing that the resulting embeddings are useful for a variety of downstream applications, such as intelligibility evaluation and the ability to leverage rich pre-trained phonetic embeddings in speech generation task. Finally, we discuss potential applications with interesting implications for the speech generation and recognition fields.
</details>
<details>
<summary>摘要</summary>
多种例子在文献中证明深度学习模型可以处理多modal数据。最近，CLIP使得深度学习系统可以学习图像和文本描述之间共享的射频空间，得到了 Zero-shot 或几个shot结果在下游任务中。在这篇论文中，我们探索了同样的想法，但应用于语音频域，其中的语音和听音空间通常共存。我们使用 CLIP 模型，以学习语音和听音空间的共享表示。结果表明，我们的模型具有较高的敏感性，对于phoneme的改变，当替换20%的phoneme时，得分下降91%。同时，我们的模型具有较高的鲁棒性，对于不同类型的噪音，只有10%的性能下降，当混合75%的高斯噪音。我们还提供了实验证据，表明结果的嵌入可以用于多种下游应用，如语音可读性评估和使用Rich预训练的phonetic嵌入进行语音生成任务。最后，我们讨论了应用的潜在应用，具有对语音生成和识别领域的 interessing 后果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/07/24/cs.SD_2023_07_24/" data-id="cllsk9gqq004k9c88euldbvjt" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/24/cs.LG_2023_07_24/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-07-24 18:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/24/eess.AS_2023_07_24/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.AS - 2023-07-24 22:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'', root:''}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
