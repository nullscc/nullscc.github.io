
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-07-09 17:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Ultrasonic Image’s Annotation Removal: A Self-supervised Noise2Noise Approach paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.04133 repo_url: https:&#x2F;&#x2F;github.com&#x2F;grandarth&#x2F;ultrasonicimage-n2n-approach paper_autho">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-07-09 17:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/07/09/eess.IV_2023_07_09/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Ultrasonic Image’s Annotation Removal: A Self-supervised Noise2Noise Approach paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.04133 repo_url: https:&#x2F;&#x2F;github.com&#x2F;grandarth&#x2F;ultrasonicimage-n2n-approach paper_autho">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-08T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:28.460Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_07_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/09/eess.IV_2023_07_09/" class="article-date">
  <time datetime="2023-07-08T16:00:00.000Z" itemprop="datePublished">2023-07-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-07-09 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Ultrasonic-Image’s-Annotation-Removal-A-Self-supervised-Noise2Noise-Approach"><a href="#Ultrasonic-Image’s-Annotation-Removal-A-Self-supervised-Noise2Noise-Approach" class="headerlink" title="Ultrasonic Image’s Annotation Removal: A Self-supervised Noise2Noise Approach"></a>Ultrasonic Image’s Annotation Removal: A Self-supervised Noise2Noise Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04133">http://arxiv.org/abs/2307.04133</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/grandarth/ultrasonicimage-n2n-approach">https://github.com/grandarth/ultrasonicimage-n2n-approach</a></li>
<li>paper_authors: Yuanheng Zhang, Nan Jiang, Zhaoheng Xie, Junying Cao, Yueyang Teng</li>
<li>for: 提高医疗报告质量的高级医疗图像标注数据集的自动化处理。</li>
<li>methods: 使用噪声作为预 Text task，使用Noise2Noise scheme进行模型训练，以恢复图像到干净状态。</li>
<li>results: 对不同类型的标注数据进行测试，大多数基于Noise2Noise scheme的模型在噪声恢复任务中表现出色，特别是使用自定义U-Net结构在Body marker标注数据集上得到了最佳效果，具有高精度和高重建相似性。<details>
<summary>Abstract</summary>
Accurately annotated ultrasonic images are vital components of a high-quality medical report. Hospitals often have strict guidelines on the types of annotations that should appear on imaging results. However, manually inspecting these images can be a cumbersome task. While a neural network could potentially automate the process, training such a model typically requires a dataset of paired input and target images, which in turn involves significant human labour. This study introduces an automated approach for detecting annotations in images. This is achieved by treating the annotations as noise, creating a self-supervised pretext task and using a model trained under the Noise2Noise scheme to restore the image to a clean state. We tested a variety of model structures on the denoising task against different types of annotation, including body marker annotation, radial line annotation, etc. Our results demonstrate that most models trained under the Noise2Noise scheme outperformed their counterparts trained with noisy-clean data pairs. The costumed U-Net yielded the most optimal outcome on the body marker annotation dataset, with high scores on segmentation precision and reconstruction similarity. We released our code at https://github.com/GrandArth/UltrasonicImage-N2N-Approach.
</details>
<details>
<summary>摘要</summary>
高品质医疗报告中的准确标注图像是关键组成部分。医院通常有严格的图像标注规范，但手动检查这些图像可以是一项繁琐的任务。使用神经网络自动化这个过程可能是一个解决方案，但是训练这种模型通常需要一个包含输入和目标图像的 paired 数据集，这需要很多人工劳动。本研究提出了一种自动化图像标注检测方法。这是通过将标注视为噪声，创建一种自我超vised pretext task，并使用基于 Noise2Noise 方案训练的模型来还原图像为一个清晰的状态来实现的。我们测试了不同类型的标注，包括体 markers 标注和径向线标注等，并评估了不同模型结构的性能。我们的结果表明，大多数基于 Noise2Noise 方案训练的模型在杂噪clean 数据对比下表现出色，并且用于体 markers 标注集上的 customized U-Net 得到了最佳的结果，其segmentation精度和重建相似度均达到了高水平。我们的代码可以在 <https://github.com/GrandArth/UltrasonicImage-N2N-Approach> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Low-Light-Images-Using-Infrared-Encoded-Images"><a href="#Enhancing-Low-Light-Images-Using-Infrared-Encoded-Images" class="headerlink" title="Enhancing Low-Light Images Using Infrared-Encoded Images"></a>Enhancing Low-Light Images Using Infrared-Encoded Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04122">http://arxiv.org/abs/2307.04122</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wyf0912/ELIEI">https://github.com/wyf0912/ELIEI</a></li>
<li>paper_authors: Shulin Tian, Yufei Wang, Renjie Wan, Wenhan Yang, Alex C. Kot, Bihan Wen</li>
<li>for: 提高低光照图像的可见度和细节表示</li>
<li>methods: 移除卡口内的折射光镜Filter，使用更多的照明信息从近infraredpectrum中获取更高的信噪比</li>
<li>results: 对比 referencetest dataset，提出的方法能够更好地提高低光照图像的可见度和细节表示，并且量化和 каче地比较好<details>
<summary>Abstract</summary>
Low-light image enhancement task is essential yet challenging as it is ill-posed intrinsically. Previous arts mainly focus on the low-light images captured in the visible spectrum using pixel-wise loss, which limits the capacity of recovering the brightness, contrast, and texture details due to the small number of income photons. In this work, we propose a novel approach to increase the visibility of images captured under low-light environments by removing the in-camera infrared (IR) cut-off filter, which allows for the capture of more photons and results in improved signal-to-noise ratio due to the inclusion of information from the IR spectrum. To verify the proposed strategy, we collect a paired dataset of low-light images captured without the IR cut-off filter, with corresponding long-exposure reference images with an external filter. The experimental results on the proposed dataset demonstrate the effectiveness of the proposed method, showing better performance quantitatively and qualitatively. The dataset and code are publicly available at https://wyf0912.github.io/ELIEI/
</details>
<details>
<summary>摘要</summary>
低光照图像提升任务是必备又挑战的，因为它是一个内在不定的问题。先前的艺术主要通过像素精度损失来处理低光照图像，这限制了恢复图像的亮度、对比度和细节的能力，因为可得到的光子数很少。在这个工作中，我们提议一种新的方法，利用取消相机内置的红外（IR）剔除filter，以获取更多的光子数，从而提高信号噪声比。为验证提议的策略，我们收集了一个对应的低光照图像集和长曝光参照图像集，使用外部滤镜获取。实验结果表明，提议的方法具有更好的数量和质量性能。数据集和代码在https://wyf0912.github.io/ELIEI/上公开。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Building-Semantic-Segmentation-Accuracy-with-Super-Resolution-and-Deep-Learning-Investigating-the-Impact-of-Spatial-Resolution-on-Various-Datasets"><a href="#Enhancing-Building-Semantic-Segmentation-Accuracy-with-Super-Resolution-and-Deep-Learning-Investigating-the-Impact-of-Spatial-Resolution-on-Various-Datasets" class="headerlink" title="Enhancing Building Semantic Segmentation Accuracy with Super Resolution and Deep Learning: Investigating the Impact of Spatial Resolution on Various Datasets"></a>Enhancing Building Semantic Segmentation Accuracy with Super Resolution and Deep Learning: Investigating the Impact of Spatial Resolution on Various Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04101">http://arxiv.org/abs/2307.04101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiling Guo, Xiaodan Shi, Haoran Zhang, Dou Huang, Xiaoya Song, Jinyue Yan, Ryosuke Shibasaki</li>
<li>for: 本研究旨在探讨深度学习基于远程感知技术的建筑 semantics 分割中 spatial resolution 的影响。</li>
<li>methods: 本研究使用了 super-resolution 和 down-sampling 技术将 remote sensing 图像转化为多个空间分辨率，然后选择了 UNet 和 FPN 两种深度学习模型进行训练和测试。</li>
<li>results: 实验结果显示，建筑 semantics 分割结果受到空间分辨率的影响，并且在约 0.3m 的空间分辨率下达到最佳成本效果。<details>
<summary>Abstract</summary>
The development of remote sensing and deep learning techniques has enabled building semantic segmentation with high accuracy and efficiency. Despite their success in different tasks, the discussions on the impact of spatial resolution on deep learning based building semantic segmentation are quite inadequate, which makes choosing a higher cost-effective data source a big challenge. To address the issue mentioned above, in this study, we create remote sensing images among three study areas into multiple spatial resolutions by super-resolution and down-sampling. After that, two representative deep learning architectures: UNet and FPN, are selected for model training and testing. The experimental results obtained from three cities with two deep learning models indicate that the spatial resolution greatly influences building segmentation results, and with a better cost-effectiveness around 0.3m, which we believe will be an important insight for data selection and preparation.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "The development of remote sensing and deep learning techniques has enabled building semantic segmentation with high accuracy and efficiency. Despite their success in different tasks, the discussions on the impact of spatial resolution on deep learning based building semantic segmentation are quite inadequate, which makes choosing a higher cost-effective data source a big challenge. To address the issue mentioned above, in this study, we create remote sensing images among three study areas into multiple spatial resolutions by super-resolution and down-sampling. After that, two representative deep learning architectures: UNet and FPN, are selected for model training and testing. The experimental results obtained from three cities with two deep learning models indicate that the spatial resolution greatly influences building segmentation results, and with a better cost-effectiveness around 0.3m, which we believe will be an important insight for data selection and preparation." into Chinese (Simplified)📝发展远程感知和深度学习技术，使得建筑 semantic segmentation 的准确率和效率得到了大幅提高。然而，关于深度学习基于建筑 semantic segmentation 的空间分辨率影响的讨论，尚不充分，这使得选择更高效果和经济的数据源成为一大挑战。为解决上述问题，本研究中将Remote sensing 图像在三个研究区域中进行多个空间分辨率的创建，通过超分辨和降采样。然后，选择了两种代表性的深度学习架构：UNet 和 FPN。通过三座城市的两个深度学习模型的实验结果显示，空间分辨率对建筑 segmentation 结果产生了极大的影响，并且在约0.3米的成本效果上表现较好。我们认为这将成为数据选择和准备的重要意见。
</details></li>
</ul>
<hr>
<h2 id="Combining-transmission-speckle-photography-and-convolutional-neural-network-for-determination-of-fat-content-in-cow’s-milk-–-an-exercise-in-classification-of-parameters-of-a-complex-suspension"><a href="#Combining-transmission-speckle-photography-and-convolutional-neural-network-for-determination-of-fat-content-in-cow’s-milk-–-an-exercise-in-classification-of-parameters-of-a-complex-suspension" class="headerlink" title="Combining transmission speckle photography and convolutional neural network for determination of fat content in cow’s milk – an exercise in classification of parameters of a complex suspension"></a>Combining transmission speckle photography and convolutional neural network for determination of fat content in cow’s milk – an exercise in classification of parameters of a complex suspension</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15069">http://arxiv.org/abs/2307.15069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kwasi Nyandey, Daniel Jakubczyk</li>
<li>for: direct classification and recognition of milk fat content classes</li>
<li>methods: combined transmission speckle photography and machine learning</li>
<li>results: unambiguous recognition of milk fat content classes with high accuracy (100% and ~99%)<details>
<summary>Abstract</summary>
We have combined transmission speckle photography and machine learning for direct classification and recognition of milk fat content classes. Our aim was hinged on the fact that parameters of scattering particles (and the dispersion medium) can be linked to the intensity distribution (speckle) observed when coherent light is transmitted through a scattering medium. For milk, it is primarily the size distribution and concentration of fat globules, which constitutes the total fat content. Consequently, we trained convolutional neural network to recognise and classify laser speckle from different fat content classes (0.5, 1.5, 2.0 and 3.2%). We investigated four exposure-time protocols and obtained the highest performance for shorter exposure times, in which the intensity histograms are kept similar for all images and the most probable intensity in the speckle pattern is close to zero. Our neural network was able to recognize the milk fat content classes unambiguously and we obtained the highest test and independent classification accuracies of 100 and ~99% respectively. It indicates that the parameters of other complex realistic suspensions could be classified with similar methods.
</details>
<details>
<summary>摘要</summary>
我们将传输扫描光学和机器学习结合，以直接分类和识别牛奶脂肪含量类别。我们的目标是利用散射体粒子（以及散射媒体）的参数与扫描光通过散射媒体所观察到的INTENSITY分布（扫描光斑）之间存在关系。在牛奶中，主要是脂肪球体大小分布和总脂肪含量。因此，我们将 convolutional neural network 训练来识别和分类不同脂肪含量类别（0.5%, 1.5%, 2.0%和3.2%）。我们研究了四种曝光时间协议，并获得了最高性能，其中曝光时间较短，图像INTENSITY频谱均匀，最 probable intensity 在扫描光斑模式中接近零。我们的神经网络能够不 ambiguously 识别牛奶脂肪含量类别，并在测试和独立分类准确率达100%和~99%。这表明可以使用类似方法来分类其他复杂的真实涂敷。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/09/eess.IV_2023_07_09/" data-id="cllta0lj0007fny88c7mvhjer" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/09/eess.AS_2023_07_09/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.AS - 2023-07-09 22:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/08/cs.LG_2023_07_08/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-07-08 18:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
