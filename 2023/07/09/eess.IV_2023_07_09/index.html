
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-07-09 17:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Ultrasonic Imageâ€™s Annotation Removal: A Self-supervised Noise2Noise Approach paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.04133 repo_url: https:&#x2F;&#x2F;github.com&#x2F;grandarth&#x2F;ultrasonicimage-n2n-approach paper_autho">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-07-09 17:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/07/09/eess.IV_2023_07_09/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Ultrasonic Imageâ€™s Annotation Removal: A Self-supervised Noise2Noise Approach paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.04133 repo_url: https:&#x2F;&#x2F;github.com&#x2F;grandarth&#x2F;ultrasonicimage-n2n-approach paper_autho">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-08T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:28.460Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_07_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/09/eess.IV_2023_07_09/" class="article-date">
  <time datetime="2023-07-08T16:00:00.000Z" itemprop="datePublished">2023-07-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-07-09 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Ultrasonic-Imageâ€™s-Annotation-Removal-A-Self-supervised-Noise2Noise-Approach"><a href="#Ultrasonic-Imageâ€™s-Annotation-Removal-A-Self-supervised-Noise2Noise-Approach" class="headerlink" title="Ultrasonic Imageâ€™s Annotation Removal: A Self-supervised Noise2Noise Approach"></a>Ultrasonic Imageâ€™s Annotation Removal: A Self-supervised Noise2Noise Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04133">http://arxiv.org/abs/2307.04133</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/grandarth/ultrasonicimage-n2n-approach">https://github.com/grandarth/ultrasonicimage-n2n-approach</a></li>
<li>paper_authors: Yuanheng Zhang, Nan Jiang, Zhaoheng Xie, Junying Cao, Yueyang Teng</li>
<li>for: æé«˜åŒ»ç–—æŠ¥å‘Šè´¨é‡çš„é«˜çº§åŒ»ç–—å›¾åƒæ ‡æ³¨æ•°æ®é›†çš„è‡ªåŠ¨åŒ–å¤„ç†ã€‚</li>
<li>methods: ä½¿ç”¨å™ªå£°ä½œä¸ºé¢„ Text taskï¼Œä½¿ç”¨Noise2Noise schemeè¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œä»¥æ¢å¤å›¾åƒåˆ°å¹²å‡€çŠ¶æ€ã€‚</li>
<li>results: å¯¹ä¸åŒç±»å‹çš„æ ‡æ³¨æ•°æ®è¿›è¡Œæµ‹è¯•ï¼Œå¤§å¤šæ•°åŸºäºNoise2Noise schemeçš„æ¨¡å‹åœ¨å™ªå£°æ¢å¤ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨è‡ªå®šä¹‰U-Netç»“æ„åœ¨Body markeræ ‡æ³¨æ•°æ®é›†ä¸Šå¾—åˆ°äº†æœ€ä½³æ•ˆæœï¼Œå…·æœ‰é«˜ç²¾åº¦å’Œé«˜é‡å»ºç›¸ä¼¼æ€§ã€‚<details>
<summary>Abstract</summary>
Accurately annotated ultrasonic images are vital components of a high-quality medical report. Hospitals often have strict guidelines on the types of annotations that should appear on imaging results. However, manually inspecting these images can be a cumbersome task. While a neural network could potentially automate the process, training such a model typically requires a dataset of paired input and target images, which in turn involves significant human labour. This study introduces an automated approach for detecting annotations in images. This is achieved by treating the annotations as noise, creating a self-supervised pretext task and using a model trained under the Noise2Noise scheme to restore the image to a clean state. We tested a variety of model structures on the denoising task against different types of annotation, including body marker annotation, radial line annotation, etc. Our results demonstrate that most models trained under the Noise2Noise scheme outperformed their counterparts trained with noisy-clean data pairs. The costumed U-Net yielded the most optimal outcome on the body marker annotation dataset, with high scores on segmentation precision and reconstruction similarity. We released our code at https://github.com/GrandArth/UltrasonicImage-N2N-Approach.
</details>
<details>
<summary>æ‘˜è¦</summary>
é«˜å“è´¨åŒ»ç–—æŠ¥å‘Šä¸­çš„å‡†ç¡®æ ‡æ³¨å›¾åƒæ˜¯å…³é”®ç»„æˆéƒ¨åˆ†ã€‚åŒ»é™¢é€šå¸¸æœ‰ä¸¥æ ¼çš„å›¾åƒæ ‡æ³¨è§„èŒƒï¼Œä½†æ‰‹åŠ¨æ£€æŸ¥è¿™äº›å›¾åƒå¯ä»¥æ˜¯ä¸€é¡¹ç¹ççš„ä»»åŠ¡ã€‚ä½¿ç”¨ç¥ç»ç½‘ç»œè‡ªåŠ¨åŒ–è¿™ä¸ªè¿‡ç¨‹å¯èƒ½æ˜¯ä¸€ä¸ªè§£å†³æ–¹æ¡ˆï¼Œä½†æ˜¯è®­ç»ƒè¿™ç§æ¨¡å‹é€šå¸¸éœ€è¦ä¸€ä¸ªåŒ…å«è¾“å…¥å’Œç›®æ ‡å›¾åƒçš„ paired æ•°æ®é›†ï¼Œè¿™éœ€è¦å¾ˆå¤šäººå·¥åŠ³åŠ¨ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–å›¾åƒæ ‡æ³¨æ£€æµ‹æ–¹æ³•ã€‚è¿™æ˜¯é€šè¿‡å°†æ ‡æ³¨è§†ä¸ºå™ªå£°ï¼Œåˆ›å»ºä¸€ç§è‡ªæˆ‘è¶…vised pretext taskï¼Œå¹¶ä½¿ç”¨åŸºäº Noise2Noise æ–¹æ¡ˆè®­ç»ƒçš„æ¨¡å‹æ¥è¿˜åŸå›¾åƒä¸ºä¸€ä¸ªæ¸…æ™°çš„çŠ¶æ€æ¥å®ç°çš„ã€‚æˆ‘ä»¬æµ‹è¯•äº†ä¸åŒç±»å‹çš„æ ‡æ³¨ï¼ŒåŒ…æ‹¬ä½“ markers æ ‡æ³¨å’Œå¾„å‘çº¿æ ‡æ³¨ç­‰ï¼Œå¹¶è¯„ä¼°äº†ä¸åŒæ¨¡å‹ç»“æ„çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå¤§å¤šæ•°åŸºäº Noise2Noise æ–¹æ¡ˆè®­ç»ƒçš„æ¨¡å‹åœ¨æ‚å™ªclean æ•°æ®å¯¹æ¯”ä¸‹è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”ç”¨äºä½“ markers æ ‡æ³¨é›†ä¸Šçš„ customized U-Net å¾—åˆ°äº†æœ€ä½³çš„ç»“æœï¼Œå…¶segmentationç²¾åº¦å’Œé‡å»ºç›¸ä¼¼åº¦å‡è¾¾åˆ°äº†é«˜æ°´å¹³ã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»¥åœ¨ <https://github.com/GrandArth/UltrasonicImage-N2N-Approach> ä¸Šè·å–ã€‚
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Low-Light-Images-Using-Infrared-Encoded-Images"><a href="#Enhancing-Low-Light-Images-Using-Infrared-Encoded-Images" class="headerlink" title="Enhancing Low-Light Images Using Infrared-Encoded Images"></a>Enhancing Low-Light Images Using Infrared-Encoded Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04122">http://arxiv.org/abs/2307.04122</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wyf0912/ELIEI">https://github.com/wyf0912/ELIEI</a></li>
<li>paper_authors: Shulin Tian, Yufei Wang, Renjie Wan, Wenhan Yang, Alex C. Kot, Bihan Wen</li>
<li>for: æé«˜ä½å…‰ç…§å›¾åƒçš„å¯è§åº¦å’Œç»†èŠ‚è¡¨ç¤º</li>
<li>methods: ç§»é™¤å¡å£å†…çš„æŠ˜å°„å…‰é•œFilterï¼Œä½¿ç”¨æ›´å¤šçš„ç…§æ˜ä¿¡æ¯ä»è¿‘infraredpectrumä¸­è·å–æ›´é«˜çš„ä¿¡å™ªæ¯”</li>
<li>results: å¯¹æ¯” referencetest datasetï¼Œæå‡ºçš„æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æé«˜ä½å…‰ç…§å›¾åƒçš„å¯è§åº¦å’Œç»†èŠ‚è¡¨ç¤ºï¼Œå¹¶ä¸”é‡åŒ–å’Œ ĞºĞ°Ñ‡Ğµåœ°æ¯”è¾ƒå¥½<details>
<summary>Abstract</summary>
Low-light image enhancement task is essential yet challenging as it is ill-posed intrinsically. Previous arts mainly focus on the low-light images captured in the visible spectrum using pixel-wise loss, which limits the capacity of recovering the brightness, contrast, and texture details due to the small number of income photons. In this work, we propose a novel approach to increase the visibility of images captured under low-light environments by removing the in-camera infrared (IR) cut-off filter, which allows for the capture of more photons and results in improved signal-to-noise ratio due to the inclusion of information from the IR spectrum. To verify the proposed strategy, we collect a paired dataset of low-light images captured without the IR cut-off filter, with corresponding long-exposure reference images with an external filter. The experimental results on the proposed dataset demonstrate the effectiveness of the proposed method, showing better performance quantitatively and qualitatively. The dataset and code are publicly available at https://wyf0912.github.io/ELIEI/
</details>
<details>
<summary>æ‘˜è¦</summary>
ä½å…‰ç…§å›¾åƒæå‡ä»»åŠ¡æ˜¯å¿…å¤‡åˆæŒ‘æˆ˜çš„ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªå†…åœ¨ä¸å®šçš„é—®é¢˜ã€‚å…ˆå‰çš„è‰ºæœ¯ä¸»è¦é€šè¿‡åƒç´ ç²¾åº¦æŸå¤±æ¥å¤„ç†ä½å…‰ç…§å›¾åƒï¼Œè¿™é™åˆ¶äº†æ¢å¤å›¾åƒçš„äº®åº¦ã€å¯¹æ¯”åº¦å’Œç»†èŠ‚çš„èƒ½åŠ›ï¼Œå› ä¸ºå¯å¾—åˆ°çš„å…‰å­æ•°å¾ˆå°‘ã€‚åœ¨è¿™ä¸ªå·¥ä½œä¸­ï¼Œæˆ‘ä»¬æè®®ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œåˆ©ç”¨å–æ¶ˆç›¸æœºå†…ç½®çš„çº¢å¤–ï¼ˆIRï¼‰å‰”é™¤filterï¼Œä»¥è·å–æ›´å¤šçš„å…‰å­æ•°ï¼Œä»è€Œæé«˜ä¿¡å·å™ªå£°æ¯”ã€‚ä¸ºéªŒè¯æè®®çš„ç­–ç•¥ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸€ä¸ªå¯¹åº”çš„ä½å…‰ç…§å›¾åƒé›†å’Œé•¿æ›å…‰å‚ç…§å›¾åƒé›†ï¼Œä½¿ç”¨å¤–éƒ¨æ»¤é•œè·å–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæè®®çš„æ–¹æ³•å…·æœ‰æ›´å¥½çš„æ•°é‡å’Œè´¨é‡æ€§èƒ½ã€‚æ•°æ®é›†å’Œä»£ç åœ¨https://wyf0912.github.io/ELIEI/ä¸Šå…¬å¼€ã€‚
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Building-Semantic-Segmentation-Accuracy-with-Super-Resolution-and-Deep-Learning-Investigating-the-Impact-of-Spatial-Resolution-on-Various-Datasets"><a href="#Enhancing-Building-Semantic-Segmentation-Accuracy-with-Super-Resolution-and-Deep-Learning-Investigating-the-Impact-of-Spatial-Resolution-on-Various-Datasets" class="headerlink" title="Enhancing Building Semantic Segmentation Accuracy with Super Resolution and Deep Learning: Investigating the Impact of Spatial Resolution on Various Datasets"></a>Enhancing Building Semantic Segmentation Accuracy with Super Resolution and Deep Learning: Investigating the Impact of Spatial Resolution on Various Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04101">http://arxiv.org/abs/2307.04101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiling Guo, Xiaodan Shi, Haoran Zhang, Dou Huang, Xiaoya Song, Jinyue Yan, Ryosuke Shibasaki</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨æ·±åº¦å­¦ä¹ åŸºäºè¿œç¨‹æ„ŸçŸ¥æŠ€æœ¯çš„å»ºç­‘ semantics åˆ†å‰²ä¸­ spatial resolution çš„å½±å“ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº† super-resolution å’Œ down-sampling æŠ€æœ¯å°† remote sensing å›¾åƒè½¬åŒ–ä¸ºå¤šä¸ªç©ºé—´åˆ†è¾¨ç‡ï¼Œç„¶åé€‰æ‹©äº† UNet å’Œ FPN ä¸¤ç§æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ã€‚</li>
<li>results: å®éªŒç»“æœæ˜¾ç¤ºï¼Œå»ºç­‘ semantics åˆ†å‰²ç»“æœå—åˆ°ç©ºé—´åˆ†è¾¨ç‡çš„å½±å“ï¼Œå¹¶ä¸”åœ¨çº¦ 0.3m çš„ç©ºé—´åˆ†è¾¨ç‡ä¸‹è¾¾åˆ°æœ€ä½³æˆæœ¬æ•ˆæœã€‚<details>
<summary>Abstract</summary>
The development of remote sensing and deep learning techniques has enabled building semantic segmentation with high accuracy and efficiency. Despite their success in different tasks, the discussions on the impact of spatial resolution on deep learning based building semantic segmentation are quite inadequate, which makes choosing a higher cost-effective data source a big challenge. To address the issue mentioned above, in this study, we create remote sensing images among three study areas into multiple spatial resolutions by super-resolution and down-sampling. After that, two representative deep learning architectures: UNet and FPN, are selected for model training and testing. The experimental results obtained from three cities with two deep learning models indicate that the spatial resolution greatly influences building segmentation results, and with a better cost-effectiveness around 0.3m, which we believe will be an important insight for data selection and preparation.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>> translate "The development of remote sensing and deep learning techniques has enabled building semantic segmentation with high accuracy and efficiency. Despite their success in different tasks, the discussions on the impact of spatial resolution on deep learning based building semantic segmentation are quite inadequate, which makes choosing a higher cost-effective data source a big challenge. To address the issue mentioned above, in this study, we create remote sensing images among three study areas into multiple spatial resolutions by super-resolution and down-sampling. After that, two representative deep learning architectures: UNet and FPN, are selected for model training and testing. The experimental results obtained from three cities with two deep learning models indicate that the spatial resolution greatly influences building segmentation results, and with a better cost-effectiveness around 0.3m, which we believe will be an important insight for data selection and preparation." into Chinese (Simplified)ğŸ“å‘å±•è¿œç¨‹æ„ŸçŸ¥å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œä½¿å¾—å»ºç­‘ semantic segmentation çš„å‡†ç¡®ç‡å’Œæ•ˆç‡å¾—åˆ°äº†å¤§å¹…æé«˜ã€‚ç„¶è€Œï¼Œå…³äºæ·±åº¦å­¦ä¹ åŸºäºå»ºç­‘ semantic segmentation çš„ç©ºé—´åˆ†è¾¨ç‡å½±å“çš„è®¨è®ºï¼Œå°šä¸å……åˆ†ï¼Œè¿™ä½¿å¾—é€‰æ‹©æ›´é«˜æ•ˆæœå’Œç»æµçš„æ•°æ®æºæˆä¸ºä¸€å¤§æŒ‘æˆ˜ã€‚ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬ç ”ç©¶ä¸­å°†Remote sensing å›¾åƒåœ¨ä¸‰ä¸ªç ”ç©¶åŒºåŸŸä¸­è¿›è¡Œå¤šä¸ªç©ºé—´åˆ†è¾¨ç‡çš„åˆ›å»ºï¼Œé€šè¿‡è¶…åˆ†è¾¨å’Œé™é‡‡æ ·ã€‚ç„¶åï¼Œé€‰æ‹©äº†ä¸¤ç§ä»£è¡¨æ€§çš„æ·±åº¦å­¦ä¹ æ¶æ„ï¼šUNet å’Œ FPNã€‚é€šè¿‡ä¸‰åº§åŸå¸‚çš„ä¸¤ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹çš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œç©ºé—´åˆ†è¾¨ç‡å¯¹å»ºç­‘ segmentation ç»“æœäº§ç”Ÿäº†æå¤§çš„å½±å“ï¼Œå¹¶ä¸”åœ¨çº¦0.3ç±³çš„æˆæœ¬æ•ˆæœä¸Šè¡¨ç°è¾ƒå¥½ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™å°†æˆä¸ºæ•°æ®é€‰æ‹©å’Œå‡†å¤‡çš„é‡è¦æ„è§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Combining-transmission-speckle-photography-and-convolutional-neural-network-for-determination-of-fat-content-in-cowâ€™s-milk-â€“-an-exercise-in-classification-of-parameters-of-a-complex-suspension"><a href="#Combining-transmission-speckle-photography-and-convolutional-neural-network-for-determination-of-fat-content-in-cowâ€™s-milk-â€“-an-exercise-in-classification-of-parameters-of-a-complex-suspension" class="headerlink" title="Combining transmission speckle photography and convolutional neural network for determination of fat content in cowâ€™s milk â€“ an exercise in classification of parameters of a complex suspension"></a>Combining transmission speckle photography and convolutional neural network for determination of fat content in cowâ€™s milk â€“ an exercise in classification of parameters of a complex suspension</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15069">http://arxiv.org/abs/2307.15069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kwasi Nyandey, Daniel Jakubczyk</li>
<li>for: direct classification and recognition of milk fat content classes</li>
<li>methods: combined transmission speckle photography and machine learning</li>
<li>results: unambiguous recognition of milk fat content classes with high accuracy (100% and ~99%)<details>
<summary>Abstract</summary>
We have combined transmission speckle photography and machine learning for direct classification and recognition of milk fat content classes. Our aim was hinged on the fact that parameters of scattering particles (and the dispersion medium) can be linked to the intensity distribution (speckle) observed when coherent light is transmitted through a scattering medium. For milk, it is primarily the size distribution and concentration of fat globules, which constitutes the total fat content. Consequently, we trained convolutional neural network to recognise and classify laser speckle from different fat content classes (0.5, 1.5, 2.0 and 3.2%). We investigated four exposure-time protocols and obtained the highest performance for shorter exposure times, in which the intensity histograms are kept similar for all images and the most probable intensity in the speckle pattern is close to zero. Our neural network was able to recognize the milk fat content classes unambiguously and we obtained the highest test and independent classification accuracies of 100 and ~99% respectively. It indicates that the parameters of other complex realistic suspensions could be classified with similar methods.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬å°†ä¼ è¾“æ‰«æå…‰å­¦å’Œæœºå™¨å­¦ä¹ ç»“åˆï¼Œä»¥ç›´æ¥åˆ†ç±»å’Œè¯†åˆ«ç‰›å¥¶è„‚è‚ªå«é‡ç±»åˆ«ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åˆ©ç”¨æ•£å°„ä½“ç²’å­ï¼ˆä»¥åŠæ•£å°„åª’ä½“ï¼‰çš„å‚æ•°ä¸æ‰«æå…‰é€šè¿‡æ•£å°„åª’ä½“æ‰€è§‚å¯Ÿåˆ°çš„INTENSITYåˆ†å¸ƒï¼ˆæ‰«æå…‰æ–‘ï¼‰ä¹‹é—´å­˜åœ¨å…³ç³»ã€‚åœ¨ç‰›å¥¶ä¸­ï¼Œä¸»è¦æ˜¯è„‚è‚ªçƒä½“å¤§å°åˆ†å¸ƒå’Œæ€»è„‚è‚ªå«é‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°† convolutional neural network è®­ç»ƒæ¥è¯†åˆ«å’Œåˆ†ç±»ä¸åŒè„‚è‚ªå«é‡ç±»åˆ«ï¼ˆ0.5%, 1.5%, 2.0%å’Œ3.2%ï¼‰ã€‚æˆ‘ä»¬ç ”ç©¶äº†å››ç§æ›å…‰æ—¶é—´åè®®ï¼Œå¹¶è·å¾—äº†æœ€é«˜æ€§èƒ½ï¼Œå…¶ä¸­æ›å…‰æ—¶é—´è¾ƒçŸ­ï¼Œå›¾åƒINTENSITYé¢‘è°±å‡åŒ€ï¼Œæœ€ probable intensity åœ¨æ‰«æå…‰æ–‘æ¨¡å¼ä¸­æ¥è¿‘é›¶ã€‚æˆ‘ä»¬çš„ç¥ç»ç½‘ç»œèƒ½å¤Ÿä¸ ambiguously è¯†åˆ«ç‰›å¥¶è„‚è‚ªå«é‡ç±»åˆ«ï¼Œå¹¶åœ¨æµ‹è¯•å’Œç‹¬ç«‹åˆ†ç±»å‡†ç¡®ç‡è¾¾100%å’Œ~99%ã€‚è¿™è¡¨æ˜å¯ä»¥ä½¿ç”¨ç±»ä¼¼æ–¹æ³•æ¥åˆ†ç±»å…¶ä»–å¤æ‚çš„çœŸå®æ¶‚æ•·ã€‚
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/09/eess.IV_2023_07_09/" data-id="cllta0lj0007fny88c7mvhjer" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/09/eess.AS_2023_07_09/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.AS - 2023-07-09 22:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/08/cs.LG_2023_07_08/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-07-08 18:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
