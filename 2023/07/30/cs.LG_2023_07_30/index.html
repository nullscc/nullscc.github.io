
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-07-30 18:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Efficient Federated Learning via Local Adaptive Amended Optimizer with Linear Speedup paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.00522 repo_url: None paper_authors: Yan Sun, Li Shen, Hao Sun, Liang Ding, Da">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-07-30 18:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/07/30/cs.LG_2023_07_30/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Efficient Federated Learning via Local Adaptive Amended Optimizer with Linear Speedup paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.00522 repo_url: None paper_authors: Yan Sun, Li Shen, Hao Sun, Liang Ding, Da">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-29T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:22.849Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_07_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/30/cs.LG_2023_07_30/" class="article-date">
  <time datetime="2023-07-29T16:00:00.000Z" itemprop="datePublished">2023-07-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-07-30 18:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Efficient-Federated-Learning-via-Local-Adaptive-Amended-Optimizer-with-Linear-Speedup"><a href="#Efficient-Federated-Learning-via-Local-Adaptive-Amended-Optimizer-with-Linear-Speedup" class="headerlink" title="Efficient Federated Learning via Local Adaptive Amended Optimizer with Linear Speedup"></a>Efficient Federated Learning via Local Adaptive Amended Optimizer with Linear Speedup</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00522">http://arxiv.org/abs/2308.00522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yan Sun, Li Shen, Hao Sun, Liang Ding, Dacheng Tao</li>
<li>for: 提高 federated learning（FL）中的 adaptive 优化效率，解决 rugged convergence 和 client drifts 问题。</li>
<li>methods: 提出了一种基于 momentum 的 globally adaptive 和 locally amended 优化算法， named Federated Local ADaptive Amended optimizer (\textit{FedLADA），通过 estimate global average offset 和 correct local offset 来提高 empirical training speed 和 mitigate heterogeneous over-fitting。</li>
<li>results: 在实际世界数据集上进行了广泛的实验，证明了我们提出的 \textit{FedLADA} 可以减少 communication rounds 并 достичь更高的准确率，比如基础方法更高。<details>
<summary>Abstract</summary>
Adaptive optimization has achieved notable success for distributed learning while extending adaptive optimizer to federated Learning (FL) suffers from severe inefficiency, including (i) rugged convergence due to inaccurate gradient estimation in global adaptive optimizer; (ii) client drifts exacerbated by local over-fitting with the local adaptive optimizer. In this work, we propose a novel momentum-based algorithm via utilizing the global gradient descent and locally adaptive amended optimizer to tackle these difficulties. Specifically, we incorporate a locally amended technique to the adaptive optimizer, named Federated Local ADaptive Amended optimizer (\textit{FedLADA}), which estimates the global average offset in the previous communication round and corrects the local offset through a momentum-like term to further improve the empirical training speed and mitigate the heterogeneous over-fitting. Theoretically, we establish the convergence rate of \textit{FedLADA} with a linear speedup property on the non-convex case under the partial participation settings. Moreover, we conduct extensive experiments on the real-world dataset to demonstrate the efficacy of our proposed \textit{FedLADA}, which could greatly reduce the communication rounds and achieves higher accuracy than several baselines.
</details>
<details>
<summary>摘要</summary>
适应优化在分布式学习中取得了显著的成功，但扩展适应优化到联邦学习（FL）中受到严重的不fficient，包括（i） rugged convergence due to inaccurate gradient estimation in global adaptive optimizer;（ii） client drifts exacerbated by local over-fitting with the local adaptive optimizer。在这种工作中，我们提议一种新的旋转矩阵-based algorithm，使用全球梯度下降和本地适应修改优化器来解决这些困难。Specifically, we incorporate a locally amended technique to the adaptive optimizer, named Federated Local ADaptive Amended optimizer (\textit{FedLADA}), which estimates the global average offset in the previous communication round and corrects the local offset through a momentum-like term to further improve the empirical training speed and mitigate the heterogeneous over-fitting。理论上，我们证明\textit{FedLADA}在非对称情况下具有线性快速性质。此外，我们对实际数据集进行了广泛的实验，以确认我们的提议的\textit{FedLADA}的有效性，它可以大幅减少通信runds和实现更高的准确率than several baselines。
</details></li>
</ul>
<hr>
<h2 id="DRL4Route-A-Deep-Reinforcement-Learning-Framework-for-Pick-up-and-Delivery-Route-Prediction"><a href="#DRL4Route-A-Deep-Reinforcement-Learning-Framework-for-Pick-up-and-Delivery-Route-Prediction" class="headerlink" title="DRL4Route: A Deep Reinforcement Learning Framework for Pick-up and Delivery Route Prediction"></a>DRL4Route: A Deep Reinforcement Learning Framework for Pick-up and Delivery Route Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16246">http://arxiv.org/abs/2307.16246</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maoxiaowei97/drl4route">https://github.com/maoxiaowei97/drl4route</a></li>
<li>paper_authors: Xiaowei Mao, Haomin Wen, Hengrui Zhang, Huaiyu Wan, Lixia Wu, Jianbin Zheng, Haoyuan Hu, Youfang Lin</li>
<li>For: 预测工作者的服务路线（PDRP），即根据工作者当前任务池估算未来服务路线。* Methods: 基于强化学习的 Deep Reinforcement Learning（DRL）框架，combines 前一代深度学习模型的行为学习能力和强化学习的非 differentiable 目标优化能力。* Results: 对实际数据集进行了广泛的 Offline 实验和在线部署，并显示了对 Location Square Deviation（LSD）和 Accuracy@3（ACC@3）的改进，比 existed 方法提高 0.9%-2.7% 和 2.4%-3.2%。<details>
<summary>Abstract</summary>
Pick-up and Delivery Route Prediction (PDRP), which aims to estimate the future service route of a worker given his current task pool, has received rising attention in recent years. Deep neural networks based on supervised learning have emerged as the dominant model for the task because of their powerful ability to capture workers' behavior patterns from massive historical data. Though promising, they fail to introduce the non-differentiable test criteria into the training process, leading to a mismatch in training and test criteria. Which considerably trims down their performance when applied in practical systems. To tackle the above issue, we present the first attempt to generalize Reinforcement Learning (RL) to the route prediction task, leading to a novel RL-based framework called DRL4Route. It combines the behavior-learning abilities of previous deep learning models with the non-differentiable objective optimization ability of reinforcement learning. DRL4Route can serve as a plug-and-play component to boost the existing deep learning models. Based on the framework, we further implement a model named DRL4Route-GAE for PDRP in logistic service. It follows the actor-critic architecture which is equipped with a Generalized Advantage Estimator that can balance the bias and variance of the policy gradient estimates, thus achieving a more optimal policy. Extensive offline experiments and the online deployment show that DRL4Route-GAE improves Location Square Deviation (LSD) by 0.9%-2.7%, and Accuracy@3 (ACC@3) by 2.4%-3.2% over existing methods on the real-world dataset.
</details>
<details>
<summary>摘要</summary>
Pick-up and Delivery Route Prediction (PDRP) 已经在最近几年内收到了越来越多的关注，旨在预测工作者当前任务池中的未来服务路线。基于深度学习的神经网络在这个任务上已经成为主流模型，因为它们可以很好地捕捉工作者的行为模式从大量历史数据中。虽然有前途，但它们却无法将不可导的测试标准引入训练过程中，导致训练和测试标准之间的匹配性受到影响，从而限制它们在实际系统中的表现。为解决这个问题，我们提出了将强化学习（Reinforcement Learning，RL）应用于路径预测任务，并提出了一个基于RL的框架called DRL4Route。它将前面的深度学习模型的行为学习能力与RL的非导数优化能力结合起来，以提高路径预测的性能。DRL4Route可以作为现有深度学习模型的插件来使用。基于该框架，我们进一步实现了一个模型名为DRL4Route-GAE，用于PDRP在物流服务中。它采用actor-critic架构，并配备一个通用优化估计器，可以平衡政策评估器的偏移和方差，从而实现更优的政策。在大量的离线实验和在线部署中，DRL4Route-GAE在实际数据集上提高了Location Square Deviation（LSD）和Accuracy@3（ACC@3）的性能，分别提高了0.9%-2.7%和2.4%-3.2%。
</details></li>
</ul>
<hr>
<h2 id="Synaptic-Plasticity-Models-and-Bio-Inspired-Unsupervised-Deep-Learning-A-Survey"><a href="#Synaptic-Plasticity-Models-and-Bio-Inspired-Unsupervised-Deep-Learning-A-Survey" class="headerlink" title="Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep Learning: A Survey"></a>Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep Learning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16236">http://arxiv.org/abs/2307.16236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriele Lagani, Fabrizio Falchi, Claudio Gennaro, Giuseppe Amato</li>
<li>for: 探讨生物基于深度学习的研究方向，包括生物启发式 synaptic plasticity 模型的应用在深度学习场景下，以及与顺序神经网络（SNNs）的相似模型。</li>
<li>methods: 文章探讨了一系列生物启发式的深度学习模型，包括 synaptic plasticity 模型的应用在深度学习场景下，以及与 SNNs 的相似模型。</li>
<li>results: 文章表明，生物启发式的深度学习模型可以提高深度学习的稳定性和可靠性，并且可以启发新的深度学习算法和模型。<details>
<summary>Abstract</summary>
Recently emerged technologies based on Deep Learning (DL) achieved outstanding results on a variety of tasks in the field of Artificial Intelligence (AI). However, these encounter several challenges related to robustness to adversarial inputs, ecological impact, and the necessity of huge amounts of training data. In response, researchers are focusing more and more interest on biologically grounded mechanisms, which are appealing due to the impressive capabilities exhibited by biological brains. This survey explores a range of these biologically inspired models of synaptic plasticity, their application in DL scenarios, and the connections with models of plasticity in Spiking Neural Networks (SNNs). Overall, Bio-Inspired Deep Learning (BIDL) represents an exciting research direction, aiming at advancing not only our current technologies but also our understanding of intelligence.
</details>
<details>
<summary>摘要</summary>
现代深度学习（DL）技术在人工智能（AI）领域取得了杰出的成绩，但这些技术受到了对抗式输入、生态影响和对训练数据的需求等挑战。为了回应这些挑战，研究人员在注意力中心的生物灵活机制，这些机制具有生物大脑的优秀表现所吸引了研究人员的注意。本调查探讨了一些生物启发的 synaptic plasticity 模型，其应用于深度学习情况下，以及与对抗式神经网络（SNNs）的模型之间的连结。总之，生物启发深度学习（BIDL）是一个充满挑战和探索的研究方向，它不仅可以提高我们现有的技术，而且可以帮助我们更好地理解智慧的本质。
</details></li>
</ul>
<hr>
<h2 id="Spiking-Neural-Networks-and-Bio-Inspired-Supervised-Deep-Learning-A-Survey"><a href="#Spiking-Neural-Networks-and-Bio-Inspired-Supervised-Deep-Learning-A-Survey" class="headerlink" title="Spiking Neural Networks and Bio-Inspired Supervised Deep Learning: A Survey"></a>Spiking Neural Networks and Bio-Inspired Supervised Deep Learning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16235">http://arxiv.org/abs/2307.16235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriele Lagani, Fabrizio Falchi, Claudio Gennaro, Giuseppe Amato</li>
<li>for: 这个论文旨在为人工智能技术的发展提供生物学和神经科学领域的灵感，并进行了生物体系中计算和 synaptic plasticity 的介绍，以及现代神经网络模型的详细介绍。</li>
<li>methods: 论文使用了生物体系中计算的主要原则，以及现代神经网络模型的许多bio-inspired training方法，包括不同于backprop的优化方法，以提高现代神经网络模型的计算能力和生物可能性。</li>
<li>results: 论文提出了一种 Bio-Inspired Deep Learning (BIDL) 方法，可以提高现代神经网络模型的计算能力和生物可能性，并且可以作为传统的 backprop 优化方法的替代方案。<details>
<summary>Abstract</summary>
For a long time, biology and neuroscience fields have been a great source of inspiration for computer scientists, towards the development of Artificial Intelligence (AI) technologies. This survey aims at providing a comprehensive review of recent biologically-inspired approaches for AI. After introducing the main principles of computation and synaptic plasticity in biological neurons, we provide a thorough presentation of Spiking Neural Network (SNN) models, and we highlight the main challenges related to SNN training, where traditional backprop-based optimization is not directly applicable. Therefore, we discuss recent bio-inspired training methods, which pose themselves as alternatives to backprop, both for traditional and spiking networks. Bio-Inspired Deep Learning (BIDL) approaches towards advancing the computational capabilities and biological plausibility of current models.
</details>
<details>
<summary>摘要</summary>
for a long time, biology and neuroscience fields have been a great source of inspiration for computer scientists, towards the development of Artificial Intelligence (AI) technologies. This survey aims at providing a comprehensive review of recent biologically-inspired approaches for AI. After introducing the main principles of computation and synaptic plasticity in biological neurons, we provide a thorough presentation of Spiking Neural Network (SNN) models, and we highlight the main challenges related to SNN training, where traditional backprop-based optimization is not directly applicable. Therefore, we discuss recent bio-inspired training methods, which pose themselves as alternatives to backprop, both for traditional and spiking networks. Bio-Inspired Deep Learning (BIDL) approaches towards advancing the computational capabilities and biological plausibility of current models.Here's the text in Traditional Chinese:for a long time, biology and neuroscience fields have been a great source of inspiration for computer scientists, towards the development of Artificial Intelligence (AI) technologies. This survey aims at providing a comprehensive review of recent biologically-inspired approaches for AI. After introducing the main principles of computation and synaptic plasticity in biological neurons, we provide a thorough presentation of Spiking Neural Network (SNN) models, and we highlight the main challenges related to SNN training, where traditional backprop-based optimization is not directly applicable. Therefore, we discuss recent bio-inspired training methods, which pose themselves as alternatives to backprop, both for traditional and spiking networks. Bio-Inspired Deep Learning (BIDL) approaches towards advancing the computational capabilities and biological plausibility of current models.
</details></li>
</ul>
<hr>
<h2 id="Robust-Electric-Vehicle-Balancing-of-Autonomous-Mobility-On-Demand-System-A-Multi-Agent-Reinforcement-Learning-Approach"><a href="#Robust-Electric-Vehicle-Balancing-of-Autonomous-Mobility-On-Demand-System-A-Multi-Agent-Reinforcement-Learning-Approach" class="headerlink" title="Robust Electric Vehicle Balancing of Autonomous Mobility-On-Demand System: A Multi-Agent Reinforcement Learning Approach"></a>Robust Electric Vehicle Balancing of Autonomous Mobility-On-Demand System: A Multi-Agent Reinforcement Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16228">http://arxiv.org/abs/2307.16228</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sihong He, Shuo Han, Fei Miao<br>for:This paper focuses on designing a multi-agent reinforcement learning (MARL) framework for electric autonomous vehicles (EAVs) balancing in future autonomous mobility-on-demand (AMoD) systems, with the goal of improving the supply-demand ratio and charging utilization rate across the whole city.methods:The proposed MARL-based framework considers both the EAVs supply and mobility demand uncertainties, and uses adversarial agents to model these uncertainties. The proposed algorithm is called REBAMA (Robust E-AMoD Balancing MARL) and it trains a robust EAVs balancing policy to improve the reward, charging utilization fairness, and supply-demand fairness.results:Experiments show that the proposed REBAMA algorithm performs better compared with a non-robust MARL method and a robust optimization-based method, with improvements of 19.28%, 28.18%, and 3.97% in terms of reward, charging utilization fairness, and supply-demand fairness, respectively.<details>
<summary>Abstract</summary>
Electric autonomous vehicles (EAVs) are getting attention in future autonomous mobility-on-demand (AMoD) systems due to their economic and societal benefits. However, EAVs' unique charging patterns (long charging time, high charging frequency, unpredictable charging behaviors, etc.) make it challenging to accurately predict the EAVs supply in E-AMoD systems. Furthermore, the mobility demand's prediction uncertainty makes it an urgent and challenging task to design an integrated vehicle balancing solution under supply and demand uncertainties. Despite the success of reinforcement learning-based E-AMoD balancing algorithms, state uncertainties under the EV supply or mobility demand remain unexplored. In this work, we design a multi-agent reinforcement learning (MARL)-based framework for EAVs balancing in E-AMoD systems, with adversarial agents to model both the EAVs supply and mobility demand uncertainties that may undermine the vehicle balancing solutions. We then propose a robust E-AMoD Balancing MARL (REBAMA) algorithm to train a robust EAVs balancing policy to balance both the supply-demand ratio and charging utilization rate across the whole city. Experiments show that our proposed robust method performs better compared with a non-robust MARL method that does not consider state uncertainties; it improves the reward, charging utilization fairness, and supply-demand fairness by 19.28%, 28.18%, and 3.97%, respectively. Compared with a robust optimization-based method, the proposed MARL algorithm can improve the reward, charging utilization fairness, and supply-demand fairness by 8.21%, 8.29%, and 9.42%, respectively.
</details>
<details>
<summary>摘要</summary>
电动自驾车 (EAVs) 在未来的自动化移动服务系统 (AMoD) 中受到关注，因为它们具有经济和社会上的优势。然而，EAVs 的充电模式 (长时间充电、高频充电、不可预测的充电行为等) 使得预测 EAVs 供应很具有挑战性。此外，移动需求的预测不确定性使得设计一个集成的车辆均衡解决方案变得非常重要和挑战。虽然束规学学习基于 E-AMoD 的均衡算法得到了成功，但是状态不确定性下的 EV 供应或移动需求仍然未得到探讨。在这种情况下，我们设计了一个多代理束规学 (MARL) 基础框架，用于 EAVs 均衡在 E-AMoD 系统中。我们在这个框架中引入了对 EAVs 供应和移动需求不确定性的模型，使得我们可以训练一个robust的 EAVs 均衡策略，以保证供应和需求之间的均衡。实验表明，我们提出的robust方法在比较非robust MARL 方法时表现更好，提高了奖励、充电使用公平和供应需求公平的三个指标，分别提高了19.28%、28.18%和3.97%。相比于一种robust优化基于方法，我们的 MARL 算法可以提高这些指标的提升，分别为8.21%、8.29%和9.42%。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-the-Neural-Network-Training-for-OCR-Error-Correction-of-Historical-Hebrew-Texts"><a href="#Optimizing-the-Neural-Network-Training-for-OCR-Error-Correction-of-Historical-Hebrew-Texts" class="headerlink" title="Optimizing the Neural Network Training for OCR Error Correction of Historical Hebrew Texts"></a>Optimizing the Neural Network Training for OCR Error Correction of Historical Hebrew Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16220">http://arxiv.org/abs/2307.16220</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/smartinternz02/SI-GuidedProject-2307-1622049182">https://github.com/smartinternz02/SI-GuidedProject-2307-1622049182</a></li>
<li>paper_authors: Omri Suissa, Avshalom Elmalech, Maayan Zhitomirsky-Geffet</li>
<li>for: 这篇论文的目的是提出一种可以在历史文献中进行Optical Character Recognition（OCR）后修正的轻量级神经网络训练方法，并 Investigate which type of dataset is the most effective for OCR post-correction of historical documents.</li>
<li>methods: 该论文使用了一种新的方法，即通过自动生成语言和任务特定的训练数据来提高神经网络的OCR后修正结果。</li>
<li>results: 研究发现，使用该方法训练神经网络可以更有效地 Correct OCR errors，并且其性能与其他现有的神经网络和复杂的检查器相比较高。<details>
<summary>Abstract</summary>
Over the past few decades, large archives of paper-based documents such as books and newspapers have been digitized using Optical Character Recognition. This technology is error-prone, especially for historical documents. To correct OCR errors, post-processing algorithms have been proposed based on natural language analysis and machine learning techniques such as neural networks. Neural network's disadvantage is the vast amount of manually labeled data required for training, which is often unavailable. This paper proposes an innovative method for training a light-weight neural network for Hebrew OCR post-correction using significantly less manually created data. The main research goal is to develop a method for automatically generating language and task-specific training data to improve the neural network results for OCR post-correction, and to investigate which type of dataset is the most effective for OCR post-correction of historical documents. To this end, a series of experiments using several datasets was conducted. The evaluation corpus was based on Hebrew newspapers from the JPress project. An analysis of historical OCRed newspapers was done to learn common language and corpus-specific OCR errors. We found that training the network using the proposed method is more effective than using randomly generated errors. The results also show that the performance of the neural network for OCR post-correction strongly depends on the genre and area of the training data. Moreover, neural networks that were trained with the proposed method outperform other state-of-the-art neural networks for OCR post-correction and complex spellcheckers. These results may have practical implications for many digital humanities projects.
</details>
<details>
<summary>摘要</summary>
在过去几十年，大量的纸质文档如书籍和报纸被使用光学字符识别技术进行数字化。这种技术存在误差，尤其是对历史文档。为了纠正OCR误差，基于自然语言分析和机器学习技术的后处理算法被提议。但是，这些算法需要大量的手动标注数据来训练，却经常不可得。这篇论文提出了一种创新的方法，用于使用较少的手动创建数据来训练轻量级神经网络进行希伯来文OCR后处理。研究的主要目标是开发一种自动生成语言和任务特定的训练数据，以改进神经网络的OCR后处理结果，并investigate历史文档OCR后处理中最有效的数据集。为此，我们进行了一系列实验，使用了多个数据集。评估集基于希伯来报纸JPress项目。我们分析了历史OCR后的报纸，以了解希伯来文OCR误差的常见语言和核心特征。我们发现，使用我们提议的方法训练神经网络比使用随机生成的误差更有效。结果还表明，神经网络在OCR后处理中的表现强烈取决于训练数据的类别和地区。此外，使用我们提议的方法训练的神经网络在OCR后处理和复杂的拼写检查器之间表现更好。这些结果可能对数字人文学科项目产生实际影响。
</details></li>
</ul>
<hr>
<h2 id="Improving-Probabilistic-Bisimulation-for-MDPs-Using-Machine-Learning"><a href="#Improving-Probabilistic-Bisimulation-for-MDPs-Using-Machine-Learning" class="headerlink" title="Improving Probabilistic Bisimulation for MDPs Using Machine Learning"></a>Improving Probabilistic Bisimulation for MDPs Using Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02519">http://arxiv.org/abs/2308.02519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammadsadegh Mohaghegh, Khayyam Salehi</li>
<li>for:  This paper aims to address the state space explosion problem in applying model checking to complex systems, and to improve the efficiency of probabilistic bisimulation for stochastic systems with nondeterministic behaviors.</li>
<li>methods:  The paper proposes a new technique that uses machine learning classification techniques to approximate the partition of the state space of a given probabilistic model into its bisimulation classes. The technique is based on the PRISM program of the model and constructs small versions of the model to train a classifier.</li>
<li>results:  The experimental results show that the proposed approach can significantly decrease the running time compared to state-of-the-art tools.<details>
<summary>Abstract</summary>
The utilization of model checking has been suggested as a formal verification technique for analyzing critical systems. However, the primary challenge in applying to complex systems is state space explosion problem. To address this issue, bisimulation minimization has emerged as a prominent method for reducing the number of states in a labeled transition system, aiming to overcome the difficulties associated with the state space explosion problem. In the case of systems exhibiting stochastic behaviors, probabilistic bisimulation is employed to minimize a given model, obtaining its equivalent form with fewer states. Recently, various techniques have been introduced to decrease the time complexity of the iterative methods used to compute probabilistic bisimulation for stochastic systems that display nondeterministic behaviors. In this paper, we propose a new technique to partition the state space of a given probabilistic model to its bisimulation classes. This technique uses the PRISM program of a given model and constructs some small versions of the model to train a classifier. It then applies machine learning classification techniques to approximate the related partition. The resulting partition is used as an initial one for the standard bisimulation technique in order to reduce the running time of the method. The experimental results show that the approach can decrease significantly the running time compared to state-of-the-art tools.
</details>
<details>
<summary>摘要</summary>
utilization of model checking 被建议为形式验证技术，用于分析重要系统。然而，主要挑战在应用于复杂系统时是状态空间爆炸问题。为解决这个问题， bisimulation minimization  emerged as a prominent method for reducing the number of states in a labeled transition system, aiming to overcome the difficulties associated with the state space explosion problem. In the case of systems exhibiting stochastic behaviors, probabilistic bisimulation is employed to minimize a given model, obtaining its equivalent form with fewer states. Recently, various techniques have been introduced to decrease the time complexity of the iterative methods used to compute probabilistic bisimulation for stochastic systems that display nondeterministic behaviors. In this paper, we propose a new technique to partition the state space of a given probabilistic model to its bisimulation classes. This technique uses the PRISM program of a given model and constructs some small versions of the model to train a classifier. It then applies machine learning classification techniques to approximate the related partition. The resulting partition is used as an initial one for the standard bisimulation technique in order to reduce the running time of the method. The experimental results show that the approach can decrease significantly the running time compared to state-of-the-art tools.Note: Simplified Chinese is also known as "Mandarin" or "Standard Chinese". It is the official language of China and is widely spoken in many countries.
</details></li>
</ul>
<hr>
<h2 id="Text-Analysis-Using-Deep-Neural-Networks-in-Digital-Humanities-and-Information-Science"><a href="#Text-Analysis-Using-Deep-Neural-Networks-in-Digital-Humanities-and-Information-Science" class="headerlink" title="Text Analysis Using Deep Neural Networks in Digital Humanities and Information Science"></a>Text Analysis Using Deep Neural Networks in Digital Humanities and Information Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16217">http://arxiv.org/abs/2307.16217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omri Suissa, Avshalom Elmalech, Maayan Zhitomirsky-Geffet</li>
<li>for: This paper aims to explore the challenges of using deep neural networks (DNNs) for analyzing text resources in Digital Humanities (DH) research, and to provide a practical decision model for DH experts to choose the appropriate deep learning approaches for their research.</li>
<li>methods: The paper analyzes multiple use-cases of DH studies in recent literature and their possible solutions, and discusses the challenges of (un)availability of training data and a need for domain adaptation when using DNNs for NLP tasks in DH research.</li>
<li>results: The paper aims to raise awareness of the benefits of utilizing deep learning models in the DH community, and provides a practical decision model for DH experts to choose the appropriate deep learning approaches for their research.<details>
<summary>Abstract</summary>
Combining computational technologies and humanities is an ongoing effort aimed at making resources such as texts, images, audio, video, and other artifacts digitally available, searchable, and analyzable. In recent years, deep neural networks (DNN) dominate the field of automatic text analysis and natural language processing (NLP), in some cases presenting a super-human performance. DNNs are the state-of-the-art machine learning algorithms solving many NLP tasks that are relevant for Digital Humanities (DH) research, such as spell checking, language detection, entity extraction, author detection, question answering, and other tasks. These supervised algorithms learn patterns from a large number of "right" and "wrong" examples and apply them to new examples. However, using DNNs for analyzing the text resources in DH research presents two main challenges: (un)availability of training data and a need for domain adaptation. This paper explores these challenges by analyzing multiple use-cases of DH studies in recent literature and their possible solutions and lays out a practical decision model for DH experts for when and how to choose the appropriate deep learning approaches for their research. Moreover, in this paper, we aim to raise awareness of the benefits of utilizing deep learning models in the DH community.
</details>
<details>
<summary>摘要</summary>
使用计算机技术和人文学科结合，目前正在努力使文本、图像、音频、视频等资源成为数字化可用、搜索可能、分析可能的。在过去几年，深度神经网络（DNN）在自动文本分析和自然语言处理（NLP）领域占据了主导地位，在某些情况下表现出超人般的能力。DNN是目前最好的机器学习算法，用于解决数字人文学科（DH）研究中的许多NLP任务，如拼写检查、语言检测、实体提取、作者检测、问答等任务。这些有监督的算法从大量“正确”和“错误”的示例中学习出模式，并将其应用到新的示例上。然而，在DH研究中使用DNN分析文本资源存在两大挑战：数据库的可用性和领域适应。本文分析了一些DH研究中的多个用例，并探讨了其可能的解决方案，并提出了实用的决策模型，以帮助DH专家在选择适合的深度学习方法时作出决策。此外，本文的目的还是提高使用深度学习模型在DH社区的认识。
</details></li>
</ul>
<hr>
<h2 id="Question-Answering-with-Deep-Neural-Networks-for-Semi-Structured-Heterogeneous-Genealogical-Knowledge-Graphs"><a href="#Question-Answering-with-Deep-Neural-Networks-for-Semi-Structured-Heterogeneous-Genealogical-Knowledge-Graphs" class="headerlink" title="Question Answering with Deep Neural Networks for Semi-Structured Heterogeneous Genealogical Knowledge Graphs"></a>Question Answering with Deep Neural Networks for Semi-Structured Heterogeneous Genealogical Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16214">http://arxiv.org/abs/2307.16214</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/omrivm/uncle-bert">https://github.com/omrivm/uncle-bert</a></li>
<li>paper_authors: Omri Suissa, Maayan Zhitomirsky-Geffet, Avshalom Elmalech</li>
<li>for: 这个研究旨在开发一种基于家谱树的问答系统，以便更好地回答家谱相关的问题。</li>
<li>methods: 该研究使用了转换器基于模型，将家谱数据转换成知识图，然后与不结构化文本结合，最后使用一个转换器基于模型进行训练。</li>
<li>results: 研究发现，相比于当前的状态艺术问答模型，专门为家谱问题开发的模型（叔叔BERT）在精度和复杂度之间取得了良好的平衡。此外，该方法可能有实际意义 для家谱研究和实际项目，使家谱数据更加可访问性。<details>
<summary>Abstract</summary>
With the rising popularity of user-generated genealogical family trees, new genealogical information systems have been developed. State-of-the-art natural question answering algorithms use deep neural network (DNN) architecture based on self-attention networks. However, some of these models use sequence-based inputs and are not suitable to work with graph-based structure, while graph-based DNN models rely on high levels of comprehensiveness of knowledge graphs that is nonexistent in the genealogical domain. Moreover, these supervised DNN models require training datasets that are absent in the genealogical domain. This study proposes an end-to-end approach for question answering using genealogical family trees by: 1) representing genealogical data as knowledge graphs, 2) converting them to texts, 3) combining them with unstructured texts, and 4) training a trans-former-based question answering model. To evaluate the need for a dedicated approach, a comparison between the fine-tuned model (Uncle-BERT) trained on the auto-generated genealogical dataset and state-of-the-art question-answering models was per-formed. The findings indicate that there are significant differences between answering genealogical questions and open-domain questions. Moreover, the proposed methodology reduces complexity while increasing accuracy and may have practical implications for genealogical research and real-world projects, making genealogical data accessible to experts as well as the general public.
</details>
<details>
<summary>摘要</summary>
随着用户生成的家谱树的流行，新的家谱信息系统已经被开发出来。现代自然问答算法使用深度神经网络（DNN）架构，基于自注意网络。然而，一些这些模型使用序列化输入，不适合处理图Structured data，而图Structured DNN模型需要高度完整的知识图的存在，在家谱领域是缺失的。此外，这些监督式DNN模型需要家谱领域缺失的训练数据。本研究提出了一种终端方法，通过以下步骤来回答问题：1）将家谱数据转换为知识图，2）将其转换为文本，3）将其与未结构化文本结合，4）使用 transformer 基于模型进行问答。为了评估需要专门的方法，对 fine-tuned 模型（Uncle-BERT）在自动生成的家谱数据上进行了训练，并与当前的问答模型进行了比较。研究发现，回答家谱问题和开放领域问题存在显著差异。此外，提出的方法可以减少复杂性，提高准确性，并有实际意义 для家谱研究和实际项目，让家谱数据对专家和普通公众都可访问。
</details></li>
</ul>
<hr>
<h2 id="Toward-a-Period-Specific-Optimized-Neural-Network-for-OCR-Error-Correction-of-Historical-Hebrew-Texts"><a href="#Toward-a-Period-Specific-Optimized-Neural-Network-for-OCR-Error-Correction-of-Historical-Hebrew-Texts" class="headerlink" title="Toward a Period-Specific Optimized Neural Network for OCR Error Correction of Historical Hebrew Texts"></a>Toward a Period-Specific Optimized Neural Network for OCR Error Correction of Historical Hebrew Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16213">http://arxiv.org/abs/2307.16213</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omri Suissa, Maayan Zhitomirsky-Geffet, Avshalom Elmalech</li>
<li>For: The paper is written for the purpose of developing a new method for OCR post-correction in Hebrew using neural networks.* Methods: The paper uses a multi-phase method for generating artificial training datasets with OCR errors and optimizing hyperparameters for building an effective neural network for OCR post-correction in Hebrew.* Results: The paper aims to achieve high accuracy in OCR post-correction for Hebrew documents, despite the challenges posed by the language’s unique features and the lack of sufficient training data.Here is the information in Simplified Chinese text:</li>
<li>for: 本研究旨在开发一种基于神经网络的 Hebrew OCR 后修正方法。</li>
<li>methods: 本文使用多阶段方法生成人工训练集，以优化 OCR 错误和超参数，建立高效的 Hebrew OCR 后修正神经网络。</li>
<li>results: 本研究目标是实现高精度的 Hebrew OCR 后修正，突破语言特殊特征和训练数据不足的障碍。<details>
<summary>Abstract</summary>
Over the past few decades, large archives of paper-based historical documents, such as books and newspapers, have been digitized using the Optical Character Recognition (OCR) technology. Unfortunately, this broadly used technology is error-prone, especially when an OCRed document was written hundreds of years ago. Neural networks have shown great success in solving various text processing tasks, including OCR post-correction. The main disadvantage of using neural networks for historical corpora is the lack of sufficiently large training datasets they require to learn from, especially for morphologically-rich languages like Hebrew. Moreover, it is not clear what are the optimal structure and values of hyperparameters (predefined parameters) of neural networks for OCR error correction in Hebrew due to its unique features. Furthermore, languages change across genres and periods. These changes may affect the accuracy of OCR post-correction neural network models. To overcome these challenges, we developed a new multi-phase method for generating artificial training datasets with OCR errors and hyperparameters optimization for building an effective neural network for OCR post-correction in Hebrew.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Optical Character Recognition" is translated as "文本识别" (wén tiān shí bǐ)* "Hebrew" is translated as "希伯来语" (xī bā lǐ yǔ)* "hyperparameters" is translated as "超参数" (chāo jiān xiàng)* "OCR post-correction" is translated as "OCR 后修正" (OCR hòu jiǔ zhèng)* "morphologically-rich languages" is translated as "富有形态语言" (fù yǒu xíng tài yǔ yán)* "genres and periods" is translated as "类型和时期" (lèi xìng yǔ shí qī)Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China.
</details></li>
</ul>
<hr>
<h2 id="Robust-Multi-Agent-Reinforcement-Learning-with-State-Uncertainty"><a href="#Robust-Multi-Agent-Reinforcement-Learning-with-State-Uncertainty" class="headerlink" title="Robust Multi-Agent Reinforcement Learning with State Uncertainty"></a>Robust Multi-Agent Reinforcement Learning with State Uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16212">http://arxiv.org/abs/2307.16212</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sihongho/robust_marl_with_state_uncertainty">https://github.com/sihongho/robust_marl_with_state_uncertainty</a></li>
<li>paper_authors: Sihong He, Songyang Han, Sanbao Su, Shuo Han, Shaofeng Zou, Fei Miao</li>
<li>for: 研究多智能体强化学习（MARL）中存在不确定状态的问题，以提高其在实际应用中的稳定性。</li>
<li>methods: 提出了一种基于Markov游戏的状态干扰对手（MG-SPA）模型，并提出了一种robust平衡（RE）作为解题方法。采用了一种robust多智能体Q学习（RMAQ）算法和一种robust多智能体actor-critic（RMAAC）算法来实现RE。</li>
<li>results: 通过实验表明，RMAQ算法可以寻求optimal值函数，而RMAAC算法在多个多智能体环境中比较多个MARL和robust MARL方法表现更好，特别是在存在状态不确定性时。<details>
<summary>Abstract</summary>
In real-world multi-agent reinforcement learning (MARL) applications, agents may not have perfect state information (e.g., due to inaccurate measurement or malicious attacks), which challenges the robustness of agents' policies. Though robustness is getting important in MARL deployment, little prior work has studied state uncertainties in MARL, neither in problem formulation nor algorithm design. Motivated by this robustness issue and the lack of corresponding studies, we study the problem of MARL with state uncertainty in this work. We provide the first attempt to the theoretical and empirical analysis of this challenging problem. We first model the problem as a Markov Game with state perturbation adversaries (MG-SPA) by introducing a set of state perturbation adversaries into a Markov Game. We then introduce robust equilibrium (RE) as the solution concept of an MG-SPA. We conduct a fundamental analysis regarding MG-SPA such as giving conditions under which such a robust equilibrium exists. Then we propose a robust multi-agent Q-learning (RMAQ) algorithm to find such an equilibrium, with convergence guarantees. To handle high-dimensional state-action space, we design a robust multi-agent actor-critic (RMAAC) algorithm based on an analytical expression of the policy gradient derived in the paper. Our experiments show that the proposed RMAQ algorithm converges to the optimal value function; our RMAAC algorithm outperforms several MARL and robust MARL methods in multiple multi-agent environments when state uncertainty is present. The source code is public on \url{https://github.com/sihongho/robust_marl_with_state_uncertainty}.
</details>
<details>
<summary>摘要</summary>
在实际多智能体学习（MARL）应用中，智能体可能没有完美状态信息（例如由于不准确测量或恶意攻击），这会对智能体策略的稳定性造成挑战。虽然稳定性在MARL部署中变得越来越重要，但是前一个研究少了关于状态不确定性的问题， neither in problem formulation nor algorithm design. 为了解决这个稳定性问题和相关的研究不足，我们在这个工作中研究了MARL中的状态不确定性问题。我们首先将问题模型为一个Markov Game with state perturbation adversaries（MG-SPA），并将 robust equilibrium（RE）作为该问题的解题概念。我们进行了基本的分析，包括确定MG-SPA下的robust equilibrium是否存在的条件。然后，我们提出了一种robust multi-agent Q-learning（RMAQ）算法，用于找到该equilibrium，并提供了确定性保证。为了处理高维状态动作空间，我们设计了一种基于分析表达的多智能体actor-critic（RMAAC）算法。我们的实验表明，我们的RMAQ算法可以 converge to the optimal value function，而RMAAC算法在多个多智能体环境中出perform several MARL and robust MARL方法。代码可以在 \url{https://github.com/sihongho/robust_marl_with_state_uncertainty} 上找到。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Uncertainly-Missing-and-Ambiguous-Visual-Modality-in-Multi-Modal-Entity-Alignment"><a href="#Rethinking-Uncertainly-Missing-and-Ambiguous-Visual-Modality-in-Multi-Modal-Entity-Alignment" class="headerlink" title="Rethinking Uncertainly Missing and Ambiguous Visual Modality in Multi-Modal Entity Alignment"></a>Rethinking Uncertainly Missing and Ambiguous Visual Modality in Multi-Modal Entity Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16210">http://arxiv.org/abs/2307.16210</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zjukg/UMAEA">https://github.com/zjukg/UMAEA</a></li>
<li>paper_authors: Zhuo Chen, Lingbing Guo, Yin Fang, Yichi Zhang, Jiaoyan Chen, Jeff Z. Pan, Yangning Li, Huajun Chen, Wen Zhang</li>
<li>for: 本研究旨在解决多Modal Entity Alignment (MMEA)中存在的感知模式缺失和内在ambiguity问题，以提高MMEA的稳定性和可靠性。</li>
<li>methods: 本研究使用了最新的MMEA模型，并在我们提出的MMEA-UMVM数据集上进行了benchmarking，以评估模型的性能。我们还提出了一种新的Robust Multi-Modal Entity Alignment (UMAEA)方法，该方法能够有效地处理不确定的感知模式缺失和内在ambiguity问题。</li>
<li>results: 我们的研究表明，在面临感知模式缺失和内在ambiguity的情况下，现有的MMEA模型容易过拟合感知模式噪音，导致性能下降或很快衰退。而我们的UMAEA方法能够稳定地处理这些问题，在97个benchmark分区中都达到了最高水平，Significantly surpassing现有的基eline模型。<details>
<summary>Abstract</summary>
As a crucial extension of entity alignment (EA), multi-modal entity alignment (MMEA) aims to identify identical entities across disparate knowledge graphs (KGs) by exploiting associated visual information. However, existing MMEA approaches primarily concentrate on the fusion paradigm of multi-modal entity features, while neglecting the challenges presented by the pervasive phenomenon of missing and intrinsic ambiguity of visual images. In this paper, we present a further analysis of visual modality incompleteness, benchmarking latest MMEA models on our proposed dataset MMEA-UMVM, where the types of alignment KGs covering bilingual and monolingual, with standard (non-iterative) and iterative training paradigms to evaluate the model performance. Our research indicates that, in the face of modality incompleteness, models succumb to overfitting the modality noise, and exhibit performance oscillations or declines at high rates of missing modality. This proves that the inclusion of additional multi-modal data can sometimes adversely affect EA. To address these challenges, we introduce UMAEA , a robust multi-modal entity alignment approach designed to tackle uncertainly missing and ambiguous visual modalities. It consistently achieves SOTA performance across all 97 benchmark splits, significantly surpassing existing baselines with limited parameters and time consumption, while effectively alleviating the identified limitations of other models. Our code and benchmark data are available at https://github.com/zjukg/UMAEA.
</details>
<details>
<summary>摘要</summary>
如果你想要学习多modalentity alignment（MMEA），这篇文章可能会帮助你。MMEA是一种扩展entity alignment（EA）的技术，用于在不同知识图（KG）之间标识相同的实体，通过利用相关的视觉信息。然而，现有的MMEA方法主要集中在多modalentity特征的 fusions paradigma，而忽视了视觉图像中的普遍现象：缺失和内在的模糊性。在这篇文章中，我们进行了更进一步的视觉模态不完整性的分析，并使用我们提出的MMEA-UMVM数据集进行了最新的MMEA模型的benchmarking。我们的研究表明，在面临视觉模态不完整性时，模型容易过拟合modal noise，并且在高比例的缺失modal时会出现性能波动或下降。这表明，在 inclusion of additional multi-modal data可以有时对EA造成负面影响。为了解决这些挑战，我们提出了UMAEA，一种robust的多modalentity alignment方法，可以有效地处理不确定、缺失和模糊的视觉模态。它在所有97个benchmark split中均 achievement SOTA表现，明显超过了现有的基eline，同时具有限制parameters和时间消耗的优势。我们的代码和benchmark数据可以在https://github.com/zjukg/UMAEA中找到。
</details></li>
</ul>
<hr>
<h2 id="Around-the-GLOBE-Numerical-Aggregation-Question-Answering-on-Heterogeneous-Genealogical-Knowledge-Graphs-with-Deep-Neural-Networks"><a href="#Around-the-GLOBE-Numerical-Aggregation-Question-Answering-on-Heterogeneous-Genealogical-Knowledge-Graphs-with-Deep-Neural-Networks" class="headerlink" title="Around the GLOBE: Numerical Aggregation Question-Answering on Heterogeneous Genealogical Knowledge Graphs with Deep Neural Networks"></a>Around the GLOBE: Numerical Aggregation Question-Answering on Heterogeneous Genealogical Knowledge Graphs with Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16208">http://arxiv.org/abs/2307.16208</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omri Suissa, Maayan Zhitomirsky-Geffet, Avshalom Elmalech</li>
<li>for: 这个研究旨在提供一种数字化文化遗产领域中的 numeral 聚合Question Answering（QA）方法，以便帮助研究者（以及一般公众）对大量文献进行 distant 读取和分析。</li>
<li>methods: 该研究使用了一种基于 transformer 的 end-to-end 方法，包括自动生成训练数据集、transformer 基于表格选择方法、优化的 transformer 基于 numeral 聚合 QA 模型。</li>
<li>results: 研究发现，提出的方法（GLOBE）在这个任务上的准确率为 87%，比现有的模型和管道高出 66%。这种方法可能有实际应用在 genealogical 信息中心和博物馆，使 genealogical 数据研究变得容易和可扩展。<details>
<summary>Abstract</summary>
One of the key AI tools for textual corpora exploration is natural language question-answering (QA). Unlike keyword-based search engines, QA algorithms receive and process natural language questions and produce precise answers to these questions, rather than long lists of documents that need to be manually scanned by the users. State-of-the-art QA algorithms based on DNNs were successfully employed in various domains. However, QA in the genealogical domain is still underexplored, while researchers in this field (and other fields in humanities and social sciences) can highly benefit from the ability to ask questions in natural language, receive concrete answers and gain insights hidden within large corpora. While some research has been recently conducted for factual QA in the genealogical domain, to the best of our knowledge, there is no previous research on the more challenging task of numerical aggregation QA (i.e., answering questions combining aggregation functions, e.g., count, average, max). Numerical aggregation QA is critical for distant reading and analysis for researchers (and the general public) interested in investigating cultural heritage domains. Therefore, in this study, we present a new end-to-end methodology for numerical aggregation QA for genealogical trees that includes: 1) an automatic method for training dataset generation; 2) a transformer-based table selection method, and 3) an optimized transformer-based numerical aggregation QA model. The findings indicate that the proposed architecture, GLOBE, outperforms the state-of-the-art models and pipelines by achieving 87% accuracy for this task compared to only 21% by current state-of-the-art models. This study may have practical implications for genealogical information centers and museums, making genealogical data research easy and scalable for experts as well as the general public.
</details>
<details>
<summary>摘要</summary>
一种关键的人工智能工具 для文本 corpus 探索是自然语言问答 (QA)。不同于关键词搜索引擎，QA 算法会根据自然语言问题提供和处理精确的答案，而不是需要用户手动扫描大量文档。现状的QA算法基于深度学习神经网络 (DNN) 在不同领域得到了成功应用。然而，在家谱领域，QA仍然受到了不 enough 的研究，而家谱领域的研究人员 (以及人文社科领域的其他研究人员) 可以很大程度上受益于可以使用自然语言问题提出问题，并获得文档中隐藏的智能。虽然有些最近的研究已经对家谱领域的事实 QA 进行了研究，但到目前为止，没有任何一项研究关于更加复杂的数字积算 QA (即回答组合函数，例如计数、平均值、最大值)。数字积算 QA 对于远程阅读和分析是非常重要的，因此在这种情况下，我们提出了一种新的综合方法。方法包括：1. 自动生成训练数据集方法2. 基于 transformer 的表格选择方法3. 优化 transformer 基于数字积算 QA 模型结果表明，我们提出的建筑 GLOBE 模型在这种任务上的准确率为 87%，比现状最佳模型和管道的准确率（21%）高出了大幅度。这种研究可能对家谱信息中心和博物馆产生实质性的实用效果，使家谱数据研究变得容易且可扩展。
</details></li>
</ul>
<hr>
<h2 id="Deep-Convolutional-Neural-Networks-with-Zero-Padding-Feature-Extraction-and-Learning"><a href="#Deep-Convolutional-Neural-Networks-with-Zero-Padding-Feature-Extraction-and-Learning" class="headerlink" title="Deep Convolutional Neural Networks with Zero-Padding: Feature Extraction and Learning"></a>Deep Convolutional Neural Networks with Zero-Padding: Feature Extraction and Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16203">http://arxiv.org/abs/2307.16203</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liubc17/eDCNN_zero_padding">https://github.com/liubc17/eDCNN_zero_padding</a></li>
<li>paper_authors: Zhi Han, Baichen Liu, Shao-Bo Lin, Ding-Xuan Zhou</li>
<li>for: 这 paper 研究了深度卷积神经网络 (DCNNs) 中 zero-padding 的表现。</li>
<li>methods: 这 paper 使用了 zero-padding 在特征提取和学习中，并验证了 zero-padding 的翻译相对性和抽象层 pooling 的翻译不变性。</li>
<li>results: 这 paper 显示了任何深度全连接神经网络 (DFCNs) 都可以通过 DCNNs  WITH zero-padding 来表示，这表明 DCNNs  WITH zero-padding 在特征提取方面比 DFCNs 更好。此外， paper 还证明了 DCNNs  WITH zero-padding 的通用一致性和学习过程中的翻译不变性。<details>
<summary>Abstract</summary>
This paper studies the performance of deep convolutional neural networks (DCNNs) with zero-padding in feature extraction and learning. After verifying the roles of zero-padding in enabling translation-equivalence, and pooling in its translation-invariance driven nature, we show that with similar number of free parameters, any deep fully connected networks (DFCNs) can be represented by DCNNs with zero-padding. This demonstrates that DCNNs with zero-padding is essentially better than DFCNs in feature extraction. Consequently, we derive universal consistency of DCNNs with zero-padding and show its translation-invariance in the learning process. All our theoretical results are verified by numerical experiments including both toy simulations and real-data running.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Shuffled-Differentially-Private-Federated-Learning-for-Time-Series-Data-Analytics"><a href="#Shuffled-Differentially-Private-Federated-Learning-for-Time-Series-Data-Analytics" class="headerlink" title="Shuffled Differentially Private Federated Learning for Time Series Data Analytics"></a>Shuffled Differentially Private Federated Learning for Time Series Data Analytics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16196">http://arxiv.org/abs/2307.16196</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenxi Huang, Chaoyang Jiang, Zhenghua Chen</li>
<li>for: 这个研究旨在提供一个隐私保证的联合学习算法，以便在时间序列数据上进行优化性的表现，同时保证客户端的隐私。</li>
<li>methods: 本研究使用了本地差异隐私和排序技术，以实现隐私增强和隐私扩大。具体来说，本研究使用了本地差异隐私来扩展隐私保证的信任范围到客户端，并使用排序技术来实现隐私增强。</li>
<li>results: 实验结果显示，在小客户和大客户情况下，本研究的方法仅对应不同程度的精度损失，而且在同等隐私保证水平下，本研究的方法比中央差异隐私联合学习方法表现更好。<details>
<summary>Abstract</summary>
Trustworthy federated learning aims to achieve optimal performance while ensuring clients' privacy. Existing privacy-preserving federated learning approaches are mostly tailored for image data, lacking applications for time series data, which have many important applications, like machine health monitoring, human activity recognition, etc. Furthermore, protective noising on a time series data analytics model can significantly interfere with temporal-dependent learning, leading to a greater decline in accuracy. To address these issues, we develop a privacy-preserving federated learning algorithm for time series data. Specifically, we employ local differential privacy to extend the privacy protection trust boundary to the clients. We also incorporate shuffle techniques to achieve a privacy amplification, mitigating the accuracy decline caused by leveraging local differential privacy. Extensive experiments were conducted on five time series datasets. The evaluation results reveal that our algorithm experienced minimal accuracy loss compared to non-private federated learning in both small and large client scenarios. Under the same level of privacy protection, our algorithm demonstrated improved accuracy compared to the centralized differentially private federated learning in both scenarios.
</details>
<details>
<summary>摘要</summary>
信任worthy的联合学习目标是实现最佳性能，同时保障客户端的隐私。现有的隐私保护联合学习方法大多是针对图像数据，缺乏应用于时间序列数据，这种数据在机器健康监测、人类活动识别等领域有重要应用。此外，在时间序列数据分析模型上加入保护噪声可能会对时间相互dependent的学习产生干扰，导致准确率下降。为解决这些问题，我们开发了一种针对时间序列数据的隐私保护联合学习算法。具体来说，我们使用本地差分隐私来扩展隐私保护信任圈到客户端。我们还 incorporate 混淆技术来实现隐私增强， Mitigating the accuracy decline caused by leveraging local differential privacy。我们对五个时间序列数据集进行了广泛的实验。结果表明，我们的算法在小客户和大客户场景中都体现出较少的准确率下降，与非隐私联合学习相比。在同一个隐私保护水平下，我们的算法在两个场景中表现出比中央差分隐私联合学习更好的准确率表现。
</details></li>
</ul>
<hr>
<h2 id="An-Efficient-Approach-to-Mitigate-Numerical-Instability-in-Backpropagation-for-16-bit-Neural-Network-Training"><a href="#An-Efficient-Approach-to-Mitigate-Numerical-Instability-in-Backpropagation-for-16-bit-Neural-Network-Training" class="headerlink" title="An Efficient Approach to Mitigate Numerical Instability in Backpropagation for 16-bit Neural Network Training"></a>An Efficient Approach to Mitigate Numerical Instability in Backpropagation for 16-bit Neural Network Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16189">http://arxiv.org/abs/2307.16189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juyoung Yun</li>
<li>for: 解决深度神经网络训练阶段的数值不稳定性问题，尤其是在使用流行的优化算法RMSProp和Adam时。</li>
<li>methods: 通过深入探究epsillon参数在这些优化算法中的作用，发现epsillon的值可以影响数值稳定性，并提出了一种新的约束方法来缓解这些问题。</li>
<li>results: 该方法可以有效约束数值不稳定性问题，使得16位计算中的深度神经网络训练得到改进，并开启了更有效和稳定的模型训练新途径。<details>
<summary>Abstract</summary>
In this research, we delve into the intricacies of the numerical instability observed in 16-bit computations of machine learning models, particularly when employing popular optimization algorithms such as RMSProp and Adam. This instability is commonly experienced during the training phase of deep neural networks, leading to disrupted learning processes and hindering the effective deployment of such models. We identify the single hyperparameter, epsilon, as the main culprit behind this numerical instability. An in-depth exploration of the role of epsilon in these optimizers within 16-bit computations reveals that a minor adjustment of its value can restore the functionality of RMSProp and Adam, consequently enabling the effective utilization of 16-bit neural networks. We propose a novel method to mitigate the identified numerical instability issues. This method capitalizes on the updates from the Adam optimizer and significantly improves the robustness of the learning process in 16-bit computations. This study contributes to better understanding of optimization in low-precision computations and provides an effective solution to a longstanding issue in training deep neural networks, opening new avenues for more efficient and stable model training.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ESP-Exploiting-Symmetry-Prior-for-Multi-Agent-Reinforcement-Learning"><a href="#ESP-Exploiting-Symmetry-Prior-for-Multi-Agent-Reinforcement-Learning" class="headerlink" title="ESP: Exploiting Symmetry Prior for Multi-Agent Reinforcement Learning"></a>ESP: Exploiting Symmetry Prior for Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16186">http://arxiv.org/abs/2307.16186</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Yu, Rongye Shi, Pu Feng, Yongkai Tian, Jie Luo, Wenjun Wu</li>
<li>for: 本文提出了一种基于多智能探索学习（MARL）的框架，用于利用先验知识来提高数据效率。</li>
<li>methods: 本文提出了一种将数据增强和一种有judicious设计的一致损失函数integrated into current MARL方法中。</li>
<li>results: 实验结果表明，提出的方法可以在多种复杂任务上提高数据效率，并且在物理多机器人测试环境中显示出优势。<details>
<summary>Abstract</summary>
Multi-agent reinforcement learning (MARL) has achieved promising results in recent years. However, most existing reinforcement learning methods require a large amount of data for model training. In addition, data-efficient reinforcement learning requires the construction of strong inductive biases, which are ignored in the current MARL approaches. Inspired by the symmetry phenomenon in multi-agent systems, this paper proposes a framework for exploiting prior knowledge by integrating data augmentation and a well-designed consistency loss into the existing MARL methods. In addition, the proposed framework is model-agnostic and can be applied to most of the current MARL algorithms. Experimental tests on multiple challenging tasks demonstrate the effectiveness of the proposed framework. Moreover, the proposed framework is applied to a physical multi-robot testbed to show its superiority.
</details>
<details>
<summary>摘要</summary>
多智能 reinforcement learning (MARL) 在过去几年取得了有前途的成果。然而，现有的 reinforcement learning 方法通常需要大量数据来训练模型。此外，数据效率 reinforcement learning 需要建立强大的推理假设，这些假设在当前的 MARL 方法中被忽略。 inspirited by the symmetry phenomenon in multi-agent systems, this paper proposes a framework for exploiting prior knowledge by integrating data augmentation and a well-designed consistency loss into the existing MARL methods. In addition, the proposed framework is model-agnostic and can be applied to most of the current MARL algorithms. Experimental tests on multiple challenging tasks demonstrate the effectiveness of the proposed framework. Moreover, the proposed framework is applied to a physical multi-robot testbed to show its superiority.Note: The translation is written in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Unified-Model-for-Image-Video-Audio-and-Language-Tasks"><a href="#Unified-Model-for-Image-Video-Audio-and-Language-Tasks" class="headerlink" title="Unified Model for Image, Video, Audio and Language Tasks"></a>Unified Model for Image, Video, Audio and Language Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16184">http://arxiv.org/abs/2307.16184</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mshukor/unival">https://github.com/mshukor/unival</a></li>
<li>paper_authors: Mustafa Shukor, Corentin Dancette, Alexandre Rame, Matthieu Cord</li>
<li>for: 这个论文的目标是建立一个可以支持多种模式和任务的大型语言模型（LLM），以实现通用Agent的梦想。</li>
<li>methods: 作者提出了一种基于任务均衡和多模式课程学习的方法，称为UnIVAL，可以有效地支持多种模式和任务。</li>
<li>results: 对于图像和视频文本任务，UnIVAL模型表现竞争力强，而且可以通过对audio文本任务进行训练而达到竞争力水平，即使没有直接使用audio模式进行训练。此外，作者还提出了一种多模式模型融合方法，通过将不同多模式任务的模型权重进行插值来实现。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have made the ambitious quest for generalist agents significantly far from being a fantasy. A key hurdle for building such general models is the diversity and heterogeneity of tasks and modalities. A promising solution is unification, allowing the support of a myriad of tasks and modalities within one unified framework. While few large models (e.g., Flamingo (Alayrac et al., 2022), trained on massive datasets, can support more than two modalities, current small to mid-scale unified models are still limited to 2 modalities, usually image-text or video-text. The question that we ask is: is it possible to build efficiently a unified model that can support all modalities? To answer this, we propose UnIVAL, a step further towards this ambitious goal. Without relying on fancy datasets sizes or models with billions of parameters, the ~ 0.25B parameter UnIVAL model goes beyond two modalities and unifies text, images, video, and audio into a single model. Our model is efficiently pretrained on many tasks, based on task balancing and multimodal curriculum learning. UnIVAL shows competitive performance to existing state-of-the-art approaches, across image and video-text tasks. The feature representations learned from image and video-text modalities, allows the model to achieve competitive performance when finetuned on audio-text tasks, despite not being pretrained on audio. Thanks to the unified model, we propose a novel study on multimodal model merging via weight interpolation of models trained on different multimodal tasks, showing their benefits in particular for out-of-distribution generalization. Finally, we motivate unification by showing the synergy between tasks. The model weights and code are released here: https://github.com/mshukor/UnIVAL.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）已经让普通的通用代理人成为了现实。一个关键的障碍是任务和模式的多样性和不同性。一个有 Promise的解决方案是统一，允许支持多种任务和模式的内部框架。虽然有些大型模型（如Flamingo（Alayrac等，2022））可以支持更多 чем两种模式，当前小规模的统一模型仍然受限于2种模式，通常是图像文本或视频文本。我们提出的问题是：是否可以有效地建立一个统一模型，可以支持所有的模式？为了回答这个问题，我们提出了UnIVAL，是一个更进一步的目标。不需要庞大的数据集或多亿参数的模型，UnIVAL模型的约0.25B参数可以超越两种模式，并将文本、图像、视频和音频 integrate到一个模型中。我们的模型通过多任务的补做和多模式的学习环境来高效预训练。UnIVAL在图像和视频文本任务上显示出与现状最佳的性能，而且通过在音频文本任务上训练模型，即使模型没有直接预训练音频，也可以达到竞争性表现。此外，我们还提出了一种新的研究方法，即通过多模式模型的权重插值来合并不同的多模式任务，并证明其在特定的异常情况下的优势。最后，我们驱动了统一的理念，证明任务之间的共同性。模型参数和代码在GitHub上发布：https://github.com/mshukor/UnIVAL。
</details></li>
</ul>
<hr>
<h2 id="Redundancy-aware-unsupervised-rankings-for-collections-of-gene-sets"><a href="#Redundancy-aware-unsupervised-rankings-for-collections-of-gene-sets" class="headerlink" title="Redundancy-aware unsupervised rankings for collections of gene sets"></a>Redundancy-aware unsupervised rankings for collections of gene sets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16182">http://arxiv.org/abs/2307.16182</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chiara Balestra, Carlo Maj, Emmanuel Müller, Andreas Mayr</li>
<li>for: 本研究的目的是增加生物信息学中的集合（gene set）的可解释性和可读性，通过将重要性分数用于集合中的路径来排序和简化集合。</li>
<li>methods: 本研究使用了Shapley值来计算重要性分数，并提出了一种缓解常见的幂数复杂性问题的技巧。此外，本研究还包括了一种 redundancy 意识的排序方法，以便在获得的排序中包括重复的集合。</li>
<li>results: 本研究的结果表明，使用重要性分数可以减少集合中的维度，同时保持高度的覆盖率。此外，本研究还发现，通过包括重复的集合在内，可以提高Gene Sets Enrichment Analysis的有用性。<details>
<summary>Abstract</summary>
The biological roles of gene sets are used to group them into collections. These collections are often characterized by being high-dimensional, overlapping, and redundant families of sets, thus precluding a straightforward interpretation and study of their content. Bioinformatics looked for solutions to reduce their dimension or increase their intepretability. One possibility lies in aggregating overlapping gene sets to create larger pathways, but the modified biological pathways are hardly biologically justifiable. We propose to use importance scores to rank the pathways in the collections studying the context from a set covering perspective. The proposed Shapley values-based scores consider the distribution of the singletons and the size of the sets in the families; Furthermore, a trick allows us to circumvent the usual exponential complexity of Shapley values' computation. Finally, we address the challenge of including a redundancy awareness in the obtained rankings where, in our case, sets are redundant if they show prominent intersections.   The rankings can be used to reduce the dimension of collections of gene sets, such that they show lower redundancy and still a high coverage of the genes. We further investigate the impact of our selection on Gene Sets Enrichment Analysis. The proposed method shows a practical utility in bioinformatics to increase the interpretability of the collections of gene sets and a step forward to include redundancy into Shapley values computations.
</details>
<details>
<summary>摘要</summary>
生物学角色集合用于将集合分组。这些集合经常是高维ensional，重叠，并且异常的家族集合，从而阻碍直接解释和研究其内容。生物信息学寻找解决方案来减少其维度或增加其可读性。一种可能性在于将重叠的基因集合聚合成更大的路径，但修改的生物路径几乎不可能被生物学上正确地证明。我们提议使用重要性分数来排序路径集合，研究从集合覆盖角度来看。我们的提案基于值得推荐的值来计算分数，考虑集合内单个基因的分布和集合大小。此外，我们还使用一种技巧来绕过通常的指数复杂性计算值得推荐的值。最后，我们解决了包含重复性意识的获得的排名中的挑战，在我们的情况下，集合是重复的，如果它们显著交叉。这些排名可以用来减少集合基因集的维度，以便它们仍然具有较高的覆盖率。我们进一步调查了我们的选择对基因集sets强化分析的影响。我们的方法显示了生物信息学中实用的一步，以增加集合基因集的可读性，并且是包含重复性的Shapley值计算的一步进行。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-learning-of-density-ratios-in-RKHS"><a href="#Adaptive-learning-of-density-ratios-in-RKHS" class="headerlink" title="Adaptive learning of density ratios in RKHS"></a>Adaptive learning of density ratios in RKHS</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16164">http://arxiv.org/abs/2307.16164</a></li>
<li>repo_url: None</li>
<li>paper_authors: Werner Zellinger, Stefan Kindermann, Sergei V. Pereverzyev</li>
<li>for: 这篇论文关注于估计两个概率密度之间的比率，从数据分布中启发的机器学习和统计问题。</li>
<li>methods: 本文研究了一种使用正则化布雷格曼异常的拟合器希尔伯特空间（RKHS）中的概率密度比率估计方法，并提出了新的finite-sample error bounds。</li>
<li>results: 本文的方法可以在特定情况下实现最优的最小化error rate，并且可以自动选择最佳参数。<details>
<summary>Abstract</summary>
Estimating the ratio of two probability densities from finitely many observations of the densities is a central problem in machine learning and statistics with applications in two-sample testing, divergence estimation, generative modeling, covariate shift adaptation, conditional density estimation, and novelty detection. In this work, we analyze a large class of density ratio estimation methods that minimize a regularized Bregman divergence between the true density ratio and a model in a reproducing kernel Hilbert space (RKHS). We derive new finite-sample error bounds, and we propose a Lepskii type parameter choice principle that minimizes the bounds without knowledge of the regularity of the density ratio. In the special case of quadratic loss, our method adaptively achieves a minimax optimal error rate. A numerical illustration is provided.
</details>
<details>
<summary>摘要</summary>
计算两个概率密度的比率从有限多个观察值中是机器学习和统计中的中心问题，具有应用于两样本测试、差异估计、生成模型、covariate shift适应、Conditional density估计和新事物探测等领域。在这种工作中，我们分析了一大类density比率估计方法，这些方法在一个 reproduce kernel Hilbert space（RKHS）中减少了一个规范化Bregman divergence的值。我们提出了新的finite-sample error bounds，并提出了一种Lepskii类型参数选择原则，该原则可以在不知道概率密度比率的正规性情况下最小化 bounds。在特定的quadratic loss情况下，我们的方法可以自适应实现一个最优的error rate。一个数字图文示例。
</details></li>
</ul>
<hr>
<h2 id="Variance-Control-for-Distributional-Reinforcement-Learning"><a href="#Variance-Control-for-Distributional-Reinforcement-Learning" class="headerlink" title="Variance Control for Distributional Reinforcement Learning"></a>Variance Control for Distributional Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16152">http://arxiv.org/abs/2307.16152</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kuangqi927/qem">https://github.com/kuangqi927/qem</a></li>
<li>paper_authors: Qi Kuang, Zhoufan Zhu, Liwen Zhang, Fan Zhou</li>
<li>for: 这个论文主要研究了分布式强化学习（DRL）中获得的Q函数估计器的有效性。</li>
<li>methods: 作者通过分析错误的方法来了解Q函数估计器在分布式 Setting中的误差，并构建了一种新的估计器——量化扩展均值（QEM）。</li>
<li>results: 作者在多个Atari和Mujoco标准任务上广泛评估了他们的QEMRL算法，并证明了它在样本效率和吞吐量性方面达到了 significiant改进。<details>
<summary>Abstract</summary>
Although distributional reinforcement learning (DRL) has been widely examined in the past few years, very few studies investigate the validity of the obtained Q-function estimator in the distributional setting. To fully understand how the approximation errors of the Q-function affect the whole training process, we do some error analysis and theoretically show how to reduce both the bias and the variance of the error terms. With this new understanding, we construct a new estimator \emph{Quantiled Expansion Mean} (QEM) and introduce a new DRL algorithm (QEMRL) from the statistical perspective. We extensively evaluate our QEMRL algorithm on a variety of Atari and Mujoco benchmark tasks and demonstrate that QEMRL achieves significant improvement over baseline algorithms in terms of sample efficiency and convergence performance.
</details>
<details>
<summary>摘要</summary>
尽管分布式强化学习（DRL）在过去几年中得到了广泛的研究，但是非常少的研究检查了在分布上得到的Q函数估计器的有效性。为了全面理解Q函数估计器的抽象错误对整个训练过程的影响，我们进行了错误分析并从统计角度提出了一种新的估计器——量词扩展含义（QEM），以及一种基于统计学的新DRL算法（QEMRL）。我们对多种Atari和Mujoco benchmark任务进行了广泛的评估，并证明了QEMRL在样本效率和收敛性方面具有显著改善。
</details></li>
</ul>
<hr>
<h2 id="An-Effective-LSTM-DDPM-Scheme-for-Energy-Theft-Detection-and-Forecasting-in-Smart-Grid"><a href="#An-Effective-LSTM-DDPM-Scheme-for-Energy-Theft-Detection-and-Forecasting-in-Smart-Grid" class="headerlink" title="An Effective LSTM-DDPM Scheme for Energy Theft Detection and Forecasting in Smart Grid"></a>An Effective LSTM-DDPM Scheme for Energy Theft Detection and Forecasting in Smart Grid</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16149">http://arxiv.org/abs/2307.16149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xun Yuan, Yang Yang, Arwa Alromih, Prosanta Gope, Biplab Sikdar</li>
<li>for: 本研究目的是解决智能电网系统中的能源偷窃检测和能源消耗预测两大挑战。</li>
<li>methods: 该研究提出的解决方案 combining long short-term memory（LSTM）和杂噪扩散概率模型（DDPM），通过生成输入重建和预测来实现输入重建和预测错误。</li>
<li>results: 经过广泛的实验，该研究的方案在智能电网系统中的能源偷窃检测和能源消耗预测问题中表现出色，与基eline方法相比，该方案可以更好地检测能源偷窃攻击。<details>
<summary>Abstract</summary>
Energy theft detection (ETD) and energy consumption forecasting (ECF) are two interconnected challenges in smart grid systems. Addressing these issues collectively is crucial for ensuring system security. This paper addresses the interconnected challenges of ETD and ECF in smart grid systems. The proposed solution combines long short-term memory (LSTM) and a denoising diffusion probabilistic model (DDPM) to generate input reconstruction and forecasting. By leveraging the reconstruction and forecasting errors, the system identifies instances of energy theft, with the methods based on reconstruction error and forecasting error complementing each other in detecting different types of attacks. Through extensive experiments on real-world and synthetic datasets, the proposed scheme outperforms baseline methods in ETD and ECF problems. The ensemble method significantly enhances ETD performance, accurately detecting energy theft attacks that baseline methods fail to detect. The research offers a comprehensive and effective solution for addressing ETD and ECF challenges, demonstrating promising results and improved security in smart grid systems.
</details>
<details>
<summary>摘要</summary>
智能电网系统中的能源盗取检测（ETD）和能源消耗预测（ECF）是两个相互连接的挑战。它们的解决需要共同进行，以确保系统安全。这篇论文解决了智能电网系统中的ETD和ECF两个挑战。提议的解决方案结合了长期短期记忆（LSTM）和杂散抽取概率模型（DDPM），生成输入重建和预测。通过利用重建和预测错误，系统可以识别能源盗取行为，基于重建错误和预测错误，不同类型的攻击可以得到优秀的识别。经过了实验，提议的方案在实际和synthetic数据集上出现了显著的改进，与基线方法相比，ETD和ECF问题中的性能显著提高。ensemble方法在ETD问题中显示出了杰出的表现，可以准确地检测基eline方法无法检测的能源盗取攻击。这项研究提供了智能电网系统中ETD和ECF问题的全面和有效的解决方案，实验结果表明，该方案在安全性方面具有明显的改进。
</details></li>
</ul>
<hr>
<h2 id="Pupil-Learning-Mechanism"><a href="#Pupil-Learning-Mechanism" class="headerlink" title="Pupil Learning Mechanism"></a>Pupil Learning Mechanism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16141">http://arxiv.org/abs/2307.16141</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rua-Huan Tsaih, Yu-Hang Chien, Shih-Yi Chien</li>
<li>for: 本研究目的是解决人工神经网络中的消逝梯度和透过率问题。</li>
<li>methods: 本研究采用了诊眼学习过程，包括解释、选择、理解、填充和组织等步骤， derivation of the pupil learning mechanism (PLM) for modifying the network structure and weights of 2-layer neural networks (2LNNs).</li>
<li>results: 实验结果表明，PLM模块设计得当，并且提出的PLM模型在对比线性回归模型和传统的反propagation-based 2LNN模型的基础上显示出了超越性。<details>
<summary>Abstract</summary>
Studies on artificial neural networks rarely address both vanishing gradients and overfitting issues. In this study, we follow the pupil learning procedure, which has the features of interpreting, picking, understanding, cramming, and organizing, to derive the pupil learning mechanism (PLM) by which to modify the network structure and weights of 2-layer neural networks (2LNNs). The PLM consists of modules for sequential learning, adaptive learning, perfect learning, and less-overfitted learning. Based upon a copper price forecasting dataset, we conduct an experiment to validate the PLM module design modules, and an experiment to evaluate the performance of PLM. The empirical results indeed approve the PLM module design and show the superiority of the proposed PLM model over the linear regression model and the conventional backpropagation-based 2LNN model.
</details>
<details>
<summary>摘要</summary>
研究人工神经网络很少考虑过量衰减和过拟合问题。在这项研究中，我们采用了学生学习过程，具有解释、选择、理解、填充和组织等特点， derivation of pupil learning mechanism (PLM)，用于修改网络结构和权重。PLM包括顺序学习模块、适应学习模块、完美学习模块和较少过拟合学习模块。基于一个铜价预测数据集，我们进行了实验验证PLM模块设计和PLM模型的性能。实验结果确实证明了PLM模块的设计和提案的PLM模型的优越性，比Linear Regression模型和传统的反射层次神经网络模型更好。
</details></li>
</ul>
<hr>
<h2 id="User-Controlled-Knowledge-Fusion-in-Large-Language-Models-Balancing-Creativity-and-Hallucination"><a href="#User-Controlled-Knowledge-Fusion-in-Large-Language-Models-Balancing-Creativity-and-Hallucination" class="headerlink" title="User-Controlled Knowledge Fusion in Large Language Models: Balancing Creativity and Hallucination"></a>User-Controlled Knowledge Fusion in Large Language Models: Balancing Creativity and Hallucination</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16139">http://arxiv.org/abs/2307.16139</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Zhang</li>
<li>for:  This paper aims to improve the balance between the creativity and accuracy of large language models (LLMs) by introducing a user-controllable mechanism that modulates the degree of faithfulness to external knowledge.</li>
<li>methods:  The proposed approach uses a numerical tag during the fine-tuning phase of the LLM’s training to represent the degree of faithfulness to reference knowledge. The tag is computed through a combination of ROUGE scores, Sentence-BERT embeddings, and the LLM’s self-evaluation score. During model inference, users can manipulate the tag to control the degree of the LLM’s reliance on external knowledge.</li>
<li>results:  The paper presents extensive experiments across various scenarios, demonstrating the adaptability and efficacy of the proposed approach in ensuring the quality and accuracy of the LLM’s responses. The results show that the approach can enhance the versatility of LLMs while maintaining a balance between creativity and hallucination.<details>
<summary>Abstract</summary>
In modern dialogue systems, the use of Large Language Models (LLMs) has grown exponentially due to their capacity to generate diverse, relevant, and creative responses. Despite their strengths, striking a balance between the LLMs' creativity and their faithfulness to external knowledge remains a key challenge. This paper presents an innovative user-controllable mechanism that modulates the balance between an LLM's imaginative capabilities and its adherence to factual information. Our approach incorporates a numerical tag during the fine-tuning phase of the LLM's training, representing the degree of faithfulness to the reference knowledge in the generated responses. This degree is computed through an automated process that measures lexical overlap using ROUGE scores, semantic similarity using Sentence-BERT embeddings, and an LLM's self-evaluation score. During model inference, users can manipulate this numerical tag, thus controlling the degree of the LLM's reliance on external knowledge. We conduct extensive experiments across various scenarios, demonstrating the adaptability of our method and its efficacy in ensuring the quality and accuracy of the LLM's responses. The results highlight the potential of our approach to enhance the versatility of LLMs while maintaining a balance between creativity and hallucination.
</details>
<details>
<summary>摘要</summary>
现代对话系统中，大语言模型（LLM）的使用量已经呈指数增长趋势，这是因为它们可以生成多样、相关、创新的回答。然而，在这些模型的应用中，保持LLM的创造力和外部知识的准确性之间的平衡仍然是一个关键挑战。这篇论文提出了一种新的用户可控的机制，可以调整LLM的创造力和外部知识的准确性之间的平衡。我们的方法在训练LLM的细化阶段添加了一个数字标签，表示生成回答中对参考知识的忠诚度的程度。这个程度通过自动化的过程计算，使用ROUGE分数、句子BERT嵌入和LLM自我评价分数来测量lexical overlap和semantic similarity。在模型推理阶段，用户可以控制这个数字标签，以控制LLM对外部知识的依赖程度。我们进行了多种enario的广泛实验，证明了我们的方法的适应性和精度。结果显示了我们的方法可以提高LLM的多样性，同时保持创造力和幻想的平衡。
</details></li>
</ul>
<hr>
<h2 id="Deep-Unrolling-Networks-with-Recurrent-Momentum-Acceleration-for-Nonlinear-Inverse-Problems"><a href="#Deep-Unrolling-Networks-with-Recurrent-Momentum-Acceleration-for-Nonlinear-Inverse-Problems" class="headerlink" title="Deep Unrolling Networks with Recurrent Momentum Acceleration for Nonlinear Inverse Problems"></a>Deep Unrolling Networks with Recurrent Momentum Acceleration for Nonlinear Inverse Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16120">http://arxiv.org/abs/2307.16120</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhouqp631/dunets-rma">https://github.com/zhouqp631/dunets-rma</a></li>
<li>paper_authors: Qingping Zhou, Jiayu Qian, Junqi Tang, Jinglai Li</li>
<li>for: 解决非线性 inverse 图像问题</li>
<li>methods: 使用 momentum 加速 (RMA) 框架，包括 long short-term memory Recurrent Neural Network (LSTM-RNN) 模型，以便学习和保留前一个梯度的知识。</li>
<li>results: 在 two 个非线性 inverse 问题中，提供了实验结果，其中第一个问题中的改进效果随着问题的非线性度而增大，第二个问题中的结果进一步证明了 RMA 方案可以在强不稳定问题中明显提高 DuNets 的性能。<details>
<summary>Abstract</summary>
Combining the strengths of model-based iterative algorithms and data-driven deep learning solutions, deep unrolling networks (DuNets) have become a popular tool to solve inverse imaging problems. While DuNets have been successfully applied to many linear inverse problems, nonlinear problems tend to impair the performance of the method. Inspired by momentum acceleration techniques that are often used in optimization algorithms, we propose a recurrent momentum acceleration (RMA) framework that uses a long short-term memory recurrent neural network (LSTM-RNN) to simulate the momentum acceleration process. The RMA module leverages the ability of the LSTM-RNN to learn and retain knowledge from the previous gradients. We apply RMA to two popular DuNets -- the learned proximal gradient descent (LPGD) and the learned primal-dual (LPD) methods, resulting in LPGD-RMA and LPD-RMA respectively. We provide experimental results on two nonlinear inverse problems: a nonlinear deconvolution problem, and an electrical impedance tomography problem with limited boundary measurements. In the first experiment we have observed that the improvement due to RMA largely increases with respect to the nonlinearity of the problem. The results of the second example further demonstrate that the RMA schemes can significantly improve the performance of DuNets in strongly ill-posed problems.
</details>
<details>
<summary>摘要</summary>
通过结合模型基于迭代算法和数据驱动深度学习解决方案，深度拥叠网络（DuNets）已成为解析逆问题的流行工具。虽然DuNets已成功应用于许多线性逆问题，但非线性问题往往会伤害方法的性能。 draw inspiration from momentum acceleration techniques often used in optimization algorithms, we propose a recurrent momentum acceleration (RMA) framework that uses a long short-term memory recurrent neural network (LSTM-RNN) to simulate the momentum acceleration process. The RMA module leverages the ability of the LSTM-RNN to learn and retain knowledge from the previous gradients. We apply RMA to two popular DuNets -- the learned proximal gradient descent (LPGD) and the learned primal-dual (LPD) methods, resulting in LPGD-RMA and LPD-RMA respectively. We provide experimental results on two nonlinear inverse problems: a nonlinear deconvolution problem, and an electrical impedance tomography problem with limited boundary measurements. In the first experiment we have observed that the improvement due to RMA largely increases with respect to the nonlinearity of the problem. The results of the second example further demonstrate that the RMA schemes can significantly improve the performance of DuNets in strongly ill-posed problems.Here's the translation in Traditional Chinese:通过结合模型基于迭代算法和数据驱动深度学习解决方案，深度拥叠网络（DuNets）已成为解析逆问题的流行工具。处理非线性问题的性能往往会受到方法的影响。 draw inspiration from momentum acceleration techniques often used in optimization algorithms, we propose a recurrent momentum acceleration (RMA) framework that uses a long short-term memory recurrent neural network (LSTM-RNN) to simulate the momentum acceleration process. The RMA module leverages the ability of the LSTM-RNN to learn and retain knowledge from the previous gradients. We apply RMA to two popular DuNets -- the learned proximal gradient descent (LPGD) and the learned primal-dual (LPD) methods, resulting in LPGD-RMA and LPD-RMA respectively. We provide experimental results on two nonlinear inverse problems: a nonlinear deconvolution problem, and an electrical impedance tomography problem with limited boundary measurements. In the first experiment we have observed that the improvement due to RMA largely increases with respect to the nonlinearity of the problem. The results of the second example further demonstrate that the RMA schemes can significantly improve the performance of DuNets in strongly ill-posed problems.
</details></li>
</ul>
<hr>
<h2 id="TMPNN-High-Order-Polynomial-Regression-Based-on-Taylor-Map-Factorization"><a href="#TMPNN-High-Order-Polynomial-Regression-Based-on-Taylor-Map-Factorization" class="headerlink" title="TMPNN: High-Order Polynomial Regression Based on Taylor Map Factorization"></a>TMPNN: High-Order Polynomial Regression Based on Taylor Map Factorization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16105">http://arxiv.org/abs/2307.16105</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/andiva/tmpnn">https://github.com/andiva/tmpnn</a></li>
<li>paper_authors: Andrei Ivanov, Stefan Maria Ailuro</li>
<li>for: 这 paper 用于构建高阶多变量回归模型，以解决非线性模式的表达问题。</li>
<li>methods: 该方法基于泰勒地图分解，自然地实现多目标回归和内部关系的捕捉。</li>
<li>results: 通过对 UCI 开放访问数据集、Feynman 符号回归数据集和 Friedman-1 数据集进行比较，提出的方法与现状强度回归方法相当，并在特定任务上超越它们。<details>
<summary>Abstract</summary>
Polynomial regression is widely used and can help to express nonlinear patterns. However, considering very high polynomial orders may lead to overfitting and poor extrapolation ability for unseen data. The paper presents a method for constructing a high-order polynomial regression based on the Taylor map factorization. This method naturally implements multi-target regression and can capture internal relationships between targets. Additionally, we introduce an approach for model interpretation in the form of systems of differential equations. By benchmarking on UCI open access datasets, Feynman symbolic regression datasets, and Friedman-1 datasets, we demonstrate that the proposed method performs comparable to the state-of-the-art regression methods and outperforms them on specific tasks.
</details>
<details>
<summary>摘要</summary>
多项式回传函数广泛使用，可以表示非线性征式。然而，考虑非常高的多项式顺位可能会导致过拟合和未见数据的拟合能力不佳。本文提出一种基于泰勒对映缩减法的高顺位多项式回传函数建构方法。这种方法自然实现多目标回传和对目标之间的内部关系捕捉。此外，我们导入一种模型解释方法，即透过多元方程系统来实现。经过UCIC公开数据集、Feynman符号回传函数数据集和Friedman-1数据集的对比，我们示出了提案方法与现有回传方法相比，在特定任务上表现相似，甚至超过它们。
</details></li>
</ul>
<hr>
<h2 id="AI-Increases-Global-Access-to-Reliable-Flood-Forecasts"><a href="#AI-Increases-Global-Access-to-Reliable-Flood-Forecasts" class="headerlink" title="AI Increases Global Access to Reliable Flood Forecasts"></a>AI Increases Global Access to Reliable Flood Forecasts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16104">http://arxiv.org/abs/2307.16104</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research-datasets/global_streamflow_model_paper">https://github.com/google-research-datasets/global_streamflow_model_paper</a></li>
<li>paper_authors: Grey Nearing, Deborah Cohen, Vusumuzi Dube, Martin Gauch, Oren Gilon, Shaun Harrigan, Avinatan Hassidim, Frederik Kratzert, Asher Metzger, Sella Nevo, Florian Pappenberger, Christel Prudhomme, Guy Shalev, Shlomo Shenzis, Tadele Tekalign, Dana Weitzner, Yoss Matias</li>
<li>for: 这个论文的目的是为了开发一种基于人工智能的洪水预测模型，以提供更加准确和有效的洪水预测。</li>
<li>methods: 这个论文使用了人工智能技术来预测洪水事件，并对比了这种方法与现有的全球ydrology模型。</li>
<li>results: 这个论文的结果表明，使用人工智能模型可以在不同的大陆和返回期下实现更高的准确率和更早的预测时间，特别是在无测流域中。<details>
<summary>Abstract</summary>
Floods are one of the most common and impactful natural disasters, with a disproportionate impact in developing countries that often lack dense streamflow monitoring networks. Accurate and timely warnings are critical for mitigating flood risks, but accurate hydrological simulation models typically must be calibrated to long data records in each watershed where they are applied. We developed an Artificial Intelligence (AI) model to predict extreme hydrological events at timescales up to 7 days in advance. This model significantly outperforms current state of the art global hydrology models (the Copernicus Emergency Management Service Global Flood Awareness System) across all continents, lead times, and return periods. AI is especially effective at forecasting in ungauged basins, which is important because only a few percent of the world's watersheds have stream gauges, with a disproportionate number of ungauged basins in developing countries that are especially vulnerable to the human impacts of flooding. We produce forecasts of extreme events in South America and Africa that achieve reliability approaching the current state of the art in Europe and North America, and we achieve reliability at between 4 and 6-day lead times that are similar to current state of the art nowcasts (0-day lead time). Additionally, we achieve accuracies over 10-year return period events that are similar to current accuracies over 2-year return period events, meaning that AI can provide warnings earlier and over larger and more impactful events. The model that we develop in this paper has been incorporated into an operational early warning system that produces publicly available (free and open) forecasts in real time in over 80 countries. This work using AI and open data highlights a need for increasing the availability of hydrological data to continue to improve global access to reliable flood warnings.
</details>
<details>
<summary>摘要</summary>
洪水是一种非常常见且影响深远的自然灾害，尤其在发展中国家，那里缺乏密集的流量监测网络。准确和及时的警告对洪水风险的减轻具有极大的重要性，但是需要准确的水文模型进行准确地预测。我们开发了一种人工智能（AI）模型，用于预测7天内的极端水文事件。这种模型在全球各大洲、不同的领head time和返回期下都有显著的优势，特别是在不具有流量监测的河流basin中。因为只有很少的世界河流basin拥有流量监测，而发展中国家占这些basin的大多数，因此AI的预测特别有用。我们在南美和非洲预测极端事件的可靠性与现有的全球水文模型（Copernicus Emergency Management Service Global Flood Awareness System）相似，并且在4-6天的领head time内达到了类似的可靠性。此外，我们在10年返回期内达到了类似的准确率，这意味着AI可以提供更早的警告和更大的影响。我们在这篇论文中开发的模型已经被integrated into an operational early warning system，该系统在实时生成公共可用（免费开放）的预测。这种使用AI和开放数据的工作 highlights the need for increasing the availability of hydrological data to continue to improve global access to reliable flood warnings.
</details></li>
</ul>
<hr>
<h2 id="On-Neural-Network-approximation-of-ideal-adversarial-attack-and-convergence-of-adversarial-training"><a href="#On-Neural-Network-approximation-of-ideal-adversarial-attack-and-convergence-of-adversarial-training" class="headerlink" title="On Neural Network approximation of ideal adversarial attack and convergence of adversarial training"></a>On Neural Network approximation of ideal adversarial attack and convergence of adversarial training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16099">http://arxiv.org/abs/2307.16099</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rajdeep Haldar, Qifan Song</li>
<li>for: 本文针对了适用于防御模型的敌对攻击。</li>
<li>methods: 本文使用了一个trainable函数来表示敌对攻击，不需要进一步的梯度计算。首先， authors 证明了理论上最佳的攻击可以表示为平滑的piece-wise函数（piece-wise Holder函数）。然后， authors 使用了一个神经网络来近似这些函数。最后， authors 将理想的攻击过程模拟为一个神经网络，并将防御训练降到了一个数学游戏中。</li>
<li>results: 本文获得了防御训练中敌对攻击损失的减少率，并且提出了这种减少率与训练数据量的关系。<details>
<summary>Abstract</summary>
Adversarial attacks are usually expressed in terms of a gradient-based operation on the input data and model, this results in heavy computations every time an attack is generated. In this work, we solidify the idea of representing adversarial attacks as a trainable function, without further gradient computation. We first motivate that the theoretical best attacks, under proper conditions, can be represented as smooth piece-wise functions (piece-wise H\"older functions). Then we obtain an approximation result of such functions by a neural network. Subsequently, we emulate the ideal attack process by a neural network and reduce the adversarial training to a mathematical game between an attack network and a training model (a defense network). We also obtain convergence rates of adversarial loss in terms of the sample size $n$ for adversarial training in such a setting.
</details>
<details>
<summary>摘要</summary>
“敌对攻击通常表示为输入数据和模型之间的梯度基础操作，这会导致每次产生攻击时需要严重的计算。在这个工作中，我们固化了代表敌对攻击为可训练函数的想法，不需要进一步的梯度计算。我们首先认为，在适当的情况下，理论上最佳的攻击可以表示为稳定的块状函数（块状Holder函数）。然后我们获得了这些函数的近似结果，使用神经网络。接着，我们模拟理想的攻击过程，使用神经网络，并将对抗训练降到了数学游戏中，即攻击网络和训练模型（防御网络）之间的游戏。我们还获得了对抗训练过程中攻击损失的数量随扩展大小 $n$ 的收敛速率。”
</details></li>
</ul>
<hr>
<h2 id="ADR-GNN-Advection-Diffusion-Reaction-Graph-Neural-Networks"><a href="#ADR-GNN-Advection-Diffusion-Reaction-Graph-Neural-Networks" class="headerlink" title="ADR-GNN: Advection-Diffusion-Reaction Graph Neural Networks"></a>ADR-GNN: Advection-Diffusion-Reaction Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16092">http://arxiv.org/abs/2307.16092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moshe Eliasof, Eldad Haber, Eran Treister</li>
<li>for: 本研究旨在提出一种基于扩散-吸引-反应系统的图 neural network 模型（ADR-GNN），用于解决图structured数据中复杂现象的学习表示。</li>
<li>methods: ADR-GNN 使用扩散、吸引和反应三种基本操作来模型信息的传递、散射和非线性变换。</li>
<li>results: 在实验中，ADR-GNN 在真实世界的节点分类和空间时间数据集上表现出优于或与状态顶尖网络相当的性能。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have shown remarkable success in learning representations for graph-structured data. However, GNNs still face challenges in modeling complex phenomena that involve advection. In this paper, we propose a novel GNN architecture based on Advection-Diffusion-Reaction systems, called ADR-GNN. Advection models the directed transportation of information, diffusion captures the local smoothing of information, and reaction represents the non-linear transformation of information in channels. We provide an analysis of the qualitative behavior of ADR-GNN, that shows the benefit of combining advection, diffusion, and reaction. To demonstrate its efficacy, we evaluate ADR-GNN on real-world node classification and spatio-temporal datasets, and show that it improves or offers competitive performance compared to state-of-the-art networks.
</details>
<details>
<summary>摘要</summary>
графические нейронные сети (GNNs) имеют огромный потенциал для обучения представлений для данных, имеющих графическую структуру. Однако, GNNs still face challenges in моделировании сложных явлений, которые involve advection. В этой статье, we propose a novel GNN architecture based on Advection-Diffusion-Reaction systems, called ADR-GNN. Advection models the directed transportation of information, diffusion captures the local smoothing of information, and reaction represents the non-linear transformation of information in channels. We provide an analysis of the qualitative behavior of ADR-GNN, that shows the benefit of combining advection, diffusion, and reaction. To demonstrate its efficacy, we evaluate ADR-GNN on real-world node classification and spatio-temporal datasets, and show that it improves or offers competitive performance compared to state-of-the-art networks.Here's the translation breakdown:* графические (graph-structured) -> 图Structured (图Structured)* GNNs -> GNNs (同义译法)* involve advection -> involve advection (同义译法)* Advection-Diffusion-Reaction systems -> Advection-Diffusion-Reaction systems (同义译法)* ADR-GNN -> ADR-GNN (同义译法)* Advection models the directed transportation of information -> Advection models the directed transportation of information (同义译法)* diffusion captures the local smoothing of information -> diffusion captures the local smoothing of information (同义译法)* reaction represents the non-linear transformation of information in channels -> reaction represents the non-linear transformation of information in channels (同义译法)* we provide an analysis of the qualitative behavior of ADR-GNN -> we provide an analysis of the qualitative behavior of ADR-GNN (同义译法)* demonstrate its efficacy -> demonstrate its efficacy (同义译法)* evaluate ADR-GNN on real-world node classification and spatio-temporal datasets -> evaluate ADR-GNN on real-world node classification and spatio-temporal datasets (同义译法)* improve or offer competitive performance -> improve or offer competitive performance (同义译法)Note that the translation is based on the simplified Chinese version of the text, and some of the vocabulary and grammar may be different from traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Rapid-Flood-Inundation-Forecast-Using-Fourier-Neural-Operator"><a href="#Rapid-Flood-Inundation-Forecast-Using-Fourier-Neural-Operator" class="headerlink" title="Rapid Flood Inundation Forecast Using Fourier Neural Operator"></a>Rapid Flood Inundation Forecast Using Fourier Neural Operator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16090">http://arxiv.org/abs/2307.16090</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Y. Sun, Zhi Li, Wonhyun Lee, Qixing Huang, Bridget R. Scanlon, Clint Dawson</li>
<li>for: 预测洪水覆盖范围和浸水深度</li>
<li>methods: 组合过程基于模型和数据驱动机器学习方法</li>
<li>results: FNO模型在预测洪水覆盖范围和浸水深度方面表现出色，对新地点应用也有良好的泛化能力。<details>
<summary>Abstract</summary>
Flood inundation forecast provides critical information for emergency planning before and during flood events. Real time flood inundation forecast tools are still lacking. High-resolution hydrodynamic modeling has become more accessible in recent years, however, predicting flood extents at the street and building levels in real-time is still computationally demanding. Here we present a hybrid process-based and data-driven machine learning (ML) approach for flood extent and inundation depth prediction. We used the Fourier neural operator (FNO), a highly efficient ML method, for surrogate modeling. The FNO model is demonstrated over an urban area in Houston (Texas, U.S.) by training using simulated water depths (in 15-min intervals) from six historical storm events and then tested over two holdout events. Results show FNO outperforms the baseline U-Net model. It maintains high predictability at all lead times tested (up to 3 hrs) and performs well when applying to new sites, suggesting strong generalization skill.
</details>
<details>
<summary>摘要</summary>
洪水涌入预测提供了紧急准备和洪水事件发生时的重要信息。现有的实时洪水涌入预测工具仍然缺乏。高分辨率 hidrodynamic 模型在过去几年内变得更加可 accessible，但是在实时预测洪水覆盖面和涌入深度方面仍然具有计算压力。我们采用了一种混合的进程基于和数据驱动的机器学习（ML）方法，用于预测洪水覆盖面和涌入深度。我们使用了Fourier neural operator（FNO）模型，这是一种非常高效的ML方法，用于模拟器。FNO 模型在德州休斯顿（Texas, U.S.）的城市区域上进行了训练，使用了六个历史洪水事件中的计算水深（每15分钟），然后在两个保留事件上进行测试。结果显示，FNO 模型在所有领先时间（最多3小时）中保持高预测性，并在应用于新的场景时表现良好，表明它具有强大的泛化能力。
</details></li>
</ul>
<hr>
<h2 id="Using-Implicit-Behavior-Cloning-and-Dynamic-Movement-Primitive-to-Facilitate-Reinforcement-Learning-for-Robot-Motion-Planning"><a href="#Using-Implicit-Behavior-Cloning-and-Dynamic-Movement-Primitive-to-Facilitate-Reinforcement-Learning-for-Robot-Motion-Planning" class="headerlink" title="Using Implicit Behavior Cloning and Dynamic Movement Primitive to Facilitate Reinforcement Learning for Robot Motion Planning"></a>Using Implicit Behavior Cloning and Dynamic Movement Primitive to Facilitate Reinforcement Learning for Robot Motion Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16062">http://arxiv.org/abs/2307.16062</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zengjie Zhang, Jayden Hong, Amir Soufi Enayati, Homayoun Najjaran</li>
<li>for: 提高多自由度机器人运动规划的效率，使其具有更快的训练速度和更好的通用性。</li>
<li>methods: 使用偏函数吸引（IBC）和动态运动基本模型（DMP）来提高非约束RL代理的训练速度和通用性。</li>
<li>results: 在模拟环境中比较研究表明，提案方法比普通RL代理更快地训练和更高的得分。在真实机器人实验中，对简单的组装任务进行了应用。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) for motion planning of multi-degree-of-freedom robots still suffers from low efficiency in terms of slow training speed and poor generalizability. In this paper, we propose a novel RL-based robot motion planning framework that uses implicit behavior cloning (IBC) and dynamic movement primitive (DMP) to improve the training speed and generalizability of an off-policy RL agent. IBC utilizes human demonstration data to leverage the training speed of RL, and DMP serves as a heuristic model that transfers motion planning into a simpler planning space. To support this, we also create a human demonstration dataset using a pick-and-place experiment that can be used for similar studies. Comparison studies in simulation reveal the advantage of the proposed method over the conventional RL agents with faster training speed and higher scores. A real-robot experiment indicates the applicability of the proposed method to a simple assembly task. Our work provides a novel perspective on using motion primitives and human demonstration to leverage the performance of RL for robot applications.
</details>
<details>
<summary>摘要</summary>
现代人工智能技术中，虚拟套件学习（RL）在多度量自由机器人运动规划方面仍然受到低效率的困扰，即训练速度慢和泛化能力差。在这篇论文中，我们提出了一种基于RL的新型机器人运动规划框架，使用隐式行为假设（IBC）和动态运动原型（DMP）来提高RL Agent的训练速度和泛化能力。IBC利用人类示范数据来加速RL训练，而DMP作为一种简化计划空间的转移模型。为支持这一点，我们还创建了一个人类示范数据集，可以用于类似的研究。在模拟环境中的比较研究表明，我们的方法比普通RL Agent快速训练和高得分。一个真实机器人实验表明了我们的方法在简单的组装任务中的可行性。我们的工作为机器人应用中的RL技术提供了一个新的视角，并使用运动原型和人类示范来提高RL的表现。
</details></li>
</ul>
<hr>
<h2 id="Click-Conversion-Multi-Task-Model-with-Position-Bias-Mitigation-for-Sponsored-Search-in-eCommerce"><a href="#Click-Conversion-Multi-Task-Model-with-Position-Bias-Mitigation-for-Sponsored-Search-in-eCommerce" class="headerlink" title="Click-Conversion Multi-Task Model with Position Bias Mitigation for Sponsored Search in eCommerce"></a>Click-Conversion Multi-Task Model with Position Bias Mitigation for Sponsored Search in eCommerce</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16060">http://arxiv.org/abs/2307.16060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yibo Wang, Yanbing Xue, Bo Liu, Musen Wen, Wenting Zhao, Stephen Guo, Philip S. Yu</li>
<li>for: 防止排名系统中的位置偏见，以提高搜索结果的公平性</li>
<li>methods: 提出了两种Position-Aware Click-Conversion (PACC)和PACC via Position Embedding (PACC-PE)模型，其中PACC通过概率 decomposing 模型 позиция信息，而PACC-PE通过神经网络模型产生产品特定的位置信息</li>
<li>results: 实验结果表明，提议的模型可以更好地预测Click-through rate (CTR)和Conversion rate (CVR)，同时可以减少位置偏见的影响，提高搜索结果的公平性。<details>
<summary>Abstract</summary>
Position bias, the phenomenon whereby users tend to focus on higher-ranked items of the search result list regardless of the actual relevance to queries, is prevailing in many ranking systems. Position bias in training data biases the ranking model, leading to increasingly unfair item rankings, click-through-rate (CTR), and conversion rate (CVR) predictions. To jointly mitigate position bias in both item CTR and CVR prediction, we propose two position-bias-free CTR and CVR prediction models: Position-Aware Click-Conversion (PACC) and PACC via Position Embedding (PACC-PE). PACC is built upon probability decomposition and models position information as a probability. PACC-PE utilizes neural networks to model product-specific position information as embedding. Experiments on the E-commerce sponsored product search dataset show that our proposed models have better ranking effectiveness and can greatly alleviate position bias in both CTR and CVR prediction.
</details>
<details>
<summary>摘要</summary>
“位置偏见”现象，用户倾向于强调搜索结果列表中的高排名项，不 regards 实际相关性，是许多排名系统中的 prevailing 现象。在训练数据中的位置偏见会偏移排名模型，导致搜索结果的不公正性，点击率（CTR）和转化率（CVR）预测也会增加不公正性。为了同时消除位置偏见在Item CTR和CVR预测中，我们提出了两种位置偏见自由的预测模型：Position-Aware Click-Conversion（PACC）和PACC via Position Embedding（PACC-PE）。PACC基于概率分解，将位置信息作为概率来model。PACC-PE通过神经网络来模型产品Specific的位置信息作为嵌入。在电商推荐搜索数据集上进行实验，我们的提出的模型得到了更好的排名效果，可以很好地消除位置偏见在CTR和CVR预测中。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-Robustness-of-Test-Selection-Methods-for-Deep-Neural-Networks"><a href="#Evaluating-the-Robustness-of-Test-Selection-Methods-for-Deep-Neural-Networks" class="headerlink" title="Evaluating the Robustness of Test Selection Methods for Deep Neural Networks"></a>Evaluating the Robustness of Test Selection Methods for Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01314">http://arxiv.org/abs/2308.01314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiang Hu, Yuejun Guo, Xiaofei Xie, Maxime Cordy, Wei Ma, Mike Papadakis, Yves Le Traon</li>
<li>for: 这篇论文的目的是探讨深度学习系统测试中的挑战和解决方法。</li>
<li>methods: 该论文使用了多种测试选择方法，以降低标注采集的时间和劳动成本。</li>
<li>results: 研究发现，这些已经报告出色结果的测试选择方法在实际场景中可能并不是一定可靠，存在潜在的坑害。<details>
<summary>Abstract</summary>
Testing deep learning-based systems is crucial but challenging due to the required time and labor for labeling collected raw data. To alleviate the labeling effort, multiple test selection methods have been proposed where only a subset of test data needs to be labeled while satisfying testing requirements. However, we observe that such methods with reported promising results are only evaluated under simple scenarios, e.g., testing on original test data. This brings a question to us: are they always reliable? In this paper, we explore when and to what extent test selection methods fail for testing. Specifically, first, we identify potential pitfalls of 11 selection methods from top-tier venues based on their construction. Second, we conduct a study on five datasets with two model architectures per dataset to empirically confirm the existence of these pitfalls. Furthermore, we demonstrate how pitfalls can break the reliability of these methods. Concretely, methods for fault detection suffer from test data that are: 1) correctly classified but uncertain, or 2) misclassified but confident. Remarkably, the test relative coverage achieved by such methods drops by up to 86.85%. On the other hand, methods for performance estimation are sensitive to the choice of intermediate-layer output. The effectiveness of such methods can be even worse than random selection when using an inappropriate layer.
</details>
<details>
<summary>摘要</summary>
测试深度学习系统是必要但困难的，因为需要大量的时间和劳动来标注采集到的原始数据。为了减轻标注劳动，多种测试选择方法已经被提议，只需要标注一 subset 的测试数据，并且满足测试要求。然而，我们发现这些方法在简单的场景下（例如，测试原始测试数据）所报道的结果并不一定可靠。在这篇论文中，我们探索这些测试选择方法在测试时是否可靠，并特别是在哪些情况下会失败。首先，我们确定了11种选择方法的潜在坑害，根据它们的构造。然后，我们在五个数据集上进行了两种模型架构的实验，以确认这些坑害的存在。此外，我们还示出了这些坑害如何使测试选择方法失效。例如，用于缺陷检测的方法会面临以下两种情况：1）正确分类但不确定的测试数据，或2）错误分类但高自信的测试数据。这些方法的测试相对覆盖率下降了86.85%。另一方面，用于性能估计的方法对选择中间层输出的选择非常敏感。使用不合适的层时，这些方法的效果可以比Random Selection更差。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Exotic-Magnetic-Phases-in-Fibonacci-Quasicrystalline-Stacking-of-Ferromagnetic-Layers-through-Machine-Learning"><a href="#Unveiling-Exotic-Magnetic-Phases-in-Fibonacci-Quasicrystalline-Stacking-of-Ferromagnetic-Layers-through-Machine-Learning" class="headerlink" title="Unveiling Exotic Magnetic Phases in Fibonacci Quasicrystalline Stacking of Ferromagnetic Layers through Machine Learning"></a>Unveiling Exotic Magnetic Phases in Fibonacci Quasicrystalline Stacking of Ferromagnetic Layers through Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16052">http://arxiv.org/abs/2307.16052</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pablo S. Cornaglia, Matias Nuñez, D. J. Garcia</li>
<li>for: 这个研究是一个涵盖费波那契镁矿层堆栈的约束理论分析，可能通过磁性van der Waals材料实现。</li>
<li>methods: 我们构建了这个磁性异构体系的模型，包括第二邻居层磁相互作用，并使用机器学习方法来探索这个异构系统的磁性行为。</li>
<li>results: 我们发现了一个独特的梯形旋转螺旋相，在堆高的函数下，磁化程度下降对数减少。此外，我们还描述了这个磁性相位图，并发现了其他斜线和非斜线相。<details>
<summary>Abstract</summary>
In this study, we conduct a comprehensive theoretical analysis of a Fibonacci quasicrystalline stacking of ferromagnetic layers, potentially realizable using van der Waals magnetic materials. We construct a model of this magnetic heterostructure, which includes up to second neighbor interlayer magnetic interactions, that displays a complex relationship between geometric frustration and magnetic order in this quasicrystalline system. To navigate the parameter space and identify distinct magnetic phases, we employ a machine learning approach, which proves to be a powerful tool in revealing the complex magnetic behavior of this system. We offer a thorough description of the magnetic phase diagram as a function of the model parameters. Notably, we discover among other collinear and non-collinear phases, a unique ferromagnetic alternating helical phase. In this non-collinear quasiperiodic ferromagnetic configuration the magnetization decreases logarithmically with the stack height.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们进行了详细的理论分析，涉及到费波南尼克镧矿层排列，可能通过碳氢化合物磁 material实现。我们构建了这种磁化层结构的模型，包括最多第二邻居层磁相互作用，显示了这种各向异性束缚磁系统中的复杂关系。为了探索参数空间并确定不同磁相态，我们使用机器学习方法，证明是一种有力的工具，可以揭示这种磁系统的复杂磁性行为。我们提供了磁相态图，作为函数于模型参数的磁相态图。值得一提的是，我们发现了一种独特的排列式镧矿磁相态，在这种非排列征 periodic ferromagnetic配置中，磁化度随堆高度呈减少对数型变化。
</details></li>
</ul>
<hr>
<h2 id="Okapi-Instruction-tuned-Large-Language-Models-in-Multiple-Languages-with-Reinforcement-Learning-from-Human-Feedback"><a href="#Okapi-Instruction-tuned-Large-Language-Models-in-Multiple-Languages-with-Reinforcement-Learning-from-Human-Feedback" class="headerlink" title="Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback"></a>Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16039">http://arxiv.org/abs/2307.16039</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nlp-uoregon/okapi">https://github.com/nlp-uoregon/okapi</a></li>
<li>paper_authors: Viet Dac Lai, Chien Van Nguyen, Nghia Trung Ngo, Thuat Nguyen, Franck Dernoncourt, Ryan A. Rossi, Thien Huu Nguyen</li>
<li>for: 这paper的目的是提高大型自然语言模型（LLM）的发展，特别是在多语言下进行 instrucion 调整，以实现模型的强大学习能力。</li>
<li>methods: 这paper使用了supervised fine-tuning（SFT）和人工反馈学习（RLHF）两种方法来进行 instrucion 调整，以便生成最佳的商业LLM（如ChatGPT）。</li>
<li>results: 这paper提出了Okapi，第一个基于RLHF的多语言 instrucion 调整系统，并提供了26种多样化语言的 instrucion 和回快数据，以便进行未来的多语言LLM研究。我们的实验表明，RLHF在多语言 instrucion 调整中比SFT更有优势。我们的框架和资源在<a target="_blank" rel="noopener" href="https://github.com/nlp-uoregon/Okapi%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/nlp-uoregon/Okapi上发布。</a><details>
<summary>Abstract</summary>
A key technology for the development of large language models (LLMs) involves instruction tuning that helps align the models' responses with human expectations to realize impressive learning abilities. Two major approaches for instruction tuning characterize supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), which are currently applied to produce the best commercial LLMs (e.g., ChatGPT). To improve the accessibility of LLMs for research and development efforts, various instruction-tuned open-source LLMs have also been introduced recently, e.g., Alpaca, Vicuna, to name a few. However, existing open-source LLMs have only been instruction-tuned for English and a few popular languages, thus hindering their impacts and accessibility to many other languages in the world. Among a few very recent work to explore instruction tuning for LLMs in multiple languages, SFT has been used as the only approach to instruction-tune LLMs for multiple languages. This has left a significant gap for fine-tuned LLMs based on RLHF in diverse languages and raised important questions on how RLHF can boost the performance of multilingual instruction tuning. To overcome this issue, we present Okapi, the first system with instruction-tuned LLMs based on RLHF for multiple languages. Okapi introduces instruction and response-ranked data in 26 diverse languages to facilitate the experiments and development of future multilingual LLM research. We also present benchmark datasets to enable the evaluation of generative LLMs in multiple languages. Our experiments demonstrate the advantages of RLHF for multilingual instruction over SFT for different base models and datasets. Our framework and resources are released at https://github.com/nlp-uoregon/Okapi.
</details>
<details>
<summary>摘要</summary>
大频率模型的发展需要关键技术之一是指令调整，帮助模型的回答与人类期望保持一致，从而实现惊人的学习能力。目前最常用的两种方法是监督精度调整（SFT）和人类反馈学习（RLHF）。这两种方法目前已经用于生产最佳商业大型语言模型（如ChatGPT）。为了提高大型语言模型的研究和开发的可用性，各种指令调整的开源大型语言模型也在最近引入，例如Alpaca和Vicuna等。然而，现有的开源大型语言模型仅仅对英语和一些流行语言进行了指令调整，因此对全球其他语言的影响和可用性很差。在最近几个月内，有一些研究尝试了对大型语言模型进行多语言指令调整，但是这些研究仅仅使用了SFT方法。这种情况留下了一个大的 gap，即RLHF可以如何提高多语言指令调整的性能。为了解决这个问题，我们提出了Okapi，第一个基于RLHF的多语言指令调整系统。Okapi使用了26种多语言的指令和回答排名数据，以便实验和未来多语言大型语言模型的研究。我们还提供了多语言生成模型的评价数据集。我们的实验表明，RLHF在多语言指令调整中比SFT更有优势。我们的框架和资源在https://github.com/nlp-uoregon/Okapi上发布。
</details></li>
</ul>
<hr>
<h2 id="Developing-novel-ligands-with-enhanced-binding-affinity-for-the-sphingosine-1-phosphate-receptor-1-using-machine-learning"><a href="#Developing-novel-ligands-with-enhanced-binding-affinity-for-the-sphingosine-1-phosphate-receptor-1-using-machine-learning" class="headerlink" title="Developing novel ligands with enhanced binding affinity for the sphingosine 1-phosphate receptor 1 using machine learning"></a>Developing novel ligands with enhanced binding affinity for the sphingosine 1-phosphate receptor 1 using machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16037">http://arxiv.org/abs/2307.16037</a></li>
<li>repo_url: None</li>
<li>paper_authors: Colin Zhang, Yang Ha</li>
<li>for: 这项研究旨在用机器学习技术加速多发性脑膜炎（MS）的药物发现过程，并通过分析蛋白质-药物交互的化学性质，揭示新的药物发现方法和蛋白质-药物交互的机制。</li>
<li>methods: 该研究使用自适应神经网络模型，将化学式转化为数学向量，生成了超过500个分子变体基于斯普尼莫德（siponimod），并从中选择了25个蛋白质S1PR1的绑定亲和度最高的ligand。</li>
<li>results: 该研究发现了6种有药理性和易合成的蛋白质S1PR1拮抗剂，并通过分析这些拮抗剂的绑定交互，揭示了一些 contribuuting to high binding affinity to S1PR1的化学性质。<details>
<summary>Abstract</summary>
Multiple sclerosis (MS) is a debilitating neurological disease affecting nearly one million people in the United States. Sphingosine-1-phosphate receptor 1, or S1PR1, is a protein target for MS. Siponimod, a ligand of S1PR1, was approved by the FDA in 2019 for MS treatment, but there is a demonstrated need for better therapies. To this end, we finetuned an autoencoder machine learning model that converts chemical formulas into mathematical vectors and generated over 500 molecular variants based on siponimod, out of which 25 compounds had higher predicted binding affinity to S1PR1. The model was able to generate these ligands in just under one hour. Filtering these compounds led to the discovery of six promising candidates with good drug-like properties and ease of synthesis. Furthermore, by analyzing the binding interactions for these ligands, we uncovered several chemical properties that contribute to high binding affinity to S1PR1. This study demonstrates that machine learning can accelerate the drug discovery process and reveal new insights into protein-drug interactions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MUSE-Multi-View-Contrastive-Learning-for-Heterophilic-Graphs"><a href="#MUSE-Multi-View-Contrastive-Learning-for-Heterophilic-Graphs" class="headerlink" title="MUSE: Multi-View Contrastive Learning for Heterophilic Graphs"></a>MUSE: Multi-View Contrastive Learning for Heterophilic Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16026">http://arxiv.org/abs/2307.16026</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengyi Yuan, Minjie Chen, Xiang Li</li>
<li>for: 本文提出了一种多视图对照学习模型（MUSE），用于解决传统Graph Neural Networks（GNN）中的标签依赖和泛化性问题。</li>
<li>methods: 本文使用了两种视图来捕捉 Egonode 和其邻居的信息，通过 GNNs 增强了对照学习，并将两种视图进行集成。在集成过程中，使用了对比学习来增强节点表示的效果。此外，本文还考虑了不同 Egonode 的邻居上下文相互作用的多样性，通过信息融合控制器来模型这种多样性。</li>
<li>results: 本文在 9 个 benchmark 数据集上进行了广泛的实验，结果显示 MUSE 在节点分类和划分任务上具有效果。<details>
<summary>Abstract</summary>
In recent years, self-supervised learning has emerged as a promising approach in addressing the issues of label dependency and poor generalization performance in traditional GNNs. However, existing self-supervised methods have limited effectiveness on heterophilic graphs, due to the homophily assumption that results in similar node representations for connected nodes. In this work, we propose a multi-view contrastive learning model for heterophilic graphs, namely, MUSE. Specifically, we construct two views to capture the information of the ego node and its neighborhood by GNNs enhanced with contrastive learning, respectively. Then we integrate the information from these two views to fuse the node representations. Fusion contrast is utilized to enhance the effectiveness of fused node representations. Further, considering that the influence of neighboring contextual information on information fusion may vary across different ego nodes, we employ an information fusion controller to model the diversity of node-neighborhood similarity at both the local and global levels. Finally, an alternating training scheme is adopted to ensure that unsupervised node representation learning and information fusion controller can mutually reinforce each other. We conduct extensive experiments to evaluate the performance of MUSE on 9 benchmark datasets. Our results show the effectiveness of MUSE on both node classification and clustering tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Discrete-neural-nets-and-polymorphic-learning"><a href="#Discrete-neural-nets-and-polymorphic-learning" class="headerlink" title="Discrete neural nets and polymorphic learning"></a>Discrete neural nets and polymorphic learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00677">http://arxiv.org/abs/2308.00677</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/caten2/tripods2021ua">https://github.com/caten2/tripods2021ua</a></li>
<li>paper_authors: Charlotte Aten</li>
<li>for: 这个论文主要是为了研究神经网络的 universal approximation  Results和 classical learning task 之间的关系。</li>
<li>methods: 作者使用了 polymorphisms of relational structures 来 introduce a learning algorithm，并用其进行 classical learning task 的解决。</li>
<li>results: 研究发现，这种 learning algorithm 可以以 polymorphisms of relational structures 来实现 classical learning task 的解决。<details>
<summary>Abstract</summary>
Theorems from universal algebra such as that of Murski\u{i} from the 1970s have a striking similarity to universal approximation results for neural nets along the lines of Cybenko's from the 1980s. We consider here a discrete analogue of the classical notion of a neural net which places these results in a unified setting. We introduce a learning algorithm based on polymorphisms of relational structures and show how to use it for a classical learning task.
</details>
<details>
<summary>摘要</summary>
theorem from universal algebra, such as Murski\u{i}'s from the 1970s, have a striking similarity to universal approximation results for neural networks along the lines of Cybenko's from the 1980s. We consider here a discrete analogue of the classical notion of a neural network, which places these results in a unified setting. We introduce a learning algorithm based on polymorphisms of relational structures and show how to use it for a classical learning task.Note:* "Murski\u{i}" should be translated as "穆尔斯基" (Mù'ěrskī)* "Cybenko" should be translated as "茨本科" (Cíbiānkē)* "polymorphisms" should be translated as "多态" (duōtāi)* "relational structures" should be translated as "关系结构" (guānxì jiégòu)
</details></li>
</ul>
<hr>
<h2 id="Fuzzy-Logic-Visual-Network-FLVN-A-neuro-symbolic-approach-for-visual-features-matching"><a href="#Fuzzy-Logic-Visual-Network-FLVN-A-neuro-symbolic-approach-for-visual-features-matching" class="headerlink" title="Fuzzy Logic Visual Network (FLVN): A neuro-symbolic approach for visual features matching"></a>Fuzzy Logic Visual Network (FLVN): A neuro-symbolic approach for visual features matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16019">http://arxiv.org/abs/2307.16019</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://gitlab.com/grains2/flvn">https://gitlab.com/grains2/flvn</a></li>
<li>paper_authors: Francesco Manigrasso, Lia Morra, Fabrizio Lamberti</li>
<li>for: 提高零shot学习（ZSL）分类的性能</li>
<li>methods: 使用逻辑tensor网络（LTN）把知识表示法与深度神经网络结合，并通过强大的高级假设来增强模型的一致性和稳定性</li>
<li>results: 在Generalized ZSL（GZSL）benchmark AWA2和CUB上达到了状态态的性能，与其他最新的ZSL方法相比，具有较少的计算开销<details>
<summary>Abstract</summary>
Neuro-symbolic integration aims at harnessing the power of symbolic knowledge representation combined with the learning capabilities of deep neural networks. In particular, Logic Tensor Networks (LTNs) allow to incorporate background knowledge in the form of logical axioms by grounding a first order logic language as differentiable operations between real tensors. Yet, few studies have investigated the potential benefits of this approach to improve zero-shot learning (ZSL) classification. In this study, we present the Fuzzy Logic Visual Network (FLVN) that formulates the task of learning a visual-semantic embedding space within a neuro-symbolic LTN framework. FLVN incorporates prior knowledge in the form of class hierarchies (classes and macro-classes) along with robust high-level inductive biases. The latter allow, for instance, to handle exceptions in class-level attributes, and to enforce similarity between images of the same class, preventing premature overfitting to seen classes and improving overall performance. FLVN reaches state of the art performance on the Generalized ZSL (GZSL) benchmarks AWA2 and CUB, improving by 1.3% and 3%, respectively. Overall, it achieves competitive performance to recent ZSL methods with less computational overhead. FLVN is available at https://gitlab.com/grains2/flvn.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/30/cs.LG_2023_07_30/" data-id="clly4xtdo005vvl886ib09beq" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/31/eess.IV_2023_07_31/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.IV - 2023-07-31 17:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/30/cs.SD_2023_07_30/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-07-30 123:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">59</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">55</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">108</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
