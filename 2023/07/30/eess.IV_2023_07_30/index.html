
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-07-30 17:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Unsupervised Decomposition Networks for Bias Field Correction in MR Image paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.16219 repo_url: https:&#x2F;&#x2F;github.com&#x2F;leongdong&#x2F;bias-decomposition-networks paper_authors: D">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-07-30 17:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/07/30/eess.IV_2023_07_30/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Unsupervised Decomposition Networks for Bias Field Correction in MR Image paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.16219 repo_url: https:&#x2F;&#x2F;github.com&#x2F;leongdong&#x2F;bias-decomposition-networks paper_authors: D">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-29T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:22.864Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_07_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/30/eess.IV_2023_07_30/" class="article-date">
  <time datetime="2023-07-29T16:00:00.000Z" itemprop="datePublished">2023-07-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-07-30 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Unsupervised-Decomposition-Networks-for-Bias-Field-Correction-in-MR-Image"><a href="#Unsupervised-Decomposition-Networks-for-Bias-Field-Correction-in-MR-Image" class="headerlink" title="Unsupervised Decomposition Networks for Bias Field Correction in MR Image"></a>Unsupervised Decomposition Networks for Bias Field Correction in MR Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16219">http://arxiv.org/abs/2307.16219</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leongdong/bias-decomposition-networks">https://github.com/leongdong/bias-decomposition-networks</a></li>
<li>paper_authors: Dong Liang, Xingyu Qiu, Kuanquan Wang, Gongning Luo, Wei Wang, Yashu Liu<br>for:这个论文主要是为了提出一种无监督的拟合网络方法，用于从偏倚图像中分离偏倚场和原始MR图像。methods:这个方法使用了一种新的卷积神经网络模型，其中包括分类部和估计部，两者在训练过程中相互优化。此外，基于多元偏倚场的损失函数也是提出的。results:实验结果表明，该方法可以准确地估计偏倚场并生成更好的偏倚 corrections。代码可以在以下链接上下载：<a target="_blank" rel="noopener" href="https://github.com/LeongDong/Bias-Decomposition-Networks%E3%80%82">https://github.com/LeongDong/Bias-Decomposition-Networks。</a><details>
<summary>Abstract</summary>
Bias field, which is caused by imperfect MR devices or imaged objects, introduces intensity inhomogeneity into MR images and degrades the performance of MR image analysis methods. Many retrospective algorithms were developed to facilitate the bias correction, to which the deep learning-based methods outperformed. However, in the training phase, the supervised deep learning-based methods heavily rely on the synthesized bias field. As the formation of the bias field is extremely complex, it is difficult to mimic the true physical property of MR images by synthesized data. While bias field correction and image segmentation are strongly related, the segmentation map is precisely obtained by decoupling the bias field from the original MR image, and the bias value is indicated by the segmentation map in reverse. Thus, we proposed novel unsupervised decomposition networks that are trained only with biased data to obtain the bias-free MR images. Networks are made up of: a segmentation part to predict the probability of every pixel belonging to each class, and an estimation part to calculate the bias field, which are optimized alternately. Furthermore, loss functions based on the combination of fuzzy clustering and the multiplicative bias field are also devised. The proposed loss functions introduce the smoothness of bias field and construct the soft relationships among different classes under intra-consistency constraints. Extensive experiments demonstrate that the proposed method can accurately estimate bias fields and produce better bias correction results. The code is available on the link: https://github.com/LeongDong/Bias-Decomposition-Networks.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translation::enable_literal扭曲场，由于偏斜MR设备或图像对象的不完美，会导致MR图像中的光度不均，从而降低MR图像分析方法的性能。许多Retrospective算法已经开发来抵消偏斜场，但在训练阶段，深度学习基于方法依赖于Synthesized偏斜场。由于偏斜场的形成非常复杂，难以通过Synthesized数据模拟真实的物理属性。而偏斜场 correction和图像 segmentation 是密切相关的，可以通过分离偏斜场和原始MR图像来获得精确的segmentation map，并且偏斜值可以通过segmentation map反向表示。因此，我们提出了一种新的无监督分解网络，用于从偏斜数据中获得偏斜场free MR图像。网络由以下两个部分组成：分类部分用于预测每个像素属于哪个类别的概率，以及估计部分用于计算偏斜场，这两个部分被 alternate 优化。此外，我们还提出了基于多项式偏斜场和杂化 clustering 的损失函数。这些损失函数引入了偏斜场的平滑性和不同类别之间的软连接，通过内部一致性约束。广泛的实验表明，提出的方法可以准确地估计偏斜场并生成更好的偏斜 correction 结果。代码可以在以下链接获取：https://github.com/LeongDong/Bias-Decomposition-Networks。
</details></li>
</ul>
<hr>
<h2 id="Gastrointestinal-Mucosal-Problems-Classification-with-Deep-Learning"><a href="#Gastrointestinal-Mucosal-Problems-Classification-with-Deep-Learning" class="headerlink" title="Gastrointestinal Mucosal Problems Classification with Deep Learning"></a>Gastrointestinal Mucosal Problems Classification with Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16198">http://arxiv.org/abs/2307.16198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammadhasan Goharian, Vahid Goharian, Hamidreza Bolhasani</li>
<li>for: 这篇论文主要是研究用于识别肠胃膜变化，以便早期诊断和预防肠癌。</li>
<li>methods: 该论文使用了深度学习技术，特别是基于 transferred learning（TL）的卷积神经网络（CNNs），对肠胃膜变化进行预测。</li>
<li>results: 研究发现，使用 Transfer Learning 基于 CNNs 的模型可以达到93%的准确率，并在实际的Endoscopy和Colonoscopy影像中进行了应用和验证。<details>
<summary>Abstract</summary>
Gastrointestinal mucosal changes can cause cancers after some years and early diagnosing them can be very useful to prevent cancers and early treatment. In this article, 8 classes of mucosal changes and anatomical landmarks including Polyps, Ulcerative Colitis, Esophagitis, Normal Z-Line, Normal Pylorus, Normal Cecum, Dyed Lifted Polyps, and Dyed Lifted Margin were predicted by deep learning. We used neural networks in this article. It is a black box artificial intelligence algorithm that works like a human neural system. In this article, Transfer Learning (TL) based on the Convolutional Neural Networks (CNNs), which is one of the well-known types of neural networks in image processing is used. We compared some famous CNN architecture including VGG, Inception, Xception, and ResNet. Our best model got 93% accuracy in test images. At last, we used our model in some real endoscopy and colonoscopy movies to classify problems.
</details>
<details>
<summary>摘要</summary>
胃肠膜变化可能导致癌症，早期诊断可以有助于预防癌症和早期治疗。在这篇文章中，我们预测了8种胃肠膜变化和解剖特征，包括��ccolets、急性coliitis、食管炎、正常Z-线、正常pylorus、正常cecum、染料提取polyps和染料提取边缘。我们使用了神经网络来实现这一点。神经网络是一种人工智能算法，它工作如人脑神经系统一样。我们使用了传输学习（TL），基于Convolutional Neural Networks（CNNs），这是一种广泛应用于图像处理中的知名神经网络类型。我们比较了一些知名的CNN架构，包括VGG、Inception、Xception和ResNet。我们的最佳模型在测试图像中达到93%的准确率。最后，我们使用了我们的模型在实际末端摄影和colonoscopy视频中分类问题。
</details></li>
</ul>
<hr>
<h2 id="StarSRGAN-Improving-Real-World-Blind-Super-Resolution"><a href="#StarSRGAN-Improving-Real-World-Blind-Super-Resolution" class="headerlink" title="StarSRGAN: Improving Real-World Blind Super-Resolution"></a>StarSRGAN: Improving Real-World Blind Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16169">http://arxiv.org/abs/2307.16169</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kynthesis/StarSRGAN">https://github.com/kynthesis/StarSRGAN</a></li>
<li>paper_authors: Khoa D. Vo, Len T. Bui</li>
<li>for: 这个研究旨在提高电脑视觉中的照片分辨率，并且不需要知道变低分辨率的过程。</li>
<li>methods: 这个研究使用了5种不同的架构，包括Real-ESRGAN模型，以提高照片的分辨率。</li>
<li>results: 实验比较结果显示，StarSRGAN可以与Real-ESRGAN实现相似的分辨率，并且具有约10%更高的MANINA和AHIQ评分。此外，StarSRGAN Lite版本可以在实时上调高分辨率，并且可以保持约90%的图像质量。<details>
<summary>Abstract</summary>
The aim of blind super-resolution (SR) in computer vision is to improve the resolution of an image without prior knowledge of the degradation process that caused the image to be low-resolution. The State of the Art (SOTA) model Real-ESRGAN has advanced perceptual loss and produced visually compelling outcomes using more complex degradation models to simulate real-world degradations. However, there is still room to improve the super-resolved quality of Real-ESRGAN by implementing recent techniques. This research paper introduces StarSRGAN, a novel GAN model designed for blind super-resolution tasks that utilize 5 various architectures. Our model provides new SOTA performance with roughly 10% better on the MANIQA and AHIQ measures, as demonstrated by experimental comparisons with Real-ESRGAN. In addition, as a compact version, StarSRGAN Lite provides approximately 7.5 times faster reconstruction speed (real-time upsampling from 540p to 4K) but can still keep nearly 90% of image quality, thereby facilitating the development of a real-time SR experience for future research. Our codes are released at https://github.com/kynthesis/StarSRGAN.
</details>
<details>
<summary>摘要</summary>
文本：隐式超分辨率（SR）的计算机视觉目标是提高图像的分辨率，不具备图像压缩过程的先前知识。现有最佳实践（SOTA）模型Real-ESRGAN已经提出了更加复杂的压缩模型来模拟实际世界中的压缩。然而，仍然有很大的提高超分辨质量的空间。这篇研究论文介绍了StarSRGAN，一种新的GAN模型，用于隐式SR任务。我们的模型使用了5种不同的建筑，并提供了新的SOTA性能，在MANIQA和AHIQ测试中比Real-ESRGAN提高约10%。此外，StarSRGAN Lite版本可以在实时upsampling从540p到4K的过程中提高速度约7.5倍，仍然可以保持图像质量的90%，因此可以促进未来研究中的实时SR经验的发展。我们的代码在https://github.com/kynthesis/StarSRGAN中发布。翻译：目标：隐式超分辨率（SR）在计算机视觉中的目标是提高图像的分辨率，不具备图像压缩过程的先前知识。现有最佳实践（SOTA）模型Real-ESRGAN已经提出了更加复杂的压缩模型来模拟实际世界中的压缩。然而，仍然有很大的提高超分辨质量的空间。我们在这篇研究论文中介绍了StarSRGAN，一种新的GAN模型，用于隐式SR任务。我们的模型使用了5种不同的建筑，并提供了新的SOTA性能，在MANIQA和AHIQ测试中比Real-ESRGAN提高约10%。此外，StarSRGAN Lite版本可以在实时upsampling从540p到4K的过程中提高速度约7.5倍，仍然可以保持图像质量的90%，因此可以促进未来研究中的实时SR经验的发展。我们的代码在https://github.com/kynthesis/StarSRGAN中发布。
</details></li>
</ul>
<hr>
<h2 id="Structure-Preserving-Synthesis-MaskGAN-for-Unpaired-MR-CT-Translation"><a href="#Structure-Preserving-Synthesis-MaskGAN-for-Unpaired-MR-CT-Translation" class="headerlink" title="Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation"></a>Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16143">http://arxiv.org/abs/2307.16143</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HieuPhan33/MaskGAN">https://github.com/HieuPhan33/MaskGAN</a></li>
<li>paper_authors: Minh Hieu Phan, Zhibin Liao, Johan W. Verjans, Minh-Son To</li>
<li>for: 这篇论文旨在解决医疗影像合成中的欠缺联系数据问题，使用CyclingGAN来利用无相关数据，但是这些方法通常会导致不准确的映射，导致影像的推导过程中的骨架结构被损坏。</li>
<li>methods: 这篇论文提出了一个名为MaskGAN的新的框架，它利用自动提取的粗糙Mask来确保结构一致性，并使用内容生成器来实现CT内容的生成。</li>
<li>results: 实验结果显示，MaskGAN在一个儿童 dataset 上表现出色，与现有的合成方法相比，MaskGAN 能够更好地保留骨架结构，并且不需要专家的标注。<details>
<summary>Abstract</summary>
Medical image synthesis is a challenging task due to the scarcity of paired data. Several methods have applied CycleGAN to leverage unpaired data, but they often generate inaccurate mappings that shift the anatomy. This problem is further exacerbated when the images from the source and target modalities are heavily misaligned. Recently, current methods have aimed to address this issue by incorporating a supplementary segmentation network. Unfortunately, this strategy requires costly and time-consuming pixel-level annotations. To overcome this problem, this paper proposes MaskGAN, a novel and cost-effective framework that enforces structural consistency by utilizing automatically extracted coarse masks. Our approach employs a mask generator to outline anatomical structures and a content generator to synthesize CT contents that align with these structures. Extensive experiments demonstrate that MaskGAN outperforms state-of-the-art synthesis methods on a challenging pediatric dataset, where MR and CT scans are heavily misaligned due to rapid growth in children. Specifically, MaskGAN excels in preserving anatomical structures without the need for expert annotations. The code for this paper can be found at https://github.com/HieuPhan33/MaskGAN.
</details>
<details>
<summary>摘要</summary>
医学图像生成是一项具有挑战性的任务，主要由于缺乏匹配数据。许多方法尝试使用CycleGAN将无匹配数据利用，但它们通常生成不准确的映射，导致身体结构的偏移。这个问题更加严重当图像来源和目标模式都是极度不一致的时候。现在，当前的方法尝试通过添加辅助分割网络来解决这个问题，但这种策略需要昂贵的和时间consuming的像素级注解。为了解决这个问题，本文提出了MaskGAN，一种新的和cost-effective的框架，它通过自动提取的大致mask来保持结构一致性。我们的方法使用mask生成器将解剖结构轮廓出来，并使用内容生成器将CT内容与这些结构相关。我们的实验表明，MaskGAN在一个具有挑战性的педиатric dataset上表现出色，其中MR和CT扫描是由儿童快速增长而导致的极度不一致。具体来说，MaskGAN能够保持解剖结构，不需要专家注解。相关代码可以在https://github.com/HieuPhan33/MaskGAN中找到。
</details></li>
</ul>
<hr>
<h2 id="Implicit-Neural-Representation-in-Medical-Imaging-A-Comparative-Survey"><a href="#Implicit-Neural-Representation-in-Medical-Imaging-A-Comparative-Survey" class="headerlink" title="Implicit Neural Representation in Medical Imaging: A Comparative Survey"></a>Implicit Neural Representation in Medical Imaging: A Comparative Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16142">http://arxiv.org/abs/2307.16142</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mindflow-institue/awesome-implicit-neural-representations-in-medical-imaging">https://github.com/mindflow-institue/awesome-implicit-neural-representations-in-medical-imaging</a></li>
<li>paper_authors: Amirali Molaei, Amirhossein Aminimehr, Armin Tavakoli, Amirhossein Kazerouni, Bobby Azad, Reza Azad, Dorit Merhof</li>
<li>for: 本文提供了一个全面的审视，探讨了在医疗图像分析领域中使用隐藏神经表示（INR）模型的可能性。</li>
<li>methods: 本文使用了INR模型来解决各种医疗图像分析任务，包括图像重建、分割、注册、新视角生成和压缩。</li>
<li>results: 本文总结了INR模型在医疗图像分析领域的优势和局限性，以及其在不同任务中的应用可能性。<details>
<summary>Abstract</summary>
Implicit neural representations (INRs) have gained prominence as a powerful paradigm in scene reconstruction and computer graphics, demonstrating remarkable results. By utilizing neural networks to parameterize data through implicit continuous functions, INRs offer several benefits. Recognizing the potential of INRs beyond these domains, this survey aims to provide a comprehensive overview of INR models in the field of medical imaging. In medical settings, numerous challenging and ill-posed problems exist, making INRs an attractive solution. The survey explores the application of INRs in various medical imaging tasks, such as image reconstruction, segmentation, registration, novel view synthesis, and compression. It discusses the advantages and limitations of INRs, highlighting their resolution-agnostic nature, memory efficiency, ability to avoid locality biases, and differentiability, enabling adaptation to different tasks. Furthermore, the survey addresses the challenges and considerations specific to medical imaging data, such as data availability, computational complexity, and dynamic clinical scene analysis. It also identifies future research directions and opportunities, including integration with multi-modal imaging, real-time and interactive systems, and domain adaptation for clinical decision support. To facilitate further exploration and implementation of INRs in medical image analysis, we have provided a compilation of cited studies along with their available open-source implementations on \href{https://github.com/mindflow-institue/Awesome-Implicit-Neural-Representations-in-Medical-imaging}. Finally, we aim to consistently incorporate the most recent and relevant papers regularly.
</details>
<details>
<summary>摘要</summary>
启发性神经表示（INR）在场景重建和计算机图形领域已经得到广泛应用，显示了惊人的成果。通过使用神经网络参数化数据 mediante implicit continuous functions，INRs 提供了多种优点。认识到 INRs 在医疗领域的潜力，这篇评论旨在为医疗影像分析领域中 INR 模型的全面概述。在医疗设置中，存在许多困难和不确定的问题，使得 INRs 成为一种有appeal的解决方案。评论 explore INRs 在各种医疗影像任务中的应用，如图像重建、分割、注册、新视图生成和压缩。它讨论了 INRs 的优点和局限性，包括其无关分辨率的 natura, 内存有效性、避免本地偏好和可导 differentiability，以及其适应不同任务的能力。此外，评论还讨论了医疗影像数据特有的挑战和考虑因素，如数据可用性、计算复杂度和动态临床场景分析。 finally, 评论 indentify 未来研究方向和机会，包括与多 modal 影像集成、实时交互系统和适应医疗决策支持。为便于进一步探索和应用 INRs 在医疗影像分析中，我们提供了一份 cited studies 和其可用的开源实现，可以在 \href{https://github.com/mindflow-institue/Awesome-Implicit-Neural-Representations-in-Medical-imaging} 中找到。最后，我们计划不断 incorporate 最新和最相关的论文，以便保持评论的有效性和相关性。
</details></li>
</ul>
<hr>
<h2 id="RIS-Enhanced-Semantic-Communications-Adaptive-to-User-Requirements"><a href="#RIS-Enhanced-Semantic-Communications-Adaptive-to-User-Requirements" class="headerlink" title="RIS-Enhanced Semantic Communications Adaptive to User Requirements"></a>RIS-Enhanced Semantic Communications Adaptive to User Requirements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16100">http://arxiv.org/abs/2307.16100</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peiwen Jiang, Chao-Kai Wen, Shi Jin, Geoffrey Ye Li<br>for:这个论文是为了提出一种基于智能表面的含义通信方法，以满足不同的用户需求和环境条件。methods:这个方法使用智能表面分配含义内容，并考虑用户运动和视线干扰，以保护重要的含义在困难的通信条件下。results:实验结果表明，该方法可以在多种通信条件下实现可靠的任务性能，但在严重的通信条件下，一些含义部分可能会被损坏。为此，提出了一种重建方法，以提高视觉接受度。此外，该方法还可以有效地调整智能表面资源，以实现多用户共享和有效的资源分配。<details>
<summary>Abstract</summary>
Semantic communication significantly reduces required bandwidth by understanding semantic meaning of the transmitted. However, current deep learning-based semantic communication methods rely on joint source-channel coding design and end-to-end training, which limits their adaptability to new physical channels and user requirements. Reconfigurable intelligent surfaces (RIS) offer a solution by customizing channels in different environments. In this study, we propose the RIS-SC framework, which allocates semantic contents with varying levels of RIS assistance to satisfy the changing user requirements. It takes into account user movement and line-of-sight obstructions, enabling the RIS resource to protect important semantics in challenging channel conditions. The simulation results indicate reasonable task performance, but some semantic parts that have no effect on task performances are abandoned under severe channel conditions. To address this issue, a reconstruction method is also introduced to improve visual acceptance by inferring those missing semantic parts. Furthermore, the framework can adjust RIS resources in friendly channel conditions to save and allocate them efficiently among multiple users. Simulation results demonstrate the adaptability and efficiency of the RIS-SC framework across diverse channel conditions and user requirements.
</details>
<details>
<summary>摘要</summary>
semantic communication 可以减少需要的带宽，因为它理解传输的 semantics 含义。但是，当前的深度学习基于 semantic communication 方法都是通过共同源-通道编码设计和端到端训练，这限制了它们在新的物理通道和用户需求中的适应性。智能表面重配置 (RIS) 提供了一个解决方案，通过自定义不同环境中的通道来满足用户的变化需求。本研究提出了 RIS-SC 框架，它根据用户的运动和视线干扰，对重要的 semantics 进行了保护，并允许 RIS 资源在具有挑战性通道条件下进行有效地分配。实验结果表明，任务性能是可理解的，但在严重的通道条件下，一些无关任务性能的 semantics 会被遗弃。为解决这个问题，我们还提出了一种重建方法，可以通过推理来恢复这些缺失的 semantics。此外，框架还可以在友好的通道条件下调整 RIS 资源，以有效地分配它们 Among multiple users。实验结果表明 RIS-SC 框架在多样化的通道条件和用户需求下具有适应性和效率。
</details></li>
</ul>
<hr>
<h2 id="A-New-Multi-Level-Hazy-Image-and-Video-Dataset-for-Benchmark-of-Dehazing-Methods"><a href="#A-New-Multi-Level-Hazy-Image-and-Video-Dataset-for-Benchmark-of-Dehazing-Methods" class="headerlink" title="A New Multi-Level Hazy Image and Video Dataset for Benchmark of Dehazing Methods"></a>A New Multi-Level Hazy Image and Video Dataset for Benchmark of Dehazing Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16050">http://arxiv.org/abs/2307.16050</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bedrettin Cetinkaya, Yucel Cimtay, Fatma Nazli Gunay, Gokce Nur Yilmaz</li>
<li>for: 本研究准备了一个新的多级雾化彩色图像数据集，以便比较不同的雾化方法和模型在这个数据集上的表现。</li>
<li>methods: 本研究使用了五种不同的雾化方法&#x2F;模型，包括传统方法和深度学习方法，并对其在数据集上的表现进行比较。</li>
<li>results: 研究结果显示，传统方法在不同场景下能够更好地普适化雾化问题，而深度学习方法在跨数据集雾化 Task 中表现较差。<details>
<summary>Abstract</summary>
The changing level of haze is one of the main factors which affects the success of the proposed dehazing methods. However, there is a lack of controlled multi-level hazy dataset in the literature. Therefore, in this study, a new multi-level hazy color image dataset is presented. Color video data is captured for two real scenes with a controlled level of haze. The distance of the scene objects from the camera, haze level, and ground truth (clear image) are available so that different dehazing methods and models can be benchmarked. In this study, the dehazing performance of five different dehazing methods/models is compared on the dataset based on SSIM, PSNR, VSI and DISTS image quality metrics. Results show that traditional methods can generalize the dehazing problem better than many deep learning based methods. The performance of deep models depends mostly on the scene and is generally poor on cross-dataset dehazing.
</details>
<details>
<summary>摘要</summary>
气凝度的变化是提议的抑霾方法的成功因素之一，但在文献中缺乏控制多级霾化数据集。因此，本研究提供了一个新的多级霾化彩色图像数据集。通过捕捉两个真实场景的颜色视频数据，实现控制霾度水平。场景对象与摄像头的距离、霾度水平和明确图像（清晰图像）的信息都可以用，以便不同的抑霾方法和模型进行比较。本研究根据SSIM、PSNR、VSI和DISTS图像质量指标对五种不同的抑霾方法/模型进行比较。结果显示，传统方法在不同场景下能更好地泛化抑霾问题，而深度学习基于方法在cross-dataset抑霾中表现较差。
</details></li>
</ul>
<hr>
<h2 id="CoVid-19-Detection-leveraging-Vision-Transformers-and-Explainable-AI"><a href="#CoVid-19-Detection-leveraging-Vision-Transformers-and-Explainable-AI" class="headerlink" title="CoVid-19 Detection leveraging Vision Transformers and Explainable AI"></a>CoVid-19 Detection leveraging Vision Transformers and Explainable AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16033">http://arxiv.org/abs/2307.16033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pangoth Santhosh Kumar, Kundrapu Supriya, Mallikharjuna Rao K<br>for: The paper is written for the purpose of developing an end-to-end framework using vision transformers for the diagnosis of lung diseases, specifically pneumonia, Covid 19, and lung opacity.methods: The paper uses a specialized Compact Convolution Transformers (CCT) model, which is trained and evaluated on datasets such as the Covid 19 Radiography Database.results: The CCT model achieves better accuracy for both training and validation purposes on the Covid 19 Radiography Database, demonstrating its effectiveness in detecting lung diseases.Here’s the simplified Chinese text format you requested:for: 这篇论文的目的是为了开发一种基于视transformer的终端框架，用于诊断肺病，具体来说是识别 Covid 19、肺病和肺混浊等病种。methods: 这篇论文使用了一种特殊的 Compact Convolution Transformers (CCT) 模型，通过对 Covid 19 Radiography Database 等数据集进行训练和评估。results: CCT 模型在 Covid 19 Radiography Database 上的训练和验证目标上具有更高的准确率，表明其在诊断肺病方面的效果。<details>
<summary>Abstract</summary>
Lung disease is a common health problem in many parts of the world. It is a significant risk to people health and quality of life all across the globe since it is responsible for five of the top thirty leading causes of death. Among them are COVID 19, pneumonia, and tuberculosis, to name just a few. It is critical to diagnose lung diseases in their early stages. Several different models including machine learning and image processing have been developed for this purpose. The earlier a condition is diagnosed, the better the patient chances of making a full recovery and surviving into the long term. Thanks to deep learning algorithms, there is significant promise for the autonomous, rapid, and accurate identification of lung diseases based on medical imaging. Several different deep learning strategies, including convolutional neural networks (CNN), vanilla neural networks, visual geometry group based networks (VGG), and capsule networks , are used for the goal of making lung disease forecasts. The standard CNN has a poor performance when dealing with rotated, tilted, or other aberrant picture orientations. As a result of this, within the scope of this study, we have suggested a vision transformer based approach end to end framework for the diagnosis of lung disorders. In the architecture, data augmentation, training of the suggested models, and evaluation of the models are all included. For the purpose of detecting lung diseases such as pneumonia, Covid 19, lung opacity, and others, a specialised Compact Convolution Transformers (CCT) model have been tested and evaluated on datasets such as the Covid 19 Radiography Database. The model has achieved a better accuracy for both its training and validation purposes on the Covid 19 Radiography Database.
</details>
<details>
<summary>摘要</summary>
肺病是全球许多地区的常见健康问题。它对人类健康和生活质量产生了重大风险，因为它负责全球前三十名死亡原因之一的五种疾病。这些疾病包括 COVID-19、肺炎和 tuberkulose 等。EARLY DIAGNOSIS 是关键，因为根据医学影像识别肺病的早期可以提高病人恢复的可能性和长期生存的可能性。感谢深度学习算法，肺病的自动识别已经有了显著的前景。这些深度学习策略包括卷积神经网络（CNN）、普通神经网络、视 geometry group 基于网络（VGG）和卷积神经网络（Capsule Networks）。然而，标准的 CNN 在不正常的图像方向下表现不佳，因此在这种研究范围内，我们建议使用视transformer 基础的终端框架进行肺病诊断。在架构、数据增强、模型训练和模型评估方面都包括了具体的实现。为了检测肺病如肺炎、 COVID-19、肺透明度等，我们在 Covid 19 成像数据库上测试了一种专门的 Compact Convolution Transformers（CCT）模型。该模型在训练和验证目的上在 Covid 19 成像数据库上达到了更高的准确率。
</details></li>
</ul>
<hr>
<h2 id="LOTUS-Learning-to-Optimize-Task-based-US-representations"><a href="#LOTUS-Learning-to-Optimize-Task-based-US-representations" class="headerlink" title="LOTUS: Learning to Optimize Task-based US representations"></a>LOTUS: Learning to Optimize Task-based US representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16021">http://arxiv.org/abs/2307.16021</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yordanka Velikova, Mohammad Farid Azampour, Walter Simson, Vanessa Gonzalez Duque, Nassir Navab</li>
<li>for: 这个研究旨在提高ultrasound图像中的器官分类，以便应用于诊断和监控。</li>
<li>methods: 本研究使用了一种新的方法，即使用CT扫描来生成适当的ultrasound图像，并通过射线投影来模拟对质料的传播。</li>
<li>results: 研究获得了一些有 promise的量值结果，并且在调整过的图像上进行了质感评估。<details>
<summary>Abstract</summary>
Anatomical segmentation of organs in ultrasound images is essential to many clinical applications, particularly for diagnosis and monitoring. Existing deep neural networks require a large amount of labeled data for training in order to achieve clinically acceptable performance. Yet, in ultrasound, due to characteristic properties such as speckle and clutter, it is challenging to obtain accurate segmentation boundaries, and precise pixel-wise labeling of images is highly dependent on the expertise of physicians. In contrast, CT scans have higher resolution and improved contrast, easing organ identification. In this paper, we propose a novel approach for learning to optimize task-based ultra-sound image representations. Given annotated CT segmentation maps as a simulation medium, we model acoustic propagation through tissue via ray-casting to generate ultrasound training data. Our ultrasound simulator is fully differentiable and learns to optimize the parameters for generating physics-based ultrasound images guided by the downstream segmentation task. In addition, we train an image adaptation network between real and simulated images to achieve simultaneous image synthesis and automatic segmentation on US images in an end-to-end training setting. The proposed method is evaluated on aorta and vessel segmentation tasks and shows promising quantitative results. Furthermore, we also conduct qualitative results of optimized image representations on other organs.
</details>
<details>
<summary>摘要</summary>
医学应用中的组织部分分割是非常重要的，特别是诊断和监测。现有的深入神经网络需要大量标注数据来进行训练，以达到клиничеacceptable的性能。然而，在ultrasound中，由特征性质如斑点和噪声而导致的分割边界很难获取，并且专业医生们对图像进行精确的像素化标注受限。相比之下，CT扫描有更高的分辨率和更好的对比，使器官识别更容易。在这篇论文中，我们提出了一种新的方法，用于学习优化任务基于ultrasound图像表示。通过模拟射线折射来模拟声波的传播，我们生成了基于物理的ultrasound训练数据。我们的ultrasound simulator是完全可微分的，可以学习优化参数，以便通过下游分割任务来导引物理基于ultrasound图像的生成。此外，我们还训练了一种图像适应网络，以实现同时的图像合成和自动分割在US图像上。我们的提posed方法在AAA和血管分割任务中表现出了有力的量化结果。此外，我们还进行了其他器官的优化图像表示的质量分析。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/30/eess.IV_2023_07_30/" data-id="clmjn91q700fv0j8853vocn54" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/30/cs.SD_2023_07_30/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.SD - 2023-07-30 123:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/29/cs.SD_2023_07_29/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-07-29</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
