
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-07-19 17:00:00 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Flexible Physical Unclonable Functions based on non-deterministically distributed Dye-Doped Fibers and Droplets paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.11000 repo_url: None paper_authors: Mauro Daniel Lu">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-07-19 17:00:00">
<meta property="og:url" content="http://example.com/2023/07/19/eess.IV_2023_07_19/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Flexible Physical Unclonable Functions based on non-deterministically distributed Dye-Doped Fibers and Droplets paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.11000 repo_url: None paper_authors: Mauro Daniel Lu">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-18T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:34.626Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_07_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/19/eess.IV_2023_07_19/" class="article-date">
  <time datetime="2023-07-18T16:00:00.000Z" itemprop="datePublished">2023-07-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-07-19 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Flexible-Physical-Unclonable-Functions-based-on-non-deterministically-distributed-Dye-Doped-Fibers-and-Droplets"><a href="#Flexible-Physical-Unclonable-Functions-based-on-non-deterministically-distributed-Dye-Doped-Fibers-and-Droplets" class="headerlink" title="Flexible Physical Unclonable Functions based on non-deterministically distributed Dye-Doped Fibers and Droplets"></a>Flexible Physical Unclonable Functions based on non-deterministically distributed Dye-Doped Fibers and Droplets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11000">http://arxiv.org/abs/2308.11000</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mauro Daniel Luigi Bruno, Giuseppe Emanuele Lio, Antonio Ferraro, Sara Nocentini, Giuseppe Papuzzo, Agostino Forestiero, Giovanni Desiderio, Maria Penelope De Santo, Diederik Sybolt Wiersma, Roberto Caputo, Giovanni Golemme, Francesco Riboli, Riccardo Cristoforo Barberi</li>
<li>for: 防止伪造（anti-counterfeiting）解释</li>
<li>methods: 使用电子推抛和电子插填技术生成弹性自由悬浮膜，嵌入不同的物理不可复制函数（PUF）金钥</li>
<li>results: 产生了复杂的多层验证机制，包括激光谱发射、特征fluorescence谱和挑战回应对（CRP）识别协议，实现了高度安全的物品保护。<details>
<summary>Abstract</summary>
The development of new anti-counterfeiting solutions is a constant challenge and involves several research fields. Much interest is devoted to systems that are impossible to clone, based on the Physical Unclonable Function (PUF) paradigm. In this work, new strategies based on electrospinning and electrospraying of dye-doped polymeric materials are presented for the manufacturing of flexible free-standing films that embed different PUF keys. Films can be used to fabricate anticounterfeiting labels having three encryption levels: i) a map of fluorescent polymer droplets, with non deterministic positions on a dense yarn of polymer nanofibers; ii) a characteristic fluorescence spectrum for each label; iii) a challenge-response pairs (CRPs) identification protocol based on the strong nature of the physical unclonable function. The intrinsic uniqueness introduced by the deposition techniques encodes enough complexity into the optical anti-counterfeiting tag to generate thousands of cryptographic keys. The simple and cheap fabrication process as well as the multilevel authentication makes such colored polymeric unclonable tags a practical solution in the secure protection of merchandise in our daily life.
</details>
<details>
<summary>摘要</summary>
新型防伪措施的开发是一项不断挑战，涉及到多个研究领域。许多研究人员对于不可复制的系统产生了极大的兴趣，基于物理不可克隆函数（PUF）的思想。在这项工作中，我们提出了基于电子涂敷和电子扑灭的染料含 polymer 材料制造 flexible 自由浮动膜，其中包含不同的 PUF 密钥。这些膜可以用来制造防伪标签，具有三级加密：1. 涂敷在细菌纤维上的荟毒染料液体，具有不决定的位置和密度，形成一个复杂的荟毒染料液体地图。2. 每个标签具有独特的激发谱，作为特征性的防伪特征。3. 基于强大的物理不可克隆函数，实现了挑战-回答协议（CRP）的标识。由于电子涂敷和电子扑灭的固有特性，这些染料含 polymer 材料中的复杂性足以生成 тысячи个加密密钥。此外，制造过程简单、便宜，同时具有多级身份验证功能，使这种颜色染料含 polymer 防伪标签在我们日常生活中的安全保护中成为了实用的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Blind-Image-Quality-Assessment-Using-Multi-Stream-Architecture-with-Spatial-and-Channel-Attention"><a href="#Blind-Image-Quality-Assessment-Using-Multi-Stream-Architecture-with-Spatial-and-Channel-Attention" class="headerlink" title="Blind Image Quality Assessment Using Multi-Stream Architecture with Spatial and Channel Attention"></a>Blind Image Quality Assessment Using Multi-Stream Architecture with Spatial and Channel Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09857">http://arxiv.org/abs/2307.09857</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hassan Khalid, Nisar Ahmed</li>
<li>for: 本文提出了一种基于多流空间和通道注意力的自适应图像质量评估算法，以解决图像质量评估的难题，图像内容和扭曲都有所不同。</li>
<li>methods: 该算法首先使用两个不同的背景网络生成混合特征，然后通过空间和通道注意力来提供高权重到关键区域。</li>
<li>results: 实验结果表明，该算法可以更准确地预测图像质量，与人类感知评估呈高相关性。此外，该算法还具有优秀的泛化性，特别是在关键区域的感知信息上。<details>
<summary>Abstract</summary>
BIQA (Blind Image Quality Assessment) is an important field of study that evaluates images automatically. Although significant progress has been made, blind image quality assessment remains a difficult task since images vary in content and distortions. Most algorithms generate quality without emphasizing the important region of interest. In order to solve this, a multi-stream spatial and channel attention-based algorithm is being proposed. This algorithm generates more accurate predictions with a high correlation to human perceptual assessment by combining hybrid features from two different backbones, followed by spatial and channel attention to provide high weights to the region of interest. Four legacy image quality assessment datasets are used to validate the effectiveness of our proposed approach. Authentic and synthetic distortion image databases are used to demonstrate the effectiveness of the proposed method, and we show that it has excellent generalization properties with a particular focus on the perceptual foreground information.
</details>
<details>
<summary>摘要</summary>
BIQA (无视图质量评估) 是一个重要的研究领域，它自动评估图像质量。尽管已经取得了 significante 进步，但无视图质量评估仍然是一个困难的任务，因为图像的内容和损害都很多样。大多数算法生成的质量不强调重要的注意点区域。为解决这个问题，我们提出了一种多流程空间和通道注意力基于的算法。这种算法将 combining 两种不同的背景器，然后进行空间和通道注意力，以提供高权重的注意点区域，从而生成更加准确的预测，与人类感知评估高相关性。我们使用四个传统图像质量评估Dataset来验证我们的提议的有效性。我们还使用authentic和synthetic损害图像库来示示我们的方法的总体化能力，并示出它在特定的感知前景信息方面具有优秀的一致性。
</details></li>
</ul>
<hr>
<h2 id="Cryo-forum-A-framework-for-orientation-recovery-with-uncertainty-measure-with-the-application-in-cryo-EM-image-analysis"><a href="#Cryo-forum-A-framework-for-orientation-recovery-with-uncertainty-measure-with-the-application-in-cryo-EM-image-analysis" class="headerlink" title="Cryo-forum: A framework for orientation recovery with uncertainty measure with the application in cryo-EM image analysis"></a>Cryo-forum: A framework for orientation recovery with uncertainty measure with the application in cryo-EM image analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09847">http://arxiv.org/abs/2307.09847</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/phonchi/cryo-forum">https://github.com/phonchi/cryo-forum</a></li>
<li>paper_authors: Szu-Chi Chung<br>for:This paper aims to improve the efficiency and accuracy of orientation parameter estimation in single-particle cryo-electron microscopy (cryo-EM) by introducing a novel approach that uses a 10-dimensional feature vector and a Quadratically-Constrained Quadratic Program to derive the predicted orientation as a unit quaternion, supplemented by an uncertainty metric.methods:The proposed method uses a deep learning approach with a 10-dimensional feature vector to represent the orientation and applies a Quadratically-Constrained Quadratic Program to derive the predicted orientation as a unit quaternion, supplemented by an uncertainty metric. The method also includes a unique loss function that considers the pairwise distances between orientations.results:The proposed method effectively recovers orientations from 2D cryo-EM images in an end-to-end manner, and the inclusion of uncertainty quantification allows for direct clean-up of the dataset at the 3D level. The proposed method is packaged into a user-friendly software suite named cryo-forum, designed for easy accessibility by developers.<details>
<summary>Abstract</summary>
In single-particle cryo-electron microscopy (cryo-EM), the efficient determination of orientation parameters for 2D projection images poses a significant challenge yet is crucial for reconstructing 3D structures. This task is complicated by the high noise levels present in the cryo-EM datasets, which often include outliers, necessitating several time-consuming 2D clean-up processes. Recently, solutions based on deep learning have emerged, offering a more streamlined approach to the traditionally laborious task of orientation estimation. These solutions often employ amortized inference, eliminating the need to estimate parameters individually for each image. However, these methods frequently overlook the presence of outliers and may not adequately concentrate on the components used within the network. This paper introduces a novel approach that uses a 10-dimensional feature vector to represent the orientation and applies a Quadratically-Constrained Quadratic Program to derive the predicted orientation as a unit quaternion, supplemented by an uncertainty metric. Furthermore, we propose a unique loss function that considers the pairwise distances between orientations, thereby enhancing the accuracy of our method. Finally, we also comprehensively evaluate the design choices involved in constructing the encoder network, a topic that has not received sufficient attention in the literature. Our numerical analysis demonstrates that our methodology effectively recovers orientations from 2D cryo-EM images in an end-to-end manner. Importantly, the inclusion of uncertainty quantification allows for direct clean-up of the dataset at the 3D level. Lastly, we package our proposed methods into a user-friendly software suite named cryo-forum, designed for easy accessibility by the developers.
</details>
<details>
<summary>摘要</summary>
Single-particle cryo-electron microscopy (cryo-EM)中，确定图像方向的效率是一项关键任务，但是它对于重建3D结构非常重要。这项任务受到高噪音水平的影响，其中包括异常值，需要进行多次时间消耗的2D清洁过程。近年来，基于深度学习的解决方案在这个领域出现了，它们通常使用整合参数，从而消除每个图像需要独立计算参数的劳动 INTRODUCTION  This paper introduces a novel approach to solving the challenging task of orientation estimation in single-particle cryo-electron microscopy (cryo-EM). Our method uses a 10-dimensional feature vector to represent the orientation and applies a Quadratically-Constrained Quadratic Program (QCQP) to derive the predicted orientation as a unit quaternion, supplemented by an uncertainty metric. Additionally, we propose a unique loss function that considers the pairwise distances between orientations, thereby enhancing the accuracy of our method. We also comprehensively evaluate the design choices involved in constructing the encoder network, a topic that has not received sufficient attention in the literature. Our numerical analysis demonstrates that our methodology effectively recovers orientations from 2D cryo-EM images in an end-to-end manner. Importantly, the inclusion of uncertainty quantification allows for direct clean-up of the dataset at the 3D level. Finally, we package our proposed methods into a user-friendly software suite named cryo-forum, designed for easy accessibility by developers.Here's the translation in Traditional Chinese:Single-particle cryo-electron microscopy (cryo-EM)中，确定图像方向的效率是一项关键任务，但是它对于重建3D结构非常重要。这项任务受到高噪音水平的影响，其中包括异常值，需要进行多次时间消耗的2D清洁过程。近年来，基于深度学习的解决方案在这个领域出现了，它们通常使用整合参数，从而消除每个图像需要独立计算参数的劳动。本文介绍了一种新的方法，使用10维特征向量表示方向，并使用Quadratically-Constrained Quadratic Program (QCQP)来 derive预测的方向为单元量QUATERNION，并且附加了一个不确定度度量。此外，我们还提出了一个唯一的损失函数，该函数考虑了方向之间的对比度，从而提高了我们的方法的准确性。我们还对构建编码器网络的设计选择进行了全面的评估，这是在文献中未得到足够的注意的。我们的数值分析表明，我们的方法可以有效地从2D cryo-EM图像中提取方向，并且包含不确定度度量，可以直接清理3D数据集。最后，我们将我们的提posed方法包装成一个易用的软件套件，名为cryo-forum，用于开发者的易用性。
</details></li>
</ul>
<hr>
<h2 id="Compressive-Image-Scanning-Microscope"><a href="#Compressive-Image-Scanning-Microscope" class="headerlink" title="Compressive Image Scanning Microscope"></a>Compressive Image Scanning Microscope</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09841">http://arxiv.org/abs/2307.09841</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ajay Gunalan, Marco Castello, Simonluca Piazza, Shunlei Li, Alberto Diaspro, Leonardo S. Mattos, Paolo Bianchini</li>
<li>for: 实现压缩扫描镜 microscope (LSM) 图像扫描镜 (ISM) 中的压缩扫描，使用单 photon 荷电压缩 (SPAD) 数组探测器。</li>
<li>methods: 采用固定抽象策略，在数据获取过程中跳过 alternate 行列，从而降低了数据点数量，并消除了计算不同抽象矩阵的需要。 通过利用 SPAD 数组产生的平行图像，提高了压缩-ISM 图像的重建质量，相比标准压缩扫描 LSM 图像。</li>
<li>results: 研究结果表明，我们的方法可以生成高质量图像，同时降低数据获取时间和抗衰减的问题，具有减少光泳蚀的潜在优势。<details>
<summary>Abstract</summary>
We present a novel approach to implement compressive sensing in laser scanning microscopes (LSM), specifically in image scanning microscopy (ISM), using a single-photon avalanche diode (SPAD) array detector. Our method addresses two significant limitations in applying compressive sensing to LSM: the time to compute the sampling matrix and the quality of reconstructed images. We employ a fixed sampling strategy, skipping alternate rows and columns during data acquisition, which reduces the number of points scanned by a factor of four and eliminates the need to compute different sampling matrices. By exploiting the parallel images generated by the SPAD array, we improve the quality of the reconstructed compressive-ISM images compared to standard compressive confocal LSM images. Our results demonstrate the effectiveness of our approach in producing higher-quality images with reduced data acquisition time and potential benefits in reducing photobleaching.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，使用单 photon 泵发射器（SPAD）数组探测器实现压缩扫描镜 microscope（LSM）中的图像扫描镜（ISM）。我们的方法解决了在应用压缩扫描到 LSM 中的两个主要限制：计算抽象矩阵的时间和重建图像的质量。我们采用固定抽象策略，在数据收集过程中跳过 alternate 行和列，从而将数据点数减少为四分之一，并消除计算不同抽象矩阵的需求。通过利用 SPAD 数组生成的并行图像，我们提高了压缩-ISM 图像的重建质量，相比标准压缩扫描 LSM 图像。我们的结果表明我们的方法的有效性，可以生成高质量图像，降低数据收集时间和避免 фото腐蚀。
</details></li>
</ul>
<hr>
<h2 id="Fix-your-downsampling-ASAP-Be-natively-more-robust-via-Aliasing-and-Spectral-Artifact-free-Pooling"><a href="#Fix-your-downsampling-ASAP-Be-natively-more-robust-via-Aliasing-and-Spectral-Artifact-free-Pooling" class="headerlink" title="Fix your downsampling ASAP! Be natively more robust via Aliasing and Spectral Artifact free Pooling"></a>Fix your downsampling ASAP! Be natively more robust via Aliasing and Spectral Artifact free Pooling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09804">http://arxiv.org/abs/2307.09804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julia Grabinski, Janis Keuper, Margret Keuper</li>
<li>for: 本研究旨在提高卷积神经网络对常见损害和攻击的Native robustness。</li>
<li>methods: 本研究使用了FLC pooling和ASAP pooling两种方法来解决采样错误导致的扭曲问题。</li>
<li>results: 对于常见损害和攻击，ASAP pooling可以提高卷积神经网络的Native robustness，而不会影响clean accuracy。<details>
<summary>Abstract</summary>
Convolutional neural networks encode images through a sequence of convolutions, normalizations and non-linearities as well as downsampling operations into potentially strong semantic embeddings. Yet, previous work showed that even slight mistakes during sampling, leading to aliasing, can be directly attributed to the networks' lack in robustness. To address such issues and facilitate simpler and faster adversarial training, [12] recently proposed FLC pooling, a method for provably alias-free downsampling - in theory. In this work, we conduct a further analysis through the lens of signal processing and find that such current pooling methods, which address aliasing in the frequency domain, are still prone to spectral leakage artifacts. Hence, we propose aliasing and spectral artifact-free pooling, short ASAP. While only introducing a few modifications to FLC pooling, networks using ASAP as downsampling method exhibit higher native robustness against common corruptions, a property that FLC pooling was missing. ASAP also increases native robustness against adversarial attacks on high and low resolution data while maintaining similar clean accuracy or even outperforming the baseline.
</details>
<details>
<summary>摘要</summary>
convolutional neural networks 使用序列化核函数、标准化函数、非线性函数以及下采样操作来生成强式 semantic embeddings。然而，过去的工作表明，甚至小范围的采样错误，导致扭曲，可以直接归因于网络的不稳定性。为解决这些问题并实现更加简单和快速的对抗训练，[12] 最近提出了FLC pooling方法，该方法可以在理论上确保无偏Sampling。在这项工作中，我们通过信号处理的视角进行进一步的分析，发现现有的 pooling 方法，它们在频域中处理偏折补，仍然存在频率泄漏 artifacts。因此，我们提出了偏折补和频率 artifact-free pooling，简称ASAP。尽管ASAP只对 FLC pooling 进行了一些修改，但使用 ASAP 作为下采样方法的网络显示出更高的原生 Robustness  against common corruptions，这是FLC pooling缺失的性能。此外，ASAP 还提高了高和低分辨率数据上的对抗攻击 Robustness，保持同样的干净精度或者甚至超越基线。
</details></li>
</ul>
<hr>
<h2 id="DiffDP-Radiotherapy-Dose-Prediction-via-a-Diffusion-Model"><a href="#DiffDP-Radiotherapy-Dose-Prediction-via-a-Diffusion-Model" class="headerlink" title="DiffDP: Radiotherapy Dose Prediction via a Diffusion Model"></a>DiffDP: Radiotherapy Dose Prediction via a Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09794">http://arxiv.org/abs/2307.09794</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/scufzh/DiffDP">https://github.com/scufzh/DiffDP</a></li>
<li>paper_authors: Zhenghao Feng, Lu Wen, Peng Wang, Binyu Yan, Xi Wu, Jiliu Zhou, Yan Wang</li>
<li>for: 这个研究旨在提高放射治疗规划中的剂量分布预测效率和质量，并解决现有方法的过滤问题。</li>
<li>methods: 本研究提出了一个散射基于的剂量预测模型（DiffDP），包括一个前进过程和一个返回过程。在前进过程中，DiffDP将剂量分布图表转换为 Gaussian 噪声，并训练一个噪声预测器来预测添加在每个时间步骤中的噪声。在返回过程中，它从原始 Gaussian 噪声中移除噪声，并使用已经训练的噪声预测器来将噪声移除，最后终端出力预测的剂量分布图表。</li>
<li>results: 实验结果显示，DiffDP 模型能够对放射治疗规划中的130例肛瘤癌患者的剂量分布预测获得高度的准确性和稳定性。<details>
<summary>Abstract</summary>
Currently, deep learning (DL) has achieved the automatic prediction of dose distribution in radiotherapy planning, enhancing its efficiency and quality. However, existing methods suffer from the over-smoothing problem for their commonly used L_1 or L_2 loss with posterior average calculations. To alleviate this limitation, we innovatively introduce a diffusion-based dose prediction (DiffDP) model for predicting the radiotherapy dose distribution of cancer patients. Specifically, the DiffDP model contains a forward process and a reverse process. In the forward process, DiffDP gradually transforms dose distribution maps into Gaussian noise by adding small noise and trains a noise predictor to predict the noise added in each timestep. In the reverse process, it removes the noise from the original Gaussian noise in multiple steps with the well-trained noise predictor and finally outputs the predicted dose distribution map. To ensure the accuracy of the prediction, we further design a structure encoder to extract anatomical information from patient anatomy images and enable the noise predictor to be aware of the dose constraints within several essential organs, i.e., the planning target volume and organs at risk. Extensive experiments on an in-house dataset with 130 rectum cancer patients demonstrate the s
</details>
<details>
<summary>摘要</summary>
现在，深度学习（DL）已经实现了辐射规划中自动预测剂量分布的方法，提高了其效率和质量。然而，现有方法受到通用的L_1或L_2损失函数和后期平均计算的过滤限制。为了解决这一限制，我们创新地引入了DiffDP模型，用于预测肿瘤病人辐射剂量分布。具体来说，DiffDP模型包括一个前进过程和一个反向过程。在前进过程中，DiffDP逐渐将剂量分布图转化为高斯噪声图，并在每个时间步骤中训练一个噪声预测器来预测添加到图中的噪声。在反向过程中，它将噪声从原始高斯噪声图中除去，并在多个步骤中使用已经训练好的噪声预测器来除噪。最后，它输出预测的剂量分布图。为确保预测的准确性，我们还设计了结构编码器，用于从患者身体图像中提取解剖信息，并使噪声预测器对剂量约束（如规划目标体和风险器）产生影响。在我们自有数据集上进行了广泛的实验，结果表明DiffDP模型可以准确地预测肿瘤病人辐射剂量分布。
</details></li>
</ul>
<hr>
<h2 id="CPCM-Contextual-Point-Cloud-Modeling-for-Weakly-supervised-Point-Cloud-Semantic-Segmentation"><a href="#CPCM-Contextual-Point-Cloud-Modeling-for-Weakly-supervised-Point-Cloud-Semantic-Segmentation" class="headerlink" title="CPCM: Contextual Point Cloud Modeling for Weakly-supervised Point Cloud Semantic Segmentation"></a>CPCM: Contextual Point Cloud Modeling for Weakly-supervised Point Cloud Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10316">http://arxiv.org/abs/2307.10316</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lizhaoliu-Lec/CPCM">https://github.com/lizhaoliu-Lec/CPCM</a></li>
<li>paper_authors: Lizhao Liu, Zhuangwei Zhuang, Shangxin Huang, Xunlong Xiao, Tianhang Xiang, Cen Chen, Jingdong Wang, Mingkui Tan</li>
<li>for: 降低高成本的精度注释（less than 0.1% points are labeled），实现弱精度的点云semantic segmentation。</li>
<li>methods: 使用RegionMask和CMT两部分， RegionMask将点云分割成多个区域，然后使用CMT进行masked training，具体来说是将预测结果作为mask产生更多的训练样本。</li>
<li>results: 在ScanNet V2和S3DIS benchmark上得到了state-of-the-art的表现，证明CPCM可以减少高成本的精度注释，并且具有较高的Semantic Segmentation性能。<details>
<summary>Abstract</summary>
We study the task of weakly-supervised point cloud semantic segmentation with sparse annotations (e.g., less than 0.1% points are labeled), aiming to reduce the expensive cost of dense annotations. Unfortunately, with extremely sparse annotated points, it is very difficult to extract both contextual and object information for scene understanding such as semantic segmentation. Motivated by masked modeling (e.g., MAE) in image and video representation learning, we seek to endow the power of masked modeling to learn contextual information from sparsely-annotated points. However, directly applying MAE to 3D point clouds with sparse annotations may fail to work. First, it is nontrivial to effectively mask out the informative visual context from 3D point clouds. Second, how to fully exploit the sparse annotations for context modeling remains an open question. In this paper, we propose a simple yet effective Contextual Point Cloud Modeling (CPCM) method that consists of two parts: a region-wise masking (RegionMask) strategy and a contextual masked training (CMT) method. Specifically, RegionMask masks the point cloud continuously in geometric space to construct a meaningful masked prediction task for subsequent context learning. CMT disentangles the learning of supervised segmentation and unsupervised masked context prediction for effectively learning the very limited labeled points and mass unlabeled points, respectively. Extensive experiments on the widely-tested ScanNet V2 and S3DIS benchmarks demonstrate the superiority of CPCM over the state-of-the-art.
</details>
<details>
<summary>摘要</summary>
我们研究弱监督点云Semantic segmentation问题，即使用非常少的标注点（例如， menos de 0.1% 的点被标注），以降低严重的标注成本。然而，与极其罕见的标注点相对，� Extracting both contextual and object information for scene understanding, such as semantic segmentation, is very difficult. 为了解决这个问题，我们启发自masked modeling（例如，MAE）在图像和视频表示学习中的应用。我们想要通过masked modeling来学习点云中的上下文信息，但是直接将MAE应用于罕见的3D点云可能不具有效果。首先，是非常困难从3D点云中有效地排除有用的视觉上下文。其次，如何完全利用有限的标注点来模型上下文仍是一个开放的问题。在这篇论文中，我们提出了一种简单 yet effective的Contextual Point Cloud Modeling（CPCM）方法，它包括两部分：RegionMask和CMT。Specifically，RegionMask在空间上连续地遮盖点云，以构建一个有意义的masked prediction任务，以便后续的上下文学习。CMT分离了supervised segmentation和无supervised masked context prediction的学习，以便有效地学习非常有限的标注点和大量的无标注点。我们在ScanNet V2和S3DIS标准测试集上进行了广泛的实验，并证明了CPCM的优越性。
</details></li>
</ul>
<hr>
<h2 id="NTIRE-2023-Quality-Assessment-of-Video-Enhancement-Challenge"><a href="#NTIRE-2023-Quality-Assessment-of-Video-Enhancement-Challenge" class="headerlink" title="NTIRE 2023 Quality Assessment of Video Enhancement Challenge"></a>NTIRE 2023 Quality Assessment of Video Enhancement Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09729">http://arxiv.org/abs/2307.09729</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaohong Liu, Xiongkuo Min, Wei Sun, Yulun Zhang, Kai Zhang, Radu Timofte, Guangtao Zhai, Yixuan Gao, Yuqin Cao, Tengchuan Kou, Yunlong Dong, Ziheng Jia, Yilin Li, Wei Wu, Shuming Hu, Sibin Deng, Pengxiang Xiao, Ying Chen, Kai Li, Kai Zhao, Kun Yuan, Ming Sun, Heng Cong, Hao Wang, Lingzhi Fu, Yusheng Zhang, Rongyu Zhang, Hang Shi, Qihang Xu, Longan Xiao, Zhiliang Ma, Mirko Agarla, Luigi Celona, Claudio Rota, Raimondo Schettini, Zhiwei Huang, Yanan Li, Xiaotao Wang, Lei Lei, Hongye Liu, Wei Hong, Ironhead Chuang, Allen Lin, Drake Guan, Iris Chen, Kae Lou, Willy Huang, Yachun Tasi, Yvonne Kao, Haotian Fan, Fangyuan Kong, Shiqi Zhou, Hao Liu, Yu Lai, Shanshan Chen, Wenqi Wang, Haoning Wu, Chaofeng Chen, Chunzheng Zhu, Zekun Guo, Shiling Zhao, Haibing Yin, Hongkui Wang, Hanene Brachemi Meftah, Sid Ahmed Fezza, Wassim Hamidouche, Olivier Déforges, Tengfei Shi, Azadeh Mansouri, Hossein Motamednia, Amir Hossein Bakhtiari, Ahmad Mahmoudi Aznaveh</li>
<li>for: 本研究报告NTIRE 2023年视频改善质量评估挑战，它将在CVPR 2023年新形态图像恢复和提高工作坊（NTIRE）中举行。这个挑战的目的是解决视频处理领域中的一个主要挑战，即视频质量评估（VQA）。</li>
<li>methods: 挑战使用VDPVE数据集，包括600个彩色、亮度和对比度增强的视频，310个除噪视频和301个滤除晃动视频。挑战共有167名参与者。61个参与者队伍在开发阶段提交了1368份提交，而37个队伍在最终测试阶段提交了176份提交。最终，19个队伍提交了他们的模型和相关资料，并详细介绍了他们使用的方法。一些方法比基准方法更好，而赢家的方法表现出了优秀的预测性能。</li>
<li>results: 本研究发现，一些方法比基准方法更好，而赢家的方法表现出了优秀的预测性能。<details>
<summary>Abstract</summary>
This paper reports on the NTIRE 2023 Quality Assessment of Video Enhancement Challenge, which will be held in conjunction with the New Trends in Image Restoration and Enhancement Workshop (NTIRE) at CVPR 2023. This challenge is to address a major challenge in the field of video processing, namely, video quality assessment (VQA) for enhanced videos. The challenge uses the VQA Dataset for Perceptual Video Enhancement (VDPVE), which has a total of 1211 enhanced videos, including 600 videos with color, brightness, and contrast enhancements, 310 videos with deblurring, and 301 deshaked videos. The challenge has a total of 167 registered participants. 61 participating teams submitted their prediction results during the development phase, with a total of 3168 submissions. A total of 176 submissions were submitted by 37 participating teams during the final testing phase. Finally, 19 participating teams submitted their models and fact sheets, and detailed the methods they used. Some methods have achieved better results than baseline methods, and the winning methods have demonstrated superior prediction performance.
</details>
<details>
<summary>摘要</summary>
During the development phase, 61 participating teams submitted their prediction results, with a total of 3168 submissions. In the final testing phase, 176 submissions were submitted by 37 participating teams. Finally, 19 participating teams submitted their models and fact sheets, detailing the methods they used. Some of the methods achieved better results than baseline methods, and the winning methods demonstrated superior prediction performance.
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Driven-Multi-Scale-Feature-Fusion-Network-for-Real-time-Image-Deraining"><a href="#Uncertainty-Driven-Multi-Scale-Feature-Fusion-Network-for-Real-time-Image-Deraining" class="headerlink" title="Uncertainty-Driven Multi-Scale Feature Fusion Network for Real-time Image Deraining"></a>Uncertainty-Driven Multi-Scale Feature Fusion Network for Real-time Image Deraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09728">http://arxiv.org/abs/2307.09728</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ming Tong, Xuefeng Yan, Yongzhen Wang</li>
<li>for: 提高视觉基于测量系统中雨纹影响下的图像识别精度，并且在资源有限的设备上实现实时处理。</li>
<li>methods: 提出了一种基于uncertainty的多尺度特征融合网络（UMFFNet），通过学习映射分布来估计uncertainty，并通过不确定性信息动态强制特征提取和精度提高。</li>
<li>results: 对比其他状态艺术方法，UMFFNet在减少预测错误和提高图像涂抹效果方面达到了显著的性能提高，而且具有少量参数的优势。<details>
<summary>Abstract</summary>
Visual-based measurement systems are frequently affected by rainy weather due to the degradation caused by rain streaks in captured images, and existing imaging devices struggle to address this issue in real-time. While most efforts leverage deep networks for image deraining and have made progress, their large parameter sizes hinder deployment on resource-constrained devices. Additionally, these data-driven models often produce deterministic results, without considering their inherent epistemic uncertainty, which can lead to undesired reconstruction errors. Well-calibrated uncertainty can help alleviate prediction errors and assist measurement devices in mitigating risks and improving usability. Therefore, we propose an Uncertainty-Driven Multi-Scale Feature Fusion Network (UMFFNet) that learns the probability mapping distribution between paired images to estimate uncertainty. Specifically, we introduce an uncertainty feature fusion block (UFFB) that utilizes uncertainty information to dynamically enhance acquired features and focus on blurry regions obscured by rain streaks, reducing prediction errors. In addition, to further boost the performance of UMFFNet, we fused feature information from multiple scales to guide the network for efficient collaborative rain removal. Extensive experiments demonstrate that UMFFNet achieves significant performance improvements with few parameters, surpassing state-of-the-art image deraining methods.
</details>
<details>
<summary>摘要</summary>
“视觉基于的测量系统经常受到雨水的影响，因为捕捉图像中的雨纹会导致图像质量下降。现有的成像设备很难在实时中解决这个问题。大多数努力都是基于深度网络进行图像抖干，并且已经取得了进步，但这些深度网络的参数较大，导致部署在资源有限的设备上存在问题。此外，这些数据驱动模型经常生成决定性的结果，而不考虑其内在的可能性不确定性，这可能导致不想要的重建错误。因此，我们提出了一种基于不确定性的多级特征融合网络（UMFFNet），它学习图像对的概率分布来Estimate uncertainty。具体来说，我们引入了不确定性特征融合块（UFFB），它利用不确定性信息来动态增强获取的特征并专注于雨水纹理覆盖的模糊区域，从而减少预测错误。此外，为了进一步提高 UMFFNet 的性能，我们将特征信息从多个级别融合，以便导引网络协同进行雨水 removing。广泛的实验表明， UMFFNet 可以在几个参数下实现显著的性能提升，超过当前的图像抖干方法。”
</details></li>
</ul>
<hr>
<h2 id="Flexible-single-multimode-fiber-imaging-using-white-LED"><a href="#Flexible-single-multimode-fiber-imaging-using-white-LED" class="headerlink" title="Flexible single multimode fiber imaging using white LED"></a>Flexible single multimode fiber imaging using white LED</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09714">http://arxiv.org/abs/2307.09714</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minyu Fan, Kun Liu, Jie Zhu, Yu Cao, Sha Wang</li>
<li>for: 这项研究旨在实现多模式纤维（MMF）成像，以增强其在成像和光学通信中的应用可靠性。</li>
<li>methods: 这项研究使用白光LED和堆叠UNet实现MMF成像，并通过通道缝合技术提高重建效果。</li>
<li>results: 实验结果显示，MMF成像系统具有良好的鲁棒性特性，即平均归一化相关系数（PCC）为0.83，并且可以通过修改纤维形状进行灵活应用。<details>
<summary>Abstract</summary>
Multimode fiber (MMF) has been proven to have good potential in imaging and optical communication because of its advantages of small diameter and large mode numbers. However, due to the mode coupling and modal dispersion, it is very sensitive to environmental changes. Minor changes in the fiber shape can lead to difficulties in information reconstruction. Here, white LED and cascaded Unet are used to achieve MMF imaging to eliminate the effect of fiber perturbations. The output speckle patterns in three different color channels of the CCD camera produced by transferring images through the MMF are concatenated and inputted into the cascaded Unet using channel stitching technology to improve the reconstruction effects. The average Pearson correlation coefficient (PCC) of the reconstructed images from the Fashion-MINIST dataset is 0.83. In order to check the flexibility of such a system, perturbation tests on the image reconstruction capability by changing the fiber shapes are conducted. The experimental results show that the MMF imaging system has good robustness properties, i. e. the average PCC remains 0.83 even after completely changing the shape of the MMF. This research potentially provides a flexible approach for the practical application of MMF imaging.
</details>
<details>
<summary>摘要</summary>
多模式纤维（MMF）因其小直径和大模数的优点，在成像和光通信中具有良好的潜力。然而，由于模式相互作用和模态散射，MMF在环境变化时非常敏感。小量的纤维形状变化可能导致信息重建困难。在这里，白光LED和堆叠网络被用来实现MMF成像，以消除纤维变化的影响。 transferred images through the MMF的输出雾Patterns in three different color channels of the CCD camera were concatenated and inputted into the cascaded Unet using channel stitching technology to improve the reconstruction effects. The average Pearson correlation coefficient (PCC) of the reconstructed images from the Fashion-MINIST dataset was 0.83. To check the flexibility of the system, perturbation tests on the image reconstruction capability by changing the fiber shapes were conducted. The experimental results showed that the MMF imaging system has good robustness properties, i.e., the average PCC remained 0.83 even after completely changing the shape of the MMF. This research potentially provides a flexible approach for the practical application of MMF imaging.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Transformer-based-Dual-domain-Network-for-Few-view-Dedicated-Cardiac-SPECT-Image-Reconstructions"><a href="#Transformer-based-Dual-domain-Network-for-Few-view-Dedicated-Cardiac-SPECT-Image-Reconstructions" class="headerlink" title="Transformer-based Dual-domain Network for Few-view Dedicated Cardiac SPECT Image Reconstructions"></a>Transformer-based Dual-domain Network for Few-view Dedicated Cardiac SPECT Image Reconstructions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09624">http://arxiv.org/abs/2307.09624</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huidong Xie, Bo Zhou, Xiongchao Chen, Xueqi Guo, Stephanie Thorn, Yi-Hwa Liu, Ge Wang, Albert Sinusas, Chi Liu</li>
<li>for: 用于提高心血管疾病诊断的高质量3D射电心脏成像 reconstruction。</li>
<li>methods: 提议使用3D transformer-based dual-domain网络TIP-Net，首先直接将投影数据重构为3D射电心脏成像，然后使用图像域重构网络进行进一步重构。</li>
<li>results: 在Cardiac catheterization图像和临床诊断 interpretations中，与前一代方法相比，我们的方法生成的图像具有更高的心脏缺陷对比度，可能启用站立几视图专用射电心脏成像仪器实现高质量缺陷visualization。<details>
<summary>Abstract</summary>
Cardiovascular disease (CVD) is the leading cause of death worldwide, and myocardial perfusion imaging using SPECT has been widely used in the diagnosis of CVDs. The GE 530/570c dedicated cardiac SPECT scanners adopt a stationary geometry to simultaneously acquire 19 projections to increase sensitivity and achieve dynamic imaging. However, the limited amount of angular sampling negatively affects image quality. Deep learning methods can be implemented to produce higher-quality images from stationary data. This is essentially a few-view imaging problem. In this work, we propose a novel 3D transformer-based dual-domain network, called TIP-Net, for high-quality 3D cardiac SPECT image reconstructions. Our method aims to first reconstruct 3D cardiac SPECT images directly from projection data without the iterative reconstruction process by proposing a customized projection-to-image domain transformer. Then, given its reconstruction output and the original few-view reconstruction, we further refine the reconstruction using an image-domain reconstruction network. Validated by cardiac catheterization images, diagnostic interpretations from nuclear cardiologists, and defect size quantified by an FDA 510(k)-cleared clinical software, our method produced images with higher cardiac defect contrast on human studies compared with previous baseline methods, potentially enabling high-quality defect visualization using stationary few-view dedicated cardiac SPECT scanners.
</details>
<details>
<summary>摘要</summary>
Cardiovascular disease (CVD) 是全球最主要的死亡原因，而我脏性 perfusion imaging 使用 SPECT 已广泛应用于 CVD 的诊断。GE 530/570c 专门的卡ди亚特 SPECT 扫描仪采用站ARY geometry 同时获取 19 个投影，以提高敏感度和实现动态扫描。然而，有限的角度采样会影响图像质量。深度学习方法可以在站ARY数据上生成更高质量的图像。这是一个几视图图像问题。在这种情况下，我们提出了一种新的3D transformer-based dual-domain网络，称为 TIP-Net，用于高质量3D卡ди亚特 SPECT 图像重建。我们的方法首先从投影数据直接重建3D卡ди亚特 SPECT 图像，而不需要迭代重建过程。然后，给我们的重建输出和原始几视图重建结果，我们进一步修改重建结果使用图像领域重建网络。被 cardiac catheterization 图像、核心 Cardiologist 的诊断和 FDA 510(k) 审核通过的产品软件进行评估，我们的方法在人类研究中比前一代方法产生了更高的心脏缺陷对比，可能使得使用站ARY几视图专门卡ди亚特 SPECT 扫描仪获得高质量缺陷可视化。
</details></li>
</ul>
<hr>
<h2 id="A-comparative-analysis-of-SRGAN-models"><a href="#A-comparative-analysis-of-SRGAN-models" class="headerlink" title="A comparative analysis of SRGAN models"></a>A comparative analysis of SRGAN models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09456">http://arxiv.org/abs/2307.09456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fatemeh Rezapoor Nikroo, Ajinkya Deshmukh, Anantha Sharma, Adrian Tam, Kaarthik Kumar, Cleo Norris, Aditya Dangi</li>
<li>for: 这个研究检验了多种现代SRGAN（超分解生成对抗网络）模型，ESRGAN、Real-ESRGAN和EDSR，在一个基准数据集上进行了评估。</li>
<li>methods: 这些模型使用了一个管道来恶化图像，以评估其性能。</li>
<li>results: 我们发现，EDSR-BASE模型从huggingface中表现最佳，在量化指标和主观视觉质量评估中都显示出最高的性能，同时具有最小的计算开销。 EDSR生成的图像具有更高的峰峰信号噪声比（PSNR）和结构相似性指标（SSIM）值，并且可以通过Tesseract OCR引擎获得高质量的文本Recognition结果。这些发现表明，EDSR是一种稳定和有效的单图像超分解方法，可以用于应用场景需要高品质视觉和优化计算。<details>
<summary>Abstract</summary>
In this study, we evaluate the performance of multiple state-of-the-art SRGAN (Super Resolution Generative Adversarial Network) models, ESRGAN, Real-ESRGAN and EDSR, on a benchmark dataset of real-world images which undergo degradation using a pipeline. Our results show that some models seem to significantly increase the resolution of the input images while preserving their visual quality, this is assessed using Tesseract OCR engine. We observe that EDSR-BASE model from huggingface outperforms the remaining candidate models in terms of both quantitative metrics and subjective visual quality assessments with least compute overhead. Specifically, EDSR generates images with higher peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) values and are seen to return high quality OCR results with Tesseract OCR engine. These findings suggest that EDSR is a robust and effective approach for single-image super-resolution and may be particularly well-suited for applications where high-quality visual fidelity is critical and optimized compute.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们评估了多种现状顶尖SRGAN（超分解生成对抗网络）模型，ESRGAN、Real-ESRGAN和EDSR，对一个参考数据集中的实际图像进行了降低管道处理。我们的结果显示，一些模型能够明显提高输入图像的分辨率，同时保持图像的视觉质量，这被评估使用Tesseract OCR引擎。我们发现，来自huggingface的EDSR-BASE模型在对比其他候选模型的情况下，在量化指标和主观视觉质量评估中具有最低的计算开销。具体来说，EDSR模型能够生成高 peak signal-to-noise ratio（PSNR）和结构相似度指数（SSIM）值更高的图像，并且能够返回高质量的OCR结果。这些发现表明，EDSR是一种稳定有效的单图像超分解方法，可能是应用场景需要高质量视觉准确性和优化计算的情况下的佳选。
</details></li>
</ul>
<hr>
<h2 id="Measuring-Student-Behavioral-Engagement-using-Histogram-of-Actions"><a href="#Measuring-Student-Behavioral-Engagement-using-Histogram-of-Actions" class="headerlink" title="Measuring Student Behavioral Engagement using Histogram of Actions"></a>Measuring Student Behavioral Engagement using Histogram of Actions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09420">http://arxiv.org/abs/2307.09420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Abdelkawy, Islam Alkabbany, Asem Ali, Aly Farag</li>
<li>For: 这个论文旨在提出一种新的学生行为参与度测量技术，通过识别学生的行为来预测学生行为参与度水平。* Methods: 该方法使用人体骨架模型来模拟学生的姿势和上半身运动，并使用3D-CNN模型来学习学生上半身的动态。然后将动作识别结果转化为行为参与度 histogram，并将 histogram 作为输入给 SVM 分类器来判断学生是否参与度高或低。* Results: 对于1414个2分钟视频段和112个视频段，实验结果显示学生的动作可以达到100%的准确率（83.63%），而该方法也可以捕捉课堂中学生的平均参与度。<details>
<summary>Abstract</summary>
In this paper, we propose a novel technique for measuring behavioral engagement through students' actions recognition. The proposed approach recognizes student actions then predicts the student behavioral engagement level. For student action recognition, we use human skeletons to model student postures and upper body movements. To learn the dynamics of student upper body, a 3D-CNN model is used. The trained 3D-CNN model is used to recognize actions within every 2minute video segment then these actions are used to build a histogram of actions which encodes the student actions and their frequencies. This histogram is utilized as an input to SVM classifier to classify whether the student is engaged or disengaged. To evaluate the proposed framework, we build a dataset consisting of 1414 2-minute video segments annotated with 13 actions and 112 video segments annotated with two engagement levels. Experimental results indicate that student actions can be recognized with top 1 accuracy 83.63% and the proposed framework can capture the average engagement of the class.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的行为参与度测量技术，通过学生的动作识别。我们的方法首先识别学生的动作，然后预测学生的行为参与度水平。为实现学生动作识别，我们使用人体骨架模型来模拟学生的姿势和上半身运动。为了学习学生上半身的动态，我们使用3D-CNN模型进行训练。训练完成后，我们使用3D-CNN模型来识别每个2分钟视频段中的动作，然后将这些动作组织成一个动作频率分布图。这个图表被用作SVM分类器的输入，以判断学生是否参与度高或低。为评估我们的框架，我们建立了一个包含1414个2分钟视频段和112个视频段的数据集，每个视频段被标注为13种动作和两个参与度水平。实验结果表明，我们的方法可以识别学生动作的准确率达83.63%，并且可以捕捉出课堂中学生的平均参与度。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/07/19/eess.IV_2023_07_19/" data-id="cllt6z4kt0085q3886e6ec1ls" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/19/eess.AS_2023_07_19/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.AS - 2023-07-19 22:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/18/cs.LG_2023_07_18/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-07-18 18:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
