
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-07-19 18:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Android in the Wild: A Large-Scale Dataset for Android Device Control paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.10088 repo_url: https:&#x2F;&#x2F;github.com&#x2F;google-research&#x2F;google-research paper_authors: Christopher">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-07-19 18:00:00">
<meta property="og:url" content="http://nullscc.github.io/2023/07/19/cs.LG_2023_07_19/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Android in the Wild: A Large-Scale Dataset for Android Device Control paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.10088 repo_url: https:&#x2F;&#x2F;github.com&#x2F;google-research&#x2F;google-research paper_authors: Christopher">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-18T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:34.425Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_07_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/19/cs.LG_2023_07_19/" class="article-date">
  <time datetime="2023-07-18T16:00:00.000Z" itemprop="datePublished">2023-07-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-07-19 18:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Android-in-the-Wild-A-Large-Scale-Dataset-for-Android-Device-Control"><a href="#Android-in-the-Wild-A-Large-Scale-Dataset-for-Android-Device-Control" class="headerlink" title="Android in the Wild: A Large-Scale Dataset for Android Device Control"></a>Android in the Wild: A Large-Scale Dataset for Android Device Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10088">http://arxiv.org/abs/2307.10088</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research/google-research">https://github.com/google-research/google-research</a></li>
<li>paper_authors: Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, Timothy Lillicrap</li>
<li>for: 这个研究的目的是为了开发一种可以理解人类自然语言指令并直接控制数字设备用户界面的设备控制系统。</li>
<li>methods: 这个研究使用了一个大型的Android设备控制数据集（Android in the Wild，简称AITW），该数据集包含人类对设备交互的示例，以及相应的自然语言指令。数据集包含715万个 Episodes，涵盖30万个不同的指令，以及四个版本的Android（v10-13）和八种设备类型（Pixel 2 XL到Pixel 6）。</li>
<li>results: 研究人员开发了两个代理，并在数据集上进行了性能测试。数据集采用了一种新的挑战：从视觉上看到的动作需要被推断出来，而不是直接从UI元素上进行操作。此外，动作空间包括精确的手势（例如，水平滚动来操作轮播widget）。研究人员将数据集组织成了一种可以驱动device控制系统的Robustness分析，即如何在新的任务描述、应用程序或平台版本下进行性能测试。<details>
<summary>Abstract</summary>
There is a growing interest in device-control systems that can interpret human natural language instructions and execute them on a digital device by directly controlling its user interface. We present a dataset for device-control research, Android in the Wild (AITW), which is orders of magnitude larger than current datasets. The dataset contains human demonstrations of device interactions, including the screens and actions, and corresponding natural language instructions. It consists of 715k episodes spanning 30k unique instructions, four versions of Android (v10-13),and eight device types (Pixel 2 XL to Pixel 6) with varying screen resolutions. It contains multi-step tasks that require semantic understanding of language and visual context. This dataset poses a new challenge: actions available through the user interface must be inferred from their visual appearance. And, instead of simple UI element-based actions, the action space consists of precise gestures (e.g., horizontal scrolls to operate carousel widgets). We organize our dataset to encourage robustness analysis of device-control systems, i.e., how well a system performs in the presence of new task descriptions, new applications, or new platform versions. We develop two agents and report performance across the dataset. The dataset is available at https://github.com/google-research/google-research/tree/master/android_in_the_wild.
</details>
<details>
<summary>摘要</summary>
“现有一 growing interest in 设备控制系统，可以理解人类自然语言指令并直接控制其用户界面。我们提供了一个设备控制研究数据集，Android in the Wild（AITW），其规模比现有数据集有很大的提升。该数据集包含人类设备交互示例，包括屏幕和操作，以及相应的自然语言指令。其包含30k个独特指令，4个版本的Android（v10-13），8种设备类型（Pixel 2 XL到Pixel 6），具有不同的屏幕分辨率。它包含多步任务，需要语言和视觉上下文的含义理解。这个数据集呈现了一个新的挑战：通过视觉表现来推理操作。而不是简单的UI元素基于的操作，操作空间包括精确的手势（例如，水平滚动来操作轮播组件）。我们将数据集分类，以便对设备控制系统的Robustness分析，即系统在新的任务描述、新的应用程序或新版本平台上的性能如何。我们开发了两个代理，并在数据集上进行性能测试。数据集可以在https://github.com/google-research/google-research/tree/master/android_in_the_wild上获取。”
</details></li>
</ul>
<hr>
<h2 id="A-Dual-Formulation-for-Probabilistic-Principal-Component-Analysis"><a href="#A-Dual-Formulation-for-Probabilistic-Principal-Component-Analysis" class="headerlink" title="A Dual Formulation for Probabilistic Principal Component Analysis"></a>A Dual Formulation for Probabilistic Principal Component Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10078">http://arxiv.org/abs/2307.10078</a></li>
<li>repo_url: None</li>
<li>paper_authors: Henri De Plaen, Johan A. K. Suykens</li>
<li>for: 这篇论文描述了在希尔伯特空间上的概率主成分分析，并证明了优解方案可以表示在对偶空间上。</li>
<li>methods: 该论文使用了概率主成分分析和对偶空间的技术。</li>
<li>results: 论文证明了概率主成分分析可以开发出生成框架，并 illustrate了这种方法在一个玩具数据集和一个实际数据集上的工作。<details>
<summary>Abstract</summary>
In this paper, we characterize Probabilistic Principal Component Analysis in Hilbert spaces and demonstrate how the optimal solution admits a representation in dual space. This allows us to develop a generative framework for kernel methods. Furthermore, we show how it englobes Kernel Principal Component Analysis and illustrate its working on a toy and a real dataset.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们Characterize了概率主成分分析（Probabilistic Principal Component Analysis，PPCA）在希尔伯特空间中的形式，并证明了优解方式可以在副空间中表示。这允许我们开发一个基于kernel方法的生成框架。此外，我们还证明了PPCA包含内核主成分分析（Kernel Principal Component Analysis，KPCA），并在一个玩具数据集和一个实际数据集上 Illustrates its work。Note: "希尔伯特空间" (Hilbert space) is translated as "希尔伯特空间" in Simplified Chinese, and "主成分分析" (Principal Component Analysis) is translated as "主成分分析" in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Accuracy-Estimation-of-Deep-Visual-Models-using-Domain-Adaptive-Adversarial-Perturbation-without-Source-Samples"><a href="#Unsupervised-Accuracy-Estimation-of-Deep-Visual-Models-using-Domain-Adaptive-Adversarial-Perturbation-without-Source-Samples" class="headerlink" title="Unsupervised Accuracy Estimation of Deep Visual Models using Domain-Adaptive Adversarial Perturbation without Source Samples"></a>Unsupervised Accuracy Estimation of Deep Visual Models using Domain-Adaptive Adversarial Perturbation without Source Samples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10062">http://arxiv.org/abs/2307.10062</a></li>
<li>repo_url: None</li>
<li>paper_authors: JoonHo Lee, Jae Oh Woo, Hankyu Moon, Kwonho Lee</li>
<li>for: 这篇论文是为了解决深度视觉模型在不同分布上表现下降的问题，因为存在源和目标分布之间的差异。</li>
<li>methods: 这篇论文使用了 Pseudo-labels 来估计目标统计中的偏差，并且运用了最近的源自由领域适应算法。</li>
<li>results: 这篇论文的结果显示，使用 Pseudo-labels 来估计模型在目标统计中的偏差，可以实现不需要源数据和标签的情况下，并且比过去需要源数据和标签的方法表现更好。<details>
<summary>Abstract</summary>
Deploying deep visual models can lead to performance drops due to the discrepancies between source and target distributions. Several approaches leverage labeled source data to estimate target domain accuracy, but accessing labeled source data is often prohibitively difficult due to data confidentiality or resource limitations on serving devices. Our work proposes a new framework to estimate model accuracy on unlabeled target data without access to source data. We investigate the feasibility of using pseudo-labels for accuracy estimation and evolve this idea into adopting recent advances in source-free domain adaptation algorithms. Our approach measures the disagreement rate between the source hypothesis and the target pseudo-labeling function, adapted from the source hypothesis. We mitigate the impact of erroneous pseudo-labels that may arise due to a high ideal joint hypothesis risk by employing adaptive adversarial perturbation on the input of the target model. Our proposed source-free framework effectively addresses the challenging distribution shift scenarios and outperforms existing methods requiring source data and labels for training.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Deploying deep visual models can lead to performance drops due to the discrepancies between source and target distributions. Several approaches leverage labeled source data to estimate target domain accuracy, but accessing labeled source data is often prohibitively difficult due to data confidentiality or resource limitations on serving devices. Our work proposes a new framework to estimate model accuracy on unlabeled target data without access to source data. We investigate the feasibility of using pseudo-labels for accuracy estimation and evolve this idea into adopting recent advances in source-free domain adaptation algorithms. Our approach measures the disagreement rate between the source hypothesis and the target pseudo-labeling function, adapted from the source hypothesis. We mitigate the impact of erroneous pseudo-labels that may arise due to a high ideal joint hypothesis risk by employing adaptive adversarial perturbation on the input of the target model. Our proposed source-free framework effectively addresses the challenging distribution shift scenarios and outperforms existing methods requiring source data and labels for training."into Simplified Chinese.Here's the translation:<<SYS>>发布深度视觉模型可能会导致性能下降，这是因为源和目标分布之间存在差异。一些方法利用标注的源数据来估计目标频道准确率，但是获取标注的源数据经常是由数据保密或服务器上的资源限制所困难。我们的工作提出了一个新的框架，可以在无法访问源数据的情况下估计目标频道准确率。我们investigate了使用 Pseudo-labels 来估计准确率，并采用了最新的无源适应频道适应算法。我们的方法测量源假设和目标假设函数之间的不一致率，这个概念来自源假设。我们还使用适应式对抗扰动来减轻高理想共同风险引起的假设扰动的影响。我们的提议的无源框架可以有效地解决分布转换场景，并且超越了需要源数据和标签进行训练的现有方法。
</details></li>
</ul>
<hr>
<h2 id="Accurate-deep-learning-sub-grid-scale-models-for-large-eddy-simulations"><a href="#Accurate-deep-learning-sub-grid-scale-models-for-large-eddy-simulations" class="headerlink" title="Accurate deep learning sub-grid scale models for large eddy simulations"></a>Accurate deep learning sub-grid scale models for large eddy simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10060">http://arxiv.org/abs/2307.10060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rikhi Bose, Arunabha M. Roy<br>for:这两种模型是用于大涨�edy simulation（LES）目的的尺度下降涨�edy模型。methods:这两种模型使用了深度学习（DL）算法，与现有的分析模型技术不同，可以生成高阶复杂非线性关系 между输入和输出。results:两种模型在不同的滤波宽度和 Reynolds 数下进行了预测，并且在统计性能指标上表现出了不同的result。 simpler 模型具有更好的特征学习能力，因此在预测 SGS 压力方面表现出了更高的水平。<details>
<summary>Abstract</summary>
We present two families of sub-grid scale (SGS) turbulence models developed for large-eddy simulation (LES) purposes. Their development required the formulation of physics-informed robust and efficient Deep Learning (DL) algorithms which, unlike state-of-the-art analytical modeling techniques can produce high-order complex non-linear relations between inputs and outputs. Explicit filtering of data from direct simulations of the canonical channel flow at two friction Reynolds numbers $Re_\tau\approx 395$ and 590 provided accurate data for training and testing. The two sets of models use different network architectures. One of the architectures uses tensor basis neural networks (TBNN) and embeds the simplified analytical model form of the general effective-viscosity hypothesis, thus incorporating the Galilean, rotational and reflectional invariances. The other architecture is that of a relatively simple network, that is able to incorporate the Galilean invariance only. However, this simpler architecture has better feature extraction capacity owing to its ability to establish relations between and extract information from cross-components of the integrity basis tensors and the SGS stresses. Both sets of models are used to predict the SGS stresses for feature datasets generated with different filter widths, and at different Reynolds numbers. It is shown that due to the simpler model's better feature learning capabilities, it outperforms the invariance embedded model in statistical performance metrics. In a priori tests, both sets of models provide similar levels of dissipation and backscatter. Based on the test results, both sets of models should be usable in a posteriori actual LESs.
</details>
<details>
<summary>摘要</summary>
我们提出了两家互相关联的子grid尺度湍流（SGS）模型，用于大扰拍� simulation（LES）目的。它们的发展需要了实现物理学 Informed 强大和有效的深度学习（DL）算法，不同于现有的分析模型技术，可以生成高阶复杂的非线性关系。我们从直接实验的标准 кананиче流道流动中获取了精确的数据进行训练和测试。这两组模型使用不同的网络架构。一个架构使用tensor基础神经网络（TBNN），并将简化的分析模型形式给嵌入通用有效黏度假设，因此包含加利ле安、Rotational和反射symmetries。另一个架构是一个相对简单的网络，它能够包含加利ле安对称性，但这个简单的架构具有更好的特征提取能力，因为它可以在标�âxis和SGS压力之间建立关系和提取信息。这两组模型用于预测SGS压力的预测dataset，生成了不同滤镜宽度和 Reynolds 数的特征。根据测试结果，简单的模型在统计性能指标上表现较好，但两组模型在预�期� TEST 中提供了相似的损失和反射。因此，这两组模型可以在 posteriori 实际 LES 中使用。
</details></li>
</ul>
<hr>
<h2 id="Convergence-Guarantees-for-Stochastic-Subgradient-Methods-in-Nonsmooth-Nonconvex-Optimization"><a href="#Convergence-Guarantees-for-Stochastic-Subgradient-Methods-in-Nonsmooth-Nonconvex-Optimization" class="headerlink" title="Convergence Guarantees for Stochastic Subgradient Methods in Nonsmooth Nonconvex Optimization"></a>Convergence Guarantees for Stochastic Subgradient Methods in Nonsmooth Nonconvex Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10053">http://arxiv.org/abs/2307.10053</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xnchxy/GeneralSGD">https://github.com/xnchxy/GeneralSGD</a></li>
<li>paper_authors: Nachuan Xiao, Xiaoyin Hu, Kim-Chuan Toh</li>
<li>for:  investigate the convergence properties of stochastic gradient descent (SGD) method and its variants, especially in training neural networks built from nonsmooth activation functions.</li>
<li>methods:  develop a novel framework that assigns different timescales to stepsizes for updating the momentum terms and variables, and prove the global convergence of the proposed framework in both single-timescale and two-timescale cases.</li>
<li>results:  prove that the proposed framework encompasses a wide range of well-known SGD-type methods, including heavy-ball SGD, SignSGD, Lion, normalized SGD and clipped SGD, and demonstrate the high efficiency of the analyzed SGD-type methods through preliminary numerical experiments.<details>
<summary>Abstract</summary>
In this paper, we investigate the convergence properties of the stochastic gradient descent (SGD) method and its variants, especially in training neural networks built from nonsmooth activation functions. We develop a novel framework that assigns different timescales to stepsizes for updating the momentum terms and variables, respectively. Under mild conditions, we prove the global convergence of our proposed framework in both single-timescale and two-timescale cases. We show that our proposed framework encompasses a wide range of well-known SGD-type methods, including heavy-ball SGD, SignSGD, Lion, normalized SGD and clipped SGD. Furthermore, when the objective function adopts a finite-sum formulation, we prove the convergence properties for these SGD-type methods based on our proposed framework. In particular, we prove that these SGD-type methods find the Clarke stationary points of the objective function with randomly chosen stepsizes and initial points under mild assumptions. Preliminary numerical experiments demonstrate the high efficiency of our analyzed SGD-type methods.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究束定的泛化特性，尤其是使用不规则激活函数construct neural networks的SGD方法和其变体。我们开发了一种新的框架，它在更新势能阶段和变量阶段分别 assign different timescales。在某些轻度条件下，我们证明了我们提议的框架的全球收敛性。我们还证明了我们的框架包括许多已知的SGD类型方法，包括重力SGD、SignSGD、Lion、normalized SGD和clipped SGD。在对象函数采用finite-sum形式时，我们证明了这些SGD类型方法的收敛性基于我们的框架。具体来说，我们证明这些SGD类型方法可以随机选择步长和初始点，并在一定假设下找到对象函数的clarke站点。先前的数值实验表明了我们分析的SGD类型方法的高效性。
</details></li>
</ul>
<hr>
<h2 id="Contextual-Reliability-When-Different-Features-Matter-in-Different-Contexts"><a href="#Contextual-Reliability-When-Different-Features-Matter-in-Different-Contexts" class="headerlink" title="Contextual Reliability: When Different Features Matter in Different Contexts"></a>Contextual Reliability: When Different Features Matter in Different Contexts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10026">http://arxiv.org/abs/2307.10026</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gaurav Ghosal, Amrith Setlur, Daniel S. Brown, Anca D. Dragan, Aditi Raghunathan</li>
<li>for: 这篇论文旨在解决深度神经网络在依据偶极关系时出现catastrophic failure的问题。</li>
<li>methods: 该论文提出了一种新的设定 Contextual Reliability，以及一种两stage的解决方案 Explicit Non-spurious feature Prediction (ENP)，可以在不同上下文中选择适当的特征来避免偶极关系。</li>
<li>results: 论文通过理论和实验验证了ENP的优势，并提供了新的Contextual Reliability的benchmark。<details>
<summary>Abstract</summary>
Deep neural networks often fail catastrophically by relying on spurious correlations. Most prior work assumes a clear dichotomy into spurious and reliable features; however, this is often unrealistic. For example, most of the time we do not want an autonomous car to simply copy the speed of surrounding cars -- we don't want our car to run a red light if a neighboring car does so. However, we cannot simply enforce invariance to next-lane speed, since it could provide valuable information about an unobservable pedestrian at a crosswalk. Thus, universally ignoring features that are sometimes (but not always) reliable can lead to non-robust performance. We formalize a new setting called contextual reliability which accounts for the fact that the "right" features to use may vary depending on the context. We propose and analyze a two-stage framework called Explicit Non-spurious feature Prediction (ENP) which first identifies the relevant features to use for a given context, then trains a model to rely exclusively on these features. Our work theoretically and empirically demonstrates the advantages of ENP over existing methods and provides new benchmarks for contextual reliability.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard writing system used in mainland China. The translation is based on the standardized grammar and vocabulary of Simplified Chinese, and may differ from the traditional Chinese writing system used in Hong Kong and Taiwan.
</details></li>
</ul>
<hr>
<h2 id="Europepolls-A-Dataset-of-Country-Level-Opinion-Polling-Data-for-the-European-Union-and-the-UK"><a href="#Europepolls-A-Dataset-of-Country-Level-Opinion-Polling-Data-for-the-European-Union-and-the-UK" class="headerlink" title="Europepolls: A Dataset of Country-Level Opinion Polling Data for the European Union and the UK"></a>Europepolls: A Dataset of Country-Level Opinion Polling Data for the European Union and the UK</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10022">http://arxiv.org/abs/2307.10022</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/konstantinos-p/europepolls">https://github.com/konstantinos-p/europepolls</a></li>
<li>paper_authors: Konstantinos Pitas</li>
<li>for: 这个论文的目的是提供一个开放的国家级历史意见调查数据集 для欧盟和英国，填补现有的意见调查数据 gap。</li>
<li>methods: 这个论文使用Wikipedia数据和pandas库进行了处理，并提供了Raw和Preprocessed数据在.csv格式下。</li>
<li>results: 这个论文的数据集可以帮助研究人员通过对多 modal数据（新闻文章、经济指标、社交媒体）和选举行为之间的复杂相互作用进行研究。<details>
<summary>Abstract</summary>
I propose an open dataset of country-level historical opinion polling data for the European Union and the UK. The dataset aims to fill a gap in available opinion polling data for the European Union. Some existing datasets are restricted to the past five years, limiting research opportunities. At the same time, some larger proprietary datasets exist but are available only in a visual preprocessed time series format. Finally, while other large datasets for individual countries might exist, these could be inaccessible due to language barriers. The data was gathered from Wikipedia, and preprocessed using the pandas library. Both the raw and the preprocessed data are in the .csv format. I hope that given the recent advances in LLMs and deep learning in general, this large dataset will enable researchers to uncover complex interactions between multimodal data (news articles, economic indicators, social media) and voting behavior. The raw data, the preprocessed data, and the preprocessing scripts are available on GitHub.
</details>
<details>
<summary>摘要</summary>
我提议一个开放的国家级历史民意调查数据集 для欧盟和英国。这个数据集的目标是填充有限的available民意调查数据集 для欧盟。一些现有的数据集只有过去五年的数据，限制了研究机会。同时，一些更大的专有数据集存在，但只能在可视化预处理时间序列格式下获得。此外，尚存在其他各国大数据集，但可能因语言障碍而无法访问。数据来源于Wikipedia，并使用pandas库进行预处理。原始数据和预处理后的数据均在.csv格式下可用。我希望，随着最近的LLMs和深度学习技术的发展，这个大数据集将帮助研究人员发现多Modal数据（新闻文章、经济指标、社交媒体）和选举行为之间的复杂交互。原始数据、预处理后数据以及预处理脚本都可以在GitHub上下载。
</details></li>
</ul>
<hr>
<h2 id="TbExplain-A-Text-based-Explanation-Method-for-Scene-Classification-Models-with-the-Statistical-Prediction-Correction"><a href="#TbExplain-A-Text-based-Explanation-Method-for-Scene-Classification-Models-with-the-Statistical-Prediction-Correction" class="headerlink" title="TbExplain: A Text-based Explanation Method for Scene Classification Models with the Statistical Prediction Correction"></a>TbExplain: A Text-based Explanation Method for Scene Classification Models with the Statistical Prediction Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10003">http://arxiv.org/abs/2307.10003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirhossein Aminimehr, Pouya Khani, Amirali Molaei, Amirmohammad Kazemeini, Erik Cambria</li>
<li>for: 该文章目的是提高黑盒机器学习模型的可解释性，并使用 XAI 技术和预训练对象检测器来提供场景分类模型的文本基于解释。</li>
<li>methods: 该文章使用了一种名为 TbExplain 的框架，该框架使用 XAI 技术和预训练对象检测器来提供文本基于解释，并在初始预测不可靠时使用一种新的方法来修正预测和文本基于解释。</li>
<li>results: 对于Scene Classification Task，TbExplain 与 ResNet 变体进行了量化和质量测试，结果表明 TbExplain 可以提高分类精度。<details>
<summary>Abstract</summary>
The field of Explainable Artificial Intelligence (XAI) aims to improve the interpretability of black-box machine learning models. Building a heatmap based on the importance value of input features is a popular method for explaining the underlying functions of such models in producing their predictions. Heatmaps are almost understandable to humans, yet they are not without flaws. Non-expert users, for example, may not fully understand the logic of heatmaps (the logic in which relevant pixels to the model's prediction are highlighted with different intensities or colors). Additionally, objects and regions of the input image that are relevant to the model prediction are frequently not entirely differentiated by heatmaps. In this paper, we propose a framework called TbExplain that employs XAI techniques and a pre-trained object detector to present text-based explanations of scene classification models. Moreover, TbExplain incorporates a novel method to correct predictions and textually explain them based on the statistics of objects in the input image when the initial prediction is unreliable. To assess the trustworthiness and validity of the text-based explanations, we conducted a qualitative experiment, and the findings indicated that these explanations are sufficiently reliable. Furthermore, our quantitative and qualitative experiments on TbExplain with scene classification datasets reveal an improvement in classification accuracy over ResNet variants.
</details>
<details>
<summary>摘要</summary>
黑盒机器学习模型的解释 artificial intelligence (XAI) 目标是将模型预测的解释提高到可解释的程度。建立基于输入特征价值的热力地图是一种广泛使用的方法来解释这些模型在预测中的下面运作。热力地图可以让人类更好地理解，但是它们并不是无懈的。例如，非专家用户可能无法完全理解热力地图中的逻辑（在预测中重要的像素点获得不同的强度或颜色）。此外，输入图像中对模型预测的重要物件和区域也经常不会被热力地图完全区别开来。在这篇文章中，我们提出了一个名为 TbExplain 的框架，它利用 XAI 技术和预训的物件探测器来提供文本基于解释Scene Classification 模型。此外，TbExplain 还包括一种新的方法，可以根据输入图像中物件的统计数据修正预测并文本解释它们，当初始预测不可靠时。为了评估文本解释的可信度和有效性，我们进行了一个质感实验，结果显示了这些解释是可靠的。此外，我们还进行了量化和质感实验，结果显示 TbExplain 在Scene Classification 数据集上具有改善类别精度的能力。
</details></li>
</ul>
<hr>
<h2 id="Impact-of-Disentanglement-on-Pruning-Neural-Networks"><a href="#Impact-of-Disentanglement-on-Pruning-Neural-Networks" class="headerlink" title="Impact of Disentanglement on Pruning Neural Networks"></a>Impact of Disentanglement on Pruning Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09994">http://arxiv.org/abs/2307.09994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carl Shneider, Peyman Rostami, Anis Kacem, Nilotpal Sinha, Abd El Rahman Shabayek, Djamila Aouada</li>
<li>for: 实现Edge设备上深度学习神经网络的运行，以完成实际世界中的任务特定目标，需要对神经网络进行压缩。</li>
<li>methods: 使用缓存自动encoder网络生成分离的内存表现，以实现模型压缩。</li>
<li>results: 透过强制网络学习分离表现，进行条件压缩，以实现分离的实现。<details>
<summary>Abstract</summary>
Deploying deep learning neural networks on edge devices, to accomplish task specific objectives in the real-world, requires a reduction in their memory footprint, power consumption, and latency. This can be realized via efficient model compression. Disentangled latent representations produced by variational autoencoder (VAE) networks are a promising approach for achieving model compression because they mainly retain task-specific information, discarding useless information for the task at hand. We make use of the Beta-VAE framework combined with a standard criterion for pruning to investigate the impact of forcing the network to learn disentangled representations on the pruning process for the task of classification. In particular, we perform experiments on MNIST and CIFAR10 datasets, examine disentanglement challenges, and propose a path forward for future works.
</details>
<details>
<summary>摘要</summary>
部署深度学习神经网络在边缘设备上，以实现实际世界中的任务特定目标，需要减少其内存占用量、功耗和延迟时间。这可以通过有效的模型压缩来实现。Variational autoencoder（VAE）网络生成的分离 latent representation 是一种有前途的方法，因为它们主要保留任务特定的信息，抛弃不必要的信息。我们使用 Beta-VAE 框架并与标准的剪枝 criterion 结合，研究在 classification 任务上强制网络学习分离表示的影响。在具体来说，我们在 MNIST 和 CIFAR10 数据集上进行实验，探讨分离挑战，并提出未来工作的道路。
</details></li>
</ul>
<hr>
<h2 id="UniMatch-A-Unified-User-Item-Matching-Framework-for-the-Multi-purpose-Merchant-Marketing"><a href="#UniMatch-A-Unified-User-Item-Matching-Framework-for-the-Multi-purpose-Merchant-Marketing" class="headerlink" title="UniMatch: A Unified User-Item Matching Framework for the Multi-purpose Merchant Marketing"></a>UniMatch: A Unified User-Item Matching Framework for the Multi-purpose Merchant Marketing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09989">http://arxiv.org/abs/2307.09989</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qifang Zhao, Tianyu Li, Meng Du, Yu Jiang, Qinghui Sun, Zhongyao Wang, Hong Liu, Huan Xu</li>
<li>for: 降低私域市场营销Cloud服务的成本</li>
<li>methods: 一个简单的用户项目匹配框架，同时实现用户定向和项目推荐，只需一个模型</li>
<li>results: 比 estado-of-the-art方法更高的性能，减少计算资源和日常维护成本<details>
<summary>Abstract</summary>
When doing private domain marketing with cloud services, the merchants usually have to purchase different machine learning models for the multiple marketing purposes, leading to a very high cost. We present a unified user-item matching framework to simultaneously conduct item recommendation and user targeting with just one model. We empirically demonstrate that the above concurrent modeling is viable via modeling the user-item interaction matrix with the multinomial distribution, and propose a bidirectional bias-corrected NCE loss for the implementation. The proposed loss function guides the model to learn the user-item joint probability $p(u,i)$ instead of the conditional probability $p(i|u)$ or $p(u|i)$ through correcting both the users and items' biases caused by the in-batch negative sampling. In addition, our framework is model-agnostic enabling a flexible adaptation of different model architectures. Extensive experiments demonstrate that our framework results in significant performance gains in comparison with the state-of-the-art methods, with greatly reduced cost on computing resources and daily maintenance.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:在私有领域市场营销中，商户通常需要购买不同的机器学习模型来满足多种营销目的，这会导致极高的成本。我们提出了一个统一用户 Item 匹配框架，可同时进行 Item 推荐和用户定向。我们采用多omial分布来模型用户 Item 交互矩阵，并提出了一种双向偏置修正 NCE 损失函数。该损失函数导引模型学习用户 Item 共同概率 $p(u,i)$，而不是用户或 Item 的 conditional probability $p(i|u)$ 或 $p(u|i)$。我们的框架是模型无关的，可以适应不同的模型架构。我们的实验结果表明，我们的框架可以与状态 искус法方法进行比较，并且具有明显的成本减少和日常维护降低。
</details></li>
</ul>
<hr>
<h2 id="TinyTrain-Deep-Neural-Network-Training-at-the-Extreme-Edge"><a href="#TinyTrain-Deep-Neural-Network-Training-at-the-Extreme-Edge" class="headerlink" title="TinyTrain: Deep Neural Network Training at the Extreme Edge"></a>TinyTrain: Deep Neural Network Training at the Extreme Edge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09988">http://arxiv.org/abs/2307.09988</a></li>
<li>repo_url: None</li>
<li>paper_authors: Young D. Kwon, Rui Li, Stylianos I. Venieris, Jagmohan Chauhan, Nicholas D. Lane, Cecilia Mascolo</li>
<li>for: 这篇论文的目的是提出一种基于设备上训练的个人化和隐私保护方法，并且因应物联网设备的数据稀缺和 computation 资源的限制，以提高训练时间的效率和精度。</li>
<li>methods: 本论文提出的方法是基于 selective 更新和硬件约束的实现，将网络模型分成多个部分，并且适当地更新每个部分，以减少训练时间和内存负载。</li>
<li>results: 本论文的实验结果显示，该方法可以与基于整个网络的 fine-tuning 方法相比，提高精度并且减少训练时间和内存负载。另外，该方法还可以在实际的边缘设备上进行训练，并且可以与现有的设备进行集成。<details>
<summary>Abstract</summary>
On-device training is essential for user personalisation and privacy. With the pervasiveness of IoT devices and microcontroller units (MCU), this task becomes more challenging due to the constrained memory and compute resources, and the limited availability of labelled user data. Nonetheless, prior works neglect the data scarcity issue, require excessively long training time (e.g. a few hours), or induce substantial accuracy loss ($\geq$10\%). We propose TinyTrain, an on-device training approach that drastically reduces training time by selectively updating parts of the model and explicitly coping with data scarcity. TinyTrain introduces a task-adaptive sparse-update method that dynamically selects the layer/channel based on a multi-objective criterion that jointly captures user data, the memory, and the compute capabilities of the target device, leading to high accuracy on unseen tasks with reduced computation and memory footprint. TinyTrain outperforms vanilla fine-tuning of the entire network by 3.6-5.0\% in accuracy, while reducing the backward-pass memory and computation cost by up to 2,286$\times$ and 7.68$\times$, respectively. Targeting broadly used real-world edge devices, TinyTrain achieves 9.5$\times$ faster and 3.5$\times$ more energy-efficient training over status-quo approaches, and 2.8$\times$ smaller memory footprint than SOTA approaches, while remaining within the 1 MB memory envelope of MCU-grade platforms.
</details>
<details>
<summary>摘要</summary>
<<SYS>>通过设备直接训练实现用户个性化和隐私，因为互联网对象（IoT）设备和微控制器单元（MCU）的普遍存在，这种任务变得更加挑战性。然而，先前的研究忽视了数据缺乏问题，需要过长的训练时间（例如几个小时），或者导致重大的准确性损失（至少10%）。我们提出了TinyTrain，一种在设备上进行训练的方法，可以减少训练时间，并且明确地处理数据缺乏问题。TinyTrain引入了任务适应的稀疏更新方法，可以在多目标 criterion 中动态选择层/通道，以高度准确地处理未见任务，并且减少了计算和存储开销。相比于普通的精度调整，TinyTrain提高了9.5倍的训练速度和3.5倍的能效性，同时占用的内存占用面积只有1MB，比SOTAapproach小3.8倍。targeting 广泛使用的实际世界边缘设备，TinyTrain实现了9.5倍的训练速度和3.5倍的能效性，同时占用的内存占用面积只有1MB，比SOTAapproach小3.8倍。Note: "SOTA" stands for "State of the Art", which means the current best performance in a particular field or task.
</details></li>
</ul>
<hr>
<h2 id="Learner-Referral-for-Cost-Effective-Federated-Learning-Over-Hierarchical-IoT-Networks"><a href="#Learner-Referral-for-Cost-Effective-Federated-Learning-Over-Hierarchical-IoT-Networks" class="headerlink" title="Learner Referral for Cost-Effective Federated Learning Over Hierarchical IoT Networks"></a>Learner Referral for Cost-Effective Federated Learning Over Hierarchical IoT Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09977">http://arxiv.org/abs/2307.09977</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yulan Gao, Ziqiang Ye, Yue Xiao, Wei Xiang</li>
<li>for: 提高 federated learning 中数据隐私护卫和资源占用的问题</li>
<li>methods: 提出了joint learner referral协助 federated client选择（LRef-FedCS），通信和计算资源调度，以及本地模型准确率优化（LMAO）方法，以最小化worst-case参与者的成本，保证 federated learning 在层次互联网络中长期公平</li>
<li>results: 通过LYAPUNOV优化技术，将原问题转化为Stepwise Joint Optimization Problem（JOP），并使用分布式LRef-FedCS方法和自适应全球最佳匹配搜索（SGHS）算法进行解决，实现了规模化和可扩展性。numerical simulation和实验结果表明，提出的LRef-FedCS方法可以很好地平衡追求高全球准确率和降低成本。<details>
<summary>Abstract</summary>
The paradigm of federated learning (FL) to address data privacy concerns by locally training parameters on resource-constrained clients in a distributed manner has garnered significant attention. Nonetheless, FL is not applicable when not all clients within the coverage of the FL server are registered with the FL network. To bridge this gap, this paper proposes joint learner referral aided federated client selection (LRef-FedCS), along with communications and computing resource scheduling, and local model accuracy optimization (LMAO) methods. These methods are designed to minimize the cost incurred by the worst-case participant and ensure the long-term fairness of FL in hierarchical Internet of Things (HieIoT) networks. Utilizing the Lyapunov optimization technique, we reformulate the original problem into a stepwise joint optimization problem (JOP). Subsequently, to tackle the mixed-integer non-convex JOP, we separatively and iteratively address LRef-FedCS and LMAO through the centralized method and self-adaptive global best harmony search (SGHS) algorithm, respectively. To enhance scalability, we further propose a distributed LRef-FedCS approach based on a matching game to replace the centralized method described above. Numerical simulations and experimental results on the MNIST/CIFAR-10 datasets demonstrate that our proposed LRef-FedCS approach could achieve a good balance between pursuing high global accuracy and reducing cost.
</details>
<details>
<summary>摘要</summary>
“联邦学习（FL）的模式以地方化训练参数，在资源有限的客户端上进行分布式方式，吸引了广泛关注。然而，FL不适用于所有客户端都注册在FL网络中。为了填补这个差距，这篇文章提出了共同学习者引导协助 federated client 选择（LRef-FedCS），以及通信和计算资源分配，本地模型精度优化（LMAO）方法。这些方法旨在最小化最差情况下的成本，并确保长期的公平性在层次互联网络（HieIoT）中。使用 Lyapunov 优化技术，我们将原始问题转换为一个步骤化的共同优化问题（JOP）。然后，为了解决混合整数非凸 JOP，我们采用分类和迭代地 addresses LRef-FedCS 和 LMAO，分别使用中央化方法和自适应全球最佳搜索（SGHS）算法。为了提高可扩展性，我们还提出了基于对称游戏的分布式 LRef-FedCS 方法。数据验证和实验结果显示，我们的提出的 LRef-FedCS 方法可以实现高度的全球精度，同时降低成本。”
</details></li>
</ul>
<hr>
<h2 id="Towards-green-AI-based-software-systems-an-architecture-centric-approach-GAISSA"><a href="#Towards-green-AI-based-software-systems-an-architecture-centric-approach-GAISSA" class="headerlink" title="Towards green AI-based software systems: an architecture-centric approach (GAISSA)"></a>Towards green AI-based software systems: an architecture-centric approach (GAISSA)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09964">http://arxiv.org/abs/2307.09964</a></li>
<li>repo_url: None</li>
<li>paper_authors: Silverio Martínez-Fernández, Xavier Franch, Francisco Durán</li>
<li>for: 提高人工智能系统的能效性，满足社会上的能源协调需求。</li>
<li>methods: 提供数据科学家和软件工程师工具支持，建立体系中心的方法，用于模拟和开发绿色人工智能系统。</li>
<li>results: 初步研究结果表明，GAISSA项目可能实现其目标。<details>
<summary>Abstract</summary>
Nowadays, AI-based systems have achieved outstanding results and have outperformed humans in different domains. However, the processes of training AI models and inferring from them require high computational resources, which pose a significant challenge in the current energy efficiency societal demand. To cope with this challenge, this research project paper describes the main vision, goals, and expected outcomes of the GAISSA project. The GAISSA project aims at providing data scientists and software engineers tool-supported, architecture-centric methods for the modelling and development of green AI-based systems. Although the project is in an initial stage, we describe the current research results, which illustrate the potential to achieve GAISSA objectives.
</details>
<details>
<summary>摘要</summary>
现在，基于人工智能的系统已经取得了出色的成绩，在不同的领域都超越了人类。然而，训练人工智能模型和从其进行推理需要高度的计算资源，这对现代社会能源效率的要求带来了重大挑战。为了解决这个挑战，本研究项目paper描述了GAISSA项目的主要视野、目标和预期结果。GAISSA项目旨在为数据科学家和软件工程师提供工具支持、体系中心的方法，以开发可持续的人工智能基本系统。虽然项目还在初始阶段，我们将介绍当前的研究成果，以ILLUSTRATEGAISSA目标的潜在实现可能性。
</details></li>
</ul>
<hr>
<h2 id="XSkill-Cross-Embodiment-Skill-Discovery"><a href="#XSkill-Cross-Embodiment-Skill-Discovery" class="headerlink" title="XSkill: Cross Embodiment Skill Discovery"></a>XSkill: Cross Embodiment Skill Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09955">http://arxiv.org/abs/2307.09955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengda Xu, Zhenjia Xu, Cheng Chi, Manuela Veloso, Shuran Song</li>
<li>for: 本研究的目的是开发一种基于模仿学习的机器人学习框架，以便从人类视频中提取可重用的机器人操作技能。</li>
<li>methods: 本研究使用了一种名为XSkill的模仿学习框架，该框架可以在没有标注数据的情况下，从人类和机器人的抓取视频中分析出各种操作技能的核心特征，并将其转移到机器人的动作中。</li>
<li>results: 实验结果表明，XSkill可以帮助机器人学习并执行未经见过的任务，并且可以很好地组合已知的技能来实现新的任务。此外，XSkill还可以在实际环境中进行应用。<details>
<summary>Abstract</summary>
Human demonstration videos are a widely available data source for robot learning and an intuitive user interface for expressing desired behavior. However, directly extracting reusable robot manipulation skills from unstructured human videos is challenging due to the big embodiment difference and unobserved action parameters. To bridge this embodiment gap, this paper introduces XSkill, an imitation learning framework that 1) discovers a cross-embodiment representation called skill prototypes purely from unlabeled human and robot manipulation videos, 2) transfers the skill representation to robot actions using conditional diffusion policy, and finally, 3) composes the learned skill to accomplish unseen tasks specified by a human prompt video. Our experiments in simulation and real-world environments show that the discovered skill prototypes facilitate both skill transfer and composition for unseen tasks, resulting in a more general and scalable imitation learning framework. The performance of XSkill is best understood from the anonymous website: https://xskillcorl.github.io.
</details>
<details>
<summary>摘要</summary>
人类示例视频是机器人学习的广泛可用数据源，同时也是一种直观的用户界面，可以表达机器人需要的行为。然而，直接从无结构的人类视频中提取可重用的机器人操作技能是困难的，因为人类和机器人之间存在大的实体差异和未观察到的动作参数。为bridging这个实体差距，这篇论文提出了XSkill，一个仿效学习框架，它可以：1. 从无标签的人类和机器人操作视频中发现跨实体的表示，称为技能原型。2. 使用条件扩散策略将技能表示转移到机器人动作中。3. 使用人类提示视频来组合学习的技能，以完成未见任务。我们在 simulations 和实际环境中进行了实验，结果显示，XSkill 可以成功地传递和组合学习的技能，以便更通用和扩展的仿效学习框架。XSkill 的性能可以通过无名网站：https://xskillcorl.github.io 来了解。
</details></li>
</ul>
<hr>
<h2 id="Impatient-Bandits-Optimizing-Recommendations-for-the-Long-Term-Without-Delay"><a href="#Impatient-Bandits-Optimizing-Recommendations-for-the-Long-Term-Without-Delay" class="headerlink" title="Impatient Bandits: Optimizing Recommendations for the Long-Term Without Delay"></a>Impatient Bandits: Optimizing Recommendations for the Long-Term Without Delay</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09943">http://arxiv.org/abs/2307.09943</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/spotify-research/impatient-bandits">https://github.com/spotify-research/impatient-bandits</a></li>
<li>paper_authors: Thomas M. McDonald, Lucas Maystre, Mounia Lalmas, Daniel Russo, Kamil Ciosek</li>
<li>for: 本研究旨在提高在线平台上的推荐系统的长期满意度。</li>
<li>methods: 本文使用多重臂拥有延迟奖励的多臂针对问题，并开发了一种基于这种预测模型的针对算法。</li>
<li>results: 对于一个Podcast推荐问题，我们的方法比使用短期或媒体期间的奖励来优化推荐的方法显著提高了性能。<details>
<summary>Abstract</summary>
Recommender systems are a ubiquitous feature of online platforms. Increasingly, they are explicitly tasked with increasing users' long-term satisfaction. In this context, we study a content exploration task, which we formalize as a multi-armed bandit problem with delayed rewards. We observe that there is an apparent trade-off in choosing the learning signal: Waiting for the full reward to become available might take several weeks, hurting the rate at which learning happens, whereas measuring short-term proxy rewards reflects the actual long-term goal only imperfectly. We address this challenge in two steps. First, we develop a predictive model of delayed rewards that incorporates all information obtained to date. Full observations as well as partial (short or medium-term) outcomes are combined through a Bayesian filter to obtain a probabilistic belief. Second, we devise a bandit algorithm that takes advantage of this new predictive model. The algorithm quickly learns to identify content aligned with long-term success by carefully balancing exploration and exploitation. We apply our approach to a podcast recommendation problem, where we seek to identify shows that users engage with repeatedly over two months. We empirically validate that our approach results in substantially better performance compared to approaches that either optimize for short-term proxies, or wait for the long-term outcome to be fully realized.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本的简化中文版本：<</SYS>>在线平台上，推荐系统已成为普遍的功能之一。随着用户满意度的提高，推荐系统的目标也变得更加明确。在这种情况下，我们研究一种内容探索任务，我们将其形式化为带延迟奖励的多重武器问题。我们发现，选择学习信号存在显著的负担：等待全部奖励成熔化可能需要几周时间，从而降低学习速度，而且仅仅估计短期代理奖励只能做到部分准确。我们解决这个挑战在两个步骤中：1. 我们开发了一种预测延迟奖励的模型，该模型包括所有到目前为止获得的信息。全部观察结果以及短期（短期或中期）的结果都会在bayesian滤波器中结合，以获得一个概率性信念。2. 我们设计了一种武器算法，该算法利用新的预测模型来快速地识别符合长期成功的内容。该算法会优化探索和利用的平衡，以确保快速地学习到符合长期目标的内容。我们在电台推荐问题中应用了我们的方法，我们寻找用户在两个月内重复听众的电台节目。我们经验证ified that our approach leads to substantially better performance compared to approaches that either optimize for short-term proxies or wait for the long-term outcome to be fully realized.
</details></li>
</ul>
<hr>
<h2 id="TREEMENT-Interpretable-Patient-Trial-Matching-via-Personalized-Dynamic-Tree-Based-Memory-Network"><a href="#TREEMENT-Interpretable-Patient-Trial-Matching-via-Personalized-Dynamic-Tree-Based-Memory-Network" class="headerlink" title="TREEMENT: Interpretable Patient-Trial Matching via Personalized Dynamic Tree-Based Memory Network"></a>TREEMENT: Interpretable Patient-Trial Matching via Personalized Dynamic Tree-Based Memory Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09942">http://arxiv.org/abs/2307.09942</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brandon Theodorou, Cao Xiao, Jimeng Sun<br>for:The paper aims to improve the accuracy and interpretability of patient trial matching in clinical trials using machine learning models.methods:The proposed model, TREEMENT, utilizes hierarchical clinical ontologies and an attentional beam-search query to learn a personalized patient representation and offer a granular level of alignment for improved performance and interpretability.results:TREEMENT outperforms the best baseline by 7% in terms of error reduction in criteria-level matching and achieves state-of-the-art results in its trial-level matching ability. Additionally, the model provides good interpretability, making the results easier to adopt.Here is the result in Simplified Chinese text:for: 这篇论文目标是通过机器学习模型提高临床试验中患者匹配的准确性和可解释性。methods: 提议的模型TREEMENT使用层次医疗ontoлоги和搜索查询来学习个性化患者表示并提供精细的对齐，以提高性能和可解释性。results: TREEMENT比最佳基eline提高7%的error reduction在匹配水平上，并在试验水平上达到了状态的最佳结果。此外，模型还提供了良好的可解释性，使得结果更容易采用。<details>
<summary>Abstract</summary>
Clinical trials are critical for drug development but often suffer from expensive and inefficient patient recruitment. In recent years, machine learning models have been proposed for speeding up patient recruitment via automatically matching patients with clinical trials based on longitudinal patient electronic health records (EHR) data and eligibility criteria of clinical trials. However, they either depend on trial-specific expert rules that cannot expand to other trials or perform matching at a very general level with a black-box model where the lack of interpretability makes the model results difficult to be adopted.   To provide accurate and interpretable patient trial matching, we introduce a personalized dynamic tree-based memory network model named TREEMENT. It utilizes hierarchical clinical ontologies to expand the personalized patient representation learned from sequential EHR data, and then uses an attentional beam-search query learned from eligibility criteria embedding to offer a granular level of alignment for improved performance and interpretability. We evaluated TREEMENT against existing models on real-world datasets and demonstrated that TREEMENT outperforms the best baseline by 7% in terms of error reduction in criteria-level matching and achieves state-of-the-art results in its trial-level matching ability. Furthermore, we also show TREEMENT can offer good interpretability to make the model results easier for adoption.
</details>
<details>
<summary>摘要</summary>
临床试验是药品开发的关键，但经常受到昂贵和不效率的病人招募困扰。在最近几年，机器学习模型被提议用于加速病人招募，通过基于长期电子医疗记录（EHR）数据和临床试验参与条件的自动匹配病人与临床试验。然而，这些模型 Either rely on临床试验专家规则，无法扩展到其他试验，或者使用黑盒模型，其缺乏可读性使得模型结果具有困难采用。为了提供准确和可读的病人试验匹配，我们引入了一种个性化动态树型记忆网络模型 named TREEMENT。它利用层次临床 ontology 扩展个性化病人表示，然后使用静态搜索查询学习自适应搜索来提供精细水平的匹配，以提高性能和可读性。我们对实际数据进行了对比，并证明 TREEMENT 相比最佳基线模型，在 критери产生级别匹配中减少错误率7%，并达到了当前的试验级别匹配能力的国际水平。此外，我们还证明 TREEMENT 可以提供良好的可读性，使得模型结果更易于采用。
</details></li>
</ul>
<hr>
<h2 id="Spuriosity-Didn’t-Kill-the-Classifier-Using-Invariant-Predictions-to-Harness-Spurious-Features"><a href="#Spuriosity-Didn’t-Kill-the-Classifier-Using-Invariant-Predictions-to-Harness-Spurious-Features" class="headerlink" title="Spuriosity Didn’t Kill the Classifier: Using Invariant Predictions to Harness Spurious Features"></a>Spuriosity Didn’t Kill the Classifier: Using Invariant Predictions to Harness Spurious Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09933">http://arxiv.org/abs/2307.09933</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cian Eastwood, Shashank Singh, Andrei Liviu Nicolicioiu, Marin Vlastelica, Julius von Kügelgen, Bernhard Schölkopf</li>
<li>for: 避免因Domain shift而导致的失败，recent works寻找提取具有稳定或不变关系的标签特征。</li>
<li>methods: 我们的主要贡献是提出一种基于pseudo-labels的方法，可以在无标签测试领域中使用不稳定特征提高性能。</li>
<li>results: 我们经过 teorically 和实验 validate 了我们的方法，并证明它可以无需测试领域标签学习一个 asymptotically-optimal 预测器。<details>
<summary>Abstract</summary>
To avoid failures on out-of-distribution data, recent works have sought to extract features that have a stable or invariant relationship with the label across domains, discarding the "spurious" or unstable features whose relationship with the label changes across domains. However, unstable features often carry complementary information about the label that could boost performance if used correctly in the test domain. Our main contribution is to show that it is possible to learn how to use these unstable features in the test domain without labels. In particular, we prove that pseudo-labels based on stable features provide sufficient guidance for doing so, provided that stable and unstable features are conditionally independent given the label. Based on this theoretical insight, we propose Stable Feature Boosting (SFB), an algorithm for: (i) learning a predictor that separates stable and conditionally-independent unstable features; and (ii) using the stable-feature predictions to adapt the unstable-feature predictions in the test domain. Theoretically, we prove that SFB can learn an asymptotically-optimal predictor without test-domain labels. Empirically, we demonstrate the effectiveness of SFB on real and synthetic data.
</details>
<details>
<summary>摘要</summary>
为了避免对不同频谱数据的失败，现代工作强调提取具有稳定或不变的关系于标签的特征，抛弃“费劳恐怖”或不稳定的特征，这些特征的关系于标签在不同频谱上发生变化。然而，不稳定的特征经常携带标签相关的补做信息，如果正确使用，可能会提高性能。我们的主要贡献在于显示可以在测试频谱上使用这些不稳定特征，无需标签。具体来说，我们证明可以基于稳定特征生成 pseudo-标签，并使用这些 pseudo-标签 来适应不稳定特征的预测。根据这个理论认识，我们提出了稳定特征增强（SFB）算法，它包括：（i）学习分离稳定和条件独立的不稳定特征；（ii）使用稳定特征预测来适应不稳定特征的预测。理论上，我们证明 SFB 可以在无需测试频谱标签的情况下学习极限优化的预测器。实际上，我们在实际和模拟数据上证明了 SFB 的效果。
</details></li>
</ul>
<hr>
<h2 id="DISA-DIfferentiable-Similarity-Approximation-for-Universal-Multimodal-Registration"><a href="#DISA-DIfferentiable-Similarity-Approximation-for-Universal-Multimodal-Registration" class="headerlink" title="DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration"></a>DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09931">http://arxiv.org/abs/2307.09931</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/imfusiongmbh/disa-universal-multimodal-registration">https://github.com/imfusiongmbh/disa-universal-multimodal-registration</a></li>
<li>paper_authors: Matteo Ronchetti, Wolfgang Wein, Nassir Navab, Oliver Zettinig, Raphael Prevost</li>
<li>for: 用于 Multimodal image registration 的 challenging 问题，以便实现许多图像引导程序中的匹配。</li>
<li>methods: 我们提出了一种基于 Machine Learning 的框架，用于创建可表达性的跨Modal 描述符，以便快速进行可变形 global registration。我们通过将现有的 similarity metric  aproximated 为一个点积分在小型 convolutional neural network (CNN) 的特征空间中，使得可以在不需要注册数据的情况下进行训练。</li>
<li>results: 我们的方法比 local patch-based metrics 快速多个数量级，可以直接应用于临床 setting，只需要将 similarity measure 替换为我们的方法。我们的实验在三个不同的 dataset 上表明，我们的方法可以很好地泛化到新的 setting，无需特殊的重新训练。我们将我们的训练代码和数据公开发布。<details>
<summary>Abstract</summary>
Multimodal image registration is a challenging but essential step for numerous image-guided procedures. Most registration algorithms rely on the computation of complex, frequently non-differentiable similarity metrics to deal with the appearance discrepancy of anatomical structures between imaging modalities. Recent Machine Learning based approaches are limited to specific anatomy-modality combinations and do not generalize to new settings. We propose a generic framework for creating expressive cross-modal descriptors that enable fast deformable global registration. We achieve this by approximating existing metrics with a dot-product in the feature space of a small convolutional neural network (CNN) which is inherently differentiable can be trained without registered data. Our method is several orders of magnitude faster than local patch-based metrics and can be directly applied in clinical settings by replacing the similarity measure with the proposed one. Experiments on three different datasets demonstrate that our approach generalizes well beyond the training data, yielding a broad capture range even on unseen anatomies and modality pairs, without the need for specialized retraining. We make our training code and data publicly available.
</details>
<details>
<summary>摘要</summary>
多modal图像注册是一个挑战性的、但是必要的步骤，用于访问多种图像访问领域中的图像注册。大多数注册算法依赖computational complex, frequently non-differentiable similarity metrics来处理不同图像模式之间的体结构出现的差异。近期的机器学习基于的方法仅对特定的体部-图像组合有限制，并且不能扩展到新的设定。我们提出了一个通用的框架，用于创建expressive cross-modal描述子，以实现快速的扭转全局注册。我们实现了这一点通过简化现有的度量，使其变成一个内置在小型卷积神经网（CNN）中的dot产品，这个度量是可微的且可以在训练过程中被训练。我们的方法比Local patch-based度量更快速，并且可以直接在供应链中应用，只需替换度量即可。我们在三个不同的数据集上进行了实验，结果显示了我们的方法可以广泛地适用于新的体部-图像组合，而且不需要特别的重训。我们将我们的训练代码和数据公开。
</details></li>
</ul>
<hr>
<h2 id="TimeTuner-Diagnosing-Time-Representations-for-Time-Series-Forecasting-with-Counterfactual-Explanations"><a href="#TimeTuner-Diagnosing-Time-Representations-for-Time-Series-Forecasting-with-Counterfactual-Explanations" class="headerlink" title="TimeTuner: Diagnosing Time Representations for Time-Series Forecasting with Counterfactual Explanations"></a>TimeTuner: Diagnosing Time Representations for Time-Series Forecasting with Counterfactual Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09916">http://arxiv.org/abs/2307.09916</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/catherinehao/timetuner">https://github.com/catherinehao/timetuner</a></li>
<li>paper_authors: Jianing Hao, Qing Shi, Yilin Ye, Wei Zeng</li>
<li>for: 本文旨在提供一种可视化分析框架，帮助分析者理解时序序列表示如何与模型预测相关。</li>
<li>methods: 本文使用两个阶段技术：首先，利用counterfactual解释连接时序序列表示、多变量特征和模型预测之间的关系。其次，设计多种协调视图，包括分割基于相关性的分布图和嵌入的双向条带，并提供了用户可以进行变换选择过程、浏览特征空间和评估模型性能的交互。</li>
<li>results: 基于实际时序序预测数据， authors demonstrates the applicability of TimeTuner 框架在单variate 太阳黑点和多variate 空气污染的预测中。feedback from domain experts表明，TimeTuner 可以帮助描述时序序列表示和导向特征工程过程。<details>
<summary>Abstract</summary>
Deep learning (DL) approaches are being increasingly used for time-series forecasting, with many efforts devoted to designing complex DL models. Recent studies have shown that the DL success is often attributed to effective data representations, fostering the fields of feature engineering and representation learning. However, automated approaches for feature learning are typically limited with respect to incorporating prior knowledge, identifying interactions among variables, and choosing evaluation metrics to ensure that the models are reliable. To improve on these limitations, this paper contributes a novel visual analytics framework, namely TimeTuner, designed to help analysts understand how model behaviors are associated with localized correlations, stationarity, and granularity of time-series representations. The system mainly consists of the following two-stage technique: We first leverage counterfactual explanations to connect the relationships among time-series representations, multivariate features and model predictions. Next, we design multiple coordinated views including a partition-based correlation matrix and juxtaposed bivariate stripes, and provide a set of interactions that allow users to step into the transformation selection process, navigate through the feature space, and reason the model performance. We instantiate TimeTuner with two transformation methods of smoothing and sampling, and demonstrate its applicability on real-world time-series forecasting of univariate sunspots and multivariate air pollutants. Feedback from domain experts indicates that our system can help characterize time-series representations and guide the feature engineering processes.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）方法在时间序列预测中越来越广泛应用，许多努力投入于设计复杂的DL模型。 latest studies have shown that DL success often attributed to effective data representations, which has led to the development of fields of feature engineering and representation learning. However, automated approaches for feature learning are typically limited in terms of incorporating prior knowledge, identifying interactions among variables, and choosing evaluation metrics to ensure the models are reliable. To improve on these limitations, this paper proposes a novel visual analytics framework, namely TimeTuner, designed to help analysts understand how model behaviors are associated with localized correlations, stationarity, and granularity of time-series representations. The system consists of the following two-stage technique:First, we leverage counterfactual explanations to connect the relationships among time-series representations, multivariate features, and model predictions. Next, we design multiple coordinated views, including a partition-based correlation matrix and juxtaposed bivariate stripes, and provide a set of interactions that allow users to step into the transformation selection process, navigate through the feature space, and reason the model performance.We instantiate TimeTuner with two transformation methods of smoothing and sampling, and demonstrate its applicability on real-world time-series forecasting of univariate sunspots and multivariate air pollutants. Feedback from domain experts indicates that our system can help characterize time-series representations and guide the feature engineering processes.
</details></li>
</ul>
<hr>
<h2 id="Deep-projection-networks-for-learning-time-homogeneous-dynamical-systems"><a href="#Deep-projection-networks-for-learning-time-homogeneous-dynamical-systems" class="headerlink" title="Deep projection networks for learning time-homogeneous dynamical systems"></a>Deep projection networks for learning time-homogeneous dynamical systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09912">http://arxiv.org/abs/2307.09912</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vladimir R. Kostic, Pietro Novelli, Riccardo Grazzi, Karim Lounici, Massimiliano Pontil</li>
<li>for: 学习时间Homogeneous动力系统的有用表示。</li>
<li>methods: 使用一种基于神经网络的方法，通过优化一个对象函数来学习一个转移运算符。</li>
<li>results: 提出了一种稳定且可靠的方法，可以应用于具有挑战性的场景。并且可以提高前期方法的性能。<details>
<summary>Abstract</summary>
We consider the general class of time-homogeneous dynamical systems, both discrete and continuous, and study the problem of learning a meaningful representation of the state from observed data. This is instrumental for the task of learning a forward transfer operator of the system, that in turn can be used for forecasting future states or observables. The representation, typically parametrized via a neural network, is associated with a projection operator and is learned by optimizing an objective function akin to that of canonical correlation analysis (CCA). However, unlike CCA, our objective avoids matrix inversions and therefore is generally more stable and applicable to challenging scenarios. Our objective is a tight relaxation of CCA and we further enhance it by proposing two regularization schemes, one encouraging the orthogonality of the components of the representation while the other exploiting Chapman-Kolmogorov's equation. We apply our method to challenging discrete dynamical systems, discussing improvements over previous methods, as well as to continuous dynamical systems.
</details>
<details>
<summary>摘要</summary>
我团队考虑了时间homogeneous的动力系统，包括离散和连续两种类型，并研究了从观测数据中学习一个有意义的状态表示。这个表示通常通过神经网络进行参数化，与投影运算器相关，并通过优化一个类似于 canonical correlation analysis（CCA）的目标函数来学习。不同于CCA，我们的目标函数不需要矩阵反转，因此更加稳定和适用于复杂的场景。我们的目标函数是CCA的紧张relaxation，并且我们提出了两种正则化方案，一个促进表示Components的正交性，另一个利用 Chapman-Kolmogorov 方程。我们应用我们的方法于复杂的离散动力系统和连续动力系统，讨论了以前方法的改进和性能提升。
</details></li>
</ul>
<hr>
<h2 id="Repeated-Observations-for-Classification"><a href="#Repeated-Observations-for-Classification" class="headerlink" title="Repeated Observations for Classification"></a>Repeated Observations for Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09896">http://arxiv.org/abs/2307.09896</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hüseyin Afşer, László Györfi, Harro Walk</li>
<li>for: 本研究讲卷非 Parametric 分类问题，具体来说是在重复观测 $\bV_1,\dots ,\bV_t $ 中进行分类。</li>
<li>methods: 本文提出了一些简单的分类规则，其中 conditional error probabilities 的渐进速度为 exponential。</li>
<li>results: 本文分析了一些特定的模型，如 robust detection by nominal densities、prototype classification、linear transformation、linear classification、scaling。<details>
<summary>Abstract</summary>
We study the problem nonparametric classification with repeated observations. Let $\bX$ be the $d$ dimensional feature vector and let $Y$ denote the label taking values in $\{1,\dots ,M\}$. In contrast to usual setup with large sample size $n$ and relatively low dimension $d$, this paper deals with the situation, when instead of observing a single feature vector $\bX$ we are given $t$ repeated feature vectors $\bV_1,\dots ,\bV_t $. Some simple classification rules are presented such that the conditional error probabilities have exponential convergence rate of convergence as $t\to\infty$. In the analysis, we investigate particular models like robust detection by nominal densities, prototype classification, linear transformation, linear classification, scaling.
</details>
<details>
<summary>摘要</summary>
我们研究非参数化分类问题，带有重复观测。假设 $\bX$ 是 $d$ 维特征向量，$Y$ 表示标签，取值在 $\{1,\ldots,M\}$ 中。相比于通常情况，我们在大样本大小 $n$ 和低维度 $d$ 的情况下进行研究，这篇论文则考虑了在多个特征向量 $\bV_1,\ldots,\bV_t$ 的情况下，并提出了一些简单的分类规则，其 conditional error probability 的渐进速度为 $t\to\infty$ 的几何速度。在分析中，我们研究了特定的模型，如 robust detection by nominal densities、prototype classification、linear transformation、linear classification、scaling。
</details></li>
</ul>
<hr>
<h2 id="Symmetric-Equilibrium-Learning-of-VAEs"><a href="#Symmetric-Equilibrium-Learning-of-VAEs" class="headerlink" title="Symmetric Equilibrium Learning of VAEs"></a>Symmetric Equilibrium Learning of VAEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09883">http://arxiv.org/abs/2307.09883</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boris Flach, Dmitrij Schlesinger, Alexander Shekhovtsov</li>
<li>for: 该研究旨在为缺乏准确模型的情况下，提高变量自动编码器（VAE）的学习方法。</li>
<li>methods: 该方法使用了尼什均衡学习方法，将数据空间和隐藏空间之间的分布映射到另一个空间中，并使用随机抽取来获取数据和隐藏变量的访问。</li>
<li>results: 实验表明，该方法可以learned comparable models with ELBO learning,并且可以应用于一些不可能通过标准VAE学习的任务。<details>
<summary>Abstract</summary>
We view variational autoencoders (VAE) as decoder-encoder pairs, which map distributions in the data space to distributions in the latent space and vice versa. The standard learning approach for VAEs, i.e. maximisation of the evidence lower bound (ELBO), has an obvious asymmetry in that respect. Moreover, it requires a closed form a-priori latent distribution. This limits the applicability of VAEs in more complex scenarios, such as general semi-supervised learning and employing complex generative models as priors. We propose a Nash equilibrium learning approach that relaxes these restrictions and allows learning VAEs in situations where both the data and the latent distributions are accessible only by sampling. The flexibility and simplicity of this approach allows its application to a wide range of learning scenarios and downstream tasks. We show experimentally that the models learned by this method are comparable to those obtained by ELBO learning and demonstrate its applicability for tasks that are not accessible by standard VAE learning.
</details>
<details>
<summary>摘要</summary>
我们视variational autoencoders（VAE）为编码器-解码器对，它们将数据空间中的分布映射到 latent space 中的分布，并将其反转。标准的 VAE 学习方法，即最大化证据下界（ELBO）的最大化，具有明显的偏好性，即需要预先知道 latent distribution 的closed form。这限制了 VAE 在更复杂的场景下的应用，如总是 semi-supervised learning 和使用复杂的生成模型为先验。我们提议一种 Nash 平衡学习方法，它可以放弃这些限制，允许在数据和 latent 分布可以通过采样获取时学习 VAE。这种灵活性和简洁性使其适用于各种学习场景和下游任务。我们通过实验表明，与 ELBO 学习相比，这种方法学习的模型相似，并且在不可用标准 VAE 学习的任务上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Likelihood-Estimation-with-One-way-Flows"><a href="#Adversarial-Likelihood-Estimation-with-One-way-Flows" class="headerlink" title="Adversarial Likelihood Estimation with One-way Flows"></a>Adversarial Likelihood Estimation with One-way Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09882">http://arxiv.org/abs/2307.09882</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omri Ben-Dov, Pravir Singh Gupta, Victoria Abrevaya, Michael J. Black, Partha Ghosh</li>
<li>for: 这个论文的目的是提出一种新的生成模型，以提高生成模型的质量和可靠性。</li>
<li>methods: 这个论文使用了生成对抗网络（GANs），并提出了一种新的方法，即使用能量基分布来最大化生成器的概率密度。这种方法可以提供更好的模式覆盖率和生成器的可靠性。</li>
<li>results: 实验结果表明，这种新的生成模型可以比传统的GANs快速 converges，并且可以生成高质量的样本，同时也可以避免过拟合常用的数据集。此外，这种模型还可以生成了维度低的积分表示。<details>
<summary>Abstract</summary>
Generative Adversarial Networks (GANs) can produce high-quality samples, but do not provide an estimate of the probability density around the samples. However, it has been noted that maximizing the log-likelihood within an energy-based setting can lead to an adversarial framework where the discriminator provides unnormalized density (often called energy). We further develop this perspective, incorporate importance sampling, and show that 1) Wasserstein GAN performs a biased estimate of the partition function, and we propose instead to use an unbiased estimator; 2) when optimizing for likelihood, one must maximize generator entropy. This is hypothesized to provide a better mode coverage. Different from previous works, we explicitly compute the density of the generated samples. This is the key enabler to designing an unbiased estimator of the partition function and computation of the generator entropy term. The generator density is obtained via a new type of flow network, called one-way flow network, that is less constrained in terms of architecture, as it does not require to have a tractable inverse function. Our experimental results show that we converge faster, produce comparable sample quality to GANs with similar architecture, successfully avoid over-fitting to commonly used datasets and produce smooth low-dimensional latent representations of the training data.
</details>
<details>
<summary>摘要</summary>
生成对抗网络（GAN）可以生成高质量样本，但不提供样本周围的概率密度估计。然而，有观点认为，在能量基础设定下，最大化律引函数可以导致对抗框架，其中对抗器提供了不归一化密度（常称为能量）。我们进一步发展这一观点，包括重要抽样，并表明：1）沃氏距离GAN实际上计算了偏置函数，我们提议使用不偏置的估计器；2）在优化概率时，需要最大化生成器的熵。这被假设为提供更好的模式覆盖率。与前一些工作不同，我们显式计算生成样本的密度。这是关键启用设计不偏置估计器和计算生成器熵项的能力。生成器密度通过一种新的流网络，称为一向流网络，可以更加自由地设计架构，不需要具有可导函数。我们的实验结果表明，我们可以更快 converges，生成高质量样本，并成功避免过拟合常用的数据集，生成平滑低维度的训练数据表示。
</details></li>
</ul>
<hr>
<h2 id="Detecting-Vulnerable-Nodes-in-Urban-Infrastructure-Interdependent-Network"><a href="#Detecting-Vulnerable-Nodes-in-Urban-Infrastructure-Interdependent-Network" class="headerlink" title="Detecting Vulnerable Nodes in Urban Infrastructure Interdependent Network"></a>Detecting Vulnerable Nodes in Urban Infrastructure Interdependent Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09866">http://arxiv.org/abs/2307.09866</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tsinghua-fib-lab/kdd2023-id546-urbaninfra">https://github.com/tsinghua-fib-lab/kdd2023-id546-urbaninfra</a></li>
<li>paper_authors: Jinzhu Mao, Liu Cao, Chen Gao, Huandong Wang, Hangyu Fan, Depeng Jin, Yong Li</li>
<li>for: 这个论文的目的是理解和 caracterizing 城市基础设施的投入性弱点，以便保护敏感设施和设计强健的 topological structure。</li>
<li>methods: 该论文使用了图神经网络和强化学习来模型城市系统中的互相关联关系，并通过实际数据训练来准确地评估城市系统的投入性弱点。</li>
<li>results: 该论文的实验结果表明，该系统可以准确地捕捉城市系统中的风险层次和投入性弱点，并且可以在不同的请求下进行跨越训练和转移学习。<details>
<summary>Abstract</summary>
Understanding and characterizing the vulnerability of urban infrastructures, which refers to the engineering facilities essential for the regular running of cities and that exist naturally in the form of networks, is of great value to us. Potential applications include protecting fragile facilities and designing robust topologies, etc. Due to the strong correlation between different topological characteristics and infrastructure vulnerability and their complicated evolution mechanisms, some heuristic and machine-assisted analysis fall short in addressing such a scenario. In this paper, we model the interdependent network as a heterogeneous graph and propose a system based on graph neural network with reinforcement learning, which can be trained on real-world data, to characterize the vulnerability of the city system accurately. The presented system leverages deep learning techniques to understand and analyze the heterogeneous graph, which enables us to capture the risk of cascade failure and discover vulnerable infrastructures of cities. Extensive experiments with various requests demonstrate not only the expressive power of our system but also transferring ability and necessity of the specific components.
</details>
<details>
<summary>摘要</summary>
理解和Characterizing城市基础设施的敏感性具有很大的价值。这种基础设施包括城市的工程设施，这些设施存在自然形成的网络结构，并且具有各种不同的特性。具体来说，我们可以通过保护脆弱的设施和设计Robust的topology等方式来应用这种理解和Characterizing技术。由于城市基础设施的特征相互 correlated，以及其复杂的演化机制，一些顺序和机器学习的分析方法无法正确地处理这种情况。为此，我们在这篇论文中提出了一种基于图 neural network with reinforcement learning的方法，可以在实际数据上训练，以准确地Characterizing城市系统的敏感性。这种方法利用深度学习技术来理解和分析异质图，使得我们能够捕捉城市系统的风险和潜在的脆弱性。我们在实际中进行了多种请求的实验，并证明了我们的系统不仅具有表达力，还能够转移和应用特定的组件。
</details></li>
</ul>
<hr>
<h2 id="Towards-a-population-informed-approach-to-the-definition-of-data-driven-models-for-structural-dynamics"><a href="#Towards-a-population-informed-approach-to-the-definition-of-data-driven-models-for-structural-dynamics" class="headerlink" title="Towards a population-informed approach to the definition of data-driven models for structural dynamics"></a>Towards a population-informed approach to the definition of data-driven models for structural dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09862">http://arxiv.org/abs/2307.09862</a></li>
<li>repo_url: None</li>
<li>paper_authors: G. Tsialiamanis, N. Dervilis, D. J. Wagg, K. Worden</li>
<li>for: 这个研究的目的是激励使用基于物理学的模型，以学习相似物理下的现象关系。</li>
<li>methods: 这个研究使用了两种meta-学习算法：模型独立meta-学习（MAML）算法和条件神经过程（CNP）模型。</li>
<li>results: 这两种算法都能够如意地approxime quantities of interest，并且表现与传统机器学习算法类似，即与可用的结构数量有关。<details>
<summary>Abstract</summary>
Machine learning has affected the way in which many phenomena for various domains are modelled, one of these domains being that of structural dynamics. However, because machine-learning algorithms are problem-specific, they often fail to perform efficiently in cases of data scarcity. To deal with such issues, combination of physics-based approaches and machine learning algorithms have been developed. Although such methods are effective, they also require the analyser's understanding of the underlying physics of the problem. The current work is aimed at motivating the use of models which learn such relationships from a population of phenomena, whose underlying physics are similar. The development of such models is motivated by the way that physics-based models, and more specifically finite element models, work. Such models are considered transferrable, explainable and trustworthy, attributes which are not trivially imposed or achieved for machine-learning models. For this reason, machine-learning approaches are less trusted by industry and often considered more difficult to form validated models. To achieve such data-driven models, a population-based scheme is followed here and two different machine-learning algorithms from the meta-learning domain are used. The two algorithms are the model-agnostic meta-learning (MAML) algorithm and the conditional neural processes (CNP) model. The algorithms seem to perform as intended and outperform a traditional machine-learning algorithm at approximating the quantities of interest. Moreover, they exhibit behaviour similar to traditional machine learning algorithms (e.g. neural networks or Gaussian processes), concerning their performance as a function of the available structures in the training population.
</details>
<details>
<summary>摘要</summary>
机器学习对许多领域的现象模型ing有所影响，其中一个领域是结构动力学。然而，由于机器学习算法是问题特定的，因此在数据稀缺时可能不具有高效性。为解决这些问题，物理学习和机器学习算法的组合被开发出来。虽然这些方法有效，但它们也需要分析者对问题的physics的理解。目前的工作是鼓励使用基于人口的现象模型，其中的physics是相似的。这种模型被考虑为可传递、可解释和可信任的，这些特性不是很容易实现或掌握的。因此，机器学习方法在业务中被视为更难以验证和更不可靠。为实现这些数据驱动模型，在这里采用了人口学习的方式，并使用了来自多元学习领域的两种机器学习算法：模型独立多学习（MAML）算法和条件神经过程（CNP）模型。这两种算法似乎按计划进行，并在可用的结构数量上表现出类似的行为，与传统机器学习算法（如神经网络或高斯过程）的性能相似。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Learning-for-Credit-Index-Option-Hedging"><a href="#Reinforcement-Learning-for-Credit-Index-Option-Hedging" class="headerlink" title="Reinforcement Learning for Credit Index Option Hedging"></a>Reinforcement Learning for Credit Index Option Hedging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09844">http://arxiv.org/abs/2307.09844</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Mandelli, Marco Pinciroli, Michele Trapletti, Edoardo Vittori</li>
<li>for: 本研究旨在通过强化学习方法找到债务指数选项最佳保险策略。</li>
<li>methods: 我们采用实用的方法，强调实际性，即逐步时间、交易成本等因素。我们应用了现代算法TRVO算法，并证明 derivated 保险策略超越了实践中的黑ilder&amp;奔矢 delta 保险策略。</li>
<li>results: 我们的研究表明，使用TRVO算法可以找到更好的保险策略，并且在实际市场数据上测试了我们的政策。<details>
<summary>Abstract</summary>
In this paper, we focus on finding the optimal hedging strategy of a credit index option using reinforcement learning. We take a practical approach, where the focus is on realism i.e. discrete time, transaction costs; even testing our policy on real market data. We apply a state of the art algorithm, the Trust Region Volatility Optimization (TRVO) algorithm and show that the derived hedging strategy outperforms the practitioner's Black & Scholes delta hedge.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们关注适用再强化学习来找到债务指数选项最佳减抵策略。我们采取了一种实用的方法，即注重实际情况，包括精确的时间预测、交易成本等，并对实际市场数据进行测试。我们使用了现代算法，即信任区域波动优化（TRVO）算法，并证明 derivated的减抵策略超过了实践中的布拉克-舒勒斯 delta 减抵策。
</details></li>
</ul>
<hr>
<h2 id="Near-Linear-Time-Projection-onto-the-ell-1-infty-Ball-Application-to-Sparse-Autoencoders"><a href="#Near-Linear-Time-Projection-onto-the-ell-1-infty-Ball-Application-to-Sparse-Autoencoders" class="headerlink" title="Near-Linear Time Projection onto the $\ell_{1,\infty}$ Ball; Application to Sparse Autoencoders"></a>Near-Linear Time Projection onto the $\ell_{1,\infty}$ Ball; Application to Sparse Autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09836">http://arxiv.org/abs/2307.09836</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/memo-p/projection">https://github.com/memo-p/projection</a></li>
<li>paper_authors: Guillaume Perez, Laurent Condat, Michel Barlaud</li>
<li>for: 快速训练大规模神经网络的简化。</li>
<li>methods: 使用 $\ell_{1,2}$ 和 $\ell_{1,\infty}$ 投影技术来简化和减少神经网络的总成本。</li>
<li>results: 提出了一种新的 $\ell_{1,\infty}$  нор球投影算法，其最坏时间复杂度为 $\mathcal{O}\big(nm+J\log(nm)\big)$，其中 $J$ 是一个减少到 0 的隐式常数，当精度高时，它减少到 $nm$。此外，提议在自动编码器训练中加入 $\ell_{1,\infty}$ 球投影以实现特征选择和神经网络精炼。在生物应用中，只有非常小的一部分数据 ($&lt;2%$) 是有关的，因此在编码器中进行简化。我们示出了我们的方法在生物 caso 和一般精炼 caso 中都是最快的。<details>
<summary>Abstract</summary>
Looking for sparsity is nowadays crucial to speed up the training of large-scale neural networks. Projections onto the $\ell_{1,2}$ and $\ell_{1,\infty}$ are among the most efficient techniques to sparsify and reduce the overall cost of neural networks. In this paper, we introduce a new projection algorithm for the $\ell_{1,\infty}$ norm ball. The worst-case time complexity of this algorithm is $\mathcal{O}\big(nm+J\log(nm)\big)$ for a matrix in $\mathbb{R}^{n\times m}$. $J$ is a term that tends to 0 when the sparsity is high, and to $nm$ when the sparsity is low. Its implementation is easy and it is guaranteed to converge to the exact solution in a finite time. Moreover, we propose to incorporate the $\ell_{1,\infty}$ ball projection while training an autoencoder to enforce feature selection and sparsity of the weights. Sparsification appears in the encoder to primarily do feature selection due to our application in biology, where only a very small part ($<2\%$) of the data is relevant. We show that both in the biological case and in the general case of sparsity that our method is the fastest.
</details>
<details>
<summary>摘要</summary>
现在，检测稀畴性已经成为训练大规模神经网络的关键。Project onto the $\ell_{1,2}$ and $\ell_{1,\infty}$ 是最高效的技术来稀畴化和减少神经网络的总成本。在这篇论文中，我们介绍了一种新的 projection algorithm for the $\ell_{1,\infty}$  нор球。这个算法的最坏情况时间复杂度是 $\mathcal{O}\big(nm+J\log(nm)\big)$，其中 $J$ 是一个很小的常数，当稀畴性高时，它会逐渐递减至 $nm$，而当稀畴性低时，它会保持不变。它的实现非常简单，并且可以在有限时间内 converges to the exact solution。此外，我们还提议在训练 autoencoder 时添加 $\ell_{1,\infty}$ 球 projection，以便在权重中实现特征选择和稀畴化。在生物应用中，只有非常小的一部分（ Less than 2%）的数据是有关的，因此在encoder中进行稀畴化。我们表明，在生物和通用稀畴情况下，我们的方法是最快的。
</details></li>
</ul>
<hr>
<h2 id="Deep-Operator-Network-Approximation-Rates-for-Lipschitz-Operators"><a href="#Deep-Operator-Network-Approximation-Rates-for-Lipschitz-Operators" class="headerlink" title="Deep Operator Network Approximation Rates for Lipschitz Operators"></a>Deep Operator Network Approximation Rates for Lipschitz Operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09835">http://arxiv.org/abs/2307.09835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christoph Schwab, Andreas Stein, Jakob Zech</li>
<li>for: 这个论文是用于研究深度运算网络（Deep Operator Networks，简称DON）如何用于模拟 lipschitz（或 holder）连续函数 $\mathcal G:\mathcal X\to\mathcal Y$ 之间的交互。</li>
<li>methods: 这个论文使用了 linear encoders $\mathcal E$ 和 decoders $\mathcal D$，以及一个参数化坐标映射网络来模拟 $\mathcal G$。</li>
<li>results: 这个论文得到了 $\mathcal G$ 的表达率下界，不需要 $\mathcal G$ 是幂极连续的假设。<details>
<summary>Abstract</summary>
We establish universality and expression rate bounds for a class of neural Deep Operator Networks (DON) emulating Lipschitz (or H\"older) continuous maps $\mathcal G:\mathcal X\to\mathcal Y$ between (subsets of) separable Hilbert spaces $\mathcal X$, $\mathcal Y$. The DON architecture considered uses linear encoders $\mathcal E$ and decoders $\mathcal D$ via (biorthogonal) Riesz bases of $\mathcal X$, $\mathcal Y$, and an approximator network of an infinite-dimensional, parametric coordinate map that is Lipschitz continuous on the sequence space $\ell^2(\mathbb N)$. Unlike previous works ([Herrmann, Schwab and Zech: Neural and Spectral operator surrogates: construction and expression rate bounds, SAM Report, 2022], [Marcati and Schwab: Exponential Convergence of Deep Operator Networks for Elliptic Partial Differential Equations, SAM Report, 2022]), which required for example $\mathcal G$ to be holomorphic, the present expression rate results require mere Lipschitz (or H\"older) continuity of $\mathcal G$. Key in the proof of the present expression rate bounds is the use of either super-expressive activations (e.g. [Yarotski: Elementary superexpressive activations, Int. Conf. on ML, 2021], [Shen, Yang and Zhang: Neural network approximation: Three hidden layers are enough, Neural Networks, 2021], and the references there) which are inspired by the Kolmogorov superposition theorem, or of nonstandard NN architectures with standard (ReLU) activations as recently proposed in [Zhang, Shen and Yang: Neural Network Architecture Beyond Width and Depth, Adv. in Neural Inf. Proc. Sys., 2022]. We illustrate the abstract results by approximation rate bounds for emulation of a) solution operators for parametric elliptic variational inequalities, and b) Lipschitz maps of Hilbert-Schmidt operators.
</details>
<details>
<summary>摘要</summary>
我们设定了一个简单的 Deep Operator Network (DON) 架构，用于模拟 Lipschitz （或Holder）连续的映射 $\mathcal G:\mathcal X\to\mathcal Y$  zwischen（subsets of）separable Hilbert spaces $\mathcal X$, $\mathcal Y$。DON 架构使用线性映射 $\mathcal E$ 和 $\mathcal D$ via（biorthogonal）Riesz bases of $\mathcal X$, $\mathcal Y$，并且使用一个具有无穷维度的参数coordinate map的近似器网络。不同于先前的研究（[Herrmann, Schwab and Zech: Neural and Spectral operator surrogates: construction and expression rate bounds, SAM Report, 2022]，[Marcati and Schwab: Exponential Convergence of Deep Operator Networks for Elliptic Partial Differential Equations, SAM Report, 2022]），我们的表达率结果不需要 $\mathcal G$ 是全纯函数。我们的关键证明是使用超对映射（例如 [Yarotski: Elementary superexpressive activations, Int. Conf. on ML, 2021]，[Shen, Yang and Zhang: Neural network approximation: Three hidden layers are enough, Neural Networks, 2021]，和其他文献），或者非标准的NN架构（使用 ReLU 活化），如 recent proposed in [Zhang, Shen and Yang: Neural Network Architecture Beyond Width and Depth, Adv. in Neural Inf. Proc. Sys., 2022]。我们透过实际的例子来详细说明我们的抽象结果，包括对参数elliptic variational inequality的解析器架构和对Hilbert-Schmidt operator的Lipschitz映射。
</details></li>
</ul>
<hr>
<h2 id="What-do-neural-networks-learn-in-image-classification-A-frequency-shortcut-perspective"><a href="#What-do-neural-networks-learn-in-image-classification-A-frequency-shortcut-perspective" class="headerlink" title="What do neural networks learn in image classification? A frequency shortcut perspective"></a>What do neural networks learn in image classification? A frequency shortcut perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09829">http://arxiv.org/abs/2307.09829</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nis-research/nn-frequency-shortcuts">https://github.com/nis-research/nn-frequency-shortcuts</a></li>
<li>paper_authors: Shunxin Wang, Raymond Veldhuis, Christoph Brune, Nicola Strisciuglio</li>
<li>for:  investigate the mechanisms of representation learning in neural networks (NNs) for classification tasks</li>
<li>methods:  experiments on synthetic datasets and natural images, proposal of a metric to measure class-wise frequency characteristics, and a method to identify frequency shortcuts</li>
<li>results:  NNs tend to find simple solutions for classification, and what they learn first during training depends on the most distinctive frequency characteristics; frequency shortcuts can be transferred across datasets and cannot be fully avoided by larger model capacity and data augmentation.<details>
<summary>Abstract</summary>
Frequency analysis is useful for understanding the mechanisms of representation learning in neural networks (NNs). Most research in this area focuses on the learning dynamics of NNs for regression tasks, while little for classification. This study empirically investigates the latter and expands the understanding of frequency shortcuts. First, we perform experiments on synthetic datasets, designed to have a bias in different frequency bands. Our results demonstrate that NNs tend to find simple solutions for classification, and what they learn first during training depends on the most distinctive frequency characteristics, which can be either low- or high-frequencies. Second, we confirm this phenomenon on natural images. We propose a metric to measure class-wise frequency characteristics and a method to identify frequency shortcuts. The results show that frequency shortcuts can be texture-based or shape-based, depending on what best simplifies the objective. Third, we validate the transferability of frequency shortcuts on out-of-distribution (OOD) test sets. Our results suggest that frequency shortcuts can be transferred across datasets and cannot be fully avoided by larger model capacity and data augmentation. We recommend that future research should focus on effective training schemes mitigating frequency shortcut learning.
</details>
<details>
<summary>摘要</summary>
频率分析对神经网络（NN）的表征学arning有用。大多数研究集中在NN的回归任务上进行了学习动力学研究，而对于分类任务的研究则比较少。这个研究通过实验探索类别分类任务中NN的学习机制，并扩展了频率短cut的理解。我们首先在人工生成的Synthetic dataset上进行了实验，这些dataset具有不同频率带的偏见。我们的结果表明，NN在分类任务上倾向于找到简单的解决方案，并且在训练过程中学习的第一个频率特征取决于数据的最明显频率特征，这些特征可以是低频或高频。然后，我们 validate这种现象在自然图像上，并提出了一种类别频率特征的度量和一种频率短cut的标识方法。结果表明，频率短cut可以是Texture-based或Shape-based，取决于最简化目标所需的频率特征。最后，我们证明了频率短cut的转移性，即频率短cut可以在不同的数据集上转移，并且不能完全由更大的模型容量和数据增强来避免。我们建议未来的研究应该关注有效地减少频率短cut的学习。
</details></li>
</ul>
<hr>
<h2 id="Multi-modal-Learning-based-Prediction-for-Disease"><a href="#Multi-modal-Learning-based-Prediction-for-Disease" class="headerlink" title="Multi-modal Learning based Prediction for Disease"></a>Multi-modal Learning based Prediction for Disease</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09823">http://arxiv.org/abs/2307.09823</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/batuhankmkaraman/mlbasedad">https://github.com/batuhankmkaraman/mlbasedad</a></li>
<li>paper_authors: Yaran Chen, Xueyu Chen, Yu Han, Haoran Li, Dongbin Zhao, Jingzhong Li, Xu Wang</li>
<li>for: 预测非アルコール性脂肪性肝病（NAFLD）的精准方法，以避免进一步的肝硬化和cirrhosis。</li>
<li>methods: combines comprehensive clinical dataset（FLDData）和多Modal学习基于NAFLD预测方法（DeepFLD），并使用多Modal输入，包括metadata和facial images，以实现更加精准的诊断。</li>
<li>results: DeepFLD模型在不同的 dataset 上表现出了良好的性能，并且可以使用only facial images as input，这表明了这种方法的可行性和简洁性。<details>
<summary>Abstract</summary>
Non alcoholic fatty liver disease (NAFLD) is the most common cause of chronic liver disease, which can be predicted accurately to prevent advanced fibrosis and cirrhosis. While, a liver biopsy, the gold standard for NAFLD diagnosis, is invasive, expensive, and prone to sampling errors. Therefore, non-invasive studies are extremely promising, yet they are still in their infancy due to the lack of comprehensive research data and intelligent methods for multi-modal data. This paper proposes a NAFLD diagnosis system (DeepFLDDiag) combining a comprehensive clinical dataset (FLDData) and a multi-modal learning based NAFLD prediction method (DeepFLD). The dataset includes over 6000 participants physical examinations, laboratory and imaging studies, extensive questionnaires, and facial images of partial participants, which is comprehensive and valuable for clinical studies. From the dataset, we quantitatively analyze and select clinical metadata that most contribute to NAFLD prediction. Furthermore, the proposed DeepFLD, a deep neural network model designed to predict NAFLD using multi-modal input, including metadata and facial images, outperforms the approach that only uses metadata. Satisfactory performance is also verified on other unseen datasets. Inspiringly, DeepFLD can achieve competitive results using only facial images as input rather than metadata, paving the way for a more robust and simpler non-invasive NAFLD diagnosis.
</details>
<details>
<summary>摘要</summary>
非アルコール性脂肪肝病（NAFLD）は、 Chronic liver disease の最も一般的な原因であり、进行度の评価や cirrhosis の予防を正确に予测することができます。 しかし、liver biopsy は、攻撃的で、高価で、标本の误差による不确実性があります。 そこで、非侵入的な研究は、非常に有望ですが、まだ、复数のモードのデータに対するインテリジェントな方法の欠如により、まだ infant の状态であります。 この论文では、NAFLD の诊断システム (DeepFLDDiag) を提案しています。 このシステムは、 comprehensive clinical dataset (FLDData) と、多modal 学习に基づいた NAFLD 予测法 (DeepFLD) を组み合わせています。 データセットには、6000人以上の参加者の物理调查、医疗所の検查结果、広范な质问naires、および一部の参加者の颜画像が含まれています。 このデータセットから、NAFLD 予测において最も贡献度の高い临床 metadata を量的に分析し、选択します。 さらに、提案された DeepFLD は、多modal 入力、 metadata と颜画像を使用して NAFLD を予测するための深层学习ネットワーク モデルです。 この方法は、 metadata のみを使用したアプローチよりも、优れた性能を示します。 さらに、 DeepFLD は、他の未観なデータセットでも、优れた性能を示しています。 惊くべきは、 DeepFLD は、颜画像のみを入力として使用することで、NAFLD の诊断を可能にしています。 これにより、非侵入的な NAFLD 诊断がより Robust かつ简単になります。
</details></li>
</ul>
<hr>
<h2 id="Deep-unrolling-Shrinkage-Network-for-Dynamic-MR-imaging"><a href="#Deep-unrolling-Shrinkage-Network-for-Dynamic-MR-imaging" class="headerlink" title="Deep unrolling Shrinkage Network for Dynamic MR imaging"></a>Deep unrolling Shrinkage Network for Dynamic MR imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09818">http://arxiv.org/abs/2307.09818</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yhao-z/dus-net">https://github.com/yhao-z/dus-net</a></li>
<li>paper_authors: Yinghao Zhang, Xiaodi Li, Weihang Li, Yue Hu</li>
<li>for: 这篇论文的目的是提出一个新的深度推广网络（DUS-Net），用于优化逐条件 $l_1$ 正规化验证类MR复原模型。</li>
<li>methods: 这篇论文使用了一个新的潜在频率注意力（AST）来学习每个通道的阈值，并将其应用到这些通道上。具体来说，这篇论文提出了一个叫做深度推广推广网络（DUS-Net），用于优化逐条件 $l_1$ 正规化验证类MR复原模型。</li>
<li>results: 实验结果显示，提案的 DUS-Net 可以超过现有的方法的性能。<details>
<summary>Abstract</summary>
Deep unrolling networks that utilize sparsity priors have achieved great success in dynamic magnetic resonance (MR) imaging. The convolutional neural network (CNN) is usually utilized to extract the transformed domain, and then the soft thresholding (ST) operator is applied to the CNN-transformed data to enforce the sparsity priors. However, the ST operator is usually constrained to be the same across all channels of the CNN-transformed data. In this paper, we propose a novel operator, called soft thresholding with channel attention (AST), that learns the threshold for each channel. In particular, we put forward a novel deep unrolling shrinkage network (DUS-Net) by unrolling the alternating direction method of multipliers (ADMM) for optimizing the transformed $l_1$ norm dynamic MR reconstruction model. Experimental results on an open-access dynamic cine MR dataset demonstrate that the proposed DUS-Net outperforms the state-of-the-art methods. The source code is available at \url{https://github.com/yhao-z/DUS-Net}.
</details>
<details>
<summary>摘要</summary>
深度折衣网络可以在动态磁共振成像中取得出色的成果。通常情况下，卷积神经网络（CNN）会被用来提取转换域，然后使用软resholding（ST）运算符来实现稀疏约束。然而，ST运算符通常会被限制为所有通道的CNN转换数据中的同一个常数。在这篇论文中，我们提出了一种新的运算符，即通道注意力软resholding（AST），它可以学习每个通道的阈值。具体来说，我们提出了一种新的深度折衣缩放网络（DUS-Net），通过对归一化方法的多个分量（ADMM）进行深度折衣，来优化转换$l_1$ нор的动态MR重建模型。实验结果表明，提出的DUS-Net在一个公开的动态磁共振MR数据集上表现出色，超过了当前的状态艺方法。源代码可以在 \url{https://github.com/yhao-z/DUS-Net} 中获取。
</details></li>
</ul>
<hr>
<h2 id="Manifold-Learning-with-Sparse-Regularised-Optimal-Transport"><a href="#Manifold-Learning-with-Sparse-Regularised-Optimal-Transport" class="headerlink" title="Manifold Learning with Sparse Regularised Optimal Transport"></a>Manifold Learning with Sparse Regularised Optimal Transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09816">http://arxiv.org/abs/2307.09816</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zsteve/QROT">https://github.com/zsteve/QROT</a></li>
<li>paper_authors: Stephen Zhang, Gilles Mordant, Tetsuya Matsumoto, Geoffrey Schiebinger</li>
<li>for: 本文旨在提出一种替代方法，用于在高维拓扑空间中检测数据的稀疏拓扑结构。</li>
<li>methods: 本文提出了一种使用对称最优运输算法，加上二阶regularization的方法，可以生成一个稀疏和自适应的凝合矩阵，这个矩阵可以被看作是一种泛化的bistochastic核心normalization。</li>
<li>results: 本文证明了这种核函数在维度下降时是一个Laplace-type операátor的逼近，并且在异eskedastic随机噪声下展现了稳定性和高效性。此外，本文还提出了一种高效的计算方法，可以快速计算出这种核函数。<details>
<summary>Abstract</summary>
Manifold learning is a central task in modern statistics and data science. Many datasets (cells, documents, images, molecules) can be represented as point clouds embedded in a high dimensional ambient space, however the degrees of freedom intrinsic to the data are usually far fewer than the number of ambient dimensions. The task of detecting a latent manifold along which the data are embedded is a prerequisite for a wide family of downstream analyses. Real-world datasets are subject to noisy observations and sampling, so that distilling information about the underlying manifold is a major challenge. We propose a method for manifold learning that utilises a symmetric version of optimal transport with a quadratic regularisation that constructs a sparse and adaptive affinity matrix, that can be interpreted as a generalisation of the bistochastic kernel normalisation. We prove that the resulting kernel is consistent with a Laplace-type operator in the continuous limit, establish robustness to heteroskedastic noise and exhibit these results in simulations. We identify a highly efficient computational scheme for computing this optimal transport for discrete data and demonstrate that it outperforms competing methods in a set of examples.
</details>
<details>
<summary>摘要</summary>
《映射学习》是现代统计和数据科学中的一个中心任务。许多数据集（细胞、文档、图像、分子）可以被视为高维 ambient 空间中的点云，但实际上数据中的自由度通常比高维维度更少。探索数据中的秘密映射是一个必要的前提，以便进行广泛的后续分析。实际数据受到噪声观测和采样的影响，因此提取数据下面的潜在映射信息是一个主要挑战。我们提出一种基于对称的最优运输方法，并添加了quadratic regularization，可以生成一个稀疏和适应性的亲和力矩阵，这可以被视为泛化带准kernel normalization。我们证明了这种kernel在绝对连续限制下是一个 Laplace-type 算子，并证明其对于不同类型的噪声具有稳定性。我们还提出了一种高效的计算方案，可以快速计算这种最优运输，并在一些示例中证明其超过竞争方法。
</details></li>
</ul>
<hr>
<h2 id="GenKL-An-Iterative-Framework-for-Resolving-Label-Ambiguity-and-Label-Non-conformity-in-Web-Images-Via-a-New-Generalized-KL-Divergence"><a href="#GenKL-An-Iterative-Framework-for-Resolving-Label-Ambiguity-and-Label-Non-conformity-in-Web-Images-Via-a-New-Generalized-KL-Divergence" class="headerlink" title="GenKL: An Iterative Framework for Resolving Label Ambiguity and Label Non-conformity in Web Images Via a New Generalized KL Divergence"></a>GenKL: An Iterative Framework for Resolving Label Ambiguity and Label Non-conformity in Web Images Via a New Generalized KL Divergence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09810">http://arxiv.org/abs/2307.09810</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/codetopaper/genkl">https://github.com/codetopaper/genkl</a></li>
<li>paper_authors: Xia Huang, Kai Fong Ernest Chong</li>
<li>for: 这个论文是为了解决在网络图像数据集中存在异常（Non-Conforming，NC）实例的问题，包括寻找和重新标注NC实例。</li>
<li>methods: 这个论文使用了一种新的方法，即$(\alpha, \beta)$-泛化KL差分（GenKL），来标识和重新标注NC实例。GenKL方法基于KL差分，但是通过使用参数α和β来扩展KL差分，以便更好地捕捉NC实例的特征。</li>
<li>results: 这个论文的实验结果表明，使用GenKL方法可以更好地标识和重新标注NC实例，从而提高图像分类器的性能。在三个网络图像数据集上（Clothing1M、Food101&#x2F;Food101N和mini WebVision 1.0），这个论文实现了新的状态ixel图像分类器的性能：81.34%、85.73%和78.99%&#x2F;92.54%（top-1&#x2F;top-5）。<details>
<summary>Abstract</summary>
Web image datasets curated online inherently contain ambiguous in-distribution (ID) instances and out-of-distribution (OOD) instances, which we collectively call non-conforming (NC) instances. In many recent approaches for mitigating the negative effects of NC instances, the core implicit assumption is that the NC instances can be found via entropy maximization. For "entropy" to be well-defined, we are interpreting the output prediction vector of an instance as the parameter vector of a multinomial random variable, with respect to some trained model with a softmax output layer. Hence, entropy maximization is based on the idealized assumption that NC instances have predictions that are "almost" uniformly distributed. However, in real-world web image datasets, there are numerous NC instances whose predictions are far from being uniformly distributed. To tackle the limitation of entropy maximization, we propose $(\alpha, \beta)$-generalized KL divergence, $\mathcal{D}_{\text{KL}}^{\alpha, \beta}(p\|q)$, which can be used to identify significantly more NC instances. Theoretical properties of $\mathcal{D}_{\text{KL}}^{\alpha, \beta}(p\|q)$ are proven, and we also show empirically that a simple use of $\mathcal{D}_{\text{KL}}^{\alpha, \beta}(p\|q)$ outperforms all baselines on the NC instance identification task. Building upon $(\alpha,\beta)$-generalized KL divergence, we also introduce a new iterative training framework, GenKL, that identifies and relabels NC instances. When evaluated on three web image datasets, Clothing1M, Food101/Food101N, and mini WebVision 1.0, we achieved new state-of-the-art classification accuracies: $81.34\%$, $85.73\%$ and $78.99\%$/$92.54\%$ (top-1/top-5), respectively.
</details>
<details>
<summary>摘要</summary>
<<SYS>>对于在线绘制的图像集，内置了不符合规范（ID）实例和外部不符合规范（OOD）实例，我们总之叫做非 conforming（NC）实例。在当今许多避免 NC 实例的负面影响的方法中，核心隐含假设是通过熵 maximization 找到 NC 实例。为了使熵定义，我们将输出预测向量视为一个 multinomial 随机变量的参数向量，与一个已经训练过的模型的软准输出层相关。因此，熵 maximization 基于理想化的假设，NC 实例的预测是“几乎” uniform 分布。但在实际的网络图像集中，有很多 NC 实例 whose 预测与这种假设不符。为了解决熵 maximization 的局限性，我们提出了 $\alpha$, $\beta$ 权重化 KL 差分， $\mathcal{D}_{\text{KL}}^{\alpha, \beta}(p\|q)$，可以更好地标识 NC 实例。我们证明了 $\mathcal{D}_{\text{KL}}^{\alpha, \beta}(p\|q)$ 的理论性质，并通过实验表明，使用 $\mathcal{D}_{\text{KL}}^{\alpha, \beta}(p\|q)$ 可以超过所有基eline的 NC 实例标识性能。基于 $\alpha$, $\beta$ 权重化 KL 差分，我们还提出了一种新的迭代训练框架，GenKL，可以标识并重新标注 NC 实例。当我们在 Clothing1M、Food101/Food101N 和 mini WebVision 1.0 三个网络图像集上评估时，我们实现了新的状态纪录级别的分类精度：81.34%，85.73% 和 78.99% / 92.54% (top-1/top-5)，分别。
</details></li>
</ul>
<hr>
<h2 id="Graph-Federated-Learning-Based-on-the-Decentralized-Framework"><a href="#Graph-Federated-Learning-Based-on-the-Decentralized-Framework" class="headerlink" title="Graph Federated Learning Based on the Decentralized Framework"></a>Graph Federated Learning Based on the Decentralized Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09801">http://arxiv.org/abs/2307.09801</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peilin Liu, Yanni Tang, Mingyue Zhang, Wu Chen</li>
<li>for: 提高数据隐私和模型精度的 federated learning 应用</li>
<li>methods: 基于分布式机器学习的 Decentralized 框架，通过对节点间数据相似性进行权重补做，集成 gradients 信息</li>
<li>results: 比较 FedAvg、Fedprox、GCFL 和 GCFL+ 方法，实验结果表明提议方法的表现更佳<details>
<summary>Abstract</summary>
Graph learning has a wide range of applications in many scenarios, which require more need for data privacy. Federated learning is an emerging distributed machine learning approach that leverages data from individual devices or data centers to improve the accuracy and generalization of the model, while also protecting the privacy of user data. Graph-federated learning is mainly based on the classical federated learning framework i.e., the Client-Server framework. However, the Client-Server framework faces problems such as a single point of failure of the central server and poor scalability of network topology. First, we introduce the decentralized framework to graph-federated learning. Second, determine the confidence among nodes based on the similarity of data among nodes, subsequently, the gradient information is then aggregated by linear weighting based on confidence. Finally, the proposed method is compared with FedAvg, Fedprox, GCFL, and GCFL+ to verify the effectiveness of the proposed method. Experiments demonstrate that the proposed method outperforms other methods.
</details>
<details>
<summary>摘要</summary>
《图学学习有广泛的应用场景，需要更加严格的数据隐私。联邦学习是一种在多个设备或数据中心之间分布式机器学习方法，可以提高模型准确性和通用性，同时保护用户数据隐私。基于类传统联邦学习框架，图联邦学习主要面临中央服务器单点失败和网络拓扑粗糙等问题。我们首先介绍了分布式框架到图联邦学习。其次，通过数据之间的相似性确定节点间的信任度，然后通过线性权重平均对Gradient信息进行聚合。最后，我们对提出的方法与FedAvg、Fedprox、GCFL和GCFL+进行比较，以证明提出的方法的有效性。实验表明，提出的方法在其他方法之上表现出色。》Note: The translation is done using a machine translation tool, and may not be perfect or idiomatic.
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Forecasting-with-Coherent-Aggregation"><a href="#Probabilistic-Forecasting-with-Coherent-Aggregation" class="headerlink" title="Probabilistic Forecasting with Coherent Aggregation"></a>Probabilistic Forecasting with Coherent Aggregation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09797">http://arxiv.org/abs/2307.09797</a></li>
<li>repo_url: None</li>
<li>paper_authors: Geoffrey Négiar, Ruijun Ma, O. Nangba Meetei, Mengfei Cao, Michael W. Mahoney</li>
<li>for: 这篇论文的目的是提出一个新的模型，用于生成具有层次结构的概率预测，并且将这些预测与层次结构相互关联。</li>
<li>methods: 这篇论文使用了一个对应分析模型，使用卷积神经网来生成因素模型的参数，以及因素负载和基层分布的参数。这个模型可以跟踪基层分布的变化，并且可以用于估计基层分布的数据。</li>
<li>results: 这篇论文的结果显示，该模型在三个层次预测数据集上得到了 significan 改善，具体来说是在11.8-41.4%之间。这篇论文还分析了模型参数对基层分布和因素数的影响。<details>
<summary>Abstract</summary>
Obtaining accurate probabilistic forecasts while respecting hierarchical information is an important operational challenge in many applications, perhaps most obviously in energy management, supply chain planning, and resource allocation. The basic challenge, especially for multivariate forecasting, is that forecasts are often required to be coherent with respect to the hierarchical structure. In this paper, we propose a new model which leverages a factor model structure to produce coherent forecasts by construction. This is a consequence of a simple (exchangeability) observation: permuting \textit{}base-level series in the hierarchy does not change their aggregates. Our model uses a convolutional neural network to produce parameters for the factors, their loadings and base-level distributions; it produces samples which can be differentiated with respect to the model's parameters; and it can therefore optimize for any sample-based loss function, including the Continuous Ranked Probability Score and quantile losses. We can choose arbitrary continuous distributions for the factor and the base-level distributions. We compare our method to two previous methods which can be optimized end-to-end, while enforcing coherent aggregation. Our model achieves significant improvements: between $11.8-41.4\%$ on three hierarchical forecasting datasets. We also analyze the influence of parameters in our model with respect to base-level distribution and number of factors.
</details>
<details>
<summary>摘要</summary>
取得准确的 probabilistic 预测，同时尊重层次结构是许多应用中的重要操作挑战。例如，能源管理、供应链规划和资源分配等领域。基本挑战是， especially for multivariate forecasting， 预测需要具有层次结构的准确性。在这篇论文中，我们提出了一种新的模型，利用因子模型结构生成具有层次结构的准确预测。这是由于一个简单的观察结论：在层次结构中重新排序基级系列不会改变它们的汇总值。我们的模型使用卷积神经网络生成因子、加载和基级分布的参数，并且可以生成可 differentiable 的采样，因此可以优化任何采样基于损失函数，包括连续排名 probability 分数和量程损失。我们可以选择任何连续分布来描述因子和基级分布。我们与之前两种可以满足端到端优化的方法进行比较，我们的模型在三个层次预测 dataset 上实现了显著提升：11.8-41.4%。我们还分析了我们模型中参数的影响，具体来说是基级分布和因子数量。
</details></li>
</ul>
<hr>
<h2 id="Forecasting-Early-with-Meta-Learning"><a href="#Forecasting-Early-with-Meta-Learning" class="headerlink" title="Forecasting Early with Meta Learning"></a>Forecasting Early with Meta Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09796">http://arxiv.org/abs/2307.09796</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/super-shayan/feml">https://github.com/super-shayan/feml</a></li>
<li>paper_authors: Shayan Jawed, Kiran Madhusudhanan, Vijaya Krishna Yalavarthi, Lars Schmidt-Thieme</li>
<li>for: 这个论文是为了提高时间序列预测的性能而设计的。</li>
<li>methods: 这个论文使用的方法是基于 Meta learning 技术，通过在不同的数据集上学习并将这些知识应用到目标数据集上进行预测。具体来说，这个论文提出了一种基于 Convolutional 嵌入层的 Meta learning 方法，该方法可以在有限的 Historic 观察数据上进行预测。</li>
<li>results: 这个论文的结果表明，使用 Meta learning 技术可以提高时间序列预测的性能，并且可以通过在不同的数据集上学习并将这些知识应用到目标数据集上进行预测。此外，这个论文还提出了一些与 Joint learning、Multi-task learning 和经典预测基准相关的解决方案。<details>
<summary>Abstract</summary>
In the early observation period of a time series, there might be only a few historic observations available to learn a model. However, in cases where an existing prior set of datasets is available, Meta learning methods can be applicable. In this paper, we devise a Meta learning method that exploits samples from additional datasets and learns to augment time series through adversarial learning as an auxiliary task for the target dataset. Our model (FEML), is equipped with a shared Convolutional backbone that learns features for varying length inputs from different datasets and has dataset specific heads to forecast for different output lengths. We show that FEML can meta learn across datasets and by additionally learning on adversarial generated samples as auxiliary samples for the target dataset, it can improve the forecasting performance compared to single task learning, and various solutions adapted from Joint learning, Multi-task learning and classic forecasting baselines.
</details>
<details>
<summary>摘要</summary>
在时间序列初期观察期，可能只有一些历史观察值available to learn a model。但在有存在先前数据集的情况下，元学习方法可能可用。在这篇论文中，我们提出了一种元学习方法，利用额外数据集中的样本进行增强时间序列的学习，并通过对Target dataset的挑战性样本进行 adversarial learning 作为 auxillary task。我们的模型（FEML）具有共享的卷积几何学习特征，可以从不同的数据集中学习不同长度的输入特征，并具有特定数据集的头部来预测不同的输出长度。我们表明，FEML可以在不同数据集之间进行元学习，并通过额外学习 adversarial生成的样本来提高预测性能，并比单任务学习和多任务学习、经典预测基elines。
</details></li>
</ul>
<hr>
<h2 id="From-West-to-East-Who-can-understand-the-music-of-the-others-better"><a href="#From-West-to-East-Who-can-understand-the-music-of-the-others-better" class="headerlink" title="From West to East: Who can understand the music of the others better?"></a>From West to East: Who can understand the music of the others better?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09795">http://arxiv.org/abs/2307.09795</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pxaris/ccml">https://github.com/pxaris/ccml</a></li>
<li>paper_authors: Charilaos Papaioannou, Emmanouil Benetos, Alexandros Potamianos</li>
<li>for: 本研究旨在探讨现代音乐情感Recognition（MIR）领域的深度学习模型是否可以用于不同的音乐文化和风格，以及是否可以建立类似的音乐音频嵌入模型，训练于不同文化或风格的数据。</li>
<li>methods: 本研究使用了转移学习方法，以了解不同音乐文化之间的相似性。研究使用了两个西方音乐数据集，两个来自东地中海文化的传统&#x2F;民谱数据集，以及两个印度古典音乐数据集。三个深度音频嵌入模型，包括两个CNN结构和一个Transformer结构，在不同的目标频道上进行了自动标注。</li>
<li>results: 实验结果显示，通过转移学习，在所有频道上达到了竞争性的表现，而最佳来源数据集则随着不同的音乐文化而变化。实现和训练过的模型都公开提供在公共存储库中。<details>
<summary>Abstract</summary>
Recent developments in MIR have led to several benchmark deep learning models whose embeddings can be used for a variety of downstream tasks. At the same time, the vast majority of these models have been trained on Western pop/rock music and related styles. This leads to research questions on whether these models can be used to learn representations for different music cultures and styles, or whether we can build similar music audio embedding models trained on data from different cultures or styles. To that end, we leverage transfer learning methods to derive insights about the similarities between the different music cultures to which the data belongs to. We use two Western music datasets, two traditional/folk datasets coming from eastern Mediterranean cultures, and two datasets belonging to Indian art music. Three deep audio embedding models are trained and transferred across domains, including two CNN-based and a Transformer-based architecture, to perform auto-tagging for each target domain dataset. Experimental results show that competitive performance is achieved in all domains via transfer learning, while the best source dataset varies for each music culture. The implementation and the trained models are both provided in a public repository.
</details>
<details>
<summary>摘要</summary>
Note:* "MIR" stands for Music Information Retrieval.* "CNN" stands for Convolutional Neural Network.* "Transformer" is a type of neural network architecture.* "auto-tagging" refers to the task of assigning genre or style labels to music tracks.* "public repository" refers to a publicly accessible repository of code and data, such as GitHub.
</details></li>
</ul>
<hr>
<h2 id="IncDSI-Incrementally-Updatable-Document-Retrieval"><a href="#IncDSI-Incrementally-Updatable-Document-Retrieval" class="headerlink" title="IncDSI: Incrementally Updatable Document Retrieval"></a>IncDSI: Incrementally Updatable Document Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10323">http://arxiv.org/abs/2307.10323</a></li>
<li>repo_url: None</li>
<li>paper_authors: Varsha Kishore, Chao Wan, Justin Lovelace, Yoav Artzi, Kilian Q. Weinberger</li>
<li>for: 这篇论文主要目标是提出一种方法，使得在文档检索模型已经训练完成后，可以快速添加新文档（约20-50ms每个文档），而不需要重新训练整个数据集或部分数据集。</li>
<li>methods: 该方法基于约束优化问题，通过微小地改变网络参数来添加新文档，而不需要重新训练整个模型。</li>
<li>results: 该方法可以在很短的时间内（约20-50ms每个文档）添加新文档，并且与重新训练整个模型相比，其性能甚至可以达到相同的水平。<details>
<summary>Abstract</summary>
Differentiable Search Index is a recently proposed paradigm for document retrieval, that encodes information about a corpus of documents within the parameters of a neural network and directly maps queries to corresponding documents. These models have achieved state-of-the-art performances for document retrieval across many benchmarks. These kinds of models have a significant limitation: it is not easy to add new documents after a model is trained. We propose IncDSI, a method to add documents in real time (about 20-50ms per document), without retraining the model on the entire dataset (or even parts thereof). Instead we formulate the addition of documents as a constrained optimization problem that makes minimal changes to the network parameters. Although orders of magnitude faster, our approach is competitive with re-training the model on the whole dataset and enables the development of document retrieval systems that can be updated with new information in real-time. Our code for IncDSI is available at https://github.com/varshakishore/IncDSI.
</details>
<details>
<summary>摘要</summary>
“差分搜寻索引”是一种最近提出的文档搜寻方法，它将文档库中的信息嵌入 ней网络中，直接将查询与相应的文档映射到一起。这些模型在许多测试场景中获得了最佳性能。然而，这种模型具有一个重要的限制：添加新文档需要重新训练整个模型。我们提出了一种方法，可以在实时（约20-50ms每个文档）添加新文档，不需要重新训练整个模型或部分模型。我们将添加文档视为一个受限制的优化问题，使得模型参数几乎不会改变。这种方法比较快速，但是和重新训练模型一样竞争，可以在实时更新文档搜寻系统。我们的IncDSI代码可以在 GitHub 上找到。
</details></li>
</ul>
<hr>
<h2 id="A-Note-on-Hardness-of-Computing-Recursive-Teaching-Dimension"><a href="#A-Note-on-Hardness-of-Computing-Recursive-Teaching-Dimension" class="headerlink" title="A Note on Hardness of Computing Recursive Teaching Dimension"></a>A Note on Hardness of Computing Recursive Teaching Dimension</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09792">http://arxiv.org/abs/2307.09792</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pasin Manurangsi</li>
<li>for: 计算概念类中的可 recursive teaching dimension (RTD) 问题需要 $n^{\Omega(\log n)}$ 时间。</li>
<li>methods: 使用 exponential time hypothesis (ETH)。</li>
<li>results: 与布尔特力算法的时间复杂度 ($n^{O(\log n)}$) 相同。<details>
<summary>Abstract</summary>
In this short note, we show that the problem of computing the recursive teaching dimension (RTD) for a concept class (given explicitly as input) requires $n^{\Omega(\log n)}$-time, assuming the exponential time hypothesis (ETH). This matches the running time $n^{O(\log n)}$ of the brute-force algorithm for the problem.
</details>
<details>
<summary>摘要</summary>
在这个短notes中，我们证明计算概念类中的回归教学维度（RTD）问题需要$n^{\Omega(\log n)}$时间，假设快速幂时间假设（ETH）。这与简洁力算法的运行时间$n^{O(\log n)}$匹配。
</details></li>
</ul>
<hr>
<h2 id="Reproducibility-in-Machine-Learning-Driven-Research"><a href="#Reproducibility-in-Machine-Learning-Driven-Research" class="headerlink" title="Reproducibility in Machine Learning-Driven Research"></a>Reproducibility in Machine Learning-Driven Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10320">http://arxiv.org/abs/2307.10320</a></li>
<li>repo_url: None</li>
<li>paper_authors: Harald Semmelrock, Simone Kopeinik, Dieter Theiler, Tony Ross-Hellauer, Dominik Kowald<br>for:* 这个论文旨在探讨机器学习（ML）驱动研究中的可重现性问题。methods:* 本论文通过文献综述来反思现有的ML可重现性问题，以及在不同研究领域中存在的可重现性障碍和挑战。results:* 本论文发现了一些可能支持ML可重现性的驱动因素，包括工具、做法和干预措施。这些可能有助于决策不同解决方案的可行性。<details>
<summary>Abstract</summary>
Research is facing a reproducibility crisis, in which the results and findings of many studies are difficult or even impossible to reproduce. This is also the case in machine learning (ML) and artificial intelligence (AI) research. Often, this is the case due to unpublished data and/or source-code, and due to sensitivity to ML training conditions. Although different solutions to address this issue are discussed in the research community such as using ML platforms, the level of reproducibility in ML-driven research is not increasing substantially. Therefore, in this mini survey, we review the literature on reproducibility in ML-driven research with three main aims: (i) reflect on the current situation of ML reproducibility in various research fields, (ii) identify reproducibility issues and barriers that exist in these research fields applying ML, and (iii) identify potential drivers such as tools, practices, and interventions that support ML reproducibility. With this, we hope to contribute to decisions on the viability of different solutions for supporting ML reproducibility.
</details>
<details>
<summary>摘要</summary>
研究现正面临一场可重现危机，许多研究结果和发现很难或甚至无法重现。这同样适用于机器学习（ML）和人工智能（AI）研究。常常这是因为未公开的数据和/或源代码，以及对ML训练条件的敏感性。虽然研究社区中有许多解决这个问题的方案，如使用ML平台，但ML驱动研究的可重现性并没有显著增长。因此，在这次小调查中，我们会回顾ML驱动研究的可重现性在不同领域的当前情况，认定这些领域中ML可重现性的问题和障碍，以及可能的驱动因素，如工具、做法和干预措施，以支持ML可重现性。我们希望通过这些研究来帮助决策不同解决方案的可行性。
</details></li>
</ul>
<hr>
<h2 id="ZeroQuant-FP-A-Leap-Forward-in-LLMs-Post-Training-W4A8-Quantization-Using-Floating-Point-Formats"><a href="#ZeroQuant-FP-A-Leap-Forward-in-LLMs-Post-Training-W4A8-Quantization-Using-Floating-Point-Formats" class="headerlink" title="ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats"></a>ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09782">http://arxiv.org/abs/2307.09782</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/DeepSpeed">https://github.com/microsoft/DeepSpeed</a></li>
<li>paper_authors: Xiaoxia Wu, Zhewei Yao, Yuxiong He</li>
<li>for: 这个研究旨在探讨大语言模型（LLM）中的 computationally efficient 和 maintaining model quality 之间的平衡。</li>
<li>methods: 本研究使用 floating-point（FP）quantization，特别是FP8和FP4，作为可能的解决方案。</li>
<li>results: 我们的调查发现，FP8 activation 在 LLM 中比 integer（INT8）更出色，尤其是模型具有超过一亿个参数时。 在 weight quantization 方面，我们发现 FP4 与 INT4 相比，表现相似或更好，并且可以让部署在 FP 支持的硬件上，如 H100。 对于精度Alignment问题，我们提出了两个构成簇�limitations的缩减方法，它们几乎没有影响 W4A8 模型的性能。 此外，我们还将量化方法与 Low Rank Compensation (LoRC) 策略相结合，尤其是在较小的模型中，实现更好的性能。 总的来说，本研究显示 FP quantization 具有巨大的潜力，并且可以实现高效的部署在资源有限的环境中。<details>
<summary>Abstract</summary>
In the complex domain of large language models (LLMs), striking a balance between computational efficiency and maintaining model quality is a formidable challenge. Navigating the inherent limitations of uniform quantization, particularly when dealing with outliers, and motivated by the launch of NVIDIA's H100 hardware, this study delves into the viability of floating-point (FP) quantization, particularly focusing on FP8 and FP4, as a potential solution. Our comprehensive investigation reveals that for LLMs, FP8 activation consistently outshines its integer (INT8) equivalent, with the performance edge becoming more noticeable in models possessing parameters beyond one billion. For weight quantization, our findings indicate that FP4 exhibits comparable, if not superior, performance to INT4, simplifying deployment on FP-supported hardware like H100. To mitigate the overhead from precision alignment caused by the disparity between weights and activations, we propose two scaling constraints for weight quantization that negligibly impact the performance compared to the standard W4A8 model. We additionally enhance our quantization methods by integrating the Low Rank Compensation (LoRC) strategy, yielding improvements especially in smaller models. The results of our investigation emphasize the immense potential of FP quantization for LLMs, paving the way for high-efficiency deployment in resource-limited settings.
</details>
<details>
<summary>摘要</summary>
在大型语言模型（LLM）领域中，实现计算效率和模型质量之间的平衡是一项具有挑战性的任务。在固定量化的限制下，特别是处理异常值时，这种研究探讨了浮点（FP）量化的可能性，主要关注FP8和FP4。我们的全面调查发现，对于LLMs，FP8活动频繁表现出优于其整数（INT8）等效的优势，这种优势在模型参数超过一百亿时变得更加明显。对于权量量化，我们的发现表明，FP4与INT4相比，表现相对或者甚至更好，使得在FP支持的硬件上进行部署变得更加简单。为了减少精度对齐所导致的开销，我们提议了两种缩放约束 для权量量化，对于标准W4A8模型来说，影响非常小。此外，我们还改进了我们的量化方法，通过 интеграating Low Rank Compensation（LoRC）策略，尤其在较小的模型中，带来了改进。我们的研究结果表明，FP量化对LLMs具有巨大的潜力，开展高效的部署在有限的资源环境中。
</details></li>
</ul>
<hr>
<h2 id="Text2Layer-Layered-Image-Generation-using-Latent-Diffusion-Model"><a href="#Text2Layer-Layered-Image-Generation-using-Latent-Diffusion-Model" class="headerlink" title="Text2Layer: Layered Image Generation using Latent Diffusion Model"></a>Text2Layer: Layered Image Generation using Latent Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09781">http://arxiv.org/abs/2307.09781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyang Zhang, Wentian Zhao, Xin Lu, Jeff Chien</li>
<li>for: 本研究旨在探讨层 composer 的层 composite 图像生成问题，以提高图像编辑的效率和质量。</li>
<li>methods: 提议使用自适应编码器和扩散模型，同时生成背景、前景、层Mask和组合图像。</li>
<li>results: 实验结果表明，提议的方法可以生成高质量的层 composite 图像，并为未来的研究提供了 benchmark。<details>
<summary>Abstract</summary>
Layer compositing is one of the most popular image editing workflows among both amateurs and professionals. Motivated by the success of diffusion models, we explore layer compositing from a layered image generation perspective. Instead of generating an image, we propose to generate background, foreground, layer mask, and the composed image simultaneously. To achieve layered image generation, we train an autoencoder that is able to reconstruct layered images and train diffusion models on the latent representation. One benefit of the proposed problem is to enable better compositing workflows in addition to the high-quality image output. Another benefit is producing higher-quality layer masks compared to masks produced by a separate step of image segmentation. Experimental results show that the proposed method is able to generate high-quality layered images and initiates a benchmark for future work.
</details>
<details>
<summary>摘要</summary>
层 Compositing 是现代图像编辑中最受欢迎的工作流程之一，both amateur 和专业人士都广泛使用。受扩散模型的成功启发，我们从层图生成的角度来探讨层 Compositing。而不是生成一个图像，我们提议同时生成背景、前景、层面Mask 和组合图像。为实现层图生成，我们训练了一个可重建层图的 autoencoder，并在幂示表示中训练扩散模型。这种方法的一个优点是能够实现更好的组合工作流程，同时生成高质量的层面Mask。另一个优点是生成的层面Mask 比传统的图像分割步骤生成的Mask 更高质量。实验结果表明，我们的方法可以生成高质量的层图，并成为未来工作的标准 referral。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Single-Feature-Importance-with-ICECREAM"><a href="#Beyond-Single-Feature-Importance-with-ICECREAM" class="headerlink" title="Beyond Single-Feature Importance with ICECREAM"></a>Beyond Single-Feature Importance with ICECREAM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09779">http://arxiv.org/abs/2307.09779</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Oesterle, Patrick Blöbaum, Atalanti A. Mastakouri, Elke Kirschbaum</li>
<li>for: 本研究旨在找到模型中任务的可读性和根本原因分析方法，以及哪些变量协会导致特定的输出结果。</li>
<li>methods: 本研究提出了一种信息量度量方法，用于衡量变量协会对目标变量分布的影响。这种方法可以识别哪些变量是获取特定结果的关键因素，而不是单独评估每个变量的重要性。</li>
<li>results: 在synthetic和实际数据上进行了实验，并显示了ICECREAM在可读性和根本原因分析任务中的表现优于现有方法，并在两个任务中达到了很高的准确率。<details>
<summary>Abstract</summary>
Which set of features was responsible for a certain output of a machine learning model? Which components caused the failure of a cloud computing application? These are just two examples of questions we are addressing in this work by Identifying Coalition-based Explanations for Common and Rare Events in Any Model (ICECREAM). Specifically, we propose an information-theoretic quantitative measure for the influence of a coalition of variables on the distribution of a target variable. This allows us to identify which set of factors is essential to obtain a certain outcome, as opposed to well-established explainability and causal contribution analysis methods which can assign contributions only to individual factors and rank them by their importance. In experiments with synthetic and real-world data, we show that ICECREAM outperforms state-of-the-art methods for explainability and root cause analysis, and achieves impressive accuracy in both tasks.
</details>
<details>
<summary>摘要</summary>
本文提出了一种基于联盟的解释方法，可以帮助解释模型的输出和云计算应用的失败。这种方法基于信息理论量化度量变量联盟对目标变量的分布的影响。这允许我们确定哪些变量是获得某种结果的关键因素，而不是单独评估每个变量的重要性。在合成和实际数据上的实验中，我们发现ICECREAM比现有的解释和根本原因分析方法更高效，并在两个任务中达到了出色的准确率。
</details></li>
</ul>
<hr>
<h2 id="Polyffusion-A-Diffusion-Model-for-Polyphonic-Score-Generation-with-Internal-and-External-Controls"><a href="#Polyffusion-A-Diffusion-Model-for-Polyphonic-Score-Generation-with-Internal-and-External-Controls" class="headerlink" title="Polyffusion: A Diffusion Model for Polyphonic Score Generation with Internal and External Controls"></a>Polyffusion: A Diffusion Model for Polyphonic Score Generation with Internal and External Controls</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10304">http://arxiv.org/abs/2307.10304</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aik2mlj/polyffusion">https://github.com/aik2mlj/polyffusion</a></li>
<li>paper_authors: Lejun Min, Junyan Jiang, Gus Xia, Jingwei Zhao</li>
<li>for: 该论文旨在提出一种Diffusion模型，用于生成多重音乐曲谱。</li>
<li>methods: 该模型使用image-like piano roll表示法，并使用内部控制和外部控制两种方法来生成音乐。内部控制是指用户先定义一部分音乐，然后让模型填充其余部分，类似于隐藏音乐生成（或音乐填充）任务。外部控制通过跨注意机制使用外部 yet related的信息，如和声、文化或其他特征来控制模型。</li>
<li>results: 我们的模型在多种音乐创作任务上显著超越了现有的Transformer和采样基eline，并且使用预训练分离表示的外部条件可以更有效地控制音乐生成。<details>
<summary>Abstract</summary>
We propose Polyffusion, a diffusion model that generates polyphonic music scores by regarding music as image-like piano roll representations. The model is capable of controllable music generation with two paradigms: internal control and external control. Internal control refers to the process in which users pre-define a part of the music and then let the model infill the rest, similar to the task of masked music generation (or music inpainting). External control conditions the model with external yet related information, such as chord, texture, or other features, via the cross-attention mechanism. We show that by using internal and external controls, Polyffusion unifies a wide range of music creation tasks, including melody generation given accompaniment, accompaniment generation given melody, arbitrary music segment inpainting, and music arrangement given chords or textures. Experimental results show that our model significantly outperforms existing Transformer and sampling-based baselines, and using pre-trained disentangled representations as external conditions yields more effective controls.
</details>
<details>
<summary>摘要</summary>
我们提出了Polyffusion模型，它可以生成多重音乐分解的乐谱，通过对音乐视为像图式钢琴表示来实现。这个模型具有可控的音乐生成功能，可以通过两种方法进行控制：内部控制和外部控制。内部控制是指用户先定义一部分乐谱，然后让模型填充其余部分，与遮盖音乐生成（或音乐填充）任务类似。外部控制通过跨注意机制使用外部 yet related的信息，如和声、文本或其他特征，来控制模型。我们表明，通过内部和外部控制，Polyffusion模型可以整合多种音乐创作任务，包括给予伴奏的旋律生成、给予旋律的伴奏生成、随机音乐段填充和基于和声或Texture的乐谱安排。实验结果显示，我们的模型在与传统Transformer和采样基础的比较中表现出色，并且使用预训练分离的表示作为外部条件可以更有效地控制。
</details></li>
</ul>
<hr>
<h2 id="Eliminating-Label-Leakage-in-Tree-Based-Vertical-Federated-Learning"><a href="#Eliminating-Label-Leakage-in-Tree-Based-Vertical-Federated-Learning" class="headerlink" title="Eliminating Label Leakage in Tree-Based Vertical Federated Learning"></a>Eliminating Label Leakage in Tree-Based Vertical Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10318">http://arxiv.org/abs/2307.10318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hideaki Takahashi, Jingjing Liu, Yang Liu</li>
<li>for: 防止树型 federated learning 模型泄露 privacy 信息</li>
<li>methods:  introduce 一种新的标签推理攻击 ID2Graph, 使用 instance space 中每个节点的集合来推理私有训练标签</li>
<li>results: 对多个数据集进行了全面的实验，显示 ID2Graph 攻击可以带来重大的隐私泄露风险，而 ID-LMID 防御机制可以有效地遏制标签泄露。<details>
<summary>Abstract</summary>
Vertical federated learning (VFL) enables multiple parties with disjoint features of a common user set to train a machine learning model without sharing their private data. Tree-based models have become prevalent in VFL due to their interpretability and efficiency. However, the vulnerability of tree-based VFL has not been sufficiently investigated. In this study, we first introduce a novel label inference attack, ID2Graph, which utilizes the sets of record-IDs assigned to each node (i.e., instance space) to deduce private training labels. The ID2Graph attack generates a graph structure from training samples, extracts communities from the graph, and clusters the local dataset using community information. To counteract label leakage from the instance space, we propose an effective defense mechanism, ID-LMID, which prevents label leakage by focusing on mutual information regularization. Comprehensive experiments conducted on various datasets reveal that the ID2Graph attack presents significant risks to tree-based models such as Random Forest and XGBoost. Further evaluations on these benchmarks demonstrate that ID-LMID effectively mitigates label leakage in such instances.
</details>
<details>
<summary>摘要</summary>
vertical Federated learning (VFL) 使多个党有不同特征的共同用户集来训练机器学习模型无需分享私人数据。 树状模型在 VFL 中变得普遍使用，因为它们具有可读性和效率。 然而，树状 VFL 的攻击性尚未得到足够的研究。 在这项研究中，我们首先介绍了一种新的标签推理攻击，ID2Graph，它利用训练样本中每个节点（即实例空间）分配的集合ID来推理私人训练标签。 ID2Graph 攻击生成了一个图структура从训练样本中，提取了图中的社区信息，并使用社区信息对本地数据进行分 clustering。 为了防止实例空间中的标签泄露，我们提议一种有效的防御机制，ID-LMID，它通过强调相互信息规范来防止标签泄露。 对于各种数据集进行了全面的实验，我们发现 ID2Graph 攻击对树状模型 such as Random Forest 和 XGBoost 具有显著的风险。 进一步对这些标准 benchmark 进行了评估，我们发现 ID-LMID 能够有效地 mitigate 标签泄露。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Learning-for-WiFi-CSI-Based-Human-Activity-Recognition-A-Systematic-Study"><a href="#Self-Supervised-Learning-for-WiFi-CSI-Based-Human-Activity-Recognition-A-Systematic-Study" class="headerlink" title="Self-Supervised Learning for WiFi CSI-Based Human Activity Recognition: A Systematic Study"></a>Self-Supervised Learning for WiFi CSI-Based Human Activity Recognition: A Systematic Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02412">http://arxiv.org/abs/2308.02412</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JJJinx/SSLCSI">https://github.com/JJJinx/SSLCSI</a></li>
<li>paper_authors: Ke Xu, Jiangtao Wang, Hongyuan Zhu, Dingchang Zheng</li>
<li>for: 这篇论文主要针对的是 WiFi CSI 基于 HAR 的深度学习应用，尤其是在面临数据缺乏问题时，如何通过 SSL 算法来学习有用的表示。</li>
<li>methods: 这篇论文使用了多种类型的 SSL 算法进行研究，包括已经研究过的和尚未研究过的类型，并在 WiFi CSI 基于 HAR 的三个公共数据集上进行了实验。</li>
<li>results: 实验结果表明，SSL 算法在 WiFi CSI 基于 HAR 应用中具有潜在的应用前景，但现有的工作存在一些局限性和盲点，需要进一步改进。<details>
<summary>Abstract</summary>
Recently, with the advancement of the Internet of Things (IoT), WiFi CSI-based HAR has gained increasing attention from academic and industry communities. By integrating the deep learning technology with CSI-based HAR, researchers achieve state-of-the-art performance without the need of expert knowledge. However, the scarcity of labeled CSI data remains the most prominent challenge when applying deep learning models in the context of CSI-based HAR due to the privacy and incomprehensibility of CSI-based HAR data. On the other hand, SSL has emerged as a promising approach for learning meaningful representations from data without heavy reliance on labeled examples. Therefore, considerable efforts have been made to address the challenge of insufficient data in deep learning by leveraging SSL algorithms. In this paper, we undertake a comprehensive inventory and analysis of the potential held by different categories of SSL algorithms, including those that have been previously studied and those that have not yet been explored, within the field. We provide an in-depth investigation of SSL algorithms in the context of WiFi CSI-based HAR. We evaluate four categories of SSL algorithms using three publicly available CSI HAR datasets, each encompassing different tasks and environmental settings. To ensure relevance to real-world applications, we design performance metrics that align with specific requirements. Furthermore, our experimental findings uncover several limitations and blind spots in existing work, highlighting the barriers that need to be addressed before SSL can be effectively deployed in real-world WiFi-based HAR applications. Our results also serve as a practical guideline for industry practitioners and provide valuable insights for future research endeavors in this field.
</details>
<details>
<summary>摘要</summary>
现在，因为互联网物联网（IoT）的发展，WiFi CSI基于HAR（人体动作识别）获得了学术和业界的越来越多的关注。通过将深度学习技术与CSI基于HAR结合，研究人员可以达到 estado del arte的性能，无需专业知识。然而，CSI基于HAR数据的缺乏标注数据仍然是在应用深度学习模型时最大的挑战。另一方面，SSL（自然语言处理）已经出现为了学习不需要大量标注数据的有用表示。因此，在深度学习中使用SSL算法的努力已经被做出了很多。在这篇论文中，我们进行了全面的SSL算法类型的评估和分析，包括已经研究过的和尚未研究过的类型。我们对WiFi CSI基于HAR中SSL算法进行了深入的调查。我们使用三个公共可用的CSI HAR数据集，每个数据集都包括不同的任务和环境设置，来评估四类SSL算法。为确保与实际应用相关，我们定义了与特定需求相符的性能指标。我们的实验结果表明，SSL算法在WiFi CSI基于HAR中具有潜在的优势，但是也存在一些局限性和盲点。我们的结果还提供了实际应用中SSL的实用指南，并为未来在这一领域的研究提供了价值的洞察。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Spatial-Temporal-Variational-Quantum-Circuit-to-Enable-Deep-Learning-on-NISQ-Devices"><a href="#A-Novel-Spatial-Temporal-Variational-Quantum-Circuit-to-Enable-Deep-Learning-on-NISQ-Devices" class="headerlink" title="A Novel Spatial-Temporal Variational Quantum Circuit to Enable Deep Learning on NISQ Devices"></a>A Novel Spatial-Temporal Variational Quantum Circuit to Enable Deep Learning on NISQ Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09771">http://arxiv.org/abs/2307.09771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinyang Li, Zhepeng Wang, Zhirui Hu, Prasanna Date, Ang Li, Weiwen Jiang</li>
<li>for: 本研究旨在推动量子计算机器学（Quantum Computing）在机器学习（Machine Learning）中的发展，提出一种新的空间-时间设计方法（ST-VQC），以提高量子学习模型的非线性性和鲁棒性。</li>
<li>methods: 本研究使用了量子逻辑设计（Quantum Circuit Design）和自动优化框架（Automated Optimization Framework），并对不同设计参数进行系统性分析。</li>
<li>results: 实验结果表明，ST-VQC可以在IBM量子处理器上（IBM_cairo和ibmq_lima）实现30%以上的准确率提升，并在非线性的 sintetic 数据上达到27.9%的提升。<details>
<summary>Abstract</summary>
Quantum computing presents a promising approach for machine learning with its capability for extremely parallel computation in high-dimension through superposition and entanglement. Despite its potential, existing quantum learning algorithms, such as Variational Quantum Circuits(VQCs), face challenges in handling more complex datasets, particularly those that are not linearly separable. What's more, it encounters the deployability issue, making the learning models suffer a drastic accuracy drop after deploying them to the actual quantum devices. To overcome these limitations, this paper proposes a novel spatial-temporal design, namely ST-VQC, to integrate non-linearity in quantum learning and improve the robustness of the learning model to noise. Specifically, ST-VQC can extract spatial features via a novel block-based encoding quantum sub-circuit coupled with a layer-wise computation quantum sub-circuit to enable temporal-wise deep learning. Additionally, a SWAP-Free physical circuit design is devised to improve robustness. These designs bring a number of hyperparameters. After a systematic analysis of the design space for each design component, an automated optimization framework is proposed to generate the ST-VQC quantum circuit. The proposed ST-VQC has been evaluated on two IBM quantum processors, ibm_cairo with 27 qubits and ibmq_lima with 7 qubits to assess its effectiveness. The results of the evaluation on the standard dataset for binary classification show that ST-VQC can achieve over 30% accuracy improvement compared with existing VQCs on actual quantum computers. Moreover, on a non-linear synthetic dataset, the ST-VQC outperforms a linear classifier by 27.9%, while the linear classifier using classical computing outperforms the existing VQC by 15.58%.
</details>
<details>
<summary>摘要</summary>
量子计算技术在机器学习方面具有极大的潜力，它可以通过超position和异构的方式实现高维度的极其平行计算。然而，现有的量子学习算法，如量子环路Circuits（VQCs），在处理更复杂的数据集时存在挑战，特别是不可分 Linearly separable 的数据集。此外，它们在真实的量子设备上部署时会导致学习模型准确率受到极大的影响。为了解决这些限制，本文提出了一种新的空间-时间设计方法，即 ST-VQC，以整合量子学习中的非线性性。具体来说，ST-VQC可以通过一种新的块基编码量子子子电路和层 wise 计算量子子电路来提取空间特征，并且可以在时间维度上进行深度学习。此外，为了提高 robustness，我们还提出了一种 SWAP-Free 物理电路设计。这些设计带来了一些参数。经系统性分析每个设计 ком成分，我们提出了一个自动优化框架，以生成 ST-VQC 量子电路。我们对 IBM 量子处理器 ibm_cairo 和 ibmq_lima 进行评估，结果显示，ST-VQC 在标准的 binary classification 数据集上可以达到30%以上的准确率提升，并在非线性synthetic 数据集上出perform linear classifier 和现有 VQC 的 27.9%和15.58%。
</details></li>
</ul>
<hr>
<h2 id="How-Curvature-Enhance-the-Adaptation-Power-of-Framelet-GCNs"><a href="#How-Curvature-Enhance-the-Adaptation-Power-of-Framelet-GCNs" class="headerlink" title="How Curvature Enhance the Adaptation Power of Framelet GCNs"></a>How Curvature Enhance the Adaptation Power of Framelet GCNs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09768">http://arxiv.org/abs/2307.09768</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dshi3553usyd/curvature_enhanced_graph_convolution">https://github.com/dshi3553usyd/curvature_enhanced_graph_convolution</a></li>
<li>paper_authors: Dai Shi, Yi Guo, Zhiqi Shao, Junbin Gao</li>
<li>for: 提高图像学习模型在图形数据上的性能，具体来说是解决图像中的过熔问题。</li>
<li>methods: 利用图形 Ricci  curvature 作为特殊转换函数 $\zeta$，并证明这种方法可以解决图像中的过熔问题。</li>
<li>results: 对比州对照数据集，提出一种基于 Ricci  curvature 的图像模型，其性能高于现有基eline。<details>
<summary>Abstract</summary>
Graph neural network (GNN) has been demonstrated powerful in modeling graph-structured data. However, despite many successful cases of applying GNNs to various graph classification and prediction tasks, whether the graph geometrical information has been fully exploited to enhance the learning performance of GNNs is not yet well understood. This paper introduces a new approach to enhance GNN by discrete graph Ricci curvature. Specifically, the graph Ricci curvature defined on the edges of a graph measures how difficult the information transits on one edge from one node to another based on their neighborhoods. Motivated by the geometric analogy of Ricci curvature in the graph setting, we prove that by inserting the curvature information with different carefully designed transformation function $\zeta$, several known computational issues in GNN such as over-smoothing can be alleviated in our proposed model. Furthermore, we verified that edges with very positive Ricci curvature (i.e., $\kappa_{i,j} \approx 1$) are preferred to be dropped to enhance model's adaption to heterophily graph and one curvature based graph edge drop algorithm is proposed. Comprehensive experiments show that our curvature-based GNN model outperforms the state-of-the-art baselines in both homophily and heterophily graph datasets, indicating the effectiveness of involving graph geometric information in GNNs.
</details>
<details>
<summary>摘要</summary>
граф neural network (GNN) 已经被证明能够有效地处理图Structured数据。然而，尽管许多GNN应用于不同的图分类和预测任务中得到了成功，whether the graph geometric information has been fully exploited to enhance the learning performance of GNNs is not yet well understood. This paper introduces a new approach to enhance GNN by discrete graph Ricci curvature. Specifically, the graph Ricci curvature defined on the edges of a graph measures how difficult the information transits on one edge from one node to another based on their neighborhoods. Motivated by the geometric analogy of Ricci curvature in the graph setting, we prove that by inserting the curvature information with different carefully designed transformation function $\zeta$, several known computational issues in GNN such as over-smoothing can be alleviated in our proposed model. Furthermore, we verified that edges with very positive Ricci curvature (i.e., $\kappa_{i,j} \approx 1$) are preferred to be dropped to enhance model's adaption to heterophily graph and one curvature based graph edge drop algorithm is proposed. Comprehensive experiments show that our curvature-based GNN model outperforms the state-of-the-art baselines in both homophily and heterophily graph datasets, indicating the effectiveness of involving graph geometric information in GNNs.
</details></li>
</ul>
<hr>
<h2 id="Sig-Splines-universal-approximation-and-convex-calibration-of-time-series-generative-models"><a href="#Sig-Splines-universal-approximation-and-convex-calibration-of-time-series-generative-models" class="headerlink" title="Sig-Splines: universal approximation and convex calibration of time series generative models"></a>Sig-Splines: universal approximation and convex calibration of time series generative models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09767">http://arxiv.org/abs/2307.09767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Magnus Wiese, Phillip Murray, Ralf Korn</li>
<li>for: 这 paper 的目的是为了提出一种基于神经流函数的新型生成模型，用于处理多变量离散时间序列数据。</li>
<li>methods: 该算法采用了神经流函数的构造方式，并结合了线性变换和签名变换，作为传统神经网络的替换。这种方法不仅实现了神经网络的通用性，同时还引入了模型参数的 convexity。</li>
<li>results: 研究人员通过实验和比较分析，证明了该模型的可靠性和精度。<details>
<summary>Abstract</summary>
We propose a novel generative model for multivariate discrete-time time series data. Drawing inspiration from the construction of neural spline flows, our algorithm incorporates linear transformations and the signature transform as a seamless substitution for traditional neural networks. This approach enables us to achieve not only the universality property inherent in neural networks but also introduces convexity in the model's parameters.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的生成模型，用于多变量离散时间序列数据。我们的算法灵感来自神经剖流的构造，并在神经网络中 linear transformation 和签名变换作为一种自然的替换。这种方法不仅可以保持神经网络的通用性，同时还可以在模型参数中引入凸形性。
</details></li>
</ul>
<hr>
<h2 id="Reinforcing-POD-based-model-reduction-techniques-in-reaction-diffusion-complex-networks-using-stochastic-filtering-and-pattern-recognition"><a href="#Reinforcing-POD-based-model-reduction-techniques-in-reaction-diffusion-complex-networks-using-stochastic-filtering-and-pattern-recognition" class="headerlink" title="Reinforcing POD based model reduction techniques in reaction-diffusion complex networks using stochastic filtering and pattern recognition"></a>Reinforcing POD based model reduction techniques in reaction-diffusion complex networks using stochastic filtering and pattern recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09762">http://arxiv.org/abs/2307.09762</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhishek Ajayakumar, Soumyendu Raha</li>
<li>for: 本研究旨在提高受到输入数据干扰的复杂网络模型的准确性。</li>
<li>methods: 我们提出了一种基于模式识别理论和随机滤波理论的算法框架，用于增强复杂网络模型的输出。</li>
<li>results: 我们的研究结果表明，我们的方法可以在受到输入数据干扰的情况下提高复杂网络模型的准确性。<details>
<summary>Abstract</summary>
Complex networks are used to model many real-world systems. However, the dimensionality of these systems can make them challenging to analyze. Dimensionality reduction techniques like POD can be used in such cases. However, these models are susceptible to perturbations in the input data. We propose an algorithmic framework that combines techniques from pattern recognition (PR) and stochastic filtering theory to enhance the output of such models. The results of our study show that our method can improve the accuracy of the surrogate model under perturbed inputs. Deep Neural Networks (DNNs) are susceptible to adversarial attacks. However, recent research has revealed that neural Ordinary Differential Equations (ODEs) exhibit robustness in specific applications. We benchmark our algorithmic framework with a Neural ODE-based approach as a reference.
</details>
<details>
<summary>摘要</summary>
困难网络是现实世界系统模型的重要工具。然而，这些系统的维度可能会使其分析困难。降维技术如POD可以使用在这些情况下。然而，这些模型容易受输入数据的干扰。我们提出一种算法框架， combining技术从图像识别(PR)和随机滤波理论来增强这些模型的输出。我们的研究结果表明，我们的方法可以在受到干扰输入时提高代理模型的准确性。深度神经网络(DNNs)容易受到恶意攻击。然而，最近的研究表明，神经Ordinary Differential Equations(ODEs)在特定应用场景下具有抗震性。我们对比了我们的算法框架与神经ODE-based方法作为参考。
</details></li>
</ul>
<hr>
<h2 id="FedBug-A-Bottom-Up-Gradual-Unfreezing-Framework-for-Federated-Learning"><a href="#FedBug-A-Bottom-Up-Gradual-Unfreezing-Framework-for-Federated-Learning" class="headerlink" title="FedBug: A Bottom-Up Gradual Unfreezing Framework for Federated Learning"></a>FedBug: A Bottom-Up Gradual Unfreezing Framework for Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10317">http://arxiv.org/abs/2307.10317</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iandrover/fedbug">https://github.com/iandrover/fedbug</a></li>
<li>paper_authors: Chia-Hsiang Kao, Yu-Chiang Frank Wang</li>
<li>for: 这个论文旨在解决联合学习中的客户端漂移问题 (Client Drift in Federated Learning)。</li>
<li>methods: 这个论文提出了一个名为 FedBug（联合学习底层逐层解冻）的新的联合学习框架，通过将客户端模型中的各层parameters作为服务器在每个全球轮次分配的参考点，实现跨客户模型的同步。</li>
<li>results: 这个论文透过实验证明了 FedBug 的超越 FedAvg 的减少速率，并且运行在不同的数据集、训练情况和网络架构下，实现了广泛的应用和可靠性。<details>
<summary>Abstract</summary>
Federated Learning (FL) offers a collaborative training framework, allowing multiple clients to contribute to a shared model without compromising data privacy. Due to the heterogeneous nature of local datasets, updated client models may overfit and diverge from one another, commonly known as the problem of client drift. In this paper, we propose FedBug (Federated Learning with Bottom-Up Gradual Unfreezing), a novel FL framework designed to effectively mitigate client drift. FedBug adaptively leverages the client model parameters, distributed by the server at each global round, as the reference points for cross-client alignment. Specifically, on the client side, FedBug begins by freezing the entire model, then gradually unfreezes the layers, from the input layer to the output layer. This bottom-up approach allows models to train the newly thawed layers to project data into a latent space, wherein the separating hyperplanes remain consistent across all clients. We theoretically analyze FedBug in a novel over-parameterization FL setup, revealing its superior convergence rate compared to FedAvg. Through comprehensive experiments, spanning various datasets, training conditions, and network architectures, we validate the efficacy of FedBug. Our contributions encompass a novel FL framework, theoretical analysis, and empirical validation, demonstrating the wide potential and applicability of FedBug.
</details>
<details>
<summary>摘要</summary>
联合学习（FL）提供了一个合作训练框架，让多个客户端参与共同建立一个共享模型，不会妥协数据隐私。由于本地数据的不同性，客户端模型的更新可能会过滤和分化，通常称为客户遗传问题。在这篇论文中，我们提出了FedBug（联合学习底层逐层解冻），一个新的FL框架，可以有效地解决客户遗传问题。FedBug在客户端上运行，从入口层开始逐层解冻，直到出口层为止。这个底层逐层解冻方法让模型在新解冻的层上训练，将数据 проек到一个共同的潜在空间中，并且在所有客户端上保持对应的分隔至对应的抽象面。我们在一个新的过 Parameterization FL 设置下进行了理论分析，发现FedBug的快速渐近率比 FedAvg 更高。通过实验调查，覆盖了不同的数据、训练情况和网络架构，我们证实了FedBug的有效性。我们的贡献包括一个新的FL框架、理论分析和实验验证，显示了FedBug的广泛应用和可能性。
</details></li>
</ul>
<hr>
<h2 id="Constructing-Extreme-Learning-Machines-with-zero-Spectral-Bias"><a href="#Constructing-Extreme-Learning-Machines-with-zero-Spectral-Bias" class="headerlink" title="Constructing Extreme Learning Machines with zero Spectral Bias"></a>Constructing Extreme Learning Machines with zero Spectral Bias</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09759">http://arxiv.org/abs/2307.09759</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaumudi Joshi, Vukka Snigdha, Arya Kumar Bhattacharya</li>
<li>for: 这篇论文旨在探讨饱和学习机制下的特征偏迁现象，以及如何通过修改ELM的结构来消除这种现象。</li>
<li>methods: 这篇论文使用了EXTREME LEARNING MACHINE（ELM）和傅里叶特征嵌入来研究特征偏迁现象。</li>
<li>results: 研究发现，ELM可以通过修改结构来完全消除特征偏迁现象，并且在实际应用中如物理学 Informed Neural Networks（PINNs）中可以实现高频分解。<details>
<summary>Abstract</summary>
The phenomena of Spectral Bias, where the higher frequency components of a function being learnt in a feedforward Artificial Neural Network (ANN) are seen to converge more slowly than the lower frequencies, is observed ubiquitously across ANNs. This has created technology challenges in fields where resolution of higher frequencies is crucial, like in Physics Informed Neural Networks (PINNs). Extreme Learning Machines (ELMs) that obviate an iterative solution process which provides the theoretical basis of Spectral Bias (SB), should in principle be free of the same. This work verifies the reliability of this assumption, and shows that it is incorrect. However, the structure of ELMs makes them naturally amenable to implementation of variants of Fourier Feature Embeddings, which have been shown to mitigate SB in ANNs. This approach is implemented and verified to completely eliminate SB, thus bringing into feasibility the application of ELMs for practical problems like PINNs where resolution of higher frequencies is essential.
</details>
<details>
<summary>摘要</summary>
偏差现象（Spectral Bias）在循环神经网络（ANN）中，高频组分在学习过程中 converges 更慢速度 than 低频组分，这种现象在各种 ANN 中都是普遍存在的。这对于需要高频分辨率的领域，如物理学 Informed Neural Networks（PINNs），带来了技术挑战。Extreme Learning Machines（ELMs），它们不需要迭代解决过程，这个过程提供了 spectral bias （SB）的理论基础，应该是免疫 SB 的。但是，实际情况表明这是错误的假设。然而，ELMs 的结构使其自然地适合 implements  Fourier Feature Embeddings 的 variants，这些 variants 已经在 ANN 中证明可以 mitigate SB。这种方法在实践中被实现并证明可以完全消除 SB，因此使得 ELMs 在实际问题中，如 PINNs，可以实现高频分辨率的应用。
</details></li>
</ul>
<hr>
<h2 id="Improved-Distribution-Matching-for-Dataset-Condensation"><a href="#Improved-Distribution-Matching-for-Dataset-Condensation" class="headerlink" title="Improved Distribution Matching for Dataset Condensation"></a>Improved Distribution Matching for Dataset Condensation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09742">http://arxiv.org/abs/2307.09742</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uitrbn/idm">https://github.com/uitrbn/idm</a></li>
<li>paper_authors: Ganlong Zhao, Guanbin Li, Yipeng Qin, Yizhou Yu</li>
<li>for: 减少深度学习数据存储成本和训练努力</li>
<li>methods: 基于分布匹配的新方法，包括分区和扩展扩充、高效和丰富模型采样、类具有分布常数化</li>
<li>results: 比之前的优化方法更高效，可以处理更大的数据集和模型，并且实验证明其效果<details>
<summary>Abstract</summary>
Dataset Condensation aims to condense a large dataset into a smaller one while maintaining its ability to train a well-performing model, thus reducing the storage cost and training effort in deep learning applications. However, conventional dataset condensation methods are optimization-oriented and condense the dataset by performing gradient or parameter matching during model optimization, which is computationally intensive even on small datasets and models. In this paper, we propose a novel dataset condensation method based on distribution matching, which is more efficient and promising. Specifically, we identify two important shortcomings of naive distribution matching (i.e., imbalanced feature numbers and unvalidated embeddings for distance computation) and address them with three novel techniques (i.e., partitioning and expansion augmentation, efficient and enriched model sampling, and class-aware distribution regularization). Our simple yet effective method outperforms most previous optimization-oriented methods with much fewer computational resources, thereby scaling data condensation to larger datasets and models. Extensive experiments demonstrate the effectiveness of our method. Codes are available at https://github.com/uitrbn/IDM
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Mood-Classification-of-Bangla-Songs-Based-on-Lyrics"><a href="#Mood-Classification-of-Bangla-Songs-Based-on-Lyrics" class="headerlink" title="Mood Classification of Bangla Songs Based on Lyrics"></a>Mood Classification of Bangla Songs Based on Lyrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10314">http://arxiv.org/abs/2307.10314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maliha Mahajebin, Mohammad Rifat Ahmmad Rashid, Nafees Mansoor</li>
<li>for: 本研究旨在分析孟加拉歌曲的情感，并使用自然语言处理和BERT算法对数据进行分析。</li>
<li>methods: 该研究使用了4000首孟加拉歌曲的歌词、类型和BERT算法进行分析，并将歌曲分为四种情感：快乐、悲伤、爱情和放松。</li>
<li>results: 该研究发现，4000首歌曲中有1513首表达悲伤情感，1362首表达爱情情感，886首表达快乐情感，以及239首表达放松情感。这些结果表明了不同的情感可以通过歌曲来表达，并且可以通过自然语言处理和BERT算法来自动分类歌曲的情感。<details>
<summary>Abstract</summary>
Music can evoke various emotions, and with the advancement of technology, it has become more accessible to people. Bangla music, which portrays different human emotions, lacks sufficient research. The authors of this article aim to analyze Bangla songs and classify their moods based on the lyrics. To achieve this, this research has compiled a dataset of 4000 Bangla song lyrics, genres, and used Natural Language Processing and the Bert Algorithm to analyze the data. Among the 4000 songs, 1513 songs are represented for the sad mood, 1362 for the romantic mood, 886 for happiness, and the rest 239 are classified as relaxation. By embedding the lyrics of the songs, the authors have classified the songs into four moods: Happy, Sad, Romantic, and Relaxed. This research is crucial as it enables a multi-class classification of songs' moods, making the music more relatable to people's emotions. The article presents the automated result of the four moods accurately derived from the song lyrics.
</details>
<details>
<summary>摘要</summary>
音乐可以触发不同的情感，而技术的进步使得音乐更加可 accessible 给人们。孟加拉音乐，表达不同人类情感的音乐，尚未得到充分的研究。本文的作者想要分析孟加拉歌曲，根据歌词来划分不同的情感。为此，这项研究编译了4000首孟加拉歌曲的歌词、类型，并使用自然语言处理和bert算法来分析数据。 Among the 4000 songs, 1513 songs are represented for the sad mood, 1362 for the romantic mood, 886 for happiness, and the rest 239 are classified as relaxation. By embedding the lyrics of the songs, the authors have classified the songs into four moods: Happy, Sad, Romantic, and Relaxed. This research is crucial as it enables a multi-class classification of songs' moods, making the music more relatable to people's emotions. The article presents the automated result of the four moods accurately derived from the song lyrics.
</details></li>
</ul>
<hr>
<h2 id="RaTE-a-Reproducible-automatic-Taxonomy-Evaluation-by-Filling-the-Gap"><a href="#RaTE-a-Reproducible-automatic-Taxonomy-Evaluation-by-Filling-the-Gap" class="headerlink" title="RaTE: a Reproducible automatic Taxonomy Evaluation by Filling the Gap"></a>RaTE: a Reproducible automatic Taxonomy Evaluation by Filling the Gap</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09706">http://arxiv.org/abs/2307.09706</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cestlucas/rate">https://github.com/cestlucas/rate</a></li>
<li>paper_authors: Tianjian Gao, Phillipe Langlais</li>
<li>for:  automatic taxonomy evaluation (ATE) is proposed to evaluate the quality of automatically constructed taxonomies.</li>
<li>methods:  the proposed RaTE scoring procedure relies on a pre-trained language model to evaluate the quality of taxonomies without relying on manual evaluation.</li>
<li>results:  the proposed RaTE scoring procedure is found to correlate well with human judgments, and artificially degrading a taxonomy leads to a decreased RaTE score.<details>
<summary>Abstract</summary>
Taxonomies are an essential knowledge representation, yet most studies on automatic taxonomy construction (ATC) resort to manual evaluation to score proposed algorithms. We argue that automatic taxonomy evaluation (ATE) is just as important as taxonomy construction. We propose RaTE, an automatic label-free taxonomy scoring procedure, which relies on a large pre-trained language model. We apply our evaluation procedure to three state-of-the-art ATC algorithms with which we built seven taxonomies from the Yelp domain, and show that 1) RaTE correlates well with human judgments and 2) artificially degrading a taxonomy leads to decreasing RaTE score.
</details>
<details>
<summary>摘要</summary>
税onomy 是知识表现的重要部分，但大多数自动税onomy建构（ATC）研究仍然靠manual评估提出的算法。我们认为自动税onomy评估（ATE）也是非常重要的。我们提出了一个名为 RaTE 的自动无标签税onomy评估方法，它基于一个大型预训语言模型。我们将这个评估方法应用到了三种现有ATC算法，并在Yelp领域建立了七个税onomy，并显示了以下两个结果：1） RaTE 与人工评价有很好的相关性，2）让税onomy降低到人工水平会导致 RaTE 分数下降。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Guided-Generation-for-Large-Language-Models"><a href="#Efficient-Guided-Generation-for-Large-Language-Models" class="headerlink" title="Efficient Guided Generation for Large Language Models"></a>Efficient Guided Generation for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09702">http://arxiv.org/abs/2307.09702</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/normal-computing/outlines">https://github.com/normal-computing/outlines</a></li>
<li>paper_authors: Brandon T. Willard, Rémi Louf</li>
<li>for: 这个论文的目的是描述如何使用 finite-state machine 框架来帮助文本生成器生成文本。</li>
<li>methods: 这个论文使用了 regular expressions 和 context-free grammars 来导航文本生成器的词汇表，并使用了一个索引来捕捉文本生成器的结构。</li>
<li>results: 这个方法可以减少文本生成器的负载，同时允许束缚逻辑和语义约束，并且可以生成可靠的接口。对比现有的方法，这个方法显示出了明显的性能提升。<details>
<summary>Abstract</summary>
In this article we show how the problem of neural text generation can be constructively reformulated in terms of transitions between the states of a finite-state machine. This framework leads to an efficient approach to guiding text generation with regular expressions and context-free grammars by allowing the construction of an index over a language model's vocabulary. The approach is model agnostic, allows one to enforce domain-specific knowledge and constraints, and enables the construction of reliable interfaces by guaranteeing the structure of the generated text. It adds little overhead to the token sequence generation process and significantly outperforms existing solutions. An implementation is provided in the open source Python library Outlines
</details>
<details>
<summary>摘要</summary>
在这篇文章中，我们示出了如何将神经文本生成问题重新定义为finite-state机器的状态转移问题。这个框架导致了一种高效地使用正则表达式和context-free语法来引导文本生成的方法，并允许建立语言模型词汇表的索引。该方法是模型无关的，可以落实域特定的知识和约束，并使得生成的文本结构可靠。它增加了token序列生成过程中的负担，并显著超越了现有的解决方案。我们在Python库Outlines中提供了一个实现。
</details></li>
</ul>
<hr>
<h2 id="STRAPPER-Preference-based-Reinforcement-Learning-via-Self-training-Augmentation-and-Peer-Regularization"><a href="#STRAPPER-Preference-based-Reinforcement-Learning-via-Self-training-Augmentation-and-Peer-Regularization" class="headerlink" title="STRAPPER: Preference-based Reinforcement Learning via Self-training Augmentation and Peer Regularization"></a>STRAPPER: Preference-based Reinforcement Learning via Self-training Augmentation and Peer Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09692">http://arxiv.org/abs/2307.09692</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rll-research/bpref">https://github.com/rll-research/bpref</a></li>
<li>paper_authors: Yachen Kang, Li He, Jinxin Liu, Zifeng Zhuang, Donglin Wang</li>
<li>for: 本研究旨在解决 preference-based reinforcement learning (PbRL) 中人类干预的问题，即将复杂的奖励函数学习到机器人行为中。</li>
<li>methods: 研究使用了 semi-supervised learning 和 consistency regularization 来学习 PbRL 模型，并提出了一种自动训练法和 peer regularization 来提高模型的学习效果。</li>
<li>results: 实验表明，该方法可以在不同的 semi-supervised 情况下学习出高效的机器人行为，并且可以避免 similarity trap 问题，从而提高奖励学习的可靠性。<details>
<summary>Abstract</summary>
Preference-based reinforcement learning (PbRL) promises to learn a complex reward function with binary human preference. However, such human-in-the-loop formulation requires considerable human effort to assign preference labels to segment pairs, hindering its large-scale applications. Recent approache has tried to reuse unlabeled segments, which implicitly elucidates the distribution of segments and thereby alleviates the human effort. And consistency regularization is further considered to improve the performance of semi-supervised learning. However, we notice that, unlike general classification tasks, in PbRL there exits a unique phenomenon that we defined as similarity trap in this paper. Intuitively, human can have diametrically opposite preferredness for similar segment pairs, but such similarity may trap consistency regularization fail in PbRL. Due to the existence of similarity trap, such consistency regularization improperly enhances the consistency possiblity of the model's predictions between segment pairs, and thus reduces the confidence in reward learning, since the augmented distribution does not match with the original one in PbRL. To overcome such issue, we present a self-training method along with our proposed peer regularization, which penalizes the reward model memorizing uninformative labels and acquires confident predictions. Empirically, we demonstrate that our approach is capable of learning well a variety of locomotion and robotic manipulation behaviors using different semi-supervised alternatives and peer regularization.
</details>
<details>
<summary>摘要</summary>
preference-based reinforcement learning (PbRL) 承诺学习复杂的奖励函数使用二进制人类偏好。然而，这种人loop形式需要较大的人类努力来分配偏好标签对 segment pair，限制其大规模应用。 recent approaches  reuse unlabeled segments，即implicitly elucidates the distribution of segments，thereby alleviating the human effort. In addition, consistency regularization is further considered to improve the performance of semi-supervised learning. However, we notice that, unlike general classification tasks, in PbRL there exists a unique phenomenon that we defined as similarity trap in this paper. Intuitively, human can have diametrically opposite preferredness for similar segment pairs, but such similarity may trap consistency regularization fail in PbRL. Due to the existence of similarity trap, such consistency regularization improperly enhances the consistency possibility of the model's predictions between segment pairs, and thus reduces the confidence in reward learning, since the augmented distribution does not match with the original one in PbRL. To overcome such issue, we present a self-training method along with our proposed peer regularization, which penalizes the reward model memorizing uninformative labels and acquires confident predictions. Empirically, we demonstrate that our approach is capable of learning well a variety of locomotion and robotic manipulation behaviors using different semi-supervised alternatives and peer regularization.
</details></li>
</ul>
<hr>
<h2 id="Joint-Service-Caching-Communication-and-Computing-Resource-Allocation-in-Collaborative-MEC-Systems-A-DRL-based-Two-timescale-Approach"><a href="#Joint-Service-Caching-Communication-and-Computing-Resource-Allocation-in-Collaborative-MEC-Systems-A-DRL-based-Two-timescale-Approach" class="headerlink" title="Joint Service Caching, Communication and Computing Resource Allocation in Collaborative MEC Systems: A DRL-based Two-timescale Approach"></a>Joint Service Caching, Communication and Computing Resource Allocation in Collaborative MEC Systems: A DRL-based Two-timescale Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09691">http://arxiv.org/abs/2307.09691</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qianqian Liu, Haixia Zhang, Xin Zhang, Dongfeng Yuan</li>
<li>for: 本研究旨在解决多元Edge Computing（MEC）系统中的服务质量（QoS）要求问题，因为限制的多维度资源带来了很大的挑战。</li>
<li>methods: 本研究提出了一个协同MEC框架，通过Edge服务器之间的资源共享来提高长期QoS和减少缓存交换成本。该框架包括服务缓存、协同下载、计算和通信资源分配等方面的优化。</li>
<li>results: 研究结果表明，提出的算法比基eline算法在平均QoS和缓存交换成本方面具有显著优势。<details>
<summary>Abstract</summary>
Meeting the strict Quality of Service (QoS) requirements of terminals has imposed a signiffcant challenge on Multiaccess Edge Computing (MEC) systems, due to the limited multidimensional resources. To address this challenge, we propose a collaborative MEC framework that facilitates resource sharing between the edge servers, and with the aim to maximize the long-term QoS and reduce the cache switching cost through joint optimization of service caching, collaborative offfoading, and computation and communication resource allocation. The dual timescale feature and temporal recurrence relationship between service caching and other resource allocation make solving the problem even more challenging. To solve it, we propose a deep reinforcement learning (DRL)-based dual timescale scheme, called DGL-DDPG, which is composed of a short-term genetic algorithm (GA) and a long short-term memory network-based deep deterministic policy gradient (LSTM-DDPG). In doing so, we reformulate the optimization problem as a Markov decision process (MDP) where the small-timescale resource allocation decisions generated by an improved GA are taken as the states and input into a centralized LSTM-DDPG agent to generate the service caching decision for the large-timescale. Simulation results demonstrate that our proposed algorithm outperforms the baseline algorithms in terms of the average QoS and cache switching cost.
</details>
<details>
<summary>摘要</summary>
To solve this problem, we propose a deep reinforcement learning (DRL)-based dual timescale scheme, called DGL-DDPG, which consists of a short-term genetic algorithm (GA) and a long short-term memory network-based deep deterministic policy gradient (LSTM-DDPG). We reformulate the optimization problem as a Markov decision process (MDP) where the small-timescale resource allocation decisions generated by an improved GA are taken as the states and input into a centralized LSTM-DDPG agent to generate the service caching decision for the large-timescale.Simulation results show that our proposed algorithm outperforms baseline algorithms in terms of average QoS and cache switching cost.
</details></li>
</ul>
<hr>
<h2 id="Amazon-M2-A-Multilingual-Multi-locale-Shopping-Session-Dataset-for-Recommendation-and-Text-Generation"><a href="#Amazon-M2-A-Multilingual-Multi-locale-Shopping-Session-Dataset-for-Recommendation-and-Text-Generation" class="headerlink" title="Amazon-M2: A Multilingual Multi-locale Shopping Session Dataset for Recommendation and Text Generation"></a>Amazon-M2: A Multilingual Multi-locale Shopping Session Dataset for Recommendation and Text Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09688">http://arxiv.org/abs/2307.09688</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Jin, Haitao Mao, Zheng Li, Haoming Jiang, Chen Luo, Hongzhi Wen, Haoyu Han, Hanqing Lu, Zhengyang Wang, Ruirui Li, Zhen Li, Monica Xiao Cheng, Rahul Goutam, Haiyang Zhang, Karthik Subbian, Suhang Wang, Yizhou Sun, Jiliang Tang, Bing Yin, Xianfeng Tang<br>for:The paper is written to present a new multilingual dataset for session-based recommendation, which can help improve personalization and understanding of user preferences.methods:The paper uses a dataset of millions of user sessions from six different locales, with products in English, German, Japanese, French, Italian, and Spanish.results:The paper introduces three tasks based on the dataset: next-product recommendation, next-product recommendation with domain shifts, and next-product title generation. The paper also hosts a competition in the KDD CUP 2023 and attracts thousands of users and submissions.<details>
<summary>Abstract</summary>
Modeling customer shopping intentions is a crucial task for e-commerce, as it directly impacts user experience and engagement. Thus, accurately understanding customer preferences is essential for providing personalized recommendations. Session-based recommendation, which utilizes customer session data to predict their next interaction, has become increasingly popular. However, existing session datasets have limitations in terms of item attributes, user diversity, and dataset scale. As a result, they cannot comprehensively capture the spectrum of user behaviors and preferences. To bridge this gap, we present the Amazon Multilingual Multi-locale Shopping Session Dataset, namely Amazon-M2. It is the first multilingual dataset consisting of millions of user sessions from six different locales, where the major languages of products are English, German, Japanese, French, Italian, and Spanish. Remarkably, the dataset can help us enhance personalization and understanding of user preferences, which can benefit various existing tasks as well as enable new tasks. To test the potential of the dataset, we introduce three tasks in this work: (1) next-product recommendation, (2) next-product recommendation with domain shifts, and (3) next-product title generation. With the above tasks, we benchmark a range of algorithms on our proposed dataset, drawing new insights for further research and practice. In addition, based on the proposed dataset and tasks, we hosted a competition in the KDD CUP 2023 and have attracted thousands of users and submissions. The winning solutions and the associated workshop can be accessed at our website https://kddcup23.github.io/.
</details>
<details>
<summary>摘要</summary>
<<SYS>>模型客户购物意图是电商中的一项重要任务，因为它直接影响用户体验和参与度。因此，正确理解客户首选是电商中非常重要的。会话基本建议，利用客户会话数据预测他们下一次的交互，在过去几年变得越来越受欢迎。然而，现有的会话数据集受到物品属性、用户多样性和数据规模的限制，无法全面捕捉用户行为和首选。为了bridging这个差距，我们提出了亚马逊多语言多地区购物会话数据集（Amazon-M2）。这是首个多语言的数据集，包含来自六个不同的地区的数百万个用户会话，其中主要语言为英文、德语、日语、法语、意大利语和西班牙语。这个数据集可以帮助我们提高个性化和用户首选的理解，从而对电商中的既有任务也能够提供新的任务。为了评估该数据集的潜力，我们在这篇论文中引入了三个任务：（1）下一个产品推荐，（2）下一个产品推荐与领域变化，以及（3）下一个产品标题生成。我们使用这些任务对我们的提posed数据集进行了测试，从而获得了新的发现和实践经验。此外，基于我们的提posed数据集和任务，我们在KDD CUP 2023中组织了一个比赛，并吸引了千计用户和提交。赢家的解决方案和相关工作均可在我们的官方网站（https://kddcup23.github.io/）上查看。
</details></li>
</ul>
<hr>
<h2 id="Convex-Geometry-of-ReLU-layers-Injectivity-on-the-Ball-and-Local-Reconstruction"><a href="#Convex-Geometry-of-ReLU-layers-Injectivity-on-the-Ball-and-Local-Reconstruction" class="headerlink" title="Convex Geometry of ReLU-layers, Injectivity on the Ball and Local Reconstruction"></a>Convex Geometry of ReLU-layers, Injectivity on the Ball and Local Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09672">http://arxiv.org/abs/2307.09672</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/danedane-haider/alpha-rectifying-frames">https://github.com/danedane-haider/alpha-rectifying-frames</a></li>
<li>paper_authors: Daniel Haider, Martin Ehler, Peter Balazs</li>
<li>for: study the injectivity of a ReLU-layer on the closed ball of $\mathbb{R}^n$ and its non-negative part.</li>
<li>methods: use a frame-theoretic setting and convex geometry to develop a computationally feasible method for verifying the injectivity of a ReLU-layer under reasonable restrictions.</li>
<li>results: provide explicit reconstruction formulas inspired by the duality concept from frame theory, allowing for the possibility of quantifying the invertibility of a ReLU-layer and a concrete reconstruction algorithm for any input vector on the ball.Here’s the Chinese text:</li>
<li>for: 研究固定ball of $\mathbb{R}^n$中ReLU层的注入性和非负部分。</li>
<li>methods: 使用框架理论设定和几何学来开发一种可行的方法，用于在有理性限制下验证ReLU层的注入性。</li>
<li>results: 提供启发式的重构方程，基于框架理论中的对偶性概念，使得可以对ReLU层的注入性进行量化，并提供一个可行的重构算法 для任何输入向量在球体上。<details>
<summary>Abstract</summary>
The paper uses a frame-theoretic setting to study the injectivity of a ReLU-layer on the closed ball of $\mathbb{R}^n$ and its non-negative part. In particular, the interplay between the radius of the ball and the bias vector is emphasized. Together with a perspective from convex geometry, this leads to a computationally feasible method of verifying the injectivity of a ReLU-layer under reasonable restrictions in terms of an upper bound of the bias vector. Explicit reconstruction formulas are provided, inspired by the duality concept from frame theory. All this gives rise to the possibility of quantifying the invertibility of a ReLU-layer and a concrete reconstruction algorithm for any input vector on the ball.
</details>
<details>
<summary>摘要</summary>
文章使用框理 Setting来研究ReLU层在闭合球上的具体性和非负部分。特别是弹性和偏置向量之间的关系。通过几何学的视角，可以实现有理性的层逻辑检查，并提供了可行的重建方法。这些方法基于框理中的对偶概念，并且可以量化ReLU层的具体性和任意输入向量的重建。
</details></li>
</ul>
<hr>
<h2 id="JAZZVAR-A-Dataset-of-Variations-found-within-Solo-Piano-Performances-of-Jazz-Standards-for-Music-Overpainting"><a href="#JAZZVAR-A-Dataset-of-Variations-found-within-Solo-Piano-Performances-of-Jazz-Standards-for-Music-Overpainting" class="headerlink" title="JAZZVAR: A Dataset of Variations found within Solo Piano Performances of Jazz Standards for Music Overpainting"></a>JAZZVAR: A Dataset of Variations found within Solo Piano Performances of Jazz Standards for Music Overpainting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09670">http://arxiv.org/abs/2307.09670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eleanor Row, Jingjing Tang, George Fazekas</li>
<li>for: 这个论文的目的是为了提供一个基于MIDI格式的 jazz标准曲目变化搜索数据集，以便进行音乐信息检索（MIR）和音乐生成等应用。</li>
<li>methods: 该论文使用了人工提取的 jazz 演奏中的变化段落，并将其与原始曲目的旋律和和声组合成对应的原始和变化段落对。</li>
<li>results: 该论文介绍了数据集的准备和排序过程，以及对数据集的分析和一种基于Transformer模型的音乐覆盖任务的基线实现。 I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Jazz pianists often uniquely interpret jazz standards. Passages from these interpretations can be viewed as sections of variation. We manually extracted such variations from solo jazz piano performances. The JAZZVAR dataset is a collection of 502 pairs of Variation and Original MIDI segments. Each Variation in the dataset is accompanied by a corresponding Original segment containing the melody and chords from the original jazz standard. Our approach differs from many existing jazz datasets in the music information retrieval (MIR) community, which often focus on improvisation sections within jazz performances. In this paper, we outline the curation process for obtaining and sorting the repertoire, the pipeline for creating the Original and Variation pairs, and our analysis of the dataset. We also introduce a new generative music task, Music Overpainting, and present a baseline Transformer model trained on the JAZZVAR dataset for this task. Other potential applications of our dataset include expressive performance analysis and performer identification.
</details>
<details>
<summary>摘要</summary>
爵士钢琴家们经常独特地 интерпретирова jazz标准。这些 интерпретаción 中的段落可以被视为变化段落。我们手动从爵士钢琴独奏中提取出这些变化。JAZZVAR 数据集包含 502对变化和原始 MIDI 段落的对。每个变化段落在数据集中都有对应的原始段落，其中包含原始爵士标准的旋律和和声。我们的方法与许多现有的 jazz 数据集在音乐信息检索（MIR）社区中的做法不同，这些数据集通常专注于 jazz 表演中的即兴段落。在这篇文章中，我们介绍了收录和排序 репертуа 的过程，以及创建 Original 和变化对的管道。我们还介绍了我们对这些数据集的分析，以及一种基于这些数据集的新的生成音乐任务——音乐覆盖。此外，这些数据集还有表达性表演分析和演奏者识别的潜在应用。
</details></li>
</ul>
<hr>
<h2 id="Towards-A-Unified-Agent-with-Foundation-Models"><a href="#Towards-A-Unified-Agent-with-Foundation-Models" class="headerlink" title="Towards A Unified Agent with Foundation Models"></a>Towards A Unified Agent with Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09668">http://arxiv.org/abs/2307.09668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Norman Di Palo, Arunkumar Byravan, Leonard Hasenclever, Markus Wulfmeier, Nicolas Heess, Martin Riedmiller</li>
<li>for: 本研究旨在嵌入和利用语言模型和视觉语言模型的能力，以解决人工智能束缚学习（RL）Agent的基本挑战。</li>
<li>methods: 我们设计了一个框架，使用语言作为RL Agent的核心理解工具，并研究如何通过语言来解决RL中的基本挑战，如有效尝试、再利用经验数据、计划技能和从观察学习。</li>
<li>results: 我们在一个具有罕见奖励的 simulate robotic manipulation 环境中测试了我们的方法，并证明了在尝试效率和再利用经验数据方面的显著性能提升。此外，我们还 illustrate了如何通过 reuse 学习的技能来解决新任务或模仿人类专家的视频。<details>
<summary>Abstract</summary>
Language Models and Vision Language Models have recently demonstrated unprecedented capabilities in terms of understanding human intentions, reasoning, scene understanding, and planning-like behaviour, in text form, among many others. In this work, we investigate how to embed and leverage such abilities in Reinforcement Learning (RL) agents. We design a framework that uses language as the core reasoning tool, exploring how this enables an agent to tackle a series of fundamental RL challenges, such as efficient exploration, reusing experience data, scheduling skills, and learning from observations, which traditionally require separate, vertically designed algorithms. We test our method on a sparse-reward simulated robotic manipulation environment, where a robot needs to stack a set of objects. We demonstrate substantial performance improvements over baselines in exploration efficiency and ability to reuse data from offline datasets, and illustrate how to reuse learned skills to solve novel tasks or imitate videos of human experts.
</details>
<details>
<summary>摘要</summary>
language models和视觉语言模型在文本形式中表现出了无 precedent的理解人类意图、逻辑、场景理解和规划行为等能力。在这项工作中，我们调查了如何将这些能力embedded在奖励学习（RL）代理人中。我们设计了一个框架，用语言作为核心的理解工具，探索如何使得代理人通过语言来解决RL中的一系列基本挑战，如高效探索、重用经验数据、调度技能和学习从观察中学习，这些传统需要分开、垂直设计的算法。我们在一个缺乏奖励的 simulated robotic manipulation环境中测试了我们的方法，where a robot needs to stack a set of objects。我们示出了与基elinesubstantial performance improvement在探索效率和重用数据from offline datasets，并解释了如何重用已学的技能来解决新任务或模仿人类专家的视频。
</details></li>
</ul>
<hr>
<h2 id="Anticipating-Technical-Expertise-and-Capability-Evolution-in-Research-Communities-using-Dynamic-Graph-Transformers"><a href="#Anticipating-Technical-Expertise-and-Capability-Evolution-in-Research-Communities-using-Dynamic-Graph-Transformers" class="headerlink" title="Anticipating Technical Expertise and Capability Evolution in Research Communities using Dynamic Graph Transformers"></a>Anticipating Technical Expertise and Capability Evolution in Research Communities using Dynamic Graph Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09665">http://arxiv.org/abs/2307.09665</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sameera Horawalavithana, Ellyn Ayton, Anastasiya Usenko, Robin Cosbey, Svitlana Volkova</li>
<li>for: This paper aims to anticipate technical expertise and capability evolution trends in safety-critical domains like nuclear nonproliferation and rapidly emerging fields like artificial intelligence.</li>
<li>methods: The paper extends traditional statistical relational learning approaches and formulates a problem of anticipating technical expertise and capability evolution using dynamic heterogeneous graph representations. It develops novel capabilities to forecast collaboration patterns, authorship behavior, and technical capability evolution at different granularities.</li>
<li>results: The paper demonstrates that its dynamic graph transformer (DGT) models predict collaboration, partnership, and expertise patterns with high accuracy, exceeding the best-performing static graph baseline models by 30-80% across AI and NN domains. The models accurately predict which established scientists will collaborate with early career scientists and vice-versa in the AI domain.Here is the simplified Chinese text for the three key points:</li>
<li>for: 这篇论文目标是预测安全关键领域如核不扩散和人工智能等领域技术能力和能力演化趋势。</li>
<li>methods: 该论文基于传统统计关系学学习方法，并将技术能力和能力演化问题转化为动态不同类型图表示。它开发出了新的能力预测合作模式、作者行为和技术能力演化等方面的预测能力。</li>
<li>results: 该论文示出了其动态图变换器（DGT）模型在AI和NN领域的合作、合作伙伴和技术能力演化方面预测精度高，超过了最佳静止图基线模型的30-80%。DGT模型可以准确预测在AI领域中已有名气的科学家和初出茅廊的科学家之间的合作。<details>
<summary>Abstract</summary>
The ability to anticipate technical expertise and capability evolution trends globally is essential for national and global security, especially in safety-critical domains like nuclear nonproliferation (NN) and rapidly emerging fields like artificial intelligence (AI). In this work, we extend traditional statistical relational learning approaches (e.g., link prediction in collaboration networks) and formulate a problem of anticipating technical expertise and capability evolution using dynamic heterogeneous graph representations. We develop novel capabilities to forecast collaboration patterns, authorship behavior, and technical capability evolution at different granularities (e.g., scientist and institution levels) in two distinct research fields. We implement a dynamic graph transformer (DGT) neural architecture, which pushes the state-of-the-art graph neural network models by (a) forecasting heterogeneous (rather than homogeneous) nodes and edges, and (b) relying on both discrete -- and continuous -- time inputs. We demonstrate that our DGT models predict collaboration, partnership, and expertise patterns with 0.26, 0.73, and 0.53 mean reciprocal rank values for AI and 0.48, 0.93, and 0.22 for NN domains. DGT model performance exceeds the best-performing static graph baseline models by 30-80% across AI and NN domains. Our findings demonstrate that DGT models boost inductive task performance, when previously unseen nodes appear in the test data, for the domains with emerging collaboration patterns (e.g., AI). Specifically, models accurately predict which established scientists will collaborate with early career scientists and vice-versa in the AI domain.
</details>
<details>
<summary>摘要</summary>
“预测技术专业和能力发展趋势 globally 是国家和全球安全的重要因素，尤其在安全关键领域如核不扩散（NN）和快速发展的领域如人工智能（AI）。在这个工作中，我们扩展传统的统计关系学习方法（例如连接预测在合作网络中），并将问题定义为预测技术专业和能力发展。我们发展了一些新的能力来预测合作模式、作者行为和技术能力发展的不同粒度（例如科学家和机构层级）在两个不同的研究领域。我们实现了动态图 transformer（DGT）神经架构，这个架构超过了当前最好的静止图基eline模型的性能，因为它可以预测不同类型的node和edge，并且可以使用不同的时间点进行预测。我们的DGT模型在AI和NN领域中预测了合作、合作伙伴和技术能力发展的模式，其中的mean reciprocal rank值为0.26、0.73、0.53和0.48、0.93、0.22。相比于最佳的静止图基eline模型，DGT模型在AI和NN领域中的表现提高了30-80%。我们的发现证明了DGT模型可以提高induction任务的表现，当前所未见的node出现在试验数据时。具体来说，模型可以正确预测在AI领域中已成熟的科学家和初级科学家之间的合作关系。”
</details></li>
</ul>
<hr>
<h2 id="Physics-based-Reduced-Order-Modeling-for-Uncertainty-Quantification-of-Guided-Wave-Propagation-using-Bayesian-Optimization"><a href="#Physics-based-Reduced-Order-Modeling-for-Uncertainty-Quantification-of-Guided-Wave-Propagation-using-Bayesian-Optimization" class="headerlink" title="Physics-based Reduced Order Modeling for Uncertainty Quantification of Guided Wave Propagation using Bayesian Optimization"></a>Physics-based Reduced Order Modeling for Uncertainty Quantification of Guided Wave Propagation using Bayesian Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09661">http://arxiv.org/abs/2307.09661</a></li>
<li>repo_url: None</li>
<li>paper_authors: G. I. Drakoulas, T. V. Gortsas, D. Polyzos<br>for: This paper focuses on the development of a machine learning-based reduced order model (BO-ML-ROM) for guided wave propagation (GWP) in structural health monitoring (SHM) applications.methods: The proposed BO-ML-ROM is integrated with a Bayesian optimization framework to adaptively sample the parameters for training, and the finite element method is used for high-fidelity simulations.results: The results show that the BO-ML-ROM outperforms one-shot sampling methods in terms of accuracy and speed-up, and demonstrates its value for uncertainty quantification (UQ) in GWP. The predicted results also reveal the efficiency of the proposed method for SHM applications.<details>
<summary>Abstract</summary>
In the context of digital twins, structural health monitoring (SHM) constitutes the backbone of condition-based maintenance, facilitating the interconnection between virtual and physical assets. Guided wave propagation (GWP) is commonly employed for the inspection of structures in SHM. However, GWP is sensitive to variations in the material properties of the structure, leading to false alarms. In this direction, uncertainty quantification (UQ) is regularly applied to improve the reliability of predictions. Computational mechanics is a useful tool for the simulation of GWP, and is often applied for UQ. Even so, the application of UQ methods requires numerous simulations, while large-scale, transient numerical GWP solutions increase the computational cost. Reduced order models (ROMs) are commonly employed to provide numerical results in a limited amount of time. In this paper, we propose a machine learning (ML)-based ROM, mentioned as BO-ML-ROM, to decrease the computational time related to the simulation of the GWP. The ROM is integrated with a Bayesian optimization (BO) framework, to adaptively sample the parameters for the ROM training. The finite element method is used for the simulation of the high-fidelity models. The formulated ROM is used for forward UQ of the GWP in an aluminum plate with varying material properties. To determine the influence of each parameter perturbation, a global, variance-based sensitivity analysis is implemented based on Sobol' indices. It is shown that Bayesian optimization outperforms one-shot sampling methods, both in terms of accuracy and speed-up. The predicted results reveal the efficiency of BO-ML-ROM for GWP and demonstrate its value for UQ.
</details>
<details>
<summary>摘要</summary>
在数字双身的Context中，结构健康监测（SHM）是维护基于状况的核心，实现虚拟和物理资产之间的连接。广播波传播（GWP）通常用于结构的检测，但GWP受结构物理属性变化的影响，导致假警告。为了改善预测的可靠性，不确定量衡（UQ）经常应用。计算机机械是GWP的计算机模拟的有用工具，并常用于UQ。然而，UQ方法的应用需要许多 simulations，而大规模、Transient numerical GWP解决方案会增加计算成本。减少的模型（ROMs）通常用于提供数字化的结果，以降低相关的计算时间。在这篇论文中，我们提出了一种基于机器学习（ML）的减少模型，称为BO-ML-ROM，以降低GWP的计算时间。BO-ML-ROM与抽象优化（BO）框架相结合，以适应参数的参数采样。finite element方法用于高精度模型的Simulation。我们使用 Sobol 指数进行全局、基于差异的敏感分析，以确定每个参数的影响。结果表明，bayesian optimization在精度和速度两个方面都超过了一次采样方法。预测结果表明，BO-ML-ROM可以高效地用于GWP和UQ。
</details></li>
</ul>
<hr>
<h2 id="Neural-Priority-Queues-for-Graph-Neural-Networks"><a href="#Neural-Priority-Queues-for-Graph-Neural-Networks" class="headerlink" title="Neural Priority Queues for Graph Neural Networks"></a>Neural Priority Queues for Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09660">http://arxiv.org/abs/2307.09660</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rishabh Jain, Petar Veličković, Pietro Liò</li>
<li>for: 本研究旨在扩展Graph Neural Networks (GNNs)，通过添加外部存储来提高神经算法的推理能力。</li>
<li>methods: 本文提出了一种名为Neural Priority Queues的神经网络模块，该模块是一种可微分的优先队列模型，用于GNNs。</li>
<li>results: 实验结果表明，Neural PQs可以准确地捕捉长距离依赖关系，并且可以与算法性reasoning相结合。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have shown considerable success in neural algorithmic reasoning. Many traditional algorithms make use of an explicit memory in the form of a data structure. However, there has been limited exploration on augmenting GNNs with external memory. In this paper, we present Neural Priority Queues, a differentiable analogue to algorithmic priority queues, for GNNs. We propose and motivate a desiderata for memory modules, and show that Neural PQs exhibit the desiderata, and reason about their use with algorithmic reasoning. This is further demonstrated by empirical results on the CLRS-30 dataset. Furthermore, we find the Neural PQs useful in capturing long-range interactions, as empirically shown on a dataset from the Long-Range Graph Benchmark.
</details>
<details>
<summary>摘要</summary>
图 neural networks (GNNs) 在神经算法理解中表现出了considerable 的成功。许多传统算法都会使用一个显式的记忆structure，但是对于 GNNs 的外部记忆的探索却有限。在这篇论文中，我们提出了一种名为 Neural Priority Queues（神经优先队列）的可微分的优先队列，并论述了这种模块的愿景。我们还进行了对这种模块的reasoning，并通过实验结果在 CLRS-30 数据集上展示了其效果。此外，我们发现 Neural PQs 可以有效地捕捉长距离交互，如在 Long-Range Graph Benchmark 上的数据集中所示。Here's the translation of the text into Traditional Chinese:граф値 neural networks (GNNs) 在神经算法理解中表现出了considerable 的成功。许多传统算法都会使用一个显式的内存strucure，但是对于 GNNs 的外部记忆的探索却有限。在这篇论文中，我们提出了一种名为 Neural Priority Queues（神经优先队列）的可微分的优先队列，并诉述了这种模块的愿景。我们还进行了对这种模块的reasoning，并通过实验结果在 CLRS-30 数据集上展示了其效果。此外，我们发现 Neural PQs 可以有效地捕捉长距离交互，如在 Long-Range Graph Benchmark 上的数据集中所示。
</details></li>
</ul>
<hr>
<h2 id="HAT-CL-A-Hard-Attention-to-the-Task-PyTorch-Library-for-Continual-Learning"><a href="#HAT-CL-A-Hard-Attention-to-the-Task-PyTorch-Library-for-Continual-Learning" class="headerlink" title="HAT-CL: A Hard-Attention-to-the-Task PyTorch Library for Continual Learning"></a>HAT-CL: A Hard-Attention-to-the-Task PyTorch Library for Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09653">http://arxiv.org/abs/2307.09653</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xduan7/hat-cl">https://github.com/xduan7/hat-cl</a></li>
<li>paper_authors: Xiaotian Duan</li>
<li>for: 这篇论文主要针对Continual Learning中的忘记现象（Catastrophic Forgetting），以及现有的Hard-Attention-to-the-Task（HAT）机制在解决这个问题上的潜在应用。</li>
<li>methods: 该论文提出了一种名为HAT-CL的用户友好、PyTorch兼容的HAT机制重新设计，它不仅自动 manipulate梯度还可以快速地将PyTorch模块转换为HAT模块。此外，HAT-CL还提供了一套可以快速地与现有架构集成的模块，以及与TIMM库集成的准备好使用的HAT网络。</li>
<li>results: 该论文通过对多个实验来展示HAT-CL的表现，并实现了在不同的模型和应用场景中的广泛应用。此外，论文还介绍了一些新的面积 manipulate技术，这些技术在多个实验中均有显著的改进。<details>
<summary>Abstract</summary>
Catastrophic forgetting, the phenomenon in which a neural network loses previously obtained knowledge during the learning of new tasks, poses a significant challenge in continual learning. The Hard-Attention-to-the-Task (HAT) mechanism has shown potential in mitigating this problem, but its practical implementation has been complicated by issues of usability and compatibility, and a lack of support for existing network reuse. In this paper, we introduce HAT-CL, a user-friendly, PyTorch-compatible redesign of the HAT mechanism. HAT-CL not only automates gradient manipulation but also streamlines the transformation of PyTorch modules into HAT modules. It achieves this by providing a comprehensive suite of modules that can be seamlessly integrated into existing architectures. Additionally, HAT-CL offers ready-to-use HAT networks that are smoothly integrated with the TIMM library. Beyond the redesign and reimplementation of HAT, we also introduce novel mask manipulation techniques for HAT, which have consistently shown improvements across various experiments. Our work paves the way for a broader application of the HAT mechanism, opening up new possibilities in continual learning across diverse models and applications.
</details>
<details>
<summary>摘要</summary>
神经网络在学习新任务时会遇到严重的忘却现象，这个现象称为 catastrophic forgetting。这个问题对于 continual learning 造成了一定的挑战。在这篇论文中，我们介绍了一个名为 HAT-CL 的用户友好、PyTorch 相容的 HAT 机制重新设计。HAT-CL 不��ky仅自动调整Gradient，而且可以将 PyTorch 模组转换为 HAT 模组。它实现了这一点通过提供了一个完整的模组套件，可以视需要与现有的架构集成。此外，HAT-CL 还提供了一些专门的 mask 处理技术，这些技术在不同的实验中一直表现出了改善。我们的工作为 continual learning 开启了新的可能性，扩展了不同的模型和应用。
</details></li>
</ul>
<hr>
<h2 id="Application-of-BadNets-in-Spam-Filters"><a href="#Application-of-BadNets-in-Spam-Filters" class="headerlink" title="Application of BadNets in Spam Filters"></a>Application of BadNets in Spam Filters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09649">http://arxiv.org/abs/2307.09649</a></li>
<li>repo_url: None</li>
<li>paper_authors: Swagnik Roychoudhury, Akshaj Kumar Veldanda</li>
<li>for: 保护用户 FROM 不良和危险的电子邮件</li>
<li>methods: 利用机器学习模型攻击 spam 筛选器</li>
<li>results: 显示了机器学习模型供应链中的潜在漏洞，需要谨慎评估和维护 spam 筛选器<details>
<summary>Abstract</summary>
Spam filters are a crucial component of modern email systems, as they help to protect users from unwanted and potentially harmful emails. However, the effectiveness of these filters is dependent on the quality of the machine learning models that power them. In this paper, we design backdoor attacks in the domain of spam filtering. By demonstrating the potential vulnerabilities in the machine learning model supply chain, we highlight the need for careful consideration and evaluation of the models used in spam filters. Our results show that the backdoor attacks can be effectively used to identify vulnerabilities in spam filters and suggest the need for ongoing monitoring and improvement in this area.
</details>
<details>
<summary>摘要</summary>
电子邮件系统中的垃圾邮件过滤器是现代电子邮件系统中的一个重要组件，它们帮助保护用户免受不必要和 potentially 有害的电子邮件。然而，垃圾邮件过滤器的效iveness 是机器学习模型的质量所 decide。在这篇论文中，我们设计了垃圾邮件过滤器中的后门攻击。我们通过示例出现了机器学习模型供应链中的潜在漏洞，并高亮了需要小心评估和评估这些模型。我们的结果表明，后门攻击可以有效地找到垃圾邮件过滤器中的漏洞，并建议进行持续监测和改进。
</details></li>
</ul>
<hr>
<h2 id="Promoting-Exploration-in-Memory-Augmented-Adam-using-Critical-Momenta"><a href="#Promoting-Exploration-in-Memory-Augmented-Adam-using-Critical-Momenta" class="headerlink" title="Promoting Exploration in Memory-Augmented Adam using Critical Momenta"></a>Promoting Exploration in Memory-Augmented Adam using Critical Momenta</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09638">http://arxiv.org/abs/2307.09638</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chandar-lab/cmoptimizer">https://github.com/chandar-lab/cmoptimizer</a></li>
<li>paper_authors: Pranshu Malviya, Gonçalo Mordido, Aristide Baratin, Reza Babanezhad Harikandeh, Jerry Huang, Simon Lacoste-Julien, Razvan Pascanu, Sarath Chandar</li>
<li>for: 提高 Adam 优化器的性能和泛化能力。</li>
<li>methods: 提出了一种基于缓存的 Adam 优化器，通过使用缓存来增强探索较平坦的极值地形，从而提高 Adam 的性能和泛化能力。</li>
<li>results: 经验表明，该方法可以提高多种 Adam 变体在标准的supervised语言模型和图像分类任务中的性能。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Adaptive gradient-based optimizers, particularly Adam, have left their mark in training large-scale deep learning models. The strength of such optimizers is that they exhibit fast convergence while being more robust to hyperparameter choice. However, they often generalize worse than non-adaptive methods. Recent studies have tied this performance gap to flat minima selection: adaptive methods tend to find solutions in sharper basins of the loss landscape, which in turn hurts generalization. To overcome this issue, we propose a new memory-augmented version of Adam that promotes exploration towards flatter minima by using a buffer of critical momentum terms during training. Intuitively, the use of the buffer makes the optimizer overshoot outside the basin of attraction if it is not wide enough. We empirically show that our method improves the performance of several variants of Adam on standard supervised language modelling and image classification tasks.
</details>
<details>
<summary>摘要</summary>
“ adaptive 梯度下降方法，特别是 Adam，在训练大规模深度学习模型方面留下了深刻的影响。这些优化器的优点在于它们具有快速收敛和较好的参数选择不敏感性。然而，它们通常比非适应方法对于泛化性较差。 recent studies 表明，这个性能差距可以归因于扁平的最小值选择：适应方法倾向找到锋角的损失地形上的解，这会对泛化造成负面影响。为了解决这个问题，我们提出了一个新的内存增强版 Adam，使用训练过程中的缓存来增强探索，以便找到更平滑的最小值。我们 empirically 显示，我们的方法可以提高多种 Adam 的 variant 在标准的supervised language modeling 和图像识别 зада务上的性能。”Note that Simplified Chinese is a written language, and the word order and grammar may be different from Traditional Chinese, which is spoken in Taiwan and some other parts of the world.
</details></li>
</ul>
<hr>
<h2 id="Towards-Federated-Foundation-Models-Scalable-Dataset-Pipelines-for-Group-Structured-Learning"><a href="#Towards-Federated-Foundation-Models-Scalable-Dataset-Pipelines-for-Group-Structured-Learning" class="headerlink" title="Towards Federated Foundation Models: Scalable Dataset Pipelines for Group-Structured Learning"></a>Towards Federated Foundation Models: Scalable Dataset Pipelines for Group-Structured Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09619">http://arxiv.org/abs/2307.09619</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research/dataset_grouper">https://github.com/google-research/dataset_grouper</a></li>
<li>paper_authors: Zachary Charles, Nicole Mitchell, Krishna Pillutla, Michael Reneer, Zachary Garrett</li>
<li>for: 这个论文是为了实现大规模 federated learning  simulation，特别是基于基础模型的个性化适应和任务特定的适应。</li>
<li>methods: 这个论文使用了一个名为 Dataset Grouper 的库，可以创建大规模的分组结构化数据集，以便在现有的软件框架中进行 federated learning 模拟。Dataset Grouper 具有三个优势：首先，它可以处理具有很多分组的大规模数据集，而不需要将整个数据集装入内存中。其次，它允许用户根据自己的需求选择基础数据集和分区方式。最后，它是框架无关的。</li>
<li>results: 根据实验结果，使用 Dataset Grouper 可以实现大规模 federated learning 模拟，并且算法如 FedAvg 在这种规模下更像是元学习方法而不是empirical risk minimization方法，这表明它们在下游个性化和任务特定的适应中具有更高的实用性。<details>
<summary>Abstract</summary>
We introduce a library, Dataset Grouper, to create large-scale group-structured (e.g., federated) datasets, enabling federated learning simulation at the scale of foundation models. This library allows the creation of group-structured versions of existing datasets based on user-specified partitions, and directly leads to a variety of useful heterogeneous datasets that can be plugged into existing software frameworks. Dataset Grouper offers three key advantages. First, it scales to settings where even a single group's dataset is too large to fit in memory. Second, it provides flexibility, both in choosing the base (non-partitioned) dataset and in defining partitions. Finally, it is framework-agnostic. We empirically demonstrate that Dataset Grouper allows for large-scale federated language modeling simulations on datasets that are orders of magnitude larger than in previous work. Our experimental results show that algorithms like FedAvg operate more as meta-learning methods than as empirical risk minimization methods at this scale, suggesting their utility in downstream personalization and task-specific adaptation.
</details>
<details>
<summary>摘要</summary>
我们介绍一个库，Dataset Grouper，用于创建大规模群结构化（例如，联邦）数据集，以实现联邦学习的大规模模拟，与基础模型一样规模。这个库允许创建基于用户指定的分区的群结构化版本的现有数据集，直接导致一些灵活的多元数据集，可以插入现有的软件框架。Dataset Grouper具有三个关键优势：首先，它可以扩展到可以在内存中存储的数据集的规模上。其次，它提供了flexibility，可以选择基础（非分区）数据集和定义分区。最后，它是框架无关的。我们经验表明，Dataset Grouper可以实现大规模的联邦语言模型 simulations，比前一个工作中的规模多 orders of magnitude。我们的实验结果表明，在这种规模下，算法如 FedAvg 操作更像是meta-学习方法，而不是empirical risk minimization方法，这表明它们在下游个性化和任务特定适应中的 utility。
</details></li>
</ul>
<hr>
<h2 id="Gradient-strikes-back-How-filtering-out-high-frequencies-improves-explanations"><a href="#Gradient-strikes-back-How-filtering-out-high-frequencies-improves-explanations" class="headerlink" title="Gradient strikes back: How filtering out high frequencies improves explanations"></a>Gradient strikes back: How filtering out high frequencies improves explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09591">http://arxiv.org/abs/2307.09591</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sabine Muzellec, Léo Andéol, Thomas Fel, Rufin VanRullen, Thomas Serre<br>for:这篇论文旨在解释深度神经网络做出决策的原因，特别是对于新的预测基于的解释方法和older的梯度基于的方法之间的比较。methods:这篇论文使用了三种 represntative的视觉分类模型的梯度极值点的分析，以解释这些模型决策的原因。results:研究发现，梯度基于的方法存在高频噪声，而预测基于的方法则具有更好的解释性。通过应用低通滤波器，研究人员提出了一种改进梯度基于的解释方法，并证明了这种方法可以提高解释性。<details>
<summary>Abstract</summary>
Recent years have witnessed an explosion in the development of novel prediction-based attribution methods, which have slowly been supplanting older gradient-based methods to explain the decisions of deep neural networks. However, it is still not clear why prediction-based methods outperform gradient-based ones. Here, we start with an empirical observation: these two approaches yield attribution maps with very different power spectra, with gradient-based methods revealing more high-frequency content than prediction-based methods. This observation raises multiple questions: What is the source of this high-frequency information, and does it truly reflect decisions made by the system? Lastly, why would the absence of high-frequency information in prediction-based methods yield better explainability scores along multiple metrics? We analyze the gradient of three representative visual classification models and observe that it contains noisy information emanating from high-frequencies. Furthermore, our analysis reveals that the operations used in Convolutional Neural Networks (CNNs) for downsampling appear to be a significant source of this high-frequency content -- suggesting aliasing as a possible underlying basis. We then apply an optimal low-pass filter for attribution maps and demonstrate that it improves gradient-based attribution methods. We show that (i) removing high-frequency noise yields significant improvements in the explainability scores obtained with gradient-based methods across multiple models -- leading to (ii) a novel ranking of state-of-the-art methods with gradient-based methods at the top. We believe that our results will spur renewed interest in simpler and computationally more efficient gradient-based methods for explainability.
</details>
<details>
<summary>摘要</summary>
最近几年内，深度神经网络的各种新的预测基于的归因方法在发展中，慢慢地取代了老的梯度基于的方法来解释深度神经网络的决策。然而，还没有得出为什么预测基于的方法超过梯度基于的方法的原因。我们从一个实际观察开始：这两种方法生成的归因地图具有非常不同的功率спект，预测基于的方法生成的地图具有较低的高频信息，而梯度基于的方法生成的地图具有更多的高频信息。这个观察问题提出了多个问题：高频信息的来源是什么，这些信息是否真的反映系统做出的决策？最后，为什么预测基于的方法生成的地图缺乏高频信息，会导致多个维度的解释分数得到改善？我们分析了三个代表性的视觉分类模型的梯度，发现梯度包含各种干扰信息，其中大部分来自高频信息。此外，我们的分析发现，在Convolutional Neural Networks（CNNs）中用于下采样的操作会导致高频干扰信息的产生——建议可能存在干扰作用为基础。我们应用最佳低通滤波器来生成归因地图，并证明了将高频干扰信息除掉可以提高梯度基于的方法的解释分数。我们发现：（i）从高频干扰信息中除掉信息可以提高梯度基于的方法的解释分数，（ii）这些方法在多个模型上的解释分数得到了改善。我们认为，我们的结果将重新启发使用更简单和计算效率更高的梯度基于的方法来解释深度神经网络的决策。
</details></li>
</ul>
<hr>
<h2 id="Self-Compatibility-Evaluating-Causal-Discovery-without-Ground-Truth"><a href="#Self-Compatibility-Evaluating-Causal-Discovery-without-Ground-Truth" class="headerlink" title="Self-Compatibility: Evaluating Causal Discovery without Ground Truth"></a>Self-Compatibility: Evaluating Causal Discovery without Ground Truth</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09552">http://arxiv.org/abs/2307.09552</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philipp M. Faller, Leena Chennuru Vankadara, Atalanti A. Mastakouri, Francesco Locatello, Dominik Janzing</li>
<li>for: This paper is written for those interested in causal discovery and the challenges of evaluating causal discovery algorithms without ground truth.</li>
<li>methods: The paper proposes a novel method for falsifying the output of a causal discovery algorithm in the absence of ground truth, based on the idea of compatibility between causal graphs learned on different subsets of variables.</li>
<li>results: The paper shows that detecting incompatibilities between causal graphs can provide strong evidence for the correctness of the causal model, and can aid in causal model selection.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了讨论 causal discovery 和无真实参照的 causal discovery 算法评估问题而写的。</li>
<li>methods: 论文提出了一种基于不同变量集合上的 causal 图兼容性的方法来证伪 causal discovery 算法的输出，以及这种方法的可行性和有效性。</li>
<li>results: 论文表明，通过检测 causal 图兼容性可以提供强有力的证据，证明 causal 模型的正确性，并可以帮助选择最佳 causal 模型。<details>
<summary>Abstract</summary>
As causal ground truth is incredibly rare, causal discovery algorithms are commonly only evaluated on simulated data. This is concerning, given that simulations reflect common preconceptions about generating processes regarding noise distributions, model classes, and more. In this work, we propose a novel method for falsifying the output of a causal discovery algorithm in the absence of ground truth. Our key insight is that while statistical learning seeks stability across subsets of data points, causal learning should seek stability across subsets of variables. Motivated by this insight, our method relies on a notion of compatibility between causal graphs learned on different subsets of variables. We prove that detecting incompatibilities can falsify wrongly inferred causal relations due to violation of assumptions or errors from finite sample effects. Although passing such compatibility tests is only a necessary criterion for good performance, we argue that it provides strong evidence for the causal models whenever compatibility entails strong implications for the joint distribution. We also demonstrate experimentally that detection of incompatibilities can aid in causal model selection.
</details>
<details>
<summary>摘要</summary>
为了检验 causal discovery 算法的性能，通常只使用 simulated data。这种做法有一定的问题，因为模拟数据可能不准确反映真实的生成过程。在这篇文章中，我们提出了一种新的方法来证明 causal discovery 算法的输出是否正确。我们的关键发现是，在统计学习中，搜索点对点的稳定性，而 causal learning 则应该搜索变量之间的稳定性。我们根据这个发现，我们的方法基于变量之间的兼容性来判断 causal 图是否正确。我们证明，如果检测到兼容性问题，那么可以推翻由于假设或样本效应导致的错误的 causal 关系。虽然通过兼容性测试只是一个必要的条件，但我们认为它提供了强有力的证据，当兼容性意味着变量之间的共同分布时。我们还通过实验表明，检测到兼容性问题可以帮助选择合适的 causal 模型。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-sports-commentary-in-order-to-automatically-recognize-events-and-extract-insights"><a href="#Analyzing-sports-commentary-in-order-to-automatically-recognize-events-and-extract-insights" class="headerlink" title="Analyzing sports commentary in order to automatically recognize events and extract insights"></a>Analyzing sports commentary in order to automatically recognize events and extract insights</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10303">http://arxiv.org/abs/2307.10303</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yanismiraoui/analyzing-sports-commentary-in-order-to-automatically-recognize-events-and-extract-insights">https://github.com/yanismiraoui/analyzing-sports-commentary-in-order-to-automatically-recognize-events-and-extract-insights</a></li>
<li>paper_authors: Yanis Miraoui</li>
<li>for: 这个论文目的是通过不同的自然语言处理技术和方法自动识别体育活动中的主要动作。</li>
<li>methods: 论文使用了多种自然语言处理技术和方法，包括分类和情感分析，对不同来源的直播体育评论进行分析，并将主要动作分类为不同类别。</li>
<li>results: 论文发现，使用 sentiment analysis 可以帮助检测主要动作。<details>
<summary>Abstract</summary>
In this paper, we carefully investigate how we can use multiple different Natural Language Processing techniques and methods in order to automatically recognize the main actions in sports events. We aim to extract insights by analyzing live sport commentaries from different sources and by classifying these major actions into different categories. We also study if sentiment analysis could help detect these main actions.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们仔细研究了如何使用多种自然语言处理技术和方法来自动识别体育活动中的主要动作。我们希望通过分析不同来源的直播体育评论，抽出关键信息，并将这些主要动作分为不同类别。此外，我们还研究了情感分析是否可以帮助检测这些主要动作。
</details></li>
</ul>
<hr>
<h2 id="The-semantic-landscape-paradigm-for-neural-networks"><a href="#The-semantic-landscape-paradigm-for-neural-networks" class="headerlink" title="The semantic landscape paradigm for neural networks"></a>The semantic landscape paradigm for neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09550">http://arxiv.org/abs/2307.09550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shreyas Gokhale</li>
<li>for: 这篇论文旨在提供一种概念和数学框架，用于解释深度神经网络的训练剖析和性能。</li>
<li>methods: 该论文使用了 semantic landscape 概念和数学模型，描述了深度神经网络的训练剖析为图形的交通动力学问题。</li>
<li>results: 研究发现，深度神经网络的训练剖析和性能可以通过Random Walk 和孤立点的统计学来解释，并且这些现象与物理学中的吸附和爆炸现象有关。<details>
<summary>Abstract</summary>
Deep neural networks exhibit a fascinating spectrum of phenomena ranging from predictable scaling laws to the unpredictable emergence of new capabilities as a function of training time, dataset size and network size. Analysis of these phenomena has revealed the existence of concepts and algorithms encoded within the learned representations of these networks. While significant strides have been made in explaining observed phenomena separately, a unified framework for understanding, dissecting, and predicting the performance of neural networks is lacking. Here, we introduce the semantic landscape paradigm, a conceptual and mathematical framework that describes the training dynamics of neural networks as trajectories on a graph whose nodes correspond to emergent algorithms that are instrinsic to the learned representations of the networks. This abstraction enables us to describe a wide range of neural network phenomena in terms of well studied problems in statistical physics. Specifically, we show that grokking and emergence with scale are associated with percolation phenomena, and neural scaling laws are explainable in terms of the statistics of random walks on graphs. Finally, we discuss how the semantic landscape paradigm complements existing theoretical and practical approaches aimed at understanding and interpreting deep neural networks.
</details>
<details>
<summary>摘要</summary>
深度神经网络展现出非常有趣的spectrum，从可预测的尺度法则到由训练时间、数据集大小和网络大小决定的新功能的不可预测出现。对这些现象的分析发现了神经网络学习的表示中隐藏的概念和算法。虽然在解释每个现象 separately 已经做出了很大的进步，但是一个综合的框架 для理解、分解和预测神经网络的性能是缺失的。在这里，我们引入 semantic landscape 概念，它是一种概念和数学框架，用于描述神经网络的训练剖ogram，其节点对应于神经网络学习的表示中隐藏的算法。这种抽象使得可以将各种神经网络现象描述为已知的统计物理问题。具体来说，我们显示了感知和emergence with scale 与聚变现象相关，而神经网络尺度法则可以通过图形的随机游走统计来解释。最后，我们讨论了semantic landscape 概念如何补充现有的理论和实践方法，以便更好地理解和解释深度神经网络。
</details></li>
</ul>
<hr>
<h2 id="DreaMR-Diffusion-driven-Counterfactual-Explanation-for-Functional-MRI"><a href="#DreaMR-Diffusion-driven-Counterfactual-Explanation-for-Functional-MRI" class="headerlink" title="DreaMR: Diffusion-driven Counterfactual Explanation for Functional MRI"></a>DreaMR: Diffusion-driven Counterfactual Explanation for Functional MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09547">http://arxiv.org/abs/2307.09547</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/icon-lab/dreamr">https://github.com/icon-lab/dreamr</a></li>
<li>paper_authors: Hasan Atakan Bedel, Tolga Çukur</li>
<li>for: 本研究旨在提供高特异性、可信度和准确性的fMRI解释方法，以便更好地理解脑响应和认知状态之间的关系。</li>
<li>methods: 本研究使用了 diffusion-driven counterfactual method，称为DreaMR，来生成高特异性、可信度和准确性的fMRI样本。DreaMR使用了分数多阶段蒸发准则来提高抽取效率，并使用了 transformer 架构来考虑长距离空间时间 Context。</li>
<li>results: 对于 neuroimaging 数据集，DreaMR 的实验结果表明，它在比 state-of-the-art counterfactual 方法更高的特异性、可信度和效率下生成样本。<details>
<summary>Abstract</summary>
Deep learning analyses have offered sensitivity leaps in detection of cognitive states from functional MRI (fMRI) measurements across the brain. Yet, as deep models perform hierarchical nonlinear transformations on their input, interpreting the association between brain responses and cognitive states is challenging. Among common explanation approaches for deep fMRI classifiers, attribution methods show poor specificity and perturbation methods show limited plausibility. While counterfactual generation promises to address these limitations, previous methods use variational or adversarial priors that yield suboptimal sample fidelity. Here, we introduce the first diffusion-driven counterfactual method, DreaMR, to enable fMRI interpretation with high specificity, plausibility and fidelity. DreaMR performs diffusion-based resampling of an input fMRI sample to alter the decision of a downstream classifier, and then computes the minimal difference between the original and counterfactual samples for explanation. Unlike conventional diffusion methods, DreaMR leverages a novel fractional multi-phase-distilled diffusion prior to improve sampling efficiency without compromising fidelity, and it employs a transformer architecture to account for long-range spatiotemporal context in fMRI scans. Comprehensive experiments on neuroimaging datasets demonstrate the superior specificity, fidelity and efficiency of DreaMR in sample generation over state-of-the-art counterfactual methods for fMRI interpretation.
</details>
<details>
<summary>摘要</summary>
深度学习分析已经实现了脑功能成像（fMRI）测量中识别认知状态的敏感性大幅提升。然而，由于深度模型对输入数据进行堆叠非线性变换，因此解释脑响应和认知状态之间的关系是困难的。常见的解释方法 для深度fMRI分类器包括负权重分布和挑战性分布，但这些方法具有较低的特点和可信度。在这些限制下，Counterfactual生成技术被提出，可以解决这些问题。在这种情况下，我们介绍了第一个Diffusion驱动的Counterfactual方法，即DreaMR，它可以在高特点和可信度下提供fMRI解释。DreaMR通过对输入fMRI样本进行Diffusion基于扩散的重新抽样，然后计算抽样和原始样本之间的最小差异来进行解释。与传统的Diffusion方法不同，DreaMR利用了一种新的分数多相阶段混合抽样法，以提高抽样效率而不损失可靠性，同时使用了一种变换器架构来考虑fMRI扫描中的长距离空间时间关系。经过了多个neuroimaging数据集的实验，我们发现DreaMR在样本生成方面比前STATE-OF-THE-ART counterfactual方法更高的特点和可信度。
</details></li>
</ul>
<hr>
<h2 id="Can-Neural-Network-Memorization-Be-Localized"><a href="#Can-Neural-Network-Memorization-Be-Localized" class="headerlink" title="Can Neural Network Memorization Be Localized?"></a>Can Neural Network Memorization Be Localized?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09542">http://arxiv.org/abs/2307.09542</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pratyushmaini/localizing-memorization">https://github.com/pratyushmaini/localizing-memorization</a></li>
<li>paper_authors: Pratyush Maini, Michael C. Mozer, Hanie Sedghi, Zachary C. Lipton, J. Zico Kolter, Chiyuan Zhang</li>
<li>for: 本研究探讨了深度过参数网络中 memorization 和通用化的关系，并提出了一种新的 dropout 方法。</li>
<li>methods: 本研究使用了 gradient accounting、layer rewinding 和 retraining 等三种实验来证明，memorization 不仅局限于最后几层，而且可以在不同层的多个 neuron 中发现。</li>
<li>results: 研究发现，memorization 通常局限于模型中很少的 neuron 或通道（大约5），并且可以通过 example-tied dropout 方法来引导模型对特定示例进行 memorization。这种方法可以减少模型对 memorized 示例的准确率从 100% 降至 3%，同时也降低了通用化差距。<details>
<summary>Abstract</summary>
Recent efforts at explaining the interplay of memorization and generalization in deep overparametrized networks have posited that neural networks $\textit{memorize}$ "hard" examples in the final few layers of the model. Memorization refers to the ability to correctly predict on $\textit{atypical}$ examples of the training set. In this work, we show that rather than being confined to individual layers, memorization is a phenomenon confined to a small set of neurons in various layers of the model. First, via three experimental sources of converging evidence, we find that most layers are redundant for the memorization of examples and the layers that contribute to example memorization are, in general, not the final layers. The three sources are $\textit{gradient accounting}$ (measuring the contribution to the gradient norms from memorized and clean examples), $\textit{layer rewinding}$ (replacing specific model weights of a converged model with previous training checkpoints), and $\textit{retraining}$ (training rewound layers only on clean examples). Second, we ask a more generic question: can memorization be localized $\textit{anywhere}$ in a model? We discover that memorization is often confined to a small number of neurons or channels (around 5) of the model. Based on these insights we propose a new form of dropout -- $\textit{example-tied dropout}$ that enables us to direct the memorization of examples to an apriori determined set of neurons. By dropping out these neurons, we are able to reduce the accuracy on memorized examples from $100\%\to3\%$, while also reducing the generalization gap.
</details>
<details>
<summary>摘要</summary>
Recent research has focused on the interplay between memorization and generalization in deep overparametrized networks, suggesting that neural networks memorize "hard" examples in the final few layers of the model. Memorization refers to the ability to correctly predict atypical examples in the training set. In this study, we found that memorization is not limited to individual layers, but rather is confined to a small set of neurons in various layers of the model.First, we provided three lines of evidence to support this claim:1. Gradient accounting: We measured the contribution to the gradient norms from memorized and clean examples, and found that most layers are redundant for memorization, and the layers that contribute to example memorization are not the final layers.2. Layer rewinding: We replaced specific model weights of a converged model with previous training checkpoints, and found that memorization is not limited to the final layers.3. Retraining: We trained rewound layers only on clean examples, and found that memorization is often confined to a small number of neurons or channels (around 5) of the model.Based on these insights, we proposed a new form of dropout called "example-tied dropout" that enables us to direct the memorization of examples to an apriori determined set of neurons. By dropping out these neurons, we were able to reduce the accuracy on memorized examples from 100% to 3%, while also reducing the generalization gap.
</details></li>
</ul>
<hr>
<h2 id="Forecasting-the-steam-mass-flow-in-a-powerplant-using-the-parallel-hybrid-network"><a href="#Forecasting-the-steam-mass-flow-in-a-powerplant-using-the-parallel-hybrid-network" class="headerlink" title="Forecasting the steam mass flow in a powerplant using the parallel hybrid network"></a>Forecasting the steam mass flow in a powerplant using the parallel hybrid network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09483">http://arxiv.org/abs/2307.09483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrii Kurkin, Jonas Hegemann, Mo Kordzanganeh, Alexey Melnikov</li>
<li>for: 提高热电厂的操作效率和成本 reduction</li>
<li>methods: 使用并行混合神经网络架构，结合参数化量子电路和常见的批处理神经网络，进行热质流量预测</li>
<li>results: 相比单独的古典和量子模型，并行混合模型可以更好地预测热质流量15分钟内的趋势，其MSE损失值较低，相对error between ground truth and model predictions also smaller.<details>
<summary>Abstract</summary>
Efficient and sustainable power generation is a crucial concern in the energy sector. In particular, thermal power plants grapple with accurately predicting steam mass flow, which is crucial for operational efficiency and cost reduction. In this study, we use a parallel hybrid neural network architecture that combines a parametrized quantum circuit and a conventional feed-forward neural network specifically designed for time-series prediction in industrial settings to enhance predictions of steam mass flow 15 minutes into the future. Our results show that the parallel hybrid model outperforms standalone classical and quantum models, achieving more than 5.7 and 4.9 times lower mean squared error (MSE) loss on the test set after training compared to pure classical and pure quantum networks, respectively. Furthermore, the hybrid model demonstrates smaller relative errors between the ground truth and the model predictions on the test set, up to 2 times better than the pure classical model. These findings contribute to the broader scientific understanding of how integrating quantum and classical machine learning techniques can be applied to real-world challenges faced by the energy sector, ultimately leading to optimized power plant operations.
</details>
<details>
<summary>摘要</summary>
efficient和可持续的电力生产是能源领域的关键问题。特别是热电厂面临精准预测蒸汽质量流量是操作效率和成本减少的关键。在这项研究中，我们使用并行的混合神经网络架构，将Parametrized Quantum Circuit和常规往返神经网络特性为时间序列预测在工业设置中进行整合，以提高蒸汽质量流量15分钟后的预测。我们的结果表明，并行混合模型比纯经典和量子网络模型更高效，在测试集上培育后的MSE损失下降了5.7和4.9倍，分别比纯经典和量子网络模型更低。此外，混合模型在测试集上与真实值之间的相对误差也更小，达到了2倍于纯经典模型。这些发现对把量子和经典机器学习技术应用于能源领域的实际挑战提供了贡献，ultimately leading to optimized power plant operations.
</details></li>
</ul>
<hr>
<h2 id="Overthinking-the-Truth-Understanding-how-Language-Models-Process-False-Demonstrations"><a href="#Overthinking-the-Truth-Understanding-how-Language-Models-Process-False-Demonstrations" class="headerlink" title="Overthinking the Truth: Understanding how Language Models Process False Demonstrations"></a>Overthinking the Truth: Understanding how Language Models Process False Demonstrations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09476">http://arxiv.org/abs/2307.09476</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dannyallover/overthinking_the_truth">https://github.com/dannyallover/overthinking_the_truth</a></li>
<li>paper_authors: Danny Halawi, Jean-Stanislas Denain, Jacob Steinhardt</li>
<li>for: 研究模型在几拍学习中复制困难任务的能力，并探究模型在复制过程中可能会复制错误或危险内容。</li>
<li>methods: 通过分析模型的内部表示，研究模型在复制过程中可能会出现的两种现象：过度思考和假推导头。</li>
<li>results: 研究发现，在 Early layers 中，正确和错误示例都可以induce相似的模型行为，但是在某个”极限层” 之后，正确示例的准确率逐渐下降，而错误示例的准确率则保持相对高。此外，假推导头可能是过度思考的一个可能机制，它们在 Late layers 中 attend 并复制错误信息，并且它们的消除可以减少过度思考。<details>
<summary>Abstract</summary>
Modern language models can imitate complex patterns through few-shot learning, enabling them to complete challenging tasks without fine-tuning. However, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context. We study harmful imitation through the lens of a model's internal representations, and identify two related phenomena: overthinking and false induction heads. The first phenomenon, overthinking, appears when we decode predictions from intermediate layers, given correct vs. incorrect few-shot demonstrations. At early layers, both demonstrations induce similar model behavior, but the behavior diverges sharply at some "critical layer", after which the accuracy given incorrect demonstrations progressively decreases. The second phenomenon, false induction heads, are a possible mechanistic cause of overthinking: these are heads in late layers that attend to and copy false information from previous demonstrations, and whose ablation reduces overthinking. Beyond scientific understanding, our results suggest that studying intermediate model computations could be a promising avenue for understanding and guarding against harmful model behaviors.
</details>
<details>
<summary>摘要</summary>
现代语言模型可以通过几招学习模式来模仿复杂的模式，从而完成复杂任务 без fine-tuning。然而，模仿也可能导致模型复制错误或有害内容，如果在上下文中存在。我们通过模型内部表示的视角来研究危险的模仿行为，并发现了两种相关的现象：过度思考和假推导头。首先，过度思考发生在在中间层次处decode预测结果时，当给出正确和错误几招示范时。在早期层次上，两个示范都会导致模型的行为相似，但是行为在某个"极限层"之后开始分别下降，并且在这个层次上预测错误的情况下，精度逐渐下降。其次，假推导头是可能导致过度思考的机制之一：这些是在后期层次上的头部，它们会从前一个示范中复制错误信息，并且它们的除去可以减少过度思考。我们的研究结果表明，研究模型的中间计算可能是了解和防止模型危险行为的有效途径。
</details></li>
</ul>
<hr>
<h2 id="A-Cryogenic-Memristive-Neural-Decoder-for-Fault-tolerant-Quantum-Error-Correction"><a href="#A-Cryogenic-Memristive-Neural-Decoder-for-Fault-tolerant-Quantum-Error-Correction" class="headerlink" title="A Cryogenic Memristive Neural Decoder for Fault-tolerant Quantum Error Correction"></a>A Cryogenic Memristive Neural Decoder for Fault-tolerant Quantum Error Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09463">http://arxiv.org/abs/2307.09463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frédéric Marcotte, Pierre-Antoine Mouny, Victor Yon, Gebremedhin A. Dagnew, Bohdan Kulchytskyy, Sophie Rochette, Yann Beilliard, Dominique Drouin, Pooya Ronagh</li>
<li>for: 这个论文主要target是为了提出一种基于卷积 neural network的量子错误修复（QEC）方法，以提高量子计算机系统的可靠性和稳定性。</li>
<li>methods: 这个论文使用了一种基于卷积 neural network的扩充器来加速量子错误修复的幂等处理，并使用了一种卷积 память设备来实现分形 matrix-vector 乘法。</li>
<li>results: 研究人员通过数值实验和实际测量来 investigate the impact of TiO$_\textrm{x}$-based memristive devices’ non-idealities on decoding accuracy,并开发了一种硬件意识的训练方法来减轻这些影响，以实现一种 Pseudo-threshold 值为 $9.23\times 10^{-4}$。<details>
<summary>Abstract</summary>
Neural decoders for quantum error correction (QEC) rely on neural networks to classify syndromes extracted from error correction codes and find appropriate recovery operators to protect logical information against errors. Despite the good performance of neural decoders, important practical requirements remain to be achieved, such as minimizing the decoding time to meet typical rates of syndrome generation in repeated error correction schemes, and ensuring the scalability of the decoding approach as the code distance increases. Designing a dedicated integrated circuit to perform the decoding task in co-integration with a quantum processor appears necessary to reach these decoding time and scalability requirements, as routing signals in and out of a cryogenic environment to be processed externally leads to unnecessary delays and an eventual wiring bottleneck. In this work, we report the design and performance analysis of a neural decoder inference accelerator based on an in-memory computing (IMC) architecture, where crossbar arrays of resistive memory devices are employed to both store the synaptic weights of the decoder neural network and perform analog matrix-vector multiplications during inference. In proof-of-concept numerical experiments supported by experimental measurements, we investigate the impact of TiO$_\textrm{x}$-based memristive devices' non-idealities on decoding accuracy. Hardware-aware training methods are developed to mitigate the loss in accuracy, allowing the memristive neural decoders to achieve a pseudo-threshold of $9.23\times 10^{-4}$ for the distance-three surface code, whereas the equivalent digital neural decoder achieves a pseudo-threshold of $1.01\times 10^{-3}$. This work provides a pathway to scalable, fast, and low-power cryogenic IMC hardware for integrated QEC.
</details>
<details>
<summary>摘要</summary>
neural decoders for quantum error correction (QEC) 利用神经网络来分类错误码和找到适当的恢复操作器，以保护逻辑信息免受错误的影响。 despite the good performance of neural decoders, there are still important practical requirements to be met, such as reducing the decoding time to meet the typical rates of syndrome generation in repeated error correction schemes, and ensuring the scalability of the decoding approach as the code distance increases. 设计专门的集成电路来执行解码任务和Quantum Processor co-integration appears necessary to reach these decoding time and scalability requirements, as routing signals in and out of a cryogenic environment to be processed externally leads to unnecessary delays and an eventual wiring bottleneck.In this work, we report the design and performance analysis of a neural decoder inference accelerator based on an in-memory computing (IMC) architecture, where crossbar arrays of resistive memory devices are employed to both store the synaptic weights of the decoder neural network and perform analog matrix-vector multiplications during inference. In proof-of-concept numerical experiments supported by experimental measurements, we investigate the impact of TiO$_\textrm{x}$-based memristive devices' non-idealities on decoding accuracy. Hardware-aware training methods are developed to mitigate the loss in accuracy, allowing the memristive neural decoders to achieve a pseudo-threshold of $9.23\times 10^{-4}$ for the distance-three surface code, whereas the equivalent digital neural decoder achieves a pseudo-threshold of $1.01\times 10^{-3}$. This work provides a pathway to scalable, fast, and low-power cryogenic IMC hardware for integrated QEC.
</details></li>
</ul>
<hr>
<h2 id="Does-Circuit-Analysis-Interpretability-Scale-Evidence-from-Multiple-Choice-Capabilities-in-Chinchilla"><a href="#Does-Circuit-Analysis-Interpretability-Scale-Evidence-from-Multiple-Choice-Capabilities-in-Chinchilla" class="headerlink" title="Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla"></a>Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09458">http://arxiv.org/abs/2307.09458</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tom Lieberum, Matthew Rahtz, János Kramár, Neel Nanda, Geoffrey Irving, Rohin Shah, Vladimir Mikulik</li>
<li>for: This paper aims to test the scalability of circuit analysis on a large language model (Chinchilla) and understand the internal mechanisms of the model’s correct answer identification.</li>
<li>methods: The paper uses existing techniques such as logit attribution, attention pattern visualization, and activation patching to analyze the model’s behavior, and applies these techniques to the 70B Chinchilla model.</li>
<li>results: The paper finds that the existing techniques can be applied to Chinchilla with mixed results, and identifies a small set of <code>output nodes&#39; (attention heads and MLPs) that are responsible for correct answer identification. Additionally, the paper shows that the query and key subspaces of the </code>correct letter’ heads represent an &#96;Nth item in an enumeration’ feature, but this explanation is only partial and there is more to learn about the heads’ behavior on a more general distribution.<details>
<summary>Abstract</summary>
\emph{Circuit analysis} is a promising technique for understanding the internal mechanisms of language models. However, existing analyses are done in small models far from the state of the art. To address this, we present a case study of circuit analysis in the 70B Chinchilla model, aiming to test the scalability of circuit analysis. In particular, we study multiple-choice question answering, and investigate Chinchilla's capability to identify the correct answer \emph{label} given knowledge of the correct answer \emph{text}. We find that the existing techniques of logit attribution, attention pattern visualization, and activation patching naturally scale to Chinchilla, allowing us to identify and categorize a small set of `output nodes' (attention heads and MLPs).   We further study the `correct letter' category of attention heads aiming to understand the semantics of their features, with mixed results. For normal multiple-choice question answers, we significantly compress the query, key and value subspaces of the head without loss of performance when operating on the answer labels for multiple-choice questions, and we show that the query and key subspaces represent an `Nth item in an enumeration' feature to at least some extent. However, when we attempt to use this explanation to understand the heads' behaviour on a more general distribution including randomized answer labels, we find that it is only a partial explanation, suggesting there is more to learn about the operation of `correct letter' heads on multiple choice question answering.
</details>
<details>
<summary>摘要</summary>
\emph{电路分析} 是一种有前途的技术，用于理解语言模型的内部机制。然而，现有的分析都是在小型模型上进行的，远离当前领域的状态。为了解决这个问题，我们提出了一个案例研究，在70B Chinchilla模型中进行电路分析。具体来说，我们研究了多选题回答问题，并研究了 Chinchilla 是否可以根据正确的答案文本来确定正确的答案标签。我们发现，现有的逻辑拟合、注意力图像化和活动覆盖技术可以自然地扩展到 Chinchilla，allowing us to identify和分类一小组`输出节点'（注意头和 MLP）。我们进一步研究了 `正确字符' 类型的注意头，以了解其特征semantics。结果表明，对于常见的多选题答案，我们可以压缩查询、键和值子空间中的头部分，无损性能。此外，我们发现，查询和键子空间代表了 `N个元素的排序' 特征，至少在一定程度上。然而，当我们尝试使用这种解释来理解 `正确字符' 头的行为时，我们发现这只是一个 partial explanation， suggesting there is more to learn about the operation of `correct letter' heads on multiple choice question answering.
</details></li>
</ul>
<hr>
<h2 id="Smooth-Attention-for-Deep-Multiple-Instance-Learning-Application-to-CT-Intracranial-Hemorrhage-Detection"><a href="#Smooth-Attention-for-Deep-Multiple-Instance-Learning-Application-to-CT-Intracranial-Hemorrhage-Detection" class="headerlink" title="Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection"></a>Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09457">http://arxiv.org/abs/2307.09457</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yunanwu2168/sa-mil">https://github.com/yunanwu2168/sa-mil</a></li>
<li>paper_authors: Yunan Wu, Francisco M. Castro-Macías, Pablo Morales-Álvarez, Rafael Molina, Aggelos K. Katsaggelos</li>
<li>for: 这个研究旨在提出一种基于深度学习的多个实例学习（MIL）模型，以优化医疗图像诊断中的bag label unknown instance label问题。</li>
<li>methods: 该模型使用了smooth attention mechanism来保证每个实例在bag中受到的注意程度是相似的，从而学习了bag中实例之间的 spatial dependencies。</li>
<li>results: 对于头CT扫描图像中的脑内出血检测，该模型比非smooth attention MIL模型在scan和slice水平上都有更好的表现，并且超越了当前的state-of-the-art MIL方法。<details>
<summary>Abstract</summary>
Multiple Instance Learning (MIL) has been widely applied to medical imaging diagnosis, where bag labels are known and instance labels inside bags are unknown. Traditional MIL assumes that instances in each bag are independent samples from a given distribution. However, instances are often spatially or sequentially ordered, and one would expect similar diagnostic importance for neighboring instances. To address this, in this study, we propose a smooth attention deep MIL (SA-DMIL) model. Smoothness is achieved by the introduction of first and second order constraints on the latent function encoding the attention paid to each instance in a bag. The method is applied to the detection of intracranial hemorrhage (ICH) on head CT scans. The results show that this novel SA-DMIL: (a) achieves better performance than the non-smooth attention MIL at both scan (bag) and slice (instance) levels; (b) learns spatial dependencies between slices; and (c) outperforms current state-of-the-art MIL methods on the same ICH test set.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Convergent-regularization-in-inverse-problems-and-linear-plug-and-play-denoisers"><a href="#Convergent-regularization-in-inverse-problems-and-linear-plug-and-play-denoisers" class="headerlink" title="Convergent regularization in inverse problems and linear plug-and-play denoisers"></a>Convergent regularization in inverse problems and linear plug-and-play denoisers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09441">http://arxiv.org/abs/2307.09441</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andreas Hauptmann, Subhadip Mukherjee, Carola-Bibiane Schönlieb, Ferdia Sherry</li>
<li>for: 这篇论文的目的是研究插入式减噪（PnP）方法在图像反问题中的收敛性和稳定性。</li>
<li>methods: 这篇论文使用了经典的正则化理论和最近的数据驱动方法来研究PnP方法的收敛性。</li>
<li>results: 该论文提出了一种新的spectral filtering技术来控制PnP方法中的正则化强度，并证明了PnP方法在linear denoiser情况下是一种收敛的正则化方案。<details>
<summary>Abstract</summary>
Plug-and-play (PnP) denoising is a popular iterative framework for solving imaging inverse problems using off-the-shelf image denoisers. Their empirical success has motivated a line of research that seeks to understand the convergence of PnP iterates under various assumptions on the denoiser. While a significant amount of research has gone into establishing the convergence of the PnP iteration for different regularity conditions on the denoisers, not much is known about the asymptotic properties of the converged solution as the noise level in the measurement tends to zero, i.e., whether PnP methods are provably convergent regularization schemes under reasonable assumptions on the denoiser. This paper serves two purposes: first, we provide an overview of the classical regularization theory in inverse problems and survey a few notable recent data-driven methods that are provably convergent regularization schemes. We then continue to discuss PnP algorithms and their established convergence guarantees. Subsequently, we consider PnP algorithms with linear denoisers and propose a novel spectral filtering technique to control the strength of regularization arising from the denoiser. Further, by relating the implicit regularization of the denoiser to an explicit regularization functional, we rigorously show that PnP with linear denoisers leads to a convergent regularization scheme. More specifically, we prove that in the limit as the noise vanishes, the PnP reconstruction converges to the minimizer of a regularization potential subject to the solution satisfying the noiseless operator equation. The theoretical analysis is corroborated by numerical experiments for the classical inverse problem of tomographic image reconstruction.
</details>
<details>
<summary>摘要</summary>
固定-插入（PnP）去噪是一种流行的迭代框架，用于解决图像逆问题。它们的实际成功激发了一条研究，旨在理解PnP迭代的收敛性，以及其解决图像逆问题的效果。虽然一些研究已经证明了PnP迭代的收敛性，但尚不多了关于随着测量噪声水平的下降，解决图像逆问题的稳定性和准确性的研究。这篇论文旨在两个目的：首先，提供经典的反射理论的概述，以及一些近期的数据驱动方法，这些方法是已知的收敛性规则。然后，我们继续讨论PnP算法，并证明它们的收敛性保证。接着，我们考虑PnP算法与线性去噪器的结合，并提出一种新的频谱筛选技术，用于控制由去噪器带来的正则化强度。最后，我们正式证明PnP算法与线性去噪器的结合是一种收敛的正则化方案，并且在随着噪声水平下降，PnP重建结果会 converges到一个具有正则化潜在能量的最小值解。实际分析证明了这一结论的正确性。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Conditional-Slot-Attention-for-Object-Centric-Learning"><a href="#Unsupervised-Conditional-Slot-Attention-for-Object-Centric-Learning" class="headerlink" title="Unsupervised Conditional Slot Attention for Object Centric Learning"></a>Unsupervised Conditional Slot Attention for Object Centric Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09437">http://arxiv.org/abs/2307.09437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Avinash Kori, Francesco Locatello, Francesca Toni, Ben Glocker</li>
<li>for: 本研究旨在提出一种不需要监督的方法，以学习对象中心表示，并解决多个对象实例绑定到专门的对象槽中的问题。</li>
<li>methods: 本研究使用了iterative attention来学习可组合的表示，并使用了一个新的概率槽词典（PSD）来实现特定对象槽绑定。</li>
<li>results: 研究发现，使用我们提出的方法可以在多个下游任务中提供场景组合能力，并在一些几个步骤适应性Visual reasoning任务中提供了显著的提升，同时与槽注意力在对象发现任务中表现相似或更好。<details>
<summary>Abstract</summary>
Extracting object-level representations for downstream reasoning tasks is an emerging area in AI. Learning object-centric representations in an unsupervised setting presents multiple challenges, a key one being binding an arbitrary number of object instances to a specialized object slot. Recent object-centric representation methods like Slot Attention utilize iterative attention to learn composable representations with dynamic inference level binding but fail to achieve specialized slot level binding. To address this, in this paper we propose Unsupervised Conditional Slot Attention using a novel Probabilistic Slot Dictionary (PSD). We define PSD with (i) abstract object-level property vectors as key and (ii) parametric Gaussian distribution as its corresponding value. We demonstrate the benefits of the learnt specific object-level conditioning distributions in multiple downstream tasks, namely object discovery, compositional scene generation, and compositional visual reasoning. We show that our method provides scene composition capabilities and a significant boost in a few shot adaptability tasks of compositional visual reasoning, while performing similarly or better than slot attention in object discovery tasks
</details>
<details>
<summary>摘要</summary>
“抽取对象级别表示是人工智能领域的一个emerging领域。在无监督的setting下学习对象中心的表示呈poses多个挑战，其中一个关键问题是将多个对象实例绑定到特殊的对象槽。现有的对象中心表示方法如Slot Attention使用迭代注意力学习可组合表示，但它们无法实现特殊槽级别的绑定。为解决这个问题，本文提出了无监督条件槽注意力（UCSD），使用一种新的概率槽词典（PSD）。我们定义PSD中的键为抽取对象级别性质 вектор，值为参数化的高斯分布。我们示出了学习的特定对象级别conditioning分布的多个下游任务的好处，包括对象发现、compositional scene generation和compositional visual reasoning。我们表明了我们的方法在多个shot adaptability任务中提供了场景组合能力，并在对象发现任务中与槽注意力相当或更好的表现”
</details></li>
</ul>
<hr>
<h2 id="Scaling-Laws-for-Imitation-Learning-in-NetHack"><a href="#Scaling-Laws-for-Imitation-Learning-in-NetHack" class="headerlink" title="Scaling Laws for Imitation Learning in NetHack"></a>Scaling Laws for Imitation Learning in NetHack</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09423">http://arxiv.org/abs/2307.09423</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jens Tuyls, Dhruv Madeka, Kari Torkkola, Dean Foster, Karthik Narasimhan, Sham Kakade</li>
<li>for: 这个论文旨在研究 whether carefully scaling up model and data size can bring improvements in the imitation learning setting.</li>
<li>methods: 作者使用了 Inspired by recent work in Natural Language Processing (NLP) where “scaling up” has resulted in increasingly more capable LLMs, 并在 NetHack 游戏中进行了实验。</li>
<li>results: 作者发现 IL 损失和 mean return 与 compute budget 相关， resulting in power laws for training compute-optimal IL agents with respect to model size and number of samples。 In addition, 作者发现这些 agent 在所有设置下都比之前的状态艺术高出至少 2x。<details>
<summary>Abstract</summary>
Imitation Learning (IL) is one of the most widely used methods in machine learning. Yet, while powerful, many works find it is often not able to fully recover the underlying expert behavior. However, none of these works deeply investigate the role of scaling up the model and data size. Inspired by recent work in Natural Language Processing (NLP) where "scaling up" has resulted in increasingly more capable LLMs, we investigate whether carefully scaling up model and data size can bring similar improvements in the imitation learning setting. To demonstrate our findings, we focus on the game of NetHack, a challenging environment featuring procedural generation, stochasticity, long-term dependencies, and partial observability. We find IL loss and mean return scale smoothly with the compute budget and are strongly correlated, resulting in power laws for training compute-optimal IL agents with respect to model size and number of samples. We forecast and train several NetHack agents with IL and find they outperform prior state-of-the-art by at least 2x in all settings. Our work both demonstrates the scaling behavior of imitation learning in a challenging domain, as well as the viability of scaling up current approaches for increasingly capable agents in NetHack, a game that remains elusively hard for current AI systems.
</details>
<details>
<summary>摘要</summary>
copied text模仿学习（IL）是机器学习中最广泛使用的方法之一。然而，虽然强大，但许多研究发现它经常无法完全回归到专家行为之下。然而，这些研究并没有深入调查模型和数据集大小的扩大对IL的影响。 inspirited by recent work in自然语言处理（NLP），我们调查了在IL Setting中，是否可以通过扩大模型和数据集大小来获得类似的改进。为了证明我们的发现，我们在NetHack游戏中做了实验，这是一个复杂的环境，具有过程生成、随机性、长期依赖和部分可见性。我们发现IL损失和平均回报与计算预算成直线关系，并且存在计算优化IL代理的强力法律。我们预测和训练了一些NetHack代理，并发现它们在所有设置下都高于之前的状态艺术。我们的工作不仅证明了IL在复杂环境中的扩大行为，还证明了可以通过扩大当前方法来创建更有能力的NetHack代理。
</details></li>
</ul>
<hr>
<h2 id="Causality-oriented-robustness-exploiting-general-additive-interventions"><a href="#Causality-oriented-robustness-exploiting-general-additive-interventions" class="headerlink" title="Causality-oriented robustness: exploiting general additive interventions"></a>Causality-oriented robustness: exploiting general additive interventions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10299">http://arxiv.org/abs/2307.10299</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xwshen51/drig">https://github.com/xwshen51/drig</a></li>
<li>paper_authors: Xinwei Shen, Peter Bühlmann, Armeen Taeb</li>
<li>for: 这篇论文的目的是为了提出一种robust prediction模型，可以在实际应用中面对分布差异时提供更好的预测性。</li>
<li>methods: 这篇论文使用了 causality-oriented robustness的方法，即 Distributional Robustness via Invariant Gradients (DRIG)，它在训练资料中使用通用的加法性改变来获得对于未见到的改变的Robust预测。</li>
<li>results: 论文证明了 DRIG 在线性设定下可以获得robust预测，并且显示了该方法可以在不同的挑战中提供更好的预测性。此外，论文还显示了该方法可以在半监督学习设定下进一步改善预测性。最后，论文通过synthetic simulations和单元细胞数据的实验验证了该方法的有效性。<details>
<summary>Abstract</summary>
Since distribution shifts are common in real-world applications, there is a pressing need for developing prediction models that are robust against such shifts. Existing frameworks, such as empirical risk minimization or distributionally robust optimization, either lack generalizability for unseen distributions or rely on postulated distance measures. Alternatively, causality offers a data-driven and structural perspective to robust predictions. However, the assumptions necessary for causal inference can be overly stringent, and the robustness offered by such causal models often lacks flexibility. In this paper, we focus on causality-oriented robustness and propose Distributional Robustness via Invariant Gradients (DRIG), a method that exploits general additive interventions in training data for robust predictions against unseen interventions, and naturally interpolates between in-distribution prediction and causality. In a linear setting, we prove that DRIG yields predictions that are robust among a data-dependent class of distribution shifts. Furthermore, we show that our framework includes anchor regression (Rothenh\"ausler et al.\ 2021) as a special case, and that it yields prediction models that protect against more diverse perturbations. We extend our approach to the semi-supervised domain adaptation setting to further improve prediction performance. Finally, we empirically validate our methods on synthetic simulations and on single-cell data.
</details>
<details>
<summary>摘要</summary>
因为分布Shift是实际应用中的常见现象，因此有一个急需要开发对不visible distribution的预测模型，这些模型需要对不可见的分布进行Robust预测。现有的框架，如empirical risk minimization或distributionally robust optimization，或者lack普遍性 для未见分布，或者基于假设的距离度量。 Alternatively， causality 提供了数据驱动和结构性的预测Robustness。然而， causal inference 的假设可能过于严格，而且Robustness 提供的灵活性不够。在这篇论文中，我们关注 causality-oriented Robustness，并提出 Distributional Robustness via Invariant Gradients (DRIG)，这种方法利用训练数据中的通用加itive intervention来预测未见 intervention 的Robust预测，并自然地在 in-distribution prediction 和 causality 之间进行 interpolate。在线性设定下，我们证明 DRIG 的预测是对 data-dependent 类型的分布Shift 的Robust。此外，我们显示了我们的框架包含 anchor regression (Rothenh\"ausler et al.\ 2021) 的特殊情况，并且它的预测模型可以保护对更多的扰动。我们将我们的方法扩展到 semi-supervised domain adaptation 设定，以进一步改进预测性能。最后，我们在 Synthetic simulations 和 single-cell data 上进行了实验验证。
</details></li>
</ul>
<hr>
<h2 id="Online-Learning-with-Costly-Features-in-Non-stationary-Environments"><a href="#Online-Learning-with-Costly-Features-in-Non-stationary-Environments" class="headerlink" title="Online Learning with Costly Features in Non-stationary Environments"></a>Online Learning with Costly Features in Non-stationary Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09388">http://arxiv.org/abs/2307.09388</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/saeedghoorchian/ncc-bandits">https://github.com/saeedghoorchian/ncc-bandits</a></li>
<li>paper_authors: Saeed Ghoorchian, Evgenii Kortukov, Setareh Maghsudi</li>
<li>for: maximizing long-term rewards in sequential decision-making problems</li>
<li>methods: extending the contextual bandit setting to observe subsets of features’ states</li>
<li>results: sublinear regret guarantee and superior performance in a real-world scenario<details>
<summary>Abstract</summary>
Maximizing long-term rewards is the primary goal in sequential decision-making problems. The majority of existing methods assume that side information is freely available, enabling the learning agent to observe all features' states before making a decision. In real-world problems, however, collecting beneficial information is often costly. That implies that, besides individual arms' reward, learning the observations of the features' states is essential to improve the decision-making strategy. The problem is aggravated in a non-stationary environment where reward and cost distributions undergo abrupt changes over time. To address the aforementioned dual learning problem, we extend the contextual bandit setting and allow the agent to observe subsets of features' states. The objective is to maximize the long-term average gain, which is the difference between the accumulated rewards and the paid costs on average. Therefore, the agent faces a trade-off between minimizing the cost of information acquisition and possibly improving the decision-making process using the obtained information. To this end, we develop an algorithm that guarantees a sublinear regret in time. Numerical results demonstrate the superiority of our proposed policy in a real-world scenario.
</details>
<details>
<summary>摘要</summary>
最大化长期收益是Sequential decision-making问题的主要目标。现有的大多数方法假设可以免费获得侧 информация，允许学习机器人在做出决策之前观察所有特征状态。然而，在实际问题中，收集有利信息可能是成本的。这意味着， besides 个体武器的奖励，学习特征状态的观察也是重要的，以提高决策策略。这个问题在非站点环境中更加严重，因为奖励和成本分布在时间上发生了快速变化。为解决上述双学习问题，我们将Contextual bandit设置扩展，允许机器人观察特征状态的子集。目标是最大化长期均值收益，即收益和付出的均值差。因此，机器人面临一种折衔决策，即最小化信息收集成本，可能通过获得的信息改善决策过程。为此，我们开发了一个 garantía sublinear regret in time的算法。实际结果表明我们提出的策略在实际场景中具有优势。
</details></li>
</ul>
<hr>
<h2 id="Batched-Predictors-Generalize-within-Distribution"><a href="#Batched-Predictors-Generalize-within-Distribution" class="headerlink" title="Batched Predictors Generalize within Distribution"></a>Batched Predictors Generalize within Distribution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09379">http://arxiv.org/abs/2307.09379</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andreas Loukas, Pan Kessel</li>
<li>for: 该论文研究批处理预测器的通用性 Properties，即模型用于预测小组例的平均标签。</li>
<li>methods: 该论文使用适当的总化带性质来证明批处理预测器的泛化保证更强，与标准每个样本预测方法相比，批处理预测器的泛化保证会加速指数增长。</li>
<li>results: 该论文通过实验 validate theoretically proved that batched predictors have exponentially stronger generalization guarantees than the standard per-sample approach, and the proposed bound holds independently of overparametrization.<details>
<summary>Abstract</summary>
We study the generalization properties of batched predictors, i.e., models tasked with predicting the mean label of a small set (or batch) of examples. The batched prediction paradigm is particularly relevant for models deployed to determine the quality of a group of compounds in preparation for offline testing. By utilizing a suitable generalization of the Rademacher complexity, we prove that batched predictors come with exponentially stronger generalization guarantees as compared to the standard per-sample approach. Surprisingly, the proposed bound holds independently of overparametrization. Our theoretical insights are validated experimentally for various tasks, architectures, and applications.
</details>
<details>
<summary>摘要</summary>
我们研究批处理预测器的通用性特性，即用于预测一小集（或批）示例的模型。批处理预测方式在准备进行线上测试时对模型 particulary relevanter。我们利用适当的总体化Rademacher复杂度，证明批处理预测器在标准每个样本预测方法相比具有ексиponentially强的泛化保证。意外的是，我们的理论发现不受过参数化的影响。我们通过实验 validate our theoretical insights for various tasks, architectures, and applications.Note: "批处理" (batched) in Chinese is usually translated as "批量处理" (batch processing), but in this context, I used "批处理预测器" (batched predictor) to emphasize the prediction aspect.
</details></li>
</ul>
<hr>
<h2 id="Data-Cross-Segmentation-for-Improved-Generalization-in-Reinforcement-Learning-Based-Algorithmic-Trading"><a href="#Data-Cross-Segmentation-for-Improved-Generalization-in-Reinforcement-Learning-Based-Algorithmic-Trading" class="headerlink" title="Data Cross-Segmentation for Improved Generalization in Reinforcement Learning Based Algorithmic Trading"></a>Data Cross-Segmentation for Improved Generalization in Reinforcement Learning Based Algorithmic Trading</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09377">http://arxiv.org/abs/2307.09377</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vikram Duvvur, Aashay Mehta, Edward Sun, Bo Wu, Ken Yew Chan, Jeff Schneider</li>
<li>for: 这篇论文主要是为了探讨用机器学习技术在算法交易系统中的应用，以及如何在贱liquid的资产市场和 differentiated assets 市场中实现更好的交易result。</li>
<li>methods: 这篇论文提出了一种基于强化学习（RL）算法的交易策略，使用学习的预测模型生成交易信号，并 Addresses the challenges of thinly traded financial markets and differentiated assets markets。</li>
<li>results: 在 tested on 20+ years of equity data from Bursa Malaysia，RL algorithm 能够在不同的市场情况下提供更好的交易result，包括减少交易成本和提高投资回报。<details>
<summary>Abstract</summary>
The use of machine learning in algorithmic trading systems is increasingly common. In a typical set-up, supervised learning is used to predict the future prices of assets, and those predictions drive a simple trading and execution strategy. This is quite effective when the predictions have sufficient signal, markets are liquid, and transaction costs are low. However, those conditions often do not hold in thinly traded financial markets and markets for differentiated assets such as real estate or vehicles. In these markets, the trading strategy must consider the long-term effects of taking positions that are relatively more difficult to change. In this work, we propose a Reinforcement Learning (RL) algorithm that trades based on signals from a learned predictive model and addresses these challenges. We test our algorithm on 20+ years of equity data from Bursa Malaysia.
</details>
<details>
<summary>摘要</summary>
machine learning在算法交易系统中的使用越来越普遍。在一般情况下，监督学习用于预测未来资产价格，并且这些预测驱动简单的交易和执行策略。这很有效果当预测具有足够的信号，市场活跃，交易成本低。然而，这些条件frequently不成立在稀采资产市场和不同化资产市场，如房地产或车辆。在这些市场中，交易策略必须考虑持有position的长期影响。在这种情况下，我们提出一种奖励学习（RL）算法，基于学习的预测信号来交易。我们在马来西亚证券交易所20多年的股票数据上测试了我们的算法。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://nullscc.github.io/2023/07/19/cs.LG_2023_07_19/" data-id="cllshxso900212u88biorelpu" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/20/eess.IV_2023_07_20/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.IV - 2023-07-20 17:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/19/cs.SD_2023_07_19/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-07-19 123:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'', root:''}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
