
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-07-23 123:00:00 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="A meta learning scheme for fast accent domain expansion in Mandarin speech recognition paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.12262 repo_url: None paper_authors: Ziwei Zhu, Changhao Shan, Bihong Zhang,">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-07-23 123:00:00">
<meta property="og:url" content="http://example.com/2023/07/23/cs.SD_2023_07_23/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="A meta learning scheme for fast accent domain expansion in Mandarin speech recognition paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.12262 repo_url: None paper_authors: Ziwei Zhu, Changhao Shan, Bihong Zhang,">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-22T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:36.920Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_07_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/23/cs.SD_2023_07_23/" class="article-date">
  <time datetime="2023-07-22T16:00:00.000Z" itemprop="datePublished">2023-07-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-07-23 123:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-meta-learning-scheme-for-fast-accent-domain-expansion-in-Mandarin-speech-recognition"><a href="#A-meta-learning-scheme-for-fast-accent-domain-expansion-in-Mandarin-speech-recognition" class="headerlink" title="A meta learning scheme for fast accent domain expansion in Mandarin speech recognition"></a>A meta learning scheme for fast accent domain expansion in Mandarin speech recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12262">http://arxiv.org/abs/2307.12262</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziwei Zhu, Changhao Shan, Bihong Zhang, Jian Yu</li>
<li>for: 这篇论文的目的是实现中文自动识别（ASR）中的腔调领域扩展，并且不会对中文ASR的性能造成损害。</li>
<li>methods: 这篇论文使用了元学习技术来实现快速的腔调领域扩展，这种技术可以在多个领域中学习通用的关系，而不是仅对特定领域进行适材化。</li>
<li>results: 这篇论文的方法在腔调领域扩展任务中表现出色，相比基准模型，它的改进率为37%，并且在大量数据下也显示了4%的改进率。<details>
<summary>Abstract</summary>
Spoken languages show significant variation across mandarin and accent. Despite the high performance of mandarin automatic speech recognition (ASR), accent ASR is still a challenge task. In this paper, we introduce meta-learning techniques for fast accent domain expansion in mandarin speech recognition, which expands the field of accents without deteriorating the performance of mandarin ASR. Meta-learning or learn-to-learn can learn general relation in multi domains not only for over-fitting a specific domain. So we select meta-learning in the domain expansion task. This more essential learning will cause improved performance on accent domain extension tasks. We combine the methods of meta learning and freeze of model parameters, which makes the recognition performance more stable in different cases and the training faster about 20%. Our approach significantly outperforms other methods about 3% relatively in the accent domain expansion task. Compared to the baseline model, it improves relatively 37% under the condition that the mandarin test set remains unchanged. In addition, it also proved this method to be effective on a large amount of data with a relative performance improvement of 4% on the accent test set.
</details>
<details>
<summary>摘要</summary>
《 spoken languages show significant variation across mandarin and accent. despite the high performance of mandarin automatic speech recognition (ASR), accent ASR is still a challenge task. in this paper, we introduce meta-learning techniques for fast accent domain expansion in mandarin speech recognition, which expands the field of accents without deteriorating the performance of mandarin ASR. meta-learning or learn-to-learn can learn general relations in multi-domains not only for over-fitting a specific domain. so we select meta-learning in the domain expansion task. this more essential learning will cause improved performance on accent domain extension tasks. we combine the methods of meta-learning and freeze of model parameters, which makes the recognition performance more stable in different cases and the training faster about 20%. our approach significantly outperforms other methods about 3% relatively in the accent domain expansion task. compared to the baseline model, it improves relatively 37% under the condition that the mandarin test set remains unchanged. in addition, it also proved this method to be effective on a large amount of data with a relative performance improvement of 4% on the accent test set.》Note that Simplified Chinese is the official standard for written Chinese in mainland China and is used in this translation. Traditional Chinese is also widely used, especially in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="MyVoice-Arabic-Speech-Resource-Collaboration-Platform"><a href="#MyVoice-Arabic-Speech-Resource-Collaboration-Platform" class="headerlink" title="MyVoice: Arabic Speech Resource Collaboration Platform"></a>MyVoice: Arabic Speech Resource Collaboration Platform</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02503">http://arxiv.org/abs/2308.02503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yousseif Elshahawy, Yassine El Kheir, Shammur Absar Chowdhury, Ahmed Ali</li>
<li>for: 增强阿拉伯语言 dialectal 技术	+ The paper is written to improve Arabic dialectal speech technologies.</li>
<li>methods: 使用 crowdsource 平台收集阿拉伯语言Speech	+ The paper uses a crowdsourcing platform to collect Arabic speech data.</li>
<li>results: 提供大量的阿拉伯语言 диалект Speech 数据	+ The paper provides a large amount of Arabic dialect speech data.Here’s the simplified Chinese text in the format you requested:</li>
<li>for: 增强阿拉伯语言 диалект speech 技术</li>
<li>methods: 使用 crowdsource 平台收集阿拉伯语言Speech</li>
<li>results: 提供大量的阿拉伯语言 диалект Speech 数据<details>
<summary>Abstract</summary>
We introduce MyVoice, a crowdsourcing platform designed to collect Arabic speech to enhance dialectal speech technologies. This platform offers an opportunity to design large dialectal speech datasets; and makes them publicly available. MyVoice allows contributors to select city/country-level fine-grained dialect and record the displayed utterances. Users can switch roles between contributors and annotators. The platform incorporates a quality assurance system that filters out low-quality and spurious recordings before sending them for validation. During the validation phase, contributors can assess the quality of recordings, annotate them, and provide feedback which is then reviewed by administrators. Furthermore, the platform offers flexibility to admin roles to add new data or tasks beyond dialectal speech and word collection, which are displayed to contributors. Thus, enabling collaborative efforts in gathering diverse and large Arabic speech data.
</details>
<details>
<summary>摘要</summary>
我们介绍MyVoice，一个人员征集平台，旨在收集阿拉伯语言的口语，以提高方言技术。这个平台为参与者提供了设置城市/国家精细方言的机会，并将这些大量方言数据公开提供。MyVoice让参与者可以选择城市/国家精细方言，并记录显示的言语。用户可以在角色之间切换，从参与者转为注释者。平台内置了质量保证系统，过滤出低质量和假记录，然后将其发送给验证。在验证阶段，参与者可以评估录音质量，注释和提供反馈，该反馈会被管理员审核。此外，平台允许管理角色添加新的数据或任务，这些数据和任务将被显示给参与者。因此，MyVoiceEnable了多方合作的集合各种多样的阿拉伯语言数据。
</details></li>
</ul>
<hr>
<h2 id="Signal-Reconstruction-from-Mel-spectrogram-Based-on-Bi-level-Consistency-of-Full-band-Magnitude-and-Phase"><a href="#Signal-Reconstruction-from-Mel-spectrogram-Based-on-Bi-level-Consistency-of-Full-band-Magnitude-and-Phase" class="headerlink" title="Signal Reconstruction from Mel-spectrogram Based on Bi-level Consistency of Full-band Magnitude and Phase"></a>Signal Reconstruction from Mel-spectrogram Based on Bi-level Consistency of Full-band Magnitude and Phase</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12232">http://arxiv.org/abs/2307.12232</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/YoshikiMas/signal-reconstruction-from-mel-spectrogram">https://github.com/YoshikiMas/signal-reconstruction-from-mel-spectrogram</a></li>
<li>paper_authors: Yoshiki Masuyama, Natsuki Ueno, Nobutaka Ono</li>
<li>for: 重建时域信号从低维 спектрограм中</li>
<li>methods: 利用低维 STFT矩阵和mel-spectrogram之间的双层关系进行优化算法</li>
<li>results: 对话、音乐和环境信号进行了实验，并得到了有效的重建结果<details>
<summary>Abstract</summary>
We propose an optimization-based method for reconstructing a time-domain signal from a low-dimensional spectral representation such as a mel-spectrogram. Phase reconstruction has been studied to reconstruct a time-domain signal from the full-band short-time Fourier transform (STFT) magnitude. The Griffin-Lim algorithm (GLA) has been widely used because it relies only on the redundancy of STFT and is applicable to various audio signals. In this paper, we jointly reconstruct the full-band magnitude and phase by considering the bi-level relationships among the time-domain signal, its STFT coefficients, and its mel-spectrogram. The proposed method is formulated as a rigorous optimization problem and estimates the full-band magnitude based on the criterion used in GLA. Our experiments demonstrate the effectiveness of the proposed method on speech, music, and environmental signals.
</details>
<details>
<summary>摘要</summary>
我们提出了一种优化方法来重建时域信号从低维 спектрограм中。 phase reconstruction 已经研究过来重建时域信号从全带宽短时域傅立叙恩（STFT）大小。格里夫金-林算法（GLA）广泛使用，因为它仅仅利用 STFT 的重复性，适用于多种音频信号。在这篇论文中，我们同时重建全带大小和相位，通过考虑时域信号、其 STFT 系数和mel-spectrogram之间的双层关系。我们的方法形式为严格的优化问题，根据 GLA 使用的标准 criterion 来估算全带大小。我们的实验表明，我们的方法对语音、乐队和环境信号都有效。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Integration-of-Speech-Separation-and-Recognition-with-Self-Supervised-Learning-Representation"><a href="#Exploring-the-Integration-of-Speech-Separation-and-Recognition-with-Self-Supervised-Learning-Representation" class="headerlink" title="Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation"></a>Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12231">http://arxiv.org/abs/2307.12231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yoshiki Masuyama, Xuankai Chang, Wangyou Zhang, Samuele Cornell, Zhong-Qiu Wang, Nobutaka Ono, Yanmin Qian, Shinji Watanabe</li>
<li>for: 这 paper 的目的是提高 reverberant 和噪音混合的多 speaker ASR 性能。</li>
<li>methods: 这 paper 使用多核频谱映射（TF-GridNet）和自动学习表征（SSLR）来实现 speech separation，并在 ASR 后续模型中使用最佳特征。</li>
<li>results: 该 paper 在 reverberant WHAMR! 测试集上达到了 2.5% 单词错误率，相比之下 existing mask-based MVDR 扩散抑制和 filterbank 集成（28.9%），显著提高了多 speaker ASR 性能。<details>
<summary>Abstract</summary>
Neural speech separation has made remarkable progress and its integration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we explore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recognition performance from the case with filterbank features. To further improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separation and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%).
</details>
<details>
<summary>摘要</summary>
neural speech separation 已经取得了很大的进步，与自动语音识别（ASR）的结合是一种重要的方向，可以实现多个说话人ASR。本文提供了具有深度探索的speech separation在噪音混叠和噪音混叠的scenario中的研究，作为ASR前端。在详细的实践中，我们探索了多通道分离方法、面积基于的束缚映射和复杂的spectral mapping，以及最佳的ASR后端模型中的特征。我们使用了最近的自动学习表示（SSLR）作为特征，并提高了使用filterbank特征的认识性能。为了进一步提高多个说话人认识性能，我们提出了一种特殊的训练策略，用于将speech separation和认识结合使用SSLR。我们的提议使用TF-GridNet基于的复杂的spectral mapping和WavLM基于的SSLR实现了WHAMR！测试集的2.5%词错率，与使用面积基于的MVDR束缚映射和filterbank结合（28.9%）相比，有所显著提高。
</details></li>
</ul>
<hr>
<h2 id="Backdoor-Attacks-against-Voice-Recognition-Systems-A-Survey"><a href="#Backdoor-Attacks-against-Voice-Recognition-Systems-A-Survey" class="headerlink" title="Backdoor Attacks against Voice Recognition Systems: A Survey"></a>Backdoor Attacks against Voice Recognition Systems: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13643">http://arxiv.org/abs/2307.13643</a></li>
<li>repo_url: None</li>
<li>paper_authors: Baochen Yan, Jiahe Lan, Zheng Yan</li>
<li>for: 本研究旨在探讨voice recognition systems (VRSs)受到后门攻击的漏洞，以提高VRSs的安全性和隐私性。</li>
<li>methods: 本文使用了深度学习对VRSs进行漏洞探测和分析，并提出了一组评价标准来评估后门攻击方法的性能。</li>
<li>results: 本文对VRSs受到的后门攻击进行了全面的检查和分类，并对现有的攻击方法进行了分析和评价。同时，本文还介绍了经典的后门防御方法和通用音频防御技术，并评估了它们在VRSs上的可行性。<details>
<summary>Abstract</summary>
Voice Recognition Systems (VRSs) employ deep learning for speech recognition and speaker recognition. They have been widely deployed in various real-world applications, from intelligent voice assistance to telephony surveillance and biometric authentication. However, prior research has revealed the vulnerability of VRSs to backdoor attacks, which pose a significant threat to the security and privacy of VRSs. Unfortunately, existing literature lacks a thorough review on this topic. This paper fills this research gap by conducting a comprehensive survey on backdoor attacks against VRSs. We first present an overview of VRSs and backdoor attacks, elucidating their basic knowledge. Then we propose a set of evaluation criteria to assess the performance of backdoor attack methods. Next, we present a comprehensive taxonomy of backdoor attacks against VRSs from different perspectives and analyze the characteristic of different categories. After that, we comprehensively review existing attack methods and analyze their pros and cons based on the proposed criteria. Furthermore, we review classic backdoor defense methods and generic audio defense techniques. Then we discuss the feasibility of deploying them on VRSs. Finally, we figure out several open issues and further suggest future research directions to motivate the research of VRSs security.
</details>
<details>
<summary>摘要</summary>
声认系统（VRS）通过深度学习实现语音识别和speaker识别。它们在不同的实际应用中广泛应用，从智能声助到电信监测和生物特征验证。然而，先前的研究发现VRS受到后门攻击的潜在威胁，这种威胁对VRS的安全性和隐私具有重要性。然而，现有的文献缺乏对这个话题的全面回顾。这篇论文填补了这个研究空白，通过对后门攻击VRS的全面评估。我们首先提供VRS和后门攻击的概述，然后提出评估后门攻击方法的评价标准。接着，我们提出了对VRS后门攻击的全面分类，分析不同类别的特点。然后，我们对现有的攻击方法进行了全面的回顾和分析，并评估它们的优缺点。此外，我们还评估了经典的后门防御方法和通用音频防御技术，以及它们在VRS上的可行性。最后，我们提出了一些未解决的问题和未来研究方向，以促进VRS的安全研究。
</details></li>
</ul>
<hr>
<h2 id="Modality-Confidence-Aware-Training-for-Robust-End-to-End-Spoken-Language-Understanding"><a href="#Modality-Confidence-Aware-Training-for-Robust-End-to-End-Spoken-Language-Understanding" class="headerlink" title="Modality Confidence Aware Training for Robust End-to-End Spoken Language Understanding"></a>Modality Confidence Aware Training for Robust End-to-End Spoken Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12134">http://arxiv.org/abs/2307.12134</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suyoun Kim, Akshat Shrivastava, Duc Le, Ju Lin, Ozlem Kalinli, Michael L. Seltzer</li>
<li>for: 这个论文目的是提高端到端语言理解（SLU）系统的Robustness，使其在语音识别错误时仍能准确地理解语音。</li>
<li>methods: 这个论文使用了一种单一模型，利用预训练的语音识别模型（ASR）的音频和文本表示，并且在设备流式enario下超过传统的管道SLU系统。但是，E2E SLU系统仍然在文本表示质量低时表现弱。为了解决这个问题，我们提出了一种新的E2E SLU系统，利用音频和文本表示的协同级别来增强ASR错误的Robustness。</li>
<li>results: 我们在STOP数据集上进行了实验，并发现了我们的方法可以提高SLU系统的准确率。此外，我们还进行了分析，以证明我们的方法的有效性。<details>
<summary>Abstract</summary>
End-to-end (E2E) spoken language understanding (SLU) systems that generate a semantic parse from speech have become more promising recently. This approach uses a single model that utilizes audio and text representations from pre-trained speech recognition models (ASR), and outperforms traditional pipeline SLU systems in on-device streaming scenarios. However, E2E SLU systems still show weakness when text representation quality is low due to ASR transcription errors. To overcome this issue, we propose a novel E2E SLU system that enhances robustness to ASR errors by fusing audio and text representations based on the estimated modality confidence of ASR hypotheses. We introduce two novel techniques: 1) an effective method to encode the quality of ASR hypotheses and 2) an effective approach to integrate them into E2E SLU models. We show accuracy improvements on STOP dataset and share the analysis to demonstrate the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
最近，端到端（E2E）的语言理解系统（SLU）在使用抽象语音识别模型（ASR）时得到了更多的承诺。这种方法使用单一的模型，利用来自ASR预训练模型的音频和文本表示，并在设备上流处理方式上超越传统的管道SLU系统。然而，E2E SLU系统仍然在文本表示质量低下时显示弱点，即ASR译写错误。为了解决这个问题，我们提议一种新的E2E SLU系统，通过将音频和文本表示 fusion 以提高对ASR译写错误的抗锋性。我们介绍了两种新技术：1）一种有效的ASR假设质量编码方法，和2）一种有效的将其集成到E2E SLU模型中的方法。我们在STOP数据集上进行了实验，并分享分析结果，以证明我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Estimating-speaker-direction-on-a-humanoid-robot-with-binaural-acoustic-signals"><a href="#Estimating-speaker-direction-on-a-humanoid-robot-with-binaural-acoustic-signals" class="headerlink" title="Estimating speaker direction on a humanoid robot with binaural acoustic signals"></a>Estimating speaker direction on a humanoid robot with binaural acoustic signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12129">http://arxiv.org/abs/2307.12129</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranav Barot, Katja Mombaur, Ewen MacDonald</li>
<li>for: 这个论文是为了实现人类与机器人之间的语音互动，计算机器人需要估计人类对话者的位置。</li>
<li>methods: 这个论文使用了一种方法来优化DOA估计参数，同时考虑了实时应用场景。这个方法是基于人工头部的双耳声源定位框架。收集了实际数据，并对其进行了注释。</li>
<li>results: 这个论文通过了一种简单的搜索方法和一种 bayesian 模型来优化参数，并对实时应用场景的延迟效应进行了研究。结果被验证和讨论。<details>
<summary>Abstract</summary>
To achieve human-like behaviour during speech interactions, it is necessary for a humanoid robot to estimate the location of a human talker. Here, we present a method to optimize the parameters used for the direction of arrival (DOA) estimation, while also considering real-time applications for human-robot interaction scenarios. This method is applied to binaural sound source localization framework on a humanoid robotic head. Real data is collected and annotated for this work. Optimizations are performed via a brute force method and a Bayesian model based method, results are validated and discussed, and effects on latency for real-time use are also explored.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)为实现人类语音交流中的人类行为，Robot需要估算人类说话者的位置。在这里，我们提出了一种优化DOA估计参数的方法，同时考虑了人机交互场景中的实时应用。这种方法是应用于人类机械头上的双耳声源定位框架。收集了实际数据并对其进行了注释。我们通过粗暴方法和 bayesian模型基于方法进行优化，并对结果进行验证和讨论，还研究了实时使用时的延迟效应。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/07/23/cs.SD_2023_07_23/" data-id="cllt6z4j6004bq3889fom7339" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/23/cs.LG_2023_07_23/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-07-23 18:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/23/eess.IV_2023_07_23/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-07-23 17:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
