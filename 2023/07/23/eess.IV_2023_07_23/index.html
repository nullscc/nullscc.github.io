
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-07-23 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="ES2Net: An Efficient Spectral-Spatial Network for Hyperspectral Image Change Detection paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.12327 repo_url: None paper_authors: Qingren Yao, Yuan Zhou, Wei Xiang for: 本">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-07-23">
<meta property="og:url" content="https://nullscc.github.io/2023/07/23/eess.IV_2023_07_23/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="ES2Net: An Efficient Spectral-Spatial Network for Hyperspectral Image Change Detection paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.12327 repo_url: None paper_authors: Qingren Yao, Yuan Zhou, Wei Xiang for: 本">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-23T09:00:00.000Z">
<meta property="article:modified_time" content="2023-09-23T08:40:04.807Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_07_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/23/eess.IV_2023_07_23/" class="article-date">
  <time datetime="2023-07-23T09:00:00.000Z" itemprop="datePublished">2023-07-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-07-23
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ES2Net-An-Efficient-Spectral-Spatial-Network-for-Hyperspectral-Image-Change-Detection"><a href="#ES2Net-An-Efficient-Spectral-Spatial-Network-for-Hyperspectral-Image-Change-Detection" class="headerlink" title="ES2Net: An Efficient Spectral-Spatial Network for Hyperspectral Image Change Detection"></a>ES2Net: An Efficient Spectral-Spatial Network for Hyperspectral Image Change Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12327">http://arxiv.org/abs/2307.12327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingren Yao, Yuan Zhou, Wei Xiang</li>
<li>for: 本研究旨在提高迁移图像特征检测精度，具体目的是针对高spectralresolution干扰图像（HSIs）进行迁移特征检测。</li>
<li>methods: 本研究提出了一种综合利用深度学习和频谱筛选技术的迁移特征检测网络（ES2Net），其中包括一个学习式频谱筛选模块，可以自动选择适合迁移检测的频谱 bands。此外，为了考虑不同频谱 bands 之间的复杂非线性关系，我们还提出了一种帧度级空间注意力机制。</li>
<li>results: 实验表明，ES2Net 比其他当前领域state-of-the-art方法更高效和精度。<details>
<summary>Abstract</summary>
Hyperspectral image change detection (HSI-CD) aims to identify the differences in bitemporal HSIs. To mitigate spectral redundancy and improve the discriminativeness of changing features, some methods introduced band selection technology to select bands conducive for CD. However, these methods are limited by the inability to end-to-end training with the deep learning-based feature extractor and lack considering the complex nonlinear relationship among bands. In this paper, we propose an end-to-end efficient spectral-spatial change detection network (ES2Net) to address these issues. Specifically, we devised a learnable band selection module to automatically select bands conducive to CD. It can be jointly optimized with a feature extraction network and capture the complex nonlinear relationships among bands. Moreover, considering the large spatial feature distribution differences among different bands, we design the cluster-wise spatial attention mechanism that assigns a spatial attention factor to each individual band to individually improve the feature discriminativeness for each band. Experiments on three widely used HSI-CD datasets demonstrate the effectiveness and superiority of this method compared with other state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
干支图像变化检测（HSI-CD）的目标是确定双时间干支图像之间的差异。为了减少 спектраль的重复性和提高变化特征的抑制力，一些方法引入了频率选择技术，以选择适合变化检测的频率。然而，这些方法受到练习深度学习基于特征提取器的端到端训练的限制，以及各频率之间的复杂非线性关系的忽略。在本文中，我们提出了一种练习效率的 spectral-spatial 变化检测网络（ES2Net），以解决这些问题。具体来说，我们设计了一个学习型频率选择模块，可以自动选择适合变化检测的频率。这个模块可以与特征提取网络jointly 优化，并 capture 各频率之间的复杂非线性关系。此外，考虑到不同频率之间的大规模空间特征分布差异，我们设计了各个频率的各自精度注意力机制，以进一步提高每个频率的特征抑制力。在三个广泛使用的HSI-CD数据集上进行了实验，我们发现该方法与其他当前状态的方法相比，具有更高的效iveness和优势。
</details></li>
</ul>
<hr>
<h2 id="Development-of-pericardial-fat-count-images-using-a-combination-of-three-different-deep-learning-models"><a href="#Development-of-pericardial-fat-count-images-using-a-combination-of-three-different-deep-learning-models" class="headerlink" title="Development of pericardial fat count images using a combination of three different deep-learning models"></a>Development of pericardial fat count images using a combination of three different deep-learning models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12316">http://arxiv.org/abs/2307.12316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takaaki Matsunaga, Atsushi Kono, Hidetoshi Matsuo, Kaoru Kitagawa, Mizuho Nishio, Hiromi Hashimura, Yu Izawa, Takayoshi Toba, Kazuki Ishikawa, Akie Katsuki, Kazuyuki Ohmura, Takamichi Murakami</li>
<li>for: 这个研究旨在使用深度学习模型从胸部X射线图像中生成胸部脂肪计数图像（PFCIs），以评估胸部脂肪的水平。</li>
<li>methods: 这个研究使用了3种不同的深度学习模型，包括CycleGAN，将胸部CT图像投影到2D图像上，并使用高像素值表示脂肪聚集。</li>
<li>results: 比较了使用提案方法生成的PFCIs和单一CycleGAN-based模型生成的PFCIs，发现提案方法生成的PFCIs具有更高的图像质量指标（SSIM、MSE、MAE）。<details>
<summary>Abstract</summary>
Rationale and Objectives: Pericardial fat (PF), the thoracic visceral fat surrounding the heart, promotes the development of coronary artery disease by inducing inflammation of the coronary arteries. For evaluating PF, this study aimed to generate pericardial fat count images (PFCIs) from chest radiographs (CXRs) using a dedicated deep-learning model.   Materials and Methods: The data of 269 consecutive patients who underwent coronary computed tomography (CT) were reviewed. Patients with metal implants, pleural effusion, history of thoracic surgery, or that of malignancy were excluded. Thus, the data of 191 patients were used. PFCIs were generated from the projection of three-dimensional CT images, where fat accumulation was represented by a high pixel value. Three different deep-learning models, including CycleGAN, were combined in the proposed method to generate PFCIs from CXRs. A single CycleGAN-based model was used to generate PFCIs from CXRs for comparison with the proposed method. To evaluate the image quality of the generated PFCIs, structural similarity index measure (SSIM), mean squared error (MSE), and mean absolute error (MAE) of (i) the PFCI generated using the proposed method and (ii) the PFCI generated using the single model were compared.   Results: The mean SSIM, MSE, and MAE were as follows: 0.856, 0.0128, and 0.0357, respectively, for the proposed model; and 0.762, 0.0198, and 0.0504, respectively, for the single CycleGAN-based model.   Conclusion: PFCIs generated from CXRs with the proposed model showed better performance than those with the single model. PFCI evaluation without CT may be possible with the proposed method.
</details>
<details>
<summary>摘要</summary>
理解和目标：胸膈脂肪（PF），脊梗内脂肪环绕心脏，促进了折射病变的发展。为评估PF，本研究旨在通过专门的深度学习模型生成胸膈脂肪计数图像（PFCIs）从胸部X射线图（CXRs）中。材料和方法：本研究查阅了269例 consecutively患者的折射 computed tomography（CT）数据。患有金属设备、肿胀、历史折射手术或肿瘤患者被排除。因此，该研究使用了191例患者的数据。PFCIs通过三维CT图像的投影，表示脂肪堆积的高像素值来生成。三种不同的深度学习模型，包括CycleGAN，被组合在提议的方法中来生成PFCIs从CXRs。单独使用CycleGAN基于模型生成PFCIs从CXRs作为比较。为评估生成的PFCIs的图像质量，使用了结构相似度指标（SSIM）、平均方差（MSE）和平均绝对错误（MAE）进行比较。结果：生成的PFCIs的SSIM、MSE和MAE分别为：0.856、0.0128和0.0357；而单独使用CycleGAN基于模型生成的PFCIs的SSIM、MSE和MAE分别为：0.762、0.0198和0.0504。结论：提议的方法生成的PFCIs在SSIM、MSE和MAE指标上表现更好于单独使用CycleGAN基于模型生成的PFCIs。PFCI评估可能不需要CT成像。
</details></li>
</ul>
<hr>
<h2 id="Simultaneous-temperature-estimation-and-nonuniformity-correction-from-multiple-frames"><a href="#Simultaneous-temperature-estimation-and-nonuniformity-correction-from-multiple-frames" class="headerlink" title="Simultaneous temperature estimation and nonuniformity correction from multiple frames"></a>Simultaneous temperature estimation and nonuniformity correction from multiple frames</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12297">http://arxiv.org/abs/2307.12297</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navot Oz, Omri Berman, Nir Sochen, David Mendelovich, Iftach Klapp</li>
<li>for: 用于温度测量的低成本紫外线相机中的非均匀性和温度测量偏差问题的解决方案。</li>
<li>methods: 基于物理捕获模型和深度学习核函数网络（KPN）的方法，通过将多帧图像 fusion 来实现同时的温度估计和非均匀性修正。另外，还提出了一个新的偏移块，可以将 ambient 温度包含在模型中，以便估计相机的偏移。</li>
<li>results: 通过对实际数据进行测试，得到了与高科技质量的放射计相机相比，仅有0.27-0.54℃的小差异。这种方法可以提供高精度和高效的同时温度估计和非均匀性修正解决方案，对各种实际应用场景具有重要意义。<details>
<summary>Abstract</summary>
Infrared (IR) cameras are widely used for temperature measurements in various applications, including agriculture, medicine, and security. Low-cost IR camera have an immense potential to replace expansive radiometric cameras in these applications, however low-cost microbolometer-based IR cameras are prone to spatially-variant nonuniformity and to drift in temperature measurements, which limits their usability in practical scenarios.   To address these limitations, we propose a novel approach for simultaneous temperature estimation and nonuniformity correction from multiple frames captured by low-cost microbolometer-based IR cameras. We leverage the physical image acquisition model of the camera and incorporate it into a deep learning architecture called kernel estimation networks (KPN), which enables us to combine multiple frames despite imperfect registration between them. We also propose a novel offset block that incorporates the ambient temperature into the model and enables us to estimate the offset of the camera, which is a key factor in temperature estimation.   Our findings demonstrate that the number of frames has a significant impact on the accuracy of temperature estimation and nonuniformity correction. Moreover, our approach achieves a significant improvement in performance compared to vanilla KPN, thanks to the offset block. The method was tested on real data collected by a low-cost IR camera mounted on a UAV, showing only a small average error of $0.27^\circ C-0.54^\circ C$ relative to costly scientific-grade radiometric cameras.   Our method provides an accurate and efficient solution for simultaneous temperature estimation and nonuniformity correction, which has important implications for a wide range of practical applications.
</details>
<details>
<summary>摘要</summary>
低成本红外线（IR）镜头在不同应用中广泛使用，包括农业、医学和安全领域。低成本微波温度计IR镜头具有巨大的潜在可能性，以取代昂贵的几何学测量镜头，但是它们受到空间不均匀和测量偏差的限制，这限制了它们在实际应用中的可用性。为了解决这些限制，我们提出了一个新的方法，可以同时进行测量温度和非均匀调正。我们利用镜头的物理摄取模型，并将其 integrate into a deep learning architecture called kernel estimation networks (KPN)，这使得我们可以融合多帧影像，即使它们不具有完美的对齐。我们还提出了一个 novel offset block，它包含了环境温度，并允许我们估计镜头的偏移，这是温度估计中的关键因素。我们的研究表明，影像数量有显著的影响温度估计和非均匀调正的精度。此外，我们的方法在比vanilla KPN更好的性能，感谢偏移层的存在。我们的方法在实际应用中使用低成本IR镜头，与较贵的科学级测量镜头相比， пока有小平均误差为0.27-0.54℃。我们的方法提供了一个精确和高效的温度估计和非均匀调正方法，具有广泛的实际应用。
</details></li>
</ul>
<hr>
<h2 id="ASCON-Anatomy-aware-Supervised-Contrastive-Learning-Framework-for-Low-dose-CT-Denoising"><a href="#ASCON-Anatomy-aware-Supervised-Contrastive-Learning-Framework-for-Low-dose-CT-Denoising" class="headerlink" title="ASCON: Anatomy-aware Supervised Contrastive Learning Framework for Low-dose CT Denoising"></a>ASCON: Anatomy-aware Supervised Contrastive Learning Framework for Low-dose CT Denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12225">http://arxiv.org/abs/2307.12225</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hao1635/ASCON">https://github.com/hao1635/ASCON</a></li>
<li>paper_authors: Zhihao Chen, Qi Gao, Yi Zhang, Hongming Shan</li>
<li>for: 这个论文是设计来进行低剂量 Computed Tomography（CT）扫描图像干扰除掉噪声的方法。</li>
<li>methods: 这个方法使用了两个新的设计：一个高效的自我注意力-based U-Net（ESAU-Net）和一个多尺度解剖对比网络（MAC-Net）。ESAU-Net使用了通道对自我注意力的机制来更好地捕捉全域-地方互动，而MAC-Net则包括一个单元非对比模组来捕捉解剖信息和一个像素对比模组来维持自体解剖一致性。</li>
<li>results: 实验结果显示，ASCON在两个公共的低剂量 CT 扫描干扰 dataset 上表现出色，较之前的模型更好。此外，ASCON还提供了解剖解释，允许在低剂量 CT 扫描中进行解剖可读性检查。<details>
<summary>Abstract</summary>
While various deep learning methods have been proposed for low-dose computed tomography (CT) denoising, most of them leverage the normal-dose CT images as the ground-truth to supervise the denoising process. These methods typically ignore the inherent correlation within a single CT image, especially the anatomical semantics of human tissues, and lack the interpretability on the denoising process. In this paper, we propose a novel Anatomy-aware Supervised CONtrastive learning framework, termed ASCON, which can explore the anatomical semantics for low-dose CT denoising while providing anatomical interpretability. The proposed ASCON consists of two novel designs: an efficient self-attention-based U-Net (ESAU-Net) and a multi-scale anatomical contrastive network (MAC-Net). First, to better capture global-local interactions and adapt to the high-resolution input, an efficient ESAU-Net is introduced by using a channel-wise self-attention mechanism. Second, MAC-Net incorporates a patch-wise non-contrastive module to capture inherent anatomical information and a pixel-wise contrastive module to maintain intrinsic anatomical consistency. Extensive experimental results on two public low-dose CT denoising datasets demonstrate superior performance of ASCON over state-of-the-art models. Remarkably, our ASCON provides anatomical interpretability for low-dose CT denoising for the first time. Source code is available at https://github.com/hao1635/ASCON.
</details>
<details>
<summary>摘要</summary>
“Various deep learning methods have been proposed for low-dose computed tomography (CT) denoising, but most of them rely on normal-dose CT images as ground truth to supervise the denoising process, ignoring the inherent correlation within a single CT image and lacking interpretability. In this paper, we propose a novel Anatomy-aware Supervised CONtrastive learning framework, termed ASCON, which can explore the anatomical semantics for low-dose CT denoising while providing anatomical interpretability. The proposed ASCON consists of two novel designs: an efficient self-attention-based U-Net (ESAU-Net) and a multi-scale anatomical contrastive network (MAC-Net). First, to better capture global-local interactions and adapt to high-resolution input, an efficient ESAU-Net is introduced using a channel-wise self-attention mechanism. Second, MAC-Net incorporates a patch-wise non-contrastive module to capture inherent anatomical information and a pixel-wise contrastive module to maintain intrinsic anatomical consistency. Extensive experimental results on two public low-dose CT denoising datasets demonstrate superior performance of ASCON over state-of-the-art models, and remarkably, our ASCON provides anatomical interpretability for low-dose CT denoising for the first time. Source code is available at https://github.com/hao1635/ASCON.”Note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. The other form is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="SCPAT-GAN-Structural-Constrained-and-Pathology-Aware-Convolutional-Transformer-GAN-for-Virtual-Histology-Staining-of-Human-Coronary-OCT-images"><a href="#SCPAT-GAN-Structural-Constrained-and-Pathology-Aware-Convolutional-Transformer-GAN-for-Virtual-Histology-Staining-of-Human-Coronary-OCT-images" class="headerlink" title="SCPAT-GAN: Structural Constrained and Pathology Aware Convolutional Transformer-GAN for Virtual Histology Staining of Human Coronary OCT images"></a>SCPAT-GAN: Structural Constrained and Pathology Aware Convolutional Transformer-GAN for Virtual Histology Staining of Human Coronary OCT images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12138">http://arxiv.org/abs/2307.12138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueshen Li, Hongshan Liu, Xiaoyu Song, Brigitta C. Brott, Silvio H. Litovsky, Yu Gan</li>
<li>for: 用于生成基于OCT图像的虚拟病理信息，以更好地指导心血管疾病的治疗。</li>
<li>methods: 使用 transformer生成对抗网络，并在结构层进行病理导向的制约，以生成虚拟染色H&amp;E压痕。</li>
<li>results: 提高了现有方法的病理准确率和结构准确率，并可以在不需要大量 paired 训练数据的情况下生成虚拟病理信息。<details>
<summary>Abstract</summary>
There is a significant need for the generation of virtual histological information from coronary optical coherence tomography (OCT) images to better guide the treatment of coronary artery disease. However, existing methods either require a large pixel-wisely paired training dataset or have limited capability to map pathological regions. To address these issues, we proposed a structural constrained, pathology aware, transformer generative adversarial network, namely SCPAT-GAN, to generate virtual stained H&E histology from OCT images. The proposed SCPAT-GAN advances existing methods via a novel design to impose pathological guidance on structural layers using transformer-based network.
</details>
<details>
<summary>摘要</summary>
“ coronary optical coherence tomography（OCT）图像中的虚拟 histological 信息的生成具有抑制 coronary artery disease 的治疗方法的重要需求。然而，现有的方法 either require a large paired training dataset or have limited capability to map pathological regions。为解决这些问题，我们提出了一种基于 transformer 的权重约束、病理相关的生成对抗网络，即 SCPAT-GAN，用于从 OCT 图像中生成虚拟染色 H&E 病理图像。我们的提议的 SCPAT-GAN 在现有方法中提供了一种新的设计，通过 transformer 基于的网络来强制 paths 层中的病理指导。”Note: Please keep in mind that the translation is done by a machine and may not be perfect. If you have any further questions or need more accurate translations, please feel free to ask!
</details></li>
</ul>
<hr>
<h2 id="Improving-temperature-estimation-in-low-cost-infrared-cameras-using-deep-neural-networks"><a href="#Improving-temperature-estimation-in-low-cost-infrared-cameras-using-deep-neural-networks" class="headerlink" title="Improving temperature estimation in low-cost infrared cameras using deep neural networks"></a>Improving temperature estimation in low-cost infrared cameras using deep neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12130">http://arxiv.org/abs/2307.12130</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navot Oz, Nir Sochen, David Mendelovich, Iftach Klapp</li>
<li>for: 提高低成本热相机的温度准确性和纠正非均匀性。</li>
<li>methods: 开发了一个考虑 ambient temperature 的非均匀性模拟器，并提出了一种基于全连接神经网络的热体温度估算方法，通过使用单个图像和摄像头自身测量的 ambient temperature 来纠正非均匀性。</li>
<li>results: 比前作下降了约 $1^\circ C$ 的平均温度误差，并且通过应用物理约束降低了误差的 $4%$。 验证数据集上的平均温度误差为 $0.37^\circ C$，并在实际场景中也得到了相当的结果。<details>
<summary>Abstract</summary>
Low-cost thermal cameras are inaccurate (usually $\pm 3^\circ C$) and have space-variant nonuniformity across their detector. Both inaccuracy and nonuniformity are dependent on the ambient temperature of the camera. The main goal of this work was to improve the temperature accuracy of low-cost cameras and rectify the nonuniformity.   A nonuniformity simulator that accounts for the ambient temperature was developed. An end-to-end neural network that incorporates the ambient temperature at image acquisition was introduced. The neural network was trained with the simulated nonuniformity data to estimate the object's temperature and correct the nonuniformity, using only a single image and the ambient temperature measured by the camera itself. Results show that the proposed method lowered the mean temperature error by approximately $1^\circ C$ compared to previous works. In addition, applying a physical constraint on the network lowered the error by an additional $4\%$.   The mean temperature error over an extensive validation dataset was $0.37^\circ C$. The method was verified on real data in the field and produced equivalent results.
</details>
<details>
<summary>摘要</summary>
低成本热相机的精度受到 ambient temperature 的影响（通常在 $\pm 3^\circ C$ 范围内），并且具有空间不均的非统一性，这两个问题都与热相机的 ambient temperature 相关。本研究的主要目标是提高低成本热相机的温度精度和修正非统一性。我们开发了一个考虑 ambient temperature 的非统一性模拟器，并提出了一种基于 neural network 的方法，该方法可以使用单个图像和热相机自己测量的 ambient temperature 来估计物体温度并修正非统一性。实验结果表明，我们的方法可以降低mean温度错误约为 $1^\circ C$  compared to 前一代方法。此外，通过 físical 约束对网络进行限制，可以降低错误约为 $4\%$。整体来说，我们的方法在广泛验证数据集上的 mean 温度错误为 $0.37^\circ C$。此外，我们的方法在实际场景中也得到了Equivalent 的结果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/23/eess.IV_2023_07_23/" data-id="clp869u67017gk588d10cbf09" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/23/cs.LG_2023_07_23/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-07-23
        
      </div>
    </a>
  
  
    <a href="/2023/07/22/cs.CV_2023_07_22/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.CV - 2023-07-22</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">66</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">81</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">140</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
