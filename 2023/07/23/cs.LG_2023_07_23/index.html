
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-07-23 18:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations? paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.12344 repo_url: https:&#x2F;&#x2F;github.com&#x2F;ss-sun&#x2F;right-for-the-wrong-reason pape">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-07-23 18:00:00">
<meta property="og:url" content="http://nullscc.github.io/2023/07/23/cs.LG_2023_07_23/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations? paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.12344 repo_url: https:&#x2F;&#x2F;github.com&#x2F;ss-sun&#x2F;right-for-the-wrong-reason pape">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-22T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:36.886Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_07_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/23/cs.LG_2023_07_23/" class="article-date">
  <time datetime="2023-07-22T16:00:00.000Z" itemprop="datePublished">2023-07-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-07-23 18:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Right-for-the-Wrong-Reason-Can-Interpretable-ML-Techniques-Detect-Spurious-Correlations"><a href="#Right-for-the-Wrong-Reason-Can-Interpretable-ML-Techniques-Detect-Spurious-Correlations" class="headerlink" title="Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?"></a>Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12344">http://arxiv.org/abs/2307.12344</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ss-sun/right-for-the-wrong-reason">https://github.com/ss-sun/right-for-the-wrong-reason</a></li>
<li>paper_authors: Susu Sun, Lisa M. Koch, Christian F. Baumgartner</li>
<li>for: 本文旨在评估一种解释技术的能力，以确定它是否能够正确地检测模型中的假 correlate。</li>
<li>methods: 本文使用了五种后处解释技术和一种自然 interpretable 的方法来检测一个胸部 X-ray 诊断任务中的三种人工添加的干扰因素。</li>
<li>results: 结果显示，使用 SHAP 和 Attri-Net 方法可以准确地检测模型中的假 correlate，并且这些方法可以被用来可靠地检测模型的错误行为。<details>
<summary>Abstract</summary>
While deep neural network models offer unmatched classification performance, they are prone to learning spurious correlations in the data. Such dependencies on confounding information can be difficult to detect using performance metrics if the test data comes from the same distribution as the training data. Interpretable ML methods such as post-hoc explanations or inherently interpretable classifiers promise to identify faulty model reasoning. However, there is mixed evidence whether many of these techniques are actually able to do so. In this paper, we propose a rigorous evaluation strategy to assess an explanation technique's ability to correctly identify spurious correlations. Using this strategy, we evaluate five post-hoc explanation techniques and one inherently interpretable method for their ability to detect three types of artificially added confounders in a chest x-ray diagnosis task. We find that the post-hoc technique SHAP, as well as the inherently interpretable Attri-Net provide the best performance and can be used to reliably identify faulty model behavior.
</details>
<details>
<summary>摘要</summary>
深度神经网络模型具有无可比的分类性能，但它们容易学习数据中的假 correlate。这些依赖干扰信息的相互作用可以通过性能指标难以探测，尤其如果测试数据来自同一个分布。可解释的机器学习方法，如后期解释或自然解释的分类器，承诺能够识别模型的恶劣逻辑。然而，有证据表明，许多这些技术并不能够做到这一点。在这篇论文中，我们提出了一种充分的评估策略，用于评估一种解释技术的能力，并可以准确地识别模型中的假 correlate。使用这种策略，我们评估了五种后期解释技术和一种自然解释的方法，并发现这些技术可以可靠地识别模型的假 correlate。特别是，SHAP技术和自然解释的Attri-Net具有最好的表现，可以用于可靠地识别模型的恶劣逻辑。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Learning-for-Audio-Based-Emotion-Recognition"><a href="#Self-Supervised-Learning-for-Audio-Based-Emotion-Recognition" class="headerlink" title="Self-Supervised Learning for Audio-Based Emotion Recognition"></a>Self-Supervised Learning for Audio-Based Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12343">http://arxiv.org/abs/2307.12343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peranut Nimitsurachat, Peter Washington</li>
<li>for: 这个论文的目的是提高 audio 输入数据 的情感识别模型性能，以便在心理健康、市场营销、游戏和社交媒体分析等领域中建立交互式系统。</li>
<li>methods: 这个论文使用了自动学习（SSL）方法，通过预测数据自身的特性来学习，而不需要大量的指导标签。</li>
<li>results: 研究发现，使用 SSL 预训练后 fine-tune 使得模型性能在所有评价指标上都提高，特别是在较易分类的情绪（如快乐、悲伤和愤怒）方面。此外，研究还发现，当输入数据具有嵌入特征表示时，自动学习更加有用。<details>
<summary>Abstract</summary>
Emotion recognition models using audio input data can enable the development of interactive systems with applications in mental healthcare, marketing, gaming, and social media analysis. While the field of affective computing using audio data is rich, a major barrier to achieve consistently high-performance models is the paucity of available training labels. Self-supervised learning (SSL) is a family of methods which can learn despite a scarcity of supervised labels by predicting properties of the data itself. To understand the utility of self-supervised learning for audio-based emotion recognition, we have applied self-supervised learning pre-training to the classification of emotions from the CMU- MOSEI's acoustic modality. Unlike prior papers that have experimented with raw acoustic data, our technique has been applied to encoded acoustic data. Our model is first pretrained to uncover the randomly-masked timestamps of the acoustic data. The pre-trained model is then fine-tuned using a small sample of annotated data. The performance of the final model is then evaluated via several evaluation metrics against a baseline deep learning model with an identical backbone architecture. We find that self-supervised learning consistently improves the performance of the model across all metrics. This work shows the utility of self-supervised learning for affective computing, demonstrating that self-supervised learning is most useful when the number of training examples is small, and that the effect is most pronounced for emotions which are easier to classify such as happy, sad and anger. This work further demonstrates that self-supervised learning works when applied to embedded feature representations rather than the traditional approach of pre-training on the raw input space.
</details>
<details>
<summary>摘要</summary>
“情感识别模型使用音频输入数据可以实现互动系统的开发，应用于心理健康、市场营销、游戏和社交媒体分析。然而，使得模型表现优秀的主要障碍是标注数据的稀缺。基于这点，我们应用了自我超vised学习（SSL）方法，使得模型能够学习，即使标注数据稀缺。为了了解Audio-based情感识别中自我超vised学习的 utility，我们在CMU-MOSEI的音频数据中进行了自我超vised学习预训练。与先前的研究不同，我们的技术将encoded acoustic data应用于模型。我们的模型首先预训练，以找出随机遮盖的时间戳。然后，预训练模型被精度调整使用一小样本的注释数据。最终模型的性能被评估使用多个评价指标，并与基eline深度学习模型进行比较。我们发现，自我超vised学习可以一直提高模型的性能，并且效果最 pronounced  для易于分类的情感，如快乐、悲伤和愤怒。此外，我们发现自我超vised学习在嵌入特征表示中应用得更加有效，而不是传统的预训练在原始输入空间。”
</details></li>
</ul>
<hr>
<h2 id="Rapid-detection-of-soil-carbonates-by-means-of-NIR-spectroscopy-deep-learning-methods-and-phase-quantification-by-powder-Xray-diffraction"><a href="#Rapid-detection-of-soil-carbonates-by-means-of-NIR-spectroscopy-deep-learning-methods-and-phase-quantification-by-powder-Xray-diffraction" class="headerlink" title="Rapid detection of soil carbonates by means of NIR spectroscopy, deep learning methods and phase quantification by powder Xray diffraction"></a>Rapid detection of soil carbonates by means of NIR spectroscopy, deep learning methods and phase quantification by powder Xray diffraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12341">http://arxiv.org/abs/2307.12341</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lykourgos Chiniadis, Petros Tamvakis</li>
<li>for: 这个研究旨在提高农业生产和土壤属性分析，以实现生态平衡和环境可持续性。</li>
<li>methods: 本研究使用FT NIR reflectanceспектроскопия和深度学习方法来预测土壤中碳酸含量。</li>
<li>results: 研究获得了很好的预测结果，能够快速和高精度地预测土壤中碳酸含量，并且在未前seen的土壤标本上还能够获得良好的预测结果。<details>
<summary>Abstract</summary>
Soil NIR spectral absorbance/reflectance libraries are utilized towards improving agricultural production and analysis of soil properties which are key prerequisite for agroecological balance and environmental sustainability. Carbonates in particular, represent a soil property which is mostly affected even by mild, let alone extreme, changes of environmental conditions during climate change. In this study we propose a rapid and efficient way to predict carbonates content in soil by means of FT NIR reflectance spectroscopy and by use of deep learning methods. We exploited multiple machine learning methods, such as: 1) a MLP Regressor and 2) a CNN and compare their performance with other traditional ML algorithms such as PLSR, Cubist and SVM on the combined dataset of two NIR spectral libraries: KSSL (USDA), a dataset of soil samples reflectance spectra collected nationwide, and LUCAS TopSoil (European Soil Library) which contains soil sample absorbance spectra from all over the European Union, and use them to predict carbonate content on never before seen soil samples. Soil samples in KSSL and in TopSoil spectral libraries were acquired in the spectral region of visNIR, however in this study, only the NIR spectral region was utilized. Quantification of carbonates by means of Xray Diffraction is in good agreement with the volumetric method and the MLP prediction. Our work contributes to rapid carbonates content prediction in soil samples in cases where: 1) no volumetric method is available and 2) only NIR spectra absorbance data are available. Up till now and to the best of our knowledge, there exists no other study, that presents a prediction model trained on such an extensive dataset with such promising results on unseen data, undoubtedly supporting the notion that deep learning models present excellent prediction tools for soil carbonates content.
</details>
<details>
<summary>摘要</summary>
soil NIR spectral absorbance/reflectance 图书馆是用于改进农业生产和土壤性质分析，这些特性是生态平衡和环境可持续发展的关键前提。碳酸盐尤其是，它们在气候变化中，即使是轻度的环境变化，都会受到影响。本研究提出了一种快速和高效的碳酸盐含量预测方法，使用FT NIR反射спектроскопия和深度学习方法。我们利用了多种机器学习方法，如：1）多层感知网络（MLP）回归器和2）卷积神经网络（CNN），并与其他传统的机器学习算法，如：PLSR、Cubist和SVM，进行比较。我们使用了两个NIR光谱图书馆：KSSL（美国农业部）和LUCAS TopSoil（欧盟土壤图书馆）的共同数据集，这两个数据集包含了不同地区的土壤样本吸收和反射光谱数据。我们只使用了NIR光谱区域。我们通过X射Diffraction测量碳酸盐含量和MLP预测值进行比较，结果表明我们的预测模型具有良好的准确性。我们的工作可以帮助在无法使用液体方法和只有NIR光谱吸收数据时预测土壤中碳酸盐含量。到目前为止，我们没有发现任何一项研究，可以在类似的大规模数据集上提出类似的预测模型，这证明了深度学习模型在土壤碳酸盐含量预测中具有极高的预测精度。
</details></li>
</ul>
<hr>
<h2 id="TabADM-Unsupervised-Tabular-Anomaly-Detection-with-Diffusion-Models"><a href="#TabADM-Unsupervised-Tabular-Anomaly-Detection-with-Diffusion-Models" class="headerlink" title="TabADM: Unsupervised Tabular Anomaly Detection with Diffusion Models"></a>TabADM: Unsupervised Tabular Anomaly Detection with Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12336">http://arxiv.org/abs/2307.12336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guy Zamberg, Moshe Salhov, Ofir Lindenbaum, Amir Averbuch</li>
<li>for: 本研究旨在提出一种基于扩散的概率模型，用于不监督型异常检测。</li>
<li>methods: 该模型通过unique reject scheme来减弱异常样本对密度估计的影响，并在推理阶段通过确定低密度区域来识别异常样本。</li>
<li>results: 实验结果表明，该方法能够提高异常检测的能力，并且相比基eline方法具有更好的稳定性和较少的参数调整。<details>
<summary>Abstract</summary>
Tables are an abundant form of data with use cases across all scientific fields. Real-world datasets often contain anomalous samples that can negatively affect downstream analysis. In this work, we only assume access to contaminated data and present a diffusion-based probabilistic model effective for unsupervised anomaly detection. Our model is trained to learn the density of normal samples by utilizing a unique rejection scheme to attenuate the influence of anomalies on the density estimation. At inference, we identify anomalies as samples in low-density regions. We use real data to demonstrate that our method improves detection capabilities over baselines. Furthermore, our method is relatively stable to the dimension of the data and does not require extensive hyperparameter tuning.
</details>
<details>
<summary>摘要</summary>
tables是一种非常常见的数据形式，在各个科学领域中有各种应用场景。实际数据中经常存在异常样本，这些异常样本可能会对后续分析产生负面影响。在这种情况下，我们只假设有杂质数据访问，并提出了一种基于扩散的 probabilistic 模型，用于不监督的异常检测。我们的模型通过利用异常样本的独特拒绝方案来减少异常样本对density估计的影响。在推理阶段，我们将异常样本标识为density低区域中的样本。我们使用实际数据来证明我们的方法可以提高异常检测的能力，并且我们的方法相对稳定于数据维度和不需要大量的 гипер参数调整。
</details></li>
</ul>
<hr>
<h2 id="An-axiomatized-PDE-model-of-deep-neural-networks"><a href="#An-axiomatized-PDE-model-of-deep-neural-networks" class="headerlink" title="An axiomatized PDE model of deep neural networks"></a>An axiomatized PDE model of deep neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12333">http://arxiv.org/abs/2307.12333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tangjun Wang, Wenqi Tao, Chenglong Bao, Zuoqiang Shi</li>
<li>for: 研究深度神经网络（DNN）与 partiall differential equation（PDE）之间的关系，以推导DNN的普遍型PDE模型。</li>
<li>methods: 将DNN视为一个演化算符，从简单的基模型出发，通过一些合理的假设，证明演化算符实际上由湍涨-散射方程推导出来。</li>
<li>results: 根据湍涨-散射方程，提出了一种新的训练方法 для ResNet，实验验证了该方法的性能。<details>
<summary>Abstract</summary>
Inspired by the relation between deep neural network (DNN) and partial differential equations (PDEs), we study the general form of the PDE models of deep neural networks. To achieve this goal, we formulate DNN as an evolution operator from a simple base model. Based on several reasonable assumptions, we prove that the evolution operator is actually determined by convection-diffusion equation. This convection-diffusion equation model gives mathematical explanation for several effective networks. Moreover, we show that the convection-diffusion model improves the robustness and reduces the Rademacher complexity. Based on the convection-diffusion equation, we design a new training method for ResNets. Experiments validate the performance of the proposed method.
</details>
<details>
<summary>摘要</summary>
受深度神经网络（DNN）和偏微分方程（PDE）之间的关系启发，我们研究深度神经网络的总体形式PDE模型。为 достичь这个目标，我们将DNN视为一个基本模型的演化运算器。基于一些合理的假设，我们证明了演化运算器实际上是受扩散-演化方程的决定。这个扩散-演化方程模型为各种有效网络提供了数学解释。此外，我们还证明了这个模型可以提高robustness并降低Rademacher复杂度。基于扩散-演化方程，我们设计了一种新的训练方法 дляResNets。实验证明了我们的提案的性能。
</details></li>
</ul>
<hr>
<h2 id="Tackling-the-Curse-of-Dimensionality-with-Physics-Informed-Neural-Networks"><a href="#Tackling-the-Curse-of-Dimensionality-with-Physics-Informed-Neural-Networks" class="headerlink" title="Tackling the Curse of Dimensionality with Physics-Informed Neural Networks"></a>Tackling the Curse of Dimensionality with Physics-Informed Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12306">http://arxiv.org/abs/2307.12306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheyuan Hu, Khemraj Shukla, George Em Karniadakis, Kenji Kawaguchi</li>
<li>for: 解决高维partial differential equations (PDEs)问题，提高计算效率和扩展性。</li>
<li>methods: 使用Stochastic Dimension Gradient Descent (SDGD)方法，将梯度分解成不同维度的部分，随机选择每个维度的部分进行训练physics-informed neural networks (PINNs)。</li>
<li>results: 实验表明，SDGD可以快速解决许多高维PDEs问题，如Hamilton-Jacobi-Bellman (HJB)和Schrödinger方程在千个维度中的解决。例如，在单个GPU上使用SDGD和PINNs mesh-free方法解决了100,000维的非线性PDEs问题。<details>
<summary>Abstract</summary>
The curse-of-dimensionality (CoD) taxes computational resources heavily with exponentially increasing computational cost as the dimension increases. This poses great challenges in solving high-dimensional PDEs as Richard Bellman first pointed out over 60 years ago. While there has been some recent success in solving numerically partial differential equations (PDEs) in high dimensions, such computations are prohibitively expensive, and true scaling of general nonlinear PDEs to high dimensions has never been achieved. In this paper, we develop a new method of scaling up physics-informed neural networks (PINNs) to solve arbitrary high-dimensional PDEs. The new method, called Stochastic Dimension Gradient Descent (SDGD), decomposes a gradient of PDEs into pieces corresponding to different dimensions and samples randomly a subset of these dimensional pieces in each iteration of training PINNs. We theoretically prove the convergence guarantee and other desired properties of the proposed method. We experimentally demonstrate that the proposed method allows us to solve many notoriously hard high-dimensional PDEs, including the Hamilton-Jacobi-Bellman (HJB) and the Schr\"{o}dinger equations in thousands of dimensions very fast on a single GPU using the PINNs mesh-free approach. For instance, we solve nontrivial nonlinear PDEs (one HJB equation and one Black-Scholes equation) in 100,000 dimensions in 6 hours on a single GPU using SDGD with PINNs. Since SDGD is a general training methodology of PINNs, SDGD can be applied to any current and future variants of PINNs to scale them up for arbitrary high-dimensional PDEs.
</details>
<details>
<summary>摘要</summary>
科学家们长期面临的困难之一是维度的束缚，即维度的数量的增加会导致计算成本呈指数增长。这使得解决高维度偏微分方程（PDE）变得非常困难，尤其是当维度增加时。虽然在最近的几年中，有些研究人员已经成功地使用数值方法来解决高维度PDE，但这些计算仍然是非常昂贵的，而且真正的维度扩展到高维度PDE还未得到解决。在这篇论文中，我们提出了一种新的方法，即随机维度梯度下降（SDGD），用于扩展物理学 Informed Neural Networks（PINNs）来解决任意高维度PDE。SDGD方法将PDE的梯度分解为不同维度的部分，然后随机选择这些维度的部分进行每次训练PINNs。我们证明了该方法的收敛保证和其他愿望的性质。我们通过实验示例，SDGD方法可以很快地解决许多困难高维度PDE，包括汉密尔顿-雅各布-贝尔曼（HJB）和施罗德Equation在千个维度中的解决。例如，我们在6个GPU上使用SDGD方法和PINNs无格法解决100,000维度的非线性PDE，需要6个小时。由于SDGD是PINNs的一种普适的训练方法，因此SDGD可以应用于任何当前和未来的PINNs变体，以扩展它们到任意高维度PDE。
</details></li>
</ul>
<hr>
<h2 id="Physics-Informed-Machine-Learning-of-Argon-Gas-Driven-Melt-Pool-Dynamics"><a href="#Physics-Informed-Machine-Learning-of-Argon-Gas-Driven-Melt-Pool-Dynamics" class="headerlink" title="Physics-Informed Machine Learning of Argon Gas-Driven Melt Pool Dynamics"></a>Physics-Informed Machine Learning of Argon Gas-Driven Melt Pool Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12304">http://arxiv.org/abs/2307.12304</a></li>
<li>repo_url: None</li>
<li>paper_authors: R. Sharma, W. Grace Guo, M. Raissi, Y. B. Guo</li>
<li>For: 这篇论文旨在提出一种基于物理定律的机器学习方法（Physics-Informed Machine Learning，PIML），用于预测鋳件运动动力学（melt pool dynamics），并且不需要训练资料。* Methods: 这篇论文使用了一种整合神经网络与物理定律的方法，即physics-informed neural network（PINN），并且将模型常数通过数据验证。* Results: 这篇论文获得了一个高效的预测模型，可以预测鋳件运动动力学的温度、流速和压力等变量，而且不需要解决非线性的奈特-斯托克斯方程。<details>
<summary>Abstract</summary>
Melt pool dynamics in metal additive manufacturing (AM) is critical to process stability, microstructure formation, and final properties of the printed materials. Physics-based simulation including computational fluid dynamics (CFD) is the dominant approach to predict melt pool dynamics. However, the physics-based simulation approaches suffer from the inherent issue of very high computational cost. This paper provides a physics-informed machine learning (PIML) method by integrating neural networks with the governing physical laws to predict the melt pool dynamics such as temperature, velocity, and pressure without using any training data on velocity. This approach avoids solving the highly non-linear Navier-Stokes equation numerically, which significantly reduces the computational cost. The difficult-to-determine model constants of the governing equations of the melt pool can also be inferred through data-driven discovery. In addition, the physics-informed neural network (PINN) architecture has been optimized for efficient model training. The data-efficient PINN model is attributed to the soft penalty by incorporating governing partial differential equations (PDEs), initial conditions, and boundary conditions in the PINN model.
</details>
<details>
<summary>摘要</summary>
metal 添加制造（AM）中的熔Pool动力学是关键的过程稳定性、微structure形成和打印材料的最终性能。物理基础的计算 fluid dynamics（CFD）是预测熔Pool动力学的主要方法。然而，物理基础的计算方法受到非常高的计算成本的限制。本文提出了一种基于物理学习（PIML）方法，通过结合神经网络和管理物理法律来预测熔Pool动力学特性，包括温度、速度和压力，不需要使用任何速度训练数据。这种方法可以避免解决非线性的 Navier-Stokes 方程 numerically，从而减少计算成本。此外，通过数据驱动发现，可以通过迁移学习来推导Difficult-to-determine模型常数。此外， physics-informed neural network（PINN）的建立也得到了优化。数据效率的 PINN 模型归功于软罚 penalty，该 penalty包括 governing partial differential equations（PDEs）、初始条件和边界条件。
</details></li>
</ul>
<hr>
<h2 id="RANSAC-NN-Unsupervised-Image-Outlier-Detection-using-RANSAC"><a href="#RANSAC-NN-Unsupervised-Image-Outlier-Detection-using-RANSAC" class="headerlink" title="RANSAC-NN: Unsupervised Image Outlier Detection using RANSAC"></a>RANSAC-NN: Unsupervised Image Outlier Detection using RANSAC</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12301">http://arxiv.org/abs/2307.12301</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mxtsai/ransac-nn">https://github.com/mxtsai/ransac-nn</a></li>
<li>paper_authors: Chen-Han Tsai, Yu-Shao Peng</li>
<li>For: 本文提出了一种特有的无监督图像异常点检测算法RANSAC-NN，用于确保计算机视觉任务中图像数据的质量和准确性。* Methods: 本文使用了RANSAC方法，对图像进行比较，自动预测每个图像的异常分数，无需额外训练或标签信息。* Results: 对于15种多样化的数据集，RANSAC-NN在与状态对照算法进行比较中，无需调整任何超参数，一直表现出优异的成绩。此外，文章还提供了对每个RANSAC-NN组件的详细分析，以及其应用于图像涂抹检测的可能性。<details>
<summary>Abstract</summary>
Image outlier detection (OD) is crucial for ensuring the quality and accuracy of image datasets used in computer vision tasks. The majority of OD algorithms, however, have not been targeted toward image data. Consequently, the results of applying such algorithms to images are often suboptimal. In this work, we propose RANSAC-NN, a novel unsupervised OD algorithm specifically designed for images. By comparing images in a RANSAC-based approach, our algorithm automatically predicts the outlier score of each image without additional training or label information. We evaluate RANSAC-NN against state-of-the-art OD algorithms on 15 diverse datasets. Without any hyperparameter tuning, RANSAC-NN consistently performs favorably in contrast to other algorithms in almost every dataset category. Furthermore, we provide a detailed analysis to understand each RANSAC-NN component, and we demonstrate its potential applications in image mislabeled detection. Code for RANSAC-NN is provided at https://github.com/mxtsai/ransac-nn
</details>
<details>
<summary>摘要</summary>
Image outlier detection (OD) 是计算机视觉任务中关键的质量和准确性的保证。大多数OD算法却没有专门针对图像数据进行设计，因此在应用这些算法到图像时，结果通常不佳。在这种工作中，我们提出了RANSAC-NN，一种新的无监督OD算法，专门为图像设计。我们通过比较图像在RANSAC基于的方法中，自动地预测每个图像的异常分数，无需其他训练或标签信息。我们对RANSAC-NN与当前最佳OD算法进行比较，在15个多样化的数据集上进行评估。无需任何超参数调整，RANSAC-NN在大多数数据集类别中一直表现出优于其他算法。此外，我们还提供了每个RANSAC-NN组件的详细分析，并证明其在图像涂抹检测中的潜在应用。RANSAC-NN的代码可以在https://github.com/mxtsai/ransac-nn中找到。
</details></li>
</ul>
<hr>
<h2 id="ResWCAE-Biometric-Pattern-Image-Denoising-Using-Residual-Wavelet-Conditioned-Autoencoder"><a href="#ResWCAE-Biometric-Pattern-Image-Denoising-Using-Residual-Wavelet-Conditioned-Autoencoder" class="headerlink" title="ResWCAE: Biometric Pattern Image Denoising Using Residual Wavelet-Conditioned Autoencoder"></a>ResWCAE: Biometric Pattern Image Denoising Using Residual Wavelet-Conditioned Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12255">http://arxiv.org/abs/2307.12255</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youzhi Liang, Wen Liang</li>
<li>For: 这种论文主要用于提高Internet of Things（IoT）设备中的生物 metric验证系统的可靠性，特别是在高噪音水平下。* Methods: 该论文提出了一种轻量级和可靠的深度学习架构，即Residual Wavelet-Conditioned Convolutional Autoencoder（Res-WCAE），用于生物指纹图像的降噪。Res-WCAE包括两个编码器和一个解码器，其中一个编码器是图像编码器，另一个编码器是波峰编码器，用于在卷积变换Domain中使用抽象和细节子图像来取得特征表示。* Results: 实验结果表明，Res-WCAE比其他state-of-the-art降噪方法更高效，特别是在高噪音水平下的 heavily degraded fingerprint images。总的来说，Res-WCAE显示出在compact IoT设备中的潜在应用价值。<details>
<summary>Abstract</summary>
The utilization of biometric authentication with pattern images is increasingly popular in compact Internet of Things (IoT) devices. However, the reliability of such systems can be compromised by image quality issues, particularly in the presence of high levels of noise. While state-of-the-art deep learning algorithms designed for generic image denoising have shown promise, their large number of parameters and lack of optimization for unique biometric pattern retrieval make them unsuitable for these devices and scenarios. In response to these challenges, this paper proposes a lightweight and robust deep learning architecture, the Residual Wavelet-Conditioned Convolutional Autoencoder (Res-WCAE) with a Kullback-Leibler divergence (KLD) regularization, designed specifically for fingerprint image denoising. Res-WCAE comprises two encoders - an image encoder and a wavelet encoder - and one decoder. Residual connections between the image encoder and decoder are leveraged to preserve fine-grained spatial features, where the bottleneck layer conditioned on the compressed representation of features obtained from the wavelet encoder using approximation and detail subimages in the wavelet-transform domain. The effectiveness of Res-WCAE is evaluated against several state-of-the-art denoising methods, and the experimental results demonstrate that Res-WCAE outperforms these methods, particularly for heavily degraded fingerprint images in the presence of high levels of noise. Overall, Res-WCAE shows promise as a solution to the challenges faced by biometric authentication systems in compact IoT devices.
</details>
<details>
<summary>摘要</summary>
“在 compact 互联网络设备中，使用生物特征识别技术的应用正在增加。然而，这些系统的可靠性可能受到图像质量问题的影响，特别是在高水平的噪声存在下。现有的深度学习算法，特别是适应器的深度学习算法，在涉及到通用图像颜色化的应用中已经显示了损害。这些算法的问题在于它们的参数数量过多，并且不适合专门针对生物特征图像恢复。面对这些挑战，本文提出了一个轻量级和可靠的深度学习架构——差异条件条件径缩对称卷积推断器（Res-WCAE）。Res-WCAE 包括两个Encoder——图像Encoder和wavelet Encoder——以及一个解oder。在图像Encoder和解oder之间的径缩连接，使得精细空间特征保留了，并且使用径缩后的特征表示进行条件径缩。实验结果显示，Res-WCAE 在高水平的噪声下进行图像恢复时表现更好，特别是针对严重损害的生物特征图像。总的来说，Res-WCAE 显示出在 compact IoT 设备中的应用潜力。”
</details></li>
</ul>
<hr>
<h2 id="Explainable-Depression-Detection-via-Head-Motion-Patterns"><a href="#Explainable-Depression-Detection-via-Head-Motion-Patterns" class="headerlink" title="Explainable Depression Detection via Head Motion Patterns"></a>Explainable Depression Detection via Head Motion Patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12241">http://arxiv.org/abs/2307.12241</a></li>
<li>repo_url: None</li>
<li>paper_authors: Monika Gahalawat, Raul Fernandez Rojas, Tanaya Guha, Ramanathan Subramanian, Roland Goecke</li>
<li>for: 检测抑郁症状</li>
<li>methods: 基于基本头部动作单元（kinemes）的方法，包括从头部动作数据中挖掘kinemes以及从健康控制人群中学习kineme Patterns，并使用机器学习方法进行评估</li>
<li>results: 研究发现，头部动作 patrterns 可以作为抑郁症状的生物标志，并且可以通过分析重建错误的统计来进行识别。在 BlackDog 和 AVEC2013 数据集上，我们实现了最高的 F1 分数为 0.79 和 0.82，分别为二分类 episodic 薄片和视频中的抑郁症状识别。<details>
<summary>Abstract</summary>
While depression has been studied via multimodal non-verbal behavioural cues, head motion behaviour has not received much attention as a biomarker. This study demonstrates the utility of fundamental head-motion units, termed \emph{kinemes}, for depression detection by adopting two distinct approaches, and employing distinctive features: (a) discovering kinemes from head motion data corresponding to both depressed patients and healthy controls, and (b) learning kineme patterns only from healthy controls, and computing statistics derived from reconstruction errors for both the patient and control classes. Employing machine learning methods, we evaluate depression classification performance on the \emph{BlackDog} and \emph{AVEC2013} datasets. Our findings indicate that: (1) head motion patterns are effective biomarkers for detecting depressive symptoms, and (2) explanatory kineme patterns consistent with prior findings can be observed for the two classes. Overall, we achieve peak F1 scores of 0.79 and 0.82, respectively, over BlackDog and AVEC2013 for binary classification over episodic \emph{thin-slices}, and a peak F1 of 0.72 over videos for AVEC2013.
</details>
<details>
<summary>摘要</summary>
“而对于抑郁症的研究，头部运动讯号并未获得很多注意。本研究表明了抑郁检测的有用性，通过使用头部运动中的基本单元（称为“运动单元”），并运用了不同的方法和特征。我们使用机器学习方法进行评估，在“BlackDog”和“AVEC2013”数据集上进行分类。我们的结果显示：（1）头部运动模式是抑郁症的有效生物 marker，（2）可以获得解释性的运动单元模式，与先前的发现一致。总的来说，我们在“BlackDog”和“AVEC2013”上取得了最高的 F1 分数，分别为 0.79 和 0.82，以及“AVEC2013”上的影片分类中的最高 F1 分数为 0.72。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. The traditional Chinese form of the text is also provided for reference:“而对于抑郁症的研究，头部运动讯号并未获得很多注意。本研究表明了抑郁检测的有用性，通过使用头部运动中的基本单元（称为“运动单元”），并运用了不同的方法和特征。我们使用机器学习方法进行评估，在“BlackDog”和“AVEC2013”数据集上进行分类。我们的结果显示：（1）头部运动模式是抑郁症的有效生物 marker，（2）可以获得解释性的运动单元模式，与先前的发现一致。总的来说，我们在“BlackDog”和“AVEC2013”上取得了最高的 F1 分数，分别为 0.79 和 0.82，以及“AVEC2013”上的影片分类中的最高 F1 分数为 0.72。”
</details></li>
</ul>
<hr>
<h2 id="Demonstration-of-a-Response-Time-Based-Remaining-Useful-Life-RUL-Prediction-for-Software-Systems"><a href="#Demonstration-of-a-Response-Time-Based-Remaining-Useful-Life-RUL-Prediction-for-Software-Systems" class="headerlink" title="Demonstration of a Response Time Based Remaining Useful Life (RUL) Prediction for Software Systems"></a>Demonstration of a Response Time Based Remaining Useful Life (RUL) Prediction for Software Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12237">http://arxiv.org/abs/2307.12237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ray Islam, Peter Sandborn</li>
<li>for: 这个论文旨在应用PHM概念来预测软件系统的故障和寿命。</li>
<li>methods: 该论文使用了usage参数（如发布数量和类型）和性能参数（如响应时间）来预测软件系统的RUL。</li>
<li>results: 实际数据和预测模型的比较表明，PHM概念可以成功地应用于软件系统，并且可以计算出系统的寿命来做管理决策。<details>
<summary>Abstract</summary>
Prognostic and Health Management (PHM) has been widely applied to hardware systems in the electronics and non-electronics domains but has not been explored for software. While software does not decay over time, it can degrade over release cycles. Software health management is confined to diagnostic assessments that identify problems, whereas prognostic assessment potentially indicates when in the future a problem will become detrimental. Relevant research areas such as software defect prediction, software reliability prediction, predictive maintenance of software, software degradation, and software performance prediction, exist, but all of these represent diagnostic models built upon historical data, none of which can predict an RUL for software. This paper addresses the application of PHM concepts to software systems for fault predictions and RUL estimation. Specifically, this paper addresses how PHM can be used to make decisions for software systems such as version update and upgrade, module changes, system reengineering, rejuvenation, maintenance scheduling, budgeting, and total abandonment. This paper presents a method to prognostically and continuously predict the RUL of a software system based on usage parameters (e.g., the numbers and categories of releases) and performance parameters (e.g., response time). The model developed has been validated by comparing actual data, with the results that were generated by predictive models. Statistical validation (regression validation, and k-fold cross validation) has also been carried out. A case study, based on publicly available data for the Bugzilla application is presented. This case study demonstrates that PHM concepts can be applied to software systems and RUL can be calculated to make system management decisions.
</details>
<details>
<summary>摘要</summary>
《预测和软件健康管理（PHM）在硬件系统中广泛应用，但尚未探讨软件。虽然软件不会逐渐衰老，但可能会逐渐下降。软件健康管理仅仅是诊断评估，而预测评估可能可以预测未来何时会出现问题。相关的研究领域包括软件缺陷预测、软件可靠性预测、软件维护预测、软件衰老和软件性能预测，但这些都是基于历史数据建立的诊断模型，无法预测软件的寿命。本文应用PHM概念到软件系统，以预测问题和寿命。本文讨论了如何使用PHM来做软件系统的决策，包括版本更新和升级、模块更改、系统重新设计、重生、维护计划、预算和完全抛弃。本文提出了一种基于使用量和性能参数（例如版本数和响应时间）的软件系统的RUL预测方法。该模型已经验证了实际数据，并通过比较预测模型生成的结果进行了统计验证（回归验证和kfold横断验证）。一个基于公共可用数据的 Bugzilla 应用的案例研究也被提供，这种案例展示了PHM概念可以应用于软件系统，并且可以计算出软件系统的寿命以进行系统管理决策。
</details></li>
</ul>
<hr>
<h2 id="Multi-Modal-Machine-Learning-for-Assessing-Gaming-Skills-in-Online-Streaming-A-Case-Study-with-CS-GO"><a href="#Multi-Modal-Machine-Learning-for-Assessing-Gaming-Skills-in-Online-Streaming-A-Case-Study-with-CS-GO" class="headerlink" title="Multi-Modal Machine Learning for Assessing Gaming Skills in Online Streaming: A Case Study with CS:GO"></a>Multi-Modal Machine Learning for Assessing Gaming Skills in Online Streaming: A Case Study with CS:GO</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12236">http://arxiv.org/abs/2307.12236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Longxiang Zhang, Wenping Wang</li>
<li>for: 本研究旨在提供个性化推荐和服务促销给在线流媒体平台的用户。</li>
<li>methods: 本研究使用最新的端到端模型学习joint representation of multiple modalities，并进行了大量的实验 validate其效果。</li>
<li>results: 研究发现，提议的模型能够学习有意义的表示，但同时也具有识别用户的问题。未来工作将集中在解决这一问题上。<details>
<summary>Abstract</summary>
Online streaming is an emerging market that address much attention. Assessing gaming skills from videos is an important task for streaming service providers to discover talented gamers. Service providers require the information to offer customized recommendation and service promotion to their customers. Meanwhile, this is also an important multi-modal machine learning tasks since online streaming combines vision, audio and text modalities. In this study we begin by identifying flaws in the dataset and proceed to clean it manually. Then we propose several variants of latest end-to-end models to learn joint representation of multiple modalities. Through our extensive experimentation, we demonstrate the efficacy of our proposals. Moreover, we identify that our proposed models is prone to identifying users instead of learning meaningful representations. We purpose future work to address the issue in the end.
</details>
<details>
<summary>摘要</summary>
互联网直播是一个崛起的市场，吸引了很多注意。从视频中评估玩家技巧是直播服务提供商必须的重要任务，以便为客户提供个性化推荐和服务推广。同时，这也是一个重要的多modal机器学习任务，因为在线直播结合了视觉、音频和文本modalities。在这种研究中，我们开始由 dataset 中的欠点进行识别，然后手动清理它。然后，我们提出了一些最新的终端模型，以学习多modalities 的共同表示。通过我们的广泛实验，我们证明了我们的建议的有效性。此外，我们发现我们的建议模型容易被用户认出，而不是学习有意义的表示。我们未来的工作是解决这个问题。
</details></li>
</ul>
<hr>
<h2 id="EchoGLAD-Hierarchical-Graph-Neural-Networks-for-Left-Ventricle-Landmark-Detection-on-Echocardiograms"><a href="#EchoGLAD-Hierarchical-Graph-Neural-Networks-for-Left-Ventricle-Landmark-Detection-on-Echocardiograms" class="headerlink" title="EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms"></a>EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12229">http://arxiv.org/abs/2307.12229</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/masoudmo/echoglad">https://github.com/masoudmo/echoglad</a></li>
<li>paper_authors: Masoud Mokhtari, Mobina Mahdavi, Hooman Vaseli, Christina Luong, Purang Abolmaesumi, Teresa S. M. Tsang, Renjie Liao</li>
<li>for: 本研究旨在自动检测心脏左心室的四个标志位置以及计算左心室内部积分和周围肌肉约重。</li>
<li>methods: 我们提出了一种基于电子心室图像的层次图神经网络（GNN）方法，即EchoGLAD，以实现左心室标志位置检测。我们还引入了多层级指导学习框架，以实现多resolution的标志位置检测。</li>
<li>results: 我们在公共和私人数据集上进行了测试，并在ID和OOD两种设定下测试了我们的模型。在ID设定下，我们实现了左心室标志位置检测的最佳约错值（MAE）为1.46毫米和1.86毫米。而在OOD设定下，我们的模型表现比之前的方法更好，测试MAE为4.3毫米。<details>
<summary>Abstract</summary>
The functional assessment of the left ventricle chamber of the heart requires detecting four landmark locations and measuring the internal dimension of the left ventricle and the approximate mass of the surrounding muscle. The key challenge of automating this task with machine learning is the sparsity of clinical labels, i.e., only a few landmark pixels in a high-dimensional image are annotated, leading many prior works to heavily rely on isotropic label smoothing. However, such a label smoothing strategy ignores the anatomical information of the image and induces some bias. To address this challenge, we introduce an echocardiogram-based, hierarchical graph neural network (GNN) for left ventricle landmark detection (EchoGLAD). Our main contributions are: 1) a hierarchical graph representation learning framework for multi-resolution landmark detection via GNNs; 2) induced hierarchical supervision at different levels of granularity using a multi-level loss. We evaluate our model on a public and a private dataset under the in-distribution (ID) and out-of-distribution (OOD) settings. For the ID setting, we achieve the state-of-the-art mean absolute errors (MAEs) of 1.46 mm and 1.86 mm on the two datasets. Our model also shows better OOD generalization than prior works with a testing MAE of 4.3 mm.
</details>
<details>
<summary>摘要</summary>
心脏左心室功能评估需要检测四个标志点和测量左心室内部维度以及周围肌肉的大约质量。主要挑战是通过机器学习自动化这项任务的挑战是仅有一些临床标签，即只有一些标志点像素在高维度图像上被标注，导致许多先前的工作都是依赖于均匀标签平滑化。然而，这种标签平滑化策略忽略了图像的解剖学信息并且带来一定的偏见。为解决这个挑战，我们提出了一种用电声图像基于的层次图像神经网络（EchoGLAD） для左心室标志点检测。我们的主要贡献包括：1. 一种层次图像表示学习框架，通过图像神经网络来实现多分辨率标志点检测。2. 通过多级层次权重来实现层次supervision，以提高模型的精度和一致性。我们在公共和私人数据集上进行了测试，在ID设定下，我们实现了左心室标志点检测的状态各orden MaE值为1.46毫米和1.86毫米。我们的模型还在OOD设定下表现出了更好的泛化性，测试MaE值为4.3毫米。
</details></li>
</ul>
<hr>
<h2 id="The-identification-of-garbage-dumps-in-the-rural-areas-of-Cyprus-through-the-application-of-deep-learning-to-satellite-imagery"><a href="#The-identification-of-garbage-dumps-in-the-rural-areas-of-Cyprus-through-the-application-of-deep-learning-to-satellite-imagery" class="headerlink" title="The identification of garbage dumps in the rural areas of Cyprus through the application of deep learning to satellite imagery"></a>The identification of garbage dumps in the rural areas of Cyprus through the application of deep learning to satellite imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02502">http://arxiv.org/abs/2308.02502</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew Keith Wilkinson</li>
<li>For: The paper aims to address the issue of illegal garbage dumping in rural areas of Cyprus by using artificial intelligence techniques and satellite imagery to identify garbage dumps.* Methods: The authors collected a novel dataset of images and used data augmentation techniques to increase the size of the dataset. They trained an artificial neural network, specifically a convolutional neural network, to recognize the presence or absence of garbage in new images.* Results: The resulting model was able to correctly identify images containing garbage in approximately 90% of cases, demonstrating the efficacy of the approach. The authors envision that this model could form the basis of a future system that could systematically analyze the entire landscape of Cyprus to build a comprehensive “garbage” map of the island.Here’s the same information in Simplified Chinese:*  FOR: 这篇论文目标是解决Cyprus遍布的非法垃圾排放问题，使用人工智能技术和卫星影像来识别垃圾场。*  METHODS: 作者收集了一个新的图像集，并使用数据扩展技术来增加该集的大小。他们使用人工神经网络，特别是卷积神经网络，来识别新图像中是否存在垃圾。*  RESULTS: 结果表明，该模型可以正确地识别新图像中是否存在垃圾，达到了约90%的准确率。作者认为，这种方法可以成为未来系统性地分析Cyprus岛全域的垃圾地图的基础。<details>
<summary>Abstract</summary>
Garbage disposal is a challenging problem throughout the developed world. In Cyprus, as elsewhere, illegal ``fly-tipping" is a significant issue, especially in rural areas where few legal garbage disposal options exist. However, there is a lack of studies that attempt to measure the scale of this problem, and few resources available to address it. A method of automating the process of identifying garbage dumps would help counter this and provide information to the relevant authorities. The aim of this study was to investigate the degree to which artificial intelligence techniques, together with satellite imagery, can be used to identify illegal garbage dumps in the rural areas of Cyprus. This involved collecting a novel dataset of images that could be categorised as either containing, or not containing, garbage. The collection of such datasets in sufficient raw quantities is time consuming and costly. Therefore a relatively modest baseline set of images was collected, then data augmentation techniques used to increase the size of this dataset to a point where useful machine learning could occur. From this set of images an artificial neural network was trained to recognise the presence or absence of garbage in new images. A type of neural network especially suited to this task known as ``convolutional neural networks" was used. The efficacy of the resulting model was evaluated using an independently collected dataset of test images. The result was a deep learning model that could correctly identify images containing garbage in approximately 90\% of cases. It is envisaged that this model could form the basis of a future system that could systematically analyse the entire landscape of Cyprus to build a comprehensive ``garbage" map of the island.
</details>
<details>
<summary>摘要</summary>
垃圾处理是发达国家的挑战之一，特别是在贫困地区，因为有限的法定垃圾处理选择。然而，没有多少研究尝试度量这个问题的规模，以及有效的解决方案。这项研究的目的是使用人工智能技术和卫星影像来识别Cyprus郊区非法抛弃垃圾。这包括收集一个新的图像集，可以分为含有或无垃圾。收集这些图像集的时间和成本很高，因此只收集了一个有限的基线集。然后使用数据扩充技术来增加这个集的大小，使其具有可行的机器学习。通过这个集，我们训练了一个人工神经网络，可以识别新图像中是否含有垃圾。我们使用了适合这个任务的特殊类型的神经网络，即卷积神经网络。我们评估了这个模型的效果，使用独立收集的测试集。结果是，这个模型可以在90%的情况下正确地识别含有垃圾的图像。我们想象，这个模型可以成为未来对Cyprus岛全景进行系统性分析，建立一个“垃圾”地图。
</details></li>
</ul>
<hr>
<h2 id="Geometry-Aware-Adaptation-for-Pretrained-Models"><a href="#Geometry-Aware-Adaptation-for-Pretrained-Models" class="headerlink" title="Geometry-Aware Adaptation for Pretrained Models"></a>Geometry-Aware Adaptation for Pretrained Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12226">http://arxiv.org/abs/2307.12226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicholas Roberts, Xintong Li, Dyah Adila, Sonia Cromp, Tzu-Heng Huang, Jitian Zhao, Frederic Sala</li>
<li>for: 这个论文是为了提高零下学习模型的性能而设计的，特别是在标签空间中的类别数量很大的情况下。</li>
<li>methods: 这个论文使用了一种简单的方法，即在标签空间中使用Fréchetmean来代替标准的argmax预测规则，以提高模型的预测性能。</li>
<li>results: 该方法可以在ImageNet上获得最高达29.7%的相对提升，并且可以扩展到数万个类别。在没有外部度量的情况下，使用自动生成的metric自Embeddings中可以获得10.5%的提升。<details>
<summary>Abstract</summary>
Machine learning models -- including prominent zero-shot models -- are often trained on datasets whose labels are only a small proportion of a larger label space. Such spaces are commonly equipped with a metric that relates the labels via distances between them. We propose a simple approach to exploit this information to adapt the trained model to reliably predict new classes -- or, in the case of zero-shot prediction, to improve its performance -- without any additional training. Our technique is a drop-in replacement of the standard prediction rule, swapping argmax with the Fr\'echet mean. We provide a comprehensive theoretical analysis for this approach, studying (i) learning-theoretic results trading off label space diameter, sample complexity, and model dimension, (ii) characterizations of the full range of scenarios in which it is possible to predict any unobserved class, and (iii) an optimal active learning-like next class selection procedure to obtain optimal training classes for when it is not possible to predict the entire range of unobserved classes. Empirically, using easily-available external metrics, our proposed approach, Loki, gains up to 29.7% relative improvement over SimCLR on ImageNet and scales to hundreds of thousands of classes. When no such metric is available, Loki can use self-derived metrics from class embeddings and obtains a 10.5% improvement on pretrained zero-shot models such as CLIP.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Improving-Out-of-Distribution-Robustness-of-Classifiers-via-Generative-Interpolation"><a href="#Improving-Out-of-Distribution-Robustness-of-Classifiers-via-Generative-Interpolation" class="headerlink" title="Improving Out-of-Distribution Robustness of Classifiers via Generative Interpolation"></a>Improving Out-of-Distribution Robustness of Classifiers via Generative Interpolation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12219">http://arxiv.org/abs/2307.12219</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoyue Bai, Ceyuan Yang, Yinghao Xu, S. -H. Gary Chan, Bolei Zhou</li>
<li>for: 该研究旨在提高神经网络模型对不同分布数据的鲁棒性。</li>
<li>methods: 该研究使用生成模型作为数据增强来源，通过混合多个领域生成器来生成多样化的异常样本。</li>
<li>results: 实验显示，提posed方法可以Explicitly increases the diversity of training domains and achieves consistent improvements over baselines across datasets and multiple different distribution shifts.<details>
<summary>Abstract</summary>
Deep neural networks achieve superior performance for learning from independent and identically distributed (i.i.d.) data. However, their performance deteriorates significantly when handling out-of-distribution (OoD) data, where the training and test are drawn from different distributions. In this paper, we explore utilizing the generative models as a data augmentation source for improving out-of-distribution robustness of neural classifiers. Specifically, we develop a simple yet effective method called Generative Interpolation to fuse generative models trained from multiple domains for synthesizing diverse OoD samples. Training a generative model directly on the source domains tends to suffer from mode collapse and sometimes amplifies the data bias. Instead, we first train a StyleGAN model on one source domain and then fine-tune it on the other domains, resulting in many correlated generators where their model parameters have the same initialization thus are aligned. We then linearly interpolate the model parameters of the generators to spawn new sets of generators. Such interpolated generators are used as an extra data augmentation source to train the classifiers. The interpolation coefficients can flexibly control the augmentation direction and strength. In addition, a style-mixing mechanism is applied to further improve the diversity of the generated OoD samples. Our experiments show that the proposed method explicitly increases the diversity of training domains and achieves consistent improvements over baselines across datasets and multiple different distribution shifts.
</details>
<details>
<summary>摘要</summary>
Training a generative model directly on the source domains can suffer from mode collapse and amplify data bias. Instead, we first train a StyleGAN model on one source domain and then fine-tune it on the other domains, resulting in many correlated generators with aligned model parameters. We then linearly interpolate the model parameters of the generators to spawn new sets of generators. Such interpolated generators are used as an extra data augmentation source to train the classifiers. The interpolation coefficients can flexibly control the augmentation direction and strength.In addition, we apply a style-mixing mechanism to further improve the diversity of the generated OoD samples. Our experiments show that the proposed method explicitly increases the diversity of training domains and achieves consistent improvements over baselines across datasets and multiple different distribution shifts.
</details></li>
</ul>
<hr>
<h2 id="Mental-Workload-Estimation-with-Electroencephalogram-Signals-by-Combining-Multi-Space-Deep-Models"><a href="#Mental-Workload-Estimation-with-Electroencephalogram-Signals-by-Combining-Multi-Space-Deep-Models" class="headerlink" title="Mental Workload Estimation with Electroencephalogram Signals by Combining Multi-Space Deep Models"></a>Mental Workload Estimation with Electroencephalogram Signals by Combining Multi-Space Deep Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02409">http://arxiv.org/abs/2308.02409</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hong-Hai Nguyen, Ngumimi Karen Iyortsuun, Hyung-Jeong Yang, Guee-Sang Lee, Soo-Hyung Kim</li>
<li>for: This paper aims to classify mental workload into three states and estimate continuum levels to improve early detection of mental health problems.</li>
<li>methods: The method combines multiple dimensions of space and uses Temporal Convolutional Networks in the time domain, as well as a new architecture called the Multi-Dimensional Residual Block in the frequency domain.</li>
<li>results: The proposed method is expected to provide accurate estimates of mental workload and improve the early detection of mental health problems.Here’s the text in Simplified Chinese:</li>
<li>for: 这篇论文目标是分类心理工作负荷为三种状态并估算连续水平，以提高早期心理健康问题的检测。</li>
<li>methods: 该方法结合了多个空间维度，使用时域卷积神经网络和频域中的新架构——多维度剩余块，以获得最佳的心理估算结果。</li>
<li>results: 该方法预期能够提供准确的心理工作负荷估算结果，并且有助于早期检测心理健康问题。<details>
<summary>Abstract</summary>
The human brain is in a continuous state of activity during both work and rest. Mental activity is a daily process, and when the brain is overworked, it can have negative effects on human health. In recent years, great attention has been paid to early detection of mental health problems because it can help prevent serious health problems and improve quality of life. Several signals are used to assess mental state, but the electroencephalogram (EEG) is widely used by researchers because of the large amount of information it provides about the brain. This paper aims to classify mental workload into three states and estimate continuum levels. Our method combines multiple dimensions of space to achieve the best results for mental estimation. In the time domain approach, we use Temporal Convolutional Networks, and in the frequency domain, we propose a new architecture called the Multi-Dimensional Residual Block, which combines residual blocks.
</details>
<details>
<summary>摘要</summary>
人脑在工作和休息时都处于不断活动的状态。 mental activity是每天的过程，当脑部过度劳累时，可能会对人类健康产生负面影响。 recent years，大量关注 Early Detection of mental health problems，因为这可以帮助预防严重的健康问题并提高生活质量。 Several signals are used to assess mental state，但是 electroencephalogram (EEG) 是研究人员最广泛使用的，因为它可以提供大量关于脑部的信息。 This paper aims to classify mental workload into three states and estimate continuum levels. Our method combines multiple dimensions of space to achieve the best results for mental estimation. In the time domain approach, we use Temporal Convolutional Networks，and in the frequency domain, we propose a new architecture called the Multi-Dimensional Residual Block，which combines residual blocks.
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Agents-For-Attacking-Inaudible-Voice-Activated-Devices"><a href="#Adversarial-Agents-For-Attacking-Inaudible-Voice-Activated-Devices" class="headerlink" title="Adversarial Agents For Attacking Inaudible Voice Activated Devices"></a>Adversarial Agents For Attacking Inaudible Voice Activated Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12204">http://arxiv.org/abs/2307.12204</a></li>
<li>repo_url: None</li>
<li>paper_authors: Forrest McKee, David Noever</li>
<li>For: The paper focuses on the risk of inaudible attacks on voice-activated devices and the need for new cybersecurity measures to address this emerging threat.* Methods: The authors use reinforcement learning to analyze the vulnerability of novel Internet of Things (IoT) configurations to inaudible attacks, and they evaluate six reinforcement learning algorithms to determine the most effective approach.* Results: The authors find that Deep-Q learning with exploitation is the most effective algorithm for rapidly owning all nodes in a baseline network model, highlighting the critical need for understanding non-conventional networks and new cybersecurity measures to address the growing threat of inaudible attacks.<details>
<summary>Abstract</summary>
The paper applies reinforcement learning to novel Internet of Thing configurations. Our analysis of inaudible attacks on voice-activated devices confirms the alarming risk factor of 7.6 out of 10, underlining significant security vulnerabilities scored independently by NIST National Vulnerability Database (NVD). Our baseline network model showcases a scenario in which an attacker uses inaudible voice commands to gain unauthorized access to confidential information on a secured laptop. We simulated many attack scenarios on this baseline network model, revealing the potential for mass exploitation of interconnected devices to discover and own privileged information through physical access without adding new hardware or amplifying device skills. Using Microsoft's CyberBattleSim framework, we evaluated six reinforcement learning algorithms and found that Deep-Q learning with exploitation proved optimal, leading to rapid ownership of all nodes in fewer steps. Our findings underscore the critical need for understanding non-conventional networks and new cybersecurity measures in an ever-expanding digital landscape, particularly those characterized by mobile devices, voice activation, and non-linear microphones susceptible to malicious actors operating stealth attacks in the near-ultrasound or inaudible ranges. By 2024, this new attack surface might encompass more digital voice assistants than people on the planet yet offer fewer remedies than conventional patching or firmware fixes since the inaudible attacks arise inherently from the microphone design and digital signal processing.
</details>
<details>
<summary>摘要</summary>
文章应用再强化学习来解决新型互联网设备的配置问题。我们对无声攻击的voice-activated设备进行分析，并确认了攻击性能isk因子为7.6/10，这标识了严重的安全漏洞。我们的基线网络模型示例了一个攻击者通过无声语音命令获取未授权访问 laptop 上的机密信息的场景。我们在这个基线网络模型上模拟了许多攻击场景，发现了可以通过物理访问而不需要新硬件或提高设备技能的攻击的潜在性。使用Microsoft的CyberBattleSim框架，我们评估了六种再强化学习算法，发现了使用攻击的 Deep-Q 学习算法，可以在 fewer steps 中获得所有节点的权限。我们的发现强调了理解非传统网络和新的安全措施在不断扩展的数字景观中的重要性，特别是包括移动设备、voice activation和非线性 microphone 在内的攻击 surface。到2024年，这个新的攻击表面可能会包括更多的数字voice助手than人类在地球上， yet offer fewer remedies than conventional patching or firmware fixes since the inaudible attacks arise inherently from the microphone design and digital signal processing。
</details></li>
</ul>
<hr>
<h2 id="NCART-Neural-Classification-and-Regression-Tree-for-Tabular-Data"><a href="#NCART-Neural-Classification-and-Regression-Tree-for-Tabular-Data" class="headerlink" title="NCART: Neural Classification and Regression Tree for Tabular Data"></a>NCART: Neural Classification and Regression Tree for Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12198">http://arxiv.org/abs/2307.12198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaqi Luo, Shixin Xu</li>
<li>for: 本研究旨在提出一种可解释性神经网络模型，以增强深度学习模型在表格数据分析中的可靠性和可读性。</li>
<li>methods: 本研究提出了一种名为神经分类和回归树（NCART）的新型可解释性神经网络模型，它将多个可导式无知树纳入神经网络架构中，以维持神经网络的可读性，同时充分利用神经网络的端到端能力。</li>
<li>results: 对于不同的数据集和深度学习模型，NCART模型在可靠性和可读性两个方面具有突出的优势，并且在大规模数据处理和小规模数据处理中均有出色的表现，比较稳定和可靠。<details>
<summary>Abstract</summary>
Deep learning models have become popular in the analysis of tabular data, as they address the limitations of decision trees and enable valuable applications like semi-supervised learning, online learning, and transfer learning. However, these deep-learning approaches often encounter a trade-off. On one hand, they can be computationally expensive when dealing with large-scale or high-dimensional datasets. On the other hand, they may lack interpretability and may not be suitable for small-scale datasets. In this study, we propose a novel interpretable neural network called Neural Classification and Regression Tree (NCART) to overcome these challenges. NCART is a modified version of Residual Networks that replaces fully-connected layers with multiple differentiable oblivious decision trees. By integrating decision trees into the architecture, NCART maintains its interpretability while benefiting from the end-to-end capabilities of neural networks. The simplicity of the NCART architecture makes it well-suited for datasets of varying sizes and reduces computational costs compared to state-of-the-art deep learning models. Extensive numerical experiments demonstrate the superior performance of NCART compared to existing deep learning models, establishing it as a strong competitor to tree-based models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Monadic-Deep-Learning"><a href="#Monadic-Deep-Learning" class="headerlink" title="Monadic Deep Learning"></a>Monadic Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12187">http://arxiv.org/abs/2307.12187</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ThoughtWorksInc/monadic-deep-learning">https://github.com/ThoughtWorksInc/monadic-deep-learning</a></li>
<li>paper_authors: Bo Yang, Zhihao Zhang Kirisame Marisa, Kai Shi</li>
<li>for: 这篇论文是为了解决在静态类型语言中运行神经网络模型时，拥有自动演算的问题。</li>
<li>methods: 这篇论文使用了一种新的自动演算方法，可以在静态类型函数中包含多个可训练变量，并且可以与元语言进行兼容。它还使用了一些幺征和幺征转换，使用者可以创建具有卷积的表达式，表示动态神经网络。</li>
<li>results: 该论文的实验结果表明，使用DeepLearning.scala可以创建复杂的神经网络表达式，并且仍然保持类型安全。<details>
<summary>Abstract</summary>
The Java and Scala community has built a very successful big data ecosystem. However, most of neural networks running on it are modeled in dynamically typed programming languages. These dynamically typed deep learning frameworks treat neural networks as differentiable expressions that contain many trainable variable, and perform automatic differentiation on those expressions when training them.   Until 2019, none of the learning frameworks in statically typed languages provided the expressive power of traditional frameworks. Their users are not able to use custom algorithms unless creating plenty of boilerplate code for hard-coded back-propagation.   We solved this problem in DeepLearning.scala 2. Our contributions are:   1. We discovered a novel approach to perform automatic differentiation in reverse mode for statically typed functions that contain multiple trainable variable, and can interoperate freely with the metalanguage.   2. We designed a set of monads and monad transformers, which allow users to create monadic expressions that represent dynamic neural networks.   3. Along with these monads, we provide some applicative functors, to perform multiple calculations in parallel.   With these features, users of DeepLearning.scala were able to create complex neural networks in an intuitive and concise way, and still maintain type safety.
</details>
<details>
<summary>摘要</summary>
Java和Scala社区已经建立了一个非常成功的大数据生态系统。然而，大多数在其上运行的神经网络是使用动态类型编程语言进行模型化的。这些动态类型深度学习框架将神经网络视为可导数学表达，并在训练时自动进行差分。 until 2019, 在静态类型语言中的学习框架都没有提供了传统框架的表达力。他们的用户无法使用自定义算法，除非创建大量的硬编码反射分布。我们解决了这个问题，在 DeepLearning.scala 2 中我们的贡献包括：1. 我们发现了一种新的方法，可以在静态类型函数中自动进行差分，并且可以与金属语言进行自由交互。2. 我们设计了一组幂道和幂道变换，允许用户创建幂道表达式，表示动态神经网络。3. 同时，我们提供了一些应用程序赋值，以进行多个计算并行执行。与这些特性相结合，DeepLearning.scala 的用户可以在直观和简洁的方式创建复杂的神经网络，同时保持类型安全性。
</details></li>
</ul>
<hr>
<h2 id="Machine-learning-discovers-invariants-of-braids-and-flat-braids"><a href="#Machine-learning-discovers-invariants-of-braids-and-flat-braids" class="headerlink" title="Machine learning discovers invariants of braids and flat braids"></a>Machine learning discovers invariants of braids and flat braids</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12185">http://arxiv.org/abs/2307.12185</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexei Lisitsa, Mateo Salles, Alexei Vernitski</li>
<li>for: 这个论文用机器学习分类布里耳（或平面布里耳）的例子，并将其分为不重要和重要两类。</li>
<li>methods: 这个论文使用了指导学习，使用神经网络（多层感知器）进行超级vised学习。</li>
<li>results: 通过这个方法，我们能够解释布里耳的结构，并证明它们的结构为真理。最终，我们发现了新的便利的布里耳 invariants，包括完全的平面布里耳 invariants。<details>
<summary>Abstract</summary>
We use machine learning to classify examples of braids (or flat braids) as trivial or non-trivial. Our ML takes form of supervised learning using neural networks (multilayer perceptrons). When they achieve good results in classification, we are able to interpret their structure as mathematical conjectures and then prove these conjectures as theorems. As a result, we find new convenient invariants of braids, including a complete invariant of flat braids.
</details>
<details>
<summary>摘要</summary>
我们使用机器学习来分类拥有别种辫的示例（或平面辫）为轻量级或非轻量级。我们的ML形式为指导学习使用神经网络（多层感知器）。当它们在分类中获得良好的结果时，我们可以解释它们的结构为数学 conjecture，然后证明这些推论为定理。因此，我们发现新的便利的辫 invariants，包括完整的平面辫 invariants。Note: "辫" (braids) is a word that is commonly used in Chinese to refer to braids, but it can also have other meanings depending on the context. In this translation, I have used the more specific term "平面辫" (flat braids) to refer to the specific type of braids being discussed in the text.
</details></li>
</ul>
<hr>
<h2 id="Prototype-Driven-and-Multi-Expert-Integrated-Multi-Modal-MR-Brain-Tumor-Image-Segmentation"><a href="#Prototype-Driven-and-Multi-Expert-Integrated-Multi-Modal-MR-Brain-Tumor-Image-Segmentation" class="headerlink" title="Prototype-Driven and Multi-Expert Integrated Multi-Modal MR Brain Tumor Image Segmentation"></a>Prototype-Driven and Multi-Expert Integrated Multi-Modal MR Brain Tumor Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12180">http://arxiv.org/abs/2307.12180</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/linzy0227/pdminet">https://github.com/linzy0227/pdminet</a></li>
<li>paper_authors: Yafei Zhang, Zhiyuan Li, Huafeng Li, Dapeng Tao</li>
<li>for: 这种研究旨在提高多模式磁共振（MR）脑肿瘤图像分割方法，以便更好地确定和地理位置脑肿瘤子区域。</li>
<li>methods: 该方法使用脑肿瘤prototype驱动的多专家融合，以高亮每个脑肿瘤子区域的特征。具体来说，我们提出了一种互传机制，将不同模式特征传输给每个另一个，以解决单模式特征缺乏信息的问题。此外，我们还提出了一种基于学习的脑肿瘤代表特征表示和融合方法，将代表特征与脑肿瘤特征相结合，生成相应的活化地图。</li>
<li>results: 对三个竞赛脑肿瘤分割数据集进行实验，研究结果表明，该方法的性能较高，可以更好地确定和地理位置脑肿瘤子区域。<details>
<summary>Abstract</summary>
For multi-modal magnetic resonance (MR) brain tumor image segmentation, current methods usually directly extract the discriminative features from input images for tumor sub-region category determination and localization. However, the impact of information aliasing caused by the mutual inclusion of tumor sub-regions is often ignored. Moreover, existing methods usually do not take tailored efforts to highlight the single tumor sub-region features. To this end, a multi-modal MR brain tumor segmentation method with tumor prototype-driven and multi-expert integration is proposed. It could highlight the features of each tumor sub-region under the guidance of tumor prototypes. Specifically, to obtain the prototypes with complete information, we propose a mutual transmission mechanism to transfer different modal features to each other to address the issues raised by insufficient information on single-modal features. Furthermore, we devise a prototype-driven feature representation and fusion method with the learned prototypes, which implants the prototypes into tumor features and generates corresponding activation maps. With the activation maps, the sub-region features consistent with the prototype category can be highlighted. A key information enhancement and fusion strategy with multi-expert integration is designed to further improve the segmentation performance. The strategy can integrate the features from different layers of the extra feature extraction network and the features highlighted by the prototypes. Experimental results on three competition brain tumor segmentation datasets prove the superiority of the proposed method.
</details>
<details>
<summary>摘要</summary>
为了实现多模态核磁共振（MR）脑肿瘤图像分割，当前方法通常直接从输入图像中提取特征来确定和地理化肿瘤子区域。然而，现有方法通常忽略了信息干扰，这是由肿瘤子区域之间的共包含引起的。此外，现有方法通常不会为单个肿瘤子区域特征进行特化努力。为此，我们提出了一种多模态MR脑肿瘤分割方法，该方法可以根据肿瘤prototype驱动并进行多专家集成。它可以在肿瘤prototype的指导下高亮每个肿瘤子区域的特征。具体来说，为了获得完整信息的肿瘤prototype，我们提出了一种相互传输机制，将不同模态特征传输给每个模式，以解决单模态特征不具备完整信息的问题。此外，我们还提出了一种基于学习的肿瘤prototype驱动特征表示和融合方法，将肿瘤prototype直接植入到肿瘤特征中，并生成相应的活化地图。通过活化地图，可以高亮与肿瘤prototype类别相符的子区域特征。为了进一步提高分割性能，我们还设计了一种多专家集成和信息增强策略，该策略可以集成不同层次的特性提取网络和肿瘤prototype中的特征。实验结果表明，提出的方法在三个竞赛脑肿瘤分割数据集上表现出优异性。
</details></li>
</ul>
<hr>
<h2 id="Learn-to-Compress-LtC-Efficient-Learning-based-Streaming-Video-Analytics"><a href="#Learn-to-Compress-LtC-Efficient-Learning-based-Streaming-Video-Analytics" class="headerlink" title="Learn to Compress (LtC): Efficient Learning-based Streaming Video Analytics"></a>Learn to Compress (LtC): Efficient Learning-based Streaming Video Analytics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12171">http://arxiv.org/abs/2307.12171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quazi Mishkatul Alam, Israat Haque, Nael Abu-Ghazaleh</li>
<li>for: 这篇论文旨在建立一个高效的对当地视频分析管道，以节省网络带宽和电力耗用。</li>
<li>methods: 论文提出了一个名为LtC的协同框架，它在视频来源和分析服务器之间实现了高效的视频流减少。LtC使用了全功能的分析算法作为教师，将视频流分解为不同区域，并将这些区域节省为高质量，而其他区域则进行攻击性压缩。此外，LtC还具有一个基于特征差分的时间滤波器，以删除不具新讯息的帧。</li>
<li>results: LtC比最近发表的流行框架使用28-35% less 网络带宽，具有45%短的响应延迟，而且与最近发表的流行框架相比，LtC的分析性能相似。<details>
<summary>Abstract</summary>
Video analytics are often performed as cloud services in edge settings, mainly to offload computation, and also in situations where the results are not directly consumed at the video sensors. Sending high-quality video data from the edge devices can be expensive both in terms of bandwidth and power use. In order to build a streaming video analytics pipeline that makes efficient use of these resources, it is therefore imperative to reduce the size of the video stream. Traditional video compression algorithms are unaware of the semantics of the video, and can be both inefficient and harmful for the analytics performance. In this paper, we introduce LtC, a collaborative framework between the video source and the analytics server, that efficiently learns to reduce the video streams within an analytics pipeline. Specifically, LtC uses the full-fledged analytics algorithm at the server as a teacher to train a lightweight student neural network, which is then deployed at the video source. The student network is trained to comprehend the semantic significance of various regions within the videos, which is used to differentially preserve the crucial regions in high quality while the remaining regions undergo aggressive compression. Furthermore, LtC also incorporates a novel temporal filtering algorithm based on feature-differencing to omit transmitting frames that do not contribute new information. Overall, LtC is able to use 28-35% less bandwidth and has up to 45% shorter response delay compared to recently published state of the art streaming frameworks while achieving similar analytics performance.
</details>
<details>
<summary>摘要</summary>
视频分析通常作为云服务在边缘设置下进行，主要是为了减轻计算负担，以及在视频感知不直接在视频传感器上进行处理。从边缘设备传输高质量视频数据可能会产生巨大的带宽和电力成本。为建立高效的流动视频分析管道，因此需要减小视频流。传统的视频压缩算法不理解视频的 semantics，可能会导致不具有效果和对分析性能有害。在本文中，我们介绍了 LtC，一个在视频源和分析服务器之间合作的框架。Specifically，LtC 使用全功能的分析算法作为教师，将轻量级神经网络部署到视频源上，以提高分析管道中视频流的压缩率。此外，LtC 还包括一种基于特征差分的新的时间滤波算法，以便忽略不含新信息的帧。总的来说，LtC 可以使用28-35%的带宽和45%的响应延迟，相比最新的流动框架，而达到类似的分析性能。
</details></li>
</ul>
<hr>
<h2 id="Optimized-Network-Architectures-for-Large-Language-Model-Training-with-Billions-of-Parameters"><a href="#Optimized-Network-Architectures-for-Large-Language-Model-Training-with-Billions-of-Parameters" class="headerlink" title="Optimized Network Architectures for Large Language Model Training with Billions of Parameters"></a>Optimized Network Architectures for Large Language Model Training with Billions of Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12169">http://arxiv.org/abs/2307.12169</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiyang Wang, Manya Ghobadi, Kayvon Shakeri, Ying Zhang, Naader Hasani</li>
<li>for: 这个论文挑战了任何到任何网络的训练大语言模型（LLM）的传统模式。</li>
<li>methods: 我们提出了一种新的网络架构，它准确地反映了 LLM 的通信需求。我们将集群分成多个 GPU 之间的非阻塞any-to-any高速连接，称为 HB Domain。只有在 HB Domain 内部的 GPU 之间进行通信。</li>
<li>results: 我们的提议可以减少网络成本达到 75%，而无需妥协 LLM 训练性能。<details>
<summary>Abstract</summary>
This paper challenges the well-established paradigm for building any-to-any networks for training Large Language Models (LLMs). We show that LLMs exhibit a unique communication pattern where only small groups of GPUs require high-bandwidth any-to-any communication within them, to achieve near-optimal training performance. Across these groups of GPUs, the communication is insignificant, sparse, and homogeneous. We propose a new network architecture that closely resembles the communication requirement of LLMs. Our architecture partitions the cluster into sets of GPUs interconnected with non-blocking any-to-any high-bandwidth interconnects that we call HB domains. Across the HB domains, the network only connects GPUs with communication demands. We call this network a "rail-only" connection, and show that our proposed architecture reduces the network cost by up to 75% compared to the state-of-the-art any-to-any Clos networks without compromising the performance of LLM training.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Facial-Point-Graphs-for-Amyotrophic-Lateral-Sclerosis-Identification"><a href="#Facial-Point-Graphs-for-Amyotrophic-Lateral-Sclerosis-Identification" class="headerlink" title="Facial Point Graphs for Amyotrophic Lateral Sclerosis Identification"></a>Facial Point Graphs for Amyotrophic Lateral Sclerosis Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12159">http://arxiv.org/abs/2307.12159</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nícolas Barbosa Gomes, Arissa Yoshida, Mateus Roder, Guilherme Camargo de Oliveira, João Paulo Papa</li>
<li>for: 旨在早期诊断amyotrophic lateral sclerosis (ALS)，以提供更好的治疗机会，提高病人的生活质量。</li>
<li>methods: 利用计算机方法分析病人的面部表情，通过学习面部图形信息，自动识别ALS。</li>
<li>results: 在渥太华Neuroface数据集上，提议的方法比前一代结果更高，表现出优异的潜力。<details>
<summary>Abstract</summary>
Identifying Amyotrophic Lateral Sclerosis (ALS) in its early stages is essential for establishing the beginning of treatment, enriching the outlook, and enhancing the overall well-being of those affected individuals. However, early diagnosis and detecting the disease's signs is not straightforward. A simpler and cheaper way arises by analyzing the patient's facial expressions through computational methods. When a patient with ALS engages in specific actions, e.g., opening their mouth, the movement of specific facial muscles differs from that observed in a healthy individual. This paper proposes Facial Point Graphs to learn information from the geometry of facial images to identify ALS automatically. The experimental outcomes in the Toronto Neuroface dataset show the proposed approach outperformed state-of-the-art results, fostering promising developments in the area.
</details>
<details>
<summary>摘要</summary>
早期诊断阿勒阵性肌萎缩综合征（ALS）是非常重要的，可以为患者提供更好的诊断、改善结果和全面健康状况。然而，早期诊断和识别病种的征象不是一件容易的事情。这项研究提出使用计算机方法分析患者的面部表情，以自动识别ALS。当患者进行特定动作时，如打开嘴巴，健康人的面部肌肉运动与ALS患者不同。这项研究使用面部点云图来学习面部图像的几何特征，并在多伦多神经面数据集上进行实验，结果表明该方法在比较前一些现有方法的情况下，表现出了更好的成果，这些成果有助于推动该领域的发展。
</details></li>
</ul>
<hr>
<h2 id="DIP-RL-Demonstration-Inferred-Preference-Learning-in-Minecraft"><a href="#DIP-RL-Demonstration-Inferred-Preference-Learning-in-Minecraft" class="headerlink" title="DIP-RL: Demonstration-Inferred Preference Learning in Minecraft"></a>DIP-RL: Demonstration-Inferred Preference Learning in Minecraft</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12158">http://arxiv.org/abs/2307.12158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ellen Novoseller, Vinicius G. Goecks, David Watkins, Josh Miller, Nicholas Waytowich</li>
<li>for: 本研究旨在解决在无结构的真实世界中，RL算法学习Sequential Decision-Making时，因为奖励信号不明确而无法正确地捕捉所需行为的问题。</li>
<li>methods: 本研究提出了Demonstration-Inferred Preference Reinforcement Learning（DIP-RL）算法，利用人类示范在三种不同的方式，包括训练自动编码器、在RL训练批处理中使用示范数据，以及从示范数据中推断行为偏好来学习一个奖励函数，以 guidRL算法学习。</li>
<li>results: 结果表明，DIP-RL可以引导RL算法学习一个奖励函数，该奖励函数反映人类的偏好，并且DIP-RL相比基eline表现竞争力强。<details>
<summary>Abstract</summary>
In machine learning for sequential decision-making, an algorithmic agent learns to interact with an environment while receiving feedback in the form of a reward signal. However, in many unstructured real-world settings, such a reward signal is unknown and humans cannot reliably craft a reward signal that correctly captures desired behavior. To solve tasks in such unstructured and open-ended environments, we present Demonstration-Inferred Preference Reinforcement Learning (DIP-RL), an algorithm that leverages human demonstrations in three distinct ways, including training an autoencoder, seeding reinforcement learning (RL) training batches with demonstration data, and inferring preferences over behaviors to learn a reward function to guide RL. We evaluate DIP-RL in a tree-chopping task in Minecraft. Results suggest that the method can guide an RL agent to learn a reward function that reflects human preferences and that DIP-RL performs competitively relative to baselines. DIP-RL is inspired by our previous work on combining demonstrations and pairwise preferences in Minecraft, which was awarded a research prize at the 2022 NeurIPS MineRL BASALT competition, Learning from Human Feedback in Minecraft. Example trajectory rollouts of DIP-RL and baselines are located at https://sites.google.com/view/dip-rl.
</details>
<details>
<summary>摘要</summary>
机器学习的决策行为中，一个算法式代理人 learns to interact with环境，收到一个奖励信号作为反馈。但在许多未知的实际环境中，这个奖励信号是未知的，人类无法可靠地设计一个正确地捕捉 desired behavior的奖励信号。为解决这类未知和开放的环境中的任务，我们提出了 Demonstration-Inferred Preference Reinforcement Learning (DIP-RL)，一个算法，它利用人类示范的方式，包括训练 autoencoder、将示范资料用于RL 训练批次的seed、以及对行为偏好进行推断，以学习一个奖励函数，导引RL。我们在 Minecraft 中进行了树割任务的评估，结果表明 DI P-RL 可以将RL代理人学习一个与人类偏好相符的奖励函数，并且 DI P-RL 与基准相比表现竞争。DIP-RL 是我们之前的 Combining Demonstrations and Pairwise Preferences in Minecraft 研究的推展，该研究在2022年 NeurIPS MineRL BASALT 竞赛中获奖。具体的示例 Rollout 可以在 https://sites.google.com/view/dip-rl 获取。
</details></li>
</ul>
<hr>
<h2 id="Identifying-contributors-to-supply-chain-outcomes-in-a-multi-echelon-setting-a-decentralised-approach"><a href="#Identifying-contributors-to-supply-chain-outcomes-in-a-multi-echelon-setting-a-decentralised-approach" class="headerlink" title="Identifying contributors to supply chain outcomes in a multi-echelon setting: a decentralised approach"></a>Identifying contributors to supply chain outcomes in a multi-echelon setting: a decentralised approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12157">http://arxiv.org/abs/2307.12157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Schoepf, Jack Foster, Alexandra Brintrup</li>
<li>for: 这篇论文目的是解决多层供应链中 metric 变化的原因难以确定的问题，尤其是在供应链内部分不可见的情况下。</li>
<li>methods: 该论文提议使用可解释人工智能 для分布式计算产品质量变化的原因。该方法不需要供应链成员分享数据，因为所有计算都发生在分布式环境中。</li>
<li>results: 实验结果表明，使用该方法可以更好地检测产品质量变化的原因，比起中央化使用Shapley添加тив解释法。<details>
<summary>Abstract</summary>
Organisations often struggle to identify the causes of change in metrics such as product quality and delivery duration. This task becomes increasingly challenging when the cause lies outside of company borders in multi-echelon supply chains that are only partially observable. Although traditional supply chain management has advocated for data sharing to gain better insights, this does not take place in practice due to data privacy concerns. We propose the use of explainable artificial intelligence for decentralised computing of estimated contributions to a metric of interest in a multi-stage production process. This approach mitigates the need to convince supply chain actors to share data, as all computations occur in a decentralised manner. Our method is empirically validated using data collected from a real multi-stage manufacturing process. The results demonstrate the effectiveness of our approach in detecting the source of quality variations compared to a centralised approach using Shapley additive explanations.
</details>
<details>
<summary>摘要</summary>
企业们经常难以确定生产质量和交付时间的变化的原因。这个任务在多层供应链中，尤其是在只有部分可见的情况下，变得更加困难。传统的供应链管理强调了数据分享，以获得更好的洞察，但在实践中，这并不总是可行的，因为数据隐私问题。我们建议使用可解释人工智能 для分布式计算估算变量的贡献，以避免需要供应链 Actors 分享数据。我们的方法在实验 validate 中被证明，可以准确地检测质量变化的来源，比中央化使用 Shapley 添加тив性解释更有效。
</details></li>
</ul>
<hr>
<h2 id="Real-Time-Neural-Video-Recovery-and-Enhancement-on-Mobile-Devices"><a href="#Real-Time-Neural-Video-Recovery-and-Enhancement-on-Mobile-Devices" class="headerlink" title="Real-Time Neural Video Recovery and Enhancement on Mobile Devices"></a>Real-Time Neural Video Recovery and Enhancement on Mobile Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12152">http://arxiv.org/abs/2307.12152</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Zhaoyuan He, Yifan Yang, Lili Qiu, Kyoungjun Park</li>
<li>for: 这个论文是为了优化移动设备上的视频流处理而写的。</li>
<li>methods: 这个论文使用了一种新的视频帧恢复算法、一种新的超解算法和一种接收器增强视频比特率适应算法。</li>
<li>results: 该论文的实验表明，该approach可以在不同的网络环境下支持30帧&#x2F;秒的实时增强，并且可以提高视频流经验质量（Quality of Experience，QoE）24%-82%。<details>
<summary>Abstract</summary>
As mobile devices become increasingly popular for video streaming, it's crucial to optimize the streaming experience for these devices. Although deep learning-based video enhancement techniques are gaining attention, most of them cannot support real-time enhancement on mobile devices. Additionally, many of these techniques are focused solely on super-resolution and cannot handle partial or complete loss or corruption of video frames, which is common on the Internet and wireless networks.   To overcome these challenges, we present a novel approach in this paper. Our approach consists of (i) a novel video frame recovery scheme, (ii) a new super-resolution algorithm, and (iii) a receiver enhancement-aware video bit rate adaptation algorithm. We have implemented our approach on an iPhone 12, and it can support 30 frames per second (FPS). We have evaluated our approach in various networks such as WiFi, 3G, 4G, and 5G networks. Our evaluation shows that our approach enables real-time enhancement and results in a significant increase in video QoE (Quality of Experience) of 24\% - 82\% in our video streaming system.
</details>
<details>
<summary>摘要</summary>
As mobile devices become increasingly popular for video streaming, it's crucial to optimize the streaming experience for these devices. Although deep learning-based video enhancement techniques are gaining attention, most of them cannot support real-time enhancement on mobile devices. Additionally, many of these techniques are focused solely on super-resolution and cannot handle partial or complete loss or corruption of video frames, which is common on the Internet and wireless networks.  To overcome these challenges, we present a novel approach in this paper. Our approach consists of (i) a novel video frame recovery scheme, (ii) a new super-resolution algorithm, and (iii) a receiver enhancement-aware video bit rate adaptation algorithm. We have implemented our approach on an iPhone 12, and it can support 30 frames per second (FPS). We have evaluated our approach in various networks such as WiFi, 3G, 4G, and 5G networks. Our evaluation shows that our approach enables real-time enhancement and results in a significant increase in video QoE (Quality of Experience) of 24% - 82% in our video streaming system.
</details></li>
</ul>
<hr>
<h2 id="Learned-Gridification-for-Efficient-Point-Cloud-Processing"><a href="#Learned-Gridification-for-Efficient-Point-Cloud-Processing" class="headerlink" title="Learned Gridification for Efficient Point Cloud Processing"></a>Learned Gridification for Efficient Point Cloud Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14354">http://arxiv.org/abs/2307.14354</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/computri/gridifier">https://github.com/computri/gridifier</a></li>
<li>paper_authors: Putri A. van der Linden, David W. Romero, Erik J. Bekkers</li>
<li>for: 这 paper 的目的是解决点云处理中的可插义问题，使得点云处理可以更加有效率。</li>
<li>methods: 这 paper 使用了学习的 gridification 技术来将点云转换成一个紧凑的、规则的网格，然后使用这个网格来进行后续的操作。</li>
<li>results: 这 paper 通过理论和实验分析，证明 gridified 网络可以更好地扩展到大规模点云数据，同时保持竞争性的 результа们。<details>
<summary>Abstract</summary>
Neural operations that rely on neighborhood information are much more expensive when deployed on point clouds than on grid data due to the irregular distances between points in a point cloud. In a grid, on the other hand, we can compute the kernel only once and reuse it for all query positions. As a result, operations that rely on neighborhood information scale much worse for point clouds than for grid data, specially for large inputs and large neighborhoods.   In this work, we address the scalability issue of point cloud methods by tackling its root cause: the irregularity of the data. We propose learnable gridification as the first step in a point cloud processing pipeline to transform the point cloud into a compact, regular grid. Thanks to gridification, subsequent layers can use operations defined on regular grids, e.g., Conv3D, which scale much better than native point cloud methods. We then extend gridification to point cloud to point cloud tasks, e.g., segmentation, by adding a learnable de-gridification step at the end of the point cloud processing pipeline to map the compact, regular grid back to its original point cloud form. Through theoretical and empirical analysis, we show that gridified networks scale better in terms of memory and time than networks directly applied on raw point cloud data, while being able to achieve competitive results. Our code is publicly available at https://github.com/computri/gridifier.
</details>
<details>
<summary>摘要</summary>
神经操作依赖 neighboorhood 信息在点云上比在格点数据上更加昂贵，因为点云中点的距离不规则。而在格点数据上，我们可以一次计算核函数，然后重用其于所有查询位置。因此，基于 neighborhood 信息的操作在点云上尤其是大输入和大 neighboorhood 时会扩散很快。在这种情况下，我们解决点云处理的扩散问题的根本原因——点云数据的不规则性。我们提出了可学习的 gridification 来将点云转换成一个紧凑、规则的网格。由于 gridification，后续层可以使用基于网格的操作，例如 Conv3D，这些操作在规模上能够更好地扩展。我们还扩展了 gridification 到点云到点云任务，例如分割，通过添加一个可学习的 de-gridification 步骤，将紧凑的网格还原回原始的点云形式。我们通过理论和实验分析表明，gridified 网络在内存和时间上比直接应用于原始点云数据更加高效，同时能够达到竞争性的结果。我们的代码可以在 https://github.com/computri/gridifier 上获取。
</details></li>
</ul>
<hr>
<h2 id="CorrFL-Correlation-Based-Neural-Network-Architecture-for-Unavailability-Concerns-in-a-Heterogeneous-IoT-Environment"><a href="#CorrFL-Correlation-Based-Neural-Network-Architecture-for-Unavailability-Concerns-in-a-Heterogeneous-IoT-Environment" class="headerlink" title="CorrFL: Correlation-Based Neural Network Architecture for Unavailability Concerns in a Heterogeneous IoT Environment"></a>CorrFL: Correlation-Based Neural Network Architecture for Unavailability Concerns in a Heterogeneous IoT Environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12149">http://arxiv.org/abs/2307.12149</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Western-OC2-Lab/CorrFL">https://github.com/Western-OC2-Lab/CorrFL</a></li>
<li>paper_authors: Ibrahim Shaer, Abdallah Shami</li>
<li>for: 解决 Federated Learning 中模型结构不同和 IoT 节点不可用的问题</li>
<li>methods: 提出了 Correlation-based Federated Learning（CorrFL）方法，通过 проекting 模型权重到共同 latent space 来Address 模型不同性，并最小化缺 absent 模型的重建loss</li>
<li>results: 通过对一个实际用 caso 进行评估，对比 CorrFL 模型和不同用 caso 的 referential model，得到 CorrFL 模型在预测性能和数据交换量的影响下表现更优于 referential model。<details>
<summary>Abstract</summary>
The Federated Learning (FL) paradigm faces several challenges that limit its application in real-world environments. These challenges include the local models' architecture heterogeneity and the unavailability of distributed Internet of Things (IoT) nodes due to connectivity problems. These factors posit the question of "how can the available models fill the training gap of the unavailable models?". This question is referred to as the "Oblique Federated Learning" problem. This problem is encountered in the studied environment that includes distributed IoT nodes responsible for predicting CO2 concentrations. This paper proposes the Correlation-based FL (CorrFL) approach influenced by the representational learning field to address this problem. CorrFL projects the various model weights to a common latent space to address the model heterogeneity. Its loss function minimizes the reconstruction loss when models are absent and maximizes the correlation between the generated models. The latter factor is critical because of the intersection of the feature spaces of the IoT devices. CorrFL is evaluated on a realistic use case, involving the unavailability of one IoT device and heightened activity levels that reflect occupancy. The generated CorrFL models for the unavailable IoT device from the available ones trained on the new environment are compared against models trained on different use cases, referred to as the benchmark model. The evaluation criteria combine the mean absolute error (MAE) of predictions and the impact of the amount of exchanged data on the prediction performance improvement. Through a comprehensive experimental procedure, the CorrFL model outperformed the benchmark model in every criterion.
</details>
<details>
<summary>摘要</summary>
联邦学习（FL）模式面临多个挑战，限制其在实际环境中的应用。这些挑战包括本地模型的架构多样性和分布式互联物联网（IoT）节点的无法访问问题。这个问题被称为“偏置联邦学习”问题。这个问题在包括分布式IoT节点，负责预测二氧化碳浓度的环境中被研究。这篇文章提出了基于相关学习（CorrFL）方法，以解决这个问题。CorrFL方法将不同模型的 weights 投射到共同的潜在空间，以解决模型多样性。它的损失函数最小化缺失模型时的重建损失，并 maximizes 模型之间的相互相关性。这个第二个因素是critical，因为IoT设备的特征空间之间的交汇。CorrFL方法被评估在实际的使用案例中，包括一个缺失IoT设备和高水平的活动水平，反映occupancy。生成的CorrFL模型在缺失IoT设备上培育的新环境中，与不同使用案例中的参考模型（benchmark model）进行比较。评估标准包括预测误差的总平均误差（MAE）和预测性能改善的资料交换量的影响。通过充分的实验程序，CorrFL模型在每个标准中都高于参考模型。
</details></li>
</ul>
<hr>
<h2 id="Applications-of-Machine-Learning-to-Modelling-and-Analysing-Dynamical-Systems"><a href="#Applications-of-Machine-Learning-to-Modelling-and-Analysing-Dynamical-Systems" class="headerlink" title="Applications of Machine Learning to Modelling and Analysing Dynamical Systems"></a>Applications of Machine Learning to Modelling and Analysing Dynamical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03763">http://arxiv.org/abs/2308.03763</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vedanta Thapar</li>
<li>for: 这个论文主要研究了使用物理学 Informed Neural Networks 分析非线性汉密尔顿动力系统，具有一个运动量的第一 интеграル。</li>
<li>methods: 该论文提出了一种结合现有汉密尔顿神经网络结构的 Adaptable Symplectic Recurrent Neural Networks 架构，该架构可以保持汉密尔顿方程和相对空间的 симплектиче结构，同时预测动力学行为的整个参数空间。</li>
<li>results: 该论文表明，该架构在预测汉密尔顿动力学中，特别是在含有多个参数的潜能中表现出色，显著超过了之前的神经网络预测。furthermore, the authors demonstrate the robustness of their method by applying it to the nonlinear Henon-Heiles potential under chaotic, quasiperiodic and periodic conditions.<details>
<summary>Abstract</summary>
We explore the use of Physics Informed Neural Networks to analyse nonlinear Hamiltonian Dynamical Systems with a first integral of motion. In this work, we propose an architecture which combines existing Hamiltonian Neural Network structures into Adaptable Symplectic Recurrent Neural Networks which preserve Hamilton's equations as well as the symplectic structure of phase space while predicting dynamics for the entire parameter space. This architecture is found to significantly outperform previously proposed neural networks when predicting Hamiltonian dynamics especially in potentials which contain multiple parameters. We demonstrate its robustness using the nonlinear Henon-Heiles potential under chaotic, quasiperiodic and periodic conditions.   The second problem we tackle is whether we can use the high dimensional nonlinear capabilities of neural networks to predict the dynamics of a Hamiltonian system given only partial information of the same. Hence we attempt to take advantage of Long Short Term Memory networks to implement Takens' embedding theorem and construct a delay embedding of the system followed by mapping the topologically invariant attractor to the true form. This architecture is then layered with Adaptable Symplectic nets to allow for predictions which preserve the structure of Hamilton's equations. We show that this method works efficiently for single parameter potentials and provides accurate predictions even over long periods of time.
</details>
<details>
<summary>摘要</summary>
我们探讨使用物理 Informed Neural Networks（PINNs）分析非线性哈密顿动力系统，并研究一种可适应射步器的 Hamiltonian Neural Network 结构，以保持哈密顿方程和相似性结构的阶段空间。我们提议一种结合现有 Hamiltonian Neural Network 结构的 Adaptable Symplectic Recurrent Neural Networks（ASRNNs），可以在整个参数空间预测动力学。我们发现这种结构在多参数potential中表现出色， especialy in potentials containing multiple parameters.我们通过非线性 Henon-Heiles potential的混沌、 quasi-periodic 和 periodic 条件进行了 robustness 测试。第二个问题是可以使用高维非线性神经网络预测哈密顿系统的动力学，即使只有部分系统的信息。因此，我们尝试使用 Long Short Term Memory 网络实现 Takens 嵌入定理，并将系统的延迟嵌入映射到真实的形式。然后，我们将 Adaptable Symplectic nets 层在这个嵌入中，以保持哈密顿方程的结构。我们发现这种方法可以高效地预测单参数 potentials，并提供了精度的预测，即使在长时间内。
</details></li>
</ul>
<hr>
<h2 id="A-Vision-for-Cleaner-Rivers-Harnessing-Snapshot-Hyperspectral-Imaging-to-Detect-Macro-Plastic-Litter"><a href="#A-Vision-for-Cleaner-Rivers-Harnessing-Snapshot-Hyperspectral-Imaging-to-Detect-Macro-Plastic-Litter" class="headerlink" title="A Vision for Cleaner Rivers: Harnessing Snapshot Hyperspectral Imaging to Detect Macro-Plastic Litter"></a>A Vision for Cleaner Rivers: Harnessing Snapshot Hyperspectral Imaging to Detect Macro-Plastic Litter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12145">http://arxiv.org/abs/2307.12145</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/river-lab/hyperspectral_macro_plastic_detection">https://github.com/river-lab/hyperspectral_macro_plastic_detection</a></li>
<li>paper_authors: Nathaniel Hanson, Ahmet Demirkaya, Deniz Erdoğmuş, Aron Stubbins, Taşkın Padır, Tales Imbiriba</li>
<li>For: 本研究旨在提出一种计算成像方法来快速、自动地检测废弃塑料杂物在河流环境中的扩散。* Methods: 该研究使用可见短波infrared hyperspectral成像技术进行废弃塑料杂物的检测，并采用机器学习分类方法来提高检测精度。* Results: 实验结果表明，使用Snapshot Visible-Shortwave Infrared hyperspectral成像技术可以在实时Tracking废弃塑料杂物中实现高检测精度，特别是在复杂的场景下。 Code、数据和模型都可以在线获取：<a target="_blank" rel="noopener" href="https://github.com/RIVeR-Lab/hyperspectral_macro_plastic_detection%E3%80%82">https://github.com/RIVeR-Lab/hyperspectral_macro_plastic_detection。</a><details>
<summary>Abstract</summary>
Plastic waste entering the riverine harms local ecosystems leading to negative ecological and economic impacts. Large parcels of plastic waste are transported from inland to oceans leading to a global scale problem of floating debris fields. In this context, efficient and automatized monitoring of mismanaged plastic waste is paramount. To address this problem, we analyze the feasibility of macro-plastic litter detection using computational imaging approaches in river-like scenarios. We enable near-real-time tracking of partially submerged plastics by using snapshot Visible-Shortwave Infrared hyperspectral imaging. Our experiments indicate that imaging strategies associated with machine learning classification approaches can lead to high detection accuracy even in challenging scenarios, especially when leveraging hyperspectral data and nonlinear classifiers. All code, data, and models are available online: https://github.com/RIVeR-Lab/hyperspectral_macro_plastic_detection.
</details>
<details>
<summary>摘要</summary>
пластик废弃进入河流系统会对当地生态系统造成负面影响，导致生态和经济上的负面影响。大量废弃 пласти克材料从内陆运输到海洋，导致全球范围内漂浮垃圾场景。在这种情况下，高效和自动化的废弃 пласти克碎屑监测是非常重要。为解决这个问题，我们分析了使用计算成像方法检测废弃 macro  пласти克碎屑的可行性。我们通过使用快照 Visible-Shortwave Infrared  hyperspectral成像获得了近实时检测部分抛光的废弃 пласти克的能力。我们的实验表明，使用计算机视觉分类方法可以在复杂的场景中获得高检测精度，特别是当使用 hyperspectral 数据和非线性分类器时。所有代码、数据和模型都可以在 GitHub 上获取：https://github.com/RIVeR-Lab/hyperspectral_macro_plastic_detection。
</details></li>
</ul>
<hr>
<h2 id="Emergence-of-Adaptive-Circadian-Rhythms-in-Deep-Reinforcement-Learning"><a href="#Emergence-of-Adaptive-Circadian-Rhythms-in-Deep-Reinforcement-Learning" class="headerlink" title="Emergence of Adaptive Circadian Rhythms in Deep Reinforcement Learning"></a>Emergence of Adaptive Circadian Rhythms in Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12143">http://arxiv.org/abs/2307.12143</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aqeel13932/mn_project">https://github.com/aqeel13932/mn_project</a></li>
<li>paper_authors: Aqeel Labash, Florian Fletzer, Daniel Majoral, Raul Vicente</li>
<li>for: 这项研究旨在研究深度学习Agent中的 circadian-like 征性rhythm的出现。</li>
<li>methods: 作者在一个可靠 periodic variation 的环境中部署了 Agent，并通过解决一个搜寻任务来学习。作者系统地描述了 Agent 的行为 während learning, 并证明了 Agent 内部的 rhythm 的出现是内在的和可调整的。</li>
<li>results: 作者通过分析 bifurcation 和 phase response 曲线表示了人工神经元发展出 dynamics 以支持环境 rhythm 的内化。从动力系统视角来看， adapting 进程由神经元动力学中的稳定 periodic orbit 的出现和相应的 phase response 响应而进行。<details>
<summary>Abstract</summary>
Adapting to regularities of the environment is critical for biological organisms to anticipate events and plan. A prominent example is the circadian rhythm corresponding to the internalization by organisms of the $24$-hour period of the Earth's rotation. In this work, we study the emergence of circadian-like rhythms in deep reinforcement learning agents. In particular, we deployed agents in an environment with a reliable periodic variation while solving a foraging task. We systematically characterize the agent's behavior during learning and demonstrate the emergence of a rhythm that is endogenous and entrainable. Interestingly, the internal rhythm adapts to shifts in the phase of the environmental signal without any re-training. Furthermore, we show via bifurcation and phase response curve analyses how artificial neurons develop dynamics to support the internalization of the environmental rhythm. From a dynamical systems view, we demonstrate that the adaptation proceeds by the emergence of a stable periodic orbit in the neuron dynamics with a phase response that allows an optimal phase synchronisation between the agent's dynamics and the environmental rhythm.
</details>
<details>
<summary>摘要</summary>
适应环境的规律是生物体存活的关键，以预测事件和规划。一个典型的例子是生物体内部的Circadian rhythm，即通过生物体内部的$24$-hour期的地球旋转的内化。在这项工作中，我们研究了深度学习代理人在环境中的Circadian-like rhythm的出现。特别是，我们在一个可靠的 periodic variation 环境中部署了代理人，并在学习过程中系统地描述代理人的行为。我们发现，代理人在学习过程中发展了一种内生的rhythm，该rhythm可以适应环境的阶段的变化，而无需重新训练。此外，我们通过分支和相位回快分析，证明了人工神经元的动力学发展了以支持内化环境的rhythm。从动力学视角来看，适应过程是通过神经元动力学中的稳定 periodic orbit 的出现，并且该 periodic orbit 的相位响应允许生物体动力学和环境的相位同步。
</details></li>
</ul>
<hr>
<h2 id="Unlocking-Carbon-Reduction-Potential-with-Reinforcement-Learning-for-the-Three-Dimensional-Loading-Capacitated-Vehicle-Routing-Problem"><a href="#Unlocking-Carbon-Reduction-Potential-with-Reinforcement-Learning-for-the-Three-Dimensional-Loading-Capacitated-Vehicle-Routing-Problem" class="headerlink" title="Unlocking Carbon Reduction Potential with Reinforcement Learning for the Three-Dimensional Loading Capacitated Vehicle Routing Problem"></a>Unlocking Carbon Reduction Potential with Reinforcement Learning for the Three-Dimensional Loading Capacitated Vehicle Routing Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12136">http://arxiv.org/abs/2307.12136</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Schoepf, Stephen Mak, Julian Senoner, Liming Xu, Netland Torbjörn, Alexandra Brintrup</li>
<li>For: The paper aims to improve the efficiency of heavy goods vehicle routing in the supply chain delivery system, with a focus on increasing the loading efficiency and reducing carbon emissions.* Methods: The paper proposes using reinforcement learning to solve the three-dimensional loading capacitated vehicle routing problem, which has not been previously studied in the literature. The authors claim that their method scales approximately linearly with the problem size and can handle large-scale logistics optimization.* Results: The authors demonstrate the effectiveness of their reinforcement learning model by benchmarking it against state-of-the-art methods and showing that it performs within an average gap of 3.83% to 8.10% compared to established methods. They also claim that their model lays the foundation for this research stream and represents a promising first step towards large-scale logistics optimization with reinforcement learning.Here is the simplified Chinese text for the three information points:* 用途：文章目的是提高庞大货物运输系统中的卡车路径规划效率，尤其是提高荷载效率和减少碳排放。* 方法：文章提出使用强化学习解决三维荷载限制的卡车路径规划问题，这问题在操作研究领域已经广泛研究，但是没有使用强化学习来解决这个问题。作者们称其方法的时间复杂度接近线性增长，可以承受大规模的物流优化。* 结果：作者们通过对state-of-the-art方法进行比较，证明其模型在三维荷载限制卡车路径规划问题上的性能准确性在3.83%到8.10%之间。他们还称，其模型不仅表现出了扩大物流优化的潜力，还为这一研究流程提供了基础。<details>
<summary>Abstract</summary>
Heavy goods vehicles are vital backbones of the supply chain delivery system but also contribute significantly to carbon emissions with only 60% loading efficiency in the United Kingdom. Collaborative vehicle routing has been proposed as a solution to increase efficiency, but challenges remain to make this a possibility. One key challenge is the efficient computation of viable solutions for co-loading and routing. Current operations research methods suffer from non-linear scaling with increasing problem size and are therefore bound to limited geographic areas to compute results in time for day-to-day operations. This only allows for local optima in routing and leaves global optimisation potential untouched. We develop a reinforcement learning model to solve the three-dimensional loading capacitated vehicle routing problem in approximately linear time. While this problem has been studied extensively in operations research, no publications on solving it with reinforcement learning exist. We demonstrate the favourable scaling of our reinforcement learning model and benchmark our routing performance against state-of-the-art methods. The model performs within an average gap of 3.83% to 8.10% compared to established methods. Our model not only represents a promising first step towards large-scale logistics optimisation with reinforcement learning but also lays the foundation for this research stream.
</details>
<details>
<summary>摘要</summary>
厉害货物车辆是供应链交通系统的重要脊梁，但它们也对碳排放做出了重要贡献。在英国，厉害货物车辆的加载效率只有60%， Collaborative vehicle routing 被提议为一种解决方案，但是还有一些挑战需要解决。一个关键的挑战是计算可行的协同加载和路由解决方案的有效性。现有的运筹学方法受到增加的问题大小的非线性缩放，因此只能在有限的地理区域内计算结果，这只能确定本地最佳解决方案，留下全球优化潜力。我们开发了一个强化学习模型，以解决三维协同加载货物车辆路由问题。这个问题在运筹学中已经广泛研究，但是没有关于使用强化学习解决这个问题的出版物。我们示出了我们模型的有利扩展性，并将其比较了现有的路由性能。我们的模型与现有方法的性能差距在3.83%到8.10%之间。我们的模型不仅表现出了有 promise的首步，也为这个研究流量开创了基础。
</details></li>
</ul>
<hr>
<h2 id="The-Sample-Complexity-of-Multi-Distribution-Learning-for-VC-Classes"><a href="#The-Sample-Complexity-of-Multi-Distribution-Learning-for-VC-Classes" class="headerlink" title="The Sample Complexity of Multi-Distribution Learning for VC Classes"></a>The Sample Complexity of Multi-Distribution Learning for VC Classes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12135">http://arxiv.org/abs/2307.12135</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranjal Awasthi, Nika Haghtalab, Eric Zhao</li>
<li>for: 这篇论文旨在探讨多分布学习的自然推广，即在多个数据分布下学习的情况。</li>
<li>methods: 该论文使用了游戏动力学的思想来解决多分布学习中的一些挑战。</li>
<li>results: 研究人员通过提出新的算法和分析方法，成功地降低了多分布学习中的样本复杂性下界。<details>
<summary>Abstract</summary>
Multi-distribution learning is a natural generalization of PAC learning to settings with multiple data distributions. There remains a significant gap between the known upper and lower bounds for PAC-learnable classes. In particular, though we understand the sample complexity of learning a VC dimension d class on $k$ distributions to be $O(\epsilon^{-2} \ln(k)(d + k) + \min\{\epsilon^{-1} dk, \epsilon^{-4} \ln(k) d\})$, the best lower bound is $\Omega(\epsilon^{-2}(d + k \ln(k)))$. We discuss recent progress on this problem and some hurdles that are fundamental to the use of game dynamics in statistical learning.
</details>
<details>
<summary>摘要</summary>
多分布学习是自然推广的PAC学习设置中的多个数据分布。尚未知道PAC可学习的类的差距。特别是，虽然我们理解了学习VCdimension d类型在k个分布上的样本复杂度为O（ε^{-2} ln(k)(d + k) + мин\{\ε^{-1} dk, ε^{-4} ln(k) d\}），但最好的下界为Ω（ε^{-2}(d + k ln(k))）。我们讨论了这个问题的最新进展和使用游戏动力学在统计学习中的核心障碍。
</details></li>
</ul>
<hr>
<h2 id="AI-on-the-Road-A-Comprehensive-Analysis-of-Traffic-Accidents-and-Accident-Detection-System-in-Smart-Cities"><a href="#AI-on-the-Road-A-Comprehensive-Analysis-of-Traffic-Accidents-and-Accident-Detection-System-in-Smart-Cities" class="headerlink" title="AI on the Road: A Comprehensive Analysis of Traffic Accidents and Accident Detection System in Smart Cities"></a>AI on the Road: A Comprehensive Analysis of Traffic Accidents and Accident Detection System in Smart Cities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12128">http://arxiv.org/abs/2307.12128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Victor Adewopo, Nelly Elsayed, Zag Elsayed, Murat Ozer, Victoria Wangia-Anderson, Ahmed Abdelgawad</li>
<li>for: 提高交通管理和交通事故预防</li>
<li>methods: 使用交通监控摄像头和动作识别系统检测和响应交通事故</li>
<li>results: 提高交通管理和交通事故严重性<details>
<summary>Abstract</summary>
Accident detection and traffic analysis is a critical component of smart city and autonomous transportation systems that can reduce accident frequency, severity and improve overall traffic management. This paper presents a comprehensive analysis of traffic accidents in different regions across the United States using data from the National Highway Traffic Safety Administration (NHTSA) Crash Report Sampling System (CRSS). To address the challenges of accident detection and traffic analysis, this paper proposes a framework that uses traffic surveillance cameras and action recognition systems to detect and respond to traffic accidents spontaneously. Integrating the proposed framework with emergency services will harness the power of traffic cameras and machine learning algorithms to create an efficient solution for responding to traffic accidents and reducing human errors. Advanced intelligence technologies, such as the proposed accident detection systems in smart cities, will improve traffic management and traffic accident severity. Overall, this study provides valuable insights into traffic accidents in the US and presents a practical solution to enhance the safety and efficiency of transportation systems.
</details>
<details>
<summary>摘要</summary>
智能城市和自主交通系统中的意外检测和交通分析是一项重要 комponet，可以降低意外频率、严重性和改善总体交通管理。这篇论文对美国各地不同地区的交通意外进行了全面的分析，使用国家公路交通安全管理局（NHTSA）的交通事故报告采样系统（CRSS）的数据。为了解决意外检测和交通分析的挑战，这篇论文提出了一个框架，该框架使用交通把握摄像头和行为识别系统来自动检测和应对交通意外。将该框架与急救服务集成，将让交通摄像头和机器学习算法帮助创造一种高效的交通意外应急应对解决方案。智能技术，如提出的交通意外检测系统，将改善交通管理和交通意外严重性。总的来说，这篇论文对美国交通意外进行了价值的分析，并提出了实用的解决方案，以提高交通系统的安全和效率。
</details></li>
</ul>
<hr>
<h2 id="Synthesis-of-Batik-Motifs-using-a-Diffusion-–-Generative-Adversarial-Network"><a href="#Synthesis-of-Batik-Motifs-using-a-Diffusion-–-Generative-Adversarial-Network" class="headerlink" title="Synthesis of Batik Motifs using a Diffusion – Generative Adversarial Network"></a>Synthesis of Batik Motifs using a Diffusion – Generative Adversarial Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12122">http://arxiv.org/abs/2307.12122</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/octadion/diffusion-stylegan2-ada-pytorch">https://github.com/octadion/diffusion-stylegan2-ada-pytorch</a></li>
<li>paper_authors: One Octadion, Novanto Yudistira, Diva Kurnianingtyas</li>
<li>For: 协助batik设计师或手工艺术家创造独特和高品质的batik模样，以减少生产时间和成本。* Methods: 使用StyleGAN2-Ada和Diffusion技术生成真实和高品质的synthetic batik模样，并对模型架构进行调整，使用了高品质的batik数据集。* Results: 根据质量和量itative评估，模型能够生成authentic和高品质的batik模样，具有细部艺术变化。<details>
<summary>Abstract</summary>
Batik, a unique blend of art and craftsmanship, is a distinct artistic and technological creation for Indonesian society. Research on batik motifs is primarily focused on classification. However, further studies may extend to the synthesis of batik patterns. Generative Adversarial Networks (GANs) have been an important deep learning model for generating synthetic data, but often face challenges in the stability and consistency of results. This research focuses on the use of StyleGAN2-Ada and Diffusion techniques to produce realistic and high-quality synthetic batik patterns. StyleGAN2-Ada is a variation of the GAN model that separates the style and content aspects in an image, whereas diffusion techniques introduce random noise into the data. In the context of batik, StyleGAN2-Ada and Diffusion are used to produce realistic synthetic batik patterns. This study also made adjustments to the model architecture and used a well-curated batik dataset. The main goal is to assist batik designers or craftsmen in producing unique and quality batik motifs with efficient production time and costs. Based on qualitative and quantitative evaluations, the results show that the model tested is capable of producing authentic and quality batik patterns, with finer details and rich artistic variations. The dataset and code can be accessed here:https://github.com/octadion/diffusion-stylegan2-ada-pytorch
</details>
<details>
<summary>摘要</summary>
batik，一种独特的艺术和手工艺术，是印度尼西亚社会的独特艺术和技术创作。研究 batik 图案主要集中在分类方面，但可能会扩展到batik 图案的合成。生成 adversarial 网络（GANs）是深度学习模型，可以生成合成数据，但常面临稳定性和一致性的挑战。本研究使用 StyleGAN2-Ada 和扩散技术生成高质量和真实的合成 batik 图案。StyleGAN2-Ada 是 GAN 模型中的一种变体，可以将图像中的风格和内容分开，而扩散技术则是在数据中引入随机噪音。在 batik 的 context中，StyleGAN2-Ada 和扩散被用来生成真实的合成 batik 图案。本研究还对模型结构进行了调整，使用了高质量的 batik 数据集。目标是帮助 batik 设计师或手工艺师生成唯一和高质量的 batik 图案，降低生产时间和成本。根据 качеitative 和量化的评价，结果表明模型测试可以生成authentic 和高质量的 batik 图案，具有细节和艺术变化。数据集和代码可以在以下链接中获取：https://github.com/octadion/diffusion-stylegan2-ada-pytorch
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://nullscc.github.io/2023/07/23/cs.LG_2023_07_23/" data-id="cllshr35t001ro9881360dqrh" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/24/eess.IV_2023_07_24/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.IV - 2023-07-24 17:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/23/cs.SD_2023_07_23/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-07-23 123:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'', root:''}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
