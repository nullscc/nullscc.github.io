
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-07-26 17:00:00 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Artifact Restoration in Histology Images with Diffusion Probabilistic Models paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.14262 repo_url: https:&#x2F;&#x2F;github.com&#x2F;zhenqi-he&#x2F;artifusion paper_authors: Zhenqi He, Junj">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-07-26 17:00:00">
<meta property="og:url" content="http://example.com/2023/07/26/eess.IV_2023_07_26/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Artifact Restoration in Histology Images with Diffusion Probabilistic Models paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.14262 repo_url: https:&#x2F;&#x2F;github.com&#x2F;zhenqi-he&#x2F;artifusion paper_authors: Zhenqi He, Junj">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-26T00:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:38.813Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_07_26" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/26/eess.IV_2023_07_26/" class="article-date">
  <time datetime="2023-07-26T00:00:00.000Z" itemprop="datePublished">2023-07-26</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-07-26 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Artifact-Restoration-in-Histology-Images-with-Diffusion-Probabilistic-Models"><a href="#Artifact-Restoration-in-Histology-Images-with-Diffusion-Probabilistic-Models" class="headerlink" title="Artifact Restoration in Histology Images with Diffusion Probabilistic Models"></a>Artifact Restoration in Histology Images with Diffusion Probabilistic Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14262">http://arxiv.org/abs/2307.14262</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhenqi-he/artifusion">https://github.com/zhenqi-he/artifusion</a></li>
<li>paper_authors: Zhenqi He, Junjun He, Jin Ye, Yiqing Shen</li>
<li>for:  histological whole slide images (WSIs) restoration</li>
<li>methods:  denoising diffusion probabilistic model (ArtiFusion) with novel Swin-Transformer denoising architecture and time token scheme</li>
<li>results:  effective restoration of artifact-free regions with preserved tissue structures and stain style, as demonstrated through extensive evaluations<details>
<summary>Abstract</summary>
Histological whole slide images (WSIs) can be usually compromised by artifacts, such as tissue folding and bubbles, which will increase the examination difficulty for both pathologists and Computer-Aided Diagnosis (CAD) systems. Existing approaches to restoring artifact images are confined to Generative Adversarial Networks (GANs), where the restoration process is formulated as an image-to-image transfer. Those methods are prone to suffer from mode collapse and unexpected mistransfer in the stain style, leading to unsatisfied and unrealistic restored images. Innovatively, we make the first attempt at a denoising diffusion probabilistic model for histological artifact restoration, namely ArtiFusion.Specifically, ArtiFusion formulates the artifact region restoration as a gradual denoising process, and its training relies solely on artifact-free images to simplify the training complexity.Furthermore, to capture local-global correlations in the regional artifact restoration, a novel Swin-Transformer denoising architecture is designed, along with a time token scheme. Our extensive evaluations demonstrate the effectiveness of ArtiFusion as a pre-processing method for histology analysis, which can successfully preserve the tissue structures and stain style in artifact-free regions during the restoration. Code is available at https://github.com/zhenqi-he/ArtiFusion.
</details>
<details>
<summary>摘要</summary>
历史图像整体扫描图像（WSIs）通常会受到artefact的影响，如组织卷绕和气泡，这会增加病理学家和计算机支持诊断（CAD）系统的评估难度。现有的恢复artefact图像方法是基于生成对抗网络（GANs），其中恢复过程是形式化为图像-图像传输。这些方法容易受到模式坍塌和意外传输的问题，导致 restored 图像不满意和不真实。我们在这里做出了一个尝试，提出了一种杂样整合模型，称为ArtiFusion。具体来说，ArtiFusion 将artefact区域恢复视为一个渐进的杂样去噪过程，其训练仅仅基于artefact-free 图像，以简化训练复杂性。此外，为了捕捉区域artefact恢复中的局部-全局相关性，我们设计了一种新的Swin-Transformer杂样去噪架构，以及一种时间токен方案。我们的广泛评估表明ArtiFusion 作为历史图像分析的预处理方法，可以成功保持组织结构和染料风格在artefact-free 区域中，并且可以成功恢复artefact。代码可以在 <https://github.com/zhenqi-he/ArtiFusion> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Visual-Saliency-Detection-in-Advanced-Driver-Assistance-Systems"><a href="#Visual-Saliency-Detection-in-Advanced-Driver-Assistance-Systems" class="headerlink" title="Visual Saliency Detection in Advanced Driver Assistance Systems"></a>Visual Saliency Detection in Advanced Driver Assistance Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03770">http://arxiv.org/abs/2308.03770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Rundo, Michael Sebastian Rundo, Concetto Spampinato</li>
<li>for: 这个研究旨在提供一种智能系统，用于评估司机的注意力水平和场景理解，以提高安全性。</li>
<li>methods: 该系统使用了 semantic segmentation 3D deep network，以及基于 PPG 信号的驾驶员睡眠状况检测。</li>
<li>results: 实验结果表明，该系统能够准确地评估司机的注意力水平和场景理解，并且可以提高安全性。<details>
<summary>Abstract</summary>
Visual Saliency refers to the innate human mechanism of focusing on and extracting important features from the observed environment. Recently, there has been a notable surge of interest in the field of automotive research regarding the estimation of visual saliency. While operating a vehicle, drivers naturally direct their attention towards specific objects, employing brain-driven saliency mechanisms that prioritize certain elements over others. In this investigation, we present an intelligent system that combines a drowsiness detection system for drivers with a scene comprehension pipeline based on saliency. To achieve this, we have implemented a specialized 3D deep network for semantic segmentation, which has been pretrained and tailored for processing the frames captured by an automotive-grade external camera. The proposed pipeline was hosted on an embedded platform utilizing the STA1295 core, featuring ARM A7 dual-cores, and embeds an hardware accelerator. Additionally, we employ an innovative biosensor embedded on the car steering wheel to monitor the driver drowsiness, gathering the PhotoPlethysmoGraphy (PPG) signal of the driver. A dedicated 1D temporal deep convolutional network has been devised to classify the collected PPG time-series, enabling us to assess the driver level of attentiveness. Ultimately, we compare the determined attention level of the driver with the corresponding saliency-based scene classification to evaluate the overall safety level. The efficacy of the proposed pipeline has been validated through extensive experimental results.
</details>
<details>
<summary>摘要</summary>
“视觉吸引力”指代人类在观察环境时自然地吸引注意力并提取重要特征。在汽车研究领域，最近有一场关注visual saliency的浪潮。驾驶时，司机会自然地将注意力集中在特定的对象上，使用大脑驱动的吸引力机制来优先级化元素。在这次研究中，我们提出了一个智能系统，其结合了驾驶员睡眠检测系统和基于吸引力的场景理解管道。为此，我们实施了一种特殊的3D深度网络 для semantic segmentation，该网络在自动汽车级外部摄像头捕捉的帧中进行了预训练和定制。我们的管道在使用STA1295核心的嵌入式平台上执行，该平台feature ARM A7双核心。此外，我们还使用了一种创新的车辙吸引监测系统，该系统通过捕捉驾驶员的PhotoPlethysmoGraphy（PPG）信号来监测驾驶员的睡眠状况。一个专门设计的1D时间深度卷积网络用于分类收集的PPG时间序列，以评估驾驶员的注意力水平。最后，我们将驾驶员确定的注意力水平与相应的吸引力基于场景分类进行比较，以评估整体安全水平。我们的实验结果表明，提案的管道具有良好的效果。
</details></li>
</ul>
<hr>
<h2 id="Non-Linear-Self-Augmentation-Deep-Pipeline-for-Cancer-Treatment-outcome-Prediction"><a href="#Non-Linear-Self-Augmentation-Deep-Pipeline-for-Cancer-Treatment-outcome-Prediction" class="headerlink" title="Non-Linear Self Augmentation Deep Pipeline for Cancer Treatment outcome Prediction"></a>Non-Linear Self Augmentation Deep Pipeline for Cancer Treatment outcome Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14398">http://arxiv.org/abs/2307.14398</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Rundo, Concetto Spampinato, Michael Rundo</li>
<li>for: 这篇论文目的是提出一种新的预测治疗效果的策略，以帮助更好地选择和优化适合接受免疫治疗的患者。</li>
<li>methods: 该策略使用一种非线性细胞建筑和深入下渠分类器，从胸腹部CT图像提取和优化2D特征，以提高治疗效果预测的准确率。</li>
<li>results: 作者们通过一个具有推动力的实验研究，证明该策略的效果惊人，预测精度高达93%。<details>
<summary>Abstract</summary>
Immunotherapy emerges as promising approach for treating cancer. Encouraging findings have validated the efficacy of immunotherapy medications in addressing tumors, resulting in prolonged survival rates and notable reductions in toxicity compared to conventional chemotherapy methods. However, the pool of eligible patients for immunotherapy remains relatively small, indicating a lack of comprehensive understanding regarding the physiological mechanisms responsible for favorable treatment response in certain individuals while others experience limited benefits. To tackle this issue, the authors present an innovative strategy that harnesses a non-linear cellular architecture in conjunction with a deep downstream classifier. This approach aims to carefully select and enhance 2D features extracted from chest-abdomen CT images, thereby improving the prediction of treatment outcomes. The proposed pipeline has been meticulously designed to seamlessly integrate with an advanced embedded Point of Care system. In this context, the authors present a compelling case study focused on Metastatic Urothelial Carcinoma (mUC), a particularly aggressive form of cancer. Performance evaluation of the proposed approach underscores its effectiveness, with an impressive overall accuracy of approximately 93%
</details>
<details>
<summary>摘要</summary>
免疫疗法在肿瘤治疗中出现为可能的新方向。有力的证据证明免疫疗药在治疗肿瘤时的效果，导致了存活时间的延长和化学治疗方法相比的质量下降。然而，有效治疗的候选者人数仍然很小，这表明我们对治疗成功的生理机制还没有充分理解。为了解决这个问题，作者提出了一种创新的策略，利用非线性细胞体系和深入的下游分类器。这种方法的目的是通过精心选择和提高胸腹部CT图像中的2D特征，提高治疗结果预测的准确性。提案的管道已经仔细设计，能够与高级嵌入式Point of Care系统集成。在这个上下文中，作者提出了一个吸引人的案例研究，专门针对肿瘤肝癌（mUC）。研究表明，提案的方法的效果非常出色，总准确率大约为93%。
</details></li>
</ul>
<hr>
<h2 id="Tackling-Scattering-and-Reflective-Flare-in-Mobile-Camera-Systems-A-Raw-Image-Dataset-for-Enhanced-Flare-Removal"><a href="#Tackling-Scattering-and-Reflective-Flare-in-Mobile-Camera-Systems-A-Raw-Image-Dataset-for-Enhanced-Flare-Removal" class="headerlink" title="Tackling Scattering and Reflective Flare in Mobile Camera Systems: A Raw Image Dataset for Enhanced Flare Removal"></a>Tackling Scattering and Reflective Flare in Mobile Camera Systems: A Raw Image Dataset for Enhanced Flare Removal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14180">http://arxiv.org/abs/2307.14180</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fengbo Lan, Chang Wen Chen</li>
<li>for: 这个论文是为了提高移动设备的相机系统和图像质量，并解决激光和折射照明的问题。</li>
<li>methods: 这个论文使用了原始图像数据集，并评估了不同的移动设备和摄像头设定。</li>
<li>results: 该数据集包含了2,000个高品质的全分辨率原始图像对，并且可以进一步分解为30,000个对照图像 patches，以涵盖各种摄影状况。<details>
<summary>Abstract</summary>
The increasing prevalence of mobile devices has led to significant advancements in mobile camera systems and improved image quality. Nonetheless, mobile photography still grapples with challenging issues such as scattering and reflective flare. The absence of a comprehensive real image dataset tailored for mobile phones hinders the development of effective flare mitigation techniques. To address this issue, we present a novel raw image dataset specifically designed for mobile camera systems, focusing on flare removal. Capitalizing on the distinct properties of raw images, this dataset serves as a solid foundation for developing advanced flare removal algorithms. It encompasses a wide variety of real-world scenarios captured with diverse mobile devices and camera settings. The dataset comprises over 2,000 high-quality full-resolution raw image pairs for scattering flare and 1,100 for reflective flare, which can be further segmented into up to 30,000 and 2,200 paired patches, respectively, ensuring broad adaptability across various imaging conditions. Experimental results demonstrate that networks trained with synthesized data struggle to cope with complex lighting settings present in this real image dataset. We also show that processing data through a mobile phone's internal ISP compromises image quality while using raw image data presents significant advantages for addressing the flare removal problem. Our dataset is expected to enable an array of new research in flare removal and contribute to substantial improvements in mobile image quality, benefiting mobile photographers and end-users alike.
</details>
<details>
<summary>摘要</summary>
“由于移动设备的普及，移动摄像系统的技术和图像质量有了显著的进步。然而，移动摄影仍然面临着许多挑战，如散射和反射炙。由于没有专门为移动电话设计的全面真实图像集，因此发展效果的炙光除除射技术受到了限制。为解决这个问题，我们提供了一个新的Raw图像集，专门针对移动摄像系统，强调炙光除除。利用Raw图像的特点，这个集合作为开发高级炙光除除算法的坚实基础。它包括多种真实世界的场景，通过不同的移动设备和摄像设置捕捉。该集合包含了2,000多个高质量的全分辨率Raw图像对，用于散射炙，以及1,100个对，用于反射炙，可以进一步分解为30,000多个和2,200多个对的 patches。这确保了在多种捕捉条件下的广泛适用性。我们的实验结果表明，使用生成的数据进行训练的网络在真实图像集中表现不佳，而使用Raw图像数据具有显著的优势，能够有效地解决炙光除除问题。我们的集合预期会启动一系列新的研究，并为移动图像质量带来显著改善，对移动摄影者和用户都有利。”
</details></li>
</ul>
<hr>
<h2 id="Memory-Efficient-Graph-Convolutional-Networks-for-Object-Classification-and-Detection-with-Event-Cameras"><a href="#Memory-Efficient-Graph-Convolutional-Networks-for-Object-Classification-and-Detection-with-Event-Cameras" class="headerlink" title="Memory-Efficient Graph Convolutional Networks for Object Classification and Detection with Event Cameras"></a>Memory-Efficient Graph Convolutional Networks for Object Classification and Detection with Event Cameras</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14124">http://arxiv.org/abs/2307.14124</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kamil Jeziorek, Andrea Pinna, Tomasz Kryjak</li>
<li>for: This paper focuses on developing an efficient graph convolutional network (GCN) for event camera data, which is characterized by high temporal resolution, high dynamic range, low latency, and resistance to image blur.</li>
<li>methods: The paper compares different graph convolution operations and evaluates their performance in terms of execution time, number of trainable model parameters, data format requirements, and training outcomes.</li>
<li>results: The paper achieves a 450-fold reduction in the number of parameters for the feature extraction module and a 4.5-fold reduction in the size of the data representation while maintaining a classification accuracy of 52.3%, which is 6.3% higher compared to the operation used in state-of-the-art approaches. Additionally, the object detection architecture implemented in the paper achieved an accuracy of 53.7% <a href="mailto:&#109;&#65;&#80;&#64;&#x30;&#x2e;&#53;">&#109;&#65;&#80;&#64;&#x30;&#x2e;&#53;</a> and an execution rate of 82 graphs per second on the N-Caltech101 dataset.Here’s the simplified Chinese text for the three key information points:</li>
<li>for: 这篇论文关注了使用图 convolutional network (GCN) 处理事件摄像头数据，该数据具有高时间分辨率、高动态范围、低延迟和图像模糊鲁棒性。</li>
<li>methods: 论文比较了不同的图 convolution 操作，并评估其在执行时间、可训练模型参数数量、数据格式要求和训练结果等方面的性能。</li>
<li>results: 论文实现了Feature extraction模块中参数数量减少450倍，数据表示形式减少4.5倍，保持52.3%的分类精度，比state-of-the-art方法高6.3%。此外，在 N-Caltech101 数据集上，实现了Object detection 架构，达到了53.7% <a href="mailto:&#x6d;&#x41;&#x50;&#x40;&#48;&#x2e;&#53;">&#x6d;&#x41;&#x50;&#x40;&#48;&#x2e;&#53;</a> 的准确率和82个图像每秒执行速率。<details>
<summary>Abstract</summary>
Recent advances in event camera research emphasize processing data in its original sparse form, which allows the use of its unique features such as high temporal resolution, high dynamic range, low latency, and resistance to image blur. One promising approach for analyzing event data is through graph convolutional networks (GCNs). However, current research in this domain primarily focuses on optimizing computational costs, neglecting the associated memory costs. In this paper, we consider both factors together in order to achieve satisfying results and relatively low model complexity. For this purpose, we performed a comparative analysis of different graph convolution operations, considering factors such as execution time, the number of trainable model parameters, data format requirements, and training outcomes. Our results show a 450-fold reduction in the number of parameters for the feature extraction module and a 4.5-fold reduction in the size of the data representation while maintaining a classification accuracy of 52.3%, which is 6.3% higher compared to the operation used in state-of-the-art approaches. To further evaluate performance, we implemented the object detection architecture and evaluated its performance on the N-Caltech101 dataset. The results showed an accuracy of 53.7 % mAP@0.5 and reached an execution rate of 82 graphs per second.
</details>
<details>
<summary>摘要</summary>
Recent advances in event camera research emphasize processing data in its original sparse form, which allows the use of its unique features such as high temporal resolution, high dynamic range, low latency, and resistance to image blur. One promising approach for analyzing event data is through graph convolutional networks (GCNs). However, current research in this domain primarily focuses on optimizing computational costs, neglecting the associated memory costs. In this paper, we consider both factors together in order to achieve satisfying results and relatively low model complexity. For this purpose, we performed a comparative analysis of different graph convolution operations, considering factors such as execution time, the number of trainable model parameters, data format requirements, and training outcomes. Our results show a 450-fold reduction in the number of parameters for the feature extraction module and a 4.5-fold reduction in the size of the data representation while maintaining a classification accuracy of 52.3%, which is 6.3% higher compared to the operation used in state-of-the-art approaches. To further evaluate performance, we implemented the object detection architecture and evaluated its performance on the N-Caltech101 dataset. The results showed an accuracy of 53.7 % mAP@0.5 and reached an execution rate of 82 graphs per second.  Translation:最近的事件摄像头研究进展强调处理原始稀缺数据，利用其特有的特征，如高时间分辨率、高动态范围、低延迟和图像模糊抗衰减。一种承袭的方法是使用图像会议网络（GCNs）来分析事件数据。然而，当前研究领域主要关注计算成本，忽略相关的内存成本。在这篇论文中，我们同时考虑这两个因素，以实现满意的结果和相对较低的模型复杂度。为此，我们对不同的图像会议操作进行比较分析，考虑因素包括执行时间、可训练模型参数数量、数据格式要求和训练结果。我们的结果显示，在特征提取模块中减少了450倍的参数数量，并在数据表示中减少了4.5倍的大小，同时保持了52.3%的分类精度，与现有方法相比增加6.3%。为了进一步评估性能，我们实现了对象检测架构，并在N-Caltech101数据集上评估其性能。结果显示，在0.5的MAP@53.7%的情况下，执行速率达到82个图像每秒。
</details></li>
</ul>
<hr>
<h2 id="Periocular-biometrics-databases-algorithms-and-directions"><a href="#Periocular-biometrics-databases-algorithms-and-directions" class="headerlink" title="Periocular biometrics: databases, algorithms and directions"></a>Periocular biometrics: databases, algorithms and directions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14111">http://arxiv.org/abs/2307.14111</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fernando Alonso-Fernandez, Josef Bigun</li>
<li>for: 本文是一篇审查现有的周遭生物METRICS研究状况，提供了关于这一领域最重要的问题和现有文献的全面概述。</li>
<li>methods: 本文使用了周遭生物METRICS的多种方法，包括eyelids、lashes和eyebrows等特征提取方法，以及 gender classification和ethnicity classification等应用方法。</li>
<li>results: 本文提出了周遭生物METRICS的现状和未来发展趋势，并对现有文献进行了全面的概述和分析。<details>
<summary>Abstract</summary>
Periocular biometrics has been established as an independent modality due to concerns on the performance of iris or face systems in uncontrolled conditions. Periocular refers to the facial region in the eye vicinity, including eyelids, lashes and eyebrows. It is available over a wide range of acquisition distances, representing a trade-off between the whole face (which can be occluded at close distances) and the iris texture (which do not have enough resolution at long distances). Since the periocular region appears in face or iris images, it can be used also in conjunction with these modalities. Features extracted from the periocular region have been also used successfully for gender classification and ethnicity classification, and to study the impact of gender transformation or plastic surgery in the recognition performance. This paper presents a review of the state of the art in periocular biometric research, providing an insight of the most relevant issues and giving a thorough coverage of the existing literature. Future research trends are also briefly discussed.
</details>
<details>
<summary>摘要</summary>
《眼睛附近特征》已成为独立模式，由于肉眼或面部系统在不控制的情况下表现不佳。《眼睛附近特征》指的是脸部附近的眼睛区域，包括眼皮、毛发和毛发。它可以在各种获取距离范围内使用，表示距离整个脸部（可能会被近距离干扰）和眼球文字（没有足够的分辨率）之间的权衡。由于眼睛附近区域会出现在脸部或眼球图像中，因此它也可以与这些模式结合使用。从眼睛附近区域提取的特征已经成功地用于性别识别和种族识别，以及研究性别转换或整形手术对识别性的影响。这篇评论文章介绍了眼睛附近生物ometrics研究的现状，提供了最相关的问题的权衡和全面的文献评论。未来研究趋势也 briefly discussed。
</details></li>
</ul>
<hr>
<h2 id="Video-Decoding-Energy-Estimation-Using-Processor-Events"><a href="#Video-Decoding-Energy-Estimation-Using-Processor-Events" class="headerlink" title="Video Decoding Energy Estimation Using Processor Events"></a>Video Decoding Energy Estimation Using Processor Events</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14000">http://arxiv.org/abs/2307.14000</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Herglotz, André Kaup</li>
<li>for: 这篇论文是为了量化软件视频解码器的处理能量而写的。</li>
<li>methods: 该论文使用处理器事件如指令计数或缓存失败来准确估算软件视频解码器的处理能量。</li>
<li>results: 该论文通过对ARM基本评估平台进行能量测量，并使用专门的 profiling 软件对处理器事件进行计数，证明了我们的观察结果的一般可靠性。 用于不同的编码器和解码器实现，该方法可以准确地估算最新的视频编码标准HEVC和VP9中的真正解码能量，带有小于6%的平均估计误差。<details>
<summary>Abstract</summary>
In this paper, we show that processor events like instruction counts or cache misses can be used to accurately estimate the processing energy of software video decoders. Therefore, we perform energy measurements on an ARM-based evaluation platform and count processor level events using a dedicated profiling software. Measurements are performed for various codecs and decoder implementations to prove the general viability of our observations. Using the estimation method proposed in this paper, the true decoding energy for various recent video coding standards including HEVC and VP9 can be estimated with a mean estimation error that is smaller than 6%.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们证明处理器事件如指令计数或缓存miss可以准确地估计软件视频解码器的处理能量。因此，我们使用特定的 profiling 软件对ARM基本评估平台进行能量测量，并对不同的编码器和解码器实现进行测量。通过我们提出的估算方法，可以对最新的视频编码标准，包括HEVC和VP9，进行准确的估算， mean estimation error小于6%。
</details></li>
</ul>
<hr>
<h2 id="Hybrid-Representation-Enhanced-Sampling-for-Bayesian-Active-Learning-in-Musculoskeletal-Segmentation-of-Lower-Extremities"><a href="#Hybrid-Representation-Enhanced-Sampling-for-Bayesian-Active-Learning-in-Musculoskeletal-Segmentation-of-Lower-Extremities" class="headerlink" title="Hybrid Representation-Enhanced Sampling for Bayesian Active Learning in Musculoskeletal Segmentation of Lower Extremities"></a>Hybrid Representation-Enhanced Sampling for Bayesian Active Learning in Musculoskeletal Segmentation of Lower Extremities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13986">http://arxiv.org/abs/2307.13986</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ganping Li, Yoshito Otake, Mazen Soufi, Masashi Taniguchi, Masahide Yagi, Noriaki Ichihashi, Keisuke Uemura, Masaki Takao, Nobuhiko Sugano, Yoshinobu Sato<br>for: 降低医学图像分割任务中的手动标注成本methods: 使用 bayesian active learning 框架和 bayesian U-net，并采用混合表示性更新抽象策略，选择高密度和多样性的不确定样本进行手动修正，以优化最大化与未标注实例相似性，最小化与已有训练数据相似性。results: 在两个lower extremity（LE）数据集上，提出的方法与其他方法相比，在两种抽取规则下表现出超越或等效性，并且量化结果表明抽取规则的影响。我们的ablation study表明，将密度和多样性 criterion相结合使用，在musculoskeletal segmentation中表现出最佳性能。<details>
<summary>Abstract</summary>
Purpose: Obtaining manual annotations to train deep learning (DL) models for auto-segmentation is often time-consuming. Uncertainty-based Bayesian active learning (BAL) is a widely-adopted method to reduce annotation efforts. Based on BAL, this study introduces a hybrid representation-enhanced sampling strategy that integrates density and diversity criteria to save manual annotation costs by efficiently selecting the most informative samples.   Methods: The experiments are performed on two lower extremity (LE) datasets of MRI and CT images by a BAL framework based on Bayesian U-net. Our method selects uncertain samples with high density and diversity for manual revision, optimizing for maximal similarity to unlabeled instances and minimal similarity to existing training data. We assess the accuracy and efficiency using Dice and a proposed metric called reduced annotation cost (RAC), respectively. We further evaluate the impact of various acquisition rules on BAL performance and design an ablation study for effectiveness estimation.   Results: The proposed method showed superiority or non-inferiority to other methods on both datasets across two acquisition rules, and quantitative results reveal the pros and cons of the acquisition rules. Our ablation study in volume-wise acquisition shows that the combination of density and diversity criteria outperforms solely using either of them in musculoskeletal segmentation.   Conclusion: Our sampling method is proven efficient in reducing annotation costs in image segmentation tasks. The combination of the proposed method and our BAL framework provides a semi-automatic way for efficient annotation of medical image datasets.
</details>
<details>
<summary>摘要</summary>
目的：获取手动标注以训练深度学习（DL）模型的自动分割是经常占用时间。不确定性基于抽象学习（BAL）是一种广泛采用的方法，可以减少手动标注的努力。根据BAL，本研究引入了混合表示形式增强选择策略，将高密度和多样性的不确定样本选择为人工修改，以最大化与未标注实例的相似性，最小化与现有训练数据的相似性。方法：我们在两个下肢（LE）数据集上进行了MRI和CT图像的实验，使用基于抽象网络的BAL框架。我们的方法选择了不确定性高的高密度多样性样本进行人工修改，以优化最大化与未标注实例相似性，最小化与现有训练数据相似性。我们使用 dice和我们所提出的metriccalled减少注解成本（RAC）进行评价精度和效率。我们进一步evaluate了不同的获取规则对BAL性能的影响，并进行了效果鉴定研究。结果：我们的方法在两个数据集上表现出优异或等效于其他方法，并且量化结果表明了不同获取规则的优缺点。我们的拓展研究表明，将密度和多样性 критериria组合使用在musculoskeletal segmentation中表现出最佳效果。结论：我们的采样方法可以有效减少图像分割任务中的注解成本。将我们的采样方法与BAL框架结合使用，可以提供一种 semi-自动的方式，以便效率地注解医疗影像数据集。
</details></li>
</ul>
<hr>
<h2 id="Analysis-of-Video-Quality-Datasets-via-Design-of-Minimalistic-Video-Quality-Models"><a href="#Analysis-of-Video-Quality-Datasets-via-Design-of-Minimalistic-Video-Quality-Models" class="headerlink" title="Analysis of Video Quality Datasets via Design of Minimalistic Video Quality Models"></a>Analysis of Video Quality Datasets via Design of Minimalistic Video Quality Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13981">http://arxiv.org/abs/2307.13981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Sun, Wen Wen, Xiongkuo Min, Long Lan, Guangtao Zhai, Kede Ma</li>
<li>for: 本研究旨在帮助理解现有的视频质量评估（VQA）数据集，以便更好地评估当前的视频质量评估模型。</li>
<li>methods: 本研究使用了最简单的视频质量评估模型，包括视频预处理（快速空间时间抑制）、空间质量分析器和选项性的时间质量分析器，以及质量回归器。</li>
<li>results: 研究发现大多数数据集受到容易的数据集问题的影响，一些甚至可以使用盲图质量评估（BIQA）解决方案。研究还比较了不同的模型变体在不同数据集上的性能，并对模型的设计决策进行了ablation研究。<details>
<summary>Abstract</summary>
Blind video quality assessment (BVQA) plays an indispensable role in monitoring and improving the end-users' viewing experience in various real-world video-enabled media applications. As an experimental field, the improvements of BVQA models have been measured primarily on a few human-rated VQA datasets. Thus, it is crucial to gain a better understanding of existing VQA datasets in order to properly evaluate the current progress in BVQA. Towards this goal, we conduct a first-of-its-kind computational analysis of VQA datasets via designing minimalistic BVQA models. By minimalistic, we restrict our family of BVQA models to build only upon basic blocks: a video preprocessor (for aggressive spatiotemporal downsampling), a spatial quality analyzer, an optional temporal quality analyzer, and a quality regressor, all with the simplest possible instantiations. By comparing the quality prediction performance of different model variants on eight VQA datasets with realistic distortions, we find that nearly all datasets suffer from the easy dataset problem of varying severity, some of which even admit blind image quality assessment (BIQA) solutions. We additionally justify our claims by contrasting our model generalizability on these VQA datasets, and by ablating a dizzying set of BVQA design choices related to the basic building blocks. Our results cast doubt on the current progress in BVQA, and meanwhile shed light on good practices of constructing next-generation VQA datasets and models.
</details>
<details>
<summary>摘要</summary>
视频质量评估（BVQA）在视频启用媒体应用中扮演了不可或缺的角色，负责监测和改进用户的观看体验。作为实验领域，BVQA模型的改进主要通过一些人类评分的视频质量评估 dataset（VQA dataset）进行评估。因此，更深刻地理解现有VQA dataset是非常重要的。为了实现这个目标，我们首次对VQA dataset进行了计算性的分析，通过设计最简的BVQA模型来评估现有VQA dataset的质量。我们的BVQA模型仅使用了基本块：视频预处理（为激进的空间时间采样）、空间质量分析器、可选的时间质量分析器和质量回归器，其中每个块都使用最简的实现。通过对不同模型变体在八个VQA dataset上的质量预测性能进行比较，我们发现大多数dataset受到不同程度的易于评估（Easy dataset problem），一些甚至可以使用盲图质量评估（BIQA）解决方案。此外，我们还对不同的BVQA设计选择进行了比较，并将其与不同的VQA dataset进行了对比。我们的结果表明，现有的BVQA进展并不如人们所期望的，同时也提供了构建下一代VQA dataset和模型的好做法。
</details></li>
</ul>
<hr>
<h2 id="A-real-time-material-breakage-detection-for-offshore-wind-turbines-based-on-improved-neural-network-algorithm"><a href="#A-real-time-material-breakage-detection-for-offshore-wind-turbines-based-on-improved-neural-network-algorithm" class="headerlink" title="A real-time material breakage detection for offshore wind turbines based on improved neural network algorithm"></a>A real-time material breakage detection for offshore wind turbines based on improved neural network algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13765">http://arxiv.org/abs/2307.13765</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yantong Liu</li>
<li>for: 这种研究旨在提高海上风电机维护效率，为可持续能源生产做出重要贡献。</li>
<li>methods: 这种方法使用了一种改进后的YOLOv8物体检测模型，以及一个卷积杂志注意模块（CBAM），以提高特征识别能力。</li>
<li>results: 研究发现，使用这种方法可以提高杂点检测稳定性，代表了重要的维护技术突破。<details>
<summary>Abstract</summary>
The integrity of offshore wind turbines, pivotal for sustainable energy generation, is often compromised by surface material defects. Despite the availability of various detection techniques, limitations persist regarding cost-effectiveness, efficiency, and applicability. Addressing these shortcomings, this study introduces a novel approach leveraging an advanced version of the YOLOv8 object detection model, supplemented with a Convolutional Block Attention Module (CBAM) for improved feature recognition. The optimized loss function further refines the learning process. Employing a dataset of 5,432 images from the Saemangeum offshore wind farm and a publicly available dataset, our method underwent rigorous testing. The findings reveal a substantial enhancement in defect detection stability, marking a significant stride towards efficient turbine maintenance. This study's contributions illuminate the path for future research, potentially revolutionizing sustainable energy practices.
</details>
<details>
<summary>摘要</summary>
Offshore风电机的完整性，对可再生能源生产是关键的，但这些材料表面的瑕疵常常会对其产生影响。虽然有各种检测技术可供选择，但这些技术受到成本、效率和应用限制。本研究提出了一种新的方法，利用进化版YOLOv8物体检测模型，加上卷积块注意模块（CBAM），以提高特征识别。通过优化损失函数，进一步适应学习。使用了5432张采集自韩国岛风电农场和公开available的数据集，我们的方法经过了严格的测试。发现的结果表明，我们的方法可以增强瑕疵检测稳定性，代表了可再生能源实践中的一个重要突破。本研究的贡献，照明了未来研究的道路，有望革命化可再生能源实践。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/07/26/eess.IV_2023_07_26/" data-id="cllt9pry2008mol88arywbm2f" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/26/eess.AS_2023_07_26/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.AS - 2023-07-26 22:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/25/cs.LG_2023_07_25/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-07-25 18:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
