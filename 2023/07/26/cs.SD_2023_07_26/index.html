
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-07-26 123:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Say Goodbye to RNN-T Loss: A Novel CIF-based Transducer Architecture for Automatic Speech Recognition paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.14132 repo_url: None paper_authors: Tian-Hao Zhang, Dinghao Z">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-07-26 123:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/07/26/cs.SD_2023_07_26/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Say Goodbye to RNN-T Loss: A Novel CIF-based Transducer Architecture for Automatic Speech Recognition paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.14132 repo_url: None paper_authors: Tian-Hao Zhang, Dinghao Z">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-25T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:38.709Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_07_26" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/26/cs.SD_2023_07_26/" class="article-date">
  <time datetime="2023-07-25T16:00:00.000Z" itemprop="datePublished">2023-07-26</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-07-26 123:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Say-Goodbye-to-RNN-T-Loss-A-Novel-CIF-based-Transducer-Architecture-for-Automatic-Speech-Recognition"><a href="#Say-Goodbye-to-RNN-T-Loss-A-Novel-CIF-based-Transducer-Architecture-for-Automatic-Speech-Recognition" class="headerlink" title="Say Goodbye to RNN-T Loss: A Novel CIF-based Transducer Architecture for Automatic Speech Recognition"></a>Say Goodbye to RNN-T Loss: A Novel CIF-based Transducer Architecture for Automatic Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14132">http://arxiv.org/abs/2307.14132</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tian-Hao Zhang, Dinghao Zhou, Guiping Zhong, Baoxiang Li</li>
<li>for: 提高 ASR 模型的效率和精度</li>
<li>methods: 提出一种新的 CIF-Transducer 模型，具有终端听写 Integrate-and-Fire 机制，从而减少计算复杂度并让预测网络扮演更重要的角色。同时，引入 Funnel-CIF、Context Blocks、Unified Gating 和 Bilinear Pooling 等网络结构和辅助训练策略来进一步提高性能。</li>
<li>results: 在 AISHELL-1 和 WenetSpeech 等 dataset 上进行了实验，显示 CIF-T 模型可以与 RNN-T 模型相比，并且具有较低的计算复杂度和更高的精度。<details>
<summary>Abstract</summary>
RNN-T models are widely used in ASR, which rely on the RNN-T loss to achieve length alignment between input audio and target sequence. However, the implementation complexity and the alignment-based optimization target of RNN-T loss lead to computational redundancy and a reduced role for predictor network, respectively. In this paper, we propose a novel model named CIF-Transducer (CIF-T) which incorporates the Continuous Integrate-and-Fire (CIF) mechanism with the RNN-T model to achieve efficient alignment. In this way, the RNN-T loss is abandoned, thus bringing a computational reduction and allowing the predictor network a more significant role. We also introduce Funnel-CIF, Context Blocks, Unified Gating and Bilinear Pooling joint network, and auxiliary training strategy to further improve performance. Experiments on the 178-hour AISHELL-1 and 10000-hour WenetSpeech datasets show that CIF-T achieves state-of-the-art results with lower computational overhead compared to RNN-T models.
</details>
<details>
<summary>摘要</summary>
RNN-T模型广泛应用于语音识别领域，它们依赖于RNN-T损失来实现输入音频和目标序列之间的长度对齐。然而，实现复杂性和对齐基于优化目标导致RNN-T模型中的计算循环和预测网络的role减小。在这篇论文中，我们提出了一种新的模型名为CIF-Transducer（CIF-T），它将Continuous Integrate-and-Fire（CIF）机制与RNN-T模型结合起来实现有效的对齐。这样，RNN-T损失可以被抛弃，从而实现计算减少和预测网络的更大角色。我们还引入了滤波器-CIF、上下文块、统一阀值和bilinear pooling的联合网络，以及辅助训练策略，以进一步提高性能。实验表明，CIF-T在AISHELL-1和WenetSpeech数据集上达到了状态机器的Result，同时计算负担相对较低。
</details></li>
</ul>
<hr>
<h2 id="The-Hidden-Dance-of-Phonemes-and-Visage-Unveiling-the-Enigmatic-Link-between-Phonemes-and-Facial-Features"><a href="#The-Hidden-Dance-of-Phonemes-and-Visage-Unveiling-the-Enigmatic-Link-between-Phonemes-and-Facial-Features" class="headerlink" title="The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features"></a>The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13953">http://arxiv.org/abs/2307.13953</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liao Qu, Xianwei Zou, Xiang Li, Yandong Wen, Rita Singh, Bhiksha Raj</li>
<li>for: 本研究揭示了声音和脸部特征之间的潜在关系。传统的voice-face相关性研究通常使用长时间的声音输入，包括从声音生成脸像和从声音重建3D脸镜。但在voice-based犯罪调查中，可能有限的声音证据。此外，从生物学角度来看，每个语音段（phoneme）对应不同的空气流和脸部运动。因此，发现声音和脸部特征之间的隐藏关系是有利的。</li>
<li>methods: 本研究提出了一个细化的分析管道，用于探索声音和脸部之间的关系，即phonemes v.s. 脸部 anthropometric measurements (AM)。我们建立了每个声音-AM对的估计器，并通过假设检测来评估相关性。我们的结果表明，AMs在元音上更易预测，特别是与塞擦音相关。此外，我们发现，在某些AM的发音时，它们更易预测。</li>
<li>results: 我们的结果支持生物学中的相关性理论，并为未来的speech-face多模态学习奠定基础。<details>
<summary>Abstract</summary>
This work unveils the enigmatic link between phonemes and facial features. Traditional studies on voice-face correlations typically involve using a long period of voice input, including generating face images from voices and reconstructing 3D face meshes from voices. However, in situations like voice-based crimes, the available voice evidence may be short and limited. Additionally, from a physiological perspective, each segment of speech -- phoneme -- corresponds to different types of airflow and movements in the face. Therefore, it is advantageous to discover the hidden link between phonemes and face attributes. In this paper, we propose an analysis pipeline to help us explore the voice-face relationship in a fine-grained manner, i.e., phonemes v.s. facial anthropometric measurements (AM). We build an estimator for each phoneme-AM pair and evaluate the correlation through hypothesis testing. Our results indicate that AMs are more predictable from vowels compared to consonants, particularly with plosives. Additionally, we observe that if a specific AM exhibits more movement during phoneme pronunciation, it is more predictable. Our findings support those in physiology regarding correlation and lay the groundwork for future research on speech-face multimodal learning.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose an analysis pipeline to explore the voice-face relationship in a fine-grained manner, specifically, phonemes versus facial anthropometric measurements (AM). We build an estimator for each phoneme-AM pair and evaluate the correlation through hypothesis testing. Our results show that AMs are more predictable from vowels compared to consonants, particularly with plosives. Additionally, we find that if a specific AM exhibits more movement during phoneme pronunciation, it is more predictable. Our findings support those in physiology regarding the correlation and lay the foundation for future research on speech-face multimodal learning.
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Voice-Face-Correlation-A-Geometry-View"><a href="#Rethinking-Voice-Face-Correlation-A-Geometry-View" class="headerlink" title="Rethinking Voice-Face Correlation: A Geometry View"></a>Rethinking Voice-Face Correlation: A Geometry View</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13948">http://arxiv.org/abs/2307.13948</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lxa9867/VAF">https://github.com/lxa9867/VAF</a></li>
<li>paper_authors: Xiang Li, Yandong Wen, Muqiao Yang, Jinglu Wang, Rita Singh, Bhiksha Raj</li>
<li>for:  investigate the capability of reconstructing 3D facial shape from voice from a geometry perspective without any semantic information.</li>
<li>methods: propose a voice-anthropometric measurement (AM)-face paradigm, which identifies predictable facial AMs from the voice and uses them to guide 3D face reconstruction.</li>
<li>results: significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium.<details>
<summary>Abstract</summary>
Previous works on voice-face matching and voice-guided face synthesis demonstrate strong correlations between voice and face, but mainly rely on coarse semantic cues such as gender, age, and emotion. In this paper, we aim to investigate the capability of reconstructing the 3D facial shape from voice from a geometry perspective without any semantic information. We propose a voice-anthropometric measurement (AM)-face paradigm, which identifies predictable facial AMs from the voice and uses them to guide 3D face reconstruction. By leveraging AMs as a proxy to link the voice and face geometry, we can eliminate the influence of unpredictable AMs and make the face geometry tractable. Our approach is evaluated on our proposed dataset with ground-truth 3D face scans and corresponding voice recordings, and we find significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium. Our work offers a new perspective on voice-face correlation and can serve as a good empirical study for anthropometry science.
</details>
<details>
<summary>摘要</summary>
previous works on voice-face matching and voice-guided face synthesis have shown strong correlations between voice and face, but mainly rely on coarse semantic cues such as gender, age, and emotion. in this paper, we aim to investigate the capability of reconstructing the 3D facial shape from voice from a geometry perspective without any semantic information. we propose a voice-anthropometric measurement (AM)-face paradigm, which identifies predictable facial AMs from the voice and uses them to guide 3D face reconstruction. by leveraging AMs as a proxy to link the voice and face geometry, we can eliminate the influence of unpredictable AMs and make the face geometry tractable. our approach is evaluated on our proposed dataset with ground-truth 3D face scans and corresponding voice recordings, and we find significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium. our work offers a new perspective on voice-face correlation and can serve as a good empirical study for anthropometry science.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know and I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Perceptual-Quality-Enhancement-of-Sound-Field-Synthesis-Based-on-Combination-of-Pressure-and-Amplitude-Matching"><a href="#Perceptual-Quality-Enhancement-of-Sound-Field-Synthesis-Based-on-Combination-of-Pressure-and-Amplitude-Matching" class="headerlink" title="Perceptual Quality Enhancement of Sound Field Synthesis Based on Combination of Pressure and Amplitude Matching"></a>Perceptual Quality Enhancement of Sound Field Synthesis Based on Combination of Pressure and Amplitude Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13941">http://arxiv.org/abs/2307.13941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keisuke Kimura, Shoichi Koyama, Hiroshi Saruwatari</li>
<li>for: 提高听众对声场的 восприятие质量</li>
<li>methods: 使用压力和幅度匹配方法组合来减少高频 synthesis 错误，并使用声场的人类听觉特性来synthesize 横向声localization</li>
<li>results: 比较实验和听觉测试表明，提出的方法可以提高声场synthesized的 восприятие质量，比传统压力匹配方法更好。<details>
<summary>Abstract</summary>
A sound field synthesis method enhancing perceptual quality is proposed. Sound field synthesis using multiple loudspeakers enables spatial audio reproduction with a broad listening area; however, synthesis errors at high frequencies called spatial aliasing artifacts are unavoidable. To minimize these artifacts, we propose a method based on the combination of pressure and amplitude matching. On the basis of the human's auditory properties, synthesizing the amplitude distribution will be sufficient for horizontal sound localization. Furthermore, a flat amplitude response should be synthesized as much as possible to avoid coloration. Therefore, we apply amplitude matching, which is a method to synthesize the desired amplitude distribution with arbitrary phase distribution, for high frequencies and conventional pressure matching for low frequencies. Experimental results of numerical simulations and listening tests using a practical system indicated that the perceptual quality of the sound field synthesized by the proposed method was improved from that synthesized by pressure matching.
</details>
<details>
<summary>摘要</summary>
一种提高听觉质量的声场合成方法被提出。使用多个扬音器实现声场合成可以提供广泛的听众区域，但高频合成错误无法避免。为减少这些错误，我们提出基于压力和幅度匹配的方法。根据人类听觉特性，Synthesizing amplitude distribution是足够的 для水平声localization。此外，尽可能地Synthesizing平坦的幅度响应可以避免染色。因此，我们应用幅度匹配，即Synthesizing所需的幅度分布的任意相位分布，高频和普通压力匹配low frequency。实验结果表明，由提出的方法synthesize的听field质量与压力匹配相比有所提高。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Interactions-between-Target-Positive-and-Negative-Information-for-Acoustic-Echo-Cancellation"><a href="#Exploring-the-Interactions-between-Target-Positive-and-Negative-Information-for-Acoustic-Echo-Cancellation" class="headerlink" title="Exploring the Interactions between Target Positive and Negative Information for Acoustic Echo Cancellation"></a>Exploring the Interactions between Target Positive and Negative Information for Acoustic Echo Cancellation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13888">http://arxiv.org/abs/2307.13888</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chang Han, Xinmeng Xu, Weiping Tu, Yuhong Yang, Yajie Liu</li>
<li>for: 降低干扰信号的干扰 echo 抑制（AEC），以便保留近端语音最少受到损害。</li>
<li>methods: 我们提出了一种新的AEC模型encoder-decoder架构，以质量指导的方式使用目标负信息（如干扰信号和特征）来帮助模型更好地分辨目标语音和干扰信号的模式。</li>
<li>results: 我们的CMNet模型在实验中表现出了较好的性能，比如最近的方法。<details>
<summary>Abstract</summary>
Acoustic echo cancellation (AEC) aims to remove interference signals while leaving near-end speech least distorted. As the indistinguishable patterns between near-end speech and interference signals, near-end speech can't be separated completely, causing speech distortion and interference signals residual. We observe that besides target positive information, e.g., ground-truth speech and features, the target negative information, such as interference signals and features, helps make pattern of target speech and interference signals more discriminative. Therefore, we present a novel AEC model encoder-decoder architecture with the guidance of negative information termed as CMNet. A collaboration module (CM) is designed to establish the correlation between the target positive and negative information in a learnable manner via three blocks: target positive, target negative, and interactive block. Experimental results demonstrate our CMNet achieves superior performance than recent methods.
</details>
<details>
<summary>摘要</summary>
宽band acoustic echo cancellation（AEC）目的是去除干扰信号，保留近端语音最小变形。由于干扰信号和近端语音干扰信号的模式相同，因此无法完全分离近端语音，导致语音扭曲和干扰信号剩下。我们发现，除了目标正面信息（如真实语音和特征）之外，目标负面信息（如干扰信号和特征）也有助于使target speech和干扰信号的模式更加分化。因此，我们提出了一种基于encoder-decoder架构的新型AEC模型，称为CMNet。协作模块（CM）通过三个块（目标正面、目标负面和互动块）来在学习方式下建立目标正面和负面信息之间的相关性。实验结果表明，我们的CMNet在latest方法之上具有更高的性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/26/cs.SD_2023_07_26/" data-id="cllsj9wz6004guv8889gkd6gk" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/26/cs.LG_2023_07_26/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-07-26 18:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/26/eess.AS_2023_07_26/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.AS - 2023-07-26 22:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'', root:''}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
