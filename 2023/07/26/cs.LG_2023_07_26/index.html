
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-07-26 18:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Fluorescent Neuronal Cells v2: Multi-Task, Multi-Format Annotations for Deep Learning in Microscopy paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.14243 repo_url: None paper_authors: Luca Clissa, Antonio Macalu">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-07-26 18:00:00">
<meta property="og:url" content="http://nullscc.github.io/2023/07/26/cs.LG_2023_07_26/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Fluorescent Neuronal Cells v2: Multi-Task, Multi-Format Annotations for Deep Learning in Microscopy paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.14243 repo_url: None paper_authors: Luca Clissa, Antonio Macalu">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-25T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:38.599Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_07_26" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/26/cs.LG_2023_07_26/" class="article-date">
  <time datetime="2023-07-25T16:00:00.000Z" itemprop="datePublished">2023-07-26</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-07-26 18:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Fluorescent-Neuronal-Cells-v2-Multi-Task-Multi-Format-Annotations-for-Deep-Learning-in-Microscopy"><a href="#Fluorescent-Neuronal-Cells-v2-Multi-Task-Multi-Format-Annotations-for-Deep-Learning-in-Microscopy" class="headerlink" title="Fluorescent Neuronal Cells v2: Multi-Task, Multi-Format Annotations for Deep Learning in Microscopy"></a>Fluorescent Neuronal Cells v2: Multi-Task, Multi-Format Annotations for Deep Learning in Microscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14243">http://arxiv.org/abs/2307.14243</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Clissa, Antonio Macaluso, Roberto Morelli, Alessandra Occhinegro, Emiliana Piscitiello, Ludovico Taddei, Marco Luppi, Roberto Amici, Matteo Cerri, Timna Hitrec, Lorenzo Rinaldi, Antonio Zoccoli</li>
<li>for: This paper is written for researchers and scientists in the fields of life sciences and deep learning, with the goal of facilitating innovative research and methodological advancements in fluorescence microscopy analysis.</li>
<li>methods: The paper provides a collection of fluorescence microscopy images and ground-truth annotations for several learning tasks, including semantic segmentation, object detection, and counting. The images are annotated with diverse markers to highlight the anatomical or functional characteristics of rodent neuronal cells.</li>
<li>results: The paper aims to facilitate breakthroughs in fluorescence microscopy analysis and promote cutting-edge discoveries in life sciences by providing a comprehensive and accessible dataset for researchers to explore and benchmark their methods.<details>
<summary>Abstract</summary>
Fluorescent Neuronal Cells v2 is a collection of fluorescence microscopy images and the corresponding ground-truth annotations, designed to foster innovative research in the domains of Life Sciences and Deep Learning. This dataset encompasses three image collections in which rodent neuronal cells' nuclei and cytoplasm are stained with diverse markers to highlight their anatomical or functional characteristics. Alongside the images, we provide ground-truth annotations for several learning tasks, including semantic segmentation, object detection, and counting. The contribution is two-fold. First, given the variety of annotations and their accessible formats, we envision our work facilitating methodological advancements in computer vision approaches for segmentation, detection, feature learning, unsupervised and self-supervised learning, transfer learning, and related areas. Second, by enabling extensive exploration and benchmarking, we hope Fluorescent Neuronal Cells v2 will catalyze breakthroughs in fluorescence microscopy analysis and promote cutting-edge discoveries in life sciences. The data are available at: https://amsacta.unibo.it/id/eprint/7347
</details>
<details>
<summary>摘要</summary>
fluorescent neuronal cells v2 是一个包含 fluorescence microscopy 图像和相应的真实标注的集合，旨在推动生命科学和深度学习领域的创新研究。这个数据集包括三个图像集，其中 rodent neuronal cells 的核心和细胞质是使用不同标记物来高亮其形态或功能特征。同时，我们提供了真实标注数据，用于多种学习任务，包括semantic segmentation、object detection和计数。我们的贡献是两重。首先，由于数据集的多样性和可访问的格式，我们期望我们的工作会促进计算机视觉方法的进步，包括分割、检测、特征学习、自监学习、转移学习等领域。其次，通过提供广泛的探索和测试，我们希望 fluorescent neuronal cells v2 会促进 fluorescence microscopy 分析的进步，并促进生命科学的前沿研究。数据可以在以下地址获取：https://amsacta.unibo.it/id/eprint/7347
</details></li>
</ul>
<hr>
<h2 id="Evolving-Multi-Objective-Neural-Network-Controllers-for-Robot-Swarms"><a href="#Evolving-Multi-Objective-Neural-Network-Controllers-for-Robot-Swarms" class="headerlink" title="Evolving Multi-Objective Neural Network Controllers for Robot Swarms"></a>Evolving Multi-Objective Neural Network Controllers for Robot Swarms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14237">http://arxiv.org/abs/2307.14237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karl Mason, Sabine Hauert</li>
<li>for: 本研究旨在提出一种多目标进化神经网络控制器来解决群体 робоット控制问题，以实现多个目标的同时满足。</li>
<li>methods: 该研究使用了一种多目标进化神经网络方法，通过在低精度Python模拟环境中训练控制器，然后在Webots高精度模拟环境中测试和评估控制器。</li>
<li>results: 研究结果表明，提出的方法可以有效地控制每个机器人，并且可以根据目标权重的调整而变化机器人群体的行为。同时，研究还证明了这种控制器可以在高精度模拟环境中进行规模化扩展，无需进一步 retrained。<details>
<summary>Abstract</summary>
Many swarm robotics tasks consist of multiple conflicting objectives. This research proposes a multi-objective evolutionary neural network approach to developing controllers for swarms of robots. The swarm robot controllers are trained in a low-fidelity Python simulator and then tested in a high-fidelity simulated environment using Webots. Simulations are then conducted to test the scalability of the evolved multi-objective robot controllers to environments with a larger number of robots. The results presented demonstrate that the proposed approach can effectively control each of the robots. The robot swarm exhibits different behaviours as the weighting for each objective is adjusted. The results also confirm that multi-objective neural network controllers evolved in a low-fidelity simulator can be transferred to high-fidelity simulated environments and that the controllers can scale to environments with a larger number of robots without further retraining needed.
</details>
<details>
<summary>摘要</summary>
多数蜂群控制任务包含多个冲突目标。本研究提出了一种多目标进化神经网络方法来开发蜂群机器人控制器。蜂群机器人控制器在低精度Python模拟器中训练，然后在使用Webots高精度模拟环境进行测试。为了评估算法的扩展性，在不同目标权重的情况下进行了多个 simulations。结果表明，提议的方法可以有效控制每个机器人。机器人蜂群在不同目标权重下展现出不同的行为。结果还证明了在低精度模拟器中进行多目标神经网络控制器的进化后，可以将控制器转移到高精度模拟环境中，并且控制器可以扩展到包含更多机器人的环境无需进行进一步 retrained。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-are-Competitive-Near-Cold-start-Recommenders-for-Language-and-Item-based-Preferences"><a href="#Large-Language-Models-are-Competitive-Near-Cold-start-Recommenders-for-Language-and-Item-based-Preferences" class="headerlink" title="Large Language Models are Competitive Near Cold-start Recommenders for Language- and Item-based Preferences"></a>Large Language Models are Competitive Near Cold-start Recommenders for Language- and Item-based Preferences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14225">http://arxiv.org/abs/2307.14225</a></li>
<li>repo_url: None</li>
<li>paper_authors: Scott Sanner, Krisztian Balog, Filip Radlinski, Ben Wedin, Lucas Dixon</li>
<li>for: 研究使用大语言模型（LLM）来提供建议，并比较其与现有的项目相关的协同维度（CF）方法。</li>
<li>methods: 采集了基于项目和语言的偏好的用户评分数据，并使用这些数据来训练LLM。</li>
<li>results: LLM可以在冷启动情况下提供竞争力强的建议，尤其是在没有超参数（zero-shot）或只有几个标签（few-shot）的情况下。这些结果表明语言基本偏好表示更加可解释和可读取。<details>
<summary>Abstract</summary>
Traditional recommender systems leverage users' item preference history to recommend novel content that users may like. However, modern dialog interfaces that allow users to express language-based preferences offer a fundamentally different modality for preference input. Inspired by recent successes of prompting paradigms for large language models (LLMs), we study their use for making recommendations from both item-based and language-based preferences in comparison to state-of-the-art item-based collaborative filtering (CF) methods. To support this investigation, we collect a new dataset consisting of both item-based and language-based preferences elicited from users along with their ratings on a variety of (biased) recommended items and (unbiased) random items. Among numerous experimental results, we find that LLMs provide competitive recommendation performance for pure language-based preferences (no item preferences) in the near cold-start case in comparison to item-based CF methods, despite having no supervised training for this specific task (zero-shot) or only a few labels (few-shot). This is particularly promising as language-based preference representations are more explainable and scrutable than item-based or vector-based representations.
</details>
<details>
<summary>摘要</summary>
传统推荐系统利用用户ITEM喜好历史来推荐新内容，但现代对话界面允许用户通过语言基于喜好表达新的可能性。 inspirited by recent successes of prompting paradigms for large language models (LLMs), we investigate their use for making recommendations from both item-based and language-based preferences in comparison to state-of-the-art item-based collaborative filtering (CF) methods. To support this investigation, we collect a new dataset consisting of both item-based and language-based preferences elicited from users along with their ratings on a variety of (biased) recommended items and (unbiased) random items. Among numerous experimental results, we find that LLMs provide competitive recommendation performance for pure language-based preferences (no item preferences) in the near cold-start case in comparison to item-based CF methods, despite having no supervised training for this specific task (zero-shot) or only a few labels (few-shot). This is particularly promising as language-based preference representations are more explainable and scrutable than item-based or vector-based representations.
</details></li>
</ul>
<hr>
<h2 id="Online-Modeling-and-Monitoring-of-Dependent-Processes-under-Resource-Constraints"><a href="#Online-Modeling-and-Monitoring-of-Dependent-Processes-under-Resource-Constraints" class="headerlink" title="Online Modeling and Monitoring of Dependent Processes under Resource Constraints"></a>Online Modeling and Monitoring of Dependent Processes under Resource Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14208">http://arxiv.org/abs/2307.14208</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tanapol Kosolwattana, Huazheng Wang, Ying Lin</li>
<li>for: 监测受限资源的依赖过程集体，检测异常事件是非常重要的。</li>
<li>methods: 提出了一种基于在线协同学习的资源分配策略，以适应高风险过程的探索和依赖动力学的探索。</li>
<li>results: 通过理论分析和实验验证，效果良好。<details>
<summary>Abstract</summary>
Monitoring a population of dependent processes under limited resources is critical for abnormal events detection. A novel online collaborative learning method is proposed to adaptively allocate the resources for exploitation of high-risk processes and exploration of dependent dynamics. Efficiency of the proposed method is proved through theoretical analysis and experiments.
</details>
<details>
<summary>摘要</summary>
监测具有限制资源的依赖过程群体是检测异常事件的关键。一种基于在线合作学习的新方法是提议的，以适应分配资源以便利用高风险过程的探索和依赖动态的探索。我们通过理论分析和实验证明了提议的方法的效率。
</details></li>
</ul>
<hr>
<h2 id="Application-of-Random-Forest-and-Support-Vector-Machine-for-Investigation-of-Pressure-Filtration-Performance-a-Zinc-Plant-Filter-Cake-Modeling"><a href="#Application-of-Random-Forest-and-Support-Vector-Machine-for-Investigation-of-Pressure-Filtration-Performance-a-Zinc-Plant-Filter-Cake-Modeling" class="headerlink" title="Application of Random Forest and Support Vector Machine for Investigation of Pressure Filtration Performance, a Zinc Plant Filter Cake Modeling"></a>Application of Random Forest and Support Vector Machine for Investigation of Pressure Filtration Performance, a Zinc Plant Filter Cake Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14199">http://arxiv.org/abs/2307.14199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masoume Kazemi, Davood Moradkhani, Alireza Abbas Alipour</li>
<li>for: 这个研究旨在预测压缩滤过程中的萃取物湿度。</li>
<li>methods: 研究使用Random Forest（RF）和Support Vector Machine（SVM）模型来模型压缩滤过程。</li>
<li>results: 研究发现，Random Forest Regression（RFR）模型在预测萃取物湿度方面比Support Vector Regression（SVR）模型表现更好，RFR模型的准确预测率较高。<details>
<summary>Abstract</summary>
The hydrometallurgical method of zinc production involves leaching zinc from ore and then separating the solid residue from the liquid solution by pressure filtration. This separation process is very important since the solid residue contains some moisture that can reduce the amount of zinc recovered. This study modeled the pressure filtration process through Random Forest (RF) and Support Vector Machine (SVM). The models take continuous variables (extracted features) from the lab samples as inputs. Thus, regression models namely Random Forest Regression (RFR) and Support Vector Regression (SVR) were chosen. A total dataset was obtained during the pressure filtration process in two conditions: 1) Polypropylene (S1) and 2) Polyester fabrics (S2). To predict the cake moisture, solids concentration (0.2 and 0.38), temperature (35 and 65 centigrade), pH (2, 3.5, and 5), pressure, cake thickness (14, 20, 26, and 34 mm), air-blow time (2, 10 and 15 min) and filtration time were applied as input variables. The models' predictive accuracy was evaluated by the coefficient of determination (R2) parameter. The results revealed that the RFR model is superior to the SVR model for cake moisture prediction.
</details>
<details>
<summary>摘要</summary>
《氧化锌生产技术中的压力分离过程是非常重要的，因为固体剩下物含有一定的湿度，这可能会减少锌的回收量。本研究使用Random Forest（RF）和Support Vector Machine（SVM）模型来模拟压力分离过程。这些模型接受实验室样本中的连续变量（提取特征）作为输入。因此，回归模型Random Forest Regression（RFR）和Support Vector Regression（SVR）被选择。一个总体数据集在压力分离过程中得到了，包括了两种条件：1）聚乙烯（S1）和2）聚醚纤维（S2）。为预测固体湿度，输入变量包括了粉煤浓度（0.2和0.38）、温度（35和65℃）、pH（2、3.5和5）、压力、压力分离厚度（14、20、26和34mm）、气流时间（2、10和15分）和过滤时间。模型的预测精度被评估通过R2参数。结果显示，RFR模型在预测固体湿度方面比SVR模型更高。》Note: Please note that the translation is in Simplified Chinese, and the word order and sentence structure may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="Efficient-Learning-of-Discrete-Continuous-Computation-Graphs"><a href="#Efficient-Learning-of-Discrete-Continuous-Computation-Graphs" class="headerlink" title="Efficient Learning of Discrete-Continuous Computation Graphs"></a>Efficient Learning of Discrete-Continuous Computation Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14193">http://arxiv.org/abs/2307.14193</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nec-research/dccg">https://github.com/nec-research/dccg</a></li>
<li>paper_authors: David Friede, Mathias Niepert</li>
<li>for: 该论文旨在探讨混合抽象和连续模型在超级vised和强化学习中的应用。</li>
<li>methods: 该论文使用了泛化的推理抽象技术，并提出了两种新的策略来解决训练过程中的问题。</li>
<li>results: 实验结果显示，使用混合抽象和连续模型可以训练更复杂的模型，并且这些模型在一些基准数据集上的泛化性比其连续counterpart更好。<details>
<summary>Abstract</summary>
Numerous models for supervised and reinforcement learning benefit from combinations of discrete and continuous model components. End-to-end learnable discrete-continuous models are compositional, tend to generalize better, and are more interpretable. A popular approach to building discrete-continuous computation graphs is that of integrating discrete probability distributions into neural networks using stochastic softmax tricks. Prior work has mainly focused on computation graphs with a single discrete component on each of the graph's execution paths. We analyze the behavior of more complex stochastic computations graphs with multiple sequential discrete components. We show that it is challenging to optimize the parameters of these models, mainly due to small gradients and local minima. We then propose two new strategies to overcome these challenges. First, we show that increasing the scale parameter of the Gumbel noise perturbations during training improves the learning behavior. Second, we propose dropout residual connections specifically tailored to stochastic, discrete-continuous computation graphs. With an extensive set of experiments, we show that we can train complex discrete-continuous models which one cannot train with standard stochastic softmax tricks. We also show that complex discrete-stochastic models generalize better than their continuous counterparts on several benchmark datasets.
</details>
<details>
<summary>摘要</summary>
多种超visited和强化学习模型受益于混合 discrete和连续模型组件。结构化的end-to-end学习可以使模型更加易于理解和掌控。常见的方法是通过将抽象概率分布 integrate到神经网络中使用随机softmax技巧。先前的工作主要集中在单个执行路径上的 computation graphs上。我们分析了多个级别的随机计算图，并证明这些模型的参数优化具有挑战性，主要是因为小Gradient和地方最小值。我们 then propose two new strategies to overcome these challenges. First, we show that increasing the scale parameter of the Gumbel noise perturbations during training improves the learning behavior. Second, we propose dropout residual connections specifically tailored to stochastic, discrete-continuous computation graphs. With an extensive set of experiments, we show that we can train complex discrete-continuous models that cannot be trained with standard stochastic softmax tricks. We also show that complex discrete-stochastic models generalize better than their continuous counterparts on several benchmark datasets.
</details></li>
</ul>
<hr>
<h2 id="A-comparison-of-machine-learning-surrogate-models-of-street-scale-flooding-in-Norfolk-Virginia"><a href="#A-comparison-of-machine-learning-surrogate-models-of-street-scale-flooding-in-Norfolk-Virginia" class="headerlink" title="A comparison of machine learning surrogate models of street-scale flooding in Norfolk, Virginia"></a>A comparison of machine learning surrogate models of street-scale flooding in Norfolk, Virginia</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14185">http://arxiv.org/abs/2307.14185</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diana McSpadden, Steven Goldenberg, Binata Roy, Malachi Schram, Jonathan L. Goodall, Heather Richter</li>
<li>for: 这个研究是为了解决低洼海岸城市（如诺福克，维iginia）的街道洪水问题，这些问题会影响交通和排水系统，并可能导致财产损害。</li>
<li>methods: 这个研究使用了一种前一版的代理模型（基于随机森林算法），以及两种深度学习模型：Long Short-Term Memory（LSTM）和Gated Recurrent Unit（GRU）。</li>
<li>results: 研究发现，使用LSTM和GRU深度学习模型可以提高预测uncertainty的交通和排水系统的性能，并且这些模型可以有效地 интеGRATE多种、多模态的特征。<details>
<summary>Abstract</summary>
Low-lying coastal cities, exemplified by Norfolk, Virginia, face the challenge of street flooding caused by rainfall and tides, which strain transportation and sewer systems and can lead to property damage. While high-fidelity, physics-based simulations provide accurate predictions of urban pluvial flooding, their computational complexity renders them unsuitable for real-time applications. Using data from Norfolk rainfall events between 2016 and 2018, this study compares the performance of a previous surrogate model based on a random forest algorithm with two deep learning models: Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU). This investigation underscores the importance of using a model architecture that supports the communication of prediction uncertainty and the effective integration of relevant, multi-modal features.
</details>
<details>
<summary>摘要</summary>
低海拔沿海城市，如尼科尔（Norfolk），面临洪水泛滥的挑战，这会压力交通和废水系统，并可能导致财产损害。虽然高精度的物理学基模型可以准确预测城市洪水，但计算复杂性使其不适用于实时应用。通过使用2016-2018年尼科尔降水事件的数据，本研究比较了之前的随机森林算法基于模型和两种深度学习模型：长期快速储存（LSTM）和闭合循环单元（GRU）的表现。这一研究强调使用一种支持预测不确定性的模型架构，并有效地结合相关的多模式特征。
</details></li>
</ul>
<hr>
<h2 id="Learning-Disentangled-Discrete-Representations"><a href="#Learning-Disentangled-Discrete-Representations" class="headerlink" title="Learning Disentangled Discrete Representations"></a>Learning Disentangled Discrete Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14151">http://arxiv.org/abs/2307.14151</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/david-friede/lddr">https://github.com/david-friede/lddr</a></li>
<li>paper_authors: David Friede, Christian Reimers, Heiner Stuckenschmidt, Mathias Niepert</li>
<li>for: 本研究旨在探讨抽象空间中的离散表示的优点，以及它们如何促进分离表示的学习。</li>
<li>methods: 研究者采用了一种特制的 categorical variational autoencoder（CVAE），以取代标准的 Gaussian variational autoencoder（VAE），以便更好地学习分离表示。</li>
<li>results: 研究者通过 analytical 和 empirical 的方法，证明了离散 VAE 在学习分离表示方面的优势，并提出了首个无监督的模型选择策略，以便寻找更好的分离表示模型。<details>
<summary>Abstract</summary>
Recent successes in image generation, model-based reinforcement learning, and text-to-image generation have demonstrated the empirical advantages of discrete latent representations, although the reasons behind their benefits remain unclear. We explore the relationship between discrete latent spaces and disentangled representations by replacing the standard Gaussian variational autoencoder (VAE) with a tailored categorical variational autoencoder. We show that the underlying grid structure of categorical distributions mitigates the problem of rotational invariance associated with multivariate Gaussian distributions, acting as an efficient inductive prior for disentangled representations. We provide both analytical and empirical findings that demonstrate the advantages of discrete VAEs for learning disentangled representations. Furthermore, we introduce the first unsupervised model selection strategy that favors disentangled representations.
</details>
<details>
<summary>摘要</summary>
最近的图像生成、模型基于返回学习和文本到图像生成的成功表明了不连续含义空间的实际优势，即使其中的原因还未得到清晰解释。我们调查了不连续含义空间和分离表示之间的关系，并将标准的 Gaussian 变量自动机（VAE）替换为适应 categorical 变量自动机。我们发现， categorical 分布下的网格结构可以有效地解决多变量 Gaussian 分布中的旋转不变性问题，并作为较为有效的 inductive prior  для分离表示。我们提供了both analytical和empirical的发现，证明不连续 VAE 在学习分离表示方面的优势。此外，我们还介绍了首个无监督模型选择策略，该策略会偏好分离表示。
</details></li>
</ul>
<hr>
<h2 id="Toward-Design-of-Synthetic-Active-Inference-Agents-by-Mere-Mortals"><a href="#Toward-Design-of-Synthetic-Active-Inference-Agents-by-Mere-Mortals" class="headerlink" title="Toward Design of Synthetic Active Inference Agents by Mere Mortals"></a>Toward Design of Synthetic Active Inference Agents by Mere Mortals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14145">http://arxiv.org/abs/2307.14145</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bert de Vries</li>
<li>for: 这个论文旨在解决智能设备边缘处理中active inference代理的实现问题，以便快速普及活跃推理技术。</li>
<li>methods: 论文提出了一个工具箱，用于支持非专家工程师开发工作的活跃推理代理。该工具箱具有加速policy探索的能力，以便在限制性的边缘设备上实现效果。</li>
<li>results: 论文预示了一个在进程中使用该工具箱的示例应用，显示了在边缘设备上实现活跃推理代理的可能性。<details>
<summary>Abstract</summary>
The theoretical properties of active inference agents are impressive, but how do we realize effective agents in working hardware and software on edge devices? This is an interesting problem because the computational load for policy exploration explodes exponentially, while the computational resources are very limited for edge devices. In this paper, we discuss the necessary features for a software toolbox that supports a competent non-expert engineer to develop working active inference agents. We introduce a toolbox-in-progress that aims to accelerate the democratization of active inference agents in a similar way as TensorFlow propelled applications of deep learning technology.
</details>
<details>
<summary>摘要</summary>
理论上，活动推理代理的特性很吸引人，但实际如何在工作硬件和软件上实现有效的代理呢？这是一个有趣的问题，因为策略探索的计算荷载会指数增长，而边缘设备的计算资源却很有限。在这篇文章中，我们讨论了实现非专业工程师开发工作的活动推理代理所需的必要特性。我们介绍了一个进度中的工具箱，旨在通过减少代理开发的计算负担，使活动推理代理在边缘设备上更加普及。
</details></li>
</ul>
<hr>
<h2 id="Piecewise-Stationary-Combinatorial-Semi-Bandit-with-Causally-Related-Rewards"><a href="#Piecewise-Stationary-Combinatorial-Semi-Bandit-with-Causally-Related-Rewards" class="headerlink" title="Piecewise-Stationary Combinatorial Semi-Bandit with Causally Related Rewards"></a>Piecewise-Stationary Combinatorial Semi-Bandit with Causally Related Rewards</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14138">http://arxiv.org/abs/2307.14138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Behzad Nourani-Koliji, Steven Bilaj, Amir Rezaei Balef, Setareh Maghsudi</li>
<li>for:  solving the piecewise stationary combinatorial semi-bandit problem in a nonstationary environment with causally related rewards</li>
<li>methods: using the Upper Confidence Bound (UCB) algorithm with an adaptive approach that includes a change-point detector based on the Generalized Likelihood Ratio (GLR) test and a mechanism to trace the variations of the underlying graph structure</li>
<li>results: establishing a regret upper bound that reflects the effects of the number of structural- and distribution changes on the performance, and demonstrating superior performance in numerical experiments compared to state-of-the-art benchmarks.Here’s the Chinese translation of the three points:</li>
<li>for: 解决 piecewise stationary  combinatorial semi-bandit 问题在不站台环境中，其中 reward 具有 causal 关系</li>
<li>methods: 使用 Upper Confidence Bound (UCB) 算法，并采用适应的方法，包括基于 Generalized Likelihood Ratio (GLR) 测试的 change-point 探测器和跟踪Underlying graph structure的机制</li>
<li>results: 确定了不同 structural- 和 distribution 变化的影响，并在实验中证明了与 state-of-the-art benchmarks 相比，提出的方案具有更高的实用性。<details>
<summary>Abstract</summary>
We study the piecewise stationary combinatorial semi-bandit problem with causally related rewards. In our nonstationary environment, variations in the base arms' distributions, causal relationships between rewards, or both, change the reward generation process. In such an environment, an optimal decision-maker must follow both sources of change and adapt accordingly. The problem becomes aggravated in the combinatorial semi-bandit setting, where the decision-maker only observes the outcome of the selected bundle of arms. The core of our proposed policy is the Upper Confidence Bound (UCB) algorithm. We assume the agent relies on an adaptive approach to overcome the challenge. More specifically, it employs a change-point detector based on the Generalized Likelihood Ratio (GLR) test. Besides, we introduce the notion of group restart as a new alternative restarting strategy in the decision making process in structured environments. Finally, our algorithm integrates a mechanism to trace the variations of the underlying graph structure, which captures the causal relationships between the rewards in the bandit setting. Theoretically, we establish a regret upper bound that reflects the effects of the number of structural- and distribution changes on the performance. The outcome of our numerical experiments in real-world scenarios exhibits applicability and superior performance of our proposal compared to the state-of-the-art benchmarks.
</details>
<details>
<summary>摘要</summary>
我们研究 Piecewise 站ARY  combinatorial 半带�û问题，其中 reward 的生成过程受到基 arms 的分布变化、 causal 关系 between rewards 以及 Both 的变化影响。在这种非站ARY 环境中，一个优化的决策者需要同时考虑这些变化并适应应对。在 combinatorial 半带 Setting 中，决策者只能观察选择的 bundle of arms 的结果。我们的提议的策略是使用 Upper Confidence Bound (UCB) 算法。我们假设agent 采用adaptive approach来解决这个挑战。具体来说，它使用基于 Generalized Likelihood Ratio (GLR) 测试的变化点检测器。此外，我们引入了 group restart 作为一种新的重启策略，用于在结构化环境中决策过程中。最后，我们的算法包括跟踪Underlying graph structure的变化，这些变化捕捉了 reward 在半带 Setting 中的 causal 关系。从理论角度来看，我们确立了 regret Upper bound，这个 Upper bound 反映了变量的数量和分布变化对性能的影响。我们的numerical experiments 在实际场景中展示了我们的提议的应用和优于现有benchmarks。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Generative-Modeling-with-Limited-Data-Few-Shots-and-Zero-Shot"><a href="#A-Survey-on-Generative-Modeling-with-Limited-Data-Few-Shots-and-Zero-Shot" class="headerlink" title="A Survey on Generative Modeling with Limited Data, Few Shots, and Zero Shot"></a>A Survey on Generative Modeling with Limited Data, Few Shots, and Zero Shot</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14397">http://arxiv.org/abs/2307.14397</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sutd-visual-computing-group/awesome-generative-modeling-under-data-constraints">https://github.com/sutd-visual-computing-group/awesome-generative-modeling-under-data-constraints</a></li>
<li>paper_authors: Milad Abdollahzadeh, Touba Malekzadeh, Christopher T. H. Teo, Keshigeyan Chandrasegaran, Guimeng Liu, Ngai-Man Cheung</li>
<li>for: 本研究旨在探讨生成模型在数据约束下学习，即生成模型学习数据分布的新数据。</li>
<li>methods: 本研究使用了限制数据量、几个shot和零shot等数据约束来学习生成模型。</li>
<li>results: 研究发现了生成模型在数据约束下的学习 task 和方法之间存在交互关系，并提出了未来研究的潜在方向。Here’s a more detailed explanation of each point:</li>
<li>for: The paper is focused on generative modeling under data constraint, specifically exploring the task of learning to generate new data that is statistically similar to the training data distribution, but with limited data availability.</li>
<li>methods: The paper discusses the use of limited data, few shots, and zero shots as data constraints for generative modeling, and proposes two taxonomies for GM-DC tasks and approaches.</li>
<li>results: The study highlights research gaps, research trends, and potential avenues for future exploration in the field of GM-DC, and provides a comprehensive overview of the current state of the field.<details>
<summary>Abstract</summary>
In machine learning, generative modeling aims to learn to generate new data statistically similar to the training data distribution. In this paper, we survey learning generative models under limited data, few shots and zero shot, referred to as Generative Modeling under Data Constraint (GM-DC). This is an important topic when data acquisition is challenging, e.g. healthcare applications. We discuss background, challenges, and propose two taxonomies: one on GM-DC tasks and another on GM-DC approaches. Importantly, we study interactions between different GM-DC tasks and approaches. Furthermore, we highlight research gaps, research trends, and potential avenues for future exploration. Project website: https://gmdc-survey.github.io.
</details>
<details>
<summary>摘要</summary>
在机器学习中，生成模型目标是学习生成新数据，与训练数据分布 statistically similar。在这篇论文中，我们对受限数据的生成模型学习进行报告，包括几 shot、零 shot 等。这是数据收集困难的场景，如医疗应用。我们介绍背景、挑战和两种分类：一种是生成模型下数据约束任务（GM-DC），另一种是生成模型下数据约束方法（GM-DC）。重要的是，我们研究不同的 GM-DC 任务和方法之间的交互。此外，我们还提出了未来探索的研究漏洞、趋势和潜在的发展方向。项目网站：https://gmdc-survey.github.io。Note: The translation is done using Google Translate and may not be perfect.
</details></li>
</ul>
<hr>
<h2 id="Developing-and-Evaluating-Tiny-to-Medium-Sized-Turkish-BERT-Models"><a href="#Developing-and-Evaluating-Tiny-to-Medium-Sized-Turkish-BERT-Models" class="headerlink" title="Developing and Evaluating Tiny to Medium-Sized Turkish BERT Models"></a>Developing and Evaluating Tiny to Medium-Sized Turkish BERT Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14134">http://arxiv.org/abs/2307.14134</a></li>
<li>repo_url: None</li>
<li>paper_authors: Himmet Toprak Kesgin, Muzaffer Kaan Yuce, Mehmet Fatih Amasyali</li>
<li>for: This paper aims to bridge the research gap in less-resourced languages by introducing and evaluating tiny, mini, small, and medium-sized uncased Turkish BERT models.</li>
<li>methods: The authors trained these models on a diverse dataset encompassing over 75GB of text from multiple sources and tested them on several tasks, including mask prediction, sentiment analysis, news classification, and zero-shot classification.</li>
<li>results: Despite their smaller size, the models exhibited robust performance, including zero-shot task, while ensuring computational efficiency and faster execution times.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目标是bridging the research gap in less-resourced languages, 通过引入和评估 Turkish BERT 模型的不同大小。</li>
<li>methods: 作者使用了多种数据集，包括多个来源的文本，用于训练这些模型，并在多个任务上进行测试，包括偏好预测、情感分析、新闻分类和零 shot 分类。</li>
<li>results:  despite their smaller size, the models exhibited robust performance, including zero-shot task, while ensuring computational efficiency and faster execution times.<details>
<summary>Abstract</summary>
This study introduces and evaluates tiny, mini, small, and medium-sized uncased Turkish BERT models, aiming to bridge the research gap in less-resourced languages. We trained these models on a diverse dataset encompassing over 75GB of text from multiple sources and tested them on several tasks, including mask prediction, sentiment analysis, news classification, and, zero-shot classification. Despite their smaller size, our models exhibited robust performance, including zero-shot task, while ensuring computational efficiency and faster execution times. Our findings provide valuable insights into the development and application of smaller language models, especially in the context of the Turkish language.
</details>
<details>
<summary>摘要</summary>
Note:* "tiny" is 微小 (wēixiǎo) in Simplified Chinese* "mini" is 小型 (xiǎoxīng) in Simplified Chinese* "small" is 小 (xiǎo) in Simplified Chinese* "medium-sized" is 中等 (zhōngděng) in Simplified Chinese* "uncased" is 无框 (wúkē) in Simplified Chinese* "mask prediction" is 面 predicate (miàn zhèng) in Simplified Chinese* "sentiment analysis" is 情感分析 (qínggǎn fēnxiǎn) in Simplified Chinese* "news classification" is 新闻分类 (xīnwén fēnclass) in Simplified Chinese* "zero-shot classification" is 零枪分类 (zhèng qiāng fēnclass) in Simplified Chinese
</details></li>
</ul>
<hr>
<h2 id="GraphRNN-Revisited-An-Ablation-Study-and-Extensions-for-Directed-Acyclic-Graphs"><a href="#GraphRNN-Revisited-An-Ablation-Study-and-Extensions-for-Directed-Acyclic-Graphs" class="headerlink" title="GraphRNN Revisited: An Ablation Study and Extensions for Directed Acyclic Graphs"></a>GraphRNN Revisited: An Ablation Study and Extensions for Directed Acyclic Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14109">http://arxiv.org/abs/2307.14109</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taniya Das, Mark Koch, Maya Ravichandran, Nikhil Khatri</li>
<li>for: 学习图格生成模型</li>
<li>methods: 使用深度学习架构GraphRNN，并对基线模型进行评估和ablation study</li>
<li>results: 发现BFS traversal对模型性能有贡献，并将GraphRNN扩展到生成指定图的方法得到了显著改进。Here’s the breakdown of each point in English:</li>
<li>for: The paper is written for learning generative models for graphs using deep learning-based architectures.</li>
<li>methods: The paper uses a reproduced implementation of the GraphRNN architecture and evaluates it against baseline models using new metrics. The authors also perform an ablation study to analyze the contribution of the BFS traversal to model performance.</li>
<li>results: The authors find that the BFS traversal contributes significantly to model performance and extend GraphRNN to generate directed acyclic graphs by replacing the BFS traversal with a topological sort. They demonstrate significant improvement over a directed-multiclass variant of GraphRNN on a real-world dataset.<details>
<summary>Abstract</summary>
GraphRNN is a deep learning-based architecture proposed by You et al. for learning generative models for graphs. We replicate the results of You et al. using a reproduced implementation of the GraphRNN architecture and evaluate this against baseline models using new metrics. Through an ablation study, we find that the BFS traversal suggested by You et al. to collapse representations of isomorphic graphs contributes significantly to model performance. Additionally, we extend GraphRNN to generate directed acyclic graphs by replacing the BFS traversal with a topological sort. We demonstrate that this method improves significantly over a directed-multiclass variant of GraphRNN on a real-world dataset.
</details>
<details>
<summary>摘要</summary>
GRAPHRNN是一种深度学习基于架构，由尤等人提出用于学习图生成模型。我们对GRAPHRNN архитектура进行了重现，并使用新的基准模型进行评估。通过一项减少研究，我们发现了You等人提出的深度首次旋转（BFS） traverse 可以帮助模型性能。此外，我们还将GRAPHRNN扩展为生成直接无环图，通过将BFS traverse 替换为拓扑排序。我们示出这种方法可以在真实世界数据上提高表现。
</details></li>
</ul>
<hr>
<h2 id="Actions-Speak-What-You-Want-Provably-Sample-Efficient-Reinforcement-Learning-of-the-Quantal-Stackelberg-Equilibrium-from-Strategic-Feedbacks"><a href="#Actions-Speak-What-You-Want-Provably-Sample-Efficient-Reinforcement-Learning-of-the-Quantal-Stackelberg-Equilibrium-from-Strategic-Feedbacks" class="headerlink" title="Actions Speak What You Want: Provably Sample-Efficient Reinforcement Learning of the Quantal Stackelberg Equilibrium from Strategic Feedbacks"></a>Actions Speak What You Want: Provably Sample-Efficient Reinforcement Learning of the Quantal Stackelberg Equilibrium from Strategic Feedbacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14085">http://arxiv.org/abs/2307.14085</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyu Chen, Mengdi Wang, Zhuoran Yang</li>
<li>for: 学习一种量化Stackelberg平衡（QSE）在一个 episodic Markov 游戏中，其中有一个领导者和一个追随者结构。</li>
<li>methods: 使用 reinforcement learning（RL）和 maximum likelihood estimation（MLE）来学习领导者的决策问题，并使用模型之间的不确定性来实现在线和离线设置中的最佳性。</li>
<li>results: 提出了一些样本效率的算法，可以在函数approximation的上下文中解决领导者的决策问题，并且可以实现sublinear regret上限。 besides, 特别注重linear和偏函数设置下的计算效率。<details>
<summary>Abstract</summary>
We study reinforcement learning (RL) for learning a Quantal Stackelberg Equilibrium (QSE) in an episodic Markov game with a leader-follower structure. In specific, at the outset of the game, the leader announces her policy to the follower and commits to it. The follower observes the leader's policy and, in turn, adopts a quantal response policy by solving an entropy-regularized policy optimization problem induced by leader's policy. The goal of the leader is to find her optimal policy, which yields the optimal expected total return, by interacting with the follower and learning from data. A key challenge of this problem is that the leader cannot observe the follower's reward, and needs to infer the follower's quantal response model from his actions against leader's policies. We propose sample-efficient algorithms for both the online and offline settings, in the context of function approximation. Our algorithms are based on (i) learning the quantal response model via maximum likelihood estimation and (ii) model-free or model-based RL for solving the leader's decision making problem, and we show that they achieve sublinear regret upper bounds. Moreover, we quantify the uncertainty of these estimators and leverage the uncertainty to implement optimistic and pessimistic algorithms for online and offline settings. Besides, when specialized to the linear and myopic setting, our algorithms are also computationally efficient. Our theoretical analysis features a novel performance-difference lemma which incorporates the error of quantal response model, which might be of independent interest.
</details>
<details>
<summary>摘要</summary>
我们研究强化学习（RL）来学习一个量化StackelbergEquilibrium（QSE）在一个集合Markov游戏中，具有领导者-跟者结构。具体来说，在游戏开始时，领导者宣布她的策略给跟者，并将其约束。跟者根据领导者的策略采取一个量化回应策略， solving一个带有Entropy规范的策略优化问题。领导者的目标是找到她的优化策略，以便在与跟者交互和学习数据的过程中获得最佳预期总返回。一个关键问题是领导者无法观察跟者的奖励，她需要从跟者的行为中推断出跟者的量化回应模型。我们提出了一些样本效率的算法，包括在线和离线设置下的最大 likelihood估计和模型自由或模型基于RL，并证明它们可以实现sublinear regret上界。此外，我们还评估了这些估计器的不确定性，并利用这些不确定性来实现在线和离线设置下的optimistic和pessimistic算法。此外，当特化到线性和偏惰设置时，我们的算法也是计算高效的。我们的理论分析包括一个新的表现差lemm，该lemm incorporates the error of quantal response model，这可能是独立的兴趣。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-simulate-partially-known-spatio-temporal-dynamics-with-trainable-difference-operators"><a href="#Learning-to-simulate-partially-known-spatio-temporal-dynamics-with-trainable-difference-operators" class="headerlink" title="Learning to simulate partially known spatio-temporal dynamics with trainable difference operators"></a>Learning to simulate partially known spatio-temporal dynamics with trainable difference operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14395">http://arxiv.org/abs/2307.14395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Huang, Zhuoyuan Li, Hongsheng Liu, Zidong Wang, Hongye Zhou, Bin Dong, Bei Hua</li>
<li>for: 用神经网络模拟空间-时间动态的研究在最近几年得到了很多关注，但大多数现有方法采用纯数据驱动的黑盒模型，具有限制精度和可读性。</li>
<li>methods: 我们提出一种新的混合建模架构，称为PDE-Net++，它将可训练的差分算子与黑盒模型相结合，并包含部分先验知识。我们还提出了两种不同的差分层：可训练差分层（TFDL）和可训练动态差分层（TDDL）。</li>
<li>results: 数值实验表明，PDE-Net++的预测精度高于黑盒模型，并且在推理范围内具有更好的推理性能。<details>
<summary>Abstract</summary>
Recently, using neural networks to simulate spatio-temporal dynamics has received a lot of attention. However, most existing methods adopt pure data-driven black-box models, which have limited accuracy and interpretability. By combining trainable difference operators with black-box models, we propose a new hybrid architecture explicitly embedded with partial prior knowledge of the underlying PDEs named PDE-Net++. Furthermore, we introduce two distinct options called the trainable flipping difference layer (TFDL) and the trainable dynamic difference layer (TDDL) for the difference operators. Numerous numerical experiments have demonstrated that PDE-Net++ has superior prediction accuracy and better extrapolation performance than black-box models.
</details>
<details>
<summary>摘要</summary>
最近，使用神经网络模拟空间时间动力学受到了广泛关注。然而，现有大多数方法采用纯数据驱动黑盒模型，它们的准确率和可解释性受限。我们提出了一种新的混合架构，名为PDE-Net++，它通过与黑盒模型结合可编程的差异运算器来承载部分先验知识。此外，我们还提出了两种不同的选项，即可编程折衣层（TFDL）和可编程动态差异层（TDDL），用于差异运算器。数值实验证明，PDE-Net++在预测精度和推迟性方面都高于黑盒模型。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Domain-Discrepancy-Adjustment-for-Active-Multi-Domain-Adaptation"><a href="#Dynamic-Domain-Discrepancy-Adjustment-for-Active-Multi-Domain-Adaptation" class="headerlink" title="Dynamic Domain Discrepancy Adjustment for Active Multi-Domain Adaptation"></a>Dynamic Domain Discrepancy Adjustment for Active Multi-Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14068">http://arxiv.org/abs/2307.14068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Long Liu, Bo Zhou, Zhipeng Zhao, Zening Liu</li>
<li>for: 本研究旨在提出一种新的多源不监督领域适应（MUDA）方法，以便从相关的源领域传递知识到未标注的目标领域。</li>
<li>methods: 我们提出了一种名为动态领域差异调整 для活动多频道适应（D3AAMDA）的新方法，它在训练过程中根据源领域和目标领域之间的分布差异度设置多源动态调整机制，以有效地利用每个源领域的本地有利特征信息。此外，我们还提出了一种多源活动边界选择策略（MABS），它通过一种引导的动态边界损失来设计高效的选择函数，以提高对目标领域的泛化。</li>
<li>results: 我们对常用的领域适应数据集进行了广泛的测试和比较，并证明了我们的方法的突出优势。<details>
<summary>Abstract</summary>
Multi-source unsupervised domain adaptation (MUDA) aims to transfer knowledge from related source domains to an unlabeled target domain. While recent MUDA methods have shown promising results, most focus on aligning the overall feature distributions across source domains, which can lead to negative effects due to redundant features within each domain. Moreover, there is a significant performance gap between MUDA and supervised methods. To address these challenges, we propose a novel approach called Dynamic Domain Discrepancy Adjustment for Active Multi-Domain Adaptation (D3AAMDA). Firstly, we establish a multi-source dynamic modulation mechanism during the training process based on the degree of distribution differences between source and target domains. This mechanism controls the alignment level of features between each source domain and the target domain, effectively leveraging the local advantageous feature information within the source domains. Additionally, we propose a Multi-source Active Boundary Sample Selection (MABS) strategy, which utilizes a guided dynamic boundary loss to design an efficient query function for selecting important samples. This strategy achieves improved generalization to the target domain with minimal sampling costs. We extensively evaluate our proposed method on commonly used domain adaptation datasets, comparing it against existing UDA and ADA methods. The experimental results unequivocally demonstrate the superiority of our approach.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:多源无监督领域适应 (MUDA) 目标是将相关源领域知识传递到未标注目标领域。而现有的 MUDA 方法多数强调对所有源领域的特征分布进行对齐，这可能会导致因每个领域中的重复特征而出现负面的影响。此外，与超级vised方法相比，MUDA方法存在显著的性能差距。为解决这些挑战，我们提出了一种新的方法，即动态领域差异调整器 для活动多领域适应 (D3AAMDA)。首先，我们在训练过程中建立了多源动态调整机制，根据源领域和目标领域特征分布之间的差异程度来控制每个源领域和目标领域之间的特征对齐水平。这种机制有效地利用了每个源领域的本地有利特征信息。其次，我们提出了多源活动边界选择策略 (MABS)，该策略通过指导动态边界损失来设计高效的查询函数，选择重要的样本。这种策略可以在保持高度一致性的情况下，实现到目标领域的改进一致性，并且减少采样成本。我们对常用的领域适应 datasets 进行了广泛的测试和比较，与现有的 UDA 和 ADA 方法进行了比较。实验结果明确地表明了我们的方法的优越性。
</details></li>
</ul>
<hr>
<h2 id="Hypergraph-Isomorphism-Computation"><a href="#Hypergraph-Isomorphism-Computation" class="headerlink" title="Hypergraph Isomorphism Computation"></a>Hypergraph Isomorphism Computation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14394">http://arxiv.org/abs/2307.14394</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Feng, Jiashu Han, Shihui Ying, Yue Gao</li>
<li>for: 本研究旨在 Addressing the 图HS isomorphism problem, which is a fundamental problem in network analysis, and capturing both low-order and high-order structural information.</li>
<li>methods: 本 paper 提出了一种基于 Weisfiler-Lehman 测试算法的hypergraph isomorphism测试问题解决方案，并基于该算法提出了一个总体的 hypergraph Weisfeiler-Lehman kernel框架。</li>
<li>results: Results 表明，与其他常见的kernel-based方法相比，提出的方法在 hypergraph 分类任务中具有显著的优势，runtime running over 80 times faster when handling complex hypergraph structures。<details>
<summary>Abstract</summary>
The isomorphism problem is a fundamental problem in network analysis, which involves capturing both low-order and high-order structural information. In terms of extracting low-order structural information, graph isomorphism algorithms analyze the structural equivalence to reduce the solver space dimension, which demonstrates its power in many applications, such as protein design, chemical pathways, and community detection. For the more commonly occurring high-order relationships in real-life scenarios, the problem of hypergraph isomorphism, which effectively captures these high-order structural relationships, cannot be straightforwardly addressed using graph isomorphism methods. Besides, the existing hypergraph kernel methods may suffer from high memory consumption or inaccurate sub-structure identification, thus yielding sub-optimal performance. In this paper, to address the abovementioned problems, we first propose the hypergraph Weisfiler-Lehman test algorithm for the hypergraph isomorphism test problem by generalizing the Weisfiler-Lehman test algorithm from graphs to hypergraphs. Secondly, based on the presented algorithm, we propose a general hypergraph Weisfieler-Lehman kernel framework and implement two instances, which are Hypergraph Weisfeiler-Lehamn Subtree Kernel and Hypergraph Weisfeiler-Lehamn Hyperedge Kernel. In order to fulfill our research objectives, a comprehensive set of experiments was meticulously designed, including seven graph classification datasets and 12 hypergraph classification datasets. Results on hypergraph classification datasets show significant improvements compared to other typical kernel-based methods, which demonstrates the effectiveness of the proposed methods. In our evaluation, we found that our proposed methods outperform the second-best method in terms of runtime, running over 80 times faster when handling complex hypergraph structures.
</details>
<details>
<summary>摘要</summary>
“iso关系问题”是网络分析中的基本问题，它涵盖了低阶和高阶结构信息的捕捉。从抽象低阶结构信息的角度来看，网络同构算方法可以降低解析空间维度，实现了许多应用，如蛋白质设计、化学路径和社区探测。然而，实际生活中更常出现高阶关系，这些高阶关系不能直接使用网络同构算方法进行处理。此外，现有的超graphkernel方法可能会导致高内存消耗或不精确的子结构识别，因此表现不佳。在本文中，我们提出了超graphWeisfiler-Lehman测试算法，用于超graph isomorphism测试问题的解决方案。其次，我们基于这个算法提出了一个通用的超graphWeisfeiler-Lehman核心框架，并实现了两个实例：超graphWeisfeiler-Lehman子树核心和超graphWeisfeiler-Lehman超组件核心。为了实现我们的研究目标，我们谨慎设计了一系列实验，包括七个图类别数据集和十二个超graph类别数据集。结果显示，我们的提案方法在超graph类别数据集上表现出色，较其他常用的核心基本方法更好。在我们的评估中，我们发现了我们的提案方法在复杂超graph结构时表现出80倍以上的执行时间优化。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Applications-In-Healthcare-The-State-Of-Knowledge-and-Future-Directions"><a href="#Machine-Learning-Applications-In-Healthcare-The-State-Of-Knowledge-and-Future-Directions" class="headerlink" title="Machine Learning Applications In Healthcare: The State Of Knowledge and Future Directions"></a>Machine Learning Applications In Healthcare: The State Of Knowledge and Future Directions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14067">http://arxiv.org/abs/2307.14067</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mrinmoy Roy, Sarwar J. Minar, Porarthi Dhar, A T M Omor Faruq</li>
<li>For: This study aims to gather and present Machine Learning (ML) applications in various areas of healthcare, such as community level work, risk management&#x2F;preventive care, healthcare operation management, remote care, and early detection, to provide quick access to necessary information and reduce the knowledge gap of clinicians about ML applications.* Methods: The study uses a tabular format to provide relevant references with descriptions for each ML application, allowing healthcare professionals to access the information more effectively.* Results: The study aims to motivate healthcare professionals towards adopting more ML-based healthcare systems and to inform people about the applicability of ML in the healthcare industry.<details>
<summary>Abstract</summary>
Detection of easily missed hidden patterns with fast processing power makes machine learning (ML) indispensable to today's healthcare system. Though many ML applications have already been discovered and many are still under investigation, only a few have been adopted by current healthcare systems. As a result, there exists an enormous opportunity in healthcare system for ML but distributed information, scarcity of properly arranged and easily explainable documentation in related sector are major impede which are making ML applications difficult to healthcare professionals. This study aimed to gather ML applications in different areas of healthcare concisely and more effectively so that necessary information can be accessed immediately with relevant references. We divided our study into five major groups: community level work, risk management/ preventive care, healthcare operation management, remote care, and early detection. Dividing these groups into subgroups, we provided relevant references with description in tabular form for quick access. Our objective is to inform people about ML applicability in healthcare industry, reduce the knowledge gap of clinicians about the ML applications and motivate healthcare professionals towards more machine learning based healthcare system.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Detection of easily missed hidden patterns with fast processing power makes machine learning (ML) indispensable to today's healthcare system. Though many ML applications have already been discovered and many are still under investigation, only a few have been adopted by current healthcare systems. As a result, there exists an enormous opportunity in healthcare system for ML but distributed information, scarcity of properly arranged and easily explainable documentation in related sector are major impede which are making ML applications difficult to healthcare professionals. This study aimed to gather ML applications in different areas of healthcare concisely and more effectively so that necessary information can be accessed immediately with relevant references. We divided our study into five major groups: community level work, risk management/ preventive care, healthcare operation management, remote care, and early detection. Dividing these groups into subgroups, we provided relevant references with description in tabular form for quick access. Our objective is to inform people about ML applicability in healthcare industry, reduce the knowledge gap of clinicians about the ML applications and motivate healthcare professionals towards more machine learning based healthcare system." into Simplified Chinese.干� TRANSLATE "检测容易过look的隐藏模式，快速处理能力使机器学习（ML）成为今天的医疗系统中不可或缺的一部分。虽然已经发现了许多ML应用，但只有一些被当前的医疗系统采用。因此，医疗系统中存在巨大的机会，但信息散布、相关领域的文献不足和不易描述的问题是主要的阻碍因素。这项研究的目标是将不同领域的ML应用集中并更有效地呈现，以便立即获取相关参考。我们将研究分为五个主要组：社区层次的工作、风险管理/预防护理、医疗运营管理、远程护理和早期检测。将这些组分成子组，并提供相关参考和描述的表格形式，以便快速访问。我们的目标是让人们了解医疗业中ML的可能性，减少医生对ML应用的知识差距，并激励医疗专业人员更加倾向于基于机器学习的医疗系统。
</details></li>
</ul>
<hr>
<h2 id="Pre-Training-with-Diffusion-models-for-Dental-Radiography-segmentation"><a href="#Pre-Training-with-Diffusion-models-for-Dental-Radiography-segmentation" class="headerlink" title="Pre-Training with Diffusion models for Dental Radiography segmentation"></a>Pre-Training with Diffusion models for Dental Radiography segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14066">http://arxiv.org/abs/2307.14066</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jérémy Rousseau, Christian Alaka, Emma Covili, Hippolyte Mayard, Laura Misrachi, Willy Au</li>
<li>for: 针对医疗放射学像 segmentation, 特别是牙科放射学像，存在高度有限制的标注成本，需要专业知识和劳动 INTENSIVE 注解。</li>
<li>methods: 我们提议使用 Denoising Diffusion Probabilistic Models (DDPM) 的快速预训练方法，这种方法已经在生成模型方面显示出卓越的表现。</li>
<li>results: 我们的实验结果表明，使用我们提议的方法可以实现 Label 效率的提高，不需要下游任务中的建筑修改。 我们的方法与当前状态的预训练方法相当。<details>
<summary>Abstract</summary>
Medical radiography segmentation, and specifically dental radiography, is highly limited by the cost of labeling which requires specific expertise and labor-intensive annotations. In this work, we propose a straightforward pre-training method for semantic segmentation leveraging Denoising Diffusion Probabilistic Models (DDPM), which have shown impressive results for generative modeling. Our straightforward approach achieves remarkable performance in terms of label efficiency and does not require architectural modifications between pre-training and downstream tasks. We propose to first pre-train a Unet by exploiting the DDPM training objective, and then fine-tune the resulting model on a segmentation task. Our experimental results on the segmentation of dental radiographs demonstrate that the proposed method is competitive with state-of-the-art pre-training methods.
</details>
<details>
<summary>摘要</summary>
医疗X射线段化和特别是牙科X射线段化受到标注成本的限制，这导致了标注过程的专业知识和劳动 INTENSIVE 的需求。在这项工作中，我们提出了一种简单的预训练方法 дляsemantic segmentation，利用Denosing Diffusion Probabilistic Models（DDPM），这种模型在生成模型方面表现出色。我们的简单的方法可以在预训练和下游任务之间不需要任何建筑修改，同时可以 дости到很好的标签效率。我们首先预训练了Unet模型，然后在DDPM训练目标下进行了微调。我们的实验结果表明，提议的方法与状态的预训练方法竞争。
</details></li>
</ul>
<hr>
<h2 id="Topologically-Regularized-Multiple-Instance-Learning-for-Red-Blood-Cell-Disease-Classification"><a href="#Topologically-Regularized-Multiple-Instance-Learning-for-Red-Blood-Cell-Disease-Classification" class="headerlink" title="Topologically-Regularized Multiple Instance Learning for Red Blood Cell Disease Classification"></a>Topologically-Regularized Multiple Instance Learning for Red Blood Cell Disease Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14025">http://arxiv.org/abs/2307.14025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Salome Kazeminia, Ario Sadafi, Asya Makhro, Anna Bogdanova, Carsten Marr, Bastian Rieck</li>
<li>for: 这个论文的目的是用单个红血球图像自动诊断罕见贫血病。</li>
<li>methods: 这个论文使用的方法是基于多个精度扩展的扩展特征，以保持数据集的特性特征。</li>
<li>results: 该方法在71名患有罕见贫血病的患者和521张红血球图像上达到了超过3%的性能提升。<details>
<summary>Abstract</summary>
Diagnosing rare anemia disorders using microscopic images is challenging for skilled specialists and machine-learning methods alike. Due to thousands of disease-relevant cells in a single blood sample, this constitutes a complex multiple-instance learning (MIL) problem. While the spatial neighborhood of red blood cells is not meaningful per se, the topology, i.e., the geometry of blood samples as a whole, contains informative features to remedy typical MIL issues, such as vanishing gradients and overfitting when training on limited data. We thus develop a topology-based approach that extracts multi-scale topological features from bags of single red blood cell images. The topological features are used to regularize the model, enforcing the preservation of characteristic topological properties of the data. Applied to a dataset of 71 patients suffering from rare anemia disorders with 521 microscopic images of red blood cells, our experiments show that topological regularization is an effective method that leads to more than 3% performance improvements for the automated classification of rare anemia disorders based on single-cell images. This is the first approach that uses topological properties for regularizing the MIL process.
</details>
<details>
<summary>摘要</summary>
诊断罕见血红素疾病使用微型图像是困难的，both skilled specialists and machine learning methods alike. Due to thousands of disease-relevant cells in a single blood sample, this constitutes a complex multiple-instance learning (MIL) problem. While the spatial neighborhood of red blood cells is not meaningful per se, the topology, i.e., the geometry of blood samples as a whole, contains informative features to remedy typical MIL issues, such as vanishing gradients and overfitting when training on limited data. We thus develop a topology-based approach that extracts multi-scale topological features from bags of single red blood cell images. The topological features are used to regularize the model, enforcing the preservation of characteristic topological properties of the data. Applied to a dataset of 71 patients suffering from rare anemia disorders with 521 microscopic images of red blood cells, our experiments show that topological regularization is an effective method that leads to more than 3% performance improvements for the automated classification of rare anemia disorders based on single-cell images. This is the first approach that uses topological properties for regularizing the MIL process.Here's the translation in Traditional Chinese as well:诊断罕见血红素疾病使用微型图像是困难的，both skilled specialists and machine learning methods alike. Due to thousands of disease-relevant cells in a single blood sample, this constitutes a complex multiple-instance learning (MIL) problem. While the spatial neighborhood of red blood cells is not meaningful per se, the topology, i.e., the geometry of blood samples as a whole, contains informative features to remedy typical MIL issues, such as vanishing gradients and overfitting when training on limited data. We thus develop a topology-based approach that extracts multi-scale topological features from bags of single red blood cell images. The topological features are used to regularize the model, enforcing the preservation of characteristic topological properties of the data. Applied to a dataset of 71 patients suffering from rare anemia disorders with 521 microscopic images of red blood cells, our experiments show that topological regularization is an effective method that leads to more than 3% performance improvements for the automated classification of rare anemia disorders based on single-cell images. This is the first approach that uses topological properties for regularizing the MIL process.
</details></li>
</ul>
<hr>
<h2 id="Are-Transformers-with-One-Layer-Self-Attention-Using-Low-Rank-Weight-Matrices-Universal-Approximators"><a href="#Are-Transformers-with-One-Layer-Self-Attention-Using-Low-Rank-Weight-Matrices-Universal-Approximators" class="headerlink" title="Are Transformers with One Layer Self-Attention Using Low-Rank Weight Matrices Universal Approximators?"></a>Are Transformers with One Layer Self-Attention Using Low-Rank Weight Matrices Universal Approximators?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14023">http://arxiv.org/abs/2307.14023</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tokio Kajitsuka, Issei Sato</li>
<li>for: 本研究探讨了Transformer模型的表达能力，并解决了现有分析中的深层层次问题。</li>
<li>methods: 本研究使用了 clarify softmax函数和Boltzmann算子之间的连接，以证明单层自注意层可以完全捕捉输入序列的上下文。</li>
<li>results: 研究显示，单层Transformer模型具有内存化能力，而且由一个自注意层和两个预测神经网络组成的Transformer模型是一个universal approximator。<details>
<summary>Abstract</summary>
Existing analyses of the expressive capacity of Transformer models have required excessively deep layers for data memorization, leading to a discrepancy with the Transformers actually used in practice. This is primarily due to the interpretation of the softmax function as an approximation of the hardmax function. By clarifying the connection between the softmax function and the Boltzmann operator, we prove that a single layer of self-attention with low-rank weight matrices possesses the capability to perfectly capture the context of an entire input sequence. As a consequence, we show that single-layer Transformer has a memorization capacity for finite samples, and that Transformers consisting of one self-attention layer with two feed-forward neural networks are universal approximators for continuous functions on a compact domain.
</details>
<details>
<summary>摘要</summary>
existing 分析吧Transformer模型的表达能力已经需要过度深度层次，导致与实际使用中的Transformer模型存在差异。这主要是由于软MAX函数的解释为硬MAX函数的近似。通过证明软MAX函数和博尔tz曼算子之间的连接，我们证明了一层自注意力层次可以完美地捕捉整个输入序列的上下文。因此，我们显示了单层Transformer具有内存化能力，并且Transformer由一层自注意力层次和两个预处理神经网络组成的模型是continue 函数在封闭区域上的universal approximator。
</details></li>
</ul>
<hr>
<h2 id="MCMC-Correction-of-Score-Based-Diffusion-Models-for-Model-Composition"><a href="#MCMC-Correction-of-Score-Based-Diffusion-Models-for-Model-Composition" class="headerlink" title="MCMC-Correction of Score-Based Diffusion Models for Model Composition"></a>MCMC-Correction of Score-Based Diffusion Models for Model Composition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14012">http://arxiv.org/abs/2307.14012</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jackonelli/mcmc_corr_score_diffusion">https://github.com/jackonelli/mcmc_corr_score_diffusion</a></li>
<li>paper_authors: Anders Sjöberg, Jakob Lindqvist, Magnus Önnheim, Mats Jirstrand, Lennart Svensson</li>
<li>for: 这个论文的目的是提出一种新的推 diffusion 方法，以便在不同的分布中进行采样。</li>
<li>methods: 这个论文使用了分Diffusion models可以被参数化为得分或能量函数。这个研究使用了得分函数参数化，并通过 Metropolis–Hastings  correction step来进行扩展采样。</li>
<li>results: 研究发现，使用得分函数参数化的方法可以 Achieve similar or better performance than使用能量函数参数化的方法。此外，这种方法还可以 reuse existing diffusion models and combine with various Markov-Chain Monte Carlo (MCMC) methods。<details>
<summary>Abstract</summary>
Diffusion models can be parameterised in terms of either a score or an energy function. The energy parameterisation has better theoretical properties, mainly that it enables an extended sampling procedure with a Metropolis--Hastings correction step, based on the change in total energy in the proposed samples. However, it seems to yield slightly worse performance, and more importantly, due to the widespread popularity of score-based diffusion, there are limited availability of off-the-shelf pre-trained energy-based ones. This limitation undermines the purpose of model composition, which aims to combine pre-trained models to sample from new distributions. Our proposal, however, suggests retaining the score parameterization and instead computing the energy-based acceptance probability through line integration of the score function. This allows us to re-use existing diffusion models and still combine the reverse process with various Markov-Chain Monte Carlo (MCMC) methods. We evaluate our method on a 2D experiment and find that it achieve similar or arguably better performance than the energy parameterisation.
</details>
<details>
<summary>摘要</summary>
Diffusion models 可以被参数化为得分或能量函数。能量参数化有更好的理论性质，主要是允许扩展采样过程，并基于变化总能量进行 Metropolis-Hastings 修正步骤。然而，它似乎在性能上略微 inferior，而且由于得分基 diffusion 的普遍投身，有限的可用性。这限制了模型组合的目的，即将预训练模型组合以采样新的分布。我们的提议是保留得分参数化，并通过线tegration 计算能量基于接受概率。这允许我们 reuse 现有的 diffusion 模型，并且与多种 Markov-Chain Monte Carlo (MCMC) 方法结合。我们在 2D 实验中评估了我们的方法，并发现它们可以达到相似或更好的性能。
</details></li>
</ul>
<hr>
<h2 id="Diff-E-Diffusion-based-Learning-for-Decoding-Imagined-Speech-EEG"><a href="#Diff-E-Diffusion-based-Learning-for-Decoding-Imagined-Speech-EEG" class="headerlink" title="Diff-E: Diffusion-based Learning for Decoding Imagined Speech EEG"></a>Diff-E: Diffusion-based Learning for Decoding Imagined Speech EEG</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14389">http://arxiv.org/abs/2307.14389</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yorgoon/diffe">https://github.com/yorgoon/diffe</a></li>
<li>paper_authors: Soowon Kim, Young-Eun Lee, Seo-Hyun Lee, Seong-Whan Lee</li>
<li>for: 这篇研究旨在实现透过想像的语音来进行沟通，并使用数据减预测模型（DDPM）和条件自动encoder（Diff-E）来实现这一目标。</li>
<li>methods: 本研究使用DDPM和Diff-E实现透过想像的语音的EEG信号解oding，并与传统机器学习技术和基准模型进行比较。</li>
<li>results: 结果显示，Diff-E可以对EEG信号进行高精度的解oding，并提高透过想像的语音的沟通率，具有实际应用价值，可能对于开发基于想像语音的脑computer界面（BCI）有所帮助。<details>
<summary>Abstract</summary>
Decoding EEG signals for imagined speech is a challenging task due to the high-dimensional nature of the data and low signal-to-noise ratio. In recent years, denoising diffusion probabilistic models (DDPMs) have emerged as promising approaches for representation learning in various domains. Our study proposes a novel method for decoding EEG signals for imagined speech using DDPMs and a conditional autoencoder named Diff-E. Results indicate that Diff-E significantly improves the accuracy of decoding EEG signals for imagined speech compared to traditional machine learning techniques and baseline models. Our findings suggest that DDPMs can be an effective tool for EEG signal decoding, with potential implications for the development of brain-computer interfaces that enable communication through imagined speech.
</details>
<details>
<summary>摘要</summary>
<<SYS>> traduced the text into Simplified Chinese.<</SYS>>解oding EEG信号为想象的语言是一项复杂的任务，因为数据的高维度和信号噪声比率低。在最近几年，杂 diffusion probabilistic models（DDPMs）已经出现为不同领域的表征学习提出了可能的方法。我们的研究提出了一种使用 DDPMs 和名为 Diff-E 的条件 autoencoder 来解码 EEG 信号的新方法。结果表明，Diff-E 可以在解码 EEG 信号的想象语言方面提高精度，比传统机器学习技术和基线模型更高。我们的发现表明，DDPMs 可以是 EEG 信号解码的有效工具，具有可能为 brain-computer interface 的发展带来沟通通过想象语言的潜在意义。
</details></li>
</ul>
<hr>
<h2 id="Fast-algorithms-for-k-submodular-maximization-subject-to-a-matroid-constraint"><a href="#Fast-algorithms-for-k-submodular-maximization-subject-to-a-matroid-constraint" class="headerlink" title="Fast algorithms for k-submodular maximization subject to a matroid constraint"></a>Fast algorithms for k-submodular maximization subject to a matroid constraint</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13996">http://arxiv.org/abs/2307.13996</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuxian Niu, Qian Liu, Yang Zhou, Min Li</li>
<li>for: 该文章目的是 Maximize $k$-submodular functions under a matroid constraint.</li>
<li>methods: 文章使用 Threshold-Decreasing Algorithm, 比较效率高于 greedy algorithm, yet with little loss in approximation ratio.</li>
<li>results: 文章提供了 $(1&#x2F;2 - \epsilon)$ 和 $(1&#x2F;3 - \epsilon)$ 两种精度的算法，用于 monotone 和 non-monotone $k$-submodular function maximization, 其复杂度为 $O(\frac{n(k\cdot EO + IO)}{\epsilon} \log \frac{r}{\epsilon})$.<details>
<summary>Abstract</summary>
In this paper, we apply a Threshold-Decreasing Algorithm to maximize $k$-submodular functions under a matroid constraint, which reduces the query complexity of the algorithm compared to the greedy algorithm with little loss in approximation ratio. We give a $(\frac{1}{2} - \epsilon)$-approximation algorithm for monotone $k$-submodular function maximization, and a $(\frac{1}{3} - \epsilon)$-approximation algorithm for non-monotone case, with complexity $O(\frac{n(k\cdot EO + IO)}{\epsilon} \log \frac{r}{\epsilon})$, where $r$ denotes the rank of the matroid, and $IO, EO$ denote the number of oracles to evaluate whether a subset is an independent set and to compute the function value of $f$, respectively. Since the constraint of total size can be looked as a special matroid, called uniform matroid, then we present the fast algorithm for maximizing $k$-submodular functions subject to a total size constraint as corollaries. corollaries.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们采用一个阈值递减算法来最大化$k$- су布模函数 beneath 一个 matroid 约束，这会降低我们的查询复杂度相比于使用排序算法，减少了对于approximation ratio的损失。我们提供了一个$(1/2-\epsilon)$-近似算法 для升高 monotone $k$- submodular function，以及一个$(1/3-\epsilon)$-近似算法 for non-monotone case，其复杂度为$O(\frac{n(k\cdot EO+IO)}{\epsilon} \log \frac{r}{\epsilon})$，其中$r$表示 matroid 的排名，$IO, EO$表示计算 whether a subset is an independent set 和计算 $f$ 函数值的oracle数量。由于总大小的约束可以看作特殊的 matroid，called uniform matroid，所以我们将在 corollaries 中提供fast algorithm for maximizing $k$- submodular functions subject to a total size constraint.
</details></li>
</ul>
<hr>
<h2 id="Take-Your-Pick-Enabling-Effective-Personalized-Federated-Learning-within-Low-dimensional-Feature-Space"><a href="#Take-Your-Pick-Enabling-Effective-Personalized-Federated-Learning-within-Low-dimensional-Feature-Space" class="headerlink" title="Take Your Pick: Enabling Effective Personalized Federated Learning within Low-dimensional Feature Space"></a>Take Your Pick: Enabling Effective Personalized Federated Learning within Low-dimensional Feature Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13995">http://arxiv.org/abs/2307.13995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guogang Zhu, Xuefeng Liu, Shaojie Tang, Jianwei Niu, Xinghao Wu, Jiaxing Shen</li>
<li>for: 该论文旨在提出一种基于个性化联合学习（PFL）的新框架，以便每个客户端可以根据自己的本地任务进行个性化模型训练。</li>
<li>methods: 该论文使用了一种基于特征选择的PFL方法，通过对每个客户端的本地数据进行特征选择，以便在低维度特征空间中进行联合学习。</li>
<li>results: 实验结果表明，该方法可以有效地选择每个客户端的任务相关特征，提高了跨Domain FL模型的性能。<details>
<summary>Abstract</summary>
Personalized federated learning (PFL) is a popular framework that allows clients to have different models to address application scenarios where clients' data are in different domains. The typical model of a client in PFL features a global encoder trained by all clients to extract universal features from the raw data and personalized layers (e.g., a classifier) trained using the client's local data. Nonetheless, due to the differences between the data distributions of different clients (aka, domain gaps), the universal features produced by the global encoder largely encompass numerous components irrelevant to a certain client's local task. Some recent PFL methods address the above problem by personalizing specific parameters within the encoder. However, these methods encounter substantial challenges attributed to the high dimensionality and non-linearity of neural network parameter space. In contrast, the feature space exhibits a lower dimensionality, providing greater intuitiveness and interpretability as compared to the parameter space. To this end, we propose a novel PFL framework named FedPick. FedPick achieves PFL in the low-dimensional feature space by selecting task-relevant features adaptively for each client from the features generated by the global encoder based on its local data distribution. It presents a more accessible and interpretable implementation of PFL compared to those methods working in the parameter space. Extensive experimental results show that FedPick could effectively select task-relevant features for each client and improve model performance in cross-domain FL.
</details>
<details>
<summary>摘要</summary>
个性化联合学习（PFL）是一种广泛使用的框架，允许客户端有不同的模型来处理应用场景中客户端数据的不同领域。典型的客户端模型在PFL中包括全局编码器，该编码器由所有客户端训练以提取Raw数据中的通用特征，以及客户端本地数据中的个性层（例如分类器）。然而，由于客户端数据的不同分布（即领域差距），全局编码器生成的通用特征具有许多无关于特定客户端本地任务的组成部分。一些最近的PFL方法解决了上述问题，通过在编码器中个性化特定参数。然而，这些方法遇到了高维度和非线性的神经网络参数空间的重大挑战。相比之下，特征空间的维度较低，提供了更好的直观性和可解释性，与参数空间相比。为此，我们提出了一种新的PFL框架名为FedPick。FedPick在低维特征空间中实现PFL，通过为每个客户端适应性地选择本地数据分布中任务相关的特征来选择任务相关的特征。它提供了与参数空间中方法相比更加直观和可解释的PFL实现。经验证明，FedPick可以有效地选择每个客户端的任务相关特征，并在跨领域FL中提高模型性能。
</details></li>
</ul>
<hr>
<h2 id="BovineTalk-Machine-Learning-for-Vocalization-Analysis-of-Dairy-Cattle-under-Negative-Affective-States"><a href="#BovineTalk-Machine-Learning-for-Vocalization-Analysis-of-Dairy-Cattle-under-Negative-Affective-States" class="headerlink" title="BovineTalk: Machine Learning for Vocalization Analysis of Dairy Cattle under Negative Affective States"></a>BovineTalk: Machine Learning for Vocalization Analysis of Dairy Cattle under Negative Affective States</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13994">http://arxiv.org/abs/2307.13994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dinu Gavojdian, Teddy Lazebnik, Madalina Mincu, Ariel Oren, Ioana Nicolae, Anna Zamansky</li>
<li>for: 这个研究是为了开发和验证对livestock动物的情绪状态非侵入式指标，以便将其integrate到户外评估协议中。</li>
<li>methods: 这个研究使用了牛的 vocals 作为情绪状态的指标，并使用了深度学习和可解释机器学习来分类低频和高频牛叫。</li>
<li>results: 研究发现，使用深度学习和可解释机器学习可以达到87.2%和89.4%的准确率 для低频和高频牛叫的分类，以及68.9%和72.5%的牛 individuation 精度。<details>
<summary>Abstract</summary>
There is a critical need to develop and validate non-invasive animal-based indicators of affective states in livestock species, in order to integrate them into on-farm assessment protocols, potentially via the use of precision livestock farming (PLF) tools. One such promising approach is the use of vocal indicators. The acoustic structure of vocalizations and their functions were extensively studied in important livestock species, such as pigs, horses, poultry and goats, yet cattle remain understudied in this context to date. Cows were shown to produce two types vocalizations: low-frequency calls (LF), produced with the mouth closed, or partially closed, for close distance contacts and open mouth emitted high-frequency calls (HF), produced for long distance communication, with the latter considered to be largely associated with negative affective states. Moreover, cattle vocalizations were shown to contain information on individuality across a wide range of contexts, both negative and positive. Nowadays, dairy cows are facing a series of negative challenges and stressors in a typical production cycle, making vocalizations during negative affective states of special interest for research. One contribution of this study is providing the largest to date pre-processed (clean from noises) dataset of lactating adult multiparous dairy cows during negative affective states induced by visual isolation challenges. Here we present two computational frameworks - deep learning based and explainable machine learning based, to classify high and low-frequency cattle calls, and individual cow voice recognition. Our models in these two frameworks reached 87.2% and 89.4% accuracy for LF and HF classification, with 68.9% and 72.5% accuracy rates for the cow individual identification, respectively.
</details>
<details>
<summary>摘要</summary>
有一项急需要的发展和验证，那就是通过非侵入性的动物基因体系来评估livestock种类的情绪状态，以便将其 integrate into 农场评估协议中。一种可能的方法是使用 vocal indicators。studied extensively in important livestock species such as pigs, horses, poultry and goats, but cattle have been understudied in this context to date. Cows produce two types of vocalizations: low-frequency calls (LF), produced with the mouth closed or partially closed for close distance contacts, and open mouth emitted high-frequency calls (HF), produced for long distance communication, with the latter considered to be largely associated with negative affective states. Moreover, cattle vocalizations contain information on individuality across a wide range of contexts, both negative and positive. Nowadays, dairy cows are facing a series of negative challenges and stressors in a typical production cycle, making vocalizations during negative affective states of special interest for research. This study provides the largest to date pre-processed (clean from noises) dataset of lactating adult multiparous dairy cows during negative affective states induced by visual isolation challenges. Here we present two computational frameworks - deep learning based and explainable machine learning based - to classify high and low-frequency cattle calls, and individual cow voice recognition. Our models in these two frameworks reached 87.2% and 89.4% accuracy for LF and HF classification, with 68.9% and 72.5% accuracy rates for the cow individual identification, respectively.
</details></li>
</ul>
<hr>
<h2 id="Differentiable-short-time-Fourier-transform-with-respect-to-the-hop-length"><a href="#Differentiable-short-time-Fourier-transform-with-respect-to-the-hop-length" class="headerlink" title="Differentiable short-time Fourier transform with respect to the hop length"></a>Differentiable short-time Fourier transform with respect to the hop length</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02421">http://arxiv.org/abs/2308.02421</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maxime-leiber/dstft">https://github.com/maxime-leiber/dstft</a></li>
<li>paper_authors: Maxime Leiber, Yosra Marnissi, Axel Barrau, Mohammed El Badaoui</li>
<li>for: 提出一种可微分的快时傅立叙变换（STFT），允许通过continuous hop length或frame temporal position的优化来提高时间位置控制。</li>
<li>methods: 使用continuous hop length和frame temporal position的优化，提供更精细的时间位置控制，并且可以使用计算效率更高的优化方法，如梯度下降。</li>
<li>results: 通过 simulations 示例，证明了我们的方法的有效性，并且可以轻松地与现有的算法和神经网络集成。<details>
<summary>Abstract</summary>
In this paper, we propose a differentiable version of the short-time Fourier transform (STFT) that allows for gradient-based optimization of the hop length or the frame temporal position by making these parameters continuous. Our approach provides improved control over the temporal positioning of frames, as the continuous nature of the hop length allows for a more finely-tuned optimization. Furthermore, our contribution enables the use of optimization methods such as gradient descent, which are more computationally efficient than conventional discrete optimization methods. Our differentiable STFT can also be easily integrated into existing algorithms and neural networks. We present a simulated illustration to demonstrate the efficacy of our approach and to garner interest from the research community.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种可微分的短时傅立叙变换（STFT），允许通过使这些参数变为连续的数值来进行梯度下降优化。我们的方法可以提供更好的控制时间位置，因为连续的跳跃长度允许更细化优化。此外，我们的贡献允许使用优化方法，如梯度下降，这些方法更有效率 than conventional discrete optimization methods。我们的可微分STFT也可以轻松地与现有的算法和神经网络集成。我们提供了一个 simulated 示例，以示出我们的方法的有效性并引起研究者的关注。
</details></li>
</ul>
<hr>
<h2 id="METAVerse-Meta-Learning-Traversability-Cost-Map-for-Off-Road-Navigation"><a href="#METAVerse-Meta-Learning-Traversability-Cost-Map-for-Off-Road-Navigation" class="headerlink" title="METAVerse: Meta-Learning Traversability Cost Map for Off-Road Navigation"></a>METAVerse: Meta-Learning Traversability Cost Map for Off-Road Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13991">http://arxiv.org/abs/2307.13991</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junwon Seo, Taekyung Kim, Seongyong Ahn, Kiho Kwak</li>
<li>for: 这篇论文是为了提出一种能够在不同环境中准确地估计地形通行性的自适应导航方法。</li>
<li>methods: 该方法使用了元学习框架，通过自动学习方式将多种环境中的车辆-地形交互反馈纳入模型中，以减少估计uncertainty。</li>
<li>results: 该方法可以在不同环境中获得一个准确和可靠的地形通行性估计模型，并且可以通过与预测控制器结合使用，实现安全和稳定的导航。<details>
<summary>Abstract</summary>
Autonomous navigation in off-road conditions requires an accurate estimation of terrain traversability. However, traversability estimation in unstructured environments is subject to high uncertainty due to the variability of numerous factors that influence vehicle-terrain interaction. Consequently, it is challenging to obtain a generalizable model that can accurately predict traversability in a variety of environments. This paper presents METAVerse, a meta-learning framework for learning a global model that accurately and reliably predicts terrain traversability across diverse environments. We train the traversability prediction network to generate a dense and continuous-valued cost map from a sparse LiDAR point cloud, leveraging vehicle-terrain interaction feedback in a self-supervised manner. Meta-learning is utilized to train a global model with driving data collected from multiple environments, effectively minimizing estimation uncertainty. During deployment, online adaptation is performed to rapidly adapt the network to the local environment by exploiting recent interaction experiences. To conduct a comprehensive evaluation, we collect driving data from various terrains and demonstrate that our method can obtain a global model that minimizes uncertainty. Moreover, by integrating our model with a model predictive controller, we demonstrate that the reduced uncertainty results in safe and stable navigation in unstructured and unknown terrains.
</details>
<details>
<summary>摘要</summary>
自主导航在无结构环境中需要准确地估计地形通行性。然而，在无结构环境中的通行性估计受到许多因素的变化影响，这些因素包括车辆和地形之间的互动。因此，建立一个泛化模型，以准确地预测不同环境中的通行性，是一项挑战。这篇论文提出了METAVerse，一个基于元学习的框架，用于学习一个准确和可靠地预测地形通行性的全球模型。我们在训练通行性预测网络时，使用自动学习的方式，从稀疏的 LiDAR 点云中生成一个密集和连续的成本图，以利用车辆和地形之间的互动反馈。元学习被用来训练全球模型，以使其在多个环境中具有最小的估计不确定性。在部署过程中，我们通过在线适应来快速地适应当地环境，并且通过利用最近的互动经验来进行更新。我们通过收集来自不同地形的驾驶数据，证明了我们的方法可以获得一个全球模型，以准确地预测不同环境中的通行性。此外，我们将我们的模型与预测控制器结合，以验证了减少了不确定性的结果，可以在未知的无结构环境中安全和稳定地导航。
</details></li>
</ul>
<hr>
<h2 id="Differentiable-adaptive-short-time-Fourier-transform-with-respect-to-the-window-length"><a href="#Differentiable-adaptive-short-time-Fourier-transform-with-respect-to-the-window-length" class="headerlink" title="Differentiable adaptive short-time Fourier transform with respect to the window length"></a>Differentiable adaptive short-time Fourier transform with respect to the window length</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02418">http://arxiv.org/abs/2308.02418</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maxime-leiber/dstft">https://github.com/maxime-leiber/dstft</a></li>
<li>paper_authors: Maxime Leiber, Yosra Marnissi, Axel Barrau, Mohammed El Badaoui</li>
<li>for: 这 paper 用于提出了一种基于梯度的方法，用于在实时进行 STFT 的参数优化，包括每帧和每频率窗口长的优化。</li>
<li>methods: 这 paper 使用了梯度下降优化方法，将 STFT 中的窗口长作为连续参数，使得 STFT 可以适应变化的时域频率特征。</li>
<li>results: 作者验证了这种方法在震动分析中的性能，并证明了它可以同时适应变化的时域频率特征，而且可以通过梯度下降优化方法进行快速优化。<details>
<summary>Abstract</summary>
This paper presents a gradient-based method for on-the-fly optimization for both per-frame and per-frequency window length of the short-time Fourier transform (STFT), related to previous work in which we developed a differentiable version of STFT by making the window length a continuous parameter. The resulting differentiable adaptive STFT possesses commendable properties, such as the ability to adapt in the same time-frequency representation to both transient and stationary components, while being easily optimized by gradient descent. We validate the performance of our method in vibration analysis.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:这篇论文提出了一种基于梯度的方法，用于在实时进行STFT（短时傅立叶变换）的框架和频率窗口长度的在线优化，与之前的工作相关，我们将STFT中的窗口长度变为连续参数。结果的可微STFT具有了许多优点，如适应到同时域频率上的激变和站立部分，同时也容易使用梯度下降优化。我们通过振荡分析 validate the performance of our method。
</details></li>
</ul>
<hr>
<h2 id="This-is-not-correct-Negation-aware-Evaluation-of-Language-Generation-Systems"><a href="#This-is-not-correct-Negation-aware-Evaluation-of-Language-Generation-Systems" class="headerlink" title="This is not correct! Negation-aware Evaluation of Language Generation Systems"></a>This is not correct! Negation-aware Evaluation of Language Generation Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13989">http://arxiv.org/abs/2307.13989</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dmlls/cannot-dataset">https://github.com/dmlls/cannot-dataset</a></li>
<li>paper_authors: Miriam Anschütz, Diego Miguel Lozano, Georg Groh</li>
<li>for: 本研究旨在提高大型自然语言处理模型对否定语言的识别能力，以提高模型对语言表达中的意义更加精准地识别和理解。</li>
<li>methods: 本研究使用了规则基本的句子否定工具，创建了CANNOT negation评估数据集，并使用了句子转换器和评估指标进行了细化。</li>
<li>results: 对现有的评估指标进行评估，本研究的细化模型在否定句子上表现出色，与基本模型在其他杂化情况下的表现相比，有很大的提升。<details>
<summary>Abstract</summary>
Large language models underestimate the impact of negations on how much they change the meaning of a sentence. Therefore, learned evaluation metrics based on these models are insensitive to negations. In this paper, we propose NegBLEURT, a negation-aware version of the BLEURT evaluation metric. For that, we designed a rule-based sentence negation tool and used it to create the CANNOT negation evaluation dataset. Based on this dataset, we fine-tuned a sentence transformer and an evaluation metric to improve their negation sensitivity. Evaluating these models on existing benchmarks shows that our fine-tuned models outperform existing metrics on the negated sentences by far while preserving their base models' performances on other perturbations.
</details>
<details>
<summary>摘要</summary>
大型语言模型会在判断句子意义时忽略否定影响。因此，基于这些模型的评估指标会对否定不敏感。在这篇文章中，我们提出了NegBLEURT评估指标，这是一个对否定敏感的BLEURT评估指标。为了建立这个评估指标，我们设计了一个基于规则的句子否定工具，并使用这个工具创建了CANNOT评估集。基于这个集，我们精炼了句子变换器和评估指标，以提高它们对否定的敏感度。对现有的benchmark测试显示，我们精炼的模型在否定句子上表现明显比其他评估指标好，而且保留了基本模型在其他扰动下的表现。
</details></li>
</ul>
<hr>
<h2 id="Controlling-the-Latent-Space-of-GANs-through-Reinforcement-Learning-A-Case-Study-on-Task-based-Image-to-Image-Translation"><a href="#Controlling-the-Latent-Space-of-GANs-through-Reinforcement-Learning-A-Case-Study-on-Task-based-Image-to-Image-Translation" class="headerlink" title="Controlling the Latent Space of GANs through Reinforcement Learning: A Case Study on Task-based Image-to-Image Translation"></a>Controlling the Latent Space of GANs through Reinforcement Learning: A Case Study on Task-based Image-to-Image Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13978">http://arxiv.org/abs/2307.13978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahyar Abbasian, Taha Rajabzadeh, Ahmadreza Moradipari, Seyed Amir Hossein Aqajari, Hongsheng Lu, Amir Rahmani</li>
<li>for: 这篇论文旨在解决Generative Adversarial Networks (GAN) 的控制问题，提高 GAN 的生成效果。</li>
<li>methods: 本论文提出了一种新的方法，通过融合对抗学习（RL）代理人与潜在空间 GAN（l-GAN），以生成适当的出力。RL 代理人通过精心设计的奖励策略，获得了在潜在空间中穿梭并生成出力的能力。</li>
<li>results: 本论文通过使用 MNIST dataset 进行了一系列实验，包括一个示例任务：加法。实验结果证实了我们的方法效果。本论文的创新的RL代理人与 GAN 模型融合，具有很大的应用前途。<details>
<summary>Abstract</summary>
Generative Adversarial Networks (GAN) have emerged as a formidable AI tool to generate realistic outputs based on training datasets. However, the challenge of exerting control over the generation process of GANs remains a significant hurdle. In this paper, we propose a novel methodology to address this issue by integrating a reinforcement learning (RL) agent with a latent-space GAN (l-GAN), thereby facilitating the generation of desired outputs. More specifically, we have developed an actor-critic RL agent with a meticulously designed reward policy, enabling it to acquire proficiency in navigating the latent space of the l-GAN and generating outputs based on specified tasks. To substantiate the efficacy of our approach, we have conducted a series of experiments employing the MNIST dataset, including arithmetic addition as an illustrative task. The outcomes of these experiments serve to validate our methodology. Our pioneering integration of an RL agent with a GAN model represents a novel advancement, holding great potential for enhancing generative networks in the future.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Mathematical-Modeling-of-BCG-based-Bladder-Cancer-Treatment-Using-Socio-Demographics"><a href="#Mathematical-Modeling-of-BCG-based-Bladder-Cancer-Treatment-Using-Socio-Demographics" class="headerlink" title="Mathematical Modeling of BCG-based Bladder Cancer Treatment Using Socio-Demographics"></a>Mathematical Modeling of BCG-based Bladder Cancer Treatment Using Socio-Demographics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15084">http://arxiv.org/abs/2307.15084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elizaveta Savchenko, Ariel Rosenfeld, Svetlana Bunimovich-Mendrazitsky</li>
<li>for: 这个研究旨在提出一种个性化的数学模型，用于预测bcg基于治疗的临床动态。</li>
<li>methods: 该研究采用了一种已知的bcg治疗模型，并将机器学习组件 integrate到模型中，以时间性地调整和重配置关键参数。</li>
<li>results: 使用实际临床数据，研究表明，个性化模型与原始模型相比，在预测bcg治疗结束时的癌细胞数量方面有14.8%的改善，平均而言。<details>
<summary>Abstract</summary>
Cancer is one of the most widespread diseases around the world with millions of new patients each year. Bladder cancer is one of the most prevalent types of cancer affecting all individuals alike with no obvious prototypical patient. The current standard treatment for BC follows a routine weekly Bacillus Calmette-Guerin (BCG) immunotherapy-based therapy protocol which is applied to all patients alike. The clinical outcomes associated with BCG treatment vary significantly among patients due to the biological and clinical complexity of the interaction between the immune system, treatments, and cancer cells. In this study, we take advantage of the patient's socio-demographics to offer a personalized mathematical model that describes the clinical dynamics associated with BCG-based treatment. To this end, we adopt a well-established BCG treatment model and integrate a machine learning component to temporally adjust and reconfigure key parameters within the model thus promoting its personalization. Using real clinical data, we show that our personalized model favorably compares with the original one in predicting the number of cancer cells at the end of the treatment, with 14.8% improvement, on average.
</details>
<details>
<summary>摘要</summary>
肿瘤是全球最普遍的疾病之一，每年新生发病人数达到百万。膀胱癌是所有人都受到影响的最常见的抑肿癌类型，没有明显的典型患者形象。现有的标准治疗办法是每周一次的细菌Calmette-Guerin（BCG）免疫疗法，该办法适用于所有患者。但是，BCG治疗的临床结果因免疫系统、治疗和肿瘤细胞之间的生物和临床复杂性而异常变化。在这项研究中，我们利用患者的社会民生数据来提供个性化的数学模型，描述BCG基于治疗的临床动态。为此，我们采用了已知的BCG治疗模型，并将机器学习组件加入，以时间地调整和重新配置关键参数，以便个性化。使用实际临床数据，我们表明，我们的个性化模型与原始模型相比，在预测治疗结束后肿瘤细胞数量方面表现出了14.8%的改善，平均而言。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Deep-Neural-Networks-via-Linear-Separability-of-Hidden-Layers"><a href="#Understanding-Deep-Neural-Networks-via-Linear-Separability-of-Hidden-Layers" class="headerlink" title="Understanding Deep Neural Networks via Linear Separability of Hidden Layers"></a>Understanding Deep Neural Networks via Linear Separability of Hidden Layers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13962">http://arxiv.org/abs/2307.13962</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Zhang, Xinyu Chen, Wensheng Li, Lixue Liu, Wei Wu, Dacheng Tao</li>
<li>for: 本研究用来研究深度神经网络的特点，特别是深度神经网络的线性可分性。</li>
<li>methods: 本研究提出了基于米诺夫做ifferencedifference measure（MD-LSM）来评估线性可分性度的两个点集。然后，我们证明了深度神经网络训练性能和线性可分性度之间存在同步关系，即如果更新权重可以提高线性可分性度，则更新后的网络将在训练过程中表现更好，并且相反。此外，我们还研究了活化函数和网络大小（包括宽度和深度）对隐藏层的线性可分性度的影响。</li>
<li>results: 我们通过实验 validate了我们的发现，测试了一些流行的深度网络，包括多层感知器（MLP）、卷积神经网络（CNN）、深度信念网络（DBN）、ResNet、VGGNet、AlexNet、视transformer（ViT）和GoogLeNet。<details>
<summary>Abstract</summary>
In this paper, we measure the linear separability of hidden layer outputs to study the characteristics of deep neural networks. In particular, we first propose Minkowski difference based linear separability measures (MD-LSMs) to evaluate the linear separability degree of two points sets. Then, we demonstrate that there is a synchronicity between the linear separability degree of hidden layer outputs and the network training performance, i.e., if the updated weights can enhance the linear separability degree of hidden layer outputs, the updated network will achieve a better training performance, and vice versa. Moreover, we study the effect of activation function and network size (including width and depth) on the linear separability of hidden layers. Finally, we conduct the numerical experiments to validate our findings on some popular deep networks including multilayer perceptron (MLP), convolutional neural network (CNN), deep belief network (DBN), ResNet, VGGNet, AlexNet, vision transformer (ViT) and GoogLeNet.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们测量了深度神经网络中隐藏层输出的线性分割性，以study深度神经网络的特点。特别是，我们首先提出了Minkowski差值基于的线性分割度量指标（MD-LSM），用于评估两个点集的线性分割度。然后，我们示出了隐藏层输出线性分割度和网络训练性能之间的同步关系，即如果更新的权重可以提高隐藏层输出的线性分割度，则更新后的网络将达到更好的训练性能，并且相反。此外，我们研究了活动函数和网络大小（包括宽和深）对隐藏层的线性分割性的影响。最后，我们进行了实验验证我们的发现，并在一些流行的深度神经网络，如多层感知网络（MLP）、卷积神经网络（CNN）、深度信念网络（DBN）、ResNet、VGGNet、AlexNet、视Transformer（ViT）和GoogLeNet等进行了数值实验。
</details></li>
</ul>
<hr>
<h2 id="Flexible-Differentially-Private-Vertical-Federated-Learning-with-Adaptive-Feature-Embeddings"><a href="#Flexible-Differentially-Private-Vertical-Federated-Learning-with-Adaptive-Feature-Embeddings" class="headerlink" title="Flexible Differentially Private Vertical Federated Learning with Adaptive Feature Embeddings"></a>Flexible Differentially Private Vertical Federated Learning with Adaptive Feature Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02362">http://arxiv.org/abs/2308.02362</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxi Mi, Hongquan Liu, Yewei Xia, Yiheng Sun, Jihong Guan, Shuigeng Zhou<br>for: This paper focuses on the delicate balance between data privacy and task utility in vertical federated learning (VFL) under differential privacy (DP).methods: The authors propose a flexible and generic approach that decouples the privacy and utility goals, addressing them successively. They first derive a privacy guarantee using norm clipping on shared feature embeddings, and then optimize task utility through adaptive adjustments on the scale and distribution of feature embeddings.results: The proposed VFL-AFE framework exhibits effectiveness against privacy attacks and retains favorable task utility, as demonstrated through extensive experiments.<details>
<summary>Abstract</summary>
The emergence of vertical federated learning (VFL) has stimulated concerns about the imperfection in privacy protection, as shared feature embeddings may reveal sensitive information under privacy attacks. This paper studies the delicate equilibrium between data privacy and task utility goals of VFL under differential privacy (DP). To address the generality issue of prior arts, this paper advocates a flexible and generic approach that decouples the two goals and addresses them successively. Specifically, we initially derive a rigorous privacy guarantee by applying norm clipping on shared feature embeddings, which is applicable across various datasets and models. Subsequently, we demonstrate that task utility can be optimized via adaptive adjustments on the scale and distribution of feature embeddings in an accuracy-appreciative way, without compromising established DP mechanisms. We concretize our observation into the proposed VFL-AFE framework, which exhibits effectiveness against privacy attacks and the capacity to retain favorable task utility, as substantiated by extensive experiments.
</details>
<details>
<summary>摘要</summary>
vertical federated learning (VFL)的出现引起了隐私保护不足的担忧，因为分享特征嵌入可能在隐私攻击下泄露敏感信息。这篇论文研究了VFL中数据隐私和任务用途目标之间的紧耦合关系，并提出了一种flexible和通用的方法来解决这个问题。我们首先通过应用norm clipping来 derive privacy guarantee，这种方法适用于各种数据集和模型。然后，我们示出了通过 adaptive adjustments来 optimize任务用途，无需牺牲已有的隐私机制。我们将这些观察集成为VFL-AFE框架，该框架具有防止隐私攻击和保持任务用途的能力，并经过了广泛的实验证明。
</details></li>
</ul>
<hr>
<h2 id="Entropy-Neural-Estimation-for-Graph-Contrastive-Learning"><a href="#Entropy-Neural-Estimation-for-Graph-Contrastive-Learning" class="headerlink" title="Entropy Neural Estimation for Graph Contrastive Learning"></a>Entropy Neural Estimation for Graph Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13944">http://arxiv.org/abs/2307.13944</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kunzhan/M-ILBO">https://github.com/kunzhan/M-ILBO</a></li>
<li>paper_authors: Yixuan Ma, Xiaolin Zhang, Peng Zhang, Kun Zhan</li>
<li>for: 本文提出了一种基于对比学习的图像分类方法，用于提取图像中的高级特征表示。</li>
<li>methods: 本文使用了一种基于Maximum Mutual Information的方法来估计数据集的熵，并提出了一种简单 yet effective的子集采样策略来实现对比表示之间的对比。</li>
<li>results: 实验结果表明，提出的方法可以在七个图像benchmark上 achieve competitive performance，并且可以增强图像encoder的表示能力。<details>
<summary>Abstract</summary>
Contrastive learning on graphs aims at extracting distinguishable high-level representations of nodes. In this paper, we theoretically illustrate that the entropy of a dataset can be approximated by maximizing the lower bound of the mutual information across different views of a graph, \ie, entropy is estimated by a neural network. Based on this finding, we propose a simple yet effective subset sampling strategy to contrast pairwise representations between views of a dataset. In particular, we randomly sample nodes and edges from a given graph to build the input subset for a view. Two views are fed into a parameter-shared Siamese network to extract the high-dimensional embeddings and estimate the information entropy of the entire graph. For the learning process, we propose to optimize the network using two objectives, simultaneously. Concretely, the input of the contrastive loss function consists of positive and negative pairs. Our selection strategy of pairs is different from previous works and we present a novel strategy to enhance the representation ability of the graph encoder by selecting nodes based on cross-view similarities. We enrich the diversity of the positive and negative pairs by selecting highly similar samples and totally different data with the guidance of cross-view similarity scores, respectively. We also introduce a cross-view consistency constraint on the representations generated from the different views. This objective guarantees the learned representations are consistent across views from the perspective of the entire graph. We conduct extensive experiments on seven graph benchmarks, and the proposed approach achieves competitive performance compared to the current state-of-the-art methods. The source code will be publicly released once this paper is accepted.
</details>
<details>
<summary>摘要</summary>
contrastive learning on graphs aims to extract distinguishable high-level node representations. in this paper, we prove that the entropy of a dataset can be approximated by maximizing the lower bound of the mutual information across different graph views, \ie, entropy is estimated by a neural network. based on this finding, we propose a simple yet effective subset sampling strategy to contrast pairwise representations between views of a dataset. specifically, we randomly sample nodes and edges from a given graph to build the input subset for a view. two views are fed into a parameter-shared siamese network to extract high-dimensional embeddings and estimate the information entropy of the entire graph. for the learning process, we propose to optimize the network using two objectives simultaneously. concretely, the input of the contrastive loss function consists of positive and negative pairs. our selection strategy of pairs is different from previous works, and we present a novel strategy to enhance the representation ability of the graph encoder by selecting nodes based on cross-view similarities. we enrich the diversity of the positive and negative pairs by selecting highly similar samples and totally different data with the guidance of cross-view similarity scores, respectively. we also introduce a cross-view consistency constraint on the representations generated from the different views. this objective guarantees the learned representations are consistent across views from the perspective of the entire graph. we conduct extensive experiments on seven graph benchmarks, and the proposed approach achieves competitive performance compared to the current state-of-the-art methods. the source code will be publicly released once this paper is accepted.
</details></li>
</ul>
<hr>
<h2 id="Topology-aware-Robust-Optimization-for-Out-of-distribution-Generalization"><a href="#Topology-aware-Robust-Optimization-for-Out-of-distribution-Generalization" class="headerlink" title="Topology-aware Robust Optimization for Out-of-distribution Generalization"></a>Topology-aware Robust Optimization for Out-of-distribution Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13943">http://arxiv.org/abs/2307.13943</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/joffery/tro">https://github.com/joffery/tro</a></li>
<li>paper_authors: Fengchun Qiao, Xi Peng</li>
<li>for: 本研究旨在提高机器学习模型对异常输入的抗性，以提高高风险应用中的模型可靠性。</li>
<li>methods: 本研究提出了一种基于分布 topology 的 robust optimization 方法，包括两个优化目标：分布学习和学习 на topology。</li>
<li>results: 实验表明，这种方法可以在多种任务中（包括分类、回归和semantic segmentation）明显超过现有方法，并且发现数据驱动的分布 topology 与领域知识是相一致的，从而提高了方法的可解释性。<details>
<summary>Abstract</summary>
Out-of-distribution (OOD) generalization is a challenging machine learning problem yet highly desirable in many high-stake applications. Existing methods suffer from overly pessimistic modeling with low generalization confidence. As generalizing to arbitrary test distributions is impossible, we hypothesize that further structure on the topology of distributions is crucial in developing strong OOD resilience. To this end, we propose topology-aware robust optimization (TRO) that seamlessly integrates distributional topology in a principled optimization framework. More specifically, TRO solves two optimization objectives: (1) Topology Learning which explores data manifold to uncover the distributional topology; (2) Learning on Topology which exploits the topology to constrain robust optimization for tightly-bounded generalization risks. We theoretically demonstrate the effectiveness of our approach and empirically show that it significantly outperforms the state of the arts in a wide range of tasks including classification, regression, and semantic segmentation. Moreover, we empirically find the data-driven distributional topology is consistent with domain knowledge, enhancing the explainability of our approach.
</details>
<details>
<summary>摘要</summary>
OUT-OF-DISTRIBUTION (OOD) 泛化是一个具有挑战性的机器学习问题，但在许多高风险应用中很需要。现有的方法受到过度的悲观预测，导致模型的泛化自信度较低。由于泛化到任意的测试分布是不可能的，我们假设预设分布的数学结构是决定性的。为了解决这个问题，我们提出了分布数学结构意识的强健泛化优化（TRO）。TRO通过将分布数学结构与强健优化紧密融合，实现了具有理性的优化框架。具体而言，TRO解决以下两个优化目标：1. 分布学习：探索数据构造，发现分布的数学结构。2. 学习于分布：利用分布的数学结构，对泛化优化进行紧密的约束，以降低泛化风险。我们理论上显示TRO的有效性，并实践显示它在许多任务中，包括分类、回归和semantic segmentation，与现有的方法相比，表现得更好。此外，我们实践发现，使用数据驱动的分布数学结构可以与领域知识相互匹配，增加了我们的方法的解释性。
</details></li>
</ul>
<hr>
<h2 id="Improving-Semi-Supervised-Semantic-Segmentation-with-Dual-Level-Siamese-Structure-Network"><a href="#Improving-Semi-Supervised-Semantic-Segmentation-with-Dual-Level-Siamese-Structure-Network" class="headerlink" title="Improving Semi-Supervised Semantic Segmentation with Dual-Level Siamese Structure Network"></a>Improving Semi-Supervised Semantic Segmentation with Dual-Level Siamese Structure Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13938">http://arxiv.org/abs/2307.13938</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kunzhan/DSSN">https://github.com/kunzhan/DSSN</a></li>
<li>paper_authors: Zhibo Tain, Xiaolin Zhang, Peng Zhang, Kun Zhan</li>
<li>for: 提高semantic segmentation的效果，使用both labeled和无标例数据，减少标注训练示例的成本</li>
<li>methods: 提出了一种基于 dual-level Siamese structure network (DSSN) 的像素级对比学习方法，通过在低级图像空间和高级特征空间都使用强制修改视图进行对比，以最大化使用可用的无标例数据</li>
<li>results: 实现了在PASCAL VOC 2012和Cityscapes两个 dataset 上的状态级 результаategraph，与其他 SSS 算法相比，表现出了显著的优异，并且可以减少标注训练示例的成本<details>
<summary>Abstract</summary>
Semi-supervised semantic segmentation (SSS) is an important task that utilizes both labeled and unlabeled data to reduce expenses on labeling training examples. However, the effectiveness of SSS algorithms is limited by the difficulty of fully exploiting the potential of unlabeled data. To address this, we propose a dual-level Siamese structure network (DSSN) for pixel-wise contrastive learning. By aligning positive pairs with a pixel-wise contrastive loss using strong augmented views in both low-level image space and high-level feature space, the proposed DSSN is designed to maximize the utilization of available unlabeled data. Additionally, we introduce a novel class-aware pseudo-label selection strategy for weak-to-strong supervision, which addresses the limitations of most existing methods that do not perform selection or apply a predefined threshold for all classes. Specifically, our strategy selects the top high-confidence prediction of the weak view for each class to generate pseudo labels that supervise the strong augmented views. This strategy is capable of taking into account the class imbalance and improving the performance of long-tailed classes. Our proposed method achieves state-of-the-art results on two datasets, PASCAL VOC 2012 and Cityscapes, outperforming other SSS algorithms by a significant margin.
</details>
<details>
<summary>摘要</summary>
semi-supervised semantic segmentation (SSS) 是一个重要的任务，它利用了标注和无标注数据来降低标注训练示例的成本。然而，SSS 算法的效果受到无标注数据的难以完全利用的限制。为解决这个问题，我们提议一种 dual-level Siamese structure network (DSSN)  для像素级别的对比学习。通过在低级图像空间和高级特征空间都使用强化的扩展视图对正对应的对比损失进行对 Pablo 的对比，我们设计了 DSSN，以便最大化可用的无标注数据的利用。此外，我们引入了一种新的类感知 pseudo-label 选择策略，这种策略可以考虑类别偏度，并提高长尾类别的性能。我们的策略是选择每个类型的高信息报告值作为弱视图中的 pseudo 标签，以便使得强视图中的augmented views得到supervise。我们的提议方法在 PASCAL VOC 2012 和 Cityscapes 两个 dataset 上实现了领先的状态，比其他 SSS 算法高出了一定的幅度。
</details></li>
</ul>
<hr>
<h2 id="trajdata-A-Unified-Interface-to-Multiple-Human-Trajectory-Datasets"><a href="#trajdata-A-Unified-Interface-to-Multiple-Human-Trajectory-Datasets" class="headerlink" title="trajdata: A Unified Interface to Multiple Human Trajectory Datasets"></a>trajdata: A Unified Interface to Multiple Human Trajectory Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13924">http://arxiv.org/abs/2307.13924</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nvlabs/trajdata">https://github.com/nvlabs/trajdata</a></li>
<li>paper_authors: Boris Ivanovic, Guanyu Song, Igor Gilitschenski, Marco Pavone</li>
<li>for: This paper aims to provide a unified interface to multiple human trajectory datasets for researchers to train and evaluate methods more efficiently.</li>
<li>methods: The paper presents a simple, uniform, and efficient representation and API for trajectory and map data, which enables a comprehensive empirical evaluation of existing trajectory datasets.</li>
<li>results: The paper conducts a comprehensive empirical evaluation of existing trajectory datasets, providing users with a rich understanding of the data underpinning much of current pedestrian and AV motion forecasting research, and proposing suggestions for future datasets from these insights.<details>
<summary>Abstract</summary>
The field of trajectory forecasting has grown significantly in recent years, partially owing to the release of numerous large-scale, real-world human trajectory datasets for autonomous vehicles (AVs) and pedestrian motion tracking. While such datasets have been a boon for the community, they each use custom and unique data formats and APIs, making it cumbersome for researchers to train and evaluate methods across multiple datasets. To remedy this, we present trajdata: a unified interface to multiple human trajectory datasets. At its core, trajdata provides a simple, uniform, and efficient representation and API for trajectory and map data. As a demonstration of its capabilities, in this work we conduct a comprehensive empirical evaluation of existing trajectory datasets, providing users with a rich understanding of the data underpinning much of current pedestrian and AV motion forecasting research, and proposing suggestions for future datasets from these insights. trajdata is permissively licensed (Apache 2.0) and can be accessed online at https://github.com/NVlabs/trajdata
</details>
<details>
<summary>摘要</summary>
“ trajectory forecasting 领域在最近几年内发展非常快，一部分这是因为大量的实际世界人 trajectory 数据集（AVs 和行人运动跟踪）的发布。 although these datasets have been a blessing for the community, they each use custom and unique data formats and APIs, making it cumbersome for researchers to train and evaluate methods across multiple datasets. to address this, we present trajdata: a unified interface to multiple human trajectory datasets. at its core, trajdata provides a simple, uniform, and efficient representation and API for trajectory and map data. as a demonstration of its capabilities, in this work we conduct a comprehensive empirical evaluation of existing trajectory datasets, providing users with a rich understanding of the data underpinning much of current pedestrian and AV motion forecasting research, and proposing suggestions for future datasets from these insights. trajdata is licensed under Apache 2.0 and can be accessed online at https://github.com/NVlabs/trajdata.”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="HyperFed-Hyperbolic-Prototypes-Exploration-with-Consistent-Aggregation-for-Non-IID-Data-in-Federated-Learning"><a href="#HyperFed-Hyperbolic-Prototypes-Exploration-with-Consistent-Aggregation-for-Non-IID-Data-in-Federated-Learning" class="headerlink" title="HyperFed: Hyperbolic Prototypes Exploration with Consistent Aggregation for Non-IID Data in Federated Learning"></a>HyperFed: Hyperbolic Prototypes Exploration with Consistent Aggregation for Non-IID Data in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14384">http://arxiv.org/abs/2307.14384</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinting Liao, Weiming Liu, Chaochao Chen, Pengyang Zhou, Huabin Zhu, Yanchao Tan, Jun Wang, Yue Qi</li>
<li>for: 提高 Federated Learning（FL）在非同一个分布（non-IID）环境下的性能。</li>
<li>methods: 提议 HyperFed，它包含以下三个主要模块： hyperbolic prototype Tammes initialization（HPTI）、 hyperbolic prototype learning（HPL）和 consistent aggregation（CA）。 HPTI在服务器端 constructions  uniformly distributed和 fixed class prototypes，并将其分享给客户端以匹配类统计，从而引导客户端的具有一致性的特征表示。 HPL在每个客户端上使用分享的类prototype在 hyperbolic 模型空间中捕捉本地数据中的层次信息。 CA在服务器端 mitigates the impact of inconsistent deviations from clients to server。</li>
<li>results: 对四个数据集进行了广泛的研究，证明 HyperFed 可以有效地提高 FL 在 non-IID 环境下的性能。<details>
<summary>Abstract</summary>
Federated learning (FL) collaboratively models user data in a decentralized way. However, in the real world, non-identical and independent data distributions (non-IID) among clients hinder the performance of FL due to three issues, i.e., (1) the class statistics shifting, (2) the insufficient hierarchical information utilization, and (3) the inconsistency in aggregating clients. To address the above issues, we propose HyperFed which contains three main modules, i.e., hyperbolic prototype Tammes initialization (HPTI), hyperbolic prototype learning (HPL), and consistent aggregation (CA). Firstly, HPTI in the server constructs uniformly distributed and fixed class prototypes, and shares them with clients to match class statistics, further guiding consistent feature representation for local clients. Secondly, HPL in each client captures the hierarchical information in local data with the supervision of shared class prototypes in the hyperbolic model space. Additionally, CA in the server mitigates the impact of the inconsistent deviations from clients to server. Extensive studies of four datasets prove that HyperFed is effective in enhancing the performance of FL under the non-IID set.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 协同模型用户数据的方式是分布在多个客户端上的。然而，在现实中，客户端上的数据分布非常不同（非同一个），这会降低 FL 的性能，因为三个问题：1. 类别统计移动2. 不充分利用层次信息3. 客户端聚合不一致为解决这些问题，我们提出了 HyperFed，它包含三个主要模块：1. hyperbolic prototype Tammes initialization (HPTI)：在服务器端constructs uniformly distributed和fixed class prototypes，并将其分享给客户端，以匹配类统计，并且指导客户端的准确特征表示。2. hyperbolic prototype learning (HPL)：在每个客户端上，通过在hyperbolic模型空间的supervision，使得客户端上的数据具有层次结构信息。3. consistent aggregation (CA)：在服务器端，使得客户端的不一致偏差的影响被减轻。对四个数据集进行了广泛的研究，证明了 HyperFed 能够在非同一个情况下提高 FL 的性能。
</details></li>
</ul>
<hr>
<h2 id="Simulation-based-Inference-for-Cardiovascular-Models"><a href="#Simulation-based-Inference-for-Cardiovascular-Models" class="headerlink" title="Simulation-based Inference for Cardiovascular Models"></a>Simulation-based Inference for Cardiovascular Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13918">http://arxiv.org/abs/2307.13918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antoine Wehenkel, Jens Behrmann, Andrew C. Miller, Guillermo Sapiro, Ozan Sener, Marco Cuturi, Jörn-Henrik Jacobsen</li>
<li>for: studying cardiovascular systems in-silico and simulating whole-body hemodynamics</li>
<li>methods: statistical inference and simulation-based inference (SBI)</li>
<li>results: potential for estimating new biomarkers from standard-of-care measurements, existence of sub-populations with distinct uncertainty regimes, and gap analysis between in-vivo and in-silico data<details>
<summary>Abstract</summary>
Over the past decades, hemodynamics simulators have steadily evolved and have become tools of choice for studying cardiovascular systems in-silico. While such tools are routinely used to simulate whole-body hemodynamics from physiological parameters, solving the corresponding inverse problem of mapping waveforms back to plausible physiological parameters remains both promising and challenging. Motivated by advances in simulation-based inference (SBI), we cast this inverse problem as statistical inference. In contrast to alternative approaches, SBI provides \textit{posterior distributions} for the parameters of interest, providing a \textit{multi-dimensional} representation of uncertainty for \textit{individual} measurements. We showcase this ability by performing an in-silico uncertainty analysis of five biomarkers of clinical interest comparing several measurement modalities. Beyond the corroboration of known facts, such as the feasibility of estimating heart rate, our study highlights the potential of estimating new biomarkers from standard-of-care measurements. SBI reveals practically relevant findings that cannot be captured by standard sensitivity analyses, such as the existence of sub-populations for which parameter estimation exhibits distinct uncertainty regimes. Finally, we study the gap between in-vivo and in-silico with the MIMIC-III waveform database and critically discuss how cardiovascular simulations can inform real-world data analysis.
</details>
<details>
<summary>摘要</summary>
Inspired by advances in simulation-based inference (SBI), we approach this inverse problem as a statistical inference problem. Unlike other methods, SBI provides a distribution of posterior probabilities for the parameters of interest, offering a comprehensive and multi-dimensional representation of uncertainty for individual measurements.We demonstrate the power of SBI by performing an in-silico uncertainty analysis of five biomarkers of clinical interest using different measurement modalities. Our study not only confirms established findings, such as the feasibility of estimating heart rate, but also highlights the potential of estimating new biomarkers from standard-of-care measurements.SBI reveals practical insights that cannot be obtained through standard sensitivity analyses, such as the existence of sub-populations with distinct uncertainty regimes. Furthermore, we explore the gap between in-vivo and in-silico using the MIMIC-III waveform database and discuss how cardiovascular simulations can inform real-world data analysis.
</details></li>
</ul>
<hr>
<h2 id="BayesDAG-Gradient-Based-Posterior-Sampling-for-Causal-Discovery"><a href="#BayesDAG-Gradient-Based-Posterior-Sampling-for-Causal-Discovery" class="headerlink" title="BayesDAG: Gradient-Based Posterior Sampling for Causal Discovery"></a>BayesDAG: Gradient-Based Posterior Sampling for Causal Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13917">http://arxiv.org/abs/2307.13917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yashas Annadani, Nick Pawlowski, Joel Jennings, Stefan Bauer, Cheng Zhang, Wenbo Gong</li>
<li>for:  This paper aims to develop a scalable Bayesian causal discovery framework for inferring the posterior distribution over causal models from observed data, addressing computational challenges in joint inference over combinatorial space of Directed Acyclic Graphs (DAGs) and nonlinear functions.</li>
<li>methods:  The proposed framework uses stochastic gradient Markov Chain Monte Carlo (SG-MCMC) to directly sample DAGs from the posterior without requiring any DAG regularization, simultaneously drawing function parameter samples and applicable to both linear and nonlinear causal models.</li>
<li>results:  Empirical evaluations on synthetic and real-world datasets demonstrate the effectiveness of the proposed approach compared to state-of-the-art baselines.<details>
<summary>Abstract</summary>
Bayesian causal discovery aims to infer the posterior distribution over causal models from observed data, quantifying epistemic uncertainty and benefiting downstream tasks. However, computational challenges arise due to joint inference over combinatorial space of Directed Acyclic Graphs (DAGs) and nonlinear functions. Despite recent progress towards efficient posterior inference over DAGs, existing methods are either limited to variational inference on node permutation matrices for linear causal models, leading to compromised inference accuracy, or continuous relaxation of adjacency matrices constrained by a DAG regularizer, which cannot ensure resulting graphs are DAGs. In this work, we introduce a scalable Bayesian causal discovery framework based on stochastic gradient Markov Chain Monte Carlo (SG-MCMC) that overcomes these limitations. Our approach directly samples DAGs from the posterior without requiring any DAG regularization, simultaneously draws function parameter samples and is applicable to both linear and nonlinear causal models. To enable our approach, we derive a novel equivalence to the permutation-based DAG learning, which opens up possibilities of using any relaxed gradient estimator defined over permutations. To our knowledge, this is the first framework applying gradient-based MCMC sampling for causal discovery. Empirical evaluations on synthetic and real-world datasets demonstrate our approach's effectiveness compared to state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
bayesian causal discovery aimed to infer the posterior distribution over causal models from observed data, quantifying epistemic uncertainty and benefiting downstream tasks. However, computational challenges arise due to joint inference over combinatorial space of Directed Acyclic Graphs (DAGs) and nonlinear functions. Despite recent progress towards efficient posterior inference over DAGs, existing methods are either limited to variational inference on node permutation matrices for linear causal models, leading to compromised inference accuracy, or continuous relaxation of adjacency matrices constrained by a DAG regularizer, which cannot ensure resulting graphs are DAGs. In this work, we introduce a scalable Bayesian causal discovery framework based on stochastic gradient Markov Chain Monte Carlo (SG-MCMC) that overcomes these limitations. Our approach directly samples DAGs from the posterior without requiring any DAG regularization, simultaneously draws function parameter samples and is applicable to both linear and nonlinear causal models. To enable our approach, we derive a novel equivalence to the permutation-based DAG learning, which opens up possibilities of using any relaxed gradient estimator defined over permutations. To our knowledge, this is the first framework applying gradient-based MCMC sampling for causal discovery. Empirical evaluations on synthetic and real-world datasets demonstrate our approach's effectiveness compared to state-of-the-art baselines.
</details></li>
</ul>
<hr>
<h2 id="Online-learning-in-bandits-with-predicted-context"><a href="#Online-learning-in-bandits-with-predicted-context" class="headerlink" title="Online learning in bandits with predicted context"></a>Online learning in bandits with predicted context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13916">http://arxiv.org/abs/2307.13916</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongyi Guo, Susan Murphy</li>
<li>for:  solves the contextual bandit problem with non-diminishing context error.</li>
<li>methods:  uses an extension of the measurement error model in classical statistics to the online decision-making setting.</li>
<li>results:  achieves sublinear regret compared to the appropriate benchmark, despite the non-diminishing context error.<details>
<summary>Abstract</summary>
We consider the contextual bandit problem where at each time, the agent only has access to a noisy version of the context and the error variance (or an estimator of this variance). This setting is motivated by a wide range of applications where the true context for decision-making is unobserved, and only a prediction of the context by a potentially complex machine learning algorithm is available. When the context error is non-diminishing, classical bandit algorithms fail to achieve sublinear regret. We propose the first online algorithm in this setting with sublinear regret compared to the appropriate benchmark. The key idea is to extend the measurement error model in classical statistics to the online decision-making setting, which is nontrivial due to the policy being dependent on the noisy context observations.
</details>
<details>
<summary>摘要</summary>
我们考虑了上下文搅拌问题，在每个时间点，代理人只有访问不准确的上下文和错误方差（或一个估计这种方差）的权限。这种设定是由各种应用领域中真实的上下文决策不可见，只有一个复杂机器学习算法预测的上下文预测所 inspirited。当上下文错误不断增长时， klasik bandit算法无法实现子线性 regret。我们提议的首个在这种设定下的在线算法具有子线性 regret，相对于合适的 benchmark。关键思想是将经典统计中的测量错误模型扩展到在线决策设定中，这是因为政策对于听到的不准确上下文观察有依赖关系。
</details></li>
</ul>
<hr>
<h2 id="Graph-Neural-Networks-based-Hybrid-Framework-For-Predicting-Particle-Crushing-Strength"><a href="#Graph-Neural-Networks-based-Hybrid-Framework-For-Predicting-Particle-Crushing-Strength" class="headerlink" title="Graph Neural Networks-based Hybrid Framework For Predicting Particle Crushing Strength"></a>Graph Neural Networks-based Hybrid Framework For Predicting Particle Crushing Strength</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13909">http://arxiv.org/abs/2307.13909</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/doujiang-zheng/gnn-for-particle-crushing">https://github.com/doujiang-zheng/gnn-for-particle-crushing</a></li>
<li>paper_authors: Tongya Zheng, Tianli Zhang, Qingzheng Guan, Wenjie Huang, Zunlei Feng, Mingli Song, Chun Chen</li>
<li>for:  This paper aims to apply Graph Neural Networks (GNNs) to model the mechanical behaviors of particle crushing and predict particle crushing strength.</li>
<li>methods: The authors use a hybrid framework based on GNNs to predict particle crushing strength in a particle fragment view, and compare their method against traditional machine learning methods and a plain Multi-Layer Perceptron (MLP).</li>
<li>results: The authors verify the effectiveness of their hybrid framework through numerical simulations and discuss the usefulness of different features through gradient attribution explanation.Here is the same information in Simplified Chinese:</li>
<li>for: 这篇论文目的是应用图гра树神经网络（GNNs）来模型粉体压碎的机械行为并预测粉体压碎强度。</li>
<li>methods: 作者使用一种混合方法基于GNNs来预测粉体压碎强度在粉体Fragment视图中，并与传统机器学习方法和简单的多层感知网络（MLP）进行比较。</li>
<li>results: 作者通过数值仿真 verify了他们的混合方法的有效性，并通过Gradient attribute解释来评估不同特征的用于预测。<details>
<summary>Abstract</summary>
Graph Neural Networks have emerged as an effective machine learning tool for multi-disciplinary tasks such as pharmaceutical molecule classification and chemical reaction prediction, because they can model non-euclidean relationships between different entities. Particle crushing, as a significant field of civil engineering, describes the breakage of granular materials caused by the breakage of particle fragment bonds under the modeling of numerical simulations, which motivates us to characterize the mechanical behaviors of particle crushing through the connectivity of particle fragments with Graph Neural Networks (GNNs). However, there lacks an open-source large-scale particle crushing dataset for research due to the expensive costs of laboratory tests or numerical simulations. Therefore, we firstly generate a dataset with 45,000 numerical simulations and 900 particle types to facilitate the research progress of machine learning for particle crushing. Secondly, we devise a hybrid framework based on GNNs to predict particle crushing strength in a particle fragment view with the advances of state of the art GNNs. Finally, we compare our hybrid framework against traditional machine learning methods and the plain MLP to verify its effectiveness. The usefulness of different features is further discussed through the gradient attribution explanation w.r.t the predictions. Our data and code are released at https://github.com/doujiang-zheng/GNN-For-Particle-Crushing.
</details>
<details>
<summary>摘要</summary>
Graph Neural Networks (GNNs) 已成为多学科领域中有效的机器学习工具，如药品分类和化学反应预测，因为它们可以模型不同实体之间的非欧几何关系。在 грануляр材料破碎中， particle crushing 是一个重要的领域，描述了由 particle fragment 键结的破碎物质的破碎过程，这种情况激发我们通过 GNNs 来描述破碎物质的机械行为。然而，由于实验室试验或数值仿真的高昂成本，在这个领域中没有公开的大规模 particle crushing 数据集，这限制了研究的进步。因此，我们首先生成了一个包含 45,000 个数值仿真和 900 种 particle type 的数据集，以便促进机器学习在 particle crushing 中的研究进步。其次，我们提出了基于 GNNs 的混合框架，用于预测 particle crushing 强度在 particle fragment 视图中。最后，我们与传统机器学习方法和简单的多层感知网络进行比较，以验证我们的混合框架的有效性。此外，我们还对不同特征的使用进行了探索，并通过对预测结果的梯度评估来进行解释。我们的数据和代码在 GitHub 上发布，请参考 https://github.com/doujiang-zheng/GNN-For-Particle-Crushing。
</details></li>
</ul>
<hr>
<h2 id="Robustness-Verification-of-Deep-Neural-Networks-using-Star-Based-Reachability-Analysis-with-Variable-Length-Time-Series-Input"><a href="#Robustness-Verification-of-Deep-Neural-Networks-using-Star-Based-Reachability-Analysis-with-Variable-Length-Time-Series-Input" class="headerlink" title="Robustness Verification of Deep Neural Networks using Star-Based Reachability Analysis with Variable-Length Time Series Input"></a>Robustness Verification of Deep Neural Networks using Star-Based Reachability Analysis with Variable-Length Time Series Input</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13907">http://arxiv.org/abs/2307.13907</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neelanjana Pal, Diego Manzanas Lopez, Taylor T Johnson</li>
<li>for: 这篇论文的目的是提出一种基于神经网络的时间序列数据分析方法，以便在实际应用中进行精准的预测和维护。</li>
<li>methods: 这篇论文使用了时间序列 regression neural network (TSRegNN)，并使用了变量长度输入数据来简化输入处理和提高网络架构的通用性。</li>
<li>results: 该论文通过使用星形可达性分析和一些性能指标来检验神经网络的可靠性，并证明了神经网络在真实应用中的精准预测和可靠性是受到输入噪声的影响的。<details>
<summary>Abstract</summary>
Data-driven, neural network (NN) based anomaly detection and predictive maintenance are emerging research areas. NN-based analytics of time-series data offer valuable insights into past behaviors and estimates of critical parameters like remaining useful life (RUL) of equipment and state-of-charge (SOC) of batteries. However, input time series data can be exposed to intentional or unintentional noise when passing through sensors, necessitating robust validation and verification of these NNs. This paper presents a case study of the robustness verification approach for time series regression NNs (TSRegNN) using set-based formal methods. It focuses on utilizing variable-length input data to streamline input manipulation and enhance network architecture generalizability. The method is applied to two data sets in the Prognostics and Health Management (PHM) application areas: (1) SOC estimation of a Lithium-ion battery and (2) RUL estimation of a turbine engine. The NNs' robustness is checked using star-based reachability analysis, and several performance measures evaluate the effect of bounded perturbations in the input on network outputs, i.e., future outcomes. Overall, the paper offers a comprehensive case study for validating and verifying NN-based analytics of time-series data in real-world applications, emphasizing the importance of robustness testing for accurate and reliable predictions, especially considering the impact of noise on future outcomes.
</details>
<details>
<summary>摘要</summary>
数据驱动、基于神经网络（NN）的异常检测和预测维护是当前的研究领域之一。NN基础的时间序列数据分析可以为过去行为提供有价值的洞察，包括设备的剩余有用生命（RUL）和电池的状态充电（SOC）等重要参数的估计。然而，输入时间序列数据可能会受到意外或非意外的噪声的影响，因此需要对这些NN进行robust验证和验证。本文介绍了一种基于集合形式方法的NN验证方法，旨在使用可变长度的输入数据来简化输入处理并提高网络架构的通用性。这种方法在两个PHM应用领域的数据集上进行了应用：（1）锂离子电池SOC估计和（2）涡轮机RUL估计。通过星形可达性分析来检查NN的 Robustness，并使用一些性能指标来评估输入噪声的影响 på network输出，即未来的结果。总的来说，本文提供了一个完整的实践案例，用于验证和验证基于时间序列数据的NN分析，强调验证过程中噪声的影响，以确保准确可靠的预测，特别是在噪声的情况下。
</details></li>
</ul>
<hr>
<h2 id="Corruption-Robust-Lipschitz-Contextual-Search"><a href="#Corruption-Robust-Lipschitz-Contextual-Search" class="headerlink" title="Corruption-Robust Lipschitz Contextual Search"></a>Corruption-Robust Lipschitz Contextual Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13903">http://arxiv.org/abs/2307.13903</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiliang Zuo</li>
<li>for: 学习一个 lipschitz 函数，对于随机选择的上下文向量 $x_t$ 和 adversary 选择的真实函数值 $f(x_t)$ 进行推断。</li>
<li>methods: 使用 natural yet powerful technique sanity check，并设计了 robust 算法，可以在 $C$ 轮游戏中减少总损失。</li>
<li>results: 对于均匀损失， learner 可以取得 regret $O(C\log T)$，其中 $d &#x3D; 1$ 时为 $O(C\log T)$，$d &gt; 1$ 时为 $O_d(C\log T + T^{(d-1)&#x2F;d})$。对于价格损失，learner 可以取得 regret $\widetilde{O}(T^{d&#x2F;(d+1)} + C\cdot T^{1&#x2F;(d+1)})$。<details>
<summary>Abstract</summary>
I study the problem of learning a Lipschitz function with corrupted binary signals. The learner tries to learn a Lipschitz function $f$ that the adversary chooses. In each round, the adversary selects a context vector $x_t$ in the input space, and the learner makes a guess to the true function value $f(x_t)$ and receives a binary signal indicating whether the guess was high or low. In a total of $C$ rounds, the signal may be corrupted, though the value of $C$ is unknown to the learner. The learner's goal is to incur a small cumulative loss. I present a natural yet powerful technique sanity check, which proves useful in designing corruption-robust algorithms. I design algorithms which (treating the Lipschitz parameter $L$ as constant): for the symmetric loss, the learner achieves regret $O(C\log T)$ with $d = 1$ and $O_d(C\log T + T^{(d-1)/d})$ with $d > 1$; for the pricing loss the learner achieves regret $\widetilde{O} (T^{d/(d+1)} + C\cdot T^{1/(d+1)})$.
</details>
<details>
<summary>摘要</summary>
我研究学习受损函数问题，即 adversary 选择函数 $f$，learner 根据输入空间中的 context vector $x_t$ 猜测函数值，并接受一个 binary signal 表示猜测是高或低。在总共 $C$ 轮中，signal 可能受损，但 learner 不知道 $C$ 的值。learner 的目标是减少总的损失。我提出了一种自然强大的检查技术，对于设计受损函数 algorithms 非常有用。我设计了 algorithms，对于对称损失函数，learner 可以达到 regret $O(C\log T)$  avec $d = 1$ 和 $O_d(C\log T + T^{(d-1)/d})$  avec $d > 1$ ;对于价格损失函数，learner 可以达到 regret $\widetilde{O} (T^{d/(d+1)} + C\cdot T^{1/(d+1)})$。
</details></li>
</ul>
<hr>
<h2 id="Regularizing-Neural-Networks-with-Meta-Learning-Generative-Models"><a href="#Regularizing-Neural-Networks-with-Meta-Learning-Generative-Models" class="headerlink" title="Regularizing Neural Networks with Meta-Learning Generative Models"></a>Regularizing Neural Networks with Meta-Learning Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13899">http://arxiv.org/abs/2307.13899</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shin’ya Yamaguchi, Daiki Chijiwa, Sekitoshi Kanai, Atsutoshi Kumagai, Hisashi Kashima</li>
<li>for: 提高深度学习中的生成数据增强（Generative Data Augmentation，GDA）的方法，以便在小样本大小的情况下提高分类精度。</li>
<li>methods: 提出了一种新的生成数据增强策略——元生成准则（Meta Generative Regularization，MGR），通过在特征提取器中使用生成样本来减少Validation损失，从而避免生成数据增强导致的性能下降。</li>
<li>results: 对六个预测集进行了实验，发现MGR可以避免生成数据增强导致的性能下降，并在小样本大小的情况下稳定地超越基elines。<details>
<summary>Abstract</summary>
This paper investigates methods for improving generative data augmentation for deep learning. Generative data augmentation leverages the synthetic samples produced by generative models as an additional dataset for classification with small dataset settings. A key challenge of generative data augmentation is that the synthetic data contain uninformative samples that degrade accuracy. This is because the synthetic samples do not perfectly represent class categories in real data and uniform sampling does not necessarily provide useful samples for tasks. In this paper, we present a novel strategy for generative data augmentation called meta generative regularization (MGR). To avoid the degradation of generative data augmentation, MGR utilizes synthetic samples in the regularization term for feature extractors instead of in the loss function, e.g., cross-entropy. These synthetic samples are dynamically determined to minimize the validation losses through meta-learning. We observed that MGR can avoid the performance degradation of na\"ive generative data augmentation and boost the baselines. Experiments on six datasets showed that MGR is effective particularly when datasets are smaller and stably outperforms baselines.
</details>
<details>
<summary>摘要</summary>
Instead of using synthetic samples in the loss function, such as cross-entropy, MGR incorporates them into the regularization term for feature extractors. This approach allows for the dynamic determination of synthetic samples to minimize validation losses through meta-learning. Our experiments on six datasets demonstrated that MGR can effectively avoid the performance degradation of traditional generative data augmentation and consistently outperform baselines, particularly when the datasets are smaller.
</details></li>
</ul>
<hr>
<h2 id="Efficient-Estimation-of-the-Local-Robustness-of-Machine-Learning-Models"><a href="#Efficient-Estimation-of-the-Local-Robustness-of-Machine-Learning-Models" class="headerlink" title="Efficient Estimation of the Local Robustness of Machine Learning Models"></a>Efficient Estimation of the Local Robustness of Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13885">http://arxiv.org/abs/2307.13885</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tessa Han, Suraj Srinivas, Himabindu Lakkaraju</li>
<li>for: 本研究旨在提高机器学习模型对噪声输入数据的Robustness。</li>
<li>methods: 本文提出了首个分析性Estimators来效率地计算多类推论模型的本地Robustness，通过地方线性函数近似和多变量Normal CDF，并与随机抖动、软max概率之间的关系进行链接。</li>
<li>results: 本文confirm empirically这些Estimators可以有效和高效地计算标准深度学习模型的本地Robustness，并用于多种任务，如测试模型的Robustness偏见和找到数据集中噪声扰动的例子。<details>
<summary>Abstract</summary>
Machine learning models often need to be robust to noisy input data. The effect of real-world noise (which is often random) on model predictions is captured by a model's local robustness, i.e., the consistency of model predictions in a local region around an input. However, the na\"ive approach to computing local robustness based on Monte-Carlo sampling is statistically inefficient, leading to prohibitive computational costs for large-scale applications. In this work, we develop the first analytical estimators to efficiently compute local robustness of multi-class discriminative models using local linear function approximation and the multivariate Normal CDF. Through the derivation of these estimators, we show how local robustness is connected to concepts such as randomized smoothing and softmax probability. We also confirm empirically that these estimators accurately and efficiently compute the local robustness of standard deep learning models. In addition, we demonstrate these estimators' usefulness for various tasks involving local robustness, such as measuring robustness bias and identifying examples that are vulnerable to noise perturbation in a dataset. By developing these analytical estimators, this work not only advances conceptual understanding of local robustness, but also makes its computation practical, enabling the use of local robustness in critical downstream applications.
</details>
<details>
<summary>摘要</summary>
（Machine learning模型经常需要对噪音输入数据具有鲁棒性。噪音的影响在模型预测中被捕捉在一个本地区域中，即模型在输入附近的一致性。但是，使用 Monte-Carlo 采样来计算本地鲁棒性的方法是统计不有效的，会导致大规模应用程序的计算成本过高。在这种情况下，我们开发了第一个分析式估计器，可以有效地计算多类推论模型的本地鲁棒性。我们使用本地线性函数近似和多变量正态分布函数来 derive这些估计器，并证明了本地鲁棒性与随机缓和软max概率之间的关系。我们还通过实验证明了这些估计器可以准确地和高效地计算标准深度学习模型的本地鲁棒性。此外，我们还示出了这些估计器在不同任务中的有用性，如测量鲁棒性偏见和 dataset 中噪音扰动的示例。通过开发这些分析式估计器，这些研究不仅提高了本地鲁棒性的概念理解，还使其计算变得实际可行，使其在重要的下游应用中使用。）
</details></li>
</ul>
<hr>
<h2 id="ExeDec-Execution-Decomposition-for-Compositional-Generalization-in-Neural-Program-Synthesis"><a href="#ExeDec-Execution-Decomposition-for-Compositional-Generalization-in-Neural-Program-Synthesis" class="headerlink" title="ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis"></a>ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13883">http://arxiv.org/abs/2307.13883</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kensen Shi, Joey Hong, Manzil Zaheer, Pengcheng Yin, Charles Sutton</li>
<li>for: 本研究旨在探讨人工智能程序生成方法是否具有分解复杂任务为 simpler subtask 的能力，以及这种能力是否可以推广到更复杂的任务。</li>
<li>methods: 本研究使用了多种形式的 compositional generalization，包括程序执行目标预测、程序分解和重构等，以形成一个 meta-benchmark，用于评估不同方法的总体性能。</li>
<li>results: 研究发现，使用 ExeDec  decomposition-based 程序生成策略可以更好地满足不同任务的需求，并且具有显著的总体性能和 compositional generalization 能力，比基eline方法更好。<details>
<summary>Abstract</summary>
When writing programs, people have the ability to tackle a new complex task by decomposing it into smaller and more familiar subtasks. While it is difficult to measure whether neural program synthesis methods have similar capabilities, we can measure whether they compositionally generalize, that is, whether a model that has been trained on the simpler subtasks is subsequently able to solve more complex tasks. In this paper, we characterize several different forms of compositional generalization that are desirable in program synthesis, forming a meta-benchmark which we use to create generalization tasks for two popular datasets, RobustFill and DeepCoder. We then propose ExeDec, a novel decomposition-based synthesis strategy that predicts execution subgoals to solve problems step-by-step informed by program execution at each step. ExeDec has better synthesis performance and greatly improved compositional generalization ability compared to baselines.
</details>
<details>
<summary>摘要</summary>
当编写程序时，人们有能力将复杂任务拆分成更熟悉的子任务。虽然无法测量神经程序合成方法的类似能力，但我们可以测量它们是否可以扩展，即一个已经在更简单的子任务上训练的模型是否可以解决更复杂的任务。在这篇论文中，我们描述了几种不同的拆分总结的形式，这些形式是程序合成中的总结，我们使用这些形式创建了一个元benchmark，并用这个元benchmark来创建了RobustFill和DeepCoder两个 популяр的数据集的通用化任务。然后，我们提出了ExeDec，一种新的分解基本的合成策略，该策略预测执行子任务以解决问题步骤通过程序执行的信息。ExeDec的合成性能和拆分总结能力都比基eline要好。
</details></li>
</ul>
<hr>
<h2 id="Good-Lattice-Training-Physics-Informed-Neural-Networks-Accelerated-by-Number-Theory"><a href="#Good-Lattice-Training-Physics-Informed-Neural-Networks-Accelerated-by-Number-Theory" class="headerlink" title="Good Lattice Training: Physics-Informed Neural Networks Accelerated by Number Theory"></a>Good Lattice Training: Physics-Informed Neural Networks Accelerated by Number Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13869">http://arxiv.org/abs/2307.13869</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takashi Matsubara, Takaharu Yaguchi</li>
<li>for: 解决 partial differential equations (PDEs) 的novel和高效的方法</li>
<li>methods: 使用 physics-informed loss 训练神经网络，并选择合适的 collocation points</li>
<li>results: 提出 good lattice training (GLT) 技术，可以在小量的 collocation points 下达到竞争力的性能，并且比uniformly random sampling或Latin hypercube sampling 更有效率<details>
<summary>Abstract</summary>
Physics-informed neural networks (PINNs) offer a novel and efficient approach to solving partial differential equations (PDEs). Their success lies in the physics-informed loss, which trains a neural network to satisfy a given PDE at specific points and to approximate the solution. However, the solutions to PDEs are inherently infinite-dimensional, and the distance between the output and the solution is defined by an integral over the domain. Therefore, the physics-informed loss only provides a finite approximation, and selecting appropriate collocation points becomes crucial to suppress the discretization errors, although this aspect has often been overlooked. In this paper, we propose a new technique called good lattice training (GLT) for PINNs, inspired by number theoretic methods for numerical analysis. GLT offers a set of collocation points that are effective even with a small number of points and for multi-dimensional spaces. Our experiments demonstrate that GLT requires 2--20 times fewer collocation points (resulting in lower computational cost) than uniformly random sampling or Latin hypercube sampling, while achieving competitive performance.
</details>
<details>
<summary>摘要</summary>
物理学 Informed neural networks (PINNs) 提供了一种新的和高效的方法来解决部分 differential equations (PDEs)。它们的成功归功于物理学 Informed loss，该loss 训练一个神经网络满足给定 PDE 的特定点和近似解。然而，解决 PDE 的解是自然 infinite-dimensional，而且Distance  между输出和解是通过Domain 上的积分来定义的。因此，物理学 Informed loss 只提供了finite approximation，并且选择合适的拓扑点变得非常重要，以抑制精度损失，尽管这一点经常被忽略。在这篇论文中，我们提出了一种新的技术called good lattice training (GLT) for PINNs， inspirited by numerical analysis 的数学方法。GLT 提供了一组高效的拓扑点，可以在小量点和多维空间中实现高效。我们的实验表明，GLT 需要2--20倍 fewer collocation points（相对于随机抽样或拉丁hypercube sampling），而且可以达到竞争性的性能。
</details></li>
</ul>
<hr>
<h2 id="Learning-sources-of-variability-from-high-dimensional-observational-studies"><a href="#Learning-sources-of-variability-from-high-dimensional-observational-studies" class="headerlink" title="Learning sources of variability from high-dimensional observational studies"></a>Learning sources of variability from high-dimensional observational studies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13868">http://arxiv.org/abs/2307.13868</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ebridge2/cdcorr">https://github.com/ebridge2/cdcorr</a></li>
<li>paper_authors: Eric W. Bridgeford, Jaewon Chung, Brian Gilbert, Sambit Panda, Adam Li, Cencheng Shen, Alexandra Badea, Brian Caffo, Joshua T. Vogelstein</li>
<li>for: 该研究探讨了 causal inference 是否会影响观察到的结果，以及这种方法在各种生物领域中的应用，包括疫苗和药物开发、政策干预等。</li>
<li>methods: 该研究扩展了 causal estimands 到多维或任意可测量空间上的结果，并将 nominal 变量的 causal estimands 转化为 causal discrepancy tests。提出了一种简单的方法来调整 Conditional Independence tests，并证明这些测试是 universally consistent causal discrepancy tests。</li>
<li>results: 数据实验表明，该方法（Causal CDcorr）在样本数量和权限下的效果和力度都有所提高，相比之下存在的其他策略。该方法的代码都是开源的，可以在 github.com&#x2F;ebridge2&#x2F;cdcorr 上下载。<details>
<summary>Abstract</summary>
Causal inference studies whether the presence of a variable influences an observed outcome. As measured by quantities such as the "average treatment effect," this paradigm is employed across numerous biological fields, from vaccine and drug development to policy interventions. Unfortunately, the majority of these methods are often limited to univariate outcomes. Our work generalizes causal estimands to outcomes with any number of dimensions or any measurable space, and formulates traditional causal estimands for nominal variables as causal discrepancy tests. We propose a simple technique for adjusting universally consistent conditional independence tests and prove that these tests are universally consistent causal discrepancy tests. Numerical experiments illustrate that our method, Causal CDcorr, leads to improvements in both finite sample validity and power when compared to existing strategies. Our methods are all open source and available at github.com/ebridge2/cdcorr.
</details>
<details>
<summary>摘要</summary>
causal inference studies whether a variable's presence affects an observed outcome. as measured by quantities such as the "average treatment effect," this paradigm is employed across numerous biological fields, from vaccine and drug development to policy interventions. unfortunately, the majority of these methods are often limited to univariate outcomes. our work generalizes causal estimands to outcomes with any number of dimensions or any measurable space, and formulates traditional causal estimands for nominal variables as causal discrepancy tests. we propose a simple technique for adjusting universally consistent conditional independence tests and prove that these tests are universally consistent causal discrepancy tests. numerical experiments illustrate that our method, causal CDcorr, leads to improvements in both finite sample validity and power when compared to existing strategies. our methods are all open source and available at github.com/ebridge2/cdcorr.Here's the translation in Traditional Chinese:causal inference studies whether a variable's presence affects an observed outcome. as measured by quantities such as the "average treatment effect," this paradigm is employed across numerous biological fields, from vaccine and drug development to policy interventions. unfortunately, the majority of these methods are often limited to univariate outcomes. our work generalizes causal estimands to outcomes with any number of dimensions or any measurable space, and formulates traditional causal estimands for nominal variables as causal discrepancy tests. we propose a simple technique for adjusting universally consistent conditional independence tests and prove that these tests are universally consistent causal discrepancy tests. numerical experiments illustrate that our method, causal CDcorr, leads to improvements in both finite sample validity and power when compared to existing strategies. our methods are all open source and available at github.com/ebridge2/cdcorr.
</details></li>
</ul>
<hr>
<h2 id="Pretrained-Deep-2-5D-Models-for-Efficient-Predictive-Modeling-from-Retinal-OCT"><a href="#Pretrained-Deep-2-5D-Models-for-Efficient-Predictive-Modeling-from-Retinal-OCT" class="headerlink" title="Pretrained Deep 2.5D Models for Efficient Predictive Modeling from Retinal OCT"></a>Pretrained Deep 2.5D Models for Efficient Predictive Modeling from Retinal OCT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13865">http://arxiv.org/abs/2307.13865</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taha Emre, Marzieh Oghbaie, Arunava Chakravarty, Antoine Rivail, Sophie Riedl, Julia Mai, Hendrik P. N. Scholl, Sobha Sivaprasad, Daniel Rueckert, Andrew Lotery, Ursula Schmidt-Erfurth, Hrvoje Bogunović</li>
<li>for: 预测老年眼肿症（AMD）进展，提高医疗影像识别的精度和效率。</li>
<li>methods: 组合2D和3D技术，使用卷积神经网络（CNN）、长短期记忆（LSTM）和变换器，提高性能和数据效率。</li>
<li>results: 在两个大 longitudinal OCT 数据集上，验证了这些架构和预训练方法的效果，可以准确预测在6个月内进展到湿性年轻眼肿症（AMD）。<details>
<summary>Abstract</summary>
In the field of medical imaging, 3D deep learning models play a crucial role in building powerful predictive models of disease progression. However, the size of these models presents significant challenges, both in terms of computational resources and data requirements. Moreover, achieving high-quality pretraining of 3D models proves to be even more challenging. To address these issues, hybrid 2.5D approaches provide an effective solution for utilizing 3D volumetric data efficiently using 2D models. Combining 2D and 3D techniques offers a promising avenue for optimizing performance while minimizing memory requirements. In this paper, we explore 2.5D architectures based on a combination of convolutional neural networks (CNNs), long short-term memory (LSTM), and Transformers. In addition, leveraging the benefits of recent non-contrastive pretraining approaches in 2D, we enhanced the performance and data efficiency of 2.5D techniques even further. We demonstrate the effectiveness of architectures and associated pretraining on a task of predicting progression to wet age-related macular degeneration (AMD) within a six-month period on two large longitudinal OCT datasets.
</details>
<details>
<summary>摘要</summary>
医疗影像领域中，3D深度学习模型在建立疾病发展预测模型方面扮演着关键角色。然而，这些模型的大小带来了计算资源和数据需求的挑战。此外，实现高质量预训练3D模型也是非常困难的。为解决这些问题，混合2.5D方法提供了一种高效地利用3D栅格数据的方法。将2D和3D技术结合起来，可以提高性能的同时减少内存需求。在这篇论文中，我们探讨了基于卷积神经网络（CNN）、长期短记忆（LSTM）和变换器的2.5D架构。此外，利用2D非对抗预训练方法的优点，我们进一步提高了2.5D技术的性能和数据效率。我们在两个大 longitudinal OCT数据集上预测了在6个月内进行湿性macular degeneration（AMD）的发展预测任务，以证明架构和预训练的效果。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Design-Analog-Circuits-to-Meet-Threshold-Specifications"><a href="#Learning-to-Design-Analog-Circuits-to-Meet-Threshold-Specifications" class="headerlink" title="Learning to Design Analog Circuits to Meet Threshold Specifications"></a>Learning to Design Analog Circuits to Meet Threshold Specifications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13861">http://arxiv.org/abs/2307.13861</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/indylab/circuit-synthesis">https://github.com/indylab/circuit-synthesis</a></li>
<li>paper_authors: Dmitrii Krylov, Pooya Khajeh, Junhan Ouyang, Thomas Reeves, Tongkai Liu, Hiba Ajmal, Hamidreza Aghasi, Roy Fox</li>
<li>for: 本研究旨在提出一种基于 simulation 数据的自动化Analog和广播频率电路设计方法，以替代专业设计师的手动设计。</li>
<li>methods: 该方法通过学习 inverse function 从 desired performance metrics 中学习出 circuit parameters。</li>
<li>results: 该方法可以在5% error margin 下达到90%的成功率，并且可以提高数据使用效率。In English:</li>
<li>for: The paper proposes an automated design method for analog and radio-frequency circuits using supervised or reinforcement learning from simulation data, as an alternative to manual expert design.</li>
<li>methods: The method learns an inverse function from desired performance metrics to circuit parameters.</li>
<li>results: The method achieves a success rate of over 90% with an error margin of 5%, and improves data efficiency by up to an order of magnitude.<details>
<summary>Abstract</summary>
Automated design of analog and radio-frequency circuits using supervised or reinforcement learning from simulation data has recently been studied as an alternative to manual expert design. It is straightforward for a design agent to learn an inverse function from desired performance metrics to circuit parameters. However, it is more common for a user to have threshold performance criteria rather than an exact target vector of feasible performance measures. In this work, we propose a method for generating from simulation data a dataset on which a system can be trained via supervised learning to design circuits to meet threshold specifications. We moreover perform the to-date most extensive evaluation of automated analog circuit design, including experimenting in a significantly more diverse set of circuits than in prior work, covering linear, nonlinear, and autonomous circuit configurations, and show that our method consistently reaches success rate better than 90% at 5% error margin, while also improving data efficiency by upward of an order of magnitude. A demo of this system is available at circuits.streamlit.app
</details>
<details>
<summary>摘要</summary>
自动设计分析和频率电路使用监督或增强学习法，从实验数据学习，对于专家设计来说是一种新的替代方案。它容易 для设计代理人从需求性能指标学习逆函数。但是，用户更常会有阈值性能标准而不是精确的可行性表现标准。在这个工作中，我们提出了将从实验数据生成一个可以透过监督学习训练系统，以满足阈值需求的数据集。我们还进行了过去最大的自动分析电路设计评估，包括在更加多样化的电路配置中实验，包括线性、非线性和自主电路配置，并证明了我们的方法可以在5% error margin下，实现90%的成功率，同时也提高了数据效率，提高了一个阶层。一个demo这个系统可以在circuits.streamlit.app中找到。
</details></li>
</ul>
<hr>
<h2 id="On-the-unreasonable-vulnerability-of-transformers-for-image-restoration-–-and-an-easy-fix"><a href="#On-the-unreasonable-vulnerability-of-transformers-for-image-restoration-–-and-an-easy-fix" class="headerlink" title="On the unreasonable vulnerability of transformers for image restoration – and an easy fix"></a>On the unreasonable vulnerability of transformers for image restoration – and an easy fix</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13856">http://arxiv.org/abs/2307.13856</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shashank Agnihotri, Kanchana Vaishnavi Gandikota, Julia Grabinski, Paramanand Chandramouli, Margret Keuper</li>
<li>for: 这种研究探讨了使用视Transformers（ViTs）进行图像修复任务中的图像修复模型的Robustness。</li>
<li>methods: 我们使用Projected Gradient Descent（PGD）和CosPGD，一种特定于像素预测任务的敏感攻击，来评估这些模型的Robustness。</li>
<li>results: 我们发现，与前期研究所 advocated的相反，这些模型在敏感攻击下高度易受攻击。我们通过对这些模型进行Robustness Training来提高其Robustness，但结果不太乐见。另外，NAFNet和Baseline网络的设计选择，基于iid性性能而不是Robust generalization，似乎与模型的Robustness相抵触。<details>
<summary>Abstract</summary>
Following their success in visual recognition tasks, Vision Transformers(ViTs) are being increasingly employed for image restoration. As a few recent works claim that ViTs for image classification also have better robustness properties, we investigate whether the improved adversarial robustness of ViTs extends to image restoration. We consider the recently proposed Restormer model, as well as NAFNet and the "Baseline network" which are both simplified versions of a Restormer. We use Projected Gradient Descent (PGD) and CosPGD, a recently proposed adversarial attack tailored to pixel-wise prediction tasks for our robustness evaluation. Our experiments are performed on real-world images from the GoPro dataset for image deblurring. Our analysis indicates that contrary to as advocated by ViTs in image classification works, these models are highly susceptible to adversarial attacks. We attempt to improve their robustness through adversarial training. While this yields a significant increase in robustness for Restormer, results on other networks are less promising. Interestingly, the design choices in NAFNet and Baselines, which were based on iid performance, and not on robust generalization, seem to be at odds with the model robustness. Thus, we investigate this further and find a fix.
</details>
<details>
<summary>摘要</summary>
“随着当前视觉变数任务的成功，视觉对映器（ViT）正在不断地被应用于图像修复。一些最近的研究表明，ViT在图像分类任务中也有更好的韧性特性，我们进一步探索这些韧性特性是否扩展到图像修复。我们考虑了RecentRestormer模型，以及NAFNet和基eline网络，这些都是Restormer的简化版本。我们使用预测向量的投影Gradient Descent（PGD）和CosPGD，这是一种特别针对像素精度预测任务的攻击方法来进行我们的Robustness评估。我们对GoPro图像滤过 dataset上的实际图像进行了实验。我们的分析表明，与在图像分类任务中所说的相反，这些模型在这些任务中具有强大的攻击敏感性。我们尝试通过对模型进行防御训练来改善其 Robustness。与此同时，我们发现NAFNet和基eline网络的设计决策，它们是基于独立性表现而不是关于韧性的设计决策，似乎与模型的韧性不匹配。因此，我们进一步探索这个问题，并发现一个解决方案。”
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Sharpened-Cosine-Similarity"><a href="#Exploring-the-Sharpened-Cosine-Similarity" class="headerlink" title="Exploring the Sharpened Cosine Similarity"></a>Exploring the Sharpened Cosine Similarity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13855">http://arxiv.org/abs/2307.13855</a></li>
<li>repo_url: None</li>
<li>paper_authors: Skyler Wu, Fred Lu, Edward Raff, James Holt</li>
<li>for: 本研究探讨了使用新的激活函数Sharpened Cosine Similarity（SCS） instead of传统的卷积层来进行图像分类。</li>
<li>methods: 本研究使用了多种 CNN 架构，并对 CIFAR-10 数据集进行了大规模的实验分析。</li>
<li>results: 研究发现，使用 SCS 可能不会提高准确率，但可能学习更易于理解的表示。此外，在某些情况下，SCS 可能会提高鲁棒性。<details>
<summary>Abstract</summary>
Convolutional layers have long served as the primary workhorse for image classification. Recently, an alternative to convolution was proposed using the Sharpened Cosine Similarity (SCS), which in theory may serve as a better feature detector. While multiple sources report promising results, there has not been to date a full-scale empirical analysis of neural network performance using these new layers. In our work, we explore SCS's parameter behavior and potential as a drop-in replacement for convolutions in multiple CNN architectures benchmarked on CIFAR-10. We find that while SCS may not yield significant increases in accuracy, it may learn more interpretable representations. We also find that, in some circumstances, SCS may confer a slight increase in adversarial robustness.
</details>
<details>
<summary>摘要</summary>
卷积层长期以来为图像分类任务中的主力工具。最近，一种使用加强的余弦相似性（SCS）来代替卷积的方案被提出，据说可能更好地检测特征。虽然多种来源报道了这些新层的批处结果，但到目前为止没有进行了全面的实验分析。在我们的工作中，我们探索SCS的参数行为和作为卷积Drop-in替换的潜在可能性，并在CIFAR-10上对多种CNN Architecture进行了多种测试。我们发现，虽然SCS可能不会导致显著增加准确率，但它可能学习更易于理解的表示。此外，在某些情况下，SCS可能会增加一定的逆向抗性。
</details></li>
</ul>
<hr>
<h2 id="WebArena-A-Realistic-Web-Environment-for-Building-Autonomous-Agents"><a href="#WebArena-A-Realistic-Web-Environment-for-Building-Autonomous-Agents" class="headerlink" title="WebArena: A Realistic Web Environment for Building Autonomous Agents"></a>WebArena: A Realistic Web Environment for Building Autonomous Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13854">http://arxiv.org/abs/2307.13854</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/web-arena-x/webarena">https://github.com/web-arena-x/webarena</a></li>
<li>paper_authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig</li>
<li>for: 这种研究旨在创建一个高度实际和可重现的自动化代理控制环境，以便用自然语言命令管理日常任务。</li>
<li>methods: 这篇论文使用了现代自然语言处理技术，如理智语言模型（GPT-4），以及一些最新的解释和行为决策技术。</li>
<li>results: 研究发现，使用现有的状态elia-of-the-art语言模型（GPT-4）解决复杂任务时存在挑战，最高终端任务成功率只有10.59%。这些结果表明需要进一步发展更加可靠的自动化代理，并且现有的语言模型在这些实际任务中的表现并不理想。<details>
<summary>Abstract</summary>
With generative AI advances, the exciting potential for autonomous agents to manage daily tasks via natural language commands has emerged. However, cur rent agents are primarily created and tested in simplified synthetic environments, substantially limiting real-world scenario representation. In this paper, we build an environment for agent command and control that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on websites, and we create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and are designed to emulate tasks that humans routinely perform on the internet. We design and implement several autonomous agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 10.59%. These results highlight the need for further development of robust agents, that current state-of-the-art LMs are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress. Our code, data, environment reproduction resources, and video demonstrations are publicly available at https://webarena.dev/.
</details>
<details>
<summary>摘要</summary>
“受到生成AI的推进，自动化代理人可以通过自然语言指令进行日常任务管理，这具有吸引人的潜力。然而，目前的代理人主要是在简化的人工环境中设计和测试，实际上仅仅代表了实际世界的一部分。在这篇论文中，我们建立了一个高度现实和可重现的环境，以便自动化代理人进行命令和控制。 Specifically，我们专注在网站上进行任务的代理人，并创建了四种常见的领域中的完整网站：电子商务、社群讨论论坛、协同软件开发和内容管理。我们的环境扩展了工具（例如地图）和外部知识库（例如用户手册），以促进人类化的任务解决。在这基础之上，我们发布了一组对任务完成的评估标准，这些任务具有多样性、长期性和模拟人类在网页上进行常规任务的特点。我们设计和实现了一些自动化代理人，应用最新的技术，例如理解才行。结果显示，解决复杂任务是具有挑战性：我们的最佳GPT-4基于代理人仅取得了10.59%的终端任务成功率。这些结果显示现代LM的表现仍有很大的改善空间，并且WebArena可以用来衡量这种进步。我们的代码、数据、环境重现资源和视频示例都公开 disponíveis于https://webarena.dev/.”
</details></li>
</ul>
<hr>
<h2 id="SplitFed-resilience-to-packet-loss-Where-to-split-that-is-the-question"><a href="#SplitFed-resilience-to-packet-loss-Where-to-split-that-is-the-question" class="headerlink" title="SplitFed resilience to packet loss: Where to split, that is the question"></a>SplitFed resilience to packet loss: Where to split, that is the question</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13851">http://arxiv.org/abs/2307.13851</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chamani Shiranthika, Zahra Hafezi Kafshgari, Parvaneh Saeedi, Ivan V. Bajić</li>
<li>for: 这篇论文探讨了Split Federated Learning（SplitFed或SFL）在面对包列失败的情况下的稳定性。</li>
<li>methods: 这篇论文使用了将模型分割在两个点上（浅分割和深分割），然后测试这些分割点对模型的准确率是否有 statistically significant difference。</li>
<li>results: 实验结果表明，使用深分割点可以获得更高的准确率。<details>
<summary>Abstract</summary>
Decentralized machine learning has broadened its scope recently with the invention of Federated Learning (FL), Split Learning (SL), and their hybrids like Split Federated Learning (SplitFed or SFL). The goal of SFL is to reduce the computational power required by each client in FL and parallelize SL while maintaining privacy. This paper investigates the robustness of SFL against packet loss on communication links. The performance of various SFL aggregation strategies is examined by splitting the model at two points -- shallow split and deep split -- and testing whether the split point makes a statistically significant difference to the accuracy of the final model. Experiments are carried out on a segmentation model for human embryo images and indicate the statistically significant advantage of a deeper split point.
</details>
<details>
<summary>摘要</summary>
《协同学习的扩展：从分布式学习到分布式 Federated Learning》Recently, decentralized machine learning has expanded its scope with the invention of Federated Learning (FL), Split Learning (SL), and their hybrids like Split Federated Learning (SplitFed or SFL). The goal of SFL is to reduce the computational power required by each client in FL and parallelize SL while maintaining privacy. This paper investigates the robustness of SFL against packet loss on communication links. The performance of various SFL aggregation strategies is examined by splitting the model at two points -- shallow split and deep split -- and testing whether the split point makes a statistically significant difference to the accuracy of the final model. Experiments are carried out on a segmentation model for human embryo images and indicate the statistically significant advantage of a deeper split point.
</details></li>
</ul>
<hr>
<h2 id="MAEA-Multimodal-Attribution-for-Embodied-AI"><a href="#MAEA-Multimodal-Attribution-for-Embodied-AI" class="headerlink" title="MAEA: Multimodal Attribution for Embodied AI"></a>MAEA: Multimodal Attribution for Embodied AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13850">http://arxiv.org/abs/2307.13850</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vidhi Jain, Jayant Sravan Tamarapalli, Sahiti Yerramilli, Yonatan Bisk<br>for: 这个论文的目的是解决embodied AI中的多modal感知问题，因为输入信息可能包含高度相互补充的信息。methods: 这篇论文使用了解释ALFRED数据集上不同策略中每种modal输入的贡献分析，以便理解每种modal输入的全局趋势。results: 该研究发现了一种名为MAEA的框架，可以计算任何可微分策略的全局贡献分析。此外，研究还显示了在EAI策略中语言和视觉贡献的下一个行为分析。<details>
<summary>Abstract</summary>
Understanding multimodal perception for embodied AI is an open question because such inputs may contain highly complementary as well as redundant information for the task. A relevant direction for multimodal policies is understanding the global trends of each modality at the fusion layer. To this end, we disentangle the attributions for visual, language, and previous action inputs across different policies trained on the ALFRED dataset. Attribution analysis can be utilized to rank and group the failure scenarios, investigate modeling and dataset biases, and critically analyze multimodal EAI policies for robustness and user trust before deployment. We present MAEA, a framework to compute global attributions per modality of any differentiable policy. In addition, we show how attributions enable lower-level behavior analysis in EAI policies for language and visual attributions.
</details>
<details>
<summary>摘要</summary>
（文本翻译）理解多模态识别对固有AI是一个开放的问题，因为输入可能包含高度相互补做的信息。一个有关的方向是理解不同模态的全球趋势在融合层。为此，我们分离不同策略在ALFRED数据集上训练的视觉、语言和前一个动作输入的归因。归因分析可以用来排序和分组失败场景，调查模型和数据集偏见，并对多模态EAI策略进行robustness和用户信任的检验。我们提出了MAEA框架，用于计算任何可导策略的全球归因。此外，我们还示出了归因如何帮助分析EAI策略的低级行为。
</details></li>
</ul>
<hr>
<h2 id="Relationship-between-Batch-Size-and-Number-of-Steps-Needed-for-Nonconvex-Optimization-of-Stochastic-Gradient-Descent-using-Armijo-Line-Search"><a href="#Relationship-between-Batch-Size-and-Number-of-Steps-Needed-for-Nonconvex-Optimization-of-Stochastic-Gradient-Descent-using-Armijo-Line-Search" class="headerlink" title="Relationship between Batch Size and Number of Steps Needed for Nonconvex Optimization of Stochastic Gradient Descent using Armijo Line Search"></a>Relationship between Batch Size and Number of Steps Needed for Nonconvex Optimization of Stochastic Gradient Descent using Armijo Line Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13831">http://arxiv.org/abs/2307.13831</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuki Tsukada, Hideaki Iiduka</li>
<li>for: 本研究探讨了使用Stochastic gradient descent（SGD）训练深度学习模型时，学习率是如何选择的。</li>
<li>methods: 本研究使用了Armijo线earch方法来选择学习率，并进行了非 convex 优化的收敛分析。</li>
<li>results: 研究发现，当批处理大小增加时，SGD 的训练步数逐渐减少，并且存在一个最优批处理大小，可以最小化 Stochastic first-order oracle（SFO）复杂度。同时， numerics 支持了这些理论结论。<details>
<summary>Abstract</summary>
Stochastic gradient descent (SGD) is the simplest deep learning optimizer with which to train deep neural networks. While SGD can use various learning rates, such as constant or diminishing rates, the previous numerical results showed that SGD performs better than other deep learning optimizers using when it uses learning rates given by line search methods. In this paper, we perform a convergence analysis on SGD with a learning rate given by an Armijo line search for nonconvex optimization. The analysis indicates that the upper bound of the expectation of the squared norm of the full gradient becomes small when the number of steps and the batch size are large. Next, we show that, for SGD with the Armijo-line-search learning rate, the number of steps needed for nonconvex optimization is a monotone decreasing convex function of the batch size; that is, the number of steps needed for nonconvex optimization decreases as the batch size increases. Furthermore, we show that the stochastic first-order oracle (SFO) complexity, which is the stochastic gradient computation cost, is a convex function of the batch size; that is, there exists a critical batch size that minimizes the SFO complexity. Finally, we provide numerical results that support our theoretical results. The numerical results indicate that the number of steps needed for training deep neural networks decreases as the batch size increases and that there exist the critical batch sizes that can be estimated from the theoretical results.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Offline-Reinforcement-Learning-with-On-Policy-Q-Function-Regularization"><a href="#Offline-Reinforcement-Learning-with-On-Policy-Q-Function-Regularization" class="headerlink" title="Offline Reinforcement Learning with On-Policy Q-Function Regularization"></a>Offline Reinforcement Learning with On-Policy Q-Function Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13824">http://arxiv.org/abs/2307.13824</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laixi Shi, Robert Dadashi, Yuejie Chi, Pablo Samuel Castro, Matthieu Geist</li>
<li>for: 本研究的目的是解决offline reinforcement learning中的扩展错误问题，即history dataset和期望策略之间的分布shift问题。</li>
<li>methods: 本研究使用Q函数regularization来解决扩展错误问题，而不是直接regularizing towards behavior policy。</li>
<li>results: 两个提议算法在D4RLbenchmark上表现出色，展示了强的性能。<details>
<summary>Abstract</summary>
The core challenge of offline reinforcement learning (RL) is dealing with the (potentially catastrophic) extrapolation error induced by the distribution shift between the history dataset and the desired policy. A large portion of prior work tackles this challenge by implicitly/explicitly regularizing the learning policy towards the behavior policy, which is hard to estimate reliably in practice. In this work, we propose to regularize towards the Q-function of the behavior policy instead of the behavior policy itself, under the premise that the Q-function can be estimated more reliably and easily by a SARSA-style estimate and handles the extrapolation error more straightforwardly. We propose two algorithms taking advantage of the estimated Q-function through regularizations, and demonstrate they exhibit strong performance on the D4RL benchmarks.
</details>
<details>
<summary>摘要</summary>
核心挑战是线上强化学习（RL）是处理由历史数据集和需要的策略之间的分布转移所引起的（可能Catastrophic）推理错误的。大多数先前的工作是通过直接/间接地规范学习策略向行为策略进行补做，这在实践中很难估算。在这种工作中，我们建议将规范向行为策略的Q函数进行补做，因为Q函数可以更加可靠地和容易地通过SARSA样式的估计来估计，并且更直观地处理推理错误。我们提出了两种利用估计Q函数的算法，并在D4RL标准各项目上展示它们的强大表现。
</details></li>
</ul>
<hr>
<h2 id="Fitting-Auditory-Filterbanks-with-Multiresolution-Neural-Networks"><a href="#Fitting-Auditory-Filterbanks-with-Multiresolution-Neural-Networks" class="headerlink" title="Fitting Auditory Filterbanks with Multiresolution Neural Networks"></a>Fitting Auditory Filterbanks with Multiresolution Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13821">http://arxiv.org/abs/2307.13821</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lostanlen/lostanlen2023waspaa">https://github.com/lostanlen/lostanlen2023waspaa</a></li>
<li>paper_authors: Vincent Lostanlen, Daniel Haider, Han Han, Mathieu Lagrange, Peter Balazs, Martin Ehler</li>
<li>for: 用于音频识别和分类 tasks</li>
<li>methods: 使用 multiresolution neural network (MuReNN)，具有分解的卷积操作符，以及知识填充（knowledge distillation，KD）技术</li>
<li>results: 与 state-of-the-art 比较，MuReNN 在 hold-out 集合上的好处 fit 和 Heisenberg 时域本地化方面达到了最佳性能。<details>
<summary>Abstract</summary>
Waveform-based deep learning faces a dilemma between nonparametric and parametric approaches. On one hand, convolutional neural networks (convnets) may approximate any linear time-invariant system; yet, in practice, their frequency responses become more irregular as their receptive fields grow. On the other hand, a parametric model such as LEAF is guaranteed to yield Gabor filters, hence an optimal time-frequency localization; yet, this strong inductive bias comes at the detriment of representational capacity. In this paper, we aim to overcome this dilemma by introducing a neural audio model, named multiresolution neural network (MuReNN). The key idea behind MuReNN is to train separate convolutional operators over the octave subbands of a discrete wavelet transform (DWT). Since the scale of DWT atoms grows exponentially between octaves, the receptive fields of the subsequent learnable convolutions in MuReNN are dilated accordingly. For a given real-world dataset, we fit the magnitude response of MuReNN to that of a well-established auditory filterbank: Gammatone for speech, CQT for music, and third-octave for urban sounds, respectively. This is a form of knowledge distillation (KD), in which the filterbank ''teacher'' is engineered by domain knowledge while the neural network ''student'' is optimized from data. We compare MuReNN to the state of the art in terms of goodness of fit after KD on a hold-out set and in terms of Heisenberg time-frequency localization. Compared to convnets and Gabor convolutions, we find that MuReNN reaches state-of-the-art performance on all three optimization problems.
</details>
<details>
<summary>摘要</summary>
文本形式的深度学习面临着非 Parametric 和 Parametric approaches 之间的矛盾。一方面，卷积神经网络（convnets）可以近似任何线性时间不变系统；然而，在实践中，它们的频谱响应会随着它们的触发区域增大而变得更加不规则。另一方面，一个 Parametric 模型如 LEAF 可以确保生成 Gabor 滤波器，因此获得最佳时间频域准确性；然而，这样强大的推导缺陷会导致表达能力受限。在这篇论文中，我们想要超越这个矛盾，我们提出了一种神经音频模型，即多尺度神经网络（MuReNN）。MuReNN 的关键思想是在 Octave 子域上分别训练独立的卷积操作。由于 DWT 原子的尺度在 Octave 上呈指数增长，MuReNN 中的后续学习可以通过扩展权重来进行扩展。对于一个真实世界数据集，我们将 MuReNN 的质量响应与一个已知的听觉滤波器 banks：Gammatone  для语音、CQT  для音乐和第三 Octave  для城市声音，分别进行适应。这是一种知识储存（KD），在哪里听觉滤波器 ''教师'' 是通过领域知识设计的，而神经网络 ''学生'' 是通过数据优化的。我们将 MuReNN 与现状最佳的方法进行比较，包括在 KD 中的准确性评价和 Heisenberg 时间频域本地化评价。相比 ConvNets 和 Gabor 卷积，我们发现 MuReNN 在三个优化问题上都达到了状态机器的性能。
</details></li>
</ul>
<hr>
<h2 id="Gradient-Based-Spectral-Embeddings-of-Random-Dot-Product-Graphs"><a href="#Gradient-Based-Spectral-Embeddings-of-Random-Dot-Product-Graphs" class="headerlink" title="Gradient-Based Spectral Embeddings of Random Dot Product Graphs"></a>Gradient-Based Spectral Embeddings of Random Dot Product Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13818">http://arxiv.org/abs/2307.13818</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/marfiori/efficient-ase">https://github.com/marfiori/efficient-ase</a></li>
<li>paper_authors: Marcelo Fiori, Bernardo Marenco, Federico Larroca, Paola Bermolen, Gonzalo Mateos</li>
<li>for: 这个论文的目的是提出一种基于非对映准则的图像学习框架，以解决图像 embeddings 问题。</li>
<li>methods: 论文使用了非对映准则优化方法，包括首页梯度下降法和梯度搅拌法，来解决图像 embeddings 问题。</li>
<li>results: 实验结果表明，该方法可以更好地解决图像 embeddings 问题，并且可以更好地处理流动图像和缺失边数据。<details>
<summary>Abstract</summary>
The Random Dot Product Graph (RDPG) is a generative model for relational data, where nodes are represented via latent vectors in low-dimensional Euclidean space. RDPGs crucially postulate that edge formation probabilities are given by the dot product of the corresponding latent positions. Accordingly, the embedding task of estimating these vectors from an observed graph is typically posed as a low-rank matrix factorization problem. The workhorse Adjacency Spectral Embedding (ASE) enjoys solid statistical properties, but it is formally solving a surrogate problem and can be computationally intensive. In this paper, we bring to bear recent advances in non-convex optimization and demonstrate their impact to RDPG inference. We advocate first-order gradient descent methods to better solve the embedding problem, and to organically accommodate broader network embedding applications of practical relevance. Notably, we argue that RDPG embeddings of directed graphs loose interpretability unless the factor matrices are constrained to have orthogonal columns. We thus develop a novel feasible optimization method in the resulting manifold. The effectiveness of the graph representation learning framework is demonstrated on reproducible experiments with both synthetic and real network data. Our open-source algorithm implementations are scalable, and unlike the ASE they are robust to missing edge data and can track slowly-varying latent positions from streaming graphs.
</details>
<details>
<summary>摘要</summary>
Random Dot Product Graph（RDPG）是一种生成模型，用于关系数据，其中节点被表示为低维欧几何空间中的latent vector。 RDPG假设边的形成概率为latent vector的点积。因此，从观察到的图像进行嵌入的任务通常是一个低维矩阵分解问题。ASE是工作马力，但它是一个代理问题，可能 computationally intensive。在这篇论文中，我们利用了最近的非极体优化技术，并证明它们对RDPG推理有益。我们建议使用首项梯度下降法来更好地解决嵌入问题，并能够自然地涵盖更广泛的网络嵌入应用。另外，我们 argue that RDPG嵌入导向图 Unless the factor matrices are constrained to have orthogonal columns，因此我们开发了一种新的可行优化方法。我们的图表示学术框架在 reproduceable experiments中得到了证明，并且我们的开源算法实现可扩展，不同于ASE，它们可以承受缺失边数据和从流式图进行逐步嵌入。
</details></li>
</ul>
<hr>
<h2 id="How-to-Scale-Your-EMA"><a href="#How-to-Scale-Your-EMA" class="headerlink" title="How to Scale Your EMA"></a>How to Scale Your EMA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13813">http://arxiv.org/abs/2307.13813</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ZulqarnainZilli/-9-Email-Marketing-Tips-For-Content-Marketers">https://github.com/ZulqarnainZilli/-9-Email-Marketing-Tips-For-Content-Marketers</a></li>
<li>paper_authors: Dan Busbridge, Jason Ramapuram, Pierre Ablin, Tatiana Likhomanenko, Eeshan Gunesh Dhekane, Xavier Suau, Russ Webb</li>
<li>for: 本文旨在探讨如何在批处理大小不同时保持训练动力的问题。</li>
<li>methods: 本文提出了一种优化策略，即在EMA模型的存在下，采用线性增量法来调整学习率，以实现在批处理大小不同时保持训练动力。</li>
<li>results: 本文通过多种 arquitectures、优化器和数据模式的实验，证明了该优化策略的有效性。 另外，本文还示出了EMA模型在目标模型优化中的作用，并在小批处理和大批处理下实现了SSL方法的训练。 特别是，在BYOL方法中，通过适当调整学习率，在批处理大小为24576时实现了6倍的wall-clock时间减少。<details>
<summary>Abstract</summary>
Preserving training dynamics across batch sizes is an important tool for practical machine learning as it enables the trade-off between batch size and wall-clock time. This trade-off is typically enabled by a scaling rule, for example, in stochastic gradient descent, one should scale the learning rate linearly with the batch size. Another important tool for practical machine learning is the model Exponential Moving Average (EMA), which is a model copy that does not receive gradient information, but instead follows its target model with some momentum. This model EMA can improve the robustness and generalization properties of supervised learning, stabilize pseudo-labeling, and provide a learning signal for Self-Supervised Learning (SSL). Prior works have treated the model EMA separately from optimization, leading to different training dynamics across batch sizes and lower model performance. In this work, we provide a scaling rule for optimization in the presence of model EMAs and demonstrate its validity across a range of architectures, optimizers, and data modalities. We also show the rule's validity where the model EMA contributes to the optimization of the target model, enabling us to train EMA-based pseudo-labeling and SSL methods at small and large batch sizes. For SSL, we enable training of BYOL up to batch size 24,576 without sacrificing performance, optimally a 6$\times$ wall-clock time reduction.
</details>
<details>
<summary>摘要</summary>
Previous works have treated the model EMA separately from optimization, leading to different training dynamics across batch sizes and lower model performance. In this work, we provide a scaling rule for optimization in the presence of model EMAs and demonstrate its validity across a range of architectures, optimizers, and data modalities. We also show the rule's validity where the model EMA contributes to the optimization of the target model, enabling us to train EMA-based pseudo-labeling and SSL methods at small and large batch sizes.For SSL, we enable training of BYOL up to batch size 24,576 without sacrificing performance, resulting in a 6 times wall-clock time reduction.
</details></li>
</ul>
<hr>
<h2 id="When-Multi-Task-Learning-Meets-Partial-Supervision-A-Computer-Vision-Review"><a href="#When-Multi-Task-Learning-Meets-Partial-Supervision-A-Computer-Vision-Review" class="headerlink" title="When Multi-Task Learning Meets Partial Supervision: A Computer Vision Review"></a>When Multi-Task Learning Meets Partial Supervision: A Computer Vision Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14382">http://arxiv.org/abs/2307.14382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maxime Fontana, Michael Spratling, Miaojing Shi</li>
<li>For: 本研究探讨了多任务学习（MTL）在不同半指导下的应用。* Methods: 本研究使用了多个参数共享技术来传递知识 между任务。* Results: 本研究介绍了多任务优化问题中的多种挑战，并提出了基于任务关系分组的方法来解决这些挑战。In English, this translates to:* For: This study explores the application of multi-task learning (MTL) under different partial supervision settings.* Methods: The study uses multiple parameter sharing techniques to transfer knowledge between tasks.* Results: The study introduces challenges arising from the multi-objective optimization scheme and proposes a method based on task relationships to address these challenges.<details>
<summary>Abstract</summary>
Multi-Task Learning (MTL) aims to learn multiple tasks simultaneously while exploiting their mutual relationships. By using shared resources to simultaneously calculate multiple outputs, this learning paradigm has the potential to have lower memory requirements and inference times compared to the traditional approach of using separate methods for each task. Previous work in MTL has mainly focused on fully-supervised methods, as task relationships can not only be leveraged to lower the level of data-dependency of those methods but they can also improve performance. However, MTL introduces a set of challenges due to a complex optimisation scheme and a higher labeling requirement. This review focuses on how MTL could be utilised under different partial supervision settings to address these challenges. First, this review analyses how MTL traditionally uses different parameter sharing techniques to transfer knowledge in between tasks. Second, it presents the different challenges arising from such a multi-objective optimisation scheme. Third, it introduces how task groupings can be achieved by analysing task relationships. Fourth, it focuses on how partially supervised methods applied to MTL can tackle the aforementioned challenges. Lastly, this review presents the available datasets, tools and benchmarking results of such methods.
</details>
<details>
<summary>摘要</summary>
First, the review examines how MTL traditionally uses parameter sharing techniques to transfer knowledge between tasks. Second, it discusses the challenges arising from the multi-objective optimization scheme. Third, it introduces task groupings based on task relationships. Fourth, it focuses on how partially supervised methods can be applied to MTL to tackle the challenges. Finally, the review presents available datasets, tools, and benchmarking results for such methods.
</details></li>
</ul>
<hr>
<h2 id="EdgeConvEns-Convolutional-Ensemble-Learning-for-Edge-Intelligence"><a href="#EdgeConvEns-Convolutional-Ensemble-Learning-for-Edge-Intelligence" class="headerlink" title="EdgeConvEns: Convolutional Ensemble Learning for Edge Intelligence"></a>EdgeConvEns: Convolutional Ensemble Learning for Edge Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14381">http://arxiv.org/abs/2307.14381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilkay Sikdokur, İnci M. Baytaş, Arda Yurdakul</li>
<li>for: 这项研究旨在提出一种基于 convolutional ensemble learning 的深入边缘智能方法，以提高边缘设备的训练效果和预测性能。</li>
<li>methods: 本研究使用了 Federation Learning 方法，并在边缘设备上实现了多个不同计算能力的 FPGA 设备上的独立训练。同时，通过将学习到的特征传输到中央服务器进行集成训练，以提高总预测性能。</li>
<li>results: 实验结果表明，EdgeConvEns 可以在不同训练场景下比 estado-of-the-art 方法具有更好的预测性能，同时减少了数据传输量和通信次数。<details>
<summary>Abstract</summary>
Deep edge intelligence aims to deploy deep learning models that demand computationally expensive training in the edge network with limited computational power. Moreover, many deep edge intelligence applications require handling distributed data that cannot be transferred to a central server due to privacy concerns. Decentralized learning methods, such as federated learning, offer solutions where models are learned collectively by exchanging learned weights. However, they often require complex models that edge devices may not handle and multiple rounds of network communication to achieve state-of-the-art performances. This study proposes a convolutional ensemble learning approach, coined EdgeConvEns, that facilitates training heterogeneous weak models on edge and learning to ensemble them where data on edge are heterogeneously distributed. Edge models are implemented and trained independently on Field-Programmable Gate Array (FPGA) devices with various computational capacities. Learned data representations are transferred to a central server where the ensemble model is trained with the learned features received from the edge devices to boost the overall prediction performance. Extensive experiments demonstrate that the EdgeConvEns can outperform the state-of-the-art performance with fewer communications and less data in various training scenarios.
</details>
<details>
<summary>摘要</summary>
深入智能目标是在边缘网络中部署需要计算负担强大的深度学习模型，但边缘设备的计算能力有限。此外，许多深入智能应用需要处理分布式数据，这些数据无法被传输到中央服务器 due to 隐私问题。分布式学习方法，如联邦学习，可以解决这些问题，但它们通常需要复杂的模型，边缘设备可能无法处理，并且需要多轮的网络通信来达到状态艺术性能。本研究提出了一种 convolutional ensemble learning 方法，名为 EdgeConvEns，它可以在边缘设备上训练多种不同的弱模型，并将这些模型 ensemble 在边缘设备上。边缘设备上实现和训练独立的 Field-Programmable Gate Array (FPGA) 设备，并将学习到的数据表示传输到中央服务器，以在中央服务器上训练 ensemble 模型，以提高总预测性能。经验示出，EdgeConvEns 可以在不同的训练场景下超越当前的状态艺术性能，并且需要更少的通信和数据量。
</details></li>
</ul>
<hr>
<h2 id="Source-Condition-Double-Robust-Inference-on-Functionals-of-Inverse-Problems"><a href="#Source-Condition-Double-Robust-Inference-on-Functionals-of-Inverse-Problems" class="headerlink" title="Source Condition Double Robust Inference on Functionals of Inverse Problems"></a>Source Condition Double Robust Inference on Functionals of Inverse Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13793">http://arxiv.org/abs/2307.13793</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew Bennett, Nathan Kallus, Xiaojie Mao, Whitney Newey, Vasilis Syrgkanis, Masatoshi Uehara</li>
<li>for: 这个论文主要是为了研究 linear inverse problems 中参数估计的方法。</li>
<li>methods: 论文使用了 doubly robust representation 方法，该方法基于解决 primal 和 dual 两个线性 inverse problems 的解。</li>
<li>results: 论文提供了一种 asymptotically normal 的 parameter estimation method, 不需要知道 primal 或 dual 问题是哪个更加正确的问题。这个结果基于一种新的 iterated Tikhonov regularized adversarial estimators 方法，该方法可以应用于 general hypothesis spaces 上的 linear inverse problems。<details>
<summary>Abstract</summary>
We consider estimation of parameters defined as linear functionals of solutions to linear inverse problems. Any such parameter admits a doubly robust representation that depends on the solution to a dual linear inverse problem, where the dual solution can be thought as a generalization of the inverse propensity function. We provide the first source condition double robust inference method that ensures asymptotic normality around the parameter of interest as long as either the primal or the dual inverse problem is sufficiently well-posed, without knowledge of which inverse problem is the more well-posed one. Our result is enabled by novel guarantees for iterated Tikhonov regularized adversarial estimators for linear inverse problems, over general hypothesis spaces, which are developments of independent interest.
</details>
<details>
<summary>摘要</summary>
我们考虑参数估计为线性函数解析方法的线性逆问题。任何参数都可以得到双重稳定表示，这种表示取决于解析方法的对偶问题的解，可以看作总化倒数逆函数的扩展。我们提供了第一个源condition double robust推断方法，可以在参数关心范围内保证参数归一化正常性，只要 primal 或 dual 逆问题够正确，不需要知道哪个逆问题更加正确。我们的结果基于新的 iterated Tikhonov regularized adversarial estimator 的 guarantees，这些保证在一般假设空间上适用，是独立的研究成果。
</details></li>
</ul>
<hr>
<h2 id="Histogram-Layer-Time-Delay-Neural-Networks-for-Passive-Sonar-Classification"><a href="#Histogram-Layer-Time-Delay-Neural-Networks-for-Passive-Sonar-Classification" class="headerlink" title="Histogram Layer Time Delay Neural Networks for Passive Sonar Classification"></a>Histogram Layer Time Delay Neural Networks for Passive Sonar Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13788">http://arxiv.org/abs/2307.13788</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/peeples-lab/hltdnn">https://github.com/peeples-lab/hltdnn</a></li>
<li>paper_authors: Jarin Ritu, Ethan Barnes, Riley Martell, Alexandra Van Dine, Joshua Peeples</li>
<li>for: 本研究旨在提高海上陌生探测中的潜水噪音目标检测，因为噪音波的传播复杂，目标识别具有挑战性。</li>
<li>methods: 本研究提出了一种新的方法，它将时间延迟神经网络和 histogram 层结合使用，以利用噪音波观测记录中的统计上下文来提高特征学习和海上陌生探测中的噪音目标识别。</li>
<li>results: 对比基eline模型，本研究的方法显示出了更高的识别精度，这表明了在噪音目标识别中吸收统计上下文的优势。代码可以公开获得。<details>
<summary>Abstract</summary>
Underwater acoustic target detection in remote marine sensing operations is challenging due to complex sound wave propagation. Despite the availability of reliable sonar systems, target recognition remains a difficult problem. Various methods address improved target recognition. However, most struggle to disentangle the high-dimensional, non-linear patterns in the observed target recordings. In this work, a novel method combines a time delay neural network and histogram layer to incorporate statistical contexts for improved feature learning and underwater acoustic target classification. The proposed method outperforms the baseline model, demonstrating the utility in incorporating statistical contexts for passive sonar target recognition. The code for this work is publicly available.
</details>
<details>
<summary>摘要</summary>
水下声学目标检测在远程海洋探测操作中存在很大的挑战，主要是声波传播复杂。尽管有可靠的声纳系统，但目标识别仍然是一个困难的问题。各种方法尝试了改进目标识别，但大多数都无法分离高维、非线性的目标记录特征。在这种情况下，我们提出了一种新的方法，它将时间延迟神经网络和分布图层结合在一起，以利用统计上下文来改进声学目标识别。我们的方法比基eline模型更高效，这 demonstartes了在声学目标识别中提供统计上下文的重要性。代码已经公开 availible。
</details></li>
</ul>
<hr>
<h2 id="The-GANfather-Controllable-generation-of-malicious-activity-to-improve-defence-systems"><a href="#The-GANfather-Controllable-generation-of-malicious-activity-to-improve-defence-systems" class="headerlink" title="The GANfather: Controllable generation of malicious activity to improve defence systems"></a>The GANfather: Controllable generation of malicious activity to improve defence systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13787">http://arxiv.org/abs/2307.13787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ricardo Ribeiro Pereira, Jacopo Bono, João Tiago Ascensão, David Aparício, Pedro Ribeiro, Pedro Bizarro</li>
<li>for: 帮助防御系统检测恶意活动，不需要标注数据</li>
<li>methods: 提出了一种基于生成器网络的方法，通过引入额外目标函数来奖励生成恶意样本</li>
<li>results: 在两个实际应用中（货币洗钱和推荐系统），我们成功地使用这种方法生成了恶意样本，并训练了一个新的防御系统来捕捉这些样本。<details>
<summary>Abstract</summary>
Machine learning methods to aid defence systems in detecting malicious activity typically rely on labelled data. In some domains, such labelled data is unavailable or incomplete. In practice this can lead to low detection rates and high false positive rates, which characterise for example anti-money laundering systems. In fact, it is estimated that 1.7--4 trillion euros are laundered annually and go undetected. We propose The GANfather, a method to generate samples with properties of malicious activity, without label requirements. We propose to reward the generation of malicious samples by introducing an extra objective to the typical Generative Adversarial Networks (GANs) loss. Ultimately, our goal is to enhance the detection of illicit activity using the discriminator network as a novel and robust defence system. Optionally, we may encourage the generator to bypass pre-existing detection systems. This setup then reveals defensive weaknesses for the discriminator to correct. We evaluate our method in two real-world use cases, money laundering and recommendation systems. In the former, our method moves cumulative amounts close to 350 thousand dollars through a network of accounts without being detected by an existing system. In the latter, we recommend the target item to a broad user base with as few as 30 synthetic attackers. In both cases, we train a new defence system to capture the synthetic attacks.
</details>
<details>
<summary>摘要</summary>
机器学习方法通常需要标注数据来帮助防御系统检测恶意活动。在某些领域，这些标注数据可能不可 obtenía或 incomplete。这可能导致检测率低下 false positive 率高，这种情况例如反洗钱系统。实际上，每年可能有 1.7--4 亿欧元被骗财，并未被发现。我们提出了 The GANfather，一种方法，可以生成具有恶意活动特性的样本，不需要标注。我们提出了在 Typical Generative Adversarial Networks (GANs) 损失函数中引入一个额外的目标，以奖励生成恶意样本。最终，我们的目标是通过使用探测器网络作为一种新的和可靠的防御系统，提高恶意活动的检测。选择地，我们可以让生成器 circumvent 现有的检测系统。这种设置然后 revelas defensive weaknesses for the discriminator to correct。我们在两个实际应用中评估了我们的方法：反洗钱和推荐系统。在前一个应用中，我们通过一个网络的账户来传递累计金额达 350 万美元，而不被现有系统检测到。在后一个应用中，我们通过 Synthetic 攻击者来推荐目标项目，并且只需要 30 个 synthetic 攻击者。在两个案例中，我们训练了一个新的防御系统，以捕捉 Synthetic 攻击。
</details></li>
</ul>
<hr>
<h2 id="Robust-Assignment-of-Labels-for-Active-Learning-with-Sparse-and-Noisy-Annotations"><a href="#Robust-Assignment-of-Labels-for-Active-Learning-with-Sparse-and-Noisy-Annotations" class="headerlink" title="Robust Assignment of Labels for Active Learning with Sparse and Noisy Annotations"></a>Robust Assignment of Labels for Active Learning with Sparse and Noisy Annotations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14380">http://arxiv.org/abs/2307.14380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Kałuża, Andrzej Janusz, Dominik Ślęzak</li>
<li>for: 解决激活学习中数据标注错误的问题</li>
<li>methods: 提出了两种新的注释统一算法，利用无标示部分的样本空间</li>
<li>results: 在四个公共数据集上进行了实验，表明提案的方法在估计注释者可靠性和实际标签分配方面具有robustness和superiority，并且比州对数据集中的简单多数投票更为有效。<details>
<summary>Abstract</summary>
Supervised classification algorithms are used to solve a growing number of real-life problems around the globe. Their performance is strictly connected with the quality of labels used in training. Unfortunately, acquiring good-quality annotations for many tasks is infeasible or too expensive to be done in practice. To tackle this challenge, active learning algorithms are commonly employed to select only the most relevant data for labeling. However, this is possible only when the quality and quantity of labels acquired from experts are sufficient. Unfortunately, in many applications, a trade-off between annotating individual samples by multiple annotators to increase label quality vs. annotating new samples to increase the total number of labeled instances is necessary. In this paper, we address the issue of faulty data annotations in the context of active learning. In particular, we propose two novel annotation unification algorithms that utilize unlabeled parts of the sample space. The proposed methods require little to no intersection between samples annotated by different experts. Our experiments on four public datasets indicate the robustness and superiority of the proposed methods in both, the estimation of the annotator's reliability, and the assignment of actual labels, against the state-of-the-art algorithms and the simple majority voting.
</details>
<details>
<summary>摘要</summary>
超visisted分类算法在全球各地的实际问题中得到应用。它们的性能与训练中使用的标签质量有着紧密的关系。然而，获取高质量标签是在实践中不可能或太昂贵了。为解决这个挑战，活动学算法通常被使用来选择仅需要标注的数据。然而，这只有当获取专家标注的标签质量和量足够时才能够实现。在许多应用程序中，需要考虑 annotating individual samples by multiple annotators 来提高标签质量 vs. annotating new samples 来增加标注的总数。在这篇论文中，我们对活动学中的假数据标注进行了研究。我们提出了两种新的标注统一算法，它们可以利用样本空间中的无标注部分。我们的方法需要标注者之间的交叉少到无。我们在四个公共数据集上进行了实验，结果表明我们的方法在计算标注者的可靠性和实际标注中具有更高的稳定性和优势，相比于当前的算法和简单多数投票。
</details></li>
</ul>
<hr>
<h2 id="Accuracy-Amplification-in-Differentially-Private-Logistic-Regression-A-Pre-Training-Approach"><a href="#Accuracy-Amplification-in-Differentially-Private-Logistic-Regression-A-Pre-Training-Approach" class="headerlink" title="Accuracy Amplification in Differentially Private Logistic Regression: A Pre-Training Approach"></a>Accuracy Amplification in Differentially Private Logistic Regression: A Pre-Training Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13771">http://arxiv.org/abs/2307.13771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Hoseinpour, Milad Hoseinpour, Ali Aghagolzadeh</li>
<li>for: 保护隐私的机器学习模型训练数据。</li>
<li>methods: 使用预训练模块和 differential privacy 的 logistic regression 模型。</li>
<li>results: 通过预训练模块，可以提高 differential privacy 下的机器学习模型的准确率。In more detail, the paper aims to improve the accuracy of a differentially private logistic regression model by using a pre-training module. The authors first pre-train the model on a public dataset without privacy concerns, and then fine-tune the model using the private dataset with differential privacy constraints. The results show that adding the pre-training module significantly improves the accuracy of the differentially private logistic regression model.<details>
<summary>Abstract</summary>
Machine learning (ML) models can memorize training datasets. As a result, training ML models over private datasets can violate the privacy of individuals. Differential privacy (DP) is a rigorous privacy notion to preserve the privacy of underlying training datasets in ML models. Yet, training ML models in a DP framework usually degrades the accuracy of ML models. This paper aims to boost the accuracy of a DP-ML model, specifically a logistic regression model, via a pre-training module. In more detail, we initially pre-train our model on a public training dataset that there is no privacy concern about it. Then, we fine-tune our model via the DP logistic regression with the private dataset. In the numerical results, we show that adding a pre-training module significantly improves the accuracy of the DP logistic regression.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）模型可以记忆训练数据集。因此，在训练ML模型时，可能会违反个人隐私。不同隐私（DP）是一种严格的隐私概念，用于保护训练数据集的隐私。然而，在DP框架下训练ML模型通常会降低模型的准确率。这篇论文目的是提高DP-ML模型的准确率，特别是使用Logistic Regression模型。在更多的细节中，我们首先在没有隐私问题的公共训练数据集上进行预训练。然后，我们使用DP Logistic Regression模型进行细化。在实验结果中，我们发现，添加预训练模块可以显著提高DP Logistic Regression模型的准确率。
</details></li>
</ul>
<hr>
<h2 id="ClusterSeq-Enhancing-Sequential-Recommender-Systems-with-Clustering-based-Meta-Learning"><a href="#ClusterSeq-Enhancing-Sequential-Recommender-Systems-with-Clustering-based-Meta-Learning" class="headerlink" title="ClusterSeq: Enhancing Sequential Recommender Systems with Clustering based Meta-Learning"></a>ClusterSeq: Enhancing Sequential Recommender Systems with Clustering based Meta-Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13766">http://arxiv.org/abs/2307.13766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammmadmahdi Maheri, Reza Abdollahzadeh, Bardia Mohammadi, Mina Rafiei, Jafar Habibi, Hamid R. Rabiee</li>
<li>for: 解决用户冷启始问题，提高续传推荐系统的效果。</li>
<li>methods:  combining meta-learning with user and item-side information，并使用动态信息在用户序列中增强物品预测精度。</li>
<li>results: 比如 existing meta-learning methods，我们的提案方法实现了16-39%的提升在 Mean Reciprocal Rank (MRR) 中。<details>
<summary>Abstract</summary>
In practical scenarios, the effectiveness of sequential recommendation systems is hindered by the user cold-start problem, which arises due to limited interactions for accurately determining user preferences. Previous studies have attempted to address this issue by combining meta-learning with user and item-side information. However, these approaches face inherent challenges in modeling user preference dynamics, particularly for "minor users" who exhibit distinct preferences compared to more common or "major users." To overcome these limitations, we present a novel approach called ClusterSeq, a Meta-Learning Clustering-Based Sequential Recommender System. ClusterSeq leverages dynamic information in the user sequence to enhance item prediction accuracy, even in the absence of side information. This model preserves the preferences of minor users without being overshadowed by major users, and it capitalizes on the collective knowledge of users within the same cluster. Extensive experiments conducted on various benchmark datasets validate the effectiveness of ClusterSeq. Empirical results consistently demonstrate that ClusterSeq outperforms several state-of-the-art meta-learning recommenders. Notably, compared to existing meta-learning methods, our proposed approach achieves a substantial improvement of 16-39% in Mean Reciprocal Rank (MRR).
</details>
<details>
<summary>摘要</summary>
在实际应用场景中，顺序推荐系统的效果受用户冷启问题的限制，这种问题 arise due to 用户与ITEM之间的互动有限，难以准确地确定用户的偏好。先前的研究尝试通过meta-学习和用户项信息的结合来解决这个问题，但这些方法面临用户偏好动态模型化的挑战，特别是对"小用户"而言，他们的偏好与"大用户"不同。为了超越这些限制，我们提出了一种新的方法 called ClusterSeq，这是一种基于 clustering 的 Meta-Learning Sequential Recommender System。ClusterSeq 利用用户序列中的动态信息来提高项预测精度，即使没有副信息。这个模型保留了小用户的偏好，不会被大用户所掩盖，同时利用用户集中的共同知识来提高推荐的准确性。在多个标准 benchmark 数据集上进行了广泛的实验，证明 ClusterSeq 的效果。实验结果表明，ClusterSeq 在 MRR 方面与state-of-the-art meta-learning recommenders 相比，具有16-39%的显著提高。
</details></li>
</ul>
<hr>
<h2 id="Implicitly-Normalized-Explicitly-Regularized-Density-Estimation"><a href="#Implicitly-Normalized-Explicitly-Regularized-Density-Estimation" class="headerlink" title="Implicitly Normalized Explicitly Regularized Density Estimation"></a>Implicitly Normalized Explicitly Regularized Density Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13763">http://arxiv.org/abs/2307.13763</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mark Kozdoba, Binyamin Perets, Shie Mannor</li>
<li>for: 非 Parametric density estimation 方法</li>
<li>methods: 使用 Sobolev  нор调整 density</li>
<li>results: 比 Kernel Density Estimation 方法更加不偏，可以Clearly interpret 模型偏差Here’s a more detailed explanation of each point:1. for: The paper proposes a new approach to non-parametric density estimation, which is based on regularizing a Sobolev norm of the density. This approach is different from traditional Kernel Density Estimation (KDE) methods, and it aims to provide a more interpretable and less biased estimation of the density.2. methods: The proposed method uses a regularization term that is based on the Sobolev norm of the density, which is a measure of the smoothness of the density. The optimization problem needed to determine the density is non-convex, and standard gradient methods do not perform well. However, the paper shows that with an appropriate initialization and using natural gradients, one can obtain well-performing solutions.3. results: The paper evaluates the proposed method on the comprehensive recent Anomaly Detection benchmark suite, ADBench, and finds that it ranks second best among more than 15 algorithms. The method provides unnormalized densities, which prevents the use of log-likelihood for cross validation. However, the paper shows that one can instead adapt Fisher Divergence based Score Matching methods for this task.<details>
<summary>Abstract</summary>
We propose a new approach to non-parametric density estimation, that is based on regularizing a Sobolev norm of the density. This method is provably different from Kernel Density Estimation, and makes the bias of the model clear and interpretable. While there is no closed analytic form for the associated kernel, we show that one can approximate it using sampling. The optimization problem needed to determine the density is non-convex, and standard gradient methods do not perform well. However, we show that with an appropriate initialization and using natural gradients, one can obtain well performing solutions. Finally, while the approach provides unnormalized densities, which prevents the use of log-likelihood for cross validation, we show that one can instead adapt Fisher Divergence based Score Matching methods for this task. We evaluate the resulting method on the comprehensive recent Anomaly Detection benchmark suite, ADBench, and find that it ranks second best, among more than 15 algorithms.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的非Parametric数量估计方法，基于调整 Sobolev norm 的数量估计。这种方法与 Kernel Density Estimation 不同，可以将模型的偏见显示出来并实现可解释性。虽然没有关注点的关注函数，但我们显示可以使用抽样来近似它。估计问题是非对称的，标准的梯度法不太好。但我们显示，对于适当的初始化和使用自然梯度，可以得到良好的解。最后，这种方法提供的数量估计无法取得正规化的数量估计，因此无法使用log-likelihood进行cross validate。但我们显示可以使用Fisher Divergence based Score Matching方法来处理这个问题。我们在Anomaly Detection benchmark suite ADBench 上进行了评估，发现其排名第二，与其他15种方法相比。
</details></li>
</ul>
<hr>
<h2 id="UPREVE-An-End-to-End-Causal-Discovery-Benchmarking-System"><a href="#UPREVE-An-End-to-End-Causal-Discovery-Benchmarking-System" class="headerlink" title="UPREVE: An End-to-End Causal Discovery Benchmarking System"></a>UPREVE: An End-to-End Causal Discovery Benchmarking System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13757">http://arxiv.org/abs/2307.13757</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suraj Jyothi Unni, Paras Sheth, Kaize Ding, Huan Liu, K. Selcuk Candan</li>
<li>for: 本研究旨在提供一个轻松使用、可自定义的web基台用于探索复杂社会行为系统中的 causal 关系。</li>
<li>methods: 本研究使用了多种算法同时运行、可视化 causal 关系以及评估学习的 causal 图的精度。</li>
<li>results: 本研究提出了一个轻松使用、可自定义的 web 基台 UPREVE，可以帮助研究者和实践者更好地探索和理解社会行为系统中的 causal 关系，以便更好地做出决策。<details>
<summary>Abstract</summary>
Discovering causal relationships in complex socio-behavioral systems is challenging but essential for informed decision-making. We present Upload, PREprocess, Visualize, and Evaluate (UPREVE), a user-friendly web-based graphical user interface (GUI) designed to simplify the process of causal discovery. UPREVE allows users to run multiple algorithms simultaneously, visualize causal relationships, and evaluate the accuracy of learned causal graphs. With its accessible interface and customizable features, UPREVE empowers researchers and practitioners in social computing and behavioral-cultural modeling (among others) to explore and understand causal relationships effectively. Our proposed solution aims to make causal discovery more accessible and user-friendly, enabling users to gain valuable insights for better decision-making.
</details>
<details>
<summary>摘要</summary>
发现复杂社会行为系统中的 causal 关系是具有挑战性和重要性的，但是这对于 informed decision-making 是必需的。我们提出了 Upload、PREprocess、Visualize 和 Evaluate（UPREVE），一个用户友好的网页式 графического用户界面（GUI），用于简化 causal discovery 的过程。UPREVE 允许用户同时运行多个算法，可视化 causal 关系，并评估学习的 causal 图的准确性。它的访问ible 界面和可定制功能，使得研究者和实践者在社会计算和文化模型中能够有效地探索和理解 causal 关系，从而获得有价值的发现，以便更好的决策。我们的提议的解决方案计划使 causal discovery 更加访问ible和用户友好，以便用户可以更好地理解 causal 关系，并做出更好的决策。
</details></li>
</ul>
<hr>
<h2 id="Solution-Path-of-Time-varying-Markov-Random-Fields-with-Discrete-Regularization"><a href="#Solution-Path-of-Time-varying-Markov-Random-Fields-with-Discrete-Regularization" class="headerlink" title="Solution Path of Time-varying Markov Random Fields with Discrete Regularization"></a>Solution Path of Time-varying Markov Random Fields with Discrete Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13750">http://arxiv.org/abs/2307.13750</a></li>
<li>repo_url: None</li>
<li>paper_authors: Salar Fattahi, Andres Gomez</li>
<li>for: 本研究实际问题是如何推断时间变化的Markov随机场景（MRF），特别是具有不同的纯粹和时间规律化的参数。</li>
<li>methods: 本文提出了一种新的参数调整问题，使用紧缩的纯粹规律化来实现参数的简洁。这个问题可以 Parametrically 解决，并且可以在实际的设置下 scale 到高维度。</li>
<li>results: 本文展示了一个全新的解法，可以实现时间变化MRF的全解析解，并且可以在实际的设置下进行交互验证。这个解法可以在 $\mathcal{O}(pT^3) $ 时间内解决，其中 $T$ 是时间步骤数量，$p$ 是时间变化MRF中参数的数量。这个解法可以高效地进行交互验证，并且可以在实际的设置下获得提供了详细的解析解。<details>
<summary>Abstract</summary>
We study the problem of inferring sparse time-varying Markov random fields (MRFs) with different discrete and temporal regularizations on the parameters. Due to the intractability of discrete regularization, most approaches for solving this problem rely on the so-called maximum-likelihood estimation (MLE) with relaxed regularization, which neither results in ideal statistical properties nor scale to the dimensions encountered in realistic settings. In this paper, we address these challenges by departing from the MLE paradigm and resorting to a new class of constrained optimization problems with exact, discrete regularization to promote sparsity in the estimated parameters. Despite the nonconvex and discrete nature of our formulation, we show that it can be solved efficiently and parametrically for all sparsity levels. More specifically, we show that the entire solution path of the time-varying MRF for all sparsity levels can be obtained in $\mathcal{O}(pT^3)$, where $T$ is the number of time steps and $p$ is the number of unknown parameters at any given time. The efficient and parametric characterization of the solution path renders our approach highly suitable for cross-validation, where parameter estimation is required for varying regularization values. Despite its simplicity and efficiency, we show that our proposed approach achieves provably small estimation error for different classes of time-varying MRFs, namely Gaussian and discrete MRFs, with as few as one sample per time. Utilizing our algorithm, we can recover the complete solution path for instances of time-varying MRFs featuring over 30 million variables in less than 12 minutes on a standard laptop computer. Our code is available at \url{https://sites.google.com/usc.edu/gomez/data}.
</details>
<details>
<summary>摘要</summary>
我们研究了推理缺省时间变化Markov随机场（MRF）的问题，具有不同的离散和时间 regularization 参数。由于离散 regularization 的不可解性，大多数解决这个问题的方法都 rely on 最大 likelihood estimation（MLE）with relaxed regularization，这并不会导致理想的统计性质，也无法扩展到实际中的维度。在这篇论文中，我们解决这些挑战，我们不再采用 MLE 模型，而是基于一种新的受限制优化问题，以便在优化参数时Promote 缺省性。尽管我们的形式ulation 是非对称和离散的，但我们证明可以高效地解决它，并且可以 Parametrically 解决所有缺省级别。更specifically，我们证明可以在 $\mathcal{O}(pT^3)$ 时间内解决整个时间变化 MRF 的全解路径，其中 $T$ 是时间步骤数量，$p$ 是时间步骤中未知参数的数量。我们的方法高效、可 parametric，因此非常适合 Cross-validation，其中需要不同的 regularization 值进行参数估计。尽管它简单、高效，但我们证明我们的方法可以在不同类型的时间变化 MRF 上取得可观测小的估计误差，只需要一个时间步骤中的一个样本。使用我们的算法，我们可以在 less than 12 分钟内将 over 30 万个变量的解决路径全部回归到标准笔记计算机上。我们的代码可以在 \url{https://sites.google.com/usc.edu/gomez/data} 上找到。
</details></li>
</ul>
<hr>
<h2 id="mL-BFGS-A-Momentum-based-L-BFGS-for-Distributed-Large-Scale-Neural-Network-Optimization"><a href="#mL-BFGS-A-Momentum-based-L-BFGS-for-Distributed-Large-Scale-Neural-Network-Optimization" class="headerlink" title="mL-BFGS: A Momentum-based L-BFGS for Distributed Large-Scale Neural Network Optimization"></a>mL-BFGS: A Momentum-based L-BFGS for Distributed Large-Scale Neural Network Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13744">http://arxiv.org/abs/2307.13744</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Niu, Zalan Fabian, Sunwoo Lee, Mahdi Soltanolkotabi, Salman Avestimehr</li>
<li>for: 大规模神经网络优化中使用 quasi-Newton 方法遇到了 significiant 挑战，主要是在 Hessian 相关计算中添加了额外的计算成本，以及在随机训练中存在稳定性问题。</li>
<li>methods: 我们提出了一种轻量级的摩托矢量 L-BFGS 算法（mL-BFGS），该算法在大规模分布式深度神经网络优化中采用了几乎免费的摩托矢量计划，从而减少了随机噪声在 Hessian 中的影响，使得梯度下降过程更加稳定。</li>
<li>results: 我们通过使用 mL-BFGS 对一些标准神经网络模型进行训练，并与基elines（SGD、Adam 等）进行比较，发现 mL-BFGS 可以 achieve both noticeable iteration-wise 和 wall-clock 速度减少。<details>
<summary>Abstract</summary>
Quasi-Newton methods still face significant challenges in training large-scale neural networks due to additional compute costs in the Hessian related computations and instability issues in stochastic training. A well-known method, L-BFGS that efficiently approximates the Hessian using history parameter and gradient changes, suffers convergence instability in stochastic training. So far, attempts that adapt L-BFGS to large-scale stochastic training incur considerable extra overhead, which offsets its convergence benefits in wall-clock time. In this paper, we propose mL-BFGS, a lightweight momentum-based L-BFGS algorithm that paves the way for quasi-Newton (QN) methods in large-scale distributed deep neural network (DNN) optimization. mL-BFGS introduces a nearly cost-free momentum scheme into L-BFGS update and greatly reduces stochastic noise in the Hessian, therefore stabilizing convergence during stochastic optimization. For model training at a large scale, mL-BFGS approximates a block-wise Hessian, thus enabling distributing compute and memory costs across all computing nodes. We provide a supporting convergence analysis for mL-BFGS in stochastic settings. To investigate mL-BFGS potential in large-scale DNN training, we train benchmark neural models using mL-BFGS and compare performance with baselines (SGD, Adam, and other quasi-Newton methods). Results show that mL-BFGS achieves both noticeable iteration-wise and wall-clock speedup.
</details>
<details>
<summary>摘要</summary>
尽管凯撒-牛顿方法在训练大规模神经网络方面仍然面临着重要的挑战，主要是在资源成本上。这些方法在训练中需要额外的计算成本，以及约束变量的稳定性问题。L-BFGS方法，一种广泛使用的凯撒-牛顿方法，在随机训练中存在很大的问题，即迭代不稳定。目前，尝试将L-BFGS方法应用于大规模随机训练中，增加了非常大的额外开销，这将导致训练时间的增加。在这篇论文中，我们提出了一种轻量级的摩托率基于L-BFGS算法，称为mL-BFGS。该算法在大规模随机训练中使用历史参数和梯度变化来高效地估算梯度，从而稳定训练过程。此外，mL-BFGS还可以将块级别的梯度估算分布到所有计算节点上，从而实现分布式计算和存储成本的分摊。我们还提供了支持mL-BFGS在随机设定下的收敛分析。为了评估mL-BFGS在大规模神经网络训练中的潜力，我们使用mL-BFGS训练了一些标准神经网络模型，并与基线方法（SGD、Adam等）进行比较。结果表明，mL-BFGS在训练过程中能够达到显著的迭代减速和wall-clock减速。
</details></li>
</ul>
<hr>
<h2 id="ARB-Advanced-Reasoning-Benchmark-for-Large-Language-Models"><a href="#ARB-Advanced-Reasoning-Benchmark-for-Large-Language-Models" class="headerlink" title="ARB: Advanced Reasoning Benchmark for Large Language Models"></a>ARB: Advanced Reasoning Benchmark for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13692">http://arxiv.org/abs/2307.13692</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tomohiro Sawada, Daniel Paleka, Alexander Havrilla, Pranav Tadepalli, Paula Vidas, Alexander Kranias, John J. Nay, Kshitij Gupta, Aran Komatsuzaki</li>
<li>for: 本研究旨在提供一个新的 benchmark，以测试大型语言模型（LLM）的推理能力和领域知识。</li>
<li>methods: 本研究使用了多个领域的高级推理问题组成的 ARB benchmark，以测试 LLM 的推理能力和领域知识。</li>
<li>results: 研究发现，现有的 GPT-4 和 Claude 模型在 ARB benchmark 上的得分尚未达到专家水平，特别是在更加具有挑战性的任务上。此外，研究还引入了一种基于 rubric 的评估方法，以提高自动和助动评估能力。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated remarkable performance on various quantitative reasoning and knowledge benchmarks. However, many of these benchmarks are losing utility as LLMs get increasingly high scores, despite not yet reaching expert performance in these domains. We introduce ARB, a novel benchmark composed of advanced reasoning problems in multiple fields. ARB presents a more challenging test than prior benchmarks, featuring problems in mathematics, physics, biology, chemistry, and law. As a subset of ARB, we introduce a challenging set of math and physics problems which require advanced symbolic reasoning and domain knowledge. We evaluate recent models such as GPT-4 and Claude on ARB and demonstrate that current models score well below 50% on more demanding tasks. In order to improve both automatic and assisted evaluation capabilities, we introduce a rubric-based evaluation approach, allowing GPT-4 to score its own intermediate reasoning steps. Further, we conduct a human evaluation of the symbolic subset of ARB, finding promising agreement between annotators and GPT-4 rubric evaluation scores.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="High-Probability-Analysis-for-Non-Convex-Stochastic-Optimization-with-Clipping"><a href="#High-Probability-Analysis-for-Non-Convex-Stochastic-Optimization-with-Clipping" class="headerlink" title="High Probability Analysis for Non-Convex Stochastic Optimization with Clipping"></a>High Probability Analysis for Non-Convex Stochastic Optimization with Clipping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13680">http://arxiv.org/abs/2307.13680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaojie Li, Yong Liu</li>
<li>for: This paper studies the theoretical guarantees of stochastic optimization algorithms with gradient clipping in the non-convex setting.</li>
<li>methods: The paper uses high probability analysis to derive the optimization bound and the generalization bound simultaneously for popular stochastic optimization algorithms with gradient clipping, including stochastic gradient descent and its variants of momentum and adaptive stepsizes.</li>
<li>results: The paper provides a relatively complete picture for the theoretical guarantee of stochastic optimization algorithms with clipping, and shows that gradient clipping is a promising technique for dealing with the heavy-tailed behavior that emerged in stochastic optimization.<details>
<summary>Abstract</summary>
Gradient clipping is a commonly used technique to stabilize the training process of neural networks. A growing body of studies has shown that gradient clipping is a promising technique for dealing with the heavy-tailed behavior that emerged in stochastic optimization as well. While gradient clipping is significant, its theoretical guarantees are scarce. Most theoretical guarantees only provide an in-expectation analysis and only focus on optimization performance. In this paper, we provide high probability analysis in the non-convex setting and derive the optimization bound and the generalization bound simultaneously for popular stochastic optimization algorithms with gradient clipping, including stochastic gradient descent and its variants of momentum and adaptive stepsizes. With the gradient clipping, we study a heavy-tailed assumption that the gradients only have bounded $\alpha$-th moments for some $\alpha \in (1, 2]$, which is much weaker than the standard bounded second-moment assumption. Overall, our study provides a relatively complete picture for the theoretical guarantee of stochastic optimization algorithms with clipping.
</details>
<details>
<summary>摘要</summary>
Gradient clipping 是一种常用的技术来稳定神经网络的训练过程。一个不断增长的研究表明，gradient clipping 是一种有前途的技术，用于处理随机优化中的重 tailed 行为。虽然 gradient clipping 具有重要的意义，但其理论保证却很少。大多数理论保证都仅提供了预期分析，并仅关注优化性能。在这篇论文中，我们提供了高概率分析，在非对称设定下，并同时 deriv 出优化 bound 和泛化 bound  для各种束缚随机优化算法，包括梯度下降和其 variants of momentum 和 adaptive stepsizes。通过束缚，我们研究了一种具有bounded $\alpha$-th moment的梯度假设，其中 $\alpha \in (1, 2]$，这是标准二次均值假设的一个弱化版本。总的来说，我们的研究提供了一个相对完整的理论保证的图景，用于随机优化算法的 clipping。
</details></li>
</ul>
<hr>
<h2 id="RED-CoMETS-An-ensemble-classifier-for-symbolically-represented-multivariate-time-series"><a href="#RED-CoMETS-An-ensemble-classifier-for-symbolically-represented-multivariate-time-series" class="headerlink" title="RED CoMETS: An ensemble classifier for symbolically represented multivariate time series"></a>RED CoMETS: An ensemble classifier for symbolically represented multivariate time series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13679">http://arxiv.org/abs/2307.13679</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zy18811/red-comets">https://github.com/zy18811/red-comets</a></li>
<li>paper_authors: Luca A. Bennett, Zahraa S. Abdallah</li>
<li>for: 这篇论文主要应用在多重时间序列分类中，尤其是在金融、医疗、工程等实际应用中。</li>
<li>methods: 本论文提出了一个新的协 ensemble分类器，名为RED CoMETS（随机增强的多重时间序列协 ensemble分类器），它是基于Co-eye（一个特别设计来进行单一时间序列分类的协 ensemble分类器）的扩展，能够处理多重时间序列资料。</li>
<li>results: RED CoMETS在UCAR档案中的评估数据上展现出了竞争性的精度，与现有的多重设定中的州际技术相比，其中最高的报告精度为’HandMovementDirection’档案。此外，提案的方法可以与Co-eye相比，大大降低 computation time，使其成为多重时间序列分类中效率和可靠的选择。<details>
<summary>Abstract</summary>
Multivariate time series classification is a rapidly growing research field with practical applications in finance, healthcare, engineering, and more. The complexity of classifying multivariate time series data arises from its high dimensionality, temporal dependencies, and varying lengths. This paper introduces a novel ensemble classifier called RED CoMETS (Random Enhanced Co-eye for Multivariate Time Series), which addresses these challenges. RED CoMETS builds upon the success of Co-eye, an ensemble classifier specifically designed for symbolically represented univariate time series, and extends its capabilities to handle multivariate data. The performance of RED CoMETS is evaluated on benchmark datasets from the UCR archive, where it demonstrates competitive accuracy when compared to state-of-the-art techniques in multivariate settings. Notably, it achieves the highest reported accuracy in the literature for the 'HandMovementDirection' dataset. Moreover, the proposed method significantly reduces computation time compared to Co-eye, making it an efficient and effective choice for multivariate time series classification.
</details>
<details>
<summary>摘要</summary>
多变量时间序列分类是一个快速发展的研究领域，有实际应用于金融、医疗、工程等领域。multivariate时间序列数据的复杂性来自其高维度、时间相关性和不同长度。本文介绍一种新的ensemble分类器called RED CoMETS（随机增强Co-eye for Multivariate Time Series），这种方法可以解决这些挑战。RED CoMETS是基于Co-eye，一种专门为symbolically represented univariate时间序列设计的ensemble分类器，扩展其能力以处理多变量数据。本文的实验结果表明，RED CoMETS在UCAR数据库中的benchmark数据集上表现竞争性高，与当前最佳的多变量技术相比。特别是，它在'HandMovementDirection'数据集上达到了 literaturereported最高的准确率。此外，提议的方法可以减少Co-eye的计算时间，使其成为efficient和effective的多变量时间序列分类选择。
</details></li>
</ul>
<hr>
<h2 id="FedDRL-A-Trustworthy-Federated-Learning-Model-Fusion-Method-Based-on-Staged-Reinforcement-Learning"><a href="#FedDRL-A-Trustworthy-Federated-Learning-Model-Fusion-Method-Based-on-Staged-Reinforcement-Learning" class="headerlink" title="FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on Staged Reinforcement Learning"></a>FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on Staged Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13716">http://arxiv.org/abs/2307.13716</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leiming Chen, Cihao Dong, Sibo Qiao, Ziling Huang, Kai Wang, Yuming Nie, Zhaoxiang Hou, Cheewei Tan</li>
<li>for: This paper aims to address the issues of model heterogeneity and malicious behavior in traditional federated learning by proposing a reinforcement learning-based model fusion approach called FedDRL.</li>
<li>methods: The FedDRL algorithm uses a two-stage approach to first filter out malicious models and select trusted client models, and then adaptively adjust the weights of the trusted client models to achieve optimal model fusion.</li>
<li>results: The experimental results show that the FedDRL algorithm has higher reliability than other algorithms while maintaining accuracy in five different model fusion scenarios.Here’s the Chinese version of the three key points:</li>
<li>for: 这篇论文目标是解决传统联合学习中的模型不同和恶意行为问题，提出一种基于强化学习的模型融合方法called FedDRL。</li>
<li>methods: FedDRL算法使用两stage方法，先过滤恶意模型，然后自适应调整被信任客户端模型的权重以实现优化的模型融合。</li>
<li>results: 实验结果表明，FedDRL算法在五种不同的模型融合场景中具有更高的可靠性，同时保持准确性。<details>
<summary>Abstract</summary>
Traditional federated learning uses the number of samples to calculate the weights of each client model and uses this fixed weight value to fusion the global model. However, in practical scenarios, each client's device and data heterogeneity leads to differences in the quality of each client's model. Thus the contribution to the global model is not wholly determined by the sample size. In addition, if clients intentionally upload low-quality or malicious models, using these models for aggregation will lead to a severe decrease in global model accuracy. Traditional federated learning algorithms do not address these issues. To solve this probelm, we propose FedDRL, a model fusion approach using reinforcement learning based on a two staged approach. In the first stage, Our method could filter out malicious models and selects trusted client models to participate in the model fusion. In the second stage, the FedDRL algorithm adaptively adjusts the weights of the trusted client models and aggregates the optimal global model. We also define five model fusion scenarios and compare our method with two baseline algorithms in those scenarios. The experimental results show that our algorithm has higher reliability than other algorithms while maintaining accuracy.
</details>
<details>
<summary>摘要</summary>
传统的联合学习使用客户端模型的数量来计算每个客户端模型的权重值，并使用这些固定权重值来融合全球模型。然而，在实际场景中，每个客户端的设备和数据多样性导致每个客户端模型的质量差异很大。因此，传统的联合学习算法不能准确地评估每个客户端模型的贡献。此外，如果客户端故意上传低质量或黑客模型，那么使用这些模型进行融合会导致全球模型的准确率受到严重的影响。传统的联合学习算法无法解决这些问题。为解决这个问题，我们提出了 FedDRL，一种基于奖励学习的模型融合方法。在第一阶段，我们的方法可以过滤出黑客模型并选择可信客户端模型参与融合。在第二阶段，FedDRL算法可以自适应地调整可信客户端模型的权重值，并融合最佳的全球模型。我们还定义了五种模型融合场景，并与两个基准算法进行比较。实验结果显示，我们的算法在可靠性和准确率之间具有良好的平衡。
</details></li>
</ul>
<hr>
<h2 id="Towards-an-AI-Accountability-Policy"><a href="#Towards-an-AI-Accountability-Policy" class="headerlink" title="Towards an AI Accountability Policy"></a>Towards an AI Accountability Policy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13658">http://arxiv.org/abs/2307.13658</a></li>
<li>repo_url: None</li>
<li>paper_authors: Przemyslaw Grabowicz, Nicholas Perello, Yair Zick</li>
<li>For: The white paper offers a set of interconnected recommendations for an AI accountability policy in response to the “AI Accountability Policy Request for Comments” by the National Telecommunications and Information Administration of the United States.* Methods: The white paper provides a set of recommendations for an AI accountability policy, including the development of transparent and explainable AI, the establishment of accountability mechanisms for AI systems, and the promotion of human-centered AI.* Results: The white paper aims to provide a comprehensive framework for an AI accountability policy that can be used to ensure the responsible development and use of AI in various industries and applications.<details>
<summary>Abstract</summary>
This white paper is a response to the "AI Accountability Policy Request for Comments" by the National Telecommunications and Information Administration of the United States. The question numbers for which comments were requested are provided in superscripts at the end of key sentences answering the respective questions. The white paper offers a set of interconnected recommendations for an AI accountability policy.
</details>
<details>
<summary>摘要</summary>
这份白皮书是对美国国家电信和信息管理局（NTIA）发布的“人工智能负责任政策请求意见”的回应。文中提到的问题号以句末的括号形式标注。本白皮书提出了一组相互连接的人工智能负责任政策建议。Note: Please note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="GNN4FR-A-Lossless-GNN-based-Federated-Recommendation-Framework"><a href="#GNN4FR-A-Lossless-GNN-based-Federated-Recommendation-Framework" class="headerlink" title="GNN4FR: A Lossless GNN-based Federated Recommendation Framework"></a>GNN4FR: A Lossless GNN-based Federated Recommendation Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01197">http://arxiv.org/abs/2308.01197</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guowei Wu, Weike Pan, Zhong Ming</li>
<li>for: 隐私保护下构建全Graph Neural Networks（GNNs）推荐系统。</li>
<li>methods: 使用lossless federated recommendation framework based on GNN，实现全图训练，保持高阶结构信息完整性。</li>
<li>results: 与非联合方法相等，具有完整高阶结构信息，遵循隐私保护法规。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have gained wide popularity in recommender systems due to their capability to capture higher-order structure information among the nodes of users and items. However, these methods need to collect personal interaction data between a user and the corresponding items and then model them in a central server, which would break the privacy laws such as GDPR. So far, no existing work can construct a global graph without leaking each user's private interaction data (i.e., his or her subgraph). In this paper, we are the first to design a novel lossless federated recommendation framework based on GNN, which achieves full-graph training with complete high-order structure information, enabling the training process to be equivalent to the corresponding un-federated counterpart. In addition, we use LightGCN to instantiate an example of our framework and show its equivalence.
</details>
<details>
<summary>摘要</summary>
graph neural networks (GNNs) 已经在推荐系统中得到广泛应用，因为它们可以捕捉用户和物品之间的高阶结构信息。然而，这些方法需要收集每个用户与对应的物品之间的个人互动数据，然后模型在中央服务器上，这会违反隐私法规，如GDPR。目前没有任何现有的工作可以构建全球图without泄露每个用户的私人互动数据（即他或她的子图）。在这篇论文中，我们是首次设计了一种新的无损联邦推荐框架基于GNN，实现了全图训练，并保留了高阶结构信息完整性。此外，我们使用LightGCN来实现这种框架的示例，并证明其等价性。
</details></li>
</ul>
<hr>
<h2 id="Safety-Margins-for-Reinforcement-Learning"><a href="#Safety-Margins-for-Reinforcement-Learning" class="headerlink" title="Safety Margins for Reinforcement Learning"></a>Safety Margins for Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13642">http://arxiv.org/abs/2307.13642</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Grushin, Walt Woods, Alvaro Velasquez, Simon Khan</li>
<li>For: The paper is written for identifying and mitigating unsafe situations in autonomous controllers, particularly in freight transportation applications.* Methods: The paper proposes using a proxy criticality metric that can be computed in real-time to identify when an agent is approaching a potentially catastrophic situation. The metric is based on the mean reduction in reward given some number of random actions.* Results: The paper demonstrates the effectiveness of its approach by evaluating it on learned policies from APE-X and A3C within an Atari environment. The results show that safety margins decrease as agents approach failure states, indicating the potential for catastrophic outcomes.<details>
<summary>Abstract</summary>
Any autonomous controller will be unsafe in some situations. The ability to quantitatively identify when these unsafe situations are about to occur is crucial for drawing timely human oversight in, e.g., freight transportation applications. In this work, we demonstrate that the true criticality of an agent's situation can be robustly defined as the mean reduction in reward given some number of random actions. Proxy criticality metrics that are computable in real-time (i.e., without actually simulating the effects of random actions) can be compared to the true criticality, and we show how to leverage these proxy metrics to generate safety margins, which directly tie the consequences of potentially incorrect actions to an anticipated loss in overall performance. We evaluate our approach on learned policies from APE-X and A3C within an Atari environment, and demonstrate how safety margins decrease as agents approach failure states. The integration of safety margins into programs for monitoring deployed agents allows for the real-time identification of potentially catastrophic situations.
</details>
<details>
<summary>摘要</summary>
任何自主控制器都会在某些情况下不安全。能够量化地识别这些不安全情况的发生是对于启动人工监督的时间而言至关重要，例如在货物运输应用中。在这项工作中，我们示出了一种真实的批处性可以坚定地定义为一些随机动作后的奖励减少的平均值。我们称这种代理批处性指标可以在实时（无需实际模拟动作的影响）计算，并且可以与真实批处性进行比较。我们还示出了如何利用这些代理指标生成安全余地，这些安全余地直接反映了可能 incorrect 动作的后果和预期的性能损失。我们在 APE-X 和 A3C 学习政策中的 Atari 环境中评估了我们的方法，并示出了安全余地如何随agent接近失败状态而逐渐减少。将安全余地集成到部署过程中的监控程序中，可以实现实时识别可能 catastrophic 的情况。
</details></li>
</ul>
<hr>
<h2 id="DBGSA-A-Novel-Data-Adaptive-Bregman-Clustering-Algorithm"><a href="#DBGSA-A-Novel-Data-Adaptive-Bregman-Clustering-Algorithm" class="headerlink" title="DBGSA: A Novel Data Adaptive Bregman Clustering Algorithm"></a>DBGSA: A Novel Data Adaptive Bregman Clustering Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14375">http://arxiv.org/abs/2307.14375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ying Xiao, Hou-biao Li, Yu-pu Zhang</li>
<li>for: 提高非对称数据集中 clustering 算法的精度和稳定性。</li>
<li>methods: 提议一种基于 Bregman 差分参数优化的数据驱动 clustering 算法 (DBGSA)， combines 宇宙 gravitational algorithm 将相似点靠拢在数据集中。 构造了 gravitational coefficient equation  WITH special property 逐步减少影响因子。 使用 Bregman divergence generalized power mean information loss minimization 标识群集中心。</li>
<li>results: 对四个 simulated 数据集和六个实际数据集进行了广泛的实验，结果表明 DBGSA 在不同 clustering 算法中的准确率平均提高了63.8%，与其他类似方法和改进的数据集相比。 还设立了三维网格搜索来比较不同参数值的影响，发现我们的模型提供的参数集是优化的。<details>
<summary>Abstract</summary>
With the development of Big data technology, data analysis has become increasingly important. Traditional clustering algorithms such as K-means are highly sensitive to the initial centroid selection and perform poorly on non-convex datasets. In this paper, we address these problems by proposing a data-driven Bregman divergence parameter optimization clustering algorithm (DBGSA), which combines the Universal Gravitational Algorithm to bring similar points closer in the dataset. We construct a gravitational coefficient equation with a special property that gradually reduces the influence factor as the iteration progresses. Furthermore, we introduce the Bregman divergence generalized power mean information loss minimization to identify cluster centers and build a hyperparameter identification optimization model, which effectively solves the problems of manual adjustment and uncertainty in the improved dataset. Extensive experiments are conducted on four simulated datasets and six real datasets. The results demonstrate that DBGSA significantly improves the accuracy of various clustering algorithms by an average of 63.8\% compared to other similar approaches like enhanced clustering algorithms and improved datasets. Additionally, a three-dimensional grid search was established to compare the effects of different parameter values within threshold conditions, and it was discovered the parameter set provided by our model is optimal. This finding provides strong evidence of the high accuracy and robustness of the algorithm.
</details>
<details>
<summary>摘要</summary>
随着大数据技术的发展，数据分析已成为非常重要。传统的聚类算法如K-means受初始中心选择的影响很大，在非对称数据集上表现不佳。在这篇论文中，我们解决这些问题，提出一种基于数据驱动的Bregman差分参数优化聚类算法（DBGSA）。我们将 Universal Gravitational Algorithm 用于将相似点靠拢在数据集中。我们构建了一个引力系数方程，其特点是逐步减少影响因子。此外，我们引入Bregman差分总能平均信息损失来确定聚类中心，并构建了一个超参数标准化模型，可以有效解决手动调整和不确定性问题。我们在四个 simulated 数据集和六个实际数据集上进行了广泛的实验。结果表明，DBGSA可以在不同的聚类算法下提高准确率的平均提升率为63.8%，比其他相似方法的改进数据集和优化超参数更高。此外，我们设立了三维网格搜索，并发现我们的模型提供的参数集是优化的。这一结论提供了高精度和稳定性的证据。
</details></li>
</ul>
<hr>
<h2 id="Turning-hazardous-volatile-matter-compounds-into-fuel-by-catalytic-steam-reforming-An-evolutionary-machine-learning-approach"><a href="#Turning-hazardous-volatile-matter-compounds-into-fuel-by-catalytic-steam-reforming-An-evolutionary-machine-learning-approach" class="headerlink" title="Turning hazardous volatile matter compounds into fuel by catalytic steam reforming: An evolutionary machine learning approach"></a>Turning hazardous volatile matter compounds into fuel by catalytic steam reforming: An evolutionary machine learning approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05750">http://arxiv.org/abs/2308.05750</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alireza Shafizadeh, Hossein Shahbeik, Mohammad Hossein Nadian, Vijai Kumar Gupta, Abdul-Sattar Nizami, Su Shiung Lam, Wanxi Peng, Junting Pan, Meisam Tabatabaei, Mortaza Aghbashlo</li>
<li>for: 这种研究旨在开发一种基于机器学习的研究框架，用于模拟、理解和优化 catalytic steam reforming 过程中的材料和反应条件。</li>
<li>methods: 该研究使用了 X-ray diffraction analysis 等化学&#x2F;Texture analysis 获取输入特征，并使用了 Literature compile 一个包括多种催化剂特性和反应条件的数据库。研究采用了 six 种机器学习模型，并使用了粒子群搜索算法进行优化。</li>
<li>results: 研究结果表明， ensemble 机器学习模型可以提供高度预测性（R2 &gt; 0.976） для toluene 转化和产物分布。最佳的 tar 转化率高于 77.2% 可以在 637.44-725.62 ℃ 的温度范围内实现，催化剂 BET 表面积在 476.03-638.55 m2&#x2F;g 范围内。<details>
<summary>Abstract</summary>
Chemical and biomass processing systems release volatile matter compounds into the environment daily. Catalytic reforming can convert these compounds into valuable fuels, but developing stable and efficient catalysts is challenging. Machine learning can handle complex relationships in big data and optimize reaction conditions, making it an effective solution for addressing the mentioned issues. This study is the first to develop a machine-learning-based research framework for modeling, understanding, and optimizing the catalytic steam reforming of volatile matter compounds. Toluene catalytic steam reforming is used as a case study to show how chemical/textural analyses (e.g., X-ray diffraction analysis) can be used to obtain input features for machine learning models. Literature is used to compile a database covering a variety of catalyst characteristics and reaction conditions. The process is thoroughly analyzed, mechanistically discussed, modeled by six machine learning models, and optimized using the particle swarm optimization algorithm. Ensemble machine learning provides the best prediction performance (R2 > 0.976) for toluene conversion and product distribution. The optimal tar conversion (higher than 77.2%) is obtained at temperatures between 637.44 and 725.62 {\deg}C, with a steam-to-carbon molar ratio of 5.81-7.15 and a catalyst BET surface area 476.03-638.55 m2/g. The feature importance analysis satisfactorily reveals the effects of input descriptors on model prediction. Operating conditions (50.9%) and catalyst properties (49.1%) are equally important in modeling. The developed framework can expedite the search for optimal catalyst characteristics and reaction conditions, not only for catalytic chemical processing but also for related research areas.
</details>
<details>
<summary>摘要</summary>
化学和生物质处理系统每天都会发布可燃物质分子到环境中。catalytic reforming可以将这些分子转化为有价值的燃料，但是开发稳定和高效的催化剂是挑战。机器学习可以处理复杂的关系在大数据中，因此可以成为改进反应条件的有效解决方案。这项研究是首次开发了基于机器学习研究框架，用于模拟、理解和优化催化液气 reforming的气相催化剂。使用苯酚为例，通过化学/文化分析（例如X射晶体分析）获得输入特征，并使用文献编译一份包括多种催化剂特性和反应条件的数据库。通过综合分析、机制分析、六种机器学习模型和粒子群优化算法，我们获得了最佳的苯酚转化率（高于77.2%）和产品分布。 ensemble机器学习提供了最佳预测性能（R2 > 0.976）。最佳的沸腾 conversions（高于77.2%）在637.44-725.62℃的温度范围内，催化剂BET表面积476.03-638.55 m2/g。特征重要性分析准确地显示输入特征对模型预测的影响。操作条件（50.9%）和催化剂性质（49.1%）具有相等的重要性。我们开发的框架可以减少催化剂特性和反应条件的搜索，不仅限于催化化学处理，还可以应用于相关领域的研究。
</details></li>
</ul>
<hr>
<h2 id="Scaling-machine-learning-based-chemical-plant-simulation-A-method-for-fine-tuning-a-model-to-induce-stable-fixed-points"><a href="#Scaling-machine-learning-based-chemical-plant-simulation-A-method-for-fine-tuning-a-model-to-induce-stable-fixed-points" class="headerlink" title="Scaling machine learning-based chemical plant simulation: A method for fine-tuning a model to induce stable fixed points"></a>Scaling machine learning-based chemical plant simulation: A method for fine-tuning a model to induce stable fixed points</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13621">http://arxiv.org/abs/2307.13621</a></li>
<li>repo_url: None</li>
<li>paper_authors: Malte Esders, Gimmy Alex Fernandez Ramirez, Michael Gastegger, Satya Swarup Samal</li>
<li>for: 这篇论文是为了提出一种基于机器学习（ML）的化学厂模型，以取代理性的首要原理模型。</li>
<li>methods: 这篇论文使用了一种结构化的方法，即每个厂区域都由一个ML模型表示。然后，这些模型被连接到一个流程图像中，以便进行数据预测。</li>
<li>results: 作者发现，对于较小的厂房，这种方法效果非常好，但是对于更大的厂房，由于巨大的循环逻辑引起的循环解决问题会导致不稳定。作者分析了这个问题，并提出了一种方法来细调ML模型，以便通过常规方法解决循环问题。<details>
<summary>Abstract</summary>
Idealized first-principles models of chemical plants can be inaccurate. An alternative is to fit a Machine Learning (ML) model directly to plant sensor data. We use a structured approach: Each unit within the plant gets represented by one ML model. After fitting the models to the data, the models are connected into a flowsheet-like directed graph. We find that for smaller plants, this approach works well, but for larger plants, the complex dynamics arising from large and nested cycles in the flowsheet lead to instabilities in the cycle solver. We analyze this problem in depth and show that it is not merely a specialized concern but rather a more pervasive challenge that will likely occur whenever ML is applied to larger plants. To address this problem, we present a way to fine-tune ML models such that solving cycles with the usual methods becomes robust again.
</details>
<details>
<summary>摘要</summary>
理想化的基本原理模型可能不准确。一种alternative是直接将机器学习（ML）模型适应到厂区传感器数据。我们采用一种结构化方法：每个喷槽内部的设备都被一个ML模型表示。在模型适应数据后，模型被连接成一个流程图像类似的导向图。我们发现对小型厂区来说，这种方法工作良好，但对更大的厂区来说，由于大量和嵌入的循环在流程图中导致循环解决器的不稳定。我们对这个问题进行深入分析，并证明这不仅是特殊情况，而是更广泛的挑战， ML在更大的厂区中应用时会遇到这种问题。为解决这个问题，我们提出了一种细化ML模型的方法，使得通过常规方法解决循环变得稳定。
</details></li>
</ul>
<hr>
<h2 id="AI-and-ethics-in-insurance-a-new-solution-to-mitigate-proxy-discrimination-in-risk-modeling"><a href="#AI-and-ethics-in-insurance-a-new-solution-to-mitigate-proxy-discrimination-in-risk-modeling" class="headerlink" title="AI and ethics in insurance: a new solution to mitigate proxy discrimination in risk modeling"></a>AI and ethics in insurance: a new solution to mitigate proxy discrimination in risk modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13616">http://arxiv.org/abs/2307.13616</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marguerite Sauce, Antoine Chancel, Antoine Ly</li>
<li>for: 这个论文的目的是探讨如何使用机器学习算法实现更公平的保险业务，以及如何避免对保险公司的偏见。</li>
<li>methods: 这个论文使用了欧洲人权公约中关于歧视的指南，以及使用敏感个人数据在算法中的regulation。它还提出了一种新的方法，基于数 Linear Algebra的概念，以降低对直接歧视的风险。</li>
<li>results: 该论文的研究表明，使用这种新方法可以减少对直接歧视的风险，同时保持保险公司的资产风险评估和价格段化的精度。这种方法的使用也是简单易用，并且在一个具体的生命保险选择中得到了Promising的性能。<details>
<summary>Abstract</summary>
The development of Machine Learning is experiencing growing interest from the general public, and in recent years there have been numerous press articles questioning its objectivity: racism, sexism, \dots Driven by the growing attention of regulators on the ethical use of data in insurance, the actuarial community must rethink pricing and risk selection practices for fairer insurance. Equity is a philosophy concept that has many different definitions in every jurisdiction that influence each other without currently reaching consensus. In Europe, the Charter of Fundamental Rights defines guidelines on discrimination, and the use of sensitive personal data in algorithms is regulated. If the simple removal of the protected variables prevents any so-called `direct' discrimination, models are still able to `indirectly' discriminate between individuals thanks to latent interactions between variables, which bring better performance (and therefore a better quantification of risk, segmentation of prices, and so on). After introducing the key concepts related to discrimination, we illustrate the complexity of quantifying them. We then propose an innovative method, not yet met in the literature, to reduce the risks of indirect discrimination thanks to mathematical concepts of linear algebra. This technique is illustrated in a concrete case of risk selection in life insurance, demonstrating its simplicity of use and its promising performance.
</details>
<details>
<summary>摘要</summary>
机器学习的发展正在吸引越来越多的一般大众注意，最近几年有许多新闻报导质疑其中的公正性： racism、性别歧视、等等。随着规制机关对数据的伦理使用的关注，保险业界必须重新思考定价和风险选择实践，以确保更公正的保险。“公正”是一个哲学概念，在每个司法管辖区域都有不同的定义，这些定义彼此影响并未能达成现在的共识。在欧洲，《权利宪法》提供了关于歧视的指南，而使用敏感个人数据在算法中的使用则被规管。即使简单地删除保护变数，模型仍然能够间接歧视个人，因为变数之间的隐藏互动带来更好的性能（并因此更好地评估风险、价格分类等）。我们首先介绍了歧视的定义，然后详细介绍了该现象的复杂性。接着，我们提出了一种新的方法，未经过文献中的应用，以减少间接歧视的风险。这种方法基于数学概念的线性代数，并在生命保险中的风险选择中实现了简单且有前途的使用。
</details></li>
</ul>
<hr>
<h2 id="Team-Intro-to-AI-team8-at-CoachAI-Badminton-Challenge-2023-Advanced-ShuttleNet-for-Shot-Predictions"><a href="#Team-Intro-to-AI-team8-at-CoachAI-Badminton-Challenge-2023-Advanced-ShuttleNet-for-Shot-Predictions" class="headerlink" title="Team Intro to AI team8 at CoachAI Badminton Challenge 2023: Advanced ShuttleNet for Shot Predictions"></a>Team Intro to AI team8 at CoachAI Badminton Challenge 2023: Advanced ShuttleNet for Shot Predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13715">http://arxiv.org/abs/2307.13715</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shih-Hong Chen, Pin-Hsuan Chou, Yong-Fu Liu, Chien-An Han</li>
<li>for: 提高现有框架ShuttleNet在预测羽毛球发球类型和位置的性能，通过利用过去的撕击。</li>
<li>methods: 利用过去的撕击来预测羽毛球发球类型和位置。</li>
<li>results: 在IJCAI 2023的CoachAI Badminton Challenge中获得了明显更好的结果，比基eline更高，最终取得了比赛的第一名，代码也公开了。<details>
<summary>Abstract</summary>
In this paper, our objective is to improve the performance of the existing framework ShuttleNet in predicting badminton shot types and locations by leveraging past strokes. We participated in the CoachAI Badminton Challenge at IJCAI 2023 and achieved significantly better results compared to the baseline. Ultimately, our team achieved the first position in the competition and we made our code available.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们的目标是通过利用过去的击球来提高现有框架ShuttleNet在预测羽毛球shot类型和位置的性能。我们参加了IJCAI 2023年的CoachAI Badminton Challenge，并实现了相比基eline的显著提高。最终，我们的团队取得了比赛的第一名，并将代码公开。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Forecasting-capturing-and-activation-of-carbon-dioxide-CO-2-Integration-of-Time-Series-Analysis-Machine-Learning-and-Material-Design"><a href="#Forecasting-capturing-and-activation-of-carbon-dioxide-CO-2-Integration-of-Time-Series-Analysis-Machine-Learning-and-Material-Design" class="headerlink" title="Forecasting, capturing and activation of carbon-dioxide (CO$_2$): Integration of Time Series Analysis, Machine Learning, and Material Design"></a>Forecasting, capturing and activation of carbon-dioxide (CO$_2$): Integration of Time Series Analysis, Machine Learning, and Material Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14374">http://arxiv.org/abs/2307.14374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suchetana Sadhukhan, Vivek Kumar Yadav</li>
<li>For: This study provides a comprehensive time series analysis of daily industry-specific, country-wise CO2 emissions from January 2019 to February 2023, with a focus on European countries (EU27 &amp; UK, Italy, Germany, Spain) and India.* Methods: The research uses near-real-time activity data from the Carbon Monitor research initiative, and performs a principal component analysis (PCA) to determine the key contributors to CO2 emissions. The study also employs a 7-day moving averaged dataset for further analysis and uses Long Short-Term Memory (LSTM) models to predict emissions.* Results: The study finds that the Power, Industry, and Ground Transport sectors account for a significant portion of the variance in the dataset, and the LSTM models achieve high efficiency with $R^2$ values ranging from 0.8242 to 0.995 for various countries and sectors. Additionally, the study proposes the use of scandium and boron&#x2F;aluminium-based thin films as exceptionally efficient materials for capturing CO2.<details>
<summary>Abstract</summary>
This study provides a comprehensive time series analysis of daily industry-specific, country-wise CO$_2$ emissions from January 2019 to February 2023. The research focuses on the Power, Industry, Ground Transport, Domestic Aviation, and International Aviation sectors in European countries (EU27 & UK, Italy, Germany, Spain) and India, utilizing near-real-time activity data from the Carbon Monitor research initiative. To identify regular emission patterns, the data from the year 2020 is excluded due to the disruptive effects caused by the COVID-19 pandemic. The study then performs a principal component analysis (PCA) to determine the key contributors to CO$_2$ emissions. The analysis reveals that the Power, Industry, and Ground Transport sectors account for a significant portion of the variance in the dataset. A 7-day moving averaged dataset is employed for further analysis to facilitate robust predictions. This dataset captures both short-term and long-term trends and enhances the quality of the data for prediction purposes. The study utilizes Long Short-Term Memory (LSTM) models on the 7-day moving averaged dataset to effectively predict emissions and provide insights for policy decisions, mitigation strategies, and climate change efforts. During the training phase, the stability and convergence of the LSTM models are ensured, which guarantees their reliability in the testing phase. The evaluation of the loss function indicates this reliability. The model achieves high efficiency, as demonstrated by $R^2$ values ranging from 0.8242 to 0.995 for various countries and sectors. Furthermore, there is a proposal for utilizing scandium and boron/aluminium-based thin films as exceptionally efficient materials for capturing CO$_2$ (with a binding energy range from -3.0 to -3.5 eV). These materials are shown to surpass the affinity of graphene and boron nitride sheets in this regard.
</details>
<details>
<summary>摘要</summary>
To facilitate robust predictions, a 7-day moving averaged dataset is employed for further analysis. This dataset captures both short-term and long-term trends and enhances the quality of the data for prediction purposes. The study utilizes Long Short-Term Memory (LSTM) models on the 7-day moving averaged dataset to effectively predict emissions and provide insights for policy decisions, mitigation strategies, and climate change efforts. The stability and convergence of the LSTM models are ensured during the training phase, which guarantees their reliability in the testing phase. The evaluation of the loss function indicates this reliability.The model achieves high efficiency, as demonstrated by $R^2$ values ranging from 0.8242 to 0.995 for various countries and sectors. Furthermore, the study proposes the use of scandium and boron/aluminum-based thin films as exceptionally efficient materials for capturing CO2. These materials are shown to surpass the affinity of graphene and boron nitride sheets in this regard.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://nullscc.github.io/2023/07/26/cs.LG_2023_07_26/" data-id="cllshxsod002c2u88g8rwes0u" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/27/eess.IV_2023_07_27/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.IV - 2023-07-27 17:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/26/cs.SD_2023_07_26/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-07-26 123:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'', root:''}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
