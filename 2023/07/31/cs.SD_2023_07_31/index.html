
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-07-31 123:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Audio-Visual Segmentation by Exploring Cross-Modal Mutual Semantics paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.16620 repo_url: None paper_authors: Chen Liu, Peike Li, Xingqun Qi, Hu Zhang, Lincheng Li, Dado">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-07-31 123:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/07/31/cs.SD_2023_07_31/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Audio-Visual Segmentation by Exploring Cross-Modal Mutual Semantics paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.16620 repo_url: None paper_authors: Chen Liu, Peike Li, Xingqun Qi, Hu Zhang, Lincheng Li, Dado">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-30T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:23.306Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_07_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/31/cs.SD_2023_07_31/" class="article-date">
  <time datetime="2023-07-30T16:00:00.000Z" itemprop="datePublished">2023-07-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-07-31 123:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Audio-Visual-Segmentation-by-Exploring-Cross-Modal-Mutual-Semantics"><a href="#Audio-Visual-Segmentation-by-Exploring-Cross-Modal-Mutual-Semantics" class="headerlink" title="Audio-Visual Segmentation by Exploring Cross-Modal Mutual Semantics"></a>Audio-Visual Segmentation by Exploring Cross-Modal Mutual Semantics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16620">http://arxiv.org/abs/2307.16620</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Liu, Peike Li, Xingqun Qi, Hu Zhang, Lincheng Li, Dadong Wang, Xin Yu</li>
<li>for: 本研究旨在解决现有Audio-Visual Segmentation（AVS）方法受 dataset bias 的问题，即尝试将各种视频中的各种对象都与音频信号相关联，即使这些对象并不是真正的听音对象。</li>
<li>methods: 我们提出了一种Audio-VisualInstance-Aware Segmentation方法，包括两个关键步骤：首先，使用对象分割网络对视频中的对象进行分割；其次，将音频信号与可能的听音对象相关联。为了解决对象分割网络的混淆，我们提出了一种静音对象检测目标函数。此外，我们还提出了一种基于音频视频语义相关性的音频分类目标函数，以便将音频信号与听音对象相关联。</li>
<li>results: 我们的方法在AVS数据集上进行实验，结果表明，我们的方法可以准确地分割听音对象，不受dataset bias的影响。<details>
<summary>Abstract</summary>
The audio-visual segmentation (AVS) task aims to segment sounding objects from a given video. Existing works mainly focus on fusing audio and visual features of a given video to achieve sounding object masks. However, we observed that prior arts are prone to segment a certain salient object in a video regardless of the audio information. This is because sounding objects are often the most salient ones in the AVS dataset. Thus, current AVS methods might fail to localize genuine sounding objects due to the dataset bias. In this work, we present an audio-visual instance-aware segmentation approach to overcome the dataset bias. In a nutshell, our method first localizes potential sounding objects in a video by an object segmentation network, and then associates the sounding object candidates with the given audio. We notice that an object could be a sounding object in one video but a silent one in another video. This would bring ambiguity in training our object segmentation network as only sounding objects have corresponding segmentation masks. We thus propose a silent object-aware segmentation objective to alleviate the ambiguity. Moreover, since the category information of audio is unknown, especially for multiple sounding sources, we propose to explore the audio-visual semantic correlation and then associate audio with potential objects. Specifically, we attend predicted audio category scores to potential instance masks and these scores will highlight corresponding sounding instances while suppressing inaudible ones. When we enforce the attended instance masks to resemble the ground-truth mask, we are able to establish audio-visual semantics correlation. Experimental results on the AVS benchmarks demonstrate that our method can effectively segment sounding objects without being biased to salient objects.
</details>
<details>
<summary>摘要</summary>
audio-visual分 segmentation（AVS）任务的目标是从视频中分割听起的对象。现有的方法主要是将视频和听起的特征进行融合，以实现听起的对象面积。然而，我们发现现有的方法容易因为数据偏见而分割某些突出的对象，这是因为听起的对象通常是视频中最突出的对象。因此，当前的AVS方法可能无法准确地 lokalisieren真正的听起的对象，这是因为数据偏见。在这个工作中，我们提出了一种audio-visual实例特征分割方法，以解决数据偏见问题。总的来说，我们的方法首先在视频中 lokalisieren潜在的听起对象，然后将这些对象与给定的听起相关联。我们注意到，一个对象可能在一个视频中是听起的对象，而在另一个视频中是无声的对象。这会导致我们的对象分割网络在训练时存在含糊不清的问题，因为只有听起的对象有相应的分割面积。我们因此提出了一种无声对象特征分割目标，以解决这个问题。此外，由于听起的音频类别信息未知，特别是多个听起源，我们提出了探索音频-视频semantic相关性，然后将音频与潜在对象相关联。具体来说，我们将预测的音频类别分数attend到 potential实例面积，这些分数会高亮对应的听起实例，而suppress不可见的实例。当我们强制enforce attend instance masks和ground truth mask相似，我们就能够建立音频-视频semantic相关性。实验结果表明，我们的方法可以效果地分割听起的对象，不受数据偏见的影响。
</details></li>
</ul>
<hr>
<h2 id="SAMbA-Speech-enhancement-with-Asynchronous-ad-hoc-Microphone-Arrays"><a href="#SAMbA-Speech-enhancement-with-Asynchronous-ad-hoc-Microphone-Arrays" class="headerlink" title="SAMbA: Speech enhancement with Asynchronous ad-hoc Microphone Arrays"></a>SAMbA: Speech enhancement with Asynchronous ad-hoc Microphone Arrays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16582">http://arxiv.org/abs/2307.16582</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas Furnon, Romain Serizel, Slim Essid, Irina Illina</li>
<li>for: 提高随机麦克风数组中的语音增强，即使麦克风数组中的设备具有不同的采样时间和采样速率偏差。</li>
<li>methods: 使用深度神经网络（DNN）来实现语音增强，该方法适用于随机麦克风数组，并能够忍受不同设备的采样时间和采样速率偏差。我们使用注意力机制，使DNN和整个管道具有对偏差的抗性。</li>
<li>results: 我们表明，使用注意力机制可以使DNN和整个管道具有对偏差的抗性，并且不需要进行费时的数据同步。此外，我们还发现，注意力机制可以自动地学习出偏差参数，无需进行supervised学习。<details>
<summary>Abstract</summary>
Speech enhancement in ad-hoc microphone arrays is often hindered by the asynchronization of the devices composing the microphone array. Asynchronization comes from sampling time offset and sampling rate offset which inevitably occur when the microphones are embedded in different hardware components. In this paper, we propose a deep neural network (DNN)-based speech enhancement solution that is suited for applications in ad-hoc microphone arrays because it is distributed and copes with asynchronization. We show that asynchronization has a limited impact on the spatial filtering and mostly affects the performance of the DNNs. Instead of resynchronising the signals, which requires costly processing steps, we use an attention mechanism which makes the DNNs, thus our whole pipeline, robust to asynchronization. We also show that the attention mechanism leads to the asynchronization parameters in an unsupervised manner.
</details>
<details>
<summary>摘要</summary>
听音提升在协作式麦克麦得组中经常受到设备组成部件之间的不同步问题的干扰。不同步问题来自探测时间偏移和探测速率偏移，这些偏移是不可避免的，因为麦克麦得在不同的硬件组件中嵌入。在这篇论文中，我们提出了一种基于深度神经网络（DNN）的听音提升解决方案，适用于协作式麦克麦得组中的应用。我们表明，不同步问题对听音提升的空间滤波几乎没有影响，主要影响DNN的性能。而不如同步化信号，这需要成本高昂的处理步骤，我们使用了注意力机制，使DNNs、也就是我们整个管道，对不同步问题进行抗性。此外，我们还表明了注意力机制对不同步问题的无监督学习。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Conditional-Latent-Diffusion-for-Audio-visual-Segmentation"><a href="#Contrastive-Conditional-Latent-Diffusion-for-Audio-visual-Segmentation" class="headerlink" title="Contrastive Conditional Latent Diffusion for Audio-visual Segmentation"></a>Contrastive Conditional Latent Diffusion for Audio-visual Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16579">http://arxiv.org/abs/2307.16579</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxin Mao, Jing Zhang, Mochu Xiang, Yunqiu Lv, Yiran Zhong, Yuchao Dai<br>for:The paper is written for exploring the contribution of audio in audio-visual segmentation (AVS) tasks, and proposing a latent diffusion model with contrastive learning to explicitly maximize the contribution of audio.methods:The paper uses a latent diffusion model with contrastive learning to learn a semantic-correlated representation of audio and visual features, and to ensure that the conditional variable (audio) contributes to the final segmentation map.results:Experimental results on a benchmark dataset verify the effectiveness of the proposed solution, demonstrating improved performance in AVS tasks using the proposed method. The code and results are available online via the project page: <a target="_blank" rel="noopener" href="https://github.com/OpenNLPLab/DiffusionAVS">https://github.com/OpenNLPLab/DiffusionAVS</a>.Here’s the Chinese version:for:这篇论文是为了探讨音频在音视频分割（AVS）任务中的贡献，并提出一种含批学模型来显著地增强音频的贡献。methods:这篇论文使用含批学模型来学习音视频特征之间的含义相关性，并确保条件变量（音频）对最终分割图的贡献。results:对于一个 benchmark 数据集，实验结果表明提出的解决方案有效，在 AVS 任务中使用提posed方法可以提高性能。代码和结果可以在项目页面上获取：<a target="_blank" rel="noopener" href="https://github.com/OpenNLPLab/DiffusionAVS">https://github.com/OpenNLPLab/DiffusionAVS</a>.<details>
<summary>Abstract</summary>
We propose a latent diffusion model with contrastive learning for audio-visual segmentation (AVS) to extensively explore the contribution of audio. We interpret AVS as a conditional generation task, where audio is defined as the conditional variable for sound producer(s) segmentation. With our new interpretation, it is especially necessary to model the correlation between audio and the final segmentation map to ensure its contribution. We introduce a latent diffusion model to our framework to achieve semantic-correlated representation learning. Specifically, our diffusion model learns the conditional generation process of the ground-truth segmentation map, leading to ground-truth aware inference when we perform the denoising process at the test stage. As a conditional diffusion model, we argue it is essential to ensure that the conditional variable contributes to model output. We then introduce contrastive learning to our framework to learn audio-visual correspondence, which is proven consistent with maximizing the mutual information between model prediction and the audio data. In this way, our latent diffusion model via contrastive learning explicitly maximizes the contribution of audio for AVS. Experimental results on the benchmark dataset verify the effectiveness of our solution. Code and results are online via our project page: https://github.com/OpenNLPLab/DiffusionAVS.
</details>
<details>
<summary>摘要</summary>
我们提出了一种具有对比学习的潜在扩散模型，用于广泛探索音频的贡献。我们将音频视为条件变量，用于音频生产者 segmentation 的定义。我们新的解释需要模型音频和最终分割图的 correlation，以确保其贡献。我们引入了一种潜在扩散模型到我们的框架，以实现含义相关的表示学习。具体来说，我们的扩散模型学习了真实分割图的生成过程，使得在测试阶段进行锈除操作时，能够保证模型的输出具有真实分割图的意义。作为一个条件扩散模型，我们认为其必须确保条件变量对模型输出做出贡献。我们然后引入了对比学习，以学习音频和视频之间的对应关系，这已经被证明可以具有最大化模型预测和音频数据之间的共同信息。这样，我们的潜在扩散模型通过对比学习显著地提高了音频对 AVS 的贡献。实验结果表明我们的解决方案的效果。代码和结果可以在我们项目页面上找到：https://github.com/OpenNLPLab/DiffusionAVS。
</details></li>
</ul>
<hr>
<h2 id="DiffProsody-Diffusion-based-Latent-Prosody-Generation-for-Expressive-Speech-Synthesis-with-Prosody-Conditional-Adversarial-Training"><a href="#DiffProsody-Diffusion-based-Latent-Prosody-Generation-for-Expressive-Speech-Synthesis-with-Prosody-Conditional-Adversarial-Training" class="headerlink" title="DiffProsody: Diffusion-based Latent Prosody Generation for Expressive Speech Synthesis with Prosody Conditional Adversarial Training"></a>DiffProsody: Diffusion-based Latent Prosody Generation for Expressive Speech Synthesis with Prosody Conditional Adversarial Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16549">http://arxiv.org/abs/2307.16549</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hsoh0306/diffprosody">https://github.com/hsoh0306/diffprosody</a></li>
<li>paper_authors: Hyung-Seok Oh, Sang-Hoon Lee, Seong-Whan Lee</li>
<li>for: 这个研究旨在提高表达系统的表达性能，尤其是在考虑到谱系模型的问题。</li>
<li>methods: 该研究使用了一种名为DiffProsody的新方法，它使用了扩散基本的约束逻辑生成器和抽象训练来生成表达。</li>
<li>results: 实验结果表明，DiffProsody可以在16倍的速度上生成表达，并且表现出了更高的质量和精度。<details>
<summary>Abstract</summary>
Expressive text-to-speech systems have undergone significant advancements owing to prosody modeling, but conventional methods can still be improved. Traditional approaches have relied on the autoregressive method to predict the quantized prosody vector; however, it suffers from the issues of long-term dependency and slow inference. This study proposes a novel approach called DiffProsody in which expressive speech is synthesized using a diffusion-based latent prosody generator and prosody conditional adversarial training. Our findings confirm the effectiveness of our prosody generator in generating a prosody vector. Furthermore, our prosody conditional discriminator significantly improves the quality of the generated speech by accurately emulating prosody. We use denoising diffusion generative adversarial networks to improve the prosody generation speed. Consequently, DiffProsody is capable of generating prosody 16 times faster than the conventional diffusion model. The superior performance of our proposed method has been demonstrated via experiments.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传统方法仅仅可以预测衡量vector，但是它们受到长期依赖和慢速预测的限制。这种研究提出了一种新的方法，即DiffProsody，它使用扩散基于秘密词法生成器和词法强制约束 adversarial training来生成表达主义的speech。我们的发现表明，DiffProsody可以快速生成高质量的speech，并且可以准确地模仿词法。我们使用了denoising扩散生成 adversarial networks来提高词法生成速度，因此DiffProsody可以在16倍之前生成词法。经过实验，我们证明了DiffProsody的优秀性。Note: Please note that the translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that form instead.
</details></li>
</ul>
<hr>
<h2 id="SpatialNet-Extensively-Learning-Spatial-Information-for-Multichannel-Joint-Speech-Separation-Denoising-and-Dereverberation"><a href="#SpatialNet-Extensively-Learning-Spatial-Information-for-Multichannel-Joint-Speech-Separation-Denoising-and-Dereverberation" class="headerlink" title="SpatialNet: Extensively Learning Spatial Information for Multichannel Joint Speech Separation, Denoising and Dereverberation"></a>SpatialNet: Extensively Learning Spatial Information for Multichannel Joint Speech Separation, Denoising and Dereverberation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16516">http://arxiv.org/abs/2307.16516</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/audio-westlakeu/nbss">https://github.com/audio-westlakeu/nbss</a></li>
<li>paper_authors: Changsheng Quan, Xiaofei Li</li>
<li>for: 这个论文旨在提出一种基于神经网络的多通道联合音声分离、减阈和抑抑抗抑谔（joint speech separation, denoising, and dereverberation）方法，名为空间网络（SpatialNet）。</li>
<li>methods: 该网络在短时傅里叶变换（STFT）域内进行端到端音声提高。它主要由交叠的窄频块和跨频块组成，分别利用窄频和跨频的空间信息。窄频块独立处理频率，使用自我注意机制和时间卷积层进行空间特征基本的 speaker 集群和时间平滑&#x2F;过滤。跨频块独立处理帧，使用全频线性层和频谱卷积层来分别学习所有频率和邻近频率之间的相关性。</li>
<li>results: 在多个模拟和实际数据集上进行了实验，研究结果表明：1）提出的网络实现了大多数任务的状态之冠性表现；2）提出的网络受到频谱泛化问题的影响很小；3）提出的网络确实进行了 speaker 集群（通过注意图表示）。<details>
<summary>Abstract</summary>
This work proposes a neural network to extensively exploit spatial information for multichannel joint speech separation, denoising and dereverberation, named SpatialNet.In the short-time Fourier transform (STFT) domain, the proposed network performs end-to-end speech enhancement. It is mainly composed of interleaved narrow-band and cross-band blocks to respectively exploit narrow-band and cross-band spatial information. The narrow-band blocks process frequencies independently, and use self-attention mechanism and temporal convolutional layers to respectively perform spatial-feature-based speaker clustering and temporal smoothing/filtering. The cross-band blocks processes frames independently, and use full-band linear layer and frequency convolutional layers to respectively learn the correlation between all frequencies and adjacent frequencies. Experiments are conducted on various simulated and real datasets, and the results show that 1) the proposed network achieves the state-of-the-art performance on almost all tasks; 2) the proposed network suffers little from the spectral generalization problem; and 3) the proposed network is indeed performing speaker clustering (demonstrated by attention maps).
</details>
<details>
<summary>摘要</summary>
这个工作提出了一种神经网络，旨在广泛利用空间信息进行多通道共同声音分离、减震和抑噪，称为空间网络（SpatialNet）。在短时傅立干（STFT）频域内，该网络实现了端到端声音提升。它主要由交叠式窄频和跨频块组成，分别利用窄频和跨频空间信息。窄频块独立处理频率，使用自注意机制和时间径向层来分别进行空间特征基于的speaker分组和时间平滑/过滤。跨频块独立处理帧，使用全频线性层和频率径向层来分别学习所有频率和邻近频率之间的相关性。在各种 simulate 和实际数据集上进行了实验，结果显示：1）提出的网络在大多数任务上达到了状态之arte的性能; 2）提出的网络受到相对较少的spectral泛化问题; 3）提出的网络实际进行了speaker clustering（通过注意力地图来证明）。
</details></li>
</ul>
<hr>
<h2 id="LP-MusicCaps-LLM-Based-Pseudo-Music-Captioning"><a href="#LP-MusicCaps-LLM-Based-Pseudo-Music-Captioning" class="headerlink" title="LP-MusicCaps: LLM-Based Pseudo Music Captioning"></a>LP-MusicCaps: LLM-Based Pseudo Music Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16372">http://arxiv.org/abs/2307.16372</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/seungheondoh/lp-music-caps">https://github.com/seungheondoh/lp-music-caps</a></li>
<li>paper_authors: SeungHeon Doh, Keunwoo Choi, Jongpil Lee, Juhan Nam</li>
<li>for: 提高音乐数据的理解和管理</li>
<li>methods: 使用大型自然语言模型生成描述句</li>
<li>results: 比超级参考模型提高音乐描述模型的性能<details>
<summary>Abstract</summary>
Automatic music captioning, which generates natural language descriptions for given music tracks, holds significant potential for enhancing the understanding and organization of large volumes of musical data. Despite its importance, researchers face challenges due to the costly and time-consuming collection process of existing music-language datasets, which are limited in size. To address this data scarcity issue, we propose the use of large language models (LLMs) to artificially generate the description sentences from large-scale tag datasets. This results in approximately 2.2M captions paired with 0.5M audio clips. We term it Large Language Model based Pseudo music caption dataset, shortly, LP-MusicCaps. We conduct a systemic evaluation of the large-scale music captioning dataset with various quantitative evaluation metrics used in the field of natural language processing as well as human evaluation. In addition, we trained a transformer-based music captioning model with the dataset and evaluated it under zero-shot and transfer-learning settings. The results demonstrate that our proposed approach outperforms the supervised baseline model.
</details>
<details>
<summary>摘要</summary>
自动化音乐标题生成，可以生成音乐轨道上的自然语言描述，具有潜在的大量音乐数据理解和组织化的潜力。然而，研究人员面临数据缺乏问题，因为现有的音乐语言数据集收集成本费时和成本高。为解决这个数据缺乏问题，我们提议使用大型语言模型（LLM）人工生成描述句子，从大规模标签数据集中获得约220万个描述句子和50万个音频剪辑。我们称之为大语言模型基于假音乐描述数据集（LP-MusicCaps）。我们进行了音乐描述集的系统评估，使用了自然语言处理领域常用的评价指标，以及人工评估。此外，我们将 transformer 型音乐描述模型与数据集进行训练，并在零基础和转移学习设置下评估其性能。结果表明，我们的提议方法在超过基线模型的情况下表现出色。
</details></li>
</ul>
<hr>
<h2 id="Mispronunciation-detection-using-self-supervised-speech-representations"><a href="#Mispronunciation-detection-using-self-supervised-speech-representations" class="headerlink" title="Mispronunciation detection using self-supervised speech representations"></a>Mispronunciation detection using self-supervised speech representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16324">http://arxiv.org/abs/2307.16324</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JazminVidal/ssl-mispron">https://github.com/JazminVidal/ssl-mispron</a></li>
<li>paper_authors: Jazmin Vidal, Pablo Riera, Luciana Ferrer</li>
<li>for: 这个论文是关于second language learners的发音识别 tasks的研究。</li>
<li>methods: 作者使用了自动学习（SSL）模型，并对其进行了多种表示方式的比较。</li>
<li>results: 研究发现，使用直接为目标任务训练的模型 perfoms最好，而大多数上游模型在该任务上的表现相似。I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
In recent years, self-supervised learning (SSL) models have produced promising results in a variety of speech-processing tasks, especially in contexts of data scarcity. In this paper, we study the use of SSL models for the task of mispronunciation detection for second language learners. We compare two downstream approaches: 1) training the model for phone recognition (PR) using native English data, and 2) training a model directly for the target task using non-native English data. We compare the performance of these two approaches for various SSL representations as well as a representation extracted from a traditional DNN-based speech recognition model. We evaluate the models on L2Arctic and EpaDB, two datasets of non-native speech annotated with pronunciation labels at the phone level. Overall, we find that using a downstream model trained for the target task gives the best performance and that most upstream models perform similarly for the task.
</details>
<details>
<summary>摘要</summary>
Here's the Traditional Chinese translation:近年来，自主学习（SSL）模型在各种语音处理任务中获得了显著的成果，尤其在数据稀缺的情况下。在这篇文章中，我们研究了使用SSL模型来探索第二语言学习者的误发音探测。我们比较了两种下游方法：1）使用本地英文数据进行话音识别（PR）训练，和2）直接使用非本地英文数据进行目标任务的训练。我们对各种SSL表现进行比较，以及从传统的 Deep Neural Network（DNN）数据识别模型中提取的表现。我们将模型评估在L2Arctic和EpaDB两个非本地语音数据集上，这两个数据集各自包含了话音的标签。总的来说，我们发现使用直接进行目标任务的训练模型能够获得最好的性能，而大多数上游模型在这个任务上表现相似。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/31/cs.SD_2023_07_31/" data-id="clluro5kp008aq98851zj6naa" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/31/cs.LG_2023_07_31/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-07-31 18:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/31/eess.AS_2023_07_31/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.AS - 2023-07-31 22:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
