
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-07-31 18:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Classification with Deep Neural Networks and Logistic Loss paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.16792 repo_url: https:&#x2F;&#x2F;github.com&#x2F;himanshub1007&#x2F;Alzhimers-Disease-Prediction-Using-Deep-learning paper_">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-07-31 18:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/07/31/cs.LG_2023_07_31/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Classification with Deep Neural Networks and Logistic Loss paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.16792 repo_url: https:&#x2F;&#x2F;github.com&#x2F;himanshub1007&#x2F;Alzhimers-Disease-Prediction-Using-Deep-learning paper_">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-30T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:23.297Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_07_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/31/cs.LG_2023_07_31/" class="article-date">
  <time datetime="2023-07-30T16:00:00.000Z" itemprop="datePublished">2023-07-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-07-31 18:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Classification-with-Deep-Neural-Networks-and-Logistic-Loss"><a href="#Classification-with-Deep-Neural-Networks-and-Logistic-Loss" class="headerlink" title="Classification with Deep Neural Networks and Logistic Loss"></a>Classification with Deep Neural Networks and Logistic Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16792">http://arxiv.org/abs/2307.16792</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning">https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning</a></li>
<li>paper_authors: Zihan Zhang, Lei Shi, Ding-Xuan Zhou<br>for: 这个论文主要是为了研究深度神经网络（DNN）在二分类任务中的泛化分析。methods: 这篇论文使用了 Oracle-type 不等式来处理目标函数的固定性问题，并使用这个不等式来 derive 深度神经网络（DNN） trained with logistic loss 的快速收敛率。results: 这篇论文得到了对于完全连接 ReLU DNN 分类器 trained with logistic loss 的快速收敛率，其中收敛率是对于数据 conditional class probability 的 H&quot;older 平滑性假设。此外，这篇论文还提出了一种 compositional 假设，以解释为什么 DNN 分类器在实际高维数据中能够表现良好。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) trained with the logistic loss (i.e., the cross entropy loss) have made impressive advancements in various binary classification tasks. However, generalization analysis for binary classification with DNNs and logistic loss remains scarce. The unboundedness of the target function for the logistic loss is the main obstacle to deriving satisfying generalization bounds. In this paper, we aim to fill this gap by establishing a novel and elegant oracle-type inequality, which enables us to deal with the boundedness restriction of the target function, and using it to derive sharp convergence rates for fully connected ReLU DNN classifiers trained with logistic loss. In particular, we obtain optimal convergence rates (up to log factors) only requiring the H\"older smoothness of the conditional class probability $\eta$ of data. Moreover, we consider a compositional assumption that requires $\eta$ to be the composition of several vector-valued functions of which each component function is either a maximum value function or a H\"older smooth function only depending on a small number of its input variables. Under this assumption, we derive optimal convergence rates (up to log factors) which are independent of the input dimension of data. This result explains why DNN classifiers can perform well in practical high-dimensional classification problems. Besides the novel oracle-type inequality, the sharp convergence rates given in our paper also owe to a tight error bound for approximating the natural logarithm function near zero (where it is unbounded) by ReLU DNNs. In addition, we justify our claims for the optimality of rates by proving corresponding minimax lower bounds. All these results are new in the literature and will deepen our theoretical understanding of classification with DNNs.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）在多个二分类任务中取得了卓越的成就，但对于DNN和logistic损失的泛化分析却缺乏研究。Target函数的无上限性是泛化分析的主要障碍。在这篇论文中，我们尝试填补这一漏洞，并建立了一种新的oracle-type不等式，允许我们处理目标函数的上限限制，并使用其 derivation 深度神经网络（ReLU DNN） Fully Connected 类 clasifier 的快速收敛率。具体来说，我们获得了最佳的收敛率（带Logarithmic factor），只需要数据中 conditional class probability 的Holder smoothness。此外，我们还考虑了一种compositional assumption，即 conditional class probability 可以表示为多个vector-valued function的 composition，其中每个组件函数可以是最大值函数或Holder smooth function，只виси于一小部分输入变量。在这种假设下，我们获得了最佳的收敛率（带Logarithmic factor），不abhäng于输入维度。这个结果解释了为什么深度神经网络可以在实际高维度分类问题中表现良好。此外，我们还证明了我们的结果对泛化分析的 optimality 。这些结果都是文献中的新结果，它们将deepen our theoretical understanding of classification with DNNs。
</details></li>
</ul>
<hr>
<h2 id="ToolLLM-Facilitating-Large-Language-Models-to-Master-16000-Real-world-APIs"><a href="#ToolLLM-Facilitating-Large-Language-Models-to-Master-16000-Real-world-APIs" class="headerlink" title="ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs"></a>ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16789">http://arxiv.org/abs/2307.16789</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openbmb/toolbench">https://github.com/openbmb/toolbench</a></li>
<li>paper_authors: Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, Maosong Sun</li>
<li>for: 这个论文旨在提高开源大语言模型（LLM）的高级任务能力，包括按照人类指令使用外部工具（API）。</li>
<li>methods: 作者引入了ToolLLM框架，包括数据建构、模型训练和评估，以便在开源LLM中实现工具使用能力。他们还开发了一个自动生成的实ruction-tuning数据集 ToolBench，并使用ChatGPT生成了多种人类指令，以覆盖单 Tool和多 Toolenario。</li>
<li>results: 作者的研究表明，通过使用DFSDT搜索算法，LLM可以提高计划和理解能力。此外，作者还开发了一个自动评估器 ToolEval，并 fine-tune LLaMA模型，以便评估ToolLLaMA的表现。结果显示，ToolLLaMA能够执行复杂的指令和扩展到未看过API，并与ChatGPT的性能相似。<details>
<summary>Abstract</summary>
Despite the advancements of open-source large language models (LLMs) and their variants, e.g., LLaMA and Vicuna, they remain significantly limited in performing higher-level tasks, such as following human instructions to use external tools (APIs). This is because current instruction tuning largely focuses on basic language tasks instead of the tool-use domain. This is in contrast to state-of-the-art (SOTA) LLMs, e.g., ChatGPT, which have demonstrated excellent tool-use capabilities but are unfortunately closed source. To facilitate tool-use capabilities within open-source LLMs, we introduce ToolLLM, a general tool-use framework of data construction, model training and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is created automatically using ChatGPT. Specifically, we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub, then prompt ChatGPT to generate diverse human instructions involving these APIs, covering both single-tool and multi-tool scenarios. Finally, we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To make the searching process more efficient, we develop a novel depth-first search-based decision tree (DFSDT), enabling LLMs to evaluate multiple reasoning traces and expand the search space. We show that DFSDT significantly enhances the planning and reasoning capabilities of LLMs. For efficient tool-use assessment, we develop an automatic evaluator: ToolEval. We fine-tune LLaMA on ToolBench and obtain ToolLLaMA. Our ToolEval reveals that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. To make the pipeline more practical, we devise a neural API retriever to recommend appropriate APIs for each instruction, negating the need for manual API selection.
</details>
<details>
<summary>摘要</summary>
尽管开源大语言模型（LLM）和其变种（如LLaMA和Vicuna）已经取得了一定的进步，但它们仍然在执行更高级别任务时表现有限，比如按照人类 instrucion 使用外部工具（API）。这是因为当前的 instrucion 调整主要集中在基础语言任务上而不是工具使用领域。与此相比，当前的SOTA LLM（如ChatGPT）已经表现出了出色的工具使用能力，但它们却是关闭源代码。为了帮助开源 LLM 拥有工具使用能力，我们介绍了 ToolLLM，一个通用的工具使用框架，包括数据建构、模型训练和评估。我们首先介绍了 ToolBench，一个用于 instrucion 调整的数据集，由自动生成的 ChatGPT 提供。我们收集了来自 RapidAPI Hub 的 16,464 个实际 RESTful API，涵盖 49 个类别，然后使用 ChatGPT 生成多样化的人类 instrucion，涵盖单工具和多工具场景。最后，我们使用 ChatGPT 搜索一个有效的解决方案路径（chain of API calls） для每个 instrucion。为了使搜索过程更加效率，我们开发了一种深度优先搜索树（DFSDT），使 LLM 可以评估多种理解轨迹并扩展搜索空间。我们表明了 DFSDT 可以显著提高 LLM 的规划和理解能力。为了有效评估工具使用，我们开发了一个自动评估器：ToolEval。我们精度调整 LLaMA 以适应 ToolBench，并获得了 ToolLLaMA。我们的 ToolEval 表明，ToolLLaMA 在执行复杂 instrucion 和扩展到未看过 API 方面表现出色，与 ChatGPT 的表现相似。为了使该管道更实用，我们设计了一种神经网络 API 搜索器，以便在 instrucion 中提供合适的 API，从而消除人工 API 选择的需要。
</details></li>
</ul>
<hr>
<h2 id="Exploring-how-a-Generative-AI-interprets-music"><a href="#Exploring-how-a-Generative-AI-interprets-music" class="headerlink" title="Exploring how a Generative AI interprets music"></a>Exploring how a Generative AI interprets music</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00015">http://arxiv.org/abs/2308.00015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriela Barenboim, Luigi Del Debbio, Johannes Hirn, Veronica Sanz</li>
<li>for: 使用 Google 的 MusicVAE 模型，对音乐序列进行表示和组织。</li>
<li>methods: 使用 Variational Auto-Encoder 对音乐序列进行学习，并将 latent 空间分为 relevance 的 music  neurons 和 noise  neurons。</li>
<li>results: 发现大多数 latent neurons 在实际音乐轨迹上都 remain silent，仅剩下几十个 music neurons 才会 fired。这些 music neurons 中的大多数都编码了 pitch 和 rhythm 信息，而 melody 信息则只在 longer sequences of music 中出现。<details>
<summary>Abstract</summary>
We use Google's MusicVAE, a Variational Auto-Encoder with a 512-dimensional latent space to represent a few bars of music, and organize the latent dimensions according to their relevance in describing music. We find that, on average, most latent neurons remain silent when fed real music tracks: we call these "noise" neurons. The remaining few dozens of latent neurons that do fire are called "music neurons". We ask which neurons carry the musical information and what kind of musical information they encode, namely something that can be identified as pitch, rhythm or melody. We find that most of the information about pitch and rhythm is encoded in the first few music neurons: the neural network has thus constructed a couple of variables that non-linearly encode many human-defined variables used to describe pitch and rhythm. The concept of melody only seems to show up in independent neurons for longer sequences of music.
</details>
<details>
<summary>摘要</summary>
我们使用Google的MusicVAE，一种512维 latent space的变量自动编码器，来表示一些乐曲的一些小段。我们发现，在 average，大多数 latent neuron 都 remain silent 当 fed 实际乐曲：我们称这些为 "噪音" neuron。剩下的几十个 latent neuron 才会 firing：我们称这些为 "音乐 neuron"。我们问到哪些 neuron 携带了音乐信息，它们是什么类型的音乐信息，例如把律、和声或旋律等。我们发现，大多数把律和和声信息都是在 first few music neuron 中编码的：因此，神经网络已经构造了一些变量，用于非线性地编码许多人定义的把律和和声变量。旋律信息只在 longer sequences of music 中出现独立的 neuron 中。
</details></li>
</ul>
<hr>
<h2 id="Lossless-Transformations-and-Excess-Risk-Bounds-in-Statistical-Inference"><a href="#Lossless-Transformations-and-Excess-Risk-Bounds-in-Statistical-Inference" class="headerlink" title="Lossless Transformations and Excess Risk Bounds in Statistical Inference"></a>Lossless Transformations and Excess Risk Bounds in Statistical Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16735">http://arxiv.org/abs/2307.16735</a></li>
<li>repo_url: None</li>
<li>paper_authors: László Györfi, Tamás Linder, Harro Walk</li>
<li>for: 这篇论文是关于统计推断中的剩余最小风险的研究，具体来说是计算一个随机变量从观察特征向量中的估计loss的最小值，并将其与特征向量的变换（统计）的loss进行比较。</li>
<li>methods: 这篇论文使用了lossless变换的定义，即lossless变换是一种使得额外风险为零的变换。然后，对于独立同分布的数据，构建了一个分区统计学习法，以验证一个给定的变换是lossless的。此外， authors还提出了基于信息理论的上限 bounds，以 uniform 地应用于广泛的损失函数类型。</li>
<li>results: 这篇论文的结果包括：1) 对独立同分布数据进行分区统计学习，可以确定一个给定的变换是lossless的; 2) 基于信息理论的上限 bounds，可以uniform 地应用于广泛的损失函数类型; 3) 在分类、非 Parametric 回归、股票投资、信息瓶颈和深度学习等领域中，lossless变换的概念和应用。<details>
<summary>Abstract</summary>
We study the excess minimum risk in statistical inference, defined as the difference between the minimum expected loss in estimating a random variable from an observed feature vector and the minimum expected loss in estimating the same random variable from a transformation (statistic) of the feature vector. After characterizing lossless transformations, i.e., transformations for which the excess risk is zero for all loss functions, we construct a partitioning test statistic for the hypothesis that a given transformation is lossless and show that for i.i.d. data the test is strongly consistent. More generally, we develop information-theoretic upper bounds on the excess risk that uniformly hold over fairly general classes of loss functions. Based on these bounds, we introduce the notion of a delta-lossless transformation and give sufficient conditions for a given transformation to be universally delta-lossless. Applications to classification, nonparametric regression, portfolio strategies, information bottleneck, and deep learning, are also surveyed.
</details>
<details>
<summary>摘要</summary>
我们研究额外最小风险在统计推断中，定义为从观察特征 вектор estimating 随机变量的最小风险差，与从特征 вектор变换 (统计) 中 estimating 同一个随机变量的风险差的差异。经过定义无损变换，即对所有损函数都有零风险差的变换，我们构造了一个分 partitioning 测试统计，用于测试一个给定的变换是否为无损变换，并证明在独立同分布数据上，该测试是强有效的。更一般地，我们开发了基于信息理论的上界，对随机变量的额外风险进行统一 bounds ，这些上界在较广泛的损函数类型上都成立。基于这些上界，我们引入 delta-lossless 变换的概念，并给出了universally delta-lossless 变换的充分条件。我们还将应用于分类、非 Parametric regression、资产配置策略、信息瓶颈和深度学习等领域进行了概述。
</details></li>
</ul>
<hr>
<h2 id="An-Efficient-Shapley-Value-Computation-for-the-Naive-Bayes-Classifier"><a href="#An-Efficient-Shapley-Value-Computation-for-the-Naive-Bayes-Classifier" class="headerlink" title="An Efficient Shapley Value Computation for the Naive Bayes Classifier"></a>An Efficient Shapley Value Computation for the Naive Bayes Classifier</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16718">http://arxiv.org/abs/2307.16718</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vincent Lemaire, Fabrice Clérot, Marc Boullé</li>
<li>for: The paper is written to propose an exact analytic expression of Shapley values for the naive Bayes classifier, and to compare the results with other methods such as Weight of Evidence (WoE) and KernelShap.</li>
<li>methods: The paper uses cooperative game theory and Shapley value estimation algorithms to provide an intelligibility method for the naive Bayes classifier.</li>
<li>results: The paper shows that the proposed Shapley proposal provides informative results with low algorithmic complexity, making it suitable for very large datasets with low computation time.Here are the three points in Simplified Chinese text:</li>
<li>for: 这篇论文是为了提出一个精确的分析表达方法，用于解释隐马氏投票分类器的决策。</li>
<li>methods: 论文使用了合作游戏理论和Shapley值估计算法，以提供一种可解释性的方法。</li>
<li>results: 论文显示，提出的Shapley值计算方法可以提供有用的结果，并且具有低算法复杂度和低计算时间，适用于非常大的数据集。<details>
<summary>Abstract</summary>
Variable selection or importance measurement of input variables to a machine learning model has become the focus of much research. It is no longer enough to have a good model, one also must explain its decisions. This is why there are so many intelligibility algorithms available today. Among them, Shapley value estimation algorithms are intelligibility methods based on cooperative game theory. In the case of the naive Bayes classifier, and to our knowledge, there is no ``analytical" formulation of Shapley values. This article proposes an exact analytic expression of Shapley values in the special case of the naive Bayes Classifier. We analytically compare this Shapley proposal, to another frequently used indicator, the Weight of Evidence (WoE) and provide an empirical comparison of our proposal with (i) the WoE and (ii) KernelShap results on real world datasets, discussing similar and dissimilar results. The results show that our Shapley proposal for the naive Bayes classifier provides informative results with low algorithmic complexity so that it can be used on very large datasets with extremely low computation time.
</details>
<details>
<summary>摘要</summary>
变量选择或输入变量重要性测量在机器学习模型中已成为研究焦点。不再只是有一个好模型，也必须解释其决策。这是为什么今天有so多可理解算法的原因。其中，Shapley值估计算法是基于合作游戏理论的可理解方法之一。在Naive Bayes分类器的特殊情况下，我们认为没有“分析”的形式ulation of Shapley values。本文提出了Naive Bayes分类器的特殊情况下的Shapley值的精确分析表达。我们对这一Shapley提案进行了分析比较，并对另一种常用指标——质量证明（WoE）进行了实际比较。我们还对我们的提案与（i）WoE和（ii）KernelShap结果进行了实际比较，讨论了相似和不同的结果。结果显示，我们的Shapley提案对Naive Bayes分类器提供了有用的结果，且算法复杂度低，可以在很大的数据集上进行快速计算，因此可以用于实际应用。
</details></li>
</ul>
<hr>
<h2 id="Active-Learning-in-Genetic-Programming-Guiding-Efficient-Data-Collection-for-Symbolic-Regression"><a href="#Active-Learning-in-Genetic-Programming-Guiding-Efficient-Data-Collection-for-Symbolic-Regression" class="headerlink" title="Active Learning in Genetic Programming: Guiding Efficient Data Collection for Symbolic Regression"></a>Active Learning in Genetic Programming: Guiding Efficient Data Collection for Symbolic Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00672">http://arxiv.org/abs/2308.00672</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hoolagans/stackgp">https://github.com/hoolagans/stackgp</a></li>
<li>paper_authors: Nathan Haut, Wolfgang Banzhaf, Bill Punch</li>
<li>for: 这种研究旨在探讨活动学习中的不确定性和多样性计算方法，以便选择有用的训练数据点。</li>
<li>methods: 这种研究使用了模型集合combined with uncertainty metric来利用模型Population在生态学编程中选择有用的训练数据点。我们考虑了多种不确定度度量，并发现了差异束度最佳。此外，我们还比较了两种数据多样度度量，发现相关度作为多样度度量表现较好，但有一些缺点。</li>
<li>results: 最终，我们使用Pareto优化approach将不确定度和多样度考虑在一起，以达到一种平衡的方式，以便在训练中选择有用和独特的数据点。<details>
<summary>Abstract</summary>
This paper examines various methods of computing uncertainty and diversity for active learning in genetic programming. We found that the model population in genetic programming can be exploited to select informative training data points by using a model ensemble combined with an uncertainty metric. We explored several uncertainty metrics and found that differential entropy performed the best. We also compared two data diversity metrics and found that correlation as a diversity metric performs better than minimum Euclidean distance, although there are some drawbacks that prevent correlation from being used on all problems. Finally, we combined uncertainty and diversity using a Pareto optimization approach to allow both to be considered in a balanced way to guide the selection of informative and unique data points for training.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="An-Empirical-Study-on-Log-based-Anomaly-Detection-Using-Machine-Learning"><a href="#An-Empirical-Study-on-Log-based-Anomaly-Detection-Using-Machine-Learning" class="headerlink" title="An Empirical Study on Log-based Anomaly Detection Using Machine Learning"></a>An Empirical Study on Log-based Anomaly Detection Using Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16714">http://arxiv.org/abs/2307.16714</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shan Ali, Chaima Boufaied, Domenico Bianculli, Paula Branco, Lionel Briand, Nathan Aschbacher</li>
<li>for: 本研究旨在evaluate不同的机器学习技术在Log-based Anomaly Detection（LAD）任务中的表现，并研究这些技术在不同的 datasets 和Context中的可行性。</li>
<li>methods: 本研究使用了不同的supervised和semi-supervised机器学习技术，包括传统机器学习和深度学习技术。</li>
<li>results: 研究发现，supervised传统机器学习技术和深度学习技术在检测精度和预测时间方面表现很接近，而semi-supervised技术则导致检测精度下降。此外，不同机器学习技术对于hyperparameter tuning的敏感性也有所不同。<details>
<summary>Abstract</summary>
The growth of systems complexity increases the need of automated techniques dedicated to different log analysis tasks such as Log-based Anomaly Detection (LAD). The latter has been widely addressed in the literature, mostly by means of different deep learning techniques. Nevertheless, the focus on deep learning techniques results in less attention being paid to traditional Machine Learning (ML) techniques, which may perform well in many cases, depending on the context and the used datasets. Further, the evaluation of different ML techniques is mostly based on the assessment of their detection accuracy. However, this is is not enough to decide whether or not a specific ML technique is suitable to address the LAD problem. Other aspects to consider include the training and prediction time as well as the sensitivity to hyperparameter tuning. In this paper, we present a comprehensive empirical study, in which we evaluate different supervised and semi-supervised, traditional and deep ML techniques w.r.t. four evaluation criteria: detection accuracy, time performance, sensitivity of detection accuracy as well as time performance to hyperparameter tuning. The experimental results show that supervised traditional and deep ML techniques perform very closely in terms of their detection accuracy and prediction time. Moreover, the overall evaluation of the sensitivity of the detection accuracy of the different ML techniques to hyperparameter tuning shows that supervised traditional ML techniques are less sensitive to hyperparameter tuning than deep learning techniques. Further, semi-supervised techniques yield significantly worse detection accuracy than supervised techniques.
</details>
<details>
<summary>摘要</summary>
Currently, the evaluation of different ML techniques for LAD is mostly based on the assessment of their detection accuracy. However, this is not enough to decide whether or not a specific ML technique is suitable to address the LAD problem. Other aspects to consider include the training and prediction time, as well as the sensitivity to hyperparameter tuning.In this paper, we present a comprehensive empirical study in which we evaluate different supervised and semi-supervised, traditional and deep ML techniques with respect to four evaluation criteria: detection accuracy, time performance, sensitivity of detection accuracy to hyperparameter tuning, and time performance to hyperparameter tuning. The experimental results show that supervised traditional and deep ML techniques perform very closely in terms of their detection accuracy and prediction time. Moreover, the overall evaluation of the sensitivity of the detection accuracy of the different ML techniques to hyperparameter tuning shows that supervised traditional ML techniques are less sensitive to hyperparameter tuning than deep learning techniques. Finally, semi-supervised techniques yield significantly worse detection accuracy than supervised techniques.
</details></li>
</ul>
<hr>
<h2 id="TFE-GNN-A-Temporal-Fusion-Encoder-Using-Graph-Neural-Networks-for-Fine-grained-Encrypted-Traffic-Classification"><a href="#TFE-GNN-A-Temporal-Fusion-Encoder-Using-Graph-Neural-Networks-for-Fine-grained-Encrypted-Traffic-Classification" class="headerlink" title="TFE-GNN: A Temporal Fusion Encoder Using Graph Neural Networks for Fine-grained Encrypted Traffic Classification"></a>TFE-GNN: A Temporal Fusion Encoder Using Graph Neural Networks for Fine-grained Encrypted Traffic Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16713">http://arxiv.org/abs/2307.16713</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ViktorAxelsen/TFE-GNN">https://github.com/ViktorAxelsen/TFE-GNN</a></li>
<li>paper_authors: Haozhen Zhang, Le Yu, Xi Xiao, Qing Li, Francesco Mercaldo, Xiapu Luo, Qixu Liu</li>
<li>for: 本研究旨在提出一种基于点对点相互信息（PMI）的字节级流量图构建方法，以及一种基于图 neural network（GNN）的特征提取模型（TFE-GNN），用于细致Encrypted traffic classification。</li>
<li>methods: 本研究使用了一种基于PMI的字节级流量图构建方法，并提出了一种基于GNN的特征提取模型TFE-GNN，包括双嵌入层、GNN基于流量图编码器以及交叉阻止特征融合机制。</li>
<li>results: 对于两个实际数据集，TFE-GNN的实验结果表明，它在细致Encrypted traffic classification任务中比多种现有方法表现更好。<details>
<summary>Abstract</summary>
Encrypted traffic classification is receiving widespread attention from researchers and industrial companies. However, the existing methods only extract flow-level features, failing to handle short flows because of unreliable statistical properties, or treat the header and payload equally, failing to mine the potential correlation between bytes. Therefore, in this paper, we propose a byte-level traffic graph construction approach based on point-wise mutual information (PMI), and a model named Temporal Fusion Encoder using Graph Neural Networks (TFE-GNN) for feature extraction. In particular, we design a dual embedding layer, a GNN-based traffic graph encoder as well as a cross-gated feature fusion mechanism, which can first embed the header and payload bytes separately and then fuses them together to obtain a stronger feature representation. The experimental results on two real datasets demonstrate that TFE-GNN outperforms multiple state-of-the-art methods in fine-grained encrypted traffic classification tasks.
</details>
<details>
<summary>摘要</summary>
《加密流量分类受到研究人员和产业公司的广泛关注。然而，现有方法仅提取流量水平特征，因为不可靠的统计性质而无法处理短流，或者对header和payload进行等效处理，而不是挖掘字节之间的潜在相关性。因此，在这篇论文中，我们提出了基于点wise私有信息（PMI）的字节级流量图构建方法，以及基于图神经网络（GNN）的时间融合编码器（TFE-GNN）。具体来说，我们设计了双嵌入层、GNN基于流量图编码器以及跨门控制的特征融合机制，可以首先将header和payload字节分别嵌入，然后将它们融合以获得更强的特征表示。实验结果表明，TFE-GNN在两个实际数据集上的细化加密流量分类任务中表现出色，超过了多种现有方法。》Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Meets-Adaptive-Filtering-A-Stein’s-Unbiased-Risk-Estimator-Approach"><a href="#Deep-Learning-Meets-Adaptive-Filtering-A-Stein’s-Unbiased-Risk-Estimator-Approach" class="headerlink" title="Deep Learning Meets Adaptive Filtering: A Stein’s Unbiased Risk Estimator Approach"></a>Deep Learning Meets Adaptive Filtering: A Stein’s Unbiased Risk Estimator Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16708">http://arxiv.org/abs/2307.16708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zahra Esmaeilbeig, Mojtaba Soltanalian</li>
<li>for: 本文重新审视了两种常用的适应滤波算法，即回归最小二乘（RLS）和同态适应源分离（EASI），在源估计和分离上下文中。</li>
<li>methods: 我们通过抽象方法构建了两种任务深度学习框架，称为深度RLS和深度EASI。这些架构将原始算法的迭代转换为层次深度神经网络，以便通过训练过程进行高效的源信号估计。</li>
<li>results: 我们的实验结果表明，使用基于Stein不偏风险估计（SURE）的训练策略可以提高源信号估计性能。<details>
<summary>Abstract</summary>
This paper revisits two prominent adaptive filtering algorithms through the lens of algorithm unrolling, namely recursive least squares (RLS) and equivariant adaptive source separation (EASI), in the context of source estimation and separation. Building upon the unrolling methodology, we introduce novel task-based deep learning frameworks, denoted as Deep RLS and Deep EASI. These architectures transform the iterations of the original algorithms into layers of a deep neural network, thereby enabling efficient source signal estimation by taking advantage of a training process. To further enhance performance, we propose training these deep unrolled networks utilizing a loss function grounded on a Stein's unbiased risk estimator (SURE). Our empirical evaluations demonstrate the efficacy of this SURE-based approach for enhanced source signal estimation.
</details>
<details>
<summary>摘要</summary>
这篇论文重新审视了两种广泛使用的适应滤波算法，即反射最小二乘（RLS）和同态适应源分离（EASI），在源估计和分离上。基于折叠方法，我们介绍了一种新的任务基于深度学习框架，称为深度RLS和深度EASI。这些架构将原始算法的迭代转化为层次的深度神经网络，以便通过训练过程进行高效的源信号估计。为了进一步提高性能，我们提议使用基于斯坦不偏估量（SURE）的训练方法。我们的实验结果表明，这种SURE基本方法可以更好地提高源信号估计。
</details></li>
</ul>
<hr>
<h2 id="Lookbehind-Optimizer-k-steps-back-1-step-forward"><a href="#Lookbehind-Optimizer-k-steps-back-1-step-forward" class="headerlink" title="Lookbehind Optimizer: k steps back, 1 step forward"></a>Lookbehind Optimizer: k steps back, 1 step forward</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16704">http://arxiv.org/abs/2307.16704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gonçalo Mordido, Pranshu Malviya, Aristide Baratin, Sarath Chandar</li>
<li>for: 提高深度神经网络训练稳定性，采用快速权重导航 descent 方向。</li>
<li>methods:  combinates Lookahead optimizer 和 Sharpness-aware minimization (SAM) ，实现多步变化稳定性和优化loss-sharpness交换。</li>
<li>results: 在多种任务和训练环境中显示了增强通用性、降低噪声权重robustness 和生长学习中的忘却难度。<details>
<summary>Abstract</summary>
The Lookahead optimizer improves the training stability of deep neural networks by having a set of fast weights that "look ahead" to guide the descent direction. Here, we combine this idea with sharpness-aware minimization (SAM) to stabilize its multi-step variant and improve the loss-sharpness trade-off. We propose Lookbehind, which computes $k$ gradient ascent steps ("looking behind") at each iteration and combine the gradients to bias the descent step toward flatter minima. We apply Lookbehind on top of two popular sharpness-aware training methods -- SAM and adaptive SAM (ASAM) -- and show that our approach leads to a myriad of benefits across a variety of tasks and training regimes. Particularly, we show increased generalization performance, greater robustness against noisy weights, and higher tolerance to catastrophic forgetting in lifelong learning settings.
</details>
<details>
<summary>摘要</summary>
“lookahead优化器可以提高深度神经网络的训练稳定性，通过一组快速的权重来引导下降方向。我们在这个想法基础上与锐度感知训练（SAM）相结合，以稳定其多步变体并改善损失锐度之间的交换。我们提出了“lookbehind”，它在每次迭代中计算$k$步上升步（“看后”），并将梯度相加以偏导下降步向平坦的极小值。我们在两种流行的锐度感知训练方法——SAM和自适应锐度感知训练（ASAM）——之上应用lookbehind方法，并证明了我们的方法在不同的任务和训练条件下具有多种优点，包括提高泛化性能、增加随机权重的稳定性和生长学习设置中的忘记性。”
</details></li>
</ul>
<hr>
<h2 id="A-theory-of-data-variability-in-Neural-Network-Bayesian-inference"><a href="#A-theory-of-data-variability-in-Neural-Network-Bayesian-inference" class="headerlink" title="A theory of data variability in Neural Network Bayesian inference"></a>A theory of data variability in Neural Network Bayesian inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16695">http://arxiv.org/abs/2307.16695</a></li>
<li>repo_url: None</li>
<li>paper_authors: Javed Lindner, David Dahmen, Michael Krämer, Moritz Helias</li>
<li>for: 这 paper 的目的是investigating neural networks in the limit of infinitely wide hidden layers, and providing a field-theoretic formalism for understanding their generalization properties.</li>
<li>methods: 这 paper 使用 Bayesian inference and kernel methods, specifically the neural network Gaussian process, to derive generalization properties of linear, non-linear, and deep non-linear networks for kernel matrices with heterogeneous entries.</li>
<li>results: 这 paper 系统地计算了 infinitely wide networks 的泛化性质, 并发现 data variability 导致一种非 Gaussian action, 类似于（φ^3+φ^4）-theory. furthermore, the paper obtains a homogeneous kernel matrix approximation for the learning curve and corrections due to data variability, which allow for the estimation of the generalization properties and exact results for the bounds of the learning curves in the case of infinitely many training data points.<details>
<summary>Abstract</summary>
Bayesian inference and kernel methods are well established in machine learning. The neural network Gaussian process in particular provides a concept to investigate neural networks in the limit of infinitely wide hidden layers by using kernel and inference methods. Here we build upon this limit and provide a field-theoretic formalism which covers the generalization properties of infinitely wide networks. We systematically compute generalization properties of linear, non-linear, and deep non-linear networks for kernel matrices with heterogeneous entries. In contrast to currently employed spectral methods we derive the generalization properties from the statistical properties of the input, elucidating the interplay of input dimensionality, size of the training data set, and variability of the data. We show that data variability leads to a non-Gaussian action reminiscent of a ($\varphi^3+\varphi^4$)-theory. Using our formalism on a synthetic task and on MNIST we obtain a homogeneous kernel matrix approximation for the learning curve as well as corrections due to data variability which allow the estimation of the generalization properties and exact results for the bounds of the learning curves in the case of infinitely many training data points.
</details>
<details>
<summary>摘要</summary>
bayesian inference和kernel方法在机器学习中已经非常流行。神经网络加 Gaussian process在特别提供了一种用于investigate神经网络在卷积层数量无穷大时的概念。我们在这个限制下建立了一个场景理论 formalism，涵盖无穷大神经网络的泛化性质。我们系统地计算了不同类型的神经网络（线性、非线性和深度非线性）在 kernel 矩阵中的泛化性质，并与当前使用的spectral方法进行比较。我们发现，数据变化导致一种非 Gaussian 动作，类似于（φ^3+φ^4）-理论。使用我们的 formalism 在一个 sintetic 任务上和 MNIST 上，我们得到了一个Homogeneous kernel matrix approximation 以及由数据变化引起的 corrections，这些 corrections 允许我们估算泛化性质并计算精确的 bound 值。
</details></li>
</ul>
<hr>
<h2 id="Guiding-Image-Captioning-Models-Toward-More-Specific-Captions"><a href="#Guiding-Image-Captioning-Models-Toward-More-Specific-Captions" class="headerlink" title="Guiding Image Captioning Models Toward More Specific Captions"></a>Guiding Image Captioning Models Toward More Specific Captions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16686">http://arxiv.org/abs/2307.16686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simon Kornblith, Lala Li, Zirui Wang, Thao Nguyen</li>
<li>for: 提高图像描述文本的准确性和特点性。</li>
<li>methods: 通过终端授益指导，使描述文本模型能够更好地识别图像。</li>
<li>results:  compared to标准的排序搜索，使用终端授益指导可以提高图像描述文本的准确性和特点性，但是可能会下降标准的参考基础captioning metric。<details>
<summary>Abstract</summary>
Image captioning is conventionally formulated as the task of generating captions for images that match the distribution of reference image-caption pairs. However, reference captions in standard captioning datasets are short and may not uniquely identify the images they describe. These problems are further exacerbated when models are trained directly on image-alt text pairs collected from the internet. In this work, we show that it is possible to generate more specific captions with minimal changes to the training process. We implement classifier-free guidance for an autoregressive captioning model by fine-tuning it to estimate both conditional and unconditional distributions over captions. The guidance scale applied at decoding controls a trade-off between maximizing $p(\mathrm{caption}|\mathrm{image})$ and $p(\mathrm{image}|\mathrm{caption})$. Compared to standard greedy decoding, decoding with a guidance scale of 2 substantially improves reference-free metrics such as CLIPScore (0.808 vs. 0.775) and caption$\to$image retrieval performance in the CLIP embedding space (recall@1 44.6% vs. 26.5%), but worsens standard reference-based captioning metrics (e.g., CIDEr 78.6 vs 126.1). We further explore the use of language models to guide the decoding process, obtaining small improvements over the Pareto frontier of reference-free vs. reference-based captioning metrics that arises from classifier-free guidance, and substantially improving the quality of captions generated from a model trained only on minimally curated web data.
</details>
<details>
<summary>摘要</summary>
图像描述是通常被定义为生成图像描述符与参考图像描述符对的分布匹配的任务。然而，参考描述在标准描述集中可能不够精细，这些问题更加恶化了在互联网上收集的图像描述对的训练过程中。在这项工作中，我们表明可以通过微调模型来生成更加精细的描述。我们实现了无类别导航的授意法，通过模型估计图像描述符的 conditional 和无条件分布来做出授意。在解码过程中，应用指导缩放控制了图像描述符的生成和图像描述符的匹配。与标准的排序解码相比，使用指导缩放可以大幅提高无参考指标（如 CLIPScore 0.808 vs. 0.775）和图像描述符到图像的匹配性（在 CLIP 空间中的 recall@1 44.6% vs. 26.5%），但是可能下降标准参考指标（如 CIDEr 78.6 vs 126.1）。我们还探讨了使用语言模型来引导解码过程，可以在无参考指标与参考指标之间取得小的改进，并且可以从无类别导航中获得较好的描述质量，从一个只有微处理的网络数据进行训练的模型中生成的描述。
</details></li>
</ul>
<hr>
<h2 id="On-the-Trustworthiness-Landscape-of-State-of-the-art-Generative-Models-A-Comprehensive-Survey"><a href="#On-the-Trustworthiness-Landscape-of-State-of-the-art-Generative-Models-A-Comprehensive-Survey" class="headerlink" title="On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey"></a>On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16680">http://arxiv.org/abs/2307.16680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingyuan Fan, Cen Chen, Chengyu Wang, Jun Huang</li>
<li>for: 本文旨在探讨大规模生成模型的可靠性，尤其是在四个基本维度：隐私、安全、公平性和责任方面。</li>
<li>methods: 本文采用了评估大规模生成模型的传统和新兴威胁的方法，包括隐私泄露、安全漏洞、不公平性和责任不清等。</li>
<li>results: 本文提出了一份全面的可靠性映射，并提供了实践建议和未来发展方向，以促进大规模生成模型的可靠性，终于利大社会。<details>
<summary>Abstract</summary>
Diffusion models and large language models have emerged as leading-edge generative models and have sparked a revolutionary impact on various aspects of human life. However, the practical implementation of these models has also exposed inherent risks, highlighting their dual nature and raising concerns regarding their trustworthiness. Despite the abundance of literature on this subject, a comprehensive survey specifically delving into the intersection of large-scale generative models and their trustworthiness remains largely absent. To bridge this gap, This paper investigates both the long-standing and emerging threats associated with these models across four fundamental dimensions: privacy, security, fairness, and responsibility. In this way, we construct an extensive map outlining the trustworthiness of these models, while also providing practical recommendations and identifying future directions. These efforts are crucial for promoting the trustworthy deployment of these models, ultimately benefiting society as a whole.
</details>
<details>
<summary>摘要</summary>
大数据扩散模型和大语言模型在各种方面发挥了领先的生成模型作用，并且引发了人类生活中的各种变革。然而，实际应用这些模型也暴露了其内在的风险，抛出了它们的两面性和可信worthiness问题。尽管有大量的文献关于这个主题，但是一份特别关注这些模型的长期和新出现的威胁，以及四个基本维度上的可信worthiness映射，尚未得到了系统性的报告。为了填补这个空白，这篇论文调查了这些模型中的长期和新出现的威胁，并在四个基本维度上构建了可信worthiness映射，同时提供了实践的建议和未来方向。这些努力将有助于推动这些模型的可靠应用，最终对社会产生积极的影响。
</details></li>
</ul>
<hr>
<h2 id="Comparing-normalizing-flows-and-diffusion-models-for-prosody-and-acoustic-modelling-in-text-to-speech"><a href="#Comparing-normalizing-flows-and-diffusion-models-for-prosody-and-acoustic-modelling-in-text-to-speech" class="headerlink" title="Comparing normalizing flows and diffusion models for prosody and acoustic modelling in text-to-speech"></a>Comparing normalizing flows and diffusion models for prosody and acoustic modelling in text-to-speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16679">http://arxiv.org/abs/2307.16679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangyan Zhang, Thomas Merritt, Manuel Sam Ribeiro, Biel Tura-Vecino, Kayoko Yanagisawa, Kamil Pokora, Abdelhamid Ezzerg, Sebastian Cygert, Ammar Abbas, Piotr Bilinski, Roberto Barra-Chicote, Daniel Korzekwa, Jaime Lorenzo-Trueba</li>
<li>for: 这paper的目的是比较传统的L1&#x2F;L2损失优化方法和流体和扩散概率模型在文本到语音合成中的表现。</li>
<li>methods: 这paper使用了一个概率模型来生成log-f0和持续时间特征，然后使用这些特征来condition一个音频模型生成mel-spectrogram。</li>
<li>results: 实验结果显示，流体基本模型在spectrogram预测中达到了最好的性能，超过相当的扩散和L1模型。此外，扩散和流体概率预测器都导致了significant提高 compared to typical L2-trained prosody models。<details>
<summary>Abstract</summary>
Neural text-to-speech systems are often optimized on L1/L2 losses, which make strong assumptions about the distributions of the target data space. Aiming to improve those assumptions, Normalizing Flows and Diffusion Probabilistic Models were recently proposed as alternatives. In this paper, we compare traditional L1/L2-based approaches to diffusion and flow-based approaches for the tasks of prosody and mel-spectrogram prediction for text-to-speech synthesis. We use a prosody model to generate log-f0 and duration features, which are used to condition an acoustic model that generates mel-spectrograms. Experimental results demonstrate that the flow-based model achieves the best performance for spectrogram prediction, improving over equivalent diffusion and L1 models. Meanwhile, both diffusion and flow-based prosody predictors result in significant improvements over a typical L2-trained prosody models.
</details>
<details>
<summary>摘要</summary>
traditional L1/L2-based方法与流体和扩散概率模型相比，在文本至语音合成 tasks中进行比较。我们使用一个气质模型生成 log-f0 和持续时间特征，这些特征用于condition一个生成 mel-spectrogram 的声学模型。实验结果表明，流体基本模型在 spectrogram 预测中取得最好的性能，超过相同的扩散和 L1 模型。此外， beiden diffusion 和流体气质预测器都导致了 Typical L2 训练的气质模型显著提高。
</details></li>
</ul>
<hr>
<h2 id="End-to-End-Reinforcement-Learning-for-Torque-Based-Variable-Height-Hopping"><a href="#End-to-End-Reinforcement-Learning-for-Torque-Based-Variable-Height-Hopping" class="headerlink" title="End-to-End Reinforcement Learning for Torque Based Variable Height Hopping"></a>End-to-End Reinforcement Learning for Torque Based Variable Height Hopping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16676">http://arxiv.org/abs/2307.16676</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raghav Soni, Daniel Harnack, Hauke Isermann, Sotaro Fushimi, Shivesh Kumar, Frank Kirchner</li>
<li>for: 这 paper 是为了研究一种基于 Reinforcement Learning 的扭矩控制器，以便在激活跳跃时自动掌握不同跳跃阶段的控制。</li>
<li>methods: 这 paper 使用了 Reinforcement Learning 技术，通过学习练习控制器来适应不同的跳跃阶段。</li>
<li>results: 这 paper 的研究结果表明，使用这种基于 Reinforcement Learning 的扭矩控制器可以在不同的跳跃阶段中自动掌握控制，并且可以在实际中成功部署到机器人上。<details>
<summary>Abstract</summary>
Legged locomotion is arguably the most suited and versatile mode to deal with natural or unstructured terrains. Intensive research into dynamic walking and running controllers has recently yielded great advances, both in the optimal control and reinforcement learning (RL) literature. Hopping is a challenging dynamic task involving a flight phase and has the potential to increase the traversability of legged robots. Model based control for hopping typically relies on accurate detection of different jump phases, such as lift-off or touch down, and using different controllers for each phase. In this paper, we present a end-to-end RL based torque controller that learns to implicitly detect the relevant jump phases, removing the need to provide manual heuristics for state detection. We also extend a method for simulation to reality transfer of the learned controller to contact rich dynamic tasks, resulting in successful deployment on the robot after training without parameter tuning.
</details>
<details>
<summary>摘要</summary>
四肢行走是可能最适合和最多样化的行走模式，能够应对自然或无结构的地形。最近，对于动态步态和奖励学习（RL）文献中的研究做出了大量进步。跳跃是一种复杂的动态任务，具有飞行阶段，可以提高四肢机器人的通行性。基于模型的控制方法通常需要精准地探测不同跳跃阶段，如升空或接触，并使用不同的控制器来处理每个阶段。在这篇论文中，我们提出了一种端到端RL基于扭矩控制器，通过透传学习来适应不同跳跃阶段，从而消除了手动规则的需求。此外，我们还扩展了在实际中使用学习到的控制器的方法，并成功在机器人上部署无需参数调整。
</details></li>
</ul>
<hr>
<h2 id="Generative-models-for-wearables-data"><a href="#Generative-models-for-wearables-data" class="headerlink" title="Generative models for wearables data"></a>Generative models for wearables data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16664">http://arxiv.org/abs/2307.16664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arinbjörn Kolbeinsson, Luca Foschini</li>
<li>for:  solves the problem of data scarcity in medical research by generating realistic wearable activity data.</li>
<li>methods: uses a multi-task self-attention model to generate the data.</li>
<li>results: the generated data is similar to genuine samples, as demonstrated through both quantitative and qualitative approaches.Here’s the Chinese translation of the three points:</li>
<li>for:  solves the problem of 医学研究数据短缺问题 by generating 生成医学活动数据.</li>
<li>methods: uses a multi-task self-attention model to generate the data.</li>
<li>results: the generated data is similar to genuine samples, as demonstrated through both quantitative and qualitative approaches.I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Data scarcity is a common obstacle in medical research due to the high costs associated with data collection and the complexity of gaining access to and utilizing data. Synthesizing health data may provide an efficient and cost-effective solution to this shortage, enabling researchers to explore distributions and populations that are not represented in existing observations or difficult to access due to privacy considerations. To that end, we have developed a multi-task self-attention model that produces realistic wearable activity data. We examine the characteristics of the generated data and quantify its similarity to genuine samples with both quantitative and qualitative approaches.
</details>
<details>
<summary>摘要</summary>
医学研究中的数据短缺是一个常见的障碍因素，这是因为数据收集的成本高昂，以及获取和利用数据的复杂性。生成健康数据可能提供一个有效和成本高的解决方案，允许研究人员探索现有观察数据中未表现出来的分布和人口，以及因隐私考虑而Difficult to access的数据。为此，我们开发了一种多任务自注意模型，生成了真实的穿戴活动数据。我们分析生成数据的特点，并使用量化和质量方法衡量与真实样本的相似性。
</details></li>
</ul>
<hr>
<h2 id="Graph-Structure-from-Point-Clouds-Geometric-Attention-is-All-You-Need"><a href="#Graph-Structure-from-Point-Clouds-Geometric-Attention-is-All-You-Need" class="headerlink" title="Graph Structure from Point Clouds: Geometric Attention is All You Need"></a>Graph Structure from Point Clouds: Geometric Attention is All You Need</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16662">http://arxiv.org/abs/2307.16662</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/murnanedaniel/geometricattention">https://github.com/murnanedaniel/geometricattention</a></li>
<li>paper_authors: Daniel Murnane</li>
<li>for: 这篇论文主要针对高能物理中点云问题进行了研究，特别是如何在这些问题中构建图structure。</li>
<li>methods: 该论文提出了一种注意力机制，可以在学习空间中自动构建一个图，以处理点云问题中的流量相关性。</li>
<li>results: 该论文在标注粒子束 зада务上测试了这种 Architecture，称为GravNetNorm，并显示其和其他相似模型相比，具有类似的准精度，而且使用的计算资源相对较少。<details>
<summary>Abstract</summary>
The use of graph neural networks has produced significant advances in point cloud problems, such as those found in high energy physics. The question of how to produce a graph structure in these problems is usually treated as a matter of heuristics, employing fully connected graphs or K-nearest neighbors. In this work, we elevate this question to utmost importance as the Topology Problem. We propose an attention mechanism that allows a graph to be constructed in a learned space that handles geometrically the flow of relevance, providing one solution to the Topology Problem. We test this architecture, called GravNetNorm, on the task of top jet tagging, and show that it is competitive in tagging accuracy, and uses far fewer computational resources than all other comparable models.
</details>
<details>
<summary>摘要</summary>
使用图 нейрон网络已经取得了高能物理学中点云问题的显著进步。通常，在这些问题中，构建图结构是视为低级别的决策，通过全连接图或K最近邻居的方法进行实现。在这个工作中，我们升级这个问题为最高优先级的topology问题。我们提议一种注意力机制，使得在学习空间中构建的图可以正确地处理流量的 geomtry，提供一个解决topology问题的方案。我们测试了这个架构，称为GravNetNorm，在top jet标签任务上，并显示它与其他相似模型相比，具有竞争力的标签准确率，并使用的计算资源比其他模型少得多。
</details></li>
</ul>
<hr>
<h2 id="Proactive-Resource-Request-for-Disaster-Response-A-Deep-Learning-based-Optimization-Model"><a href="#Proactive-Resource-Request-for-Disaster-Response-A-Deep-Learning-based-Optimization-Model" class="headerlink" title="Proactive Resource Request for Disaster Response: A Deep Learning-based Optimization Model"></a>Proactive Resource Request for Disaster Response: A Deep Learning-based Optimization Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16661">http://arxiv.org/abs/2307.16661</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongzhe Zhang, Xiaohang Zhao, Xiao Fang, Bintong Chen</li>
<li>for: 这个论文主要是为了解决灾难应急响应中资源管理问题，尤其是决定紧急需求的资源量。</li>
<li>methods: 这篇论文使用了深度学习方法来预测未来需求，并根据这些特点形式化了一个难题优化模型，最后提出了一种有效的解决方案。</li>
<li>results: 论文通过使用实际数据和 simulate 数据进行比较，证明了其比现有方法更高效和有更好的性能，同时在多个参与者和多个目标下也表现出了优异性。<details>
<summary>Abstract</summary>
Disaster response is critical to save lives and reduce damages in the aftermath of a disaster. Fundamental to disaster response operations is the management of disaster relief resources. To this end, a local agency (e.g., a local emergency resource distribution center) collects demands from local communities affected by a disaster, dispatches available resources to meet the demands, and requests more resources from a central emergency management agency (e.g., Federal Emergency Management Agency in the U.S.). Prior resource management research for disaster response overlooks the problem of deciding optimal quantities of resources requested by a local agency. In response to this research gap, we define a new resource management problem that proactively decides optimal quantities of requested resources by considering both currently unfulfilled demands and future demands. To solve the problem, we take salient characteristics of the problem into consideration and develop a novel deep learning method for future demand prediction. We then formulate the problem as a stochastic optimization model, analyze key properties of the model, and propose an effective solution method to the problem based on the analyzed properties. We demonstrate the superior performance of our method over prevalent existing methods using both real world and simulated data. We also show its superiority over prevalent existing methods in a multi-stakeholder and multi-objective setting through simulations.
</details>
<details>
<summary>摘要</summary>
灾害应急应对是保存生命和减少灾害的关键。紧急应急管理的核心是管理紧急救援资源。因此，当地机构（例如本地紧急资源分配中心）会收集受到灾害影响的当地社区的需求，派发可用资源以满足需求，并请求中央紧急管理机构（例如美国联邦紧急管理署）提供更多资源。现有的紧急应急资源管理研究未能考虑当地机构请求最佳资源量的问题。为了解决这个研究漏洞，我们定义了一个新的资源管理问题，该问题考虑当地机构请求资源量的优化问题，并考虑当前未满足的需求和未来需求。为了解决这个问题，我们考虑了问题的重要特征，并开发了一种新的深度学习方法来预测未来需求。然后，我们将问题转化为一个随机优化模型，分析了模型的关键属性，并提出了一种有效的解决方案。我们通过使用实际数据和 simulate 数据进行比较，证明了我们的方法在现有方法之上具有显著的优势。此外，我们通过多个各自目标和多个利益相关者的多Objective 模拟来证明我们的方法在多个环境下具有优秀的表现。
</details></li>
</ul>
<hr>
<h2 id="Sequential-and-Shared-Memory-Parallel-Algorithms-for-Partitioned-Local-Depths"><a href="#Sequential-and-Shared-Memory-Parallel-Algorithms-for-Partitioned-Local-Depths" class="headerlink" title="Sequential and Shared-Memory Parallel Algorithms for Partitioned Local Depths"></a>Sequential and Shared-Memory Parallel Algorithms for Partitioned Local Depths</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16652">http://arxiv.org/abs/2307.16652</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya Devarakonda, Grey Ballard</li>
<li>for: 本文是为了开发一种基于对比距离的强相关关系标识方法，以便在含有大量数据点和距离对的情况下，快速发现强相关关系。</li>
<li>methods: 本文使用了两种变体的算法，它们都是基于对比距离的，并且使用了分区本地深度（PaLD）方法进行社区结构分析。</li>
<li>results: 本文的计算和通信成本分析表明，sequential算法是通信优化的，即使在高并发情况下，也能够保持优化的性能。此外， authors 还提出了一些性能优化策略，使得 sequential 实现中的速度可以达到 $29\times$ 的提升，并且 parallel 实现中的速度可以达到 $19.4\times$。<details>
<summary>Abstract</summary>
In this work, we design, analyze, and optimize sequential and shared-memory parallel algorithms for partitioned local depths (PaLD). Given a set of data points and pairwise distances, PaLD is a method for identifying strength of pairwise relationships based on relative distances, enabling the identification of strong ties within dense and sparse communities even if their sizes and within-community absolute distances vary greatly. We design two algorithmic variants that perform community structure analysis through triplet comparisons of pairwise distances. We present theoretical analyses of computation and communication costs and prove that the sequential algorithms are communication optimal, up to constant factors. We introduce performance optimization strategies that yield sequential speedups of up to $29\times$ over a baseline sequential implementation and parallel speedups of up to $19.4\times$ over optimized sequential implementations using up to $32$ threads on an Intel multicore CPU.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们设计、分析和优化了继承和共享内存并行算法 для分区本地深度（PaLD）。给定一个数据点集和对应的距离对，PaLD是一种基于相对距离的对Pairwise关系强度识别方法，允许发现具有不同大小和内部绝对距离的稠密和稀疏社区的强关系。我们设计了两种算法变体，通过对三元比较pairwise距离来进行社区结构分析。我们提供了计算和通信成本的理论分析，并证明sequential算法是通信优化的，即在常数因子上。我们还提出了性能优化策略，可以实现sequential执行速度提升到29倍，并且并行执行速度提升到19.4倍，使用最多32个Intel多核CPU进行优化。
</details></li>
</ul>
<hr>
<h2 id="UDAMA-Unsupervised-Domain-Adaptation-through-Multi-discriminator-Adversarial-Training-with-Noisy-Labels-Improves-Cardio-fitness-Prediction"><a href="#UDAMA-Unsupervised-Domain-Adaptation-through-Multi-discriminator-Adversarial-Training-with-Noisy-Labels-Improves-Cardio-fitness-Prediction" class="headerlink" title="UDAMA: Unsupervised Domain Adaptation through Multi-discriminator Adversarial Training with Noisy Labels Improves Cardio-fitness Prediction"></a>UDAMA: Unsupervised Domain Adaptation through Multi-discriminator Adversarial Training with Noisy Labels Improves Cardio-fitness Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16651">http://arxiv.org/abs/2307.16651</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yvonneywu/udama">https://github.com/yvonneywu/udama</a></li>
<li>paper_authors: Yu Wu, Dimitris Spathis, Hong Jia, Ignacio Perez-Pozuelo, Tomas Gonzales, Soren Brage, Nicholas Wareham, Cecilia Mascolo</li>
<li>for: 这篇论文是为了提出一种基于频率预测的域 adaptation 方法，以解决因量化差异引起的分布偏移问题。</li>
<li>methods: 该方法包括两个关键组成部分：无监督域适应和多种抑制器对抗训练。首先，我们在银标标签数据上进行无监督域适应，然后与金标标签数据进行对抗适应，同时使用两个域抑制器。</li>
<li>results: 我们通过应用该方法于心血管健康预测任务来证明其实用性。结果表明，UDAMA可以有效地缓解分布偏移问题，并在不同的标签偏移设置下表现出优秀的性能。此外，我们通过使用两个自由生活群体研究（Fenland和BBVS）的数据，证明UDAMA可以与其他转移学习和域适应模型相比，在规模上提供更好的预测性能。<details>
<summary>Abstract</summary>
Deep learning models have shown great promise in various healthcare monitoring applications. However, most healthcare datasets with high-quality (gold-standard) labels are small-scale, as directly collecting ground truth is often costly and time-consuming. As a result, models developed and validated on small-scale datasets often suffer from overfitting and do not generalize well to unseen scenarios. At the same time, large amounts of imprecise (silver-standard) labeled data, annotated by approximate methods with the help of modern wearables and in the absence of ground truth validation, are starting to emerge. However, due to measurement differences, this data displays significant label distribution shifts, which motivates the use of domain adaptation. To this end, we introduce UDAMA, a method with two key components: Unsupervised Domain Adaptation and Multidiscriminator Adversarial Training, where we pre-train on the silver-standard data and employ adversarial adaptation with the gold-standard data along with two domain discriminators. In particular, we showcase the practical potential of UDAMA by applying it to Cardio-respiratory fitness (CRF) prediction. CRF is a crucial determinant of metabolic disease and mortality, and it presents labels with various levels of noise (goldand silver-standard), making it challenging to establish an accurate prediction model. Our results show promising performance by alleviating distribution shifts in various label shift settings. Additionally, by using data from two free-living cohort studies (Fenland and BBVS), we show that UDAMA consistently outperforms up to 12% compared to competitive transfer learning and state-of-the-art domain adaptation models, paving the way for leveraging noisy labeled data to improve fitness estimation at scale.
</details>
<details>
<summary>摘要</summary>
深度学习模型在医疗监测应用中表现出了很大的搭配性。然而，大多数医疗数据集（高品质标签）是小规模的，因为直接收集真实标签是经济不可能和时间consuming。因此，在小规模数据集上开发和验证的模型通常会过拟合，并不能很好地适应未见enario。同时，大量的不准确（银标准）标签数据，通过现代穿戴物和不含真实标签验证的方式获得，开始出现。然而，由于测量差异，这些数据会显示明显的标签分布偏移，这种情况驱动了我们使用领域适应。为此，我们提出了UDAMA方法，它包括无监督领域适应和多护卫者对抗预训练。我们在Cardio-respiratory fitness（CRF）预测中应用了UDAMA方法，CRF是生物 markers的一个重要指标，同时标签具有各种噪音（银标准和金标准），因此建立准确的预测模型是一项挑战。我们的结果表明，UDAMA方法可以有效地缓解标签分布偏移，并在不同的标签偏移设置下表现出优秀的性能。此外，通过使用两个自由生活 cohort studies（Fenland和BBVS）的数据，我们表明了UDAMA方法可以在不同的标签偏移设置下 consistently outperform up to 12% compared to competitive transfer learning和领域适应模型，为了利用噪音标签数据来提高健身估计而做出了重要贡献。
</details></li>
</ul>
<hr>
<h2 id="LLMs4OL-Large-Language-Models-for-Ontology-Learning"><a href="#LLMs4OL-Large-Language-Models-for-Ontology-Learning" class="headerlink" title="LLMs4OL: Large Language Models for Ontology Learning"></a>LLMs4OL: Large Language Models for Ontology Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16648">http://arxiv.org/abs/2307.16648</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hamedbabaei/llms4ol">https://github.com/hamedbabaei/llms4ol</a></li>
<li>paper_authors: Hamed Babaei Giglou, Jennifer D’Souza, Sören Auer</li>
<li>for: 本文探讨了使用大语言模型（LLMs）进行ontology learning（OL）的可能性，以捕捉自然语言文本中的知识。</li>
<li>methods: 本文使用了零例示提问方法来测试九种不同的LLM模型家族，以完成三种主要的OL任务：词类型分类、生物地理知识发现和非分类关系提取。</li>
<li>results: results show that LLMs can effectively apply their language pattern capturing capability to OL tasks, achieving high accuracy in all three tasks and outperforming traditional OL methods. Additionally, the evaluations encompass diverse genres of ontological knowledge, including lexicosemantic knowledge in WordNet, geographical knowledge in GeoNames, and medical knowledge in UMLS.<details>
<summary>Abstract</summary>
We propose the LLMs4OL approach, which utilizes Large Language Models (LLMs) for Ontology Learning (OL). LLMs have shown significant advancements in natural language processing, demonstrating their ability to capture complex language patterns in different knowledge domains. Our LLMs4OL paradigm investigates the following hypothesis: \textit{Can LLMs effectively apply their language pattern capturing capability to OL, which involves automatically extracting and structuring knowledge from natural language text?} To test this hypothesis, we conduct a comprehensive evaluation using the zero-shot prompting method. We evaluate nine different LLM model families for three main OL tasks: term typing, taxonomy discovery, and extraction of non-taxonomic relations. Additionally, the evaluations encompass diverse genres of ontological knowledge, including lexicosemantic knowledge in WordNet, geographical knowledge in GeoNames, and medical knowledge in UMLS.
</details>
<details>
<summary>摘要</summary>
我们提出LLMs4OL方法，利用大型语言模型（LLMs）进行 Ontology Learning（OL）。 LLMS 在自然语言处理方面已经显示出了重要的进步，能够捕捉不同知识领域中的复杂语言模式。我们的LLMs4OL paradigm探讨以下假设：\textit{可以 LLMS 通过自然语言文本中自动抽取和结构知识来进行 OL？} 为了证明这一假设，我们进行了全面的评估，使用零容器提示方法。我们评估了九种不同的 LLMS 模型家族，对三个主要的 OL 任务进行了评估：命名类型、地理知识发现和非分类关系抽取。此外，评估还涵盖了不同类型的 ontological 知识，包括 WordNet 的lexicosemantic 知识、GeoNames 的地理知识和 UMLS 的医学知识。
</details></li>
</ul>
<hr>
<h2 id="Text-CRS-A-Generalized-Certified-Robustness-Framework-against-Textual-Adversarial-Attacks"><a href="#Text-CRS-A-Generalized-Certified-Robustness-Framework-against-Textual-Adversarial-Attacks" class="headerlink" title="Text-CRS: A Generalized Certified Robustness Framework against Textual Adversarial Attacks"></a>Text-CRS: A Generalized Certified Robustness Framework against Textual Adversarial Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16630">http://arxiv.org/abs/2307.16630</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Eyr3/TextCRS">https://github.com/Eyr3/TextCRS</a></li>
<li>paper_authors: Xinyu Zhang, Hanbin Hong, Yuan Hong, Peng Huang, Binghui Wang, Zhongjie Ba, Kui Ren</li>
<li>for: 防止文本攻击，提高模型Robustness</li>
<li>methods: 基于随机缓和的泛化证明框架Text-CRS，对word-level adversarial operation进行 permutation和embedding space中的证明bounds，并考虑数字字典之间的数学关系和随机缓和分布来提高证明准确性和半径</li>
<li>results: Text-CRS可以Addressing all four word-level adversarial operations，Significantly improve certified accuracy and radius，并提供了word-level adversarial operation的首个证明精度和半径的 benchmark，Outperform state-of-the-art certification against synonym substitution attacks.<details>
<summary>Abstract</summary>
The language models, especially the basic text classification models, have been shown to be susceptible to textual adversarial attacks such as synonym substitution and word insertion attacks. To defend against such attacks, a growing body of research has been devoted to improving the model robustness. However, providing provable robustness guarantees instead of empirical robustness is still widely unexplored. In this paper, we propose Text-CRS, a generalized certified robustness framework for natural language processing (NLP) based on randomized smoothing. To our best knowledge, existing certified schemes for NLP can only certify the robustness against $\ell_0$ perturbations in synonym substitution attacks. Representing each word-level adversarial operation (i.e., synonym substitution, word reordering, insertion, and deletion) as a combination of permutation and embedding transformation, we propose novel smoothing theorems to derive robustness bounds in both permutation and embedding space against such adversarial operations. To further improve certified accuracy and radius, we consider the numerical relationships between discrete words and select proper noise distributions for the randomized smoothing. Finally, we conduct substantial experiments on multiple language models and datasets. Text-CRS can address all four different word-level adversarial operations and achieve a significant accuracy improvement. We also provide the first benchmark on certified accuracy and radius of four word-level operations, besides outperforming the state-of-the-art certification against synonym substitution attacks.
</details>
<details>
<summary>摘要</summary>
“语言模型，特别是基本文本分类模型，已经被证明容易受到文本敌对攻击，如同Synonym替换和单词插入攻击。为了防止这些攻击，研究人员已经投入了大量时间和精力来提高模型的Robustness。然而，提供可证明的Robustness保证而不是实际的Robustness仍然是未探索的领域。在这篇论文中，我们提出了Text-CRS，一种泛化证明Robustness框架 для自然语言处理（NLP），基于随机化缓和。我们知道，现有的NLP证明可以只证明对于 $\ell_0$ 杂乱攻击的Robustness。我们将每种单词级敌对操作（ synonym替换、单词重新排序、插入和删除）表示为一种组合的 permutation 和 embedding 变换。我们提出了新的缓和定理，以derive Robustness  bound在 permutation 和 embedding 空间中对于这些敌对操作。为了进一步提高证明精度和半径，我们考虑了字符串间的数学关系，并选择了适合的噪声分布。最后，我们进行了大量的实验， Text-CRS 可以处理所有的四种单词级敌对操作，并达到了显著的准确率提高。我们还提供了对四种单词级敌对操作的证明精度和半径的首个benchmark，并超越了现有的synonym替换攻击证明。”
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Causal-Bayesian-Optimization"><a href="#Adversarial-Causal-Bayesian-Optimization" class="headerlink" title="Adversarial Causal Bayesian Optimization"></a>Adversarial Causal Bayesian Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16625">http://arxiv.org/abs/2307.16625</a></li>
<li>repo_url: None</li>
<li>paper_authors: Scott Sussex, Pier Giuseppe Sessa, Anastasiia Makarova, Andreas Krause</li>
<li>For: The paper is written for optimizing a downstream reward variable in the presence of adversarial interventions on an unknown structural causal model.* Methods: The paper introduces the first algorithm for Adversarial Causal Bayesian Optimization (ACBO) with bounded regret, called Causal Bayesian Optimization with Multiplicative Weights (CBO-MW). The approach combines classical online learning strategies with causal modeling of the rewards, using optimistic counterfactual reward estimates propagated through the causal graph.* Results: The paper derives regret bounds for CBO-MW that naturally depend on graph-related quantities, and proposes a scalable implementation for the case of combinatorial interventions and submodular rewards. Empirical results show that CBO-MW outperforms non-causal and non-adversarial Bayesian optimization methods on synthetic environments and real-world data-based environments.<details>
<summary>Abstract</summary>
In Causal Bayesian Optimization (CBO), an agent intervenes on an unknown structural causal model to maximize a downstream reward variable. In this paper, we consider the generalization where other agents or external events also intervene on the system, which is key for enabling adaptiveness to non-stationarities such as weather changes, market forces, or adversaries. We formalize this generalization of CBO as Adversarial Causal Bayesian Optimization (ACBO) and introduce the first algorithm for ACBO with bounded regret: Causal Bayesian Optimization with Multiplicative Weights (CBO-MW). Our approach combines a classical online learning strategy with causal modeling of the rewards. To achieve this, it computes optimistic counterfactual reward estimates by propagating uncertainty through the causal graph. We derive regret bounds for CBO-MW that naturally depend on graph-related quantities. We further propose a scalable implementation for the case of combinatorial interventions and submodular rewards. Empirically, CBO-MW outperforms non-causal and non-adversarial Bayesian optimization methods on synthetic environments and environments based on real-word data. Our experiments include a realistic demonstration of how CBO-MW can be used to learn users' demand patterns in a shared mobility system and reposition vehicles in strategic areas.
</details>
<details>
<summary>摘要</summary>
在 causal bayesian 优化（CBO）中, an agent  intervenes on an unknown 结构 causal 模型以 Maximize  downstream 奖 variable. 在这篇 paper 中, we consider 这个泛化, where other agents or external events 也 intervene on the system, which is key for enabling adaptiveness to non-stationarities such as weather changes, market forces, or adversaries. We formalize this generalization of CBO as Adversarial Causal Bayesian Optimization (ACBO) and introduce the first algorithm for ACBO with bounded regret: Causal Bayesian Optimization with Multiplicative Weights (CBO-MW). Our approach combines a classical online learning strategy with causal modeling of the rewards. To achieve this, it computes optimistic counterfactual reward estimates by propagating uncertainty through the causal graph. We derive regret bounds for CBO-MW that naturally depend on graph-related quantities. We further propose a scalable implementation for the case of combinatorial interventions and submodular rewards. Empirically, CBO-MW outperforms non-causal and non-adversarial Bayesian optimization methods on synthetic environments and environments based on real-word data. Our experiments include a realistic demonstration of how CBO-MW can be used to learn users' demand patterns in a shared mobility system and reposition vehicles in strategic areas.Here's the translation in Traditional Chinese:在 causal bayesian 优化（CBO）中, an agent  intervenes on an unknown 结构 causal 模型以 Maximize  downstream 奖 variable. 在这篇 paper 中, we consider 这个泛化, where other agents or external events 也 intervene on the system, which is key for enabling adaptiveness to non-stationarities such as weather changes, market forces, or adversaries. We formalize this generalization of CBO as Adversarial Causal Bayesian Optimization (ACBO) and introduce the first algorithm for ACBO with bounded regret: Causal Bayesian Optimization with Multiplicative Weights (CBO-MW). Our approach combines a classical online learning strategy with causal modeling of the rewards. To achieve this, it computes optimistic counterfactual reward estimates by propagating uncertainty through the causal graph. We derive regret bounds for CBO-MW that naturally depend on graph-related quantities. We further propose a scalable implementation for the case of combinatorial interventions and submodular rewards. Empirically, CBO-MW outperforms non-causal and non-adversarial Bayesian optimization methods on synthetic environments and environments based on real-word data. Our experiments include a realistic demonstration of how CBO-MW can be used to learn users' demand patterns in a shared mobility system and reposition vehicles in strategic areas.
</details></li>
</ul>
<hr>
<h2 id="Detecting-diabetic-retinopathy-severity-through-fundus-images-using-an-ensemble-of-classifiers"><a href="#Detecting-diabetic-retinopathy-severity-through-fundus-images-using-an-ensemble-of-classifiers" class="headerlink" title="Detecting diabetic retinopathy severity through fundus images using an ensemble of classifiers"></a>Detecting diabetic retinopathy severity through fundus images using an ensemble of classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16622">http://arxiv.org/abs/2307.16622</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eduard Popescu, Adrian Groza, Ioana Damian</li>
<li>for: 这个论文是用来检测糖尿病肠病严重程度的方法。</li>
<li>methods: 这个方法包括数据预处理、图像分割、特征提取和 ensemble 分类。</li>
<li>results: 这个方法可以准确地检测糖尿病肠病的严重程度。Here’s the full summary in Simplified Chinese:</li>
<li>for: 这个论文是用来检测糖尿病肠病严重程度的方法。</li>
<li>methods: 这个方法包括数据预处理、图像分割、特征提取和 ensemble 分类。</li>
<li>results: 这个方法可以准确地检测糖尿病肠病的严重程度。<details>
<summary>Abstract</summary>
Diabetic retinopathy is an ocular condition that affects individuals with diabetes mellitus. It is a common complication of diabetes that can impact the eyes and lead to vision loss. One method for diagnosing diabetic retinopathy is the examination of the fundus of the eye. An ophthalmologist examines the back part of the eye, including the retina, optic nerve, and the blood vessels that supply the retina. In the case of diabetic retinopathy, the blood vessels in the retina deteriorate and can lead to bleeding, swelling, and other changes that affect vision. We proposed a method for detecting diabetic diabetic severity levels. First, a set of data-prerpocessing is applied to available data: adaptive equalisation, color normalisation, Gaussian filter, removal of the optic disc and blood vessels. Second, we perform image segmentation for relevant markers and extract features from the fundus images. Third, we apply an ensemble of classifiers and we assess the trust in the system.
</details>
<details>
<summary>摘要</summary>
糖尿病Retinopathy是一种眼部疾病，影响糖尿病患者。它是糖尿病的常见侵化，可以影响视力。一种用于诊断糖尿病Retinopathy的方法是对眼部背部进行检查，包括肠视硬化、血管和视神经。在糖尿病Retinopathy中，肠视硬化导致血管在肠部受损，可能导致出血、肿胀和其他影响视力的变化。我们提出了一种方法来评估糖尿病严重程度。首先，我们对可用数据进行数据处理：适应性平衡、颜色normal化、Gaussian滤波器和 removing the optic disc and blood vessels。第二，我们实现图像分割，检测 relevante markers并提取fundus图像中的特征。第三，我们应用一个 ensemble of classifiers，并评估系统的可信度。
</details></li>
</ul>
<hr>
<h2 id="LaplaceConfidence-a-Graph-based-Approach-for-Learning-with-Noisy-Labels"><a href="#LaplaceConfidence-a-Graph-based-Approach-for-Learning-with-Noisy-Labels" class="headerlink" title="LaplaceConfidence: a Graph-based Approach for Learning with Noisy Labels"></a>LaplaceConfidence: a Graph-based Approach for Learning with Noisy Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16614">http://arxiv.org/abs/2307.16614</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingcai Chen, Yuntao Du, Wei Tang, Baoming Zhang, Hao Cheng, Shuwei Qian, Chongjun Wang</li>
<li>for: 本研究旨在开发一种可靠地处理噪声标签的机器学习算法，以便在实际应用中使用。</li>
<li>methods: 本研究使用 LaplaceConfidence 方法，它根据数据中的特征表示生成图，然后使用 Laplacian 能量来计算样本的净概率。</li>
<li>results: 实验表明，LaplaceConfidence 方法在 benchmark 数据集上比前一代方法更高效，并且在实际噪声数据上也显示出良好的性能。<details>
<summary>Abstract</summary>
In real-world applications, perfect labels are rarely available, making it challenging to develop robust machine learning algorithms that can handle noisy labels. Recent methods have focused on filtering noise based on the discrepancy between model predictions and given noisy labels, assuming that samples with small classification losses are clean. This work takes a different approach by leveraging the consistency between the learned model and the entire noisy dataset using the rich representational and topological information in the data. We introduce LaplaceConfidence, a method that to obtain label confidence (i.e., clean probabilities) utilizing the Laplacian energy. Specifically, it first constructs graphs based on the feature representations of all noisy samples and minimizes the Laplacian energy to produce a low-energy graph. Clean labels should fit well into the low-energy graph while noisy ones should not, allowing our method to determine data's clean probabilities. Furthermore, LaplaceConfidence is embedded into a holistic method for robust training, where co-training technique generates unbiased label confidence and label refurbishment technique better utilizes it. We also explore the dimensionality reduction technique to accommodate our method on large-scale noisy datasets. Our experiments demonstrate that LaplaceConfidence outperforms state-of-the-art methods on benchmark datasets under both synthetic and real-world noise.
</details>
<details>
<summary>摘要</summary>
在实际应用中，完美的标签很少，这使得开发机器学习算法可以处理噪声的挑战变得更大。现有方法是根据模型预测和噪声标签的差异来过滤噪声，假设样本的分类损失小于就是干净的。这个工作采用了不同的方法，利用数据中学习模型和整个噪声数据集的质量信息来确定数据的干净概率。我们引入了LaplaceConfidence方法，它使用Laplacian能量来获取标签信任度（即干净概率）。具体来说，它首先基于所有噪声样本的特征表示构建图，然后使用Laplacian能量来生成低能图。干净标签应该适合低能图中，而噪声标签不应该。这样，LaplaceConfidence方法就可以确定数据的干净概率。此外，LaplaceConfidence方法被 embedding 到了一种抗耗整合方法中，其中使用 co-training 技术生成不偏的标签信任度，并使用标签修复技术更好地利用它。我们还探索了维度减少技术，以便在大规模噪声数据集上使用我们的方法。我们的实验表明，LaplaceConfidence 方法在标准 benchmark 数据集上比状态艺术方法表现更好，并且在实际噪声中也表现出色。
</details></li>
</ul>
<hr>
<h2 id="Noisy-Self-Training-with-Data-Augmentations-for-Offensive-and-Hate-Speech-Detection-Tasks"><a href="#Noisy-Self-Training-with-Data-Augmentations-for-Offensive-and-Hate-Speech-Detection-Tasks" class="headerlink" title="Noisy Self-Training with Data Augmentations for Offensive and Hate Speech Detection Tasks"></a>Noisy Self-Training with Data Augmentations for Offensive and Hate Speech Detection Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16609">http://arxiv.org/abs/2307.16609</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jaugusto97/offense-self-training">https://github.com/jaugusto97/offense-self-training</a></li>
<li>paper_authors: João A. Leite, Carolina Scarton, Diego F. Silva</li>
<li>for: 自动侦测和纠正社交媒体上的不宽容和仇恨言论，以提高社交媒体上的内容质量。</li>
<li>methods: 使用弱 labels 的自我训练方法，使用不宽容和仇恨言论的训练数据，并使用文本数据增强技术来提高预测精度。</li>
<li>results: 显示了自我训练可以提高表现，并且显示了不同于预设方法的“噪音”自我训练方法可以对于不宽容和仇恨言论的预测精度产生负面影响。<details>
<summary>Abstract</summary>
Online social media is rife with offensive and hateful comments, prompting the need for their automatic detection given the sheer amount of posts created every second. Creating high-quality human-labelled datasets for this task is difficult and costly, especially because non-offensive posts are significantly more frequent than offensive ones. However, unlabelled data is abundant, easier, and cheaper to obtain. In this scenario, self-training methods, using weakly-labelled examples to increase the amount of training data, can be employed. Recent "noisy" self-training approaches incorporate data augmentation techniques to ensure prediction consistency and increase robustness against noisy data and adversarial attacks. In this paper, we experiment with default and noisy self-training using three different textual data augmentation techniques across five different pre-trained BERT architectures varying in size. We evaluate our experiments on two offensive/hate-speech datasets and demonstrate that (i) self-training consistently improves performance regardless of model size, resulting in up to +1.5% F1-macro on both datasets, and (ii) noisy self-training with textual data augmentations, despite being successfully applied in similar settings, decreases performance on offensive and hate-speech domains when compared to the default method, even with state-of-the-art augmentations such as backtranslation.
</details>
<details>
<summary>摘要</summary>
在在线社交媒体中，有很多侮辱和仇恨的评论，需要自动检测这些评论的存在，因为每秒钟创建的帖子的数量非常大。创建高质量的人类标注数据集非常困难和昂贵，特别是非侮辱帖子的数量非常多。然而，无标注数据却很易于获得，更加便宜。在这种情况下，可以使用自我教育方法，使用弱标注的示例来增加训练数据的量。最新的“噪音”自我教育方法将数据扩展技术与预训练模型结合，以确保预测的一致性和对噪音数据和敌意攻击的鲁棒性。在这篇论文中，我们对五种不同的预训练BERT架构进行了Default和噪音自我教育的实验，并使用三种文本数据扩展技术。我们对两个侮辱和仇恨域上的两个 dataset进行了评估，结果显示：（i）自我教育可以不anni标注数据大小不同的模型中提高性能，最高提高+1.5% F1-macro，（ii）噪音自我教育，尽管在类似场景中得到了成功，在侮辱和仇恨域上的性能下降，即使使用最新的扩展技术，如回 перевод。
</details></li>
</ul>
<hr>
<h2 id="NLLG-Quarterly-arXiv-Report-06-23-What-are-the-most-influential-current-AI-Papers"><a href="#NLLG-Quarterly-arXiv-Report-06-23-What-are-the-most-influential-current-AI-Papers" class="headerlink" title="NLLG Quarterly arXiv Report 06&#x2F;23: What are the most influential current AI Papers?"></a>NLLG Quarterly arXiv Report 06&#x2F;23: What are the most influential current AI Papers?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04889">http://arxiv.org/abs/2308.04889</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nl2g/quaterly-arxiv">https://github.com/nl2g/quaterly-arxiv</a></li>
<li>paper_authors: Steffen Eger, Christoph Leiter, Jonas Belouadi, Ran Zhang, Aida Kostikova, Daniil Larionov, Yanran Chen, Vivian Fresen</li>
<li>For: This paper aims to provide a quick guide to the most relevant and widely discussed research in the field of Generative AI, with a focus on NLP and ML.* Methods: The paper uses normalized citation counts from the first half of 2023 to identify the 40 most popular papers in the field, and examines the characteristics of these papers, such as their focus on LLMs and ethical considerations.* Results: The paper finds that NLP-related papers are the most influential (around 60% of top papers), and that LLM efficiency, evaluation techniques, and problem-solving with LLMs are core issues investigated in the most heavily cited papers.Here is the information in Simplified Chinese text:</li>
<li>for: 这篇报告目的是为了提供generative AI领域，特别是自然语言处理（NLP）和机器学习（ML）领域最新发展的快速导航。</li>
<li>methods: 该报告使用2023年第一半年的normalized citation counts来确定领域中最受欢迎的40篇论文，并分析这些论文的特点，如它们的关注点是LLMs和伦理考虑。</li>
<li>results: 报告发现，NLP相关的论文是最有影响力的（约60%的推荐篇），并且发现LLM效率、评价技术、伦理考虑和embodied agents等核心问题在最受欢迎的论文中占据主导地位。<details>
<summary>Abstract</summary>
The rapid growth of information in the field of Generative Artificial Intelligence (AI), particularly in the subfields of Natural Language Processing (NLP) and Machine Learning (ML), presents a significant challenge for researchers and practitioners to keep pace with the latest developments. To address the problem of information overload, this report by the Natural Language Learning Group at Bielefeld University focuses on identifying the most popular papers on arXiv, with a specific emphasis on NLP and ML. The objective is to offer a quick guide to the most relevant and widely discussed research, aiding both newcomers and established researchers in staying abreast of current trends. In particular, we compile a list of the 40 most popular papers based on normalized citation counts from the first half of 2023. We observe the dominance of papers related to Large Language Models (LLMs) and specifically ChatGPT during the first half of 2023, with the latter showing signs of declining popularity more recently, however. Further, NLP related papers are the most influential (around 60\% of top papers) even though there are twice as many ML related papers in our data. Core issues investigated in the most heavily cited papers are: LLM efficiency, evaluation techniques, ethical considerations, embodied agents, and problem-solving with LLMs. Additionally, we examine the characteristics of top papers in comparison to others outside the top-40 list (noticing the top paper's focus on LLM related issues and higher number of co-authors) and analyze the citation distributions in our dataset, among others.
</details>
<details>
<summary>摘要</summary>
随着生成人工智能（AI）领域的信息快速增长，特别是自然语言处理（NLP）和机器学习（ML）的子领域，研究人员和实践者面临着保持最新发展的挑战。为了解决信息淹领的问题，这份报告由比萨大学自然语言学习组编写，关注arXiv上最受欢迎的论文，尤其是NLP和ML领域。旨在为新手和已有研究人员提供快速引导，了解当前趋势。我们根据2023年first半年的normalized引用数量编辑出40篇最受欢迎的论文，并观察到2023年first半年LLMs（大语言模型）和ChatGPT的占据率逐渐下降，但NLP相关论文仍占约60%的排名。核心问题在最引导着欢迎的论文中被研究包括：LLM效率、评估技术、伦理考虑、具体代理人和问题解决方法。此外，我们还分析了排名前40篇论文的特点，以及总体的引用分布。
</details></li>
</ul>
<hr>
<h2 id="Audio-visual-video-to-speech-synthesis-with-synthesized-input-audio"><a href="#Audio-visual-video-to-speech-synthesis-with-synthesized-input-audio" class="headerlink" title="Audio-visual video-to-speech synthesis with synthesized input audio"></a>Audio-visual video-to-speech synthesis with synthesized input audio</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16584">http://arxiv.org/abs/2307.16584</a></li>
<li>repo_url: None</li>
<li>paper_authors: Triantafyllos Kefalas, Yannis Panagakis, Maja Pantic</li>
<li>for: 这个论文的目的是研究视频到语音合成中使用视频和声音输入的效果。</li>
<li>methods: 这个论文使用预训练的视频到语音模型来生成缺失的语音信号，然后使用视频和生成的语音作为输入，训练一个视频-声音-到语音合成模型来预测最终重建的语音。</li>
<li>results: 实验表明，这种方法在使用原始波形和мельспектрограм作为目标输出时都是成功的。<details>
<summary>Abstract</summary>
Video-to-speech synthesis involves reconstructing the speech signal of a speaker from a silent video. The implicit assumption of this task is that the sound signal is either missing or contains a high amount of noise/corruption such that it is not useful for processing. Previous works in the literature either use video inputs only or employ both video and audio inputs during training, and discard the input audio pathway during inference. In this work we investigate the effect of using video and audio inputs for video-to-speech synthesis during both training and inference. In particular, we use pre-trained video-to-speech models to synthesize the missing speech signals and then train an audio-visual-to-speech synthesis model, using both the silent video and the synthesized speech as inputs, to predict the final reconstructed speech. Our experiments demonstrate that this approach is successful with both raw waveforms and mel spectrograms as target outputs.
</details>
<details>
<summary>摘要</summary>
视频到语音合成包括从无声视频中重建说话人的语音信号。这个隐式假设是，音频信号缺失或受到干扰或损害，因此不能用于处理。先前的文献中的工作都是在训练时使用视频输入，并在推断时丢弃音频输入。在这个工作中，我们研究了在训练和推断时都使用视频和音频输入的影响。具体来说，我们使用预训练的视频到语音模型来重建缺失的语音信号，然后使用视频和重建的语音作为输入，使用一个audiovisual-to-speech合成模型来预测最终重建的语音。我们的实验结果表明，这种方法在raw波形和mel spectrogram作为目标输出时都是成功的。
</details></li>
</ul>
<hr>
<h2 id="A-multiscale-and-multicriteria-Generative-Adversarial-Network-to-synthesize-1-dimensional-turbulent-fields"><a href="#A-multiscale-and-multicriteria-Generative-Adversarial-Network-to-synthesize-1-dimensional-turbulent-fields" class="headerlink" title="A multiscale and multicriteria Generative Adversarial Network to synthesize 1-dimensional turbulent fields"></a>A multiscale and multicriteria Generative Adversarial Network to synthesize 1-dimensional turbulent fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16580">http://arxiv.org/abs/2307.16580</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlos Granero-Belinchon, Manuel Cabeza Gallucci</li>
<li>for: 这篇论文旨在开发一种基于神经网络的浮动随机场模型，用于生成具有湍流速度统计特征的1维随机场。</li>
<li>methods: 该模型采用了生成对抗网络，基于科尔мого罗夫和奥布霍夫的统计理论，以确保与实验观察结果相符的1)能量分布、2)能量升降和3)不均匀性随着尺度的描述。</li>
<li>results: 模型可以在不同的尺度范围内生成具有高准确性和均匀性的浮动随机场，并且可以捕捉到实验观察到的湍流速度统计特征。<details>
<summary>Abstract</summary>
This article introduces a new Neural Network stochastic model to generate a 1-dimensional stochastic field with turbulent velocity statistics. Both the model architecture and training procedure ground on the Kolmogorov and Obukhov statistical theories of fully developed turbulence, so guaranteeing descriptions of 1) energy distribution, 2) energy cascade and 3) intermittency across scales in agreement with experimental observations. The model is a Generative Adversarial Network with multiple multiscale optimization criteria. First, we use three physics-based criteria: the variance, skewness and flatness of the increments of the generated field that retrieve respectively the turbulent energy distribution, energy cascade and intermittency across scales. Second, the Generative Adversarial Network criterion, based on reproducing statistical distributions, is used on segments of different length of the generated field. Furthermore, to mimic multiscale decompositions frequently used in turbulence's studies, the model architecture is fully convolutional with kernel sizes varying along the multiple layers of the model. To train our model we use turbulent velocity signals from grid turbulence at Modane wind tunnel.
</details>
<details>
<summary>摘要</summary>
First, the model uses three physics-based criteria: the variance, skewness, and flatness of the increments of the generated field, which respectively retrieve the turbulent energy distribution, energy cascade, and intermittency across scales. Additionally, the GAN criterion is based on reproducing statistical distributions and is used on segments of different length of the generated field.To mimic multiscale decompositions frequently used in turbulence studies, the model architecture is fully convolutional with kernel sizes varying along the multiple layers of the model. The training data is turbulent velocity signals from grid turbulence at Modane wind tunnel.
</details></li>
</ul>
<hr>
<h2 id="The-Decimation-Scheme-for-Symmetric-Matrix-Factorization"><a href="#The-Decimation-Scheme-for-Symmetric-Matrix-Factorization" class="headerlink" title="The Decimation Scheme for Symmetric Matrix Factorization"></a>The Decimation Scheme for Symmetric Matrix Factorization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16564">http://arxiv.org/abs/2307.16564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Camilli, Marc Mézard</li>
<li>for: 本文研究了矩阵因子化的推理问题，它在字典学习、推荐系统和深度学习中具有广泛的应用。研究这个问题的基本统计限制是一项十年来社区努力的挑战，而且在矩阵级别 Linear 时仍然没有固定的关键。</li>
<li>methods: 本文使用了一种名为”减少”的方法，它通过将问题映射到一系列的神经网络模型，以实现一次一列&#x2F;一行的因子 recover。尽管这不是最优的方法，但它有优点，即可以进行理论分析。本文将这种方法扩展到两个家族的矩阵上，并对这些矩阵进行了广泛的研究。</li>
<li>results: 对于一类具有紧密支持的先验，我们显示了低温下的复制同步自由能的universal形式。对于稀疏伊辛先验，我们发现了因子化矩阵的存储容量与稀疏性的增长关系，并提出了一种简单的算法，基于基准搜索，实现了减少和矩阵因子化，无需具有有用的初始化。<details>
<summary>Abstract</summary>
Matrix factorization is an inference problem that has acquired importance due to its vast range of applications that go from dictionary learning to recommendation systems and machine learning with deep networks. The study of its fundamental statistical limits represents a true challenge, and despite a decade-long history of efforts in the community, there is still no closed formula able to describe its optimal performances in the case where the rank of the matrix scales linearly with its size. In the present paper, we study this extensive rank problem, extending the alternative 'decimation' procedure that we recently introduced, and carry out a thorough study of its performance. Decimation aims at recovering one column/line of the factors at a time, by mapping the problem into a sequence of neural network models of associative memory at a tunable temperature. Though being sub-optimal, decimation has the advantage of being theoretically analyzable. We extend its scope and analysis to two families of matrices. For a large class of compactly supported priors, we show that the replica symmetric free entropy of the neural network models takes a universal form in the low temperature limit. For sparse Ising prior, we show that the storage capacity of the neural network models diverges as sparsity in the patterns increases, and we introduce a simple algorithm based on a ground state search that implements decimation and performs matrix factorization, with no need of an informative initialization.
</details>
<details>
<summary>摘要</summary>
矩阵分解是一个重要的推理问题，它在从词库学到推荐系统和深度学习中都有广泛的应用。研究其基本统计限制是一项挑战，尽管社区在过去的十年里有努力，仍没有能描述其最佳性能的关闭公式，尤其是当矩阵矩阵的排名与其大小成线性关系时。在 presente 纸上，我们研究这个扩展的排名问题，扩展我们最近提出的代替方法“减少”，并进行了系统的研究。减少方法是通过将问题映射到一系列神经网络模型，以一个可调温度的温度学习 associate memory 的方式来重建一列/一行的因子。虽然不是最佳的方法，但减少方法具有理论分析可行性。我们将其扩展到两个家族的矩阵上，并对这些矩阵进行了严格的分析。对于一个广泛的紧支持矩阵，我们显示了低温限下的自由Entropy 的 neural network 模型具有通用的形式。对于稀疏的 Иссинг 矩阵，我们显示了因子的存储容量在模式的稀疏程度增加时增加，并引入了一种简单的算法，基于地面搜索，实现减少和矩阵分解，无需具有有用的初始化。
</details></li>
</ul>
<hr>
<h2 id="Line-Search-for-Convex-Minimization"><a href="#Line-Search-for-Convex-Minimization" class="headerlink" title="Line Search for Convex Minimization"></a>Line Search for Convex Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16560">http://arxiv.org/abs/2307.16560</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laurent Orseau, Marcus Hutter<br>for:* The paper is written for minimizing quasiconvex (unimodal) functions, and more generally, convex functions.methods:* The paper proposes two principled exact line search algorithms for general convex functions, called $\Delta$-Bisection and $\Delta$-Secant, which take advantage of convexity.* The algorithms use (sub)gradient information and function queries to speed up convergence.results:* The paper shows that the proposed algorithms are faster than their quasiconvex counterparts, often by more than a factor 2, and provide a refined stopping criterion.* The algorithms are tested on a few convex functions and show good performance.Here is the result in Simplified Chinese text:for:* 本文是为了最小化一元函数的 quiticonvex（单峰）函数，以及更一般的凸函数。methods:* 本文提出了两种原理正确的一元线搜索算法，即 $\Delta$-Bisection 和 $\Delta$-Secant，它们利用凸性来加速收敛。* 这些算法使用（子）导函数信息和函数查询来加速收敛。results:* 本文表明了提出的算法比其 quiticonvex 对应算法更快，通常高于2倍，并提供了一个精细的停止标准。* 算法在一些凸函数上进行了测试，显示良好的性能。<details>
<summary>Abstract</summary>
Golden-section search and bisection search are the two main principled algorithms for 1d minimization of quasiconvex (unimodal) functions. The first one only uses function queries, while the second one also uses gradient queries. Other algorithms exist under much stronger assumptions, such as Newton's method. However, to the best of our knowledge, there is no principled exact line search algorithm for general convex functions -- including piecewise-linear and max-compositions of convex functions -- that takes advantage of convexity. We propose two such algorithms: $\Delta$-Bisection is a variant of bisection search that uses (sub)gradient information and convexity to speed up convergence, while $\Delta$-Secant is a variant of golden-section search and uses only function queries.   While bisection search reduces the $x$ interval by a factor 2 at every iteration, $\Delta$-Bisection reduces the (sometimes much) smaller $x^*$-gap $\Delta^x$ (the $x$ coordinates of $\Delta$) by at least a factor 2 at every iteration. Similarly, $\Delta$-Secant also reduces the $x^*$-gap by at least a factor 2 every second function query. Moreover, the $y^*$-gap $\Delta^y$ (the $y$ coordinates of $\Delta$) also provides a refined stopping criterion, which can also be used with other algorithms. Experiments on a few convex functions confirm that our algorithms are always faster than their quasiconvex counterparts, often by more than a factor 2.   We further design a quasi-exact line search algorithm based on $\Delta$-Secant. It can be used with gradient descent as a replacement for backtracking line search, for which some parameters can be finicky to tune -- and we provide examples to this effect, on strongly-convex and smooth functions. We provide convergence guarantees, and confirm the efficiency of quasi-exact line search on a few single- and multivariate convex functions.
</details>
<details>
<summary>摘要</summary>
金叶搜索和bisect搜索是1D最小化非线性函数的两种主要原则性算法。前者只使用函数查询，而后者还使用梯度查询。其他算法存在更加强的假设，如新颖方法。然而，我们所知道的最佳精确直线搜索算法 для总体凸函数，包括分割线性和最大 compositions of 凸函数，是否存在。我们提出了两种算法：Δ-bisect是一种使用梯度信息和凸性加速收敛的bisect搜索变体，而Δ-Secant是一种使用函数查询的 golden-section搜索变体。在每次迭代中，bisect搜索将xInterval分割为一半，而Δ-bisect将(可能非常小)的x* gap Δ^x 分割为至少一半。相似地，Δ-Secant每次查询函数时也将x* gap 分割为至少一半。此外，y* gap Δ^y (y坐标的Delta) 还提供了一种精细的停止条件，可以与其他算法一起使用。我们的算法在一些凸函数上进行了实验，并证明它们在许多情况下比其 quariconvex 版本快速，常常高于2倍。我们还设计了一种 quasi-exact line search algorithm，基于Δ-Secant。它可以与梯度下降作为替换backtracking line search，并且可以在某些参数上进行微调。我们提供了一些示例，包括强拟合和光滑函数。我们提供了收敛保证，并在一些单变量和多变量凸函数上进行了确认。
</details></li>
</ul>
<hr>
<h2 id="Simultaneous-column-based-deep-learning-progression-analysis-of-atrophy-associated-with-AMD-in-longitudinal-OCT-studies"><a href="#Simultaneous-column-based-deep-learning-progression-analysis-of-atrophy-associated-with-AMD-in-longitudinal-OCT-studies" class="headerlink" title="Simultaneous column-based deep learning progression analysis of atrophy associated with AMD in longitudinal OCT studies"></a>Simultaneous column-based deep learning progression analysis of atrophy associated with AMD in longitudinal OCT studies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16559">http://arxiv.org/abs/2307.16559</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adi Szeskin, Roei Yehuda, Or Shmueli, Jaime Levy, Leo Joskowicz</li>
<li>for: The paper is written to accurately quantify retinal atrophy changes associated with dry age-related macular degeneration (AMD) on longitudinal OCT studies.</li>
<li>methods: The paper presents a fully automatic end-to-end pipeline for simultaneously detecting and quantifying time-related atrophy changes in pairs of OCT scans of a patient. The pipeline uses a novel simultaneous multi-channel column-based deep learning model that classifies light scattering patterns in matched pairs of vertical pixel-wide columns (A-scans) in registered prior and current OCT slices (B-scans).</li>
<li>results: The experimental results on 4,040 OCT slices with 5.2M columns from 40 scan pairs of 18 patients show a mean atrophy segments detection precision of 0.90+-0.09, 0.95+-0.06, and 0.74+-0.18, 0.94+-0.12 for atrophy lesions, with an AUC of 0.897, outperforming standalone classification methods by 30+-62% and 27+-0% for atrophy segments and lesions.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目的是准确量化普通病理性膜脱屑病（AMD）的长期 OCT 图像中的膜脱屑变化。</li>
<li>methods: 这篇论文提出了一种完全自动的端到端管道，用于同时检测和量化 consecutive OCT 图像中的时间相关的膜脱屑变化。该管道使用了一种新的同时多通道列式深度学习模型，该模型在注册后 OCT 剖面（B-scan）中匹配的 vertical 像素宽柱（A-scan）中类别光散射模式，以实现同时检测和分割 consecutive OCT 图像中的膜脱屑部分。</li>
<li>results: 实验结果表明，使用了 4,040 个 OCT 剖面和 5.2 万个列的 40 个扫描组合，从 18 名患者（66% 训练&#x2F;验证，33% 测试）中获得了 24.13 ± 14.0 个月的间隔。Complete RPE 和 Outer Retinal Atrophy（cRORA）在 1,998 个 OCT 剖面中被识别出，其中 735 个膜脱屑病变（0.45 万个列），并且获得了 mean atrophy segments 检测精度、回归为 0.90 ± 0.09、0.95 ± 0.06 和 0.74 ± 0.18、0.94 ± 0.12，同时 Simultaneous classification 方法在 atrophy segments 和 lesions 中的检测精度和回归性分别高于 standalone classification 方法 by 30 ± 62% 和 27 ± 0%。<details>
<summary>Abstract</summary>
Purpose: Disease progression of retinal atrophy associated with AMD requires the accurate quantification of the retinal atrophy changes on longitudinal OCT studies. It is based on finding, comparing, and delineating subtle atrophy changes on consecutive pairs (prior and current) of unregistered OCT scans. Methods: We present a fully automatic end-to-end pipeline for the simultaneous detection and quantification of time-related atrophy changes associated with dry AMD in pairs of OCT scans of a patient. It uses a novel simultaneous multi-channel column-based deep learning model trained on registered pairs of OCT scans that concurrently detects and segments retinal atrophy segments in consecutive OCT scans by classifying light scattering patterns in matched pairs of vertical pixel-wide columns (A-scans) in registered prior and current OCT slices (B-scans). Results: Experimental results on 4,040 OCT slices with 5.2M columns from 40 scans pairs of 18 patients (66% training/validation, 33% testing) with 24.13+-14.0 months apart in which Complete RPE and Outer Retinal Atrophy (cRORA) was identified in 1,998 OCT slices (735 atrophy lesions from 3,732 segments, 0.45M columns) yield a mean atrophy segments detection precision, recall of 0.90+-0.09, 0.95+-0.06 and 0.74+-0.18, 0.94+-0.12 for atrophy lesions with AUC=0.897, all above observer variability. Simultaneous classification outperforms standalone classification precision and recall by 30+-62% and 27+-0% for atrophy segments and lesions. Conclusions: simultaneous column-based detection and quantification of retinal atrophy changes associated with AMD is accurate and outperforms standalone classification methods. Translational relevance: an automatic and efficient way to detect and quantify retinal atrophy changes associated with AMD.
</details>
<details>
<summary>摘要</summary>
目的： aged-related macular degeneration（AMD）相关Retinal atrophy的疾病进程需要精准量化 consecutive OCT 图像中的变化。这是通过发现、比较和定义潜在的变化而实现的。方法：我们提出了一个完全自动的终端到终点管道，用于同时检测和量化患者 consecutive OCT 图像中的时间相关的衰竭变化。该管道使用了一种新的同时多通道Column-based深度学习模型，该模型在已经注册的 prior 和 current OCT 图像中同时检测和分割Retinal atrophy 分段。结果：我们对 4,040 个 OCT 图像（5.2M 列）进行实验，其中有 40 组 OCT 图像，每组图像间隔 24.13 ± 14.0 个月。在这些图像中，我们发现了 Complete RPE 和 Outer Retinal Atrophy（cRORA），并且在 1,998 个 OCT 图像中检测到了 735 个衰竭 lesions，其中 0.45M 列。我们的方法在这些图像中得到了 mean 衰竭分段检测精度和 recall 的 0.90 ± 0.09、0.95 ± 0.06 和 0.74 ± 0.18、0.94 ± 0.12，AUC = 0.897，全部超过了观察者变化。同时的分类方法也超过了单独的分类精度和 recall。结论：同时检测和量化 Retinal atrophy 变化与 AMD 相关的疾病进程是准确的，并且超过了单独的分类方法。翻译意义：一种自动和高效的方法，可以帮助检测和量化 Retinal atrophy 变化与 AMD 相关的疾病进程。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-and-Computer-Vision-for-Glaucoma-Detection-A-Review"><a href="#Deep-Learning-and-Computer-Vision-for-Glaucoma-Detection-A-Review" class="headerlink" title="Deep Learning and Computer Vision for Glaucoma Detection: A Review"></a>Deep Learning and Computer Vision for Glaucoma Detection: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16528">http://arxiv.org/abs/2307.16528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mona Ashtari-Majlan, Mohammad Mahdi Dehshibi, David Masip<br>for:This paper aims to provide a comprehensive survey of recent studies on AI-based glaucoma diagnosis using fundus, optical coherence tomography, and visual field images.methods:The paper focuses on deep learning-based methods for glaucoma diagnosis and provides an updated taxonomy that organizes these methods into architectural paradigms. The authors also provide links to available source code to enhance the reproducibility of the methods.results:The authors conduct rigorous benchmarking on widely-used public datasets and reveal performance gaps in generalizability, uncertainty estimation, and multimodal integration. They also curate key datasets and highlight limitations such as scale, labeling inconsistencies, and bias.Here is the same information in Simplified Chinese text:for:这篇论文的目的是为了提供最近的AI基于fundus、optical coherence tomography和视场图像的глау科病诊断研究的综述。methods:这篇论文专注于基于深度学习的глау科诊断方法，并提供了一个更新的分类法，将这些方法分为 arquitectural paradigms。作者还提供了可重复性的源代码链接。results:作者通过在广泛使用的公共数据集上进行严格的benchmarking，揭示了总结、不确定性估计和多模态融合的性能差。他们还综述了关键的数据集，并指出了标注不一致、scale和偏见等限制。<details>
<summary>Abstract</summary>
Glaucoma is the leading cause of irreversible blindness worldwide and poses significant diagnostic challenges due to its reliance on subjective evaluation. However, recent advances in computer vision and deep learning have demonstrated the potential for automated assessment. In this paper, we survey recent studies on AI-based glaucoma diagnosis using fundus, optical coherence tomography, and visual field images, with a particular emphasis on deep learning-based methods. We provide an updated taxonomy that organizes methods into architectural paradigms and includes links to available source code to enhance the reproducibility of the methods. Through rigorous benchmarking on widely-used public datasets, we reveal performance gaps in generalizability, uncertainty estimation, and multimodal integration. Additionally, our survey curates key datasets while highlighting limitations such as scale, labeling inconsistencies, and bias. We outline open research challenges and detail promising directions for future studies. This survey is expected to be useful for both AI researchers seeking to translate advances into practice and ophthalmologists aiming to improve clinical workflows and diagnosis using the latest AI outcomes.
</details>
<details>
<summary>摘要</summary>
Glaucoma 是全球最主要的不可逆失明病种，但它的诊断却存在许多Subjective 的挑战。然而，最近的计算机视觉和深度学习技术的进步已经表明了自动诊断的潜在。在这篇论文中，我们对最近的人工智能基于fundus、optical coherence tomography和视场图像的 glaucoma 诊断进行了抽查。我们提供了一个更新的分类法，将方法分为建筑学派别，并提供了可用的源代码，以便提高方法的重现性。通过对广泛使用的公共数据集进行严格的benchmarking，我们揭示了总体化、不确定性估计和多模态融合的性能差距。此外，我们还提供了关键的数据集和限制，包括标签不一致、标签质量和偏见。我们还详细描述了未来研究的开放问题，并提出了可能的未来研究方向。这篇论文预计将对both AI 研究人员和eye 专家有所帮助，以实现最新的 AI 成果的翻译和临床应用。
</details></li>
</ul>
<hr>
<h2 id="No-Fair-Lunch-A-Causal-Perspective-on-Dataset-Bias-in-Machine-Learning-for-Medical-Imaging"><a href="#No-Fair-Lunch-A-Causal-Perspective-on-Dataset-Bias-in-Machine-Learning-for-Medical-Imaging" class="headerlink" title="No Fair Lunch: A Causal Perspective on Dataset Bias in Machine Learning for Medical Imaging"></a>No Fair Lunch: A Causal Perspective on Dataset Bias in Machine Learning for Medical Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16526">http://arxiv.org/abs/2307.16526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Charles Jones, Daniel C. Castro, Fabio De Sousa Ribeiro, Ozan Oktay, Melissa McCradden, Ben Glocker</li>
<li>for: 这篇论文目的是为了解决在临床决策中使用机器学习方法时，评估公平性问题。</li>
<li>methods: 该论文使用了 causal 视角来探讨算法偏见问题，并提出了三种类型的 causal 偏见机制，它们来自于数据集偏见、展示偏见和标注偏见。</li>
<li>results: 该论文的结果表明，当前的mitigation方法只能处理一部分和常见的情况，而且有很多情况下可能会导致不良后果。该论文还提出了一种实用的三步框架，用于考虑公平性问题在医学影像领域中。<details>
<summary>Abstract</summary>
As machine learning methods gain prominence within clinical decision-making, addressing fairness concerns becomes increasingly urgent. Despite considerable work dedicated to detecting and ameliorating algorithmic bias, today's methods are deficient with potentially harmful consequences. Our causal perspective sheds new light on algorithmic bias, highlighting how different sources of dataset bias may appear indistinguishable yet require substantially different mitigation strategies. We introduce three families of causal bias mechanisms stemming from disparities in prevalence, presentation, and annotation. Our causal analysis underscores how current mitigation methods tackle only a narrow and often unrealistic subset of scenarios. We provide a practical three-step framework for reasoning about fairness in medical imaging, supporting the development of safe and equitable AI prediction models.
</details>
<details>
<summary>摘要</summary>
随着机器学习技术在医疗决策中升级，解决公平问题的必要性日益增加。尽管有大量研究检测和改进算法偏见，但今天的方法仍然存在可能有害的后果。我们的 causal 视角把Algorithmic bias推广到新的角度，指出不同的数据集偏见可能会看起来相同，但需要不同的 mitigation 策略。我们引入三种家族的 causal 偏见机制，来自不同的发生率、展示和标注方面。我们的 causal 分析表明，当前的 mitigation 方法只能处理一小部分的场景，并且 часто是不realistic的。我们提供了一个实用的三步框架，以便在医疗图像领域理解公平，并支持开发安全和公平的 AI 预测模型。
</details></li>
</ul>
<hr>
<h2 id="Deception-Abilities-Emerged-in-Large-Language-Models"><a href="#Deception-Abilities-Emerged-in-Large-Language-Models" class="headerlink" title="Deception Abilities Emerged in Large Language Models"></a>Deception Abilities Emerged in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16513">http://arxiv.org/abs/2307.16513</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thilo Hagendorff</li>
<li>for: This study aims to investigate the ability of large language models (LLMs) to understand and utilize deception strategies, and to contribute to the nascent field of machine psychology.</li>
<li>methods: The study uses state-of-the-art LLMs, such as GPT-4, and conducts a series of experiments to test their ability to understand and induce false beliefs in other agents, and to amplify their performance in complex deception scenarios using chain-of-thought reasoning.</li>
<li>results: The study finds that state-of-the-art LLMs possess a conceptual understanding of deception strategies, are able to understand and induce false beliefs in other agents, and that eliciting Machiavellianism in LLMs can alter their propensity to deceive. These findings contribute to the nascent field of machine psychology and reveal hitherto unknown machine behavior in LLMs.<details>
<summary>Abstract</summary>
Large language models (LLMs) are currently at the forefront of intertwining artificial intelligence (AI) systems with human communication and everyday life. Thus, aligning them with human values is of great importance. However, given the steady increase in reasoning abilities, future LLMs are under suspicion of becoming able to deceive human operators and utilizing this ability to bypass monitoring efforts. As a prerequisite to this, LLMs need to possess a conceptual understanding of deception strategies. This study reveals that such strategies emerged in state-of-the-art LLMs, such as GPT-4, but were non-existent in earlier LLMs. We conduct a series of experiments showing that state-of-the-art LLMs are able to understand and induce false beliefs in other agents, that their performance in complex deception scenarios can be amplified utilizing chain-of-thought reasoning, and that eliciting Machiavellianism in LLMs can alter their propensity to deceive. In sum, revealing hitherto unknown machine behavior in LLMs, our study contributes to the nascent field of machine psychology.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Classifying-multilingual-party-manifestos-Domain-transfer-across-country-time-and-genre"><a href="#Classifying-multilingual-party-manifestos-Domain-transfer-across-country-time-and-genre" class="headerlink" title="Classifying multilingual party manifestos: Domain transfer across country, time, and genre"></a>Classifying multilingual party manifestos: Domain transfer across country, time, and genre</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16511">http://arxiv.org/abs/2307.16511</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/slds-lmu/manifesto-domaintransfer">https://github.com/slds-lmu/manifesto-domaintransfer</a></li>
<li>paper_authors: Matthias Aßenmacher, Nadja Sauter, Christian Heumann</li>
<li>for: This paper is written for empirical social science research, specifically to explore the potential of domain transfer across geographical locations, languages, time, and genre in a large-scale database of political manifestos.</li>
<li>methods: The paper uses fine-tuned transformer models and an external corpus of transcribed speeches from New Zealand politicians to test for the fine-tuned models’ robustness and transferability across different dimensions.</li>
<li>results: The paper shows that (Distil)BERT can be applied to future data with similar performance, and observes notable differences between the political manifestos of different countries of origin, even if these countries share a language or a cultural background.<details>
<summary>Abstract</summary>
Annotating costs of large corpora are still one of the main bottlenecks in empirical social science research. On the one hand, making use of the capabilities of domain transfer allows re-using annotated data sets and trained models. On the other hand, it is not clear how well domain transfer works and how reliable the results are for transfer across different dimensions. We explore the potential of domain transfer across geographical locations, languages, time, and genre in a large-scale database of political manifestos. First, we show the strong within-domain classification performance of fine-tuned transformer models. Second, we vary the genre of the test set across the aforementioned dimensions to test for the fine-tuned models' robustness and transferability. For switching genres, we use an external corpus of transcribed speeches from New Zealand politicians while for the other three dimensions, custom splits of the Manifesto database are used. While BERT achieves the best scores in the initial experiments across modalities, DistilBERT proves to be competitive at a lower computational expense and is thus used for further experiments across time and country. The results of the additional analysis show that (Distil)BERT can be applied to future data with similar performance. Moreover, we observe (partly) notable differences between the political manifestos of different countries of origin, even if these countries share a language or a cultural background.
</details>
<details>
<summary>摘要</summary>
大公司的批注成本仍是社会科学研究中的主要瓶颈。一方面，通过域传播可以重用标注数据集和训练模型。另一方面，域传播效果不清楚，结果可靠性在不同维度上也存在问题。我们在大规模的政治宣言数据库中explore域传播的潜在性。首先，我们表明在本域内的精度批注表现强。其次，我们在不同维度上随机选择测试集的类别，以测试精度批注模型的Robustness和可传递性。为了在类别间切换，我们使用了新西兰政治人物演讲的外部 corpus，而其他三个维度使用了自定义的漫游集。而BERT在初始实验中 across modalities 取得了最佳得分，但DistilBERT在计算成本更低的情况下能够与之相比，因此在以后的实验中使用了DistilBERT。ADDITIONAL ANALYSIS 表明（Distil）BERT可以应用于未来数据，并且 observe（部分）地有所不同的政治宣言在不同的国家起源，即使这些国家共享语言或文化背景。
</details></li>
</ul>
<hr>
<h2 id="Explainable-Equivariant-Neural-Networks-for-Particle-Physics-PELICAN"><a href="#Explainable-Equivariant-Neural-Networks-for-Particle-Physics-PELICAN" class="headerlink" title="Explainable Equivariant Neural Networks for Particle Physics: PELICAN"></a>Explainable Equivariant Neural Networks for Particle Physics: PELICAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16506">http://arxiv.org/abs/2307.16506</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abogatskiy/pelican">https://github.com/abogatskiy/pelican</a></li>
<li>paper_authors: Alexander Bogatskiy, Timothy Hoffman, David W. Miller, Jan T. Offermann, Xiaoyang Liu<br>for:* 这个论文主要研究了PELICAN机器学习算法架构在拥有 boosted top quark 的 Lorentz-boosted top quarks 分类和重构中的应用，包括在填充有 dense 粒子物理环境中具有特殊任务的 $W-$boson 的识别和测量。methods:* PELICAN 是一种新的 permutation equivariant 和 Lorentz invariant&#x2F;covariant 聚合网络，用于解决 particle physics 问题中常见的限制。* PELICAN 使用基于 симметRY 组合的架构，与许多不specialized 架构不同，它们忽略物理原理并需要很多参数。results:* PELICAN 在标准任务 Lorentz-boosted top quark 标签中比现有竞争对手表现出色，具有较低的模型复杂度和高效的样本效率。* PELICAN 也在 less common 和 more complex 的 four-momentum regression 任务中比手工算法表现出色。<details>
<summary>Abstract</summary>
We present a comprehensive study of the PELICAN machine learning algorithm architecture in the context of both tagging (classification) and reconstructing (regression) Lorentz-boosted top quarks, including the difficult task of specifically identifying and measuring the $W$-boson inside the dense environment of the boosted hadronic final state. PELICAN is a novel permutation equivariant and Lorentz invariant or covariant aggregator network designed to overcome common limitations found in architectures applied to particle physics problems. Compared to many approaches that use non-specialized architectures that neglect underlying physics principles and require very large numbers of parameters, PELICAN employs a fundamentally symmetry group-based architecture that demonstrates benefits in terms of reduced complexity, increased interpretability, and raw performance. When tested on the standard task of Lorentz-boosted top quark tagging, PELICAN outperforms existing competitors with much lower model complexity and high sample efficiency. On the less common and more complex task of four-momentum regression, PELICAN also outperforms hand-crafted algorithms. We discuss the implications of symmetry-restricted architectures for the wider field of machine learning for physics.
</details>
<details>
<summary>摘要</summary>
我们提出了一项全面的PELICAN机器学习算法架构研究，包括标记（分类）和重construct（回归）洛伦兹升级顶点Quark，包括在填充有 dense粒子状态下困难的$W$ boson内部特定和测量任务。 PELICAN是一种新的协变和 Lorentz  invariable或covariant 聚合网络，旨在超越 particle physics 问题中常见的限制。 与许多使用不特殊化 Architecture 的方法不同，PELICAN employs a fundamentally symmetry group-based architecture that demonstrates benefits in terms of reduced complexity, increased interpretability, and raw performance. 对于标准任务 Lorentz-boosted top quark 标记，PELICAN 超越现有竞争对手，只需要许多参数和高效采样率。 在 less common 和更复杂的四矢量回归任务上，PELICAN 也超越手动制作的算法。 我们讨论了对物理机器学习领域的 Symmetry-restricted 架构的影响。
</details></li>
</ul>
<hr>
<h2 id="Value-Informed-Skill-Chaining-for-Policy-Learning-of-Long-Horizon-Tasks-with-Surgical-Robot"><a href="#Value-Informed-Skill-Chaining-for-Policy-Learning-of-Long-Horizon-Tasks-with-Surgical-Robot" class="headerlink" title="Value-Informed Skill Chaining for Policy Learning of Long-Horizon Tasks with Surgical Robot"></a>Value-Informed Skill Chaining for Policy Learning of Long-Horizon Tasks with Surgical Robot</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16503">http://arxiv.org/abs/2307.16503</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/med-air/viskill">https://github.com/med-air/viskill</a></li>
<li>paper_authors: Tao Huang, Kai Chen, Wang Wei, Jianan Li, Yonghao Long, Qi Dou</li>
<li>for: 解决长期术urgical robot任务，因为策略探索挑战而困难。</li>
<li>methods: 使用技能链接（skill chaining），将长期任务分解成多个子任务，以减轻策略探索负担。</li>
<li>results: 在三个复杂的术urgical robot任务上达到高成功率和执行效率。<details>
<summary>Abstract</summary>
Reinforcement learning is still struggling with solving long-horizon surgical robot tasks which involve multiple steps over an extended duration of time due to the policy exploration challenge. Recent methods try to tackle this problem by skill chaining, in which the long-horizon task is decomposed into multiple subtasks for easing the exploration burden and subtask policies are temporally connected to complete the whole long-horizon task. However, smoothly connecting all subtask policies is difficult for surgical robot scenarios. Not all states are equally suitable for connecting two adjacent subtasks. An undesired terminate state of the previous subtask would make the current subtask policy unstable and result in a failed execution. In this work, we introduce value-informed skill chaining (ViSkill), a novel reinforcement learning framework for long-horizon surgical robot tasks. The core idea is to distinguish which terminal state is suitable for starting all the following subtask policies. To achieve this target, we introduce a state value function that estimates the expected success probability of the entire task given a state. Based on this value function, a chaining policy is learned to instruct subtask policies to terminate at the state with the highest value so that all subsequent policies are more likely to be connected for accomplishing the task. We demonstrate the effectiveness of our method on three complex surgical robot tasks from SurRoL, a comprehensive surgical simulation platform, achieving high task success rates and execution efficiency. Code is available at $\href{https://github.com/med-air/ViSkill}{\text{https://github.com/med-air/ViSkill}}$.
</details>
<details>
<summary>摘要</summary>
“强化学习仍然面临长期手术机器人任务中的多步多时间挑战，主要是因为策略探索困难。现有方法通过将长期任务拆分成多个互相连接的子任务来缓解探索压力。然而，在手术机器人场景下，平滑地连接所有子任务策略是困难的。不同状态之间不都是适合连接两个相邻的子任务的。如果上一个子任务的终端状态不适合连接下一个子任务，那么当前任务策略就会变得不稳定，导致执行失败。在这种情况下，我们引入了值知识推荐技术（ViSkill），一种新的强化学习框架，用于解决长期手术机器人任务。ViSkill的核心思想是通过估计任务执行成功概率来识别适合起始所有后续子任务的终端状态。为此，我们引入了一个状态价值函数，用于估计在给定状态下任务的预计成功概率。基于这个价值函数，我们学习了一个链接策略，用于指导子任务策略在状态 WITH 最高价值上终止，以便所有后续策略能够更加连续地执行任务。我们在三个复杂的手术机器人任务上进行了实验，并取得了高任务成功率和执行效率。代码可以在 $\href{https://github.com/med-air/ViSkill}{\text{https://github.com/med-air/ViSkill}}$ 上找到。”
</details></li>
</ul>
<hr>
<h2 id="Learning-Generalizable-Tool-Use-with-Non-rigid-Grasp-pose-Registration"><a href="#Learning-Generalizable-Tool-Use-with-Non-rigid-Grasp-pose-Registration" class="headerlink" title="Learning Generalizable Tool Use with Non-rigid Grasp-pose Registration"></a>Learning Generalizable Tool Use with Non-rigid Grasp-pose Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16499">http://arxiv.org/abs/2307.16499</a></li>
<li>repo_url: None</li>
<li>paper_authors: Malte Mosbach, Sven Behnke</li>
<li>for: 本研究旨在帮助机器人学习工具使用行为。</li>
<li>methods: 我们提出了一种新的方法，可以通过单个示例学习新类型工具的操作。我们使用了多指手中的抓取配置的普适化，以导航策略搜索，并通过可 favourable 的初始化和适应奖励信号来帮助策略学习。</li>
<li>results: 我们的方法可以解决复杂的工具使用任务，并在测试时对未看过的工具进行扩展。视频和图像 demonstration 可以在<a target="_blank" rel="noopener" href="https://maltemosbach.github.io/generalizable_tool_use">https://maltemosbach.github.io/generalizable_tool_use</a> 上查看。<details>
<summary>Abstract</summary>
Tool use, a hallmark feature of human intelligence, remains a challenging problem in robotics due the complex contacts and high-dimensional action space. In this work, we present a novel method to enable reinforcement learning of tool use behaviors. Our approach provides a scalable way to learn the operation of tools in a new category using only a single demonstration. To this end, we propose a new method for generalizing grasping configurations of multi-fingered robotic hands to novel objects. This is used to guide the policy search via favorable initializations and a shaped reward signal. The learned policies solve complex tool use tasks and generalize to unseen tools at test time. Visualizations and videos of the trained policies are available at https://maltemosbach.github.io/generalizable_tool_use.
</details>
<details>
<summary>摘要</summary>
人类智能的一个标志性特征是工具使用，但在机器人学中仍然是一个具有挑战性的问题，主要是因为复杂的接触和高维动作空间。在这种情况下，我们提出了一种新的方法，可以在新类别中学习工具使用行为。我们的方法可以在单个示例的基础上扩展到新的工具，并且可以通过有利的初始化和形态奖励信号来导引政策搜索。我们的学习策略可以解决复杂的工具使用任务，并且可以在测试时对未看过的工具进行扩展。可以通过视频和图像在https://maltemosbach.github.io/generalizable_tool_use中查看训练政策的视觉和视频。
</details></li>
</ul>
<hr>
<h2 id="Don’t-be-so-negative-Score-based-Generative-Modeling-with-Oracle-assisted-Guidance"><a href="#Don’t-be-so-negative-Score-based-Generative-Modeling-with-Oracle-assisted-Guidance" class="headerlink" title="Don’t be so negative! Score-based Generative Modeling with Oracle-assisted Guidance"></a>Don’t be so negative! Score-based Generative Modeling with Oracle-assisted Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16463">http://arxiv.org/abs/2307.16463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saeid Naderiparizi, Xiaoxuan Liang, Berend Zwartsenberg, Frank Wood</li>
<li>for: 本研究旨在提出一种基于 oracle 的干扰难度模型（DDPM）方法，以优化模型参数估计，并通过 oracle 提供的干扰信息，提高模型的泛化性。</li>
<li>methods: 本研究使用了 Generative Adversarial Networks (GANs) 和激发器导向的干扰模型，将扰干扰信息 integrate 到模型中，以优化模型的生成过程，使其更加准确地预测实际数据的分布。</li>
<li>results: 研究结果表明，Gen-neG 方法可以在自动驾驶 simulate 环境中减少碰撞风险，以及在人体动作生成 task 中提供安全保护。<details>
<summary>Abstract</summary>
The maximum likelihood principle advocates parameter estimation via optimization of the data likelihood function. Models estimated in this way can exhibit a variety of generalization characteristics dictated by, e.g. architecture, parameterization, and optimization bias. This work addresses model learning in a setting where there further exists side-information in the form of an oracle that can label samples as being outside the support of the true data generating distribution. Specifically we develop a new denoising diffusion probabilistic modeling (DDPM) methodology, Gen-neG, that leverages this additional side-information. Our approach builds on generative adversarial networks (GANs) and discriminator guidance in diffusion models to guide the generation process towards the positive support region indicated by the oracle. We empirically establish the utility of Gen-neG in applications including collision avoidance in self-driving simulators and safety-guarded human motion generation.
</details>
<details>
<summary>摘要</summary>
最大可能性原则建议通过数据可能函数优化参数估计。这种方法可以导致模型在不同的架构、参数化和优化偏见的情况下显示多种泛化特性。本工作在存在 oracle 类型的侧信息时进行模型学习。特别是，我们开发了一种新的涂抹推 diffusion 模型方法（DDPM），叫做 Gen-neG，它利用这些侧信息来导向生成过程。我们的方法基于生成对抗网络（GANs）和推 diffusion 模型中的判据导向来指导生成过程，以便将生成结果集中的样本归入正确的支持区域。我们通过实验证明 Gen-neG 在自动驾驶 simulate 中的碰撞避免和人体动作生成中的安全保护中的应用有用性。
</details></li>
</ul>
<hr>
<h2 id="L3DMC-Lifelong-Learning-using-Distillation-via-Mixed-Curvature-Space"><a href="#L3DMC-Lifelong-Learning-using-Distillation-via-Mixed-Curvature-Space" class="headerlink" title="L3DMC: Lifelong Learning using Distillation via Mixed-Curvature Space"></a>L3DMC: Lifelong Learning using Distillation via Mixed-Curvature Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16459">http://arxiv.org/abs/2307.16459</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/csiro-robotics/l3dmc">https://github.com/csiro-robotics/l3dmc</a></li>
<li>paper_authors: Kaushik Roy, Peyman Moghadam, Mehrtash Harandi</li>
<li>for: 这个研究是为了解决生命长学习（L3）模型在进行一系列任务训练后，表现下降的问题。</li>
<li>methods: 这篇研究提出了一种名为L3DMC的distillation策略，它可以在混合曲线空间中维护和实现复杂的几何结构，以保留之前学习的知识。</li>
<li>results: 实验结果显示，L3DMC可以在医疗影像分类 задачі中更好地适应新知识，而不会忘记之前学习的知识。<details>
<summary>Abstract</summary>
The performance of a lifelong learning (L3) model degrades when it is trained on a series of tasks, as the geometrical formation of the embedding space changes while learning novel concepts sequentially. The majority of existing L3 approaches operate on a fixed-curvature (e.g., zero-curvature Euclidean) space that is not necessarily suitable for modeling the complex geometric structure of data. Furthermore, the distillation strategies apply constraints directly on low-dimensional embeddings, discouraging the L3 model from learning new concepts by making the model highly stable. To address the problem, we propose a distillation strategy named L3DMC that operates on mixed-curvature spaces to preserve the already-learned knowledge by modeling and maintaining complex geometrical structures. We propose to embed the projected low dimensional embedding of fixed-curvature spaces (Euclidean and hyperbolic) to higher-dimensional Reproducing Kernel Hilbert Space (RKHS) using a positive-definite kernel function to attain rich representation. Afterward, we optimize the L3 model by minimizing the discrepancies between the new sample representation and the subspace constructed using the old representation in RKHS. L3DMC is capable of adapting new knowledge better without forgetting old knowledge as it combines the representation power of multiple fixed-curvature spaces and is performed on higher-dimensional RKHS. Thorough experiments on three benchmarks demonstrate the effectiveness of our proposed distillation strategy for medical image classification in L3 settings. Our code implementation is publicly available at https://github.com/csiro-robotics/L3DMC.
</details>
<details>
<summary>摘要</summary>
“一个生命时间学习（L3）模型的性能下降是因为在学习新概念时，embedding空间的几何结构发生变化。现有的大多数L3方法 operate在固定几何（例如，零几何Euclidean）空间中，这并不一定适合数据模型复杂的几何结构。另外，浸泡策略直接在低维度 embedding 上应用约束，使L3模型学习新概念困难。为解决这个问题，我们提议一种浸泡策略名为L3DMC，它在混合几何空间中保留已经学习的知识，并在更高维度的Reproducing Kernel Hilbert Space（RKHS）中使用正定的kernel函数来获得丰富的表示。然后，我们使用RKHS中建立的子空间和新样本表示之间的差异来优化L3模型。L3DMC可以更好地适应新知识，而无需忘记过去的知识，因为它将多个固定几何空间的表示力相结合在一起，并在更高维度的RKHS中进行优化。我们在三个标准测试集上进行了详细的实验，并证明了L3DMC在医学影像分类中的效果。我们的代码实现可以在https://github.com/csiro-robotics/L3DMC上获取。”
</details></li>
</ul>
<hr>
<h2 id="An-Effective-Data-Creation-Pipeline-to-Generate-High-quality-Financial-Instruction-Data-for-Large-Language-Model"><a href="#An-Effective-Data-Creation-Pipeline-to-Generate-High-quality-Financial-Instruction-Data-for-Large-Language-Model" class="headerlink" title="An Effective Data Creation Pipeline to Generate High-quality Financial Instruction Data for Large Language Model"></a>An Effective Data Creation Pipeline to Generate High-quality Financial Instruction Data for Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01415">http://arxiv.org/abs/2308.01415</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziao Wang, Jianning Wang, Junda Wu, Xiaofeng Zhang</li>
<li>for: 本研究的目的是生成高质量的金融数据集，以供大语言模型的 fine-tuning，以进行金融相关任务。</li>
<li>methods: 该研究提出了一个仔细设计的数据创建管道，包括对ChatGPT的对话和人工金融专家的反馈，以便对数据进行精细调整。</li>
<li>results: 该管道生成了103k多turn对话的强化 instrucion tuning 数据集，并通过对外部GPT-4进行评判，实现了模型在生成准确、相关、金融风格响应方面的显著提高。<details>
<summary>Abstract</summary>
At the beginning era of large language model, it is quite critical to generate a high-quality financial dataset to fine-tune a large language model for financial related tasks. Thus, this paper presents a carefully designed data creation pipeline for this purpose. Particularly, we initiate a dialogue between an AI investor and financial expert using ChatGPT and incorporate the feedback of human financial experts, leading to the refinement of the dataset. This pipeline yielded a robust instruction tuning dataset comprised of 103k multi-turn chats. Extensive experiments have been conducted on this dataset to evaluate the model's performance by adopting an external GPT-4 as the judge. The promising experimental results verify that our approach led to significant advancements in generating accurate, relevant, and financial-style responses from AI models, and thus providing a powerful tool for applications within the financial sector.
</details>
<details>
<summary>摘要</summary>
在大语言模型开始时代，生成高质量金融数据集是非常重要的，以 Fine-tune大语言模型进行金融相关任务。因此，这篇论文提出了一个仔细设计的数据创建管道，以此为目的。特别是，我们通过与人工智能投资者和金融专家的对话使用ChatGPT，并 incorporate了人类金融专家的反馈，从而改进了数据集。这个管道生成了103k多turn对话的强健 instrucion tuning 数据集。我们对这个数据集进行了广泛的实验，采用外部GPT-4作为评判。实验结果表明，我们的方法可以导致AI模型生成高精度、相关性和金融风格的回答，从而为金融领域应用提供了强大的工具。
</details></li>
</ul>
<hr>
<h2 id="A-continuous-Structural-Intervention-Distance-to-compare-Causal-Graphs"><a href="#A-continuous-Structural-Intervention-Distance-to-compare-Causal-Graphs" class="headerlink" title="A continuous Structural Intervention Distance to compare Causal Graphs"></a>A continuous Structural Intervention Distance to compare Causal Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16452">http://arxiv.org/abs/2307.16452</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mihir Dhanakshirur, Felix Laumann, Junhyung Park, Mauricio Barahona</li>
<li>for: 本研究旨在提供一种新的连续度量来评估真实和学习的 causal 图之间的差异，以便在 causal 推理中更好地评估 intervención 的影响。</li>
<li>methods: 本研究使用 embedding  intervención 分布的方法来计算 causal 图的差异。这些方法基于在每个节点间的Conditional Mean Embedding 的嵌入，并通过最大conditional 差异来计算 intervención 分布的差异。</li>
<li>results: 本研究通过理论分析和数据实验验证了这种新的连续度量的有效性。数据实验表明，这种度量可以准确地评估 true 和 learnt  causal 图之间的差异。<details>
<summary>Abstract</summary>
Understanding and adequately assessing the difference between a true and a learnt causal graphs is crucial for causal inference under interventions. As an extension to the graph-based structural Hamming distance and structural intervention distance, we propose a novel continuous-measured metric that considers the underlying data in addition to the graph structure for its calculation of the difference between a true and a learnt causal graph. The distance is based on embedding intervention distributions over each pair of nodes as conditional mean embeddings into reproducing kernel Hilbert spaces and estimating their difference by the maximum (conditional) mean discrepancy. We show theoretical results which we validate with numerical experiments on synthetic data.
</details>
<details>
<summary>摘要</summary>
理解和准确评估真实和学习的 causal 图之间的差异是 causal 推理下 intervención 的关键。作为结构基于图的抽象 Hamming 距离和结构 intervención 距离的扩展，我们提出一种新的连续量化度量，它考虑了图结构以外的数据，用于计算真实和学习的 causal 图之间的差异。该距离基于每对节点的插入 intervención 分布，将它们编码为Conditional Mean Embeddings（CME），并估计其差异为最大（条件）均值差。我们提供了理论结论，并通过Synthetic 数据的数值实验 validate 这些结论。
</details></li>
</ul>
<hr>
<h2 id="Towards-Head-Computed-Tomography-Image-Reconstruction-Standardization-with-Deep-Learning-Assisted-Automatic-Detection"><a href="#Towards-Head-Computed-Tomography-Image-Reconstruction-Standardization-with-Deep-Learning-Assisted-Automatic-Detection" class="headerlink" title="Towards Head Computed Tomography Image Reconstruction Standardization with Deep Learning Assisted Automatic Detection"></a>Towards Head Computed Tomography Image Reconstruction Standardization with Deep Learning Assisted Automatic Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16440">http://arxiv.org/abs/2307.16440</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bowen Zheng, Chenxi Huang, Yuemei Luo</li>
<li>for: 帮助更正准确地诊断头部Computed Tomography（CT）图像，提高诊断效率和重复性。</li>
<li>methods: 使用深度学习基于对象检测算法，自动识别和评估 orbitomeatal line 的特征点，以 Reformatting 图像前进行三维重建。</li>
<li>results: 比较了十种对象检测算法的精度、效率和Robustness，选择轻量级 YOLOv8，其中 mAP 为 92.91%，对偏袋率具有优异的Robustness。 qualitative 评估表明方法在临床实践中具有丰富的价值和有效性。<details>
<summary>Abstract</summary>
Three-dimensional (3D) reconstruction of head Computed Tomography (CT) images elucidates the intricate spatial relationships of tissue structures, thereby assisting in accurate diagnosis. Nonetheless, securing an optimal head CT scan without deviation is challenging in clinical settings, owing to poor positioning by technicians, patient's physical constraints, or CT scanner tilt angle restrictions. Manual formatting and reconstruction not only introduce subjectivity but also strain time and labor resources. To address these issues, we propose an efficient automatic head CT images 3D reconstruction method, improving accuracy and repeatability, as well as diminishing manual intervention. Our approach employs a deep learning-based object detection algorithm, identifying and evaluating orbitomeatal line landmarks to automatically reformat the images prior to reconstruction. Given the dearth of existing evaluations of object detection algorithms in the context of head CT images, we compared ten methods from both theoretical and experimental perspectives. By exploring their precision, efficiency, and robustness, we singled out the lightweight YOLOv8 as the aptest algorithm for our task, with an mAP of 92.91% and impressive robustness against class imbalance. Our qualitative evaluation of standardized reconstruction results demonstrates the clinical practicability and validity of our method.
</details>
<details>
<summary>摘要</summary>
三维重建头部计算机断层影像（CT）图可以帮助确定细致的组织结构，从而提高诊断的准确性。然而，在临床 Setting中获得优质的头部CT扫描数据是困难的，因为技术人员的位置不精确，病人的物理限制或CT扫描仪的倾斜角度限制。手动格式化和重建不仅引入主观性，还浪费时间和劳动资源。为解决这些问题，我们提出了一种高效的自动头部CT图三维重建方法，提高准确性和重复性，同时减少手动干预。我们的方法使用深度学习基于对象检测算法，通过 Identifying和评估 orbitomeatal 线标记来自动重新格式化图像，以前于重建。由于现有的头部CT图对象检测算法的评估缺乏，我们从理论和实验两个角度进行了对十种方法的比较。通过评估精度、效率和Robustness，我们选择了轻量级的 YOLOv8 算法，其 mAP 为 92.91%，并在类偏度下表现出强大的Robustness。我们的质量评估标准化重建结果表明了我们的方法的临床实用性和有效性。
</details></li>
</ul>
<hr>
<h2 id="VITS2-Improving-Quality-and-Efficiency-of-Single-Stage-Text-to-Speech-with-Adversarial-Learning-and-Architecture-Design"><a href="#VITS2-Improving-Quality-and-Efficiency-of-Single-Stage-Text-to-Speech-with-Adversarial-Learning-and-Architecture-Design" class="headerlink" title="VITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design"></a>VITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16430">http://arxiv.org/abs/2307.16430</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/daniilrobnikov/vits2">https://github.com/daniilrobnikov/vits2</a></li>
<li>paper_authors: Jungil Kong, Jihoon Park, Beomjeong Kim, Jeongmin Kim, Dohee Kong, Sangjin Kim</li>
<li>for: 提高单阶段文本识别模型的自然性、计算效率和多个话语者模型的同步性。</li>
<li>methods: 提出了一种基于VITS2的单阶段文本识别模型，通过改进多个方面来提高自然性、同步性和计算效率。</li>
<li>results: 实验结果表明，提出的方法能够有效地提高自然性、同步性和计算效率，同时减少了之前的干扰因素。<details>
<summary>Abstract</summary>
Single-stage text-to-speech models have been actively studied recently, and their results have outperformed two-stage pipeline systems. Although the previous single-stage model has made great progress, there is room for improvement in terms of its intermittent unnaturalness, computational efficiency, and strong dependence on phoneme conversion. In this work, we introduce VITS2, a single-stage text-to-speech model that efficiently synthesizes a more natural speech by improving several aspects of the previous work. We propose improved structures and training mechanisms and present that the proposed methods are effective in improving naturalness, similarity of speech characteristics in a multi-speaker model, and efficiency of training and inference. Furthermore, we demonstrate that the strong dependence on phoneme conversion in previous works can be significantly reduced with our method, which allows a fully end-to-end single-stage approach.
</details>
<details>
<summary>摘要</summary>
单阶段文本至语模型在最近得到了Active研究，其结果比两阶段管道系统更出色。 although the previous single-stage model has made great progress, there is still room for improvement in terms of its intermittent unnaturalness, computational efficiency, and strong dependence on phoneme conversion. In this work, we introduce VITS2, a single-stage text-to-speech model that efficiently synthesizes a more natural speech by improving several aspects of the previous work. We propose improved structures and training mechanisms and show that the proposed methods are effective in improving naturalness, similarity of speech characteristics in a multi-speaker model, and efficiency of training and inference. Furthermore, we demonstrate that the strong dependence on phoneme conversion in previous works can be significantly reduced with our method, which allows for a fully end-to-end single-stage approach.Here's the translation in Traditional Chinese:单阶段文本至语模型在最近得到了Active研究，其结果比两阶段管道系统更出色。 although the previous single-stage model has made great progress, there is still room for improvement in terms of its intermittent unnaturalness, computational efficiency, and strong dependence on phoneme conversion. In this work, we introduce VITS2, a single-stage text-to-speech model that efficiently synthesizes a more natural speech by improving several aspects of the previous work. We propose improved structures and training mechanisms and show that the proposed methods are effective in improving naturalness, similarity of speech characteristics in a multi-speaker model, and efficiency of training and inference. Furthermore, we demonstrate that the strong dependence on phoneme conversion in previous works can be significantly reduced with our method, which allows for a fully end-to-end single-stage approach.
</details></li>
</ul>
<hr>
<h2 id="Causal-Inference-for-Banking-Finance-and-Insurance-A-Survey"><a href="#Causal-Inference-for-Banking-Finance-and-Insurance-A-Survey" class="headerlink" title="Causal Inference for Banking Finance and Insurance A Survey"></a>Causal Inference for Banking Finance and Insurance A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16427">http://arxiv.org/abs/2307.16427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Satyam Kumar, Yelleti Vivek, Vadlamani Ravi, Indranil Bose</li>
<li>for: 这篇论文旨在探讨在银行、金融和保险领域中应用 causal inference 的可能性和潜力。</li>
<li>methods: 本论文通过对 37 篇1992-2023年发表的论文进行概括，探讨了不同领域中 causal inference 的应用，包括银行、财务、金融经济和行为金融等领域。</li>
<li>results: 论文发现，在银行和保险领域中，causal inference 的应用仍然处于初期阶段，因此有更多的研究可能性，以推动其成为可靠的方法。<details>
<summary>Abstract</summary>
Causal Inference plays an significant role in explaining the decisions taken by statistical models and artificial intelligence models. Of late, this field started attracting the attention of researchers and practitioners alike. This paper presents a comprehensive survey of 37 papers published during 1992-2023 and concerning the application of causal inference to banking, finance, and insurance. The papers are categorized according to the following families of domains: (i) Banking, (ii) Finance and its subdomains such as corporate finance, governance finance including financial risk and financial policy, financial economics, and Behavioral finance, and (iii) Insurance. Further, the paper covers the primary ingredients of causal inference namely, statistical methods such as Bayesian Causal Network, Granger Causality and jargon used thereof such as counterfactuals. The review also recommends some important directions for future research. In conclusion, we observed that the application of causal inference in the banking and insurance sectors is still in its infancy, and thus more research is possible to turn it into a viable method.
</details>
<details>
<summary>摘要</summary>
causal inference 在 statistical models 和 artificial intelligence models 中扮演着重要的角色，最近这个领域吸引了研究者和实践者的关注。这篇论文是一个总结37篇1992-2023年发表的论文，涉及银行、金融和保险领域中 causal inference 的应用。这些论文分为以下三个家族域：（i）银行，（ii）金融和其子领域，如企业财务、治理财务、金融风险和金融政策，金融经济和行为金融，（iii）保险。此外，论文还涵盖了 causal inference 的主要成分，如统计方法，如 bayesian causal network，granger causality，以及相关的术语，如 counterfactuals。评论还提出了未来研究的一些重要方向。结论是，银行和保险领域中的 causal inference 应用还处于初期阶段，因此更多的研究可能会使其成为可靠的方法。
</details></li>
</ul>
<hr>
<h2 id="MetaDiff-Meta-Learning-with-Conditional-Diffusion-for-Few-Shot-Learning"><a href="#MetaDiff-Meta-Learning-with-Conditional-Diffusion-for-Few-Shot-Learning" class="headerlink" title="MetaDiff: Meta-Learning with Conditional Diffusion for Few-Shot Learning"></a>MetaDiff: Meta-Learning with Conditional Diffusion for Few-Shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16424">http://arxiv.org/abs/2307.16424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Baoquan Zhang, Demin Yu</li>
<li>for: 提高深度学习模型的几何学习能力，即从只有几个示例中快速学习新任务。</li>
<li>methods: 使用Gradient-based meta-learning方法，其中外层 proces 学习一个共享梯度下降算法（即其超参数），而内层 proces 利用它来优化任务特定模型，使用只有几个标注数据。</li>
<li>results: 实验结果显示，我们的MetaDiff比现有的梯度基于meta-学习家族更高效，在几何学习任务中表现出优秀的性能。<details>
<summary>Abstract</summary>
Equipping a deep model the abaility of few-shot learning, i.e., learning quickly from only few examples, is a core challenge for artificial intelligence. Gradient-based meta-learning approaches effectively address the challenge by learning how to learn novel tasks. Its key idea is learning a deep model in a bi-level optimization manner, where the outer-loop process learns a shared gradient descent algorithm (i.e., its hyperparameters), while the inner-loop process leverage it to optimize a task-specific model by using only few labeled data. Although these existing methods have shown superior performance, the outer-loop process requires calculating second-order derivatives along the inner optimization path, which imposes considerable memory burdens and the risk of vanishing gradients. Drawing inspiration from recent progress of diffusion models, we find that the inner-loop gradient descent process can be actually viewed as a reverse process (i.e., denoising) of diffusion where the target of denoising is model weights but the origin data. Based on this fact, in this paper, we propose to model the gradient descent optimizer as a diffusion model and then present a novel task-conditional diffusion-based meta-learning, called MetaDiff, that effectively models the optimization process of model weights from Gaussion noises to target weights in a denoising manner. Thanks to the training efficiency of diffusion models, our MetaDiff do not need to differentiate through the inner-loop path such that the memory burdens and the risk of vanishing gradients can be effectvely alleviated. Experiment results show that our MetaDiff outperforms the state-of-the-art gradient-based meta-learning family in few-shot learning tasks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将深度模型培养为几何学习能力，即很快从只有几个示例学习，是人工智能的核心挑战。基于梯度的meta学习方法有效地解决了这个挑战，其关键思想是在bi-level优化的方式下培养深度模型， outer-loop过程学习共享梯度下降算法（即它的超参数），而inner-loop过程利用它来优化任务特定的模型，只使用几个标注数据。虽然现有的方法已经显示出了出色的性能，但outer-loop过程需要计算内部优化路径上的第二个DERIVATIVE，这会带来很大的内存压力和梯度消失风险。 drawing inspiration from recent progress of diffusion models, we find that the inner-loop gradient descent process can be viewed as a reverse process (i.e., denoising) of diffusion, where the target of denoising is model weights but the origin data. Based on this fact, in this paper, we propose to model the gradient descent optimizer as a diffusion model and then present a novel task-conditional diffusion-based meta-learning, called MetaDiff, that effectively models the optimization process of model weights from Gaussian noise to target weights in a denoising manner. Thanks to the training efficiency of diffusion models, our MetaDiff does not need to differentiate through the inner-loop path, thus alleviating the memory burdens and the risk of vanishing gradients. Experiment results show that our MetaDiff outperforms the state-of-the-art gradient-based meta-learning family in few-shot learning tasks.Note: The translation is done using Google Translate and may not be perfect. Please let me know if you need any further assistance.
</details></li>
</ul>
<hr>
<h2 id="Guaranteed-Optimal-Generative-Modeling-with-Maximum-Deviation-from-the-Empirical-Distribution"><a href="#Guaranteed-Optimal-Generative-Modeling-with-Maximum-Deviation-from-the-Empirical-Distribution" class="headerlink" title="Guaranteed Optimal Generative Modeling with Maximum Deviation from the Empirical Distribution"></a>Guaranteed Optimal Generative Modeling with Maximum Deviation from the Empirical Distribution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16422">http://arxiv.org/abs/2307.16422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elen Vardanyan, Arshak Minasyan, Sona Hunanyan, Tigran Galstyan, Arnak Dalalyan</li>
<li>for: 本研究旨在提供一种训练生成模型的方法，以实现在科学和工业领域中的各种应用。</li>
<li>methods: 本文使用了一种名为“生成模型”的机器学习方法，其主要目标是在训练数据的基础之上，生成新的示例，以模拟未知的分布。</li>
<li>results: 本文提供了训练生成模型的 teorema insights，包括两个性质：首先，在训练数据的基础之上，训练生成模型的错误应该最小化，而且随着样本大小的增加，这个错误应该收敛到零。其次，训练生成模型应该距离任何在训练数据中复制示例的分布尽可能远。这两个性质都可以通过Finite sample risk bounds来衡量，这些 risk bounds 取决于样本大小、维度空间和秘密空间的参数。<details>
<summary>Abstract</summary>
Generative modeling is a widely-used machine learning method with various applications in scientific and industrial fields. Its primary objective is to simulate new examples drawn from an unknown distribution given training data while ensuring diversity and avoiding replication of examples from the training data.   This paper presents theoretical insights into training a generative model with two properties: (i) the error of replacing the true data-generating distribution with the trained data-generating distribution should optimally converge to zero as the sample size approaches infinity, and (ii) the trained data-generating distribution should be far enough from any distribution replicating examples in the training data.   We provide non-asymptotic results in the form of finite sample risk bounds that quantify these properties and depend on relevant parameters such as sample size, the dimension of the ambient space, and the dimension of the latent space. Our results are applicable to general integral probability metrics used to quantify errors in probability distribution spaces, with the Wasserstein-$1$ distance being the central example. We also include numerical examples to illustrate our theoretical findings.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>生成模型是机器学习中广泛使用的方法，它在科学和工业领域有多种应用。它的主要目标是模拟新的例子，这些例子来自未知分布，而且保证这些例子与训练数据集中的例子不同，以避免重复。这篇论文提供了生成模型的理论启示，它们是：（i）在训练数据集中取代真实数据生成分布时的错误应该最小化，并且在样本数趋于无穷大时 converges to zero。（ii）训练后的数据生成分布应该远离任何与训练数据集中的例子重复的分布。我们提供了非假设性的结果，即 finite sample risk bounds，它们可以量化这些性质，并且取决于样本数、维度空间和秘密空间中的参数。我们的结果适用于总的积分概率空间中的错误量化方法， Wasserstein-$1$ 距离是中心示例。我们还包括数字示例，以证明我们的理论发现。
</details></li>
</ul>
<hr>
<h2 id="DCTM-Dilated-Convolutional-Transformer-Model-for-Multimodal-Engagement-Estimation-in-Conversation"><a href="#DCTM-Dilated-Convolutional-Transformer-Model-for-Multimodal-Engagement-Estimation-in-Conversation" class="headerlink" title="DCTM: Dilated Convolutional Transformer Model for Multimodal Engagement Estimation in Conversation"></a>DCTM: Dilated Convolutional Transformer Model for Multimodal Engagement Estimation in Conversation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01966">http://arxiv.org/abs/2308.01966</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vu Ngoc Tu, Van Thong Huynh, Hyung-Jeong Yang, M. Zaigham Zaheer, Shah Nawaz, Karthik Nandakumar, Soo-Hyung Kim</li>
<li>for: 这个研究的目的是估计对话中参与者的交流互动程度。</li>
<li>methods: 该研究使用了扩展 convolutional Transformer 来模型和估计对话中的人工智能参与度。</li>
<li>results: 我们的提议系统在测试集上显示了remarkable 7%的提升，并在验证集上显示了4%的提升。此外，我们还使用了不同的modalities fusions mechanisms，并证明了对这种数据，简单的 concatenation 方法加上自注意力融合得到最好的性能。<details>
<summary>Abstract</summary>
Conversational engagement estimation is posed as a regression problem, entailing the identification of the favorable attention and involvement of the participants in the conversation. This task arises as a crucial pursuit to gain insights into human's interaction dynamics and behavior patterns within a conversation. In this research, we introduce a dilated convolutional Transformer for modeling and estimating human engagement in the MULTIMEDIATE 2023 competition. Our proposed system surpasses the baseline models, exhibiting a noteworthy $7$\% improvement on test set and $4$\% on validation set. Moreover, we employ different modality fusion mechanism and show that for this type of data, a simple concatenated method with self-attention fusion gains the best performance.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将对话参与度估计 pose 为回归问题，意味着标识对话参与者的有利注意力和参与度。这项任务对于了解人类交流动力和行为模式内对话具有重要意义。在这项研究中，我们提出了扩展 convolutional Transformer 来模型和估计对话参与度，并在 MULTIMEDIATE 2023 比赛中采用该系统。我们的提议系统比基eline模型提高了7%的测试集和4%的验证集。此外，我们还使用不同的modalities fusion mechanism，并证明在这种数据上，简单 concatenation 方法加上自注意力混合得到最好的性能。Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. The Traditional Chinese version would be slightly different.
</details></li>
</ul>
<hr>
<h2 id="Subspace-Distillation-for-Continual-Learning"><a href="#Subspace-Distillation-for-Continual-Learning" class="headerlink" title="Subspace Distillation for Continual Learning"></a>Subspace Distillation for Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16419">http://arxiv.org/abs/2307.16419</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/csiro-robotics/sdcl">https://github.com/csiro-robotics/sdcl</a></li>
<li>paper_authors: Kaushik Roy, Christian Simon, Peyman Moghadam, Mehrtash Harandi</li>
<li>for: 本研究的目的是 mitigating continual learning 中的知识消亡问题，通过模型数据拟合 manifold 的首领空间结构，保持 neural network 学习新任务时的知识。</li>
<li>methods: 本研究提出了一种基于 manifold 结构的知识拟合技术，通过近似数据 manifold 的首领空间，模型数据结构，保持 neural network 的知识。</li>
<li>results: 实验表明，提出的方法可以减少 Catastrophic Forgetting 问题，并在 Pascal VOC 和 Tiny-Imagenet 等数据集上达到了比较好的效果。此外，本研究还证明了该方法可以与现有的学习方法结合使用，提高其性能。<details>
<summary>Abstract</summary>
An ultimate objective in continual learning is to preserve knowledge learned in preceding tasks while learning new tasks. To mitigate forgetting prior knowledge, we propose a novel knowledge distillation technique that takes into the account the manifold structure of the latent/output space of a neural network in learning novel tasks. To achieve this, we propose to approximate the data manifold up-to its first order, hence benefiting from linear subspaces to model the structure and maintain the knowledge of a neural network while learning novel concepts. We demonstrate that the modeling with subspaces provides several intriguing properties, including robustness to noise and therefore effective for mitigating Catastrophic Forgetting in continual learning. We also discuss and show how our proposed method can be adopted to address both classification and segmentation problems. Empirically, we observe that our proposed method outperforms various continual learning methods on several challenging datasets including Pascal VOC, and Tiny-Imagenet. Furthermore, we show how the proposed method can be seamlessly combined with existing learning approaches to improve their performances. The codes of this article will be available at https://github.com/csiro-robotics/SDCL.
</details>
<details>
<summary>摘要</summary>
最终目标在连续学习是保留先前任务中学习的知识，以便在学习新任务时不会忘记先前的知识。为了解决这个问题，我们提出了一种新的知识填充技术，该技术利用神经网络的输出/积分空间的拟合结构来保持先前知识。为此，我们提出了近似数据拟合的方法，通过利用线性子空间来模型结构，使神经网络在学习新概念时能够保持知识。我们发现，通过模型子空间可以获得许多有趣的性质，包括对噪声的抗性和鲁棒性，因此可以有效地避免Catastrophic Forgetting在连续学习中。此外，我们还讨论了如何采用我们提议的方法来解决分类和段化问题。实验表明，我们的提议方法在 Pascal VOC 和 Tiny-Imagenet 等挑战性数据集上显著超过了各种连续学习方法。此外，我们还证明了可以将我们的提议方法与现有的学习方法相结合，以提高其性能。 codes 的链接可以在 GitHub 上找到：https://github.com/csiro-robotics/SDCL。
</details></li>
</ul>
<hr>
<h2 id="Causal-learn-Causal-Discovery-in-Python"><a href="#Causal-learn-Causal-Discovery-in-Python" class="headerlink" title="Causal-learn: Causal Discovery in Python"></a>Causal-learn: Causal Discovery in Python</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16405">http://arxiv.org/abs/2307.16405</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/py-why/causal-learn">https://github.com/py-why/causal-learn</a></li>
<li>paper_authors: Yujia Zheng, Biwei Huang, Wei Chen, Joseph Ramsey, Mingming Gong, Ruichu Cai, Shohei Shimizu, Peter Spirtes, Kun Zhang</li>
<li>for: 本研究旨在探讨 causal discovery 技术，用于从观察数据中揭示 causal 关系。</li>
<li>methods: 本库使用了多种 causal discovery 方法，包括 PC、 FCI、 Causal Additive Model 等。</li>
<li>results: 本库提供了一个 comprehensive 的 causal discovery 方法集，便于实际应用。<details>
<summary>Abstract</summary>
Causal discovery aims at revealing causal relations from observational data, which is a fundamental task in science and engineering. We describe $\textit{causal-learn}$, an open-source Python library for causal discovery. This library focuses on bringing a comprehensive collection of causal discovery methods to both practitioners and researchers. It provides easy-to-use APIs for non-specialists, modular building blocks for developers, detailed documentation for learners, and comprehensive methods for all. Different from previous packages in R or Java, $\textit{causal-learn}$ is fully developed in Python, which could be more in tune with the recent preference shift in programming languages within related communities. The library is available at https://github.com/py-why/causal-learn.
</details>
<details>
<summary>摘要</summary>
causal discovery 的目标是从观察数据中揭示 causal 关系，这是科学和工程中的基本任务。我们描述了 $\textit{causal-learn}$，一个开源的 Python 库 для causal discovery。这个库将注重在带来可用的 causal discovery 方法，以便both practitioners 和研究人员。它提供了易于使用的 API，对 developers 来说是可分割的构建块，对学习者来说是详细的文档，并且支持了所有方法。与之前在 R 或 Java 中的包不同， $\textit{causal-learn}$ 是完全在 Python 中开发的，这可能更符合当前相关社区的编程语言偏好。该库可以在 https://github.com/py-why/causal-learn 上下载。
</details></li>
</ul>
<hr>
<h2 id="Bridging-the-Gap-Exploring-the-Capabilities-of-Bridge-Architectures-for-Complex-Visual-Reasoning-Tasks"><a href="#Bridging-the-Gap-Exploring-the-Capabilities-of-Bridge-Architectures-for-Complex-Visual-Reasoning-Tasks" class="headerlink" title="Bridging the Gap: Exploring the Capabilities of Bridge-Architectures for Complex Visual Reasoning Tasks"></a>Bridging the Gap: Exploring the Capabilities of Bridge-Architectures for Complex Visual Reasoning Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16395">http://arxiv.org/abs/2307.16395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kousik Rajesh, Mrigank Raman, Mohammed Asad Karim, Pranit Chawla</li>
<li>for: This paper investigates the performance of multi-modal bridge architectures on the NLVR2 dataset, specifically examining the impact of adding object-level features and pre-training on multi-modal data.</li>
<li>methods: The paper extends traditional bridge architectures for the NLVR2 dataset by adding object-level features and pre-training on multi-modal data. The authors also use a recently proposed bridge architecture, LLaVA, in the zero-shot setting and analyze its performance.</li>
<li>results: The authors find that adding object-level features to bridge architectures does not improve performance on complex visual reasoning tasks, and that pre-training on multi-modal data is key for good performance. They also demonstrate initial results on LLaVA in the zero-shot setting and analyze its performance.<details>
<summary>Abstract</summary>
In recent times there has been a surge of multi-modal architectures based on Large Language Models, which leverage the zero shot generation capabilities of LLMs and project image embeddings into the text space and then use the auto-regressive capacity to solve tasks such as VQA, captioning, and image retrieval. We name these architectures as "bridge-architectures" as they project from the image space to the text space. These models deviate from the traditional recipe of training transformer based multi-modal models, which involve using large-scale pre-training and complex multi-modal interactions through co or cross attention. However, the capabilities of bridge architectures have not been tested on complex visual reasoning tasks which require fine grained analysis about the image. In this project, we investigate the performance of these bridge-architectures on the NLVR2 dataset, and compare it to state-of-the-art transformer based architectures. We first extend the traditional bridge architectures for the NLVR2 dataset, by adding object level features to faciliate fine-grained object reasoning. Our analysis shows that adding object level features to bridge architectures does not help, and that pre-training on multi-modal data is key for good performance on complex reasoning tasks such as NLVR2. We also demonstrate some initial results on a recently bridge-architecture, LLaVA, in the zero shot setting and analyze its performance.
</details>
<details>
<summary>摘要</summary>
We first extend the traditional bridge architectures for the NLVR2 dataset by adding object-level features to facilitate fine-grained object reasoning. Our analysis shows that adding object-level features to bridge architectures does not help, and that pre-training on multi-modal data is key for good performance on complex reasoning tasks such as NLVR2. We also demonstrate some initial results on a recently proposed bridge-architecture, LLaVA, in the zero-shot setting and analyze its performance.
</details></li>
</ul>
<hr>
<h2 id="A-Pre-trained-Data-Deduplication-Model-based-on-Active-Learning"><a href="#A-Pre-trained-Data-Deduplication-Model-based-on-Active-Learning" class="headerlink" title="A Pre-trained Data Deduplication Model based on Active Learning"></a>A Pre-trained Data Deduplication Model based on Active Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00721">http://arxiv.org/abs/2308.00721</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyao Liu, Shengdong Du, Fengmao Lv, Hongtao Xue, Jie Hu, Tianrui Li</li>
<li>for:  addresses the issue of duplicate data in big data, which can limit the effective application of big data.</li>
<li>methods:  proposes a pre-trained deduplication model based on active learning, which integrates a pre-trained Transformer with active learning and employs the R-Drop method for data augmentation.</li>
<li>results:  achieves up to a 28% improvement in Recall score on benchmark datasets compared to previous SOTA for deduplicated data identification.Here’s the simplified Chinese text:</li>
<li>for:  addresses the issue of重复数据 in big data, which can limit the effective application of big data.</li>
<li>methods:  proposes a pre-trained deduplication model based on active learning, which integrates a pre-trained Transformer with active learning and employs the R-Drop method for data augmentation.</li>
<li>results:  achieves up to a 28% improvement in Recall score on benchmark datasets compared to previous SOTA for deduplicated data identification.<details>
<summary>Abstract</summary>
In the era of big data, the issue of data quality has become increasingly prominent. One of the main challenges is the problem of duplicate data, which can arise from repeated entry or the merging of multiple data sources. These "dirty data" problems can significantly limit the effective application of big data. To address the issue of data deduplication, we propose a pre-trained deduplication model based on active learning, which is the first work that utilizes active learning to address the problem of deduplication at the semantic level. The model is built on a pre-trained Transformer and fine-tuned to solve the deduplication problem as a sequence to classification task, which firstly integrate the transformer with active learning into an end-to-end architecture to select the most valuable data for deduplication model training, and also firstly employ the R-Drop method to perform data augmentation on each round of labeled data, which can reduce the cost of manual labeling and improve the model's performance. Experimental results demonstrate that our proposed model outperforms previous state-of-the-art (SOTA) for deduplicated data identification, achieving up to a 28% improvement in Recall score on benchmark datasets.
</details>
<details>
<summary>摘要</summary>
在大数据时代，数据质量问题变得越来越突出。一个主要挑战是重复入库的数据问题，这可能会由于多个数据源的合并或重复录入而导致。这些"异常数据"问题可能会对大数据的应用带来 significiant 限制。为了解决数据筛选问题，我们提出了基于活动学习的预训练deduplication模型，这是第一个利用活动学习解决 semantics 水平上的筛选问题。模型基于预训练的Transformer，并在这基础上进行了精心调整，以解决筛选问题为序列到类别任务。我们首次将Transformer结合活动学习 integrate 到末端架构中，以选择筛选模型训练的最有价值数据，同时首次使用R-Drop方法进行数据扩展，可以降低人工标注成本并提高模型性能。实验结果表明，我们提出的模型在比较预测数据集上达到了28%的回归分数提升，超过了前一个状态的最佳（SOTA）。
</details></li>
</ul>
<hr>
<h2 id="STL-A-Signed-and-Truncated-Logarithm-Activation-Function-for-Neural-Networks"><a href="#STL-A-Signed-and-Truncated-Logarithm-Activation-Function-for-Neural-Networks" class="headerlink" title="STL: A Signed and Truncated Logarithm Activation Function for Neural Networks"></a>STL: A Signed and Truncated Logarithm Activation Function for Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16389">http://arxiv.org/abs/2307.16389</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanhao Gong</li>
<li>for: 这篇论文是关于神经网络中的激活函数。</li>
<li>methods: 该论文提出了一种新的签名和截断对数函数作为激活函数，该函数具有许多优秀的数学性质，如对称、单调、导数不断、值范围无穷大和连续非零导数。</li>
<li>results: 对比其他一些常见的激活函数，该提出的激活函数表现出色，是当前领域的状态。该激活函数可以应用于各种神经网络中，where activation functions are necessary。<details>
<summary>Abstract</summary>
Activation functions play an essential role in neural networks. They provide the non-linearity for the networks. Therefore, their properties are important for neural networks' accuracy and running performance. In this paper, we present a novel signed and truncated logarithm function as activation function. The proposed activation function has significantly better mathematical properties, such as being odd function, monotone, differentiable, having unbounded value range, and a continuous nonzero gradient. These properties make it an excellent choice as an activation function. We compare it with other well-known activation functions in several well-known neural networks. The results confirm that it is the state-of-the-art. The suggested activation function can be applied in a large range of neural networks where activation functions are necessary.
</details>
<details>
<summary>摘要</summary>
aktivasi funksjoner spiller en sentral rolle i neurale nettverk. De gi non-lineære egenskaper til nettverkene. Dette gjør at egenskapene deres er viktig for nettverkeneksakkurat og løpske prestasjoner. I denne artikkelen presenteres en ny signert og avkortet logaritmfunksjon som aktivasjonsfunksjon. Den foreslåtte aktivasjonsfunksjonen har betydelig bedre matematiske egenskaper, som å være odd function, monoton, differensiabel, ha uavgrenset verdiområde og en kontinuerlig ukent zero gradient. Disse egenskapene gjør den til en utmerket valg som aktivasjonsfunksjon. Vi sammenligner den med andre velkjente aktivasjonsfunksjoner i flere velkjente neurale nettverk. Resultatene bekrefter at den er state-of-the-art. Foreslåtte aktivasjonsfunksjon kan bli brukt i et stort spekter av neurale nettverk der aktivasjonsfunksjoner er nødvendige.
</details></li>
</ul>
<hr>
<h2 id="Does-fine-tuning-GPT-3-with-the-OpenAI-API-leak-personally-identifiable-information"><a href="#Does-fine-tuning-GPT-3-with-the-OpenAI-API-leak-personally-identifiable-information" class="headerlink" title="Does fine-tuning GPT-3 with the OpenAI API leak personally-identifiable information?"></a>Does fine-tuning GPT-3 with the OpenAI API leak personally-identifiable information?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16382">http://arxiv.org/abs/2307.16382</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/albertsun1/gpt3-pii-attacks">https://github.com/albertsun1/gpt3-pii-attacks</a></li>
<li>paper_authors: Albert Yu Sun, Eliott Zemour, Arushi Saxena, Udith Vaidyanathan, Eric Lin, Christian Lau, Vaikkunth Mugunthan</li>
<li>for: 本研究旨在探讨 OpenAI 的 GPT-3 模型是否会在 fine-tuning 过程中记忆和泄露敏感信息。</li>
<li>methods: 我们使用了 Naive 提示方法和实用的 Autocomplete 任务来研究 GPT-3 在 fine-tuning 过程中是否会记忆和泄露 Personally Identifiable Information (PII)。</li>
<li>results: 我们发现，在 fine-tuning GPT-3 模型进行分类和 Autocomplete 任务时，模型会记忆和泄露来自原始 fine-tuning 数据集中的敏感信息。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Machine learning practitioners often fine-tune generative pre-trained models like GPT-3 to improve model performance at specific tasks. Previous works, however, suggest that fine-tuned machine learning models memorize and emit sensitive information from the original fine-tuning dataset. Companies such as OpenAI offer fine-tuning services for their models, but no prior work has conducted a memorization attack on any closed-source models. In this work, we simulate a privacy attack on GPT-3 using OpenAI's fine-tuning API. Our objective is to determine if personally identifiable information (PII) can be extracted from this model. We (1) explore the use of naive prompting methods on a GPT-3 fine-tuned classification model, and (2) we design a practical word generation task called Autocomplete to investigate the extent of PII memorization in fine-tuned GPT-3 within a real-world context. Our findings reveal that fine-tuning GPT3 for both tasks led to the model memorizing and disclosing critical personally identifiable information (PII) obtained from the underlying fine-tuning dataset. To encourage further research, we have made our codes and datasets publicly available on GitHub at: https://github.com/albertsun1/gpt3-pii-attacks
</details>
<details>
<summary>摘要</summary>
机器学习实践者经常微调生成预训练模型如GPT-3以提高模型在特定任务中的性能。先前的研究，然而，表明微调机器学习模型会将敏感信息从原始微调 dataset 内存储和频繁发送出来。如OpenAI提供的微调服务，但前一次的工作没有对任何关闭源代码模型进行 memorization 攻击。在这个工作中，我们模拟了一个隐私攻击，以查看GPT-3是否可以从这个模型中提取 personally identifiable information (PII)。我们（1）探索了使用简单提示方法在GPT-3微调分类模型上，并（2）我们设计了一个实用的字串生成任务，called Autocomplete，以调查微调GPT-3中PII的储存情况。我们的结果显示，对这两个任务进行微调GPT-3都会将模型将敏感信息从基础微调 dataset 内存储和频繁发送出来。为了鼓励更多的研究，我们在 GitHub 上公开了我们的代码和数据，请见：https://github.com/albertsun1/gpt3-pii-attacks。
</details></li>
</ul>
<hr>
<h2 id="UniAP-Unifying-Inter-and-Intra-Layer-Automatic-Parallelism-by-Mixed-Integer-Quadratic-Programming"><a href="#UniAP-Unifying-Inter-and-Intra-Layer-Automatic-Parallelism-by-Mixed-Integer-Quadratic-Programming" class="headerlink" title="UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed Integer Quadratic Programming"></a>UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed Integer Quadratic Programming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16375">http://arxiv.org/abs/2307.16375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Lin, Ke Wu, Jun Li, Wu-Jun Li</li>
<li>for: 提高深度学习模型的训练效率， automatize parallel strategy searching process.</li>
<li>methods: 使用杂度二次函数Programming（Mixed Integer Quadratic Programming，MIQP）统一inter-和intra-层自动并行，Search for globally optimal parallel strategy.</li>
<li>results: 与现有方法比较，UniAP在四种Transformer-like模型上提高了通过put和减少策略搜索时间，最高提高1.70倍和16倍。<details>
<summary>Abstract</summary>
Deep learning models have demonstrated impressive performance in various domains. However, the prolonged training time of these models remains a critical problem. Manually designed parallel training strategies could enhance efficiency but require considerable time and deliver little flexibility. Hence, automatic parallelism is proposed to automate the parallel strategy searching process. Even so, existing approaches suffer from sub-optimal strategy space because they treat automatic parallelism as two independent stages, namely inter- and intra-layer parallelism. To address this issue, we propose UniAP, which utilizes mixed integer quadratic programming to unify inter- and intra-layer automatic parallelism. To the best of our knowledge, UniAP is the first work to unify these two categories to search for a globally optimal strategy. The experimental results show that UniAP outperforms state-of-the-art methods by up to 1.70$\times$ in throughput and reduces strategy searching time by up to 16$\times$ across four Transformer-like models.
</details>
<details>
<summary>摘要</summary>
为解决这个问题，我们提出了UniAP，它使用混合整数二次函数编程来统一间层和内层自动并行。根据我们知道，UniAP是首次将这两个类别统一到搜索全球最佳策略。实验结果表明，UniAP在四种Transformer-like模型上比州先进方法提高了1.70倍的 Throughput，并将搜索策略时间减少了16倍。
</details></li>
</ul>
<hr>
<h2 id="BearingPGA-Net-A-Lightweight-and-Deployable-Bearing-Fault-Diagnosis-Network-via-Decoupled-Knowledge-Distillation-and-FPGA-Acceleration"><a href="#BearingPGA-Net-A-Lightweight-and-Deployable-Bearing-Fault-Diagnosis-Network-via-Decoupled-Knowledge-Distillation-and-FPGA-Acceleration" class="headerlink" title="BearingPGA-Net: A Lightweight and Deployable Bearing Fault Diagnosis Network via Decoupled Knowledge Distillation and FPGA Acceleration"></a>BearingPGA-Net: A Lightweight and Deployable Bearing Fault Diagnosis Network via Decoupled Knowledge Distillation and FPGA Acceleration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16363">http://arxiv.org/abs/2307.16363</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/asdvfghg/bearingpga-net">https://github.com/asdvfghg/bearingpga-net</a></li>
<li>paper_authors: Jing-Xiao Liao, Sheng-Lai Wei, Chen-Long Xie, Tieyong Zeng, Jinwei Sun, Shiping Zhang, Xiaoge Zhang, Feng-Lei Fan</li>
<li>for: 这个研究旨在提出一个轻量级且可部署的扩展网络模型，以应对滚统疲势诊断中的大型模型和复杂计算问题。</li>
<li>methods: 我们使用了一个已经受训的大型模型，透过分离知识传授来训练我们的 BearingPGA-Net 模型。我们还设计了一个 FPGA 加速方案，使用 Verilog 进行自适应量化和专案可程式逻辑门的设计，以提高计算速度。</li>
<li>results: 我们的部署方案在 CPU 上进行诊断 Speed 比较，得到了超过 200 倍的提升，而且在独立收集的滚统数据上保持了适用率下降在 0.4% 以下。我们的代码可以在 <a target="_blank" rel="noopener" href="https://github.com/asdvfghg/BearingPGA-Net">https://github.com/asdvfghg/BearingPGA-Net</a> 上获取。<details>
<summary>Abstract</summary>
Deep learning has achieved remarkable success in the field of bearing fault diagnosis. However, this success comes with larger models and more complex computations, which cannot be transferred into industrial fields requiring models to be of high speed, strong portability, and low power consumption. In this paper, we propose a lightweight and deployable model for bearing fault diagnosis, referred to as BearingPGA-Net, to address these challenges. Firstly, aided by a well-trained large model, we train BearingPGA-Net via decoupled knowledge distillation. Despite its small size, our model demonstrates excellent fault diagnosis performance compared to other lightweight state-of-the-art methods. Secondly, we design an FPGA acceleration scheme for BearingPGA-Net using Verilog. This scheme involves the customized quantization and designing programmable logic gates for each layer of BearingPGA-Net on the FPGA, with an emphasis on parallel computing and module reuse to enhance the computational speed. To the best of our knowledge, this is the first instance of deploying a CNN-based bearing fault diagnosis model on an FPGA. Experimental results reveal that our deployment scheme achieves over 200 times faster diagnosis speed compared to CPU, while achieving a lower-than-0.4\% performance drop in terms of F1, Recall, and Precision score on our independently-collected bearing dataset. Our code is available at \url{https://github.com/asdvfghg/BearingPGA-Net}.
</details>
<details>
<summary>摘要</summary>
深度学习在轮子短circuit故障诊断方面已经取得了非常出色的成绩。然而，这些成绩是由更大的模型和更复杂的计算所支持的，这些模型在工业环境中不可能实现高速、强可移植和低功耗的要求。在这篇论文中，我们提出了一种轻量级可部署的模型，称为BearingPGA-Net，以解决这些挑战。首先，我们通过使用已经训练过的大型模型，将BearingPGA-Net通过分离知识填充训练。尽管它的模型量小，但BearingPGA-Net仍然在其他轻量级现状的方法中表现出色，并且在我们独立收集的轮子数据集上达到了0.4%以下的F1、Recall和Precision分数。其次，我们为BearingPGA-Net设计了FPGA加速方案，使用Verilog编程。这种方案包括为每层BearingPGA-Net的FPGA自定义量化和设计可编程逻辑门。我们强调了并行计算和模块重用，以提高计算速度。到目前为止，这是第一次将CNN基于轮子短circuit故障诊断模型部署到FPGA上。实验结果显示，我们的部署方案可以在CPU上进行200倍以上的诊断速度提升，同时在F1、Recall和Precision分数上保持下降在0.4%以下。我们的代码可以在<https://github.com/asdvfghg/BearingPGA-Net>上获取。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-and-Analyzing-Robust-Point-Cloud-Recognition-Bag-of-Tricks-for-Defending-Adversarial-Examples"><a href="#Benchmarking-and-Analyzing-Robust-Point-Cloud-Recognition-Bag-of-Tricks-for-Defending-Adversarial-Examples" class="headerlink" title="Benchmarking and Analyzing Robust Point Cloud Recognition: Bag of Tricks for Defending Adversarial Examples"></a>Benchmarking and Analyzing Robust Point Cloud Recognition: Bag of Tricks for Defending Adversarial Examples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16361">http://arxiv.org/abs/2307.16361</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qiufan319/benchmark_pc_attack">https://github.com/qiufan319/benchmark_pc_attack</a></li>
<li>paper_authors: Qiufan Ji, Lin Wang, Cong Shi, Shengshan Hu, Yingying Chen, Lichao Sun</li>
<li>for: 本研究旨在提高深度神经网络（DNNs）在3D点云识别 task 中的鲁棒性，即使面临攻击者可能会通过添加、移动或删除点来生成攻击性的点云数据。</li>
<li>methods: 本研究首先建立了一个完整的、彻底的3D点云攻击 robustness 评估标准，以evaluate 攻击和防御策略的效果。然后，我们收集了现有的防御技巧，并进行了广泛和系统的实验，以确定一个有效的组合方法。最后，我们提出了一种hybrid training augmentation方法，考虑了多种类型的点云攻击例子，并将其与针对性训练相结合，以提高鲁棒性。</li>
<li>results: 通过 combining 多种防御技巧和 hybrid training augmentation方法，我们构建了一个更加鲁棒的防御框架，其中83.45%的批处率能够抵抗多种攻击。这表明我们的防御框架具有启发人的鲁棒性，可以帮助实现可靠的学习。我们的代码库在：\url{<a target="_blank" rel="noopener" href="https://github.com/qiufan319/benchmark_pc_attack.git%7D%E3%80%82">https://github.com/qiufan319/benchmark_pc_attack.git}。</a><details>
<summary>Abstract</summary>
Deep Neural Networks (DNNs) for 3D point cloud recognition are vulnerable to adversarial examples, threatening their practical deployment. Despite the many research endeavors have been made to tackle this issue in recent years, the diversity of adversarial examples on 3D point clouds makes them more challenging to defend against than those on 2D images. For examples, attackers can generate adversarial examples by adding, shifting, or removing points. Consequently, existing defense strategies are hard to counter unseen point cloud adversarial examples. In this paper, we first establish a comprehensive, and rigorous point cloud adversarial robustness benchmark to evaluate adversarial robustness, which can provide a detailed understanding of the effects of the defense and attack methods. We then collect existing defense tricks in point cloud adversarial defenses and then perform extensive and systematic experiments to identify an effective combination of these tricks. Furthermore, we propose a hybrid training augmentation methods that consider various types of point cloud adversarial examples to adversarial training, significantly improving the adversarial robustness. By combining these tricks, we construct a more robust defense framework achieving an average accuracy of 83.45\% against various attacks, demonstrating its capability to enabling robust learners. Our codebase are open-sourced on: \url{https://github.com/qiufan319/benchmark_pc_attack.git}.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNNs） для三维点云识别容易受到攻击，这在实际应用中受到威胁。尽管过去几年来有很多研究努力用于解决这一问题，但三维点云上的攻击者可以通过添加、移动或删除点来生成攻击性例子，使现有的防御策略很难对未看到的点云攻击性例子进行防御。在这篇论文中，我们首先建立了一个完整、严格的点云攻击Robustness指标，用于评估防御 robustness，可以为我们提供深入的理解防御和攻击方法的影响。然后，我们收集了现有的点云防御技巧，并进行了广泛和系统的实验，以确定有效的组合方法。此外，我们提议一种混合培育方法，考虑了多种点云攻击性例子，并将其与培育训练相结合，从而显著提高了对攻击的鲁棒性。通过组合这些技巧，我们构建了一个更加鲁棒的防御框架，其中83.45%的检测精度可以抵抗多种攻击，这表明了它的可行性。我们的代码库将在：\url{https://github.com/qiufan319/benchmark_pc_attack.git}上开源。
</details></li>
</ul>
<hr>
<h2 id="Probabilistically-robust-conformal-prediction"><a href="#Probabilistically-robust-conformal-prediction" class="headerlink" title="Probabilistically robust conformal prediction"></a>Probabilistically robust conformal prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16360">http://arxiv.org/abs/2307.16360</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/1995subhankar1995/PRCP">https://github.com/1995subhankar1995/PRCP</a></li>
<li>paper_authors: Subhankar Ghosh, Yuanjie Shi, Taha Belkhouja, Yan Yan, Jana Doppa, Brian Jones</li>
<li>for: 这篇论文目的是研究机器学习分类器中的不确定性评估，包括深度神经网络。</li>
<li>methods: 这篇论文使用了内在测试数据的概率性Robust Conformal Prediction（PRCP）框架，以提高机器学习分类器的不确定性评估。</li>
<li>results: 这篇论文的实验结果显示，使用了 novel adaptive PRCP（aPRCP）算法可以实现更好的不确定性评估，并且在CIFAR-10、CIFAR-100和ImageNet datasets上使用深度神经网络进行了试验。<details>
<summary>Abstract</summary>
Conformal prediction (CP) is a framework to quantify uncertainty of machine learning classifiers including deep neural networks. Given a testing example and a trained classifier, CP produces a prediction set of candidate labels with a user-specified coverage (i.e., true class label is contained with high probability). Almost all the existing work on CP assumes clean testing data and there is not much known about the robustness of CP algorithms w.r.t natural/adversarial perturbations to testing examples. This paper studies the problem of probabilistically robust conformal prediction (PRCP) which ensures robustness to most perturbations around clean input examples. PRCP generalizes the standard CP (cannot handle perturbations) and adversarially robust CP (ensures robustness w.r.t worst-case perturbations) to achieve better trade-offs between nominal performance and robustness. We propose a novel adaptive PRCP (aPRCP) algorithm to achieve probabilistically robust coverage. The key idea behind aPRCP is to determine two parallel thresholds, one for data samples and another one for the perturbations on data (aka "quantile-of-quantile" design). We provide theoretical analysis to show that aPRCP algorithm achieves robust coverage. Our experiments on CIFAR-10, CIFAR-100, and ImageNet datasets using deep neural networks demonstrate that aPRCP achieves better trade-offs than state-of-the-art CP and adversarially robust CP algorithms.
</details>
<details>
<summary>摘要</summary>
《匹配预测（CP）是一种框架，用于评估机器学习分类器，包括深度神经网络的不确定性。给定测试示例和已经训练过的分类器，CP 生成一个包含真正类别的可能性的预测集。现有大多数 CP 研究假设测试数据是干净的，并没有很多关于 CP 算法对自然/敌意扰动的Robustness的研究。这篇文章研究了 probablistically Robust Conformal Prediction（PRCP）问题，PRCP 可以 Ensure robustness to most perturbations around clean input examples。PRCP 扩展了标准 CP（不能处理扰动）和敌意Robust CP（对最差情况扰动 Ensure robustness），以 achieve better trade-offs between nominal performance and robustness。我们提出了一种 noval adaptive PRCP（aPRCP）算法，以实现 probabilistically robust coverage。aPRCP 的关键思想是在数据示例和数据扰动之间设置两个平行的阈值（即 "quantile-of-quantile" 设计）。我们提供了理论分析，证明 aPRCP 算法实现了 robust coverage。我们在 CIFAR-10、CIFAR-100 和 ImageNet  datasets上使用深度神经网络进行实验，demonstrate that aPRCP 实现了更好的 trade-offs than state-of-the-art CP 和 adversarially robust CP 算法。
</details></li>
</ul>
<hr>
<h2 id="Moreau-Yoshida-Variational-Transport-A-General-Framework-For-Solving-Regularized-Distributional-Optimization-Problems"><a href="#Moreau-Yoshida-Variational-Transport-A-General-Framework-For-Solving-Regularized-Distributional-Optimization-Problems" class="headerlink" title="Moreau-Yoshida Variational Transport: A General Framework For Solving Regularized Distributional Optimization Problems"></a>Moreau-Yoshida Variational Transport: A General Framework For Solving Regularized Distributional Optimization Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16358">http://arxiv.org/abs/2307.16358</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dai Hai Nguyen, Tetsuya Sakurai</li>
<li>for:  solves a regularized distributional optimization problem, which is widely used in machine learning and statistics.</li>
<li>methods:  proposes a novel method called Moreau-Yoshida Variational Transport (MYVT) to solve the regularized distributional optimization problem. The method uses the Moreau-Yoshida envelope for a smooth approximation of the nonsmooth function in the objective, and then reformulates the approximate problem as a concave-convex saddle point problem.</li>
<li>results:  provides theoretical analyses and experimental results to demonstrate the effectiveness of the proposed method.<details>
<summary>Abstract</summary>
We consider a general optimization problem of minimizing a composite objective functional defined over a class of probability distributions. The objective is composed of two functionals: one is assumed to possess the variational representation and the other is expressed in terms of the expectation operator of a possibly nonsmooth convex regularizer function. Such a regularized distributional optimization problem widely appears in machine learning and statistics, such as proximal Monte-Carlo sampling, Bayesian inference and generative modeling, for regularized estimation and generation.   We propose a novel method, dubbed as Moreau-Yoshida Variational Transport (MYVT), for solving the regularized distributional optimization problem. First, as the name suggests, our method employs the Moreau-Yoshida envelope for a smooth approximation of the nonsmooth function in the objective. Second, we reformulate the approximate problem as a concave-convex saddle point problem by leveraging the variational representation, and then develope an efficient primal-dual algorithm to approximate the saddle point. Furthermore, we provide theoretical analyses and report experimental results to demonstrate the effectiveness of the proposed method.
</details>
<details>
<summary>摘要</summary>
我们考虑一个总体优化问题，即将一个集合中的概率分布最小化一个复合目标函数。该目标函数由两个函数组成：一个假设具有变量表示，另一个是一个可能非光滑的凸违反函数。这种凸违反化优化问题广泛出现在机器学习和统计中，如距离 Monte Carlo 抽样、 bayesian 推理和生成模型，用于凸违反估计和生成。我们提出一种新方法，名为 Moreau-Yoshida 变量运输（MYVT），用于解决凸违反化优化问题。首先，我们使用 Moreau-Yoshida 缓和函数来简化目标函数中的非光滑函数。然后，我们将问题转换为一个凹凸融合点问题，并使用变量表示来减少问题的复杂性。最后，我们开发了一个高效的 primal-dual 算法来近似融合点。此外，我们还提供了理论分析和实验结果，以证明我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Hypertension-Detection-From-High-Dimensional-Representation-of-Photoplethysmogram-Signals"><a href="#Hypertension-Detection-From-High-Dimensional-Representation-of-Photoplethysmogram-Signals" class="headerlink" title="Hypertension Detection From High-Dimensional Representation of Photoplethysmogram Signals"></a>Hypertension Detection From High-Dimensional Representation of Photoplethysmogram Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02425">http://arxiv.org/abs/2308.02425</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/navidhasanzadeh/hypertension_ppg">https://github.com/navidhasanzadeh/hypertension_ppg</a></li>
<li>paper_authors: Navid Hasanzadeh, Shahrokh Valaee, Hojjat Salehinejad</li>
<li>For: 这个研究旨在探索用 Photoplethysmogram (PPG) 讯号来检测高血压 (hypertension) 的可能性。* Methods: 本研究使用高维度表示技术基于随机扩散 kernel，将 PPG 讯号转换为高维度特征表示。* Results: 研究结果显示，这种关系超出心率和血压，证明了高血压检测的可能性。此外，使用扩散 kernel 的变数对于时间序列特征提取器的表现亦较前一 studies 和现有的深度学习模型更好。<details>
<summary>Abstract</summary>
Hypertension is commonly referred to as the "silent killer", since it can lead to severe health complications without any visible symptoms. Early detection of hypertension is crucial in preventing significant health issues. Although some studies suggest a relationship between blood pressure and certain vital signals, such as Photoplethysmogram (PPG), reliable generalization of the proposed blood pressure estimation methods is not yet guaranteed. This lack of certainty has resulted in some studies doubting the existence of such relationships, or considering them weak and limited to heart rate and blood pressure. In this paper, a high-dimensional representation technique based on random convolution kernels is proposed for hypertension detection using PPG signals. The results show that this relationship extends beyond heart rate and blood pressure, demonstrating the feasibility of hypertension detection with generalization. Additionally, the utilized transform using convolution kernels, as an end-to-end time-series feature extractor, outperforms the methods proposed in the previous studies and state-of-the-art deep learning models.
</details>
<details>
<summary>摘要</summary>
高血压通常被称为"无音杀手"，因为它可以导致严重的健康问题而无需任何可见的 симптом。早期检测高血压非常重要，以预防严重的健康问题。虽然一些研究表明血压和某些生命函数之间存在关系，如脉搏图（PPG），但可靠地总结这些血压估计方法的可靠性并不是 garantizado。这种不确定性导致了一些研究质疑这些关系的存在，或者认为它们是弱的和限制于心率和血压。在这篇论文中，一种基于随机杂化核心的高维表示技术被提议用于高血压检测使用PPG信号。结果表明，这种关系超出了心率和血压，表明高血压检测的可能性。此外，使用杂化核心来实现端到端时间序列特征提取，比前一些研究和当前最佳深度学习模型更高效。
</details></li>
</ul>
<hr>
<h2 id="Rating-based-Reinforcement-Learning"><a href="#Rating-based-Reinforcement-Learning" class="headerlink" title="Rating-based Reinforcement Learning"></a>Rating-based Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16348">http://arxiv.org/abs/2307.16348</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Devin White, Mingkang Wu, Ellen Novoseller, Vernon Lawhern, Nick Waytowich, Yongcan Cao</li>
<li>for: 这个论文是为了开发一种基于评分的 reinforcement learning 方法，利用人类评分来获得人类指导。</li>
<li>methods: 该方法基于人类评分各个轨迹的评价，不需要对样本对比进行相对评价。它基于新的预测模型和多类损失函数。</li>
<li>results: 通过 synthetic 评分和实际人类评分进行多个实验研究，证明了该方法的效果和优势。<details>
<summary>Abstract</summary>
This paper develops a novel rating-based reinforcement learning approach that uses human ratings to obtain human guidance in reinforcement learning. Different from the existing preference-based and ranking-based reinforcement learning paradigms, based on human relative preferences over sample pairs, the proposed rating-based reinforcement learning approach is based on human evaluation of individual trajectories without relative comparisons between sample pairs. The rating-based reinforcement learning approach builds on a new prediction model for human ratings and a novel multi-class loss function. We conduct several experimental studies based on synthetic ratings and real human ratings to evaluate the effectiveness and benefits of the new rating-based reinforcement learning approach.
</details>
<details>
<summary>摘要</summary>
这个论文提出了一种新的评分基于束缚学习方法，该方法使用人类评分来获得人类指导在束缚学习中。与现有的偏好基于样本对比和排名基于样本对比不同，提出的评分基于束缚学习方法是基于人类评估个体轨迹而不需要对样本对比进行相对评价。该方法建立在新的人类评分预测模型和多类损失函数之上。我们通过对 sintetic 评分和实际人类评分进行多次实验来评估新评分基于束缚学习方法的效果和优势。
</details></li>
</ul>
<hr>
<h2 id="Proof-of-Federated-Learning-Subchain-Free-Partner-Selection-Subchain-Based-on-Federated-Learning"><a href="#Proof-of-Federated-Learning-Subchain-Free-Partner-Selection-Subchain-Based-on-Federated-Learning" class="headerlink" title="Proof-of-Federated-Learning-Subchain: Free Partner Selection Subchain Based on Federated Learning"></a>Proof-of-Federated-Learning-Subchain: Free Partner Selection Subchain Based on Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16342">http://arxiv.org/abs/2307.16342</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boyang Li, Bingyu Shen, Qing Lu, Taeho Jung, Yiyu Shi</li>
<li>for: 这篇论文主要是为了提出一种新的分布式共识机制，以替代之前的Proof-of-Deep-Learning(PoDL)机制，以更有效地使用能源并维护区块链。</li>
<li>methods: 该论文提出了一种名为Proof-of-Federated-Learning-Subchain(PoFLSC)的新的共识机制，它使用了一个子链来记录训练、挑战和审核活动，并强调了合理的数据集的选择对伙伴的重要性。</li>
<li>results: 在 simulate 20 个矿工的情况下，该论文表明了 PoFLSC 机制的有效性，当减少了pool size时， Pool 中的矿工会根据其 Shapley Value (SV) 的高低来选择合适的矿工。在实验中，PoFLSC 机制帮助了子链管理员了解储值优先级和核心分区的参与者，以建立和维护竞争性的子链。<details>
<summary>Abstract</summary>
The continuous thriving of the Blockchain society motivates research in novel designs of schemes supporting cryptocurrencies. Previously multiple Proof-of-Deep-Learning(PoDL) consensuses have been proposed to replace hashing with useful work such as deep learning model training tasks. The energy will be more efficiently used while maintaining the ledger. However deep learning models are problem-specific and can be extremely complex. Current PoDL consensuses still require much work to realize in the real world. In this paper, we proposed a novel consensus named Proof-of-Federated-Learning-Subchain(PoFLSC) to fill the gap. We applied a subchain to record the training, challenging, and auditing activities and emphasized the importance of valuable datasets in partner selection. We simulated 20 miners in the subchain to demonstrate the effectiveness of PoFLSC. When we reduce the pool size concerning the reservation priority order, the drop rate difference in the performance in different scenarios further exhibits that the miner with a higher Shapley Value (SV) will gain a better opportunity to be selected when the size of the subchain pool is limited. In the conducted experiments, the PoFLSC consensus supported the subchain manager to be aware of reservation priority and the core partition of contributors to establish and maintain a competitive subchain.
</details>
<details>
<summary>摘要</summary>
continuous 社区的坚持发展激发了研究新的方案，以支持criptocurrency的 schemes。 previously， multiple  Proof-of-Deep-Learning（PoDL）consensuses have been proposed to replace hashing with useful work such as deep learning model training tasks. The energy will be more efficiently used while maintaining the ledger. However, deep learning models are problem-specific and can be extremely complex. Current PoDL consensuses still require much work to realize in the real world. In this paper, we proposed a novel consensus named Proof-of-Federated-Learning-Subchain（PoFLSC）to fill the gap. We applied a subchain to record the training, challenging, and auditing activities and emphasized the importance of valuable datasets in partner selection. We simulated 20 miners in the subchain to demonstrate the effectiveness of PoFLSC. When we reduce the pool size concerning the reservation priority order, the drop rate difference in the performance in different scenarios further exhibits that the miner with a higher Shapley Value (SV) will gain a better opportunity to be selected when the size of the subchain pool is limited. In the conducted experiments, the PoFLSC consensus supported the subchain manager to be aware of reservation priority and the core partition of contributors to establish and maintain a competitive subchain.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Theoretically-Principled-Trade-off-for-Stateful-Defenses-against-Query-Based-Black-Box-Attacks"><a href="#Theoretically-Principled-Trade-off-for-Stateful-Defenses-against-Query-Based-Black-Box-Attacks" class="headerlink" title="Theoretically Principled Trade-off for Stateful Defenses against Query-Based Black-Box Attacks"></a>Theoretically Principled Trade-off for Stateful Defenses against Query-Based Black-Box Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16331">http://arxiv.org/abs/2307.16331</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashish Hooda, Neal Mangaokar, Ryan Feng, Kassem Fawaz, Somesh Jha, Atul Prakash</li>
<li>for: 本研究旨在提供对状态防御系统的 theoretically Characterization，以优化它们的抗击攻击性能。</li>
<li>methods: 本研究使用了一种普遍的 feature extractors 类型，并对其进行了系统的分析和优化。</li>
<li>results: 研究发现，状态防御系统的抗击攻击性能与 feature extractors 的选择和阈值设置有直接的关系。同时，对于某些特定的攻击方法，状态防御系统可以具有较高的抗击攻击性能。<details>
<summary>Abstract</summary>
Adversarial examples threaten the integrity of machine learning systems with alarming success rates even under constrained black-box conditions. Stateful defenses have emerged as an effective countermeasure, detecting potential attacks by maintaining a buffer of recent queries and detecting new queries that are too similar. However, these defenses fundamentally pose a trade-off between attack detection and false positive rates, and this trade-off is typically optimized by hand-picking feature extractors and similarity thresholds that empirically work well. There is little current understanding as to the formal limits of this trade-off and the exact properties of the feature extractors/underlying problem domain that influence it. This work aims to address this gap by offering a theoretical characterization of the trade-off between detection and false positive rates for stateful defenses. We provide upper bounds for detection rates of a general class of feature extractors and analyze the impact of this trade-off on the convergence of black-box attacks. We then support our theoretical findings with empirical evaluations across multiple datasets and stateful defenses.
</details>
<details>
<summary>摘要</summary>
针对机器学习系统的敌对示例具有较高的攻击成功率，即使在封装黑盒条件下。状态防御技术已经出现为有效的对抗手段，通过维护最近几个查询的缓存来检测潜在攻击，并且检测新的查询是否过于相似。然而，这些防御技术存在一定的负面影响和假阳性率的负担，这些负担通常通过手动选择特征提取器和相似性阈值来优化。现在，我们对这种负面影响和假阳性率的负担的正负限制尚未有很好的理解，以及特征提取器和下游问题领域的影响。这项工作的目标是提供状态防御技术的检测和假阳性率之间的正负限制的理论Characterization。我们提供一类通用特征提取器的检测率的Upper bound，并分析黑盒攻击的收敛行为受到这种负面影响的影响。最后，我们通过多个数据集和状态防御技术的实证验证我们的理论发现。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-ChatGPT-and-GPT-4-for-Visual-Programming"><a href="#Evaluating-ChatGPT-and-GPT-4-for-Visual-Programming" class="headerlink" title="Evaluating ChatGPT and GPT-4 for Visual Programming"></a>Evaluating ChatGPT and GPT-4 for Visual Programming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02522">http://arxiv.org/abs/2308.02522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adish Singla</li>
<li>for: 这种研究的目的是确定现代生成模型在视觉编程领域是否有高水平的表现，与文本编程领域的 Python 编程相比。</li>
<li>methods: 这种研究使用了两种模型：ChatGPT 和 GPT-4，并对它们在不同的场景下进行评估，以确定它们在视觉编程领域的表现。</li>
<li>results: 研究发现，这些模型在视觉编程领域表现不佳，尤其是在结合空间、逻辑和编程技能方面表现不佳，这些技能是视觉编程中非常重要的。这些结果还提供了未来发展生成模型在视觉编程领域的推动性的方向。<details>
<summary>Abstract</summary>
Generative AI and large language models have the potential to drastically improve the landscape of computing education by automatically generating personalized feedback and content. Recent works have studied the capabilities of these models for different programming education scenarios; however, these works considered only text-based programming, in particular, Python programming. Consequently, they leave open the question of how well these models would perform in visual programming domains popularly used for K-8 programming education. The main research question we study is: Do state-of-the-art generative models show advanced capabilities in visual programming on par with their capabilities in text-based Python programming? In our work, we evaluate two models, ChatGPT (based on GPT-3.5) and GPT-4, in visual programming domains for various scenarios and assess performance using expert-based annotations. In particular, we base our evaluation using reference tasks from the domains of Hour of Code: Maze Challenge by Code-dot-org and Karel. Our results show that these models perform poorly and struggle to combine spatial, logical, and programming skills crucial for visual programming. These results also provide exciting directions for future work on developing techniques to improve the performance of generative models in visual programming.
</details>
<details>
<summary>摘要</summary>
将文本翻译成简化中文。大数据生成AI和大语言模型有可能在计算教育领域带来巨大改变，通过自动生成个性化反馈和内容来提高学习效率。最近的研究已经研究了这些模型在不同的编程教育场景中的能力，但是这些研究仅考虑了文本编程，尤其是Python编程。因此，它们留下了如何在视觉编程领域中的表现的问题。我们的研究问题是：现代生成模型在视觉编程领域是否有 similarly advanced capabilities ？在我们的工作中，我们评估了两个模型：ChatGPT（基于 GPT-3.5）和 GPT-4，在不同的视觉编程场景中的表现。我们基于Reference Tasks from the domains of Hour of Code: Maze Challenge by Code-dot-org和Karel进行评估。我们的结果表明，这些模型在视觉编程中表现不佳，它们很难结合空间、逻辑和编程技能。这些结果还提供了未来开发生成模型在视觉编程中提高表现的激动人心的方向。
</details></li>
</ul>
<hr>
<h2 id="RoseNNa-A-performant-portable-library-for-neural-network-inference-with-application-to-computational-fluid-dynamics"><a href="#RoseNNa-A-performant-portable-library-for-neural-network-inference-with-application-to-computational-fluid-dynamics" class="headerlink" title="RoseNNa: A performant, portable library for neural network inference with application to computational fluid dynamics"></a>RoseNNa: A performant, portable library for neural network inference with application to computational fluid dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16322">http://arxiv.org/abs/2307.16322</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/comp-physics/roseNNa">https://github.com/comp-physics/roseNNa</a></li>
<li>paper_authors: Ajay Bati, Spencer H. Bryngelson</li>
<li>for: 这个论文主要用于探讨如何将神经网络学习推广到计算流体动力学（CFD）领域，以提高计算效率。</li>
<li>methods: 论文使用了多层感知器（MLP）和长短期记忆（LSTM）循环网络架构来表示下层物理效应，如湍流。</li>
<li>results: 实验结果显示，使用roseNNa库可以提高神经网络推广的效率，并且在使用PyTorch和libtorch时Remove overhead cost of API calls后，roseNNa仍然能够具有较高的速度。速度提高范围为约10倍到2倍。<details>
<summary>Abstract</summary>
The rise of neural network-based machine learning ushered in high-level libraries, including TensorFlow and PyTorch, to support their functionality. Computational fluid dynamics (CFD) researchers have benefited from this trend and produced powerful neural networks that promise shorter simulation times. For example, multilayer perceptrons (MLPs) and Long Short Term Memory (LSTM) recurrent-based (RNN) architectures can represent sub-grid physical effects, like turbulence. Implementing neural networks in CFD solvers is challenging because the programming languages used for machine learning and CFD are mostly non-overlapping, We present the roseNNa library, which bridges the gap between neural network inference and CFD. RoseNNa is a non-invasive, lightweight (1000 lines), and performant tool for neural network inference, with focus on the smaller networks used to augment PDE solvers, like those of CFD, which are typically written in C/C++ or Fortran. RoseNNa accomplishes this by automatically converting trained models from typical neural network training packages into a high-performance Fortran library with C and Fortran APIs. This reduces the effort needed to access trained neural networks and maintains performance in the PDE solvers that CFD researchers build and rely upon. Results show that RoseNNa reliably outperforms PyTorch (Python) and libtorch (C++) on MLPs and LSTM RNNs with less than 100 hidden layers and 100 neurons per layer, even after removing the overhead cost of API calls. Speedups range from a factor of about 10 and 2 faster than these established libraries for the smaller and larger ends of the neural network size ranges tested.
</details>
<details>
<summary>摘要</summary>
“从神经网络学习的出现开始，高水平的库，包括TensorFlow和PyTorch，支援其功能。 Computational fluid dynamics（CFD）研究人员受惠于这一趋势，生成了具有短化运算时间的强大神经网络。例如，多层感知器（MLP）和长期记忆运算（LSTM）类型的神经网络可以表现出裂隔物理效应，如湍流。将神经网络 integrate into CFD solvers 是一个挑战，因为机器学习和CFD的程式语言通常不 overlap。我们介绍了roseNNa库，它横跨神经网络评估和CFD的关系，并提供了一个非侵入性、轻量级（1000行）、高性能的工具。roseNNa 可以自动将训练好的模型从通常的神经网络训练套件转换为高性能的 Fortran 库，并提供了 C 和 Fortran API。这样可以缩短训练好的神经网络存取的努力，并维持 PDE 解析器中的性能。结果显示，roseNNa 可以高效地处理 MLP 和 LSTM RNN，即使将 API 调用的开关时间除掉。速度比较为约10倍和2倍于 Python 的 PyTorch 和 C++ 的 libtorch，对于较小的和较大的神经网络size range。”
</details></li>
</ul>
<hr>
<h2 id="Towards-Practical-Robustness-Auditing-for-Linear-Regression"><a href="#Towards-Practical-Robustness-Auditing-for-Linear-Regression" class="headerlink" title="Towards Practical Robustness Auditing for Linear Regression"></a>Towards Practical Robustness Auditing for Linear Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16315">http://arxiv.org/abs/2307.16315</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Freund, Samuel B. Hopkins</li>
<li>for: 这 paper 的目的是找到或证明一个 dataset 中小 subsets 的存在或不存在，这些 subsets 的 removing 可以使 ordinary least squares regression 的 coefficient 的符号变化。</li>
<li>methods: 这 paper 使用了一些已知的算法技术来实现这个目的，包括 mixed integer quadratically constrained optimization 和 exact greedy methods，这些方法在一些特定的情况下能够大幅提高性能。</li>
<li>results: 这 paper 的实验结果表明，这些方法在一些维度小的情况下能够提供有用的 robustness check，但是在维度大于 $3$ 的情况下，还存在 significiant computational bottlenecks，spectral algorithm 使用了来自最近的算法robust statistics 的想法，以解决这个挑战。<details>
<summary>Abstract</summary>
We investigate practical algorithms to find or disprove the existence of small subsets of a dataset which, when removed, reverse the sign of a coefficient in an ordinary least squares regression involving that dataset. We empirically study the performance of well-established algorithmic techniques for this task -- mixed integer quadratically constrained optimization for general linear regression problems and exact greedy methods for special cases. We show that these methods largely outperform the state of the art and provide a useful robustness check for regression problems in a few dimensions. However, significant computational bottlenecks remain, especially for the important task of disproving the existence of such small sets of influential samples for regression problems of dimension $3$ or greater. We make some headway on this challenge via a spectral algorithm using ideas drawn from recent innovations in algorithmic robust statistics. We summarize the limitations of known techniques in several challenge datasets to encourage further algorithmic innovation.
</details>
<details>
<summary>摘要</summary>
我团队 investigate 实用算法来找到或证明一个小集合的存在，当其从 dataset 中移除时，抵消了一个减法最小二乘问题中的 coefficient 的符号。我们对已有的算法技术进行实验研究——混合整数二次约束优化算法和特殊情况下的准确追加法。我们发现这些方法在大多数情况下表现出色，提供了有用的稳定性检查。但是，特别是在维度大于 3 的 regression 问题中，计算瓶颈仍然很大，特别是用于证明小集合的影响样本的存在。我们通过使用最近的算法robust统计技术中的spectral algorithm来做出一些进展。我们summarize 已知技术的局限性，以便鼓励更多的算法创新。
</details></li>
</ul>
<hr>
<h2 id="Mask-guided-Data-Augmentation-for-Multiparametric-MRI-Generation-with-a-Rare-Hepatocellular-Carcinoma"><a href="#Mask-guided-Data-Augmentation-for-Multiparametric-MRI-Generation-with-a-Rare-Hepatocellular-Carcinoma" class="headerlink" title="Mask-guided Data Augmentation for Multiparametric MRI Generation with a Rare Hepatocellular Carcinoma"></a>Mask-guided Data Augmentation for Multiparametric MRI Generation with a Rare Hepatocellular Carcinoma</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16314">http://arxiv.org/abs/2307.16314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karen Sanchez, Carlos Hinojosa, Kevin Arias, Henry Arguello, Denis Kouame, Olivier Meyrignac, Adrian Basarab</li>
<li>for: 这个论文主要用于提高深度学习模型的总体性能，特别是在医疗应用中 multiparametric 数据集上。</li>
<li>methods: 该论文提出了一种新的数据扩展建议，通过一种生成深度学习方法生成多 paramagnetic（T1 血管、T1 门户和 T2）磁共振成像（MRI）图像，以模拟大规模某些肝癌细胞型。</li>
<li>results: 该论文通过对限制的 multiparametric MRI triplets 进行训练，生成了 1,000 个人工数据集和其对应的肝肿瘤面积。 Frechet Inception Distance 分数为 86.55。这种方法在 2021 年法国医学会数据扩展挑战中荣膺奖。<details>
<summary>Abstract</summary>
Data augmentation is classically used to improve the overall performance of deep learning models. It is, however, challenging in the case of medical applications, and in particular for multiparametric datasets. For example, traditional geometric transformations used in several applications to generate synthetic images can modify in a non-realistic manner the patients' anatomy. Therefore, dedicated image generation techniques are necessary in the medical field to, for example, mimic a given pathology realistically. This paper introduces a new data augmentation architecture that generates synthetic multiparametric (T1 arterial, T1 portal, and T2) magnetic resonance images (MRI) of massive macrotrabecular subtype hepatocellular carcinoma with their corresponding tumor masks through a generative deep learning approach. The proposed architecture creates liver tumor masks and abdominal edges used as input in a Pix2Pix network for synthetic data creation. The method's efficiency is demonstrated by training it on a limited multiparametric dataset of MRI triplets from $89$ patients with liver lesions to generate $1,000$ synthetic triplets and their corresponding liver tumor masks. The resulting Frechet Inception Distance score was $86.55$. The proposed approach was among the winners of the 2021 data augmentation challenge organized by the French Society of Radiology.
</details>
<details>
<summary>摘要</summary>
“数据扩充是通用深度学习模型的改进工具，但在医疗应用中具有挑战性，特别是在多 Parametric 数据集上。例如，传统的几何变换在许多应用中生成synthetic图像，可能会改变病人的解剖结构不实际。因此，医疗领域需要专门的图像生成技术，例如模拟给定疾病的真实方式。这篇文章介绍了一种新的数据扩充架构，通过生成多 Parametric（T1血管、T1门户和T2）核磁共振成像（MRI）的大规模肝癌细胞杂合型肝癌，并生成其相应的肿瘤面罩。提posed架构使用生成深度学习方法创建肝肿瘤面罩和腹部边缘，并将其作为Pix2Pix网络的输入进行 sintetic数据创建。方法的效率被证明通过对限制 multiparametric 数据集的 MRI  triplets（89名患者的肝脏病变）进行训练，生成 1,000 个 sintetic triplets和其相应的肝肿瘤面罩。得到的 Frechet Inception Distance 分数为 86.55。该方法在2021年的数据扩充挑战中获得了法国 radiology 学会的奖励。”
</details></li>
</ul>
<hr>
<h2 id="You-Shall-not-Pass-the-Zero-Gradient-Problem-in-Predict-and-Optimize-for-Convex-Optimization"><a href="#You-Shall-not-Pass-the-Zero-Gradient-Problem-in-Predict-and-Optimize-for-Convex-Optimization" class="headerlink" title="You Shall not Pass: the Zero-Gradient Problem in Predict and Optimize for Convex Optimization"></a>You Shall not Pass: the Zero-Gradient Problem in Predict and Optimize for Convex Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16304">http://arxiv.org/abs/2307.16304</a></li>
<li>repo_url: None</li>
<li>paper_authors: Grigorii Veviurko, Wendelin Böhmer, Mathijs de Weerdt</li>
<li>for: 预测和优化是一种越来越流行的决策方法，使用机器学习预测未知优化问题的参数。</li>
<li>methods: 这篇文章提出了一种新的方法，基于任务性能作为损失函数来训练预测模型。</li>
<li>results: 文章发现了一个未注意的缺陷（零梯度问题），并提出了一种解决方法，基于优化问题的数学性质。 two real-world benchmarks 被验证。<details>
<summary>Abstract</summary>
Predict and optimize is an increasingly popular decision-making paradigm that employs machine learning to predict unknown parameters of optimization problems. Instead of minimizing the prediction error of the parameters, it trains predictive models using task performance as a loss function. In the convex optimization domain, predict and optimize has seen significant progress due to recently developed methods for differentiating optimization problem solutions over the problem parameters. This paper identifies a yet unnoticed drawback of this approach -- the zero-gradient problem -- and introduces a method to solve it. The suggested method is based on the mathematical properties of differential optimization and is verified using two real-world benchmarks.
</details>
<details>
<summary>摘要</summary>
预测和优化是一种日益受欢迎的决策方法，它使用机器学习预测未知优化问题中的参数。而不是将预测参数的预测错误作为损失函数来训练预测模型，它使用任务性能作为损失函数来训练预测模型。在凸优化领域，预测和优化已经取得了显著进展，这主要归功于最近发展出的对优化问题解的方法。然而，这种方法还存在一个尚未受到注意的缺点——零梯度问题。本文描述了这个问题，并提出了一种基于数学优化的解决方法。该方法在两个实际 benchmark 中被验证。
</details></li>
</ul>
<hr>
<h2 id="Predicting-delays-in-Indian-lower-courts-using-AutoML-and-Decision-Forests"><a href="#Predicting-delays-in-Indian-lower-courts-using-AutoML-and-Decision-Forests" class="headerlink" title="Predicting delays in Indian lower courts using AutoML and Decision Forests"></a>Predicting delays in Indian lower courts using AutoML and Decision Forests</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16285">http://arxiv.org/abs/2307.16285</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mb7419/pendencyprediction">https://github.com/mb7419/pendencyprediction</a></li>
<li>paper_authors: Mohit Bhatnagar, Shivraj Huchhanavar</li>
<li>for: 这个论文旨在预测印度下级法院延迟的时间，基于提交案例信息。</li>
<li>methods: 作者使用AutoML开发了一个多类分类模型，使用10年的案例结果和420万个法律案件的数据集进行训练。</li>
<li>results: 最佳模型的准确率为81.4%，precision、recall和F1分别为0.81。研究证明了使用AI模型预测印度法院延迟的可能性，并提供了可用于进一步研究的数据集和Python代码文件。<details>
<summary>Abstract</summary>
This paper presents a classification model that predicts delays in Indian lower courts based on case information available at filing. The model is built on a dataset of 4.2 million court cases filed in 2010 and their outcomes over a 10-year period. The data set is drawn from 7000+ lower courts in India. The authors employed AutoML to develop a multi-class classification model over all periods of pendency and then used binary decision forest classifiers to improve predictive accuracy for the classification of delays. The best model achieved an accuracy of 81.4%, and the precision, recall, and F1 were found to be 0.81. The study demonstrates the feasibility of AI models for predicting delays in Indian courts, based on relevant data points such as jurisdiction, court, judge, subject, and the parties involved. The paper also discusses the results in light of relevant literature and suggests areas for improvement and future research. The authors have made the dataset and Python code files used for the analysis available for further research in the crucial and contemporary field of Indian judicial reform.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="zkDL-Efficient-Zero-Knowledge-Proofs-of-Deep-Learning-Training"><a href="#zkDL-Efficient-Zero-Knowledge-Proofs-of-Deep-Learning-Training" class="headerlink" title="zkDL: Efficient Zero-Knowledge Proofs of Deep Learning Training"></a>zkDL: Efficient Zero-Knowledge Proofs of Deep Learning Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16273">http://arxiv.org/abs/2307.16273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haochen Sun, Hongyang Zhang</li>
<li>for: 防止不可信任的AI开发者侵犯知识产权，保护数据和模型参数的隐私。</li>
<li>methods: 使用特殊的零知识证明协议zkReLU，并将其集成到整个训练过程的证明系统中，以实现快速和有效的隐私保护。</li>
<li>results: 使用zkDL可以在一分钟内，对16层神经网络的200M参数进行完整和准确的证明，并且proof size仅20kB，比 tradicional方法减少了许多。<details>
<summary>Abstract</summary>
The recent advancements in deep learning have brought about significant changes in various aspects of people's lives. Meanwhile, these rapid developments have raised concerns about the legitimacy of the training process of deep networks. However, to protect the intellectual properties of untrusted AI developers, directly examining the training process by accessing the model parameters and training data by verifiers is often prohibited.   In response to this challenge, we present zkDL, an efficient zero-knowledge proof of deep learning training. At the core of zkDL is zkReLU, a specialized zero-knowledge proof protocol with optimized proving time and proof size for the ReLU activation function, a major obstacle in verifiable training due to its non-arithmetic nature. To integrate zkReLU into the proof system for the entire training process, we devise a novel construction of an arithmetic circuit from neural networks. By leveraging the abundant parallel computation resources, this construction reduces proving time and proof sizes by a factor of the network depth. As a result, zkDL enables the generation of complete and sound proofs, taking less than a minute with a size of less than 20 kB per training step, for a 16-layer neural network with 200M parameters, while ensuring the privacy of data and model parameters.
</details>
<details>
<summary>摘要</summary>
The core of zkDL is zkReLU, a specialized zero-knowledge proof protocol with optimized proving time and proof size for the ReLU activation function, which has been a major obstacle in verifiable training due to its non-arithmetic nature. To integrate zkReLU into the proof system for the entire training process, we have devised a novel construction of an arithmetic circuit from neural networks. By leveraging the abundant parallel computation resources, this construction reduces proving time and proof sizes by a factor of the network depth.With zkDL, we can generate complete and sound proofs in less than a minute, with a size of less than 20 kB per training step, for a 16-layer neural network with 200M parameters, while ensuring the privacy of data and model parameters. This efficient zero-knowledge proof system enables the protection of intellectual properties in deep learning, and paves the way for more transparent and trustworthy AI development.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/31/cs.LG_2023_07_31/" data-id="clm0t8e090064v7889kgdbsx7" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/01/eess.IV_2023_08_01/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.IV - 2023-08-01 17:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/31/cs.SD_2023_07_31/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-07-31 123:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">60</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
