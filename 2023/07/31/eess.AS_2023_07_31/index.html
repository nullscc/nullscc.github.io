
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.AS - 2023-07-31 22:00:00 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Multilingual context-based pronunciation learning for Text-to-Speech paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.16709 repo_url: None paper_authors: Giulia Comini, Manuel Sam Ribeiro, Fan Yang, Heereen Shim,">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.AS - 2023-07-31 22:00:00">
<meta property="og:url" content="http://example.com/2023/07/31/eess.AS_2023_07_31/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Multilingual context-based pronunciation learning for Text-to-Speech paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.16709 repo_url: None paper_authors: Giulia Comini, Manuel Sam Ribeiro, Fan Yang, Heereen Shim,">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-31T00:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:41.621Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.AS_2023_07_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/31/eess.AS_2023_07_31/" class="article-date">
  <time datetime="2023-07-31T00:00:00.000Z" itemprop="datePublished">2023-07-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.AS - 2023-07-31 22:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Multilingual-context-based-pronunciation-learning-for-Text-to-Speech"><a href="#Multilingual-context-based-pronunciation-learning-for-Text-to-Speech" class="headerlink" title="Multilingual context-based pronunciation learning for Text-to-Speech"></a>Multilingual context-based pronunciation learning for Text-to-Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16709">http://arxiv.org/abs/2307.16709</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giulia Comini, Manuel Sam Ribeiro, Fan Yang, Heereen Shim, Jaime Lorenzo-Trueba</li>
<li>for: 这个论文的目的是提出一个多语言统一的前端系统，用于解决语音识别相关的任务。</li>
<li>methods: 该论文使用的方法包括图书馆搜集和语音识别模型，以及一些语言特定的规则和约束。</li>
<li>results: 研究人员在这篇论文中提出了一个多语言统一的前端系统，并进行了对多种语言和任务的评估，发现该系统在不同语言和任务上具有竞争力。<details>
<summary>Abstract</summary>
Phonetic information and linguistic knowledge are an essential component of a Text-to-speech (TTS) front-end. Given a language, a lexicon can be collected offline and Grapheme-to-Phoneme (G2P) relationships are usually modeled in order to predict the pronunciation for out-of-vocabulary (OOV) words. Additionally, post-lexical phonology, often defined in the form of rule-based systems, is used to correct pronunciation within or between words. In this work we showcase a multilingual unified front-end system that addresses any pronunciation related task, typically handled by separate modules. We evaluate the proposed model on G2P conversion and other language-specific challenges, such as homograph and polyphones disambiguation, post-lexical rules and implicit diacritization. We find that the multilingual model is competitive across languages and tasks, however, some trade-offs exists when compared to equivalent monolingual solutions.
</details>
<details>
<summary>摘要</summary>
phonetic information和语言知识是文本读取（TTS）前端的重要组成部分。给定一种语言，一个词典可以在线收集，并且使用格拉姆-至-声音（G2P）关系预测未在词汇表中出现的词语的发音。此外，后 lexical phonology，通常通过规则系统表示，用于在或 между词语中 corrections 发音。在这种工作中，我们展示了一种多语言统一前端系统，可以处理任何发音相关的任务，通常由单独的模块处理。我们评估提议的模型在G2P转换和其他语言特有的挑战中，如同形词和多音字识别、后 lexical 规则和隐式 диакриت化。我们发现该多语言模型在语言和任务方面都具有竞争力，但有一些贸易offs existed 与等效的单语言解决方案相比。
</details></li>
</ul>
<hr>
<h2 id="Improving-grapheme-to-phoneme-conversion-by-learning-pronunciations-from-speech-recordings"><a href="#Improving-grapheme-to-phoneme-conversion-by-learning-pronunciations-from-speech-recordings" class="headerlink" title="Improving grapheme-to-phoneme conversion by learning pronunciations from speech recordings"></a>Improving grapheme-to-phoneme conversion by learning pronunciations from speech recordings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16643">http://arxiv.org/abs/2307.16643</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manuel Sam Ribeiro, Giulia Comini, Jaime Lorenzo-Trueba</li>
<li>for: 这 paper 的目的是提高 Grapheme-to-Phoneme (G2P) 任务的精度。</li>
<li>methods: 这 paper 使用了学习语音录制中的示例来改进 G2P 转换。</li>
<li>results: 实验结果表明，这种方法可以在不同语言和数据量不足的情况下 consistently 提高 G2P 系统的phone error rate。<details>
<summary>Abstract</summary>
The Grapheme-to-Phoneme (G2P) task aims to convert orthographic input into a discrete phonetic representation. G2P conversion is beneficial to various speech processing applications, such as text-to-speech and speech recognition. However, these tend to rely on manually-annotated pronunciation dictionaries, which are often time-consuming and costly to acquire. In this paper, we propose a method to improve the G2P conversion task by learning pronunciation examples from audio recordings. Our approach bootstraps a G2P with a small set of annotated examples. The G2P model is used to train a multilingual phone recognition system, which then decodes speech recordings with a phonetic representation. Given hypothesized phoneme labels, we learn pronunciation dictionaries for out-of-vocabulary words, and we use those to re-train the G2P system. Results indicate that our approach consistently improves the phone error rate of G2P systems across languages and amount of available data.
</details>
<details>
<summary>摘要</summary>
文本描述：grapheme-to-phoneme（G2P）任务的目标是将文字输入转换为精确的声音表示。G2P转换对于各种语音处理应用程序非常有利，如文本读取和语音识别。然而，这些应用程序通常需要手动编制的发音词典，这可能会耗费很多时间和成本。在这篇论文中，我们提出了一种方法，可以通过学习音频记录中的发音示例来改进G2P转换任务。我们的方法从一小 subsets of annotated examples开始，使用G2P模型来训练多语言的 телеphone recognition系统。给出的假设发音标签，我们可以学习出现在词汇表外的发音词典，并使用这些词典来重新训练G2P系统。结果表明，我们的方法可以在不同语言和数据量的情况下一直提高G2P系统的声音错误率。
</details></li>
</ul>
<hr>
<h2 id="All-In-One-Metrical-And-Functional-Structure-Analysis-With-Neighborhood-Attentions-on-Demixed-Audio"><a href="#All-In-One-Metrical-And-Functional-Structure-Analysis-With-Neighborhood-Attentions-on-Demixed-Audio" class="headerlink" title="All-In-One Metrical And Functional Structure Analysis With Neighborhood Attentions on Demixed Audio"></a>All-In-One Metrical And Functional Structure Analysis With Neighborhood Attentions on Demixed Audio</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16425">http://arxiv.org/abs/2307.16425</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mir-aidj/all-in-one">https://github.com/mir-aidj/all-in-one</a></li>
<li>paper_authors: Taejun Kim, Juhan Nam</li>
<li>for: 这篇论文的目的是提出一种可靠的音乐信息检索（MIR）模型，以捕捉音乐的复杂层次结构。</li>
<li>methods: 该模型使用分辨率增强的听spectrogram作为输入，并使用扩展 neighboorhood attention机制来捕捉音乐的长期时间关系，以及非扩展的attention机制来捕捉乐器的本地关系。</li>
<li>results: 根据 Harmonix Set 数据集的测试结果，该模型可以同时高效地完成 beat 跟踪、downbeat 跟踪、功能结构分割和标注等四个任务，而且相比最新的状态前景模型，该模型具有较低的参数数量。此外，我们的减少学习研究表明，同时学习 beat、downbeat 和段落可以导致提高表现，每个任务彼此互助。<details>
<summary>Abstract</summary>
Music is characterized by complex hierarchical structures. Developing a comprehensive model to capture these structures has been a significant challenge in the field of Music Information Retrieval (MIR). Prior research has mainly focused on addressing individual tasks for specific hierarchical levels, rather than providing a unified approach. In this paper, we introduce a versatile, all-in-one model that jointly performs beat and downbeat tracking as well as functional structure segmentation and labeling. The model leverages source-separated spectrograms as inputs and employs dilated neighborhood attentions to capture temporal long-term dependencies, along with non-dilated attentions for local instrumental dependencies. Consequently, the proposed model achieves state-of-the-art performance in all four tasks on the Harmonix Set while maintaining a relatively lower number of parameters compared to recent state-of-the-art models. Furthermore, our ablation study demonstrates that the concurrent learning of beats, downbeats, and segments can lead to enhanced performance, with each task mutually benefiting from the others.
</details>
<details>
<summary>摘要</summary>
音乐具有复杂的层次结构，开发一个完整的模型以捕捉这些结构是音乐信息检索（MIR）领域的主要挑战。先前的研究主要集中在解决特定层次级别的任务上，而不是提供一个统一的方法。在本文中，我们提出了一个通用的、一体化模型，同时进行了 Beat 和下 Beat 追踪、功能结构分 segmentation 和标注。该模型使用分离后的spectrogram作为输入，并使用扩展 neighborhood 的注意力来捕捉时间长期依赖关系，以及非扩展的注意力来捕捉本地乐器依赖关系。因此，我们提出的模型在 Harmonix 集上实现了状态太的表现在四个任务中，同时具有相对较低的参数数量 compared to 最新的状态太模型。此外，我们的ablation 研究表明，同时学习 Beat、下 Beat 和分 segmentation 可以导致提高表现，每个任务受到另外两个任务的帮助。
</details></li>
</ul>
<hr>
<h2 id="Robust-Self-Supervised-Speech-Embeddings-for-Child-Adult-Classification-in-Interactions-involving-Children-with-Autism"><a href="#Robust-Self-Supervised-Speech-Embeddings-for-Child-Adult-Classification-in-Interactions-involving-Children-with-Autism" class="headerlink" title="Robust Self Supervised Speech Embeddings for Child-Adult Classification in Interactions involving Children with Autism"></a>Robust Self Supervised Speech Embeddings for Child-Adult Classification in Interactions involving Children with Autism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16398">http://arxiv.org/abs/2307.16398</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rimita Lahiri, Tiantian Feng, Rajat Hebbar, Catherine Lord, So Hyun Kim, Shrikanth Narayanan</li>
<li>for: 本研究旨在探讨听取儿童与成人之间的语音交流中的speaker classification问题，以便自动地确定听取的speaker是哪个人。</li>
<li>methods: 本研究使用了两种最新的自我超vision算法：Wav2vec 2.0和WavLM，并采用了对比性损失函数来预训练我们的模型。</li>
<li>results: 本研究在两个临床交流数据集中获得了9-13%的相对改进，与现有基eline相比，以达到更高的分类F1分数。此外，我们还分析了不同子群体的听取情况，并评估了不同的预训练条件下的模型性能。<details>
<summary>Abstract</summary>
We address the problem of detecting who spoke when in child-inclusive spoken interactions i.e., automatic child-adult speaker classification. Interactions involving children are richly heterogeneous due to developmental differences. The presence of neurodiversity e.g., due to Autism, contributes additional variability. We investigate the impact of additional pre-training with more unlabelled child speech on the child-adult classification performance. We pre-train our model with child-inclusive interactions, following two recent self-supervision algorithms, Wav2vec 2.0 and WavLM, with a contrastive loss objective. We report 9 - 13% relative improvement over the state-of-the-art baseline with regards to classification F1 scores on two clinical interaction datasets involving children with Autism. We also analyze the impact of pre-training under different conditions by evaluating our model on interactions involving different subgroups of children based on various demographic factors.
</details>
<details>
<summary>摘要</summary>
我们 Addresses 了儿童包含的语音交流自动听众识别问题（即儿童成人说话分类）。由于儿童的发展差异，这些交流具有丰富的多样性。另外，由于自闭症等神经多样性，会添加更多的不确定性。我们研究额外预训练使用更多的无标签儿童语音后，对儿童成人分类性能产生了9-13%的相对改进。我们使用了两种最新的自我超视觉算法（Wav2vec 2.0和WavLM），采用了对比损失目标函数进行预训练。我们在两个临床交流数据集中对儿童自闭症进行了分类，并分析了不同子群体儿童的影响。
</details></li>
</ul>
<hr>
<h2 id="Pre-training-End-to-end-ASR-Models-with-Augmented-Speech-Samples-Queried-by-Text"><a href="#Pre-training-End-to-end-ASR-Models-with-Augmented-Speech-Samples-Queried-by-Text" class="headerlink" title="Pre-training End-to-end ASR Models with Augmented Speech Samples Queried by Text"></a>Pre-training End-to-end ASR Models with Augmented Speech Samples Queried by Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16332">http://arxiv.org/abs/2307.16332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eric Sun, Jinyu Li, Jian Xue, Yifan Gong</li>
<li>for: 提高语言扩展 END-to-END 自动语音识别系统的困难，增加语音数据的做准备。</li>
<li>methods: 提出一种新的方法，通过将无对应语音数据和文本数据混合生成增强样本，实现低成本的语音数据增强，不需要额外的语音数据。</li>
<li>results: 结果表明，将20,000小时增强样本与12,500小时原始译文数据混合，可以实现8.7%的字词错误率降低，与使用多语言原始75,000小时Raw speech数据预训练的模型相当。同时，将增强样本与多语言数据混合预训练新模型，可以实现12.2%的字词错误率降低，进一步证明了我们的方法的有效性。<details>
<summary>Abstract</summary>
In end-to-end automatic speech recognition system, one of the difficulties for language expansion is the limited paired speech and text training data. In this paper, we propose a novel method to generate augmented samples with unpaired speech feature segments and text data for model pre-training, which has the advantage of low cost without using additional speech data. When mixing 20,000 hours augmented speech data generated by our method with 12,500 hours original transcribed speech data for Italian Transformer transducer model pre-training, we achieve 8.7% relative word error rate reduction. The pre-trained model achieves similar performance as the model pre-trained with multilingual transcribed 75,000 hours raw speech data. When merging the augmented speech data with the multilingual data to pre-train a new model, we achieve even more relative word error rate reduction of 12.2% over the baseline, which further verifies the effectiveness of our method for speech data augmentation.
</details>
<details>
<summary>摘要</summary>
在全自动语音识别系统中，扩展语言的一个Difficulty是有限的对话语音和文本训练数据。在这篇论文中，我们提出了一种新的方法，用于生成增强样本，并使用无对应的语音特征段和文本数据进行模型预训练，这有优点，不需要额外的语音数据。当混合我们生成的20,000小时增强语音数据和12,500小时原始转录的语音数据进行意大 transformer 抽取器模型预训练，我们实现了8.7%的Relative Word Error Rate（RWER）减少。预训练模型与使用多语言 raw 语音数据75,000小时预训练的模型之间具有相似的性能。当把增强语音数据与多语言数据混合预训练一新模型时，我们实现了更高的RWER减少，达12.2%，这进一步证明了我们的方法对语音数据增强的有效性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/07/31/eess.AS_2023_07_31/" data-id="cllt9prxc006iol88go8h346l" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/31/cs.SD_2023_07_31/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.SD - 2023-07-31 123:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/31/eess.IV_2023_07_31/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-07-31 17:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
