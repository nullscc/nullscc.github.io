
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-07-31 17:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Hybrid quantum transfer learning for crack image classification on NISQ hardware paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.16723 repo_url: None paper_authors: Alexander Geng, Ali Moghiseh, Claudia Redenbac">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-07-31 17:00:00">
<meta property="og:url" content="http://nullscc.github.io/2023/07/31/eess.IV_2023_07_31/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Hybrid quantum transfer learning for crack image classification on NISQ hardware paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.16723 repo_url: None paper_authors: Alexander Geng, Ali Moghiseh, Claudia Redenbac">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-30T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:41.652Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_07_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/31/eess.IV_2023_07_31/" class="article-date">
  <time datetime="2023-07-30T16:00:00.000Z" itemprop="datePublished">2023-07-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-07-31 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Hybrid-quantum-transfer-learning-for-crack-image-classification-on-NISQ-hardware"><a href="#Hybrid-quantum-transfer-learning-for-crack-image-classification-on-NISQ-hardware" class="headerlink" title="Hybrid quantum transfer learning for crack image classification on NISQ hardware"></a>Hybrid quantum transfer learning for crack image classification on NISQ hardware</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16723">http://arxiv.org/abs/2307.16723</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Geng, Ali Moghiseh, Claudia Redenbach, Katja Schladitz</li>
<li>for: 该论文旨在探讨量子计算机可以在减少量子比特数量的情况下处理数据的可能性，以及对现代量子机器学习算法的应用。</li>
<li>methods: 该论文使用了变量量子机器学习算法，并对量子传输学习进行了应用，用于检测灰度图像中的裂隙。</li>
<li>results: 研究发现，使用量子传输学习可以提高图像检测的性能和准确率，并且对于小型图像，量子传输学习的训练时间比标准量子机器学习算法更短。<details>
<summary>Abstract</summary>
Quantum computers possess the potential to process data using a remarkably reduced number of qubits compared to conventional bits, as per theoretical foundations. However, recent experiments have indicated that the practical feasibility of retrieving an image from its quantum encoded version is currently limited to very small image sizes. Despite this constraint, variational quantum machine learning algorithms can still be employed in the current noisy intermediate scale quantum (NISQ) era. An example is a hybrid quantum machine learning approach for edge detection. In our study, we present an application of quantum transfer learning for detecting cracks in gray value images. We compare the performance and training time of PennyLane's standard qubits with IBM's qasm\_simulator and real backends, offering insights into their execution efficiency.
</details>
<details>
<summary>摘要</summary>
量子计算机具有可能处理数据使用非常减少的量子比特数量，根据理论基础。然而，现实实验表明，现在可以从量子编码版本中恢复图像的实际可行性受限于非常小的图像大小。尽管存在这些限制，量子机器学习算法仍可以在当今的半古矿量化（NISQ）时代应用。我们的研究中描述了使用量子传输学习检测灰度图像中的裂隙。我们比较了彭尼lane的标准量子比特与IBM的qasm_simulator和真实后端的执行效率。这些比较提供了关于其执行效率的新的视角。
</details></li>
</ul>
<hr>
<h2 id="Conditioning-Generative-Latent-Optimization-to-solve-Imaging-Inverse-Problems"><a href="#Conditioning-Generative-Latent-Optimization-to-solve-Imaging-Inverse-Problems" class="headerlink" title="Conditioning Generative Latent Optimization to solve Imaging Inverse Problems"></a>Conditioning Generative Latent Optimization to solve Imaging Inverse Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16670">http://arxiv.org/abs/2307.16670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Braure, Kévin Ginsburger</li>
<li>for: 解决医学影像重构问题，提高影像重构质量</li>
<li>methods: 使用 score-based 生成模型，不需要大量的超参数和backward操作</li>
<li>results: 在缺省视图 CT 中，比起现有的得分基础方法，提高了重构质量，并且可以扩展到非线性 IIP 问题<details>
<summary>Abstract</summary>
Computed Tomography (CT) is a prominent example of Imaging Inverse Problem (IIP), highlighting the unrivalled performances of data-driven methods in degraded measurements setups like sparse X-ray projections. Although a significant proportion of deep learning approaches benefit from large supervised datasets to directly map experimental measurements to medical scans, they cannot generalize to unknown acquisition setups. In contrast, fully unsupervised techniques, most notably using score-based generative models, have recently demonstrated similar or better performances compared to supervised approaches to solve IIPs while being flexible at test time regarding the imaging setup. However, their use cases are limited by two factors: (a) they need considerable amounts of training data to have good generalization properties and (b) they require a backward operator, like Filtered-Back-Projection in the case of CT, to condition the learned prior distribution of medical scans to experimental measurements. To overcome these issues, we propose an unsupervised conditional approach to the Generative Latent Optimization framework (cGLO), in which the parameters of a decoder network are initialized on an unsupervised dataset. The decoder is then used for reconstruction purposes, by performing Generative Latent Optimization with a loss function directly comparing simulated measurements from proposed reconstructions to experimental measurements. The resulting approach, tested on sparse-view CT using multiple training dataset sizes, demonstrates better reconstruction quality compared to state-of-the-art score-based strategies in most data regimes and shows an increasing performance advantage for smaller training datasets and reduced projection angles. Furthermore, cGLO does not require any backward operator and could expand use cases even to non-linear IIPs.
</details>
<details>
<summary>摘要</summary>
计算Tomography（CT）是一个典型的成像反问题（IIP）示例， highlighting the unrivalled performances of data-driven methods in degraded measurements setups like sparse X-ray projections. Although a significant proportion of deep learning approaches benefit from large supervised datasets to directly map experimental measurements to medical scans, they cannot generalize to unknown acquisition setups. In contrast, fully unsupervised techniques, most notably using score-based generative models, have recently demonstrated similar or better performances compared to supervised approaches to solve IIPs while being flexible at test time regarding the imaging setup. However, their use cases are limited by two factors: (a) they need considerable amounts of training data to have good generalization properties and (b) they require a backward operator, like Filtered-Back-Projection in the case of CT, to condition the learned prior distribution of medical scans to experimental measurements. To overcome these issues, we propose an unsupervised conditional approach to the Generative Latent Optimization framework (cGLO), in which the parameters of a decoder network are initialized on an unsupervised dataset. The decoder is then used for reconstruction purposes, by performing Generative Latent Optimization with a loss function directly comparing simulated measurements from proposed reconstructions to experimental measurements. The resulting approach, tested on sparse-view CT using multiple training dataset sizes, demonstrates better reconstruction quality compared to state-of-the-art score-based strategies in most data regimes and shows an increasing performance advantage for smaller training datasets and reduced projection angles. Furthermore, cGLO does not require any backward operator and could expand use cases even to non-linear IIPs.
</details></li>
</ul>
<hr>
<h2 id="Towards-General-Low-Light-Raw-Noise-Synthesis-and-Modeling"><a href="#Towards-General-Low-Light-Raw-Noise-Synthesis-and-Modeling" class="headerlink" title="Towards General Low-Light Raw Noise Synthesis and Modeling"></a>Towards General Low-Light Raw Noise Synthesis and Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16508">http://arxiv.org/abs/2307.16508</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fengzhang427/LRD">https://github.com/fengzhang427/LRD</a></li>
<li>paper_authors: Feng Zhang, Bin Xu, Zhiqiang Li, Xinran Liu, Qingbo Lu, Changxin Gao, Nong Sang</li>
<li>for:  This paper aims to address the problem of modeling and synthesizing low-light raw noise in computational photography and image processing applications.</li>
<li>methods:  The proposed method uses a generative model to synthesize signal-independent noise, and a physics- and learning-based manner to synthesize signal-dependent and signal-independent noise. Additionally, an effective multi-scale discriminator called Fourier transformer discriminator (FTD) is used to distinguish the noise distribution accurately.</li>
<li>results:  The proposed method can generate highly similar noise distribution to real noise, and performs favorably against state-of-the-art methods on different sensors.Here is the summary in Simplified Chinese:</li>
<li>for: 这篇论文目的是解决计算 fotografraphy 和图像处理应用中的低光照 raw 噪声模型化和生成问题。</li>
<li>methods: 提议的方法使用生成模型来生成信号独立噪声，并在物理和学习基础上同时生成信号依赖和独立噪声。此外，使用高级多核 scale discriminator（FTD）来准确地 отличи出噪声分布。</li>
<li>results: 提议的方法可以生成高度类似于实际噪声的噪声分布，并在不同感知器上与现状方法进行比较。<details>
<summary>Abstract</summary>
Modeling and synthesizing low-light raw noise is a fundamental problem for computational photography and image processing applications. Although most recent works have adopted physics-based models to synthesize noise, the signal-independent noise in low-light conditions is far more complicated and varies dramatically across camera sensors, which is beyond the description of these models. To address this issue, we introduce a new perspective to synthesize the signal-independent noise by a generative model. Specifically, we synthesize the signal-dependent and signal-independent noise in a physics- and learning-based manner, respectively. In this way, our method can be considered as a general model, that is, it can simultaneously learn different noise characteristics for different ISO levels and generalize to various sensors. Subsequently, we present an effective multi-scale discriminator termed Fourier transformer discriminator (FTD) to distinguish the noise distribution accurately. Additionally, we collect a new low-light raw denoising (LRD) dataset for training and benchmarking. Qualitative validation shows that the noise generated by our proposed noise model can be highly similar to the real noise in terms of distribution. Furthermore, extensive denoising experiments demonstrate that our method performs favorably against state-of-the-art methods on different sensors.
</details>
<details>
<summary>摘要</summary>
模型和生成低光照Raw杂变是计算摄影和图像处理应用的基本问题。 Although most recent works have adopted physics-based models to synthesize noise, the signal-independent noise in low-light conditions is far more complicated and varies dramatically across camera sensors, which is beyond the description of these models. To address this issue, we introduce a new perspective to synthesize the signal-independent noise by a generative model. Specifically, we synthesize the signal-dependent and signal-independent noise in a physics- and learning-based manner, respectively. In this way, our method can be considered as a general model, that is, it can simultaneously learn different noise characteristics for different ISO levels and generalize to various sensors. Subsequently, we present an effective multi-scale discriminator termed Fourier transformer discriminator (FTD) to distinguish the noise distribution accurately. Additionally, we collect a new low-light raw denoising (LRD) dataset for training and benchmarking. Qualitative validation shows that the noise generated by our proposed noise model can be highly similar to the real noise in terms of distribution. Furthermore, extensive denoising experiments demonstrate that our method performs favorably against state-of-the-art methods on different sensors.Here's the translation in Traditional Chinese as well:模型和生成低光照Raw杂变是计算摄影和图像处理应用的基本问题。 Although most recent works have adopted physics-based models to synthesize noise, the signal-independent noise in low-light conditions is far more complicated and varies dramatically across camera sensors, which is beyond the description of these models. To address this issue, we introduce a new perspective to synthesize the signal-independent noise by a generative model. Specifically, we synthesize the signal-dependent and signal-independent noise in a physics- and learning-based manner, respectively. In this way, our method can be considered as a general model, that is, it can simultaneously learn different noise characteristics for different ISO levels and generalize to various sensors. Subsequently, we present an effective multi-scale discriminator termed Fourier transformer discriminator (FTD) to distinguish the noise distribution accurately. Additionally, we collect a new low-light raw denoising (LRD) dataset for training and benchmarking. Qualitative validation shows that the noise generated by our proposed noise model can be highly similar to the real noise in terms of distribution. Furthermore, extensive denoising experiments demonstrate that our method performs favorably against state-of-the-art methods on different sensors.
</details></li>
</ul>
<hr>
<h2 id="A-hybrid-approach-for-improving-U-Net-variants-in-medical-image-segmentation"><a href="#A-hybrid-approach-for-improving-U-Net-variants-in-medical-image-segmentation" class="headerlink" title="A hybrid approach for improving U-Net variants in medical image segmentation"></a>A hybrid approach for improving U-Net variants in medical image segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16462">http://arxiv.org/abs/2307.16462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aitik Gupta, Dr. Joydip Dhar</li>
<li>for: 这个研究旨在降低预测网络参数需求，保持适用于一些医疗影像分类任务，如皮肤淋巴瘤分类，通过深度分解卷积和注意力系统。</li>
<li>methods: 这个研究主要专注于审查现有的深度学习方法，包括MultiResUNet、Attention U-Net 和其他变化。这些方法使用注意力特征向量或地图对重要信息加以重要性评估，以提高精度。但是，这些网络参数较多，导致过拟合和测试时间较长。</li>
<li>results: 这个研究希望通过使用深度分解卷积和注意力系统，实现降低网络参数需求，并维持适用于一些医疗影像分类任务的性能。<details>
<summary>Abstract</summary>
Medical image segmentation is vital to the area of medical imaging because it enables professionals to more accurately examine and understand the information offered by different imaging modalities. The technique of splitting a medical image into various segments or regions of interest is known as medical image segmentation. The segmented images that are produced can be used for many different things, including diagnosis, surgery planning, and therapy evaluation.   In initial phase of research, major focus has been given to review existing deep-learning approaches, including researches like MultiResUNet, Attention U-Net, classical U-Net, and other variants. The attention feature vectors or maps dynamically add important weights to critical information, and most of these variants use these to increase accuracy, but the network parameter requirements are somewhat more stringent. They face certain problems such as overfitting, as their number of trainable parameters is very high, and so is their inference time.   Therefore, the aim of this research is to reduce the network parameter requirements using depthwise separable convolutions, while maintaining performance over some medical image segmentation tasks such as skin lesion segmentation using attention system and residual connections.
</details>
<details>
<summary>摘要</summary>
医疗图像分割是医疗图像领域的关键技术，它使医生和科学家可以更加准确地检查和理解不同的医疗图像模式中提供的信息。医疗图像分割的过程包括将医疗图像分成多个区域或区域兴趣。生成的分割图像可以用于诊断、手术规划和治疗评估等多种应用。在初期研究阶段，主要关注了现有的深度学习方法，包括MultiResUNet、Attention U-Net、 классиical U-Net 和其他变体。这些变体使用注意力特征向量或地图来 dynamically添加重要的权重，以提高准确性。然而，这些变体的网络参数需求较高，即使在训练和执行时间方面也存在一定的问题，如过拟合和计算时间过长。因此，本研究的目标是通过depthwise separable convolutions来降低网络参数需求，保持一定的性能水平，而且在一些医疗图像分割任务中，如皮肤病变分割使用注意力系统和 residual connections。
</details></li>
</ul>
<hr>
<h2 id="High-Dynamic-Range-Image-Reconstruction-via-Deep-Explicit-Polynomial-Curve-Estimation"><a href="#High-Dynamic-Range-Image-Reconstruction-via-Deep-Explicit-Polynomial-Curve-Estimation" class="headerlink" title="High Dynamic Range Image Reconstruction via Deep Explicit Polynomial Curve Estimation"></a>High Dynamic Range Image Reconstruction via Deep Explicit Polynomial Curve Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16426">http://arxiv.org/abs/2307.16426</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jqtangust/epce-hdr">https://github.com/jqtangust/epce-hdr</a></li>
<li>paper_authors: Jiaqi Tang, Xiaogang Xu, Sixing Hu, Ying-Cong Chen<br>for:  This paper aims to address the challenge of High Dynamic Range (HDR) reconstruction from Low Dynamic Range (LDR) images, which is limited by the narrow dynamic range of digital images and the diverse tone-mapping functions used in different imaging systems.methods:  The proposed method uses a learnable network to estimate the tone mapping function and its corresponding HDR image in one network. The tone mapping function is described by a polynomial, which is automatically adjusted according to the tone space of the LDR image.results:  The proposed method achieves State-of-the-Art (SOTA) performance under different tone-mapping functions and generalizes well to new datasets, including both synthetic and real images.<details>
<summary>Abstract</summary>
Due to limited camera capacities, digital images usually have a narrower dynamic illumination range than real-world scene radiance. To resolve this problem, High Dynamic Range (HDR) reconstruction is proposed to recover the dynamic range to better represent real-world scenes. However, due to different physical imaging parameters, the tone-mapping functions between images and real radiance are highly diverse, which makes HDR reconstruction extremely challenging. Existing solutions can not explicitly clarify a corresponding relationship between the tone-mapping function and the generated HDR image, but this relationship is vital when guiding the reconstruction of HDR images. To address this problem, we propose a method to explicitly estimate the tone mapping function and its corresponding HDR image in one network. Firstly, based on the characteristics of the tone mapping function, we construct a model by a polynomial to describe the trend of the tone curve. To fit this curve, we use a learnable network to estimate the coefficients of the polynomial. This curve will be automatically adjusted according to the tone space of the Low Dynamic Range (LDR) image, and reconstruct the real HDR image. Besides, since all current datasets do not provide the corresponding relationship between the tone mapping function and the LDR image, we construct a new dataset with both synthetic and real images. Extensive experiments show that our method generalizes well under different tone-mapping functions and achieves SOTA performance.
</details>
<details>
<summary>摘要</summary>
To address this problem, we propose a method to explicitly estimate the tone mapping function and its corresponding HDR image in one network. Firstly, based on the characteristics of the tone mapping function, we construct a model using a polynomial to describe the trend of the tone curve. To fit this curve, we use a learnable network to estimate the coefficients of the polynomial. This curve will be automatically adjusted according to the tone space of the Low Dynamic Range (LDR) image, and reconstruct the real HDR image.Moreover, since all current datasets do not provide the corresponding relationship between the tone mapping function and the LDR image, we construct a new dataset with both synthetic and real images. Extensive experiments show that our method generalizes well under different tone-mapping functions and achieves state-of-the-art (SOTA) performance.
</details></li>
</ul>
<hr>
<h2 id="DRAW-Defending-Camera-shooted-RAW-against-Image-Manipulation"><a href="#DRAW-Defending-Camera-shooted-RAW-against-Image-Manipulation" class="headerlink" title="DRAW: Defending Camera-shooted RAW against Image Manipulation"></a>DRAW: Defending Camera-shooted RAW against Image Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16418">http://arxiv.org/abs/2307.16418</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoxiao Hu, Qichao Ying, Zhenxing Qian, Sheng Li, Xinpeng Zhang</li>
<li>for: 防止图像被黑客修改和篡改</li>
<li>methods: 使用 Multi-frequency Partial Fusion Network (MPF-Net) 和隐藏水印技术保护原始 RAW 数据</li>
<li>results: 在多个知名 RAW 数据集上进行了广泛的实验，证明方法的有效性<details>
<summary>Abstract</summary>
RAW files are the initial measurement of scene radiance widely used in most cameras, and the ubiquitously-used RGB images are converted from RAW data through Image Signal Processing (ISP) pipelines. Nowadays, digital images are risky of being nefariously manipulated. Inspired by the fact that innate immunity is the first line of body defense, we propose DRAW, a novel scheme of defending images against manipulation by protecting their sources, i.e., camera-shooted RAWs. Specifically, we design a lightweight Multi-frequency Partial Fusion Network (MPF-Net) friendly to devices with limited computing resources by frequency learning and partial feature fusion. It introduces invisible watermarks as protective signal into the RAW data. The protection capability can not only be transferred into the rendered RGB images regardless of the applied ISP pipeline, but also is resilient to post-processing operations such as blurring or compression. Once the image is manipulated, we can accurately identify the forged areas with a localization network. Extensive experiments on several famous RAW datasets, e.g., RAISE, FiveK and SIDD, indicate the effectiveness of our method. We hope that this technique can be used in future cameras as an option for image protection, which could effectively restrict image manipulation at the source.
</details>
<details>
<summary>摘要</summary>
Raw 文件是现场辐射广泛使用的初始测量数据，而通用的RGB图像则是通过图像信号处理（ISP）管道从 Raw 数据转换。在当今的数字图像中，图像被虚构性修改的风险日益增加。 Drawing 灵感于人体自然免疫系统的第一线防御，我们提出了一种新的图像防止修改方法，即保护图像的来源，即摄像机拍摄的 Raw。我们设计了一种轻量级多频分解网络（MPF-Net），可以在设备有限的计算资源下运行。这种网络通过频率学习和部分特征融合，将隐藏的水印插入 Raw 数据中。这种保护能力不仅可以在渲染后的 RGB 图像中传递，而且对压缩或滤清等后处理操作也具有抗性。如果图像被修改，我们可以使用本地化网络来准确地确定forge 区域。我们在许多知名 RAW 数据集（如 RAISE、FiveK 和 SIDD）进行了广泛的实验，并表明了我们的方法的有效性。我们希望这种技术可以在未来的相机中作为图像保护选项，以防止图像修改。
</details></li>
</ul>
<hr>
<h2 id="Multi-modal-Graph-Neural-Network-for-Early-Diagnosis-of-Alzheimer’s-Disease-from-sMRI-and-PET-Scans"><a href="#Multi-modal-Graph-Neural-Network-for-Early-Diagnosis-of-Alzheimer’s-Disease-from-sMRI-and-PET-Scans" class="headerlink" title="Multi-modal Graph Neural Network for Early Diagnosis of Alzheimer’s Disease from sMRI and PET Scans"></a>Multi-modal Graph Neural Network for Early Diagnosis of Alzheimer’s Disease from sMRI and PET Scans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16366">http://arxiv.org/abs/2307.16366</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanteng Zhanga, Xiaohai He, Yi Hao Chan, Qizhi Teng, Jagath C. Rajapakse</li>
<li>for: 这个论文是为了早期诊断阿尔兹海默病（AD）而写的。</li>
<li>methods: 这个论文使用了图神经网络（GNN）来处理非欧几何图像，并在人口图框架中结合了成像特征和临床信息。</li>
<li>results: 这个研究表明，结合多种数据类型的多模态方法可以提高AD诊断性能，并且提供了技术参考和支持多变量多模态诊断方法的需求。<details>
<summary>Abstract</summary>
In recent years, deep learning models have been applied to neuroimaging data for early diagnosis of Alzheimer's disease (AD). Structural magnetic resonance imaging (sMRI) and positron emission tomography (PET) images provide structural and functional information about the brain, respectively. Combining these features leads to improved performance than using a single modality alone in building predictive models for AD diagnosis. However, current multi-modal approaches in deep learning, based on sMRI and PET, are mostly limited to convolutional neural networks, which do not facilitate integration of both image and phenotypic information of subjects. We propose to use graph neural networks (GNN) that are designed to deal with problems in non-Euclidean domains. In this study, we demonstrate how brain networks can be created from sMRI or PET images and be used in a population graph framework that can combine phenotypic information with imaging features of these brain networks. Then, we present a multi-modal GNN framework where each modality has its own branch of GNN and a technique is proposed to combine the multi-modal data at both the level of node vectors and adjacency matrices. Finally, we perform late fusion to combine the preliminary decisions made in each branch and produce a final prediction. As multi-modality data becomes available, multi-source and multi-modal is the trend of AD diagnosis. We conducted explorative experiments based on multi-modal imaging data combined with non-imaging phenotypic information for AD diagnosis and analyzed the impact of phenotypic information on diagnostic performance. Results from experiments demonstrated that our proposed multi-modal approach improves performance for AD diagnosis, and this study also provides technical reference and support the need for multivariate multi-modal diagnosis methods.
</details>
<details>
<summary>摘要</summary>
在最近的几年中，深度学习模型已经应用于神经成像数据，以提前诊断阿尔ц海默病（AD）。Structural磁共振成像（sMRI）和 пози트рон发射Tomography（PET）图像提供了脑部结构和功能信息，分别。将这些特征相结合，可以提高建模AD诊断性能，比使用单一模式 alone。然而，目前的多Modal Approach在深度学习中，基于sMRI和PET，主要是基于卷积神经网络，这些网络不会涉及到图像和subject的特征信息的集成。我们提议使用图гра数据（GNN），这些GNN是非欧几何问题的专门设计。在这个研究中，我们示出了如何从sMRI或PET图像中创建脑网络，并在人群图ogram框架中结合图像特征和subject的特征信息。然后，我们提出了一种多Modal GNN框架，其中每个模式有自己的GNN分支，并提出了将多Modal数据在每个分支和邻接矩阵的水平进行组合。最后，我们进行了晚期融合，将每个分支的初步决策相互融合，并生成最终预测。随着多Modal数据变得更加可用，多源多Modal是AD诊断的趋势。我们基于多Modal成像数据和非成像特征信息进行了探索性实验，分析了影响诊断性能的非成像信息。实验结果表明，我们提出的多Modal方法可以提高AD诊断性能，这也提供了技术参考，支持多变量多Modal诊断方法的需求。
</details></li>
</ul>
<hr>
<h2 id="Cardiac-MRI-Orientation-Recognition-and-Standardization-using-Deep-Neural-Networks"><a href="#Cardiac-MRI-Orientation-Recognition-and-Standardization-using-Deep-Neural-Networks" class="headerlink" title="Cardiac MRI Orientation Recognition and Standardization using Deep Neural Networks"></a>Cardiac MRI Orientation Recognition and Standardization using Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00615">http://arxiv.org/abs/2308.00615</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rxzhen/mscmr-orient">https://github.com/rxzhen/mscmr-orient</a></li>
<li>paper_authors: Ruoxuan Zhen</li>
<li>for: 本研究旨在提高医疗影像处理任务的效果，通过深度学习方法对医疗影像的方向识别和预测进行改进。</li>
<li>methods: 本研究使用深度神经网络对医疗影像的方向进行分类和标准化，并提出了一种传输学习策略，以适应不同的模式和序列。</li>
<li>results: 在不同的CMR模式和序列上进行了广泛的实验，Validation accuracy分别为100.0%、100.0%和99.4%，证明了我们的模型的稳定性和效果。<details>
<summary>Abstract</summary>
Orientation recognition and standardization play a crucial role in the effectiveness of medical image processing tasks. Deep learning-based methods have proven highly advantageous in orientation recognition and prediction tasks. In this paper, we address the challenge of imaging orientation in cardiac MRI and present a method that employs deep neural networks to categorize and standardize the orientation. To cater to multiple sequences and modalities of MRI, we propose a transfer learning strategy, enabling adaptation of our model from a single modality to diverse modalities. We conducted comprehensive experiments on CMR images from various modalities, including bSSFP, T2, and LGE. The validation accuracies achieved were 100.0\%, 100.0\%, and 99.4\%, confirming the robustness and effectiveness of our model. Our source code and network models are available at https://github.com/rxzhen/MSCMR-orient
</details>
<details>
<summary>摘要</summary>
媒体方向识别和标准化在医学影像处理任务中发挥关键作用。深度学习基于方法在媒体方向识别和预测任务中表现出了高度的优势。本文探讨了卡达影像中的媒体方向识别挑战，并提出了使用深度神经网络来分类和标准化媒体方向。为了适应不同的序列和模式，我们提议了传输学习策略，以便我们的模型从单一模式适应到多种模式。我们在不同的CMR模式图像上进行了广泛的实验，包括bSSFP、T2和LGE。实验结果显示，我们的模型在这些模式上的验证精度分别为100.0%、100.0%和99.4%，这证明了我们的模型的稳定性和效果。我们的源代码和网络模型可以在<https://github.com/rxzhen/MSCMR-orient>获取。
</details></li>
</ul>
<hr>
<h2 id="An-objective-validation-of-polyp-and-instrument-segmentation-methods-in-colonoscopy-through-Medico-2020-polyp-segmentation-and-MedAI-2021-transparency-challenges"><a href="#An-objective-validation-of-polyp-and-instrument-segmentation-methods-in-colonoscopy-through-Medico-2020-polyp-segmentation-and-MedAI-2021-transparency-challenges" class="headerlink" title="An objective validation of polyp and instrument segmentation methods in colonoscopy through Medico 2020 polyp segmentation and MedAI 2021 transparency challenges"></a>An objective validation of polyp and instrument segmentation methods in colonoscopy through Medico 2020 polyp segmentation and MedAI 2021 transparency challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16262">http://arxiv.org/abs/2307.16262</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/georgebatch/kvasir-seg">https://github.com/georgebatch/kvasir-seg</a></li>
<li>paper_authors: Debesh Jha, Vanshali Sharma, Debapriya Banik, Debayan Bhattacharya, Kaushiki Roy, Steven A. Hicks, Nikhil Kumar Tomar, Vajira Thambawita, Adrian Krenzer, Ge-Peng Ji, Sahadev Poudel, George Batchkala, Saruar Alam, Awadelrahman M. A. Ahmed, Quoc-Huy Trinh, Zeshan Khan, Tien-Phat Nguyen, Shruti Shrestha, Sabari Nathan, Jeonghwan Gwak, Ritika K. Jha, Zheyuan Zhang, Alexander Schlaefer, Debotosh Bhattacharjee, M. K. Bhuyan, Pradip K. Das, Sravanthi Parsa, Sharib Ali, Michael A. Riegler, Pål Halvorsen, Ulas Bagci, Thomas De Lange</li>
<li>for: 这篇研究旨在探讨自动分析潜水镜影像的技术，以实时检测潜在癌症的变化。</li>
<li>methods: 这篇研究使用深度学习技术来协助镜头检查医生检测和分类潜在癌症和异常。</li>
<li>results: 这篇研究发现了许多高精度的方法，并且评估了这些方法的可读性和可重现性。<details>
<summary>Abstract</summary>
Automatic analysis of colonoscopy images has been an active field of research motivated by the importance of early detection of precancerous polyps. However, detecting polyps during the live examination can be challenging due to various factors such as variation of skills and experience among the endoscopists, lack of attentiveness, and fatigue leading to a high polyp miss-rate. Deep learning has emerged as a promising solution to this challenge as it can assist endoscopists in detecting and classifying overlooked polyps and abnormalities in real time. In addition to the algorithm's accuracy, transparency and interpretability are crucial to explaining the whys and hows of the algorithm's prediction. Further, most algorithms are developed in private data, closed source, or proprietary software, and methods lack reproducibility. Therefore, to promote the development of efficient and transparent methods, we have organized the "Medico automatic polyp segmentation (Medico 2020)" and "MedAI: Transparency in Medical Image Segmentation (MedAI 2021)" competitions. We present a comprehensive summary and analyze each contribution, highlight the strength of the best-performing methods, and discuss the possibility of clinical translations of such methods into the clinic. For the transparency task, a multi-disciplinary team, including expert gastroenterologists, accessed each submission and evaluated the team based on open-source practices, failure case analysis, ablation studies, usability and understandability of evaluations to gain a deeper understanding of the models' credibility for clinical deployment. Through the comprehensive analysis of the challenge, we not only highlight the advancements in polyp and surgical instrument segmentation but also encourage qualitative evaluation for building more transparent and understandable AI-based colonoscopy systems.
</details>
<details>
<summary>摘要</summary>
自动分析幽门影像是一个活跃的研究领域，受到早期发现前期癌肿的重要性启发。然而，在实时检查中检测肿块可以是困难的，因为医生的技能和经验水平各不相同，注意力不集中，疲劳导致肿块漏检率高。深度学习作为一种有前途的解决方案，可以帮助医生在实时检查中检测和分类掉肿和异常。除了算法的准确性外，透明性和可解释性是解释算法预测的关键因素。然而，大多数算法是在私有数据、关闭源代码或商业软件上开发的，lacks reproducibility。因此，为促进效率和透明的方法的发展，我们组织了“医疗自动肿块分割（Medico 2020）”和“MedAI：医疗图像分割透明（MedAI 2021）”比赛。我们对每一个提交进行了全面的总结和分析，抛光最佳方法的优势，并讨论了这些方法在临床应用中的可能性。在透明任务中，一个多 disciplinary 团队，包括专业的 Gastroenterologist，对每一个提交进行了评估，评估基于开源实践、失败案例分析、剪辑研究、可用性和理解度来深入了解模型的可靠性。通过全面分析挑战，我们不仅强调了肿块和手术工具分割的进步，还鼓励了更加TRANSPARENT和可理解的 AI 基于幽门检查系统的建立。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://nullscc.github.io/2023/07/31/eess.IV_2023_07_31/" data-id="cllshr383008so988ciatc2se" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/31/eess.AS_2023_07_31/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.AS - 2023-07-31 22:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/30/cs.LG_2023_07_30/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-07-30 18:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'', root:''}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
