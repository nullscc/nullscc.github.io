
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-07-07 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Detecting the Sensing Area of A Laparoscopic Probe in Minimally Invasive Cancer Surgery paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.03662 repo_url: https:&#x2F;&#x2F;github.com&#x2F;br0202&#x2F;sensing_area_detection paper_auth">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-07-07">
<meta property="og:url" content="https://nullscc.github.io/2023/07/07/eess.IV_2023_07_07/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Detecting the Sensing Area of A Laparoscopic Probe in Minimally Invasive Cancer Surgery paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.03662 repo_url: https:&#x2F;&#x2F;github.com&#x2F;br0202&#x2F;sensing_area_detection paper_auth">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-07T09:00:00.000Z">
<meta property="article:modified_time" content="2023-09-23T08:32:39.376Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_07_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/07/eess.IV_2023_07_07/" class="article-date">
  <time datetime="2023-07-07T09:00:00.000Z" itemprop="datePublished">2023-07-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-07-07
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Detecting-the-Sensing-Area-of-A-Laparoscopic-Probe-in-Minimally-Invasive-Cancer-Surgery"><a href="#Detecting-the-Sensing-Area-of-A-Laparoscopic-Probe-in-Minimally-Invasive-Cancer-Surgery" class="headerlink" title="Detecting the Sensing Area of A Laparoscopic Probe in Minimally Invasive Cancer Surgery"></a>Detecting the Sensing Area of A Laparoscopic Probe in Minimally Invasive Cancer Surgery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03662">http://arxiv.org/abs/2307.03662</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/br0202/sensing_area_detection">https://github.com/br0202/sensing_area_detection</a></li>
<li>paper_authors: Baoru Huang, Yicheng Hu, Anh Nguyen, Stamatia Giannarou, Daniel S. Elson</li>
<li>for: This paper aims to improve the accuracy of endoscopic radio-guided cancer detection and resection by developing a novel method for detecting the sensing area of a tethered laparoscopic gamma detector.</li>
<li>methods: The proposed method uses a simple regression network to leverage high-dimensional image features and probe position information to visualize gamma activity origination on the tissue surface.</li>
<li>results: The authors demonstrated the effectiveness of their method through intensive experimentation using two publicly released datasets captured with a custom-designed, portable stereo laparoscope system, establishing a new performance benchmark.<details>
<summary>Abstract</summary>
In surgical oncology, it is challenging for surgeons to identify lymph nodes and completely resect cancer even with pre-operative imaging systems like PET and CT, because of the lack of reliable intraoperative visualization tools. Endoscopic radio-guided cancer detection and resection has recently been evaluated whereby a novel tethered laparoscopic gamma detector is used to localize a preoperatively injected radiotracer. This can both enhance the endoscopic imaging and complement preoperative nuclear imaging data. However, gamma activity visualization is challenging to present to the operator because the probe is non-imaging and it does not visibly indicate the activity origination on the tissue surface. Initial failed attempts used segmentation or geometric methods, but led to the discovery that it could be resolved by leveraging high-dimensional image features and probe position information. To demonstrate the effectiveness of this solution, we designed and implemented a simple regression network that successfully addressed the problem. To further validate the proposed solution, we acquired and publicly released two datasets captured using a custom-designed, portable stereo laparoscope system. Through intensive experimentation, we demonstrated that our method can successfully and effectively detect the sensing area, establishing a new performance benchmark. Code and data are available at https://github.com/br0202/Sensing_area_detection.git
</details>
<details>
<summary>摘要</summary>
在外科onkoloji中，外科医生很难以识别lymph node和完全remove癌症，即使使用前 operated imaging系统like PET和CT，因为缺乏可靠的手术过程中的视觉化工具。 Recently, endoscopic radio-guided cancer detection and resection has been evaluated, which uses a novel tethered laparoscopic gamma detector to localize a preoperatively injected radiotracer. This can both enhance endoscopic imaging and complement preoperative nuclear imaging data. However, gamma activity visualization is challenging to present to the operator because the probe is non-imaging and it does not visibly indicate the activity origination on the tissue surface. Early attempts used segmentation or geometric methods, but these were not successful. Instead, we found that the problem could be resolved by leveraging high-dimensional image features and probe position information. To demonstrate the effectiveness of this solution, we designed and implemented a simple regression network that successfully addressed the problem. To further validate the proposed solution, we acquired and publicly released two datasets captured using a custom-designed, portable stereo laparoscope system. Through extensive experimentation, we demonstrated that our method can successfully and effectively detect the sensing area, establishing a new performance benchmark. Code and data are available at <https://github.com/br0202/Sensing_area_detection.git>.
</details></li>
</ul>
<hr>
<h2 id="VesselVAE-Recursive-Variational-Autoencoders-for-3D-Blood-Vessel-Synthesis"><a href="#VesselVAE-Recursive-Variational-Autoencoders-for-3D-Blood-Vessel-Synthesis" class="headerlink" title="VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis"></a>VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03592">http://arxiv.org/abs/2307.03592</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paula Feldman, Miguel Fainstein, Viviana Siless, Claudio Delrieux, Emmanuel Iarussi</li>
<li>For: 该论文旨在提出一种数据驱动的生成框架，用于synthesizing blood vessel 3D geometry。* Methods: 该方法使用Recursive Variational Neural Network（RVNN），全面利用血管的层次结构，学习低维抽象空间，包括分支连接性和表面特征。* Results: 该方法可以生成高度相似的真实和 sintetic 数据，包括半径（.97）、长度（.95）和折叠度（.96）。通过深度神经网络的力量，该方法生成的3D血管模型具有高度准确和多样性，这对医疗和手术培训、血液动力学 simulations 等方面都非常重要。<details>
<summary>Abstract</summary>
We present a data-driven generative framework for synthesizing blood vessel 3D geometry. This is a challenging task due to the complexity of vascular systems, which are highly variating in shape, size, and structure. Existing model-based methods provide some degree of control and variation in the structures produced, but fail to capture the diversity of actual anatomical data. We developed VesselVAE, a recursive variational Neural Network that fully exploits the hierarchical organization of the vessel and learns a low-dimensional manifold encoding branch connectivity along with geometry features describing the target surface. After training, the VesselVAE latent space can be sampled to generate new vessel geometries. To the best of our knowledge, this work is the first to utilize this technique for synthesizing blood vessels. We achieve similarities of synthetic and real data for radius (.97), length (.95), and tortuosity (.96). By leveraging the power of deep neural networks, we generate 3D models of blood vessels that are both accurate and diverse, which is crucial for medical and surgical training, hemodynamic simulations, and many other purposes.
</details>
<details>
<summary>摘要</summary>
我们提出了一种基于数据的生成框架，用于 sintesizing 血管三维几何结构。这是一项具有挑战性的任务，因为血管系统的复杂性以及形态、大小和结构的多样性。现有的模型基于方法可以提供一定的控制和变化，但是它们无法捕捉实际 анатомиче数据的多样性。我们开发了 VesselVAE，一种嵌入式的变量神经网络，它完全利用血管的层次结构，并学习低维度拟合空间，包括连接分支和表面几何特征。经过训练，VesselVAE 的缓存空间可以采样生成新的血管几何结构。根据我们所知，这是第一次利用这种技术来 sintesizing 血管。我们实现了真实数据和 sintesizing 数据之间的相似性（.97）、长度（.95）和折叠性（.96）。通过利用深度神经网络的力量，我们生成了高度准确和多样的血管三维模型，这对医疗和手术培训、血液动力学计算以及许多其他目的都是关键。
</details></li>
</ul>
<hr>
<h2 id="Physical-Color-Calibration-of-Digital-Pathology-Scanners-for-Robust-Artificial-Intelligence-Assisted-Cancer-Diagnosis"><a href="#Physical-Color-Calibration-of-Digital-Pathology-Scanners-for-Robust-Artificial-Intelligence-Assisted-Cancer-Diagnosis" class="headerlink" title="Physical Color Calibration of Digital Pathology Scanners for Robust Artificial Intelligence Assisted Cancer Diagnosis"></a>Physical Color Calibration of Digital Pathology Scanners for Robust Artificial Intelligence Assisted Cancer Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05519">http://arxiv.org/abs/2307.05519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoyi Ji, Richard Salmon, Nita Mulliqi, Umair Khan, Yinxi Wang, Anders Blilie, Henrik Olsson, Bodil Ginnerup Pedersen, Karina Dalsgaard Sørensen, Benedicte Parm Ulhøi, Svein R Kjosavik, Emilius AM Janssen, Mattias Rantalainen, Lars Egevad, Pekka Ruusuvuori, Martin Eklund, Kimmo Kartasalo</li>
<li>for: 提高艾特ints家用于数字病理学的可靠性和应用性</li>
<li>methods: 使用物理颜色准确标准化扫描仪的扫描图像，以提高人工智能系统的评估性和可靠性</li>
<li>results: 实验结果表明，物理颜色准确标准化可以标准化扫描仪的扫描图像，提高艾特ints家的评估性和可靠性，使其在临床应用中更加可靠和可行<details>
<summary>Abstract</summary>
The potential of artificial intelligence (AI) in digital pathology is limited by technical inconsistencies in the production of whole slide images (WSIs), leading to degraded AI performance and posing a challenge for widespread clinical application as fine-tuning algorithms for each new site is impractical. Changes in the imaging workflow can also lead to compromised diagnoses and patient safety risks. We evaluated whether physical color calibration of scanners can standardize WSI appearance and enable robust AI performance. We employed a color calibration slide in four different laboratories and evaluated its impact on the performance of an AI system for prostate cancer diagnosis on 1,161 WSIs. Color standardization resulted in consistently improved AI model calibration and significant improvements in Gleason grading performance. The study demonstrates that physical color calibration provides a potential solution to the variation introduced by different scanners, making AI-based cancer diagnostics more reliable and applicable in clinical settings.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）在数字 PATHOLOGY 的潜力受到数字标本（WSIs）的技术不一致所限制，导致 AI 性能下降，使得广泛应用在临床中采用困难。变化在扫描 workflow 也可能导致诊断错误和患者安全风险。我们评估了扫描器的物理色彩准确性是否可以标准化 WSI 的外观，并使 AI 系统在 1,161 个标本上表现出色。结果显示，物理色彩准确性可以提高 AI 模型准确性，并且在 Gleason 分期性能上显示了明显的改善。这一研究表明，物理色彩准确性可以解决不同扫描器导致的变化，使 AI 基于 cancer 诊断更可靠和在临床设置中实用。
</details></li>
</ul>
<hr>
<h2 id="A-Deep-Active-Contour-Model-for-Delineating-Glacier-Calving-Fronts"><a href="#A-Deep-Active-Contour-Model-for-Delineating-Glacier-Calving-Fronts" class="headerlink" title="A Deep Active Contour Model for Delineating Glacier Calving Fronts"></a>A Deep Active Contour Model for Delineating Glacier Calving Fronts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03461">http://arxiv.org/abs/2307.03461</a></li>
<li>repo_url: None</li>
<li>paper_authors: Konrad Heidler, Lichao Mou, Erik Loebel, Mirko Scheinert, Sébastien Lefèvre, Xiao Xiang Zhu</li>
<li>for: 这个论文是关于冰川分裂前模型的研究，旨在提出一种基于 outline 找出的方法，以提高冰川分裂前探测器的准确率。</li>
<li>methods: 该方法 combine 了 Convolutional Neural Networks (CNNs) 和 active contour model，以提取特征和描述 outline。</li>
<li>results: 通过在格陵兰冰川的多个大规模数据集上训练和评估，该方法被证明超过基于 segmentation 和 edge-detection 的方法。此外，该方法还可以更好地计算模型预测结果的不确定性。<details>
<summary>Abstract</summary>
Choosing how to encode a real-world problem as a machine learning task is an important design decision in machine learning. The task of glacier calving front modeling has often been approached as a semantic segmentation task. Recent studies have shown that combining segmentation with edge detection can improve the accuracy of calving front detectors. Building on this observation, we completely rephrase the task as a contour tracing problem and propose a model for explicit contour detection that does not incorporate any dense predictions as intermediate steps. The proposed approach, called ``Charting Outlines by Recurrent Adaptation'' (COBRA), combines Convolutional Neural Networks (CNNs) for feature extraction and active contour models for the delineation. By training and evaluating on several large-scale datasets of Greenland's outlet glaciers, we show that this approach indeed outperforms the aforementioned methods based on segmentation and edge-detection. Finally, we demonstrate that explicit contour detection has benefits over pixel-wise methods when quantifying the models' prediction uncertainties. The project page containing the code and animated model predictions can be found at \url{https://khdlr.github.io/COBRA/}.
</details>
<details>
<summary>摘要</summary>
选择如何编码实际问题为机器学习任务是机器学习设计决策的重要一环。冰川脱落前模型的任务经常被看作为语义分割任务。 latest studies have shown that combining segmentation with edge detection can improve the accuracy of calving front detectors. 在这个基础上，我们完全重新表述任务为一个 outline tracing 问题，并提出一种不包含任何稠密预测的模型。我们称之为“Charting Outlines by Recurrent Adaptation”（COBRA），它将 Convolutional Neural Networks（CNNs）用于特征提取和活动 kontur 模型来进行定义。通过对瑞典格陵兰冰川出口的数据进行训练和评估，我们示出了这种方法实际上超过了以前基于 segmentation 和 edge-detection 的方法。最后，我们示出了明确的 outline 检测在量化模型预测不确定性时的优势。关于这个项目，包含代码和动画预测的项目页面可以在 \url{https://khdlr.github.io/COBRA/} 上找到。
</details></li>
</ul>
<hr>
<h2 id="Non-iterative-Coarse-to-fine-Transformer-Networks-for-Joint-Affine-and-Deformable-Image-Registration"><a href="#Non-iterative-Coarse-to-fine-Transformer-Networks-for-Joint-Affine-and-Deformable-Image-Registration" class="headerlink" title="Non-iterative Coarse-to-fine Transformer Networks for Joint Affine and Deformable Image Registration"></a>Non-iterative Coarse-to-fine Transformer Networks for Joint Affine and Deformable Image Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03421">http://arxiv.org/abs/2307.03421</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mungomeng/registration-nice-trans">https://github.com/mungomeng/registration-nice-trans</a></li>
<li>paper_authors: Mingyuan Meng, Lei Bi, Michael Fulham, Dagan Feng, Jinman Kim</li>
<li>for: 这个论文主要是为了提出一种基于深度学习的非迭代抽象图像 региSTRATION方法，以提高图像REGISTRATION的精度和效率。</li>
<li>methods: 这个方法使用了非迭代抽象图像REGISTRATION的单个网络，并首次将 transformers 引入到 NICE 图像REGISTRATION 框架中，以模型图像之间的长距离相关性。</li>
<li>results: 经过广泛的七个公共数据集的实验，这个方法的注意力 Transformer 在 NICE 图像REGISTRATION 中表现出了优于当前状态艺术的精度和效率。<details>
<summary>Abstract</summary>
Image registration is a fundamental requirement for medical image analysis. Deep registration methods based on deep learning have been widely recognized for their capabilities to perform fast end-to-end registration. Many deep registration methods achieved state-of-the-art performance by performing coarse-to-fine registration, where multiple registration steps were iterated with cascaded networks. Recently, Non-Iterative Coarse-to-finE (NICE) registration methods have been proposed to perform coarse-to-fine registration in a single network and showed advantages in both registration accuracy and runtime. However, existing NICE registration methods mainly focus on deformable registration, while affine registration, a common prerequisite, is still reliant on time-consuming traditional optimization-based methods or extra affine registration networks. In addition, existing NICE registration methods are limited by the intrinsic locality of convolution operations. Transformers may address this limitation for their capabilities to capture long-range dependency, but the benefits of using transformers for NICE registration have not been explored. In this study, we propose a Non-Iterative Coarse-to-finE Transformer network (NICE-Trans) for image registration. Our NICE-Trans is the first deep registration method that (i) performs joint affine and deformable coarse-to-fine registration within a single network, and (ii) embeds transformers into a NICE registration framework to model long-range relevance between images. Extensive experiments with seven public datasets show that our NICE-Trans outperforms state-of-the-art registration methods on both registration accuracy and runtime.
</details>
<details>
<summary>摘要</summary>
医疗图像分析中的图像注册是一项基本要求。基于深度学习的深度注册方法在过去几年内得到了广泛的认可，因为它们可以快速完成端到端注册。许多深度注册方法在多个注册步骤中使用了缩放网络，以实现粗细到细节的注册。然而，现有的NICE注册方法主要关注于可变性注册，而平移注册，是医疗图像注册的常见前提，仍然是通过传统的优化方法或额外的平移注册网络来实现的。此外，现有的NICE注册方法受到了卷积操作的本地性的限制。使用transformer可以解决这一限制，因为它可以捕捉图像之间的长距离相关性。但是，使用transformer进行NICE注册的好处尚未得到了探讨。在本研究中，我们提出了一种非iterative粗细到细节的transformer网络（NICE-Trans） для图像注册。我们的NICE-Trans是首个深度注册方法，它（i）在单个网络中同时实现了平移和可变性的粗细到细节注册，（ii）将transformer引入NICE注册框架，以模型图像之间的长距离相关性。我们在七个公共数据集上进行了广泛的实验，结果表明，我们的NICE-Trans比状态之前的注册方法在注册精度和运行时间上都有提高。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Hyperspectral-and-Multispectral-Images-Fusion-Based-on-the-Cycle-Consistency"><a href="#Unsupervised-Hyperspectral-and-Multispectral-Images-Fusion-Based-on-the-Cycle-Consistency" class="headerlink" title="Unsupervised Hyperspectral and Multispectral Images Fusion Based on the Cycle Consistency"></a>Unsupervised Hyperspectral and Multispectral Images Fusion Based on the Cycle Consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03413">http://arxiv.org/abs/2307.03413</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shuaikaishi/CycFusion">https://github.com/shuaikaishi/CycFusion</a></li>
<li>paper_authors: Shuaikai Shi, Lijun Zhang, Yoann Altmann, Jie Chen</li>
<li>for: 这个论文主要针对的问题是如何实现高spectral resolution和高 spatial resolution的图像拼接，并且提出了一种基于循环一致的无监督拼接模型。</li>
<li>methods: 该模型基于循环一致的概念，将低spectral resolution的干扰图像（LrHSI）和高spectral resolution的多spectral图像（HrMSI）映射到高spectral resolution的图像中，并且通过单个变换和双重变换的对比来学习域转换。</li>
<li>results: 对于多个数据集，该模型的实验结果表明，与其他无监督拼接方法相比，该模型能够更好地实现高spectral resolution和高 spatial resolution的图像拼接，并且可以在不知道干扰参数的情况下进行拼接。<details>
<summary>Abstract</summary>
Hyperspectral images (HSI) with abundant spectral information reflected materials property usually perform low spatial resolution due to the hardware limits. Meanwhile, multispectral images (MSI), e.g., RGB images, have a high spatial resolution but deficient spectral signatures. Hyperspectral and multispectral image fusion can be cost-effective and efficient for acquiring both high spatial resolution and high spectral resolution images. Many of the conventional HSI and MSI fusion algorithms rely on known spatial degradation parameters, i.e., point spread function, spectral degradation parameters, spectral response function, or both of them. Another class of deep learning-based models relies on the ground truth of high spatial resolution HSI and needs large amounts of paired training images when working in a supervised manner. Both of these models are limited in practical fusion scenarios. In this paper, we propose an unsupervised HSI and MSI fusion model based on the cycle consistency, called CycFusion. The CycFusion learns the domain transformation between low spatial resolution HSI (LrHSI) and high spatial resolution MSI (HrMSI), and the desired high spatial resolution HSI (HrHSI) are considered to be intermediate feature maps in the transformation networks. The CycFusion can be trained with the objective functions of marginal matching in single transform and cycle consistency in double transforms. Moreover, the estimated PSF and SRF are embedded in the model as the pre-training weights, which further enhances the practicality of our proposed model. Experiments conducted on several datasets show that our proposed model outperforms all compared unsupervised fusion methods. The codes of this paper will be available at this address: https: //github.com/shuaikaishi/CycFusion for reproducibility.
</details>
<details>
<summary>摘要</summary>
干扰影像（HSI）具有丰富的 спектраль信息，通常由硬件限制而导致低空间分辨率。而多spectral影像（MSI），例如RGB影像，具有高空间分辨率，但缺乏 спектраль特征。干扰影像和多spectral影像的图像混合可以是成本效益和高效的方式，以获得高空间分辨率和高спектраль分辨率的图像。许多传统的HSI和MSI混合算法取决于已知的空间退化参数，例如点扩散函数、spectral退化参数、spectral响应函数或其中之一。另一类的深度学习模型需要大量的实验训练图像，且需要高空间分辨率HSI的实验训练图像。这两类模型在实际混合应用场景中有限制。在这篇论文中，我们提出了一种不需要实验训练图像的干扰影像和多spectral影像混合模型，基于循环一致性，称为CycFusion。CycFusion学习了干扰影像低空间分辨率（LrHSI）和高空间分辨率多spectral影像（HrMSI）之间的Domain转换，并考虑了欲有的高空间分辨率HSI作为中间特征图。CycFusion可以通过单个转换和双转换的对应函数来训练，并且可以在干扰影像和多spectral影像之间进行自适应的混合。此外，我们还在模型中嵌入了估计的PSF和SRF，以进一步提高模型的实用性。我们在多个数据集上进行了实验，结果显示，我们的提出的模型在无监督情况下超过所有相比的无监督混合方法。模型代码将在以下地址可用：https: //github.com/shuaikaishi/CycFusion，以便重现。
</details></li>
</ul>
<hr>
<h2 id="Towards-Robust-SDRTV-to-HDRTV-via-Dual-Inverse-Degradation-Network"><a href="#Towards-Robust-SDRTV-to-HDRTV-via-Dual-Inverse-Degradation-Network" class="headerlink" title="Towards Robust SDRTV-to-HDRTV via Dual Inverse Degradation Network"></a>Towards Robust SDRTV-to-HDRTV via Dual Inverse Degradation Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03394">http://arxiv.org/abs/2307.03394</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kepeng Xu, Gang He, Li Xu, Xingchao Yang, Ming Sun, Yuzhi Wang, Zijia Ma, Haoqiang Fan, Xing Wen</li>
<li>for: 提高SDRTV至HDRTV的转换效果，并且解决转换过程中存在的编码痕迹的增强问题。</li>
<li>methods: 提出了一种双 inverse degradation SDRTV-to-HDRTV网络DIDNet，包括时间空间特征对齐模块和双调度卷积，以及波帕特注意模块，以提高颜色恢复能力和编码痕迹除减能力。</li>
<li>results: 与当前状态艺术方法相比，提出的方法在量化结果、视觉质量和推理时间等方面具有显著优势，因此可以在实际应用场景中提高SDRTV至HDRTV的转换效果。<details>
<summary>Abstract</summary>
Recently, the transformation of standard dynamic range TV (SDRTV) to high dynamic range TV (HDRTV) is in high demand due to the scarcity of HDRTV content. However, the conversion of SDRTV to HDRTV often amplifies the existing coding artifacts in SDRTV which deteriorate the visual quality of the output. In this study, we propose a dual inverse degradation SDRTV-to-HDRTV network DIDNet to address the issue of coding artifact restoration in converted HDRTV, which has not been previously studied. Specifically, we propose a temporal-spatial feature alignment module and dual modulation convolution to remove coding artifacts and enhance color restoration ability. Furthermore, a wavelet attention module is proposed to improve SDRTV features in the frequency domain. An auxiliary loss is introduced to decouple the learning process for effectively restoring from dual degradation. The proposed method outperforms the current state-of-the-art method in terms of quantitative results, visual quality, and inference times, thus enhancing the performance of the SDRTV-to-HDRTV method in real-world scenarios.
</details>
<details>
<summary>摘要</summary>
最近，标准动态范围电视（SDRTV）到高动态范围电视（HDRTV）的转换受到了高动态范围内容缺乏的限制。然而，将SDRTV转换为HDRTV经常会加剧SDRTV中存在的编码 artifacts，从而降低输出视质。在这项研究中，我们提出了一个双重逆减 SDRTV-to-HDRTV 网络 DIDNet，以解决转换后 HDRTV 中的编码 artifact 纠正问题，这一问题尚未被研究。特别是，我们提出了时空特征对齐模块和双扩涨 convolution来除除编码 artifacts和提高颜色纠正能力。此外，我们还提出了wavelet 注意力模块，以提高 SDRTV 特征在频域中。还引入了一个 auxillary 损失函数，以分离学习过程中的纠正过程，从而提高 SDRTV-to-HDRTV 方法的实际场景性能。根据量化结果和视觉质量，我们的方法超过了当前状态的艺术方法，从而提高 SDRTV-to-HDRTV 方法的性能。
</details></li>
</ul>
<hr>
<h2 id="CheXmask-a-large-scale-dataset-of-anatomical-segmentation-masks-for-multi-center-chest-x-ray-images"><a href="#CheXmask-a-large-scale-dataset-of-anatomical-segmentation-masks-for-multi-center-chest-x-ray-images" class="headerlink" title="CheXmask: a large-scale dataset of anatomical segmentation masks for multi-center chest x-ray images"></a>CheXmask: a large-scale dataset of anatomical segmentation masks for multi-center chest x-ray images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03293">http://arxiv.org/abs/2307.03293</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ngaggion/chexmask-database">https://github.com/ngaggion/chexmask-database</a></li>
<li>paper_authors: Nicolás Gaggion, Candelaria Mosquera, Lucas Mansilla, Martina Aineseder, Diego H. Milone, Enzo Ferrante</li>
<li>for: 这个研究的目的是为了提供一个大型、多中心的胸部X射影分类数据集，以便帮助开发更好的人工智能模型。</li>
<li>methods: 这个研究使用了HybridGNet模型来确保所有数据集中的分类都是一致的和高品质的。</li>
<li>results: 这个研究产生了676,803个分类掩模，并进行了严格的验证，包括专业医生评价和自动化品质控制，以验证所有掩模的品质。<details>
<summary>Abstract</summary>
The development of successful artificial intelligence models for chest X-ray analysis relies on large, diverse datasets with high-quality annotations. While several databases of chest X-ray images have been released, most include disease diagnosis labels but lack detailed pixel-level anatomical segmentation labels. To address this gap, we introduce an extensive chest X-ray multi-center segmentation dataset with uniform and fine-grain anatomical annotations for images coming from six well-known publicly available databases: CANDID-PTX, ChestX-ray8, Chexpert, MIMIC-CXR-JPG, Padchest, and VinDr-CXR, resulting in 676,803 segmentation masks. Our methodology utilizes the HybridGNet model to ensure consistent and high-quality segmentations across all datasets. Rigorous validation, including expert physician evaluation and automatic quality control, was conducted to validate the resulting masks. Additionally, we provide individualized quality indices per mask and an overall quality estimation per dataset. This dataset serves as a valuable resource for the broader scientific community, streamlining the development and assessment of innovative methodologies in chest X-ray analysis. The CheXmask dataset is publicly available at: \url{https://physionet.org/content/chexmask-cxr-segmentation-data/}.
</details>
<details>
<summary>摘要</summary>
发展成功人工智能模型需要大量多样化的数据集，以便进行胸部X射线分析。虽然一些胸部X射线图像数据库已经发布，但大多数只包含疾病诊断标签，缺乏细致的像素级别解剖标注。为了解决这个问题，我们介绍了一个胸部X射线多中心分割数据集，包括六个公共可用的数据库：CANDID-PTX、ChestX-ray8、Chexpert、MIMIC-CXR-JPG、Padchest和VinDr-CXR，共计676,803个分割mask。我们的方法使用HybridGNet模型，确保分割结果具有一致性和高质量。我们进行了严格的验证，包括专业医生评估和自动化质量控制，以验证结果。此外，我们还提供了每个mask的个性化质量指数和每个数据集的总质量估计。这个数据集对科学社区来说是一个重要的资源，可以促进胸部X射线分析领域的创新和评估。CheXmask数据集可以在以下链接获取：https://physionet.org/content/chexmask-cxr-segmentation-data/。
</details></li>
</ul>
<hr>
<h2 id="ADASSM-Adversarial-Data-Augmentation-in-Statistical-Shape-Models-From-Images"><a href="#ADASSM-Adversarial-Data-Augmentation-in-Statistical-Shape-Models-From-Images" class="headerlink" title="ADASSM: Adversarial Data Augmentation in Statistical Shape Models From Images"></a>ADASSM: Adversarial Data Augmentation in Statistical Shape Models From Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03273">http://arxiv.org/abs/2307.03273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mokshagna Sai Teja Karanam, Tushar Kataria, Krithika Iyer, Shireen Elhabian</li>
<li>for: 这paper的目的是提出一种基于深度学习的图像到统计形态模型（SSM）框架，以提高图像到SSM的准确率和效率。</li>
<li>methods: 该paper使用了深度学习模型来学习图像中的形态表示，并使用了KDE方法生成形态增强样本以帮助图像到SSM网络实现比较高的准确率。</li>
<li>results: 该paper的实验结果表明，使用该novel strategy可以提高图像到SSM网络的准确率，并且可以避免深度学习模型的图像基于Texture偏好。<details>
<summary>Abstract</summary>
Statistical shape models (SSM) have been well-established as an excellent tool for identifying variations in the morphology of anatomy across the underlying population. Shape models use consistent shape representation across all the samples in a given cohort, which helps to compare shapes and identify the variations that can detect pathologies and help in formulating treatment plans. In medical imaging, computing these shape representations from CT/MRI scans requires time-intensive preprocessing operations, including but not limited to anatomy segmentation annotations, registration, and texture denoising. Deep learning models have demonstrated exceptional capabilities in learning shape representations directly from volumetric images, giving rise to highly effective and efficient Image-to-SSM networks. Nevertheless, these models are data-hungry and due to the limited availability of medical data, deep learning models tend to overfit. Offline data augmentation techniques, that use kernel density estimation based (KDE) methods for generating shape-augmented samples, have successfully aided Image-to-SSM networks in achieving comparable accuracy to traditional SSM methods. However, these augmentation methods focus on shape augmentation, whereas deep learning models exhibit image-based texture bias resulting in sub-optimal models. This paper introduces a novel strategy for on-the-fly data augmentation for the Image-to-SSM framework by leveraging data-dependent noise generation or texture augmentation. The proposed framework is trained as an adversary to the Image-to-SSM network, augmenting diverse and challenging noisy samples. Our approach achieves improved accuracy by encouraging the model to focus on the underlying geometry rather than relying solely on pixel values.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Fully-Automated-and-Explainable-Algorithm-for-the-Prediction-of-Malignant-Transformation-in-Oral-Epithelial-Dysplasia"><a href="#A-Fully-Automated-and-Explainable-Algorithm-for-the-Prediction-of-Malignant-Transformation-in-Oral-Epithelial-Dysplasia" class="headerlink" title="A Fully Automated and Explainable Algorithm for the Prediction of Malignant Transformation in Oral Epithelial Dysplasia"></a>A Fully Automated and Explainable Algorithm for the Prediction of Malignant Transformation in Oral Epithelial Dysplasia</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03757">http://arxiv.org/abs/2307.03757</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam J Shephard, Raja Muhammad Saad Bashir, Hanya Mahmood, Mostafa Jahanifar, Fayyaz Minhas, Shan E Ahmed Raza, Kris D McCombe, Stephanie G Craig, Jacqueline James, Jill Brooks, Paul Nankivell, Hisham Mehanna, Syed Ali Khurram, Nasir M Rajpoot<br>for: 这个研究的目的是提出一种完全自动化的算法，以便预测口腔细膜癌变的发生，基于可读取的核仁特征。methods: 这个算法使用了自己设计的Segmentation模型，以及一种浅层神经网络，以检测和分类口腔细膜中的核仁。results: 该算法在三个外部数据集上进行了验证，AUROC值为0.74，表明其可以准确预测口腔细膜癌变的发生。此外，Survival分析表明该算法可以预测癌变的发生，并且可以更好地预测比 manually-assigned WHO和二元等级。<details>
<summary>Abstract</summary>
Oral epithelial dysplasia (OED) is a premalignant histopathological diagnosis given to lesions of the oral cavity. Its grading suffers from significant inter-/intra- observer variability, and does not reliably predict malignancy progression, potentially leading to suboptimal treatment decisions. To address this, we developed a novel artificial intelligence algorithm that can assign an Oral Malignant Transformation (OMT) risk score, based on histological patterns in the in Haematoxylin and Eosin stained whole slide images, to quantify the risk of OED progression. The algorithm is based on the detection and segmentation of nuclei within (and around) the epithelium using an in-house segmentation model. We then employed a shallow neural network fed with interpretable morphological/spatial features, emulating histological markers. We conducted internal cross-validation on our development cohort (Sheffield; n = 193 cases) followed by independent validation on two external cohorts (Birmingham and Belfast; n = 92 cases). The proposed OMTscore yields an AUROC = 0.74 in predicting whether an OED progresses to malignancy or not. Survival analyses showed the prognostic value of our OMTscore for predicting malignancy transformation, when compared to the manually-assigned WHO and binary grades. Analysis of the correctly predicted cases elucidated the presence of peri-epithelial and epithelium-infiltrating lymphocytes in the most predictive patches of cases that transformed (p < 0.0001). This is the first study to propose a completely automated algorithm for predicting OED transformation based on interpretable nuclear features, whilst being validated on external datasets. The algorithm shows better-than-human-level performance for prediction of OED malignant transformation and offers a promising solution to the challenges of grading OED in routine clinical practice.
</details>
<details>
<summary>摘要</summary>
口腔粘膜细胞变化（OED）是口腔部位的一种前癌性病理诊断，但其分级存在很大的干扰因素和内部/外部观察者的不一致，不能准确预测癌变进程，可能导致不佳的治疗决策。为了解决这个问题，我们开发了一种新的人工智能算法，可以根据染色体板术影像中的细胞核特征分配口腔癌变转换风险分数（OMT分数），以评估OED转换癌变的风险。这种算法基于检测和分类细胞核的具体方法，使用了我们自己开发的分割模型。然后，我们使用了一个浅层神经网络，以便使用可读性特征来模拟 histological markers。我们在我们的开发组（Sheffield）进行了内部十字验证（n = 193例），然后在两个外部组（Birmingham和Belfast）进行了独立验证（n = 92例）。我们的提出的 OMT 分数可以在预测OED转换癌变是否存在的问题上达到 AUROC = 0.74 的性能。我们的存活分析表明，我们的 OMT 分数具有预测癌变转换的 проGNosis 价值，比较于由人工分配的 WHO 和二分类分数。分析 Correctly 预测的 случаeschannel 显示，在转换的 caso 中存在辐射半径内的卵细胞和细胞核的卫星细胞（p < 0.0001）。这是首个提出一种完全自动化的 OED 转换预测算法，基于可读性的核特征，并在外部数据上进行了验证。这种算法在预测 OED 癌变转换性能上达到了人类水平，并且提供了一个可能的解决方案，以解决 OED 的分级挑战。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Aware-Image-Compressed-Sensing"><a href="#Semantic-Aware-Image-Compressed-Sensing" class="headerlink" title="Semantic-Aware Image Compressed Sensing"></a>Semantic-Aware Image Compressed Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03246">http://arxiv.org/abs/2307.03246</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bowen Zhang, Zhijin Qin, Geoffrey Ye Li</li>
<li>for: 提高图像压缩感知效率</li>
<li>methods: 使用基于深度学习的图像压缩感知系统，并使用策略网络分析图像semantic信息，确定不同图像区域的测量矩阵</li>
<li>results: 提出了一种基于semantic信息的图像压缩感知系统，实现了提高图像压缩感知效率<details>
<summary>Abstract</summary>
Deep learning based image compressed sensing (CS) has achieved great success. However, existing CS systems mainly adopt a fixed measurement matrix to images, ignoring the fact the optimal measurement numbers and bases are different for different images. To further improve the sensing efficiency, we propose a novel semantic-aware image CS system. In our system, the encoder first uses a fixed number of base CS measurements to sense different images. According to the base CS results, the encoder then employs a policy network to analyze the semantic information in images and determines the measurement matrix for different image areas. At the decoder side, a semantic-aware initial reconstruction network is developed to deal with the changes of measurement matrices used at the encoder. A rate-distortion training loss is further introduced to dynamically adjust the average compression ratio for the semantic-aware CS system and the policy network is trained jointly with the encoder and the decoder in an en-to-end manner by using some proxy functions. Numerical results show that the proposed semantic-aware image CS system is superior to the traditional ones with fixed measurement matrices.
</details>
<details>
<summary>摘要</summary>
深度学习基于图像压缩感知（CS）技术已经取得了很大成功。然而，现有的CS系统主要采用固定的测量矩阵来对图像进行测量，忽略了图像的优化测量数量和基准的不同。为了进一步提高感知效率，我们提出了一种新的Semantic-aware图像CS系统。在我们的系统中，Encoder首先使用固定数量的基础CS测量来感知不同的图像。根据基础CS结果，Encoder然后使用一个政策网络分析图像中的Semantic信息，并确定不同的图像区域的测量矩阵。在Decoder端，一个Semantic-aware初始重建网络被开发以处理测量矩阵的变化。此外，我们还引入了一个率-压缩训练损失，以 Dynamically adjust the average compression ratio for the Semantic-aware CS system。Policy网络与Encoder和Decoder在一起训练，使用某些代理函数。numerical results show that the proposed Semantic-aware image CS system is superior to traditional ones with fixed measurement matrices.Note: Please note that the translation is in Simplified Chinese, and some words or phrases may have been translated differently in Traditional Chinese.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/07/eess.IV_2023_07_07/" data-id="clollf9dm011lqc881viacr7g" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/07/cs.LG_2023_07_07/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-07-07
        
      </div>
    </a>
  
  
    <a href="/2023/07/06/cs.SD_2023_07_06/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-07-06</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">123</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">123</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">123</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">123</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">113</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">63</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
