
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-07-20 17:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction like Radiologists paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.10824 repo_url: None paper_authors: Jianpeng Zhang, Xianghua Ye, Jianfeng Zh">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-07-20 17:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/07/20/eess.IV_2023_07_20/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction like Radiologists paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.10824 repo_url: None paper_authors: Jianpeng Zhang, Xianghua Ye, Jianfeng Zh">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-19T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:35.305Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_07_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/20/eess.IV_2023_07_20/" class="article-date">
  <time datetime="2023-07-19T16:00:00.000Z" itemprop="datePublished">2023-07-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-07-20 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Parse-and-Recall-Towards-Accurate-Lung-Nodule-Malignancy-Prediction-like-Radiologists"><a href="#Parse-and-Recall-Towards-Accurate-Lung-Nodule-Malignancy-Prediction-like-Radiologists" class="headerlink" title="Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction like Radiologists"></a>Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction like Radiologists</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10824">http://arxiv.org/abs/2307.10824</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianpeng Zhang, Xianghua Ye, Jianfeng Zhang, Yuxing Tang, Minfeng Xu, Jianfei Guo, Xin Chen, Zaiyi Liu, Jingren Zhou, Le Lu, Ling Zhang</li>
<li>for: 这份研究是为了提高肺癌早期检测的精度和准确性。</li>
<li>methods: 这种方法是基于 radiologist-inspired 的，包括 context parsing 和 prototype recalling 两个模组。context parsing 模组首先将 nodule 的 context 结构分解，然后将 contextual information 统计分析，以更好地理解 nodule。prototype recalling 模组使用 prototype-based learning 技术，将先前学习的案例转换为 prototype，并在训练过程中在线更新。</li>
<li>results: 这种方法在两个不同的低剂量和非血痰检查 scenariodemonstrate 了高度的检测性能。<details>
<summary>Abstract</summary>
Lung cancer is a leading cause of death worldwide and early screening is critical for improving survival outcomes. In clinical practice, the contextual structure of nodules and the accumulated experience of radiologists are the two core elements related to the accuracy of identification of benign and malignant nodules. Contextual information provides comprehensive information about nodules such as location, shape, and peripheral vessels, and experienced radiologists can search for clues from previous cases as a reference to enrich the basis of decision-making. In this paper, we propose a radiologist-inspired method to simulate the diagnostic process of radiologists, which is composed of context parsing and prototype recalling modules. The context parsing module first segments the context structure of nodules and then aggregates contextual information for a more comprehensive understanding of the nodule. The prototype recalling module utilizes prototype-based learning to condense previously learned cases as prototypes for comparative analysis, which is updated online in a momentum way during training. Building on the two modules, our method leverages both the intrinsic characteristics of the nodules and the external knowledge accumulated from other nodules to achieve a sound diagnosis. To meet the needs of both low-dose and noncontrast screening, we collect a large-scale dataset of 12,852 and 4,029 nodules from low-dose and noncontrast CTs respectively, each with pathology- or follow-up-confirmed labels. Experiments on several datasets demonstrate that our method achieves advanced screening performance on both low-dose and noncontrast scenarios.
</details>
<details>
<summary>摘要</summary>
肺癌是全球最主要的死亡原因之一，早期检测对生存结果有着关键性。在临床实践中， nodule 的上下文结构和 radiologist 的总体经验是检测 benign 和 malig-nant nodule 的准确性的两个核心元素。上下文信息提供 nodule 的全面信息，如位置、形状和周围血管，经验丰富的 radiologist 可以通过前例来寻找指导，以增强决策的基础。在这篇论文中，我们提出了 radiologist-inspired 方法，它由上下文分析和原型回忆模块组成。上下文分析模块首先将 nodule 的上下文结构分割，然后对 nodule 进行更全面的理解。原型回忆模块利用原型基本学习来压缩已学习的案例，并在训练中进行在线更新。基于这两个模块，我们的方法可以充分利用 nodule 的内在特征和外部知识来实现准确的诊断。为了满足低剂量和非对照检测的需求，我们收集了12,852和4,029个 nodule 的低剂量和非对照 CT 数据，每个数据都有 pathology-或 follow-up-确认的标签。实验表明，我们的方法在低剂量和非对照场景中实现了高级别的检测性能。
</details></li>
</ul>
<hr>
<h2 id="A-novel-integrated-method-of-detection-grasping-for-specific-object-based-on-the-box-coordinate-matching"><a href="#A-novel-integrated-method-of-detection-grasping-for-specific-object-based-on-the-box-coordinate-matching" class="headerlink" title="A novel integrated method of detection-grasping for specific object based on the box coordinate matching"></a>A novel integrated method of detection-grasping for specific object based on the box coordinate matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11783">http://arxiv.org/abs/2307.11783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zongmin Liu, Jirui Wang, Jie Li, Zufeng Li, Kai Ren, Peng Shi</li>
<li>for: 提高服务机器人对老人和残疾人的照顾质量</li>
<li>methods: 提出了一种基于盒坐标匹配的检测-抓取融合方法，包括对SOLOv2实例分 segmentation模型进行修改，并在GR-CNN模型中添加ASPP和CAM模块，以便优化抓取估计。</li>
<li>results: 通过对检测和抓取任务进行分别验证，证明提出的改进模型具有较高的性能。此外，在一个虚拟平台上进行了具体的抓取任务，证明了提出的DG-BCM算法的可行性和有效性。<details>
<summary>Abstract</summary>
To better care for the elderly and disabled, it is essential for service robots to have an effective fusion method of object detection and grasp estimation. However, limited research has been observed on the combination of object detection and grasp estimation. To overcome this technical difficulty, a novel integrated method of detection-grasping for specific object based on the box coordinate matching is proposed in this paper. Firstly, the SOLOv2 instance segmentation model is improved by adding channel attention module (CAM) and spatial attention module (SAM). Then, the atrous spatial pyramid pooling (ASPP) and CAM are added to the generative residual convolutional neural network (GR-CNN) model to optimize grasp estimation. Furthermore, a detection-grasping integrated algorithm based on box coordinate matching (DG-BCM) is proposed to obtain the fusion model of object detection and grasp estimation. For verification, experiments on object detection and grasp estimation are conducted separately to verify the superiority of improved models. Additionally, grasping tasks for several specific objects are implemented on a simulation platform, demonstrating the feasibility and effectiveness of DG-BCM algorithm proposed in this paper.
</details>
<details>
<summary>摘要</summary>
Firstly, the SOLOv2 instance segmentation model is improved by adding channel attention module (CAM) and spatial attention module (SAM). Then, the atrous spatial pyramid pooling (ASPP) and CAM are added to the generative residual convolutional neural network (GR-CNN) model to optimize grasp estimation.Furthermore, a detection-grasping integrated algorithm based on box coordinate matching (DG-BCM) is proposed to obtain the fusion model of object detection and grasp estimation. For verification, experiments on object detection and grasp estimation are conducted separately to verify the superiority of the improved models. Additionally, grasping tasks for several specific objects are implemented on a simulation platform, demonstrating the feasibility and effectiveness of the DG-BCM algorithm proposed in this paper.
</details></li>
</ul>
<hr>
<h2 id="Aggressive-saliency-aware-point-cloud-compression"><a href="#Aggressive-saliency-aware-point-cloud-compression" class="headerlink" title="Aggressive saliency-aware point cloud compression"></a>Aggressive saliency-aware point cloud compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10741">http://arxiv.org/abs/2307.10741</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eleftheria Psatha, Dimitrios Laskos, Gerasimos Arvanitis, Konstantinos Moustakas</li>
<li>for: 提供一种 geometry-based 的端到端压缩方案，用于压缩点云数据，以满足具有准确表示三维场景的需求。</li>
<li>methods: 方法包括分离可见和不可见点云，计算基于点云的几何特征和用户位置的四个吸引地图，并使用这些地图来确定每个点的重要性水平，从而在编码过程中使用不同数量的比特来表示不同区域。</li>
<li>results: 比较研究表明，提出的方法在小比特率下可以获得显著更好的结果，比如MPEG的 geometry-based point cloud compression（G-PCC）算法。<details>
<summary>Abstract</summary>
The increasing demand for accurate representations of 3D scenes, combined with immersive technologies has led point clouds to extensive popularity. However, quality point clouds require a large amount of data and therefore the need for compression methods is imperative. In this paper, we present a novel, geometry-based, end-to-end compression scheme, that combines information on the geometrical features of the point cloud and the user's position, achieving remarkable results for aggressive compression schemes demanding very small bit rates. After separating visible and non-visible points, four saliency maps are calculated, utilizing the point cloud's geometry and distance from the user, the visibility information, and the user's focus point. A combination of these maps results in a final saliency map, indicating the overall significance of each point and therefore quantizing different regions with a different number of bits during the encoding process. The decoder reconstructs the point cloud making use of delta coordinates and solving a sparse linear system. Evaluation studies and comparisons with the geometry-based point cloud compression (G-PCC) algorithm by the Moving Picture Experts Group (MPEG), carried out for a variety of point clouds, demonstrate that the proposed method achieves significantly better results for small bit rates.
</details>
<details>
<summary>摘要</summary>
随着三维场景的准确表示需求的增加，加上 immerse 技术的普及，点云在现场得到了广泛的应用。然而，高质量点云需要大量数据，因此压缩方法的需求是急需的。在这篇论文中，我们提出了一种新的、基于几何特征的、端到端压缩方案，该方案结合点云的几何特征和用户位置信息，实现了非常出色的压缩效果，尤其是在具有非常小的比特率的情况下。我们首先将可见和不可见点分开，然后计算四个重要度地图，利用点云的几何特征、用户位置信息和用户注意点信息。这些地图的组合得到了最终的重要度地图，该地图指示每个点的重要程度，因此在编码过程中对不同区域进行不同数量的比特数量化。解码器使用delta坐标和解决一个稀疏线性系统来重建点云。我们对一系列点云进行了评估和与基于几何特征的点云压缩算法（G-PCC）由国际电影专家组织（MPEG）提出的方法进行了比较，结果显示，我们的方法在小比特率情况下得到了显著更好的效果。
</details></li>
</ul>
<hr>
<h2 id="Prediction-of-sunflower-leaf-area-at-vegetative-stage-by-image-analysis-and-application-to-the-estimation-of-water-stress-response-parameters-in-post-registration-varieties"><a href="#Prediction-of-sunflower-leaf-area-at-vegetative-stage-by-image-analysis-and-application-to-the-estimation-of-water-stress-response-parameters-in-post-registration-varieties" class="headerlink" title="Prediction of sunflower leaf area at vegetative stage by image analysis and application to the estimation of water stress response parameters in post-registration varieties"></a>Prediction of sunflower leaf area at vegetative stage by image analysis and application to the estimation of water stress response parameters in post-registration varieties</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11110">http://arxiv.org/abs/2307.11110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Casadebaig, Nicolas Blanchet, Nicolas Bernard Langlade<br>for:本研究旨在提供一种自动测量阳光花的发育和生理响应，以便为农民选择最适合的品种，以及为了了解植物对环境的响应的生物、遗传和分子基础。methods:在INRAE Toulouse的Heliaphen高通量现场测量平台上，我们设计了两个实验，每个实验有8种植物（2*96植物），并通过光栅获取了不同水压强度下植物的图像，并在每次评估期间手动测量植物的叶面积。results:通过分析图像，我们提取了植物的形态特征，并评估了不同模型以估算植物总叶面积。使用了一种线性模型和后续平滑处理，我们可以Relative squared error of 11%和效率93%来估算植物总叶面积。这些估算值与手动测量值相比较，显示了图像基于叶面积估算的方法的可靠性。此外，我们还计算了植物的叶展和营养响应（LE和TR），并与手动测量值进行比较。结果显示，图像基于叶面积估算的LE和TR参数估算值与手动测量值呈0.61和0.81的相关系数，表明这些参数可以用于模拟。<details>
<summary>Abstract</summary>
The automatic measurement of developmental and physiological responses of sunflowers to water stress represents an applied challenge for a better knowledge of the varieties available to growers, but also a fundamental one for identifying the biological, genetic and molecular bases of plant response to their environment.On INRAE Toulouse's Heliaphen high-throughput phenotyping platform, we set up two experiments, each with 8 varieties (2*96 plants), and acquired images of plants subjected or not to water stress, using a light barrier on a daily basis. At the same time, we manually measured the leaf surfaces of these plants every other day for the duration of the stress, which lasted around ten days. The images were analyzed to extract morphological characteristics of the segmented plants and different models were evaluated to estimate total plant leaf areas using these data.A linear model with a posteriori smoothing was used to estimate total leaf area with a relative squared error of 11% and an efficiency of 93%. Leaf areas estimated conventionally or with the developed model were used to calculate the leaf expansion and transpiration responses (LER and TR) used in the SUNFLO crop model for 8 sunflower varieties studied. Correlation coefficients of 0.61 and 0.81 for LER and TR respectively validate the use of image-based leaf area estimation. However, the estimated values for LER are lower than for the manual method on Heliaphen, but closer overall to the manual method on greenhouse-grown plants, potentially suggesting an overestimation of stress sensitivity.It can be concluded that the LE and TR parameter estimates can be used for simulations. The low cost of this method (compared with manual measurements), the possibility of parallelizing and repeating measurements on the Heliaphen platform, and of benefiting from the Heliaphen platform's data management, are major improvements for valorizing the SUNFLO model and characterizing the drought sensitivity of cultivated varieties.
</details>
<details>
<summary>摘要</summary>
自动测量阳光花的发展和生理响应对水压力是一种应用和基础研究，可以帮助选择有效的种植者，同时也可以更好地了解植物对环境的响应。在国家农业研究所图卢兹分所的Heliaphen高通量fenotyping平台上，我们设计了两个实验，每个实验有8种(2*96植物)，并在日常基础上获取了水压力和不受水压力影响的植物图像，使用了光栅。同时，我们手动测量了这些植物的叶表面每两天，持续约10天。图像分析后，我们使用了不同的模型来估算植物总叶面积。我们使用了一种线性模型，并在后续的滑动平均处理中估算了总叶面积，得到的Relative squared error为11%，效率为93%。这些估算的叶面积值被用来计算植物叶展和蒸发响应（LER和TR），并被用于SUNFLO作物模型中。 corr coefficient为0.61和0.81， validate了使用图像基于叶面积估算。 although the estimated LER values were lower than those obtained by the manual method on Heliaphen, they were closer to the manual method on greenhouse-grown plants, suggesting that the image-based method may overestimate stress sensitivity. Therefore, the LE and TR parameter estimates can be used for simulations, and the low cost of this method, the possibility of parallelizing and repeating measurements on the Heliaphen platform, and the benefits of using the Heliaphen platform's data management, are major improvements for valorizing the SUNFLO model and characterizing the drought sensitivity of cultivated varieties.
</details></li>
</ul>
<hr>
<h2 id="Depth-from-Defocus-Technique-A-Simple-Calibration-Free-Approach-for-Dispersion-Size-Measurement"><a href="#Depth-from-Defocus-Technique-A-Simple-Calibration-Free-Approach-for-Dispersion-Size-Measurement" class="headerlink" title="Depth from Defocus Technique: A Simple Calibration-Free Approach for Dispersion Size Measurement"></a>Depth from Defocus Technique: A Simple Calibration-Free Approach for Dispersion Size Measurement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10678">http://arxiv.org/abs/2307.10678</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saini Jatin Rao, Shubham Sharma, Saptarshi Basu, Cameron Tropea</li>
<li>for: 这个论文主要用于探讨了一种基于图像技术的粒子大小和位势测量方法，用于应用于多种场景，如涂抹液滴粒子大小、多相流中粒子物质的跟踪以及机器视觉系统中的目标标记检测。</li>
<li>methods: 该方法基于’深度 FROM 杂谱’（DFD）技术，使用单 Camera 设置，并且有一个简单的光学配置和简单的准备过程，使得这种方法可以在更广泛的应用中使用。</li>
<li>results: 该方法可以准确地测量粒子的大小和位势，并且可以在不同的环境下进行测量。<details>
<summary>Abstract</summary>
Dispersed particle size measurement is crucial in a variety of applications, be it in the sizing of spray droplets, tracking of particulate matter in multiphase flows, or the detection of target markers in machine vision systems. Further to sizing, such systems are characterised by extracting quantitative information like spatial position and associated velocity of the dispersed phase particles. In the present study we propose an imaging based volumetric measurement approach for estimating the size and position of spherically dispersed particles. The approach builds on the 'Depth from Defocus' (DFD) technique using a single camera approach. The simple optical configuration, consisting of a shadowgraph setup and a straightforward calibration procedure, makes this method readily deployable and accessible for broader applications.
</details>
<details>
<summary>摘要</summary>
粒子大小分布测量在多种应用中非常重要，包括涂抹液滴大小测量、多相流体中固体粒子的跟踪以及机器视觉系统中目标标记的检测。此外，这些系统还可以提取量化信息，如粒子颗粒的空间位置和相关速度。在 presente 研究中，我们提出了一种基于图像测量的粒子大小和位置估算方法。该方法基于 '深度从杂化'（DFD）技术，使用单个摄像头设计。该简单的光学配置和简单的准备过程，使得这种方法可以广泛应用和易于部署。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Unified-Demosaicing-for-Bayer-and-Non-Bayer-Patterned-Image-Sensors"><a href="#Efficient-Unified-Demosaicing-for-Bayer-and-Non-Bayer-Patterned-Image-Sensors" class="headerlink" title="Efficient Unified Demosaicing for Bayer and Non-Bayer Patterned Image Sensors"></a>Efficient Unified Demosaicing for Bayer and Non-Bayer Patterned Image Sensors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10667">http://arxiv.org/abs/2307.10667</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haechang Lee, Dongwon Park, Wongi Jeong, Kijeong Kim, Hyunwoo Je, Dongil Ryu, Se Young Chun</li>
<li>for: 本研究旨在提出一种能够应对不同光照条件下的非拜尔摄像头感知器（CMOS）的高效掺杂重建方法，以提高摄像头的性能和灵活性。</li>
<li>methods: 该方法基于知识学习，使用了CFA-adaptive筛选器，只需要1%的关键筛选器来满足不同CFA模式下的掺杂重建需求。</li>
<li>results: 对于 synthetic 和实际拍摄的RAW数据，该方法实现了与大规模模型相当的掺杂重建性能，并且通过在掺杂重建过程中使用元学习（KLAP-M），可以有效消除不同摄像头的杂质特征。<details>
<summary>Abstract</summary>
As the physical size of recent CMOS image sensors (CIS) gets smaller, the latest mobile cameras are adopting unique non-Bayer color filter array (CFA) patterns (e.g., Quad, Nona, QxQ), which consist of homogeneous color units with adjacent pixels. These non-Bayer sensors are superior to conventional Bayer CFA thanks to their changeable pixel-bin sizes for different light conditions but may introduce visual artifacts during demosaicing due to their inherent pixel pattern structures and sensor hardware characteristics. Previous demosaicing methods have primarily focused on Bayer CFA, necessitating distinct reconstruction methods for non-Bayer patterned CIS with various CFA modes under different lighting conditions. In this work, we propose an efficient unified demosaicing method that can be applied to both conventional Bayer RAW and various non-Bayer CFAs' RAW data in different operation modes. Our Knowledge Learning-based demosaicing model for Adaptive Patterns, namely KLAP, utilizes CFA-adaptive filters for only 1% key filters in the network for each CFA, but still manages to effectively demosaic all the CFAs, yielding comparable performance to the large-scale models. Furthermore, by employing meta-learning during inference (KLAP-M), our model is able to eliminate unknown sensor-generic artifacts in real RAW data, effectively bridging the gap between synthetic images and real sensor RAW. Our KLAP and KLAP-M methods achieved state-of-the-art demosaicing performance in both synthetic and real RAW data of Bayer and non-Bayer CFAs.
</details>
<details>
<summary>摘要</summary>
为了适应现代CMOS图像感知器（CIS）的尺寸逐渐减小，现场摄像头开始采用不同的非拜尔颜色筛子阵（CFA）模式（例如quad、non、QxQ），这些非拜尔筛子阵由相邻的像素组成同色单元。相比传统的拜尔筛子阵，这些非拜尔筛子阵具有可变像素宽度，适应不同的照明条件，但可能会在拼接过程中产生视觉artefacts，这是因为其内置的像素结构和感知器硬件特性。现有的拼接方法主要针对拜尔筛子阵，需要不同的重建方法 для不同的CFA模式和照明条件。在这项工作中，我们提出了一种高效的统一拼接方法，可以应用于拜尔RAW和不同的非拜尔CFAs的RAW数据中。我们的知识学习基于模板（KLAP）使用了CFA适应的筛子 filters，只需要1%的关键筛子在网络中，仍能有效拼接所有CFAs，与大规模模型相当。此外，通过在推理中使用元学习（KLAP-M），我们的模型可以减少真实感知器特有的未知遗传artefacts，有效地跨越真实图像和Synthetic图像之间的差异。我们的KLAP和KLAP-M方法在Synthetic和实际RAW数据中实现了拼接性能的状态计算。
</details></li>
</ul>
<hr>
<h2 id="Physics-Driven-Turbulence-Image-Restoration-with-Stochastic-Refinement"><a href="#Physics-Driven-Turbulence-Image-Restoration-with-Stochastic-Refinement" class="headerlink" title="Physics-Driven Turbulence Image Restoration with Stochastic Refinement"></a>Physics-Driven Turbulence Image Restoration with Stochastic Refinement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10603">http://arxiv.org/abs/2307.10603</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vita-group/pirn">https://github.com/vita-group/pirn</a></li>
<li>paper_authors: Ajay Jaiswal, Xingguang Zhang, Stanley H. Chan, Zhangyang Wang</li>
<li>for: 这个论文是为了解决长距离光学感知系统中的大气扰动引起的图像扭曲问题。</li>
<li>methods: 这篇论文提出了将物理基础的模拟器直接 integrate到网络训练过程中，以帮助网络分离随机性和扭曲以及下面的图像。此外，为了解决“平均效应”和模拟数据和实际世界扰动之间的领域差异，我们还提出了PiRN-SR，以提高感知质量。</li>
<li>results: 我们的PiRN和PiRN-SR可以提高对实际世界未知扰动条件的总体化和提供当前最高的修复精度和感知质量。我们的代码可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/VITA-Group/PiRN%7D">https://github.com/VITA-Group/PiRN}</a> 上获取。<details>
<summary>Abstract</summary>
Image distortion by atmospheric turbulence is a stochastic degradation, which is a critical problem in long-range optical imaging systems. A number of research has been conducted during the past decades, including model-based and emerging deep-learning solutions with the help of synthetic data. Although fast and physics-grounded simulation tools have been introduced to help the deep-learning models adapt to real-world turbulence conditions recently, the training of such models only relies on the synthetic data and ground truth pairs. This paper proposes the Physics-integrated Restoration Network (PiRN) to bring the physics-based simulator directly into the training process to help the network to disentangle the stochasticity from the degradation and the underlying image. Furthermore, to overcome the ``average effect" introduced by deterministic models and the domain gap between the synthetic and real-world degradation, we further introduce PiRN with Stochastic Refinement (PiRN-SR) to boost its perceptual quality. Overall, our PiRN and PiRN-SR improve the generalization to real-world unknown turbulence conditions and provide a state-of-the-art restoration in both pixel-wise accuracy and perceptual quality. Our codes are available at \url{https://github.com/VITA-Group/PiRN}.
</details>
<details>
<summary>摘要</summary>
图像扭曲因大气扰动是一种随机干扰，对远距离光学图像系统来说是一个关键问题。过去几十年内，有多种研究进行了，包括基于模型的和新兴深度学习解决方案，使用了 sintetic 数据。虽然最近引入了快速基于物理的 simulator 来帮助深度学习模型适应实际大气扰动条件，但是这些模型的训练仅仅基于 sintetic 数据和真实数据对。这篇论文提出了 integrating 物理Restoration 网络（PiRN），将物理基于的 simulator 直接引入到训练过程中，帮助网络分离随机性和扰动以及真实图像。此外，为了超越 deterministic 模型的“平均效应”和 sintetic 和实际扰动之间的领域差异，我们进一步引入 PiRN with Stochastic Refinement（PiRN-SR），提高其 perceived 质量。总的来说，我们的 PiRN 和 PiRN-SR 在实际未知大气扰动条件下提高了总体化能力和 perceived 质量，成为当前最佳的Restoration。我们的代码可以在 \url{https://github.com/VITA-Group/PiRN} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Is-Grad-CAM-Explainable-in-Medical-Images"><a href="#Is-Grad-CAM-Explainable-in-Medical-Images" class="headerlink" title="Is Grad-CAM Explainable in Medical Images?"></a>Is Grad-CAM Explainable in Medical Images?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10506">http://arxiv.org/abs/2307.10506</a></li>
<li>repo_url: None</li>
<li>paper_authors: Subhashis Suara, Aayush Jha, Pratik Sinha, Arif Ahmed Sekh</li>
<li>For: The paper is written for the field of artificial intelligence (AI) and medical imaging, with a focus on increasing interpretability and trust in deep learning models for effective diagnosis and treatment planning.* Methods: The paper explores the principles of Explainable Deep Learning and its relevance to medical imaging, discusses various explainability techniques and their limitations, and examines medical imaging applications of Grad-CAM.* Results: The findings highlight the potential of Explainable Deep Learning and Grad-CAM in improving the accuracy and interpretability of deep learning models in medical imaging.Here’s the Chinese version of the three information points:* For: 这篇论文是为人工智能（AI）和医疗影像领域写的，旨在提高深度学习模型的准确性和可解释性，以便更好地诊断和规划治疗。* Methods: 论文探讨了可解释深度学习的原则和它在医疗影像领域的应用，讨论了不同的解释技术和其局限性，并对Grad-CAM在医疗影像应用进行了检查。* Results: 结果表明可解释深度学习和Grad-CAM在医疗影像领域可以提高深度学习模型的准确性和可解释性。I hope that helps!<details>
<summary>Abstract</summary>
Explainable Deep Learning has gained significant attention in the field of artificial intelligence (AI), particularly in domains such as medical imaging, where accurate and interpretable machine learning models are crucial for effective diagnosis and treatment planning. Grad-CAM is a baseline that highlights the most critical regions of an image used in a deep learning model's decision-making process, increasing interpretability and trust in the results. It is applied in many computer vision (CV) tasks such as classification and explanation. This study explores the principles of Explainable Deep Learning and its relevance to medical imaging, discusses various explainability techniques and their limitations, and examines medical imaging applications of Grad-CAM. The findings highlight the potential of Explainable Deep Learning and Grad-CAM in improving the accuracy and interpretability of deep learning models in medical imaging. The code is available in (will be available).
</details>
<details>
<summary>摘要</summary>
<<SYS>> Deep Learning 可 explainer 在人工智能领域内得到了广泛关注，特别是在医学影像领域，因为 Deep Learning 模型的准确性和可解释性对于诊断和治疗规划是非常重要的。 Grad-CAM 是一个基准，可以帮助找出 Deep Learning 模型在做出决策时使用的最重要的图像区域，从而增加模型的可解释性和信任度。 它在许多计算机视觉（CV）任务中使用，如分类和解释。本研究探讨了 Explainable Deep Learning 的原则和医学影像领域的相关性，讨论了不同的解释技术和其限制，并检查了 Grad-CAM 在医学影像应用中的效果。发现结果表明，Explainable Deep Learning 和 Grad-CAM 在医学影像领域可以提高 Deep Learning 模型的准确性和可解释性。代码将在 (will be available) 上提供。>>I hope this helps! Let me know if you have any further questions or if there's anything else I can help with.
</details></li>
</ul>
<hr>
<h2 id="Metaverse-A-Young-Gamer’s-Perspective"><a href="#Metaverse-A-Young-Gamer’s-Perspective" class="headerlink" title="Metaverse: A Young Gamer’s Perspective"></a>Metaverse: A Young Gamer’s Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10439">http://arxiv.org/abs/2307.10439</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ivan V. Bajić, Teo Saeedi-Bajić, Kai Saeedi-Bajić</li>
<li>for: 这个论文旨在了解10岁以下儿童对Metaverse的需求和要求，以便在开发Metaverse技术时能更好地了解这个年轻的用户群体。</li>
<li>methods: 这篇论文使用了年轻游戏玩家的视角来探讨Metaverse，包括他们对Metaverse和物理世界之间的关系，以及他们对Metaverse的期望。</li>
<li>results: 这篇论文提出了一些关于Metaverse技术研发的挑战，以及年轻用户对这些技术的期望。这些结论可能有助于制定更详细的主观实验，以及对Metaverse技术的研发。<details>
<summary>Abstract</summary>
When developing technologies for the Metaverse, it is important to understand the needs and requirements of end users. Relatively little is known about the specific perspectives on the use of the Metaverse by the youngest audience: children ten and under. This paper explores the Metaverse from the perspective of a young gamer. It examines their understanding of the Metaverse in relation to the physical world and other technologies they may be familiar with, looks at some of their expectations of the Metaverse, and then relates these to the specific multimedia signal processing (MMSP) research challenges. The perspectives presented in the paper may be useful for planning more detailed subjective experiments involving young gamers, as well as informing the research on MMSP technologies targeted at these users.
</details>
<details>
<summary>摘要</summary>
When developing technologies for the Metaverse, it is important to understand the needs and requirements of end users. Relatively little is known about the specific perspectives on the use of the Metaverse by the youngest audience: children ten and under. This paper explores the Metaverse from the perspective of a young gamer. It examines their understanding of the Metaverse in relation to the physical world and other technologies they may be familiar with, looks at some of their expectations of the Metaverse, and then relates these to the specific multimedia signal processing (MMSP) research challenges. The perspectives presented in the paper may be useful for planning more detailed subjective experiments involving young gamers, as well as informing the research on MMSP technologies targeted at these users.Here's the translation in Traditional Chinese:当开发Metaverse技术时，了解使用者的需求和要求是非常重要。目前对最年轻的使用者——十岁以下的儿童的Metaverse使用情况所知甚少。本文从年轻的游戏者的角度出发，探讨Metaverse与物理世界以及他们可能熟悉的其他技术之间的关系，并考虑他们对Metaverse的期望，最后将这些观点与多媒体信号处理（MMSP）技术研究挑战相关。文中的观点可能对进行更详细的主观实验 involving young gamers 有所帮助，也可以帮助MMSP技术的研究，targeted at these users。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Latent-Autoencoder-with-Self-Attention-for-Structural-Image-Synthesis"><a href="#Adversarial-Latent-Autoencoder-with-Self-Attention-for-Structural-Image-Synthesis" class="headerlink" title="Adversarial Latent Autoencoder with Self-Attention for Structural Image Synthesis"></a>Adversarial Latent Autoencoder with Self-Attention for Structural Image Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10166">http://arxiv.org/abs/2307.10166</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiajie Fan, Laure Vuaille, Hao Wang, Thomas Bäck</li>
<li>for: This paper is written for facilitating industrial engineering processes using Generative Engineering Design approaches driven by Deep Generative Models (DGM).</li>
<li>methods: The paper proposes a novel model called Self-Attention Adversarial Latent Autoencoder (SA-ALAE) to generate feasible design images of complex engineering parts.</li>
<li>results: The proposed SA-ALAE model allows users to explore novel variants of an existing design and control the generation process by operating in latent space. The potential of SA-ALAE is demonstrated through generating engineering blueprints in a real automotive design task.<details>
<summary>Abstract</summary>
Generative Engineering Design approaches driven by Deep Generative Models (DGM) have been proposed to facilitate industrial engineering processes. In such processes, designs often come in the form of images, such as blueprints, engineering drawings, and CAD models depending on the level of detail. DGMs have been successfully employed for synthesis of natural images, e.g., displaying animals, human faces and landscapes. However, industrial design images are fundamentally different from natural scenes in that they contain rich structural patterns and long-range dependencies, which are challenging for convolution-based DGMs to generate. Moreover, DGM-driven generation process is typically triggered based on random noisy inputs, which outputs unpredictable samples and thus cannot perform an efficient industrial design exploration. We tackle these challenges by proposing a novel model Self-Attention Adversarial Latent Autoencoder (SA-ALAE), which allows generating feasible design images of complex engineering parts. With SA-ALAE, users can not only explore novel variants of an existing design, but also control the generation process by operating in latent space. The potential of SA-ALAE is shown by generating engineering blueprints in a real automotive design task.
</details>
<details>
<summary>摘要</summary>
生成工程设计方法，驱动深度生成模型（DGM），已经提议用于促进工程设计过程。在这些过程中，设计通常以图像形式出现，如蓝图、工程图纸和CAD模型，具体取决于级别。DGM已经成功应用于自然图像的合成，例如显示动物、人脸和风景。然而，工程设计图像与自然场景不同，它们具有丰富的结构征特和长距离依赖关系，这些关系使得混合基于的DGM难以生成。此外，DGM驱动的生成过程通常是基于随机噪音输入触发的，这会输出不可预测的样本，因此无法实现有效的工程设计探索。我们解决这些挑战，提出了一种新的模型，即Self-Attention Adversarial Latent Autoencoder（SA-ALAE）。SA-ALAE允许生成复杂工程部件的可能性。用户不仅可以探索现有设计的新变体，还可以在幂空间控制生成过程。我们在一个真实的汽车设计任务中展示了SA-ALAE的潜力。
</details></li>
</ul>
<hr>
<h2 id="Make-A-Volume-Leveraging-Latent-Diffusion-Models-for-Cross-Modality-3D-Brain-MRI-Synthesis"><a href="#Make-A-Volume-Leveraging-Latent-Diffusion-Models-for-Cross-Modality-3D-Brain-MRI-Synthesis" class="headerlink" title="Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D Brain MRI Synthesis"></a>Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D Brain MRI Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10094">http://arxiv.org/abs/2307.10094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lingting Zhu, Zeyue Xue, Zhenchao Jin, Xian Liu, Jingzhen He, Ziwei Liu, Lequan Yu</li>
<li>for: 这个研究的目的是提出一个新的多modal医疗影像合成方法，以便在医疗影像领域中实现多种应用。</li>
<li>methods: 这个方法使用了2D backbone和一个扩散基础的框架，named Make-A-Volume，来进行跨modal 3D医疗影像合成。它使用了一个潜在的扩散模型来学习跨modal的slice-wise mapping，并且在2D slice-mapping模型中插入了一系列的类型层来缓和频率矩阵的问题。</li>
<li>results: 实验结果显示，这个Make-A-Volume框架可以实现高效的多modal医疗影像合成，并且可以保持体积一致性。<details>
<summary>Abstract</summary>
Cross-modality medical image synthesis is a critical topic and has the potential to facilitate numerous applications in the medical imaging field. Despite recent successes in deep-learning-based generative models, most current medical image synthesis methods rely on generative adversarial networks and suffer from notorious mode collapse and unstable training. Moreover, the 2D backbone-driven approaches would easily result in volumetric inconsistency, while 3D backbones are challenging and impractical due to the tremendous memory cost and training difficulty. In this paper, we introduce a new paradigm for volumetric medical data synthesis by leveraging 2D backbones and present a diffusion-based framework, Make-A-Volume, for cross-modality 3D medical image synthesis. To learn the cross-modality slice-wise mapping, we employ a latent diffusion model and learn a low-dimensional latent space, resulting in high computational efficiency. To enable the 3D image synthesis and mitigate volumetric inconsistency, we further insert a series of volumetric layers in the 2D slice-mapping model and fine-tune them with paired 3D data. This paradigm extends the 2D image diffusion model to a volumetric version with a slightly increasing number of parameters and computation, offering a principled solution for generic cross-modality 3D medical image synthesis. We showcase the effectiveness of our Make-A-Volume framework on an in-house SWI-MRA brain MRI dataset and a public T1-T2 brain MRI dataset. Experimental results demonstrate that our framework achieves superior synthesis results with volumetric consistency.
</details>
<details>
<summary>摘要</summary>
跨模态医疗图像合成是一个关键问题，它具有潜在的应用前景在医疗图像领域。虽然最近的深度学习生成模型已经取得了一定的成功，但大多数当前的医疗图像合成方法仍然基于生成对抗网络，它们受到模式坍缩和训练不稳定的问题困扰。此外，2D脊梁驱动的方法容易导致立体不一致，而3D脊梁却受到巨大的存储成本和训练困难。在本文中，我们提出一种新的方法来合成3D医疗图像，通过利用2D脊梁来实现。我们提出了一种扩展2D图像扩散模型到3D版本的方法，并通过在2D层次映射模型中插入多个立体层来实现3D图像合成。我们采用了潜在空间的扩散模型来学习跨模态的片断 wise 映射，从而实现高效的计算。我们在自有的SWI-MRA脑MRI数据集和公共的T1-T2脑MRI数据集上展示了我们的Make-A-Volume框架的效果，结果表明我们的方法可以实现高质量的合成结果，同时保持立体一致。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/20/eess.IV_2023_07_20/" data-id="cllsj1rp40081pf88agv7dv0l" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/20/cs.SD_2023_07_20/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.SD - 2023-07-20 123:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/19/cs.LG_2023_07_19/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-07-19 18:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'', root:''}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
