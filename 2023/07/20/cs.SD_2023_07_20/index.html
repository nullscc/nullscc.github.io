
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-07-20 123:00:00 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Transfer Learning and Bias Correction with Pre-trained Audio Embeddings paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.10834 repo_url: https:&#x2F;&#x2F;github.com&#x2F;changhongw&#x2F;audio-embedding-bias paper_authors: Changhong">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-07-20 123:00:00">
<meta property="og:url" content="http://example.com/2023/07/20/cs.SD_2023_07_20/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Transfer Learning and Bias Correction with Pre-trained Audio Embeddings paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.10834 repo_url: https:&#x2F;&#x2F;github.com&#x2F;changhongw&#x2F;audio-embedding-bias paper_authors: Changhong">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-19T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:35.275Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_07_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/20/cs.SD_2023_07_20/" class="article-date">
  <time datetime="2023-07-19T16:00:00.000Z" itemprop="datePublished">2023-07-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-07-20 123:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Transfer-Learning-and-Bias-Correction-with-Pre-trained-Audio-Embeddings"><a href="#Transfer-Learning-and-Bias-Correction-with-Pre-trained-Audio-Embeddings" class="headerlink" title="Transfer Learning and Bias Correction with Pre-trained Audio Embeddings"></a>Transfer Learning and Bias Correction with Pre-trained Audio Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10834">http://arxiv.org/abs/2307.10834</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/changhongw/audio-embedding-bias">https://github.com/changhongw/audio-embedding-bias</a></li>
<li>paper_authors: Changhong Wang, Gaël Richard, Brian McFee</li>
<li>for: 这篇论文的目的是研究预训练的音频表示中的偏见现象，以及如何通过后处理来减轻这些偏见的影响。</li>
<li>methods: 这篇论文使用了三种不同的预训练表示（VGGish、OpenL3和YAMNet），并对这些表示进行了比较，以评估它们在不同数据集上的性能。它们还研究了数据集Identify和种类分布是否会影响表示的偏见。</li>
<li>results: 研究发现，不同的预训练表示在不同数据集上的性能有很大差异，而且这些差异与数据集的Identify和种类分布有关。此外，研究还提出了一些后处理方法，以减轻预训练表示中的偏见影响，并提高数据集之间的泛化性能。<details>
<summary>Abstract</summary>
Deep neural network models have become the dominant approach to a large variety of tasks within music information retrieval (MIR). These models generally require large amounts of (annotated) training data to achieve high accuracy. Because not all applications in MIR have sufficient quantities of training data, it is becoming increasingly common to transfer models across domains. This approach allows representations derived for one task to be applied to another, and can result in high accuracy with less stringent training data requirements for the downstream task. However, the properties of pre-trained audio embeddings are not fully understood. Specifically, and unlike traditionally engineered features, the representations extracted from pre-trained deep networks may embed and propagate biases from the model's training regime. This work investigates the phenomenon of bias propagation in the context of pre-trained audio representations for the task of instrument recognition. We first demonstrate that three different pre-trained representations (VGGish, OpenL3, and YAMNet) exhibit comparable performance when constrained to a single dataset, but differ in their ability to generalize across datasets (OpenMIC and IRMAS). We then investigate dataset identity and genre distribution as potential sources of bias. Finally, we propose and evaluate post-processing countermeasures to mitigate the effects of bias, and improve generalization across datasets.
</details>
<details>
<summary>摘要</summary>
We first demonstrate that three different pre-trained representations (VGGish, OpenL3, and YAMNet) exhibit comparable performance when constrained to a single dataset, but differ in their ability to generalize across datasets (OpenMIC and IRMAS). We then investigate dataset identity and genre distribution as potential sources of bias. Finally, we propose and evaluate post-processing countermeasures to mitigate the effects of bias, and improve generalization across datasets.Here is the translation in Simplified Chinese:深度神经网络模型已成为Music信息检索（MIR）中主要的方法。这些模型通常需要大量（注解）训练数据以达到高精度。然而，不是所有MIR应用程序具有足够的训练数据，因此正在越来越常将模型转移到另一个领域。这种方法允许 representations derived for one task 被应用到另一个任务上，并可以通过使用 less stringent 的训练数据要求来实现高精度。然而，预训练音频表示的特性没有完全理解。 Specifically, the representations extracted from pre-trained deep networks may embed and propagate biases from the model's training regime. This work investigates the phenomenon of bias propagation in the context of pre-trained audio representations for the task of instrument recognition.我们首先示出了三个不同的预训练表示（VGGish, OpenL3, and YAMNet）在单个数据集上受限制时的相似性，但是它们在不同的数据集（OpenMIC和IRMAS）上的泛化能力不同。然后，我们调查了数据集标识和类型分布作为可能的偏见源。最后，我们提出并评估了后处理措施来 Mitigate the effects of bias，并提高数据集之间的泛化能力。
</details></li>
</ul>
<hr>
<h2 id="Cross-Corpus-Multilingual-Speech-Emotion-Recognition-Amharic-vs-Other-Languages"><a href="#Cross-Corpus-Multilingual-Speech-Emotion-Recognition-Amharic-vs-Other-Languages" class="headerlink" title="Cross-Corpus Multilingual Speech Emotion Recognition: Amharic vs. Other Languages"></a>Cross-Corpus Multilingual Speech Emotion Recognition: Amharic vs. Other Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10814">http://arxiv.org/abs/2307.10814</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ephrem Afele Retta, Richard Sutcliffe, Jabar Mahmood, Michael Abebe Berwo, Eiad Almekhlafi, Sajjad Ahmed Khan, Shehzad Ashraf Chaudhry, Mustafa Mhamed, Jun Feng</li>
<li>for: 这个论文的目的是研究跨语言和多语言情绪识别（SER）任务，以及如何使用不同语言的数据进行训练。</li>
<li>methods: 这个论文使用了AlexNet、VGGE（一种修改后的VGG模型）和ResNet50三种模型进行实验，并将所有数据集映射到只有两个类别（正面和负面）上。</li>
<li>results: 结果表明，使用英语或德语作为源语言，并使用阿姆哈里语作为目标语言可以获得最佳结果（在Experiment 2中）。此外，使用多种非阿姆哈里语言进行训练，然后测试在阿姆哈里语上的结果比使用单一非阿姆哈里语言进行训练更好（在Experiment 3中）。总的来说，结果表明，跨语言和多语言训练是缺乏语言资源时的有效策略。<details>
<summary>Abstract</summary>
In a conventional Speech emotion recognition (SER) task, a classifier for a given language is trained on a pre-existing dataset for that same language. However, where training data for a language does not exist, data from other languages can be used instead. We experiment with cross-lingual and multilingual SER, working with Amharic, English, German and URDU. For Amharic, we use our own publicly-available Amharic Speech Emotion Dataset (ASED). For English, German and Urdu we use the existing RAVDESS, EMO-DB and URDU datasets. We followed previous research in mapping labels for all datasets to just two classes, positive and negative. Thus we can compare performance on different languages directly, and combine languages for training and testing. In Experiment 1, monolingual SER trials were carried out using three classifiers, AlexNet, VGGE (a proposed variant of VGG), and ResNet50. Results averaged for the three models were very similar for ASED and RAVDESS, suggesting that Amharic and English SER are equally difficult. Similarly, German SER is more difficult, and Urdu SER is easier. In Experiment 2, we trained on one language and tested on another, in both directions for each pair: Amharic<->German, Amharic<->English, and Amharic<->Urdu. Results with Amharic as target suggested that using English or German as source will give the best result. In Experiment 3, we trained on several non-Amharic languages and then tested on Amharic. The best accuracy obtained was several percent greater than the best accuracy in Experiment 2, suggesting that a better result can be obtained when using two or three non-Amharic languages for training than when using just one non-Amharic language. Overall, the results suggest that cross-lingual and multilingual training can be an effective strategy for training a SER classifier when resources for a language are scarce.
</details>
<details>
<summary>摘要</summary>
传统的语音情感识别（SER）任务中，一个分类器会被训练在某种语言的已有数据集上。但是当语言的训练数据不存在时，可以使用其他语言的数据 instead。我们在埃塞俄比亚语、英语、德语和乌尔都语之间进行了交叉语言和多语言 SER 实验。对于埃塞俄比亚语，我们使用了我们自己公开的埃塞俄比亚语 Speech Emotion Dataset (ASED)。对于英语、德语和乌尔都语，我们使用了现有的 RAVDESS、EMO-DB 和 URDU 数据集。我们按照之前的研究方法将所有数据集的标签映射到两个类别：正面和负面。因此，我们可以直接比较不同语言的性能，并将语言组合在训练和测试中。在实验1中，我们使用了三个模型：AlexNet、VGGE 和 ResNet50，进行了各自的MONOLINGUAL SER 实验。结果显示，ASED 和 RAVDESS 的结果几乎相同， suggesting that Amharic and English SER are equally difficult. Similarly, German SER is more difficult, and Urdu SER is easier.在实验2中，我们将一种语言作为目标语言，并将另一种语言作为来源语言，在两个方向上进行了每个对。结果表明，使用英语或德语作为来源语言，对埃塞俄比亚语作为目标语言的结果最佳。在实验3中，我们将多种非埃塞俄比亚语言作为训练语言，然后对埃塞俄比亚语进行测试。最佳的结果比实验2中的最佳结果高出几个百分点，表明使用两或三种非埃塞俄比亚语言进行训练可以获得更好的结果。总之，结果表明，交叉语言和多语言训练是缺乏语言资源时的效果的 SER 分类器训练策略。
</details></li>
</ul>
<hr>
<h2 id="Perceptual-Quality-Assessment-of-Omnidirectional-Audio-visual-Signals"><a href="#Perceptual-Quality-Assessment-of-Omnidirectional-Audio-visual-Signals" class="headerlink" title="Perceptual Quality Assessment of Omnidirectional Audio-visual Signals"></a>Perceptual Quality Assessment of Omnidirectional Audio-visual Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10813">http://arxiv.org/abs/2307.10813</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xilei Zhu, Huiyu Duan, Yuqin Cao, Yuxin Zhu, Yucheng Zhu, Jing Liu, Li Chen, Xiongkuo Min, Guangtao Zhai</li>
<li>for: 这个论文主要针对的是评估全方位视频（ODV）的质量，以提高用户的体验质量（QoE）。</li>
<li>methods: 该论文首先创建了一个大规模的全方位音频视频质量评估数据集，并开发了三种基准方法来评估全方位音频视频质量（OAVQA）。这些方法结合了现有的单模式音频和视频评估模型，通过多模式融合策略进行评估。</li>
<li>results: 该论文在数据集上验证了音频视频融合方法的效iveness，提供了一个新的全方位QoE评估标准。数据集可以在 GitHub 上下载。<details>
<summary>Abstract</summary>
Omnidirectional videos (ODVs) play an increasingly important role in the application fields of medical, education, advertising, tourism, etc. Assessing the quality of ODVs is significant for service-providers to improve the user's Quality of Experience (QoE). However, most existing quality assessment studies for ODVs only focus on the visual distortions of videos, while ignoring that the overall QoE also depends on the accompanying audio signals. In this paper, we first establish a large-scale audio-visual quality assessment dataset for omnidirectional videos, which includes 375 distorted omnidirectional audio-visual (A/V) sequences generated from 15 high-quality pristine omnidirectional A/V contents, and the corresponding perceptual audio-visual quality scores. Then, we design three baseline methods for full-reference omnidirectional audio-visual quality assessment (OAVQA), which combine existing state-of-the-art single-mode audio and video QA models via multimodal fusion strategies. We validate the effectiveness of the A/V multimodal fusion method for OAVQA on our dataset, which provides a new benchmark for omnidirectional QoE evaluation. Our dataset is available at https://github.com/iamazxl/OAVQA.
</details>
<details>
<summary>摘要</summary>
“全向视频（ODV）在医疗、教育、广告、旅游等领域的应用越来越重要。评估ODV的质量是服务提供者提高用户体验质量（QoE）的关键。然而，现有的大多数ODV质量评估研究只关注视频的视觉扭曲，而忽略了音频信号的影响。本文首先建立了一个大规模的音频视频质量评估 dataset，包括375个扭曲的全向音频视频（A/V）序列，来自15个高质量的原始全向A/V内容。然后，我们设计了三种基eline方法 для全向音频视频质量评估（OAVQA），将现有的单模式音频和视频质量评估模型通过多模式融合策略结合。我们验证了这种A/V多模式融合方法在我们的数据集上的效果，提供了一个新的 benchmark для全向QoE评估。我们的数据集可以在 GitHub 上获取。”
</details></li>
</ul>
<hr>
<h2 id="Vesper-A-Compact-and-Effective-Pretrained-Model-for-Speech-Emotion-Recognition"><a href="#Vesper-A-Compact-and-Effective-Pretrained-Model-for-Speech-Emotion-Recognition" class="headerlink" title="Vesper: A Compact and Effective Pretrained Model for Speech Emotion Recognition"></a>Vesper: A Compact and Effective Pretrained Model for Speech Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10757">http://arxiv.org/abs/2307.10757</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/happycolor/vesper">https://github.com/happycolor/vesper</a></li>
<li>paper_authors: Weidong Chen, Xiaofen Xing, Peihao Chen, Xiangmin Xu</li>
<li>for: 本研究旨在适应大规模预训练模型（PTM）到语音情感识别任务。PTMs在人工智能领域亮起新的光，但它们是为普通任务设计的，因此在特定任务中可以进一步提高效果。同时，在实际应用中使用PTMs可能会受到其较大的大小的限制。</li>
<li>methods: 本研究提出了一种优化大规模PTMs для特定任务的研究方向，并在语音情感识别任务上提出了一种改进的情感特定预训练 encoder called Vesper。Vesper在基于WavLM的语音Dataset上进行预训练，并考虑情感特征。为了提高情感信息的敏感度，Vesper使用情感导向的masking策略来标识需要遮盖的区域。然后，Vesper使用层次和交叉层自我超级视图来提高其捕捉语音和语义表示的能力，这两者都是情感识别的关键。</li>
<li>results: 对于IEMOCAP、MELD和CREMA-D datasets的实验结果显示，Vesper WITH 4层比WavLM Base WITH 12层有更好的性能，而Vesper WITH 12层比WavLM Large WITH 24层有更好的性能。<details>
<summary>Abstract</summary>
This paper presents a paradigm that adapts general large-scale pretrained models (PTMs) to speech emotion recognition task. Although PTMs shed new light on artificial general intelligence, they are constructed with general tasks in mind, and thus, their efficacy for specific tasks can be further improved. Additionally, employing PTMs in practical applications can be challenging due to their considerable size. Above limitations spawn another research direction, namely, optimizing large-scale PTMs for specific tasks to generate task-specific PTMs that are both compact and effective. In this paper, we focus on the speech emotion recognition task and propose an improved emotion-specific pretrained encoder called Vesper. Vesper is pretrained on a speech dataset based on WavLM and takes into account emotional characteristics. To enhance sensitivity to emotional information, Vesper employs an emotion-guided masking strategy to identify the regions that need masking. Subsequently, Vesper employs hierarchical and cross-layer self-supervision to improve its ability to capture acoustic and semantic representations, both of which are crucial for emotion recognition. Experimental results on the IEMOCAP, MELD, and CREMA-D datasets demonstrate that Vesper with 4 layers outperforms WavLM Base with 12 layers, and the performance of Vesper with 12 layers surpasses that of WavLM Large with 24 layers.
</details>
<details>
<summary>摘要</summary>
The proposed approach is called Vesper, which is pre-trained on a speech dataset based on WavLM and takes into account emotional characteristics. To enhance sensitivity to emotional information, Vesper uses an emotion-guided masking strategy to identify the regions that need masking. The model also employs hierarchical and cross-layer self-supervision to improve its ability to capture acoustic and semantic representations, which are crucial for emotion recognition.Experimental results on the IEMOCAP, MELD, and CREMA-D datasets show that Vesper with 4 layers outperforms WavLM Base with 12 layers, and the performance of Vesper with 12 layers surpasses that of WavLM Large with 24 layers. This demonstrates the effectiveness of the proposed approach and the potential of Vesper as a task-specific PTM for speech emotion recognition.
</details></li>
</ul>
<hr>
<h2 id="PAS-Partial-Additive-Speech-Data-Augmentation-Method-for-Noise-Robust-Speaker-Verification"><a href="#PAS-Partial-Additive-Speech-Data-Augmentation-Method-for-Noise-Robust-Speaker-Verification" class="headerlink" title="PAS: Partial Additive Speech Data Augmentation Method for Noise Robust Speaker Verification"></a>PAS: Partial Additive Speech Data Augmentation Method for Noise Robust Speaker Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10628">http://arxiv.org/abs/2307.10628</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rst0070/Partial_Additive_Speech">https://github.com/rst0070/Partial_Additive_Speech</a></li>
<li>paper_authors: Wonbin Kim, Hyun-seo Shin, Ju-ho Kim, Jungwoo Heo, Chan-yeong Lim, Ha-Jin Yu</li>
<li>for: 提高SV系统在噪音环境中的准确率和质量</li>
<li>methods: 使用新的加法噪音方法（partial additive speech，PAS）来训练SV系统，以降低噪音环境对SV系统的影响</li>
<li>results: PAS方法比传统的加法噪音方法（EER）提高4.64%和5.01%，并通过注意模块和 speaker embedding 分析表明方法的效果。<details>
<summary>Abstract</summary>
Background noise reduces speech intelligibility and quality, making speaker verification (SV) in noisy environments a challenging task. To improve the noise robustness of SV systems, additive noise data augmentation method has been commonly used. In this paper, we propose a new additive noise method, partial additive speech (PAS), which aims to train SV systems to be less affected by noisy environments. The experimental results demonstrate that PAS outperforms traditional additive noise in terms of equal error rates (EER), with relative improvements of 4.64% and 5.01% observed in SE-ResNet34 and ECAPA-TDNN. We also show the effectiveness of proposed method by analyzing attention modules and visualizing speaker embeddings.
</details>
<details>
<summary>摘要</summary>
背景噪声会降低语音清晰度和质量，使 speaker verification（SV）在噪声环境中成为一项具有挑战性的任务。为了改善噪声Robustness Of SV系统，通常使用添加噪声数据增强方法。在这篇论文中，我们提出了一种新的添加噪声方法，即 partial additive speech（PAS），以培养SV系统对噪声环境更加敏感。实验结果表明，PAS在EER方面表现出比传统添加噪声更好的result，对SE-ResNet34和ECAPA-TDNN的相对改善为4.64%和5.01%。我们还通过分析注意力模块和Visualize speaker embeddings来证明提出的方法的效果。
</details></li>
</ul>
<hr>
<h2 id="Transsion-TSUP’s-speech-recognition-system-for-ASRU-2023-MADASR-Challenge"><a href="#Transsion-TSUP’s-speech-recognition-system-for-ASRU-2023-MADASR-Challenge" class="headerlink" title="Transsion TSUP’s speech recognition system for ASRU 2023 MADASR Challenge"></a>Transsion TSUP’s speech recognition system for ASRU 2023 MADASR Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11778">http://arxiv.org/abs/2307.11778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoxiao Li, Gaosheng Zhang, An Zhu, Weiyong Li, Shuming Fang, Xiaoyue Yang, Jianchao Zhu</li>
<li>for: 这篇论文旨在开发一个适应低资源印度语言的语音识别系统，并在ASRU 2023 MADASR Challenge 中提交了四个track的解决方案。</li>
<li>methods: 该系统使用了ASR模型，并在track 1和2中使用了压缩形态器encoder和双向转换器decoder，同时使用了共同CTC-Attention训练损失。此外，还使用了外部KenLM语言模型进行TLG beam search解码。在track 3和4中，使用了预训练的IndicWhisper模型，并在挑战数据集和公共可用数据集上进行了finetuning。另外， modifying the whisper beam search decoding方法以支持外部KenLM语言模型，使得更好地利用挑战中提供的额外文本。</li>
<li>results: 该方法在 Bengali语言的四个track中获得了24.17%、24.43%、15.97%和15.97%的单词错误率（WER），而在 Bhojpuri语言的四个track中获得了19.61%、19.54%、15.48%和15.48%的WER。这些结果表明该方法的效果。<details>
<summary>Abstract</summary>
This paper presents a speech recognition system developed by the Transsion Speech Understanding Processing Team (TSUP) for the ASRU 2023 MADASR Challenge. The system focuses on adapting ASR models for low-resource Indian languages and covers all four tracks of the challenge. For tracks 1 and 2, the acoustic model utilized a squeezeformer encoder and bidirectional transformer decoder with joint CTC-Attention training loss. Additionally, an external KenLM language model was used during TLG beam search decoding. For tracks 3 and 4, pretrained IndicWhisper models were employed and finetuned on both the challenge dataset and publicly available datasets. The whisper beam search decoding was also modified to support an external KenLM language model, which enabled better utilization of the additional text provided by the challenge. The proposed method achieved word error rates (WER) of 24.17%, 24.43%, 15.97%, and 15.97% for Bengali language in the four tracks, and WER of 19.61%, 19.54%, 15.48%, and 15.48% for Bhojpuri language in the four tracks. These results demonstrate the effectiveness of the proposed method.
</details>
<details>
<summary>摘要</summary>
For tracks 1 and 2, the acoustic model utilized a squeezeformer encoder and bidirectional transformer decoder with joint CTC-Attention training loss. Additionally, an external KenLM language model was used during TLG beam search decoding.For tracks 3 and 4, pretrained IndicWhisper models were employed and finetuned on both the challenge dataset and publicly available datasets. The whisper beam search decoding was also modified to support an external KenLM language model, which enabled better utilization of the additional text provided by the challenge.The proposed method achieved word error rates (WER) of 24.17%, 24.43%, 15.97%, and 15.97% for Bengali language in the four tracks, and WER of 19.61%, 19.54%, 15.48%, and 15.48% for Bhojpuri language in the four tracks. These results demonstrate the effectiveness of the proposed method.Translation in Simplified Chinese:这篇论文介绍了由Transsion Speech Understanding Processing Team (TSUP)开发的一种基于ASR模型的报告识别系统，用于ASRU 2023 MADASR Challenge。该系统专注于适应低资源印度语言的ASR模型，并覆盖了挑战的四个轨道。 для轨道1和2，语音模型使用了压缩转换器encoder和irectional transformer decoder，并使用了共同CTC-Attention培根精度训练。此外，还使用了外部KenLM语言模型进行TLG搜索 décoding。 для轨道3和4，采用了预训练的IndicWhisper模型，并在挑战数据集和公共可用数据集上进行了Finite tuning。幽amma beam search décoding也被修改，以支持外部KenLM语言模型，从而更好地利用挑战中提供的额外文本。提议的方法实现了Word Error Rate（WER）为24.17%、24.43%、15.97%和15.97%的 Bengali语言，以及WER为19.61%、19.54%、15.48%和15.48%的 Bhojpuri语言。这些结果表明了提议的方法的效果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/07/20/cs.SD_2023_07_20/" data-id="cllsk9gqp004g9c8824gc8vdl" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/20/cs.LG_2023_07_20/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-07-20 18:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/20/eess.IV_2023_07_20/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-07-20 17:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'', root:''}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
