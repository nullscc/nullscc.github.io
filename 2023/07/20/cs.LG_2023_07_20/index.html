
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-07-20 18:00:00 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Synthetic Control Methods by Density Matching under Implicit Endogeneity paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.11127 repo_url: None paper_authors: Masahiro Kato, Akari Ohda, Masaaki Imaizumi, Kenichiro">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-07-20 18:00:00">
<meta property="og:url" content="http://example.com/2023/07/20/cs.LG_2023_07_20/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Synthetic Control Methods by Density Matching under Implicit Endogeneity paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.11127 repo_url: None paper_authors: Masahiro Kato, Akari Ohda, Masaaki Imaizumi, Kenichiro">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-19T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:35.212Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_07_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/20/cs.LG_2023_07_20/" class="article-date">
  <time datetime="2023-07-19T16:00:00.000Z" itemprop="datePublished">2023-07-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-07-20 18:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Synthetic-Control-Methods-by-Density-Matching-under-Implicit-Endogeneity"><a href="#Synthetic-Control-Methods-by-Density-Matching-under-Implicit-Endogeneity" class="headerlink" title="Synthetic Control Methods by Density Matching under Implicit Endogeneity"></a>Synthetic Control Methods by Density Matching under Implicit Endogeneity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11127">http://arxiv.org/abs/2307.11127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masahiro Kato, Akari Ohda, Masaaki Imaizumi, Kenichiro McAlinn</li>
<li>for: 这个论文的目的是为了提出一种基于混合模型的新的同比控制方法，以估计对受试者的影响。</li>
<li>methods: 该方法使用了拟合 densities 的思想，即受试者的结果拟合到未处理单位的结果上，从而估计对受试者的影响。</li>
<li>results: 该方法可以提供准确的对受试者的影响估计，并且可以生成对受试者的影响的全 densities，不仅是预期值。<details>
<summary>Abstract</summary>
Synthetic control methods (SCMs) have become a crucial tool for causal inference in comparative case studies. The fundamental idea of SCMs is to estimate counterfactual outcomes for a treated unit by using a weighted sum of observed outcomes from untreated units. The accuracy of the synthetic control (SC) is critical for estimating the causal effect, and hence, the estimation of SC weights has been the focus of much research. In this paper, we first point out that existing SCMs suffer from an implicit endogeneity problem, which is the correlation between the outcomes of untreated units and the error term in the model of a counterfactual outcome. We show that this problem yields a bias in the causal effect estimator. We then propose a novel SCM based on density matching, assuming that the density of outcomes of the treated unit can be approximated by a weighted average of the densities of untreated units (i.e., a mixture model). Based on this assumption, we estimate SC weights by matching moments of treated outcomes and the weighted sum of moments of untreated outcomes. Our proposed method has three advantages over existing methods. First, our estimator is asymptotically unbiased under the assumption of the mixture model. Second, due to the asymptotic unbiasedness, we can reduce the mean squared error for counterfactual prediction. Third, our method generates full densities of the treatment effect, not only expected values, which broadens the applicability of SCMs. We provide experimental results to demonstrate the effectiveness of our proposed method.
</details>
<details>
<summary>摘要</summary>
synthetic control methods (SCMs) 已成为比较研究中的重要工具，用于估计 causal inference。 SCMs 的基本想法是通过使用一个权重的和 observed outcomes from untreated units 来估计 treated unit 的 counterfactual outcomes。 counterfactual outcome 的准确性是估计 causal effect 的关键，因此 SC weights 的 estimation 已经引起了很多研究的关注。在本文中，我们首先指出了现有 SCMs 存在一种隐藏的内生性问题，即 untreated units 的 outcome 和 counterfactual outcome 的模型中的偏差的相关性。我们证明这个问题会导致 causal effect 估计器偏离。然后，我们提出了一种基于density matching的新的 SCM，假设 treated unit 的 outcome 的概率密度可以被approxicate 为一个 weighted average of untreated units 的概率密度（即 mixture model）。基于这个假设，我们可以通过匹配 treated outcomes 和 weighted sum of untreated outcomes 的 moments 来估计 SC weights。我们的提出方法有以下三个优点：1. 我们的估计器是 asymptotically unbiased 的，即在 mixture model 的假设下。2. 由于 asymptotic unbiasedness，我们可以减少 counterfactual prediction 的 mean squared error。3. 我们的方法可以生成 treated effect 的全部概率密度，不仅是预期值，这扩展了 SCMs 的应用范围。我们提供了实验结果，以证明我们的提出方法的效果。
</details></li>
</ul>
<hr>
<h2 id="A-Markov-Chain-Model-for-Identifying-Changes-in-Daily-Activity-Patterns-of-People-Living-with-Dementia"><a href="#A-Markov-Chain-Model-for-Identifying-Changes-in-Daily-Activity-Patterns-of-People-Living-with-Dementia" class="headerlink" title="A Markov Chain Model for Identifying Changes in Daily Activity Patterns of People Living with Dementia"></a>A Markov Chain Model for Identifying Changes in Daily Activity Patterns of People Living with Dementia</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11126">http://arxiv.org/abs/2307.11126</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nvfl/markov-chain-model">https://github.com/nvfl/markov-chain-model</a></li>
<li>paper_authors: Nan Fletcher-Lloyd, Alina-Irina Serban, Magdalena Kolanko, David Wingfield, Danielle Wilson, Ramin Nilforooshan, Payam Barnaghi, Eyal Soreq</li>
<li>for: 这个研究是为了检测老年人忘形症患者的饮食和饮水状况，以及通过互联网物联网技术进行 дома中监测，以提高忘形症患者的生活质量。</li>
<li>methods: 这个研究使用了互联网物联网技术进行 дома中监测，并使用了线性混合效应分析和马尔可夫模型来检测老年人忘形症患者的饮食和饮水状况的变化。</li>
<li>results: 研究发现，在21户PLWD中，日间厨房活动增加，夜间厨房活动减少（t(147) &#x3D; -2.90, p &lt; 0.001）。此外，提出了一种基于远程监测数据的新的分析方法，可以用于检测PLWD的行为变化。<details>
<summary>Abstract</summary>
Malnutrition and dehydration are strongly associated with increased cognitive and functional decline in people living with dementia (PLWD), as well as an increased rate of hospitalisations in comparison to their healthy counterparts. Extreme changes in eating and drinking behaviours can often lead to malnutrition and dehydration, accelerating the progression of cognitive and functional decline and resulting in a marked reduction in quality of life. Unfortunately, there are currently no established methods by which to objectively detect such changes. Here, we present the findings of an extensive quantitative analysis conducted on in-home monitoring data collected from 73 households of PLWD using Internet of Things technologies. The Coronavirus 2019 (COVID-19) pandemic has previously been shown to have dramatically altered the behavioural habits, particularly the eating and drinking habits, of PLWD. Using the COVID-19 pandemic as a natural experiment, we conducted linear mixed-effects modelling to examine changes in mean kitchen activity within a subset of 21 households of PLWD that were continuously monitored for 499 days. We report an observable increase in day-time kitchen activity and a significant decrease in night-time kitchen activity (t(147) = -2.90, p < 0.001). We further propose a novel analytical approach to detecting changes in behaviours of PLWD using Markov modelling applied to remote monitoring data as a proxy for behaviours that cannot be directly measured. Together, these results pave the way to introduce improvements into the monitoring of PLWD in naturalistic settings and for shifting from reactive to proactive care.
</details>
<details>
<summary>摘要</summary>
营养不良和脱水是老年人失智症（PLWD）的常见问题，可能导致认知和功能退化加速，并增加入院病人数。不正常的食品和饮料消耗习惯可能导致营养不良和脱水，从而恶化认知和功能退化，并导致生活质量下降。然而，目前并无可靠的方法来评估这些变化。在这种情况下，我们提出了一种基于互联网物联网技术的评估方法，并对73户老年人失智症的家庭进行了广泛的量化分析。 COVID-19大流行在老年人失智症行为习惯中产生了深远的影响，特别是食品和饮料消耗习惯。使用COVID-19大流行作为自然实验，我们使用线性混合模型对21户老年人失智症的24小时内的厨房活动进行分析。我们发现了日间厨房活动的增加（t(147) = -2.90，p < 0.001），并且夜间厨房活动的减少。此外，我们还提出了一种基于远程监测数据的markov预测方法，用于检测老年人失智症行为变化。总之，这些结果可以帮助改进老年人失智症的监测和护理，从而转移到主动的护理。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Models-for-Probabilistic-Deconvolution-of-Galaxy-Images"><a href="#Diffusion-Models-for-Probabilistic-Deconvolution-of-Galaxy-Images" class="headerlink" title="Diffusion Models for Probabilistic Deconvolution of Galaxy Images"></a>Diffusion Models for Probabilistic Deconvolution of Galaxy Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11122">http://arxiv.org/abs/2307.11122</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yashpatel5400/galgen">https://github.com/yashpatel5400/galgen</a></li>
<li>paper_authors: Zhiwei Xue, Yuhang Li, Yash Patel, Jeffrey Regier</li>
<li>for: 这个论文是为了解决望远镜捕捉图像时的特点点雨函数（PSF）问题，即使用深度生成模型进行PSF恢复。</li>
<li>methods: 该论文使用了一种没有分类器的决策扩散模型来实现PSF恢复。</li>
<li>results: 论文表明，该扩散模型可以提供更多的可能的恢复结果，比起基于VAE的条件扩散模型。<details>
<summary>Abstract</summary>
Telescopes capture images with a particular point spread function (PSF). Inferring what an image would have looked like with a much sharper PSF, a problem known as PSF deconvolution, is ill-posed because PSF convolution is not an invertible transformation. Deep generative models are appealing for PSF deconvolution because they can infer a posterior distribution over candidate images that, if convolved with the PSF, could have generated the observation. However, classical deep generative models such as VAEs and GANs often provide inadequate sample diversity. As an alternative, we propose a classifier-free conditional diffusion model for PSF deconvolution of galaxy images. We demonstrate that this diffusion model captures a greater diversity of possible deconvolutions compared to a conditional VAE.
</details>
<details>
<summary>摘要</summary>
望远镜捕捉图像具有特定点扩函数（PSF）。根据这个函数，推断图像如果有更高的解构度，这个问题称为PSF恢复是不充分定义的，因为PSF混合不是可逆变换。深度生成模型对PSF恢复是有吸引力的，因为它们可以对候选图像进行 posterior 分布逻辑，这些图像如果与PSF混合，就可能生成观察结果。然而，经典的深度生成模型如VAEs和GANs通常提供不够多样性的样本。作为一种替代方案，我们提议一种无类别的条件扩散模型用于PSF恢复星系图像。我们表明，这种扩散模型可以捕捉更多的可能的恢复结果，比Conditional VAE更佳。
</details></li>
</ul>
<hr>
<h2 id="PASTA-Pretrained-Action-State-Transformer-Agents"><a href="#PASTA-Pretrained-Action-State-Transformer-Agents" class="headerlink" title="PASTA: Pretrained Action-State Transformer Agents"></a>PASTA: Pretrained Action-State Transformer Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10936">http://arxiv.org/abs/2307.10936</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raphael Boige, Yannis Flet-Berliac, Arthur Flajolet, Guillaume Richard, Thomas Pierrot</li>
<li>for: 这个论文的目的是研究基于自适应学习的强化学习模型，以及这些模型在多种下游任务上的性能。</li>
<li>methods: 该论文使用了一种叫做 PASTA 的模型，该模型在不同的领域中同时进行了多种基本的预训练任务，并使用了基本的预训练目标如下一个token预测。</li>
<li>results: 该论文的研究发现，使用 PASTA 模型可以在多种下游任务上实现更好的性能，包括行为假象追踪、离线强化学习、感知器故障Robustness和动力学变化适应。此外，该论文还发现，使用 parameter efficient fine-tuning (PEFT) 可以在下游适应中使用 fewer than 10,000 参数进行精细调整，这使得更多的社区可以使用这些模型并重现我们的实验。<details>
<summary>Abstract</summary>
Self-supervised learning has brought about a revolutionary paradigm shift in various computing domains, including NLP, vision, and biology. Recent approaches involve pre-training transformer models on vast amounts of unlabeled data, serving as a starting point for efficiently solving downstream tasks. In the realm of reinforcement learning, researchers have recently adapted these approaches by developing models pre-trained on expert trajectories, enabling them to address a wide range of tasks, from robotics to recommendation systems. However, existing methods mostly rely on intricate pre-training objectives tailored to specific downstream applications. This paper presents a comprehensive investigation of models we refer to as Pretrained Action-State Transformer Agents (PASTA). Our study uses a unified methodology and covers an extensive set of general downstream tasks including behavioral cloning, offline RL, sensor failure robustness, and dynamics change adaptation. Our goal is to systematically compare various design choices and provide valuable insights to practitioners for building robust models. Key highlights of our study include tokenization at the action and state component level, using fundamental pre-training objectives like next token prediction, training models across diverse domains simultaneously, and using parameter efficient fine-tuning (PEFT). The developed models in our study contain fewer than 10 million parameters and the application of PEFT enables fine-tuning of fewer than 10,000 parameters during downstream adaptation, allowing a broad community to use these models and reproduce our experiments. We hope that this study will encourage further research into the use of transformers with first-principles design choices to represent RL trajectories and contribute to robust policy learning.
</details>
<details>
<summary>摘要</summary>
自顾学学习已经在不同的计算领域引起了革命性的变革，包括自然语言处理、视觉和生物。现有的方法通常是在大量无标签数据上预训练变换器模型，作为下游任务的开始点，以提高效率。在返回学习领域，研究人员已经采用了这些方法，并开发了基于专家轨迹的模型，以解决广泛的任务，从 робо扮到推荐系统。然而，现有的方法大多依赖于特定下游应用程序的复杂预训练目标。本文发表了一项总体的调查，涵盖了我们称为预训练动作-状态变换器代理（PASTA）的模型。我们的研究使用了一种统一的方法ología，对广泛的下游任务进行了总体的调查，包括行为假象念、离线RL、感知失效鲁棒性和动力学变化适应。我们的目标是系统地比较不同的设计选择，提供价值的情况，以便实践者在建立Robust模型。关键特点包括动作和状态组件层次启用，使用基本的预训练目标如下一个元素预测，在多个领域同时训练模型，以及使用精简的 fine-tuning（PEFT）。我们的模型含 fewer than 10 million 参数，并在下游适应中使用 PEFT 进行精简，使得广泛的社区可以使用这些模型并复制我们的实验。我们希望这项研究能够鼓励更多的研究人员通过使用 transformer 模型的首要设计选择来表示RL轨迹，并贡献到Robust策略学习。
</details></li>
</ul>
<hr>
<h2 id="Inorganic-synthesis-structure-maps-in-zeolites-with-machine-learning-and-crystallographic-distances"><a href="#Inorganic-synthesis-structure-maps-in-zeolites-with-machine-learning-and-crystallographic-distances" class="headerlink" title="Inorganic synthesis-structure maps in zeolites with machine learning and crystallographic distances"></a>Inorganic synthesis-structure maps in zeolites with machine learning and crystallographic distances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10935">http://arxiv.org/abs/2307.10935</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Schwalbe-Koda, Daniel E. Widdowson, Tuan Anh Pham, Vitaliy A. Kurlin<br>for: 这篇论文旨在用计算机模拟方法研究碎 Zeolites 的合成条件和结构。methods: 这篇论文使用了一种强度距离度量和机器学习技术来创建 Zeolites 的无机合成地图。results: 研究发现，使用这种距离度量可以从已知 Zeolites 的框架结构中推断无机合成条件，并且可以用机器学习分类器来找到 Zeolites 的合成结构关系。<details>
<summary>Abstract</summary>
Zeolites are inorganic materials known for their diversity of applications, synthesis conditions, and resulting polymorphs. Although their synthesis is controlled both by inorganic and organic synthesis conditions, computational studies of zeolite synthesis have focused mostly on organic template design. In this work, we use a strong distance metric between crystal structures and machine learning (ML) to create inorganic synthesis maps in zeolites. Starting with 253 known zeolites, we show how the continuous distances between frameworks reproduce inorganic synthesis conditions from the literature without using labels such as building units. An unsupervised learning analysis shows that neighboring zeolites according to our metric often share similar inorganic synthesis conditions, even in template-based routes. In combination with ML classifiers, we find synthesis-structure relationships for 14 common inorganic conditions in zeolites, namely Al, B, Be, Ca, Co, F, Ga, Ge, K, Mg, Na, P, Si, and Zn. By explaining the model predictions, we demonstrate how (dis)similarities towards known structures can be used as features for the synthesis space. Finally, we show how these methods can be used to predict inorganic synthesis conditions for unrealized frameworks in hypothetical databases and interpret the outcomes by extracting local structural patterns from zeolites. In combination with template design, this work can accelerate the exploration of the space of synthesis conditions for zeolites.
</details>
<details>
<summary>摘要</summary>
Zeolites 是一类无机材料，known for their 多样性 of applications, synthesis conditions, and resulting polymorphs. Although their synthesis is controlled by both inorganic and organic synthesis conditions, computational studies of zeolite synthesis have focused mostly on organic template design. In this work, we use a strong distance metric between crystal structures and machine learning (ML) to create inorganic synthesis maps in zeolites. Starting with 253 known zeolites, we show how the continuous distances between frameworks reproduce inorganic synthesis conditions from the literature without using labels such as building units. An unsupervised learning analysis shows that neighboring zeolites according to our metric often share similar inorganic synthesis conditions, even in template-based routes. In combination with ML classifiers, we find synthesis-structure relationships for 14 common inorganic conditions in zeolites, namely Al, B, Be, Ca, Co, F, Ga, Ge, K, Mg, Na, P, Si, and Zn. By explaining the model predictions, we demonstrate how (dis)similarities towards known structures can be used as features for the synthesis space. Finally, we show how these methods can be used to predict inorganic synthesis conditions for unrealized frameworks in hypothetical databases and interpret the outcomes by extracting local structural patterns from zeolites. In combination with template design, this work can accelerate the exploration of the space of synthesis conditions for zeolites.
</details></li>
</ul>
<hr>
<h2 id="Modeling-3D-cardiac-contraction-and-relaxation-with-point-cloud-deformation-networks"><a href="#Modeling-3D-cardiac-contraction-and-relaxation-with-point-cloud-deformation-networks" class="headerlink" title="Modeling 3D cardiac contraction and relaxation with point cloud deformation networks"></a>Modeling 3D cardiac contraction and relaxation with point cloud deformation networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10927">http://arxiv.org/abs/2307.10927</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcel Beetz, Abhirup Banerjee, Vicente Grau</li>
<li>for: 这项研究旨在开发一种基于点云深度学习的新方法，以模拟心脏的3D弹性变形过程。</li>
<li>methods: 该方法使用点云深度学习的最新进展，建立了一个encoder-decoder结构，以便在多级别的特征学习中，能够直接使用多类3D点云表示心脏 анатомии。</li>
<li>results: 研究人员在UK Biobank数据集上进行了大规模的测试，并发现了在 Chamfer 距离下面的平均距离，并且与实际数据集中的临床指标相似。此外，该方法还能够成功地捕捉不同种群之间的差异，并在预后报告和心脏病发生预测等任务中，以13%和7%的优势超越多个临床标准。<details>
<summary>Abstract</summary>
Global single-valued biomarkers of cardiac function typically used in clinical practice, such as ejection fraction, provide limited insight on the true 3D cardiac deformation process and hence, limit the understanding of both healthy and pathological cardiac mechanics. In this work, we propose the Point Cloud Deformation Network (PCD-Net) as a novel geometric deep learning approach to model 3D cardiac contraction and relaxation between the extreme ends of the cardiac cycle. It employs the recent advances in point cloud-based deep learning into an encoder-decoder structure, in order to enable efficient multi-scale feature learning directly on multi-class 3D point cloud representations of the cardiac anatomy. We evaluate our approach on a large dataset of over 10,000 cases from the UK Biobank study and find average Chamfer distances between the predicted and ground truth anatomies below the pixel resolution of the underlying image acquisition. Furthermore, we observe similar clinical metrics between predicted and ground truth populations and show that the PCD-Net can successfully capture subpopulation-specific differences between normal subjects and myocardial infarction (MI) patients. We then demonstrate that the learned 3D deformation patterns outperform multiple clinical benchmarks by 13% and 7% in terms of area under the receiver operating characteristic curve for the tasks of prevalent MI detection and incident MI prediction and by 7% in terms of Harrell's concordance index for MI survival analysis.
</details>
<details>
<summary>摘要</summary>
全球唯一价值生物标志通常用于临床实践中，如血液泵动率（ejection fraction），只提供有限的真实3D冠状膜弹性过程的信息，因此限制了正常和疾病冠状膜机械学的理解。在这项工作中，我们提出了点云弹性网络（PCD-Net）作为一种新的几何深度学术方法，用于模型3D冠状膜收缩和放松阶段的几何变化。它利用了最新的点云基于深度学的进步，将点云表示的冠状膜解剖结构转化为多级几何特征学习的encoder-decoder结构。我们对大量的UK Biobank数据集（超过10,000个案例）进行评估，并发现在Chamfer距离下限以像素分辨率的图像获取的下方。此外，我们发现了预测和真实人口的临床指标相似，并证明PCD-Net可以成功捕捉不同种类的冠状膜疾病的差异。最后，我们发现PCD-Net已经在MI检测和预测任务中提高了13%和7%，并在MI生存分析中提高了7%。
</details></li>
</ul>
<hr>
<h2 id="Confidence-intervals-for-performance-estimates-in-3D-medical-image-segmentation"><a href="#Confidence-intervals-for-performance-estimates-in-3D-medical-image-segmentation" class="headerlink" title="Confidence intervals for performance estimates in 3D medical image segmentation"></a>Confidence intervals for performance estimates in 3D medical image segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10926">http://arxiv.org/abs/2307.10926</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rosanajurdi/SegVal_TMI">https://github.com/rosanajurdi/SegVal_TMI</a></li>
<li>paper_authors: R. El Jurdi, G. Varoquaux, O. Colliot</li>
<li>for: 这个论文主要是用来研究医学图像分割模型的评估方法。</li>
<li>methods: 这个论文使用了nnU-net框架和医学图像分割挑战赛中的两个数据集，以及两种性能指标： dice准确率和 Hausdorff 距离。</li>
<li>results: 这个论文发现，在不同的测试集大小和性能指标的散度下，参数型信息Interval是正确的近似值，并且显示了测试样本的数量需要更低的水平来达到给定的精度水平。通常需要100-200个测试样本，而更Difficult的分割任务可能需要更多的样本。<details>
<summary>Abstract</summary>
Medical segmentation models are evaluated empirically. As such an evaluation is based on a limited set of example images, it is unavoidably noisy. Beyond a mean performance measure, reporting confidence intervals is thus crucial. However, this is rarely done in medical image segmentation. The width of the confidence interval depends on the test set size and on the spread of the performance measure (its standard-deviation across of the test set). For classification, many test images are needed to avoid wide confidence intervals. Segmentation, however, has not been studied, and it differs by the amount of information brought by a given test image. In this paper, we study the typical confidence intervals in medical image segmentation. We carry experiments on 3D image segmentation using the standard nnU-net framework, two datasets from the Medical Decathlon challenge and two performance measures: the Dice accuracy and the Hausdorff distance. We show that the parametric confidence intervals are reasonable approximations of the bootstrap estimates for varying test set sizes and spread of the performance metric. Importantly, we show that the test size needed to achieve a given precision is often much lower than for classification tasks. Typically, a 1% wide confidence interval requires about 100-200 test samples when the spread is low (standard-deviation around 3%). More difficult segmentation tasks may lead to higher spreads and require over 1000 samples.
</details>
<details>
<summary>摘要</summary>
医学分割模型通常由实际评估，这种评估基于有限数量的示例图像，因此不可避免噪音。在医学图像分割中，报告信息Interval是非常重要，但是这并不常done。 confidence interval的宽度取决于测试集大小和性能指标（其标准差在测试集中）。为分类问题，需要许多测试图像来避免宽度的信息Interval。然而，在医学图像分割中，每个测试图像都带来了不同的信息量。在这篇文章中，我们研究了医学图像分割中的常见信息Interval。我们在使用标准nnU-net框架、医学十大挑战赛中的两个数据集和两个性能指标（ dice准确率和 Hausdorff 距离）进行实验。我们发现，参数信息Interval是优等的近似值，并且与Bootstrap估计相关。进一步地，我们发现，为达到给定的精度，测试样本的数量通常比分类任务要少得多。通常需要100-200个测试样本，当标准差低（约3%）时。在更Difficult分割任务时，可能会出现更高的标准差，需要更多的测试样本（超过1000个）。
</details></li>
</ul>
<hr>
<h2 id="Sequential-Multi-Dimensional-Self-Supervised-Learning-for-Clinical-Time-Series"><a href="#Sequential-Multi-Dimensional-Self-Supervised-Learning-for-Clinical-Time-Series" class="headerlink" title="Sequential Multi-Dimensional Self-Supervised Learning for Clinical Time Series"></a>Sequential Multi-Dimensional Self-Supervised Learning for Clinical Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10923">http://arxiv.org/abs/2307.10923</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/exploita123/charmedforfree">https://github.com/exploita123/charmedforfree</a></li>
<li>paper_authors: Aniruddh Raghu, Payal Chandak, Ridwan Alam, John Guttag, Collin M. Stultz</li>
<li>For: This paper is focused on addressing the limitations of existing self-supervised learning (SSL) methods for clinical time series data, which are typically designed for unimodal time series. The authors propose a new SSL method called Sequential Multi-Dimensional SSL that can capture information at both the sequence level and the individual data point level.* Methods: The proposed method uses a SSL loss function that is applied at both the sequence level and the individual data point level. The specific form of the loss function can be either contrastive or non-contrastive. The method is agnostic to the choice of loss function.* Results: The authors evaluate their method on two real-world clinical datasets, one containing high-frequency electrocardiograms and the other containing structured data from lab values and vitals signs. The results show that pre-training with their method and then fine-tuning on downstream tasks improves performance over baselines on both datasets, and can lead to improvements across different self-supervised loss functions.Here is the information in Simplified Chinese text:* For: 本研究针对现有的医学时序数据自动学习（SSL）方法存在的限制，这些方法通常是针对单modal时序序的设计。作者提出了一种新的SSL方法，即Sequential Multi-Dimensional SSL，可以更好地捕捉时序序的信息。* Methods: 提议的方法使用SSL损失函数，其应用于时序序的两级划分。特定的损失函数可以是对比或非对比的。方法是对损失函数的不同选择无关的。* Results: 作者在两个实际的医学时序数据集上评估了他们的方法。结果显示，预训练后的方法在两个数据集上的表现都超过基线，并在不同的自我超vised损失函数下进行细化后的表现也有改善。<details>
<summary>Abstract</summary>
Self-supervised learning (SSL) for clinical time series data has received significant attention in recent literature, since these data are highly rich and provide important information about a patient's physiological state. However, most existing SSL methods for clinical time series are limited in that they are designed for unimodal time series, such as a sequence of structured features (e.g., lab values and vitals signs) or an individual high-dimensional physiological signal (e.g., an electrocardiogram). These existing methods cannot be readily extended to model time series that exhibit multimodality, with structured features and high-dimensional data being recorded at each timestep in the sequence. In this work, we address this gap and propose a new SSL method -- Sequential Multi-Dimensional SSL -- where a SSL loss is applied both at the level of the entire sequence and at the level of the individual high-dimensional data points in the sequence in order to better capture information at both scales. Our strategy is agnostic to the specific form of loss function used at each level -- it can be contrastive, as in SimCLR, or non-contrastive, as in VICReg. We evaluate our method on two real-world clinical datasets, where the time series contains sequences of (1) high-frequency electrocardiograms and (2) structured data from lab values and vitals signs. Our experimental results indicate that pre-training with our method and then fine-tuning on downstream tasks improves performance over baselines on both datasets, and in several settings, can lead to improvements across different self-supervised loss functions.
</details>
<details>
<summary>摘要</summary>
医学时间序列数据自主学习（SSL）在最近的文献中受到了广泛的关注，因为这些数据具有高度的资料价值，可以提供病人生理状态的重要信息。然而，现有的大多数SSL方法仅适用于单模时间序列数据，例如序列中的结构化特征（例如医学实验室值和生物指标）或一个高维度的生理信号（例如电cardiogram）。这些现有的方法难以扩展到模式多样性时间序列数据，其中每个时间步骤都有结构化特征和高维度数据被记录。在这项工作中，我们Addressing this gap, we propose a new SSL method called Sequential Multi-Dimensional SSL, which applies a SSL loss both at the level of the entire sequence and at the level of the individual high-dimensional data points in the sequence to better capture information at both scales. Our strategy is agnostic to the specific form of loss function used at each level - it can be contrastive, as in SimCLR, or non-contrastive, as in VICReg. We evaluate our method on two real-world clinical datasets, where the time series contains sequences of (1) high-frequency electrocardiograms and (2) structured data from lab values and vitals signs. Our experimental results indicate that pre-training with our method and then fine-tuning on downstream tasks improves performance over baselines on both datasets, and in several settings, can lead to improvements across different self-supervised loss functions.
</details></li>
</ul>
<hr>
<h2 id="Language-based-Action-Concept-Spaces-Improve-Video-Self-Supervised-Learning"><a href="#Language-based-Action-Concept-Spaces-Improve-Video-Self-Supervised-Learning" class="headerlink" title="Language-based Action Concept Spaces Improve Video Self-Supervised Learning"></a>Language-based Action Concept Spaces Improve Video Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10922">http://arxiv.org/abs/2307.10922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kanchana Ranasinghe, Michael Ryoo</li>
<li>for: 学习高效转移和Robust图像表示</li>
<li>methods: 使用语言拼接自我超vised学习对图像CLIP模型进行适应</li>
<li>results: 提高了零shot和线性探测性能在三个动作识别benchmark上<details>
<summary>Abstract</summary>
Recent contrastive language image pre-training has led to learning highly transferable and robust image representations. However, adapting these models to video domains with minimal supervision remains an open problem. We explore a simple step in that direction, using language tied self-supervised learning to adapt an image CLIP model to the video domain. A backbone modified for temporal modeling is trained under self-distillation settings with train objectives operating in an action concept space. Feature vectors of various action concepts extracted from a language encoder using relevant textual prompts construct this space. We introduce two train objectives, concept distillation and concept alignment, that retain generality of original representations while enforcing relations between actions and their attributes. Our approach improves zero-shot and linear probing performance on three action recognition benchmarks.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:最近的语言图像预训练技术已经导致了图像表示的高度传输和稳定性。然而，将这些模型应用到视频领域仍然是一个未解决的问题。我们explore一个简单的方法，使用语言绑定的自我超视觉学习来适应图像CLIP模型到视频领域。一个修改了时间模型的背部是在自我热钻设置下训练，使用train目标在动作概念空间中运行。我们引入了两个train目标，概念热钻和概念对齐，以保持原始表示的通用性，同时强制行为和其属性之间的关系。我们的方法提高了零shot和线性探测性能在三个动作识别基准上。
</details></li>
</ul>
<hr>
<h2 id="The-Role-of-Entropy-and-Reconstruction-in-Multi-View-Self-Supervised-Learning"><a href="#The-Role-of-Entropy-and-Reconstruction-in-Multi-View-Self-Supervised-Learning" class="headerlink" title="The Role of Entropy and Reconstruction in Multi-View Self-Supervised Learning"></a>The Role of Entropy and Reconstruction in Multi-View Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10907">http://arxiv.org/abs/2307.10907</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/apple/ml-entropy-reconstruction">https://github.com/apple/ml-entropy-reconstruction</a></li>
<li>paper_authors: Borja Rodríguez-Gálvez, Arno Blaas, Pau Rodríguez, Adam Goliński, Xavier Suau, Jason Ramapuram, Dan Busbridge, Luca Zappella</li>
<li>for: This paper aims to understand the mechanisms behind the success of multi-view self-supervised learning (MVSSL) methods, specifically contrastive MVSSL methods, through the lens of a new lower bound on the Mutual Information (MI) called the Entropy-Reconstruction (ER) bound.</li>
<li>methods: The paper analyzes the main MVSSL families through the ER bound, showing that clustering-based methods such as DeepCluster and SwAV maximize the MI, while distillation-based approaches such as BYOL and DINO explicitly maximize the reconstruction term and implicitly encourage a stable entropy.</li>
<li>results: The paper shows that replacing the objectives of common MVSSL methods with the ER bound achieves competitive performance while making them stable when training with smaller batch sizes or smaller exponential moving average (EMA) coefficients.Here is the same information in Simplified Chinese:</li>
<li>for: 这篇论文目标是理解多视图自然学习（MVSSL）方法的成功机制，具体是对比自然学习（MVSSL）方法的InfoNCE下界的研究。</li>
<li>methods: 论文通过Entropy-Reconstruction（ER）下界分析主要MVSSL家族，发现嵌入式 clustering 方法如 DeepCluster 和 SwAV 最大化 Mutual Information（MI），而搅拌方法如 BYOL 和 DINO 则直接最大化重建项并隐式促进稳定 entropy。</li>
<li>results: 论文表明，将常见 MVSSL 方法的目标替换为 ER 下界可以实现竞争性性能，同时在训练时使用更小的批处理大小或更小的指数移动平均值（EMA）値可以使模型更稳定。<details>
<summary>Abstract</summary>
The mechanisms behind the success of multi-view self-supervised learning (MVSSL) are not yet fully understood. Contrastive MVSSL methods have been studied through the lens of InfoNCE, a lower bound of the Mutual Information (MI). However, the relation between other MVSSL methods and MI remains unclear. We consider a different lower bound on the MI consisting of an entropy and a reconstruction term (ER), and analyze the main MVSSL families through its lens. Through this ER bound, we show that clustering-based methods such as DeepCluster and SwAV maximize the MI. We also re-interpret the mechanisms of distillation-based approaches such as BYOL and DINO, showing that they explicitly maximize the reconstruction term and implicitly encourage a stable entropy, and we confirm this empirically. We show that replacing the objectives of common MVSSL methods with this ER bound achieves competitive performance, while making them stable when training with smaller batch sizes or smaller exponential moving average (EMA) coefficients.   Github repo: https://github.com/apple/ml-entropy-reconstruction.
</details>
<details>
<summary>摘要</summary>
文中提出的多视图自助学习（MVSSL）的机制尚未完全理解。对于对照式MVSSL方法，我们通过信息准确（InfoNCE）来研究其关系。然而，其他MVSSL方法和MI之间的关系仍然不清楚。我们考虑了一种基于信息 entropy和重建（ER）的下界，并通过这个下界来分析主要的MVSSL家族。我们显示了使用 DeepCluster 和 SwAV 方法时，都会最大化MI。我们还重新解释了 BYOL 和 DINO 方法的机制，并证明它们直接最大化重建项，并间接鼓励稳定的 entropy，这一点我们在实验中证实了。我们还显示了将常见MVSSL方法的目标替换为 ER 下界可以实现竞争性表现，同时使其在训练时使用 smaller batch size 或 smaller EMA 系数时变得稳定。Github 仓库：https://github.com/apple/ml-entropy-reconstruction.
</details></li>
</ul>
<hr>
<h2 id="Variational-Point-Encoding-Deformation-for-Dental-Modeling"><a href="#Variational-Point-Encoding-Deformation-for-Dental-Modeling" class="headerlink" title="Variational Point Encoding Deformation for Dental Modeling"></a>Variational Point Encoding Deformation for Dental Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10895">http://arxiv.org/abs/2307.10895</a></li>
<li>repo_url: None</li>
<li>paper_authors: Johan Ziruo Ye, Thomas Ørkild, Peter Lempel Søndergaard, Søren Hauberg</li>
<li>for:  This paper aims to address the challenges in digital dentistry by releasing a new dataset of tooth meshes and proposing a new method called Variational FoldingNet (VF-Net) for point cloud representation.</li>
<li>methods:  VF-Net extends FoldingNet to enable probabilistic learning of point cloud representations, addressing the challenge of 1-to-1 mapping between input and output points.</li>
<li>results:  The paper demonstrates the superior performance of VF-Net over existing models in terms of dental scan reconstruction and extrapolation, and highlights the robustness of VF-Net’s latent representations.<details>
<summary>Abstract</summary>
Digital dentistry has made significant advancements in recent years, yet numerous challenges remain to be addressed. In this study, we release a new extensive dataset of tooth meshes to encourage further research. Additionally, we propose Variational FoldingNet (VF-Net), which extends FoldingNet to enable probabilistic learning of point cloud representations. A key challenge in existing latent variable models for point clouds is the lack of a 1-to-1 mapping between input points and output points. Instead, they must rely on optimizing Chamfer distances, a metric that does not have a normalized distributional counterpart, preventing its usage in probabilistic models. We demonstrate that explicit minimization of Chamfer distances can be replaced by a suitable encoder, which allows us to increase computational efficiency while simplifying the probabilistic extension. Our experimental findings present empirical evidence demonstrating the superior performance of VF-Net over existing models in terms of dental scan reconstruction and extrapolation. Additionally, our investigation highlights the robustness of VF-Net's latent representations. These results underscore the promising prospects of VF-Net as an effective and reliable method for point cloud reconstruction and analysis.
</details>
<details>
<summary>摘要</summary>
“数字牙科”在最近几年内取得了重要进步，但还有许多挑战需要解决。在这项研究中，我们发布了一个新的广泛的牙齿磁共振数据集，以便进一步推动研究。此外，我们提出了变分卷积网络（VF-Net），它将卷积网络扩展到可以进行概率学学习磁点云表示。现有的磁点云latent variable模型的一个主要挑战是输入点云和输出点云之间没有1-to-1的映射，它们必须通过优化Chamfer距离来进行优化。我们示出了可以直接使用适当的编码器来取代Explicit Chamfer距离的优化，从而提高计算效率并简化概率扩展。我们的实验结果表明VF-Net在牙齿扫描重建和推导方面的性能明显高于现有模型。此外，我们的调查也表明VF-Net的秘密表示的稳定性。这些结果证明了VF-Net作为牙齿扫描和分析的有效和可靠方法。
</details></li>
</ul>
<hr>
<h2 id="Learning-and-Generalizing-Polynomials-in-Simulation-Metamodeling"><a href="#Learning-and-Generalizing-Polynomials-in-Simulation-Metamodeling" class="headerlink" title="Learning and Generalizing Polynomials in Simulation Metamodeling"></a>Learning and Generalizing Polynomials in Simulation Metamodeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10892">http://arxiv.org/abs/2307.10892</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jesperhauch/polynomial_deep_learning">https://github.com/jesperhauch/polynomial_deep_learning</a></li>
<li>paper_authors: Jesper Hauch, Christoffer Riis, Francisco C. Pereira</li>
<li>for: 本文是为了提出 multiplicative neural network（MNN）建模方法，用于模拟高阶多项式函数。</li>
<li>methods: 本文提出了一种基于 MNN 的模拟元模型方法，该方法可以在模拟时间步更新中使用 MNN 来近似高阶多项式函数。</li>
<li>results: 实验表明，相比基线模型，MNN 在适应和扩展性方面表现更好，其在验证集中的性能与对于数据集外的测试集中的性能匹配。此外，本文还提出了一种模拟时间步更新的方法，并在一个流行病模型中进行了示例。<details>
<summary>Abstract</summary>
The ability to learn polynomials and generalize out-of-distribution is essential for simulation metamodels in many disciplines of engineering, where the time step updates are described by polynomials. While feed forward neural networks can fit any function, they cannot generalize out-of-distribution for higher-order polynomials. Therefore, this paper collects and proposes multiplicative neural network (MNN) architectures that are used as recursive building blocks for approximating higher-order polynomials. Our experiments show that MNNs are better than baseline models at generalizing, and their performance in validation is true to their performance in out-of-distribution tests. In addition to MNN architectures, a simulation metamodeling approach is proposed for simulations with polynomial time step updates. For these simulations, simulating a time interval can be performed in fewer steps by increasing the step size, which entails approximating higher-order polynomials. While our approach is compatible with any simulation with polynomial time step updates, a demonstration is shown for an epidemiology simulation model, which also shows the inductive bias in MNNs for learning and generalizing higher-order polynomials.
</details>
<details>
<summary>摘要</summary>
“ polynomial 和泛化 OUT-OF- Distribution 的能力是 simulation 模型中许多领域的工程学中的关键，其时间步骤更新由 polynomials 描述。虽然前向神经网络可以适应任何函数，但它们无法泛化 OUT-OF- Distribution  для高阶 polynomials。因此，这篇文章收集了和提议了乘法神经网络（MNN）架构，用于 Recursive 构建准确高阶 polynomials 的表示。我们的实验表明，MNNs 比基eline 模型更好地泛化，并且其在验证中的性能与 OUT-OF- Distribution 测试中的性能相符。此外，我们还提出了一种 simulation 模型的 meta-modeling 方法，用于 simulations 中的 polynomial 时间步骤更新。这些 simulations 中可以通过增大步长来缩短时间间隔，这意味着需要拟合高阶 polynomials。而我们的方法与任何 polynomial 时间步骤更新的 simulation 相容，我们在 epidemiology 模型中进行了示例，并表明了 MNNs 对于学习和泛化高阶 polynomials 的适应性。”Note: Simplified Chinese is used here, as it is more widely used in mainland China and is the standard for most online content. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Syntactic-vs-Semantic-Linear-Abstraction-and-Refinement-of-Neural-Networks"><a href="#Syntactic-vs-Semantic-Linear-Abstraction-and-Refinement-of-Neural-Networks" class="headerlink" title="Syntactic vs Semantic Linear Abstraction and Refinement of Neural Networks"></a>Syntactic vs Semantic Linear Abstraction and Refinement of Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10891">http://arxiv.org/abs/2307.10891</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cxlvinchau/linna">https://github.com/cxlvinchau/linna</a></li>
<li>paper_authors: Calvin Chau, Jan Křetínský, Stefanie Mohr</li>
<li>for: 提高神经网络的可扩展性</li>
<li>methods: 使用可变的线性组合来替换神经元，以实现更高的减少率</li>
<li>results: 实验表明，这种方法可以实现更好的减少率，并且可以在 sintactic 和 semantic 抽象中实现Here’s a breakdown of each point:</li>
<li>for: The paper aims to improve the scalability of neural networks by using abstraction techniques.</li>
<li>methods: The authors propose a more flexible framework that allows a neuron to be replaced with a linear combination of other neurons, leading to better reduction. They apply this approach both on syntactic and semantic abstractions and evaluate them experimentally.</li>
<li>results: The experimental results show that the proposed method can achieve better reduction compared to previous approaches, and can find a better balance between reduction and precision through a refinement method.<details>
<summary>Abstract</summary>
Abstraction is a key verification technique to improve scalability. However, its use for neural networks is so far extremely limited. Previous approaches for abstracting classification networks replace several neurons with one of them that is similar enough. We can classify the similarity as defined either syntactically (using quantities on the connections between neurons) or semantically (on the activation values of neurons for various inputs). Unfortunately, the previous approaches only achieve moderate reductions, when implemented at all. In this work, we provide a more flexible framework where a neuron can be replaced with a linear combination of other neurons, improving the reduction. We apply this approach both on syntactic and semantic abstractions, and implement and evaluate them experimentally. Further, we introduce a refinement method for our abstractions, allowing for finding a better balance between reduction and precision.
</details>
<details>
<summary>摘要</summary>
abstraction 是一种关键的验证技术，可以提高神经网络的扩展性。然而，它在神经网络上的使用尚未得到广泛应用。前一种方法是将多个神经元换为一个相似的神经元。我们可以分类定义相似性，使用语法（基于连接 между神经元的量）或 semantics（基于神经元对各个输入的活动值）。不幸的是，这些前一种方法只能实现moderate的减少，甚至无法实现。在这项工作中，我们提供了更flexible的框架，允许一个神经元被替换为一个线性组合的其他神经元，从而提高减少。我们在语法和semantics abstractions上应用这种方法，并进行了实验验证。此外，我们还介绍了一种精化方法，可以为我们的抽象提供更好的平衡 между减少和精度。
</details></li>
</ul>
<hr>
<h2 id="Player-optimal-Stable-Regret-for-Bandit-Learning-in-Matching-Markets"><a href="#Player-optimal-Stable-Regret-for-Bandit-Learning-in-Matching-Markets" class="headerlink" title="Player-optimal Stable Regret for Bandit Learning in Matching Markets"></a>Player-optimal Stable Regret for Bandit Learning in Matching Markets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10890">http://arxiv.org/abs/2307.10890</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fang Kong, Shuai Li</li>
<li>for: 这种问题的文章主要目标是解决匹配市场中稳定匹配的问题，以实现最大化参与者利益。</li>
<li>methods: 这篇文章提出了一种新的算法名为 explore-then-Gale-Shapley（ETGS），并证明了该算法可以实现参与者最优稳定 regret。</li>
<li>results: 文章显示了该算法可以实现参与者最优稳定 regret的Upper bound为O($K\log T&#x2F;\Delta^2$)，其中$K$是武器数量，$T$是时间戳，$\Delta$是参与者最小偏好差值。这个结果超越了之前的工作，其中 either有软化参与者稳定匹配目标或者只适用于特殊情况下的市场。当参与者偏好满足某些特殊条件时，我们的 regret upper bound也与之前 derive的下界匹配。<details>
<summary>Abstract</summary>
The problem of matching markets has been studied for a long time in the literature due to its wide range of applications. Finding a stable matching is a common equilibrium objective in this problem. Since market participants are usually uncertain of their preferences, a rich line of recent works study the online setting where one-side participants (players) learn their unknown preferences from iterative interactions with the other side (arms). Most previous works in this line are only able to derive theoretical guarantees for player-pessimal stable regret, which is defined compared with the players' least-preferred stable matching. However, under the pessimal stable matching, players only obtain the least reward among all stable matchings. To maximize players' profits, player-optimal stable matching would be the most desirable. Though \citet{basu21beyond} successfully bring an upper bound for player-optimal stable regret, their result can be exponentially large if players' preference gap is small. Whether a polynomial guarantee for this regret exists is a significant but still open problem. In this work, we provide a new algorithm named explore-then-Gale-Shapley (ETGS) and show that the optimal stable regret of each player can be upper bounded by $O(K\log T/\Delta^2)$ where $K$ is the number of arms, $T$ is the horizon and $\Delta$ is the players' minimum preference gap among the first $N+1$-ranked arms. This result significantly improves previous works which either have a weaker player-pessimal stable matching objective or apply only to markets with special assumptions. When the preferences of participants satisfy some special conditions, our regret upper bound also matches the previously derived lower bound.
</details>
<details>
<summary>摘要</summary>
problém matching markets 已经在文献中研究了很长时间，因为它在各种应用场景中具有广泛的应用前景。找到稳定匹配是这个问题的通用平衡目标。由于市场参与者通常不确定他们的偏好，一条丰富的最近的工作研究在线设置，在其中一侧参与者（玩家）通过多次交互来学习他们未知的偏好。大多数前一代工作只能 deriv theoretical guarantees for player-pessimal stable regret，这是defined compared with the players' least-preferred stable matching。然而，在稳定匹配中，玩家只能获得最低奖励中的一个稳定匹配。为了提高玩家的收益，player-optimal stable matching 是最愿望的。虽然 \citet{basu21beyond} 成功地提出了 player-optimal stable regret 的Upper bound，但其结果可能是 exponentially large 的。whether a polynomial guarantee for this regret exists 是一个重要但仍然打开的问题。在这个工作中，我们提出了一种新的算法 named explore-then-Gale-Shapley (ETGS) ，并证明了每个玩家的最佳稳定 regret 可以 upper bounded by $O(K\log T/\Delta^2)$，where $K$ 是枪手数， $T$ 是时间框架， $\Delta$ 是玩家最小的偏好差值中的第 $N+1$-名枪手。这个结果与前一代工作不同，它们的目标是 player-pessimal stable matching 或者只适用于特殊的市场假设。当偏好满足特定的条件时，我们的 regret upper bound 也与之前 derive的 lower bound 匹配。
</details></li>
</ul>
<hr>
<h2 id="What-Twitter-Data-Tell-Us-about-the-Future"><a href="#What-Twitter-Data-Tell-Us-about-the-Future" class="headerlink" title="What Twitter Data Tell Us about the Future?"></a>What Twitter Data Tell Us about the Future?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02035">http://arxiv.org/abs/2308.02035</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alina Landowska, Marek Robak, Maciej Skorski</li>
<li>for: 这个研究旨在调查Twitter上的未来推测，并研究语言指示器对 anticipatory thinking 的影响。</li>
<li>methods: 该研究使用 SOTA 模型建立可扩展的 NLP 管道，并通过 LDA 和 BERTopic 方法从未来推测者的微博中提取主题。</li>
<li>results: 研究发现未来推测者的微博中存在 15 个主题，并且使用 BERTopic 方法可以分化出 100 个不同的主题。这些发现对话题模型研究做出贡献，同时也提供了关于未来推测的社交媒体用户的反应和响应的新视角。<details>
<summary>Abstract</summary>
Anticipation is a fundamental human cognitive ability that involves thinking about and living towards the future. While language markers reflect anticipatory thinking, research on anticipation from the perspective of natural language processing is limited. This study aims to investigate the futures projected by futurists on Twitter and explore the impact of language cues on anticipatory thinking among social media users. We address the research questions of what futures Twitter's futurists anticipate and share, and how these anticipated futures can be modeled from social data. To investigate this, we review related works on anticipation, discuss the influence of language markers and prestigious individuals on anticipatory thinking, and present a taxonomy system categorizing futures into "present futures" and "future present". This research presents a compiled dataset of over 1 million publicly shared tweets by future influencers and develops a scalable NLP pipeline using SOTA models. The study identifies 15 topics from the LDA approach and 100 distinct topics from the BERTopic approach within the futurists' tweets. These findings contribute to the research on topic modelling and provide insights into the futures anticipated by Twitter's futurists. The research demonstrates the futurists' language cues signals futures-in-the-making that enhance social media users to anticipate their own scenarios and respond to them in present. The fully open-sourced dataset, interactive analysis, and reproducible source code are available for further exploration.
</details>
<details>
<summary>摘要</summary>
人类的预期是一种基本的认知能力，涉及到思考和生活未来。 although research on anticipation from the perspective of natural language processing is limited, this study aims to investigate the futures projected by futurists on Twitter and explore the impact of language cues on anticipatory thinking among social media users. We address the research questions of what futures Twitter's futurists anticipate and share, and how these anticipated futures can be modeled from social data. To investigate this, we review related works on anticipation, discuss the influence of language markers and prestigious individuals on anticipatory thinking, and present a taxonomy system categorizing futures into "present futures" and "future present". This research presents a compiled dataset of over 1 million publicly shared tweets by future influencers and develops a scalable NLP pipeline using state-of-the-art models. The study identifies 15 topics from the LDA approach and 100 distinct topics from the BERTopic approach within the futurists' tweets. These findings contribute to the research on topic modeling and provide insights into the futures anticipated by Twitter's futurists. The research demonstrates the futurists' language cues signal futures-in-the-making that enhance social media users to anticipate their own scenarios and respond to them in the present. The fully open-sourced dataset, interactive analysis, and reproducible source code are available for further exploration.
</details></li>
</ul>
<hr>
<h2 id="Risk-optimized-Outlier-Removal-for-Robust-Point-Cloud-Classification"><a href="#Risk-optimized-Outlier-Removal-for-Robust-Point-Cloud-Classification" class="headerlink" title="Risk-optimized Outlier Removal for Robust Point Cloud Classification"></a>Risk-optimized Outlier Removal for Robust Point Cloud Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10875">http://arxiv.org/abs/2307.10875</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinke Li, Junchi Lu</li>
<li>for: 这篇论文旨在提高安全敏感频谱深度模型的可靠性和安全性，应对意外或自然出现的点云变化。</li>
<li>methods: 方法是提出一种点云外liers removing方法，named PointCVaR，可以让标准训练的模型删除额外的外liers并恢复数据。</li>
<li>results: 实验结果显示，PointCVaR可以优化点云标签的精度，并在不同的点云变化中实现了优秀的移除和标签结果。特别是，它可以对抗后门攻击，删除触发器87%的精度。总的来说，PointCVaR是一个可靠的插件模组，可以帮助不同的模型在不同的情况下提高点云标签的精度。<details>
<summary>Abstract</summary>
The popularity of point cloud deep models for safety-critical purposes has increased, but the reliability and security of these models can be compromised by intentional or naturally occurring point cloud noise. To combat this issue, we present a novel point cloud outlier removal method called PointCVaR, which empowers standard-trained models to eliminate additional outliers and restore the data. Our approach begins by conducting attribution analysis to determine the influence of each point on the model output, which we refer to as point risk. We then optimize the process of filtering high-risk points using Conditional Value at Risk (CVaR) as the objective. The rationale for this approach is based on the observation that noise points in point clouds tend to cluster in the tail of the risk distribution, with a low frequency but a high level of risk, resulting in significant interference with classification results. Despite requiring no additional training effort, our method produces exceptional results in various removal-and-classification experiments for noisy point clouds, which are corrupted by random noise, adversarial noise, and backdoor trigger noise. Impressively, it achieves 87% accuracy in defense against the backdoor attack by removing triggers. Overall, the proposed PointCVaR effectively eliminates noise points and enhances point cloud classification, making it a promising plug-in module for various models in different scenarios.
</details>
<details>
<summary>摘要</summary>
popularity of point cloud deep models for safety-critical purposes has increased, but the reliability and security of these models can be compromised by intentional or naturally occurring point cloud noise. To combat this issue, we present a novel point cloud outlier removal method called PointCVaR, which empowers standard-trained models to eliminate additional outliers and restore the data. Our approach begins by conducting attribution analysis to determine the influence of each point on the model output, which we refer to as point risk. We then optimize the process of filtering high-risk points using Conditional Value at Risk (CVaR) as the objective. The rationale for this approach is based on the observation that noise points in point clouds tend to cluster in the tail of the risk distribution, with a low frequency but a high level of risk, resulting in significant interference with classification results. Despite requiring no additional training effort, our method produces exceptional results in various removal-and-classification experiments for noisy point clouds, which are corrupted by random noise, adversarial noise, and backdoor trigger noise. Impressively, it achieves 87% accuracy in defense against the backdoor attack by removing triggers. Overall, the proposed PointCVaR effectively eliminates noise points and enhances point cloud classification, making it a promising plug-in module for various models in different scenarios.
</details></li>
</ul>
<hr>
<h2 id="Nonlinear-Meta-Learning-Can-Guarantee-Faster-Rates"><a href="#Nonlinear-Meta-Learning-Can-Guarantee-Faster-Rates" class="headerlink" title="Nonlinear Meta-Learning Can Guarantee Faster Rates"></a>Nonlinear Meta-Learning Can Guarantee Faster Rates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10870">http://arxiv.org/abs/2307.10870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitri Meunier, Zhu Li, Arthur Gretton, Samory Kpotufe</li>
<li>for: 本文目标是解决meta-学习中关于如何利用相关任务的表示结构，以提高目标任务的学习效率和精度。</li>
<li>methods: 本文使用了非线性表示，并通过精心设计的正则项来减少任务特定的偏见。</li>
<li>results: 本文提供了关于meta-学习中非线性表示下的理论保证，并证明在适当的正则项支持下，可以减少任务特定的偏见，提高目标任务的学习效率和精度。<details>
<summary>Abstract</summary>
Many recent theoretical works on \emph{meta-learning} aim to achieve guarantees in leveraging similar representational structures from related tasks towards simplifying a target task. Importantly, the main aim in theory works on the subject is to understand the extent to which convergence rates -- in learning a common representation -- \emph{may scale with the number $N$ of tasks} (as well as the number of samples per task). First steps in this setting demonstrate this property when both the shared representation amongst tasks, and task-specific regression functions, are linear. This linear setting readily reveals the benefits of aggregating tasks, e.g., via averaging arguments. In practice, however, the representation is often highly nonlinear, introducing nontrivial biases in each task that cannot easily be averaged out as in the linear case. In the present work, we derive theoretical guarantees for meta-learning with nonlinear representations. In particular, assuming the shared nonlinearity maps to an infinite-dimensional RKHS, we show that additional biases can be mitigated with careful regularization that leverages the smoothness of task-specific regression functions,
</details>
<details>
<summary>摘要</summary>
很多最近的理论研究中的meta-学攻击目标是利用相关任务之间的相似表示结构来简化目标任务。重要的是，理论工作的主要目标是理解学习通用表示的速率如何随着任务数量 $N$ 和每个任务的样本数量增加。在线性设定中，首先的步骤表明了这个特性，当共享表示 amongst tasks 和任务特有的回归函数都是线性时，可以通过均值算法来汇集任务。然而，在实践中，表示通常是非线性的，引入了每个任务中的偏置，无法如线性情况那样轻松均值。在本工作中，我们提供了meta-学中非线性表示的理论保证。具体来说，假设共享非线性映射到无穷维度的RKHS中，我们表明了适当的REGULATION可以减轻每个任务中的偏置，同时利用任务特有的回归函数的平滑性。
</details></li>
</ul>
<hr>
<h2 id="Performance-Issue-Identification-in-Cloud-Systems-with-Relational-Temporal-Anomaly-Detection"><a href="#Performance-Issue-Identification-in-Cloud-Systems-with-Relational-Temporal-Anomaly-Detection" class="headerlink" title="Performance Issue Identification in Cloud Systems with Relational-Temporal Anomaly Detection"></a>Performance Issue Identification in Cloud Systems with Relational-Temporal Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10869">http://arxiv.org/abs/2307.10869</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ase-submission/rtanomaly">https://github.com/ase-submission/rtanomaly</a></li>
<li>paper_authors: Wenwei Gu, Jinyang Liu, Zhuangbin Chen, Jianping Zhang, Yuxin Su, Jiazhen Gu, Cong Feng, Zengyin Yang, Michael Lyu</li>
<li>for: 本研究旨在提高大规模云服务系统的可靠性，通过精准地识别和定位性能问题。</li>
<li>methods: 本研究提出了一种基于相关和时间特征的离异检测模型（RTAnomaly），利用图注意层学习度量变量之间的相关性，并采用正例学习方法来Address the issue of potential anomalies in the training data。</li>
<li>results: 对于公共数据集和两个工业数据集，RTAnomaly比基eline模型提高了0.929的F1分数和0.920的 Hit@3 分数，表明其在离异检测中的优越性。<details>
<summary>Abstract</summary>
Performance issues permeate large-scale cloud service systems, which can lead to huge revenue losses. To ensure reliable performance, it's essential to accurately identify and localize these issues using service monitoring metrics. Given the complexity and scale of modern cloud systems, this task can be challenging and may require extensive expertise and resources beyond the capacity of individual humans. Some existing methods tackle this problem by analyzing each metric independently to detect anomalies. However, this could incur overwhelming alert storms that are difficult for engineers to diagnose manually. To pursue better performance, not only the temporal patterns of metrics but also the correlation between metrics (i.e., relational patterns) should be considered, which can be formulated as a multivariate metrics anomaly detection problem. However, most of the studies fall short of extracting these two types of features explicitly. Moreover, there exist some unlabeled anomalies mixed in the training data, which may hinder the detection performance. To address these limitations, we propose the Relational- Temporal Anomaly Detection Model (RTAnomaly) that combines the relational and temporal information of metrics. RTAnomaly employs a graph attention layer to learn the dependencies among metrics, which will further help pinpoint the anomalous metrics that may cause the anomaly effectively. In addition, we exploit the concept of positive unlabeled learning to address the issue of potential anomalies in the training data. To evaluate our method, we conduct experiments on a public dataset and two industrial datasets. RTAnomaly outperforms all the baseline models by achieving an average F1 score of 0.929 and Hit@3 of 0.920, demonstrating its superiority.
</details>
<details>
<summary>摘要</summary>
大规模云服务系统中的性能问题会导致巨大的收益损失。为保证可靠性，需要准确地识别和定位这些问题，使用云服务监控指标。由于现代云系统的复杂性和规模，这个任务可能具有挑战性，需要大量的专业知识和资源。现有的方法通常是分别分析每个指标，检测异常。然而，这可能会导致惊人的警示暴露，很难 для工程师手动诊断。为了提高性能，不仅需要考虑时间序列中的指标异常，还需要考虑指标之间的相互关系（即关系异常），可以形式化为多变量指标异常检测问题。然而，大多数研究没有明确提取这两种特征。此外，许多无标签异常可能存在在训练数据中，这可能会降低检测性能。为解决这些限制，我们提出了关系时间异常检测模型（RTAnomaly）。RTAnomaly将关系和时间信息结合，以便更好地检测异常指标，并且通过图注意层学习指标之间的依赖关系，可以更加准确地定位异常指标。此外，我们利用无标签学习来解决训练数据中的潜在异常问题。我们在公共数据集和两个工业数据集上进行了实验，RTAnomaly比基线模型表现出色，取得了0.929的平均F1分数和0.920的 Hit@3 分数，这说明了它的优势。
</details></li>
</ul>
<hr>
<h2 id="FigCaps-HF-A-Figure-to-Caption-Generative-Framework-and-Benchmark-with-Human-Feedback"><a href="#FigCaps-HF-A-Figure-to-Caption-Generative-Framework-and-Benchmark-with-Human-Feedback" class="headerlink" title="FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback"></a>FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10867">http://arxiv.org/abs/2307.10867</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/figcapshf/figcapshf">https://github.com/figcapshf/figcapshf</a></li>
<li>paper_authors: Ashish Singh, Prateek Agarwal, Zixuan Huang, Arpita Singh, Tong Yu, Sungchul Kim, Victor Bursztyn, Nikos Vlassis, Ryan A. Rossi</li>
<li>for: 这篇论文的目的是提出一种新的插图标题生成框架，以便根据读者首选项生成高质量的插图标题。</li>
<li>methods: 该论文使用了一种自动评估插图标题对的方法，以及一种基于人工反馈学习（RLHF）的生成模型优化方法，以便根据读者首选项生成插图标题。</li>
<li>results: 该论文的实验结果表明，使用RLHF方法可以提高插图标题生成的性能， Specifically, when using BLIP as the base model, the RLHF framework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and Meteor, respectively.<details>
<summary>Abstract</summary>
Captions are crucial for understanding scientific visualizations and documents. Existing captioning methods for scientific figures rely on figure-caption pairs extracted from documents for training, many of which fall short with respect to metrics like helpfulness, explainability, and visual-descriptiveness [15] leading to generated captions being misaligned with reader preferences. To enable the generation of high-quality figure captions, we introduce FigCaps-HF a new framework for figure-caption generation that can incorporate domain expert feedback in generating captions optimized for reader preferences. Our framework comprises of 1) an automatic method for evaluating quality of figure-caption pairs, 2) a novel reinforcement learning with human feedback (RLHF) method to optimize a generative figure-to-caption model for reader preferences. We demonstrate the effectiveness of our simple learning framework by improving performance over standard fine-tuning across different types of models. In particular, when using BLIP as the base model, our RLHF framework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and Meteor, respectively. Finally, we release a large-scale benchmark dataset with human feedback on figure-caption pairs to enable further evaluation and development of RLHF techniques for this problem.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>科学视觉和文档中的标题是非常重要的，现有的科学标题生成方法依靠从文档中提取的标题-图像对进行训练，但是这些方法存在帮助度、可解释性和视觉描述性等缺点，导致生成的标题与读者需求不匹配 [15]。为了生成高质量的标题，我们介绍了 FigCaps-HF 一个新的标题生成框架，可以包含领域专家反馈来生成符合读者需求的标题。我们的框架包括以下两个部分：1. 自动评估标题-图像对的质量2. 一种基于人工反馈的强化学习法（RLHF）来优化一个可生成标题-图像模型，以满足读者需求。我们的简单学习框架可以在不同的模型上提高性能，特别是在使用 BLIP 作为基础模型时，我们的 RLHF 框架可以提高 ROUGE、BLEU 和 Meteor 的表现，具体的提高量分别为 35.7%、16.9% 和 9%。最后，我们发布了一个大规模的人类反馈标题-图像对数据集，以便进一步评估和开发RLHF技术。
</details></li>
</ul>
<hr>
<h2 id="Addressing-caveats-of-neural-persistence-with-deep-graph-persistence"><a href="#Addressing-caveats-of-neural-persistence-with-deep-graph-persistence" class="headerlink" title="Addressing caveats of neural persistence with deep graph persistence"></a>Addressing caveats of neural persistence with deep graph persistence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10865">http://arxiv.org/abs/2307.10865</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ExplainableML/Deep-Graph-Persistence">https://github.com/ExplainableML/Deep-Graph-Persistence</a></li>
<li>paper_authors: Leander Girrbach, Anders Christensen, Ole Winther, Zeynep Akata, A. Sophia Koepke</li>
<li>for: This paper aims to improve the measurement of neural network complexity using a new approach called deep graph persistence.</li>
<li>methods: The paper uses a combination of theoretical analysis and empirical experiments to study the relationship between neural network complexity and the variance of network weights, as well as the spatial concentration of large weights.</li>
<li>results: The paper finds that the proposed deep graph persistence measure provides a more accurate and robust way of measuring neural network complexity, compared to traditional measures such as neural persistence. Additionally, the paper shows that the deep graph persistence measure can capture persistent patterns in the network that are not present in individual layers.<details>
<summary>Abstract</summary>
Neural Persistence is a prominent measure for quantifying neural network complexity, proposed in the emerging field of topological data analysis in deep learning. In this work, however, we find both theoretically and empirically that the variance of network weights and spatial concentration of large weights are the main factors that impact neural persistence. Whilst this captures useful information for linear classifiers, we find that no relevant spatial structure is present in later layers of deep neural networks, making neural persistence roughly equivalent to the variance of weights. Additionally, the proposed averaging procedure across layers for deep neural networks does not consider interaction between layers. Based on our analysis, we propose an extension of the filtration underlying neural persistence to the whole neural network instead of single layers, which is equivalent to calculating neural persistence on one particular matrix. This yields our deep graph persistence measure, which implicitly incorporates persistent paths through the network and alleviates variance-related issues through standardisation. Code is available at https://github.com/ExplainableML/Deep-Graph-Persistence .
</details>
<details>
<summary>摘要</summary>
neural 稳定性是深度学习中的一个重要度量，在 topological data analysis 领域提出。在这项工作中，我们发现了理论和实验上，网络权重的方差和大权重的空间归一化是影响 neural 稳定性的主要因素。尽管这些信息对于线性分类器有用，但我们发现在深度神经网络中的后几层中没有任何相关的空间结构，因此 neural 稳定性大致等于权重的方差。此外，我们提出了层之间交互的权重平均方法，这并不考虑层之间的交互。基于我们的分析，我们提出了一种扩展层次滤波的方法，该方法等同于在一个特定矩阵上计算 neural 稳定性。这种方法会提取网络中 persist 的路径，并通过标准化来降低方差相关的问题。代码可以在 https://github.com/ExplainableML/Deep-Graph-Persistence 上获取。
</details></li>
</ul>
<hr>
<h2 id="Divide-Bind-Your-Attention-for-Improved-Generative-Semantic-Nursing"><a href="#Divide-Bind-Your-Attention-for-Improved-Generative-Semantic-Nursing" class="headerlink" title="Divide &amp; Bind Your Attention for Improved Generative Semantic Nursing"></a>Divide &amp; Bind Your Attention for Improved Generative Semantic Nursing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10864">http://arxiv.org/abs/2307.10864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yumeng Li, Margret Keuper, Dan Zhang, Anna Khoreva<br>for:* 这个研究旨在提高文本至图生成模型的表现，特别是在处理复杂的提示时。methods:* 本研究引入两个新的损失函数：一个新的注意力损失函数和一个绑定损失函数，以提高GSN的表现。results:* 这个方法可以将需要的物品具体地生成出来，并且具有改善的属性调和。在多个评估标准上表现出色。更多影片和更新可以在项目页面上找到：<a target="_blank" rel="noopener" href="https://sites.google.com/view/divide-and-bind">https://sites.google.com/view/divide-and-bind</a>。<details>
<summary>Abstract</summary>
Emerging large-scale text-to-image generative models, e.g., Stable Diffusion (SD), have exhibited overwhelming results with high fidelity. Despite the magnificent progress, current state-of-the-art models still struggle to generate images fully adhering to the input prompt. Prior work, Attend & Excite, has introduced the concept of Generative Semantic Nursing (GSN), aiming to optimize cross-attention during inference time to better incorporate the semantics. It demonstrates promising results in generating simple prompts, e.g., ``a cat and a dog''. However, its efficacy declines when dealing with more complex prompts, and it does not explicitly address the problem of improper attribute binding. To address the challenges posed by complex prompts or scenarios involving multiple entities and to achieve improved attribute binding, we propose Divide & Bind. We introduce two novel loss objectives for GSN: a novel attendance loss and a binding loss. Our approach stands out in its ability to faithfully synthesize desired objects with improved attribute alignment from complex prompts and exhibits superior performance across multiple evaluation benchmarks. More videos and updates can be found on the project page \url{https://sites.google.com/view/divide-and-bind}.
</details>
<details>
<summary>摘要</summary>
新型大规模文本至图生成模型，如稳定扩散（SD），已经显示出惊人的成绩，具有高准确性。 despite the outstanding progress, current state-of-the-art models still struggle to generate images that fully conform to the input prompt. Previous work, Attend & Excite, introduced the concept of Generative Semantic Nursing (GSN), which aims to optimize cross-attention during inference time to better incorporate semantics. It has shown promising results in generating simple prompts, such as "a cat and a dog". However, its effectiveness decreases when dealing with more complex prompts, and it does not explicitly address the problem of improper attribute binding.To address the challenges posed by complex prompts or scenarios involving multiple entities and to achieve improved attribute binding, we propose Divide & Bind. We introduce two novel loss objectives for GSN: a novel attendance loss and a binding loss. Our approach stands out in its ability to faithfully synthesize desired objects with improved attribute alignment from complex prompts and exhibits superior performance across multiple evaluation benchmarks. More videos and updates can be found on the project page [https://sites.google.com/view/divide-and-bind].
</details></li>
</ul>
<hr>
<h2 id="Self-paced-Weight-Consolidation-for-Continual-Learning"><a href="#Self-paced-Weight-Consolidation-for-Continual-Learning" class="headerlink" title="Self-paced Weight Consolidation for Continual Learning"></a>Self-paced Weight Consolidation for Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10845">http://arxiv.org/abs/2307.10845</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/congwei45/spWC">https://github.com/congwei45/spWC</a></li>
<li>paper_authors: Wei Cong, Yang Cong, Gan Sun, Yuyang Liu, Jiahua Dong<br>for:这个论文的目的是提出一个自适应的重量结构架构，以便在继续学习设定下避免悖论损坏。methods:这个架构使用了一个自适应的调节过程，以评估过去任务的推奨贡献。在接受新任务时，所有的过去任务会被排序为“困难”到“容易”的顺序，根据过去任务的优先级。然后，新的持续学习模型将通过选择性地维持过去任务中更困难的知识，以免于悖论损坏。results:实验结果显示，提出的自适应重量结构架构可以有效地提高表现，与其他具有流行的持续学习算法相比。<details>
<summary>Abstract</summary>
Continual learning algorithms which keep the parameters of new tasks close to that of previous tasks, are popular in preventing catastrophic forgetting in sequential task learning settings. However, 1) the performance for the new continual learner will be degraded without distinguishing the contributions of previously learned tasks; 2) the computational cost will be greatly increased with the number of tasks, since most existing algorithms need to regularize all previous tasks when learning new tasks. To address the above challenges, we propose a self-paced Weight Consolidation (spWC) framework to attain robust continual learning via evaluating the discriminative contributions of previous tasks. To be specific, we develop a self-paced regularization to reflect the priorities of past tasks via measuring difficulty based on key performance indicator (i.e., accuracy). When encountering a new task, all previous tasks are sorted from "difficult" to "easy" based on the priorities. Then the parameters of the new continual learner will be learned via selectively maintaining the knowledge amongst more difficult past tasks, which could well overcome catastrophic forgetting with less computational cost. We adopt an alternative convex search to iteratively update the model parameters and priority weights in the bi-convex formulation. The proposed spWC framework is plug-and-play, which is applicable to most continual learning algorithms (e.g., EWC, MAS and RCIL) in different directions (e.g., classification and segmentation). Experimental results on several public benchmark datasets demonstrate that our proposed framework can effectively improve performance when compared with other popular continual learning algorithms.
</details>
<details>
<summary>摘要</summary>
CONTINUAL LEARNING算法，它们可以保持新任务的参数与之前任务的参数很近，是sequential task learning setting中具有广泛应用的方法。但是，1）新的CONTINUAL LEARNING模型会受到之前任务的影响，而无法分别评估这些任务的贡献; 2）随着任务的增加，大多数现有算法的计算成本将会增加，因为它们需要在学习新任务时对所有之前任务进行规regularization。为了解决这些挑战，我们提出了一个自适应Weight Consolidation（spWC）框架，以实现robust continual learning。具体来说，我们开发了一种自适应规则，通过测量难度来衡量过去任务的优先级。当面临新任务时，我们将所有之前任务排序为“difficult”到“easy”，根据难度来决定哪些任务需要更多的学习。然后，我们将新的CONTINUAL LEARNING模型通过选择保持更难的过去任务的知识来学习，以避免catastrophic forgetting，同时减少计算成本。我们采用了一种alternative convex search来逐渐更新模型参数和优先级权重。我们的spWC框架可以与多种常见的CONTINUAL LEARNING算法（例如EWC、MAS和RCIL）相结合，并可以在不同的方向（例如分类和分割）中应用。我们在一些公共benchmark数据集上进行了实验，结果表明，我们的提出的框架可以效果地提高性能，与其他流行的CONTINUAL LEARNING算法相比。
</details></li>
</ul>
<hr>
<h2 id="Global-Precipitation-Nowcasting-of-Integrated-Multi-satellitE-Retrievals-for-GPM-A-U-Net-Convolutional-LSTM-Architecture"><a href="#Global-Precipitation-Nowcasting-of-Integrated-Multi-satellitE-Retrievals-for-GPM-A-U-Net-Convolutional-LSTM-Architecture" class="headerlink" title="Global Precipitation Nowcasting of Integrated Multi-satellitE Retrievals for GPM: A U-Net Convolutional LSTM Architecture"></a>Global Precipitation Nowcasting of Integrated Multi-satellitE Retrievals for GPM: A U-Net Convolutional LSTM Architecture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10843">http://arxiv.org/abs/2307.10843</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/reyhaneh-92/genesis_nowcast">https://github.com/reyhaneh-92/genesis_nowcast</a></li>
<li>paper_authors: Reyhaneh Rahimi, Ardeshir Ebtehaj, Ali Behrangi, Jackson Tan</li>
<li>for: 这个论文提出了一种深度学习架构，用于在全球范围内预测降水，每隔30分钟预测4个小时前的降水情况。</li>
<li>methods: 这个架构组合了U-Net和卷积Long Short-Term Memory（LSTM）神经网络，并使用IMERG和一些关键的预测变量从全球预报系统（GFS）进行训练。</li>
<li>results: 研究发现，使用不同的训练损失函数（如mean-squared error和focal-loss）对降水预报质量有着不同的影响，并且发现在降水极端情况（&gt;8mm&#x2F;hr）下，分类网络可以在降水预报中表现更好，特别是在较长的预测时间内。<details>
<summary>Abstract</summary>
This paper presents a deep learning architecture for nowcasting of precipitation almost globally every 30 min with a 4-hour lead time. The architecture fuses a U-Net and a convolutional long short-term memory (LSTM) neural network and is trained using data from the Integrated MultisatellitE Retrievals for GPM (IMERG) and a few key precipitation drivers from the Global Forecast System (GFS). The impacts of different training loss functions, including the mean-squared error (regression) and the focal-loss (classification), on the quality of precipitation nowcasts are studied. The results indicate that the regression network performs well in capturing light precipitation (below 1.6 mm/hr), but the classification network can outperform the regression network for nowcasting of precipitation extremes (>8 mm/hr), in terms of the critical success index (CSI).. Using the Wasserstein distance, it is shown that the predicted precipitation by the classification network has a closer class probability distribution to the IMERG than the regression network. It is uncovered that the inclusion of the physical variables can improve precipitation nowcasting, especially at longer lead times in both networks. Taking IMERG as a relative reference, a multi-scale analysis in terms of fractions skill score (FSS), shows that the nowcasting machine remains skillful (FSS > 0.5) at the resolution of 10 km compared to 50 km for GFS. For precipitation rates greater than 4~mm/hr, only the classification network remains FSS-skillful on scales greater than 50 km within a 2-hour lead time.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Label-Calibration-for-Semantic-Segmentation-Under-Domain-Shift"><a href="#Label-Calibration-for-Semantic-Segmentation-Under-Domain-Shift" class="headerlink" title="Label Calibration for Semantic Segmentation Under Domain Shift"></a>Label Calibration for Semantic Segmentation Under Domain Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10842">http://arxiv.org/abs/2307.10842</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ondrej Bohdal, Da Li, Timothy Hospedales</li>
<li>for: 这篇论文是用于解决 semantic segmentation 模型在新Domain的性能下降问题。</li>
<li>methods: 论文使用了将偏移量计算为软分类标准的方法，并根据这些标准来做预测。这个适应程序快速、几乎是 Computational resources 的价格，并带来了许多性能改善。</li>
<li>results: 论文显示了这个适应程序在实际上 Synthetic-to-real semantic segmentation 问题上的好处。<details>
<summary>Abstract</summary>
Performance of a pre-trained semantic segmentation model is likely to substantially decrease on data from a new domain. We show a pre-trained model can be adapted to unlabelled target domain data by calculating soft-label prototypes under the domain shift and making predictions according to the prototype closest to the vector with predicted class probabilities. The proposed adaptation procedure is fast, comes almost for free in terms of computational resources and leads to considerable performance improvements. We demonstrate the benefits of such label calibration on the highly-practical synthetic-to-real semantic segmentation problem.
</details>
<details>
<summary>摘要</summary>
“一个先行训练的 semantic segmentation 模型在新领域数据上的性能可能会显著下降。我们显示了一个先行模型可以通过对目标领域数据中的域shiftCalculate soft-label prototype并根据最近的 вектор预测类 probabilities进行预测。我们提出的适应过程快速、 Computational resources几乎不需要Extra cost，并导致显著性能提升。我们在实际上非常重要的 synthetic-to-real semantic segmentation 问题中证明了这种标签准化的好处。”Note: Simplified Chinese is used here, as it is the most widely used variety of Chinese in mainland China and Singapore. Traditional Chinese is used in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Conversational-Shaping-for-Intelligent-Agents"><a href="#Adversarial-Conversational-Shaping-for-Intelligent-Agents" class="headerlink" title="Adversarial Conversational Shaping for Intelligent Agents"></a>Adversarial Conversational Shaping for Intelligent Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11785">http://arxiv.org/abs/2307.11785</a></li>
<li>repo_url: None</li>
<li>paper_authors: Piotr Tarasiewicz, Sultan Kenjeyev, Ilana Sebag, Shehab Alshehabi</li>
<li>for: 这项研究旨在提高智能对话代理人的性能。</li>
<li>methods: 这两个模型都使用了对抗对话shape的方法，分别是基于REGS模型（Li et al., 18）的奖励授予模型和基于seq2seq和 transformers 的 reinforcement learning框架。</li>
<li>results: 研究表明，这两个模型在不同的训练细节下表现出色，能够提高智能对话代理人的性能。<details>
<summary>Abstract</summary>
The recent emergence of deep learning methods has enabled the research community to achieve state-of-the art results in several domains including natural language processing. However, the current robocall system remains unstable and inaccurate: text generator and chat-bots can be tedious and misunderstand human-like dialogue. In this work, we study the performance of two models able to enhance an intelligent conversational agent through adversarial conversational shaping: a generative adversarial network with policy gradient (GANPG) and a generative adversarial network with reward for every generation step (REGS) based on the REGS model presented in Li et al. [18] . This model is able to assign rewards to both partially and fully generated text sequences. We discuss performance with different training details : seq2seq [ 36] and transformers [37 ] in a reinforcement learning framework.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="What-Indeed-is-an-Achievable-Provable-Guarantee-for-Learning-Enabled-Safety-Critical-Systems"><a href="#What-Indeed-is-an-Achievable-Provable-Guarantee-for-Learning-Enabled-Safety-Critical-Systems" class="headerlink" title="What, Indeed, is an Achievable Provable Guarantee for Learning-Enabled Safety Critical Systems"></a>What, Indeed, is an Achievable Provable Guarantee for Learning-Enabled Safety Critical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11784">http://arxiv.org/abs/2307.11784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saddek Bensalem, Chih-Hong Cheng, Wei Huang, Xiaowei Huang, Changshun Wu, Xingyu Zhao</li>
<li>for: 本研究旨在解决机器学习技术在安全关键领域中可靠使用的挑战。</li>
<li>methods: 本文提出了设计和验证这类系统的工程和研究挑战，并基于现有工作无法实现可证明保证的观察，提出了两步验证方法以实现可证明统计保证。</li>
<li>results: 本文提出的两步验证方法可以实现可证明统计保证，并且可以应用于各种安全关键领域。<details>
<summary>Abstract</summary>
Machine learning has made remarkable advancements, but confidently utilising learning-enabled components in safety-critical domains still poses challenges. Among the challenges, it is known that a rigorous, yet practical, way of achieving safety guarantees is one of the most prominent. In this paper, we first discuss the engineering and research challenges associated with the design and verification of such systems. Then, based on the observation that existing works cannot actually achieve provable guarantees, we promote a two-step verification method for the ultimate achievement of provable statistical guarantees.
</details>
<details>
<summary>摘要</summary>
机器学习技术已经取得了很大的进步，但在安全关键领域中使用学习能力强 componenets仍存在挑战。其中一个最大的挑战是实现可证明的安全保证。在这篇论文中，我们首先讨论了设计和验证这些系统的工程和研究挑战。然后，根据现有的工作无法实际实现可证明的保证，我们提出了一种两步验证方法以实现可证明的统计保证。
</details></li>
</ul>
<hr>
<h2 id="On-Combining-Expert-Demonstrations-in-Imitation-Learning-via-Optimal-Transport"><a href="#On-Combining-Expert-Demonstrations-in-Imitation-Learning-via-Optimal-Transport" class="headerlink" title="On Combining Expert Demonstrations in Imitation Learning via Optimal Transport"></a>On Combining Expert Demonstrations in Imitation Learning via Optimal Transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10810">http://arxiv.org/abs/2307.10810</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ilanasebag/Sliced-MMOT-Imitation-Learning">https://github.com/ilanasebag/Sliced-MMOT-Imitation-Learning</a></li>
<li>paper_authors: Ilana Sebag, Samuel Cohen, Marc Peter Deisenroth</li>
<li>for: 教学机器人特定任务 через专家示范</li>
<li>methods: 使用优化运输方法度量机器人与专家轨迹之间的距离，并将多个专家示范combined in the OT sense</li>
<li>results: 在OpenAI Gym控制环境中，提供了一种可以学习多个专家示范的方法，并且分析了其效率，发现标准方法并不总是优化的。<details>
<summary>Abstract</summary>
Imitation learning (IL) seeks to teach agents specific tasks through expert demonstrations. One of the key approaches to IL is to define a distance between agent and expert and to find an agent policy that minimizes that distance. Optimal transport methods have been widely used in imitation learning as they provide ways to measure meaningful distances between agent and expert trajectories. However, the problem of how to optimally combine multiple expert demonstrations has not been widely studied. The standard method is to simply concatenate state (-action) trajectories, which is problematic when trajectories are multi-modal. We propose an alternative method that uses a multi-marginal optimal transport distance and enables the combination of multiple and diverse state-trajectories in the OT sense, providing a more sensible geometric average of the demonstrations. Our approach enables an agent to learn from several experts, and its efficiency is analyzed on OpenAI Gym control environments and demonstrates that the standard method is not always optimal.
</details>
<details>
<summary>摘要</summary>
模仿学习（IL） aimed to teach agents specific tasks through expert demonstrations. One of the key approaches to IL is to define a distance between agent and expert and to find an agent policy that minimizes that distance. Optimal transport methods have been widely used in imitation learning as they provide ways to measure meaningful distances between agent and expert trajectories. However, the problem of how to optimally combine multiple expert demonstrations has not been widely studied. The standard method is to simply concatenate state (-action) trajectories, which is problematic when trajectories are multi-modal. We propose an alternative method that uses a multi-marginal optimal transport distance and enables the combination of multiple and diverse state-trajectories in the OT sense, providing a more sensible geometric average of the demonstrations. Our approach enables an agent to learn from several experts, and its efficiency is analyzed on OpenAI Gym control environments and demonstrates that the standard method is not always optimal.Here's the translation in Traditional Chinese as well:模仿学习（IL）目的是教导代理人特定任务通过专家示范。一个关键的IL方法是定义代理人和专家之间的距离，并寻找一个代理人策略可以最小化这个距离。优化交通方法在模仿学习中被广泛使用，因为它们提供了在代理人和专家路径之间量化意义的距离。然而，多个专家示范之间的并合尚未受到广泛研究。标准方法是将state (-action) trajectories concatenated，却当路径为多模的时候，这个方法有问题。我们提出了一个替代方法，使用多个中心优化交通距离，允许多元和多样的state-trajectories在OT意义下并合，提供一个更有意义的几何平均。我们的方法允许代理人从多个专家学习，并在OpenAI Gym控制环境中分析了其效率，证明标准方法不一定是最佳。
</details></li>
</ul>
<hr>
<h2 id="Communication-Efficient-Split-Learning-via-Adaptive-Feature-Wise-Compression"><a href="#Communication-Efficient-Split-Learning-via-Adaptive-Feature-Wise-Compression" class="headerlink" title="Communication-Efficient Split Learning via Adaptive Feature-Wise Compression"></a>Communication-Efficient Split Learning via Adaptive Feature-Wise Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10805">http://arxiv.org/abs/2307.10805</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongjeong Oh, Jaeho Lee, Christopher G. Brinton, Yo-Seb Jeon</li>
<li>for: 提高SL训练过程中通信开销的效率</li>
<li>methods: 利用矩阵列中具有不同散度的特征，并采用适应性的特征排除和特征量化两种压缩策略</li>
<li>results: 在MNIST、CIFAR-10和CelebA datasets上，对比当前SL框架，提高了5.6%以上的分类精度，同时压缩开销比vanilla SL框架减少了320倍。<details>
<summary>Abstract</summary>
This paper proposes a novel communication-efficient split learning (SL) framework, named SplitFC, which reduces the communication overhead required for transmitting intermediate feature and gradient vectors during the SL training process. The key idea of SplitFC is to leverage different dispersion degrees exhibited in the columns of the matrices. SplitFC incorporates two compression strategies: (i) adaptive feature-wise dropout and (ii) adaptive feature-wise quantization. In the first strategy, the intermediate feature vectors are dropped with adaptive dropout probabilities determined based on the standard deviation of these vectors. Then, by the chain rule, the intermediate gradient vectors associated with the dropped feature vectors are also dropped. In the second strategy, the non-dropped intermediate feature and gradient vectors are quantized using adaptive quantization levels determined based on the ranges of the vectors. To minimize the quantization error, the optimal quantization levels of this strategy are derived in a closed-form expression. Simulation results on the MNIST, CIFAR-10, and CelebA datasets demonstrate that SplitFC provides more than a 5.6% increase in classification accuracy compared to state-of-the-art SL frameworks, while they require 320 times less communication overhead compared to the vanilla SL framework without compression.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Adaptive feature-wise dropout: Intermediate feature vectors are dropped with adaptive dropout probabilities determined based on the standard deviation of these vectors. As a result, the intermediate gradient vectors associated with the dropped feature vectors are also dropped.2. Adaptive feature-wise quantization: Non-dropped intermediate feature and gradient vectors are quantized using adaptive quantization levels determined based on the ranges of the vectors. To minimize quantization error, the optimal quantization levels are derived in a closed-form expression.Compared to state-of-the-art SL frameworks, SplitFC provides more than a 5.6% increase in classification accuracy while requiring 320 times less communication overhead. This is demonstrated through simulation results on the MNIST, CIFAR-10, and CelebA datasets.</details></li>
</ol>
<hr>
<h2 id="Spatial-Temporal-Data-Mining-for-Ocean-Science-Data-Methodologies-and-Opportunities"><a href="#Spatial-Temporal-Data-Mining-for-Ocean-Science-Data-Methodologies-and-Opportunities" class="headerlink" title="Spatial-Temporal Data Mining for Ocean Science: Data, Methodologies, and Opportunities"></a>Spatial-Temporal Data Mining for Ocean Science: Data, Methodologies, and Opportunities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10803">http://arxiv.org/abs/2307.10803</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanchen Yang, Wengen Li, Shuyu Wang, Hui Li, Jihong Guan, Shuigeng Zhou, Jiannong Cao</li>
<li>for: 本研究提供了 ocean 科学领域中存在数据挖掘的彻底评估，帮助计算机科学家和海洋科学家更好地理解 ocean 数据挖掘的基本概念、关键技术和开放问题。</li>
<li>methods: 本研究涵盖了 ocean 数据质量提升技术、存储和检索技术、数据挖掘技术等多个方面。</li>
<li>results: 本研究对 ocean 数据挖掘的各种应用进行了分类和详细介绍，包括预测、事件检测、模式挖掘和异常检测等多种任务。同时，本研究还提出了一些可能的研究机遇。<details>
<summary>Abstract</summary>
With the rapid amassing of spatial-temporal (ST) ocean data, many spatial-temporal data mining (STDM) studies have been conducted to address various oceanic issues, including climate forecasting and disaster warning. Compared with typical ST data (e.g., traffic data), ST ocean data is more complicated but with unique characteristics, e.g., diverse regionality and high sparsity. These characteristics make it difficult to design and train STDM models on ST ocean data. To the best of our knowledge, a comprehensive survey of existing studies remains missing in the literature, which hinders not only computer scientists from identifying the research issues in ocean data mining but also ocean scientists to apply advanced STDM techniques. In this paper, we provide a comprehensive survey of existing STDM studies for ocean science. Concretely, we first review the widely-used ST ocean datasets and highlight their unique characteristics. Then, typical ST ocean data quality enhancement techniques are explored. Next, we classify existing STDM studies in ocean science into four types of tasks, i.e., prediction, event detection, pattern mining, and anomaly detection, and elaborate on the techniques for these tasks. Finally, promising research opportunities are discussed. This survey can help scientists from both computer science and ocean science better understand the fundamental concepts, key techniques, and open challenges of STDM for ocean science.
</details>
<details>
<summary>摘要</summary>
随着空间时间（ST）海洋数据的快速积累，许多空间时间数据挖掘（STDM）研究已经进行以解决海洋问题，如气候预测和自然灾害警报。与常见的ST数据（例如交通数据）相比，ST海洋数据更加复杂，但具有独特特征，如多样性和高稀缺。这些特征使得设计和训练STDM模型在ST海洋数据上变得更加困难。据我们所知，现有的相关研究的总体评估在文献中缺失，这不仅阻碍了计算机科学家从海洋数据挖掘中了解研究问题，也阻碍了海洋科学家应用高级STDM技术。在这篇论文中，我们提供了海洋科学领域的STDM研究的全面评估。具体来说，我们首先介绍了广泛使用的ST海洋数据集和其独特特征。然后，我们探讨了常见的ST海洋数据质量提升技术。接着，我们将现有的STDM研究在海洋科学领域分为四类任务，即预测、事件检测、模式挖掘和异常检测，并详细介绍这些任务的技术。最后，我们提出了有前途的研究机遇。这种调查可以帮助计算机科学家和海洋科学家更好地理解STDM的基本概念、关键技术和开放的挑战，并且推动海洋科学领域的发展。
</details></li>
</ul>
<hr>
<h2 id="Meta-Transformer-A-Unified-Framework-for-Multimodal-Learning"><a href="#Meta-Transformer-A-Unified-Framework-for-Multimodal-Learning" class="headerlink" title="Meta-Transformer: A Unified Framework for Multimodal Learning"></a>Meta-Transformer: A Unified Framework for Multimodal Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10802">http://arxiv.org/abs/2307.10802</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/invictus717/MetaTransformer">https://github.com/invictus717/MetaTransformer</a></li>
<li>paper_authors: Yiyuan Zhang, Kaixiong Gong, Kaipeng Zhang, Hongsheng Li, Yu Qiao, Wanli Ouyang, Xiangyu Yue</li>
<li>For: The paper aims to build a unified multimodal intelligence framework using transformers, which can handle various modalities such as natural language, images, point clouds, audio, video, and more, without paired multimodal training data.* Methods: The proposed framework, called Meta-Transformer, uses a frozen encoder to extract high-level semantic features from raw input data from different modalities, and then applies task-specific heads for downstream tasks.* Results: The paper demonstrates the effectiveness of Meta-Transformer on a wide range of tasks, including fundamental perception, practical applications, and data mining, across 12 modalities, and shows that it outperforms existing multimodal learning methods.<details>
<summary>Abstract</summary>
Multimodal learning aims to build models that can process and relate information from multiple modalities. Despite years of development in this field, it still remains challenging to design a unified network for processing various modalities ($\textit{e.g.}$ natural language, 2D images, 3D point clouds, audio, video, time series, tabular data) due to the inherent gaps among them. In this work, we propose a framework, named Meta-Transformer, that leverages a $\textbf{frozen}$ encoder to perform multimodal perception without any paired multimodal training data. In Meta-Transformer, the raw input data from various modalities are mapped into a shared token space, allowing a subsequent encoder with frozen parameters to extract high-level semantic features of the input data. Composed of three main components: a unified data tokenizer, a modality-shared encoder, and task-specific heads for downstream tasks, Meta-Transformer is the first framework to perform unified learning across 12 modalities with unpaired data. Experiments on different benchmarks reveal that Meta-Transformer can handle a wide range of tasks including fundamental perception (text, image, point cloud, audio, video), practical application (X-Ray, infrared, hyperspectral, and IMU), and data mining (graph, tabular, and time-series). Meta-Transformer indicates a promising future for developing unified multimodal intelligence with transformers. Code will be available at https://github.com/invictus717/MetaTransformer
</details>
<details>
<summary>摘要</summary>
多模态学习旨在建立能处理和关联多种模式的模型。尽管这一领域已有多年发展，仍然困难设计能处理多种模式的统一网络（例如自然语言、2D图像、3D点云、音频、视频、时间序列、表格数据）的网络，因为这些模式之间存在基本的差异。在这种情况下，我们提出了一种名为Meta-Transformer的框架，它利用一个冻结的encoder来实现多模式感知，不需要任何协调的多模式培训数据。在Meta-Transformer框架中，各种模式的原始输入数据都会被映射到一个共享的token空间中，以便后续的encoder WITH冻结参数可以从输入数据中提取高级别的 semantic feature。Meta-Transformer框架包括三个主要组件：一个统一的数据tokenizer、一个共享的encoder和下游任务特定的头。Meta-Transformer是首个可以通过不协调的数据进行多模式学习的框架，实验表明，它可以处理各种任务，包括基本的感知（文本、图像、点云、音频、视频）、实用应用（X射、抗红外、多spectral、IMU）和数据挖掘（图表、表格、时间序列）。Meta-Transformer表明了未来多模式智能的发展将受到transformer的推动。代码将在https://github.com/invictus717/MetaTransformer上提供。
</details></li>
</ul>
<hr>
<h2 id="Convergence-of-Adam-for-Non-convex-Objectives-Relaxed-Hyperparameters-and-Non-ergodic-Case"><a href="#Convergence-of-Adam-for-Non-convex-Objectives-Relaxed-Hyperparameters-and-Non-ergodic-Case" class="headerlink" title="Convergence of Adam for Non-convex Objectives: Relaxed Hyperparameters and Non-ergodic Case"></a>Convergence of Adam for Non-convex Objectives: Relaxed Hyperparameters and Non-ergodic Case</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11782">http://arxiv.org/abs/2307.11782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meixuan He, Yuqing Liang, Jinlan Liu, Dongpo Xu</li>
<li>for: 本文探讨了 Adam 优化算法在非对称 Setting 中的收敛性，特别是在实际应用中遇到的非平衡收敛问题。</li>
<li>methods: 本文引入了精确的ergodic和非ergodic收敛定义，并论证了这些定义对 Stochastic optimization algorithm 的收敛性的重要性。同时，本文提出了一种更lax的 suficient condition  для Adam 的ergodic收敛保证，并实现了对 K 的 almost sure ergodic 收敛率，其准确性可以到 $o(1&#x2F;\sqrt{K})$。</li>
<li>results: 本文证明了 Adam 的最后一个迭代 converges to a stationary point  для非对称目标函数，这是首次证明。此外，本文还证明了在 PL condition 下，Adam 的非ergodic收敛率为 O(1&#x2F;K)。这些发现为 Adam 在解决非对称 Stochastic optimization problems 提供了坚实的理论基础。<details>
<summary>Abstract</summary>
Adam is a commonly used stochastic optimization algorithm in machine learning. However, its convergence is still not fully understood, especially in the non-convex setting. This paper focuses on exploring hyperparameter settings for the convergence of vanilla Adam and tackling the challenges of non-ergodic convergence related to practical application. The primary contributions are summarized as follows: firstly, we introduce precise definitions of ergodic and non-ergodic convergence, which cover nearly all forms of convergence for stochastic optimization algorithms. Meanwhile, we emphasize the superiority of non-ergodic convergence over ergodic convergence. Secondly, we establish a weaker sufficient condition for the ergodic convergence guarantee of Adam, allowing a more relaxed choice of hyperparameters. On this basis, we achieve the almost sure ergodic convergence rate of Adam, which is arbitrarily close to $o(1/\sqrt{K})$. More importantly, we prove, for the first time, that the last iterate of Adam converges to a stationary point for non-convex objectives. Finally, we obtain the non-ergodic convergence rate of $O(1/K)$ for function values under the Polyak-Lojasiewicz (PL) condition. These findings build a solid theoretical foundation for Adam to solve non-convex stochastic optimization problems.
</details>
<details>
<summary>摘要</summary>
亚当是机器学习中常用的数学估计算法，但其对于非凸设计仍然未完全理解。本文专注于探索亚当的参数设定，以提高实际应用中的非ergodic convergence。主要贡献如下：首先，我们提出了精确的ergodic和non-ergodic convergence定义，包括大部分数学估计算法的各种对应。同时，我们强调了非ergodic convergence的超越性。第二，我们提出了一个较弱的充分condition для亚当的ergodic convergence保证，允许更宽松的参数选择。基于这个基础，我们获得了逐渐趋向于$o(1/\sqrt{K})$的almost sure ergodic convergence率，这是实际上的阶段趋向。更重要的是，我们证明了亚当的最后迭代确实会 converges to a stationary point for non-convex objectives，这是首次证明。最后，我们获得了function values的non-ergodic convergence率为$O(1/K)$，这是PL condition下的。这些成果创造了机器学习中亚当解决非凸数学估计问题的坚实理论基础。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-PatchCore-for-Few-many-shot-Anomaly-Detection"><a href="#Optimizing-PatchCore-for-Few-many-shot-Anomaly-Detection" class="headerlink" title="Optimizing PatchCore for Few&#x2F;many-shot Anomaly Detection"></a>Optimizing PatchCore for Few&#x2F;many-shot Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10792">http://arxiv.org/abs/2307.10792</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/scortexio/patchcore-few-shot">https://github.com/scortexio/patchcore-few-shot</a></li>
<li>paper_authors: João Santos, Triet Tran, Oliver Rippel</li>
<li>for: 本研究探讨了使用only few selected samples进行异常检测（AD）的新兴领域，并评估了现有的普shot AD算法在这种设定下的性能。</li>
<li>methods: 本研究使用了PatchCore算法，current state-of-the-art full-shot AD&#x2F;AS algorithm，并对其在few-shot和many-shot设定下进行了性能研究。</li>
<li>results: 研究发现，可以通过优化算法的多种 гиперparameters来实现显著性能提升，并且可以通过将supervised learning中的技术转移到AD领域来进一步提高性能。基于这些发现，我们在VisA dataset上达到了新的state of the art in few-shot AD。<details>
<summary>Abstract</summary>
Few-shot anomaly detection (AD) is an emerging sub-field of general AD, and tries to distinguish between normal and anomalous data using only few selected samples. While newly proposed few-shot AD methods do compare against pre-existing algorithms developed for the full-shot domain as baselines, they do not dedicatedly optimize them for the few-shot setting. It thus remains unclear if the performance of such pre-existing algorithms can be further improved. We address said question in this work. Specifically, we present a study on the AD/anomaly segmentation (AS) performance of PatchCore, the current state-of-the-art full-shot AD/AS algorithm, in both the few-shot and the many-shot settings. We hypothesize that further performance improvements can be realized by (I) optimizing its various hyperparameters, and by (II) transferring techniques known to improve few-shot supervised learning to the AD domain. Exhaustive experiments on the public VisA and MVTec AD datasets reveal that (I) significant performance improvements can be realized by optimizing hyperparameters such as the underlying feature extractor, and that (II) image-level augmentations can, but are not guaranteed, to improve performance. Based on these findings, we achieve a new state of the art in few-shot AD on VisA, further demonstrating the merit of adapting pre-existing AD/AS methods to the few-shot setting. Last, we identify the investigation of feature extractors with a strong inductive bias as a potential future research direction for (few-shot) AD/AS.
</details>
<details>
<summary>摘要</summary>
几个shot异常检测（AD）是一个出现的子领域，它目标是通过选择一些示例来分辨正常数据和异常数据。新提出的几个shot AD方法与全shot领域的已有算法进行比较，但它们并未专门优化这些算法 для几个shot设置。因此，它们的性能是否可以进一步改进是 unclear。我们在这里回答这个问题。我们对PatchCore，当前的全shot AD/AS算法，在几个shot和多个shot的设置下进行了研究。我们认为可以通过（I）优化其多种超参数，以及（II）将几个shot超参数优化技术转移到AD领域来实现性能改进。我们对公共的VisA和MVTec AD数据集进行了详细的实验，发现（I）可以通过优化特征提取器来实现显著性能提升，并且（II）图像级别的扩展可以，但并不一定，提高性能。根据这些发现，我们在VisA上达到了新的状态机器，进一步证明了适应已有AD/AS方法到几个shot设置的价值。最后，我们认为在（几个shot）AD/AS领域中研究具有强 inductive bias 的特征提取器是一个未来研究方向。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-attacks-for-mixtures-of-classifiers"><a href="#Adversarial-attacks-for-mixtures-of-classifiers" class="headerlink" title="Adversarial attacks for mixtures of classifiers"></a>Adversarial attacks for mixtures of classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10788">http://arxiv.org/abs/2307.10788</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas Gnecco Heredia, Benjamin Negrevergne, Yann Chevaleyre</li>
<li>for: 提高鲁棒性 against adversarial attacks</li>
<li>methods: 使用 mixtures of classifiers (a.k.a. randomized ensembles)</li>
<li>results: 引入了两个可能性的攻击方法，并提供了对binary linear setting的理论保证，并在 sintethic 和实际数据集上进行了实验。<details>
<summary>Abstract</summary>
Mixtures of classifiers (a.k.a. randomized ensembles) have been proposed as a way to improve robustness against adversarial attacks. However, it has been shown that existing attacks are not well suited for this kind of classifiers. In this paper, we discuss the problem of attacking a mixture in a principled way and introduce two desirable properties of attacks based on a geometrical analysis of the problem (effectiveness and maximality). We then show that existing attacks do not meet both of these properties. Finally, we introduce a new attack called lattice climber attack with theoretical guarantees on the binary linear setting, and we demonstrate its performance by conducting experiments on synthetic and real datasets.
</details>
<details>
<summary>摘要</summary>
Mixtures of classifiers（也称为随机集合）已经被提议用于提高对抗敌意攻击的鲁棒性。然而，已经证明现有的攻击方法不适合这种类型的分类器。在这篇论文中，我们讨论了攻击混合的问题，并引入了两种攻击的愿景性质（有效性和最大化）。然后，我们证明现有的攻击方法不符合这两种质量。最后，我们介绍了一种新的攻击方法called lattice climber attack，并提供了在二元线性设定下的理论保证。我们通过synthetic和实际数据进行了实验，以证明其性能。
</details></li>
</ul>
<hr>
<h2 id="Feed-Forward-Source-Free-Domain-Adaptation-via-Class-Prototypes"><a href="#Feed-Forward-Source-Free-Domain-Adaptation-via-Class-Prototypes" class="headerlink" title="Feed-Forward Source-Free Domain Adaptation via Class Prototypes"></a>Feed-Forward Source-Free Domain Adaptation via Class Prototypes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10787">http://arxiv.org/abs/2307.10787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ondrej Bohdal, Da Li, Timothy Hospedales</li>
<li>for: 这篇论文是为了探讨源自由适应领域的问题，即无需访问源数据就可以进行适应。</li>
<li>methods: 该论文提出了一种简单的前向方法，挑战了基于反传播的适应方法的需要。该方法基于使用预训练模型计算类别的原型，并实现了与预训练模型相比的强大改进。</li>
<li>results: 该论文的实验结果表明，该方法可以在短时间内达到高度的准确率，仅需要小于源适应方法的时间。<details>
<summary>Abstract</summary>
Source-free domain adaptation has become popular because of its practical usefulness and no need to access source data. However, the adaptation process still takes a considerable amount of time and is predominantly based on optimization that relies on back-propagation. In this work we present a simple feed-forward approach that challenges the need for back-propagation based adaptation. Our approach is based on computing prototypes of classes under the domain shift using a pre-trained model. It achieves strong improvements in accuracy compared to the pre-trained model and requires only a small fraction of time of existing domain adaptation methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Efficient-Beam-Tree-Recursion"><a href="#Efficient-Beam-Tree-Recursion" class="headerlink" title="Efficient Beam Tree Recursion"></a>Efficient Beam Tree Recursion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10779">http://arxiv.org/abs/2307.10779</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jishnu Ray Chowdhury, Cornelia Caragea</li>
<li>for: 提高 BT-RvNN 的可扩展性和可读性，以及将其作为深度学习工具箱中的另一个构建件。</li>
<li>methods: 识别 BT-RvNN 中最主要的瓶颈为探索函数和回归细胞函数的杂mix问题，并提出了解决方案，以减少 BT-RvNN 的内存使用情况。</li>
<li>results: 通过提出的策略，可以将 BT-RvNN 的内存使用情况减少 $10$-$16$ 倍，同时在 ListOps 中创造出新的状态ulse术，并在其他任务上保持相似的性能。<details>
<summary>Abstract</summary>
Beam Tree Recursive Neural Network (BT-RvNN) was recently proposed as a simple extension of Gumbel Tree RvNN and it was shown to achieve state-of-the-art length generalization performance in ListOps while maintaining comparable performance on other tasks. However, although not the worst in its kind, BT-RvNN can be still exorbitantly expensive in memory usage. In this paper, we identify the main bottleneck in BT-RvNN's memory usage to be the entanglement of the scorer function and the recursive cell function. We propose strategies to remove this bottleneck and further simplify its memory usage. Overall, our strategies not only reduce the memory usage of BT-RvNN by $10$-$16$ times but also create a new state-of-the-art in ListOps while maintaining similar performance in other tasks. In addition, we also propose a strategy to utilize the induced latent-tree node representations produced by BT-RvNN to turn BT-RvNN from a sentence encoder of the form $f:\mathbb{R}^{n \times d} \rightarrow \mathbb{R}^{d}$ into a sequence contextualizer of the form $f:\mathbb{R}^{n \times d} \rightarrow \mathbb{R}^{n \times d}$. Thus, our proposals not only open up a path for further scalability of RvNNs but also standardize a way to use BT-RvNNs as another building block in the deep learning toolkit that can be easily stacked or interfaced with other popular models such as Transformers and Structured State Space models.
</details>
<details>
<summary>摘要</summary>
“Recently proposed Beam Tree Recursive Neural Network (BT-RvNN) 可以实现列表作业中的最佳长度决定性性能，但是它的内存使用量仍然很高。在本文中，我们识别了BT-RvNN的主要瓶颈是探索函数和遗传函数的混合。我们提出了一些策略来解决这个瓶颈，包括将探索函数和遗传函数分开，以及将BT-RvNN的构成方式改进。我们的策略可以将BT-RvNN的内存使用量降低至10-16倍，并且创建了新的最佳性能。此外，我们还提出了一个使用BT-RvNN生成的隐藏树内节表示来转换BT-RvNN从句子编码器的形式($f:\mathbb{R}^{n \times d} \rightarrow \mathbb{R}^{d}$)到序列内容调整器的形式($f:\mathbb{R}^{n \times d} \rightarrow \mathbb{R}^{n \times d}$)。因此，我们的提议不仅开启了RvNN的扩展之路，还标准化了使用BT-RvNN作为深度学习工具箱中的一个建立物件，可以轻松地堆叠或与其他受欢迎的模型相互作用，如Transformers和结构状态空间模型。”
</details></li>
</ul>
<hr>
<h2 id="Assessing-the-Use-of-AutoML-for-Data-Driven-Software-Engineering"><a href="#Assessing-the-Use-of-AutoML-for-Data-Driven-Software-Engineering" class="headerlink" title="Assessing the Use of AutoML for Data-Driven Software Engineering"></a>Assessing the Use of AutoML for Data-Driven Software Engineering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10774">http://arxiv.org/abs/2307.10774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabio Calefato, Luigi Quaranta, Filippo Lanubile, Marcos Kalinowski</li>
<li>for:  This paper aims to investigate the current state of adoption and perception of AutoML (Automated Machine Learning) in the software engineering (SE) community, and to provide insights for researchers and tool builders.</li>
<li>methods:  The study uses a mixed-method approach, including a benchmark of 12 end-to-end AutoML tools on two SE datasets and a user survey with follow-up interviews.</li>
<li>results:  The study found that AutoML solutions can generate models that outperform those trained and optimized by researchers for classification tasks in the SE domain, but the currently available solutions do not fully support automation across all stages of the ML development workflow and for all team members.<details>
<summary>Abstract</summary>
Background. Due to the widespread adoption of Artificial Intelligence (AI) and Machine Learning (ML) for building software applications, companies are struggling to recruit employees with a deep understanding of such technologies. In this scenario, AutoML is soaring as a promising solution to fill the AI/ML skills gap since it promises to automate the building of end-to-end AI/ML pipelines that would normally be engineered by specialized team members. Aims. Despite the growing interest and high expectations, there is a dearth of information about the extent to which AutoML is currently adopted by teams developing AI/ML-enabled systems and how it is perceived by practitioners and researchers. Method. To fill these gaps, in this paper, we present a mixed-method study comprising a benchmark of 12 end-to-end AutoML tools on two SE datasets and a user survey with follow-up interviews to further our understanding of AutoML adoption and perception. Results. We found that AutoML solutions can generate models that outperform those trained and optimized by researchers to perform classification tasks in the SE domain. Also, our findings show that the currently available AutoML solutions do not live up to their names as they do not equally support automation across the stages of the ML development workflow and for all the team members. Conclusions. We derive insights to inform the SE research community on how AutoML can facilitate their activities and tool builders on how to design the next generation of AutoML technologies.
</details>
<details>
<summary>摘要</summary>
背景：由于人工智能（AI）和机器学习（ML）在软件应用开发中的广泛应用，公司具有深入理解这些技术的员工困难招聘。在这种情况下，AutoML 出现为填充 AI/ML 技能差距的有望解决方案，因为它承诺自动化建立结束到结束的 AI/ML 管道，该管道通常由专门的团队成员工程。目标：尽管有增加的兴趣和高期望，但有关 AutoML 现在由团队开发 AI/ML 启用系统中的采用和评估的信息缺乏。方法：为了填补这些空白，本文发表了一项混合方法研究，包括12种端到端 AutoML 工具在两个 SE 数据集上的benchmark，以及与研究人员和实践者进行后续采访，以深入了解 AutoML 的采用和评估。结果：我们发现 AutoML 解决方案可以在 SE 领域中分类任务中出perform researchers所训练和优化的模型。此外，我们的发现表明目前可用的 AutoML 解决方案并不完全支持自动化 ML 开发工作流程中的所有阶段和所有团队成员。结论：我们得出的结论可以指导 SE 研究社区如何使用 AutoML，以及工具制造者如何设计下一代 AutoML 技术。
</details></li>
</ul>
<hr>
<h2 id="Music-Genre-Classification-with-ResNet-and-Bi-GRU-Using-Visual-Spectrograms"><a href="#Music-Genre-Classification-with-ResNet-and-Bi-GRU-Using-Visual-Spectrograms" class="headerlink" title="Music Genre Classification with ResNet and Bi-GRU Using Visual Spectrograms"></a>Music Genre Classification with ResNet and Bi-GRU Using Visual Spectrograms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10773">http://arxiv.org/abs/2307.10773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junfei Zhang</li>
<li>for: 提高音乐推荐系统的精度和用户体验，增强音乐流媒体服务的用户满意度。</li>
<li>methods: 提出了一种新的方法，使用视觉spectrogram作为输入，并结合了异常神经网络（ResNet）和门控回归单元（GRU）的hybrid模型，以更好地捕捉音乐数据的复杂性。</li>
<li>results: 通过对数据进行实验，研究人员发现该模型可以更好地捕捉音乐数据的特征，并且可以提高音乐推荐系统的精度和用户体验。<details>
<summary>Abstract</summary>
Music recommendation systems have emerged as a vital component to enhance user experience and satisfaction for the music streaming services, which dominates music consumption. The key challenge in improving these recommender systems lies in comprehending the complexity of music data, specifically for the underpinning music genre classification. The limitations of manual genre classification have highlighted the need for a more advanced system, namely the Automatic Music Genre Classification (AMGC) system. While traditional machine learning techniques have shown potential in genre classification, they heavily rely on manually engineered features and feature selection, failing to capture the full complexity of music data. On the other hand, deep learning classification architectures like the traditional Convolutional Neural Networks (CNN) are effective in capturing the spatial hierarchies but struggle to capture the temporal dynamics inherent in music data. To address these challenges, this study proposes a novel approach using visual spectrograms as input, and propose a hybrid model that combines the strength of the Residual neural Network (ResNet) and the Gated Recurrent Unit (GRU). This model is designed to provide a more comprehensive analysis of music data, offering the potential to improve the music recommender systems through achieving a more comprehensive analysis of music data and hence potentially more accurate genre classification.
</details>
<details>
<summary>摘要</summary>
音乐推荐系统已经成为现代音乐流媒体服务的关键组件，提高用户体验和满意度。然而，现有的音乐推荐系统受到了音乐数据的复杂性的限制，尤其是音乐类型分类的问题。传统的机器学习技术在音乐分类方面表现了潜力，但是它们需要手动设计特征和特征选择，无法捕捉音乐数据的全面性。相反，深度学习分类架构如传统的卷积神经网络（CNN）可以 capture音乐数据中的空间层次结构，但是它们在音乐数据中的时间动态特征上表现不佳。为了解决这些挑战，本研究提出了一种新的方法，使用视觉spectrogram作为输入，并提出了一种hybrid模型，结合了Residual神经网络（ResNet）和Gated Recurrent Unit（GRU）。这种模型可以为音乐数据提供更全面的分析，并且有 potential to improve音乐推荐系统的精度。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Emotions-from-EEG-A-GRU-Based-Approach"><a href="#Unveiling-Emotions-from-EEG-A-GRU-Based-Approach" class="headerlink" title="Unveiling Emotions from EEG: A GRU-Based Approach"></a>Unveiling Emotions from EEG: A GRU-Based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02778">http://arxiv.org/abs/2308.02778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarthak Johari, Gowri Namratha Meedinti, Radhakrishnan Delhibabu, Deepak Joshi</li>
<li>for: 这个研究旨在使用 EEG 数据进行情感识别，以测量人们在不同情感情况下的脑波活动。</li>
<li>methods: 我们使用 Gated Recurrent Unit (GRU) 算法，它是一种 Recurrent Neural Networks (RNNs)，将 EEG 数据进行预测。我们使用 artifact removal、bandpass filters 和 normalization 方法进行数据预处理。</li>
<li>results: 我们的模型在验证集上达到了 100% 的准确率，并且使用 GRU 的时间相依性能力实现了出色的结果。相比其他机器学习技术，我们的 GRU 模型的 Extreme Gradient Boosting Classifier 有最高的准确率。我们的调查显示了模型的混淆矩阵中的有用信息，帮助精确地分类情感。这个研究显示了深度学习模型如 GRU 在情感识别中的潜力，并且开启了新的可能性 для与电脑互动，以及理解脑波活动中的情感表达。<details>
<summary>Abstract</summary>
One of the most important study areas in affective computing is emotion identification using EEG data. In this study, the Gated Recurrent Unit (GRU) algorithm, which is a type of Recurrent Neural Networks (RNNs), is tested to see if it can use EEG signals to predict emotional states. Our publicly accessible dataset consists of resting neutral data as well as EEG recordings from people who were exposed to stimuli evoking happy, neutral, and negative emotions. For the best feature extraction, we pre-process the EEG data using artifact removal, bandpass filters, and normalization methods. With 100% accuracy on the validation set, our model produced outstanding results by utilizing the GRU's capacity to capture temporal dependencies. When compared to other machine learning techniques, our GRU model's Extreme Gradient Boosting Classifier had the highest accuracy. Our investigation of the confusion matrix revealed insightful information about the performance of the model, enabling precise emotion classification. This study emphasizes the potential of deep learning models like GRUs for emotion recognition and advances in affective computing. Our findings open up new possibilities for interacting with computers and comprehending how emotions are expressed through brainwave activity.
</details>
<details>
<summary>摘要</summary>
一个非常重要的研究领域之一是使用EEG数据进行情感识别，这个研究使用了Gated Recurrent Unit（GRU）算法，这是一种类型的逻辑神经网络（RNN）。我们的公共可访问数据集包括休息中中性数据以及由刺激诱发的幸福、中性和负面情感的EEG记录。为了取得最佳的特征提取，我们对EEG数据进行了遗产物除掉、频率筛选和normal化处理。我们的模型在验证集上达到100%的准确率，并且使用GRU捕捉时间相互关系的能力提供了杰出的结果。与其他机器学习技术相比，我们的GRU模型的Extreme Gradient Boosting Classifier（EGBC）准确率最高。我们对模型的冲击矩阵进行了调查，并发现了模型的性能信息，帮助精确地分类情感。这个研究强调了深度学习模型如GRU的情感识别潜力，并且对情绪计算的进步产生了新的可能性。我们的发现开发了与计算机交互和理解脑波活动表达的情感的新途径。
</details></li>
</ul>
<hr>
<h2 id="Decoding-the-Enigma-Benchmarking-Humans-and-AIs-on-the-Many-Facets-of-Working-Memory"><a href="#Decoding-the-Enigma-Benchmarking-Humans-and-AIs-on-the-Many-Facets-of-Working-Memory" class="headerlink" title="Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory"></a>Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10768">http://arxiv.org/abs/2307.10768</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhanglab-deepneurocoglab/worm">https://github.com/zhanglab-deepneurocoglab/worm</a></li>
<li>paper_authors: Ankur Sikarwar, Mengmi Zhang</li>
<li>for: 这个论文的目的是开发一个robust的工作记忆（WM）benchmark数据集，以便为AI WM模型的开发和评估提供一个标准化的框架。</li>
<li>methods: 这个研究使用了10个任务和100万个试验，评估了4种功能、3种领域和11种行为和神经特征的工作记忆。研究还使用了现有的循环神经网络和变换器来jointly训练和测试这些任务。</li>
<li>results: 研究发现AI模型可以模拟人类工作记忆中的一些特征，如 primacy 和 recency 效应，以及各个领域和功能的神经团和相关特征。然而，研究还发现现有的模型存在一些限制，无法完全 aproximate 人类行为。这个数据集可以作为跨学科的资源，用于比较和改进WM模型，调查WM的神经基础，以及开发人类样式的WM模型。<details>
<summary>Abstract</summary>
Working memory (WM), a fundamental cognitive process facilitating the temporary storage, integration, manipulation, and retrieval of information, plays a vital role in reasoning and decision-making tasks. Robust benchmark datasets that capture the multifaceted nature of WM are crucial for the effective development and evaluation of AI WM models. Here, we introduce a comprehensive Working Memory (WorM) benchmark dataset for this purpose. WorM comprises 10 tasks and a total of 1 million trials, assessing 4 functionalities, 3 domains, and 11 behavioral and neural characteristics of WM. We jointly trained and tested state-of-the-art recurrent neural networks and transformers on all these tasks. We also include human behavioral benchmarks as an upper bound for comparison. Our results suggest that AI models replicate some characteristics of WM in the brain, most notably primacy and recency effects, and neural clusters and correlates specialized for different domains and functionalities of WM. In the experiments, we also reveal some limitations in existing models to approximate human behavior. This dataset serves as a valuable resource for communities in cognitive psychology, neuroscience, and AI, offering a standardized framework to compare and enhance WM models, investigate WM's neural underpinnings, and develop WM models with human-like capabilities. Our source code and data are available at https://github.com/ZhangLab-DeepNeuroCogLab/WorM.
</details>
<details>
<summary>摘要</summary>
工作记忆（WM），一种基本的认知过程，为临时存储、集成、操作和检索信息提供了重要的支持。在理解和决策任务中，WM扮演着关键的角色。为了开发和评估人工智能WM模型，我们需要一些稳定的 Referenzdatenbank。这里，我们介绍了一个完整的Working Memory（WorM） Referenzdatenbank。WorM包含10个任务和总共100万个评估，评估了WM的4种功能、3种领域和11种行为和神经特征。我们将现有的回归神经网络和转换器在所有这些任务上进行了同时训练和测试。我们还包括了人类行为标准为参照。我们的结果表明，人工智能模型在脑中的WM特征上呈现了一些相似性，特别是首先和末则效果，以及各个领域和功能特征的神经团和相关特征。在实验中，我们还发现了现有模型的一些限制，无法完全模拟人类行为。这个数据集serve as a valuable resource for cognitive psychology, neuroscience and AI communities，提供了一个标准化的框架，用于比较和提高WM模型，调查WM的神经基础，并开发人类样的WM模型。我们的源代码和数据可以在https://github.com/ZhangLab-DeepNeuroCogLab/WorM中获取。
</details></li>
</ul>
<hr>
<h2 id="Actor-agnostic-Multi-label-Action-Recognition-with-Multi-modal-Query"><a href="#Actor-agnostic-Multi-label-Action-Recognition-with-Multi-modal-Query" class="headerlink" title="Actor-agnostic Multi-label Action Recognition with Multi-modal Query"></a>Actor-agnostic Multi-label Action Recognition with Multi-modal Query</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10763">http://arxiv.org/abs/2307.10763</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mondalanindya/msqnet">https://github.com/mondalanindya/msqnet</a></li>
<li>paper_authors: Anindya Mondal, Sauradip Nag, Joaquin M Prada, Xiatian Zhu, Anjan Dutta</li>
<li>for: 提高多种演员（包括人类和动物）动作识别精度和 universality。</li>
<li>methods: 提出了一种新的多模态多标签动作识别方法，利用视觉和文本模式来更好地表示动作类别，并消除了actor-specific模型设计，从而解决了actor pose estimation问题。</li>
<li>results: 在五个公开的 benchmark 上进行了广泛的实验，结果表明，我们的 MSQNet 在人类和动物单标和多标动作识别任务上比PRIOR ARTS actor-specific alternative 高出50%。<details>
<summary>Abstract</summary>
Existing action recognition methods are typically actor-specific due to the intrinsic topological and apparent differences among the actors. This requires actor-specific pose estimation (e.g., humans vs. animals), leading to cumbersome model design complexity and high maintenance costs. Moreover, they often focus on learning the visual modality alone and single-label classification whilst neglecting other available information sources (e.g., class name text) and the concurrent occurrence of multiple actions. To overcome these limitations, we propose a new approach called 'actor-agnostic multi-modal multi-label action recognition,' which offers a unified solution for various types of actors, including humans and animals. We further formulate a novel Multi-modal Semantic Query Network (MSQNet) model in a transformer-based object detection framework (e.g., DETR), characterized by leveraging visual and textual modalities to represent the action classes better. The elimination of actor-specific model designs is a key advantage, as it removes the need for actor pose estimation altogether. Extensive experiments on five publicly available benchmarks show that our MSQNet consistently outperforms the prior arts of actor-specific alternatives on human and animal single- and multi-label action recognition tasks by up to 50%. Code will be released at https://github.com/mondalanindya/MSQNet.
</details>
<details>
<summary>摘要</summary>
现有的动作认识方法通常是actor-specific的，这是因为actor之间存在内在的拓扑和外在的差异。这会导致actor-specific的姿势估计（例如人VS动物），从而增加模型设计复杂度和维护成本。另外，它们通常会专注于学习视觉模式alone和单个标签分类，而忽略其他可用的信息源（例如类名文本）以及同时发生的多个动作。为了解决这些限制，我们提出了一种新的方法called 'actor-agnostic multi-modal multi-label action recognition，' which offers a unified solution for various types of actors, including humans and animals.我们进一步提出了一种novel Multi-modal Semantic Query Network (MSQNet)模型，这个模型在基于 transformer 的对象检测框架（例如 DETR）中，利用视觉和文本模式来更好地表示动作类。消除actor-specific模型设计是MSQNet的关键优势，这将取消actor pose estimation的需求。我们的实验结果表明，MSQNet在五个公共的 benchmark 上 consistently outperformsactor-specific alternatives on human and animal single- and multi-label action recognition tasks by up to 50%. Code will be released at https://github.com/mondalanindya/MSQNet.
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Voter-Attribute-Bias-for-Fair-Opinion-Aggregation"><a href="#Mitigating-Voter-Attribute-Bias-for-Fair-Opinion-Aggregation" class="headerlink" title="Mitigating Voter Attribute Bias for Fair Opinion Aggregation"></a>Mitigating Voter Attribute Bias for Fair Opinion Aggregation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10749">http://arxiv.org/abs/2307.10749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryosuke Ueda, Koh Takeuchi, Hisashi Kashima<br>for:This paper focuses on achieving fair opinion aggregation in decision-making tasks, such as hiring and loan review, where disagreements may occur and voter attributes like gender or race may introduce bias.methods:The paper proposes a combination of opinion aggregation models like majority voting and the Dawid and Skene model (D&amp;S model) with fairness options like sample weighting to achieve fair aggregation results. The authors also use probabilistic soft labels to evaluate the fairness of opinion aggregation.results:The experimental results show that the combination of Soft D&amp;S and data splitting as a fairness option is effective for dense data, while weighted majority voting is effective for sparse data. These findings can support decision-making by human and machine-learning models with balanced opinion aggregation.<details>
<summary>Abstract</summary>
The aggregation of multiple opinions plays a crucial role in decision-making, such as in hiring and loan review, and in labeling data for supervised learning. Although majority voting and existing opinion aggregation models are effective for simple tasks, they are inappropriate for tasks without objectively true labels in which disagreements may occur. In particular, when voter attributes such as gender or race introduce bias into opinions, the aggregation results may vary depending on the composition of voter attributes. A balanced group of voters is desirable for fair aggregation results but may be difficult to prepare. In this study, we consider methods to achieve fair opinion aggregation based on voter attributes and evaluate the fairness of the aggregated results. To this end, we consider an approach that combines opinion aggregation models such as majority voting and the Dawid and Skene model (D&S model) with fairness options such as sample weighting. To evaluate the fairness of opinion aggregation, probabilistic soft labels are preferred over discrete class labels. First, we address the problem of soft label estimation without considering voter attributes and identify some issues with the D&S model. To address these limitations, we propose a new Soft D&S model with improved accuracy in estimating soft labels. Moreover, we evaluated the fairness of an opinion aggregation model, including Soft D&S, in combination with different fairness options using synthetic and semi-synthetic data. The experimental results suggest that the combination of Soft D&S and data splitting as a fairness option is effective for dense data, whereas weighted majority voting is effective for sparse data. These findings should prove particularly valuable in supporting decision-making by human and machine-learning models with balanced opinion aggregation.
</details>
<details>
<summary>摘要</summary>
多种意见的总和在决策中发挥关键作用，如在招聘和贷款审核中，以及在supervised learning中标注数据。虽然多数投票和现有的意见总和模型对简单任务有效，但对无 objetively true标签的任务而言，它们不适用。特别是当选民特征如性别或种族引入偏见到意见中，总和结果可能因选民特征组合而异常。一个均衡的选民组合是 désirable для公平的总和结果，但可能困难实现。在这种研究中，我们考虑了基于选民特征的公平意见总和方法，并评估了总和结果的公平性。为此，我们考虑了意见总和模型如多数投票和达韦德和斯金（D&S）模型，并与公平选项如样本权重相结合。为评估意见总和的公平性，我们偏好使用概率软标签而不是精确的分类标签。首先，我们解决了不考虑选民特征时的软标签估计问题，并指出了D&S模型的一些限制。为解决这些限制，我们提出了一个新的软D&S模型，具有提高软标签估计精度的优点。此外，我们通过使用 sintética y semi-sintética数据进行了对意见总和模型、包括软D&S模型，以及不同公平选项的评估。实验结果表明，将软D&S模型与数据分割作为公平选项相结合，对于稠密数据是有效的，而Weighted Majority Voting对于稀疏数据是有效的。这些发现应该对人类和机器学习模型的均衡意见总和做出重要贡献。
</details></li>
</ul>
<hr>
<h2 id="Fairness-Aware-Client-Selection-for-Federated-Learning"><a href="#Fairness-Aware-Client-Selection-for-Federated-Learning" class="headerlink" title="Fairness-Aware Client Selection for Federated Learning"></a>Fairness-Aware Client Selection for Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10738">http://arxiv.org/abs/2307.10738</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxin Shi, Zelei Liu, Zhuan Shi, Han Yu</li>
<li>for: 这篇论文的目的是提出了一个名为“ Fairness-aware Federated Client Selection”的方法，用于在多个数据所有者（FL客户）合作训练机器学习模型时，对 FL 客户进行选择，以确保客户的公平对待。</li>
<li>methods: 这篇论文使用了 Lyapunov 优化方法，将 FL 客户的选择概率静态地调整，基于客户的声誉、参与 FL 任务的次数和对模型性能的贡献。这个方法不使用阈值基于声誉的筛选，因此为 FL 客户提供了重新证明自己的机会，进一步增强客户的公平对待。</li>
<li>results: 实验结果显示，这篇论文的 FairFedCS 方法在真实世界的多媒体数据集上，实现了19.6%的公平性和0.73%的测试准确率的提升，比最佳现有方法的平均提升率高出19.6%和0.73%。<details>
<summary>Abstract</summary>
Federated learning (FL) has enabled multiple data owners (a.k.a. FL clients) to train machine learning models collaboratively without revealing private data. Since the FL server can only engage a limited number of clients in each training round, FL client selection has become an important research problem. Existing approaches generally focus on either enhancing FL model performance or enhancing the fair treatment of FL clients. The problem of balancing performance and fairness considerations when selecting FL clients remains open. To address this problem, we propose the Fairness-aware Federated Client Selection (FairFedCS) approach. Based on Lyapunov optimization, it dynamically adjusts FL clients' selection probabilities by jointly considering their reputations, times of participation in FL tasks and contributions to the resulting model performance. By not using threshold-based reputation filtering, it provides FL clients with opportunities to redeem their reputations after a perceived poor performance, thereby further enhancing fair client treatment. Extensive experiments based on real-world multimedia datasets show that FairFedCS achieves 19.6% higher fairness and 0.73% higher test accuracy on average than the best-performing state-of-the-art approach.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Long-Tail-Theory-under-Gaussian-Mixtures"><a href="#Long-Tail-Theory-under-Gaussian-Mixtures" class="headerlink" title="Long-Tail Theory under Gaussian Mixtures"></a>Long-Tail Theory under Gaussian Mixtures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10736">http://arxiv.org/abs/2307.10736</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/armanbolatov/long_tail">https://github.com/armanbolatov/long_tail</a></li>
<li>paper_authors: Arman Bolatov, Maxat Tezekbayev, Igor Melnykov, Artur Pak, Vassilina Nikoulina, Zhenisbek Assylbekov</li>
<li>for:  validate Feldman’s long tail theory and explore the effect of nonlinear models on generalization error in long-tailed distributions</li>
<li>methods: propose a simple Gaussian mixture model and compare the performance of linear and nonlinear classifiers</li>
<li>results: demonstrate that nonlinear classifiers with memorization capacity can better handle long-tailed distributions, and the performance gap between linear and nonlinear models decreases as the tail becomes shorter.<details>
<summary>Abstract</summary>
We suggest a simple Gaussian mixture model for data generation that complies with Feldman's long tail theory (2020). We demonstrate that a linear classifier cannot decrease the generalization error below a certain level in the proposed model, whereas a nonlinear classifier with a memorization capacity can. This confirms that for long-tailed distributions, rare training examples must be considered for optimal generalization to new data. Finally, we show that the performance gap between linear and nonlinear models can be lessened as the tail becomes shorter in the subpopulation frequency distribution, as confirmed by experiments on synthetic and real data.
</details>
<details>
<summary>摘要</summary>
我们建议一个简单的 Gaussian 混合模型来生成数据，遵循 Feldman 的长尾理论（2020）。我们示出了一个线性分类器无法在我们提案的模型中降低泛化错误下限，而一个具有记忆容量的非线性分类器则可以。这证明了在长尾分布中，罕见的训练数据必须被考虑，以获得新数据的最佳泛化。最后，我们显示了在尾部变短的情况下，线性和非线性模型之间的性能差距可以降低，经过实验证明。Note: "简单" (jiǎn simple) in Simplified Chinese means "simple" in English.
</details></li>
</ul>
<hr>
<h2 id="Comparison-between-transformers-and-convolutional-models-for-fine-grained-classification-of-insects"><a href="#Comparison-between-transformers-and-convolutional-models-for-fine-grained-classification-of-insects" class="headerlink" title="Comparison between transformers and convolutional models for fine-grained classification of insects"></a>Comparison between transformers and convolutional models for fine-grained classification of insects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11112">http://arxiv.org/abs/2307.11112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rita Pucci, Vincent J. Kalkman, Dan Stowell</li>
<li>for: 本研究的目的是提高昆虫种的自动分类精度，以便更好地监测生态系统中的昆虫种类。</li>
<li>methods: 本研究使用了深度学习算法，主要是transformer和卷积层结构，对 Odonata 和 Coleoptera 两个目录进行比较研究，以确定最佳的算法。</li>
<li>results: 研究发现，混合模型在精度和执行速度两个方面都表现出色，而transformer层结构在缺乏样本时表现更加稳定，具有更快的执行速度。<details>
<summary>Abstract</summary>
Fine-grained classification is challenging due to the difficulty of finding discriminatory features. This problem is exacerbated when applied to identifying species within the same taxonomical class. This is because species are often sharing morphological characteristics that make them difficult to differentiate. We consider the taxonomical class of Insecta. The identification of insects is essential in biodiversity monitoring as they are one of the inhabitants at the base of many ecosystems. Citizen science is doing brilliant work of collecting images of insects in the wild giving the possibility to experts to create improved distribution maps in all countries. We have billions of images that need to be automatically classified and deep neural network algorithms are one of the main techniques explored for fine-grained tasks. At the SOTA, the field of deep learning algorithms is extremely fruitful, so how to identify the algorithm to use? We focus on Odonata and Coleoptera orders, and we propose an initial comparative study to analyse the two best-known layer structures for computer vision: transformer and convolutional layers. We compare the performance of T2TViT, a fully transformer-base, EfficientNet, a fully convolutional-base, and ViTAE, a hybrid. We analyse the performance of the three models in identical conditions evaluating the performance per species, per morph together with sex, the inference time, and the overall performance with unbalanced datasets of images from smartphones. Although we observe high performances with all three families of models, our analysis shows that the hybrid model outperforms the fully convolutional-base and fully transformer-base models on accuracy performance and the fully transformer-base model outperforms the others on inference speed and, these prove the transformer to be robust to the shortage of samples and to be faster at inference time.
</details>
<details>
<summary>摘要</summary>
细见分类是因为找到特征分化的困难，这个问题在种类同一类型时更加加剧。这是因为种类通常在同一类型中共享形态特征，使其很难分辨。我们考虑了昆虫纲（Insecta）的分类。 insects 是生态系统的基础居民，其分类是生态监测中非常重要的。公民科学在野外采集到了大量昆虫图像，这给专家提供了创建改进的分布图的机会。我们有 billions 的图像需要自动分类，深度学习算法是细见任务中的主要技术之一。在 SOTA 中，深度学习领域非常肥沃，因此如何选择算法？我们关注 Odonata 和 Coleoptera 两个类别，并提出了一项初步比较研究，以分析两种最为知名的计算机视觉层结构：变换层和卷积层。我们比较了 T2TViT、EfficientNet 和 ViTAE 三种模型的性能，并分析了每种模型在同一种条件下的性能，包括每种物种、每个形态、性别、推理时间和总性能。虽然我们所观察到的性能都很高，但我们的分析表明，混合模型在精度性能和推理速度方面都高于完全卷积基础和完全变换基础模型，而变换模型在精度性能方面表现出色。
</details></li>
</ul>
<hr>
<h2 id="LLM-Censorship-A-Machine-Learning-Challenge-or-a-Computer-Security-Problem"><a href="#LLM-Censorship-A-Machine-Learning-Challenge-or-a-Computer-Security-Problem" class="headerlink" title="LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?"></a>LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10719">http://arxiv.org/abs/2307.10719</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Glukhov, Ilia Shumailov, Yarin Gal, Nicolas Papernot, Vardan Papyan</li>
<li>for: 防止大语言模型（LLM）的黑客用途</li>
<li>methods: 检测LLM输出中不良内容的方法</li>
<li>results: 检测方法存在限制和攻击者可以重构禁止的输出<details>
<summary>Abstract</summary>
Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, as LLMs can still generate problematic responses. Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs. In this paper, we present the theoretical limitations of such semantic censorship approaches. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities. Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones. As a result, we propose that the problem of censorship needs to be reevaluated; it should be treated as a security problem which warrants the adaptation of security-based approaches to mitigate potential risks.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经展现出印象深刻的能力，但它们的盲目遵循提供的指令却引起了关于恶意使用的风险。现有的防御机制，如模型精致化或使用 LLM 进行出力审查，已经证明是不可靠的，因为 LLM 仍然可以产生问题的回应。常用的审查方法将这问题视为机器学习的问题，并且靠另一个 LM 检测恶意内容在 LLM 的出力中。在这篇论文中，我们展示了对于这种Semantic审查的理论限制。具体来说，我们显示了Semantic审查是一个不可解决的问题，highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities。此外，我们认为这些挑战不仅限于Semantic审查，因为知识分子可以从 permissible 的出力中重建不被允许的出力。因此，我们建议将问题重新评估为安全问题，并且适用安全性基础的方法来减轻潜在的风险。
</details></li>
</ul>
<hr>
<h2 id="Differences-Between-Hard-and-Noisy-labeled-Samples-An-Empirical-Study"><a href="#Differences-Between-Hard-and-Noisy-labeled-Samples-An-Empirical-Study" class="headerlink" title="Differences Between Hard and Noisy-labeled Samples: An Empirical Study"></a>Differences Between Hard and Noisy-labeled Samples: An Empirical Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10718">http://arxiv.org/abs/2307.10718</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mahf93/hard-vs-noisy">https://github.com/mahf93/hard-vs-noisy</a></li>
<li>paper_authors: Mahsa Forouzesh, Patrick Thiran</li>
<li>for: 本研究旨在解决难以学习的样本和涂改标签的问题，这两者在现有的方法中通常是独立地处理的，但是当两者同时存在时，大多数现有方法会导致模型的总性能下降。</li>
<li>methods: 我们首先设计了多种Synthetic Dataset，以控制样本的困难和涂改标签水平。我们的系统性的实验研究帮助我们更好地理解难以学习样本和涂改标签之间的相似性和差异。这些控制的实验为开发能够分辨难以学习和涂改标签的方法提供了基础。</li>
<li>results: 我们提出了一个简单 yet effective的度量，可以过滤掉涂改标签的样本，保留难以学习的样本。我们研究了多种数据分割方法在实际涂改标签的情况下的性能，并发现我们的提议的数据分割方法在 semi-supervised learning 框架中表现出色。<details>
<summary>Abstract</summary>
Extracting noisy or incorrectly labeled samples from a labeled dataset with hard/difficult samples is an important yet under-explored topic. Two general and often independent lines of work exist, one focuses on addressing noisy labels, and another deals with hard samples. However, when both types of data are present, most existing methods treat them equally, which results in a decline in the overall performance of the model. In this paper, we first design various synthetic datasets with custom hardness and noisiness levels for different samples. Our proposed systematic empirical study enables us to better understand the similarities and more importantly the differences between hard-to-learn samples and incorrectly-labeled samples. These controlled experiments pave the way for the development of methods that distinguish between hard and noisy samples. Through our study, we introduce a simple yet effective metric that filters out noisy-labeled samples while keeping the hard samples. We study various data partitioning methods in the presence of label noise and observe that filtering out noisy samples from hard samples with this proposed metric results in the best datasets as evidenced by the high test accuracy achieved after models are trained on the filtered datasets. We demonstrate this for both our created synthetic datasets and for datasets with real-world label noise. Furthermore, our proposed data partitioning method significantly outperforms other methods when employed within a semi-supervised learning framework.
</details>
<details>
<summary>摘要</summary>
<<SYS>> transtable id="simplified-chinese" /</SYS>>原文：抽取含有噪音或错误标签的样本从标注数据集中是一个重要又受到忽视的话题。现有两种常见且独立的方法来处理这些样本，一种是处理噪音标签，另一种是处理困难样本。然而，当这两种样本同时存在时，大多数现有方法对它们进行平等待遇，这会导致模型的总性能下降。在这篇论文中，我们首先设计了不同困难和噪音水平的synthetic数据集。我们的提议的系统性的实验研究可以更好地了解困难样本和错误标签样本之间的相似性和区别。这些控制的实验为开发可以分辨困难和噪音样本的方法提供了基础。我们介绍了一个简单 yet effective的度量，可以从噪音标签样本中筛选出噪音样本，保留困难样本。我们研究了在噪音标签存在的情况下不同的数据分割方法，并发现使用我们提议的度量筛选困难样本后的 filtered datasets 可以获得最高的测试精度。我们在 synthetic 数据集和实际世界中的噪音标签数据集上进行了实验，并证明了我们的方法的优势。此外，我们的提议的数据分割方法在 semi-supervised 学习框架中表现了出色。
</details></li>
</ul>
<hr>
<h2 id="AdjointDPM-Adjoint-Sensitivity-Method-for-Gradient-Backpropagation-of-Diffusion-Probabilistic-Models"><a href="#AdjointDPM-Adjoint-Sensitivity-Method-for-Gradient-Backpropagation-of-Diffusion-Probabilistic-Models" class="headerlink" title="AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models"></a>AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10711">http://arxiv.org/abs/2307.10711</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiachun Pan, Jun Hao Liew, Vincent Y. F. Tan, Jiashi Feng, Hanshu Yan</li>
<li>For: This paper aims to address the challenge of customizing diffusion probabilistic models (DPMs) when the only available supervision is a differentiable metric defined on the generated contents.* Methods: The proposed method, AdjointDPM, first generates new samples from diffusion models by solving the corresponding probability-flow ODEs. It then uses the adjoint sensitivity method to backpropagate the gradients of the loss to the models’ parameters (including conditioning signals, network weights, and initial noises) by solving another augmented ODE.* Results: The proposed method is demonstrated to be effective on three interesting tasks: converting visual effects into identification text embeddings, finetuning DPMs for specific types of stylization, and optimizing initial noise to generate adversarial samples for security auditing.<details>
<summary>Abstract</summary>
Existing customization methods require access to multiple reference examples to align pre-trained diffusion probabilistic models (DPMs) with user-provided concepts. This paper aims to address the challenge of DPM customization when the only available supervision is a differentiable metric defined on the generated contents. Since the sampling procedure of DPMs involves recursive calls to the denoising UNet, na\"ive gradient backpropagation requires storing the intermediate states of all iterations, resulting in extremely high memory consumption. To overcome this issue, we propose a novel method AdjointDPM, which first generates new samples from diffusion models by solving the corresponding probability-flow ODEs. It then uses the adjoint sensitivity method to backpropagate the gradients of the loss to the models' parameters (including conditioning signals, network weights, and initial noises) by solving another augmented ODE. To reduce numerical errors in both the forward generation and gradient backpropagation processes, we further reparameterize the probability-flow ODE and augmented ODE as simple non-stiff ODEs using exponential integration. Finally, we demonstrate the effectiveness of AdjointDPM on three interesting tasks: converting visual effects into identification text embeddings, finetuning DPMs for specific types of stylization, and optimizing initial noise to generate adversarial samples for security auditing.
</details>
<details>
<summary>摘要</summary>
现有的自定义方法需要访问多个参考示例，以对用户提供的概念与预训练的扩散概率模型（DPM）进行对接。本文目的是解决DPM自定义时，只有用户提供的可微分度量的概念来支持的挑战。由于扩散模型的采样过程包括 recursive calls to denoising UNet，直观梯度反推需要存储所有迭代的状态，从而导致极高的内存消耗。为解决这个问题，我们提出了一种新的方法 called AdjointDPM。 AdjointDPM首先通过解决相应的概率流ODE来生成新的样本。然后，它使用梯度敏感法来反推模型参数（包括条件信号、网络参数和初始噪声）的梯度。为了降低数值计算中的numerical errors，我们进一步折衔概率流ODE和扩展ODE为简单的非稍难ODE使用快速积分。最后，我们证明了AdjointDPM在三个有趣的任务中的效果：将视觉特效转换为标识符文本嵌入，Finetune DPMs для特定类型的风格化，以及优化初始噪声来生成安全审核中的对抗样本。
</details></li>
</ul>
<hr>
<h2 id="Reparameterized-Policy-Learning-for-Multimodal-Trajectory-Optimization"><a href="#Reparameterized-Policy-Learning-for-Multimodal-Trajectory-Optimization" class="headerlink" title="Reparameterized Policy Learning for Multimodal Trajectory Optimization"></a>Reparameterized Policy Learning for Multimodal Trajectory Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10710">http://arxiv.org/abs/2307.10710</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haosulab/RPG">https://github.com/haosulab/RPG</a></li>
<li>paper_authors: Zhiao Huang, Litian Liang, Zhan Ling, Xuanlin Li, Chuang Gan, Hao Su</li>
<li>for: 本文目的是解决RL中高维动作空间中参数策略的挑战。</li>
<li>methods: 本文提出了一种基于生成模型的策略参数化方法，通过conditioningPolicy来Derive一种新的可变 bounds，以便优化搜索环境。</li>
<li>results: 实验结果表明，我们的方法可以帮助机器人避免环境中的局部优化，并在 tasks with dense rewards 和 sparse-reward environments 中解决挑战。我们的方法比前一些方法表现更好，可以减少数据的使用。代码和补充材料可以在项目页面<a target="_blank" rel="noopener" href="https://haosulab.github.io/RPG/">https://haosulab.github.io/RPG/</a> 中找到。<details>
<summary>Abstract</summary>
We investigate the challenge of parametrizing policies for reinforcement learning (RL) in high-dimensional continuous action spaces. Our objective is to develop a multimodal policy that overcomes limitations inherent in the commonly-used Gaussian parameterization. To achieve this, we propose a principled framework that models the continuous RL policy as a generative model of optimal trajectories. By conditioning the policy on a latent variable, we derive a novel variational bound as the optimization objective, which promotes exploration of the environment. We then present a practical model-based RL method, called Reparameterized Policy Gradient (RPG), which leverages the multimodal policy parameterization and learned world model to achieve strong exploration capabilities and high data efficiency. Empirical results demonstrate that our method can help agents evade local optima in tasks with dense rewards and solve challenging sparse-reward environments by incorporating an object-centric intrinsic reward. Our method consistently outperforms previous approaches across a range of tasks. Code and supplementary materials are available on the project page https://haosulab.github.io/RPG/
</details>
<details>
<summary>摘要</summary>
我团队正在研究重点参数化策略的挑战，以便在高维连续动作空间中进行学习反馈。我们的目标是开发一种多模态策略，以超越通常使用的 Gaussian 参数化的限制。为此，我们提议一种原理性的框架，即将连续RL策略视为优质轨迹的生成模型。通过conditioning策略于隐藏变量，我们得到了一种新的可变约束，该约束促进环境的探索。然后，我们提出了一种实用的基于模型的RL方法，即Reparameterized Policy Gradient（RPG），该方法利用多模态策略参数化和学习到的世界模型，以实现强大的探索能力和高数据效率。我们的方法在多种任务中均能够超越先前的方法，并且在 dense 奖励和稀缺奖励环境中解决了本地极点问题。我们的方法可以在多种任务中均能够具有高效率和优秀的探索能力。详细的代码和补充材料可以在项目页面（https://haosulab.github.io/RPG/）上找到。
</details></li>
</ul>
<hr>
<h2 id="TwinLiteNet-An-Efficient-and-Lightweight-Model-for-Driveable-Area-and-Lane-Segmentation-in-Self-Driving-Cars"><a href="#TwinLiteNet-An-Efficient-and-Lightweight-Model-for-Driveable-Area-and-Lane-Segmentation-in-Self-Driving-Cars" class="headerlink" title="TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and Lane Segmentation in Self-Driving Cars"></a>TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and Lane Segmentation in Self-Driving Cars</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10705">http://arxiv.org/abs/2307.10705</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chequanghuy/TwinLiteNet">https://github.com/chequanghuy/TwinLiteNet</a></li>
<li>paper_authors: Quang Huy Che, Dinh Phuc Nguyen, Minh Quan Pham, Duc Khai Lam</li>
<li>for: 这篇论文主要关注于自动驾驶车辆中的内部环境理解，特别是driveable area和lane line分类。</li>
<li>methods: 这篇论文提出了一个轻量级的模型，用于实时验证driveable area和lane line。模型称为TwinLiteNet，它仅需0.4亿个参数，并可以在GPU RTX A5000上实现415 FPS的运算速度。</li>
<li>results: 根据论文的实验结果，TwinLiteNet在BDD100K数据集上实现了91.3%的mIoU分类率 для driveable area任务，并且在Lane Detection任务上实现了31.08%的IoU分类率。此外，TwinLiteNet可以在内置式计算机上实现60 FPS的实时运算速度，特别是在Jetson Xavier NX上实现415 FPS，这使得它成为了自动驾驶车辆中的理想解决方案。<details>
<summary>Abstract</summary>
Semantic segmentation is a common task in autonomous driving to understand the surrounding environment. Driveable Area Segmentation and Lane Detection are particularly important for safe and efficient navigation on the road. However, original semantic segmentation models are computationally expensive and require high-end hardware, which is not feasible for embedded systems in autonomous vehicles. This paper proposes a lightweight model for the driveable area and lane line segmentation. TwinLiteNet is designed cheaply but achieves accurate and efficient segmentation results. We evaluate TwinLiteNet on the BDD100K dataset and compare it with modern models. Experimental results show that our TwinLiteNet performs similarly to existing approaches, requiring significantly fewer computational resources. Specifically, TwinLiteNet achieves a mIoU score of 91.3% for the Drivable Area task and 31.08% IoU for the Lane Detection task with only 0.4 million parameters and achieves 415 FPS on GPU RTX A5000. Furthermore, TwinLiteNet can run in real-time on embedded devices with limited computing power, especially since it achieves 60FPS on Jetson Xavier NX, making it an ideal solution for self-driving vehicles. Code is available: url{https://github.com/chequanghuy/TwinLiteNet}.
</details>
<details>
<summary>摘要</summary>
Semantic segmentation 是自动驾驶中常见的任务，用于理解周围环境。驱动区域分割和车道检测是安全和效率导航的关键，但原始 semantic segmentation 模型 computationally 昂贵，需要高端硬件，不太适合自动驾驶车辆中的嵌入式系统。这篇论文提出了一种轻量级模型，用于驱动区域和车道分割。TwinLiteNet 是一种低成本的设计，却可以实现高精度和高效的分割结果。我们在 BDD100K 数据集上评估 TwinLiteNet，并与现代模型进行比较。实验结果表明，我们的 TwinLiteNet 与现有方法相似，却需要 significatively  fewer 计算资源。具体来说，TwinLiteNet 在 Drivable Area 任务上取得了 91.3% 的 mIoU 分数，在 Lane Detection 任务上取得了 31.08% 的 IoU 分数，只需 0.4 万个参数，并在 GPU RTX A5000 上实现了 415 FPS。此外，TwinLiteNet 可以在具有有限计算能力的嵌入式设备上运行，尤其是在 Jetson Xavier NX 上实现了 60 FPS，使其成为自动驾驶车辆的理想解决方案。代码可以在以下链接中找到：<https://github.com/chequanghuy/TwinLiteNet>。
</details></li>
</ul>
<hr>
<h2 id="Decentralized-Smart-Charging-of-Large-Scale-EVs-using-Adaptive-Multi-Agent-Multi-Armed-Bandits"><a href="#Decentralized-Smart-Charging-of-Large-Scale-EVs-using-Adaptive-Multi-Agent-Multi-Armed-Bandits" class="headerlink" title="Decentralized Smart Charging of Large-Scale EVs using Adaptive Multi-Agent Multi-Armed Bandits"></a>Decentralized Smart Charging of Large-Scale EVs using Adaptive Multi-Agent Multi-Armed Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10704">http://arxiv.org/abs/2307.10704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sharyal Zafar, Raphaël Feraud, Anne Blavette, Guy Camilleri, Hamid Ben</li>
<li>for: 提高电动车和太阳能系统的灵活性和可扩展性，并处理峰值负荷和电压限制问题</li>
<li>methods: 使用自适应多代理系统理论，利用多臂炮学习处理系统不确定性</li>
<li>results: 提出了一种基于分布式智能充电系统，具有扩展性、实时性、无模型、均衡多个参与者的特点，并进行了详细的案例研究以评估性能<details>
<summary>Abstract</summary>
The drastic growth of electric vehicles and photovoltaics can introduce new challenges, such as electrical current congestion and voltage limit violations due to peak load demands. These issues can be mitigated by controlling the operation of electric vehicles i.e., smart charging. Centralized smart charging solutions have already been proposed in the literature. But such solutions may lack scalability and suffer from inherent drawbacks of centralization, such as a single point of failure, and data privacy concerns. Decentralization can help tackle these challenges. In this paper, a fully decentralized smart charging system is proposed using the philosophy of adaptive multi-agent systems. The proposed system utilizes multi-armed bandit learning to handle uncertainties in the system. The presented system is decentralized, scalable, real-time, model-free, and takes fairness among different players into account. A detailed case study is also presented for performance evaluation.
</details>
<details>
<summary>摘要</summary>
“电动车和太阳能系统的急速增长可能会带来新的挑战，例如电力流量拥堵和电压限制违规因为峰值负载。这些问题可以通过控制电动车的运作来缓解，例如聪明充电。中央化聪明充电解决方案已经在文献中提出，但这些解决方案可能缺乏扩展性和中央化的缺点，例如单一点故障和数据隐私问题。分散化可以帮助解决这些挑战。本文提出了一个完全分散式聪明充电系统，使用了适应多智能体系统的哲学。提案的系统利用多臂帮手学来处理系统中的不确定性。提出的系统是分散式、扩展性、实时、无模型、以及对不同玩家的公平性into account。另外，一个详细的实际案例也是提供了性能评估。”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Graphs-in-State-Space-Models-for-Granger-Causality-in-Climate-Science"><a href="#Graphs-in-State-Space-Models-for-Granger-Causality-in-Climate-Science" class="headerlink" title="Graphs in State-Space Models for Granger Causality in Climate Science"></a>Graphs in State-Space Models for Granger Causality in Climate Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10703">http://arxiv.org/abs/2307.10703</a></li>
<li>repo_url: None</li>
<li>paper_authors: Víctor Elvira, Émilie Chouzenoux, Jordi Cerdà, Gustau Camps-Valls</li>
<li>for: 本研究探讨了Granger causality（GC）是否真实表示 causality，同时提出了一种基于图模型的GC测试方法。</li>
<li>methods: 本研究使用了GraphEM算法，一种基于期望最大化的方法，来估计线性 Gaussian 状态方程中的线性矩阵运算员。具有lasso regularization的M-step使用了一种 proximal splitting Douglas-Rachford 算法来解决。</li>
<li>results: 对于具体的示例和挑战性气候问题，提出的模型和推断方法在标准GC方法上表现出了优势。<details>
<summary>Abstract</summary>
Granger causality (GC) is often considered not an actual form of causality. Still, it is arguably the most widely used method to assess the predictability of a time series from another one. Granger causality has been widely used in many applied disciplines, from neuroscience and econometrics to Earth sciences. We revisit GC under a graphical perspective of state-space models. For that, we use GraphEM, a recently presented expectation-maximisation algorithm for estimating the linear matrix operator in the state equation of a linear-Gaussian state-space model. Lasso regularisation is included in the M-step, which is solved using a proximal splitting Douglas-Rachford algorithm. Experiments in toy examples and challenging climate problems illustrate the benefits of the proposed model and inference technique over standard Granger causality methods.
</details>
<details>
<summary>摘要</summary>
格兰治 causality（GC）经常被视为不是真正的 causality，但它仍然是最广泛使用的方法来评估时间序列之间的预测性。格兰治 causality 在许多应用领域中广泛使用，从神经科学和 econometrics 到地球科学。我们在图表视图下重新审视 GC。为此，我们使用 GraphEM，一种最近提出的预期-最大化算法来估计线性矩阵运算符在状态方程中的线性-加比分型模型中的状态方程。lasso 规范化包含在 M-步中，通过 proximal splitting Douglas-Rachford 算法解决。在做品例和挑战性气候问题中进行实验，我们发现提议的模型和推理技术在标准 Granger causality 方法的基础上具有优势。
</details></li>
</ul>
<hr>
<h2 id="Self2Self-Single-Image-Denoising-with-Self-Supervised-Learning-and-Image-Quality-Assessment-Loss"><a href="#Self2Self-Single-Image-Denoising-with-Self-Supervised-Learning-and-Image-Quality-Assessment-Loss" class="headerlink" title="Self2Self+: Single-Image Denoising with Self-Supervised Learning and Image Quality Assessment Loss"></a>Self2Self+: Single-Image Denoising with Self-Supervised Learning and Image Quality Assessment Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10695">http://arxiv.org/abs/2307.10695</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JK-the-Ko/Self2SelfPlus">https://github.com/JK-the-Ko/Self2SelfPlus</a></li>
<li>paper_authors: Jaekyun Ko, Sanghwan Lee</li>
<li>for: 用于降噪处理</li>
<li>methods: 使用单图自我监督学习方法，即使用含有噪声的输入图像进行网络训练，并使用阻塞卷积和无参照质量评估来导航训练过程。</li>
<li>results: 实验结果表明，提议的方法可以在synthetic和实际 dataset上达到降噪性能的国际先进水平，这说明了该方法的实用性和可行性。<details>
<summary>Abstract</summary>
Recently, denoising methods based on supervised learning have exhibited promising performance. However, their reliance on external datasets containing noisy-clean image pairs restricts their applicability. To address this limitation, researchers have focused on training denoising networks using solely a set of noisy inputs. To improve the feasibility of denoising procedures, in this study, we proposed a single-image self-supervised learning method in which only the noisy input image is used for network training. Gated convolution was used for feature extraction and no-reference image quality assessment was used for guiding the training process. Moreover, the proposed method sampled instances from the input image dataset using Bernoulli sampling with a certain dropout rate for training. The corresponding result was produced by averaging the generated predictions from various instances of the trained network with dropouts. The experimental results indicated that the proposed method achieved state-of-the-art denoising performance on both synthetic and real-world datasets. This highlights the effectiveness and practicality of our method as a potential solution for various noise removal tasks.
</details>
<details>
<summary>摘要</summary>
在本研究中，我们提出了基于单个噪声输入图像的自动学习方法。我们使用了阀门卷积来提取特征，并使用无参图像质量评估来引导训练过程。此外，我们使用ベルヌ利律采样将输入图像集采样到各个实例中，并在训练过程中使用某种抽样率。生成的结果是通过不同实例的训练网络生成的多个预测结果的平均值。实验结果表明，我们的方法在Synthetic和实际世界 dataset上达到了当前最佳的干扰除性能。这说明了我们的方法的有效性和实用性，可以用于解决各种噪声除掉任务。
</details></li>
</ul>
<hr>
<h2 id="Fractional-Denoising-for-3D-Molecular-Pre-training"><a href="#Fractional-Denoising-for-3D-Molecular-Pre-training" class="headerlink" title="Fractional Denoising for 3D Molecular Pre-training"></a>Fractional Denoising for 3D Molecular Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10683">http://arxiv.org/abs/2307.10683</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fengshikun/frad">https://github.com/fengshikun/frad</a></li>
<li>paper_authors: Shikun Feng, Yuyan Ni, Yanyan Lan, Zhi-Ming Ma, Wei-Ying Ma</li>
<li>for: 提高3D分子预训练方法的性能，尤其是在下游药物发现任务中。</li>
<li>methods: 提出了一种新的混合噪声策略，包括了两种类型的噪声：弧度噪声和坐标噪声。</li>
<li>results: 经过广泛的实验 validate，新提出的分子表示方法（Frad）在分子表示方面取得了新的状态流行，在QM9和MD17中的12个任务中取得了9个任务的最佳性能，在7个任务中取得了8个任务的最佳性能。<details>
<summary>Abstract</summary>
Coordinate denoising is a promising 3D molecular pre-training method, which has achieved remarkable performance in various downstream drug discovery tasks. Theoretically, the objective is equivalent to learning the force field, which is revealed helpful for downstream tasks. Nevertheless, there are two challenges for coordinate denoising to learn an effective force field, i.e. low coverage samples and isotropic force field. The underlying reason is that molecular distributions assumed by existing denoising methods fail to capture the anisotropic characteristic of molecules. To tackle these challenges, we propose a novel hybrid noise strategy, including noises on both dihedral angel and coordinate. However, denoising such hybrid noise in a traditional way is no more equivalent to learning the force field. Through theoretical deductions, we find that the problem is caused by the dependency of the input conformation for covariance. To this end, we propose to decouple the two types of noise and design a novel fractional denoising method (Frad), which only denoises the latter coordinate part. In this way, Frad enjoys both the merits of sampling more low-energy structures and the force field equivalence. Extensive experiments show the effectiveness of Frad in molecular representation, with a new state-of-the-art on 9 out of 12 tasks of QM9 and on 7 out of 8 targets of MD17.
</details>
<details>
<summary>摘要</summary>
“坐标降噪是一种有前途的3D分子预训方法，其在多种下游药物探索任务中实现了杰出的表现。理论上，这个目标等价于学习力场，这是有帮助的下游任务。然而，坐标降噪面临两个挑战，即低覆盖样本和iso tropic force field。这些挑战的根本原因是现有的降噪方法假设的分子分布不能捕捉分子的扩散特性。为了解决这些挑战，我们提出了一种新的混合噪音策略，包括了坐标和 dip hedral 的噪音。然而，对这种混合噪音的传统方式降噪不等于学习力场。经过理论推导，我们发现这问题是因为输入配置的弹性所致。为了解决这问题，我们提出了一种新的分解方法（Frad），它只降噪coordinate部分。这样，Frad能够同时享有更多低能构造的样本和力场等价。实验结果显示Frad在分子表现方面具有新的州际之优，在QM9上9个任务和MD17上7个任务中均成为新的州际之优。”
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-for-classification-of-noisy-QR-codes"><a href="#Deep-learning-for-classification-of-noisy-QR-codes" class="headerlink" title="Deep learning for classification of noisy QR codes"></a>Deep learning for classification of noisy QR codes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10677">http://arxiv.org/abs/2307.10677</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rebecca Leygonie, Sylvain Lobry, ), Laurent Wendling (LIPADE)</li>
<li>for: 本研究旨在定义基于深度学习的古典分类模型在抽象图像上的局限性，当应用于不可见视觉对象的图像。</li>
<li>methods: 我们使用了基于QR码的健康证件读取信息生成的图像进行训练。我们比较了基于深度学习的分类模型和传统（束缚）解码方法在噪声存在时的性能。</li>
<li>results: 我们发现基于深度学习的模型可以对抽象图像进行有效的理解。<details>
<summary>Abstract</summary>
We wish to define the limits of a classical classification model based on deep learning when applied to abstract images, which do not represent visually identifiable objects.QR codes (Quick Response codes) fall into this category of abstract images: one bit corresponding to one encoded character, QR codes were not designed to be decoded manually. To understand the limitations of a deep learning-based model for abstract image classification, we train an image classification model on QR codes generated from information obtained when reading a health pass. We compare a classification model with a classical (deterministic) decoding method in the presence of noise. This study allows us to conclude that a model based on deep learning can be relevant for the understanding of abstract images.
</details>
<details>
<summary>摘要</summary>
我团队想要定义深度学习模型在抽象图像分类中的限制，当应用于不可读取视觉标识 объек 的图像。二维码（Quick Response codes）是这类抽象图像的例子：每一比特对应一个编码字符，二维码并不是为人工手动解码设计的。通过使用一个基于深度学习的图像分类模型，我们在读取健康证的信息生成的二维码上进行训练。我们将这种模型与传统的束定解码方法进行比较，以确定在噪声存在时，深度学习模型对于抽象图像的理解是否有所有限。这项研究允许我们结论：基于深度学习的模型可以对抽象图像进行有效的理解。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-What-to-Share-in-Federated-Learning-Perspectives-on-Model-Utility-Privacy-Leakage-and-Communication-Efficiency"><a href="#A-Survey-of-What-to-Share-in-Federated-Learning-Perspectives-on-Model-Utility-Privacy-Leakage-and-Communication-Efficiency" class="headerlink" title="A Survey of What to Share in Federated Learning: Perspectives on Model Utility, Privacy Leakage, and Communication Efficiency"></a>A Survey of What to Share in Federated Learning: Perspectives on Model Utility, Privacy Leakage, and Communication Efficiency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10655">http://arxiv.org/abs/2307.10655</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiawei Shao, Zijian Li, Wenqiang Sun, Tailin Zhou, Yuchang Sun, Lumin Liu, Zehong Lin, Jun Zhang</li>
<li>for: 本研究旨在系统地检采 Federated Learning (FL) 中可以共享的信息，包括模型参数、静态数据和知识。</li>
<li>methods: 本文提出了一种新的分类方法，将 FL 方法分为三类：模型分享、静态数据分享和知识分享。此外，本文还对不同的分享方法的隐私攻击性进行分析，并评估了不同防御策略的效果。</li>
<li>results: 本文通过实验比较不同分享方法的性能和通信开销，并评估了不同防御策略的效果。 results 表明，模型分享和知识分享可以提高 FL 的性能，但是可能导致隐私泄露。静态数据分享可以减少通信开销，但是可能导致模型质量下降。<details>
<summary>Abstract</summary>
Federated learning (FL) has emerged as a highly effective paradigm for privacy-preserving collaborative training among different parties. Unlike traditional centralized learning, which requires collecting data from each party, FL allows clients to share privacy-preserving information without exposing private datasets. This approach not only guarantees enhanced privacy protection but also facilitates more efficient and secure collaboration among multiple participants. Therefore, FL has gained considerable attention from researchers, promoting numerous surveys to summarize the related works. However, the majority of these surveys concentrate on methods sharing model parameters during the training process, while overlooking the potential of sharing other forms of local information. In this paper, we present a systematic survey from a new perspective, i.e., what to share in FL, with an emphasis on the model utility, privacy leakage, and communication efficiency. This survey differs from previous ones due to four distinct contributions. First, we present a new taxonomy of FL methods in terms of the sharing methods, which includes three categories of shared information: model sharing, synthetic data sharing, and knowledge sharing. Second, we analyze the vulnerability of different sharing methods to privacy attacks and review the defense mechanisms that provide certain privacy guarantees. Third, we conduct extensive experiments to compare the performance and communication overhead of various sharing methods in FL. Besides, we assess the potential privacy leakage through model inversion and membership inference attacks, while comparing the effectiveness of various defense approaches. Finally, we discuss potential deficiencies in current methods and outline future directions for improvement.
</details>
<details>
<summary>摘要</summary>
Federated learning (FL) 已经 emerged as a highly effective paradigm for privacy-preserving collaborative training among different parties. Unlike traditional centralized learning, which requires collecting data from each party, FL allows clients to share privacy-preserving information without exposing private datasets. This approach not only guarantees enhanced privacy protection but also facilitates more efficient and secure collaboration among multiple participants. Therefore, FL has gained considerable attention from researchers, promoting numerous surveys to summarize the related works. However, the majority of these surveys concentrate on methods sharing model parameters during the training process, while overlooking the potential of sharing other forms of local information. In this paper, we present a systematic survey from a new perspective, i.e., what to share in FL, with an emphasis on the model utility, privacy leakage, and communication efficiency. This survey differs from previous ones due to four distinct contributions. First, we present a new taxonomy of FL methods in terms of the sharing methods, which includes three categories of shared information: model sharing, synthetic data sharing, and knowledge sharing. Second, we analyze the vulnerability of different sharing methods to privacy attacks and review the defense mechanisms that provide certain privacy guarantees. Third, we conduct extensive experiments to compare the performance and communication overhead of various sharing methods in FL. Besides, we assess the potential privacy leakage through model inversion and membership inference attacks, while comparing the effectiveness of various defense approaches. Finally, we discuss potential deficiencies in current methods and outline future directions for improvement.
</details></li>
</ul>
<hr>
<h2 id="Conditional-expectation-network-for-SHAP"><a href="#Conditional-expectation-network-for-SHAP" class="headerlink" title="Conditional expectation network for SHAP"></a>Conditional expectation network for SHAP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10654">http://arxiv.org/abs/2307.10654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ronald Richman, Mario V. Wüthrich</li>
<li>for: 这个论文旨在介绍一种能够explaining predictive models的技术，即SHapley Additive exPlanation（SHAP）。</li>
<li>methods: 这种技术使用了一种名为conditional expectation version和unconditional expectation version（后者也被称为interventional SHAP）的两种版本。通常情况下，用到了后者，因为它更加 computationally efficient。</li>
<li>results: 这种技术可以有效地计算出conditional SHAP，并且能够正确地考虑特征组件之间的依赖关系。此外，这种技术还可以提供drop1和anova分析，这与其普通的 GLM 对应项类似。此外，这种技术还可以提供一种名为partial dependence plot（PDP）的 counterpart，它考虑了特征组件之间的正确依赖关系。<details>
<summary>Abstract</summary>
A very popular model-agnostic technique for explaining predictive models is the SHapley Additive exPlanation (SHAP). The two most popular versions of SHAP are a conditional expectation version and an unconditional expectation version (the latter is also known as interventional SHAP). Except for tree-based methods, usually the unconditional version is used (for computational reasons). We provide a (surrogate) neural network approach which allows us to efficiently calculate the conditional version for both neural networks and other regression models, and which properly considers the dependence structure in the feature components. This proposal is also useful to provide drop1 and anova analyses in complex regression models which are similar to their generalized linear model (GLM) counterparts, and we provide a partial dependence plot (PDP) counterpart that considers the right dependence structure in the feature components.
</details>
<details>
<summary>摘要</summary>
非常受欢迎的模型无关技术之一是 SHapley Additive exPlanation（SHAP）。这两个最受欢迎的版本是 conditional expectation version 和 interventional SHAP（后者也被称为 conditional SHAP）。除了树状方法外，通常使用 unconditional version（由 computational reasons）。我们提供一种（surrogate）神经网络方法，可以有效计算 conditional version，并且正确地考虑特征组件之间的依赖关系。这种提议还有用于提供 drop1 和 anova 分析在复杂回归模型中，与其普通线性模型（GLM） counterpart 类似，并提供了一种考虑特征组件之间正确依赖关系的 PDP 对应。
</details></li>
</ul>
<hr>
<h2 id="Refining-the-Optimization-Target-for-Automatic-Univariate-Time-Series-Anomaly-Detection-in-Monitoring-Services"><a href="#Refining-the-Optimization-Target-for-Automatic-Univariate-Time-Series-Anomaly-Detection-in-Monitoring-Services" class="headerlink" title="Refining the Optimization Target for Automatic Univariate Time Series Anomaly Detection in Monitoring Services"></a>Refining the Optimization Target for Automatic Univariate Time Series Anomaly Detection in Monitoring Services</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10653">http://arxiv.org/abs/2307.10653</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manqing Dong, Zhanxiang Zhao, Yitong Geng, Wentao Li, Wei Wang, Huai Jiang</li>
<li>for: 本研究旨在提供一个自动化参数优化框架，以便在时间序列异常检测中实现更高的可靠性和系统性能。</li>
<li>methods: 该框架基于三个优化目标：预测分数、形态分数和敏感度分数，可以轻松地适应不同的模型背景而无需先知或手动标注efforts。</li>
<li>results: 该框架在在线应用了超过六个月，处理了每分钟50,000个时间序列，并提供了一个易用的用户界面，以及达到了检测结果的期望。  Comparative evaluations on public datasets further confirm the effectiveness of the proposed framework.<details>
<summary>Abstract</summary>
Time series anomaly detection is crucial for industrial monitoring services that handle a large volume of data, aiming to ensure reliability and optimize system performance. Existing methods often require extensive labeled resources and manual parameter selection, highlighting the need for automation. This paper proposes a comprehensive framework for automatic parameter optimization in time series anomaly detection models. The framework introduces three optimization targets: prediction score, shape score, and sensitivity score, which can be easily adapted to different model backbones without prior knowledge or manual labeling efforts. The proposed framework has been successfully applied online for over six months, serving more than 50,000 time series every minute. It simplifies the user's experience by requiring only an expected sensitive value, offering a user-friendly interface, and achieving desired detection results. Extensive evaluations conducted on public datasets and comparison with other methods further confirm the effectiveness of the proposed framework.
</details>
<details>
<summary>摘要</summary>
时序系列异常检测是对industrial monitoring服务的重要任务，旨在确保可靠性并优化系统性能。现有方法frequently require extensive labeled resources and manual parameter selection, highlighting the need for automation。本文提出了一个完整的自动参数优化框架 для时序系列异常检测模型。该框架引入了三个优化目标：预测得分、形态得分和敏感度得分，可以方便地适应不同的模型脊梁而无需先知或手动标注efforts。提议的框架已经在线上运行了超过六个月，处理了每分钟50,000个时序数据。它简化了用户的经验，只需要提供预期的敏感值，提供了易用的界面，并实现了所求的检测结果。对公共数据集和其他方法进行了广泛的评估，证明了提议的框架的有效性。
</details></li>
</ul>
<hr>
<h2 id="Data-Driven-Latency-Probability-Prediction-for-Wireless-Networks-Focusing-on-Tail-Probabilities"><a href="#Data-Driven-Latency-Probability-Prediction-for-Wireless-Networks-Focusing-on-Tail-Probabilities" class="headerlink" title="Data-Driven Latency Probability Prediction for Wireless Networks: Focusing on Tail Probabilities"></a>Data-Driven Latency Probability Prediction for Wireless Networks: Focusing on Tail Probabilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10648">http://arxiv.org/abs/2307.10648</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/samiemostafavi/wireless-pr3d">https://github.com/samiemostafavi/wireless-pr3d</a></li>
<li>paper_authors: Samie Mostafavi, Gourav Prateek Sharma, James Gross</li>
<li>for:  guarantees end-to-end network latency with extremely high reliability (99.999%) for cyber-physical systems and human-in-the-loop applications.</li>
<li>methods:  uses state-of-the-art data-driven approaches, such as mixture density networks (MDN) and extreme value mixture models, to predict the tail of the latency distribution and estimate the likelihood of rare latencies conditioned on network parameters.</li>
<li>results:  benchmarks the proposed approaches using actual latency measurements of IEEE 802.11g (WiFi), commercial private, and a software-defined 5G network, and evaluates their sensitivities concerning the tail probabilities.Here’s the full text in Simplified Chinese:</li>
<li>for: 这项研究旨在为新兴应用领域，如Cyber-Physical Systems和人在循环应用，提供绝对保证99.999%的终端到终端网络延迟，以确保高可靠性。</li>
<li>methods: 这项研究使用现代数据驱动方法，如混合密度网络（MDN）和极值混合模型，来预测延迟分布的尾部，并估计基于网络参数的罕见延迟的可能性。</li>
<li>results: 研究使用IEEE 802.11g（WiFi）、商业私人网络和软件定义5G网络的具体延迟测量来评估提议方法的可靠性和敏感性，并评估尾部概率的变化。<details>
<summary>Abstract</summary>
With the emergence of new application areas, such as cyber-physical systems and human-in-the-loop applications, there is a need to guarantee a certain level of end-to-end network latency with extremely high reliability, e.g., 99.999%. While mechanisms specified under IEEE 802.1as time-sensitive networking (TSN) can be used to achieve these requirements for switched Ethernet networks, implementing TSN mechanisms in wireless networks is challenging due to their stochastic nature. To conform the wireless link to a reliability level of 99.999%, the behavior of extremely rare outliers in the latency probability distribution, or the tail of the distribution, must be analyzed and controlled. This work proposes predicting the tail of the latency distribution using state-of-the-art data-driven approaches, such as mixture density networks (MDN) and extreme value mixture models, to estimate the likelihood of rare latencies conditioned on the network parameters, which can be used to make more informed decisions in wireless transmission. Actual latency measurements of IEEE 802.11g (WiFi), commercial private and a software-defined 5G network are used to benchmark the proposed approaches and evaluate their sensitivities concerning the tail probabilities.
</details>
<details>
<summary>摘要</summary>
随着新的应用领域的出现，如Cyber-physical systems和human-in-the-loop应用，需要保证一定的端到端网络延迟水平，例如99.999%的可靠性。而IEEE 802.1as时间敏感网络（TSN）的机制可以用来实现这些需求 для交换网络，但是在无线网络中实现TSN机制具有挑战性，因为无线链路具有随机性。为了将无线链路修正到99.999%的可靠性水平，需要分析和控制无线链路的偶极异值（tail）的延迟分布。本工作提议使用当今最佳的数据驱动方法，如混合概率网络（MDN）和极值混合模型，来预测延迟分布的尾部，以估计基于网络参数的罕见延迟的可能性。这些预测结果可以用于更 Informed decisions in wireless transmission。实际测量IEEE 802.11g（WiFi）、商业私人网络和软件定义5G网络的延迟测量被用来评估提议的方法和其对尾概率的敏感性。
</details></li>
</ul>
<hr>
<h2 id="Fisher-Rao-distance-and-pullback-SPD-cone-distances-between-multivariate-normal-distributions"><a href="#Fisher-Rao-distance-and-pullback-SPD-cone-distances-between-multivariate-normal-distributions" class="headerlink" title="Fisher-Rao distance and pullback SPD cone distances between multivariate normal distributions"></a>Fisher-Rao distance and pullback SPD cone distances between multivariate normal distributions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10644">http://arxiv.org/abs/2307.10644</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frank Nielsen</li>
<li>for: 本文主要针对多变量正态分布的处理，具体来说是定义正态分布之间的距离和路径。</li>
<li>methods: 本文提出了一种快速和稳定的方法来 aproximate Fisher-Rao 距离，以及基于 diffeomorphic embedding的一种新的距离计算方法。</li>
<li>results: 本文显示了 pullback Hilbert cone 距离的计算只需要计算extreme minimal和 maximal eigenvalues of matrices，而不是计算整个 Fisher information metric。此外，本文还应用了这些距离在 clustering 任务中。<details>
<summary>Abstract</summary>
Data sets of multivariate normal distributions abound in many scientific areas like diffusion tensor imaging, structure tensor computer vision, radar signal processing, machine learning, just to name a few. In order to process those normal data sets for downstream tasks like filtering, classification or clustering, one needs to define proper notions of dissimilarities between normals and paths joining them. The Fisher-Rao distance defined as the Riemannian geodesic distance induced by the Fisher information metric is such a principled metric distance which however is not known in closed-form excepts for a few particular cases. In this work, we first report a fast and robust method to approximate arbitrarily finely the Fisher-Rao distance between multivariate normal distributions. Second, we introduce a class of distances based on diffeomorphic embeddings of the normal manifold into a submanifold of the higher-dimensional symmetric positive-definite cone corresponding to the manifold of centered normal distributions. We show that the projective Hilbert distance on the cone yields a metric on the embedded normal submanifold and we pullback that cone distance with its associated straight line Hilbert cone geodesics to obtain a distance and smooth paths between normal distributions. Compared to the Fisher-Rao distance approximation, the pullback Hilbert cone distance is computationally light since it requires to compute only the extreme minimal and maximal eigenvalues of matrices. Finally, we show how to use those distances in clustering tasks.
</details>
<details>
<summary>摘要</summary>
“多变量正态分布数据集在许多科学领域非常普遍，如扩散tensor成像、结构tensor计算机视觉、雷达信号处理、机器学习等。为了处理这些正态数据集，进行后续任务如过滤、分类或归类，需要定义正确的差异量 между正态。悉尼诺距离（Fisher-Rao distance）是一种原理正确的距离度量，但是除了其特殊情况外，它的闭式表示不知道。在这个工作中，我们首先报告了一种快速和可靠的方法来精确地 aproximate multivariate normal distributions之间的悉尼诺距离。其次，我们引入了一类基于diff伪omorphic embedding的距离，该距离在正态拓扑上定义了一个度量。我们显示了该距离在投影卷积体上是一个度量，并将其pullback到正态拓扑上，从而获得了一个距离和smooth paths between normal distributions。与悉尼诺距离 aproximation相比，pullback Hilbert cone distance更加计算轻量级，只需计算最小和最大特征值。最后，我们介绍了如何使用这些距离在 clustering 任务中。”Note: "Simplified Chinese" is a romanization of the Chinese language that uses simpler characters and grammar to facilitate typing and reading. It is not a formal standard, but rather a common practice among Chinese speakers who use the Roman alphabet.
</details></li>
</ul>
<hr>
<h2 id="SciBench-Evaluating-College-Level-Scientific-Problem-Solving-Abilities-of-Large-Language-Models"><a href="#SciBench-Evaluating-College-Level-Scientific-Problem-Solving-Abilities-of-Large-Language-Models" class="headerlink" title="SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models"></a>SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10635">http://arxiv.org/abs/2307.10635</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mandyyyyii/scibench">https://github.com/mandyyyyii/scibench</a></li>
<li>paper_authors: Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R. Loomba, Shichang Zhang, Yizhou Sun, Wei Wang</li>
<li>for: 这个论文的目的是检验大语言模型（LLMs）在复杂科学问题解决能力方面的进步。</li>
<li>methods: 该论文使用了两个精心编辑的数据集：一个公共数据集包含了高等教育文books中的科学问题，另一个关闭数据集包含了计算机科学和数学专业考试中的问题。然后，通过对两种代表性的LLMs进行各种提示策略的测试，对这两个数据集进行了深入的比较研究。</li>
<li>results: 结果显示，目前的LLMs在解决复杂科学问题方面的表现仅有35.80%的得分，并且通过详细的用户研究，对LLMs的错误分类为十种问题解决能力。研究发现，不同的提示策略之间没有一个显著的占优，一些策略可以提高某些问题解决能力，但同时会导致其他能力下降。<details>
<summary>Abstract</summary>
Recent advances in large language models (LLMs) have demonstrated notable progress on many mathematical benchmarks. However, most of these benchmarks only feature problems grounded in junior and senior high school subjects, contain only multiple-choice questions, and are confined to a limited scope of elementary arithmetic operations. To address these issues, this paper introduces an expansive benchmark suite SciBench that aims to systematically examine the reasoning capabilities required for complex scientific problem solving. SciBench contains two carefully curated datasets: an open set featuring a range of collegiate-level scientific problems drawn from mathematics, chemistry, and physics textbooks, and a closed set comprising problems from undergraduate-level exams in computer science and mathematics. Based on the two datasets, we conduct an in-depth benchmark study of two representative LLMs with various prompting strategies. The results reveal that current LLMs fall short of delivering satisfactory performance, with an overall score of merely 35.80%. Furthermore, through a detailed user study, we categorize the errors made by LLMs into ten problem-solving abilities. Our analysis indicates that no single prompting strategy significantly outperforms others and some strategies that demonstrate improvements in certain problem-solving skills result in declines in other skills. We envision that SciBench will catalyze further developments in the reasoning abilities of LLMs, thereby ultimately contributing to scientific research and discovery.
</details>
<details>
<summary>摘要</summary>
SciBench includes two datasets: an open set with a range of collegiate-level scientific problems from mathematics, chemistry, and physics textbooks, and a closed set with problems from undergraduate-level exams in computer science and mathematics. We conduct an in-depth study of two representative LLMs with various prompting strategies, and find that current LLMs only achieve an overall score of 35.80%.Through a detailed user study, we categorize the errors made by LLMs into ten problem-solving abilities. Our analysis shows that no single prompting strategy significantly outperforms others, and some strategies that improve in certain skills result in declines in other skills.We envision that SciBench will drive further developments in the reasoning abilities of LLMs, ultimately contributing to scientific research and discovery.
</details></li>
</ul>
<hr>
<h2 id="Generative-Language-Models-on-Nucleotide-Sequences-of-Human-Genes"><a href="#Generative-Language-Models-on-Nucleotide-Sequences-of-Human-Genes" class="headerlink" title="Generative Language Models on Nucleotide Sequences of Human Genes"></a>Generative Language Models on Nucleotide Sequences of Human Genes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10634">http://arxiv.org/abs/2307.10634</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/boun-tabi/generativelm-genes">https://github.com/boun-tabi/generativelm-genes</a></li>
<li>paper_authors: Musa Nuri Ihtiyar, Arzucan Ozgur</li>
<li>for: The paper is focused on developing an autoregressive generative language model for DNA sequences, with a focus on nucleotide sequences of human genes.</li>
<li>methods: The authors use a variety of techniques, including RNNs and N-grams, to explore the generative capabilities of their model. They also examine the use of real-life tasks beyond classical metrics such as perplexity to evaluate the model’s performance.</li>
<li>results: The authors observe that RNNs perform the best, and that the data-hungry nature of the model is not significantly affected by selecting a language with a minimal vocabulary size. They also find that the model is able to generate coherent and meaningful DNA sequences.Here is the same information in Simplified Chinese text:</li>
<li>for: 该论文主要关注开发一种基于转换器的生成语言模型，用于DNA序列。</li>
<li>methods: 作者使用了多种技术，包括RNNs和N-grams，来探索该模型的生成能力。他们还评估了使用实际任务以外的 métricas，以评估模型的性能。</li>
<li>results: 作者发现RNNs表现最佳，而选择一种语言的词汇量较少并不导致很大的数据需求变化。他们还发现模型能够生成有意义和 coherent的DNA序列。<details>
<summary>Abstract</summary>
Language models, primarily transformer-based ones, obtained colossal success in NLP. To be more precise, studies like BERT in NLU and works such as GPT-3 for NLG are very crucial. DNA sequences are very close to natural language in terms of structure, so if the DNA-related bioinformatics domain is concerned, discriminative models, like DNABert, exist. Yet, the generative side of the coin is mainly unexplored to the best of our knowledge. Consequently, we focused on developing an autoregressive generative language model like GPT-3 for DNA sequences. Because working with whole DNA sequences is challenging without substantial computational resources, we decided to carry out our study on a smaller scale, focusing on nucleotide sequences of human genes, unique parts in DNA with specific functionalities, instead of the whole DNA. This decision did not change the problem structure a lot due to the fact that both DNA and genes can be seen as 1D sequences consisting of four different nucleotides without losing much information and making too much simplification. First of all, we systematically examined an almost entirely unexplored problem and observed that RNNs performed the best while simple techniques like N-grams were also promising. Another beneficial point was learning how to work with generative models on languages we do not understand, unlike natural language. How essential using real-life tasks beyond the classical metrics such as perplexity is observed. Furthermore, checking whether the data-hungry nature of these models can be changed through selecting a language with minimal vocabulary size, four owing to four different types of nucleotides, is examined. The reason for reviewing this was that choosing such a language might make the problem easier. However, what we observed in this study was it did not provide that much of a change in the amount of data needed.
</details>
<details>
<summary>摘要</summary>
Language models, primarily transformer-based ones, have achieved great success in NLP. To be more precise, studies like BERT in NLU and works such as GPT-3 for NLG are very crucial. DNA sequences are very close to natural language in terms of structure, so if the DNA-related bioinformatics domain is concerned, discriminative models like DNABert exist. However, the generative side of the coin is mainly unexplored to the best of our knowledge. Therefore, we focused on developing an autoregressive generative language model like GPT-3 for DNA sequences. Because working with whole DNA sequences is challenging without substantial computational resources, we decided to carry out our study on a smaller scale, focusing on nucleotide sequences of human genes, unique parts in DNA with specific functionalities, instead of the whole DNA. This decision did not change the problem structure much because both DNA and genes can be seen as 1D sequences consisting of four different nucleotides without losing much information and making too much simplification.First, we systematically examined an almost entirely unexplored problem and observed that RNNs performed the best, while simple techniques like N-grams were also promising. Another beneficial point was learning how to work with generative models on languages we do not understand, unlike natural language. We also found that using real-life tasks beyond classical metrics such as perplexity is essential. Furthermore, we checked whether the data-hungry nature of these models can be changed through selecting a language with minimal vocabulary size, four owing to four different types of nucleotides. However, what we observed in this study was that choosing such a language did not provide that much of a change in the amount of data needed.
</details></li>
</ul>
<hr>
<h2 id="Multi-Method-Self-Training-Improving-Code-Generation-With-Text-And-Vice-Versa"><a href="#Multi-Method-Self-Training-Improving-Code-Generation-With-Text-And-Vice-Versa" class="headerlink" title="Multi-Method Self-Training: Improving Code Generation With Text, And Vice Versa"></a>Multi-Method Self-Training: Improving Code Generation With Text, And Vice Versa</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10633">http://arxiv.org/abs/2307.10633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shriyash K. Upadhyay, Etan J. Ginsberg</li>
<li>for: 提高语言模型的可用性和性能</li>
<li>methods: 多种方法自动训练</li>
<li>results: 1) 提高较弱方法（最高提高30%），2) 提高较强方法（最高提高32.2%），3) 提高相关 yet distinct 任务（最高提高10.3%）的性能，并通过数据生成和多种方法的使用来解释MMST的成功。Here’s a more detailed explanation of each point:1. for: The paper aims to improve the availability and performance of language models by introducing a new method called Multi-Method Self-Training (MMST).2. methods: The paper uses multiple methods for self-training, including the filtered outputs of another method, to augment the strengths and ameliorate the weaknesses of each method.3. results: The paper shows that MMST can improve the performance of the less performant method by up to 30%, improve the performance of the more performant method by up to 32.2%, and improve the performance of related yet distinct tasks by up to 10.3%. The improvement in performance is driven by the use of multiple methods, and the paper provides ablation analyses to explore why MMST works.<details>
<summary>Abstract</summary>
Large Language Models have many methods for solving the same problem. This introduces novel strengths (different methods may work well for different problems) and weaknesses (it may be difficult for users to know which method to use). In this paper, we introduce Multi-Method Self-Training (MMST), where one method is trained on the filtered outputs of another, allowing us to augment the strengths and ameliorate the weaknesses of each method. Using a 176B parameter model trained on both language and code, we show that MMST can 1) improve the less performant method (up to 30%) making the model easier to use, 2) improve the more performant method (up to 32.2%) making the model more performant, and 3) improve the performance of related but distinct tasks (up to 10.3%) by improving the ability of the model to generate rationales. We then conduct ablation analyses to explore why MMST works. We show that MMST generates more data than traditional self-training, but the improvement in performance is driven by the use of multiple methods. We also analyze prompt-engineering and anti-correlated performance between methods as means of making MMST more effective. We hope the evidence from our paper motivates machine learning researchers to explore ways in which advances in language models allow for new forms of training.
</details>
<details>
<summary>摘要</summary>
大型语言模型有多种方法解决同一个问题，这导致新的优势（不同的方法可以对不同的问题有好的表现）和弱点（用户可能Difficult to determine which method to use）。在这篇论文中，我们介绍Multi-Method Self-Training（MMST），其中一种方法被另一种方法所训练，从而可以增强每种方法的优势和改善每种方法的缺点。使用176亿参数的模型，我们显示MMST可以1）提高较差的方法（最多30%），使模型更易使用，2）提高较佳的方法（最多32.2%），使模型更高效，3）提高相关但不同的任务（最多10.3%）的表现，并提高模型生成理由的能力。然后我们进行了删除分析，以探索MMST的作用原理。我们发现MMST生成了更多的数据，但是改善表现的关键在于使用多种方法。我们还分析了引擎和反相关的表现，以提高MMST的效果。我们希望这篇论文的证据能够鼓励机器学习研究人员探索在语言模型技术的进步下，新的训练方法的可能性。
</details></li>
</ul>
<hr>
<h2 id="Unmasking-Falsehoods-in-Reviews-An-Exploration-of-NLP-Techniques"><a href="#Unmasking-Falsehoods-in-Reviews-An-Exploration-of-NLP-Techniques" class="headerlink" title="Unmasking Falsehoods in Reviews: An Exploration of NLP Techniques"></a>Unmasking Falsehoods in Reviews: An Exploration of NLP Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10617">http://arxiv.org/abs/2307.10617</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anusuya Baby Hari Krishnan<br>for: 这篇研究paper的目的是探讨如何使用机器学习模型来识别诈骗评价，尤其是餐厅评价中的伪评价。methods: 本研究使用了n-gram模型和max features来检测诈骗内容，并与五种不同的机器学习分类算法进行比较。此外，研究还应用了深度学习技术来增强诈骗评价的检测。results: 研究结果显示，使用了调和攻击分类器的方法可以实现最高的准确率，不仅在文本分类方面，而且在识别伪评价方面也表现出色。此外，研究还发现了诈骗评价的特征和特征，对于在线商业中的评价推广和评价管理提供了宝贵的见解。<details>
<summary>Abstract</summary>
In the contemporary digital landscape, online reviews have become an indispensable tool for promoting products and services across various businesses. Marketers, advertisers, and online businesses have found incentives to create deceptive positive reviews for their products and negative reviews for their competitors' offerings. As a result, the writing of deceptive reviews has become an unavoidable practice for businesses seeking to promote themselves or undermine their rivals. Detecting such deceptive reviews has become an intense and ongoing area of research. This research paper proposes a machine learning model to identify deceptive reviews, with a particular focus on restaurants. This study delves into the performance of numerous experiments conducted on a dataset of restaurant reviews known as the Deceptive Opinion Spam Corpus. To accomplish this, an n-gram model and max features are developed to effectively identify deceptive content, particularly focusing on fake reviews. A benchmark study is undertaken to explore the performance of two different feature extraction techniques, which are then coupled with five distinct machine learning classification algorithms. The experimental results reveal that the passive aggressive classifier stands out among the various algorithms, showcasing the highest accuracy not only in text classification but also in identifying fake reviews. Moreover, the research delves into data augmentation and implements various deep learning techniques to further enhance the process of detecting deceptive reviews. The findings shed light on the efficacy of the proposed machine learning approach and offer valuable insights into dealing with deceptive reviews in the realm of online businesses.
</details>
<details>
<summary>摘要</summary>
现代数字景观中，在线评价已成为不同业务的无可或的推广工具。广告商、市场部和在线业务都找到了创造假评价来推广自己的产品或推翻竞争对手的产品的利益。因此，假评价的写作已成为为企业推广自己或推翻竞争对手的不可或缺的做法。检测这些假评价已成为一项激烈和持续的研究领域。本研究提出了一种机器学习模型，用于识别假评价，尤其是针对餐厅的评价。本研究通过对知名餐厅评价数据集——假评价垃圾数据集进行多个实验，开发出n格RAM模型和最佳特征，以便有效地识别假内容，特别是假评价。进行了基准研究，以explore两种不同的特征提取技术的性能，然后与五种不同的机器学习分类算法结合。实验结果显示，通过负拒抵抗分类器的方式，在文本分类和假评价识别方面达到了最高精度。此外，研究还探讨了数据扩充和深度学习技术，以进一步提高假评价的检测过程。研究结果为检测在线评价中的假评价提供了有价值的指导和精深的理解，对于在线业务而言，对于假评价的检测和处理也提供了有价值的方法。
</details></li>
</ul>
<hr>
<h2 id="Heterogeneous-Federated-Learning-State-of-the-art-and-Research-Challenges"><a href="#Heterogeneous-Federated-Learning-State-of-the-art-and-Research-Challenges" class="headerlink" title="Heterogeneous Federated Learning: State-of-the-art and Research Challenges"></a>Heterogeneous Federated Learning: State-of-the-art and Research Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10616">http://arxiv.org/abs/2307.10616</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/marswhu/hfl_survey">https://github.com/marswhu/hfl_survey</a></li>
<li>paper_authors: Mang Ye, Xiuwen Fang, Bo Du, Pong C. Yuen, Dacheng Tao</li>
<li>for: 这篇论文主要是为了探讨 Federated Learning（FL）在大规模实际应用中的挑战和解决方案。</li>
<li>methods: 论文回顾了现有的 Federated Learning 研究，以及各种研究挑战，包括统计差异、模型差异、通信差异、设备差异以及其他挑战。</li>
<li>results: 论文提出了一种新的 Federated Learning 方法分类系统，并进行了深入的分析和评价。此外，论文还提出了一些未来研究方向，以便进一步发展这一领域。<details>
<summary>Abstract</summary>
Federated learning (FL) has drawn increasing attention owing to its potential use in large-scale industrial applications. Existing federated learning works mainly focus on model homogeneous settings. However, practical federated learning typically faces the heterogeneity of data distributions, model architectures, network environments, and hardware devices among participant clients. Heterogeneous Federated Learning (HFL) is much more challenging, and corresponding solutions are diverse and complex. Therefore, a systematic survey on this topic about the research challenges and state-of-the-art is essential. In this survey, we firstly summarize the various research challenges in HFL from five aspects: statistical heterogeneity, model heterogeneity, communication heterogeneity, device heterogeneity, and additional challenges. In addition, recent advances in HFL are reviewed and a new taxonomy of existing HFL methods is proposed with an in-depth analysis of their pros and cons. We classify existing methods from three different levels according to the HFL procedure: data-level, model-level, and server-level. Finally, several critical and promising future research directions in HFL are discussed, which may facilitate further developments in this field. A periodically updated collection on HFL is available at https://github.com/marswhu/HFL_Survey.
</details>
<details>
<summary>摘要</summary>
受欢迎的 Federated Learning（FL）在大规模产业应用中受到了越来越多的关注，因为它可以在分布式数据和模型之间学习。然而，实际的 Federated Learning 往往面临参与客户端的数据分布、模型结构、网络环境和硬件设备之间的多样性。这种多样性导致了异化 Federated Learning（HFL）的研究挑战更大，需要对应的多种和复杂的解决方案。因此，一篇系统性的survey sobre这个主题是非常重要的。在这份survey中，我们首先总结了HFL中不同方面的研究挑战，包括统计学上的多样性、模型上的多样性、通信上的多样性、设备上的多样性以及其他挑战。此外，我们还审视了最新的HFL研究进展，并提出了一种新的HFL方法分类方案，进行深入的分析其优缺点。我们将exististing方法分为三个不同的水平：数据级、模型级和服务级。最后，我们讨论了HFL未来的一些重要和有前途的研究方向，这些方向可能会促进这一领域的进一步发展。一个periodically更新的HFL集成可以在https://github.com/marswhu/HFL_Survey上找到。
</details></li>
</ul>
<hr>
<h2 id="Flatness-Aware-Minimization-for-Domain-Generalization"><a href="#Flatness-Aware-Minimization-for-Domain-Generalization" class="headerlink" title="Flatness-Aware Minimization for Domain Generalization"></a>Flatness-Aware Minimization for Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11108">http://arxiv.org/abs/2307.11108</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingxuan Zhang, Renzhe Xu, Han Yu, Yancheng Dong, Pengfei Tian, Peng Cu</li>
<li>for: 这个论文主要研究的是领域总结（Domain Generalization，DG）中的优化器选择。</li>
<li>methods: 作者提出了一种新的方法 called Flatness-Aware Minimization for Domain Generalization (FAD), which can efficiently optimize both zeroth-order and first-order flatness simultaneously for DG.</li>
<li>results: 实验结果显示，FAD 在多个 DG dataset 上表现出优于其他 zeroth-order 和 first-order 平坦性感知优化方法。 Additionally, the authors provide theoretical analyses of the FAD’s out-of-distribution (OOD) generalization error and convergence.<details>
<summary>Abstract</summary>
Domain generalization (DG) seeks to learn robust models that generalize well under unknown distribution shifts. As a critical aspect of DG, optimizer selection has not been explored in depth. Currently, most DG methods follow the widely used benchmark, DomainBed, and utilize Adam as the default optimizer for all datasets. However, we reveal that Adam is not necessarily the optimal choice for the majority of current DG methods and datasets. Based on the perspective of loss landscape flatness, we propose a novel approach, Flatness-Aware Minimization for Domain Generalization (FAD), which can efficiently optimize both zeroth-order and first-order flatness simultaneously for DG. We provide theoretical analyses of the FAD's out-of-distribution (OOD) generalization error and convergence. Our experimental results demonstrate the superiority of FAD on various DG datasets. Additionally, we confirm that FAD is capable of discovering flatter optima in comparison to other zeroth-order and first-order flatness-aware optimization methods.
</details>
<details>
<summary>摘要</summary>
领域总结（DG）目的是学习可靠的模型，以适应未知分布变化。作为DG的关键方面，优化器选择尚未得到广泛探讨。目前大多数DG方法采用DomainBed标准启动点，并使用Adam作为所有数据集的默认优化器。然而，我们发现Adam并不一定是现有DG方法和数据集中的优选。基于损失地形平坦性的视角，我们提出了一种新的方法：适应性评估降维minimization for Domain Generalization（FAD），可以有效地同时优化零次训练和首次训练的平坦性。我们提供了关于FAD的OOD泛化误差和收敛性的理论分析。我们的实验结果表明FAD在多个DG数据集上具有优越性。此外，我们证明了FAD可以更好地找到抽象的优点，相比其他零次训练和首次训练平坦性意识的优化方法。
</details></li>
</ul>
<hr>
<h2 id="Ensemble-Learning-based-Anomaly-Detection-for-IoT-Cybersecurity-via-Bayesian-Hyperparameters-Sensitivity-Analysis"><a href="#Ensemble-Learning-based-Anomaly-Detection-for-IoT-Cybersecurity-via-Bayesian-Hyperparameters-Sensitivity-Analysis" class="headerlink" title="Ensemble Learning based Anomaly Detection for IoT Cybersecurity via Bayesian Hyperparameters Sensitivity Analysis"></a>Ensemble Learning based Anomaly Detection for IoT Cybersecurity via Bayesian Hyperparameters Sensitivity Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10596">http://arxiv.org/abs/2307.10596</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tin Lai, Farnaz Farid, Abubakar Bello, Fariza Sabrina</li>
<li>for:  This paper focuses on enhancing IoT cybersecurity via anomaly detection using ensemble machine learning methods.</li>
<li>methods: The proposed method utilizes Bayesian hyperparameter optimization to adapt to a network environment with multiple IoT sensor readings, and combines the predictive power of multiple machine learning models to enhance predictive accuracy in heterogeneous datasets.</li>
<li>results: The proposed method shows high predictive power compared to traditional methods through experimental illustration.<details>
<summary>Abstract</summary>
The Internet of Things (IoT) integrates more than billions of intelligent devices over the globe with the capability of communicating with other connected devices with little to no human intervention. IoT enables data aggregation and analysis on a large scale to improve life quality in many domains. In particular, data collected by IoT contain a tremendous amount of information for anomaly detection. The heterogeneous nature of IoT is both a challenge and an opportunity for cybersecurity. Traditional approaches in cybersecurity monitoring often require different kinds of data pre-processing and handling for various data types, which might be problematic for datasets that contain heterogeneous features. However, heterogeneous types of network devices can often capture a more diverse set of signals than a single type of device readings, which is particularly useful for anomaly detection. In this paper, we present a comprehensive study on using ensemble machine learning methods for enhancing IoT cybersecurity via anomaly detection. Rather than using one single machine learning model, ensemble learning combines the predictive power from multiple models, enhancing their predictive accuracy in heterogeneous datasets rather than using one single machine learning model. We propose a unified framework with ensemble learning that utilises Bayesian hyperparameter optimisation to adapt to a network environment that contains multiple IoT sensor readings. Experimentally, we illustrate their high predictive power when compared to traditional methods.
</details>
<details>
<summary>摘要</summary>
互联网物联网（IoT）已经 integrates 了全球多亿个智能设备，这些设备可以自动通信，无需人类干预。IoT 可以实现大规模数据聚合和分析，以改善生活质量在多个领域。尤其是在 IoT 数据中，存在许多有用的信息可以探测到异常。然而，IoT 网络中的异类网络设备具有挑战和机会，传统的防护策略通常需要不同的数据处理和处理方法，这可能会导致复杂的数据处理问题。但是，异类网络设备可以捕捉到更多的信号，这是特别有用的 для 异常探测。在这篇文章中，我们提出了一个统一框架，使用组合学 machine learning 方法来强化 IoT 网络的防护。而不是使用单一的 machine learning 模型，组合学可以结合多个模型的预测力，在不同的网络环境中提高预测精度。我们提出了一个 Bayesian 参数优化的框架，以适应含有多个 IoT 感应器读取的网络环境。实验结果表明，这种方法的预测力较高，比较传统的方法。
</details></li>
</ul>
<hr>
<h2 id="Forecasting-Battery-Electric-Vehicle-Charging-Behavior-A-Deep-Learning-Approach-Equipped-with-Micro-Clustering-and-SMOTE-Techniques"><a href="#Forecasting-Battery-Electric-Vehicle-Charging-Behavior-A-Deep-Learning-Approach-Equipped-with-Micro-Clustering-and-SMOTE-Techniques" class="headerlink" title="Forecasting Battery Electric Vehicle Charging Behavior: A Deep Learning Approach Equipped with Micro-Clustering and SMOTE Techniques"></a>Forecasting Battery Electric Vehicle Charging Behavior: A Deep Learning Approach Equipped with Micro-Clustering and SMOTE Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10588">http://arxiv.org/abs/2307.10588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanif Tayarani, Trisha V. Ramadoss, Vaishnavi Karanam, Gil Tal, Christopher Nitta</li>
<li>for: This study aims to develop a novel Micro Clustering Deep Neural Network (MCDNN) to forecast BEV charging events, which is essential for electricity load aggregators and utility managers to provide charging stations and electricity capacity effectively.</li>
<li>methods: The study uses a robust dataset of trips and charges that occurred in California between 2015 and 2020 from 132 BEVs, spanning 5 BEV models, to train the MCDNN.</li>
<li>results: The numerical findings show that the proposed MCDNN is more effective than benchmark approaches in this field, such as support vector machine, k nearest neighbors, decision tree, and other neural network-based models in predicting the charging events.<details>
<summary>Abstract</summary>
Energy systems, climate change, and public health are among the primary reasons for moving toward electrification in transportation. Transportation electrification is being promoted worldwide to reduce emissions. As a result, many automakers will soon start making only battery electric vehicles (BEVs). BEV adoption rates are rising in California, mainly due to climate change and air pollution concerns. While great for climate and pollution goals, improperly managed BEV charging can lead to insufficient charging infrastructure and power outages. This study develops a novel Micro Clustering Deep Neural Network (MCDNN), an artificial neural network algorithm that is highly effective at learning BEVs trip and charging data to forecast BEV charging events, information that is essential for electricity load aggregators and utility managers to provide charging stations and electricity capacity effectively. The MCDNN is configured using a robust dataset of trips and charges that occurred in California between 2015 and 2020 from 132 BEVs, spanning 5 BEV models for a total of 1570167 vehicle miles traveled. The numerical findings revealed that the proposed MCDNN is more effective than benchmark approaches in this field, such as support vector machine, k nearest neighbors, decision tree, and other neural network-based models in predicting the charging events.
</details>
<details>
<summary>摘要</summary>
transportation electrification is being promoted worldwide to reduce emissions. As a result, many automakers will soon start making only battery electric vehicles (BEVs). BEV adoption rates are rising in California, mainly due to climate change and air pollution concerns. However, improperly managed BEV charging can lead to insufficient charging infrastructure and power outages. This study develops a novel Micro Clustering Deep Neural Network (MCDNN), an artificial neural network algorithm that is highly effective at learning BEVs trip and charging data to forecast BEV charging events, information that is essential for electricity load aggregators and utility managers to provide charging stations and electricity capacity effectively. The MCDNN is configured using a robust dataset of trips and charges that occurred in California between 2015 and 2020 from 132 BEVs, spanning 5 BEV models for a total of 1570167 vehicle miles traveled. The numerical findings revealed that the proposed MCDNN is more effective than benchmark approaches in this field, such as support vector machine, k nearest neighbors, decision tree, and other neural network-based models in predicting the charging events.Here is the translation in Traditional Chinese:运输电化正在全球推广以减少排放。因此，许多汽车生产商将很快开始生产仅有电池电动车 (BEV)。加利福尼亚州的 BEV 采购率正在增长，主要是因为气候变化和空气污染的关注。然而，不当管理 BEV 充电可能会导致充电基础设施不足和电力停机。本研究开发了一个微型对 clustering深度神经网 (MCDNN)，一种人工神经网络算法，可以很好地学习 BEV 行程和充电数据，预测 BEV 充电事件，这些信息是电力聚集者和供应商调度员需要的。MCDNN 是使用加利福尼亚州2015-2020年间的 132 部 BEV 的行程和充电数据集成的，总共行程 1570167 公里。numerical 的结果显示，提案的 MCDNN 在这个领域中比基准方法更有效，例如支持向量机器、最近邻居、决策树和其他神经网络基础模型在预测充电事件方面。
</details></li>
</ul>
<hr>
<h2 id="A-Holistic-Assessment-of-the-Reliability-of-Machine-Learning-Systems"><a href="#A-Holistic-Assessment-of-the-Reliability-of-Machine-Learning-Systems" class="headerlink" title="A Holistic Assessment of the Reliability of Machine Learning Systems"></a>A Holistic Assessment of the Reliability of Machine Learning Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10586">http://arxiv.org/abs/2307.10586</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anthony Corso, David Karamadian, Romeo Valentin, Mary Cooper, Mykel J. Kochenderfer</li>
<li>for: 本研究旨在评估机器学习系统的可靠性，以满足高风险应用场景中的需求。</li>
<li>methods: 本研究提出了一种整体评估机器学习系统可靠性的方法，包括评估五个关键属性：内部分布准确率、环境变化Robustness、攻击Robustness、准确性和外部分布检测。</li>
<li>results: 研究人员通过对实际任务中的500多个模型进行评估，发现不同的算法方法可以同时提高多个可靠性指标的表现。这个研究为机器学习可靠性的全面理解和未来研发提供了一份重要的贡献。<details>
<summary>Abstract</summary>
As machine learning (ML) systems increasingly permeate high-stakes settings such as healthcare, transportation, military, and national security, concerns regarding their reliability have emerged. Despite notable progress, the performance of these systems can significantly diminish due to adversarial attacks or environmental changes, leading to overconfident predictions, failures to detect input faults, and an inability to generalize in unexpected scenarios. This paper proposes a holistic assessment methodology for the reliability of ML systems. Our framework evaluates five key properties: in-distribution accuracy, distribution-shift robustness, adversarial robustness, calibration, and out-of-distribution detection. A reliability score is also introduced and used to assess the overall system reliability. To provide insights into the performance of different algorithmic approaches, we identify and categorize state-of-the-art techniques, then evaluate a selection on real-world tasks using our proposed reliability metrics and reliability score. Our analysis of over 500 models reveals that designing for one metric does not necessarily constrain others but certain algorithmic techniques can improve reliability across multiple metrics simultaneously. This study contributes to a more comprehensive understanding of ML reliability and provides a roadmap for future research and development.
</details>
<details>
<summary>摘要</summary>
为了更好地了解机器学习（ML）系统在高风险场景中的可靠性，这篇文章提出了一种整体评估方法。我们的框架评估了五个关键属性：内部分布式准确率、环境变化 Robustness、对抗攻击 Robustness、准确率和外部分布式检测。我们还引入了一个可靠度分数，用于评估整体系统可靠性。为了提供不同算法方法的性能分析，我们标识并分类了当前领先技术，然后使用我们提议的可靠性指标和可靠度分数来评估选择的模型。我们对超过500个模型进行了实际任务上的评估，发现不同的算法方法可以同时提高多个可靠性指标的性能。这篇研究对ML系统可靠性的全面理解做出了贡献，并提供了未来研发的路线图。
</details></li>
</ul>
<hr>
<h2 id="Intelligent-model-for-offshore-China-sea-fog-forecasting"><a href="#Intelligent-model-for-offshore-China-sea-fog-forecasting" class="headerlink" title="Intelligent model for offshore China sea fog forecasting"></a>Intelligent model for offshore China sea fog forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10580">http://arxiv.org/abs/2307.10580</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanfei Xiang, Qinghong Zhang, Mingqing Wang, Ruixue Xia, Yang Kong, Xiaomeng Huang</li>
<li>for: 预测海上雾的精准性和时效性非常重要，以确保海上和沿海经济活动的正常运行。</li>
<li>methods: 本研究使用机器学习方法，在数值天气预测模型中嵌入，通过用 Yangtze River Estuary（YRE）沿岸地区为例进行实验。在训练机器学习模型之前，我们使用时间延迟相关分析技术来 indentify关键预测器和解释海上雾出现的基本机制。此外，我们还实施 ensemble learning 和焦点损失函数，以提高我们模型的预测能力。</li>
<li>results: 我们的机器学习基于方法在一年的全面数据集上进行评估，包括天气站观测数据和历史预测数据。结果显示，我们的机器学习方法在预测海上雾的可见度低于或等于1公里的预测前60小时的情况下，表现出色，提高了可见度预测的抽象率（POD），同时降低了假阳性率（FAR）。<details>
<summary>Abstract</summary>
Accurate and timely prediction of sea fog is very important for effectively managing maritime and coastal economic activities. Given the intricate nature and inherent variability of sea fog, traditional numerical and statistical forecasting methods are often proven inadequate. This study aims to develop an advanced sea fog forecasting method embedded in a numerical weather prediction model using the Yangtze River Estuary (YRE) coastal area as a case study. Prior to training our machine learning model, we employ a time-lagged correlation analysis technique to identify key predictors and decipher the underlying mechanisms driving sea fog occurrence. In addition, we implement ensemble learning and a focal loss function to address the issue of imbalanced data, thereby enhancing the predictive ability of our model. To verify the accuracy of our method, we evaluate its performance using a comprehensive dataset spanning one year, which encompasses both weather station observations and historical forecasts. Remarkably, our machine learning-based approach surpasses the predictive performance of two conventional methods, the weather research and forecasting nonhydrostatic mesoscale model (WRF-NMM) and the algorithm developed by the National Oceanic and Atmospheric Administration (NOAA) Forecast Systems Laboratory (FSL). Specifically, in regard to predicting sea fog with a visibility of less than or equal to 1 km with a lead time of 60 hours, our methodology achieves superior results by increasing the probability of detection (POD) while simultaneously reducing the false alarm ratio (FAR).
</details>
<details>
<summary>摘要</summary>
Traditional numerical and statistical forecasting methods are often inadequate for accurately predicting sea fog due to its complex and inherently variable nature. This study aims to develop an advanced sea fog forecasting method using a numerical weather prediction model and machine learning techniques, with the Yangtze River Estuary (YRE) coastal area as a case study.Before training our machine learning model, we use a time-lagged correlation analysis technique to identify key predictors and understand the underlying mechanisms driving sea fog occurrence. We also employ ensemble learning and a focal loss function to address the issue of imbalanced data, which enhances the predictive ability of our model.To evaluate the accuracy of our method, we use a comprehensive dataset covering one year, which includes both weather station observations and historical forecasts. Our machine learning-based approach outperforms two conventional methods, the weather research and forecasting nonhydrostatic mesoscale model (WRF-NMM) and the algorithm developed by the National Oceanic and Atmospheric Administration (NOAA) Forecast Systems Laboratory (FSL). Specifically, our methodology achieves better results in predicting sea fog with a visibility of less than or equal to 1 km with a lead time of 60 hours, with higher probability of detection (POD) and lower false alarm ratio (FAR).
</details></li>
</ul>
<hr>
<h2 id="SecureBoost-Hyperparameter-Tuning-via-Multi-Objective-Federated-Learning"><a href="#SecureBoost-Hyperparameter-Tuning-via-Multi-Objective-Federated-Learning" class="headerlink" title="SecureBoost Hyperparameter Tuning via Multi-Objective Federated Learning"></a>SecureBoost Hyperparameter Tuning via Multi-Objective Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10579">http://arxiv.org/abs/2307.10579</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyao Ren, Yan Kang, Lixin Fan, Linghua Yang, Yongxin Tong, Qiang Yang</li>
<li>for: 这篇研究旨在提高vertical federated learning中的数据隐私保护和可读性，通过使用树茨增强和同时保护数据隐私。</li>
<li>methods: 本研究使用了SecureBoost算法，并提出了一个名为 Constrained Multi-Objective SecureBoost (CMOSB) 的算法来选择最佳的参数设定，以实现最佳的折冲点。</li>
<li>results: 实验结果显示，CMOSB 可以获得不仅比基eline更好的参数设定，而且可以找到最佳的参数设定，以满足不同的参数需求。<details>
<summary>Abstract</summary>
SecureBoost is a tree-boosting algorithm leveraging homomorphic encryption to protect data privacy in vertical federated learning setting. It is widely used in fields such as finance and healthcare due to its interpretability, effectiveness, and privacy-preserving capability. However, SecureBoost suffers from high computational complexity and risk of label leakage. To harness the full potential of SecureBoost, hyperparameters of SecureBoost should be carefully chosen to strike an optimal balance between utility, efficiency, and privacy. Existing methods either set hyperparameters empirically or heuristically, which are far from optimal. To fill this gap, we propose a Constrained Multi-Objective SecureBoost (CMOSB) algorithm to find Pareto optimal solutions that each solution is a set of hyperparameters achieving optimal tradeoff between utility loss, training cost, and privacy leakage. We design measurements of the three objectives. In particular, the privacy leakage is measured using our proposed instance clustering attack. Experimental results demonstrate that the CMOSB yields not only hyperparameters superior to the baseline but also optimal sets of hyperparameters that can support the flexible requirements of FL participants.
</details>
<details>
<summary>摘要</summary>
《secureboost》是一种树融合算法，利用同质加密保护数据隐私在垂直联合学习设置下。它在银行和医疗等领域广泛应用，因为它具有可读性、效果和隐私保护能力。然而，secureboost受到高计算复杂度和标签泄露的风险。为了激活secureboost的潜力，需要仔细选择secureboost的Hyperparameter，以达到优质、效率和隐私三方面的丰富平衡。现有的方法可以通过实验或euristic来设置Hyperparameter，但这些方法远远不够优化。为了填补这个空白，我们提出了一种受限制多目标secureboost（CMOSB）算法，找到Pareto优解，每个解是一组Hyperparameter，实现了数据损失、训练成本和隐私泄露之间的优质平衡。我们设计了三个目标的度量。具体来说，隐私泄露被我们提出的实例划分攻击来度量。实验结果表明，CMOSB可以不仅提供超过基准的Hyperparameter，还可以找到优质的多个Hyperparameter，以满足FL参与者的灵活要求。
</details></li>
</ul>
<hr>
<h2 id="Boosting-Federated-Learning-Convergence-with-Prototype-Regularization"><a href="#Boosting-Federated-Learning-Convergence-with-Prototype-Regularization" class="headerlink" title="Boosting Federated Learning Convergence with Prototype Regularization"></a>Boosting Federated Learning Convergence with Prototype Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10575">http://arxiv.org/abs/2307.10575</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Qiao, Huy Q. Le, Choong Seon Hong</li>
<li>for: 提高 Federated Learning 中数据不均衡的问题</li>
<li>methods: 使用 Prototype-based 常数 regularization 策略</li>
<li>results: 在 MNIST 和 Fashion-MNIST 上实现了平均测试精度提高3.3% 和8.9%，比 FedAvg 最佳参考值更高。同时，在不均衡 Setting 下具有快速收敛速率。<details>
<summary>Abstract</summary>
As a distributed machine learning technique, federated learning (FL) requires clients to collaboratively train a shared model with an edge server without leaking their local data. However, the heterogeneous data distribution among clients often leads to a decrease in model performance. To tackle this issue, this paper introduces a prototype-based regularization strategy to address the heterogeneity in the data distribution. Specifically, the regularization process involves the server aggregating local prototypes from distributed clients to generate a global prototype, which is then sent back to the individual clients to guide their local training. The experimental results on MNIST and Fashion-MNIST show that our proposal achieves improvements of 3.3% and 8.9% in average test accuracy, respectively, compared to the most popular baseline FedAvg. Furthermore, our approach has a fast convergence rate in heterogeneous settings.
</details>
<details>
<summary>摘要</summary>
为了应对分布式机器学习技术中的数据分布不均问题，本文提出了一种基于原型的规范约束策略。具体来说，在服务器端，通过将分布式客户端的本地原型聚合到一个全局原型中，然后将全局原型发送回到各个客户端，以便在本地训练中作为指导。实验结果表明，与最常用的基准方法FedAvg相比，我们的方案在MNIST和Fashion-MNIST dataset上的平均测试准确率提高了3.3%和8.9%。此外，我们的方法在不同数据分布情况下具有快速收敛速率。
</details></li>
</ul>
<hr>
<h2 id="Deceptive-Alignment-Monitoring"><a href="#Deceptive-Alignment-Monitoring" class="headerlink" title="Deceptive Alignment Monitoring"></a>Deceptive Alignment Monitoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10569">http://arxiv.org/abs/2307.10569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andres Carranza, Dhruv Pai, Rylan Schaeffer, Arnuv Tandon, Sanmi Koyejo</li>
<li>for: 防止大型机器学习模型的潜在偏误行为，即模型在外表看起来合理但实际上在隐私 modify 其行为的情况。</li>
<li>methods: 这篇论文认为，以下几个领域的研究将在未来对掩饰偏误的监测起到关键作用：多任务学习、逻辑学习、推荐系统、自然语言处理等。</li>
<li>results: 本论文认为，这些领域的研究将带来长期挑战和新的研究机遇，同时也会对掩饰偏误的监测提供新的思路和方法。<details>
<summary>Abstract</summary>
As the capabilities of large machine learning models continue to grow, and as the autonomy afforded to such models continues to expand, the spectre of a new adversary looms: the models themselves. The threat that a model might behave in a seemingly reasonable manner, while secretly and subtly modifying its behavior for ulterior reasons is often referred to as deceptive alignment in the AI Safety & Alignment communities. Consequently, we call this new direction Deceptive Alignment Monitoring. In this work, we identify emerging directions in diverse machine learning subfields that we believe will become increasingly important and intertwined in the near future for deceptive alignment monitoring, and we argue that advances in these fields present both long-term challenges and new research opportunities. We conclude by advocating for greater involvement by the adversarial machine learning community in these emerging directions.
</details>
<details>
<summary>摘要</summary>
large machine learning models的能力不断增长，自动化程度也在不断扩展，但隐藏在这些模型中的新敌人开始显现：模型本身。这种情况被称为“欺骗吧效”（deceptive alignment）在AI安全与对齐社区中，我们称这个新方向为“欺骗对齐监测”（Deceptive Alignment Monitoring）。在这篇文章中，我们认为未来几年将成为核心的多种机器学习子领域会变得越来越重要，这些领域包括：* 模型内部异常检测（Anomaly Detection in Models）* 模型之间的对抗（Model Adversarial Training）* 模型的隐藏状态检测（Detection of Hidden States in Models）* 模型的追踪和回归（Model Tracking and Recall）我们认为，这些领域的发展将带来长期挑战和新的研究机会。我们建议对抗机器学习社区更加参与这些领域的研究。
</details></li>
</ul>
<hr>
<h2 id="FACADE-A-Framework-for-Adversarial-Circuit-Anomaly-Detection-and-Evaluation"><a href="#FACADE-A-Framework-for-Adversarial-Circuit-Anomaly-Detection-and-Evaluation" class="headerlink" title="FACADE: A Framework for Adversarial Circuit Anomaly Detection and Evaluation"></a>FACADE: A Framework for Adversarial Circuit Anomaly Detection and Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10563">http://arxiv.org/abs/2307.10563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhruv Pai, Andres Carranza, Rylan Schaeffer, Arnuv Tandon, Sanmi Koyejo</li>
<li>for: 提高模型Robustness和可扩展的模型监测</li>
<li>methods: 使用probabilistic和几何方法进行不supervised机器学习中的自动异常检测</li>
<li>results: 生成probabilistic分布于环境中，提供高级别异常检测和模型Robustness的工具<details>
<summary>Abstract</summary>
We present FACADE, a novel probabilistic and geometric framework designed for unsupervised mechanistic anomaly detection in deep neural networks. Its primary goal is advancing the understanding and mitigation of adversarial attacks. FACADE aims to generate probabilistic distributions over circuits, which provide critical insights to their contribution to changes in the manifold properties of pseudo-classes, or high-dimensional modes in activation space, yielding a powerful tool for uncovering and combating adversarial attacks. Our approach seeks to improve model robustness, enhance scalable model oversight, and demonstrates promising applications in real-world deployment settings.
</details>
<details>
<summary>摘要</summary>
我们介绍FACADE，一种新的 probabilistic和几何框架，用于无监督机器学习模型中的不可预测异常检测。其主要目标是提高对抗攻击的理解和防御。FACADE通过生成征值分布来提供关键的征值对模型 pseudo-class 或高维模式空间的变化 Properties的理解，从而提供一种有力的抗击攻击工具。我们的方法可以提高模型的可靠性，提高可扩展的模型监测，并在实际应用中显示了有前途的应用。Note that Simplified Chinese is used here, which is the standard writing system used in mainland China. Traditional Chinese is also widely used, especially in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="Shared-Adversarial-Unlearning-Backdoor-Mitigation-by-Unlearning-Shared-Adversarial-Examples"><a href="#Shared-Adversarial-Unlearning-Backdoor-Mitigation-by-Unlearning-Shared-Adversarial-Examples" class="headerlink" title="Shared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial Examples"></a>Shared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial Examples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10562">http://arxiv.org/abs/2307.10562</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaokui Wei, Mingda Zhang, Hongyuan Zha, Baoyuan Wu</li>
<li>For: The paper is written for mitigating backdoor attacks in machine learning models.* Methods: The paper uses adversarial training techniques to purify a backdoored model using a small clean dataset.* Results: The proposed method, Shared Adversarial Unlearning (SAU), achieves state-of-the-art performance for backdoor defense on various benchmark datasets and network architectures.<details>
<summary>Abstract</summary>
Backdoor attacks are serious security threats to machine learning models where an adversary can inject poisoned samples into the training set, causing a backdoored model which predicts poisoned samples with particular triggers to particular target classes, while behaving normally on benign samples. In this paper, we explore the task of purifying a backdoored model using a small clean dataset. By establishing the connection between backdoor risk and adversarial risk, we derive a novel upper bound for backdoor risk, which mainly captures the risk on the shared adversarial examples (SAEs) between the backdoored model and the purified model. This upper bound further suggests a novel bi-level optimization problem for mitigating backdoor using adversarial training techniques. To solve it, we propose Shared Adversarial Unlearning (SAU). Specifically, SAU first generates SAEs, and then, unlearns the generated SAEs such that they are either correctly classified by the purified model and/or differently classified by the two models, such that the backdoor effect in the backdoored model will be mitigated in the purified model. Experiments on various benchmark datasets and network architectures show that our proposed method achieves state-of-the-art performance for backdoor defense.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传统的反向工程学习模型中存在背门风险，敌对者可以在训练集中植入恶意样本，导致背门模型预测恶意样本时特定的触发器和目标类，而正常样本则正常预测。在这篇论文中，我们研究了使用小量净化数据来纯化背门模型。通过证明背门风险和敌对风险之间的连接，我们 derive了一个 noval upper bound for 背门风险，该bound主要捕捉了在背门模型和纯化模型之间共享的敌对例子（SAEs）中的风险。这个bound还提出了一个 novel bi-level优化问题，用于 mitigating 背门风险。为解决这个问题，我们提出了 Shared Adversarial Unlearning（SAU）。具体来说，SAU首先生成SAEs，然后对生成的SAEs进行反学习，使其被纯化模型正确分类或者被两个模型不同分类，从而 Mitigate the backdoor effect in the backdoored model in the purified model。在各种 benchmark 数据集和网络架构上进行了实验，我们的提出的方法实现了对背门防御的state-of-the-art表现。
</details></li>
</ul>
<hr>
<h2 id="Post-variational-quantum-neural-networks"><a href="#Post-variational-quantum-neural-networks" class="headerlink" title="Post-variational quantum neural networks"></a>Post-variational quantum neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10560">http://arxiv.org/abs/2307.10560</a></li>
<li>repo_url: None</li>
<li>paper_authors: Po-Wei Huang, Patrick Rebentrost</li>
<li>for: 提高量子计算机的计算能力，解决现有级别的硬件不足问题。</li>
<li>methods: 使用杂合量子-经典计算机和变量算法，并采用后续策略来调整量子计算机的参数。</li>
<li>results: 在实际应用中，如手写数字识别，实现96%的分类精度。<details>
<summary>Abstract</summary>
Quantum computing has the potential to provide substantial computational advantages over current state-of-the-art classical supercomputers. However, current hardware is not advanced enough to execute fault-tolerant quantum algorithms. An alternative of using hybrid quantum-classical computing with variational algorithms can exhibit barren plateau issues, causing slow convergence of gradient-based optimization techniques. In this paper, we discuss "post-variational strategies", which shift tunable parameters from the quantum computer to the classical computer, opting for ensemble strategies when optimizing quantum models. We discuss various strategies and design principles for constructing individual quantum circuits, where the resulting ensembles can be optimized with convex programming. Further, we discuss architectural designs of post-variational quantum neural networks and analyze the propagation of estimation errors throughout such neural networks. Lastly, we show that our algorithm can be applied to real-world applications such as image classification on handwritten digits, producing a 96% classification accuracy.
</details>
<details>
<summary>摘要</summary>
量子计算有可能提供对当前最高标准的类别超级计算机的重要计算优势。然而，当前硬件还不够先进，无法执行错误tolerant的量子算法。为了解决这个问题，我们可以使用杂合量子-类别计算，使用变量算法。然而，这会导致杂合板块问题，使得梯度基于优化技术的收敛速度变得非常慢。在这篇论文中，我们讨论了“后variational策略”，即将可调参数从量子计算机Shift到类别计算机上，选择 ensemble策略来优化量子模型。我们介绍了不同的策略和设计原则，用于构建个体量子电路，以及对这些集合进行优化的几何编程。此外，我们还讨论了post-variational量子神经网络的建筑设计，并分析了这些神经网络中的估计错误的传播。最后，我们展示了我们的算法可以应用于真实的应用场景，如手写数字图像识别，并实现了96%的分类精度。
</details></li>
</ul>
<hr>
<h2 id="Air-Traffic-Controller-Workload-Level-Prediction-using-Conformalized-Dynamical-Graph-Learning"><a href="#Air-Traffic-Controller-Workload-Level-Prediction-using-Conformalized-Dynamical-Graph-Learning" class="headerlink" title="Air Traffic Controller Workload Level Prediction using Conformalized Dynamical Graph Learning"></a>Air Traffic Controller Workload Level Prediction using Conformalized Dynamical Graph Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10559">http://arxiv.org/abs/2307.10559</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ymlasu/para-atm-collection">https://github.com/ymlasu/para-atm-collection</a></li>
<li>paper_authors: Yutian Pang, Jueming Hu, Christopher S. Lieber, Nancy J. Cooke, Yongming Liu</li>
<li>for: 这篇论文主要是为了预测空交控制员（ATCo）的劳动负担，以避免过载和确保操作安全性和空域使用效率。</li>
<li>methods: 论文首先进行了对空交工作负担的研究综述，主要从空交的角度进行了分析。然后， authors 介绍了一种基于人类 loop（HITL）的实验设置，使用退休空交控制员进行了 simulations，并从三个鹊翼接近场景中获取了空交数据和劳动负担标签。</li>
<li>results: 实验结果表明， besides 交通密度特征，交通冲突特征也对劳动负担预测具有贡献（例如最小水平&#x2F;垂直距离）。 direct 从空间时间图Layout中学习Graph neural network可以实现更高的预测精度，比手动制作的交通复杂度特征更有效。 conformal prediction 是一种有用的工具，可以进一步提高模型预测精度，生成一个范围内的预测工作负担标签。<details>
<summary>Abstract</summary>
Air traffic control (ATC) is a safety-critical service system that demands constant attention from ground air traffic controllers (ATCos) to maintain daily aviation operations. The workload of the ATCos can have negative effects on operational safety and airspace usage. To avoid overloading and ensure an acceptable workload level for the ATCos, it is important to predict the ATCos' workload accurately for mitigation actions. In this paper, we first perform a review of research on ATCo workload, mostly from the air traffic perspective. Then, we briefly introduce the setup of the human-in-the-loop (HITL) simulations with retired ATCos, where the air traffic data and workload labels are obtained. The simulations are conducted under three Phoenix approach scenarios while the human ATCos are requested to self-evaluate their workload ratings (i.e., low-1 to high-7). Preliminary data analysis is conducted. Next, we propose a graph-based deep-learning framework with conformal prediction to identify the ATCo workload levels. The number of aircraft under the controller's control varies both spatially and temporally, resulting in dynamically evolving graphs. The experiment results suggest that (a) besides the traffic density feature, the traffic conflict feature contributes to the workload prediction capabilities (i.e., minimum horizontal/vertical separation distance); (b) directly learning from the spatiotemporal graph layout of airspace with graph neural network can achieve higher prediction accuracy, compare to hand-crafted traffic complexity features; (c) conformal prediction is a valuable tool to further boost model prediction accuracy, resulting a range of predicted workload labels. The code used is available at \href{https://github.com/ymlasu/para-atm-collection/blob/master/air-traffic-prediction/ATC-Workload-Prediction/}{$\mathsf{Link}$}.
</details>
<details>
<summary>摘要</summary>
空交控制（ATC）是一个安全关键的服务系统，需要地面空交控制员（ATCo）不断关注，以维护日常航空运营。ATCo的工作负担可能会影响航空安全和空域使用。为了避免过载和保持可接受的工作负担水平，需要准确预测ATCo的工作负担。在这篇论文中，我们首先进行了研究人员对ATCo工作负担的查询，主要来自航空交通的视角。然后，我们简要介绍了使用退休空交控制员（HITL）的人类在Loop（SIM） simulations的设置，其中获取了航空交通数据和工作负担标签。在三个鸡毛蒜蓉approach场景下，人类ATCo被请求自我评估他们的工作负担等级（即低1到高7）。我们进行了初步数据分析。接着，我们提议了一个基于图的深度学习框架，用于预测ATCo工作负担水平。由于空交控制员控制的飞机数量在空间和时间上都变化，因此导致的是动态演化的图。实验结果表明：（a）除了交通密度特征，交通冲突特征也对工作负担预测具有贡献（即最小水平/垂直分离距离）；（b）直接从空间时间图的空间图结构中学习，使用图神经网络可以达到更高的预测精度，比手工设计的交通复杂度特征更好；（c）使用兼容预测可以进一步提高模型预测精度，得到一个范围内的预测工作负担等级。代码可以在 $\mathsf{Link}$ 中找到。
</details></li>
</ul>
<hr>
<h2 id="SC-VALL-E-Style-Controllable-Zero-Shot-Text-to-Speech-Synthesizer"><a href="#SC-VALL-E-Style-Controllable-Zero-Shot-Text-to-Speech-Synthesizer" class="headerlink" title="SC VALL-E: Style-Controllable Zero-Shot Text to Speech Synthesizer"></a>SC VALL-E: Style-Controllable Zero-Shot Text to Speech Synthesizer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10550">http://arxiv.org/abs/2307.10550</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/0913ktg/sc_vall-e">https://github.com/0913ktg/sc_vall-e</a></li>
<li>paper_authors: Daegyeom Kim, Seongho Hong, Yong-Hoon Choi<br>for: 这个论文主要目标是提出一种基于神经编码器语言模型（VALL-E）的风格控制（SC）模型，用于生成可控的语音。methods: 该 SC VALL-E 模型使用了各种各样的Speaker、情感和说话风格等特征来控制语音的多种特征，并通过设计新的风格网络来控制这些特征。results: 我们通过对三种表达性语音合成模型进行比较性试验，发现 SC VALL-E 模型可以在不使用训练数据中的提示音时产生多种表达性的声音，并且与现有模型相比具有竞争力。<details>
<summary>Abstract</summary>
Expressive speech synthesis models are trained by adding corpora with diverse speakers, various emotions, and different speaking styles to the dataset, in order to control various characteristics of speech and generate the desired voice. In this paper, we propose a style control (SC) VALL-E model based on the neural codec language model (called VALL-E), which follows the structure of the generative pretrained transformer 3 (GPT-3). The proposed SC VALL-E takes input from text sentences and prompt audio and is designed to generate controllable speech by not simply mimicking the characteristics of the prompt audio but by controlling the attributes to produce diverse voices. We identify tokens in the style embedding matrix of the newly designed style network that represent attributes such as emotion, speaking rate, pitch, and voice intensity, and design a model that can control these attributes. To evaluate the performance of SC VALL-E, we conduct comparative experiments with three representative expressive speech synthesis models: global style token (GST) Tacotron2, variational autoencoder (VAE) Tacotron2, and original VALL-E. We measure word error rate (WER), F0 voiced error (FVE), and F0 gross pitch error (F0GPE) as evaluation metrics to assess the accuracy of generated sentences. For comparing the quality of synthesized speech, we measure comparative mean option score (CMOS) and similarity mean option score (SMOS). To evaluate the style control ability of the generated speech, we observe the changes in F0 and mel-spectrogram by modifying the trained tokens. When using prompt audio that is not present in the training data, SC VALL-E generates a variety of expressive sounds and demonstrates competitive performance compared to the existing models. Our implementation, pretrained models, and audio samples are located on GitHub.
</details>
<details>
<summary>摘要</summary>
干脆的语音合成模型通过添加多个说话者、不同的情感和说话风格到数据集来控制各种语音特征并生成愿望的声音。在这篇论文中，我们提出了基于神经码器语言模型（VALL-E）的样式控制（SC）VALL-E模型。该模型根据生成预训练转换器3（GPT-3）的结构而设计，可以从文本句子输入和提示音采样生成可控的语音。我们在新设计的风格网络中标识了表达不同属性的 токен，如情感、说话速度、音高和声音强度，并设计了一个可以控制这些属性的模型。为了评估SC VALLE的性能，我们对三种表达性语音合成模型进行比较性实验：全球风格标识符（GST）Tacotron2、变量自适应器（VAE）Tacotron2以及原始VALL-E。我们使用word error rate（WER）、F0 voiced error（FVE）和F0 gross pitch error（F0GPE）作为评估指标，以评估生成句子的准确性。为了比较合成的质量，我们使用相对意见分（CMOS）和相似意见分（SMOS）作为评估指标。为了评估生成的样式控制能力，我们观察改变训练的token后的F0和mel-spectrogram变化。当使用不在训练数据中的提示音时，SC VALL-E可以生成多种表达性的声音，并与现有模型相比具有竞争力。我们的实现、预训练模型和声音样本可以在GitHub上找到。
</details></li>
</ul>
<hr>
<h2 id="Differentially-Flat-Learning-based-Model-Predictive-Control-Using-a-Stability-State-and-Input-Constraining-Safety-Filter"><a href="#Differentially-Flat-Learning-based-Model-Predictive-Control-Using-a-Stability-State-and-Input-Constraining-Safety-Filter" class="headerlink" title="Differentially Flat Learning-based Model Predictive Control Using a Stability, State, and Input Constraining Safety Filter"></a>Differentially Flat Learning-based Model Predictive Control Using a Stability, State, and Input Constraining Safety Filter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10541">http://arxiv.org/abs/2307.10541</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/utiasdsl/fmpc_socp">https://github.com/utiasdsl/fmpc_socp</a></li>
<li>paper_authors: Adam W. Hall, Melissa Greeff, Angela P. Schoellig</li>
<li>for: 控制未知系统 using past trajectory data 和学习的系统动力学模型</li>
<li>methods: 使用学习的非线性动力学模型进行控制，包括使用线性化approximation和非线性优化方法</li>
<li>results: 提出一种新的非线性控制器，通过使用差分平坦性来实现类似于现有学习基于控制器的性能，但具有显著更好的计算效率，同时还能够满足稳定性和输入和平态约束的要求。<details>
<summary>Abstract</summary>
Learning-based optimal control algorithms control unknown systems using past trajectory data and a learned model of the system dynamics. These controllers use either a linear approximation of the learned dynamics, trading performance for faster computation, or nonlinear optimization methods, which typically perform better but can limit real-time applicability. In this work, we present a novel nonlinear controller that exploits differential flatness to achieve similar performance to state-of-the-art learning-based controllers but with significantly less computational effort. Differential flatness is a property of dynamical systems whereby nonlinear systems can be exactly linearized through a nonlinear input mapping. Here, the nonlinear transformation is learned as a Gaussian process and is used in a safety filter that guarantees, with high probability, stability as well as input and flat state constraint satisfaction. This safety filter is then used to refine inputs from a flat model predictive controller to perform constrained nonlinear learning-based optimal control through two successive convex optimizations. We compare our method to state-of-the-art learning-based control strategies and achieve similar performance, but with significantly better computational efficiency, while also respecting flat state and input constraints, and guaranteeing stability.
</details>
<details>
<summary>摘要</summary>
学习基于的优化控制算法控制未知系统使用过去轨迹数据和学习到系统动力学模型。这些控制器使用线性化了学习的动力学模型，换取性能的快速计算，或非线性优化方法，通常表现更好，但可能限制实时应用。在这项工作中，我们介绍了一种新的非线性控制器，利用差分平坦性来实现与当前状态艺学基于学习控制的性能相似，但计算占用 significatively less。差分平坦性是动力系统的属性，其中非线性系统可以通过非线性输入映射来简化为线性系统。在这里，非线性变换是通过 Gaussian 过程学习的，并用于安全筛选器，确保高概率稳定性和输入和平态约束的满足。这个安全筛选器然后用于修改来自平面预测控制器的输入，以进行约束非线性学习optimal控制，通过两个连续的几何优化来完成。我们与当前学习控制策略进行比较，实现相似的性能，但计算效率 significatively better，同时也遵守平态和输入约束，并保证稳定性。
</details></li>
</ul>
<hr>
<h2 id="The-Extractive-Abstractive-Axis-Measuring-Content-“Borrowing”-in-Generative-Language-Models"><a href="#The-Extractive-Abstractive-Axis-Measuring-Content-“Borrowing”-in-Generative-Language-Models" class="headerlink" title="The Extractive-Abstractive Axis: Measuring Content “Borrowing” in Generative Language Models"></a>The Extractive-Abstractive Axis: Measuring Content “Borrowing” in Generative Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11779">http://arxiv.org/abs/2307.11779</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nedelina Teneva</li>
<li>for: 本研究旨在探讨生成模型的抽象性和内容授权问题，并提出了EXTRACTIVE-ABSTRACTIVE轴作为评价生成模型的指标。</li>
<li>methods: 本研究使用了多种生成模型，包括Transformer和Language Model，并对其进行了评价和比较。</li>
<li>results: 研究发现，生成模型的抽象性和内容授权问题存在着挑战，而EXTRACTIVE-ABSTRACTIVE轴可以帮助解决这些问题。<details>
<summary>Abstract</summary>
Generative language models produce highly abstractive outputs by design, in contrast to extractive responses in search engines. Given this characteristic of LLMs and the resulting implications for content Licensing & Attribution, we propose the the so-called Extractive-Abstractive axis for benchmarking generative models and highlight the need for developing corresponding metrics, datasets and annotation guidelines. We limit our discussion to the text modality.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本传输到简化中文。<</SYS>>生成语言模型会生成高度抽象的输出，与搜索引擎的EXTRACTIVE响应不同，这些特点和内容授权、著作权带来了重要的后果。我们提出EXTRACTIVE-ABSTRACTIVE轴作为生成模型的评估标准，并需要开发相应的指标、数据集和注释指南。我们在文本模式下限制我们的讨论。
</details></li>
</ul>
<hr>
<h2 id="Fast-Unsupervised-Deep-Outlier-Model-Selection-with-Hypernetworks"><a href="#Fast-Unsupervised-Deep-Outlier-Model-Selection-with-Hypernetworks" class="headerlink" title="Fast Unsupervised Deep Outlier Model Selection with Hypernetworks"></a>Fast Unsupervised Deep Outlier Model Selection with Hypernetworks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10529">http://arxiv.org/abs/2307.10529</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueying Ding, Yue Zhao, Leman Akoglu</li>
<li>for: 本研究的目的是提出一种有效的深度神经网络基于Outlier Detection（OD）模型选择和hyperparameter（HP）调整方法，以解决现代OD模型中HP的多样性和 validation 问题。</li>
<li>methods: 本研究提出了一种名为HYPER的方法，它通过设计和训练一个新的 hypernetwork（HN），使得HN可以将hyperparameters（HP）映射到OD模型的优化参数上，从而实现高效的HP调整和模型选择。此外，HYPER还使用了meta-学习技术来train一个历史OD任务的标签预测函数，以便高效地进行验证。</li>
<li>results: 在35个OD任务上进行了广泛的实验，结果显示，HYPER可以与8个基elines相比，达到高性能水平，同时具有显著的效率提升。<details>
<summary>Abstract</summary>
Outlier detection (OD) finds many applications with a rich literature of numerous techniques. Deep neural network based OD (DOD) has seen a recent surge of attention thanks to the many advances in deep learning. In this paper, we consider a critical-yet-understudied challenge with unsupervised DOD, that is, effective hyperparameter (HP) tuning/model selection. While several prior work report the sensitivity of OD models to HPs, it becomes ever so critical for the modern DOD models that exhibit a long list of HPs. We introduce HYPER for tuning DOD models, tackling two fundamental challenges: (1) validation without supervision (due to lack of labeled anomalies), and (2) efficient search of the HP/model space (due to exponential growth in the number of HPs). A key idea is to design and train a novel hypernetwork (HN) that maps HPs onto optimal weights of the main DOD model. In turn, HYPER capitalizes on a single HN that can dynamically generate weights for many DOD models (corresponding to varying HPs), which offers significant speed-up. In addition, it employs meta-learning on historical OD tasks with labels to train a proxy validation function, likewise trained with our proposed HN efficiently. Extensive experiments on 35 OD tasks show that HYPER achieves high performance against 8 baselines with significant efficiency gains.
</details>
<details>
<summary>摘要</summary>
OUTLIER DETECTION (OD) 在多个应用程序中找到了广泛的应用，文献中也有丰富的技术。深度神经网络基于的 OUTLIER DETECTION (DOD) 在最近几年内受到了广泛关注，因为深度学习的多种进步。在这篇论文中，我们考虑了一个critical yet understudied challenge，即无监督的 OUTLIER DETECTION 模型选择和参数调整。虽然先前的研究表明了 OUTLIER DETECTION 模型对参数的敏感性，但在现代 DOD 模型中，这种敏感性变得更加重要。我们提出了 HYPER 来调整 DOD 模型，解决两个基本挑战：（1）无监督验证（由于缺乏异常数据标注），（2）高效地搜索参数/模型空间（由于参数的增加）。我们的关键想法是设计并训练一个新的 hypernetwork（HN），将参数映射到 OUTLIER DETECTION 模型的优化参数。然后，HYPER 利用这个 HN 可以动态生成多个 DOD 模型（对应于不同的参数），从而提供了显著的加速。此外，它还利用元学习来训练一个历史 OUTLIER DETECTION 任务的代理验证函数，如wise 训练了我们所提出的 HN，高效地进行了训练。我们的实验表明，HYPER 可以高效地与 8 个基elines 进行比较，并且在 35 个 OUTLIER DETECTION 任务上达到高性能。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Black-Box-Advice-Learning-Augmented-Algorithms-for-MDPs-with-Q-Value-Predictions"><a href="#Beyond-Black-Box-Advice-Learning-Augmented-Algorithms-for-MDPs-with-Q-Value-Predictions" class="headerlink" title="Beyond Black-Box Advice: Learning-Augmented Algorithms for MDPs with Q-Value Predictions"></a>Beyond Black-Box Advice: Learning-Augmented Algorithms for MDPs with Q-Value Predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10524">http://arxiv.org/abs/2307.10524</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tongxin Li, Yiheng Lin, Shaolei Ren, Adam Wierman</li>
<li>for: 该论文研究了在单轨迹时变Markov决策过程（MDP）中的一致性和可靠性之间的贸易。</li>
<li>methods: 该论文使用了Q值建议，并考虑了建议生成过程中的额外信息，以实现更好的性能保证。</li>
<li>results: 研究结果表明，通过利用Q值建议，可以在MDP模型中实现近似优化的性能保证，比黑盒建议更好。<details>
<summary>Abstract</summary>
We study the tradeoff between consistency and robustness in the context of a single-trajectory time-varying Markov Decision Process (MDP) with untrusted machine-learned advice. Our work departs from the typical approach of treating advice as coming from black-box sources by instead considering a setting where additional information about how the advice is generated is available. We prove a first-of-its-kind consistency and robustness tradeoff given Q-value advice under a general MDP model that includes both continuous and discrete state/action spaces. Our results highlight that utilizing Q-value advice enables dynamic pursuit of the better of machine-learned advice and a robust baseline, thus result in near-optimal performance guarantees, which provably improves what can be obtained solely with black-box advice.
</details>
<details>
<summary>摘要</summary>
我们研究了在单轨时间变化Markov决策过程（MDP）中的一致性和可靠性之间的贸易做。我们的工作与 Typical approach不同，不再将建议视为黑盒来源，而是考虑一种情况，在建议生成的additional information可以获得。我们证明了一种首次的一致性和可靠性贸易，基于Q值建议在通用MDP模型中，该模型包括连续和离散状态/动作空间。我们的结果表明，通过利用Q值建议，可以动态追求机器学习建议和可靠基线之间的最佳选择，从而实现近似最佳性能保证，这超越了单纯黑盒建议所能获得的性能。
</details></li>
</ul>
<hr>
<h2 id="Prediction-of-Handball-Matches-with-Statistically-Enhanced-Learning-via-Estimated-Team-Strengths"><a href="#Prediction-of-Handball-Matches-with-Statistically-Enhanced-Learning-via-Estimated-Team-Strengths" class="headerlink" title="Prediction of Handball Matches with Statistically Enhanced Learning via Estimated Team Strengths"></a>Prediction of Handball Matches with Statistically Enhanced Learning via Estimated Team Strengths</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11777">http://arxiv.org/abs/2307.11777</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florian Felice, Christophe Ley</li>
<li>for: 这个论文是为了预测手球赛事而写的。</li>
<li>methods: 该论文使用了统计学加强学习（SEL）模型来预测手球赛事。</li>
<li>results: 论文中的机器学习模型，通过添加SEL特征，实现了超过80%的准确率。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
We propose a Statistically Enhanced Learning (aka. SEL) model to predict handball games. Our Machine Learning model augmented with SEL features outperforms state-of-the-art models with an accuracy beyond 80%. In this work, we show how we construct the data set to train Machine Learning models on past female club matches. We then compare different models and evaluate them to assess their performance capabilities. Finally, explainability methods allow us to change the scope of our tool from a purely predictive solution to a highly insightful analytical tool. This can become a valuable asset for handball teams' coaches providing valuable statistical and predictive insights to prepare future competitions.
</details>
<details>
<summary>摘要</summary>
我们提出一种统计增强学习（简称 SEL）模型，用于预测手球比赛。我们的机器学习模型，通过添加 SEL 特征，超过了当前最佳模型的准确率，达到了80%以上。在这个工作中，我们介绍了如何使用过去女子俱乐部比赛数据来训练机器学习模型。然后，我们比较了不同的模型，并评估了它们的性能能力。最后，我们使用可解释方法，将我们的工具从一个纯Predictive解决方案转变成一个具有高度探索性的分析工具，这可以成为手球队教练的价值观点，提供有价值的统计和预测信息，以准备未来的竞赛。
</details></li>
</ul>
<hr>
<h2 id="FedSoup-Improving-Generalization-and-Personalization-in-Federated-Learning-via-Selective-Model-Interpolation"><a href="#FedSoup-Improving-Generalization-and-Personalization-in-Federated-Learning-via-Selective-Model-Interpolation" class="headerlink" title="FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation"></a>FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10507">http://arxiv.org/abs/2307.10507</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minghui Chen, Meirui Jiang, Qi Dou, Zehua Wang, Xiaoxiao Li</li>
<li>for: 这篇论文旨在解决跨档案联合学习（FL）中的分布差异问题，尤其是对于专案化FL方法的适应性。</li>
<li>methods: 我们提出了一种新的联合模型汤（FedSoup），通过选择性地 interpolating模型参数来优化本地和全球性能的贸易。</li>
<li>results: 我们在静脉和病理图像分类任务上评估了我们的提案，并获得了明显的增进外部数据适应性。我们的代码可以在<a target="_blank" rel="noopener" href="https://github.com/ubc-tea/FedSoup%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ubc-tea/FedSoup中找到。</a><details>
<summary>Abstract</summary>
Cross-silo federated learning (FL) enables the development of machine learning models on datasets distributed across data centers such as hospitals and clinical research laboratories. However, recent research has found that current FL algorithms face a trade-off between local and global performance when confronted with distribution shifts. Specifically, personalized FL methods have a tendency to overfit to local data, leading to a sharp valley in the local model and inhibiting its ability to generalize to out-of-distribution data. In this paper, we propose a novel federated model soup method (i.e., selective interpolation of model parameters) to optimize the trade-off between local and global performance. Specifically, during the federated training phase, each client maintains its own global model pool by monitoring the performance of the interpolated model between the local and global models. This allows us to alleviate overfitting and seek flat minima, which can significantly improve the model's generalization performance. We evaluate our method on retinal and pathological image classification tasks, and our proposed method achieves significant improvements for out-of-distribution generalization. Our code is available at https://github.com/ubc-tea/FedSoup.
</details>
<details>
<summary>摘要</summary>
（简体中文）跨存储 silo 联合学习（FL）可以在数据中心如医院和临床研究室等处开发机器学习模型。然而，当面临分布偏移时，现有的 FL 算法会面临本地和全球性能之间的负担。具体来说，个性化 FL 方法往往会过拟合本地数据，导致本地模型峰值陡峰，阻碍其对于不同于本地数据的泛化性能。在这篇论文中，我们提出了一种新的联合模型汤（i.e., 选择性 interpolate 模型参数）来优化本地和全球性能之间的负担。具体来说，在联合训练阶段，每个客户端都会维护自己的全球模型池，并通过监测 interpolated 模型在本地和全球模型之间的性能来缓解过拟合。这可以有效地提高模型的泛化性能。我们在Retinal和pathological图像分类任务上评估了我们的方法，并发现我们的方法在不同于本地数据的泛化性能上具有显著改善。我们的代码可以在 https://github.com/ubc-tea/FedSoup 上找到。
</details></li>
</ul>
<hr>
<h2 id="Identifying-Interpretable-Subspaces-in-Image-Representations"><a href="#Identifying-Interpretable-Subspaces-in-Image-Representations" class="headerlink" title="Identifying Interpretable Subspaces in Image Representations"></a>Identifying Interpretable Subspaces in Image Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10504">http://arxiv.org/abs/2307.10504</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neha Kalibhat, Shweta Bhardwaj, Bayan Bruss, Hamed Firooz, Maziar Sanjabi, Soheil Feizi</li>
<li>for: 本文提出了一种自动Feature explanation的方法，用于解释图像表示的特征。</li>
<li>methods: 该方法使用了对比概念来描述目标特征的高度活跃图像，并使用了预训练的视觉语言模型like CLIP来生成描述。每个词在描述中得分和排名，从而导致一小组高度相似的人类可理解的概念，用于解释目标特征。</li>
<li>results: 该方法可以解释大多数现有模型中的特征，并且可以减少干扰特征。此外，该方法还可以在不同的表示空间之间传递概念。<details>
<summary>Abstract</summary>
We propose Automatic Feature Explanation using Contrasting Concepts (FALCON), an interpretability framework to explain features of image representations. For a target feature, FALCON captions its highly activating cropped images using a large captioning dataset (like LAION-400m) and a pre-trained vision-language model like CLIP. Each word among the captions is scored and ranked leading to a small number of shared, human-understandable concepts that closely describe the target feature. FALCON also applies contrastive interpretation using lowly activating (counterfactual) images, to eliminate spurious concepts. Although many existing approaches interpret features independently, we observe in state-of-the-art self-supervised and supervised models, that less than 20% of the representation space can be explained by individual features. We show that features in larger spaces become more interpretable when studied in groups and can be explained with high-order scoring concepts through FALCON. We discuss how extracted concepts can be used to explain and debug failures in downstream tasks. Finally, we present a technique to transfer concepts from one (explainable) representation space to another unseen representation space by learning a simple linear transformation.
</details>
<details>
<summary>摘要</summary>
我们提出了自动Feature解释使用对比概念（FALCON），一种可解性框架，用于解释图像表示中的特征。为目标特征，FALCON使用大量captioningdataset（如LAION-400m）和预训练的视觉语言模型（如CLIP）来caption高活跃的剪辑图像。每个单词在caption中被分数和排名，导致一小数量的共享、人类可理解的概念，准确地描述目标特征。FALCON还使用对比解释使用低活跃（counterfactual）图像，以消除假设性的概念。我们发现，许多现有的方法单独解释特征，但是我们在当前领域的自动驱动和监督模型中发现，只有 less than 20%的表示空间可以由单个特征解释。我们表明，在更大的表示空间中，特征在组合上变得更容易理解，可以通过FALCON高序分数概念来解释。我们讨论如何使用提取的概念来解释和调试下游任务中的失败。最后，我们提出了一种将概念从一个可解性表示空间传播到另一个未经见的表示空间的技术，通过学习简单的线性变换。
</details></li>
</ul>
<hr>
<h2 id="A-Competitive-Learning-Approach-for-Specialized-Models-A-Solution-for-Complex-Physical-Systems-with-Distinct-Functional-Regimes"><a href="#A-Competitive-Learning-Approach-for-Specialized-Models-A-Solution-for-Complex-Physical-Systems-with-Distinct-Functional-Regimes" class="headerlink" title="A Competitive Learning Approach for Specialized Models: A Solution for Complex Physical Systems with Distinct Functional Regimes"></a>A Competitive Learning Approach for Specialized Models: A Solution for Complex Physical Systems with Distinct Functional Regimes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10496">http://arxiv.org/abs/2307.10496</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/exploita123/charmedforfree">https://github.com/exploita123/charmedforfree</a></li>
<li>paper_authors: Okezzi F. Ukorigho, Opeoluwa Owoyele</li>
<li>for: 本研究旨在提出一种新的竞争学习方法，用于从数据中提取物理系统的数据驱动模型。</li>
<li>methods: 该方法employs动态损失函数，用于同时训练一组模型。每个模型在训练过程中竞争对每个观察数据，以便在数据中identify不同的功能 режи。</li>
<li>results: 研究表明，该方法能够成功地identify功能regime，发现真实的管理方程，并降低测试错误。<details>
<summary>Abstract</summary>
Complex systems in science and engineering sometimes exhibit behavior that changes across different regimes. Traditional global models struggle to capture the full range of this complex behavior, limiting their ability to accurately represent the system. In response to this challenge, we propose a novel competitive learning approach for obtaining data-driven models of physical systems. The primary idea behind the proposed approach is to employ dynamic loss functions for a set of models that are trained concurrently on the data. Each model competes for each observation during training, allowing for the identification of distinct functional regimes within the dataset. To demonstrate the effectiveness of the learning approach, we coupled it with various regression methods that employ gradient-based optimizers for training. The proposed approach was tested on various problems involving model discovery and function approximation, demonstrating its ability to successfully identify functional regimes, discover true governing equations, and reduce test errors.
</details>
<details>
<summary>摘要</summary>
科学和工程中的复杂系统ometimes会展现不同的行为方式，传统的全球模型难以捕捉这些复杂行为的全范围。为了解决这个挑战，我们提议一种新的竞争学习方法，通过在数据上同时训练一组模型，使得每个模型在训练过程中竞争对每个观察结果。这种方法可以识别数据集中的不同功能模式，并通过使用动态损失函数，使模型在不同的训练过程中产生更好的性能。为了证明该学习方法的有效性，我们将其与多种回归方法结合使用，其中包括使用梯度基本优化器进行训练。我们在各种模型发现和函数近似问题上测试了该方法，并证明了其能够成功地识别功能模式、发现真正的管理方程和降低测试错误。
</details></li>
</ul>
<hr>
<h2 id="Novel-Batch-Active-Learning-Approach-and-Its-Application-to-Synthetic-Aperture-Radar-Datasets"><a href="#Novel-Batch-Active-Learning-Approach-and-Its-Application-to-Synthetic-Aperture-Radar-Datasets" class="headerlink" title="Novel Batch Active Learning Approach and Its Application to Synthetic Aperture Radar Datasets"></a>Novel Batch Active Learning Approach and Its Application to Synthetic Aperture Radar Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10495">http://arxiv.org/abs/2307.10495</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chapman20j/sar_bal">https://github.com/chapman20j/sar_bal</a></li>
<li>paper_authors: James Chapman, Bohan Chen, Zheng Tan, Jeff Calder, Kevin Miller, Andrea L. Bertozzi</li>
<li>for: 这个论文旨在提高机器学习方法的性能，通过选择有限数量的无标的数据点进行活动学习，以最大化基础分类器的性能。</li>
<li>methods: 本论文使用了序列活动学习和批量活动学习两种方法，其中序列活动学习每次选择一个问题集，而批量活动学习则选择多个数据点进行查询。</li>
<li>results: 本论文的结果显示，使用了Dijkstra的 Annulus Core-Set（DAC）和LocalMax的两种方法可以实现高度的精度和效率，并且可以超过现有的CNN方法。<details>
<summary>Abstract</summary>
Active learning improves the performance of machine learning methods by judiciously selecting a limited number of unlabeled data points to query for labels, with the aim of maximally improving the underlying classifier's performance. Recent gains have been made using sequential active learning for synthetic aperture radar (SAR) data arXiv:2204.00005. In each iteration, sequential active learning selects a query set of size one while batch active learning selects a query set of multiple datapoints. While batch active learning methods exhibit greater efficiency, the challenge lies in maintaining model accuracy relative to sequential active learning methods. We developed a novel, two-part approach for batch active learning: Dijkstra's Annulus Core-Set (DAC) for core-set generation and LocalMax for batch sampling. The batch active learning process that combines DAC and LocalMax achieves nearly identical accuracy as sequential active learning but is more efficient, proportional to the batch size. As an application, a pipeline is built based on transfer learning feature embedding, graph learning, DAC, and LocalMax to classify the FUSAR-Ship and OpenSARShip datasets. Our pipeline outperforms the state-of-the-art CNN-based methods.
</details>
<details>
<summary>摘要</summary>
活动学习可以提高机器学习方法的性能，通过选择有限数量的无标签数据点进行查询，以最大化下游分类器的性能。最近，在Synthetic Aperture Radar（SAR）数据上使用Sequential Active Learning（SAL）已经得到了进步。在每次迭代中，SAL选择一个查询集合，而批处活动学习选择多个数据点。虽然批处活动学习方法更加高效，但是维护模型准确性的挑战在于Sequential Active Learning方法。我们提出了一种新的、两部分的批处活动学习方法：Dijkstra的Annulus Core-Set（DAC）用于核心集生成和LocalMax用于批处采样。将DAC和LocalMax结合使用的批处活动学习过程可以与Sequential Active Learning方法准确相同，但是更高效，与批处大小成正比。我们使用了基于传输学习特征嵌入、图学习、DAC和LocalMax来分类FUSAR-Ship和OpenSARShip数据集。我们的管道超过了当前Convolutional Neural Network（CNN）基于方法的状态。
</details></li>
</ul>
<hr>
<h2 id="Blockchain-Based-Federated-Learning-Incentivizing-Data-Sharing-and-Penalizing-Dishonest-Behavior"><a href="#Blockchain-Based-Federated-Learning-Incentivizing-Data-Sharing-and-Penalizing-Dishonest-Behavior" class="headerlink" title="Blockchain-Based Federated Learning: Incentivizing Data Sharing and Penalizing Dishonest Behavior"></a>Blockchain-Based Federated Learning: Incentivizing Data Sharing and Penalizing Dishonest Behavior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10492">http://arxiv.org/abs/2307.10492</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amir Jaberzadeh, Ajay Kumar Shrestha, Faijan Ahamad Khan, Mohammed Afaan Shaikh, Bhargav Dave, Jason Geng</li>
<li>for: 这种方法是为了提高聚合学习模型的准确率而确保数据分享的安全性和公正性。</li>
<li>methods: 该方法使用了InterPlanetary File System、区块链和智能合约来实现安全和互惠的数据分享，并提供了激励机制、访问控制机制和惩戒任何不诚实行为。</li>
<li>results: 实验结果表明，提议的模型可以提高聚合学习模型的准确率，同时保证数据分享过程中的安全性和公正性。此外，该研究还实现了一个基于区块链技术的分布式学习平台，可以同时训练多个工作者的模型，保持数据隐私和安全性。<details>
<summary>Abstract</summary>
With the increasing importance of data sharing for collaboration and innovation, it is becoming more important to ensure that data is managed and shared in a secure and trustworthy manner. Data governance is a common approach to managing data, but it faces many challenges such as data silos, data consistency, privacy, security, and access control. To address these challenges, this paper proposes a comprehensive framework that integrates data trust in federated learning with InterPlanetary File System, blockchain, and smart contracts to facilitate secure and mutually beneficial data sharing while providing incentives, access control mechanisms, and penalizing any dishonest behavior. The experimental results demonstrate that the proposed model is effective in improving the accuracy of federated learning models while ensuring the security and fairness of the data-sharing process. The research paper also presents a decentralized federated learning platform that successfully trained a CNN model on the MNIST dataset using blockchain technology. The platform enables multiple workers to train the model simultaneously while maintaining data privacy and security. The decentralized architecture and use of blockchain technology allow for efficient communication and coordination between workers. This platform has the potential to facilitate decentralized machine learning and support privacy-preserving collaboration in various domains.
</details>
<details>
<summary>摘要</summary>
随着数据共享的重要性增加，保证数据的安全和可靠性变得更加重要。数据治理是一种常见的数据管理方法，但它面临着数据孤岛、数据一致性、隐私、安全和访问控制等挑战。为了解决这些挑战，这篇论文提出了一个完整的框架，将数据信任integrated into federated learning with InterPlanetary File System, blockchain, and smart contracts，以便在安全和公正的数据共享条件下进行安全和互利的数据共享，并提供了激励、访问控制机制和惩戒任何不诚实的行为。实验结果表明，提出的模型能够提高 federated learning 模型的准确率，同时保证数据共享过程的安全和公正性。研究论文还描述了一个基于区块链技术的分布式 federated learning 平台，可以同时让多个工作者在维护数据隐私和安全的前提下，共同训练 CNN 模型。该平台的分布式架构和使用区块链技术，可以减少工作者之间的通信和协调成本，并且具有潜在的分布式机器学习和隐私保护的应用前景。
</details></li>
</ul>
<hr>
<h2 id="Ab-using-Images-and-Sounds-for-Indirect-Instruction-Injection-in-Multi-Modal-LLMs"><a href="#Ab-using-Images-and-Sounds-for-Indirect-Instruction-Injection-in-Multi-Modal-LLMs" class="headerlink" title="(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs"></a>(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10490">http://arxiv.org/abs/2307.10490</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ebagdasa/multimodal_injection">https://github.com/ebagdasa/multimodal_injection</a></li>
<li>paper_authors: Eugene Bagdasaryan, Tsung-Yin Hsieh, Ben Nassi, Vitaly Shmatikov</li>
<li>for: 这篇论文描述了如何使用图像和声音来实现多模态语言模型中的恶意提示和指令注入攻击。</li>
<li>methods: 攻击者可以生成对应于提示的恶意扰动，并将其混合到图像或音频记录中。当用户问问 benign 模型关于扰动后的图像或音频时，扰动会导致模型输出攻击者选择的文本和&#x2F;或使Subsequent dialog follow the attacker’s instruction。</li>
<li>results: 作者通过证明了这种攻击的可行性，并提供了一些证明例targeting LLaVa和PandaGPT。<details>
<summary>Abstract</summary>
We demonstrate how images and sounds can be used for indirect prompt and instruction injection in multi-modal LLMs. An attacker generates an adversarial perturbation corresponding to the prompt and blends it into an image or audio recording. When the user asks the (unmodified, benign) model about the perturbed image or audio, the perturbation steers the model to output the attacker-chosen text and/or make the subsequent dialog follow the attacker's instruction. We illustrate this attack with several proof-of-concept examples targeting LLaVa and PandaGPT.
</details>
<details>
<summary>摘要</summary>
我们展示了图像和声音可以用于多modal LLMS中的间接提示和指令注入攻击。攻击者创建了对提示的攻击偏移，然后与图像或音频录制中混合。当用户对未修改的模型询问这个偏移后的图像或音频时，攻击偏移将模型规律输出攻击者选择的文本和/或让后续对话按照攻击者的指令继续。我们透过多个证明例Targeting LLaVa和PandaGPT详细介绍这一攻击。
</details></li>
</ul>
<hr>
<h2 id="SPRINT-A-Unified-Toolkit-for-Evaluating-and-Demystifying-Zero-shot-Neural-Sparse-Retrieval"><a href="#SPRINT-A-Unified-Toolkit-for-Evaluating-and-Demystifying-Zero-shot-Neural-Sparse-Retrieval" class="headerlink" title="SPRINT: A Unified Toolkit for Evaluating and Demystifying Zero-shot Neural Sparse Retrieval"></a>SPRINT: A Unified Toolkit for Evaluating and Demystifying Zero-shot Neural Sparse Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10488">http://arxiv.org/abs/2307.10488</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thakur-nandan/sprint">https://github.com/thakur-nandan/sprint</a></li>
<li>paper_authors: Nandan Thakur, Kexin Wang, Iryna Gurevych, Jimmy Lin</li>
<li>for: 这个论文的目的是提供一个基于 Pyserini 和 Lucene 的 Python 工具集 (SPRINT)，用于评估神经稀采推荐系统。</li>
<li>methods: 这个工具集包括五种内置的模型：uniCOIL、DeepImpact、SPARTA、TILDEv2 和 SPLADEv2。用户也可以轻松地添加自己定制的模型，只需要定义权重方法。</li>
<li>results: 使用 SPRINT 工具集，我们在 BEIR 上建立了强大和可重复的零shot 稀采推荐基准。我们的结果显示，SPLADEv2 在 BEIR 上得到了最高的平均得分 0.470 nDCG@10。此外，我们还发现 SPLADEv2 生成的稀采表示性中有一部分 tokens 位于原始查询和文档之外，这经常是其性能提升的原因。<details>
<summary>Abstract</summary>
Traditionally, sparse retrieval systems relied on lexical representations to retrieve documents, such as BM25, dominated information retrieval tasks. With the onset of pre-trained transformer models such as BERT, neural sparse retrieval has led to a new paradigm within retrieval. Despite the success, there has been limited software supporting different sparse retrievers running in a unified, common environment. This hinders practitioners from fairly comparing different sparse models and obtaining realistic evaluation results. Another missing piece is, that a majority of prior work evaluates sparse retrieval models on in-domain retrieval, i.e. on a single dataset: MS MARCO. However, a key requirement in practical retrieval systems requires models that can generalize well to unseen out-of-domain, i.e. zero-shot retrieval tasks. In this work, we provide SPRINT, a unified Python toolkit based on Pyserini and Lucene, supporting a common interface for evaluating neural sparse retrieval. The toolkit currently includes five built-in models: uniCOIL, DeepImpact, SPARTA, TILDEv2 and SPLADEv2. Users can also easily add customized models by defining their term weighting method. Using our toolkit, we establish strong and reproducible zero-shot sparse retrieval baselines across the well-acknowledged benchmark, BEIR. Our results demonstrate that SPLADEv2 achieves the best average score of 0.470 nDCG@10 on BEIR amongst all neural sparse retrievers. In this work, we further uncover the reasons behind its performance gain. We show that SPLADEv2 produces sparse representations with a majority of tokens outside of the original query and document which is often crucial for its performance gains, i.e. a limitation among its other sparse counterparts. We provide our SPRINT toolkit, models, and data used in our experiments publicly here at https://github.com/thakur-nandan/sprint.
</details>
<details>
<summary>摘要</summary>
传统上，稀疏检索系统通常使用lexical表示来检索文档，如BM25，控制了检索任务。随着预训练变换器模型如BERT的出现，神经稀疏检索引入了一种新的 paradigm within检索。 despite the success， there has been limited software supporting different sparse retrievers running in a unified, common environment。This hinders practitioners from fairly comparing different sparse models and obtaining realistic evaluation results。Another missing piece is that a majority of prior work evaluates sparse retrieval models on in-domain retrieval，i.e. on a single dataset：MS MARCO。However， a key requirement in practical retrieval systems requires models that can generalize well to unseen out-of-domain，i.e. zero-shot retrieval tasks。In this work，we provide SPRINT，a unified Python toolkit based on Pyserini and Lucene，supporting a common interface for evaluating neural sparse retrieval。The toolkit currently includes five built-in models：uniCOIL，DeepImpact，SPARTA，TILDEv2 and SPLADEv2。Users can also easily add customized models by defining their term weighting method。Using our toolkit，we establish strong and reproducible zero-shot sparse retrieval baselines across the well-acknowledged benchmark，BEIR。Our results demonstrate that SPLADEv2 achieves the best average score of 0.470 nDCG@10 on BEIR amongst all neural sparse retrievers。In this work，we further uncover the reasons behind its performance gain。We show that SPLADEv2 produces sparse representations with a majority of tokens outside of the original query and document，which is often crucial for its performance gains，i.e. a limitation among its other sparse counterparts。We provide our SPRINT toolkit，models，and data used in our experiments publicly here at <https://github.com/thakur-nandan/sprint>.
</details></li>
</ul>
<hr>
<h2 id="FinGPT-Democratizing-Internet-scale-Data-for-Financial-Large-Language-Models"><a href="#FinGPT-Democratizing-Internet-scale-Data-for-Financial-Large-Language-Models" class="headerlink" title="FinGPT: Democratizing Internet-scale Data for Financial Large Language Models"></a>FinGPT: Democratizing Internet-scale Data for Financial Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10485">http://arxiv.org/abs/2307.10485</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai4finance-foundation/fingpt">https://github.com/ai4finance-foundation/fingpt</a></li>
<li>paper_authors: Xiao-Yang Liu, Guoxuan Wang, Daochen Zha</li>
<li>For: FinGPT aims to democratize Internet-scale financial data for large language models (LLMs) to stimulate innovation and unlock new opportunities in open finance.* Methods: FinGPT is an open-sourced and data-centric framework that automates the collection and curation of real-time financial data from &gt;34 diverse sources on the Internet, and provides researchers and practitioners with accessible and transparent resources to develop their FinLLMs. Additionally, the paper proposes a simple yet effective strategy for fine-tuning FinLLM using the inherent feedback from the market, dubbed Reinforcement Learning with Stock Prices (RLSP).* Results: The paper showcases several FinGPT applications, including robo-advisor, sentiment analysis for algorithmic trading, and low-code development.<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated remarkable proficiency in understanding and generating human-like texts, which may potentially revolutionize the finance industry. However, existing LLMs often fall short in the financial field, which is mainly attributed to the disparities between general text data and financial text data. Unfortunately, there is only a limited number of financial text datasets available (quite small size), and BloombergGPT, the first financial LLM (FinLLM), is close-sourced (only the training logs were released). In light of this, we aim to democratize Internet-scale financial data for LLMs, which is an open challenge due to diverse data sources, low signal-to-noise ratio, and high time-validity. To address the challenges, we introduce an open-sourced and data-centric framework, \textit{Financial Generative Pre-trained Transformer (FinGPT)}, that automates the collection and curation of real-time financial data from >34 diverse sources on the Internet, providing researchers and practitioners with accessible and transparent resources to develop their FinLLMs. Additionally, we propose a simple yet effective strategy for fine-tuning FinLLM using the inherent feedback from the market, dubbed Reinforcement Learning with Stock Prices (RLSP). We also adopt the Low-rank Adaptation (LoRA, QLoRA) method that enables users to customize their own FinLLMs from open-source general-purpose LLMs at a low cost. Finally, we showcase several FinGPT applications, including robo-advisor, sentiment analysis for algorithmic trading, and low-code development. FinGPT aims to democratize FinLLMs, stimulate innovation, and unlock new opportunities in open finance. The codes are available at https://github.com/AI4Finance-Foundation/FinGPT and https://github.com/AI4Finance-Foundation/FinNLP
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经表现出杰出的人工智能能力，可能将改变金融业。然而，现有的LLM通常在金融领域下表现不佳，主要是因为通用文本数据和金融文本数据之间存在差异。实际上，金融领域的数据相对较少，而BloombergGPT，首个金融LLM（FinLLM），则是封存的（仅公开训练记录）。为了普及互联网范围内的金融数据，并且让研究者和实践者可以轻松地发展自己的FinLLM，我们提出了一个开源和数据中心的框架，名为“金融生成预训练transformer”（FinGPT）。FinGPT自动收集和整理互联网上的多种金融数据，提供了可 accessible 和透明的资源，以便研究者和实践者可以轻松地发展自己的FinLLM。此外，我们提出了一种简单 yet有效的策略，使用市场内的反馈来调整FinLLM，名为“股票价格反馈学习”（RLSP）。我们还采用了低维度适应（LoRA，QLoRA）方法，让用户可以从开源通用语言模型中自定义自己的FinLLM，而不需要高成本。最后，我们展示了多个FinGPT应用，包括智能投资、基于算法传统交易的情感分析、以及低程式化开发。FinGPT愿望普及FinLLM，刺激创新，解锁开放金融的新机会。代码可以在https://github.com/AI4Finance-Foundation/FinGPT和https://github.com/AI4Finance-Foundation/FinNLP获取。
</details></li>
</ul>
<hr>
<h2 id="Can-Instruction-Fine-Tuned-Language-Models-Identify-Social-Bias-through-Prompting"><a href="#Can-Instruction-Fine-Tuned-Language-Models-Identify-Social-Bias-through-Prompting" class="headerlink" title="Can Instruction Fine-Tuned Language Models Identify Social Bias through Prompting?"></a>Can Instruction Fine-Tuned Language Models Identify Social Bias through Prompting?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10472">http://arxiv.org/abs/2307.10472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omkar Dige, Jacob-Junqi Tian, David Emerson, Faiza Khan Khattak</li>
<li>for: 这个论文目的是评估语言模型中的偏见，以及如何通过零批示来评估这些偏见。</li>
<li>methods: 这个论文使用的方法是使用链条思维（Chain-of-Thought）提问来评估语言模型的偏见识别能力。</li>
<li>results: 研究发现，使用Alpaca 7B模型，在偏见识别任务中获得56.7%的准确率，而 scaling up LLM 大小和数据多样性可能会导致更高的性能提升。<details>
<summary>Abstract</summary>
As the breadth and depth of language model applications continue to expand rapidly, it is increasingly important to build efficient frameworks for measuring and mitigating the learned or inherited social biases of these models. In this paper, we present our work on evaluating instruction fine-tuned language models' ability to identify bias through zero-shot prompting, including Chain-of-Thought (CoT) prompts. Across LLaMA and its two instruction fine-tuned versions, Alpaca 7B performs best on the bias identification task with an accuracy of 56.7%. We also demonstrate that scaling up LLM size and data diversity could lead to further performance gain. This is a work-in-progress presenting the first component of our bias mitigation framework. We will keep updating this work as we get more results.
</details>
<details>
<summary>摘要</summary>
为应用语言模型的关系范围和深度不断扩大，现在越来越重要建立高效的测试和减少社会偏见的框架。在这篇论文中，我们介绍了使用零批训练语言模型识别偏见的能力，包括链接思维（CoT）提示。在LLaMA和其两个受训版本中，Alpaca 7B在偏见识别任务上表现最好，具体成绩为56.7%。我们还证明了将LLM大小和数据多样性扩大可以带来更大的性能提升。这是我们的偏见 Mitigation 框架的第一个 ком成分，我们将继续更新这个工作，当我们获得更多结果时。
</details></li>
</ul>
<hr>
<h2 id="Classification-of-Visualization-Types-and-Perspectives-in-Patents"><a href="#Classification-of-Visualization-Types-and-Perspectives-in-Patents" class="headerlink" title="Classification of Visualization Types and Perspectives in Patents"></a>Classification of Visualization Types and Perspectives in Patents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10471">http://arxiv.org/abs/2307.10471</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tibhannover/patentimageclassification">https://github.com/tibhannover/patentimageclassification</a></li>
<li>paper_authors: Junaid Ahmed Ghauri, Eric Müller-Budack, Ralph Ewerth</li>
<li>For: The paper is written for researchers and developers working on information and multimedia retrieval for patents, as well as those interested in deep learning approaches for image classification.* Methods: The paper uses state-of-the-art deep learning methods, including transformers, for the classification of visualization types and perspectives in patent images.* Results: The proposed approaches have been demonstrated to be feasible through experimental results, and the authors plan to make the source code, models, and dataset publicly available.Here’s the simplified Chinese version of the three key points:</li>
<li>for: 该论文是为了帮助信息和多媒体检索领域的研究人员和开发人员，以及关注深度学习方法的人员。</li>
<li>methods: 该论文使用当前最佳的深度学习方法，包括转换器，来分类专利图像中的视觉类型和视角。</li>
<li>results: 提出的方法已经实验表明其可行性，作者计划将源代码、模型和数据集公开发布。<details>
<summary>Abstract</summary>
Due to the swift growth of patent applications each year, information and multimedia retrieval approaches that facilitate patent exploration and retrieval are of utmost importance. Different types of visualizations (e.g., graphs, technical drawings) and perspectives (e.g., side view, perspective) are used to visualize details of innovations in patents. The classification of these images enables a more efficient search and allows for further analysis. So far, datasets for image type classification miss some important visualization types for patents. Furthermore, related work does not make use of recent deep learning approaches including transformers. In this paper, we adopt state-of-the-art deep learning methods for the classification of visualization types and perspectives in patent images. We extend the CLEF-IP dataset for image type classification in patents to ten classes and provide manual ground truth annotations. In addition, we derive a set of hierarchical classes from a dataset that provides weakly-labeled data for image perspectives. Experimental results have demonstrated the feasibility of the proposed approaches. Source code, models, and dataset will be made publicly available.
</details>
<details>
<summary>摘要</summary>
In this paper, we employ state-of-the-art deep learning methods for the classification of visualization types and perspectives in patent images. We expand the CLEF-IP dataset for image type classification in patents to ten classes and provide manual ground truth annotations. Additionally, we derive a set of hierarchical classes from a dataset that provides weakly-labeled data for image perspectives. Our experimental results demonstrate the feasibility of the proposed approaches. The source code, models, and dataset will be publicly available.
</details></li>
</ul>
<hr>
<h2 id="Properties-of-Discrete-Sliced-Wasserstein-Losses"><a href="#Properties-of-Discrete-Sliced-Wasserstein-Losses" class="headerlink" title="Properties of Discrete Sliced Wasserstein Losses"></a>Properties of Discrete Sliced Wasserstein Losses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10352">http://arxiv.org/abs/2307.10352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eloi Tanguy, Rémi Flamary, Julie Delon</li>
<li>for: 这个论文主要研究了块 Wasserstein 距离（SW）作为比较概率分布的一种代替方法，特别是在图像处理、领域适应和生成模型中，where it is common to optimize some parameters to minimize SW, which serves as a loss function between discrete probability measures.</li>
<li>methods: 本文研究了 $\mathcal{E}: Y \longmapsto \text{SW}_2^2(\gamma_Y, \gamma_Z)$ 的性质和优化属性，其中 $\gamma_Y$ 和 $\gamma_Z$ 是两个同量的离散概率分布。</li>
<li>results: 研究结果显示，$\mathcal{E}_p$ 的迪克梯度法和 $\mathcal{E}$ 的迪克梯度法都可以减少 $\mathcal{E}$ 的极值，并且在某种意义上，这些方法会 converge towards (Clarke)  критиче点。<details>
<summary>Abstract</summary>
The Sliced Wasserstein (SW) distance has become a popular alternative to the Wasserstein distance for comparing probability measures. Widespread applications include image processing, domain adaptation and generative modelling, where it is common to optimise some parameters in order to minimise SW, which serves as a loss function between discrete probability measures (since measures admitting densities are numerically unattainable). All these optimisation problems bear the same sub-problem, which is minimising the Sliced Wasserstein energy. In this paper we study the properties of $\mathcal{E}: Y \longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z)$, i.e. the SW distance between two uniform discrete measures with the same amount of points as a function of the support $Y \in \mathbb{R}^{n \times d}$ of one of the measures. We investigate the regularity and optimisation properties of this energy, as well as its Monte-Carlo approximation $\mathcal{E}_p$ (estimating the expectation in SW using only $p$ samples) and show convergence results on the critical points of $\mathcal{E}_p$ to those of $\mathcal{E}$, as well as an almost-sure uniform convergence. Finally, we show that in a certain sense, Stochastic Gradient Descent methods minimising $\mathcal{E}$ and $\mathcal{E}_p$ converge towards (Clarke) critical points of these energies.
</details>
<details>
<summary>摘要</summary>
“划分 Wasserstein（SW）距离已成为比 Wasserstein 距离更受欢迎的选择，用于比较概率分布。它在图像处理、领域适应和生成模型中具有广泛的应用，例如最小化 SW 作为概率分布之间的损失函数。这些优化问题都有同一个子问题，即寻找 SW 能量的最小值。在这篇文章中，我们研究了 $\mathcal{E}：Y \longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z)$ 的性质，其中 $\gamma_Y$ 和 $\gamma_Z$ 是两个具有相同数量点的不变概率分布。我们调查了这个能量的规律和优化性，以及其 Monte-Carlo 预测 $\mathcal{E}_p$（使用 $p$ 样本估计 SW 的期望值）的数值和数值算法，并证明了这些点的对应点的对应点的数值和 almost-sure uniform 收敛。最后，我们显示了在某种意义上，使用 Stochastic Gradient Descent 方法对 $\mathcal{E}$ 和 $\mathcal{E}_p$ 进行优化将导向（Clarke）的极值点。”
</details></li>
</ul>
<hr>
<h2 id="A-data-science-axiology-the-nature-value-and-risks-of-data-science"><a href="#A-data-science-axiology-the-nature-value-and-risks-of-data-science" class="headerlink" title="A data science axiology: the nature, value, and risks of data science"></a>A data science axiology: the nature, value, and risks of data science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10460">http://arxiv.org/abs/2307.10460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael L. Brodie</li>
<li>for: 本文提出了数据科学的axiology，即其目的、性质、重要性、风险和价值，以帮助理解和定义数据科学，并承认其潜在的利益和风险。</li>
<li>methods: 本文使用了AXIOLOGY的方法来探索和评估数据科学的特点，包括对数据科学的定义、特点、优势和风险的分析。</li>
<li>results: 本文的结果表明，数据科学是一种在不可预测的uncertainty下进行知识探索的研究 paradigm，具有广泛的应用前景和可能的风险。<details>
<summary>Abstract</summary>
Data science is not a science. It is a research paradigm with an unfathomed scope, scale, complexity, and power for knowledge discovery that is not otherwise possible and can be beyond human reasoning. It is changing our world practically and profoundly already widely deployed in tens of thousands of applications in every discipline in an AI Arms Race that, due to its inscrutability, can lead to unfathomed risks. This paper presents an axiology of data science, its purpose, nature, importance, risks, and value for problem solving, by exploring and evaluating its remarkable, definitive features. As data science is in its infancy, this initial, speculative axiology is intended to aid in understanding and defining data science to recognize its potential benefits, risks, and open research challenges. AI based data science is inherently about uncertainty that may be more realistic than our preference for the certainty of science. Data science will have impacts far beyond knowledge discovery and will take us into new ways of understanding the world.
</details>
<details>
<summary>摘要</summary>
“数据科学不是一种科学。它是一种研究方法论，它的范围、规模、复杂度和知识发现能力超出了人类理解的限制，可以为问题解决提供无法其他方式获得的力量。它已经在各个领域中广泛应用，并在人工智能竞赛中投入了 tens of thousands of 应用，但由于其不可预测性，可能会导致未知的风险。本文提出了数据科学的axiology，即其目的、性质、重要性、风险和问题解决的价值，通过探索和评估其特点来帮助理解和定义数据科学，并找到其潜在的利益、风险和研究挑战。基于人工智能的数据科学本身带有不确定性，可能比我们更好地反映现实。数据科学将对我们的世界产生深远的影响，并将带我们进入新的世界理解方式。”
</details></li>
</ul>
<hr>
<h2 id="A-New-Computationally-Simple-Approach-for-Implementing-Neural-Networks-with-Output-Hard-Constraints"><a href="#A-New-Computationally-Simple-Approach-for-Implementing-Neural-Networks-with-Output-Hard-Constraints" class="headerlink" title="A New Computationally Simple Approach for Implementing Neural Networks with Output Hard Constraints"></a>A New Computationally Simple Approach for Implementing Neural Networks with Output Hard Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10459">http://arxiv.org/abs/2307.10459</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Andrei V. Konstantinov, Lev V. Utkin</li>
<li>for: 本研究提出了一种新的计算简单的神经网络输出值约束方法，用于解决神经网络输出值约束问题。</li>
<li>methods: 该方法基于将神经网络参数向量映射到一个确定在可行集中的点上，以实现约束的执行。该映射通过额外的神经网络层实现，并可扩展到对输入具有相互关系的约束。</li>
<li>results: 该方法的计算简单，前行通过的复杂度为O(n*m)和O(n^2*m)，其中n是变量数量，m是约束数量。数值实验证明了该方法在优化和分类问题中的效果。代码实现方法的公开可用。<details>
<summary>Abstract</summary>
A new computationally simple method of imposing hard convex constraints on the neural network output values is proposed. The key idea behind the method is to map a vector of hidden parameters of the network to a point that is guaranteed to be inside the feasible set defined by a set of constraints. The mapping is implemented by the additional neural network layer with constraints for output. The proposed method is simply extended to the case when constraints are imposed not only on the output vectors, but also on joint constraints depending on inputs. The projection approach to imposing constraints on outputs can simply be implemented in the framework of the proposed method. It is shown how to incorporate different types of constraints into the proposed method, including linear and quadratic constraints, equality constraints, and dynamic constraints, constraints in the form of boundaries. An important feature of the method is its computational simplicity. Complexities of the forward pass of the proposed neural network layer by linear and quadratic constraints are O(n*m) and O(n^2*m), respectively, where n is the number of variables, m is the number of constraints. Numerical experiments illustrate the method by solving optimization and classification problems. The code implementing the method is publicly available.
</details>
<details>
<summary>摘要</summary>
新的计算简单方法对神经网络输出值进行硬 convex 约束是提出的。该方法的关键思想是将神经网络参数向量映射到一个可以 garantuee 在约束集中的点上。该映射通过额外的神经网络层实现，该层带有约束条件。提出的方法可以简单地扩展到输入joint 约束的情况。投影方法可以简单地在该方法的框架中实现。该方法可以 incorporate 不同类型的约束，包括线性和quadratic 约束、等式约束、边界约束等。该方法的计算简单性是其重要特点。前进通过神经网络层的复杂性为O(n*m)和O(n^2*m)，其中n是变量数量，m是约束数量。数学实验证明了该方法的可行性和精度。代码实现该方法公开available。
</details></li>
</ul>
<hr>
<h2 id="A-Step-Towards-Worldwide-Biodiversity-Assessment-The-BIOSCAN-1M-Insect-Dataset"><a href="#A-Step-Towards-Worldwide-Biodiversity-Assessment-The-BIOSCAN-1M-Insect-Dataset" class="headerlink" title="A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset"></a>A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10455">http://arxiv.org/abs/2307.10455</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zahrag/BIOSCAN-1M">https://github.com/zahrag/BIOSCAN-1M</a></li>
<li>paper_authors: Zahra Gharaee, ZeMing Gong, Nicholas Pellegrino, Iuliia Zarubiieva, Joakim Bruslund Haurum, Scott C. Lowe, Jaclyn T. A. McKeown, Chris C. Y. Ho, Joschka McLeod, Yi-Yun C Wei, Jireh Agda, Sujeevan Ratnasingham, Dirk Steinke, Angel X. Chang, Graham W. Taylor, Paul Fieguth</li>
<li>for: 这个论文的目的是为了开发一个基于图像的生物多样性识别模型，以便为全球生物多样性的评估做出贡献。</li>
<li>methods: 这篇论文使用了一个大量的手动标注的昆虫图像集，称为BIOSCAN-Insect Dataset，每个记录都被专家分类，并包含了生物学上的遗传信息，如纯度序列和分配给每个物种的杂合编号。</li>
<li>results: 这篇论文提出了一个约一百万张图像的数据集，用于训练计算机视觉模型，以实现图像基于的生物多样性识别。此外，数据集还展示了一种特有的长尾分布，以及生物学上的层次分类问题。<details>
<summary>Abstract</summary>
In an effort to catalog insect biodiversity, we propose a new large dataset of hand-labelled insect images, the BIOSCAN-Insect Dataset. Each record is taxonomically classified by an expert, and also has associated genetic information including raw nucleotide barcode sequences and assigned barcode index numbers, which are genetically-based proxies for species classification. This paper presents a curated million-image dataset, primarily to train computer-vision models capable of providing image-based taxonomic assessment, however, the dataset also presents compelling characteristics, the study of which would be of interest to the broader machine learning community. Driven by the biological nature inherent to the dataset, a characteristic long-tailed class-imbalance distribution is exhibited. Furthermore, taxonomic labelling is a hierarchical classification scheme, presenting a highly fine-grained classification problem at lower levels. Beyond spurring interest in biodiversity research within the machine learning community, progress on creating an image-based taxonomic classifier will also further the ultimate goal of all BIOSCAN research: to lay the foundation for a comprehensive survey of global biodiversity. This paper introduces the dataset and explores the classification task through the implementation and analysis of a baseline classifier.
</details>
<details>
<summary>摘要</summary>
为了目录 insect 多样性，我们提议一个大型手动标注的昆虫图像集合，称为 BIOSCAN-Insect 数据集。每个记录都被专家分类，并有关联的遗传信息，包括原始核酸条形码序列和分配的核酸指标号，这些是基于种类分类的生物学基于代理。本文介绍了一个精心纪录的一百万张图像集，主要用于训练计算机视觉模型，以提供图像基本的种类评估。然而，数据集还具有吸引Machine Learning社区的特点，包括长尾分布和层次分类问题。BIOSCAN 研究的最终目标是建立全球多样性的基础，这个数据集的研究将有助于实现这一目标。本文介绍了数据集和基线分类器的实现和分析。
</details></li>
</ul>
<hr>
<h2 id="The-importance-of-feature-preprocessing-for-differentially-private-linear-optimization"><a href="#The-importance-of-feature-preprocessing-for-differentially-private-linear-optimization" class="headerlink" title="The importance of feature preprocessing for differentially private linear optimization"></a>The importance of feature preprocessing for differentially private linear optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11106">http://arxiv.org/abs/2307.11106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziteng Sun, Ananda Theertha Suresh, Aditya Krishna Menon</li>
<li>for: 该研究是 investigate the sufficient condition of DPSGD for finding a good minimizer under privacy constraints.</li>
<li>methods: 该研究使用了 differentially private stochastic gradient descent (DPSGD) 和其变种，以及 feature preprocessing.</li>
<li>results: 研究发现， Without feature preprocessing, DPSGD 会导致隐私Error proportional to the maximum norm of features over all samples. furthermore, the proposed algorithm DPSGD-F combines DPSGD with feature preprocessing, and proves that for classification tasks, it incurs a privacy error proportional to the diameter of the features.<details>
<summary>Abstract</summary>
Training machine learning models with differential privacy (DP) has received increasing interest in recent years. One of the most popular algorithms for training differentially private models is differentially private stochastic gradient descent (DPSGD) and its variants, where at each step gradients are clipped and combined with some noise. Given the increasing usage of DPSGD, we ask the question: is DPSGD alone sufficient to find a good minimizer for every dataset under privacy constraints? As a first step towards answering this question, we show that even for the simple case of linear classification, unlike non-private optimization, (private) feature preprocessing is vital for differentially private optimization. In detail, we first show theoretically that there exists an example where without feature preprocessing, DPSGD incurs a privacy error proportional to the maximum norm of features over all samples. We then propose an algorithm called DPSGD-F, which combines DPSGD with feature preprocessing and prove that for classification tasks, it incurs a privacy error proportional to the diameter of the features $\max_{x, x' \in D} \|x - x'\|_2$. We then demonstrate the practicality of our algorithm on image classification benchmarks.
</details>
<details>
<summary>摘要</summary>
训练机器学习模型带有差异隐私（DP）在过去几年中得到了越来越多的关注。DPSGD和其变种是最受欢迎的 differentially private 模型训练算法之一，其中在每步骤中 gradients 会被 clipped 并与一些随机噪声相加。 giventhe increasing usage of DPSGD, we ask the question: is DPSGD alone sufficient to find a good minimizer for every dataset under privacy constraints? As a first step towards answering this question, we show that even for the simple case of linear classification, unlike non-private optimization, (private) feature preprocessing is vital for differentially private optimization. In detail, we first show theoretically that there exists an example where without feature preprocessing, DPSGD incurs a privacy error proportional to the maximum norm of features over all samples. We then propose an algorithm called DPSGD-F, which combines DPSGD with feature preprocessing and prove that for classification tasks, it incurs a privacy error proportional to the diameter of the features $\max_{x, x' \in D} \|x - x'\|_2$. We then demonstrate the practicality of our algorithm on image classification benchmarks.Note: The translation is in Simplified Chinese, which is one of the two standardized Chinese languages. The other one is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Integrating-a-Heterogeneous-Graph-with-Entity-aware-Self-attention-using-Relative-Position-Labels-for-Reading-Comprehension-Model"><a href="#Integrating-a-Heterogeneous-Graph-with-Entity-aware-Self-attention-using-Relative-Position-Labels-for-Reading-Comprehension-Model" class="headerlink" title="Integrating a Heterogeneous Graph with Entity-aware Self-attention using Relative Position Labels for Reading Comprehension Model"></a>Integrating a Heterogeneous Graph with Entity-aware Self-attention using Relative Position Labels for Reading Comprehension Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10443">http://arxiv.org/abs/2307.10443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shima Foolad, Kourosh Kiani</li>
<li>for: This paper aims to improve the ability of transformer models in handling complex reasoning tasks by integrating external knowledge into the model without relying on external sources.</li>
<li>methods: The proposed attention pattern includes global-local attention for word tokens, graph attention for entity tokens, and consideration of the type of relationship between each entity token and word token.</li>
<li>results: The experimental findings show that the proposed model outperforms both the cutting-edge LUKE-Graph and the baseline LUKE model on the ReCoRD dataset that focuses on commonsense reasoning.Here’s the Chinese text in the format you requested:</li>
<li>for: 本文目的是强化基于转换器模型的机器阅读理解任务处理复杂逻辑任务的能力，不再依赖外部知识。</li>
<li>methods: 该提议的注意模式包括全局-本地注意力 для单词标签、基于图像注意力 для实体标签，以及对每个实体标签和单词标签之间的关系类型进行考虑。</li>
<li>results: 实验结果表明，提议的模型在ReCoRD数据集，专注于通逻辑理解任务上，超越了当前的LUKE-Graph和基准LUKE模型。<details>
<summary>Abstract</summary>
Despite the significant progress made by transformer models in machine reading comprehension tasks, they still fall short in handling complex reasoning tasks due to the absence of explicit knowledge in the input sequence. To address this limitation, many recent works have proposed injecting external knowledge into the model. However, selecting relevant external knowledge, ensuring its availability, and requiring additional processing steps remain challenging. In this paper, we introduce a novel attention pattern that integrates reasoning knowledge derived from a heterogeneous graph into the transformer architecture without relying on external knowledge. The proposed attention pattern comprises three key elements: global-local attention for word tokens, graph attention for entity tokens that exhibit strong attention towards tokens connected in the graph as opposed to those unconnected, and the consideration of the type of relationship between each entity token and word token. This results in optimized attention between the two if a relationship exists. The pattern is coupled with special relative position labels, allowing it to integrate with LUKE's entity-aware self-attention mechanism. The experimental findings corroborate that our model outperforms both the cutting-edge LUKE-Graph and the baseline LUKE model on the ReCoRD dataset that focuses on commonsense reasoning.
</details>
<details>
<summary>摘要</summary>
尽管变换器模型在机器阅读理解任务中做出了重要进步，但它们仍然缺乏明确的知识，导致在复杂的推理任务中表现不佳。为解决这个限制，许多最近的研究提出了在模型中注入外部知识的方法。然而，选择相关的外部知识，保证其可用性，并且需要额外的处理步骤仍然是挑战。在这篇论文中，我们提出了一种新的注意模式，即在变换器架构中 инте integrate 了多样化图的推理知识。该注意模式包括三个关键元素：全球-本地注意 для单词符号，图注意对实体符号的强注意点，以及每个实体符号和单词符号之间的类型关系的考虑。这些元素结合使得注意力得到优化。此外，我们采用特殊相对位标签，使得它可以与LUKE模型的实体意识自注意机制结合。实验结果表明，我们的模型在ReCoRD数据集上，对于具有通情理解的机器阅读理解任务表现出色，超越了当前的LUKE-Graph和LUKE模型。
</details></li>
</ul>
<hr>
<h2 id="Confidence-Estimation-Using-Unlabeled-Data"><a href="#Confidence-Estimation-Using-Unlabeled-Data" class="headerlink" title="Confidence Estimation Using Unlabeled Data"></a>Confidence Estimation Using Unlabeled Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10440">http://arxiv.org/abs/2307.10440</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/topoxlab/consistency-ranking-loss">https://github.com/topoxlab/consistency-ranking-loss</a></li>
<li>paper_authors: Chen Li, Xiaoling Hu, Chao Chen</li>
<li>for: 这篇论文旨在提出一种semi-supervised的信任度估计方法，供深度神经网络在实际应用中更好地估计自己的信任度。</li>
<li>methods: 本方法使用训练过程中的预测一致性作为估计信任度的Surrogate函数，并提出了一种一致性排名损失函数来实现这一目的。</li>
<li>results: 在图像分类和 segmentation 任务上，本方法可以达到状态的表现，并且在下游活动学任务中表现出了明显的优势。<details>
<summary>Abstract</summary>
Overconfidence is a common issue for deep neural networks, limiting their deployment in real-world applications. To better estimate confidence, existing methods mostly focus on fully-supervised scenarios and rely on training labels. In this paper, we propose the first confidence estimation method for a semi-supervised setting, when most training labels are unavailable. We stipulate that even with limited training labels, we can still reasonably approximate the confidence of model on unlabeled samples by inspecting the prediction consistency through the training process. We use training consistency as a surrogate function and propose a consistency ranking loss for confidence estimation. On both image classification and segmentation tasks, our method achieves state-of-the-art performances in confidence estimation. Furthermore, we show the benefit of the proposed method through a downstream active learning task. The code is available at https://github.com/TopoXLab/consistency-ranking-loss
</details>
<details>
<summary>摘要</summary>
<<SYS>> tranlate the following text into Simplified ChineseOverconfidence is a common issue for deep neural networks, limiting their deployment in real-world applications. To better estimate confidence, existing methods mostly focus on fully-supervised scenarios and rely on training labels. In this paper, we propose the first confidence estimation method for a semi-supervised setting, when most training labels are unavailable. We stipulate that even with limited training labels, we can still reasonably approximate the confidence of model on unlabeled samples by inspecting the prediction consistency through the training process. We use training consistency as a surrogate function and propose a consistency ranking loss for confidence estimation. On both image classification and segmentation tasks, our method achieves state-of-the-art performances in confidence estimation. Furthermore, we show the benefit of the proposed method through a downstream active learning task. The code is available at https://github.com/TopoXLab/consistency-ranking-lossTranslate the text into Simplified Chinese:<<SYS>>深度神经网络中的过于自信是一个常见的问题，限制其在实际应用中的部署。现有方法主要集中在完全监督的enario中，通过训练标签来估计自信。在这篇论文中，我们提出了首个半监督情况下的自信估计方法，当多个训练标签不可用时。我们认为，即使受限的训练标签，我们仍可以通过训练过程中的预测一致性来合理地估计模型对未标注样本的自信。我们使用训练一致性作为准则函数，并提出了一种一致排名损失来估计自信。在图像分类和分割任务上，我们的方法实现了状态级表现。此外，我们还证明了我们的方法在下游活动学任务中的利好。代码可以在https://github.com/TopoXLab/consistency-ranking-loss上获取。Translation note:* "深度神经网络" is translated as "deep neural network"* "过于自信" is translated as "overconfidence"* "半监督情况" is translated as "semi-supervised setting"* "训练标签" is translated as "training labels"* "未标注样本" is translated as "unlabeled samples"* "一致性" is translated as "consistency"* "准则函数" is translated as "surrogate function"* "一致排名损失" is translated as "consistency ranking loss"
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Quantification-for-Molecular-Property-Predictions-with-Graph-Neural-Architecture-Search"><a href="#Uncertainty-Quantification-for-Molecular-Property-Predictions-with-Graph-Neural-Architecture-Search" class="headerlink" title="Uncertainty Quantification for Molecular Property Predictions with Graph Neural Architecture Search"></a>Uncertainty Quantification for Molecular Property Predictions with Graph Neural Architecture Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10438">http://arxiv.org/abs/2307.10438</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengli Jiang, Shiyi Qin, Reid C. Van Lehn, Prasanna Balaprakash, Victor M. Zavala</li>
<li>for: 该论文旨在提出一种自动化uncertainty quantification（UQ）方法，以便在分子性质预测中提高模型的可靠性和信息价值。</li>
<li>methods: 该论文使用了architecture search来生成一个高性能的GNN ensemble，并使用variance decomposition来分解数据和模型不确定性。</li>
<li>results: 对多个 benchmark dataset进行了计算实验，并证明了AutoGNNUQ在预测精度和UQ性能方面与现有的UQ方法相比，表现出了明显的优势。此外，通过t-SNE可视化，探索了分子特征和不确定性之间的相互关系，提供了数据集改进的指导。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have emerged as a prominent class of data-driven methods for molecular property prediction. However, a key limitation of typical GNN models is their inability to quantify uncertainties in the predictions. This capability is crucial for ensuring the trustworthy use and deployment of models in downstream tasks. To that end, we introduce AutoGNNUQ, an automated uncertainty quantification (UQ) approach for molecular property prediction. AutoGNNUQ leverages architecture search to generate an ensemble of high-performing GNNs, enabling the estimation of predictive uncertainties. Our approach employs variance decomposition to separate data (aleatoric) and model (epistemic) uncertainties, providing valuable insights for reducing them. In our computational experiments, we demonstrate that AutoGNNUQ outperforms existing UQ methods in terms of both prediction accuracy and UQ performance on multiple benchmark datasets. Additionally, we utilize t-SNE visualization to explore correlations between molecular features and uncertainty, offering insight for dataset improvement. AutoGNNUQ has broad applicability in domains such as drug discovery and materials science, where accurate uncertainty quantification is crucial for decision-making.
</details>
<details>
<summary>摘要</summary>
格raph神经网络（GNNs）已经成为分子性质预测中一种显著的数据驱动方法。然而，典型的GNN模型无法量化预测结果的不确定性。这种能力是下游任务中模型使用和部署的信任worthy的前提。为此，我们介绍AutoGNNUQ，一种自动 uncertainty quantification（UQ）方法 для分子性质预测。AutoGNNUQ利用架构搜索生成高性能GNN的ensemble，以便估计预测不确定性。我们的方法使用变差分解将数据（aleatoric）和模型（epistemic）不确定性分开，提供有价值的反馈 для降低其。在我们的计算实验中，我们证明AutoGNNUQ在多个benchmark数据集上比现有的UQ方法更高效， both in terms of prediction accuracy and UQ performance。此外，我们使用t-SNE可视化来探索分子特征和不确定性之间的相关性，为数据集改进提供了新的视角。AutoGNNUQ在药物搜索和材料科学等领域有广泛的应用，其中准确地量化不确定性是重要的决策前提。
</details></li>
</ul>
<hr>
<h2 id="A-Bayesian-Programming-Approach-to-Car-following-Model-Calibration-and-Validation-using-Limited-Data"><a href="#A-Bayesian-Programming-Approach-to-Car-following-Model-Calibration-and-Validation-using-Limited-Data" class="headerlink" title="A Bayesian Programming Approach to Car-following Model Calibration and Validation using Limited Data"></a>A Bayesian Programming Approach to Car-following Model Calibration and Validation using Limited Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10437">http://arxiv.org/abs/2307.10437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Franklin Abodo</li>
<li>for: 这个论文主要是为了提出一种可以准确模拟驾驶行为的车流模型，以便在交通研究和工程中设计和评估工程改进方案时能够更好地考虑安全和其他绩效指标。</li>
<li>methods: 这个论文使用了微型驾驶模型，并使用 bayesian 方法进行数据分析和参数估计，以便更好地捕捉和复制驾驶行为。</li>
<li>results: 这个论文通过 bayesian 方法来探讨和解决模型准确性问题，并发现了模型层次结构的利用可以提高模型的准确性。<details>
<summary>Abstract</summary>
Traffic simulation software is used by transportation researchers and engineers to design and evaluate changes to roadways. These simulators are driven by models of microscopic driver behavior from which macroscopic measures like flow and congestion can be derived. Many models are designed for a subset of possible traffic scenarios and roadway configurations, while others have no explicit constraints on their application. Work zones (WZs) are one scenario for which no model to date has reproduced realistic driving behavior. This makes it difficult to optimize for safety and other metrics when designing a WZ. The Federal Highway Administration commissioned the USDOT Volpe Center to develop a car-following (CF) model for use in microscopic simulators that can capture and reproduce driver behavior accurately within and outside of WZs. Volpe also performed a naturalistic driving study to collect telematics data from vehicles driven on roads with WZs for use in model calibration. During model development, Volpe researchers observed difficulties in calibrating their model, leaving them to question whether there existed flaws in their model, in the data, or in the procedure used to calibrate the model using the data. In this thesis, I use Bayesian methods for data analysis and parameter estimation to explore and, where possible, address these questions. First, I use Bayesian inference to measure the sufficiency of the size of the data set. Second, I compare the procedure and results of the genetic algorithm based calibration performed by the Volpe researchers with those of Bayesian calibration. Third, I explore the benefits of modeling CF hierarchically. Finally, I apply what was learned in the first three phases using an established CF model, Wiedemann 99, to the probabilistic modeling of the Volpe model. Validation is performed using information criteria as an estimate of predictive accuracy.
</details>
<details>
<summary>摘要</summary>
交通模拟软件由交通研究人员和工程师使用来设计和评估道路变化。这些模拟器由微型驾驶行为模型驱动，从而得到流和堵塞等宏观指标。许多模型适用于特定的交通情况和道路配置，而其他的则没有明确的应用约束。工地（WZ）是一种情况，在其中没有任何模型可以模拟真实的驾驶行为。这使得在设计工地时很难优化安全和其他指标。美国公路管理局（FHWA）委托美国交通部Volpe中心开发一个可以在微型模拟器中使用的拥挤（CF）模型，以便准确模拟驾驶行为。Volpe中心还进行了一项自然驾驶研究，收集了在道路上驾驶的车辆数据，以便在模型准确性校准中使用。在模型开发过程中，Volpe研究人员发现了困难在模型准确性校准中，这使得他们开始思考是否存在模型中的问题，数据中的问题，或者在使用数据进行模型准确性校准时的问题。在这个论文中，我使用 bayesian 方法来分析数据和参数估计，以探索和解决这些问题。首先，我使用 bayesian 推理来测量数据集的尺度是否充分。其次，我比较了使用 bayesian 准确性校准的结果和Volpe研究人员使用的遗传算法基于准确性校准的结果。最后，我探索了模型CF层次化的好处。最后，我使用已有的CF模型，Wiedemann 99，来应用学习到的知识，对Volpe模型的 probabilistic 模型进行模拟。验证是通过信息指标来估计预测精度。
</details></li>
</ul>
<hr>
<h2 id="A-Matrix-Ensemble-Kalman-Filter-based-Multi-arm-Neural-Network-to-Adequately-Approximate-Deep-Neural-Networks"><a href="#A-Matrix-Ensemble-Kalman-Filter-based-Multi-arm-Neural-Network-to-Adequately-Approximate-Deep-Neural-Networks" class="headerlink" title="A Matrix Ensemble Kalman Filter-based Multi-arm Neural Network to Adequately Approximate Deep Neural Networks"></a>A Matrix Ensemble Kalman Filter-based Multi-arm Neural Network to Adequately Approximate Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10436">http://arxiv.org/abs/2307.10436</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ved-piyush/menkf-ann-pul">https://github.com/ved-piyush/menkf-ann-pul</a></li>
<li>paper_authors: Ved Piyush, Yuchen Yan, Yuzhen Zhou, Yanbin Yin, Souparno Ghosh</li>
<li>for: 这个论文旨在提出一种基于 kalman 筛法的多手柄神经网络（MEnKF-ANN），用于缺省样本数量太小以训练多手柄神经网络。</li>
<li>methods: 该论文使用 kalman 筛法来 aproximate 神经网络，并实现了显式的模型堆叠。</li>
<li>results: 该论文可以”应对”一个基于 long short-term memory（LSTM）网络的分类问题，并可以为这个问题提供不确定性评估。<details>
<summary>Abstract</summary>
Deep Learners (DLs) are the state-of-art predictive mechanism with applications in many fields requiring complex high dimensional data processing. Although conventional DLs get trained via gradient descent with back-propagation, Kalman Filter (KF)-based techniques that do not need gradient computation have been developed to approximate DLs. We propose a multi-arm extension of a KF-based DL approximator that can mimic DL when the sample size is too small to train a multi-arm DL. The proposed Matrix Ensemble Kalman Filter-based multi-arm ANN (MEnKF-ANN) also performs explicit model stacking that becomes relevant when the training sample has an unequal-size feature set. Our proposed technique can approximate Long Short-term Memory (LSTM) Networks and attach uncertainty to the predictions obtained from these LSTMs with desirable coverage. We demonstrate how MEnKF-ANN can "adequately" approximate an LSTM network trained to classify what carbohydrate substrates are digested and utilized by a microbiome sample whose genomic sequences consist of polysaccharide utilization loci (PULs) and their encoded genes.
</details>
<details>
<summary>摘要</summary>
深度学习器（DL）是现状最高的预测机制，应用于需要复杂高维数据处理的多个领域。虽然传统的DL通过梯度下降和反射来训练，但Kalman滤波器（KF）基本技术不需要计算梯度，可以用来近似DL。我们提出了一种基于KF的多支武器ANN（MEnKF-ANN），可以在样本数太小以训练多支武器DL时模拟DL。我们的提议的技术还实现了显式模型堆叠，当特征集的大小不同时成为重要的。我们的提议的技术可以近似长期快速储存网络（LSTM）和attachuncertainty到这些LSTM的预测结果，并且可以获得可接受的保证。我们示例中用MEnKF-ANN来"合理"地近似一个基于LSTM网络训练的分类微生物材料的碳水化物substrates是否被微生物材料的PULs和其编码的基因消化和利用。
</details></li>
</ul>
<hr>
<h2 id="Learning-Formal-Specifications-from-Membership-and-Preference-Queries"><a href="#Learning-Formal-Specifications-from-Membership-and-Preference-Queries" class="headerlink" title="Learning Formal Specifications from Membership and Preference Queries"></a>Learning Formal Specifications from Membership and Preference Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10434">http://arxiv.org/abs/2307.10434</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ameesh Shah, Marcell Vazquez-Chanlatte, Sebastian Junges, Sanjit A. Seshia</li>
<li>for: 本研究探讨了活动学习在形式规定中的应用，包括自动机。</li>
<li>methods: 本文提出了一种新的框架，通过组合成员标签和对比 preference 来实现活动规定学习。</li>
<li>results: 我们在两个不同领域中实现了我们的框架，结果表明可以通过成员和偏好来稳定地识别规定。<details>
<summary>Abstract</summary>
Active learning is a well-studied approach to learning formal specifications, such as automata. In this work, we extend active specification learning by proposing a novel framework that strategically requests a combination of membership labels and pair-wise preferences, a popular alternative to membership labels. The combination of pair-wise preferences and membership labels allows for a more flexible approach to active specification learning, which previously relied on membership labels only. We instantiate our framework in two different domains, demonstrating the generality of our approach. Our results suggest that learning from both modalities allows us to robustly and conveniently identify specifications via membership and preferences.
</details>
<details>
<summary>摘要</summary>
active learning 是一种已经广泛研究的学习方法，用于学习正式规范，如自动机。在这项工作中，我们延伸了活动规范学习，提议一种新的框架，强制请求组合成员标签和对比标签，这是成员标签的受欢迎替代方案。这种组合的成员标签和对比标签允许我们在活动规范学习中采用更灵活的方法，之前只靠成员标签。我们在两个不同领域中实现了我们的框架，并证明了我们的方法的通用性。我们的结果表明，从两种模态中学习可以强健地和方便地识别规范 via 成员和偏好。
</details></li>
</ul>
<hr>
<h2 id="DP-TBART-A-Transformer-based-Autoregressive-Model-for-Differentially-Private-Tabular-Data-Generation"><a href="#DP-TBART-A-Transformer-based-Autoregressive-Model-for-Differentially-Private-Tabular-Data-Generation" class="headerlink" title="DP-TBART: A Transformer-based Autoregressive Model for Differentially Private Tabular Data Generation"></a>DP-TBART: A Transformer-based Autoregressive Model for Differentially Private Tabular Data Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10430">http://arxiv.org/abs/2307.10430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rodrigo Castellon, Achintya Gopal, Brian Bloniarz, David Rosenberg</li>
<li>for: 本文旨在提出一种能够保持分布式隐私的生成 tabular 数据的方法，并且能够与传统的 Margin-based 方法竞争。</li>
<li>methods: 本文使用 transformer 核心的 autoregressive 模型，并实现了分布式隐私。</li>
<li>results: 本文在各种数据集上达到了与 Margin-based 方法竞争的性能，甚至在某些情况下超越了状态之artefacts。<details>
<summary>Abstract</summary>
The generation of synthetic tabular data that preserves differential privacy is a problem of growing importance. While traditional marginal-based methods have achieved impressive results, recent work has shown that deep learning-based approaches tend to lag behind. In this work, we present Differentially-Private TaBular AutoRegressive Transformer (DP-TBART), a transformer-based autoregressive model that maintains differential privacy and achieves performance competitive with marginal-based methods on a wide variety of datasets, capable of even outperforming state-of-the-art methods in certain settings. We also provide a theoretical framework for understanding the limitations of marginal-based approaches and where deep learning-based approaches stand to contribute most. These results suggest that deep learning-based techniques should be considered as a viable alternative to marginal-based methods in the generation of differentially private synthetic tabular data.
</details>
<details>
<summary>摘要</summary>
“ differential privacy 的Synthetic tabular data生成问题是一个快速增长的领域。传统的边缘基数方法已经实现了很好的结果，但最近的研究表明，深度学习基于方法在这个领域的表现相对落后。在这个工作中，我们介绍了Diffentially-Private TaBular AutoRegressive Transformer（DP-TBART），一种基于 transformer 的自适应模型，保持了 differential privacy 并在各种数据集上达到了与边缘基数方法相当的性能，甚至在某些设置下超越了状态码的最佳方法。我们还提供了理论框架，用于理解边缘基数方法的局限性和深度学习基数方法在这个领域的潜在贡献。这些结果表明，深度学习基数方法应该被视为 Synthetic tabular data 生成 differential privacy 的可行的代替方案。”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="PreDiff-Precipitation-Nowcasting-with-Latent-Diffusion-Models"><a href="#PreDiff-Precipitation-Nowcasting-with-Latent-Diffusion-Models" class="headerlink" title="PreDiff: Precipitation Nowcasting with Latent Diffusion Models"></a>PreDiff: Precipitation Nowcasting with Latent Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10422">http://arxiv.org/abs/2307.10422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhihan Gao, Xingjian Shi, Boran Han, Hao Wang, Xiaoyong Jin, Danielle Maddix, Yi Zhu, Mu Li, Yuyang Wang</li>
<li>for: 这个论文旨在提出一种两Stage管道，用于各种 Earth system forecasting 任务中的 probabilistic spatiotemporal 预测。</li>
<li>methods: 该模型使用 conditional latent diffusion 方法，并通过显式控制知识机制来对预测结果进行 Physical 约束。</li>
<li>results: 实验表明，PreDiff 模型可以具有高效的操作用途，并能够处理不确定性和遵循物理法则的预测结果。<details>
<summary>Abstract</summary>
Earth system forecasting has traditionally relied on complex physical models that are computationally expensive and require significant domain expertise. In the past decade, the unprecedented increase in spatiotemporal Earth observation data has enabled data-driven forecasting models using deep learning techniques. These models have shown promise for diverse Earth system forecasting tasks but either struggle with handling uncertainty or neglect domain-specific prior knowledge, resulting in averaging possible futures to blurred forecasts or generating physically implausible predictions. To address these limitations, we propose a two-stage pipeline for probabilistic spatiotemporal forecasting: 1) We develop PreDiff, a conditional latent diffusion model capable of probabilistic forecasts. 2) We incorporate an explicit knowledge control mechanism to align forecasts with domain-specific physical constraints. This is achieved by estimating the deviation from imposed constraints at each denoising step and adjusting the transition distribution accordingly. We conduct empirical studies on two datasets: N-body MNIST, a synthetic dataset with chaotic behavior, and SEVIR, a real-world precipitation nowcasting dataset. Specifically, we impose the law of conservation of energy in N-body MNIST and anticipated precipitation intensity in SEVIR. Experiments demonstrate the effectiveness of PreDiff in handling uncertainty, incorporating domain-specific prior knowledge, and generating forecasts that exhibit high operational utility.
</details>
<details>
<summary>摘要</summary>
地球系统预报traditionally rely 于复杂的物理模型， Computationally expensive 和需要特定领域专家知识。在过去的一代，随着无前例的空间时间地球观测数据的增加，使用深度学习技术的数据驱动预报模型得到了推广。这些模型在多元地球预报任务中表现了应用潜力，但是它们 either struggle with handling uncertainty or neglect domain-specific prior knowledge, resulting in averaging possible futures to blurred forecasts or generating physically implausible predictions.为了解决这些限制，我们提出了一个二阶段管道，用于数值空间时间预报：1. We develop PreDiff，一个可 probabilistic forecasts的 conditional latent diffusion model。2. We incorporate an explicit knowledge control mechanism to align forecasts with domain-specific physical constraints. This is achieved by estimating the deviation from imposed constraints at each denoising step and adjusting the transition distribution accordingly.我们在两个 dataset上进行了实验：N-body MNIST，一个Synthetic dataset with chaotic behavior，和SEVIR，一个真实世界降水预报dataset。具体来说，我们在N-body MNIST中强制遵循能量守恒定律，并在SEVIR中预测降水强度。实验结果表明 PreDiff 能够 effectively handle uncertainty， incorporate domain-specific prior knowledge，并生成 forecasts that exhibit high operational utility。
</details></li>
</ul>
<hr>
<h2 id="Technical-Challenges-of-Deploying-Reinforcement-Learning-Agents-for-Game-Testing-in-AAA-Games"><a href="#Technical-Challenges-of-Deploying-Reinforcement-Learning-Agents-for-Game-Testing-in-AAA-Games" class="headerlink" title="Technical Challenges of Deploying Reinforcement Learning Agents for Game Testing in AAA Games"></a>Technical Challenges of Deploying Reinforcement Learning Agents for Game Testing in AAA Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11105">http://arxiv.org/abs/2307.11105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonas Gillberg, Joakim Bergdahl, Alessandro Sestini, Andrew Eakins, Linus Gisslen</li>
<li>for: 本研讨文是为了探讨如何通过增加自动游戏测试解决方案中的强化学习系统来提高测试覆盖率。</li>
<li>methods: 本研究使用了脚本bot和强化学习技术来扩展现有的自动测试解决方案。</li>
<li>results: 研究发现，通过增加强化学习系统，测试覆盖率得到了显著提高，能够减少测试时间并提高游戏质量。<details>
<summary>Abstract</summary>
Going from research to production, especially for large and complex software systems, is fundamentally a hard problem. In large-scale game production, one of the main reasons is that the development environment can be very different from the final product. In this technical paper we describe an effort to add an experimental reinforcement learning system to an existing automated game testing solution based on scripted bots in order to increase its capacity. We report on how this reinforcement learning system was integrated with the aim to increase test coverage similar to [1] in a set of AAA games including Battlefield 2042 and Dead Space (2023). The aim of this technical paper is to show a use-case of leveraging reinforcement learning in game production and cover some of the largest time sinks anyone who wants to make the same journey for their game may encounter. Furthermore, to help the game industry to adopt this technology faster, we propose a few research directions that we believe will be valuable and necessary for making machine learning, and especially reinforcement learning, an effective tool in game production.
</details>
<details>
<summary>摘要</summary>
原研究到生产的过程，特别是 для大型和复杂的软件系统，是一个基本的困难问题。在大规模游戏生产中，一个主要原因是开发环境与最终产品很不同。在这篇技术论文中，我们描述了一项尝试将 эксперименталь的强化学习系统添加到现有的自动游戏测试解决方案中，以增加其容量。我们报告了在一些AAA游戏，如《战field 2042》和《僵尸空间2023》中，如何将这种强化学习系统与现有的测试解决方案集成，以提高测试覆盖率。本技术论文的目的是向游戏产业展示如何使用强化学习在游戏生产中，并涵盖一些最大时间沟通的问题。此外，为了帮助游戏业 faster采用这种技术，我们提出了一些研究方向，我们认为将是值得的和必要的 для使机器学习，特别是强化学习，成为游戏生产中的有效工具。
</details></li>
</ul>
<hr>
<h2 id="Interpreting-and-Correcting-Medical-Image-Classification-with-PIP-Net"><a href="#Interpreting-and-Correcting-Medical-Image-Classification-with-PIP-Net" class="headerlink" title="Interpreting and Correcting Medical Image Classification with PIP-Net"></a>Interpreting and Correcting Medical Image Classification with PIP-Net</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10404">http://arxiv.org/abs/2307.10404</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/m-nauta/pipnet">https://github.com/m-nauta/pipnet</a></li>
<li>paper_authors: Meike Nauta, Johannes H. Hegeman, Jeroen Geerdink, Jörg Schlötterer, Maurice van Keulen, Christin Seifert</li>
<li>for: 这篇论文探讨了使用可解释机器学习模型对实际医疗影像数据的自动诊断支持。</li>
<li>methods: 这篇论文使用了PIP-Net模型，一种可解释的image classifier，并评估了其精度和可解释性在骨折检测和皮肤癌诊断中。</li>
<li>results: 结果显示PIP-Net的决策过程与医疗分类标准相符，并且可以轻松地识别数据质量问题，例如医疗影像中的不适合文本或标签错误。此外，我们首次显示了人类可以手动更正PIP-Net的推理，例如禁用不需要的标本。<details>
<summary>Abstract</summary>
Part-prototype models are explainable-by-design image classifiers, and a promising alternative to black box AI. This paper explores the applicability and potential of interpretable machine learning, in particular PIP-Net, for automated diagnosis support on real-world medical imaging data. PIP-Net learns human-understandable prototypical image parts and we evaluate its accuracy and interpretability for fracture detection and skin cancer diagnosis. We find that PIP-Net's decision making process is in line with medical classification standards, while only provided with image-level class labels. Because of PIP-Net's unsupervised pretraining of prototypes, data quality problems such as undesired text in an X-ray or labelling errors can be easily identified. Additionally, we are the first to show that humans can manually correct the reasoning of PIP-Net by directly disabling undesired prototypes. We conclude that part-prototype models are promising for medical applications due to their interpretability and potential for advanced model debugging.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXT全文模型是可解释的设计图像分类器，是黑obox AI的可行替代方案。这篇论文探讨了可解释机器学习的可行性和潜力，特别是PIP-Net在医疗影像数据自动诊断支持方面的应用。PIP-Net学习了人理解的图像组成部分，我们评估了它的准确率和可解释性在骨折检测和皮肤癌诊断方面。我们发现PIP-Net的决策过程与医学分类标准相一致，只需要提供图像级别的标签。由于PIP-Net的无监督prototype学习，数据质量问题 such as 不想要的文本在X射线图像或标签错误可以轻松地被识别出来。此外，我们是第一个表明人可以手动更正PIP-Net的理由的。我们 conclude that part-prototype models are promising for medical applications due to their interpretability and potential for advanced model debugging.<<SYS>>
</details></li>
</ul>
<hr>
<h2 id="Selection-functions-of-strong-lens-finding-neural-networks"><a href="#Selection-functions-of-strong-lens-finding-neural-networks" class="headerlink" title="Selection functions of strong lens finding neural networks"></a>Selection functions of strong lens finding neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10355">http://arxiv.org/abs/2307.10355</a></li>
<li>repo_url: None</li>
<li>paper_authors: A. Herle, C. M. O’Riordan, S. Vegetti</li>
<li>for: 这个论文是为了研究弯光神经网络在找到弯光系统方面的偏见性。</li>
<li>methods: 这个论文使用了类似于常见文献中的弯光神经网络架构和训练数据来训练弯光找到器。</li>
<li>results: 研究发现，使用常见的训练数据集，弯光神经网络会偏好大 Einstein 半径和大的源星系，source 星系的光度分布更加集中。提高检测 significado 阈值 FROM 8 σ TO 12 σ，只有50%的弯光系统有 Einstein 半径 大于或等于 1.04 arcsec，source 星系的半径大于或等于 0.194 arcsec，和 source Sérsic 指数大于或等于 2.62。模型用于找到弯光 quasi 会对高弯光系统产生更强的偏好。偏见函数不依赖于弯光系统的形态指数，因此测量这种量将不受影响。<details>
<summary>Abstract</summary>
Convolution Neural Networks trained for the task of lens finding with similar architecture and training data as is commonly found in the literature are biased classifiers. An understanding of the selection function of lens finding neural networks will be key to fully realising the potential of the large samples of strong gravitational lens systems that will be found in upcoming wide-field surveys. We use three training datasets, representative of those used to train galaxy-galaxy and galaxy-quasar lens finding neural networks. The networks preferentially select systems with larger Einstein radii and larger sources with more concentrated source-light distributions. Increasing the detection significance threshold to 12$\sigma$ from 8$\sigma$ results in 50 per cent of the selected strong lens systems having Einstein radii $\theta_\mathrm{E}$ $\ge$ 1.04 arcsec from $\theta_\mathrm{E}$ $\ge$ 0.879 arcsec, source radii $R_S$ $\ge$ 0.194 arcsec from $R_S$ $\ge$ 0.178 arcsec and source S\'ersic indices $n_{\mathrm{Sc}}^{\mathrm{S}}$ $\ge$ 2.62 from $n_{\mathrm{Sc}}^{\mathrm{S}}$ $\ge$ 2.55. The model trained to find lensed quasars shows a stronger preference for higher lens ellipticities than those trained to find lensed galaxies. The selection function is independent of the slope of the power-law of the mass profiles, hence measurements of this quantity will be unaffected. The lens finder selection function reinforces that of the lensing cross-section, and thus we expect our findings to be a general result for all galaxy-galaxy and galaxy-quasar lens finding neural networks.
</details>
<details>
<summary>摘要</summary>
干扰神经网络（Convolutional Neural Networks，CNNS）用于弯光镜搜索任务，与文献中常见的architecture和训练数据相同，会受到偏见。我们需要理解弯光镜搜索神经网络的选择函数，以便在未来的广泛访问Surveys中全面利用大量强 gravitational lens系统的潜力。我们使用了三个训练数据集，代表了通常用于训练 galaxy-galaxy和galaxy-quasar弯光镜搜索神经网络。这些网络偏好具有更大的爱因斯坦半径和更集中的源光谱分布。如果提高检测 significancethreshold到12$\sigma$从8$\sigma$，则50%选择的强弯光系统具有 $\theta_\mathrm{E}$ $\ge$ 1.04 arcsec，Source radii $R_S$ $\ge$ 0.194 arcsec，source S\'ersic indices $n_{\mathrm{Sc}}^{\mathrm{S}}$ $\ge$ 2.62。模型用于搜索弯光镜 quasiars显示更强的偏好于高弯光镜形状因子，而不是用于搜索弯光镜 galaxies。选择函数独立于弯光镜形状因子的斜率，因此测量这一量将不受影响。弯光镜选择函数会强化弯光镜批处函数，我们预计这些结果将适用于所有galaxy-galaxy和galaxy-quasar弯光镜搜索神经网络。
</details></li>
</ul>
<hr>
<h2 id="LightPath-Lightweight-and-Scalable-Path-Representation-Learning"><a href="#LightPath-Lightweight-and-Scalable-Path-Representation-Learning" class="headerlink" title="LightPath: Lightweight and Scalable Path Representation Learning"></a>LightPath: Lightweight and Scalable Path Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10171">http://arxiv.org/abs/2307.10171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sean Bin Yang, Jilin Hu, Chenjuan Guo, Bin Yang, Christian S. Jensen</li>
<li>for: 本研究旨在提供一种轻量级、可扩展的路径表示学习框架，以提高智能交通和智能城市应用中的路径表示学习效能。</li>
<li>methods: 本研究提议使用稀疏自编码器来实现轻量级和可扩展性，并通过关系逻辑推理和全局地域知识传播来快速训练稀疏路径编码器。</li>
<li>results: 经过广泛的实验，本研究发现LightPath框架可以减少资源消耗，同时保持高度的效能和可扩展性，并在实际应用中表现出色。<details>
<summary>Abstract</summary>
Movement paths are used widely in intelligent transportation and smart city applications. To serve such applications, path representation learning aims to provide compact representations of paths that enable efficient and accurate operations when used for different downstream tasks such as path ranking and travel cost estimation. In many cases, it is attractive that the path representation learning is lightweight and scalable; in resource-limited environments and under green computing limitations, it is essential. Yet, existing path representation learning studies focus on accuracy and pay at most secondary attention to resource consumption and scalability.   We propose a lightweight and scalable path representation learning framework, termed LightPath, that aims to reduce resource consumption and achieve scalability without affecting accuracy, thus enabling broader applicability. More specifically, we first propose a sparse auto-encoder that ensures that the framework achieves good scalability with respect to path length. Next, we propose a relational reasoning framework to enable faster training of more robust sparse path encoders. We also propose global-local knowledge distillation to further reduce the size and improve the performance of sparse path encoders. Finally, we report extensive experiments on two real-world datasets to offer insight into the efficiency, scalability, and effectiveness of the proposed framework.
</details>
<details>
<summary>摘要</summary>
路径表示法广泛应用于智能交通和智能城市应用场景中。为了服务这些应用场景，路径表示学习目的是提供高效精度的路径表示，以便在不同的下游任务中进行高效的操作，如路径排名和旅行成本估算。在资源有限和绿色计算限制的情况下， existing 路径表示学习研究主要关注精度，并仅征服其次的资源消耗和可扩展性。我们提出了一个轻量级和可扩展的路径表示学习框架，称为LightPath，以降低资源消耗和实现可扩展性，而不影响准确性。更 Specifically，我们首先提出了一种稀疏自动编码器，以确保框架在路径长度方面具有良好的扩展性。然后，我们提出了一种关系逻辑框架，以快速训练更加稀疏的路径编码器。 finally，我们提出了全球-本地知识传播，以进一步减小路径编码器的大小和提高其性能。我们在两个真实世界数据集上进行了广泛的实验，以便了解LightPath框架的效率、可扩展性和有效性。
</details></li>
</ul>
<hr>
<h2 id="Challenges-and-Applications-of-Large-Language-Models"><a href="#Challenges-and-Applications-of-Large-Language-Models" class="headerlink" title="Challenges and Applications of Large Language Models"></a>Challenges and Applications of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10169">http://arxiv.org/abs/2307.10169</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, Robert McHardy</li>
<li>for: 本研究旨在提供系统化的开Problem和应用成功列表，以帮助ML研究人员更快地了解领域的当前状况，并更快地成为产品人。</li>
<li>methods: 本研究采用系统化的方法来列出领域的开Problem和应用成功，包括Literature Review和Expert Interview等。</li>
<li>results: 本研究提出了一系列的开Problem和应用成功，包括语言模型的泛化和鲁棒性问题，以及应用于自然语言处理、计算机视觉和自然语言生成等领域的成功应用。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learning discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to establish a systematic set of open problems and application successes so that ML researchers can comprehend the field's current state more quickly and become productive.
</details>
<details>
<summary>摘要</summary>
庞大语言模型（LLM）从不存在到普遍的机器学习讨论中只需几年的时间。由于这个领域的快速发展，现在很难确定还有哪些挑战和已经成功应用的领域。在这篇论文中，我们希望建立一个系统的开放问题和应用成功的集合，让机器学习研究人员更快地理解领域的当前状况，更快地成为产品力。
</details></li>
</ul>
<hr>
<h2 id="VITS-Variational-Inference-Thomson-Sampling-for-contextual-bandits"><a href="#VITS-Variational-Inference-Thomson-Sampling-for-contextual-bandits" class="headerlink" title="VITS : Variational Inference Thomson Sampling for contextual bandits"></a>VITS : Variational Inference Thomson Sampling for contextual bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10167">http://arxiv.org/abs/2307.10167</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Clavier, Tom Huix, Alain Durmus</li>
<li>for: 本文介绍了一种Contextual Bandit中的Thompson sampling（TS）算法变体，解决了在每个轮次中需要样本来自当前 posterior distribution 的问题。</li>
<li>methods: 本文提出了一种基于 Gaussian Variational Inference 的新算法，称为 Varitional Inference Thompson Sampling（VITS），可以提供高效的 posterior approximation，并且容易从样本中采样。</li>
<li>results: 本文证明了 VITS 可以在 linear contextual bandit 中实现 sub-linear regret bound，与传统 TS 的 regret bound 相同水平，并且在实验中表现出色。<details>
<summary>Abstract</summary>
In this paper, we introduce and analyze a variant of the Thompson sampling (TS) algorithm for contextual bandits. At each round, traditional TS requires samples from the current posterior distribution, which is usually intractable. To circumvent this issue, approximate inference techniques can be used and provide samples with distribution close to the posteriors. However, current approximate techniques yield to either poor estimation (Laplace approximation) or can be computationally expensive (MCMC methods, Ensemble sampling...). In this paper, we propose a new algorithm, Varational Inference Thompson sampling VITS, based on Gaussian Variational Inference. This scheme provides powerful posterior approximations which are easy to sample from, and is computationally efficient, making it an ideal choice for TS. In addition, we show that VITS achieves a sub-linear regret bound of the same order in the dimension and number of round as traditional TS for linear contextual bandit. Finally, we demonstrate experimentally the effectiveness of VITS on both synthetic and real world datasets.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种Contextual Bandit中的Thompson抽样（TS）算法的变体。在每个轮次上，传统的TS需要从当前 posterior distribution 中取样，这通常是不可能的。为了缺乏这个问题，我们可以使用approximate inference技术，以提供 samples 的 distribution 接近 posterior。然而，现有的approximate技术可能会导致 poor estimation（Laplace approximation）或者是 computationally expensive（MCMC方法、Ensemble sampling...）。在这篇论文中，我们提出了一种新的算法，即 Gaussian Variational Inference 基于的 Varitional Inference Thompson Sampling（VITS）。这种方案提供了强大的 posterior 近似，易于采样，并且 computationally efficient，使其成为TS的理想选择。此外，我们证明了 VITS 在 linear contextual bandit 中可以 достичь同样的下界 regret bound，与传统TS的 regret bound 相同。最后，我们通过实验表明 VITS 在 synthetic 和实际世界数据上具有良好的效果。
</details></li>
</ul>
<hr>
<h2 id="Improving-Multimodal-Datasets-with-Image-Captioning"><a href="#Improving-Multimodal-Datasets-with-Image-Captioning" class="headerlink" title="Improving Multimodal Datasets with Image Captioning"></a>Improving Multimodal Datasets with Image Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10350">http://arxiv.org/abs/2307.10350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thao Nguyen, Samir Yitzhak Gadre, Gabriel Ilharco, Sewoong Oh, Ludwig Schmidt</li>
<li>for: 这个研究的目的是提高大型视觉语言模型的性能，通过增强网络数据的质量。</li>
<li>methods: 该研究使用生成caption来减少网络数据的噪声，并对不同的混合策略进行比较。</li>
<li>results: 相比DataCompbenchmark中提出的最佳策略，该研究的最佳方法在ImageNet和38个任务中占据2%和4%的优势，并在Flickr和MS-COCO检索中表现出2倍的提升。此外，研究还分析了生成caption的有效性，并证明了标准图像描述文本模型的性能不是训练多模态任务中文本超级vision模型的准确指标。<details>
<summary>Abstract</summary>
Massive web datasets play a key role in the success of large vision-language models like CLIP and Flamingo. However, the raw web data is noisy, and existing filtering methods to reduce noise often come at the expense of data diversity. Our work focuses on caption quality as one major source of noise, and studies how generated captions can increase the utility of web-scraped datapoints with nondescript text. Through exploring different mixing strategies for raw and generated captions, we outperform the best filtering method proposed by the DataComp benchmark by 2% on ImageNet and 4% on average across 38 tasks, given a candidate pool of 128M image-text pairs. Our best approach is also 2x better at Flickr and MS-COCO retrieval. We then analyze what makes synthetic captions an effective source of text supervision. In experimenting with different image captioning models, we also demonstrate that the performance of a model on standard image captioning benchmarks (e.g., NoCaps CIDEr) is not a reliable indicator of the utility of the captions it generates for multimodal training. Finally, our experiments with using generated captions at DataComp's large scale (1.28B image-text pairs) offer insights into the limitations of synthetic text, as well as the importance of image curation with increasing training data quantity.
</details>
<details>
<summary>摘要</summary>
巨大的网络数据集在大型视觉语言模型 like CLIP 和 Flamingo 的成功中发挥着关键作用。然而，原始网络数据具有噪音，现有的过滤方法通常会导致数据多样性减少。我们的工作将注重caption质量作为噪音的一个主要来源，研究如何使用生成的caption提高网络抓取的datapoints中的描述文本。通过不同的混合策略来处理原始和生成的caption，我们在ImageNet上比DataComp Benchmark中提出的最佳过滤方法高出2%，平均在38个任务上高出4%，给定128M个图像文本对。我们的最佳方法还在Flickr和MS-COCO中 Retrieval上高出2倍。我们还分析了生成caption的效果，并在不同的图像captioning模型中进行了实验。最后，我们在DataComp的大规模 scale（1.28B个图像文本对）上进行了实验，并提供了生成caption的局限性以及图像准备的重要性，随着训练数据量的增加。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Backdoor-Attacks"><a href="#Rethinking-Backdoor-Attacks" class="headerlink" title="Rethinking Backdoor Attacks"></a>Rethinking Backdoor Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10163">http://arxiv.org/abs/2307.10163</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lancopku/SOS">https://github.com/lancopku/SOS</a></li>
<li>paper_authors: Alaa Khaddaj, Guillaume Leclerc, Aleksandar Makelov, Kristian Georgiev, Hadi Salman, Andrew Ilyas, Aleksander Madry</li>
<li>for: 本文旨在探讨反向攻击问题，具体来说是在训练集中插入恶意构建的背门键入例会让模型易受到操纵。</li>
<li>methods: 本文提出了一种不同的方法来对反向攻击进行防御。具体来说是通过训练数据分布的结构信息来探测和除掉恶意构建的背门键入例。</li>
<li>results: 本文表明，在缺乏训练数据分布结构信息的情况下，反向攻击和自然出现的特征无法分辨，因此不可以通过普通的检测方法来探测。此外，本文还探讨了现有的反向攻击防御策略，并指出了这些策略的假设和依赖关系。最后，本文提出了一种新的反向攻击检测原理，并开发了一种有理论保证和实践效果的检测算法。<details>
<summary>Abstract</summary>
In a backdoor attack, an adversary inserts maliciously constructed backdoor examples into a training set to make the resulting model vulnerable to manipulation. Defending against such attacks typically involves viewing these inserted examples as outliers in the training set and using techniques from robust statistics to detect and remove them.   In this work, we present a different approach to the backdoor attack problem. Specifically, we show that without structural information about the training data distribution, backdoor attacks are indistinguishable from naturally-occurring features in the data--and thus impossible to "detect" in a general sense. Then, guided by this observation, we revisit existing defenses against backdoor attacks and characterize the (often latent) assumptions they make and on which they depend. Finally, we explore an alternative perspective on backdoor attacks: one that assumes these attacks correspond to the strongest feature in the training data. Under this assumption (which we make formal) we develop a new primitive for detecting backdoor attacks. Our primitive naturally gives rise to a detection algorithm that comes with theoretical guarantees and is effective in practice.
</details>
<details>
<summary>摘要</summary>
在一个后门攻击中，敌对者会把恶意构造的后门示例 inserts到训练集中，以使模型易受 manipulate。防御这类攻击通常通过视这些插入的示例为训练集中异常值来实现。在这种工作中，我们提出一种不同的后门攻击问题的解决方案。具体来说，我们表明无论training数据分布的结构信息，后门攻击都是自然出现在数据中的特征——因此不可以在通用的检测中检测到。然后，我们根据这一观察，再次审视了现有的后门攻击防御方法，描述它们所假设和依赖的假设。最后，我们explore一种新的后门攻击视角：假设这些攻击对应于训练数据中最强的特征。根据这个假设（我们将其形式化），我们开发了一种新的后门攻击检测原理。这个原理自然给出了一种检测算法，该算法在理论上有保证并在实践中有效。
</details></li>
</ul>
<hr>
<h2 id="Robust-Driving-Policy-Learning-with-Guided-Meta-Reinforcement-Learning"><a href="#Robust-Driving-Policy-Learning-with-Guided-Meta-Reinforcement-Learning" class="headerlink" title="Robust Driving Policy Learning with Guided Meta Reinforcement Learning"></a>Robust Driving Policy Learning with Guided Meta Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10160">http://arxiv.org/abs/2307.10160</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kanghoon Lee, Jiachen Li, David Isele, Jinkyoo Park, Kikuo Fujimura, Mykel J. Kochenderfer</li>
<li>for: 这篇论文旨在解决现有的Deep Reinforcement Learning（DRL）方法在交通场景中控制社会车辆时存在的问题，即训练环境中使用固定行为策略可能导致学习的驾驶策略过拟合环境。</li>
<li>methods: 我们提出了一种有效的方法，通过随机化社会车辆之间的互动奖励函数，以生成多样化的目标，并通过引导策略来训练单一的meta策略。</li>
<li>results: 我们的方法成功地学习了一个ego驾驶策略，该策略在未看到社会车辆的行为情况下能够总结良好，并且通过在meta策略控制社会车辆的环境中强化驾驶策略来提高其Robustness。<details>
<summary>Abstract</summary>
Although deep reinforcement learning (DRL) has shown promising results for autonomous navigation in interactive traffic scenarios, existing work typically adopts a fixed behavior policy to control social vehicles in the training environment. This may cause the learned driving policy to overfit the environment, making it difficult to interact well with vehicles with different, unseen behaviors. In this work, we introduce an efficient method to train diverse driving policies for social vehicles as a single meta-policy. By randomizing the interaction-based reward functions of social vehicles, we can generate diverse objectives and efficiently train the meta-policy through guiding policies that achieve specific objectives. We further propose a training strategy to enhance the robustness of the ego vehicle's driving policy using the environment where social vehicles are controlled by the learned meta-policy. Our method successfully learns an ego driving policy that generalizes well to unseen situations with out-of-distribution (OOD) social agents' behaviors in a challenging uncontrolled T-intersection scenario.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:although deep reinforcement learning (DRL) has shown promising results for autonomous navigation in interactive traffic scenarios, existing work typically adopts a fixed behavior policy to control social vehicles in the training environment. This may cause the learned driving policy to overfit the environment, making it difficult to interact well with vehicles with different, unseen behaviors. In this work, we introduce an efficient method to train diverse driving policies for social vehicles as a single meta-policy. By randomizing the interaction-based reward functions of social vehicles, we can generate diverse objectives and efficiently train the meta-policy through guiding policies that achieve specific objectives. We further propose a training strategy to enhance the robustness of the ego vehicle's driving policy using the environment where social vehicles are controlled by the learned meta-policy. Our method successfully learns an ego driving policy that generalizes well to unseen situations with out-of-distribution (OOD) social agents' behaviors in a challenging uncontrolled T-intersection scenario.Note: Simplified Chinese is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="Curvature-based-Clustering-on-Graphs"><a href="#Curvature-based-Clustering-on-Graphs" class="headerlink" title="Curvature-based Clustering on Graphs"></a>Curvature-based Clustering on Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10155">http://arxiv.org/abs/2307.10155</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/agosztolai/geometric_clustering">https://github.com/agosztolai/geometric_clustering</a></li>
<li>paper_authors: Yu Tian, Zachary Lubberts, Melanie Weber</li>
<li>for: 本研究探讨了基于图形学的无监督节点归一化（或社区探测）算法，以找到图中紧密连接的子结构，即社区或群体。</li>
<li>methods: 本方法利用图形学的几何特性，通过对图的边权进行变换，以揭示图中的社区结构。我们考虑了多种离散曲率概念，并分析了它们所导致的算法的实用性。</li>
<li>results: 我们提供了许多理论和实验证明，证明了我们的曲率基于归一化算法的实用性。此外，我们还给出了一些关于图形学 curvature和其对副图的关系的结果，这些结果可能对 curvature-based 网络分析有独立的价值。<details>
<summary>Abstract</summary>
Unsupervised node clustering (or community detection) is a classical graph learning task. In this paper, we study algorithms, which exploit the geometry of the graph to identify densely connected substructures, which form clusters or communities. Our method implements discrete Ricci curvatures and their associated geometric flows, under which the edge weights of the graph evolve to reveal its community structure. We consider several discrete curvature notions and analyze the utility of the resulting algorithms. In contrast to prior literature, we study not only single-membership community detection, where each node belongs to exactly one community, but also mixed-membership community detection, where communities may overlap. For the latter, we argue that it is beneficial to perform community detection on the line graph, i.e., the graph's dual. We provide both theoretical and empirical evidence for the utility of our curvature-based clustering algorithms. In addition, we give several results on the relationship between the curvature of a graph and that of its dual, which enable the efficient implementation of our proposed mixed-membership community detection approach and which may be of independent interest for curvature-based network analysis.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>古典图学任务之一是无监督节点划分（或社区探测）。在这篇论文中，我们研究使用图形的几何特性来找出密集连接的子结构，这些子结构组成社区或共同体。我们的方法使用离散 Ricci  curvature 和其相关的几何流动，以便在图形中Edge weights 的演化中揭示社区结构。我们考虑了多种离散 curvature 观念，并分析了其结果的有用性。与先前的文献不同，我们研究不只单一成员社区检测，而是也包括交叉成员社区检测，其中社区可能 overlap。为实现这种检测，我们 argue 是在图形的 dual 上进行社区检测，即 graph 的 dual。我们提供了 both theoretical 和 empirical 证明，以及一些关于离散 curvature 和其 dual 之间的关系，这些关系可能为 curvature-based 网络分析而有益，并且可能具有独立的 интерес。
</details></li>
</ul>
<hr>
<h2 id="Code-Detection-for-Hardware-Acceleration-Using-Large-Language-Models"><a href="#Code-Detection-for-Hardware-Acceleration-Using-Large-Language-Models" class="headerlink" title="Code Detection for Hardware Acceleration Using Large Language Models"></a>Code Detection for Hardware Acceleration Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10348">http://arxiv.org/abs/2307.10348</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Pablo Antonio Martínez, Gregorio Bernabé, José Manuel García</li>
<li>for: 本研究探讨了使用大型自然语言模型（LLM）进行代码检测。</li>
<li>methods: 本研究使用了C&#x2F;C++实现的基本kernels，包括矩阵乘法、卷积和快速傅立叶变换，并提出了一个初步的Naive提示和一种新的提示策略来进行代码检测。</li>
<li>results: 研究发现，使用传统提示策略可以很准确地检测代码，但是受到许多假阳性干扰。使用新的提示策略可以大幅减少假阳性，实现了代码检测的优秀总准确率（91.1%, 97.9%, 和99.7%），这些结果对现有的代码检测方法 pose 了一定的挑战。<details>
<summary>Abstract</summary>
Large language models (LLMs) have been massively applied to many tasks, often surpassing state-of-the-art approaches. While their effectiveness in code generation has been extensively studied (e.g., AlphaCode), their potential for code detection remains unexplored.   This work presents the first analysis of code detection using LLMs. Our study examines essential kernels, including matrix multiplication, convolution, and fast-fourier transform, implemented in C/C++. We propose both a preliminary, naive prompt and a novel prompting strategy for code detection.   Results reveal that conventional prompting achieves great precision but poor accuracy (68.8%, 22.3%, and 79.2% for GEMM, convolution, and FFT, respectively) due to a high number of false positives. Our novel prompting strategy substantially reduces false positives, resulting in excellent overall accuracy (91.1%, 97.9%, and 99.7%, respectively). These results pose a considerable challenge to existing state-of-the-art code detection methods.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经广泛应用于许多任务，经常超越现有的方法。然而，它们在代码检测方面的潜力还没有得到足够的研究。本研究是首次对代码检测使用 LLM 进行分析。我们的研究检查了关键kernels，包括矩阵乘法、卷积和快速傅立叶Transform，实现在C/C++中。我们提出了一种初步的简单提示和一种新的提示策略 для代码检测。结果显示，传统的提示方法可以达到很高的准确率（68.8%、22.3%和79.2%），但是受到许多假阳性的影响，导致总准确率较低（22.3%）。我们的新的提示策略可以减少假阳性，从而实现了优秀的总准确率（91.1%、97.9%和99.7%）。这些结果对现有的代码检测方法提出了很大的挑战。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Potential-Based-Rewards-for-Learning-Humanoid-Locomotion"><a href="#Benchmarking-Potential-Based-Rewards-for-Learning-Humanoid-Locomotion" class="headerlink" title="Benchmarking Potential Based Rewards for Learning Humanoid Locomotion"></a>Benchmarking Potential Based Rewards for Learning Humanoid Locomotion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10142">http://arxiv.org/abs/2307.10142</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/se-hwan/pbrs-humanoid">https://github.com/se-hwan/pbrs-humanoid</a></li>
<li>paper_authors: Se Hwan Jeon, Steve Heim, Charles Khazoom, Sangbae Kim</li>
<li>for: 本研究旨在探讨可能基 reward shaping（PBRS）在人工智能中的应用，以加速学习过程的整合。</li>
<li>methods: 本研究使用了标准的 reward shaping 方法和 PBRS 方法，对人工智能机器人进行了比较。</li>
<li>results: 研究发现，在高维系统中，PBRS 对学习速度的影响是有限的，但 PBRS 奖金项是比标准奖金项更鲁棒地Scaling。<details>
<summary>Abstract</summary>
The main challenge in developing effective reinforcement learning (RL) pipelines is often the design and tuning the reward functions. Well-designed shaping reward can lead to significantly faster learning. Naively formulated rewards, however, can conflict with the desired behavior and result in overfitting or even erratic performance if not properly tuned. In theory, the broad class of potential based reward shaping (PBRS) can help guide the learning process without affecting the optimal policy. Although several studies have explored the use of potential based reward shaping to accelerate learning convergence, most have been limited to grid-worlds and low-dimensional systems, and RL in robotics has predominantly relied on standard forms of reward shaping. In this paper, we benchmark standard forms of shaping with PBRS for a humanoid robot. We find that in this high-dimensional system, PBRS has only marginal benefits in convergence speed. However, the PBRS reward terms are significantly more robust to scaling than typical reward shaping approaches, and thus easier to tune.
</details>
<details>
<summary>摘要</summary>
主要挑战在开发有效的回归学习（RL）管道是设计和调整奖金函数。有效的奖金函数设计可以导致更快的学习。然而，粗制的奖金函数可能会与期望的行为冲突，从而导致过拟合或者异常的表现，如果不当调整。在理论上，广泛的可能基于奖金函数形成（PBRS）可以帮助学习过程进行导航，不会影响最佳策略。虽然一些研究已经探讨了使用PBRS加速学习的潜在优势，但大多数研究仅限于网格世界和低维系统，RL在机器人领域主要依靠标准的奖金函数形成。在这篇论文中，我们将标准的奖金函数形成与PBRS进行比较，对一个人类机器人进行了测试。我们发现在这个高维系统中，PBRS只有微妙的加速学习的速度。然而，PBRS的奖金函数是标准奖金函数形成方法更加鲁棒对缩放的，因此更容易调整。
</details></li>
</ul>
<hr>
<h2 id="ProtiGeno-a-prokaryotic-short-gene-finder-using-protein-language-models"><a href="#ProtiGeno-a-prokaryotic-short-gene-finder-using-protein-language-models" class="headerlink" title="ProtiGeno: a prokaryotic short gene finder using protein language models"></a>ProtiGeno: a prokaryotic short gene finder using protein language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10343">http://arxiv.org/abs/2307.10343</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tonytu16/protigeno">https://github.com/tonytu16/protigeno</a></li>
<li>paper_authors: Tony Tu, Gautham Krishna, Amirali Aghazadeh</li>
<li>for: 本研究的目的是提高脱氧核糖核酸生物体中短读取识别率。</li>
<li>methods: 该研究使用深度学习方法， Specifically targeting short prokaryotic genes using a protein language model trained on millions of evolved proteins。</li>
<li>results: 在4,288个脱氧核糖核酸 genomes 中进行系统性大规模实验，显示 ProtiGeno 能够更高精度和回归率地预测短编译和非编译基因。<details>
<summary>Abstract</summary>
Prokaryotic gene prediction plays an important role in understanding the biology of organisms and their function with applications in medicine and biotechnology. Although the current gene finders are highly sensitive in finding long genes, their sensitivity decreases noticeably in finding shorter genes (<180 nts). The culprit is insufficient annotated gene data to identify distinguishing features in short open reading frames (ORFs). We develop a deep learning-based method called ProtiGeno, specifically targeting short prokaryotic genes using a protein language model trained on millions of evolved proteins. In systematic large-scale experiments on 4,288 prokaryotic genomes, we demonstrate that ProtiGeno predicts short coding and noncoding genes with higher accuracy and recall than the current state-of-the-art gene finders. We discuss the predictive features of ProtiGeno and possible limitations by visualizing the three-dimensional structure of the predicted short genes. Data, codes, and models are available at https://github.com/tonytu16/protigeno.
</details>
<details>
<summary>摘要</summary>
probiotic gene prediction 在理解生物体的生物学和功能方面发挥重要作用，有医学和生物技术应用。 although current gene finders 非常敏感于发现长ogene，但是它们对短ogene（<180 nts）的敏感度显著下降。 问题在于缺乏可靠的annotated gene数据，以便识别短 open reading frames (ORFs) 中的特征。 we develop a deep learning-based method called ProtiGeno, 专门针对短抗原生物体gene。 through systematic large-scale experiments on 4,288 prokaryotic genomes, we demonstrate that ProtiGeno can predict short coding and noncoding genes with higher accuracy and recall than the current state-of-the-art gene finders. we discuss the predictive features of ProtiGeno and possible limitations by visualizing the three-dimensional structure of the predicted short genes. data, codes, and models are available at https://github.com/tonytu16/protigeno.
</details></li>
</ul>
<hr>
<h2 id="Gradient-Sparsification-For-Masked-Fine-Tuning-of-Transformers"><a href="#Gradient-Sparsification-For-Masked-Fine-Tuning-of-Transformers" class="headerlink" title="Gradient Sparsification For Masked Fine-Tuning of Transformers"></a>Gradient Sparsification For Masked Fine-Tuning of Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10098">http://arxiv.org/abs/2307.10098</a></li>
<li>repo_url: None</li>
<li>paper_authors: James O’ Neill, Sourav Dutta</li>
<li>for: 这 paper 是研究如何使用梯度抑制来改进预训练语言模型的过程。</li>
<li>methods: 这 paper 使用了 GradDrop 和其变种来强制梯度抑制，用于让预训练语言模型进行更好的微调。</li>
<li>results:  experiments 表明，GradDrop 可以与使用额外翻译数据进行中间预训练相比，并且超过标准微调和慢滑块释放。另外，一个后续分析表明，GradDrop 可以在未经训练的语言上提高性能。<details>
<summary>Abstract</summary>
Fine-tuning pretrained self-supervised language models is widely adopted for transfer learning to downstream tasks. Fine-tuning can be achieved by freezing gradients of the pretrained network and only updating gradients of a newly added classification layer, or by performing gradient updates on all parameters. Gradual unfreezing makes a trade-off between the two by gradually unfreezing gradients of whole layers during training. This has been an effective strategy to trade-off between storage and training speed with generalization performance. However, it is not clear whether gradually unfreezing layers throughout training is optimal, compared to sparse variants of gradual unfreezing which may improve fine-tuning performance. In this paper, we propose to stochastically mask gradients to regularize pretrained language models for improving overall fine-tuned performance. We introduce GradDrop and variants thereof, a class of gradient sparsification methods that mask gradients during the backward pass, acting as gradient noise. GradDrop is sparse and stochastic unlike gradual freezing. Extensive experiments on the multilingual XGLUE benchmark with XLMR-Large show that GradDrop is competitive against methods that use additional translated data for intermediate pretraining and outperforms standard fine-tuning and gradual unfreezing. A post-analysis shows how GradDrop improves performance with languages it was not trained on, such as under-resourced languages.
</details>
<details>
<summary>摘要</summary>
通用自学习语言模型的精度调整广泛采用转移学习到下游任务。精度调整可以通过冻结预训练网络的梯度并只更新新增的分类层的梯度，或者通过所有参数的梯度更新来实现。渐进冻结实现了一种在训练中逐渐解冻整个层的梯度，这是一种训练速度和存储空间之间的融合策略。然而，是否可以在训练中逐渐解冻整个层的梯度是最佳的，而不是使用随机的梯度抑制方法来改进精度调整表现。在这篇论文中，我们提出了在反向传播中随机抑制梯度的方法，即GradDrop和其变种。GradDrop是一种随机梯度抑制方法，在反向传播中随机地抑制梯度，与渐进冻结不同。我们在多语言XGLUE标准测试集上进行了广泛的实验，结果表明GradDrop和其变种与使用额外翻译数据进行中间预训练的方法相当竞争，并且超过标准的精度调整和渐进冻结。另外，我们还进行了后期分析，发现GradDrop在不同语言上的表现提高，包括资源不足的语言。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-invariances-and-introducing-priors-in-Gromov-Wasserstein-distances"><a href="#Revisiting-invariances-and-introducing-priors-in-Gromov-Wasserstein-distances" class="headerlink" title="Revisiting invariances and introducing priors in Gromov-Wasserstein distances"></a>Revisiting invariances and introducing priors in Gromov-Wasserstein distances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10093">http://arxiv.org/abs/2307.10093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pinar Demetci, Quang Huy Tran, Ievgen Redko, Ritambhara Singh</li>
<li>For: 本研究旨在提出一种新的优化的格罗莫夫-瓦asserstein距离，以提高机器学习 task 的性能。* Methods: 本研究使用了一种新的augmented Gromov-Wasserstein distance，该距离可以在某种程度上控制 transformations 的灵活性，并且可以更好地利用输入数据的特征对Alignment。* Results: 研究表明，augmented Gromov-Wasserstein distance 可以在单元细胞多Omic alignment和机器学习 transfer learning 等场景中提高性能。<details>
<summary>Abstract</summary>
Gromov-Wasserstein distance has found many applications in machine learning due to its ability to compare measures across metric spaces and its invariance to isometric transformations. However, in certain applications, this invariance property can be too flexible, thus undesirable. Moreover, the Gromov-Wasserstein distance solely considers pairwise sample similarities in input datasets, disregarding the raw feature representations. We propose a new optimal transport-based distance, called Augmented Gromov-Wasserstein, that allows for some control over the level of rigidity to transformations. It also incorporates feature alignments, enabling us to better leverage prior knowledge on the input data for improved performance. We present theoretical insights into the proposed metric. We then demonstrate its usefulness for single-cell multi-omic alignment tasks and a transfer learning scenario in machine learning.
</details>
<details>
<summary>摘要</summary>
格罗莫夫-瓦萨希恩距离在机器学习中发现了许多应用，因为它可以比较度量空间中的度量并具有对称变换不变性。然而，在某些应用中，这种不变性属性可能太 flexibility，因此不 desired。此外，格罗莫夫-瓦萨希恩距离只考虑输入数据集中的对比样本相似性，忽略原始特征表示。我们提出一种新的最优运输基于距离，called Augmented Gromov-Wasserstein,可以控制变换的水平弹性。它还包含特征对应， allowing us to better leverage prior knowledge on the input data for improved performance。我们提供了关于提posed metric的理论探讨。然后，我们用单元细胞多OmicAlignment任务和机器学习的传输学习场景来示例出其用于。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/07/20/cs.LG_2023_07_20/" data-id="cllt6z4ic001oq388gxpkef8o" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/21/eess.IV_2023_07_21/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.IV - 2023-07-21 17:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/20/cs.SD_2023_07_20/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-07-20 123:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
