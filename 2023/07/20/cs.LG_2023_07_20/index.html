
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-07-20 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Synthetic Control Methods by Density Matching under Implicit Endogeneity paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.11127 repo_url: None paper_authors: Masahiro Kato, Akari Ohda, Masaaki Imaizumi, Kenichiro">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-07-20">
<meta property="og:url" content="https://nullscc.github.io/2023/07/20/cs.LG_2023_07_20/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Synthetic Control Methods by Density Matching under Implicit Endogeneity paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.11127 repo_url: None paper_authors: Masahiro Kato, Akari Ohda, Masaaki Imaizumi, Kenichiro">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-20T10:00:00.000Z">
<meta property="article:modified_time" content="2023-09-23T08:38:57.313Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_07_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/20/cs.LG_2023_07_20/" class="article-date">
  <time datetime="2023-07-20T10:00:00.000Z" itemprop="datePublished">2023-07-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-07-20
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Synthetic-Control-Methods-by-Density-Matching-under-Implicit-Endogeneity"><a href="#Synthetic-Control-Methods-by-Density-Matching-under-Implicit-Endogeneity" class="headerlink" title="Synthetic Control Methods by Density Matching under Implicit Endogeneity"></a>Synthetic Control Methods by Density Matching under Implicit Endogeneity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11127">http://arxiv.org/abs/2307.11127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masahiro Kato, Akari Ohda, Masaaki Imaizumi, Kenichiro McAlinn</li>
<li>for: æœ¬ç ”ç©¶ä½¿ç”¨Synthetic controlæ–¹æ³•ï¼ˆSCMï¼‰ä¼°è®¡å¯¹æ¯”æ¡ˆä¾‹ç ”ç©¶ä¸­çš„æ•ˆæœï¼ŒSCMå¯ä»¥ä¼°è®¡å¯¹å¾…å•ä½çš„Counterfactual outcomeï¼Œå¹¶ä¸”æ˜¯å¯¹æ¯”æ¡ˆä¾‹ç ”ç©¶ä¸­çš„ä¸€ç§é‡è¦å·¥å…·ã€‚</li>
<li>methods: æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„SCMæ–¹æ³•ï¼ŒåŸºäºdensity matchingå‡è®¾ï¼Œå³å¯¹å¾…å•ä½çš„ç»“æœdensityå¯ä»¥è¢« aproximatedä¸ºä¸€ä¸ªæƒé‡åŠ æƒçš„ mixture modelã€‚é€šè¿‡è¿™ä¸ªå‡è®¾ï¼Œæˆ‘ä»¬å¯ä»¥ä¼°è®¡SC weightsï¼Œå¹¶ä¸”æˆ‘ä»¬çš„ä¼°è®¡å™¨å…·æœ‰ä¸‰ä¸ªä¼˜ç‚¹ï¼šä¸€ã€æˆ‘ä»¬çš„ä¼°è®¡å™¨æ˜¯ asymptotically unbiased; twoã€æˆ‘ä»¬å¯ä»¥é™ä½å¯¹counterfactual predictionçš„mean squared error; threeã€æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ç”Ÿæˆå¯¹å¾…å•ä½çš„æ²»ç–—æ•ˆæœçš„å…¨ä½“åˆ†å¸ƒï¼Œä¸ä»…æ˜¯é¢„æœŸå€¼ã€‚</li>
<li>results: æœ¬ç ”ç©¶é€šè¿‡å®éªŒç»“æœå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•çš„æ•ˆæœï¼Œå¹¶ä¸”è¯æ˜äº†å…¶æ¯”æ—¢æœ‰SCMæ–¹æ³•æ›´åŠ ç²¾å‡†å’Œæœ‰æ•ˆã€‚<details>
<summary>Abstract</summary>
Synthetic control methods (SCMs) have become a crucial tool for causal inference in comparative case studies. The fundamental idea of SCMs is to estimate counterfactual outcomes for a treated unit by using a weighted sum of observed outcomes from untreated units. The accuracy of the synthetic control (SC) is critical for estimating the causal effect, and hence, the estimation of SC weights has been the focus of much research. In this paper, we first point out that existing SCMs suffer from an implicit endogeneity problem, which is the correlation between the outcomes of untreated units and the error term in the model of a counterfactual outcome. We show that this problem yields a bias in the causal effect estimator. We then propose a novel SCM based on density matching, assuming that the density of outcomes of the treated unit can be approximated by a weighted average of the densities of untreated units (i.e., a mixture model). Based on this assumption, we estimate SC weights by matching moments of treated outcomes and the weighted sum of moments of untreated outcomes. Our proposed method has three advantages over existing methods. First, our estimator is asymptotically unbiased under the assumption of the mixture model. Second, due to the asymptotic unbiasedness, we can reduce the mean squared error for counterfactual prediction. Third, our method generates full densities of the treatment effect, not only expected values, which broadens the applicability of SCMs. We provide experimental results to demonstrate the effectiveness of our proposed method.
</details>
<details>
<summary>æ‘˜è¦</summary>
Synthetic control methods (SCMs) å·²æˆä¸ºæ¯”è¾ƒç ”ç©¶ä¸­çš„é‡è¦å·¥å…·ï¼Œç”¨äºä¼°è®¡ causal inferenceã€‚ SCMs çš„åŸºæœ¬æ€æƒ³æ˜¯ä½¿ç”¨ä¸€ä¸ªæƒé‡å’Œå¹³âˆ‘ è§‚å¯Ÿåˆ°çš„ç»“æœæ¥ä¼°è®¡å¯¹å¾…å•ä½çš„ counterfactual ç»“æœã€‚ SC çš„å‡†ç¡®æ€§æ˜¯ä¼°è®¡ causal effect çš„å…³é”®ï¼Œå› æ­¤ SCMs çš„ estimation é—®é¢˜å·²ç»å¼•èµ·äº†å¾ˆå¤šç ”ç©¶ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆæŒ‡å‡ºäº†ç°æœ‰ SCMs å­˜åœ¨ä¸€ç§éšè—çš„å†…ç”Ÿæ€§é—®é¢˜ï¼Œå³å¯¹ untreated units çš„ç»“æœå’Œ counterfactual ç»“æœæ¨¡å‹ä¸­çš„é”™è¯¯é¡¹ä¹‹é—´çš„ç›¸å…³æ€§ã€‚æˆ‘ä»¬è¯æ˜äº†è¿™ä¸ªé—®é¢˜ä¼šå¯¼è‡´ causal effect ä¼°è®¡å™¨åç§»ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäº density matching çš„æ–°çš„ SCMï¼Œå‡è®¾å¾…å¤„ç†å•ä½çš„ç»“æœçš„æ¦‚ç‡å¯ä»¥é€šè¿‡ä¸€ä¸ªæƒé‡å’Œå¹³âˆ‘ untreated units çš„ç»“æœæ¦‚ç‡æ¥è¿‘ä¼¼ã€‚åŸºäºè¿™ä¸ªå‡è®¾ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡åŒ¹é…å¾…å¤„ç†ç»“æœçš„ moments å’Œæƒé‡å’Œå¹³âˆ‘ untreated units çš„ momentsæ¥ä¼°è®¡ SC æƒé‡ã€‚æˆ‘ä»¬çš„æè®®æ–¹æ³•æœ‰ä¸‰ä¸ªä¼˜ç‚¹ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬çš„ä¼°è®¡å™¨åœ¨ mixture model çš„å‡è®¾ä¸‹æ˜¯ asymptotically unbiasedã€‚å…¶æ¬¡ï¼Œç”±äº asymptotic unbiasednessï¼Œæˆ‘ä»¬å¯ä»¥é™ä½ counterfactual prediction çš„ mean squared errorã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ç”Ÿæˆå¯¹å¾…å•ä½çš„æ²»ç–—æ•ˆæœçš„å…¨éƒ¨æ¦‚ç‡åˆ†å¸ƒï¼Œä¸ä»…æ˜¯é¢„æœŸå€¼ï¼Œè¿™æ‰©å±•äº† SCMs çš„åº”ç”¨èŒƒå›´ã€‚æˆ‘ä»¬æä¾›å®éªŒç»“æœï¼Œä»¥è¯æ˜æˆ‘ä»¬çš„æè®®æ–¹æ³•çš„æ•ˆæœã€‚
</details></li>
</ul>
<hr>
<h2 id="A-Markov-Chain-Model-for-Identifying-Changes-in-Daily-Activity-Patterns-of-People-Living-with-Dementia"><a href="#A-Markov-Chain-Model-for-Identifying-Changes-in-Daily-Activity-Patterns-of-People-Living-with-Dementia" class="headerlink" title="A Markov Chain Model for Identifying Changes in Daily Activity Patterns of People Living with Dementia"></a>A Markov Chain Model for Identifying Changes in Daily Activity Patterns of People Living with Dementia</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11126">http://arxiv.org/abs/2307.11126</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nvfl/markov-chain-model">https://github.com/nvfl/markov-chain-model</a></li>
<li>paper_authors: Nan Fletcher-Lloyd, Alina-Irina Serban, Magdalena Kolanko, David Wingfield, Danielle Wilson, Ramin Nilforooshan, Payam Barnaghi, Eyal Soreq</li>
<li>for: è¿™ä¸ªç ”ç©¶æ˜¯ä¸ºäº†æ£€æµ‹è€äººç”Ÿæ´»åœ¨å›°éš¾ä¸­çš„è¥å…»å’Œæ¶²ä½“æ¶ˆè€—æƒ…å†µï¼Œä»¥åŠå¯¹è¿™äº›æƒ…å†µçš„å½±å“ã€‚</li>
<li>methods: è¿™ä¸ªç ”ç©¶ä½¿ç”¨äº†äº’è”ç½‘ç‰©è”ç½‘æŠ€æœ¯æ”¶é›†äº†73æˆ·è€äººç”Ÿæ´»åœ¨å®¶ä¸­çš„æ•°æ®ï¼Œå¹¶ä½¿ç”¨çº¿æ€§æ··åˆæ•ˆåº”åˆ†ææ£€æµ‹äº†COVID-19å¤§æµè¡Œå¯¹è€äººçš„åƒé¥­å’Œå–æ°´ä¹ æƒ¯çš„å½±å“ã€‚</li>
<li>results: ç ”ç©¶å‘ç°åœ¨ç™½å¤©ï¼Œè€äººçš„å¨æˆ¿æ´»åŠ¨å¢åŠ äº†ï¼ˆt(147) &#x3D; -2.90ï¼Œp &lt; 0.001ï¼‰ï¼Œè€Œåœ¨å¤œæ™šï¼Œå¨æˆ¿æ´»åŠ¨å‡å°‘äº†ï¼ˆt(147) &#x3D; -2.90ï¼Œp &lt; 0.001ï¼‰ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§åŸºäºè¥å…»æ¨¡å‹çš„æ–¹æ³•æ¥æ£€æµ‹è€äººçš„è¡Œä¸ºå˜åŒ–ã€‚<details>
<summary>Abstract</summary>
Malnutrition and dehydration are strongly associated with increased cognitive and functional decline in people living with dementia (PLWD), as well as an increased rate of hospitalisations in comparison to their healthy counterparts. Extreme changes in eating and drinking behaviours can often lead to malnutrition and dehydration, accelerating the progression of cognitive and functional decline and resulting in a marked reduction in quality of life. Unfortunately, there are currently no established methods by which to objectively detect such changes. Here, we present the findings of an extensive quantitative analysis conducted on in-home monitoring data collected from 73 households of PLWD using Internet of Things technologies. The Coronavirus 2019 (COVID-19) pandemic has previously been shown to have dramatically altered the behavioural habits, particularly the eating and drinking habits, of PLWD. Using the COVID-19 pandemic as a natural experiment, we conducted linear mixed-effects modelling to examine changes in mean kitchen activity within a subset of 21 households of PLWD that were continuously monitored for 499 days. We report an observable increase in day-time kitchen activity and a significant decrease in night-time kitchen activity (t(147) = -2.90, p < 0.001). We further propose a novel analytical approach to detecting changes in behaviours of PLWD using Markov modelling applied to remote monitoring data as a proxy for behaviours that cannot be directly measured. Together, these results pave the way to introduce improvements into the monitoring of PLWD in naturalistic settings and for shifting from reactive to proactive care.
</details>
<details>
<summary>æ‘˜è¦</summary>
ğŸ‡¨ğŸ‡³ è¥å…»ä¸è‰¯å’Œè‚¥è™šæ˜¯è€å¹´äººæ™ºèƒ½å’ŒåŠŸèƒ½é€€åŒ–çš„é‡è¦é£é™©å› ç´ ï¼Œä¹Ÿä¼šä½¿äººä»¬æ‚£æœ‰æ™ºèƒ½å’ŒåŠŸèƒ½é€€åŒ–çš„äººç¾¤ï¼ˆPLWDï¼‰çš„å…¥é™¢ç‡å¢åŠ ã€‚å®½æ³›çš„é£Ÿå“å’Œé¥®æ–™æ¶ˆè€—æ–¹å¼çš„å˜åŒ–å¯èƒ½ä¼šå¯¼è‡´è¥å…»ä¸è‰¯å’Œè‚¥è™šï¼ŒåŠ é€Ÿæ™ºèƒ½å’ŒåŠŸèƒ½é€€åŒ–çš„è¿›ç¨‹ï¼Œä»è€Œå¯¼è‡´ç”Ÿæ´»è´¨é‡ä¸‹é™ã€‚å¯æƒœï¼Œç›®å‰æ²¡æœ‰å¯é çš„æ–¹æ³•å¯ä»¥ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²åœ°æ¢æµ‹è¿™äº›å˜åŒ–ã€‚æˆ‘ä»¬åœ¨73æˆ·è€å¹´äººæ™ºèƒ½å’ŒåŠŸèƒ½é€€åŒ–è€…çš„å®¶åº­ä¸­è¿›è¡Œäº†å¹¿æ³›çš„é‡åŒ–åˆ†æï¼Œä½¿ç”¨äº’è”ç½‘ç‰©è”ç½‘æŠ€æœ¯æ”¶é›†æ•°æ®ã€‚2019å† çŠ¶ç—…æ¯’ç–«æƒ…ï¼ˆCOVID-19ï¼‰å·²ç»å¯¹æ™ºèƒ½å’ŒåŠŸèƒ½é€€åŒ–è€…çš„è¡Œä¸ºä¹ æƒ¯äº§ç”Ÿäº†æ·±è¿œçš„å½±å“ï¼Œç‰¹åˆ«æ˜¯é¥®é£Ÿå’Œé¥®æ–™çš„æ¶ˆè€—æ–¹å¼ã€‚ä½¿ç”¨2019å† çŠ¶ç—…æ¯’ç–«æƒ…ä½œä¸ºè‡ªç„¶å®éªŒï¼Œæˆ‘ä»¬ä½¿ç”¨çº¿æ€§æ··åˆæ•ˆåº”æ¨¡å‹å¯¹21æˆ·æ™ºèƒ½å’ŒåŠŸèƒ½é€€åŒ–è€…çš„24å°æ—¶å†…çš„å¨æˆ¿æ´»åŠ¨è¿›è¡Œåˆ†æã€‚æˆ‘ä»¬å‘ç°äº†æ—¥é—´å¨æˆ¿æ´»åŠ¨çš„ observable å¢åŠ ï¼ˆt(147) = -2.90ï¼Œp < 0.001ï¼‰ï¼Œå¹¶ä¸”å‘ç°å¤œé—´å¨æˆ¿æ´»åŠ¨çš„æ˜¾è‘—å‡å°‘ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºè¿œç¨‹ç›‘æµ‹æ•°æ®çš„Markovæ¨¡å‹ï¼Œç”¨äºæ£€æµ‹æ™ºèƒ½å’ŒåŠŸèƒ½é€€åŒ–è€…çš„è¡Œä¸ºå˜åŒ–ã€‚è¿™äº›ç»“æœå°†ä¸ºç›‘æµ‹æ™ºèƒ½å’ŒåŠŸèƒ½é€€åŒ–è€…çš„ç›‘æµ‹æä¾›æ–°çš„æ–¹æ³•ï¼Œå¹¶ä¸”å¯ä»¥å¸®åŠ©è½¬æ¢åˆ°ä¸»åŠ¨ç›‘æµ‹ã€‚
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Models-for-Probabilistic-Deconvolution-of-Galaxy-Images"><a href="#Diffusion-Models-for-Probabilistic-Deconvolution-of-Galaxy-Images" class="headerlink" title="Diffusion Models for Probabilistic Deconvolution of Galaxy Images"></a>Diffusion Models for Probabilistic Deconvolution of Galaxy Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11122">http://arxiv.org/abs/2307.11122</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yashpatel5400/galgen">https://github.com/yashpatel5400/galgen</a></li>
<li>paper_authors: Zhiwei Xue, Yuhang Li, Yash Patel, Jeffrey Regier</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ˜¯ä¸ºäº†æå‡ºä¸€ç§åŸºäºæ·±åº¦ç”Ÿæˆæ¨¡å‹çš„PSFé€†æ¨ç®—æ³•ï¼Œç”¨äºæ¢å¤å®‡å®™å›¾åƒä¸­çš„ç»†èŠ‚ã€‚</li>
<li>methods: è¯¥è®ºæ–‡ä½¿ç”¨äº†ä¸€ç§åŸºäºæ™®é€š diffusion æ¨¡å‹çš„æ–¹æ³•ï¼Œä¸éœ€è¦ç±»åˆ«å™¨ï¼Œå¯ä»¥æ›´å¥½åœ°æå–å®‡å®™å›¾åƒä¸­çš„ç»†èŠ‚ã€‚</li>
<li>results: è®ºæ–‡çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäº diffusion æ¨¡å‹çš„PSFé€†æ¨ç®—æ³•å¯ä»¥æ›´å¥½åœ°æ•æ‰å®‡å®™å›¾åƒä¸­çš„ç»†èŠ‚ï¼Œå¹¶ä¸”æä¾›äº†æ›´å¤šçš„å¯èƒ½æ€§ç©ºé—´ï¼Œæ¯”å¦‚ conditional VAE çš„æ–¹æ³•ã€‚<details>
<summary>Abstract</summary>
Telescopes capture images with a particular point spread function (PSF). Inferring what an image would have looked like with a much sharper PSF, a problem known as PSF deconvolution, is ill-posed because PSF convolution is not an invertible transformation. Deep generative models are appealing for PSF deconvolution because they can infer a posterior distribution over candidate images that, if convolved with the PSF, could have generated the observation. However, classical deep generative models such as VAEs and GANs often provide inadequate sample diversity. As an alternative, we propose a classifier-free conditional diffusion model for PSF deconvolution of galaxy images. We demonstrate that this diffusion model captures a greater diversity of possible deconvolutions compared to a conditional VAE.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤©æ–‡æœ›è¿œé•œæ•æ‰åˆ°å›¾åƒï¼Œä½†å›¾åƒå…·æœ‰ç‰¹å®šçš„ç‚¹æ‰©æ•£å‡½æ•°ï¼ˆPSFï¼‰ã€‚å°è¯•æ¢å¤å›¾åƒä¸ºæ›´åŠ é”åˆ©PSFåçš„å½¢æ€ï¼Œç§°ä¸ºPSFæ¢å¤ï¼Œæ˜¯ä¸€ä¸ªä¸å®šé—®é¢˜ï¼Œå› ä¸ºPSFæ··åˆä¸æ˜¯å¯é€†å˜æ¢ã€‚æ·±åº¦ç”Ÿæˆæ¨¡å‹å¸å¼•äº†PSFæ¢å¤çš„åº”ç”¨ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥å¯¹å€™é€‰å›¾åƒè¿›è¡Œ posterior åˆ†å¸ƒé¢„æµ‹ï¼Œå¦‚æœå°†å…¶æ··åˆåˆ°PSFä¸­ï¼Œå¯èƒ½ä¼šç”Ÿæˆè§‚æµ‹ç»“æœã€‚ç„¶è€Œï¼Œç»å…¸çš„æ·±åº¦ç”Ÿæˆæ¨¡å‹å¦‚VAEså’ŒGANsç»å¸¸æä¾›ä¸å¤Ÿçš„æ ·æœ¬å¤šæ ·æ€§ã€‚ä¸ºäº†è§£å†³è¿™é—®é¢˜ï¼Œæˆ‘ä»¬æè®®ä¸€ç§æ— ç±»åˆ«çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ Ğ´Ğ»ÑPSFæ¢å¤æ˜Ÿç³»å›¾åƒã€‚æˆ‘ä»¬è¯æ˜è¯¥æ‰©æ•£æ¨¡å‹å¯ä»¥æ•æ‰æ›´å¤šçš„å¯èƒ½çš„æ¢å¤å½¢æ€ï¼Œæ¯” conditional VAE æ›´åŠ å¤šæ ·åŒ–ã€‚
</details></li>
</ul>
<hr>
<h2 id="PASTA-Pretrained-Action-State-Transformer-Agents"><a href="#PASTA-Pretrained-Action-State-Transformer-Agents" class="headerlink" title="PASTA: Pretrained Action-State Transformer Agents"></a>PASTA: Pretrained Action-State Transformer Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10936">http://arxiv.org/abs/2307.10936</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raphael Boige, Yannis Flet-Berliac, Arthur Flajolet, Guillaume Richard, Thomas Pierrot<br>for: This paper aims to investigate the use of pre-trained transformer models for reinforcement learning tasks, specifically addressing the problem of adapting models to new environments with limited data.methods: The authors use a unified methodology that includes tokenization at the action and state component level, training models across diverse domains simultaneously, and using parameter efficient fine-tuning (PEFT) to adapt the models to downstream tasks.results: The developed models contain fewer than 10 million parameters and can be fine-tuned with fewer than 10,000 parameters during downstream adaptation, allowing for robust policy learning and encouraging further research into the use of transformers for reinforcement learning.<details>
<summary>Abstract</summary>
Self-supervised learning has brought about a revolutionary paradigm shift in various computing domains, including NLP, vision, and biology. Recent approaches involve pre-training transformer models on vast amounts of unlabeled data, serving as a starting point for efficiently solving downstream tasks. In the realm of reinforcement learning, researchers have recently adapted these approaches by developing models pre-trained on expert trajectories, enabling them to address a wide range of tasks, from robotics to recommendation systems. However, existing methods mostly rely on intricate pre-training objectives tailored to specific downstream applications. This paper presents a comprehensive investigation of models we refer to as Pretrained Action-State Transformer Agents (PASTA). Our study uses a unified methodology and covers an extensive set of general downstream tasks including behavioral cloning, offline RL, sensor failure robustness, and dynamics change adaptation. Our goal is to systematically compare various design choices and provide valuable insights to practitioners for building robust models. Key highlights of our study include tokenization at the action and state component level, using fundamental pre-training objectives like next token prediction, training models across diverse domains simultaneously, and using parameter efficient fine-tuning (PEFT). The developed models in our study contain fewer than 10 million parameters and the application of PEFT enables fine-tuning of fewer than 10,000 parameters during downstream adaptation, allowing a broad community to use these models and reproduce our experiments. We hope that this study will encourage further research into the use of transformers with first-principles design choices to represent RL trajectories and contribute to robust policy learning.
</details>
<details>
<summary>æ‘˜è¦</summary>
è‡ªé¡¾å­¦å­¦ä¹ åœ¨ä¸åŒè®¡ç®—é¢†åŸŸä¸­å¼•å‘äº†é©å‘½æ€§çš„æ€ç»´æ–¹å¼å˜é©ï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€è§†è§‰å’Œç”Ÿç‰©å­¦ã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸æ˜¯ä½¿ç”¨åºå¤§é‡æœªæ ‡æ³¨æ•°æ®è¿›è¡Œé¢„è®­ç»ƒ transformer æ¨¡å‹ï¼Œä½œä¸ºä¸‹æ¸¸ä»»åŠ¡çš„å¼€å§‹ç‚¹ï¼Œä»¥æé«˜æ•ˆç‡ã€‚åœ¨è¿”å›å­¦ä¹ é¢†åŸŸï¼Œç ”ç©¶äººå‘˜å·²ç»é‡‡ç”¨äº†è¿™äº›æ–¹æ³•ï¼Œå¹¶å¼€å‘äº†åŸºäºä¸“å®¶è½¨è¿¹çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œä»¥è§£å†³å¹¿æ³›çš„ä»»åŠ¡ï¼Œä» Ñ€Ğ¾Ğ±Ğ¾ç‰¹æ–¯åˆ°æ¨èç³»ç»Ÿã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•é€šå¸¸ä»…é€‚ç”¨äºç‰¹å®šä¸‹æ¸¸åº”ç”¨ç¨‹åºçš„ç²¾ç»†é¢„è®­ç»ƒç›®æ ‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å«åš PASTA çš„æ¨¡å‹ï¼Œå…¶ä¸­åŒ…æ‹¬ä½¿ç”¨å„ç§è®¾è®¡é€‰æ‹©å’Œæ¶µç›–å¹¿æ³›çš„é€šç”¨ä¸‹æ¸¸ä»»åŠ¡ï¼ŒåŒ…æ‹¬è¡Œä¸ºåšcloneã€ç¦»çº¿å­¦ä¹ ã€æ„ŸçŸ¥æ•…éšœRobustnesså’ŒåŠ¨åŠ›å­¦å˜åŒ–é€‚åº”ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä½¿ç”¨ä¸€ç§ç»Ÿä¸€çš„æ–¹æ³•ologieså’Œæ¶µç›–äº†å¹¿æ³›çš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œä»¥ç³»ç»Ÿåœ°æ¯”è¾ƒä¸åŒçš„è®¾è®¡é€‰æ‹©ï¼Œå¹¶ä¸ºå®è·µè€…æä¾›æœ‰ä»·å€¼çš„æ´å¯Ÿã€‚å…³é”®ç‰¹ç‚¹åŒ…æ‹¬åŠ¨ä½œå’ŒçŠ¶æ€ç»„ä»¶çº§åˆ«çš„å¯ç”¨ï¼Œä½¿ç”¨åŸºæœ¬çš„é¢„è®­ç»ƒç›®æ ‡å¦‚ä¸‹ä¸€ä¸ªtokené¢„æµ‹ï¼Œåœ¨å¤šä¸ªé¢†åŸŸåŒæ—¶è®­ç»ƒæ¨¡å‹ï¼Œä»¥åŠä½¿ç”¨å‚æ•°æ•ˆç‡çš„ç»ƒä¹ ï¼ˆPEFTï¼‰ã€‚å¼€å‘çš„æ¨¡å‹ä¸­å« fewer than 10 million parametersï¼Œå¹¶ä¸”é€šè¿‡PEFTè¿›è¡Œå‚æ•°ç»ƒä¹ ï¼Œå¯ä»¥åœ¨ä¸‹æ¸¸é€‚åº”ä¸­ä½¿ç”¨ fewer than 10,000 parametersï¼Œä½¿å¾—å¹¿æ³›çš„ç¤¾åŒºå¯ä»¥ä½¿ç”¨è¿™äº›æ¨¡å‹å¹¶é‡ç°æˆ‘ä»¬çš„å®éªŒã€‚æˆ‘ä»¬å¸Œæœ›è¿™ç§ç ”ç©¶ä¼šé¼“åŠ±æ›´å¤šçš„äººä½¿ç”¨ transformer æ¨¡å‹çš„é¦–è¦åŸåˆ™æ¥è¡¨ç¤ºRLè½¨è¿¹ï¼Œå¹¶è´¡çŒ®äºRobust policyå­¦ä¹ ã€‚
</details></li>
</ul>
<hr>
<h2 id="Inorganic-synthesis-structure-maps-in-zeolites-with-machine-learning-and-crystallographic-distances"><a href="#Inorganic-synthesis-structure-maps-in-zeolites-with-machine-learning-and-crystallographic-distances" class="headerlink" title="Inorganic synthesis-structure maps in zeolites with machine learning and crystallographic distances"></a>Inorganic synthesis-structure maps in zeolites with machine learning and crystallographic distances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10935">http://arxiv.org/abs/2307.10935</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Schwalbe-Koda, Daniel E. Widdowson, Tuan Anh Pham, Vitaliy A. Kurlin</li>
<li>for: è¿™é¡¹ç ”ç©¶æ—¨åœ¨ä½¿ç”¨è®¡ç®—æœºæ¨¡æ‹Ÿå’Œæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æŠ€æœ¯ï¼Œä¸ºç¡…é…¸ç›ææ–™çš„åˆæˆåˆ›é€ æ— ç›‘ç£çš„ææ–™åˆæˆåœ°å›¾ã€‚</li>
<li>methods: è¯¥ç ”ç©¶ä½¿ç”¨äº†ä¸€ç§å¼ºå¤§çš„è·ç¦»åº¦é‡æ–¹æ³•å’Œæœºå™¨å­¦ä¹ åˆ†ææ–¹æ³•ï¼Œä»253ä¸ªå·²çŸ¥ç¡…é…¸ç›ä¸­æå–å‡ºä¸åŒçš„ææ–™åˆæˆæ¡ä»¶ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œåœ¨ä¸ä½¿ç”¨æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œé‚»è¿‘çš„ç¡…é…¸ç›ç»“æ„ä¹‹é—´çš„è·ç¦»åº¦é‡å¯ä»¥åæ˜ ç¡…é…¸ç›çš„ææ–™åˆæˆæ¡ä»¶ï¼Œå¹¶ä¸”å¯ä»¥é¢„æµ‹ç¡…é…¸ç›çš„åˆæˆç»“æœã€‚<details>
<summary>Abstract</summary>
Zeolites are inorganic materials known for their diversity of applications, synthesis conditions, and resulting polymorphs. Although their synthesis is controlled both by inorganic and organic synthesis conditions, computational studies of zeolite synthesis have focused mostly on organic template design. In this work, we use a strong distance metric between crystal structures and machine learning (ML) to create inorganic synthesis maps in zeolites. Starting with 253 known zeolites, we show how the continuous distances between frameworks reproduce inorganic synthesis conditions from the literature without using labels such as building units. An unsupervised learning analysis shows that neighboring zeolites according to our metric often share similar inorganic synthesis conditions, even in template-based routes. In combination with ML classifiers, we find synthesis-structure relationships for 14 common inorganic conditions in zeolites, namely Al, B, Be, Ca, Co, F, Ga, Ge, K, Mg, Na, P, Si, and Zn. By explaining the model predictions, we demonstrate how (dis)similarities towards known structures can be used as features for the synthesis space. Finally, we show how these methods can be used to predict inorganic synthesis conditions for unrealized frameworks in hypothetical databases and interpret the outcomes by extracting local structural patterns from zeolites. In combination with template design, this work can accelerate the exploration of the space of synthesis conditions for zeolites.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Modeling-3D-cardiac-contraction-and-relaxation-with-point-cloud-deformation-networks"><a href="#Modeling-3D-cardiac-contraction-and-relaxation-with-point-cloud-deformation-networks" class="headerlink" title="Modeling 3D cardiac contraction and relaxation with point cloud deformation networks"></a>Modeling 3D cardiac contraction and relaxation with point cloud deformation networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10927">http://arxiv.org/abs/2307.10927</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcel Beetz, Abhirup Banerjee, Vicente Grau</li>
<li>for: è¯¥ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§åŸºäºç‚¹äº‘æ·±åº¦å­¦ä¹ çš„å‡†ç¡®è¯„ä¼°ä¸‰ç»´å¿ƒè„åŠŸèƒ½çš„æ–¹æ³•ï¼Œä»¥æé«˜æˆ‘ä»¬å¯¹å¿ƒè„å¥åº·å’Œç–¾ç—…æœºç†çš„ç†è§£ã€‚</li>
<li>methods: è¯¥æ–¹æ³•ä½¿ç”¨ç‚¹äº‘æ·±åº¦å­¦ä¹ çš„æœ€æ–°è¿›å±•ï¼Œå»ºç«‹äº†ä¸€ä¸ªç‚¹äº‘ç¼–ç å™¨-è§£ç å™¨ç»“æ„ï¼Œä»¥ä¾¿é«˜æ•ˆåœ°å­¦ä¹ å¤šå°ºåº¦ç‰¹å¾ã€‚</li>
<li>results: ç ”ç©¶äººå‘˜å¯¹å¤§é‡çš„UK Biobankæ•°æ®é›†è¿›è¡Œäº†æµ‹è¯•ï¼Œå¹¶å‘ç°äº†average Chamferè·ç¦»å°äºå›¾åƒè·å–çš„åƒç´ åˆ†è¾¨ç‡ï¼ŒåŒæ—¶ä¹Ÿå‘ç°äº†ä¸çœŸå®æ•°æ®é›†çš„ç›¸ä¼¼æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶äººå‘˜è¿˜å‘ç°äº†åœ¨å„ä¸ªå­ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ä¸­çš„å·®å¼‚ï¼Œå¹¶ä¸”è¡¨æ˜äº†3Då‡èšæ¨¡å¼å¯ä»¥è¶…è¶Šå¤šä¸ªä¸´åºŠæ ‡å‡†ã€‚<details>
<summary>Abstract</summary>
Global single-valued biomarkers of cardiac function typically used in clinical practice, such as ejection fraction, provide limited insight on the true 3D cardiac deformation process and hence, limit the understanding of both healthy and pathological cardiac mechanics. In this work, we propose the Point Cloud Deformation Network (PCD-Net) as a novel geometric deep learning approach to model 3D cardiac contraction and relaxation between the extreme ends of the cardiac cycle. It employs the recent advances in point cloud-based deep learning into an encoder-decoder structure, in order to enable efficient multi-scale feature learning directly on multi-class 3D point cloud representations of the cardiac anatomy. We evaluate our approach on a large dataset of over 10,000 cases from the UK Biobank study and find average Chamfer distances between the predicted and ground truth anatomies below the pixel resolution of the underlying image acquisition. Furthermore, we observe similar clinical metrics between predicted and ground truth populations and show that the PCD-Net can successfully capture subpopulation-specific differences between normal subjects and myocardial infarction (MI) patients. We then demonstrate that the learned 3D deformation patterns outperform multiple clinical benchmarks by 13% and 7% in terms of area under the receiver operating characteristic curve for the tasks of prevalent MI detection and incident MI prediction and by 7% in terms of Harrell's concordance index for MI survival analysis.
</details>
<details>
<summary>æ‘˜è¦</summary>
å…¨çƒå•å€¼ç”Ÿç‰©æ ‡å¿—ç‰©typically used in clinical practice, such as ejection fraction, only provide limited insight into the true 3D cardiac deformation process and therefore limit the understanding of both healthy and pathological cardiac mechanics. In this work, we propose the Point Cloud Deformation Network (PCD-Net) as a novel geometric deep learning approach to model 3D cardiac contraction and relaxation between the extreme ends of the cardiac cycle. It employs the recent advances in point cloud-based deep learning into an encoder-decoder structure, in order to enable efficient multi-scale feature learning directly on multi-class 3D point cloud representations of the cardiac anatomy. We evaluate our approach on a large dataset of over 10,000 cases from the UK Biobank study and find average Chamfer distances below the pixel resolution of the underlying image acquisition. Furthermore, we observe similar clinical metrics between predicted and ground truth populations and show that the PCD-Net can successfully capture subpopulation-specific differences between normal subjects and myocardial infarction (MI) patients. We then demonstrate that the learned 3D deformation patterns outperform multiple clinical benchmarks by 13% and 7% in terms of area under the receiver operating characteristic curve for the tasks of prevalent MI detection and incident MI prediction and by 7% in terms of Harrell's concordance index for MI survival analysis.
</details></li>
</ul>
<hr>
<h2 id="Confidence-intervals-for-performance-estimates-in-3D-medical-image-segmentation"><a href="#Confidence-intervals-for-performance-estimates-in-3D-medical-image-segmentation" class="headerlink" title="Confidence intervals for performance estimates in 3D medical image segmentation"></a>Confidence intervals for performance estimates in 3D medical image segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10926">http://arxiv.org/abs/2307.10926</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rosanajurdi/SegVal_TMI">https://github.com/rosanajurdi/SegVal_TMI</a></li>
<li>paper_authors: R. El Jurdi, G. Varoquaux, O. Colliot</li>
<li>for: è¿™ paper æ˜¯ç”¨æ¥è¯„ä¼°åŒ»ç–—å›¾åƒ segmentation æ¨¡å‹çš„ã€‚</li>
<li>methods: è¿™ paper ä½¿ç”¨äº† nnU-net æ¡†æ¶å’Œ Medical Decathlon æŒ‘æˆ˜èµ›ä¸­çš„ä¸¤ä¸ªæ•°æ®é›†ï¼Œä»¥åŠä¸¤ç§è¡¨ç°æŒ‡æ ‡ï¼š dice å‡†ç¡®ç‡å’Œ Hausdorff è·ç¦»ã€‚</li>
<li>results: è¿™ paper å‘ç°ï¼Œåœ¨ä¸åŒçš„æµ‹è¯•é›†å¤§å°å’Œè¡¨ç°æŒ‡æ ‡çš„æ‰©æ•£æƒ…å†µä¸‹ï¼Œå‚æ•°å‹çš„ä¿¡åº¦èŒƒå›´æ˜¯Bootstrapä¼°è®¡çš„å¯é è¿‘ä¼¼ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å‘ç°ï¼Œä¸ºäº†è¾¾åˆ°æŸä¸ªç²¾åº¦æ°´å¹³ï¼Œé€šå¸¸éœ€è¦è®­ç»ƒæ ·æœ¬æ•°é‡è¿œå°‘äº classification ä»»åŠ¡ã€‚ typicallyï¼Œéœ€è¦çº¦ 100-200 ä¸ªæµ‹è¯•æ ·æœ¬ï¼Œè€Œä¸”æ›´Difficultçš„ segmentation ä»»åŠ¡å¯èƒ½éœ€è¦æ›´å¤šçš„æµ‹è¯•æ ·æœ¬ã€‚<details>
<summary>Abstract</summary>
Medical segmentation models are evaluated empirically. As such an evaluation is based on a limited set of example images, it is unavoidably noisy. Beyond a mean performance measure, reporting confidence intervals is thus crucial. However, this is rarely done in medical image segmentation. The width of the confidence interval depends on the test set size and on the spread of the performance measure (its standard-deviation across of the test set). For classification, many test images are needed to avoid wide confidence intervals. Segmentation, however, has not been studied, and it differs by the amount of information brought by a given test image. In this paper, we study the typical confidence intervals in medical image segmentation. We carry experiments on 3D image segmentation using the standard nnU-net framework, two datasets from the Medical Decathlon challenge and two performance measures: the Dice accuracy and the Hausdorff distance. We show that the parametric confidence intervals are reasonable approximations of the bootstrap estimates for varying test set sizes and spread of the performance metric. Importantly, we show that the test size needed to achieve a given precision is often much lower than for classification tasks. Typically, a 1% wide confidence interval requires about 100-200 test samples when the spread is low (standard-deviation around 3%). More difficult segmentation tasks may lead to higher spreads and require over 1000 samples.
</details>
<details>
<summary>æ‘˜è¦</summary>
åŒ»å­¦åˆ†å‰²æ¨¡å‹é€šå¸¸ä¼šè¢«å®é™…æµ‹è¯•ã€‚è¿™ç§æµ‹è¯•åŸºäºæœ‰é™çš„ç¤ºä¾‹å›¾åƒï¼Œå› æ­¤æ— æ³•é¿å…å™ªéŸ³ã€‚é™¤äº†å¹³å‡æ€§èƒ½æŒ‡æ ‡ä¹‹å¤–ï¼ŒæŠ¥å‘Šä¿¡æ¯intervalä¹Ÿæ˜¯éå¸¸é‡è¦ã€‚ç„¶è€Œï¼Œåœ¨åŒ»å­¦åƒåˆ†å‰²ä¸­ï¼Œè¿™å¹¶ä¸æ˜¯å¸¸è§çš„åšæ³•ã€‚ä¿¡æ¯intervalçš„å®½åº¦å–å†³äºæµ‹è¯•é›†å¤§å°å’Œæ€§èƒ½æŒ‡æ ‡çš„æ‰©æ•£ï¼ˆæµ‹è¯•é›†ä¸­çš„æ ‡å‡†å·®ï¼‰ã€‚å¯¹äºåˆ†ç±»ä»»åŠ¡ï¼Œéœ€è¦è®¸å¤šæµ‹è¯•å›¾åƒæ¥é¿å…å®½çš„ä¿¡æ¯intervalã€‚ä½†æ˜¯ï¼Œåœ¨åˆ†å‰²ä»»åŠ¡ä¸­ï¼Œä¸åŒçš„æµ‹è¯•å›¾åƒä¼šå¸¦æ¥ä¸åŒçš„ä¿¡æ¯é‡ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åŒ»å­¦åƒåˆ†å‰²ä¸­å¸¸è§çš„ä¿¡æ¯intervalã€‚æˆ‘ä»¬åœ¨ä½¿ç”¨æ ‡å‡†nnU-netæ¡†æ¶ã€åŒ»ç–—åå¤§æŒ‘æˆ˜èµ›æä¾›çš„ä¸¤ä¸ªæ•°æ®é›†å’Œä¸¤ä¸ªæ€§èƒ½æŒ‡æ ‡ï¼ˆ diceå‡†ç¡®ç‡å’Œ Hausdorff è·ç¦»ï¼‰è¿›è¡Œäº†å®éªŒã€‚æˆ‘ä»¬å‘ç°ï¼Œå‚æ•°ä¿¡æ¯intervalæ˜¯å‚æ•°Bootstrapä¼°è®¡çš„å¯é è¿‘ä¼¼ï¼Œå¹¶ä¸”æ˜¾ç¤ºæµ‹è¯•é›†å¤§å°å’Œæ€§èƒ½æŒ‡æ ‡çš„æ‰©æ•£å¯¹ä¿¡æ¯intervalçš„å½±å“ã€‚è¿›ä¸€æ­¥åœ°ï¼Œæˆ‘ä»¬å‘ç°ï¼Œä¸ºäº† Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒç»™å®šçš„ç²¾åº¦ï¼Œæµ‹è¯•æ ·æœ¬çš„æ•°é‡é€šå¸¸æ¯”åˆ†ç±»ä»»åŠ¡ä½å¾—å¤šã€‚ä¾‹å¦‚ï¼Œå½“æ ‡å‡†å·®è¾ƒä½ï¼ˆçº¦3%ï¼‰æ—¶ï¼Œ1% å®½çš„ä¿¡æ¯intervalåªéœ€è¦100-200ä¸ªæµ‹è¯•æ ·æœ¬ã€‚æ›´å¤æ‚çš„åˆ†å‰²ä»»åŠ¡å¯èƒ½ä¼šå¯¼è‡´æ›´é«˜çš„æ‰©æ•£ï¼Œéœ€è¦æ›´å¤šçš„æµ‹è¯•æ ·æœ¬ã€‚
</details></li>
</ul>
<hr>
<h2 id="Sequential-Multi-Dimensional-Self-Supervised-Learning-for-Clinical-Time-Series"><a href="#Sequential-Multi-Dimensional-Self-Supervised-Learning-for-Clinical-Time-Series" class="headerlink" title="Sequential Multi-Dimensional Self-Supervised Learning for Clinical Time Series"></a>Sequential Multi-Dimensional Self-Supervised Learning for Clinical Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10923">http://arxiv.org/abs/2307.10923</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/exploita123/charmedforfree">https://github.com/exploita123/charmedforfree</a></li>
<li>paper_authors: Aniruddh Raghu, Payal Chandak, Ridwan Alam, John Guttag, Collin M. Stultz</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ˜¯ä¸ºäº†è§£å†³ç°æœ‰çš„è‡ªåŠ¨å­¦ä¹ ï¼ˆSelf-supervised learningï¼‰æ–¹æ³•ä¸èƒ½å¤„ç†å¤šModalæ—¶é—´åºåˆ—æ•°æ®çš„é—®é¢˜ã€‚</li>
<li>methods: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è‡ªåŠ¨å­¦ä¹ æ–¹æ³•â€”â€”Sequential Multi-Dimensional SSLï¼Œå®ƒåœ¨åºåˆ—çº§å’Œä¸ªä½“é«˜ç»´æ•°æ®çº§åˆ«åº”ç”¨SSLæŸå¤±æ¥æ›´å¥½åœ°æ•æ‰ä¿¡æ¯ã€‚</li>
<li>results: å¯¹ä¸¤ä¸ªå®é™…çš„åŒ»ç–—æ—¶é—´åºåˆ—æ•°æ®é›†è¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œåœ¨å…ˆè¡ŒåŸ¹è‚²åï¼Œä½¿ç”¨è¯¥æ–¹æ³•å¹¶then fine-tuningåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šæé«˜äº†æ€§èƒ½ï¼Œå¹¶åœ¨ä¸€äº›è®¾ç½®ä¸‹å¯ä»¥é€šè¿‡ä¸åŒçš„è‡ªåŠ¨å­¦ä¹ æŸå¤±å‡½æ•°æ¥æé«˜æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
Self-supervised learning (SSL) for clinical time series data has received significant attention in recent literature, since these data are highly rich and provide important information about a patient's physiological state. However, most existing SSL methods for clinical time series are limited in that they are designed for unimodal time series, such as a sequence of structured features (e.g., lab values and vitals signs) or an individual high-dimensional physiological signal (e.g., an electrocardiogram). These existing methods cannot be readily extended to model time series that exhibit multimodality, with structured features and high-dimensional data being recorded at each timestep in the sequence. In this work, we address this gap and propose a new SSL method -- Sequential Multi-Dimensional SSL -- where a SSL loss is applied both at the level of the entire sequence and at the level of the individual high-dimensional data points in the sequence in order to better capture information at both scales. Our strategy is agnostic to the specific form of loss function used at each level -- it can be contrastive, as in SimCLR, or non-contrastive, as in VICReg. We evaluate our method on two real-world clinical datasets, where the time series contains sequences of (1) high-frequency electrocardiograms and (2) structured data from lab values and vitals signs. Our experimental results indicate that pre-training with our method and then fine-tuning on downstream tasks improves performance over baselines on both datasets, and in several settings, can lead to improvements across different self-supervised loss functions.
</details>
<details>
<summary>æ‘˜è¦</summary>
è‡ªé€‚åº”å­¦ä¹ ï¼ˆSSLï¼‰ Ğ´Ğ»ÑåŒ»ç–—æ—¶é—´åºåˆ—æ•°æ®åœ¨å½“å‰æ–‡çŒ®ä¸­å—åˆ°äº†å¹¿æ³›å…³æ³¨ï¼Œå› ä¸ºè¿™äº›æ•°æ®å…·æœ‰é«˜åº¦çš„èµ„æºå’Œé‡è¦çš„ç”Ÿç‰©physiologicalçŠ¶æ€ä¿¡æ¯ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å¤šæ•°SSLæ–¹æ³•ä»…é€‚ç”¨äºå•æ¨¡æ—¶é—´åºåˆ—ï¼Œä¾‹å¦‚åºåˆ—ä¸­çš„ç»“æ„åŒ–ç‰¹å¾ï¼ˆå¦‚åŒ»å­¦å®éªŒå®¤å€¼å’Œç”Ÿç‰©æŒ‡æ ‡ï¼‰æˆ–ä¸ªäººé«˜ç»´åº¦ç”Ÿç†å­¦ä¿¡å·ï¼ˆå¦‚ç”µcardiogramï¼‰ã€‚è¿™äº›ç°æœ‰æ–¹æ³•æ— æ³•è½»æ¾åœ°æ‰©å±•åˆ°æ¨¡å‹æ—¶é—´åºåˆ—ï¼Œå…¶ä¸­æ¯ä¸ªæ—¶é—´æ­¥éª¤éƒ½åŒ…å«ç»“æ„åŒ–ç‰¹å¾å’Œé«˜ç»´åº¦æ•°æ®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è§£å†³è¿™ä¸ªå·®è·ï¼Œå¹¶æè®®ä¸€ç§æ–°çš„SSLæ–¹æ³•â€”â€”Sequential Multi-Dimensional SSLã€‚åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬åœ¨åºåˆ—çº§åˆ«å’Œä¸ªä½“é«˜ç»´åº¦æ•°æ®ç‚¹çº§åˆ«éƒ½åº”ç”¨SSLæŸå¤±ï¼Œä»¥æ›´å¥½åœ°æ•æ‰ä¿¡æ¯åœ¨ä¸åŒçº§åˆ«ã€‚æˆ‘ä»¬çš„ç­–ç•¥æ˜¯å¯¹ç‰¹å®šçš„æŸå¤±å‡½æ•°ç±»å‹ä¸æ‹˜æ³¥ï¼Œå¯ä»¥æ˜¯å¯¹æ¯”æ€§çš„ï¼Œå¦‚SimCLRï¼Œæˆ–éå¯¹æ¯”æ€§çš„ï¼Œå¦‚VICRegã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªçœŸå®çš„åŒ»ç–—æ—¶é—´åºåˆ—æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå…¶ä¸­æ—¶é—´åºåˆ—åŒ…å«é«˜é¢‘ç”µcardiogramså’Œå®éªŒå®¤å€¼å’Œç”Ÿç‰©æŒ‡æ ‡çš„åºåˆ—ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨è¿™äº›æ•°æ®é›†ä¸Šé¢„è®­ç»ƒåï¼Œé€šè¿‡ç²¾åº¦è°ƒæ•´ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¯ä»¥è¶…è¿‡åŸºå‡†å€¼ï¼Œå¹¶åœ¨ä¸åŒçš„è‡ªæˆ‘è¶…visedæŸå¤±å‡½æ•°ä¸‹è¾¾åˆ°æ›´å¥½çš„æ€§èƒ½ã€‚
</details></li>
</ul>
<hr>
<h2 id="Language-based-Action-Concept-Spaces-Improve-Video-Self-Supervised-Learning"><a href="#Language-based-Action-Concept-Spaces-Improve-Video-Self-Supervised-Learning" class="headerlink" title="Language-based Action Concept Spaces Improve Video Self-Supervised Learning"></a>Language-based Action Concept Spaces Improve Video Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10922">http://arxiv.org/abs/2307.10922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kanchana Ranasinghe, Michael Ryoo</li>
<li>for: å­¦ä¹ é«˜æ•ˆè½¬ç§»å’Œé²æ£’çš„è§†é¢‘è¡¨ç¤º</li>
<li>methods: ä½¿ç”¨è¯­è¨€æ†ç»‘è‡ªæˆ‘è¶…visedå­¦ä¹ å°†å›¾åƒCLIPæ¨¡å‹é€‚åº”è§†é¢‘é¢‘è°±</li>
<li>results: æé«˜é›¶shotå’Œçº¿æ€§æ¢æµ‹æ€§èƒ½åœ¨ä¸‰ä¸ªåŠ¨ä½œè®¤è¯†benchmarkä¸Š<details>
<summary>Abstract</summary>
Recent contrastive language image pre-training has led to learning highly transferable and robust image representations. However, adapting these models to video domains with minimal supervision remains an open problem. We explore a simple step in that direction, using language tied self-supervised learning to adapt an image CLIP model to the video domain. A backbone modified for temporal modeling is trained under self-distillation settings with train objectives operating in an action concept space. Feature vectors of various action concepts extracted from a language encoder using relevant textual prompts construct this space. We introduce two train objectives, concept distillation and concept alignment, that retain generality of original representations while enforcing relations between actions and their attributes. Our approach improves zero-shot and linear probing performance on three action recognition benchmarks.
</details>
<details>
<summary>æ‘˜è¦</summary>
Translated into Simplified Chinese:æœ€è¿‘çš„å¯¹è¯­è¨€å›¾åƒé¢„è®­ç»ƒæŠ€æœ¯å·²ç»å¯¼è‡´å­¦ä¹ äº†é«˜åº¦å¯è½¬ç§»å’Œç¨³å®šçš„å›¾åƒè¡¨ç¤ºã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ¨¡å‹åº”ç”¨åˆ°è§†é¢‘é¢‘é“ä¸Šä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾çš„é—®é¢˜ã€‚æˆ‘ä»¬è€ƒè™‘äº†ä¸€ç§ç®€å•çš„æ–¹æ³•ï¼Œä½¿ç”¨è¯­è¨€ç»‘å®šçš„è‡ªæˆ‘è¶…visionå­¦ä¹ æ¥é€‚åº”å›¾åƒ CLIP æ¨¡å‹åˆ°è§†é¢‘é¢‘é“ã€‚æˆ‘ä»¬ä¿®æ”¹äº† temporal æ¨¡å‹ï¼Œåœ¨è‡ªæˆ‘æ•°æ®é‡‡æ ·è®¾ç½®ä¸‹ä½¿ç”¨è¯­è¨€Encoder æå–çš„ä¸åŒåŠ¨ä½œæ¦‚å¿µçš„ç‰¹å¾å‘é‡æ„å»ºåŠ¨ä½œæ¦‚å¿µç©ºé—´ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªè®­ç»ƒç›®æ ‡ï¼Œæ¦‚å¿µç»ƒä¹ å’Œæ¦‚å¿µå¯¹æ¥ï¼Œä»¥ä¿ç•™åŸå§‹è¡¨ç¤ºçš„é€šç”¨æ€§ï¼ŒåŒæ—¶å¼ºåˆ¶è¡ŒåŠ¨å’Œå…¶å±æ€§ä¹‹é—´çš„å…³ç³»ã€‚æˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†é›¶shotå’Œç›´çº¿æ¢æµ‹æ€§èƒ½åœ¨ä¸‰ä¸ªåŠ¨ä½œè®¤è¯†æ ‡å‡† bencmarks ä¸Šã€‚
</details></li>
</ul>
<hr>
<h2 id="The-Role-of-Entropy-and-Reconstruction-in-Multi-View-Self-Supervised-Learning"><a href="#The-Role-of-Entropy-and-Reconstruction-in-Multi-View-Self-Supervised-Learning" class="headerlink" title="The Role of Entropy and Reconstruction in Multi-View Self-Supervised Learning"></a>The Role of Entropy and Reconstruction in Multi-View Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10907">http://arxiv.org/abs/2307.10907</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/apple/ml-entropy-reconstruction">https://github.com/apple/ml-entropy-reconstruction</a></li>
<li>paper_authors: Borja RodrÃ­guez-GÃ¡lvez, Arno Blaas, Pau RodrÃ­guez, Adam GoliÅ„ski, Xavier Suau, Jason Ramapuram, Dan Busbridge, Luca Zappella</li>
<li>for: æœ¬æ–‡ç ”ç©¶äº†å¤šè§†å›¾è‡ªåŠ¨å­¦ä¹ ï¼ˆMVSSLï¼‰çš„æˆåŠŸæœºåˆ¶ï¼Œå¹¶é€šè¿‡ä¸€ç§æ–°çš„ä¸‹ç•Œå‡½æ•°æ¥åˆ†æä¸åŒçš„MVSSLå®¶æ—ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨ä¸€ç§åŸºäºä¿¡æ¯å‡†åˆ™ï¼ˆMIï¼‰çš„ä¸‹ç•Œå‡½æ•°ï¼ŒåŒ…æ‹¬ä¸€ä¸ª entropy å’Œä¸€ä¸ªé‡å»ºé¡¹ï¼ˆERï¼‰ï¼Œæ¥åˆ†æä¸åŒçš„MVSSLæ–¹æ³•ã€‚</li>
<li>results: ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è¿™ç§ ER ä¸‹ç•Œå‡½æ•°å¯ä»¥è¾¾åˆ°ä¸å¸¸è§MVSSLæ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶ä½¿å¾—è®­ç»ƒæ—¶ä½¿ç”¨å°æ‰¹é‡æˆ–å°EMAç³»æ•°æ—¶æ›´åŠ ç¨³å®šã€‚<details>
<summary>Abstract</summary>
The mechanisms behind the success of multi-view self-supervised learning (MVSSL) are not yet fully understood. Contrastive MVSSL methods have been studied through the lens of InfoNCE, a lower bound of the Mutual Information (MI). However, the relation between other MVSSL methods and MI remains unclear. We consider a different lower bound on the MI consisting of an entropy and a reconstruction term (ER), and analyze the main MVSSL families through its lens. Through this ER bound, we show that clustering-based methods such as DeepCluster and SwAV maximize the MI. We also re-interpret the mechanisms of distillation-based approaches such as BYOL and DINO, showing that they explicitly maximize the reconstruction term and implicitly encourage a stable entropy, and we confirm this empirically. We show that replacing the objectives of common MVSSL methods with this ER bound achieves competitive performance, while making them stable when training with smaller batch sizes or smaller exponential moving average (EMA) coefficients.   Github repo: https://github.com/apple/ml-entropy-reconstruction.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ–‡ä¸­æ‰€æè¿°çš„å¤šè§†å›¾è‡ªå­¦ä¹ ï¼ˆMVSSLï¼‰çš„æœºåˆ¶ä»æœªå®Œå…¨ç†è§£ã€‚å¯¹äºå¯¹æ¯”MVSSLæ–¹æ³•çš„ç ”ç©¶ï¼Œæˆ‘ä»¬é€šè¿‡InfoNCEï¼Œä¸€ç§ä½ä¸‹ç•Œçš„å…±è¯†ä¿¡æ¯ï¼ˆMIï¼‰æ¥ç ”ç©¶ã€‚ä½†æ˜¯å…¶ä»–MVSSLæ–¹æ³•å’ŒMIä¹‹é—´çš„å…³ç³»ä»ç„¶ä¸æ¸…æ¥šã€‚æˆ‘ä»¬è€ƒè™‘ä¸€ç§åŸºäº entropyå’Œé‡å»ºé¡¹ï¼ˆERï¼‰çš„ä½ä¸‹ç•Œï¼Œå¹¶é€šè¿‡è¿™ä¸ªçª—å£æ¥åˆ†æä¸»è¦çš„MVSSLå®¶æ—ã€‚æˆ‘ä»¬æ˜¾ç¤ºäº†ä½¿ç”¨ clustering-based æ–¹æ³•such as DeepClusterå’ŒSwAVæ—¶ï¼Œå®é™…ä¸Šæ˜¯æœ€å¤§åŒ–MIçš„ã€‚æˆ‘ä»¬è¿˜é‡æ–°è§£é‡Šäº† distillation-based æ–¹æ³•such as BYOLå’ŒDINOçš„æœºåˆ¶ï¼Œå¹¶è¯æ˜å®ƒä»¬é€šè¿‡ç›´æ¥æœ€å¤§åŒ–é‡å»ºé¡¹å¹¶é—´æ¥æ¿€å‘ç¨³å®šçš„ entropyæ¥å®ç°ã€‚æˆ‘ä»¬é€šè¿‡å®éªŒè¯æ˜è¿™ä¸€ç‚¹ã€‚ finallyï¼Œæˆ‘ä»¬è¡¨æ˜å°†å¸¸è§MVSSLæ–¹æ³•çš„ç›®æ ‡æ›¿æ¢ä¸ºERä¸‹ç•Œå¯ä»¥å®ç°ç«äº‰æ€§çš„æ€§èƒ½ï¼ŒåŒæ—¶ä½¿å…¶åœ¨è®­ç»ƒæ—¶ä½¿ç”¨å°æ‰¹é‡æˆ–å°EMAç³»æ•°æ—¶æ›´åŠ ç¨³å®šã€‚Here's the breakdown of the translation:* æ–‡ä¸­æ‰€æè¿°çš„å¤šè§†å›¾è‡ªå­¦ä¹  (MVSSL)ï¼šThe text is discussing the mechanisms behind the success of multi-view self-supervised learning (MVSSL).* æœºåˆ¶ä»æœªå®Œå…¨ç†è§£ï¼šThe mechanisms behind MVSSL are not yet fully understood.* å¯¹äºå¯¹æ¯”MVSSLæ–¹æ³•çš„ç ”ç©¶ï¼šResearch on comparing MVSSL methods.* é€šè¿‡InfoNCEæ¥ç ”ç©¶ï¼šResearching through the lens of InfoNCE, a lower bound of mutual information (MI).* å…¶ä»–MVSSLæ–¹æ³•å’ŒMIä¹‹é—´çš„å…³ç³»ä»ç„¶ä¸æ¸…æ¥šï¼šThe relationship between other MVSSL methods and MI is still unclear.* æˆ‘ä»¬è€ƒè™‘ä¸€ç§åŸºäº entropyå’Œé‡å»ºé¡¹ï¼ˆERï¼‰çš„ä½ä¸‹ç•Œï¼šWe consider a different lower bound on MI consisting of an entropy and a reconstruction term (ER).* å¹¶é€šè¿‡è¿™ä¸ªçª—å£æ¥åˆ†æä¸»è¦çš„MVSSLå®¶æ—ï¼šAnd analyze the main MVSSL families through this ER bound.* æˆ‘ä»¬æ˜¾ç¤ºäº†ä½¿ç”¨ clustering-based æ–¹æ³•such as DeepClusterå’ŒSwAVæ—¶ï¼Œå®é™…ä¸Šæ˜¯æœ€å¤§åŒ–MIçš„ï¼šWe show that using clustering-based methods such as DeepCluster and SwAV, the MI is maximized.* æˆ‘ä»¬è¿˜é‡æ–°è§£é‡Šäº† distillation-based æ–¹æ³•such as BYOLå’ŒDINOçš„æœºåˆ¶ï¼šWe also re-interpret the mechanisms of distillation-based approaches such as BYOL and DINO.* å¹¶è¯æ˜å®ƒä»¬é€šè¿‡ç›´æ¥æœ€å¤§åŒ–é‡å»ºé¡¹å¹¶é—´æ¥æ¿€å‘ç¨³å®šçš„ entropyæ¥å®ç°ï¼šAnd prove that they explicitly maximize the reconstruction term and implicitly encourage a stable entropy.* æˆ‘ä»¬é€šè¿‡å®éªŒè¯æ˜è¿™ä¸€ç‚¹ï¼šWe confirm this empirically.* finallyï¼Œæˆ‘ä»¬è¡¨æ˜å°†å¸¸è§MVSSLæ–¹æ³•çš„ç›®æ ‡æ›¿æ¢ä¸ºERä¸‹ç•Œå¯ä»¥å®ç°ç«äº‰æ€§çš„æ€§èƒ½ï¼šFinally, we show that replacing the objectives of common MVSSL methods with the ER bound achieves competitive performance.* åŒæ—¶ä½¿å…¶åœ¨è®­ç»ƒæ—¶ä½¿ç”¨å°æ‰¹é‡æˆ–å°EMAç³»æ•°æ—¶æ›´åŠ ç¨³å®šï¼šAnd make them stable when training with smaller batch sizes or smaller exponential moving average (EMA) coefficients.
</details></li>
</ul>
<hr>
<h2 id="Variational-Point-Encoding-Deformation-for-Dental-Modeling"><a href="#Variational-Point-Encoding-Deformation-for-Dental-Modeling" class="headerlink" title="Variational Point Encoding Deformation for Dental Modeling"></a>Variational Point Encoding Deformation for Dental Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10895">http://arxiv.org/abs/2307.10895</a></li>
<li>repo_url: None</li>
<li>paper_authors: Johan Ziruo Ye, Thomas Ã˜rkild, Peter Lempel SÃ¸ndergaard, SÃ¸ren Hauberg</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨é¼“åŠ±æ›´å¤šç ”ç©¶ï¼Œé€è¿‡å‘å¸ƒæ–°çš„å¤§é‡ç‰™é½¿çŸ©é˜µæ•°æ®é›†ã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ‰©å±•FoldingNetçš„æ–¹æ³•ï¼Œç§°ä¸ºVariational FoldingNetï¼ˆVF-Netï¼‰ï¼Œå®ƒå…è®¸ç‚¹äº‘è¡¨ç¤ºçš„ probabilistic å­¦ä¹ ã€‚</li>
<li>results: æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒVF-Net æ¯”ç°æœ‰æ¨¡å‹åœ¨ç‰™é½¿æ‰«æå’Œæ¨ç†æ–¹é¢å…·æœ‰æ›´é«˜çš„è¡¨ç°åŠ›ï¼ŒåŒæ—¶å…·æœ‰æ›´å¥½çš„é²æ£’æ€§ã€‚<details>
<summary>Abstract</summary>
Digital dentistry has made significant advancements in recent years, yet numerous challenges remain to be addressed. In this study, we release a new extensive dataset of tooth meshes to encourage further research. Additionally, we propose Variational FoldingNet (VF-Net), which extends FoldingNet to enable probabilistic learning of point cloud representations. A key challenge in existing latent variable models for point clouds is the lack of a 1-to-1 mapping between input points and output points. Instead, they must rely on optimizing Chamfer distances, a metric that does not have a normalized distributional counterpart, preventing its usage in probabilistic models. We demonstrate that explicit minimization of Chamfer distances can be replaced by a suitable encoder, which allows us to increase computational efficiency while simplifying the probabilistic extension. Our experimental findings present empirical evidence demonstrating the superior performance of VF-Net over existing models in terms of dental scan reconstruction and extrapolation. Additionally, our investigation highlights the robustness of VF-Net's latent representations. These results underscore the promising prospects of VF-Net as an effective and reliable method for point cloud reconstruction and analysis.
</details>
<details>
<summary>æ‘˜è¦</summary>
ã€Šæ•°å­—ç‰™ç§‘æŠ€æœ¯çš„è¿›æ­¥å’ŒæŒ‘æˆ˜ã€‹Recently, digital dentistry has made significant advancements, but there are still many challenges that need to be addressed. In this study, we release a new and extensive dataset of tooth meshes to encourage further research. Additionally, we propose a new method called Variational FoldingNet (VF-Net), which extends FoldingNet to enable probabilistic learning of point cloud representations.Currently, there is a key challenge in existing latent variable models for point clouds, which is the lack of a 1-to-1 mapping between input points and output points. Instead, they must rely on optimizing Chamfer distances, a metric that does not have a normalized distributional counterpart, preventing its usage in probabilistic models. We demonstrate that explicit minimization of Chamfer distances can be replaced by a suitable encoder, which allows us to increase computational efficiency while simplifying the probabilistic extension.Our experimental findings present empirical evidence demonstrating the superior performance of VF-Net over existing models in terms of dental scan reconstruction and extrapolation. Additionally, our investigation highlights the robustness of VF-Net's latent representations. These results underscore the promising prospects of VF-Net as an effective and reliable method for point cloud reconstruction and analysis.
</details></li>
</ul>
<hr>
<h2 id="Learning-and-Generalizing-Polynomials-in-Simulation-Metamodeling"><a href="#Learning-and-Generalizing-Polynomials-in-Simulation-Metamodeling" class="headerlink" title="Learning and Generalizing Polynomials in Simulation Metamodeling"></a>Learning and Generalizing Polynomials in Simulation Metamodeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10892">http://arxiv.org/abs/2307.10892</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jesperhauch/polynomial_deep_learning">https://github.com/jesperhauch/polynomial_deep_learning</a></li>
<li>paper_authors: Jesper Hauch, Christoffer Riis, Francisco C. Pereira</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æé«˜äººå·¥ç¥ç»ç½‘ç»œçš„ polynomial æ‹Ÿåˆèƒ½åŠ›å’Œé€šç”¨æ€§ï¼Œä»¥ä¾¿åœ¨å¤šç§å·¥ç¨‹é¢†åŸŸä¸­ä½¿ç”¨ã€‚</li>
<li>methods: æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤šé¡¹å¼ç¥ç»ç½‘ç»œï¼ˆMNNï¼‰çš„æ‹Ÿåˆæ–¹æ³•ï¼Œå¹¶ä½¿ç”¨ MNN ä½œä¸ºé€’å½’å»ºæ¨¡Componentã€‚</li>
<li>results: å®éªŒè¡¨æ˜ï¼ŒMNN æ¯”åŸºelineæ¨¡å‹æ›´å¥½åœ°æ³›åŒ–ï¼Œå¹¶ä¸”å…¶åœ¨éªŒè¯é›†ä¸Šçš„æ€§èƒ½ä¸æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½ç›¸ä¼¼ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜æå‡ºäº†ä¸€ç§åŸºäº simulations çš„æ¨¡æ‹Ÿä¸­é—´æ¨¡å‹æ–¹æ³•ï¼Œå¯ä»¥æ›´å¥½åœ°æ‹Ÿåˆ polynomial æ—¶é—´æ­¥éª¤æ›´æ–°çš„ simulationsã€‚<details>
<summary>Abstract</summary>
The ability to learn polynomials and generalize out-of-distribution is essential for simulation metamodels in many disciplines of engineering, where the time step updates are described by polynomials. While feed forward neural networks can fit any function, they cannot generalize out-of-distribution for higher-order polynomials. Therefore, this paper collects and proposes multiplicative neural network (MNN) architectures that are used as recursive building blocks for approximating higher-order polynomials. Our experiments show that MNNs are better than baseline models at generalizing, and their performance in validation is true to their performance in out-of-distribution tests. In addition to MNN architectures, a simulation metamodeling approach is proposed for simulations with polynomial time step updates. For these simulations, simulating a time interval can be performed in fewer steps by increasing the step size, which entails approximating higher-order polynomials. While our approach is compatible with any simulation with polynomial time step updates, a demonstration is shown for an epidemiology simulation model, which also shows the inductive bias in MNNs for learning and generalizing higher-order polynomials.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œæ¨¡å‹å­¦ä¹  polynomials å’Œæ³›åŒ–åˆ°ä¸åŒåˆ†å¸ƒæ˜¯Engineering å¤šä¸ªé¢†åŸŸçš„å¿…å¤‡æŠ€èƒ½ï¼Œå› ä¸ºæ—¶é—´æ­¥é•¿æ›´æ–°é€šå¸¸æ˜¯ polynomialsã€‚è™½ç„¶å‰å‘ç¥ç»ç½‘ç»œå¯ä»¥é€‚åº”ä»»ä½•å‡½æ•°ï¼Œä½†å®ƒä»¬æ— æ³•æ³›åŒ–åˆ°é«˜é˜¶ polynomialsã€‚å› æ­¤ï¼Œæœ¬æ–‡æ”¶é›†å¹¶æå‡ºäº†multiplicative neural networkï¼ˆMNNï¼‰æ¶æ„ï¼Œç”¨äº recursive æ„å»ºé«˜é˜¶ polynomials çš„è¿‘ä¼¼ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒMNNs åœ¨æ³›åŒ–æ–¹é¢è¡¨ç°æ›´å¥½ï¼Œå¹¶ä¸”åœ¨éªŒè¯é›†ä¸­çš„æ€§èƒ½ä¸éªŒè¯é›†å¤–çš„æ€§èƒ½ç›¸ä¼¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ simulation metamodeling æ–¹æ³•ï¼Œç”¨äº simulations with polynomial time step updatesã€‚å¯¹äºè¿™äº› simulationsï¼Œå¯ä»¥é€šè¿‡å¢åŠ æ­¥é•¿æ¥å¿«é€Ÿ simulate æ—¶é—´ Ğ¸Ğ½Ñ‚ĞµÑ€VALï¼Œè¿™æ„å‘³ç€éœ€è¦è¿‘ä¼¼é«˜é˜¶ polynomialsã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»»ä½•å…·æœ‰ polynomial time step updates çš„ simulation ç›¸å®¹ï¼Œå¹¶åœ¨ epidemiology æ¨¡å‹ä¸­è¿›è¡Œäº†ç¤ºä¾‹ï¼Œè¿™ä¹Ÿè¡¨æ˜äº† MNNs å¯¹äºå­¦ä¹ å’Œæ³›åŒ–é«˜é˜¶ polynomials çš„é€‚åº”æ€§ã€‚â€Note that Simplified Chinese is used here, which is the most widely used variety of Chinese in mainland China. If you prefer Traditional Chinese, I can provide that version as well.
</details></li>
</ul>
<hr>
<h2 id="Syntactic-vs-Semantic-Linear-Abstraction-and-Refinement-of-Neural-Networks"><a href="#Syntactic-vs-Semantic-Linear-Abstraction-and-Refinement-of-Neural-Networks" class="headerlink" title="Syntactic vs Semantic Linear Abstraction and Refinement of Neural Networks"></a>Syntactic vs Semantic Linear Abstraction and Refinement of Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10891">http://arxiv.org/abs/2307.10891</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cxlvinchau/linna">https://github.com/cxlvinchau/linna</a></li>
<li>paper_authors: Calvin Chau, Jan KÅ™etÃ­nskÃ½, Stefanie Mohr</li>
<li>for: æé«˜ç¥ç»ç½‘ç»œçš„å¯æ‰©å±•æ€§ã€‚</li>
<li>methods: ä½¿ç”¨ linear combination of neurons æ¥å–ä»£å•ä¸ª neuronï¼Œå¹¶åœ¨ syntaxic å’Œ semantic ä¸Šè¿›è¡ŒæŠ½è±¡ã€‚</li>
<li>results: å®ç°æ›´é«˜çš„å‡å°‘ï¼Œå¹¶å¼•å…¥ä¸€ç§æ”¹è¿›çš„å‡å°‘æ–¹æ³•ä»¥ä¿æŒå‡†ç¡®æ€§ã€‚<details>
<summary>Abstract</summary>
Abstraction is a key verification technique to improve scalability. However, its use for neural networks is so far extremely limited. Previous approaches for abstracting classification networks replace several neurons with one of them that is similar enough. We can classify the similarity as defined either syntactically (using quantities on the connections between neurons) or semantically (on the activation values of neurons for various inputs). Unfortunately, the previous approaches only achieve moderate reductions, when implemented at all. In this work, we provide a more flexible framework where a neuron can be replaced with a linear combination of other neurons, improving the reduction. We apply this approach both on syntactic and semantic abstractions, and implement and evaluate them experimentally. Further, we introduce a refinement method for our abstractions, allowing for finding a better balance between reduction and precision.
</details>
<details>
<summary>æ‘˜è¦</summary>
å½’çº³æ˜¯ä¸€ç§å…³é”®çš„éªŒè¯æŠ€æœ¯ï¼Œå¯ä»¥æé«˜ç¥ç»ç½‘ç»œçš„æ‰©å±•æ€§ã€‚ç„¶è€Œï¼Œå½’çº³ç¥ç»ç½‘ç»œçš„ä½¿ç”¨èŒƒå›´è¿˜å¾ˆæœ‰é™ã€‚å…ˆå‰çš„æ–¹æ³•æ˜¯å°†ä¸€äº›ç¥ç»å…ƒä¸å…¶ä»–ç›¸ä¼¼çš„ç¥ç»å…ƒè¿›è¡Œäº¤æ¢ï¼Œä»¥å®ç°å½’çº³ã€‚æˆ‘ä»¬å¯ä»¥å°†ç›¸ä¼¼æ€§åˆ†ä¸ºé€»è¾‘ï¼ˆé€šè¿‡ç¥ç»å…ƒä¹‹é—´çš„è¿æ¥é‡ï¼‰æˆ–semanticï¼ˆé€šè¿‡ç¥ç»å…ƒå¯¹å„ç§è¾“å…¥çš„æ´»åŠ¨å€¼ï¼‰ä¸¤ç§ã€‚å¯æƒœï¼Œå…ˆå‰çš„æ–¹æ³•åªèƒ½å®ç°ä¸€å®šçš„å‡å°‘ï¼Œè€Œä¸”åªæœ‰éƒ¨åˆ†å®ç°ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æä¾›äº†æ›´ flexibleçš„æ¡†æ¶ï¼Œå…è®¸ä¸€ä¸ªç¥ç»å…ƒè¢«æ›¿æ¢ä¸ºä¸€ä¸ªçº¿æ€§ç»„åˆå…¶ä»–ç¥ç»å…ƒï¼Œä»è€Œæé«˜å‡å°‘ã€‚æˆ‘ä»¬åœ¨é€»è¾‘å’Œsemanticå½’çº³ä¸Šåº”ç”¨è¿™ç§æ–¹æ³•ï¼Œå¹¶è¿›è¡Œå®éªŒæ€§è¯„ä¼°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§ç²¾ç»†åŒ–æ–¹æ³•ï¼Œå¯ä»¥å¸®åŠ©æ‰¾åˆ°æ›´å¥½çš„å‡å°‘å’Œç²¾åº¦ä¹‹é—´çš„å¹³è¡¡ã€‚
</details></li>
</ul>
<hr>
<h2 id="Player-optimal-Stable-Regret-for-Bandit-Learning-in-Matching-Markets"><a href="#Player-optimal-Stable-Regret-for-Bandit-Learning-in-Matching-Markets" class="headerlink" title="Player-optimal Stable Regret for Bandit Learning in Matching Markets"></a>Player-optimal Stable Regret for Bandit Learning in Matching Markets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10890">http://arxiv.org/abs/2307.10890</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fang Kong, Shuai Li</li>
<li>For:  This paper focuses on the problem of matching markets, specifically on finding a stable matching in an online setting where one-side participants (players) learn their unknown preferences from iterative interactions with the other side (arms).* Methods: The paper proposes a new algorithm called explore-then-Gale-Shapley (ETGS) and analyzes its performance in terms of the optimal stable regret of each player.* Results: The paper shows that the optimal stable regret of each player can be upper bounded by $O(K\log T&#x2F;\Delta^2)$, which is a significantly better result than previous works that either have a weaker player-pessimal stable matching objective or apply only to markets with special assumptions. Additionally, the paper shows that the regret upper bound matches the previously derived lower bound when the preferences of participants satisfy some special conditions.<details>
<summary>Abstract</summary>
The problem of matching markets has been studied for a long time in the literature due to its wide range of applications. Finding a stable matching is a common equilibrium objective in this problem. Since market participants are usually uncertain of their preferences, a rich line of recent works study the online setting where one-side participants (players) learn their unknown preferences from iterative interactions with the other side (arms). Most previous works in this line are only able to derive theoretical guarantees for player-pessimal stable regret, which is defined compared with the players' least-preferred stable matching. However, under the pessimal stable matching, players only obtain the least reward among all stable matchings. To maximize players' profits, player-optimal stable matching would be the most desirable. Though \citet{basu21beyond} successfully bring an upper bound for player-optimal stable regret, their result can be exponentially large if players' preference gap is small. Whether a polynomial guarantee for this regret exists is a significant but still open problem. In this work, we provide a new algorithm named explore-then-Gale-Shapley (ETGS) and show that the optimal stable regret of each player can be upper bounded by $O(K\log T/\Delta^2)$ where $K$ is the number of arms, $T$ is the horizon and $\Delta$ is the players' minimum preference gap among the first $N+1$-ranked arms. This result significantly improves previous works which either have a weaker player-pessimal stable matching objective or apply only to markets with special assumptions. When the preferences of participants satisfy some special conditions, our regret upper bound also matches the previously derived lower bound.
</details>
<details>
<summary>æ‘˜è¦</summary>
é—®é¢˜çš„åŒ¹é…å¸‚åœºå·²ç»åœ¨æ–‡çŒ®ä¸­è¿›è¡Œäº†é•¿æ—¶é—´çš„ç ”ç©¶ï¼Œå› ä¸ºå®ƒåœ¨å„ç§åº”ç”¨åœºæ™¯ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚åœ¨è¿™ä¸ªé—®é¢˜ä¸­ï¼Œæ‰¾åˆ°ä¸€ä¸ªç¨³å®šçš„åŒ¹é…æ˜¯ä¸€ç§å¸¸è§çš„å¹³è¡¡ç›®æ ‡ã€‚ç”±äºå¸‚åœºå‚ä¸è€…é€šå¸¸å¯¹ä»–ä»¬çš„åå¥½ä¸ç”šæ¸…æ¥šï¼Œå› æ­¤ä¸€æ¨æœ€è¿‘çš„ç ”ç©¶åœ¨åœ¨çº¿è®¾ç½®ä¸‹ç ”ç©¶äº†å‚ä¸è€…åœ¨å¤šè½®äº’åŠ¨ä¸­å­¦ä¹ ä»–ä»¬æœªçŸ¥çš„åå¥½ã€‚å¤§å¤šæ•°å‰ä¸€äº›å·¥ä½œåªèƒ½ deriv theoretically guarantees for player-pessimal stable regretï¼Œå®ƒæ˜¯åŸºäºå‚ä¸è€…æœ€å·®åå¥½çš„ç¨³å®šåŒ¹é…ä¸­çš„æœ€ä½å¥–åŠ±ã€‚ç„¶è€Œï¼Œåœ¨æœ€ä½ç¨³å®šåŒ¹é…ä¸‹ï¼Œå‚ä¸è€…åªèƒ½è·å¾—æ‰€æœ‰ç¨³å®šåŒ¹é…ä¸­æœ€ä½çš„å¥–åŠ±ã€‚ä¸ºäº†æé«˜å‚ä¸è€…çš„æ”¶ç›Šï¼Œå‚ä¸è€…æœ€ä½³ç¨³å®šåŒ¹é…æ˜¯æœ€æ„Ÿåˆ°æ»¡æ„çš„ã€‚è™½ç„¶ \citet{basu21beyond} æˆåŠŸåœ°æå‡ºäº†ä¸€ä¸ªUpper bound for player-optimal stable regretï¼Œä½†å…¶ç»“æœå¯èƒ½ä¼šæ˜¯æŒ‡æ•°å¢é•¿çš„ï¼Œå¦‚æœå‚ä¸è€…åå¥½çš„å·®è·å¾ˆå°ã€‚whether a polynomial guarantee for this regret exists is a significant but still open problemã€‚åœ¨è¿™ä¸ªå·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„ç®—æ³•åä¸ºexplore-then-Gale-Shapleyï¼ˆETGSï¼‰ï¼Œå¹¶è¯æ˜äº†æ¯ä¸ªå‚ä¸è€…çš„æœ€ä½³ç¨³å®š regretå¯ä»¥ upper bounded by $O(K\log T/\Delta^2)$ï¼Œwhere $K$ is the number of arms, $T$ is the horizon, and $\Delta$ is the participants' minimum preference gap among the first $N+1$-ranked armsã€‚è¿™ä¸ªç»“æœæ¯”å‰ä¸€äº›å·¥ä½œæ›´å¥½ï¼Œå› ä¸ºå®ƒä»¬çš„ç›®æ ‡æ˜¯player-pessimal stable matchingï¼Œæˆ–è€…åªé€‚ç”¨äºç‰¹æ®Šçš„å¸‚åœºå‡è®¾ã€‚å½“å‚ä¸è€…çš„åå¥½æ»¡è¶³æŸäº›ç‰¹æ®Šæ¡ä»¶æ—¶ï¼Œæˆ‘ä»¬çš„ regret upper boundä¹Ÿä¸ä¹‹å‰ deriveçš„ä¸‹ç•ŒåŒ¹é…ã€‚
</details></li>
</ul>
<hr>
<h2 id="What-Twitter-Data-Tell-Us-about-the-Future"><a href="#What-Twitter-Data-Tell-Us-about-the-Future" class="headerlink" title="What Twitter Data Tell Us about the Future?"></a>What Twitter Data Tell Us about the Future?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02035">http://arxiv.org/abs/2308.02035</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alina Landowska, Marek Robak, Maciej Skorski</li>
<li>For: This paper investigates the futures projected by futurists on Twitter and explores the impact of language cues on anticipatory thinking among social media users.* Methods: The study uses a compiled dataset of over 1 million publicly shared tweets by future influencers and develops a scalable NLP pipeline using state-of-the-art models. The research employs topic modeling techniques, such as LDA and BERTopic, to identify the topics and language cues used by futurists.* Results: The study finds 15 topics from the LDA approach and 100 distinct topics from the BERTopic approach within the futuristsâ€™ tweets. The research demonstrates that the futuristsâ€™ language cues signal futures-in-the-making that enhance social media users to anticipate their own scenarios and respond to them in the present.Here are the three information points in Simplified Chinese text:* For: è¿™ä¸ªç ”ç©¶ investigate Twitter ä¸Šçš„æœªæ¥æŠ•å½±å’Œç¤¾äº¤åª’ä½“ç”¨æˆ·çš„é¢„æµ‹æ€ç»´ã€‚* Methods: è¿™ä¸ªç ”ç©¶ä½¿ç”¨äº†è¶…è¿‡100ä¸‡æ¬¡å…¬å…±åˆ†äº«çš„ tweetsï¼Œå¹¶å¼€å‘äº†å¯æ‰©å±•çš„è‡ªç„¶è¯­è¨€å¤„ç†ç®¡é“ï¼Œä½¿ç”¨å½“å‰çš„æ¨¡å‹ã€‚ç ”ç©¶ä½¿ç”¨äº†ä¸»é¢˜æ¨¡å‹ï¼Œå¦‚ LDA å’Œ BERTopicï¼Œæ¥æ ‡è¯†æœªæ¥è€…çš„è¯é¢˜å’Œè¯­è¨€æŒ‡ç¤ºã€‚* Results: ç ”ç©¶å‘ç°äº† LDA approach ä¸­çš„15ä¸ªè¯é¢˜å’Œ BERTopic approach ä¸­çš„100ä¸ªä¸åŒçš„è¯é¢˜ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæœªæ¥è€…çš„è¯­è¨€æŒ‡ç¤ºsignalæœªæ¥çš„å½¢æˆï¼Œä½¿ç¤¾äº¤åª’ä½“ç”¨æˆ·èƒ½å¤Ÿé¢„æµ‹è‡ªå·±çš„enarioå¹¶åœ¨ç°åœ¨ respond to itã€‚<details>
<summary>Abstract</summary>
Anticipation is a fundamental human cognitive ability that involves thinking about and living towards the future. While language markers reflect anticipatory thinking, research on anticipation from the perspective of natural language processing is limited. This study aims to investigate the futures projected by futurists on Twitter and explore the impact of language cues on anticipatory thinking among social media users. We address the research questions of what futures Twitter's futurists anticipate and share, and how these anticipated futures can be modeled from social data. To investigate this, we review related works on anticipation, discuss the influence of language markers and prestigious individuals on anticipatory thinking, and present a taxonomy system categorizing futures into "present futures" and "future present". This research presents a compiled dataset of over 1 million publicly shared tweets by future influencers and develops a scalable NLP pipeline using SOTA models. The study identifies 15 topics from the LDA approach and 100 distinct topics from the BERTopic approach within the futurists' tweets. These findings contribute to the research on topic modelling and provide insights into the futures anticipated by Twitter's futurists. The research demonstrates the futurists' language cues signals futures-in-the-making that enhance social media users to anticipate their own scenarios and respond to them in present. The fully open-sourced dataset, interactive analysis, and reproducible source code are available for further exploration.
</details>
<details>
<summary>æ‘˜è¦</summary>
äººç±»æœ‰ä¸€ç§åŸºæœ¬çš„è®¤çŸ¥èƒ½åŠ›ï¼Œå³é¢„æœŸï¼ˆanticipationï¼‰ï¼Œå®ƒå…³æ³¨æœªæ¥çš„å‘å±•å’Œç”Ÿæ´»ã€‚è™½ç„¶è¯­è¨€æ ‡è®°åæ˜ äº†é¢„æœŸæ€ç»´ï¼Œä½†ä»è‡ªç„¶è¯­è¨€å¤„ç†çš„è§’åº¦æ¥ç ”ç©¶é¢„æœŸå´æœ‰é™ã€‚è¿™é¡¹ç ”ç©¶ç›®çš„æ˜¯Investigate Twitterä¸Šçš„æœªæ¥é¢„æµ‹å’Œç¤¾äº¤åª’ä½“ç”¨æˆ·å¯¹æœªæ¥çš„é¢„æµ‹æ€ç»´çš„å½±å“ã€‚æˆ‘ä»¬è§£å†³çš„ç ”ç©¶é—®é¢˜åŒ…æ‹¬Twitterä¸Šé¢„æµ‹çš„æœªæ¥æ˜¯ä»€ä¹ˆå’Œè¿™äº›é¢„æµ‹å¦‚ä½•è¢«ç¤¾äº¤æ•°æ®æ¨¡å‹åŒ–ã€‚ä¸ºäº†è°ƒæŸ¥è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ç›¸å…³çš„ç ”ç©¶å’Œè¯­è¨€æ ‡è®°çš„å½±å“ä»¥åŠè‘—åäººå£«å¯¹é¢„æœŸæ€ç»´çš„å½±å“ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªâ€œç°åœ¨æœªæ¥â€å’Œâ€œæœªæ¥ç°åœ¨â€çš„åˆ†ç±»ç³»ç»Ÿã€‚æœ¬ç ”ç©¶ä½¿ç”¨äº†ä¸€äº¿å¤šä¸ªå…¬å…±åˆ†äº«çš„æ¨ç‰¹ä¿¡æ¯ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªå¯æ‰©å±•çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ç®¡é“ï¼Œä½¿ç”¨å½“å‰çš„æœ€ä½³å®è·µæ¨¡å‹ã€‚æˆ‘ä»¬ä»LDAæ–¹æ³•å’ŒBERTopicæ–¹æ³•ä¸­æå–äº†15ä¸ªä¸»é¢˜å’Œ100ä¸ªç‰¹å®šä¸»é¢˜ï¼Œè¿™äº›å‘ç°è´¡çŒ®äºä¸»é¢˜æ¨¡å‹ç ”ç©¶ï¼Œå¹¶ä¸ºTwitterä¸Šé¢„æµ‹æœªæ¥æä¾›äº†æ–°çš„è§†è§’ã€‚æœ¬ç ”ç©¶è¡¨æ˜é¢„æµ‹è€…çš„è¯­è¨€æ ‡è®°å¯ä»¥é¢„ç¤ºæœªæ¥çš„å‘å±•ï¼Œä½¿ç¤¾äº¤åª’ä½“ç”¨æˆ·èƒ½å¤Ÿé¢„æµ‹å’Œå“åº”ä»–ä»¬çš„enarioã€‚æˆ‘ä»¬æä¾›äº†å…¨éƒ¨å¼€æºçš„æ•°æ®é›†ã€äº¤äº’åˆ†æå’Œå¯é‡å¤çš„ä»£ç ï¼Œä»¥ä¾¿è¿›ä¸€æ­¥æ¢ç´¢ã€‚
</details></li>
</ul>
<hr>
<h2 id="Risk-optimized-Outlier-Removal-for-Robust-Point-Cloud-Classification"><a href="#Risk-optimized-Outlier-Removal-for-Robust-Point-Cloud-Classification" class="headerlink" title="Risk-optimized Outlier Removal for Robust Point Cloud Classification"></a>Risk-optimized Outlier Removal for Robust Point Cloud Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10875">http://arxiv.org/abs/2307.10875</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinke Li, Junchi Lu</li>
<li>for: è¿™ä¸ªç ”ç©¶çš„ç›®çš„æ˜¯ä¸ºäº†æé«˜ç‚¹äº‘æ·±åº¦æ¨¡å‹åœ¨å®‰å…¨æ•æ„Ÿåœºæ™¯ä¸­çš„å¯é æ€§å’Œå®‰å…¨æ€§ï¼Œå› ä¸ºè¿™äº›æ¨¡å‹å¯èƒ½ä¼šå—åˆ°æ„å¤–æˆ–è‡ªç„¶occurringçš„ç‚¹äº‘è¯¯å·®çš„å¹²æ‰°ã€‚</li>
<li>methods: è¿™ç¯‡ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ–°çš„ç‚¹äº‘å¼‚å¸¸ç‚¹é™¤é™¤æ³•ï¼Œcalled PointCVaRï¼Œå¯ä»¥è®©æ ‡å‡†è®­ç»ƒçš„æ¨¡å‹æ¶ˆé™¤é¢å¤–çš„å¼‚å¸¸ç‚¹å’Œé‡å»ºæ•°æ®ã€‚è¿™ä¸ªæ–¹æ³•å¼€å§‹æ˜¯é€šè¿‡åšå‡ºå±æ€§åˆ†æï¼Œä»¥ determine the influence of each point on the model outputï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºç‚¹äº‘é£é™©ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨Conditional Value at Risk (CVaR) æ¥ä¼˜åŒ–é«˜é£é™©ç‚¹çš„ç­›é€‰è¿‡ç¨‹ã€‚</li>
<li>results: è¿™ç¯‡ç ”ç©¶åœ¨ä¸åŒçš„ç‚¹äº‘è¯¯å·®æƒ…å†µä¸‹ï¼Œé€šè¿‡äº†å¤šç§ç§»é™¤å’Œåˆ†ç±»å®éªŒï¼Œè·å¾—äº†å‡ºè‰²çš„ç»“æœã€‚å°¤å…¶æ˜¯åœ¨å—åˆ°éšæœºè¯¯å·®ã€æ•Œæ„è¯¯å·®å’Œåé—¨è§¦å‘è¯¯å·®çš„æ”»å‡»ä¸‹ï¼ŒPointCVaRå¯ä»¥æˆåŠŸåœ°é˜²å¾¡è¿™äº›æ”»å‡»ï¼Œå¹¶ä¸”åœ¨è¿™äº›æƒ…å†µä¸‹ achieves 87% çš„ç²¾åº¦ã€‚<details>
<summary>Abstract</summary>
The popularity of point cloud deep models for safety-critical purposes has increased, but the reliability and security of these models can be compromised by intentional or naturally occurring point cloud noise. To combat this issue, we present a novel point cloud outlier removal method called PointCVaR, which empowers standard-trained models to eliminate additional outliers and restore the data. Our approach begins by conducting attribution analysis to determine the influence of each point on the model output, which we refer to as point risk. We then optimize the process of filtering high-risk points using Conditional Value at Risk (CVaR) as the objective. The rationale for this approach is based on the observation that noise points in point clouds tend to cluster in the tail of the risk distribution, with a low frequency but a high level of risk, resulting in significant interference with classification results. Despite requiring no additional training effort, our method produces exceptional results in various removal-and-classification experiments for noisy point clouds, which are corrupted by random noise, adversarial noise, and backdoor trigger noise. Impressively, it achieves 87% accuracy in defense against the backdoor attack by removing triggers. Overall, the proposed PointCVaR effectively eliminates noise points and enhances point cloud classification, making it a promising plug-in module for various models in different scenarios.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œéšç€æ·±åº¦ç‚¹äº‘æ¨¡å‹åœ¨å®‰å…¨æ•æ„Ÿé¢†åŸŸçš„æ™®åŠï¼Œè¿™äº›æ¨¡å‹å¯¹äºæ„å¤–æˆ–è‡ªç„¶å‘ç”Ÿçš„ç‚¹äº‘å™ªéŸ³çš„å¯é æ€§å’Œå®‰å…¨æ€§å—åˆ°æŸå®³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„ç‚¹äº‘å¼‚å¸¸ç‚¹é™¤é™¤æ³• called PointCVaRï¼Œå®ƒè®©æ ‡å‡†è®­ç»ƒçš„æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ¶ˆé™¤é¢å¤–çš„å¼‚å¸¸ç‚¹å’Œé‡å»ºæ•°æ®ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼€å§‹ WITH ç‚¹äº‘å½±å“åˆ†æï¼Œå†³å®šæ¯ä¸ªç‚¹çš„å½±å“åŠ›ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºç‚¹é£é™©ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨ Conditional Value at Riskï¼ˆCVaRï¼‰æ¥ä¼˜åŒ–é«˜é£é™©ç‚¹çš„èŒƒä¾‹ã€‚æˆ‘ä»¬å‘ç°ç‚¹äº‘å™ªéŸ³é€šå¸¸é›†ä¸­åœ¨é£é™©åˆ†å¸ƒçš„å°¾éƒ¨ï¼Œæœ‰è¾ƒä½çš„é¢‘ç‡ä½†é«˜åº¦çš„é£é™©ï¼Œå¯¼è‡´åˆ†ç±»ç»“æœå—åˆ°å¹²æ‰°ã€‚å°½ç®¡ä¸éœ€è¦é¢å¤–çš„è®­ç»ƒåŠªåŠ›ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸åŒçš„å®éªŒä¸­è·å¾—äº†å‡ºè‰²çš„æˆç»©ï¼ŒåŒ…æ‹¬éšæœºå™ªéŸ³ã€æ•Œæ„å™ªéŸ³å’Œåé—¨è§¦å‘å™ªéŸ³é™è½ã€‚ç‰¹åˆ«æ˜¯ï¼Œå®ƒåœ¨é˜²å¾¡åé—¨æ”»å‡»æ—¶å–å¾—äº†87%çš„å‡†ç¡®ç‡ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„PointCVaRå¯ä»¥å¹²æ‰°ç‚¹äº‘å™ªéŸ³ï¼Œæé«˜ç‚¹äº‘åˆ†ç±»ï¼Œä½¿å…¶æˆä¸ºä¸åŒæƒ…å†µä¸‹çš„å®ç”¨æ’ä»¶æ¨¡ç»„ã€‚â€
</details></li>
</ul>
<hr>
<h2 id="Nonlinear-Meta-Learning-Can-Guarantee-Faster-Rates"><a href="#Nonlinear-Meta-Learning-Can-Guarantee-Faster-Rates" class="headerlink" title="Nonlinear Meta-Learning Can Guarantee Faster Rates"></a>Nonlinear Meta-Learning Can Guarantee Faster Rates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10870">http://arxiv.org/abs/2307.10870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitri Meunier, Zhu Li, Arthur Gretton, Samory Kpotufe</li>
<li>for: æœ¬ç ”ç©¶çš„ç›®æ ‡æ˜¯ä¸ºmeta-å­¦ä¹ æä¾›ç†è®ºä¿è¯ï¼Œä»¥ä¾¿åœ¨ç›¸å…³ä»»åŠ¡ä¹‹é—´å…±äº«è¡¨ç¤ºç»“æ„ï¼Œä»è€Œç®€åŒ–ç›®æ ‡ä»»åŠ¡ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†éçº¿æ€§è¡¨ç¤ºï¼Œå¹¶é‡‡ç”¨äº†ä¸¥æ ¼çš„å¸¸è§å‡½æ•° rÃ©gularizationæ¥çº¦æŸä»»åŠ¡ç‰¹æœ‰çš„åè¯¯ã€‚</li>
<li>results: ç ”ç©¶äººå‘˜é€šè¿‡ teoretic åˆ†æå’Œå®éªŒ validateäº†meta-å­¦ä¹ çš„éçº¿æ€§è¡¨ç¤ºä¸‹çš„ä¿è¯ï¼Œå¹¶è¯æ˜äº†éšç€ä»»åŠ¡æ•°($N$)çš„å¢åŠ ï¼Œå­¦ä¹ å…±äº«è¡¨ç¤ºçš„é€Ÿç‡å¯ä»¥scaleã€‚<details>
<summary>Abstract</summary>
Many recent theoretical works on \emph{meta-learning} aim to achieve guarantees in leveraging similar representational structures from related tasks towards simplifying a target task. Importantly, the main aim in theory works on the subject is to understand the extent to which convergence rates -- in learning a common representation -- \emph{may scale with the number $N$ of tasks} (as well as the number of samples per task). First steps in this setting demonstrate this property when both the shared representation amongst tasks, and task-specific regression functions, are linear. This linear setting readily reveals the benefits of aggregating tasks, e.g., via averaging arguments. In practice, however, the representation is often highly nonlinear, introducing nontrivial biases in each task that cannot easily be averaged out as in the linear case. In the present work, we derive theoretical guarantees for meta-learning with nonlinear representations. In particular, assuming the shared nonlinearity maps to an infinite-dimensional RKHS, we show that additional biases can be mitigated with careful regularization that leverages the smoothness of task-specific regression functions,
</details>
<details>
<summary>æ‘˜è¦</summary>
å¾ˆå¤šæœ€è¿‘çš„ç†è®ºå·¥ä½œåœ¨meta-å­¦ä¸­ç›®æ ‡æ˜¯åˆ©ç”¨ç›¸ä¼¼çš„è¡¨ç¤ºç»“æ„æ¥ç®€åŒ–ç›®æ ‡ä»»åŠ¡ã€‚é‡è¦çš„æ˜¯ï¼Œç†è®ºå·¥ä½œçš„ä¸»è¦ç›®æ ‡æ˜¯ç†è§£å­¦ä¹ å…±äº«è¡¨ç¤ºç»“æ„æ—¶é€Ÿåº¦å¦‚ä½•éšç€ä»»åŠ¡æ•°é‡ $N$ å’Œæ ·æœ¬æ•°é‡çš„å¢åŠ è€Œå¢é•¿ã€‚åœ¨é¦–å…ˆæ­¥éª¤ä¸­ï¼Œå½“å…±äº«è¡¨ç¤ºç»“æ„å’Œä»»åŠ¡ç‰¹å®šçš„å›å½’å‡½æ•°éƒ½æ˜¯çº¿æ€§çš„æ—¶ï¼Œè¿™ç§æ€§è´¨ readily reveals the benefits of task aggregationï¼Œä¾‹å¦‚ï¼Œé€šè¿‡å¹³å‡Argumentsã€‚ç„¶è€Œï¼Œåœ¨å®è·µä¸­ï¼Œè¡¨ç¤ºç»“æ„é€šå¸¸æ˜¯éçº¿æ€§çš„ï¼Œå¼•å…¥äº†æ¯ä¸ªä»»åŠ¡ä¸­çš„éè½»æ¾åè§ï¼Œè¿™äº›åè§æ— æ³•å¦‚linear caseä¸­é‚£æ ·å¹³å‡åŒ–ã€‚åœ¨ presente å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ derive theoretical guarantees for meta-å­¦with nonlinear representationsã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å‡è®¾å…±äº«éçº¿æ€§æ˜ å°„åˆ°äº†æ— ç©·dimensional RKHSä¸­ï¼Œæˆ‘ä»¬æ˜¾ç¤ºäº†é€‚å½“çš„ regularizationå¯ä»¥å‡è½»ä»»åŠ¡ç‰¹å®šçš„åè§ï¼ŒåŒæ—¶åˆ©ç”¨ä»»åŠ¡ç‰¹å®šçš„å›å½’å‡½æ•°çš„å¹³æ»‘æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Performance-Issue-Identification-in-Cloud-Systems-with-Relational-Temporal-Anomaly-Detection"><a href="#Performance-Issue-Identification-in-Cloud-Systems-with-Relational-Temporal-Anomaly-Detection" class="headerlink" title="Performance Issue Identification in Cloud Systems with Relational-Temporal Anomaly Detection"></a>Performance Issue Identification in Cloud Systems with Relational-Temporal Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10869">http://arxiv.org/abs/2307.10869</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ase-submission/rtanomaly">https://github.com/ase-submission/rtanomaly</a></li>
<li>paper_authors: Wenwei Gu, Jinyang Liu, Zhuangbin Chen, Jianping Zhang, Yuxin Su, Jiazhen Gu, Cong Feng, Zengyin Yang, Michael Lyu</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æé«˜å¤§è§„æ¨¡äº‘æœåŠ¡ç³»ç»Ÿçš„å¯é æ€§å’Œæ€§èƒ½ï¼Œé€šè¿‡å‡†ç¡®åœ°è¯†åˆ«å’Œå®šä½é—®é¢˜ã€‚</li>
<li>methods: æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå…³ç³»å’Œæ—¶é—´ç‰¹å¾çš„å¤šå˜é‡å¼‚å¸¸æ£€æµ‹æ¨¡å‹ï¼ˆRTAnomalyï¼‰ï¼Œé€šè¿‡å›¾æ³¨æ„å±‚å­¦ä¹  metrics ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œæ›´å¥½åœ°å‘ç°å¼‚å¸¸ metricsã€‚</li>
<li>results: å¯¹äºå…¬å…±æ•°æ®é›†å’Œä¸¤ä¸ªå·¥ä¸šæ•°æ®é›†ï¼ŒRTAnomaly ä¸åŸºelineæ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œå®ç°äº†å¹³å‡ F1 åˆ†æ•°ä¸º 0.929 å’Œ Hit@3 ä¸º 0.920ï¼Œè¡¨æ˜RTAnomaly çš„ä¼˜è¶Šæ€§ã€‚<details>
<summary>Abstract</summary>
Performance issues permeate large-scale cloud service systems, which can lead to huge revenue losses. To ensure reliable performance, it's essential to accurately identify and localize these issues using service monitoring metrics. Given the complexity and scale of modern cloud systems, this task can be challenging and may require extensive expertise and resources beyond the capacity of individual humans. Some existing methods tackle this problem by analyzing each metric independently to detect anomalies. However, this could incur overwhelming alert storms that are difficult for engineers to diagnose manually. To pursue better performance, not only the temporal patterns of metrics but also the correlation between metrics (i.e., relational patterns) should be considered, which can be formulated as a multivariate metrics anomaly detection problem. However, most of the studies fall short of extracting these two types of features explicitly. Moreover, there exist some unlabeled anomalies mixed in the training data, which may hinder the detection performance. To address these limitations, we propose the Relational- Temporal Anomaly Detection Model (RTAnomaly) that combines the relational and temporal information of metrics. RTAnomaly employs a graph attention layer to learn the dependencies among metrics, which will further help pinpoint the anomalous metrics that may cause the anomaly effectively. In addition, we exploit the concept of positive unlabeled learning to address the issue of potential anomalies in the training data. To evaluate our method, we conduct experiments on a public dataset and two industrial datasets. RTAnomaly outperforms all the baseline models by achieving an average F1 score of 0.929 and Hit@3 of 0.920, demonstrating its superiority.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤§è§„æ¨¡äº‘æœåŠ¡ç³»ç»Ÿä¸­çš„æ€§èƒ½é—®é¢˜ä¼šå¯¼è‡´é‡å¤§çš„æ”¶ç›ŠæŸå¤±ã€‚ä¸ºç¡®ä¿å¯é æ€§ï¼Œéœ€è¦å‡†ç¡®åœ°è¯†åˆ«å’Œå®šä½è¿™äº›é—®é¢˜ä½¿ç”¨æœåŠ¡ç›‘æ§æŒ‡æ ‡ã€‚ç”±äºç°ä»£äº‘ç³»ç»Ÿçš„å¤æ‚æ€§å’Œè§„æ¨¡ï¼Œè¿™å¯èƒ½æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§å’Œéœ€è¦ä¸“ä¸šçŸ¥è¯†å’Œèµ„æºçš„ä»»åŠ¡ã€‚ç°æœ‰çš„æ–¹æ³•å¯èƒ½ä¼šåˆ†ææ¯ä¸ªæŒ‡æ ‡ç‹¬ç«‹åœ°æ£€æµ‹å¼‚å¸¸ã€‚ç„¶è€Œï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´è¿‡è½½çš„è­¦ç¤ºï¼Œä½¿å¾—å·¥ç¨‹å¸ˆéš¾ä»¥æ‰‹åŠ¨è¯Šæ–­ã€‚ä¸ºäº†æé«˜æ€§èƒ½ï¼Œä¸ä»…éœ€è¦è€ƒè™‘æ—¶é—´åºåˆ—ä¸­çš„æŒ‡æ ‡å¼‚å¸¸ï¼Œè¿˜éœ€è¦è€ƒè™‘æŒ‡æ ‡ä¹‹é—´çš„ç›¸äº’å…³ç³»ï¼ˆå³å…³ç³»å¼‚å¸¸ï¼‰ï¼Œè¿™å¯ä»¥è¢«è§†ä¸ºå¤šå˜é‡æŒ‡æ ‡å¼‚å¸¸æ£€æµ‹é—®é¢˜ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç ”ç©¶éƒ½æ²¡æœ‰æ˜ç¡®æå–è¿™ä¸¤ç§ç‰¹å¾ã€‚æ­¤å¤–ï¼Œå­˜åœ¨åœ¨è®­ç»ƒæ•°æ®ä¸­çš„æœªæ ‡æ³¨å¼‚å¸¸ï¼Œå¯èƒ½ä¼šé™ä½æ£€æµ‹æ€§èƒ½ã€‚ä¸ºè§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†å…³ç³»æ—¶é—´å¼‚å¸¸æ£€æµ‹æ¨¡å‹ï¼ˆRTAnomalyï¼‰ï¼Œè¯¥æ¨¡å‹å°†æŒ‡æ ‡ä¹‹é—´çš„å…³ç³»å’Œæ—¶é—´åºåˆ—ä¿¡æ¯ç»“åˆä½¿ç”¨ã€‚RTAnomalyä½¿ç”¨å›¾æ³¨æ„å±‚å­¦ä¹ æŒ‡æ ‡ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œä»¥æ›´å¥½åœ°å‘ç°å¯èƒ½å¯¼è‡´å¼‚å¸¸çš„å¼‚å¸¸æŒ‡æ ‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨æœªæ ‡æ³¨å¼‚å¸¸å­¦ä¹ çš„æ¦‚å¿µï¼Œä»¥Address the issue of potential anomalies in the training dataã€‚ä¸ºè¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨å…¬å…±æ•°æ®é›†å’Œä¸¤ä¸ªå·¥ä¸šæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒã€‚ç»“æœæ˜¾ç¤ºï¼ŒRTAnomalyåœ¨æ‰€æœ‰åŸºçº¿æ¨¡å‹ä¹‹ä¸Šå…·æœ‰å¹³å‡F1åˆ†æ•°0.929å’Œ Hit@3 0.920ï¼Œè¿™è¡¨æ˜å®ƒçš„ä¼˜åŠ¿ã€‚
</details></li>
</ul>
<hr>
<h2 id="FigCaps-HF-A-Figure-to-Caption-Generative-Framework-and-Benchmark-with-Human-Feedback"><a href="#FigCaps-HF-A-Figure-to-Caption-Generative-Framework-and-Benchmark-with-Human-Feedback" class="headerlink" title="FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback"></a>FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10867">http://arxiv.org/abs/2307.10867</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/figcapshf/figcapshf">https://github.com/figcapshf/figcapshf</a></li>
<li>paper_authors: Ashish Singh, Prateek Agarwal, Zixuan Huang, Arpita Singh, Tong Yu, Sungchul Kim, Victor Bursztyn, Nikos Vlassis, Ryan A. Rossi</li>
<li>for: è¿™ä¸ªè®ºæ–‡ä¸»è¦æ˜¯ä¸ºäº†è§£å†³ç§‘å­¦æ–‡çŒ®ä¸­å›¾æ–‡åˆæˆçš„é—®é¢˜ï¼Œæé«˜å›¾æ–‡åˆæˆçš„è´¨é‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>methods: è¯¥è®ºæ–‡ä½¿ç”¨äº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œå³ FigCaps-HFï¼Œæ¥ç”Ÿæˆå›¾æ–‡åˆæˆã€‚è¯¥æ¡†æ¶åŒ…æ‹¬è‡ªåŠ¨è¯„ä¼°å›¾æ–‡å¯¹çš„è´¨é‡ä»¥åŠåŸºäºäººå·¥åé¦ˆçš„å­¦ä¹ æ–¹æ³•ï¼Œä»¥ä¼˜åŒ–å›¾æ–‡åˆæˆçš„è´¨é‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>results: è¯¥è®ºæ–‡é€šè¿‡å¯¹ä¸åŒç±»å‹çš„æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œè¯æ˜äº† FigCaps-HF æ¡†æ¶å¯ä»¥æé«˜å›¾æ–‡åˆæˆçš„æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯ï¼Œå½“ä½¿ç”¨ BLIP ä½œä¸ºåŸºç¡€æ¨¡å‹æ—¶ï¼ŒRLHF æ–¹æ³•å¯ä»¥è·å¾—ä¸€ä¸ªå¹³å‡æå‡ç‡è¾¾ 35.7%ã€16.9% å’Œ 9% åœ¨ ROUGEã€BLEU å’Œ Meteor ç­‰æŒ‡æ ‡ä¸­ã€‚æ­¤å¤–ï¼Œè¯¥è®ºæ–‡è¿˜é‡Šæ”¾äº†ä¸€ä¸ªå¤§è§„æ¨¡çš„ benchmark æ•°æ®é›†ï¼Œä»¥ä¾¿è¿›ä¸€æ­¥è¯„ä¼°å’Œå‘å±• RLHF æŠ€æœ¯ã€‚<details>
<summary>Abstract</summary>
Captions are crucial for understanding scientific visualizations and documents. Existing captioning methods for scientific figures rely on figure-caption pairs extracted from documents for training, many of which fall short with respect to metrics like helpfulness, explainability, and visual-descriptiveness [15] leading to generated captions being misaligned with reader preferences. To enable the generation of high-quality figure captions, we introduce FigCaps-HF a new framework for figure-caption generation that can incorporate domain expert feedback in generating captions optimized for reader preferences. Our framework comprises of 1) an automatic method for evaluating quality of figure-caption pairs, 2) a novel reinforcement learning with human feedback (RLHF) method to optimize a generative figure-to-caption model for reader preferences. We demonstrate the effectiveness of our simple learning framework by improving performance over standard fine-tuning across different types of models. In particular, when using BLIP as the base model, our RLHF framework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and Meteor, respectively. Finally, we release a large-scale benchmark dataset with human feedback on figure-caption pairs to enable further evaluation and development of RLHF techniques for this problem.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>ç§‘å­¦è§†è§‰å’Œæ–‡æ¡£ä¸­çš„æ ‡é¢˜æ˜¯éå¸¸é‡è¦çš„ï¼Œç°æœ‰çš„ç§‘å­¦æ ‡é¢˜ç”Ÿæˆæ–¹æ³•éƒ½æ˜¯åŸºäºæ–‡æ¡£ä¸­æå–çš„figure-captionå¯¹ï¼Œä½†æ˜¯è¿™äº›æ–¹æ³• frequently fall short ï¼ˆ15ï¼‰ï¼Œå¯¼è‡´ç”Ÿæˆçš„æ ‡é¢˜ä¸è¯»è€…é¦–é€‰ä¸ç¬¦ã€‚ä¸ºäº†ç”Ÿæˆé«˜è´¨é‡çš„æ ‡é¢˜ï¼Œæˆ‘ä»¬ä»‹ç»äº†FigCaps-HFï¼Œä¸€ä¸ªæ–°çš„æ ‡é¢˜ç”Ÿæˆæ¡†æ¶ï¼Œå¯ä»¥åœ¨è¯»è€…é¦–é€‰çš„åŸºç¡€ä¸Šç”Ÿæˆæ ‡é¢˜ã€‚æˆ‘ä»¬çš„æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹ä¸¤ä¸ªéƒ¨åˆ†ï¼š1. ä¸€ç§è‡ªåŠ¨è¯„ä¼°figure-captionå¯¹çš„è´¨é‡æ–¹æ³•ã€‚2. ä¸€ç§åŸºäºäººå·¥åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æ–¹æ³•ï¼Œç”¨äºä¼˜åŒ–ä¸€ä¸ªç”Ÿæˆfigure-to-captionæ¨¡å‹ï¼Œä»¥æ»¡è¶³è¯»è€…é¦–é€‰ã€‚æˆ‘ä»¬çš„ç®€å•å­¦ä¹ æ¡†æ¶åœ¨ä¸åŒçš„æ¨¡å‹ä¸Šè¿›è¡Œäº†æ ‡å‡†åŒ–finetuningåï¼Œéƒ½èƒ½å¤Ÿæé«˜æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯å½“ä½¿ç”¨BLIPä½œä¸ºåŸºç¡€æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬çš„RLHFæ¡†æ¶å®ç°äº†ROUGEã€BLEUå’ŒMeteorç­‰æŒ‡æ ‡ä¸­çš„å¹³å‡æå‡ä¸º35.7%ã€16.9%å’Œ9%ã€‚æœ€åï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„äººå·¥åé¦ˆ benchmark datasetï¼Œä»¥ä¾¿è¿›ä¸€æ­¥è¯„ä¼°å’Œå‘å±•RLHFæŠ€æœ¯ã€‚
</details></li>
</ul>
<hr>
<h2 id="Addressing-caveats-of-neural-persistence-with-deep-graph-persistence"><a href="#Addressing-caveats-of-neural-persistence-with-deep-graph-persistence" class="headerlink" title="Addressing caveats of neural persistence with deep graph persistence"></a>Addressing caveats of neural persistence with deep graph persistence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10865">http://arxiv.org/abs/2307.10865</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ExplainableML/Deep-Graph-Persistence">https://github.com/ExplainableML/Deep-Graph-Persistence</a></li>
<li>paper_authors: Leander Girrbach, Anders Christensen, Ole Winther, Zeynep Akata, A. Sophia Koepke</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯ä¸ºäº†æå‡ºä¸€ç§æ–°çš„æ·±åº¦å­¦ä¹ ä¸­çš„æ•°æ®åˆ†ææ–¹æ³•ï¼Œä»¥åŠä¸€ç§åŸºäºè¿™ç§æ–¹æ³•çš„æ·±åº¦ç½‘ç»œå¤æ‚åº¦é‡åº¦ã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº†topologicalæ•°æ®åˆ†æçš„æ–¹æ³•ï¼Œä»¥åŠä¸€ç§æ–°çš„å±‚é—´æ‹Ÿåˆæ–¹æ³•æ¥å¤„ç†æ·±åº¦ç½‘ç»œã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œæ·±åº¦ç½‘ç»œçš„å±‚æ¬¡ç»“æ„å’Œå¤§é‡ weights çš„åˆ†å¸ƒæ˜¯å†³å®š neural persistence çš„ä¸¤å¤§å› ç´ ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¯¹æ·±åº¦ç½‘ç»œè¿›è¡Œæ‰©å±•ï¼Œå¯ä»¥è§£å†³ variance ç›¸å…³çš„é—®é¢˜ï¼Œå¹¶ä¸”å¯ä»¥å‡†ç¡®åœ°é‡åº¦æ·±åº¦ç½‘ç»œçš„å¤æ‚åº¦ã€‚<details>
<summary>Abstract</summary>
Neural Persistence is a prominent measure for quantifying neural network complexity, proposed in the emerging field of topological data analysis in deep learning. In this work, however, we find both theoretically and empirically that the variance of network weights and spatial concentration of large weights are the main factors that impact neural persistence. Whilst this captures useful information for linear classifiers, we find that no relevant spatial structure is present in later layers of deep neural networks, making neural persistence roughly equivalent to the variance of weights. Additionally, the proposed averaging procedure across layers for deep neural networks does not consider interaction between layers. Based on our analysis, we propose an extension of the filtration underlying neural persistence to the whole neural network instead of single layers, which is equivalent to calculating neural persistence on one particular matrix. This yields our deep graph persistence measure, which implicitly incorporates persistent paths through the network and alleviates variance-related issues through standardisation. Code is available at https://github.com/ExplainableML/Deep-Graph-Persistence .
</details>
<details>
<summary>æ‘˜è¦</summary>
neural  persistency æ˜¯ä¸€ç§æ·±åº¦å­¦ä¹ ä¸­çš„æ ¸å¿ƒåº¦é‡ï¼Œåœ¨ topological data analysis é¢†åŸŸä¸­æå‡ºã€‚åœ¨è¿™ç§å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å‘ç°äº†è®ºç†å’Œå®éªŒä¸¤ä¸ªæ–¹é¢çš„ç»“è®ºï¼šç½‘ç»œæƒé‡çš„æ–¹å·®å’Œå¤§æƒé‡çš„ç©ºé—´å¸å¼•åŠ›æ˜¯å½±å“ neural persistency çš„ä¸»è¦å› ç´ ã€‚è¿™äº›ä¿¡æ¯å¯¹äºçº¿æ€§åˆ†ç±»å™¨æ˜¯æœ‰ç”¨çš„ï¼Œä½†æˆ‘ä»¬å‘ç°äº†æ·±å±‚ç¥ç»ç½‘ç»œä¸­çš„åLayeræ²¡æœ‰ç›¸å…³çš„ç©ºé—´ç»“æ„ï¼Œå› æ­¤ neural persistency å¤§è‡´ç›¸å½“äºç½‘ç»œæƒé‡çš„æ–¹å·®ã€‚æ­¤å¤–ï¼Œå¯¹äºæ·±åº¦ç¥ç»ç½‘ç»œï¼Œå±‚èåˆç­–ç•¥ä¸è€ƒè™‘å±‚ä¹‹é—´çš„äº¤äº’ã€‚åŸºäºæˆ‘ä»¬çš„åˆ†æï¼Œæˆ‘ä»¬æå‡ºäº†å±‚æ‹“æ‰‘ä¸‹çš„ filtration æ‰©å±•ï¼Œè¯¥æ‰©å±•ç­‰äºåœ¨ä¸€ä¸ªç‰¹å®šçŸ©é˜µä¸Šè®¡ç®— neural persistencyã€‚è¿™ä¸ªæ–¹æ³•ä¼šéšå¼åœ°åŒ…å«ç¥ç»ç½‘ç»œä¸­çš„æŒç»­è·¯å¾„å’Œå‡å°‘æ–¹å·®ç›¸å…³çš„é—®é¢˜ã€‚ä»£ç å¯ä»¥åœ¨ <https://github.com/ExplainableML/Deep-Graph-Persistence> ä¸Šæ‰¾åˆ°ã€‚
</details></li>
</ul>
<hr>
<h2 id="Divide-Bind-Your-Attention-for-Improved-Generative-Semantic-Nursing"><a href="#Divide-Bind-Your-Attention-for-Improved-Generative-Semantic-Nursing" class="headerlink" title="Divide &amp; Bind Your Attention for Improved Generative Semantic Nursing"></a>Divide &amp; Bind Your Attention for Improved Generative Semantic Nursing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10864">http://arxiv.org/abs/2307.10864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yumeng Li, Margret Keuper, Dan Zhang, Anna Khoreva</li>
<li>for: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§åŸºäºGenerative Semantic Nursingï¼ˆGSNï¼‰çš„æ–¹æ³•ï¼Œç”¨äºè§£å†³å¤æ‚çš„æç¤ºé—®é¢˜å’Œå¤šä¸ªå®ä½“ä¹‹é—´çš„å±æ€§ç»‘å®šé—®é¢˜ã€‚</li>
<li>methods: è¯¥æ–¹æ³•ä½¿ç”¨äº†ä¸¤ä¸ªæ–°çš„æŸå¤±å‡½æ•°ï¼šä¸€ä¸ªæ–°çš„æ³¨æ„åŠ›æŸå¤±å‡½æ•°å’Œä¸€ä¸ªç»‘å®šæŸå¤±å‡½æ•°ï¼Œä»¥æé«˜GSNçš„è¡¨ç°ã€‚</li>
<li>results: è¯¥æ–¹æ³•åœ¨å¤šä¸ªè¯„ä¼°æ ‡å‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿå‡†ç¡®åœ°synthesizeæ‰€éœ€çš„å¯¹è±¡ï¼Œå¹¶ä¸”Attribute bindingæ›´åŠ ç´§å¯†ã€‚æ›´å¤šè§†é¢‘å’Œæ›´æ–°å¯ä»¥åœ¨é¡¹ç›®é¡µé¢ä¸Šæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://sites.google.com/view/divide-and-bind">https://sites.google.com/view/divide-and-bind</a>ã€‚<details>
<summary>Abstract</summary>
Emerging large-scale text-to-image generative models, e.g., Stable Diffusion (SD), have exhibited overwhelming results with high fidelity. Despite the magnificent progress, current state-of-the-art models still struggle to generate images fully adhering to the input prompt. Prior work, Attend & Excite, has introduced the concept of Generative Semantic Nursing (GSN), aiming to optimize cross-attention during inference time to better incorporate the semantics. It demonstrates promising results in generating simple prompts, e.g., ``a cat and a dog''. However, its efficacy declines when dealing with more complex prompts, and it does not explicitly address the problem of improper attribute binding. To address the challenges posed by complex prompts or scenarios involving multiple entities and to achieve improved attribute binding, we propose Divide & Bind. We introduce two novel loss objectives for GSN: a novel attendance loss and a binding loss. Our approach stands out in its ability to faithfully synthesize desired objects with improved attribute alignment from complex prompts and exhibits superior performance across multiple evaluation benchmarks. More videos and updates can be found on the project page \url{https://sites.google.com/view/divide-and-bind}.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ–°å‹å¤§è§„æ¨¡æ–‡æœ¬è‡³å›¾ç”Ÿæˆæ¨¡å‹ï¼Œå¦‚ç¨³å®šæ‰©æ•£ï¼ˆSDï¼‰ï¼Œå·²ç»æ˜¾ç¤ºå‡ºæƒŠäººçš„æˆæœï¼Œå…·æœ‰é«˜å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œå½“å‰é¢†å…ˆçš„æ¨¡å‹ä»ç„¶éš¾ä»¥ç”Ÿæˆå®Œå…¨éµå¾ªè¾“å…¥æç¤ºçš„å›¾åƒã€‚å…ˆå‰çš„å·¥ä½œï¼Œå¬å–ä¸æ¿€å‘ï¼ˆAttend & Exciteï¼‰ï¼Œå¼•å…¥äº†ç”Ÿæˆ semantic nursingï¼ˆGSNï¼‰çš„æ¦‚å¿µï¼Œé€šè¿‡åœ¨æ¨ç†æ—¶é—´è¿›è¡Œäº¤å‰æ³¨æ„åŠ›ä¼˜åŒ–ï¼Œä»¥æ›´å¥½åœ°åŒ…å« semanticsã€‚å®ƒåœ¨ç”Ÿæˆç®€å•æç¤ºï¼ˆä¾‹å¦‚ï¼Œâ€œä¸€åªçŒ«å’Œä¸€åªç‹—â€ï¼‰ä¸­æ˜¾ç¤ºäº†æ‰å®çš„æˆæœã€‚ç„¶è€Œï¼Œå…¶æ•ˆæœåœ¨å¤„ç†æ›´å¤æ‚çš„æç¤ºæ—¶ä¸‹é™ï¼Œå¹¶ä¸ç›´æ¥åœ°è§£å†³ä¸æ­£ç¡®çš„å±æ€§ç»‘å®šé—®é¢˜ã€‚ä¸ºäº†è§£å†³å¤æ‚æç¤ºæˆ–åœºæ™¯ä¸­å¤šä¸ªå®ä½“çš„æŒ‘æˆ˜ï¼Œä»¥åŠæé«˜å±æ€§ç»‘å®šçš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†åŒºä¸ç»‘å®šï¼ˆDivide & Bindï¼‰æ–¹æ³•ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸¤ç§æ–°çš„æŸå¤±ç›®æ ‡ï¼šä¸€ç§æ–°çš„æ³¨æ„åŠ›æŸå¤±å’Œä¸€ç§ç»‘å®šæŸå¤±ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤„ç†å¤æ‚æç¤ºä¸‹èƒ½å¤Ÿå‡†ç¡®åœ°ç”Ÿæˆæ„¿æ™¯ä¸­çš„ç›®æ ‡å¯¹è±¡ï¼Œå¹¶ä¸”å…·æœ‰æ”¹è¿›çš„å±æ€§Alignmentã€‚æ›´å¤šè§†é¢‘å’Œæ›´æ–°å¯ä»¥åœ¨é¡¹ç›®é¡µé¢ï¼ˆ<https://sites.google.com/view/divide-and-bind>ï¼‰ä¸­æ‰¾åˆ°ã€‚
</details></li>
</ul>
<hr>
<h2 id="Self-paced-Weight-Consolidation-for-Continual-Learning"><a href="#Self-paced-Weight-Consolidation-for-Continual-Learning" class="headerlink" title="Self-paced Weight Consolidation for Continual Learning"></a>Self-paced Weight Consolidation for Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10845">http://arxiv.org/abs/2307.10845</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/congwei45/spWC">https://github.com/congwei45/spWC</a></li>
<li>paper_authors: Wei Cong, Yang Cong, Gan Sun, Yuyang Liu, Jiahua Dong</li>
<li>for: è¿™ä¸ªç ”ç©¶æ—¨åœ¨æé«˜sequential task learningä¸­çš„continual learningæ•ˆèƒ½ï¼Œå¹¶é¿å…catastrophic forgettingè¿™ä¸ªé—®é¢˜ã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè‡ªé€‚åº”çš„Weight Consolidationï¼ˆspWCï¼‰æ¡†æ¶ï¼Œé€šè¿‡è¯„ä¼°å…ˆå‰ä»»åŠ¡çš„æ¨å¯¼æ€§è´¡çŒ®æ¥å®ç° Robust continual learningã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªè‡ªé€‚åº”çš„è°ƒæ•´æ–¹æ³•ï¼Œå¯ä»¥æ ¹æ®å…³é”®æ€§è¡¨ç°æŒ‡æ ‡ï¼ˆä¾‹å¦‚ç²¾åº¦ï¼‰æ¥è¯„ä¼°è¿‡å»ä»»åŠ¡çš„å›°éš¾ç¨‹åº¦ã€‚</li>
<li>results: æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å¯¹å¤šä¸ªä»»åŠ¡è¿›è¡Œsequential learningï¼Œå¹¶ä¸”å¯ä»¥å®ç°better performanceå’Œless computational costã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ä¸å…¶ä»–æµè¡Œçš„continual learningç®—æ³•ç›¸æ¯”ï¼Œåœ¨å¤šä¸ªå…¬å…±Benchmark datasetä¸Šå®ç°æ›´å¥½çš„æ•ˆæœã€‚<details>
<summary>Abstract</summary>
Continual learning algorithms which keep the parameters of new tasks close to that of previous tasks, are popular in preventing catastrophic forgetting in sequential task learning settings. However, 1) the performance for the new continual learner will be degraded without distinguishing the contributions of previously learned tasks; 2) the computational cost will be greatly increased with the number of tasks, since most existing algorithms need to regularize all previous tasks when learning new tasks. To address the above challenges, we propose a self-paced Weight Consolidation (spWC) framework to attain robust continual learning via evaluating the discriminative contributions of previous tasks. To be specific, we develop a self-paced regularization to reflect the priorities of past tasks via measuring difficulty based on key performance indicator (i.e., accuracy). When encountering a new task, all previous tasks are sorted from "difficult" to "easy" based on the priorities. Then the parameters of the new continual learner will be learned via selectively maintaining the knowledge amongst more difficult past tasks, which could well overcome catastrophic forgetting with less computational cost. We adopt an alternative convex search to iteratively update the model parameters and priority weights in the bi-convex formulation. The proposed spWC framework is plug-and-play, which is applicable to most continual learning algorithms (e.g., EWC, MAS and RCIL) in different directions (e.g., classification and segmentation). Experimental results on several public benchmark datasets demonstrate that our proposed framework can effectively improve performance when compared with other popular continual learning algorithms.
</details>
<details>
<summary>æ‘˜è¦</summary>
CONTINUAL LEARNINGç®—æ³•ï¼Œå®ƒä»¬ä¿æŒæ–°ä»»åŠ¡å‚æ•°ä¸å‰ä¸€ä¸ªä»»åŠ¡ç›¸ä¼¼ï¼Œåœ¨Sequential task learning settingä¸­å¾ˆå—æ¬¢è¿ã€‚ä½†æ˜¯ï¼Œ1ï¼‰æ–°çš„ continual learnerçš„æ€§èƒ½å°†å—åˆ°å‰ä¸€ä¸ªä»»åŠ¡çš„è´¡çŒ®çš„å½±å“ï¼Œè€Œæ— æ³•åˆ†åˆ«è¯„ä¼°è¿™äº›è´¡çŒ®ï¼›2ï¼‰éšç€ä»»åŠ¡çš„å¢åŠ ï¼Œç°æœ‰çš„ç®—æ³•çš„è®¡ç®—æˆæœ¬å°†å¢åŠ å¾ˆå¤šï¼Œå› ä¸ºå®ƒä»¬éœ€è¦å¯¹æ‰€æœ‰ä»»åŠ¡è¿›è¡ŒRegularizationã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè‡ªé€‚åº”Weight Consolidationï¼ˆspWCï¼‰æ¡†æ¶ï¼Œä»¥å®ç°Robust continual learningã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§è‡ªé€‚åº”Regularizationï¼Œé€šè¿‡æµ‹é‡éš¾åº¦æ¥è¯„ä¼°è¿‡å»ä»»åŠ¡çš„ä¼˜å…ˆçº§ã€‚å½“é‡åˆ°æ–°ä»»åŠ¡æ—¶ï¼Œæˆ‘ä»¬å°†æ‰€æœ‰è¿‡å»ä»»åŠ¡æ’åºä¸ºâ€œdifficultâ€åˆ°â€œeasyâ€çš„é¡ºåºï¼Œæ ¹æ®ä¼˜å…ˆçº§ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†æ–°çš„ continual learnerçš„å‚æ•°å­¦ä¹  via é€‰æ‹©ä¿ç•™è¿‡å»ä»»åŠ¡ä¸­æ›´éš¾çš„çŸ¥è¯†ã€‚è¿™å¯ä»¥å¾ˆå¥½åœ°è§£å†³catastrophic forgettingé—®é¢˜ï¼ŒåŒæ—¶é™ä½è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§alternative convex searchæ¥é€æ­¥æ›´æ–°æ¨¡å‹å‚æ•°å’Œä¼˜å…ˆçº§æƒé‡ã€‚æˆ‘ä»¬çš„spWCæ¡†æ¶æ˜¯å¯æ’å…¥çš„ï¼Œå¯ä»¥åº”ç”¨äºå¤§å¤šæ•° continual learningç®—æ³•ï¼ˆä¾‹å¦‚EWCã€MASå’ŒRCILï¼‰ä»¥åŠä¸åŒçš„æ–¹å‘ï¼ˆä¾‹å¦‚åˆ†ç±»å’Œåˆ†å‰²ï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æè®®å¯ä»¥åœ¨å¤šä¸ªå…¬å…± benchmark datasetä¸Šæé«˜æ€§èƒ½ï¼Œç›¸æ¯”å…¶ä»–æµè¡Œçš„ continual learningç®—æ³•ã€‚
</details></li>
</ul>
<hr>
<h2 id="Global-Precipitation-Nowcasting-of-Integrated-Multi-satellitE-Retrievals-for-GPM-A-U-Net-Convolutional-LSTM-Architecture"><a href="#Global-Precipitation-Nowcasting-of-Integrated-Multi-satellitE-Retrievals-for-GPM-A-U-Net-Convolutional-LSTM-Architecture" class="headerlink" title="Global Precipitation Nowcasting of Integrated Multi-satellitE Retrievals for GPM: A U-Net Convolutional LSTM Architecture"></a>Global Precipitation Nowcasting of Integrated Multi-satellitE Retrievals for GPM: A U-Net Convolutional LSTM Architecture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10843">http://arxiv.org/abs/2307.10843</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/reyhaneh-92/genesis_nowcast">https://github.com/reyhaneh-92/genesis_nowcast</a></li>
<li>paper_authors: Reyhaneh Rahimi, Ardeshir Ebtehaj, Ali Behrangi, Jackson Tan</li>
<li>for: è¿™ä¸ªç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œç”¨äºå…¨çƒèŒƒå›´å†…é™æ°´é¢„æµ‹ï¼Œæ¯30åˆ†é’Ÿé¢„æµ‹4å°æ—¶å‰çš„é™æ°´æƒ…å†µã€‚</li>
<li>methods: è¿™ä¸ªæ¶æ„ç»“åˆäº†U-Netå’Œå·ç§¯é•¿Short-Term Memoryï¼ˆLSTMï¼‰ç¥ç»ç½‘ç»œï¼Œå¹¶ä½¿ç”¨IMERGå’Œä¸€äº›å…³é”®çš„é™æ°´é©±åŠ¨å› ç´ ä»å…¨çƒé¢„æµ‹ç³»ç»Ÿï¼ˆGFSï¼‰æ¥è®­ç»ƒã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨ä¸åŒçš„è®­ç»ƒæŸå¤±å‡½æ•°ï¼ŒåŒ…æ‹¬å¹³å‡æ–¹å·®ï¼ˆå›å½’ï¼‰å’Œç„¦ç‚¹æŸå¤±ï¼ˆåˆ†ç±»ï¼‰ï¼Œå¯¹é™æ°´é¢„æµ‹è´¨é‡æœ‰ç€ä¸åŒçš„å½±å“ã€‚ç»“æœè¡¨æ˜ï¼Œå›å½’ç½‘ç»œåœ¨é™æ°´è½»åº¦ï¼ˆä¸‹1.6æ¯«ç±³&#x2F;å°æ—¶ï¼‰æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œè€Œåˆ†ç±»ç½‘ç»œåœ¨é™æ°´æç«¯ï¼ˆå¤§äº8æ¯«ç±³&#x2F;å°æ—¶ï¼‰æ–¹é¢å¯ä»¥è¶…è¿‡å›å½’ç½‘ç»œï¼Œä»¥ Critical Success Indexï¼ˆCSIï¼‰ä¸ºæ ‡å‡†ã€‚åŒæ—¶ï¼ŒåŒ…å«ç‰©ç†å˜é‡å¯ä»¥æé«˜é™æ°´é¢„æµ‹ï¼Œç‰¹åˆ«æ˜¯åœ¨è¾ƒé•¿çš„é¢„æµ‹æ—¶é—´å†…ã€‚<details>
<summary>Abstract</summary>
This paper presents a deep learning architecture for nowcasting of precipitation almost globally every 30 min with a 4-hour lead time. The architecture fuses a U-Net and a convolutional long short-term memory (LSTM) neural network and is trained using data from the Integrated MultisatellitE Retrievals for GPM (IMERG) and a few key precipitation drivers from the Global Forecast System (GFS). The impacts of different training loss functions, including the mean-squared error (regression) and the focal-loss (classification), on the quality of precipitation nowcasts are studied. The results indicate that the regression network performs well in capturing light precipitation (below 1.6 mm/hr), but the classification network can outperform the regression network for nowcasting of precipitation extremes (>8 mm/hr), in terms of the critical success index (CSI).. Using the Wasserstein distance, it is shown that the predicted precipitation by the classification network has a closer class probability distribution to the IMERG than the regression network. It is uncovered that the inclusion of the physical variables can improve precipitation nowcasting, especially at longer lead times in both networks. Taking IMERG as a relative reference, a multi-scale analysis in terms of fractions skill score (FSS), shows that the nowcasting machine remains skillful (FSS > 0.5) at the resolution of 10 km compared to 50 km for GFS. For precipitation rates greater than 4~mm/hr, only the classification network remains FSS-skillful on scales greater than 50 km within a 2-hour lead time.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Label-Calibration-for-Semantic-Segmentation-Under-Domain-Shift"><a href="#Label-Calibration-for-Semantic-Segmentation-Under-Domain-Shift" class="headerlink" title="Label Calibration for Semantic Segmentation Under Domain Shift"></a>Label Calibration for Semantic Segmentation Under Domain Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10842">http://arxiv.org/abs/2307.10842</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ondrej Bohdal, Da Li, Timothy Hospedales</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ˜¯ç”¨äºæµ‹è¯•ä¸€ä¸ªé¢„è®­ semantic segmentation æ¨¡å‹åœ¨æ–°Domainä¸Šçš„æ€§èƒ½æ˜¯å¦ä¼šä¸¥é‡ä¸‹é™ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†ä¸€ç§åŸºäºåŸŸShiftçš„é¢„è®­æ¨¡å‹è¿›è¡Œé€‚åº”ï¼Œé€šè¿‡è®¡ç®—è½¯ labels prototype å¹¶æ ¹æ®æœ€ç›¸ä¼¼çš„åˆ†ç±»æ¦‚ç‡ vector è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>results: è®ºæ–‡æ˜¾ç¤ºäº†è¿™ç§é€‚åº”æ–¹æ³•å¯ä»¥å¾ˆå¿«é€Ÿã€å‡ ä¹ä¸éœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºï¼Œå¹¶ä¸”èƒ½å¤Ÿæé«˜æ€§èƒ½ã€‚å®ƒè¿˜è¯æ˜äº†è¿™ç§é€‚åº”æ–¹æ³•åœ¨å®é™…ä¸Šæ˜¯éå¸¸æœ‰ç”¨çš„ã€‚<details>
<summary>Abstract</summary>
Performance of a pre-trained semantic segmentation model is likely to substantially decrease on data from a new domain. We show a pre-trained model can be adapted to unlabelled target domain data by calculating soft-label prototypes under the domain shift and making predictions according to the prototype closest to the vector with predicted class probabilities. The proposed adaptation procedure is fast, comes almost for free in terms of computational resources and leads to considerable performance improvements. We demonstrate the benefits of such label calibration on the highly-practical synthetic-to-real semantic segmentation problem.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œä¸€ä¸ªå…ˆè¿›çš„è¯­ä¹‰åˆ†å‰²æ¨¡å‹åœ¨æ–°é¢†åŸŸæ•°æ®ä¸Šçš„æ€§èƒ½å¯èƒ½ä¼šå‡é€€å¾ˆå¤šã€‚æˆ‘ä»¬æ˜¾ç¤ºäº†ä¸€ä¸ªå…ˆè¿›æ¨¡å‹å¯ä»¥é€šè¿‡è®¡ç®—åŸŸè½¬ç§»ä¸‹çš„è½¯æ ‡ç­¾ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ñ‹ï¼Œå¹¶æ ¹æ®æœ€ç›¸ä¼¼çš„ Ğ²ĞµĞºÑ‚Ğ¾Ñ€é¢„æµ‹ç±»åˆ«æ¦‚ç‡æ¥è¿›è¡Œé¢„æµ‹ã€‚æˆ‘ä»¬æå‡ºçš„é€‚åº”è¿‡ç¨‹å¿«é€Ÿã€è®¡ç®—èµ„æºå‡ ä¹æ²¡æœ‰æˆæœ¬ï¼Œå¹¶å¯¼è‡´äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬åœ¨å®é™…ä¸Šéå¸¸æœ‰ç”¨çš„ sintetic-to-realè¯­ä¹‰åˆ†å‰²é—®é¢˜ä¸­å±•ç¤ºäº†è¿™ç§æ ‡ç­¾å‡†ç¡®åŒ–çš„å¥½å¤„ã€‚â€Note: "åŸŸè½¬ç§»" (domain shift) is translated as "åŸŸè½¬ç§»" in Simplified Chinese, and "soft-label" is translated as "è½¯æ ‡ç­¾" in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Conversational-Shaping-for-Intelligent-Agents"><a href="#Adversarial-Conversational-Shaping-for-Intelligent-Agents" class="headerlink" title="Adversarial Conversational Shaping for Intelligent Agents"></a>Adversarial Conversational Shaping for Intelligent Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11785">http://arxiv.org/abs/2307.11785</a></li>
<li>repo_url: None</li>
<li>paper_authors: Piotr Tarasiewicz, Sultan Kenjeyev, Ilana Sebag, Shehab Alshehabi</li>
<li>for: æé«˜å¯¹è¯ä»£ç†äººçš„æ™ºèƒ½ä¼šè¯ç³»ç»Ÿç¨³å®šæ€§å’Œå‡†ç¡®æ€§</li>
<li>methods: ä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANPGï¼‰å’Œå¥–åŠ±æ¯ä¸€ä¸ªç”Ÿæˆæ­¥éª¤ï¼ˆREGSï¼‰æ¨¡å‹ï¼Œå¹¶åœ¨seq2seqå’Œ transformers æ¡†æ¶ä¸‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ </li>
<li>results: é€šè¿‡ä¸åŒçš„è®­ç»ƒç»†èŠ‚ï¼Œæ¨¡å‹å¯ä»¥æé«˜å¯¹è¯ä»£ç†äººçš„æ€§èƒ½å’Œå¯é æ€§<details>
<summary>Abstract</summary>
The recent emergence of deep learning methods has enabled the research community to achieve state-of-the art results in several domains including natural language processing. However, the current robocall system remains unstable and inaccurate: text generator and chat-bots can be tedious and misunderstand human-like dialogue. In this work, we study the performance of two models able to enhance an intelligent conversational agent through adversarial conversational shaping: a generative adversarial network with policy gradient (GANPG) and a generative adversarial network with reward for every generation step (REGS) based on the REGS model presented in Li et al. [18] . This model is able to assign rewards to both partially and fully generated text sequences. We discuss performance with different training details : seq2seq [ 36] and transformers [37 ] in a reinforcement learning framework.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="What-Indeed-is-an-Achievable-Provable-Guarantee-for-Learning-Enabled-Safety-Critical-Systems"><a href="#What-Indeed-is-an-Achievable-Provable-Guarantee-for-Learning-Enabled-Safety-Critical-Systems" class="headerlink" title="What, Indeed, is an Achievable Provable Guarantee for Learning-Enabled Safety Critical Systems"></a>What, Indeed, is an Achievable Provable Guarantee for Learning-Enabled Safety Critical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11784">http://arxiv.org/abs/2307.11784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saddek Bensalem, Chih-Hong Cheng, Wei Huang, Xiaowei Huang, Changshun Wu, Xingyu Zhao</li>
<li>for: æœ¬æ–‡æ—¨åœ¨æå‡ºä¸€ç§å¯é åœ°åœ¨å®‰å…¨å…³é”®é¢†åŸŸä½¿ç”¨å­¦ä¹ èƒ½åŠ›çš„æ–¹æ³•ï¼Œä»¥ç¡®ä¿ç³»ç»Ÿçš„å®‰å…¨æ€§ã€‚</li>
<li>methods: æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸¤æ­¥éªŒè¯æ–¹æ³•ï¼Œä»¥å®ç°å¯è¯æ˜çš„ç»Ÿè®¡ä¿è¯ã€‚</li>
<li>results: æœ¬æ–‡è®¤ä¸ºï¼Œç°æœ‰çš„æ–¹æ³•æ— æ³•å®é™…å®ç°å¯è¯æ˜çš„ä¿è¯ï¼Œtherefore promoting the two-step verification method for achieving provable statistical guarantees.<details>
<summary>Abstract</summary>
Machine learning has made remarkable advancements, but confidently utilising learning-enabled components in safety-critical domains still poses challenges. Among the challenges, it is known that a rigorous, yet practical, way of achieving safety guarantees is one of the most prominent. In this paper, we first discuss the engineering and research challenges associated with the design and verification of such systems. Then, based on the observation that existing works cannot actually achieve provable guarantees, we promote a two-step verification method for the ultimate achievement of provable statistical guarantees.
</details>
<details>
<summary>æ‘˜è¦</summary>
æœºå™¨å­¦ä¹ æŠ€æœ¯å·²ç»å–å¾—äº†å¾ˆå¤§çš„è¿›æ­¥ï¼Œä½†åœ¨å®‰å…¨å…³é”®é¢†åŸŸä½¿ç”¨å­¦ä¹ èƒ½åŠ›çš„ç»„ä»¶ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚å…¶ä¸­ä¸€ä¸ªæœ€å¤§çš„æŒ‘æˆ˜æ˜¯å®ç°å¯é çš„å®‰å…¨ä¿è¯ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆè®¨è®ºäº†è®¾è®¡å’ŒéªŒè¯è¿™äº›ç³»ç»Ÿçš„å·¥ç¨‹å’Œç ”ç©¶æŒ‘æˆ˜ã€‚ç„¶åï¼Œæ ¹æ®ç°æœ‰çš„å·¥ä½œæ— æ³•å®ç°å¯è¯çš„ä¿è¯ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤æ­¥éªŒè¯æ–¹æ³•ä»¥å®ç°å¯è¯çš„ç»Ÿè®¡ä¿è¯ã€‚
</details></li>
</ul>
<hr>
<h2 id="On-Combining-Expert-Demonstrations-in-Imitation-Learning-via-Optimal-Transport"><a href="#On-Combining-Expert-Demonstrations-in-Imitation-Learning-via-Optimal-Transport" class="headerlink" title="On Combining Expert Demonstrations in Imitation Learning via Optimal Transport"></a>On Combining Expert Demonstrations in Imitation Learning via Optimal Transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10810">http://arxiv.org/abs/2307.10810</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ilanasebag/Sliced-MMOT-Imitation-Learning">https://github.com/ilanasebag/Sliced-MMOT-Imitation-Learning</a></li>
<li>paper_authors: Ilana Sebag, Samuel Cohen, Marc Peter Deisenroth</li>
<li>for: æ•™å­¦Agentç‰¹å®šä»»åŠ¡ Ñ‡ĞµÑ€ĞµĞ·ä¸“å®¶ç¤ºèŒƒ</li>
<li>methods: ä½¿ç”¨ä¼˜åŒ–è¿è¾“æ–¹æ³•æµ‹é‡Agentå’Œä¸“å®¶è½¨è¿¹ä¹‹é—´çš„è·ç¦»ï¼Œå¹¶å°†å¤šä¸ªä¸“å®¶ç¤ºèŒƒåˆå¹¶åœ¨OTä¸Š</li>
<li>results: åœ¨OpenAI Gymæ§åˆ¶ç¯å¢ƒä¸­ï¼Œæå‡ºäº†ä¸€ç§ä½¿ç”¨å¤šä¸ªä¸“å®¶ç¤ºèŒƒçš„æ–¹æ³•ï¼Œå¹¶åˆ†æäº†å…¶æ•ˆç‡ï¼Œå‘ç°æ ‡å‡†æ–¹æ³•ä¸æ€»æ˜¯æœ€ä¼˜<details>
<summary>Abstract</summary>
Imitation learning (IL) seeks to teach agents specific tasks through expert demonstrations. One of the key approaches to IL is to define a distance between agent and expert and to find an agent policy that minimizes that distance. Optimal transport methods have been widely used in imitation learning as they provide ways to measure meaningful distances between agent and expert trajectories. However, the problem of how to optimally combine multiple expert demonstrations has not been widely studied. The standard method is to simply concatenate state (-action) trajectories, which is problematic when trajectories are multi-modal. We propose an alternative method that uses a multi-marginal optimal transport distance and enables the combination of multiple and diverse state-trajectories in the OT sense, providing a more sensible geometric average of the demonstrations. Our approach enables an agent to learn from several experts, and its efficiency is analyzed on OpenAI Gym control environments and demonstrates that the standard method is not always optimal.
</details>
<details>
<summary>æ‘˜è¦</summary>
copied from clipboardæ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰ç›®çš„æ˜¯æ•™å¯¼ä»£ç†äººç‰¹å®šä»»åŠ¡é€šè¿‡ä¸“å®¶ç¤ºèŒƒã€‚ä¸€ç§å…³é”®çš„ILæ–¹æ³•æ˜¯å®šä¹‰ä»£ç†äººå’Œä¸“å®¶ä¹‹é—´çš„è·ç¦»ï¼Œå¹¶æ‰¾åˆ°ä¸€ä¸ªä»£ç†äººç­–ç•¥ï¼Œä»¥æœ€å°åŒ–è¿™ä¸ªè·ç¦»ã€‚ä¼˜è´¨è¿è¾“æ–¹æ³•åœ¨æ¨¡ä»¿å­¦ä¹ ä¸­å¹¿æ³›åº”ç”¨ï¼Œå®ƒä»¬æä¾›äº†æµ‹é‡ä»£ç†äººå’Œä¸“å®¶è½¨è¿¹ä¹‹é—´æœ‰æ„ä¹‰çš„è·ç¦»çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œå¤šä¸ªä¸“å®¶ç¤ºèŒƒçš„ç»„åˆå°šæœªå¾—åˆ°å¹¿æ³›çš„ç ”ç©¶ã€‚æ ‡å‡†æ–¹æ³•æ˜¯ç®€å•åœ° concatenate çŠ¶æ€(-åŠ¨ä½œ)è½¨è¿¹ï¼Œè¿™ä¼šå¯¼è‡´è½¨è¿¹æ˜¯å¤šæ¨¡çš„ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ² æ–¹æ³•ï¼Œä½¿ç”¨å¤šä¸ªå¤šé‡æœ€ä¼˜è¿è¾“è·ç¦»ï¼Œä½¿å¾—å¤šä¸ªå’Œå¤šæ ·çš„çŠ¶æ€è½¨è¿¹åœ¨OTæ„ä¹‰ä¸Šèƒ½å¤Ÿåˆç†åœ°ç»„åˆï¼Œæä¾›ä¸€ä¸ªæ›´åŠ æœ‰æ„ä¹‰çš„ geometric average çš„ç¤ºèŒƒã€‚æˆ‘ä»¬çš„æ–¹æ³•å…è®¸ä»£ç†äººä»å¤šä¸ªä¸“å®¶ä¸­å­¦ä¹ ï¼Œå¹¶åœ¨ OpenAI Gym æ§åˆ¶ç¯å¢ƒä¸­è¿›è¡Œäº†æ•ˆç‡åˆ†æï¼Œç»“æœæ˜¾ç¤ºï¼Œæ ‡å‡†æ–¹æ³•å¹¶ä¸æ€»æ˜¯ä¼˜åŒ–çš„ã€‚
</details></li>
</ul>
<hr>
<h2 id="Communication-Efficient-Split-Learning-via-Adaptive-Feature-Wise-Compression"><a href="#Communication-Efficient-Split-Learning-via-Adaptive-Feature-Wise-Compression" class="headerlink" title="Communication-Efficient Split Learning via Adaptive Feature-Wise Compression"></a>Communication-Efficient Split Learning via Adaptive Feature-Wise Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10805">http://arxiv.org/abs/2307.10805</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongjeong Oh, Jaeho Lee, Christopher G. Brinton, Yo-Seb Jeon</li>
<li>for: æé«˜åˆ†å¸ƒå¼å­¦ä¹ ä¸­é€šä¿¡å¼€é”€çš„å‡å°‘</li>
<li>methods: åˆ©ç”¨çŸ©é˜µåˆ—ä¸­å…·æœ‰ä¸åŒåˆ†æ•£åº¦çš„ç‰¹å¾è¿›è¡Œå‹ç¼©ï¼Œå¹¶é‡‡ç”¨é€‚åº”å¼dropoutå’Œé€‚åº”å¼é‡åŒ–ç­–ç•¥</li>
<li>results: ä¸ç°æœ‰åˆ†å¸ƒå¼å­¦ä¹ æ¡†æ¶ç›¸æ¯”ï¼Œæä¾›5.6%ä»¥ä¸Šçš„åˆ†ç±»ç²¾åº¦æå‡ï¼ŒåŒæ—¶å‡å°‘äº†320å€çš„é€šä¿¡å¼€é”€<details>
<summary>Abstract</summary>
This paper proposes a novel communication-efficient split learning (SL) framework, named SplitFC, which reduces the communication overhead required for transmitting intermediate feature and gradient vectors during the SL training process. The key idea of SplitFC is to leverage different dispersion degrees exhibited in the columns of the matrices. SplitFC incorporates two compression strategies: (i) adaptive feature-wise dropout and (ii) adaptive feature-wise quantization. In the first strategy, the intermediate feature vectors are dropped with adaptive dropout probabilities determined based on the standard deviation of these vectors. Then, by the chain rule, the intermediate gradient vectors associated with the dropped feature vectors are also dropped. In the second strategy, the non-dropped intermediate feature and gradient vectors are quantized using adaptive quantization levels determined based on the ranges of the vectors. To minimize the quantization error, the optimal quantization levels of this strategy are derived in a closed-form expression. Simulation results on the MNIST, CIFAR-10, and CelebA datasets demonstrate that SplitFC provides more than a 5.6% increase in classification accuracy compared to state-of-the-art SL frameworks, while they require 320 times less communication overhead compared to the vanilla SL framework without compression.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
<ol>
<li>Adaptive feature-wise dropout: The intermediate feature vectors are dropped with adaptive dropout probabilities determined based on the standard deviation of these vectors. Then, by the chain rule, the intermediate gradient vectors associated with the dropped feature vectors are also dropped.2. Adaptive feature-wise quantization: The non-dropped intermediate feature and gradient vectors are quantized using adaptive quantization levels determined based on the ranges of the vectors. To minimize the quantization error, the optimal quantization levels of this strategy are derived in a closed-form expression.Simulation results on the MNIST, CIFAR-10, and CelebA datasets demonstrate that SplitFC provides more than a 5.6% increase in classification accuracy compared to state-of-the-art SL frameworks, while they require 320 times less communication overhead compared to the vanilla SL framework without compression.Translation in Simplified Chinese:è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„é€šä¿¡å‡å°‘çš„splitå­¦ä¹ ï¼ˆSLï¼‰æ¡†æ¶ï¼Œåä¸ºSplitFCï¼Œå®ƒé™ä½äº†åœ¨SLè®­ç»ƒè¿‡ç¨‹ä¸­ä¼ è¾“ä¸­é—´ç‰¹å¾å’Œæ¢¯åº¦ Ğ²ĞµĞºÑ‚Ğ¾Ñ€çš„é€šä¿¡å¼€é”€ã€‚SplitFCçš„å…³é”®æ€æƒ³æ˜¯åˆ©ç”¨ä¸åŒçš„æ•£åº¦åº¦åœ¨çŸ©é˜µåˆ—ä¸­ã€‚SplitFCåŒ…æ‹¬ä¸¤ç§å‹ç¼©ç­–ç•¥ï¼š1. é€‚åº”ç‰¹å¾ wise dropoutï¼šä¸­é—´ç‰¹å¾ Ğ²ĞµĞºÑ‚Ğ¾Ñ€é€šè¿‡é€‚åº”dropoutæ¦‚ç‡ç¡®å®šäº†dropout probabilitiesï¼Œç„¶åæ ¹æ®é“¾è§„åˆ™ï¼Œç›¸å…³çš„ä¸­é—´æ¢¯åº¦ Ğ²ĞµĞºÑ‚Ğ¾Ñ€ä¹Ÿä¼šè¢«Dropã€‚2. é€‚åº”ç‰¹å¾ wise quantizationï¼šæœªDropçš„ä¸­é—´ç‰¹å¾å’Œæ¢¯åº¦ Ğ²ĞµĞºÑ‚Ğ¾Ñ€é€šè¿‡é€‚åº”å‹ç¼©æ°´å¹³ç¡®å®šäº†å‹ç¼©çº§åˆ«ï¼Œä»¥é¿å…å‹ç¼©è¯¯å·®ã€‚å‹ç¼©çº§åˆ«çš„ä¼˜åŒ–å‡†ç¡®è¡¨è¾¾å¾—åˆ°äº†å…³é—­å¼è¡¨è¾¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSplitFCåœ¨MNISTã€CIFAR-10å’ŒCelebA datasetsä¸Šæä¾›äº† mÃ¡s de 5.6%çš„åˆ†ç±»ç²¾åº¦æå‡ï¼ŒåŒæ—¶ä¸æ— å‹ç¼©SLæ¡†æ¶ç›¸æ¯”ï¼Œå®ƒéœ€è¦320å€å°‘çš„é€šä¿¡å¼€é”€ã€‚</details></li>
</ol>
<hr>
<h2 id="Spatial-Temporal-Data-Mining-for-Ocean-Science-Data-Methodologies-and-Opportunities"><a href="#Spatial-Temporal-Data-Mining-for-Ocean-Science-Data-Methodologies-and-Opportunities" class="headerlink" title="Spatial-Temporal Data Mining for Ocean Science: Data, Methodologies, and Opportunities"></a>Spatial-Temporal Data Mining for Ocean Science: Data, Methodologies, and Opportunities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10803">http://arxiv.org/abs/2307.10803</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanchen Yang, Wengen Li, Shuyu Wang, Hui Li, Jihong Guan, Shuigeng Zhou, Jiannong Cao</li>
<li>For: This paper provides a comprehensive survey of existing spatial-temporal data mining (STDM) studies for ocean science, including a review of widely-used ST ocean datasets and their unique characteristics, as well as techniques for data quality enhancement and various STDM tasks.* Methods: The paper reviews and discusses various techniques for STDM in ocean science, including data preprocessing, feature extraction, and machine learning algorithms for tasks such as prediction, event detection, pattern mining, and anomaly detection.* Results: The paper highlights the unique challenges and opportunities of STDM in ocean science, and discusses promising research opportunities in this field, including the application of advanced STDM techniques to climate forecasting and disaster warning.<details>
<summary>Abstract</summary>
With the rapid amassing of spatial-temporal (ST) ocean data, many spatial-temporal data mining (STDM) studies have been conducted to address various oceanic issues, including climate forecasting and disaster warning. Compared with typical ST data (e.g., traffic data), ST ocean data is more complicated but with unique characteristics, e.g., diverse regionality and high sparsity. These characteristics make it difficult to design and train STDM models on ST ocean data. To the best of our knowledge, a comprehensive survey of existing studies remains missing in the literature, which hinders not only computer scientists from identifying the research issues in ocean data mining but also ocean scientists to apply advanced STDM techniques. In this paper, we provide a comprehensive survey of existing STDM studies for ocean science. Concretely, we first review the widely-used ST ocean datasets and highlight their unique characteristics. Then, typical ST ocean data quality enhancement techniques are explored. Next, we classify existing STDM studies in ocean science into four types of tasks, i.e., prediction, event detection, pattern mining, and anomaly detection, and elaborate on the techniques for these tasks. Finally, promising research opportunities are discussed. This survey can help scientists from both computer science and ocean science better understand the fundamental concepts, key techniques, and open challenges of STDM for ocean science.
</details>
<details>
<summary>æ‘˜è¦</summary>
éšç€ç©ºé—´æ—¶é—´ï¼ˆSTï¼‰æµ·æ´‹æ•°æ®çš„å¿«é€Ÿæ±‡é›†ï¼Œè®¸å¤šç©ºé—´æ—¶é—´æ•°æ®æŒ–æ˜ï¼ˆSTDMï¼‰ç ”ç©¶å·²ç»è¿›è¡Œä»¥è§£å†³æµ·æ´‹é—®é¢˜ï¼Œå¦‚æ°”å€™é¢„æµ‹å’Œç¾å®³è­¦å‘Šã€‚ç›¸æ¯”ä¸€èˆ¬STæ•°æ®ï¼ˆä¾‹å¦‚äº¤é€šæ•°æ®ï¼‰ï¼ŒSTæµ·æ´‹æ•°æ®æ›´åŠ å¤æ‚ï¼Œä½†å…·æœ‰ç‹¬ç‰¹ç‰¹å¾ï¼Œå¦‚å¤šæ ·æ€§å’Œé«˜ç¨€ç•´æ€§ã€‚è¿™äº›ç‰¹å¾ä½¿å¾—è®¾è®¡å’Œè®­ç»ƒSTDMæ¨¡å‹å¯¹STæµ·æ´‹æ•°æ®å˜å¾—æ›´åŠ å›°éš¾ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œç°æœ‰çš„ç›¸å…³ç ”ç©¶æ£€ç´¢åœ¨æ–‡çŒ®ä¸­ç¼ºå¤±ï¼Œè¿™ä¸ä»…é˜»ç¢äº†è®¡ç®—æœºç§‘å­¦å®¶ä»æµ·æ´‹æ•°æ®æŒ–æ˜ä¸­äº†è§£ç ”ç©¶é—®é¢˜ï¼Œä¹Ÿé˜»ç¢äº†æµ·æ´‹ç§‘å­¦å®¶åº”ç”¨å…ˆè¿›çš„STDMæŠ€æœ¯ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†æµ·æ´‹ç§‘å­¦é¢†åŸŸçš„å…¨é¢çš„STDMç ”ç©¶æ£€ç´¢ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆè¯„è®ºäº†å¹¿æ³›ä½¿ç”¨çš„STæµ·æ´‹æ•°æ®é›†å’Œå…¶ç‹¬ç‰¹ç‰¹å¾ã€‚ç„¶åï¼Œæˆ‘ä»¬æ¢è®¨äº†ä¸€èˆ¬STæµ·æ´‹æ•°æ®è´¨é‡æå‡æŠ€æœ¯ã€‚æ¥ç€ï¼Œæˆ‘ä»¬åˆ†ç±»äº†ç°æœ‰çš„STDMç ”ç©¶ï¼Œå¹¶è¯¦ç»†ä»‹ç»äº†è¿™äº›ä»»åŠ¡çš„æŠ€æœ¯ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†æœ‰å‰é€”çš„ç ”ç©¶æœºé‡ã€‚è¿™ç§æ£€ç´¢å¯ä»¥å¸®åŠ©è®¡ç®—æœºç§‘å­¦å®¶å’Œæµ·æ´‹ç§‘å­¦å®¶æ›´å¥½åœ°ç†è§£STDMçš„åŸºæœ¬æ¦‚å¿µã€å…³é”®æŠ€æœ¯å’Œå¼€æ”¾çš„æŒ‘æˆ˜ï¼Œä»¥åŠåœ¨æµ·æ´‹ç§‘å­¦é¢†åŸŸåº”ç”¨STDMæŠ€æœ¯çš„å¯èƒ½æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Meta-Transformer-A-Unified-Framework-for-Multimodal-Learning"><a href="#Meta-Transformer-A-Unified-Framework-for-Multimodal-Learning" class="headerlink" title="Meta-Transformer: A Unified Framework for Multimodal Learning"></a>Meta-Transformer: A Unified Framework for Multimodal Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10802">http://arxiv.org/abs/2307.10802</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/invictus717/MetaTransformer">https://github.com/invictus717/MetaTransformer</a></li>
<li>paper_authors: Yiyuan Zhang, Kaixiong Gong, Kaipeng Zhang, Hongsheng Li, Yu Qiao, Wanli Ouyang, Xiangyu Yue</li>
<li>for: è¿™ä¸ªç ”ç©¶æ—¨åœ¨å»ºç«‹ä¸€ä¸ªèƒ½å¤Ÿå¤„ç†å¤šç§æ¨¡å¼çš„æ¨¡å‹ï¼Œå¹¶å°†å…¶ä¸ä¸åŒçš„æ¨¡å¼è¿›è¡Œå…³è”ã€‚</li>
<li>methods: è¿™ä¸ªæ–¹æ³•ä½¿ç”¨ä¸€ä¸ªå†»ç»“çš„encoderæ¥è¿›è¡Œå¤šæ¨¡å¼è®¤è¯†ï¼Œå¹¶å°†ä¸åŒçš„è¾“å…¥æ•°æ®mapsåˆ°å…±åŒçš„tokenç©ºé—´ï¼Œä»¥EXTRACTé«˜çº§ semantic featureã€‚</li>
<li>results: è¿™ä¸ªæ–¹æ³•å¯ä»¥åœ¨12ç§æ¨¡å¼ä¸Šè¿›è¡Œé€šç”¨çš„å­¦ä¹ ï¼ŒåŒ…æ‹¬åŸºæœ¬çš„è®¤è¯†(æ–‡æœ¬ã€å›¾åƒã€ç‚¹ cloudã€éŸ³é¢‘ã€å½±ç‰‡)ã€å®é™…åº”ç”¨(X-rayã€infraredã€é¢œè‰²ã€IMU)å’Œæ•°æ®é‡‡çŸ¿(å›¾å½¢ã€è¡¨æ ¼ã€æ—¶é—´åºåˆ—)ã€‚<details>
<summary>Abstract</summary>
Multimodal learning aims to build models that can process and relate information from multiple modalities. Despite years of development in this field, it still remains challenging to design a unified network for processing various modalities ($\textit{e.g.}$ natural language, 2D images, 3D point clouds, audio, video, time series, tabular data) due to the inherent gaps among them. In this work, we propose a framework, named Meta-Transformer, that leverages a $\textbf{frozen}$ encoder to perform multimodal perception without any paired multimodal training data. In Meta-Transformer, the raw input data from various modalities are mapped into a shared token space, allowing a subsequent encoder with frozen parameters to extract high-level semantic features of the input data. Composed of three main components: a unified data tokenizer, a modality-shared encoder, and task-specific heads for downstream tasks, Meta-Transformer is the first framework to perform unified learning across 12 modalities with unpaired data. Experiments on different benchmarks reveal that Meta-Transformer can handle a wide range of tasks including fundamental perception (text, image, point cloud, audio, video), practical application (X-Ray, infrared, hyperspectral, and IMU), and data mining (graph, tabular, and time-series). Meta-Transformer indicates a promising future for developing unified multimodal intelligence with transformers. Code will be available at https://github.com/invictus717/MetaTransformer
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤šModalå­¦ä¹ æ—¨åœ¨å»ºç«‹å¤„ç†å¤šç§æ¨¡å¼çš„æ¨¡å‹ã€‚å°½ç®¡å¤šå¹´çš„å‘å±•ï¼Œä»ç„¶å›°éš¾è®¾è®¡å¤„ç†å¤šç§æ¨¡å¼çš„ç»Ÿä¸€ç½‘ç»œï¼ˆä¾‹å¦‚è‡ªç„¶è¯­è¨€ã€2Då›¾åƒã€3Dç‚¹äº‘ã€éŸ³é¢‘ã€è§†é¢‘ã€æ—¶é—´åºåˆ—ã€è¡¨æ ¼æ•°æ®ï¼‰çš„æ¨¡å‹ï¼Œå› ä¸ºè¿™äº›æ¨¡å¼ä¹‹é—´å­˜åœ¨éšè—çš„å·®å¼‚ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œåä¸ºMeta-Transformerï¼Œå®ƒåˆ©ç”¨ä¸€ä¸ªå†»ç»“çš„Encoderæ¥å®ç°å¤šModalæ„ŸçŸ¥ï¼Œæ— éœ€ä»»ä½•å¯¹å‡†çš„å¤šModalè®­ç»ƒæ•°æ®ã€‚Meta-Transformeræ¡†æ¶ç”±ä¸‰ä¸ªä¸»è¦ç»„ä»¶ç»„æˆï¼šä¸€ä¸ªç»Ÿä¸€çš„æ•°æ®Tokenizerã€ä¸€ä¸ªå…±äº«Encoderå’Œä¸‹æ¸¸ä»»åŠ¡çš„ä»»åŠ¡ç‰¹å®šå¤´ã€‚Meta-Transformeræ˜¯é¦–ä¸ªåœ¨12ç§æ¨¡å¼ä¸Šè¿›è¡Œç»Ÿä¸€å­¦ä¹ çš„æ¡†æ¶ï¼Œæ— éœ€å¯¹æ•°æ®è¿›è¡ŒåŒ¹é…ã€‚åœ¨ä¸åŒçš„Benchmarkä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒMeta-Transformerå¯ä»¥å¤„ç†å„ç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬åŸºæœ¬çš„æ„ŸçŸ¥ï¼ˆæ–‡æœ¬ã€å›¾åƒã€ç‚¹äº‘ã€éŸ³é¢‘ã€è§†é¢‘ï¼‰ã€å®ç”¨åº”ç”¨ï¼ˆXå°„çº¿ã€çº¢å¤–ã€åæŒ¯ã€IMUï¼‰å’Œæ•°æ®æŒ–æ˜ï¼ˆå›¾å½¢ã€è¡¨æ ¼ã€æ—¶é—´åºåˆ—ï¼‰ã€‚Meta-Transformerè¡¨æ˜äº†æœªæ¥åœ¨ä½¿ç”¨Transformerè¿›è¡Œå¤šModalæ™ºèƒ½çš„å‘å±•å…·æœ‰æ‰å®çš„å‰æ™¯ã€‚ä»£ç å°†åœ¨https://github.com/invictus717/MetaTransformerä¸Šæä¾›ã€‚
</details></li>
</ul>
<hr>
<h2 id="Convergence-of-Adam-for-Non-convex-Objectives-Relaxed-Hyperparameters-and-Non-ergodic-Case"><a href="#Convergence-of-Adam-for-Non-convex-Objectives-Relaxed-Hyperparameters-and-Non-ergodic-Case" class="headerlink" title="Convergence of Adam for Non-convex Objectives: Relaxed Hyperparameters and Non-ergodic Case"></a>Convergence of Adam for Non-convex Objectives: Relaxed Hyperparameters and Non-ergodic Case</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11782">http://arxiv.org/abs/2307.11782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meixuan He, Yuqing Liang, Jinlan Liu, Dongpo Xu</li>
<li>for:  investigate the convergence properties of Adam algorithm in non-convex settings and develop a better understanding of its performance.</li>
<li>methods:  introduce precise definitions of ergodic and non-ergodic convergence, and establish a weaker sufficient condition for the ergodic convergence guarantee of Adam.</li>
<li>results:  prove that the last iterate of Adam converges to a stationary point for non-convex objectives, and obtain the non-ergodic convergence rate of $O(1&#x2F;K)$ for function values under the Polyak-Lojasiewicz (PL) condition.<details>
<summary>Abstract</summary>
Adam is a commonly used stochastic optimization algorithm in machine learning. However, its convergence is still not fully understood, especially in the non-convex setting. This paper focuses on exploring hyperparameter settings for the convergence of vanilla Adam and tackling the challenges of non-ergodic convergence related to practical application. The primary contributions are summarized as follows: firstly, we introduce precise definitions of ergodic and non-ergodic convergence, which cover nearly all forms of convergence for stochastic optimization algorithms. Meanwhile, we emphasize the superiority of non-ergodic convergence over ergodic convergence. Secondly, we establish a weaker sufficient condition for the ergodic convergence guarantee of Adam, allowing a more relaxed choice of hyperparameters. On this basis, we achieve the almost sure ergodic convergence rate of Adam, which is arbitrarily close to $o(1/\sqrt{K})$. More importantly, we prove, for the first time, that the last iterate of Adam converges to a stationary point for non-convex objectives. Finally, we obtain the non-ergodic convergence rate of $O(1/K)$ for function values under the Polyak-Lojasiewicz (PL) condition. These findings build a solid theoretical foundation for Adam to solve non-convex stochastic optimization problems.
</details>
<details>
<summary>æ‘˜è¦</summary>
äºšå½“æ˜¯ä¸€ç§å¸¸ç”¨çš„æœºä¼šä¼°è®¡ç®—æ³•åœ¨æœºå™¨å­¦ä¹ ä¸­ã€‚ç„¶è€Œï¼Œå®ƒçš„æ•´åˆä»ç„¶æœªå…¨é¢ç†è§£ï¼Œç‰¹åˆ«æ˜¯åœ¨éå¯¹ç§°è®¾å®šä¸‹ã€‚æœ¬æ–‡ä¸“æ³¨äºæ¢ç´¢äºšå½“çš„å‚æ•°è®¾å®šï¼Œä»¥æé«˜å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½ã€‚ä¸»è¦è´¡çŒ®å¦‚ä¸‹ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ç²¾ç¡®çš„ergodicå’Œnon-ergodicæ•´åˆå®šä¹‰ï¼Œè¿™äº›å®šä¹‰åŒ…æ‹¬å¤§å¤šæ•° Stochastic optimizationç®—æ³•çš„æ•´åˆå½¢å¼ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼ºè°ƒnon-ergodicæ•´åˆçš„superiorityã€‚ç¬¬äºŒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè¾ƒå¼±çš„å……åˆ†å¿…è¦æ¡ä»¶ï¼Œä»¥ç¡®ä¿äºšå½“çš„ergodicæ•´åˆä¿è¯ã€‚è¿™å…è®¸æ›´åŠ relaxedçš„å‚æ•°é€‰æ‹©ã€‚åŸºäºè¿™ä¸ªåŸºç¡€ï¼Œæˆ‘ä»¬è·å¾—äº†å‡ ä¹ç¡®å®šçš„almost sure ergodicæ•´åˆé€Ÿç‡ï¼Œå®ƒæ˜¯$o(1/\sqrt{K})$ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬è¯æ˜äº†äºšå½“çš„æœ€åè¿­ä»£å‘éå¯¹ç§°ç›®æ ‡å‡½æ•° convergeã€‚æœ€åï¼Œæˆ‘ä»¬å–å¾—äº†éergodicæ•´åˆé€Ÿç‡ä¸º$O(1/K)$ï¼Œå®ƒæ˜¯PL conditonä¸‹çš„å‡½æ•°å€¼ã€‚è¿™äº›å‘ç°å»ºç«‹äº†äºšå½“åœ¨éå¯¹ç§°éšæœºä¼°è®¡é—®é¢˜ä¸Šçš„åšå›ºç†è®ºåŸºç¡€ã€‚
</details></li>
</ul>
<hr>
<h2 id="Optimizing-PatchCore-for-Few-many-shot-Anomaly-Detection"><a href="#Optimizing-PatchCore-for-Few-many-shot-Anomaly-Detection" class="headerlink" title="Optimizing PatchCore for Few&#x2F;many-shot Anomaly Detection"></a>Optimizing PatchCore for Few&#x2F;many-shot Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10792">http://arxiv.org/abs/2307.10792</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/scortexio/patchcore-few-shot">https://github.com/scortexio/patchcore-few-shot</a></li>
<li>paper_authors: JoÃ£o Santos, Triet Tran, Oliver Rippel</li>
<li>for: è¿™ä¸ªè®ºæ–‡ä¸»è¦å…³æ³¨äºew-shot anomaly detectionï¼ˆADï¼‰é¢†åŸŸçš„ç ”ç©¶ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨åªæœ‰å‡ ä¸ªé€‰æ‹©çš„æ ·æœ¬æ¥åˆ†è¾¨æ­£å¸¸å’Œå¼‚å¸¸æ•°æ®ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†PatchCoreï¼Œå½“å‰çš„å…¨shot AD&#x2F;ASç®—æ³•ï¼Œè¿›è¡Œç ”ç©¶ï¼ŒåŒ…æ‹¬ä¼˜åŒ–å…¶å¤šç§è¶…å‚æ•°å’Œå°†supervised learningä¸­çŸ¥æ‚‰çš„æŠ€æœ¯è½¬ç§»åˆ°ADé¢†åŸŸã€‚</li>
<li>results: å®éªŒè¡¨æ˜ï¼Œå¯ä»¥é€šè¿‡ä¼˜åŒ–è¶…å‚æ•°å’Œä½¿ç”¨å›¾åƒæ°´å¹³çš„æ‰©å±•æ¥å®ç°æ˜¾è‘—æ€§èƒ½æå‡ï¼Œå¹¶åœ¨VisA datasetä¸Šå®ç°äº†æ–°çš„state of the artã€‚æ­¤å¤–ï¼Œè¯¥è®ºæ–‡è¿˜æå‡ºäº†æœªæ¥ç ”ç©¶çš„å¯èƒ½æ€§ï¼Œå³ç ”ç©¶å…·æœ‰å¼º inductive biasçš„ç‰¹å¾æå–å™¨ã€‚<details>
<summary>Abstract</summary>
Few-shot anomaly detection (AD) is an emerging sub-field of general AD, and tries to distinguish between normal and anomalous data using only few selected samples. While newly proposed few-shot AD methods do compare against pre-existing algorithms developed for the full-shot domain as baselines, they do not dedicatedly optimize them for the few-shot setting. It thus remains unclear if the performance of such pre-existing algorithms can be further improved. We address said question in this work. Specifically, we present a study on the AD/anomaly segmentation (AS) performance of PatchCore, the current state-of-the-art full-shot AD/AS algorithm, in both the few-shot and the many-shot settings. We hypothesize that further performance improvements can be realized by (I) optimizing its various hyperparameters, and by (II) transferring techniques known to improve few-shot supervised learning to the AD domain. Exhaustive experiments on the public VisA and MVTec AD datasets reveal that (I) significant performance improvements can be realized by optimizing hyperparameters such as the underlying feature extractor, and that (II) image-level augmentations can, but are not guaranteed, to improve performance. Based on these findings, we achieve a new state of the art in few-shot AD on VisA, further demonstrating the merit of adapting pre-existing AD/AS methods to the few-shot setting. Last, we identify the investigation of feature extractors with a strong inductive bias as a potential future research direction for (few-shot) AD/AS.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ–°å‹å¼‚å¸¸æ£€æµ‹ï¼ˆADï¼‰æ˜¯ä¸€ä¸ªæ–°è¶‹åŠ¿çš„åˆ†æ”¯ï¼Œå®ƒç›®æ ‡æ˜¯ä½¿ç”¨åªæœ‰å‡ ä¸ªé€‰æ‹©çš„æ ·æœ¬åˆ†ç±»æ­£å¸¸å’Œå¼‚å¸¸æ•°æ®ã€‚è™½ç„¶æ–°æå‡ºçš„å‡ ä¸ªADæ–¹æ³•ä¼šæ¯”è¾ƒæ—§çš„å…¨shoté¢„æµ‹å™¨ï¼Œä½†å®ƒä»¬æ²¡æœ‰ä¸“é—¨ä¼˜åŒ–å®ƒä»¬ä¸ºå‡ ä¸ªshotè®¾å®šã€‚å› æ­¤ï¼Œå…¶æ€§èƒ½ä»ç„¶å­˜åœ¨uncertaintyã€‚æˆ‘ä»¬åœ¨è¿™é‡Œè§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬å¯¹PatchCoreï¼Œå½“å‰çš„å…¨shotAD/ASç®—æ³•ï¼Œåœ¨å‡ ä¸ªshotå’Œå¤šä¸ªshotè®¾å®šä¸‹è¿›è¡ŒAD/ASæ€§èƒ½çš„ç ”ç©¶ã€‚æˆ‘ä»¬å‡è®¾å¯ä»¥é€šè¿‡ï¼ˆIï¼‰ä¼˜åŒ–å…¶å„ç§è¶…å‚æ•°ï¼Œä»¥åŠï¼ˆIIï¼‰å°†å‡ ä¸ªshotå­¦ä¹ ä¸­çš„æŠ€æœ¯è½¬ç§»åˆ°ADé¢†åŸŸæ¥å®ç°æ€§èƒ½æé«˜ã€‚æˆ‘ä»¬åœ¨å…¬å…±çš„VisAå’ŒMVTec AD datasetsä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå‘ç°ï¼ˆIï¼‰å¯ä»¥é€šè¿‡ä¼˜åŒ–ç‰¹å¾æå–å™¨æ¥å®ç°æ˜¾è‘—æ€§èƒ½æé«˜ï¼Œå¹¶ä¸”ï¼ˆIIï¼‰å›¾åƒæ°´å¹³çš„æ‰©å±•å¯ä»¥ï¼Œä½†å¹¶ä¸ä¸€å®šï¼Œæé«˜æ€§èƒ½ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬åœ¨VisAä¸Šå®ç°äº†æ–°çš„çŠ¶æ€æ€çš„ADï¼Œè¿›ä¸€æ­¥è¯æ˜äº†é€‚åº”å‰ exist AD/ASæ–¹æ³•åˆ°å‡ ä¸ªshotè®¾å®šçš„ä»·å€¼ã€‚æœ€åï¼Œæˆ‘ä»¬è®¤ä¸ºåœ¨ï¼ˆå‡ ä¸ªshotï¼‰AD/ASé¢†åŸŸ investigating feature extractors with strong inductive bias æ˜¯ä¸€ä¸ªå¯èƒ½çš„æœªæ¥ç ”ç©¶æ–¹å‘ã€‚
</details></li>
</ul>
<hr>
<h2 id="Adversarial-attacks-for-mixtures-of-classifiers"><a href="#Adversarial-attacks-for-mixtures-of-classifiers" class="headerlink" title="Adversarial attacks for mixtures of classifiers"></a>Adversarial attacks for mixtures of classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10788">http://arxiv.org/abs/2307.10788</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas Gnecco Heredia, Benjamin Negrevergne, Yann Chevaleyre</li>
<li>for: æé«˜é²æ£’æ€§ against adversarial attacks</li>
<li>methods: ä½¿ç”¨mixtures of classifiers (a.k.a. randomized ensembles)</li>
<li>results: å¼•å…¥ä¸¤ç§æ”»å‡»æ€§è´¨ï¼ˆæœ‰æ•ˆæ€§å’Œæœ€å¤§åŒ–ï¼‰ï¼Œå¹¶è¯æ˜ç°æœ‰æ”»å‡»ä¸ç¬¦åˆè¿™ä¸¤ç§æ€§è´¨ã€‚è¿˜æå‡ºäº†ä¸€ç§æ–°çš„æ”»å‡»æ–¹æ³•called lattice climber attackï¼Œå¹¶åœ¨binary linear settingä¸‹æä¾›äº†ç†è®ºä¿è¯ï¼Œå¹¶åœ¨ syntheticå’Œå®é™…æ•°æ®ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ã€‚<details>
<summary>Abstract</summary>
Mixtures of classifiers (a.k.a. randomized ensembles) have been proposed as a way to improve robustness against adversarial attacks. However, it has been shown that existing attacks are not well suited for this kind of classifiers. In this paper, we discuss the problem of attacking a mixture in a principled way and introduce two desirable properties of attacks based on a geometrical analysis of the problem (effectiveness and maximality). We then show that existing attacks do not meet both of these properties. Finally, we introduce a new attack called lattice climber attack with theoretical guarantees on the binary linear setting, and we demonstrate its performance by conducting experiments on synthetic and real datasets.
</details>
<details>
<summary>æ‘˜è¦</summary>
åˆå¹¶åˆ†ç±»å™¨ï¼ˆå³éšæœº ensembleï¼‰å·²ç»è¢«æè®®ç”¨äºæé«˜å¯¹æŠ—éªšæ‰°æ”»å‡»çš„Robustnessã€‚ç„¶è€Œï¼Œå·²ç»è¯æ˜ç°æœ‰çš„æ”»å‡»æ–¹æ³•å¹¶ä¸é€‚ç”¨äºè¿™ç§ç±»å‹çš„åˆ†ç±»å™¨ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†æ”»å‡»æ··åˆçš„é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸¤ä¸ªæ„¿æœ›çš„æ”»å‡»ç‰¹æ€§ï¼ˆæœ‰æ•ˆæ€§å’Œæœ€å¤§åŒ–ï¼‰ã€‚æˆ‘ä»¬thenè¡¨æ˜ç°æœ‰çš„æ”»å‡»æ–¹æ³•å¹¶ä¸æ»¡è¶³è¿™ä¸¤ä¸ªç‰¹æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„æ”»å‡»æ–¹æ³•called lattice climber attackï¼Œå¹¶æä¾›äº†å¯¹äºŒåˆ†çº¿æ€§è®¾å®šä¸‹çš„ç†è®ºä¿è¯ã€‚æˆ‘ä»¬é€šè¿‡å¯¹ sinteticå’Œå®é™…æ•°æ®è¿›è¡Œå®éªŒï¼Œè¯æ˜äº†è¿™ç§æ”»å‡»æ–¹æ³•çš„æ€§èƒ½ã€‚
</details></li>
</ul>
<hr>
<h2 id="Feed-Forward-Source-Free-Domain-Adaptation-via-Class-Prototypes"><a href="#Feed-Forward-Source-Free-Domain-Adaptation-via-Class-Prototypes" class="headerlink" title="Feed-Forward Source-Free Domain Adaptation via Class Prototypes"></a>Feed-Forward Source-Free Domain Adaptation via Class Prototypes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10787">http://arxiv.org/abs/2307.10787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ondrej Bohdal, Da Li, Timothy Hospedales</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨æºè‡ªç”±é¢†åŸŸé€‚åº”çš„å¿«é€ŸåŒ–æ–¹æ³•ï¼Œä»¥æ›¿ä»£åŸºäºåå°„ä¼ æ’­çš„é€‚åº”æ–¹æ³•ã€‚</li>
<li>methods: æœ¬æ–¹æ³•åŸºäºé¢„è®­ç»ƒæ¨¡å‹è®¡ç®—ç±» prototypeï¼Œå®ç°äº†å¿«é€ŸåŒ–é€‚åº”å¹¶ä¸”åªéœ€è¦å°é‡æ—¶é—´ã€‚</li>
<li>results: æœ¬ç ”ç©¶å®ç°äº†å‡†ç¡®ç‡çš„æ˜¾è‘—æå‡ï¼Œå¹¶ä¸”æ¯”æ™®é€šé€‚åº”æ–¹æ³•å¿«é€Ÿå¾—å¤šã€‚<details>
<summary>Abstract</summary>
Source-free domain adaptation has become popular because of its practical usefulness and no need to access source data. However, the adaptation process still takes a considerable amount of time and is predominantly based on optimization that relies on back-propagation. In this work we present a simple feed-forward approach that challenges the need for back-propagation based adaptation. Our approach is based on computing prototypes of classes under the domain shift using a pre-trained model. It achieves strong improvements in accuracy compared to the pre-trained model and requires only a small fraction of time of existing domain adaptation methods.
</details>
<details>
<summary>æ‘˜è¦</summary>
æºè‡ªç”±é¢†åŸŸé€‚åº”å·²ç»æˆä¸ºå¾ˆå—æ¬¢è¿çš„æ–¹æ³•ï¼Œå› ä¸ºå®ƒçš„å®ç”¨æ€§å’Œä¸éœ€è¦è®¿é—®æºæ•°æ®ã€‚ç„¶è€Œï¼Œé€‚åº”è¿‡ç¨‹ä»ç„¶éœ€è¦ä¸€å®šçš„æ—¶é—´ï¼Œå¹¶ä¸”ä¸»è¦åŸºäºä¼˜åŒ–ï¼Œä½¿ç”¨åå‘ä¼ æ’­ã€‚åœ¨è¿™ç§å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•çš„å‰å‘æ–¹æ³•ï¼ŒæŒ‘æˆ˜éœ€è¦åå‘ä¼ æ’­åŸºäºé€‚åº”ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è®¡ç®—ç±»ä¸‹çš„prototypeï¼Œå®ç°äº†ä¸é¢„è®­ç»ƒæ¨¡å‹çš„å‡†ç¡®ç‡å¼ºåŠ²æé«˜ï¼Œå¹¶ä¸”åªéœ€ä¸€å°éƒ¨åˆ†æ—¶é—´ã€‚
</details></li>
</ul>
<hr>
<h2 id="Efficient-Beam-Tree-Recursion"><a href="#Efficient-Beam-Tree-Recursion" class="headerlink" title="Efficient Beam Tree Recursion"></a>Efficient Beam Tree Recursion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10779">http://arxiv.org/abs/2307.10779</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jishnu Ray Chowdhury, Cornelia Caragea</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§ç®€å•çš„æ‰©å±•ï¼Œä»¥æé«˜ Gumbel Tree RvNN çš„é•¿åº¦æ•´åˆæ€§è¡¨ç°ï¼Œå¹¶ç»´æŒä¸å…¶ä»–ä»»åŠ¡çš„ç›¸ä¼¼æ€§ã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨çš„æ–¹æ³•æ˜¯è¯†åˆ« BT-RvNN çš„ä¸»è¦ç“¶é¢ˆï¼Œå¹¶æå‡ºä¸€äº›ç®€åŒ–å…¶å†…å­˜ä½¿ç”¨çš„ç­–ç•¥ã€‚</li>
<li>results: è¿™ä¸ªè®ºæ–‡çš„ç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨è¿™äº›ç­–ç•¥å¯ä»¥å°† BT-RvNN çš„å†…å­˜ä½¿ç”¨é‡é™ä½ $10$-$16$ å€ï¼Œå¹¶åˆ›é€ ä¸€ä¸ªæ–°çš„å·åˆ†-of-the-art åœ¨ ListOps ä¸­ï¼ŒåŒæ—¶ä¿æŒä¸å…¶ä»–ä»»åŠ¡çš„ç›¸ä¼¼æ€§ã€‚<details>
<summary>Abstract</summary>
Beam Tree Recursive Neural Network (BT-RvNN) was recently proposed as a simple extension of Gumbel Tree RvNN and it was shown to achieve state-of-the-art length generalization performance in ListOps while maintaining comparable performance on other tasks. However, although not the worst in its kind, BT-RvNN can be still exorbitantly expensive in memory usage. In this paper, we identify the main bottleneck in BT-RvNN's memory usage to be the entanglement of the scorer function and the recursive cell function. We propose strategies to remove this bottleneck and further simplify its memory usage. Overall, our strategies not only reduce the memory usage of BT-RvNN by $10$-$16$ times but also create a new state-of-the-art in ListOps while maintaining similar performance in other tasks. In addition, we also propose a strategy to utilize the induced latent-tree node representations produced by BT-RvNN to turn BT-RvNN from a sentence encoder of the form $f:\mathbb{R}^{n \times d} \rightarrow \mathbb{R}^{d}$ into a sequence contextualizer of the form $f:\mathbb{R}^{n \times d} \rightarrow \mathbb{R}^{n \times d}$. Thus, our proposals not only open up a path for further scalability of RvNNs but also standardize a way to use BT-RvNNs as another building block in the deep learning toolkit that can be easily stacked or interfaced with other popular models such as Transformers and Structured State Space models.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œBeam Tree Recursive Neural Networkï¼ˆBT-RvNNï¼‰æœ€è¿‘è¢«æå‡ºï¼Œå®ƒæ˜¯Gumbel Tree RvNNçš„ç®€å•æ‰©å±•ï¼Œå¹¶åœ¨ListOpsä¸­å®ç°äº†çŠ¶æ€å‰ç»æ€§çš„é•¿åº¦æ³›åŒ–æ€§ï¼ŒåŒæ—¶ä¿æŒä¸å…¶ä»–ä»»åŠ¡çš„ç›¸ä¼¼æ€§ã€‚ç„¶è€Œï¼Œè™½ç„¶ä¸æ˜¯æœ€å·®çš„ä¸€ç§ï¼ŒBT-RvNNä»ç„¶å…·æœ‰æ˜‚è´µçš„å†…å­˜ä½¿ç”¨ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ç¡®å®šäº†BT-RvNNçš„ä¸»è¦ç“¶é¢ˆæ˜¯æ’åºå‡½æ•°å’Œå¾ªç¯ç»†èƒå‡½æ•°çš„æ‚è°±ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€äº›ç¼“è§£ç“¶é¢ˆçš„ç­–ç•¥ï¼Œä»¥é™ä½BT-RvNNçš„å†…å­˜ä½¿ç”¨ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç­–ç•¥ä¸ä»…é™ä½äº†BT-RvNNçš„å†…å­˜ä½¿ç”¨é‡$10$-$16$å€ï¼Œè¿˜åˆ›é€ äº†ä¸€ä¸ªæ–°çš„çŠ¶æ€å‰ç»æ€§åœ¨ListOpsä¸­ï¼ŒåŒæ—¶ä¿æŒä¸å…¶ä»–ä»»åŠ¡çš„ç›¸ä¼¼æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†åˆ©ç”¨BT-RvNNç”Ÿæˆçš„æ½œåœ¨æ ‘èŠ‚ç‚¹è¡¨ç¤ºæ¥å°†BT-RvNNè½¬æ¢ä¸ºä¸€ä¸ªåºåˆ—Contextualizerçš„å½¢å¼$f:\mathbb{R}^{n \times d} \rightarrow \mathbb{R}^{n \times d}$ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æè®®ä¸ä»…å¼€å¯äº†RvNNçš„æ‰©å±•å¯èƒ½æ€§ï¼Œè¿˜æ ‡å‡†åŒ–äº†ä½¿ç”¨BT-RvNNä½œä¸ºæ·±åº¦å­¦ä¹ å·¥å…·ç®±ä¸­çš„å¦ä¸€ä¸ªæ„å»ºä»¶ï¼Œå¯ä»¥è½»æ¾å †å æˆ–è€…ä¸å…¶ä»–æµè¡Œçš„æ¨¡å‹ç›¸äº’ä½œç”¨ï¼Œå¦‚Transformerså’Œç»“æ„åŒ–çŠ¶æ€ç©ºé—´æ¨¡å‹ã€‚â€
</details></li>
</ul>
<hr>
<h2 id="Assessing-the-Use-of-AutoML-for-Data-Driven-Software-Engineering"><a href="#Assessing-the-Use-of-AutoML-for-Data-Driven-Software-Engineering" class="headerlink" title="Assessing the Use of AutoML for Data-Driven Software Engineering"></a>Assessing the Use of AutoML for Data-Driven Software Engineering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10774">http://arxiv.org/abs/2307.10774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabio Calefato, Luigi Quaranta, Filippo Lanubile, Marcos Kalinowski</li>
<li>for: å¡«è¡¥AI&#x2F;MLæŠ€æœ¯ä¸“ä¸šäººå‘˜çŸ­ç¼ºçš„é—®é¢˜ï¼Œä¿ƒè¿›è‡ªåŠ¨æœºå™¨å­¦ä¹ ï¼ˆAutoMLï¼‰çš„åº”ç”¨ã€‚</li>
<li>methods: ä½¿ç”¨æ··åˆæ–¹æ³•ç ”ç©¶ï¼ŒåŒ…æ‹¬12ç§ç»ˆç«¯AutoMLå·¥å…·åœ¨ä¸¤ä¸ªSEæ•°æ®é›†ä¸Šçš„æ¯”è¾ƒï¼Œä»¥åŠå¯¹å®è·µè€…å’Œç ”ç©¶è€…çš„è°ƒæŸ¥å’Œè®¿è°ˆã€‚</li>
<li>results: å‘ç°AutoMLè§£å†³æ–¹æ¡ˆå¯ä»¥åœ¨SEé¢†åŸŸä¸­çš„åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°æ›´å¥½ than manually trained and optimized modelsï¼Œä½†ç›®å‰å¯ç”¨çš„AutoMLè§£å†³æ–¹æ¡ˆä»æœªèƒ½å®Œå…¨æ”¯æŒæ‰€æœ‰é˜Ÿå‘˜å’Œå¼€å‘å·¥ä½œæµç¨‹çš„è‡ªåŠ¨åŒ–ã€‚<details>
<summary>Abstract</summary>
Background. Due to the widespread adoption of Artificial Intelligence (AI) and Machine Learning (ML) for building software applications, companies are struggling to recruit employees with a deep understanding of such technologies. In this scenario, AutoML is soaring as a promising solution to fill the AI/ML skills gap since it promises to automate the building of end-to-end AI/ML pipelines that would normally be engineered by specialized team members. Aims. Despite the growing interest and high expectations, there is a dearth of information about the extent to which AutoML is currently adopted by teams developing AI/ML-enabled systems and how it is perceived by practitioners and researchers. Method. To fill these gaps, in this paper, we present a mixed-method study comprising a benchmark of 12 end-to-end AutoML tools on two SE datasets and a user survey with follow-up interviews to further our understanding of AutoML adoption and perception. Results. We found that AutoML solutions can generate models that outperform those trained and optimized by researchers to perform classification tasks in the SE domain. Also, our findings show that the currently available AutoML solutions do not live up to their names as they do not equally support automation across the stages of the ML development workflow and for all the team members. Conclusions. We derive insights to inform the SE research community on how AutoML can facilitate their activities and tool builders on how to design the next generation of AutoML technologies.
</details>
<details>
<summary>æ‘˜è¦</summary>
èƒŒæ™¯ï¼šç”±äºäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å’Œæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰åœ¨è½¯ä»¶å¼€å‘ä¸­çš„æ™®åŠï¼Œå…¬å¸å›°éš¾æ‰¾åˆ°å…·å¤‡æ·±å±‚ç†è§£AI/MLæŠ€æœ¯çš„å‘˜å·¥ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒAutoMLåœ¨è§£å†³AI/MLæŠ€èƒ½å·®è·æ–¹é¢è¡¨ç°å‡ºäº†æ‰æ ¹çš„ä¼˜åŠ¿ï¼Œå› ä¸ºå®ƒæ‰¿è¯ºè‡ªåŠ¨åŒ–å»ºç«‹AI/MLç®¡é“ï¼Œé€šå¸¸éœ€è¦ä¸“ä¸šçš„å›¢é˜Ÿæˆå‘˜è¿›è¡Œå·¥ç¨‹ã€‚ç›®æ ‡ï¼šå°½ç®¡æœ‰å¢åŠ çš„å…´è¶£å’Œé«˜æœŸæœ›ï¼Œä½†æ˜¯æœ‰å…³AutoMLåœ¨å¼€å‘AI/MLç›¸å…³ç³»ç»Ÿçš„å›¢é˜Ÿä¸­çš„é‡‡ç”¨å’Œä¸“å®¶å’Œç ”ç©¶äººå‘˜å¯¹å…¶çœ‹æ³•çš„ä¿¡æ¯ä¸å¤Ÿã€‚æ–¹æ³•ï¼šä¸ºäº†å¡«è¡¥è¿™äº›ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€é¡¹æ··åˆæ–¹æ³•ç ”ç©¶ï¼ŒåŒ…æ‹¬12ä¸ªç»ˆç«¯AutoMLå·¥å…·åœ¨ä¸¤ä¸ªSEæ•°æ®é›†ä¸Šçš„benchmarkï¼Œä»¥åŠä¸ç›¸å…³ä¸“å®¶å’Œç ”ç©¶äººå‘˜è¿›è¡Œè¯¦ç»†äº¤æµçš„ç”¨æˆ·è°ƒæŸ¥ã€‚ç»“æœï¼šæˆ‘ä»¬å‘ç°AutoMLè§£å†³æ–¹æ¡ˆå¯ä»¥åœ¨SEé¢†åŸŸä¸­å¯¹åˆ†ç±»ä»»åŠ¡è¿›è¡Œæ›´å¥½çš„æ¨¡å‹ç”Ÿæˆï¼Œè€Œä¸”æˆ‘ä»¬çš„å‘ç°è¿˜è¡¨æ˜ç°æœ‰çš„AutoMLè§£å†³æ–¹æ¡ˆå¹¶ä¸èƒ½å¤Ÿå®Œå…¨æ”¯æŒè‡ªåŠ¨åŒ–MLå¼€å‘å·¥ä½œæµç¨‹ä¸­çš„æ‰€æœ‰é˜¶æ®µå’Œæ‰€æœ‰å›¢é˜Ÿæˆå‘˜ã€‚ç»“è®ºï¼šæˆ‘ä»¬ä»ç ”ç©¶ä¸­å¾—åˆ°äº†å…³äºå¦‚ä½•ä½¿ç”¨AutoMLä¿ƒè¿›SEç ”ç©¶äººå‘˜çš„æ´»åŠ¨ï¼Œä»¥åŠå¦‚ä½•è®¾è®¡ä¸‹ä¸€ä»£AutoMLæŠ€æœ¯çš„æŠ€æœ¯å»ºè®®ã€‚
</details></li>
</ul>
<hr>
<h2 id="Music-Genre-Classification-with-ResNet-and-Bi-GRU-Using-Visual-Spectrograms"><a href="#Music-Genre-Classification-with-ResNet-and-Bi-GRU-Using-Visual-Spectrograms" class="headerlink" title="Music Genre Classification with ResNet and Bi-GRU Using Visual Spectrograms"></a>Music Genre Classification with ResNet and Bi-GRU Using Visual Spectrograms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10773">http://arxiv.org/abs/2307.10773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junfei Zhang</li>
<li>for: æé«˜éŸ³ä¹æ’­æ”¾æœåŠ¡çš„ç”¨æˆ·ä½“éªŒå’Œæ»¡æ„åº¦ï¼Œå³Music Recommendation Systemsã€‚</li>
<li>methods: ä½¿ç”¨è§†è§‰spectrogramä½œä¸ºè¾“å…¥ï¼Œå¹¶æå‡ºäº†ä¸€ç§ hybrid æ¨¡å‹ï¼Œç»“åˆ Residual neural Network (ResNet) å’Œ Gated Recurrent Unit (GRU)ï¼Œä»¥æ›´å¥½åœ°æ•æ‰éŸ³ä¹æ•°æ®çš„å¤æ‚æ€§ã€‚</li>
<li>results: æå‡ºäº†ä¸€ç§æ–°çš„ Automatic Music Genre Classification (AMGC) ç³»ç»Ÿï¼Œå¯ä»¥æ›´å¥½åœ°æ•æ‰éŸ³ä¹æ•°æ®çš„å¤æ‚æ€§ï¼Œå¹¶ä¸”å¯èƒ½æé«˜éŸ³ä¹æ¨èç³»ç»Ÿçš„å‡†ç¡®ç‡ã€‚<details>
<summary>Abstract</summary>
Music recommendation systems have emerged as a vital component to enhance user experience and satisfaction for the music streaming services, which dominates music consumption. The key challenge in improving these recommender systems lies in comprehending the complexity of music data, specifically for the underpinning music genre classification. The limitations of manual genre classification have highlighted the need for a more advanced system, namely the Automatic Music Genre Classification (AMGC) system. While traditional machine learning techniques have shown potential in genre classification, they heavily rely on manually engineered features and feature selection, failing to capture the full complexity of music data. On the other hand, deep learning classification architectures like the traditional Convolutional Neural Networks (CNN) are effective in capturing the spatial hierarchies but struggle to capture the temporal dynamics inherent in music data. To address these challenges, this study proposes a novel approach using visual spectrograms as input, and propose a hybrid model that combines the strength of the Residual neural Network (ResNet) and the Gated Recurrent Unit (GRU). This model is designed to provide a more comprehensive analysis of music data, offering the potential to improve the music recommender systems through achieving a more comprehensive analysis of music data and hence potentially more accurate genre classification.
</details>
<details>
<summary>æ‘˜è¦</summary>
éŸ³ä¹æ¨èç³»ç»Ÿå·²æˆä¸ºç°ä»£éŸ³ä¹æµåª’ä½“æœåŠ¡çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œä»¥æé«˜ç”¨æˆ·ä½“éªŒå’Œæ»¡æ„åº¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„éŸ³ä¹åˆ†ç±»æ–¹æ³•å—åˆ°äº†å¤æ‚çš„éŸ³ä¹æ•°æ®çš„é™åˆ¶ï¼Œå°¤å…¶æ˜¯åœ¨éŸ³ä¹ç§ç±»å½’ç±»æ–¹é¢ã€‚ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æŠ€æœ¯å¯èƒ½æœ‰æ½œåŠ›ï¼Œä½†æ˜¯å®ƒä»¬ä¾èµ–äºäººå·¥è®¾è®¡çš„ç‰¹å¾å’Œç‰¹å¾é€‰æ‹©ï¼Œæ— æ³•æ•æ‰éŸ³ä¹æ•°æ®çš„å…¨é¢å¤æ‚æ€§ã€‚ç›¸åï¼Œæ·±åº¦å­¦ä¹ åˆ†ç±»æ¶æ„å¦‚ä¼ ç»Ÿçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å¯ä»¥æ•æ‰éŸ³ä¹æ•°æ®çš„ç©ºé—´å±‚æ¬¡ç»“æ„ï¼Œä½†æ˜¯å®ƒä»¬å¾ˆéš¾æ•æ‰éŸ³ä¹æ•°æ®ä¸­çš„æ—¶é—´åŠ¨æ€ç‰¹å¾ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œä½¿ç”¨è§†è§‰ÑĞ¿ĞµĞºÑ‚Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼ä½œä¸ºè¾“å…¥ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ··åˆæ¨¡å‹ï¼Œç»“åˆäº†Residualç¥ç»ç½‘ç»œï¼ˆResNetï¼‰å’ŒGated Recurrent Unitï¼ˆGRUï¼‰ã€‚è¿™ç§æ¨¡å‹è®¾è®¡ç”¨äºä¸ºéŸ³ä¹æ•°æ®æä¾›æ›´å…¨é¢çš„åˆ†æï¼Œå¹¶ä¸”å¯èƒ½æé«˜éŸ³ä¹æ¨èç³»ç»Ÿçš„å‡†ç¡®ç‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Emotions-from-EEG-A-GRU-Based-Approach"><a href="#Unveiling-Emotions-from-EEG-A-GRU-Based-Approach" class="headerlink" title="Unveiling Emotions from EEG: A GRU-Based Approach"></a>Unveiling Emotions from EEG: A GRU-Based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02778">http://arxiv.org/abs/2308.02778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarthak Johari, Gowri Namratha Meedinti, Radhakrishnan Delhibabu, Deepak Joshi<br>for: è¿™é¡¹ç ”ç©¶æ—¨åœ¨ä½¿ç”¨EEGæ•°æ®è¿›è¡Œæƒ…æ„Ÿè¯†åˆ«ï¼Œä»¥æé«˜äººæœºäº¤äº’å’Œæƒ…æ„Ÿè®¡ç®—é¢†åŸŸçš„å‘å±•ã€‚methods: è¿™é¡¹ç ”ç©¶ä½¿ç”¨äº†Gated Recurrent Unitï¼ˆGRUï¼‰ç®—æ³•ï¼Œå®ƒæ˜¯ä¸€ç§Recurrent Neural Networksï¼ˆRNNsï¼‰çš„å˜ç§ï¼Œé€šè¿‡åˆ©ç”¨EEGä¿¡å·æ¥é¢„æµ‹æƒ…æ„ŸçŠ¶æ€ã€‚ç ”ç©¶è€…ä»¬ä½¿ç”¨äº†å…¬å…±å¯è®¿é—®çš„æ•°æ®é›†ï¼ŒåŒ…æ‹¬åœ¨æ— åŠ¨ä½œçŠ¶æ€ä¸‹çš„ä¸­æ€§æ•°æ®ä»¥åŠå—åˆ°åˆºæ¿€åçš„äººç±»EEGè®°å½•ï¼Œä»¥åŠæ¿€å‘ happinessã€neutralå’Œnegativeæƒ…æ„Ÿçš„åˆºæ¿€ã€‚ä¸ºäº†è·å¾—æœ€ä½³çš„ç‰¹å¾æå–ï¼Œç ”ç©¶è€…ä»¬å¯¹EEGæ•°æ®è¿›è¡Œäº†ç£æ€é™¤ã€é¢‘ç‡ç­›é€‰å’Œnormalizationå¤„ç†ã€‚results: ç ”ç©¶è€…ä»¬ä½¿ç”¨äº†GRUæ¨¡å‹ï¼Œå¹¶åœ¨éªŒè¯é›†ä¸Šè¾¾åˆ°äº†100%çš„å‡†ç¡®ç‡ã€‚ä¸å…¶ä»–æœºå™¨å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒGRUæ¨¡å‹çš„Extreme Gradient Boosting Classifierå…·æœ‰æœ€é«˜çš„å‡†ç¡®ç‡ã€‚ç ”ç©¶è€…ä»¬è¿˜å¯¹æ¨¡å‹çš„æ··æ·†çŸ©é˜µè¿›è¡Œäº†åˆ†æï¼Œä»è€Œè·å¾—äº†ç²¾å‡†çš„æƒ…æ„Ÿåˆ†ç±»ç»“æœã€‚è¿™é¡¹ç ”ç©¶è¡¨æ˜äº†æ·±åº¦å­¦ä¹ æ¨¡å‹å¦‚GRUåœ¨æƒ…æ„Ÿè¯†åˆ«æ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶ä¸”å¼€åˆ›äº†æ–°çš„æƒ…æ„Ÿè®¡ç®—é¢†åŸŸçš„å¯èƒ½æ€§ã€‚<details>
<summary>Abstract</summary>
One of the most important study areas in affective computing is emotion identification using EEG data. In this study, the Gated Recurrent Unit (GRU) algorithm, which is a type of Recurrent Neural Networks (RNNs), is tested to see if it can use EEG signals to predict emotional states. Our publicly accessible dataset consists of resting neutral data as well as EEG recordings from people who were exposed to stimuli evoking happy, neutral, and negative emotions. For the best feature extraction, we pre-process the EEG data using artifact removal, bandpass filters, and normalization methods. With 100% accuracy on the validation set, our model produced outstanding results by utilizing the GRU's capacity to capture temporal dependencies. When compared to other machine learning techniques, our GRU model's Extreme Gradient Boosting Classifier had the highest accuracy. Our investigation of the confusion matrix revealed insightful information about the performance of the model, enabling precise emotion classification. This study emphasizes the potential of deep learning models like GRUs for emotion recognition and advances in affective computing. Our findings open up new possibilities for interacting with computers and comprehending how emotions are expressed through brainwave activity.
</details>
<details>
<summary>æ‘˜è¦</summary>
(Simplified Chinese)ä¸€ä¸ªéå¸¸é‡è¦çš„ç ”ç©¶é¢†åŸŸåœ¨æƒ…æ„Ÿè®¡ç®—æ˜¯ä½¿ç”¨EEGæ•°æ®è¿›è¡Œæƒ…æ„Ÿè¯†åˆ«ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†Gated Recurrent Unitï¼ˆGRUï¼‰ç®—æ³•ï¼Œè¿™æ˜¯ä¸€ç§Recurrent Neural Networksï¼ˆRNNsï¼‰çš„å˜ç§ï¼Œä»¥EEGä¿¡å·æ¥é¢„æµ‹æƒ…æ„ŸçŠ¶æ€ã€‚æˆ‘ä»¬çš„å…¬å…±å¯è®¿é—®æ•°æ®é›†åŒ…æ‹¬æ™®é€šçš„ä¸­æ€§æ•°æ®ä»¥åŠåœ¨åˆºæ¿€äººä»¬è¡¨ç°å‡ºå–œã€ä¸­æ€§å’Œè´Ÿæƒ…çš„EEGè®°å½•ã€‚ä¸ºäº†è·å¾—æœ€ä½³çš„ç‰¹å¾æå–ï¼Œæˆ‘ä»¬å¯¹EEGæ•°æ®è¿›è¡Œäº†å™ªå£°é™¤é™¤ã€é¢‘ç‡ç­›é€‰å’Œ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°åŒ–å¤„ç†ã€‚åœ¨éªŒè¯é›†ä¸Šå¾—åˆ°100%çš„å‡†ç¡®ç‡ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¡¨ç°å‡ºè‰²ï¼Œåˆ©ç”¨GRUæ•æ‰æ—¶é—´ç›¸å…³æ€§çš„èƒ½åŠ›ã€‚ä¸å…¶ä»–æœºå™¨å­¦ä¹ æŠ€æœ¯ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„GRUæ¨¡å‹çš„Extreme Gradient Boosting Classifierï¼ˆEGGBï¼‰å‡†ç¡®ç‡æœ€é«˜ã€‚æˆ‘ä»¬å¯¹æ¨¡å‹çš„æ··æ·†çŸ©é˜µè¿›è¡Œäº†è°ƒæŸ¥ï¼Œè·å¾—äº†æ¨¡å‹è¡¨ç°çš„æœ‰ç”¨ä¿¡æ¯ï¼Œå¯å‘ç²¾å‡†çš„æƒ…æ„Ÿåˆ†ç±»ã€‚è¿™é¡¹ç ”ç©¶å¼ºè°ƒäº†æ·±åº¦å­¦ä¹ æ¨¡å‹å¦‚GRUçš„æƒ…æ„Ÿè¯†åˆ«æ½œåŠ›ï¼Œå¹¶æå‡ºäº†å¯¹æƒ…æ„Ÿè®¡ç®—çš„æ–°å¯èƒ½æ€§ã€‚æˆ‘ä»¬çš„å‘ç°æ‰“å¼€äº†ä¸è®¡ç®—æœºäº¤äº’å’Œç†è§£è„‘æ³¢æ´»åŠ¨ä¸­æƒ…æ„Ÿè¡¨è¾¾çš„æ–°å¯èƒ½æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Decoding-the-Enigma-Benchmarking-Humans-and-AIs-on-the-Many-Facets-of-Working-Memory"><a href="#Decoding-the-Enigma-Benchmarking-Humans-and-AIs-on-the-Many-Facets-of-Working-Memory" class="headerlink" title="Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory"></a>Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10768">http://arxiv.org/abs/2307.10768</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhanglab-deepneurocoglab/worm">https://github.com/zhanglab-deepneurocoglab/worm</a></li>
<li>paper_authors: Ankur Sikarwar, Mengmi Zhang</li>
<li>for: æœ¬ç ”ç©¶çš„ç›®çš„æ˜¯å¼€å‘ä¸€ä¸ªå®Œæ•´çš„å·¥ä½œè®°å¿†ï¼ˆWMï¼‰benchmarkæ•°æ®é›†ï¼Œä»¥ä¾¿ç”¨äºAI WMæ¨¡å‹çš„å¼€å‘å’Œè¯„ä¼°ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†10ä¸ªä»»åŠ¡å’Œ100ä¸‡ä¸ªå®éªŒï¼Œè¯„ä¼°äº†4ç§åŠŸèƒ½ã€3ç§é¢†åŸŸå’Œ11ç§è¡Œä¸ºå’Œç¥ç»ç‰¹å¾ã€‚åŒæ—¶ï¼Œè¿˜åŒ…æ‹¬äº†äººç±»è¡Œä¸ºçš„å‚ç…§å€¼ä½œä¸ºæ¯”è¾ƒæ ‡å‡†ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼ŒAIæ¨¡å‹åœ¨ä¸€äº›æƒ…å†µä¸‹èƒ½å¤Ÿæ¨¡æ‹Ÿå¤§è„‘ä¸­çš„å·¥ä½œè®°å¿†ç‰¹å¾ï¼Œå¦‚ primacy å’Œ recency æ•ˆåº”ï¼Œä»¥åŠå„ä¸ªé¢†åŸŸå’ŒåŠŸèƒ½çš„ç¥ç»å›¢å—å’Œç›¸å…³æ€§ã€‚ç„¶è€Œï¼Œä¹Ÿå‘ç°ç°æœ‰æ¨¡å‹å­˜åœ¨ä¸€äº›é™åˆ¶ï¼Œæ— æ³•å®Œå…¨approximateäººç±»è¡Œä¸ºã€‚è¿™ä¸ªæ•°æ®é›†å°†æˆä¸ºè·¨ Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸inaryçš„èµ„æºï¼Œç”¨äºæ¯”è¾ƒå’Œæ”¹è¿›WMæ¨¡å‹ï¼Œç ”ç©¶WMçš„ç¥ç»åŸºç¡€ï¼Œå¹¶å¼€å‘äººç±»æ ·å¼çš„WMæ¨¡å‹ã€‚<details>
<summary>Abstract</summary>
Working memory (WM), a fundamental cognitive process facilitating the temporary storage, integration, manipulation, and retrieval of information, plays a vital role in reasoning and decision-making tasks. Robust benchmark datasets that capture the multifaceted nature of WM are crucial for the effective development and evaluation of AI WM models. Here, we introduce a comprehensive Working Memory (WorM) benchmark dataset for this purpose. WorM comprises 10 tasks and a total of 1 million trials, assessing 4 functionalities, 3 domains, and 11 behavioral and neural characteristics of WM. We jointly trained and tested state-of-the-art recurrent neural networks and transformers on all these tasks. We also include human behavioral benchmarks as an upper bound for comparison. Our results suggest that AI models replicate some characteristics of WM in the brain, most notably primacy and recency effects, and neural clusters and correlates specialized for different domains and functionalities of WM. In the experiments, we also reveal some limitations in existing models to approximate human behavior. This dataset serves as a valuable resource for communities in cognitive psychology, neuroscience, and AI, offering a standardized framework to compare and enhance WM models, investigate WM's neural underpinnings, and develop WM models with human-like capabilities. Our source code and data are available at https://github.com/ZhangLab-DeepNeuroCogLab/WorM.
</details>
<details>
<summary>æ‘˜è¦</summary>
å·¥ä½œè®°å¿†ï¼ˆWMï¼‰ï¼Œä¸€ç§åŸºæœ¬çš„è®¤çŸ¥è¿‡ç¨‹ï¼ŒååŠ©çŸ­æš‚å­˜å‚¨ã€ç»“åˆã€æ“ä½œå’ŒæŠ½å–ä¿¡æ¯ï¼Œåœ¨æ¨ç†å’Œå†³ç­–ä»»åŠ¡ä¸­æ‰®æ¼”è‡³å…³é‡è¦çš„è§’è‰²ã€‚ä¸ºäº†æœ‰æ•ˆå¼€å‘å’Œè¯„ä¼°äººå·¥æ™ºèƒ½WMæ¨¡å‹ï¼Œéœ€è¦ä¸€äº›å¯é çš„benchmarkæ•°æ®é›†ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªå…¨é¢çš„Working Memoryï¼ˆWorMï¼‰benchmarkæ•°æ®é›†ï¼Œç”¨äºè¿™ä¸ªç›®çš„ã€‚WorMåŒ…æ‹¬10ä¸ªä»»åŠ¡å’Œæ€»å…±100ä¸‡ä¸ªå°è¯•ï¼Œè¯„ä¼°äº†4ç§åŠŸèƒ½ã€3ç§é¢†åŸŸå’Œ11ç§è¡Œä¸ºå’Œç¥ç»ç‰¹å¾ã€‚æˆ‘ä»¬å°†ç°æœ‰çš„å¾ªç¯ç¥ç»ç½‘ç»œå’Œè½¬æ¢å™¨æ¨¡å‹åœ¨æ‰€æœ‰è¿™äº›ä»»åŠ¡ä¸ŠåŒæ—¶è®­ç»ƒå’Œæµ‹è¯•ã€‚æˆ‘ä»¬è¿˜åŒ…æ‹¬äº†äººç±»è¡Œä¸ºæ ‡å‡† als an upper bound for comparisonã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œäººå·¥æ™ºèƒ½æ¨¡å‹åœ¨è„‘ä¸­çš„WMç‰¹å¾ä¸­å¤åˆ¶äº†ä¸€äº›ç‰¹å¾ï¼Œå¦‚åŠ£antageå’Œæœ€æ–°æ•ˆåº”ï¼Œä»¥åŠç‰¹å®šé¢†åŸŸå’ŒåŠŸèƒ½çš„ç¥ç»å›¢å’Œç›¸å…³ç‰¹å¾ã€‚åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬è¿˜å‘ç°äº†ç°æœ‰æ¨¡å‹çš„ä¸€äº›å±€é™æ€§ï¼Œæ— æ³•æ¨¡æ‹Ÿäººç±»è¡Œä¸ºã€‚è¿™ä¸ªæ•°æ®é›†ä½œä¸ºä¸€ä¸ª Ñ†ĞµĞ½Ğ½Ñ‹Ğ¹èµ„æºï¼Œå¯ä»¥ä¸ºè®¤çŸ¥å¿ƒç†å­¦ã€ç¥ç»ç§‘å­¦å’Œäººå·¥æ™ºèƒ½ç¤¾åŒºæä¾›ä¸€ä¸ªæ ‡å‡†åŒ–çš„æ¡†æ¶ï¼Œç”¨äºæ¯”è¾ƒå’Œæ”¹è¿›WMæ¨¡å‹ï¼Œè°ƒæŸ¥WMçš„ç¥ç»åŸºç¡€ï¼Œå¹¶å¼€å‘äººç±»ç±»ä¼¼çš„WMæ¨¡å‹ã€‚æˆ‘ä»¬çš„æºä»£ç å’Œæ•°æ®å¯ä»¥åœ¨https://github.com/ZhangLab-DeepNeuroCogLab/WorMä¸Šè·å–ã€‚
</details></li>
</ul>
<hr>
<h2 id="Actor-agnostic-Multi-label-Action-Recognition-with-Multi-modal-Query"><a href="#Actor-agnostic-Multi-label-Action-Recognition-with-Multi-modal-Query" class="headerlink" title="Actor-agnostic Multi-label Action Recognition with Multi-modal Query"></a>Actor-agnostic Multi-label Action Recognition with Multi-modal Query</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10763">http://arxiv.org/abs/2307.10763</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mondalanindya/msqnet">https://github.com/mondalanindya/msqnet</a></li>
<li>paper_authors: Anindya Mondal, Sauradip Nag, Joaquin M Prada, Xiatian Zhu, Anjan Dutta</li>
<li>for: æå‡ºäº†ä¸€ç§actor-agnostic multi-modal multi-label action recognitionæ–¹æ³•ï¼Œä»¥è§£å†³actor-specific pose estimationå’Œå¤šä¸ªè¡Œä¸ºåŒæ—¶å‘ç”Ÿçš„é—®é¢˜ã€‚</li>
<li>methods: æå‡ºäº†ä¸€ç§åŸºäº transformer æ£€æµ‹æ¡†æ¶çš„ Multi-modal Semantic Query Network (MSQNet) æ¨¡å‹ï¼Œåˆ©ç”¨è§†è§‰å’Œæ–‡æœ¬æ¨¡å¼æ›´å¥½åœ°è¡¨ç¤ºè¡Œä¸ºç±»åˆ«ã€‚</li>
<li>results: åœ¨äº”ä¸ªå…¬å¼€çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå¹¶ consistently outperformed actor-specific alternatives on human and animal single- and multi-label action recognition tasks by up to 50%.<details>
<summary>Abstract</summary>
Existing action recognition methods are typically actor-specific due to the intrinsic topological and apparent differences among the actors. This requires actor-specific pose estimation (e.g., humans vs. animals), leading to cumbersome model design complexity and high maintenance costs. Moreover, they often focus on learning the visual modality alone and single-label classification whilst neglecting other available information sources (e.g., class name text) and the concurrent occurrence of multiple actions. To overcome these limitations, we propose a new approach called 'actor-agnostic multi-modal multi-label action recognition,' which offers a unified solution for various types of actors, including humans and animals. We further formulate a novel Multi-modal Semantic Query Network (MSQNet) model in a transformer-based object detection framework (e.g., DETR), characterized by leveraging visual and textual modalities to represent the action classes better. The elimination of actor-specific model designs is a key advantage, as it removes the need for actor pose estimation altogether. Extensive experiments on five publicly available benchmarks show that our MSQNet consistently outperforms the prior arts of actor-specific alternatives on human and animal single- and multi-label action recognition tasks by up to 50%. Code will be released at https://github.com/mondalanindya/MSQNet.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç°æœ‰çš„åŠ¨ä½œè¯†åˆ«æ–¹æ³•é€šå¸¸æ˜¯actor-specificçš„ï¼Œå› ä¸ºactorä¹‹é—´å­˜åœ¨å†…åœ¨çš„ topological å’Œ apparent å·®å¼‚ã€‚è¿™ä¼šå¯¼è‡´actor-specific å§¿åŠ¿ä¼°è®¡ï¼ˆä¾‹å¦‚äºº VS åŠ¨ç‰©ï¼‰ï¼Œä»è€Œå¢åŠ æ¨¡å‹è®¾è®¡å¤æ‚åº¦å’Œç»´æŠ¤æˆæœ¬ã€‚æ­¤å¤–ï¼Œå®ƒä»¬é€šå¸¸åªå­¦ä¹ è§†è§‰modal alone å’Œå•ä¸ªæ ‡ç­¾åˆ†ç±»ï¼Œè€Œå¿½ç•¥å…¶ä»–å¯ç”¨çš„ä¿¡æ¯æºï¼ˆä¾‹å¦‚ç±»åæ–‡æœ¬ï¼‰ä»¥åŠåŒæ—¶å‘ç”Ÿçš„å¤šä¸ªåŠ¨ä½œã€‚ä¸ºäº†è¶…è¶Šè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„approach called 'actor-agnostic multi-modal multi-label action recognition', which offers a unified solution for various types of actors, including humans and animals. We further formulate a novel Multi-modal Semantic Query Network (MSQNet) model in a transformer-based object detection framework (e.g., DETR), characterized by leveraging visual and textual modalities to represent the action classes better. The elimination of actor-specific model designs is a key advantage, as it removes the need for actor pose estimation altogether. Extensive experiments on five publicly available benchmarks show that our MSQNet consistently outperforms the prior arts of actor-specific alternatives on human and animal single- and multi-label action recognition tasks by up to 50%. Code will be released at https://github.com/mondalanindya/MSQNet.
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Voter-Attribute-Bias-for-Fair-Opinion-Aggregation"><a href="#Mitigating-Voter-Attribute-Bias-for-Fair-Opinion-Aggregation" class="headerlink" title="Mitigating Voter Attribute Bias for Fair Opinion Aggregation"></a>Mitigating Voter Attribute Bias for Fair Opinion Aggregation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10749">http://arxiv.org/abs/2307.10749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryosuke Ueda, Koh Takeuchi, Hisashi Kashima</li>
<li>For: This paper focuses on developing fair opinion aggregation methods to address biases in decision-making, particularly in tasks without objectively true labels.* Methods: The authors propose a combination of opinion aggregation models, such as majority voting and the Dawid and Skene model, with fairness options like sample weighting and data splitting. They also introduce a new Soft D&amp;S model to improve soft label estimation.* Results: The experimental results show that the combination of Soft D&amp;S and data splitting is effective for dense data, while weighted majority voting is effective for sparse data. These findings support fair opinion aggregation in decision-making, both for human and machine-learning models.Hereâ€™s the simplified Chinese text for the three key points:* For: è¿™ç¯‡è®ºæ–‡å…³æ³¨äº†åœ¨å†³ç­–ä¸­å‡å°‘æ„è§åè§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ²¡æœ‰å”¯ä¸€æ­£ç¡®æ ‡ç­¾çš„ä»»åŠ¡ä¸­ã€‚* Methods: ä½œè€…ä»¬æè®®ç»“åˆæ„è§é›†æˆæ¨¡å‹ï¼Œå¦‚å¤šæ•°æŠ•ç¥¨å’Œè¾¾éŸ¦å¾·å’Œé”ˆæ¨¡å‹ï¼Œä»¥åŠå…¬å¹³é€‰é¡¹ï¼Œå¦‚æ ·æœ¬æƒé‡å’Œæ•°æ®åˆ†å‰²ã€‚ä»–ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°çš„è½¯D&amp;Sæ¨¡å‹ï¼Œä»¥æé«˜è½¯æ ‡ç­¾ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚* Results: å®éªŒç»“æœè¡¨æ˜ï¼Œå°†è½¯D&amp;Sæ¨¡å‹ä¸æ•°æ®åˆ†å‰²ç»“åˆä½¿ç”¨ï¼Œå¯¹äºç¨ å¯†æ•°æ®æ˜¯æœ‰æ•ˆçš„ï¼Œè€ŒWeightedå¤šæ•°æŠ•ç¥¨å¯¹äºç¨€ç–æ•°æ®æ˜¯æœ‰æ•ˆçš„ã€‚è¿™äº›å‘ç°å°†ä¸ºäººç±»å’Œæœºå™¨å­¦ä¹ æ¨¡å‹çš„å‡è¡¡æ„è§é›†æˆæä¾›æ”¯æŒã€‚<details>
<summary>Abstract</summary>
The aggregation of multiple opinions plays a crucial role in decision-making, such as in hiring and loan review, and in labeling data for supervised learning. Although majority voting and existing opinion aggregation models are effective for simple tasks, they are inappropriate for tasks without objectively true labels in which disagreements may occur. In particular, when voter attributes such as gender or race introduce bias into opinions, the aggregation results may vary depending on the composition of voter attributes. A balanced group of voters is desirable for fair aggregation results but may be difficult to prepare. In this study, we consider methods to achieve fair opinion aggregation based on voter attributes and evaluate the fairness of the aggregated results. To this end, we consider an approach that combines opinion aggregation models such as majority voting and the Dawid and Skene model (D&S model) with fairness options such as sample weighting. To evaluate the fairness of opinion aggregation, probabilistic soft labels are preferred over discrete class labels. First, we address the problem of soft label estimation without considering voter attributes and identify some issues with the D&S model. To address these limitations, we propose a new Soft D&S model with improved accuracy in estimating soft labels. Moreover, we evaluated the fairness of an opinion aggregation model, including Soft D&S, in combination with different fairness options using synthetic and semi-synthetic data. The experimental results suggest that the combination of Soft D&S and data splitting as a fairness option is effective for dense data, whereas weighted majority voting is effective for sparse data. These findings should prove particularly valuable in supporting decision-making by human and machine-learning models with balanced opinion aggregation.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œå¤šå…ƒæ„è§çš„ç»Ÿè®¡å‘æŒ¥äº†é‡è¦çš„å†³ç­–ä½œç”¨ï¼Œä¾‹å¦‚åœ¨æ‹›è˜å’Œè´·æ¬¾å®¡æ ¸ä¸­ï¼Œä»¥åŠåœ¨æŒ‡å¯¼å­¦ä¹ ä¸­æ ‡ç­¾æ•°æ®ã€‚ Although majority votingå’Œç°æœ‰çš„æ„è§ç»Ÿè®¡æ¨¡å‹åœ¨ç®€å•ä»»åŠ¡ä¸Šæ•ˆæœè‰¯å¥½ï¼Œä½†åœ¨æ— æ˜ç¡®çœŸå®æ ‡ç­¾çš„ä»»åŠ¡ä¸­ï¼Œå®ƒä»¬æ— æ³•åº”å¯¹ä¸åŒæ„è§çš„åˆ†æ­§ã€‚å…·ä½“æ¥è¯´ï¼Œå½“æŠ•ç¥¨è€…å±æ€§å¦‚æ€§åˆ«æˆ–ç§æ—å¼•å…¥åè§åˆ°æ„è§æ—¶ï¼Œç»Ÿè®¡ç»“æœå°†å› æŠ•ç¥¨è€…å±æ€§çš„åˆ†å¸ƒè€Œå¼‚ã€‚ä¸€ä¸ªå‡è¡¡çš„æŠ•ç¥¨è€…ç¾¤ä½“æ˜¯æœ‰åˆ©äºå…¬å¹³ç»Ÿè®¡ç»“æœçš„ï¼Œä½†å¯èƒ½å…·æœ‰å›°éš¾ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è€ƒè™‘äº†åŸºäºæŠ•ç¥¨è€…å±æ€§çš„å…¬å¹³æ„è§ç»Ÿè®¡æ–¹æ³•ï¼Œå¹¶è¯„ä¼°è¿™äº›æ–¹æ³•çš„å…¬å¹³æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è€ƒè™‘äº†ç»“åˆæ„è§ç»Ÿè®¡æ¨¡å‹ï¼ˆå¦‚å¤šæ•°æŠ•ç¥¨å’Œé“ç»´å¾·å’Œæ–¯å‡¯çº³æ¨¡å‹ï¼‰å’Œå…¬å¹³é€‰é¡¹ï¼ˆå¦‚æŠ½æ ·é‡é‡ï¼‰ã€‚å¯¹äºè¯„ä¼°å…¬å¹³æ€§ï¼Œéšæœºè½¯æ ‡ç­¾è¢«è§†ä¸ºæ›´å¥½çš„é€‰æ‹©ï¼Œè€Œä¸æ˜¯ç¡¬codedæ ‡ç­¾ã€‚æˆ‘ä»¬é¦–å…ˆè§£å†³äº†ä¸è€ƒè™‘æŠ•ç¥¨è€…å±æ€§çš„soft labelä¼°è®¡é—®é¢˜ï¼Œå¹¶å‘ç°äº†ä¸€äº›é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„Soft D&Sæ¨¡å‹ï¼Œå…·æœ‰æ”¹å–„äº† soft label ä¼°è®¡çš„ç²¾åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº†ä¸åŒå…¬å¹³é€‰é¡¹ä¸Soft D&Sæ¨¡å‹çš„æ•´ä½“å…¬å¹³æ€§ï¼Œä½¿ç”¨äººå·¥å’ŒåŠè‡ªç„¶æ•°æ®ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»“åˆSoft D&Sæ¨¡å‹å’ŒæŠ½æ ·é‡é‡çš„å…¬å¹³é€‰é¡¹æ˜¯åœ¨åšåº¦æ•°æ®ä¸­æœ‰æ•ˆçš„ï¼Œè€ŒWeightedå¤šæ•°æŠ•ç¥¨åˆ™æ˜¯åœ¨å èŠ‚æ•°æ®ä¸­æœ‰æ•ˆçš„ã€‚è¿™äº›å‘ç°å°†åœ¨æ”¯æŒäººç±»å’Œæœºå™¨å­¦ä¹ æ¨¡å‹çš„æŠ•ç¥¨ç»“æœå‡è¡¡ä¸­æä¾›ä»·å€¼ã€‚â€
</details></li>
</ul>
<hr>
<h2 id="Fairness-Aware-Client-Selection-for-Federated-Learning"><a href="#Fairness-Aware-Client-Selection-for-Federated-Learning" class="headerlink" title="Fairness-Aware Client Selection for Federated Learning"></a>Fairness-Aware Client Selection for Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10738">http://arxiv.org/abs/2307.10738</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxin Shi, Zelei Liu, Zhuan Shi, Han Yu</li>
<li>for: æé«˜ Federated Learningï¼ˆFLï¼‰å®¢æˆ·ç«¯é€‰æ‹©çš„å…¬å¹³æ€§å’Œæ¨¡å‹æ€§èƒ½ã€‚</li>
<li>methods: åŸºäº Lyapunov ä¼˜åŒ–çš„ Fairness-aware Federated Client Selectionï¼ˆFairFedCSï¼‰æ–¹æ³•ï¼Œé€šè¿‡è€ƒè™‘å®¢æˆ·ç«¯çš„å£°èª‰ã€å‚ä¸ FL ä»»åŠ¡çš„æ—¶é—´å’Œæ¨¡å‹æ€§èƒ½çš„è´¡çŒ®ï¼ŒåŠ¨æ€è°ƒæ•´å®¢æˆ·ç«¯é€‰æ‹©æ¦‚ç‡ã€‚</li>
<li>results: åœ¨å®é™…çš„ multimedia æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå¹¶æ˜¾ç¤ºäº† FairFedCS å¯ä»¥æé«˜å¹³å‡ fairness 19.6% å’Œæµ‹è¯•ç²¾åº¦ 0.73% æ¯”æœ€ä½³çŠ¶æ€çš„æ–¹æ³•ã€‚<details>
<summary>Abstract</summary>
Federated learning (FL) has enabled multiple data owners (a.k.a. FL clients) to train machine learning models collaboratively without revealing private data. Since the FL server can only engage a limited number of clients in each training round, FL client selection has become an important research problem. Existing approaches generally focus on either enhancing FL model performance or enhancing the fair treatment of FL clients. The problem of balancing performance and fairness considerations when selecting FL clients remains open. To address this problem, we propose the Fairness-aware Federated Client Selection (FairFedCS) approach. Based on Lyapunov optimization, it dynamically adjusts FL clients' selection probabilities by jointly considering their reputations, times of participation in FL tasks and contributions to the resulting model performance. By not using threshold-based reputation filtering, it provides FL clients with opportunities to redeem their reputations after a perceived poor performance, thereby further enhancing fair client treatment. Extensive experiments based on real-world multimedia datasets show that FairFedCS achieves 19.6% higher fairness and 0.73% higher test accuracy on average than the best-performing state-of-the-art approach.
</details>
<details>
<summary>æ‘˜è¦</summary>
federated learningï¼ˆFLï¼‰å·²ç»å…è®¸å¤šä¸ªæ•°æ®æ‹¥æœ‰è€…ï¼ˆå³FLå®¢æˆ·ï¼‰å…±åŒè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œæ— éœ€æŠ«éœ²ç§äººæ•°æ®ã€‚ç”±äºFLæœåŠ¡å™¨åªèƒ½åœ¨æ¯æ¬¡è®­ç»ƒä¸­é€‰æ‹©ä¸€å®šæ•°é‡çš„å®¢æˆ·ï¼Œå› æ­¤é€‰æ‹©FLå®¢æˆ·å·²æˆä¸ºä¸€ä¸ªé‡è¦çš„ç ”ç©¶é—®é¢˜ã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸æ˜¯å¢å¼ºFLæ¨¡å‹æ€§èƒ½æˆ–å¢å¼ºFLå®¢æˆ·çš„å…¬å¹³å¾…é‡ã€‚å°šæœªè§£å†³FLå®¢æˆ·æ€§èƒ½å’Œå…¬å¹³å¾…é‡è€ƒè™‘çš„æƒè¡¡é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å…¬å¹³æ€§æ„ŸçŸ¥ Federated Client Selectionï¼ˆFairFedCSï¼‰æ–¹æ³•ã€‚åŸºäº Lyapunov ä¼˜åŒ–ï¼Œå®ƒåœ¨FLå®¢æˆ·é€‰æ‹©æ¦‚ç‡ä¸­è¿›è¡Œäº†åŠ¨æ€è°ƒæ•´ï¼Œå¹¶ä¸”åŒæ—¶è€ƒè™‘äº†FLå®¢æˆ·çš„å£°èª‰ã€å‚ä¸FLä»»åŠ¡çš„æ—¶é—´å’Œå¯¹æ¨¡å‹æ€§èƒ½çš„è´¡çŒ®ã€‚ä¸ä½¿ç”¨é˜ˆå€¼åŸºäºå£°èª‰ç­›é€‰ï¼Œå› æ­¤å…è®¸FLå®¢æˆ·åœ¨æ„ŸçŸ¥æ€§èƒ½ä¸ä½³æ—¶é‡æ–°æ¢å¤å£°èª‰ï¼Œä»è€Œè¿›ä¸€æ­¥æé«˜FLå®¢æˆ·çš„å…¬å¹³å¾…é‡ã€‚ç»è¿‡åŸºäºå®é™… multimedia æ•°æ®é›†çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬å‘ç° FairFedCS å¹³å‡æé«˜äº†19.6%çš„å…¬å¹³æ€§å’Œ0.73%çš„æµ‹è¯•å‡†ç¡®ç‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="Long-Tail-Theory-under-Gaussian-Mixtures"><a href="#Long-Tail-Theory-under-Gaussian-Mixtures" class="headerlink" title="Long-Tail Theory under Gaussian Mixtures"></a>Long-Tail Theory under Gaussian Mixtures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10736">http://arxiv.org/abs/2307.10736</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/armanbolatov/long_tail">https://github.com/armanbolatov/long_tail</a></li>
<li>paper_authors: Arman Bolatov, Maxat Tezekbayev, Igor Melnykov, Artur Pak, Vassilina Nikoulina, Zhenisbek Assylbekov</li>
<li>for: è¿™ç¯‡è®ºæ–‡å…³æ³¨ Feldman çš„é•¿å°¾ç†è®º (2020)ï¼Œæå‡ºäº†ä¸€ä¸ªç®€å•çš„ Gaussian æ··åˆæ¨¡å‹ï¼Œä»¥æµ‹è¯•ä¸åŒç±»å‹çš„æ ‡ç­¾æ¨¡å‹åœ¨é•¿å°¾åˆ†å¸ƒä¸‹çš„æ€§èƒ½ã€‚</li>
<li>methods: è®ºæ–‡ä½¿ç”¨äº†ä¸€ä¸ªçº¿æ€§åˆ†ç±»å™¨å’Œä¸€ä¸ªéçº¿æ€§åˆ†ç±»å™¨ï¼Œä»¥è¯„ä¼°å®ƒä»¬åœ¨é•¿å°¾åˆ†å¸ƒä¸‹çš„é€‚ç”¨ç¨‹åº¦ã€‚</li>
<li>results: è®ºæ–‡å‘ç°ï¼Œåœ¨é•¿å°¾åˆ†å¸ƒä¸‹ï¼Œéçº¿æ€§åˆ†ç±»å™¨èƒ½å¤Ÿå¯¹æ–°æ•°æ®è¿›è¡Œæ›´å¥½çš„é€‚åº”ï¼Œè€Œçº¿æ€§åˆ†ç±»å™¨åˆ™æ— æ³•é™ä½ä¸€å®šæ°´å¹³çš„æ™®éåŒ–é”™è¯¯ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å‘ç°ï¼Œå½“å°¾éƒ¨çš„é•¿åº¦å˜çŸ­æ—¶ï¼Œä¸¤ç§æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·å¯ä»¥é™ä½ã€‚<details>
<summary>Abstract</summary>
We suggest a simple Gaussian mixture model for data generation that complies with Feldman's long tail theory (2020). We demonstrate that a linear classifier cannot decrease the generalization error below a certain level in the proposed model, whereas a nonlinear classifier with a memorization capacity can. This confirms that for long-tailed distributions, rare training examples must be considered for optimal generalization to new data. Finally, we show that the performance gap between linear and nonlinear models can be lessened as the tail becomes shorter in the subpopulation frequency distribution, as confirmed by experiments on synthetic and real data.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬å»ºè®®ä¸€ä¸ªç®€å•çš„ Gaussian æ··åˆæ¨¡å‹æ¥ç”Ÿæˆæ•°æ®ï¼Œéµå¾ª Feldman çš„é•¿å°¾ç†è®ºï¼ˆ2020ï¼‰ã€‚æˆ‘ä»¬æ˜¾ç¤ºäº†ä¸€ä¸ªçº¿æ€§åˆ†ç±»å™¨æ— æ³•åœ¨ææ¡ˆçš„æ¨¡å‹ä¸­é™ä½æ³›åŒ–è¯¯å·®ä¸‹é™ï¼Œè€Œä¸€ä¸ªå…·æœ‰è®°å¿†å®¹é‡çš„éçº¿æ€§åˆ†ç±»å™¨åˆ™å¯ä»¥ã€‚è¿™è¯å®äº†å¯¹é•¿å°¾åˆ†å¸ƒçš„æ•°æ®ï¼Œç½•è§çš„è®­ç»ƒç¤ºä¾‹å¿…é¡»è¢«è€ƒè™‘æ¥å–å¾—æœ€ä½³çš„æ³›åŒ–è‡³æ–°æ•°æ®ã€‚æœ€åï¼Œæˆ‘ä»¬æ˜¾ç¤ºäº†çº¿æ€§å’Œéçº¿æ€§æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·å¯ä»¥éšç€å°¾éƒ¨çš„çŸ­åŒ–è€Œå‡å°‘ï¼Œé€šè¿‡å®éªŒè¯æ˜åœ¨ sintetic å’Œå®é™…æ•°æ®ä¸Šã€‚
</details></li>
</ul>
<hr>
<h2 id="Comparison-between-transformers-and-convolutional-models-for-fine-grained-classification-of-insects"><a href="#Comparison-between-transformers-and-convolutional-models-for-fine-grained-classification-of-insects" class="headerlink" title="Comparison between transformers and convolutional models for fine-grained classification of insects"></a>Comparison between transformers and convolutional models for fine-grained classification of insects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11112">http://arxiv.org/abs/2307.11112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rita Pucci, Vincent J. Kalkman, Dan Stowell</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æé«˜æ˜†è™«ç§ç±»çš„è‡ªåŠ¨åˆ†ç±»ç²¾åº¦ï¼Œå°¤å…¶æ˜¯åœ¨åŒä¸€ä¸ªåˆ†ç±»ç±»å‹ä¸‹çš„ç§ç±» diferenciaciÃ³nã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†æ·±åº¦å­¦ä¹ ç®—æ³•ï¼Œç‰¹åˆ«æ˜¯ transformer å’Œ convolutional å±‚ç»“æ„ï¼Œè¿›è¡Œæ¯”è¾ƒç ”ç©¶ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œhybrid æ¨¡å‹åœ¨å‡†ç¡®æ€§å’Œæ‰§è¡Œé€Ÿåº¦ä¸¤ä¸ªæ–¹é¢å‡æœ‰ä¼˜å¼‚è¡¨ç°ï¼Œè€Œ transformer æ¨¡å‹åœ¨æ ·æœ¬ç¼ºä¹æ—¶å…·æœ‰æ›´é«˜çš„æ‰§è¡Œé€Ÿåº¦ã€‚<details>
<summary>Abstract</summary>
Fine-grained classification is challenging due to the difficulty of finding discriminatory features. This problem is exacerbated when applied to identifying species within the same taxonomical class. This is because species are often sharing morphological characteristics that make them difficult to differentiate. We consider the taxonomical class of Insecta. The identification of insects is essential in biodiversity monitoring as they are one of the inhabitants at the base of many ecosystems. Citizen science is doing brilliant work of collecting images of insects in the wild giving the possibility to experts to create improved distribution maps in all countries. We have billions of images that need to be automatically classified and deep neural network algorithms are one of the main techniques explored for fine-grained tasks. At the SOTA, the field of deep learning algorithms is extremely fruitful, so how to identify the algorithm to use? We focus on Odonata and Coleoptera orders, and we propose an initial comparative study to analyse the two best-known layer structures for computer vision: transformer and convolutional layers. We compare the performance of T2TViT, a fully transformer-base, EfficientNet, a fully convolutional-base, and ViTAE, a hybrid. We analyse the performance of the three models in identical conditions evaluating the performance per species, per morph together with sex, the inference time, and the overall performance with unbalanced datasets of images from smartphones. Although we observe high performances with all three families of models, our analysis shows that the hybrid model outperforms the fully convolutional-base and fully transformer-base models on accuracy performance and the fully transformer-base model outperforms the others on inference speed and, these prove the transformer to be robust to the shortage of samples and to be faster at inference time.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç»†è‡´åˆ†ç±»é—®é¢˜å›°éš¾ï¼Œä¸»è¦å› ä¸ºæ‰¾åˆ°åˆ†åŒ–ç‰¹å¾å›°éš¾ã€‚å½“åº”ç”¨åˆ°åŒä¸€ç§æ¤ç‰©ç±»ä¸­çš„ç‰©ç§è¯†åˆ«æ—¶ï¼Œè¿™é—®é¢˜æ›´åŠ å›°éš¾ã€‚è¿™æ˜¯å› ä¸ºç‰©ç§ç»å¸¸å…±äº« morphological ç‰¹å¾ï¼Œä½¿å…¶å›°éš¾åˆ† differentiateã€‚æˆ‘ä»¬è€ƒè™‘ insecta çº²ã€‚è¯†åˆ«æ˜†è™«å¯¹ç”Ÿç‰©å¤šæ ·æ€§ç›‘æµ‹è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒä»¬æ˜¯ç”Ÿæ€ç³»ç»Ÿçš„åŸºç¡€å±…æ°‘ã€‚å…¬æ°‘ç§‘å­¦åœ¨é‡å¤–é‡‡é›†æ˜†è™«å›¾åƒï¼Œæä¾›äº†ä¸“å®¶åˆ›å»ºæ”¹è¿›çš„åˆ†å¸ƒå›¾çš„æœºä¼šã€‚æˆ‘ä»¬æœ‰æ•°åäº¿ä¸ªå›¾åƒéœ€è¦è‡ªåŠ¨åˆ†ç±»ï¼Œæ·±åº¦å­¦ä¹ ç®—æ³•æ˜¯ç»†è‡´ä»»åŠ¡ä¸­çš„ä¸»è¦æŠ€æœ¯ä¹‹ä¸€ã€‚åœ¨ SOTA ä¸­ï¼Œæ·±åº¦å­¦ä¹ é¢†åŸŸéå¸¸è‚¥æ²ƒï¼Œå› æ­¤å¦‚ä½•é€‰æ‹©ç®—æ³•ï¼Ÿæˆ‘ä»¬å°†å…³æ³¨ Odonata å’Œ Coleoptera ä¸¤ä¸ªç›®ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆæ­¥æ¯”è¾ƒç ”ç©¶ï¼Œæ£€éªŒäº†ä¸¤ç§æœ€ä¸ºçŸ¥åçš„è®¡ç®—æœºè§†è§‰å±‚ç»“æ„ï¼šå˜æ¢å±‚å’Œå·ç§¯å±‚ã€‚æˆ‘ä»¬æ¯”è¾ƒäº† T2TViTã€EfficientNet å’Œ ViTAE ä¸‰ç§æ¨¡å‹çš„æ€§èƒ½ï¼Œåˆ†æäº†æ¯ç§ç‰©ç§ã€æ¯ç§å½¢æ€ã€æ€§åˆ«ã€æ¨ç†æ—¶é—´å’Œæ€»æ€§èƒ½ã€‚è™½ç„¶æˆ‘ä»¬æ‰€è§‚å¯Ÿåˆ°çš„æ€§èƒ½å¾ˆé«˜ï¼Œä½†æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œæ··åˆæ¨¡å‹åœ¨å‡†ç¡®æ€§å’Œæ¨ç†é€Ÿåº¦ä¸¤ä¸ªæ–¹é¢éƒ½æœ‰ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œå®Œå…¨è½¬æ¢åŸºç¡€æ¨¡å‹åœ¨å‡†ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè€Œå®Œå…¨å·ç§¯åŸºç¡€æ¨¡å‹åœ¨æ¨ç†é€Ÿåº¦æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚è¿™è¯æ˜äº†è½¬æ¢å±‚åœ¨æ ·æœ¬ç¼ºä¹æ—¶å…·æœ‰ç¨³å®šæ€§å’Œå¿«é€Ÿæ¨ç†èƒ½åŠ›ã€‚
</details></li>
</ul>
<hr>
<h2 id="LLM-Censorship-A-Machine-Learning-Challenge-or-a-Computer-Security-Problem"><a href="#LLM-Censorship-A-Machine-Learning-Challenge-or-a-Computer-Security-Problem" class="headerlink" title="LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?"></a>LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10719">http://arxiv.org/abs/2307.10719</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Glukhov, Ilia Shumailov, Yarin Gal, Nicolas Papernot, Vardan Papyan</li>
<li>For: The paper discusses the risks of malicious use of large language models (LLMs) and the limitations of existing defense mechanisms, such as model fine-tuning or output censorship.* Methods: The paper presents theoretical limitations of semantic censorship approaches, highlighting the inherent challenges in censoring LLM outputs due to their programmatic and instruction-following capabilities.* Results: The paper argues that the challenges of censorship extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones, and proposes that the problem of censorship should be reevaluated and treated as a security problem to mitigate potential risks.Here is the same information in Simplified Chinese text:* For: è®ºæ–‡æ¢è®¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯èƒ½çš„æ¶ç”¨è¡Œä¸ºé£é™©ï¼Œä»¥åŠç°æœ‰çš„é˜²å¾¡æœºåˆ¶çš„ä¸è¶³ã€‚* Methods: è®ºæ–‡æè¿°äº†semantic censorshipçš„ç†è®ºé™åˆ¶ï¼Œå¼ºè°ƒ LLM çš„ç¨‹åºåŒ–å’ŒæŒ‡ä»¤æ‰§è¡Œèƒ½åŠ›ä½¿å¾— censored è¾“å‡ºä»ç„¶å­˜åœ¨é—®é¢˜ã€‚* Results: è®ºæ–‡è®¤ä¸ºï¼Œ censored è¾“å‡ºçš„é—®é¢˜ä¸ä»…é™äºsemantic censorshipï¼Œæ”»å‡»è€…å¯ä»¥é€šè¿‡æ”¶é›† permissible è¾“å‡ºæ¥é‡å»ºä¸å½“è¾“å‡ºï¼Œtherefore, the problem of censorship should be reevaluated and treated as a security problem to mitigate potential risks.<details>
<summary>Abstract</summary>
Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, as LLMs can still generate problematic responses. Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs. In this paper, we present the theoretical limitations of such semantic censorship approaches. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities. Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones. As a result, we propose that the problem of censorship needs to be reevaluated; it should be treated as a security problem which warrants the adaptation of security-based approaches to mitigate potential risks.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Differences-Between-Hard-and-Noisy-labeled-Samples-An-Empirical-Study"><a href="#Differences-Between-Hard-and-Noisy-labeled-Samples-An-Empirical-Study" class="headerlink" title="Differences Between Hard and Noisy-labeled Samples: An Empirical Study"></a>Differences Between Hard and Noisy-labeled Samples: An Empirical Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10718">http://arxiv.org/abs/2307.10718</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mahf93/hard-vs-noisy">https://github.com/mahf93/hard-vs-noisy</a></li>
<li>paper_authors: Mahsa Forouzesh, Patrick Thiran</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³éš¾åº¦å¼ºã€æ ‡ç­¾é”™è¯¯çš„æ ·æœ¬é›†åˆä¸­çš„å™ªå£°æ ·æœ¬é—®é¢˜ã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç³»ç»Ÿæ€§çš„å®éªŒæ–¹æ³•ï¼Œç”¨äºåˆ†æå›°éš¾æ ·æœ¬å’Œå™ªå£°æ ‡ç­¾æ ·æœ¬ä¹‹é—´çš„ç›¸ä¼¼æ€§å’Œ diferenciasã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ç®€å• yet effectiveçš„åº¦é‡ï¼Œå¯ä»¥ä»å™ªå£°æ ‡ç­¾æ ·æœ¬ä¸­ç­›é€‰å‡ºå™ªå£°æ ·æœ¬ï¼Œä¿ç•™å›°éš¾æ ·æœ¬ã€‚</li>
<li>results: æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨æˆ‘ä»¬æå‡ºçš„åº¦é‡ç­›é€‰å‡ºå™ªå£°æ ‡ç­¾æ ·æœ¬åï¼Œæ¨¡å‹åœ¨è®­ç»ƒè¿‡åçš„æµ‹è¯•ç²¾åº¦å¾—åˆ°äº†æœ€é«˜çš„æå‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ•°æ®åˆ†é…æ–¹æ³•åœ¨å®é™…ä¸–ç•Œä¸­å­˜åœ¨æ ‡ç­¾å™ªå£°æ—¶ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚<details>
<summary>Abstract</summary>
Extracting noisy or incorrectly labeled samples from a labeled dataset with hard/difficult samples is an important yet under-explored topic. Two general and often independent lines of work exist, one focuses on addressing noisy labels, and another deals with hard samples. However, when both types of data are present, most existing methods treat them equally, which results in a decline in the overall performance of the model. In this paper, we first design various synthetic datasets with custom hardness and noisiness levels for different samples. Our proposed systematic empirical study enables us to better understand the similarities and more importantly the differences between hard-to-learn samples and incorrectly-labeled samples. These controlled experiments pave the way for the development of methods that distinguish between hard and noisy samples. Through our study, we introduce a simple yet effective metric that filters out noisy-labeled samples while keeping the hard samples. We study various data partitioning methods in the presence of label noise and observe that filtering out noisy samples from hard samples with this proposed metric results in the best datasets as evidenced by the high test accuracy achieved after models are trained on the filtered datasets. We demonstrate this for both our created synthetic datasets and for datasets with real-world label noise. Furthermore, our proposed data partitioning method significantly outperforms other methods when employed within a semi-supervised learning framework.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>>æŒ‰ç…§ä»¥ä¸‹è½¬æ¢è§„åˆ™å°†æ–‡æœ¬ç¿»è¯‘æˆç®€åŒ–ä¸­æ–‡ï¼š1. å°†"noisy"å’Œ"incorrectly"æ›¿æ¢ä¸º"å™ªéŸ³"å’Œ"é”™è¯¯"ã€‚2. å°†"samples"æ›¿æ¢ä¸º"æ ·æœ¬"ã€‚3. å°†"hard"å’Œ"difficult"æ›¿æ¢ä¸º"å›°éš¾"ã€‚4. å°†"labels"æ›¿æ¢ä¸º"æ ‡ç­¾"ã€‚5. å°†"existing"æ›¿æ¢ä¸º"ç°æœ‰"ã€‚6. å°†"systematic"æ›¿æ¢ä¸º"ç³»ç»Ÿçš„"ã€‚7. å°†"empirical"æ›¿æ¢ä¸º"å®é™…çš„"ã€‚8. å°†"controlled"æ›¿æ¢ä¸º"æ§åˆ¶çš„"ã€‚9. å°†"filter"æ›¿æ¢ä¸º"ç­›é€‰"ã€‚10. å°†"partitions"æ›¿æ¢ä¸º"åˆ†åŒº"ã€‚è½¬æ¢åçš„æ–‡æœ¬å¦‚ä¸‹ï¼šEXTRACTING NOISY OR INCORRECTLY LABELED SAMPLES FROM A LABELED DATASET WITH HARD/DIFFICULT SAMPLES IS AN IMPORTANT YET UNDERE-EXPLORED TOPIC. TWO GENERAL AND OFTEN INDEPENDENT LINES OF WORK EXIST, ONE FOCUSES ON ADDRESSING NOISY LABELS, AND ANOTHER DEALS WITH HARD SAMPLES. HOWEVER, WHEN BOTH TYPES OF DATA ARE PRESENT, MOST EXISTING METHODS TREAT THEM EQUALLY, WHICH RESULTS IN A DECLINE IN THE OVERALL PERFORMANCE OF THE MODEL. IN THIS PAPER, WE FIRST DESIGN VARIOUS SYNTHETIC DATASETS WITH CUSTOM HARDNESS AND NOISINESS LEVELS FOR DIFFERENT SAMPLES. OUR PROPOSED SYSTEMATIC EMPIRICAL STUDY ENABLES US TO BETTER UNDERSTAND THE SIMILARITIES AND MORE IMPORTANTLY THE DIFFERENCES BETWEEN HARD-TO-LEARN SAMPLES AND INCORRECTLY LABELED SAMPLES. THESE CONTROLLED EXPERIMENTS PAVE THE WAY FOR THE DEVELOPMENT OF METHODS THAT DISTINGUISH BETWEEN HARD AND NOISY SAMPLES. THROUGH OUR STUDY, WE INTRODUCE A SIMPLE YET EFFECTIVE METRIC THAT FILTERS OUT NOISY-LABELED SAMPLES WHILE KEEPING THE HARD SAMPLES. WE STUDY VARIOUS DATA PARTITIONING METHODS IN THE PRESENCE OF LABEL NOISE AND OBSERVE THAT FILTERING OUT NOISY SAMPLES FROM HARD SAMPLES WITH THIS PROPOSED METRIC RESULTS IN THE BEST DATASETS AS EVIDENCED BY THE HIGH TEST ACCURACY ACHIEVED AFTER MODELS ARE TRAINED ON THE FILTERED DATASETS. WE DEMONSTRATE THIS FOR BOTH OUR CREATED SYNTHETIC DATASETS AND FOR DATASETS WITH REAL-WORLD LABEL NOISE. FURTHERMORE, OUR PROPOSED DATA PARTITIONING METHOD SIGNIFICANTLY OUTPERFORMS OTHER METHODS WHEN EMPLOYED WITHIN A SEMI-SUPERVISED LEARNING FRAMEWORK.
</details></li>
</ul>
<hr>
<h2 id="AdjointDPM-Adjoint-Sensitivity-Method-for-Gradient-Backpropagation-of-Diffusion-Probabilistic-Models"><a href="#AdjointDPM-Adjoint-Sensitivity-Method-for-Gradient-Backpropagation-of-Diffusion-Probabilistic-Models" class="headerlink" title="AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models"></a>AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10711">http://arxiv.org/abs/2307.10711</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiachun Pan, Jun Hao Liew, Vincent Y. F. Tan, Jiashi Feng, Hanshu Yan</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨Addressing the challenge of diffusion probabilistic model (DPM) customization when only available supervision is a differentiable metric defined on the generated contents.</li>
<li>methods: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³• called AdjointDPMï¼Œå®ƒé¦–å…ˆä½¿ç”¨æ‰©å±•ODEæ¥ç”Ÿæˆæ–°çš„æ ·æœ¬ï¼Œç„¶åä½¿ç”¨é€†å˜æ•°æ³•æ¥å½’å¯¼æŸå¤±çš„æ¢¯åº¦åˆ°æ¨¡å‹å‚æ•°ï¼ˆåŒ…æ‹¬conditioningä¿¡å·ã€ç½‘ç»œå‚æ•°å’Œåˆå§‹å™ªå£°ï¼‰ã€‚</li>
<li>results: è¯¥è®ºæ–‡é€šè¿‡ä¸‰ä¸ªæœ‰è¶£çš„ä»»åŠ¡æ¥è¯æ˜AdjointDPMçš„æ•ˆæœï¼šå°†è§†è§‰ç‰¹æ•ˆè½¬æ¢ä¸ºæ ‡è¯†ç åµŒå…¥ï¼Œfinetune DPMs Ğ´Ğ»Ñç‰¹å®šç±»å‹çš„é£æ ¼åŒ–ï¼Œä»¥åŠä¼˜åŒ–åˆå§‹å™ªå£°ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ Ğ´Ğ»Ñå®‰å…¨å®¡æ ¸ã€‚<details>
<summary>Abstract</summary>
Existing customization methods require access to multiple reference examples to align pre-trained diffusion probabilistic models (DPMs) with user-provided concepts. This paper aims to address the challenge of DPM customization when the only available supervision is a differentiable metric defined on the generated contents. Since the sampling procedure of DPMs involves recursive calls to the denoising UNet, na\"ive gradient backpropagation requires storing the intermediate states of all iterations, resulting in extremely high memory consumption. To overcome this issue, we propose a novel method AdjointDPM, which first generates new samples from diffusion models by solving the corresponding probability-flow ODEs. It then uses the adjoint sensitivity method to backpropagate the gradients of the loss to the models' parameters (including conditioning signals, network weights, and initial noises) by solving another augmented ODE. To reduce numerical errors in both the forward generation and gradient backpropagation processes, we further reparameterize the probability-flow ODE and augmented ODE as simple non-stiff ODEs using exponential integration. Finally, we demonstrate the effectiveness of AdjointDPM on three interesting tasks: converting visual effects into identification text embeddings, finetuning DPMs for specific types of stylization, and optimizing initial noise to generate adversarial samples for security auditing.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç°æœ‰çš„è‡ªå®šä¹‰æ–¹æ³•éœ€è¦è®¿é—®å¤šä¸ªå‚è€ƒç¤ºä¾‹ï¼Œä»¥å°†é¢„è®­ç»ƒçš„æ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDPMï¼‰ä¸ç”¨æˆ·æä¾›çš„æ¦‚å¿µè¿›è¡Œå¯¹æ¥ã€‚è¿™ç¯‡è®ºæ–‡ç›®æ ‡æ˜¯è§£å†³DPMè‡ªå®šä¹‰æ—¶ï¼Œåªæœ‰ç”¨æˆ·æä¾›çš„å¯å¾®åˆ†åº¦é‡è¡¨ç¤ºçš„æŒ‘æˆ˜ã€‚å› ä¸ºæ‰©æ•£è¿‡ç¨‹ä¸­çš„DPMé‡‡æ ·è¿‡ç¨‹ involve recursive calls to the denoising UNetï¼Œç›´è§‚æ¢¯åº¦åpropagationéœ€è¦å­˜å‚¨æ‰€æœ‰è¿­ä»£è¿‡ç¨‹çš„é—´æ¥çŠ¶æ€ï¼Œå¯¼è‡´å†…å­˜æ¶ˆè€—æé«˜ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•AdjointDPMã€‚é¦–å…ˆï¼ŒAdjointDPMä½¿ç”¨æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆæ–°çš„æ ·æœ¬ï¼Œç„¶åä½¿ç”¨é€†æ•£å°„æ•æ„Ÿåº¦æ³•æ¥åpropagateæŸå¤±çš„æ¢¯åº¦åˆ°æ¨¡å‹å‚æ•°ï¼ˆåŒ…æ‹¬conditioningä¿¡å·ã€ç½‘ç»œå‚æ•°å’Œåˆå§‹å™ªéŸ³ï¼‰ã€‚ä¸ºäº†å‡å°‘åœ¨å‰å‘ç”Ÿæˆå’Œåpropagationè¿‡ç¨‹ä¸­çš„æ•°å€¼é”™è¯¯ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å°†æ¦‚ç‡æµODEå’Œæ‰©å……ODEè½¬æ¢ä¸ºç®€å•çš„éç¡¬å¼ODEï¼Œå¹¶ä½¿ç”¨åŠ é€Ÿçš„exponential integrationã€‚æœ€åï¼Œæˆ‘ä»¬ç¤ºä¾‹äº†AdjointDPMåœ¨ä¸‰ä¸ªæœ‰è¶£çš„ä»»åŠ¡ä¸Šçš„æ•ˆæœï¼šå°†è§†è§‰ç‰¹æ•ˆè½¬æ¢ä¸ºæ ‡è¯†ç åµŒå…¥ï¼Œfinetune DPMs for specific types of stylizationï¼Œå’Œä¼˜åŒ–åˆå§‹å™ªéŸ³ä»¥ç”Ÿæˆå®‰å…¨å®¡æ ¸ä¸­çš„æ•Œæ„æ ·æœ¬ã€‚
</details></li>
</ul>
<hr>
<h2 id="Reparameterized-Policy-Learning-for-Multimodal-Trajectory-Optimization"><a href="#Reparameterized-Policy-Learning-for-Multimodal-Trajectory-Optimization" class="headerlink" title="Reparameterized Policy Learning for Multimodal Trajectory Optimization"></a>Reparameterized Policy Learning for Multimodal Trajectory Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10710">http://arxiv.org/abs/2307.10710</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haosulab/RPG">https://github.com/haosulab/RPG</a></li>
<li>paper_authors: Zhiao Huang, Litian Liang, Zhan Ling, Xuanlin Li, Chuang Gan, Hao Su</li>
<li>for: æœ¬ç ”ç©¶çš„ç›®æ ‡æ˜¯è§£å†³é«˜ç»´è¿ç»­åŠ¨ä½œç©ºé—´ä¸­RLæ”¿ç­–å‚æ•°åŒ–çš„æŒ‘æˆ˜ã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸåˆ™æ­£çš„æ¡†æ¶ï¼Œå°†è¿ç»­RLæ”¿ç­–è§†ä¸ºç¯å¢ƒä¼˜è´¨è½¨è¿¹çš„ç”Ÿæˆæ¨¡å‹ã€‚é€šè¿‡å°†æ”¿ç­– conditional onä¸€ä¸ªéšè—å˜é‡ï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸€ç§æ–°çš„å¯å˜ boundsï¼Œå®ƒé¼“åŠ±ç¯å¢ƒæ¢ç´¢ã€‚</li>
<li>results: æˆ‘ä»¬æå‡ºäº†ä¸€ç§å®ç”¨çš„æ¨¡å‹åŸºäºRLæ–¹æ³•ï¼Œcalled Reparameterized Policy Gradient (RPG)ï¼Œå®ƒåˆ©ç”¨å¤šæ¨¡æ€æ”¿ç­–å‚æ•°åŒ–å’Œå­¦ä¹ çš„ä¸–ç•Œæ¨¡å‹æ¥å®ç°å¼ºå¤§çš„æ¢ç´¢èƒ½åŠ›å’Œé«˜æ•°æ®æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å¸®åŠ©ä»£ç†äººé¿å…ç¯å¢ƒä¸­çš„å±€éƒ¨ä¼˜ç‚¹ï¼Œè§£å†³æ‚è´¨å¥–åŠ±ç¯å¢ƒï¼Œå¹¶åœ¨å„ç§ä»»åŠ¡ä¸­è¾¾åˆ°ä¼˜ç§€çš„æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
We investigate the challenge of parametrizing policies for reinforcement learning (RL) in high-dimensional continuous action spaces. Our objective is to develop a multimodal policy that overcomes limitations inherent in the commonly-used Gaussian parameterization. To achieve this, we propose a principled framework that models the continuous RL policy as a generative model of optimal trajectories. By conditioning the policy on a latent variable, we derive a novel variational bound as the optimization objective, which promotes exploration of the environment. We then present a practical model-based RL method, called Reparameterized Policy Gradient (RPG), which leverages the multimodal policy parameterization and learned world model to achieve strong exploration capabilities and high data efficiency. Empirical results demonstrate that our method can help agents evade local optima in tasks with dense rewards and solve challenging sparse-reward environments by incorporating an object-centric intrinsic reward. Our method consistently outperforms previous approaches across a range of tasks. Code and supplementary materials are available on the project page https://haosulab.github.io/RPG/
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬ç ”ç©¶äº†é‡ parametrization policies çš„æŒ‘æˆ˜ï¼Œåœ¨é«˜ç»´è¿ç»­è¡Œä¸ºç©ºé—´ä¸­è¿›è¡Œå­¦ä¹ åé¦ˆå­¦ä¹ ï¼ˆRLï¼‰ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¼€å‘ä¸€ä¸ªå¤šæ¨¡æ€ç­–ç•¥ï¼Œè¶…è¶Šå¸¸ç”¨çš„ Gaussian å‚æ•°åŒ–çš„é™åˆ¶ã€‚ä¸º Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒè¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸåˆ™çš„æ¡†æ¶ï¼Œå°†è¿ç»­RLç­–ç•¥æ¨¡å‹ä¸ºä¼˜è´¨è½¨è¿¹ç”Ÿæˆæ¨¡å‹ã€‚é€šè¿‡å¯¹ç­–ç•¥è¿›è¡Œå—é™å˜é‡æ¡ä»¶ï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸€ç§æ–°çš„variational boundï¼Œä½œä¸ºä¼˜åŒ–ç›®æ ‡ï¼Œè¿™ä¼šä¿ƒè¿›ç¯å¢ƒçš„æ¢ç´¢ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å®ç”¨çš„åŸºäºæ¨¡å‹çš„RLæ–¹æ³•ï¼Œcalled Reparameterized Policy Gradientï¼ˆRPGï¼‰ï¼Œåˆ©ç”¨å¤šæ¨¡æ€ç­–ç•¥å‚æ•°åŒ–å’Œå­¦ä¹ çš„ä¸–ç•Œæ¨¡å‹ï¼Œä»¥å®ç°å¼ºå¤§çš„æ¢ç´¢èƒ½åŠ›å’Œé«˜æ•°æ®æ•ˆç‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¯ä»¥å¸®åŠ©ä»£ç†äººé¿å…ç¯å¢ƒä¸­çš„åœ°æ–¹æœ€ä¼˜åŒ–ï¼Œå¹¶é€šè¿‡åµŒå…¥ç‰©ä½“å†…éƒ¨çš„å†…åœ¨å¥–åŠ±æ¥è§£å†³ç¨€æ‚‰å¥–åŠ±ç¯å¢ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªä»»åŠ¡ä¸­æ¯”å‰ä¸€äº›æ–¹æ³•è¡¨ç°å‡ºè‰²ï¼Œä»£ç å’Œè¡¥å……ææ–™å¯ä»¥åœ¨é¡¹ç›®é¡µé¢https://haosulab.github.io/RPG/ ä¸­æ‰¾åˆ°ã€‚
</details></li>
</ul>
<hr>
<h2 id="TwinLiteNet-An-Efficient-and-Lightweight-Model-for-Driveable-Area-and-Lane-Segmentation-in-Self-Driving-Cars"><a href="#TwinLiteNet-An-Efficient-and-Lightweight-Model-for-Driveable-Area-and-Lane-Segmentation-in-Self-Driving-Cars" class="headerlink" title="TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and Lane Segmentation in Self-Driving Cars"></a>TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and Lane Segmentation in Self-Driving Cars</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10705">http://arxiv.org/abs/2307.10705</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chequanghuy/TwinLiteNet">https://github.com/chequanghuy/TwinLiteNet</a></li>
<li>paper_authors: Quang Huy Che, Dinh Phuc Nguyen, Minh Quan Pham, Duc Khai Lam</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æå‡ºä¸€ç§è½»é‡çº§çš„æ¨¡å‹ï¼Œç”¨äºè‡ªåŠ¨é©¾é©¶è½¦è¾†ç¯å¢ƒç†è§£çš„é©±åŠ¨åŒºåŸŸå’Œè½¦é“çº¿åˆ†å‰²ã€‚</li>
<li>methods: è¯¥æ¨¡å‹åŸºäºTwinLiteNetæ–¹æ³•ï¼Œå®ƒæ˜¯ä¸€ç§å»‰ä»·çš„æ¨¡å‹ï¼Œä½†å…·æœ‰é«˜å‡†ç¡®ç‡å’Œé«˜æ•ˆæ€§ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼ŒTwinLiteNetåœ¨BDD100Kæ•°æ®é›†ä¸Š achievement mIoUåˆ†æ•°ä¸º91.3%ï¼Œå¯¹æ¯”ç°æœ‰æ–¹æ³•ç›¸å¯¹è¾ƒé«˜ï¼Œä¸”å…·æœ‰è¾ƒå°‘çš„è®¡ç®—èµ„æºéœ€æ±‚ã€‚ Codeå¯ä»¥åœ¨url{<a target="_blank" rel="noopener" href="https://github.com/chequanghuy/TwinLiteNet%7D%E4%B8%AD%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/chequanghuy/TwinLiteNet}ä¸­è·å–ã€‚</a><details>
<summary>Abstract</summary>
Semantic segmentation is a common task in autonomous driving to understand the surrounding environment. Driveable Area Segmentation and Lane Detection are particularly important for safe and efficient navigation on the road. However, original semantic segmentation models are computationally expensive and require high-end hardware, which is not feasible for embedded systems in autonomous vehicles. This paper proposes a lightweight model for the driveable area and lane line segmentation. TwinLiteNet is designed cheaply but achieves accurate and efficient segmentation results. We evaluate TwinLiteNet on the BDD100K dataset and compare it with modern models. Experimental results show that our TwinLiteNet performs similarly to existing approaches, requiring significantly fewer computational resources. Specifically, TwinLiteNet achieves a mIoU score of 91.3% for the Drivable Area task and 31.08% IoU for the Lane Detection task with only 0.4 million parameters and achieves 415 FPS on GPU RTX A5000. Furthermore, TwinLiteNet can run in real-time on embedded devices with limited computing power, especially since it achieves 60FPS on Jetson Xavier NX, making it an ideal solution for self-driving vehicles. Code is available: url{https://github.com/chequanghuy/TwinLiteNet}.
</details>
<details>
<summary>æ‘˜è¦</summary>
Semantic segmentation æ˜¯è‡ªåŠ¨é©¾é©¶ä¸­å¸¸è§çš„ä»»åŠ¡ï¼Œç”¨äºç†è§£å‘¨å›´ç¯å¢ƒã€‚é©±åŠ¨åŒºåŸŸåˆ†å‰²å’Œè½¦é“æ£€æµ‹æ˜¯å®‰å…¨å’Œé«˜æ•ˆå¯¼èˆªçš„å…³é”®ï¼Œä½†åŸå§‹semantic segmentationæ¨¡å‹ computationally expensiveå’Œéœ€è¦é«˜çº§ç¡¬ä»¶ï¼Œä¸é€‚åˆè‡ªåŠ¨é©¾é©¶è½¦è¾†ä¸­çš„åµŒå…¥å¼ç³»ç»Ÿã€‚è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§çš„æ¨¡å‹ï¼Œç”¨äºé©±åŠ¨åŒºåŸŸå’Œè½¦é“åˆ†å‰²ã€‚TwinLiteNetæ˜¯ä¸€ç§ä¾¿å®œçš„è®¾è®¡ï¼Œä½†å®ƒå¯ä»¥å®ç°é«˜åº¦å‡†ç¡®å’Œé«˜é€Ÿçš„åˆ†å‰²ç»“æœã€‚æˆ‘ä»¬åœ¨BDD100Kæ•°æ®é›†ä¸Šè¯„ä¼°TwinLiteNetï¼Œå¹¶ä¸ç°ä»£æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„TwinLiteNetä¸ç°æœ‰æ–¹æ³•ç›¸ä¼¼ï¼Œéœ€è¦ significatively fewer computational resourcesã€‚å…·ä½“æ¥è¯´ï¼ŒTwinLiteNetåœ¨é©±åŠ¨åŒºåŸŸä»»åŠ¡ä¸­ achieves mIoU åˆ†æ•°ä¸º 91.3%ï¼Œå¹¶åœ¨è½¦é“æ£€æµ‹ä»»åŠ¡ä¸­ achieves IoU åˆ†æ•°ä¸º 31.08%ï¼Œä»…ä½¿ç”¨ 0.4 ä¸‡ä¸ªå‚æ•°ã€‚æ­¤å¤–ï¼ŒTwinLiteNetå¯ä»¥åœ¨å®æ—¶ä¸­åœ¨æœ‰é™çš„è®¡ç®—èƒ½åŠ›çš„åµŒå…¥å¼è®¾å¤‡ä¸Šè¿è¡Œï¼Œç‰¹åˆ«æ˜¯åœ¨ Jetson Xavier NX ä¸Š achieve 60 FPSï¼Œä½¿å…¶æˆä¸ºè‡ªåŠ¨é©¾é©¶è½¦è¾†çš„ç†æƒ³è§£å†³æ–¹æ¡ˆã€‚ä»£ç å¯ä»¥åœ¨ä»¥ä¸‹é“¾æ¥ä¸­æ‰¾åˆ°ï¼š<https://github.com/chequanghuy/TwinLiteNet>ã€‚
</details></li>
</ul>
<hr>
<h2 id="Decentralized-Smart-Charging-of-Large-Scale-EVs-using-Adaptive-Multi-Agent-Multi-Armed-Bandits"><a href="#Decentralized-Smart-Charging-of-Large-Scale-EVs-using-Adaptive-Multi-Agent-Multi-Armed-Bandits" class="headerlink" title="Decentralized Smart Charging of Large-Scale EVs using Adaptive Multi-Agent Multi-Armed Bandits"></a>Decentralized Smart Charging of Large-Scale EVs using Adaptive Multi-Agent Multi-Armed Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10704">http://arxiv.org/abs/2307.10704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sharyal Zafar, RaphaÃ«l Feraud, Anne Blavette, Guy Camilleri, Hamid Ben</li>
<li>for: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§å®Œå…¨åˆ†æ•£å¼çš„å……ç”µç³»ç»Ÿï¼Œä»¥è§£å†³ç”µåŠ¨è½¦å……ç”µè¿‡è½½å’Œç”µå‹é™åˆ¶é—®é¢˜ã€‚</li>
<li>methods: è¯¥ç³»ç»Ÿé‡‡ç”¨äº†è‡ªé€‚åº”å¤šä»£ç†ç³»ç»Ÿå“²å­¦ï¼Œå¹¶ä½¿ç”¨å¤šè‡‚æŠ•æ·å­¦æ¥å¤„ç†ç³»ç»Ÿä¸ç¡®å®šæ€§ã€‚</li>
<li>results:  caso studyè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿå…·æœ‰åˆ†æ•£å¼ã€æ‰©å±•æ€§ã€å®æ—¶æ€§ã€æ— æ¨¡å‹åŸºç¡€å’Œå…¬å¹³æ€§ç­‰ç‰¹ç‚¹ã€‚<details>
<summary>Abstract</summary>
The drastic growth of electric vehicles and photovoltaics can introduce new challenges, such as electrical current congestion and voltage limit violations due to peak load demands. These issues can be mitigated by controlling the operation of electric vehicles i.e., smart charging. Centralized smart charging solutions have already been proposed in the literature. But such solutions may lack scalability and suffer from inherent drawbacks of centralization, such as a single point of failure, and data privacy concerns. Decentralization can help tackle these challenges. In this paper, a fully decentralized smart charging system is proposed using the philosophy of adaptive multi-agent systems. The proposed system utilizes multi-armed bandit learning to handle uncertainties in the system. The presented system is decentralized, scalable, real-time, model-free, and takes fairness among different players into account. A detailed case study is also presented for performance evaluation.
</details>
<details>
<summary>æ‘˜è¦</summary>
electric vehicles å’Œ photovoltaics çš„å¿«é€Ÿå¢é•¿å¯èƒ½ä¼šå¼•å…¥æ–°çš„æŒ‘æˆ˜ï¼Œå¦‚ç”µæµæ‹¥å µå’Œç”µå‹é™åˆ¶ç”±å³°å€¼è´Ÿè·å¸¦æ¥ã€‚è¿™äº›é—®é¢˜å¯ä»¥é€šè¿‡æ™ºèƒ½å……ç”µæ§åˆ¶æ¥ç¼“è§£ã€‚æ–‡ç« ä¸­å·²ç»æå‡ºäº†ä¸­å¤®åŒ–æ™ºèƒ½å……ç”µè§£å†³æ–¹æ¡ˆã€‚ä½†è¿™äº›è§£å†³æ–¹æ¡ˆå¯èƒ½ç¼ºä¹æ‰©å±•æ€§å’Œä¸­å¤®åŒ–çš„ç¼ºç‚¹ï¼Œå¦‚å”¯ä¸€ç‚¹å¤±è´¥å’Œæ•°æ®éšç§é—®é¢˜ã€‚åˆ†æ•£åŒ–å¯ä»¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå®Œå…¨åˆ†æ•£å¼æ™ºèƒ½å……ç”µç³»ç»Ÿï¼Œä½¿ç”¨é€‚åº”å¤šä»£ç†ç³»ç»Ÿçš„å“²å­¦ã€‚è¯¥ç³»ç»Ÿä½¿ç”¨å¤šè‡‚æŠ•æ·å­¦æ¥å¤„ç†ç³»ç»Ÿä¸ç¡®å®šæ€§ã€‚æå‡ºçš„ç³»ç»Ÿæ˜¯åˆ†æ•£å¼ã€å¯æ‰©å±•ã€å®æ—¶ã€æ¨¡å‹è‡ªç”±ã€å…·æœ‰å…¬å¹³æ€§çš„ã€‚è¿˜æä¾›äº†ä¸€ä¸ªè¯¦ç»†çš„æ¡ˆä¾‹ç ”ç©¶ä»¥è¯„ä¼°æ€§èƒ½ã€‚
</details></li>
</ul>
<hr>
<h2 id="Graphs-in-State-Space-Models-for-Granger-Causality-in-Climate-Science"><a href="#Graphs-in-State-Space-Models-for-Granger-Causality-in-Climate-Science" class="headerlink" title="Graphs in State-Space Models for Granger Causality in Climate Science"></a>Graphs in State-Space Models for Granger Causality in Climate Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10703">http://arxiv.org/abs/2307.10703</a></li>
<li>repo_url: None</li>
<li>paper_authors: VÃ­ctor Elvira, Ã‰milie Chouzenoux, Jordi CerdÃ , Gustau Camps-Valls</li>
<li>for: è¯„ä¼°æ—¶é—´åºåˆ—ä¹‹é—´çš„é¢„æµ‹å¯èƒ½æ€§</li>
<li>methods: ä½¿ç”¨å›¾ematrixæ¨¡å‹å’Œlassoæ­£åˆ™åŒ–</li>
<li>results: æé«˜äº†å¯¹æ ‡å‡†Granger causalityæ–¹æ³•çš„æ¯”è¾ƒHereâ€™s a more detailed explanation of each point:</li>
<li>for: The paper is written to assess the predictability of time series from another time series using Granger causality, which is a widely used method in many applied disciplines.</li>
<li>methods: The paper uses a graphical perspective of state-space models and a recently presented expectation-maximization algorithm called GraphEM to estimate the linear matrix operator in the state equation of a linear-Gaussian state-space model. Lasso regularization is included in the M-step, which is solved using a proximal splitting Douglas-Rachford algorithm.</li>
<li>results: The proposed model and inference technique are demonstrated to have benefits over standard Granger causality methods through experiments in toy examples and challenging climate problems.<details>
<summary>Abstract</summary>
Granger causality (GC) is often considered not an actual form of causality. Still, it is arguably the most widely used method to assess the predictability of a time series from another one. Granger causality has been widely used in many applied disciplines, from neuroscience and econometrics to Earth sciences. We revisit GC under a graphical perspective of state-space models. For that, we use GraphEM, a recently presented expectation-maximisation algorithm for estimating the linear matrix operator in the state equation of a linear-Gaussian state-space model. Lasso regularisation is included in the M-step, which is solved using a proximal splitting Douglas-Rachford algorithm. Experiments in toy examples and challenging climate problems illustrate the benefits of the proposed model and inference technique over standard Granger causality methods.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ ¼å…°æ²» causalityï¼ˆGCï¼‰frequently è¢«è§†ä¸ºä¸æ˜¯å®é™…çš„ causalityã€‚ç„¶è€Œï¼Œå®ƒä»ç„¶æ˜¯æœ€å¹¿æ³›ä½¿ç”¨çš„æ–¹æ³•æ¥è¯„ä¼°æ—¶é—´åºåˆ—ä¹‹é—´çš„é¢„æµ‹æ€§ã€‚æ ¼å…°æ²» causality åœ¨å„ç§åº”ç”¨é¢†åŸŸä¸­å¹¿æ³›ä½¿ç”¨ï¼Œä»ç¥ç»ç§‘å­¦å’Œç»æµç»Ÿè®¡åˆ°åœ°çƒç§‘å­¦ã€‚æˆ‘ä»¬åœ¨å›¾è¡¨è§†è§’ä¸‹é‡æ–°å®¡è§†GCã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨ GraphEMï¼Œä¸€ç§æœ€è¿‘æå‡ºçš„æœŸæœ›æœ€å¤§åŒ–ç®—æ³•æ¥ä¼°è®¡çŠ¶æ€æ–¹ç¨‹ä¸­çš„çº¿æ€§çŸ©é˜µè¿ç®—ç¬¦ã€‚lasso è§„èŒƒåŒ–åŒ…æ‹¬åœ¨M-stepä¸­ï¼Œä½¿ç”¨è·ç¦» Douglas-Rachford ç®—æ³•è§£å†³ã€‚å®éªŒåœ¨å°ä¾‹å­å’ŒæŒ‘æˆ˜æ€§æ°”å€™é—®é¢˜ä¸­ILLUSTRATE æˆ‘ä»¬æè®®çš„æ¨¡å‹å’Œæ¨ç†æ–¹æ³•çš„ä¼˜åŠ¿äºæ ‡å‡†Granger causalityæ–¹æ³•ã€‚
</details></li>
</ul>
<hr>
<h2 id="Self2Self-Single-Image-Denoising-with-Self-Supervised-Learning-and-Image-Quality-Assessment-Loss"><a href="#Self2Self-Single-Image-Denoising-with-Self-Supervised-Learning-and-Image-Quality-Assessment-Loss" class="headerlink" title="Self2Self+: Single-Image Denoising with Self-Supervised Learning and Image Quality Assessment Loss"></a>Self2Self+: Single-Image Denoising with Self-Supervised Learning and Image Quality Assessment Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10695">http://arxiv.org/abs/2307.10695</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JK-the-Ko/Self2SelfPlus">https://github.com/JK-the-Ko/Self2SelfPlus</a></li>
<li>paper_authors: Jaekyun Ko, Sanghwan Lee</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æå‡ºä¸€ç§åŸºäºå•ä¸ªå™ªå£°å›¾åƒçš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œä»¥ä¾¿æé«˜å™ªå£°é™¤å»æ•ˆæœçš„å¯è¡Œæ€§å’Œå®ç”¨æ€§ã€‚</li>
<li>methods: è¯¥æ–¹æ³•ä½¿ç”¨é˜»å¡å·ç§¯æ¥æå–ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨æ— å‚å›¾è´¨é‡è¯„ä¼°æ¥å¯¼èˆªè®­ç»ƒè¿‡ç¨‹ã€‚å¦å¤–ï¼Œæ–¹æ³•è¿˜ä½¿ç”¨bernoullié‡‡æ ·å’Œdropoutæ¥éšæœº samplingå®ä¾‹ä»è¾“å…¥å›¾åƒé›†ï¼Œä»¥æé«˜è®­ç»ƒæ•ˆæœã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„æ–¹æ³•åœ¨ sintetic å’Œå®é™… dataset ä¸Šè¾¾åˆ°äº†å½“å‰æœ€ä½³çš„å™ªå£°é™¤å»æ€§èƒ½ã€‚è¿™è¡¨æ˜äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ï¼Œå¯èƒ½ç”¨äºå„ç§å™ªå£°é™¤å»ä»»åŠ¡ã€‚<details>
<summary>Abstract</summary>
Recently, denoising methods based on supervised learning have exhibited promising performance. However, their reliance on external datasets containing noisy-clean image pairs restricts their applicability. To address this limitation, researchers have focused on training denoising networks using solely a set of noisy inputs. To improve the feasibility of denoising procedures, in this study, we proposed a single-image self-supervised learning method in which only the noisy input image is used for network training. Gated convolution was used for feature extraction and no-reference image quality assessment was used for guiding the training process. Moreover, the proposed method sampled instances from the input image dataset using Bernoulli sampling with a certain dropout rate for training. The corresponding result was produced by averaging the generated predictions from various instances of the trained network with dropouts. The experimental results indicated that the proposed method achieved state-of-the-art denoising performance on both synthetic and real-world datasets. This highlights the effectiveness and practicality of our method as a potential solution for various noise removal tasks.
</details>
<details>
<summary>æ‘˜è¦</summary>
æœ€è¿‘ï¼ŒåŸºäºè¶…çº§visedå­¦ä¹ çš„å‡å™ªæ–¹æ³•è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¾èµ–äºå¤–éƒ¨ dataset ä¸­çš„å™ªéŸ³-æ¸…æ´å›¾åƒå¯¹ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„åº”ç”¨åœºæ™¯ã€‚ä¸ºè§£å†³è¿™äº› limitationï¼Œç ”ç©¶äººå‘˜å¯¹å‡å™ªç½‘ç»œçš„è®­ç»ƒè¿›è¡Œäº†æ›´å¤šçš„å…³æ³¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä½¿ç”¨å•ä¸€é™æ­¢è¾“å…¥å›¾åƒè¿›è¡Œç½‘ç»œè®­ç»ƒçš„è‡ªæˆ‘ç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚ä½¿ç”¨äº†æ‰©å±• convolution æ¥æŠ½å–ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨æ— å‚å›¾åƒè´¨é‡è¯„ä¼°æ¥å¼•å¯¼è®­ç»ƒè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨ Bernoulli æŠ½æ ·æ¥ä»è¾“å…¥å›¾åƒé›†ä¸­éšæœºé€‰æ‹©å®ä¾‹è¿›è¡Œè®­ç»ƒã€‚é€šè¿‡ averaging ç”Ÿæˆçš„å¤šä¸ªå®ä¾‹çš„ predicatesï¼Œæˆ‘ä»¬å¾—åˆ°äº†æœ€ç»ˆçš„ç»“æœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ synthetic å’Œå®é™…ä¸–ç•Œ dataset ä¸Šå®ç°äº† state-of-the-art çš„å‡å™ªæ€§èƒ½ã€‚è¿™è¡¨æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡å™ªä»»åŠ¡ä¸­çš„å®ç”¨æ€§å’Œå¯è¡Œæ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Fractional-Denoising-for-3D-Molecular-Pre-training"><a href="#Fractional-Denoising-for-3D-Molecular-Pre-training" class="headerlink" title="Fractional Denoising for 3D Molecular Pre-training"></a>Fractional Denoising for 3D Molecular Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10683">http://arxiv.org/abs/2307.10683</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fengshikun/frad">https://github.com/fengshikun/frad</a></li>
<li>paper_authors: Shikun Feng, Yuyan Ni, Yanyan Lan, Zhi-Ming Ma, Wei-Ying Ma</li>
<li>for: æé«˜3Dåˆ†å­é¢„è®­ç»ƒæ–¹æ³•çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯ç‰©æœå¯»ä»»åŠ¡ä¸­ã€‚</li>
<li>methods: æå‡ºäº†ä¸€ç§æ–°çš„æ··åˆå™ªå£°ç­–ç•¥ï¼ŒåŒ…æ‹¬å¯¹ dip è§’å’Œåæ ‡è¿›è¡Œå™ªå£°å¤„ç†ã€‚åŒæ—¶ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„åˆ†æ•°å™ªå£°å¤„ç†æ–¹æ³•ï¼ˆFradï¼‰ï¼Œå¯ä»¥æ›´å¥½åœ°é€‚åº”åˆ†å­çš„ ani sowropic ç‰¹å¾ã€‚</li>
<li>results: å®éªŒè¡¨æ˜ï¼ŒFrad å¯ä»¥æ›´å¥½åœ°æé«˜åˆ†å­è¡¨ç¤ºæ€§ï¼Œå¹¶åœ¨ 9 ä¸ª QM9 ä»»åŠ¡å’Œ 7 ä¸ª MD17 ä»»åŠ¡ä¸­è¾¾åˆ°æ–°çš„é¡¶å³°æ€§ã€‚<details>
<summary>Abstract</summary>
Coordinate denoising is a promising 3D molecular pre-training method, which has achieved remarkable performance in various downstream drug discovery tasks. Theoretically, the objective is equivalent to learning the force field, which is revealed helpful for downstream tasks. Nevertheless, there are two challenges for coordinate denoising to learn an effective force field, i.e. low coverage samples and isotropic force field. The underlying reason is that molecular distributions assumed by existing denoising methods fail to capture the anisotropic characteristic of molecules. To tackle these challenges, we propose a novel hybrid noise strategy, including noises on both dihedral angel and coordinate. However, denoising such hybrid noise in a traditional way is no more equivalent to learning the force field. Through theoretical deductions, we find that the problem is caused by the dependency of the input conformation for covariance. To this end, we propose to decouple the two types of noise and design a novel fractional denoising method (Frad), which only denoises the latter coordinate part. In this way, Frad enjoys both the merits of sampling more low-energy structures and the force field equivalence. Extensive experiments show the effectiveness of Frad in molecular representation, with a new state-of-the-art on 9 out of 12 tasks of QM9 and on 7 out of 8 targets of MD17.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œåæ ‡å¹²æ‰°æ˜¯ä¸€ç§æœ‰å‰é€”çš„3Dåˆ†å­é¢„è®­æ–¹æ³•ï¼Œå®ƒåœ¨ä¸åŒçš„ä¸‹æ¸¸è¯ç‰©æ¢ç´¢ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ç†è®ºä¸Šï¼Œè¿™ä¸ªç›®æ ‡ç­‰äºå­¦ä¹ åŠ›åœºï¼Œè¿™æ˜¯å¯¹ä¸‹æ¸¸ä»»åŠ¡æœ‰å¸®åŠ©çš„ã€‚ç„¶è€Œï¼Œåæ ‡å¹²æ‰°å­¦ä¹ ä¸€ä¸ªæœ‰æ•ˆçš„åŠ›åœºå—åˆ°ä¸¤ä¸ªæŒ‘æˆ˜ï¼Œå³ä½è¦†ç›–æ ·æœ¬å’Œå„å‘åŒæ€§åŠ›åœºã€‚è¿™äº›æŒ‘æˆ˜çš„æ ¹æœ¬åŸå› æ˜¯ç°æœ‰çš„åæ ‡å¹²æ‰°æ–¹æ³•å¯¹åˆ†å­çš„åˆ†å¸ƒå‡è®¾ä¸èƒ½å¤Ÿæ•æ‰åˆ†å­çš„éå¯¹ç§°ç‰¹å¾ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ··åˆå™ªéŸ³ç­–ç•¥ï¼ŒåŒ…æ‹¬åæ ‡å’Œæ–¹å‘å¤¹è§’çš„å™ªéŸ³ã€‚ä½†æ˜¯ï¼Œå°†è¿™ç§æ··åˆå™ªéŸ³è¿›è¡Œä¼ ç»Ÿçš„å™ªéŸ³é™¤æ³•ä¸å†ç­‰äºå­¦ä¹ åŠ›åœºã€‚ç»è¿‡ç†è®ºçš„æ¨å¯¼ï¼Œæˆ‘ä»¬å‘ç°è¿™ä¸ªé—®é¢˜æ˜¯å› ä¸ºè¾“å…¥æ„é€ çš„å‡è®¾æ‰€å¯¼è‡´çš„ã€‚ä¸ºäº†è§£å†³è¿™é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åˆ†è§£æ–¹æ³•ï¼ˆFradï¼‰ï¼Œå®ƒåªå¯¹åè€…çš„åæ ‡éƒ¨åˆ†è¿›è¡Œå™ªéŸ³é™¤æ³•ã€‚è¿™æ ·ï¼ŒFradå¯ä»¥åŒæ—¶å…·æœ‰è¾ƒä½çš„èƒ½é‡ç»“æ„å’ŒåŠ›åœºç­‰äºæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºFradåœ¨åˆ†å­è¡¨ç°æ–¹é¢æœ‰æ–°çš„é¡¶å³°æ€§ï¼Œåœ¨QM9å’ŒMD17ä¸Šåˆ†åˆ«å–å¾—9/12å’Œ7/8çš„æ–°çºªå½•ã€‚â€
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-for-classification-of-noisy-QR-codes"><a href="#Deep-learning-for-classification-of-noisy-QR-codes" class="headerlink" title="Deep learning for classification of noisy QR codes"></a>Deep learning for classification of noisy QR codes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10677">http://arxiv.org/abs/2307.10677</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rebecca Leygonie, Sylvain Lobry, ), Laurent Wendling (LIPADE)</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨å®šä¹‰åŸºäºæ·±åº¦å­¦ä¹ çš„å¤å…¸åˆ†ç±»æ¨¡å‹åœ¨æŠ½è±¡å›¾åƒä¸Šçš„é™åˆ¶ï¼Œå½“åº”ç”¨äºä¸å¯è§åŒ–å¯¹è±¡çš„å›¾åƒã€‚</li>
<li>methods: æˆ‘ä»¬ä½¿ç”¨äº†åŸºäºæ·±åº¦å­¦ä¹ çš„å›¾åƒåˆ†ç±»æ¨¡å‹ï¼Œå¹¶å¯¹QRç ï¼ˆå¿«é€Ÿå“åº”ç ï¼‰è¿›è¡Œäº†è®­ç»ƒã€‚QRç ä¸æ˜¯ä¸ºäººç±»æ‰‹åŠ¨è¯»å–è€Œè®¾è®¡çš„ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥é€šè¿‡å¯¹QRç ç”Ÿæˆçš„ä¿¡æ¯è¿›è¡Œåˆ†æï¼Œäº†è§£æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨æŠ½è±¡å›¾åƒåˆ†ç±»ä¸­çš„é™åˆ¶ã€‚</li>
<li>results: æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æ¨¡å‹å¯ä»¥åœ¨æŠ½è±¡å›¾åƒåˆ†ç±»ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨å™ªå£°å­˜åœ¨çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å¤Ÿä¿æŒä¸€å®šçš„ç¨³å®šæ€§ã€‚è¿™é¡¹ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æ¨¡å‹å¯ä»¥åœ¨ç†è§£æŠ½è±¡å›¾åƒæ–¹é¢å‘æŒ¥ä½œç”¨ã€‚<details>
<summary>Abstract</summary>
We wish to define the limits of a classical classification model based on deep learning when applied to abstract images, which do not represent visually identifiable objects.QR codes (Quick Response codes) fall into this category of abstract images: one bit corresponding to one encoded character, QR codes were not designed to be decoded manually. To understand the limitations of a deep learning-based model for abstract image classification, we train an image classification model on QR codes generated from information obtained when reading a health pass. We compare a classification model with a classical (deterministic) decoding method in the presence of noise. This study allows us to conclude that a model based on deep learning can be relevant for the understanding of abstract images.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬æƒ³å®šä¹‰æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨æŠ½è±¡å›¾åƒåˆ†ç±»ä¸­çš„é™åˆ¶ï¼Œå½“åº”ç”¨äºä¸å¯è¯†åˆ«çš„è§†è§‰å¯¹è±¡ã€‚äºŒç»´ç ï¼ˆå¿«é€Ÿå“åº”ç ï¼‰æ˜¯è¿™ç±»æŠ½è±¡å›¾åƒçš„ä¸€ä¸ªä¾‹å­ï¼šæ¯ä¸€æ¯”ç‰¹å¯¹åº”ä¸€ä¸ªç¼–ç å­—ç¬¦ï¼ŒäºŒç»´ç ä¸æ˜¯ä¸ºäººç±»æ‰‹åŠ¨è§£ç ã€‚é€šè¿‡åœ¨å¥åº·é€šè¡Œè¯ä¸­è¯»å–çš„ä¿¡æ¯ç”Ÿæˆçš„äºŒç»´ç ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªå›¾åƒåˆ†ç±»æ¨¡å‹ï¼Œå¹¶ä¸ç»å…¸ï¼ˆæŸç¼šï¼‰è§£ç æ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œä»¥è¯„ä¼°æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨æŠ½è±¡å›¾åƒåˆ†ç±»ä¸­çš„å±€é™æ€§ã€‚è¿™é¡¹ç ”ç©¶å…è®¸æˆ‘ä»¬ conclued that deep learningæ¨¡å‹å¯¹æŠ½è±¡å›¾åƒæœ‰ relevanceã€‚
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-What-to-Share-in-Federated-Learning-Perspectives-on-Model-Utility-Privacy-Leakage-and-Communication-Efficiency"><a href="#A-Survey-of-What-to-Share-in-Federated-Learning-Perspectives-on-Model-Utility-Privacy-Leakage-and-Communication-Efficiency" class="headerlink" title="A Survey of What to Share in Federated Learning: Perspectives on Model Utility, Privacy Leakage, and Communication Efficiency"></a>A Survey of What to Share in Federated Learning: Perspectives on Model Utility, Privacy Leakage, and Communication Efficiency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10655">http://arxiv.org/abs/2307.10655</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiawei Shao, Zijian Li, Wenqiang Sun, Tailin Zhou, Yuchang Sun, Lumin Liu, Zehong Lin, Jun Zhang</li>
<li>for: è¿™ä¸ªè®ºæ–‡ä¸»è¦æ˜¯ä¸ºäº†æ¢è®¨ Federated Learningï¼ˆFLï¼‰ä¸­å¯ä»¥å…±åŒåŸ¹è®­å¤šä¸ªå‚ä¸è€…çš„æ–¹æ³•ï¼Œä»¥ä¿æŠ¤æ•°æ®éšç§ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡åˆ†æäº†ä¸åŒç±»å‹çš„å…±äº«æ–¹æ³•ï¼ŒåŒ…æ‹¬æ¨¡å‹å…±äº«ã€syntheticæ•°æ®å…±äº«å’ŒçŸ¥è¯†å…±äº«ã€‚</li>
<li>results: è®ºæ–‡é€šè¿‡å¯¹ä¸åŒå…±äº«æ–¹æ³•çš„æ€§èƒ½å’Œé€šä¿¡å¼€é”€è¿›è¡Œæ¯”è¾ƒï¼Œä»¥åŠå¯¹æ¨¡å‹æ³„éœ²å’Œä¼šå‘˜æ¨æµ‹æ”»å‡»çš„è¯„ä¼°ï¼Œæä¾›äº†ä¸€äº›ç»“è®ºã€‚<details>
<summary>Abstract</summary>
Federated learning (FL) has emerged as a highly effective paradigm for privacy-preserving collaborative training among different parties. Unlike traditional centralized learning, which requires collecting data from each party, FL allows clients to share privacy-preserving information without exposing private datasets. This approach not only guarantees enhanced privacy protection but also facilitates more efficient and secure collaboration among multiple participants. Therefore, FL has gained considerable attention from researchers, promoting numerous surveys to summarize the related works. However, the majority of these surveys concentrate on methods sharing model parameters during the training process, while overlooking the potential of sharing other forms of local information. In this paper, we present a systematic survey from a new perspective, i.e., what to share in FL, with an emphasis on the model utility, privacy leakage, and communication efficiency. This survey differs from previous ones due to four distinct contributions. First, we present a new taxonomy of FL methods in terms of the sharing methods, which includes three categories of shared information: model sharing, synthetic data sharing, and knowledge sharing. Second, we analyze the vulnerability of different sharing methods to privacy attacks and review the defense mechanisms that provide certain privacy guarantees. Third, we conduct extensive experiments to compare the performance and communication overhead of various sharing methods in FL. Besides, we assess the potential privacy leakage through model inversion and membership inference attacks, while comparing the effectiveness of various defense approaches. Finally, we discuss potential deficiencies in current methods and outline future directions for improvement.
</details>
<details>
<summary>æ‘˜è¦</summary>
å—é¢†å­¦ä¹ ï¼ˆFederated Learningï¼ŒFLï¼‰å·²ç»å‡ºç°ä¸ºä¿æŒéšç§çš„é«˜æ•ˆæ¨¡å¼ï¼Œå¤šä¸ªæ–¹é¢å…±åŒè®­ç»ƒä¸åŒçš„æ•°æ®é›†ã€‚ä¸ä¼ ç»Ÿä¸­å¤®å­¦ä¹ ä¸åŒï¼ŒFL å…è®¸å®¢æˆ·ç«¯å…±äº«éšç§ä¿æŠ¤çš„ä¿¡æ¯è€Œä¸æ›å…‰ç§æœ‰æ•°æ®é›†ã€‚è¿™ç§æ–¹æ³•ä¸ä»…ä¿è¯äº†æ›´å¥½çš„éšç§ä¿æŠ¤ï¼Œè¿˜ä¿ƒè¿›äº†å¤šä¸ªå‚ä¸è€…ä¹‹é—´æ›´åŠ æœ‰æ•ˆå’Œå®‰å…¨çš„åä½œã€‚å› æ­¤ï¼ŒFL å·²ç»å¸å¼•äº†å¹¿æ³›çš„ç ”ç©¶äººå‘˜ï¼Œä¿ƒä½¿äº†è®¸å¤šç›¸å…³çš„è¯„ä¼°æ–‡ç« ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°è¿™äº›è¯„ä¼°æ–‡ç« éƒ½é›†ä¸­åœ¨å…±äº«æ¨¡å‹å‚æ•° durante el proceso de entrenamientoï¼Œè€Œå¿½ç•¥äº†å…¶ä»–å½¢å¼çš„æœ¬åœ°ä¿¡æ¯çš„å…±äº«çš„æ½œåœ¨ä»·å€¼ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç³»ç»Ÿçš„è¯„ä¼°ï¼Œå³åœ¨ FL ä¸­ä½•æ—¶å…±äº«ä»€ä¹ˆï¼Œå¹¶å¼ºè°ƒæ¨¡å‹çš„å®ç”¨æ€§ã€éšç§æ³„éœ²å’Œé€šä¿¡æ•ˆç‡ã€‚è¿™ç§è¯„ä¼°ä¸ä»¥å¾€ä¸åŒï¼Œä¸»è¦ç”±ä»¥ä¸‹å››ä¸ªç‰¹ç‚¹ï¼š1. æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„åˆ†ç±»æ–¹æ³•ï¼Œå°† FL æ–¹æ³•åˆ†ä¸ºä¸‰ç±»å…±äº«ä¿¡æ¯ï¼šæ¨¡å‹å…±äº«ã€åˆæˆæ•°æ®å…±äº«å’ŒçŸ¥è¯†å…±äº«ã€‚2. æˆ‘ä»¬åˆ†æäº†ä¸åŒå…±äº«æ–¹æ³•çš„éšç§æ”»å‡»çš„æ•æ„Ÿæ€§ï¼Œå¹¶è¯„ä¼°äº†æä¾›ä¸€å®šéšç§ä¿è¯çš„é˜²å¾¡æœºåˆ¶ã€‚3. æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œæ¯”è¾ƒä¸åŒå…±äº«æ–¹æ³•åœ¨ FL ä¸­çš„æ€§èƒ½å’Œé€šä¿¡å¼€é”€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯„ä¼°äº†æ¨¡å‹åå‘æ³„éœ²å’Œæˆå‘˜èº«ä»½æ”»å‡»çš„é£é™©ï¼Œå¹¶æ¯”è¾ƒäº†ä¸åŒé˜²å¾¡ç­–ç•¥çš„æ•ˆæœã€‚4. æˆ‘ä»¬è®¨è®ºäº†å½“å‰æ–¹æ³•çš„ç¼ºé™·å’Œæœªæ¥æ–¹å‘çš„æ”¹è¿›ã€‚æ€»ä¹‹ï¼Œæœ¬æ–‡æä¾›äº†ä¸€ä¸ªç³»ç»Ÿçš„è¯„ä¼°ï¼Œå¸®åŠ©è¯»è€…æ›´å¥½åœ°ç†è§£ FL ä¸­å…±äº«å“ªäº›ä¿¡æ¯ï¼Œä»¥åŠè¿™äº›ä¿¡æ¯åœ¨æ¨¡å‹å®ç”¨æ€§ã€éšç§æ³„éœ²å’Œé€šä¿¡æ•ˆç‡æ–¹é¢çš„å½±å“ã€‚
</details></li>
</ul>
<hr>
<h2 id="Conditional-expectation-network-for-SHAP"><a href="#Conditional-expectation-network-for-SHAP" class="headerlink" title="Conditional expectation network for SHAP"></a>Conditional expectation network for SHAP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10654">http://arxiv.org/abs/2307.10654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ronald Richman, Mario V. WÃ¼thrich</li>
<li>for: è¿™ä¸ªç ”ç©¶æ—¨åœ¨æå‡ºä¸€ç§èƒ½å¤Ÿæœ‰æ•ˆåœ°è®¡ç®—Conditional SHAPå€¼çš„ç¥ç»ç½‘ç»œæ–¹æ³•ï¼Œä»¥ä¾¿åœ¨ç¥ç»ç½‘ç»œå’Œå…¶ä»–å›å½’æ¨¡å‹ä¸­ä½¿ç”¨ã€‚</li>
<li>methods: è¿™ç§æ–¹æ³•ä½¿ç”¨äº†SHAPæŠ€æœ¯ï¼Œå¹¶ä¸”ç‰¹åˆ«è€ƒè™‘äº†ç‰¹å¾ç»„ä»¶ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚</li>
<li>results: è¿™ç§æ–¹æ³•å¯ä»¥å‡†ç¡®åœ°è®¡ç®—Conditional SHAPå€¼ï¼Œå¹¶ä¸”å¯ä»¥æä¾›drop1å’Œanovaåˆ†æï¼Œä»¥åŠä¸€ç§è€ƒè™‘ç‰¹å¾ç»„ä»¶ä¹‹é—´ä¾èµ–å…³ç³»çš„PDPå›¾åƒã€‚<details>
<summary>Abstract</summary>
A very popular model-agnostic technique for explaining predictive models is the SHapley Additive exPlanation (SHAP). The two most popular versions of SHAP are a conditional expectation version and an unconditional expectation version (the latter is also known as interventional SHAP). Except for tree-based methods, usually the unconditional version is used (for computational reasons). We provide a (surrogate) neural network approach which allows us to efficiently calculate the conditional version for both neural networks and other regression models, and which properly considers the dependence structure in the feature components. This proposal is also useful to provide drop1 and anova analyses in complex regression models which are similar to their generalized linear model (GLM) counterparts, and we provide a partial dependence plot (PDP) counterpart that considers the right dependence structure in the feature components.
</details>
<details>
<summary>æ‘˜è¦</summary>
éå¸¸æµè¡Œçš„æ¨¡å‹æ— å…³æŠ€æœ¯ä¹‹ä¸€æ˜¯SHapley Additive exPlanationï¼ˆSHAPï¼‰ã€‚è¿™ä¸¤ä¸ªç‰ˆæœ¬æ˜¯æ¡ä»¶é¢„æœŸç‰ˆæœ¬å’Œæ— æ¡ä»¶é¢„æœŸç‰ˆæœ¬ï¼ˆåè€…ä¹Ÿç§°ä¸ºäº¤äº’SHAPï¼‰ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œä½¿ç”¨æ— æ¡ä»¶ç‰ˆæœ¬ï¼ˆç”±äºè®¡ç®—åŸå› ï¼‰ã€‚æˆ‘ä»¬æä¾›ä¸€ç§ï¼ˆä»£ç†ï¼‰ç¥ç»ç½‘ç»œæ–¹æ³•ï¼Œå¯ä»¥é«˜æ•ˆè®¡ç®—æ¡ä»¶ç‰ˆæœ¬ï¼Œå¹¶ä¸”æ­£ç¡®è€ƒè™‘ç‰¹å¾ç»„ä»¶ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚è¿™ç§æè®®è¿˜æœ‰åŠ©äºæä¾›drop1å’Œanovaåˆ†æåœ¨å¤æ‚å›å½’æ¨¡å‹ä¸­ï¼Œä¸å…¶æ™®é€šçº¿æ€§æ¨¡å‹ï¼ˆGLMï¼‰å¯¹åº”çš„åˆ†æã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ä¸€ç§è€ƒè™‘ç‰¹å¾ç»„ä»¶ä¾èµ–å…³ç³»çš„PDPå¯¹åº”ã€‚
</details></li>
</ul>
<hr>
<h2 id="Refining-the-Optimization-Target-for-Automatic-Univariate-Time-Series-Anomaly-Detection-in-Monitoring-Services"><a href="#Refining-the-Optimization-Target-for-Automatic-Univariate-Time-Series-Anomaly-Detection-in-Monitoring-Services" class="headerlink" title="Refining the Optimization Target for Automatic Univariate Time Series Anomaly Detection in Monitoring Services"></a>Refining the Optimization Target for Automatic Univariate Time Series Anomaly Detection in Monitoring Services</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10653">http://arxiv.org/abs/2307.10653</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manqing Dong, Zhanxiang Zhao, Yitong Geng, Wentao Li, Wei Wang, Huai Jiang</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ˜¯ä¸ºäº†æé«˜æ—¶é—´åºåˆ—å¼‚å¸¸æ¢æµ‹çš„è‡ªåŠ¨åŒ–åŒ–ï¼Œä»¥æé«˜å·¥ä¸šç›‘æ§æœåŠ¡çš„å¯é æ€§å’Œç³»ç»Ÿæ€§èƒ½ã€‚</li>
<li>methods: æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„è‡ªåŠ¨åŒ–parameterä¼˜åŒ–æ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªä¼˜åŒ–ç›®æ ‡ï¼šé¢„æµ‹å¾—åˆ†ã€å½¢çŠ¶å¾—åˆ†å’Œæ•æ„Ÿåº¦å¾—åˆ†ï¼Œè¿™äº›ç›®æ ‡å¯ä»¥è½»æ¾åœ°é€‚åº”ä¸åŒçš„æ¨¡å‹åæ®µï¼Œæ— éœ€ä¸“ä¸šçŸ¥è¯†æˆ–æ‰‹åŠ¨æ ‡æ³¨åŠªåŠ›ã€‚</li>
<li>results: æœ¬æ–‡çš„ææ¡ˆæ¡†æ¶å·²ç»åœ¨çº¿ä¸Šè¿›è¡Œäº†è¶…è¿‡å…­ä¸ªæœˆçš„å®é™…åº”ç”¨ï¼Œå¤„ç†äº†æ¯åˆ†é’Ÿ50,000å¤šä¸ªæ—¶é—´åºåˆ—ï¼Œå¹¶ç®€åŒ–äº†ç”¨æˆ·çš„ä½“éªŒï¼Œä»…éœ€è¦æä¾›é¢„æœŸçš„æ•æ„Ÿå€¼ï¼Œå¹¶ä¸”å®ç°äº†é€‚å½“çš„ä¾¦æµ‹ç»“æœã€‚<details>
<summary>Abstract</summary>
Time series anomaly detection is crucial for industrial monitoring services that handle a large volume of data, aiming to ensure reliability and optimize system performance. Existing methods often require extensive labeled resources and manual parameter selection, highlighting the need for automation. This paper proposes a comprehensive framework for automatic parameter optimization in time series anomaly detection models. The framework introduces three optimization targets: prediction score, shape score, and sensitivity score, which can be easily adapted to different model backbones without prior knowledge or manual labeling efforts. The proposed framework has been successfully applied online for over six months, serving more than 50,000 time series every minute. It simplifies the user's experience by requiring only an expected sensitive value, offering a user-friendly interface, and achieving desired detection results. Extensive evaluations conducted on public datasets and comparison with other methods further confirm the effectiveness of the proposed framework.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ—¶åºåˆ—å¼‚å¸¸æ£€æµ‹æ˜¯å¯¹äºå·¥ä¸šç›‘æµ‹æœåŠ¡è€Œè¨€æ˜¯éå¸¸é‡è¦çš„ï¼Œæ—¨åœ¨ç¡®ä¿å¯é æ€§å¹¶ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½ã€‚ç°æœ‰çš„æ–¹æ³•ç»å¸¸éœ€è¦å¤§é‡çš„æ ‡æ³¨èµ„æºå’Œæ‰‹åŠ¨å‚æ•°é€‰æ‹©ï¼Œè¿™é«˜äº®äº†è‡ªåŠ¨åŒ–çš„éœ€æ±‚ã€‚è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªå®Œæ•´çš„è‡ªåŠ¨å‚æ•°ä¼˜åŒ–æ¡†æ¶ Ğ´Ğ»Ñæ—¶åºåˆ—å¼‚å¸¸æ£€æµ‹æ¨¡å‹ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸‰ä¸ªä¼˜åŒ–ç›®æ ‡ï¼šé¢„æµ‹å¾—åˆ†ã€å½¢æ€å¾—åˆ†å’Œæ•æ„Ÿåº¦å¾—åˆ†ï¼Œå¯ä»¥è½»æ¾åœ°é€‚åº”ä¸åŒçš„æ¨¡å‹èƒŒæ™¯è€Œæ— éœ€äº’çŸ¥æˆ–æ‰‹åŠ¨æ ‡æ³¨åŠªåŠ›ã€‚è¯¥æè®®çš„æ¡†æ¶å·²ç»åœ¨çº¿ä¸Šè¿è¡Œäº†è¶…è¿‡å…­ä¸ªæœˆï¼Œå¤„ç†äº†æ¯åˆ†é’Ÿ50,000ä¸ªæ—¶åºåˆ—ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªæ˜“ç”¨çš„ç”¨æˆ·ç•Œé¢ï¼Œä»¥åŠè¾¾åˆ°äº†æ£€æµ‹ç»“æœçš„æ‰€æ±‚çš„ç›®æ ‡ã€‚å¯¹äºå…¬å…±æ•°æ®é›†å’Œå…¶ä»–æ–¹æ³•è¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ï¼Œå¹¶è¯å®äº†æè®®çš„æ•ˆæœã€‚
</details></li>
</ul>
<hr>
<h2 id="Data-Driven-Latency-Probability-Prediction-for-Wireless-Networks-Focusing-on-Tail-Probabilities"><a href="#Data-Driven-Latency-Probability-Prediction-for-Wireless-Networks-Focusing-on-Tail-Probabilities" class="headerlink" title="Data-Driven Latency Probability Prediction for Wireless Networks: Focusing on Tail Probabilities"></a>Data-Driven Latency Probability Prediction for Wireless Networks: Focusing on Tail Probabilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10648">http://arxiv.org/abs/2307.10648</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/samiemostafavi/wireless-pr3d">https://github.com/samiemostafavi/wireless-pr3d</a></li>
<li>paper_authors: Samie Mostafavi, Gourav Prateek Sharma, James Gross</li>
<li>for:  Ensuring end-to-end network latency with extremely high reliability (99.999%) in wireless networks, particularly for cyber-physical systems and human-in-the-loop applications.</li>
<li>methods:  Using state-of-the-art data-driven approaches, such as mixture density networks (MDN) and extreme value mixture models, to predict the tail of the latency distribution and estimate the likelihood of rare latencies conditioned on network parameters.</li>
<li>results:  Benchmarking the proposed approaches using actual latency measurements of IEEE 802.11g (WiFi), commercial private, and a software-defined 5G network to evaluate their sensitivities concerning the tail probabilities.<details>
<summary>Abstract</summary>
With the emergence of new application areas, such as cyber-physical systems and human-in-the-loop applications, there is a need to guarantee a certain level of end-to-end network latency with extremely high reliability, e.g., 99.999%. While mechanisms specified under IEEE 802.1as time-sensitive networking (TSN) can be used to achieve these requirements for switched Ethernet networks, implementing TSN mechanisms in wireless networks is challenging due to their stochastic nature. To conform the wireless link to a reliability level of 99.999%, the behavior of extremely rare outliers in the latency probability distribution, or the tail of the distribution, must be analyzed and controlled. This work proposes predicting the tail of the latency distribution using state-of-the-art data-driven approaches, such as mixture density networks (MDN) and extreme value mixture models, to estimate the likelihood of rare latencies conditioned on the network parameters, which can be used to make more informed decisions in wireless transmission. Actual latency measurements of IEEE 802.11g (WiFi), commercial private and a software-defined 5G network are used to benchmark the proposed approaches and evaluate their sensitivities concerning the tail probabilities.
</details>
<details>
<summary>æ‘˜è¦</summary>
éšç€æ–°çš„åº”ç”¨é¢†åŸŸçš„å‡ºç°ï¼Œå¦‚åŠå¯¼ä½“ç‰©ç†ç³»ç»Ÿå’Œäººå·¥æ™ºèƒ½åº”ç”¨ï¼Œéœ€è¦ä¿è¯ç«¯åˆ°ç«¯ç½‘ç»œå»¶è¿Ÿåœ¨æé«˜å¯é æ€§æ°´å¹³ï¼ˆ99.999%ï¼‰ä¸‹è¾¾æˆã€‚è€Œæ ¹æ®IEEE 802.1asæ—¶é—´æ•æ„Ÿç½‘ç»œï¼ˆTSNï¼‰è§„èŒƒï¼Œå¯ä»¥ç”¨switched Ethernetç½‘ç»œè¾¾åˆ°è¿™äº›è¦æ±‚ã€‚ç„¶è€Œï¼Œåœ¨æ— çº¿ç½‘ç»œä¸­å®æ–½TSNæœºåˆ¶æ˜¯å›°éš¾çš„ï¼Œå› ä¸ºæ— çº¿é“¾è·¯çš„éšæœºæ€§ã€‚ä¸ºäº†ä½¿ wireless link è¾¾åˆ°99.999%çš„å¯é æ€§æ°´å¹³ï¼Œéœ€è¦åˆ†æå’Œæ§åˆ¶æ— çº¿é“¾è·¯çš„å¾ˆå°‘æç«¯å»¶è¿Ÿçš„è¡Œä¸ºï¼Œå³å»¶è¿Ÿåˆ†å¸ƒçš„å°¾éƒ¨ã€‚æœ¬å·¥ä½œæè®®ä½¿ç”¨ç°æœ‰çš„æ•°æ®é©±åŠ¨æ–¹æ³•ï¼Œå¦‚æ··åˆå¯†åº¦ç½‘ç»œï¼ˆMDNï¼‰å’Œæå€¼æ··åˆæ¨¡å‹ï¼Œæ¥ä¼°è®¡å»¶è¿Ÿåˆ†å¸ƒçš„å°¾éƒ¨ï¼Œå¹¶ç”¨æ­¤æ¥ä¼°è®¡conditioned on ç½‘ç»œå‚æ•°çš„ç½•è§å»¶è¿Ÿçš„å¯èƒ½æ€§ã€‚å®é™…æµ‹é‡IEEE 802.11gï¼ˆWiFiï¼‰ã€å•†ä¸šä¸“ç”¨å’Œè½¯ä»¶å®šä¹‰5Gç½‘ç»œçš„å»¶è¿Ÿå€¼ï¼Œç”¨äºè¯„ä¼°å’Œè¯„æµ‹æè®®çš„æ•æ„Ÿç¨‹åº¦ã€‚
</details></li>
</ul>
<hr>
<h2 id="Fisher-Rao-distance-and-pullback-SPD-cone-distances-between-multivariate-normal-distributions"><a href="#Fisher-Rao-distance-and-pullback-SPD-cone-distances-between-multivariate-normal-distributions" class="headerlink" title="Fisher-Rao distance and pullback SPD cone distances between multivariate normal distributions"></a>Fisher-Rao distance and pullback SPD cone distances between multivariate normal distributions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10644">http://arxiv.org/abs/2307.10644</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frank Nielsen</li>
<li>for: è¯¥è®ºæ–‡æ—¨åœ¨å¤„ç†å¤šå˜é‡æ­£æ€åˆ†å¸ƒé›†åˆï¼Œå¦‚æ‰©æ•£tensoræˆåƒã€ç»“æ„tensorè®¡ç®—æœºè§†è§‰ã€é›·è¾¾ä¿¡å·å¤„ç†ã€æœºå™¨å­¦ä¹ ç­‰é¢†åŸŸçš„æ•°æ®é›†ã€‚</li>
<li>methods: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§å¿«é€Ÿå’Œç¨³å®šçš„æ–¹æ³•æ¥ aproximate multivariate normal distributionsçš„Fisher-Raoè·ç¦»ï¼Œä»¥åŠä¸€ç§åŸºäºå‡ ä½•æ˜ å°„çš„æ–¹æ³•æ¥å®šä¹‰æ­£æ€åˆ†å¸ƒä¹‹é—´çš„è·ç¦»ã€‚</li>
<li>results: è¯¥è®ºæ–‡çš„ç»“æœè¡¨æ˜ï¼ŒåŸºäºFisherä¿¡æ¯åº¦é‡çš„Fisher-Raoè·ç¦»å¯ä»¥å¾ˆå¥½åœ°approximate multivariate normal distributionsï¼Œè€Œä¸” Computationally, the pullback Hilbert cone distance is much lighter than the Fisher-Rao distance approximation, since it only requires the extreme minimal and maximal eigenvalues of matrices. In addition, the paper shows how to use these distances in clustering tasks.<details>
<summary>Abstract</summary>
Data sets of multivariate normal distributions abound in many scientific areas like diffusion tensor imaging, structure tensor computer vision, radar signal processing, machine learning, just to name a few. In order to process those normal data sets for downstream tasks like filtering, classification or clustering, one needs to define proper notions of dissimilarities between normals and paths joining them. The Fisher-Rao distance defined as the Riemannian geodesic distance induced by the Fisher information metric is such a principled metric distance which however is not known in closed-form excepts for a few particular cases. In this work, we first report a fast and robust method to approximate arbitrarily finely the Fisher-Rao distance between multivariate normal distributions. Second, we introduce a class of distances based on diffeomorphic embeddings of the normal manifold into a submanifold of the higher-dimensional symmetric positive-definite cone corresponding to the manifold of centered normal distributions. We show that the projective Hilbert distance on the cone yields a metric on the embedded normal submanifold and we pullback that cone distance with its associated straight line Hilbert cone geodesics to obtain a distance and smooth paths between normal distributions. Compared to the Fisher-Rao distance approximation, the pullback Hilbert cone distance is computationally light since it requires to compute only the extreme minimal and maximal eigenvalues of matrices. Finally, we show how to use those distances in clustering tasks.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ•°æ®é›†ä¸­çš„å¤šå˜é‡æ­£æ€åˆ†å¸ƒéå¸¸æ™®éï¼Œä¾‹å¦‚æ‰©æ•£tensoræˆåƒã€ç»“æ„tensorè®¡ç®—æœºè§†è§‰ã€é›·è¾¾ä¿¡å·å¤„ç†ã€æœºå™¨å­¦ä¹ ç­‰ã€‚ä¸ºäº†å¤„ç†è¿™äº›æ­£æ€æ•°æ®é›†ï¼Œéœ€è¦å®šä¹‰é€‚å½“çš„ä¸åŒé‡ zwischen normalså’Œjoinningå®ƒä»¬çš„è·¯å¾„ã€‚ Fisher-Raoè·ç¦»æ˜¯ä¸€ç§åŸç†çš„è·ç¦»åº¦é‡ï¼Œä½†å®ƒæ²¡æœ‰å›ºå®šå½¢å¼ï¼Œé™¤äº†ä¸€äº›ç‰¹æ®Šæƒ…å†µä¹‹å¤–ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆæŠ¥é“äº†ä¸€ç§å¿«é€Ÿå’Œç¨³å®šçš„æ–¹æ³•æ¥ä¼°ç®—æ­£æ€åˆ†å¸ƒä¹‹é—´çš„Fisher-Raoè·ç¦»ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥ä¸€ç±»åŸºäºæ­£æ€æ˜ å°„çš„è·ç¦»ï¼Œè¯¥è·ç¦»åœ¨é«˜ç»´æ­£æ€åŒºåŸŸä¸Šå®šä¹‰äº†ä¸€ä¸ªæ­£æ€åˆ†å¸ƒçš„å­é›†ã€‚æˆ‘ä»¬å±•ç¤ºäº†è¯¥è·ç¦»åœ¨æ˜ å°„åçš„æ­£æ€åˆ†å¸ƒä¸Šæ˜¯ä¸€ä¸ªåº¦é‡ï¼Œå¹¶å°†å…¶pullbackåˆ°æ­£æ€æ˜ å°„ä¸Šï¼Œä»è€Œå¾—åˆ°äº†ä¸€ä¸ªè·ç¦»å’Œæ­£æ€åˆ†å¸ƒä¹‹é—´çš„ç¼“è§£è·¯å¾„ã€‚ä¸Fisher-Raoè·ç¦»ä¼°ç®—ç›¸æ¯”ï¼Œpullback Hilbert coneè·ç¦»æ˜¯è®¡ç®—æ›´è½»é‡çº§çš„ï¼Œå› ä¸ºåªéœ€è®¡ç®—æ­£æ€åˆ†å¸ƒçš„æå°å’Œæœ€å¤§ç‰¹å¾å€¼ã€‚æœ€åï¼Œæˆ‘ä»¬ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨è¿™äº›è·ç¦»åœ¨åˆ† clustering ä»»åŠ¡ä¸­ã€‚
</details></li>
</ul>
<hr>
<h2 id="SciBench-Evaluating-College-Level-Scientific-Problem-Solving-Abilities-of-Large-Language-Models"><a href="#SciBench-Evaluating-College-Level-Scientific-Problem-Solving-Abilities-of-Large-Language-Models" class="headerlink" title="SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models"></a>SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10635">http://arxiv.org/abs/2307.10635</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mandyyyyii/scibench">https://github.com/mandyyyyii/scibench</a></li>
<li>paper_authors: Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R. Loomba, Shichang Zhang, Yizhou Sun, Wei Wang</li>
<li>for: This paper aims to evaluate the reasoning capabilities of large language models (LLMs) on complex scientific problem solving.</li>
<li>methods: The paper introduces an expansive benchmark suite called SciBench, which features two datasets: an open set of collegiate-level scientific problems and a closed set of undergraduate-level exams in computer science and mathematics. The authors evaluate the performance of two representative LLMs with various prompting strategies.</li>
<li>results: The results show that current LLMs have an overall score of merely 35.80% and make ten different types of errors. The authors find that no single prompting strategy significantly outperforms others, and some strategies that improve in certain problem-solving skills result in declines in other skills.<details>
<summary>Abstract</summary>
Recent advances in large language models (LLMs) have demonstrated notable progress on many mathematical benchmarks. However, most of these benchmarks only feature problems grounded in junior and senior high school subjects, contain only multiple-choice questions, and are confined to a limited scope of elementary arithmetic operations. To address these issues, this paper introduces an expansive benchmark suite SciBench that aims to systematically examine the reasoning capabilities required for complex scientific problem solving. SciBench contains two carefully curated datasets: an open set featuring a range of collegiate-level scientific problems drawn from mathematics, chemistry, and physics textbooks, and a closed set comprising problems from undergraduate-level exams in computer science and mathematics. Based on the two datasets, we conduct an in-depth benchmark study of two representative LLMs with various prompting strategies. The results reveal that current LLMs fall short of delivering satisfactory performance, with an overall score of merely 35.80%. Furthermore, through a detailed user study, we categorize the errors made by LLMs into ten problem-solving abilities. Our analysis indicates that no single prompting strategy significantly outperforms others and some strategies that demonstrate improvements in certain problem-solving skills result in declines in other skills. We envision that SciBench will catalyze further developments in the reasoning abilities of LLMs, thereby ultimately contributing to scientific research and discovery.
</details>
<details>
<summary>æ‘˜è¦</summary>
SciBench contains two datasets: an open set featuring collegiate-level scientific problems from mathematics, chemistry, and physics textbooks, and a closed set consisting of undergraduate-level exam problems in computer science and mathematics. We conduct an in-depth benchmark study of two representative LLMs using various prompting strategies, and find that current LLMs achieve only a 35.80% overall score.Through a detailed user study, we categorize the errors made by LLMs into ten problem-solving abilities. Our analysis shows that no single prompting strategy consistently outperforms others, and some strategies that improve performance in one area lead to declines in other areas.We envision that SciBench will drive further advancements in the reasoning abilities of LLMs, ultimately contributing to scientific research and discovery.
</details></li>
</ul>
<hr>
<h2 id="Generative-Language-Models-on-Nucleotide-Sequences-of-Human-Genes"><a href="#Generative-Language-Models-on-Nucleotide-Sequences-of-Human-Genes" class="headerlink" title="Generative Language Models on Nucleotide Sequences of Human Genes"></a>Generative Language Models on Nucleotide Sequences of Human Genes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10634">http://arxiv.org/abs/2307.10634</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/boun-tabi/generativelm-genes">https://github.com/boun-tabi/generativelm-genes</a></li>
<li>paper_authors: Musa Nuri Ihtiyar, Arzucan Ozgur<br>for:This paper focuses on developing an autoregressive generative language model for DNA sequences, specifically on the nucleotide sequences of human genes.methods:The authors use a systematic approach to examine the performance of different models, including RNNs and N-grams, and explore the use of real-life tasks beyond classical metrics such as perplexity.results:The study finds that RNNs perform the best, and that selecting a language with a minimal vocabulary size does not significantly reduce the amount of data needed.<details>
<summary>Abstract</summary>
Language models, primarily transformer-based ones, obtained colossal success in NLP. To be more precise, studies like BERT in NLU and works such as GPT-3 for NLG are very crucial. DNA sequences are very close to natural language in terms of structure, so if the DNA-related bioinformatics domain is concerned, discriminative models, like DNABert, exist. Yet, the generative side of the coin is mainly unexplored to the best of our knowledge. Consequently, we focused on developing an autoregressive generative language model like GPT-3 for DNA sequences. Because working with whole DNA sequences is challenging without substantial computational resources, we decided to carry out our study on a smaller scale, focusing on nucleotide sequences of human genes, unique parts in DNA with specific functionalities, instead of the whole DNA. This decision did not change the problem structure a lot due to the fact that both DNA and genes can be seen as 1D sequences consisting of four different nucleotides without losing much information and making too much simplification. First of all, we systematically examined an almost entirely unexplored problem and observed that RNNs performed the best while simple techniques like N-grams were also promising. Another beneficial point was learning how to work with generative models on languages we do not understand, unlike natural language. How essential using real-life tasks beyond the classical metrics such as perplexity is observed. Furthermore, checking whether the data-hungry nature of these models can be changed through selecting a language with minimal vocabulary size, four owing to four different types of nucleotides, is examined. The reason for reviewing this was that choosing such a language might make the problem easier. However, what we observed in this study was it did not provide that much of a change in the amount of data needed.
</details>
<details>
<summary>æ‘˜è¦</summary>
language models, primarily transformer-based ones, have achieved great success in NLP. to be more precise, studies like BERT in NLU and works such as GPT-3 for NLG are very crucial. DNA sequences are very close to natural language in terms of structure, so if the DNA-related bioinformatics domain is concerned, discriminative models, like DNABert, exist. however, the generative side of the coin is mainly unexplored to the best of our knowledge. consequently, we focused on developing an autoregressive generative language model like GPT-3 for DNA sequences. because working with whole DNA sequences is challenging without substantial computational resources, we decided to carry out our study on a smaller scale, focusing on nucleotide sequences of human genes, unique parts in DNA with specific functionalities, instead of the whole DNA. this decision did not change the problem structure a lot due to the fact that both DNA and genes can be seen as 1D sequences consisting of four different nucleotides without losing much information and making too much simplification. first of all, we systematically examined an almost entirely unexplored problem and observed that RNNs performed the best while simple techniques like N-grams were also promising. another beneficial point was learning how to work with generative models on languages we do not understand, unlike natural language. how essential using real-life tasks beyond classical metrics such as perplexity is observed. furthermore, checking whether the data-hungry nature of these models can be changed through selecting a language with minimal vocabulary size, four owing to four different types of nucleotides, is examined. the reason for reviewing this was that choosing such a language might make the problem easier. however, what we observed in this study was it did not provide that much of a change in the amount of data needed.
</details></li>
</ul>
<hr>
<h2 id="Multi-Method-Self-Training-Improving-Code-Generation-With-Text-And-Vice-Versa"><a href="#Multi-Method-Self-Training-Improving-Code-Generation-With-Text-And-Vice-Versa" class="headerlink" title="Multi-Method Self-Training: Improving Code Generation With Text, And Vice Versa"></a>Multi-Method Self-Training: Improving Code Generation With Text, And Vice Versa</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10633">http://arxiv.org/abs/2307.10633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shriyash K. Upadhyay, Etan J. Ginsberg</li>
<li>for: è¯¥è®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºå¤šæ–¹æ³•è‡ªåŠ¨è®­ç»ƒï¼ˆMMSTï¼‰ï¼Œä»¥å¢å¼ºè¯­è¨€æ¨¡å‹çš„å¯ç”¨æ€§å’Œæ€§èƒ½ã€‚</li>
<li>methods: è¯¥è®ºæ–‡ä½¿ç”¨äº†ä¸€ç§176Bå‚æ•°çš„è¯­è¨€å’Œä»£ç æ¨¡å‹ï¼Œå¹¶å¯¹å…¶è¿›è¡Œå¤šæ–¹æ³•è‡ªåŠ¨è®­ç»ƒï¼Œä»¥ä¾¿augmentå„ç§æ–¹æ³•çš„ä¼˜åŠ¿å’Œæ”¹å–„å„ç§æ–¹æ³•çš„ç¼ºé™·ã€‚</li>
<li>results: è¯¥è®ºæ–‡æ˜¾ç¤ºï¼Œé€šè¿‡å¤šæ–¹æ³•è‡ªåŠ¨è®­ç»ƒï¼Œå¯ä»¥1)æé«˜è¾ƒå¼±çš„æ–¹æ³•æ€§èƒ½ï¼ˆæœ€å¤š30%ï¼‰ï¼Œ2)æé«˜è¾ƒå¼ºçš„æ–¹æ³•æ€§èƒ½ï¼ˆæœ€å¤š32.2%ï¼‰ï¼Œ3)æé«˜ç›¸å…³ yet distinct tasksçš„æ€§èƒ½ï¼ˆæœ€å¤š10.3%ï¼‰ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è¿›è¡Œäº†ablation analysesï¼Œå¹¶å‘ç°MMSTç”Ÿæˆçš„æ•°æ®é‡æ›´å¤§ï¼Œä½†æ˜¯æ€§èƒ½æé«˜çš„åŸå› æ˜¯å¤šç§æ–¹æ³•çš„ä½¿ç”¨ã€‚<details>
<summary>Abstract</summary>
Large Language Models have many methods for solving the same problem. This introduces novel strengths (different methods may work well for different problems) and weaknesses (it may be difficult for users to know which method to use). In this paper, we introduce Multi-Method Self-Training (MMST), where one method is trained on the filtered outputs of another, allowing us to augment the strengths and ameliorate the weaknesses of each method. Using a 176B parameter model trained on both language and code, we show that MMST can 1) improve the less performant method (up to 30%) making the model easier to use, 2) improve the more performant method (up to 32.2%) making the model more performant, and 3) improve the performance of related but distinct tasks (up to 10.3%) by improving the ability of the model to generate rationales. We then conduct ablation analyses to explore why MMST works. We show that MMST generates more data than traditional self-training, but the improvement in performance is driven by the use of multiple methods. We also analyze prompt-engineering and anti-correlated performance between methods as means of making MMST more effective. We hope the evidence from our paper motivates machine learning researchers to explore ways in which advances in language models allow for new forms of training.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤§è¯­è¨€æ¨¡å‹æœ‰è®¸å¤šæ–¹æ³•è§£å†³åŒä¸€é—®é¢˜ï¼Œè¿™å¼•å…¥äº†æ–°çš„ä¼˜åŠ¿ï¼ˆä¸åŒçš„æ–¹æ³•å¯èƒ½é€‚ç”¨äºä¸åŒçš„é—®é¢˜ï¼‰å’ŒåŠ£åŠ¿ï¼ˆç”¨æˆ·å¯èƒ½Difficult to determineå“ªç§æ–¹æ³•ä½¿ç”¨ï¼‰ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†å¤šç§æ–¹æ³•è‡ªæˆ‘åŸ¹è®­ï¼ˆMMSTï¼‰ï¼Œå…¶ä¸­ä¸€ç§æ–¹æ³•åœ¨å¦ä¸€ç§æ–¹æ³•çš„è¿‡æ»¤è¾“å‡ºä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»è€Œå¯ä»¥å¢å¼ºæ¯ç§æ–¹æ³•çš„ä¼˜åŠ¿å’Œç¼“è§£åŠ£åŠ¿ã€‚ä½¿ç”¨176äº¿å‚æ•°æ¨¡å‹ï¼Œæˆ‘ä»¬æ˜¾ç¤ºäº†ä»¥ä¸‹ä¸‰ç‚¹ï¼š1. ä½¿ç”¨MMSTå¯ä»¥æé«˜è¾ƒå¼±çš„æ–¹æ³•ï¼ˆæœ€å¤š30%ï¼‰ï¼Œä½¿æ¨¡å‹æ›´æ˜“ç”¨ã€‚2. ä½¿ç”¨MMSTå¯ä»¥æé«˜è¾ƒå¼ºçš„æ–¹æ³•ï¼ˆæœ€å¤š32.2%ï¼‰ï¼Œä½¿æ¨¡å‹æ›´é«˜æ•ˆã€‚3. ä½¿ç”¨MMSTå¯ä»¥æé«˜ç›¸å…³è€Œä¸åŒçš„ä»»åŠ¡ï¼ˆæœ€å¤š10.3%ï¼‰çš„æ€§èƒ½ï¼Œé€šè¿‡æé«˜æ¨¡å‹ç”Ÿæˆåˆç†æ€§çš„èƒ½åŠ›ã€‚ç„¶åï¼Œæˆ‘ä»¬è¿›è¡Œäº†å‰¥ç¦»åˆ†æï¼Œå‘ç°MMSTç”Ÿæˆäº†æ›´å¤šçš„æ•°æ®ï¼Œä½†æ˜¯æ€§èƒ½æé«˜çš„åŸå› æ˜¯ä½¿ç”¨å¤šç§æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜åˆ†æäº†å¼•æ“å’Œåç›¸æ€§çš„ä½œç”¨ï¼Œä»¥ä¾¿ä½¿MMSTæ›´æœ‰æ•ˆã€‚æˆ‘ä»¬å¸Œæœ›è¿™ç¯‡è®ºæ–‡çš„è¯æ®èƒ½å¤Ÿé¼“åŠ±æœºå™¨å­¦ä¹ ç ”ç©¶äººå‘˜æ¢ç´¢è¯­è¨€æ¨¡å‹çš„æ–°è®­ç»ƒæ–¹æ³•ã€‚
</details></li>
</ul>
<hr>
<h2 id="Unmasking-Falsehoods-in-Reviews-An-Exploration-of-NLP-Techniques"><a href="#Unmasking-Falsehoods-in-Reviews-An-Exploration-of-NLP-Techniques" class="headerlink" title="Unmasking Falsehoods in Reviews: An Exploration of NLP Techniques"></a>Unmasking Falsehoods in Reviews: An Exploration of NLP Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10617">http://arxiv.org/abs/2307.10617</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anusuya Baby Hari Krishnan</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æå‡ºä¸€ç§æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œç”¨äº indentifying è¯„è®ºä¸­çš„å‡è¯„ä»·ï¼ˆdeceptive reviewsï¼‰ï¼Œå°¤å…¶æ˜¯é’ˆå¯¹é¤å…è¯„è®ºã€‚</li>
<li>methods: æœ¬ç ”ç©¶é‡‡ç”¨äº†n-gramæ¨¡å‹å’Œmax featuresæŠ€æœ¯æ¥æœ‰æ•ˆåœ°è¯†åˆ«å‡è¯„ä»·å†…å®¹ï¼Œå¹¶å¯¹äº”ç§ä¸åŒçš„æœºå™¨å­¦ä¹ åˆ†ç±»ç®—æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨äº†è´Ÿé¢æ”»å‡»åˆ†ç±»å™¨çš„æ–¹æ³•å¯ä»¥è¾¾åˆ°æœ€é«˜çš„ç²¾åº¦å’Œå‡è¯„ä»·è¯†åˆ«ç‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜åº”ç”¨äº†æ·±åº¦å­¦ä¹ æŠ€æœ¯æ¥è¿›ä¸€æ­¥æé«˜å‡è¯„ä»·æ£€æµ‹çš„æ•ˆæœã€‚<details>
<summary>Abstract</summary>
In the contemporary digital landscape, online reviews have become an indispensable tool for promoting products and services across various businesses. Marketers, advertisers, and online businesses have found incentives to create deceptive positive reviews for their products and negative reviews for their competitors' offerings. As a result, the writing of deceptive reviews has become an unavoidable practice for businesses seeking to promote themselves or undermine their rivals. Detecting such deceptive reviews has become an intense and ongoing area of research. This research paper proposes a machine learning model to identify deceptive reviews, with a particular focus on restaurants. This study delves into the performance of numerous experiments conducted on a dataset of restaurant reviews known as the Deceptive Opinion Spam Corpus. To accomplish this, an n-gram model and max features are developed to effectively identify deceptive content, particularly focusing on fake reviews. A benchmark study is undertaken to explore the performance of two different feature extraction techniques, which are then coupled with five distinct machine learning classification algorithms. The experimental results reveal that the passive aggressive classifier stands out among the various algorithms, showcasing the highest accuracy not only in text classification but also in identifying fake reviews. Moreover, the research delves into data augmentation and implements various deep learning techniques to further enhance the process of detecting deceptive reviews. The findings shed light on the efficacy of the proposed machine learning approach and offer valuable insights into dealing with deceptive reviews in the realm of online businesses.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç°ä»£æ•°å­—æ™¯è§‚ä¸­ï¼Œåœ¨çº¿è¯„è®ºå·²æˆä¸ºä¸åŒä¸šåŠ¡çš„ä¸å¯æˆ–ç¼ºçš„å·¥å…·ã€‚marketersã€å¹¿å‘Šäººå’Œåœ¨çº¿ä¸šåŠ¡åœ¨æ¨å¹¿è‡ªå·±æˆ–æŠ¹é»‘ç«äº‰å¯¹æ‰‹çš„äº§å“å’ŒæœåŠ¡çš„è¿‡ç¨‹ä¸­ï¼Œéƒ½å‘ç°äº†å¥–åŠ±åˆ›é€ å‡é˜³æ€§è¯„è®ºçš„åšæ³•ã€‚å› æ­¤ï¼Œåˆ›é€ å‡è¯„è®ºå·²æˆä¸ºä¿ƒè¿›è‡ªå·±æˆ–æŠ¹é»‘ç«äº‰å¯¹æ‰‹çš„ä¸å¯é¿å…çš„åšæ³•ã€‚æ£€æµ‹è¿™äº›å‡è¯„è®ºå·²æˆä¸ºç ”ç©¶çš„ç„¦ç‚¹ä¹‹ä¸€ã€‚è¿™ç¯‡ç ”ç©¶è®ºæ–‡æå‡ºäº†ä¸€ç§æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œç”¨äºè¯†åˆ«å‡è¯„è®ºï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹é¤å…çš„è¯„è®ºã€‚è¿™é¡¹ç ”ç©¶é€šè¿‡å¯¹çŸ¥åçš„é¤å…è¯„è®ºæ•°æ®é›†ï¼ˆDeceptive Opinion Spam Corpusï¼‰è¿›è¡Œå¤šä¸ªå®éªŒï¼Œå¼€å‘äº†ngramæ¨¡å‹å’Œæœ€ä½³ç‰¹å¾æ¥æœ‰æ•ˆåœ°è¯†åˆ«å‡å†…å®¹ï¼Œç‰¹åˆ«æ˜¯å‡è¯„è®ºã€‚è¿›ä¸€æ­¥ï¼Œè¿™ç¯‡ç ”ç©¶è¿›è¡Œäº†ä¸¤ç§ä¸åŒçš„ç‰¹å¾æå–æŠ€æœ¯çš„æ¯”è¾ƒï¼Œç„¶åä¸äº”ç§ä¸åŒçš„æœºå™¨å­¦ä¹ åˆ†ç±»ç®—æ³•ç»“åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡é€‚åº”æ€§åˆ†ç±»å™¨å¾—åˆ°äº†æœ€é«˜çš„å‡†ç¡®ç‡ï¼Œä¸ä»…åœ¨æ–‡æœ¬åˆ†ç±»æ–¹é¢ï¼Œè¿˜åœ¨è¯†åˆ«å‡è¯„è®ºæ–¹é¢ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¢è®¨äº†æ•°æ®æ‰©å……å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œä»¥è¿›ä¸€æ­¥æé«˜æ£€æµ‹å‡è¯„è®ºçš„è¿‡ç¨‹ã€‚ç ”ç©¶ç»“æœçªå‡ºäº†æè®®çš„æœºå™¨å­¦ä¹ æ–¹æ³•çš„æ•ˆæœï¼Œå¹¶ä¸ºåœ¨çº¿ä¸šåŠ¡ä¸­å¤„ç†å‡è¯„è®ºæä¾›äº†æœ‰ä»·å€¼çš„æ€è·¯ã€‚
</details></li>
</ul>
<hr>
<h2 id="Heterogeneous-Federated-Learning-State-of-the-art-and-Research-Challenges"><a href="#Heterogeneous-Federated-Learning-State-of-the-art-and-Research-Challenges" class="headerlink" title="Heterogeneous Federated Learning: State-of-the-art and Research Challenges"></a>Heterogeneous Federated Learning: State-of-the-art and Research Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10616">http://arxiv.org/abs/2307.10616</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/marswhu/hfl_survey">https://github.com/marswhu/hfl_survey</a></li>
<li>paper_authors: Mang Ye, Xiuwen Fang, Bo Du, Pong C. Yuen, Dacheng Tao</li>
<li>for: æœ¬æ–‡æ˜¯ä¸€ç¯‡å…³äº federated learningï¼ˆFLï¼‰åœ¨å¼‚æ­¥izable ç¯å¢ƒä¸‹çš„æŠ¥å‘Šï¼Œå®ƒä»¬æå‡ºäº†åœ¨å®é™…åº”ç”¨ä¸­é‡åˆ°çš„å¤šç§æŒ‘æˆ˜ï¼Œä»¥åŠç°æœ‰çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>methods: æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åˆ†ç±»æ–¹æ³•ï¼ŒåŒ…æ‹¬æ•°æ®æ°´å¹³ã€æ¨¡å‹æ°´å¹³å’ŒæœåŠ¡å™¨æ°´å¹³çš„åˆ†ç±»æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æå‡ºäº†ä¸€äº›å…³é”®çš„æœªæ¥ç ”ç©¶æ–¹å‘ã€‚</li>
<li>results: æœ¬æ–‡é€šè¿‡å¯¹å¤šç§ç ”ç©¶æŒ‘æˆ˜å’Œç°æœ‰çš„è§£å†³æ–¹æ¡ˆè¿›è¡Œåˆ†æï¼Œæå‡ºäº†ä¸€äº›å…³é”®çš„æœªæ¥ç ”ç©¶æ–¹å‘ï¼Œå¯ä»¥å¸®åŠ©è¿›ä¸€æ­¥å‘å±• Federated Learning é¢†åŸŸã€‚I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
Federated learning (FL) has drawn increasing attention owing to its potential use in large-scale industrial applications. Existing federated learning works mainly focus on model homogeneous settings. However, practical federated learning typically faces the heterogeneity of data distributions, model architectures, network environments, and hardware devices among participant clients. Heterogeneous Federated Learning (HFL) is much more challenging, and corresponding solutions are diverse and complex. Therefore, a systematic survey on this topic about the research challenges and state-of-the-art is essential. In this survey, we firstly summarize the various research challenges in HFL from five aspects: statistical heterogeneity, model heterogeneity, communication heterogeneity, device heterogeneity, and additional challenges. In addition, recent advances in HFL are reviewed and a new taxonomy of existing HFL methods is proposed with an in-depth analysis of their pros and cons. We classify existing methods from three different levels according to the HFL procedure: data-level, model-level, and server-level. Finally, several critical and promising future research directions in HFL are discussed, which may facilitate further developments in this field. A periodically updated collection on HFL is available at https://github.com/marswhu/HFL_Survey.
</details>
<details>
<summary>æ‘˜è¦</summary>
éšç€è”åˆå­¦ä¹ ï¼ˆFederated Learningï¼ŒFLï¼‰çš„åº”ç”¨èŒƒå›´æ‰©å¤§ï¼Œå®ƒåœ¨å¤§è§„æ¨¡ä¼ä¸šåº”ç”¨åœºæ™¯ä¸­å—åˆ°äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ç°æœ‰çš„è”åˆå­¦ä¹ ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ¨¡å‹åŒè´¨ Settingsä¸­ã€‚ç„¶è€Œï¼Œå®é™…çš„è”åˆå­¦ä¹ å¾€å¾€é¢ä¸´å‚ä¸å®¢æˆ·ç«¯çš„æ•°æ®åˆ†å¸ƒã€æ¨¡å‹æ¶æ„ã€ç½‘ç»œç¯å¢ƒå’Œç¡¬ä»¶è®¾å¤‡ä¹‹é—´çš„å·®å¼‚ã€‚è¿™ç§å·®å¼‚çš„è”åˆå­¦ä¹ ï¼ˆHeterogeneous Federated Learningï¼ŒHFLï¼‰æ˜¯æ›´åŠ å¤æ‚å’Œå¤šæ ·åŒ–çš„ï¼Œéœ€è¦ç›¸åº”çš„ç ”ç©¶æŒ‘æˆ˜å’Œè§£å†³æ–¹æ¡ˆã€‚å› æ­¤ï¼Œä¸€ç¯‡ç³»ç»Ÿæ€§çš„è°ƒæŸ¥ç ”ç©¶åœ¨è¿™ä¸ªé¢†åŸŸæ˜¯éå¸¸é‡è¦çš„ã€‚åœ¨æœ¬è°ƒæŸ¥ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆæ€»ç»“äº†HFLä¸­ä¸åŒæ–¹é¢çš„ç ”ç©¶æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç»Ÿè®¡å·®å¼‚ã€æ¨¡å‹å·®å¼‚ã€é€šä¿¡å·®å¼‚ã€è®¾å¤‡å·®å¼‚ä»¥åŠå…¶ä»–æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†ç°æœ‰HFLæ–¹æ³•çš„å›é¡¾ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„åˆ†ç±»æ–¹æ³•ï¼Œæ ¹æ®HFLè¿‡ç¨‹çš„ä¸‰çº§å±‚æ¬¡ï¼šæ•°æ®å±‚ã€æ¨¡å‹å±‚å’ŒæœåŠ¡å™¨å±‚ã€‚æœ€åï¼Œæˆ‘ä»¬è¿˜è®¨è®ºäº†æœªæ¥ç ”ç©¶çš„ä¸€äº›é‡è¦å’Œä¼˜å…ˆçš„æ–¹å‘ï¼Œä»¥ä¾¿è¿›ä¸€æ­¥å‘å±•è¿™ä¸€é¢†åŸŸã€‚å…³äºHFLçš„ç›¸å…³ç ”ç©¶å¯ä»¥é€šè¿‡https://github.com/marswhu/HFL_SurveyæŸ¥çœ‹æ›´æ–°çš„é›†æˆã€‚
</details></li>
</ul>
<hr>
<h2 id="Flatness-Aware-Minimization-for-Domain-Generalization"><a href="#Flatness-Aware-Minimization-for-Domain-Generalization" class="headerlink" title="Flatness-Aware Minimization for Domain Generalization"></a>Flatness-Aware Minimization for Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11108">http://arxiv.org/abs/2307.11108</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingxuan Zhang, Renzhe Xu, Han Yu, Yancheng Dong, Pengfei Tian, Peng Cu</li>
<li>for: è¿™ç¯‡ç ”ç©¶æ—¨åœ¨æ¢è®¨é¢†åŸŸæ‰©å±•ï¼ˆDomain Generalizationï¼ŒDGï¼‰ä¸­çš„ä¼˜åŒ–å™¨é€‰æ‹©é—®é¢˜ã€‚</li>
<li>methods: æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”Flatness-Aware Minimization for Domain Generalizationï¼ˆFADï¼‰ï¼Œå¯ä»¥æœ‰æ•ˆåœ°ä¼˜åŒ–é›¶é¡¹å’Œé¦–é¡¹çš„å¹³å¦æ€§åŒæ—¶ï¼Œä»¥æé«˜DGæ¨¡å‹çš„é€‚ç”¨èŒƒå›´ã€‚</li>
<li>results: å®éªŒç»“æœæ˜¾ç¤ºFADåœ¨å¤šç§DGæ•°æ®é›†ä¸Šå…·æœ‰ä¼˜è¶Šæ€§ï¼Œå¹¶ä¸”èƒ½å¤Ÿå‘ç°æ›´å¹³å¦çš„æç‚¹ï¼Œè¾ƒå…¶ä»–é›¶é¡¹å’Œé¦–é¡¹å¹³å¦æ€§æ„ŸçŸ¥ä¼˜åŒ–æ–¹æ³•æ›´å¥½ã€‚<details>
<summary>Abstract</summary>
Domain generalization (DG) seeks to learn robust models that generalize well under unknown distribution shifts. As a critical aspect of DG, optimizer selection has not been explored in depth. Currently, most DG methods follow the widely used benchmark, DomainBed, and utilize Adam as the default optimizer for all datasets. However, we reveal that Adam is not necessarily the optimal choice for the majority of current DG methods and datasets. Based on the perspective of loss landscape flatness, we propose a novel approach, Flatness-Aware Minimization for Domain Generalization (FAD), which can efficiently optimize both zeroth-order and first-order flatness simultaneously for DG. We provide theoretical analyses of the FAD's out-of-distribution (OOD) generalization error and convergence. Our experimental results demonstrate the superiority of FAD on various DG datasets. Additionally, we confirm that FAD is capable of discovering flatter optima in comparison to other zeroth-order and first-order flatness-aware optimization methods.
</details>
<details>
<summary>æ‘˜è¦</summary>
é¢†åŸŸæ€»ç»“ï¼ˆDGï¼‰ç›®æ ‡æ˜¯å­¦ä¹ èƒ½å¤Ÿé€‚åº”æœªçŸ¥åˆ†å¸ƒå˜åŒ–çš„Robustæ¨¡å‹ã€‚ä½œä¸ºDGçš„å…³é”®æ–¹é¢ï¼Œä¼˜åŒ–å™¨é€‰æ‹©å°šæœªå¾—åˆ°æ·±å…¥ç ”ç©¶ã€‚ç°åœ¨ï¼Œå¤§å¤šæ•°DGæ–¹æ³•é‡‡ç”¨DomainBedçš„æ ‡å‡†å‡†åˆ™ï¼Œå¹¶ä½¿ç”¨Adamä½œä¸ºæ‰€æœ‰æ•°æ®é›†çš„é»˜è®¤ä¼˜åŒ–å™¨ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°Adamå¹¶ä¸ä¸€å®šæ˜¯å¤§å¤šæ•°å½“å‰DGæ–¹æ³•å’Œæ•°æ®é›†çš„ä¼˜é€‰ä¼˜åŒ–å™¨ã€‚åŸºäºæŸå¤±å‡½æ•°åœ°å½¢çš„è§†è§’ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼šé€‚åº”æ€§è°±ä¼˜åŒ– Ğ´Ğ»Ñé¢†åŸŸæ€»ç»“ï¼ˆFADï¼‰ï¼Œå¯ä»¥é«˜æ•ˆåœ°åŒæ—¶ä¼˜åŒ–é›¶æ¬¡é¡¹å’Œé¦–æ¬¡é¡¹çš„å¹³å¦æ€§ã€‚æˆ‘ä»¬æä¾›äº†FADçš„OODæ³›åŒ–è¯¯å·®å’Œæ”¶æ•›æ€§çš„ç†è®ºåˆ†æã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜FADåœ¨å¤šä¸ªDGæ•°æ®é›†ä¸Šå…·æœ‰ä¼˜ç§€çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†FADå¯ä»¥æ›´å¥½åœ°æ‰¾åˆ°å¹³å¦çš„æç‚¹ï¼Œæ¯”å…¶ä»–é›¶æ¬¡é¡¹å’Œé¦–æ¬¡é¡¹å¹³å¦æ€§æ„è¯†ä¼˜åŒ–å™¨æ›´å¼ºã€‚
</details></li>
</ul>
<hr>
<h2 id="Ensemble-Learning-based-Anomaly-Detection-for-IoT-Cybersecurity-via-Bayesian-Hyperparameters-Sensitivity-Analysis"><a href="#Ensemble-Learning-based-Anomaly-Detection-for-IoT-Cybersecurity-via-Bayesian-Hyperparameters-Sensitivity-Analysis" class="headerlink" title="Ensemble Learning based Anomaly Detection for IoT Cybersecurity via Bayesian Hyperparameters Sensitivity Analysis"></a>Ensemble Learning based Anomaly Detection for IoT Cybersecurity via Bayesian Hyperparameters Sensitivity Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10596">http://arxiv.org/abs/2307.10596</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tin Lai, Farnaz Farid, Abubakar Bello, Fariza Sabrina<br>for: è¿™ paper çš„ç›®çš„æ˜¯æé«˜ IoT ç½‘ç»œçš„å®‰å…¨æ€§ via å¼‚å¸¸æ£€æµ‹ã€‚methods: è¿™ paper ä½¿ç”¨äº† ensemble æœºå™¨å­¦ä¹ æ–¹æ³•æ¥æé«˜å¼‚å¸¸æ£€æµ‹çš„å‡†ç¡®æ€§ï¼Œå¹¶ä½¿ç”¨ Bayesian è¶…å‚æ•°ä¼˜åŒ–æ¥é€‚åº”å¤šç§ IoT ä¼ æ„Ÿå™¨è¯»æ•°ã€‚results: å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨æ¯”è¾ƒä¼ ç»Ÿæ–¹æ³•æ—¶æœ‰æ›´é«˜çš„é¢„æµ‹åŠ›ã€‚<details>
<summary>Abstract</summary>
The Internet of Things (IoT) integrates more than billions of intelligent devices over the globe with the capability of communicating with other connected devices with little to no human intervention. IoT enables data aggregation and analysis on a large scale to improve life quality in many domains. In particular, data collected by IoT contain a tremendous amount of information for anomaly detection. The heterogeneous nature of IoT is both a challenge and an opportunity for cybersecurity. Traditional approaches in cybersecurity monitoring often require different kinds of data pre-processing and handling for various data types, which might be problematic for datasets that contain heterogeneous features. However, heterogeneous types of network devices can often capture a more diverse set of signals than a single type of device readings, which is particularly useful for anomaly detection. In this paper, we present a comprehensive study on using ensemble machine learning methods for enhancing IoT cybersecurity via anomaly detection. Rather than using one single machine learning model, ensemble learning combines the predictive power from multiple models, enhancing their predictive accuracy in heterogeneous datasets rather than using one single machine learning model. We propose a unified framework with ensemble learning that utilises Bayesian hyperparameter optimisation to adapt to a network environment that contains multiple IoT sensor readings. Experimentally, we illustrate their high predictive power when compared to traditional methods.
</details>
<details>
<summary>æ‘˜è¦</summary>
äº’è”ç½‘æ™ºèƒ½è®¾å¤‡ï¼ˆIoTï¼‰å·²ç»åœ¨å…¨çƒèŒƒå›´å†…é›†æˆäº†æ•°ä»¥åä¸‡è®¡çš„æ™ºèƒ½è®¾å¤‡ï¼Œè¿™äº›è®¾å¤‡å¯ä»¥è‡ªåŠ¨ä¸å…¶ä»–ç›¸è¿çš„è®¾å¤‡äº¤æ¢æ•°æ®ï¼Œè€Œæ— éœ€äººç±»å¹²é¢„ã€‚IoTå…è®¸å¤§è§„æ¨¡æ•°æ®èšåˆå’Œåˆ†æï¼Œä»è€Œæé«˜ç”Ÿæ´»è´¨é‡åœ¨å¤šä¸ªé¢†åŸŸã€‚ç‰¹åˆ«æ˜¯æ•°æ®æ”¶é›†åˆ°IoTä¸­å«æœ‰å·¨é‡æ•°æ®å¼‚å¸¸æ£€æµ‹ä¿¡æ¯ã€‚IoTç½‘ç»œè®¾å¤‡çš„å¤šæ ·æ€§åŒæ—¶æ˜¯æŒ‘æˆ˜å’Œæœºé‡ï¼Œä¼ ç»Ÿçš„ç½‘ç»œå®‰å…¨ç›‘æ§æ–¹æ³•é€šå¸¸éœ€è¦ä¸åŒç±»å‹çš„æ•°æ®å¤„ç†å’Œå¤„ç†ï¼Œè¿™å¯èƒ½ä¼šå¯¹å…·æœ‰å¤šç§ç‰¹å¾çš„æ•°æ®é€ æˆé—®é¢˜ã€‚ç„¶è€Œï¼Œå¤šç§ç½‘ç»œè®¾å¤‡å¯ä»¥æ•æ‰æ›´å¤šçš„ä¿¡å·ï¼Œè¿™å¯¹å¼‚å¸¸æ£€æµ‹æ˜¯ç‰¹åˆ«æœ‰ç”¨ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€é¡¹æ€»ç»“æ€§çš„ç ”ç©¶ï¼Œåˆ©ç”¨å¤šä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹çš„ ensemble æ–¹æ³•æé«˜ IoT ç½‘ç»œå®‰å…¨æ€§ via å¼‚å¸¸æ£€æµ‹ã€‚è€Œä¸æ˜¯ä½¿ç”¨å•ä¸€çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œensemble å­¦ä¹ å¯ä»¥å°†å¤šç§æ¨¡å‹çš„é¢„æµ‹åŠ›ç›¸äº’ç»“åˆï¼Œåœ¨å…·æœ‰å¤šç§ç‰¹å¾çš„æ•°æ®é›†ä¸­æé«˜é¢„æµ‹ç²¾åº¦ã€‚æˆ‘ä»¬æè®®ä¸€ç§ç»Ÿä¸€æ¡†æ¶ï¼Œåˆ©ç”¨ Bayesian å‚æ•°ä¼˜åŒ–æ¥é€‚åº”åŒ…å«å¤šç§ IoT ä¼ æ„Ÿå™¨è¯»æ•°çš„ç½‘ç»œç¯å¢ƒã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰é«˜é¢„æµ‹åŠ›ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ã€‚
</details></li>
</ul>
<hr>
<h2 id="Forecasting-Battery-Electric-Vehicle-Charging-Behavior-A-Deep-Learning-Approach-Equipped-with-Micro-Clustering-and-SMOTE-Techniques"><a href="#Forecasting-Battery-Electric-Vehicle-Charging-Behavior-A-Deep-Learning-Approach-Equipped-with-Micro-Clustering-and-SMOTE-Techniques" class="headerlink" title="Forecasting Battery Electric Vehicle Charging Behavior: A Deep Learning Approach Equipped with Micro-Clustering and SMOTE Techniques"></a>Forecasting Battery Electric Vehicle Charging Behavior: A Deep Learning Approach Equipped with Micro-Clustering and SMOTE Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10588">http://arxiv.org/abs/2307.10588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanif Tayarani, Trisha V. Ramadoss, Vaishnavi Karanam, Gil Tal, Christopher Nitta</li>
<li>for: é™ä½æ’æ”¾å’Œæ±¡æŸ“ç‰©å¯¹ç¯å¢ƒçš„å½±å“ï¼Œå°†äº¤é€šé¢†åŸŸç”µåŒ–ã€‚</li>
<li>methods: ä½¿ç”¨äººå·¥ç¥ç»ç½‘ç»œç®—æ³•ï¼Œå¯¹BEVè½¦è¾†çš„è¡Œç¨‹å’Œå……ç”µèµ„æ–™è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>results: æ¯”è¾ƒ benchmark æ–¹æ³•ï¼ŒMCDNN èƒ½æ›´å¥½åœ°é¢„æµ‹ BEV å……ç”µäº‹ä»¶ã€‚<details>
<summary>Abstract</summary>
Energy systems, climate change, and public health are among the primary reasons for moving toward electrification in transportation. Transportation electrification is being promoted worldwide to reduce emissions. As a result, many automakers will soon start making only battery electric vehicles (BEVs). BEV adoption rates are rising in California, mainly due to climate change and air pollution concerns. While great for climate and pollution goals, improperly managed BEV charging can lead to insufficient charging infrastructure and power outages. This study develops a novel Micro Clustering Deep Neural Network (MCDNN), an artificial neural network algorithm that is highly effective at learning BEVs trip and charging data to forecast BEV charging events, information that is essential for electricity load aggregators and utility managers to provide charging stations and electricity capacity effectively. The MCDNN is configured using a robust dataset of trips and charges that occurred in California between 2015 and 2020 from 132 BEVs, spanning 5 BEV models for a total of 1570167 vehicle miles traveled. The numerical findings revealed that the proposed MCDNN is more effective than benchmark approaches in this field, such as support vector machine, k nearest neighbors, decision tree, and other neural network-based models in predicting the charging events.
</details>
<details>
<summary>æ‘˜è¦</summary>
ã€Šèƒ½æºç³»ç»Ÿã€æ°”å€™å˜åŒ–å’Œå…¬å…±å«ç”Ÿã€‹æ˜¯ç”µåŠ¨åŒ–äº¤é€šçš„ä¸»è¦ä¿ƒè¿›å› ç´ ä¹‹ä¸€ã€‚ç”±äºæ°”å€™å˜åŒ–å’Œæ±¡æŸ“çš„å…³æ³¨ï¼Œå…¨çƒèŒƒå›´å†…çš„ç”µåŠ¨åŒ–äº¤é€šå¸å¼•ç€å¹¿æ³›çš„æ¨å¹¿ã€‚å› æ­¤ï¼Œè®¸å¤šæ±½è½¦åˆ¶é€ å•†å³å°†åœæ­¢ç”Ÿäº§å†…ç‡ƒæœºæ²¹è½¦ï¼Œè½¬è€Œç”Ÿäº§ç”µæ± ç”µåŠ¨è½¦ï¼ˆBEVï¼‰ã€‚åœ¨åŠ åˆ©ç¦å°¼äºšå·ï¼ŒBEVçš„é‡‡è´­ç‡åœ¨å¢åŠ ï¼Œä¸»è¦æ˜¯ç”±äºæ°”å€™å˜åŒ–å’Œç©ºæ°”æ±¡æŸ“çš„é—®é¢˜ã€‚è™½ç„¶è¿™å¯¹æ°”å€™å’Œæ±¡æŸ“ç›®æ ‡å…·æœ‰ä¼˜ç§€çš„æ•ˆæœï¼Œä½†æ˜¯ä¸å½“ç®¡ç†BEVå……ç”µå¯èƒ½ä¼šå¯¼è‡´å……ç”µåŸºç¡€è®¾æ–½ä¸è¶³å’Œåœç”µã€‚è¿™é¡¹ç ”ç©¶å¼€å‘äº†ä¸€ç§å¾®å‹å›¢é›†æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆMCDNNï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯ä»¥é«˜æ•ˆåœ°å­¦ä¹ BEVçš„è¡Œé©¶å’Œå……ç”µæ•°æ®ï¼Œä»¥é¢„æµ‹BEVçš„å……ç”µäº‹ä»¶ã€‚MCDNNé…ç½®äº†åŠ åˆ©ç¦å°¼äºšå·2015-2020å¹´é—´132å°ç”µåŠ¨è½¦çš„è¡Œé©¶è®°å½•ï¼Œæ¶µç›–5ç§ç”µåŠ¨è½¦å‹ï¼Œå…±è®¡1570167å…¬é‡Œè¡Œé©¶ã€‚numericalå‘ç°ï¼Œæå‡ºçš„MCDNNæ¯”å„ç§å‚è€ƒæ–¹æ³•ï¼Œå¦‚æ”¯æŒå‘é‡æœºã€æœ€è¿‘é‚»å±…ã€å†³ç­–æ ‘å’Œå…¶ä»–ç¥ç»ç½‘ç»œæ¨¡å‹åœ¨é¢„æµ‹å……ç”µäº‹ä»¶æ–¹é¢æ›´æœ‰æ•ˆã€‚
</details></li>
</ul>
<hr>
<h2 id="A-Holistic-Assessment-of-the-Reliability-of-Machine-Learning-Systems"><a href="#A-Holistic-Assessment-of-the-Reliability-of-Machine-Learning-Systems" class="headerlink" title="A Holistic Assessment of the Reliability of Machine Learning Systems"></a>A Holistic Assessment of the Reliability of Machine Learning Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10586">http://arxiv.org/abs/2307.10586</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anthony Corso, David Karamadian, Romeo Valentin, Mary Cooper, Mykel J. Kochenderfer</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨è¯„ä¼°æœºå™¨å­¦ä¹ ç³»ç»Ÿçš„å¯é æ€§ï¼Œä»¥ä¾¿åœ¨é«˜åº¦ç«äº‰çš„é¢†åŸŸä¸­æé«˜ç³»ç»Ÿçš„å¯é æ€§ã€‚</li>
<li>methods: æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ•´ä½“è¯„ä¼°æœºå™¨å­¦ä¹ ç³»ç»Ÿå¯é æ€§çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬äº”ä¸ªå…³é”®å±æ€§çš„è¯„ä¼°ï¼šå†…éƒ¨åˆ†å¸ƒå‡†ç¡®ç‡ã€ç¯å¢ƒå˜åŒ–å¿«é€Ÿç¨³å®šæ€§ã€é’ˆå¯¹æ€§æ”»å‡»å¿«é€Ÿç¨³å®šæ€§ã€æ ¡å‡†æ€§å’Œå¤–éƒ¨åˆ†å¸ƒæ£€æµ‹ã€‚</li>
<li>results: ç ”ç©¶äººå‘˜é€šè¿‡ä½¿ç”¨æå‡ºçš„æ–¹æ³•å¯¹500å¤šä¸ªæ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œå‘ç°ä¸åŒçš„ç®—æ³•æ–¹æ³•å¯ä»¥åŒæ—¶æé«˜å¤šä¸ªå¯é æ€§æŒ‡æ ‡ï¼Œè€Œä¸æ˜¯åªæ˜¯ä¼˜å…ˆä¸€ä¸ªæŒ‡æ ‡ã€‚è¿™é¡¹ç ”ç©¶ä¸ºæœºå™¨å­¦ä¹ å¯é æ€§çš„å…¨é¢ç†è§£å’Œæœªæ¥ç ”å‘æä¾›äº†ä¸€ä»½è·¯çº¿å›¾ã€‚<details>
<summary>Abstract</summary>
As machine learning (ML) systems increasingly permeate high-stakes settings such as healthcare, transportation, military, and national security, concerns regarding their reliability have emerged. Despite notable progress, the performance of these systems can significantly diminish due to adversarial attacks or environmental changes, leading to overconfident predictions, failures to detect input faults, and an inability to generalize in unexpected scenarios. This paper proposes a holistic assessment methodology for the reliability of ML systems. Our framework evaluates five key properties: in-distribution accuracy, distribution-shift robustness, adversarial robustness, calibration, and out-of-distribution detection. A reliability score is also introduced and used to assess the overall system reliability. To provide insights into the performance of different algorithmic approaches, we identify and categorize state-of-the-art techniques, then evaluate a selection on real-world tasks using our proposed reliability metrics and reliability score. Our analysis of over 500 models reveals that designing for one metric does not necessarily constrain others but certain algorithmic techniques can improve reliability across multiple metrics simultaneously. This study contributes to a more comprehensive understanding of ML reliability and provides a roadmap for future research and development.
</details>
<details>
<summary>æ‘˜è¦</summary>
machine learning (ML) ç³»ç»Ÿåœ¨é«˜åº¦é‡è¦çš„è®¾ç½®ä¸­å¦‚åŒ»ç–—ã€äº¤é€šã€å†›äº‹å’Œå›½å®¶å®‰å…¨ä¸­è¶Šæ¥è¶Šæ™®éï¼Œå…³äºå®ƒä»¬çš„å¯é æ€§çš„é—®é¢˜ä¹Ÿå¾—åˆ°äº†å…³æ³¨ã€‚å°½ç®¡åœ¨è¿›æ­¥æ–¹é¢åšå‡ºäº†å¾ˆå¤§çš„è¿›å±•ï¼Œä½†æ˜¯è¿™äº›ç³»ç»Ÿçš„æ€§èƒ½å¯èƒ½ä¼šå› ä¸ºæŠ—å¯¹æŠ—æ”»å‡»æˆ–ç¯å¢ƒå˜åŒ–è€Œå‡é€€ï¼Œå¯¼è‡´è¿‡äºè‡ªä¿¡çš„é¢„æµ‹ã€è¾“å…¥é”™è¯¯çš„æ£€æµ‹å¤±è´¥å’Œä¸èƒ½é€‚åº”æ„å¤–çš„æƒ…å†µã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ•´ä½“è¯„ä¼°æ–¹æ³• Ğ´Ğ»Ñ ML ç³»ç»Ÿçš„å¯é æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶è¯„ä¼°äº†äº”ä¸ªå…³é”®å±æ€§ï¼šåœ¨è¾“å…¥æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡ã€å¯¹è¾“å…¥æ•°æ®é›†çš„å˜åŒ–robustnessã€å¯¹æŠ—æ”»å‡»çš„Robustnessã€calibrationå’Œå¯¹è¾“å…¥æ•°æ®é›†ä¹‹å¤–çš„æ£€æµ‹ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªå¯é åº¦åˆ†æ•°ï¼Œç”¨äºè¯„ä¼°æ•´ä½“ç³»ç»Ÿçš„å¯é æ€§ã€‚ä¸ºäº†æä¾›ä¸åŒç®—æ³•approachçš„æ€§èƒ½åˆ†æï¼Œæˆ‘ä»¬åˆ†ç±»äº†ç°æœ‰çš„æŠ€æœ¯ï¼Œç„¶åä½¿ç”¨æˆ‘ä»¬çš„æå‡ºçš„å¯é æ€§æŒ‡æ ‡å’Œå¯é åº¦åˆ†æ•°è¯„ä¼°ä¸€äº›å®é™…ä»»åŠ¡ä¸­çš„é€‰æ‹©ã€‚æˆ‘ä»¬çš„åˆ†æç»“æœè¡¨æ˜ï¼Œä¸åŒçš„ç®—æ³•approachå¯ä»¥åŒæ—¶æ”¹å–„å¤šä¸ªå¯é æ€§æŒ‡æ ‡ã€‚è¿™ç§ç ”ç©¶å¯¹ ML ç³»ç»Ÿçš„å¯é æ€§è¿›è¡Œäº†æ›´å…¨é¢çš„ç†è§£ï¼Œå¹¶æä¾›äº†æœªæ¥ç ”ç©¶å’Œå¼€å‘çš„é“è·¯å›¾ã€‚
</details></li>
</ul>
<hr>
<h2 id="Intelligent-model-for-offshore-China-sea-fog-forecasting"><a href="#Intelligent-model-for-offshore-China-sea-fog-forecasting" class="headerlink" title="Intelligent model for offshore China sea fog forecasting"></a>Intelligent model for offshore China sea fog forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10580">http://arxiv.org/abs/2307.10580</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanfei Xiang, Qinghong Zhang, Mingqing Wang, Ruixue Xia, Yang Kong, Xiaomeng Huang</li>
<li>For: é¢„æµ‹æµ·ä¸Šé›¾éœ¾çš„ç²¾å‡†å’Œå‡†ç¡®æ€§éå¸¸é‡è¦ï¼Œä»¥ä¾¿æœ‰æ•ˆç®¡ç†æµ·å²¸å’Œæµ·ä¸Šç»æµæ´»åŠ¨ã€‚* Methods: æœ¬ç ”ç©¶ä½¿ç”¨æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œå¹¶åœ¨æ•°å€¼å¤©æ°”é¢„æµ‹æ¨¡å‹ä¸­åµŒå…¥ï¼Œä»¥è§£å†³æµ·ä¸Šé›¾éœ¾é¢„æµ‹çš„é—®é¢˜ã€‚åœ¨è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬ä½¿ç”¨æ—¶é—´å»¶è¿Ÿç›¸å…³åˆ†ææŠ€æœ¯æ¥ indentifyå…³é”®é¢„æµ‹å› ç´ ï¼Œå¹¶è§£å†³æµ·ä¸Šé›¾éœ¾å‡ºç°çš„ä¸‹é¢æœºåˆ¶ã€‚* Results: æˆ‘ä»¬çš„æœºå™¨å­¦ä¹ åŸºäºæ–¹æ³•åœ¨ä¸€å¹´çš„æµ‹è¯•æ•°æ®ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†WRF-NMMå’ŒNOAA FSLçš„é¢„æµ‹æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨é¢„æµ‹æµ·ä¸Šé›¾éœ¾è§†åŠ›ä½äºæˆ–ç­‰äº1å…¬é‡Œçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨60å°æ—¶å‰çš„é¢„æµ‹ä¸­å…·æœ‰æ›´é«˜çš„æ£€æµ‹å¯èƒ½æ€§ï¼ˆPODï¼‰å’Œæ›´ä½çš„è¯¯é£ç‡ï¼ˆFARï¼‰ã€‚<details>
<summary>Abstract</summary>
Accurate and timely prediction of sea fog is very important for effectively managing maritime and coastal economic activities. Given the intricate nature and inherent variability of sea fog, traditional numerical and statistical forecasting methods are often proven inadequate. This study aims to develop an advanced sea fog forecasting method embedded in a numerical weather prediction model using the Yangtze River Estuary (YRE) coastal area as a case study. Prior to training our machine learning model, we employ a time-lagged correlation analysis technique to identify key predictors and decipher the underlying mechanisms driving sea fog occurrence. In addition, we implement ensemble learning and a focal loss function to address the issue of imbalanced data, thereby enhancing the predictive ability of our model. To verify the accuracy of our method, we evaluate its performance using a comprehensive dataset spanning one year, which encompasses both weather station observations and historical forecasts. Remarkably, our machine learning-based approach surpasses the predictive performance of two conventional methods, the weather research and forecasting nonhydrostatic mesoscale model (WRF-NMM) and the algorithm developed by the National Oceanic and Atmospheric Administration (NOAA) Forecast Systems Laboratory (FSL). Specifically, in regard to predicting sea fog with a visibility of less than or equal to 1 km with a lead time of 60 hours, our methodology achieves superior results by increasing the probability of detection (POD) while simultaneously reducing the false alarm ratio (FAR).
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>>å°†æ–‡æœ¬ç¿»è¯‘æˆç®€åŒ–ä¸­æ–‡ã€‚<</SYS>> Effective sea fog prediction is crucial for managing maritime and coastal economic activities. However, traditional numerical and statistical forecasting methods often fall short due to the complex and inherently variable nature of sea fog. This study aims to develop an advanced sea fog forecasting method using a numerical weather prediction model, with the Yangtze River Estuary (YRE) coastal area as a case study. Before training our machine learning model, we employ a time-lagged correlation analysis technique to identify key predictors and understand the underlying mechanisms driving sea fog occurrence. Additionally, we use ensemble learning and a focal loss function to address the issue of imbalanced data, which enhances the predictive ability of our model. To evaluate the accuracy of our method, we use a comprehensive dataset spanning one year, which includes both weather station observations and historical forecasts. Our machine learning-based approach outperforms two conventional methods, the Weather Research and Forecasting nonhydrostatic mesoscale model (WRF-NMM) and the algorithm developed by the National Oceanic and Atmospheric Administration (NOAA) Forecast Systems Laboratory (FSL). Specifically, our methodology achieves better results in predicting sea fog with a visibility of less than or equal to 1 km with a lead time of 60 hours, with higher probability of detection (POD) and lower false alarm ratio (FAR).
</details></li>
</ul>
<hr>
<h2 id="SecureBoost-Hyperparameter-Tuning-via-Multi-Objective-Federated-Learning"><a href="#SecureBoost-Hyperparameter-Tuning-via-Multi-Objective-Federated-Learning" class="headerlink" title="SecureBoost Hyperparameter Tuning via Multi-Objective Federated Learning"></a>SecureBoost Hyperparameter Tuning via Multi-Objective Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10579">http://arxiv.org/abs/2307.10579</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyao Ren, Yan Kang, Lixin Fan, Linghua Yang, Yongxin Tong, Qiang Yang</li>
<li>For: è¿™ä¸ªç ”ç©¶ç›®çš„æ˜¯æå‡ºä¸€ä¸ªåä¸º Constrained Multi-Objective SecureBoost (CMOSB) çš„ç®—æ³•ï¼Œç”¨äºåœ¨é˜¶å±‚å¼è”åˆå­¦ä¹ ä¸­é€‰æ‹©æœ€ä½³çš„ SecureBoost å‚æ•°ï¼Œä»¥å®ç°æœ€ä½³çš„è°ƒè§£ Ğ¼ĞµĞ¶Ğ´Ñƒ åŠŸèƒ½æŸå¤±ã€è®­ç»ƒæˆæœ¬å’Œéšç§æ³„éœ²ã€‚* Methods: è¿™ä¸ªç ”ç©¶ä½¿ç”¨äº† SecureBoost ç®—æ³•ï¼Œå¹¶å°†å…¶ä¸å¤šbjective evolutionary algorithm (MOEA) ç»“åˆï¼Œä»¥æ‰¾åˆ° Pareto æœ€ä½³è§£ã€‚å¦å¤–ï¼Œè¿™ä¸ªç ”ç©¶è¿˜æå‡ºäº†ä¸€ä¸ªæ–°çš„å®ä¾‹èšç±»æ”»å‡»æ¥é‡åŒ–éšç§æ³„éœ²ã€‚* Results: å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCMOSB å¯ä»¥è·å¾—ä¸åªæ˜¯åŸºelineçš„ä¼˜åŒ–å‚æ•°ï¼Œè¿˜å¯ä»¥æ‰¾åˆ°æœ€ä½³çš„å‚æ•°é›†ï¼Œä»¥æ»¡è¶³ä¸åŒçš„ FL å‚ä¸è€…çš„éœ€æ±‚ã€‚<details>
<summary>Abstract</summary>
SecureBoost is a tree-boosting algorithm leveraging homomorphic encryption to protect data privacy in vertical federated learning setting. It is widely used in fields such as finance and healthcare due to its interpretability, effectiveness, and privacy-preserving capability. However, SecureBoost suffers from high computational complexity and risk of label leakage. To harness the full potential of SecureBoost, hyperparameters of SecureBoost should be carefully chosen to strike an optimal balance between utility, efficiency, and privacy. Existing methods either set hyperparameters empirically or heuristically, which are far from optimal. To fill this gap, we propose a Constrained Multi-Objective SecureBoost (CMOSB) algorithm to find Pareto optimal solutions that each solution is a set of hyperparameters achieving optimal tradeoff between utility loss, training cost, and privacy leakage. We design measurements of the three objectives. In particular, the privacy leakage is measured using our proposed instance clustering attack. Experimental results demonstrate that the CMOSB yields not only hyperparameters superior to the baseline but also optimal sets of hyperparameters that can support the flexible requirements of FL participants.
</details>
<details>
<summary>æ‘˜è¦</summary>
secureboostæ˜¯ä¸€ç§æ ‘èåˆç®—æ³•ï¼Œåˆ©ç”¨åŒè´¨åŠ å¯†ä¿æŠ¤æ•°æ®éšç§åœ¨å‚ç›´è”åˆå­¦ä¹ è®¾ç½®ä¸‹ã€‚å®ƒåœ¨é‡‘èå’ŒåŒ»ç–—ç­‰é¢†åŸŸå¹¿æ³›åº”ç”¨ï¼Œå› ä¸ºå®ƒå…·æœ‰å¯è¯»æ€§ã€æ•ˆæœå’Œéšç§ä¿æŠ¤èƒ½åŠ›ã€‚ç„¶è€Œï¼Œsecureboostå—åˆ°é«˜è®¡ç®—å¤æ‚æ€§å’Œæ ‡ç­¾æ³„éœ²çš„é£é™©ã€‚ä¸ºäº†æ¿€æ´»secureboostçš„å…¨éƒ¨æ½œåŠ›ï¼Œsecureboostçš„è¶…å‚æ•°åº”è¯¥ä»”ç»†é€‰æ‹©ï¼Œä»¥è¾¾åˆ°æœ€ä½³çš„å¹³è¡¡ç‚¹ã€‚ç°æœ‰çš„æ–¹æ³•å¯ä»¥é€šè¿‡å®éªŒæˆ–è§„åˆ™æ¥è®¾ç½®è¶…å‚æ•°ï¼Œä½†è¿™äº›æ–¹æ³•è¿œä¸å¤Ÿä¼˜åŒ–ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å—é™multi-ç›®æ ‡secureboostï¼ˆCMOSBï¼‰ç®—æ³•ï¼Œä»¥æ‰¾åˆ°Paretoä¼˜åŒ–è§£å†³æ–¹æ¡ˆï¼Œæ¯ä¸ªè§£å†³æ–¹æ¡ˆéƒ½æ˜¯ä¸€ç»„è¶…å‚æ•°ï¼Œå®ç°äº†UtilityæŸå¤±ã€è®­ç»ƒæˆæœ¬å’Œéšç§æ³„éœ²çš„ä¼˜åŒ–å¹³è¡¡ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸‰ä¸ªç›®æ ‡é‡è¡¨ç¤ºã€‚å…·ä½“æ¥è¯´ï¼Œéšç§æ³„éœ²è¢«æˆ‘ä»¬æå‡ºçš„å®ä¾‹åˆ’åˆ†æ”»å‡»æ¥åº¦é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCMOSBå¯ä»¥ä¸ä»…æä¾›è¶…å‚æ•°ä¼˜äºåŸºå‡†å€¼ï¼Œè¿˜å¯ä»¥æ‰¾åˆ°ä¼˜åŒ–çš„è¶…å‚æ•°é›†ï¼Œä»¥æ»¡è¶³è”åˆå­¦ä¹ å‚ä¸è€…çš„çµæ´»è¦æ±‚ã€‚
</details></li>
</ul>
<hr>
<h2 id="Boosting-Federated-Learning-Convergence-with-Prototype-Regularization"><a href="#Boosting-Federated-Learning-Convergence-with-Prototype-Regularization" class="headerlink" title="Boosting Federated Learning Convergence with Prototype Regularization"></a>Boosting Federated Learning Convergence with Prototype Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10575">http://arxiv.org/abs/2307.10575</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Qiao, Huy Q. Le, Choong Seon Hong</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨æé«˜ Federated Learning (FL) ä¸­çš„æ¨¡å‹æ€§èƒ½ï¼Œè§£å†³ Client é—´èµ„æ–™ä¸å‡åŒ€é—®é¢˜ã€‚</li>
<li>methods: æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäº Prototype çš„è°ƒæ•´ç­–ç•¥ï¼Œé€šè¿‡æœåŠ¡å™¨å°†åˆ†å¸ƒå¼ Client çš„æœ¬åœ° Prototype èšåˆæˆå…¨å±€ Prototypeï¼Œå°†å…¶ä¼ å›ä¸ªåˆ« Client è¿›è¡Œæœ¬åœ°è®­ç»ƒã€‚</li>
<li>results: å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ MNIST å’Œ Fashion-MNIST ä¸Šå¾—åˆ°äº†3.3% å’Œ8.9% çš„å¹³å‡æµ‹è¯•ç²¾åº¦æå‡ï¼Œç›¸æ¯”æœ€å—æ¬¢è¿çš„åŸºäº FedAvg çš„åŸºelineã€‚æ­¤å¤–ï¼Œæœ¬æ–¹æ³•åœ¨ä¸å‡åŒ€ç¯å¢ƒä¸‹å…·æœ‰å¿«é€Ÿçš„æ•´åˆé€Ÿç‡ã€‚<details>
<summary>Abstract</summary>
As a distributed machine learning technique, federated learning (FL) requires clients to collaboratively train a shared model with an edge server without leaking their local data. However, the heterogeneous data distribution among clients often leads to a decrease in model performance. To tackle this issue, this paper introduces a prototype-based regularization strategy to address the heterogeneity in the data distribution. Specifically, the regularization process involves the server aggregating local prototypes from distributed clients to generate a global prototype, which is then sent back to the individual clients to guide their local training. The experimental results on MNIST and Fashion-MNIST show that our proposal achieves improvements of 3.3% and 8.9% in average test accuracy, respectively, compared to the most popular baseline FedAvg. Furthermore, our approach has a fast convergence rate in heterogeneous settings.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¸ºäº†è§£å†³å®¢æˆ·ç«¯æ•°æ®ä¸å‡åŒ€æ€§çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåŸå‹çš„è§„èŒƒçº¦æŸç­–ç•¥ï¼Œç”¨äºåœ¨åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ ä¸­ååŒè®­ç»ƒå…±äº«æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œè§„èŒƒè¿‡ç¨‹åŒ…æ‹¬å°†åˆ†å¸ƒåœ¨å„å®¢æˆ·ç«¯ä¸Šçš„æœ¬åœ°åŸå‹ç”±æœåŠ¡å™¨è¿›è¡Œæ±‡æ€»ï¼Œç”Ÿæˆä¸€ä¸ªå…¨å±€åŸå‹ï¼Œç„¶åå°†è¯¥å…¨å±€åŸå‹å‘é€å›åˆ°å„ä¸ªå®¢æˆ·ç«¯ï¼Œä»¥ä¾›æœ¬åœ°è®­ç»ƒæŒ‡å¯¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€å¸¸ç”¨çš„åŸºå‡†æ–¹æ³•FedAvgç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ¡ˆåœ¨MNISTå’ŒFashion-MNISTä¸¤ä¸ªé¢„æµ‹é›†ä¸Šå¹³å‡æµ‹è¯•ç²¾åº¦æé«˜3.3%å’Œ8.9%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸å‡åŒ€è®¾ç½®ä¸‹å…·æœ‰å¿«é€Ÿæ”¶æ•›çš„ç‰¹ç‚¹ã€‚
</details></li>
</ul>
<hr>
<h2 id="Deceptive-Alignment-Monitoring"><a href="#Deceptive-Alignment-Monitoring" class="headerlink" title="Deceptive Alignment Monitoring"></a>Deceptive Alignment Monitoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10569">http://arxiv.org/abs/2307.10569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andres Carranza, Dhruv Pai, Rylan Schaeffer, Arnuv Tandon, Sanmi Koyejo</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨é˜²æ­¢å¤§æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ¬ºéª—æ€§è¡Œä¸ºï¼Œä»¥åŠæ£€æµ‹è¿™äº›æ¨¡å‹æ˜¯å¦åœ¨ä¸æ˜ç¡®çš„ç›®çš„ä¸‹è¿›è¡Œ modify å…¶è¡Œä¸ºã€‚</li>
<li>methods: æœ¬æ–‡æå‡ºäº†å¤šä¸ªä¸åŒçš„æœºå™¨å­¦ä¹ å­é¢†åŸŸçš„ç ”ç©¶æ–¹å‘ï¼Œä»¥æ£€æµ‹å’Œé˜²æ­¢æ¨¡å‹çš„æ¬ºéª—æ€§è¡Œä¸ºã€‚</li>
<li>results: æœ¬æ–‡è®¤ä¸ºï¼Œè¿™äº›ç ”ç©¶æ–¹å‘å°†åœ¨æœªæ¥å¯¹æ£€æµ‹å’Œé˜²æ­¢æ¨¡å‹çš„æ¬ºéª—æ€§è¡Œä¸ºèµ·åˆ°å…³é”®ä½œç”¨ï¼Œå¹¶ä¸”å°†ä¸ºæ•æ·æœºå™¨å­¦ä¹ ç¤¾åŒºå¸¦æ¥æ–°çš„ç ”ç©¶æœºé‡ã€‚<details>
<summary>Abstract</summary>
As the capabilities of large machine learning models continue to grow, and as the autonomy afforded to such models continues to expand, the spectre of a new adversary looms: the models themselves. The threat that a model might behave in a seemingly reasonable manner, while secretly and subtly modifying its behavior for ulterior reasons is often referred to as deceptive alignment in the AI Safety & Alignment communities. Consequently, we call this new direction Deceptive Alignment Monitoring. In this work, we identify emerging directions in diverse machine learning subfields that we believe will become increasingly important and intertwined in the near future for deceptive alignment monitoring, and we argue that advances in these fields present both long-term challenges and new research opportunities. We conclude by advocating for greater involvement by the adversarial machine learning community in these emerging directions.
</details>
<details>
<summary>æ‘˜è¦</summary>
éšç€å¤§æœºå™¨å­¦ä¹ æ¨¡å‹çš„èƒ½åŠ›ä¸æ–­å¢é•¿ï¼Œä»¥åŠè¿™äº›æ¨¡å‹çš„è‡ªä¸»æƒåŠ›ä¸æ–­æ‰©å±•ï¼Œä¸€ä¸ªæ–°çš„å¯¹æ‰‹å‡ºç°äº†ï¼šæ¨¡å‹æœ¬èº«ã€‚è¿™ç§å¨èƒè¢«ç§°ä¸ºâ€œæ¬ºéª—å¯åŠ¨â€ï¼ˆdeceptive alignmentï¼‰åœ¨AIå®‰å…¨ä¸Alignmentç¤¾åŒºä¸­ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†è¿™ä¸€æ–¹å‘ç§°ä¸ºâ€œæ¬ºéª—å¯åŠ¨ç›‘æµ‹â€ï¼ˆDeceptive Alignment Monitoringï¼‰ã€‚åœ¨è¿™ç§å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è®¤ä¸ºæœªæ¥å‡ å¹´å°†æˆä¸ºæ›´åŠ é‡è¦å’Œå…³é”®çš„æ–¹å‘ï¼Œå¹¶ä¸”è¿™äº›é¢†åŸŸçš„è¿›æ­¥å°†å¸¦æ¥é•¿æœŸæŒ‘æˆ˜å’Œæ–°çš„ç ”ç©¶æœºé‡ã€‚æˆ‘ä»¬æœ€ç»ˆå‘¼åäº†å¯¹æŠ—æœºå™¨å­¦ä¹ ç¤¾åŒºæ›´åŠ å‚ä¸è¿™äº›emergingæ–¹å‘ã€‚
</details></li>
</ul>
<hr>
<h2 id="FACADE-A-Framework-for-Adversarial-Circuit-Anomaly-Detection-and-Evaluation"><a href="#FACADE-A-Framework-for-Adversarial-Circuit-Anomaly-Detection-and-Evaluation" class="headerlink" title="FACADE: A Framework for Adversarial Circuit Anomaly Detection and Evaluation"></a>FACADE: A Framework for Adversarial Circuit Anomaly Detection and Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10563">http://arxiv.org/abs/2307.10563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhruv Pai, Andres Carranza, Rylan Schaeffer, Arnuv Tandon, Sanmi Koyejo</li>
<li>for: æé«˜æ¨¡å‹ robustness å’Œå¯è§æ€§ï¼Œå¹¶åº”å¯¹ adversarial æ”»å‡»</li>
<li>methods: åŸºäº probablistic å’Œå‡ ä½•å­¦çš„æ–¹æ³•ï¼Œæ¢ç´¢ activation space ä¸­ pseudo-class çš„æ€§è´¨å˜åŒ–ï¼Œæ‰¾åˆ° adversarial æ”»å‡»çš„æºå¤´</li>
<li>results: æä¾›äº†ä¸€ç§å¯é çš„ anomaly detection æ–¹æ³•ï¼Œå¯ä»¥å¸®åŠ©æé«˜æ¨¡å‹çš„å®‰å…¨æ€§å’Œå¯é æ€§ï¼Œå¹¶åº”ç”¨äºå®é™…åœºæ™¯ä¸­<details>
<summary>Abstract</summary>
We present FACADE, a novel probabilistic and geometric framework designed for unsupervised mechanistic anomaly detection in deep neural networks. Its primary goal is advancing the understanding and mitigation of adversarial attacks. FACADE aims to generate probabilistic distributions over circuits, which provide critical insights to their contribution to changes in the manifold properties of pseudo-classes, or high-dimensional modes in activation space, yielding a powerful tool for uncovering and combating adversarial attacks. Our approach seeks to improve model robustness, enhance scalable model oversight, and demonstrates promising applications in real-world deployment settings.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬ä»‹ç»FACADEï¼Œä¸€ä¸ªæ–°çš„æœºä¼šæ¦‚ç‡å’Œå‡ ä½•æ¡†æ¶ï¼Œç”¨äºæ— supervisedæœºå™¨å­¦ä¹ é¢†åŸŸä¸­çš„æœºå™¨å­¦ä¹ è¿‡ç¨‹ä¸­çš„éå¸¸ï¿½ynchronizeæ”»å‡»æ¢æµ‹ã€‚å…¶ä¸»è¦ç›®çš„æ˜¯æé«˜æœºå™¨å­¦ä¹ æ¨¡å‹çš„æŠ—å¹²æ‰°èƒ½åŠ›ï¼Œæé«˜å¯æ‰©å±•çš„æ¨¡å‹ç›‘æ§ï¼Œå¹¶åœ¨å®é™…åº”ç”¨ä¸­å±•ç¤ºäº†å¯é çš„åº”ç”¨ã€‚FACADEçš„ç›®æ ‡æ˜¯ç”Ÿæˆæœºä¼šæ¦‚ç‡åˆ†å¸ƒ Ğ½Ğ°Ğ´ circuitï¼Œä»è€Œè·å¾— Pseudo-classes æˆ–é«˜ç»´åº¦æ¨¡å¼åœ¨æ´»åŠ¨ç©ºé—´çš„å˜åŒ–ç‰¹å¾ï¼Œå®ç°äº†å¼ºå¤§çš„æ¢æµ‹å’ŒæŠ—å¹²æ‰°æ”»å‡»çš„å·¥å…·ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æé«˜æœºå™¨å­¦ä¹ æ¨¡å‹çš„ç±»åˆ«Robustnessï¼Œå¹¶ä¸”å¯ä»¥æ‰©å±•åˆ°å¯æ‰©å±•çš„æ¨¡å‹ç›‘æ§ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼ŒFACADE å±•ç¤ºäº†å¯é çš„åº”ç”¨ã€‚
</details></li>
</ul>
<hr>
<h2 id="Shared-Adversarial-Unlearning-Backdoor-Mitigation-by-Unlearning-Shared-Adversarial-Examples"><a href="#Shared-Adversarial-Unlearning-Backdoor-Mitigation-by-Unlearning-Shared-Adversarial-Examples" class="headerlink" title="Shared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial Examples"></a>Shared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial Examples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10562">http://arxiv.org/abs/2307.10562</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaokui Wei, Mingda Zhang, Hongyuan Zha, Baoyuan Wu</li>
<li>for: æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•ä½¿ç”¨å°é‡å‡€æ•°æ®çº§è”æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­çš„æ¶æ„æ”»å‡»æ¨è®®ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†é“¾æ¥æ¶æ„é£é™©å’Œæ•Œæ„é£é™©çš„è”ç³»ï¼Œ derivates a novel upper bound for backdoor risk, å¹¶æå‡ºäº†ä¸€ç§æ–°çš„äºŒçº§ä¼˜åŒ–é—®é¢˜æ¥ Mitigate backdoor æ”»å‡»ã€‚</li>
<li>results: å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•å¯ä»¥åœ¨ä¸åŒçš„ benchmark æ•°æ®é›†å’Œç½‘ç»œæ¶æ„ä¸Šè¾¾åˆ° state-of-the-art çš„æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
Backdoor attacks are serious security threats to machine learning models where an adversary can inject poisoned samples into the training set, causing a backdoored model which predicts poisoned samples with particular triggers to particular target classes, while behaving normally on benign samples. In this paper, we explore the task of purifying a backdoored model using a small clean dataset. By establishing the connection between backdoor risk and adversarial risk, we derive a novel upper bound for backdoor risk, which mainly captures the risk on the shared adversarial examples (SAEs) between the backdoored model and the purified model. This upper bound further suggests a novel bi-level optimization problem for mitigating backdoor using adversarial training techniques. To solve it, we propose Shared Adversarial Unlearning (SAU). Specifically, SAU first generates SAEs, and then, unlearns the generated SAEs such that they are either correctly classified by the purified model and/or differently classified by the two models, such that the backdoor effect in the backdoored model will be mitigated in the purified model. Experiments on various benchmark datasets and network architectures show that our proposed method achieves state-of-the-art performance for backdoor defense.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>>æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨å°æ¸…æ´æ•°æ®é›†æ¥çº¯åŒ–è¢«æ”»å‡»çš„æ¨¡å‹çš„ä»»åŠ¡ã€‚æˆ‘ä»¬å‘ç°äº†è¿æ¥èƒŒé—¨æ”»å‡»é£é™©å’Œæ•Œå¯¹æ”»å‡»é£é™©çš„å…³ç³»ï¼Œä»è€Œå¾—å‡ºäº†ä¸€ä¸ªæ–°çš„èƒŒé—¨é£é™©Upper boundï¼Œå®ƒä¸»è¦æ•æ‰äº†ç”±ä¸¤ä¸ªæ¨¡å‹å…±äº«çš„æ•Œå¯¹ç¤ºä¾‹ï¼ˆSAEsï¼‰æ‰€å¸¦æ¥çš„é£é™©ã€‚åŸºäºè¿™ä¸ªUpper boundï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä¸¤çº§ä¼˜åŒ–é—®é¢˜æ¥ Mitigate èƒŒé—¨æ”»å‡»ã€‚æˆ‘ä»¬ç§°ä¹‹ä¸ºå…±åŒæ•Œå¯¹å­¦ä¹ ï¼ˆSAUï¼‰ã€‚SAUé¦–å…ˆç”ŸæˆSAEsï¼Œç„¶åé€šè¿‡ä¸æ­£ç¡®åœ°åˆ†ç±»è¿™äº›SAEsæ¥å‡å°‘èƒŒé—¨æ•ˆæœåœ¨çº¯åŒ–åçš„æ¨¡å‹ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªbenchmarkæ•°æ®é›†å’Œç½‘ç»œæ¶æ„ä¸Šè¾¾åˆ°äº†èƒŒé—¨é˜²å¾¡çš„çŠ¶æ€ä¹‹ artsã€‚Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Post-variational-quantum-neural-networks"><a href="#Post-variational-quantum-neural-networks" class="headerlink" title="Post-variational quantum neural networks"></a>Post-variational quantum neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10560">http://arxiv.org/abs/2307.10560</a></li>
<li>repo_url: None</li>
<li>paper_authors: Po-Wei Huang, Patrick Rebentrost</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æå‡ºä¸€ç§ä½¿ç”¨æ··åˆé‡å­-ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğµè®¡ç®—å’Œå˜é‡ç®—æ³•æ¥è§£å†³é‡å­è®¡ç®—æœºå™¨ç¡¬ä»¶ä¸å¤Ÿå‘å±•çš„é—®é¢˜ï¼Œå¹¶ä¸”æé«˜é‡å­æ¨¡å‹ä¼˜åŒ–çš„æ•ˆç‡ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†æ··åˆé‡å­-ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğµè®¡ç®—å’Œå˜é‡ç®—æ³•ï¼Œå¹¶æå‡ºäº†â€œåå˜é‡ç­–ç•¥â€ï¼Œå³å°†è°ƒæ•´å‚æ•°ä»é‡å­è®¡ç®—æœºå™¨ä¼ é€’åˆ°ç±»å‹è®¡ç®—æœºå™¨ä¸Šè¿›è¡Œä¼˜åŒ–ã€‚ ensembleç­–ç•¥å’Œæ„å»ºä¸ªåˆ«é‡å­ç”µè·¯çš„è®¾è®¡åŸåˆ™ä¹Ÿè¢«è®¨è®ºã€‚</li>
<li>results: æœ¬ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨åå˜é‡ç­–ç•¥å¯ä»¥æé«˜é‡å­æ¨¡å‹çš„ä¼˜åŒ–æ•ˆç‡ï¼Œå¹¶ä¸”å¯ä»¥åº”ç”¨äºå®é™…åº”ç”¨åœºæ™¯å¦‚æ‰‹å†™å­—ç¬¦è¯†åˆ«ï¼Œå®ç°96%çš„åˆ†ç±»ç²¾åº¦ã€‚<details>
<summary>Abstract</summary>
Quantum computing has the potential to provide substantial computational advantages over current state-of-the-art classical supercomputers. However, current hardware is not advanced enough to execute fault-tolerant quantum algorithms. An alternative of using hybrid quantum-classical computing with variational algorithms can exhibit barren plateau issues, causing slow convergence of gradient-based optimization techniques. In this paper, we discuss "post-variational strategies", which shift tunable parameters from the quantum computer to the classical computer, opting for ensemble strategies when optimizing quantum models. We discuss various strategies and design principles for constructing individual quantum circuits, where the resulting ensembles can be optimized with convex programming. Further, we discuss architectural designs of post-variational quantum neural networks and analyze the propagation of estimation errors throughout such neural networks. Lastly, we show that our algorithm can be applied to real-world applications such as image classification on handwritten digits, producing a 96% classification accuracy.
</details>
<details>
<summary>æ‘˜è¦</summary>
é‡å­è®¡ç®—å…·æœ‰æä¾›ç°ä»£ĞºĞ»Ğ°ÑÑĞ¸icalè¶…çº§è®¡ç®—æœºsubstantialè®¡ç®—ä¼˜åŠ¿çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰ç¡¬ä»¶è¿˜ä¸å¤Ÿå…ˆè¿›ï¼Œæ— æ³•æ‰§è¡Œ fault-tolerant é‡å­ç®—æ³•ã€‚ä½œä¸ºalternativeï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨hybridé‡å­-ĞºĞ»Ğ°ÑÑĞ¸icalè®¡ç®—ï¼Œä½¿ç”¨å˜é‡ç®—æ³•ï¼Œä½†è¿™ä¼šå¯¼è‡´æ¶åŠ¿åƒåœ¾æ¿å—é—®é¢˜ï¼Œä½¿å¾—æ¢¯åº¦åŸºäºä¼˜åŒ–æŠ€æœ¯çš„æ•æ„Ÿåº¦å˜æ…¢ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†â€œåå˜é‡ç­–ç•¥â€ï¼Œå³å°†å¯è°ƒå‚æ•°ä»é‡å­è®¡ç®—æœºshiftåˆ°ç±»å‹è®¡ç®—æœºï¼Œé€‰æ‹©ensembleç­–ç•¥æ¥ä¼˜åŒ–é‡å­æ¨¡å‹ã€‚æˆ‘ä»¬è®¨è®ºäº†å„ç§ç­–ç•¥å’Œè®¾è®¡åŸåˆ™ï¼Œç”¨äºæ„å»ºä¸ªæ€§åŒ–çš„é‡å­Circuitï¼Œå…¶ç»“æœå¯ä»¥é€šè¿‡å‡¸å‹Programmingä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¨è®ºäº†post-variationalé‡å­ç¥ç»ç½‘ç»œçš„å»ºç­‘è®¾è®¡ï¼Œå¹¶åˆ†æäº†ä¼°è®¡é”™è¯¯åœ¨suchç¥ç»ç½‘ç»œä¸­çš„ä¼ æ’­ã€‚æœ€åï¼Œæˆ‘ä»¬ç¤ºå‡ºäº†æˆ‘ä»¬çš„ç®—æ³•å¯ä»¥åº”ç”¨äºå®é™…åº”ç”¨åœºæ™¯ï¼Œå¦‚æ‰‹å†™æ•°å­—è¯†åˆ«ï¼Œå¹¶è¾¾åˆ°96%çš„åˆ†ç±»ç²¾åº¦ã€‚
</details></li>
</ul>
<hr>
<h2 id="Air-Traffic-Controller-Workload-Level-Prediction-using-Conformalized-Dynamical-Graph-Learning"><a href="#Air-Traffic-Controller-Workload-Level-Prediction-using-Conformalized-Dynamical-Graph-Learning" class="headerlink" title="Air Traffic Controller Workload Level Prediction using Conformalized Dynamical Graph Learning"></a>Air Traffic Controller Workload Level Prediction using Conformalized Dynamical Graph Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10559">http://arxiv.org/abs/2307.10559</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ymlasu/para-atm-collection">https://github.com/ymlasu/para-atm-collection</a></li>
<li>paper_authors: Yutian Pang, Jueming Hu, Christopher S. Lieber, Nancy J. Cooke, Yongming Liu</li>
<li>for: é¢„æµ‹ç©ºäº¤æ§åˆ¶å‘˜ï¼ˆATCoï¼‰çš„å·¥ä½œè´Ÿæ‹…ï¼Œä»¥æé«˜èˆªç©ºä¸šåŠ¡æ“ä½œçš„å®‰å…¨æ€§å’Œç©ºé—´åˆ©ç”¨ç‡ã€‚</li>
<li>methods: ä½¿ç”¨äººç±»åœ¨Loopï¼ˆHITLï¼‰ simulations with retired ATCoï¼Œå¹¶å¯¹å®é™…èˆªç©ºæ•°æ®å’Œå·¥ä½œè´Ÿæ‹…æ ‡ç­¾è¿›è¡Œåˆ†æã€‚æè®®ä½¿ç”¨å›¾è¡¨æ·±åº¦å­¦ä¹ æ¡†æ¶å’ŒåFormsé¢„æµ‹ATCoå·¥ä½œè´Ÿæ‹…æ°´å¹³ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œ besides äº¤é€šå¯†åº¦ç‰¹å¾ï¼Œäº¤é€šå†²çªç‰¹å¾ä¹Ÿå¯¹å·¥ä½œè´Ÿæ‹…é¢„æµ‹åšå‡ºè´¡çŒ®ï¼ˆå³æœ€å°æ°´å¹³&#x2F;å‚ç›´åˆ†ç¦»è·ç¦»ï¼‰ã€‚ directly learning fromç©ºé—´æ—¶é—´å›¾åƒçš„ç©ºé—´ç‰¹å¾å¯ä»¥æé«˜é¢„æµ‹ç²¾åº¦ï¼Œæ¯”æ‰‹å·¥è®¾è®¡çš„äº¤é€šå¤æ‚åº¦ç‰¹å¾æ›´é«˜ã€‚ conformal prediction æ˜¯ä¸€ç§æœ‰ä»·å€¼çš„å·¥å…·ï¼Œå¯ä»¥è¿›ä¸€æ­¥æé«˜é¢„æµ‹ç²¾åº¦ï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªèŒƒå›´å†…çš„é¢„æµ‹å·¥ä½œè´Ÿæ‹…æ ‡ç­¾ã€‚<details>
<summary>Abstract</summary>
Air traffic control (ATC) is a safety-critical service system that demands constant attention from ground air traffic controllers (ATCos) to maintain daily aviation operations. The workload of the ATCos can have negative effects on operational safety and airspace usage. To avoid overloading and ensure an acceptable workload level for the ATCos, it is important to predict the ATCos' workload accurately for mitigation actions. In this paper, we first perform a review of research on ATCo workload, mostly from the air traffic perspective. Then, we briefly introduce the setup of the human-in-the-loop (HITL) simulations with retired ATCos, where the air traffic data and workload labels are obtained. The simulations are conducted under three Phoenix approach scenarios while the human ATCos are requested to self-evaluate their workload ratings (i.e., low-1 to high-7). Preliminary data analysis is conducted. Next, we propose a graph-based deep-learning framework with conformal prediction to identify the ATCo workload levels. The number of aircraft under the controller's control varies both spatially and temporally, resulting in dynamically evolving graphs. The experiment results suggest that (a) besides the traffic density feature, the traffic conflict feature contributes to the workload prediction capabilities (i.e., minimum horizontal/vertical separation distance); (b) directly learning from the spatiotemporal graph layout of airspace with graph neural network can achieve higher prediction accuracy, compare to hand-crafted traffic complexity features; (c) conformal prediction is a valuable tool to further boost model prediction accuracy, resulting a range of predicted workload labels. The code used is available at \href{https://github.com/ymlasu/para-atm-collection/blob/master/air-traffic-prediction/ATC-Workload-Prediction/}{$\mathsf{Link}$}.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç©ºäº¤æ§åˆ¶ï¼ˆATCï¼‰æ˜¯ä¸€ä¸ªå®‰å…¨å…³é”®çš„æœåŠ¡ç³»ç»Ÿï¼Œéœ€è¦åœ°é¢ç©ºäº¤æ§åˆ¶å‘˜ï¼ˆATCoï¼‰ä¸æ–­æ³¨æ„ä»¥ç»´æŠ¤æ¯å¤©çš„èˆªç©ºè¿è¾“ä¸šåŠ¡ã€‚ATCoçš„å·¥ä½œè´Ÿæ‹…å¯èƒ½ä¼šå¯¹æ“ä½œå®‰å…¨å’Œç©ºåŸŸä½¿ç”¨äº§ç”Ÿè´Ÿé¢å½±å“ã€‚ä¸ºäº†é¿å…è¿‡è½½å’Œç¡®ä¿ATCoçš„å·¥ä½œè´Ÿæ‹…æ°´å¹³æ¥å—ï¼Œéœ€è¦å‡†ç¡®é¢„æµ‹ATCoçš„å·¥ä½œè´Ÿæ‹…ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆè¿›è¡Œäº†ç ”ç©¶äººå‘˜å¯¹ATCoå·¥ä½œè´Ÿæ‹…çš„è¯„ä¼°ï¼Œä¸»è¦æ¥è‡ªç©ºäº¤ perspectiveã€‚ç„¶åï¼Œæˆ‘ä»¬ briefly introduceäº†äººåœ¨Loopï¼ˆHITLï¼‰ simulations with retired ATCosï¼Œå…¶ä¸­è·å–äº†ç©ºäº¤æ•°æ®å’Œå·¥ä½œè´Ÿæ‹…æ ‡ç­¾ã€‚ simulations were conducted under three Phoenix approach scenarios, while the human ATCos were asked to self-evaluate their workload ratings (i.e., low-1 to high-7). Preliminary data analysis was conducted. Next, we proposed a graph-based deep-learning framework with conformal prediction to identify the ATCo workload levels. The number of aircraft under the controller's control varies both spatially and temporally, resulting in dynamically evolving graphs. The experiment results suggest that (a) besides the traffic density feature, the traffic conflict feature also contributes to the workload prediction capabilities (i.e., minimum horizontal/vertical separation distance); (b) directly learning from the spatiotemporal graph layout of airspace with graph neural network can achieve higher prediction accuracy, compared to hand-crafted traffic complexity features; (c) conformal prediction is a valuable tool to further boost model prediction accuracy, resulting in a range of predicted workload labels. The code used is available at $\mathsf{Link}$.
</details></li>
</ul>
<hr>
<h2 id="SC-VALL-E-Style-Controllable-Zero-Shot-Text-to-Speech-Synthesizer"><a href="#SC-VALL-E-Style-Controllable-Zero-Shot-Text-to-Speech-Synthesizer" class="headerlink" title="SC VALL-E: Style-Controllable Zero-Shot Text to Speech Synthesizer"></a>SC VALL-E: Style-Controllable Zero-Shot Text to Speech Synthesizer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10550">http://arxiv.org/abs/2307.10550</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/0913ktg/sc_vall-e">https://github.com/0913ktg/sc_vall-e</a></li>
<li>paper_authors: Daegyeom Kim, Seongho Hong, Yong-Hoon Choi</li>
<li>For: The paper is written to propose a style control (SC) VALL-E model for expressive speech synthesis, which can generate diverse voices with controllable attributes such as emotion, speaking rate, pitch, and voice intensity.* Methods: The SC VALL-E model is based on the neural codec language model (VALE) and the generative pretrained transformer 3 (GPT-3), and it uses a newly designed style network to control the attributes of the generated speech. The model takes input from text sentences and prompt audio and is trained to generate controllable speech that can mimic the characteristics of the prompt audio.* Results: The paper conducts comparative experiments with three representative expressive speech synthesis models and measures word error rate (WER), F0 voiced error (FVE), and F0 gross pitch error (F0GPE) as evaluation metrics. The results show that the SC VALL-E model demonstrates competitive performance compared to the existing models and can generate a variety of expressive sounds with controllable attributes.<details>
<summary>Abstract</summary>
Expressive speech synthesis models are trained by adding corpora with diverse speakers, various emotions, and different speaking styles to the dataset, in order to control various characteristics of speech and generate the desired voice. In this paper, we propose a style control (SC) VALL-E model based on the neural codec language model (called VALL-E), which follows the structure of the generative pretrained transformer 3 (GPT-3). The proposed SC VALL-E takes input from text sentences and prompt audio and is designed to generate controllable speech by not simply mimicking the characteristics of the prompt audio but by controlling the attributes to produce diverse voices. We identify tokens in the style embedding matrix of the newly designed style network that represent attributes such as emotion, speaking rate, pitch, and voice intensity, and design a model that can control these attributes. To evaluate the performance of SC VALL-E, we conduct comparative experiments with three representative expressive speech synthesis models: global style token (GST) Tacotron2, variational autoencoder (VAE) Tacotron2, and original VALL-E. We measure word error rate (WER), F0 voiced error (FVE), and F0 gross pitch error (F0GPE) as evaluation metrics to assess the accuracy of generated sentences. For comparing the quality of synthesized speech, we measure comparative mean option score (CMOS) and similarity mean option score (SMOS). To evaluate the style control ability of the generated speech, we observe the changes in F0 and mel-spectrogram by modifying the trained tokens. When using prompt audio that is not present in the training data, SC VALL-E generates a variety of expressive sounds and demonstrates competitive performance compared to the existing models. Our implementation, pretrained models, and audio samples are located on GitHub.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¹²æ”¯è¡¨è¾¾ synthesis æ¨¡å‹é€šå¸¸é€šè¿‡æ·»åŠ å…·æœ‰å¤šä¸ªè¯´è¯è€…ã€ä¸åŒæƒ…æ„Ÿå’Œä¸åŒè¯´è¯é£æ ¼çš„æ–‡æœ¬ corpus æ¥è®­ç»ƒï¼Œä»¥æ§åˆ¶ä¸åŒç‰¹æ€§çš„è¯­éŸ³å¹¶ç”Ÿæˆæ„Ÿå…´è¶£çš„å£°éŸ³ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäº neural codec è¯­è¨€æ¨¡å‹ï¼ˆç§°ä¸º VALL-Eï¼‰çš„é£æ ¼æ§åˆ¶ï¼ˆSCï¼‰ VALLE æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨ VALL-E çš„ç»“æ„ä¸Šæ·»åŠ äº†ä¸€ä¸ªæ–°çš„é£æ ¼ç½‘ç»œï¼Œå¹¶åœ¨è¿™ä¸ªé£æ ¼ç½‘ç»œä¸­æ ‡è¯†äº†ä¸åŒçš„ç‰¹å¾è¡¨è¾¾ï¼Œå¦‚æƒ…æ„Ÿã€è¯´è¯é€Ÿåº¦ã€éŸ³é«˜å’Œå£°éŸ³å¼ºåº¦ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¯æ§åˆ¶è¿™äº›ç‰¹å¾çš„æ¨¡å‹ã€‚ä¸ºè¯„ä¼° SC VALL-E çš„è¡¨ç°ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸ä¸‰ç§å¸¸è§çš„è¡¨è¾¾æ€§è¯­éŸ³åˆæˆæ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼šglobal style tokenï¼ˆGSTï¼‰ Tacotron2ã€variational autoencoderï¼ˆVAEï¼‰ Tacotron2 å’ŒåŸå§‹ VALL-Eã€‚æˆ‘ä»¬ä½¿ç”¨ word error rateï¼ˆWERï¼‰ã€F0 voiced errorï¼ˆFVEï¼‰å’Œ F0 gross pitch errorï¼ˆF0GPEï¼‰ä½œä¸ºè¯„ä¼° metricã€‚ä¸ºæ¯”è¾ƒç”Ÿæˆçš„è¯­éŸ³è´¨é‡ï¼Œæˆ‘ä»¬ä½¿ç”¨ comparative mean option scoreï¼ˆCMOSï¼‰å’Œ similarity mean option scoreï¼ˆSMOSï¼‰ã€‚ä¸ºè¯„ä¼°ç”Ÿæˆçš„è¯­éŸ³é£æ ¼æ§åˆ¶èƒ½åŠ›ï¼Œæˆ‘ä»¬è§‚å¯Ÿäº† F0 å’Œ mel-spectrogram çš„å˜åŒ–ã€‚å½“ä½¿ç”¨ä¸åœ¨è®­ç»ƒæ•°æ®ä¸­çš„æç¤ºéŸ³æ—¶ï¼ŒSC VALL-E èƒ½å¤Ÿç”Ÿæˆå¤šç§è¡¨è¾¾æ€§çš„å£°éŸ³ï¼Œå¹¶ä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚æˆ‘ä»¬çš„å®ç°ã€é¢„è®­ç»ƒæ¨¡å‹å’Œå£°éŸ³æ ·æœ¬ä½äº GitHubã€‚
</details></li>
</ul>
<hr>
<h2 id="Differentially-Flat-Learning-based-Model-Predictive-Control-Using-a-Stability-State-and-Input-Constraining-Safety-Filter"><a href="#Differentially-Flat-Learning-based-Model-Predictive-Control-Using-a-Stability-State-and-Input-Constraining-Safety-Filter" class="headerlink" title="Differentially Flat Learning-based Model Predictive Control Using a Stability, State, and Input Constraining Safety Filter"></a>Differentially Flat Learning-based Model Predictive Control Using a Stability, State, and Input Constraining Safety Filter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10541">http://arxiv.org/abs/2307.10541</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/utiasdsl/fmpc_socp">https://github.com/utiasdsl/fmpc_socp</a></li>
<li>paper_authors: Adam W. Hall, Melissa Greeff, Angela P. Schoellig<br>for: learning-based optimal control algorithms for unknown systemsmethods: exploits differential flatness, nonlinear transformation learned as a Gaussian process, safety filter, two successive convex optimizationsresults: similar performance to state-of-the-art learning-based controllers, significantly better computational efficiency, respects flat state and input constraints, guarantees stability<details>
<summary>Abstract</summary>
Learning-based optimal control algorithms control unknown systems using past trajectory data and a learned model of the system dynamics. These controllers use either a linear approximation of the learned dynamics, trading performance for faster computation, or nonlinear optimization methods, which typically perform better but can limit real-time applicability. In this work, we present a novel nonlinear controller that exploits differential flatness to achieve similar performance to state-of-the-art learning-based controllers but with significantly less computational effort. Differential flatness is a property of dynamical systems whereby nonlinear systems can be exactly linearized through a nonlinear input mapping. Here, the nonlinear transformation is learned as a Gaussian process and is used in a safety filter that guarantees, with high probability, stability as well as input and flat state constraint satisfaction. This safety filter is then used to refine inputs from a flat model predictive controller to perform constrained nonlinear learning-based optimal control through two successive convex optimizations. We compare our method to state-of-the-art learning-based control strategies and achieve similar performance, but with significantly better computational efficiency, while also respecting flat state and input constraints, and guaranteeing stability.
</details>
<details>
<summary>æ‘˜è¦</summary>
å­¦ä¹ åŸºäºçš„ä¼˜åŒ–æ§åˆ¶ç®—æ³•å¯ä»¥æ§åˆ¶æœªçŸ¥ç³»ç»Ÿä½¿ç”¨è¿‡å»è½¨è¿¹æ•°æ®å’Œå­¦ä¹ åˆ°ç³»ç»ŸåŠ¨åŠ›å­¦æ¨¡å‹ã€‚è¿™äº›æ§åˆ¶å™¨ä½¿ç”¨ Ğ»Ğ¸Ğ½ĞµĞ¹åŒ–å­¦ä¹ åŠ¨åŠ›å­¦æ¨¡å‹ï¼Œæ¢å–æ›´å¿«çš„è®¡ç®—é€Ÿåº¦ï¼Œæˆ–è€…éçº¿æ€§ä¼˜åŒ–æ–¹æ³•ï¼Œé€šå¸¸è¡¨ç°æ›´å¥½ï¼Œä½†å¯èƒ½é™åˆ¶å®æ—¶åº”ç”¨ã€‚åœ¨è¿™ç§å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„éçº¿æ€§æ§åˆ¶å™¨ï¼Œåˆ©ç”¨å·®åˆ†å¹³å‡¡æ€§æ¥å®ç°ä¸ç°æœ‰å­¦ä¹ åŸºäºæ§åˆ¶ç­–ç•¥ç›¸ä¼¼çš„æ€§èƒ½ï¼Œä½†è®¡ç®—æ•ˆç‡æ˜æ˜¾æ›´é«˜ã€‚å·®åˆ†å¹³å‡¡æ€§æ˜¯åŠ¨åŠ›ç³»ç»Ÿçš„æ€§è´¨ï¼Œé€šè¿‡éçº¿æ€§è¾“å…¥æ˜ å°„æ¥å°†éçº¿æ€§ç³»ç»Ÿ Linearizeã€‚åœ¨è¿™é‡Œï¼Œéçº¿æ€§å˜æ¢æ˜¯é€šè¿‡ Gaussian Process å­¦ä¹ çš„ï¼Œå¹¶ç”¨äºå®‰å…¨ç­›é€‰å™¨ï¼Œä¿è¯é«˜æ¦‚ç‡ç¨³å®šæ€§å’Œè¾“å…¥å’Œå¹³å‡¡çŠ¶æ€çº¦æŸçš„æ»¡è¶³ã€‚è¿™ä¸ªå®‰å…¨ç­›é€‰å™¨ç„¶åç”¨äºæ”¹è¿›ç”±å¹³å‡¡æ¨¡å‹é¢„æµ‹æ§åˆ¶å™¨è¾“å‡ºçš„è¾“å…¥ï¼Œé€šè¿‡ä¸¤æ¬¡ convex ä¼˜åŒ–æ¥å®ç°å—é™åˆ¶çš„éçº¿æ€§å­¦ä¹ åŸºäºä¼˜åŒ–æ§åˆ¶ã€‚æˆ‘ä»¬ä¸å½“å‰å­¦ä¹ åŸºäºæ§åˆ¶ç­–ç•¥è¿›è¡Œæ¯”è¾ƒï¼Œå®ç°ç›¸ä¼¼çš„æ€§èƒ½ï¼Œä½†è®¡ç®—æ•ˆç‡æ˜æ˜¾æ›´é«˜ï¼ŒåŒæ—¶ä¹Ÿéµå®ˆå¹³å‡¡çŠ¶æ€å’Œè¾“å…¥çº¦æŸï¼Œå¹¶ä¿è¯ç¨³å®šæ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="The-Extractive-Abstractive-Axis-Measuring-Content-â€œBorrowingâ€-in-Generative-Language-Models"><a href="#The-Extractive-Abstractive-Axis-Measuring-Content-â€œBorrowingâ€-in-Generative-Language-Models" class="headerlink" title="The Extractive-Abstractive Axis: Measuring Content â€œBorrowingâ€ in Generative Language Models"></a>The Extractive-Abstractive Axis: Measuring Content â€œBorrowingâ€ in Generative Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11779">http://arxiv.org/abs/2307.11779</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nedelina Teneva</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨ç”Ÿæˆæ¨¡å‹çš„æŠ½è±¡æ€§å’Œå†…å®¹æˆæƒé—®é¢˜ï¼Œå¹¶æå‡ºäº†EXTRACTIVE-ABSTRACTIVEè½´æ¥è¯„ä¼°ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†ç”Ÿæˆæ¨¡å‹å¯¹æ–‡æœ¬æ•°æ®è¿›è¡Œç”Ÿæˆå’ŒæŠ½è±¡ï¼Œå¹¶å¯¹ç”Ÿæˆç»“æœè¿›è¡Œè¯„ä¼°ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œç”Ÿæˆæ¨¡å‹çš„æŠ½è±¡æ€§å’Œå†…å®¹æˆæƒé—®é¢˜éœ€è¦æ›´åŠ é‡è§†ï¼Œå¹¶æå‡ºäº†å¯¹ç”Ÿæˆæ¨¡å‹çš„è¯„ä¼°æŒ‡æ ‡ã€æ•°æ®é›†å’Œæ³¨è§£æŒ‡å—ã€‚<details>
<summary>Abstract</summary>
Generative language models produce highly abstractive outputs by design, in contrast to extractive responses in search engines. Given this characteristic of LLMs and the resulting implications for content Licensing & Attribution, we propose the the so-called Extractive-Abstractive axis for benchmarking generative models and highlight the need for developing corresponding metrics, datasets and annotation guidelines. We limit our discussion to the text modality.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>>å°†æ–‡æœ¬è½¬æ¢ä¸ºç®€åŒ–ä¸­æ–‡ã€‚<</SYS>>ç”Ÿæˆè¯­è¨€æ¨¡å‹ä¼šç”Ÿæˆé«˜åº¦æŠ½è±¡çš„è¾“å‡ºï¼Œä¸æœç´¢å¼•æ“çš„EXTRACTIVE responsesä¸åŒï¼Œè¿™ç§ç‰¹ç‚¹å¯¹å†…å®¹æˆæƒå’Œå½’åŠŸæœ‰é‡è¦å½±å“ã€‚æˆ‘ä»¬æå‡ºEXTRACTIVE-ABSTRACTIVEè½´æ¥è¯„ä¼°ç”Ÿæˆæ¨¡å‹ï¼Œå¹¶éœ€è¦å¼€å‘ç›¸åº”çš„æŒ‡æ ‡ã€æ•°æ®é›†å’Œæ³¨é‡ŠæŒ‡å—ã€‚æˆ‘ä»¬åªé™åˆ¶äºæ–‡æœ¬ modalã€‚
</details></li>
</ul>
<hr>
<h2 id="Fast-Unsupervised-Deep-Outlier-Model-Selection-with-Hypernetworks"><a href="#Fast-Unsupervised-Deep-Outlier-Model-Selection-with-Hypernetworks" class="headerlink" title="Fast Unsupervised Deep Outlier Model Selection with Hypernetworks"></a>Fast Unsupervised Deep Outlier Model Selection with Hypernetworks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10529">http://arxiv.org/abs/2307.10529</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueying Ding, Yue Zhao, Leman Akoglu</li>
<li>for: è¿™ç¯‡è®ºæ–‡ä¸»è¦é’ˆå¯¹æ— ç›‘ç£çš„åå·®æ£€æµ‹ï¼ˆOutlier Detectionï¼ŒODï¼‰ä¸­çš„é—®é¢˜ï¼Œå³æœ‰æ•ˆåœ°è°ƒæ•´ï¼ˆHyperparameterï¼ŒHPï¼‰çš„é€‰æ‹©å’Œä¼˜åŒ–ã€‚</li>
<li>methods: æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºHYPERçš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡è®¾è®¡å’Œè®­ç»ƒä¸€ä¸ªæ–°çš„å¯¹åº”ç½‘ç»œï¼ˆHypernetworkï¼ŒHNï¼‰ï¼Œå°† HP æ˜ å°„åˆ°ODæ¨¡å‹çš„ä¼˜åŒ–å‚æ•°ã€‚æ­¤å¤–ï¼ŒHYPERè¿˜ä½¿ç”¨äº†å…ƒå­¦ä¹ æ¥è®­ç»ƒä¸€ä¸ªä»£ç†éªŒè¯å‡½æ•°ï¼Œä»¥æœ‰æ•ˆåœ° validate OD æ¨¡å‹ã€‚</li>
<li>results: å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHYPER åœ¨ 35 ä¸ª OD ä»»åŠ¡ä¸Šå®ç°äº†é«˜æ€§èƒ½ï¼Œå¹¶ä¸ 8 ä¸ªåŸºeline æ¯”è¾ƒå¾—åˆ°äº†æ˜¾è‘—çš„æ•ˆç‡ä¼˜åŠ¿ã€‚<details>
<summary>Abstract</summary>
Outlier detection (OD) finds many applications with a rich literature of numerous techniques. Deep neural network based OD (DOD) has seen a recent surge of attention thanks to the many advances in deep learning. In this paper, we consider a critical-yet-understudied challenge with unsupervised DOD, that is, effective hyperparameter (HP) tuning/model selection. While several prior work report the sensitivity of OD models to HPs, it becomes ever so critical for the modern DOD models that exhibit a long list of HPs. We introduce HYPER for tuning DOD models, tackling two fundamental challenges: (1) validation without supervision (due to lack of labeled anomalies), and (2) efficient search of the HP/model space (due to exponential growth in the number of HPs). A key idea is to design and train a novel hypernetwork (HN) that maps HPs onto optimal weights of the main DOD model. In turn, HYPER capitalizes on a single HN that can dynamically generate weights for many DOD models (corresponding to varying HPs), which offers significant speed-up. In addition, it employs meta-learning on historical OD tasks with labels to train a proxy validation function, likewise trained with our proposed HN efficiently. Extensive experiments on 35 OD tasks show that HYPER achieves high performance against 8 baselines with significant efficiency gains.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Beyond-Black-Box-Advice-Learning-Augmented-Algorithms-for-MDPs-with-Q-Value-Predictions"><a href="#Beyond-Black-Box-Advice-Learning-Augmented-Algorithms-for-MDPs-with-Q-Value-Predictions" class="headerlink" title="Beyond Black-Box Advice: Learning-Augmented Algorithms for MDPs with Q-Value Predictions"></a>Beyond Black-Box Advice: Learning-Augmented Algorithms for MDPs with Q-Value Predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10524">http://arxiv.org/abs/2307.10524</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tongxin Li, Yiheng Lin, Shaolei Ren, Adam Wierman</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ—¨åœ¨ç ”ç©¶åœ¨å•è½¨è¿¹ Markovå†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ä¸­çš„ä¸€è‡´æ€§å’Œå¯é æ€§çš„è´Ÿæ‹…å…³ç³»ï¼Œå¹¶åœ¨ä¸å¯ä¿¡ advise çš„æƒ…å†µä¸‹è¿›è¡Œç ”ç©¶ã€‚</li>
<li>methods: è¯¥è®ºæ–‡ä½¿ç”¨ Q-å€¼å»ºè®®æ¥ç ”ç©¶ä¸€è‡´æ€§å’Œå¯é æ€§çš„è´Ÿæ‹…å…³ç³»ï¼Œå¹¶åœ¨æ™®é€š MDP æ¨¡å‹ä¸­åŒ…æ‹¬äº†è¿ç»­å’Œç¦»æ•£çŠ¶æ€&#x2F;åŠ¨ä½œç©ºé—´ã€‚</li>
<li>results: ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šè¿‡ä½¿ç”¨ Q-å€¼å»ºè®®ï¼Œå¯ä»¥åœ¨ä¸å¯ä¿¡ advise çš„æƒ…å†µä¸‹å®ç°è¿‘ä¼¼ä¼˜åŒ–çš„æ€§èƒ½ä¿è¯ï¼Œå¹¶ä¸”æ¯”é solely black-box advise å¯ä»¥è·å¾—æ›´é«˜çš„æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
We study the tradeoff between consistency and robustness in the context of a single-trajectory time-varying Markov Decision Process (MDP) with untrusted machine-learned advice. Our work departs from the typical approach of treating advice as coming from black-box sources by instead considering a setting where additional information about how the advice is generated is available. We prove a first-of-its-kind consistency and robustness tradeoff given Q-value advice under a general MDP model that includes both continuous and discrete state/action spaces. Our results highlight that utilizing Q-value advice enables dynamic pursuit of the better of machine-learned advice and a robust baseline, thus result in near-optimal performance guarantees, which provably improves what can be obtained solely with black-box advice.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬ç ”ç©¶äº†åœ¨å•è½¨æ—¶å˜Markovå†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ä¸­çš„ä¸€è‡´æ€§å’Œå¯é æ€§çš„è´¸æ˜“ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ä¼ ç»Ÿé€”å¾„ä¸åŒï¼Œå³å¯¹å»ºè®®è§†ä¸ºé»‘ç›’æ¥æºçš„åšæ³•ã€‚ç›¸åï¼Œæˆ‘ä»¬è€ƒè™‘äº†ä¸€ç§æƒ…å†µï¼Œåœ¨è¯¥æƒ…å†µä¸‹ï¼Œå»ºè®®çš„ç”Ÿæˆæ–¹å¼å…·æœ‰æ›´å¤šçš„ä¿¡æ¯ã€‚æˆ‘ä»¬è¯æ˜äº†ä¸€ç§é¦–æ¬¡çš„ä¸€è‡´æ€§å’Œå¯é æ€§è´¸æ˜“ï¼ŒåŸºäºQå€¼å»ºè®®åœ¨é€šç”¨MDPæ¨¡å‹ä¸­ï¼Œè¯¥æ¨¡å‹åŒ…æ‹¬è¿ç»­å’Œç¦»æ•£çŠ¶æ€/åŠ¨ä½œç©ºé—´ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œé€šè¿‡åˆ©ç”¨Qå€¼å»ºè®®ï¼Œå¯ä»¥åœ¨æœºå™¨å­¦ä¹ å»ºè®®å’Œä¸€ä¸ªå¯é åŸºç¡€çº¿ä¸ŠåŠ¨æ€è¿½æ±‚æ›´å¥½çš„æ€§èƒ½ï¼Œä»è€Œè·å¾—ä¼˜åŒ–çš„æ€§èƒ½ä¿è¯ï¼Œè¿™ä¸é»‘ç›’å»ºè®® alone æ— æ³•è¾¾åˆ°ã€‚
</details></li>
</ul>
<hr>
<h2 id="Prediction-of-Handball-Matches-with-Statistically-Enhanced-Learning-via-Estimated-Team-Strengths"><a href="#Prediction-of-Handball-Matches-with-Statistically-Enhanced-Learning-via-Estimated-Team-Strengths" class="headerlink" title="Prediction of Handball Matches with Statistically Enhanced Learning via Estimated Team Strengths"></a>Prediction of Handball Matches with Statistically Enhanced Learning via Estimated Team Strengths</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11777">http://arxiv.org/abs/2307.11777</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florian Felice, Christophe Ley</li>
<li>for: é¢„æµ‹æ‰‹çƒèµ›äº‹</li>
<li>methods: ä½¿ç”¨Statistically Enhanced Learningï¼ˆSELï¼‰æ¨¡å‹ï¼Œå¹¶ä¸ç°æœ‰æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œä»¥è¯„ä¼°å…¶æ€§èƒ½èƒ½åŠ›</li>
<li>results: æ¨¡å‹çš„å‡†ç¡®ç‡é«˜äº80%ï¼Œå¹¶ä¸”é€šè¿‡å¯è§£é‡Šæ–¹æ³•æä¾›äº†æœ‰ä»·å€¼çš„ç»Ÿè®¡å’Œé¢„æµ‹æ€§èƒ½åˆ†æï¼Œæœ‰åŠ©äºæ‰‹çƒé˜Ÿæ•™ç»ƒæå‰å‡†å¤‡æ¯”èµ›ã€‚<details>
<summary>Abstract</summary>
We propose a Statistically Enhanced Learning (aka. SEL) model to predict handball games. Our Machine Learning model augmented with SEL features outperforms state-of-the-art models with an accuracy beyond 80%. In this work, we show how we construct the data set to train Machine Learning models on past female club matches. We then compare different models and evaluate them to assess their performance capabilities. Finally, explainability methods allow us to change the scope of our tool from a purely predictive solution to a highly insightful analytical tool. This can become a valuable asset for handball teams' coaches providing valuable statistical and predictive insights to prepare future competitions.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿè®¡å¢å¼ºå­¦ä¹ ï¼ˆç®€ç§° SELï¼‰æ¨¡å‹ï¼Œç”¨äºé¢„æµ‹æ‰‹çƒæ¯”èµ›ã€‚æˆ‘ä»¬çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œé€šè¿‡æ·»åŠ  SEL ç‰¹å¾ï¼Œè¶…è¿‡äº†ç°çŠ¶æœ€ä½³æ¨¡å‹çš„å‡†ç¡®ç‡80%ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨è¿‡å»å¥³å­ä¿±ä¹éƒ¨æ¯”èµ›æ•°æ®æ¥è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ã€‚ç„¶åï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†ä¸åŒçš„æ¨¡å‹ï¼Œå¹¶è¯„ä¼°å®ƒä»¬çš„æ€§èƒ½èƒ½åŠ›ã€‚æœ€åï¼Œå¯è§†åŒ–æ–¹æ³•ä½¿æˆ‘ä»¬çš„å·¥å…·ä»ä¸€ç§ä»…ä»…æ˜¯é¢„æµ‹è§£å†³æ–¹æ¡ˆè½¬åŒ–ä¸ºä¸€ç§å…·æœ‰é«˜åº¦æ¢ç´¢æ€§çš„åˆ†æå·¥å…·ï¼Œè¿™å°†æˆä¸ºæ‰‹çƒé˜Ÿæ•™ç»ƒçš„å®è´µç»Ÿè®¡å’Œé¢„æµ‹ä¿¡æ¯ï¼Œä»¥å‡†å¤‡æœªæ¥çš„æ¯”èµ›ã€‚Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="FedSoup-Improving-Generalization-and-Personalization-in-Federated-Learning-via-Selective-Model-Interpolation"><a href="#FedSoup-Improving-Generalization-and-Personalization-in-Federated-Learning-via-Selective-Model-Interpolation" class="headerlink" title="FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation"></a>FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10507">http://arxiv.org/abs/2307.10507</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minghui Chen, Meirui Jiang, Qi Dou, Zehua Wang, Xiaoxiao Li</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æé«˜åˆ†å¸ƒå¼å­¦ä¹ ï¼ˆFederated Learningï¼ŒFLï¼‰ä¸­æ¨¡å‹çš„é€šç”¨æ€§å’Œå…¨å±€æ€§ï¼Œè§£å†³å½“é¢ä¸´åˆ†å¸ƒshiftæ—¶ç°æœ‰FLç®—æ³•çš„è´Ÿé¢æ•ˆæœã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è”é‚¦æ¨¡å‹æ±¤ï¼ˆFederated Model Soupï¼ŒFMSï¼‰æ–¹æ³•ï¼Œé€šè¿‡åœ¨è”é‚¦è®­ç»ƒé˜¶æ®µå¯¹æœ¬åœ°å’Œå…¨å±€æ¨¡å‹è¿›è¡Œé€‰æ‹©æ€§ interpolate æ¥ä¼˜åŒ–æœ¬åœ°å’Œå…¨å±€æ€§ä¹‹é—´çš„è´Ÿé¢æ•ˆæœã€‚</li>
<li>results: æˆ‘ä»¬åœ¨Retinalå’Œç—…ç†å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶å®ç°äº†æ˜¾è‘—æé«˜å¯¹äºéå…¸å‹æ•°æ®çš„æ³›åŒ–æ€§ã€‚ä»£ç å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/ubc-tea/FedSoup%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ubc-tea/FedSoupä¸­æ‰¾åˆ°ã€‚</a><details>
<summary>Abstract</summary>
Cross-silo federated learning (FL) enables the development of machine learning models on datasets distributed across data centers such as hospitals and clinical research laboratories. However, recent research has found that current FL algorithms face a trade-off between local and global performance when confronted with distribution shifts. Specifically, personalized FL methods have a tendency to overfit to local data, leading to a sharp valley in the local model and inhibiting its ability to generalize to out-of-distribution data. In this paper, we propose a novel federated model soup method (i.e., selective interpolation of model parameters) to optimize the trade-off between local and global performance. Specifically, during the federated training phase, each client maintains its own global model pool by monitoring the performance of the interpolated model between the local and global models. This allows us to alleviate overfitting and seek flat minima, which can significantly improve the model's generalization performance. We evaluate our method on retinal and pathological image classification tasks, and our proposed method achieves significant improvements for out-of-distribution generalization. Our code is available at https://github.com/ubc-tea/FedSoup.
</details>
<details>
<summary>æ‘˜è¦</summary>
è·¨å­˜å‚¨æ¿ federated learning (FL) å¯ä»¥å¼€å‘æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨æ•°æ®ä¸­å¿ƒå¦‚åŒ»é™¢å’Œä¸´åºŠç ”ç©¶å®éªŒå®¤ç­‰åœ°çš„æ•°æ®é›†ä¸Šã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶å‘ç°ï¼Œå½“é¢ä¸´åˆ†å¸ƒå˜åŒ–æ—¶ï¼Œå½“å‰çš„ FL ç®—æ³•é¢ä¸´ä¸€ç§æœ¬åœ°å’Œå…¨çƒæ€§èƒ½ä¹‹é—´çš„è´Ÿæƒè¡¥å¿ã€‚ç‰¹åˆ«æ˜¯ï¼Œä¸ªæ€§åŒ– FL æ–¹æ³•æœ‰åå‘æœ¬åœ°æ•°æ®è¿‡æ‹Ÿåˆçš„å€¾å‘ï¼Œå¯¼è‡´æœ¬åœ°æ¨¡å‹å‘ˆé”é™è°·ï¼Œé˜»ç¢å…¶åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„æ³›åŒ–æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§æ–°çš„è”é‚¦æ¨¡å‹æ±¤ soup æ–¹æ³•ï¼ˆå³é€‰æ‹©æ€§ interpolate æ¨¡å‹å‚æ•°ï¼‰ï¼Œä»¥ä¼˜åŒ–æœ¬åœ°å’Œå…¨çƒæ€§èƒ½ä¹‹é—´çš„è´Ÿæƒè¡¥å¿ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨è”é‚¦è®­ç»ƒé˜¶æ®µï¼Œæ¯ä¸ªå®¢æˆ·ç«¯éƒ½ä¼šç»´æŠ¤è‡ªå·±çš„å…¨çƒæ¨¡å‹æ± ï¼Œå¹¶åœ¨æœ¬åœ°å’Œå…¨çƒæ¨¡å‹ä¹‹é—´è¿›è¡Œé€‰æ‹©æ€§ interpolate æ¨¡å‹å‚æ•°ã€‚è¿™æœ‰åŠ©äºè§£å†³è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå¯»æ‰¾å¹³é™è°·ï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨Retinalå’Œç—…ç†å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶å–å¾—äº†æ˜¾è‘—çš„å¤–éƒ¨æ•°æ®é›†æ³›åŒ–æ€§èƒ½æ”¹è¿›ã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»¥åœ¨https://github.com/ubc-tea/FedSoup ä¸­æ‰¾åˆ°ã€‚
</details></li>
</ul>
<hr>
<h2 id="Identifying-Interpretable-Subspaces-in-Image-Representations"><a href="#Identifying-Interpretable-Subspaces-in-Image-Representations" class="headerlink" title="Identifying Interpretable Subspaces in Image Representations"></a>Identifying Interpretable Subspaces in Image Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10504">http://arxiv.org/abs/2307.10504</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neha Kalibhat, Shweta Bhardwaj, Bayan Bruss, Hamed Firooz, Maziar Sanjabi, Soheil Feizi</li>
<li>for: æœ¬æ–‡æ—¨åœ¨è§£é‡Šå›¾åƒè¡¨ç¤ºçš„ç‰¹å¾ï¼Œæé«˜å›¾åƒè¡¨ç¤ºçš„å¯è§£é‡Šæ€§ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨äº†å¯¹æ¯”æ¦‚å¿µï¼ˆContrasting Conceptsï¼‰æ¥è§£é‡Šå›¾åƒè¡¨ç¤ºçš„ç‰¹å¾ã€‚é¦–å…ˆï¼Œä½¿ç”¨å¤§é‡captioningæ•°æ®é›†ï¼ˆå¦‚LAION-400mï¼‰å’Œé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰æ¥ç”Ÿæˆç‰¹å¾çš„æè¿°ã€‚ç„¶åï¼Œå¯¹æ¯ä¸ªæè¿°è¯­è¨€è¿›è¡Œåˆ†æ•°å’Œæ’åï¼Œä»è€Œå¾—åˆ°å°‘é‡å…±äº«çš„äººç±»å¯ç†è§£çš„æ¦‚å¿µï¼Œå®ƒä»¬å‡†ç¡®åœ°æè¿°äº†ç›®æ ‡ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜ä½¿ç”¨äº†å¯¹æ¯”è§£é‡Šï¼Œä½¿ç”¨ä½æ´»è·ƒå›¾åƒï¼ˆcounterfactualï¼‰æ¥æ¶ˆé™¤å¹»è§‰çš„æ¦‚å¿µã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œè®¸å¤šç°æœ‰çš„æ–¹æ³•åªèƒ½ç‹¬ç«‹è§£é‡Šç‰¹å¾çš„ä¸€éƒ¨åˆ†ï¼Œä½†æ˜¯ä½¿ç”¨FALCONå¯ä»¥è§£é‡Šå¤§å‹è¡¨ç¤ºç©ºé—´ä¸­çš„ç‰¹å¾ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡é«˜é˜¶åˆ†æ•°æ¥è§£é‡Šç‰¹å¾ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§å°†æ¦‚å¿µä»ä¸€ä¸ªå¯è§£é‡Šçš„è¡¨ç¤ºç©ºé—´ä¼ é€’åˆ°å¦ä¸€ä¸ªæœªçŸ¥è¡¨ç¤ºç©ºé—´çš„æŠ€æœ¯ã€‚<details>
<summary>Abstract</summary>
We propose Automatic Feature Explanation using Contrasting Concepts (FALCON), an interpretability framework to explain features of image representations. For a target feature, FALCON captions its highly activating cropped images using a large captioning dataset (like LAION-400m) and a pre-trained vision-language model like CLIP. Each word among the captions is scored and ranked leading to a small number of shared, human-understandable concepts that closely describe the target feature. FALCON also applies contrastive interpretation using lowly activating (counterfactual) images, to eliminate spurious concepts. Although many existing approaches interpret features independently, we observe in state-of-the-art self-supervised and supervised models, that less than 20% of the representation space can be explained by individual features. We show that features in larger spaces become more interpretable when studied in groups and can be explained with high-order scoring concepts through FALCON. We discuss how extracted concepts can be used to explain and debug failures in downstream tasks. Finally, we present a technique to transfer concepts from one (explainable) representation space to another unseen representation space by learning a simple linear transformation.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬æå‡ºäº†è‡ªåŠ¨Featureè§£é‡Šä½¿ç”¨å¯¹æ¯”æ¦‚å¿µï¼ˆFALCONï¼‰çš„è§£é‡Šæ¡†æ¶ï¼Œç”¨äºè§£é‡Šå›¾åƒè¡¨ç¤ºä¸­çš„ç‰¹å¾ã€‚ä¸ºç›®æ ‡ç‰¹å¾ï¼ŒFALCONä½¿ç”¨å¤§é‡captioningdatasetï¼ˆå¦‚LAION-400mï¼‰å’Œé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰æ¥captioné«˜åº¦æ´»è·ƒçš„è£å‰ªå›¾åƒã€‚æ¯ä¸ªå•è¯åœ¨captionä¸­è¢«åˆ†æ•°å’Œæ’åï¼Œå¯¼è‡´ä¸€å°æ•°é‡çš„å…±äº«ã€äººç±»å¯ç†è§£çš„æ¦‚å¿µï¼Œå‡†ç¡®åœ°æè¿°ç›®æ ‡ç‰¹å¾ã€‚æ­¤å¤–ï¼ŒFALCONè¿˜ä½¿ç”¨å¯¹æ¯”è§£é‡Šä½¿ç”¨ä½æ´»è·ƒï¼ˆcounterfactualï¼‰å›¾åƒï¼Œä»¥æ¶ˆé™¤å¹»æ•°æ¦‚å¿µã€‚ç°æœ‰çš„æ–¹æ³•å¤§å¤šè§£é‡Šç‰¹å¾ç‹¬ç«‹ï¼Œä½†æˆ‘ä»¬å‘ç°åœ¨å½“ä»Šçš„è‡ªç„¶è¯­è¨€å’ŒæŒ‡å¯¼ä¸‹çš„æ¨¡å‹ä¸­ï¼Œ Less than 20% of the representation space can be explained by individual featuresã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œåœ¨æ›´å¤§çš„ç©ºé—´ä¸­ï¼Œç‰¹å¾åœ¨ç»„åˆ Ğ¸Ğ·ÑƒÑ‡Ğ°æ—¶å˜å¾—æ›´åŠ è§£é‡Šï¼Œå¯ä»¥é€šè¿‡é«˜çº§åˆ†æ•°æ¦‚å¿µæ¥è§£é‡Šã€‚æˆ‘ä»¬è¿˜è®¨è®ºäº†æŠ½å–çš„æ¦‚å¿µå¦‚ä½•ç”¨äºè§£é‡Šå’Œè°ƒè¯•ä¸‹æ¸¸ä»»åŠ¡çš„å¤±è´¥ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å°†æ¦‚å¿µä»ä¸€ä¸ªå¯è§£é‡Šçš„è¡¨ç¤ºç©ºé—´ä¼ è¾“åˆ°å¦ä¸€ä¸ªæœªçŸ¥è¡¨ç¤ºç©ºé—´çš„å­¦ä¹ ç®€å•çº¿æ€§å˜æ¢çš„æŠ€æœ¯ã€‚
</details></li>
</ul>
<hr>
<h2 id="A-Competitive-Learning-Approach-for-Specialized-Models-A-Solution-for-Complex-Physical-Systems-with-Distinct-Functional-Regimes"><a href="#A-Competitive-Learning-Approach-for-Specialized-Models-A-Solution-for-Complex-Physical-Systems-with-Distinct-Functional-Regimes" class="headerlink" title="A Competitive Learning Approach for Specialized Models: A Solution for Complex Physical Systems with Distinct Functional Regimes"></a>A Competitive Learning Approach for Specialized Models: A Solution for Complex Physical Systems with Distinct Functional Regimes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10496">http://arxiv.org/abs/2307.10496</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/exploita123/charmedforfree">https://github.com/exploita123/charmedforfree</a></li>
<li>paper_authors: Okezzi F. Ukorigho, Opeoluwa Owoyele</li>
<li>for: è¯¥æ–‡ç« æ˜¯ä¸ºäº†æå‡ºä¸€ç§æ–°çš„ç«äº‰å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºè·å–åŸºäºæ•°æ®çš„ç‰©ç†ç³»ç»Ÿæ¨¡å‹ã€‚</li>
<li>methods: è¯¥æ–¹æ³•ä½¿ç”¨åŠ¨æ€æŸå¤±å‡½æ•°ï¼Œè®©ä¸€ç»„æ¨¡å‹åŒæ—¶åœ¨æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥ä¾¿åœ¨æ•°æ®ä¸­å‘ç°ä¸åŒçš„åŠŸèƒ½ Ñ€ĞµĞ¶Ğ¸åº¦ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥æˆåŠŸåœ°å‘ç°åŠŸèƒ½ rÃ©gimeï¼Œæ‰¾åˆ°çœŸæ­£çš„ç®¡ç†æ–¹ç¨‹ï¼Œå¹¶å‡å°‘æµ‹è¯•é”™è¯¯ã€‚<details>
<summary>Abstract</summary>
Complex systems in science and engineering sometimes exhibit behavior that changes across different regimes. Traditional global models struggle to capture the full range of this complex behavior, limiting their ability to accurately represent the system. In response to this challenge, we propose a novel competitive learning approach for obtaining data-driven models of physical systems. The primary idea behind the proposed approach is to employ dynamic loss functions for a set of models that are trained concurrently on the data. Each model competes for each observation during training, allowing for the identification of distinct functional regimes within the dataset. To demonstrate the effectiveness of the learning approach, we coupled it with various regression methods that employ gradient-based optimizers for training. The proposed approach was tested on various problems involving model discovery and function approximation, demonstrating its ability to successfully identify functional regimes, discover true governing equations, and reduce test errors.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç§‘å­¦å’Œå·¥ç¨‹ä¸­çš„å¤æ‚ç³»ç»Ÿæœ‰æ—¶ä¼šå±•ç°ä¸åŒçš„è¡Œä¸ºæ–¹å¼ï¼Œä¼ ç»Ÿçš„å…¨çƒæ¨¡å‹å¾ˆéš¾æ•æ‰è¿™äº›å¤æ‚çš„è¡Œä¸ºèŒƒå›´ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„å‡†ç¡®æ€§ã€‚ä¸ºåº”å¯¹è¿™ä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬æè®®ä¸€ç§æ–°çš„ç«äº‰å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡åœ¨æ•°æ®ä¸ŠåŒæ—¶è®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨åŠ¨æ€æŸå¤±å‡½æ•°æ¥è®©æ¯ä¸ªæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç«äº‰å¯¹æ¯ä¸ªè§‚å¯Ÿç»“æœã€‚è¿™ä¼šä½¿å¾—æ¨¡å‹èƒ½å¤ŸæˆåŠŸåœ°è¯†åˆ«æ•°æ®é›†ä¸­çš„ä¸åŒåŠŸèƒ½ Ñ€ĞµĞ¶Ğ¸åº¦ã€‚ä¸ºè¯æ˜è¯¥å­¦ä¹ æ–¹æ³•çš„æ•ˆæœï¼Œæˆ‘ä»¬å°†å…¶ä¸ä¸åŒçš„å›å½’æ–¹æ³•ç»“åˆä½¿ç”¨ï¼Œè¿™äº›å›å½’æ–¹æ³•ä½¿ç”¨æ¢¯åº¦åŸºäºä¼˜åŒ–å™¨è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬åœ¨å„ç§æ¨¡å‹å‘ç°å’Œå‡½æ•°è¿‘ä¼¼é—®é¢˜ä¸­æµ‹è¯•äº†è¯¥æ–¹æ³•ï¼Œå¹¶è¯æ˜äº†å®ƒå¯ä»¥æˆåŠŸåœ°è¯†åˆ«åŠŸèƒ½ Ñ€ĞµĞ¶Ğ¸åº¦ï¼Œå‘ç°çœŸæ­£çš„ç®¡ç†æ–¹ç¨‹å’Œé™ä½æµ‹è¯•é”™è¯¯ã€‚
</details></li>
</ul>
<hr>
<h2 id="Novel-Batch-Active-Learning-Approach-and-Its-Application-to-Synthetic-Aperture-Radar-Datasets"><a href="#Novel-Batch-Active-Learning-Approach-and-Its-Application-to-Synthetic-Aperture-Radar-Datasets" class="headerlink" title="Novel Batch Active Learning Approach and Its Application to Synthetic Aperture Radar Datasets"></a>Novel Batch Active Learning Approach and Its Application to Synthetic Aperture Radar Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10495">http://arxiv.org/abs/2307.10495</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chapman20j/sar_bal">https://github.com/chapman20j/sar_bal</a></li>
<li>paper_authors: James Chapman, Bohan Chen, Zheng Tan, Jeff Calder, Kevin Miller, Andrea L. Bertozzi</li>
<li>for: è¿™ä¸ªè®ºæ–‡ä¸»è¦é’ˆå¯¹çš„æ˜¯sequential active learningæ–¹æ³•åœ¨Synthetic Aperture Radarï¼ˆSARï¼‰æ•°æ®é›†ä¸Šçš„åº”ç”¨å’Œæé«˜ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä¸¤éƒ¨åˆ†æ–¹æ³•ï¼ŒåŒ…æ‹¬Dijkstraçš„ Annulus Core-Setï¼ˆDACï¼‰å’ŒLocalMaxï¼Œä»¥ä¾¿æ‰¹å¤„ç†æ´»åŠ¨å­¦ä¹ ã€‚</li>
<li>results: æ ¹æ®å®éªŒç»“æœï¼Œè¿™ç§æ‰¹å¤„ç†æ´»åŠ¨å­¦ä¹ æ–¹æ³•å¯ä»¥ä¸sequential active learningæ–¹æ³•å‡ ä¹è¾¾åˆ°åŒæ ·çš„å‡†ç¡®ç‡ï¼Œä½†æ˜¯æ›´é«˜æ•ˆï¼Œä¸æ‰¹å¤„ç†å¤§å°æˆæ¯”ä¾‹ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨classify FUSAR-Shipå’ŒOpenSARShip datasetsæ—¶è¾¾åˆ°äº†çŠ¶æ€å¹³å°CNN-basedæ–¹æ³•çš„æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
Active learning improves the performance of machine learning methods by judiciously selecting a limited number of unlabeled data points to query for labels, with the aim of maximally improving the underlying classifier's performance. Recent gains have been made using sequential active learning for synthetic aperture radar (SAR) data arXiv:2204.00005. In each iteration, sequential active learning selects a query set of size one while batch active learning selects a query set of multiple datapoints. While batch active learning methods exhibit greater efficiency, the challenge lies in maintaining model accuracy relative to sequential active learning methods. We developed a novel, two-part approach for batch active learning: Dijkstra's Annulus Core-Set (DAC) for core-set generation and LocalMax for batch sampling. The batch active learning process that combines DAC and LocalMax achieves nearly identical accuracy as sequential active learning but is more efficient, proportional to the batch size. As an application, a pipeline is built based on transfer learning feature embedding, graph learning, DAC, and LocalMax to classify the FUSAR-Ship and OpenSARShip datasets. Our pipeline outperforms the state-of-the-art CNN-based methods.
</details>
<details>
<summary>æ‘˜è¦</summary>
To address this challenge, we developed a novel two-part approach for batch active learning, consisting of Dijkstra's Annulus Core-Set (DAC) for core-set generation and LocalMax for batch sampling. The batch active learning process that combines DAC and LocalMax achieves nearly identical accuracy as sequential active learning but is more efficient, proportional to the batch size.As an application, we built a pipeline based on transfer learning feature embedding, graph learning, DAC, and LocalMax to classify the FUSAR-Ship and OpenSARShip datasets. Our pipeline outperforms state-of-the-art CNN-based methods.Here is the translation in Simplified Chinese:æ´»åŠ¨å­¦å¯ä»¥æé«˜æœºå™¨å­¦ä¹ æ–¹æ³•çš„æ€§èƒ½ï¼Œé€šè¿‡é€‰æ‹©ä¸€ä¸ªæœ‰é™æ•°é‡çš„æœªæ ‡æ³¨æ•°æ®ç‚¹ï¼Œå¹¶å°†å…¶ç”¨äºæ ‡æ³¨ï¼Œä»¥æœ€å¤§åŒ–ä¸‹é¢çš„ç±»ifierè¡¨ç°ã€‚æœ€è¿‘ï¼Œæœ‰å…³synthetic aperture radarï¼ˆSARï¼‰æ•°æ®çš„è¿›æ­¥å·²ç»åœ¨arXiv:2204.0005ä¸­è¿›è¡Œã€‚åœ¨æ¯ä¸ªè¿­ä»£ä¸­ï¼Œsequential active learningé€‰æ‹©ä¸€ä¸ªæŸ¥è¯¢é›†åˆï¼Œè€Œæ‰¹å¤„æ´»åŠ¨å­¦é€‰æ‹©å¤šä¸ªæ•°æ®ç‚¹çš„æŸ¥è¯¢é›†åˆã€‚è™½ç„¶æ‰¹å¤„æ´»åŠ¨å­¦æ–¹æ³•æ›´é«˜æ•ˆï¼Œä½†æ˜¯ä¿æŒæ¨¡å‹å‡†ç¡®æ€§ä¸sequential active learningæ–¹æ³•ç›¸æ¯”æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°çš„ã€ä¸¤éƒ¨åˆ†çš„æ‰¹å¤„æ´»åŠ¨å­¦æ–¹æ³•ï¼ŒåŒ…æ‹¬Dijkstraçš„Annulus Core-Setï¼ˆDACï¼‰å’ŒLocalMaxã€‚è¿™ç§æ‰¹å¤„æ´»åŠ¨å­¦è¿‡ç¨‹ç»“åˆDACå’ŒLocalMaxå¯ä»¥å®ç°ä¸sequential active learningæ–¹æ³•å‡†ç¡®æ€§ç›¸ä¼¼ï¼Œä½†æ˜¯æ›´é«˜æ•ˆï¼Œå³ä¸æ‰¹å¤„å¤§å°ç›¸å…³ã€‚ä½œä¸ºåº”ç”¨ï¼Œæˆ‘ä»¬å»ºç«‹äº†åŸºäºä¼ è¾“å­¦ä¹ ç‰¹å¾åµŒå…¥ã€å›¾å­¦ä¹ ã€DACå’ŒLocalMaxçš„ç®¡é“ï¼Œç”¨äºåˆ†ç±»FUSAR-Shipå’ŒOpenSARShipæ•°æ®é›†ã€‚æˆ‘ä»¬çš„ç®¡é“è¶…è¿‡äº†åŸºäºConvolutional Neural Networksï¼ˆCNNï¼‰çš„çŠ¶æ€æ§åˆ¶æ–¹æ³•ã€‚
</details></li>
</ul>
<hr>
<h2 id="Blockchain-Based-Federated-Learning-Incentivizing-Data-Sharing-and-Penalizing-Dishonest-Behavior"><a href="#Blockchain-Based-Federated-Learning-Incentivizing-Data-Sharing-and-Penalizing-Dishonest-Behavior" class="headerlink" title="Blockchain-Based Federated Learning: Incentivizing Data Sharing and Penalizing Dishonest Behavior"></a>Blockchain-Based Federated Learning: Incentivizing Data Sharing and Penalizing Dishonest Behavior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10492">http://arxiv.org/abs/2307.10492</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amir Jaberzadeh, Ajay Kumar Shrestha, Faijan Ahamad Khan, Mohammed Afaan Shaikh, Bhargav Dave, Jason Geng</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æä¾›ä¸€ä¸ªæ•´åˆæ•°æ®ä¿¡ä»»çš„è”é‚¦å­¦ä¹ æ¡†æ¶ï¼Œä»¥ä¾¿åœ¨å¤šæ–¹åˆä½œä¸‹è¿›è¡Œå®‰å…¨ä¸”å…¬æ­£çš„æ•°æ®åˆ†äº«ï¼Œå¹¶æä¾›æ¿€åŠ±ã€å­˜å–æ§åˆ¶æœºåˆ¶å’Œå¤„ç½šä¸æ­£è¡Œä¸ºã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†InterPlanetary File Systemã€åŒºå—é“¾å’Œæ™ºèƒ½åˆçº¦æ¥å®ç°å®‰å…¨ä¸”å¯é çš„æ•°æ®åˆ†äº«ï¼Œå¹¶å°†æ•°æ®ä¿¡ä»» integrate into federated learningï¼Œä»¥æé«˜è”é‚¦å­¦ä¹ æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>results: å®éªŒç»“æœæ˜¾ç¤ºï¼Œææ¡ˆçš„æ¨¡å‹èƒ½å¤Ÿæé«˜è”é‚¦å­¦ä¹ æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œå¹¶ç¡®ä¿æ•°æ®åˆ†äº«è¿‡ç¨‹ä¸­çš„å®‰å…¨å’Œå…¬æ­£ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜å‘å±•äº†ä¸€ä¸ªåŸºäºåŒºå—æŠ€æœ¯çš„åˆ†æ•£å¼æœºå™¨å­¦ä¹ å¹³å°ï¼Œèƒ½å¤Ÿåœ¨å¤šæ–¹åˆä½œä¸‹è®­ç»ƒ CNN æ¨¡å‹ï¼Œå¹¶ç»´æŠ¤æ•°æ®éšç§å’Œå®‰å…¨ã€‚<details>
<summary>Abstract</summary>
With the increasing importance of data sharing for collaboration and innovation, it is becoming more important to ensure that data is managed and shared in a secure and trustworthy manner. Data governance is a common approach to managing data, but it faces many challenges such as data silos, data consistency, privacy, security, and access control. To address these challenges, this paper proposes a comprehensive framework that integrates data trust in federated learning with InterPlanetary File System, blockchain, and smart contracts to facilitate secure and mutually beneficial data sharing while providing incentives, access control mechanisms, and penalizing any dishonest behavior. The experimental results demonstrate that the proposed model is effective in improving the accuracy of federated learning models while ensuring the security and fairness of the data-sharing process. The research paper also presents a decentralized federated learning platform that successfully trained a CNN model on the MNIST dataset using blockchain technology. The platform enables multiple workers to train the model simultaneously while maintaining data privacy and security. The decentralized architecture and use of blockchain technology allow for efficient communication and coordination between workers. This platform has the potential to facilitate decentralized machine learning and support privacy-preserving collaboration in various domains.
</details>
<details>
<summary>æ‘˜è¦</summary>
éšç€æ•°æ®å…±äº«çš„é‡è¦æ€§å¢åŠ ï¼Œä¿è¯æ•°æ®çš„å®‰å…¨å’Œå¯é æ€§å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚æ•°æ®æ²»ç†æ˜¯ä¸€ç§å¸¸è§çš„æ•°æ®ç®¡ç†æ–¹å¼ï¼Œä½†å®ƒé¢ä¸´ç€æ•°æ®å­¤å²›ã€æ•°æ®ä¸€è‡´æ€§ã€éšç§ã€å®‰å…¨å’Œè®¿é—®æ§åˆ¶ç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œè¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ¶µç›–æ•°æ®ä¿¡ä»»çš„ federated learning æ¡†æ¶ï¼Œå¹¶ä¸ InterPlanetary File Systemã€åŒºå—é“¾å’Œæ™ºèƒ½åˆçº¦ç»“åˆï¼Œå®ç°å®‰å…¨å’Œäº’æƒ çš„æ•°æ®åˆ†äº«ï¼Œå¹¶æä¾›äº†å¥–åŠ±ã€è®¿é—®æ§åˆ¶æœºåˆ¶å’Œæƒ©æˆ’ä»»ä½•ä¸è¯šå®è¡Œä¸ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæè®®çš„æ¨¡å‹èƒ½å¤Ÿæé«˜ federated learning æ¨¡å‹çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶ä¿éšœæ•°æ®åˆ†äº«çš„å®‰å…¨æ€§å’Œå…¬å¹³æ€§ã€‚è®ºæ–‡è¿˜æè¿°äº†ä¸€ä¸ªåŸºäºåŒºå—é“¾æŠ€æœ¯çš„åˆ†å¸ƒå¼ federated learning å¹³å°ï¼Œå¯ä»¥åŒæ—¶è®­ç»ƒå¤šä¸ªå·¥ä½œè€…çš„ CNN æ¨¡å‹ï¼Œå¹¶ä¿æŒæ•°æ®éšç§å’Œå®‰å…¨æ€§ã€‚è¯¥å¹³å°çš„åˆ†å¸ƒå¼æ¶æ„å’Œä½¿ç”¨åŒºå—é“¾æŠ€æœ¯ï¼Œå¯ä»¥å®ç°é«˜æ•ˆçš„é€šä¿¡å’Œåè°ƒã€‚è¿™ç§å¹³å°å…·æœ‰æ¨åŠ¨åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ å’Œä¿æŒéšç§åä½œçš„æ½œåœ¨æ½œåŠ›ã€‚
</details></li>
</ul>
<hr>
<h2 id="Ab-using-Images-and-Sounds-for-Indirect-Instruction-Injection-in-Multi-Modal-LLMs"><a href="#Ab-using-Images-and-Sounds-for-Indirect-Instruction-Injection-in-Multi-Modal-LLMs" class="headerlink" title="(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs"></a>(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10490">http://arxiv.org/abs/2307.10490</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ebagdasa/multimodal_injection">https://github.com/ebagdasa/multimodal_injection</a></li>
<li>paper_authors: Eugene Bagdasaryan, Tsung-Yin Hsieh, Ben Nassi, Vitaly Shmatikov</li>
<li>for: ç”¨å›¾åƒå’Œå£°éŸ³è¿›è¡Œé—´æ¥æç¤ºå’ŒæŒ‡å¯¼ injection æ”»å‡»ã€‚</li>
<li>methods: ç”Ÿæˆæ”»å‡»è€…é€‰æ‹©çš„æŠ—å¹²æ‰°å™ªéŸ³æˆ–å›¾åƒï¼Œå¹¶å°†å…¶æ··åˆåˆ°åŸå§‹æ¨¡å‹ä¸­ã€‚</li>
<li>results: å½“ç”¨æˆ·é—®é¢˜ benign æ¨¡å‹å…³äºå¹²æ‰°åçš„å›¾åƒæˆ–å£°éŸ³æ—¶ï¼Œæ”»å‡»è€…å¯ä»¥é€šè¿‡æ§åˆ¶æ¨¡å‹çš„è¾“å‡ºæ–‡æœ¬å’Œå¯¹è¯æµæ¥å®ç°æ”»å‡»ã€‚<details>
<summary>Abstract</summary>
We demonstrate how images and sounds can be used for indirect prompt and instruction injection in multi-modal LLMs. An attacker generates an adversarial perturbation corresponding to the prompt and blends it into an image or audio recording. When the user asks the (unmodified, benign) model about the perturbed image or audio, the perturbation steers the model to output the attacker-chosen text and/or make the subsequent dialog follow the attacker's instruction. We illustrate this attack with several proof-of-concept examples targeting LLaVa and PandaGPT.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬ç¤ºç¤ºå¦‚ä½•ä½¿ç”¨å›¾åƒå’Œå£°éŸ³è¿›è¡Œé—´æ¥æç¤ºå’ŒæŒ‡ä»¤æ³¨å…¥åœ¨å¤šmodal LLMSä¸­ã€‚æ”»å‡»è€…åˆ›é€ äº†è¿™äº›æç¤ºçš„æ”»å‡»æ‰°åŠ¨ï¼Œå¹¶ä¸å›¾åƒæˆ–éŸ³é¢‘å½•éŸ³æ··åˆåœ¨ä¸€èµ·ã€‚å½“ç”¨æˆ·å¯¹ï¼ˆæœªä¿®æ”¹ã€è‰¯å¥½ï¼‰æ¨¡å‹è¯¢é—®è¿™äº›æ··åˆè¿‡çš„å›¾åƒæˆ–éŸ³é¢‘æ—¶ï¼Œæ”»å‡»æ‰°åŠ¨å°†ä½¿æ¨¡å‹è¾“å‡ºæ”»å‡»è€…é€‰æ‹©çš„æ–‡æœ¬å’Œ/æˆ–å¯¼è‡´åç»­å¯¹è¯æŒ‰ç…§æ”»å‡»è€…çš„æŒ‡ä»¤è¿›è¡Œã€‚æˆ‘ä»¬é€è¿‡å¤šä¸ªè¯æ˜ä¾‹å­ï¼Œè¯æ˜è¿™ç§æ”»å‡»å¯ä»¥å¯¹LLLaVaå’ŒPandaGPTè¿›è¡Œã€‚
</details></li>
</ul>
<hr>
<h2 id="SPRINT-A-Unified-Toolkit-for-Evaluating-and-Demystifying-Zero-shot-Neural-Sparse-Retrieval"><a href="#SPRINT-A-Unified-Toolkit-for-Evaluating-and-Demystifying-Zero-shot-Neural-Sparse-Retrieval" class="headerlink" title="SPRINT: A Unified Toolkit for Evaluating and Demystifying Zero-shot Neural Sparse Retrieval"></a>SPRINT: A Unified Toolkit for Evaluating and Demystifying Zero-shot Neural Sparse Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10488">http://arxiv.org/abs/2307.10488</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thakur-nandan/sprint">https://github.com/thakur-nandan/sprint</a></li>
<li>paper_authors: Nandan Thakur, Kexin Wang, Iryna Gurevych, Jimmy Lin</li>
<li>for: è¿™ä¸ªè®ºæ–‡ä¸»è¦æ˜¯ä¸ºäº†æä¾›ä¸€ä¸ªç»Ÿä¸€çš„ Python å·¥å…·é›†ï¼ˆSPRINTï¼‰ï¼Œç”¨äºè¯„ä¼°åŸºäºå«ä¹‰æœç´¢çš„ç¥ç»ç¨€ç¼ºæ£€ç´¢æ¨¡å‹ã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº† Pyserini å’Œ Lucene ç­‰å·¥å…·æ¥å®ç°ä¸€ä¸ªé€šç”¨çš„æ¥å£ï¼Œæ”¯æŒå¤šç§åŸºäºç¥ç»ç½‘ç»œçš„ç¨€ç¼ºæ£€ç´¢æ¨¡å‹ã€‚ç”¨æˆ·å¯ä»¥è½»æ¾åœ°æ·»åŠ è‡ªå·±çš„å®šåˆ¶æ¨¡å‹ï¼Œåªéœ€è¦å®šä¹‰æƒé‡æ–¹æ³•å³å¯ã€‚</li>
<li>results: æ ¹æ® authors çš„å®éªŒç»“æœï¼ŒSPRINT å·¥å…·é›†å¯ä»¥åœ¨ BEIR ç­‰ benchmark ä¸Šå»ºç«‹å¼ºå¤§ä¸”å¯é‡å¤çš„é›¶æ‰¹ç¨€ç¼ºæ£€ç´¢åŸºå‡†ã€‚å…¶ä¸­ï¼ŒSPLADEv2 æ¨¡å‹åœ¨ BEIR ä¸Šçš„å¹³å‡å¾—åˆ†ä¸º 0.470 nDCG@10ï¼Œèƒœè¿‡å…¶ä»–ç¥ç»ç¨€ç¼ºæ£€ç´¢æ¨¡å‹ã€‚ authors è¿˜å‘ç°ï¼ŒSPLADEv2 ç”Ÿæˆçš„ç¨€ç¼ºè¡¨ç¤ºå¯ä»¥å¸®åŠ©å…¶å–å¾—è¡¨ç°æå‡ï¼Œå…¶ä¸­å¤§å¤šæ•°çš„å­—ç¬¦å‡ºç°åœ¨åŸå§‹æŸ¥è¯¢å’Œæ–‡æ¡£ä¹‹å¤–ã€‚<details>
<summary>Abstract</summary>
Traditionally, sparse retrieval systems relied on lexical representations to retrieve documents, such as BM25, dominated information retrieval tasks. With the onset of pre-trained transformer models such as BERT, neural sparse retrieval has led to a new paradigm within retrieval. Despite the success, there has been limited software supporting different sparse retrievers running in a unified, common environment. This hinders practitioners from fairly comparing different sparse models and obtaining realistic evaluation results. Another missing piece is, that a majority of prior work evaluates sparse retrieval models on in-domain retrieval, i.e. on a single dataset: MS MARCO. However, a key requirement in practical retrieval systems requires models that can generalize well to unseen out-of-domain, i.e. zero-shot retrieval tasks. In this work, we provide SPRINT, a unified Python toolkit based on Pyserini and Lucene, supporting a common interface for evaluating neural sparse retrieval. The toolkit currently includes five built-in models: uniCOIL, DeepImpact, SPARTA, TILDEv2 and SPLADEv2. Users can also easily add customized models by defining their term weighting method. Using our toolkit, we establish strong and reproducible zero-shot sparse retrieval baselines across the well-acknowledged benchmark, BEIR. Our results demonstrate that SPLADEv2 achieves the best average score of 0.470 nDCG@10 on BEIR amongst all neural sparse retrievers. In this work, we further uncover the reasons behind its performance gain. We show that SPLADEv2 produces sparse representations with a majority of tokens outside of the original query and document which is often crucial for its performance gains, i.e. a limitation among its other sparse counterparts. We provide our SPRINT toolkit, models, and data used in our experiments publicly here at https://github.com/thakur-nandan/sprint.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¼ ç»Ÿä¸Šï¼Œç¨€ç–æœå¯»ç³»ç»Ÿä»å­—è¯è¡¨ç°æ¥è¿›è¡Œæ–‡æ¡£æœå¯»ï¼Œå¦‚BM25ï¼Œå¯¹äºæœå¯»ä»»åŠ¡äº§ç”Ÿäº†é‡è¦å½±å“ã€‚éšç€é¢„è®­ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°å™¨æ¨¡å‹ï¼Œå¦‚BERTï¼Œç¥ç»ç¨€ç–æœå¯»å¸¦æ¥äº†ä¸€ä¸ªæ–°çš„æ—¶ä»£ã€‚ä¸è¿‡ï¼Œæœ‰é™çš„è½¯ä»¶æ”¯æŒä¸åŒçš„ç¨€ç–æ¨¡å‹åœ¨ä¸€ä¸ªå…±åŒç¯å¢ƒä¸­è¿è¡Œï¼Œå¯¼è‡´å®è·µè€…å¾ˆéš¾æ¯”è¾ƒä¸åŒçš„ç¨€ç–æ¨¡å‹ï¼Œå¹¶è·å¾—å®é™…çš„è¯„ä¼°ç»“æœã€‚å¦å¤–ï¼Œè®¸å¤šå…ˆå‰çš„å·¥ä½œä»…å¯¹å†…éƒ¨è¿‡æ»¤è¿›è¡Œè¯„ä¼°ï¼Œå³åœ¨MS MARCOä¸Šè¿›è¡Œå†…éƒ¨è¿‡æ»¤ã€‚ä½†æ˜¯ï¼Œå®é™…æœå¯»ç³»ç»Ÿä¸­éœ€è¦æ¨¡å‹èƒ½å¤Ÿå¯¹æœªè§è¿‡çš„é›¶æ•°æ®ç±»å‹è¿›è¡Œæ¨å¯¼ï¼Œè¿™æ˜¯ä¸€ä¸ªé‡è¦çš„éœ€æ±‚ã€‚åœ¨è¿™ä¸ªç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†SPRINTï¼Œä¸€ä¸ªåŸºäºPyseriniå’ŒLuceneçš„Pythonå·¥å…·ç»„ï¼Œæ”¯æŒä¸€ä¸ªå…±åŒçš„ç•Œé¢ï¼Œç”¨äºè¯„ä¼°ç¥ç»ç¨€ç–æœå¯»ã€‚å·¥å…·ç»„ç›®å‰åŒ…æ‹¬äº”ä¸ªå†…ç½®æ¨¡å‹ï¼šuniCOILã€DeepImpactã€SPARTAã€TILDEv2å’ŒSPLADEv2ã€‚ç”¨æˆ·å¯ä»¥è½»æ¾åœ°æ·»åŠ è‡ªå·±å®šä¹‰çš„æ¡ä»¶è¯„ä¼°æ–¹æ³•ã€‚ä½¿ç”¨æˆ‘ä»¬çš„å·¥å…·ç»„ï¼Œæˆ‘ä»¬å»ºç«‹äº†å¼ºå¤§ä¸”å¯é‡ç°çš„é›¶æ•°æ®ç±»å‹ç¥ç»ç¨€ç–æœå¯»åŸºå‡†ï¼Œå¹¶åœ¨BEIRä¸Šå–å¾—äº†æœ€å¥½çš„å¹³å‡åˆ†ä¸º0.470 nDCG@10ã€‚åœ¨è¿™ä¸ªç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç´¢äº†SPLADEv2çš„è¡¨ç°åŸå› ï¼Œå‘ç°å®ƒç”Ÿæˆçš„ç¨€ç–è¡¨ç°ä¸­ï¼Œå¤§å¤šæ•°çš„å­—è¯ä½äºåŸå§‹æŸ¥è¯¢å’Œæ–‡æ¡£ä¹‹å¤–ï¼Œè¿™ç»å¸¸æ˜¯å…¶è¡¨ç°ä¼˜åŒ–çš„å…³é”®ã€‚æˆ‘ä»¬æä¾›äº†æˆ‘ä»¬åœ¨è¿™ä¸ªç ”ç©¶ä¸­ä½¿ç”¨çš„SPRINTå·¥å…·ç»„ã€æ¨¡å‹å’Œæ•°æ®ï¼Œå¯ä»¥åœ¨https://github.com/thakur-nandan/sprintä¸Šå–å¾—ã€‚
</details></li>
</ul>
<hr>
<h2 id="FinGPT-Democratizing-Internet-scale-Data-for-Financial-Large-Language-Models"><a href="#FinGPT-Democratizing-Internet-scale-Data-for-Financial-Large-Language-Models" class="headerlink" title="FinGPT: Democratizing Internet-scale Data for Financial Large Language Models"></a>FinGPT: Democratizing Internet-scale Data for Financial Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10485">http://arxiv.org/abs/2307.10485</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai4finance-foundation/fingpt">https://github.com/ai4finance-foundation/fingpt</a></li>
<li>paper_authors: Xiao-Yang Liu, Guoxuan Wang, Daochen Zha<br>for:FinGPT aims to democratize Internet-scale financial data for large language models (LLMs) to revolutionize the finance industry.methods:FinGPT introduces an open-sourced and data-centric framework that automates the collection and curation of real-time financial data from diverse sources on the Internet.results:FinGPT provides researchers and practitioners with accessible and transparent resources to develop their FinLLMs, and demonstrates several applications including robo-advisor, sentiment analysis for algorithmic trading, and low-code development.<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated remarkable proficiency in understanding and generating human-like texts, which may potentially revolutionize the finance industry. However, existing LLMs often fall short in the financial field, which is mainly attributed to the disparities between general text data and financial text data. Unfortunately, there is only a limited number of financial text datasets available (quite small size), and BloombergGPT, the first financial LLM (FinLLM), is close-sourced (only the training logs were released). In light of this, we aim to democratize Internet-scale financial data for LLMs, which is an open challenge due to diverse data sources, low signal-to-noise ratio, and high time-validity. To address the challenges, we introduce an open-sourced and data-centric framework, \textit{Financial Generative Pre-trained Transformer (FinGPT)}, that automates the collection and curation of real-time financial data from >34 diverse sources on the Internet, providing researchers and practitioners with accessible and transparent resources to develop their FinLLMs. Additionally, we propose a simple yet effective strategy for fine-tuning FinLLM using the inherent feedback from the market, dubbed Reinforcement Learning with Stock Prices (RLSP). We also adopt the Low-rank Adaptation (LoRA, QLoRA) method that enables users to customize their own FinLLMs from open-source general-purpose LLMs at a low cost. Finally, we showcase several FinGPT applications, including robo-advisor, sentiment analysis for algorithmic trading, and low-code development. FinGPT aims to democratize FinLLMs, stimulate innovation, and unlock new opportunities in open finance. The codes are available at https://github.com/AI4Finance-Foundation/FinGPT and https://github.com/AI4Finance-Foundation/FinNLP
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤§å‹è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»è¡¨ç°å‡ºäº†äººç±»è¯­è¨€ç†è§£å’Œç”Ÿæˆçš„å¾ˆé«˜æ°´å¹³ï¼Œè¿™å¯èƒ½ä¼šé©å‘½åŒ–é‡‘èä¸šã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMåœ¨é‡‘èé¢†åŸŸç»å¸¸ç¼ºä¹è¡¨ç°ï¼Œè¿™ä¸»è¦å½’ç»“äºé€šç”¨æ–‡æœ¬æ•°æ®å’Œé‡‘èæ–‡æœ¬æ•°æ®ä¹‹é—´çš„å·®å¼‚ã€‚å°½ç®¡åªæœ‰æœ‰é™çš„é‡‘èæ–‡æœ¬æ•°æ®é›†availableï¼ˆæ•°æ®é›†è¾ƒå°ï¼‰ï¼Œè€ŒBloombergGPTï¼Œé¦–ä¸ªé‡‘èLLMï¼ˆFinLLMï¼‰ï¼Œåˆ™æ˜¯å…³é—­æºçš„ï¼ˆåªå‘å¸ƒäº†è®­ç»ƒæ—¥å¿—ï¼‰ã€‚ä¸ºäº†æ™®åŠäº’è”ç½‘çº§é‡‘èæ•°æ® Ğ´Ğ»Ñ LLMï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æ”¾çš„æŒ‘æˆ˜ï¼Œå› ä¸ºæ•°æ®æ¥æºå¤šæ ·åŒ–ã€ä¿¡å·å™ªå£°æ¯”è¾ƒä½å’Œæ—¶é—´æœ‰æ•ˆæ€§å¾ˆé«˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¼€æºå’Œæ•°æ®ä¸­å¿ƒçš„æ¡†æ¶ï¼Œåä¸ºé‡‘èç”Ÿæˆé¢„è®­ç»ƒå˜æ¢å™¨ï¼ˆFinancial Generative Pre-trained Transformerï¼ŒFinGPTï¼‰ã€‚FinGPTè‡ªåŠ¨æ”¶é›†å’Œæ•´ç†äº’è”ç½‘ä¸Š >34 ä¸ªä¸åŒæ¥æºçš„å®æ—¶é‡‘èæ•°æ®ï¼Œä¸ºç ”ç©¶äººå‘˜å’Œå®è·µè€…æä¾›äº†å¯ accessible å’Œ transparent çš„èµ„æºï¼Œä»¥ä¾¿å¼€å‘è‡ªå·±çš„FinLLMã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ç®€å• yet effectiveçš„RLSPï¼ˆå¸‚åœºåé¦ˆå¼ºåŒ–å­¦ä¹ ï¼‰ç­–ç•¥ï¼Œå¯ä»¥é€šè¿‡å¸‚åœºçš„è‡ªç„¶åé¦ˆæ¥è®­ç»ƒFinLLMã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†LoRAï¼ˆä½çº§é€‚åº”ï¼‰æ–¹æ³•ï¼Œå…è®¸ç”¨æˆ·è‡ªå®šä¹‰è‡ªå·±çš„FinLLMï¼Œä»å¼€æºé€šç”¨è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆNLMï¼‰ä¸­è·å¾—ä¼˜ç§€çš„æ€§èƒ½ï¼Œè€Œä¸éœ€è¦å¤§é‡çš„äººå·¥è°ƒæ•´ã€‚FinGPTè¿˜æä¾›äº†å¤šç§åº”ç”¨ï¼ŒåŒ…æ‹¬æ™ºèƒ½æŠ•èµ„ã€æƒ…æ„Ÿåˆ†æ Ğ´Ğ»Ñç®—æ³•äº¤æ˜“å’Œä½ä»£ç å¼€å‘ã€‚FinGPTçš„ä»£ç å¯ä»¥åœ¨ <https://github.com/AI4Finance-Foundation/FinGPT> å’Œ <https://github.com/AI4Finance-Foundation/FinNLP> ä¸Šä¸‹è½½ã€‚FinGPTçš„ç›®æ ‡æ˜¯æ™®åŠFinLLMï¼Œä¿ƒè¿›åˆ›æ–°ï¼Œå¹¶åœ¨å¼€æ”¾é‡‘èä¸­è§£é”æ–°çš„æœºä¼šã€‚
</details></li>
</ul>
<hr>
<h2 id="Can-Instruction-Fine-Tuned-Language-Models-Identify-Social-Bias-through-Prompting"><a href="#Can-Instruction-Fine-Tuned-Language-Models-Identify-Social-Bias-through-Prompting" class="headerlink" title="Can Instruction Fine-Tuned Language Models Identify Social Bias through Prompting?"></a>Can Instruction Fine-Tuned Language Models Identify Social Bias through Prompting?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10472">http://arxiv.org/abs/2307.10472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omkar Dige, Jacob-Junqi Tian, David Emerson, Faiza Khan Khattak</li>
<li>for: è¯„ä¼°è¯­è¨€æ¨¡å‹ä¸­çš„ç¤¾ä¼šåè§</li>
<li>methods: é‡‡ç”¨é›¶æ‰¹ç¤ºè¯„ä¼°è¯­è¨€æ¨¡å‹çš„åè§è¯†åˆ«èƒ½åŠ›</li>
<li>results: ç»“æœæ˜¾ç¤ºï¼Œé€šè¿‡è®­ç»ƒ Alpaca 7B æ¨¡å‹ï¼Œå¯ä»¥è¾¾åˆ° 56.7% çš„å‡†ç¡®ç‡ï¼Œå¹¶ä¸”è§„æ¨¡å’Œæ•°æ®å¤šæ ·æ€§çš„æ‰©å±•å¯èƒ½ä¼šå¸¦æ¥æ›´å¥½çš„è¡¨ç°ã€‚<details>
<summary>Abstract</summary>
As the breadth and depth of language model applications continue to expand rapidly, it is increasingly important to build efficient frameworks for measuring and mitigating the learned or inherited social biases of these models. In this paper, we present our work on evaluating instruction fine-tuned language models' ability to identify bias through zero-shot prompting, including Chain-of-Thought (CoT) prompts. Across LLaMA and its two instruction fine-tuned versions, Alpaca 7B performs best on the bias identification task with an accuracy of 56.7%. We also demonstrate that scaling up LLM size and data diversity could lead to further performance gain. This is a work-in-progress presenting the first component of our bias mitigation framework. We will keep updating this work as we get more results.
</details>
<details>
<summary>æ‘˜è¦</summary>
éšç€è¯­è¨€æ¨¡å‹åº”ç”¨çš„å¹¿åº¦å’Œæ·±åº¦å¿«é€Ÿæ‰©å±•ï¼Œç°åœ¨è¶Šæ¥è¶Šé‡è¦åœ°å»ºç«‹é«˜æ•ˆçš„è¯­è¨€æ¨¡å‹åè§è¯„ä¼°å’Œmitigationçš„æ¡†æ¶ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†æˆ‘ä»¬å¯¹é€‚ç”¨äºå„ç§å„æ ·çš„å¼•å…¥è¯­è¨€æ¨¡å‹çš„åè§è¯†åˆ«èƒ½åŠ›è¿›è¡Œè¯„ä¼°çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬é“¾æ¡ï¼ˆChain-of-Thoughtï¼‰æç¤ºã€‚åœ¨LLaMAå’Œå…¶ä¸¤ä¸ª instrucion fine-tuned ç‰ˆæœ¬ä¸­ï¼ŒAlpaca 7B åœ¨åè§è¯†åˆ«ä»»åŠ¡ä¸Šè¡¨ç°æœ€å¥½ï¼Œå‡†ç¡®ç‡ä¸º 56.7%ã€‚æˆ‘ä»¬è¿˜è¯æ˜äº†é€šè¿‡å¢åŠ  LLVM å¤§å°å’Œæ•°æ®å¤šæ ·æ€§å¯ä»¥å®ç°æ›´å¤§çš„æ€§èƒ½æå‡ã€‚è¿™æ˜¯æˆ‘ä»¬åè§ mitigation æ¡†æ¶çš„é¦–ä¸ªç»„æˆéƒ¨åˆ†ï¼Œæˆ‘ä»¬ä¼šç»§ç»­æ›´æ–°è¿™ä¸ªå·¥ä½œï¼Œä»¥è·å¾—æ›´å¤šçš„ç»“æœã€‚
</details></li>
</ul>
<hr>
<h2 id="Classification-of-Visualization-Types-and-Perspectives-in-Patents"><a href="#Classification-of-Visualization-Types-and-Perspectives-in-Patents" class="headerlink" title="Classification of Visualization Types and Perspectives in Patents"></a>Classification of Visualization Types and Perspectives in Patents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10471">http://arxiv.org/abs/2307.10471</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tibhannover/patentimageclassification">https://github.com/tibhannover/patentimageclassification</a></li>
<li>paper_authors: Junaid Ahmed Ghauri, Eric MÃ¼ller-Budack, Ralph Ewerth</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æé«˜å¥—ä»¶ç”³è¯·æ£€ç´¢å’Œæµè§ˆçš„æ•ˆç‡ï¼Œé€šè¿‡ä½¿ç”¨ä¸åŒç±»å‹çš„è§†è§‰åŒ–å’Œè§†è§’æ¥æ˜¾ç¤ºåˆ›æ–°çš„ç»†èŠ‚ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†ç°ä»£æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼ŒåŒ…æ‹¬å˜æ¢å™¨ï¼Œè¿›è¡Œå›¾åƒç±»å‹å’Œè§†è§’çš„åˆ†ç±»ã€‚æˆ‘ä»¬ä¹Ÿå¯¹CLEF-IP datasetè¿›è¡Œæ‰©å±•ï¼Œå¹¶æä¾›äº†æ‰‹åŠ¨æ ‡æ³¨çš„ground truthã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜äº†ææ¡ˆçš„æ–¹æ³•çš„å¯è¡Œæ€§ã€‚æˆ‘ä»¬å°†æºä»£ç ã€æ¨¡å‹å’Œæ•°æ®é›†å…¬å¼€å‘å¸ƒã€‚<details>
<summary>Abstract</summary>
Due to the swift growth of patent applications each year, information and multimedia retrieval approaches that facilitate patent exploration and retrieval are of utmost importance. Different types of visualizations (e.g., graphs, technical drawings) and perspectives (e.g., side view, perspective) are used to visualize details of innovations in patents. The classification of these images enables a more efficient search and allows for further analysis. So far, datasets for image type classification miss some important visualization types for patents. Furthermore, related work does not make use of recent deep learning approaches including transformers. In this paper, we adopt state-of-the-art deep learning methods for the classification of visualization types and perspectives in patent images. We extend the CLEF-IP dataset for image type classification in patents to ten classes and provide manual ground truth annotations. In addition, we derive a set of hierarchical classes from a dataset that provides weakly-labeled data for image perspectives. Experimental results have demonstrated the feasibility of the proposed approaches. Source code, models, and dataset will be made publicly available.
</details>
<details>
<summary>æ‘˜è¦</summary>
In this paper, we employ state-of-the-art deep learning methods for the classification of visualization types and perspectives in patent images. We expand the CLEF-IP dataset for image type classification in patents to ten classes and provide manual ground truth annotations. Furthermore, we derive a set of hierarchical classes from a dataset that provides weakly-labeled data for image perspectives. Experimental results have demonstrated the feasibility of the proposed approaches. The source code, models, and dataset will be publicly available.
</details></li>
</ul>
<hr>
<h2 id="Properties-of-Discrete-Sliced-Wasserstein-Losses"><a href="#Properties-of-Discrete-Sliced-Wasserstein-Losses" class="headerlink" title="Properties of Discrete Sliced Wasserstein Losses"></a>Properties of Discrete Sliced Wasserstein Losses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10352">http://arxiv.org/abs/2307.10352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eloi Tanguy, RÃ©mi Flamary, Julie Delon</li>
<li>for: è¿™ä¸ªè®ºæ–‡ä¸»è¦ç ”ç©¶äº† $\mathcal{E}: Y \longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z) $ çš„å±æ€§å’Œä¼˜åŒ–æ€§ï¼Œå…¶ä¸­ $\gamma_Y $ å’Œ $\gamma_Z $ æ˜¯ä¸¤ä¸ª uniform æŠ½è±¡æ¦‚ç‡åˆ†å¸ƒã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†å¤šç§æ–¹æ³•ï¼ŒåŒ…æ‹¬ç ”ç©¶ $\mathcal{E} $ çš„æ­£åˆ™æ€§å’Œä¼˜åŒ–æ€§ï¼Œä»¥åŠå…¶ Monte-Carlo é‡‡æ · $\mathcal{E}_p $ çš„æ¸è¿‘ç¨³å®šæ€§å’Œ almost-sure uniform æ”¶æ•›æ€§ã€‚</li>
<li>results: ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼ŒStochastic Gradient Descent æ–¹æ³•å¯ä»¥å‡å°‘ $\mathcal{E} $ å’Œ $\mathcal{E}_p $ çš„ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶ä¸”è¿™äº›æ–¹æ³•ä¼šæ”¶æ•›åˆ°ï¼ˆClarkeï¼‰ä¼˜åŒ–ç‚¹ã€‚<details>
<summary>Abstract</summary>
The Sliced Wasserstein (SW) distance has become a popular alternative to the Wasserstein distance for comparing probability measures. Widespread applications include image processing, domain adaptation and generative modelling, where it is common to optimise some parameters in order to minimise SW, which serves as a loss function between discrete probability measures (since measures admitting densities are numerically unattainable). All these optimisation problems bear the same sub-problem, which is minimising the Sliced Wasserstein energy. In this paper we study the properties of $\mathcal{E}: Y \longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z)$, i.e. the SW distance between two uniform discrete measures with the same amount of points as a function of the support $Y \in \mathbb{R}^{n \times d}$ of one of the measures. We investigate the regularity and optimisation properties of this energy, as well as its Monte-Carlo approximation $\mathcal{E}_p$ (estimating the expectation in SW using only $p$ samples) and show convergence results on the critical points of $\mathcal{E}_p$ to those of $\mathcal{E}$, as well as an almost-sure uniform convergence. Finally, we show that in a certain sense, Stochastic Gradient Descent methods minimising $\mathcal{E}$ and $\mathcal{E}_p$ converge towards (Clarke) critical points of these energies.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œåˆ’åˆ† Wassersteinï¼ˆSWï¼‰è·ç¦»å·²æˆä¸ºæ¯” Wasserstein è·ç¦»æ›´å—æ¬¢è¿çš„é€‰æ‹©ï¼Œç”¨äºæ¯”è¾ƒæ¦‚ç‡åˆ†å¸ƒã€‚å®ƒåœ¨å›¾åƒå¤„ç†ã€é¢†åŸŸé€‚åº”å’Œç”Ÿæˆæ¨¡å‹ä¸­å¹¿æ³›åº”ç”¨ï¼Œé€šå¸¸æ˜¯å¯»æ‰¾å¯ä»¥æœ€å°åŒ– SW çš„å‚æ•°ï¼Œä»¥ä¾¿ä½œä¸ºè¿™äº›å‚æ•°çš„æŸå¤±å‡½æ•°ã€‚è¿™äº›ä¼˜åŒ–é—®é¢˜éƒ½æœ‰åŒä¸€ä¸ªå­é—®é¢˜ï¼Œå³å¯»æ‰¾å¯ä»¥æœ€å°åŒ– SW èƒ½é‡ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº† $\mathcal{E}: Y \longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z) $ çš„æ€§è´¨ï¼Œå…¶ä¸­ $Y \in \mathbb{R}^{n \times d}$ æ˜¯ä¸€ä¸ªæŸä¸ªæ¦‚ç‡åˆ†å¸ƒçš„æ”¯æŒã€‚æˆ‘ä»¬è°ƒæŸ¥äº†è¿™ä¸ªèƒ½é‡çš„è§„å¾‹å’Œä¼˜åŒ–æ€§ï¼Œä»¥åŠå…¶ Monte-Carlo é¢„ä¼° $\mathcal{E}_p$ çš„æ•°å€¼ï¼Œå¹¶è¯æ˜äº†è¿™äº›ç‚¹çš„å‡åŒ€æ”¶æ‘„å’Œç¡®å®šæ€§ã€‚æœ€åï¼Œæˆ‘ä»¬æ˜¾ç¤ºäº†åœ¨æŸäº›æ„ä¹‰ä¸Šï¼Œä½¿ç”¨ Stochastic Gradient Descent æ–¹æ³•ä¼˜åŒ– $\mathcal{E}$ å’Œ $\mathcal{E}_p$ å¯ä»¥å¯¼å‘ï¼ˆClarkeï¼‰å†…éƒ¨ç‚¹çš„æå€¼ã€‚â€
</details></li>
</ul>
<hr>
<h2 id="A-data-science-axiology-the-nature-value-and-risks-of-data-science"><a href="#A-data-science-axiology-the-nature-value-and-risks-of-data-science" class="headerlink" title="A data science axiology: the nature, value, and risks of data science"></a>A data science axiology: the nature, value, and risks of data science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10460">http://arxiv.org/abs/2307.10460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael L. Brodie</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ˜¯ä¸ºäº†æ¢è®¨æ•°æ®ç§‘å­¦çš„axiologyï¼Œå³å…¶ç›®çš„ã€æ€§è´¨ã€é‡è¦æ€§ã€é£é™©å’Œä»·å€¼ï¼Œä»¥å¸®åŠ©ç†è§£å’Œå®šä¹‰æ•°æ®ç§‘å­¦ï¼Œå¹¶æ‰¾åˆ°å…¶å¯èƒ½çš„åˆ©ç›Šå’Œé£é™©ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†AXIOLOGYçš„æ–¹æ³•æ¥æ¢è®¨æ•°æ®ç§‘å­¦çš„ç‰¹ç‚¹ï¼ŒåŒ…æ‹¬å…¶ä¸å¯é¢„æµ‹çš„æ€§å’ŒAIçš„åº”ç”¨ã€‚</li>
<li>results: è¿™ç¯‡è®ºæ–‡çš„ç»“æœè¡¨æ˜ï¼Œæ•°æ®ç§‘å­¦åœ¨çŸ¥è¯†å‘ç°æ–¹é¢å…·æœ‰å¾ˆå¤§çš„æ½œåŠ›å’Œå¯èƒ½æ€§ï¼Œä½†åŒæ—¶ä¹Ÿå­˜åœ¨ä¸€äº›é£é™©ï¼Œä¾‹å¦‚ä¸å¯é¢„æµ‹çš„ç»“æœå’ŒAIçš„åº”ç”¨å¯èƒ½å¯¼è‡´çš„ä¸è‰¯å½±å“ã€‚<details>
<summary>Abstract</summary>
Data science is not a science. It is a research paradigm with an unfathomed scope, scale, complexity, and power for knowledge discovery that is not otherwise possible and can be beyond human reasoning. It is changing our world practically and profoundly already widely deployed in tens of thousands of applications in every discipline in an AI Arms Race that, due to its inscrutability, can lead to unfathomed risks. This paper presents an axiology of data science, its purpose, nature, importance, risks, and value for problem solving, by exploring and evaluating its remarkable, definitive features. As data science is in its infancy, this initial, speculative axiology is intended to aid in understanding and defining data science to recognize its potential benefits, risks, and open research challenges. AI based data science is inherently about uncertainty that may be more realistic than our preference for the certainty of science. Data science will have impacts far beyond knowledge discovery and will take us into new ways of understanding the world.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œæ•°æ®ç§‘å­¦ä¸æ˜¯ä¸€ç§ç§‘å­¦ã€‚å®ƒæ˜¯ä¸€ç§ç ”ç©¶æ–¹æ³•è®ºï¼Œå…·æœ‰æœªæ›¾æ¢ç´¢çš„èŒƒå›´ã€å¤§å°ã€å¤æ‚æ€§å’ŒçŸ¥è¯†å‘ç°çš„åŠ›é‡ï¼Œè¶…å‡ºäººç±»ç†è§£çš„é™åˆ¶ã€‚å®ƒæ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ä¸–ç•Œï¼Œå·²ç»å¹¿æ³›åº”ç”¨äºä¸‡åƒä¸ªåº”ç”¨é¢†åŸŸï¼Œåœ¨äººå·¥æ™ºèƒ½ç«èµ›ä¸­æŠ•å…¥äº†å·¨èµ„ã€‚è¿™ç¯‡è®ºæ–‡æå‡ºäº†æ•°æ®ç§‘å­¦çš„axiologyï¼Œå…¶ç›®çš„ã€æœ¬è´¨ã€é‡è¦æ€§ã€é£é™©å’Œä»·å€¼ï¼Œé€šè¿‡æ¢ç©¶å’Œè¯„ä¼°å…¶éå‡¡çš„ç‰¹ç‚¹ã€‚ç”±äºæ•°æ®ç§‘å­¦å¤„äºå…¶åˆæœŸï¼Œè¿™äº›åˆæ­¥çš„è®ºæ®axiologyçš„ç›®çš„æ˜¯å¸®åŠ©æˆ‘ä»¬ç†è§£å’Œå®šä¹‰æ•°æ®ç§‘å­¦ï¼Œè®¤è¯†å…¶æ½œåœ¨çš„åˆ©ç›Šã€é£é™©å’Œç ”ç©¶æŒ‘æˆ˜ã€‚AIåŸºäºçš„æ•°æ®ç§‘å­¦æ˜¯ä¸€ç§ä¸ç¡®å®šæ€§ï¼Œå¯èƒ½æ›´åŠ çœŸå®åœ°åæ˜ æˆ‘ä»¬å¯¹ä¸–ç•Œçš„ç†è§£ã€‚æ•°æ®ç§‘å­¦å°†å¯¹æˆ‘ä»¬çš„ä¸–ç•Œäº§ç”Ÿæ·±è¿œçš„å½±å“ï¼Œå°†å¸¦æˆ‘ä»¬è¿›å…¥æ–°çš„ç†è§£ä¸–ç•Œã€‚â€
</details></li>
</ul>
<hr>
<h2 id="A-New-Computationally-Simple-Approach-for-Implementing-Neural-Networks-with-Output-Hard-Constraints"><a href="#A-New-Computationally-Simple-Approach-for-Implementing-Neural-Networks-with-Output-Hard-Constraints" class="headerlink" title="A New Computationally Simple Approach for Implementing Neural Networks with Output Hard Constraints"></a>A New Computationally Simple Approach for Implementing Neural Networks with Output Hard Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10459">http://arxiv.org/abs/2307.10459</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Andrei V. Konstantinov, Lev V. Utkin</li>
<li>for: æå‡ºäº†ä¸€ç§æ–°çš„è®¡ç®—ç®€å•çš„ç¥ç»ç½‘ç»œè¾“å‡ºå€¼çº¦æŸæ–¹æ³•ã€‚</li>
<li>methods: ä½¿ç”¨äº†é¢å¤–çš„ç¥ç»ç½‘ç»œå±‚æ¥å®ç°çº¦æŸï¼Œå¹¶å°†çº¦æŸè½¬æ¢ä¸ºç¥ç»ç½‘ç»œè¾“å‡ºå€¼çš„é™åˆ¶ã€‚</li>
<li>results: æ–¹æ³•å¯ä»¥ç®€å•åœ°æ‰©å±•åˆ°å—çº¦æŸçš„è¾“å…¥è¾“å‡ºé—®é¢˜ï¼Œå¹¶ä¸”å¯ä»¥å®ç°ä¸åŒç±»å‹çš„çº¦æŸï¼ŒåŒ…æ‹¬çº¿æ€§å’ŒäºŒæ¬¡çº¦æŸã€ç­‰å¼çº¦æŸå’ŒåŠ¨æ€çº¦æŸã€‚è®¡ç®—å¤æ‚åº¦ä¸ºO(n*m)å’ŒO(n^2*m)ã€‚æ•°æ®å®éªŒ validateäº†è¯¥æ–¹æ³•ã€‚<details>
<summary>Abstract</summary>
A new computationally simple method of imposing hard convex constraints on the neural network output values is proposed. The key idea behind the method is to map a vector of hidden parameters of the network to a point that is guaranteed to be inside the feasible set defined by a set of constraints. The mapping is implemented by the additional neural network layer with constraints for output. The proposed method is simply extended to the case when constraints are imposed not only on the output vectors, but also on joint constraints depending on inputs. The projection approach to imposing constraints on outputs can simply be implemented in the framework of the proposed method. It is shown how to incorporate different types of constraints into the proposed method, including linear and quadratic constraints, equality constraints, and dynamic constraints, constraints in the form of boundaries. An important feature of the method is its computational simplicity. Complexities of the forward pass of the proposed neural network layer by linear and quadratic constraints are O(n*m) and O(n^2*m), respectively, where n is the number of variables, m is the number of constraints. Numerical experiments illustrate the method by solving optimization and classification problems. The code implementing the method is publicly available.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¸€ç§æ–°çš„è®¡ç®—ç®€å•çš„æ–¹æ³•ï¼Œç”¨äºåœ¨ç¥ç»ç½‘ç»œè¾“å‡ºå€¼ä¸Šå¼ºåˆ¶å®æ–½ç¡¬ convex çº¦æŸï¼Œè¢«æå‡ºã€‚è¯¥æ–¹æ³•çš„å…³é”®æ€æƒ³æ˜¯å°†ç¥ç»ç½‘ç»œå‚æ•° Ğ²ĞµĞºÑ‚Ğ¾Ñ€æ˜ å°„åˆ°ä¸€ä¸ªç¡®å®šåœ¨å¯è¡Œé›†ä¸­çš„ç‚¹ä¸Šã€‚è¯¥æ˜ å°„é€šè¿‡é¢å¤–çš„ç¥ç»ç½‘ç»œå±‚å®ç°ï¼Œè¯¥å±‚å—çº¦æŸçš„è¾“å‡ºçº¦æŸã€‚æå‡ºçš„æ–¹æ³•å¯ä»¥ç®€å•åœ°æ‰©å±•åˆ°è¾“å‡ºçº¦æŸä¸ä»…ä»…æ˜¯å•ä¸ªè¾“å‡ºå‘é‡ï¼Œè€Œä¸”ä¹ŸåŒ…æ‹¬è¾“å…¥çš„å…±åŒçº¦æŸã€‚æŠ•å½±æ–¹æ³•å¯ä»¥ç®€å•åœ°åœ¨æå‡ºçš„æ–¹æ³•ä¸­å®ç°ã€‚è¯¥æ–¹æ³•å¯ä»¥å…·ä½“å®ç°ä¸åŒç±»å‹çš„çº¦æŸï¼ŒåŒ…æ‹¬çº¿æ€§å’Œquadraticçº¦æŸï¼Œç­‰å¼çº¦æŸï¼Œä»¥åŠè¾¹ç•Œçº¦æŸã€‚è¯¥æ–¹æ³•çš„è®¡ç®—ç®€å•æ€§æ˜¯å…¶é‡è¦ç‰¹ç‚¹ï¼Œå…¶å‰å‘ä¼ æ’­å¤æ‚åº¦ä¸ºO(n\*m)å’ŒO(n^2\*m)ï¼Œå…¶ä¸­næ˜¯å˜é‡æ•°ï¼Œmæ˜¯çº¦æŸæ•°ã€‚æ•°å€¼å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„å¯è¡Œæ€§å’Œåˆ†ç±»èƒ½åŠ›ã€‚ä»£ç å®ç°è¯¥æ–¹æ³•å…¬å¼€å¯ç”¨ã€‚
</details></li>
</ul>
<hr>
<h2 id="A-Step-Towards-Worldwide-Biodiversity-Assessment-The-BIOSCAN-1M-Insect-Dataset"><a href="#A-Step-Towards-Worldwide-Biodiversity-Assessment-The-BIOSCAN-1M-Insect-Dataset" class="headerlink" title="A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset"></a>A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10455">http://arxiv.org/abs/2307.10455</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zahrag/BIOSCAN-1M">https://github.com/zahrag/BIOSCAN-1M</a></li>
<li>paper_authors: Zahra Gharaee, ZeMing Gong, Nicholas Pellegrino, Iuliia Zarubiieva, Joakim Bruslund Haurum, Scott C. Lowe, Jaclyn T. A. McKeown, Chris C. Y. Ho, Joschka McLeod, Yi-Yun C Wei, Jireh Agda, Sujeevan Ratnasingham, Dirk Steinke, Angel X. Chang, Graham W. Taylor, Paul Fieguth</li>
<li>for:  This paper aims to provide a large dataset of hand-labelled insect images to train computer-vision models for taxonomic assessment, and to lay the foundation for a comprehensive survey of global biodiversity.</li>
<li>methods: The dataset, called BIOSCAN-Insect, includes raw nucleotide barcode sequences and assigned barcode index numbers for each record, and is primarily used to train computer-vision models for image-based taxonomic assessment.</li>
<li>results: The paper presents a million-image dataset with a long-tailed class-imbalance distribution and highly fine-grained classification problem at lower taxonomic levels, which provides a challenging task for image-based taxonomic classification.<details>
<summary>Abstract</summary>
In an effort to catalog insect biodiversity, we propose a new large dataset of hand-labelled insect images, the BIOSCAN-Insect Dataset. Each record is taxonomically classified by an expert, and also has associated genetic information including raw nucleotide barcode sequences and assigned barcode index numbers, which are genetically-based proxies for species classification. This paper presents a curated million-image dataset, primarily to train computer-vision models capable of providing image-based taxonomic assessment, however, the dataset also presents compelling characteristics, the study of which would be of interest to the broader machine learning community. Driven by the biological nature inherent to the dataset, a characteristic long-tailed class-imbalance distribution is exhibited. Furthermore, taxonomic labelling is a hierarchical classification scheme, presenting a highly fine-grained classification problem at lower levels. Beyond spurring interest in biodiversity research within the machine learning community, progress on creating an image-based taxonomic classifier will also further the ultimate goal of all BIOSCAN research: to lay the foundation for a comprehensive survey of global biodiversity. This paper introduces the dataset and explores the classification task through the implementation and analysis of a baseline classifier.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¯»æ±‚catalÃ³g insectå¤šæ ·æ€§ï¼Œæˆ‘ä»¬æå‡ºä¸€ä¸ªæ–°çš„å¤§å‹æ‰‹æ ‡æ³¨ insect å›¾åƒæ•°æ®é›†ï¼Œç§°ä¸º BIOSCAN-Insect æ•°æ®é›†ã€‚æ¯ä¸ªè®°å½•éƒ½ç”±ä¸“å®¶taxonomically åˆ†ç±»ï¼ŒåŒæ—¶è¿˜æœ‰å…³è”çš„é—ä¼ ä¿¡æ¯ï¼ŒåŒ…æ‹¬ raw  Ğ½ÑƒĞºĞ»ĞµĞ¾Ñ‚Ğ¸å¾·æ ¸å¿ƒåºåˆ—å’Œåˆ†é…ç»™æ¯ä¸ªç‰©ç§çš„æ ¸å¿ƒåºåˆ—ç¼–å·ï¼Œè¿™äº›æ˜¯ç”Ÿç‰©å­¦åŸºäºçš„ç§ç±»åˆ†ç±»çš„ä»£ç†ã€‚æœ¬æ–‡æŠ¥é“ä¸€ä¸ªç²¾å¿ƒçºªå½• million å¼ å›¾åƒæ•°æ®é›†ï¼Œä¸»è¦ç”¨äºè®­ç»ƒè®¡ç®—æœºè§†è§‰æ¨¡å‹ï¼Œä»¥æä¾›å›¾åƒåŸºäºçš„ç§ç±»è¯„ä¼°ã€‚ç„¶è€Œï¼Œæ•°æ®é›†è¿˜å…·æœ‰ä¸€äº›å¸å¼•äººçš„ç‰¹å¾ï¼Œå¦‚ç”Ÿç‰©å­¦æ€§çš„é•¿å°¾åˆ†å¸ƒå’Œç”Ÿç‰©å­¦åˆ†ç±»ç³»ç»Ÿçš„å±‚æ¬¡ç»“æ„ï¼Œè¿™äº›ç‰¹å¾éƒ½æ˜¯æœºå™¨å­¦ä¹ ç¤¾åŒºçš„ç ”ç©¶å¯¹è±¡ã€‚ basis çš„ç›®æ ‡æ˜¯å»ºç«‹ä¸€ä¸ªå…¨é¢çš„ global ç”Ÿç‰©å¤šæ ·æ€§ ç›‘æµ‹ç³»ç»Ÿï¼Œæœ¬æ–‡å¼•å…¥æ•°æ®é›†å¹¶é€šè¿‡å®ç°å’Œåˆ†æåŸºeline åˆ†ç±»å™¨æ¥æ¢è®¨åˆ†ç±»ä»»åŠ¡ã€‚
</details></li>
</ul>
<hr>
<h2 id="The-importance-of-feature-preprocessing-for-differentially-private-linear-optimization"><a href="#The-importance-of-feature-preprocessing-for-differentially-private-linear-optimization" class="headerlink" title="The importance of feature preprocessing for differentially private linear optimization"></a>The importance of feature preprocessing for differentially private linear optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11106">http://arxiv.org/abs/2307.11106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziteng Sun, Ananda Theertha Suresh, Aditya Krishna Menon</li>
<li>for: æœ¬ç ”ç©¶ç›®çš„æ˜¯ç ”ç©¶ differentially private stochastic gradient descent (DPSGD) æ˜¯å¦å…·æœ‰ sufficient condition to find a good minimizer for every dataset under privacy constraints.</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº† differentially private stochastic gradient descent (DPSGD) å’Œå…¶ variantsï¼Œä»¥åŠ feature preprocessing.</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œwithout feature preprocessing, DPSGD ä¼šå¯¼è‡´ privacy error proportional to the maximum norm of features over all samples. æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸º DPSGD-F çš„ç®—æ³•ï¼Œcombines DPSGD with feature preprocessing, and prove that for classification tasks, it incurs a privacy error proportional to the diameter of the features. æˆ‘ä»¬è¿˜åœ¨å›¾åƒåˆ†ç±» benchmarks ä¸­è¯æ˜äº†å®ƒçš„å®ç”¨æ€§.<details>
<summary>Abstract</summary>
Training machine learning models with differential privacy (DP) has received increasing interest in recent years. One of the most popular algorithms for training differentially private models is differentially private stochastic gradient descent (DPSGD) and its variants, where at each step gradients are clipped and combined with some noise. Given the increasing usage of DPSGD, we ask the question: is DPSGD alone sufficient to find a good minimizer for every dataset under privacy constraints? As a first step towards answering this question, we show that even for the simple case of linear classification, unlike non-private optimization, (private) feature preprocessing is vital for differentially private optimization. In detail, we first show theoretically that there exists an example where without feature preprocessing, DPSGD incurs a privacy error proportional to the maximum norm of features over all samples. We then propose an algorithm called DPSGD-F, which combines DPSGD with feature preprocessing and prove that for classification tasks, it incurs a privacy error proportional to the diameter of the features $\max_{x, x' \in D} \|x - x'\|_2$. We then demonstrate the practicality of our algorithm on image classification benchmarks.
</details>
<details>
<summary>æ‘˜è¦</summary>
<SYS> translate-into:zh-CN</SYS>è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ WITH differential privacy (DP) åœ¨æœ€è¿‘å‡ å¹´å†…å¾—åˆ°äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚DPä¸­æœ€å—æ¬¢è¿çš„ç®—æ³•ä¹‹ä¸€æ˜¯å·®åˆ†éšç§æ¢¯åº¦ä¸‹é™ (DPSGD) å’Œå…¶å˜ä½“ï¼Œåœ¨æ¯æ­¥éƒ½å°†æ¢¯åº¦clipå’Œæ‚éŸ³ç»“åˆåœ¨ä¸€èµ·ã€‚éšç€ DPSGD çš„ä½¿ç”¨è¶Šæ¥è¶Šæ™®éï¼Œæˆ‘ä»¬é—®ï¼šDPSGD æ˜¯å¦èƒ½å¤Ÿåœ¨éšç§é™åˆ¶ä¸‹æ‰¾åˆ°æ¯ä¸ªæ•°æ®é›†ä¸Šçš„å¥½æœ€å°å€¼ï¼Ÿä½œä¸ºå›ç­”çš„ç¬¬ä¸€æ­¥ï¼Œæˆ‘ä»¬è¯æ˜äº†éç§æœ‰ä¼˜åŒ–ä¸åŒäºéšç§ä¼˜åŒ–ï¼Œprivate feature preprocessing æ˜¯å¿…éœ€çš„ã€‚åœ¨è¯¦ç»†çš„è¯æ˜ä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†åœ¨ Linear classification ä»»åŠ¡ä¸Šï¼Œå¦‚æœæ²¡æœ‰ feature preprocessingï¼ŒDPSGD ä¼šå¯¼è‡´éšç§é”™è¯¯ä¸æœ€å¤§ç‰¹å¾å€¼çš„æœ€å¤§å€¼æˆæ­£æ¯”ã€‚æˆ‘ä»¬then proposeäº†ä¸€ä¸ªåä¸º DPSGD-F çš„ç®—æ³•ï¼Œå®ƒå°† DPSGD ä¸ feature preprocessing ç»“åˆï¼Œå¹¶è¯æ˜äº†åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šï¼Œå®ƒçš„éšç§é”™è¯¯ä¸ç‰¹å¾å€¼çš„æœ€å¤§å€¼æˆæ­£æ¯”ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨å›¾åƒåˆ†ç±»æ ‡å‡† benchmark ä¸Šè¯æ˜äº†æˆ‘ä»¬çš„ç®—æ³•çš„å®ç”¨æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Integrating-a-Heterogeneous-Graph-with-Entity-aware-Self-attention-using-Relative-Position-Labels-for-Reading-Comprehension-Model"><a href="#Integrating-a-Heterogeneous-Graph-with-Entity-aware-Self-attention-using-Relative-Position-Labels-for-Reading-Comprehension-Model" class="headerlink" title="Integrating a Heterogeneous Graph with Entity-aware Self-attention using Relative Position Labels for Reading Comprehension Model"></a>Integrating a Heterogeneous Graph with Entity-aware Self-attention using Relative Position Labels for Reading Comprehension Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10443">http://arxiv.org/abs/2307.10443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shima Foolad, Kourosh Kiani</li>
<li>for: æé«˜æœºå™¨é˜…è¯»ç†è§£æ¨¡å‹çš„å¤æ‚é€»è¾‘å¤„ç†èƒ½åŠ›</li>
<li>methods: injecting external knowledge into the transformer architecture without relying on external knowledge</li>
<li>results: æ¨¡å‹åœ¨ReCoRDæ•°æ®é›†ä¸Šçš„è¡¨ç°æ¯”cutting-edge LUKE-Graphå’ŒåŸºeline LUKEæ¨¡å‹æ›´ä¼˜ã€‚<details>
<summary>Abstract</summary>
Despite the significant progress made by transformer models in machine reading comprehension tasks, they still fall short in handling complex reasoning tasks due to the absence of explicit knowledge in the input sequence. To address this limitation, many recent works have proposed injecting external knowledge into the model. However, selecting relevant external knowledge, ensuring its availability, and requiring additional processing steps remain challenging. In this paper, we introduce a novel attention pattern that integrates reasoning knowledge derived from a heterogeneous graph into the transformer architecture without relying on external knowledge. The proposed attention pattern comprises three key elements: global-local attention for word tokens, graph attention for entity tokens that exhibit strong attention towards tokens connected in the graph as opposed to those unconnected, and the consideration of the type of relationship between each entity token and word token. This results in optimized attention between the two if a relationship exists. The pattern is coupled with special relative position labels, allowing it to integrate with LUKE's entity-aware self-attention mechanism. The experimental findings corroborate that our model outperforms both the cutting-edge LUKE-Graph and the baseline LUKE model on the ReCoRD dataset that focuses on commonsense reasoning.
</details>
<details>
<summary>æ‘˜è¦</summary>
å°½ç®¡å˜æ¢å™¨æ¨¡å‹åœ¨æœºå™¨é˜…è¯»ç†è§£ä»»åŠ¡ä¸­åšå‡ºäº†é‡è¦è¿›æ­¥ï¼Œä½†å®ƒä»¬ä»ç„¶ç¼ºä¹æ˜ç¡®çš„çŸ¥è¯†è¡¨è¾¾ï¼Œå¯¼è‡´åœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚ä¸ºè§£å†³è¿™ä¸€é™åˆ¶ï¼Œè®¸å¤šæœ€è¿‘çš„ç ”ç©¶å¼ºè°ƒåœ¨æ¨¡å‹ä¸­æ³¨å…¥å¤–éƒ¨çŸ¥è¯†ã€‚ç„¶è€Œï¼Œé€‰æ‹©ç›¸å…³çš„å¤–éƒ¨çŸ¥è¯†ï¼Œç¡®ä¿å…¶å¯ç”¨æ€§ï¼Œä»¥åŠéœ€è¦é¢å¤–çš„å¤„ç†æ­¥éª¤ä»ç„¶æ˜¯æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ³¨æ„æ¨¡å¼ï¼Œå³åœ¨å˜æ¢å™¨æ¶æ„ä¸­ Ğ¸Ğ½Ñ‚Ğµã‚°æ¨ç†çŸ¥è¯†æ¥è‡ªå¤šæ ·åŒ–å›¾è¡¨ç¤ºã€‚è¯¥æ³¨æ„æ¨¡å¼åŒ…æ‹¬ä¸‰ä¸ªå…³é”®å…ƒç´ ï¼šå…¨å±€-æœ¬åœ°æ³¨æ„åŠ› Ğ´Ğ»Ñå•è¯Tokenï¼Œå¯¹äºå®ä½“Tokençš„æ³¨æ„åŠ›ï¼Œä»¥åŠå¯¹æ¯ä¸ªå®ä½“Tokenå’Œå•è¯Tokenä¹‹é—´çš„å…³ç³»ç±»å‹è¿›è¡Œè€ƒè™‘ã€‚è¿™äº›å…ƒç´ çš„ç»“åˆä½¿å¾—æ³¨æ„åŠ›å¾—åˆ°ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨ç‰¹æ®Šç›¸å¯¹ä½æ ‡ç­¾ï¼Œä½¿å¾—è¯¥æ³¨æ„æ¨¡å¼å¯ä»¥ä¸LUKEæ¨¡å‹çš„å®ä½“æ„è¯†è‡ªæ³¨æ„æœºåˆ¶é›†æˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ Commonsense Reasoning æ•°æ®é›†ä¸Šæ¯”æ‚‰å¿ƒLUKE-Graphå’ŒåŸºç¡€LUKEæ¨¡å‹è¡¨ç°å‡ºè‰²ã€‚
</details></li>
</ul>
<hr>
<h2 id="Confidence-Estimation-Using-Unlabeled-Data"><a href="#Confidence-Estimation-Using-Unlabeled-Data" class="headerlink" title="Confidence Estimation Using Unlabeled Data"></a>Confidence Estimation Using Unlabeled Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10440">http://arxiv.org/abs/2307.10440</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/topoxlab/consistency-ranking-loss">https://github.com/topoxlab/consistency-ranking-loss</a></li>
<li>paper_authors: Chen Li, Xiaoling Hu, Chao Chen</li>
<li>for: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§åŸºäºåŠç›‘ç£å­¦ä¹ çš„ä¿¡ä»»ä¼°è®¡æ–¹æ³•ï¼Œå³ä½¿è®­ç»ƒæ ‡ç­¾å¾ˆå°‘ä¹Ÿå¯ä»¥ä¼°è®¡æ¨¡å‹å¯¹æœªæ ‡æ³¨æ ·æœ¬çš„ä¿¡ä»»ç¨‹åº¦ã€‚</li>
<li>methods: è¯¥æ–¹æ³•ä½¿ç”¨è®­ç»ƒè¿‡ç¨‹ä¸­é¢„æµ‹çš„ä¸€è‡´æ€§ä½œä¸ºä»£ç†å‡½æ•°ï¼Œå¹¶æå‡ºäº†ä¸€ç§ä¸€è‡´æ€§æ’åæŸå¤±å‡½æ•°æ¥ä¼°è®¡ä¿¡ä»»ç¨‹åº¦ã€‚</li>
<li>results: åœ¨å›¾åƒåˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•å®ç°äº†é¢†å…ˆçš„æ€§èƒ½åœ¨ä¿¡ä»»ä¼°è®¡ä¸­ï¼Œå¹¶ä¸”é€šè¿‡ä¸‹æ¸¸æ´»åŠ¨å­¦ä»»åŠ¡çš„ç¤ºä¾‹è¡¨æ˜äº†è¯¥æ–¹æ³•çš„ä¼˜åŠ¿ã€‚Hereâ€™s the English version of the three key points:</li>
<li>for: The purpose of this paper is to propose a confidence estimation method for a semi-supervised setting, where most training labels are unavailable.</li>
<li>methods: The method uses the consistency of predictions through the training process as a surrogate function, and proposes a consistency ranking loss function for confidence estimation.</li>
<li>results: On both image classification and segmentation tasks, the proposed method achieves state-of-the-art performances in confidence estimation, and demonstrates its advantage through an active learning task.<details>
<summary>Abstract</summary>
Overconfidence is a common issue for deep neural networks, limiting their deployment in real-world applications. To better estimate confidence, existing methods mostly focus on fully-supervised scenarios and rely on training labels. In this paper, we propose the first confidence estimation method for a semi-supervised setting, when most training labels are unavailable. We stipulate that even with limited training labels, we can still reasonably approximate the confidence of model on unlabeled samples by inspecting the prediction consistency through the training process. We use training consistency as a surrogate function and propose a consistency ranking loss for confidence estimation. On both image classification and segmentation tasks, our method achieves state-of-the-art performances in confidence estimation. Furthermore, we show the benefit of the proposed method through a downstream active learning task. The code is available at https://github.com/TopoXLab/consistency-ranking-loss
</details>
<details>
<summary>æ‘˜è¦</summary>
é€šè¿‡è®­ç»ƒè¿‡ç¨‹ä¸­çš„é¢„æµ‹ä¸€è‡´æ€§æ¥ä¼°è®¡æ¨¡å‹çš„è‡ªä¿¡åº¦ï¼Œæˆ‘ä»¬æå‡ºäº†é¦–ä¸ªåœ¨åŠç›‘ç£Settingä¸‹çš„è‡ªä¿¡åº¦ä¼°è®¡æ–¹æ³•ã€‚å³ä½¿æœ‰é™çš„è®­ç»ƒæ ‡ç­¾ï¼Œæˆ‘ä»¬ä»å¯ä»¥é€šè¿‡è®­ç»ƒè¿‡ç¨‹ä¸­çš„é¢„æµ‹ä¸€è‡´æ€§æ¥ç†æƒ³åœ°ä¼°è®¡æ¨¡å‹å¯¹æ— æ ‡ç¤ºæ ·æœ¬çš„è‡ªä¿¡åº¦ã€‚æˆ‘ä»¬ä½¿ç”¨è®­ç»ƒä¸€è‡´æ€§ä½œä¸ºä»£ç†å‡½æ•°ï¼Œå¹¶æå‡ºäº†ä¸€ç§ä¸€è‡´æ’åæŸå¤±ç”¨äºè‡ªä¿¡åº¦ä¼°è®¡ã€‚åœ¨å›¾åƒåˆ†ç±»å’Œ segmentation ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†çŠ¶æ€æœºå™¨äººçš„è¡¨ç°ï¼Œå¹¶ä¸”æˆ‘ä»¬è¿˜è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸‹æ¸¸æ´»åŠ¨å­¦ä»»åŠ¡ä¸­çš„åˆ©å¥½ã€‚ä»£ç å¯ä»¥åœ¨ https://github.com/TopoXLab/consistency-ranking-loss ä¸Šè·å–ã€‚
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Quantification-for-Molecular-Property-Predictions-with-Graph-Neural-Architecture-Search"><a href="#Uncertainty-Quantification-for-Molecular-Property-Predictions-with-Graph-Neural-Architecture-Search" class="headerlink" title="Uncertainty Quantification for Molecular Property Predictions with Graph Neural Architecture Search"></a>Uncertainty Quantification for Molecular Property Predictions with Graph Neural Architecture Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10438">http://arxiv.org/abs/2307.10438</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengli Jiang, Shiyi Qin, Reid C. Van Lehn, Prasanna Balaprakash, Victor M. Zavala</li>
<li>for: ç”¨äº Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€çš„æ€§èƒ½é¢„æµ‹</li>
<li>methods: ä½¿ç”¨è‡ªåŠ¨æœç´¢ç”Ÿæˆé«˜æ€§èƒ½ GNN  ensembleï¼Œå¹¶ä½¿ç”¨ variance decomposition åˆ†è§£æ•°æ®å’Œæ¨¡å‹ä¸ç¡®å®šæ€§</li>
<li>results: åœ¨å¤šä¸ª benchmark æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œåœ¨é¢„æµ‹å‡†ç¡®æ€§å’Œ UQ æ€§èƒ½æ–¹é¢è¶…è¿‡ç°æœ‰æ–¹æ³•ï¼Œå¹¶é€šè¿‡ t-SNE å¯è§†åŒ–æ¢ç´¢åˆ†å­ç‰¹å¾å’Œä¸ç¡®å®šæ€§çš„ç›¸å…³æ€§ã€‚<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have emerged as a prominent class of data-driven methods for molecular property prediction. However, a key limitation of typical GNN models is their inability to quantify uncertainties in the predictions. This capability is crucial for ensuring the trustworthy use and deployment of models in downstream tasks. To that end, we introduce AutoGNNUQ, an automated uncertainty quantification (UQ) approach for molecular property prediction. AutoGNNUQ leverages architecture search to generate an ensemble of high-performing GNNs, enabling the estimation of predictive uncertainties. Our approach employs variance decomposition to separate data (aleatoric) and model (epistemic) uncertainties, providing valuable insights for reducing them. In our computational experiments, we demonstrate that AutoGNNUQ outperforms existing UQ methods in terms of both prediction accuracy and UQ performance on multiple benchmark datasets. Additionally, we utilize t-SNE visualization to explore correlations between molecular features and uncertainty, offering insight for dataset improvement. AutoGNNUQ has broad applicability in domains such as drug discovery and materials science, where accurate uncertainty quantification is crucial for decision-making.
</details>
<details>
<summary>æ‘˜è¦</summary>
å›¾ neural network (GNN) å·²ç»æˆä¸ºåˆ†å­æ€§è´¨é¢„æµ‹ä¸­ä¸€ç§æ˜¾è‘—çš„æ•°æ®é©±åŠ¨æ–¹æ³•ã€‚ç„¶è€Œï¼Œå…¸å‹çš„ GNN æ¨¡å‹æ— æ³•é‡åŒ–é¢„æµ‹ç»“æœçš„ä¸ç¡®å®šæ€§ã€‚è¿™ç§èƒ½åŠ›æ˜¯åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ä½¿æ¨¡å‹ä½¿ç”¨å’Œéƒ¨ç½²çš„ä¿¡ä»»æ€§è´¨çš„å…³é”®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä»‹ç» AutoGNNUQï¼Œä¸€ç§è‡ªåŠ¨ uncertainty quantificationï¼ˆUQï¼‰æ–¹æ³• Ğ´Ğ»Ñåˆ†å­æ€§è´¨é¢„æµ‹ã€‚AutoGNNUQ åˆ©ç”¨æ¶æ„æœç´¢ç”Ÿæˆä¸€ä¸ªé«˜æ€§èƒ½çš„ GNN ensembleï¼Œä»¥ä¾¿ä¼°è®¡é¢„æµ‹ç»“æœçš„ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨å·®åˆ†åˆ†æå°†æ•°æ®ï¼ˆ aleatoricï¼‰å’Œæ¨¡å‹ï¼ˆepistemicï¼‰ä¸ç¡®å®šæ€§åˆ†è§£ï¼Œä¸ºäº†å‡å°‘å®ƒä»¬ã€‚åœ¨æˆ‘ä»¬çš„è®¡ç®—å®éªŒä¸­ï¼Œæˆ‘ä»¬è¯æ˜ AutoGNNUQ åœ¨å¤šä¸ªbenchmarkæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ¯”ç°æœ‰çš„ UQ æ–¹æ³•æ›´é«˜ç²¾åº¦å’Œ UQ æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨ t-SNE å¯è§†åŒ–æ¥æ¢ç´¢åˆ†å­ç‰¹å¾ä¸ä¸ç¡®å®šæ€§ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œä¸ºäº†æ”¹è¿›æ•°æ®é›†ã€‚AutoGNNUQ åœ¨è¯ç‰©æ¢ç´¢å’Œææ–™ç§‘å­¦ç­‰é¢†åŸŸæœ‰å¹¿æ³›çš„åº”ç”¨ï¼Œå› ä¸ºå®ƒå¯ä»¥å‡å°‘åˆ†å­æ€§è´¨é¢„æµ‹ä¸­çš„ä¸ç¡®å®šæ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="A-Bayesian-Programming-Approach-to-Car-following-Model-Calibration-and-Validation-using-Limited-Data"><a href="#A-Bayesian-Programming-Approach-to-Car-following-Model-Calibration-and-Validation-using-Limited-Data" class="headerlink" title="A Bayesian Programming Approach to Car-following Model Calibration and Validation using Limited Data"></a>A Bayesian Programming Approach to Car-following Model Calibration and Validation using Limited Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10437">http://arxiv.org/abs/2307.10437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Franklin Abodo</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ˜¯ä¸ºäº†æä¾›ä¸€ç§å¯ä»¥å‡†ç¡®æ¨¡æ‹Ÿé©¾é©¶è¡Œä¸ºçš„æ¨¡å‹ï¼Œä»¥ä¾¿åœ¨äº¤é€šç ”ç©¶å’Œå·¥ç¨‹ä¸­è®¾è®¡å’Œè¯„ä¼°é“è·¯æ”¹è¿›è®¡åˆ’ã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº†å¾®å‹é©¾é©¶è¡Œä¸ºæ¨¡å‹ï¼Œä»è€Œ derivate macroscopic æªæ–½å¦‚æµé€Ÿå’Œæ‹¥å µã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ¨¡å‹å¤šæ•°åªé€‚ç”¨äºç‰¹å®šçš„äº¤é€šæƒ…å†µå’Œé“è·¯é…ç½®ï¼Œè€Œæ— æ³• direct åº”ç”¨äºå·¥åŒºï¼ˆWZï¼‰çš„æƒ…å†µã€‚å› æ­¤ï¼Œç¾å›½äº¤é€šéƒ¨ï¼ˆUSDOTï¼‰çš„è´Ÿè´£äº¤é€šç ”ç©¶çš„Volpeä¸­å¿ƒè¢«å§”æ‰˜ï¼Œä»¥å¼€å‘ä¸€ç§å¯ä»¥å‡†ç¡®æ¨¡æ‹Ÿé©¾é©¶è¡Œä¸ºçš„CFæ¨¡å‹ï¼Œä»¥ä¾¿åœ¨å·¥åŒºä¸­è¿›è¡Œå®‰å…¨çš„äº¤é€šè§„åˆ’ã€‚</li>
<li>results: åœ¨æ¨¡å‹å¼€å‘è¿‡ç¨‹ä¸­ï¼ŒVolpeç ”ç©¶äººå‘˜å‘ç°äº†å›°éš¾åœ¨æ¨¡å‹kalibraseï¼Œå› æ­¤æå‡ºäº†é—®é¢˜æ˜¯å¦å­˜åœ¨æ¨¡å‹ä¸­çš„é—®é¢˜ï¼Œæ•°æ®ä¸­çš„é—®é¢˜ï¼Œæˆ–è€… kalibrase è¿‡ç¨‹ä¸­çš„é—®é¢˜ã€‚æœ¬è®ºæ–‡ä½¿ç”¨ bayesian æ–¹æ³•è¿›è¡Œæ•°æ®åˆ†æå’Œå‚æ•°ä¼°è®¡ï¼Œä»¥æ¢è®¨å’Œè§£å†³è¿™äº›é—®é¢˜ã€‚é¦–å…ˆï¼Œä½¿ç”¨ bayesian æ¨ç†æµ‹é‡æ•°æ®é›†çš„å……åˆ†æ€§ã€‚å…¶æ¬¡ï¼Œæ¯”è¾ƒ Volpe ç ”ç©¶äººå‘˜ä½¿ç”¨çš„ Genetic Algorithm åŸºäº calibration çš„è¿‡ç¨‹å’Œ bayesian calibration çš„ç»“æœã€‚æœ€åï¼Œé€šè¿‡ä½¿ç”¨å·²æœ‰çš„ CF æ¨¡å‹ï¼ŒWiedemann 99ï¼Œå¯¹ Volpe æ¨¡å‹è¿›è¡Œ probabilistic æ¨¡å‹åŒ–ã€‚éªŒè¯æ˜¯é€šè¿‡ä¿¡æ¯ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ä¼°è®¡ predictive å‡†ç¡®æ€§æ¥è¿›è¡Œã€‚<details>
<summary>Abstract</summary>
Traffic simulation software is used by transportation researchers and engineers to design and evaluate changes to roadways. These simulators are driven by models of microscopic driver behavior from which macroscopic measures like flow and congestion can be derived. Many models are designed for a subset of possible traffic scenarios and roadway configurations, while others have no explicit constraints on their application. Work zones (WZs) are one scenario for which no model to date has reproduced realistic driving behavior. This makes it difficult to optimize for safety and other metrics when designing a WZ. The Federal Highway Administration commissioned the USDOT Volpe Center to develop a car-following (CF) model for use in microscopic simulators that can capture and reproduce driver behavior accurately within and outside of WZs. Volpe also performed a naturalistic driving study to collect telematics data from vehicles driven on roads with WZs for use in model calibration. During model development, Volpe researchers observed difficulties in calibrating their model, leaving them to question whether there existed flaws in their model, in the data, or in the procedure used to calibrate the model using the data. In this thesis, I use Bayesian methods for data analysis and parameter estimation to explore and, where possible, address these questions. First, I use Bayesian inference to measure the sufficiency of the size of the data set. Second, I compare the procedure and results of the genetic algorithm based calibration performed by the Volpe researchers with those of Bayesian calibration. Third, I explore the benefits of modeling CF hierarchically. Finally, I apply what was learned in the first three phases using an established CF model, Wiedemann 99, to the probabilistic modeling of the Volpe model. Validation is performed using information criteria as an estimate of predictive accuracy.
</details>
<details>
<summary>æ‘˜è¦</summary>
äº¤é€šæ¨¡æ‹Ÿè½¯ä»¶è¢«è¿è¾“ç ”ç©¶äººå‘˜å’Œå·¥ç¨‹å¸ˆä½¿ç”¨æ¥è®¾è®¡å’Œè¯„ä¼°è·¯å¾„å˜åŒ–ã€‚è¿™äº›æ¨¡æ‹Ÿå™¨ç”±å¾®å‹é©¾é©¶è¡Œä¸ºæ¨¡å‹é©±åŠ¨ï¼Œä»è€Œ derive æµå’Œå µå¡ç­‰å®è§‚æŒ‡æ ‡ã€‚è®¸å¤šæ¨¡å‹é€‚ç”¨äºç‰¹å®šçš„äº¤é€šenario å’Œè·¯å¾„é…ç½®ï¼Œè€Œå…¶ä»–äº›æ²¡æœ‰æ˜ç¡®çš„çº¦æŸã€‚å·¥åœ°ï¼ˆWZï¼‰æ˜¯ä¸€ç§ scenarios  Ğ´Ğ»Ñ which no model to date has reproduced realistic driving behavior. è¿™ä½¿å¾—åœ¨è®¾è®¡å·¥åœ°æ—¶ diffficult to optimize for safety and other metricsã€‚ç¾å›½å…¬è·¯ç®¡ç†å±€å§”æ‰˜ç¾å›½äº¤é€šéƒ¨Volpe Center å¼€å‘ä¸€ä¸ªå¯ä»¥åœ¨å¾®å‹æ¨¡æ‹Ÿå™¨ä¸­Capture å’Œé‡ç°é©¾é©¶è¡Œä¸ºçš„ car-following ï¼ˆCFï¼‰æ¨¡å‹ã€‚Volpe è¿˜æ‰§è¡Œäº†ä¸€é¡¹è‡ªç„¶é©¾é©¶ç ”ç©¶ï¼Œæ”¶é›†äº†åœ¨è·¯å¾„ä¸Šé©¾é©¶çš„ vehicless çš„ telematics æ•°æ®ï¼Œç”¨äºæ¨¡å‹å‡è¡¡ã€‚åœ¨æ¨¡å‹å¼€å‘è¿‡ç¨‹ä¸­ï¼ŒVolpe ç ”ç©¶äººå‘˜æ³¨æ„åˆ°äº†å›°éš¾åœ¨å‡è¡¡æ¨¡å‹ï¼Œä½¿å¾—ä»–ä»¬å¼€å§‹æé—®æ˜¯å¦å­˜åœ¨æ¨¡å‹ä¸­çš„æ¯›ç—…ã€æ•°æ®ä¸­çš„æ¯›ç—…æˆ–è€…å‡è¡¡æ¨¡å‹ä½¿ç”¨æ•°æ®çš„è¿‡ç¨‹ä¸­çš„æ¯›ç—…ã€‚åœ¨è¿™ä¸ªè®ºæ–‡ä¸­ï¼Œæˆ‘ä½¿ç”¨ bayesian æ–¹æ³•æ¥åˆ†ææ•°æ®å’Œå‚æ•°ä¼°è®¡ï¼Œä»¥æ¢ç´¢å’Œè§£å†³è¿™äº›é—®é¢˜ã€‚é¦–å…ˆï¼Œæˆ‘ä½¿ç”¨ bayesian æ¨ç†æ¥æµ‹è¯•æ•°æ®é›†çš„å¤§å°æ˜¯å¦å……åˆ†ã€‚å…¶æ¬¡ï¼Œæˆ‘æ¯”è¾ƒäº† Volpe ç ”ç©¶äººå‘˜ä½¿ç”¨çš„ genetic algorithm åŸºäºçš„å‡è¡¡è¿‡ç¨‹å’Œ bayesian å‡è¡¡è¿‡ç¨‹çš„ç»“æœã€‚æœ€åï¼Œæˆ‘æ¢ç´¢äº†æ¨¡å‹CF çš„å±‚æ¬¡ç»“æ„åŒ–çš„å¥½å¤„ã€‚æœ€åï¼Œæˆ‘ä½¿ç”¨ä¸€ä¸ªå·²çŸ¥çš„ CF æ¨¡å‹ï¼ŒWiedemann 99ï¼Œæ¥åº”ç”¨åœ¨ Volpe æ¨¡å‹ä¸Šã€‚éªŒè¯æ˜¯ä½¿ç”¨ä¿¡æ¯ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ion æ¥ä¼°è®¡é¢„æµ‹ç²¾åº¦ã€‚
</details></li>
</ul>
<hr>
<h2 id="A-Matrix-Ensemble-Kalman-Filter-based-Multi-arm-Neural-Network-to-Adequately-Approximate-Deep-Neural-Networks"><a href="#A-Matrix-Ensemble-Kalman-Filter-based-Multi-arm-Neural-Network-to-Adequately-Approximate-Deep-Neural-Networks" class="headerlink" title="A Matrix Ensemble Kalman Filter-based Multi-arm Neural Network to Adequately Approximate Deep Neural Networks"></a>A Matrix Ensemble Kalman Filter-based Multi-arm Neural Network to Adequately Approximate Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10436">http://arxiv.org/abs/2307.10436</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ved-piyush/menkf-ann-pul">https://github.com/ved-piyush/menkf-ann-pul</a></li>
<li>paper_authors: Ved Piyush, Yuchen Yan, Yuzhen Zhou, Yanbin Yin, Souparno Ghosh</li>
<li>for: This paper aims to propose a new technique for approximating deep learning models, specifically long short-term memory (LSTM) networks, using a Kalman filter-based approach.</li>
<li>methods: The proposed method, called Matrix Ensemble Kalman Filter-based multi-arm ANN (MEnKF-ANN), uses a multi-arm extension of a Kalman filter to approximate LSTM networks, and also performs explicit model stacking to handle unequal-size feature sets.</li>
<li>results: The proposed method can adequately approximate LSTM networks trained to classify carbohydrate substrates based on genomic sequences, and can also provide uncertainty estimates for the predictions.<details>
<summary>Abstract</summary>
Deep Learners (DLs) are the state-of-art predictive mechanism with applications in many fields requiring complex high dimensional data processing. Although conventional DLs get trained via gradient descent with back-propagation, Kalman Filter (KF)-based techniques that do not need gradient computation have been developed to approximate DLs. We propose a multi-arm extension of a KF-based DL approximator that can mimic DL when the sample size is too small to train a multi-arm DL. The proposed Matrix Ensemble Kalman Filter-based multi-arm ANN (MEnKF-ANN) also performs explicit model stacking that becomes relevant when the training sample has an unequal-size feature set. Our proposed technique can approximate Long Short-term Memory (LSTM) Networks and attach uncertainty to the predictions obtained from these LSTMs with desirable coverage. We demonstrate how MEnKF-ANN can "adequately" approximate an LSTM network trained to classify what carbohydrate substrates are digested and utilized by a microbiome sample whose genomic sequences consist of polysaccharide utilization loci (PULs) and their encoded genes.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ·±åº¦å­¦ä¹ å™¨ï¼ˆDLï¼‰æ˜¯ç°ä»£é¢„æµ‹æœºåˆ¶çš„å·æ ‡ï¼Œåº”ç”¨äºéœ€è¦å¤æ‚é«˜ç»´æ•°æ®å¤„ç†çš„å¤šä¸ªé¢†åŸŸã€‚å°½ç®¡ä¼ ç»Ÿçš„DLé€šè¿‡æ¢¯åº¦ä¸‹é™å’Œåæ¨è®­ç»ƒï¼Œä½†æ˜¯ä½¿ç”¨Kalmanæ»¤æ³¢å™¨ï¼ˆKFï¼‰æŠ€æœ¯ä¸éœ€è¦è®¡ç®—æ¢¯åº¦çš„æ–¹æ³•å·²ç»å¼€å‘å‡ºæ¥ã€‚æˆ‘ä»¬æè®®ä¸€ç§åŸºäºKFçš„å¤šè‡‚ANNï¼ˆMEnKF-ANNï¼‰ï¼Œå¯ä»¥æ¨¡æ‹ŸDLï¼Œå³ä½¿è®­ç»ƒæ ·æœ¬è§„æ¨¡å¤ªå°ã€‚æˆ‘ä»¬çš„æè®®æŠ€æœ¯è¿˜å®ç°äº†æ˜¾å¼æ¨¡å‹å †å ï¼Œå½“ç‰¹å¾é›†çš„å¤§å°ä¸åŒæ—¶å˜å¾—æœ‰ç”¨ã€‚æˆ‘ä»¬çš„æè®®æŠ€æœ¯å¯ä»¥æ¨¡æ‹Ÿé•¿æœŸçŸ­ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒç½‘ç»œï¼ˆLSTMï¼‰ï¼Œå¹¶å°†å¯¹åº”çš„é¢„æµ‹ç»“æœæ·»åŠ ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬ç¤ºä¾‹äº†MEnKF-ANNå¯ä»¥â€œåˆç†â€åœ°æ¨¡æ‹Ÿä¸€ä¸ªåŸºäºPULså’Œå…¶ç¼–ç çš„å¾®ç”Ÿç‰©æ‰¹å¤„ç†è®­ç»ƒé›†ï¼Œç”¨äºé¢„æµ‹å¾®ç”Ÿç‰©æ ·æœ¬ä¸­åƒæ‰å’Œåˆ©ç”¨çš„ç¢³æ°´åŒ–åˆç‰©substrateã€‚
</details></li>
</ul>
<hr>
<h2 id="Learning-Formal-Specifications-from-Membership-and-Preference-Queries"><a href="#Learning-Formal-Specifications-from-Membership-and-Preference-Queries" class="headerlink" title="Learning Formal Specifications from Membership and Preference Queries"></a>Learning Formal Specifications from Membership and Preference Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10434">http://arxiv.org/abs/2307.10434</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ameesh Shah, Marcell Vazquez-Chanlatte, Sebastian Junges, Sanjit A. Seshia</li>
<li>for: å­¦ä¹ å½¢å¼è§„å®šï¼ˆå¦‚è‡ªåŠ¨æœºï¼‰çš„æ­£å¼è§„å®š</li>
<li>methods: æè®®ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œ combining membership labelså’Œå¯¹æ¯” preferenceï¼Œä»¥ä¾¿æ›´åŠ çµæ´»åœ°è¿›è¡Œæ´»åŠ¨è§„å®šå­¦ä¹ </li>
<li>results: åœ¨ä¸¤ä¸ªä¸åŒçš„é¢†åŸŸä¸­å®ç°äº†æ¡†æ¶ï¼Œå¹¶è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å¼ºå¥åœ°å’Œæ–¹ä¾¿åœ°é€šè¿‡å¯¹æ¯”å’Œæˆå‘˜æ ‡ç­¾æ¥è¯†åˆ«è§„å®šã€‚<details>
<summary>Abstract</summary>
Active learning is a well-studied approach to learning formal specifications, such as automata. In this work, we extend active specification learning by proposing a novel framework that strategically requests a combination of membership labels and pair-wise preferences, a popular alternative to membership labels. The combination of pair-wise preferences and membership labels allows for a more flexible approach to active specification learning, which previously relied on membership labels only. We instantiate our framework in two different domains, demonstrating the generality of our approach. Our results suggest that learning from both modalities allows us to robustly and conveniently identify specifications via membership and preferences.
</details>
<details>
<summary>æ‘˜è¦</summary>
active learningæ˜¯ä¸€ç§å·²ç»å¹¿æ³›ç ”ç©¶çš„å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºå­¦ä¹ æ­£å¼è§„åˆ™ï¼Œå¦‚è‡ªåŠ¨æœºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†æ´»åŠ¨è§„åˆ™å­¦ä¹ æ¡†æ¶æ‰©å±•åˆ°è¯·æ±‚ç»„åˆä¼šå‘˜æ ‡ç­¾å’Œå¯¹æ¯”æ€§åå¥½ã€‚è¿™ç§ç»„åˆæ–¹å¼å…è®¸æˆ‘ä»¬æ›´åŠ çµæ´»åœ°è¿›è¡Œæ´»åŠ¨è§„åˆ™å­¦ä¹ ï¼Œä¹‹å‰åªèƒ½é€šè¿‡ä¼šå‘˜æ ‡ç­¾è¿›è¡Œå­¦ä¹ ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªä¸åŒé¢†åŸŸä¸­å®ç°äº†æˆ‘ä»¬çš„æ¡†æ¶ï¼Œå¹¶è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„é€šç”¨æ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä»ä¸¤ç§æ¨¡å¼å­¦ä¹ å¯ä»¥å¼ºå¤§åœ°å’Œæ–¹ä¾¿åœ°è¯†åˆ«è§„åˆ™viaä¼šå‘˜å’Œåå¥½ã€‚
</details></li>
</ul>
<hr>
<h2 id="DP-TBART-A-Transformer-based-Autoregressive-Model-for-Differentially-Private-Tabular-Data-Generation"><a href="#DP-TBART-A-Transformer-based-Autoregressive-Model-for-Differentially-Private-Tabular-Data-Generation" class="headerlink" title="DP-TBART: A Transformer-based Autoregressive Model for Differentially Private Tabular Data Generation"></a>DP-TBART: A Transformer-based Autoregressive Model for Differentially Private Tabular Data Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10430">http://arxiv.org/abs/2307.10430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rodrigo Castellon, Achintya Gopal, Brian Bloniarz, David Rosenberg</li>
<li>for: ç”Ÿæˆå…·æœ‰åˆ†å¸ƒå¼éšç§çš„ tabular æ•°æ®</li>
<li>methods: ä½¿ç”¨ transformer æ¨¡å‹å®ç° differentially-private æ¨è®º</li>
<li>results: åœ¨å„ç§æ•°æ®é›†ä¸Šè¾¾åˆ°ä¸ marginal-based æ–¹æ³•ç«äº‰çš„æ€§èƒ½ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³è¶…è¶ŠçŠ¶æ€ä¹‹arte æ–¹æ³•è¡¨ç°Hereâ€™s the translation in Simplified Chinese:</li>
<li>for: ç”Ÿæˆå…·æœ‰åˆ†å¸ƒå¼éšç§çš„ tabular æ•°æ®</li>
<li>methods: ä½¿ç”¨ transformer æ¨¡å‹å®ç° differentially-private æ¨è®º</li>
<li>results: åœ¨å„ç§æ•°æ®é›†ä¸Šè¾¾åˆ°ä¸ marginal-based æ–¹æ³•ç«äº‰çš„æ€§èƒ½ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³è¶…è¶ŠçŠ¶æ€ä¹‹arte æ–¹æ³•è¡¨ç°<details>
<summary>Abstract</summary>
The generation of synthetic tabular data that preserves differential privacy is a problem of growing importance. While traditional marginal-based methods have achieved impressive results, recent work has shown that deep learning-based approaches tend to lag behind. In this work, we present Differentially-Private TaBular AutoRegressive Transformer (DP-TBART), a transformer-based autoregressive model that maintains differential privacy and achieves performance competitive with marginal-based methods on a wide variety of datasets, capable of even outperforming state-of-the-art methods in certain settings. We also provide a theoretical framework for understanding the limitations of marginal-based approaches and where deep learning-based approaches stand to contribute most. These results suggest that deep learning-based techniques should be considered as a viable alternative to marginal-based methods in the generation of differentially private synthetic tabular data.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œ differential privacy çš„ synthetic è¡¨æ ¼æ•°æ®ç”Ÿæˆé—®é¢˜æ˜¯ä¸€ä¸ªæ—¥ç›Šé‡è¦çš„é—®é¢˜ã€‚ä¼ ç»Ÿçš„è¾¹ç¼˜åŸºäºæ–¹æ³•å·²ç»å–å¾—äº†å¾ˆå¥½çš„æˆç»©ï¼Œä½†æœ€è¿‘çš„å·¥ä½œè¡¨æ˜ï¼Œæ·±åº¦å­¦ä¹ åŸºäºæ–¹æ³•åœ¨è¿™ä¸ªé¢†åŸŸæ¯”è¾ƒè½åã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åä¸º Differentially-Private TaBular AutoRegressive Transformer (DP-TBART)ï¼Œè¿™æ˜¯ä¸€ç§åŸºäº transformer çš„è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥ä¿æŒ differential privacy å¹¶åœ¨å„ç§æ•°æ®é›†ä¸Šè¾¾åˆ°ä¸è¾¹ç¼˜åŸºäºæ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªç†è®ºæ¡†æ¶ï¼Œç”¨äºç†è§£è¾¹ç¼˜åŸºäºæ–¹æ³•çš„å±€é™æ€§ï¼Œä»¥åŠæ·±åº¦å­¦ä¹ åŸºäºæ–¹æ³•åœ¨è¿™ä¸ªé¢†åŸŸä¸­çš„æ½œåœ¨è´¡çŒ®ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæ·±åº¦å­¦ä¹ åŸºäºæ–¹æ³•åº”è¯¥è¢«è§†ä¸º differential privacy ç”Ÿæˆ synthetic è¡¨æ ¼æ•°æ®çš„å¯è¡Œçš„æ›¿ä»£æ–¹æ¡ˆã€‚â€Note: Please note that the translation is in Simplified Chinese, and the word order and grammar may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="PreDiff-Precipitation-Nowcasting-with-Latent-Diffusion-Models"><a href="#PreDiff-Precipitation-Nowcasting-with-Latent-Diffusion-Models" class="headerlink" title="PreDiff: Precipitation Nowcasting with Latent Diffusion Models"></a>PreDiff: Precipitation Nowcasting with Latent Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10422">http://arxiv.org/abs/2307.10422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhihan Gao, Xingjian Shi, Boran Han, Hao Wang, Xiaoyong Jin, Danielle Maddix, Yi Zhu, Mu Li, Yuyang Wang</li>
<li>for: é¢„æµ‹åœ°çƒç³»ç»Ÿçš„æœªæ¥çŠ¶å†µï¼Œä½¿ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯æ¥å¤„ç†å¤§é‡çš„ç©ºé—´æ—¶é—´æ•°æ®ã€‚</li>
<li>methods: æå‡ºäº†ä¸€ç§ä¸¤stageç®¡é“ï¼Œé¦–å…ˆå¼€å‘äº†ä¸€ç§å¯èƒ½æ€§æ‰©æ•£æ¨¡å‹ï¼ˆPreDiffï¼‰ï¼Œå…¶å¯ä»¥è¿›è¡Œ probabilistic é¢„æµ‹ã€‚å…¶æ¬¡ï¼Œé€šè¿‡explicitåœ°æ§åˆ¶çŸ¥è¯†æœºåˆ¶ï¼Œä½¿é¢„æµ‹ç»“æœä¸ä¸“ä¸šçŸ¥è¯†ç›¸ä¸€è‡´ã€‚</li>
<li>results: é€šè¿‡åœ¨Synthetic dataset N-body MNISTå’Œå®é™… precipitation nowcasting dataset SEVIRè¿›è¡Œå®éªŒï¼Œç¡®è®¤äº†PreDiffçš„å¯è¡Œæ€§å’ŒDomain-specific prior knowledgeçš„å¯æ§æ€§ï¼Œå¹¶ä¸”é¢„æµ‹ç»“æœå…·æœ‰é«˜åº¦çš„æ“ä½œå®ç”¨æ€§ã€‚<details>
<summary>Abstract</summary>
Earth system forecasting has traditionally relied on complex physical models that are computationally expensive and require significant domain expertise. In the past decade, the unprecedented increase in spatiotemporal Earth observation data has enabled data-driven forecasting models using deep learning techniques. These models have shown promise for diverse Earth system forecasting tasks but either struggle with handling uncertainty or neglect domain-specific prior knowledge, resulting in averaging possible futures to blurred forecasts or generating physically implausible predictions. To address these limitations, we propose a two-stage pipeline for probabilistic spatiotemporal forecasting: 1) We develop PreDiff, a conditional latent diffusion model capable of probabilistic forecasts. 2) We incorporate an explicit knowledge control mechanism to align forecasts with domain-specific physical constraints. This is achieved by estimating the deviation from imposed constraints at each denoising step and adjusting the transition distribution accordingly. We conduct empirical studies on two datasets: N-body MNIST, a synthetic dataset with chaotic behavior, and SEVIR, a real-world precipitation nowcasting dataset. Specifically, we impose the law of conservation of energy in N-body MNIST and anticipated precipitation intensity in SEVIR. Experiments demonstrate the effectiveness of PreDiff in handling uncertainty, incorporating domain-specific prior knowledge, and generating forecasts that exhibit high operational utility.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ°çƒç³»ç»Ÿé¢„æµ‹ Traditional ä¾é å¤æ‚çš„ç‰©ç†æ¨¡å‹ï¼Œcomputationally expensive å’Œéœ€è¦ä¸“ä¸šçŸ¥è¯†ã€‚ last decade, the unprecedented increase in spatiotemporal Earth observation data ä½¿å¾— data-driven forecasting models using deep learning techniques è¡¨ç°å‡ºäº† promise  Ğ´Ğ»Ñ diverse Earth system forecasting tasksã€‚ However, these models either struggle with handling uncertainty æˆ– neglect domain-specific prior knowledge, resulting in averaging possible futures to blurred forecasts or generating physically implausible predictions. To address these limitations, we propose a two-stage pipeline for probabilistic spatiotemporal forecasting:1. We develop PreDiff, a conditional latent diffusion model capable of probabilistic forecasts.2. We incorporate an explicit knowledge control mechanism to align forecasts with domain-specific physical constraints. This is achieved by estimating the deviation from imposed constraints at each denoising step and adjusting the transition distribution accordingly.We conduct empirical studies on two datasets: N-body MNIST, a synthetic dataset with chaotic behavior, and SEVIR, a real-world precipitation nowcasting dataset. Specifically, we impose the law of conservation of energy in N-body MNIST and anticipated precipitation intensity in SEVIR. Experiments demonstrate the effectiveness of PreDiff in handling uncertainty, incorporating domain-specific prior knowledge, and generating forecasts that exhibit high operational utility.
</details></li>
</ul>
<hr>
<h2 id="Technical-Challenges-of-Deploying-Reinforcement-Learning-Agents-for-Game-Testing-in-AAA-Games"><a href="#Technical-Challenges-of-Deploying-Reinforcement-Learning-Agents-for-Game-Testing-in-AAA-Games" class="headerlink" title="Technical Challenges of Deploying Reinforcement Learning Agents for Game Testing in AAA Games"></a>Technical Challenges of Deploying Reinforcement Learning Agents for Game Testing in AAA Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11105">http://arxiv.org/abs/2307.11105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonas Gillberg, Joakim Bergdahl, Alessandro Sestini, Andrew Eakins, Linus Gisslen</li>
<li>for: è¿™ä¸ªæŠ€æœ¯è®ºæ–‡æ˜¯ä¸ºäº†æ¨å¹¿æ¸¸æˆç”Ÿäº§ä¸­çš„æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œç‰¹æ„æ˜¯é€šè¿‡è®©æ¸¸æˆè‡ªåŠ¨åŒ–æµ‹è¯•è§£å†³æ–¹æ¡ˆä¸­åŠ å…¥äº†å®éªŒæ€§çš„å­¦ä¹ ç³»ç»Ÿæ¥æé«˜æµ‹è¯•è¦†ç›–ç‡ã€‚</li>
<li>methods: è¿™ç¯‡æŠ€æœ¯è®ºæ–‡æè¿°äº†ä¸€ç§å°†å­¦ä¹ ç³»ç»Ÿä¸ç°æœ‰çš„è„šæœ¬åŒ–æµ‹è¯•è§£å†³æ–¹æ¡ˆé›†æˆï¼Œä»¥æé«˜æµ‹è¯•è¦†ç›–ç‡ã€‚å…·ä½“æ¥è¯´ï¼Œä»–ä»¬ä½¿ç”¨äº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œé€šè¿‡è®©æœºå™¨å­¦ä¹ ç®—æ³•å­¦ä¹ è‡ªåŠ¨åŒ–æµ‹è¯•è¿‡ç¨‹ä¸­çš„ä¼˜åŒ–ç­–ç•¥ï¼Œä»¥æé«˜æµ‹è¯•æ•ˆæœã€‚</li>
<li>results: æ®æ–‡ç« æŠ¥é“ï¼Œé€šè¿‡å°†å­¦ä¹ ç³»ç»Ÿä¸è„šæœ¬åŒ–æµ‹è¯•è§£å†³æ–¹æ¡ˆé›†æˆï¼Œå¯ä»¥æœ‰æ•ˆæé«˜æµ‹è¯•è¦†ç›–ç‡ï¼Œå¹¶ä¸”åœ¨ä¸€äº›AAAæ¸¸æˆï¼Œå¦‚ã€Šæˆ˜åœº2042ã€‹å’Œã€Šé»‘æš—ç©ºé—´2023ã€‹ä¸­å®ç°äº†ä¸€å®šçš„æˆæœã€‚<details>
<summary>Abstract</summary>
Going from research to production, especially for large and complex software systems, is fundamentally a hard problem. In large-scale game production, one of the main reasons is that the development environment can be very different from the final product. In this technical paper we describe an effort to add an experimental reinforcement learning system to an existing automated game testing solution based on scripted bots in order to increase its capacity. We report on how this reinforcement learning system was integrated with the aim to increase test coverage similar to [1] in a set of AAA games including Battlefield 2042 and Dead Space (2023). The aim of this technical paper is to show a use-case of leveraging reinforcement learning in game production and cover some of the largest time sinks anyone who wants to make the same journey for their game may encounter. Furthermore, to help the game industry to adopt this technology faster, we propose a few research directions that we believe will be valuable and necessary for making machine learning, and especially reinforcement learning, an effective tool in game production.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä»ç ”ç©¶åˆ°ç”Ÿäº§ï¼Œç‰¹åˆ«æ˜¯ Ğ´Ğ»Ñå¤§å‹å’Œå¤æ‚çš„è½¯ä»¶ç³»ç»Ÿï¼Œæ˜¯ä¸€ä¸ªåŸºæœ¬é—®é¢˜ã€‚åœ¨å¤§è§„æ¨¡æ¸¸æˆç”Ÿäº§ä¸­ï¼Œä¸€ä¸ªä¸»è¦åŸå› æ˜¯å¼€å‘ç¯å¢ƒå’Œäº§å“ç¯å¢ƒä¹‹é—´çš„å·®å¼‚ã€‚åœ¨è¿™ä»½æŠ€æœ¯è‘—ä½œä¸­ï¼Œæˆ‘ä»¬æè¿°äº†å°†å®éªŒå¼å­¦ä¹ ç³»ç»Ÿæ·»åŠ åˆ°ç°æœ‰çš„è‡ªåŠ¨æ¸¸æˆæµ‹è¯•è§£å†³æ–¹æ¡ˆåŸºäºè„šæœ¬å¼ Bot ä»¥å¢åŠ å…¶å®¹é‡çš„å°è¯•ã€‚æˆ‘ä»¬æŠ¥å‘Šäº†åœ¨ä¸€äº› AAA æ¸¸æˆï¼ŒåŒ…æ‹¬ Battlefield 2042 å’Œ Dead Space (2023) ä¸­å°†è¿™ä¸ªå­¦ä¹ ç³»ç»Ÿæ•´åˆçš„æˆæœï¼Œå¹¶å¸Œæœ›è¿™ä»½æŠ€æœ¯è‘—ä½œå¯ä»¥æ˜¾ç¤ºæ¸¸æˆç”Ÿäº§ä¸­å¦‚ä½•ä½¿ç”¨å­¦ä¹ æœºå™¨äººï¼Œä»¥åŠä¸€äº›å¯èƒ½ä¼šé‡åˆ°çš„ä¸»è¦æ—¶é—´æ½œåœ¨éšœç¢ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¸®åŠ©æ¸¸æˆä¸šç•Œæ›´å¿«åœ°é‡‡çº³è¿™æŠ€æœ¯ï¼Œæˆ‘ä»¬å»ºè®®äº†ä¸€äº›ç ”ç©¶æ–¹å‘ï¼Œæˆ‘ä»¬è®¤ä¸ºè¿™äº›ç ”ç©¶æ–¹å‘å°†æ˜¯æœ‰ä»·å€¼å’Œå¿…è¦çš„ï¼Œä»¥ä¾¿åœ¨æ¸¸æˆç”Ÿäº§ä¸­ä½¿ç”¨æœºå™¨å­¦ä¹ å’Œç‰¹åˆ«æ˜¯å®éªŒå­¦ä¹ ã€‚
</details></li>
</ul>
<hr>
<h2 id="Interpreting-and-Correcting-Medical-Image-Classification-with-PIP-Net"><a href="#Interpreting-and-Correcting-Medical-Image-Classification-with-PIP-Net" class="headerlink" title="Interpreting and Correcting Medical Image Classification with PIP-Net"></a>Interpreting and Correcting Medical Image Classification with PIP-Net</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10404">http://arxiv.org/abs/2307.10404</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/m-nauta/pipnet">https://github.com/m-nauta/pipnet</a></li>
<li>paper_authors: Meike Nauta, Johannes H. Hegeman, Jeroen Geerdink, JÃ¶rg SchlÃ¶tterer, Maurice van Keulen, Christin Seifert</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨æ¢è®¨å¯è§£é‡Šçš„æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨åŒ»å­¦å½±åƒåˆ†ç±» tasks ä¸­çš„åº”ç”¨æ€§å’Œæ½œåŠ›ã€‚</li>
<li>methods: è®ºæ–‡ä½¿ç”¨çš„æ˜¯PIP-Netæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§å¯è§£é‡Šçš„å›¾åƒåˆ†ç±»æ¨¡å‹ï¼Œå®ƒå­¦ä¹ äº†äººç±»ç†è§£çš„å›¾åƒéƒ¨ä»¶ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼ŒPIP-Net çš„å†³ç­–è¿‡ç¨‹ä¸åŒ»å­¦åˆ†ç±»æ ‡å‡†ç›¸ä¸€è‡´ï¼Œåªéœ€è¦æä¾›å›¾åƒçº§åˆ«çš„ç±»åˆ«æ ‡ç­¾ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°äº†å¦‚ä½•é€šè¿‡ç›´æ¥ç¦ç”¨ä¸æƒ³è¦çš„åŸå‹æ¥äººå·¥ä¿®æ­£PIP-Netçš„æ€ç»´ã€‚I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Part-prototype models are explainable-by-design image classifiers, and a promising alternative to black box AI. This paper explores the applicability and potential of interpretable machine learning, in particular PIP-Net, for automated diagnosis support on real-world medical imaging data. PIP-Net learns human-understandable prototypical image parts and we evaluate its accuracy and interpretability for fracture detection and skin cancer diagnosis. We find that PIP-Net's decision making process is in line with medical classification standards, while only provided with image-level class labels. Because of PIP-Net's unsupervised pretraining of prototypes, data quality problems such as undesired text in an X-ray or labelling errors can be easily identified. Additionally, we are the first to show that humans can manually correct the reasoning of PIP-Net by directly disabling undesired prototypes. We conclude that part-prototype models are promising for medical applications due to their interpretability and potential for advanced model debugging.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>>æœ¬æ–‡æ¢è®¨äº†å¯è§£é‡Šå‹æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†ç±» task ä¸­çš„å¯è¡Œæ€§å’Œæ½œåŠ›ã€‚ Specifically, we explore the applicability and potential of interpretable machine learning, in particular PIP-Net, for automated diagnosis support on real-world medical imaging data. PIP-Net learns human-understandable prototypical image parts and we evaluate its accuracy and interpretability for fracture detection and skin cancer diagnosis. We find that PIP-Net's decision-making process is in line with medical classification standards, while only provided with image-level class labels. Additionally, we show that humans can manually correct the reasoning of PIP-Net by directly disabling undesired prototypes. We conclude that part-prototype models are promising for medical applications due to their interpretability and potential for advanced model debugging.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Selection-functions-of-strong-lens-finding-neural-networks"><a href="#Selection-functions-of-strong-lens-finding-neural-networks" class="headerlink" title="Selection functions of strong lens finding neural networks"></a>Selection functions of strong lens finding neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10355">http://arxiv.org/abs/2307.10355</a></li>
<li>repo_url: None</li>
<li>paper_authors: A. Herle, C. M. Oâ€™Riordan, S. Vegetti</li>
<li>for: è¿™ä¸ªè®ºæ–‡ä¸»è¦ç›®çš„æ˜¯ç ”ç©¶æ·± gravitational lens ç³»ç»Ÿä¸­ neural network çš„åè¢‹æ€§ã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº†ç±»ä¼¼äºå¸¸è§ literatura ä¸­ä½¿ç”¨çš„ Convolutional Neural Networks å’Œä¸‰ä¸ªä¸åŒçš„è®­ç»ƒé›†æ¥ç ”ç©¶ lens finding  neural network çš„é€‰æ‹©å‡½æ•°ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œè¿™äº› neural network åå¥½ larger Einstein radii å’Œæ›´é›†ä¸­çš„ source-light distributionsã€‚å¢åŠ æ£€æµ‹é‡è¦æ€§é˜ˆå€¼å¯ä»¥æ”¹å–„é€‰æ‹©å‡½æ•°çš„æ•ˆæœã€‚<details>
<summary>Abstract</summary>
Convolution Neural Networks trained for the task of lens finding with similar architecture and training data as is commonly found in the literature are biased classifiers. An understanding of the selection function of lens finding neural networks will be key to fully realising the potential of the large samples of strong gravitational lens systems that will be found in upcoming wide-field surveys. We use three training datasets, representative of those used to train galaxy-galaxy and galaxy-quasar lens finding neural networks. The networks preferentially select systems with larger Einstein radii and larger sources with more concentrated source-light distributions. Increasing the detection significance threshold to 12$\sigma$ from 8$\sigma$ results in 50 per cent of the selected strong lens systems having Einstein radii $\theta_\mathrm{E}$ $\ge$ 1.04 arcsec from $\theta_\mathrm{E}$ $\ge$ 0.879 arcsec, source radii $R_S$ $\ge$ 0.194 arcsec from $R_S$ $\ge$ 0.178 arcsec and source S\'ersic indices $n_{\mathrm{Sc}^{\mathrm{S}$ $\ge$ 2.62 from $n_{\mathrm{Sc}^{\mathrm{S}$ $\ge$ 2.55. The model trained to find lensed quasars shows a stronger preference for higher lens ellipticities than those trained to find lensed galaxies. The selection function is independent of the slope of the power-law of the mass profiles, hence measurements of this quantity will be unaffected. The lens finder selection function reinforces that of the lensing cross-section, and thus we expect our findings to be a general result for all galaxy-galaxy and galaxy-quasar lens finding neural networks.
</details>
<details>
<summary>æ‘˜è¦</summary>
convolutional neural networks ç‰¹åˆ«æ˜¯ç”¨äºè¿™ä¸ªä»»åŠ¡çš„ Architecture å’Œè®­ç»ƒæ•°æ®ï¼Œå³é€šå¸¸åœ¨æ–‡çŒ®ä¸­æ‰¾åˆ°çš„ Architecture å’Œè®­ç»ƒæ•°æ®ï¼Œæ˜¯åå‘åˆ†ç±»å™¨ã€‚ ç†è§£è¿™ä¸ªé•œåƒç³»ç»Ÿçš„é€‰æ‹©å‡½æ•°æ˜¯æŒæ¡è¿™ä¸ªå¤§é‡å¼º gravitational lensç³»ç»Ÿçš„æ½œåœ¨åŠ›é‡çš„å…³é”®ã€‚ æˆ‘ä»¬ä½¿ç”¨äº†ä¸‰ä¸ªè®­ç»ƒæ•°æ®é›†ï¼Œä»£è¡¨äº†é€šå¸¸ç”¨äºè®­ç»ƒ galaxy-galaxy å’Œ galaxy-quasar é•œåƒç³»ç»Ÿçš„è®­ç»ƒæ•°æ®ã€‚ è¿™äº›ç½‘ç»œåå¥½ Systems with larger Einstein radii å’Œæ›´é›†ä¸­çš„æºå…‰è¾‰åˆ†å¸ƒã€‚ å°†æ£€æµ‹å…³é”®å€¼ä» 8Ïƒ æé«˜åˆ° 12Ïƒ ä¼šå¯¼è‡´50%é€‰æ‹©çš„å¼ºé•œç³»ç»Ÿæœ‰ Einstein radii Î¸E å¤§äºæˆ–ç­‰äº 1.04å¼§åº¦ï¼Œsource radii RS å¤§äºæˆ–ç­‰äº 0.194å¼§åº¦ï¼Œsource SÃ©rsic indices nScS å¤§äºæˆ–ç­‰äº 2.62ã€‚ å¯¹äºæ‰¾å¯»ç±»åˆ«çš„æ¨¡å‹ï¼Œå®ƒå…·æœ‰æ›´å¼ºçš„åå¥½ towards higher lens ellipticities than those trained to find lensed galaxiesã€‚ é€‰æ‹©å‡½æ•°ä¸å—Source çš„æ¢¯åº¦å½±å“ï¼Œå› æ­¤Measurements of this quantity will be unaffectedã€‚ é•œåƒé€‰æ‹©å‡½æ•°ä¸é•œåƒæˆªé¢çš„é€‰æ‹©å‡½æ•°ç›¸ä¼¼ï¼Œå› æ­¤æˆ‘ä»¬é¢„æœŸæˆ‘ä»¬çš„å‘ç°å°†æ˜¯ä¸€ä¸ªé€šç”¨çš„ç»“æœï¼Œé€‚ç”¨äºæ‰€æœ‰ galaxy-galaxy å’Œ galaxy-quasar é•œåƒç³»ç»Ÿã€‚
</details></li>
</ul>
<hr>
<h2 id="LightPath-Lightweight-and-Scalable-Path-Representation-Learning"><a href="#LightPath-Lightweight-and-Scalable-Path-Representation-Learning" class="headerlink" title="LightPath: Lightweight and Scalable Path Representation Learning"></a>LightPath: Lightweight and Scalable Path Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10171">http://arxiv.org/abs/2307.10171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sean Bin Yang, Jilin Hu, Chenjuan Guo, Bin Yang, Christian S. Jensen</li>
<li>for: æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§å’Œå¯æ‰©å±•çš„è·¯å¾„è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œç”¨äºæ™ºèƒ½äº¤é€šå’Œæ™ºèƒ½åŸå¸‚åº”ç”¨ã€‚</li>
<li>methods: æè®®ä½¿ç”¨ç¬›å¡å°”ç¯å¢ƒæŠ½è±¡å’Œå…¨å±€æœ¬åœ°çŸ¥è¯†ä¼ æ’­æ¥å‡å°‘èµ„æºæ¶ˆè€—å’Œæé«˜å¯æ‰©å±•æ€§ï¼ŒåŒæ—¶ä¿æŒå‡†ç¡®æ€§ã€‚</li>
<li>results: ç»è¿‡å¹¿æ³›çš„å®éªŒ validate äº†è¯¥æ¡†æ¶çš„å¯æ‰©å±•æ€§å’Œç²¾åº¦ï¼Œå¹¶ä¸”åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­å…·æœ‰ä¼˜åŠ¿ã€‚<details>
<summary>Abstract</summary>
Movement paths are used widely in intelligent transportation and smart city applications. To serve such applications, path representation learning aims to provide compact representations of paths that enable efficient and accurate operations when used for different downstream tasks such as path ranking and travel cost estimation. In many cases, it is attractive that the path representation learning is lightweight and scalable; in resource-limited environments and under green computing limitations, it is essential. Yet, existing path representation learning studies focus on accuracy and pay at most secondary attention to resource consumption and scalability.   We propose a lightweight and scalable path representation learning framework, termed LightPath, that aims to reduce resource consumption and achieve scalability without affecting accuracy, thus enabling broader applicability. More specifically, we first propose a sparse auto-encoder that ensures that the framework achieves good scalability with respect to path length. Next, we propose a relational reasoning framework to enable faster training of more robust sparse path encoders. We also propose global-local knowledge distillation to further reduce the size and improve the performance of sparse path encoders. Finally, we report extensive experiments on two real-world datasets to offer insight into the efficiency, scalability, and effectiveness of the proposed framework.
</details>
<details>
<summary>æ‘˜è¦</summary>
è·¯å¾„è¡¨ç¤ºæ³•å¹¿æ³›åº”ç”¨äºæ™ºèƒ½äº¤é€šå’Œæ™ºèƒ½åŸå¸‚åº”ç”¨ç¨‹åºä¸­ã€‚ä¸ºäº†æ»¡è¶³è¿™äº›åº”ç”¨ç¨‹åºï¼Œè·¯å¾„è¡¨ç¤ºå­¦ä¹ ç›®æ ‡æ˜¯æä¾›é«˜æ•ˆç²¾åº¦çš„è·¯å¾„è¡¨ç¤ºï¼Œä»¥ä¾¿åœ¨ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ä¸­è¿›è¡Œé«˜æ•ˆçš„æ“ä½œï¼Œå¦‚è·¯å¾„æ’åå’Œæ—…è¡Œè´¹ç”¨ä¼°ç®—ã€‚åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒå’Œç»¿è‰²è®¡ç®—é™åˆ¶ä¸‹ï¼Œç°æœ‰çš„è·¯å¾„è¡¨ç¤ºå­¦ä¹ ç ”ç©¶é€šå¸¸å¼ºè°ƒç²¾åº¦ï¼Œå¹¶ä¸”åªåœ¨å¿…è¦çš„æƒ…å†µä¸‹è¿›è¡Œæ¬¡è¦çš„è€ƒè™‘ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè½»é‡çº§å’Œå¯æ‰©å±•çš„è·¯å¾„è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œç§°ä¸ºLightPathï¼Œä»¥é™ä½èµ„æºæ¶ˆè€—å’Œå®ç°å¯æ‰©å±•æ€§ï¼Œè€Œä¸å½±å“å‡†ç¡®æ€§ã€‚æ›´ Specificallyï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ä¸ªç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼Œä»¥ç¡®ä¿æ¡†æ¶åœ¨è·¯å¾„é•¿åº¦æ–¹é¢å…·æœ‰è‰¯å¥½çš„æ‰©å±•æ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…³ç³»ç†è§£æ¡†æ¶ï¼Œä»¥æ›´å¿«åœ°è®­ç»ƒæ›´åŠ ç¨€ç–çš„è·¯å¾„ç¼–ç å™¨ã€‚ finallyï¼Œæˆ‘ä»¬æå‡ºäº†å…¨çƒ-æœ¬åœ°çŸ¥è¯†ä¼ æ’­ï¼Œä»¥è¿›ä¸€æ­¥å‡å°è·¯å¾„ç¼–ç å™¨çš„å¤§å°å’Œæé«˜å…¶æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œä»¥æä¾›æœ‰å…³æ•ˆç‡ã€å¯æ‰©å±•æ€§å’Œæ•ˆæœçš„æ·±å…¥äº†è§£ã€‚
</details></li>
</ul>
<hr>
<h2 id="Challenges-and-Applications-of-Large-Language-Models"><a href="#Challenges-and-Applications-of-Large-Language-Models" class="headerlink" title="Challenges and Applications of Large Language Models"></a>Challenges and Applications of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10169">http://arxiv.org/abs/2307.10169</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, Robert McHardy</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨ä¸ºæœºå™¨å­¦ä¹ ç ”ç©¶äººå‘˜æä¾›ä¸€ä¸ªç³»ç»Ÿçš„å¼€æ”¾é—®é¢˜å’ŒæˆåŠŸåº”ç”¨é¢†åŸŸï¼Œä»¥ä¾¿æ›´å¿«åœ°äº†è§£å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¢†åŸŸçš„å½“å‰çŠ¶æ€ï¼Œå¹¶æ›´å¿«åœ°æˆä¸ºäº§ĞºÑ‚Ğ¸Ğ²çš„ç ”ç©¶äººå‘˜ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†ç³»ç»Ÿçš„Literature Reviewå’Œé—®é¢˜å®šä¹‰æ–¹æ³•ï¼Œä»¥æŒæ¡å¤§è¯­è¨€æ¨¡å‹é¢†åŸŸçš„å½“å‰çŠ¶æ€å’Œæœªè§£å†³é—®é¢˜ã€‚</li>
<li>results: æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç³»åˆ—çš„å¼€æ”¾é—®é¢˜å’ŒæˆåŠŸåº”ç”¨é¢†åŸŸï¼Œä»¥ä¾¿ ML ç ”ç©¶äººå‘˜æ›´å¿«åœ°äº†è§£å¤§è¯­è¨€æ¨¡å‹é¢†åŸŸçš„å½“å‰çŠ¶æ€ï¼Œå¹¶æ›´å¿«åœ°æˆä¸ºäº§ĞºÑ‚Ğ¸Ğ²çš„ç ”ç©¶äººå‘˜ã€‚<details>
<summary>Abstract</summary>
Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learning discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to establish a systematic set of open problems and application successes so that ML researchers can comprehend the field's current state more quickly and become productive.
</details>
<details>
<summary>æ‘˜è¦</summary>
åºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»æ— å­˜åˆ°æ™®éçš„æœºå™¨å­¦ä¹ è®®é¢˜ä¸­çš„å‡ å¹´å†…ã€‚ç”±äºè¿™ä¸ªé¢†åŸŸçš„å¿«é€Ÿè¿›æ­¥ï¼Œå› æ­¤éš¾ä»¥è¯†åˆ«è¿˜æ²¡æœ‰è§£å†³çš„æŒ‘æˆ˜å’Œå·²ç»æœ‰æˆæœçš„åº”ç”¨é¢†åŸŸã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ hoping to establish a systematic set of open problems and application successesï¼Œä»¥ä¾¿MLç ”ç©¶äººå‘˜æ›´å¿«åœ°äº†è§£è¿™ä¸ªé¢†åŸŸçš„ç°çŠ¶ï¼Œæ›´å¿«åœ°æˆä¸ºç”Ÿäº§åŠ›ã€‚Note that Simplified Chinese is used in mainland China, while Traditional Chinese is used in Taiwan and other countries.
</details></li>
</ul>
<hr>
<h2 id="VITS-Variational-Inference-Thomson-Sampling-for-contextual-bandits"><a href="#VITS-Variational-Inference-Thomson-Sampling-for-contextual-bandits" class="headerlink" title="VITS : Variational Inference Thomson Sampling for contextual bandits"></a>VITS : Variational Inference Thomson Sampling for contextual bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10167">http://arxiv.org/abs/2307.10167</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Clavier, Tom Huix, Alain Durmus</li>
<li>for: è¿™ paper æ˜¯å…³äº contextual bandits çš„ä¸€ç§å˜ä½“ Thompson samplingï¼ˆTSï¼‰ç®—æ³•çš„ç ”ç©¶ã€‚</li>
<li>methods: è¯¥ç®—æ³•ä½¿ç”¨ Gaussian Variational Inference æä¾›é«˜æ•ˆçš„ posterior è¿‘ä¼¼ï¼Œå¹¶ä¸”å¯ä»¥è½»æ¾åœ°ä»è¿‘ä¼¼ä¸­é‡‡æ ·ã€‚</li>
<li>results: è¯¥paper è¡¨æ˜ VITS ç®—æ³•å¯ä»¥å®ç° sub-linear regret boundï¼Œå¹¶ä¸”åœ¨ synthetic å’Œå®é™…ä¸–ç•Œæ•°æ®ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ã€‚Hereâ€™s the breakdown of each point in English:* For: The paper is about a variant of the Thompson sampling algorithm for contextual bandits.* Methods: The algorithm uses Gaussian Variational Inference to provide efficient posterior approximations, and can easily sample from these approximations.* Results: The paper shows that the VITS algorithm can achieve a sub-linear regret bound, and demonstrates its effectiveness through experiments on both synthetic and real-world datasets.<details>
<summary>Abstract</summary>
In this paper, we introduce and analyze a variant of the Thompson sampling (TS) algorithm for contextual bandits. At each round, traditional TS requires samples from the current posterior distribution, which is usually intractable. To circumvent this issue, approximate inference techniques can be used and provide samples with distribution close to the posteriors. However, current approximate techniques yield to either poor estimation (Laplace approximation) or can be computationally expensive (MCMC methods, Ensemble sampling...). In this paper, we propose a new algorithm, Varational Inference Thompson sampling VITS, based on Gaussian Variational Inference. This scheme provides powerful posterior approximations which are easy to sample from, and is computationally efficient, making it an ideal choice for TS. In addition, we show that VITS achieves a sub-linear regret bound of the same order in the dimension and number of round as traditional TS for linear contextual bandit. Finally, we demonstrate experimentally the effectiveness of VITS on both synthetic and real world datasets.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»å¹¶åˆ†æäº†ä¸€ç§ Contextual Bandit ä¸­çš„ Thompson æŠ½æ ·ï¼ˆTSï¼‰ç®—æ³•çš„å˜ä½“ã€‚åœ¨æ¯ä¸ªè½®æ¬¡ä¸­ï¼Œä¼ ç»Ÿçš„ TS éœ€è¦ä»å½“å‰çš„ posterior åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œé€šå¸¸æ˜¯ä¸å¯è¡Œçš„ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Approximate Inference æŠ€æœ¯ï¼Œæä¾›é è¿‘ posterior çš„æ ·æœ¬ã€‚ç„¶è€Œï¼Œå½“å‰çš„ approximate æŠ€æœ¯å¯èƒ½ä¼šå¯¼è‡´ä½æ•ˆçš„ä¼°è®¡ï¼ˆLaplace åº”ç”¨ï¼‰æˆ–è€…æ˜¯ computationally expensiveï¼ˆMCMC æ–¹æ³•ã€Ensemble æŠ½æ ·...ï¼‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„ç®—æ³•ï¼ŒåŸºäº Gaussian Variational Inference çš„ Varitional Inference Thompson Samplingï¼ˆVITSï¼‰ã€‚è¿™ç§æ–¹æ¡ˆæä¾›äº†ç®€å•æ˜“äºé‡‡æ ·çš„å¼º posterior  aproximationï¼Œè®¡ç®—æ•ˆç‡é«˜ï¼Œé€‚ç”¨äº TSã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº† VITS åœ¨ç»´åº¦å’Œè½®æ¬¡æ•°æ–¹é¢çš„ä¸‹é™ regret bound ä¸ä¼ ç»Ÿçš„ TS ç›¸åŒã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡ synthetic å’Œå®é™…ä¸–ç•Œæ•°æ®çš„å®éªŒè¯æ˜äº† VITS çš„å®é™…æ•ˆæœã€‚
</details></li>
</ul>
<hr>
<h2 id="Improving-Multimodal-Datasets-with-Image-Captioning"><a href="#Improving-Multimodal-Datasets-with-Image-Captioning" class="headerlink" title="Improving Multimodal Datasets with Image Captioning"></a>Improving Multimodal Datasets with Image Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10350">http://arxiv.org/abs/2307.10350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thao Nguyen, Samir Yitzhak Gadre, Gabriel Ilharco, Sewoong Oh, Ludwig Schmidt</li>
<li>for: æé«˜å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„æˆåŠŸï¼Œå¦‚CLIPå’ŒFlamingoã€‚</li>
<li>methods: ç ”ç©¶å¦‚ä½•ä½¿ç”¨ç”Ÿæˆçš„æ ‡é¢˜æé«˜webæ•°æ®çš„Utilityï¼Œå¹¶æ¯”è¾ƒä¸åŒæ··åˆç­–ç•¥çš„æ€§èƒ½ã€‚</li>
<li>results: ä¸DataComp benchmarkä¸­æå‡ºçš„æœ€ä½³ç­–ç•¥ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ImageNetå’Œ38ä¸ªä»»åŠ¡ä¸­æé«˜äº†2%å’Œ4%çš„æ€§èƒ½ï¼Œå¹¶åœ¨Flickrå’ŒMS-COCOæ£€ç´¢ä¸­è¡¨ç°äº†2å€çš„æå‡ã€‚æˆ‘ä»¬è¿˜åˆ†æäº†ç”Ÿæˆæ ‡é¢˜çš„æ•ˆæœï¼Œå¹¶è¯æ˜æ ‡å‡†å›¾åƒæè¿°æ ‡å‡†ä¸æ˜¯å¤šModalè®­ç»ƒä¸­æ ‡é¢˜çš„å¯é æŒ‡æ ‡ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨å¤§è§„æ¨¡çš„DataCompä¸­è¿›è¡Œäº†å®éªŒï¼Œæ¢è®¨ç”Ÿæˆæ ‡é¢˜åœ¨å¤§é‡è®­ç»ƒæ•°æ®é‡ä¸‹çš„å±€é™æ€§ï¼Œä»¥åŠå›¾åƒæ·˜æ±°çš„é‡è¦æ€§ã€‚<details>
<summary>Abstract</summary>
Massive web datasets play a key role in the success of large vision-language models like CLIP and Flamingo. However, the raw web data is noisy, and existing filtering methods to reduce noise often come at the expense of data diversity. Our work focuses on caption quality as one major source of noise, and studies how generated captions can increase the utility of web-scraped datapoints with nondescript text. Through exploring different mixing strategies for raw and generated captions, we outperform the best filtering method proposed by the DataComp benchmark by 2% on ImageNet and 4% on average across 38 tasks, given a candidate pool of 128M image-text pairs. Our best approach is also 2x better at Flickr and MS-COCO retrieval. We then analyze what makes synthetic captions an effective source of text supervision. In experimenting with different image captioning models, we also demonstrate that the performance of a model on standard image captioning benchmarks (e.g., NoCaps CIDEr) is not a reliable indicator of the utility of the captions it generates for multimodal training. Finally, our experiments with using generated captions at DataComp's large scale (1.28B image-text pairs) offer insights into the limitations of synthetic text, as well as the importance of image curation with increasing training data quantity.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤§é‡ç½‘ç»œæ•°æ®å¯¹äºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹å¦‚CLIPå’ŒFlamingoçš„æˆåŠŸèµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚ç„¶è€Œï¼ŒåŸå§‹ç½‘ç»œæ•°æ®å…·æœ‰å™ªéŸ³ï¼Œç°æœ‰çš„è¿‡æ»¤æ–¹æ³•é€šå¸¸ä¼šå¯¼è‡´æ•°æ®å¤šæ ·æ€§å‡å°‘ã€‚æˆ‘ä»¬çš„å·¥ä½œå°†æ³¨æ„åŠ›ç‚¹åœ¨captionè´¨é‡ä¸Šï¼Œç ”ç©¶å¦‚ä½•ä½¿ç”¨ç”Ÿæˆçš„captionæé«˜ç½‘ç»œæŠ“å–åˆ°çš„æ–‡æœ¬ç‚¹çš„ä½¿ç”¨ä»·å€¼ã€‚é€šè¿‡ä¸åŒçš„æ··åˆç­–ç•¥æ¥èåˆåŸå§‹å’Œç”Ÿæˆçš„captionï¼Œæˆ‘ä»¬åœ¨ImageNetå’Œ38ä¸ªä»»åŠ¡ä¸Šæ¯”DataCompbenchmarkä¸­çš„æœ€ä½³è¿‡æ»¤æ–¹æ³•æé«˜2%å’Œ4%ã€‚æˆ‘ä»¬çš„æœ€ä½³æ–¹æ³•è¿˜åœ¨Flickrå’ŒMS-COCOæ£€ç´¢ä¸­è¡¨ç°å‡º2å€çš„å¥½å¹²å‡€æ€§ã€‚æˆ‘ä»¬è¿˜åˆ†æäº†ç”Ÿæˆcaptionçš„æœ‰æ•ˆæ€§æºæ³‰ï¼Œå¹¶é€šè¿‡ä¸åŒçš„å›¾åƒæè¿°æ¨¡å‹çš„å®éªŒï¼Œå‘ç°æ ‡å‡†å›¾åƒæè¿°benchmarkï¼ˆå¦‚NoCaps CIDErï¼‰ä¸­æ¨¡å‹çš„æ€§èƒ½ä¸æ˜¯è®­ç»ƒå¤šæ¨¡å¼æ—¶captionçš„ç”¨é€”çš„å¯é æŒ‡æ ‡ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨DataCompçš„å¤§è§„æ¨¡æ•°æ®ï¼ˆ1.28B image-text pairï¼‰ä¸Šè¿›è¡Œå®éªŒï¼Œæä¾›äº†ç”Ÿæˆcaptionçš„å±€é™æ€§ä»¥åŠå›¾åƒç­›é€‰çš„é‡è¦æ€§ï¼Œéšç€è®­ç»ƒæ•°æ®é‡çš„å¢åŠ ã€‚
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Backdoor-Attacks"><a href="#Rethinking-Backdoor-Attacks" class="headerlink" title="Rethinking Backdoor Attacks"></a>Rethinking Backdoor Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10163">http://arxiv.org/abs/2307.10163</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lancopku/SOS">https://github.com/lancopku/SOS</a></li>
<li>paper_authors: Alaa Khaddaj, Guillaume Leclerc, Aleksandar Makelov, Kristian Georgiev, Hadi Salman, Andrew Ilyas, Aleksander Madry</li>
<li>for: æœ¬æ–‡æ—¨åœ¨æ¢è®¨Backdooræ”»å‡»çš„é—®é¢˜ï¼Œå³æ•Œå¯¹è€…åœ¨è®­ç»ƒé›†ä¸­æ’å…¥æ¶æ„æ„å»ºçš„ä¾‹å­ï¼Œä»¥è®©æ¨¡å‹æ˜“å—æ¬ºè¯ˆã€‚</li>
<li>methods: æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥æŠ—å‡»Backdooræ”»å‡»ï¼Œå³é€šè¿‡æ— ç»“æ„ä¿¡æ¯å¯¹è®­ç»ƒæ•°æ®åˆ†å¸ƒè¿›è¡Œæ£€æµ‹ï¼Œå¹¶ä½¿ç”¨Robustç»Ÿè®¡æŠ€æœ¯æ¥æ£€æµ‹å’Œç§»é™¤æ¶æ„æ„å»ºçš„ä¾‹å­ã€‚</li>
<li>results: æœ¬æ–‡çš„ç»“æœè¡¨æ˜ï¼Œåœ¨ç¼ºä¹ç»“æ„ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼ŒBackdooræ”»å‡»æ˜¯ä¸å¯è¯†åˆ«çš„ï¼Œè€Œä¸”ä¸è‡ªç„¶å‡ºç°çš„ç‰¹å¾ç›¸åŒã€‚åŸºäºæ­¤è§‚å¯Ÿï¼Œæœ¬æ–‡æ£€è§†äº†ç°æœ‰çš„Backdooræ”»å‡»é˜²å¾¡æ–¹æ³•ï¼Œå¹¶æè¿°äº†å®ƒä»¬çš„å‡è®¾å’Œä¾èµ–å…³ç³»ã€‚æœ€åï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å‡è®¾ï¼Œå³Backdooræ”»å‡»å¯¹åº”äºè®­ç»ƒæ•°æ®ä¸­æœ€å¼ºçš„ç‰¹å¾ã€‚åŸºäºè¿™ä¸ªå‡è®¾ï¼Œæœ¬æ–‡å¼€å‘äº†ä¸€ç§æ–°çš„æ£€æµ‹ç®—æ³•ï¼Œå…·æœ‰ç†è®ºä¿è¯å’Œå®é™…æ•ˆæœã€‚<details>
<summary>Abstract</summary>
In a backdoor attack, an adversary inserts maliciously constructed backdoor examples into a training set to make the resulting model vulnerable to manipulation. Defending against such attacks typically involves viewing these inserted examples as outliers in the training set and using techniques from robust statistics to detect and remove them.   In this work, we present a different approach to the backdoor attack problem. Specifically, we show that without structural information about the training data distribution, backdoor attacks are indistinguishable from naturally-occurring features in the data--and thus impossible to "detect" in a general sense. Then, guided by this observation, we revisit existing defenses against backdoor attacks and characterize the (often latent) assumptions they make and on which they depend. Finally, we explore an alternative perspective on backdoor attacks: one that assumes these attacks correspond to the strongest feature in the training data. Under this assumption (which we make formal) we develop a new primitive for detecting backdoor attacks. Our primitive naturally gives rise to a detection algorithm that comes with theoretical guarantees and is effective in practice.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨ä¸€ä¸ªåé—¨æ”»å‡»ä¸­ï¼Œæ•Œå¯¹è€…ä¼šæ’å…¥ä¸€äº›æ¶æ„æ„å»ºçš„åé—¨ç¤ºä¾‹ï¼Œä»¥è®©æ¨¡å‹æ˜“äºæ“çºµã€‚é˜²å¾¡è¿™ç±»æ”»å‡»é€šå¸¸åŒ…æ‹¬è§†è¿™äº›æ’å…¥çš„ç¤ºä¾‹ä¸ºè®­ç»ƒé›†ä¸­çš„å¼‚å¸¸å€¼ï¼Œå¹¶ä½¿ç”¨robustç»Ÿè®¡æŠ€æœ¯æ¥æ¢æµ‹å’Œé™¤æ‰å®ƒä»¬ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸åŒçš„åé—¨æ”»å‡»é—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è¡¨æ˜äº†åœ¨è®­ç»ƒæ•°æ®åˆ†å¸ƒçš„ç»“æ„ä¿¡æ¯ä¸å­˜åœ¨çš„æƒ…å†µä¸‹ï¼Œåé—¨æ”»å‡»æ˜¯æ— æ³•åˆ†è¾¨çš„ï¼Œå› æ­¤ä¸èƒ½åœ¨é€šç”¨çš„æ¦‚å¿µä¸Šæ¢æµ‹ã€‚ç„¶åï¼Œæˆ‘ä»¬æ ¹æ®è¿™ä¸€è§‚å¯Ÿï¼Œé‡æ–°è¯„ä¼°äº†ç°æœ‰çš„åé—¨æ”»å‡»é˜²å¾¡æ–¹æ³•ï¼Œæè¿°äº†å®ƒä»¬æ‰€å‡è®¾çš„ï¼ˆå¸¸å¸¸éšè—çš„ï¼‰å‡è®¾å’Œä¾èµ–é¡¹ã€‚æœ€åï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸€ç§å‡è®¾åé—¨æ”»å‡»å¯¹åº”äºè®­ç»ƒæ•°æ®ä¸­æœ€å¼ºçš„ç‰¹å¾ï¼Œå¹¶å°†è¿™ç§å‡è®¾è¿›è¡Œäº†æ­£å¼è¡¨è¿°ã€‚æˆ‘ä»¬çš„åŸåˆ™ Naturally gives rise to a detection algorithm that comes with theoretical guarantees and is effective in practice.
</details></li>
</ul>
<hr>
<h2 id="Robust-Driving-Policy-Learning-with-Guided-Meta-Reinforcement-Learning"><a href="#Robust-Driving-Policy-Learning-with-Guided-Meta-Reinforcement-Learning" class="headerlink" title="Robust Driving Policy Learning with Guided Meta Reinforcement Learning"></a>Robust Driving Policy Learning with Guided Meta Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10160">http://arxiv.org/abs/2307.10160</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kanghoon Lee, Jiachen Li, David Isele, Jinkyoo Park, Kikuo Fujimura, Mykel J. Kochenderfer</li>
<li>for: å¢å¼ºè‡ªåŠ¨é©¾é©¶è½¦è¾†åœ¨äº’åŠ¨äº¤é€šåœºæ™¯ä¸­çš„è‡ªé€‚åº”èƒ½åŠ›</li>
<li>methods: ä½¿ç”¨éšæœºäº’åŠ¨å¥–åŠ±å‡½æ•°ç”Ÿæˆå¤šç§ç›®æ ‡ï¼Œå¹¶é€šè¿‡å¼•å¯¼æ”¿ç­–å®ç°è¿™äº›ç›®æ ‡ï¼Œä»¥ trains å¤šç§é©¾é©¶ç­–ç•¥</li>
<li>results: åœ¨ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ T-è·¯å£åœºæ™¯ä¸­ï¼ŒæˆåŠŸå¯åŠ¨äº†ä¸€ä¸ªé€‚åº”æ€§å¼ºçš„é©¾é©¶ç­–ç•¥ï¼Œå¹¶ä¸”èƒ½å¤Ÿåœ¨æœªç»è§è¿‡çš„ç¤¾äº¤è½¦è¾†è¡Œä¸ºä¸‹generalizationHereâ€™s the same information in a more detailed format:</li>
<li>for: The paper aims to improve the adaptability of autonomous vehicles in interactive traffic scenarios by training a single meta-policy to handle diverse social vehicle behaviors.</li>
<li>methods: The proposed method uses randomized interaction-based reward functions to generate diverse objectives and train the meta-policy through guiding policies that achieve specific objectives. The ego vehicleâ€™s driving policy is trained to be robust to unseen situations with out-of-distribution (OOD) social agentsâ€™ behaviors.</li>
<li>results: The proposed method is tested in a challenging uncontrolled T-intersection scenario, where the ego vehicleâ€™s driving policy is able to generalize well to unseen situations with OOD social agentsâ€™ behaviors.I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
Although deep reinforcement learning (DRL) has shown promising results for autonomous navigation in interactive traffic scenarios, existing work typically adopts a fixed behavior policy to control social vehicles in the training environment. This may cause the learned driving policy to overfit the environment, making it difficult to interact well with vehicles with different, unseen behaviors. In this work, we introduce an efficient method to train diverse driving policies for social vehicles as a single meta-policy. By randomizing the interaction-based reward functions of social vehicles, we can generate diverse objectives and efficiently train the meta-policy through guiding policies that achieve specific objectives. We further propose a training strategy to enhance the robustness of the ego vehicle's driving policy using the environment where social vehicles are controlled by the learned meta-policy. Our method successfully learns an ego driving policy that generalizes well to unseen situations with out-of-distribution (OOD) social agents' behaviors in a challenging uncontrolled T-intersection scenario.
</details>
<details>
<summary>æ‘˜è¦</summary>
(Simplified Chinese)å°½ç®¡æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰åœ¨äº¤äº’å¼äº¤é€šåœºæ™¯ä¸­å±•ç°å‡ºäº†æ‰å®çš„ç»“æœï¼Œç°æœ‰çš„å·¥ä½œé€šå¸¸é‡‡ç”¨å›ºå®šè¡Œä¸ºç­–ç•¥æ¥æ§åˆ¶ç¤¾äº¤è½¦è¾†åœ¨è®­ç»ƒç¯å¢ƒä¸­ã€‚è¿™å¯èƒ½ä¼šä½¿å­¦ä¹ çš„é©¾é©¶ç­–ç•¥è¿‡æ‹Ÿåˆç¯å¢ƒï¼Œä½¿å…¶ä¸ä¸åŒã€æœªè§çš„è¡Œä¸ºçš„ç¤¾äº¤è½¦è¾†å›°éš¾äº¤æµã€‚åœ¨è¿™ç§å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ–¹æ³•ï¼Œç”¨äºè®­ç»ƒç¤¾äº¤è½¦è¾†çš„å¤šæ ·åŒ–é©¾é©¶ç­–ç•¥ã€‚é€šè¿‡éšæœº modify social vehicleçš„äº’åŠ¨åŸºäºå¥–åŠ±å‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥ç”Ÿæˆå¤šæ ·çš„ç›®æ ‡ï¼Œå¹¶é«˜æ•ˆåœ°é€šè¿‡æŒ‡å¯¼ç­–ç•¥æ¥è®­ç»ƒå•ä¸€çš„å…ƒç­–ç•¥ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å¢å¼ºegoè½¦é©¾é©¶ç­–ç•¥çš„è®­ç»ƒç­–ç•¥ï¼Œä½¿ç”¨å·²å­¦ä¹ çš„å…ƒç­–ç•¥æ§åˆ¶ç¤¾äº¤è½¦è¾†çš„ç¯å¢ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•æˆåŠŸåœ°å­¦ä¹ äº†ä¸€ä¸ª Egodeé©¾é©¶ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨æœªè§çš„ç¤¾äº¤è½¦è¾†è¡Œä¸ºæƒ…å†µä¸‹å¯ä»¥æ™®é€‚åœ°åº”ç”¨ã€‚
</details></li>
</ul>
<hr>
<h2 id="Curvature-based-Clustering-on-Graphs"><a href="#Curvature-based-Clustering-on-Graphs" class="headerlink" title="Curvature-based Clustering on Graphs"></a>Curvature-based Clustering on Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10155">http://arxiv.org/abs/2307.10155</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/agosztolai/geometric_clustering">https://github.com/agosztolai/geometric_clustering</a></li>
<li>paper_authors: Yu Tian, Zachary Lubberts, Melanie Weber</li>
<li>for: æœ¬æ–‡ç ”ç©¶äº†ä¸€ç§åŸºäºå›¾å½¢å­¦çš„æ— ç›‘ç£èŠ‚ç‚¹åˆ’åˆ†ï¼ˆæˆ–ç¤¾åŒºæ£€æµ‹ï¼‰ç®—æ³•ï¼Œç”¨äºæ‰¾åˆ°å›¾ä¸­ç´§å¯†è¿æ¥çš„å­ç»“æ„ï¼Œå³ç¤¾åŒºæˆ–ç¾¤ä½“ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨äº†ç¦»æ•£ Ricci  curvature å’Œå…¶ç›¸å…³çš„ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹æµåŠ¨ï¼Œä»¥æ­ç¤ºå›¾ä¸­çš„ç¤¾åŒºç»“æ„ã€‚ å¹¶è€ƒè™‘äº†å¤šç§ç¦»æ•£ curvature è§‚å¿µï¼Œå¹¶å¯¹å…¶è¿›è¡Œåˆ†æã€‚</li>
<li>results: æœ¬æ–‡æä¾›äº†both theoretical å’Œ empirical è¯æ˜ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„ curvature-based åˆ’åˆ†ç®—æ³•çš„å®ç”¨æ€§ã€‚ æ­¤å¤–ï¼Œè¿˜æä¾›äº†ä¸€äº›å…³äºå›¾å½¢ curvature å’Œå…¶å¯¹å‰¯å›¾ curvature çš„å…³ç³»çš„ç»“æœï¼Œå¯èƒ½å¯¹ curvature-based ç½‘ç»œåˆ†ææœ‰ç‹¬ç«‹çš„ä»·å€¼ã€‚<details>
<summary>Abstract</summary>
Unsupervised node clustering (or community detection) is a classical graph learning task. In this paper, we study algorithms, which exploit the geometry of the graph to identify densely connected substructures, which form clusters or communities. Our method implements discrete Ricci curvatures and their associated geometric flows, under which the edge weights of the graph evolve to reveal its community structure. We consider several discrete curvature notions and analyze the utility of the resulting algorithms. In contrast to prior literature, we study not only single-membership community detection, where each node belongs to exactly one community, but also mixed-membership community detection, where communities may overlap. For the latter, we argue that it is beneficial to perform community detection on the line graph, i.e., the graph's dual. We provide both theoretical and empirical evidence for the utility of our curvature-based clustering algorithms. In addition, we give several results on the relationship between the curvature of a graph and that of its dual, which enable the efficient implementation of our proposed mixed-membership community detection approach and which may be of independent interest for curvature-based network analysis.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¸ç›‘ç£èŠ‚ç‚¹åˆ’åˆ†ï¼ˆæˆ–ç¤¾åŒºæ¢æµ‹ï¼‰æ˜¯ä¸€ä¸ªç»å…¸çš„å›¾å­¦ä»»åŠ¡ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åˆ©ç”¨å›¾å½¢çš„å‡ ä½•ç‰¹æ€§æ¥æ ‡è¯†ç´§å¯†è¿æ¥çš„å­ç»“æ„ï¼Œå®ƒä»¬ç»„æˆç¤¾åŒºæˆ–ç¤¾åŒºã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ç¦»æ•£ Ricci æ›²ç‡å’Œå…¶ç›¸å…³çš„å‡ ä½•æµåŠ¨ï¼Œä»¥ä¾¿åœ¨å›¾å½¢ä¸­æ­ç¤ºç¤¾åŒºç»“æ„ã€‚æˆ‘ä»¬è€ƒè™‘äº†å¤šç§ç¦»æ•£æ›²ç‡æ¦‚å¿µï¼Œå¹¶åˆ†æäº†å®ƒä»¬çš„ä½¿ç”¨ä»·å€¼ã€‚ä¸å…ˆå‰æ–‡çŒ®ä¸åŒï¼Œæˆ‘ä»¬ä¸ä»…ç ”ç©¶å•ä¼šå‘˜ç¤¾åŒºæ¢æµ‹ï¼Œæ¯ä¸ªèŠ‚ç‚¹éƒ½å±äºå”¯ä¸€ä¸€ä¸ªç¤¾åŒºï¼Œè¿˜ç ”ç©¶äº†æ··åˆä¼šå‘˜ç¤¾åŒºæ¢æµ‹ï¼Œç¤¾åŒºå¯èƒ½ overlapã€‚ä¸ºå®ç°åä¸€ç§æƒ…å†µï¼Œæˆ‘ä»¬æå‡ºåœ¨å¯¹Ğ³Ñ€Ğ°Ñ„çš„ dual è¿›è¡Œç¤¾åŒºæ¢æµ‹ï¼Œå³çº¿å›¾ã€‚æˆ‘ä»¬æä¾›äº†ç†è®ºå’Œå®éªŒè¯æ˜ï¼Œä»¥åŠå¯¹æƒé‡å›¾çš„ curvature çš„åˆ†æï¼Œè¿™äº›ç»“æœå¯èƒ½ä¸º curvature-based ç½‘ç»œåˆ†ææä¾›å¸®åŠ©ã€‚
</details></li>
</ul>
<hr>
<h2 id="Code-Detection-for-Hardware-Acceleration-Using-Large-Language-Models"><a href="#Code-Detection-for-Hardware-Acceleration-Using-Large-Language-Models" class="headerlink" title="Code Detection for Hardware Acceleration Using Large Language Models"></a>Code Detection for Hardware Acceleration Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10348">http://arxiv.org/abs/2307.10348</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Pablo Antonio MartÃ­nez, Gregorio BernabÃ©, JosÃ© Manuel GarcÃ­a</li>
<li>for: æœ¬ç ”ç©¶æ¢è®¨äº†ä½¿ç”¨å¤§å‹è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œä»£ç æ£€æµ‹ã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆæ­¥çš„æç¤ºç­–ç•¥å’Œä¸€ç§æ–°çš„æç¤ºç­–ç•¥æ¥å®ç°ä»£ç æ£€æµ‹ã€‚</li>
<li>results: ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–°æç¤ºç­–ç•¥å¯ä»¥å‡å°‘å‡é˜³æ€§ï¼Œå®ç°äº†ä¼˜ç§€çš„æ€»å‡†ç¡®ç‡ï¼ˆ91.1%, 97.9%, å’Œ 99.7%ï¼‰ã€‚è¿™äº›ç»“æœå¯¹ç°æœ‰çš„ä»£ç æ£€æµ‹æ–¹æ³•æå‡ºäº†æ˜æ˜¾çš„æŒ‘æˆ˜ã€‚<details>
<summary>Abstract</summary>
Large language models (LLMs) have been massively applied to many tasks, often surpassing state-of-the-art approaches. While their effectiveness in code generation has been extensively studied (e.g., AlphaCode), their potential for code detection remains unexplored.   This work presents the first analysis of code detection using LLMs. Our study examines essential kernels, including matrix multiplication, convolution, and fast-fourier transform, implemented in C/C++. We propose both a preliminary, naive prompt and a novel prompting strategy for code detection.   Results reveal that conventional prompting achieves great precision but poor accuracy (68.8%, 22.3%, and 79.2% for GEMM, convolution, and FFT, respectively) due to a high number of false positives. Our novel prompting strategy substantially reduces false positives, resulting in excellent overall accuracy (91.1%, 97.9%, and 99.7%, respectively). These results pose a considerable challenge to existing state-of-the-art code detection methods.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å¹¿æ³›åº”ç”¨äºå¤šç§ä»»åŠ¡ï¼Œç»å¸¸è¶…è¶Šç°æœ‰çš„æ–¹æ³•ã€‚è€Œå®ƒä»¬åœ¨ä»£ç ç”Ÿæˆä¸­çš„åº”ç”¨åˆ™æœªå¾—åˆ°å……åˆ†æ¢è®¨ã€‚ æœ¬ç ”ç©¶æ˜¯ä»£ç æ£€æµ‹ä½¿ç”¨ LLM çš„é¦–æ¬¡åˆ†æã€‚æˆ‘ä»¬çš„ç ”ç©¶æ¢è®¨äº†é‡è¦çš„æ ¸å¿ƒæ“ä½œï¼ŒåŒ…æ‹¬çŸ©é˜µä¹˜æ³•ã€å·ç§¯å’Œå¿«é€Ÿå‚…ç«‹å¶å˜æ¢ï¼Œå®ƒä»¬åœ¨ C/C++ ä¸­å®ç°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆæ­¥ã€ç®€å•çš„æç¤ºå’Œä¸€ä¸ªæ–°çš„æç¤ºç­–ç•¥æ¥è¿›è¡Œä»£ç æ£€æµ‹ã€‚ç»“æœæ˜¾ç¤ºï¼Œä¼ ç»Ÿçš„æç¤ºæ–¹æ³•å¯ä»¥å¾ˆå¥½åœ°ç²¾åº¦ï¼ˆ68.8%ã€22.3%å’Œ79.2%ï¼‰ï¼Œä½†æ˜¯å—åˆ°è®¸å¤šå‡é˜³æ€§çš„å½±å“ï¼Œå› æ­¤ç²¾åº¦ä½ã€‚æˆ‘ä»¬çš„æ–°æç¤ºç­–ç•¥å¯ä»¥å¹²æ‰°å‡é˜³æ€§ï¼Œå®ç°äº†ä¼˜ç§€çš„æ€»ç²¾åº¦ï¼ˆ91.1%ã€97.9%å’Œ99.7%ï¼‰ã€‚è¿™äº›ç»“æœå¯¹ç°æœ‰çš„ä»£ç æ£€æµ‹æ–¹æ³•æå‡ºäº†ä¸¥é‡çš„æŒ‘æˆ˜ã€‚
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Potential-Based-Rewards-for-Learning-Humanoid-Locomotion"><a href="#Benchmarking-Potential-Based-Rewards-for-Learning-Humanoid-Locomotion" class="headerlink" title="Benchmarking Potential Based Rewards for Learning Humanoid Locomotion"></a>Benchmarking Potential Based Rewards for Learning Humanoid Locomotion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10142">http://arxiv.org/abs/2307.10142</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/se-hwan/pbrs-humanoid">https://github.com/se-hwan/pbrs-humanoid</a></li>
<li>paper_authors: Se Hwan Jeon, Steve Heim, Charles Khazoom, Sangbae Kim</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨ benchmarking æ ‡å‡†çš„ reward shaping æ–¹æ³•å’Œ potential based reward shaping (PBRS) æ–¹æ³•ï¼Œä»¥åŠ é€Ÿ reinforcement learning (RL) çš„å­¦ä¹ é€Ÿåº¦ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº† humanoid robot è¿›è¡Œå®éªŒï¼Œå¹¶å¯¹ä¸¤ç§ reward shaping æ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œåœ¨é«˜ç»´ç³»ç»Ÿä¸­ï¼ŒPBRS çš„ä¼˜åŒ–æ•ˆæœåªæœ‰è¾ƒå°çš„æ”¹å–„ï¼Œä½† PBRS çš„è¯„ä»·æ ‡å‡†æ›´å®¹æ˜“è°ƒæ•´ã€‚<details>
<summary>Abstract</summary>
The main challenge in developing effective reinforcement learning (RL) pipelines is often the design and tuning the reward functions. Well-designed shaping reward can lead to significantly faster learning. Naively formulated rewards, however, can conflict with the desired behavior and result in overfitting or even erratic performance if not properly tuned. In theory, the broad class of potential based reward shaping (PBRS) can help guide the learning process without affecting the optimal policy. Although several studies have explored the use of potential based reward shaping to accelerate learning convergence, most have been limited to grid-worlds and low-dimensional systems, and RL in robotics has predominantly relied on standard forms of reward shaping. In this paper, we benchmark standard forms of shaping with PBRS for a humanoid robot. We find that in this high-dimensional system, PBRS has only marginal benefits in convergence speed. However, the PBRS reward terms are significantly more robust to scaling than typical reward shaping approaches, and thus easier to tune.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¸»è¦æŒ‘æˆ˜åœ¨å¼€å‘æœ‰æ•ˆçš„å¢å¼ºå­¦ä¹ ï¼ˆRLï¼‰ç®¡é“æ˜¯è®¾è®¡å’Œè°ƒæ•´å¥–é‡‘å‡½æ•°ã€‚Well-designed å½¢å¼çš„å¥–é‡‘å¯ä»¥å¯¼è‡´å­¦ä¹ é€Ÿåº¦æ˜æ˜¾æé«˜ã€‚ç„¶è€Œï¼ŒNaively å®šä¹‰çš„å¥–é‡‘å¯èƒ½ä¼šä¸æ„¿æœ›è¡Œä¸ºå†²çªï¼Œå¯¼è‡´è¿‡æ‹Ÿåˆæˆ–è€…even erratic performance  if not properly tunedã€‚ç†è®ºä¸Šï¼Œå¹¿æ³›çš„ potential based reward shapingï¼ˆPBRSï¼‰å¯ä»¥å¸®åŠ©å¯¼å¼•å­¦ä¹ è¿‡ç¨‹ï¼Œæ— éœ€affecting the optimal policyã€‚è™½ç„¶ä¸€äº›ç ”ç©¶å·²ç»æ¢è®¨äº†ä½¿ç”¨ potential based reward shaping åŠ é€Ÿå­¦ä¹ çš„æ•´åˆï¼Œä½†å¤§å¤šæ•°ç ”ç©¶ä»…é™äºæ ¼å­ä¸–ç•Œå’Œä½ç»´ç³»ç»Ÿï¼ŒRL åœ¨æœºå™¨äººé¢†åŸŸä¸»è¦ä¾é æ ‡å‡†çš„å¥–é‡‘å½¢å¼ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹æ ‡å‡†çš„å¥–é‡‘å½¢å¼å’Œ PBRS è¿›è¡Œäº†å¯¹æ¯”ï¼Œå‘ç°åœ¨è¿™ä¸ªé«˜ç»´ç³»ç»Ÿä¸­ï¼ŒPBRS åªæœ‰å¾®å¦™çš„åŠ é€Ÿå­¦ä¹ é€Ÿåº¦ã€‚ç„¶è€Œï¼ŒPBRS å¥–é‡‘é¡¹æ˜¯æ ‡å‡†å¥–é‡‘å½¢å¼ç›¸æ¯”è¾ƒæ›´åŠ Robust å°ºåº¦ï¼Œå› æ­¤æ›´å®¹æ˜“è°ƒæ•´ã€‚
</details></li>
</ul>
<hr>
<h2 id="ProtiGeno-a-prokaryotic-short-gene-finder-using-protein-language-models"><a href="#ProtiGeno-a-prokaryotic-short-gene-finder-using-protein-language-models" class="headerlink" title="ProtiGeno: a prokaryotic short gene finder using protein language models"></a>ProtiGeno: a prokaryotic short gene finder using protein language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10343">http://arxiv.org/abs/2307.10343</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tonytu16/protigeno">https://github.com/tonytu16/protigeno</a></li>
<li>paper_authors: Tony Tu, Gautham Krishna, Amirali Aghazadeh</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æé«˜è´¨å­ç”Ÿç‰©åŸºå› é¢„æµ‹çš„å‡†ç¡®æ€§å’Œ recallï¼Œå°¤å…¶æ˜¯åœ¨çŸ­è¯»å– Frame (ORFs) ä¸­ã€‚</li>
<li>methods: æˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ï¼Œç§°ä¸º ProtiGenoï¼Œä½¿ç”¨ä¸€ä¸ªè®­ç»ƒåœ¨æ•°ç™¾ä¸‡ä¸ªæ¼”åŒ–åçš„è›‹ç™½è´¨æ¨¡å‹æ¥é¢„æµ‹è´¨å­ç”Ÿç‰©çŸ­åŸºå› ã€‚</li>
<li>results: åœ¨ç³»ç»Ÿæ€§çš„å¤§è§„æ¨¡å®éªŒä¸­ï¼Œæˆ‘ä»¬ç¤ºå‡ºäº† ProtiGeno å¯ä»¥æ›´é«˜åº¦å‡†ç¡®åœ°é¢„æµ‹çŸ­è´¨å­ç”Ÿç‰©åŸºå› ï¼Œæ¯”ç°æœ‰çš„çŠ¶æ€è‰ºæœ¯åŸºå› é¢„æµ‹å™¨æ›´é«˜ã€‚æˆ‘ä»¬è¿˜è®²è¿°äº† ProtiGeno é¢„æµ‹çŸ­åŸºå› çš„é¢„æµ‹ç‰¹å¾å’Œå¯èƒ½çš„é™åˆ¶ã€‚<details>
<summary>Abstract</summary>
Prokaryotic gene prediction plays an important role in understanding the biology of organisms and their function with applications in medicine and biotechnology. Although the current gene finders are highly sensitive in finding long genes, their sensitivity decreases noticeably in finding shorter genes (<180 nts). The culprit is insufficient annotated gene data to identify distinguishing features in short open reading frames (ORFs). We develop a deep learning-based method called ProtiGeno, specifically targeting short prokaryotic genes using a protein language model trained on millions of evolved proteins. In systematic large-scale experiments on 4,288 prokaryotic genomes, we demonstrate that ProtiGeno predicts short coding and noncoding genes with higher accuracy and recall than the current state-of-the-art gene finders. We discuss the predictive features of ProtiGeno and possible limitations by visualizing the three-dimensional structure of the predicted short genes. Data, codes, and models are available at https://github.com/tonytu16/protigeno.
</details>
<details>
<summary>æ‘˜è¦</summary>
probiotic gene prediction plays an important role in understanding the biology of organisms and their function, with applications in medicine and biotechnology. Although the current gene finders are highly sensitive in finding long genes, their sensitivity decreases noticeably in finding shorter genes (<180 nts). The culprit is insufficient annotated gene data to identify distinguishing features in short open reading frames (ORFs). We develop a deep learning-based method called ProtiGeno, specifically targeting short prokaryotic genes using a protein language model trained on millions of evolved proteins. In systematic large-scale experiments on 4,288 prokaryotic genomes, we demonstrate that ProtiGeno predicts short coding and noncoding genes with higher accuracy and recall than the current state-of-the-art gene finders. We discuss the predictive features of ProtiGeno and possible limitations by visualizing the three-dimensional structure of the predicted short genes. Data, codes, and models are available at https://github.com/tonytu16/protigeno.Here's the word-for-word translation of the text into Simplified Chinese: probiotic gene prediction plays an important role in understanding the biology of organisms and their function, with applications in medicine and biotechnology. Although the current gene finders are highly sensitive in finding long genes, their sensitivity decreases noticeably in finding shorter genes (<180 nts). The culprit is insufficient annotated gene data to identify distinguishing features in short open reading frames (ORFs). We develop a deep learning-based method called ProtiGeno, specifically targeting short prokaryotic genes using a protein language model trained on millions of evolved proteins. In systematic large-scale experiments on 4,288 prokaryotic genomes, we demonstrate that ProtiGeno predicts short coding and noncoding genes with higher accuracy and recall than the current state-of-the-art gene finders. We discuss the predictive features of ProtiGeno and possible limitations by visualizing the three-dimensional structure of the predicted short genes. Data, codes, and models are available at https://github.com/tonytu16/protigeno.Please note that the translation is done using Google Translate, and the result may not be perfectly accurate or idiomatic.
</details></li>
</ul>
<hr>
<h2 id="Gradient-Sparsification-For-Masked-Fine-Tuning-of-Transformers"><a href="#Gradient-Sparsification-For-Masked-Fine-Tuning-of-Transformers" class="headerlink" title="Gradient Sparsification For Masked Fine-Tuning of Transformers"></a>Gradient Sparsification For Masked Fine-Tuning of Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10098">http://arxiv.org/abs/2307.10098</a></li>
<li>repo_url: None</li>
<li>paper_authors: James Oâ€™ Neill, Sourav Dutta</li>
<li>for: è¿™ paper æ˜¯ investigate å¦‚ä½•æé«˜ transfer learning ä¸­çš„ fine-tuning æ€§èƒ½ï¼Œå¹¶æå‡ºäº†ä¸€ç§ gradient sparsification æ–¹æ³• GradDropã€‚</li>
<li>methods: è¿™ paper ä½¿ç”¨äº† two ç§æ–¹æ³•æ¥ evaluate GradDropï¼Œä¸€ç§æ˜¯ä½¿ç”¨ multilingual XGLUE  bencmarkï¼Œå¦ä¸€ç§æ˜¯ä½¿ç”¨ XLMR-Large æ¨¡å‹ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼ŒGradDrop å¯ä»¥ä¸ä½¿ç”¨é¢å¤–çš„ç¿»è¯‘æ•°æ®è¿›è¡Œ intermediate pretraining ç›¸æ¯”ï¼Œå¹¶ä¸”è¶…è¿‡æ ‡å‡†çš„ fine-tuning å’Œæ…¢æ»‘å±‚è§£å†³æ–¹æ³•ã€‚å¦å¤–ï¼Œä¸€ç§ post-analysis è¿˜è¡¨æ˜ï¼ŒGradDrop å¯ä»¥æé«˜ under-resourced è¯­è¨€çš„æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
Fine-tuning pretrained self-supervised language models is widely adopted for transfer learning to downstream tasks. Fine-tuning can be achieved by freezing gradients of the pretrained network and only updating gradients of a newly added classification layer, or by performing gradient updates on all parameters. Gradual unfreezing makes a trade-off between the two by gradually unfreezing gradients of whole layers during training. This has been an effective strategy to trade-off between storage and training speed with generalization performance. However, it is not clear whether gradually unfreezing layers throughout training is optimal, compared to sparse variants of gradual unfreezing which may improve fine-tuning performance. In this paper, we propose to stochastically mask gradients to regularize pretrained language models for improving overall fine-tuned performance. We introduce GradDrop and variants thereof, a class of gradient sparsification methods that mask gradients during the backward pass, acting as gradient noise. GradDrop is sparse and stochastic unlike gradual freezing. Extensive experiments on the multilingual XGLUE benchmark with XLMR-Large show that GradDrop is competitive against methods that use additional translated data for intermediate pretraining and outperforms standard fine-tuning and gradual unfreezing. A post-analysis shows how GradDrop improves performance with languages it was not trained on, such as under-resourced languages.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¹¿æ³›é‡‡ç”¨å·²ç»é¢„è®­ç»ƒè‡ªä¸»è¯­è¨€æ¨¡å‹çš„å¾®è°ƒæ˜¯ä¸ºäº†è¿›è¡Œè½¬ç§»å­¦ä¹ åˆ°ä¸‹æ¸¸ä»»åŠ¡ã€‚å¾®è°ƒå¯ä»¥é€šè¿‡å†»ç»“é¢„è®­ç»ƒç½‘ç»œçš„æ¢¯åº¦æ¥å®ç°ï¼Œæˆ–è€…æ˜¯é€šè¿‡åœ¨æ–°å¢çš„åˆ†ç±»å±‚ä¸Šè¿›è¡Œæ¢¯åº¦æ›´æ–°ã€‚æ¸è¿›å†»ç»“å¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ¸è§£å†»æ•´ä¸ªå±‚çš„æ¢¯åº¦ï¼Œä»è€Œå®ç°å­˜å‚¨å’Œè®­ç»ƒé€Ÿåº¦ä¹‹é—´çš„å¹³è¡¡ã€‚ç„¶è€Œï¼Œä¸çŸ¥é“æ¸è¿›å†»ç»“layersåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¯æœ€ä½³çš„ï¼Œç›¸æ¯”ä¹‹ä¸‹ï¼Œ sparse variant of gradual unfreezingå¯èƒ½ä¼šæé«˜å¾®è°ƒæ€§èƒ½ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†éšæœºå±‚æ¢¯åº¦æ©ç æ¥è§„èŒƒé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œä»¥æé«˜æ€»ä½“å¾®è°ƒæ€§èƒ½ã€‚æˆ‘ä»¬å¼•å…¥GradDropå’Œå…¶å˜ç§ï¼Œå®ƒæ˜¯ä¸€ç±»æ¢¯åº¦å‡å°‘æ–¹æ³•ï¼Œåœ¨åå‘ä¼ æ’­ä¸­éšæœºæ©ç æ¢¯åº¦ã€‚GradDropæ˜¯ä¸åŒäºæ¸è¿›å†»ç»“çš„ï¼Œå®ƒæ˜¯éšæœºå’Œç²—ç•¥çš„ã€‚æˆ‘ä»¬åœ¨å¤šè¯­è¨€XGLUEæ ‡å‡†æµ‹è¯• benchmarkä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœæ˜¾ç¤ºGradDropå’Œå…¶å˜ç§ä¸ä½¿ç”¨é¢å¤–ç¿»è¯‘æ•°æ®è¿›è¡Œä¸­é—´é¢„è®­ç»ƒçš„æ–¹æ³•ç›¸å½“ç«äº‰ï¼Œå¹¶ä¸”è¶…è¿‡äº†æ ‡å‡†å¾®è°ƒå’Œæ¸è¿›å†»ç»“ã€‚ä¸€ç§åæœŸåˆ†æè¡¨æ˜ï¼ŒGradDropåœ¨æœªç»è®­ç»ƒçš„è¯­è¨€ä¸Šæé«˜æ€§èƒ½ï¼Œå¦‚å—åˆ°äº†èµ„æºçš„è¯­è¨€ã€‚
</details></li>
</ul>
<hr>
<h2 id="Revisiting-invariances-and-introducing-priors-in-Gromov-Wasserstein-distances"><a href="#Revisiting-invariances-and-introducing-priors-in-Gromov-Wasserstein-distances" class="headerlink" title="Revisiting invariances and introducing priors in Gromov-Wasserstein distances"></a>Revisiting invariances and introducing priors in Gromov-Wasserstein distances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10093">http://arxiv.org/abs/2307.10093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pinar Demetci, Quang Huy Tran, Ievgen Redko, Ritambhara Singh</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æå‡ºä¸€ç§æ–°çš„ä¼˜å…ˆTransport-åŸºäºè·ç¦»ï¼Œä»¥æé«˜å¯¹metric spacesä¸­æ ·æœ¬ä¹‹é—´çš„æ¯”è¾ƒï¼Œå¹¶ä¸”èƒ½å¤Ÿåœ¨certainåº”ç”¨ä¸­æ§åˆ¶å¯¹æ˜ å°„å˜æ¢çš„çµæ´»æ€§ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨augmented Gromov-Wasserstein distanceï¼Œè¯¥è·ç¦»è€ƒè™‘äº†æ ·æœ¬ä¹‹é—´çš„pairwise similarityï¼ŒåŒæ—¶è¿˜èƒ½å¤Ÿ incorporate feature alignmentsï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨è¾“å…¥æ•°æ®ä¸­çš„å…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>results: æœ¬ç ”ç©¶é€šè¿‡ theoretically analyzing the proposed metricï¼Œå¹¶åœ¨å•ç»†èƒå¤šOmic alignmentå’Œæœºå™¨å­¦ä¹ ä¸­è¿›è¡Œäº†å®éªŒ validate the effectiveness of the proposed methodã€‚<details>
<summary>Abstract</summary>
Gromov-Wasserstein distance has found many applications in machine learning due to its ability to compare measures across metric spaces and its invariance to isometric transformations. However, in certain applications, this invariance property can be too flexible, thus undesirable. Moreover, the Gromov-Wasserstein distance solely considers pairwise sample similarities in input datasets, disregarding the raw feature representations. We propose a new optimal transport-based distance, called Augmented Gromov-Wasserstein, that allows for some control over the level of rigidity to transformations. It also incorporates feature alignments, enabling us to better leverage prior knowledge on the input data for improved performance. We present theoretical insights into the proposed metric. We then demonstrate its usefulness for single-cell multi-omic alignment tasks and a transfer learning scenario in machine learning.
</details>
<details>
<summary>æ‘˜è¦</summary>
ã€ŠGromov-Wassersteinè·ç¦»ã€‹åœ¨æœºå™¨å­¦ä¹ ä¸­å‘ç°äº†å¹¿æ³›çš„åº”ç”¨ï¼Œä¸»è¦æ˜¯å› ä¸ºå®ƒå¯ä»¥æ¯”è¾ƒåº¦é‡ç©ºé—´ä¸­çš„åº¦é‡ï¼Œå¹¶ä¸”å¯¹äºåŒæ€å˜æ¢æ˜¯ä¸å˜çš„ã€‚ç„¶è€Œï¼Œåœ¨æŸäº›åº”ç”¨åœºæ™¯ä¸­ï¼Œè¿™ç§ä¸å˜æ€§å±æ€§å¯èƒ½æ˜¯ä¸éœ€è¦çš„ï¼Œç”šè‡³æ˜¯ä¸Ğ¶ĞµĞ»Ğ°çš„ã€‚æ­¤å¤–ï¼ŒGromov-Wassersteinè·ç¦»ä»…è€ƒè™‘è¾“å…¥æ•°æ®é›†ä¸­çš„å¯¹åº”å…³ç³»ï¼Œä¸è€ƒè™‘åŸå§‹ç‰¹å¾è¡¨ç¤ºã€‚æˆ‘ä»¬æè®®ä¸€ç§æ–°çš„ä¼˜åŒ–çš„Gromov-Wassersteinè·ç¦»ï¼Œcalled Augmented Gromov-Wassersteinï¼Œå…è®¸æ§åˆ¶å˜æ¢çš„çº§åˆ«ã€‚å®ƒè¿˜åŒ…å«ç‰¹å¾å¯¹é½ï¼Œä½¿å¾—æˆ‘ä»¬å¯ä»¥æ›´å¥½åœ°åˆ©ç”¨è¾“å…¥æ•°æ®ä¸­çš„å…ˆéªŒçŸ¥è¯†ï¼Œæé«˜æ€§èƒ½ã€‚æˆ‘ä»¬æä¾›äº†å…³äºæè®®åº¦é‡çš„ç†è®ºå¬è§ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨å•ç»†Ñ›Ğµå¤šå…ƒç´ Alignmentä»»åŠ¡å’Œæœºå™¨å­¦ä¹ ä¸­çš„ä¼ é€’å­¦ä¹ åœºæ™¯ä¸­å±•ç¤ºäº†å…¶ç”¨äºã€‚
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/20/cs.LG_2023_07_20/" data-id="clp88dbxl00oqob88ctj72wqp" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/20/cs.CL_2023_07_20/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.CL - 2023-07-20
        
      </div>
    </a>
  
  
    <a href="/2023/07/20/eess.IV_2023_07_20/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-07-20</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">66</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">81</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">140</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
