
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-07-29 123:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="MSStyleTTS: Multi-Scale Style Modeling with Hierarchical Context Information for Expressive Speech Synthesis paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.16012 repo_url: None paper_authors: Shun Lei, Yixuan Z">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-07-29 123:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/07/29/cs.SD_2023_07_29/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="MSStyleTTS: Multi-Scale Style Modeling with Hierarchical Context Information for Expressive Speech Synthesis paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.16012 repo_url: None paper_authors: Shun Lei, Yixuan Z">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-28T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:22.430Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_07_29" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/29/cs.SD_2023_07_29/" class="article-date">
  <time datetime="2023-07-28T16:00:00.000Z" itemprop="datePublished">2023-07-29</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-07-29 123:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="MSStyleTTS-Multi-Scale-Style-Modeling-with-Hierarchical-Context-Information-for-Expressive-Speech-Synthesis"><a href="#MSStyleTTS-Multi-Scale-Style-Modeling-with-Hierarchical-Context-Information-for-Expressive-Speech-Synthesis" class="headerlink" title="MSStyleTTS: Multi-Scale Style Modeling with Hierarchical Context Information for Expressive Speech Synthesis"></a>MSStyleTTS: Multi-Scale Style Modeling with Hierarchical Context Information for Expressive Speech Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16012">http://arxiv.org/abs/2307.16012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shun Lei, Yixuan Zhou, Liyang Chen, Zhiyong Wu, Xixin Wu, Shiyin Kang, Helen Meng</li>
<li>for: 这篇论文旨在提高人机交互场景中的自然Expressive speech synthesis，如Audiobooks、Podcasts和语音助手等。</li>
<li>methods: 该论文提出了一种基于多层次上下文信息的 Style modeling 方法，以capture和预测不同级别的样式信息，从而实现自然Expressive speech synthesis。两个子模块，包括多层次样式抽取器和多层次样式预测器，与基于 FastSpeech 2 的语音模型一起训练。预测器利用了层次结构关系来探索上下文信息，并预测样式嵌入的多级别。抽取器从真实的语音样本中提取多层次样式嵌入，并直接导航式预测样式。</li>
<li>results: 论文的实验结果表明，提出的方法在域内和域外的 audiobook 数据集上显著超越了三个基线。此外，我们还进行了上下文信息和多层次样式表示的分析，这些分析结果从未被讨论过。<details>
<summary>Abstract</summary>
Expressive speech synthesis is crucial for many human-computer interaction scenarios, such as audiobooks, podcasts, and voice assistants. Previous works focus on predicting the style embeddings at one single scale from the information within the current sentence. Whereas, context information in neighboring sentences and multi-scale nature of style in human speech are neglected, making it challenging to convert multi-sentence text into natural and expressive speech. In this paper, we propose MSStyleTTS, a style modeling method for expressive speech synthesis, to capture and predict styles at different levels from a wider range of context rather than a sentence. Two sub-modules, including multi-scale style extractor and multi-scale style predictor, are trained together with a FastSpeech 2 based acoustic model. The predictor is designed to explore the hierarchical context information by considering structural relationships in context and predict style embeddings at global-level, sentence-level and subword-level. The extractor extracts multi-scale style embedding from the ground-truth speech and explicitly guides the style prediction. Evaluations on both in-domain and out-of-domain audiobook datasets demonstrate that the proposed method significantly outperforms the three baselines. In addition, we conduct the analysis of the context information and multi-scale style representations that have never been discussed before.
</details>
<details>
<summary>摘要</summary>
干脆的语音合成是许多人机交互场景中的关键，如Audiobook、Podcast和语音助手。先前的工作主要关注在一个层次上从当前句子中预测样式嵌入。然而，邻近句子的上下文信息和人类语音中的多级样式特征被忽略，使得将多句子文本转化为自然和干脆的语音很困难。在这篇论文中，我们提出了MSStyleTTS方法，用于启发式语音合成，以捕捉和预测不同级别的样式。我们在 FastSpeech 2 基础模型中训练了两个子模块：多级样式抽取器和多级样式预测器。预测器通过考虑语音结构关系来探索层次上下文信息，并预测样式嵌入。抽取器从真实语音中提取多级样式嵌入，并直接引导样式预测。我们对域外和域内 Audiobook 数据集进行评估，结果显示，我们的方法与基eline比较出色。此外，我们还进行了上下文信息和多级样式表示的分析，这些分析从未被讨论过。
</details></li>
</ul>
<hr>
<h2 id="Moisesdb-A-dataset-for-source-separation-beyond-4-stems"><a href="#Moisesdb-A-dataset-for-source-separation-beyond-4-stems" class="headerlink" title="Moisesdb: A dataset for source separation beyond 4-stems"></a>Moisesdb: A dataset for source separation beyond 4-stems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15913">http://arxiv.org/abs/2307.15913</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/moises-ai/moises-db">https://github.com/moises-ai/moises-db</a></li>
<li>paper_authors: Igor Pereira, Felipe Araújo, Filip Korzeniowski, Richard Vogl</li>
<li>for: 这篇论文是为了介绍一个新的音乐源分离数据集，即MoisesDB dataset，用于音乐源分离。</li>
<li>methods: 该数据集包含240首歌曲，来自45位艺术家，涵盖了12种 музыкальных类型。每首歌曲都有其各自的音频源，分为两级层次的概念分类法，以便建立和评估细化的音乐源分离系统。</li>
<li>results: 本文提供了一个简单易用的Python库，可以下载、处理和使用MoisesDB数据集。此外，文章还提供了数据集的详细文档和分析，以及一些基准结果，用于评估不同精度的开源分离模型。<details>
<summary>Abstract</summary>
In this paper, we introduce the MoisesDB dataset for musical source separation. It consists of 240 tracks from 45 artists, covering twelve musical genres. For each song, we provide its individual audio sources, organized in a two-level hierarchical taxonomy of stems. This will facilitate building and evaluating fine-grained source separation systems that go beyond the limitation of using four stems (drums, bass, other, and vocals) due to lack of data. To facilitate the adoption of this dataset, we publish an easy-to-use Python library to download, process and use MoisesDB. Alongside a thorough documentation and analysis of the dataset contents, this work provides baseline results for open-source separation models for varying separation granularities (four, five, and six stems), and discuss their results.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了Musical Source Separation的MoisesDB数据集。它包含240首歌曲，来自45位艺术家，涵盖了12种音乐类型。每首歌曲都有其自己的声音来源，按照两级层次的分类系统组织，可以促进建立和评估细致的音乐源分离系统。为了促进这个数据集的采用，我们在Python库中发布了一个易于使用的下载、处理和使用的MoisesDB库。同时，我们也提供了详细的文档和数据集的分析，以及不同的分离精度（四、五、六个声音来源）的基准结果。
</details></li>
</ul>
<hr>
<h2 id="UniBriVL-Robust-Universal-Representation-and-Generation-of-Audio-Driven-Diffusion-Models"><a href="#UniBriVL-Robust-Universal-Representation-and-Generation-of-Audio-Driven-Diffusion-Models" class="headerlink" title="UniBriVL: Robust Universal Representation and Generation of Audio Driven Diffusion Models"></a>UniBriVL: Robust Universal Representation and Generation of Audio Driven Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15898">http://arxiv.org/abs/2307.15898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sen Fang, Bowen Gao, Yangjian Wu, Jingwen Cai, Teik Toe Teoh</li>
<li>for: 本研究旨在提出一种基于视界语言（BriVL）的全新语言表示学习方法，以实现多模态应用。</li>
<li>methods: 该方法使用UniBriVL将音频、图像和文本 embedding到共享空间中，解决了语言表示学习中的重要挑战，并能够有效捕捉音频和图像之间的相关性。</li>
<li>results: 实验结果表明，UniBriVL在下游任务中表现出色，并且可以从音频中生成合适的图像。这种方法有很多应用可能性，如语音识别、音频处理和描述系统等。<details>
<summary>Abstract</summary>
Multimodal large models have been recognized for their advantages in various performance and downstream tasks. The development of these models is crucial towards achieving general artificial intelligence in the future. In this paper, we propose a novel universal language representation learning method called UniBriVL, which is based on Bridging-Vision-and-Language (BriVL). Universal BriVL embeds audio, image, and text into a shared space, enabling the realization of various multimodal applications. Our approach addresses major challenges in robust language (both text and audio) representation learning and effectively captures the correlation between audio and image. Additionally, we demonstrate the qualitative evaluation of the generated images from UniBriVL, which serves to highlight the potential of our approach in creating images from audio. Overall, our experimental results demonstrate the efficacy of UniBriVL in downstream tasks and its ability to choose appropriate images from audio. The proposed approach has the potential for various applications such as speech recognition, music signal processing, and captioning systems.
</details>
<details>
<summary>摘要</summary>
多Modal大型模型已被认可为在多种表现和下游任务中具有优势。这些模型的开发对于实现未来的通用人工智能是关键。本文提出了一种新的通用语言表示学习方法，即UniBriVL，该方法基于bridging-vision-and-language（BriVL）。这种通用BriVL嵌入音频、图像和文本到共享空间中，使得实现多种多Modal应用程序 becomes possible。我们的方法解决了文本和音频语言表示学习中的主要挑战，并有效地捕捉 audio和图像之间的相关性。此外，我们还对UniBriVL生成的图像进行质量评估，以强调我们的方法在创建图像 FROM audio 方面的潜在性。总的来说，我们的实验结果表明UniBriVL在下游任务中的效果和其能够选择合适的图像 FROM audio。该方法在语音识别、音频信号处理和captioning系统等应用中具有潜在的应用前景。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/29/cs.SD_2023_07_29/" data-id="clltau93l0084cr88hwvy18to" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/30/eess.IV_2023_07_30/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.IV - 2023-07-30 17:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/29/eess.AS_2023_07_29/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.AS - 2023-07-29 22:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
