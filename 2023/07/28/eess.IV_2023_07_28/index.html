
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-07-28 17:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="A Survey on Deep Learning in Medical Image Registration: New Technologies, Uncertainty, Evaluation Metrics, and Beyond paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.15615 repo_url: None paper_authors: Junyu Ch">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-07-28 17:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/07/28/eess.IV_2023_07_28/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="A Survey on Deep Learning in Medical Image Registration: New Technologies, Uncertainty, Evaluation Metrics, and Beyond paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.15615 repo_url: None paper_authors: Junyu Ch">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-27T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:22.000Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_07_28" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/28/eess.IV_2023_07_28/" class="article-date">
  <time datetime="2023-07-27T16:00:00.000Z" itemprop="datePublished">2023-07-28</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-07-28 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Survey-on-Deep-Learning-in-Medical-Image-Registration-New-Technologies-Uncertainty-Evaluation-Metrics-and-Beyond"><a href="#A-Survey-on-Deep-Learning-in-Medical-Image-Registration-New-Technologies-Uncertainty-Evaluation-Metrics-and-Beyond" class="headerlink" title="A Survey on Deep Learning in Medical Image Registration: New Technologies, Uncertainty, Evaluation Metrics, and Beyond"></a>A Survey on Deep Learning in Medical Image Registration: New Technologies, Uncertainty, Evaluation Metrics, and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15615">http://arxiv.org/abs/2307.15615</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyu Chen, Yihao Liu, Shuwen Wei, Zhangxing Bian, Shalini Subramanian, Aaron Carass, Jerry L. Prince, Yong Du</li>
<li>for: 本研究团队提出了一种基于深度学习的医学图像注册方法，以推动医学图像注册领域的发展。</li>
<li>methods: 本研究使用了多种深度学习网络，包括ResNet和U-Net，以及不同的相似度度量和减杂化正则化。</li>
<li>results: 本研究提出了一种全面的深度学习基于图像注册方法，包括网络架构、损失函数和注册不确定性估计。此外，本研究还提出了用于评估深度学习模型在注册任务中的评价指标。<details>
<summary>Abstract</summary>
Over the past decade, deep learning technologies have greatly advanced the field of medical image registration. The initial developments, such as ResNet-based and U-Net-based networks, laid the groundwork for deep learning-driven image registration. Subsequent progress has been made in various aspects of deep learning-based registration, including similarity measures, deformation regularizations, and uncertainty estimation. These advancements have not only enriched the field of deformable image registration but have also facilitated its application in a wide range of tasks, including atlas construction, multi-atlas segmentation, motion estimation, and 2D-3D registration. In this paper, we present a comprehensive overview of the most recent advancements in deep learning-based image registration. We begin with a concise introduction to the core concepts of deep learning-based image registration. Then, we delve into innovative network architectures, loss functions specific to registration, and methods for estimating registration uncertainty. Additionally, this paper explores appropriate evaluation metrics for assessing the performance of deep learning models in registration tasks. Finally, we highlight the practical applications of these novel techniques in medical imaging and discuss the future prospects of deep learning-based image registration.
</details>
<details>
<summary>摘要</summary>
We begin with a concise introduction to the core concepts of deep learning-based image registration. Then, we delve into innovative network architectures, loss functions specific to registration, and methods for estimating registration uncertainty. Additionally, this paper explores appropriate evaluation metrics for assessing the performance of deep learning models in registration tasks. Finally, we highlight the practical applications of these novel techniques in medical imaging and discuss the future prospects of deep learning-based image registration.Translated into Simplified Chinese:过去十年，深度学习技术在医疗影像registrations中取得了大量进步。初期发展，如ResNet基于和U-Net基于的网络，为深度学习驱动的影像registrations奠定了基础。后续的进步包括相似度度量、形态规范和注意力估计等方面，这些进步不仅涌现了弹性影像registrations的领域，还为各种任务，如建立Atlas、多Atlas分割、运动估计和2D-3Dregistrations提供了应用。在本文中，我们提供了最新的深度学习基于影像registrations的概括。我们从核心概念的介绍开始，然后探讨了新的网络架构、特定于registrations的损失函数和注意力估计方法。此外，这篇论文还探讨了评价深度学习模型在registrations任务中表现的适当评价指标。最后，我们强调了这些新技术在医疗影像中的实际应用和未来前景。
</details></li>
</ul>
<hr>
<h2 id="Integrated-Digital-Reconstruction-of-Welded-Components-Supporting-Improved-Fatigue-Life-Prediction"><a href="#Integrated-Digital-Reconstruction-of-Welded-Components-Supporting-Improved-Fatigue-Life-Prediction" class="headerlink" title="Integrated Digital Reconstruction of Welded Components: Supporting Improved Fatigue Life Prediction"></a>Integrated Digital Reconstruction of Welded Components: Supporting Improved Fatigue Life Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15604">http://arxiv.org/abs/2307.15604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anders Faarbæk Mikkelstrup, Morten Kristiansen</li>
<li>for: 提高锚固结构的质量和安全性</li>
<li>methods: 使用自动化高频机械冲击（HFMI）处理、图像处理、简单滤波技术和非线性优化算法对拼接部位进行数字重建</li>
<li>results: 实现了 generic、cost-effective、flexible 和 rapid 的数字重建方法，帮助提高锚固结构的设计、质量监控和HFMI处理记录<details>
<summary>Abstract</summary>
In the design of offshore jacket foundations, fatigue life is crucial. Post-weld treatment has been proposed to enhance the fatigue performance of welded joints, where particularly high-frequency mechanical impact (HFMI) treatment has been shown to improve fatigue performance significantly. Automated HFMI treatment has improved quality assurance and can lead to cost-effective design when combined with accurate fatigue life prediction. However, the finite element method (FEM), commonly used for predicting fatigue life in complex or multi-axial joints, relies on a basic CAD depiction of the weld, failing to consider the actual weld geometry and defects. Including the actual weld geometry in the FE model improves fatigue life prediction and possible crack location prediction but requires a digital reconstruction of the weld. Current digital reconstruction methods are time-consuming or require specialised scanning equipment and potential component relocation. The proposed framework instead uses an industrial manipulator combined with a line scanner to integrate digital reconstruction as part of the automated HFMI treatment setup. This approach applies standard image processing, simple filtering techniques, and non-linear optimisation for aligning and merging overlapping scans. A screened Poisson surface reconstruction finalises the 3D model to create a meshed surface. The outcome is a generic, cost-effective, flexible, and rapid method that enables generic digital reconstruction of welded parts, aiding in component design, overall quality assurance, and documentation of the HFMI treatment.
</details>
<details>
<summary>摘要</summary>
在海上钻井基础设计中，腐蚀性 жизни是关键。Post-weld处理被提议来提高焊接缝合的腐蚀性能，特别是高频机械冲击（HFMI）处理，可以大幅提高腐蚀性能。自动化HFMI处理可以提高质量保证和降低设计成本，当与准确的腐蚀生命预测结合使用时。然而，通用finite element方法（FEM），常用于预测复杂或多轴缝合的腐蚀生命，基于简化的CAD描述，忽略了实际焊接形状和缺陷。包含实际焊接形状在FEM模型中可以提高腐蚀生命预测和可能的裂化位置预测，但需要数字重建焊接。当前的数字重建方法需要大量时间或特殊的扫描设备，以及可能的组件重新位置。我们提出的框架则使用工业护手机和线扫描器结合数字重建，并通过标准的图像处理、简单的滤波技术和非线性优化来对 overlap 扫描进行对接和合并。屏幕Poisson面重建最终生成3D模型，生成一个通用、成本效果、灵活、快速的方法，以便在焊接部件的设计、总质量保证和HFMI处理的文档中进行数字重建。
</details></li>
</ul>
<hr>
<h2 id="OAFuser-Towards-Omni-Aperture-Fusion-for-Light-Field-Semantic-Segmentation-of-Road-Scenes"><a href="#OAFuser-Towards-Omni-Aperture-Fusion-for-Light-Field-Semantic-Segmentation-of-Road-Scenes" class="headerlink" title="OAFuser: Towards Omni-Aperture Fusion for Light Field Semantic Segmentation of Road Scenes"></a>OAFuser: Towards Omni-Aperture Fusion for Light Field Semantic Segmentation of Road Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15588">http://arxiv.org/abs/2307.15588</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/feibryantkit/oafuser">https://github.com/feibryantkit/oafuser</a></li>
<li>paper_authors: Fei Teng, Jiaming Zhang, Kunyu Peng, Kailun Yang, Yaonan Wang, Rainer Stiefelhagen</li>
<li>for: 增强自动驾驶场景理解的图像Semantic Segmentation，使用光场相机提供了丰富的angular和空间信息。</li>
<li>methods: 提议Omni-Aperture Fusion模型（OAFuser），利用中心视图的denseContext和sub-aperture图像中的angular信息来生成semantically-consistent的结果，同时采用Sub-Aperture Fusion Module（SAFM）将sub-aperture图像嵌入angular特征中，不需要额外内存成本。</li>
<li>results: 在UrbanLF-Real和-Syn数据集上实现了state-of-the-art性能，在UrbanLF-Real Extended数据集上达到了84.93%的mIoU记录，比前一个最佳记录提高+4.53%。<details>
<summary>Abstract</summary>
Light field cameras can provide rich angular and spatial information to enhance image semantic segmentation for scene understanding in the field of autonomous driving. However, the extensive angular information of light field cameras contains a large amount of redundant data, which is overwhelming for the limited hardware resource of intelligent vehicles. Besides, inappropriate compression leads to information corruption and data loss. To excavate representative information, we propose an Omni-Aperture Fusion model (OAFuser), which leverages dense context from the central view and discovers the angular information from sub-aperture images to generate a semantically-consistent result. To avoid feature loss during network propagation and simultaneously streamline the redundant information from the light field camera, we present a simple yet very effective Sub-Aperture Fusion Module (SAFM) to embed sub-aperture images into angular features without any additional memory cost. Furthermore, to address the mismatched spatial information across viewpoints, we present Center Angular Rectification Module (CARM) realized feature resorting and prevent feature occlusion caused by asymmetric information. Our proposed OAFuser achieves state-of-the-art performance on the UrbanLF-Real and -Syn datasets and sets a new record of 84.93% in mIoU on the UrbanLF-Real Extended dataset, with a gain of +4.53%. The source code of OAFuser will be made publicly available at https://github.com/FeiBryantkit/OAFuser.
</details>
<details>
<summary>摘要</summary>
光场相机可以提供rich的ANGLE和空间信息，以增强图像Semantic Segmentation，以提高自动驾驶场景理解。然而，光场相机的广泛ANGLE信息包含大量冗余数据，对智能车辆的硬件资源造成拥堵。此外，不当压缩可能会导致信息损害和数据丢失。为了提取代表性信息，我们提议了一种Omni-Aperture Fusion模型（OAFuser），它利用中心视图的dense上下文和子视图图像ANGLE信息来生成具有相同semantic consistency的结果。为了避免网络传播过程中的特征损失和同时压缩光场相机中的冗余信息，我们提出了一种简单 yet powerful的Sub-Aperture Fusion模块（SAFM），可以将子视图图像与ANGLE特征进行嵌入，无需额外内存成本。此外，为了解决不同视角之间的匹配问题，我们提出了Center Angular Rectification Module（CARM），通过实现特征重定向和避免特征堵塞，解决了由不同视角所导致的匹配问题。我们的OAFuser模型在UrbanLF-Real和UrbanLF-Syn数据集上达到了状态机器人的性能记录，具有84.93%的mIoU在UrbanLF-Real Extended数据集上，与前一个记录之间的增幅为+4.53%。OAFuser模型的源代码将于https://github.com/FeiBryantkit/OAFuser上公开。
</details></li>
</ul>
<hr>
<h2 id="Defocus-Blur-Synthesis-and-Deblurring-via-Interpolation-and-Extrapolation-in-Latent-Space"><a href="#Defocus-Blur-Synthesis-and-Deblurring-via-Interpolation-and-Extrapolation-in-Latent-Space" class="headerlink" title="Defocus Blur Synthesis and Deblurring via Interpolation and Extrapolation in Latent Space"></a>Defocus Blur Synthesis and Deblurring via Interpolation and Extrapolation in Latent Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15461">http://arxiv.org/abs/2307.15461</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nis-research/linear-latent-blur">https://github.com/nis-research/linear-latent-blur</a></li>
<li>paper_authors: Ioana Mazilu, Shunxin Wang, Sven Dummer, Raymond Veldhuis, Christoph Brune, Nicola Strisciuglio</li>
<li>For: 这项研究旨在提高微scopic图像质量，以便更好地进行医疗诊断和疾病分析。* Methods: 该项目使用自适应柱面网络和正则化技术，以实现对图像的杂谱和解析。* Results: 研究人员通过训练自适应网络并应用正则化技术，实现了对图像的杂谱和解析，提高了微scopic图像的质量，并可以用作数据增强技术。<details>
<summary>Abstract</summary>
Though modern microscopes have an autofocusing system to ensure optimal focus, out-of-focus images can still occur when cells within the medium are not all in the same focal plane, affecting the image quality for medical diagnosis and analysis of diseases. We propose a method that can deblur images as well as synthesize defocus blur. We train autoencoders with implicit and explicit regularization techniques to enforce linearity relations among the representations of different blur levels in the latent space. This allows for the exploration of different blur levels of an object by linearly interpolating/extrapolating the latent representations of images taken at different focal planes. Compared to existing works, we use a simple architecture to synthesize images with flexible blur levels, leveraging the linear latent space. Our regularized autoencoders can effectively mimic blur and deblur, increasing data variety as a data augmentation technique and improving the quality of microscopic images, which would be beneficial for further processing and analysis.
</details>
<details>
<summary>摘要</summary>
modern microscopes 已经有自动对焦系统，但是仍然可能出现不清晰的图像，因为细胞在媒体中不 все在同一个 фокус平面上，这会影响医疗诊断和疾病分析的图像质量。我们提出了一种方法，可以对图像进行恢复和模拟杂化。我们使用隐式和显式正则化技术来规范各个混杂水平的表示在隐藏空间中的线性关系。这allow for随着不同 фокус平面上图像的latent representation的线性 interpolate/extrapolate，以探索不同混杂水平的对象图像。相比之下，我们使用简单的建筑来 sintesize图像，并且可以在隐藏空间中Linearly interpolate/extrapolate不同混杂水平，从而增加数据的多样性，并提高微scopic图像的质量，这将对进一步处理和分析产生有利影响。
</details></li>
</ul>
<hr>
<h2 id="ERCPMP-An-Endoscopic-Image-and-Video-Dataset-for-Colorectal-Polyps-Morphology-and-Pathology"><a href="#ERCPMP-An-Endoscopic-Image-and-Video-Dataset-for-Colorectal-Polyps-Morphology-and-Pathology" class="headerlink" title="ERCPMP: An Endoscopic Image and Video Dataset for Colorectal Polyps Morphology and Pathology"></a>ERCPMP: An Endoscopic Image and Video Dataset for Colorectal Polyps Morphology and Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15444">http://arxiv.org/abs/2307.15444</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mojgan Forootan, Mohsen Rajabnia, Ahmad R Mafi, Hamed Azhdari Tehrani, Erfan Ghadirzadeh, Mahziar Setayeshfar, Zahra Ghaffari, Mohammad Tashakoripour, Mohammad Reza Zali, Hamidreza Bolhasani</li>
<li>for: This paper is written for the purpose of developing accurate algorithms for medical prediction, detection, diagnosis, treatment, and prognosis, specifically for colorectal polyps.</li>
<li>methods: The paper uses a dataset called ERCPMP, which contains demographic, morphological, and pathological data, endoscopic images, and videos of 191 patients with colorectal polyps. The dataset includes information based on the latest international gastroenterology classification references, such as Paris, Pit, and JNET classification.</li>
<li>results: The paper provides a dataset that can be used for the development and evaluation of algorithms for the recognition of colorectal polyps morphology and pathology. The dataset is available on Elsevier Mendeley Dataverse and is currently under development, with the latest version accessible via a specific website.Here is the information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了开发准确的医疗预测、检测、诊断、治疗和预后评估算法，特别是为了Rectal Polyps。</li>
<li>methods: 这篇论文使用了名为ERCPMP的数据集，该数据集包含了191名患有Rectal Polyps的病人的民生、形态和病理数据，以及这些病人的endoscopic图像和视频。数据集包括根据最新的国际肠胃病学分类标准，如Paris、Pit和JNET分类标准。</li>
<li>results: 这篇论文提供了一个用于开发和评估Recognize Colorectal Polyps Morphology and Pathology的算法的数据集。数据集可以在Elsevier Mendeley Dataverse上获取，并且目前正在开发中，最新的版本可以通过特定的网站获取。<details>
<summary>Abstract</summary>
In the recent years, artificial intelligence (AI) and its leading subtypes, machine learning (ML) and deep learning (DL) and their applications are spreading very fast in various aspects such as medicine. Today the most important challenge of developing accurate algorithms for medical prediction, detection, diagnosis, treatment and prognosis is data. ERCPMP is an Endoscopic Image and Video Dataset for Recognition of Colorectal Polyps Morphology and Pathology. This dataset contains demographic, morphological and pathological data, endoscopic images and videos of 191 patients with colorectal polyps. Morphological data is included based on the latest international gastroenterology classification references such as Paris, Pit and JNET classification. Pathological data includes the diagnosis of the polyps including Tubular, Villous, Tubulovillous, Hyperplastic, Serrated, Inflammatory and Adenocarcinoma with Dysplasia Grade & Differentiation. The current version of this dataset is published and available on Elsevier Mendeley Dataverse and since it is under development, the latest version is accessible via: https://databiox.com.
</details>
<details>
<summary>摘要</summary>
在最近的几年中，人工智能（AI）和其主要分支——机器学习（ML）和深度学习（DL）在各种领域的应用速度很快，如医学。今天最重要的挑战是开发准确的医疗算法，包括预测、检测、诊断、治疗和预后。ERCPMP是一个endorscopic Image和视频Dataset，用于识别肠RECTALPolyp的形态和病理学特征。该数据集包括191名患者的肠RECTALPolyp的民生、形态和病理学数据，以及endooscopic图像和视频。形态数据按照最新的国际肠胃科分类标准进行编码，包括Paris、Pit和JNET分类。病理数据包括肠RECTALPolyp的诊断，包括管肠、芽孢、管芽孢、高级瘤、炎性、卵极和adenocarcinoma，以及分化度和分化度。当前版本的数据集已经发布，可以在Elsevier Mendeley Dataverse上获取，而最新版本可以通过以下链接获取：https://databiox.com。
</details></li>
</ul>
<hr>
<h2 id="RAWIW-RAW-Image-Watermarking-Robust-to-ISP-Pipeline"><a href="#RAWIW-RAW-Image-Watermarking-Robust-to-ISP-Pipeline" class="headerlink" title="RAWIW: RAW Image Watermarking Robust to ISP Pipeline"></a>RAWIW: RAW Image Watermarking Robust to ISP Pipeline</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15443">http://arxiv.org/abs/2307.15443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kang Fu, Xiaohong Liu, Jun Jia, Zicheng Zhang, Yicong Peng, Jia Wang, Guangtao Zhai</li>
<li>for: 这篇论文的目的是为了提供一种基于深度学习的 RAW 图像水印保护方法，以保护 RAW 图像的版权。</li>
<li>methods: 该方法使用了深度学习网络来实现 RAW 图像水印保护，并将 copyright 信息直接嵌入 RAW 图像中，以便在不同的 post-processing 方法后可以提取 copyright 信息。</li>
<li>results: 该方法在实验中成功地实现了跨频道的版权保护，并且可以在不同的传输损害 circumstance 下保持图像质量和水印的可靠性。<details>
<summary>Abstract</summary>
Invisible image watermarking is essential for image copyright protection. Compared to RGB images, RAW format images use a higher dynamic range to capture the radiometric characteristics of the camera sensor, providing greater flexibility in post-processing and retouching. Similar to the master recording in the music industry, RAW images are considered the original format for distribution and image production, thus requiring copyright protection. Existing watermarking methods typically target RGB images, leaving a gap for RAW images. To address this issue, we propose the first deep learning-based RAW Image Watermarking (RAWIW) framework for copyright protection. Unlike RGB image watermarking, our method achieves cross-domain copyright protection. We directly embed copyright information into RAW images, which can be later extracted from the corresponding RGB images generated by different post-processing methods. To achieve end-to-end training of the framework, we integrate a neural network that simulates the ISP pipeline to handle the RAW-to-RGB conversion process. To further validate the generalization of our framework to traditional ISP pipelines and its robustness to transmission distortion, we adopt a distortion network. This network simulates various types of noises introduced during the traditional ISP pipeline and transmission. Furthermore, we employ a three-stage training strategy to strike a balance between robustness and concealment of watermarking. Our extensive experiments demonstrate that RAWIW successfully achieves cross-domain copyright protection for RAW images while maintaining their visual quality and robustness to ISP pipeline distortions.
</details>
<details>
<summary>摘要</summary>
隐形图像水印是图像版权保护的关键技术。相比RGB图像，RAW格式图像利用更高的动态范围来捕捉相机传感器的 радиметрические特征，提供更大的后处理和修剪灵活性。类似于音乐行业中的母带录制，RAW图像被视为原始格式，需要版权保护。现有的水印方法通常针对RGB图像，留下了RAW图像的空白。为了解决这个问题，我们提出了首个基于深度学习的RAW图像水印（RAWIW）框架，用于版权保护。与RGB图像水印不同，我们的方法实现了跨频道的版权保护。我们直接嵌入版权信息到RAW图像中，可以在不同的后处理方法生成的RGB图像中提取。为实现整个框架的端到端训练，我们将神经网络与ISP管道相互关联，以处理RAW图像到RGB图像的转换过程。此外，我们采用了一个扰动网络，以模拟传输过程中引入的各种噪声。此外，我们采用了三stage训练策略，以保持水印的鲁棒性和隐藏性。我们的广泛实验表明，RAWIW成功实现了RAW图像的跨频道版权保护，保持了图像的视觉质量和传输过程中的鲁棒性。
</details></li>
</ul>
<hr>
<h2 id="MLIC-Linear-Complexity-Multi-Reference-Entropy-Modeling-for-Learned-Image-Compression"><a href="#MLIC-Linear-Complexity-Multi-Reference-Entropy-Modeling-for-Learned-Image-Compression" class="headerlink" title="MLIC++: Linear Complexity Multi-Reference Entropy Modeling for Learned Image Compression"></a>MLIC++: Linear Complexity Multi-Reference Entropy Modeling for Learned Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15421">http://arxiv.org/abs/2307.15421</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiangweibeta/mlic">https://github.com/jiangweibeta/mlic</a></li>
<li>paper_authors: Wei Jiang, Ronggang Wang</li>
<li>for: 这篇论文是为了提出一种基于多参考 entropy 模型的学习图像压缩方法，以提高图像压缩的效率和质量。</li>
<li>methods: 该方法使用 linear complexity global correlations capturing，通过软max操作的减法分解，以取代之前的 attention 方法。</li>
<li>results: 相比 VTM-17.0，该方法可以在 PSNR 指标下减少 BD-rate 12.44%，并且更高效。Code 将在 GitHub 上提供。<details>
<summary>Abstract</summary>
Recently, multi-reference entropy model has been proposed, which captures channel-wise, local spatial, and global spatial correlations. Previous works adopt attention for global correlation capturing, however, the quadratic cpmplexity limits the potential of high-resolution image coding. In this paper, we propose the linear complexity global correlations capturing, via the decomposition of softmax operation. Based on it, we propose the MLIC$^{++}$, a learned image compression with linear complexity for multi-reference entropy modeling. Our MLIC$^{++}$ is more efficient and it reduces BD-rate by 12.44% on the Kodak dataset compared to VTM-17.0 when measured in PSNR. Code will be available at https://github.com/JiangWeibeta/MLIC.
</details>
<details>
<summary>摘要</summary>
最近，多参照 entropy 模型已经被提出，这个模型可以捕捉通道级、本地空间和全局空间相关性。先前的工作采用了注意力来捕捉全局相关性，但是quadratic 复杂度限制了高分辨率图像编码的潜力。在这篇论文中，我们提议使用线性复杂度全球相关性捕捉，通过软max操作的分解。基于这，我们提议了MLIC++，一种学习图像压缩的线性复杂度全球相关性模型。我们的 MLIC++ 比VTM-17.0在Kodak数据集上BD-rate下降12.44%，相对PSNR测量下降。代码将在 GitHub 上发布，地址为https://github.com/JiangWeibeta/MLIC。
</details></li>
</ul>
<hr>
<h2 id="Fast-Dust-Sand-Image-Enhancement-Based-on-Color-Correction-and-New-Membership-Function"><a href="#Fast-Dust-Sand-Image-Enhancement-Based-on-Color-Correction-and-New-Membership-Function" class="headerlink" title="Fast Dust Sand Image Enhancement Based on Color Correction and New Membership Function"></a>Fast Dust Sand Image Enhancement Based on Color Correction and New Membership Function</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15230">http://arxiv.org/abs/2307.15230</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Hakem Alsaeedi, Suha Mohammed Hadi, Yarub Alazzawi</li>
<li>for: 提高灰尘环境中图像质量和可见度</li>
<li>methods: 基于颜色修正和新成员函数，提出了一种新的图像增强模型，包括三个阶段：色差修正、雾气除除和对比和亮度提高</li>
<li>results: 通过对多个真实的灰尘图像进行测试和评估，研究发现该解决方案比现有研究更有效地除去红色和黄色投影，提供高质量和数量的灰尘图像<details>
<summary>Abstract</summary>
Images captured in dusty environments suffering from poor visibility and quality. Enhancement of these images such as sand dust images plays a critical role in various atmospheric optics applications. In this work, proposed a new model based on Color Correction and new membership function to enhance san dust images. The proposed model consists of three phases: correction of color shift, removal of haze, and enhancement of contrast and brightness. The color shift is corrected using a new membership function to adjust the values of U and V in the YUV color space. The Adaptive Dark Channel Prior (A-DCP) is used for haze removal. The stretching contrast and improving image brightness are based on Contrast Limited Adaptive Histogram Equalization (CLAHE). The proposed model tests and evaluates through many real sand dust images. The experimental results show that the proposed solution is outperformed the current studies in terms of effectively removing the red and yellow cast and provides high quality and quantity dust images.
</details>
<details>
<summary>摘要</summary>
图像 capture in 尘埃环境中，由于低可见度和质量，需要进行图像增强。在这种应用中，提出了一种基于颜色修正和新成员函数的图像增强模型。该模型包括三个阶段：颜色偏移 correction，霾除，以及对比和亮度提高。颜色偏移使用新的成员函数来调整 YUV 色彩空间中 U 和 V 的值。使用 Adaptive Dark Channel Prior (A-DCP) 进行霾除。对比和亮度提高基于 Contrast Limited Adaptive Histogram Equalization (CLAHE)。提出的模型在多个真实的沙尘图像上进行测试和评估。实验结果表明，提出的解决方案在效果上超过现有研究，可以有效地去除红色和黄色投影，并提供高质量和量的尘埃图像。
</details></li>
</ul>
<hr>
<h2 id="Generative-AI-for-Medical-Imaging-extending-the-MONAI-Framework"><a href="#Generative-AI-for-Medical-Imaging-extending-the-MONAI-Framework" class="headerlink" title="Generative AI for Medical Imaging: extending the MONAI Framework"></a>Generative AI for Medical Imaging: extending the MONAI Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15208">http://arxiv.org/abs/2307.15208</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/project-monai/generativemodels">https://github.com/project-monai/generativemodels</a></li>
<li>paper_authors: Walter H. L. Pinaya, Mark S. Graham, Eric Kerfoot, Petru-Daniel Tudosiu, Jessica Dafflon, Virginia Fernandez, Pedro Sanchez, Julia Wolleb, Pedro F. da Costa, Ashay Patel, Hyungjin Chung, Can Zhao, Wei Peng, Zelong Liu, Xueyan Mei, Oeslle Lucena, Jong Chul Ye, Sotirios A. Tsaftaris, Prerna Dogra, Andrew Feng, Marc Modat, Parashkev Nachev, Sebastien Ourselin, M. Jorge Cardoso</li>
<li>For: The paper aims to provide a freely available open-source platform for training, evaluating, and deploying generative models for medical imaging applications.* Methods: The paper presents MONAI Generative Models, which include a variety of architectures such as diffusion models, autoregressive transformers, and GANs. The models are implemented in a generalizable fashion, allowing for extension to 2D or 3D scenarios and different modalities such as CT, MRI, and X-Ray data.* Results: The paper demonstrates the effectiveness of the proposed platform by reproducing state-of-the-art studies in a standardized way and providing pre-trained models for the community. The results show that the models can be extended to different anatomical areas and modalities, and the platform is modular and extensible, ensuring long-term maintainability and future feature extension.Here’s the simplified Chinese text for the three key points:</li>
<li>for: 这篇论文的目的是提供医疗影像领域的自由开源平台，用于训练、评估和部署生成模型。</li>
<li>methods: 论文提出了MONAI生成模型，包括各种架构，如扩散模型、自然语言转换模型和GAN等。这些模型被实现在可扩展的方式下，可以扩展到2D或3D场景和不同的感知频谱数据。</li>
<li>results: 论文通过 reproduce state-of-the-art studies in a standardized way和提供社区可用的预训练模型，证明了提案的平台的效果。结果显示，模型可以扩展到不同的解剖区域和感知频谱数据，并且平台具有可扩展和维护的特点，以便将来扩展功能。<details>
<summary>Abstract</summary>
Recent advances in generative AI have brought incredible breakthroughs in several areas, including medical imaging. These generative models have tremendous potential not only to help safely share medical data via synthetic datasets but also to perform an array of diverse applications, such as anomaly detection, image-to-image translation, denoising, and MRI reconstruction. However, due to the complexity of these models, their implementation and reproducibility can be difficult. This complexity can hinder progress, act as a use barrier, and dissuade the comparison of new methods with existing works. In this study, we present MONAI Generative Models, a freely available open-source platform that allows researchers and developers to easily train, evaluate, and deploy generative models and related applications. Our platform reproduces state-of-art studies in a standardised way involving different architectures (such as diffusion models, autoregressive transformers, and GANs), and provides pre-trained models for the community. We have implemented these models in a generalisable fashion, illustrating that their results can be extended to 2D or 3D scenarios, including medical images with different modalities (like CT, MRI, and X-Ray data) and from different anatomical areas. Finally, we adopt a modular and extensible approach, ensuring long-term maintainability and the extension of current applications for future features.
</details>
<details>
<summary>摘要</summary>
最近的生成AI技术突破有很大的进步，特别是在医学影像领域。这些生成模型具有很大的潜力，不仅可以帮助安全地分享医学数据通过生成的数据集，还可以执行多种多样的应用，如异常检测、图像到图像翻译、减噪和MRI重建。然而，由于这些模型的复杂性，其实现和复制可能会困难。这种复杂性可能会阻碍进步，成为使用障碍和阻碍新方法与现有工作的比较。在本研究中，我们介绍MONAI生成模型平台，这是一个免费、开源的平台，允许研究人员和开发人员轻松地训练、评估和部署生成模型和相关应用。我们的平台可以复制现状的研究，使用不同的架构（如扩散模型、自适应变换和GAN），并提供了社区可用的预训练模型。我们实现了这些模型的普适性，说明它们的结果可以扩展到2D或3D场景，包括医学影像不同模式（如CT、MRI和X射数据）和不同解剖区域。最后，我们采用了模块化和可扩展的方法，确保长期维护和未来特性的扩展。
</details></li>
</ul>
<hr>
<h2 id="Sparsity-aware-coding-for-single-photon-sensitive-vision-using-Selective-Sensing"><a href="#Sparsity-aware-coding-for-single-photon-sensitive-vision-using-Selective-Sensing" class="headerlink" title="Sparsity aware coding for single photon sensitive vision using Selective Sensing"></a>Sparsity aware coding for single photon sensitive vision using Selective Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15184">http://arxiv.org/abs/2307.15184</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yizhou Lu, Trevor Seets, Ehsan Ahmadi, Felipe Gutierrez-Barragan, Andreas Velten</li>
<li>for: 提高图像技术的表达能力</li>
<li>methods: 利用训练数据学习偏好，优化编码策略为下游分类任务</li>
<li>results: 比传统编码策略更高的编码性能和总准确率，适用于光子计数设备下的图像分类任务<details>
<summary>Abstract</summary>
Optical coding has been widely adopted to improve the imaging techniques. Traditional coding strategies developed under additive Gaussian noise fail to perform optimally in the presence of Poisson noise. It has been observed in previous studies that coding performance varies significantly between these two noise models. In this work, we introduce a novel approach called selective sensing, which leverages training data to learn priors and optimizes the coding strategies for downstream classification tasks. By adapting to the specific characteristics of photon-counting sensors, the proposed method aims to improve coding performance under Poisson noise and enhance overall classification accuracy. Experimental and simulated results demonstrate the effectiveness of selective sensing in comparison to traditional coding strategies, highlighting its potential for practical applications in photon counting scenarios where Poisson noise are prevalent.
</details>
<details>
<summary>摘要</summary>
光学编码已广泛应用以提高成像技术。传统的编码策略在添加性 Gaussian 噪声下发展而来，不能在Poisson噪声下表现优化。先前的研究发现，编码性能在这两种噪声模型之间有很大差异。在这种工作中，我们提出了一种新的方法 called 选择感知，通过使用训练数据来学习假设和优化编码策略，以提高下游分类任务的性能。适应光子计数器的特点，提出的方法可以在Poisson噪声下提高编码性能并提高总分类精度。实验和 simulate 结果表明，选择感知方法比传统编码策略更有效，这 highlights 其在光子计数器场景中的应用潜力。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/28/eess.IV_2023_07_28/" data-id="clly3606g00dhdd88h45ffat7" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/28/eess.AS_2023_07_28/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.AS - 2023-07-28 22:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/27/cs.LG_2023_07_27/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-07-27 18:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">24</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">24</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">24</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">57</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">55</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">104</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
