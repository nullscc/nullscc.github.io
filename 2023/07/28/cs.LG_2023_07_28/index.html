
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-07-28 18:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="TriadNet: Sampling-free predictive intervals for lesional volume in 3D brain MR images paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.15638 repo_url: https:&#x2F;&#x2F;github.com&#x2F;benolmbrt&#x2F;TriadNet paper_authors: Benjami">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-07-28 18:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/07/28/cs.LG_2023_07_28/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="TriadNet: Sampling-free predictive intervals for lesional volume in 3D brain MR images paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.15638 repo_url: https:&#x2F;&#x2F;github.com&#x2F;benolmbrt&#x2F;TriadNet paper_authors: Benjami">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-27T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:39.898Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_07_28" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/28/cs.LG_2023_07_28/" class="article-date">
  <time datetime="2023-07-27T16:00:00.000Z" itemprop="datePublished">2023-07-28</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-07-28 18:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="TriadNet-Sampling-free-predictive-intervals-for-lesional-volume-in-3D-brain-MR-images"><a href="#TriadNet-Sampling-free-predictive-intervals-for-lesional-volume-in-3D-brain-MR-images" class="headerlink" title="TriadNet: Sampling-free predictive intervals for lesional volume in 3D brain MR images"></a>TriadNet: Sampling-free predictive intervals for lesional volume in 3D brain MR images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15638">http://arxiv.org/abs/2307.15638</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/benolmbrt/TriadNet">https://github.com/benolmbrt/TriadNet</a></li>
<li>paper_authors: Benjamin Lambert, Florence Forbes, Senan Doyle, Michel Dojat</li>
<li>for: 用于评估脑损害（如血栓或肿瘤）的体积，以估计患者的预后和导航治疗策略。</li>
<li>methods: 使用深度卷积神经网络（CNN）进行分割，并同时提供评估范围，以帮助估计脑损害的体积。</li>
<li>results: 在BraTS 2021数据集上，TriadNet方法比其他解决方案更加高效，可在一秒钟内提供评估范围和脑损害体积。<details>
<summary>Abstract</summary>
The volume of a brain lesion (e.g. infarct or tumor) is a powerful indicator of patient prognosis and can be used to guide the therapeutic strategy. Lesional volume estimation is usually performed by segmentation with deep convolutional neural networks (CNN), currently the state-of-the-art approach. However, to date, few work has been done to equip volume segmentation tools with adequate quantitative predictive intervals, which can hinder their usefulness and acceptation in clinical practice. In this work, we propose TriadNet, a segmentation approach relying on a multi-head CNN architecture, which provides both the lesion volumes and the associated predictive intervals simultaneously, in less than a second. We demonstrate its superiority over other solutions on BraTS 2021, a large-scale MRI glioblastoma image database.
</details>
<details>
<summary>摘要</summary>
���sterreich brain lesion (e.g. infarct or tumor) 的尺寸是一个非常重要的患者预测指标，可以用来引导治疗策略。 lesional volume estimation 通常是使用深度卷积神经网络 (CNN) 进行，目前是领域的先进方法。然而，到目前为止，几乎没有工作是将量 segmentation 工具与适当的量预测 інтервал相结合，这可能会妨碍它们在临床实践中的使用和接受度。在这个工作中，我们提出了 TriadNet，一种基于多头 CNN 架构的 segmentation 方法，可以同时提供lesion 的尺寸和相应的预测 интервал，几乎在一秒内。我们在 BraTS 2021 大规模 MRI 肿瘤图像数据库上进行了比较，该方法的超越性在�relation 中被证明。
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Analysis-of-Machine-Learning-Methods-for-Lane-Change-Intention-Recognition-Using-Vehicle-Trajectory-Data"><a href="#A-Comparative-Analysis-of-Machine-Learning-Methods-for-Lane-Change-Intention-Recognition-Using-Vehicle-Trajectory-Data" class="headerlink" title="A Comparative Analysis of Machine Learning Methods for Lane Change Intention Recognition Using Vehicle Trajectory Data"></a>A Comparative Analysis of Machine Learning Methods for Lane Change Intention Recognition Using Vehicle Trajectory Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15625">http://arxiv.org/abs/2307.15625</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renteng Yuan</li>
<li>for: 本研究旨在提高自动驾驶车辆对周围环境的理解，认识安全隐患，以提高交通安全性。</li>
<li>methods: 本研究使用不同机器学习方法来识别LC意图从高维时序数据中。</li>
<li>results: 结果显示， ensemble方法可以减少类型II和类型III分类错误的影响，而LightGBM Algorithm在模型训练效率方面与XGBoost Algorithm进行了六倍提升。<details>
<summary>Abstract</summary>
Accurately detecting and predicting lane change (LC)processes can help autonomous vehicles better understand their surrounding environment, recognize potential safety hazards, and improve traffic safety. This paper focuses on LC processes and compares different machine learning methods' performance to recognize LC intention from high-dimensionality time series data. To validate the performance of the proposed models, a total number of 1023 vehicle trajectories is extracted from the CitySim dataset. For LC intention recognition issues, the results indicate that with ninety-eight percent of classification accuracy, ensemble methods reduce the impact of Type II and Type III classification errors. Without sacrificing recognition accuracy, the LightGBM demonstrates a sixfold improvement in model training efficiency than the XGBoost algorithm.
</details>
<details>
<summary>摘要</summary>
Lane 变化（LC）过程的准确探测和预测可以帮助自动驾驶车辆更好地了解它所在环境，认出安全隐患，提高交通安全性。本文关注LC过程，比较不同机器学习方法在 recognize LC 意图从高维时序数据中的表现。为验证提案模型的性能，从CitySim数据集中抽取了1023辆车辆的轨迹。对LC意图认识问题，结果表明， ensemble 方法可以降低 Type II 和 Type III 类错误的影响，并不 sacrificing 认识精度。LightGBM 算法比 XGBoost 算法速度6倍提高，无需牺牲认识精度。
</details></li>
</ul>
<hr>
<h2 id="Shrink-Perturb-Improves-Architecture-Mixing-during-Population-Based-Training-for-Neural-Architecture-Search"><a href="#Shrink-Perturb-Improves-Architecture-Mixing-during-Population-Based-Training-for-Neural-Architecture-Search" class="headerlink" title="Shrink-Perturb Improves Architecture Mixing during Population Based Training for Neural Architecture Search"></a>Shrink-Perturb Improves Architecture Mixing during Population Based Training for Neural Architecture Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15621">http://arxiv.org/abs/2307.15621</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/awesomelemon/pbt-nas">https://github.com/awesomelemon/pbt-nas</a></li>
<li>paper_authors: Alexander Chebykin, Arkadiy Dushatskiy, Tanja Alderliesten, Peter A. N. Bosman</li>
<li>for: 这个研究显示同时对 neural network 进行训练和混合是一种可行的方法来进行神经建筑搜索 (Neural Architecture Search, NAS)。</li>
<li>methods: 这个方法使用 Population Based Training (PBT) 算法来进行参数优化，并在训练过程中运用 shrink-perturb 技术来替换不好performing 网络，并将其替换为良好performing 网络的结果。</li>
<li>results: PBT-NAS 在具有挑战性的任务（如图像生成和强化学习）上表现出色，较baseline (随机搜索和突变基于 PBT) 高效。<details>
<summary>Abstract</summary>
In this work, we show that simultaneously training and mixing neural networks is a promising way to conduct Neural Architecture Search (NAS). For hyperparameter optimization, reusing the partially trained weights allows for efficient search, as was previously demonstrated by the Population Based Training (PBT) algorithm. We propose PBT-NAS, an adaptation of PBT to NAS where architectures are improved during training by replacing poorly-performing networks in a population with the result of mixing well-performing ones and inheriting the weights using the shrink-perturb technique. After PBT-NAS terminates, the created networks can be directly used without retraining. PBT-NAS is highly parallelizable and effective: on challenging tasks (image generation and reinforcement learning) PBT-NAS achieves superior performance compared to baselines (random search and mutation-based PBT).
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们显示同时训练和混合神经网络是对神经建筑搜索（NAS）的有前途的方法。为Hyperparameter优化，重用部分训练过的权重可以实现高效的搜索，这已经由人口基本训练（PBT）算法所证明。我们提议PBT-NAS，即将PBT适应到NAS中，在训练过程中通过将不好performing网络 populations中替换为well-performing网络的结果和权重使用缩短perturb技术来改进建筑。PBT-NAS终止后，创造的网络可以直接使用无需重新训练。PBT-NAS高度并行化和有效：在复杂任务（图像生成和强化学习）中，PBT-NAS比基eline（随机搜索和变换基eline PBT）表现出色。
</details></li>
</ul>
<hr>
<h2 id="Robust-Distortion-free-Watermarks-for-Language-Models"><a href="#Robust-Distortion-free-Watermarks-for-Language-Models" class="headerlink" title="Robust Distortion-free Watermarks for Language Models"></a>Robust Distortion-free Watermarks for Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15593">http://arxiv.org/abs/2307.15593</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jthickstun/watermark">https://github.com/jthickstun/watermark</a></li>
<li>paper_authors: Rohith Kuditipudi, John Thickstun, Tatsunori Hashimoto, Percy Liang</li>
<li>for: 本研究旨在对于autoregressive语言模型中植入水印，以确保文本的 Integrity 和 Authenticity。</li>
<li>methods: 本研究使用了两种抽样方法：倒数抽样和对数最小抽样。实际上，我们将随机水印钥匙映射到语言模型中的样本中，以生成水印文本。检测水印文本的方法是让任何知道水印钥匙的人将文本与随机数据进行比较。</li>
<li>results: 本研究透过实际应用三个语言模型（OPT-1.3B、LLaMA-7B和Alpaca-7B），评估了这种水印方法的 statistically 的能力和对各种重建攻击的Robustness。结果显示，对OPT-1.3B和LLaMA-7B模型，我们可以在40-50%的字串替换、插入或删除后，仍然可靠地检测水印文本（p ≤ 0.01），并且可以在35个字串上进行检测。对Alpaca-7B模型，我们实现了对对话响应的水印可行性研究，但因响应的 entropy 较低，检测难度较高，只有约25%的响应可以在p ≤ 0.01下检测，并且水印也较不抵抗某些自动重建攻击。<details>
<summary>Abstract</summary>
We propose a methodology for planting watermarks in text from an autoregressive language model that are robust to perturbations without changing the distribution over text up to a certain maximum generation budget. We generate watermarked text by mapping a sequence of random numbers -- which we compute using a randomized watermark key -- to a sample from the language model. To detect watermarked text, any party who knows the key can align the text to the random number sequence. We instantiate our watermark methodology with two sampling schemes: inverse transform sampling and exponential minimum sampling. We apply these watermarks to three language models -- OPT-1.3B, LLaMA-7B and Alpaca-7B -- to experimentally validate their statistical power and robustness to various paraphrasing attacks. Notably, for both the OPT-1.3B and LLaMA-7B models, we find we can reliably detect watermarked text ($p \leq 0.01$) from $35$ tokens even after corrupting between $40$-$50$\% of the tokens via random edits (i.e., substitutions, insertions or deletions). For the Alpaca-7B model, we conduct a case study on the feasibility of watermarking responses to typical user instructions. Due to the lower entropy of the responses, detection is more difficult: around $25\%$ of the responses -- whose median length is around $100$ tokens -- are detectable with $p \leq 0.01$, and the watermark is also less robust to certain automated paraphrasing attacks we implement.
</details>
<details>
<summary>摘要</summary>
我们提出了一种植入水印在文本中的方法，可以在文本中植入水印而不改变文本的分布，最多Generate watermarked text by mapping a sequence of random numbers -- which we compute using a randomized watermark key -- to a sample from the language model. To detect watermarked text, any party who knows the key can align the text to the random number sequence. We instantiate our watermark methodology with two sampling schemes: inverse transform sampling and exponential minimum sampling. We apply these watermarks to three language models -- OPT-1.3B, LLaMA-7B and Alpaca-7B -- to experimentally validate their statistical power and robustness to various paraphrasing attacks. Notably, for both the OPT-1.3B and LLaMA-7B models, we find we can reliably detect watermarked text ($p \leq 0.01$) from 35 tokens even after corrupting between 40-50% of the tokens via random edits (i.e., substitutions, insertions or deletions). For the Alpaca-7B model, we conduct a case study on the feasibility of watermarking responses to typical user instructions. Due to the lower entropy of the responses, detection is more difficult: around 25% of the responses -- whose median length is around 100 tokens -- are detectable with $p \leq 0.01$, and the watermark is also less robust to certain automated paraphrasing attacks we implement.
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-structure-of-cognitive-tasks-with-transfer-learning"><a href="#Evaluating-the-structure-of-cognitive-tasks-with-transfer-learning" class="headerlink" title="Evaluating the structure of cognitive tasks with transfer learning"></a>Evaluating the structure of cognitive tasks with transfer learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02408">http://arxiv.org/abs/2308.02408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bruno Aristimunha, Raphael Y. de Camargo, Walter H. Lopez Pinaya, Sylvain Chevallier, Alexandre Gramfort, Cedric Rommel<br>for:This study aims to investigate the transferability of deep learning representations between different EEG decoding tasks, with the goal of mitigating data scarcity in this setting.methods:The authors use state-of-the-art decoding models and conduct extensive experiments on two recently released EEG datasets, ERP CORE and M$^3$CV, containing over 140 subjects and 11 distinct cognitive tasks. They measure the transferability of learned representations by pre-training deep neural networks on one task and assessing their ability to decode subsequent tasks.results:The authors find that significant improvements in decoding performance can be obtained through transfer learning, with gains of up to 28% compared to the pure supervised approach. They also discover evidence that certain decoding paradigms elicit specific and narrow brain activities, while others benefit from pre-training on a broad range of representations. The transfer maps generated provide insights into the hierarchical relations between cognitive tasks, enhancing our understanding of how these tasks are connected from a neuroscientific standpoint.Here is the result in Simplified Chinese text:for: 这个研究的目的是 investigate EEG 解oding 中深度学习表示性的转移性，以减轻这种设定中的数据稀缺性。methods: 作者使用最新的解oding 模型，在两个最近发布的 EEG 数据集 ERP CORE 和 M$^3$CV 上进行了广泛的实验，这两个数据集包含了140名参与者和11种不同的认知任务。作者通过在一个任务上预训深度神经网络，然后评估它对接下来的任务的解码性能。results: 作者发现，通过转移学习可以获得显著的解码性能提升，相比约28% 的纯监督方法。他们还发现，certain 解码方法会引起特定和窄的脑活动，而其他方法则受益于在广泛的表示上进行预训。转移图生成的结果提供了认知任务之间的层次关系的了解，从神经科学的角度来看，这些任务之间存在着很强的相互关联。<details>
<summary>Abstract</summary>
Electroencephalography (EEG) decoding is a challenging task due to the limited availability of labelled data. While transfer learning is a promising technique to address this challenge, it assumes that transferable data domains and task are known, which is not the case in this setting. This study investigates the transferability of deep learning representations between different EEG decoding tasks. We conduct extensive experiments using state-of-the-art decoding models on two recently released EEG datasets, ERP CORE and M$^3$CV, containing over 140 subjects and 11 distinct cognitive tasks. We measure the transferability of learned representations by pre-training deep neural networks on one task and assessing their ability to decode subsequent tasks. Our experiments demonstrate that, even with linear probing transfer, significant improvements in decoding performance can be obtained, with gains of up to 28% compare with the pure supervised approach. Additionally, we discover evidence that certain decoding paradigms elicit specific and narrow brain activities, while others benefit from pre-training on a broad range of representations. By revealing which tasks transfer well and demonstrating the benefits of transfer learning for EEG decoding, our findings have practical implications for mitigating data scarcity in this setting. The transfer maps generated also provide insights into the hierarchical relations between cognitive tasks, hence enhancing our understanding of how these tasks are connected from a neuroscientific standpoint.
</details>
<details>
<summary>摘要</summary>
电子脑波图像分析（EEG）解oding是一项复杂的任务，因为标注数据的可用性有限。而转移学习是一种有前途的技术，但它假设了可以确定的数据领域和任务，这并不是这个设置的情况。这项研究探讨了EEG解oding任务中深度学习表示的传输性。我们进行了广泛的实验，使用最新的解oding模型在两个最近发布的EEG数据集上进行了测试，包括ERP CORE和M$^3$CV数据集，这两个数据集包含了140名参与者和11种不同的认知任务。我们测量了传输学习中获得的表示的传输性，通过在一个任务上预训练深度神经网络，并评估其对后续任务的解oding性能。我们的实验结果表明，即使使用线性探索转移，也可以获得显著的提高，与纯粹的监督方法相比，提高达28%。此外，我们发现了一些解oding方案会引起特定和窄频范围的大脑活动，而其他解oding方案则需要预训练在广泛的表示上。我们的发现可以实际地 mitigate数据缺乏问题在这个设置下，并且提供了转移学习在EEG解oding中的实际应用。此外，我们生成的转移图也提供了认知任务之间的层次关系的新的视角，从神经科学的角度来看，这有助于我们更好地理解这些任务之间的连接。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-algorithms-for-k-center-on-graphs"><a href="#Dynamic-algorithms-for-k-center-on-graphs" class="headerlink" title="Dynamic algorithms for k-center on graphs"></a>Dynamic algorithms for k-center on graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15557">http://arxiv.org/abs/2307.15557</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/swati1024/torrents">https://github.com/swati1024/torrents</a></li>
<li>paper_authors: Emilio Cruciani, Sebastian Forster, Gramoz Goranci, Yasamin Nazari, Antonis Skarlatos</li>
<li>for: 这paper是为了解决动态图中的$k$-center问题，目标是将输入分成$k$个集合，使最大距离任何数据点与最近的中心之间的距离最小化。</li>
<li>methods: 这paper使用了 deterministic decremental $(2+\epsilon)$-approximation algorithm和randomized incremental $(4+\epsilon)$-approximation algorithm，两者具有规化更新时间 $kn^{o(1)}$  для权重图。</li>
<li>results: 这paper得到了一个fully dynamic $(2+\epsilon)$-approximation algorithm，其更新时间与现有最佳上限 bounds for maintaining $(1+\epsilon)$-approximate single-source distances in graphs几乎相同。<details>
<summary>Abstract</summary>
In this paper we give the first efficient algorithms for the $k$-center problem on dynamic graphs undergoing edge updates. In this problem, the goal is to partition the input into $k$ sets by choosing $k$ centers such that the maximum distance from any data point to the closest center is minimized. It is known that it is NP-hard to get a better than $2$ approximation for this problem.   While in many applications the input may naturally be modeled as a graph, all prior works on $k$-center problem in dynamic settings are on metrics. In this paper, we give a deterministic decremental $(2+\epsilon)$-approximation algorithm and a randomized incremental $(4+\epsilon)$-approximation algorithm, both with amortized update time $kn^{o(1)}$ for weighted graphs. Moreover, we show a reduction that leads to a fully dynamic $(2+\epsilon)$-approximation algorithm for the $k$-center problem, with worst-case update time that is within a factor $k$ of the state-of-the-art upper bound for maintaining $(1+\epsilon)$-approximate single-source distances in graphs. Matching this bound is a natural goalpost because the approximate distances of each vertex to its center can be used to maintain a $(2+\epsilon)$-approximation of the graph diameter and the fastest known algorithms for such a diameter approximation also rely on maintaining approximate single-source distances.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提供了首个高效的动态图-$k$-中心问题的算法。在这个问题中，我们需要将输入分成$k$个集合，选择$k$个中心，使得最大的数据点与最近的中心之间的距离最小化。已知这是NP困难的，不能获得更好于2的近似值。在许多应用中，输入可以自然地表示为图，但所有先前的动态设置-$k$-中心问题的研究都是基于度量。在这篇论文中，我们提供了一个 deterministic 递减$(2+\epsilon)$-近似算法和一个随机化增量$(4+\epsilon)$-近似算法，两者具有 $kn^{o(1)}$ 的增量时间。此外，我们还证明了一种减少，导致一个完全动态$(2+\epsilon)$-近似算法，其最坏情况升级时间与当前最佳Upper bound  für maintaining $(1+\epsilon)$-近似单源距离在图上相同。这个目标是一个自然的目标，因为每个顶点与其中心的近似距离可以用来维护一个$(2+\epsilon)$-近似的图 diameters，而最快known algorithms for such a diameters approximation也 rely on maintaining approximate single-source distances。
</details></li>
</ul>
<hr>
<h2 id="On-the-Trade-off-Between-Efficiency-and-Precision-of-Neural-Abstraction"><a href="#On-the-Trade-off-Between-Efficiency-and-Precision-of-Neural-Abstraction" class="headerlink" title="On the Trade-off Between Efficiency and Precision of Neural Abstraction"></a>On the Trade-off Between Efficiency and Precision of Neural Abstraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15546">http://arxiv.org/abs/2307.15546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alec Edwards, Mirco Giacobbe, Alessandro Abate</li>
<li>for: 这个论文的目的是提出新的神经抽象方法，以便形式地近似复杂非线性动力模型。</li>
<li>methods: 这个论文使用了神经网络和正式推理 Synthesis 技术来生成神经抽象模型，并使用了不同类型的激活函数（如 sigmoid 函数）来生成不同类型的神经抽象模型。</li>
<li>results: 该论文通过实验表明，不同类型的神经抽象模型在精度和生成时间、安全验证时间等方面存在负面选择，同时也提出了一种基于 reachability 计算的方法来加速神经抽象模型的安全验证。<details>
<summary>Abstract</summary>
Neural abstractions have been recently introduced as formal approximations of complex, nonlinear dynamical models. They comprise a neural ODE and a certified upper bound on the error between the abstract neural network and the concrete dynamical model. So far neural abstractions have exclusively been obtained as neural networks consisting entirely of $ReLU$ activation functions, resulting in neural ODE models that have piecewise affine dynamics, and which can be equivalently interpreted as linear hybrid automata. In this work, we observe that the utility of an abstraction depends on its use: some scenarios might require coarse abstractions that are easier to analyse, whereas others might require more complex, refined abstractions. We therefore consider neural abstractions of alternative shapes, namely either piecewise constant or nonlinear non-polynomial (specifically, obtained via sigmoidal activations). We employ formal inductive synthesis procedures to generate neural abstractions that result in dynamical models with these semantics. Empirically, we demonstrate the trade-off that these different neural abstraction templates have vis-a-vis their precision and synthesis time, as well as the time required for their safety verification (done via reachability computation). We improve existing synthesis techniques to enable abstraction of higher-dimensional models, and additionally discuss the abstraction of complex neural ODEs to improve the efficiency of reachability analysis for these models.
</details>
<details>
<summary>摘要</summary>
In this work, we recognize that the utility of an abstraction depends on its use: some scenarios may require coarse abstractions that are easier to analyze, while others may require more complex, refined abstractions. We therefore consider neural abstractions of alternative shapes, such as piecewise constant or nonlinear non-polynomial (specifically, obtained via sigmoidal activations). We use formal inductive synthesis procedures to generate neural abstractions that result in dynamical models with these semantics.Empirically, we demonstrate the trade-off between these different neural abstraction templates in terms of their precision and synthesis time, as well as the time required for their safety verification (done via reachability computation). We improve existing synthesis techniques to enable abstraction of higher-dimensional models, and additionally discuss the abstraction of complex neural ODEs to improve the efficiency of reachability analysis for these models.
</details></li>
</ul>
<hr>
<h2 id="Beating-Backdoor-Attack-at-Its-Own-Game"><a href="#Beating-Backdoor-Attack-at-Its-Own-Game" class="headerlink" title="Beating Backdoor Attack at Its Own Game"></a>Beating Backdoor Attack at Its Own Game</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15539">http://arxiv.org/abs/2307.15539</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/damianliumin/non-adversarial_backdoor">https://github.com/damianliumin/non-adversarial_backdoor</a></li>
<li>paper_authors: Min Liu, Alberto Sangiovanni-Vincentelli, Xiangyu Yue</li>
<li>for: 防御深度神经网络（DNNs）免疫背门攻击，使其在恶势力数据上不受影响，但是在触发特定模式后会 manipulate 网络行为。</li>
<li>methods: 基于背门攻击的防御框架，通过在恶势力样本上注入非对抗性背门，对攻击者的背门在恶势力数据上产生限制，而无影响于干净数据。</li>
<li>results: 对多个 benchmark 和不同的架构进行了广泛的实验，结果显示，我们的方法可以 achieve state-of-the-art 防御效果，同时具有最低的干净数据性能下降。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) are vulnerable to backdoor attack, which does not affect the network's performance on clean data but would manipulate the network behavior once a trigger pattern is added. Existing defense methods have greatly reduced attack success rate, but their prediction accuracy on clean data still lags behind a clean model by a large margin. Inspired by the stealthiness and effectiveness of backdoor attack, we propose a simple but highly effective defense framework which injects non-adversarial backdoors targeting poisoned samples. Following the general steps in backdoor attack, we detect a small set of suspected samples and then apply a poisoning strategy to them. The non-adversarial backdoor, once triggered, suppresses the attacker's backdoor on poisoned data, but has limited influence on clean data. The defense can be carried out during data preprocessing, without any modification to the standard end-to-end training pipeline. We conduct extensive experiments on multiple benchmarks with different architectures and representative attacks. Results demonstrate that our method achieves state-of-the-art defense effectiveness with by far the lowest performance drop on clean data. Considering the surprising defense ability displayed by our framework, we call for more attention to utilizing backdoor for backdoor defense. Code is available at https://github.com/damianliumin/non-adversarial_backdoor.
</details>
<details>
<summary>摘要</summary>
Inspired by the stealthiness and effectiveness of backdoor attacks, we propose a simple yet highly effective defense framework that injects non-adversarial backdoors targeting poisoned samples. We follow the general steps of backdoor attacks and detect a small set of suspected samples, then apply a poisoning strategy to them. The non-adversarial backdoor, once triggered, suppresses the attacker's backdoor on poisoned data, but has little influence on clean data.Our defense can be carried out during data preprocessing, without modifying the standard end-to-end training pipeline. We conduct extensive experiments on multiple benchmarks with different architectures and representative attacks. The results show that our method achieves state-of-the-art defense effectiveness with the least amount of performance drop on clean data.Considering the surprising defense ability displayed by our framework, we suggest paying more attention to utilizing backdoors for backdoor defense. Our code is available at <https://github.com/damianliumin/non-adversarial_backdoor>.
</details></li>
</ul>
<hr>
<h2 id="RFID-Assisted-Indoor-Localization-Using-Hybrid-Wireless-Data-Fusion"><a href="#RFID-Assisted-Indoor-Localization-Using-Hybrid-Wireless-Data-Fusion" class="headerlink" title="RFID-Assisted Indoor Localization Using Hybrid Wireless Data Fusion"></a>RFID-Assisted Indoor Localization Using Hybrid Wireless Data Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02410">http://arxiv.org/abs/2308.02410</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abouzar Ghavami, Ali Abedi</li>
<li>for: 本研究旨在提出一种混合部分基地站indoor地位定位方法，用于跟踪indoor环境中的 объекты。</li>
<li>methods: 该方法使用开发的Radio Frequency Identification（RFID）跟踪设备和多种Internet of Things（IoT）无线通信协议，并将RFID标签安装在每个部分的边缘。RFID跟踪设备确定部分，并使用提posed的无线混合方法计算 objet的位置。</li>
<li>results: 实验结果验证了提出的分析结果，并verify了不同的Bluetooth、WiFi和ZigBee技术中的RSSI基地站位置估计。<details>
<summary>Abstract</summary>
Wireless localization is essential for tracking objects in indoor environments. Internet of Things (IoT) enables localization through its diverse wireless communication protocols. In this paper, a hybrid section-based indoor localization method using a developed Radio Frequency Identification (RFID) tracking device and multiple IoT wireless technologies is proposed. In order to reduce the cost of the RFID tags, the tags are installed only on the borders of each section. The RFID tracking device identifies the section, and the proposed wireless hybrid method finds the location of the object inside the section. The proposed hybrid method is analytically driven by linear location estimates obtained from different IoT wireless technologies. The experimental results using developed RFID tracking device and RSSI-based localization for Bluetooth, WiFi and ZigBee technologies verifies the analytical results.
</details>
<details>
<summary>摘要</summary>
无线地位是内部环境中物品跟踪的重要组成部分。互联网物品（IoT）为地位定位提供了多种无线通信协议。本文提出了一种混合部分基于RFID跟踪设备和多种IoT无线技术的hybrid地位定位方法。为了降低RFID标签的成本，标签仅在每个部分的边界安装。RFID跟踪设备识别 section，提案的无线混合方法在 section 中确定物品的位置。提议的混合方法由不同的IoT无线技术提供的线性位置估计驱动。实验结果使用开发的RFID跟踪设备和基于Bluetooth、WiFi和ZigBee技术的RSSI地位定位证明了分析结果。
</details></li>
</ul>
<hr>
<h2 id="The-Applicability-of-Federated-Learning-to-Official-Statistics"><a href="#The-Applicability-of-Federated-Learning-to-Official-Statistics" class="headerlink" title="The Applicability of Federated Learning to Official Statistics"></a>The Applicability of Federated Learning to Official Statistics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15503">http://arxiv.org/abs/2307.15503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joshua Stock, Oliver Hauke, Julius Weißmann, Hannes Federrath</li>
<li>for: 这个研究探讨了 Federated Learning（FL）在官方统计领域的潜力，并证明了 FL 模型的性能与中央学习方法相当。同时，通过保护数据持有者隐私，FL 可以开放更广泛的数据访问，最终提高官方统计。</li>
<li>methods: 该研究通过三个不同的应用场景进行了模拟，包括医疗保险数据集、细颗粒污染数据集和移动 ради oscillopection data set。所有这些数据集都来自于官方统计领域相关领域。我们提供了中央和 FL 算法的比较分析，并对每个模拟进行了详细的分析。在所有三个应用场景中，我们成功地使用 FL 模型来达到中央模型标准准的性能。</li>
<li>results: 我们的关键观察和其意义对于将这些模拟应用于实践中transfer的总结。我们结论出，FL 有可能在未来官方统计领域中emerge as a pivotal technology。<details>
<summary>Abstract</summary>
This work investigates the potential of Federated Learning (FL) for official statistics and shows how well the performance of FL models can keep up with centralized learning methods. At the same time, its utilization can safeguard the privacy of data holders, thus facilitating access to a broader range of data and ultimately enhancing official statistics. By simulating three different use cases, important insights on the applicability of the technology are gained. The use cases are based on a medical insurance data set, a fine dust pollution data set and a mobile radio coverage data set - all of which are from domains close to official statistics. We provide a detailed analysis of the results, including a comparison of centralized and FL algorithm performances for each simulation. In all three use cases, we were able to train models via FL which reach a performance very close to the centralized model benchmarks. Our key observations and their implications for transferring the simulations into practice are summarized. We arrive at the conclusion that FL has the potential to emerge as a pivotal technology in future use cases of official statistics.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "official statistics" is translated as "官方统计" (guāngrò tōngjī)* "Federated Learning" is translated as "联合学习" (liánhé xuéxí)* "centralized learning methods" is translated as "中央化学习方法" (zhōngyānghuà xuéxí fāngfa)* "data holders" is translated as "数据持有者" (shùdà jiāyuxiǎ)* "broader range of data" is translated as "更广泛的数据" (gèng guǎngkuò de shùdà)* "ultimately enhancing official statistics" is translated as "最终提高官方统计" (zui zhì tīgēng guāngrò tōngjī)* "simulations" is translated as "模拟" (móxī)* "use cases" is translated as "应用场景" (yìngyòu jiàngxiàng)* "medical insurance data set" is translated as "医疗保险数据集" (yīkuò bǎoxiǎo shùdiàn jī)* "fine dust pollution data set" is translated as "细排毒污染数据集" (xì purguī chóngwù róngshí shùdiàn jī)* "mobile radio coverage data set" is translated as "移动通信覆盖数据集" (yídòng tōngxìn fùhài shùdiàn jī)
</details></li>
</ul>
<hr>
<h2 id="AbDiffuser-Full-Atom-Generation-of-In-Vitro-Functioning-Antibodies"><a href="#AbDiffuser-Full-Atom-Generation-of-In-Vitro-Functioning-Antibodies" class="headerlink" title="AbDiffuser: Full-Atom Generation of In-Vitro Functioning Antibodies"></a>AbDiffuser: Full-Atom Generation of In-Vitro Functioning Antibodies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05027">http://arxiv.org/abs/2308.05027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karolis Martinkus, Jan Ludwiczak, Kyunghyun Cho, Wei-Ching Liang, Julien Lafrance-Vanasse, Isidro Hotzel, Arvind Rajpal, Yan Wu, Richard Bonneau, Vladimir Gligorijevic, Andreas Loukas</li>
<li>for: 这篇论文是为了设计高级别的抗体3D结构和序列而设计的一种equivariant和physics-informed扩散模型。</li>
<li>methods: 该模型基于一种新的蛋白质结构表示方法，使用了一种新的对称保持方案，并利用了强大的扩散约束来提高减噪过程。</li>
<li>results: 数值实验表明，AbDiffuser可以准确地生成与参照集的序列和结构性质匹配的抗体。实验室实验证明，所有16个HER2抗体的表达水平很高，选择的设计被证明是高级别紧绑物。<details>
<summary>Abstract</summary>
We introduce AbDiffuser, an equivariant and physics-informed diffusion model for the joint generation of antibody 3D structures and sequences. AbDiffuser is built on top of a new representation of protein structure, relies on a novel architecture for aligned proteins, and utilizes strong diffusion priors to improve the denoising process. Our approach improves protein diffusion by taking advantage of domain knowledge and physics-based constraints; handles sequence-length changes; and reduces memory complexity by an order of magnitude enabling backbone and side chain generation. We validate AbDiffuser in silico and in vitro. Numerical experiments showcase the ability of AbDiffuser to generate antibodies that closely track the sequence and structural properties of a reference set. Laboratory experiments confirm that all 16 HER2 antibodies discovered were expressed at high levels and that 57.1% of selected designs were tight binders.
</details>
<details>
<summary>摘要</summary>
我们介绍AbDiffuser，一个具有对称和物理知识散射模型，用于同时生成抗体三维结构和序列。AbDiffuser基于一个新的蛋白结构表示方法，利用一个新的保持蛋白结构的架构，并利用强大的散射假设来改善降噪过程。我们的方法可以改善蛋白 diffusion  by leveraging domain knowledge and physics-based constraints; 可以处理序列长度变化; 并可以降低内存复杂度，将蛋白质量减少一个级数。我们在silico和in vitro验证AbDiffuser。numeraical experiments showcase AbDiffuser的能力可以生成蛋白质量 closely track 引用集的序列和结构属性。实验室实验确认所有16个HER2抗体的发现都是高水平的表达，并且57.1%的选择设计是紧致系结者。
</details></li>
</ul>
<hr>
<h2 id="Curiosity-Driven-Reinforcement-Learning-based-Low-Level-Flight-Control"><a href="#Curiosity-Driven-Reinforcement-Learning-based-Low-Level-Flight-Control" class="headerlink" title="Curiosity-Driven Reinforcement Learning based Low-Level Flight Control"></a>Curiosity-Driven Reinforcement Learning based Low-Level Flight Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15724">http://arxiv.org/abs/2307.15724</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/a-ramezani/cdrl-l2fc_u_hcm">https://github.com/a-ramezani/cdrl-l2fc_u_hcm</a></li>
<li>paper_authors: Amir Ramezani Dooraki, Alexandros Iosifidis</li>
<li>for: 本研究旨在开发一种基于Curiosity的自主学习控制算法，以便 quadcopter 可以穿过障碍物并控制飞机的 Ya 方向指向所需的位置。</li>
<li>methods: 本研究使用了 prediction error 来实现 Curiosity 的新方法，并在 on-policy、off-policy、on-policy plus Curiosity 和提议算法上进行了测试。</li>
<li>results: results 表明，提议算法可以学习优质策略并最大化奖励，其他算法无法实现。<details>
<summary>Abstract</summary>
Curiosity is one of the main motives in many of the natural creatures with measurable levels of intelligence for exploration and, as a result, more efficient learning. It makes it possible for humans and many animals to explore efficiently by searching for being in states that make them surprised with the goal of learning more about what they do not know. As a result, while being curious, they learn better. In the machine learning literature, curiosity is mostly combined with reinforcement learning-based algorithms as an intrinsic reward. This work proposes an algorithm based on the drive of curiosity for autonomous learning to control by generating proper motor speeds from odometry data. The quadcopter controlled by our proposed algorithm can pass through obstacles while controlling the Yaw direction of the quad-copter toward the desired location. To achieve that, we also propose a new curiosity approach based on prediction error. We ran tests using on-policy, off-policy, on-policy plus curiosity, and the proposed algorithm and visualized the effect of curiosity in evolving exploration patterns. Results show the capability of the proposed algorithm to learn optimal policy and maximize reward where other algorithms fail to do so.
</details>
<details>
<summary>摘要</summary>
《尝试性》是许多自然生物智商水平可测量的动物主要动机之一，用于探索和学习。它使得人类和许多动物可以效率地探索，通过搜索被 surprisal 的目的来学习更多关于他们所不知道的事物。因此，在尝试性的情况下，他们的学习效果更好。在机器学习文献中，尝试性通常与回报学习基于的算法结合使用。这项工作提出一种基于尝试性的自主学习控制算法，通过生成适当的电机速度从遥感数据来控制四旋翼机。我们的提议的算法可以让四旋翼机通过障碍物而行，同时控制机器人的横坐标向所需的位置。为了实现这一目标，我们还提出了一种基于预测错误的新的尝试性方法。我们在使用在政策、离政策、在政策 plus 尝试性和我们的算法进行测试，并将尝试性的影响在演化探索模式中视觉化。结果显示我们的算法可以学习优化策略，并在其他算法无法做到的情况下获得最高奖励。
</details></li>
</ul>
<hr>
<h2 id="From-continuous-time-formulations-to-discretization-schemes-tensor-trains-and-robust-regression-for-BSDEs-and-parabolic-PDEs"><a href="#From-continuous-time-formulations-to-discretization-schemes-tensor-trains-and-robust-regression-for-BSDEs-and-parabolic-PDEs" class="headerlink" title="From continuous-time formulations to discretization schemes: tensor trains and robust regression for BSDEs and parabolic PDEs"></a>From continuous-time formulations to discretization schemes: tensor trains and robust regression for BSDEs and parabolic PDEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15496">http://arxiv.org/abs/2307.15496</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lorenzrichter/PDE-backward-solver">https://github.com/lorenzrichter/PDE-backward-solver</a></li>
<li>paper_authors: Lorenz Richter, Leon Sallandt, Nikolas Nüsken</li>
<li>for: 解决高维纬度的partial differential equations（PDEs）数学应用中的困难问题，特别是谱系束问题。</li>
<li>methods: 使用Monte Carlo方法和变分方法，并使用神经网络来近似函数。</li>
<li>results: 提出了一种基于tensor train的新方法，可以在高维纬度下实现高精度和高效计算。该方法可以在迭代计算中实现一个有利的平衡 между精度和计算效率。<details>
<summary>Abstract</summary>
The numerical approximation of partial differential equations (PDEs) poses formidable challenges in high dimensions since classical grid-based methods suffer from the so-called curse of dimensionality. Recent attempts rely on a combination of Monte Carlo methods and variational formulations, using neural networks for function approximation. Extending previous work (Richter et al., 2021), we argue that tensor trains provide an appealing framework for parabolic PDEs: The combination of reformulations in terms of backward stochastic differential equations and regression-type methods holds the promise of leveraging latent low-rank structures, enabling both compression and efficient computation. Emphasizing a continuous-time viewpoint, we develop iterative schemes, which differ in terms of computational efficiency and robustness. We demonstrate both theoretically and numerically that our methods can achieve a favorable trade-off between accuracy and computational efficiency. While previous methods have been either accurate or fast, we have identified a novel numerical strategy that can often combine both of these aspects.
</details>
<details>
<summary>摘要</summary>
“对于高维度的偏微分方程（PDEs） numerically 的推估 pose formidable 挑战，因为经典的格子基方法受到称为“对维度的诅咒”的限制。 latest attempts  rely on  комби��ination of Monte Carlo 方法和量�ltiple�formulations，使用神经网络 для函数�approximation。 extending previous work (Richter et al., 2021)， we argue that tensor trains provide an appealing framework for parabolic PDEs：the combination of reformulations in terms of backward stochastic differential equations and regression-type methods can leverage latent low-rank structures, enabling both compression and efficient computation。 emphasizing a continuous-time viewpoint， we develop iterative schemes， which differ in terms of computational efficiency and robustness。 we demonstrate both theoretically and numerically that our methods can achieve a favorable trade-off between accuracy and computational efficiency。 while previous methods have been either accurate or fast， we have identified a novel numerical strategy that can often combine both of these aspects。”Note: The translation is in Simplified Chinese, which is one of the two standard varieties of Chinese. The other variety is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="FeedbackLogs-Recording-and-Incorporating-Stakeholder-Feedback-into-Machine-Learning-Pipelines"><a href="#FeedbackLogs-Recording-and-Incorporating-Stakeholder-Feedback-into-Machine-Learning-Pipelines" class="headerlink" title="FeedbackLogs: Recording and Incorporating Stakeholder Feedback into Machine Learning Pipelines"></a>FeedbackLogs: Recording and Incorporating Stakeholder Feedback into Machine Learning Pipelines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15475">http://arxiv.org/abs/2307.15475</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew Barker, Emma Kallina, Dhananjay Ashok, Katherine M. Collins, Ashley Casovan, Adrian Weller, Ameet Talwalkar, Valerie Chen, Umang Bhatt</li>
<li>for: 这篇论文是为了描述一种用于记录多个参与者反馈的机器学习（ML）管道的方法。</li>
<li>methods: 该方法使用FeedbackLogs，一种补充现有ML管道文档的添加物，来跟踪多个参与者的反馈输入。每个日志记录了反馈收集过程中重要的细节，反馈自身，以及如何将反馈更新到ML管道中。</li>
<li>results: 该论文提出了一种形式化的反馈收集过程，并提供了具体的应用场景，例如使用FeedbackLogs作为算法审核证据和记录参与者反馈更新的工具。<details>
<summary>Abstract</summary>
Even though machine learning (ML) pipelines affect an increasing array of stakeholders, there is little work on how input from stakeholders is recorded and incorporated. We propose FeedbackLogs, addenda to existing documentation of ML pipelines, to track the input of multiple stakeholders. Each log records important details about the feedback collection process, the feedback itself, and how the feedback is used to update the ML pipeline. In this paper, we introduce and formalise a process for collecting a FeedbackLog. We also provide concrete use cases where FeedbackLogs can be employed as evidence for algorithmic auditing and as a tool to record updates based on stakeholder feedback.
</details>
<details>
<summary>摘要</summary>
即使机器学习（ML）管道影响到越来越多的各种利益相关者，仍有很少关于如何记录和汇报利益各方的工作。我们提议使用FeedbackLogs，作为现有的ML管道文档的补充，来跟踪多个利益各方的输入。每个日志记录了反馈收集过程中重要的细节，反馈本身，以及如何使用反馈更新ML管道。在这篇论文中，我们介绍了收集FeedbackLog的过程，并提供了具体的应用场景，如算法审核的证据和利益各方反馈更新的工具。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Noisy-Label-Learning-in-Real-world-Annotation-Scenarios-from-the-Noise-type-Perspective"><a href="#Rethinking-Noisy-Label-Learning-in-Real-world-Annotation-Scenarios-from-the-Noise-type-Perspective" class="headerlink" title="Rethinking Noisy Label Learning in Real-world Annotation Scenarios from the Noise-type Perspective"></a>Rethinking Noisy Label Learning in Real-world Annotation Scenarios from the Noise-type Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16889">http://arxiv.org/abs/2307.16889</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fuxiailab/protosemi">https://github.com/fuxiailab/protosemi</a></li>
<li>paper_authors: Renyu Zhu, Haoyu Liu, Runze Wu, Minmin Lin, Tangjie Lv, Changjie Fan, Haobo Wang</li>
<li>for: 本研究强调学习受损标签的问题，即在实际标注场景中的噪声问题。噪声可分为两类：事实噪声和抽象噪声。</li>
<li>methods: 我们提出了一种基于样本选择的噪声标签学习方法，称为Proto-semi。Proto-semi首先将所有样本分为确定的和不确定的数据集via warm-up。然后，利用确定数据集，构建prototype vector来捕捉类征。接着，计算不确定样本与prototype vector之间的距离，以便噪声分类。根据这些距离，标签是否需要更正或保留，从而实现标签的更新。最后，我们引入了一种半supervised学习方法来增强训练。</li>
<li>results: 在一个实际标注数据集上进行了实验，证明Proto-semi在面临受损标签问题时具有强大的鲁棒性。同时，我们的prototype-based repartitioning策略被证明可以有效地减少噪声标签的不良影响。我们的代码和数据可以在<a target="_blank" rel="noopener" href="https://github.com/fuxiAIlab/ProtoSemi%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/fuxiAIlab/ProtoSemi上下载。</a><details>
<summary>Abstract</summary>
In this paper, we investigate the problem of learning with noisy labels in real-world annotation scenarios, where noise can be categorized into two types: factual noise and ambiguity noise. To better distinguish these noise types and utilize their semantics, we propose a novel sample selection-based approach for noisy label learning, called Proto-semi. Proto-semi initially divides all samples into the confident and unconfident datasets via warm-up. By leveraging the confident dataset, prototype vectors are constructed to capture class characteristics. Subsequently, the distances between the unconfident samples and the prototype vectors are calculated to facilitate noise classification. Based on these distances, the labels are either corrected or retained, resulting in the refinement of the confident and unconfident datasets. Finally, we introduce a semi-supervised learning method to enhance training. Empirical evaluations on a real-world annotated dataset substantiate the robustness of Proto-semi in handling the problem of learning from noisy labels. Meanwhile, the prototype-based repartitioning strategy is shown to be effective in mitigating the adverse impact of label noise. Our code and data are available at https://github.com/fuxiAIlab/ProtoSemi.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了在实际标注场景中学习含噪标签的问题，噪声可以分为两类：事实噪声和模糊噪声。为了更好地 отли奇这两种噪声的 semantics，我们提出了一种基于样本选择的含噪标签学习方法，称为Proto-semi。Proto-semi首先将所有样本分为信任度高和低两个集合 via warm-up。通过利用信任度高的集合，我们构建了类特征的原型向量。然后，我们计算了不确定样本和原型向量之间的距离，以便噪声分类。根据这些距离，我们是否更正或保留标签，从而实现了标签的纠正和提升。最后，我们引入了半supervised学习方法，以提高训练的效果。实际评估表明，Proto-semi在处理含噪标签学习问题中具有坚定的可靠性。同时，我们的prototype-based重新分配策略能够减少噪声标签的负面影响。我们的代码和数据可以在https://github.com/fuxiAIlab/ProtoSemi上获取。
</details></li>
</ul>
<hr>
<h2 id="Testing-the-Depth-of-ChatGPT’s-Comprehension-via-Cross-Modal-Tasks-Based-on-ASCII-Art-GPT3-5’s-Abilities-in-Regard-to-Recognizing-and-Generating-ASCII-Art-Are-Not-Totally-Lacking"><a href="#Testing-the-Depth-of-ChatGPT’s-Comprehension-via-Cross-Modal-Tasks-Based-on-ASCII-Art-GPT3-5’s-Abilities-in-Regard-to-Recognizing-and-Generating-ASCII-Art-Are-Not-Totally-Lacking" class="headerlink" title="Testing the Depth of ChatGPT’s Comprehension via Cross-Modal Tasks Based on ASCII-Art: GPT3.5’s Abilities in Regard to Recognizing and Generating ASCII-Art Are Not Totally Lacking"></a>Testing the Depth of ChatGPT’s Comprehension via Cross-Modal Tasks Based on ASCII-Art: GPT3.5’s Abilities in Regard to Recognizing and Generating ASCII-Art Are Not Totally Lacking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16806">http://arxiv.org/abs/2307.16806</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Bayani</li>
<li>for: 本研究旨在检验GPT3.5模型在视觉任务中的能力，包括图像识别、图像分解和图像生成等。</li>
<li>methods: 该研究使用GPT3.5模型进行图像处理，包括图像识别、图像分解和图像生成等任务。</li>
<li>results: 研究发现GPT3.5模型在视觉任务中表现出色，能够准确地识别图像，并且在图像分解和图像生成任务中也表现出优异。<details>
<summary>Abstract</summary>
Over the eight months since its release, ChatGPT and its underlying model, GPT3.5, have garnered massive attention, due to their potent mix of capability and accessibility. While a niche-industry of papers have emerged examining the scope of capabilities these models possess, the information fed to and extracted from these networks has been either natural language text or stylized, code-like language. Drawing inspiration from the prowess we expect a truly human-level intelligent agent to have across multiple signal modalities, in this work we examine GPT3.5's aptitude for visual tasks, where the inputs feature content provided as ASCII-art without overt distillation into a lingual summary. We conduct experiments analyzing the model's performance on image recognition tasks after various transforms typical in visual settings, trials investigating knowledge of image parts, and tasks covering image generation.
</details>
<details>
<summary>摘要</summary>
在过去的八个月里，ChatGPT和它的基础模型GPT3.5已经吸引了巨大的关注，因为它们具有杰出的能力和可访问性。虽然一些专业人员已经发表了许多关于这些模型的论文，但是输入和EXTRACTED FROM这些网络的信息都是自然语言文本或者带有编程语言的样式的。 Drawing inspiration from the prowess we expect a truly human-level intelligent agent to have across multiple signal modalities，在这个工作中我们 examines GPT3.5的适用性在视觉任务中，其输入内容为ASCII艺术而没有明显的概括。我们进行了对模型在图像识别任务、图像部分知识和图像生成任务中的表现的实验。
</details></li>
</ul>
<hr>
<h2 id="LUCID-GAN-Conditional-Generative-Models-to-Locate-Unfairness"><a href="#LUCID-GAN-Conditional-Generative-Models-to-Locate-Unfairness" class="headerlink" title="LUCID-GAN: Conditional Generative Models to Locate Unfairness"></a>LUCID-GAN: Conditional Generative Models to Locate Unfairness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15466">http://arxiv.org/abs/2307.15466</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/integrated-intelligence-lab/canonical_sets">https://github.com/integrated-intelligence-lab/canonical_sets</a></li>
<li>paper_authors: Andres Algaba, Carmen Mazijn, Carina Prunkl, Jan Danckaert, Vincent Ginis</li>
<li>for: 检测黑盒模型中的不正当偏见</li>
<li>methods: 使用LUCID-GAN生成潜在Input，通过描述模型的机制来暴露不正当偏见</li>
<li>results: 在UCI Adult和COMPAS数据集上实验表明，LUCID-GAN可以无需访问训练数据，检测黑盒模型中的不正当偏见<details>
<summary>Abstract</summary>
Most group fairness notions detect unethical biases by computing statistical parity metrics on a model's output. However, this approach suffers from several shortcomings, such as philosophical disagreement, mutual incompatibility, and lack of interpretability. These shortcomings have spurred the research on complementary bias detection methods that offer additional transparency into the sources of discrimination and are agnostic towards an a priori decision on the definition of fairness and choice of protected features. A recent proposal in this direction is LUCID (Locating Unfairness through Canonical Inverse Design), where canonical sets are generated by performing gradient descent on the input space, revealing a model's desired input given a preferred output. This information about the model's mechanisms, i.e., which feature values are essential to obtain specific outputs, allows exposing potential unethical biases in its internal logic. Here, we present LUCID-GAN, which generates canonical inputs via a conditional generative model instead of gradient-based inverse design. LUCID-GAN has several benefits, including that it applies to non-differentiable models, ensures that canonical sets consist of realistic inputs, and allows to assess proxy and intersectional discrimination. We empirically evaluate LUCID-GAN on the UCI Adult and COMPAS data sets and show that it allows for detecting unethical biases in black-box models without requiring access to the training data.
</details>
<details>
<summary>摘要</summary>
大多数群衡不偏观方法都是通过计算统计平衡指标来检测不正义的偏见。但这种方法存在多个缺点，如哲学意见不一致、不相容性和没有解释力。这些缺点导致了对于补充的偏见检测方法的研究，这些方法可以提供更多的透明度，了解歧视的来源，并且不需要先知定义公平和保护特征。一个最近的提案是LUCID（找到不公的方法），它使用了梯度下降来生成可能的输入，以揭露模型对特定输出的需要的输入。这个信息可以暴露模型内部的不公义偏见。在这篇文章中，我们提出了LUCID-GAN，它使用了 conditional generative model 生成可能的输入，而不是使用梯度下降的 inverse design。LUCID-GAN 有多个优点，包括可以应用于非微分的模型、 canonical sets 会包含实际的输入和可以评估代表和交叉性歧视。我们在 UCI 成人和 COMPAS 数据集上进行了实验，评估 LUCID-GAN 可以检测黑盒模型中的不公义偏见，而不需要存取训练数据。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-machine-learning-shock-capturing-technique-for-high-order-solvers"><a href="#Unsupervised-machine-learning-shock-capturing-technique-for-high-order-solvers" class="headerlink" title="Unsupervised machine-learning shock-capturing technique for high-order solvers"></a>Unsupervised machine-learning shock-capturing technique for high-order solvers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00086">http://arxiv.org/abs/2308.00086</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrés Mateo-Gabín, Kenza Tlales, Eusebio Valero, Esteban Ferrer, Gonzalo Rubio</li>
<li>for: 这篇论文的目的是提出一种基于 Gaussian Mixture Models（GMM）的无监督机器学习捕捉冲击算法，以提高高精度 Computational Fluid Dynamics（CFD）代码的稳定性和效率。</li>
<li>methods: 这篇论文使用了 GMM 测器，该测器可以无需受训练数据来捕捉冲击，并且在多种测试 caso 中展现了优异的性能，与现有的竞争方案相比。</li>
<li>results: 这篇论文的研究结果显示，GMM 测器在高 Reynolds 数的冲击测试 caso 中具有同等的效果，与 fine-tuned 的竞争方案相比，而且可以在复杂的 geometries 和多种流场配置中运作。<details>
<summary>Abstract</summary>
We present a novel unsupervised machine learning shock capturing algorithm based on Gaussian Mixture Models (GMMs). The proposed GMM sensor demonstrates remarkable accuracy in detecting shocks and is robust across diverse test cases without the need for parameter tuning. We compare the GMM-based sensor with state-of-the-art alternatives. All methods are integrated into a high-order compressible discontinuous Galerkin solver where artificial viscosity can be modulated to capture shocks. Supersonic test cases, including high Reynolds numbers, showcase the sensor's performance, demonstrating the same effectiveness as fine-tuned state-of-the-art sensors. %The nodal DG aproach allows for potential applications in sub-cell flux-differencing formulations, supersonic feature detection, and mesh refinement. The adaptive nature and ability to function without extensive training datasets make this GMM-based sensor suitable for complex geometries and varied flow configurations. Our study reveals the potential of unsupervised machine learning methods, exemplified by the GMM sensor, to improve the robustness and efficiency of advanced CFD codes.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的无监督机器学习冲击捕捉算法基于泛函混合模型（GMM）。我们的GMM感测器在捕捉冲击方面表现出了很好的准确性，并且在多种测试 случа件中具有很好的稳定性，无需进行参数调整。我们比较了GMM基于感测器与当前的状态艺术方法。所有方法都被 integrate into a high-order可 compressible discontinuous Galerkin solver中，其中人工粘性可以用来捕捉冲击。supersonic test cases, including high Reynolds numbers, showcase the sensor's performance, demonstrating the same effectiveness as fine-tuned state-of-the-art sensors。%The nodal DG approach allows for potential applications in sub-cell flux-differencing formulations, supersonic feature detection, and mesh refinement。我们的研究表明了无监督机器学习方法，例如GMM感测器，可以提高高级CFD代码的稳定性和效率。
</details></li>
</ul>
<hr>
<h2 id="Worrisome-Properties-of-Neural-Network-Controllers-and-Their-Symbolic-Representations"><a href="#Worrisome-Properties-of-Neural-Network-Controllers-and-Their-Symbolic-Representations" class="headerlink" title="Worrisome Properties of Neural Network Controllers and Their Symbolic Representations"></a>Worrisome Properties of Neural Network Controllers and Their Symbolic Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15456">http://arxiv.org/abs/2307.15456</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mimuw-rl/worrisome-nn">https://github.com/mimuw-rl/worrisome-nn</a></li>
<li>paper_authors: Jacek Cyranka, Kevin E M Church, Jean-Philippe Lessard</li>
<li>for: 研究控制器的稳定性问题在简单的强化学习 benchmark 问题中。</li>
<li>methods: 使用神经网络控制器和其低神经和符号抽象。</li>
<li>results: 发现控制器可以生成许多 persistently 低返回解决方案，这是一个非常不 desireable 的性能特性，可以被敌人轻松地利用。  simpler controllers 更易生成 persistently bad solutions。 提供了一种系统的Robustness study algorithm，并证明存在 persistently solutions 和，在某些情况下， periodic orbits。<details>
<summary>Abstract</summary>
We raise concerns about controllers' robustness in simple reinforcement learning benchmark problems. We focus on neural network controllers and their low neuron and symbolic abstractions. A typical controller reaching high mean return values still generates an abundance of persistent low-return solutions, which is a highly undesirable property, easily exploitable by an adversary. We find that the simpler controllers admit more persistent bad solutions. We provide an algorithm for a systematic robustness study and prove existence of persistent solutions and, in some cases, periodic orbits, using a computer-assisted proof methodology.
</details>
<details>
<summary>摘要</summary>
我们对控制器的稳定性表示关切，尤其是在简单的强化学习问题中。我们主要关注神经网络控制器的低神经和符号抽象。一个typical控制器可以 дости得高mean return值，但仍然生成丰富的持续性低返回解，这是一个非常不愿意的属性，易于被敌人利用。我们发现简单的控制器承认更多持续性坏解。我们提供了一个系统atic robustness研究的算法，并证明存在持续解和，在一些情况下，周期性orbits，使用了计算机助理的证明方法。
</details></li>
</ul>
<hr>
<h2 id="Autonomous-Payload-Thermal-Control"><a href="#Autonomous-Payload-Thermal-Control" class="headerlink" title="Autonomous Payload Thermal Control"></a>Autonomous Payload Thermal Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15438">http://arxiv.org/abs/2307.15438</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alejandro D. Mousist</li>
<li>for: 这篇研究是为了解决小型卫星中热控制系统的挑战，特别是当电子元件和科学仪器聚集在一起时，热 dissipation 问题的影响。</li>
<li>methods: 这篇研究使用了深度强化学习框架，包括 Soft Actor-Critic 算法，以学习在小型卫星上的热控制策略。</li>
<li>results: 实验结果显示，提案的框架能够学习控制负载处理功率，以维持 payload 的热度在操作范围内。这些结果表明，该框架可以辅助传统热控制系统，并且可以在小型卫星中提供更好的热控制功能。<details>
<summary>Abstract</summary>
In small satellites there is less room for heat control equipment, scientific instruments, and electronic components. Furthermore, the near proximity of the electronics makes power dissipation difficult, with the risk of not being able to control the temperature appropriately, reducing component lifetime and mission performance. To address this challenge, taking advantage of the advent of increasing intelligence on board satellites, a deep reinforcement learning based framework that uses Soft Actor-Critic algorithm is proposed for learning the thermal control policy onboard. The framework is evaluated both in a naive simulated environment and in a real space edge processing computer that will be shipped in the future IMAGIN-e mission and hosted in the ISS. The experiment results show that the proposed framework is able to learn to control the payload processing power to maintain the temperature under operational ranges, complementing traditional thermal control systems.
</details>
<details>
<summary>摘要</summary>
在小卫星中，due to limited space, there is less room for heat control equipment, scientific instruments, and electronic components. Furthermore, the proximity of electronics makes power dissipation difficult, which can lead to inadequate temperature control, reducing component lifetime and mission performance. To address this challenge, a deep reinforcement learning-based framework that uses the Soft Actor-Critic algorithm is proposed for learning the thermal control policy onboard. The framework is evaluated in both a simulated environment and a real space edge processing computer that will be used in the future IMAGIN-e mission and hosted in the ISS. Experiment results show that the proposed framework is able to learn to control the payload processing power to maintain the temperature within operational ranges, complementing traditional thermal control systems.
</details></li>
</ul>
<hr>
<h2 id="Improvable-Gap-Balancing-for-Multi-Task-Learning"><a href="#Improvable-Gap-Balancing-for-Multi-Task-Learning" class="headerlink" title="Improvable Gap Balancing for Multi-Task Learning"></a>Improvable Gap Balancing for Multi-Task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15429">http://arxiv.org/abs/2307.15429</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yanqidai/igb4mtl">https://github.com/yanqidai/igb4mtl</a></li>
<li>paper_authors: Yanqi Dai, Nanyi Fei, Zhiwu Lu</li>
<li>for: 提高多任务学习（MTL）中性能的研究，尤其是在使用梯度均衡而不是损失均衡。</li>
<li>methods: 提出了两种新的改进可能均衡算法（IGB），其中一种使用了简单的规则，另一种则是首次在MTL中应用深度强化学习。两种算法均 dynamically assigns任务权重来实现可 improvable gap 均衡。</li>
<li>results: 通过对两个 benchmark 数据集进行广泛的实验表明，我们的 IGB 算法在通过损失均衡和梯度均衡进行改进后，能够达到最佳性能，并且在两种算法结合使用情况下，能够实现进一步的改进。代码可以在 <a target="_blank" rel="noopener" href="https://github.com/YanqiDai/IGB4MTL">https://github.com/YanqiDai/IGB4MTL</a> 上获取。<details>
<summary>Abstract</summary>
In multi-task learning (MTL), gradient balancing has recently attracted more research interest than loss balancing since it often leads to better performance. However, loss balancing is much more efficient than gradient balancing, and thus it is still worth further exploration in MTL. Note that prior studies typically ignore that there exist varying improvable gaps across multiple tasks, where the improvable gap per task is defined as the distance between the current training progress and desired final training progress. Therefore, after loss balancing, the performance imbalance still arises in many cases. In this paper, following the loss balancing framework, we propose two novel improvable gap balancing (IGB) algorithms for MTL: one takes a simple heuristic, and the other (for the first time) deploys deep reinforcement learning for MTL. Particularly, instead of directly balancing the losses in MTL, both algorithms choose to dynamically assign task weights for improvable gap balancing. Moreover, we combine IGB and gradient balancing to show the complementarity between the two types of algorithms. Extensive experiments on two benchmark datasets demonstrate that our IGB algorithms lead to the best results in MTL via loss balancing and achieve further improvements when combined with gradient balancing. Code is available at https://github.com/YanqiDai/IGB4MTL.
</details>
<details>
<summary>摘要</summary>
在多任务学习（MTL）中，梯度均衡在最近几年内吸引了更多的研究兴趣，因为它经常会导致更好的性能。然而，损失均衡是梯度均衡的更加有效的方法，因此仍然值得进一步的探索。注意，先前的研究通常忽略了多个任务之间存在的变化可以提高的差距，其中每个任务的改进差距是指从当前的训练进度到期望的最终训练进度之间的距离。因此，在许多情况下， после损失均衡，性能差距仍然存在。在这篇论文中，我们采用损失均衡框架，提出了两种新的可 improvable gap 均衡（IGB）算法 для MTL：一个使用简单的规则，另一个（在MTL中首次）使用深度强化学习。特别是，不直接在 MTL 中平衡损失，IGB 算法选择动态分配任务权重以进行 improvable gap 均衡。此外，我们将 IGB 和梯度均衡结合使用，以示它们之间的补充性。我们在两个标准数据集上进行了广泛的实验，结果表明，我们的 IGB 算法在 MTL 中通过损失均衡得到最佳结果，并在与梯度均衡结合使用时得到进一步的改进。代码可以在 <https://github.com/YanqiDai/IGB4MTL> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Implicit-neural-representation-for-change-detection"><a href="#Implicit-neural-representation-for-change-detection" class="headerlink" title="Implicit neural representation for change detection"></a>Implicit neural representation for change detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15428">http://arxiv.org/abs/2307.15428</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Naylor, Diego Di Carlo, Arianna Traviglia, Makoto Yamada, Marco Fiorucci<br>for:这种论文的目的是检测在两个不同时间在同一地区获取的空气borne LiDAR点云之间的变化。methods:这种方法包括两个组件：神经场（NF） для连续形态重建和加aussian Mixture Model（GMM） для分类变化。NF提供了不固定格式的表示方式，可以编码不匹配的时间支持，并可以正则化以增加高频环境和减少噪声。results:这种方法在一个 benchmark 数据集上达到了10%的提升，并在实际应用中证明了 illegal excavation（掘寻）的发现。<details>
<summary>Abstract</summary>
Detecting changes that occurred in a pair of 3D airborne LiDAR point clouds, acquired at two different times over the same geographical area, is a challenging task because of unmatching spatial supports and acquisition system noise. Most recent attempts to detect changes on point clouds are based on supervised methods, which require large labelled data unavailable in real-world applications. To address these issues, we propose an unsupervised approach that comprises two components: Neural Field (NF) for continuous shape reconstruction and a Gaussian Mixture Model for categorising changes. NF offer a grid-agnostic representation to encode bi-temporal point clouds with unmatched spatial support that can be regularised to increase high-frequency details and reduce noise. The reconstructions at each timestamp are compared at arbitrary spatial scales, leading to a significant increase in detection capabilities. We apply our method to a benchmark dataset of simulated LiDAR point clouds for urban sprawling. The dataset offers different challenging scenarios with different resolutions, input modalities and noise levels, allowing a multi-scenario comparison of our method with the current state-of-the-art. We boast the previous methods on this dataset by a 10% margin in intersection over union metric. In addition, we apply our methods to a real-world scenario to identify illegal excavation (looting) of archaeological sites and confirm that they match findings from field experts.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。<</SYS>>检测两个3D空中LiDAR点云在不同时间点上的变化是一项具有棘手和获取系统噪声的挑战。大多数最新的变化检测方法基于有监督的方法，需要庞大的标注数据，这些数据在实际应用中不可获得。为解决这些问题，我们提出了一种无监督方法，包括两个组件：神经场（NF） для连续形态重建和 Gaussian Mixture Model（GMM） для分类变化。NF提供了不受格子约束的表示方式，可以编码不匹配的时间点云，并可以通过增强高频率细节和减少噪声来规范。在每个时间戳中比较重建的结果，可以在自由的空间缩放比例上实现显著提高检测能力。我们在一个 simulated LiDAR点云 benchmark dataset 上应用了我们的方法，该dataset 包括不同的挑战场景，例如不同的分辨率、输入模式和噪声水平。我们在这个dataset上比较了我们的方法与当前状态之最先进的方法，并在 intersection over union 指标上提高了10%的margin。此外，我们还应用了我们的方法到一个真实世界的场景，以检测考古遗产泥棒（looting），并证明与场地专家的发现相匹配。
</details></li>
</ul>
<hr>
<h2 id="Deep-Generative-Models-Synthetic-Tabular-Data-and-Differential-Privacy-An-Overview-and-Synthesis"><a href="#Deep-Generative-Models-Synthetic-Tabular-Data-and-Differential-Privacy-An-Overview-and-Synthesis" class="headerlink" title="Deep Generative Models, Synthetic Tabular Data, and Differential Privacy: An Overview and Synthesis"></a>Deep Generative Models, Synthetic Tabular Data, and Differential Privacy: An Overview and Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15424">http://arxiv.org/abs/2307.15424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Conor Hassan, Robert Salomone, Kerrie Mengersen</li>
<li>for: 本文提供了深入的对深度生成模型在生成数据领域的最新发展，特意关注了表格数据。</li>
<li>methods: 本文使用深度生成模型来生成synthetic数据，并讨论了使用这些模型的优势和相关挑战。</li>
<li>results: 本文详细介绍了使用深度生成模型生成表格数据的方法，并讨论了数据normalization、隐私问题和模型评估等问题。<details>
<summary>Abstract</summary>
This article provides a comprehensive synthesis of the recent developments in synthetic data generation via deep generative models, focusing on tabular datasets. We specifically outline the importance of synthetic data generation in the context of privacy-sensitive data. Additionally, we highlight the advantages of using deep generative models over other methods and provide a detailed explanation of the underlying concepts, including unsupervised learning, neural networks, and generative models. The paper covers the challenges and considerations involved in using deep generative models for tabular datasets, such as data normalization, privacy concerns, and model evaluation. This review provides a valuable resource for researchers and practitioners interested in synthetic data generation and its applications.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:这篇文章提供了对深度生成模型在生成数据方面的最新发展的全面概述，特色是对表格数据的应用。我们特别强调了在隐私敏感数据的场景下使用深度生成模型的重要性。此外，我们还提到了使用深度生成模型的优势，以及对这些模型的下降学习、神经网络和生成模型的解释。文章还讨论了使用深度生成模型处理表格数据时的挑战和考虑因素，如数据normalization、隐私问题和模型评估。这篇文章对研究人员和实践者感兴趣的人来说是一个有价值的资源。
</details></li>
</ul>
<hr>
<h2 id="Is-One-Epoch-All-You-Need-For-Multi-Fidelity-Hyperparameter-Optimization"><a href="#Is-One-Epoch-All-You-Need-For-Multi-Fidelity-Hyperparameter-Optimization" class="headerlink" title="Is One Epoch All You Need For Multi-Fidelity Hyperparameter Optimization?"></a>Is One Epoch All You Need For Multi-Fidelity Hyperparameter Optimization?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15422">http://arxiv.org/abs/2307.15422</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deephyper/benchmark">https://github.com/deephyper/benchmark</a></li>
<li>paper_authors: Romain Egele, Isabelle Guyon, Yixuan Sun, Prasanna Balaprakash</li>
<li>for: 本研究旨在提高机器学习模型的优化，使用多模型精度水平（Multi-fidelity HPO）来减少计算成本。</li>
<li>methods: 本研究使用了多种代表性的多模型精度水平优化方法，并与简单的基准线进行比较。基准线是在训练一个循环后，保留Top-K个模型，然后进行进一步训练选择最佳模型。</li>
<li>results: 结果显示，基准线 surprisingly achieved similar results to its counterparts, while requiring an order of magnitude less computation。分析学习曲线表明，存在一些主导的学习曲线，这解释了基准线的成功。这表明研究人员应该（1）总是使用建议的基准线和（2）拓宽多模型精度水平优化的 benchmarks 来包括更复杂的情况。<details>
<summary>Abstract</summary>
Hyperparameter optimization (HPO) is crucial for fine-tuning machine learning models but can be computationally expensive. To reduce costs, Multi-fidelity HPO (MF-HPO) leverages intermediate accuracy levels in the learning process and discards low-performing models early on. We compared various representative MF-HPO methods against a simple baseline on classical benchmark data. The baseline involved discarding all models except the Top-K after training for only one epoch, followed by further training to select the best model. Surprisingly, this baseline achieved similar results to its counterparts, while requiring an order of magnitude less computation. Upon analyzing the learning curves of the benchmark data, we observed a few dominant learning curves, which explained the success of our baseline. This suggests that researchers should (1) always use the suggested baseline in benchmarks and (2) broaden the diversity of MF-HPO benchmarks to include more complex cases.
</details>
<details>
<summary>摘要</summary>
（简化中文）机器学习模型精细调整（HPO）是重要的，但可能会占用大量计算资源。为了降低成本，多权限HPO（MF-HPO）利用学习过程中的中间准确级别，早期弃掉低性能的模型。我们对多种代表性的MF-HPO方法进行了比较，并将其与简单的基准线进行了比较。这个基准线是在几个epoch后，仅保留Top-K模型，然后进行进一步的训练来选择最佳模型。很奇怪地，这个基准线能够与其他方法达到类似的结果，而且需要一个数量级的计算量更少。我们对经典数据集的学习曲线进行分析，发现了一些主导的学习曲线，这解释了我们的基准线的成功。这表明研究者应该（1）在benchmark中使用我们的基准线，（2）扩展MF-HPO benchmark的多样性，以包括更复杂的场景。
</details></li>
</ul>
<hr>
<h2 id="The-Initial-Screening-Order-Problem"><a href="#The-Initial-Screening-Order-Problem" class="headerlink" title="The Initial Screening Order Problem"></a>The Initial Screening Order Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15398">http://arxiv.org/abs/2307.15398</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jose M. Alvarez, Salvatore Ruggieri</li>
<li>For: 本研究目标是解决初步屏选择问题，这是候选人选择过程中的一个关键步骤。* Methods: 本研究使用了人类化排序算法，以便在候选人池中找到第一个k个适合的候选人，而不是最佳k个适合的候选人。* Results: 研究发现，在不均衡的候选人池（例如有更多男性than女性候选人）下，人类化排序算法可能会受到不平等的努力影响，导致对保护的、下 Representatives of the group are less likely to be selected than non-protected, over-represented groups.  Additionally, the research proves other fairness results under human-like screening.<details>
<summary>Abstract</summary>
In this paper we present the initial screening order problem, a crucial step within candidate screening. It involves a human-like screener with an objective to find the first k suitable candidates rather than the best k suitable candidates in a candidate pool given an initial screening order. The initial screening order represents the way in which the human-like screener arranges the candidate pool prior to screening. The choice of initial screening order has considerable effects on the selected set of k candidates. We prove that under an unbalanced candidate pool (e.g., having more male than female candidates), the human-like screener can suffer from uneven efforts that hinder its decision-making over the protected, under-represented group relative to the non-protected, over-represented group. Other fairness results are proven under the human-like screener. This research is based on a collaboration with a large company to better understand its hiring process for potential automation. Our main contribution is the formalization of the initial screening order problem which, we argue, opens the path for future extensions of the current works on ranking algorithms, fairness, and automation for screening procedures.
</details>
<details>
<summary>摘要</summary>
在本文中，我们介绍了候选人初步排序问题，这是候选人筛选过程中的一个关键步骤。候选人类似于人类屏选员的目标是找到候选人池中的前k个适合者，而不是最佳k个适合者。候选人池的初步排序对选择的候选人集合产生了很大的影响。我们证明，在候选人池不均衡（例如，有更多♂ than ♀ candidates）的情况下，人类屏选员可能会受到不平等的努力，这会对保护的、少数群体进行不公正的决策。我们还证明了其他的公平性结果。这些研究基于与一家大公司的合作，以更好地了解其招聘过程，以便在将来扩展当前的排名算法、公平、自动化招聘过程中的研究。我们的主要贡献在于对初步排序问题的形式化，我们认为这会开启未来的扩展。
</details></li>
</ul>
<hr>
<h2 id="Noisy-Interpolation-Learning-with-Shallow-Univariate-ReLU-Networks"><a href="#Noisy-Interpolation-Learning-with-Shallow-Univariate-ReLU-Networks" class="headerlink" title="Noisy Interpolation Learning with Shallow Univariate ReLU Networks"></a>Noisy Interpolation Learning with Shallow Univariate ReLU Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15396">http://arxiv.org/abs/2307.15396</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nirmit Joshi, Gal Vardi, Nathan Srebro</li>
<li>for: 这个论文研究了采用 interpolate with minimum norm（两层ReLU网络）进行不同损失函数（$L_p$）下的噪声单变量回归问题中的极限欠拟合行为。</li>
<li>methods: 这个论文使用了 minimum norm（两层ReLU网络）来模型不同损失函数（$L_p$）下的噪声单变量回归问题。</li>
<li>results: 研究发现，使用 $L_1$ 损失函数时，欠拟合行为会减少，而使用 $L_p$ 损失函数 ($p&lt;2$) 时，也会减少欠拟合行为，但是使用 $L_p$ 损失函数 ($p\geq 2$) 时，欠拟合行为会变得更加严重。<details>
<summary>Abstract</summary>
We study the asymptotic overfitting behavior of interpolation with minimum norm ($\ell_2$ of the weights) two-layer ReLU networks for noisy univariate regression. We show that overfitting is tempered for the $L_1$ loss, and any $L_p$ loss for $p<2$, but catastrophic for $p\geq 2$.
</details>
<details>
<summary>摘要</summary>
我们研究在插值（使用最小二乘（$\ell_2$) 权重）两层ReLU网络中的极值拟合行为。我们显示，对于$L_1$损失函数，极化行为得到控制；对于任何$L_p$损失函数（$p<2），也能够控制极化行为，但对于$p\geq 2$，则会出现极端的拟合现象。Note that I've used the traditional Chinese characters for "拟合" (jìyù) and "损失函数" (shèshī fúnxìn), which are more commonly used in academic writing.
</details></li>
</ul>
<hr>
<h2 id="Does-Full-Waveform-Inversion-Benefit-from-Big-Data"><a href="#Does-Full-Waveform-Inversion-Benefit-from-Big-Data" class="headerlink" title="Does Full Waveform Inversion Benefit from Big Data?"></a>Does Full Waveform Inversion Benefit from Big Data?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15388">http://arxiv.org/abs/2307.15388</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Jin, Yinan Feng, Shihang Feng, Hanchen Wang, Yinpeng Chen, Benjamin Consolvo, Zicheng Liu, Youzuo Lin</li>
<li>for: 这篇论文研究了大数据对深度学习模型在全波形推 revert （FWI）中的影响。</li>
<li>methods: 这篇论文使用了 OpenFWI，一个最近发布的大规模多结构数据集，来训练和评估深度学习模型。</li>
<li>results: 实验表明，随着数据集的增大，深度学习模型的性能和泛化性也会提高。此外，模型容量需要与数据大小成正比以获得优化的效果。<details>
<summary>Abstract</summary>
This paper investigates the impact of big data on deep learning models for full waveform inversion (FWI). While it is well known that big data can boost the performance of deep learning models in many tasks, its effectiveness has not been validated for FWI. To address this gap, we present an empirical study that investigates how deep learning models in FWI behave when trained on OpenFWI, a collection of large-scale, multi-structural datasets published recently. Particularly, we train and evaluate the FWI models on a combination of 10 2D subsets in OpenFWI that contain 470K data pairs in total. Our experiments demonstrate that larger datasets lead to better performance and generalization of deep learning models for FWI. We further demonstrate that model capacity needs to scale in accordance with data size for optimal improvement.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Co-attention-Graph-Pooling-for-Efficient-Pairwise-Graph-Interaction-Learning"><a href="#Co-attention-Graph-Pooling-for-Efficient-Pairwise-Graph-Interaction-Learning" class="headerlink" title="Co-attention Graph Pooling for Efficient Pairwise Graph Interaction Learning"></a>Co-attention Graph Pooling for Efficient Pairwise Graph Interaction Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15377">http://arxiv.org/abs/2307.15377</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leejunhyun/coattentiongraphpooling">https://github.com/leejunhyun/coattentiongraphpooling</a></li>
<li>paper_authors: Junhyun Lee, Bumsoo Kim, Minji Jeon, Jaewoo Kang</li>
<li>for: 处理和学习图structured数据</li>
<li>methods: 使用 co-attention 在图pooling 中提取交互作表示</li>
<li>results: 在实际数据集上表现竞争力强，与现有方法相比，计算复杂性较低<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have proven to be effective in processing and learning from graph-structured data. However, previous works mainly focused on understanding single graph inputs while many real-world applications require pair-wise analysis for graph-structured data (e.g., scene graph matching, code searching, and drug-drug interaction prediction). To this end, recent works have shifted their focus to learning the interaction between pairs of graphs. Despite their improved performance, these works were still limited in that the interactions were considered at the node-level, resulting in high computational costs and suboptimal performance. To address this issue, we propose a novel and efficient graph-level approach for extracting interaction representations using co-attention in graph pooling. Our method, Co-Attention Graph Pooling (CAGPool), exhibits competitive performance relative to existing methods in both classification and regression tasks using real-world datasets, while maintaining lower computational complexity.
</details>
<details>
<summary>摘要</summary>
GRAPH NEURAL NETWORKS (GNNs) 有效地处理和学习图Structured data。然而，前一些工作主要关注单个图输入的理解，而实际应用中 часто需要对图Structured data进行对比分析（例如，场景图匹配、代码搜索和药物交互预测）。为此，最近的工作强调了对对Graph Structured data的对比分析。尽管它们的性能得到改进，但是它们仍然受到节点级别的交互所限，导致计算成本高、性能下降。为解决这个问题，我们提出了一种新的和高效的图级别方法，即协同注意力图Pooling（CAGPool）。我们的方法在实际数据集上展现了与现有方法相当的竞争性，而且计算复杂度较低。
</details></li>
</ul>
<hr>
<h2 id="Conflict-free-joint-decision-by-lag-and-zero-lag-synchronization-in-laser-network"><a href="#Conflict-free-joint-decision-by-lag-and-zero-lag-synchronization-in-laser-network" class="headerlink" title="Conflict-free joint decision by lag and zero-lag synchronization in laser network"></a>Conflict-free joint decision by lag and zero-lag synchronization in laser network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15373">http://arxiv.org/abs/2307.15373</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hisako Ito, Takatomo Mihana, Ryoichi Horisaki, Makoto Naruse</li>
<li>for: 这个论文探讨了用激光网络作为光学加速器解决竞争多重臂炮问题。</li>
<li>methods: 该研究使用了激光网络实现了协同决策，并通过零延迟和延迟同步实现了冲突避免和决策效果。</li>
<li>results: 实验表明，该系统在基本2名玩家、2个槽scenario中实现了低冲突率和高奖励，并且展示了该系统的可扩展性。<details>
<summary>Abstract</summary>
With the end of Moore's Law and the increasing demand for computing, photonic accelerators are garnering considerable attention. This is due to the physical characteristics of light, such as high bandwidth and multiplicity, and the various synchronization phenomena that emerge in the realm of laser physics. These factors come into play as computer performance approaches its limits. In this study, we explore the application of a laser network, acting as a photonic accelerator, to the competitive multi-armed bandit problem. In this context, conflict avoidance is key to maximizing environmental rewards. We experimentally demonstrate cooperative decision-making using zero-lag and lag synchronization within a network of four semiconductor lasers. Lag synchronization of chaos realizes effective decision-making and zero-delay synchronization is responsible for the realization of the collision avoidance function. We experimentally verified a low collision rate and high reward in a fundamental 2-player, 2-slot scenario, and showed the scalability of this system. This system architecture opens up new possibilities for intelligent functionalities in laser dynamics.
</details>
<details>
<summary>摘要</summary>
随着莫尔定律的结束和计算机能力的增长，光学加速器正在吸引广泛的关注。这是因为光的物理特性，如带宽和多重性，以及光学同步现象的多种表现。这些因素在计算机性能接近限制时变得重要。在这项研究中，我们探讨了使用激光网络作为光学加速器，解决竞争多臂优化问题。在这种情况下，避免冲突是最大化环境奖励的关键。我们通过实验证明了协同决策中的零延迟和延迟同步可以实现有效的决策，并证明了冲突避免功能的实现。我们在基本的2个玩家、2个槽场景中实际验证了低冲突率和高奖励。这种系统架构开启了新的智能功能在激光动力学中。
</details></li>
</ul>
<hr>
<h2 id="Toward-Transparent-Sequence-Models-with-Model-Based-Tree-Markov-Model"><a href="#Toward-Transparent-Sequence-Models-with-Model-Based-Tree-Markov-Model" class="headerlink" title="Toward Transparent Sequence Models with Model-Based Tree Markov Model"></a>Toward Transparent Sequence Models with Model-Based Tree Markov Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15367">http://arxiv.org/abs/2307.15367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chan Hsu, Wei-Chun Huang, Jun-Ting Wu, Chih-Yuan Li, Yihuang Kang</li>
<li>for: 本研究旨在解决复杂黑盒机器学习模型在序列数据上的解释性问题。</li>
<li>methods: 本研究提出了基于树的隐藏Markov模型（MOB-HSMM），这是一种具有解释性的模型，用于在医学集中检测高死亡风险事件和找到隐藏的死亡风险相关性。该模型利用了深度神经网络（DNN）中的知识来提高预测性能，同时提供了明确的解释。</li>
<li>results: 我们的实验结果表明，通过使用LSTM学习序列模式，并将其传递给MOB树，可以提高模型的预测性能。将MOB树与隐藏Markov模型（HSMM）结合在一起，可以揭示出可能的和解释的序列。<details>
<summary>Abstract</summary>
In this study, we address the interpretability issue in complex, black-box Machine Learning models applied to sequence data. We introduce the Model-Based tree Hidden Semi-Markov Model (MOB-HSMM), an inherently interpretable model aimed at detecting high mortality risk events and discovering hidden patterns associated with the mortality risk in Intensive Care Units (ICU). This model leverages knowledge distilled from Deep Neural Networks (DNN) to enhance predictive performance while offering clear explanations. Our experimental results indicate the improved performance of Model-Based trees (MOB trees) via employing LSTM for learning sequential patterns, which are then transferred to MOB trees. Integrating MOB trees with the Hidden Semi-Markov Model (HSMM) in the MOB-HSMM enables uncovering potential and explainable sequences using available information.
</details>
<details>
<summary>摘要</summary>
在这种研究中，我们解决了复杂黑盒机器学习模型应用于序列数据中的解释性问题。我们介绍了基于模型的树隐藏半马尔可夫模型（MOB-HSMM），这是一种自然解释的模型，用于检测高死亡风险事件和找到隐藏在ICU中的死亡风险相关的Pattern。这个模型利用了深度神经网络（DNN）提供的知识来提高预测性能，同时提供明确的解释。我们的实验结果表明，通过使用LSTM学习序列模式，可以提高模型树（MOB trees）的性能。将MOB树与隐藏半马尔可夫模型（HSMM）结合在一起，可以揭示可用信息中的可能和解释性序列。
</details></li>
</ul>
<hr>
<h2 id="Confident-Feature-Ranking"><a href="#Confident-Feature-Ranking" class="headerlink" title="Confident Feature Ranking"></a>Confident Feature Ranking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15361">http://arxiv.org/abs/2307.15361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bitya Neuhof, Yuval Benjamini</li>
<li>for: 本文提出了一种方法，用于解决 interpretability 问题，即 feature importance 值的解释方法。</li>
<li>methods: 本文使用了 pairwise comparisons 方法，以生成一个稳定的排名，同时也生成了相应的信任区间。</li>
<li>results: 本文 garantía que包含了“真实”（无限大样本）排名，并且允许选择 top-k 集。<details>
<summary>Abstract</summary>
Interpretation of feature importance values often relies on the relative order of the features rather than on the value itself, referred to as ranking. However, the order may be unstable due to the small sample sizes used in calculating the importance values. We propose that post-hoc importance methods produce a ranking and simultaneous confident intervals for the rankings. Based on pairwise comparisons of the feature importance values, our method is guaranteed to include the ``true'' (infinite sample) ranking with high probability and allows for selecting top-k sets.
</details>
<details>
<summary>摘要</summary>
通常来说，特征重要性值的解释通过特征之间的相对排序而进行，而不是直接查看值本身。这种排序方式被称为排名。然而，由于使用小样本计算特征重要性值的时候，排名可能不稳定。我们提议使用 posterior 重要性方法生成排名和同时确定范围。这种方法可以 garantizar在无穷样本情况下包含“真实”排名，并允许选择top-k集。Note: "posterior" in Chinese is "后期" (hòujiè), and "simultaneous" is "同时" (tóngshí).
</details></li>
</ul>
<hr>
<h2 id="Med-HALT-Medical-Domain-Hallucination-Test-for-Large-Language-Models"><a href="#Med-HALT-Medical-Domain-Hallucination-Test-for-Large-Language-Models" class="headerlink" title="Med-HALT: Medical Domain Hallucination Test for Large Language Models"></a>Med-HALT: Medical Domain Hallucination Test for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15343">http://arxiv.org/abs/2307.15343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Logesh Kumar Umapathi, Ankit Pal, Malaikannan Sankarasubbu</li>
<li>for: 本研究探讨了大语言模型（LLM）中的幻觉问题，特别是在医疗领域。幻觉可能会在医疗应用中产生严重的后果。</li>
<li>methods: 我们提出了一个新的指标和数据集，即医疗领域幻觉测试（Med-HALT），用于评估和减少幻觉。Med-HALT 数据集来自多个国家和不同的医疗检查，包括多种创新的测试方法。</li>
<li>results: 我们对主流 LLM 进行了评估，包括 Text Davinci、GPT-3.5、LlaMa-2、MPT 和 Falcon，发现了这些模型在幻觉问题上的显著差异。本研究提供了数据集的详细信息，推动了透明度和可重现性。通过这项工作，我们希望为医疗领域中更安全和可靠的语言模型的开发做出贡献。我们的指标可以在 medhalt.github.io 找到。<details>
<summary>Abstract</summary>
This research paper focuses on the challenges posed by hallucinations in large language models (LLMs), particularly in the context of the medical domain. Hallucination, wherein these models generate plausible yet unverified or incorrect information, can have serious consequences in healthcare applications. We propose a new benchmark and dataset, Med-HALT (Medical Domain Hallucination Test), designed specifically to evaluate and reduce hallucinations. Med-HALT provides a diverse multinational dataset derived from medical examinations across various countries and includes multiple innovative testing modalities. Med-HALT includes two categories of tests reasoning and memory-based hallucination tests, designed to assess LLMs's problem-solving and information retrieval abilities.   Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2, MPT, and Falcon, revealing significant differences in their performance. The paper provides detailed insights into the dataset, promoting transparency and reproducibility. Through this work, we aim to contribute to the development of safer and more reliable language models in healthcare. Our benchmark can be found at medhalt.github.io
</details>
<details>
<summary>摘要</summary>
Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2, MPT, and Falcon, and found significant differences in their performance. The paper provides detailed insights into the dataset, promoting transparency and reproducibility. Through this work, we aim to contribute to the development of safer and more reliable language models in healthcare. The Med-HALT benchmark can be found at medhalt.github.io.
</details></li>
</ul>
<hr>
<h2 id="The-Radon-Signed-Cumulative-Distribution-Transform-and-its-applications-in-classification-of-Signed-Images"><a href="#The-Radon-Signed-Cumulative-Distribution-Transform-and-its-applications-in-classification-of-Signed-Images" class="headerlink" title="The Radon Signed Cumulative Distribution Transform and its applications in classification of Signed Images"></a>The Radon Signed Cumulative Distribution Transform and its applications in classification of Signed Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15339">http://arxiv.org/abs/2307.15339</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rohdelab/PyTransKit">https://github.com/rohdelab/PyTransKit</a></li>
<li>paper_authors: Le Gong, Shiying Li, Naqib Sad Pathan, Mohammad Shifat-E-Rabbi, Gustavo K. Rohde, Abu Hasnat Mohammad Rubaiyat, Sumati Thareja</li>
<li>for: 这种新的图像表示技术基于运输学和最优运输学 mathematics, 用于图像的分类。</li>
<li>methods: 该方法结合了较为常见的拉登变换和签名总额变换，用于将图像转换为一种新的表示形式。</li>
<li>results: 该方法可以更准确地表示签名图像中的信息内容，从而实现更高的分类精度。Here is the same information in Traditional Chinese:</li>
<li>for: 这种新的图像表示技术基于运输学和最佳运输学 mathematics, 用于图像的分类。</li>
<li>methods: 这方法结合了较为常见的拉登变换和签名总额变换，用于将图像转换为一种新的表示形式。</li>
<li>results: 这方法可以更准确地表示签名图像中的信息内容，从而实现更高的分类精度。<details>
<summary>Abstract</summary>
Here we describe a new image representation technique based on the mathematics of transport and optimal transport. The method relies on the combination of the well-known Radon transform for images and a recent signal representation method called the Signed Cumulative Distribution Transform. The newly proposed method generalizes previous transport-related image representation methods to arbitrary functions (images), and thus can be used in more applications. We describe the new transform, and some of its mathematical properties and demonstrate its ability to partition image classes with real and simulated data. In comparison to existing transport transform methods, as well as deep learning-based classification methods, the new transform more accurately represents the information content of signed images, and thus can be used to obtain higher classification accuracies. The implementation of the proposed method in Python language is integrated as a part of the software package PyTransKit, available on Github.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的图像表示技术，基于运输和最优运输的数学。该方法通过结合已知的快速傅立叶变换和最近的有标签分布变换来实现。该新提议的方法可以应用于任意函数（图像），因此可以用于更多的应用。我们描述了新的变换，以及其数学性质，并通过实际数据和模拟数据示例来说明其能够更好地分类图像。与现有的运输变换方法和深度学习基于分类方法相比，新的变换可以更准确地表示签名图像的信息内容，因此可以获得更高的分类精度。我们在Python语言中实现了该方法，并将其integrated into the software package PyTransKit，可以在Github上下载。
</details></li>
</ul>
<hr>
<h2 id="Staging-E-Commerce-Products-for-Online-Advertising-using-Retrieval-Assisted-Image-Generation"><a href="#Staging-E-Commerce-Products-for-Online-Advertising-using-Retrieval-Assisted-Image-Generation" class="headerlink" title="Staging E-Commerce Products for Online Advertising using Retrieval Assisted Image Generation"></a>Staging E-Commerce Products for Online Advertising using Retrieval Assisted Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15326">http://arxiv.org/abs/2307.15326</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yueh-Ning Ku, Mikhail Kuznetsov, Shaunak Mishra, Paloma de Juan</li>
<li>for: 提高 dynamically generated product ads（DPA）的图像质量，以提高点击率（CTR）。</li>
<li>methods: 使用生成对抗网络（GAN）和检索帮助GAN（Retrieval Assisted GAN，RA-GAN）生成产品图像的stage background。</li>
<li>results: 通过对比各种方法，显示了复制粘贴阶段（Copy-Paste Staging）的效果，并通过人工评估和线上指标评价得到了良好的结果。此外，还实现了产品图像动画的创建，从而实现了视频广告的生成。<details>
<summary>Abstract</summary>
Online ads showing e-commerce products typically rely on the product images in a catalog sent to the advertising platform by an e-commerce platform. In the broader ads industry such ads are called dynamic product ads (DPA). It is common for DPA catalogs to be in the scale of millions (corresponding to the scale of products which can be bought from the e-commerce platform). However, not all product images in the catalog may be appealing when directly re-purposed as an ad image, and this may lead to lower click-through rates (CTRs). In particular, products just placed against a solid background may not be as enticing and realistic as a product staged in a natural environment. To address such shortcomings of DPA images at scale, we propose a generative adversarial network (GAN) based approach to generate staged backgrounds for un-staged product images. Generating the entire staged background is a challenging task susceptible to hallucinations. To get around this, we introduce a simpler approach called copy-paste staging using retrieval assisted GANs. In copy paste staging, we first retrieve (from the catalog) staged products similar to the un-staged input product, and then copy-paste the background of the retrieved product in the input image. A GAN based in-painting model is used to fill the holes left after this copy-paste operation. We show the efficacy of our copy-paste staging method via offline metrics, and human evaluation. In addition, we show how our staging approach can enable animations of moving products leading to a video ad from a product image.
</details>
<details>
<summary>摘要</summary>
在线广告通常显示电商产品时，会使用电商平台提供的目录，其中包括大量的产品图像。在更广泛的广告业界，这种广告方式被称为动态产品广告（DPA）。这些目录通常包含数百万个产品图像，但不 всех图像都能够直接重用为广告图像，这可能导致更低的点击率（CTR）。特别是，拥有简单背景的产品可能不那么吸引人和真实。为解决这种大规模DPA图像的缺陷，我们提出了基于生成对抗网络（GAN）的方法，生成产品在自然环境中的舞台。生成整个舞台是一项复杂的任务，易于幻化。为此，我们提出了一种简单的方法：复制粘贴舞台使用检索帮助GAN。在这种方法中，我们首先从目录中检索与输入产品相似的已经舞台化产品，然后将其中的背景粘贴到输入图像中。GAN基于填充模型用于填充这些粘贴后的孔隙。我们通过在线指标和人工评估展示了我们的复制粘贴方法的效果。此外，我们还展示了如何使用我们的舞台方法生成动画产品图像，从而将产品图像转换成视频广告。
</details></li>
</ul>
<hr>
<h2 id="Partial-observations-coarse-graining-and-equivariance-in-Koopman-operator-theory-for-large-scale-dynamical-systems"><a href="#Partial-observations-coarse-graining-and-equivariance-in-Koopman-operator-theory-for-large-scale-dynamical-systems" class="headerlink" title="Partial observations, coarse graining and equivariance in Koopman operator theory for large-scale dynamical systems"></a>Partial observations, coarse graining and equivariance in Koopman operator theory for large-scale dynamical systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15325">http://arxiv.org/abs/2307.15325</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sebastian Peitz, Hans Harder, Feliks Nüske, Friedrich Philipp, Manuel Schaller, Karl Worthmann</li>
<li>for: This paper focuses on addressing the challenge of using the Koopman operator for large-scale systems with partial observations or coarse graining.</li>
<li>methods: The authors propose a new method that takes into account the symmetries in the system dynamics to improve the efficiency of the Koopman operator approximation.</li>
<li>results: The authors show that their method can massively increase the model efficiency and provide a more accurate approximation of the Koopman operator for the underlying system, using the Kuramoto–Sivashinsky equation as a numerical example.Here’s the Chinese translation of the three points:</li>
<li>for: 这篇论文关注使用Koopman算子来分析、预测和控制复杂系统，但是只有部分观察数据或者归约到更粗糙的系统。</li>
<li>methods: 作者们提出了一种新的方法，该方法利用系统动力学的对称性来提高Koopman算子的近似精度。</li>
<li>results: 作者们表明，他们的方法可以很大程度地提高模型的效率，并且可以更 precisely  aproximate the Koopman operator for the underlying system，使用 Kuramoto–Sivashinsky 方程作为数值示例。<details>
<summary>Abstract</summary>
The Koopman operator has become an essential tool for data-driven analysis, prediction and control of complex systems, the main reason being the enormous potential of identifying linear function space representations of nonlinear dynamics from measurements. Until now, the situation where for large-scale systems, we (i) only have access to partial observations (i.e., measurements, as is very common for experimental data) or (ii) deliberately perform coarse graining (for efficiency reasons) has not been treated to its full extent. In this paper, we address the pitfall associated with this situation, that the classical EDMD algorithm does not automatically provide a Koopman operator approximation for the underlying system if we do not carefully select the number of observables. Moreover, we show that symmetries in the system dynamics can be carried over to the Koopman operator, which allows us to massively increase the model efficiency. We also briefly draw a connection to domain decomposition techniques for partial differential equations and present numerical evidence using the Kuramoto--Sivashinsky equation.
</details>
<details>
<summary>摘要</summary>
科普曼算子已成为数据驱动分析、预测和控制复杂系统的重要工具，主要原因是可以从测量数据中提取非线性动力学的线性函数空间表示。然而，在大规模系统中，我们通常只有部分观察数据（例如，测量数据）或者故意粗略化（为了提高效率）。在这篇论文中，我们探讨这种情况下经典EDMD算法不会自动为下面系统提供库曼操作符的近似值，并且显示系统动力学Symmetry可以传递到库曼操作符，从而巨大提高模型效率。此外，我们 briefly drew a connection to域 decompositions techniques for partial differential equations, and presented numerical evidence using the Kuramoto--Sivashinsky equation.Note that Simplified Chinese is a romanization of Chinese, and the actual Chinese characters may be different.
</details></li>
</ul>
<hr>
<h2 id="Robust-Visual-Sim-to-Real-Transfer-for-Robotic-Manipulation"><a href="#Robust-Visual-Sim-to-Real-Transfer-for-Robotic-Manipulation" class="headerlink" title="Robust Visual Sim-to-Real Transfer for Robotic Manipulation"></a>Robust Visual Sim-to-Real Transfer for Robotic Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15320">http://arxiv.org/abs/2307.15320</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ricardo Garcia, Robin Strudel, Shizhe Chen, Etienne Arlaud, Ivan Laptev, Cordelia Schmid</li>
<li>for: 本研究旨在帮助学习视omyotor策略在模拟环境中更加安全和有效，但是由于模拟和实际数据之间的差异，模拟训练的策略通常无法在实际机器人上成功。这种情况下，一种常见的方法是域随机化（DR）。</li>
<li>methods: 本研究系统atically explores visual域随机化方法，并对其进行了严格的 benchmarking。特别是，我们提出了一个离线代理任务——立方体localization，以选择DR参数的Texture Randomization、Lighting Randomization、物体颜色变化和摄像头参数。我们发现DR参数在离线代理任务和在线策略之间具有类似的影响。因此，我们使用离线优化的DR参数来训练视omyotor策略在模拟中，并直接将其应用到实际机器人上。</li>
<li>results: 我们的方法可以在一个多样化的机器人抓取任务上 achieve 93% 的成功率。此外，我们还评估了政策在实际场景中的Robustness，并发现我们在模拟中训练的策略在实际场景中表现更加稳定和有效。 codes、模拟环境、实际机器人数据和训练模型都可以在<a target="_blank" rel="noopener" href="https://www.di.ens.fr/willow/research/robust_s2r/">https://www.di.ens.fr/willow/research/robust_s2r/</a> obtained。<details>
<summary>Abstract</summary>
Learning visuomotor policies in simulation is much safer and cheaper than in the real world. However, due to discrepancies between the simulated and real data, simulator-trained policies often fail when transferred to real robots. One common approach to bridge the visual sim-to-real domain gap is domain randomization (DR). While previous work mainly evaluates DR for disembodied tasks, such as pose estimation and object detection, here we systematically explore visual domain randomization methods and benchmark them on a rich set of challenging robotic manipulation tasks. In particular, we propose an off-line proxy task of cube localization to select DR parameters for texture randomization, lighting randomization, variations of object colors and camera parameters. Notably, we demonstrate that DR parameters have similar impact on our off-line proxy task and on-line policies. We, hence, use off-line optimized DR parameters to train visuomotor policies in simulation and directly apply such policies to a real robot. Our approach achieves 93% success rate on average when tested on a diverse set of challenging manipulation tasks. Moreover, we evaluate the robustness of policies to visual variations in real scenes and show that our simulator-trained policies outperform policies learned using real but limited data. Code, simulation environment, real robot datasets and trained models are available at https://www.di.ens.fr/willow/research/robust_s2r/.
</details>
<details>
<summary>摘要</summary>
学习视мотор策略在模拟中比实际世界更安全和便宜。然而，由于模拟和实际数据之间的差异，模拟训练的策略通常在实际机器人上失败。一种常见的方法是域随机化（DR）来bridging模拟和实际的视域域随机化。在这种情况下，我们系统地探索了视域随机化方法，并对其进行了丰富的检验。我们提出了一个离线代理任务，即立方体 Localization，以选择DR参数进行XTURE随机化、照明随机化、物体颜色变化和摄像头参数的选择。我们发现DR参数在离线代理任务和在线策略之间具有类似的影响。因此，我们使用离线优化的DR参数来训练视мотор策略在模拟中，并直接将其应用到实际机器人上。我们的方法在一组多样化的机器人抓取任务中取得了93%的成功率。此外，我们评估了在实际场景中视图变化的影响，并发现我们在模拟中训练的策略在实际数据上表现更加稳定和高效。我们的代码、模拟环境、实际机器人数据和训练模型可以在https://www.di.ens.fr/willow/research/robust_s2r/取得。
</details></li>
</ul>
<hr>
<h2 id="SAP-sLDA-An-Interpretable-Interface-for-Exploring-Unstructured-Text"><a href="#SAP-sLDA-An-Interpretable-Interface-for-Exploring-Unstructured-Text" class="headerlink" title="SAP-sLDA: An Interpretable Interface for Exploring Unstructured Text"></a>SAP-sLDA: An Interpretable Interface for Exploring Unstructured Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01420">http://arxiv.org/abs/2308.01420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Charumathi Badrinath, Weiwei Pan, Finale Doshi-Velez</li>
<li>for: 该论文目的是提出一种基于LDA的半监督人工智能方法，用于从文本集中学习主题，并保持文档之间含义上的相互关系。</li>
<li>methods: 该方法使用了LDA算法，并在人工智能 loop中添加了一些约束来保持文档之间的相互关系。</li>
<li>results: 在 sintetic corpora 上，该方法可以生成更加 interpretable 的低维度投影，比基eline方法更好。在真实的 corpus 上，该方法也 obtaint 质量相似的结果。<details>
<summary>Abstract</summary>
A common way to explore text corpora is through low-dimensional projections of the documents, where one hopes that thematically similar documents will be clustered together in the projected space. However, popular algorithms for dimensionality reduction of text corpora, like Latent Dirichlet Allocation (LDA), often produce projections that do not capture human notions of document similarity. We propose a semi-supervised human-in-the-loop LDA-based method for learning topics that preserve semantically meaningful relationships between documents in low-dimensional projections. On synthetic corpora, our method yields more interpretable projections than baseline methods with only a fraction of labels provided. On a real corpus, we obtain qualitatively similar results.
</details>
<details>
<summary>摘要</summary>
一种常见的文本资料探索方法是通过文本文档的低维度投影，希望将主题相似的文档在投影空间中归一化。然而，受欢迎的文本维度减少算法，如潜在 Dirichlet 分配（LDA），经常生成投影，不会捕捉人类的文档相似性。我们提出了一种半监督人类在循环中 LDA 基于方法，用于学习保留含义意义的文档关系。在 sintetic  corpora 上，我们的方法可以得到更加可读的投影，只需提供一小部分的标签。在真实 corpora 上，我们获得了类似的结果。
</details></li>
</ul>
<hr>
<h2 id="DiffKendall-A-Novel-Approach-for-Few-Shot-Learning-with-Differentiable-Kendall’s-Rank-Correlation"><a href="#DiffKendall-A-Novel-Approach-for-Few-Shot-Learning-with-Differentiable-Kendall’s-Rank-Correlation" class="headerlink" title="DiffKendall: A Novel Approach for Few-Shot Learning with Differentiable Kendall’s Rank Correlation"></a>DiffKendall: A Novel Approach for Few-Shot Learning with Differentiable Kendall’s Rank Correlation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15317">http://arxiv.org/abs/2307.15317</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaipeng Zheng, Huishuai Zhang, Weiran Huang</li>
<li>for: 增强几何学习中的类别不熟悉问题，通过重要性排名特征通道的importance来改善类别识别性能。</li>
<li>methods: 使用Kendall的排名相关度代替几何相似度метри克，并提出一种特殊设计的可导损失函数来解决非导数问题。</li>
<li>results: 对各种领域和数据集进行了广泛的实验，证明了排名相关度基于方法可以大幅提高几何学习性能。<details>
<summary>Abstract</summary>
Few-shot learning aims to adapt models trained on the base dataset to novel tasks where the categories are not seen by the model before. This often leads to a relatively uniform distribution of feature values across channels on novel classes, posing challenges in determining channel importance for novel tasks. Standard few-shot learning methods employ geometric similarity metrics such as cosine similarity and negative Euclidean distance to gauge the semantic relatedness between two features. However, features with high geometric similarities may carry distinct semantics, especially in the context of few-shot learning. In this paper, we demonstrate that the importance ranking of feature channels is a more reliable indicator for few-shot learning than geometric similarity metrics. We observe that replacing the geometric similarity metric with Kendall's rank correlation only during inference is able to improve the performance of few-shot learning across a wide range of datasets with different domains. Furthermore, we propose a carefully designed differentiable loss for meta-training to address the non-differentiability issue of Kendall's rank correlation. Extensive experiments demonstrate that the proposed rank-correlation-based approach substantially enhances few-shot learning performance.
</details>
<details>
<summary>摘要</summary>
几个shot学习目标是使模型从基础数据集中适应新任务，其中类别未曾由模型见过。这经常导致新任务频道值的相对均匀分布，从而增加了决定频道重要性的挑战。标准的几个shot学习方法使用几何相似度度量，如cos相似性和负Euclidean距离，来衡量两个特征之间的semantic相似性。但是，具有高几何相似度的特征可能拥有不同的 semantics，特别在几个shot学习上。在这篇论文中，我们表明了特征通道的重要性排名是几个shot学习中更可靠的指标，而不是几何相似度度量。我们发现，在推理过程中只是将几何相似度度量替换为Kendall的排名相关性可以在各种领域的数据集上提高几个shot学习的性能。此外，我们提出了一种特殊的可导损失函数，用于meta-训练，以解决Kendall的排名相关性的不导数性问题。广泛的实验表明，我们的排名相关性基本上提高了几个shot学习的性能。
</details></li>
</ul>
<hr>
<h2 id="Differential-Evolution-Algorithm-based-Hyper-Parameters-Selection-of-Transformer-Neural-Network-Model-for-Load-Forecasting"><a href="#Differential-Evolution-Algorithm-based-Hyper-Parameters-Selection-of-Transformer-Neural-Network-Model-for-Load-Forecasting" class="headerlink" title="Differential Evolution Algorithm based Hyper-Parameters Selection of Transformer Neural Network Model for Load Forecasting"></a>Differential Evolution Algorithm based Hyper-Parameters Selection of Transformer Neural Network Model for Load Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15299">http://arxiv.org/abs/2307.15299</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anuvabsen1/meta-transformer">https://github.com/anuvabsen1/meta-transformer</a></li>
<li>paper_authors: Anuvab Sen, Arul Rhik Mazumder, Udayon Sen</li>
<li>for: 预测电力负荷的精度是许多领域的关键，但是传统的统计模型很难准确地捕捉动态电力系统的复杂 dinamics。因此，时间序列模型（ARIMA）和深度学习模型（ANN、LSTM、GRU等）经常被使用，并经常获得更高的成功。</li>
<li>methods: 本文分析了在负荷预测中使用Recently发展的Transformer模型的效果。Transformer模型具有学习长距离依赖关系的能力，因此它们有可能改善负荷预测的准确性。我们使用了多种metaheuristics，包括不同的Differential Evolution算法来找出最佳的超参数。</li>
<li>results: 我们通过对不同metaheuristics算法和Transformer模型的组合进行比较，发现这些组合在负荷预测中的性能都非常高。具体来说，使用Differential Evolution算法来优化Transformer模型可以减少负荷预测的偏差和误差。<details>
<summary>Abstract</summary>
Accurate load forecasting plays a vital role in numerous sectors, but accurately capturing the complex dynamics of dynamic power systems remains a challenge for traditional statistical models. For these reasons, time-series models (ARIMA) and deep-learning models (ANN, LSTM, GRU, etc.) are commonly deployed and often experience higher success. In this paper, we analyze the efficacy of the recently developed Transformer-based Neural Network model in Load forecasting. Transformer models have the potential to improve Load forecasting because of their ability to learn long-range dependencies derived from their Attention Mechanism. We apply several metaheuristics namely Differential Evolution to find the optimal hyperparameters of the Transformer-based Neural Network to produce accurate forecasts. Differential Evolution provides scalable, robust, global solutions to non-differentiable, multi-objective, or constrained optimization problems. Our work compares the proposed Transformer based Neural Network model integrated with different metaheuristic algorithms by their performance in Load forecasting based on numerical metrics such as Mean Squared Error (MSE) and Mean Absolute Percentage Error (MAPE). Our findings demonstrate the potential of metaheuristic-enhanced Transformer-based Neural Network models in Load forecasting accuracy and provide optimal hyperparameters for each model.
</details>
<details>
<summary>摘要</summary>
准确的负荷预测在多个领域发挥重要作用，但是正确地捕捉动态电力系统的复杂动态还是传统统计模型的挑战。因此，时间序列模型（ARIMA）和深度学习模型（ANN、LSTM、GRU等）通常被部署，并经常获得更高的成功。在这篇论文中，我们分析了在负荷预测中使用最近开发的Transformer基于神经网络模型的效果。Transformer模型具有学习长距离依赖关系的能力，因此它们有可能改善负荷预测的准确性。我们使用了多种metaheuristics，包括不同的Differential Evolution算法，以找到最佳的Hyperparameter，以生成高精度的预测结果。我们的工作对不同metaheuristic算法 integrated with Transformer基于神经网络模型的性能进行比较，并根据numerical metrics such as Mean Squared Error (MSE)和Mean Absolute Percentage Error (MAPE)进行评价。我们的发现表明metaheuristic强化的Transformer基于神经网络模型在负荷预测准确性方面具有潜在的潜力，并且提供了每个模型的最佳Hyperparameter。
</details></li>
</ul>
<hr>
<h2 id="Learning-Nonlinear-Projections-for-Reduced-Order-Modeling-of-Dynamical-Systems-using-Constrained-Autoencoders"><a href="#Learning-Nonlinear-Projections-for-Reduced-Order-Modeling-of-Dynamical-Systems-using-Constrained-Autoencoders" class="headerlink" title="Learning Nonlinear Projections for Reduced-Order Modeling of Dynamical Systems using Constrained Autoencoders"></a>Learning Nonlinear Projections for Reduced-Order Modeling of Dynamical Systems using Constrained Autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15288">http://arxiv.org/abs/2307.15288</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/grmacchio/romnet_chaos2023">https://github.com/grmacchio/romnet_chaos2023</a></li>
<li>paper_authors: Samuel E. Otto, Gregory R. Macchio, Clarence W. Rowley</li>
<li>for: 这种新发展的减少模型技术用于近似非线性动力系统的低维度抽象，从数据学习出来的抽象 manifold 上的动态系统。</li>
<li>methods: 我们使用受限的自动encoder神经网络来学习 manifold 和投影纤维，并使用不同的权重矩阵和强制活动函数来保证encoder是解码器的左逆。我们还引入了新的动力学感知成本函数，以便学习倾斜投影纤维，考虑到快速动力学和非正态感知机制。</li>
<li>results: 我们通过一个简单的三个状态模型，对某种在一个流体中的一个粗体后形成的涡流进行了示例研究，并证明了我们的方法可以有效地模型普通动态系统的抽象。此外，我们还提出了一些用于构建高维系统的减少模型的技术，包括一种新的稀热性激发 penalty，以避免encoder weights缩回的问题。<details>
<summary>Abstract</summary>
Recently developed reduced-order modeling techniques aim to approximate nonlinear dynamical systems on low-dimensional manifolds learned from data. This is an effective approach for modeling dynamics in a post-transient regime where the effects of initial conditions and other disturbances have decayed. However, modeling transient dynamics near an underlying manifold, as needed for real-time control and forecasting applications, is complicated by the effects of fast dynamics and nonnormal sensitivity mechanisms. To begin to address these issues, we introduce a parametric class of nonlinear projections described by constrained autoencoder neural networks in which both the manifold and the projection fibers are learned from data. Our architecture uses invertible activation functions and biorthogonal weight matrices to ensure that the encoder is a left inverse of the decoder. We also introduce new dynamics-aware cost functions that promote learning of oblique projection fibers that account for fast dynamics and nonnormality. To demonstrate these methods and the specific challenges they address, we provide a detailed case study of a three-state model of vortex shedding in the wake of a bluff body immersed in a fluid, which has a two-dimensional slow manifold that can be computed analytically. In anticipation of future applications to high-dimensional systems, we also propose several techniques for constructing computationally efficient reduced-order models using our proposed nonlinear projection framework. This includes a novel sparsity-promoting penalty for the encoder that avoids detrimental weight matrix shrinkage via computation on the Grassmann manifold.
</details>
<details>
<summary>摘要</summary>
近期发展的减少模型技术目的是通过从数据学习低维度拟合来近似非线性动力系统。这是一种有效的方法，在卷积过程后的吸收器恢复过程中模型动力系统的吸收器恢复过程。然而，在近似动态系统的恢复过程中，因为快速动力和非正常敏感机制的影响，模型化吸收器恢复过程是复杂的。为解决这些问题，我们提出了一个参数化的非线性投影框架，其中拟合的拟合网络使用卷积神经网络学习数据，并使用卷积神经网络的权重矩阵来保证投影网络的可逆性。我们还引入了新的动力敏感成本函数，以便在学习投影纤维时考虑快速动力和非正常性。为了证明我们的方法和特定挑战，我们提供了一个详细的三个状态模型的涡流排斥示例，该模型具有可以计算出的二维慢拟合。在未来应用于高维系统时，我们还提出了一些构建高效减少模型的技术，包括一种新的缺省值激活函数，以避免因计算格先替换而导致的负面weight矩阵缩小。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Approximation-of-Zonoids-and-Uniform-Approximation-by-Shallow-Neural-Networks"><a href="#Optimal-Approximation-of-Zonoids-and-Uniform-Approximation-by-Shallow-Neural-Networks" class="headerlink" title="Optimal Approximation of Zonoids and Uniform Approximation by Shallow Neural Networks"></a>Optimal Approximation of Zonoids and Uniform Approximation by Shallow Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15285">http://arxiv.org/abs/2307.15285</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan W. Siegel</li>
<li>for: 本研究探讨了两个相关的问题。第一个问题是判断任意ζonoid在 $\mathbb{R}^{d+1}$ 中可以在 Hausdorff 距离上被近似为一个SUM OF $n$ 条直线段。第二个问题是确定 shallow ReLU$^k$ 神经网络在其变化空间上的优化approximation rate。</li>
<li>methods: 我们使用了新的技术来解决第一个问题，并在 $d\neq 2,3$ 的情况下已经完成了解决。当 $d&#x3D;2,3$ 时，我们关闭了 logarithmic gap，完成了所有维度的解决。在第二个问题上，我们的技术可以在 $k\geq 1$ 时对 uniform norm 进行优化approximation，并能够对目标函数和其导数进行均匀approximation。</li>
<li>results: 我们的研究结果显示，我们可以在 $d\neq 2,3$ 的情况下提供优化的approximation rate，并且在 $d&#x3D;2,3$ 时关闭了 logarithmic gap。此外，我们的技术还可以在 shallow ReLU$^k$ 神经网络上实现更好的approximation rate。<details>
<summary>Abstract</summary>
We study the following two related problems. The first is to determine to what error an arbitrary zonoid in $\mathbb{R}^{d+1}$ can be approximated in the Hausdorff distance by a sum of $n$ line segments. The second is to determine optimal approximation rates in the uniform norm for shallow ReLU$^k$ neural networks on their variation spaces. The first of these problems has been solved for $d\neq 2,3$, but when $d=2,3$ a logarithmic gap between the best upper and lower bounds remains. We close this gap, which completes the solution in all dimensions. For the second problem, our techniques significantly improve upon existing approximation rates when $k\geq 1$, and enable uniform approximation of both the target function and its derivatives.
</details>
<details>
<summary>摘要</summary>
我们研究以下两个相关的问题。第一个问题是确定任意㴒形在 $\mathbb{R}^{d+1} $ 可以被对准 Hausdorff 距离的近似，使用 $n$ 条直线段。第二个问题是 Determine  shallow ReLU $^k$ 神经网络在其变数空间上的最佳近似率，我们的技术可以对 $k\geq 1$ 进行改进，并允许对目标函数和其 derivatives 进行均匀近似。Here's the breakdown of the translation:* 我们 (wǒ men) - we* 研究 (yan jiu) - study* 以下 (yǐ xià) - the following* 两个 (liǎng ge) - two* 相关 (xiāng guān) - related* 问题 (wèn tí) - problems* 第一个 (dì yī ge) - the first* 问题 (wèn tí) - problem* 是 (shì) - is* 确定 (kāo dìng) - to determine* 任意 (rèn yì) - arbitrary* 㴒形 (dào xíng) - zonoid* 在 (zhī) - in* $\mathbb{R}^{d+1}$ (REAL d+1) - the space of all real $(d+1)$-tuples* 可以 (kěn yǐ) - can* 被 (bèi) - be* 对准 (duì zhèng) - approximated* Hausdorff 距离 (Hausdorff yuè lǐ) - Hausdorff distance* 使用 (fù yòng) - using* $n$ (n) - a positive integer* 条 (tiě) - line* 直线段 (zhí xiàn tiě) - line segment* 第二个问题 (dì èr ge wèn tí) - the second problem* Determine (dì tí) - determine*  shallow ReLU $^k$ (piān shū ReLU $^k$) - shallow ReLU $^k$ neural networks* 在其变数空间上 (zhī qiè yàng yòu zhèng xiàng) - on their variation spaces* 最佳近似率 (mài jiā qióng yì lǚ) - optimal approximation rates* 我们的技术 (wǒ men de jì shù) - our techniques* 可以 (kěn yǐ) - can* 对 (duì) - towards* $k\geq 1$ (k ge 1) - when $k\geq 1$* 进行改进 (jìn zuò gǎi jì) - to improve* 并 (bìn) - and* 允许 (yùn xū) - allow* 对目标函数 (duì mù zhì fù) - towards the target function* 和其 derivatives (hè qí yī jī) - and its derivatives* 进行均匀近似 (jìn zuò jìng yì qióng) - to achieve uniform approximation
</details></li>
</ul>
<hr>
<h2 id="VeriGen-A-Large-Language-Model-for-Verilog-Code-Generation"><a href="#VeriGen-A-Large-Language-Model-for-Verilog-Code-Generation" class="headerlink" title="VeriGen: A Large Language Model for Verilog Code Generation"></a>VeriGen: A Large Language Model for Verilog Code Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00708">http://arxiv.org/abs/2308.00708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shailja Thakur, Baleegh Ahmad, Hammond Pearce, Benjamin Tan, Brendan Dolan-Gavitt, Ramesh Karri, Siddharth Garg</li>
<li>for: 本研究探讨了大型自然语言模型（LLMs）在自动硬件设计方面的能力，通过生成高质量的Verilog代码来提高设计效率。</li>
<li>methods: 我们在这种研究中使用了预先训练的LLMs，对Verilog数据集 compile from GitHub和Verilog教科书进行了微调。我们使用了特制的测试环境，包括自定义问题集和测试架构，来评估生成的Verilog代码的函数正确性。</li>
<li>results: 我们的微调后的开源CodeGen-16B模型在一个特制的测试集上表现出优于商业化状态的GPT-3.5-turbo模型，提高了1.1%的总性能。在面对更多和更复杂的问题集时，我们发现微调后的模型与状态之前的GPT-3.5-turbo模型竞争，在某些场景下 even excelling。特别是，它在不同类型问题中生成符合语法规则的Verilog代码方面提高了41%，这 highlights the potential of smaller, in-house LLMs in hardware design automation。<details>
<summary>Abstract</summary>
In this study, we explore the capability of Large Language Models (LLMs) to automate hardware design by generating high-quality Verilog code, a common language for designing and modeling digital systems. We fine-tune pre-existing LLMs on Verilog datasets compiled from GitHub and Verilog textbooks. We evaluate the functional correctness of the generated Verilog code using a specially designed test suite, featuring a custom problem set and testing benches. Here, our fine-tuned open-source CodeGen-16B model outperforms the commercial state-of-the-art GPT-3.5-turbo model with a 1.1% overall increase. Upon testing with a more diverse and complex problem set, we find that the fine-tuned model shows competitive performance against state-of-the-art gpt-3.5-turbo, excelling in certain scenarios. Notably, it demonstrates a 41% improvement in generating syntactically correct Verilog code across various problem categories compared to its pre-trained counterpart, highlighting the potential of smaller, in-house LLMs in hardware design automation.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们研究了大型自然语言模型（LLMs）是否能自动设计硬件，通过生成高质量的Verilog代码，这是数字系统设计和模拟的通用语言。我们对已有的LLMs进行了精度调整，使用从GitHub和Verilog教材中编译的Verilog数据集。我们使用专门设计的测试 suite，包括自定义问题集和测试框架，来评估生成的Verilog代码的功能正确性。我们发现，我们的精度调整的开源CodeGen-16B模型在一个更加多样化和复杂的问题集上测试时，与商业State-of-the-art GPT-3.5-turbo模型相当，并在某些场景下表现出色。特别是，它在不同类别的问题集中生成了41%的正确Verilog代码，与其预训练版本相比，这 highlights the potential of smaller, in-house LLMs in hardware design automation。
</details></li>
</ul>
<hr>
<h2 id="Recovering-high-quality-FODs-from-a-reduced-number-of-diffusion-weighted-images-using-a-model-driven-deep-learning-architecture"><a href="#Recovering-high-quality-FODs-from-a-reduced-number-of-diffusion-weighted-images-using-a-model-driven-deep-learning-architecture" class="headerlink" title="Recovering high-quality FODs from a reduced number of diffusion-weighted images using a model-driven deep learning architecture"></a>Recovering high-quality FODs from a reduced number of diffusion-weighted images using a model-driven deep learning architecture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15273">http://arxiv.org/abs/2307.15273</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jbartlett6/sdnet">https://github.com/jbartlett6/sdnet</a></li>
<li>paper_authors: J Bartlett, C E Davey, L A Johnston, J Duan</li>
<li>for: 提高 diffusion-weighted imaging (DWI) 的重建精度和速度</li>
<li>methods: 使用深度学习，特别是圆柱体减解网络 (SDNet)，对 DWI 信号进行重建</li>
<li>results: 与state-of-the-art FOD 超分辨网络 (FOD-Net) 相比，SDNet 具有竞争力的性能，并且可以通过调整 fixel 分类 penalty 来提高下游 fixel 基于分析的性能。<details>
<summary>Abstract</summary>
Fibre orientation distribution (FOD) reconstruction using deep learning has the potential to produce accurate FODs from a reduced number of diffusion-weighted images (DWIs), decreasing total imaging time. Diffusion acquisition invariant representations of the DWI signals are typically used as input to these methods to ensure that they can be applied flexibly to data with different b-vectors and b-values; however, this means the network cannot condition its output directly on the DWI signal. In this work, we propose a spherical deconvolution network, a model-driven deep learning FOD reconstruction architecture, that ensures intermediate and output FODs produced by the network are consistent with the input DWI signals. Furthermore, we implement a fixel classification penalty within our loss function, encouraging the network to produce FODs that can subsequently be segmented into the correct number of fixels and improve downstream fixel-based analysis. Our results show that the model-based deep learning architecture achieves competitive performance compared to a state-of-the-art FOD super-resolution network, FOD-Net. Moreover, we show that the fixel classification penalty can be tuned to offer improved performance with respect to metrics that rely on accurately segmented of FODs. Our code is publicly available at https://github.com/Jbartlett6/SDNet .
</details>
<details>
<summary>摘要</summary>
Diffusion-weighted imaging (DWI)的扩展重构（FOD）使用深度学习可以生成高精度的FOD，从而减少总的扫描时间。通常使用Diffusion acquisition invariant representations（DAI）来作为输入，以确保这些方法可以适应不同的b- вектор和b-值数据。然而，这意味着网络无法直接基于DWI信号来condition its output。在这种工作中，我们提出了一种圆拟网络（SDNet），一种基于模型的深度学习FOD重构架构，以确保输入DWI信号和网络输出FOD之间的一致性。此外，我们在损失函数中添加了粒子分类 penalty，以促进网络生成的FOD可以 subsequentialmente 被正确地分割为粒子，提高下游粒子基于分析的性能。我们的结果表明，模型基于的深度学习架构可以与状态 Ell的FOD超分辨率网络（FOD-Net）相比，并且我们表明，粒子分类 penalty可以调整以提高基于粒子分类的性能。我们的代码公开在 GitHub上，请参考https://github.com/Jbartlett6/SDNet。
</details></li>
</ul>
<hr>
<h2 id="An-Overview-Of-Temporal-Commonsense-Reasoning-and-Acquisition"><a href="#An-Overview-Of-Temporal-Commonsense-Reasoning-and-Acquisition" class="headerlink" title="An Overview Of Temporal Commonsense Reasoning and Acquisition"></a>An Overview Of Temporal Commonsense Reasoning and Acquisition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00002">http://arxiv.org/abs/2308.00002</a></li>
<li>repo_url: None</li>
<li>paper_authors: Georg Wenzel, Adam Jatowt</li>
<li>for: 本研究旨在提高语言模型对时间常识的理解和应用，以便更好地解决时间自然语言处理任务。</li>
<li>methods: 研究使用各种增强技术和评估策略来提高语言模型的时间常识理解能力，并对不同数据集进行评估。</li>
<li>results: 尽管使用增强技术可以提高语言模型的性能，但这些模型仍然无法接近人类水平在时间常识领域的理解和解决能力。<details>
<summary>Abstract</summary>
Temporal commonsense reasoning refers to the ability to understand the typical temporal context of phrases, actions, and events, and use it to reason over problems requiring such knowledge. This trait is essential in temporal natural language processing tasks, with possible applications such as timeline summarization, temporal question answering, and temporal natural language inference. Recent research on the performance of large language models suggests that, although they are adept at generating syntactically correct sentences and solving classification tasks, they often take shortcuts in their reasoning and fall prey to simple linguistic traps. This article provides an overview of research in the domain of temporal commonsense reasoning, particularly focusing on enhancing language model performance through a variety of augmentations and their evaluation across a growing number of datasets. However, these augmented models still struggle to approach human performance on reasoning tasks over temporal common sense properties, such as the typical occurrence times, orderings, or durations of events. We further emphasize the need for careful interpretation of research to guard against overpromising evaluation results in light of the shallow reasoning present in transformers. This can be achieved by appropriately preparing datasets and suitable evaluation metrics.
</details>
<details>
<summary>摘要</summary>
时间共识理解指的是理解phrases、actions和events的typical temporal context，并使用这种知识来解决问题。这种 trait 是 temporal natural language processing 任务的重要组成部分，可能的应用包括时间线概要、时间问答和时间自然语言推理。现代研究表明，虽然大语言模型能够生成正确的语法结构和解决分类任务，但它们经常采取缩短的思维方式，容易受到simple linguistic trap的影响。这篇文章提供了 temporal commonsense reasoning 领域的研究概述，特别是通过多种扩充和其评估在不断增长的数据集上。然而，这些扩充模型仍然无法接近人类在时间常识性属性上的理解，例如事件的典型发生时间、顺序或持续时间。我们还强调需要仔细 интерпретирова research 结果，以避免因 transformers 的浅层思维而过分评估。这可以通过适当准备数据集和合适的评估指标来实现。
</details></li>
</ul>
<hr>
<h2 id="Is-this-model-reliable-for-everyone-Testing-for-strong-calibration"><a href="#Is-this-model-reliable-for-everyone-Testing-for-strong-calibration" class="headerlink" title="Is this model reliable for everyone? Testing for strong calibration"></a>Is this model reliable for everyone? Testing for strong calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15247">http://arxiv.org/abs/2307.15247</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jjfeng/testing_strong_calibration">https://github.com/jjfeng/testing_strong_calibration</a></li>
<li>paper_authors: Jean Feng, Alexej Gossmann, Romain Pirracchio, Nicholas Petrick, Gene Pennello, Berkman Sahiner</li>
<li>for: 这个论文的目的是检验一个风险预测模型的准确性，尤其是在面临各种人口群体时。</li>
<li>methods: 该论文使用了一种基于分割点检测的新测试方法，具体来说是使用一部分数据训练一系列候选模型，然后使用另一部分数据进行分数基本和累加总和测试。</li>
<li>results: 该论文的结果表明， compared to现有方法，该新测试方法在模拟研究中具有更高的检测力和在风险预测模型的准确性检测中更 than doubled the power。<details>
<summary>Abstract</summary>
In a well-calibrated risk prediction model, the average predicted probability is close to the true event rate for any given subgroup. Such models are reliable across heterogeneous populations and satisfy strong notions of algorithmic fairness. However, the task of auditing a model for strong calibration is well-known to be difficult -- particularly for machine learning (ML) algorithms -- due to the sheer number of potential subgroups. As such, common practice is to only assess calibration with respect to a few predefined subgroups. Recent developments in goodness-of-fit testing offer potential solutions but are not designed for settings with weak signal or where the poorly calibrated subgroup is small, as they either overly subdivide the data or fail to divide the data at all. We introduce a new testing procedure based on the following insight: if we can reorder observations by their expected residuals, there should be a change in the association between the predicted and observed residuals along this sequence if a poorly calibrated subgroup exists. This lets us reframe the problem of calibration testing into one of changepoint detection, for which powerful methods already exist. We begin with introducing a sample-splitting procedure where a portion of the data is used to train a suite of candidate models for predicting the residual, and the remaining data are used to perform a score-based cumulative sum (CUSUM) test. To further improve power, we then extend this adaptive CUSUM test to incorporate cross-validation, while maintaining Type I error control under minimal assumptions. Compared to existing methods, the proposed procedure consistently achieved higher power in simulation studies and more than doubled the power when auditing a mortality risk prediction model.
</details>
<details>
<summary>摘要</summary>
在一个良好准备的风险预测模型中，平均预测概率与实际事件率之间的差距相对较小，这些模型在多元人口中也是可靠的，并满足了强的算法公平性。然而，对模型的准备进行强制适应测试是一项具有挑战性的任务，尤其是 для机器学习（ML）算法，因为数据中的可能子组的数量太多。因此，常见的做法是只对一些预先定义的子组进行准备测试。然而，现有的goodness-of-fit测试技术并不适用于弱信号的情况或者小 subgroup，因为它们可能过分分解数据或者完全无法分解数据。我们提出了一种新的测试过程，基于以下想法：如果我们可以重新排序观测值，那么在这个序列中，如果存在一个不准确预测的 subgroup，那么预测和实际值之间的关系应该发生变化。这让我们可以将准备测试转化为变点检测问题，这个问题已经有强大的解决方法。我们开始于使用一种分 splitting 技术，其中一部分数据用于训练一系列候选模型，用于预测差异，剩下的数据用于performing 一种score-based cumulative sum（CUSUM）测试。为了进一步提高力量，我们然后延展这种适应 CUSUM 测试，并在保持类型 I 错误控制的情况下，进行cross-validation。与现有方法相比，我们的方法在模拟研究中一直保持高的力量，并在预测人口风险模型时，力量超过了两倍。
</details></li>
</ul>
<hr>
<h2 id="A-Practical-Recipe-for-Federated-Learning-Under-Statistical-Heterogeneity-Experimental-Design"><a href="#A-Practical-Recipe-for-Federated-Learning-Under-Statistical-Heterogeneity-Experimental-Design" class="headerlink" title="A Practical Recipe for Federated Learning Under Statistical Heterogeneity Experimental Design"></a>A Practical Recipe for Federated Learning Under Statistical Heterogeneity Experimental Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15245">http://arxiv.org/abs/2307.15245</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mmorafah/fedzoo-bench">https://github.com/mmorafah/fedzoo-bench</a></li>
<li>paper_authors: Mahdi Morafah, Weijia Wang, Bill Lin</li>
<li>for: 本研究旨在探讨 Federated Learning (FL) 在数据不一致情况下的应用，并提供一个系统性的研究方法来了解 FL 的实验设置对性能的影响。</li>
<li>methods: 本研究使用了多种 FL 算法和实验设置，包括22种state-of-the-art方法的实现，以及一系列标准化和可定制的特性。</li>
<li>results: 研究发现，FL 实验设置对性能的影响是复杂的，并且存在多种因素的交互效应。研究还提供了一些启示和建议，以帮助设计一个有意义和奖励性的 FL 实验设置。<details>
<summary>Abstract</summary>
Federated Learning (FL) has been an area of active research in recent years. There have been numerous studies in FL to make it more successful in the presence of data heterogeneity. However, despite the existence of many publications, the state of progress in the field is unknown. Many of the works use inconsistent experimental settings and there are no comprehensive studies on the effect of FL-specific experimental variables on the results and practical insights for a more comparable and consistent FL experimental setup. Furthermore, the existence of several benchmarks and confounding variables has further complicated the issue of inconsistency and ambiguity. In this work, we present the first comprehensive study on the effect of FL-specific experimental variables in relation to each other and performance results, bringing several insights and recommendations for designing a meaningful and well-incentivized FL experimental setup. We further aid the community by releasing FedZoo-Bench, an open-source library based on PyTorch with pre-implementation of 22 state-of-the-art methods, and a broad set of standardized and customizable features available at https://github.com/MMorafah/FedZoo-Bench. We also provide a comprehensive comparison of several state-of-the-art (SOTA) methods to better understand the current state of the field and existing limitations.
</details>
<details>
<summary>摘要</summary>
federaired learning（FL）已经是近年来的一个热点研究领域。有很多研究在FL中使用不同的实验设置，但是即使有很多论文，现场的进步状况还是不清楚。许多研究使用不一致的实验设置，而且没有全面的研究对FL特有的实验变量对结果和实用经验的影响。此外，存在多个标准和干扰变量，这使得问题的不一致和混乱更加严重。在这项工作中，我们提供了FL特有的实验变量的首次全面研究，探讨它们之间的关系和性能结果的影响。我们还为社区提供了FedZoo-Bench，一个基于PyTorch的开源库，包含22种当前领先的方法的预实现，以及一系列标准化和可定制的特性。此外，我们还对多种当前领先方法进行了全面的比较，以更好地了解当前领域的状况和存在的局限性。
</details></li>
</ul>
<hr>
<h2 id="Sustainable-Transparency-in-Recommender-Systems-Bayesian-Ranking-of-Images-for-Explainability"><a href="#Sustainable-Transparency-in-Recommender-Systems-Bayesian-Ranking-of-Images-for-Explainability" class="headerlink" title="Sustainable Transparency in Recommender Systems: Bayesian Ranking of Images for Explainability"></a>Sustainable Transparency in Recommender Systems: Bayesian Ranking of Images for Explainability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01196">http://arxiv.org/abs/2308.01196</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jorge Paz-Ruza, Amparo Alonso-Betanzos, Berta Guijarro-Berdiñas, Brais Cancela, Carlos Eiras-Franco</li>
<li>for: 这个论文的目的是提高个性化解释的效果，以增强推荐系统的透明度和用户信任。</li>
<li>methods: 该论文使用了视觉内容生成的用户创建的图像，并采用了搜索引擎的排名算法，以实现更高效的个性化解释。</li>
<li>results: 相比之前的模型，该模型在六个真实世界数据集上表现出了明显的优异，并且具有了更好的效率和更小的模型大小，可以降低训练和推荐过程中的碳排放。<details>
<summary>Abstract</summary>
Recommender Systems have become crucial in the modern world, commonly guiding users towards relevant content or products, and having a large influence over the decisions of users and citizens. However, ensuring transparency and user trust in these systems remains a challenge; personalized explanations have emerged as a solution, offering justifications for recommendations. Among the existing approaches for generating personalized explanations, using visual content created by the users is one particularly promising option, showing a potential to maximize transparency and user trust. Existing models for explaining recommendations in this context face limitations: sustainability has been a critical concern, as they often require substantial computational resources, leading to significant carbon emissions comparable to the Recommender Systems where they would be integrated. Moreover, most models employ surrogate learning goals that do not align with the objective of ranking the most effective personalized explanations for a given recommendation, leading to a suboptimal learning process and larger model sizes. To address these limitations, we present BRIE, a novel model designed to tackle the existing challenges by adopting a more adequate learning goal based on Bayesian Pairwise Ranking, enabling it to achieve consistently superior performance than state-of-the-art models in six real-world datasets, while exhibiting remarkable efficiency, emitting up to 75% less CO${_2}$ during training and inference with a model up to 64 times smaller than previous approaches.
</details>
<details>
<summary>摘要</summary>
为解决这些限制，我们提出了 BRIE，一种新的模型，采用基于 bayesian pairwise ranking的更适合的学习目标，使得它能够在六个实际数据集上表现出 consistently superior performance，而且具有很好的效率，在训练和推理过程中排放 CO${_2}$ 的含量可以下降至 75%，模型也可以比前一些方法小得多，达到 64 倍。
</details></li>
</ul>
<hr>
<h2 id="Open-Problems-and-Fundamental-Limitations-of-Reinforcement-Learning-from-Human-Feedback"><a href="#Open-Problems-and-Fundamental-Limitations-of-Reinforcement-Learning-from-Human-Feedback" class="headerlink" title="Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"></a>Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15217">http://arxiv.org/abs/2307.15217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphaël Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Bıyık, Anca Dragan, David Krueger, Dorsa Sadigh, Dylan Hadfield-Menell</li>
<li>for: 本研究旨在系мати化人类反馈强化学习（RLHF）的问题和限制，以及相关方法的改进和补充。</li>
<li>methods: 本研究使用了RLHF和相关方法来训练大型自然语言模型（LLM）。</li>
<li>results: 本研究提出了对RLHF系统的审核和公布标准，以提高社会监督RLHF系统的能力。<details>
<summary>Abstract</summary>
Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems.
</details>
<details>
<summary>摘要</summary>
人工智能学习增强法（RLHF）是一种训练人工智能系统与人类目标相一致的技术。RLHF已经成为现代大语言模型（LLM）的训练中心方法。尽管如此，RLHF的问题和限制尚未得到了相对较少的公共研究。在这篇论文中，我们（1）抽查RLHF和相关方法的开放问题和基本限制;（2）概述RLHF在实践中的理解、改进和补充方法;（3）提议RLHF系统的审核和披露标准，以提高社会对RLHF系统的监督。我们的工作强调RLHF的限制，并高亮了在开发更安全的人工智能系统方面需要多种方法的合作。
</details></li>
</ul>
<hr>
<h2 id="PromptStyler-Prompt-driven-Style-Generation-for-Source-free-Domain-Generalization"><a href="#PromptStyler-Prompt-driven-Style-Generation-for-Source-free-Domain-Generalization" class="headerlink" title="PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization"></a>PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15199">http://arxiv.org/abs/2307.15199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junhyeong Cho, Gilhyun Nam, Sungyeon Kim, Hunmin Yang, Suha Kwak</li>
<li>for: 本研究旨在提出一种基于文本描述的图像预测方法，能够在无需使用图像的情况下进行域广泛化。</li>
<li>methods: 该方法使用文本特征（如“狗照片”）来代表相关的图像特征，并利用cross-modal转移现象来学习多种样式。</li>
<li>results: 该方法可以在PACS、VLCS、OfficeHome和DomainNet等 dataset上达到状态之册的表现，而无需使用任何图像进行训练。<details>
<summary>Abstract</summary>
In a joint vision-language space, a text feature (e.g., from "a photo of a dog") could effectively represent its relevant image features (e.g., from dog photos). Also, a recent study has demonstrated the cross-modal transferability phenomenon of this joint space. From these observations, we propose PromptStyler which simulates various distribution shifts in the joint space by synthesizing diverse styles via prompts without using any images to deal with source-free domain generalization. The proposed method learns to generate a variety of style features (from "a S* style of a") via learnable style word vectors for pseudo-words S*. To ensure that learned styles do not distort content information, we force style-content features (from "a S* style of a [class]") to be located nearby their corresponding content features (from "[class]") in the joint vision-language space. After learning style word vectors, we train a linear classifier using synthesized style-content features. PromptStyler achieves the state of the art on PACS, VLCS, OfficeHome and DomainNet, even though it does not require any images for training.
</details>
<details>
<summary>摘要</summary>
在共同视觉语言空间中，文本特征（如从“一张狗照片”）可以有效表示相关的图像特征（如狗照片中的特征）。此外，一项研究还证明了这个共同空间的跨Modal传送现象。从这些观察结果，我们提出了PromptStyler，它在这个共同空间中模拟了多种分布转移，使用提示而不需要使用任何图像来处理源无法适应化。我们的方法学习生成多种风格特征（如“a S* 风格的”）的learnable风格词Vector，以便在合理的领域内生成多种风格特征。为保证学习的风格特征不会扭曲内容信息，我们强制在共同视觉语言空间中的风格-内容特征（如“a S* 风格的 [类]”）与其相应的内容特征（如“[类]”）之间的距离尽量小。之后，我们使用生成的风格-内容特征进行线性分类训练，PromptStyler在PACS、VLCS、OfficeHome和DomainNet上达到了最佳状态，即使不需要任何图像进行训练。
</details></li>
</ul>
<hr>
<h2 id="Identifying-acute-illness-phenotypes-via-deep-temporal-interpolation-and-clustering-network-on-physiologic-signatures"><a href="#Identifying-acute-illness-phenotypes-via-deep-temporal-interpolation-and-clustering-network-on-physiologic-signatures" class="headerlink" title="Identifying acute illness phenotypes via deep temporal interpolation and clustering network on physiologic signatures"></a>Identifying acute illness phenotypes via deep temporal interpolation and clustering network on physiologic signatures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15719">http://arxiv.org/abs/2307.15719</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanfang Ren, Yanjun Li, Tyler J. Loftus, Jeremy Balch, Kenneth L. Abbott, Shounak Datta, Matthew M. Ruppert, Ziyuan Guan, Benjamin Shickel, Parisa Rashidi, Tezcan Ozrazgat-Baslanti, Azra Bihorac<br>for:This paper aims to identify distinct patient phenotypes based on vital sign data within the first six hours of hospital admission, with the goal of supporting early clinical decisions and improving patient outcomes.methods:The authors used a deep temporal interpolation and clustering network to analyze vital sign data from a single-center, longitudinal EHR dataset of 75,762 adults admitted to a tertiary care center for at least six hours. They derived four distinct patient phenotypes based on patterns of vital sign data.results:The authors found that the four patient phenotypes had distinct categories of disease and outcomes. Phenotype A had the most comorbid diseases and a higher rate of prolonged respiratory insufficiency, acute kidney injury, sepsis, and three-year mortality. Phenotypes B and C had mild organ dysfunction, while Phenotype D had early and persistent hypotension and a high rate of early surgery. The clustering results did not simply repeat other acuity assessments, and the tool may impact triage decisions and clinical decision-support under time constraints.<details>
<summary>Abstract</summary>
Initial hours of hospital admission impact clinical trajectory, but early clinical decisions often suffer due to data paucity. With clustering analysis for vital signs within six hours of admission, patient phenotypes with distinct pathophysiological signatures and outcomes may support early clinical decisions. We created a single-center, longitudinal EHR dataset for 75,762 adults admitted to a tertiary care center for 6+ hours. We proposed a deep temporal interpolation and clustering network to extract latent representations from sparse, irregularly sampled vital sign data and derived distinct patient phenotypes in a training cohort (n=41,502). Model and hyper-parameters were chosen based on a validation cohort (n=17,415). Test cohort (n=16,845) was used to analyze reproducibility and correlation with biomarkers. The training, validation, and testing cohorts had similar distributions of age (54-55 yrs), sex (55% female), race, comorbidities, and illness severity. Four clusters were identified. Phenotype A (18%) had most comorbid disease with higher rate of prolonged respiratory insufficiency, acute kidney injury, sepsis, and three-year mortality. Phenotypes B (33%) and C (31%) had diffuse patterns of mild organ dysfunction. Phenotype B had favorable short-term outcomes but second-highest three-year mortality. Phenotype C had favorable clinical outcomes. Phenotype D (17%) had early/persistent hypotension, high rate of early surgery, and substantial biomarker rate of inflammation but second-lowest three-year mortality. After comparing phenotypes' SOFA scores, clustering results did not simply repeat other acuity assessments. In a heterogeneous cohort, four phenotypes with distinct categories of disease and outcomes were identified by a deep temporal interpolation and clustering network. This tool may impact triage decisions and clinical decision-support under time constraints.
</details>
<details>
<summary>摘要</summary>
<<SYS>> hospital 入院初期影响临床轨迹，但早期临床决策 often 受到数据缺乏的困扰。通过在入院第六个小时内对生命 Parameters进行集成分析，患者可能会被分为不同的疾病类型和结果。我们在一所三级医疗机构中收录了75,762名成人的长期电子医疗纪录（EHR）数据，并提出了一种深度时间 interpolate 和 clustering 网络，以EXTRACT 缺乏的生命 Parameters 数据中的潜在表示。我们在训练集（n=41,502）中提出了四种不同的患者类型，其中每种类型都有明确的疾病特征和结果。我们根据验证集（n=17,415）中的模型和 гиперпараметры进行选择。测试集（n=16,845）用于确认可重复性和与生物标志物相关性。训练、验证和测试集中年龄（54-55岁）、性别（55%为女性）、种族、合并疾病和疾病严重程度具有类似分布。四种类型中的第一个（18%）有最多的相关疾病和高得 respiratory insufficiency、肾脏损害、 septic shock 和三年 mortality。第二个类型（33%）和第三个类型（31%）有散发性轻度器官功能不全的特征。第二个类型（33%）在短期内有 favorable 的临床结果，但在三年内 mortality 第二高。第三个类型（31%）有 favorable 的临床结果。第四个类型（17%）有早期/持续低血压、高 rate of early surgery 和严重的Inflammation 水平，但在三年内 mortality 第二低。在一个多样化的人群中，这种工具可能会影响抢救决策和临床决策支持系统。
</details></li>
</ul>
<hr>
<h2 id="The-Marginal-Value-of-Momentum-for-Small-Learning-Rate-SGD"><a href="#The-Marginal-Value-of-Momentum-for-Small-Learning-Rate-SGD" class="headerlink" title="The Marginal Value of Momentum for Small Learning Rate SGD"></a>The Marginal Value of Momentum for Small Learning Rate SGD</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15196">http://arxiv.org/abs/2307.15196</a></li>
<li>repo_url: None</li>
<li>paper_authors: Runzhe Wang, Sadhika Malladi, Tianhao Wang, Kaifeng Lyu, Zhiyuan Li</li>
<li>for: 该文章目的是为了解释权重动量在随机优化中的作用。</li>
<li>methods: 该文章使用了权重动量的方法来优化神经网络。</li>
<li>results: 实验表明，权重动量在实际训练 régime中没有明显的提升作用，尤其是在小学习率下。<details>
<summary>Abstract</summary>
Momentum is known to accelerate the convergence of gradient descent in strongly convex settings without stochastic gradient noise. In stochastic optimization, such as training neural networks, folklore suggests that momentum may help deep learning optimization by reducing the variance of the stochastic gradient update, but previous theoretical analyses do not find momentum to offer any provable acceleration. Theoretical results in this paper clarify the role of momentum in stochastic settings where the learning rate is small and gradient noise is the dominant source of instability, suggesting that SGD with and without momentum behave similarly in the short and long time horizons. Experiments show that momentum indeed has limited benefits for both optimization and generalization in practical training regimes where the optimal learning rate is not very large, including small- to medium-batch training from scratch on ImageNet and fine-tuning language models on downstream tasks.
</details>
<details>
<summary>摘要</summary>
偏好是知道可以加速凸降谷的梯度搜索在强度 convex 的设置中，不包括随机梯度噪声。在随机优化中，如训练神经网络，民间传说表示偏好可以减少随机梯度更新的幅度，但以前的理论分析不能找到偏好提供任何可证明的加速。本文的理论结果解释了偏好在随机设置中的作用，表明在小学习率下，偏好不会提供任何可证明的加速。实验表明，偏好对优化和泛化在实际训练 режи具有有限的 benefita，包括从scratch开始训练 ImageNet 和 fine-tuning 语言模型在下游任务上。
</details></li>
</ul>
<hr>
<h2 id="Learning-in-Repeated-Multi-Unit-Pay-As-Bid-Auctions"><a href="#Learning-in-Repeated-Multi-Unit-Pay-As-Bid-Auctions" class="headerlink" title="Learning in Repeated Multi-Unit Pay-As-Bid Auctions"></a>Learning in Repeated Multi-Unit Pay-As-Bid Auctions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15193">http://arxiv.org/abs/2307.15193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rigel Galgana, Negin Golrezaei</li>
<li>For: The paper is written to address the problem of learning how to bid in repeated multi-unit pay-as-bid auctions, with the goal of achieving a high revenue for the seller while also maximizing the welfare of the bidders.* Methods: The paper uses dynamic programming (DP) to obtain the optimal solution to the offline problem, and leverages the structure of the DP scheme to design online learning algorithms with polynomial time and space complexity under full information and bandit feedback settings.* Results: The paper achieves an upper bound on regret of $O(M\sqrt{T\log |\mathcal{B}|})$ and $O(M\sqrt{|\mathcal{B}|T\log |\mathcal{B}|})$ respectively, and demonstrates through numerical results that the resulting market dynamics mainly converge to a welfare maximizing equilibrium where bidders submit uniform bids. Additionally, the paper shows that the pay-as-bid auction consistently generates significantly higher revenue compared to its popular alternative, the uniform price auction.Here is the answer in Simplified Chinese text:* For: 本文是为了解决 repeat 的 multi-unit pay-as-bid 拍卖中的投标问题，以实现卖家获得高收益，同时 maximize 投标者的利益。* Methods: 本文使用动态规划（DP）获取 offline 问题的优化解决方案，并利用 DP 的结构设计在全信息和抽象反馈下的在线学习算法，其时间复杂度和空间复杂度均为多阶 polynomial。* Results: 本文达到了 regret 的Upper bound为 $O(M\sqrt{T\log |\mathcal{B}|})$ 和 $O(M\sqrt{|\mathcal{B}|T\log |\mathcal{B}|})$，并通过数学结果表明，market dynamics 在 converge 到一个利益最大化的均衡点，在这个点上，投标者都会提交 uniform 投标。此外，文章还证明了 pay-as-bid 拍卖在 popular 的 uniform price 拍卖上consistently 获得更高的收益。<details>
<summary>Abstract</summary>
Motivated by Carbon Emissions Trading Schemes, Treasury Auctions, and Procurement Auctions, which all involve the auctioning of homogeneous multiple units, we consider the problem of learning how to bid in repeated multi-unit pay-as-bid auctions. In each of these auctions, a large number of (identical) items are to be allocated to the largest submitted bids, where the price of each of the winning bids is equal to the bid itself. The problem of learning how to bid in pay-as-bid auctions is challenging due to the combinatorial nature of the action space. We overcome this challenge by focusing on the offline setting, where the bidder optimizes their vector of bids while only having access to the past submitted bids by other bidders. We show that the optimal solution to the offline problem can be obtained using a polynomial time dynamic programming (DP) scheme. We leverage the structure of the DP scheme to design online learning algorithms with polynomial time and space complexity under full information and bandit feedback settings. We achieve an upper bound on regret of $O(M\sqrt{T\log |\mathcal{B}|})$ and $O(M\sqrt{|\mathcal{B}|T\log |\mathcal{B}|})$ respectively, where $M$ is the number of units demanded by the bidder, $T$ is the total number of auctions, and $|\mathcal{B}|$ is the size of the discretized bid space. We accompany these results with a regret lower bound, which match the linear dependency in $M$. Our numerical results suggest that when all agents behave according to our proposed no regret learning algorithms, the resulting market dynamics mainly converge to a welfare maximizing equilibrium where bidders submit uniform bids. Lastly, our experiments demonstrate that the pay-as-bid auction consistently generates significantly higher revenue compared to its popular alternative, the uniform price auction.
</details>
<details>
<summary>摘要</summary>
驱动 by 碳排放交易制度、储蓄拍卖和购买拍卖，这些拍卖都涉及到多个单位的拍卖，我们考虑了如何在重复的多个单位支付-为-拍卖中学习投标。在每个拍卖中，大量（相同）的物品需要分配给最大提交投标价格为每个赢得拍卖价格。在支付-为-拍卖问题上学习投标是具有 combinatorial 特性的挑战。我们通过关注线上设置来解决这个挑战，bidder 在仅有过去其他投标者的投标历史的情况下优化其投标向量。我们表明了在线时间DP（动态编程）算法可以在 polynomial 时间内解决这个问题。我们利用DP算法的结构来设计在线学习算法，其时间复杂度和存储空间复杂度均为polynomial。在全信息和抽象反馈设置下，我们实现了对 regret 的上限为 $O(M\sqrt{T\log |\mathcal{B}|})$ 和 $O(M\sqrt{|\mathcal{B}|T\log |\mathcal{B}|})$，其中 $M$ 是投标者需要的单位数量， $T$ 是总共 auctions 数量，并且 $|\mathcal{B}|$ 是投标向量空间的大小。我们的数学结果表明，当所有代理人按照我们所提议的无回归学习算法行为时，市场动力主要 converge 到一个减少最大化的均衡，其中投标者会提交均匀投标。最后，我们的实验表明，支付-为-拍卖拍卖在许多情况下可以生成较高的收益，相比于它的流行替代方案 uniform 价格拍卖。
</details></li>
</ul>
<hr>
<h2 id="f-Divergence-Minimization-for-Sequence-Level-Knowledge-Distillation"><a href="#f-Divergence-Minimization-for-Sequence-Level-Knowledge-Distillation" class="headerlink" title="f-Divergence Minimization for Sequence-Level Knowledge Distillation"></a>f-Divergence Minimization for Sequence-Level Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15190">http://arxiv.org/abs/2307.15190</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/manga-uofa/fdistill">https://github.com/manga-uofa/fdistill</a></li>
<li>paper_authors: Yuqiao Wen, Zichao Li, Wenyu Du, Lili Mou</li>
<li>for: 本文提出了一种基于 generalized f-divergence 函数的序列级知识填充框架 (f-DISTILL)，用于实现语言处理领域中的知识填充。</li>
<li>methods: 本文提出了四种基于 f-DISTILL 框架的知识填充方法，并证明了现有的 SeqKD 和 ENGINE 方法是 f-DISTILL 方法的近似。此外，本文还提出了一种步骤 decomposition 方法，将不可算式的序列级分割转换为可算式的单词级损失函数。</li>
<li>results: 实验结果表明，本文提出的方法比现有的知识填充方法高效，并且使用 симметричного 填充损失函数可以更好地让学生模型学习教师分布。<details>
<summary>Abstract</summary>
Knowledge distillation (KD) is the process of transferring knowledge from a large model to a small one. It has gained increasing attention in the natural language processing community, driven by the demands of compressing ever-growing language models. In this work, we propose an f-DISTILL framework, which formulates sequence-level knowledge distillation as minimizing a generalized f-divergence function. We propose four distilling variants under our framework and show that existing SeqKD and ENGINE approaches are approximations of our f-DISTILL methods. We further derive step-wise decomposition for our f-DISTILL, reducing intractable sequence-level divergence to word-level losses that can be computed in a tractable manner. Experiments across four datasets show that our methods outperform existing KD approaches, and that our symmetric distilling losses can better force the student to learn from the teacher distribution.
</details>
<details>
<summary>摘要</summary>
知识填充（KD）是将知识从大型模型传递到小型模型的过程。随着自然语言处理领域中模型的增长，KD技术在受到压力，以压缩语言模型的规模。在这项工作中，我们提出了f-DISTILL框架，将序列级知识填充形式化为最小化一个通用f-散度函数。我们提出了四种填充变种，并证明了现有的SeqKD和ENGINE方法是我们f-DISTILL方法的近似方法。我们还 deriv了step-wise decomposition，将不可贪算的序列级散度 decomposed into可计算的单词级损失。实验结果表明，我们的方法在四个数据集上表现比现有的KD方法更好，并且我们的对称填充损失可以更好地让学生学习教师分布。
</details></li>
</ul>
<hr>
<h2 id="Rotation-Invariant-Random-Features-Provide-a-Strong-Baseline-for-Machine-Learning-on-3D-Point-Clouds"><a href="#Rotation-Invariant-Random-Features-Provide-a-Strong-Baseline-for-Machine-Learning-on-3D-Point-Clouds" class="headerlink" title="Rotation-Invariant Random Features Provide a Strong Baseline for Machine Learning on 3D Point Clouds"></a>Rotation-Invariant Random Features Provide a Strong Baseline for Machine Learning on 3D Point Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06271">http://arxiv.org/abs/2308.06271</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/meliao/rotation-invariant-random-features">https://github.com/meliao/rotation-invariant-random-features</a></li>
<li>paper_authors: Owen Melia, Eric Jonas, Rebecca Willett</li>
<li>for:  This paper is written for researchers and practitioners in the field of machine learning, particularly those interested in rotation-invariant methods for 3D point cloud data.</li>
<li>methods:  The paper proposes a simple and general-purpose method for learning rotation-invariant functions of 3D point cloud data using a random features approach. The method is based on the random features method of Rahimi &amp; Recht (2007) and is extended to be invariant to three-dimensional rotations.</li>
<li>results:  The paper shows through experiments that the proposed method matches or outperforms the performance of general-purpose rotation-invariant neural networks on standard molecular property prediction benchmark datasets QM7 and QM9. Additionally, the method is general-purpose and provides a rotation-invariant baseline on the ModelNet40 shape classification task, and has an order of magnitude smaller prediction latency than competing kernel methods.Here is the result in Simplified Chinese text:</li>
<li>for: 这篇论文是为了研究机器学习领域的研究者和实践者而写的，特别是关心 rotate-invariant 方法的人。</li>
<li>methods: 该篇论文提出了一种简单而通用的方法，用于学习三维点云数据上的 rotate-invariant 函数，使用随机特征方法。</li>
<li>results: 论文通过实验表明，提议的方法与通用的 rotate-invariant 神经网络相比，在标准分子性质预测标准 datasets QM7 和 QM9 上具有相同或更高的性能，并且在 ModelNet40 形态分类任务上提供了一个 rotate-invariant 基准。此外，该方法的预测延迟只有一个数量级的比较小于竞争性kernel方法。<details>
<summary>Abstract</summary>
Rotational invariance is a popular inductive bias used by many fields in machine learning, such as computer vision and machine learning for quantum chemistry. Rotation-invariant machine learning methods set the state of the art for many tasks, including molecular property prediction and 3D shape classification. These methods generally either rely on task-specific rotation-invariant features, or they use general-purpose deep neural networks which are complicated to design and train. However, it is unclear whether the success of these methods is primarily due to the rotation invariance or the deep neural networks. To address this question, we suggest a simple and general-purpose method for learning rotation-invariant functions of three-dimensional point cloud data using a random features approach. Specifically, we extend the random features method of Rahimi & Recht 2007 by deriving a version that is invariant to three-dimensional rotations and showing that it is fast to evaluate on point cloud data. We show through experiments that our method matches or outperforms the performance of general-purpose rotation-invariant neural networks on standard molecular property prediction benchmark datasets QM7 and QM9. We also show that our method is general-purpose and provides a rotation-invariant baseline on the ModelNet40 shape classification task. Finally, we show that our method has an order of magnitude smaller prediction latency than competing kernel methods.
</details>
<details>
<summary>摘要</summary>
“旋转协变是机器学习领域中广泛使用的抽象假设，如计算机视觉和量子化学机器学习。旋转不变的机器学习方法在许多任务上设置了状态的典范，包括分子性质预测和3D形状分类。这些方法通常是通过任务特定的旋转不变特征或通用的深度神经网络来实现的，后者具有复杂的设计和训练问题。然而，是否 rotation协变的成功主要归功于旋转不变性还是深度神经网络仍然存在一定的问题。为了解决这个问题，我们提出了一种简单和通用的方法来学习三维点云数据上的旋转不变函数，使用随机特征方法。 Specifically, we extend the random features method of Rahimi & Recht 2007 by deriving a version that is invariant to three-dimensional rotations and showing that it is fast to evaluate on point cloud data. Through experiments, we show that our method matches or outperforms the performance of general-purpose rotation-invariant neural networks on standard molecular property prediction benchmark datasets QM7 and QM9. We also show that our method is general-purpose and provides a rotation-invariant baseline on the ModelNet40 shape classification task. Finally, we show that our method has an order of magnitude smaller prediction latency than competing kernel methods.”Note: Please keep in mind that the translation is in Simplified Chinese, and the grammar and vocabulary may be different from Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="RCT-Rejection-Sampling-for-Causal-Estimation-Evaluation"><a href="#RCT-Rejection-Sampling-for-Causal-Estimation-Evaluation" class="headerlink" title="RCT Rejection Sampling for Causal Estimation Evaluation"></a>RCT Rejection Sampling for Causal Estimation Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15176">http://arxiv.org/abs/2307.15176</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kakeith/rct_rejection_sampling">https://github.com/kakeith/rct_rejection_sampling</a></li>
<li>paper_authors: Katherine A. Keith, Sergey Feldman, David Jurgens, Jonathan Bragg, Rohit Bhattacharya</li>
<li>For: The paper aims to evaluate the effectiveness of adjusting for confounding in observational data using machine learning methods for causal estimation, specifically in high-dimensional covariate settings such as text data, genomics, and the behavioral social sciences.* Methods: The paper proposes a new sampling algorithm called RCT rejection sampling, which uses subsampling randomized controlled trials (RCTs) to create confounded observational datasets and compares the causal effects from the RCTs to those from the observational data. The algorithm is designed to provide theoretical guarantees of causal identification and low bias in the estimation of causal effects.* Results: The paper shows that the proposed algorithm results in low bias when evaluated on synthetic data, and provides finite data considerations for evaluation designers who plan to use RCT rejection sampling on their own datasets. Additionally, the paper presents a novel, real-world RCT consisting of approximately 70k observations and text data as high-dimensional covariates, which serves as a proof of concept for the proposed method.<details>
<summary>Abstract</summary>
Confounding is a significant obstacle to unbiased estimation of causal effects from observational data. For settings with high-dimensional covariates -- such as text data, genomics, or the behavioral social sciences -- researchers have proposed methods to adjust for confounding by adapting machine learning methods to the goal of causal estimation. However, empirical evaluation of these adjustment methods has been challenging and limited. In this work, we build on a promising empirical evaluation strategy that simplifies evaluation design and uses real data: subsampling randomized controlled trials (RCTs) to create confounded observational datasets while using the average causal effects from the RCTs as ground-truth. We contribute a new sampling algorithm, which we call RCT rejection sampling, and provide theoretical guarantees that causal identification holds in the observational data to allow for valid comparisons to the ground-truth RCT. Using synthetic data, we show our algorithm indeed results in low bias when oracle estimators are evaluated on the confounded samples, which is not always the case for a previously proposed algorithm. In addition to this identification result, we highlight several finite data considerations for evaluation designers who plan to use RCT rejection sampling on their own datasets. As a proof of concept, we implement an example evaluation pipeline and walk through these finite data considerations with a novel, real-world RCT -- which we release publicly -- consisting of approximately 70k observations and text data as high-dimensional covariates. Together, these contributions build towards a broader agenda of improved empirical evaluation for causal estimation.
</details>
<details>
<summary>摘要</summary>
干扰是观察数据中 causal 效应的重要障碍。在高维 covariates 的设置下（如文本数据、 genomics 或行为社会科学），研究人员已经提出了适应机器学习方法以实现 causal 估计的方法。然而，实际评估这些调整方法的问题具有挑战性和局限性。在这种工作中，我们基于一种有前途的评估策略，即使用 randomized controlled trials (RCTs) 来生成干扰 Observational 数据，并使用 RCTs 的平均 causal 效应作为真实参照值。我们提出了一种新的抽样算法，称为 RCT 拒绝抽样，并提供了理论保证，表明在观察数据中， causal 识别存在。使用 sintetic 数据，我们表明我们的算法确实具有低偏度特性，当 oracle 估计器在干扰样本上进行评估时。此外，我们还高亮了评估设计师在使用 RCT 拒绝抽样时需要考虑的一些finite data 问题。作为证明，我们实现了一个示例评估管线，并详细介绍了这些finite data 问题。此外，我们还公开发布了一个 novel 的实际 RCT，包含约70k 个观察值和文本数据作为高维 covariates。总的来说，这些贡献共同推动了观察数据中 causal 估计的有效评估。
</details></li>
</ul>
<hr>
<h2 id="Causative-Cyberattacks-on-Online-Learning-based-Automated-Demand-Response-Systems"><a href="#Causative-Cyberattacks-on-Online-Learning-based-Automated-Demand-Response-Systems" class="headerlink" title="Causative Cyberattacks on Online Learning-based Automated Demand Response Systems"></a>Causative Cyberattacks on Online Learning-based Automated Demand Response Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15175">http://arxiv.org/abs/2307.15175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samrat Acharya, Yury Dvorkin, Ramesh Karri</li>
<li>for: 这 paper 是 investigate AI-based Demand Response (DR) 系统的安全性问题，具体来说是研究用户数据的敏感性和攻击者可能的攻击方式。</li>
<li>methods: 这 paper 使用了人工智能 (AI) 技术来学习用户的能源消耗习惯，并使用这些知识来设计最佳的 DR 激励方案。</li>
<li>results: 研究发现了 AI-based DR 系统的敏感性问题，并提出了一种基于实际数据的攻击策略，包括 manipulate real-time DR 激励、DR 事件数据传输和 DR 用户响应的攻击。<details>
<summary>Abstract</summary>
Power utilities are adopting Automated Demand Response (ADR) to replace the costly fuel-fired generators and to preempt congestion during peak electricity demand. Similarly, third-party Demand Response (DR) aggregators are leveraging controllable small-scale electrical loads to provide on-demand grid support services to the utilities. Some aggregators and utilities have started employing Artificial Intelligence (AI) to learn the energy usage patterns of electricity consumers and use this knowledge to design optimal DR incentives. Such AI frameworks use open communication channels between the utility/aggregator and the DR customers, which are vulnerable to \textit{causative} data integrity cyberattacks. This paper explores vulnerabilities of AI-based DR learning and designs a data-driven attack strategy informed by DR data collected from the New York University (NYU) campus buildings. The case study demonstrates the feasibility and effects of maliciously tampering with (i) real-time DR incentives, (ii) DR event data sent to DR customers, and (iii) responses of DR customers to the DR incentives.
</details>
<details>
<summary>摘要</summary>
电力公司正在采用自动化需求应答（ADR）来取代昂贵的燃料发电机和预防峰值电力需求压力。同时，第三方需求应答（DR）聚集者也在利用可控的小规模电力负荷来为Utilities提供即时电力支持服务。一些聚集者和Utilities已经开始使用人工智能（AI）来学习电力消耗者的能源使用模式，并使用这些知识来设计优化的DR激励计划。这些AI框架使用Utility/聚集者和DR客户之间的开放通信频道，这些通信频道容易受到 causative 数据完整性攻击。这篇论文探讨了 AI 基本 DR 学习的漏洞，并设计了基于 DR 数据的数据驱动攻击策略。 case study 显示了在 NYU 校园建筑物上发生的攻击的可行性和效果，包括（i）实时 DR 激励，（ii）DR 事件数据在 DR 客户身上发送，以及（iii）DR 客户对 DR 激励的应答。
</details></li>
</ul>
<hr>
<h2 id="PredictChain-Empowering-Collaboration-and-Data-Accessibility-for-AI-in-a-Decentralized-Blockchain-based-Marketplace"><a href="#PredictChain-Empowering-Collaboration-and-Data-Accessibility-for-AI-in-a-Decentralized-Blockchain-based-Marketplace" class="headerlink" title="PredictChain: Empowering Collaboration and Data Accessibility for AI in a Decentralized Blockchain-based Marketplace"></a>PredictChain: Empowering Collaboration and Data Accessibility for AI in a Decentralized Blockchain-based Marketplace</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15168">http://arxiv.org/abs/2307.15168</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai-and-blockchain/s23_predictchain">https://github.com/ai-and-blockchain/s23_predictchain</a></li>
<li>paper_authors: Matthew T. Pisano, Connor J. Patterson, Oshani Seneviratne</li>
<li>for: 这个论文旨在提供一个以区块链技术为基础的预测机器学习模型市场（PredictChain），以解决限制存储资源和训练数据的问题，并促进预测机器学习模型的共享和应用。</li>
<li>methods: 这个论文使用了区块链技术，建立了一个分布式的机器学习模型市场，让用户上传数据进行训练，或者向已训练的模型提出查询。 nodes within the blockchain network 将运行这些模型，并且提供一些典型的机器学习模型，例如成本、速度、简洁、能力和成本效益。</li>
<li>results: 这个论文的结果显示，PredictChain 可以提供一个可靠、安全、开放的机器学习模型市场，促进数据共享和模型的应用，并且可以减少依赖中央云端提供商。<details>
<summary>Abstract</summary>
Limited access to computing resources and training data poses significant challenges for individuals and groups aiming to train and utilize predictive machine learning models. Although numerous publicly available machine learning models exist, they are often unhosted, necessitating end-users to establish their computational infrastructure. Alternatively, these models may only be accessible through paid cloud-based mechanisms, which can prove costly for general public utilization. Moreover, model and data providers require a more streamlined approach to track resource usage and capitalize on subsequent usage by others, both financially and otherwise. An effective mechanism is also lacking to contribute high-quality data for improving model performance. We propose a blockchain-based marketplace called "PredictChain" for predictive machine-learning models to address these issues. This marketplace enables users to upload datasets for training predictive machine learning models, request model training on previously uploaded datasets, or submit queries to trained models. Nodes within the blockchain network, equipped with available computing resources, will operate these models, offering a range of archetype machine learning models with varying characteristics, such as cost, speed, simplicity, power, and cost-effectiveness. This decentralized approach empowers users to develop improved models accessible to the public, promotes data sharing, and reduces reliance on centralized cloud providers.
</details>
<details>
<summary>摘要</summary>
限制式计算资源和训练数据对个人和组织来说是训练和使用预测机器学习模型的主要挑战。虽然有很多公共可用的机器学习模型存在，但它们 часто是无主的，需要用户建立自己的计算基础设施。另外，这些模型可能只通过付费的云计算机制提供，这可能对普通用户来说是昂贵的。此外，模型和数据提供者需要一种更加流畅的方式来跟踪资源使用并从其他人的使用中获得经济和其他形式的回报。此外，一种有效的数据贡献机制也缺乏，以提高模型性能。我们提议一个基于区块链的市场平台，称为“预测链”（PredictChain），以解决这些问题。这个市场平台允许用户上传数据集用于训练预测机器学习模型，请求已上传数据集上的模型训练，或者提交已训练的模型来解决问题。节点 dentro el red de blockchain, equipados con recursos de cálculo disponibles, operarán estos modelos, ofreciendo una variedad de modelos de aprendizaje automático de arquetipo, como cost, velocidad, sencillez, poder y eficiencia de costos. Este enfoque descentralizado permite a los usuarios desarrollar modelos mejorados que sean accesibles al público en general, fomenta la compartición de datos y reduce la dependencia de los proveedores de nubes centralizados.
</details></li>
</ul>
<hr>
<h2 id="VISU-at-WASSA-2023-Shared-Task-Detecting-Emotions-in-Reaction-to-News-Stories-Leveraging-BERT-and-Stacked-Embeddings"><a href="#VISU-at-WASSA-2023-Shared-Task-Detecting-Emotions-in-Reaction-to-News-Stories-Leveraging-BERT-and-Stacked-Embeddings" class="headerlink" title="VISU at WASSA 2023 Shared Task: Detecting Emotions in Reaction to News Stories Leveraging BERT and Stacked Embeddings"></a>VISU at WASSA 2023 Shared Task: Detecting Emotions in Reaction to News Stories Leveraging BERT and Stacked Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15164">http://arxiv.org/abs/2307.15164</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vivek Kumar, Sushmita Singh, Prayag Tiwari</li>
<li>for: 本研究旨在开发深度学习模型，用于从新闻文章中检测情感表达。</li>
<li>methods: 该研究使用了word embedding表示法，并采用了适应性的预处理策略，以捕捉情感表达中的细节。实验使用了静态和上下文嵌入（个体和堆叠），以及BIaLSTM和Transformer模型。</li>
<li>results: 本研究在WASSA 2023共享任务中排名第十，Macro F1-Score为0.2717，证明了实施的方法在小型和不均衡数据集上的有效性。<details>
<summary>Abstract</summary>
Our system, VISU, participated in the WASSA 2023 Shared Task (3) of Emotion Classification from essays written in reaction to news articles. Emotion detection from complex dialogues is challenging and often requires context/domain understanding. Therefore in this research, we have focused on developing deep learning (DL) models using the combination of word embedding representations with tailored prepossessing strategies to capture the nuances of emotions expressed. Our experiments used static and contextual embeddings (individual and stacked) with Bidirectional Long short-term memory (BiLSTM) and Transformer based models. We occupied rank tenth in the emotion detection task by scoring a Macro F1-Score of 0.2717, validating the efficacy of our implemented approaches for small and imbalanced datasets with mixed categories of target emotions.
</details>
<details>
<summary>摘要</summary>
我们的系统，VISU，参与了2023年WASSA分享任务（3）的情感分类从新闻文章的反应文章中。情感检测从复杂对话中是挑战性的，因此在这些研究中，我们专注于开发深度学习（DL）模型，使用Word embedding表示法和定制的预处理策略来捕捉表达出的情感细节。我们的实验使用静态和上下文嵌入（个人和堆叠），以及BiLSTM和Transformer基于模型。我们在情感检测任务中占据了第十名，得分 macro F1-Score为0.2717，证明了我们实施的方法在小样本和杂种类目的情感检测任务中的效果。
</details></li>
</ul>
<hr>
<h2 id="R-LPIPS-An-Adversarially-Robust-Perceptual-Similarity-Metric"><a href="#R-LPIPS-An-Adversarially-Robust-Perceptual-Similarity-Metric" class="headerlink" title="R-LPIPS: An Adversarially Robust Perceptual Similarity Metric"></a>R-LPIPS: An Adversarially Robust Perceptual Similarity Metric</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15157">http://arxiv.org/abs/2307.15157</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/saraghazanfari/r-lpips">https://github.com/saraghazanfari/r-lpips</a></li>
<li>paper_authors: Sara Ghazanfari, Siddharth Garg, Prashanth Krishnamurthy, Farshad Khorrami, Alexandre Araujo</li>
<li>for: 这个论文的目的是提出一种robust learned perceptual image patch similarity（R-LPIPS）度量，以提高计算机视觉中的相似性度量的安全性。</li>
<li>methods: 这个论文使用了经过 adversarial 训练的深度特征来提出 R-LPIPS 度量，并通过了一系列实验证明 R-LPIPS 度量的超越性。</li>
<li>results: 论文通过了一系列实验证明，R-LPIPS 度量可以更好地抵抗针对图像的黑客攻击，并且在大规模应用中具有更高的安全性。<details>
<summary>Abstract</summary>
Similarity metrics have played a significant role in computer vision to capture the underlying semantics of images. In recent years, advanced similarity metrics, such as the Learned Perceptual Image Patch Similarity (LPIPS), have emerged. These metrics leverage deep features extracted from trained neural networks and have demonstrated a remarkable ability to closely align with human perception when evaluating relative image similarity. However, it is now well-known that neural networks are susceptible to adversarial examples, i.e., small perturbations invisible to humans crafted to deliberately mislead the model. Consequently, the LPIPS metric is also sensitive to such adversarial examples. This susceptibility introduces significant security concerns, especially considering the widespread adoption of LPIPS in large-scale applications. In this paper, we propose the Robust Learned Perceptual Image Patch Similarity (R-LPIPS) metric, a new metric that leverages adversarially trained deep features. Through a comprehensive set of experiments, we demonstrate the superiority of R-LPIPS compared to the classical LPIPS metric. The code is available at https://github.com/SaraGhazanfari/R-LPIPS.
</details>
<details>
<summary>摘要</summary>
Computer vision 领域内，相似度度量有着重要的作用，用于捕捉图像的含义。近年来，高级相似度度量，如学习后的图像特征相似度度量（LPIPS），得到了广泛应用。这些度量利用训练过的神经网络提取出来的深度特征，并在评估图像相似度时表现出了人类观察者的很好的一致性。然而，现在已经广泛接受的是，神经网络受到了针对性攻击（adversarial examples）的影响，即小于人类可见的干扰被辅助到模型中，以诱导模型进行误分类。这种攻击引入了重要的安全问题，尤其是在大规模应用中。在本文中，我们提出了一种robust learned perceptual image patch similarity（R-LPIPS）度量，该度量利用针对性训练的深度特征。通过一系列实验，我们证明了R-LPIPS度量比 классиical LPIPS度量更加高效。代码可以在 GitHub 上找到：https://github.com/SaraGhazanfari/R-LPIPS。
</details></li>
</ul>
<hr>
<h2 id="A-B-Testing-and-Best-arm-Identification-for-Linear-Bandits-with-Robustness-to-Non-stationarity"><a href="#A-B-Testing-and-Best-arm-Identification-for-Linear-Bandits-with-Robustness-to-Non-stationarity" class="headerlink" title="A&#x2F;B Testing and Best-arm Identification for Linear Bandits with Robustness to Non-stationarity"></a>A&#x2F;B Testing and Best-arm Identification for Linear Bandits with Robustness to Non-stationarity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15154">http://arxiv.org/abs/2307.15154</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhihan Xiong, Romain Camilleri, Maryam Fazel, Lalit Jain, Kevin Jamieson</li>
<li>for: 本研究探讨了线性弹性环境中的固定预算最佳臂标识问题（Fixed-budget Best-arm Identification，BAI）。</li>
<li>methods: 我们提出了一种新的算法$\mathsf{P1}$-$\mathsf{RAGE}$，该算法可以在非站ARY情况下具有较高的鲁棒性和快速的标识速度。</li>
<li>results: 我们证明了$\mathsf{P1}$-$\mathsf{RAGE}$算法的错误概率与G最佳设计相比具有较高的鲁棒性，并且在有利的情况下与最佳站ARY情况下的算法相比较快。<details>
<summary>Abstract</summary>
We investigate the fixed-budget best-arm identification (BAI) problem for linear bandits in a potentially non-stationary environment. Given a finite arm set $\mathcal{X}\subset\mathbb{R}^d$, a fixed budget $T$, and an unpredictable sequence of parameters $\left\lbrace\theta_t\right\rbrace_{t=1}^{T}$, an algorithm will aim to correctly identify the best arm $x^* := \arg\max_{x\in\mathcal{X}}x^\top\sum_{t=1}^{T}\theta_t$ with probability as high as possible. Prior work has addressed the stationary setting where $\theta_t = \theta_1$ for all $t$ and demonstrated that the error probability decreases as $\exp(-T /\rho^*)$ for a problem-dependent constant $\rho^*$. But in many real-world $A/B/n$ multivariate testing scenarios that motivate our work, the environment is non-stationary and an algorithm expecting a stationary setting can easily fail. For robust identification, it is well-known that if arms are chosen randomly and non-adaptively from a G-optimal design over $\mathcal{X}$ at each time then the error probability decreases as $\exp(-T\Delta^2_{(1)}/d)$, where $\Delta_{(1)} = \min_{x \neq x^*} (x^* - x)^\top \frac{1}{T}\sum_{t=1}^T \theta_t$. As there exist environments where $\Delta_{(1)}^2/ d \ll 1/ \rho^*$, we are motivated to propose a novel algorithm $\mathsf{P1}$-$\mathsf{RAGE}$ that aims to obtain the best of both worlds: robustness to non-stationarity and fast rates of identification in benign settings. We characterize the error probability of $\mathsf{P1}$-$\mathsf{RAGE}$ and demonstrate empirically that the algorithm indeed never performs worse than G-optimal design but compares favorably to the best algorithms in the stationary setting.
</details>
<details>
<summary>摘要</summary>
我们研究了一个固定预算最佳臂识别（BAI）问题，在线性抽象环境中。我们有一个finite arm集$\mathcal{X}\subset\mathbb{R}^d$,一个固定预算$T$,并且有一个不可预测的数列$\left\lbrace\theta_t\right\rbrace_{t=1}^{T}$。我们的算法将尝试在可能不寻常的环境下，正确地识别最佳臂$x^* := \arg\max_{x\in\mathcal{X}}x^\top\sum_{t=1}^{T}\theta_t$，对应率越高越好。先前的工作已经处理过静止环境，在其中$\theta_t = \theta_1$ для所有$t$，并证明了错误率随着$T/\rho^*$的幂函数降低。但在实际的$A/B/n$多重试验enario中，环境通常是非站ARY，导致预期的站ARY环境下的算法可能会失败。为了实现预算的稳定性和快速的识别，我们提出了一个新的算法$\mathsf{P1}$-$\mathsf{RAGE}$，它将尝试在不可预测的环境下获得最佳的两个世界：稳定性和快速的识别。我们详细描述了$\mathsf{P1}$-$\mathsf{RAGE}$的错误率，并证明了它在实际上从不过不如G-优化设计，但与最佳的站ARY环境下的算法相比，它的表现却相当出色。
</details></li>
</ul>
<hr>
<h2 id="R-Block-Regularized-Block-of-Dropout-for-convolutional-networks"><a href="#R-Block-Regularized-Block-of-Dropout-for-convolutional-networks" class="headerlink" title="R-Block: Regularized Block of Dropout for convolutional networks"></a>R-Block: Regularized Block of Dropout for convolutional networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15150">http://arxiv.org/abs/2307.15150</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liqi Wang, Qiya Hu</li>
<li>for: 这 paper 是为了提出一种基于 R-Block 的 convolutional layer REGULARIZATION 技术，以提高 convolutional neural network 的性能。</li>
<li>methods: 这 paper 使用了一种名为 R-Block 的 mutual learning 训练策略，该策略在每个样本上最小化两个生成的差异最大化子模型的输出分布之间的loss。此外，paper 还提出了两种方法来构建子模型。</li>
<li>results:  experiments 表明，R-Block 可以达到比其他已知结构化 dropout 变体更好的性能，而且 paper 的方法构建子模型也超过了其他方法。<details>
<summary>Abstract</summary>
Dropout as a regularization technique is widely used in fully connected layers while is less effective in convolutional layers. Therefore more structured forms of dropout have been proposed to regularize convolutional networks. The disadvantage of these methods is that the randomness introduced causes inconsistency between training and inference. In this paper, we apply a mutual learning training strategy for convolutional layer regularization, namely R-Block, which forces two outputs of the generated difference maximizing sub models to be consistent with each other. Concretely, R-Block minimizes the losses between the output distributions of two sub models with different drop regions for each sample in the training dataset. We design two approaches to construct such sub models. Our experiments demonstrate that R-Block achieves better performance than other existing structured dropout variants. We also demonstrate that our approaches to construct sub models outperforms others.
</details>
<details>
<summary>摘要</summary>
“Dropout”作为调整技巧广泛使用于全连接层，但在卷积层中效果较差。因此，更结构化的Dropout方法被提议来调整卷积网络。这些方法的缺点是它们引入了随机性，导致训练和推导过程中的不一致。在这篇文章中，我们运用了互动学习训练策略，即R-Block，以调整卷积层。具体来说，R-Block将两个生成的差异最大化子模型的出力分布对于每个训练数据点进行最小化。我们还设计了两种子模型构造方法。我们的实验结果显示，R-Block可以比其他已知结构化Dropout变种更好地表现。此外，我们的子模型构造方法也比其他方法更好。
</details></li>
</ul>
<hr>
<h2 id="Distilled-Feature-Fields-Enable-Few-Shot-Language-Guided-Manipulation"><a href="#Distilled-Feature-Fields-Enable-Few-Shot-Language-Guided-Manipulation" class="headerlink" title="Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation"></a>Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07931">http://arxiv.org/abs/2308.07931</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Shen, Ge Yang, Alan Yu, Jansen Wong, Leslie Pack Kaelbling, Phillip Isola</li>
<li>for: 该论文旨在bridging 2D图像特征和3D几何学之间的差距，以便机器人 manipulate 任务中更好地理解3D几何学。</li>
<li>methods: 该论文使用了distilled feature fields来结合精度的3D几何学和丰富的语义特征，并使用了几何学和语义特征来实现几个shot学习的 grasping 和 placing 任务。</li>
<li>results: 该论文通过使用来自 CLIP 视力语言模型的特征抽取来实现自然语言上的 object 指定，并能够在不同的表达和新类别对象中进行普适化。<details>
<summary>Abstract</summary>
Self-supervised and language-supervised image models contain rich knowledge of the world that is important for generalization. Many robotic tasks, however, require a detailed understanding of 3D geometry, which is often lacking in 2D image features. This work bridges this 2D-to-3D gap for robotic manipulation by leveraging distilled feature fields to combine accurate 3D geometry with rich semantics from 2D foundation models. We present a few-shot learning method for 6-DOF grasping and placing that harnesses these strong spatial and semantic priors to achieve in-the-wild generalization to unseen objects. Using features distilled from a vision-language model, CLIP, we present a way to designate novel objects for manipulation via free-text natural language, and demonstrate its ability to generalize to unseen expressions and novel categories of objects.
</details>
<details>
<summary>摘要</summary>
自我监督和语言监督的图像模型含有重要的世界知识，这种知识对于总结非常重要。然而，许多 робо械任务需要精准的3D几何理解，而这种理解通常在2D图像特征中缺失。这项工作尝试将2D-to-3D的知识泵浦 bridge，通过使用精炼的特征场来结合准确的3D几何和丰富的semantics。我们提出了一种几何学学习方法，可以在几个步骤内实现6DOF抓取和置放。使用从视觉语言模型CLIP提取的特征，我们提出了一种通过自然语言文本指定新的物品进行操作的方法，并证明其能够通过未看过的表达和新类别的物品进行总结。
</details></li>
</ul>
<hr>
<h2 id="On-Normalised-Discounted-Cumulative-Gain-as-an-Offline-Evaluation-Metric-for-Top-n-Recommendation"><a href="#On-Normalised-Discounted-Cumulative-Gain-as-an-Offline-Evaluation-Metric-for-Top-n-Recommendation" class="headerlink" title="On (Normalised) Discounted Cumulative Gain as an Offline Evaluation Metric for Top-$n$ Recommendation"></a>On (Normalised) Discounted Cumulative Gain as an Offline Evaluation Metric for Top-$n$ Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15053">http://arxiv.org/abs/2307.15053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olivier Jeunen, Ivan Potapov, Aleksei Ustimenko</li>
<li>For: The paper is focused on evaluating the effectiveness of recommendation methods using offline evaluation metrics, specifically Normalised Discounted Cumulative Gain (nDCG), and investigating when nDCG can be considered an unbiased estimator of online reward.* Methods: The paper uses a critical analysis of nDCG and its assumptions to investigate its utility in recommendation evaluation. The authors provide a derivation of nDCG from first principles and show that normalising the metric can be inconsistent and limit its practical utility.* Results: The paper presents a correlation analysis between off- and on-line experiments conducted on a large-scale recommendation platform, showing that the unbiased DCG estimates strongly correlate with online reward, even when some of the metric’s inherent assumptions are violated. However, the normalised variant of nDCG does not maintain this correlation, suggesting that it may be limited in practical utility.<details>
<summary>Abstract</summary>
Approaches to recommendation are typically evaluated in one of two ways: (1) via a (simulated) online experiment, often seen as the gold standard, or (2) via some offline evaluation procedure, where the goal is to approximate the outcome of an online experiment. Several offline evaluation metrics have been adopted in the literature, inspired by ranking metrics prevalent in the field of Information Retrieval. (Normalised) Discounted Cumulative Gain (nDCG) is one such metric that has seen widespread adoption in empirical studies, and higher (n)DCG values have been used to present new methods as the state-of-the-art in top-$n$ recommendation for many years.   Our work takes a critical look at this approach, and investigates when we can expect such metrics to approximate the gold standard outcome of an online experiment. We formally present the assumptions that are necessary to consider DCG an unbiased estimator of online reward and provide a derivation for this metric from first principles, highlighting where we deviate from its traditional uses in IR. Importantly, we show that normalising the metric renders it inconsistent, in that even when DCG is unbiased, ranking competing methods by their normalised DCG can invert their relative order. Through a correlation analysis between off- and on-line experiments conducted on a large-scale recommendation platform, we show that our unbiased DCG estimates strongly correlate with online reward, even when some of the metric's inherent assumptions are violated. This statement no longer holds for its normalised variant, suggesting that nDCG's practical utility may be limited.
</details>
<details>
<summary>摘要</summary>
通常，推荐方法会被评估通过两种方式：在线实验（即“金标准”）或者离线评估过程，目的是估算在线实验的结果。在文献中，一些离线评估指标有被采纳，其中一个常见的指标是normalized Discounted Cumulative Gain（nDCG）。这个指标在许多年里被广泛采用，高于nDCG值被视为新方法的州际之最。我们对这种方法进行批判性的分析，检查在线实验的结果时，这些指标是否能够 aproximate 的准确。我们正式地表述了必须要将DCG作为无偏度估计的假设，并提供了这个指标的 derive 的过程，并 highlight 了我们与信息检索领域的传统用法不同的地方。重要的是，我们发现了normalizing 该指标后，它变得不一致，即在DCG是无偏度的情况下，对照竞争方法的排名可能会颠倒。通过一个大规模推荐平台上的在线和离线实验相关分析，我们发现了我们的无偏度DCG估计与在线奖励强相关，即使某些指标的内在假设被违反。但是，normalized DCG 的实际用途受到限制。
</details></li>
</ul>
<hr>
<h2 id="A-Transformer-based-Approach-for-Arabic-Offline-Handwritten-Text-Recognition"><a href="#A-Transformer-based-Approach-for-Arabic-Offline-Handwritten-Text-Recognition" class="headerlink" title="A Transformer-based Approach for Arabic Offline Handwritten Text Recognition"></a>A Transformer-based Approach for Arabic Offline Handwritten Text Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15045">http://arxiv.org/abs/2307.15045</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saleh Momeni, Bagher BabaAli</li>
<li>for: 本研究旨在解决手写文本识别问题，具体是对于缺乏并发性的现有方法。</li>
<li>methods: 我们提出了两种新的架构：Transformer Transducer和标准的序列化Transformer，并对其性能进行了比较。这两种架构都使用了注意力机制，因此更容易并行化和简化。</li>
<li>results: 我们在arabic khatt dataset上进行了评估，并证明了我们的方法可以超越当前状态的方法，提高了手写文本识别的准确率。<details>
<summary>Abstract</summary>
Handwriting recognition is a challenging and critical problem in the fields of pattern recognition and machine learning, with applications spanning a wide range of domains. In this paper, we focus on the specific issue of recognizing offline Arabic handwritten text. Existing approaches typically utilize a combination of convolutional neural networks for image feature extraction and recurrent neural networks for temporal modeling, with connectionist temporal classification used for text generation. However, these methods suffer from a lack of parallelization due to the sequential nature of recurrent neural networks. Furthermore, these models cannot account for linguistic rules, necessitating the use of an external language model in the post-processing stage to boost accuracy. To overcome these issues, we introduce two alternative architectures, namely the Transformer Transducer and the standard sequence-to-sequence Transformer, and compare their performance in terms of accuracy and speed. Our approach can model language dependencies and relies only on the attention mechanism, thereby making it more parallelizable and less complex. We employ pre-trained Transformers for both image understanding and language modeling. Our evaluation on the Arabic KHATT dataset demonstrates that our proposed method outperforms the current state-of-the-art approaches for recognizing offline Arabic handwritten text.
</details>
<details>
<summary>摘要</summary>
手写Recognition是一个挑战性和重要的问题在 Pattern recognition 和机器学习 领域，它的应用领域广泛，包括文本识别、语音识别、图像识别等。在这篇论文中，我们专注于特定的问题，即Offline 的阿拉伯文手写文本识别。现有的方法通常使用 Convolutional Neural Networks  для图像特征提取和Recurrent Neural Networks  для时间模型，并使用 connectionist 类型的文本生成。然而，这些方法受到缺乏并行化的缺点，因为Recurrent Neural Networks 的Sequential Nature。此外，这些模型无法考虑语言规则，因此需要使用外部语言模型来提高准确性。为了解决这些问题，我们引入了两种 alternate 架构，namely Transformer Transducer 和标准的sequence-to-sequence Transformer，并比较了它们的性能。我们的方法可以模型语言依赖关系，只靠注意机制，因此更加并行化和简单。我们使用预训练的 Transformers  для图像理解和语言模型。我们的评估在阿拉伯文 KHATT 数据集上表明，我们的提议方法在Offline 阿拉伯文手写文本识别方面超过了当前状态的术语。
</details></li>
</ul>
<hr>
<h2 id="Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models"><a href="#Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models" class="headerlink" title="Universal and Transferable Adversarial Attacks on Aligned Language Models"></a>Universal and Transferable Adversarial Attacks on Aligned Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15043">http://arxiv.org/abs/2307.15043</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/llm-attacks/llm-attacks">https://github.com/llm-attacks/llm-attacks</a></li>
<li>paper_authors: Andy Zou, Zifan Wang, J. Zico Kolter, Matt Fredrikson</li>
<li>for: 这项研究的目的是提出一种简单 yet effective的攻击方法，使得已经被适应的语言模型产生不良行为。</li>
<li>methods: 该方法通过适应搜索技术和梯度下降方法自动生成攻击 suffix，以提高对语言模型的攻击效果。</li>
<li>results: 研究发现，生成的攻击 suffix 能够在多种黑盒模型上 induce 不良行为，包括 ChatGPT、Bard 和 Claude 等。此外，该方法还可以在多个模型和多个查询中进行 transferred 攻击。<details>
<summary>Abstract</summary>
Because "out-of-the-box" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called "jailbreaks" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods.   Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.
</details>
<details>
<summary>摘要</summary>
因为“out-of-the-box”的大型自然语言模型可以生成大量不适的内容， latest work 强调了对这些模型进行对齐，以避免不良生成。 although there has been some success in circumventing these measures -- so-called "jailbreaks" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods.  Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at <https://github.com/llm-attacks/llm-attacks>.
</details></li>
</ul>
<hr>
<h2 id="Detecting-Morphing-Attacks-via-Continual-Incremental-Training"><a href="#Detecting-Morphing-Attacks-via-Continual-Incremental-Training" class="headerlink" title="Detecting Morphing Attacks via Continual Incremental Training"></a>Detecting Morphing Attacks via Continual Incremental Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15105">http://arxiv.org/abs/2307.15105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorenzo Pellegrini, Guido Borghi, Annalisa Franco, Davide Maltoni</li>
<li>for: 该论文旨在解决在数据传输和存储限制下，无法构建单一数据集，进行批处理训练的问题。</li>
<li>methods: 该论文使用了不同数据源和Continual Learning（CL）模式来实现incremental training。</li>
<li>results: 实验结果显示，Learning without Forgetting（LwF）方法在这种场景下表现出色，并且对Morphing Attack Detection和Object Classification任务进行了调整和优化。<details>
<summary>Abstract</summary>
Scenarios in which restrictions in data transfer and storage limit the possibility to compose a single dataset -- also exploiting different data sources -- to perform a batch-based training procedure, make the development of robust models particularly challenging. We hypothesize that the recent Continual Learning (CL) paradigm may represent an effective solution to enable incremental training, even through multiple sites. Indeed, a basic assumption of CL is that once a model has been trained, old data can no longer be used in successive training iterations and in principle can be deleted. Therefore, in this paper, we investigate the performance of different Continual Learning methods in this scenario, simulating a learning model that is updated every time a new chunk of data, even of variable size, is available. Experimental results reveal that a particular CL method, namely Learning without Forgetting (LwF), is one of the best-performing algorithms. Then, we investigate its usage and parametrization in Morphing Attack Detection and Object Classification tasks, specifically with respect to the amount of new training data that became available.
</details>
<details>
<summary>摘要</summary>
具有限制数据传输和存储的场景下， compose a single dataset 并利用不同的数据源进行批处理训练过程的发展变得特别困难。我们假设，最近的 Continual Learning（CL） paradigm 可能是一种有效的解决方案，以允许逐步训练，即使在多个站点之间。实际上， CL 的基本假设是，一旦模型已经训练过，then old data 不能再被用于后续的训练迭代，并且可以被删除。因此，在这篇论文中，我们 investigate 不同的 CL 方法的性能，模拟一个可以在新数据批量可用时更新的学习模型。实验结果表明，一种特定的 CL 方法，即 Learning without Forgetting（LwF），是最佳性能的算法之一。然后，我们 investigate LwF 在 Morphing Attack Detection 和 Object Classification 任务中的使用和参数化，特别是新训练数据的Amount。
</details></li>
</ul>
<hr>
<h2 id="Speeding-up-Fourier-Neural-Operators-via-Mixed-Precision"><a href="#Speeding-up-Fourier-Neural-Operators-via-Mixed-Precision" class="headerlink" title="Speeding up Fourier Neural Operators via Mixed Precision"></a>Speeding up Fourier Neural Operators via Mixed Precision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15034">http://arxiv.org/abs/2307.15034</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neuraloperator/neuraloperator">https://github.com/neuraloperator/neuraloperator</a></li>
<li>paper_authors: Colin White, Renbo Tu, Jean Kossaifi, Gennady Pekhimenko, Kamyar Azizzadenesheli, Anima Anandkumar</li>
<li>For: 学习部分� differential equation（PDE）解operator的代理映射。* Methods: 使用Fourier neural operator（FNO），并对其进行混合精度训练。* Results: 减少训练时间和内存使用（最多34%），减少训练时间和内存使用，而无需减少准确性，在 Navier-Stokes 和 Darcy 流Equation 上进行了一个研究。<details>
<summary>Abstract</summary>
The Fourier neural operator (FNO) is a powerful technique for learning surrogate maps for partial differential equation (PDE) solution operators. For many real-world applications, which often require high-resolution data points, training time and memory usage are significant bottlenecks. While there are mixed-precision training techniques for standard neural networks, those work for real-valued datatypes on finite dimensions and therefore cannot be directly applied to FNO, which crucially operates in the (complex-valued) Fourier domain and in function spaces. On the other hand, since the Fourier transform is already an approximation (due to discretization error), we do not need to perform the operation at full precision. In this work, we (i) profile memory and runtime for FNO with full and mixed-precision training, (ii) conduct a study on the numerical stability of mixed-precision training of FNO, and (iii) devise a training routine which substantially decreases training time and memory usage (up to 34%), with little or no reduction in accuracy, on the Navier-Stokes and Darcy flow equations. Combined with the recently proposed tensorized FNO (Kossaifi et al., 2023), the resulting model has far better performance while also being significantly faster than the original FNO.
</details>
<details>
<summary>摘要</summary>
“傅欧恩抽象（FNO）是一种强大的技术，用于学习对应扩散方程（PDE）解析器的代理映射。实际应用中，通常需要大量的高分辨率数据点，训练时间和内存使用是重要的瓶颈。虽然现有混合精度训练技术可以应用于标准神经网络，但这些技术仅适用于実数型数据和有限维度上。对于FNO而言，由于快 Fourier  трансформа是一种近似（由于精度误差），因此我们不需要在全精度下进行运算。在这个研究中，我们（i）评估FNO的内存和运算时间profile，（ii）进行混合精度训练的numerical stability研究，以及（iii）开发一个可以很快地训练FNO的训练程式，可以对 Navier-Stokes 和 Darcy 流运动方程获得较好的性能，并且在34%的几何中实现了训练时间和内存的减少，而且几乎没有影响精度。 combinined with the recently proposed tensorized FNO（Kossaifi et al., 2023），得到的模型具有许多更好的性能，并且比原始FNO更快。”
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Graph-Transformer-for-Deepfake-Detection"><a href="#Self-Supervised-Graph-Transformer-for-Deepfake-Detection" class="headerlink" title="Self-Supervised Graph Transformer for Deepfake Detection"></a>Self-Supervised Graph Transformer for Deepfake Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15019">http://arxiv.org/abs/2307.15019</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aminollah Khormali, Jiann-Shiun Yuan</li>
<li>for: 本研究旨在开发一个可靠的深伪检测系统，能够在不同的数据集上保持高度的检测性能，并且能够承受常见的后期处理扰动。</li>
<li>methods: 本研究提出了一种基于视觉转换器架构的特征提取器，利用自我超VI中的自我抑制学习方法进行预训练，并将图 convolutional neural network 和转换器推荐器结合使用，以提高对抗 manipulate 性能。</li>
<li>results: 经过多种挑战性实验，包括在不同数据集上的性能测试、跨数据集检测、跨扰动检测以及对常见后期处理扰动的Robustness测试，提出的深伪检测框架在所有方面都达到了现状之上的表现。<details>
<summary>Abstract</summary>
Deepfake detection methods have shown promising results in recognizing forgeries within a given dataset, where training and testing take place on the in-distribution dataset. However, their performance deteriorates significantly when presented with unseen samples. As a result, a reliable deepfake detection system must remain impartial to forgery types, appearance, and quality for guaranteed generalizable detection performance. Despite various attempts to enhance cross-dataset generalization, the problem remains challenging, particularly when testing against common post-processing perturbations, such as video compression or blur. Hence, this study introduces a deepfake detection framework, leveraging a self-supervised pre-training model that delivers exceptional generalization ability, withstanding common corruptions and enabling feature explainability. The framework comprises three key components: a feature extractor based on vision Transformer architecture that is pre-trained via self-supervised contrastive learning methodology, a graph convolution network coupled with a Transformer discriminator, and a graph Transformer relevancy map that provides a better understanding of manipulated regions and further explains the model's decision. To assess the effectiveness of the proposed framework, several challenging experiments are conducted, including in-data distribution performance, cross-dataset, cross-manipulation generalization, and robustness against common post-production perturbations. The results achieved demonstrate the remarkable effectiveness of the proposed deepfake detection framework, surpassing the current state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
深圳检测方法已经在给定数据集上显示了有 promise的结果，可以训练和测试在同一个数据集上。然而，其性能在未看过的样本上会很差。因此，一个可靠的深圳检测系统必须保持中立于伪造类型、外观和质量上，以确保普适的检测性能。 despite various attempts to enhance cross-dataset generalization, the problem remains challenging, particularly when testing against common post-processing perturbations, such as video compression or blur. Therefore, this study introduces a deepfake detection framework, which leverages a self-supervised pre-training model that delivers exceptional generalization ability, withstanding common corruptions and enabling feature explainability. The framework consists of three key components: a feature extractor based on vision Transformer architecture that is pre-trained via self-supervised contrastive learning methodology, a graph convolution network coupled with a Transformer discriminator, and a graph Transformer relevancy map that provides a better understanding of manipulated regions and further explains the model's decision. To assess the effectiveness of the proposed framework, several challenging experiments are conducted, including in-distribution performance, cross-dataset, cross-manipulation generalization, and robustness against common post-production perturbations. The results achieved demonstrate the remarkable effectiveness of the proposed deepfake detection framework, surpassing the current state-of-the-art approaches.
</details></li>
</ul>
<hr>
<h2 id="Samplable-Anonymous-Aggregation-for-Private-Federated-Data-Analysis"><a href="#Samplable-Anonymous-Aggregation-for-Private-Federated-Data-Analysis" class="headerlink" title="Samplable Anonymous Aggregation for Private Federated Data Analysis"></a>Samplable Anonymous Aggregation for Private Federated Data Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15017">http://arxiv.org/abs/2307.15017</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kunal Talwar, Shan Wang, Audra McMillan, Vojta Jina, Vitaly Feldman, Bailey Basile, Aine Cahill, Yi Sheng Chan, Mike Chatzidakis, Junye Chen, Oliver Chick, Mona Chitnis, Suman Ganta, Yusuf Goren, Filip Granqvist, Kristine Guo, Frederic Jacobs, Omid Javidbakht, Albert Liu, Richard Low, Dan Mascenik, Steve Myers, David Park, Wonhee Park, Gianni Parsa, Tommy Pauly, Christian Priebe, Rehan Rishi, Guy Rothblum, Michael Scaria, Linmao Song, Congzheng Song, Karl Tarbe, Sebastian Vogt, Luke Winstrom, Shundong Zhou</li>
<li>for: 该 paper 的目的是设计私有统计和私有联合学习协议，以便每个设备都可以保持自己的私人数据。</li>
<li>methods: 该 paper 使用了一种简单的原理，可以实现许多常用的算法，并且可以在私有设备上进行隐私计算，不需要强信任假设。</li>
<li>results: 该 paper 提出了一个系统架构，实现了该原理，并进行了安全分析。<details>
<summary>Abstract</summary>
We revisit the problem of designing scalable protocols for private statistics and private federated learning when each device holds its private data. Our first contribution is to propose a simple primitive that allows for efficient implementation of several commonly used algorithms, and allows for privacy accounting that is close to that in the central setting without requiring the strong trust assumptions it entails. Second, we propose a system architecture that implements this primitive and perform a security analysis of the proposed system.
</details>
<details>
<summary>摘要</summary>
我团队在 revisit 私人统计和私人联合学习问题上做出了一些贡献。我们的第一个贡献是提出了一种简单的基本原理，可以有效地实现许多通常使用的算法，同时允许保持隐私计算的紧密性，无需强大的信任假设。其次，我们提出了一个实现这种基本原理的系统架构，并进行了安全分析。
</details></li>
</ul>
<hr>
<h2 id="How-Good-is-Google-Bard’s-Visual-Understanding-An-Empirical-Study-on-Open-Challenges"><a href="#How-Good-is-Google-Bard’s-Visual-Understanding-An-Empirical-Study-on-Open-Challenges" class="headerlink" title="How Good is Google Bard’s Visual Understanding? An Empirical Study on Open Challenges"></a>How Good is Google Bard’s Visual Understanding? An Empirical Study on Open Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15016">http://arxiv.org/abs/2307.15016</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/htqin/googlebard-visunderstand">https://github.com/htqin/googlebard-visunderstand</a></li>
<li>paper_authors: Haotong Qin, Ge-Peng Ji, Salman Khan, Deng-Ping Fan, Fahad Shahbaz Khan, Luc Van Gool</li>
<li>for: 这项研究旨在评估Google Bard在多模态生成器中对视觉数据（图像）的理解和解释能力，以探讨其在各种复杂计算机视觉问题中的表现。</li>
<li>methods: 该研究使用Google Bard处理图像和文本提问的能力，并在15个多样化任务场景中测试其性能，包括普通、掩蔽、医疗、水下和Remote Sensing等。</li>
<li>results: 研究发现，Google Bard在这些视觉任务场景中表现不佳，表明现有的视觉理解 gap 需要被覆盖，未来的发展应该更加注重视觉理解。<details>
<summary>Abstract</summary>
Google's Bard has emerged as a formidable competitor to OpenAI's ChatGPT in the field of conversational AI. Notably, Bard has recently been updated to handle visual inputs alongside text prompts during conversations. Given Bard's impressive track record in handling textual inputs, we explore its capabilities in understanding and interpreting visual data (images) conditioned by text questions. This exploration holds the potential to unveil new insights and challenges for Bard and other forthcoming multi-modal Generative models, especially in addressing complex computer vision problems that demand accurate visual and language understanding. Specifically, in this study, we focus on 15 diverse task scenarios encompassing regular, camouflaged, medical, under-water and remote sensing data to comprehensively evaluate Bard's performance. Our primary finding indicates that Bard still struggles in these vision scenarios, highlighting the significant gap in vision-based understanding that needs to be bridged in future developments. We expect that this empirical study will prove valuable in advancing future models, leading to enhanced capabilities in comprehending and interpreting fine-grained visual data. Our project is released on https://github.com/htqin/GoogleBard-VisUnderstand
</details>
<details>
<summary>摘要</summary>
Google的Bard已经成为对OpenAI的ChatGPT的 conversational AI的强力竞争对手。值得注意的是，Bard最近更新了可以处理视觉输入的功能，以便与文本提问一起进行对话。由于Bard在处理文本输入的表现印象，我们探索了它在理解和解释视觉数据（图像）时的能力。这种探索拥有探索新的发现和挑战，特别是在解决复杂的计算机视觉问题时，需要精准的视觉和语言理解。在这项研究中，我们关注了15种多样化的任务场景，包括常见、潜规、医疗、水下和远程感知数据，以全面评估Bard的表现。我们的主要发现表明Bard在这些视觉场景中仍然受到挑战，这 highlights了未来发展中需要跨越的视觉理解障碍。我们预计这项实证研究将为未来的模型发展提供价值，导致更高级的视觉数据解释和理解。我们的项目在https://github.com/htqin/GoogleBard-VisUnderstand上发布。
</details></li>
</ul>
<hr>
<h2 id="Harnessing-Synthetic-Active-Particles-for-Physical-Reservoir-Computing"><a href="#Harnessing-Synthetic-Active-Particles-for-Physical-Reservoir-Computing" class="headerlink" title="Harnessing Synthetic Active Particles for Physical Reservoir Computing"></a>Harnessing Synthetic Active Particles for Physical Reservoir Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15010">http://arxiv.org/abs/2307.15010</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangzun Wang, Frank Cichos</li>
<li>for: 这篇论文是研究生物系统中信息处理的一种新方法，即基于活动过程网络的增强机器学习。</li>
<li>methods: 这篇论文使用了一种叫做物理储存计算的方法，其中使用了自适应的微型推进系统来实现信息处理。</li>
<li>results: 研究发现，这种方法可以减少噪声并实现预测任务，并且可以用于研究生物系统中信息处理的自组织系统。<details>
<summary>Abstract</summary>
The processing of information is an indispensable property of living systems realized by networks of active processes with enormous complexity. They have inspired many variants of modern machine learning one of them being reservoir computing, in which stimulating a network of nodes with fading memory enables computations and complex predictions. Reservoirs are implemented on computer hardware, but also on unconventional physical substrates such as mechanical oscillators, spins, or bacteria often summarized as physical reservoir computing. Here we demonstrate physical reservoir computing with a synthetic active microparticle system that self-organizes from an active and passive component into inherently noisy nonlinear dynamical units. The self-organization and dynamical response of the unit is the result of a delayed propulsion of the microswimmer to a passive target. A reservoir of such units with a self-coupling via the delayed response can perform predictive tasks despite the strong noise resulting from Brownian motion of the microswimmers. To achieve efficient noise suppression, we introduce a special architecture that uses historical reservoir states for output. Our results pave the way for the study of information processing in synthetic self-organized active particle systems.
</details>
<details>
<summary>摘要</summary>
生物系统中信息处理是不可或缺的属性，由活动过程网络实现，其复杂性很高。它们激发了现代机器学习的许多变种，其中一种是储量计算，通过刺激网络节点的淡出记忆来实现计算和复杂预测。储量可以在计算机硬件上实现，也可以在不典型的物理基础上实现，如机械振荡、磁矢量或细菌等，通常称为物理储量计算。在这个研究中，我们使用合成活动微部件系统，该系统由活动和无功能组成部件自组织而成，具有内生的噪声和非线性动力学行为。我们发现，通过延迟微部件的推进到无功能目标，可以实现预测任务，并且可以使用历史储量状态来降低噪声。我们的结果开启了人工自组织活动粒子系统中信息处理的研究之路。
</details></li>
</ul>
<hr>
<h2 id="Verifiable-Feature-Attributions-A-Bridge-between-Post-Hoc-Explainability-and-Inherent-Interpretability"><a href="#Verifiable-Feature-Attributions-A-Bridge-between-Post-Hoc-Explainability-and-Inherent-Interpretability" class="headerlink" title="Verifiable Feature Attributions: A Bridge between Post Hoc Explainability and Inherent Interpretability"></a>Verifiable Feature Attributions: A Bridge between Post Hoc Explainability and Inherent Interpretability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15007">http://arxiv.org/abs/2307.15007</a></li>
<li>repo_url: None</li>
<li>paper_authors: Usha Bhalla, Suraj Srinivas, Himabindu Lakkaraju</li>
<li>for: 这个论文的目的是解释模型行为，提高模型的可解释性和可验证性。</li>
<li>methods: 这个论文提出了一种名为Verifiability Tuning（VerT）的方法，它可以将黑盒模型转化成具有可验证和 faithful 的特征归因方法。</li>
<li>results: 该方法在 semi-synthetic 和实际世界数据集上进行了广泛的实验，结果表明，VerT 可以生成可验证和 faithful 的特征归因，同时保持了模型的预测性能。<details>
<summary>Abstract</summary>
With the increased deployment of machine learning models in various real-world applications, researchers and practitioners alike have emphasized the need for explanations of model behaviour. To this end, two broad strategies have been outlined in prior literature to explain models. Post hoc explanation methods explain the behaviour of complex black-box models by highlighting features that are critical to model predictions; however, prior work has shown that these explanations may not be faithful, and even more concerning is our inability to verify them. Specifically, it is nontrivial to evaluate if a given attribution is correct with respect to the underlying model. Inherently interpretable models, on the other hand, circumvent these issues by explicitly encoding explanations into model architecture, meaning their explanations are naturally faithful and verifiable, but they often exhibit poor predictive performance due to their limited expressive power. In this work, we aim to bridge the gap between the aforementioned strategies by proposing Verifiability Tuning (VerT), a method that transforms black-box models into models that naturally yield faithful and verifiable feature attributions. We begin by introducing a formal theoretical framework to understand verifiability and show that attributions produced by standard models cannot be verified. We then leverage this framework to propose a method to build verifiable models and feature attributions out of fully trained black-box models. Finally, we perform extensive experiments on semi-synthetic and real-world datasets, and show that VerT produces models that (1) yield explanations that are correct and verifiable and (2) are faithful to the original black-box models they are meant to explain.
</details>
<details>
<summary>摘要</summary>
随着机器学习模型在各种实际应用中的普及，研究人员和实践者们都强调了模型行为的解释的需求。为此，先前的 литера图中提出了两种广泛的解释策略：后处解释方法通过强调模型预测中的关键特征来解释复杂的黑盒模型行为，但先前的研究表明，这些解释可能不准确，甚至更加担忧的是我们无法验证它们。另一方面，具有内在可解释性的模型通过显式地编码解释到模型架构中，因此其解释自然准确和可验证，但它们通常具有较强的表达力限制。在这个工作中，我们想要将上述两种策略相互连接起来，我们提出了一种名为Verifiability Tuning（VerT）的方法，可以将黑盒模型转化成可以自然地生成准确和可验证的特征归因的模型。我们首先介绍了一个正式的理论框架，以理解验证性，并证明标准模型生成的归因无法验证。然后，我们利用这个框架来提出一种建立可验证模型和特征归因的方法。最后，我们在半 sintética和实际数据集上进行了广泛的实验，并证明VerT可以生成具有以下两个特点的模型：1）归因是正确和可验证的，2）与原始黑盒模型相同。
</details></li>
</ul>
<hr>
<h2 id="Improved-Neural-Radiance-Fields-Using-Pseudo-depth-and-Fusion"><a href="#Improved-Neural-Radiance-Fields-Using-Pseudo-depth-and-Fusion" class="headerlink" title="Improved Neural Radiance Fields Using Pseudo-depth and Fusion"></a>Improved Neural Radiance Fields Using Pseudo-depth and Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03772">http://arxiv.org/abs/2308.03772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingliang Li, Qiang Zhou, Chaohui Yu, Zhengda Lu, Jun Xiao, Zhibin Wang, Fan Wang</li>
<li>for: 本研究主要针对 novel view synthesis 和 dense geometry modeling 问题，旨在提高 NeRF 模型的渲染质量和渲染精度。</li>
<li>methods: 我们提出了一种 multi-scale encoding volume 的方法，并将其与 NeRF 模型结合使用，以便更好地捕捉实际场景中的各种大小对象&#x2F;结构的几何信息。我们还提出了一种同时进行 depth prediction 和 radiance field reconstruction 的方法，以提高渲染的准确性。</li>
<li>results: 我们的方法在 novel view synthesis 和 dense geometry modeling 两个领域都显示出了优于传统方法的性能。具体来说，我们的方法可以在不需要Scene-specific optimization的情况下，实现高质量的渲染和准确的深度映射。<details>
<summary>Abstract</summary>
Since the advent of Neural Radiance Fields, novel view synthesis has received tremendous attention. The existing approach for the generalization of radiance field reconstruction primarily constructs an encoding volume from nearby source images as additional inputs. However, these approaches cannot efficiently encode the geometric information of real scenes with various scale objects/structures. In this work, we propose constructing multi-scale encoding volumes and providing multi-scale geometry information to NeRF models. To make the constructed volumes as close as possible to the surfaces of objects in the scene and the rendered depth more accurate, we propose to perform depth prediction and radiance field reconstruction simultaneously. The predicted depth map will be used to supervise the rendered depth, narrow the depth range, and guide points sampling. Finally, the geometric information contained in point volume features may be inaccurate due to occlusion, lighting, etc. To this end, we propose enhancing the point volume feature from depth-guided neighbor feature fusion. Experiments demonstrate the superior performance of our method in both novel view synthesis and dense geometry modeling without per-scene optimization.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:自NeRF出现以来，新视图合成受到了广泛的关注。现有的总体化NeRF重建方法主要从邻近源图像中构建编码量为附加输入。然而，这些方法无法有效地编码实际场景中的各种比例物体/结构的 геометрической信息。在这项工作中，我们提议构建多尺度编码量和提供多尺度几何信息给NeRF模型。为使构造的量最接近物体场景中的表面和渲染深度更准确，我们提议同时预测深度图和NeRF重建。预测的深度图将被用来监督渲染深度，缩小深度范围，并导引点抽取。最后，由点云特征含有的几何信息可能因 occlusion、照明等原因而不准确。为此，我们提议通过深度导向邻居特征融合提高点云特征。实验表明我们的方法在新视图合成和密集几何建模中具有优于其他方法的性能，无需Scene优化。
</details></li>
</ul>
<hr>
<h2 id="Detection-of-Children-Abuse-by-Voice-and-Audio-Classification-by-Short-Time-Fourier-Transform-Machine-Learning-implemented-on-Nvidia-Edge-GPU-device"><a href="#Detection-of-Children-Abuse-by-Voice-and-Audio-Classification-by-Short-Time-Fourier-Transform-Machine-Learning-implemented-on-Nvidia-Edge-GPU-device" class="headerlink" title="Detection of Children Abuse by Voice and Audio Classification by Short-Time Fourier Transform Machine Learning implemented on Nvidia Edge GPU device"></a>Detection of Children Abuse by Voice and Audio Classification by Short-Time Fourier Transform Machine Learning implemented on Nvidia Edge GPU device</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15101">http://arxiv.org/abs/2307.15101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiuqi Yan, Yingxian Chen, W. W. T. Fok</li>
<li>for: 这个实验的目的是使用机器学习方法探测儿童虐待的情况，以增加儿童的安全性。</li>
<li>methods: 这个实验使用机器学习来分类和识别儿童的声音，并预测声音是否为哭泣、尖叫或笑声。如果发现儿童哭泣或尖叫，则会发送警告至相关人员，以便在监控盲区中识别和对待儿童的情况。</li>
<li>results: 这个实验使用声音和影像分类来增加儿童虐待探测的精度，并获得了约92%的准确率。<details>
<summary>Abstract</summary>
The safety of children in children home has become an increasing social concern, and the purpose of this experiment is to use machine learning applied to detect the scenarios of child abuse to increase the safety of children. This experiment uses machine learning to classify and recognize a child's voice and predict whether the current sound made by the child is crying, screaming or laughing. If a child is found to be crying or screaming, an alert is immediately sent to the relevant personnel so that they can perceive what the child may be experiencing in a surveillance blind spot and respond in a timely manner. Together with a hybrid use of video image classification, the accuracy of child abuse detection can be significantly increased. This greatly reduces the likelihood that a child will receive violent abuse in the nursery and allows personnel to stop an imminent or incipient child abuse incident in time. The datasets collected from this experiment is entirely from sounds recorded on site at the children home, including crying, laughing, screaming sound and background noises. These sound files are transformed into spectrograms using Short-Time Fourier Transform, and then these image data are imported into a CNN neural network for classification, and the final trained model can achieve an accuracy of about 92% for sound detection.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXT儿童安全在儿童HOME已成为社会关注的问题，该实验的目的是使用机器学习方法检测儿童虐待情况，以提高儿童的安全性。这个实验使用机器学习来分类和识别儿童的声音，预测当前儿童发出的声音是否是哭泣、喊叫或 laughter。如果发现儿童哭泣或喊叫，则立即发送警报给相关人员，以便他们在监控隐蔽区域内timely perceive儿童的情况，并采取应对措施。与视频图像分类结合使用，可以显著提高儿童虐待检测的准确率。这将减少儿童在寻常培养中接受暴力虐待的可能性，并让人员在时间上采取制止儿童虐待事件的措施。实验所收集的数据全部来自儿童HOME的声音记录，包括哭泣、喊叫、笑声和背景噪音。这些声音文件被转换成spectrograms使用Short-Time Fourier Transform，然后这些图像数据被导入到CNN神经网络中进行分类，最终训练出的模型可以达到约92%的准确率。
</details></li>
</ul>
<hr>
<h2 id="Thinker-Learning-to-Plan-and-Act"><a href="#Thinker-Learning-to-Plan-and-Act" class="headerlink" title="Thinker: Learning to Plan and Act"></a>Thinker: Learning to Plan and Act</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14993">http://arxiv.org/abs/2307.14993</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anonymous-scrl/thinker">https://github.com/anonymous-scrl/thinker</a></li>
<li>paper_authors: Stephen Chung, Ivan Anokhin, David Krueger</li>
<li>for:  This paper proposes a novel approach called the Thinker algorithm, which enables reinforcement learning agents to autonomously interact with and utilize a learned world model.</li>
<li>methods: The Thinker algorithm wraps the environment with a world model and introduces new actions designed for interacting with the world model, allowing agents to perform planning by proposing alternative plans to the world model before selecting a final action to execute in the environment.</li>
<li>results: The algorithm achieves state-of-the-art performance and competitive results in the game of Sokoban and the Atari 2600 benchmark, respectively. Visualizations of agents trained with the Thinker algorithm demonstrate that they have learned to plan effectively with the world model to select better actions.<details>
<summary>Abstract</summary>
We propose the Thinker algorithm, a novel approach that enables reinforcement learning agents to autonomously interact with and utilize a learned world model. The Thinker algorithm wraps the environment with a world model and introduces new actions designed for interacting with the world model. These model-interaction actions enable agents to perform planning by proposing alternative plans to the world model before selecting a final action to execute in the environment. This approach eliminates the need for hand-crafted planning algorithms by enabling the agent to learn how to plan autonomously and allows for easy interpretation of the agent's plan with visualization. We demonstrate the algorithm's effectiveness through experimental results in the game of Sokoban and the Atari 2600 benchmark, where the Thinker algorithm achieves state-of-the-art performance and competitive results, respectively. Visualizations of agents trained with the Thinker algorithm demonstrate that they have learned to plan effectively with the world model to select better actions. The algorithm's generality opens a new research direction on how a world model can be used in reinforcement learning and how planning can be seamlessly integrated into an agent's decision-making process.
</details>
<details>
<summary>摘要</summary>
我们提出了“思考算法”，一种新的方法，允许强化学习代理人自主地与学习的世界模型交互。思考算法将环境包装在世界模型中，并引入了专门为与世界模型交互的模型交互动作。这些动作允许代理人通过提议不同的计划给世界模型，然后选择环境中执行的最终动作。这种方法消除了手动制定的计划算法的需要，让代理人可以自主学习计划，并且可以轻松地通过视化来解释代理人的计划。我们通过在扑克游戏和Atari 2600 benchmark中的实验成果，证明了思考算法的有效性和竞争性。视觉化的代理人训练结果显示，它们已经学习了如何通过世界模型选择更好的动作。这种算法的通用性开启了一个新的研究方向，即如何在强化学习中使用世界模型，以及如何在代理人决策过程中协调计划。
</details></li>
</ul>
<hr>
<h2 id="Incrementally-Computable-Neural-Networks-Efficient-Inference-for-Dynamic-Inputs"><a href="#Incrementally-Computable-Neural-Networks-Efficient-Inference-for-Dynamic-Inputs" class="headerlink" title="Incrementally-Computable Neural Networks: Efficient Inference for Dynamic Inputs"></a>Incrementally-Computable Neural Networks: Efficient Inference for Dynamic Inputs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14988">http://arxiv.org/abs/2307.14988</a></li>
<li>repo_url: None</li>
<li>paper_authors: Or Sharir, Anima Anandkumar</li>
<li>for: 这个论文的目的是解决深度学习中处理动态输入的挑战，如感知数据或用户输入。例如，一个基于AI的写作助手需要在文档编辑过程中实时更新建议。</li>
<li>methods: 这篇论文采用了逐步计算方法，寻求重用计算结果以降低计算成本。但是传统的网络架构中的紧密连接带来一大问题，因为even minor input changes会在网络中宣传并限制信息的重用。为解决这个问题，我们使用 вектор量化来精细化网络中的中间值，从而筛除无用的修改和隐藏神经元的值。</li>
<li>results: 我们在应用vector量化技术于transformers架构上创建了一种高效的逐步计算算法，其复杂度与修改输入的百分数成正比。我们的实验表明，在对OPT-125M预训练语言模型进行适应后，可以实现与传统方法相同的准确率，同时减少了12.1倍（中位数）的操作数量来处理序列中的原子修改。<details>
<summary>Abstract</summary>
Deep learning often faces the challenge of efficiently processing dynamic inputs, such as sensor data or user inputs. For example, an AI writing assistant is required to update its suggestions in real time as a document is edited. Re-running the model each time is expensive, even with compression techniques like knowledge distillation, pruning, or quantization. Instead, we take an incremental computing approach, looking to reuse calculations as the inputs change. However, the dense connectivity of conventional architectures poses a major obstacle to incremental computation, as even minor input changes cascade through the network and restrict information reuse. To address this, we use vector quantization to discretize intermediate values in the network, which filters out noisy and unnecessary modifications to hidden neurons, facilitating the reuse of their values. We apply this approach to the transformers architecture, creating an efficient incremental inference algorithm with complexity proportional to the fraction of the modified inputs. Our experiments with adapting the OPT-125M pre-trained language model demonstrate comparable accuracy on document classification while requiring 12.1X (median) fewer operations for processing sequences of atomic edits.
</details>
<details>
<summary>摘要</summary>
然而，传统的网络架构中的紧密连接带来了主要的障碍，因为 Even small input changes can propagate through the network and limit the information reuse. To address this, we use vector quantization to discretize intermediate values in the network, which filters out noisy and unnecessary modifications to hidden neurons, facilitating the reuse of their values. We apply this approach to the Transformers architecture, creating an efficient incremental inference algorithm with complexity proportional to the fraction of the modified inputs.我们通过对 OPT-125M 预训练语言模型进行适应，实验表明可以 achieve comparable accuracy on document classification while requiring 12.1X (median) fewer operations for processing sequences of atomic edits.
</details></li>
</ul>
<hr>
<h2 id="Take-A-Photo-3D-to-2D-Generative-Pre-training-of-Point-Cloud-Models"><a href="#Take-A-Photo-3D-to-2D-Generative-Pre-training-of-Point-Cloud-Models" class="headerlink" title="Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models"></a>Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14971">http://arxiv.org/abs/2307.14971</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangzy22/tap">https://github.com/wangzy22/tap</a></li>
<li>paper_authors: Ziyi Wang, Xumin Yu, Yongming Rao, Jie Zhou, Jiwen Lu</li>
<li>for: 提高3D视觉模型的性能</li>
<li>methods: 使用交叉注意机制生成不同指定姿态的视图图像，以供3D模型进行预训练</li>
<li>results: 在ScanObjectNN分类和ShapeNetPart segmentation任务上达到了state-of-the-art表现，并且可以跨 Architecture-oriented 方法进行改进Translation:</li>
<li>for: 提高3D视觉模型的性能</li>
<li>methods: 使用交叉注意机制生成不同指定姿态的视图图像，以供3D模型进行预训练</li>
<li>results: 在ScanObjectNN分类和ShapeNetPart segmentation任务上达到了state-of-the-art表现，并且可以跨 Architecture-oriented 方法进行改进<details>
<summary>Abstract</summary>
With the overwhelming trend of mask image modeling led by MAE, generative pre-training has shown a remarkable potential to boost the performance of fundamental models in 2D vision. However, in 3D vision, the over-reliance on Transformer-based backbones and the unordered nature of point clouds have restricted the further development of generative pre-training. In this paper, we propose a novel 3D-to-2D generative pre-training method that is adaptable to any point cloud model. We propose to generate view images from different instructed poses via the cross-attention mechanism as the pre-training scheme. Generating view images has more precise supervision than its point cloud counterpart, thus assisting 3D backbones to have a finer comprehension of the geometrical structure and stereoscopic relations of the point cloud. Experimental results have proved the superiority of our proposed 3D-to-2D generative pre-training over previous pre-training methods. Our method is also effective in boosting the performance of architecture-oriented approaches, achieving state-of-the-art performance when fine-tuning on ScanObjectNN classification and ShapeNetPart segmentation tasks. Code is available at https://github.com/wangzy22/TAP.
</details>
<details>
<summary>摘要</summary>
随着MAE领导的面照模型的潮流，生成先进教育显示了在2D视觉中性能的潜在提升 potential.然而，在3D视觉中，基于Transformer的脊梁和点云的无序性限制了生成先进教育的进一步发展.在本文中，我们提议一种适用于任何点云模型的3D-to-2D生成先进教育方法。我们提议通过交叉注意机制来生成不同指导姿态的视图图像作为预教程。通过生成视图图像，我们可以对3D背景进行更精确的监督，从而帮助3D背景更好地理解点云的几何结构和立体关系。实验结果表明，我们的提议的3D-to-2D生成先进教育方法比前期方法更有优势。此外，我们的方法也可以提高基于建筑方法的性能，在ScanObjectNN分类和ShapeNetPart分割任务上达到了状态之arte的表现。代码可以在https://github.com/wangzy22/TAP中找到。
</details></li>
</ul>
<hr>
<h2 id="Learning-locally-dominant-force-balances-in-active-particle-systems"><a href="#Learning-locally-dominant-force-balances-in-active-particle-systems" class="headerlink" title="Learning locally dominant force balances in active particle systems"></a>Learning locally dominant force balances in active particle systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14970">http://arxiv.org/abs/2307.14970</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominik Sturm, Suryanarayana Maddu, Ivo F. Sbalzarini</li>
<li>for: 本研究使用混合不监督聚类和稀疏推理算法来学习自组织活动粒子系统中的地方主导力平衡，解释大规模模式的形成。</li>
<li>methods: 本研究使用一种经典的液体动力学模型，该模型可以生成各种模式，如星形和移动浓度带。我们使用数据驱动分析发现，在某些情况下，扩散带的形成是基于地方对性的排列互动驱动的，而稳定的星形则是由强 particle间互动引起的负压缩性所形成的。</li>
<li>results: 我们的方法可以在不同的模型中揭示物理共同点，并且与分析缩放关系和实验观测相当一致。<details>
<summary>Abstract</summary>
We use a combination of unsupervised clustering and sparsity-promoting inference algorithms to learn locally dominant force balances that explain macroscopic pattern formation in self-organized active particle systems. The self-organized emergence of macroscopic patterns from microscopic interactions between self-propelled particles can be widely observed nature. Although hydrodynamic theories help us better understand the physical basis of this phenomenon, identifying a sufficient set of local interactions that shape, regulate, and sustain self-organized structures in active particle systems remains challenging. We investigate a classic hydrodynamic model of self-propelled particles that produces a wide variety of patterns, like asters and moving density bands. Our data-driven analysis shows that propagating bands are formed by local alignment interactions driven by density gradients, while steady-state asters are shaped by a mechanism of splay-induced negative compressibility arising from strong particle interactions. Our method also reveals analogous physical principles of pattern formation in a system where the speed of the particle is influenced by local density. This demonstrates the ability of our method to reveal physical commonalities across models. The physical mechanisms inferred from the data are in excellent agreement with analytical scaling arguments and experimental observations.
</details>
<details>
<summary>摘要</summary>
我们使用无监督聚类和稀盐推理算法来学习自组织活动粒子系统中的地方主导力平衡，解释大规模Pattern形成。自组织emergence的大规模模式在自然界广泛存在。虽然水力学理论可以帮助我们更好地理解这种现象的物理基础，但是在活动粒子系统中确定足够的地方交互可以形成、调控和维护自组织结构仍然是挑战。我们研究了一个 класси型的自力推动粒子模型，该模型可以生成各种模式，如星形和移动密度带。我们的数据驱动分析表明，传播带是由地方对称交互驱动的密度梯度而成形，而稳定的星形则是由强粒子交互所致的负压缩性所形成。我们的方法还发现了在粒子速度受到地方密度影响的系统中的物理共同点，这表明了我们的方法的可行性。Physical mechanisms inferred from the data are in excellent agreement with analytical scaling arguments and experimental observations.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/28/cs.LG_2023_07_28/" data-id="cllta0lhf0029ny88bvpceo1o" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/29/eess.IV_2023_07_29/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.IV - 2023-07-29 17:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/28/cs.SD_2023_07_28/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-07-28 123:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
