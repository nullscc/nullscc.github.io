
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-07-28 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="All-for-One and One-For-All: Deep learning-based feature fusion for Synthetic Speech Detection paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.15555 repo_url: None paper_authors: Daniele Mari, Davide Salvi, Paol">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-07-28">
<meta property="og:url" content="https://nullscc.github.io/2023/07/28/cs.SD_2023_07_28/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="All-for-One and One-For-All: Deep learning-based feature fusion for Synthetic Speech Detection paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.15555 repo_url: None paper_authors: Daniele Mari, Davide Salvi, Paol">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-28T15:00:00.000Z">
<meta property="article:modified_time" content="2023-09-23T08:41:45.726Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_07_28" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/28/cs.SD_2023_07_28/" class="article-date">
  <time datetime="2023-07-28T15:00:00.000Z" itemprop="datePublished">2023-07-28</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-07-28
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="All-for-One-and-One-For-All-Deep-learning-based-feature-fusion-for-Synthetic-Speech-Detection"><a href="#All-for-One-and-One-For-All-Deep-learning-based-feature-fusion-for-Synthetic-Speech-Detection" class="headerlink" title="All-for-One and One-For-All: Deep learning-based feature fusion for Synthetic Speech Detection"></a>All-for-One and One-For-All: Deep learning-based feature fusion for Synthetic Speech Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15555">http://arxiv.org/abs/2307.15555</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniele Mari, Davide Salvi, Paolo Bestagini, Simone Milani</li>
<li>for: 防止深度学习和计算机视觉技术的滥用，尤其是在语音频段，预防恶意用户利用深度模拟技术生成假语音，进而导致诈骗或身份盗窃等问题。</li>
<li>methods: 基于文献中提出的三种特征集，实现了一种将这三种特征集融合的模型，以实现对现有方法的改进。</li>
<li>results: 对不同场景和数据集进行测试，证明了该系统具有防御反反馈攻击和泛化能力。<details>
<summary>Abstract</summary>
Recent advances in deep learning and computer vision have made the synthesis and counterfeiting of multimedia content more accessible than ever, leading to possible threats and dangers from malicious users. In the audio field, we are witnessing the growth of speech deepfake generation techniques, which solicit the development of synthetic speech detection algorithms to counter possible mischievous uses such as frauds or identity thefts. In this paper, we consider three different feature sets proposed in the literature for the synthetic speech detection task and present a model that fuses them, achieving overall better performances with respect to the state-of-the-art solutions. The system was tested on different scenarios and datasets to prove its robustness to anti-forensic attacks and its generalization capabilities.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese translation:最近的深度学习和计算机视觉技术的进步使得制作和伪造多媒体内容变得更加容易，可能导致恶意用户的威胁和危险。在音频领域，我们目睹到了语音深度伪造生成技术的增长，这使得适用于防止可能的欺诈和身份盗窃的伪造语音检测算法的开发成为了一项重要的任务。在这篇论文中，我们考虑了Literature中提出的三种不同的特征集，并提出了一种将其融合的模型，实现了与当前最佳解决方案的更好的性能。该系统在不同的场景和数据集上进行了测试，以证明其对反法医攻击的抗性和泛化能力。
</details></li>
</ul>
<hr>
<h2 id="Automated-approach-for-source-location-in-shallow-waters"><a href="#Automated-approach-for-source-location-in-shallow-waters" class="headerlink" title="Automated approach for source location in shallow waters"></a>Automated approach for source location in shallow waters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15491">http://arxiv.org/abs/2307.15491</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/niclas-angele/source_localization">https://github.com/niclas-angele/source_localization</a></li>
<li>paper_authors: Angèle Niclas, Josselin Garnier</li>
<li>for: 这篇论文是为了描述一种完全自动化的 shallow water 中源点和媒体参数的恢复方法。</li>
<li>methods: 该方法使用了 teoretic 工具来理解扭变方法的稳定性，并提出了一种自动分解记录信号中的模态组分的方法。</li>
<li>results: 该方法在实验数据中展示了对实际场景中的右鲸鸟枪响和燃烧声源的有效性。<details>
<summary>Abstract</summary>
This paper proposes a fully automated method for recovering the location of a source and medium parameters in shallow waters. The scenario involves an unknown source emitting low-frequency sound waves in a shallow water environment, and a single hydrophone recording the signal. Firstly, theoretical tools are introduced to understand the robustness of the warping method and to propose and analyze an automated way to separate the modal components of the recorded signal. Secondly, using the spectrogram of each modal component, the paper investigates the best way to recover the modal travel times and provides stability estimates. Finally, a penalized minimization algorithm is presented to recover estimates of the source location and medium parameters. The proposed method is tested on experimental data of right whale gunshot and combustive sound sources, demonstrating its effectiveness in real-world scenarios.
</details>
<details>
<summary>摘要</summary>
这个论文提出了一种完全自动化的方法，用于在浅水中回归源点和媒体参数。情况是一个未知的源在浅水环境中发出低频声波，并且单个水微phone记录了信号。首先，论文介绍了理论工具，以理解扭曲方法的稳定性，并提出了自动分解记录信号的模态组分的方法。其次，使用每个模态组分的spectrogram，论文研究了最好的方法来回归模态旅行时间，并提供了稳定估计。最后，论文提出了一种惩罚最小化算法，用于回归源点和媒体参数的估计。这种方法在实验数据中进行了右鲸鱼枪声和燃烧声源的测试，证明其在实际情况下的有效性。
</details></li>
</ul>
<hr>
<h2 id="Minimally-Supervised-Speech-Synthesis-with-Conditional-Diffusion-Model-and-Language-Model-A-Comparative-Study-of-Semantic-Coding"><a href="#Minimally-Supervised-Speech-Synthesis-with-Conditional-Diffusion-Model-and-Language-Model-A-Comparative-Study-of-Semantic-Coding" class="headerlink" title="Minimally-Supervised Speech Synthesis with Conditional Diffusion Model and Language Model: A Comparative Study of Semantic Coding"></a>Minimally-Supervised Speech Synthesis with Conditional Diffusion Model and Language Model: A Comparative Study of Semantic Coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15484">http://arxiv.org/abs/2307.15484</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chunyu Qiang, Hao Li, Hao Ni, He Qu, Ruibo Fu, Tao Wang, Longbiao Wang, Jianwu Dang</li>
<li>for: 这些纸张是用于提高文本识别和自动语音合成的方法。</li>
<li>methods: 这些方法使用了扩散模型和变量自动编码器来提高提示表示能力，以及扩散模型来解决高维度和声波扭曲问题。</li>
<li>results: 对比基eline方法，这些方法表现更好，并且可以生成多种 expresión 的语音。Here’s the same information in Simplified Chinese:</li>
<li>for: 这些论文是用于提高文本识别和自动语音合成的方法。</li>
<li>methods: 这些方法使用了扩散模型和变量自动编码器来提高提示表示能力，以及扩散模型来解决高维度和声波扭曲问题。</li>
<li>results: 对比基eline方法，这些方法表现更好，并且可以生成多种 expresión 的语音。<details>
<summary>Abstract</summary>
Recently, there has been a growing interest in text-to-speech (TTS) methods that can be trained with minimal supervision by combining two types of discrete speech representations and using two sequence-to-sequence tasks to decouple TTS. To address the challenges associated with high dimensionality and waveform distortion in discrete representations, we propose Diff-LM-Speech, which models semantic embeddings into mel-spectrogram based on diffusion models and introduces a prompt encoder structure based on variational autoencoders and prosody bottlenecks to improve prompt representation capabilities. Autoregressive language models often suffer from missing and repeated words, while non-autoregressive frameworks face expression averaging problems due to duration prediction models. To address these issues, we propose Tetra-Diff-Speech, which designs a duration diffusion model to achieve diverse prosodic expressions. While we expect the information content of semantic coding to be between that of text and acoustic coding, existing models extract semantic coding with a lot of redundant information and dimensionality explosion. To verify that semantic coding is not necessary, we propose Tri-Diff-Speech. Experimental results show that our proposed methods outperform baseline methods. We provide a website with audio samples.
</details>
<details>
<summary>摘要</summary>
近期，有越来越多的关注TEXT-TO-SPEECH（TTS）方法，可以通过最小监督来训练，通过组合两种类型的不同的语音表示方式，并使用两个序列-TO-序列任务来解耦TTS。为了Address高维度和波形扭曲在不同表示方式中的挑战，我们提出了Diff-LM-Speech，它模型语意嵌入 mel-spectrogram 基于扩散模型，并引入了提高描述符能力的提问编码结构，以及基于变量自动编码器和表达瓶颈的 prosody 瓶颈。非autoregressive 框架常常面临缺失和重复的单词问题，而autoregressive 模型则面临表达平均化问题，这是因为duration prediction 模型。为了解决这些问题，我们提出了Tetra-Diff-Speech，它设计了一个扩散duration模型，以实现多种表达的多样化。我们预期semantic coding 信息的内容与文本和声音编码信息之间存在一定的相似性，但现有模型通常会提取大量的重复信息和维度爆炸。为了验证semantic coding 是否真的不必要，我们提出了Tri-Diff-Speech。我们的提议方法在实验中表现出了超越基eline方法的成绩。我们提供了一个网站，包含了各种音频样本。
</details></li>
</ul>
<hr>
<h2 id="The-FlySpeech-Audio-Visual-Speaker-Diarization-System-for-MISP-Challenge-2022"><a href="#The-FlySpeech-Audio-Visual-Speaker-Diarization-System-for-MISP-Challenge-2022" class="headerlink" title="The FlySpeech Audio-Visual Speaker Diarization System for MISP Challenge 2022"></a>The FlySpeech Audio-Visual Speaker Diarization System for MISP Challenge 2022</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15400">http://arxiv.org/abs/2307.15400</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Zhang, Huan Zhao, Yue Li, Bowen Pang, Yannan Wang, Hongji Wang, Wei Rao, Qing Wang, Lei Xie</li>
<li>for: 这个论文描述了在ICASSP 2022 上举行的第二届多Modal信息基于语音处理~(\textbf{MISP}) 挑战中提交的 FlySpeech 说话人分类系统。</li>
<li>methods: 我们开发了一个端到端的音频视频说话人分类系统（AVSD），该系统包括一个唇编码器、一个说话人编码器和一个音频视频解码器。具体来说，我们为了解决分类性能下降的问题，将说话人编码器和音频视频解码器进行共同训练。此外，我们还利用大量预训练的说话人提取器来初始化说话人编码器。</li>
<li>results: 我们的实验结果表明，我们的AVSD系统在不同的说话人数量和背景噪音水平下都具有良好的性能，并且与其他参与者的系统进行比较，我们的系统在大多数情况下具有更高的准确率。<details>
<summary>Abstract</summary>
This paper describes the FlySpeech speaker diarization system submitted to the second \textbf{M}ultimodal \textbf{I}nformation Based \textbf{S}peech \textbf{P}rocessing~(\textbf{MISP}) Challenge held in ICASSP 2022. We develop an end-to-end audio-visual speaker diarization~(AVSD) system, which consists of a lip encoder, a speaker encoder, and an audio-visual decoder. Specifically, to mitigate the degradation of diarization performance caused by separate training, we jointly train the speaker encoder and the audio-visual decoder. In addition, we leverage the large-data pretrained speaker extractor to initialize the speaker encoder.
</details>
<details>
<summary>摘要</summary>
这篇论文描述了我们在ICASSP 2022年度第二届多模态信息基于语音处理~(\textbf{MISP}) 挑战中提交的飞语音 speaker 分类系统。我们开发了一个端到端的音频视频 speaker 分类系统（AVSD），该系统包括一个唇编码器、一个说话者编码器和一个音频视频解码器。具体来说，为了解决分离训练导致的分类性能下降，我们在说话者编码器和音频视频解码器之间进行了联合训练。此外，我们还利用了大量预训练的说话者抽取器来初始化说话者编码器。
</details></li>
</ul>
<hr>
<h2 id="Improving-Audio-Text-Retrieval-via-Hierarchical-Cross-Modal-Interaction-and-Auxiliary-Captions"><a href="#Improving-Audio-Text-Retrieval-via-Hierarchical-Cross-Modal-Interaction-and-Auxiliary-Captions" class="headerlink" title="Improving Audio-Text Retrieval via Hierarchical Cross-Modal Interaction and Auxiliary Captions"></a>Improving Audio-Text Retrieval via Hierarchical Cross-Modal Interaction and Auxiliary Captions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15344">http://arxiv.org/abs/2307.15344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifei Xin, Yuexian Zou</li>
<li>for: 提高音频文本相关 retrieval（ATR）性能， ignore 细腻的cross-modal关系。</li>
<li>methods: 引入层次跨modal交互（HCI）方法，同时探索clip-sentence、segment-phrase和frame-word关系，实现多modal含义比较。</li>
<li>results: 实验显示，我们的HCI方法可以提高ATR性能，同时，我们的auxiliary captions（AC）框架可以提供更好的音频表示，并且可以作为训练数据增强。<details>
<summary>Abstract</summary>
Most existing audio-text retrieval (ATR) methods focus on constructing contrastive pairs between whole audio clips and complete caption sentences, while ignoring fine-grained cross-modal relationships, e.g., short segments and phrases or frames and words. In this paper, we introduce a hierarchical cross-modal interaction (HCI) method for ATR by simultaneously exploring clip-sentence, segment-phrase, and frame-word relationships, achieving a comprehensive multi-modal semantic comparison. Besides, we also present a novel ATR framework that leverages auxiliary captions (AC) generated by a pretrained captioner to perform feature interaction between audio and generated captions, which yields enhanced audio representations and is complementary to the original ATR matching branch. The audio and generated captions can also form new audio-text pairs as data augmentation for training. Experiments show that our HCI significantly improves the ATR performance. Moreover, our AC framework also shows stable performance gains on multiple datasets.
</details>
<details>
<summary>摘要</summary>
现有的音频文本检索（ATR）方法通常是构建整个音频clip和完整的caption句子的对比对，而忽略细致的交叉模态关系，例如短段和短语或帧和单词。在这篇论文中，我们介绍了一种层次跨模态交互（HCI）方法，同时探索clip-sentence、segment-phrase和frame-word关系，实现了多模态semantic比较的全面评估。此外，我们还提出了一种新的ATR框架，利用预训练的captioner生成的auxiliary captions（AC）来实现音频和生成caption之间的特征交互，这对audio表示具有进一步提高的性能，并且与原始ATR匹配分支相комplementary。音频和生成caption还可以组成新的音频-文本对，用于训练的数据增强。实验结果表明，我们的HCI方法具有显著的提升效果，而我们的AC框架也在多个dataset上表现稳定。
</details></li>
</ul>
<hr>
<h2 id="PCNN-A-Lightweight-Parallel-Conformer-Neural-Network-for-Efficient-Monaural-Speech-Enhancement"><a href="#PCNN-A-Lightweight-Parallel-Conformer-Neural-Network-for-Efficient-Monaural-Speech-Enhancement" class="headerlink" title="PCNN: A Lightweight Parallel Conformer Neural Network for Efficient Monaural Speech Enhancement"></a>PCNN: A Lightweight Parallel Conformer Neural Network for Efficient Monaural Speech Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15251">http://arxiv.org/abs/2307.15251</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinmeng Xu, Weiping Tu, Yuhong Yang</li>
<li>for: 提高语音增强效果</li>
<li>methods: 利用CNN和Transformer两种架构，实现并行搅动和自我注意力的结合，并设计了多支杆堆叠 convolution和自我时频频率注意力模块</li>
<li>results: 与现有方法相比，本方法在大多数评估标准中表现出色，同时具有最低的模型参数数量<details>
<summary>Abstract</summary>
Convolutional neural networks (CNN) and Transformer have wildly succeeded in multimedia applications. However, more effort needs to be made to harmonize these two architectures effectively to satisfy speech enhancement. This paper aims to unify these two architectures and presents a Parallel Conformer for speech enhancement. In particular, the CNN and the self-attention (SA) in the Transformer are fully exploited for local format patterns and global structure representations. Based on the small receptive field size of CNN and the high computational complexity of SA, we specially designed a multi-branch dilated convolution (MBDC) and a self-channel-time-frequency attention (Self-CTFA) module. MBDC contains three convolutional layers with different dilation rates for the feature from local to non-local processing. Experimental results show that our method performs better than state-of-the-art methods in most evaluation criteria while maintaining the lowest model parameters.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（CNN）和变换器（Transformer）在多媒体应用中取得了很大成功。然而，为了有效融合这两种架构，还需要更多的努力。这篇论文目标是将这两种架构融合在一起，并提出了并行转换器（Parallel Conformer） для speech enhancement。特别是，我们完全利用了 CNN 和 Transformer 中的自注意力（SA）来捕捉本地格式模式和全球结构表示。基于小覆盖区域大小和自注意力的计算复杂性，我们专门设计了多支分支扩展 convolution（MBDC）和自频时间频率注意力（Self-CTFA）模块。MBDC 包括三层扩展 convolution Layer  WITH different dilation rates，用于从本地到非本地处理。实验结果表明，我们的方法在大多数评价标准下表现更好于现有方法，同时保持最低的模型参数。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Visual-Acoustic-Matching"><a href="#Self-Supervised-Visual-Acoustic-Matching" class="headerlink" title="Self-Supervised Visual Acoustic Matching"></a>Self-Supervised Visual Acoustic Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15064">http://arxiv.org/abs/2307.15064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arjun Somayazulu, Changan Chen, Kristen Grauman</li>
<li>for: 用于实现自然语言处理任务，特别是听说场景中的声音重新synthesize。</li>
<li>methods: 提出了一种自动学习的方法，使用目标场景图像和声音进行联合学习，包括一种新的度量器来衡量剩余的声音信息。</li>
<li>results: 在多个Difficult的数据集上进行训练，与当前最佳方法进行比较，达到了更高的性能。<details>
<summary>Abstract</summary>
Acoustic matching aims to re-synthesize an audio clip to sound as if it were recorded in a target acoustic environment. Existing methods assume access to paired training data, where the audio is observed in both source and target environments, but this limits the diversity of training data or requires the use of simulated data or heuristics to create paired samples. We propose a self-supervised approach to visual acoustic matching where training samples include only the target scene image and audio -- without acoustically mismatched source audio for reference. Our approach jointly learns to disentangle room acoustics and re-synthesize audio into the target environment, via a conditional GAN framework and a novel metric that quantifies the level of residual acoustic information in the de-biased audio. Training with either in-the-wild web data or simulated data, we demonstrate it outperforms the state-of-the-art on multiple challenging datasets and a wide variety of real-world audio and environments.
</details>
<details>
<summary>摘要</summary>
它目标是使一个音频片段在目标听取环境中重新生成，以让它听起来像是在目标环境中录制的。现有的方法假设有对应的训练数据，其中包括音频在源和目标环境中的观察记录，但这限制了训练数据的多样性或需要使用模拟数据或启发法生成对应的样本。我们提出了一种自然语言处理的自主超vised Approach，其中训练样本只包括目标场景图像和音频，而不需要对应的听取环境音频作为参考。我们的方法通过一种Conditional GAN框架和一种新的度量来共同学习分离房间听取特性和重新生成音频到目标环境中，并在多个挑战性数据集和真实世界音频和环境中表现出色。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/28/cs.SD_2023_07_28/" data-id="cloh3sr1e00shh6889xcj1jtj" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/29/eess.IV_2023_07_29/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.IV - 2023-07-29
        
      </div>
    </a>
  
  
    <a href="/2023/07/28/eess.AS_2023_07_28/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.AS - 2023-07-28</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">115</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">55</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">111</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">61</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
