
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-07-28 123:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="All-for-One and One-For-All: Deep learning-based feature fusion for Synthetic Speech Detection paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.15555 repo_url: None paper_authors: Daniele Mari, Davide Salvi, Paol">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-07-28 123:00:00">
<meta property="og:url" content="http://nullscc.github.io/2023/07/28/cs.SD_2023_07_28/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="All-for-One and One-For-All: Deep learning-based feature fusion for Synthetic Speech Detection paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.15555 repo_url: None paper_authors: Daniele Mari, Davide Salvi, Paol">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-27T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:39.933Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_07_28" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/28/cs.SD_2023_07_28/" class="article-date">
  <time datetime="2023-07-27T16:00:00.000Z" itemprop="datePublished">2023-07-28</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-07-28 123:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="All-for-One-and-One-For-All-Deep-learning-based-feature-fusion-for-Synthetic-Speech-Detection"><a href="#All-for-One-and-One-For-All-Deep-learning-based-feature-fusion-for-Synthetic-Speech-Detection" class="headerlink" title="All-for-One and One-For-All: Deep learning-based feature fusion for Synthetic Speech Detection"></a>All-for-One and One-For-All: Deep learning-based feature fusion for Synthetic Speech Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15555">http://arxiv.org/abs/2307.15555</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniele Mari, Davide Salvi, Paolo Bestagini, Simone Milani</li>
<li>for: 防止深度学习和计算机视觉技术的滥用，检测和预测ounterfeit multimedia content的生成和使用。</li>
<li>methods: 使用三种不同的特征集提出在文献中，并将其融合以实现更好的性能，超过当前的解决方案。</li>
<li>results: 在不同的场景和数据集上进行测试，证明系统具有防御反应技术和普适性能。<details>
<summary>Abstract</summary>
Recent advances in deep learning and computer vision have made the synthesis and counterfeiting of multimedia content more accessible than ever, leading to possible threats and dangers from malicious users. In the audio field, we are witnessing the growth of speech deepfake generation techniques, which solicit the development of synthetic speech detection algorithms to counter possible mischievous uses such as frauds or identity thefts. In this paper, we consider three different feature sets proposed in the literature for the synthetic speech detection task and present a model that fuses them, achieving overall better performances with respect to the state-of-the-art solutions. The system was tested on different scenarios and datasets to prove its robustness to anti-forensic attacks and its generalization capabilities.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "deep learning" and "computer vision" were translated as "深度学习" and "计算机视觉" respectively, which are the common terms used in Simplified Chinese.* "synthesis" and "counterfeiting" were translated as "合成" and "假制" respectively, which are the most common terms used in Simplified Chinese.* "malicious users" was translated as "黑客" which is a common term used in Simplified Chinese to refer to malicious hackers or attackers.* "speech deepfake generation techniques" was translated as "语音深度假技术" which is a combination of "语音" (speech), "深度" (deep), and "假技术" (deepfake technique).* "synthetic speech detection algorithms" was translated as "合成语音检测算法" which is a combination of "合成" (synthetic), "语音" (speech), and "检测" (detection).* "anti-forensic attacks" was translated as "反法医攻击" which is a combination of "反" (against), "法" (law), "医" (medical), and "攻击" (attack).
</details></li>
</ul>
<hr>
<h2 id="Automated-approach-for-source-location-in-shallow-waters"><a href="#Automated-approach-for-source-location-in-shallow-waters" class="headerlink" title="Automated approach for source location in shallow waters"></a>Automated approach for source location in shallow waters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15491">http://arxiv.org/abs/2307.15491</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/niclas-angele/source_localization">https://github.com/niclas-angele/source_localization</a></li>
<li>paper_authors: Angèle Niclas, Josselin Garnier</li>
<li>for: 这个论文旨在提出一种完全自动化的方法，用于在浅水环境中恢复源位和媒体参数。</li>
<li>methods: 论文使用了一系列理论工具来理解扭曲方法的稳定性，并提出了一种自动分解记录信号中的模态组分的方法。</li>
<li>results: 论文使用实验数据测试了该方法，并得到了在实际场景中的有效性。<details>
<summary>Abstract</summary>
This paper proposes a fully automated method for recovering the location of a source and medium parameters in shallow waters. The scenario involves an unknown source emitting low-frequency sound waves in a shallow water environment, and a single hydrophone recording the signal. Firstly, theoretical tools are introduced to understand the robustness of the warping method and to propose and analyze an automated way to separate the modal components of the recorded signal. Secondly, using the spectrogram of each modal component, the paper investigates the best way to recover the modal travel times and provides stability estimates. Finally, a penalized minimization algorithm is presented to recover estimates of the source location and medium parameters. The proposed method is tested on experimental data of right whale gunshot and combustive sound sources, demonstrating its effectiveness in real-world scenarios.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种完全自动化的方法，用于在浅水环境中恢复源点和媒体参数。情况是一个未知源在浅水环境中发射低频声波，并且单个水微phone记录了信号。首先，论文介绍了一些理论工具，以理解折叠方法的稳定性，并提出了一种自动分解记录信号的模态组分的方法。然后，使用每个模态组分的spectrogram，论文研究了最好的方法来恢复模态旅行时间，并提供了稳定性估计。最后，论文提出了一种假定最小化算法，用于恢复源点和媒体参数的估计。该方法在实验数据中证明了其效果性，包括右鲸鱼枪声和燃烧声源。
</details></li>
</ul>
<hr>
<h2 id="Minimally-Supervised-Speech-Synthesis-with-Conditional-Diffusion-Model-and-Language-Model-A-Comparative-Study-of-Semantic-Coding"><a href="#Minimally-Supervised-Speech-Synthesis-with-Conditional-Diffusion-Model-and-Language-Model-A-Comparative-Study-of-Semantic-Coding" class="headerlink" title="Minimally-Supervised Speech Synthesis with Conditional Diffusion Model and Language Model: A Comparative Study of Semantic Coding"></a>Minimally-Supervised Speech Synthesis with Conditional Diffusion Model and Language Model: A Comparative Study of Semantic Coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15484">http://arxiv.org/abs/2307.15484</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chunyu Qiang, Hao Li, Hao Ni, He Qu, Ruibo Fu, Tao Wang, Longbiao Wang, Jianwu Dang</li>
<li>For: 提出了一种基于扩散模型和提取器构造的文本译话系统，以便使用 minimal supervision 进行训练，并解决高维度和波形质量问题。* Methods: 使用了扩散模型来模型语义编码，并引入了variational autoencoders和表达瓶颈来改进提示表示能力。Duration diffusion model 也被设计来实现多种表达。* Results: 相比基eline方法，提出的方法得到了更好的表现，并提供了一个网站以便听到样本 зву频。<details>
<summary>Abstract</summary>
Recently, there has been a growing interest in text-to-speech (TTS) methods that can be trained with minimal supervision by combining two types of discrete speech representations and using two sequence-to-sequence tasks to decouple TTS. To address the challenges associated with high dimensionality and waveform distortion in discrete representations, we propose Diff-LM-Speech, which models semantic embeddings into mel-spectrogram based on diffusion models and introduces a prompt encoder structure based on variational autoencoders and prosody bottlenecks to improve prompt representation capabilities. Autoregressive language models often suffer from missing and repeated words, while non-autoregressive frameworks face expression averaging problems due to duration prediction models. To address these issues, we propose Tetra-Diff-Speech, which designs a duration diffusion model to achieve diverse prosodic expressions. While we expect the information content of semantic coding to be between that of text and acoustic coding, existing models extract semantic coding with a lot of redundant information and dimensionality explosion. To verify that semantic coding is not necessary, we propose Tri-Diff-Speech. Experimental results show that our proposed methods outperform baseline methods. We provide a website with audio samples.
</details>
<details>
<summary>摘要</summary>
近些年来，关于具有少量监督的文本识语（TTS）方法的兴趣增长。为了解决高维度和波形扭曲的问题，我们提出了Diff-LM-Speech，它基于扩散模型来模型语义表示，并在mel-spectrogram上引入了variational autoencoders和慢征瓶颈来提高描述符能力。autoregressive语言模型经常会出现缺失和重复的单词，而非autoregressive框架则面临表达平均化问题，这是因为duration预测模型。为解决这些问题，我们提出了Tetra-Diff-Speech，它通过duration扩散模型来实现多种表达。而我们预期semantic coding中信息量介于文本和音频编码之间。现有模型会提取大量的重复信息和维度爆炸。为验证semantic coding不是必要的，我们提出了Tri-Diff-Speech。实验结果表明，我们提出的方法在比基eline方法表现出色。我们提供了一个网站，包含了audio samples。
</details></li>
</ul>
<hr>
<h2 id="The-FlySpeech-Audio-Visual-Speaker-Diarization-System-for-MISP-Challenge-2022"><a href="#The-FlySpeech-Audio-Visual-Speaker-Diarization-System-for-MISP-Challenge-2022" class="headerlink" title="The FlySpeech Audio-Visual Speaker Diarization System for MISP Challenge 2022"></a>The FlySpeech Audio-Visual Speaker Diarization System for MISP Challenge 2022</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15400">http://arxiv.org/abs/2307.15400</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Zhang, Huan Zhao, Yue Li, Bowen Pang, Yannan Wang, Hongji Wang, Wei Rao, Qing Wang, Lei Xie</li>
<li>for: 这篇论文描述了在ICASSP 2022年第二届多Modal信息基于语音处理~(\textbf{MISP})挑战中提交的飞Speech speaker分类系统。</li>
<li>methods: 我们开发了一个端到端的音频视频 speaker分类~(AVSD)系统，该系统包括一个唇编码器、一个说话人编码器和一个音频视频解码器。具体来说，我们为了解决分离训练导致的质量下降，我们将说话人编码器和音频视频解码器共同训练。此外，我们利用大量预训练的说话人提取器来初始化说话人编码器。</li>
<li>results: 我们的实验结果表明，在一些公共语音和视频数据集上，我们的AVSD系统可以具有高度的准确率和稳定性。<details>
<summary>Abstract</summary>
This paper describes the FlySpeech speaker diarization system submitted to the second \textbf{M}ultimodal \textbf{I}nformation Based \textbf{S}peech \textbf{P}rocessing~(\textbf{MISP}) Challenge held in ICASSP 2022. We develop an end-to-end audio-visual speaker diarization~(AVSD) system, which consists of a lip encoder, a speaker encoder, and an audio-visual decoder. Specifically, to mitigate the degradation of diarization performance caused by separate training, we jointly train the speaker encoder and the audio-visual decoder. In addition, we leverage the large-data pretrained speaker extractor to initialize the speaker encoder.
</details>
<details>
<summary>摘要</summary>
这篇论文描述了我们在ICASSP 2022年第二届多Modal信息基于语音处理~(\textbf{MISP}) 挑战中提交的 FlySpeech 语音排序系统。我们开发了一个端到端的音频视频排序系统（AVSD），该系统包括一个唇编码器、一个说话人编码器和一个音频视频解码器。具体来说，为了解决分离训练导致的排序性能下降，我们将说话人编码器和音频视频解码器并行训练。此外，我们利用大量预训练的说话人提取器来初始化说话人编码器。
</details></li>
</ul>
<hr>
<h2 id="Improving-Audio-Text-Retrieval-via-Hierarchical-Cross-Modal-Interaction-and-Auxiliary-Captions"><a href="#Improving-Audio-Text-Retrieval-via-Hierarchical-Cross-Modal-Interaction-and-Auxiliary-Captions" class="headerlink" title="Improving Audio-Text Retrieval via Hierarchical Cross-Modal Interaction and Auxiliary Captions"></a>Improving Audio-Text Retrieval via Hierarchical Cross-Modal Interaction and Auxiliary Captions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15344">http://arxiv.org/abs/2307.15344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifei Xin, Yuexian Zou</li>
<li>for: 提高语音文本检索性能（ATR）</li>
<li>methods: 层次跨模态交互（HCI）方法，同时探索clip-sentence、segment-phrase和frame-word关系，实现全面多模态semantic比较。</li>
<li>results: 实验表明，我们的HCI方法可以明显提高ATR性能，而且我们的auxiliary captions（AC）框架也在多个数据集上显示稳定性。<details>
<summary>Abstract</summary>
Most existing audio-text retrieval (ATR) methods focus on constructing contrastive pairs between whole audio clips and complete caption sentences, while ignoring fine-grained cross-modal relationships, e.g., short segments and phrases or frames and words. In this paper, we introduce a hierarchical cross-modal interaction (HCI) method for ATR by simultaneously exploring clip-sentence, segment-phrase, and frame-word relationships, achieving a comprehensive multi-modal semantic comparison. Besides, we also present a novel ATR framework that leverages auxiliary captions (AC) generated by a pretrained captioner to perform feature interaction between audio and generated captions, which yields enhanced audio representations and is complementary to the original ATR matching branch. The audio and generated captions can also form new audio-text pairs as data augmentation for training. Experiments show that our HCI significantly improves the ATR performance. Moreover, our AC framework also shows stable performance gains on multiple datasets.
</details>
<details>
<summary>摘要</summary>
现有的音频文本搜寻（ATR）方法通常是建立整个音频clip和完整的caption句子之间的对比，而忽略细微的跨媒体关系，例如短段和短句或帧和单词。在这篇文章中，我们引入了一个层次跨媒体互动（HCI）方法，同时探索clip-sentence、segment-phrase和frame-word关系，实现了多modal semantic comparison的全面探索。此外，我们还提出了一个使用预训掌握的captioner生成的auxiliary captions（AC）来进行音频和生成caption之间的特征互动，这个方法可以提高音频表示的性能，并且与原始ATR匹配分支相 komplementary。音频和生成caption可以形成新的音频文本对，用于训练。实验结果显示，我们的HCI方法能够对ATR表现提高。此外，我们的AC框架也在多个数据集上显示稳定的性能提升。
</details></li>
</ul>
<hr>
<h2 id="PCNN-A-Lightweight-Parallel-Conformer-Neural-Network-for-Efficient-Monaural-Speech-Enhancement"><a href="#PCNN-A-Lightweight-Parallel-Conformer-Neural-Network-for-Efficient-Monaural-Speech-Enhancement" class="headerlink" title="PCNN: A Lightweight Parallel Conformer Neural Network for Efficient Monaural Speech Enhancement"></a>PCNN: A Lightweight Parallel Conformer Neural Network for Efficient Monaural Speech Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15251">http://arxiv.org/abs/2307.15251</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinmeng Xu, Weiping Tu, Yuhong Yang</li>
<li>for: 这篇论文目的是提出一种能够有效融合卷积神经网络和转换器的Speech增强方法。</li>
<li>methods: 该方法使用了卷积神经网络和转换器两种不同的架构，并通过特制的多支分支扩展层和自适应时频注意力模块来挖掘本地Format模式和全球结构表示。</li>
<li>results: 实验结果表明，该方法在大多数评价标准中表现更好于当前State-of-the-art方法，而且保持最低的模型参数数量。<details>
<summary>Abstract</summary>
Convolutional neural networks (CNN) and Transformer have wildly succeeded in multimedia applications. However, more effort needs to be made to harmonize these two architectures effectively to satisfy speech enhancement. This paper aims to unify these two architectures and presents a Parallel Conformer for speech enhancement. In particular, the CNN and the self-attention (SA) in the Transformer are fully exploited for local format patterns and global structure representations. Based on the small receptive field size of CNN and the high computational complexity of SA, we specially designed a multi-branch dilated convolution (MBDC) and a self-channel-time-frequency attention (Self-CTFA) module. MBDC contains three convolutional layers with different dilation rates for the feature from local to non-local processing. Experimental results show that our method performs better than state-of-the-art methods in most evaluation criteria while maintaining the lowest model parameters.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（CNN）和转换器（Transformer）在多媒体应用中获得了很大成功。然而，为了有效融合这两种架构，还需要更多的努力。这篇论文目的是将这两种架构融合在一起，并提出了平行转换器（Parallel Conformer） для语音增强。具体来说，CNN和转换器中的自注意（SA）都被完全利用了，以捕捉本地格式模式和全球结构表示。由于小感知场大小和自注意的计算复杂度，我们专门设计了多支束层核（MBDC）和自频时空尺度注意（Self-CTFA）模块。MBDC包括三层核心层，每层都有不同的扩散率，以捕捉从本地到非本地处理的特征。实验结果表明，我们的方法在大多数评估标准下表现更好，而且保持最低的模型参数。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Visual-Acoustic-Matching"><a href="#Self-Supervised-Visual-Acoustic-Matching" class="headerlink" title="Self-Supervised Visual Acoustic Matching"></a>Self-Supervised Visual Acoustic Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15064">http://arxiv.org/abs/2307.15064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arjun Somayazulu, Changan Chen, Kristen Grauman</li>
<li>for: 这个论文的目的是提出一种自主学习的视觉音频匹配方法，以便在Target acoustic environment中重新生成音频clip，使其具有Target environment的音频特征。</li>
<li>methods: 这个方法使用conditional GAN框架，并提出了一种新的度量 metric来衡量剩余的音频信息水平。它通过自动分离room acoustics和重新生成音频，来实现视觉音频匹配。</li>
<li>results: 该方法在多个复杂的数据集上表现出色，并且可以在各种真实世界的音频和环境下进行匹配。论文的实验结果表明，它超过了现有的状态则艺的表现。<details>
<summary>Abstract</summary>
Acoustic matching aims to re-synthesize an audio clip to sound as if it were recorded in a target acoustic environment. Existing methods assume access to paired training data, where the audio is observed in both source and target environments, but this limits the diversity of training data or requires the use of simulated data or heuristics to create paired samples. We propose a self-supervised approach to visual acoustic matching where training samples include only the target scene image and audio -- without acoustically mismatched source audio for reference. Our approach jointly learns to disentangle room acoustics and re-synthesize audio into the target environment, via a conditional GAN framework and a novel metric that quantifies the level of residual acoustic information in the de-biased audio. Training with either in-the-wild web data or simulated data, we demonstrate it outperforms the state-of-the-art on multiple challenging datasets and a wide variety of real-world audio and environments.
</details>
<details>
<summary>摘要</summary>
干擦匹配目标是将音频clip重新synthesize为如果在目标听取环境中录制的样本。现有方法假设有对应的培训数据，其中包括源和目标环境中的音频，但这限制了培训数据的多样性或需要使用模拟数据或规则来创建对应的样本。我们提出了一种自主超vised Approach to visual acoustic matching，其中培训样本只包括目标场景图像和音频，而不包括不匹配的源音频参考。我们的方法同时学习分离房间听取特性和重新synthesize音频到目标环境中，通过单个GAN框架和一种新的度量量化干擦听取信息在去除biased audio中的水平。我们在使用实际网络数据或模拟数据进行培训时，demonstrate其超过了现有状态的多个挑战性数据集和多种真实的音频和环境。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://nullscc.github.io/2023/07/28/cs.SD_2023_07_28/" data-id="cllshr36p004jo988azws9czj" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/28/cs.LG_2023_07_28/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-07-28 18:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/28/eess.AS_2023_07_28/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.AS - 2023-07-28 22:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'', root:''}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
