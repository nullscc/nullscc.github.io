
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-07-25 18:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Multi-GPU Approach for Training of Graph ML Models on large CFD Meshes paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.13592 repo_url: None paper_authors: Sebastian Strönisch, Maximilian Sander, Andreas Knüpfer,">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-07-25 18:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/07/25/cs.LG_2023_07_25/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Multi-GPU Approach for Training of Graph ML Models on large CFD Meshes paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.13592 repo_url: None paper_authors: Sebastian Strönisch, Maximilian Sander, Andreas Knüpfer,">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-24T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:20.469Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_07_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/25/cs.LG_2023_07_25/" class="article-date">
  <time datetime="2023-07-24T16:00:00.000Z" itemprop="datePublished">2023-07-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-07-25 18:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Multi-GPU-Approach-for-Training-of-Graph-ML-Models-on-large-CFD-Meshes"><a href="#Multi-GPU-Approach-for-Training-of-Graph-ML-Models-on-large-CFD-Meshes" class="headerlink" title="Multi-GPU Approach for Training of Graph ML Models on large CFD Meshes"></a>Multi-GPU Approach for Training of Graph ML Models on large CFD Meshes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13592">http://arxiv.org/abs/2307.13592</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sebastian Strönisch, Maximilian Sander, Andreas Knüpfer, Marcus Meyer</li>
<li>for: 这篇论文的目的是提出一种基于 mesh 的数值方法，以快速预测流体动力学解。</li>
<li>methods: 本论文使用的方法包括 graph neural network 和 numerical solver，实现了分布式训练和 halo exchange。</li>
<li>results: 实验结果显示，提出的对 mesh 的 surrogate model 比 traditionally trained distributed model 来的预测精度较差。可能的解释、改进和未来方向都被讨论了。<details>
<summary>Abstract</summary>
Mesh-based numerical solvers are an important part in many design tool chains. However, accurate simulations like computational fluid dynamics are time and resource consuming which is why surrogate models are employed to speed-up the solution process. Machine Learning based surrogate models on the other hand are fast in predicting approximate solutions but often lack accuracy. Thus, the development of the predictor in a predictor-corrector approach is the focus here, where the surrogate model predicts a flow field and the numerical solver corrects it. This paper scales a state-of-the-art surrogate model from the domain of graph-based machine learning to industry-relevant mesh sizes of a numerical flow simulation. The approach partitions and distributes the flow domain to multiple GPUs and provides halo exchange between these partitions during training. The utilized graph neural network operates directly on the numerical mesh and is able to preserve complex geometries as well as all other properties of the mesh. The proposed surrogate model is evaluated with an application on a three dimensional turbomachinery setup and compared to a traditionally trained distributed model. The results show that the traditional approach produces superior predictions and outperforms the proposed surrogate model. Possible explanations, improvements and future directions are outlined.
</details>
<details>
<summary>摘要</summary>
mesh-based numerical solvers 是多个设计工具链中的重要部分。然而，如果需要精度的算法，例如计算流体力学，它们可能需要大量的时间和资源，因此使用代理模型来加速解决方案。机器学习基于的代理模型可以快速预测近似解决方案，但它们通常缺乏精度。因此，这里的焦点是发展一个预测器-修正器方法，其中的代理模型预测了流场，而numerical solver则修正它。在这篇文章中，我们将一个state-of-the-art的graph-based machine learning surrogate model scales down to industry-relevant mesh sizes of a numerical flow simulation.我们的方法包括 dividing and distributing the flow domain to multiple GPUs, and providing halo exchange between these partitions during training.在训练中，我们使用的graph neural network直接操作 numerical mesh，并能够保留 mesh 的复杂 геометрія和所有其他属性。我们将这个surrogate model评估于三维液体机构设置中，并与传统的分布式模型进行比较。结果显示，传统方法生成了更好的预测，并超越了我们的提案的代理模型。我们提出了可能的解释、改进和未来方向。
</details></li>
</ul>
<hr>
<h2 id="Settling-the-Sample-Complexity-of-Online-Reinforcement-Learning"><a href="#Settling-the-Sample-Complexity-of-Online-Reinforcement-Learning" class="headerlink" title="Settling the Sample Complexity of Online Reinforcement Learning"></a>Settling the Sample Complexity of Online Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13586">http://arxiv.org/abs/2307.13586</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihan Zhang, Yuxin Chen, Jason D. Lee, Simon S. Du</li>
<li>for: 这篇论文的目的是解决在在线 reinforcement learning 中的数据效率问题。</li>
<li>methods: 这篇论文使用了修改后的幂值传播（MVP）算法，并提供了一种新的 regret 分解策略和分析方法来解决在线 RL 中的统计依赖关系问题。</li>
<li>results: 这篇论文证明了 Modified MVP 算法在具有 finite-horizon 不同状态的 Markov 决策过程中可以达到最优的 regret，并且不需要任何燃烧成本。此外，论文还提供了一种 PAC 样本复杂度（i.e., 每集需要的集数），这是最优的 для整个 $\varepsilon$-范围。<details>
<summary>Abstract</summary>
A central issue lying at the heart of online reinforcement learning (RL) is data efficiency. While a number of recent works achieved asymptotically minimal regret in online RL, the optimality of these results is only guaranteed in a ``large-sample'' regime, imposing enormous burn-in cost in order for their algorithms to operate optimally. How to achieve minimax-optimal regret without incurring any burn-in cost has been an open problem in RL theory.   We settle this problem for the context of finite-horizon inhomogeneous Markov decision processes. Specifically, we prove that a modified version of Monotonic Value Propagation (MVP), a model-based algorithm proposed by \cite{zhang2020reinforcement}, achieves a regret on the order of (modulo log factors) \begin{equation*}   \min\big\{ \sqrt{SAH^3K}, \,HK \big\}, \end{equation*} where $S$ is the number of states, $A$ is the number of actions, $H$ is the planning horizon, and $K$ is the total number of episodes. This regret matches the minimax lower bound for the entire range of sample size $K\geq 1$, essentially eliminating any burn-in requirement. It also translates to a PAC sample complexity (i.e., the number of episodes needed to yield $\varepsilon$-accuracy) of $\frac{SAH^3}{\varepsilon^2}$ up to log factor, which is minimax-optimal for the full $\varepsilon$-range.   Further, we extend our theory to unveil the influences of problem-dependent quantities like the optimal value/cost and certain variances. The key technical innovation lies in the development of a new regret decomposition strategy and a novel analysis paradigm to decouple complicated statistical dependency -- a long-standing challenge facing the analysis of online RL in the sample-hungry regime.
</details>
<details>
<summary>摘要</summary>
在在线强化学习中，一个中心问题是数据效率。虽然一些最近的研究已经实现了 asymptotically 最小的 regret，但这些结果只在大样本Registry  regime 内是可靠的，需要巨大的燃烧成本来使算法运行最佳。如何在强化学习理论中实现 minimax-optimal regret 而无需燃烧成本是一个开放的问题。我们解决了这个问题，并且为 finite-horizon 不同状态 Markov decision process 提供了一个修改后的 Monotonic Value Propagation（MVP）算法，该算法可以在各种样本大小 $K\geq 1$ 的范围内实现 regret 的最小化。我们证明了这个 regret 的大小为 $(SAH^3K)^{-1} \min\left\{\sqrt{SAH^3K},HK\right\}$，其中 $S$ 是状态数量， $A$ 是动作数量， $H$ 是规划时间长度， $K$ 是总共计集数。这个 regret 与全范围内的最小下界匹配，实际上消除了所有燃烧需求。它还翻译到一个 PAC 样本复杂度（即需要多少集数来实现 $\varepsilon$-精度），该复杂度为 $\frac{SAH^3}{\varepsilon^2}$，其中 $\varepsilon$ 是精度标准。这个复杂度是 minimax-optimal 的 для整个 $\varepsilon$-范围。此外，我们还扩展了我们的理论，以探讨问题依赖的问题量，如优化值/成本和某些方差。我们的关键技术创新在于开发了一种新的 regret 分解策略和一种新的分析方法，以解决在线强化学习在样本匮乏 regime 中的复杂的统计依赖关系。
</details></li>
</ul>
<hr>
<h2 id="Piecewise-Linear-Functions-Representable-with-Infinite-Width-Shallow-ReLU-Neural-Networks"><a href="#Piecewise-Linear-Functions-Representable-with-Infinite-Width-Shallow-ReLU-Neural-Networks" class="headerlink" title="Piecewise Linear Functions Representable with Infinite Width Shallow ReLU Neural Networks"></a>Piecewise Linear Functions Representable with Infinite Width Shallow ReLU Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14373">http://arxiv.org/abs/2307.14373</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarah McCarty</li>
<li>for: 本研究探讨了无穷宽深度学习模型下的连续割合函数表示。</li>
<li>methods: 该研究使用了矩阵分解和投影方法来表示连续割合函数。</li>
<li>results: 研究证明了 ONgie et al. 的假设，即任何连续割合函数都可以通过无穷宽深度学习模型表示。<details>
<summary>Abstract</summary>
This paper analyzes representations of continuous piecewise linear functions with infinite width, finite cost shallow neural networks using the rectified linear unit (ReLU) as an activation function. Through its integral representation, a shallow neural network can be identified by the corresponding signed, finite measure on an appropriate parameter space. We map these measures on the parameter space to measures on the projective $n$-sphere cross $\mathbb{R}$, allowing points in the parameter space to be bijectively mapped to hyperplanes in the domain of the function. We prove a conjecture of Ongie et al. that every continuous piecewise linear function expressible with this kind of infinite width neural network is expressible as a finite width shallow ReLU neural network.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:这篇论文研究了无穷宽的抽象函数的表示，使用finite cost shallow neural network和Rectified Linear Unit（ReLU）作为激活函数。通过其 интеграル表示，一个抽象函数可以被相应的签名、finite measure标识在适当的参数空间上。我们将这些度量在参数空间映射到项目ive $n$-sphere cross $\mathbb{R}$上，使得参数空间中的点可以一一映射到函数的域中的hyperplane。我们证明了Ongie等人的 conjecture，即任何可表示为无穷宽 neural network的连续piecewise linear函数都可以表示为finite width shallow ReLU neural network。
</details></li>
</ul>
<hr>
<h2 id="Comparing-Forward-and-Inverse-Design-Paradigms-A-Case-Study-on-Refractory-High-Entropy-Alloys"><a href="#Comparing-Forward-and-Inverse-Design-Paradigms-A-Case-Study-on-Refractory-High-Entropy-Alloys" class="headerlink" title="Comparing Forward and Inverse Design Paradigms: A Case Study on Refractory High-Entropy Alloys"></a>Comparing Forward and Inverse Design Paradigms: A Case Study on Refractory High-Entropy Alloys</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13581">http://arxiv.org/abs/2307.13581</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arindam Debnath, Lavanya Raman, Wenjie Li, Adam M. Krajewski, Marcia Ahn, Shuang Lin, Shunli Shang, Allison M. Beese, Zi-Kui Liu, Wesley F. Reinhart</li>
<li>for: 本研究的目标是比较前向和反向设计模型 paradigm的性能，以帮助更好地选择适合的设计方法。</li>
<li>methods: 本研究使用了两个 caso studies of refractory high-entropy alloy design，分别是用于评估不同的目标和约束。</li>
<li>results: 研究结果表明，反向设计模型在某些情况下可以更高效地找到满足目标性能的材料。<details>
<summary>Abstract</summary>
The rapid design of advanced materials is a topic of great scientific interest. The conventional, ``forward'' paradigm of materials design involves evaluating multiple candidates to determine the best candidate that matches the target properties. However, recent advances in the field of deep learning have given rise to the possibility of an ``inverse'' design paradigm for advanced materials, wherein a model provided with the target properties is able to find the best candidate. Being a relatively new concept, there remains a need to systematically evaluate how these two paradigms perform in practical applications. Therefore, the objective of this study is to directly, quantitatively compare the forward and inverse design modeling paradigms. We do so by considering two case studies of refractory high-entropy alloy design with different objectives and constraints and comparing the inverse design method to other forward schemes like localized forward search, high throughput screening, and multi objective optimization.
</details>
<details>
<summary>摘要</summary>
“高级材料的快速设计是科学领域中的一个热点话题。传统的“前进”模式的材料设计方法是评估多个候选者，以确定最佳符合目标性能的候选者。然而，近年来深度学习的发展使得“反向”设计模式在高级材料设计中得到了可能。这是一个相对较新的概念，因此还需要系统地评估这两种模式在实际应用中的性能。因此，本研究的目标是直接、量化地比较前进和反向设计模式的表现。我们通过考虑两个高级碎高 entropy合金的设计案例，每个案例都有不同的目标和约束，并与其他前进方案如本地前进搜索、高通过率检索和多目标优化进行比较。”
</details></li>
</ul>
<hr>
<h2 id="Reinterpreting-survival-analysis-in-the-universal-approximator-age"><a href="#Reinterpreting-survival-analysis-in-the-universal-approximator-age" class="headerlink" title="Reinterpreting survival analysis in the universal approximator age"></a>Reinterpreting survival analysis in the universal approximator age</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13579">http://arxiv.org/abs/2307.13579</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sdittmer/survival_analysis_sumo_plus_plus">https://github.com/sdittmer/survival_analysis_sumo_plus_plus</a></li>
<li>paper_authors: Sören Dittmer, Michael Roberts, Jacobus Preller, AIX COVNET, James H. F. Rudd, John A. D. Aston, Carola-Bibiane Schönlieb</li>
<li>for: 本研究旨在提供深度学习中应用生存分析的工具，以便充分发挥生存分析的潜力。</li>
<li>methods: 本文提出了一种新的损失函数、评价指标和第一个可 provin 的 universal approximating network，可以无需数值integation生成生存曲线。</li>
<li>results: 对于一个大规模的数据集，我们的损失函数和模型在与其他方法进行比较时表现出色，得到了更好的结果。<details>
<summary>Abstract</summary>
Survival analysis is an integral part of the statistical toolbox. However, while most domains of classical statistics have embraced deep learning, survival analysis only recently gained some minor attention from the deep learning community. This recent development is likely in part motivated by the COVID-19 pandemic. We aim to provide the tools needed to fully harness the potential of survival analysis in deep learning. On the one hand, we discuss how survival analysis connects to classification and regression. On the other hand, we provide technical tools. We provide a new loss function, evaluation metrics, and the first universal approximating network that provably produces survival curves without numeric integration. We show that the loss function and model outperform other approaches using a large numerical study.
</details>
<details>
<summary>摘要</summary>
生存分析是统计工具箱中的一个重要组成部分。然而，而most domains of classical statistics已经欢迎了深度学习，生存分析只дав一些小的注意力从深度学习社区。这种最近的发展可能是由COVID-19大流行所驱动的。我们想要提供需要抓住生存分析在深度学习中的潜力的工具。一方面，我们讨论了生存分析与分类和回归之间的连接。另一方面，我们提供技术工具。我们提供了一个新的损失函数，评估指标，以及第一个可以无数学 интегралы生成存生曲线的通用近似网络。我们展示了损失函数和模型在大量数字研究中表现出色。
</details></li>
</ul>
<hr>
<h2 id="PT-mathrm-L-p-Partial-Transport-mathrm-L-p-Distances"><a href="#PT-mathrm-L-p-Partial-Transport-mathrm-L-p-Distances" class="headerlink" title="PT$\mathrm{L}^{p}$: Partial Transport $\mathrm{L}^{p}$ Distances"></a>PT$\mathrm{L}^{p}$: Partial Transport $\mathrm{L}^{p}$ Distances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13571">http://arxiv.org/abs/2307.13571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinran Liu, Yikun Bai, Huy Tran, Zhanqi Zhu, Matthew Thorpe, Soheil Kolouri</li>
<li>for: 本研究旨在提出一种新的策略来比较通用信号，基于优化交通框架。</li>
<li>methods: 该研究使用优化交通距离作为比较通用信号的metric。</li>
<li>results: 研究人员提出了一种新的策略，可以快速比较通用信号，并且可以在信号分类和最近邻居分类中应用。<details>
<summary>Abstract</summary>
Optimal transport and its related problems, including optimal partial transport, have proven to be valuable tools in machine learning for computing meaningful distances between probability or positive measures. This success has led to a growing interest in defining transport-based distances that allow for comparing signed measures and, more generally, multi-channeled signals. Transport $\mathrm{L}^{p}$ distances are notable extensions of the optimal transport framework to signed and possibly multi-channeled signals. In this paper, we introduce partial transport $\mathrm{L}^{p}$ distances as a new family of metrics for comparing generic signals, benefiting from the robustness of partial transport distances. We provide theoretical background such as the existence of optimal plans and the behavior of the distance in various limits. Furthermore, we introduce the sliced variation of these distances, which allows for rapid comparison of generic signals. Finally, we demonstrate the application of the proposed distances in signal class separability and nearest neighbor classification.
</details>
<details>
<summary>摘要</summary>
优化运输和相关的问题，包括优化部分运输，在机器学习中证明是有价值的工具，用于计算概率或正的度量的意义。这一成功引起了对定义基于运输的距离，用于比较签名授权和、更一般地多渠道信号的兴趣。运输LP距离是优化运输框架的扩展，用于比较签名或多渠道信号。在这篇论文中，我们引入部分运输LP距离作为比较普通信号的新家族距离，受到部分运输距离的稳定性的启发。我们提供了理论背景，包括优化计划的存在和距离在不同的限制下的行为。此外，我们介绍了割辑变化的这些距离，允许快速比较普通信号。最后，我们示出了提议的距离在信号分类和最近邻居分类中的应用。
</details></li>
</ul>
<hr>
<h2 id="Introducing-Hybrid-Modeling-with-Time-series-Transformers-A-Comparative-Study-of-Series-and-Parallel-Approach-in-Batch-Crystallization"><a href="#Introducing-Hybrid-Modeling-with-Time-series-Transformers-A-Comparative-Study-of-Series-and-Parallel-Approach-in-Batch-Crystallization" class="headerlink" title="Introducing Hybrid Modeling with Time-series-Transformers: A Comparative Study of Series and Parallel Approach in Batch Crystallization"></a>Introducing Hybrid Modeling with Time-series-Transformers: A Comparative Study of Series and Parallel Approach in Batch Crystallization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05749">http://arxiv.org/abs/2308.05749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niranjan Sitapure, Joseph S Kwon</li>
<li>for: 这种研究旨在开发一种基于物理学原理和机器学习模型的混合模型，以提高批凝聚过程的预测性能和可 interpretability。</li>
<li>methods: 该研究使用了注意力基于时间序列变换器（TST），并结合了多头注意力机制和位置编码来捕捉长期和短期变化的过程状态。</li>
<li>results: 研究发现，使用 TST 基于混合模型可以提高批凝聚过程的预测性能和可 interpretability，并且可以提供更好的解释性。两种不同的配置（系列和平行）的 TST 基于混合模型都被构建和比较，得到了 $[10, 50]\times10^{-4}$ 的 normalized-mean-square-error 和 $R^2$ 值大于 0.99。<details>
<summary>Abstract</summary>
Most existing digital twins rely on data-driven black-box models, predominantly using deep neural recurrent, and convolutional neural networks (DNNs, RNNs, and CNNs) to capture the dynamics of chemical systems. However, these models have not seen the light of day, given the hesitance of directly deploying a black-box tool in practice due to safety and operational issues. To tackle this conundrum, hybrid models combining first-principles physics-based dynamics with machine learning (ML) models have increased in popularity as they are considered a 'best of both worlds' approach. That said, existing simple DNN models are not adept at long-term time-series predictions and utilizing contextual information on the trajectory of the process dynamics. Recently, attention-based time-series transformers (TSTs) that leverage multi-headed attention mechanism and positional encoding to capture long-term and short-term changes in process states have shown high predictive performance. Thus, a first-of-a-kind, TST-based hybrid framework has been developed for batch crystallization, demonstrating improved accuracy and interpretability compared to traditional black-box models. Specifically, two different configurations (i.e., series and parallel) of TST-based hybrid models are constructed and compared, which show a normalized-mean-square-error (NMSE) in the range of $[10, 50]\times10^{-4}$ and an $R^2$ value over 0.99. Given the growing adoption of digital twins, next-generation attention-based hybrid models are expected to play a crucial role in shaping the future of chemical manufacturing.
</details>
<details>
<summary>摘要</summary>
现有的数字双胞虫多数采用数据驱动的黑盒模型，主要使用深度神经环境网络（DNNs）、循环神经网络（RNNs）和卷积神经网络（CNNs）来捕捉化学系统的动态。然而，这些模型尚未得到实际应用，因为安全和运营问题。为解决这个问题，将物理基础知识与机器学习（ML）模型结合的混合模型在 популяр度上升。然而，现有的简单DNN模型不具备长期时间序列预测和利用过程动态趋势的能力。在此基础之上，具有多头注意力机制和位置编码的时间序列变换器（TSTs）在过程状态中捕捉长期和短期变化，并显示出高预测性能。因此，一种首次创造的TST-基于混合框架在批凝聚中得到了应用，并实现了传统黑盒模型的改进准确性和可读性。具体来说，这种混合模型的两种不同配置（即 série和平行）在NMSE范围内为[$10, 50]乘10^-4，$R^2$值高于0.99。随着数字双胞虫的推广，下一代注意力基本的混合模型将在化学制造中扮演关键的角色。
</details></li>
</ul>
<hr>
<h2 id="Decision-Focused-Learning-Foundations-State-of-the-Art-Benchmark-and-Future-Opportunities"><a href="#Decision-Focused-Learning-Foundations-State-of-the-Art-Benchmark-and-Future-Opportunities" class="headerlink" title="Decision-Focused Learning: Foundations, State of the Art, Benchmark and Future Opportunities"></a>Decision-Focused Learning: Foundations, State of the Art, Benchmark and Future Opportunities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13565">http://arxiv.org/abs/2307.13565</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/predopt/predopt-benchmarks">https://github.com/predopt/predopt-benchmarks</a></li>
<li>paper_authors: Jayanta Mandi, James Kotary, Senne Berden, Maxime Mulamba, Victor Bucarey, Tias Guns, Ferdinando Fioretto</li>
<li>for: 这篇论文旨在提供一个全面的决策学习（DFL）综述，涵盖了不同的技术和方法，以及适用于决策模型的优化。</li>
<li>methods: 本文提出了一种新的决策学习方法，即结合预测和优化的综合系统，以解决在不确定环境下的决策问题。它还提出了一种分类DFL方法，并进行了广泛的实验评估。</li>
<li>results: 本文的实验结果表明，DFL方法可以提高决策模型的准确率和稳定性，并且可以应用于多种实际应用场景。同时，本文还提供了一些现有和未来决策学习研究的可能性。<details>
<summary>Abstract</summary>
Decision-focused learning (DFL) is an emerging paradigm in machine learning which trains a model to optimize decisions, integrating prediction and optimization in an end-to-end system. This paradigm holds the promise to revolutionize decision-making in many real-world applications which operate under uncertainty, where the estimation of unknown parameters within these decision models often becomes a substantial roadblock. This paper presents a comprehensive review of DFL. It provides an in-depth analysis of the various techniques devised to integrate machine learning and optimization models, introduces a taxonomy of DFL methods distinguished by their unique characteristics, and conducts an extensive empirical evaluation of these methods proposing suitable benchmark dataset and tasks for DFL. Finally, the study provides valuable insights into current and potential future avenues in DFL research.
</details>
<details>
<summary>摘要</summary>
决策关注学习（DFL）是一种emerging paradigm在机器学习领域，它将模型训练为优化决策，同时整合预测和优化在整个端到端系统中。这种 paradigm 承诺可以革命化决策在不确定环境中的很多实际应用中，因为在决策模型中未知参数的估计通常成为了一个重大障碍。本文提供了DFL的全面评论，包括了不同方法的整合机器学习和优化模型的深入分析，并提出了DFL方法的分类，以及对这些方法的广泛的实验评估。最后，这种研究提供了有价值的对DFL研究的当前和未来方向的洞察。
</details></li>
</ul>
<hr>
<h2 id="Node-Injection-Link-Stealing-Attack"><a href="#Node-Injection-Link-Stealing-Attack" class="headerlink" title="Node Injection Link Stealing Attack"></a>Node Injection Link Stealing Attack</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13548">http://arxiv.org/abs/2307.13548</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oualid Zari, Javier Parra-Arnau, Ayşe Ünsal, Melek Önen</li>
<li>for: 本研究探讨了图神经网络（GNNs）中隐私漏洞的攻击方法，以及如何保护隐私的方法。</li>
<li>methods: 我们提出了一种隐蔽和有效的攻击方法，可以在图数据中推断私有链接。我们还提出了一种使用差分隐私（DP）机制来减轻攻击的影响。</li>
<li>results: 我们的攻击方法在推断私有链接方面表现出色，比前一个状态的方法更高效。我们还分析了采用DP机制来保护隐私的代价和利用性。<details>
<summary>Abstract</summary>
In this paper, we present a stealthy and effective attack that exposes privacy vulnerabilities in Graph Neural Networks (GNNs) by inferring private links within graph-structured data. Focusing on the inductive setting where new nodes join the graph and an API is used to query predictions, we investigate the potential leakage of private edge information. We also propose methods to preserve privacy while maintaining model utility. Our attack demonstrates superior performance in inferring the links compared to the state of the art. Furthermore, we examine the application of differential privacy (DP) mechanisms to mitigate the impact of our proposed attack, we analyze the trade-off between privacy preservation and model utility. Our work highlights the privacy vulnerabilities inherent in GNNs, underscoring the importance of developing robust privacy-preserving mechanisms for their application.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种隐蔽的和有效的攻击，暴露了图神经网络（GNNs）中的隐私泄露问题。我们将注意力集中在新节点加入图时的拟合设定下，并使用 API 来查询预测结果。我们研究了新节点加入图时可能泄露的私有边信息的可能性。此外，我们还提出了保持隐私的方法，以保持模型的实用性。我们的攻击表现出色，在推断链接方面超过了现有的状态。此外，我们还研究了应用 differential privacy（DP）机制来减轻我们提出的攻击的影响，并分析了隐私保护和模型实用性之间的交易。我们的工作强调了 GNNs 中的隐私泄露问题，并高亮了开发Robust隐私保护机制的重要性。
</details></li>
</ul>
<hr>
<h2 id="Transfer-Learning-for-Portfolio-Optimization"><a href="#Transfer-Learning-for-Portfolio-Optimization" class="headerlink" title="Transfer Learning for Portfolio Optimization"></a>Transfer Learning for Portfolio Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13546">http://arxiv.org/abs/2307.13546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoyang Cao, Haotian Gu, Xin Guo, Mathieu Rosenbaum</li>
<li>for: 本研究探讨了通过转移学习技术解决金融股票优化问题的可能性。</li>
<li>methods: 本研究引入了一种新的概念 called “转移风险”，并在转移学习优化框架中进行了一系列的数值实验。</li>
<li>results: 实验结果显示，转移风险与转移学习方法的总性表现之间存在强相关关系，并且可以作为”转移可能性”的可靠指标；转移风险可以提供一种计算效率高的方法来选择合适的源任务；实验还为跨大陆、跨领域和跨频率的股票管理提供了有价值的新视角。<details>
<summary>Abstract</summary>
In this work, we explore the possibility of utilizing transfer learning techniques to address the financial portfolio optimization problem. We introduce a novel concept called "transfer risk", within the optimization framework of transfer learning. A series of numerical experiments are conducted from three categories: cross-continent transfer, cross-sector transfer, and cross-frequency transfer. In particular, 1. a strong correlation between the transfer risk and the overall performance of transfer learning methods is established, underscoring the significance of transfer risk as a viable indicator of "transferability"; 2. transfer risk is shown to provide a computationally efficient way to identify appropriate source tasks in transfer learning, enhancing the efficiency and effectiveness of the transfer learning approach; 3. additionally, the numerical experiments offer valuable new insights for portfolio management across these different settings.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们探讨了使用传输学习技术来解决金融投资优化问题的可能性。我们引入了一新的概念called "传输风险"，并在传输学习优化框架中进行了数学实验。我们从三类实验中进行了数值实验：跨洲传输、跨领域传输和跨频传输。具体来说，我们发现了以下结论：1. 传输风险和传输学习方法的总性表现之间存在强相关性，这说明了传输风险的重要性作为"传输可用性"的指标。2. 传输风险提供了一种计算效率高的方法来选择合适的源任务，从而提高了传输学习方法的效率和效果。3. 数值实验还提供了价值的新意见 для资产管理在不同的设置下。
</details></li>
</ul>
<hr>
<h2 id="A-model-for-efficient-dynamical-ranking-in-networks"><a href="#A-model-for-efficient-dynamical-ranking-in-networks" class="headerlink" title="A model for efficient dynamical ranking in networks"></a>A model for efficient dynamical ranking in networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13544">http://arxiv.org/abs/2307.13544</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Della Vecchia, Kibidi Neocosmos, Daniel B. Larremore, Cristopher Moore, Caterina De Bacco</li>
<li>for: 这个论文的目的是用物理学约束来推断 directive temporal networks 中每个节点的动态排名。</li>
<li>methods: 这个论文使用的方法是解一个线性方程组，需要一个参数调整，这使得算法可扩展和高效。</li>
<li>results: 这个论文的实验表明，这种方法可以更好地预测 directive temporal networks 中节点之间的互动和互动结果，在许多情况下比现有的方法表现更好。<details>
<summary>Abstract</summary>
We present a physics-inspired method for inferring dynamic rankings in directed temporal networks - networks in which each directed and timestamped edge reflects the outcome and timing of a pairwise interaction. The inferred ranking of each node is real-valued and varies in time as each new edge, encoding an outcome like a win or loss, raises or lowers the node's estimated strength or prestige, as is often observed in real scenarios including sequences of games, tournaments, or interactions in animal hierarchies. Our method works by solving a linear system of equations and requires only one parameter to be tuned. As a result, the corresponding algorithm is scalable and efficient. We test our method by evaluating its ability to predict interactions (edges' existence) and their outcomes (edges' directions) in a variety of applications, including both synthetic and real data. Our analysis shows that in many cases our method's performance is better than existing methods for predicting dynamic rankings and interaction outcomes.
</details>
<details>
<summary>摘要</summary>
我们提出了一种物理学把握的方法，用于推断导向时间网络中的动态排名。这些网络包含导向时间排序的边，每个边都反映了双方交互的结果和时间。推断每个节点的排名是实数，随着新的边加入或取消，每个节点的估计强度或威望会上升或下降，这是实际情况中常见的。我们的方法是解决一个线性系统方程，只需要一个参数调整，因此算法可扩展和高效。我们对各种应用，包括 sintetic 数据和实际数据进行测试，分析结果表明，在许多情况下，我们的方法的表现更好于现有的动态排名预测方法。
</details></li>
</ul>
<hr>
<h2 id="Model-Calibration-in-Dense-Classification-with-Adaptive-Label-Perturbation"><a href="#Model-Calibration-in-Dense-Classification-with-Adaptive-Label-Perturbation" class="headerlink" title="Model Calibration in Dense Classification with Adaptive Label Perturbation"></a>Model Calibration in Dense Classification with Adaptive Label Perturbation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13539">http://arxiv.org/abs/2307.13539</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/carlisle-liu/aslp">https://github.com/carlisle-liu/aslp</a></li>
<li>paper_authors: Jiawei Liu, Changkun Ye, Shan Wang, Ruikai Cui, Jing Zhang, Kaihao Zhang, Nick Barnes</li>
<li>for: 本研究旨在提高深度神经网络的可靠性和可信度，以便在安全应用中提供可靠的预测结果。</li>
<li>methods: 该研究提出了自适应杂化标签噪声（ASLP）方法，该方法learns一个专门的标签噪声水平 для每个训练图像。ASLP使用了我们提出的自适应二元抽象（SC-BCE）损失函数，该函数整合了标签噪声过程，包括杂化标签（DisturbLabel）和标签平滑，以 corrected calibration while maintaining classification rates。</li>
<li>results: 对于 dense binary classification 模型，ASLP可以显著提高模型的可靠性和可信度度。extensive results demonstrate that ASLP can significantly improve calibration degrees of dense binary classification models on both in-distribution and out-of-distribution data.<details>
<summary>Abstract</summary>
For safety-related applications, it is crucial to produce trustworthy deep neural networks whose prediction is associated with confidence that can represent the likelihood of correctness for subsequent decision-making. Existing dense binary classification models are prone to being over-confident. To improve model calibration, we propose Adaptive Stochastic Label Perturbation (ASLP) which learns a unique label perturbation level for each training image. ASLP employs our proposed Self-Calibrating Binary Cross Entropy (SC-BCE) loss, which unifies label perturbation processes including stochastic approaches (like DisturbLabel), and label smoothing, to correct calibration while maintaining classification rates. ASLP follows Maximum Entropy Inference of classic statistical mechanics to maximise prediction entropy with respect to missing information. It performs this while: (1) preserving classification accuracy on known data as a conservative solution, or (2) specifically improves model calibration degree by minimising the gap between the prediction accuracy and expected confidence of the target training label. Extensive results demonstrate that ASLP can significantly improve calibration degrees of dense binary classification models on both in-distribution and out-of-distribution data. The code is available on https://github.com/Carlisle-Liu/ASLP.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="INFINITY-Neural-Field-Modeling-for-Reynolds-Averaged-Navier-Stokes-Equations"><a href="#INFINITY-Neural-Field-Modeling-for-Reynolds-Averaged-Navier-Stokes-Equations" class="headerlink" title="INFINITY: Neural Field Modeling for Reynolds-Averaged Navier-Stokes Equations"></a>INFINITY: Neural Field Modeling for Reynolds-Averaged Navier-Stokes Equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13538">http://arxiv.org/abs/2307.13538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Louis Serrano, Leon Migus, Yuan Yin, Jocelyn Ahmed Mazari, Patrick Gallinari</li>
<li>for: 用于数值设计中实现高效和准确的代理模型，以便简化直接数值计算的计算负担。</li>
<li>methods: 我们提出了一种基于深度学习的含义 Neil 表示法（INFINITY），该方法可以将几何信息和物理场景编码成紧凑的表示中，并学习一个映射来从几何信息中推断物理场景。</li>
<li>results: 我们在使用 AirfRANS 数据集进行了实验，并证明了我们的方法可以在实际工业应用中达到状态体arc Performance，并且可以准确预测气动和摩擦势量。<details>
<summary>Abstract</summary>
For numerical design, the development of efficient and accurate surrogate models is paramount. They allow us to approximate complex physical phenomena, thereby reducing the computational burden of direct numerical simulations. We propose INFINITY, a deep learning model that utilizes implicit neural representations (INRs) to address this challenge. Our framework encodes geometric information and physical fields into compact representations and learns a mapping between them to infer the physical fields. We use an airfoil design optimization problem as an example task and we evaluate our approach on the challenging AirfRANS dataset, which closely resembles real-world industrial use-cases. The experimental results demonstrate that our framework achieves state-of-the-art performance by accurately inferring physical fields throughout the volume and surface. Additionally we demonstrate its applicability in contexts such as design exploration and shape optimization: our model can correctly predict drag and lift coefficients while adhering to the equations.
</details>
<details>
<summary>摘要</summary>
为numerical设计，效率和准确的代表模型的开发是关键。它们允许我们近似复杂的物理现象，从而降低直接数值计算的计算卷积。我们提出了INFINITY，一种基于深度学习的模型，利用隐藏 нейрон表示（INRs）解决这个挑战。我们的框架将几何信息和物理场所编码为紧凑的表示，并学习它们之间的映射，以推断物理场。我们使用了一个风洞设计优化问题作为示例任务，并在实际工业用例的AirfRANS数据集上评估我们的方法。实验结果表明，我们的框架可以达到当前最佳性能，准确地推断物理场所在体积和表面上。此外，我们还证明了其在设计探索和形状优化等上的可应用性：我们的模型可以正确地预测拖拽和扩散减弱系数，并遵循方程。
</details></li>
</ul>
<hr>
<h2 id="Do-algorithms-and-barriers-for-sparse-principal-component-analysis-extend-to-other-structured-settings"><a href="#Do-algorithms-and-barriers-for-sparse-principal-component-analysis-extend-to-other-structured-settings" class="headerlink" title="Do algorithms and barriers for sparse principal component analysis extend to other structured settings?"></a>Do algorithms and barriers for sparse principal component analysis extend to other structured settings?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13535">http://arxiv.org/abs/2307.13535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guanyi Wang, Mengqi Lou, Ashwin Pananjady</li>
<li>for: 这个论文研究了一个主成分分析问题，该问题采用杂散矩阵模型， captured by a class of union-of-subspace models.</li>
<li>methods: 这篇论文使用了一种自然的投影力方法，并证明了这种方法在统计上几乎最佳的邻居解的局部归一化。</li>
<li>results: 研究结果显示，许多vanilla sparse PCA的现象也出现在其结构化版本中，并且提供了特定的初始化方法和计算困难性证明。<details>
<summary>Abstract</summary>
We study a principal component analysis problem under the spiked Wishart model in which the structure in the signal is captured by a class of union-of-subspace models. This general class includes vanilla sparse PCA as well as its variants with graph sparsity. With the goal of studying these problems under a unified statistical and computational lens, we establish fundamental limits that depend on the geometry of the problem instance, and show that a natural projected power method exhibits local convergence to the statistically near-optimal neighborhood of the solution. We complement these results with end-to-end analyses of two important special cases given by path and tree sparsity in a general basis, showing initialization methods and matching evidence of computational hardness. Overall, our results indicate that several of the phenomena observed for vanilla sparse PCA extend in a natural fashion to its structured counterparts.
</details>
<details>
<summary>摘要</summary>
我们研究了一个主成分分析问题，该问题采用折衣Wishart模型， captured by a class of union-of-subspace models。这个总类包括折衣稀畴PCA和其变体具有图稀畴。为了在统一的统计和计算镜头下研究这些问题，我们确定了基本的限制，这些限制取决于问题实例的几何结构，并证明了一种自然的投影力方法在本地具有Statistically near-optimal neighborhood的 converges。我们还补充了一些特殊情况的末端分析，包括路径和树稀畴在一般基础上，并提供了初始化方法和匹配证明的计算困难。总之，我们的结果表明，许多vanilla sparse PCA中观察到的现象都可以自然地推广到其结构化对应物。
</details></li>
</ul>
<hr>
<h2 id="Differentiable-Turbulence-II"><a href="#Differentiable-Turbulence-II" class="headerlink" title="Differentiable Turbulence II"></a>Differentiable Turbulence II</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13533">http://arxiv.org/abs/2307.13533</a></li>
<li>repo_url: None</li>
<li>paper_authors: Varun Shankar, Romit Maulik, Venkatasubramanian Viswanathan</li>
<li>for: 这篇论文是为了开发数据驱动模型而写的，它利用了可微分流体模拟器来提高计算流体动力学（CFD）的效率。</li>
<li>methods: 这篇论文使用了可微分流体模拟器，并将机器学习（ML）模型 embed 到 CFD 解决方案中，以捕捉流体动力学的普遍性和先前投入成本的优势，同时具有深度学习方法的灵活性和自动训练能力。</li>
<li>results: 该方法可以在多种倾斜 backwards-facing step 中学习 sub-grid 规模 closure，并在不同的 Reynolds 数和新的几何中进行测试。结果显示，学习 closure 可以达到相当于大流体 simulate 的精度，并且在更粗细的网格上进行测试，相当于提高 10 倍的速度。<details>
<summary>Abstract</summary>
Differentiable fluid simulators are increasingly demonstrating value as useful tools for developing data-driven models in computational fluid dynamics (CFD). Differentiable turbulence, or the end-to-end training of machine learning (ML) models embedded in CFD solution algorithms, captures both the generalization power and limited upfront cost of physics-based simulations, and the flexibility and automated training of deep learning methods. We develop a framework for integrating deep learning models into a generic finite element numerical scheme for solving the Navier-Stokes equations, applying the technique to learn a sub-grid scale closure using a multi-scale graph neural network. We demonstrate the method on several realizations of flow over a backwards-facing step, testing on both unseen Reynolds numbers and new geometry. We show that the learned closure can achieve accuracy comparable to traditional large eddy simulation on a finer grid that amounts to an equivalent speedup of 10x. As the desire and need for cheaper CFD simulations grows, we see hybrid physics-ML methods as a path forward to be exploited in the near future.
</details>
<details>
<summary>摘要</summary>
“可微分流体模拟器在计算流体动力学（CFD）中越来越显示其用于开发数据驱动模型的价值。可微分湍流，或者将机器学习（ML）模型embedded在CFD解决方案算法中的末端训练，捕捉了物理基础的泛化力和入口成本的限制，以及深度学习方法的自动化训练和灵活性。我们开发了一个整合深度学习模型的普通finite element数学模型，用于解决奈尔-斯托克方程，并应用该技术来学习一个子网格规模 closure。我们在多个流过 backwards-facing step的实现中测试了该方法，并在未看过Reynolds数和新的几何结构上进行测试。我们显示了学习 closure可以达到与传统大涨流动 simulations相同的准确性，并且在较细的网格上运行，相当于提高10倍的速度。随着计算CFD simulations的成本下降的需求和需求增长，我们看到 hybrid physics-ML 方法作为未来的发展道路。”
</details></li>
</ul>
<hr>
<h2 id="Towards-Long-Term-predictions-of-Turbulence-using-Neural-Operators"><a href="#Towards-Long-Term-predictions-of-Turbulence-using-Neural-Operators" class="headerlink" title="Towards Long-Term predictions of Turbulence using Neural Operators"></a>Towards Long-Term predictions of Turbulence using Neural Operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13517">http://arxiv.org/abs/2307.13517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fernando Gonzalez, François-Xavier Demoulin, Simon Bernard</li>
<li>for: 这个论文探讨了使用神经操作来预测湍流，特点是使用傅里尔神经操作（FNO）模型。它的目标是通过机器学习开发减少的&#x2F;代理模型来进行湍流计算模拟。</li>
<li>methods: 这篇论文使用了不同的模型配置，包括UNO和U-FNET结构。与标准FNO相比，这些结构在准确性和稳定性方面表现更好。U-FNET在更高的 Reynolds 数下预测湍流表现出色。</li>
<li>results: 研究发现，迁移损失和稳定损失等正则项是神经网络模型稳定和准确预测的关键。此外，深度学习模型在液体流体预测中需要更多的测试和评估。<details>
<summary>Abstract</summary>
This paper explores Neural Operators to predict turbulent flows, focusing on the Fourier Neural Operator (FNO) model. It aims to develop reduced-order/surrogate models for turbulent flow simulations using Machine Learning. Different model configurations are analyzed, with U-NET structures (UNO and U-FNET) performing better than the standard FNO in accuracy and stability. U-FNET excels in predicting turbulence at higher Reynolds numbers. Regularization terms, like gradient and stability losses, are essential for stable and accurate predictions. The study emphasizes the need for improved metrics for deep learning models in fluid flow prediction. Further research should focus on models handling complex flows and practical benchmarking metrics.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="An-Empirical-Study-on-Fairness-Improvement-with-Multiple-Protected-Attributes"><a href="#An-Empirical-Study-on-Fairness-Improvement-with-Multiple-Protected-Attributes" class="headerlink" title="An Empirical Study on Fairness Improvement with Multiple Protected Attributes"></a>An Empirical Study on Fairness Improvement with Multiple Protected Attributes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01923">http://arxiv.org/abs/2308.01923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenpeng Chen, Jie M. Zhang, Federica Sarro, Mark Harman</li>
<li>For: The paper aims to improve the fairness of Machine Learning (ML) software regarding multiple protected attributes, as existing research has primarily focused on a single protected attribute.* Methods: The paper conducts an extensive study of fairness improvement methods for multiple protected attributes, covering 11 state-of-the-art methods and analyzing their effectiveness with different datasets, metrics, and ML models.* Results: The results show that improving fairness for a single protected attribute can lead to a decrease in fairness regarding unconsidered protected attributes, with a decrease observed in up to 88.3% of scenarios (57.5% on average). Additionally, the paper finds that accuracy can be maintained in the multiple-attribute paradigm, but precision and recall are affected, with a 5-8 times increase in impact compared to a single attribute.Here are the three points in Simplified Chinese text:* For: 本研究旨在提高机器学习软件中保护属性的多个保护属性的公平性，因为现有研究主要关注单个保护属性。* Methods: 本研究对多保护属性公平性提升方法进行了广泛的研究和分析，覆盖了11种现状最佳实践方法，并对不同的数据集、指标和机器学习模型进行了分析。* Results: 结果显示，为单个保护属性提升公平性可能会导致其他保护属性的不公平性下降，下降的情况在88.3%的场景中出现（平均为57.5%）。此外，研究发现，在多属性情况下，维持精度的可能性很高，但是精度和准确率在考虑多个保护属性时受到影响，影响的程度约为5-8倍。<details>
<summary>Abstract</summary>
Existing research mostly improves the fairness of Machine Learning (ML) software regarding a single protected attribute at a time, but this is unrealistic given that many users have multiple protected attributes. This paper conducts an extensive study of fairness improvement regarding multiple protected attributes, covering 11 state-of-the-art fairness improvement methods. We analyze the effectiveness of these methods with different datasets, metrics, and ML models when considering multiple protected attributes. The results reveal that improving fairness for a single protected attribute can largely decrease fairness regarding unconsidered protected attributes. This decrease is observed in up to 88.3% of scenarios (57.5% on average). More surprisingly, we find little difference in accuracy loss when considering single and multiple protected attributes, indicating that accuracy can be maintained in the multiple-attribute paradigm. However, the effect on precision and recall when handling multiple protected attributes is about 5 times and 8 times that of a single attribute. This has important implications for future fairness research: reporting only accuracy as the ML performance metric, which is currently common in the literature, is inadequate.
</details>
<details>
<summary>摘要</summary>
现有研究主要是在一个保护属性上提高机器学习软件的公平性，但这是不现实的，因为多个用户有多个保护属性。这篇论文进行了详细的多个保护属性公平性改进研究，涵盖11种现状最佳实践的公平性改进方法。我们分析了不同的数据集、指标和机器学习模型下这些方法的效果，并发现在考虑多个保护属性时，改进公平性可能会导致未考虑的保护属性上的公平性下降。这种下降的比例在88.3%的场景中（57.5%的平均值）。更意外的是，考虑单个和多个保护属性时的准确性损失几乎没有差异，这表示在多属性 paradigm 中，准确性可以被维持。然而，处理多个保护属性时对精度和回归的影响是单个属性的5倍和8倍。这有重要的意义 для未来的公平研究：现在在 литераature 中通常只是Reporting 准确性作为机器学习性能指标，这是不充分的。
</details></li>
</ul>
<hr>
<h2 id="Continuous-Time-Evidential-Distributions-for-Irregular-Time-Series"><a href="#Continuous-Time-Evidential-Distributions-for-Irregular-Time-Series" class="headerlink" title="Continuous Time Evidential Distributions for Irregular Time Series"></a>Continuous Time Evidential Distributions for Irregular Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13503">http://arxiv.org/abs/2307.13503</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/twkillian/edict">https://github.com/twkillian/edict</a></li>
<li>paper_authors: Taylor W. Killian, Haoran Zhang, Thomas Hartvigsen, Ava P. Amini</li>
<li>for: 这篇论文是为了探讨如何在具有不规则性的时间序列中进行预测。</li>
<li>methods: 本论文提出了一个名为EDICT的策略，可以在缺失观测的情况下，学习时间序列上的证据分布。这个分布可以在任何时间点进行准确的预测，并且可以在缺失观测的情况下扩展时间上的不确定性。</li>
<li>results: 本论文的实验结果显示，EDICT可以在具有困难的时间序列分类任务中具有竞争性的表现，并且可以在遇到噪音数据时进行不确定性指标预测。<details>
<summary>Abstract</summary>
Prevalent in many real-world settings such as healthcare, irregular time series are challenging to formulate predictions from. It is difficult to infer the value of a feature at any given time when observations are sporadic, as it could take on a range of values depending on when it was last observed. To characterize this uncertainty we present EDICT, a strategy that learns an evidential distribution over irregular time series in continuous time. This distribution enables well-calibrated and flexible inference of partially observed features at any time of interest, while expanding uncertainty temporally for sparse, irregular observations. We demonstrate that EDICT attains competitive performance on challenging time series classification tasks and enabling uncertainty-guided inference when encountering noisy data.
</details>
<details>
<summary>摘要</summary>
广泛存在在现实世界中的多种场景中，如医疗保健，不规则时间序列是难以预测的。因为观测到的时间点不固定，这些时间序列中的特征值可以在不同的时间点 prendre on a range of values。为了捕捉这种uncertainty，我们提出了EDICT策略，它在连续时间中学习不规则时间序列的证据分布。这种分布允许在任何时间点进行Well-calibrated和灵活的特征值推断，并在稀缺观测时扩展时间上的uncertainty。我们示出EDICT在具有困难时间序列分类任务的实验中达到了竞争性的性能，并在遇到噪音数据时提供了uncertainty-guided推断。
</details></li>
</ul>
<hr>
<h2 id="Deep-Reinforcement-Learning-for-Robust-Goal-Based-Wealth-Management"><a href="#Deep-Reinforcement-Learning-for-Robust-Goal-Based-Wealth-Management" class="headerlink" title="Deep Reinforcement Learning for Robust Goal-Based Wealth Management"></a>Deep Reinforcement Learning for Robust Goal-Based Wealth Management</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13501">http://arxiv.org/abs/2307.13501</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tessa Bauman, Bruno Gašperov, Stjepan Begušić, Zvonko Kostanjčar</li>
<li>for: 这个研究旨在提出一种基于深度强化学习的robust目标基础资产管理方法，以便优化财务目标管理策略。</li>
<li>methods: 这个方法使用了深度强化学习来解决sequential decision-making问题，并且将投资选择看作一个Markov decision process。</li>
<li>results: 实验结果显示，这个方法比较多的目标基础资产管理参考值得更高，both simulated和历史股票市场数据上。<details>
<summary>Abstract</summary>
Goal-based investing is an approach to wealth management that prioritizes achieving specific financial goals. It is naturally formulated as a sequential decision-making problem as it requires choosing the appropriate investment until a goal is achieved. Consequently, reinforcement learning, a machine learning technique appropriate for sequential decision-making, offers a promising path for optimizing these investment strategies. In this paper, a novel approach for robust goal-based wealth management based on deep reinforcement learning is proposed. The experimental results indicate its superiority over several goal-based wealth management benchmarks on both simulated and historical market data.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。<</SYS>>目标基于投资是一种财务管理方法，强调达成特定的金融目标。因此它自然形成为一个顺序决策问题，需要选择适当的投资直到目标达成。因此，人工智能技术适合顺序决策的强化学习可以优化这些投资策略。在这篇论文中，一种基于深度强化学习的新方法 для稳定目标基于投资管理被提出。实验结果表明其在虚拟和历史市场数据上的表现优于许多目标基于投资管理标准做法。
</details></li>
</ul>
<hr>
<h2 id="Finding-Money-Launderers-Using-Heterogeneous-Graph-Neural-Networks"><a href="#Finding-Money-Launderers-Using-Heterogeneous-Graph-Neural-Networks" class="headerlink" title="Finding Money Launderers Using Heterogeneous Graph Neural Networks"></a>Finding Money Launderers Using Heterogeneous Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13499">http://arxiv.org/abs/2307.13499</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fredjo89/heterogeneous-mpnn">https://github.com/fredjo89/heterogeneous-mpnn</a></li>
<li>paper_authors: Fredrik Johannessen, Martin Jullum</li>
<li>for: 这个论文的目的是提出一种基于图神经网络（GNN）的方法，用于检测银行账户上的货币洗钱活动。</li>
<li>methods: 该方法extend了homogeneous GNN方法，即Message Passing Neural Network（MPNN），以适应非Homogeneous图。在这个过程中，提出了一种新的简单的消息汇聚方法来汇集不同边的消息。</li>
<li>results: 研究发现，使用合适的GNN建筑可以大幅提高银行电子监测系统的检测精度。这是首先在大规模实际非Homogeneous图上应用GNN的财务洗钱检测研究。<details>
<summary>Abstract</summary>
Current anti-money laundering (AML) systems, predominantly rule-based, exhibit notable shortcomings in efficiently and precisely detecting instances of money laundering. As a result, there has been a recent surge toward exploring alternative approaches, particularly those utilizing machine learning. Since criminals often collaborate in their money laundering endeavors, accounting for diverse types of customer relations and links becomes crucial. In line with this, the present paper introduces a graph neural network (GNN) approach to identify money laundering activities within a large heterogeneous network constructed from real-world bank transactions and business role data belonging to DNB, Norway's largest bank. Specifically, we extend the homogeneous GNN method known as the Message Passing Neural Network (MPNN) to operate effectively on a heterogeneous graph. As part of this procedure, we propose a novel method for aggregating messages across different edges of the graph. Our findings highlight the importance of using an appropriate GNN architecture when combining information in heterogeneous graphs. The performance results of our model demonstrate great potential in enhancing the quality of electronic surveillance systems employed by banks to detect instances of money laundering. To the best of our knowledge, this is the first published work applying GNN on a large real-world heterogeneous network for anti-money laundering purposes.
</details>
<details>
<summary>摘要</summary>
当今反贪恶（AML）系统，主要基于规则，表现出明显的缺陷，不能够高效精准地检测贪恶活动。因此，有一些最近的研究倾向于探索新的方法，特别是使用机器学习。由于犯罪分子frequently合作其贪恶活动，考虑到多种客户关系和链接变得非常重要。在这种情况下，本文介绍了一种图 neural network（GNN）方法来在实际世界银行交易和企业角色数据上检测贪恶活动。具体来说，我们扩展了同义GNN方法known as Message Passing Neural Network（MPNN），使其在不同类型的图上运行有效。为了实现这一目的，我们提出了一种新的消息汇聚方法。我们的发现表明，在组合不同类型的图时，使用合适的GNN建筑是非常重要。我们的模型性能结果表明，我们的模型在银行电子监测系统中检测贪恶活动的可能性非常高。到目前为止，这是首次在实际世界的大规模不同类型图上应用GNN进行反贪恶用途的发表文献。
</details></li>
</ul>
<hr>
<h2 id="Zshot-An-Open-source-Framework-for-Zero-Shot-Named-Entity-Recognition-and-Relation-Extraction"><a href="#Zshot-An-Open-source-Framework-for-Zero-Shot-Named-Entity-Recognition-and-Relation-Extraction" class="headerlink" title="Zshot: An Open-source Framework for Zero-Shot Named Entity Recognition and Relation Extraction"></a>Zshot: An Open-source Framework for Zero-Shot Named Entity Recognition and Relation Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13497">http://arxiv.org/abs/2307.13497</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriele Picco, Marcos Martínez Galindo, Alberto Purpura, Leopold Fuchs, Vanessa López, Hoang Thanh Lam</li>
<li>for: 这个研究的目的是提供一个可比较多种当前最佳ZSL方法的框架，以及为industry提供可用的API来进行生产环境中的应用。</li>
<li>methods: 这个框架使用了多种现有的ZSL方法，以及一些新的技术，如pipeline ensemble和可视化工具，以提高ZSL性能。</li>
<li>results: 这个框架可以提供标准的Benchmark datasets，并且可以用于生产环境中的应用，以提高ZSL性能。<details>
<summary>Abstract</summary>
The Zero-Shot Learning (ZSL) task pertains to the identification of entities or relations in texts that were not seen during training. ZSL has emerged as a critical research area due to the scarcity of labeled data in specific domains, and its applications have grown significantly in recent years. With the advent of large pretrained language models, several novel methods have been proposed, resulting in substantial improvements in ZSL performance. There is a growing demand, both in the research community and industry, for a comprehensive ZSL framework that facilitates the development and accessibility of the latest methods and pretrained models.In this study, we propose a novel ZSL framework called Zshot that aims to address the aforementioned challenges. Our primary objective is to provide a platform that allows researchers to compare different state-of-the-art ZSL methods with standard benchmark datasets. Additionally, we have designed our framework to support the industry with readily available APIs for production under the standard SpaCy NLP pipeline. Our API is extendible and evaluable, moreover, we include numerous enhancements such as boosting the accuracy with pipeline ensembling and visualization utilities available as a SpaCy extension.
</details>
<details>
<summary>摘要</summary>
Zero-Shot Learning (ZSL) 任务是指在训练时未看到的文本中 Identify 实体或关系。 ZSL 已成为一个关键的研究领域，因为特定领域的标注数据罕见，其应用也在过去几年得到了广泛的发展。随着大型预训语言模型的出现，许多新的方法被提出，从而导致了 ZSL 性能的明显改善。现在，研究社区和产业都有很大的需求，一个全面的 ZSL 框架，以便发展和访问最新的方法和预训模型。在这种研究中，我们提出了一个新的 ZSL 框架，称为 Zshot，以解决以下问题。我们的主要目标是提供一个平台， allowing researchers 可以比较不同的当前最佳 ZSL 方法，并使用标准的 Benchmark 数据集。此外，我们的框架还支持产业，通过可用的 API，为生产中使用 SpaCy NLP 管道。我们的 API 可扩展和评估，并且包括了多种改进，如管道ensemble 和可视化工具，作为 SpaCy 扩展。
</details></li>
</ul>
<hr>
<h2 id="Duet-efficient-and-scalable-hybriD-neUral-rElation-undersTanding"><a href="#Duet-efficient-and-scalable-hybriD-neUral-rElation-undersTanding" class="headerlink" title="Duet: efficient and scalable hybriD neUral rElation undersTanding"></a>Duet: efficient and scalable hybriD neUral rElation undersTanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13494">http://arxiv.org/abs/2307.13494</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/GIS-PuppetMaster/Duet">https://github.com/GIS-PuppetMaster/Duet</a></li>
<li>paper_authors: Kaixin Zhang, Hongzhi Wang, Yabin Lu, Ziqi Li, Chang Shu, Yu Yan, Donghua Yang</li>
<li>for: 优化Cardinality Estimation的高精度方法，解决数据和工作荟载漂移问题，提高实际应用性。</li>
<li>methods: 引入 predicate 信息，基于自适应模型进行直接Cardinality estimation，不需要采样或非准 differential 过程，可以大幅降低推断复杂度。</li>
<li>results: 实验结果表明，Duet 可以实现所有设计目标，并在高Cardinality 和高维度表上达到更高的准确率，同时也在 CPU 上实现更低的推断成本。<details>
<summary>Abstract</summary>
Learned cardinality estimation methods have achieved high precision compared to traditional methods. Among learned methods, query-driven approaches face the data and workload drift problem for a long time. Although both query-driven and hybrid methods are proposed to avoid this problem, even the state-of-the-art of them suffer from high training and estimation costs, limited scalability, instability, and long-tailed distribution problem on high cardinality and high-dimensional tables, which seriously affects the practical application of learned cardinality estimators. In this paper, we prove that most of these problems are directly caused by the widely used progressive sampling. We solve this problem by introducing predicates information into the autoregressive model and propose Duet, a stable, efficient, and scalable hybrid method to estimate cardinality directly without sampling or any non-differentiable process, which can not only reduces the inference complexity from O(n) to O(1) compared to Naru and UAE but also achieve higher accuracy on high cardinality and high-dimensional tables. Experimental results show that Duet can achieve all the design goals above and be much more practical and even has a lower inference cost on CPU than that of most learned methods on GPU.
</details>
<details>
<summary>摘要</summary>
学习 cardinality 估计方法已经实现了高精度，相比传统方法。然而， query-driven 方法面临着数据和工作负荷漂移问题，持续时间很长。尽管 query-driven 和混合方法都被提出来避免这个问题，但它们都受到高训练和估计成本、有限扩展、不稳定和高维度表中长尾分布问题的影响，这些问题很大程度上限制了学习 cardinality 估计器的实际应用。在这篇论文中，我们证明了大多数这些问题都是由广泛使用进度 sampling 引起的。我们解决这个问题，通过将 predicate 信息添加到权重模型中，并提出了 Duet，一种稳定、高效、可扩展的混合方法，可以直接估计 cardinality 而不需要抽样或任何不可导的过程，这使得 Duet 可以将推理复杂度从 O(n) 降低至 O(1)，比 Naru 和 UAE 更高效。实验结果表明，Duet 可以实现所有的设计目标，并在 CPU 上比大多数学习方法在 GPU 上更实用，甚至具有更低的推理成本。
</details></li>
</ul>
<hr>
<h2 id="ECG-classification-using-Deep-CNN-and-Gramian-Angular-Field"><a href="#ECG-classification-using-Deep-CNN-and-Gramian-Angular-Field" class="headerlink" title="ECG classification using Deep CNN and Gramian Angular Field"></a>ECG classification using Deep CNN and Gramian Angular Field</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02395">http://arxiv.org/abs/2308.02395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youssef Elmir, Yassine Himeur, Abbes Amira</li>
<li>for: 这篇论文为了心电信号分析提供了一个新的特征表示方法。</li>
<li>methods: 该方法使用格里恩angular field transform将时间频率1D вектор转换为2D图像，然后使用卷积神经网络进行分类。</li>
<li>results: 实验结果显示，该方法可以提高分类性能，并且可以识别和可视化心电信号中的时间特征，如心率、心脉和心电信号的形态变化，这些特征可能不会在原始信号中显示出来。<details>
<summary>Abstract</summary>
This paper study provides a novel contribution to the field of signal processing and DL for ECG signal analysis by introducing a new feature representation method for ECG signals. The proposed method is based on transforming time frequency 1D vectors into 2D images using Gramian Angular Field transform. Moving on, the classification of the transformed ECG signals is performed using Convolutional Neural Networks (CNN). The obtained results show a classification accuracy of 97.47% and 98.65% for anomaly detection. Accordingly, in addition to improving the classification performance compared to the state-of-the-art, the feature representation helps identify and visualize temporal patterns in the ECG signal, such as changes in heart rate, rhythm, and morphology, which may not be apparent in the original signal. This has significant implications in the diagnosis and treatment of cardiovascular diseases and detection of anomalies.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Rational-kernel-based-interpolation-for-complex-valued-frequency-response-functions"><a href="#Rational-kernel-based-interpolation-for-complex-valued-frequency-response-functions" class="headerlink" title="Rational kernel-based interpolation for complex-valued frequency response functions"></a>Rational kernel-based interpolation for complex-valued frequency response functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13484">http://arxiv.org/abs/2307.13484</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stk-kriging/complex-rational-interpolation">https://github.com/stk-kriging/complex-rational-interpolation</a></li>
<li>paper_authors: Julien Bect, Niklas Georg, Ulrich Römer, Sebastian Schöps</li>
<li>for: 该论文关注用数据来近似一个复数值函数的问题，具体来说是在频域中解析方程的响应函数。</li>
<li>methods: 论文使用了kernel方法，但标准kernels并不适用。 authors还没有解释了下来kernel对应的基础函数的作用和数学意义。</li>
<li>results: 作者引入了新的复数值函数 reproduce kernel空间，并将复数值 interpolate问题转化为 minimum norm interpolate问题。 authors还 combinined interpolant with a low-order rational function，并采用了一个新的模型选择 criterion。 numerical results on examples from different fields show that the method performs well, and outperforms existing rational approximation methods.<details>
<summary>Abstract</summary>
This work is concerned with the kernel-based approximation of a complex-valued function from data, where the frequency response function of a partial differential equation in the frequency domain is of particular interest. In this setting, kernel methods are employed more and more frequently, however, standard kernels do not perform well. Moreover, the role and mathematical implications of the underlying pair of kernels, which arises naturally in the complex-valued case, remain to be addressed. We introduce new reproducing kernel Hilbert spaces of complex-valued functions, and formulate the problem of complex-valued interpolation with a kernel pair as minimum norm interpolation in these spaces. Moreover, we combine the interpolant with a low-order rational function, where the order is adaptively selected based on a new model selection criterion. Numerical results on examples from different fields, including electromagnetics and acoustic examples, illustrate the performance of the method, also in comparison to available rational approximation methods.
</details>
<details>
<summary>摘要</summary>
这项工作关注于基于数据的复数值函数近似，特别是在频域中的响应函数的情况。在这种设定下，核方法在使用中变得越来越普遍，但标准核不能达到好的性能。此外，下面的对应核对答案和其数学意义在复数值情况下还未得到了解。我们引入了新的复数值函数重复核空间，并将复数值 interpol 问题转化为最小二乘 interpol 问题在这些空间中。此外，我们将 interpolant 与低阶 rational function 结合，其阶数由一个新的模型选择 критериion 进行自适应选择。实际例如电磁学和音频例子等，数值结果表明该方法在比较可靠性和可读性方面具有优势，并与现有的 rational approximation 方法进行比较。
</details></li>
</ul>
<hr>
<h2 id="Combinatorial-Auctions-and-Graph-Neural-Networks-for-Local-Energy-Flexibility-Markets"><a href="#Combinatorial-Auctions-and-Graph-Neural-Networks-for-Local-Energy-Flexibility-Markets" class="headerlink" title="Combinatorial Auctions and Graph Neural Networks for Local Energy Flexibility Markets"></a>Combinatorial Auctions and Graph Neural Networks for Local Energy Flexibility Markets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13470">http://arxiv.org/abs/2307.13470</a></li>
<li>repo_url: None</li>
<li>paper_authors: Awadelrahman M. A. Ahmed, Frank Eliassen, Yan Zhang</li>
<li>for: 本研究提出了一个新的 combinatorial 拍卖框架，用于解决地方能源柔软性市场中的购买者对多个时间间隔的不能够组合化问题。</li>
<li>methods: 本研究使用了一个简单 yet powerful 三元 graph 表现方法，并设计了基于图像神经网络的模型，以解决下游决定问题。</li>
<li>results: 本研究的模型可以实现对于地方市场中的能源柔软性资源的有效分配，并且显示出线性推断时间复杂度相对于商业解决方案的几何级数增长。<details>
<summary>Abstract</summary>
This paper proposes a new combinatorial auction framework for local energy flexibility markets, which addresses the issue of prosumers' inability to bundle multiple flexibility time intervals. To solve the underlying NP-complete winner determination problems, we present a simple yet powerful heterogeneous tri-partite graph representation and design graph neural network-based models. Our models achieve an average optimal value deviation of less than 5\% from an off-the-shelf optimization tool and show linear inference time complexity compared to the exponential complexity of the commercial solver. Contributions and results demonstrate the potential of using machine learning to efficiently allocate energy flexibility resources in local markets and solving optimization problems in general.
</details>
<details>
<summary>摘要</summary>
Here's the translation in Simplified Chinese:这篇论文提出了一种新的 combinatorial 拍卖框架，用于解决本地能源灵活性市场中潜在用户（prosumers）无法捆绑多个灵活时间间隔的问题。为解决这个问题，我们提出了一种简单 yet 强大的多类图表示和图神经网络模型。我们的模型在评估价值上偏差不超过5%，与商业优化工具相比，并且推理时间复杂度为线性，与商业解决器的极限复杂度相比，显著提高了效率。本文的贡献和结果表明，使用机器学习可以高效地分配本地能源灵活性资源，并在总体上解决优化问题。
</details></li>
</ul>
<hr>
<h2 id="Gaussian-Graph-with-Prototypical-Contrastive-Learning-in-E-Commerce-Bundle-Recommendation"><a href="#Gaussian-Graph-with-Prototypical-Contrastive-Learning-in-E-Commerce-Bundle-Recommendation" class="headerlink" title="Gaussian Graph with Prototypical Contrastive Learning in E-Commerce Bundle Recommendation"></a>Gaussian Graph with Prototypical Contrastive Learning in E-Commerce Bundle Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13468">http://arxiv.org/abs/2307.13468</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhao-Yang Liu, Liucheng Sun, Chenwei Weng, Qijin Chen, Chengfu Huo</li>
<li>for: 提供一个精心选择的bundle推荐解决方案，以满足用户的偏好在电商平台上。</li>
<li>methods: 我们提出了一种新的 Gaussian Graph with Prototypical Contrastive Learning (GPCL)框架，其中每个用户&#x2F;bundle&#x2F;item都被表示为一个 Gaussian 分布而不是固定的 вектор。我们还设计了一个 прототип型对比学习模块，以捕捉Contextual information并减少采样偏误问题。</li>
<li>results: 我们在多个公共数据集上进行了广泛的实验，并证明了 GPCL 可以超越先前的方法，达到新的状态机器人性性。此外， GPCL 已经在实际的电商平台上部署，并实现了substantial improvements。<details>
<summary>Abstract</summary>
Bundle recommendation aims to provide a bundle of items to satisfy the user preference on e-commerce platform. Existing successful solutions are based on the contrastive graph learning paradigm where graph neural networks (GNNs) are employed to learn representations from user-level and bundle-level graph views with a contrastive learning module to enhance the cooperative association between different views. Nevertheless, they ignore the uncertainty issue which has a significant impact in real bundle recommendation scenarios due to the lack of discriminative information caused by highly sparsity or diversity. We further suggest that their instancewise contrastive learning fails to distinguish the semantically similar negatives (i.e., sampling bias issue), resulting in performance degradation. In this paper, we propose a novel Gaussian Graph with Prototypical Contrastive Learning (GPCL) framework to overcome these challenges. In particular, GPCL embeds each user/bundle/item as a Gaussian distribution rather than a fixed vector. We further design a prototypical contrastive learning module to capture the contextual information and mitigate the sampling bias issue. Extensive experiments demonstrate that benefiting from the proposed components, we achieve new state-of-the-art performance compared to previous methods on several public datasets. Moreover, GPCL has been deployed on real-world e-commerce platform and achieved substantial improvements.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate " Bundle recommendation aims to provide a bundle of items to satisfy the user preference on e-commerce platform. Existing successful solutions are based on the contrastive graph learning paradigm where graph neural networks (GNNs) are employed to learn representations from user-level and bundle-level graph views with a contrastive learning module to enhance the cooperative association between different views. Nevertheless, they ignore the uncertainty issue which has a significant impact in real bundle recommendation scenarios due to the lack of discriminative information caused by highly sparsity or diversity. We further suggest that their instancewise contrastive learning fails to distinguish the semantically similar negatives (i.e., sampling bias issue), resulting in performance degradation. In this paper, we propose a novel Gaussian Graph with Prototypical Contrastive Learning (GPCL) framework to overcome these challenges. In particular, GPCL embeds each user/bundle/item as a Gaussian distribution rather than a fixed vector. We further design a prototypical contrastive learning module to capture the contextual information and mitigate the sampling bias issue. Extensive experiments demonstrate that benefiting from the proposed components, we achieve new state-of-the-art performance compared to previous methods on several public datasets. Moreover, GPCL has been deployed on real-world e-commerce platform and achieved substantial improvements. "into Simplified Chinese:<<SYS>>bundle 推荐的目标是为在电子商务平台上满足用户的首选，现有的成功解决方案基于对用户和套件视图的对比学习 paradigm，通过图 neural networks (GNNs) 学习用户和套件视图的表示，并通过对比学习模块增强不同视图之间的合作关系。然而，它们忽略了实际推荐场景中的不确定性问题，这是由于高度稀畴或多样性导致的缺乏特征信息。我们还建议它们的实例级对比学习无法分辨Semantic 相似的负例（即采样偏见问题），导致性能下降。在这篇论文中，我们提出了一种新的 Gaussian Graph with Prototypical Contrastive Learning (GPCL) 框架，以解决这些挑战。具体来说，GPCL 将每个用户/套件/项 embed 为 Gaussian 分布而不是固定 вектор。我们还设计了一种 проtotypical 对比学习模块，以捕捉上下文信息并 mitigate 采样偏见问题。广泛的实验表明，通过我们提出的 ком成分，我们在许多公共数据集上 achieve 新的状态艺术性能，而且 GPCL 已经在真实的电子商务平台上部署，并实现了重大改进。Note: "Gaussian Distribution" in Chinese is "Gaussian 分布" (Gaussian fēn bù). "Prototypical Contrastive Learning" in Chinese is "prototypical 对比学习" (prototypical duì bǐ xué xí).
</details></li>
</ul>
<hr>
<h2 id="Integrating-processed-based-models-and-machine-learning-for-crop-yield-prediction"><a href="#Integrating-processed-based-models-and-machine-learning-for-crop-yield-prediction" class="headerlink" title="Integrating processed-based models and machine learning for crop yield prediction"></a>Integrating processed-based models and machine learning for crop yield prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13466">http://arxiv.org/abs/2307.13466</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michiel G. J. Kallenberg, Bernardo Maestrini, Ron van Bree, Paul Ravensbergen, Christos Pylianidis, Frits van Evert, Ioannis N. Athanasiadis</li>
<li>for: 预测哈比粮食产量</li>
<li>methods: 使用混合元模型方法，结合了理论驱动的生长模型和数据驱动的神经网络</li>
<li>results: 在silico中，元模型方法比基线方法（仅使用数据驱动方法）更好地预测哈比粮食产量，并在实际采用场景（303个场地和77个商业场地）中与生长模型几乎相当。然而，两种模型在这些场景中都比一个简单的线性回归模型和专业人员 manually设计的特征集和预处理方法差。<details>
<summary>Abstract</summary>
Crop yield prediction typically involves the utilization of either theory-driven process-based crop growth models, which have proven to be difficult to calibrate for local conditions, or data-driven machine learning methods, which are known to require large datasets. In this work we investigate potato yield prediction using a hybrid meta-modeling approach. A crop growth model is employed to generate synthetic data for (pre)training a convolutional neural net, which is then fine-tuned with observational data. When applied in silico, our meta-modeling approach yields better predictions than a baseline comprising a purely data-driven approach. When tested on real-world data from field trials (n=303) and commercial fields (n=77), the meta-modeling approach yields competitive results with respect to the crop growth model. In the latter set, however, both models perform worse than a simple linear regression with a hand-picked feature set and dedicated preprocessing designed by domain experts. Our findings indicate the potential of meta-modeling for accurate crop yield prediction; however, further advancements and validation using extensive real-world datasets is recommended to solidify its practical effectiveness.
</details>
<details>
<summary>摘要</summary>
Typically, crop yield prediction relies on either theory-driven process-based crop growth models, which are difficult to calibrate for local conditions, or data-driven machine learning methods, which require large datasets. In this study, we investigate potato yield prediction using a hybrid meta-modeling approach. We use a crop growth model to generate synthetic data for training a convolutional neural network, which is then fine-tuned with observational data. When applied in silico, our meta-modeling approach yields better predictions than a baseline consisting of a purely data-driven approach. When tested on real-world data from field trials (n=303) and commercial fields (n=77), the meta-modeling approach yields competitive results with respect to the crop growth model. However, both models perform worse than a simple linear regression with a hand-picked feature set and dedicated preprocessing designed by domain experts. Our findings suggest the potential of meta-modeling for accurate crop yield prediction, but further advancements and validation using extensive real-world datasets are recommended to solidify its practical effectiveness.
</details></li>
</ul>
<hr>
<h2 id="Fundamental-causal-bounds-of-quantum-random-access-memories"><a href="#Fundamental-causal-bounds-of-quantum-random-access-memories" class="headerlink" title="Fundamental causal bounds of quantum random access memories"></a>Fundamental causal bounds of quantum random access memories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13460">http://arxiv.org/abs/2307.13460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunfei Wang, Yuri Alexeev, Liang Jiang, Frederic T. Chong, Junyu Liu</li>
<li>for: This paper explores the fundamental limits of quantum random access memory (QRAM) in quantum computing applications, specifically the impact of causality on QRAM performance.</li>
<li>methods: The authors use relativistic quantum field theory and Lieb-Robinson bounds to derive bounds on the number of logical qubits that can be stored in QRAM, based on the size of the quantum system and the clock cycle time.</li>
<li>results: The paper shows that QRAM can accommodate up to $\mathcal{O}(10^7)$ logical qubits in 1 dimension, $\mathcal{O}(10^{15})$ to $\mathcal{O}(10^{20})$ in various 2D architectures, and $\mathcal{O}(10^{24})$ in 3 dimensions, with clock cycle times of approximately $10^{-3}$ seconds and a lattice spacing of about 1 micrometer. The authors contend that this causality bound broadly applies to other quantum hardware systems.<details>
<summary>Abstract</summary>
Quantum devices should operate in adherence to quantum physics principles. Quantum random access memory (QRAM), a fundamental component of many essential quantum algorithms for tasks such as linear algebra, data search, and machine learning, is often proposed to offer $\mathcal{O}(\log N)$ circuit depth for $\mathcal{O}(N)$ data size, given $N$ qubits. However, this claim appears to breach the principle of relativity when dealing with a large number of qubits in quantum materials interacting locally. In our study we critically explore the intrinsic bounds of rapid quantum memories based on causality, employing the relativistic quantum field theory and Lieb-Robinson bounds in quantum many-body systems. In this paper, we consider a hardware-efficient QRAM design in hybrid quantum acoustic systems. Assuming clock cycle times of approximately $10^{-3}$ seconds and a lattice spacing of about 1 micrometer, we show that QRAM can accommodate up to $\mathcal{O}(10^7)$ logical qubits in 1 dimension, $\mathcal{O}(10^{15})$ to $\mathcal{O}(10^{20})$ in various 2D architectures, and $\mathcal{O}(10^{24})$ in 3 dimensions. We contend that this causality bound broadly applies to other quantum hardware systems. Our findings highlight the impact of fundamental quantum physics constraints on the long-term performance of quantum computing applications in data science and suggest potential quantum memory designs for performance enhancement.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-behavioural-transformer-for-effective-collaboration-between-a-robot-and-a-non-stationary-human"><a href="#A-behavioural-transformer-for-effective-collaboration-between-a-robot-and-a-non-stationary-human" class="headerlink" title="A behavioural transformer for effective collaboration between a robot and a non-stationary human"></a>A behavioural transformer for effective collaboration between a robot and a non-stationary human</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13447">http://arxiv.org/abs/2307.13447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruaridh Mon-Williams, Theodoros Stouraitis, Sethu Vijayakumar</li>
<li>for: The paper is written for exploring how robots could better predict human behavior in non-stationary environments, and developing a principled meta-learning framework to address this challenge.</li>
<li>methods: The paper proposes a conditional transformer called Behaviour-Transform (BeTrans) that can adapt quickly to new human agents with non-stationary behaviors, and trains BeTrans on simulated human agents with different systematic biases in collaborative settings.</li>
<li>results: The paper shows that BeTrans effectively collaborates with simulated human agents and adapts faster to non-stationary simulated human agents than state-of-the-art (SOTA) techniques.Here’s the information in Simplified Chinese text:</li>
<li>for: 本研究旨在探讨机器人如何更好地预测人类行为在非站点环境中，并开发出原则正的元学习框架来解决这一挑战。</li>
<li>methods: 本研究提出了一种名为行为转换（BeTrans）的条件变换器，可以快速适应新的人类代理人非站点行为，并在合作设置下对 simulate human agents with different systematic biases进行训练。</li>
<li>results: 研究显示，BeTrans可以有效地与 simulate human agents collaborate，并在非站点人类代理人行为下更快地适应非站点人类代理人行为。<details>
<summary>Abstract</summary>
A key challenge in human-robot collaboration is the non-stationarity created by humans due to changes in their behaviour. This alters environmental transitions and hinders human-robot collaboration. We propose a principled meta-learning framework to explore how robots could better predict human behaviour, and thereby deal with issues of non-stationarity. On the basis of this framework, we developed Behaviour-Transform (BeTrans). BeTrans is a conditional transformer that enables a robot agent to adapt quickly to new human agents with non-stationary behaviours, due to its notable performance with sequential data. We trained BeTrans on simulated human agents with different systematic biases in collaborative settings. We used an original customisable environment to show that BeTrans effectively collaborates with simulated human agents and adapts faster to non-stationary simulated human agents than SOTA techniques.
</details>
<details>
<summary>摘要</summary>
人机合作中一大挑战是由人类行为引起的非站点性，这会改变环境转移并妨碍人机合作。我们提出了一种原则性的元学习框架，以解决机器人如何更好地预测人类行为，并因此处理非站点性问题。基于这个框架，我们开发了行为变换（BeTrans）。BeTrans 是一种可变条件的变换器，允许机器人代理者快速适应新的人类代理者非站点行为。我们在 simulated 人类代理者之间进行了训练，并在合作场景中表明了 BeTrans 可以快速适应非站点人类代理者，并且比标准技术更快。
</details></li>
</ul>
<hr>
<h2 id="Network-Traffic-Classification-based-on-Single-Flow-Time-Series-Analysis"><a href="#Network-Traffic-Classification-based-on-Single-Flow-Time-Series-Analysis" class="headerlink" title="Network Traffic Classification based on Single Flow Time Series Analysis"></a>Network Traffic Classification based on Single Flow Time Series Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13434">http://arxiv.org/abs/2307.13434</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/koumajos/classificationbasedonsfts">https://github.com/koumajos/classificationbasedonsfts</a></li>
<li>paper_authors: Josef Koumar, Karel Hynek, Tomáš Čejka</li>
<li>for: 本研究旨在解决当前对加密网络通信进行分析时所存在的挑战，通过网络流量监测使用IP流。</li>
<li>methods: 本研究提出了一种基于单流时间序分析的新的流扩展方法，通过统计分析、时域分析、流时间范围内的包分布、时间序列行为和频域分析，提取了69个统计特征。</li>
<li>results: 我们通过使用15种公开的数据集进行评估，并证明了提案的特征向量在网络流量分类任务中的可用性和通用性。在评估中，相比related works，我们的方法在 binary和多类分类任务中具有类似或更好的分类性能，在超过一半的评估任务中，分类性能提高了5%以上。<details>
<summary>Abstract</summary>
Network traffic monitoring using IP flows is used to handle the current challenge of analyzing encrypted network communication. Nevertheless, the packet aggregation into flow records naturally causes information loss; therefore, this paper proposes a novel flow extension for traffic features based on the time series analysis of the Single Flow Time series, i.e., a time series created by the number of bytes in each packet and its timestamp. We propose 69 universal features based on the statistical analysis of data points, time domain analysis, packet distribution within the flow timespan, time series behavior, and frequency domain analysis. We have demonstrated the usability and universality of the proposed feature vector for various network traffic classification tasks using 15 well-known publicly available datasets. Our evaluation shows that the novel feature vector achieves classification performance similar or better than related works on both binary and multiclass classification tasks. In more than half of the evaluated tasks, the classification performance increased by up to 5\%.
</details>
<details>
<summary>摘要</summary>
网络流量监测使用流程来处理当前挑战的加密网络通信分析。然而， packet 聚合到流记录 naturally causes information loss; therefore, this paper proposes a novel flow extension for traffic features based on the time series analysis of the Single Flow Time series, i.e., a time series created by the number of bytes in each packet and its timestamp. We propose 69 universal features based on the statistical analysis of data points, time domain analysis, packet distribution within the flow timespan, time series behavior, and frequency domain analysis. We have demonstrated the usability and universality of the proposed feature vector for various network traffic classification tasks using 15 well-known publicly available datasets. Our evaluation shows that the novel feature vector achieves classification performance similar or better than related works on both binary and multiclass classification tasks. In more than half of the evaluated tasks, the classification performance increased by up to 5%.Here's the translation in Traditional Chinese:网络流量监测使用流程来处理当前挑战的加密网络通信分析。然而， packet 聚合到流记录 naturally causes information loss; therefore, this paper proposes a novel flow extension for traffic features based on the time series analysis of the Single Flow Time series, i.e., a time series created by the number of bytes in each packet and its timestamp. We propose 69 universal features based on the statistical analysis of data points, time domain analysis, packet distribution within the flow timespan, time series behavior, and frequency domain analysis. We have demonstrated the usability and universality of the proposed feature vector for various network traffic classification tasks using 15 well-known publicly available datasets. Our evaluation shows that the novel feature vector achieves classification performance similar or better than related works on both binary and multiclass classification tasks. In more than half of the evaluated tasks, the classification performance increased by up to 5%.
</details></li>
</ul>
<hr>
<h2 id="Achieving-Linear-Speedup-in-Decentralized-Stochastic-Compositional-Minimax-Optimization"><a href="#Achieving-Linear-Speedup-in-Decentralized-Stochastic-Compositional-Minimax-Optimization" class="headerlink" title="Achieving Linear Speedup in Decentralized Stochastic Compositional Minimax Optimization"></a>Achieving Linear Speedup in Decentralized Stochastic Compositional Minimax Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13430">http://arxiv.org/abs/2307.13430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongchang Gao</li>
<li>for:  solve the decentralized stochastic compositional minimax problem with linear speedup</li>
<li>methods:  develop a novel decentralized stochastic compositional gradient descent ascent with momentum algorithm to reduce the consensus error in the inner-level function</li>
<li>results:  achieve linear speedup with respect to the number of workers, and applied to the imbalanced classification problem with effective experimental results.Here’s the summary in English for reference:</li>
<li>for: Solve the decentralized stochastic compositional minimax problem with linear speedup</li>
<li>methods: Develop a novel decentralized stochastic compositional gradient descent ascent with momentum algorithm to reduce the consensus error in the inner-level function</li>
<li>results: Achieve linear speedup with respect to the number of workers, and applied to the imbalanced classification problem with effective experimental results.<details>
<summary>Abstract</summary>
The stochastic compositional minimax problem has attracted a surge of attention in recent years since it covers many emerging machine learning models. Meanwhile, due to the emergence of distributed data, optimizing this kind of problem under the decentralized setting becomes badly needed. However, the compositional structure in the loss function brings unique challenges to designing efficient decentralized optimization algorithms. In particular, our study shows that the standard gossip communication strategy cannot achieve linear speedup for decentralized compositional minimax problems due to the large consensus error about the inner-level function. To address this issue, we developed a novel decentralized stochastic compositional gradient descent ascent with momentum algorithm to reduce the consensus error in the inner-level function. As such, our theoretical results demonstrate that it is able to achieve linear speedup with respect to the number of workers. We believe this novel algorithmic design could benefit the development of decentralized compositional optimization. Finally, we applied our methods to the imbalanced classification problem. The extensive experimental results provide evidence for the effectiveness of our algorithm.
</details>
<details>
<summary>摘要</summary>
“ Stochastic compositional minimax problem 在 recent years 中吸引了很多关注，因为它涵盖了许多emerging machine learning models。然而，由于分布式数据的出现，optimalizing this kind of problem under the decentralized setting becomes badly needed。然而，compositional structure in the loss function brings unique challenges to designing efficient decentralized optimization algorithms。具体来说，our study shows that the standard gossip communication strategy cannot achieve linear speedup for decentralized compositional minimax problems due to the large consensus error about the inner-level function。为了解决这个问题，我们开发了一种novel decentralized stochastic compositional gradient descent ascent with momentum algorithm，以减少inner-level function consensus error。因此，our theoretical results demonstrate that it is able to achieve linear speedup with respect to the number of workers。我们认为这种新的algorithmic design可以benefit the development of decentralized compositional optimization。最后，我们应用了我们的方法到imbalanced classification problem。our extensive experimental results provide evidence for the effectiveness of our algorithm.”Note: The translation is in Simplified Chinese, which is one of the two standard versions of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="A-signal-processing-interpretation-of-noise-reduction-convolutional-neural-networks"><a href="#A-signal-processing-interpretation-of-noise-reduction-convolutional-neural-networks" class="headerlink" title="A signal processing interpretation of noise-reduction convolutional neural networks"></a>A signal processing interpretation of noise-reduction convolutional neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13425">http://arxiv.org/abs/2307.13425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luis A. Zavala-Mondragón, Peter H. N. de With, Fons van der Sommen</li>
<li>for: 这篇论文旨在探讨深度学习中的编码-解码卷积神经网络（ED CNN）的理论基础，以及如何通过这些理论来设计更加稳定和高效的 CNN 架构。</li>
<li>methods: 本文使用了深度学习的基本原理和信号处理的基本原理来探讨 ED CNN 的内部运作，并提出了一种统一的理论框架来描述不同的 ED CNN 架构。</li>
<li>results: 本文提出了一种自 contenained的框架，可以帮助设计更加稳定和高效的 CNN 架构，并且可以帮助更多人理解深度学习中的 ED CNN 的内部运作。<details>
<summary>Abstract</summary>
Encoding-decoding CNNs play a central role in data-driven noise reduction and can be found within numerous deep-learning algorithms. However, the development of these CNN architectures is often done in ad-hoc fashion and theoretical underpinnings for important design choices is generally lacking. Up to this moment there are different existing relevant works that strive to explain the internal operation of these CNNs. Still, these ideas are either scattered and/or may require significant expertise to be accessible for a bigger audience. In order to open up this exciting field, this article builds intuition on the theory of deep convolutional framelets and explains diverse ED CNN architectures in a unified theoretical framework. By connecting basic principles from signal processing to the field of deep learning, this self-contained material offers significant guidance for designing robust and efficient novel CNN architectures.
</details>
<details>
<summary>摘要</summary>
Encoding-decoding CNNs 在数据驱动的噪声减少中扮演中央角色，可以在多种深度学习算法中找到。然而，Encoding-decoding CNNs 的建构通常是靠束式的，而且关键设计选择的理论基础通常缺失。到目前为止，有很多相关的工作努力了解Encoding-decoding CNNs 的内部运作。然而，这些想法是杂乱不尽，需要特殊的专业知识才能访问更广泛的群众。为了开拓这一领域，这篇文章建立了深度卷积框架的理论基础，并将多种ED CNN架构纳入一个统一的理论框架中。通过将信号处理的基本原理与深度学习相连接，这篇自 contenido的材料提供了设计robust和高效的新的CNN架构的重要指南。
</details></li>
</ul>
<hr>
<h2 id="Non-Intrusive-Intelligibility-Predictor-for-Hearing-Impaired-Individuals-using-Self-Supervised-Speech-Representations"><a href="#Non-Intrusive-Intelligibility-Predictor-for-Hearing-Impaired-Individuals-using-Self-Supervised-Speech-Representations" class="headerlink" title="Non Intrusive Intelligibility Predictor for Hearing Impaired Individuals using Self Supervised Speech Representations"></a>Non Intrusive Intelligibility Predictor for Hearing Impaired Individuals using Self Supervised Speech Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13423">http://arxiv.org/abs/2307.13423</a></li>
<li>repo_url: None</li>
<li>paper_authors: George Close, Thomas Hain, Stefan Goetze</li>
<li>for: 这个研究旨在扩展非侵入式的音质评分模型，以便对听力障碍用户的语音质量进行预测。</li>
<li>methods: 这个研究使用了自我超vised speech representation（SSSRs）作为输入特征，并利用了非侵入式预测模型来预测听力障碍用户的语音质量。</li>
<li>results: 研究发现，SSSRs可以作为输入特征，实现了与更复杂的系统相当的竞争性性能。对于不同的listeners和优化系统，研究还发现了预测性能的相关性分析。<details>
<summary>Abstract</summary>
Self-supervised speech representations (SSSRs) have been successfully applied to a number of speech-processing tasks, e.g. as feature extractor for speech quality (SQ) prediction, which is, in turn, relevant for assessment and training speech enhancement systems for users with normal or impaired hearing. However, exact knowledge of why and how quality-related information is encoded well in such representations remains poorly understood. In this work, techniques for non-intrusive prediction of SQ ratings are extended to the prediction of intelligibility for hearing-impaired users. It is found that self-supervised representations are useful as input features to non-intrusive prediction models, achieving competitive performance to more complex systems. A detailed analysis of the performance depending on Clarity Prediction Challenge 1 listeners and enhancement systems indicates that more data might be needed to allow generalisation to unknown systems and (hearing-impaired) individuals
</details>
<details>
<summary>摘要</summary>
自我监督的语音表示（SSSR）已经成功应用于一些语音处理任务中，例如作为语音质量（SQ）预测的特征提取器，这对于评估和训练语音增强系统 для正常或受损耳朵用户都是重要的。然而，关于为什么和如何在这些表示中储存质量相关信息的具体知识仍然不够了解。在这项工作中，对于听众评分不侵入的预测模型的扩展，发现自我监督表示是非常有用的输入特征，可以达到和更复杂的系统相当的性能。对于 Clarity Prediction Challenge 1 listeners 和增强系统的分析表明，更多的数据可能需要，以allow总结到未知系统和听众（听力障碍）个体
</details></li>
</ul>
<hr>
<h2 id="On-the-Learning-Dynamics-of-Attention-Networks"><a href="#On-the-Learning-Dynamics-of-Attention-Networks" class="headerlink" title="On the Learning Dynamics of Attention Networks"></a>On the Learning Dynamics of Attention Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13421">http://arxiv.org/abs/2307.13421</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vashisht-rahul/on-the-learning-dynamics-of-attention-networks">https://github.com/vashisht-rahul/on-the-learning-dynamics-of-attention-networks</a></li>
<li>paper_authors: Rahul Vashisht, Harish G. Ramaswamy</li>
<li>for: 本文研究了三种推理损失函数，它们分别是软注意力损失、硬注意力损失和隐变量маarginal likelihood（LVML）注意力损失。这三种损失函数都是为了找到一个<code>焦点&#39;模型，它可以选择输入中正确的段落，以及一个</code>分类’模型，它可以处理选择的段落并生成目标标签。但是它们在选择段落的方式不同，导致了不同的动态和最终结果。</li>
<li>methods: 本文使用了三种推理损失函数来训练模型，并分析了这些损失函数在不同的情况下的行为。同时，文章还提出了一种简单的混合方法，该方法将软注意力损失、硬注意力损失和LVML注意力损失的优点相互融合，并在一些半synthetic和实际世界数据集上进行了实验。</li>
<li>results: 本文的实验结果表明，软注意力损失和硬注意力损失在不同的初始化情况下，会导致模型的表现有很大差异。而LVML注意力损失则在初始化情况下表现较好，但是后续的训练过程中会逐渐下降。此外，文章还发现了这些损失函数在不同的数据集上的表现差异，并提出了一种简单的混合方法来解决这个问题。<details>
<summary>Abstract</summary>
Attention models are typically learned by optimizing one of three standard loss functions that are variously called -- soft attention, hard attention, and latent variable marginal likelihood (LVML) attention. All three paradigms are motivated by the same goal of finding two models -- a `focus' model that `selects' the right \textit{segment} of the input and a `classification' model that processes the selected segment into the target label. However, they differ significantly in the way the selected segments are aggregated, resulting in distinct dynamics and final results. We observe a unique signature of models learned using these paradigms and explain this as a consequence of the evolution of the classification model under gradient descent when the focus model is fixed. We also analyze these paradigms in a simple setting and derive closed-form expressions for the parameter trajectory under gradient flow. With the soft attention loss, the focus model improves quickly at initialization and splutters later on. On the other hand, hard attention loss behaves in the opposite fashion. Based on our observations, we propose a simple hybrid approach that combines the advantages of the different loss functions and demonstrates it on a collection of semi-synthetic and real-world datasets
</details>
<details>
<summary>摘要</summary>
注意模型通常通过优化三种标准损失函数来学习：软注意、硬注意和隐变量极大可能性（LVML）注意。这三种方法均由同一目标——找到一个`焦点'模型，可以选择输入中正确的段落，以及一个`分类'模型，可以处理选择的段落并生成目标标签。然而，它们在选取段落的方式不同，从而导致了不同的动态和最终结果。我们在这些模型中观察到了独特的签名，并解释这为Gradient Descent的演化下的分类模型的变化所致。我们还在简单的设置下分析这些方法，并derive了关键参数轨迹的关闭式表达式。在软注意损失函数下，焦点模型在初始化时快速改进，然后后来受阻。相反，硬注意损失函数表现得更opposite。基于我们的观察，我们提出了一种简单的混合方法，其combines the advantages of different loss functions, and demonstrates it on a collection of semi-synthetic and real-world datasets.Note: The translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that form instead.
</details></li>
</ul>
<hr>
<h2 id="Co-Design-of-Out-of-Distribution-Detectors-for-Autonomous-Emergency-Braking-Systems"><a href="#Co-Design-of-Out-of-Distribution-Detectors-for-Autonomous-Emergency-Braking-Systems" class="headerlink" title="Co-Design of Out-of-Distribution Detectors for Autonomous Emergency Braking Systems"></a>Co-Design of Out-of-Distribution Detectors for Autonomous Emergency Braking Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13419">http://arxiv.org/abs/2307.13419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Yuhas, Arvind Easwaran</li>
<li>for: 这个论文的目的是提出一种用于自动驾驶车辆（AV）中的风险监控方法，以确保AV的决策正确性。</li>
<li>methods: 这个论文使用了一种基于视觉的自动紧急刹车系统（AEBS），并使用了风险模型来描述LEC和OOD检测器的设计参数之间的交互关系，以及它们对系统安全的影响。</li>
<li>results: 根据这个论文，通过使用这种协设方法，可以在AEBS中减少风险达42.3%，同时保持资源利用相对均衡。<details>
<summary>Abstract</summary>
Learning enabled components (LECs), while critical for decision making in autonomous vehicles (AVs), are likely to make incorrect decisions when presented with samples outside of their training distributions. Out-of-distribution (OOD) detectors have been proposed to detect such samples, thereby acting as a safety monitor, however, both OOD detectors and LECs require heavy utilization of embedded hardware typically found in AVs. For both components, there is a tradeoff between non-functional and functional performance, and both impact a vehicle's safety. For instance, giving an OOD detector a longer response time can increase its accuracy at the expense of the LEC. We consider an LEC with binary output like an autonomous emergency braking system (AEBS) and use risk, the combination of severity and occurrence of a failure, to model the effect of both components' design parameters on each other's functional and non-functional performance, as well as their impact on system safety. We formulate a co-design methodology that uses this risk model to find the design parameters for an OOD detector and LEC that decrease risk below that of the baseline system and demonstrate it on a vision based AEBS. Using our methodology, we achieve a 42.3% risk reduction while maintaining equivalent resource utilization.
</details>
<details>
<summary>摘要</summary>
学习启用组件（LEC）在自动驾驶车辆（AV）中扮演着重要的决策角色，但当面临不同于训练数据集的样本时，LEC可能会作出错误的决策。为了检测这些样本，外围样本检测器（OOD）被提出来作为安全监视器。然而，OOD检测器和LEC都需要嵌入式硬件的严重利用，这两者之间存在功能性和非功能性性能之间的贸易关系。为了保证系统的安全，我们需要考虑这两个组件之间的影响，并根据风险模型来寻找合适的设计参数。我们提出了一种共同设计方法，该方法使用风险模型来找到一个符合风险标准的OOD检测器和LEC设计参数，并在视觉基于的自动紧急制动系统（AEBS）上进行了示例实现。结果表明，我们可以降低风险的总体减少率为42.3%，同时保持资源利用相对均衡。
</details></li>
</ul>
<hr>
<h2 id="Communication-Efficient-Orchestrations-for-URLLC-Service-via-Hierarchical-Reinforcement-Learning"><a href="#Communication-Efficient-Orchestrations-for-URLLC-Service-via-Hierarchical-Reinforcement-Learning" class="headerlink" title="Communication-Efficient Orchestrations for URLLC Service via Hierarchical Reinforcement Learning"></a>Communication-Efficient Orchestrations for URLLC Service via Hierarchical Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13415">http://arxiv.org/abs/2307.13415</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Shi, Milad Ganjalizadeh, Hossein Shokri Ghadikolaei, Marina Petrova</li>
<li>for: This paper aims to improve the efficiency of ultra-reliable low latency communications (URLLC) services in 5G networks by leveraging reinforcement learning (RL) to allocate wireless resources.</li>
<li>methods: The proposed multi-agent hierarchical RL (HRL) framework enables the implementation of multi-level policies with different control loop timescales, allowing for more efficient use of wireless resources and reduced signaling and energy consumption.</li>
<li>results: The proposed HRL framework achieves better performance than traditional RL methods in a factory automation scenario, with significantly less overhead of signal transmissions and delay.<details>
<summary>Abstract</summary>
Ultra-reliable low latency communications (URLLC) service is envisioned to enable use cases with strict reliability and latency requirements in 5G. One approach for enabling URLLC services is to leverage Reinforcement Learning (RL) to efficiently allocate wireless resources. However, with conventional RL methods, the decision variables (though being deployed at various network layers) are typically optimized in the same control loop, leading to significant practical limitations on the control loop's delay as well as excessive signaling and energy consumption. In this paper, we propose a multi-agent Hierarchical RL (HRL) framework that enables the implementation of multi-level policies with different control loop timescales. Agents with faster control loops are deployed closer to the base station, while the ones with slower control loops are at the edge or closer to the core network providing high-level guidelines for low-level actions. On a use case from the prior art, with our HRL framework, we optimized the maximum number of retransmissions and transmission power of industrial devices. Our extensive simulation results on the factory automation scenario show that the HRL framework achieves better performance as the baseline single-agent RL method, with significantly less overhead of signal transmissions and delay compared to the one-agent RL methods.
</details>
<details>
<summary>摘要</summary>
超可靠低延迟通信服务（URLLC）在5G中被看作为实现具有严格可靠性和延迟要求的应用场景。一种实现URLLC服务的方法是通过强化学习（RL）有效地分配无线资源。然而，传统RL方法中的决策变量（即在不同网络层部署）通常在同一个控制循环中优化，这会导致控制循环延迟的具体限制以及过分的信号传输和能量消耗。在这篇论文中，我们提出了一种多代理层RL（HRL）框架，该框架允许实现多级策略，并且具有不同控制循环时间尺度。在基站附近部署更快的控制循环的代理，而在边缘或核心网络附近部署更慢的控制循环的代理，以提供高级指导供低级动作。在一个工厂自动化场景中，通过我们的HRL框架，我们优化了设备的最大重传数和发射功率。我们的广泛的模拟结果表明，HRL框架在基于优化单个代理RL方法的场景下表现更好，并且具有较少的信号传输过程和延迟。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Memory-Wall-Effects-in-CNN-Engines-with-On-the-Fly-Weights-Generation"><a href="#Mitigating-Memory-Wall-Effects-in-CNN-Engines-with-On-the-Fly-Weights-Generation" class="headerlink" title="Mitigating Memory Wall Effects in CNN Engines with On-the-Fly Weights Generation"></a>Mitigating Memory Wall Effects in CNN Engines with On-the-Fly Weights Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13412">http://arxiv.org/abs/2307.13412</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stylianos I. Venieris, Javier Fernandez-Marques, Nicholas D. Lane</li>
<li>for: 这个研究旨在提高FPGA基于 convolutional neural network (CNN) 的数据传输效率和能效性。</li>
<li>methods: 研究人员提出了一个新的 CNN 数据传输系统，包括一个新的 CNN 硬件架构，以及一个自动硬件扮潮方法来优化数据传输效率。</li>
<li>results: 研究人员发现，这个新的 CNN 数据传输系统可以实现平均提高2.57倍的性能效率，并且可以实现更高的性能密度，较以前的FPGA基于 CNN 数据传输系统。<details>
<summary>Abstract</summary>
The unprecedented accuracy of convolutional neural networks (CNNs) across a broad range of AI tasks has led to their widespread deployment in mobile and embedded settings. In a pursuit for high-performance and energy-efficient inference, significant research effort has been invested in the design of FPGA-based CNN accelerators. In this context, single computation engines constitute a popular approach to support diverse CNN modes without the overhead of fabric reconfiguration. Nevertheless, this flexibility often comes with significantly degraded performance on memory-bound layers and resource underutilisation due to the suboptimal mapping of certain layers on the engine's fixed configuration. In this work, we investigate the implications in terms of CNN engine design for a class of models that introduce a pre-convolution stage to decompress the weights at run time. We refer to these approaches as on-the-fly. This paper presents unzipFPGA, a novel CNN inference system that counteracts the limitations of existing CNN engines. The proposed framework comprises a novel CNN hardware architecture that introduces a weights generator module that enables the on-chip on-the-fly generation of weights, alleviating the negative impact of limited bandwidth on memory-bound layers. We further enhance unzipFPGA with an automated hardware-aware methodology that tailors the weights generation mechanism to the target CNN-device pair, leading to an improved accuracy-performance balance. Finally, we introduce an input selective processing element (PE) design that balances the load between PEs in suboptimally mapped layers. The proposed framework yields hardware designs that achieve an average of 2.57x performance efficiency gain over highly optimised GPU designs for the same power constraints and up to 3.94x higher performance density over a diverse range of state-of-the-art FPGA-based CNN accelerators.
</details>
<details>
<summary>摘要</summary>
具有历史上无 precedent的精度的卷积神经网络（CNN）在许多人工智能任务中显示出了广泛的应用，因此在移动和嵌入式设备上广泛部署。为了实现高性能和能效的推理，大量的研究力量被投入到了基于FPGA的CNN加速器的设计中。在这个上下文中，单个计算引擎是一种广泛使用的方法来支持多种CNN模式，而无需fabric重新配置的开销。然而，这种灵活性通常会导致在储存bound层上的性能下降和资源废用，因为在计算引擎的固定配置下可能会导致一些层的不佳映射。在这种情况下，我们研究了CNN引擎设计方面的各种因素，以及在运行时可以通过压缩权重来解决这些问题的方法。我们称这种方法为“on-the-fly”方法。本文介绍了一种新的CNN推理系统——unzipFPGA，该系统可以对CNN模型进行在运行时压缩权重，从而减少储存bound层的性能下降。此外，我们还提出了一种自适应硬件方法，该方法可以根据目标CNN设备和模型之间的优化，以提高精度和性能的平衡。最后，我们还提出了一种输入选择处理元件（PE）的设计，该设计可以在不佳映射的层中均衡PE的负担。根据我们的实验结果，unzipFPGA系统可以在同等功耗下达到GPU设计的2.57倍的性能效率提升，并且在多种现有的FPGA-based CNN加速器中达到3.94倍的性能密度。
</details></li>
</ul>
<hr>
<h2 id="The-Double-Edged-Sword-of-Big-Data-and-Information-Technology-for-the-Disadvantaged-A-Cautionary-Tale-from-Open-Banking"><a href="#The-Double-Edged-Sword-of-Big-Data-and-Information-Technology-for-the-Disadvantaged-A-Cautionary-Tale-from-Open-Banking" class="headerlink" title="The Double-Edged Sword of Big Data and Information Technology for the Disadvantaged: A Cautionary Tale from Open Banking"></a>The Double-Edged Sword of Big Data and Information Technology for the Disadvantaged: A Cautionary Tale from Open Banking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13408">http://arxiv.org/abs/2307.13408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Savina Dine Kim, Galina Andreeva, Michael Rovatsos</li>
<li>for: 这篇论文探讨了在开放银行服务中可能存在的不公正隐含因素，具体是通过机器学习（ML）技术分析金融服务中的交易数据，以了解金融敏感人群的行为特征和风险评估方面的隐藏风险。</li>
<li>methods: 这篇论文使用了一个Unique的UK FinTech借据集，并比较了三种ML分类器来预测金融敏感人群的可能性，并通过划分来描述不同群体的特征组合。</li>
<li>results: 研究发现，引入的金融行为特征可以预测排除个人信息，特别是敏感或保护特征，这说明开放银行数据中隐藏的风险。研究者提出了对开放银行服务的不公正风险的批评，并建议在这种新技术环境中保持公正。<details>
<summary>Abstract</summary>
This research article analyses and demonstrates the hidden implications for fairness of seemingly neutral data coupled with powerful technology, such as machine learning (ML), using Open Banking as an example. Open Banking has ignited a revolution in financial services, opening new opportunities for customer acquisition, management, retention, and risk assessment. However, the granularity of transaction data holds potential for harm where unnoticed proxies for sensitive and prohibited characteristics may lead to indirect discrimination. Against this backdrop, we investigate the dimensions of financial vulnerability (FV), a global concern resulting from COVID-19 and rising inflation. Specifically, we look to understand the behavioral elements leading up to FV and its impact on at-risk, disadvantaged groups through the lens of fair interpretation. Using a unique dataset from a UK FinTech lender, we demonstrate the power of fine-grained transaction data while simultaneously cautioning its safe usage. Three ML classifiers are compared in predicting the likelihood of FV, and groups exhibiting different magnitudes and forms of FV are identified via clustering to highlight the effects of feature combination. Our results indicate that engineered features of financial behavior can be predictive of omitted personal information, particularly sensitive or protected characteristics, shedding light on the hidden dangers of Open Banking data. We discuss the implications and conclude fairness via unawareness is ineffective in this new technological environment.
</details>
<details>
<summary>摘要</summary>
Using a unique dataset from a UK FinTech lender, we compare three ML classifiers in predicting the likelihood of FV and identify groups exhibiting different forms of FV through clustering. Our results show that engineered features of financial behavior can be used to predict omitted personal information, including sensitive or protected characteristics. This highlights the potential dangers of using Open Banking data without proper safeguards.We conclude that fairness through unawareness is not effective in this new technological environment and discuss the implications for financial services and regulatory frameworks.
</details></li>
</ul>
<hr>
<h2 id="Counterfactual-Explanation-via-Search-in-Gaussian-Mixture-Distributed-Latent-Space"><a href="#Counterfactual-Explanation-via-Search-in-Gaussian-Mixture-Distributed-Latent-Space" class="headerlink" title="Counterfactual Explanation via Search in Gaussian Mixture Distributed Latent Space"></a>Counterfactual Explanation via Search in Gaussian Mixture Distributed Latent Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13390">http://arxiv.org/abs/2307.13390</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuan Zhao, Klaus Broelemann, Gjergji Kasneci</li>
<li>for: 本研究旨在提供一种生成counterfactual explanations（CEs）的新方法，以帮助用户更好地理解AI系统的决策过程和改进其决策结果。</li>
<li>methods: 本方法首先将autoencoder的 latent space变换成一个mixture of Gaussian distributions，然后通过线性 interpolate在query sample和目标类中心点之间生成CEs。</li>
<li>results: 我们的方法可以快速和高效地生成高质量的CEs，并且可以保持输入样本的特征。在多个图像和表格 datasets上进行了多种实验，并证明了我们的方法与现有的三种方法相比，能够更高效地返回结果，并且更加符合原始数据抽象。<details>
<summary>Abstract</summary>
Counterfactual Explanations (CEs) are an important tool in Algorithmic Recourse for addressing two questions: 1. What are the crucial factors that led to an automated prediction/decision? 2. How can these factors be changed to achieve a more favorable outcome from a user's perspective? Thus, guiding the user's interaction with AI systems by proposing easy-to-understand explanations and easy-to-attain feasible changes is essential for the trustworthy adoption and long-term acceptance of AI systems. In the literature, various methods have been proposed to generate CEs, and different quality measures have been suggested to evaluate these methods. However, the generation of CEs is usually computationally expensive, and the resulting suggestions are unrealistic and thus non-actionable. In this paper, we introduce a new method to generate CEs for a pre-trained binary classifier by first shaping the latent space of an autoencoder to be a mixture of Gaussian distributions. CEs are then generated in latent space by linear interpolation between the query sample and the centroid of the target class. We show that our method maintains the characteristics of the input sample during the counterfactual search. In various experiments, we show that the proposed method is competitive based on different quality measures on image and tabular datasets -- efficiently returns results that are closer to the original data manifold compared to three state-of-the-art methods, which are essential for realistic high-dimensional machine learning applications.
</details>
<details>
<summary>摘要</summary>
counterfactual explanations (CEs) 是 algorithmic recourse 中的一个重要工具，用于回答以下两个问题：1. 自动预测/决策中的重要因素是什么？2. 如何变化这些因素以获得更有利的结果从用户的角度？因此，为AI系统的使用者提供易于理解的解释和实现可行的改变是AI系统的信任采用和长期接受的重要前提。在文献中，多种方法已经被提出供生成 CE，并提出了不同的质量指标来评估这些方法。然而，生成 CE 通常是 computationally 昂贵的，并且从生成的建议中返回的结果通常是不现实的，因此无法实际应用。在这篇文章中，我们提出了一新的方法，用于生成一个预训 binary 分类器 的 CE。我们首先将 autoencoder 的潜在空间变成一个 Gaussian 分布的混合体。然后，我们在潜在空间中使用 straight  interpolate 生成 CE。我们显示了我们的方法可以保持输入样本的特征。在多个实验中，我们显示了我们的方法与其他三种现今最佳方法相比，在像素和数据Dataset 上表现更好，能够高效地返回更加接近原始数据构造的结果。
</details></li>
</ul>
<hr>
<h2 id="BotHawk-An-Approach-for-Bots-Detection-in-Open-Source-Software-Projects"><a href="#BotHawk-An-Approach-for-Bots-Detection-in-Open-Source-Software-Projects" class="headerlink" title="BotHawk: An Approach for Bots Detection in Open Source Software Projects"></a>BotHawk: An Approach for Bots Detection in Open Source Software Projects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13386">http://arxiv.org/abs/2307.13386</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bifenglin/bothawk">https://github.com/bifenglin/bothawk</a></li>
<li>paper_authors: Fenglin Bi, Zhiwei Zhu, Wei Wang, Xiaoya Xia, Hassan Ali Khan, Peng Pu<br>for: This research aims to investigate bots’ behavior in open-source software projects and identify bot accounts with maximum possible accuracy.methods: The team gathered a dataset of 19,779 accounts that meet standardized criteria, and analyzed their behavior across 17 features in 5 dimensions. They also created a model called BotHawk, which outperforms other models in detecting bots.results: The team identified four types of bot accounts in open-source software projects, and found that the number of followers, number of repositories, and tags contain the most relevant features to identify the account type. The BotHawk model achieved an AUC of 0.947 and an F1-score of 0.89.<details>
<summary>Abstract</summary>
Social coding platforms have revolutionized collaboration in software development, leading to using software bots for streamlining operations. However, The presence of open-source software (OSS) bots gives rise to problems including impersonation, spamming, bias, and security risks. Identifying bot accounts and behavior is a challenging task in the OSS project. This research aims to investigate bots' behavior in open-source software projects and identify bot accounts with maximum possible accuracy. Our team gathered a dataset of 19,779 accounts that meet standardized criteria to enable future research on bots in open-source projects. We follow a rigorous workflow to ensure that the data we collect is accurate, generalizable, scalable, and up-to-date. We've identified four types of bot accounts in open-source software projects by analyzing their behavior across 17 features in 5 dimensions. Our team created BotHawk, a highly effective model for detecting bots in open-source software projects. It outperforms other models, achieving an AUC of 0.947 and an F1-score of 0.89. BotHawk can detect a wider variety of bots, including CI/CD and scanning bots. Furthermore, we find that the number of followers, number of repositories, and tags contain the most relevant features to identify the account type.
</details>
<details>
<summary>摘要</summary>
社区代码平台已经革命化软件开发的协作方式，导致使用软件机器人来加速操作。然而，开源软件（OSS）机器人的存在引起了一些问题，包括隐匿、垃圾邮件、偏见和安全风险。标识机器人帐户和行为是在OSS项目中的挑战。本研究的目标是estigate机器人在开源软件项目中的行为，并尽可能准确地标识机器人帐户。我们的团队收集了19,779个符合标准化要求的帐户，以便未来研究机器人在开源项目中的行为。我们遵循了一个严格的工作流程，以确保我们收集的数据准确、可重复、可扩展和时效。我们通过分析17个特征在5个维度中，分类出了机器人帐户的四种类型。我们创建了BotHawk模型，可以高效地在开源软件项目中检测机器人。它比其他模型高效，其AUC为0.947，F1分数为0.89。BotHawk可以检测更多类型的机器人，包括CI/CD和扫描机器人。此外，我们发现帐户类型的最重要特征是粉丝数、Repository数和标签。
</details></li>
</ul>
<hr>
<h2 id="Scaff-PD-Communication-Efficient-Fair-and-Robust-Federated-Learning"><a href="#Scaff-PD-Communication-Efficient-Fair-and-Robust-Federated-Learning" class="headerlink" title="Scaff-PD: Communication Efficient Fair and Robust Federated Learning"></a>Scaff-PD: Communication Efficient Fair and Robust Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13381">http://arxiv.org/abs/2307.13381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yaodong Yu, Sai Praneeth Karimireddy, Yi Ma, Michael I. Jordan</li>
<li>for: 提高 federated learning 中的公平性和Robustness</li>
<li>methods: 使用 accelerated primal dual (APD) 算法和 bias corrected local steps (as in Scaffold)</li>
<li>results: 在 Several benchmark datasets 上 demonstrate 了 Scaff-PD 的效果，可以提高 fairness 和 robustness，同时保持竞争性的准确性。Translation:</li>
<li>for: Improving fairness and robustness in federated learning</li>
<li>methods: Using accelerated primal dual (APD) algorithm and bias corrected local steps (as in Scaffold)</li>
<li>results: Demonstrated effectiveness in improving fairness and robustness while maintaining competitive accuracy on several benchmark datasets.<details>
<summary>Abstract</summary>
We present Scaff-PD, a fast and communication-efficient algorithm for distributionally robust federated learning. Our approach improves fairness by optimizing a family of distributionally robust objectives tailored to heterogeneous clients. We leverage the special structure of these objectives, and design an accelerated primal dual (APD) algorithm which uses bias corrected local steps (as in Scaffold) to achieve significant gains in communication efficiency and convergence speed. We evaluate Scaff-PD on several benchmark datasets and demonstrate its effectiveness in improving fairness and robustness while maintaining competitive accuracy. Our results suggest that Scaff-PD is a promising approach for federated learning in resource-constrained and heterogeneous settings.
</details>
<details>
<summary>摘要</summary>
我们提出了Scaff-PD算法，这是一种快速并且通信效率高的分布robust Federated学习算法。我们的方法可以提高公平性，通过优化适应于多样化客户端的分布robust目标函数。我们利用这些目标函数的特殊结构，并设计了一种加速的 primal dual（APD）算法，使用偏置修正的本地步骤（如Scaffold）来实现重要的通信效率和速度提升。我们对多个标准数据集进行了评估，并证明了Scaff-PD在保持竞争性下提高了公平性和Robustness。我们的结果表明，Scaff-PD是一种有前途的方法 дляFederated学习在资源受限和多样化环境中。Note: Please keep in mind that the translation is done by a machine and may not be perfect. It's always a good idea to have a human translator review the translation for accuracy.
</details></li>
</ul>
<hr>
<h2 id="Submodular-Reinforcement-Learning"><a href="#Submodular-Reinforcement-Learning" class="headerlink" title="Submodular Reinforcement Learning"></a>Submodular Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13372">http://arxiv.org/abs/2307.13372</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/manish-pra/non-additive-rl">https://github.com/manish-pra/non-additive-rl</a></li>
<li>paper_authors: Manish Prajapat, Mojmír Mutný, Melanie N. Zeilinger, Andreas Krause</li>
<li>for: 这个论文的目标是推广传统的奖励学习（RL），以处理具有减少奖励的应用场景，例如覆盖控制、实验设计和信息路径规划。</li>
<li>methods: 这个论文提出了一种新的RL方法，称为submodular RL（SubRL），它使用子模卷函数来模型具有减少奖励的奖励。在某些情况下，这种方法可以通过 Policy Gradient 算法来优化。</li>
<li>results: 论文的实验结果表明，SubPO 算法可以在具有减少奖励的情况下实现常量系数近似，并且可以在高维状态和动作空间中进行地方优化。此外，论文还应用了这种方法在生物多样性监测、极限试验设计、信息路径规划和覆盖最大化等应用场景中。<details>
<summary>Abstract</summary>
In reinforcement learning (RL), rewards of states are typically considered additive, and following the Markov assumption, they are $\textit{independent}$ of states visited previously. In many important applications, such as coverage control, experiment design and informative path planning, rewards naturally have diminishing returns, i.e., their value decreases in light of similar states visited previously. To tackle this, we propose $\textit{submodular RL}$ (SubRL), a paradigm which seeks to optimize more general, non-additive (and history-dependent) rewards modelled via submodular set functions which capture diminishing returns. Unfortunately, in general, even in tabular settings, we show that the resulting optimization problem is hard to approximate. On the other hand, motivated by the success of greedy algorithms in classical submodular optimization, we propose SubPO, a simple policy gradient-based algorithm for SubRL that handles non-additive rewards by greedily maximizing marginal gains. Indeed, under some assumptions on the underlying Markov Decision Process (MDP), SubPO recovers optimal constant factor approximations of submodular bandits. Moreover, we derive a natural policy gradient approach for locally optimizing SubRL instances even in large state- and action- spaces. We showcase the versatility of our approach by applying SubPO to several applications, such as biodiversity monitoring, Bayesian experiment design, informative path planning, and coverage maximization. Our results demonstrate sample efficiency, as well as scalability to high-dimensional state-action spaces.
</details>
<details>
<summary>摘要</summary>
在强化学习（RL）中，状态奖励通常被视为加性的，并且遵循Markov假设，它们是独立的状态所访问的。在许多重要应用中，如覆盖控制、实验设计和有用路径规划，奖励自然地具有递减返回，即在类似的状态上访问的奖励值逐渐减少。为解决这个问题，我们提出了“submodular RL”（SubRL），一种模型寻求优化更一般、非加性（历史相依）的奖励，使用submodular集合函数来捕捉递减返回。然而，在一般情况下，即使在表格式设定中，我们表明了优化问题的解决很难。然而，以古德理算法的成功在经典submodular优化中为启发，我们提出了SubPO，一种简单的policy梯度基本算法，用于处理非加性奖励。在一些假设下，SubPO可以在Markov决策过程（MDP）中获得优化的常数因子approximation。此外，我们还 deriv了一种自然的policy梯度方法，用于本地优化SubRL实例，即使在高维状态-动作空间中。我们在几个应用中应用SubPO，如生物多样性监测、权重实验设计、有用路径规划和覆盖最大化。我们的结果表明Sample效率和高维状态-动作空间的扩展性。
</details></li>
</ul>
<hr>
<h2 id="Learning-Regions-of-Interest-for-Bayesian-Optimization-with-Adaptive-Level-Set-Estimation"><a href="#Learning-Regions-of-Interest-for-Bayesian-Optimization-with-Adaptive-Level-Set-Estimation" class="headerlink" title="Learning Regions of Interest for Bayesian Optimization with Adaptive Level-Set Estimation"></a>Learning Regions of Interest for Bayesian Optimization with Adaptive Level-Set Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13371">http://arxiv.org/abs/2307.13371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fengxue Zhang, Jialin Song, James Bowden, Alexander Ladd, Yisong Yue, Thomas A. Desautels, Yuxin Chen</li>
<li>for: 这个论文是关于 Bayesian 优化（BO）在高维和非站点场景中的研究。</li>
<li>methods: 该方法提议一个名为 BALLET 的框架，该框架可以自动过滤高信息区域（ROI），并使用两种 probabilistic 模型：一个粗细的 Gaussian process（GP）来识别 ROI，以及一个局部化的 GP 来优化在 ROI 中。</li>
<li>results: 该研究证明 BALLET 可以高效地缩小搜索空间，并可以比标准 BO 方法具有更紧的 regret  bound。该方法也在 synthetic 和实际优化任务上进行了 empirical 验证，并表现出了效果。<details>
<summary>Abstract</summary>
We study Bayesian optimization (BO) in high-dimensional and non-stationary scenarios. Existing algorithms for such scenarios typically require extensive hyperparameter tuning, which limits their practical effectiveness. We propose a framework, called BALLET, which adaptively filters for a high-confidence region of interest (ROI) as a superlevel-set of a nonparametric probabilistic model such as a Gaussian process (GP). Our approach is easy to tune, and is able to focus on local region of the optimization space that can be tackled by existing BO methods. The key idea is to use two probabilistic models: a coarse GP to identify the ROI, and a localized GP for optimization within the ROI. We show theoretically that BALLET can efficiently shrink the search space, and can exhibit a tighter regret bound than standard BO without ROI filtering. We demonstrate empirically the effectiveness of BALLET on both synthetic and real-world optimization tasks.
</details>
<details>
<summary>摘要</summary>
我们研究 bayesian 优化（BO）在高维和不确定场景下。现有的算法通常需要大量的 гипер参数调整，这限制了它们在实际应用中的实用性。我们提议一种框架，called BALLET，它可以动态筛选高信度区域（ROI）作为一个超级集的非Parametric  probabilistic 模型，如 Gaussian process（GP）。我们的方法容易调整，可以专注于可以使用现有BO方法优化的本地区域。关键思想是使用两个 probabilistic 模型：一个粗细的 GP 来确定 ROI，并一个局部化的 GP 进行优化在 ROI 中。我们证明了 BALLET 可以有效减小搜索空间，并可以达到标准BO  без ROI 筛选的紧凑 regret  bound。我们通过 Synthetic 和实际优化任务的实验证明了 BALLET 的实际效果。
</details></li>
</ul>
<hr>
<h2 id="Computational-Guarantees-for-Doubly-Entropic-Wasserstein-Barycenters-via-Damped-Sinkhorn-Iterations"><a href="#Computational-Guarantees-for-Doubly-Entropic-Wasserstein-Barycenters-via-Damped-Sinkhorn-Iterations" class="headerlink" title="Computational Guarantees for Doubly Entropic Wasserstein Barycenters via Damped Sinkhorn Iterations"></a>Computational Guarantees for Doubly Entropic Wasserstein Barycenters via Damped Sinkhorn Iterations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13370">http://arxiv.org/abs/2307.13370</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lénaïc Chizat, Tomas Vaškevičius</li>
<li>for:  computes doubly regularized Wasserstein barycenters, a type of entropic barycenter with inner and outer regularization strengths.</li>
<li>methods: uses damped Sinkhorn iterations followed by exact maximization&#x2F;minimization steps, with an inexact variant that uses approximate Monte Carlo sampling for discrete point clouds.</li>
<li>results: guarantees convergence for any choice of regularization parameters, and provides non-asymptotic convergence guarantees for approximating Wasserstein barycenters in the free-support&#x2F;grid-free setting.<details>
<summary>Abstract</summary>
We study the computation of doubly regularized Wasserstein barycenters, a recently introduced family of entropic barycenters governed by inner and outer regularization strengths. Previous research has demonstrated that various regularization parameter choices unify several notions of entropy-penalized barycenters while also revealing new ones, including a special case of debiased barycenters. In this paper, we propose and analyze an algorithm for computing doubly regularized Wasserstein barycenters. Our procedure builds on damped Sinkhorn iterations followed by exact maximization/minimization steps and guarantees convergence for any choice of regularization parameters. An inexact variant of our algorithm, implementable using approximate Monte Carlo sampling, offers the first non-asymptotic convergence guarantees for approximating Wasserstein barycenters between discrete point clouds in the free-support/grid-free setting.
</details>
<details>
<summary>摘要</summary>
我们研究 doubly 调整的 Wasserstein 中心的计算，这是最近引入的一家 entropic 中心，受内部和外部调整强度控制。先前的研究已经显示出不同的调整参数选择可以统一多种 entropy-penalized 中心，同时还会揭示出新的一些中心，包括特殊情况下的 debiased 中心。在这篇论文中，我们提出了一个算法来计算 doubly 调整的 Wasserstein 中心。我们的程序基于抑制 Sinkhorn 迭代 followed by exact maximization/minimization 步骤，并给出了任何调整参数的 convergence 保证。一个不精确的variant of our algorithm，可以使用 approximate Monte Carlo sampling 进行 Implement，提供了免 asymptotic convergence guarantees 的 approximating Wasserstein 中心 between discrete point clouds in the free-support/grid-free setting。
</details></li>
</ul>
<hr>
<h2 id="Prot2Text-Multimodal-Protein’s-Function-Generation-with-GNNs-and-Transformers"><a href="#Prot2Text-Multimodal-Protein’s-Function-Generation-with-GNNs-and-Transformers" class="headerlink" title="Prot2Text: Multimodal Protein’s Function Generation with GNNs and Transformers"></a>Prot2Text: Multimodal Protein’s Function Generation with GNNs and Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14367">http://arxiv.org/abs/2307.14367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hadi Abdine, Michail Chatzianastasis, Costas Bouyioukos, Michalis Vazirgiannis</li>
<li>for: 这 paper 的目的是提出一种新的蛋白质功能预测方法，即 Prot2Text，可以在文本化的方式下预测蛋白质的功能。</li>
<li>methods: 该方法使用 Graph Neural Networks(GNNs) 和 Large Language Models(LLMs)，在encoder-decoder框架中结合多种数据类型，包括蛋白质序列、结构和文本注释，以实现蛋白质功能的全面表示。</li>
<li>results: 经验表明，Prot2Text 可以准确预测蛋白质的功能，并且可以生成详细的文本描述。这些结果表明，将 GNNs 和 LLMs 融合在一起可以提供更准确的蛋白质功能预测工具。<details>
<summary>Abstract</summary>
The complex nature of big biological systems pushed some scientists to classify its understanding under the inconceivable missions. Different leveled challenges complicated this task, one of is the prediction of a protein's function. In recent years, significant progress has been made in this field through the development of various machine learning approaches. However, most existing methods formulate the task as a multi-classification problem, i.e assigning predefined labels to proteins. In this work, we propose a novel approach, \textbf{Prot2Text}, which predicts a protein function's in a free text style, moving beyond the conventional binary or categorical classifications. By combining Graph Neural Networks(GNNs) and Large Language Models(LLMs), in an encoder-decoder framework, our model effectively integrates diverse data types including proteins' sequences, structures, and textual annotations. This multimodal approach allows for a holistic representation of proteins' functions, enabling the generation of detailed and accurate descriptions. To evaluate our model, we extracted a multimodal protein dataset from SwissProt, and demonstrate empirically the effectiveness of Prot2Text. These results highlight the transformative impact of multimodal models, specifically the fusion of GNNs and LLMs, empowering researchers with powerful tools for more accurate prediction of proteins' functions. The code, the models and a demo will be publicly released.
</details>
<details>
<summary>摘要</summary>
大型生物系统的复杂性让一些科学家将其理解归类为不可思议任务。不同层次的挑战增加了这个任务的复杂度，其中之一是蛋白质功能预测。在过去的几年中，通过开发多种机器学习方法，有了显著的进步。然而，大多数现有方法将任务定义为多类别问题，即将蛋白质分配预定的标签。在这项工作中，我们提出了一种新的方法，即Prot2Text，它预测蛋白质功能在自由文本风格下，超越了传统的二分或 categorical 分类。我们将GNNs和LLMs组合在encoder-decoder框架中，以整合蛋白质序列、结构和文本注释等多种数据类型。这种多模态方法允许对蛋白质功能的整体表示，从而生成详细和准确的描述。为评估我们的模型，我们从SwissProt中提取了多模态蛋白质数据集，并通过实验证明Prot2Text的效果。这些结果highlight了多模态模型的转变性，特别是GNNs和LLMs的融合，为研究人员提供了具有准确预测蛋白质功能的强大工具。代码、模型和demo将公开发布。
</details></li>
</ul>
<hr>
<h2 id="High-Dimensional-Distributed-Gradient-Descent-with-Arbitrary-Number-of-Byzantine-Attackers"><a href="#High-Dimensional-Distributed-Gradient-Descent-with-Arbitrary-Number-of-Byzantine-Attackers" class="headerlink" title="High Dimensional Distributed Gradient Descent with Arbitrary Number of Byzantine Attackers"></a>High Dimensional Distributed Gradient Descent with Arbitrary Number of Byzantine Attackers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13352">http://arxiv.org/abs/2307.13352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Puning Zhao, Zhiguo Wan</li>
<li>for: 此研究旨在解决robust分布式学习中的Byzantine故障问题，特别是在高维问题上。</li>
<li>methods: 我们提出了一种适合高维问题的新方法，基于直接高维半验证均值估计方法。我们首先 identific一个子空间，然后使用梯度向量上传worker机器进行均值值的估计，并使用auxiliary dataset进行均值值的估计。</li>
<li>results: 我们的 teoretic分析表明，我们的新方法具有最佳峰值统计率。特别是，与前一些工作相比，我们的方法在维度方面具有显著改进。<details>
<summary>Abstract</summary>
Robust distributed learning with Byzantine failures has attracted extensive research interests in recent years. However, most of existing methods suffer from curse of dimensionality, which is increasingly serious with the growing complexity of modern machine learning models. In this paper, we design a new method that is suitable for high dimensional problems, under arbitrary number of Byzantine attackers. The core of our design is a direct high dimensional semi-verified mean estimation method. Our idea is to identify a subspace first. The components of mean value perpendicular to this subspace can be estimated via gradient vectors uploaded from worker machines, while the components within this subspace are estimated using auxiliary dataset. We then use our new method as the aggregator of distributed learning problems. Our theoretical analysis shows that the new method has minimax optimal statistical rates. In particular, the dependence on dimensionality is significantly improved compared with previous works.
</details>
<details>
<summary>摘要</summary>
“强健分布式学习具有拜尼瑞发生的研究吸引了过去几年的广泛关注。然而，大多数现有方法受到维度为因素的问题，这问题随着现代机器学习模型的复杂程度而增加。在这篇论文中，我们设计了适合高维度问题的新方法，可以承受无限多个拜尼瑞攻击者。我们的设计核心在于直接高维度半验证平均值估计法。我们的想法是首先找到一个子空间，然后使用工作机器上上传的梯度 вектор估计横向的部分，而垂直于这个子空间的部分则使用辅助数据集估计。我们随后使用我们的新方法来聚合分布式学习问题。我们的理论分析显示，新方法具有最佳均衡的统计率。尤其是在前一些作品中，它具有更好的维度依赖性。”
</details></li>
</ul>
<hr>
<h2 id="Explainable-Disparity-Compensation-for-Efficient-Fair-Ranking"><a href="#Explainable-Disparity-Compensation-for-Efficient-Fair-Ranking" class="headerlink" title="Explainable Disparity Compensation for Efficient Fair Ranking"></a>Explainable Disparity Compensation for Efficient Fair Ranking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14366">http://arxiv.org/abs/2307.14366</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abraham Gale, Amélie Marian</li>
<li>for: 这篇论文目的是解决决策系统中的不公平问题，即因数据中存在偏见导致不同人群得到的结果不同。</li>
<li>methods: 该论文提出了一种可解释的数据驱动的补偿措施，通过给弱化组群体成员分配加分来减少排名函数中的偏见。</li>
<li>results: 该论文使用实际世界的学生招生和重犯率数据进行验证，并与现有的公平排名算法进行比较，结果表明其补偿措施可以有效地减少不公平性。<details>
<summary>Abstract</summary>
Ranking functions that are used in decision systems often produce disparate results for different populations because of bias in the underlying data. Addressing, and compensating for, these disparate outcomes is a critical problem for fair decision-making. Recent compensatory measures have mostly focused on opaque transformations of the ranking functions to satisfy fairness guarantees or on the use of quotas or set-asides to guarantee a minimum number of positive outcomes to members of underrepresented groups. In this paper we propose easily explainable data-driven compensatory measures for ranking functions. Our measures rely on the generation of bonus points given to members of underrepresented groups to address disparity in the ranking function. The bonus points can be set in advance, and can be combined, allowing for considering the intersections of representations and giving better transparency to stakeholders. We propose efficient sampling-based algorithms to calculate the number of bonus points to minimize disparity. We validate our algorithms using real-world school admissions and recidivism datasets, and compare our results with that of existing fair ranking algorithms.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Feature-Importance-Measurement-based-on-Decision-Tree-Sampling"><a href="#Feature-Importance-Measurement-based-on-Decision-Tree-Sampling" class="headerlink" title="Feature Importance Measurement based on Decision Tree Sampling"></a>Feature Importance Measurement based on Decision Tree Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13333">http://arxiv.org/abs/2307.13333</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tsudalab/dt-sampler">https://github.com/tsudalab/dt-sampler</a></li>
<li>paper_authors: Chao Huang, Diptesh Das, Koji Tsuda</li>
<li>for: 这篇论文是为了提高树式模型中特征重要性分析的可解性而提出的。</li>
<li>methods: 这篇论文使用了SAT基本法来测试特征重要性，具有 fewer parameters 和更高的可解性，适用于实际问题中的分析。</li>
<li>results: 论文的实验结果表明，DT-Sampler 可以提供更高的可解性和稳定性，并且在实际问题中具有更好的性能。<details>
<summary>Abstract</summary>
Random forest is effective for prediction tasks but the randomness of tree generation hinders interpretability in feature importance analysis. To address this, we proposed DT-Sampler, a SAT-based method for measuring feature importance in tree-based model. Our method has fewer parameters than random forest and provides higher interpretability and stability for the analysis in real-world problems. An implementation of DT-Sampler is available at https://github.com/tsudalab/DT-sampler.
</details>
<details>
<summary>摘要</summary>
随机森林是预测任务中有效的，但随机树生成的 Randomness 会降低特征重要性分析的可解性。为了解决这个问题，我们提出了 DT-Sampler，一种基于 SAT 的方法来测量树型模型中特征重要性。我们的方法有 fewer parameters than random forest，并提供更高的可解性和稳定性 для实际问题的分析。DT-Sampler 的实现可以在 GitHub 上找到：https://github.com/tsudalab/DT-sampler。
</details></li>
</ul>
<hr>
<h2 id="The-Optimal-Approximation-Factors-in-Misspecified-Off-Policy-Value-Function-Estimation"><a href="#The-Optimal-Approximation-Factors-in-Misspecified-Off-Policy-Value-Function-Estimation" class="headerlink" title="The Optimal Approximation Factors in Misspecified Off-Policy Value Function Estimation"></a>The Optimal Approximation Factors in Misspecified Off-Policy Value Function Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13332">http://arxiv.org/abs/2307.13332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philip Amortila, Nan Jiang, Csaba Szepesvári</li>
<li>for: 这篇论文主要关注于 linear off-policy value function estimation 中的函数近似错误问题。</li>
<li>methods: 这篇论文使用了多种设定，包括Weighted $L_2$-norm、$L_\infty$ norm、state aliasing 和 full vs. partial coverage of the state space，来研究函数近似错误的优化因子。</li>
<li>results: 这篇论文确定了不同设定下的优化因子，并发现了两个实例特定的因子（在 $L_2(\mu)$  нор中）和一个常数（在 $L_\infty$  norm中），它们控制了在 misspecification 下的 off-policy 评估困难度。<details>
<summary>Abstract</summary>
Theoretical guarantees in reinforcement learning (RL) are known to suffer multiplicative blow-up factors with respect to the misspecification error of function approximation. Yet, the nature of such \emph{approximation factors} -- especially their optimal form in a given learning problem -- is poorly understood. In this paper we study this question in linear off-policy value function estimation, where many open questions remain. We study the approximation factor in a broad spectrum of settings, such as with the weighted $L_2$-norm (where the weighting is the offline state distribution), the $L_\infty$ norm, the presence vs. absence of state aliasing, and full vs. partial coverage of the state space. We establish the optimal asymptotic approximation factors (up to constants) for all of these settings. In particular, our bounds identify two instance-dependent factors for the $L_2(\mu)$ norm and only one for the $L_\infty$ norm, which are shown to dictate the hardness of off-policy evaluation under misspecification.
</details>
<details>
<summary>摘要</summary>
理论保证在强化学习（RL）中知道会受到函数 aproximation 错误的多项式增长。然而，这些 \emph{approximation factor}  --特别是在某个学习问题中的优化形式-- 还不够了解。在这篇论文中，我们研究这个问题，具体来说是在线性off-policy值函数估计中。我们在多种设置下研究approximation factor，包括使用权重$L_2$ norm（其权重是在线上的状态分布）、$L_\infty$ norm、状态别名存在或不存在、完整 vs.  partial coverage的状态空间。我们Establish了所有这些设置的优化 asymptotic approximation factor（几乎 constants）。特别是，我们的 bound 透露了 $L_2(\mu)$ norm 中的两个实例特定因子，以及 $L_\infty$ norm 中的一个因子，它们在不正确的函数approximation下控制了强化评估的困难程度。
</details></li>
</ul>
<hr>
<h2 id="Unleash-the-Power-of-Context-Enhancing-Large-Scale-Recommender-Systems-with-Context-Based-Prediction-Models"><a href="#Unleash-the-Power-of-Context-Enhancing-Large-Scale-Recommender-Systems-with-Context-Based-Prediction-Models" class="headerlink" title="Unleash the Power of Context: Enhancing Large-Scale Recommender Systems with Context-Based Prediction Models"></a>Unleash the Power of Context: Enhancing Large-Scale Recommender Systems with Context-Based Prediction Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01231">http://arxiv.org/abs/2308.01231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Hartman, Assaf Klein, Davorin Kopič, Natalia Silberstein</li>
<li>for: 这个研究旨在提出Context-Based Prediction Model，一种基于用户和上下文特征的预测模型，不考虑物品特征。</li>
<li>methods: 该模型采用用户和上下文特征进行预测，并在CTR预测模型中 incorporate其预测结果作为特征。</li>
<li>results: 实验表明，这种方法可以在大规模商业推荐系统中提高业务效果，而且对服务成本产生较少影响。<details>
<summary>Abstract</summary>
In this work, we introduce the notion of Context-Based Prediction Models. A Context-Based Prediction Model determines the probability of a user's action (such as a click or a conversion) solely by relying on user and contextual features, without considering any specific features of the item itself. We have identified numerous valuable applications for this modeling approach, including training an auxiliary context-based model to estimate click probability and incorporating its prediction as a feature in CTR prediction models. Our experiments indicate that this enhancement brings significant improvements in offline and online business metrics while having minimal impact on the cost of serving. Overall, our work offers a simple and scalable, yet powerful approach for enhancing the performance of large-scale commercial recommender systems, with broad implications for the field of personalized recommendations.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们介绍了含义基于预测模型。这种预测模型根据用户和上下文特征决定用户行为（如点击或购买），不考虑物品特定的特征。我们已经 indentified numerous valuable applications for this modeling approach，包括培养辅助context-based模型来估计点击概率，并将其预测作为ctr预测模型的特征。我们的实验表明，这种提升带来了显著的在线和OFFLINE商业指标的改善，而且对服务成本的影响很小。总的来说，我们的工作提供了一种简单、可扩展且强大的方法，用于提高大规模的商业推荐系统的性能，对个性化推荐领域产生了广泛的影响。
</details></li>
</ul>
<hr>
<h2 id="QuIP-2-Bit-Quantization-of-Large-Language-Models-With-Guarantees"><a href="#QuIP-2-Bit-Quantization-of-Large-Language-Models-With-Guarantees" class="headerlink" title="QuIP: 2-Bit Quantization of Large Language Models With Guarantees"></a>QuIP: 2-Bit Quantization of Large Language Models With Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13304">http://arxiv.org/abs/2307.13304</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jerry-chee/quip">https://github.com/jerry-chee/quip</a></li>
<li>paper_authors: Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, Christopher De Sa</li>
<li>for: 这个论文研究大型自然语言模型（LLM）后期参数量化。</li>
<li>methods: 我们提出了一种新的量化方法，即异幂量化处理（QuIP），它基于论文的观察，即量化受益于异幂 веса和希尔бер特矩阵，即需要准确圆拟的重量和希尔бер特矩阵与坐标轴不对齐。QuIP包括两步：（1）适应圆拟过程，最小化一个quadratic proxy目标函数;（2）高效的预处理和后处理，使得重量和希尔бер特矩阵异幂。</li>
<li>results: 我们通过实验发现，我们的异幂预处理可以提高一些现有的量化算法，并且使用只有两个位数的量化方法生成可靠的结果。我们的代码可以在<a target="_blank" rel="noopener" href="https://github.com/jerry-chee/QuIP%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jerry-chee/QuIP上找到。</a><details>
<summary>Abstract</summary>
This work studies post-training parameter quantization in large language models (LLMs). We introduce quantization with incoherence processing (QuIP), a new method based on the insight that quantization benefits from incoherent weight and Hessian matrices, i.e., from the weights and the directions in which it is important to round them accurately being unaligned with the coordinate axes. QuIP consists of two steps: (1) an adaptive rounding procedure minimizing a quadratic proxy objective; (2) efficient pre- and post-processing that ensures weight and Hessian incoherence via multiplication by random orthogonal matrices. We complement QuIP with the first theoretical analysis for an LLM-scale quantization algorithm, and show that our theory also applies to an existing method, OPTQ. Empirically, we find that our incoherence preprocessing improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight. Our code can be found at https://github.com/jerry-chee/QuIP .
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Adaptive rounding procedure that minimizes a quadratic proxy objective.2. Efficient pre- and post-processing that ensures weight and Hessian incoherence through multiplication by random orthogonal matrices.We also provide the first theoretical analysis for an LLM-scale quantization algorithm and show that our theory applies to an existing method, OPTQ. Empirically, we find that our incoherence preprocessing improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight. Our code can be found at <a target="_blank" rel="noopener" href="https://github.com/jerry-chee/QuIP">https://github.com/jerry-chee/QuIP</a>.</details></li>
</ol>
<hr>
<h2 id="Word-Sense-Disambiguation-as-a-Game-of-Neurosymbolic-Darts"><a href="#Word-Sense-Disambiguation-as-a-Game-of-Neurosymbolic-Darts" class="headerlink" title="Word Sense Disambiguation as a Game of Neurosymbolic Darts"></a>Word Sense Disambiguation as a Game of Neurosymbolic Darts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16663">http://arxiv.org/abs/2307.16663</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiansi Dong, Rafet Sifa</li>
<li>for: 本研究旨在提出一种新的神经符号方法，以提高 Word Sense Disambiguation (WSD) 任务的性能。</li>
<li>methods: 该方法基于一种嵌入式的 Configuration of Nested Balls (CNB) 模型，通过将单词嵌入与符号关系编码为包含关系的包囊中的位置来实现。</li>
<li>results: 在六组测试数据集上，该方法的实验结果在 90.1% 到 100.0% 之间，较前者的深度学习方法提高了 WSD 任务的性能。<details>
<summary>Abstract</summary>
Word Sense Disambiguation (WSD) is one of the hardest tasks in natural language understanding and knowledge engineering. The glass ceiling of 80% F1 score is recently achieved through supervised deep-learning, enriched by a variety of knowledge graphs. Here, we propose a novel neurosymbolic methodology that is able to push the F1 score above 90%. The core of our methodology is a neurosymbolic sense embedding, in terms of a configuration of nested balls in n-dimensional space. The centre point of a ball well-preserves word embedding, which partially fix the locations of balls. Inclusion relations among balls precisely encode symbolic hypernym relations among senses, and enable simple logic deduction among sense embeddings, which cannot be realised before. We trained a Transformer to learn the mapping from a contextualized word embedding to its sense ball embedding, just like playing the game of darts (a game of shooting darts into a dartboard). A series of experiments are conducted by utilizing pre-training n-ball embeddings, which have the coverage of around 70% training data and 75% testing data in the benchmark WSD corpus. The F1 scores in experiments range from 90.1% to 100.0% in all six groups of test data-sets (each group has 4 testing data with different sizes of n-ball embeddings). Our novel neurosymbolic methodology has the potential to break the ceiling of deep-learning approaches for WSD. Limitations and extensions of our current works are listed.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Modify-Training-Directions-in-Function-Space-to-Reduce-Generalization-Error"><a href="#Modify-Training-Directions-in-Function-Space-to-Reduce-Generalization-Error" class="headerlink" title="Modify Training Directions in Function Space to Reduce Generalization Error"></a>Modify Training Directions in Function Space to Reduce Generalization Error</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13290">http://arxiv.org/abs/2307.13290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Yu, Wenlian Lu, Boyu Chen</li>
<li>for: 本文研究了一种基于神经网络函数空间的修改自然梯度下降方法的理论分析。</li>
<li>methods: 本文使用了 eigendecompositions 和统计学理论来Derive the generalization error of the learned neural network function,并提出了一种基于 eigenspace 的权衡标度方法来降低总化预测误差。</li>
<li>results: 本文通过 theoretically 分析和数据示例证明了，修改神经网络函数空间的培训方向可以降低总化预测误差。此外，本文还能够解释许多现有的泛化提升方法的理论基础。<details>
<summary>Abstract</summary>
We propose theoretical analyses of a modified natural gradient descent method in the neural network function space based on the eigendecompositions of neural tangent kernel and Fisher information matrix. We firstly present analytical expression for the function learned by this modified natural gradient under the assumptions of Gaussian distribution and infinite width limit. Thus, we explicitly derive the generalization error of the learned neural network function using theoretical methods from eigendecomposition and statistics theory. By decomposing of the total generalization error attributed to different eigenspace of the kernel in function space, we propose a criterion for balancing the errors stemming from training set and the distribution discrepancy between the training set and the true data. Through this approach, we establish that modifying the training direction of the neural network in function space leads to a reduction in the total generalization error. Furthermore, We demonstrate that this theoretical framework is capable to explain many existing results of generalization enhancing methods. These theoretical results are also illustrated by numerical examples on synthetic data.
</details>
<details>
<summary>摘要</summary>
我们提出了一种基于神经网络函数空间的修改自然梯度下降方法的理论分析。我们首先提出了在假设 Gaussian 分布和无限宽限下的分析表达，从而显式地计算出学习的神经网络函数中的总泛化误差。通过对函数空间的各个 eigenspace 的总泛化误差进行分解，我们提出了一个均衡误差的准则。我们还证明了，通过在函数空间中修改训练方向，可以降低总泛化误差。此外，我们还证明了这种理论框架能够解释许多现有的泛化提升方法的结果。这些理论结果也通过synthetic数据的数值示例进行了证明。
</details></li>
</ul>
<hr>
<h2 id="Curvature-based-Transformer-for-Molecular-Property-Prediction"><a href="#Curvature-based-Transformer-for-Molecular-Property-Prediction" class="headerlink" title="Curvature-based Transformer for Molecular Property Prediction"></a>Curvature-based Transformer for Molecular Property Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13275">http://arxiv.org/abs/2307.13275</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yili Chen, Zhengyu Li, Zheng Wan, Hui Yu, Xian Wei</li>
<li>For: 该研究旨在提高基于图gram neural网络模型的分子属性预测能力，以提高人工智能化药物设计的效果。* Methods: 该研究提出了 Curvature-based Transformer，通过在图грам数据中添加 ricci 曲率信息作为节点特征来增强模型对结构信息的抽象能力。* Results: 对化学分子数据集PCQM4M-LST、MoleculeNet进行了实验，并与Uni-Mol、Graphormer等模型进行比较，结果表明，该方法可以达到领域内最佳效果。此外， ricci 曲率还能够反映结构和功能关系，描述图грам分子数据的局部几何结构。<details>
<summary>Abstract</summary>
The prediction of molecular properties is one of the most important and challenging tasks in the field of artificial intelligence-based drug design. Among the current mainstream methods, the most commonly used feature representation for training DNN models is based on SMILES and molecular graphs, although these methods are concise and effective, they also limit the ability to capture spatial information. In this work, we propose Curvature-based Transformer to improve the ability of Graph Transformer neural network models to extract structural information on molecular graph data by introducing Discretization of Ricci Curvature. To embed the curvature in the model, we add the curvature information of the graph as positional Encoding to the node features during the attention-score calculation. This method can introduce curvature information from graph data without changing the original network architecture, and it has the potential to be extended to other models. We performed experiments on chemical molecular datasets including PCQM4M-LST, MoleculeNet and compared with models such as Uni-Mol, Graphormer, and the results show that this method can achieve the state-of-the-art results. It is proved that the discretized Ricci curvature also reflects the structural and functional relationship while describing the local geometry of the graph molecular data.
</details>
<details>
<summary>摘要</summary>
“分子质量预测是人工智能基于药物设计的一个最重要和挑战性任务。现今主流方法中，最常用的特征表示方法是基于SMILES和分子图，although这些方法简洁有效，但它们也限制了捕捉空间信息的能力。在这项工作中，我们提出了Curvature-based Transformer来提高Graph Transformer神经网络模型在分子图数据上提取结构信息的能力，通过引入Discretization of Ricci Curvature。为了将曲率信息 embed到模型中，我们在计算注意力分数时将节点特征中的曲率信息作为 позициональ编码。这种方法可以在不改变原始网络架构的情况下，将曲率信息从分子图数据中引入到模型中，并且它有可能可以扩展到其他模型。我们在化学分子数据集PCQM4M-LST、MoleculeNet上进行了实验，并与Uni-Mol、Graphormer等模型进行比较，结果显示，这种方法可以达到领先的结果。这也证明了离散的 Ricci 曲率不仅描述了分子图数据的本地几何结构，而且还反映了结构和功能关系。”
</details></li>
</ul>
<hr>
<h2 id="Unbiased-Weight-Maximization"><a href="#Unbiased-Weight-Maximization" class="headerlink" title="Unbiased Weight Maximization"></a>Unbiased Weight Maximization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13270">http://arxiv.org/abs/2307.13270</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Chung<br>for:这篇论文的目的是提出一种生物学上可能的人工神经网络（ANN）训练方法，即将每个单元视为一个随机反馈学习（RL）代理，从而将网络视为一群代理。methods:该训练方法使用的是REINFORCE算法，一种本地学习规则，它是基于全局奖励信号的抑制学习。然而，这种学习方法通常很慢，并且随着网络大小增加，它的效率会降低，因为不具有有效的结构归因分配。为解决这个问题，提出了质量最大化方法，它将每个隐藏单元的奖励信号更改为该单元的出going权重的 нор，从而让每个隐藏单元可以最大化其出going权重的norm。results:在这篇研究报告中，我们分析了质量最大化方法的理论性质，并提出了一种变体，即偏向最大化方法。这种新方法提供了一种偏向学习规则，它可以增加学习速度和改善最终性能。值得注意的是，到我们所知，这是第一种可以快速学习，并且与网络单元数量成直比的人工神经网络训练方法。<details>
<summary>Abstract</summary>
A biologically plausible method for training an Artificial Neural Network (ANN) involves treating each unit as a stochastic Reinforcement Learning (RL) agent, thereby considering the network as a team of agents. Consequently, all units can learn via REINFORCE, a local learning rule modulated by a global reward signal, which aligns more closely with biologically observed forms of synaptic plasticity. Nevertheless, this learning method is often slow and scales poorly with network size due to inefficient structural credit assignment, since a single reward signal is broadcast to all units without considering individual contributions. Weight Maximization, a proposed solution, replaces a unit's reward signal with the norm of its outgoing weight, thereby allowing each hidden unit to maximize the norm of the outgoing weight instead of the global reward signal. In this research report, we analyze the theoretical properties of Weight Maximization and propose a variant, Unbiased Weight Maximization. This new approach provides an unbiased learning rule that increases learning speed and improves asymptotic performance. Notably, to our knowledge, this is the first learning rule for a network of Bernoulli-logistic units that is unbiased and scales well with the number of network's units in terms of learning speed.
</details>
<details>
<summary>摘要</summary>
一种生物学可能性的方法 для训练人工神经网络（ANN）是将每个单元视为一个随机奖励学习（RL）代理，从而考虑神经网络为一支代理队伍。因此，所有单元都可以通过REINFORCE本地学习规则，该规则由全局奖励信号调整，更加匹配生物观察到的 synaptic plasticity 形式。然而，这种学习方法通常慢并且与网络大小不好扩展，由于不效的结构归因分配。Weight Maximization，一种提议的解决方案，将单元的奖励信号替换为出向量的 нор，以此来使每个隐藏单元可以最大化出向量的 norm 而不是全局奖励信号。在这份研究报告中，我们分析Weight Maximization的理论属性并提出一种变体，即不偏向的Weight Maximization。这种新的学习规则提供了一种不偏向的学习规则，提高学习速度和长期性表现。值得注意的是，到我们所知，这是一种可以快速学习和与神经网络单元数量成正比的学习规则。
</details></li>
</ul>
<hr>
<h2 id="Federated-K-Means-Clustering-via-Dual-Decomposition-based-Distributed-Optimization"><a href="#Federated-K-Means-Clustering-via-Dual-Decomposition-based-Distributed-Optimization" class="headerlink" title="Federated K-Means Clustering via Dual Decomposition-based Distributed Optimization"></a>Federated K-Means Clustering via Dual Decomposition-based Distributed Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13267">http://arxiv.org/abs/2307.13267</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vassilios Yfantis, Achim Wagner, Martin Ruskowski</li>
<li>for: 本文旨在探讨分布式优化在机器学习中的应用，以保持隐私或提高计算效率为动机。</li>
<li>methods: 本文使用分布式算法来训练全球模型，其中每个节点只有访问自己的数据。此外，由于数据量的增加，大规模机器学习问题需要分布式训练。</li>
<li>results: 本文使用各种算法来评估分布式训练的性能，包括梯度下降法、Bundle Trust方法和准降次射线法。然而，分布式程序的整数放松问题受到强烈的限制，但是该方法可能会在未来实现高效的解决方案，在中央和分布式设置中。<details>
<summary>Abstract</summary>
The use of distributed optimization in machine learning can be motivated either by the resulting preservation of privacy or the increase in computational efficiency. On the one hand, training data might be stored across multiple devices. Training a global model within a network where each node only has access to its confidential data requires the use of distributed algorithms. Even if the data is not confidential, sharing it might be prohibitive due to bandwidth limitations. On the other hand, the ever-increasing amount of available data leads to large-scale machine learning problems. By splitting the training process across multiple nodes its efficiency can be significantly increased. This paper aims to demonstrate how dual decomposition can be applied for distributed training of $ K $-means clustering problems. After an overview of distributed and federated machine learning, the mixed-integer quadratically constrained programming-based formulation of the $ K $-means clustering training problem is presented. The training can be performed in a distributed manner by splitting the data across different nodes and linking these nodes through consensus constraints. Finally, the performance of the subgradient method, the bundle trust method, and the quasi-Newton dual ascent algorithm are evaluated on a set of benchmark problems. While the mixed-integer programming-based formulation of the clustering problems suffers from weak integer relaxations, the presented approach can potentially be used to enable an efficient solution in the future, both in a central and distributed setting.
</details>
<details>
<summary>摘要</summary>
使用分布式优化在机器学习中可以受到保持隐私或提高计算效率的两种动机。一方面，训练数据可能会被存储在多个设备上。在一个网络中，每个节点只有访问自己的Confidential数据的训练全球模型需要使用分布式算法。即使数据不是Confidential，因为带宽限制而共享它们可能是不可能的。另一方面，可用数据的量在不断增长，导致大规模机器学习问题。通过将训练过程分布在多个节点上，可以显著提高效率。本文旨在说明如何使用分布式优化解决分布式训练$ K $-mean clustering问题。首先是分布式和联邦机器学习的概述，然后是基于杂 integer quadratic programming 的 $ K $-mean clustering训练问题的混合数学表述。在这个表述中，数据可以被分布在不同的节点上，并通过协调约束相互连接。最后，在一组 benchmark 问题上评估了批量梯度法、杂 bundle trust 法和 quasi-Newton dual ascent 算法的性能。虽然分布式 programming 基于的 clustering问题的 relaxation 强度弱，但这种方法可能可以在未来在中心和分布式设置中实现高效的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Federated-Split-Learning-with-Only-Positive-Labels-for-resource-constrained-IoT-environment"><a href="#Federated-Split-Learning-with-Only-Positive-Labels-for-resource-constrained-IoT-environment" class="headerlink" title="Federated Split Learning with Only Positive Labels for resource-constrained IoT environment"></a>Federated Split Learning with Only Positive Labels for resource-constrained IoT environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13266">http://arxiv.org/abs/2307.13266</a></li>
<li>repo_url: None</li>
<li>paper_authors: Praveen Joshi, Chandra Thapa, Mohammed Hasanuzzaman, Ted Scully, Haithem Afli</li>
<li>for: 这则研究的目的是提出一个叫做 splitfed learning with positive labels (SFPL) 的方法，用于在资源有限的 IoT 设备上执行分布式合作机器学习 (DCML)，以提高资料隐私和模型训练效率。</li>
<li>methods: SFPL 使用了随机排序函数将客户端获取的抛裂数据Random shuffling 以及在推断阶段使用本地批量normalization 来提高客户端模型的精度。</li>
<li>results: SFPL 比 SFL 提高了51.54 和 32.57 倍的误差率，对于 CIFAR-100 和 CIFAR-10  dataset 分别进行了评估，并且获得了更好的结果。<details>
<summary>Abstract</summary>
Distributed collaborative machine learning (DCML) is a promising method in the Internet of Things (IoT) domain for training deep learning models, as data is distributed across multiple devices. A key advantage of this approach is that it improves data privacy by removing the necessity for the centralized aggregation of raw data but also empowers IoT devices with low computational power. Among various techniques in a DCML framework, federated split learning, known as splitfed learning (SFL), is the most suitable for efficient training and testing when devices have limited computational capabilities. Nevertheless, when resource-constrained IoT devices have only positive labeled data, multiclass classification deep learning models in SFL fail to converge or provide suboptimal results. To overcome these challenges, we propose splitfed learning with positive labels (SFPL). SFPL applies a random shuffling function to the smashed data received from clients before supplying it to the server for model training. Additionally, SFPL incorporates the local batch normalization for the client-side model portion during the inference phase. Our results demonstrate that SFPL outperforms SFL: (i) by factors of 51.54 and 32.57 for ResNet-56 and ResNet-32, respectively, with the CIFAR-100 dataset, and (ii) by factors of 9.23 and 8.52 for ResNet-32 and ResNet-8, respectively, with CIFAR-10 dataset. Overall, this investigation underscores the efficacy of the proposed SFPL framework in DCML.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传输给定文本到简化中文。<</SYS>>分布式合作机器学习（DCML）是互联网物联网（IoT）领域的一种有前途的方法，用于训练深度学习模型，因为数据分布在多个设备上。DCML的一个优点是改善数据隐私，从而消除中央化数据汇集的必要性，同时赋能低功耗的IoT设备。在DCML框架中， federated split learning（SFL）是最适合高效地训练和测试，当设备有限的计算能力时。然而，当IoT设备有限的计算能力时，SFL中的多类分类深度学习模型会失败或提供低优的结果。为了解决这些挑战，我们提出了 splitfed learning with positive labels（SFPL）。SFPL在客户端收到服务器发送的数据后，随机混淆该数据，然后将其传递给服务器进行模型训练。此外，SFPL在推理阶段在客户端模型部分中加入本地批处理。我们的结果表明，SFPL比SFL：（i）在CIFAR-100数据集上，ResNet-56和ResNet-32分别提高了51.54倍和32.57倍的性能；（ii）在CIFAR-10数据集上，ResNet-32和ResNet-8分别提高了9.23倍和8.52倍的性能。总的来说，本研究证明SFPL框架在DCML中的有效性。
</details></li>
</ul>
<hr>
<h2 id="Structural-Credit-Assignment-with-Coordinated-Exploration"><a href="#Structural-Credit-Assignment-with-Coordinated-Exploration" class="headerlink" title="Structural Credit Assignment with Coordinated Exploration"></a>Structural Credit Assignment with Coordinated Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13256">http://arxiv.org/abs/2307.13256</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Chung</li>
<li>for: 论文旨在提出一种生物学上有效的人工神经网络（ANN）训练方法，即将每个单元当作一个随机激励学习（RL）代理，从而考虑网络为一组代理。</li>
<li>methods: 该方法使用REINFORCE本地学习规则，即在全球奖励信号的修饰下，使每个单元进行学习。然而，这种学习方法的启发速度较慢，并且不能够适应网络的大小。这是由两种因素引起的：首先，所有单元独立地探索网络，其次，网络中的所有单元都用同一个奖励来评估其行为。</li>
<li>results: 作者提出了一种改进结构信用分配的方法，即使用博尔茨曼机或循环网络进行协调探索。实验结果表明，协调探索可以大幅提高多个随机和离散单元基于REINFORCE的训练速度，甚至超过STE归散梯度渐进法。<details>
<summary>Abstract</summary>
A biologically plausible method for training an Artificial Neural Network (ANN) involves treating each unit as a stochastic Reinforcement Learning (RL) agent, thereby considering the network as a team of agents. Consequently, all units can learn via REINFORCE, a local learning rule modulated by a global reward signal, which aligns more closely with biologically observed forms of synaptic plasticity. However, this learning method tends to be slow and does not scale well with the size of the network. This inefficiency arises from two factors impeding effective structural credit assignment: (i) all units independently explore the network, and (ii) a single reward is used to evaluate the actions of all units. Accordingly, methods aimed at improving structural credit assignment can generally be classified into two categories. The first category includes algorithms that enable coordinated exploration among units, such as MAP propagation. The second category encompasses algorithms that compute a more specific reward signal for each unit within the network, like Weight Maximization and its variants. In this research report, our focus is on the first category. We propose the use of Boltzmann machines or a recurrent network for coordinated exploration. We show that the negative phase, which is typically necessary to train Boltzmann machines, can be removed. The resulting learning rules are similar to the reward-modulated Hebbian learning rule. Experimental results demonstrate that coordinated exploration significantly exceeds independent exploration in training speed for multiple stochastic and discrete units based on REINFORCE, even surpassing straight-through estimator (STE) backpropagation.
</details>
<details>
<summary>摘要</summary>
一种生物学可能性的人工神经网络（ANN）训练方法是将每个单元视为随机奖励学习（RL）代理，从而考虑网络为一支代理队伍。因此，所有单元都可以通过REINFORCE本地学习规则，该规则由全局奖励信号修饰，更加匹配生物观察到的神经元某些塑性变化。然而，这种学习方法通常慢和网络大小不好扩展。这种慢速是由两个因素引起的：（i）所有单元独立探索网络，以及（ii）网络中所有单元的行为都由单一奖励评价。因此，可以分为两类方法来改进结构准确评价：（i）使单元之间协调探索的算法，如MAP协调传播；（ii）计算网络中每个单元的更加特定奖励信号，如重量最大化和其变种。在这份研究报告中，我们关注第一类方法。我们提议使用博尔ツ曼机或回归网络来实现协调探索。我们发现，通常需要训练博尔ツ曼机的负阶段可以消除。结果的学习规则与奖励修饰的麦克斯韦学习规则相似。实验结果表明，通过协调探索，独立探索的训练速度在多个随机离散单元基于REINFORCE上明显超过直通透归推导（STE）反射归推。
</details></li>
</ul>
<hr>
<h2 id="RoSAS-Deep-Semi-Supervised-Anomaly-Detection-with-Contamination-Resilient-Continuous-Supervision"><a href="#RoSAS-Deep-Semi-Supervised-Anomaly-Detection-with-Contamination-Resilient-Continuous-Supervision" class="headerlink" title="RoSAS: Deep Semi-Supervised Anomaly Detection with Contamination-Resilient Continuous Supervision"></a>RoSAS: Deep Semi-Supervised Anomaly Detection with Contamination-Resilient Continuous Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13239">http://arxiv.org/abs/2307.13239</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xuhongzuo/rosas">https://github.com/xuhongzuo/rosas</a></li>
<li>paper_authors: Hongzuo Xu, Yijie Wang, Guansong Pang, Songlei Jian, Ning Liu, Yongjun Wang</li>
<li>for: 本研究目的是提出一种新的半监督异常检测方法，以适应异常检测中的异常污染和不同异常检测环境。</li>
<li>methods: 本方法使用了一种新的杂化积分方法，通过Diffuse the abnormality of labeled anomalies，创建了新的数据样本，并使用了一种特征学习基于的目标函数来强化网络的 robustness。</li>
<li>results: 对11个实际世界数据集进行了广泛的实验，结果显示，our approach在AUC-PR指标上比前STATE-OF-THE-ART竞争对手提高20%-30%，并在不同的异常检测环境下表现更加稳定和高效。<details>
<summary>Abstract</summary>
Semi-supervised anomaly detection methods leverage a few anomaly examples to yield drastically improved performance compared to unsupervised models. However, they still suffer from two limitations: 1) unlabeled anomalies (i.e., anomaly contamination) may mislead the learning process when all the unlabeled data are employed as inliers for model training; 2) only discrete supervision information (such as binary or ordinal data labels) is exploited, which leads to suboptimal learning of anomaly scores that essentially take on a continuous distribution. Therefore, this paper proposes a novel semi-supervised anomaly detection method, which devises \textit{contamination-resilient continuous supervisory signals}. Specifically, we propose a mass interpolation method to diffuse the abnormality of labeled anomalies, thereby creating new data samples labeled with continuous abnormal degrees. Meanwhile, the contaminated area can be covered by new data samples generated via combinations of data with correct labels. A feature learning-based objective is added to serve as an optimization constraint to regularize the network and further enhance the robustness w.r.t. anomaly contamination. Extensive experiments on 11 real-world datasets show that our approach significantly outperforms state-of-the-art competitors by 20%-30% in AUC-PR and obtains more robust and superior performance in settings with different anomaly contamination levels and varying numbers of labeled anomalies. The source code is available at https://github.com/xuhongzuo/rosas/.
</details>
<details>
<summary>摘要</summary>
semi-supervised异常检测方法可以利用一些异常示例来获得显著提高的性能，比如无监督模型。然而，它们仍然受到两个限制：1）无标签异常（即异常污染）可能会在模型训练时对学习过程产生误导; 2）只利用数字化超级视图（如二进制或ORDinal数据标签），导致异常分数的学习变得不优化。因此，这篇论文提出了一种新的 semi-supervised异常检测方法，即“污染耐受连续超级指示信号”。具体来说，我们提出了一种混合 interpolate方法，将标注异常的异常性散布到新的数据样本中，从而创造了连续异常度标签。同时，污染区域可以通过新生成的数据样本，其中包含正确标签的数据，得到覆盖。我们还添加了一个特征学习基于的目标函数，以便在污染异常情况下进一步增强网络的稳定性。我们在11个实际 dataset上进行了广泛的实验，结果表明，我们的方法在 AUC-PR 方面比state-of-the-art竞争对手提高20%-30%，并且在不同的异常污染水平和异常数量的不同设置下表现出更加稳定和优秀的性能。代码可以在 https://github.com/xuhongzuo/rosas/ 获取。
</details></li>
</ul>
<hr>
<h2 id="Audio-aware-Query-enhanced-Transformer-for-Audio-Visual-Segmentation"><a href="#Audio-aware-Query-enhanced-Transformer-for-Audio-Visual-Segmentation" class="headerlink" title="Audio-aware Query-enhanced Transformer for Audio-Visual Segmentation"></a>Audio-aware Query-enhanced Transformer for Audio-Visual Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13236">http://arxiv.org/abs/2307.13236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinxiang Liu, Chen Ju, Chaofan Ma, Yanfeng Wang, Yu Wang, Ya Zhang</li>
<li>for: Audio-visual segmentation (AVS) task, specifically to segment sounding objects in video frames using audio cues.</li>
<li>methods: Introduces a multimodal transformer architecture that enables deep fusion and aggregation of audio-visual features, plus an audio-aware query-enhanced transformer decoder that explicitly focuses on the segmentation of pinpointed sounding objects based on audio signals.</li>
<li>results: Outperforms previous methods and demonstrates better generalization ability in multi-sound and open-set scenarios.Here’s the Chinese version of the three key information points:</li>
<li>for: 专门用于音频类型的视觉分割任务，即使用音频讯号来分类视觉框架中的声音物体。</li>
<li>methods: 引入了一个多模式转换器架构，允许深度融合和总结多个音频和视觉特征，以及一个专注于音频讯号的查询增强转换器解码器，帮助模型更好地对声音物体进行分类。</li>
<li>results: 比前一代方法高效，并在多音和开集条件下表现出更好的扩展性。<details>
<summary>Abstract</summary>
The goal of the audio-visual segmentation (AVS) task is to segment the sounding objects in the video frames using audio cues. However, current fusion-based methods have the performance limitations due to the small receptive field of convolution and inadequate fusion of audio-visual features. To overcome these issues, we propose a novel \textbf{Au}dio-aware query-enhanced \textbf{TR}ansformer (AuTR) to tackle the task. Unlike existing methods, our approach introduces a multimodal transformer architecture that enables deep fusion and aggregation of audio-visual features. Furthermore, we devise an audio-aware query-enhanced transformer decoder that explicitly helps the model focus on the segmentation of the pinpointed sounding objects based on audio signals, while disregarding silent yet salient objects. Experimental results show that our method outperforms previous methods and demonstrates better generalization ability in multi-sound and open-set scenarios.
</details>
<details>
<summary>摘要</summary>
系统的听音视频分割（AVS）任务的目标是使用听音信号来分割视频帧中的声音对象。然而，现有的融合方法受到小感知场和不充分融合听音视频特征的限制。为了解决这些问题，我们提出了一种新的听音感知扩展的transformer（AuTR）来解决这个任务。与现有方法不同，我们的方法使用多Modal transformer架构，允许深度融合和听音视频特征的汇集。此外，我们还开发了听音感知扩展transformer解码器，可以让模型在听音信号的指引下进行声音对象的分割，而忽略沉寂却突出的对象。实验结果表明，我们的方法在多音和开放集成方面的性能比前方法更高，并且在多音和开放集成方面表现更好。
</details></li>
</ul>
<hr>
<h2 id="Spectral-DP-Differentially-Private-Deep-Learning-through-Spectral-Perturbation-and-Filtering"><a href="#Spectral-DP-Differentially-Private-Deep-Learning-through-Spectral-Perturbation-and-Filtering" class="headerlink" title="Spectral-DP: Differentially Private Deep Learning through Spectral Perturbation and Filtering"></a>Spectral-DP: Differentially Private Deep Learning through Spectral Perturbation and Filtering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13231">http://arxiv.org/abs/2307.13231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ce Feng, Nuo Xu, Wujie Wen, Parv Venkitasubramaniam, Caiwen Ding</li>
<li>for: 本研究旨在提出一种新的干扰梯度学习方法，以实现在深度学习算法中保护个人隐私。</li>
<li>methods: 本方法combines gradient perturbation在spectral domain with spectral filtering以实现 desired privacy guarantee，并且采用了块环形based spatial restructuring来提高fully connected层的性能。</li>
<li>results: 通过对标准 benchmark datasets进行广泛的实验，研究者发现spectral-DP deep learning方法在训练从scratch和transfer learning Setting中具有更好的性能，并且提供了实现spectral-DP deep learning的指南。<details>
<summary>Abstract</summary>
Differential privacy is a widely accepted measure of privacy in the context of deep learning algorithms, and achieving it relies on a noisy training approach known as differentially private stochastic gradient descent (DP-SGD). DP-SGD requires direct noise addition to every gradient in a dense neural network, the privacy is achieved at a significant utility cost. In this work, we present Spectral-DP, a new differentially private learning approach which combines gradient perturbation in the spectral domain with spectral filtering to achieve a desired privacy guarantee with a lower noise scale and thus better utility. We develop differentially private deep learning methods based on Spectral-DP for architectures that contain both convolution and fully connected layers. In particular, for fully connected layers, we combine a block-circulant based spatial restructuring with Spectral-DP to achieve better utility. Through comprehensive experiments, we study and provide guidelines to implement Spectral-DP deep learning on benchmark datasets. In comparison with state-of-the-art DP-SGD based approaches, Spectral-DP is shown to have uniformly better utility performance in both training from scratch and transfer learning settings.
</details>
<details>
<summary>摘要</summary>
Diffusion privacy is a widely accepted measure of privacy in the context of deep learning algorithms, and achieving it relies on a noisy training approach known as differentially private stochastic gradient descent (DP-SGD). DP-SGD requires direct noise addition to every gradient in a dense neural network, and privacy is achieved at a significant utility cost. In this work, we present Spectral-DP, a new differentially private learning approach that combines gradient perturbation in the spectral domain with spectral filtering to achieve a desired privacy guarantee with a lower noise scale and thus better utility. We develop differentially private deep learning methods based on Spectral-DP for architectures that contain both convolution and fully connected layers. In particular, for fully connected layers, we combine a block-circulant based spatial restructuring with Spectral-DP to achieve better utility. Through comprehensive experiments, we study and provide guidelines to implement Spectral-DP deep learning on benchmark datasets. In comparison with state-of-the-art DP-SGD based approaches, Spectral-DP is shown to have uniformly better utility performance in both training from scratch and transfer learning settings.Here's the text with some minor adjustments to make it more natural in Simplified Chinese:Diffusion privacy is a widely accepted measure of privacy in deep learning algorithms, and achieving it relies on a noisy training approach called differentially private stochastic gradient descent (DP-SGD). DP-SGD adds noise to every gradient in a dense neural network, and privacy is achieved at the cost of utility. In this work, we present Spectral-DP, a new differentially private learning approach that combines gradient perturbation in the spectral domain with spectral filtering to achieve a desired privacy guarantee with a lower noise scale and thus better utility. We develop differentially private deep learning methods based on Spectral-DP for architectures that contain both convolution and fully connected layers. In particular, for fully connected layers, we use a block-circulant based spatial restructuring with Spectral-DP to achieve better utility. Through comprehensive experiments, we study and provide guidelines for implementing Spectral-DP deep learning on benchmark datasets. In comparison with state-of-the-art DP-SGD based approaches, Spectral-DP shows uniformly better utility performance in both training from scratch and transfer learning settings.
</details></li>
</ul>
<hr>
<h2 id="A-Primer-on-the-Data-Cleaning-Pipeline"><a href="#A-Primer-on-the-Data-Cleaning-Pipeline" class="headerlink" title="A Primer on the Data Cleaning Pipeline"></a>A Primer on the Data Cleaning Pipeline</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13219">http://arxiv.org/abs/2307.13219</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rebecca C. Steorts</li>
<li>for: 这篇论文主要用于介绍数据整理管道的科学，以及在实际应用中使用的技术和方法。</li>
<li>methods: 论文介绍了数据整理管道的四个阶段，包括数据清洁、数据预处理、数据合并和数据分析。同时，也介绍了一些常用的技术和方法，如数据匹配、数据适应和数据净化。</li>
<li>results: 论文提出了一些在实际应用中使用数据整理管道的示例和案例，并详细介绍了这些方法的优缺点和应用场景。<details>
<summary>Abstract</summary>
The availability of both structured and unstructured databases, such as electronic health data, social media data, patent data, and surveys that are often updated in real time, among others, has grown rapidly over the past decade. With this expansion, the statistical and methodological questions around data integration, or rather merging multiple data sources, has also grown. Specifically, the science of the ``data cleaning pipeline'' contains four stages that allow an analyst to perform downstream tasks, predictive analyses, or statistical analyses on ``cleaned data.'' This article provides a review of this emerging field, introducing technical terminology and commonly used methods.
</details>
<details>
<summary>摘要</summary>
在过去的一代，数据的可用性快速增长，包括电子健康数据、社交媒体数据、专利数据和调查等，这些数据经常在实时更新。这一增长也导致了数据集成问题的统计和方法问题的增长。特别是，数据清洁管道科学中的四个阶段，允许分析员在“清洁数据”上进行下游任务、预测分析或统计分析。本文提供了这个新兴领域的综述，介绍技术术语和常用方法。
</details></li>
</ul>
<hr>
<h2 id="FedMEKT-Distillation-based-Embedding-Knowledge-Transfer-for-Multimodal-Federated-Learning"><a href="#FedMEKT-Distillation-based-Embedding-Knowledge-Transfer-for-Multimodal-Federated-Learning" class="headerlink" title="FedMEKT: Distillation-based Embedding Knowledge Transfer for Multimodal Federated Learning"></a>FedMEKT: Distillation-based Embedding Knowledge Transfer for Multimodal Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13214">http://arxiv.org/abs/2307.13214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huy Q. Le, Minh N. H. Nguyen, Chu Myaet Thwal, Yu Qiao, Chaoning Zhang, Choong Seon Hong</li>
<li>for: 提出了一种基于联合学习的多Modal Federated Learning框架，以便在多个客户端上共同训练一个泛化的全局模型，而不需要分享私有数据。</li>
<li>methods: 该框架使用了 semi-supervised learning 方法，利用不同模式之间的表示来提高学习模型的性能。在系统中，我们实现了一种基于热退换的多Modal Embedding Knowledge Transfer机制（FedMEKT），该机制可以在服务器和客户端之间交换joint知识，从而更新全局核心Encoder。</li>
<li>results: 通过对三个多Modal人活动识别数据集进行了广泛的实验，我们证明了 FedMEKT 可以在线评估中实现全局Encoder的优秀表现，同时保护用户的隐私和个人数据，并且对于其他基elines比较有效。<details>
<summary>Abstract</summary>
Federated learning (FL) enables a decentralized machine learning paradigm for multiple clients to collaboratively train a generalized global model without sharing their private data. Most existing works simply propose typical FL systems for single-modal data, thus limiting its potential on exploiting valuable multimodal data for future personalized applications. Furthermore, the majority of FL approaches still rely on the labeled data at the client side, which is limited in real-world applications due to the inability of self-annotation from users. In light of these limitations, we propose a novel multimodal FL framework that employs a semi-supervised learning approach to leverage the representations from different modalities. Bringing this concept into a system, we develop a distillation-based multimodal embedding knowledge transfer mechanism, namely FedMEKT, which allows the server and clients to exchange the joint knowledge of their learning models extracted from a small multimodal proxy dataset. Our FedMEKT iteratively updates the generalized global encoders with the joint embedding knowledge from the participating clients. Thereby, to address the modality discrepancy and labeled data constraint in existing FL systems, our proposed FedMEKT comprises local multimodal autoencoder learning, generalized multimodal autoencoder construction, and generalized classifier learning. Through extensive experiments on three multimodal human activity recognition datasets, we demonstrate that FedMEKT achieves superior global encoder performance on linear evaluation and guarantees user privacy for personal data and model parameters while demanding less communication cost than other baselines.
</details>
<details>
<summary>摘要</summary>
Federation learning (FL) 允许多个客户端共同训练一个泛化全球模型，无需分享私人数据。大多数现有的工作只是提出了一般的 FL 系统，因此它们在利用有价值的多模式数据方面受到限制。此外，大多数 FL 方法仍然依赖客户端上的标注数据，这在实际应用中很少可用因为用户自身标注困难。为了解决这些限制，我们提出了一个新的多模式 FL 框架，它使用 semi-supervised 学习方法来利用不同模式的表示。将这个概念引入到系统中，我们开发了一种名为 FedMEKT 的分配基于多模式嵌入知识传递机制，该机制 Allow the server and clients to exchange the joint knowledge of their learning models extracted from a small multimodal proxy dataset. Our FedMEKT iteratively updates the generalized global encoders with the joint embedding knowledge from the participating clients. In this way, we can address the modality discrepancy and labeled data constraint in existing FL systems. Our proposed FedMEKT consists of local multimodal autoencoder learning, generalized multimodal autoencoder construction, and generalized classifier learning. Through extensive experiments on three multimodal human activity recognition datasets, we demonstrate that FedMEKT achieves superior global encoder performance on linear evaluation and guarantees user privacy for personal data and model parameters while demanding less communication cost than other baselines.
</details></li>
</ul>
<hr>
<h2 id="Transferability-of-Graph-Neural-Networks-using-Graphon-and-Sampling-Theories"><a href="#Transferability-of-Graph-Neural-Networks-using-Graphon-and-Sampling-Theories" class="headerlink" title="Transferability of Graph Neural Networks using Graphon and Sampling Theories"></a>Transferability of Graph Neural Networks using Graphon and Sampling Theories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13206">http://arxiv.org/abs/2307.13206</a></li>
<li>repo_url: None</li>
<li>paper_authors: A. Martina Neuman, Jason J. Bramburger</li>
<li>for: 本研究旨在应用graphons来提高graph neural networks（GNNs）的可转移性，以便在不同的图ogram中使用已训练的GNN模型。</li>
<li>methods: 本研究使用了两层graphon neural network（WNN）架构，并证明了这个架构可以精确地预测带限的信号。此外，本研究还证明了WNN和GNN模型在所有足够大的图ogram中具有可转移性。</li>
<li>results: 本研究获得了一个可转移的GNN模型，可以在不同的图ogram中使用，并且不需要进行广泛的 retraining。此外，本研究还提供了一个实用的WNN和GNN架构，可以处理不同的图ogram大小，并且保持性能 guarantees。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have become powerful tools for processing graph-based information in various domains. A desirable property of GNNs is transferability, where a trained network can swap in information from a different graph without retraining and retain its accuracy. A recent method of capturing transferability of GNNs is through the use of graphons, which are symmetric, measurable functions representing the limit of large dense graphs. In this work, we contribute to the application of graphons to GNNs by presenting an explicit two-layer graphon neural network (WNN) architecture. We prove its ability to approximate bandlimited signals within a specified error tolerance using a minimal number of network weights. We then leverage this result, to establish the transferability of an explicit two-layer GNN over all sufficiently large graphs in a sequence converging to a graphon. Our work addresses transferability between both deterministic weighted graphs and simple random graphs and overcomes issues related to the curse of dimensionality that arise in other GNN results. The proposed WNN and GNN architectures offer practical solutions for handling graph data of varying sizes while maintaining performance guarantees without extensive retraining.
</details>
<details>
<summary>摘要</summary>
图 neural network (GNN) 已成为处理图形信息的有力工具。一个愿望的特性是转移性，即一个训练的网络可以将信息从不同的图 swap 而不需要重新训练，并保持准确性。一种最近用于捕捉 GNN 的转移性是通过图ON 来实现，图ON 是 densely 连接的图的限制，表示大图的极限。在这项工作中，我们为 GNN 的图ON 应用提供了显式的两层图ON 神经网络（WNN）架构。我们证明它可以在指定误差容差内 aproximate 带有限制的信号，使用最小的网络重量。然后，我们利用这个结果，Establish  GNN 的转移性，包括 deterministic Weighted graph 和 simple random graph，并超越了其他 GNN 结果中的尺度端问题。提出的 WNN 和 GNN 架构可以实际地处理不同大小的图数据，保持性能保证，而无需进行大量重新训练。
</details></li>
</ul>
<hr>
<h2 id="Federated-Distributionally-Robust-Optimization-with-Non-Convex-Objectives-Algorithm-and-Analysis"><a href="#Federated-Distributionally-Robust-Optimization-with-Non-Convex-Objectives-Algorithm-and-Analysis" class="headerlink" title="Federated Distributionally Robust Optimization with Non-Convex Objectives: Algorithm and Analysis"></a>Federated Distributionally Robust Optimization with Non-Convex Objectives: Algorithm and Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14364">http://arxiv.org/abs/2307.14364</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Jiao, Kai Yang, Dongjin Song</li>
<li>for:  solve the federated distributionally robust optimization (FDRO) problem in a distributed environment with asynchronous updates and prior knowledge.</li>
<li>methods:  asynchronous distributed algorithm named Asynchronous Single-looP alternatIve gRadient projEction (ASPIRE) algorithm with the itErative Active SEt method (EASE) to handle asynchronous updates and prior knowledge.</li>
<li>results:  the proposed method can achieve fast convergence, remain robust against data heterogeneity and malicious attacks, and flexibly trade off robustness with performance.Here is the Chinese translation of the three points:</li>
<li>for: solve federated分布式robust优化（FDRO）问题在分布式环境中，带有异步更新和先验知识。</li>
<li>methods: 使用 asynchronous分布式算法Named Asynchronous Single-looP alternatIve gRadient projEction (ASPIRE) algorithm with the itErative Active SEt method (EASE)来处理异步更新和先验知识。</li>
<li>results: 提议的方法可以实现快速收敛，对数据不一致和攻击行为具有鲁棒性，并可以灵活地让强度与性能进行衡量。<details>
<summary>Abstract</summary>
Distributionally Robust Optimization (DRO), which aims to find an optimal decision that minimizes the worst case cost over the ambiguity set of probability distribution, has been widely applied in diverse applications, e.g., network behavior analysis, risk management, etc. However, existing DRO techniques face three key challenges: 1) how to deal with the asynchronous updating in a distributed environment; 2) how to leverage the prior distribution effectively; 3) how to properly adjust the degree of robustness according to different scenarios. To this end, we propose an asynchronous distributed algorithm, named Asynchronous Single-looP alternatIve gRadient projEction (ASPIRE) algorithm with the itErative Active SEt method (EASE) to tackle the federated distributionally robust optimization (FDRO) problem. Furthermore, a new uncertainty set, i.e., constrained D-norm uncertainty set, is developed to effectively leverage the prior distribution and flexibly control the degree of robustness. Finally, our theoretical analysis elucidates that the proposed algorithm is guaranteed to converge and the iteration complexity is also analyzed. Extensive empirical studies on real-world datasets demonstrate that the proposed method can not only achieve fast convergence, and remain robust against data heterogeneity as well as malicious attacks, but also tradeoff robustness with performance.
</details>
<details>
<summary>摘要</summary>
分布式可靠优化（DRO），旨在找到最优决策，以最小化不确定性集中的最差成本，已广泛应用于多个领域，如网络行为分析、风险管理等。然而，现有的DRO技术面临三大挑战：1）如何在分布式环境中处理异步更新；2）如何有效利用先验分布；3）如何适应不同场景下的稳健性。为此，我们提出了一种异步分布式算法，名为异步单独搅拌梯度投影（ASPIRE）算法，并与迭代活动集成方法（EASE）解决联邦分布式可靠优化（FDRO）问题。此外，我们开发了一种新的不确定集，即受限D-norm不确定集，以有效利用先验分布并flexibly控制稳健性水平。最后，我们的理论分析表明，提posed算法是确定 converges，并且iteration复杂度也进行了分析。实际研究表明，提posed方法可以不仅具有快速收敛，同时保持数据多样性和攻击的Robustness。
</details></li>
</ul>
<hr>
<h2 id="An-Investigation-into-Glomeruli-Detection-in-Kidney-H-E-and-PAS-Images-using-YOLO"><a href="#An-Investigation-into-Glomeruli-Detection-in-Kidney-H-E-and-PAS-Images-using-YOLO" class="headerlink" title="An Investigation into Glomeruli Detection in Kidney H&amp;E and PAS Images using YOLO"></a>An Investigation into Glomeruli Detection in Kidney H&amp;E and PAS Images using YOLO</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13199">http://arxiv.org/abs/2307.13199</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AlexeyAB/darknet">https://github.com/AlexeyAB/darknet</a></li>
<li>paper_authors: Kimia Hemmatirad, Morteza Babaie, Jeffrey Hodgin, Liron Pantanowitz, H. R. Tizhoosh</li>
<li>for: 协助病理师使用计算机化解决方案，自动检测和分类 Digitale pathology 图像，以获得诊断结论。</li>
<li>methods: 使用 YOLO-v4（You-Only-Look-Once）实时物体检测器，这是一个单一神经网络，可以预测多个 bounding box 和物体标度的组合。</li>
<li>results: 这篇论文使用 YOLO-v4 进行人脸识别，并在两个公共数据集和一个私人数据集（来自密歇根大学）进行了多个实验，并在这个私人数据集上进行了外部验证，使用两种染色物质（HE和PAS）。结果显示，自动检测人脸在人脸识别中可能性很高。<details>
<summary>Abstract</summary>
Context: Analyzing digital pathology images is necessary to draw diagnostic conclusions by investigating tissue patterns and cellular morphology. However, manual evaluation can be time-consuming, expensive, and prone to inter- and intra-observer variability. Objective: To assist pathologists using computerized solutions, automated tissue structure detection and segmentation must be proposed. Furthermore, generating pixel-level object annotations for histopathology images is expensive and time-consuming. As a result, detection models with bounding box labels may be a feasible solution. Design: This paper studies. YOLO-v4 (You-Only-Look-Once), a real-time object detector for microscopic images. YOLO uses a single neural network to predict several bounding boxes and class probabilities for objects of interest. YOLO can enhance detection performance by training on whole slide images. YOLO-v4 has been used in this paper. for glomeruli detection in human kidney images. Multiple experiments have been designed and conducted based on different training data of two public datasets and a private dataset from the University of Michigan for fine-tuning the model. The model was tested on the private dataset from the University of Michigan, serving as an external validation of two different stains, namely hematoxylin and eosin (H&E) and periodic acid-Schiff (PAS). Results: Average specificity and sensitivity for all experiments, and comparison of existing segmentation methods on the same datasets are discussed. Conclusions: Automated glomeruli detection in human kidney images is possible using modern AI models. The design and validation for different stains still depends on variability of public multi-stain datasets.
</details>
<details>
<summary>摘要</summary>
Context: 分析数字 PATHOLOGY 图像是必要的，以便从 investigate 组织趋势和细胞形态中得出诊断结论。然而，手动评估可能是昂贵的、时间consuming 和 observer 间和 observer 内的变化。目标：通过计算机化解决方案，自动检测和分类组织结构。此外，生成 Histopathology 图像像素级对象注释是昂贵的和时间consuming。因此，使用 bounding box 标签的检测模型可能是一个可行的解决方案。设计：本文研究 YOLO-v4（You-Only-Look-Once），一种实时物体检测器，可以在微scopic 图像上预测多个 bounding box 和类别概率。YOLO 可以提高检测性能，通过训练整个扫描图像。本文使用 YOLO-v4 进行人肾脏中的膜体检测。多个实验基于不同的训练数据，包括两个公共数据集和一个来自大学Michigan的私人数据集进行精度调整。模型在大学Michigan的私人数据集上进行测试，作为两种不同染料（hematoxylin和eosin（H&E）和 periodic acid-Schiff（PAS））的外部验证。结果：对所有实验的特定性和敏感性的平均值，以及相同数据集上的现有分 segmentation 方法进行比较。结论：使用现代 AI 模型，自动检测人肾脏中的膜体是可能的。设计和验证不同染料仍然取决于多个公共多染料数据集的变化。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-enhanced-Neuro-Symbolic-AI-for-Cybersecurity-and-Privacy"><a href="#Knowledge-enhanced-Neuro-Symbolic-AI-for-Cybersecurity-and-Privacy" class="headerlink" title="Knowledge-enhanced Neuro-Symbolic AI for Cybersecurity and Privacy"></a>Knowledge-enhanced Neuro-Symbolic AI for Cybersecurity and Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02031">http://arxiv.org/abs/2308.02031</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aritran Piplai, Anantaa Kotal, Seyedreza Mohseni, Manas Gaur, Sudip Mittal, Anupam Joshi</li>
<li>for: The paper is written for those who are interested in developing explainable and safe AI systems, particularly in the domains of cybersecurity and privacy.</li>
<li>methods: The paper proposes the use of Neuro-Symbolic AI, which combines the strengths of deep neural networks and explicit symbolic knowledge to enhance explainability and safety in AI systems.</li>
<li>results: The paper highlights the potential benefits of Neuro-Symbolic AI in addressing the challenges of explainability and safety in cybersecurity and privacy applications, where the need for highly accurate and understandable AI systems is particularly important.Here’s the information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了探讨如何开发可解释的、安全的人工智能系统，尤其是在防火墙和隐私两个领域。</li>
<li>methods: 论文提出使用神经符号智能，将神经网络的强点与知识图的优点相结合，以提高可解释性和安全性。</li>
<li>results: 论文指出神经符号智能在防火墙和隐私应用中可以减轻不可预测的风险，提高AI系统的可解释性和安全性。<details>
<summary>Abstract</summary>
Neuro-Symbolic Artificial Intelligence (AI) is an emerging and quickly advancing field that combines the subsymbolic strengths of (deep) neural networks and explicit, symbolic knowledge contained in knowledge graphs to enhance explainability and safety in AI systems. This approach addresses a key criticism of current generation systems, namely their inability to generate human-understandable explanations for their outcomes and ensure safe behaviors, especially in scenarios with \textit{unknown unknowns} (e.g. cybersecurity, privacy). The integration of neural networks, which excel at exploring complex data spaces, and symbolic knowledge graphs, which represent domain knowledge, allows AI systems to reason, learn, and generalize in a manner understandable to experts. This article describes how applications in cybersecurity and privacy, two most demanding domains in terms of the need for AI to be explainable while being highly accurate in complex environments, can benefit from Neuro-Symbolic AI.
</details>
<details>
<summary>摘要</summary>
neurosymbolic 人工智能（AI）是一个出现和快速发展的领域，它将深度神经网络和明确的符号知识包含在知识图中结合起来，以提高AI系统的可解释性和安全性。这种方法解决了现代AI系统的一个批评，即它们无法生成人类可理解的解释，特别是在“未知未知”（例如隐私、安全）的场景下。神经网络和符号知识图的集成，使AI系统可以理解、学习和泛化，并且可以由专家理解。本文介绍了如何在隐私和安全两个最高要求AI系统可解释性的领域中，应用Neuro-Symbolic AI。
</details></li>
</ul>
<hr>
<h2 id="Counterfactual-Explanation-Policies-in-RL"><a href="#Counterfactual-Explanation-Policies-in-RL" class="headerlink" title="Counterfactual Explanation Policies in RL"></a>Counterfactual Explanation Policies in RL</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13192">http://arxiv.org/abs/2307.13192</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shripad V. Deshmukh, Srivatsan R, Supriti Vijay, Jayakumar Subramanian, Chirag Agarwal</li>
<li>for: 本文旨在提供一种Counterfactual Explanation（Counterpol），用于解释RL策略的决策过程中的不同变量对策略表现的影响。</li>
<li>methods: 本文提出了一种 incorporating counterfactuals in supervised learning的方法，通过将愿景返回用于约束RL策略优化。同时，本文还建立了与广泛使用的信任区基本策略优化方法的理论连接。</li>
<li>results: 实验结果表明，Counterpol可以准确地生成对策略的解释，并且可以保持与原始策略几乎相同的性能。此外，本文在五个不同的RL环境中进行了广泛的实验，并证明了对不同状态和动作空间的RL环境的应用。<details>
<summary>Abstract</summary>
As Reinforcement Learning (RL) agents are increasingly employed in diverse decision-making problems using reward preferences, it becomes important to ensure that policies learned by these frameworks in mapping observations to a probability distribution of the possible actions are explainable. However, there is little to no work in the systematic understanding of these complex policies in a contrastive manner, i.e., what minimal changes to the policy would improve/worsen its performance to a desired level. In this work, we present COUNTERPOL, the first framework to analyze RL policies using counterfactual explanations in the form of minimal changes to the policy that lead to the desired outcome. We do so by incorporating counterfactuals in supervised learning in RL with the target outcome regulated using desired return. We establish a theoretical connection between Counterpol and widely used trust region-based policy optimization methods in RL. Extensive empirical analysis shows the efficacy of COUNTERPOL in generating explanations for (un)learning skills while keeping close to the original policy. Our results on five different RL environments with diverse state and action spaces demonstrate the utility of counterfactual explanations, paving the way for new frontiers in designing and developing counterfactual policies.
</details>
<details>
<summary>摘要</summary>
As Reinforcement Learning (RL) agents increasingly employed in diverse decision-making problems using reward preferences, it becomes important to ensure that policies learned by these frameworks in mapping observations to a probability distribution of the possible actions are explainable. However, there is little to no work in the systematic understanding of these complex policies in a contrastive manner, i.e., what minimal changes to the policy would improve/worsen its performance to a desired level. In this work, we present COUNTERPOL, the first framework to analyze RL policies using counterfactual explanations in the form of minimal changes to the policy that lead to the desired outcome. We do so by incorporating counterfactuals in supervised learning in RL with the target outcome regulated using desired return. We establish a theoretical connection between Counterpol and widely used trust region-based policy optimization methods in RL. Extensive empirical analysis shows the efficacy of COUNTERPOL in generating explanations for (un)learning skills while keeping close to the original policy. Our results on five different RL environments with diverse state and action spaces demonstrate the utility of counterfactual explanations, paving the way for new frontiers in designing and developing counterfactual policies.Here's the translation in Traditional Chinese:当Reinforcement Learning（RL）机器人在不同的决策问题中使用奖励偏好时，确保RL政策learned by these frameworks in mapping observations to a probability distribution of the possible actions是可解释的成为越来越重要。然而，有很少或没有关于这些复杂政策的系统性理解，即所谓的对比性分析，即政策中的最小变化会提高/下降到所需的水平。在这种情况下，我们提出了Counterpol，RL中第一个使用对比性分析来分析RL政策的框架。我们通过在RL中 incorporating counterfactuals in supervised learning with the target outcome regulated using desired return来实现这一点。我们建立了RL中广泛使用信任区基本策优方法的理论连接。我们的实验表明，Counterpol可以在保持原始政策的情况下，生成对RL策略的解释，并且在五个不同的RL环境中进行了广泛的 empirical analysis。我们的结果表明，对RL策略的对比性分析可以提供有用的解释，开启新的前iers in designing and developing counterfactual policies。
</details></li>
</ul>
<hr>
<h2 id="Neural-Memory-Decoding-with-EEG-Data-and-Representation-Learning"><a href="#Neural-Memory-Decoding-with-EEG-Data-and-Representation-Learning" class="headerlink" title="Neural Memory Decoding with EEG Data and Representation Learning"></a>Neural Memory Decoding with EEG Data and Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13181">http://arxiv.org/abs/2307.13181</a></li>
<li>repo_url: None</li>
<li>paper_authors: Glenn Bruns, Michael Haidar, Federico Rubino</li>
<li>for: 这种方法用于神经储存的快速识别，特别是从EEG数据中识别记忆。</li>
<li>methods: 该方法使用深度表示学习和指导对准损失来将EEG记录转换到低维度空间中。</li>
<li>results: 该方法可以在EEG数据中识别记忆，并且具有约78.4%的顶部准确率（机会为4%）。<details>
<summary>Abstract</summary>
We describe a method for the neural decoding of memory from EEG data. Using this method, a concept being recalled can be identified from an EEG trace with an average top-1 accuracy of about 78.4% (chance 4%). The method employs deep representation learning with supervised contrastive loss to map an EEG recording of brain activity to a low-dimensional space. Because representation learning is used, concepts can be identified even if they do not appear in the training data set. However, reference EEG data must exist for each such concept. We also show an application of the method to the problem of information retrieval. In neural information retrieval, EEG data is captured while a user recalls the contents of a document, and a list of links to predicted documents is produced.
</details>
<details>
<summary>摘要</summary>
我们描述了一种使用神经网络进行记忆解码的方法，使得从EEG数据中可以识别被回忆的概念。该方法使用深度表示学习，使EEG记录活动映射到低维度空间。由于使用表示学习，即使概念没有出现在训练数据集中，也可以识别出来。然而，需要对每个概念的参照EEG数据exist。我们还展示了该方法在信息检索问题中的应用，在神经信息检索中，EEG数据被记录在用户回忆文档内容时，并生成一个预测文档的列表。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-reliability-of-automatically-generated-pedestrian-and-bicycle-crash-surrogates"><a href="#Evaluating-the-reliability-of-automatically-generated-pedestrian-and-bicycle-crash-surrogates" class="headerlink" title="Evaluating the reliability of automatically generated pedestrian and bicycle crash surrogates"></a>Evaluating the reliability of automatically generated pedestrian and bicycle crash surrogates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13178">http://arxiv.org/abs/2307.13178</a></li>
<li>repo_url: None</li>
<li>paper_authors: Agnimitra Sengupta, S. Ilgin Guler, Vikash V. Gayah, Shannon Warchol<br>for:This research aims to improve VRU safety performance at signalized intersections by identifying and using conflicts between VRUs and motorized vehicles as a surrogate for safety performance.methods:The research uses a video-based event monitoring system to automatically detect conflicts between VRUs and motorized vehicles, and advanced data-driven models to predict confirmed conflicts.results:The findings highlight the varying importance of specific surrogates in predicting true conflicts, and can assist transportation agencies to prioritize infrastructure investments and evaluate their effectiveness.Here is the same information in Simplified Chinese text:for:这项研究目标是提高信息化交通安全性，通过检测VRUs和机动车之间的冲突来衡量安全性。methods:该研究使用视频基于事件监测系统自动检测VRUs和机动车之间的冲突，并使用高级数据驱动模型预测确认冲突。results:研究发现冲突特定的指标在预测实际冲突中的重要性各不相同，可以帮助交通部门制定有效的基础设施投资和评估其效果。<details>
<summary>Abstract</summary>
Vulnerable road users (VRUs), such as pedestrians and bicyclists, are at a higher risk of being involved in crashes with motor vehicles, and crashes involving VRUs also are more likely to result in severe injuries or fatalities. Signalized intersections are a major safety concern for VRUs due to their complex and dynamic nature, highlighting the need to understand how these road users interact with motor vehicles and deploy evidence-based countermeasures to improve safety performance. Crashes involving VRUs are relatively infrequent, making it difficult to understand the underlying contributing factors. An alternative is to identify and use conflicts between VRUs and motorized vehicles as a surrogate for safety performance. Automatically detecting these conflicts using a video-based systems is a crucial step in developing smart infrastructure to enhance VRU safety. The Pennsylvania Department of Transportation conducted a study using video-based event monitoring system to assess VRU and motor vehicle interactions at fifteen signalized intersections across Pennsylvania to improve VRU safety performance. This research builds on that study to assess the reliability of automatically generated surrogates in predicting confirmed conflicts using advanced data-driven models. The surrogate data used for analysis include automatically collectable variables such as vehicular and VRU speeds, movements, post-encroachment time, in addition to manually collected variables like signal states, lighting, and weather conditions. The findings highlight the varying importance of specific surrogates in predicting true conflicts, some being more informative than others. The findings can assist transportation agencies to collect the right types of data to help prioritize infrastructure investments, such as bike lanes and crosswalks, and evaluate their effectiveness.
</details>
<details>
<summary>摘要</summary>
易受伤的道路用户（VRU），如步行者和自行车客，在与机动车相撞时更容易受伤，同时这些相撞也更容易导致严重的伤害或死亡。信号控制交叉口是VRU安全问题的主要关注点，因为它们的复杂性和动态性使得需要更好地理解道路用户与机动车之间的互动，并采取基于证据的干预措施以提高安全性。VRU相撞事故相对较少，因此难以了解其下面的原因。因此，可以使用VRU和机动车之间的冲突作为安全性表现的代理。使用视频基本系统自动探测这些冲突是开发智能基础设施以提高VRU安全性的关键步骤。美国宾夕法尼亚州交通部门在十五个信号控制交叉口位于宾夕法尼亚州进行了视频基本系统评估VRU和机动车之间的互动，以提高VRU安全性表现。这项研究基于这项研究，以评估自动生成的代理是否可靠地预测真正的冲突。代理数据包括自动收集的变量，如机动车和VRU速度、运动和后续时间，以及手动收集的变量，如信号状态、灯光和天气条件。研究结果显示不同的代理在预测真正的冲突中的重要程度不同，一些更有用于预测。这些结果可以帮助交通机构收集相应的数据，以便优先投资基础设施，如自行车道和横道，并评估其效果。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-reconstruction-of-accelerated-cardiac-cine-MRI-using-Neural-Fields"><a href="#Unsupervised-reconstruction-of-accelerated-cardiac-cine-MRI-using-Neural-Fields" class="headerlink" title="Unsupervised reconstruction of accelerated cardiac cine MRI using Neural Fields"></a>Unsupervised reconstruction of accelerated cardiac cine MRI using Neural Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14363">http://arxiv.org/abs/2307.14363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tabita Catalán, Matías Courdurier, Axel Osses, René Botnar, Francisco Sahli Costabal, Claudia Prieto</li>
<li>for: cardiac functional assessment</li>
<li>methods: unsupervised approach based on implicit neural field representations for cardiac cine MRI</li>
<li>results: good image quality and improved temporal depiction compared to a state-of-the-art reconstruction technique<details>
<summary>Abstract</summary>
Cardiac cine MRI is the gold standard for cardiac functional assessment, but the inherently slow acquisition process creates the necessity of reconstruction approaches for accelerated undersampled acquisitions. Several regularization approaches that exploit spatial-temporal redundancy have been proposed to reconstruct undersampled cardiac cine MRI. More recently, methods based on supervised deep learning have been also proposed to further accelerate acquisition and reconstruction. However, these techniques rely on usually large dataset for training, which are not always available. In this work, we propose an unsupervised approach based on implicit neural field representations for cardiac cine MRI (so called NF-cMRI). The proposed method was evaluated in in-vivo undersampled golden-angle radial multi-coil acquisitions for undersampling factors of 26x and 52x, achieving good image quality, and comparable spatial and improved temporal depiction than a state-of-the-art reconstruction technique.
</details>
<details>
<summary>摘要</summary>
卡ди亚ц琴MRI是心脏功能评估的标准黄金标准，但它的自然slow acquisition process创造了快速重建的需求。多种基于空间-时间重复的正则化方法已经被提议用于快速重建受损缺少的cardiac cine MRI。在最近，基于监督深度学习方法已经被提议用于进一步加速获取和重建。但这些技术需要大量的训练数据，这些数据不总是可用。在这项工作中，我们提出了一种不监督的方法，基于启发神经场表示法（so called NF-cMRI）来重建受损缺少的cardiac cine MRI。我们对具有26x和52x受损因子的实验室中的生物体内无法进行了评估，并达到了良好的图像质量和相对较好的空间和改进的时间表现。
</details></li>
</ul>
<hr>
<h2 id="Multi-UAV-Speed-Control-with-Collision-Avoidance-and-Handover-aware-Cell-Association-DRL-with-Action-Branching"><a href="#Multi-UAV-Speed-Control-with-Collision-Avoidance-and-Handover-aware-Cell-Association-DRL-with-Action-Branching" class="headerlink" title="Multi-UAV Speed Control with Collision Avoidance and Handover-aware Cell Association: DRL with Action Branching"></a>Multi-UAV Speed Control with Collision Avoidance and Handover-aware Cell Association: DRL with Action Branching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13158">http://arxiv.org/abs/2307.13158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zijiang Yan, Wael Jaafar, Bassant Selim, Hina Tabassum</li>
<li>for: 优化多架空飞行器（UAV）的紧急通信和运输性能，包括碰撞避免、连接稳定和交换。</li>
<li>methods: 使用深度强化学习解决方案，形式为Markov决策过程（MDP），UAV的状态定义为速度和通信数据速率。提议一种神经网络架构，具有共享决策模块和多个网络支柱，每个支柱负责特定的行动维度在2D运输通信空间。</li>
<li>results: 对比现有参考值，实验结果显示了18.32%的显著提升。<details>
<summary>Abstract</summary>
This paper presents a deep reinforcement learning solution for optimizing multi-UAV cell-association decisions and their moving velocity on a 3D aerial highway. The objective is to enhance transportation and communication performance, including collision avoidance, connectivity, and handovers. The problem is formulated as a Markov decision process (MDP) with UAVs' states defined by velocities and communication data rates. We propose a neural architecture with a shared decision module and multiple network branches, each dedicated to a specific action dimension in a 2D transportation-communication space. This design efficiently handles the multi-dimensional action space, allowing independence for individual action dimensions. We introduce two models, Branching Dueling Q-Network (BDQ) and Branching Dueling Double Deep Q-Network (Dueling DDQN), to demonstrate the approach. Simulation results show a significant improvement of 18.32% compared to existing benchmarks.
</details>
<details>
<summary>摘要</summary>
To tackle the multi-dimensional action space, the proposed solution features a neural architecture with a shared decision module and multiple network branches, each dedicated to a specific action dimension in a 2D transportation-communication space. This design allows for independence among individual action dimensions.Two models, Branching Dueling Q-Network (BDQ) and Branching Dueling Double Deep Q-Network (Dueling DDQN), are introduced to demonstrate the approach. Simulation results show a significant improvement of 18.32% compared to existing benchmarks.Translation in Simplified Chinese:这篇论文提出了一种深度强化学解决方案，用于优化多架航空器的细胞归属决策和移动速度决策在3D空中高速道路上。目标是提高交通和通信性能，包括避免碰撞、连接和手动传输。问题被формализова为一个Markov决策过程（MDP），其中无人机的状态定义为速度和通信数据速率。为了处理多维行动空间，提议的解决方案采用了一种神经网络架构，具有共享决策模块和多个网络分支，每个分支专门处理特定的行动维度在2D交通通信空间中。这种设计允许各个行动维度独立进行决策。两种模型，分支对抗Q网络（BDQ）和分支对抗双层深度Q网络（DDQN），用以示出方法。实验结果显示，与现有标准相比，提议的方法可以获得18.32%的显著提高。
</details></li>
</ul>
<hr>
<h2 id="Discovering-interpretable-elastoplasticity-models-via-the-neural-polynomial-method-enabled-symbolic-regressions"><a href="#Discovering-interpretable-elastoplasticity-models-via-the-neural-polynomial-method-enabled-symbolic-regressions" class="headerlink" title="Discovering interpretable elastoplasticity models via the neural polynomial method enabled symbolic regressions"></a>Discovering interpretable elastoplasticity models via the neural polynomial method enabled symbolic regressions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13149">http://arxiv.org/abs/2307.13149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bahador Bahmani, Hyoung Suk Suh, WaiChing Sun</li>
<li>for: 本研究旨在提高神经网络模型的可读性，通过两步机器学习方法返回可以被人类专家理解的数学模型。</li>
<li>methods: 本研究使用了分两步的机器学习方法，首先使用超级vised学习获得单变量特征映射，然后使用符号回归将这些映射转化为数学形式。</li>
<li>results: 研究结果显示，这种分两步机器学习方法可以解决神经网络模型的缺乏可读性问题，同时提供了更多的数学模型的可视化和可理解性。<details>
<summary>Abstract</summary>
Conventional neural network elastoplasticity models are often perceived as lacking interpretability. This paper introduces a two-step machine-learning approach that returns mathematical models interpretable by human experts. In particular, we introduce a surrogate model where yield surfaces are expressed in terms of a set of single-variable feature mappings obtained from supervised learning. A postprocessing step is then used to re-interpret the set of single-variable neural network mapping functions into mathematical form through symbolic regression. This divide-and-conquer approach provides several important advantages. First, it enables us to overcome the scaling issue of symbolic regression algorithms. From a practical perspective, it enhances the portability of learned models for partial differential equation solvers written in different programming languages. Finally, it enables us to have a concrete understanding of the attributes of the materials, such as convexity and symmetries of models, through automated derivations and reasoning. Numerical examples have been provided, along with an open-source code to enable third-party validation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learnable-wavelet-neural-networks-for-cosmological-inference"><a href="#Learnable-wavelet-neural-networks-for-cosmological-inference" class="headerlink" title="Learnable wavelet neural networks for cosmological inference"></a>Learnable wavelet neural networks for cosmological inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14362">http://arxiv.org/abs/2307.14362</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chris-pedersen/learnablewavelets">https://github.com/chris-pedersen/learnablewavelets</a></li>
<li>paper_authors: Christian Pedersen, Michael Eickenberg, Shirley Ho</li>
<li>for:  cosmological inference and marginalisation over astrophysical effects</li>
<li>methods:  learnable scattering transform, a kind of convolutional neural network that uses trainable wavelets as filters</li>
<li>results:  scattering architectures are able to outperform a CNN, significantly in the case of small training data samples, and a lightweight scattering network that is highly interpretable.<details>
<summary>Abstract</summary>
Convolutional neural networks (CNNs) have been shown to both extract more information than the traditional two-point statistics from cosmological fields, and marginalise over astrophysical effects extremely well. However, CNNs require large amounts of training data, which is potentially problematic in the domain of expensive cosmological simulations, and it is difficult to interpret the network. In this work we apply the learnable scattering transform, a kind of convolutional neural network that uses trainable wavelets as filters, to the problem of cosmological inference and marginalisation over astrophysical effects. We present two models based on the scattering transform, one constructed for performance, and one constructed for interpretability, and perform a comparison with a CNN. We find that scattering architectures are able to outperform a CNN, significantly in the case of small training data samples. Additionally we present a lightweight scattering network that is highly interpretable.
</details>
<details>
<summary>摘要</summary>
几何神经网络（CNN）已经显示可以从 cosmological 场中提取更多信息，并且对astrophysical 效应进行约束非常好。然而，CNN需要大量的训练数据，这可能是cosmological  simulations 中的问题，而且它很难解释。在这项工作中，我们使用可学习散射变换，一种使用可学习的wavelet 作为滤波器的几何神经网络，来解决 cosmological 推断和astrophysical 效应的约束问题。我们提出了两种基于散射变换的模型，一种是为了性能，另一种是为了可解释性。我们对这两种模型与CNN进行比较，发现散射架构可以在小训练样本情况下显著超越CNN。此外，我们还提出了一个轻量级的散射网络，具有非常高的可解释性。
</details></li>
</ul>
<hr>
<h2 id="Extending-Path-Dependent-NJ-ODEs-to-Noisy-Observations-and-a-Dependent-Observation-Framework"><a href="#Extending-Path-Dependent-NJ-ODEs-to-Noisy-Observations-and-a-Dependent-Observation-Framework" class="headerlink" title="Extending Path-Dependent NJ-ODEs to Noisy Observations and a Dependent Observation Framework"></a>Extending Path-Dependent NJ-ODEs to Noisy Observations and a Dependent Observation Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13147">http://arxiv.org/abs/2307.13147</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/floriankrach/pd-njode">https://github.com/floriankrach/pd-njode</a></li>
<li>paper_authors: William Andersson, Jakob Heiss, Florian Krach, Josef Teichmann</li>
<li>for: 预测不规则时间序列数据的不完整和受干扰的观测</li>
<li>methods: 基于神经网络的跳动梯度方程（PD-NJ-ODE）模型</li>
<li>results: 提供了两种扩展和理论保证，以及实际示例，以解决不规则时间序列数据的不完整和受干扰问题<details>
<summary>Abstract</summary>
The Path-Dependent Neural Jump ODE (PD-NJ-ODE) is a model for predicting continuous-time stochastic processes with irregular and incomplete observations. In particular, the method learns optimal forecasts given irregularly sampled time series of incomplete past observations. So far the process itself and the coordinate-wise observation times were assumed to be independent and observations were assumed to be noiseless. In this work we discuss two extensions to lift these restrictions and provide theoretical guarantees as well as empirical examples for them.
</details>
<details>
<summary>摘要</summary>
“Path-Dependent Neural Jump ODE（PD-NJ-ODE）是一种模型，用于预测连续时间的随机过程，具有不规则和缺失的观测。具体来说，该方法学习 optimal forecasts，给 irregularly sampled time series of incomplete past observations。目前，过程本身和坐标wise observation times 被假设为独立，并且观测被假设为噪声无存。在这种工作中，我们讨论了两种扩展，以解除这些限制，并提供了理论保证以及实际示例。”Note that Simplified Chinese is a romanization of Chinese, and the translation may not be exact.
</details></li>
</ul>
<hr>
<h2 id="Does-Progress-On-Object-Recognition-Benchmarks-Improve-Real-World-Generalization"><a href="#Does-Progress-On-Object-Recognition-Benchmarks-Improve-Real-World-Generalization" class="headerlink" title="Does Progress On Object Recognition Benchmarks Improve Real-World Generalization?"></a>Does Progress On Object Recognition Benchmarks Improve Real-World Generalization?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13136">http://arxiv.org/abs/2307.13136</a></li>
<li>repo_url: None</li>
<li>paper_authors: Megan Richards, Polina Kirichenko, Diane Bouchacourt, Mark Ibrahim<br>for:* The paper aims to study generalization across geography as a more realistic measure of progress in object recognition.methods:* The authors use two datasets of objects from households across the globe to evaluate the performance of nearly 100 vision models, including recent foundation models.* They measure the disparities in performance across regions as a more fine-grained measure of real-world generalization.results:* The authors find a progress gap between standard benchmarks and real-world, geographical shifts, with standard benchmarks resulting in up to 2.5x more progress than real-world distribution shifts.* They observe large geographic disparities in performance across regions, even for foundation CLIP models, with differences of 7-20% in accuracy between regions.* Retraining the models on more representative, curated data can complement scaling as a promising direction of future work, reducing geographic disparity on both benchmarks by over two-thirds.<details>
<summary>Abstract</summary>
For more than a decade, researchers have measured progress in object recognition on ImageNet-based generalization benchmarks such as ImageNet-A, -C, and -R. Recent advances in foundation models, trained on orders of magnitude more data, have begun to saturate these standard benchmarks, but remain brittle in practice. This suggests standard benchmarks, which tend to focus on predefined or synthetic changes, may not be sufficient for measuring real world generalization. Consequently, we propose studying generalization across geography as a more realistic measure of progress using two datasets of objects from households across the globe. We conduct an extensive empirical evaluation of progress across nearly 100 vision models up to most recent foundation models. We first identify a progress gap between standard benchmarks and real-world, geographical shifts: progress on ImageNet results in up to 2.5x more progress on standard generalization benchmarks than real-world distribution shifts. Second, we study model generalization across geographies by measuring the disparities in performance across regions, a more fine-grained measure of real world generalization. We observe all models have large geographic disparities, even foundation CLIP models, with differences of 7-20% in accuracy between regions. Counter to modern intuition, we discover progress on standard benchmarks fails to improve geographic disparities and often exacerbates them: geographic disparities between the least performant models and today's best models have more than tripled. Our results suggest scaling alone is insufficient for consistent robustness to real-world distribution shifts. Finally, we highlight in early experiments how simple last layer retraining on more representative, curated data can complement scaling as a promising direction of future work, reducing geographic disparity on both benchmarks by over two-thirds.
</details>
<details>
<summary>摘要</summary>
We conducted an extensive empirical evaluation of progress across nearly 100 vision models, including the most recent foundation models. Our results show a significant gap between progress on ImageNet and real-world, geographical shifts. Specifically, progress on ImageNet results in up to 2.5 times more progress on standard generalization benchmarks than real-world distribution shifts.We also studied model generalization across geographies by measuring the disparities in performance across regions, providing a more fine-grained measure of real-world generalization. Our findings show that all models, including foundation CLIP models, have large geographic disparities, with differences of 7-20% in accuracy between regions. Surprisingly, we found that progress on standard benchmarks actually exacerbates these geographic disparities, with the disparities between the least performant models and today's best models having more than tripled.Our results suggest that scaling alone is insufficient for achieving consistent robustness to real-world distribution shifts. However, we found that simple last layer retraining on more representative, curated data can complement scaling as a promising direction of future work, reducing geographic disparity on both benchmarks by over two-thirds.
</details></li>
</ul>
<hr>
<h2 id="simPLE-a-visuotactile-method-learned-in-simulation-to-precisely-pick-localize-regrasp-and-place-objects"><a href="#simPLE-a-visuotactile-method-learned-in-simulation-to-precisely-pick-localize-regrasp-and-place-objects" class="headerlink" title="simPLE: a visuotactile method learned in simulation to precisely pick, localize, regrasp, and place objects"></a>simPLE: a visuotactile method learned in simulation to precisely pick, localize, regrasp, and place objects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13133">http://arxiv.org/abs/2307.13133</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maria Bauza, Antonia Bronars, Yifan Hou, Ian Taylor, Nikhil Chavan-Dafle, Alberto Rodriguez</li>
<li>for: 这篇论文的目的是解决 робоット抓取和置放精度和通用性之间的矛盾。</li>
<li>methods: 这篇论文使用了三个主要组成部分：任务意识 grasping，视听感知和重新抓取规划。任务意识 grasping 计算物体抓取的可能性，视听感知模型通过经验学习与实际观察进行匹配，最后计算机器人的运动路径通过短路问题解决。</li>
<li>results: 在配备了视听感知的双臂机器人上，使用 simPLE 实现了 15 种多样化的物体的精度抓取和置放，成功率高达 90% （6 种物体）和 80% （11 种物体）。视频可以在 <a target="_blank" rel="noopener" href="http://mcube.mit.edu/research/simPLE.html">http://mcube.mit.edu/research/simPLE.html</a> 上查看。<details>
<summary>Abstract</summary>
Existing robotic systems have a clear tension between generality and precision. Deployed solutions for robotic manipulation tend to fall into the paradigm of one robot solving a single task, lacking precise generalization, i.e., the ability to solve many tasks without compromising on precision. This paper explores solutions for precise and general pick-and-place. In precise pick-and-place, i.e. kitting, the robot transforms an unstructured arrangement of objects into an organized arrangement, which can facilitate further manipulation. We propose simPLE (simulation to Pick Localize and PLacE) as a solution to precise pick-and-place. simPLE learns to pick, regrasp and place objects precisely, given only the object CAD model and no prior experience. We develop three main components: task-aware grasping, visuotactile perception, and regrasp planning. Task-aware grasping computes affordances of grasps that are stable, observable, and favorable to placing. The visuotactile perception model relies on matching real observations against a set of simulated ones through supervised learning. Finally, we compute the desired robot motion by solving a shortest path problem on a graph of hand-to-hand regrasps. On a dual-arm robot equipped with visuotactile sensing, we demonstrate pick-and-place of 15 diverse objects with simPLE. The objects span a wide range of shapes and simPLE achieves successful placements into structured arrangements with 1mm clearance over 90% of the time for 6 objects, and over 80% of the time for 11 objects. Videos are available at http://mcube.mit.edu/research/simPLE.html .
</details>
<details>
<summary>摘要</summary>
现有的 роботиче系统存在一种明显的矛盾，即通用性和精度之间的较量。已部署的 робоotic manipulation解决方案通常会降低到单一任务的解决方案，缺乏精度的总体化，即能够解决多个任务而无需牺牲精度。本文探讨精度和通用性的解决方案，特别是精度的吸盘和置换。我们提出了simPLE（ simulation to Pick Localize and PLacE）解决方案，该方案可以帮助机器人准确地从未经验的情况下将物品精度地吸盘和置换。我们开发了三个主要组件：任务意识 grasping、视听感知和重新抓取规划。任务意识 grasping计算物体的稳定、可见和置换的可能性。视听感知模型通过对实际观察与已经模拟的对比，通过监管学习来学习。最后，我们通过解决一个最短路径问题来计算机器人的运动。在搭载了视听感知的双手机 robot上，我们使用 simPLE 成功地进行了15种多形态物品的吸盘和置换。这些物品的形态范围广泛，而 simPLE 在90%的时间内成功地将物品置换到结构化的排序中，具有1毫米的清晰度。视频可以在http://mcube.mit.edu/research/simPLE.html 中找到。
</details></li>
</ul>
<hr>
<h2 id="A-Differentially-Private-Weighted-Empirical-Risk-Minimization-Procedure-and-its-Application-to-Outcome-Weighted-Learning"><a href="#A-Differentially-Private-Weighted-Empirical-Risk-Minimization-Procedure-and-its-Application-to-Outcome-Weighted-Learning" class="headerlink" title="A Differentially Private Weighted Empirical Risk Minimization Procedure and its Application to Outcome Weighted Learning"></a>A Differentially Private Weighted Empirical Risk Minimization Procedure and its Application to Outcome Weighted Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13127">http://arxiv.org/abs/2307.13127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Spencer Giddens, Yiwang Zhou, Kevin R. Krull, Tara M. Brinkman, Peter X. K. Song, Fang Liu</li>
<li>for: 保护敏感数据的隐私，提供数据隐私保护的机制。</li>
<li>methods: 使用异质隐私（DP）框架，提供数据隐私保护的数学保证。</li>
<li>results: 实验研究表明，可以通过DP-wERM来实现个人化治疗规则的隐私保护，而不会影响模型的性能。<details>
<summary>Abstract</summary>
It is commonplace to use data containing personal information to build predictive models in the framework of empirical risk minimization (ERM). While these models can be highly accurate in prediction, results obtained from these models with the use of sensitive data may be susceptible to privacy attacks. Differential privacy (DP) is an appealing framework for addressing such data privacy issues by providing mathematically provable bounds on the privacy loss incurred when releasing information from sensitive data. Previous work has primarily concentrated on applying DP to unweighted ERM. We consider an important generalization to weighted ERM (wERM). In wERM, each individual's contribution to the objective function can be assigned varying weights. In this context, we propose the first differentially private wERM algorithm, backed by a rigorous theoretical proof of its DP guarantees under mild regularity conditions. Extending the existing DP-ERM procedures to wERM paves a path to deriving privacy-preserving learning methods for individualized treatment rules, including the popular outcome weighted learning (OWL). We evaluate the performance of the DP-wERM application to OWL in a simulation study and in a real clinical trial of melatonin for sleep health. All empirical results demonstrate the viability of training OWL models via wERM with DP guarantees while maintaining sufficiently useful model performance. Therefore, we recommend practitioners consider implementing the proposed privacy-preserving OWL procedure in real-world scenarios involving sensitive data.
</details>
<details>
<summary>摘要</summary>
通常使用包含个人信息的数据来建立预测模型，而这些模型可以具有非常高的预测精度。但是使用敏感数据可能会导致隐私攻击。 diffe隐私（DP）是一个吸引人的框架，可以为解决这些数据隐私问题提供数学上可证明的隐私损害 bound。先前的工作主要集中在应用DP到不带重量的ERM（unweighted ERM）上。我们考虑了一个重要的扩展，即带重量ERM（weighted ERM， wERM）。在wERM中，每个个体的对象函数中的贡献可以被分配不同的权重。在这种情况下，我们提出了首个具有DP保证的wERM算法，并提供了严格的理论证明， guaranteeing DP under mild regularity conditions。扩展现有的DP-ERM过程到wERM，这种方法可以为个性化治疗规则，包括受欢迎的结果Weighted learning（OWL），提供隐私保护的学习方法。我们在一个 simstudy和一个真实的临床试验中评估了DP-wERM应用于OWL的性能。所有实验结果都表明了在保持有用的模型性能的情况下，可以通过具有DP保证的方法来训练OWL模型。因此，我们建议实践者在含敏感数据的实际场景中考虑实施我们提议的隐私保护OWL过程。
</details></li>
</ul>
<hr>
<h2 id="A-Hybrid-Machine-Learning-Model-for-Classifying-Gene-Mutations-in-Cancer-using-LSTM-BiLSTM-CNN-GRU-and-GloVe"><a href="#A-Hybrid-Machine-Learning-Model-for-Classifying-Gene-Mutations-in-Cancer-using-LSTM-BiLSTM-CNN-GRU-and-GloVe" class="headerlink" title="A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe"></a>A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14361">http://arxiv.org/abs/2307.14361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanad Aburass, Osama Dorgham, Jamil Al Shaqsi</li>
<li>for: 这个研究旨在使用Kaggle的Personalized Medicine: Redefining Cancer Treatment dataset，混合LSTM、BiLSTM、CNN、GRU和GloVe来分类基因突变。</li>
<li>methods: 这个研究使用了一个ensemble模型，混合LSTM、BiLSTM、CNN、GRU和GloVe来分类基因突变。</li>
<li>results: 这个研究发现，ensemble模型可以优化基因突变的分类性能，并且需要训练时间较短，实现了性能和效率的完美结合。<details>
<summary>Abstract</summary>
This study presents an ensemble model combining LSTM, BiLSTM, CNN, GRU, and GloVe to classify gene mutations using Kaggle's Personalized Medicine: Redefining Cancer Treatment dataset. The results were compared against well-known transformers like as BERT, Electra, Roberta, XLNet, Distilbert, and their LSTM ensembles. Our model outperformed all other models in terms of accuracy, precision, recall, F1 score, and Mean Squared Error. Surprisingly, it also needed less training time, resulting in a perfect combination of performance and efficiency. This study demonstrates the utility of ensemble models for difficult tasks such as gene mutation classification.
</details>
<details>
<summary>摘要</summary>
这项研究把一个ensemble模型组合LSTM、BiLSTM、CNN、GRU和GloVe用于分类基因突变，使用Kaggle的个性化医疗：重定义肿瘤治疗数据集。结果与著名的变换器如BERT、Electra、Roberta、XLNet、Distilbert和其LSTMensemble进行比较。我们的模型在准确率、精度、准确率、F1分数和平均平方误差方面都高于所有其他模型，同时具有更好的效率，这表明ensemble模型在困难任务中具有优势。这项研究证明了ensemble模型在基因突变分类中的实用性。
</details></li>
</ul>
<hr>
<h2 id="Deep-Bradley-Terry-Rating-Quantifying-Properties-from-Comparisons"><a href="#Deep-Bradley-Terry-Rating-Quantifying-Properties-from-Comparisons" class="headerlink" title="Deep Bradley-Terry Rating: Quantifying Properties from Comparisons"></a>Deep Bradley-Terry Rating: Quantifying Properties from Comparisons</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13709">http://arxiv.org/abs/2307.13709</a></li>
<li>repo_url: None</li>
<li>paper_authors: Satoru Fujii</li>
<li>for: 本研究旨在开发一种能够评估不可见性的实际世界属性的机器学习框架。</li>
<li>methods: 本研究使用深度学习网络结构，将布拉德利-特里尔模型灵活地集成到机器学习框架中。此外，我们还推广了这种架构，以适应不公平环境。</li>
<li>results: 实验分析表明，DBTR可以成功地评估和估计欲要的属性。<details>
<summary>Abstract</summary>
Many properties in the real world can't be directly observed, making them difficult to learn. To deal with this challenging problem, prior works have primarily focused on estimating those properties by using graded human scores as the target label in the training. Meanwhile, rating algorithms based on the Bradley-Terry model are extensively studied to evaluate the competitiveness of players based on their match history. In this paper, we introduce the Deep Bradley-Terry Rating (DBTR), a novel machine learning framework designed to quantify and evaluate properties of unknown items. Our method seamlessly integrates the Bradley-Terry model into the neural network structure. Moreover, we generalize this architecture further to asymmetric environments with unfairness, a condition more commonly encountered in real-world settings. Through experimental analysis, we demonstrate that DBTR successfully learns to quantify and estimate desired properties.
</details>
<details>
<summary>摘要</summary>
很多现实世界中的属性难以直接观察，这使得它们学习变得更加困难。先前的工作主要通过使用排名为目标标签进行训练来估算这些属性。而rating算法基于布莱德利-泰利模型在评估玩家的竞争力方面得到了广泛的研究。在这篇论文中，我们介绍了深度布莱德利-泰利评分（DBTR），一种新的机器学习框架，用于评估和评价未知物品的属性。我们将布莱德利-泰利模型integrated into the neural network architecture。此外，我们还扩展了这个体系，以适应不平等环境，这种情况更常见于实际世界中。通过实验分析，我们证明了DBTR成功地学习和估算欲知的属性。Note: "Simplified Chinese" is a translation of "Traditional Chinese" and "Traditional Chinese" is the same as "Simplified Chinese" in mainland China.
</details></li>
</ul>
<hr>
<h2 id="Conformal-prediction-for-frequency-severity-modeling"><a href="#Conformal-prediction-for-frequency-severity-modeling" class="headerlink" title="Conformal prediction for frequency-severity modeling"></a>Conformal prediction for frequency-severity modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13124">http://arxiv.org/abs/2307.13124</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/heltongraziadei/conformal-fs">https://github.com/heltongraziadei/conformal-fs</a></li>
<li>paper_authors: Helton Graziadei, Paulo C. Marques F., Eduardo F. L. de Melo, Rodrigo S. Targino</li>
<li>for: 预测保险订单的预测范围</li>
<li>methods: 使用非参数模型独立预测方法，并利用分割兼斥预测技术以生成预测范围</li>
<li>results: 在模拟和实际数据集上展示了预测范围的有效性，并且在Random Forest中的两个阶段频率强度模型中使用剩下的机制来生成适应宽度的预测范围。<details>
<summary>Abstract</summary>
We present a nonparametric model-agnostic framework for building prediction intervals of insurance claims, with finite sample statistical guarantees, extending the technique of split conformal prediction to the domain of two-stage frequency-severity modeling. The effectiveness of the framework is showcased with simulated and real datasets. When the underlying severity model is a random forest, we extend the two-stage split conformal prediction procedure, showing how the out-of-bag mechanism can be leveraged to eliminate the need for a calibration set and to enable the production of prediction intervals with adaptive width.
</details>
<details>
<summary>摘要</summary>
我们提出了一种非参数化模型无关的框架，用于构建预测评估Interval，具有finite sample统计保证，扩展了split conformal prediction技术到频率严重模型领域。我们通过实验和真实数据示cases，证明了该框架的有效性。当下面预测模型是随机森林时，我们EXTEND了两个阶段split conformal prediction过程，并示出了如何利用out-of-bag机制来消除需要调整集的需求，并允许生成适应宽度的预测评估Interval。
</details></li>
</ul>
<hr>
<h2 id="An-Explainable-Geometric-Weighted-Graph-Attention-Network-for-Identifying-Functional-Networks-Associated-with-Gait-Impairment"><a href="#An-Explainable-Geometric-Weighted-Graph-Attention-Network-for-Identifying-Functional-Networks-Associated-with-Gait-Impairment" class="headerlink" title="An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment"></a>An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13108">http://arxiv.org/abs/2307.13108</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/favour-nerrise/xgw-gat">https://github.com/favour-nerrise/xgw-gat</a></li>
<li>paper_authors: Favour Nerrise, Qingyu Zhao, Kathleen L. Poston, Kilian M. Pohl, Ehsan Adeli</li>
<li>for: This paper aims to identify functional networks predictive of gait difficulties in individuals with Parkinson’s disease (PD) using an explainable, geometric, weighted-graph attention neural network (xGW-GAT).</li>
<li>methods: The xGW-GAT model uses resting-state functional MRI (rs-fMRI) data to represent functional connectomes as symmetric positive definite (SPD) matrices on a Riemannian manifold, and learns an attention mask yielding individual- and group-level explainability.</li>
<li>results: The xGW-GAT model successfully outperforms several existing methods and identifies functional connectivity patterns associated with gait impairment in PD, providing interpretable explanations of functional subnetworks associated with motor impairment.Here’s the Simplified Chinese text version of the three key points:</li>
<li>for: 这篇论文目标是使用可解释的、几何的、加权图像注意力神经网络（xGW-GAT）来预测parkinson病患者中的步态困难。</li>
<li>methods: xGW-GAT模型使用了休息状态功能磁共振成像（rs-fMRI）数据来表示功能连接图，并将连接图转换为几何上的正定定Matrix，从而实现个体和组级别的解释性。</li>
<li>results: xGW-GAT模型成功超越了一些现有的方法，并在parkinson病患者中预测了步态困难的功能连接图，提供了可解释的功能子网络协助抑荐。<details>
<summary>Abstract</summary>
One of the hallmark symptoms of Parkinson's Disease (PD) is the progressive loss of postural reflexes, which eventually leads to gait difficulties and balance problems. Identifying disruptions in brain function associated with gait impairment could be crucial in better understanding PD motor progression, thus advancing the development of more effective and personalized therapeutics. In this work, we present an explainable, geometric, weighted-graph attention neural network (xGW-GAT) to identify functional networks predictive of the progression of gait difficulties in individuals with PD. xGW-GAT predicts the multi-class gait impairment on the MDS Unified PD Rating Scale (MDS-UPDRS). Our computational- and data-efficient model represents functional connectomes as symmetric positive definite (SPD) matrices on a Riemannian manifold to explicitly encode pairwise interactions of entire connectomes, based on which we learn an attention mask yielding individual- and group-level explainability. Applied to our resting-state functional MRI (rs-fMRI) dataset of individuals with PD, xGW-GAT identifies functional connectivity patterns associated with gait impairment in PD and offers interpretable explanations of functional subnetworks associated with motor impairment. Our model successfully outperforms several existing methods while simultaneously revealing clinically-relevant connectivity patterns. The source code is available at https://github.com/favour-nerrise/xGW-GAT .
</details>
<details>
<summary>摘要</summary>
一种典型的parkinson病（PD）的症状是慢速损失的姿态反射，最终会导致行走困难和平衡问题。identifying脑功能干预的变化可能对于更好地理解PD的运动进程有益，从而推动更有效和个性化的治疗的开发。在这项工作中，我们提出了一种可解释的、几何学的、加权图注意力神经网络（xGW-GAT），用于预测PD患者的行走困难级别。xGW-GAT预测了UPDRS（MDS-UPDRS）中的多个步态困难。我们的计算和数据有效的模型将功能连接图表示为正定definite矩阵（SPD）在里曼曼ifold上，以显式地编码整个连接图的对称对之间的对应关系，根据这些对应关系我们学习出一个注意力 маска，以实现个体和组级别的解释。应用于我们的resting-state功能MRI（rs-fMRI）数据集中的PD患者，xGW-GAT已经确定了与步态困难相关的功能连接图模式，并提供了可解释的功能子网络相关于运动障碍的解释。我们的模型已经超越了一些现有的方法，同时揭示了临床有用的连接图模式。模型源代码可以在https://github.com/favour-nerrise/xGW-GAT 上获取。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Example-Based-Control"><a href="#Contrastive-Example-Based-Control" class="headerlink" title="Contrastive Example-Based Control"></a>Contrastive Example-Based Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13101">http://arxiv.org/abs/2307.13101</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/khatch31/laeo">https://github.com/khatch31/laeo</a></li>
<li>paper_authors: Kyle Hatch, Benjamin Eysenbach, Rafael Rafailov, Tianhe Yu, Ruslan Salakhutdinov, Sergey Levine, Chelsea Finn</li>
<li>for: 这篇论文旨在解决offline控制问题，即使环境交互成本高、指定奖励函数困难。</li>
<li>methods: 该论文提出了一种数据驱动的方法，通过对转移动态和高返回状态的示例进行标注，使用这些标注来教育RL算法。</li>
<li>results: 该方法在多种基于状态和图像的offline控制任务上表现出色，超过了基elines使用学习奖励函数的表现。此外，该方法还显示了改进的Robustness和数据集大小的扩展性。<details>
<summary>Abstract</summary>
While many real-world problems that might benefit from reinforcement learning, these problems rarely fit into the MDP mold: interacting with the environment is often expensive and specifying reward functions is challenging. Motivated by these challenges, prior work has developed data-driven approaches that learn entirely from samples from the transition dynamics and examples of high-return states. These methods typically learn a reward function from high-return states, use that reward function to label the transitions, and then apply an offline RL algorithm to these transitions. While these methods can achieve good results on many tasks, they can be complex, often requiring regularization and temporal difference updates. In this paper, we propose a method for offline, example-based control that learns an implicit model of multi-step transitions, rather than a reward function. We show that this implicit model can represent the Q-values for the example-based control problem. Across a range of state-based and image-based offline control tasks, our method outperforms baselines that use learned reward functions; additional experiments demonstrate improved robustness and scaling with dataset size.
</details>
<details>
<summary>摘要</summary>
“虽然许多实际问题可以受惠于强化学习，但这些问题很少遵循MDP模型：与环境交互是经济不可能，并且定义奖励函数是困难的。为了解决这些挑战，先前的工作已经开发了基于数据的方法，这些方法从转移动力和高返回状态中学习整个奖励函数，然后使用这个奖励函数来标记转移，最后应用在这些转移上的离线RL算法。虽然这些方法可以在许多任务上达到良好的结果，但它们可能会复杂，需要规则化和时间差更新。在这篇论文中，我们提出了一种离线、例子基本的控制方法，这种方法学习了多步转移的隐式模型，而不是奖励函数。我们证明了这种隐式模型可以表示Q值。在一系列基于状态和图像的离线控制任务上，我们的方法超过了基于学习的奖励函数的基elines，其他实验还表明了我们的方法的更好的稳定性和数据集大小的扩展性。”
</details></li>
</ul>
<hr>
<h2 id="Label-Noise-Correcting-a-Correction"><a href="#Label-Noise-Correcting-a-Correction" class="headerlink" title="Label Noise: Correcting a Correction"></a>Label Noise: Correcting a Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13100">http://arxiv.org/abs/2307.13100</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Toner, Amos Storkey</li>
<li>for:  addresses the issue of overfitting in training neural network classifiers on datasets with label noise</li>
<li>methods:  proposes a more direct approach to tackling overfitting by imposing a lower bound on the empirical risk during training, and provides theoretical results for different loss functions</li>
<li>results:  demonstrates significant enhancement of robustness in various settings with virtually no additional computational cost<details>
<summary>Abstract</summary>
Training neural network classifiers on datasets with label noise poses a risk of overfitting them to the noisy labels. To address this issue, researchers have explored alternative loss functions that aim to be more robust. However, many of these alternatives are heuristic in nature and still vulnerable to overfitting or underfitting. In this work, we propose a more direct approach to tackling overfitting caused by label noise. We observe that the presence of label noise implies a lower bound on the noisy generalised risk. Building upon this observation, we propose imposing a lower bound on the empirical risk during training to mitigate overfitting. Our main contribution is providing theoretical results that yield explicit, easily computable bounds on the minimum achievable noisy risk for different loss functions. We empirically demonstrate that using these bounds significantly enhances robustness in various settings, with virtually no additional computational cost.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用神经网络分类器训练数据集中的标签噪声会增加模型过拟合的风险。为此，研究人员尝试了一些代替的损失函数，以提高模型的Robustness。然而，大多数这些方法是启发的 Naturally, many of these methods are heuristic in nature and still vulnerable to overfitting or underfitting.在这项工作中，我们提出了一种更直接的方法来解决 label noise 导致的过拟合问题。我们注意到了标签噪声的存在下限制了总体风险的下界。基于这个观察，我们提议在训练过程中强制实施这个下界，以避免过拟合。我们的主要贡献是提供了不同损失函数的减少风险的显式计算可行的理论结果。我们在实验中证明了使用这些结果可以增强模型的Robustness，而且几乎没有额外的计算成本。Note: "神经网络" is translated as "神经网络" in Simplified Chinese, and "损失函数" is translated as "损失函数" in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Ensemble-Learning-for-Materials-Property-Prediction-with-Classical-Interatomic-Potentials-Carbon-as-an-Example"><a href="#Interpretable-Ensemble-Learning-for-Materials-Property-Prediction-with-Classical-Interatomic-Potentials-Carbon-as-an-Example" class="headerlink" title="Interpretable Ensemble Learning for Materials Property Prediction with Classical Interatomic Potentials: Carbon as an Example"></a>Interpretable Ensemble Learning for Materials Property Prediction with Classical Interatomic Potentials: Carbon as an Example</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10818">http://arxiv.org/abs/2308.10818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyu Jiang, Haofan Sun, Kamal Choudhary, Houlong Zhuang, Qiong Nian</li>
<li>for: 预测材料的重要性质</li>
<li>methods: ensemble learning（重叠学习）</li>
<li>results: 比原始物理学 potentials 更准确地预测材料的性质<details>
<summary>Abstract</summary>
Machine learning (ML) is widely used to explore crystal materials and predict their properties. However, the training is time-consuming for deep-learning models, and the regression process is a black box that is hard to interpret. Also, the preprocess to transfer a crystal structure into the input of ML, called descriptor, needs to be designed carefully. To efficiently predict important properties of materials, we propose an approach based on ensemble learning consisting of regression trees to predict formation energy and elastic constants based on small-size datasets of carbon allotropes as an example. Without using any descriptor, the inputs are the properties calculated by molecular dynamics with 9 different classical interatomic potentials. Overall, the results from ensemble learning are more accurate than those from classical interatomic potentials, and ensemble learning can capture the relatively accurate properties from the 9 classical potentials as criteria for predicting the final properties.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）广泛应用于探索晶体材料和预测其性能。然而，训练深度学习模型需要较长时间，而回归过程是一个难以解释的黑盒子。此外，将晶体结构转化为机器学习的输入，即描述符，需要仔细设计。为了高效预测材料的重要性能，我们提出了基于集成学习的方法，包括回归树来预测formation energy和塑性常数，使用小型碳材料数据集为例。不使用任何描述符，输入是通过分子动力学计算的物理属性，使用9种不同的古典间隔 potentials。总的来说，ensemble learning的结果比古典间隔 potentials更准确，并可以捕捉9种古典 potentials中的相对准确性作为预测最终性能的标准。
</details></li>
</ul>
<hr>
<h2 id="Fairness-Under-Demographic-Scarce-Regime"><a href="#Fairness-Under-Demographic-Scarce-Regime" class="headerlink" title="Fairness Under Demographic Scarce Regime"></a>Fairness Under Demographic Scarce Regime</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13081">http://arxiv.org/abs/2307.13081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patrik Joslin Kenfack, Samira Ebrahimi Kahou, Ulrich Aïvodji</li>
<li>for: 本文研究了在不完整的人口信息情况下如何建立具有更好的公平精度平衡的特征分类器。</li>
<li>methods: 本文提出了一种基于不确定性认识的特征分类器建模框架，通过在不确定性较低的样本上遵循公平约束来提高公平精度平衡。</li>
<li>results: 实验结果表明，相比 класси型特征分类器，提出的框架可以在两个数据集上建立更好的公平精度平衡，并且超越了基于真实敏感特征的约束。<details>
<summary>Abstract</summary>
Most existing works on fairness assume the model has full access to demographic information. However, there exist scenarios where demographic information is partially available because a record was not maintained throughout data collection or due to privacy reasons. This setting is known as demographic scarce regime. Prior research have shown that training an attribute classifier to replace the missing sensitive attributes (proxy) can still improve fairness. However, the use of proxy-sensitive attributes worsens fairness-accuracy trade-offs compared to true sensitive attributes. To address this limitation, we propose a framework to build attribute classifiers that achieve better fairness-accuracy trade-offs. Our method introduces uncertainty awareness in the attribute classifier and enforces fairness on samples with demographic information inferred with the lowest uncertainty. We show empirically that enforcing fairness constraints on samples with uncertain sensitive attributes is detrimental to fairness and accuracy. Our experiments on two datasets showed that the proposed framework yields models with significantly better fairness-accuracy trade-offs compared to classic attribute classifiers. Surprisingly, our framework outperforms models trained with constraints on the true sensitive attributes.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:大多数现有的公平假设模型拥有完整的人口信息。然而，有些场景下，人口信息只有部分可用，例如因为数据采集或隐私原因。这种情况被称为“人口缺乏Registry”。先前的研究表明，使用代理敏感特征来替代缺失的人口信息可以改善公平。但是，使用代理敏感特征会对公平准确性负面影响。为解决这种限制，我们提出了一个框架，用于建立具有更好的公平准确性质量的特征分类器。我们的方法将不确定性意识引入特征分类器，并在具有最低不确定性的人口信息上遵循公平约束。我们经验表明，对不确定的敏感特征进行公平约束是对公平和准确性产生负面影响。我们在两个 dataset 上进行了实验，结果显示，我们的框架可以与经典特征分类器相比，在公平准确性质量上获得显著改善。 surprisingly，我们的框架还超过了基于真实敏感特征的约束。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Certified-Training-Towards-Better-Accuracy-Robustness-Tradeoffs"><a href="#Adaptive-Certified-Training-Towards-Better-Accuracy-Robustness-Tradeoffs" class="headerlink" title="Adaptive Certified Training: Towards Better Accuracy-Robustness Tradeoffs"></a>Adaptive Certified Training: Towards Better Accuracy-Robustness Tradeoffs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13078">http://arxiv.org/abs/2307.13078</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhakshylyk Nurlanov, Frank R. Schmidt, Florian Bernard</li>
<li>for: 提高模型的鲁棒性和标准准确率之间的负荷融合。</li>
<li>methods: 基于适应证明半径的新训练方法，通过改进模型的准确率和鲁棒性来提高模型的标准准确率和鲁棒性。</li>
<li>results: 在MNIST、CIFAR-10和TinyImageNet等 datasets上实现了更高的鲁棒性和标准准确率之间的负荷融合，特别是在CIFAR-10和TinyImageNet上，模型的鲁棒性可以提高至两倍的水平，而无需增加训练时间或资源。<details>
<summary>Abstract</summary>
As deep learning models continue to advance and are increasingly utilized in real-world systems, the issue of robustness remains a major challenge. Existing certified training methods produce models that achieve high provable robustness guarantees at certain perturbation levels. However, the main problem of such models is a dramatically low standard accuracy, i.e. accuracy on clean unperturbed data, that makes them impractical. In this work, we consider a more realistic perspective of maximizing the robustness of a model at certain levels of (high) standard accuracy. To this end, we propose a novel certified training method based on a key insight that training with adaptive certified radii helps to improve both the accuracy and robustness of the model, advancing state-of-the-art accuracy-robustness tradeoffs. We demonstrate the effectiveness of the proposed method on MNIST, CIFAR-10, and TinyImageNet datasets. Particularly, on CIFAR-10 and TinyImageNet, our method yields models with up to two times higher robustness, measured as an average certified radius of a test set, at the same levels of standard accuracy compared to baseline approaches.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="General-Purpose-Multi-Modal-OOD-Detection-Framework"><a href="#General-Purpose-Multi-Modal-OOD-Detection-Framework" class="headerlink" title="General-Purpose Multi-Modal OOD Detection Framework"></a>General-Purpose Multi-Modal OOD Detection Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13069">http://arxiv.org/abs/2307.13069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Viet Duong, Qiong Wu, Zhengyi Zhou, Eric Zavesky, Jiahe Chen, Xiangzhou Liu, Wen-Ling Hsu, Huajie Shao</li>
<li>for: 本研究旨在提出一种普适的弱监督异常检测框架，以便同时在多个不同的异常场景中准确地检测异常样本。</li>
<li>methods: 我们提出了一种结合二分类器和对比学习组件的总体异常检测方法，并采用了尖锥损失来约束各种样本的几何表示之间的相似性。</li>
<li>results: 我们在多个实际世界数据集上进行了实验，并证明了我们的方法可以在三个不同的异常场景中同时具有高精度的异常检测能力，而且超过了现有的状态艺法。<details>
<summary>Abstract</summary>
Out-of-distribution (OOD) detection identifies test samples that differ from the training data, which is critical to ensuring the safety and reliability of machine learning (ML) systems. While a plethora of methods have been developed to detect uni-modal OOD samples, only a few have focused on multi-modal OOD detection. Current contrastive learning-based methods primarily study multi-modal OOD detection in a scenario where both a given image and its corresponding textual description come from a new domain. However, real-world deployments of ML systems may face more anomaly scenarios caused by multiple factors like sensor faults, bad weather, and environmental changes. Hence, the goal of this work is to simultaneously detect from multiple different OOD scenarios in a fine-grained manner. To reach this goal, we propose a general-purpose weakly-supervised OOD detection framework, called WOOD, that combines a binary classifier and a contrastive learning component to reap the benefits of both. In order to better distinguish the latent representations of in-distribution (ID) and OOD samples, we adopt the Hinge loss to constrain their similarity. Furthermore, we develop a new scoring metric to integrate the prediction results from both the binary classifier and contrastive learning for identifying OOD samples. We evaluate the proposed WOOD model on multiple real-world datasets, and the experimental results demonstrate that the WOOD model outperforms the state-of-the-art methods for multi-modal OOD detection. Importantly, our approach is able to achieve high accuracy in OOD detection in three different OOD scenarios simultaneously. The source code will be made publicly available upon publication.
</details>
<details>
<summary>摘要</summary>
外部数据（OOD）检测是确保机器学习（ML）系统的安全性和可靠性的关键之一。虽然许多方法已经开发出来检测uni-modal OOD样本，但只有一些关注多模态OOD检测。现有的对比学习基于方法主要在新领域中检测多modal OOD样本。但实际世界中部署ML系统可能会遇到更多的异常情况，如感知器故障、坏天气和环境变化。因此，本研究的目标是同时从多个不同的OOD场景中 simultanously检测ID和OOD样本。为达到这个目标，我们提议一种通用弱监督OOD检测框架，called WOOD，该框架结合了一个二分类器和一个对比学习组件，以便汲取两者的优点。为了更好地分解ID和OOD样本的准确表示，我们采用了尖锥损函数来约束 их相似性。此外，我们开发了一个新的评分度量来整合 binary 类ifier和对比学习的预测结果，以便更好地识别OOD样本。我们对多个实际世界数据集进行了实验，并实验结果表明，提议的WOOD模型在多modal OOD检测中超过了当前状态艺技。特别是，我们的方法可以同时在三个不同的OOD场景中达到高精度的OOD检测。代码将在发表后公开。
</details></li>
</ul>
<hr>
<h2 id="Personalized-Category-Frequency-prediction-for-Buy-It-Again-recommendations"><a href="#Personalized-Category-Frequency-prediction-for-Buy-It-Again-recommendations" class="headerlink" title="Personalized Category Frequency prediction for Buy It Again recommendations"></a>Personalized Category Frequency prediction for Buy It Again recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01195">http://arxiv.org/abs/2308.01195</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amit Pande, Kunal Ghosh, Rankyung Park</li>
<li>for: 提高用户体验和网站参与度，通过建议客户可能会购买的商品</li>
<li>methods: 基于个性化类别模型（PC模型）和个性化类别内Item模型（IC模型）的层次PCIC模型，包括个性化类别生成和个性化类别内Item排名</li>
<li>results: 与12基准模型进行比较，NDCG提高到16%，回归提高约2%，并在大规模数据集上进行了扩展和训练（超过8个小时），并在一家大型零售商的官方网站上进行了AB测试，得到了显著的用户参与度增长<details>
<summary>Abstract</summary>
Buy It Again (BIA) recommendations are crucial to retailers to help improve user experience and site engagement by suggesting items that customers are likely to buy again based on their own repeat purchasing patterns. Most existing BIA studies analyze guests personalized behavior at item granularity. A category-based model may be more appropriate in such scenarios. We propose a recommendation system called a hierarchical PCIC model that consists of a personalized category model (PC model) and a personalized item model within categories (IC model). PC model generates a personalized list of categories that customers are likely to purchase again. IC model ranks items within categories that guests are likely to consume within a category. The hierarchical PCIC model captures the general consumption rate of products using survival models. Trends in consumption are captured using time series models. Features derived from these models are used in training a category-grained neural network. We compare PCIC to twelve existing baselines on four standard open datasets. PCIC improves NDCG up to 16 percent while improving recall by around 2 percent. We were able to scale and train (over 8 hours) PCIC on a large dataset of 100M guests and 3M items where repeat categories of a guest out number repeat items. PCIC was deployed and AB tested on the site of a major retailer, leading to significant gains in guest engagement.
</details>
<details>
<summary>摘要</summary>
请购买再次（BIA）建议对零售商来说非常重要，可以帮助改善用户体验和网站参与度，通过建议客户可能会再次购买的商品，以便客户可以更好地找到他们需要的商品。大多数现有的BIA研究都是分析客户个性化行为的项目粒度。在这种情况下，一个类别基于的模型可能更加适合。我们提出了一种推荐系统，即层次PCIC模型，它包括个性化类别模型（PC模型）和个性化类别内项模型（IC模型）。PC模型生成了个性化的类别列表，客户可能会再次购买的类别。IC模型将类别内的项目排名，用户可能会在类别内消耗的项目。层次PCIC模型使用生存模型捕捉总体产品的消耗率，并使用时间序列模型捕捉消耗趋势。这些模型的特征被用于训练一个类别粒度的神经网络。我们与12个基线对比PCIC，在四个标准开放数据集上。PCIC提高了NDCG达16%，同时提高了回归率约2%。我们可以在8个小时内扩展和训练PCIC，并在3000万名客户和3000万个商品的大数据集上进行了大规模的实验。PCIC被部署到一家大型零售商的官方网站上，并通过AB测试，导致了网站参与度的明显增长。
</details></li>
</ul>
<hr>
<h2 id="Feature-Gradient-Flow-for-Interpreting-Deep-Neural-Networks-in-Head-and-Neck-Cancer-Prediction"><a href="#Feature-Gradient-Flow-for-Interpreting-Deep-Neural-Networks-in-Head-and-Neck-Cancer-Prediction" class="headerlink" title="Feature Gradient Flow for Interpreting Deep Neural Networks in Head and Neck Cancer Prediction"></a>Feature Gradient Flow for Interpreting Deep Neural Networks in Head and Neck Cancer Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13061">http://arxiv.org/abs/2307.13061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinzhu Jin, Jonathan C. Garneau, P. Thomas Fletcher</li>
<li>for: 这个论文是为了解释深度学习模型，以便更好地理解它们如何做出决策。</li>
<li>methods: 这篇论文使用了一种新的技术，即特征涌流，来解释深度学习模型。特征涌流是指模型在输入数据空间中的非线性坐标，表示模型使用的信息来做出决策。论文中的想法是测量模型的解释特征与模型的涌流之间的一致度，以评估特定特征对模型的重要性。</li>
<li>results: 论文在使用特征涌流来评估模型的解释特征方面进行了实验，并发现这种方法可以帮助提高模型的解释性。此外，论文还提出了一种在训练深度学习模型时添加一个regularization term来鼓励模型涌流与选择的解释特征相一致的技术。<details>
<summary>Abstract</summary>
This paper introduces feature gradient flow, a new technique for interpreting deep learning models in terms of features that are understandable to humans. The gradient flow of a model locally defines nonlinear coordinates in the input data space representing the information the model is using to make its decisions. Our idea is to measure the agreement of interpretable features with the gradient flow of a model. To then evaluate the importance of a particular feature to the model, we compare that feature's gradient flow measure versus that of a baseline noise feature. We then develop a technique for training neural networks to be more interpretable by adding a regularization term to the loss function that encourages the model gradients to align with those of chosen interpretable features. We test our method in a convolutional neural network prediction of distant metastasis of head and neck cancer from a computed tomography dataset from the Cancer Imaging Archive.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MARIO-Model-Agnostic-Recipe-for-Improving-OOD-Generalization-of-Graph-Contrastive-Learning"><a href="#MARIO-Model-Agnostic-Recipe-for-Improving-OOD-Generalization-of-Graph-Contrastive-Learning" class="headerlink" title="MARIO: Model Agnostic Recipe for Improving OOD Generalization of Graph Contrastive Learning"></a>MARIO: Model Agnostic Recipe for Improving OOD Generalization of Graph Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13055">http://arxiv.org/abs/2307.13055</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhuyun97/mario">https://github.com/zhuyun97/mario</a></li>
<li>paper_authors: Yun Zhu, Haizhou Shi, Zhenshuo Zhang, Siliang Tang</li>
<li>for: 这份研究实验是为了解决无监督学习方法在 graf 数据上的 OUT-OF-DISTRIBUTION（OOD）扩展问题。</li>
<li>methods: 我们提出了一个名为 MARIO（Model-Agnostic Recipe for Improving OOD Generalizability）的方法，它运用了两个原则来发展具有分布偏移适应能力的graph对照学习方法：(i) 信息瓶颈（IB）原则，以获得一般化表示，以及(ii) 不变原则，通过对数据进行数据增强来获得不变的表示。</li>
<li>results: 我们通过实验示出，我们的方法可以在OOD测试集上实现最佳性能，同时与已有方法相比，在内部测试集上的性能保持相似。另外，我们的方法可以对数据进行增强，并且可以适应不同的分布偏移。<details>
<summary>Abstract</summary>
In this work, we investigate the problem of out-of-distribution (OOD) generalization for unsupervised learning methods on graph data. This scenario is particularly challenging because graph neural networks (GNNs) have been shown to be sensitive to distributional shifts, even when labels are available. To address this challenge, we propose a \underline{M}odel-\underline{A}gnostic \underline{R}ecipe for \underline{I}mproving \underline{O}OD generalizability of unsupervised graph contrastive learning methods, which we refer to as MARIO. MARIO introduces two principles aimed at developing distributional-shift-robust graph contrastive methods to overcome the limitations of existing frameworks: (i) Information Bottleneck (IB) principle for achieving generalizable representations and (ii) Invariant principle that incorporates adversarial data augmentation to obtain invariant representations. To the best of our knowledge, this is the first work that investigates the OOD generalization problem of graph contrastive learning, with a specific focus on node-level tasks. Through extensive experiments, we demonstrate that our method achieves state-of-the-art performance on the OOD test set, while maintaining comparable performance on the in-distribution test set when compared to existing approaches. The source code for our method can be found at: https://github.com/ZhuYun97/MARIO
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们调查Graph数据上无监督学习方法的out-of-distribution（OOD）泛化问题。这种情况特别具有挑战性，因为图神经网络（GNNs）已经显示出对分布变换的敏感性，即使标签可用。为解决这个挑战，我们提议一种模型无关的recipe，我们称之为MARIO。MARIO包括两个原则，旨在开发对图数据进行分布变换的Robust graph contrastive学习方法，以超越现有框架的局限性：（i）信息瓶颈（IB）原则，以实现通用表示，和（ii）不变原则，通过对数据进行敌对数据增强来获得不变的表示。根据我们所知，这是第一个对图 contrastive学习的OOD泛化问题进行调查的研究，具体关注节点级任务。通过广泛的实验，我们证明了我们的方法在OOD测试集上具有状态机器的性能，而与现有方法相比，在同样的数据集上保持相对性的表现。MARIO的代码可以在以下链接找到：https://github.com/ZhuYun97/MARIO
</details></li>
</ul>
<hr>
<h2 id="Parallel-Q-Learning-Scaling-Off-policy-Reinforcement-Learning-under-Massively-Parallel-Simulation"><a href="#Parallel-Q-Learning-Scaling-Off-policy-Reinforcement-Learning-under-Massively-Parallel-Simulation" class="headerlink" title="Parallel $Q$-Learning: Scaling Off-policy Reinforcement Learning under Massively Parallel Simulation"></a>Parallel $Q$-Learning: Scaling Off-policy Reinforcement Learning under Massively Parallel Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12983">http://arxiv.org/abs/2307.12983</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Improbable-AI/pql">https://github.com/Improbable-AI/pql</a></li>
<li>paper_authors: Zechu Li, Tao Chen, Zhang-Wei Hong, Anurag Ajay, Pulkit Agrawal</li>
<li>for: This paper focuses on improving the efficiency of reinforcement learning for complex tasks using parallelization and GPU-based simulation.</li>
<li>methods: The paper proposes a Parallel $Q$-Learning (PQL) scheme that parallelizes data collection, policy learning, and value learning to achieve better sample efficiency and faster training times.</li>
<li>results: The authors demonstrate that PQL outperforms Proximal Policy Optimization (PPO) in wall-clock time while maintaining superior sample efficiency, and they scale the algorithm to tens of thousands of parallel environments using a single workstation.<details>
<summary>Abstract</summary>
Reinforcement learning is time-consuming for complex tasks due to the need for large amounts of training data. Recent advances in GPU-based simulation, such as Isaac Gym, have sped up data collection thousands of times on a commodity GPU. Most prior works used on-policy methods like PPO due to their simplicity and ease of scaling. Off-policy methods are more data efficient but challenging to scale, resulting in a longer wall-clock training time. This paper presents a Parallel $Q$-Learning (PQL) scheme that outperforms PPO in wall-clock time while maintaining superior sample efficiency of off-policy learning. PQL achieves this by parallelizing data collection, policy learning, and value learning. Different from prior works on distributed off-policy learning, such as Apex, our scheme is designed specifically for massively parallel GPU-based simulation and optimized to work on a single workstation. In experiments, we demonstrate that $Q$-learning can be scaled to \textit{tens of thousands of parallel environments} and investigate important factors affecting learning speed. The code is available at https://github.com/Improbable-AI/pql.
</details>
<details>
<summary>摘要</summary>
“强化学习需要大量的训练数据，复杂任务的强化学习需要更多的训练时间。现代GPU加速的 simulate Isaac Gym 可以在常规GPU上速度上千倍增加数据收集。大多数先前的工作使用了on-policy方法，如PPO，因为它们简单易扩展。off-policy方法更有效率，但是它们在扩展上具有挑战，导致训练时间更长。本文提出了并行$Q$-学习（PQL）方案，其在wall-clock时间内超过PPO，同时保持了离线学习的样本效率。PQL实现了这一点通过并行数据收集、政策学习和价值学习的并行。与先前的分布式离线学习方法，如Apex，不同的是，我们的方案专门针对大规模并行GPU基础设施的 simulate，并且优化了在单个工作站上运行。在实验中，我们示出了可以扩展到数以千计的并行环境，并调查了影响学习速度的重要因素。代码可以在https://github.com/Improbable-AI/pql上获取。”Note that Simplified Chinese is a standardized form of Chinese that is used in mainland China and Singapore. Traditional Chinese is used in Taiwan, Hong Kong, and other parts of the world where traditional Chinese characters are used.
</details></li>
</ul>
<hr>
<h2 id="3D-LLM-Injecting-the-3D-World-into-Large-Language-Models"><a href="#3D-LLM-Injecting-the-3D-World-into-Large-Language-Models" class="headerlink" title="3D-LLM: Injecting the 3D World into Large Language Models"></a>3D-LLM: Injecting the 3D World into Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12981">http://arxiv.org/abs/2307.12981</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/UMass-Foundation-Model/3D-LLM">https://github.com/UMass-Foundation-Model/3D-LLM</a></li>
<li>paper_authors: Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, Chuang Gan</li>
<li>for: 这个研究旨在提出一种新的3D语言模型（3D-LLM），用于描述和理解3D物体的空间关系和物理特性。</li>
<li>methods: 该研究使用了三种提示机制，并使用了3D特征提取器和2D视语言模型（VLM）作为后备网络，以有效地训练3D语言模型。</li>
<li>results: 实验结果表明，该模型在ScanQA测试集上比基eline模型表现出色，例如BLEU-1分数高于状态艺术分数 by 9%。此外，在3D描述、任务组合和3D协助对话等测试集上，该模型也比2D VLM更高。qualitative例子还显示了该模型可以执行更多的任务，超出现有LLMs和VLMs的范畴。<details>
<summary>Abstract</summary>
Large language models (LLMs) and Vision-Language Models (VLMs) have been proven to excel at multiple tasks, such as commonsense reasoning. Powerful as these models can be, they are not grounded in the 3D physical world, which involves richer concepts such as spatial relationships, affordances, physics, layout, and so on. In this work, we propose to inject the 3D world into large language models and introduce a whole new family of 3D-LLMs. Specifically, 3D-LLMs can take 3D point clouds and their features as input and perform a diverse set of 3D-related tasks, including captioning, dense captioning, 3D question answering, task decomposition, 3D grounding, 3D-assisted dialog, navigation, and so on. Using three types of prompting mechanisms that we design, we are able to collect over 300k 3D-language data covering these tasks. To efficiently train 3D-LLMs, we first utilize a 3D feature extractor that obtains 3D features from rendered multi- view images. Then, we use 2D VLMs as our backbones to train our 3D-LLMs. By introducing a 3D localization mechanism, 3D-LLMs can better capture 3D spatial information. Experiments on ScanQA show that our model outperforms state-of-the-art baselines by a large margin (e.g., the BLEU-1 score surpasses state-of-the-art score by 9%). Furthermore, experiments on our held-in datasets for 3D captioning, task composition, and 3D-assisted dialogue show that our model outperforms 2D VLMs. Qualitative examples also show that our model could perform more tasks beyond the scope of existing LLMs and VLMs. Project Page: : https://vis-www.cs.umass.edu/3dllm/.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）和视力语言模型（VLM）已经被证明可以在多个任务中表现出色，如常识验。尽管这些模型强大，但它们不是基于三维物理世界，这个世界包含丰富的概念，例如空间关系、可用性、物理、布局等。在这个工作中，我们提议将三维世界注入到大型语言模型中，并介绍一个全新的三维语言模型家族。特别是，三维语言模型可以将三维点云和它们的特征作为输入，并完成一系列三维相关任务，包括描述、密集描述、三维问题回答、任务分解、三维落实、三维辅助对话、NAVIGATION等。使用我们设计的三种推问机制，我们可以收集超过300,000个三维语言数据，覆盖这些任务。为了效率地训练三维语言模型，我们首先使用三维特征提取器，从生成的多视角图像中获取三维特征。然后，我们使用2D VLM作为我们的背景来训练我们的三维语言模型。通过引入三维本地化机制，三维语言模型可以更好地捕捉三维空间信息。实验结果显示，我们的模型在ScanQA上表现出色，输出的BLEU-1分数高于现有基eline的分数（例如，BLEU-1分数比基eline分数高出9%）。此外，我们在我们的保留集上进行了3D描述、任务分解和3D辅助对话的实验，结果显示我们的模型在2D VLM上进行了更好的表现。质性例子也显示了我们的模型可以进行更多的任务，超过现有的LLM和VLM的范围。Project Page: <https://vis-www.cs.umass.edu/3dllm/>.
</details></li>
</ul>
<hr>
<h2 id="An-Isometric-Stochastic-Optimizer"><a href="#An-Isometric-Stochastic-Optimizer" class="headerlink" title="An Isometric Stochastic Optimizer"></a>An Isometric Stochastic Optimizer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12979">http://arxiv.org/abs/2307.12979</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacob Jackson</li>
<li>for: 这篇论文是为了解释 Adam 优化器在深度学习应用中的成功，以及基于这一原理提出一种新的优化器 Iso。</li>
<li>methods: 该论文使用 Adam 优化器的原理，即每个参数的步长独立于其他参数的 нор。基于这一原理，提出一种新的优化器 Iso，其中每个参数的更新量的 norm 不受输入和输出的线性变换影响。</li>
<li>results: 论文提出的 IsoAdam 可以在训练小型 Transformer 时获得速度提升，并且可以将 Adam 中的优化参数传递给 IsoAdam，以便更好地控制训练过程。<details>
<summary>Abstract</summary>
The Adam optimizer is the standard choice in deep learning applications. I propose a simple explanation of Adam's success: it makes each parameter's step size independent of the norms of the other parameters. Based on this principle I derive Iso, a new optimizer which makes the norm of a parameter's update invariant to the application of any linear transformation to its inputs and outputs. I develop a variant of Iso called IsoAdam that allows optimal hyperparameters to be transferred from Adam, and demonstrate that IsoAdam obtains a speedup over Adam when training a small Transformer.
</details>
<details>
<summary>摘要</summary>
“阿达优化器是深度学习应用中的标准选择。我提出了阿达优化器的成功的简单解释：它使每个参数的步长独立于另一个参数的 нор。基于这个原理，我 derive Iso，一个新的优化器，其中每个参数的更新 norm 对于任何线性变换输入和输出的应用是不变。我开发了 Iso 的一种变体called IsoAdam，允许从 Adam 传输优化参数，并证明 IsoAdam 在训练小Transformer时比 Adam 快速。”Note that the word "Transformer" in the last sentence is translated as "小Transformer" (little Transformer) in Simplified Chinese, as "Transformer" is a proper noun and should be capitalized in English.
</details></li>
</ul>
<hr>
<h2 id="Provable-Benefits-of-Policy-Learning-from-Human-Preferences-in-Contextual-Bandit-Problems"><a href="#Provable-Benefits-of-Policy-Learning-from-Human-Preferences-in-Contextual-Bandit-Problems" class="headerlink" title="Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems"></a>Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12975">http://arxiv.org/abs/2307.12975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Ji, Huazheng Wang, Minshuo Chen, Tuo Zhao, Mengdi Wang</li>
<li>for: This paper focuses on the problem of reward engineering in decision-making, and how human feedback can be used to learn a reward function.</li>
<li>methods: The authors develop a theory that compares the suboptimality of policy learning methods with and without human feedback, and show that preference-based methods have lower suboptimality.</li>
<li>results: The authors prove that preference-based methods enjoy lower suboptimality in offline contextual bandits, and demonstrate the benefits of using human feedback to learn a reward function.<details>
<summary>Abstract</summary>
A crucial task in decision-making problems is reward engineering. It is common in practice that no obvious choice of reward function exists. Thus, a popular approach is to introduce human feedback during training and leverage such feedback to learn a reward function. Among all policy learning methods that use human feedback, preference-based methods have demonstrated substantial success in recent empirical applications such as InstructGPT. In this work, we develop a theory that provably shows the benefits of preference-based methods in offline contextual bandits. In particular, we improve the modeling and suboptimality analysis for running policy learning methods on human-scored samples directly. Then, we compare it with the suboptimality guarantees of preference-based methods and show that preference-based methods enjoy lower suboptimality.
</details>
<details>
<summary>摘要</summary>
决策问题中一项重要任务是奖励工程。在实践中，没有明显的奖励函数选择。因此，一种popular的方法是在训练过程中引入人工反馈，并利用这些反馈来学习奖励函数。在所有基于policy学习方法中使用人工反馈的情况下，我们发展了一种理论，可以证明 preference-based方法在Offline Contextual Bandits中的优点。具体来说，我们改进了运行policy学习方法直接使用人工分配的样本的模型和不优等分析。然后，我们与 preference-based方法的不优等保证进行比较，并证明 preference-based方法具有更低的不优等。
</details></li>
</ul>
<hr>
<h2 id="Big-Data-Supply-Chain-Management-Framework-for-Forecasting-Data-Preprocessing-and-Machine-Learning-Techniques"><a href="#Big-Data-Supply-Chain-Management-Framework-for-Forecasting-Data-Preprocessing-and-Machine-Learning-Techniques" class="headerlink" title="Big Data - Supply Chain Management Framework for Forecasting: Data Preprocessing and Machine Learning Techniques"></a>Big Data - Supply Chain Management Framework for Forecasting: Data Preprocessing and Machine Learning Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12971">http://arxiv.org/abs/2307.12971</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zeniSoida/pl1">https://github.com/zeniSoida/pl1</a></li>
<li>paper_authors: Md Abrar Jahin, Md Sakib Hossain Shovon, Jungpil Shin, Istiyaque Ahmed Ridoy, Yoichi Tomioka, M. F. Mridha</li>
<li>for: This paper aims to systematically identify and comparatively analyze state-of-the-art supply chain (SC) forecasting strategies and technologies, and to propose a novel framework incorporating Big Data Analytics in SC Management.</li>
<li>methods: The proposed framework includes problem identification, data sources, exploratory data analysis, machine-learning model training, hyperparameter tuning, performance evaluation, and optimization.</li>
<li>results: The paper discusses the need for different types of forecasting according to the period or SC objective, and recommends SC KPIs and error-measurement systems to optimize the top-performing model. It also illustrates the adverse effects of phantom inventory on forecasting and the dependence of managerial decisions on the SC KPIs for determining model performance parameters and improving operations management, transparency, and planning efficiency.Here are the three points in Simplified Chinese text:</li>
<li>for: 这篇论文目的是系统地研究现代供应链（SC）预测策略和技术，并提出一种 incorporating Big Data Analytics in SC Management的新框架。</li>
<li>methods: 该框架包括问题标识、数据源、探索数据分析、机器学习模型训练、参数优化、性能评估和优化。</li>
<li>results: 论文介绍了不同时期或SC目标的预测需求，并建议SC KPIs和错误度量系统来优化最佳模型。它还图示了预测phantom存在的副作用和SC KPIs对管理决策的依赖关系，以及如何通过SC KPIs来提高运营管理、透明度和规划效率。<details>
<summary>Abstract</summary>
This article intends to systematically identify and comparatively analyze state-of-the-art supply chain (SC) forecasting strategies and technologies. A novel framework has been proposed incorporating Big Data Analytics in SC Management (problem identification, data sources, exploratory data analysis, machine-learning model training, hyperparameter tuning, performance evaluation, and optimization), forecasting effects on human-workforce, inventory, and overall SC. Initially, the need to collect data according to SC strategy and how to collect them has been discussed. The article discusses the need for different types of forecasting according to the period or SC objective. The SC KPIs and the error-measurement systems have been recommended to optimize the top-performing model. The adverse effects of phantom inventory on forecasting and the dependence of managerial decisions on the SC KPIs for determining model performance parameters and improving operations management, transparency, and planning efficiency have been illustrated. The cyclic connection within the framework introduces preprocessing optimization based on the post-process KPIs, optimizing the overall control process (inventory management, workforce determination, cost, production and capacity planning). The contribution of this research lies in the standard SC process framework proposal, recommended forecasting data analysis, forecasting effects on SC performance, machine learning algorithms optimization followed, and in shedding light on future research.
</details>
<details>
<summary>摘要</summary>
Firstly, the article discusses the importance of collecting data according to SC strategy and the various types of forecasting needed based on the period or SC objective. The article also recommends using SC KPIs and error-measurement systems to optimize the top-performing model. Additionally, the adverse effects of phantom inventory on forecasting and the dependence of managerial decisions on SC KPIs for determining model performance parameters and improving operations management, transparency, and planning efficiency are highlighted.The proposed framework includes a cyclic connection that optimizes preprocessing based on post-process KPIs, which in turn optimizes the overall control process, including inventory management, workforce determination, cost, production, and capacity planning. The contribution of this research lies in the standard SC process framework proposal, the recommended forecasting data analysis, the forecasting effects on SC performance, the optimization of machine learning algorithms, and the shedding of light on future research directions.
</details></li>
</ul>
<hr>
<h2 id="A-Connection-between-One-Step-Regularization-and-Critic-Regularization-in-Reinforcement-Learning"><a href="#A-Connection-between-One-Step-Regularization-and-Critic-Regularization-in-Reinforcement-Learning" class="headerlink" title="A Connection between One-Step Regularization and Critic Regularization in Reinforcement Learning"></a>A Connection between One-Step Regularization and Critic Regularization in Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12968">http://arxiv.org/abs/2307.12968</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ben-eysenbach/ac-connection">https://github.com/ben-eysenbach/ac-connection</a></li>
<li>paper_authors: Benjamin Eysenbach, Matthieu Geist, Sergey Levine, Ruslan Salakhutdinov</li>
<li>for: 这个论文主要关注在Offline Reinforcement Learning（RL）中，提出了一种新的方法来处理有限数据的问题。</li>
<li>methods: 论文使用了一种新的多步批处理方法，称为多步批处理评估器补偿（Multi-Step Critic Regularization），以及一种已有的一步政策提高方法（One-Step Policy Improvement）。</li>
<li>results: 论文通过实验表明，Multi-Step Critic Regularization方法可以与一步政策提高方法相比肯定地性能，但是需要更多的计算资源。此外，论文还发现，在实际应用中，使用一步政策提高方法可以达到相对较好的性能。<details>
<summary>Abstract</summary>
As with any machine learning problem with limited data, effective offline RL algorithms require careful regularization to avoid overfitting. One-step methods perform regularization by doing just a single step of policy improvement, while critic regularization methods do many steps of policy improvement with a regularized objective. These methods appear distinct. One-step methods, such as advantage-weighted regression and conditional behavioral cloning, truncate policy iteration after just one step. This ``early stopping'' makes one-step RL simple and stable, but can limit its asymptotic performance. Critic regularization typically requires more compute but has appealing lower-bound guarantees. In this paper, we draw a close connection between these methods: applying a multi-step critic regularization method with a regularization coefficient of 1 yields the same policy as one-step RL. While practical implementations violate our assumptions and critic regularization is typically applied with smaller regularization coefficients, our experiments nevertheless show that our analysis makes accurate, testable predictions about practical offline RL methods (CQL and one-step RL) with commonly-used hyperparameters. Our results that every problem can be solved with a single step of policy improvement, but rather that one-step RL might be competitive with critic regularization on RL problems that demand strong regularization.
</details>
<details>
<summary>摘要</summary>
如同任何机器学习问题受限数据，有效的离线RL算法需要仔细的规范来避免过拟合。一步方法通过做出单一步骤的政策改进来进行规范，而批评规范方法则通过多步政策改进来实现规范。这些方法看起来各有其特点。一步方法，如利得加重回归和conditionalBehavioral Cloning，在政策迭代过程中 truncate policy iteration  после一步。这种``早停''使一步RL简单和稳定，但可能限制其极限性能。批评规范通常需要更多的计算资源，但具有吸引人的下界保证。在这篇论文中，我们 Draw a close connection between these methods：在应用多步批评规范方法时，使用规范系数为1会得到与一步RL相同的政策。虽然实践中的假设不一定成立，但我们的实验表明，我们的分析对实际的离线RL方法（CQL和一步RL）的实践中的参数进行了准确和可靠的预测。我们的结果表明，每个问题都可以通过单一步骤的政策改进来解决，但是一步RL可能与批评规范在RL问题中具有强规范的情况相继。
</details></li>
</ul>
<hr>
<h2 id="Learning-Dense-Correspondences-between-Photos-and-Sketches"><a href="#Learning-Dense-Correspondences-between-Photos-and-Sketches" class="headerlink" title="Learning Dense Correspondences between Photos and Sketches"></a>Learning Dense Correspondences between Photos and Sketches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12967">http://arxiv.org/abs/2307.12967</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cogtoolslab/photo-sketch-correspondence">https://github.com/cogtoolslab/photo-sketch-correspondence</a></li>
<li>paper_authors: Xuanchen Lu, Xiaolong Wang, Judith E Fan</li>
<li>for: 本研究旨在开发人工智能系统，能够更人类化地理解视觉图像。</li>
<li>methods: 本研究使用自我超vised学习方法，利用抽象学习和对比学习来学习 dense对应关系 между绘图和照片。</li>
<li>results: 研究发现，使用提案的方法可以比以往方法更好地预测绘图和照片之间的对应关系，但是与人类预测结果存在系统差异。<details>
<summary>Abstract</summary>
Humans effortlessly grasp the connection between sketches and real-world objects, even when these sketches are far from realistic. Moreover, human sketch understanding goes beyond categorization -- critically, it also entails understanding how individual elements within a sketch correspond to parts of the physical world it represents. What are the computational ingredients needed to support this ability? Towards answering this question, we make two contributions: first, we introduce a new sketch-photo correspondence benchmark, $\textit{PSC6k}$, containing 150K annotations of 6250 sketch-photo pairs across 125 object categories, augmenting the existing Sketchy dataset with fine-grained correspondence metadata. Second, we propose a self-supervised method for learning dense correspondences between sketch-photo pairs, building upon recent advances in correspondence learning for pairs of photos. Our model uses a spatial transformer network to estimate the warp flow between latent representations of a sketch and photo extracted by a contrastive learning-based ConvNet backbone. We found that this approach outperformed several strong baselines and produced predictions that were quantitatively consistent with other warp-based methods. However, our benchmark also revealed systematic differences between predictions of the suite of models we tested and those of humans. Taken together, our work suggests a promising path towards developing artificial systems that achieve more human-like understanding of visual images at different levels of abstraction. Project page: https://photo-sketch-correspondence.github.io
</details>
<details>
<summary>摘要</summary>
人类能够轻松地理解绘图和实际物体之间的连接，即使绘图非常不真实。此外，人类绘图理解不仅是分类，更重要的是理解绘图中的各个元素与物体实际上的部分之间的对应关系。为解答这个问题，我们提出了两个贡献：首先，我们 introduce a new sketch-photo correspondence benchmark， $\textit{PSC6k}$, which contains 150,000 annotations of 6250 sketch-photo pairs across 125 object categories, augmenting the existing Sketchy dataset with fine-grained correspondence metadata.其次，我们提出了一种自动学习的方法，用于学习绘图和照片之间的紧密对应关系。我们的模型使用一个空间变换网络来估计绘图和照片之间的扭变流，并基于最近的对话学习方法来学习对应关系。我们发现这种方法可以超过多个强大基eline，并生成与其他扭变基于方法相似的预测。然而，我们的 benchmark 还发现了人类预测和机器学习模型的系统性差异。总的来说，我们的工作表明了在不同的抽象水平上建立更人类化的视觉图像理解系统的可能性。项目页面：https://photo-sketch-correspondence.github.io
</details></li>
</ul>
<hr>
<h2 id="Synthetic-pre-training-for-neural-network-interatomic-potentials"><a href="#Synthetic-pre-training-for-neural-network-interatomic-potentials" class="headerlink" title="Synthetic pre-training for neural-network interatomic potentials"></a>Synthetic pre-training for neural-network interatomic potentials</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15714">http://arxiv.org/abs/2307.15714</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jla-gardner/nnp-pre-training">https://github.com/jla-gardner/nnp-pre-training</a></li>
<li>paper_authors: John L. A. Gardner, Kathryn T. Baker, Volker L. Deringer</li>
<li>for: 这篇论文主要用于探讨如何使用人工数据来预训育神经网络interatomic potential模型，以提高计算稳定性和精度。</li>
<li>methods: 该论文使用了一种基于graph neural network的equivariant interatomic potential模型，并使用了大量的人工数据进行预训育。</li>
<li>results: 研究发现，通过使用人工数据进行预训育，可以提高神经网络interatomic potential模型的数值稳定性和精度，并且可以避免使用大量的量子力学参考数据。<details>
<summary>Abstract</summary>
Machine learning (ML) based interatomic potentials have transformed the field of atomistic materials modelling. However, ML potentials depend critically on the quality and quantity of quantum-mechanical reference data with which they are trained, and therefore developing datasets and training pipelines is becoming an increasingly central challenge. Leveraging the idea of "synthetic" (artificial) data that is common in other areas of ML research, we here show that synthetic atomistic data, themselves obtained at scale with an existing ML potential, constitute a useful pre-training task for neural-network interatomic potential models. Once pre-trained with a large synthetic dataset, these models can be fine-tuned on a much smaller, quantum-mechanical one, improving numerical accuracy and stability in computational practice. We demonstrate feasibility for a series of equivariant graph-neural-network potentials for carbon, and we carry out initial experiments to test the limits of the approach.
</details>
<details>
<summary>摘要</summary>
Building on the idea of "synthetic" (artificial) data that is common in other areas of ML research, we show here that synthetic atomistic data, obtained at scale with an existing ML potential, can serve as a useful pre-training task for neural-network interatomic potential models. By pre-training these models with a large synthetic dataset, we can improve their numerical accuracy and stability in computational practice.We demonstrate the feasibility of this approach for a series of equivariant graph-neural-network potentials for carbon, and carry out initial experiments to test the limits of the approach.
</details></li>
</ul>
<hr>
<h2 id="Efficiently-Sampling-the-PSD-Cone-with-the-Metric-Dikin-Walk"><a href="#Efficiently-Sampling-the-PSD-Cone-with-the-Metric-Dikin-Walk" class="headerlink" title="Efficiently Sampling the PSD Cone with the Metric Dikin Walk"></a>Efficiently Sampling the PSD Cone with the Metric Dikin Walk</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12943">http://arxiv.org/abs/2307.12943</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunbum Kook, Santosh S. Vempala</li>
<li>for: 本研究旨在提高半定义程序的效率 computation.</li>
<li>methods: 本文使用了Dikin walk，并对其进行了扩展和改进，以提高混合时间和每步复杂度。</li>
<li>results: 研究人员通过适当选择 метри，使得运行时间与数量约束之间的关系变为多项式的。此外，本文还发展了内部点方法的规则，用于混合。<details>
<summary>Abstract</summary>
Semi-definite programs represent a frontier of efficient computation. While there has been much progress on semi-definite optimization, with moderate-sized instances currently solvable in practice by the interior-point method, the basic problem of sampling semi-definite solutions remains a formidable challenge. The direct application of known polynomial-time algorithms for sampling general convex bodies to semi-definite sampling leads to a prohibitively high running time. In addition, known general methods require an expensive rounding phase as pre-processing. Here we analyze the Dikin walk, by first adapting it to general metrics, then devising suitable metrics for the PSD cone with affine constraints. The resulting mixing time and per-step complexity are considerably smaller, and by an appropriate choice of the metric, the dependence on the number of constraints can be made polylogarithmic. We introduce a refined notion of self-concordant matrix functions and give rules for combining different metrics. Along the way, we further develop the theory of interior-point methods for sampling.
</details>
<details>
<summary>摘要</summary>
半定义程序表示一种高效计算的前沿。虽然有很多进步在半定义优化方面，但现在只有一些小规模实例可以通过内部点方法实际应用，而大规模实例仍然是一个困难的挑战。直接将通用曲线体的抽样算法应用到半定义抽样问题会导致非常高的运行时间。此外，已知的通用方法均需进行昂贵的舒缩阶段作为先决条件。我们分析了迪金步行，首先将其扩展到普通度量空间，然后在PSD几何中选择合适的度量。这将导致混合时间和每步复杂度减小至可polylogarithmic水平，并且通过合适的度量选择，可以消除对约束数量的依赖。我们还进一步发展了内部点方法的规范理论。
</details></li>
</ul>
<hr>
<h2 id="On-Privileged-and-Convergent-Bases-in-Neural-Network-Representations"><a href="#On-Privileged-and-Convergent-Bases-in-Neural-Network-Representations" class="headerlink" title="On Privileged and Convergent Bases in Neural Network Representations"></a>On Privileged and Convergent Bases in Neural Network Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12941">http://arxiv.org/abs/2307.12941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davis Brown, Nikhil Vyas, Yamini Bansal</li>
<li>for:  investigate whether the representations learned by neural networks possess a privileged and convergent basis</li>
<li>methods:  examine the significance of feature directions represented by individual neurons, compare the bases of networks trained with the same parameters but with varying random initializations</li>
<li>results:  (1) neural networks do not converge to a unique basis, (2) basis correlation increases significantly when a few early layers of the network are frozen identically, Linear Mode Connectivity improves with increased network width but not due to an increase in basis correlation.Here’s the full text in Simplified Chinese:</li>
<li>for: 这个研究是 investigate whether the representations learned by neural networks possess a privileged and convergent basis</li>
<li>methods: 研究具体是 examining the significance of feature directions represented by individual neurons, comparing the bases of networks trained with the same parameters but with varying random initializations</li>
<li>results: (1)  neural networks do not converge to a unique basis, (2) basis correlation increases significantly when a few early layers of the network are frozen identically, Linear Mode Connectivity improves with increased network width but not due to an increase in basis correlation.<details>
<summary>Abstract</summary>
In this study, we investigate whether the representations learned by neural networks possess a privileged and convergent basis. Specifically, we examine the significance of feature directions represented by individual neurons. First, we establish that arbitrary rotations of neural representations cannot be inverted (unlike linear networks), indicating that they do not exhibit complete rotational invariance. Subsequently, we explore the possibility of multiple bases achieving identical performance. To do this, we compare the bases of networks trained with the same parameters but with varying random initializations. Our study reveals two findings: (1) Even in wide networks such as WideResNets, neural networks do not converge to a unique basis; (2) Basis correlation increases significantly when a few early layers of the network are frozen identically.   Furthermore, we analyze Linear Mode Connectivity, which has been studied as a measure of basis correlation. Our findings give evidence that while Linear Mode Connectivity improves with increased network width, this improvement is not due to an increase in basis correlation.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们研究了神经网络学习的表示方式是否具有特权和共聚基准。 Specifically，我们研究了神经元单个方向上的特征方向是否具有重要 significanc。 我们首先证明了神经网络中的表示不能逆转（不同于线性网络），这表明它们不具备完全旋转不变性。 接着，我们探索了多个基准是否可以实现相同的性能。 To do this, we compare the bases of networks trained with the same parameters but with varying random initializations. 我们的研究发现了两点：1. 即使是宽度较大的网络，如WideResNets，神经网络也不会 converges to a unique basis。2. 基准相关性在一些早期层被冻结时显著增加。此外，我们还分析了线性模式连接，这种连接已经被研究为基准相关性的度量。 我们的发现表明，虽然线性模式连接在网络宽度增加时会改善，但这种改善不是基准相关性的提高。
</details></li>
</ul>
<hr>
<h2 id="HOOD-Real-Time-Robust-Human-Presence-and-Out-of-Distribution-Detection-with-Low-Cost-FMCW-Radar"><a href="#HOOD-Real-Time-Robust-Human-Presence-and-Out-of-Distribution-Detection-with-Low-Cost-FMCW-Radar" class="headerlink" title="HOOD: Real-Time Robust Human Presence and Out-of-Distribution Detection with Low-Cost FMCW Radar"></a>HOOD: Real-Time Robust Human Presence and Out-of-Distribution Detection with Low-Cost FMCW Radar</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02396">http://arxiv.org/abs/2308.02396</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sabri Mustafa Kahya, Muhammet Sami Yavuz, Eckehard Steinbach</li>
<li>For: 这个论文主要关注在indoor环境中的人员存在检测，使用60GHz短距离FMCW雷达，并提出了一种实时 Robust人员存在和非典型检测方法（HOOD）。* Methods: 该方法基于重构建模型，使用雷达宏和微距离Doppler图像（RDI），并将存在检测应用和非典型检测问题一起解决。* Results: 在收集的60GHz短距离FMCW雷达数据集上，HOOD方法实现了平均AUROC为94.36%，并在不同的人类情况下表现良好。此外，HOOD方法也在相对评估中表现出色，超过了现有的OOD检测方法。<details>
<summary>Abstract</summary>
Human presence detection in indoor environments using millimeter-wave frequency-modulated continuous-wave (FMCW) radar is challenging due to the presence of moving and stationary clutters in indoor places. This work proposes "HOOD" as a real-time robust human presence and out-of-distribution (OOD) detection method by exploiting 60 GHz short-range FMCW radar. We approach the presence detection application as an OOD detection problem and solve the two problems simultaneously using a single pipeline. Our solution relies on a reconstruction-based architecture and works with radar macro and micro range-Doppler images (RDIs). HOOD aims to accurately detect the "presence" of humans in the presence or absence of moving and stationary disturbers. Since it is also an OOD detector, it aims to detect moving or stationary clutters as OOD in humans' absence and predicts the current scene's output as "no presence." HOOD is an activity-free approach that performs well in different human scenarios. On our dataset collected with a 60 GHz short-range FMCW Radar, we achieve an average AUROC of 94.36%. Additionally, our extensive evaluations and experiments demonstrate that HOOD outperforms state-of-the-art (SOTA) OOD detection methods in terms of common OOD detection metrics. Our real-time experiments are available at: https://muskahya.github.io/HOOD
</details>
<details>
<summary>摘要</summary>
人体存在检测在室内环境中使用毫米波频率调制连续波 (FMCW) 雷达是具有挑战性，因为室内的移动和静止干扰物会影响检测的准确性。这项工作提出了一种实时可靠的人体存在和异常检测方法，称为“HOOD”，通过利用60GHz短距离FMCW雷达。我们将存在检测应用作为异常检测问题，并将两个问题同时解决使用单一管道。我们的解决方案基于重建建筑，并与雷达宏和微范围Doppler图像 (RDI) 结合使用。HOOD的目标是准确检测人体在存在或缺失移动和静止干扰物的情况下的存在。由于它还是异常检测器，它将在人体缺失情况下检测移动或静止干扰物为异常，并预测当前场景的输出为“无存在”。HOOD是一种无活动的方法，可以在不同的人类情况下表现良好。在我们使用60GHz短距离FMCW雷达收集的数据集上，我们实现了平均AUROC为94.36%。此外，我们的广泛的评估和实验表明，HOOD超过了当前最佳异常检测方法的表现。我们的实时实验可以在以下链接中找到：https://muskahya.github.io/HOOD
</details></li>
</ul>
<hr>
<h2 id="Contextual-Bandits-and-Imitation-Learning-via-Preference-Based-Active-Queries"><a href="#Contextual-Bandits-and-Imitation-Learning-via-Preference-Based-Active-Queries" class="headerlink" title="Contextual Bandits and Imitation Learning via Preference-Based Active Queries"></a>Contextual Bandits and Imitation Learning via Preference-Based Active Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12926">http://arxiv.org/abs/2307.12926</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ayush Sekhari, Karthik Sridharan, Wen Sun, Runzhe Wu</li>
<li>for: 本研究旨在解决 Contextual Bandits 和模仿学习问题，learner 缺乏直接行动结果奖励的知识。相反，learner 可以在每个回合中向专家查询两个动作，并获得噪声性偏好反馈。learner 的目标是同时减少执行动作的 regret，以及减少向专家查询的次数。</li>
<li>methods: 本研究使用了一种基于函数类的算法，利用在线回归权限来选择动作和决定查询专家。对 Contextual Bandits Setting 来说，我们的算法可以达到 $O(\min{\sqrt{T}, d&#x2F;\Delta})$ 的 regret bound，其中 $T$ 表示互动次数，$d$ 表示函数类的远近维度，$\Delta$ 表示最佳动作对所有上下文的最小偏好。我们的算法不需要知道 $\Delta$，并且获得的 regret bound 与标准 Contextual Bandits 设置中 observer reward signal 的情况相当。此外，我们的算法只需要向专家进行 $O(\min{T, d^2&#x2F;\Delta^2})$ 次查询。</li>
<li>results: 我们的算法可以在 Contextual Bandits 和模仿学习 setting 中减少 regret 和查询次数，同时可以在模仿学习 setting 中超越专家，即 learner 可以在专家不优秀时 Still learn to outperform the expert。这展示了 preference-based feedback 在模仿学习中的实际优势。<details>
<summary>Abstract</summary>
We consider the problem of contextual bandits and imitation learning, where the learner lacks direct knowledge of the executed action's reward. Instead, the learner can actively query an expert at each round to compare two actions and receive noisy preference feedback. The learner's objective is two-fold: to minimize the regret associated with the executed actions, while simultaneously, minimizing the number of comparison queries made to the expert. In this paper, we assume that the learner has access to a function class that can represent the expert's preference model under appropriate link functions, and provide an algorithm that leverages an online regression oracle with respect to this function class for choosing its actions and deciding when to query. For the contextual bandit setting, our algorithm achieves a regret bound that combines the best of both worlds, scaling as $O(\min\{\sqrt{T}, d/\Delta\})$, where $T$ represents the number of interactions, $d$ represents the eluder dimension of the function class, and $\Delta$ represents the minimum preference of the optimal action over any suboptimal action under all contexts. Our algorithm does not require the knowledge of $\Delta$, and the obtained regret bound is comparable to what can be achieved in the standard contextual bandits setting where the learner observes reward signals at each round. Additionally, our algorithm makes only $O(\min\{T, d^2/\Delta^2\})$ queries to the expert. We then extend our algorithm to the imitation learning setting, where the learning agent engages with an unknown environment in episodes of length $H$ each, and provide similar guarantees for regret and query complexity. Interestingly, our algorithm for imitation learning can even learn to outperform the underlying expert, when it is suboptimal, highlighting a practical benefit of preference-based feedback in imitation learning.
</details>
<details>
<summary>摘要</summary>
我们考虑了上下文游戏和仿制学习问题，learner缺乏直接行动的奖励信息。相反，learner可以在每个回合中活动地询问专家，以获得不准确的偏好反馈。learner的目标是两重：一是减少执行的奖励相关的 regret，二是减少询问专家的次数。在这篇论文中，我们假设learner有访问一个专家偏好模型函数类型的权限，并提供了一个基于这个函数类型的在线回归或acles的算法，用于选择行动和决定何时询问。为上下文游戏设置，我们的算法实现了一个 regret 的下界，其比例为 $O(\min\{\sqrt{T}, d/\Delta\})$, 其中 $T$ 表示互动次数， $d$ 表示函数类型的远征维度， $\Delta$ 表示最优行动在所有上下文中的最小偏好。我们的算法不需要了解 $\Delta$，并且获得的 regret 下界与标准上下文游戏设置中的 reward 信号observation 相当。此外，我们的算法只需要 $O(\min\{T, d^2/\Delta^2\})$ 次询问专家。然后，我们扩展了我们的算法到仿制学习设置，learner在每个回合中与未知环境互动，并提供了类似的 regret 和询问次数下界。有趣的是，我们的算法可以在仿制学习中学习出perform 更高的专家，即使专家是不优的，这 highlights 仿制学习中 preference-based 反馈的实际优势。
</details></li>
</ul>
<hr>
<h2 id="A-new-derivative-free-optimization-method-Gaussian-Crunching-Search"><a href="#A-new-derivative-free-optimization-method-Gaussian-Crunching-Search" class="headerlink" title="A new derivative-free optimization method: Gaussian Crunching Search"></a>A new derivative-free optimization method: Gaussian Crunching Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14359">http://arxiv.org/abs/2307.14359</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benny Wong</li>
<li>for: 这个研究论文的目的是提出一种新的优化方法 called Gaussian Crunching Search (GCS)，用于解决复杂问题。</li>
<li>methods: 该方法基于独特的 Gaussian distribution 行为，旨在有效地探索解决空间并尝试达到全局最优解。</li>
<li>results: 通过实验评估和与现有优化方法比较，我们展示了 GCS 的优势和特点，并证明了它在优化问题中的可行性和应用前景。<details>
<summary>Abstract</summary>
Optimization methods are essential in solving complex problems across various domains. In this research paper, we introduce a novel optimization method called Gaussian Crunching Search (GCS). Inspired by the behaviour of particles in a Gaussian distribution, GCS aims to efficiently explore the solution space and converge towards the global optimum. We present a comprehensive analysis of GCS, including its working mechanism, and potential applications. Through experimental evaluations and comparisons with existing optimization methods, we highlight the advantages and strengths of GCS. This research paper serves as a valuable resource for researchers, practitioners, and students interested in optimization, providing insights into the development and potential of Gaussian Crunching Search as a new and promising approach.
</details>
<details>
<summary>摘要</summary>
优化方法是解决复杂问题的关键，在不同领域中都非常重要。在这篇研究论文中，我们介绍了一种新的优化方法—— Gaussian Crunching Search（GCS）。GCS Drawing inspiration from the behavior of particles in a Gaussian distribution，GCS aims to efficiently explore the solution space and converge towards the global optimum。我们提供了GCS的工作机制和潜在应用，并通过实验和现有优化方法的比较，把GCS的优势和特点 highlighted。这篇研究论文对研究者、实践者和学生们有益，提供了GCS的开发和应用的深入了解，以及GCS作为一种新的优化方法的潜在价值。
</details></li>
</ul>
<hr>
<h2 id="Graph-Neural-Networks-For-Mapping-Variables-Between-Programs-–-Extended-Version"><a href="#Graph-Neural-Networks-For-Mapping-Variables-Between-Programs-–-Extended-Version" class="headerlink" title="Graph Neural Networks For Mapping Variables Between Programs – Extended Version"></a>Graph Neural Networks For Mapping Variables Between Programs – Extended Version</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13014">http://arxiv.org/abs/2307.13014</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pmorvalho/ecai23-gnns-for-mapping-variables-between-programs">https://github.com/pmorvalho/ecai23-gnns-for-mapping-variables-between-programs</a></li>
<li>paper_authors: Pedro Orvalho, Jelle Piepenbrock, Mikoláš Janota, Vasco Manquinho</li>
<li>for: 这个论文主要研究了如何使用图ael neural networks（GNNs）将两个程序之间的变量映射到一起，以便进行程序相似性、程序分析、程序修复和冲击检测等任务。</li>
<li>methods: 该论文提出了使用GNNs将两个程序的抽象 sintaxis树（ASTs）中的变量映射到一起，以便进行变量映射。</li>
<li>results:  experiments表明，该方法可以 Correctly map 83% of the evaluation dataset, 而且比现有的程序修复方法（主要基于程序结构）更高，可以修复约72%的错误程序。<details>
<summary>Abstract</summary>
Automated program analysis is a pivotal research domain in many areas of Computer Science -- Formal Methods and Artificial Intelligence, in particular. Due to the undecidability of the problem of program equivalence, comparing two programs is highly challenging. Typically, in order to compare two programs, a relation between both programs' sets of variables is required. Thus, mapping variables between two programs is useful for a panoply of tasks such as program equivalence, program analysis, program repair, and clone detection. In this work, we propose using graph neural networks (GNNs) to map the set of variables between two programs based on both programs' abstract syntax trees (ASTs). To demonstrate the strength of variable mappings, we present three use-cases of these mappings on the task of program repair to fix well-studied and recurrent bugs among novice programmers in introductory programming assignments (IPAs). Experimental results on a dataset of 4166 pairs of incorrect/correct programs show that our approach correctly maps 83% of the evaluation dataset. Moreover, our experiments show that the current state-of-the-art on program repair, greatly dependent on the programs' structure, can only repair about 72% of the incorrect programs. In contrast, our approach, which is solely based on variable mappings, can repair around 88.5%.
</details>
<details>
<summary>摘要</summary>
自动化程序分析是计算机科学多个领域的关键研究领域，特别是正式方法和人工智能。由于程序相等性的问题是不可解决的，因此比较两个程序非常困难。通常需要在两个程序的变量集之间建立一种关系，以便进行程序相等性、程序分析、程序修复和克隆检测等任务。在这项工作中，我们提议使用图神经网络（GNN）将两个程序的变量集映射到同一个空间中。为了证明变量映射的强大性，我们在程序修复任务上提供三个使用情况，包括修复 novice 程序员在入门编程作业（IPA）中经常出现的错误。实验结果表明，我们的方法可以在4166对 incorrect/correct 程序的评估集上正确地映射83%的评估集。此外，我们的实验还表明，现有的程序修复方法，很大程度上依赖于程序的结构，只能修复约72%的错误程序。相比之下，我们的方法，完全基于变量映射，可以修复约88.5%的错误程序。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/25/cs.LG_2023_07_25/" data-id="clmjn91m700640j885j4t0fwf" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/26/eess.IV_2023_07_26/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.IV - 2023-07-26 17:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/25/cs.SD_2023_07_25/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-07-25 123:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
