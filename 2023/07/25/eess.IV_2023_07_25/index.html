
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-07-25 17:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Towards Unifying Anatomy Segmentation: Automated Generation of a Full-body CT Dataset via Knowledge Aggregation and Anatomical Guidelines paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.13375 repo_url: https:&#x2F;&#x2F;g">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-07-25 17:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/07/25/eess.IV_2023_07_25/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Towards Unifying Anatomy Segmentation: Automated Generation of a Full-body CT Dataset via Knowledge Aggregation and Anatomical Guidelines paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.13375 repo_url: https:&#x2F;&#x2F;g">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-24T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:38.149Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_07_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/25/eess.IV_2023_07_25/" class="article-date">
  <time datetime="2023-07-24T16:00:00.000Z" itemprop="datePublished">2023-07-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-07-25 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Towards-Unifying-Anatomy-Segmentation-Automated-Generation-of-a-Full-body-CT-Dataset-via-Knowledge-Aggregation-and-Anatomical-Guidelines"><a href="#Towards-Unifying-Anatomy-Segmentation-Automated-Generation-of-a-Full-body-CT-Dataset-via-Knowledge-Aggregation-and-Anatomical-Guidelines" class="headerlink" title="Towards Unifying Anatomy Segmentation: Automated Generation of a Full-body CT Dataset via Knowledge Aggregation and Anatomical Guidelines"></a>Towards Unifying Anatomy Segmentation: Automated Generation of a Full-body CT Dataset via Knowledge Aggregation and Anatomical Guidelines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13375">http://arxiv.org/abs/2307.13375</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alexanderjaus/atlasdataset">https://github.com/alexanderjaus/atlasdataset</a></li>
<li>paper_authors: Alexander Jaus, Constantin Seibold, Kelsey Hermann, Alexandra Walter, Kristina Giske, Johannes Haubold, Jens Kleesiek, Rainer Stiefelhagen</li>
<li>for: 这个研究是为了生成自动生成的解剖学分割数据集，使用sequential进程，包括基于nnU-Net的pseudo标签和受体导向的pseudo标签重定向。</li>
<li>methods: 这个方法 combinese多个分割的知识库，生成了533个扫描图像的全身CT扫描数据集，提供了全面的解剖学覆盖，并且不需要手动标注。</li>
<li>results: 我们的方法通过了人工专家评估，在BTCV数据集上达到了85%的dice分数，并通过了医学有效性检查。<details>
<summary>Abstract</summary>
In this study, we present a method for generating automated anatomy segmentation datasets using a sequential process that involves nnU-Net-based pseudo-labeling and anatomy-guided pseudo-label refinement. By combining various fragmented knowledge bases, we generate a dataset of whole-body CT scans with $142$ voxel-level labels for 533 volumes providing comprehensive anatomical coverage which experts have approved. Our proposed procedure does not rely on manual annotation during the label aggregation stage. We examine its plausibility and usefulness using three complementary checks: Human expert evaluation which approved the dataset, a Deep Learning usefulness benchmark on the BTCV dataset in which we achieve 85% dice score without using its training dataset, and medical validity checks. This evaluation procedure combines scalable automated checks with labor-intensive high-quality expert checks. Besides the dataset, we release our trained unified anatomical segmentation model capable of predicting $142$ anatomical structures on CT data.
</details>
<details>
<summary>摘要</summary>
在本研究中，我们提出了一种方法，使用序列过程，使用nnU-Net基于pseudo标签和指导pseudo标签修正来生成自动化解剖数据集。通过结合不同的分割知识库，我们生成了一个整体CT扫描图像的数据集，包含142个 voxel 级别标签，对533幅图像提供了全面的解剖学覆盖，经专家认可。我们的提posed方法不依赖于手动标注 during label合并阶段。我们使用三种 complementary 检查来评估其可行性和有用性：人类专家评估，btcv dataset上的深度学习有用性 benchamrk，以及医学有效性检查。这种评估过程结合了可扩展的自动化检查和劳动密集的高质量专家检查。除了数据集之外，我们还发布了我们训练过的统一解剖 segmentation 模型，可以在CT数据上预测142个解剖结构。
</details></li>
</ul>
<hr>
<h2 id="Overcoming-Distribution-Mismatch-in-Quantizing-Image-Super-Resolution-Networks"><a href="#Overcoming-Distribution-Mismatch-in-Quantizing-Image-Super-Resolution-Networks" class="headerlink" title="Overcoming Distribution Mismatch in Quantizing Image Super-Resolution Networks"></a>Overcoming Distribution Mismatch in Quantizing Image Super-Resolution Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13337">http://arxiv.org/abs/2307.13337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheeun Hong, Kyoung Mu Lee</li>
<li>for: 这个 paper 的目的是提出一个新的对于图像超解析网络的量化方法，以缩短计算量且保持高度对于图像质量的性能。</li>
<li>methods: 这个 paper 使用了一个新的量化框架，将对于每个通道或输入图像的特征分布与量化范围进行静态匹配，而不需要在试用时进行动态适应。这个框架通过对特征分布进行直接训练时间REGULARIZATION，以降低分布差异问题。此外，为了进一步降低分布差异，paper 还引入了分布偏移，以对具有明显差异的通道进行偏移。</li>
<li>results: 实验结果显示，这个 paper 的方法可以对图像超解析网络进行有效的量化，且与同等或少于计算量的现有方法相比，可以提高图像质量。code 可以在 <a target="_blank" rel="noopener" href="https://github.com/Cheeun/ODM">https://github.com/Cheeun/ODM</a> 上获得。<details>
<summary>Abstract</summary>
Quantization is a promising approach to reduce the high computational complexity of image super-resolution (SR) networks. However, compared to high-level tasks like image classification, low-bit quantization leads to severe accuracy loss in SR networks. This is because feature distributions of SR networks are significantly divergent for each channel or input image, and is thus difficult to determine a quantization range. Existing SR quantization works approach this distribution mismatch problem by dynamically adapting quantization ranges to the variant distributions during test time. However, such dynamic adaptation incurs additional computational costs that limit the benefits of quantization. Instead, we propose a new quantization-aware training framework that effectively Overcomes the Distribution Mismatch problem in SR networks without the need for dynamic adaptation. Intuitively, the mismatch can be reduced by directly regularizing the variance in features during training. However, we observe that variance regularization can collide with the reconstruction loss during training and adversely impact SR accuracy. Thus, we avoid the conflict between two losses by regularizing the variance only when the gradients of variance regularization are cooperative with that of reconstruction. Additionally, to further reduce the distribution mismatch, we introduce distribution offsets to layers with a significant mismatch, which either scales or shifts channel-wise features. Our proposed algorithm, called ODM, effectively reduces the mismatch in distributions with minimal computational overhead. Experimental results show that ODM effectively outperforms existing SR quantization approaches with similar or fewer computations, demonstrating the importance of reducing the distribution mismatch problem. Our code is available at https://github.com/Cheeun/ODM.
</details>
<details>
<summary>摘要</summary>
“量化是一种有前途的方法，可以降低图像超分辨率网络的计算复杂度。然而，与高级任务像图像分类一样，低位数量化会导致SR网络的准确性下降。这是因为SR网络的特征分布是每个通道或输入图像都异常分散，因此决定量化范围是困难的。现有的SR量化方法通过在测试时动态适应量化范围来解决这个分布匹配问题。然而，这种动态适应带来了额外的计算成本，限制了量化的 benefita。而我们提议的新的量化掌握框架可以有效地超越分布匹配问题，无需动态适应。”“我们发现，可以通过直接在训练时对特征的方差进行正则化来减少分布匹配问题。然而，我们发现，在训练时对方差进行正则化可能会与重建loss冲突，从而 adversely affect SR准确性。因此，我们避免了这两个损失之间的冲突，通过只在方差正则化的梯度和重建loss之间协同时进行正则化。此外，为了进一步减少分布匹配问题，我们引入了分布偏移，将 channel-wise 特征的分布偏移到不同的水平。我们的提议的算法，称为ODM，可以有效地减少分布匹配问题，并且计算成本很低。”“我们的实验结果表明，ODM可以比现有的SR量化方法更高效地进行量化，同时保持相同或更多的计算资源。这表明，降低分布匹配问题的重要性。我们的代码可以在https://github.com/Cheeun/ODM中找到。”
</details></li>
</ul>
<hr>
<h2 id="A-Visual-Quality-Assessment-Method-for-Raster-Images-in-Scanned-Document"><a href="#A-Visual-Quality-Assessment-Method-for-Raster-Images-in-Scanned-Document" class="headerlink" title="A Visual Quality Assessment Method for Raster Images in Scanned Document"></a>A Visual Quality Assessment Method for Raster Images in Scanned Document</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13241">http://arxiv.org/abs/2307.13241</a></li>
<li>repo_url: None</li>
<li>paper_authors: Justin Yang, Peter Bauer, Todd Harris, Changhyung Lee, Hyeon Seok Seo, Jan P Allebach, Fengqing Zhu</li>
<li>for: 本研究探讨了扫描文档中的图像质量评估（IQA），特别是针对照片区域的视觉质量。</li>
<li>methods: 我们提出了一种基于机器学习的分类方法，用于判断扫描照片区域的视觉质量是否可接受，并使用人类评分来确定可接受的图像质量标准。</li>
<li>results: 我们的结果表明，通过在训练中添加噪声模型来模拟扫描过程中的质量下降，可以显著改善分类器的性能，以判断扫描照片区域的视觉质量是否可接受。<details>
<summary>Abstract</summary>
Image quality assessment (IQA) is an active research area in the field of image processing. Most prior works focus on visual quality of natural images captured by cameras. In this paper, we explore visual quality of scanned documents, focusing on raster image areas. Different from many existing works which aim to estimate a visual quality score, we propose a machine learning based classification method to determine whether the visual quality of a scanned raster image at a given resolution setting is acceptable. We conduct a psychophysical study to determine the acceptability at different image resolutions based on human subject ratings and use them as the ground truth to train our machine learning model. However, this dataset is unbalanced as most images were rated as visually acceptable. To address the data imbalance problem, we introduce several noise models to simulate the degradation of image quality during the scanning process. Our results show that by including augmented data in training, we can significantly improve the performance of the classifier to determine whether the visual quality of raster images in a scanned document is acceptable or not for a given resolution setting.
</details>
<details>
<summary>摘要</summary>
图像质量评估（IQA）是图像处理领域的一个活跃研究领域。大多数前一些研究都是关注自然图像摄像机拍摄的视觉质量。在这篇论文中，我们则是关注扫描文档中的矩阵图像区域的视觉质量。与许多现有的研究不同，我们提议使用机器学习基于分类方法来确定一定分辨率设置下的矩阑图像视觉质量是否可接受。我们进行了心理学研究，以确定不同分辨率下的图像质量可接受程度，并使用人类评分作为准确的标准。然而，该数据集存在偏度问题，因为大多数图像都被评为可接受。为解决这个数据偏度问题，我们引入了多种噪声模型，以模拟扫描过程中图像质量的受损。我们的结果表明，通过在训练中包含增强数据，可以显著提高类别器的性能，以确定扫描文档中矩阑图像的视觉质量是否可接受。
</details></li>
</ul>
<hr>
<h2 id="One-for-Multiple-Physics-informed-Synthetic-Data-Boosts-Generalizable-Deep-Learning-for-Fast-MRI-Reconstruction"><a href="#One-for-Multiple-Physics-informed-Synthetic-Data-Boosts-Generalizable-Deep-Learning-for-Fast-MRI-Reconstruction" class="headerlink" title="One for Multiple: Physics-informed Synthetic Data Boosts Generalizable Deep Learning for Fast MRI Reconstruction"></a>One for Multiple: Physics-informed Synthetic Data Boosts Generalizable Deep Learning for Fast MRI Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13220">http://arxiv.org/abs/2307.13220</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangziblake/pisf">https://github.com/wangziblake/pisf</a></li>
<li>paper_authors: Zi Wang, Xiaotong Yu, Chengyan Wang, Weibo Chen, Jiazheng Wang, Ying-Hua Chu, Hongwei Sun, Rushuai Li, Peiyong Li, Fan Yang, Haiwei Han, Taishan Kang, Jianzhong Lin, Chen Yang, Shufu Chang, Zhang Shi, Sha Hua, Yan Li, Juan Hu, Liuhong Zhu, Jianjun Zhou, Meijing Lin, Jiefeng Guo, Congbo Cai, Zhong Chen, Di Guo, Xiaobo Qu</li>
<li>for: 本研究旨在提高快速磁共振成像（MRI）的扫描时间，使其成为医疗诊断中的主要仪器之一。</li>
<li>methods: 本研究使用Physics-Informed Synthetic data learning框架（PISF），通过将快速MRI扫描转化为多个1D基本问题，以便泛化DL模型。同时，通过增强学习技术来提高模型的泛化能力。</li>
<li>results: 研究发现，使用PISF可以将DL模型训练在人工数据上，并在医疗实践中进行扫描，以至于可以 дости到与实际数据匹配的重建效果，同时减少了医疗数据收集的困难和成本。此外，PISF还能够在多个供应商多个中心的医疗设备上进行广泛应用，并且在10名医生的评估中得到了良好的评价。<details>
<summary>Abstract</summary>
Magnetic resonance imaging (MRI) is a principal radiological modality that provides radiation-free, abundant, and diverse information about the whole human body for medical diagnosis, but suffers from prolonged scan time. The scan time can be significantly reduced through k-space undersampling but the introduced artifacts need to be removed in image reconstruction. Although deep learning (DL) has emerged as a powerful tool for image reconstruction in fast MRI, its potential in multiple imaging scenarios remains largely untapped. This is because not only collecting large-scale and diverse realistic training data is generally costly and privacy-restricted, but also existing DL methods are hard to handle the practically inevitable mismatch between training and target data. Here, we present a Physics-Informed Synthetic data learning framework for Fast MRI, called PISF, which is the first to enable generalizable DL for multi-scenario MRI reconstruction using solely one trained model. For a 2D image, the reconstruction is separated into many 1D basic problems and starts with the 1D data synthesis, to facilitate generalization. We demonstrate that training DL models on synthetic data, integrated with enhanced learning techniques, can achieve comparable or even better in vivo MRI reconstruction compared to models trained on a matched realistic dataset, reducing the demand for real-world MRI data by up to 96%. Moreover, our PISF shows impressive generalizability in multi-vendor multi-center imaging. Its excellent adaptability to patients has been verified through 10 experienced doctors' evaluations. PISF provides a feasible and cost-effective way to markedly boost the widespread usage of DL in various fast MRI applications, while freeing from the intractable ethical and practical considerations of in vivo human data acquisitions.
</details>
<details>
<summary>摘要</summary>
Here, we present a Physics-Informed Synthetic data learning framework for Fast MRI, called PISF, which is the first to enable generalizable DL for multi-scenario MRI reconstruction using solely one trained model. For a 2D image, the reconstruction is separated into many 1D basic problems, starting with 1D data synthesis to facilitate generalization. We demonstrate that training DL models on synthetic data, integrated with enhanced learning techniques, can achieve comparable or even better in vivo MRI reconstruction compared to models trained on a matched realistic dataset, reducing the demand for real-world MRI data by up to 96%. Moreover, our PISF shows impressive generalizability in multi-vendor multi-center imaging, and its excellent adaptability to patients has been verified through 10 experienced doctors' evaluations. PISF provides a feasible and cost-effective way to markedly boost the widespread usage of DL in various fast MRI applications, while freeing from the intractable ethical and practical considerations of in vivo human data acquisitions.
</details></li>
</ul>
<hr>
<h2 id="Magnetic-Resonance-Parameter-Mapping-using-Self-supervised-Deep-Learning-with-Model-Reinforcement"><a href="#Magnetic-Resonance-Parameter-Mapping-using-Self-supervised-Deep-Learning-with-Model-Reinforcement" class="headerlink" title="Magnetic Resonance Parameter Mapping using Self-supervised Deep Learning with Model Reinforcement"></a>Magnetic Resonance Parameter Mapping using Self-supervised Deep Learning with Model Reinforcement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13211">http://arxiv.org/abs/2307.13211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wanyu Bian, Albert Jang, Fang Liu<br>for:* This paper proposes a novel self-supervised learning method, RELAX-MORE, for quantitative MRI (qMRI) reconstruction.methods:* The proposed method uses an optimization algorithm to unroll a model-based qMRI reconstruction into a deep learning framework.results:* The proposed method demonstrates excellent performance in reconstructing MR parameters, correcting imaging artifacts, removing noises, and recovering image features at imperfect imaging conditions.Here is the simplified Chinese text:for:* 这篇论文提出了一种新的自动学习方法，即RELAX-MORE，用于快速量子MRI（qMRI）重建。methods:* 该方法使用优化算法将模型基于qMRI重建拓展到深度学习框架中。results:* 该方法在不同的脑、膝和模拟实验中都显示出了高度准确和可靠的MR参数地图重建，并能够正确地纠正成像 artifacts、减少噪声和恢复图像特征。与其他状态的普通和深度学习方法相比，RELAX-MORE显著提高了效率、准确性、可靠性和通用性，有广泛的应用前景。<details>
<summary>Abstract</summary>
This paper proposes a novel self-supervised learning method, RELAX-MORE, for quantitative MRI (qMRI) reconstruction. The proposed method uses an optimization algorithm to unroll a model-based qMRI reconstruction into a deep learning framework, enabling the generation of highly accurate and robust MR parameter maps at imaging acceleration. Unlike conventional deep learning methods requiring a large amount of training data, RELAX-MORE is a subject-specific method that can be trained on single-subject data through self-supervised learning, making it accessible and practically applicable to many qMRI studies. Using the quantitative $T_1$ mapping as an example at different brain, knee and phantom experiments, the proposed method demonstrates excellent performance in reconstructing MR parameters, correcting imaging artifacts, removing noises, and recovering image features at imperfect imaging conditions. Compared with other state-of-the-art conventional and deep learning methods, RELAX-MORE significantly improves efficiency, accuracy, robustness, and generalizability for rapid MR parameter mapping. This work demonstrates the feasibility of a new self-supervised learning method for rapid MR parameter mapping, with great potential to enhance the clinical translation of qMRI.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种新的自我监督学习方法，即RELAX-MORE，用于量化磁共振成像（qMRI）重建。该方法使用优化算法将模型基于qMRI重建拓展到深度学习框架中，以生成高精度和可靠的MR参数图像，并且能够在不良成像条件下重建图像特征。不同于传统的深度学习方法需要大量的训练数据，RELAX-MORE是一种专门为各个主体学习的方法，可以通过自我监督学习方式在单个主体数据上进行训练，因此可以在许多qMRI研究中实现可行和实用。使用量化$T_1$映射为例，在不同的脑、膝和模拟器实验中，提出的方法能够出色地重建MR参数，纠正成像 artifacts，除去噪声，并保持图像特征。相比其他状态之前的传统和深度学习方法，RELAX-MORE显著提高了效率、准确性、可靠性和普适性，为快速MR参数映射带来了新的可能性，有潜力提高量化磁共振成像在临床应用中的价值。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Approaches-for-Data-Augmentation-in-Medical-Imaging-A-Review"><a href="#Deep-Learning-Approaches-for-Data-Augmentation-in-Medical-Imaging-A-Review" class="headerlink" title="Deep Learning Approaches for Data Augmentation in Medical Imaging: A Review"></a>Deep Learning Approaches for Data Augmentation in Medical Imaging: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13125">http://arxiv.org/abs/2307.13125</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Arminsbss/tumor-classification">https://github.com/Arminsbss/tumor-classification</a></li>
<li>paper_authors: Aghiles Kebaili, Jérôme Lapuyade-Lahorgue, Su Ruan</li>
<li>for: 这个评论报告的目的是对医疗图像分析领域中深度学习技术的应用进行报告，特别是关于医疗图像数据的有限性问题。</li>
<li>methods: 这篇评论报告主要考虑了三种深度生成模型，包括变分自动编码器、对抗学习网络和扩散模型，用于生成更真实和多样化的医疗图像数据。</li>
<li>results: 这篇评论报告提供了现有的深度生成模型在医疗图像分类、分割和交叉模式翻译等下游任务中的现状，以及这些模型在不同任务中的优势和局限性。<details>
<summary>Abstract</summary>
Deep learning has become a popular tool for medical image analysis, but the limited availability of training data remains a major challenge, particularly in the medical field where data acquisition can be costly and subject to privacy regulations. Data augmentation techniques offer a solution by artificially increasing the number of training samples, but these techniques often produce limited and unconvincing results. To address this issue, a growing number of studies have proposed the use of deep generative models to generate more realistic and diverse data that conform to the true distribution of the data. In this review, we focus on three types of deep generative models for medical image augmentation: variational autoencoders, generative adversarial networks, and diffusion models. We provide an overview of the current state of the art in each of these models and discuss their potential for use in different downstream tasks in medical imaging, including classification, segmentation, and cross-modal translation. We also evaluate the strengths and limitations of each model and suggest directions for future research in this field. Our goal is to provide a comprehensive review about the use of deep generative models for medical image augmentation and to highlight the potential of these models for improving the performance of deep learning algorithms in medical image analysis.
</details>
<details>
<summary>摘要</summary>
深度学习已成为医疗图像分析中常用工具，但培训数据的有限性仍然是主要挑战，特别是医疗领域，数据收集可能昂贵且受隐私法规限制。数据扩展技术可以人工增加培训样本数量，但这些技术通常生成有限和不实的结果。为解决这个问题，一些研究已经提议使用深度生成模型生成更真实和多样的数据，以遵循实际数据的分布。在这篇评论中，我们关注了医疗图像增强中三种深度生成模型：变量自适应器、对抗网络和扩散模型。我们提供这些模型的当前状态艺术和在不同下游任务中的潜在应用，包括分类、分割和对抗Modal译化。我们还评估了每种模型的优点和缺点，并提出了未来研究的方向。我们的目标是提供关于医疗图像增强中深度生成模型的全面评论，并高亮这些模型在医疗图像分析中的潜在优势和未来研究的方向。
</details></li>
</ul>
<hr>
<h2 id="In-Situ-Thickness-Measurement-of-Die-Silicon-Using-Voltage-Imaging-for-Hardware-Assurance"><a href="#In-Situ-Thickness-Measurement-of-Die-Silicon-Using-Voltage-Imaging-for-Hardware-Assurance" class="headerlink" title="In-Situ Thickness Measurement of Die Silicon Using Voltage Imaging for Hardware Assurance"></a>In-Situ Thickness Measurement of Die Silicon Using Voltage Imaging for Hardware Assurance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13118">http://arxiv.org/abs/2307.13118</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olivia P. Dizon-Paradis, Nitin Varshney, M Tanjidur Rahman, Michael Strizich, Haoting Shen, Navid Asadizanjani</li>
<li>for: 这篇论文的目的是提高物理检查方法的自动化水平，以满足政府和电子工业的固件保证需求。</li>
<li>methods: 该论文提出了一种基于电子束电压成像、图像处理和 Монте卡罗 simulations的方法，用于在物理检查过程中实时测量剩下的硅厚度，以便实现均匀的排除过程。</li>
<li>results: 该方法可以准确地测量剩下的硅厚度，并且可以在不同的材料厚度和材料特性下实现均匀的排除过程。<details>
<summary>Abstract</summary>
Hardware assurance of electronics is a challenging task and is of great interest to the government and the electronics industry. Physical inspection-based methods such as reverse engineering (RE) and Trojan scanning (TS) play an important role in hardware assurance. Therefore, there is a growing demand for automation in RE and TS. Many state-of-the-art physical inspection methods incorporate an iterative imaging and delayering workflow. In practice, uniform delayering can be challenging if the thickness of the initial layer of material is non-uniform. Moreover, this non-uniformity can reoccur at any stage during delayering and must be corrected. Therefore, it is critical to evaluate the thickness of the layers to be removed in a real-time fashion. Our proposed method uses electron beam voltage imaging, image processing, and Monte Carlo simulation to measure the thickness of remaining silicon to guide a uniform delayering process
</details>
<details>
<summary>摘要</summary>
(Note: Simplified Chinese is also known as "Mandarin" or "Standard Chinese". The translation is written in the Simplified Chinese characters, which are used in mainland China and Singapore. Traditional Chinese characters are used in Taiwan, Hong Kong, and Macau.)
</details></li>
</ul>
<hr>
<h2 id="Automatic-Infant-Respiration-Estimation-from-Video-A-Deep-Flow-based-Algorithm-and-a-Novel-Public-Benchmark"><a href="#Automatic-Infant-Respiration-Estimation-from-Video-A-Deep-Flow-based-Algorithm-and-a-Novel-Public-Benchmark" class="headerlink" title="Automatic Infant Respiration Estimation from Video: A Deep Flow-based Algorithm and a Novel Public Benchmark"></a>Automatic Infant Respiration Estimation from Video: A Deep Flow-based Algorithm and a Novel Public Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13110">http://arxiv.org/abs/2307.13110</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ostadabbas/infant-respiration-estimation">https://github.com/ostadabbas/infant-respiration-estimation</a></li>
<li>paper_authors: Sai Kumar Reddy Manne, Shaotong Zhu, Sarah Ostadabbas, Michael Wan</li>
<li>for: 这个论文是为了提供一种自动、无接触、不需要特殊设备的婴儿呼吸监测方法。</li>
<li>methods: 这个论文使用了深度学习方法， combines video-extracted optical flow input和特有的spatiotemporal convolutional processing，以便从plain video footage中提取婴儿呼吸信息。</li>
<li>results: 该方法在婴儿数据集上进行了训练和测试，与其他状态的方法相比，具有较低的呼吸速率估计误差（$\sim$2.9 breaths per minute），而其他公共模型则在婴儿主题和环境中表现较差。<details>
<summary>Abstract</summary>
Respiration is a critical vital sign for infants, and continuous respiratory monitoring is particularly important for newborns. However, neonates are sensitive and contact-based sensors present challenges in comfort, hygiene, and skin health, especially for preterm babies. As a step toward fully automatic, continuous, and contactless respiratory monitoring, we develop a deep-learning method for estimating respiratory rate and waveform from plain video footage in natural settings. Our automated infant respiration flow-based network (AIRFlowNet) combines video-extracted optical flow input and spatiotemporal convolutional processing tuned to the infant domain. We support our model with the first public annotated infant respiration dataset with 125 videos (AIR-125), drawn from eight infant subjects, set varied pose, lighting, and camera conditions. We include manual respiration annotations and optimize AIRFlowNet training on them using a novel spectral bandpass loss function. When trained and tested on the AIR-125 infant data, our method significantly outperforms other state-of-the-art methods in respiratory rate estimation, achieving a mean absolute error of $\sim$2.9 breaths per minute, compared to $\sim$4.7--6.2 for other public models designed for adult subjects and more uniform environments.
</details>
<details>
<summary>摘要</summary>
呼吸是新生儿的生命必需指标，不断的呼吸监测特别重要。然而，新生儿质感敏感，contact-based感测器会带来舒适性、卫生性和皮肤健康问题，特别是 для premature infants。为了实现完全自动、不断、无接触的呼吸监测，我们开发了一种深度学习方法，可以从普通的视频录像中提取呼吸rate和波形信息。我们的自动 infant respiration flow-based network（AIRFlowNet）结合视频提取的光流输入和适应 infant 领域的空间时间卷积处理。我们对模型进行了人工呼吸注解和优化，使用了一种新的 spectral bandpass loss function。当我们在 AIR-125  infant 数据集上训练和测试 AIRFlowNet 时，我们发现其与其他公共的 state-of-the-art 方法相比，在呼吸速率估计方面显著出perform，其中的平均绝对误差约为 2.9  breaths per minute，与其他针对 adult subjects 和更uniform 环境设计的公共模型相比，误差较低。
</details></li>
</ul>
<hr>
<h2 id="Framework-for-Automatic-PCB-Marking-Detection-and-Recognition-for-Hardware-Assurance"><a href="#Framework-for-Automatic-PCB-Marking-Detection-and-Recognition-for-Hardware-Assurance" class="headerlink" title="Framework for Automatic PCB Marking Detection and Recognition for Hardware Assurance"></a>Framework for Automatic PCB Marking Detection and Recognition for Hardware Assurance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13105">http://arxiv.org/abs/2307.13105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olivia P. Dizon-Paradis, Daniel E. Capecci, Nathan T. Jessurun, Damon L. Woodard, Mark M. Tehranipoor, Navid Asadizanjani</li>
<li>for:  automatic printed circuit board (PCB) assurance</li>
<li>methods:  utilize domain knowledge of PCB text and logos for data collection and incorporation</li>
<li>results:  proposed dataset plan and framework for high-accuracy automatic PCB marking extraction<details>
<summary>Abstract</summary>
A Bill of Materials (BoM) is a list of all components on a printed circuit board (PCB). Since BoMs are useful for hardware assurance, automatic BoM extraction (AutoBoM) is of great interest to the government and electronics industry. To achieve a high-accuracy AutoBoM process, domain knowledge of PCB text and logos must be utilized. In this study, we discuss the challenges associated with automatic PCB marking extraction and propose 1) a plan for collecting salient PCB marking data, and 2) a framework for incorporating this data for automatic PCB assurance. Given the proposed dataset plan and framework, subsequent future work, implications, and open research possibilities are detailed.
</details>
<details>
<summary>摘要</summary>
一份物料清单（BoM）是印刷电路板（PCB）上所有组件的列表。由于BoM对硬件保证有益，因此自动生成BoM（AutoBoM）对政府和电子行业都是非常有趣的。为实现高精度AutoBoM过程，需要利用PCB文本和标识符的领域知识。本研究讨论了自动PCB标识EXTRACTION的挑战和提出了以下两点计划：1. 收集PCB标识数据的计划2. 利用这些数据实现自动PCB保证的框架基于提出的数据集计划和框架，我们预计将来的工作、影响和开放的研究可能性都将在后续的研究中得到探讨。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-image-captioning-with-depth-information-using-a-Transformer-based-framework"><a href="#Enhancing-image-captioning-with-depth-information-using-a-Transformer-based-framework" class="headerlink" title="Enhancing image captioning with depth information using a Transformer-based framework"></a>Enhancing image captioning with depth information using a Transformer-based framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03767">http://arxiv.org/abs/2308.03767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aya Mahmoud Ahmed, Mohamed Yousef, Khaled F. Hussain, Yousef Bassyouni Mahdy<br>for:This paper aims to enhance the image captioning task by integrating depth information with RGB images to generate better descriptions.methods:The proposed framework uses a Transformer-based encoder-decoder architecture and fuses RGB and depth images using different approaches.results:The proposed framework generates better captions when using depth information, whether it is ground truth or estimated, and outperforms using RGB images only. The cleaned version of the NYU-v2 dataset is also proposed to address inconsistent labeling issues.<details>
<summary>Abstract</summary>
Captioning images is a challenging scene-understanding task that connects computer vision and natural language processing. While image captioning models have been successful in producing excellent descriptions, the field has primarily focused on generating a single sentence for 2D images. This paper investigates whether integrating depth information with RGB images can enhance the captioning task and generate better descriptions. For this purpose, we propose a Transformer-based encoder-decoder framework for generating a multi-sentence description of a 3D scene. The RGB image and its corresponding depth map are provided as inputs to our framework, which combines them to produce a better understanding of the input scene. Depth maps could be ground truth or estimated, which makes our framework widely applicable to any RGB captioning dataset. We explored different fusion approaches to fuse RGB and depth images. The experiments are performed on the NYU-v2 dataset and the Stanford image paragraph captioning dataset. During our work with the NYU-v2 dataset, we found inconsistent labeling that prevents the benefit of using depth information to enhance the captioning task. The results were even worse than using RGB images only. As a result, we propose a cleaned version of the NYU-v2 dataset that is more consistent and informative. Our results on both datasets demonstrate that the proposed framework effectively benefits from depth information, whether it is ground truth or estimated, and generates better captions. Code, pre-trained models, and the cleaned version of the NYU-v2 dataset will be made publically available.
</details>
<details>
<summary>摘要</summary>
captioning images是一个复杂的场景理解任务，连接计算机视觉和自然语言处理。虽然图像captioning模型已经成功地生成了出色的描述，但这个领域主要集中在生成2D图像的单句描述。这篇论文研究了是否可以将深度信息与RGB图像一起使用，以提高描述任务并生成更好的描述。为此，我们提出了基于Transformer的encoder-decoder框架，用于生成3D场景的多句描述。RGB图像和其对应的深度图被提供给我们的框架，我们将它们结合使用，以便更好地理解输入场景。深度图可以是真实的或估计的，这使我们的框架对任何RGB描述 dataset 都可以广泛应用。我们研究了不同的融合方法，以融合RGB和深度图。实验在NYU-v2 dataset和Stanford图像段落描述dataset上进行。在我们与NYU-v2 dataset的工作中，我们发现了不一致的标签，这阻碍了使用深度信息提高描述任务的好处。结果甚至比使用RGB图像只的情况even worse。因此，我们提出了一个更加一致和有用的NYU-v2 dataset。我们的结果表明，我们的提案的框架可以充分利用深度信息，无论是真实的或估计的，并生成更好的描述。代码、预训练模型和我们修改过的NYU-v2 dataset将公开发布。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/25/eess.IV_2023_07_25/" data-id="cllsj9x0d008euv88fe3e2pxk" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/25/cs.SD_2023_07_25/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.SD - 2023-07-25 123:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/24/cs.LG_2023_07_24/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-07-24 18:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'', root:''}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
