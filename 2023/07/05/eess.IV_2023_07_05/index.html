
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-07-05 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Dual Arbitrary Scale Super-Resolution for Multi-Contrast MRI paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.02334 repo_url: https:&#x2F;&#x2F;github.com&#x2F;jmzhang79&#x2F;dual-arbnet paper_authors: Jiamiao Zhang, Yichen Chi, Jun">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-07-05">
<meta property="og:url" content="https://nullscc.github.io/2023/07/05/eess.IV_2023_07_05/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Dual Arbitrary Scale Super-Resolution for Multi-Contrast MRI paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.02334 repo_url: https:&#x2F;&#x2F;github.com&#x2F;jmzhang79&#x2F;dual-arbnet paper_authors: Jiamiao Zhang, Yichen Chi, Jun">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-05T09:00:00.000Z">
<meta property="article:modified_time" content="2023-09-23T08:31:54.839Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_07_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/05/eess.IV_2023_07_05/" class="article-date">
  <time datetime="2023-07-05T09:00:00.000Z" itemprop="datePublished">2023-07-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-07-05
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Dual-Arbitrary-Scale-Super-Resolution-for-Multi-Contrast-MRI"><a href="#Dual-Arbitrary-Scale-Super-Resolution-for-Multi-Contrast-MRI" class="headerlink" title="Dual Arbitrary Scale Super-Resolution for Multi-Contrast MRI"></a>Dual Arbitrary Scale Super-Resolution for Multi-Contrast MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02334">http://arxiv.org/abs/2307.02334</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jmzhang79/dual-arbnet">https://github.com/jmzhang79/dual-arbnet</a></li>
<li>paper_authors: Jiamiao Zhang, Yichen Chi, Jun Lyu, Wenming Yang, Yapeng Tian</li>
<li>for: 这篇论文旨在提高医疗成像领域中的磁共振成像（MRI）像素化，以提高医生在诊断和治疗时的可见度。</li>
<li>methods: 本研究使用了一种基于神经网络的多标示磁共振超解析（SR）重建方法，称为Dual-ArbNet，它可以在不同的标示模式下进行SR重建，并且可以处理不同的像素比例和分辨率。</li>
<li>results: 实验结果显示，Dual-ArbNet方法在两个公共MRI数据集上具有较高的SR性能，并且可以在不同的像素比例和分辨率下进行SR重建。此外，该方法还可以运用到临床实践中。<details>
<summary>Abstract</summary>
Limited by imaging systems, the reconstruction of Magnetic Resonance Imaging (MRI) images from partial measurement is essential to medical imaging research. Benefiting from the diverse and complementary information of multi-contrast MR images in different imaging modalities, multi-contrast Super-Resolution (SR) reconstruction is promising to yield SR images with higher quality. In the medical scenario, to fully visualize the lesion, radiologists are accustomed to zooming the MR images at arbitrary scales rather than using a fixed scale, as used by most MRI SR methods. In addition, existing multi-contrast MRI SR methods often require a fixed resolution for the reference image, which makes acquiring reference images difficult and imposes limitations on arbitrary scale SR tasks. To address these issues, we proposed an implicit neural representations based dual-arbitrary multi-contrast MRI super-resolution method, called Dual-ArbNet. First, we decouple the resolution of the target and reference images by a feature encoder, enabling the network to input target and reference images at arbitrary scales. Then, an implicit fusion decoder fuses the multi-contrast features and uses an Implicit Decoding Function~(IDF) to obtain the final MRI SR results. Furthermore, we introduce a curriculum learning strategy to train our network, which improves the generalization and performance of our Dual-ArbNet. Extensive experiments in two public MRI datasets demonstrate that our method outperforms state-of-the-art approaches under different scale factors and has great potential in clinical practice.
</details>
<details>
<summary>摘要</summary>
限于快照系统，重建快照成像（MRI）图像从部分测量是医学成像研究中的关键。利用不同和补充的多比特MR成像模式的多比特超分辨（SR）重建可以获得更高质量的SR图像。在医疗场景下，为了全面显示肿瘤，辐射医生通常会在自定义的比例下缩放MR图像，而不是使用固定比例，这与大多数MRI SR方法不同。此外，现有的多比特MRI SR方法通常需要固定的参参图像分辨率，这使得获得参考图像困难，并对自定义比例SR任务带来限制。为解决这些问题，我们提出了基于卷积神经表示的双自由多比特MRI超分辨方法，称为Dual-ArbNet。首先，我们将目标和参考图像的分辨率解耦通过特征编码器，使网络可以输入自定义的目标和参考图像。然后，我们使用卷积叠加器将多比特特征进行卷积叠加，并使用偏函数IDF获取最终的MRI SR结果。此外，我们引入了课程学习策略来训练我们的网络，这有助于提高我们Dual-ArbNet的一般化和性能。广泛的实验在两个公共MRI数据集上表明，我们的方法在不同的比例因子下表现出色，有很好的潜在应用前景。
</details></li>
</ul>
<hr>
<h2 id="Joint-Hierarchical-Priors-and-Adaptive-Spatial-Resolution-for-Efficient-Neural-Image-Compression"><a href="#Joint-Hierarchical-Priors-and-Adaptive-Spatial-Resolution-for-Efficient-Neural-Image-Compression" class="headerlink" title="Joint Hierarchical Priors and Adaptive Spatial Resolution for Efficient Neural Image Compression"></a>Joint Hierarchical Priors and Adaptive Spatial Resolution for Efficient Neural Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02273">http://arxiv.org/abs/2307.02273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Ghorbel, Wassim Hamidouche, Luce Morin</li>
<li>for: 这篇论文是关于神经网络图像压缩（NIC）的研究，旨在提高NIC的性能，并且希望通过对Tranformer-based transform coding框架进行改进，以提高图像压缩的效率和质量。</li>
<li>methods: 本文使用Tranformer-based channel-wise auto-regressive prior模型来提高SwinT-ChARM的性能，并且添加了一个可学习的缩放模块来更好地提取更紧凑的缺失代码。</li>
<li>results: 实验结果表明，提出的框架可以在各种测试集上显著提高NIC的质量和效率，并且在与VVC参考编码器（VTM-18.0）和SwinT-ChARM神经编码器进行比较时，具有更好的质量和效率。<details>
<summary>Abstract</summary>
Recently, the performance of neural image compression (NIC) has steadily improved thanks to the last line of study, reaching or outperforming state-of-the-art conventional codecs. Despite significant progress, current NIC methods still rely on ConvNet-based entropy coding, limited in modeling long-range dependencies due to their local connectivity and the increasing number of architectural biases and priors, resulting in complex underperforming models with high decoding latency. Motivated by the efficiency investigation of the Tranformer-based transform coding framework, namely SwinT-ChARM, we propose to enhance the latter, as first, with a more straightforward yet effective Tranformer-based channel-wise auto-regressive prior model, resulting in an absolute image compression transformer (ICT). Through the proposed ICT, we can capture both global and local contexts from the latent representations and better parameterize the distribution of the quantized latents. Further, we leverage a learnable scaling module with a sandwich ConvNeXt-based pre-/post-processor to accurately extract more compact latent codes while reconstructing higher-quality images. Extensive experimental results on benchmark datasets showed that the proposed framework significantly improves the trade-off between coding efficiency and decoder complexity over the versatile video coding (VVC) reference encoder (VTM-18.0) and the neural codec SwinT-ChARM. Moreover, we provide model scaling studies to verify the computational efficiency of our approach and conduct several objective and subjective analyses to bring to the fore the performance gap between the adaptive image compression transformer (AICT) and the neural codec SwinT-ChARM.
</details>
<details>
<summary>摘要</summary>
近些年，神经网络图像压缩（NIC）的性能已经逐渐提高，达到或超越传统编码器的状态元。 DESPITE 这些进步，当前的 NIC 方法仍然基于 ConvNet 来实现 entropy coding，受到本地连接性的限制，以及逐渐增加的建筑学偏好和先验，导致复杂的不够表现的模型和高解码延迟。 被Transformer 基于 transform coding 框架的 SwinT-ChARM 的效率调查所驱动，我们提议使用更直观而有效的 Tranformer 基于通道 wise auto-regressive prior 模型，以提高后者。通过我们的提议的 ICT，我们可以从 latent 表示中捕捉全局和局部上下文，更好地参数化归一化的量化 latent。此外，我们利用一个可学习的缩放模块，并在 ConvNeXt 基于的预处理/后处理器中使用它来准确地提取更紧凑的 latent 代码，并在重建更高质量的图像。对于一系列的 benchmark 数据集，我们进行了广泛的实验研究，并证明了我们的框架可以显著提高对 coding 效率和解码器复杂度的质量权衡。此外，我们还提供了模型缩放研究，以证明我们的方法的计算效率。最后，我们进行了一些对象和主观分析，以强调 AICT 与 SwinT-ChARM 之间的性能差距。
</details></li>
</ul>
<hr>
<h2 id="Direct-segmentation-of-brain-white-matter-tracts-in-diffusion-MRI"><a href="#Direct-segmentation-of-brain-white-matter-tracts-in-diffusion-MRI" class="headerlink" title="Direct segmentation of brain white matter tracts in diffusion MRI"></a>Direct segmentation of brain white matter tracts in diffusion MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02223">http://arxiv.org/abs/2307.02223</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamza Kebiri, Ali Gholipour, Meritxell Bach Cuadra, Davood Karimi<br>for:白 matter  tracts 的 segmentation，即 brain 中各个区域之间的连接组织。methods:使用 deep learning 方法，直接从 diffusion MRI 数据中提取 white matter tracts。results: segmentation 精度与现有方法相当（mean Dice Similarity Coefficient 为 0.826），并且具有更高的普适性，可应用于低样本量的临床研究和不同的数据获取协议。<details>
<summary>Abstract</summary>
The brain white matter consists of a set of tracts that connect distinct regions of the brain. Segmentation of these tracts is often needed for clinical and research studies. Diffusion-weighted MRI offers unique contrast to delineate these tracts. However, existing segmentation methods rely on intermediate computations such as tractography or estimation of fiber orientation density. These intermediate computations, in turn, entail complex computations that can result in unnecessary errors. Moreover, these intermediate computations often require dense multi-shell measurements that are unavailable in many clinical and research applications. As a result, current methods suffer from low accuracy and poor generalizability. Here, we propose a new deep learning method that segments these tracts directly from the diffusion MRI data, thereby sidestepping the intermediate computation errors. Our experiments show that this method can achieve segmentation accuracy that is on par with the state of the art methods (mean Dice Similarity Coefficient of 0.826). Compared with the state of the art, our method offers far superior generalizability to undersampled data that are typical of clinical studies and to data obtained with different acquisition protocols. Moreover, we propose a new method for detecting inaccurate segmentations and show that it is more accurate than standard methods that are based on estimation uncertainty quantification. The new methods can serve many critically important clinical and scientific applications that require accurate and reliable non-invasive segmentation of white matter tracts.
</details>
<details>
<summary>摘要</summary>
脑白atter包括一组通过不同脑区域的脑 tract， segmentation 这些 tract 常需要在临床和研究实验中进行。Diffusion-weighted MRI 提供了一个唯一的对比，以定义这些 tract。然而，现有的 segmentation 方法通常需要中间计算，如 tractography 或 fibre orientation density 的估计。这些中间计算可能会导致多余的错误，并且常常需要 dense multi-shell measurements，这些 measurements 在许多临床和研究应用中不可得。因此，现有的方法受到低精度和差异化的限制。在这里，我们提出了一种新的深度学习方法，可以直接从 diffusion MRI 数据中分割 white matter tract，并且避免中间计算的错误。我们的实验表明，这种方法可以达到与现有方法相同的 segmentation 精度（mean Dice Similarity Coefficient 0.826）。相比之下，我们的方法在不同的数据采样和数据采集协议下具有更好的普适性。此外，我们还提出了一种新的方法来检测不准确的分割，并证明它比标准的方法更加准确。这些新方法可以为许多重要的临床和科学应用提供准确和可靠的非侵入式 white matter tract 的分割。
</details></li>
</ul>
<hr>
<h2 id="Compound-Attention-and-Neighbor-Matching-Network-for-Multi-contrast-MRI-Super-resolution"><a href="#Compound-Attention-and-Neighbor-Matching-Network-for-Multi-contrast-MRI-Super-resolution" class="headerlink" title="Compound Attention and Neighbor Matching Network for Multi-contrast MRI Super-resolution"></a>Compound Attention and Neighbor Matching Network for Multi-contrast MRI Super-resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02148">http://arxiv.org/abs/2307.02148</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenxuan Chen, Sirui Wu, Shuai Wang, Zhongsen Li, Jia Yang, Huifeng Yao, Xiaomeng Li, Xiaolei Song</li>
<li>for: 这个论文旨在提出一种新的多模式磁共振成像超分辨（SR）网络架构，用于解决现有的SR方法在多模式磁共振成像中存在缺陷，如缺乏合适的参考特征和缺乏频率匹配。</li>
<li>methods: 该论文提出了一种基于自我注意力和邻居匹配的网络架构，称为CANM-Net，它使用复合自我注意力机制和邻居匹配模块来捕捉多模式磁共振成像中的相互依赖关系，并将参考特征和下落特征进行适应性匹配，以实现高质量的SR图像生成。</li>
<li>results: 该论文通过在IXI、fastMRI和实际扫描数据集上进行SR任务的实验，证明了CANM-Net在透彻和跨模式磁共振成像SR中具有优于现有方法的性能，并且在不当 registrations 的情况下仍然保持良好的表现，这表明其在临床应用中具有良好的潜力。<details>
<summary>Abstract</summary>
Multi-contrast magnetic resonance imaging (MRI) reflects information about human tissue from different perspectives and has many clinical applications. By utilizing the complementary information among different modalities, multi-contrast super-resolution (SR) of MRI can achieve better results than single-image super-resolution. However, existing methods of multi-contrast MRI SR have the following shortcomings that may limit their performance: First, existing methods either simply concatenate the reference and degraded features or exploit global feature-matching between them, which are unsuitable for multi-contrast MRI SR. Second, although many recent methods employ transformers to capture long-range dependencies in the spatial dimension, they neglect that self-attention in the channel dimension is also important for low-level vision tasks. To address these shortcomings, we proposed a novel network architecture with compound-attention and neighbor matching (CANM-Net) for multi-contrast MRI SR: The compound self-attention mechanism effectively captures the dependencies in both spatial and channel dimension; the neighborhood-based feature-matching modules are exploited to match degraded features and adjacent reference features and then fuse them to obtain the high-quality images. We conduct experiments of SR tasks on the IXI, fastMRI, and real-world scanning datasets. The CANM-Net outperforms state-of-the-art approaches in both retrospective and prospective experiments. Moreover, the robustness study in our work shows that the CANM-Net still achieves good performance when the reference and degraded images are imperfectly registered, proving good potential in clinical applications.
</details>
<details>
<summary>摘要</summary>
多模式磁共振成像（MRI）可以从不同角度获取人体组织信息，有广泛的临床应用。通过利用不同模式之间的共趋性信息，多模式超解析（SR）的MRI可以实现更好的结果，而存在的方法却有以下缺点：首先，现有方法可能会简单地 concatenate 参考和压缩特征，或者利用全局特征匹配，这些方法不适合多模式MRI SR。其次，虽然许多最新的方法使用 transformer 来捕捉空间维度的长距离依赖关系，但它们忽略了通道维度的自我注意力的重要性，这对低级视觉任务来说非常重要。为了解决这些缺点，我们提出了一种新的网络架构，即嵌入式自注意和邻居匹配网络（CANM-Net），用于多模式MRI SR：嵌入式自注意机制可以有效捕捉空间和通道维度之间的依赖关系；邻居特征匹配模块可以将压缩特征和相邻参考特征匹配并融合，以获得高质量的图像。我们在 IXI、fastMRI 和实际扫描数据集上进行 SR 任务的实验，CANM-Net 比 estado-of-the-art 方法在回顾和前瞻性实验中表现出色。此外，我们的 robustness 研究显示，CANM-Net 在参考和压缩图像不完美匹配时仍能保持良好的性能，这证明它在临床应用中具有良好的潜力。
</details></li>
</ul>
<hr>
<h2 id="A-Mini-Batch-Quasi-Newton-Proximal-Method-for-Constrained-Total-Variation-Nonlinear-Image-Reconstruction"><a href="#A-Mini-Batch-Quasi-Newton-Proximal-Method-for-Constrained-Total-Variation-Nonlinear-Image-Reconstruction" class="headerlink" title="A Mini-Batch Quasi-Newton Proximal Method for Constrained Total-Variation Nonlinear Image Reconstruction"></a>A Mini-Batch Quasi-Newton Proximal Method for Constrained Total-Variation Nonlinear Image Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02043">http://arxiv.org/abs/2307.02043</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Hong, Thanh-an Pham, Irad Yavneh, Michael Unser</li>
<li>for: 这篇论文是关于计算成像，使用精确的物理模型来实现高质量重建的。</li>
<li>methods: 本文提出了一种基于强化随机批处理的准确非线性物理模型的计算成像方法，即mini-batch quasi-Newton proximal方法（BQNPM）。</li>
<li>results: 本文通过对三维反射问题进行实验和实际数据测试，证明BQNPM比ASPMs更快速地 converges，并且可以在计算成像中实现高质量的重建。<details>
<summary>Abstract</summary>
Over the years, computational imaging with accurate nonlinear physical models has drawn considerable interest due to its ability to achieve high-quality reconstructions. However, such nonlinear models are computationally demanding. A popular choice for solving the corresponding inverse problems is accelerated stochastic proximal methods (ASPMs), with the caveat that each iteration is expensive. To overcome this issue, we propose a mini-batch quasi-Newton proximal method (BQNPM) tailored to image-reconstruction problems with total-variation regularization. It involves an efficient approach that computes a weighted proximal mapping at a cost similar to that of the proximal mapping in ASPMs. However, BQNPM requires fewer iterations than ASPMs to converge. We assess the performance of BQNPM on three-dimensional inverse-scattering problems with linear and nonlinear physical models. Our results on simulated and real data show the effectiveness and efficiency of BQNPM,
</details>
<details>
<summary>摘要</summary>
随着时间的推移，计算成像技术已经吸引了广泛的关注，因为它可以实现高质量的重建。然而，这些非线性模型在计算上具有挑战性。一种受欢迎的解决方案是加速随机邻域方法（ASPMs），但每个迭代都是贵夫。为了解决这个问题，我们提议一种基于图像重建问题的权重贝叶斯方法（BQNPM）。这种方法具有计算贝叶斯映射的效率，但需要 fewer than ASPMs 的迭代次数才能达到 convergence。我们对三维反射问题进行了线性和非线性物理模型的测试，结果表明 BQNPM 的效果和效率。
</details></li>
</ul>
<hr>
<h2 id="Joint-Recovery-of-T1-T2-and-Proton-Density-Maps-Using-a-Bayesian-Approach-with-Parameter-Estimation-and-Complementary-Undersampling-Patterns"><a href="#Joint-Recovery-of-T1-T2-and-Proton-Density-Maps-Using-a-Bayesian-Approach-with-Parameter-Estimation-and-Complementary-Undersampling-Patterns" class="headerlink" title="Joint Recovery of T1, T2* and Proton Density Maps Using a Bayesian Approach with Parameter Estimation and Complementary Undersampling Patterns"></a>Joint Recovery of T1, T2* and Proton Density Maps Using a Bayesian Approach with Parameter Estimation and Complementary Undersampling Patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02015">http://arxiv.org/abs/2307.02015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuai Huang, James J. Lah, Jason W. Allen, Deqiang Qiu<br>for:  This paper aims to improve the quality of quantitative MR images recovered from undersampled measurements by incorporating the signal model of the variable-flip-angle (VFA) multi-echo 3D gradient-echo (GRE) method into the reconstruction of $T_1$, $T_2^*$, and proton density (PD) maps.methods:  The proposed approach is based on a probabilistic Bayesian formulation of the recovery problem, and uses approximate message passing with built-in parameter estimation (AMP-PE) to jointly recover distribution parameters, VFA multi-echo images, and $T_1$, $T_2^*$, and PD maps without the need for hyperparameter tuning.results:  The proposed AMP-PE approach outperforms the state-of-the-art $l1$-norm minimization approach in terms of reconstruction performance, and adopting complementary undersampling patterns across different flip angles and&#x2F;or echo times yields the best performance for $T_2^*$ and proton density mappings.<details>
<summary>Abstract</summary>
Purpose: To improve the quality of quantitative MR images recovered from undersampled measurements, we incorporate the signal model of the variable-flip-angle (VFA) multi-echo 3D gradient-echo (GRE) method into the reconstruction of $T_1$, $T_2^*$ and proton density (PD) maps. Additionally, we investigate the use of complementary undersampling patterns to determine optimal undersampling schemes for quantitative MRI.   Theory: We propose a probabilistic Bayesian formulation of the recovery problem. Our proposed approach, approximate message passing with built-in parameter estimation (AMP-PE), enables the joint recovery of distribution parameters, VFA multi-echo images, and $T_1$, $T_2^*$, and PD maps without the need for hyperparameter tuning.   Methods: We conducted both retrospective and prospective undersampling to obtain Fourier measurements using variable-density and Poisson-disk patterns. We investigated a variety of undersampling schemes, adopting complementary patterns across different flip angles and/or echo times.   Results: AMP-PE adopts a joint recovery strategy, it outperforms the state-of-the-art $l1$-norm minimization approach that follows a decoupled recovery strategy. For $T_1$ mapping, employing fixed sampling patterns across different echo times produced the best performance. Whereas for $T_2^*$ and proton density mappings, using complementary sampling patterns across different flip angles yielded the best performance.   Conclusion: AMP-PE achieves better performance by combining information from both the MR signal model and the sparse prior on VFA multi-echo images. It is equipped with automatic and adaptive parameter estimation, and works naturally with the clinical prospective undersampling scheme.
</details>
<details>
<summary>摘要</summary>
目的：提高 Undersampled 测量中的量子 MR 图像质量，我们在 reconstruction 中 incorporate 变量扭矩（VFA）多echo 3D 梯阶 echo（GRE）方法的信号模型。此外，我们还 investigate 使用 complementary 抽象样本来确定最佳的 Undersampling 方案。理论：我们提出了一种 Bayesian 形式的回归问题。我们的提议方法为 approximate message passing with built-in parameter estimation（AMP-PE），它可以同时回归分布参数、VFA multi-echo 图像和 $T_1$, $T_2^*$ 和 proton density（PD）图像，无需进行hyperparameter 调整。方法：我们在 retrospective 和 prospectively 抽象到 obtain Fourier 测量。我们 investigate 了不同的抽象方案，包括 variable-density 和 Poisson-disk 模式。结果：AMP-PE 采用了联合回归策略，其表现更好于 state-of-the-art $l1$-norm 最小化方法，后者采用了解 Coupled 回归策略。对 $T_1$ 图像，使用 fixes 抽象模式 across 不同的 echo times 得到了最佳性能。而对 $T_2^*$ 和 proton density 图像，使用 complementary 抽象模式 across 不同的扭矩 angles 得到了最佳性能。结论：AMP-PE 通过结合 MR 信号模型和 VFA multi-echo 图像的稀热先验来提高量子 MR 图像的质量。它具有自动和适应参数估计，并可以自然地与临床的前向抽象 schemes 结合。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Spectral-Demosaicing-with-Lightweight-Spectral-Attention-Networks"><a href="#Unsupervised-Spectral-Demosaicing-with-Lightweight-Spectral-Attention-Networks" class="headerlink" title="Unsupervised Spectral Demosaicing with Lightweight Spectral Attention Networks"></a>Unsupervised Spectral Demosaicing with Lightweight Spectral Attention Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01990">http://arxiv.org/abs/2307.01990</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Feng, Yongqiang Zhao, Seong G. Kong, Haijin Zeng</li>
<li>for: 这 paper 的目的是提出一种基于深度学习的无监督 spectral demosaicing 技术，以便在实际图像中进行高质量的颜色彩度恢复。</li>
<li>methods: 该 paper 使用了一种无监督学习的架构，包括提出了一种 mosaic loss function、模型结构、变换策略以及 early stopping 策略，这些组成了一个完整的无监督 spectral demosaicing 框架。</li>
<li>results: 对于实际图像，该 paper 的方法能够更好地抑制空间扭曲、保持 spectral 准确性、稳定性和计算成本，并且在 synthetic 和实际数据上进行了广泛的实验，得到了更高的性能。<details>
<summary>Abstract</summary>
This paper presents a deep learning-based spectral demosaicing technique trained in an unsupervised manner. Many existing deep learning-based techniques relying on supervised learning with synthetic images, often underperform on real-world images especially when the number of spectral bands increases. According to the characteristics of the spectral mosaic image, this paper proposes a mosaic loss function, the corresponding model structure, a transformation strategy, and an early stopping strategy, which form a complete unsupervised spectral demosaicing framework. A challenge in real-world spectral demosaicing is inconsistency between the model parameters and the computational resources of the imager. We reduce the complexity and parameters of the spectral attention module by dividing the spectral attention tensor into spectral attention matrices in the spatial dimension and spectral attention vector in the channel dimension, which is more suitable for unsupervised framework. This paper also presents Mosaic25, a real 25-band hyperspectral mosaic image dataset of various objects, illuminations, and materials for benchmarking. Extensive experiments on synthetic and real-world datasets demonstrate that the proposed method outperforms conventional unsupervised methods in terms of spatial distortion suppression, spectral fidelity, robustness, and computational cost.
</details>
<details>
<summary>摘要</summary>
One of the challenges in real-world spectral demosaicing is the inconsistency between the model parameters and the computational resources of the imager. To address this, the paper reduces the complexity and parameters of the spectral attention module by dividing the spectral attention tensor into spectral attention matrices in the spatial dimension and spectral attention vectors in the channel dimension. This is more suitable for an unsupervised framework.The paper also presents Mosaic25, a real 25-band hyperspectral mosaic image dataset of various objects, illuminations, and materials for benchmarking. Extensive experiments on synthetic and real-world datasets demonstrate that the proposed method outperforms conventional unsupervised methods in terms of spatial distortion suppression, spectral fidelity, robustness, and computational cost.Here is the Simplified Chinese translation of the text:这篇论文提出了一种基于深度学习的spectral demosaicing技术，该技术在无监督的情况下训练。现有的深度学习基于的技术 часто采用监督学习 Synthetic 图像，在实际图像中表现不佳，特别是随着频谱带数的增加。根据频谱拼接图像的特点，这篇论文提出了一种拼接损失函数、相应的模型结构、转换策略和早stopping策略，这些组成了一个完整的无监督 spectral demosaicing 框架。实际频谱拼接中的一个挑战是模型参数和捕获设备的计算资源之间的不一致。这篇论文通过将频谱注意力矩阵分割成空间维度的频谱注意力矩阵和通道维度的频谱注意力向量，来降低模型的复杂性和参数数量。这更适合无监督的框架。此外，这篇论文还提供了Mosaic25，一个实际25个频谱带的卷积合成图像数据集，包括不同的物体、照明和材料，用于对比。对于实际和 sintetic 数据集进行了广泛的实验，结果表明，提出的方法在隐藏扰乱、频谱准确、稳定性和计算成本方面都有较好的表现。
</details></li>
</ul>
<hr>
<h2 id="ToothSegNet-Image-Degradation-meets-Tooth-Segmentation-in-CBCT-Images"><a href="#ToothSegNet-Image-Degradation-meets-Tooth-Segmentation-in-CBCT-Images" class="headerlink" title="ToothSegNet: Image Degradation meets Tooth Segmentation in CBCT Images"></a>ToothSegNet: Image Degradation meets Tooth Segmentation in CBCT Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01979">http://arxiv.org/abs/2307.01979</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxiang Liu, Tianxiang Hu, Yang Feng, Wanghui Ding, Zuozhu Liu</li>
<li>for: constructing three-dimensional tooth models in computer-assisted orthodontics</li>
<li>methods: 使用ToothSegNet框架，通过生成降低图像的信息来训练分割模型，并使用通道维度的混合来减少Encoder和Decoder之间的语义差异，以及通过结构约束损失来精细调整预测的牙齿形态</li>
<li>results: 比前一代医疗图像分割方法更高精度的牙齿分割结果<details>
<summary>Abstract</summary>
In computer-assisted orthodontics, three-dimensional tooth models are required for many medical treatments. Tooth segmentation from cone-beam computed tomography (CBCT) images is a crucial step in constructing the models. However, CBCT image quality problems such as metal artifacts and blurring caused by shooting equipment and patients' dental conditions make the segmentation difficult. In this paper, we propose ToothSegNet, a new framework which acquaints the segmentation model with generated degraded images during training. ToothSegNet merges the information of high and low quality images from the designed degradation simulation module using channel-wise cross fusion to reduce the semantic gap between encoder and decoder, and also refines the shape of tooth prediction through a structural constraint loss. Experimental results suggest that ToothSegNet produces more precise segmentation and outperforms the state-of-the-art medical image segmentation methods.
</details>
<details>
<summary>摘要</summary>
在计算机协助orthodontics中，三维牙齿模型是许多医疗治疗的关键 step。然而，CBCT图像质量问题，如机器设备和病人的牙科条件所导致的锈损和模糊，使 segmentation 变得更加困难。在这篇论文中，我们提出了 ToothSegNet，一个新的框架，通过在训练过程中对生成的受损图像进行准备，使 segmentation 模型更加熟悉受损图像的特征。ToothSegNet 通过核心混合来减少编码器和解码器之间的semantic gap，并通过结构约束损失来精细调整牙齿预测的形态。实验结果表明，ToothSegNet 可以生成更加精准的 segmentation，并超越了当前医学影像 segmentation 方法的性能。
</details></li>
</ul>
<hr>
<h2 id="Millimeter-Wave-Reflectionless-Filters-Using-Advanced-Thin-Film-Fabrication"><a href="#Millimeter-Wave-Reflectionless-Filters-Using-Advanced-Thin-Film-Fabrication" class="headerlink" title="Millimeter-Wave Reflectionless Filters Using Advanced Thin-Film Fabrication"></a>Millimeter-Wave Reflectionless Filters Using Advanced Thin-Film Fabrication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01914">http://arxiv.org/abs/2307.01914</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew Morgan, Seng Loo, Tod Boyd, Miho Hunter</li>
<li>for: developing millimeter-wave, lumped-element reflectionless filters</li>
<li>methods: using advanced thin-film fabrication process with better than 2 μm feature size and integrated elements such as SiN Metal-Insulator-Metal (MIM) capacitors, bridges, and TaN Thin-Film Resistors (TFRs)</li>
<li>results: achieved higher frequency implementation than ever beforeHere’s the same information in Simplified Chinese text:</li>
<li>for: 开发毫米波、堆叠元件反射 filters</li>
<li>methods: 利用高精度薄膜制造过程，实现更高频率实现</li>
<li>results: 实现了历史上最高频率实现<details>
<summary>Abstract</summary>
We report on the development of millimeter-wave, lumped-element reflectionless filters using an advanced thin-film fabrication process. Based on previously demonstrated circuit topologies capable of achieving 50{\Omega} impedance match at all frequencies, these circuits have been implemented at higher frequencies than ever before by leveraging a thin-film process with better than 2 {\mu}m feature size and integrated elements such as SiN Metal-Insulator-Metal (MIM) capacitors, bridges, and TaN Thin-Film Resistors (TFRs).
</details>
<details>
<summary>摘要</summary>
我们报道了毫米波，积成元件反射性筛选器的发展，使用进步的薄膜制造过程。基于之前已经证明可以在所有频率上实现50Ω输Impedance匹配的电路结构，这些电路在以前没有达到过的高频范围内实现了，通过利用 better than 2μm的薄膜特性和集成元件 such as SiN Metal-Insulator-Metal (MIM) 电容器、桥和 TaN Thin-Film Resistors (TFRs)。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Deep-Learning-for-Model-Correction-in-the-Computational-Crystallography-Toolbox"><a href="#Self-Supervised-Deep-Learning-for-Model-Correction-in-the-Computational-Crystallography-Toolbox" class="headerlink" title="Self-Supervised Deep Learning for Model Correction in the Computational Crystallography Toolbox"></a>Self-Supervised Deep Learning for Model Correction in the Computational Crystallography Toolbox</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01901">http://arxiv.org/abs/2307.01901</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gigantocypris/spread">https://github.com/gigantocypris/spread</a></li>
<li>paper_authors: Vidya Ganapati, Daniel Tchon, Aaron S. Brewster, Nicholas K. Sauter</li>
<li>for:  This paper aims to use the Computational Crystallography Toolbox (CCTBX) to determine the oxidation state of individual metal atoms in a macromolecule.</li>
<li>methods:  The paper uses self-supervised deep learning to correct the scientific model in CCTBX and provide uncertainty quantification.</li>
<li>results:  The paper describes the potential impact of using self-supervised deep learning to correct the scientific model in CCTBX and provide uncertainty quantification, and provides code for forward model simulation and data analysis at <a target="_blank" rel="noopener" href="https://github.com/gigantocypris/SPREAD.Here">https://github.com/gigantocypris/SPREAD.Here</a> is the text in Simplified Chinese:</li>
<li>for: 这篇论文使用Computational Crystallography Toolbox（CCTBX）确定蛋白质中金属原子的氧化状态。</li>
<li>methods: 这篇论文使用自我超vised深度学习修正CCTBX中的科学模型，并提供不确定性评估。</li>
<li>results: 这篇论文描述了使用自我超vised深度学习修正CCTBX中的科学模型的可能影响，并提供了<a target="_blank" rel="noopener" href="https://github.com/gigantocypris/SPREAD%E4%B8%AD%E7%9A%84%E4%BB%A3%E7%A0%81%E8%BF%9B%E8%A1%8C%E5%89%8D%E5%90%91%E6%A8%A1%E5%9E%8B%E4%BB%BF%E7%9C%9F%E5%92%8C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E3%80%82">https://github.com/gigantocypris/SPREAD中的代码进行前向模型仿真和数据分析。</a><details>
<summary>Abstract</summary>
The Computational Crystallography Toolbox (CCTBX) is open-source software that allows for processing of crystallographic data, including from serial femtosecond crystallography (SFX), for macromolecular structure determination. We aim to use the modules in CCTBX to determine the oxidation state of individual metal atoms in a macromolecule. Changes in oxidation state are reflected in small shifts of the atom's X-ray absorption edge. These energy shifts can be extracted from the diffraction images recorded in serial femtosecond crystallography, given knowledge of a forward physics model. However, as the diffraction changes only slightly due to the absorption edge shift, inaccuracies in the forward physics model make it extremely challenging to observe the oxidation state. In this work, we describe the potential impact of using self-supervised deep learning to correct the scientific model in CCTBX and provide uncertainty quantification. We provide code for forward model simulation and data analysis, built from CCTBX modules, at https://github.com/gigantocypris/SPREAD , which can be integrated with machine learning. We describe open questions in algorithm development to help spur advances through dialog between crystallographers and machine learning researchers. New methods could help elucidate charge transfer processes in many reactions, including key events in photosynthesis.
</details>
<details>
<summary>摘要</summary>
《计算 кристалagraphy工具箱（CCTBX）》是一款开源软件，用于处理晶体学数据，包括 serial femtosecond crystallography（SFX），以确定大分子结构。我们想使用 CCTBX 模块来确定杂谱中金属原子的氧化状态。氧化状态的变化会导致原子的 X-射线吸收边缘微小变化。这些能量差可以从 serial femtosecond crystallography 记录的 diffraction 图像中提取，只要知道前向物理学模型。然而，由于 diffraction 变化只是微小，因此错误在物理学模型中会导致非常困难地观察氧化状态。在这种情况下，我们描述了使用自适应深度学习来更正科学模型在 CCTBX 中的可能的影响，以及提供不确定性评估。我们在 GitHub 上提供了代码，包括 forward 物理学模型的 simulate 和数据分析，可以与机器学习结合使用。我们描述了在算法开发中的开问，以帮助推动进步，通过晶体学家和机器学习研究人员之间的对话。新的方法可以帮助解释生物化学中的电子传递过程，包括照明很重要的PhotoSynthesis 过程。
</details></li>
</ul>
<hr>
<h2 id="Grad-FEC-Unequal-Loss-Protection-of-Deep-Features-in-Collaborative-Intelligence"><a href="#Grad-FEC-Unequal-Loss-Protection-of-Deep-Features-in-Collaborative-Intelligence" class="headerlink" title="Grad-FEC: Unequal Loss Protection of Deep Features in Collaborative Intelligence"></a>Grad-FEC: Unequal Loss Protection of Deep Features in Collaborative Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01846">http://arxiv.org/abs/2307.01846</a></li>
<li>repo_url: None</li>
<li>paper_authors: Korcan Uyanik, S. Faegheh Yeganli, Ivan V. Bajić</li>
<li>for: 提高edge设备和云端的智能合作系统的可靠性和Robustness，即Collaborative Intelligence（CI）系统。</li>
<li>methods: 提出了一种基于Unequal Loss Protection（ULP）的新方法，包括特征重要度估计器，以优先保护front-end生成的重要特征包。</li>
<li>results: 实验结果表明，提出的方法可以在 packet loss 的情况下显著提高CI系统的可靠性和Robustness。<details>
<summary>Abstract</summary>
Collaborative intelligence (CI) involves dividing an artificial intelligence (AI) model into two parts: front-end, to be deployed on an edge device, and back-end, to be deployed in the cloud. The deep feature tensors produced by the front-end are transmitted to the cloud through a communication channel, which may be subject to packet loss. To address this issue, in this paper, we propose a novel approach to enhance the resilience of the CI system in the presence of packet loss through Unequal Loss Protection (ULP). The proposed ULP approach involves a feature importance estimator, which estimates the importance of feature packets produced by the front-end, and then selectively applies Forward Error Correction (FEC) codes to protect important packets. Experimental results demonstrate that the proposed approach can significantly improve the reliability and robustness of the CI system in the presence of packet loss.
</details>
<details>
<summary>摘要</summary>
共同智能（CI）包括将人工智能（AI）模型分成两部分：前端，部署在边缘设备上，和后端，部署在云端。深度特征张量生成于前端将被传输到云端通过通信频道，该频道可能会出现包loss。为 Addressing this issue, in this paper, we propose a novel approach to enhance the resilience of the CI system in the presence of packet loss through Unequal Loss Protection (ULP). The proposed ULP approach involves a feature importance estimator, which estimates the importance of feature packets produced by the front-end, and then selectively applies Forward Error Correction (FEC) codes to protect important packets. Experimental results demonstrate that the proposed approach can significantly improve the reliability and robustness of the CI system in the presence of packet loss.Here's the breakdown of the translation:* 共同智能 (CI): Collaborative intelligence* 人工智能 (AI)：Artificial intelligence* 模型 (model): Model* 前端 (front-end): Front-end* 后端 (back-end): Back-end* 云端 (cloud): Cloud* 深度特征张量 (deep feature tensors): Deep feature tensors* 包loss (packet loss): Packet loss* 强化 (enhance): Enhance* 不平等损失保护 (ULP): Unequal loss protection* 特征重要性估计器 (feature importance estimator): Feature importance estimator* 前端生成的特征包 (feature packets produced by the front-end): Feature packets produced by the front-end* FEC (Forward Error Correction) 码：Forward error correction codes* 实验结果 (experimental results): Experimental results* 可以显著提高 (can significantly improve): Can significantly improve* 可靠性 (reliability): Reliability* Robustness: Robustness
</details></li>
</ul>
<hr>
<h2 id="Multi-Channel-Feature-Extraction-for-Virtual-Histological-Staining-of-Photon-Absorption-Remote-Sensing-Images"><a href="#Multi-Channel-Feature-Extraction-for-Virtual-Histological-Staining-of-Photon-Absorption-Remote-Sensing-Images" class="headerlink" title="Multi-Channel Feature Extraction for Virtual Histological Staining of Photon Absorption Remote Sensing Images"></a>Multi-Channel Feature Extraction for Virtual Histological Staining of Photon Absorption Remote Sensing Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01824">http://arxiv.org/abs/2307.01824</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marian Boktor, James E. D. Tweel, Benjamin R. Ecclestone, Jennifer Ai Ye, Paul Fieguth, Parsin Haji Reza<br>for: 这项研究旨在提高血液染色的效率和可靠性，以便在病理诊断中提供可靠的诊断信息，帮助病理医生在疾病分类、评估和治疗规划中做出更好的决策。methods: 该研究提出了一种基于深度学习的虚拟 histological 染色框架，使用 photon absorption remote sensing（PARS）图像进行特征提取，并使用一种变体的K-means方法来捕捉有价值的多模态信息。此外，该研究还提出了一种基于传统cycleGAN框架的多通道cycleGAN（MC-GAN）模型，以包括更多的特征。results: 实验结果表明，特定的特征组合可以超过传统通道的性能，并且可以提高虚拟染色结果与化学染色（H&amp;E）图像的吻合度。在人皮肤和mouse brain组织中应用，结果表明，选择最佳特征组合是关键，可以提高虚拟染色结果的可靠性和可视化质量。<details>
<summary>Abstract</summary>
Accurate and fast histological staining is crucial in histopathology, impacting diagnostic precision and reliability. Traditional staining methods are time-consuming and subjective, causing delays in diagnosis. Digital pathology plays a vital role in advancing and optimizing histology processes to improve efficiency and reduce turnaround times. This study introduces a novel deep learning-based framework for virtual histological staining using photon absorption remote sensing (PARS) images. By extracting features from PARS time-resolved signals using a variant of the K-means method, valuable multi-modal information is captured. The proposed multi-channel cycleGAN (MC-GAN) model expands on the traditional cycleGAN framework, allowing the inclusion of additional features. Experimental results reveal that specific combinations of features outperform the conventional channels by improving the labeling of tissue structures prior to model training. Applied to human skin and mouse brain tissue, the results underscore the significance of choosing the optimal combination of features, as it reveals a substantial visual and quantitative concurrence between the virtually stained and the gold standard chemically stained hematoxylin and eosin (H&E) images, surpassing the performance of other feature combinations. Accurate virtual staining is valuable for reliable diagnostic information, aiding pathologists in disease classification, grading, and treatment planning. This study aims to advance label-free histological imaging and opens doors for intraoperative microscopy applications.
</details>
<details>
<summary>摘要</summary>
准精准快的 Histological 染色是 Histopathology 中非常重要的，它直接影响诊断的准确性和可靠性。传统的染色方法需要较长的时间和主观的干预，导致诊断的延迟。数字化Patology 在提高和优化 Histology 过程中扮演着重要的角色，以提高效率和减少回转时间。本研究提出了一种基于深度学习的虚拟 Histological 染色方法，使用 photon absorption remote sensing（PARS）图像来提取特征。通过 variants of the K-means 方法提取 PARS 时间分解信号中的有价值多Modal 信息。提出的多通道 cycleGAN（MC-GAN）模型在传统 cycleGAN 框架上进行扩展，以包括额外的特征。实验结果表明，特定的特征组合能够超越传统渠道的表现，提高识别组织结构之前的标签。应用于人皮和 Mouse brain 组织样本，结果表明选择最佳特征组合非常重要，它可以提供较高的视觉和量化协调性，超过其他特征组合。准确的虚拟染色对诊断信息的可靠性至关重要，帮助病理学家在疾病分类、评分和治疗规划中做出更加准确的决策。本研究旨在提高无标签 Histological 成像，开启了Intraoperative 镜像应用的大门。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/05/eess.IV_2023_07_05/" data-id="clollf9dj011gqc884794ewz4" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/05/cs.LG_2023_07_05/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-07-05
        
      </div>
    </a>
  
  
    <a href="/2023/07/04/cs.SD_2023_07_04/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-07-04</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">123</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">123</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">123</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">123</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">113</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">63</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
