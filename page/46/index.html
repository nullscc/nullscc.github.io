
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/46/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.AI_2023_09_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/20/cs.AI_2023_09_20/" class="article-date">
  <time datetime="2023-09-20T12:00:00.000Z" itemprop="datePublished">2023-09-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/20/cs.AI_2023_09_20/">cs.AI - 2023-09-20</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="RAI4IoE-Responsible-AI-for-Enabling-the-Internet-of-Energy"><a href="#RAI4IoE-Responsible-AI-for-Enabling-the-Internet-of-Energy" class="headerlink" title="RAI4IoE: Responsible AI for Enabling the Internet of Energy"></a>RAI4IoE: Responsible AI for Enabling the Internet of Energy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11691">http://arxiv.org/abs/2309.11691</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minhui Xue, Surya Nepal, Ling Liu, Subbu Sethuvenkatraman, Xingliang Yuan, Carsten Rudolph, Ruoxi Sun, Greg Eisenhauer</li>
<li>for: 这项研究旨在开发一个公平且负责任的AI框架，以便在互联网能源（IoE）中实现可靠的能源分布。</li>
<li>methods: 该研究使用了先进的5G-6G网络和AI技术，以连接和 инте格力化可再生分布能源资源（DERs），如电动车、存储电池、风力发电和太阳能电池。这使得DER所有者作为生产者和消费者（prosumers）可以参与能源市场，并从中获得经济收益。</li>
<li>results: 该研究的目标是确保社区成员的公平参与，并负责使用他们的数据，以便在IoE中提供安全、可靠和可再生的能源服务。<details>
<summary>Abstract</summary>
This paper plans to develop an Equitable and Responsible AI framework with enabling techniques and algorithms for the Internet of Energy (IoE), in short, RAI4IoE. The energy sector is going through substantial changes fueled by two key drivers: building a zero-carbon energy sector and the digital transformation of the energy infrastructure. We expect to see the convergence of these two drivers resulting in the IoE, where renewable distributed energy resources (DERs), such as electric cars, storage batteries, wind turbines and photovoltaics (PV), can be connected and integrated for reliable energy distribution by leveraging advanced 5G-6G networks and AI technology. This allows DER owners as prosumers to participate in the energy market and derive economic incentives. DERs are inherently asset-driven and face equitable challenges (i.e., fair, diverse and inclusive). Without equitable access, privileged individuals, groups and organizations can participate and benefit at the cost of disadvantaged groups. The real-time management of DER resources not only brings out the equity problem to the IoE, it also collects highly sensitive location, time, activity dependent data, which requires to be handled responsibly (e.g., privacy, security and safety), for AI-enhanced predictions, optimization and prioritization services, and automated management of flexible resources. The vision of our project is to ensure equitable participation of the community members and responsible use of their data in IoE so that it could reap the benefits of advances in AI to provide safe, reliable and sustainable energy services.
</details>
<details>
<summary>摘要</summary>
这份研究报告计划开发一个公平和负责任的人工智能框架（RAI4IoE），用于互联网能源（IoE）领域。能源领域正在经历重大变革，这两个关键驱动因素：建立零碳素能源产业和能源基础设施的数字变革。我们预计这两个驱动因素会相互交集，导致IoE的出现，其中可再生分布式能源资源（DERs），如电动车、存储电池、风力发电和太阳能电池（PV），可以相互连接和集成，以实现可靠的能源分布，通过利用先进的5G-6G网络和人工智能技术。这允许DER所有者作为生产者和消费者（prosumers）参与能源市场，从而获得经济收益。DERs本身具有资产驱动的特点，面临公平挑战（例如，公平、多样化和包容）。如果没有公平访问，特权个人、组织和集团可以参与和获得利益，而受折磨的群体则被排除在外。IoE实时管理DER资源不仅抛出了公平问题，还收集了高度敏感的地点、时间、活动依赖数据，需要负责任地处理（例如，隐私、安全和安全），以便通过人工智能技术提供了预测、优化和优先级服务，自动管理灵活资源。我们的项目视图是确保社区成员公平参与IoE，并负责使用他们的数据，以便IoE可以通过人工智能技术的进步获得安全、可靠和可再生的能源服务。
</details></li>
</ul>
<hr>
<h2 id="LLM-Guided-Inductive-Inference-for-Solving-Compositional-Problems"><a href="#LLM-Guided-Inductive-Inference-for-Solving-Compositional-Problems" class="headerlink" title="LLM Guided Inductive Inference for Solving Compositional Problems"></a>LLM Guided Inductive Inference for Solving Compositional Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11688">http://arxiv.org/abs/2309.11688</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhigya Sodani, Lauren Moos, Matthew Mirman</li>
<li>for: 解决大语言模型（LLM）在问答任务中表现出色，但是它们的表现受限于问题中不包含在模型训练数据中的知识，需要通过直接观察或与实际世界交互来获得。</li>
<li>methods: 我们提出了一种方法，即 Recursion based extensible LLM（REBEL），它通过自动理解技术如动态规划和前进链接策略来处理开放世界、深度理解任务。REBEL使用自然语言描述来指定工具，并使用这些工具进行递归问题分解和外部工具使用。</li>
<li>results: 我们在一组需要深度嵌套使用外部工具的问题上示出了REBEL的能力，并在一个组合和对话性的 Setting中进行了证明。<details>
<summary>Abstract</summary>
While large language models (LLMs) have demonstrated impressive performance in question-answering tasks, their performance is limited when the questions require knowledge that is not included in the model's training data and can only be acquired through direct observation or interaction with the real world. Existing methods decompose reasoning tasks through the use of modules invoked sequentially, limiting their ability to answer deep reasoning tasks. We introduce a method, Recursion based extensible LLM (REBEL), which handles open-world, deep reasoning tasks by employing automated reasoning techniques like dynamic planning and forward-chaining strategies. REBEL allows LLMs to reason via recursive problem decomposition and utilization of external tools. The tools that REBEL uses are specified only by natural language description. We further demonstrate REBEL capabilities on a set of problems that require a deeply nested use of external tools in a compositional and conversational setting.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在问答任务中表现出色，但它们的表现受到训练数据中不包含的知识的限制。现有的方法通过运行模组来 decomposing 推理任务，限制它们 Answer deep reasoning tasks. We propose a method called Recursion based extensible LLM (REBEL), which handles open-world, deep reasoning tasks by using automated reasoning techniques such as dynamic planning and forward-chaining strategies. REBEL allows LLMs to reason through recursive problem decomposition and utilize external tools. The tools that REBEL uses are specified only by natural language description. We further demonstrate REBEL's capabilities on a set of problems that require a deeply nested use of external tools in a compositional and conversational setting.
</details></li>
</ul>
<hr>
<h2 id="Dr-FERMI-A-Stochastic-Distributionally-Robust-Fair-Empirical-Risk-Minimization-Framework"><a href="#Dr-FERMI-A-Stochastic-Distributionally-Robust-Fair-Empirical-Risk-Minimization-Framework" class="headerlink" title="Dr. FERMI: A Stochastic Distributionally Robust Fair Empirical Risk Minimization Framework"></a>Dr. FERMI: A Stochastic Distributionally Robust Fair Empirical Risk Minimization Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11682">http://arxiv.org/abs/2309.11682</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sina Baharlouei, Meisam Razaviyayn</li>
<li>for: This paper aims to address the issue of fair machine learning models behaving unfairly on test data due to distribution shifts.</li>
<li>methods: The proposed method is based on distributionally robust optimization under $L_p$ norm uncertainty sets with respect to the Exponential Renyi Mutual Information (ERMI) as the measure of fairness violation. The method does not require knowledge of the causal graph and can be implemented in a stochastic fashion.</li>
<li>results: The proposed framework has been evaluated through extensive experiments on real datasets consisting of distribution shifts, and the results show that it performs well in terms of fairness and efficiency.<details>
<summary>Abstract</summary>
While training fair machine learning models has been studied extensively in recent years, most developed methods rely on the assumption that the training and test data have similar distributions. In the presence of distribution shifts, fair models may behave unfairly on test data. There have been some developments for fair learning robust to distribution shifts to address this shortcoming. However, most proposed solutions are based on the assumption of having access to the causal graph describing the interaction of different features. Moreover, existing algorithms require full access to data and cannot be used when small batches are used (stochastic/batch implementation). This paper proposes the first stochastic distributionally robust fairness framework with convergence guarantees that do not require knowledge of the causal graph. More specifically, we formulate the fair inference in the presence of the distribution shift as a distributionally robust optimization problem under $L_p$ norm uncertainty sets with respect to the Exponential Renyi Mutual Information (ERMI) as the measure of fairness violation. We then discuss how the proposed method can be implemented in a stochastic fashion. We have evaluated the presented framework's performance and efficiency through extensive experiments on real datasets consisting of distribution shifts.
</details>
<details>
<summary>摘要</summary>
traditional machine learning models have been extensively studied in recent years, but most of these methods rely on the assumption that the training and test data have similar distributions. However, in the presence of distribution shifts, fair models may behave unfairly on test data. To address this shortcoming, there have been some developments in fair learning that are robust to distribution shifts, but these methods are based on the assumption of having access to the causal graph describing the interaction of different features. Moreover, existing algorithms require full access to data and cannot be used when small batches are used (stochastic/batch implementation). This paper proposes the first stochastic distributionally robust fairness framework with convergence guarantees that do not require knowledge of the causal graph. Specifically, we formulate the fair inference in the presence of distribution shift as a distributionally robust optimization problem under $L_p$ norm uncertainty sets with respect to the Exponential Renyi Mutual Information (ERMI) as the measure of fairness violation. We then discuss how the proposed method can be implemented in a stochastic fashion. We have evaluated the presented framework's performance and efficiency through extensive experiments on real datasets consisting of distribution shifts.Here's the translation in Traditional Chinese:传统机器学习模型在最近的年份已经得到了广泛的研究，但大多数这些方法假设训练和测试数据的分布相似。然而，在分布shift情况下，公平的模型可能会在测试数据上不公平。为了解决这问题，有些开发了不同的公平学习方法，但这些方法假设有存在 causal graph 描述不同特征之间的互动。此外，现有的算法需要完整的数据存取，并且无法在小批量中使用（stochastic/batch实现）。本文提出了首个可靠的分布robust公平性框架，不需要知道 causal graph。具体来说，我们将 fair inference 在分布shift情况下形式化为分布robust优化问题，并使用 $L_p$  нор uncertainty set 来度量公平违反。我们然后讨论了如何实现这个方法在抽象的方式上。我们通过实际的实验，评估了提出的框架的性能和效率，以及实际应用中的可靠性。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-with-Neural-Graphical-Models"><a href="#Federated-Learning-with-Neural-Graphical-Models" class="headerlink" title="Federated Learning with Neural Graphical Models"></a>Federated Learning with Neural Graphical Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11680">http://arxiv.org/abs/2309.11680</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Urszula Chajewska, Harsh Shrivastava</li>
<li>for: 该论文旨在创建基于专有数据的模型，以便多个客户保留专有数据控制权，同时通过共享资源提高模型准确性。</li>
<li>methods: 该论文提出了一种基于神经网络的联邦学习框架（FedNGMs），使用 probabilistic graphical models（NGMs）学习复杂的输入特征之间的非线性关系。</li>
<li>results: 该论文的 FedNGMs 框架可以避免 neuron matching 框架如 Federated Matched Averaging 的缺点，并且可以适应数据不均衡、多个参与者和limited communication bandwidth 等问题。<details>
<summary>Abstract</summary>
Federated Learning (FL) addresses the need to create models based on proprietary data in such a way that multiple clients retain exclusive control over their data, while all benefit from improved model accuracy due to pooled resources. Recently proposed Neural Graphical Models (NGMs) are Probabilistic Graphical models that utilize the expressive power of neural networks to learn complex non-linear dependencies between the input features. They learn to capture the underlying data distribution and have efficient algorithms for inference and sampling. We develop a FL framework which maintains a global NGM model that learns the averaged information from the local NGM models while keeping the training data within the client's environment. Our design, FedNGMs, avoids the pitfalls and shortcomings of neuron matching frameworks like Federated Matched Averaging that suffers from model parameter explosion. Our global model size remains constant throughout the process. In the cases where clients have local variables that are not part of the combined global distribution, we propose a `Stitching' algorithm, which personalizes the global NGM models by merging the additional variables using the client's data. FedNGM is robust to data heterogeneity, large number of participants, and limited communication bandwidth.
</details>
<details>
<summary>摘要</summary>
Federated Learning (FL) 解决了基于专有数据的模型创建的需求，以便多个客户端保留专有数据控制权，而同时各自受益于共享资源的提高模型精度。最近提出的神经图模型（NGM）是一种概率图模型，利用神经网络的表达能力来学习输入特征之间的复杂非线性关系。它们学习下面数据分布，并有效的推理和采样算法。我们开发了一个基于FL的框架，称之为FedNGMs，该框架在客户端环境中保持global NGM模型，该模型学习客户端的local NGM模型中的均值信息，而不需要将训练数据传输到客户端。我们的设计避免了神经网络匹配框架如联邦匹配平均的缺点，例如模型参数爆炸。我们的全球模型大小在训练过程中保持不变。在客户端有本地变量，这些变量不是全局共享的共同分布中的一部分时，我们提议使用"缝合"算法，将这些变量与全局NGM模型进行个性化结合。FedNGM具有对数据不一致、大量参与者和有限通信带宽的 Robustness。
</details></li>
</ul>
<hr>
<h2 id="Generative-AI-in-Mafia-like-Game-Simulation"><a href="#Generative-AI-in-Mafia-like-Game-Simulation" class="headerlink" title="Generative AI in Mafia-like Game Simulation"></a>Generative AI in Mafia-like Game Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11672">http://arxiv.org/abs/2309.11672</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MunyeongKim/Gen-AI-in-Mafia-like-Game">https://github.com/MunyeongKim/Gen-AI-in-Mafia-like-Game</a></li>
<li>paper_authors: Munyeong Kim, Sungsu Kim</li>
<li>for: 这项研究探讨了生成式人工智能模型在角色扮演 simulations 中的可能性和潜力，以游戏 Spyfall 为例。</li>
<li>methods: 研究使用 GPT-4 的高级功能，通过对游戏场景的理解、决策和互动进行了比较分析，以证明模型在这些方面的潜力。</li>
<li>results: 研究发现，GPT-4 在游戏环境中的适应性有显著提高，能够更好地提问和发表人类化的回答。然而，模型在骗取和预测对手行动方面存在限制。研究还讨论了游戏开发、财政限制和非语言限制的问题。结果表明，虽然 GPT-4 表现出了较早模型的进步，但还有更多的发展空间，尤其是在塑造更人类化的 AI 模型。<details>
<summary>Abstract</summary>
In this research, we explore the efficacy and potential of Generative AI models, specifically focusing on their application in role-playing simulations exemplified through Spyfall, a renowned mafia-style game. By leveraging GPT-4's advanced capabilities, the study aimed to showcase the model's potential in understanding, decision-making, and interaction during game scenarios. Comparative analyses between GPT-4 and its predecessor, GPT-3.5-turbo, demonstrated GPT-4's enhanced adaptability to the game environment, with significant improvements in posing relevant questions and forming human-like responses. However, challenges such as the model;s limitations in bluffing and predicting opponent moves emerged. Reflections on game development, financial constraints, and non-verbal limitations of the study were also discussed. The findings suggest that while GPT-4 exhibits promising advancements over earlier models, there remains potential for further development, especially in instilling more human-like attributes in AI.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们探索了生成AI模型的效果和潜力，特别是在游戏角色扮演 simulations中的应用。通过利用GPT-4的高级功能，研究旨在表明模型在游戏场景中的理解、决策和互动的潜力。对比GPT-4和其前一代GPT-3.5-turbo，研究发现GPT-4在游戏环境中的适应性得到了显著提升，特别是在提问和表达人类化的问题方面。然而，模型在谎言和预测对手行动方面存在限制。研究还讨论了游戏开发、财务限制和非语言限制的问题。研究结果表明，虽然GPT-4在前一代模型之上具有显著的进步，但还有可能进一步发展，尤其是在具备更多人类特征的AI方面。
</details></li>
</ul>
<hr>
<h2 id="“It’s-a-Fair-Game’’-or-Is-It-Examining-How-Users-Navigate-Disclosure-Risks-and-Benefits-When-Using-LLM-Based-Conversational-Agents"><a href="#“It’s-a-Fair-Game’’-or-Is-It-Examining-How-Users-Navigate-Disclosure-Risks-and-Benefits-When-Using-LLM-Based-Conversational-Agents" class="headerlink" title="“It’s a Fair Game’’, or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents"></a>“It’s a Fair Game’’, or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11653">http://arxiv.org/abs/2309.11653</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiping Zhang, Michelle Jia, Hao-Ping, Lee, Bingsheng Yao, Sauvik Das, Ada Lerner, Dakuo Wang, Tianshi Li</li>
<li>for: 本研究旨在帮助建立优先考虑用户隐私的大语言模型（LLM）基于对话代理（CA），以解决现有研究主要集中在模型方面，忽略用户视角的问题。</li>
<li>methods: 本研究通过分析实际的ChatGPT对话和对19名LLM基于CA用户进行semi结构化采访，发现用户在使用LLM基于CA时经常面临privacy、utilities和便利性之间的权衡决策。</li>
<li>results: 研究发现用户的错误心理模型和系统设计中的黑暗 Patterns限制了他们对隐私风险的认识和理解，同时人工智能化的交互使用者更容易对自己的敏感信息进行披露，使用者在决策中受到增加的困难。<details>
<summary>Abstract</summary>
The widespread use of Large Language Model (LLM)-based conversational agents (CAs), especially in high-stakes domains, raises many privacy concerns. Building ethical LLM-based CAs that respect user privacy requires an in-depth understanding of the privacy risks that concern users the most. However, existing research, primarily model-centered, does not provide insight into users' perspectives. To bridge this gap, we analyzed sensitive disclosures in real-world ChatGPT conversations and conducted semi-structured interviews with 19 LLM-based CA users. We found that users are constantly faced with trade-offs between privacy, utility, and convenience when using LLM-based CAs. However, users' erroneous mental models and the dark patterns in system design limited their awareness and comprehension of the privacy risks. Additionally, the human-like interactions encouraged more sensitive disclosures, which complicated users' ability to navigate the trade-offs. We discuss practical design guidelines and the needs for paradigmatic shifts to protect the privacy of LLM-based CA users.
</details>
<details>
<summary>摘要</summary>
广泛使用大语言模型（LLM）基于对话代理（CA），特别在高风险领域，引发了许多隐私问题。建立尊重用户隐私的LLM基于CA需要深入了解用户关心的隐私风险。然而，现有研究主要关注模型，未能提供用户视角的深入理解。为了补强这个差距，我们分析了实际的ChatGPT对话中的敏感泄露，并进行了19名LLM基于CA用户的semi结构化采访。我们发现，用户在使用LLM基于CA时经常面临privacy、功能和便利性之间的权衡。然而，用户的错误的认知模型和系统设计中的黑暗Patterns限制了他们对隐私风险的认识和理解。此外，人类化的互动更加鼓励用户提供更多的敏感信息，使用户更难avigate权衡。我们讨论了实用的设计指南和保护LLM基于CA用户隐私的需求。
</details></li>
</ul>
<hr>
<h2 id="Orbital-AI-based-Autonomous-Refuelling-Solution"><a href="#Orbital-AI-based-Autonomous-Refuelling-Solution" class="headerlink" title="Orbital AI-based Autonomous Refuelling Solution"></a>Orbital AI-based Autonomous Refuelling Solution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11648">http://arxiv.org/abs/2309.11648</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duarte Rondao, Lei He, Nabil Aouf</li>
<li>for: 这篇论文旨在探讨使用摄像头进行太空对接和轨道服务（OOS），并且使用人工智能（AI）来将摄像头变成主要感知器。</li>
<li>methods: 这篇论文使用了许多 convolutional neural network（CNN）Backbone架构， benchmarked on synthetically generated docking manoeuvres with the International Space Station（ISS），以获得position和态度估算。</li>
<li>results: 这篇论文的结果显示，使用AI可以将relative navigation solution扩展到多种enario，例如targets或照明条件，并且可以实现position和态度估算的高精度。实际上，该方法可以大大减少了需要的工程师时间和资源。<details>
<summary>Abstract</summary>
Cameras are rapidly becoming the choice for on-board sensors towards space rendezvous due to their small form factor and inexpensive power, mass, and volume costs. When it comes to docking, however, they typically serve a secondary role, whereas the main work is done by active sensors such as lidar. This paper documents the development of a proposed AI-based (artificial intelligence) navigation algorithm intending to mature the use of on-board visible wavelength cameras as a main sensor for docking and on-orbit servicing (OOS), reducing the dependency on lidar and greatly reducing costs. Specifically, the use of AI enables the expansion of the relative navigation solution towards multiple classes of scenarios, e.g., in terms of targets or illumination conditions, which would otherwise have to be crafted on a case-by-case manner using classical image processing methods. Multiple convolutional neural network (CNN) backbone architectures are benchmarked on synthetically generated data of docking manoeuvres with the International Space Station (ISS), achieving position and attitude estimates close to 1% range-normalised and 1 deg, respectively. The integration of the solution with a physical prototype of the refuelling mechanism is validated in laboratory using a robotic arm to simulate a berthing procedure.
</details>
<details>
<summary>摘要</summary>
随着航天器的发展，镜头在航天器上的应用也在不断扩大。镜头的小型化和低功耗、质量和体积成本使其成为航天器上的首选感知器。然而，在协 docking 过程中，镜头通常扮演着次要角色，主要工作由活动感知器 such as lidar 完成。这篇论文描述了一种基于人工智能（AI）的导航算法的开发，旨在通过使用镜头来提高协 docking 和空间服务（OOS）中的精度和可靠性。使用 AI 可以扩展相对导航解决方案到多种场景，如目标或照明条件，而这些场景之前只能通过经典图像处理方法来手动设计。本文使用多种卷积神经网络（CNN）后处理器，对人工生成的协 docking 演示数据进行了测试，实现了 Position 和 Attitude 估计的准确率接近 1% 范围内和 1 度。此外，将解决方案与实际储存机制的物理 прототип结合，在实验室中使用 робо臂模拟协 docking 过程进行验证。
</details></li>
</ul>
<hr>
<h2 id="Attentive-VQ-VAE"><a href="#Attentive-VQ-VAE" class="headerlink" title="Attentive VQ-VAE"></a>Attentive VQ-VAE</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11641">http://arxiv.org/abs/2309.11641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mariano Rivera, Angello Hoyos</li>
<li>for: 提高 VQVAE 模型的能力，保持实用参数水平</li>
<li>methods:  integrate Attentive Residual Encoder (AREN) 和 Residual Pixel Attention layer，使用多级编码器，并采用内部自注意力机制来有效地捕捉和利用 Contextual information</li>
<li>results: 实验结果表明，提案的修改可以明显提高数据表示和生成能力，使 VQVAEs 更适合各种应用。<details>
<summary>Abstract</summary>
We present a novel approach to enhance the capabilities of VQVAE models through the integration of an Attentive Residual Encoder (AREN) and a Residual Pixel Attention layer. The objective of our research is to improve the performance of VQVAE while maintaining practical parameter levels. The AREN encoder is designed to operate effectively at multiple levels, accommodating diverse architectural complexities. The key innovation is the integration of an inter-pixel auto-attention mechanism into the AREN encoder. This approach allows us to efficiently capture and utilize contextual information across latent vectors. Additionally, our models uses additional encoding levels to further enhance the model's representational power. Our attention layer employs a minimal parameter approach, ensuring that latent vectors are modified only when pertinent information from other pixels is available. Experimental results demonstrate that our proposed modifications lead to significant improvements in data representation and generation, making VQVAEs even more suitable for a wide range of applications.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，通过结合Attentive Residual Encoder（AREN）和Residual Pixel Attention层，以提高VQVAE模型的能力。我们的研究目标是提高VQVAE表现，同时保持实际参数水平。AREN编码器设计可以在多个层次上运行，适应不同的建筑复杂性。我们的关键创新是将Inter-pixel自动注意机制integrated into AREN编码器。这种方法使得我们能够效率地捕捉并利用 latent vector中的上下文信息。此外，我们的模型还使用了多个编码层，以进一步增强模型的表达力。我们的注意层采用了最小参数的方法，确保latent vector只有当其他像素中有pertinent information时才会被修改。实验结果表明，我们的修改导致了数据表示和生成的显著改进，使VQVAEs更适合各种应用。
</details></li>
</ul>
<hr>
<h2 id="A-survey-on-the-semantics-of-sequential-patterns-with-negation"><a href="#A-survey-on-the-semantics-of-sequential-patterns-with-negation" class="headerlink" title="A survey on the semantics of sequential patterns with negation"></a>A survey on the semantics of sequential patterns with negation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11638">http://arxiv.org/abs/2309.11638</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Guyet</li>
<li>for: 本研究的目的是探讨用户对逻辑不同的时间序列模式具有何种直观性？</li>
<li>methods: 本研究使用了一份问卷来探讨用户对不同 semantics 的直观性。</li>
<li>results: 研究发现用户对两种 semantics 具有直观性，但这两种 semantics 并不与现有的主流算法 semantics 一致。因此，本研究提出了一些建议，以便更好地考虑这些差异。<details>
<summary>Abstract</summary>
A sequential pattern with negation, or negative sequential pattern, takes the form of a sequential pattern for which the negation symbol may be used in front of some of the pattern's itemsets. Intuitively, such a pattern occurs in a sequence if negated itemsets are absent in the sequence. Recent work has shown that different semantics can be attributed to these pattern forms, and that state-of-the-art algorithms do not extract the same sets of patterns. This raises the important question of the interpretability of sequential pattern with negation. In this study, our focus is on exploring how potential users perceive negation in sequential patterns. Our aim is to determine whether specific semantics are more "intuitive" than others and whether these align with the semantics employed by one or more state-of-the-art algorithms. To achieve this, we designed a questionnaire to reveal the semantics' intuition of each user. This article presents both the design of the questionnaire and an in-depth analysis of the 124 responses obtained. The outcomes indicate that two of the semantics are predominantly intuitive; however, neither of them aligns with the semantics of the primary state-of-the-art algorithms. As a result, we provide recommendations to account for this disparity in the conclusions drawn.
</details>
<details>
<summary>摘要</summary>
一种顺序模式 WITH negation，或负顺序模式，的形式是一种顺序模式，其中可以在一些模式itemset前面使用否定符。Intuitively，这种模式在序列中出现，当负否定itemset缺失在序列中。 recent work has shown that different semantics can be attributed to these pattern forms, and that state-of-the-art algorithms do not extract the same sets of patterns. This raises the important question of the interpretability of sequential pattern with negation. In this study, our focus is on exploring how potential users perceive negation in sequential patterns. Our aim is to determine whether specific semantics are more "intuitive" than others and whether these align with the semantics employed by one or more state-of-the-art algorithms. To achieve this, we designed a questionnaire to reveal the semantics' intuition of each user. This article presents both the design of the questionnaire and an in-depth analysis of the 124 responses obtained. The outcomes indicate that two of the semantics are predominantly intuitive; however, neither of them aligns with the semantics of the primary state-of-the-art algorithms. As a result, we provide recommendations to account for this disparity in the conclusions drawn.Note: The word "WITH" in the original text is not translated as it is not a word in Simplified Chinese. Instead, the phrase "顺序模式 WITH negation" is translated as "顺序模式 WITH 否定" (sequential pattern with negation).
</details></li>
</ul>
<hr>
<h2 id="Cloud-Based-Hierarchical-Imitation-Learning-for-Scalable-Transfer-of-Construction-Skills-from-Human-Workers-to-Assisting-Robots"><a href="#Cloud-Based-Hierarchical-Imitation-Learning-for-Scalable-Transfer-of-Construction-Skills-from-Human-Workers-to-Assisting-Robots" class="headerlink" title="Cloud-Based Hierarchical Imitation Learning for Scalable Transfer of Construction Skills from Human Workers to Assisting Robots"></a>Cloud-Based Hierarchical Imitation Learning for Scalable Transfer of Construction Skills from Human Workers to Assisting Robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11619">http://arxiv.org/abs/2309.11619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongrui Yu, Vineet R. Kamat, Carol C. Menassa</li>
<li>for: 这个研究旨在将建筑工程中的重重和physically-demanding任务交给机器人，以降低人工伤害。</li>
<li>methods: 这个研究使用Imitation Learning（IL）技术将职人的手艺技能转移到机器人身上，以成功委托建筑工程任务和获得高品质机器人制成的成果。</li>
<li>results: 这个研究提出了一个具有实验学习（HIL）模型和云 robotics技术的虚拟示范框架，可以帮助将职人的手艺技能转移到机器人身上，并且可以重复使用这些示范，以减少人工示范的需求。这个框架可以帮助提高建筑工程中的雇员多样性和教育背景。<details>
<summary>Abstract</summary>
Assigning repetitive and physically-demanding construction tasks to robots can alleviate human workers's exposure to occupational injuries. Transferring necessary dexterous and adaptive artisanal construction craft skills from workers to robots is crucial for the successful delegation of construction tasks and achieving high-quality robot-constructed work. Predefined motion planning scripts tend to generate rigid and collision-prone robotic behaviors in unstructured construction site environments. In contrast, Imitation Learning (IL) offers a more robust and flexible skill transfer scheme. However, the majority of IL algorithms rely on human workers to repeatedly demonstrate task performance at full scale, which can be counterproductive and infeasible in the case of construction work. To address this concern, this paper proposes an immersive, cloud robotics-based virtual demonstration framework that serves two primary purposes. First, it digitalizes the demonstration process, eliminating the need for repetitive physical manipulation of heavy construction objects. Second, it employs a federated collection of reusable demonstrations that are transferable for similar tasks in the future and can thus reduce the requirement for repetitive illustration of tasks by human agents. Additionally, to enhance the trustworthiness, explainability, and ethical soundness of the robot training, this framework utilizes a Hierarchical Imitation Learning (HIL) model to decompose human manipulation skills into sequential and reactive sub-skills. These two layers of skills are represented by deep generative models, enabling adaptive control of robot actions. By delegating the physical strains of construction work to human-trained robots, this framework promotes the inclusion of workers with diverse physical capabilities and educational backgrounds within the construction industry.
</details>
<details>
<summary>摘要</summary>
<<SYS>>发现给定文本的简化中文翻译。<</SYS>>委托 repetitive 和 physically-demanding 的建筑任务给机器人，可以减轻人工工作者的职业危害风险。将必要的灵活和适应的艺术工艺技能从工作者传递到机器人是成功委托建筑任务和获得高质量机器人构建的关键。预定的运动规划脚本通常在无结构的建筑现场环境中生成僵化和碰撞的机器人行为。相比之下，学习模式（IL）提供了更加稳定和灵活的技能传递方案。然而，大多数 IL 算法需要人工工作者重复地展示任务完成，这可能是不可能的和不可预期的在建筑工作中。为解决这个问题，本文提出了一个 immerse 云 robotics 基础设施，它拥有以下两个主要目的：首先，它将示例过程数字化，从而消除重复地Physical 执行重构建筑物品的需要。其次，它使用一个 Federated 集合的可重用示例，以便在未来对类似任务进行快速协调。此外，为了增强机器人培训的可靠性、可解释性和伦理合理性，该框架使用 Hierarchical Imitation Learning（HIL）模型，将人类抓取技能 decomposed 成Sequential 和 reactive 两层。这两层技能被表示为深度生成模型，以便在机器人行为中进行适应控制。通过委托建筑工作给人类培训的机器人，这个框架推广了建筑业中不同的身体能力和教育背景的人员的包容性。
</details></li>
</ul>
<hr>
<h2 id="Hand-Gesture-Recognition-with-Two-Stage-Approach-Using-Transfer-Learning-and-Deep-Ensemble-Learning"><a href="#Hand-Gesture-Recognition-with-Two-Stage-Approach-Using-Transfer-Learning-and-Deep-Ensemble-Learning" class="headerlink" title="Hand Gesture Recognition with Two Stage Approach Using Transfer Learning and Deep Ensemble Learning"></a>Hand Gesture Recognition with Two Stage Approach Using Transfer Learning and Deep Ensemble Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11610">http://arxiv.org/abs/2309.11610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Serkan Savaş, Atilla Ergüzen</li>
<li>for: 这个研究的目的是对人工智能与 Computing 进行改进，以提高其性能。</li>
<li>methods: 这个研究使用了深度学习技术，特别是卷积神经网络，并将其应用于识别手势。</li>
<li>results: 研究获得了98.88%的准确率，这表明了深度ensemble学习技术在人工智能与Computing 中的应用潜力。<details>
<summary>Abstract</summary>
Human-Computer Interaction (HCI) has been the subject of research for many years, and recent studies have focused on improving its performance through various techniques. In the past decade, deep learning studies have shown high performance in various research areas, leading researchers to explore their application to HCI. Convolutional neural networks can be used to recognize hand gestures from images using deep architectures. In this study, we evaluated pre-trained high-performance deep architectures on the HG14 dataset, which consists of 14 different hand gesture classes. Among 22 different models, versions of the VGGNet and MobileNet models attained the highest accuracy rates. Specifically, the VGG16 and VGG19 models achieved accuracy rates of 94.64% and 94.36%, respectively, while the MobileNet and MobileNetV2 models achieved accuracy rates of 96.79% and 94.43%, respectively. We performed hand gesture recognition on the dataset using an ensemble learning technique, which combined the four most successful models. By utilizing these models as base learners and applying the Dirichlet ensemble technique, we achieved an accuracy rate of 98.88%. These results demonstrate the effectiveness of the deep ensemble learning technique for HCI and its potential applications in areas such as augmented reality, virtual reality, and game technologies.
</details>
<details>
<summary>摘要</summary>
人机交互（HCI）已经是多年的研究主题，而最近的研究强调提高其性能通过不同的技术。过去十年，深度学习研究在各个领域表现出色，导致研究者想要把它们应用于HCI。通过深度神经网络识别手势图像，可以使用深度建筑。本研究在HG14数据集上评估了22种不同的模型，其中包括VGGNet和MobileNet模型的多种版本。结果发现，VGG16和VGG19模型的准确率分别为94.64%和94.36%，而MobileNet和MobileNetV2模型的准确率分别为96.79%和94.43%。我们使用了ensemble学习技术，将这些模型作为基础学习器，并应用Dirichlet ensemble技术，达到了98.88%的准确率。这些结果表明深度ensemble学习技术在HCI中的效iveness，并在虚拟现实、扩展现实和游戏技术等领域有潜力应用。
</details></li>
</ul>
<hr>
<h2 id="Dataset-Factory-A-Toolchain-For-Generative-Computer-Vision-Datasets"><a href="#Dataset-Factory-A-Toolchain-For-Generative-Computer-Vision-Datasets" class="headerlink" title="Dataset Factory: A Toolchain For Generative Computer Vision Datasets"></a>Dataset Factory: A Toolchain For Generative Computer Vision Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11608">http://arxiv.org/abs/2309.11608</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Kharitonov, Ryan Turner</li>
<li>for: 该论文旨在解决生成AI工作流程中数据处理问题，提高数据处理效率和可重用性。</li>
<li>methods: 该论文提出了一种“数据工厂”方法，将样本存储和处理分离于元数据，并允许数据驱动操作进行批处理。</li>
<li>results: 该论文的实验结果表明，使用“数据工厂”方法可以提高生成AI工作流程的数据处理效率和可重用性。<details>
<summary>Abstract</summary>
Generative AI workflows heavily rely on data-centric tasks - such as filtering samples by annotation fields, vector distances, or scores produced by custom classifiers. At the same time, computer vision datasets are quickly approaching petabyte volumes, rendering data wrangling difficult. In addition, the iterative nature of data preparation necessitates robust dataset sharing and versioning mechanisms, both of which are hard to implement ad-hoc. To solve these challenges, we propose a "dataset factory" approach that separates the storage and processing of samples from metadata and enables data-centric operations at scale for machine learning teams and individual researchers.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="CATS-Conditional-Adversarial-Trajectory-Synthesis-for-Privacy-Preserving-Trajectory-Data-Publication-Using-Deep-Learning-Approaches"><a href="#CATS-Conditional-Adversarial-Trajectory-Synthesis-for-Privacy-Preserving-Trajectory-Data-Publication-Using-Deep-Learning-Approaches" class="headerlink" title="CATS: Conditional Adversarial Trajectory Synthesis for Privacy-Preserving Trajectory Data Publication Using Deep Learning Approaches"></a>CATS: Conditional Adversarial Trajectory Synthesis for Privacy-Preserving Trajectory Data Publication Using Deep Learning Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11587">http://arxiv.org/abs/2309.11587</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/geods/cats">https://github.com/geods/cats</a></li>
<li>paper_authors: Jinmeng Rao, Song Gao, Sijia Zhu</li>
<li>for: 本研究使用深度学习技术保护人员流动数据隐私，并生成高质量的人员流动数据。</li>
<li>methods: 本研究使用K-anonymity保证人员流动数据的分布水平隐私，并使用条件对抗训练、人流环境学习和相邻轨迹点匹配来重建轨迹 topology。</li>
<li>results: 实验结果表明，我们的方法在隐私保护、空间时间特征保持和下游实用性方面比基线方法表现更好，为人流动数据隐私研究Using生成AI技术和数据伦理问题提供新的视角。<details>
<summary>Abstract</summary>
The prevalence of ubiquitous location-aware devices and mobile Internet enables us to collect massive individual-level trajectory dataset from users. Such trajectory big data bring new opportunities to human mobility research but also raise public concerns with regard to location privacy. In this work, we present the Conditional Adversarial Trajectory Synthesis (CATS), a deep-learning-based GeoAI methodological framework for privacy-preserving trajectory data generation and publication. CATS applies K-anonymity to the underlying spatiotemporal distributions of human movements, which provides a distributional-level strong privacy guarantee. By leveraging conditional adversarial training on K-anonymized human mobility matrices, trajectory global context learning using the attention-based mechanism, and recurrent bipartite graph matching of adjacent trajectory points, CATS is able to reconstruct trajectory topology from conditionally sampled locations and generate high-quality individual-level synthetic trajectory data, which can serve as supplements or alternatives to raw data for privacy-preserving trajectory data publication. The experiment results on over 90k GPS trajectories show that our method has a better performance in privacy preservation, spatiotemporal characteristic preservation, and downstream utility compared with baseline methods, which brings new insights into privacy-preserving human mobility research using generative AI techniques and explores data ethics issues in GIScience.
</details>
<details>
<summary>摘要</summary>
“现代社会中普遍存在 ubique 位置意识设备和移动互联网，我们可以从用户收集巨大的个人化轨迹数据集。这些轨迹大数据为人类活动研究带来了新的机会，但也引起了人们关于位置隐私的担忧。在这种情况下，我们提出了 Conditional Adversarial Trajectory Synthesis（CATS），一种基于深度学习的GeoAI方法框架，用于隐私保护的轨迹数据生成和发布。CATS通过对人类活动的下层空间时间分布进行K-anonimity处理，提供了强的隐私保证。通过使用受条件 adversarial 训练的人类活动矩阵，沿着邻近轨迹点的循环双向图匹配，以及使用注意力机制进行轨迹全球上下文学习，CATS可以从受条件采样的位置中重建轨迹拓扑，并生成高质量的个人化 sintetic 轨迹数据，可以作为隐私保护下的轨迹数据发布的补充或替代。实验结果表明，我们的方法在隐私保护、空间时间特征保持和下游实用性方面表现更好于基eline方法，这带来了新的思路 для隐私保护的人类活动研究，并探讨了GIScience中的数据伦理问题。”
</details></li>
</ul>
<hr>
<h2 id="Distilling-Adversarial-Prompts-from-Safety-Benchmarks-Report-for-the-Adversarial-Nibbler-Challenge"><a href="#Distilling-Adversarial-Prompts-from-Safety-Benchmarks-Report-for-the-Adversarial-Nibbler-Challenge" class="headerlink" title="Distilling Adversarial Prompts from Safety Benchmarks: Report for the Adversarial Nibbler Challenge"></a>Distilling Adversarial Prompts from Safety Benchmarks: Report for the Adversarial Nibbler Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11575">http://arxiv.org/abs/2309.11575</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manuel Brack, Patrick Schramowski, Kristian Kersting</li>
<li>for: 本研究旨在挑战现有的生成图像模型安全性问题，通过分析潜在的攻击输入来检测模型的脆弱性。</li>
<li>methods: 研究人员使用了现有的安全准则来生成大量的攻击输入，并对这些输入和相应的图像进行分析，以探讨当前生成图像模型中的安全问题。</li>
<li>results: 研究人员发现了许多潜在的安全问题，包括输入筛选器的脆弱性和系统性的安全问题，这些问题可能会影响生成图像模型的安全性。<details>
<summary>Abstract</summary>
Text-conditioned image generation models have recently achieved astonishing image quality and alignment results. Consequently, they are employed in a fast-growing number of applications. Since they are highly data-driven, relying on billion-sized datasets randomly scraped from the web, they also produce unsafe content. As a contribution to the Adversarial Nibbler challenge, we distill a large set of over 1,000 potential adversarial inputs from existing safety benchmarks. Our analysis of the gathered prompts and corresponding images demonstrates the fragility of input filters and provides further insights into systematic safety issues in current generative image models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="BTLM-3B-8K-7B-Parameter-Performance-in-a-3B-Parameter-Model"><a href="#BTLM-3B-8K-7B-Parameter-Performance-in-a-3B-Parameter-Model" class="headerlink" title="BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model"></a>BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11568">http://arxiv.org/abs/2309.11568</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nolan Dey, Daria Soboleva, Faisal Al-Khateeb, Bowen Yang, Ribhu Pathria, Hemant Khachane, Shaheer Muhammad, Zhiming, Chen, Robert Myers, Jacob Robert Steeves, Natalia Vassilieva, Marvin Tom, Joel Hestness</li>
<li>for: 这个论文的目的是介绍一种新的语言模型，即BTLM-3B-8K，该模型在下游任务中表现出色，并且具有较小的参数大小和计算资源占用。</li>
<li>methods: 该模型使用了一些现有的技术，包括ALiBi位嵌入和SwiGLU非线性函数，并且通过优化Hyperparameter和学习环境来提高模型的性能。</li>
<li>results: 相比其他3B参数模型，BTLM-3B-8K在下游任务中表现出2-5.5%的提升，而且在长上下文任务中也表现出优秀的表现，比如MPT-7B-8K和XGen-7B-8K。此外，BTLM-3B-8K的计算资源占用相对较少，只需3GB的内存和2.5倍的计算资源。<details>
<summary>Abstract</summary>
We introduce the Bittensor Language Model, called "BTLM-3B-8K", a new state-of-the-art 3 billion parameter open-source language model. BTLM-3B-8K was trained on 627B tokens from the SlimPajama dataset with a mixture of 2,048 and 8,192 context lengths. BTLM-3B-8K outperforms all existing 3B parameter models by 2-5.5% across downstream tasks. BTLM-3B-8K is even competitive with some 7B parameter models. Additionally, BTLM-3B-8K provides excellent long context performance, outperforming MPT-7B-8K and XGen-7B-8K on tasks up to 8,192 context length. We trained the model on a cleaned and deduplicated SlimPajama dataset; aggressively tuned the \textmu P hyperparameters and schedule; used ALiBi position embeddings; and adopted the SwiGLU nonlinearity.   On Hugging Face, the most popular models have 7B parameters, indicating that users prefer the quality-size ratio of 7B models. Compacting the 7B parameter model to one with 3B parameters, with little performance impact, is an important milestone. BTLM-3B-8K needs only 3GB of memory with 4-bit precision and takes 2.5x less inference compute than 7B models, helping to open up access to a powerful language model on mobile and edge devices. BTLM-3B-8K is available under an Apache 2.0 license on Hugging Face: https://huggingface.co/cerebras/btlm-3b-8k-base.
</details>
<details>
<summary>摘要</summary>
我们介绍“BTLM-3B-8K”语言模型，是一个新的州际之冠开源语言模型，拥有30亿个参数。BTLM-3B-8K在627亿个Token的SlimPajama数据集上进行训练，并使用2048和8192的上下文长度混合训练。相比于现有的30亿个参数模型，BTLM-3B-8K在下游任务中表现出2-5.5%的提升。此外，BTLM-3B-8K在长上下文任务中表现出色，比MPT-7B-8K和XGen-7B-8K更高。我们在精简和删除了SlimPajama数据集上训练这个模型，并严格地调整了μP参数和时间表。此外，我们还使用了ALiBi位嵌入和SwiGLU非线性。在Hugging Face上，最受欢迎的模型都有70亿个参数，这表明用户对70亿个参数模型的质量-大小比例感兴趣。将70亿个参数模型缩减到30亿个参数，几乎没有影响性能，是一个重要的里程碑。BTLM-3B-8K只需要3GB的内存和4位准确，在测试过程中耗用2.5倍的计算资源，帮助开辟了一个具有强大语言模型的门槛，并且可以在移动和边缘设备上运行。BTLM-3B-8K在Hugging Face上可以免费下载：https://huggingface.co/cerebras/btlm-3b-8k-base。
</details></li>
</ul>
<hr>
<h2 id="Limitations-in-odour-recognition-and-generalisation-in-a-neuromorphic-olfactory-circuit"><a href="#Limitations-in-odour-recognition-and-generalisation-in-a-neuromorphic-olfactory-circuit" class="headerlink" title="Limitations in odour recognition and generalisation in a neuromorphic olfactory circuit"></a>Limitations in odour recognition and generalisation in a neuromorphic olfactory circuit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11555">http://arxiv.org/abs/2309.11555</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nik Dennler, André van Schaik, Michael Schmuker</li>
<li>for: 这篇论文旨在研究一种基于神经omorphic computing的芳香学习算法，以及该算法在识别气体芳香的能力。</li>
<li>methods: 该算法使用了一种基于芳香细胞网络的神经omorphic架构，并使用了一些硬件加速技术来加速计算。</li>
<li>results: 研究发现，该算法在识别不同气体芳香的能力较强，但是在重复 presentaion 的情况下，模型的泛化能力有限。此外，研究还发现了一些限制，导致部分结论需要进一步验证。<details>
<summary>Abstract</summary>
Neuromorphic computing is one of the few current approaches that have the potential to significantly reduce power consumption in Machine Learning and Artificial Intelligence. Imam & Cleland presented an odour-learning algorithm that runs on a neuromorphic architecture and is inspired by circuits described in the mammalian olfactory bulb. They assess the algorithm's performance in "rapid online learning and identification" of gaseous odorants and odorless gases (short "gases") using a set of gas sensor recordings of different odour presentations and corrupting them by impulse noise. We replicated parts of the study and discovered limitations that affect some of the conclusions drawn. First, the dataset used suffers from sensor drift and a non-randomised measurement protocol, rendering it of limited use for odour identification benchmarks. Second, we found that the model is restricted in its ability to generalise over repeated presentations of the same gas. We demonstrate that the task the study refers to can be solved with a simple hash table approach, matching or exceeding the reported results in accuracy and runtime. Therefore, a validation of the model that goes beyond restoring a learned data sample remains to be shown, in particular its suitability to odour identification tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Chain-of-Verification-Reduces-Hallucination-in-Large-Language-Models"><a href="#Chain-of-Verification-Reduces-Hallucination-in-Large-Language-Models" class="headerlink" title="Chain-of-Verification Reduces Hallucination in Large Language Models"></a>Chain-of-Verification Reduces Hallucination in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11495">http://arxiv.org/abs/2309.11495</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, Jason Weston</li>
<li>for: 本研究旨在解决大语言模型中的假信息生成问题。</li>
<li>methods: 我们提出了链式验证（CoVe）方法，该方法首先（i）生成初始回答，然后（ii）规划验证问题，（iii）独立回答验证问题，并（iv）生成最终验证后的回答。</li>
<li>results: 我们在各种任务上（如Wikidata列表问题、关闭书MultiSpanQA和长文本生成）实验表明，CoVe可以减少假信息的发生。<details>
<summary>Abstract</summary>
Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (CoVe) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show CoVe decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.
</details>
<details>
<summary>摘要</summary>
大型语言模型中的幻想（hallucination）问题仍未得到解决。我们研究语言模型是否可以对其回答进行检查和更正。我们开发了链式验证（Chain-of-Verification，CoVe）方法，它包括以下四个步骤：1. 模型首先提出一个初步答案（draft）；2. 然后，模型计划一系列的验证问题，以验证其初步答案是否正确；3. 模型独立地回答这些验证问题，以避免受其他答案的影响；4. 最后，模型生成一个经验验证的答案。在实验中，我们发现CoVe可以在多种任务上减少幻想，包括基于Wikidata的列表问题、关闭书MultiSpanQA和长文本生成等。
</details></li>
</ul>
<hr>
<h2 id="Text2Reward-Automated-Dense-Reward-Function-Generation-for-Reinforcement-Learning"><a href="#Text2Reward-Automated-Dense-Reward-Function-Generation-for-Reinforcement-Learning" class="headerlink" title="Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning"></a>Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11489">http://arxiv.org/abs/2309.11489</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, Tao Yu</li>
<li>for: 本研究旨在提供一种数据自由的游戏奖励函数生成框架，帮助解决现有RL中特殊知识或域数据的需求，从而降低开发成本。</li>
<li>methods: 本研究使用大语言模型（LLM）自动生成 dense reward functions，并将奖励函数转换为可执行的程序，以满足不同任务的需求。</li>
<li>results: 在两个机器人操作benchmark（ManiSkill2、MetaWorld）和两个mujoco的 locomotive环境中，使用生成的奖励函数让策略取得了13项17个任务的成功率和速度与专家写的奖励函数相当或更高，并且在六个新的 locomotive行为中取得了94%以上的成功率。此外，我们还证明了使用我们的方法在实际世界中部署的策略。最后，我们通过人工反馈来进一步改进策略的奖励函数。视频结果可以在<a target="_blank" rel="noopener" href="https://text-to-reward.github.io查看./">https://text-to-reward.github.io查看。</a><details>
<summary>Abstract</summary>
Designing reward functions is a longstanding challenge in reinforcement learning (RL); it requires specialized knowledge or domain data, leading to high costs for development. To address this, we introduce Text2Reward, a data-free framework that automates the generation of dense reward functions based on large language models (LLMs). Given a goal described in natural language, Text2Reward generates dense reward functions as an executable program grounded in a compact representation of the environment. Unlike inverse RL and recent work that uses LLMs to write sparse reward codes, Text2Reward produces interpretable, free-form dense reward codes that cover a wide range of tasks, utilize existing packages, and allow iterative refinement with human feedback. We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2, MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17 manipulation tasks, policies trained with generated reward codes achieve similar or better task success rates and convergence speed than expert-written reward codes. For locomotion tasks, our method learns six novel locomotion behaviors with a success rate exceeding 94%. Furthermore, we show that the policies trained in the simulator with our method can be deployed in the real world. Finally, Text2Reward further improves the policies by refining their reward functions with human feedback. Video results are available at https://text-to-reward.github.io
</details>
<details>
<summary>摘要</summary>
��utes2��ward是一种抽象的游戏机制，可以自动生成填充的奖励函数，不需要特殊的知识或域数据，从而降低开发成本。为解决这个问题，我们引入Text2Reward，一种数据自由框架，可以自动生成填充的奖励函数，基于大型自然语言模型（LLM）。给出一个用自然语言描述的目标，Text2Reward可以生成填充的奖励函数，作为可执行的程序，并将其与环境的减少表示相关联。不同于反向RL和最近的工作，使用LLM写稀疏奖励代码，Text2Reward生成的奖励代码可读性好，可以覆盖各种任务，使用现有包，并允许迭代反馈。我们在ManiSkill2和MetaWorld两个机器人 manipulate 测试环境中进行了评估，以及MuJoCo两个涂抹环境。在13个机器人 manipulate 任务中，使用生成的奖励函数训练的策略的任务成功率和速度与专家写的奖励函数相当或更好。此外，我们的方法学习了6种新的行走行为，其成功率超过94%。最后，我们表明使用我们的方法在实际世界中训练的策略可以在真实世界中部署。 Text2Reward 还可以通过人类反馈来进一步改进策略的奖励函数。视频结果可以在<https://text-to-reward.github.io> 查看。
</details></li>
</ul>
<hr>
<h2 id="Fictional-Worlds-Real-Connections-Developing-Community-Storytelling-Social-Chatbots-through-LLMs"><a href="#Fictional-Worlds-Real-Connections-Developing-Community-Storytelling-Social-Chatbots-through-LLMs" class="headerlink" title="Fictional Worlds, Real Connections: Developing Community Storytelling Social Chatbots through LLMs"></a>Fictional Worlds, Real Connections: Developing Community Storytelling Social Chatbots through LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11478">http://arxiv.org/abs/2309.11478</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuqian Sun, Hanyi Wang, Pok Man Chan, Morteza Tabibi, Yan Zhang, Huan Lu, Yuheng Chen, Chang Hee Lee, Ali Asadipour</li>
<li>for: 这篇论文旨在开发社区中可以与人互动的社交虚拟助手（SC），通过故事的用途来增强社交互动的可信度和趣味性。</li>
<li>methods: 该论文使用了语言模型GPT-3驱动的故事社交虚拟助手(“David”和”Catherine”)，并在在线游戏社区”DE (Alias)”上的Discord进行了评估。</li>
<li>results: 该研究结果表明，通过故事的使用可以增强社交虚拟助手在社区 setting中的参与度和可信度。<details>
<summary>Abstract</summary>
We address the integration of storytelling and Large Language Models (LLMs) to develop engaging and believable Social Chatbots (SCs) in community settings. Motivated by the potential of fictional characters to enhance social interactions, we introduce Storytelling Social Chatbots (SSCs) and the concept of story engineering to transform fictional game characters into "live" social entities within player communities. Our story engineering process includes three steps: (1) Character and story creation, defining the SC's personality and worldview, (2) Presenting Live Stories to the Community, allowing the SC to recount challenges and seek suggestions, and (3) Communication with community members, enabling interaction between the SC and users. We employed the LLM GPT-3 to drive our SSC prototypes, "David" and "Catherine," and evaluated their performance in an online gaming community, "DE (Alias)," on Discord. Our mixed-method analysis, based on questionnaires (N=15) and interviews (N=8) with community members, reveals that storytelling significantly enhances the engagement and believability of SCs in community settings.
</details>
<details>
<summary>摘要</summary>
我们研究将故事与大型自然语言模型（LLM）结合，以开发在社区中引人入来和 credible 的社交聊天机器人（SC）。我们被启发了虚构人物可以增强社交互动的潜力，因此我们引入了 Storytelling Social Chatbots（SSCs）和故事工程技术，将虚构游戏角色转化为社区中的 "live" 社交实体。我们的故事工程过程包括三个步骤：（1）人物和故事创作，定义 SC 的个性和观点，（2）向社区成员展示Live Story，让 SC 描述挑战和寻求建议，（3）与社区成员交流，允许 SC 与用户互动。我们使用 GPT-3 LLM 驱动我们的 SSC 原型 "David" 和 "Catherine"，并在 Discord 上的在线游戏社区 "DE (Alias)" 进行了评估。我们的混合方法分析，基于问卷 (N=15) 和采访 (N=8) 的社区成员，表明故事在社区设置中可以显著提高 SC 的参与度和吸引力。
</details></li>
</ul>
<hr>
<h2 id="Multi-view-Fuzzy-Representation-Learning-with-Rules-based-Model"><a href="#Multi-view-Fuzzy-Representation-Learning-with-Rules-based-Model" class="headerlink" title="Multi-view Fuzzy Representation Learning with Rules based Model"></a>Multi-view Fuzzy Representation Learning with Rules based Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11473">http://arxiv.org/abs/2309.11473</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Zhang, Zhaohong Deng, Te Zhang, Kup-Sze Choi, Shitong Wang</li>
<li>for: 本文提出了一种新的多视图含义学习方法，用于解决多视图数据挖掘中的一些关键挑战。</li>
<li>methods: 本方法基于可解释的 Takagi-Sugeno-Kang (TSK) 杂化系统，通过两个方面实现多视图表示学习。首先，将多视图数据转换为高维杂化特征空间，同时同时挖掘共同视图信息和每个视图特有信息。其次，提出了基于 L_(2,1) 评估方法的新规范方法，以挖掘视图之间的一致信息，并保持数据的几何结构。</li>
<li>results: 对多个标准多视图数据集进行了广泛的实验 validate the superiority of the proposed method。<details>
<summary>Abstract</summary>
Unsupervised multi-view representation learning has been extensively studied for mining multi-view data. However, some critical challenges remain. On the one hand, the existing methods cannot explore multi-view data comprehensively since they usually learn a common representation between views, given that multi-view data contains both the common information between views and the specific information within each view. On the other hand, to mine the nonlinear relationship between data, kernel or neural network methods are commonly used for multi-view representation learning. However, these methods are lacking in interpretability. To this end, this paper proposes a new multi-view fuzzy representation learning method based on the interpretable Takagi-Sugeno-Kang (TSK) fuzzy system (MVRL_FS). The method realizes multi-view representation learning from two aspects. First, multi-view data are transformed into a high-dimensional fuzzy feature space, while the common information between views and specific information of each view are explored simultaneously. Second, a new regularization method based on L_(2,1)-norm regression is proposed to mine the consistency information between views, while the geometric structure of the data is preserved through the Laplacian graph. Finally, extensive experiments on many benchmark multi-view datasets are conducted to validate the superiority of the proposed method.
</details>
<details>
<summary>摘要</summary>
多视角表示学习已经广泛研究了多视角数据的挖掘。然而，有些关键挑战仍然存在。一方面，现有的方法不能全面探索多视角数据，因为它们通常学习多视角数据中的共同信息，而不是每个视角中的特定信息。另一方面，用于挖掘非线性关系的内核或神经网络方法通常缺乏可解释性。为此，本文提出了一种新的多视角杂化表示学习方法，基于可解释的 Takagi-Sugeno-Kang（TSK）杂化系统（MVRL_FS）。该方法在两个方面实现多视角表示学习。首先，多视角数据被转换成一个高维杂化特征空间，同时探索多视角数据中的共同信息和每个视角中的特定信息。其次，基于L_(2,1)-norm回归的新规则方法被提出，以挖掘视角之间的一致信息，保留数据的几何结构通过拉普拉斯图。最后，对许多标准多视角数据集进行了广泛的实验，以验证提议方法的超越性。
</details></li>
</ul>
<hr>
<h2 id="Multi-Label-Takagi-Sugeno-Kang-Fuzzy-System"><a href="#Multi-Label-Takagi-Sugeno-Kang-Fuzzy-System" class="headerlink" title="Multi-Label Takagi-Sugeno-Kang Fuzzy System"></a>Multi-Label Takagi-Sugeno-Kang Fuzzy System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11469">http://arxiv.org/abs/2309.11469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiongdan Lou, Zhaohong Deng, Zhiyong Xiao, Kup-Sze Choi, Shitong Wang</li>
<li>for: 提高多标签分类性能</li>
<li>methods: 基于多标签相关学习和多标签回归损失的多标签杜氏辛诺干式系统（ML-TSK FS）</li>
<li>results: 对12个多标签数据集进行实验，结果表明ML-TSK FS与现有方法相比，在各种评价指标中表现竞争力强，表明它可以有效地通过辛诺干式规则模型特性和特征标签关系，提高分类性能。<details>
<summary>Abstract</summary>
Multi-label classification can effectively identify the relevant labels of an instance from a given set of labels. However,the modeling of the relationship between the features and the labels is critical to the classification performance. To this end, we propose a new multi-label classification method, called Multi-Label Takagi-Sugeno-Kang Fuzzy System (ML-TSK FS), to improve the classification performance. The structure of ML-TSK FS is designed using fuzzy rules to model the relationship between features and labels. The fuzzy system is trained by integrating fuzzy inference based multi-label correlation learning with multi-label regression loss. The proposed ML-TSK FS is evaluated experimentally on 12 benchmark multi-label datasets. 1 The results show that the performance of ML-TSK FS is competitive with existing methods in terms of various evaluation metrics, indicating that it is able to model the feature-label relationship effectively using fuzzy inference rules and enhances the classification performance.
</details>
<details>
<summary>摘要</summary>
多标签分类可以有效地从给定的标签集中确定实例的相关标签。然而，模型特性和标签之间的关系是多标签分类性能的关键因素。为此，我们提出了一种新的多标签分类方法，即多标签多SK满足系统（ML-TSK FS），以提高分类性能。ML-TSK FS的结构采用规则来模型特性和标签之间的关系。这个规则是通过多态推理和多标签相互关系学习来训练的。我们对12个多标签数据集进行实验评估了ML-TSK FS的性能。结果表明，ML-TSK FS与现有方法相比，在不同的评价指标上具有竞争力，这表明它可以通过多态推理规则来有效地模型特性和标签之间的关系，提高分类性能。
</details></li>
</ul>
<hr>
<h2 id="AudioFool-Fast-Universal-and-synchronization-free-Cross-Domain-Attack-on-Speech-Recognition"><a href="#AudioFool-Fast-Universal-and-synchronization-free-Cross-Domain-Attack-on-Speech-Recognition" class="headerlink" title="AudioFool: Fast, Universal and synchronization-free Cross-Domain Attack on Speech Recognition"></a>AudioFool: Fast, Universal and synchronization-free Cross-Domain Attack on Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11462">http://arxiv.org/abs/2309.11462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamad Fakih, Rouwaida Kanj, Fadi Kurdahi, Mohammed E. Fouda</li>
<li>for: 防止自动话语识别系统受到敌对攻击，导致系统崩溃或损坏。</li>
<li>methods: 使用对数频域修改攻击，以确保攻击具有不同特性，例如不受同步调制影响和范围滤波影响。</li>
<li>results: 透过实验和分析，发现 modified frequency domain 攻击能够实现这些特性，并且在线上 keyword classification 任务中提供了高效的攻击方法。<details>
<summary>Abstract</summary>
Automatic Speech Recognition systems have been shown to be vulnerable to adversarial attacks that manipulate the command executed on the device. Recent research has focused on exploring methods to create such attacks, however, some issues relating to Over-The-Air (OTA) attacks have not been properly addressed. In our work, we examine the needed properties of robust attacks compatible with the OTA model, and we design a method of generating attacks with arbitrary such desired properties, namely the invariance to synchronization, and the robustness to filtering: this allows a Denial-of-Service (DoS) attack against ASR systems. We achieve these characteristics by constructing attacks in a modified frequency domain through an inverse Fourier transform. We evaluate our method on standard keyword classification tasks and analyze it in OTA, and we analyze the properties of the cross-domain attacks to explain the efficiency of the approach.
</details>
<details>
<summary>摘要</summary>
自动话语识别系统已经被证明容易受到敌意攻击，这些攻击可以控制设备上执行的命令。最近的研究主要关注于探索如何创建这些攻击，但是一些过空中攻击（OTA）问题尚未得到充分解决。在我们的工作中，我们分析了需要的抗性攻击的属性，并设计了生成攻击具有任意想要的属性的方法，包括不变性和过滤器的Robustness。我们通过对射Transformer来实现这些特性，并在标准关键词分类任务上评估了我们的方法。我们还分析了跨频域攻击的性质，以解释我们的方法的高效性。
</details></li>
</ul>
<hr>
<h2 id="Generative-Agent-Based-Modeling-Unveiling-Social-System-Dynamics-through-Coupling-Mechanistic-Models-with-Generative-Artificial-Intelligence"><a href="#Generative-Agent-Based-Modeling-Unveiling-Social-System-Dynamics-through-Coupling-Mechanistic-Models-with-Generative-Artificial-Intelligence" class="headerlink" title="Generative Agent-Based Modeling: Unveiling Social System Dynamics through Coupling Mechanistic Models with Generative Artificial Intelligence"></a>Generative Agent-Based Modeling: Unveiling Social System Dynamics through Coupling Mechanistic Models with Generative Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11456">http://arxiv.org/abs/2309.11456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navid Ghaffarzadegan, Aritra Majumdar, Ross Williams, Niyousha Hosseinichimeh</li>
<li>for: 这篇论文探讨了使用生成人工智能建模社会系统的新机遇。</li>
<li>methods: 这些模型使用大语言模型如ChatGPT来表示人类决策行为在社会设置下。</li>
<li>results: 这篇论文提供了一个简单的社会规范传播模型，并对其 Results 进行了广泛的调查和敏感性分析。<details>
<summary>Abstract</summary>
We discuss the emerging new opportunity for building feedback-rich computational models of social systems using generative artificial intelligence. Referred to as Generative Agent-Based Models (GABMs), such individual-level models utilize large language models such as ChatGPT to represent human decision-making in social settings. We provide a GABM case in which human behavior can be incorporated in simulation models by coupling a mechanistic model of human interactions with a pre-trained large language model. This is achieved by introducing a simple GABM of social norm diffusion in an organization. For educational purposes, the model is intentionally kept simple. We examine a wide range of scenarios and the sensitivity of the results to several changes in the prompt. We hope the article and the model serve as a guide for building useful diffusion models that include realistic human reasoning and decision-making.
</details>
<details>
<summary>摘要</summary>
我们讨论新兴的机会：使用生成人工智能建构具有反馈丰富的社交系统模型。称为生成代理模型（GABM），这些个体级模型利用大量语言模型如ChatGPT来表示人类决策在社交设置中。我们提供一个GABM例子，将人类行为integrated到模拟模型中，通过与预训大量语言模型 Coupling 的方式。这是通过将社交norm传播模型简化为 Educational  purposes 的方式。我们评估了广泛的情况，并评估了变量的敏感度。我们希望这篇文章和模型可以serve as a guide  для建立包含现实人类思维和决策的传播模型。
</details></li>
</ul>
<hr>
<h2 id="Using-deep-learning-to-construct-stochastic-local-search-SAT-solvers-with-performance-bounds"><a href="#Using-deep-learning-to-construct-stochastic-local-search-SAT-solvers-with-performance-bounds" class="headerlink" title="Using deep learning to construct stochastic local search SAT solvers with performance bounds"></a>Using deep learning to construct stochastic local search SAT solvers with performance bounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11452">http://arxiv.org/abs/2309.11452</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/porscheofficial/sls_sat_solving_with_deep_learning">https://github.com/porscheofficial/sls_sat_solving_with_deep_learning</a></li>
<li>paper_authors: Maximilian Kramer, Paul Boes</li>
<li>for: 这 paper 是关于 Boolean Satisfiability problem (SAT) 的研究，具体来说是使用 Graph Neural Networks (GNN) 训练 oracle，以提高 Stochastic Local Search (SLS) 算法的性能。</li>
<li>methods: 这 paper 使用了 GNN 训练 oracle，并将其应用于两种 SLS 算法上，以解决随机 SAT 实例。</li>
<li>results: 研究发现，通过使用 GNN 训练 oracle，SLS 算法的性能得到了明显提高，可以解决更难的 SAT 实例，并且可以在更少的步骤数下解决。<details>
<summary>Abstract</summary>
The Boolean Satisfiability problem (SAT) is the most prototypical NP-complete problem and of great practical relevance. One important class of solvers for this problem are stochastic local search (SLS) algorithms that iteratively and randomly update a candidate assignment. Recent breakthrough results in theoretical computer science have established sufficient conditions under which SLS solvers are guaranteed to efficiently solve a SAT instance, provided they have access to suitable "oracles" that provide samples from an instance-specific distribution, exploiting an instance's local structure. Motivated by these results and the well established ability of neural networks to learn common structure in large datasets, in this work, we train oracles using Graph Neural Networks and evaluate them on two SLS solvers on random SAT instances of varying difficulty. We find that access to GNN-based oracles significantly boosts the performance of both solvers, allowing them, on average, to solve 17% more difficult instances (as measured by the ratio between clauses and variables), and to do so in 35% fewer steps, with improvements in the median number of steps of up to a factor of 8. As such, this work bridges formal results from theoretical computer science and practically motivated research on deep learning for constraint satisfaction problems and establishes the promise of purpose-trained SAT solvers with performance guarantees.
</details>
<details>
<summary>摘要</summary>
布尔满意性问题（SAT）是NP完备问题的最典型例子，具有实际重要性。一种重要的SAT解决方法是随机地更新候选分配的杂化搜索算法（SLS）。最近的理论计算机科学成果表明，如果SLS算法有访问适合的"oracle"，那么它们可以有效地解决SAT实例， provided they have access to suitable "oracles" that provide samples from an instance-specific distribution, exploiting an instance's local structure. 在这种情况下，我们使用图神经网络训练 oracle，并对两种SLS解决方法进行评估，在随机SAT实例上进行测试。我们发现，通过访问GNN基于 oracle，可以大幅提高SLS解决方法的性能，使其能够解决更难的实例（按照条件数和变量的比率来度量），并且在更少的步骤内完成（比如，在35% fewer steps中完成）。此外，我们发现，在 median number of steps 中，GNN基于 oracle 可以提高 SLS 解决方法的性能，最高可以提高8倍。因此，这项研究将理论计算机科学的成果与深度学习的实践研究相结合，并证明了专门为SAT问题训练的深度学习算法可以提供性能保证。
</details></li>
</ul>
<hr>
<h2 id="You-Only-Look-at-Screens-Multimodal-Chain-of-Action-Agents"><a href="#You-Only-Look-at-Screens-Multimodal-Chain-of-Action-Agents" class="headerlink" title="You Only Look at Screens: Multimodal Chain-of-Action Agents"></a>You Only Look at Screens: Multimodal Chain-of-Action Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11436">http://arxiv.org/abs/2309.11436</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cooelf/Auto-UI">https://github.com/cooelf/Auto-UI</a></li>
<li>paper_authors: Zhuosheng Zhang, Aston Zhang</li>
<li>for: 这篇论文旨在提高自动化用户界面（UI）代理的效率，使其可以在不需要人工干预的情况下自动完成任务。</li>
<li>methods: 该论文提出了一种多模态解决方案，即直接与界面交互，不需要环境解析或应用程序特定的API。此外，提出了一种链式动作技术，通过考虑先前和后续动作历史，帮助代理决定哪个动作执行。</li>
<li>results: 实验结果显示，Auto-UI在新的设备控制benchmark AITW上达到了状态码的性能，具有动作类型预测精度90%和总成功率74%。代码公开可用于<a target="_blank" rel="noopener" href="https://github.com/cooelf/Auto-UI%E3%80%82">https://github.com/cooelf/Auto-UI。</a><details>
<summary>Abstract</summary>
Autonomous user interface (UI) agents aim to facilitate task automation by interacting with the user interface without manual intervention. Recent studies have investigated eliciting the capabilities of large language models (LLMs) for effective engagement in diverse environments. To align with the input-output requirement of LLMs, existing approaches are developed under a sandbox setting where they rely on external tools and application-specific APIs to parse the environment into textual elements and interpret the predicted actions. Consequently, those approaches often grapple with inference inefficiency and error propagation risks. To mitigate the challenges, we introduce Auto-UI, a multimodal solution that directly interacts with the interface, bypassing the need for environment parsing or reliance on application-dependent APIs. Moreover, we propose a chain-of-action technique -- leveraging a series of intermediate previous action histories and future action plans -- to help the agent decide what action to execute. We evaluate our approach on a new device-control benchmark AITW with 30K unique instructions, spanning multi-step tasks such as application operation, web searching, and web shopping. Experimental results show that Auto-UI achieves state-of-the-art performance with an action type prediction accuracy of 90% and an overall action success rate of 74%. Code is publicly available at https://github.com/cooelf/Auto-UI.
</details>
<details>
<summary>摘要</summary>
自动化用户界面（UI）代理，目的是自动化任务，不需要人工干预。最近的研究已经利用大型自然语言模型（LLM）来实现多种环境中的有效交互。为了与输入和输出对应的LLM的需求，现有的方法采用沙盒环境，通过外部工具和应用程序特定的API来解析环境并解释预测的动作。然而，这些方法经常会遇到推理不准确和错误传递风险。为了解决这些挑战，我们提出了Auto-UI，一种多模式解决方案，可以直接与界面交互，无需解析环境或依赖于应用程序特定的API。此外，我们还提出了链条动作技术，利用前一系列的历史动作和未来动作计划，帮助代理决定执行哪一个动作。我们在新的设备控制标准AITW上进行了实验，并取得了state-of-the-art表现，具体如下：* 动作类型预测精度达90%* 总体动作成功率达74%代码可以在https://github.com/cooelf/Auto-UI上获取。
</details></li>
</ul>
<hr>
<h2 id="A-Systematic-Review-of-Few-Shot-Learning-in-Medical-Imaging"><a href="#A-Systematic-Review-of-Few-Shot-Learning-in-Medical-Imaging" class="headerlink" title="A Systematic Review of Few-Shot Learning in Medical Imaging"></a>A Systematic Review of Few-Shot Learning in Medical Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11433">http://arxiv.org/abs/2309.11433</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eva Pachetti, Sara Colantonio</li>
<li>for: 这篇文章旨在给出医学影像分析领域中几何学学习的系统评论，尤其是在几何学学习方法中实现少数扩展学习。</li>
<li>methods: 这篇文章使用了系统性的文献搜寻方法，从2018年到2023年发表的80篇相关文章中选择了相关的文献。文章将这些文献分为不同的医疗结果（如肿瘤分类、疾病分类、影像调整等）、 investigate的 анатомі学结构（如心脏、肺等）以及使用的几何学学习方法。</li>
<li>results: 文章显示了几何学学习可以在大多数的结果中超过数据不足的问题，并且meta-learning是几何学学习中最受欢迎的方法，可以适应新任务的几何学学习。此外，文章还发现了在医学影像分析中几何学学习中使用的主要技术是supervised learning和semi-supervised learning，并且这些技术在医疗影像分析中表现最佳。最后，文章发现了主要应用领域主要是心脏、肺和腹部领域。<details>
<summary>Abstract</summary>
The lack of annotated medical images limits the performance of deep learning models, which usually need large-scale labelled datasets. Few-shot learning techniques can reduce data scarcity issues and enhance medical image analysis, especially with meta-learning. This systematic review gives a comprehensive overview of few-shot learning in medical imaging. We searched the literature systematically and selected 80 relevant articles published from 2018 to 2023. We clustered the articles based on medical outcomes, such as tumour segmentation, disease classification, and image registration; anatomical structure investigated (i.e. heart, lung, etc.); and the meta-learning method used. For each cluster, we examined the papers' distributions and the results provided by the state-of-the-art. In addition, we identified a generic pipeline shared among all the studies. The review shows that few-shot learning can overcome data scarcity in most outcomes and that meta-learning is a popular choice to perform few-shot learning because it can adapt to new tasks with few labelled samples. In addition, following meta-learning, supervised learning and semi-supervised learning stand out as the predominant techniques employed to tackle few-shot learning challenges in medical imaging and also best performing. Lastly, we observed that the primary application areas predominantly encompass cardiac, pulmonary, and abdominal domains. This systematic review aims to inspire further research to improve medical image analysis and patient care.
</details>
<details>
<summary>摘要</summary>
因为医疗影像标签的缺乏，深度学习模型的性能受到限制。不过，几个shot学习技术可以解决数据缺乏问题，提高医疗影像分析，特别是在meta-learning中。这个系统性审查给出了医疗影像中几个shot学习的全面回顾。我们在2018年至2023年发布的80篇相关文献中进行了系统性搜寻，并根据医疗结果（例如肿瘤分类、病理分类、影像调整）、 investigate体部（例如心脏、肺部等）和使用的meta-learning方法进行分组。对每个分组，我们评估了文献的分布和顶尖的结果。此外，我们发现了所有研究中的通用架构。审查结果表明，几个shot学习可以在大多数结果中突破数据缺乏问题，meta-learning是最受欢迎的选择，因为它可以适应新任务 WITH FEW labelled samples。此外，在医疗影像中，以supervised learning和semi-supervised learning为主的技术被大量运用，并且表现最佳。最后，我们发现主要应用领域主要是心脏、肺部和腹部领域。这个系统性审查的目的是鼓励进一步的研究，以提高医疗影像分析和patient care。
</details></li>
</ul>
<hr>
<h2 id="Generative-Pre-Training-of-Time-Series-Data-for-Unsupervised-Fault-Detection-in-Semiconductor-Manufacturing"><a href="#Generative-Pre-Training-of-Time-Series-Data-for-Unsupervised-Fault-Detection-in-Semiconductor-Manufacturing" class="headerlink" title="Generative Pre-Training of Time-Series Data for Unsupervised Fault Detection in Semiconductor Manufacturing"></a>Generative Pre-Training of Time-Series Data for Unsupervised Fault Detection in Semiconductor Manufacturing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11427">http://arxiv.org/abs/2309.11427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sewoong Lee, JinKyou Choi, Min Su Kim</li>
<li>For: 这个研究旨在运用时间序列数据的特征来探测半导体制造中的异常现象。* Methods: 研究使用时间序列嵌入和生成预训Transformers来预训时间序列数据，并使用标 entropy损失函数来分类异常时间序列和正常时间序列。* Results: 研究表明，我们的模型在UCSD时间序列分类数据集和化学蒸发成长（CVD）设备的处理记录上都显示出更好的表现，与过去的无supervision模型相比。我们的模型在EER上的F1分数最高，并且仅仅0.026下于无supervision基准。<details>
<summary>Abstract</summary>
This paper introduces TRACE-GPT, which stands for Time-seRies Anomaly-detection with Convolutional Embedding and Generative Pre-trained Transformers. TRACE-GPT is designed to pre-train univariate time-series sensor data and detect faults on unlabeled datasets in semiconductor manufacturing. In semiconductor industry, classifying abnormal time-series sensor data from normal data is important because it is directly related to wafer defect. However, small, unlabeled, and even mixed training data without enough anomalies make classification tasks difficult. In this research, we capture features of time-series data with temporal convolutional embedding and Generative Pre-trained Transformer (GPT) to classify abnormal sequences from normal sequences using cross entropy loss. We prove that our model shows better performance than previous unsupervised models with both an open dataset, the University of California Riverside (UCR) time-series classification archive, and the process log of our Chemical Vapor Deposition (CVD) equipment. Our model has the highest F1 score at Equal Error Rate (EER) across all datasets and is only 0.026 below the supervised state-of-the-art baseline on the open dataset.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="EDMP-Ensemble-of-costs-guided-Diffusion-for-Motion-Planning"><a href="#EDMP-Ensemble-of-costs-guided-Diffusion-for-Motion-Planning" class="headerlink" title="EDMP: Ensemble-of-costs-guided Diffusion for Motion Planning"></a>EDMP: Ensemble-of-costs-guided Diffusion for Motion Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11414">http://arxiv.org/abs/2309.11414</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kallol Saha, Vishal Mandadi, Jayaram Reddy, Ajit Srikanth, Aditya Agarwal, Bipasha Sen, Arun Singh, Madhava Krishna</li>
<li>for: 这篇论文是为了提出一种 combining classical 和 deep learning 的动作规划方法，以提高动作规划的成功率和普遍性。</li>
<li>methods: 本文使用了一种叫做 Ensemble-of-costs-guided Diffusion for Motion Planning（EDMP）的方法，它 combinines 经典的动作规划算法和深度学习算法，以提高动作规划的成功率和普遍性。EDMP 使用了一个 diffusion-based network，训练在一组多元可行的动作轨迹上。在执行过程中，我们Compute scene-specific costs，如 “碰撞成本”，以导引 diffusion 生成符合场景内的碰撞条件的有效轨迹。</li>
<li>results: 本文的结果显示，EDMP 能够与 State-of-the-Art 的深度学习基于方法相比，成功率有所提高，并且保留了经典步骤的普遍性。<details>
<summary>Abstract</summary>
Classical motion planning for robotic manipulation includes a set of general algorithms that aim to minimize a scene-specific cost of executing a given plan. This approach offers remarkable adaptability, as they can be directly used off-the-shelf for any new scene without needing specific training datasets. However, without a prior understanding of what diverse valid trajectories are and without specially designed cost functions for a given scene, the overall solutions tend to have low success rates. While deep-learning-based algorithms tremendously improve success rates, they are much harder to adopt without specialized training datasets. We propose EDMP, an Ensemble-of-costs-guided Diffusion for Motion Planning that aims to combine the strengths of classical and deep-learning-based motion planning. Our diffusion-based network is trained on a set of diverse kinematically valid trajectories. Like classical planning, for any new scene at the time of inference, we compute scene-specific costs such as "collision cost" and guide the diffusion to generate valid trajectories that satisfy the scene-specific constraints. Further, instead of a single cost function that may be insufficient in capturing diversity across scenes, we use an ensemble of costs to guide the diffusion process, significantly improving the success rate compared to classical planners. EDMP performs comparably with SOTA deep-learning-based methods while retaining the generalization capabilities primarily associated with classical planners.
</details>
<details>
<summary>摘要</summary>
经典运动规划 для机器人操作包括一组通用算法，旨在最小化Scene特定的执行计划的成本。这种方法具有很好的适应性，可以直接在新场景上使用，不需要特定的训练数据。然而，不知道多元有效轨迹的特点和场景特定的成本函数，全局的解决方案通常具有低成功率。深度学习基于算法在成功率上提供了很大的改善，但是它们更难于采用，需要特定的训练数据。我们提出了EDMP，一种ensemble-of-costs-guided Diffusion for Motion Planning，旨在结合经典和深度学习基于的运动规划。我们的扩散网络被训练在一组多元可行的轨迹上。在任何新场景的推理时，我们计算场景特定的碰撞成本和导引扩散来生成符合场景特定的约束的有效轨迹。此外，而不是单一的成本函数，我们使用一个ensemble of costs来引导扩散过程，明显提高成功率相比经典规划器。EDMP和SOTA深度学习基于方法相比，保留了经典规划器的总体化能力。
</details></li>
</ul>
<hr>
<h2 id="Long-Form-End-to-End-Speech-Translation-via-Latent-Alignment-Segmentation"><a href="#Long-Form-End-to-End-Speech-Translation-via-Latent-Alignment-Segmentation" class="headerlink" title="Long-Form End-to-End Speech Translation via Latent Alignment Segmentation"></a>Long-Form End-to-End Speech Translation via Latent Alignment Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11384">http://arxiv.org/abs/2309.11384</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Polák, Ondřej Bojar</li>
<li>for: 这个论文的目的是提出一种实时同声翻译方法，可以处理长于几秒钟的语音数据。</li>
<li>methods: 这种方法使用现有的同声翻译encoder-decoder架构，并使用ST CTC进行分 segmentation。这个方法不需要额外的监督或参数，可以在实时中进行同声翻译和分 segmentation。</li>
<li>results: 在多种语言对和内外领域数据上，我们的方法可以 дости得状态的同声翻译质量，而且不需要额外的计算成本。<details>
<summary>Abstract</summary>
Current simultaneous speech translation models can process audio only up to a few seconds long. Contemporary datasets provide an oracle segmentation into sentences based on human-annotated transcripts and translations. However, the segmentation into sentences is not available in the real world. Current speech segmentation approaches either offer poor segmentation quality or have to trade latency for quality. In this paper, we propose a novel segmentation approach for a low-latency end-to-end speech translation. We leverage the existing speech translation encoder-decoder architecture with ST CTC and show that it can perform the segmentation task without supervision or additional parameters. To the best of our knowledge, our method is the first that allows an actual end-to-end simultaneous speech translation, as the same model is used for translation and segmentation at the same time. On a diverse set of language pairs and in- and out-of-domain data, we show that the proposed approach achieves state-of-the-art quality at no additional computational cost.
</details>
<details>
<summary>摘要</summary>
当前同时传输模型可以处理音频只有几秒长。当前数据提供了人注释的讲解和翻译，但实际世界中没有这样的分 segmentation。当前的Speech segmentation方法或者提供低质量的分 segmentation或者要求交换延迟和质量。在这篇论文中，我们提出了一种新的分 segmentation方法，用于低延迟的端到端 Speech translation。我们利用现有的Speech translation encoder-decoder架构和 ST CTC，并证明它可以完成分 segmentation任务无需监督或额外参数。根据我们所知，我们的方法是首次实现了实际的同时 Speech translation，因为同时使用了翻译和分 segmentation的同一模型。在多种语言对和内外领域数据上，我们示出了状态机器的质量，没有额外计算成本。
</details></li>
</ul>
<hr>
<h2 id="Discuss-Before-Moving-Visual-Language-Navigation-via-Multi-expert-Discussions"><a href="#Discuss-Before-Moving-Visual-Language-Navigation-via-Multi-expert-Discussions" class="headerlink" title="Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions"></a>Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11382">http://arxiv.org/abs/2309.11382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxing Long, Xiaoqi Li, Wenzhe Cai, Hao Dong</li>
<li>for: 这篇论文旨在提出一种新的零基础Visual Language Navigation（VLN）框架，以解决现有VLN方法单一自动思考的局限性。</li>
<li>methods: 该框架采用域专家的协助，通过讨论收集关键导航任务的信息，包括指令理解、环境识别和完成估计。</li>
<li>results: 经过广泛的实验表明，与域专家进行讨论可以有效地促进导航，提高指令相关信息的理解、更正偶极错误和筛选不一致的运动决策。相比单一自动思考，该方法在所有指标上表现出优异。<details>
<summary>Abstract</summary>
Visual language navigation (VLN) is an embodied task demanding a wide range of skills encompassing understanding, perception, and planning. For such a multifaceted challenge, previous VLN methods totally rely on one model's own thinking to make predictions within one round. However, existing models, even the most advanced large language model GPT4, still struggle with dealing with multiple tasks by single-round self-thinking. In this work, drawing inspiration from the expert consultation meeting, we introduce a novel zero-shot VLN framework. Within this framework, large models possessing distinct abilities are served as domain experts. Our proposed navigation agent, namely DiscussNav, can actively discuss with these experts to collect essential information before moving at every step. These discussions cover critical navigation subtasks like instruction understanding, environment perception, and completion estimation. Through comprehensive experiments, we demonstrate that discussions with domain experts can effectively facilitate navigation by perceiving instruction-relevant information, correcting inadvertent errors, and sifting through in-consistent movement decisions. The performances on the representative VLN task R2R show that our method surpasses the leading zero-shot VLN model by a large margin on all metrics. Additionally, real-robot experiments display the obvious advantages of our method over single-round self-thinking.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified ChineseVisual language navigation (VLN) is an embodied task that requires a wide range of skills, including understanding, perception, and planning. Previous VLN methods have relied solely on one model's own thinking to make predictions within one round. However, even the most advanced large language model GPT4 struggles with handling multiple tasks through single-round self-thinking. In this work, inspired by expert consultation meetings, we introduce a novel zero-shot VLN framework. In this framework, large models with distinct abilities serve as domain experts. Our proposed navigation agent, called DiscussNav, can actively discuss with these experts to collect essential information before moving at every step. These discussions cover critical navigation subtasks such as understanding instructions, perceiving the environment, and estimating completion. Through comprehensive experiments, we demonstrate that discussions with domain experts can effectively facilitate navigation by perceiving instruction-relevant information, correcting inadvertent errors, and filtering out inconsistent movement decisions. The performances on the representative VLN task R2R show that our method surpasses the leading zero-shot VLN model by a large margin on all metrics. Additionally, real-robot experiments display the obvious advantages of our method over single-round self-thinking.中文简体版：视觉语言导航（VLN）是一个需要各种技能的体验任务，包括理解、感知和规划。先前的VLN方法都是单一模型自己思考，但是即使最先进的大语言模型GPT4也在处理多任务时仍然陷入困难。在这个工作中，启发于专家咨询会议，我们引入了一种新的零扩展VLN框架。在这个框架中，具有不同能力的大模型服为域专家。我们提出的导航代理人称为DiscussNav，可以在每步移动之前与这些专家进行活动的讨论，收集关键导航子任务的信息。这些讨论包括理解指令、识别环境和估计完成度。通过广泛的实验，我们证明了与域专家进行讨论可以有效地促进导航，捕捉指令相关信息， исправ错误和筛选出不一致的移动决策。R2R任务表明，我们的方法在所有指标上胜过领先的零扩展VLN模型。此外，实际Robot实验也显示了我们方法在单一自我思考方面的明显优势。
</details></li>
</ul>
<hr>
<h2 id="Incremental-Blockwise-Beam-Search-for-Simultaneous-Speech-Translation-with-Controllable-Quality-Latency-Tradeoff"><a href="#Incremental-Blockwise-Beam-Search-for-Simultaneous-Speech-Translation-with-Controllable-Quality-Latency-Tradeoff" class="headerlink" title="Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff"></a>Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11379">http://arxiv.org/abs/2309.11379</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Polák, Brian Yan, Shinji Watanabe, Alex Waibel, Ondřej Bojar</li>
<li>for:  simultanous speech translation</li>
<li>methods:  blockwise self-attentional encoder models, incremental blockwise beam search, local agreement or hold-$n$ policies</li>
<li>results: 0.6-3.6 BLEU improvement without changing latency or 0.8-1.4 s latency improvement without changing quality.Here’s the full translation in Simplified Chinese:</li>
<li>for: 这篇论文主要针对同时语言翻译。</li>
<li>methods: 这篇论文使用了块状自注意力编码器模型，并使用了增量块wise beam search和本地一致或保持-$n$ 策略来控制质量和延迟的质量。</li>
<li>results: 在 MuST-C 上实验结果显示，无需改变延迟或质量，可以获得0.6-3.6 BLEU 提升，或者可以降低0.8-1.4 s 的延迟。<details>
<summary>Abstract</summary>
Blockwise self-attentional encoder models have recently emerged as one promising end-to-end approach to simultaneous speech translation. These models employ a blockwise beam search with hypothesis reliability scoring to determine when to wait for more input speech before translating further. However, this method maintains multiple hypotheses until the entire speech input is consumed -- this scheme cannot directly show a single \textit{incremental} translation to users. Further, this method lacks mechanisms for \textit{controlling} the quality vs. latency tradeoff. We propose a modified incremental blockwise beam search incorporating local agreement or hold-$n$ policies for quality-latency control. We apply our framework to models trained for online or offline translation and demonstrate that both types can be effectively used in online mode.   Experimental results on MuST-C show 0.6-3.6 BLEU improvement without changing latency or 0.8-1.4 s latency improvement without changing quality.
</details>
<details>
<summary>摘要</summary>
“块级自注意编码器模型在同时语音翻译方面最近几年来得到了一些承诺。这些模型使用块级搜索和假设可靠性分数来决定等待更多的输入语音之前继续翻译。然而，这种方法会维护多个假设，直到整个语音输入被消耗——这种方案无法直接显示单个增量翻译给用户。此外，这种方法缺乏控制质量vs延迟贸易的机制。我们提议修改增量块级搜索，并添加地方一致或保持-$n$ 策略来控制质量vs延迟的贸易。我们将我们的框架应用于在线或离线训练的模型，并证明两种类型都可以在线模式下使用。实验结果表明，在 Must-C 上得到了0.6-3.6 BLEU 提升，或0.8-1.4 s 延迟提升，无需改变质量或延迟。”
</details></li>
</ul>
<hr>
<h2 id="Preconditioned-Federated-Learning"><a href="#Preconditioned-Federated-Learning" class="headerlink" title="Preconditioned Federated Learning"></a>Preconditioned Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11378">http://arxiv.org/abs/2309.11378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyi Tao, Jindi Wu, Qun Li</li>
<li>for: 训练分布式机器学习模型，保持通信效率和隐私性。</li>
<li>methods: 基于本地适应和服务器端适应两个框架，采用新的协VAR matrix预conditioner，实现了更高的通信效率和更好的适应性。</li>
<li>results: 在 i.i.d. 和非 i.i.d. 情况下，实验结果表明我们的方法可以达到领先的性能水平。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a distributed machine learning approach that enables model training in communication efficient and privacy-preserving manner. The standard optimization method in FL is Federated Averaging (FedAvg), which performs multiple local SGD steps between communication rounds. FedAvg has been considered to lack algorithm adaptivity compared to modern first-order adaptive optimizations. In this paper, we propose new communication-efficient FL algortithms based on two adaptive frameworks: local adaptivity (PreFed) and server-side adaptivity (PreFedOp). Proposed methods adopt adaptivity by using a novel covariance matrix preconditioner. Theoretically, we provide convergence guarantees for our algorithms. The empirical experiments show our methods achieve state-of-the-art performances on both i.i.d. and non-i.i.d. settings.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种分布式机器学习方法，可以在通信效率和隐私保护的情况下进行模型训练。标准优化方法在 FL 中是联邦平均（FedAvg），它在通信轮次之间执行多个本地 SGD 步骤。FedAvg 已被认为在与现代首个适应优化相比lack algorithm adaptivity。在这篇论文中，我们提出了新的通信效率FL算法，基于两种适应框架：本地适应（PreFed）和服务器端适应（PreFedOp）。我们的方法采用适应性的novel协方差矩阵预conditioner。我们从理论上提供了收敛保证。实验表明，我们的方法在 i.i.d. 和非 i.i.d. 设置下达到了当前最佳性能。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Hand-Gesture-Featured-Human-Motor-Adaptation-in-Tool-Delivery-using-Voice-Recognition"><a href="#Dynamic-Hand-Gesture-Featured-Human-Motor-Adaptation-in-Tool-Delivery-using-Voice-Recognition" class="headerlink" title="Dynamic Hand Gesture-Featured Human Motor Adaptation in Tool Delivery using Voice Recognition"></a>Dynamic Hand Gesture-Featured Human Motor Adaptation in Tool Delivery using Voice Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11368">http://arxiv.org/abs/2309.11368</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haolin Fei, Stefano Tedeschi, Yanpei Huang, Andrew Kennedy, Ziwei Wang</li>
<li>for: 这个论文目的是提高人机合作的效率，使用多种modal interaction方式，以便用户可以专注于任务执行，而不需要额外培训用户机器人界面。</li>
<li>methods: 这个论文使用了手势认识、语音识别和可 switchable控制适应策略，以提供一个用户友好的人机合作框架。</li>
<li>results: 实验结果表明， static手势认识模块的准确率为94.3%，动态运动认识模块的准确率为97.6%。相比之下，人 Solo执行任务时，提出的方法可以提高工具交elivery的效率，而不会干扰人类意图。<details>
<summary>Abstract</summary>
Human-robot collaboration has benefited users with higher efficiency towards interactive tasks. Nevertheless, most collaborative schemes rely on complicated human-machine interfaces, which might lack the requisite intuitiveness compared with natural limb control. We also expect to understand human intent with low training data requirements. In response to these challenges, this paper introduces an innovative human-robot collaborative framework that seamlessly integrates hand gesture and dynamic movement recognition, voice recognition, and a switchable control adaptation strategy. These modules provide a user-friendly approach that enables the robot to deliver the tools as per user need, especially when the user is working with both hands. Therefore, users can focus on their task execution without additional training in the use of human-machine interfaces, while the robot interprets their intuitive gestures. The proposed multimodal interaction framework is executed in the UR5e robot platform equipped with a RealSense D435i camera, and the effectiveness is assessed through a soldering circuit board task. The experiment results have demonstrated superior performance in hand gesture recognition, where the static hand gesture recognition module achieves an accuracy of 94.3\%, while the dynamic motion recognition module reaches 97.6\% accuracy. Compared with human solo manipulation, the proposed approach facilitates higher efficiency tool delivery, without significantly distracting from human intents.
</details>
<details>
<summary>摘要</summary>
人机合作已经为用户带来更高的效率在互动任务中。然而，大多数合作方案依靠复杂的人机界面，可能缺乏自然的人机交互INTUITIVENESS。我们还期望在训练数据量少的情况下理解人类的意图。为回答这些挑战，本文介绍了一种创新的人机合作框架，它灵活地集成了手势认识、动态运动认识、语音识别和可调制控制策略。这些模块提供了一种用户友好的方法，使得机器人可以根据用户需要提供工具，特别是用户在双手工作时。因此，用户可以专注于任务执行而不需要额外培训人机界面的使用，而机器人可以理解用户的自然姿势。本文所提出的多模式互动框架在UR5e机器人平台上执行，装备了RealSense D435i摄像头，并通过焊接电路板任务进行评估。实验结果显示， static手势认识模块的准确率为94.3%，而动态运动认识模块的准确率达97.6%。相比人类独立操作，提议的方法可以提高工具交付效率，无需明显干扰人类意图。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Graph-Question-Answering-for-Materials-Science-KGQA4MAT-Developing-Natural-Language-Interface-for-Metal-Organic-Frameworks-Knowledge-Graph-MOF-KG"><a href="#Knowledge-Graph-Question-Answering-for-Materials-Science-KGQA4MAT-Developing-Natural-Language-Interface-for-Metal-Organic-Frameworks-Knowledge-Graph-MOF-KG" class="headerlink" title="Knowledge Graph Question Answering for Materials Science (KGQA4MAT): Developing Natural Language Interface for Metal-Organic Frameworks Knowledge Graph (MOF-KG)"></a>Knowledge Graph Question Answering for Materials Science (KGQA4MAT): Developing Natural Language Interface for Metal-Organic Frameworks Knowledge Graph (MOF-KG)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11361">http://arxiv.org/abs/2309.11361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan An, Jane Greenberg, Alex Kalinowski, Xintong Zhao, Xiaohua Hu, Fernando J. Uribe-Romo, Kyle Langlois, Jacob Furst, Diego A. Gómez-Gualdrón</li>
<li>for: 本研究开发了一个包括161个复杂问题的知识Graph问答板（KGQA4MAT），旨在提高材料科学领域知识Graph（MOF-KG）的访问性。</li>
<li>methods: 本研究使用了一种自然语言界面来查询MOF-KG，并开发了一个系统来使用ChatGPT将自然语言问题翻译成正式的KG查询语言。</li>
<li>results: 研究发现ChatGPT可以有效地解决不同平台和查询语言的KG问答问题，并且可以帮助加速材料科学领域知识Graph的搜索和探索。<details>
<summary>Abstract</summary>
We present a comprehensive benchmark dataset for Knowledge Graph Question Answering in Materials Science (KGQA4MAT), with a focus on metal-organic frameworks (MOFs). A knowledge graph for metal-organic frameworks (MOF-KG) has been constructed by integrating structured databases and knowledge extracted from the literature. To enhance MOF-KG accessibility for domain experts, we aim to develop a natural language interface for querying the knowledge graph. We have developed a benchmark comprised of 161 complex questions involving comparison, aggregation, and complicated graph structures. Each question is rephrased in three additional variations, resulting in 644 questions and 161 KG queries. To evaluate the benchmark, we have developed a systematic approach for utilizing ChatGPT to translate natural language questions into formal KG queries. We also apply the approach to the well-known QALD-9 dataset, demonstrating ChatGPT's potential in addressing KGQA issues for different platforms and query languages. The benchmark and the proposed approach aim to stimulate further research and development of user-friendly and efficient interfaces for querying domain-specific materials science knowledge graphs, thereby accelerating the discovery of novel materials.
</details>
<details>
<summary>摘要</summary>
我们提供了一个完整的基准数据集 для知识 graphsQuestion Answering in Materials Science (KGQA4MAT), WITH a focus on metal-organic frameworks (MOFs). A knowledge graph for metal-organic frameworks (MOF-KG) has been constructed by integrating structured databases and knowledge extracted from the literature. To enhance MOF-KG accessibility for domain experts, we aim to develop a natural language interface for querying the knowledge graph. We have developed a benchmark comprised of 161 complex questions involving comparison, aggregation, and complicated graph structures. Each question is rephrased in three additional variations, resulting in 644 questions and 161 KG queries. To evaluate the benchmark, we have developed a systematic approach for utilizing ChatGPT to translate natural language questions into formal KG queries. We also apply the approach to the well-known QALD-9 dataset, demonstrating ChatGPT's potential in addressing KGQA issues for different platforms and query languages. The benchmark and the proposed approach aim to stimulate further research and development of user-friendly and efficient interfaces for querying domain-specific materials science knowledge graphs, thereby accelerating the discovery of novel materials.Here is the translation in Traditional Chinese:我们提供了一个完整的基准数据集 для知识 graphsQuestion Answering in Materials Science (KGQA4MAT),  WITH a focus on metal-organic frameworks (MOFs). A knowledge graph for metal-organic frameworks (MOF-KG) 已经建立了由structured databases和文献中提取的知识 integrate。为了增强MOF-KG对领域专家的存取，我们目标是开发一个自然语言界面来查询知识 Graph。我们已经开发了一个包含161个复杂问题，涉及比较、总和、图像结构的问题。每个问题都有三个版本，共计644个问题和161个KG查询。为了评估基准，我们开发了一个系统性的方法，使用ChatGPT来将自然语言问题转换为正式的KG查询。我们还将这个方法应用到知名的QALD-9数据集上，展示了ChatGPT对不同平台和查询语言的应用潜力。基准和我们提出的方法的目的是促进领域专家用户友好和高效的界面来查询领域专门的材料科学知识图表，以便加速发现新材料的发现。
</details></li>
</ul>
<hr>
<h2 id="3D-Face-Reconstruction-the-Road-to-Forensics"><a href="#3D-Face-Reconstruction-the-Road-to-Forensics" class="headerlink" title="3D Face Reconstruction: the Road to Forensics"></a>3D Face Reconstruction: the Road to Forensics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11357">http://arxiv.org/abs/2309.11357</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simone Maurizio La Cava, Giulia Orrù, Martin Drahansky, Gian Luca Marcialis, Fabio Roli</li>
<li>for: 法律领域中的3D面部重建应用</li>
<li>methods: 使用Surveillance影像和照片进行3D面部重建</li>
<li>results: 略见问题，尚未确立3D面部重建在法律领域的积极角色<details>
<summary>Abstract</summary>
3D face reconstruction algorithms from images and videos are applied to many fields, from plastic surgery to the entertainment sector, thanks to their advantageous features. However, when looking at forensic applications, 3D face reconstruction must observe strict requirements that still make its possible role in bringing evidence to a lawsuit unclear. An extensive investigation of the constraints, potential, and limits of its application in forensics is still missing. Shedding some light on this matter is the goal of the present survey, which starts by clarifying the relation between forensic applications and biometrics, with a focus on face recognition. Therefore, it provides an analysis of the achievements of 3D face reconstruction algorithms from surveillance videos and mugshot images and discusses the current obstacles that separate 3D face reconstruction from an active role in forensic applications. Finally, it examines the underlying data sets, with their advantages and limitations, while proposing alternatives that could substitute or complement them.
</details>
<details>
<summary>摘要</summary>
三维面部重建算法从图像和视频应用到多个领域，从整形外科到娱乐业，因为它们的优点。但当看到审判应用时，三维面部重建必须遵守严格的要求，这些要求仍然使其在提供法律证据的角色是不清晰。为了解决这个问题，本调查的目的是 shedding some light on this matter，开始从审判应用和生物ometrics之间的关系进行清楚的解释，并对surveillance视频和抓捕图像中的3D面部重建算法的成果进行分析，并讨论当前障碍三维面部重建在审判应用中扮演活跃角色的原因。最后，它检查了下面的数据集，包括其优点和限制，并提出了代替或补充的方案。Note: Please note that the translation is in Simplified Chinese, which is used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Survey-on-Rare-Event-Prediction"><a href="#A-Comprehensive-Survey-on-Rare-Event-Prediction" class="headerlink" title="A Comprehensive Survey on Rare Event Prediction"></a>A Comprehensive Survey on Rare Event Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11356">http://arxiv.org/abs/2309.11356</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chathurangi Shyalika, Ruwan Wickramarachchi, Amit Sheth</li>
<li>For: 本研究主要针对频率低的罕见事件预测，即使用机器学习和数据分析方法来预测这些事件的发生。* Methods: 本文综述了目前预测罕见事件的方法，包括数据处理、算法方法和评估方法等，并从不同的数据模式和预测方法角度进行了梳理和分析。* Results: 本文结果显示，预测罕见事件存在许多挑战，如数据不均衡、模型偏向等问题，同时还存在许多研究缺乏或未得到充分发挥的问题。<details>
<summary>Abstract</summary>
Rare event prediction involves identifying and forecasting events with a low probability using machine learning and data analysis. Due to the imbalanced data distributions, where the frequency of common events vastly outweighs that of rare events, it requires using specialized methods within each step of the machine learning pipeline, i.e., from data processing to algorithms to evaluation protocols. Predicting the occurrences of rare events is important for real-world applications, such as Industry 4.0, and is an active research area in statistical and machine learning. This paper comprehensively reviews the current approaches for rare event prediction along four dimensions: rare event data, data processing, algorithmic approaches, and evaluation approaches. Specifically, we consider 73 datasets from different modalities (i.e., numerical, image, text, and audio), four major categories of data processing, five major algorithmic groupings, and two broader evaluation approaches. This paper aims to identify gaps in the current literature and highlight the challenges of predicting rare events. It also suggests potential research directions, which can help guide practitioners and researchers.
</details>
<details>
<summary>摘要</summary>
罕seen事件预测 involve identifying和forecasting事件with a low probability using机器学习和数据分析。由于数据分布的偏度，其中常见事件的频率远远大于罕seen事件的频率，因此需要使用特殊的方法在每个机器学习管道中，从数据处理到算法到评估协议。预测罕seen事件的发生是现实世界应用中的重要问题，如第四代工业，并是机器学习的活跃研究领域。本文全面回顾当前approaches for rare event prediction along four dimensions：罕seen事件数据、数据处理、算法approaches、和评估approaches。Specifically, we consider 73 datasets from different modalities（i.e., numerical, image, text, and audio）、四大类数据处理、五大算法组合、和两大评估方法。本文的目的是要标识当前文献中的空白和预测罕seen事件的挑战，并提出了 potential research directions，以帮助实践者和研究人员。
</details></li>
</ul>
<hr>
<h2 id="C-cdot-ASE-Learning-Conditional-Adversarial-Skill-Embeddings-for-Physics-based-Characters"><a href="#C-cdot-ASE-Learning-Conditional-Adversarial-Skill-Embeddings-for-Physics-based-Characters" class="headerlink" title="C$\cdot$ASE: Learning Conditional Adversarial Skill Embeddings for Physics-based Characters"></a>C$\cdot$ASE: Learning Conditional Adversarial Skill Embeddings for Physics-based Characters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11351">http://arxiv.org/abs/2309.11351</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiyang Dou, Xuelin Chen, Qingnan Fan, Taku Komura, Wenping Wang</li>
<li>for: 这个论文目标是为physics-based characters提供一个有效的学习推荐系统，使得这些角色可以学习多种技能并提供可控性。</li>
<li>methods: 这个系统使用了 conditional Adversarial Skill Embeddings（C$\cdot$ASE），将技能动作分成不同的子集，并使用低级别的条件模型来学习条件行为分布。</li>
<li>results: 论文表明，使用C$\cdot$ASE可以生成高度多样化和现实的技能动作，并且可以在不同的下游任务中重用。此外，该系统还提供了一个高级别的政策或用户可以使用某种技能特定的指定来控制角色的行为。<details>
<summary>Abstract</summary>
We present C$\cdot$ASE, an efficient and effective framework that learns conditional Adversarial Skill Embeddings for physics-based characters. Our physically simulated character can learn a diverse repertoire of skills while providing controllability in the form of direct manipulation of the skills to be performed. C$\cdot$ASE divides the heterogeneous skill motions into distinct subsets containing homogeneous samples for training a low-level conditional model to learn conditional behavior distribution. The skill-conditioned imitation learning naturally offers explicit control over the character's skills after training. The training course incorporates the focal skill sampling, skeletal residual forces, and element-wise feature masking to balance diverse skills of varying complexities, mitigate dynamics mismatch to master agile motions and capture more general behavior characteristics, respectively. Once trained, the conditional model can produce highly diverse and realistic skills, outperforming state-of-the-art models, and can be repurposed in various downstream tasks. In particular, the explicit skill control handle allows a high-level policy or user to direct the character with desired skill specifications, which we demonstrate is advantageous for interactive character animation.
</details>
<details>
<summary>摘要</summary>
我们提出C$\cdot$ASE框架，一种高效有效的框架，学习受条件敌意素嵌入，用于物理基础的角色。我们的物理模拟角色可以学习多种多样的技能，同时提供可控性，通过直接控制技能的执行。C$\cdot$ASE将不同的技能动作分成不同的子集，对具有相同性的样本进行训练低级别的条件模型，学习条件行为分布。通过技能条件学习，可以直接控制角色的技能，并且可以在训练过程中通过焦点技能采样、骨骼剩余力和元素特征掩码来平衡多种技能的复杂性，弥补动力匹配问题，捕捉更加普遍的行为特征。一旦训练完成，条件模型可以生成高度多样化和真实的技能，超越当前模型，并且可以在下游任务中重用。特别是，条件控制把手允许高级政策或用户指定角色的愿望技能规格，我们示示其对交互角色动画有利。
</details></li>
</ul>
<hr>
<h2 id="TRAVID-An-End-to-End-Video-Translation-Framework"><a href="#TRAVID-An-End-to-End-Video-Translation-Framework" class="headerlink" title="TRAVID: An End-to-End Video Translation Framework"></a>TRAVID: An End-to-End Video Translation Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11338">http://arxiv.org/abs/2309.11338</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prottay Kumar Adhikary, Bandaru Sugandhi, Subhojit Ghimire, Santanu Pal, Partha Pakray<br>for: 这篇论文是为了提供一种实现语言翻译的视频翻译系统，以便在不同语言背景下进行有效的沟通。methods: 该系统使用了一种综合语音和视频的翻译方法，通过具体的语音和视频对应关系来实现视频中的语言翻译。results: 该系统可以帮助学生和用户在低资源环境中进行有效的学习和沟通，同时提供了一种更加真实和吸引人的学习环境，从而提高学习效果和参与度。<details>
<summary>Abstract</summary>
In today's globalized world, effective communication with people from diverse linguistic backgrounds has become increasingly crucial. While traditional methods of language translation, such as written text or voice-only translations, can accomplish the task, they often fail to capture the complete context and nuanced information conveyed through nonverbal cues like facial expressions and lip movements. In this paper, we present an end-to-end video translation system that not only translates spoken language but also synchronizes the translated speech with the lip movements of the speaker. Our system focuses on translating educational lectures in various Indian languages, and it is designed to be effective even in low-resource system settings. By incorporating lip movements that align with the target language and matching them with the speaker's voice using voice cloning techniques, our application offers an enhanced experience for students and users. This additional feature creates a more immersive and realistic learning environment, ultimately making the learning process more effective and engaging.
</details>
<details>
<summary>摘要</summary>
今天的全球化世界中，与不同语言背景的人进行有效沟通已经变得越来越重要。传统的语言翻译方法，如文本或声音翻译，可以完成任务，但它们经常无法捕捉 spoken language 中的完整上下文和细节信息。在这篇论文中，我们提出了一个端到端视频翻译系统，不仅翻译 spoken language，还将翻译后的语音与说话人的嘴语ynchronize。我们的系统专注于翻译印度各语言的教育讲解，并且针对具有低资源系统的设置进行设计。通过使用声音恶搅技术，我们的应用程序将嘴语与目标语言的对应语音进行匹配，从而提供了一个更加真实和有趣的学习环境。这种附加的特性使得学习过程更加有效和有趣。
</details></li>
</ul>
<hr>
<h2 id="Gold-YOLO-Efficient-Object-Detector-via-Gather-and-Distribute-Mechanism"><a href="#Gold-YOLO-Efficient-Object-Detector-via-Gather-and-Distribute-Mechanism" class="headerlink" title="Gold-YOLO: Efficient Object Detector via Gather-and-Distribute Mechanism"></a>Gold-YOLO: Efficient Object Detector via Gather-and-Distribute Mechanism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11331">http://arxiv.org/abs/2309.11331</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/huawei-noah/Efficient-Computing">https://github.com/huawei-noah/Efficient-Computing</a></li>
<li>paper_authors: Chengcheng Wang, Wei He, Ying Nie, Jianyuan Guo, Chuanjian Liu, Kai Han, Yunhe Wang</li>
<li>for:  This paper aims to improve the object detection performance of YOLO-series models by introducing a new Gather-Distribute (GD) mechanism and implementing MAE-style pretraining.</li>
<li>methods:  The proposed Gold-YOLO model uses a GD mechanism that combines convolution and self-attention operations to improve multi-scale feature fusion. The model also uses MAE-style pretraining to enhance the performance.</li>
<li>results:  The Gold-YOLO model achieves an outstanding 39.9% AP on the COCO val2017 dataset and 1030 FPS on a T4 GPU, outperforming the previous SOTA model YOLOv6-3.0-N by +2.4% in terms of AP.<details>
<summary>Abstract</summary>
In the past years, YOLO-series models have emerged as the leading approaches in the area of real-time object detection. Many studies pushed up the baseline to a higher level by modifying the architecture, augmenting data and designing new losses. However, we find previous models still suffer from information fusion problem, although Feature Pyramid Network (FPN) and Path Aggregation Network (PANet) have alleviated this. Therefore, this study provides an advanced Gatherand-Distribute mechanism (GD) mechanism, which is realized with convolution and self-attention operations. This new designed model named as Gold-YOLO, which boosts the multi-scale feature fusion capabilities and achieves an ideal balance between latency and accuracy across all model scales. Additionally, we implement MAE-style pretraining in the YOLO-series for the first time, allowing YOLOseries models could be to benefit from unsupervised pretraining. Gold-YOLO-N attains an outstanding 39.9% AP on the COCO val2017 datasets and 1030 FPS on a T4 GPU, which outperforms the previous SOTA model YOLOv6-3.0-N with similar FPS by +2.4%. The PyTorch code is available at https://github.com/huawei-noah/Efficient-Computing/tree/master/Detection/Gold-YOLO, and the MindSpore code is available at https://gitee.com/mindspore/models/tree/master/research/cv/Gold_YOLO.
</details>
<details>
<summary>摘要</summary>
在过去的几年中，YOLO系列模型在实时对象检测领域取得了领先地位。许多研究尝试提高基线，通过修改架构、增强数据和设计新的损失函数。然而，我们发现先前的模型仍然受到信息融合问题的困扰，尽管Feature Pyramid Network（FPN）和Path Aggregation Network（PANet）已经减轻了这个问题。因此，本研究提出了一种高级的聚合分发机制（GD）机制，通过 convolution 和自注意操作实现。这新的设计的模型被称为 Gold-YOLO，它提高了多尺度特征融合能力，并在所有模型缩放水平上实现了理想的平衡 между延迟和准确率。此外，我们在 YOLO 系列模型中实施了 MAE 风格的预训练，让 YOLO 系列模型可以从无监督预训练中受益。Gold-YOLO-N 在 COCO val2017 数据集上达到了出色的 39.9% AP 和 T4 GPU 上的 1030 FPS，超过了先前的 SOTA 模型 YOLOv6-3.0-N 的相似 FPS 值 by +2.4%。PyTorch 代码可以在 <https://github.com/huawei-noah/Efficient-Computing/tree/master/Detection/Gold-YOLO> 找到，MindSpore 代码可以在 <https://gitee.com/mindspore/models/tree/master/research/cv/Gold_YOLO> 找到。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Pricing-of-Applications-in-Cloud-Marketplaces-using-Game-Theory"><a href="#Dynamic-Pricing-of-Applications-in-Cloud-Marketplaces-using-Game-Theory" class="headerlink" title="Dynamic Pricing of Applications in Cloud Marketplaces using Game Theory"></a>Dynamic Pricing of Applications in Cloud Marketplaces using Game Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11316">http://arxiv.org/abs/2309.11316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Safiye Ghasemi, Mohammad Reza Meybodi, Mehdi Dehghan Takht-Fooladi, Amir Masoud Rahmani</li>
<li>for: 这个论文旨在研究云市场竞争对应的价格策略，以帮助企业更好地制定价格策略。</li>
<li>methods: 该论文采用了游戏理论来设计动态价格策略，并在委员会中考虑了多家提供商的竞争。</li>
<li>results: 该论文通过数学模型来研究云市场竞争，并证明了存在和uniqueness的纳什平衡，从而为企业提供了新的动态价格策略。<details>
<summary>Abstract</summary>
The competitive nature of Cloud marketplaces as new concerns in delivery of services makes the pricing policies a crucial task for firms. so that, pricing strategies has recently attracted many researchers. Since game theory can handle such competing well this concern is addressed by designing a normal form game between providers in current research. A committee is considered in which providers register for improving their competition based pricing policies. The functionality of game theory is applied to design dynamic pricing policies. The usage of the committee makes the game a complete information one, in which each player is aware of every others payoff functions. The players enhance their pricing policies to maximize their profits. The contribution of this paper is the quantitative modeling of Cloud marketplaces in form of a game to provide novel dynamic pricing strategies; the model is validated by proving the existence and the uniqueness of Nash equilibrium of the game.
</details>
<details>
<summary>摘要</summary>
云市场的竞争性新问题在服务交付中带来了价格策略的核心任务 для公司。因此，价格策略在最近吸引了许多研究人员。由于游戏理论可以良好处理这种竞争，因此在当前研究中，设计了一个委员会，让提供者为了改善其竞争基础价格策略进行注册。通过游戏理论的应用，设计了动态价格策略。由于委员会的存在，游戏变为完全信息游戏，每个玩家知道彼此的利益函数。玩家通过优化价格策略来 maximize 利润。本文的贡献在于以游戏的形式对云市场进行量化模型化，提供了新的动态价格策略；模型的存在和uniqueness 的证明，证明了这种游戏的稳定性。
</details></li>
</ul>
<hr>
<h2 id="A-Competition-based-Pricing-Strategy-in-Cloud-Markets-using-Regret-Minimization-Techniques"><a href="#A-Competition-based-Pricing-Strategy-in-Cloud-Markets-using-Regret-Minimization-Techniques" class="headerlink" title="A Competition-based Pricing Strategy in Cloud Markets using Regret Minimization Techniques"></a>A Competition-based Pricing Strategy in Cloud Markets using Regret Minimization Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11312">http://arxiv.org/abs/2309.11312</a></li>
<li>repo_url: None</li>
<li>paper_authors: S. Ghasemi, M. R. Meybodi, M. Dehghan, A. M. Rahmani</li>
<li>for: This paper aims to address the challenge of pricing in Cloud computing marketplaces, where providers compete without knowing each other’s pricing policies.</li>
<li>methods: The paper proposes a pricing policy based on regret minimization and applies it to an incomplete-information game modeling the competition among Cloud providers. The algorithm updates the distribution of strategies based on experienced regret, leading to faster minimization of regret and increased profits for providers.</li>
<li>results: The experimental results show that the proposed pricing policy leads to much greater increases in provider profits compared to other pricing policies, and the efficiency of various regret minimization techniques in a simulated marketplace of Cloud is discussed. Additionally, the study examines the return on investment of providers in considered organizations and finds promising results.Here’s the Chinese translation of the three key points:</li>
<li>for: 这篇论文目标是解决云计算市场场所中的价格问题， provider competing without knowing each other’s pricing policies。</li>
<li>methods: 论文提出一种基于后悔最小化的价格策略，并应用到了不完全信息游戏中模拟云提供商的竞争。算法根据经验的后悔来更新策略分布，导致快速减少后悔。</li>
<li>results: 实验结果表明，提出的价格策略在其他价格策略的比较中显示出了很大的增长，并且在模拟云中的竞争市场中，不同的后悔最小化技术的效率得到了详细的讨论。此外，论文还研究了考虑了不同组织中提供商的投资回报，并发现了有前提。<details>
<summary>Abstract</summary>
Cloud computing as a fairly new commercial paradigm, widely investigated by different researchers, already has a great range of challenges. Pricing is a major problem in Cloud computing marketplace; as providers are competing to attract more customers without knowing the pricing policies of each other. To overcome this lack of knowledge, we model their competition by an incomplete-information game. Considering the issue, this work proposes a pricing policy related to the regret minimization algorithm and applies it to the considered incomplete-information game. Based on the competition based marketplace of the Cloud, providers update the distribution of their strategies using the experienced regret. The idea of iteratively applying the algorithm for updating probabilities of strategies causes the regret get minimized faster. The experimental results show much more increase in profits of the providers in comparison with other pricing policies. Besides, the efficiency of a variety of regret minimization techniques in a simulated marketplace of Cloud are discussed which have not been observed in the studied literature. Moreover, return on investment of providers in considered organizations is studied and promising results appeared.
</details>
<details>
<summary>摘要</summary>
云计算作为一种比较新的商业模式，已经广泛研究了不同的研究者。在云计算市场中，价格是一个主要的问题，Provider competing to attract more customers without knowing each other's pricing policies。为了解决这个问题，我们模拟了这个 incomplete-information game。基于云计算市场的竞争性，提供者通过经验的 regret 更新分布的策略。iteratively applying the algorithm for updating probabilities of strategies causes the regret get minimized faster。实验结果表明，与其他价格策略相比，提供者的利润增加了很多。此外，我们还发现了一些 regret minimization techniques 在云计算市场中的效率，这些result未经studied literature。此外，我们还研究了Provider的投资回报，并获得了扎实的结果。
</details></li>
</ul>
<hr>
<h2 id="Rating-Prediction-in-Conversational-Task-Assistants-with-Behavioral-and-Conversational-Flow-Features"><a href="#Rating-Prediction-in-Conversational-Task-Assistants-with-Behavioral-and-Conversational-Flow-Features" class="headerlink" title="Rating Prediction in Conversational Task Assistants with Behavioral and Conversational-Flow Features"></a>Rating Prediction in Conversational Task Assistants with Behavioral and Conversational-Flow Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11307">http://arxiv.org/abs/2309.11307</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rafaelhferreira/cta_rating_prediction">https://github.com/rafaelhferreira/cta_rating_prediction</a></li>
<li>paper_authors: Rafael Ferreira, David Semedo, João Magalhães</li>
<li>for: 预测对话任务助手（CTA）的成功可以帮助我们理解用户行为并采取相应的行动。</li>
<li>methods: 这篇论文提出了TB-Rater模型，这是一种将对话流程特征与用户行为特征结合在一起的Transformer模型，用于在CTA场景下预测用户评分。具体来说，我们使用了真实的人类-机器人对话和在Alexa TaskBot挑战中收集的用户评分数据。</li>
<li>results: 我们的结果表明，模型对话流程和用户行为方面的特征可以在单个模型中结合，以预测Offline评分。此外，对CTA特有的行为特征进行分析，可以为未来系统提供参考。<details>
<summary>Abstract</summary>
Predicting the success of Conversational Task Assistants (CTA) can be critical to understand user behavior and act accordingly. In this paper, we propose TB-Rater, a Transformer model which combines conversational-flow features with user behavior features for predicting user ratings in a CTA scenario. In particular, we use real human-agent conversations and ratings collected in the Alexa TaskBot challenge, a novel multimodal and multi-turn conversational context. Our results show the advantages of modeling both the conversational-flow and behavioral aspects of the conversation in a single model for offline rating prediction. Additionally, an analysis of the CTA-specific behavioral features brings insights into this setting and can be used to bootstrap future systems.
</details>
<details>
<summary>摘要</summary>
预测对话任务助手（CTA）的成功可以帮助我们更好地理解用户行为，从而更好地行动。在这篇论文中，我们提出了TB-Rater模型，这是一个基于转换器模型，结合对话流程特征和用户行为特征来预测用户评分在CTA场景中。具体来说，我们使用了真实的人类-机器人对话和在Alexa TaskBot挑战中收集的用户评分数据，这是一个新的多模式和多轮对话上下文。我们的结果表明，将对话流程和行为方面的特征模型在单个模型中可以在线评分中获得优势。此外，对CTA特有的行为特征进行分析，可以为未来系统提供Bootstrap。
</details></li>
</ul>
<hr>
<h2 id="FaceDiffuser-Speech-Driven-3D-Facial-Animation-Synthesis-Using-Diffusion"><a href="#FaceDiffuser-Speech-Driven-3D-Facial-Animation-Synthesis-Using-Diffusion" class="headerlink" title="FaceDiffuser: Speech-Driven 3D Facial Animation Synthesis Using Diffusion"></a>FaceDiffuser: Speech-Driven 3D Facial Animation Synthesis Using Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11306">http://arxiv.org/abs/2309.11306</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uuembodiedsocialai/FaceDiffuser">https://github.com/uuembodiedsocialai/FaceDiffuser</a></li>
<li>paper_authors: Stefan Stan, Kazi Injamamul Haque, Zerrin Yumak</li>
<li>for: 这个论文旨在解决Current methods mostly focus on deterministic deep learning methods for speech-driven 3D facial animation synthesis, which do not accurately capture non-verbal facial cues.</li>
<li>methods: 该方法基于Diffusion Technique，使用预训练的大语音表示模型HuBERT对音频输入进行编码。</li>
<li>results: 我们的方法在对比于现有方法时达到了更好或相当的结果，并且引入了一个新的基于blendshape的rigged character的数据集。Here’s the full summary in Simplified Chinese:</li>
<li>for: 这个论文旨在解决Current methods mostly focus on deterministic deep learning methods for speech-driven 3D facial animation synthesis, which do not accurately capture non-verbal facial cues.</li>
<li>methods: 该方法基于Diffusion Technique，使用预训练的大语音表示模型HuBERT对音频输入进行编码。</li>
<li>results: 我们的方法在对比于现有方法时达到了更好或相当的结果，并且引入了一个新的基于blendshape的rigged character的数据集。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Speech-driven 3D facial animation synthesis has been a challenging task both in industry and research. Recent methods mostly focus on deterministic deep learning methods meaning that given a speech input, the output is always the same. However, in reality, the non-verbal facial cues that reside throughout the face are non-deterministic in nature. In addition, majority of the approaches focus on 3D vertex based datasets and methods that are compatible with existing facial animation pipelines with rigged characters is scarce. To eliminate these issues, we present FaceDiffuser, a non-deterministic deep learning model to generate speech-driven facial animations that is trained with both 3D vertex and blendshape based datasets. Our method is based on the diffusion technique and uses the pre-trained large speech representation model HuBERT to encode the audio input. To the best of our knowledge, we are the first to employ the diffusion method for the task of speech-driven 3D facial animation synthesis. We have run extensive objective and subjective analyses and show that our approach achieves better or comparable results in comparison to the state-of-the-art methods. We also introduce a new in-house dataset that is based on a blendshape based rigged character. We recommend watching the accompanying supplementary video. The code and the dataset will be publicly available.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>人工智能驱动的3D面部动画生成问题在行业和研究中都是挑战性的。现有的方法大多涉及决定性深度学习方法，即给定一个语音输入，输出总是一样的。然而，现实中的非语言面部征标是不决定的性质。此外，大多数方法都集中在3D顶点基本的数据集和方法上，与现有的人物动画管道相容的方法scarce。为解决这些问题，我们介绍FaceDiffuser，一种非决定性深度学习模型，用于生成语音驱动的3D面部动画。我们的方法基于扩散技术，使用预训练的大语音表示模型HuBERT来编码音频输入。到目前为止，我们是第一个使用扩散方法来解决语音驱动3D面部动画生成问题。我们进行了广泛的对象和主观分析，并证明我们的方法可以与当前状态的方法相比或更好的成绩。我们还介绍了一个新的基于blendshape的人物动画数据集。建议观看附加的补充视频。代码和数据集将公开发布。
</details></li>
</ul>
<hr>
<h2 id="A-Cost-Aware-Mechanism-for-Optimized-Resource-Provisioning-in-Cloud-Computing"><a href="#A-Cost-Aware-Mechanism-for-Optimized-Resource-Provisioning-in-Cloud-Computing" class="headerlink" title="A Cost-Aware Mechanism for Optimized Resource Provisioning in Cloud Computing"></a>A Cost-Aware Mechanism for Optimized Resource Provisioning in Cloud Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11299">http://arxiv.org/abs/2309.11299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Safiye Ghasemi, Mohammad Reza Meybodi, Mehdi Dehghan Takht Fooladi, Amir Masoud Rahmani</li>
<li>for: 这篇论文旨在提出一种新的资源配置方法，以减少资源配置成本的方式来满足需求。</li>
<li>methods: 本文使用了学习自动过程来选择最适合的资源来主机每个服务，并考虑成本和服务需求。</li>
<li>results: 实验结果显示，我们的方法能够有效地运行许多不同类型的应用程序，并且可以适当地减少资源配置成本。<details>
<summary>Abstract</summary>
Due to the recent wide use of computational resources in cloud computing, new resource provisioning challenges have been emerged. Resource provisioning techniques must keep total costs to a minimum while meeting the requirements of the requests. According to widely usage of cloud services, it seems more challenging to develop effective schemes for provisioning services cost-effectively; we have proposed a novel learning based resource provisioning approach that achieves cost-reduction guarantees of demands. The contributions of our optimized resource provisioning (ORP) approach are as follows. Firstly, it is designed to provide a cost-effective method to efficiently handle the provisioning of requested applications; while most of the existing models allow only workflows in general which cares about the dependencies of the tasks, ORP performs based on services of which applications comprised and cares about their efficient provisioning totally. Secondly, it is a learning automata-based approach which selects the most proper resources for hosting each service of the demanded application; our approach considers both cost and service requirements together for deploying applications. Thirdly, a comprehensive evaluation is performed for three typical workloads: data-intensive, process-intensive and normal applications. The experimental results show that our method adapts most of the requirements efficiently, and furthermore the resulting performance meets our design goals.
</details>
<details>
<summary>摘要</summary>
The contributions of our optimized resource provisioning (ORP) approach are as follows:1. Cost-effective method: ORP provides a cost-effective method to efficiently handle the provisioning of requested applications, while most existing models only consider workflows in general and ignore the dependencies of tasks. ORP takes into account the services that applications comprise and cares about their efficient provisioning.2. Learning automata-based approach: ORP is a learning automata-based approach that selects the most appropriate resources for hosting each service of the demanded application. Our approach considers both cost and service requirements together for deploying applications.3. Comprehensive evaluation: We conducted a comprehensive evaluation for three typical workloads: data-intensive, process-intensive, and normal applications. The experimental results show that our method adapts to most of the requirements efficiently, and the resulting performance meets our design goals.
</details></li>
</ul>
<hr>
<h2 id="CPLLM-Clinical-Prediction-with-Large-Language-Models"><a href="#CPLLM-Clinical-Prediction-with-Large-Language-Models" class="headerlink" title="CPLLM: Clinical Prediction with Large Language Models"></a>CPLLM: Clinical Prediction with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11295">http://arxiv.org/abs/2309.11295</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nadavlab/CPLLM">https://github.com/nadavlab/CPLLM</a></li>
<li>paper_authors: Ofir Ben Shoham, Nadav Rappoport</li>
<li>for: 这个论文是为了提出一种基于大语言模型的临床预测方法，以便预测患者是否会在下一次访问或接下来的诊断中被诊断出某种疾病。</li>
<li>methods: 这个方法是基于已经预训练的大语言模型（LLM），通过quantization和提示来进行微调，以便预测患者的疾病风险。</li>
<li>results: 对于不同的基线模型，包括Logistic Regression、RETAIN和Med-BERT，我们的CPLLM模型在PR-AUC和ROC-AUC metric上都显示出了明显的提升，较baseline模型更高。<details>
<summary>Abstract</summary>
We present Clinical Prediction with Large Language Models (CPLLM), a method that involves fine-tuning a pre-trained Large Language Model (LLM) for clinical disease prediction. We utilized quantization and fine-tuned the LLM using prompts, with the task of predicting whether patients will be diagnosed with a target disease during their next visit or in the subsequent diagnosis, leveraging their historical diagnosis records. We compared our results versus various baselines, including Logistic Regression, RETAIN, and Med-BERT, which is the current state-of-the-art model for disease prediction using structured EHR data. Our experiments have shown that CPLLM surpasses all the tested models in terms of both PR-AUC and ROC-AUC metrics, displaying noteworthy enhancements compared to the baseline models.
</details>
<details>
<summary>摘要</summary>
我团队现在提出了临床预测使用大型语言模型（CPLLM），这种方法是通过先前训练的大型语言模型（LLM）进行精度调整，以预测患者将在下一次访问或接下来的诊断中被诊断出的疾病。我们使用量化和精度调整LLM，使其能够利用患者历史诊断记录来预测疾病。我们与various baselines进行比较，包括Logistic Regression、RETAIN和Med-BERT，这些模型都是使用结构化医疗记录数据进行疾病预测的现状之arte。我们的实验结果表明，CPLLM在PR-AUC和ROC-AUC指标上都超过了所有测试模型，显示了与基线模型相比而言的remarkable enhancements。
</details></li>
</ul>
<hr>
<h2 id="Overview-of-AuTexTification-at-IberLEF-2023-Detection-and-Attribution-of-Machine-Generated-Text-in-Multiple-Domains"><a href="#Overview-of-AuTexTification-at-IberLEF-2023-Detection-and-Attribution-of-Machine-Generated-Text-in-Multiple-Domains" class="headerlink" title="Overview of AuTexTification at IberLEF 2023: Detection and Attribution of Machine-Generated Text in Multiple Domains"></a>Overview of AuTexTification at IberLEF 2023: Detection and Attribution of Machine-Generated Text in Multiple Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11285">http://arxiv.org/abs/2309.11285</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/autextification/AuTexTification-Overview">https://github.com/autextification/AuTexTification-Overview</a></li>
<li>paper_authors: Areg Mikael Sarvazyan, José Ángel González, Marc Franco-Salvador, Francisco Rangel, Berta Chulvi, Paolo Rosso</li>
<li>for: 这篇论文描述了2023年的IberLEF工作坊中的AuTexTification分类任务，这是一个iberian语言评估论坛（SEPLN）2023年会议的一部分。</li>
<li>methods: 这篇论文描述了AuTexTification任务的两个子任务：第一个子任务是判断文本是人工生成的还是大语言模型生成的；第二个子任务是归属一个机器生成文本到六种不同的文本生成模型中。</li>
<li>results: 这篇论文描述了AuTexTification2023数据集，包含了英语和西班牙语的160,000多个文本，来自五个领域（微博、评论、新闻、法律和使用教程）。总共有114个团队参加了比赛，其中36个团队发送了175个运行，20个团队发送了工作笔记。在这篇报告中，我们介绍了AuTexTification数据集和任务，参与系统，以及结果。<details>
<summary>Abstract</summary>
This paper presents the overview of the AuTexTification shared task as part of the IberLEF 2023 Workshop in Iberian Languages Evaluation Forum, within the framework of the SEPLN 2023 conference. AuTexTification consists of two subtasks: for Subtask 1, participants had to determine whether a text is human-authored or has been generated by a large language model. For Subtask 2, participants had to attribute a machine-generated text to one of six different text generation models. Our AuTexTification 2023 dataset contains more than 160.000 texts across two languages (English and Spanish) and five domains (tweets, reviews, news, legal, and how-to articles). A total of 114 teams signed up to participate, of which 36 sent 175 runs, and 20 of them sent their working notes. In this overview, we present the AuTexTification dataset and task, the submitted participating systems, and the results.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Rethinking-Sensors-Modeling-Hierarchical-Information-Enhanced-Traffic-Forecasting"><a href="#Rethinking-Sensors-Modeling-Hierarchical-Information-Enhanced-Traffic-Forecasting" class="headerlink" title="Rethinking Sensors Modeling: Hierarchical Information Enhanced Traffic Forecasting"></a>Rethinking Sensors Modeling: Hierarchical Information Enhanced Traffic Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11284">http://arxiv.org/abs/2309.11284</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/VAN-QIAN/CIKM23-HIEST">https://github.com/VAN-QIAN/CIKM23-HIEST</a></li>
<li>paper_authors: Qian Ma, Zijian Zhang, Xiangyu Zhao, Haoliang Li, Hongwei Zhao, Yiqi Wang, Zitao Liu, Wanyu Wang</li>
<li>for: 这篇论文主要关注于城市化加速时的交通预测，并在空间时间预测中提出了一个新的方法。</li>
<li>methods: 本文提出了一个 Hierarchical Information Enhanced Spatio-Temporal prediction 方法（HIEST），它将感应器之间的依赖性分为两层：地域层和全球层。</li>
<li>results: 实验结果显示，HIEST 方法在比较于现有基eline之上获得了leading performance。<details>
<summary>Abstract</summary>
With the acceleration of urbanization, traffic forecasting has become an essential role in smart city construction. In the context of spatio-temporal prediction, the key lies in how to model the dependencies of sensors. However, existing works basically only consider the micro relationships between sensors, where the sensors are treated equally, and their macroscopic dependencies are neglected. In this paper, we argue to rethink the sensor's dependency modeling from two hierarchies: regional and global perspectives. Particularly, we merge original sensors with high intra-region correlation as a region node to preserve the inter-region dependency. Then, we generate representative and common spatio-temporal patterns as global nodes to reflect a global dependency between sensors and provide auxiliary information for spatio-temporal dependency learning. In pursuit of the generality and reality of node representations, we incorporate a Meta GCN to calibrate the regional and global nodes in the physical data space. Furthermore, we devise the cross-hierarchy graph convolution to propagate information from different hierarchies. In a nutshell, we propose a Hierarchical Information Enhanced Spatio-Temporal prediction method, HIEST, to create and utilize the regional dependency and common spatio-temporal patterns. Extensive experiments have verified the leading performance of our HIEST against state-of-the-art baselines. We publicize the code to ease reproducibility.
</details>
<details>
<summary>摘要</summary>
随着城市化的加速，城市智能化建设中的交通预测已成为一项重要的任务。在空间时间预测的上下文中，关键在于如何模型感知器之间的依赖关系。然而，现有的工作基本上只考虑了感知器之间的微型关系，忽略了感知器的宏观依赖关系。在这篇论文中，我们认为应重新考虑感知器之间的依赖模型化，从两个层次来看：地域和全球视角。具体来说，我们将原始感知器高度相关的内部节点合并为一个地域节点，以保留宏观依赖关系。然后，我们生成了代表性的全球节点，用于反映全球感知器之间的依赖关系，并提供辅助的空间时间依赖学习信息。为了保证节点表示的通用性和实际性，我们将MetaGCN integrate into physical data space。此外，我们提出了跨层次图 convolution来传递不同层次的信息。简而言之，我们提出了一种增强空间时间预测方法，即 Hierarchical Information Enhanced Spatio-Temporal prediction（HIEST），以创造和利用地域依赖关系和共同空间时间模式。我们的实验证明了HIEST在比较顶尖基准下的领先性。我们公布了代码，以便重现。
</details></li>
</ul>
<hr>
<h2 id="Open-endedness-induced-through-a-predator-prey-scenario-using-modular-robots"><a href="#Open-endedness-induced-through-a-predator-prey-scenario-using-modular-robots" class="headerlink" title="Open-endedness induced through a predator-prey scenario using modular robots"></a>Open-endedness induced through a predator-prey scenario using modular robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11275">http://arxiv.org/abs/2309.11275</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitri Kachler, Karine Miras</li>
<li>for: 这个研究探讨了如何通过探险-猎食情况引发开放演化（OEE）。</li>
<li>methods: 研究使用固定 morphology 的模块机器人，其控制器被进行进化。机器人可以发送和接收信号，并在环境中识别其他机器人的相对位置。研究还引入了一个标记系统，它改变了个体如何识别彼此的方式，并预计会增加行为复杂性。</li>
<li>results: 研究发现了适应策略的出现，证明了通过探险-猎食 dinamics 使用模块机器人来引发 OEE 的可能性。然而，这种emergence似乎需要根据行为标准来条件繁殖。<details>
<summary>Abstract</summary>
This work investigates how a predator-prey scenario can induce the emergence of Open-Ended Evolution (OEE). We utilize modular robots of fixed morphologies whose controllers are subject to evolution. In both species, robots can send and receive signals and perceive the relative positions of other robots in the environment. Specifically, we introduce a feature we call a tagging system: it modifies how individuals can perceive each other and is expected to increase behavioral complexity. Our results show the emergence of adaptive strategies, demonstrating the viability of inducing OEE through predator-prey dynamics using modular robots. Such emergence, nevertheless, seemed to depend on conditioning reproduction to an explicit behavioral criterion.
</details>
<details>
<summary>摘要</summary>
这项研究探讨了掠食-猎物情况如何引起开放演化（OEE）的出现。我们利用固定形态的模块机器人的控制器进行进化。在两种机器人中，机器人可以发送和接收信号，并且可以感知环境中其他机器人的相对位置。我们引入了一个特征，即标记系统：它改变了个体如何感知彼此，并且预期会增加行为复杂性。我们的结果显示了适应策略的出现，证明了通过掠食-猎物 dinamics 使用模块机器人来引起 OEE 的可能性。然而，这种出现似乎виси于对行为标准的条件修复 reproduce。Note: Please keep in mind that the translation is not perfect and may not capture all the nuances of the original text.
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Data-Suitability-and-Performance-Testing-Using-Fault-Injection-Testing-Framework"><a href="#Machine-Learning-Data-Suitability-and-Performance-Testing-Using-Fault-Injection-Testing-Framework" class="headerlink" title="Machine Learning Data Suitability and Performance Testing Using Fault Injection Testing Framework"></a>Machine Learning Data Suitability and Performance Testing Using Fault Injection Testing Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11274">http://arxiv.org/abs/2309.11274</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manal Rahal, Bestoun S. Ahmed, Jorgen Samuelsson<br>for:This paper aims to address the gap in testing approaches for input data in machine learning (ML) systems, specifically the resilience of ML models to intentionally-triggered data faults.methods:The proposed framework, called FIUL-Data, uses data mutators to explore vulnerabilities of ML systems against data fault injections. The framework is designed with three main ideas: mutators are not random, one mutator is applied at a time, and selected ML models are optimized beforehand.results:The FIUL-Data framework is evaluated using data from analytical chemistry, and the results show that the framework allows for the evaluation of the resilience of ML models. In most experiments, ML models show higher resilience at larger training datasets, and gradient boost performed better than support vector regression in smaller training sets. The mean squared error metric is found to be useful in evaluating the resilience of models due to its higher sensitivity to data mutation.Here is the text in Simplified Chinese:for:这篇论文目标是解决机器学习（ML）系统中输入数据测试方法的差距，具体是测试ML模型对数据fault的抗性。methods:该提议的框架是FIUL-Data，使用数据变换器来探索ML系统对数据fault的敏感性。框架设计了三个主要想法：变换器不是随机的，一个变换器在一次实例时应用，并且选择的ML模型在先前优化。results:FIUL-Data框架在分析化学中使用数据进行评估，结果显示该框架可以评估ML模型的抗性。大多数实验结果表明，ML模型在更大的训练集上显示更高的抗性，并且在较小的训练集中，梯度拟合perform луч于支持向量回归。总的来说， Mean Squared Error 度量有用于评估模型的抗性，因为它对数据变换更敏感。<details>
<summary>Abstract</summary>
Creating resilient machine learning (ML) systems has become necessary to ensure production-ready ML systems that acquire user confidence seamlessly. The quality of the input data and the model highly influence the successful end-to-end testing in data-sensitive systems. However, the testing approaches of input data are not as systematic and are few compared to model testing. To address this gap, this paper presents the Fault Injection for Undesirable Learning in input Data (FIUL-Data) testing framework that tests the resilience of ML models to multiple intentionally-triggered data faults. Data mutators explore vulnerabilities of ML systems against the effects of different fault injections. The proposed framework is designed based on three main ideas: The mutators are not random; one data mutator is applied at an instance of time, and the selected ML models are optimized beforehand. This paper evaluates the FIUL-Data framework using data from analytical chemistry, comprising retention time measurements of anti-sense oligonucleotide. Empirical evaluation is carried out in a two-step process in which the responses of selected ML models to data mutation are analyzed individually and then compared with each other. The results show that the FIUL-Data framework allows the evaluation of the resilience of ML models. In most experiments cases, ML models show higher resilience at larger training datasets, where gradient boost performed better than support vector regression in smaller training sets. Overall, the mean squared error metric is useful in evaluating the resilience of models due to its higher sensitivity to data mutation.
</details>
<details>
<summary>摘要</summary>
创建可恢复的机器学习（ML）系统已经成为确保生产准备的ML系统获得用户信任的必要手段。输入数据质量和模型对生成端到端测试的成功产生很大影响。然而，输入数据测试的方法并不够系统化，与模型测试相比相对落后。为解决这个差距，本文提出了输入数据中的异常投入测试框架（FIUL-Data），用于测试ML模型对多种意外触发的数据异常的抗性。数据变换器探索了ML系统对各种异常投入的敏感性。该框架基于以下三个主要想法：变换器不是随机的，只有一个变换器在一个时间点上应用，并且选择的ML模型在先前优化。本文通过使用分析化学数据，包括抑制肽的释放时间测量，对FIUL-Data框架进行了实证评估。实验在两步进行，先分别分析选择的ML模型对数据变换的响应，然后对各模型进行比较。结果表明，FIUL-Data框架可以评估ML模型的抗性。大多数实验情况下，ML模型在更大的训练集上显示更高的抗性，其中梯度拟合在小训练集中表现更好。总的来说，平均方差误差度量是评估ML模型抗性的有用指标。
</details></li>
</ul>
<hr>
<h2 id="Grounded-Complex-Task-Segmentation-for-Conversational-Assistants"><a href="#Grounded-Complex-Task-Segmentation-for-Conversational-Assistants" class="headerlink" title="Grounded Complex Task Segmentation for Conversational Assistants"></a>Grounded Complex Task Segmentation for Conversational Assistants</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11271">http://arxiv.org/abs/2309.11271</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rafaelhferreira/grounded_task_segmentation_cta">https://github.com/rafaelhferreira/grounded_task_segmentation_cta</a></li>
<li>paper_authors: Rafael Ferreira, David Semedo, João Magalhães</li>
<li>for: 这 paper 是为了改进 web-based  instrucional text，使其更适合 conversational  Setting。</li>
<li>methods: 该 paper 使用 Transformer-based 架构进行计算模型，以及按照 conversational enario 进行 instrucional 结构标注。</li>
<li>results: 经过测试，用户对 step 的 complexity 和 length 有所偏好，并且提出的方法可以改善原始的 web-based instrucional text，提高了 86% 的评价。<details>
<summary>Abstract</summary>
Following complex instructions in conversational assistants can be quite daunting due to the shorter attention and memory spans when compared to reading the same instructions. Hence, when conversational assistants walk users through the steps of complex tasks, there is a need to structure the task into manageable pieces of information of the right length and complexity. In this paper, we tackle the recipes domain and convert reading structured instructions into conversational structured ones. We annotated the structure of instructions according to a conversational scenario, which provided insights into what is expected in this setting. To computationally model the conversational step's characteristics, we tested various Transformer-based architectures, showing that a token-based approach delivers the best results. A further user study showed that users tend to favor steps of manageable complexity and length, and that the proposed methodology can improve the original web-based instructional text. Specifically, 86% of the evaluated tasks were improved from a conversational suitability point of view.
</details>
<details>
<summary>摘要</summary>
请求中的复杂指令可能会让用户感到困惑，这是因为与阅读相同的指令相比，用户的注意力和记忆 span 更短。因此，当 conversational assistant 通过多个步骤引导用户完成复杂任务时，需要将任务分解成可管理的小块信息，以便用户更好地理解和完成。在这篇论文中，我们将 recipes 领域中的指令结构化为 conversational 结构，并通过对话情境进行标注，从而获得了更深刻的理解。为了计算 conversational 步骤的特点，我们测试了不同的 Transformer 基 architecture，发现 token 基本法取得了最好的结果。进一步的用户研究表明，用户偏好管理 complexity 和 length 的步骤，而我们的方法ologies 可以改善原始的网络上的指令文本。特别是，86% 的评估任务得到了 conversational 适用性的改进。
</details></li>
</ul>
<hr>
<h2 id="Sequence-to-Sequence-Spanish-Pre-trained-Language-Models"><a href="#Sequence-to-Sequence-Spanish-Pre-trained-Language-Models" class="headerlink" title="Sequence-to-Sequence Spanish Pre-trained Language Models"></a>Sequence-to-Sequence Spanish Pre-trained Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11259">http://arxiv.org/abs/2309.11259</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vgaraujov/Seq2Seq-Spanish-PLMs">https://github.com/vgaraujov/Seq2Seq-Spanish-PLMs</a></li>
<li>paper_authors: Vladimir Araujo, Maria Mihaela Trusca, Rodrigo Tufiño, Marie-Francine Moens</li>
<li>for: 这篇论文旨在开发针对西班牙语序列训练的encoder-decoder模型，用于进行文本摘要、重叙和生成问答等序列转换任务。</li>
<li>methods: 该论文采用了BERT、RoBERTa和GPT等批处理语言模型的encoder-decoder架构，并对其进行了适应性的预训练，以便在西班牙语文本中进行更好的表现。</li>
<li>results: 论文通过对各模型进行了广泛的评估，发现BERT和T5模型在所有评估任务中表现最佳，而BART模型也在某些任务中表现出色。此外，该论文还将所有模型公开发布到研究社区，以促进未来的西班牙语处理研究。<details>
<summary>Abstract</summary>
In recent years, substantial advancements in pre-trained language models have paved the way for the development of numerous non-English language versions, with a particular focus on encoder-only and decoder-only architectures. While Spanish language models encompassing BERT, RoBERTa, and GPT have exhibited prowess in natural language understanding and generation, there remains a scarcity of encoder-decoder models designed for sequence-to-sequence tasks involving input-output pairs. This paper breaks new ground by introducing the implementation and evaluation of renowned encoder-decoder architectures, exclusively pre-trained on Spanish corpora. Specifically, we present Spanish versions of BART, T5, and BERT2BERT-style models and subject them to a comprehensive assessment across a diverse range of sequence-to-sequence tasks, spanning summarization, rephrasing, and generative question answering. Our findings underscore the competitive performance of all models, with BART and T5 emerging as top performers across all evaluated tasks. As an additional contribution, we have made all models publicly available to the research community, fostering future exploration and development in Spanish language processing.
</details>
<details>
<summary>摘要</summary>
近年来，大规模的预训练语言模型技术得到了广泛应用，特别是针对英语以外语言的研发。虽然西班牙语模型，包括BERT、RoBERTa和GPT，在自然语言理解和生成方面具有卓越表现，但是还缺乏适用于序列-序列任务的encoder-decoder模型。这篇论文创新地介绍了西班牙语encoder-decoder模型的实现和评估，具体来说是在西班牙语 corpus 上预训练的 BART、T5 和 BERT2BERT 样式模型。我们对这些模型进行了广泛的评估，包括概要、重新写和生成问答等序列-序列任务，我们的发现表明所有模型都具有竞争力，BART 和 T5 在所有评估任务中表现出色。此外，我们将所有模型公开发布给研究社区，以促进未来的探索和发展在西班牙语处理领域。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Multi-Agent-Reinforcement-Learning-for-Air-Combat-Maneuvering"><a href="#Hierarchical-Multi-Agent-Reinforcement-Learning-for-Air-Combat-Maneuvering" class="headerlink" title="Hierarchical Multi-Agent Reinforcement Learning for Air Combat Maneuvering"></a>Hierarchical Multi-Agent Reinforcement Learning for Air Combat Maneuvering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11247">http://arxiv.org/abs/2309.11247</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/IDSIA/marl">https://github.com/IDSIA/marl</a></li>
<li>paper_authors: Ardian Selmonaj, Oleg Szehr, Giacomo Del Rio, Alessandro Antonucci, Adrian Schneider, Michael Rüegsegger</li>
<li>for: 这个研究旨在提供一个多代理人问题决策框架，以实现精确的空中作战决策。</li>
<li>methods: 本研究使用多代理人问题决策框架，分为两个阶层：低层为单位对战斗控制的细节政策，高层为高级指挥官策略，对于任务目标进行决策。低层策略透过增加复杂训练enario和联赛自游戏的方式进行训练，而高层策略则透过已经预训练的低层策略进行训练。</li>
<li>results: 这个框架的实验验证表明，这种多代理人问题决策框架具有优化空中作战决策的功能。<details>
<summary>Abstract</summary>
The application of artificial intelligence to simulate air-to-air combat scenarios is attracting increasing attention. To date the high-dimensional state and action spaces, the high complexity of situation information (such as imperfect and filtered information, stochasticity, incomplete knowledge about mission targets) and the nonlinear flight dynamics pose significant challenges for accurate air combat decision-making. These challenges are exacerbated when multiple heterogeneous agents are involved. We propose a hierarchical multi-agent reinforcement learning framework for air-to-air combat with multiple heterogeneous agents. In our framework, the decision-making process is divided into two stages of abstraction, where heterogeneous low-level policies control the action of individual units, and a high-level commander policy issues macro commands given the overall mission targets. Low-level policies are trained for accurate unit combat control. Their training is organized in a learning curriculum with increasingly complex training scenarios and league-based self-play. The commander policy is trained on mission targets given pre-trained low-level policies. The empirical validation advocates the advantages of our design choices.
</details>
<details>
<summary>摘要</summary>
application of artificial intelligence to simulate air-to-air combat scenarios is attracting increasing attention. To date, the high-dimensional state and action spaces, the high complexity of situation information (such as imperfect and filtered information, stochasticity, incomplete knowledge about mission targets) and the nonlinear flight dynamics pose significant challenges for accurate air combat decision-making. These challenges are exacerbated when multiple heterogeneous agents are involved. We propose a hierarchical multi-agent reinforcement learning framework for air-to-air combat with multiple heterogeneous agents. In our framework, the decision-making process is divided into two stages of abstraction, where heterogeneous low-level policies control the action of individual units, and a high-level commander policy issues macro commands given the overall mission targets. Low-level policies are trained for accurate unit combat control. Their training is organized in a learning curriculum with increasingly complex training scenarios and league-based self-play. The commander policy is trained on mission targets given pre-trained low-level policies. The empirical validation advocates the advantages of our design choices.Here's the translation in Traditional Chinese:运用人工智能模拟空中武器战场情况的应用正在吸引越来越多的注意。到目前为止，高维度的状态和动作空间，高复杂的情况信息（如受损和范围信息、数据满意度、任务目标知识不完整）以及非线性的飞行动力学都对于精准的空中战斗决策带来巨大挑战。当多个不同性的代理人参与时，这些挑战更加严重。我们提出了一个层次多代理人学习框架，用于空中战斗多个不同性代理人。在我们的框架中，决策过程分为两个层次的抽象，其中专门的低层策略控制个别单位的行动，而高层策略根据全局任务目标发出大规模的指令。低层策略在增加复杂的训练enario和联赛自游中进行训练。高层策略则是根据已经预训的低层策略进行训练。实际验证表明了我们的设计选择的优点。
</details></li>
</ul>
<hr>
<h2 id="Colour-Passing-Revisited-Lifted-Model-Construction-with-Commutative-Factors"><a href="#Colour-Passing-Revisited-Lifted-Model-Construction-with-Commutative-Factors" class="headerlink" title="Colour Passing Revisited: Lifted Model Construction with Commutative Factors"></a>Colour Passing Revisited: Lifted Model Construction with Commutative Factors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11236">http://arxiv.org/abs/2309.11236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Malte Luttermann, Tanya Braun, Ralf Möller, Marcel Gehrke</li>
<li>for: 这篇论文目的是提出一种基于Symmetries的升级概率推理方法，以实现可靠地概率推理。</li>
<li>methods: 该方法使用了colour passing算法，但是现有的colour passing算法受限于特定的推理算法，并且忽略了因素的 коммутатив性。本文提出了一种基于逻辑变量的修改版colour passing算法，可以独立于特定的推理算法来构建升级表示，同时充分利用因素的 коммутатив性。</li>
<li>results: 对比于现有的colour passing算法，本文的方法可以更好地检测Symmetries，从而实现更高的压缩率和更快的在线查询速度。<details>
<summary>Abstract</summary>
Lifted probabilistic inference exploits symmetries in a probabilistic model to allow for tractable probabilistic inference with respect to domain sizes. To apply lifted inference, a lifted representation has to be obtained, and to do so, the so-called colour passing algorithm is the state of the art. The colour passing algorithm, however, is bound to a specific inference algorithm and we found that it ignores commutativity of factors while constructing a lifted representation. We contribute a modified version of the colour passing algorithm that uses logical variables to construct a lifted representation independent of a specific inference algorithm while at the same time exploiting commutativity of factors during an offline-step. Our proposed algorithm efficiently detects more symmetries than the state of the art and thereby drastically increases compression, yielding significantly faster online query times for probabilistic inference when the resulting model is applied.
</details>
<details>
<summary>摘要</summary>
增强概率推理利用模型中的对称性来实现可行的概率推理，具体来说是通过增强的可行推理算法来实现。为了应用增强推理，需要首先获得增强表示，而现有的颜色传递算法是state of the art的解决方案。然而，这个算法受到特定推理算法的限制，而且忽略了因素的 коммутатив性。我们提出了一种改进的颜色传递算法，使用逻辑变量来构建独立于特定推理算法的增强表示，同时在Offline阶段利用因素的 commutativity 来提高压缩率。我们的提议算法可以更好地检测模型中的对称性，从而导致更快的在线查询时间。
</details></li>
</ul>
<hr>
<h2 id="ChatGPT-4-as-a-Tool-for-Reviewing-Academic-Books-in-Spanish"><a href="#ChatGPT-4-as-a-Tool-for-Reviewing-Academic-Books-in-Spanish" class="headerlink" title="ChatGPT-4 as a Tool for Reviewing Academic Books in Spanish"></a>ChatGPT-4 as a Tool for Reviewing Academic Books in Spanish</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11231">http://arxiv.org/abs/2309.11231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonnathan Berrezueta-Guzman, Laura Malache-Silva, Stephan Krusche</li>
<li>For: This study evaluates the potential of ChatGPT-4 as an editing tool for Spanish literary and academic books.* Methods: The study analyzes the features and capabilities of ChatGPT-4 in terms of grammatical correction, stylistic coherence, and linguistic enrichment of texts in Spanish.* Results: ChatGPT-4 is capable of making grammatical and orthographic corrections with high accuracy and in a very short time, but faces challenges in areas such as context sensitivity and interaction with visual content. Collaboration between ChatGPT-4 and human reviewers and editors is a promising strategy for improving efficiency without compromising quality.Here are the three points in Simplified Chinese text:* For: 这项研究评估了OpenAI开发的ChatGPT-4语言模型是否能够用于西班牙文学和学术书籍的编辑。* Methods: 研究分析了ChatGPT-4模型在西班牙文 grammar修正、风格一致性和语言丰富性方面的功能和能力。* Results: ChatGPT-4能够快速和准确地进行语法和拼写修正，但在上下文敏感性和图表和表格交互方面存在挑战。人类编辑和评审者和ChatGPT-4 collaboration 可能是提高效率而无需降低质量的有效策略。<details>
<summary>Abstract</summary>
This study evaluates the potential of ChatGPT-4, an artificial intelligence language model developed by OpenAI, as an editing tool for Spanish literary and academic books. The need for efficient and accessible reviewing and editing processes in the publishing industry has driven the search for automated solutions. ChatGPT-4, being one of the most advanced language models, offers notable capabilities in text comprehension and generation. In this study, the features and capabilities of ChatGPT-4 are analyzed in terms of grammatical correction, stylistic coherence, and linguistic enrichment of texts in Spanish. Tests were conducted with 100 literary and academic texts, where the edits made by ChatGPT-4 were compared to those made by expert human reviewers and editors. The results show that while ChatGPT-4 is capable of making grammatical and orthographic corrections with high accuracy and in a very short time, it still faces challenges in areas such as context sensitivity, bibliometric analysis, deep contextual understanding, and interaction with visual content like graphs and tables. However, it is observed that collaboration between ChatGPT-4 and human reviewers and editors can be a promising strategy for improving efficiency without compromising quality. Furthermore, the authors consider that ChatGPT-4 represents a valuable tool in the editing process, but its use should be complementary to the work of human editors to ensure high-caliber editing in Spanish literary and academic books.
</details>
<details>
<summary>摘要</summary>
Tests were conducted on 100 literary and academic texts, comparing the edits made by ChatGPT-4 to those made by expert human reviewers and editors. The results show that ChatGPT-4 is capable of making grammatical and orthographic corrections with high accuracy and in a very short time. However, it still struggles with context sensitivity, bibliometric analysis, deep contextual understanding, and interaction with visual content like graphs and tables.Despite these limitations, collaboration between ChatGPT-4 and human reviewers and editors is a promising strategy for improving efficiency without compromising quality. The authors conclude that ChatGPT-4 represents a valuable tool in the editing process, but its use should be complementary to the work of human editors to ensure high-caliber editing in Spanish literary and academic books.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Diversity-in-Online-Interactions"><a href="#Leveraging-Diversity-in-Online-Interactions" class="headerlink" title="Leveraging Diversity in Online Interactions"></a>Leveraging Diversity in Online Interactions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11224">http://arxiv.org/abs/2309.11224</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nardine Osman, Bruno Rosell i Gui, Carles Sierra</li>
<li>for: 本研究旨在通过在线连接人们，帮助他们解决日常问题。</li>
<li>methods: 本研究使用了声明性规范来mediate在线交互，特别是在连接人们时利用多样性。</li>
<li>results: 在不同的大学站点上进行的试验显示，选择的profile多样性得到了相对成功，并得到了用户满意的评价。<details>
<summary>Abstract</summary>
This paper addresses the issue of connecting people online to help them find support with their day-to-day problems. We make use of declarative norms for mediating online interactions, and we specifically focus on the issue of leveraging diversity when connecting people. We run pilots at different university sites, and the results show relative success in the diversity of the selected profiles, backed by high user satisfaction.
</details>
<details>
<summary>摘要</summary>
这篇论文关注在线连接人们，以帮助他们解决日常问题。我们利用声明性规范来调控在线交互，特别是利用多样性连接人们。我们在不同的大学站点进行了试点，结果表明在选择的profile中的多样性得到了相对成功，并得到了用户满意的评价。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Retrieve-Rewrite-Answer-A-KG-to-Text-Enhanced-LLMs-Framework-for-Knowledge-Graph-Question-Answering"><a href="#Retrieve-Rewrite-Answer-A-KG-to-Text-Enhanced-LLMs-Framework-for-Knowledge-Graph-Question-Answering" class="headerlink" title="Retrieve-Rewrite-Answer: A KG-to-Text Enhanced LLMs Framework for Knowledge Graph Question Answering"></a>Retrieve-Rewrite-Answer: A KG-to-Text Enhanced LLMs Framework for Knowledge Graph Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11206">http://arxiv.org/abs/2309.11206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yike Wu, Nan Hu, Sheng Bi, Guilin Qi, Jie Ren, Anhuan Xie, Wei Song</li>
<li>for: 提高知识GraphQuestionAnswering（KGQA）任务的表现，解决rich world knowledge的问题。</li>
<li>methods: 提出了一种Answer-sensitive KG-to-Text方法，将知识Graph（KG）知识转化成文本表示，以便与语言模型（LLMs）集成。</li>
<li>results: 实验表明，提出的KG-to-Text增强的LLMs框架在KGQA任务上的答案准确率和知识声明的有用性都高于之前的KG-加强LLMs方法。<details>
<summary>Abstract</summary>
Despite their competitive performance on knowledge-intensive tasks, large language models (LLMs) still have limitations in memorizing all world knowledge especially long tail knowledge. In this paper, we study the KG-augmented language model approach for solving the knowledge graph question answering (KGQA) task that requires rich world knowledge. Existing work has shown that retrieving KG knowledge to enhance LLMs prompting can significantly improve LLMs performance in KGQA. However, their approaches lack a well-formed verbalization of KG knowledge, i.e., they ignore the gap between KG representations and textual representations. To this end, we propose an answer-sensitive KG-to-Text approach that can transform KG knowledge into well-textualized statements most informative for KGQA. Based on this approach, we propose a KG-to-Text enhanced LLMs framework for solving the KGQA task. Experiments on several KGQA benchmarks show that the proposed KG-to-Text augmented LLMs approach outperforms previous KG-augmented LLMs approaches regarding answer accuracy and usefulness of knowledge statements.
</details>
<details>
<summary>摘要</summary>
尽管大语言模型（LLMs）在知识密集任务上表现竞争性强，但它们仍有吸收全球知识的限制，特别是长尾知识。在这篇论文中，我们研究了将知识图（KG）扩展到语言模型（LMs）的方法，以解决需要丰富世界知识的问题 answering（KGQA）任务。现有的研究表明，使用KG知识来提高LLMs的提问可以显著提高LLMs在KGQA任务上的表现。然而，现有的方法忽略了KG表示和文本表示之间的差异，即KG知识的形式化表述。为了解决这问题，我们提出了一种答案相关的KG知识转换方法，可以将KG知识转换成最有用的文本表述，以便于KGQA任务。基于这种方法，我们提出了一种增强LLMs的KG-to-Text框架，用于解决KGQA任务。实验表明，我们的方法在多个KGQA bencmark上显著提高了答案准确率和知识声明的用用性。
</details></li>
</ul>
<hr>
<h2 id="Using-Artificial-Intelligence-for-the-Automation-of-Knitting-Patterns"><a href="#Using-Artificial-Intelligence-for-the-Automation-of-Knitting-Patterns" class="headerlink" title="Using Artificial Intelligence for the Automation of Knitting Patterns"></a>Using Artificial Intelligence for the Automation of Knitting Patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11202">http://arxiv.org/abs/2309.11202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Uduak Uboh</li>
<li>for: 这个研究是为了判断使用自动化系统来分类锦纹。</li>
<li>methods: 该研究使用了数据扩展和传输学习技术，采用了inception ResNet-V2作为特征提取和分类算法。</li>
<li>results: 模型的评估结果显示了高的模型精度、精度、回归率和F1分数，而且大多数类的AUC分数在(0.7-0.9)的范围内。<details>
<summary>Abstract</summary>
Knitting patterns are a crucial component in the creation and design of knitted materials. Traditionally, these patterns were taught informally, but thanks to advancements in technology, anyone interested in knitting can use the patterns as a guide to start knitting. Perhaps because knitting is mostly a hobby, with the exception of industrial manufacturing utilising specialised knitting machines, the use of Al in knitting is less widespread than its application in other fields. However, it is important to determine whether knitted pattern classification using an automated system is viable. In order to recognise and classify knitting patterns. Using data augmentation and a transfer learning technique, this study proposes a deep learning model. The Inception ResNet-V2 is the main feature extraction and classification algorithm used in the model. Metrics like accuracy, logarithmic loss, F1-score, precision, and recall score were used to evaluate the model. The model evaluation's findings demonstrate high model accuracy, precision, recall, and F1 score. In addition, the AUC score for majority of the classes was in the range (0.7-0.9). A comparative analysis was done using other pretrained models and a ResNet-50 model with transfer learning and the proposed model evaluation results surpassed all others. The major limitation for this project is time, as with more time, there might have been better accuracy over a larger number of epochs.
</details>
<details>
<summary>摘要</summary>
针脊图案是创作和设计针脊材料的关键组件。在过去，这些图案通常是通过口述传授的，但现在随着技术的进步，任何感兴趣的人都可以使用这些图案作为指南开始针脊。由于针脊主要是一项兴趣爱好，除了特殊针脊机器在工业生产中使用外，使用人工智能（AI）在针脊中的应用范围相对较少。然而，是否可以使用自动化系统来分类针脊图案是一个重要的问题。为了识别和分类针脊图案，这个研究提出了一个深度学习模型。使用Inception ResNet-V2算法作为主要特征提取和分类算法。对模型的评估结果，发现模型的准确率、精度、准确率、和F1分数均达到了高水平。此外，大多数类别的AUC分数都在(0.7-0.9)之间。与其他预训练模型和ResNet-50模型进行比较分析后，这个研究的评估结果超过了其他所有。该项目的主要限制是时间，如果有更多的时间，可能会在更多的轮次上得到更高的准确率。
</details></li>
</ul>
<hr>
<h2 id="When-to-Trust-AI-Advances-and-Challenges-for-Certification-of-Neural-Networks"><a href="#When-to-Trust-AI-Advances-and-Challenges-for-Certification-of-Neural-Networks" class="headerlink" title="When to Trust AI: Advances and Challenges for Certification of Neural Networks"></a>When to Trust AI: Advances and Challenges for Certification of Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11196">http://arxiv.org/abs/2309.11196</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marta Kwiatkowska, Xiyue Zhang</li>
<li>for: 本文旨在探讨如何确保人工智能（AI）决策的安全性，以便在实际应用中使用AI技术。</li>
<li>methods: 本文使用了证明和解释性技术来确保AI决策的安全性。</li>
<li>results: 本文提出了未来的挑战和研究方向，以确保AI决策的安全性和可靠性。<details>
<summary>Abstract</summary>
Artificial intelligence (AI) has been advancing at a fast pace and it is now poised for deployment in a wide range of applications, such as autonomous systems, medical diagnosis and natural language processing. Early adoption of AI technology for real-world applications has not been without problems, particularly for neural networks, which may be unstable and susceptible to adversarial examples. In the longer term, appropriate safety assurance techniques need to be developed to reduce potential harm due to avoidable system failures and ensure trustworthiness. Focusing on certification and explainability, this paper provides an overview of techniques that have been developed to ensure safety of AI decisions and discusses future challenges.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）在过去几年中得到了快速发展，现在它已经准备好在各种应用中使用，如自主系统、医疗诊断和自然语言处理。虽然在实际应用中早期采用AI技术有一些问题，特别是神经网络可能存在不稳定性和可靠性问题，以及可能受到敌意的示例的影响。在长期来看，我们需要开发适当的安全保障技术，以降低可预防的系统失效的可能性，并确保AI决策的可靠性。本文将关注证书和解释性，提供了安全AI决策的技术ensure的概述，并讨论未来的挑战。
</details></li>
</ul>
<hr>
<h2 id="Long-tail-Augmented-Graph-Contrastive-Learning-for-Recommendation"><a href="#Long-tail-Augmented-Graph-Contrastive-Learning-for-Recommendation" class="headerlink" title="Long-tail Augmented Graph Contrastive Learning for Recommendation"></a>Long-tail Augmented Graph Contrastive Learning for Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11177">http://arxiv.org/abs/2309.11177</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/im0qianqian/LAGCL">https://github.com/im0qianqian/LAGCL</a></li>
<li>paper_authors: Qian Zhao, Zhengwei Wu, Zhiqiang Zhang, Jun Zhou</li>
<li>for: 提高推荐系统中Graph Convolutional Networks (GCNs)的性能， Address the data sparsity issue in real-world scenarios.</li>
<li>methods: 使用contrastive learning方法，并 introduce learnable long-tail augmentation approach to enhance tail nodes， generate contrastive views based on the resulting augmented graph.</li>
<li>results: 对三个 benchmark dataset进行了extensive experiments，demonstrate the significant improvement in performance of our model over the state-of-the-arts，further analyses demonstrate the uniformity of learned representations and the superiority of LAGCL on long-tail performance.<details>
<summary>Abstract</summary>
Graph Convolutional Networks (GCNs) has demonstrated promising results for recommender systems, as they can effectively leverage high-order relationship. However, these methods usually encounter data sparsity issue in real-world scenarios. To address this issue, GCN-based recommendation methods employ contrastive learning to introduce self-supervised signals. Despite their effectiveness, these methods lack consideration of the significant degree disparity between head and tail nodes. This can lead to non-uniform representation distribution, which is a crucial factor for the performance of contrastive learning methods. To tackle the above issue, we propose a novel Long-tail Augmented Graph Contrastive Learning (LAGCL) method for recommendation. Specifically, we introduce a learnable long-tail augmentation approach to enhance tail nodes by supplementing predicted neighbor information, and generate contrastive views based on the resulting augmented graph. To make the data augmentation schema learnable, we design an auto drop module to generate pseudo-tail nodes from head nodes and a knowledge transfer module to reconstruct the head nodes from pseudo-tail nodes. Additionally, we employ generative adversarial networks to ensure that the distribution of the generated tail/head nodes matches that of the original tail/head nodes. Extensive experiments conducted on three benchmark datasets demonstrate the significant improvement in performance of our model over the state-of-the-arts. Further analyses demonstrate the uniformity of learned representations and the superiority of LAGCL on long-tail performance. Code is publicly available at https://github.com/im0qianqian/LAGCL
</details>
<details>
<summary>摘要</summary>
图像 convolutional networks (GCNs) 在推荐系统中表现出色，可以有效利用高阶关系。然而，这些方法通常在实际场景中遇到数据稀缺问题。为解决这个问题，GCN 基于的推荐方法使用对照学习引入自我超vised信号。尽管它们有效，但是它们忽视了主要度差的问题，这可能导致非均衡的表示分布，这是对对照学习方法的表现非常重要的因素。为解决这个问题，我们提出了一种长尾增强图像对照学习（LAGCL）方法。具体来说，我们引入可学习的长尾增强approach，通过预测邻居信息来增强尾节点，并基于所得到的扩展图像生成对照视图。为使数据增强 schema 学习可能，我们设计了自动Drop模块，将头节点转化为 pseudo-tail 节点，并设计了知识传递模块，将 pseudo-tail 节点还原为头节点。此外，我们使用生成对抗网络，确保生成的尾/头节点的分布与原始的尾/头节点的分布一致。我们在三个标准数据集上进行了广泛的实验，并证明了我们的模型在现状上的显著改进。进一步的分析也表明了我们学习的表示的均匀性和我们对长尾性能的优势。代码可以在https://github.com/im0qianqian/LAGCL 中找到。
</details></li>
</ul>
<hr>
<h2 id="Are-Large-Language-Models-Really-Robust-to-Word-Level-Perturbations"><a href="#Are-Large-Language-Models-Really-Robust-to-Word-Level-Perturbations" class="headerlink" title="Are Large Language Models Really Robust to Word-Level Perturbations?"></a>Are Large Language Models Really Robust to Word-Level Perturbations?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11166">http://arxiv.org/abs/2309.11166</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Harry-mic/TREval">https://github.com/Harry-mic/TREval</a></li>
<li>paper_authors: Haoyu Wang, Guozheng Ma, Cong Yu, Ning Gui, Linrui Zhang, Zhiqi Huang, Suwei Ma, Yongzhe Chang, Sen Zhang, Li Shen, Xueqian Wang, Peilin Zhao, Dacheng Tao</li>
<li>for: 本研究旨在提供一种用于评估大语言模型（LLM）的有用性和可靠性的新方法。</li>
<li>methods: 本研究提出了一种基于预训练奖励模型的评估方法，称为TREval，用于评估LLM的可靠性，特别是在面对更加困难的开放问题时。</li>
<li>results: 实验结果表明，TREval可以准确地评估LLM的可靠性，并且发现LLM经常受到单词水平的干扰，这种干扰在日常语言使用中很常见。另外，研究发现，在进行练习和强化训练后，LLM的可靠性往往会下降。<details>
<summary>Abstract</summary>
The swift advancement in the scale and capabilities of Large Language Models (LLMs) positions them as promising tools for a variety of downstream tasks. In addition to the pursuit of better performance and the avoidance of violent feedback on a certain prompt, to ensure the responsibility of the LLM, much attention is drawn to the robustness of LLMs. However, existing evaluation methods mostly rely on traditional question answering datasets with predefined supervised labels, which do not align with the superior generation capabilities of contemporary LLMs. To address this issue, we propose a novel rational evaluation approach that leverages pre-trained reward models as diagnostic tools to evaluate the robustness of LLMs, which we refer to as the Reward Model for Reasonable Robustness Evaluation (TREvaL). Our extensive empirical experiments have demonstrated that TREval provides an accurate method for evaluating the robustness of an LLM, especially when faced with more challenging open questions. Furthermore, our results demonstrate that LLMs frequently exhibit vulnerability to word-level perturbations, which are commonplace in daily language usage. Notably, we were surprised to discover that robustness tends to decrease as fine-tuning (SFT and RLHF) is conducted. The code of TREval is available in https://github.com/Harry-mic/TREval.
</details>
<details>
<summary>摘要</summary>
Large Language Models (LLMs) 的快速发展和能力提高，使其成为许多下游任务的优秀工具。除了提高性能和避免某些提示导致的暴力反馈外，为了确保 LLM 的责任，也引起了一些关注。现有的评估方法主要基于已经定义的传统问答数据集，这些数据集并不符合当代 LLM 的优秀生成能力。为解决这个问题，我们提出了一种新的合理评估方法，利用预训练的奖励模型作为诊断工具来评估 LLM 的 robustness，我们称之为 TREvaL。我们的广泛的实验证明了 TREval 能够准确地评估 LLM 的 robustness，特别是面对更加困难的开放问题。此外，我们的结果表明，LLM  часто会受到单词水平的扰动，这些扰动在日常语言使用中很常见。意外地，我们发现，在 fine-tuning (SFT 和 RLHF) 过程中，LLM 的 Robustness 往往减退。TREval 的代码可以在 GitHub 上找到：https://github.com/Harry-mic/TREval。
</details></li>
</ul>
<hr>
<h2 id="ProtoExplorer-Interpretable-Forensic-Analysis-of-Deepfake-Videos-using-Prototype-Exploration-and-Refinement"><a href="#ProtoExplorer-Interpretable-Forensic-Analysis-of-Deepfake-Videos-using-Prototype-Exploration-and-Refinement" class="headerlink" title="ProtoExplorer: Interpretable Forensic Analysis of Deepfake Videos using Prototype Exploration and Refinement"></a>ProtoExplorer: Interpretable Forensic Analysis of Deepfake Videos using Prototype Exploration and Refinement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11155">http://arxiv.org/abs/2309.11155</a></li>
<li>repo_url: None</li>
<li>paper_authors: Merel de Leeuw den Bouter, Javier Lloret Pardo, Zeno Geradts, Marcel Worring</li>
<li>for: 这个研究旨在提高深度学习模型的可读性，尤其是在高度竞争的应用场景下。</li>
<li>methods: 这篇文章提出了一个可视化分析过程模型，并基于这个模型提出了一个名为ProtoExplorer的可视化分析系统，用于探索和修改基于原型的伪动态检测模型。</li>
<li>results: 这篇文章透过对实际应用场景进行评估，确认了这个方法的可行性和有效性。<details>
<summary>Abstract</summary>
In high-stakes settings, Machine Learning models that can provide predictions that are interpretable for humans are crucial. This is even more true with the advent of complex deep learning based models with a huge number of tunable parameters. Recently, prototype-based methods have emerged as a promising approach to make deep learning interpretable. We particularly focus on the analysis of deepfake videos in a forensics context. Although prototype-based methods have been introduced for the detection of deepfake videos, their use in real-world scenarios still presents major challenges, in that prototypes tend to be overly similar and interpretability varies between prototypes. This paper proposes a Visual Analytics process model for prototype learning, and, based on this, presents ProtoExplorer, a Visual Analytics system for the exploration and refinement of prototype-based deepfake detection models. ProtoExplorer offers tools for visualizing and temporally filtering prototype-based predictions when working with video data. It disentangles the complexity of working with spatio-temporal prototypes, facilitating their visualization. It further enables the refinement of models by interactively deleting and replacing prototypes with the aim to achieve more interpretable and less biased predictions while preserving detection accuracy. The system was designed with forensic experts and evaluated in a number of rounds based on both open-ended think aloud evaluation and interviews. These sessions have confirmed the strength of our prototype based exploration of deepfake videos while they provided the feedback needed to continuously improve the system.
</details>
<details>
<summary>摘要</summary>
高度的场景中，可以提供人类可解释的机器学习模型是非常重要的。这种情况更加真实，特别是在复杂的深度学习模型中，其中有很多可调参数。最近，原型基方法在使得深度学习可解释方面表现出了扎实的抑制力。我们特别关注深度假影像在法医方面的分析。虽然原型基方法已经应用于深度假影像的检测，但在实际应用中仍然存在主要挑战，即原型往往相似，解释性 между原型异常不一致。这篇论文提出了一种可见分析过程模型，并基于这种模型提出了ProtoExplorer，一种可见分析系统，用于深度假影像检测模型的探索和细化。ProtoExplorer提供了视觉分析和视频数据中的时间滤波功能，可以识别和分析深度假影像。它还可以通过交互删除和替换原型来实现更加可解释和不偏执的预测，同时保持检测精度。系统针对法医专家进行了多轮评估，包括开放式思维回答评估和面试。这些评估过程确认了我们的原型基 explore深度假影像的优势，同时提供了需要不断改进系统的反馈。
</details></li>
</ul>
<hr>
<h2 id="CoT-BERT-Enhancing-Unsupervised-Sentence-Representation-through-Chain-of-Thought"><a href="#CoT-BERT-Enhancing-Unsupervised-Sentence-Representation-through-Chain-of-Thought" class="headerlink" title="CoT-BERT: Enhancing Unsupervised Sentence Representation through Chain-of-Thought"></a>CoT-BERT: Enhancing Unsupervised Sentence Representation through Chain-of-Thought</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11143">http://arxiv.org/abs/2309.11143</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ZBWpro/CoT-BERT">https://github.com/ZBWpro/CoT-BERT</a></li>
<li>paper_authors: Bowen Zhang, Kehua Chang, Chunping Li</li>
<li>for: 提高不supervised sentence representation learning的性能，尝试使用链条思维来解锁预训练模型中的潜在能力。</li>
<li>methods: 提出了一种两阶段方法，首先使用理解阶段对输入句子进行理解，然后使用摘要阶段对输入句子进行摘要，最后使用摘要阶段的输出作为输入句子的vector化表示。同时，对冲突学习损失函数和模板干扰技术进行精细调整，以提高提示工程的性能。</li>
<li>results: 对多个 robust baseline进行了严格的实验证明，发现CoT-BERT可以在不需要其他文本表示模型或外部数据库的情况下，与supervised sentence representation learning具有相同或更高的性能。<details>
<summary>Abstract</summary>
Unsupervised sentence representation learning aims to transform input sentences into fixed-length vectors enriched with intricate semantic information while obviating the reliance on labeled data. Recent progress within this field, propelled by contrastive learning and prompt engineering, has significantly bridged the gap between unsupervised and supervised strategies. Nonetheless, the potential utilization of Chain-of-Thought, remains largely untapped within this trajectory. To unlock latent capabilities within pre-trained models, such as BERT, we propose a two-stage approach for sentence representation: comprehension and summarization. Subsequently, the output of the latter phase is harnessed as the vectorized representation of the input sentence. For further performance enhancement, we meticulously refine both the contrastive learning loss function and the template denoising technique for prompt engineering. Rigorous experimentation substantiates our method, CoT-BERT, transcending a suite of robust baselines without necessitating other text representation models or external databases.
</details>
<details>
<summary>摘要</summary>
不监督句子表示学习目标是将输入句子转化为固定长度的向量，具有细致的 semantics信息，而不需要标注数据。在这个领域，最近的进展，受到对短文本检测和提取技术的影响，已经大幅度减少了不监督和监督方法之间的差距。然而，链式思维的潜在应用，在这个轨迹上仍然尚未得到充分利用。为了解锁预训练模型中的强化特性，我们提出了一种两阶段方法：理解和概要。然后，后一阶段的输出被用作输入句子的向量表示。为了进一步提高性能，我们仔细修改了对短文本检测和提取技术的权重，以及模板干扰技术。我们的方法，CoT-BERT，在一系列强大的基线上进行了严格的实验，并不需要其他文本表示模型或外部数据库。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Pseudo-Learning-for-Open-World-DeepFake-Attribution"><a href="#Contrastive-Pseudo-Learning-for-Open-World-DeepFake-Attribution" class="headerlink" title="Contrastive Pseudo Learning for Open-World DeepFake Attribution"></a>Contrastive Pseudo Learning for Open-World DeepFake Attribution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11132">http://arxiv.org/abs/2309.11132</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/TencentYoutuResearch/OpenWorld-DeepFakeAttribution">https://github.com/TencentYoutuResearch/OpenWorld-DeepFakeAttribution</a></li>
<li>paper_authors: Zhimin Sun, Shen Chen, Taiping Yao, Bangjie Yin, Ran Yi, Shouhong Ding, Lizhuang Ma</li>
<li>for: 评估深伪检测领域中匿名攻击的隐藏迹象，以推动相关前沿研究。</li>
<li>methods: 提出一个新的评估指标集合called Open-World DeepFake Attribution（OW-DFA），并提出一种基于对比学习的novel框架 named Contrastive Pseudo Learning（CPL）。</li>
<li>results: 经验表明，我们提出的方法在OW-DFA任务上具有优秀的表现，并且能够增强深伪检测领域的安全性。<details>
<summary>Abstract</summary>
The challenge in sourcing attribution for forgery faces has gained widespread attention due to the rapid development of generative techniques. While many recent works have taken essential steps on GAN-generated faces, more threatening attacks related to identity swapping or expression transferring are still overlooked. And the forgery traces hidden in unknown attacks from the open-world unlabeled faces still remain under-explored. To push the related frontier research, we introduce a new benchmark called Open-World DeepFake Attribution (OW-DFA), which aims to evaluate attribution performance against various types of fake faces under open-world scenarios. Meanwhile, we propose a novel framework named Contrastive Pseudo Learning (CPL) for the OW-DFA task through 1) introducing a Global-Local Voting module to guide the feature alignment of forged faces with different manipulated regions, 2) designing a Confidence-based Soft Pseudo-label strategy to mitigate the pseudo-noise caused by similar methods in unlabeled set. In addition, we extend the CPL framework with a multi-stage paradigm that leverages pre-train technique and iterative learning to further enhance traceability performance. Extensive experiments verify the superiority of our proposed method on the OW-DFA and also demonstrate the interpretability of deepfake attribution task and its impact on improving the security of deepfake detection area.
</details>
<details>
<summary>摘要</summary>
“对于伪造的挑战，随着生成技术的快速发展，已经受到了广泛的关注。然而，许多最近的研究仅对生成器生成的面部进行了重要的步骤，尚未充分处理隐藏在未知攻击中的伪造迹象。为了推进相关的前沿研究，我们提出了一个新的 bencmark 叫做 Open-World DeepFake Attribution（OW-DFA），旨在评估对不同类型的伪造面部进行权重评估。同时，我们提出了一个名为 Contrastive Pseudo Learning（CPL）的新框架，通过以下两个方法来解决问题：1）引入全球-本地投票模组，以帮助伪造面部的不同权重区域进行整合；2）设计一种基于信任的软定式标签策略，以减少 pseudo-noise 对不明文件集的影响。此外，我们将 CPL 框架扩展为多阶段模型，利用预训技术和迭代学习来进一步增强 traceability 性能。实验结果显示了我们的提案方法在 OW-DFA 中的超越性和深度伪造检测领域的解释性。”
</details></li>
</ul>
<hr>
<h2 id="Language-Oriented-Communication-with-Semantic-Coding-and-Knowledge-Distillation-for-Text-to-Image-Generation"><a href="#Language-Oriented-Communication-with-Semantic-Coding-and-Knowledge-Distillation-for-Text-to-Image-Generation" class="headerlink" title="Language-Oriented Communication with Semantic Coding and Knowledge Distillation for Text-to-Image Generation"></a>Language-Oriented Communication with Semantic Coding and Knowledge Distillation for Text-to-Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11127">http://arxiv.org/abs/2309.11127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyelin Nam, Jihong Park, Jinho Choi, Mehdi Bennis, Seong-Lyun Kim</li>
<li>for: 提出了一种语言响应式 semantic communication（LSC）框架，用于机器人与人类之间的语言交互。</li>
<li>methods: 提出了三种新算法：1）Semantic Source Coding（SSC），压缩文本提示中的主要词语，保持提示的语法结构和上下文；2）Semantic Channel Coding（SCC），使用长语言同义词代替主要词语，提高免错性；3）Semantic Knowledge Distillation（SKD），通过在学习Listener的语言风格上下文中进行启发式学习，生成适应Listener的提示。</li>
<li>results: 在进行文本生成到图像任务中，提议的方法可以实现更高的感知相似性，并降低通信频率，同时提高干扰通信频率下的Robustness。<details>
<summary>Abstract</summary>
By integrating recent advances in large language models (LLMs) and generative models into the emerging semantic communication (SC) paradigm, in this article we put forward to a novel framework of language-oriented semantic communication (LSC). In LSC, machines communicate using human language messages that can be interpreted and manipulated via natural language processing (NLP) techniques for SC efficiency. To demonstrate LSC's potential, we introduce three innovative algorithms: 1) semantic source coding (SSC) which compresses a text prompt into its key head words capturing the prompt's syntactic essence while maintaining their appearance order to keep the prompt's context; 2) semantic channel coding (SCC) that improves robustness against errors by substituting head words with their lenghthier synonyms; and 3) semantic knowledge distillation (SKD) that produces listener-customized prompts via in-context learning the listener's language style. In a communication task for progressive text-to-image generation, the proposed methods achieve higher perceptual similarities with fewer transmissions while enhancing robustness in noisy communication channels.
</details>
<details>
<summary>摘要</summary>
通过将最新的大语言模型（LLM）和生成模型与发展的语义通信（SC） paradigm结合起来，在本文我们提出了一种新的语言启发型通信（LSC）框架。在LSC中，机器通过使用人类语言消息进行通信，这些消息可以通过自然语言处理（NLP）技术进行解释和修改，以提高SC的效率。为了证明LSC的潜力，我们提出了三种新算法：1） semanticsource coding（SSC），它压缩文本提示到其主要头语言，保留提示的语法结构和上下文；2） semantics channel coding（SCC），它通过将主要头语言替换为其更长的同义词，提高了对错误的Robustness；3） semantics knowledge distillation（SKD），它通过在上下文学习收者的语言风格，生成适合收者的启发式文本。在一个进步文本到图像生成任务中，我们的提案方法可以实现更高的感知相似性，同时减少传输量，并在噪声通信频道中提高了Robustness。
</details></li>
</ul>
<hr>
<h2 id="Learning-Complete-Topology-Aware-Correlations-Between-Relations-for-Inductive-Link-Prediction"><a href="#Learning-Complete-Topology-Aware-Correlations-Between-Relations-for-Inductive-Link-Prediction" class="headerlink" title="Learning Complete Topology-Aware Correlations Between Relations for Inductive Link Prediction"></a>Learning Complete Topology-Aware Correlations Between Relations for Inductive Link Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11528">http://arxiv.org/abs/2309.11528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Wang, Hanzhu Chen, Qitan Lv, Zhihao Shi, Jiajun Chen, Huarui He, Hongtao Xie, Yongdong Zhang, Feng Wu</li>
<li>for: 提高知识图的完整性和可靠性，使其能够在无需知道实体的情况下预测链接关系。</li>
<li>methods: 提出了一种新的子图基于方法TACO，利用逻辑相关性between关系来模型图STRUCTURE。TACO方法包括 seven种topological pattern，并通过关系相关网络（RCN）来学习每种pattern的重要性。</li>
<li>results: 对比 existed state-of-the-art方法，TACO方法在预测链接关系任务中表现出了superior的性能。<details>
<summary>Abstract</summary>
Inductive link prediction -- where entities during training and inference stages can be different -- has shown great potential for completing evolving knowledge graphs in an entity-independent manner. Many popular methods mainly focus on modeling graph-level features, while the edge-level interactions -- especially the semantic correlations between relations -- have been less explored. However, we notice a desirable property of semantic correlations between relations is that they are inherently edge-level and entity-independent. This implies the great potential of the semantic correlations for the entity-independent inductive link prediction task. Inspired by this observation, we propose a novel subgraph-based method, namely TACO, to model Topology-Aware COrrelations between relations that are highly correlated to their topological structures within subgraphs. Specifically, we prove that semantic correlations between any two relations can be categorized into seven topological patterns, and then proposes Relational Correlation Network (RCN) to learn the importance of each pattern. To further exploit the potential of RCN, we propose Complete Common Neighbor induced subgraph that can effectively preserve complete topological patterns within the subgraph. Extensive experiments demonstrate that TACO effectively unifies the graph-level information and edge-level interactions to jointly perform reasoning, leading to a superior performance over existing state-of-the-art methods for the inductive link prediction task.
</details>
<details>
<summary>摘要</summary>
依"\induction link prediction" -- 在训练和推理阶段之间的实体可以不同 -- 已经展现出了完善 evolving knowledge graphs 的巨大潜力。许多受欢迎的方法主要关注图级特征，而图级交互 -- 特别是关系之间的semantic correlation -- 则得到了更少的关注。然而，我们注意到了semantic correlation between relations 的一个愉悦性质，即它们是自然的edge-level和实体独立的。这意味着semantic correlation between relations 具有潜在的很大潜力 для实体独立的 inductive link prediction 任务。针对这一观察，我们提出了一种新的子图基于方法，即 TACO，用于模型 topology-aware COrrelations between relations （TACO）。具体来说，我们证明了任意两个关系的semantic correlation可以被分类为七种 topological pattern，并提出了 Relational Correlation Network (RCN) 来学习每种pattern的重要性。为了更好地利用 RCn 的潜力，我们提出了 Complete Common Neighbor induced subgraph，可以有效地保留完整的 topological patterns within the subgraph。我们的实验表明，TACO 能够具有图级信息和边级交互的整合，以jointly perform reasoning，从而对 inductive link prediction 任务 дости得更高的性能。
</details></li>
</ul>
<hr>
<h2 id="TrueLearn-A-Python-Library-for-Personalised-Informational-Recommendations-with-Implicit-Feedback"><a href="#TrueLearn-A-Python-Library-for-Personalised-Informational-Recommendations-with-Implicit-Feedback" class="headerlink" title="TrueLearn: A Python Library for Personalised Informational Recommendations with (Implicit) Feedback"></a>TrueLearn: A Python Library for Personalised Informational Recommendations with (Implicit) Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11527">http://arxiv.org/abs/2309.11527</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxiang Qiu, Karim Djemili, Denis Elezi, Aaneel Shalman, María Pérez-Ortiz, Sahan Bulathwela</li>
<li>for: 这篇论文是为了介绍TrueLearn Python库，这是一个基于在线学习 bayesian模型的教育（或更广泛的信息）推荐系统的建构。</li>
<li>methods: 这个家族模型采用了”开放学习”概念，使用人类可理解的用户表示。为了提高可读性和让用户控制自己的模型，TrueLearn库还包含了不同的表示方式，可以帮助用户视觉化自己的学习者模型。</li>
<li>results: 论文附录了一个已经公布的隐式反馈教育数据集，并提供了评价指标来衡量模型的性能。TrueLearn库的广泛的文档和代码示例使得机器学习开发者和教育数据挖掘和学习分析专家可以很容易地使用这个库。<details>
<summary>Abstract</summary>
This work describes the TrueLearn Python library, which contains a family of online learning Bayesian models for building educational (or more generally, informational) recommendation systems. This family of models was designed following the "open learner" concept, using humanly-intuitive user representations. For the sake of interpretability and putting the user in control, the TrueLearn library also contains different representations to help end-users visualise the learner models, which may in the future facilitate user interaction with their own models. Together with the library, we include a previously publicly released implicit feedback educational dataset with evaluation metrics to measure the performance of the models. The extensive documentation and coding examples make the library highly accessible to both machine learning developers and educational data mining and learning analytic practitioners. The library and the support documentation with examples are available at https://truelearn.readthedocs.io/en/latest.
</details>
<details>
<summary>摘要</summary>
这个工作描述了TrueLearn Python库，该库包含一家在线学习 bayesian 模型，用于建立教育（或更广泛地说，信息）推荐系统。这家模型遵循“开放学习”概念，使用人类可理解的用户表示。为了提高可解性和让用户控制，TrueLearn 库还包含了不同的表示，帮助结束用户可视化学习者模型，以便未来与自己的模型进行交互。此外，我们还提供了在线学习教育数据集，以便评估模型的性能。TrueLearn 库的文档和代码示例使得机器学习开发者和教育数据挖掘和学习分析专业人士可以轻松地使用。库和支持文档，以及示例可以在 <https://truelearn.readthedocs.io/en/latest> 上获取。
</details></li>
</ul>
<hr>
<h2 id="AttentionMix-Data-augmentation-method-that-relies-on-BERT-attention-mechanism"><a href="#AttentionMix-Data-augmentation-method-that-relies-on-BERT-attention-mechanism" class="headerlink" title="AttentionMix: Data augmentation method that relies on BERT attention mechanism"></a>AttentionMix: Data augmentation method that relies on BERT attention mechanism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11104">http://arxiv.org/abs/2309.11104</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominik Lewy, Jacek Mańdziuk</li>
<li>for: 这 paper 是关于如何在自然语言处理（NLP）领域中使用混合方法进行数据增强的研究。</li>
<li>methods: 这 paper 使用了一种新的混合方法 called AttentionMix，它基于注意力机制。这种方法可以应用于任何注意力基于模型。</li>
<li>results: 在三个标准情感分类 dataset 上测试，AttentionMix 都超过了两种 Mixup 机制的参考方法以及vanilla BERT 方法。结果表明，注意力信息可以有效地用于 NLP 领域中的数据增强。<details>
<summary>Abstract</summary>
The Mixup method has proven to be a powerful data augmentation technique in Computer Vision, with many successors that perform image mixing in a guided manner. One of the interesting research directions is transferring the underlying Mixup idea to other domains, e.g. Natural Language Processing (NLP). Even though there already exist several methods that apply Mixup to textual data, there is still room for new, improved approaches. In this work, we introduce AttentionMix, a novel mixing method that relies on attention-based information. While the paper focuses on the BERT attention mechanism, the proposed approach can be applied to generally any attention-based model. AttentionMix is evaluated on 3 standard sentiment classification datasets and in all three cases outperforms two benchmark approaches that utilize Mixup mechanism, as well as the vanilla BERT method. The results confirm that the attention-based information can be effectively used for data augmentation in the NLP domain.
</details>
<details>
<summary>摘要</summary>
《混合方法》在计算机视觉领域已经证明是一种强大的数据增强技术，有许多后继者在指导下进行图像混合。一个有趣的研究方向是将基于混合的想法传递到其他领域，如自然语言处理（NLP）。虽然现有一些应用混合到文本数据的方法，但还是有很多空间 для新的、改进的方法。在这项工作中，我们介绍了一种新的混合方法，即关注混合（AttentionMix）。这种方法基于关注信息，而paper中关注BERT的注意机制。AttentionMix可以应用于任何关注基于模型。我们在3个标准情感分类dataset上进行评估，并在所有3个案例中超过了两个参考方法和vanilla BERT方法。结果表明，关注信息可以有效地用于NLP领域中的数据增强。
</details></li>
</ul>
<hr>
<h2 id="A-New-Interpretable-Neural-Network-Based-Rule-Model-for-Healthcare-Decision-Making"><a href="#A-New-Interpretable-Neural-Network-Based-Rule-Model-for-Healthcare-Decision-Making" class="headerlink" title="A New Interpretable Neural Network-Based Rule Model for Healthcare Decision Making"></a>A New Interpretable Neural Network-Based Rule Model for Healthcare Decision Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11101">http://arxiv.org/abs/2309.11101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrien Benamira, Tristan Guerand, Thomas Peyrin</li>
<li>for: 本研究旨在提出一种神经网络框架，即 $\textit{Truth Table rules}$（TT-rules），该框架结合神经网络的高性能和规则型模型的全面和准确解释性质。</li>
<li>methods: TT-rules 基于 $\textit{Truth Table nets}$（TTnet），一种初始为形式验证而开发的深度神经网络家族。通过从训练过程中提取全面和准确的规则集 $\mathcal{R}$，以便使得 TT-rules 模型能够具备全面和准确的解释性。</li>
<li>results: 我们对健康应用场景中的数据进行评估，并与现有的解释性方法进行比较。结果表明，TT-rules 能够达到与其他解释性方法相当或更高的性能，并且在大型表格数据集上进行适应也是可能的。特别是，TT-rules 成为了首个能够适应大型表格数据集，包括两个真实的 DNA 数据集，每个数据集具有超过 20K 的特征的解释性模型。<details>
<summary>Abstract</summary>
In healthcare applications, understanding how machine/deep learning models make decisions is crucial. In this study, we introduce a neural network framework, $\textit{Truth Table rules}$ (TT-rules), that combines the global and exact interpretability properties of rule-based models with the high performance of deep neural networks. TT-rules is built upon $\textit{Truth Table nets}$ (TTnet), a family of deep neural networks initially developed for formal verification. By extracting the necessary and sufficient rules $\mathcal{R}$ from the trained TTnet model (global interpretability) to yield the same output as the TTnet (exact interpretability), TT-rules effectively transforms the neural network into a rule-based model. This rule-based model supports binary classification, multi-label classification, and regression tasks for small to large tabular datasets. After outlining the framework, we evaluate TT-rules' performance on healthcare applications and compare it to state-of-the-art rule-based methods. Our results demonstrate that TT-rules achieves equal or higher performance compared to other interpretable methods. Notably, TT-rules presents the first accurate rule-based model capable of fitting large tabular datasets, including two real-life DNA datasets with over 20K features.
</details>
<details>
<summary>摘要</summary>
在医疗应用中，理解机器学习/深度学习模型的决策方法是非常重要的。在这项研究中，我们介绍了一种神经网络框架，称为“真实表格规则”（TT-rules），这种框架结合了神经网络的高性能和规则型模型的全面和准确解释性质。TT-rules基于一种名为“真实表格网络”（TTnet）的深度神经网络，该网络最初是为了正式验证而开发的。通过从训练过程中提取出神经网络模型中的必要和充分规则（global interpretability），并将这些规则转换成可以准确地预测神经网络输出的规则型模型（exact interpretability），TT-rules可以将神经网络转换成一种规则型模型。这种规则型模型支持二分类、多标签分类和回归任务，适用于小至大的表格数据集。在这项研究中，我们介绍了TT-rules的框架，并对其性能进行了健康应用的评估，并与当前的可解释方法进行了比较。我们的结果表明，TT-rules可以与其他可解释方法匹配或超越其性能。尤其是TT-rules是首个能够适用于大型表格数据集的准确规则型模型，包括两个实际的DNA数据集，每个数据集有超过20K的特征。
</details></li>
</ul>
<hr>
<h2 id="Likelihood-based-Sensor-Calibration-for-Expert-Supported-Distributed-Learning-Algorithms-in-IoT-Systems"><a href="#Likelihood-based-Sensor-Calibration-for-Expert-Supported-Distributed-Learning-Algorithms-in-IoT-Systems" class="headerlink" title="Likelihood-based Sensor Calibration for Expert-Supported Distributed Learning Algorithms in IoT Systems"></a>Likelihood-based Sensor Calibration for Expert-Supported Distributed Learning Algorithms in IoT Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11526">http://arxiv.org/abs/2309.11526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rüdiger Machhamer, Lejla Begic Fazlic, Eray Guven, David Junk, Gunes Karabulut Kurt, Stefan Naumann, Stephan Didas, Klaus-Uwe Gollmer, Ralph Bergmann, Ingo J. Timm, Guido Dartmann</li>
<li>for: 这篇论文主要是为了提高感知技术中数据测量的精度和效率。</li>
<li>methods: 本论文使用了估计斜射变换的方法，并利用专家知识进行改进。它还可以应用于软件校准、专家基于适应和联邦学习方法。</li>
<li>results: 实验和仿真数据都表明，这种解决方案可以提高测量数据的精度和效率。<details>
<summary>Abstract</summary>
An important task in the field of sensor technology is the efficient implementation of adaptation procedures of measurements from one sensor to another sensor of identical design. One idea is to use the estimation of an affine transformation between different systems, which can be improved by the knowledge of experts. This paper presents an improved solution from Glacier Research that was published back in 1973. It is shown that this solution can be adapted for software calibration of sensors, implementation of expert-based adaptation, and federated learning methods. We evaluate our research with simulations and also with real measured data of a multi-sensor board with 8 identical sensors. The results show an improvement for both the simulation and the experiments with real data.
</details>
<details>
<summary>摘要</summary>
在感测技术领域，一项重要任务是有效地实现感测器之间测量转换的方法。一种思路是使用估算投影变换，可以通过专家知识进行改进。这篇文章介绍了1973年由冰川研究所发表的改进解决方案。我们表明该解决方案可以适用于软件准确性检测、专家知识基于的调整和联邦学习方法。我们通过实验和真实测量数据来评估我们的研究。结果表明，优化后的方法可以提高仪器测量精度。Here's the translation in Traditional Chinese:在感测技术领域，一个重要任务是有效地实现感测器之间测量转换的方法。一种思路是使用估算投影变换，可以通过专家知识进行改进。这篇文章介绍了1973年由冰川研究所发表的改进解决方案。我们表明这个解决方案可以应用于软件准确性检测、专家知识基于的调整和联邦学习方法。我们通过实验和真实测量数据来评估我们的研究。结果表明，优化后的方法可以提高仪器测量精度。
</details></li>
</ul>
<hr>
<h2 id="Practical-Probabilistic-Model-based-Deep-Reinforcement-Learning-by-Integrating-Dropout-Uncertainty-and-Trajectory-Sampling"><a href="#Practical-Probabilistic-Model-based-Deep-Reinforcement-Learning-by-Integrating-Dropout-Uncertainty-and-Trajectory-Sampling" class="headerlink" title="Practical Probabilistic Model-based Deep Reinforcement Learning by Integrating Dropout Uncertainty and Trajectory Sampling"></a>Practical Probabilistic Model-based Deep Reinforcement Learning by Integrating Dropout Uncertainty and Trajectory Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11089">http://arxiv.org/abs/2309.11089</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mrjun123/DPETS">https://github.com/mrjun123/DPETS</a></li>
<li>paper_authors: Wenjun Huang, Yunduan Cui, Huiyun Li, Xinyu Wu</li>
<li>for: 这篇论文旨在解决现有的 probabilistic model-based reinforcement learning（MBRL）模型，它们基于神经网络，但它们的预测稳定性和准确性有限制。</li>
<li>methods: 该论文提出了一种新的方法，即dropout-based probabilistic ensembles with trajectory sampling（DPETS），它将Monte-Carlo dropout和 trajectory sampling结合在一起，以稳定地预测系统的不确定性。DPETS的损失函数设计用于更正神经网络的适应错误，以更准确地预测 probabilistic models。</li>
<li>results: 论文通过在多个 Mujoco  benchmark control任务和一个实际的 robot arm manipulation任务上进行评估，发现 DPETS 可以在更高的 sample efficiency 下达到更高的均返回值和快速吞吐量，同时超过了相关的 MBRL 方法。此外，DPETS 还可以在面临附加干扰和实际操作中表现出色。<details>
<summary>Abstract</summary>
This paper addresses the prediction stability, prediction accuracy and control capability of the current probabilistic model-based reinforcement learning (MBRL) built on neural networks. A novel approach dropout-based probabilistic ensembles with trajectory sampling (DPETS) is proposed where the system uncertainty is stably predicted by combining the Monte-Carlo dropout and trajectory sampling in one framework. Its loss function is designed to correct the fitting error of neural networks for more accurate prediction of probabilistic models. The state propagation in its policy is extended to filter the aleatoric uncertainty for superior control capability. Evaluated by several Mujoco benchmark control tasks under additional disturbances and one practical robot arm manipulation task, DPETS outperforms related MBRL approaches in both average return and convergence velocity while achieving superior performance than well-known model-free baselines with significant sample efficiency. The open source code of DPETS is available at https://github.com/mrjun123/DPETS.
</details>
<details>
<summary>摘要</summary>
In the evaluation, DPETS outperforms other MBRL approaches in both average return and convergence velocity on several Mujoco benchmark control tasks with additional disturbances and one practical robot arm manipulation task. It also achieves superior performance compared to well-known model-free baselines with significant sample efficiency. The open source code of DPETS is available on GitHub at https://github.com/mrjun123/DPETS.Translated into Simplified Chinese:这篇论文关注现有基于神经网络的概率模型学习（MBRL）方法的预测稳定性、预测准确性和控制能力。一种新的方法叫做 dropout-based 概率集合with trajectory sampling（DPETS）被提议，它将 Monte-Carlo dropout 和 trajectory sampling 集成到一个框架中，以稳定系统uncertainty的预测。loss函数设计用于更正神经网络的适应错误，以便更准确地预测概率模型。Policy也被扩展以筛选 aleatoric uncertainty，以提高控制能力。在评估中，DPETS 比其他 MBRL 方法在多个 Mujoco benchmark控制任务上（包括附加干扰）和一个实际的机械臂控制任务上表现出更高的平均返点和更快的连续速度，同时与许多已知的模型自由基eline表现出更好的性能，并且具有显著的样本效率。DPETS 的开源代码可以在 GitHub 上获取，地址为 <https://github.com/mrjun123/DPETS>。
</details></li>
</ul>
<hr>
<h2 id="Embed-Search-Align-DNA-Sequence-Alignment-using-Transformer-Models"><a href="#Embed-Search-Align-DNA-Sequence-Alignment-using-Transformer-Models" class="headerlink" title="Embed-Search-Align: DNA Sequence Alignment using Transformer Models"></a>Embed-Search-Align: DNA Sequence Alignment using Transformer Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11087">http://arxiv.org/abs/2309.11087</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pavan Holur, K. C. Enevoldsen, Lajoyce Mboning, Thalia Georgiou, Louis-S. Bouchard, Matteo Pellegrini, Vwani Roychowdhury</li>
<li>for: 该研究旨在开发一种基于Transformer架构的DNA序列对齐方法，以提高DNA序列对齐精度。</li>
<li>methods: 该方法使用了自动对齐精度进行自我超vised培训，并引入了DNAvector存储以实现全文搜索。</li>
<li>results: 该方法可以高度准确地对齐250个基因组中的DNA序列，并且在不同染色体和物种上进行了任务转移。<details>
<summary>Abstract</summary>
DNA sequence alignment involves assigning short DNA reads to the most probable locations on an extensive reference genome. This process is crucial for various genomic analyses, including variant calling, transcriptomics, and epigenomics. Conventional methods, refined over decades, tackle this challenge in two steps: genome indexing followed by efficient search to locate likely positions for given reads. Building on the success of Large Language Models (LLM) in encoding text into embeddings, where the distance metric captures semantic similarity, recent efforts have explored whether the same Transformer architecture can produce numerical representations for DNA sequences. Such models have shown early promise in tasks involving classification of short DNA sequences, such as the detection of coding vs non-coding regions, as well as the identification of enhancer and promoter sequences. Performance at sequence classification tasks does not, however, translate to sequence alignment, where it is necessary to conduct a genome-wide search to successfully align every read. We address this open problem by framing it as an Embed-Search-Align task. In this framework, a novel encoder model DNA-ESA generates representations of reads and fragments of the reference, which are projected into a shared vector space where the read-fragment distance is used as surrogate for alignment. In particular, DNA-ESA introduces: (1) Contrastive loss for self-supervised training of DNA sequence representations, facilitating rich sequence-level embeddings, and (2) a DNA vector store to enable search across fragments on a global scale. DNA-ESA is >97% accurate when aligning 250-length reads onto a human reference genome of 3 gigabases (single-haploid), far exceeds the performance of 6 recent DNA-Transformer model baselines and shows task transfer across chromosomes and species.
</details>
<details>
<summary>摘要</summary>
DNNA序列Alignment含义在将短DNNA读物 assigning 到参考基因组中最有可能的位置上。这个过程是生物学分析中的关键步骤，包括变异检测、转录组学和epigenomics。传统方法通过两步进行：基因组索引，然后是高效的搜索来找到给定读物的可能位置。基于大自然语言模型（LLM）在编码文本为嵌入中的成功，最近的努力是否是使用同样的Transformer架构生成DNNA序列的数字表示。这些模型在短DNNA序列分类任务中表现出了早期的 promise，例如分类 coding vs non-coding 区域以及激活器和激发器序列的识别。但是，性能在序列分类任务上不能直接转移到Alignment任务，因为需要进行全基因组搜索以成功地对每个读物进行Alignment。我们解决这个开放问题 by framing it as an Embed-Search-Align task。在这种框架中，一种新的编码器模型DNA-ESA生成了读物和参考基因组中的 фрагментов的表示，并将它们投射到一个共享的vector空间中，其中读物-фрагмент的距离作为Alignment的Surrogate。特别是，DNA-ESA引入了：（1）对DNNA序列表示进行自我超vised 训练，以获得丰富的序列水平嵌入，以及（2）DNNA vector store，以实现在全球范围内搜索多个 фрагментов。DNNA-ESA在对3 gigabases的人类参考基因组上Alignment 250个长度的读物时，准确率高于97%，大幅超过了6个最近的DNNA-Transformer模型基eline，并在 хромосомы和种类之间显示任务传递。
</details></li>
</ul>
<hr>
<h2 id="Weak-Supervision-for-Label-Efficient-Visual-Bug-Detection"><a href="#Weak-Supervision-for-Label-Efficient-Visual-Bug-Detection" class="headerlink" title="Weak Supervision for Label Efficient Visual Bug Detection"></a>Weak Supervision for Label Efficient Visual Bug Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11077">http://arxiv.org/abs/2309.11077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Farrukh Rahman</li>
<li>for: 本研究旨在提高视频游戏中的视觉质量，并 Addressing the challenge of traditional testing methods being limited by resources and unable to cover the wide range of potential bugs.</li>
<li>methods: 我们提出了一种新的方法，使用无标注游戏记录和域特定的扩充来生成数据集和自我标注目标，并在预训练或多任务设置中使用这些目标进行预训练。我们使用弱监督来扩大数据集，并实现了自主和互动式弱监督，通过不supervised clustering和&#x2F;或基于文本和几何提示的交互方式。</li>
<li>results: 我们在Giantmap游戏中测试了FPPC（首个玩家截割&#x2F;碰撞漏洞），发现我们的方法非常有效，超越了强监督基线，在实际、非常低频率、低数据量 régime中（0.336 $\rightarrow$ 0.550 F1分数）。只需5个标注的“好”示例（即0个漏洞），我们的自我标注目标就能够捕捉足够的信号，超越低标注监督设置。我们的方法可以在不同的视觉漏洞上进行应用，并且可以在视频游戏中拓展到更广泛的图像和视频任务。<details>
<summary>Abstract</summary>
As video games evolve into expansive, detailed worlds, visual quality becomes essential, yet increasingly challenging. Traditional testing methods, limited by resources, face difficulties in addressing the plethora of potential bugs. Machine learning offers scalable solutions; however, heavy reliance on large labeled datasets remains a constraint. Addressing this challenge, we propose a novel method, utilizing unlabeled gameplay and domain-specific augmentations to generate datasets & self-supervised objectives used during pre-training or multi-task settings for downstream visual bug detection. Our methodology uses weak-supervision to scale datasets for the crafted objectives and facilitates both autonomous and interactive weak-supervision, incorporating unsupervised clustering and/or an interactive approach based on text and geometric prompts. We demonstrate on first-person player clipping/collision bugs (FPPC) within the expansive Giantmap game world, that our approach is very effective, improving over a strong supervised baseline in a practical, very low-prevalence, low data regime (0.336 $\rightarrow$ 0.550 F1 score). With just 5 labeled "good" exemplars (i.e., 0 bugs), our self-supervised objective alone captures enough signal to outperform the low-labeled supervised settings. Building on large-pretrained vision models, our approach is adaptable across various visual bugs. Our results suggest applicability in curating datasets for broader image and video tasks within video games beyond visual bugs.
</details>
<details>
<summary>摘要</summary>
Traditional video game testing methods are limited by resources and have difficulty addressing the many potential bugs that exist. Machine learning offers scalable solutions, but relying on large labeled datasets is a challenge. To address this, we propose a new method that uses unlabeled gameplay and domain-specific augmentations to generate datasets and self-supervised objectives for pre-training or multi-task settings. Our method uses weak supervision to scale the datasets and can be used in both autonomous and interactive modes, incorporating unsupervised clustering and/or an interactive approach based on text and geometric prompts. We demonstrate the effectiveness of our approach on first-person player clipping/collision bugs within the Giantmap game world, achieving an F1 score of 0.550 in a practical, low-prevalence, low-data regime with just 5 labeled "good" exemplars. Our self-supervised objective captures enough signal to outperform low-labeled supervised settings, and our approach is adaptable to various visual bugs and can be applied to curating datasets for broader image and video tasks within video games.
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Tiling-A-Model-Agnostic-Adaptive-Scalable-and-Inference-Data-Centric-Approach-for-Efficient-and-Accurate-Small-Object-Detection"><a href="#Dynamic-Tiling-A-Model-Agnostic-Adaptive-Scalable-and-Inference-Data-Centric-Approach-for-Efficient-and-Accurate-Small-Object-Detection" class="headerlink" title="Dynamic Tiling: A Model-Agnostic, Adaptive, Scalable, and Inference-Data-Centric Approach for Efficient and Accurate Small Object Detection"></a>Dynamic Tiling: A Model-Agnostic, Adaptive, Scalable, and Inference-Data-Centric Approach for Efficient and Accurate Small Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11069">http://arxiv.org/abs/2309.11069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Son The Nguyen, Theja Tulabandhula, Duy Nguyen</li>
<li>for: 这篇论文主要是为了提出一种模型不偏的、可适应的、扩展性强的小对象检测方法，以提高对象检测的准确率和效率。</li>
<li>methods: 这篇论文使用了动态瓷纹法，即首先使用非重叠的瓷纹来确定初始检测结果，然后通过动态调整瓷纹的重叠率和瓷纹最小化器来解决分布在不同瓷纹之间的 Fragmented 对象，从而提高检测精度和降低计算开销。</li>
<li>results: 相比现有的模型不偏的均匀裁剪方法，Dynamic Tiling 方法在不同的对象大小和环境下都能够达到更高的检测精度和效率，并且不需要劳动的重新调整。此外，这种方法还可以在不同的操作环境下进行适应，以提高对象检测的可扩展性和灵活性。<details>
<summary>Abstract</summary>
We introduce Dynamic Tiling, a model-agnostic, adaptive, and scalable approach for small object detection, anchored in our inference-data-centric philosophy. Dynamic Tiling starts with non-overlapping tiles for initial detections and utilizes dynamic overlapping rates along with a tile minimizer. This dual approach effectively resolves fragmented objects, improves detection accuracy, and minimizes computational overhead by reducing the number of forward passes through the object detection model. Adaptable to a variety of operational environments, our method negates the need for laborious recalibration. Additionally, our large-small filtering mechanism boosts the detection quality across a range of object sizes. Overall, Dynamic Tiling outperforms existing model-agnostic uniform cropping methods, setting new benchmarks for efficiency and accuracy.
</details>
<details>
<summary>摘要</summary>
我团队介绍了一种名为动态瓷纹的模型无关、可适应、可扩展的方法，用于小物体检测。这种方法基于我们的推理数据中心的哲学，使用非 overlap 的瓷纹开始，然后采用动态重叠率和瓷纹最小化器。这种双重方法能够有效地解决分割物体，提高检测精度，并减少计算负担。我们的方法适用于多种操作环境，无需劳辑重新调整。此外，我们的大小筛选机制可以在不同的物体大小下提高检测质量。总之，动态瓷纹超过了现有的模型无关均匀割 методы，设置了新的效率和准确性的benchmark。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Relationship-between-LLM-Hallucinations-and-Prompt-Linguistic-Nuances-Readability-Formality-and-Concreteness"><a href="#Exploring-the-Relationship-between-LLM-Hallucinations-and-Prompt-Linguistic-Nuances-Readability-Formality-and-Concreteness" class="headerlink" title="Exploring the Relationship between LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality, and Concreteness"></a>Exploring the Relationship between LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality, and Concreteness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11064">http://arxiv.org/abs/2309.11064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vipula Rawte, Prachi Priya, S. M Towhidul Islam Tonmoy, S M Mehedi Zaman, Amit Sheth, Amitava Das</li>
<li>for:  investigate the influence of linguistic factors in prompts on the occurrence of LLM hallucinations</li>
<li>methods:  experimental study using prompts with varying levels of readability, formality, and concreteness</li>
<li>results:  prompts with greater formality and concreteness tend to result in reduced hallucinations, while the outcomes pertaining to readability are mixed.<details>
<summary>Abstract</summary>
As Large Language Models (LLMs) have advanced, they have brought forth new challenges, with one of the prominent issues being LLM hallucination. While various mitigation techniques are emerging to address hallucination, it is equally crucial to delve into its underlying causes. Consequently, in this preliminary exploratory investigation, we examine how linguistic factors in prompts, specifically readability, formality, and concreteness, influence the occurrence of hallucinations. Our experimental results suggest that prompts characterized by greater formality and concreteness tend to result in reduced hallucination. However, the outcomes pertaining to readability are somewhat inconclusive, showing a mixed pattern.
</details>
<details>
<summary>摘要</summary>
LLMs 的进步也带来了新的挑战，其中一个主要问题是 LLM 幻觉。虽然各种 mitigation 技术正在emerging，但是也非常重要探讨幻觉的深层原因。因此，在这项初步的探索性研究中，我们研究了提示中语言因素对幻觉的影响，特别是可读性、正式度和具体性。我们的实验结果表明，使用更正式和具体的提示可以减少幻觉，但是关于可读性的结果呈杂化的模式。
</details></li>
</ul>
<hr>
<h2 id="Design-of-Chain-of-Thought-in-Math-Problem-Solving"><a href="#Design-of-Chain-of-Thought-in-Math-Problem-Solving" class="headerlink" title="Design of Chain-of-Thought in Math Problem Solving"></a>Design of Chain-of-Thought in Math Problem Solving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11054">http://arxiv.org/abs/2309.11054</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lqtrung1998/mwp_cot_design">https://github.com/lqtrung1998/mwp_cot_design</a></li>
<li>paper_authors: Zhanming Jie, Trung Quoc Luong, Xinbo Zhang, Xiaoran Jin, Hang Li</li>
<li>for: 本研究旨在探讨链条思维（CoT）在数学问题解决中的作用，并对不同的程序CoT进行比较，包括自然语言CoT、自我描述程序、注释描述程序和非描述程序。此外，研究还 investigate了编程语言对程序CoT的影响，并对Python和Wolfram语言进行比较。</li>
<li>methods: 本研究采用了extensive experiments方法，在GSM8K、MATHQA和SVAMP上进行了评测，并发现了程序CoT在数学问题解决中的优势。特别是，最佳组合（30B参数）击败了GPT-3.5-turbo的表现，并且自然语言CoT提供了更大的多样性，因此可以通常实现更高的性能。</li>
<li>results: 研究结果显示，程序CoT在数学问题解决中具有优势，特别是自然语言CoT提供了更大的多样性，可以实现更高的性能。此外，研究还发现了Python是程序CoT的更好的编程语言。研究结果可以为未来的CoT设计提供有价值的指导，并且可以考虑编程语言和编程风格的因素进行进一步的改进。<details>
<summary>Abstract</summary>
Chain-of-Thought (CoT) plays a crucial role in reasoning for math problem solving. We conduct a comprehensive examination of methods for designing CoT, comparing conventional natural language CoT with various program CoTs, including the self-describing program, the comment-describing program, and the non-describing program. Furthermore, we investigate the impact of programming language on program CoTs, comparing Python and Wolfram Language. Through extensive experiments on GSM8K, MATHQA, and SVAMP, we find that program CoTs often have superior effectiveness in math problem solving. Notably, the best performing combination with 30B parameters beats GPT-3.5-turbo by a significant margin. The results show that self-describing program offers greater diversity and thus can generally achieve higher performance. We also find that Python is a better choice of language than Wolfram for program CoTs. The experimental results provide a valuable guideline for future CoT designs that take into account both programming language and coding style for further advancements. Our datasets and code are publicly available.
</details>
<details>
<summary>摘要</summary>
Chain-of-Thought (CoT) 在数学问题解决中扮演着关键性的角色。我们对设计 CoT 的方法进行了全面的评估，比较了自然语言 CoT 与不同的程序 CoT，包括自我描述程序、注释描述程序和非描述程序。此外，我们还 investigate了编程语言对程序 CoT 的影响，比较了 Python 和 Wolfram 语言。通过对 GSM8K、MATHQA 和 SVAMP 等数据集进行了广泛的实验，我们发现program CoT 在数学问题解决中经常具有更高的效果。特别是，使用 30B 参数的最佳组合可以很大幅度地超越 GPT-3.5-turbo。结果表明，自我描述程序可以提供更多的多样性，因此通常可以达到更高的性能。我们还发现 Python 比 Wolfram 更适合用于 program CoT。我们的实验结果提供了未来 CoT 设计的价值指南，考虑到编程语言和编程风格，以便进一步提高表达能力。我们的数据集和代码公开可用。
</details></li>
</ul>
<hr>
<h2 id="Clustered-FedStack-Intermediate-Global-Models-with-Bayesian-Information-Criterion"><a href="#Clustered-FedStack-Intermediate-Global-Models-with-Bayesian-Information-Criterion" class="headerlink" title="Clustered FedStack: Intermediate Global Models with Bayesian Information Criterion"></a>Clustered FedStack: Intermediate Global Models with Bayesian Information Criterion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11044">http://arxiv.org/abs/2309.11044</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thanveer Shaik, Xiaohui Tao, Lin Li, Niall Higgins, Raj Gururajan, Xujuan Zhou, Jianming Yong</li>
<li>for: 提高 Federated Learning（FL）在非Identical和非独立分布（non-IID）和数据偏置标签（imbalanced labels）的情况下的性能。</li>
<li>methods: 使用 Stacked Federated Learning（FedStack）框架，并采用三种集群机制：K-Means、Agglomerative和Gaussian Mixture Models。使用 Bayesian Information Criterion（BIC）确定集群数量。</li>
<li>results: Clustered FedStack模型比基eline模型 WITH clustering机制表现更好，并且使用cyclical learning rates来估计框架的整合程度。<details>
<summary>Abstract</summary>
Federated Learning (FL) is currently one of the most popular technologies in the field of Artificial Intelligence (AI) due to its collaborative learning and ability to preserve client privacy. However, it faces challenges such as non-identically and non-independently distributed (non-IID) and data with imbalanced labels among local clients. To address these limitations, the research community has explored various approaches such as using local model parameters, federated generative adversarial learning, and federated representation learning. In our study, we propose a novel Clustered FedStack framework based on the previously published Stacked Federated Learning (FedStack) framework. The local clients send their model predictions and output layer weights to a server, which then builds a robust global model. This global model clusters the local clients based on their output layer weights using a clustering mechanism. We adopt three clustering mechanisms, namely K-Means, Agglomerative, and Gaussian Mixture Models, into the framework and evaluate their performance. We use Bayesian Information Criterion (BIC) with the maximum likelihood function to determine the number of clusters. The Clustered FedStack models outperform baseline models with clustering mechanisms. To estimate the convergence of our proposed framework, we use Cyclical learning rates.
</details>
<details>
<summary>摘要</summary>
现在的 Federated Learning（FL）技术在人工智能（AI）领域中非常流行，这是因为它可以实现协同学习并保持客户端隐私。然而，FL还面临着非标一同分布（non-IID）和数据偏极性（imbalanced labels）等问题。为了解决这些局限性，研究人员已经提出了多种方法，如使用本地模型参数、联邦生成敌方搜索学习和联邦表示学习。在我们的研究中，我们提出了一种基于先前发表的 Stacked Federated Learning（FedStack）框架的 Novel Clustered FedStack 框架。本地客户端将其模型预测结果和输出层加权值发送到服务器，服务器然后建立一个强大的全局模型。这个全局模型使用一种卷积机制将本地客户端分为不同的集群。我们在框架中采用了 K-Means、Agglomerative 和 Gaussian Mixture Models 三种卷积机制，并使用 Bayesian Information Criterion（BIC）与最大似然函数来确定集群数量。Clustered FedStack 模型在基eline模型中表现出色，以便估算我们提出的框架的整合。为了估算我们的提出的框架的整合，我们使用 Cyclical learning rates。
</details></li>
</ul>
<hr>
<h2 id="Making-Small-Language-Models-Better-Multi-task-Learners-with-Mixture-of-Task-Adapters"><a href="#Making-Small-Language-Models-Better-Multi-task-Learners-with-Mixture-of-Task-Adapters" class="headerlink" title="Making Small Language Models Better Multi-task Learners with Mixture-of-Task-Adapters"></a>Making Small Language Models Better Multi-task Learners with Mixture-of-Task-Adapters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11042">http://arxiv.org/abs/2309.11042</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yukang Xie, Chengyu Wang, Junbing Yan, Jiyong Zhou, Feiqi Deng, Jun Huang</li>
<li>for: 本研究旨在提出一种基于小语言模型（less than 1B parameters）的多任务学习系统，以支持域pecific应用。</li>
<li>methods: 本研究提出了一种扩展 transformer 架构的 Mixture-of-Task-Adapters（MTA）模块，以capture intra-task 和inter-task 知识。同时，提出了一种两个阶段训练方法来优化 adapter 之间的协作。</li>
<li>results: 实验结果表明，提出的 MTA 架构和两个阶段训练方法可以达到良好的性能。此外，基于 ALTER 的 MTA-equipped 语言模型在不同领域中也得到了良好的result。<details>
<summary>Abstract</summary>
Recently, Large Language Models (LLMs) have achieved amazing zero-shot learning performance over a variety of Natural Language Processing (NLP) tasks, especially for text generative tasks. Yet, the large size of LLMs often leads to the high computational cost of model training and online deployment. In our work, we present ALTER, a system that effectively builds the multi-tAsk Learners with mixTure-of-task-adaptERs upon small language models (with <1B parameters) to address multiple NLP tasks simultaneously, capturing the commonalities and differences between tasks, in order to support domain-specific applications. Specifically, in ALTER, we propose the Mixture-of-Task-Adapters (MTA) module as an extension to the transformer architecture for the underlying model to capture the intra-task and inter-task knowledge. A two-stage training method is further proposed to optimize the collaboration between adapters at a small computational cost. Experimental results over a mixture of NLP tasks show that our proposed MTA architecture and the two-stage training method achieve good performance. Based on ALTER, we have also produced MTA-equipped language models for various domains.
</details>
<details>
<summary>摘要</summary>
最近，大型语言模型（LLMs）在多种自然语言处理（NLP）任务上实现了惊人的零shot学习性能，尤其是文本生成任务。然而，大型模型的大小经常导致模型训练和在线部署的计算成本高涨。在我们的工作中，我们提出了ALTER系统，可以有效地建立多任务学习者，通过将小型语言模型（ Parameters <1B）扩展到多个NLP任务，以便同时处理多个任务，捕捉任务之间的共同点和差异，以支持域pecific应用。具体来说，在ALTER中，我们提出了mixture-of-task-adaptERs（MTA）模块，作为 transformer 架构的增强部分，以Capture intra-task和inter-task知识。我们还提出了一种两Stage训练方法，以便在小型计算成本下优化 adapter collaboration。实验结果表明，我们的提议的MTA架构和两Stage训练方法在一组多种NLP任务上具有良好的表现。基于ALTER，我们还生成了各个领域的MTA语言模型。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-in-Intelligent-Transportation-Systems-Recent-Applications-and-Open-Problems"><a href="#Federated-Learning-in-Intelligent-Transportation-Systems-Recent-Applications-and-Open-Problems" class="headerlink" title="Federated Learning in Intelligent Transportation Systems: Recent Applications and Open Problems"></a>Federated Learning in Intelligent Transportation Systems: Recent Applications and Open Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11039">http://arxiv.org/abs/2309.11039</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiying Zhang, Jun Li, Long Shi, Ming Ding, Dinh C. Nguyen, Wuzheng Tan, Jian Weng, Zhu Han</li>
<li>for: 本研究旨在探讨基于分布式机器学习技术的智能交通系统（ITS）中的应用前景，以及在不同场景下如何使用 Federated Learning（FL）来解决智能交通系统中的问题。</li>
<li>methods: 本研究使用了分布式机器学习技术Federated Learning（FL）来解决智能交通系统中的问题，包括对象识别、交通管理和服务提供等场景。</li>
<li>results: 本研究发现了在智能交通系统中应用FL后，可以提高对象识别精度、提高交通管理效率和提高服务提供质量等。但是，FL也存在一些挑战，如数据不均匀分布、计算机力和存储空间的限制，以及隐私和安全问题。<details>
<summary>Abstract</summary>
Intelligent transportation systems (ITSs) have been fueled by the rapid development of communication technologies, sensor technologies, and the Internet of Things (IoT). Nonetheless, due to the dynamic characteristics of the vehicle networks, it is rather challenging to make timely and accurate decisions of vehicle behaviors. Moreover, in the presence of mobile wireless communications, the privacy and security of vehicle information are at constant risk. In this context, a new paradigm is urgently needed for various applications in dynamic vehicle environments. As a distributed machine learning technology, federated learning (FL) has received extensive attention due to its outstanding privacy protection properties and easy scalability. We conduct a comprehensive survey of the latest developments in FL for ITS. Specifically, we initially research the prevalent challenges in ITS and elucidate the motivations for applying FL from various perspectives. Subsequently, we review existing deployments of FL in ITS across various scenarios, and discuss specific potential issues in object recognition, traffic management, and service providing scenarios. Furthermore, we conduct a further analysis of the new challenges introduced by FL deployment and the inherent limitations that FL alone cannot fully address, including uneven data distribution, limited storage and computing power, and potential privacy and security concerns. We then examine the existing collaborative technologies that can help mitigate these challenges. Lastly, we discuss the open challenges that remain to be addressed in applying FL in ITS and propose several future research directions.
</details>
<details>
<summary>摘要</summary>
智能交通系统（ITS）因通信技术、感知技术和互联网对话的快速发展而得到推动。然而，由于车辆网络的动态特性，很难在时间上进行准确的车辆行为决策。此外，在移动无线通信的存在下，车辆信息的隐私和安全总是处于风险之中。在这种情况下，一种新的思维方式是紧迫的，以满足不同应用场景的需求。作为分布式机器学习技术，联邦学习（FL）在隐私保护和扩展可扩展性等方面受到了广泛的关注。我们进行了ITS中FL最新的发展情况的全面评估。我们首先研究了ITS中存在的主要挑战和应用FL的动机，然后评论了ITS中FL的不同场景应用，包括物体识别、交通管理和服务提供等方面的问题。此外，我们还进行了进一步的分析，探讨FL部署引入的新挑战和FL本身无法解决的内在限制，包括数据分布不均、计算和存储能力有限和隐私和安全问题。最后，我们讨论了在应用FL时存在的开放挑战，并提出了未来研究方向。
</details></li>
</ul>
<hr>
<h2 id="ModelGiF-Gradient-Fields-for-Model-Functional-Distance"><a href="#ModelGiF-Gradient-Fields-for-Model-Functional-Distance" class="headerlink" title="ModelGiF: Gradient Fields for Model Functional Distance"></a>ModelGiF: Gradient Fields for Model Functional Distance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11013">http://arxiv.org/abs/2309.11013</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zju-vipa/modelgif">https://github.com/zju-vipa/modelgif</a></li>
<li>paper_authors: Jie Song, Zhengqi Xu, Sai Wu, Gang Chen, Mingli Song</li>
<li>for: 这 paper 的目的是量化不同预训练模型之间的功能距离，以便为各种目的进行评估。</li>
<li>methods: 该 paper 使用了基于 “场” 的思想，提出了 Model Gradient Field (ModelGiF)，用于从不同预训练模型中提取同谱表示。</li>
<li>results: 实验结果表明，ModelGiF 在任务相关性判断、知识产权保护和模型忘却验证等方面具有显著的优势，与当前竞争者相比显著性更高。<details>
<summary>Abstract</summary>
The last decade has witnessed the success of deep learning and the surge of publicly released trained models, which necessitates the quantification of the model functional distance for various purposes. However, quantifying the model functional distance is always challenging due to the opacity in inner workings and the heterogeneity in architectures or tasks. Inspired by the concept of "field" in physics, in this work we introduce Model Gradient Field (abbr. ModelGiF) to extract homogeneous representations from the heterogeneous pre-trained models. Our main assumption underlying ModelGiF is that each pre-trained deep model uniquely determines a ModelGiF over the input space. The distance between models can thus be measured by the similarity between their ModelGiFs. We validate the effectiveness of the proposed ModelGiF with a suite of testbeds, including task relatedness estimation, intellectual property protection, and model unlearning verification. Experimental results demonstrate the versatility of the proposed ModelGiF on these tasks, with significantly superiority performance to state-of-the-art competitors. Codes are available at https://github.com/zju-vipa/modelgif.
</details>
<details>
<summary>摘要</summary>
过去一个十年，深度学习的成功和公共释放的训练模型的涌现，使得模型功能距离的量化变得非常重要。然而，量化模型功能距离总是困难的，因为深度学习模型的内部工作机制是不透明的，而且模型或任务的architecture和task都是多样的。引用物理学中的“场”概念，在这种工作中我们提出了Model Gradient Field（简称ModelGiF）来EXTRACT homogeneous representation from heterogeneous pre-trained models。我们假设每个预训练深度模型具有唯一的ModelGiF over the input space，因此可以通过比较这些ModelGiF的相似性来度量模型之间的距离。我们验证了提议的ModelGiF的效果通过一系列测试床，包括任务相似性预测、知识产权保护和模型忘记验证。实验结果表明提议的ModelGiF在这些任务上具有显著的优势性能，与现有的竞争对手相比。代码可以在https://github.com/zju-vipa/modelgif上获取。
</details></li>
</ul>
<hr>
<h2 id="Spiking-NeRF-Making-Bio-inspired-Neural-Networks-See-through-the-Real-World"><a href="#Spiking-NeRF-Making-Bio-inspired-Neural-Networks-See-through-the-Real-World" class="headerlink" title="Spiking NeRF: Making Bio-inspired Neural Networks See through the Real World"></a>Spiking NeRF: Making Bio-inspired Neural Networks See through the Real World</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10987">http://arxiv.org/abs/2309.10987</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingting Yao, Qinghao Hu, Tielong Liu, Zitao Mo, Zeyu Zhu, Zhengyang Zhuge, Jian Cheng</li>
<li>for: 这个论文的目的是提出一种能源优化的神经鳗网络（Spiking Neural Network，SNN），用于实现高品质的3D场景渲染，并且与生物学上的神经元运作相似。</li>
<li>methods: 这个方法使用了神经鳗网络（SNN）和射线场（NeRF）技术，将射线场与时间维度进行对应，从而使计算变成了一个发射-自由的方式，以减少能源消耗。</li>
<li>results: 实验结果显示，这个方法可以实现$76.74%$的能源优化，并且与生物学上的神经元运作相似。<details>
<summary>Abstract</summary>
Spiking neuron networks (SNNs) have been thriving on numerous tasks to leverage their promising energy efficiency and exploit their potentialities as biologically plausible intelligence. Meanwhile, the Neural Radiance Fields (NeRF) render high-quality 3D scenes with massive energy consumption, and few works delve into the energy-saving solution with a bio-inspired approach. In this paper, we propose spiking NeRF (SpikingNeRF), which aligns the radiance ray with the temporal dimension of SNN, to naturally accommodate the SNN to the reconstruction of Radiance Fields. Thus, the computation turns into a spike-based, multiplication-free manner, reducing the energy consumption. In SpikingNeRF, each sampled point on the ray is matched onto a particular time step, and represented in a hybrid manner where the voxel grids are maintained as well. Based on the voxel grids, sampled points are determined whether to be masked for better training and inference. However, this operation also incurs irregular temporal length. We propose the temporal condensing-and-padding (TCP) strategy to tackle the masked samples to maintain regular temporal length, i.e., regular tensors, for hardware-friendly computation. Extensive experiments on a variety of datasets demonstrate that our method reduces the $76.74\%$ energy consumption on average and obtains comparable synthesis quality with the ANN baseline.
</details>
<details>
<summary>摘要</summary>
神经风暴网络（SNN）在许多任务上得到了广泛应用，以利用其能效的能源和生物可能的智能潜力。然而，神经辐射场（NeRF）的渲染高质量3D场景却需要巨大的能源消耗，而很少的研究探讨了以生物静脉为导向的能源抑制方法。在这篇论文中，我们提出了神经辐射场（SpikingNeRF），它将辐射场的强度方向与SNN的时间维度对齐，以自然地让SNN参与辐射场的重建。因此，计算变成了一种快速、无 multiplication 的方式，从而降低了能源消耗。在SpikingNeRF中，每个样本点被匹配到特定的时间步，并以混合方式表示，保留了 voxel 网格。基于 voxel 网格，样本点是否需要被masking 以提高训练和推理的质量。然而，这个操作也会产生不规则的时间长度。我们提出了时间condensing-and-padding（TCP）策略，以解决masked samples的问题，以保持常规的时间长度，即常规的tensor，为硬件友好的计算。在多个dataset上进行了广泛的实验，表明我们的方法可以降低76.74%的能源消耗，并与ANN基线相当的Synthesis质量。
</details></li>
</ul>
<hr>
<h2 id="Is-GPT4-a-Good-Trader"><a href="#Is-GPT4-a-Good-Trader" class="headerlink" title="Is GPT4 a Good Trader?"></a>Is GPT4 a Good Trader?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10982">http://arxiv.org/abs/2309.10982</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bingzhe Wu<br>for: 本研究旨在检验GPT-4对经典投资理论的理解程度和对实际交易数据分析的代码解释能力。methods: 本研究使用GPT-4对特定资产的日均K线数据进行分析，基于尼采尔浪幕理论等特定理论。results: 本研究发现GPT-4在对实际交易数据分析中表现出较高的解释深度和准确率，同时提供了有价值的投资理论应用方法。<details>
<summary>Abstract</summary>
Recently, large language models (LLMs), particularly GPT-4, have demonstrated significant capabilities in various planning and reasoning tasks \cite{cheng2023gpt4,bubeck2023sparks}. Motivated by these advancements, there has been a surge of interest among researchers to harness the capabilities of GPT-4 for the automated design of quantitative factors that do not overlap with existing factor libraries, with an aspiration to achieve alpha returns \cite{webpagequant}. In contrast to these work, this study aims to examine the fidelity of GPT-4's comprehension of classic trading theories and its proficiency in applying its code interpreter abilities to real-world trading data analysis. Such an exploration is instrumental in discerning whether the underlying logic GPT-4 employs for trading is intrinsically reliable. Furthermore, given the acknowledged interpretative latitude inherent in most trading theories, we seek to distill more precise methodologies of deploying these theories from GPT-4's analytical process, potentially offering invaluable insights to human traders.   To achieve this objective, we selected daily candlestick (K-line) data from specific periods for certain assets, such as the Shanghai Stock Index. Through meticulous prompt engineering, we guided GPT-4 to analyze the technical structures embedded within this data, based on specific theories like the Elliott Wave Theory. We then subjected its analytical output to manual evaluation, assessing its interpretative depth and accuracy vis-\`a-vis these trading theories from multiple dimensions. The results and findings from this study could pave the way for a synergistic amalgamation of human expertise and AI-driven insights in the realm of trading.
</details>
<details>
<summary>摘要</summary>
最近，大语言模型（LLM），特别是GPT-4，在各种计划和理解任务中表现出了显著的能力。这些进步引起了研究人员对GPT-4的投资 alpha 回报的兴趣，并寻求通过自动设计不同于现有因素库的量化因素来实现这一目标。与这些工作不同，本研究旨在检验GPT-4对经典交易理论的理解和对实际交易数据分析中的代码解释能力。这种探索有助于判断GPT-4在交易中使用的逻辑是否具有内在的可靠性。此外，由于交易理论中的解释空间往往很大，我们寻求通过GPT-4的分析过程中提取更加精细的方法来应用这些理论，从而为人类交易员提供有价值的想法。为达到这个目标，我们选择了特定期间的一些资产的日均盘形（K-line）数据，例如上海股票指数。通过仔细的提问工程，我们导引GPT-4分析这些数据中的技术结构，基于特定的投资理论，如欧拉瓦vecenie理论。然后，我们对GPT-4的分析输出进行手动评估，评估其在这些交易理论多个维度的解释深度和准确性。研究结果和发现可能为人类专家和 AI 驱动的想法带来协同合作，为交易领域带来新的发展。
</details></li>
</ul>
<hr>
<h2 id="AI-Driven-Patient-Monitoring-with-Multi-Agent-Deep-Reinforcement-Learning"><a href="#AI-Driven-Patient-Monitoring-with-Multi-Agent-Deep-Reinforcement-Learning" class="headerlink" title="AI-Driven Patient Monitoring with Multi-Agent Deep Reinforcement Learning"></a>AI-Driven Patient Monitoring with Multi-Agent Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10980">http://arxiv.org/abs/2309.10980</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thanveer Shaik, Xiaohui Tao, Haoran Xie, Lin Li, Jianming Yong, Hong-Ning Dai</li>
<li>for: 提高医疗卫生监测效果，实现时间有效的干预和改善医疗结果。</li>
<li>methods: 使用多智能深度强化学习（DRL）方法，投入多个学习代理，每个代理负责监测特定生理参数，如心率、呼吸和体温等。这些代理与通用医疗监测环境互动，学习患者的行为模式，根据紧急程度估算，向相应的医疗应急团队（METs）发出警示。</li>
<li>results: 与多种基线模型进行比较，研究表明，提posed的DRL方法在实际生理和运动数据集PPG-DaLiA和WESAD上的表现准确性高于所有基线模型，并且通过调整Hyperparameter进行优化，进一步提高代理的总性能。<details>
<summary>Abstract</summary>
Effective patient monitoring is vital for timely interventions and improved healthcare outcomes. Traditional monitoring systems often struggle to handle complex, dynamic environments with fluctuating vital signs, leading to delays in identifying critical conditions. To address this challenge, we propose a novel AI-driven patient monitoring framework using multi-agent deep reinforcement learning (DRL). Our approach deploys multiple learning agents, each dedicated to monitoring a specific physiological feature, such as heart rate, respiration, and temperature. These agents interact with a generic healthcare monitoring environment, learn the patients' behavior patterns, and make informed decisions to alert the corresponding Medical Emergency Teams (METs) based on the level of emergency estimated. In this study, we evaluate the performance of the proposed multi-agent DRL framework using real-world physiological and motion data from two datasets: PPG-DaLiA and WESAD. We compare the results with several baseline models, including Q-Learning, PPO, Actor-Critic, Double DQN, and DDPG, as well as monitoring frameworks like WISEML and CA-MAQL. Our experiments demonstrate that the proposed DRL approach outperforms all other baseline models, achieving more accurate monitoring of patient's vital signs. Furthermore, we conduct hyperparameter optimization to fine-tune the learning process of each agent. By optimizing hyperparameters, we enhance the learning rate and discount factor, thereby improving the agents' overall performance in monitoring patient health status. Our AI-driven patient monitoring system offers several advantages over traditional methods, including the ability to handle complex and uncertain environments, adapt to varying patient conditions, and make real-time decisions without external supervision.
</details>
<details>
<summary>摘要</summary>
通过人工智能驱动的患者监测框架，我们可以提高医疗结果和患者监测效果。传统的监测系统经常在复杂和动态的环境中难以处理，导致检测重要情况的延迟。为解决这个挑战，我们提出了一种基于多代理深度学习（DRL）的新型患者监测框架。我们的方法在多个学习代理之间分配不同的生物 physiological 特征，例如心率、呼吸和体温。这些代理与一个通用医疗监测环境进行交互，学习患者的行为模式，并根据紧急程度来通知相应的医疗紧急队伍（METs）。在本研究中，我们使用实际的生理和运动数据进行评估，并与多种基准模型进行比较，包括Q学习、PPO、actor-critic、Double DQN 和 DDPG 等。我们的实验表明，提出的 DRL 方法在监测患者生命体征上的准确性比基准模型高。此外，我们还进行了 гипер参数优化，以提高每个代理的学习过程。通过优化 гипер参数，我们可以提高代理的总表现，以更好地监测患者健康状态。我们的人工智能驱动的患者监测系统具有许多优势，包括能够处理复杂和不确定的环境、适应变化的患者状况，以及不需要外部监督而行动。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/20/cs.AI_2023_09_20/" data-id="clpztdnbp004des883bti4naz" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_09_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/20/cs.CL_2023_09_20/" class="article-date">
  <time datetime="2023-09-20T11:00:00.000Z" itemprop="datePublished">2023-09-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/20/cs.CL_2023_09_20/">cs.CL - 2023-09-20</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Semi-supervised-News-Discourse-Profiling-with-Contrastive-Learning"><a href="#Semi-supervised-News-Discourse-Profiling-with-Contrastive-Learning" class="headerlink" title="Semi-supervised News Discourse Profiling with Contrastive Learning"></a>Semi-supervised News Discourse Profiling with Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11692">http://arxiv.org/abs/2309.11692</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ming Li, Ruihong Huang</li>
<li>for: 这篇论文是为了解决新闻报道中文本结构分类问题，以便在下游应用中使用。</li>
<li>methods: 该论文使用了一种新的方法，即内文对照学习 WITH 精神（ICLD），利用新闻报道的特殊结构特征进行采样，并通过对比分类来增强模型的性能。</li>
<li>results: 论文的实验结果表明，ICLD 方法可以有效地解决新闻报道中文本结构分类问题，并且比传统的监督学习方法更有效。<details>
<summary>Abstract</summary>
News Discourse Profiling seeks to scrutinize the event-related role of each sentence in a news article and has been proven useful across various downstream applications. Specifically, within the context of a given news discourse, each sentence is assigned to a pre-defined category contingent upon its depiction of the news event structure. However, existing approaches suffer from an inadequacy of available human-annotated data, due to the laborious and time-intensive nature of generating discourse-level annotations. In this paper, we present a novel approach, denoted as Intra-document Contrastive Learning with Distillation (ICLD), for addressing the news discourse profiling task, capitalizing on its unique structural characteristics. Notably, we are the first to apply a semi-supervised methodology within this task paradigm, and evaluation demonstrates the effectiveness of the presented approach.
</details>
<details>
<summary>摘要</summary>
新闻话语分析旨在研究每个新闻文章中的每句话语的事件相关性角色，并在多种下游应用中表现出有用性。特别是在给定的新闻话语背景下，每句话语会被分配到预定的类别，根据它们描述新闻事件结构。然而，现有的方法受到有限的人工标注数据的不足，这是因为生成话语水平标注的劳动和时间费时的。在这篇论文中，我们提出了一种新的方法，称为Intra-document Contrastive Learning with Distillation（ICLD），用于解决新闻话语 profiling 任务，利用它的独特结构特征。值得注意的是，我们是首次在这个任务准则下应用 semi-supervised 方法ологи，评估结果表明该方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="A-Paradigm-Shift-in-Machine-Translation-Boosting-Translation-Performance-of-Large-Language-Models"><a href="#A-Paradigm-Shift-in-Machine-Translation-Boosting-Translation-Performance-of-Large-Language-Models" class="headerlink" title="A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models"></a>A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11674">http://arxiv.org/abs/2309.11674</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fe1ixxu/alma">https://github.com/fe1ixxu/alma</a></li>
<li>paper_authors: Haoran Xu, Young Jin Kim, Amr Sharaf, Hany Hassan Awadalla</li>
<li>for: 提高 moderate-sized language models (LLMs) 在翻译任务中的表现</li>
<li>methods: 提出了一种特有的练习方法，通过首先在单语言数据上进行初始练习，然后在一小量高质量并列数据上进行后练习，以消除传统翻译模型通常依赖的庞大并列数据的需求。</li>
<li>results: 根据 LLaMA-2 为基础模型，实现了在 WMT’21 和 WMT’22 测试集上的平均提高超过 12 BLEU 和 12 COMET，在 10 个翻译方向上。表现较之前的所有工作更好，甚至超过 NLLB-54B 模型和 GPT-3.5-text-davinci-003，即使只有 7B 或 13B 参数。这种方法为机器翻译训练方法提供了基础。<details>
<summary>Abstract</summary>
Generative Large Language Models (LLMs) have achieved remarkable advancements in various NLP tasks. However, these advances have not been reflected in the translation task, especially those with moderate model sizes (i.e., 7B or 13B parameters), which still lag behind conventional supervised encoder-decoder translation models. Previous studies have attempted to improve the translation capabilities of these moderate LLMs, but their gains have been limited. In this study, we propose a novel fine-tuning approach for LLMs that is specifically designed for the translation task, eliminating the need for the abundant parallel data that traditional translation models usually depend on. Our approach consists of two fine-tuning stages: initial fine-tuning on monolingual data followed by subsequent fine-tuning on a small set of high-quality parallel data. We introduce the LLM developed through this strategy as Advanced Language Model-based trAnslator (ALMA). Based on LLaMA-2 as our underlying model, our results show that the model can achieve an average improvement of more than 12 BLEU and 12 COMET over its zero-shot performance across 10 translation directions from the WMT'21 (2 directions) and WMT'22 (8 directions) test datasets. The performance is significantly better than all prior work and even superior to the NLLB-54B model and GPT-3.5-text-davinci-003, with only 7B or 13B parameters. This method establishes the foundation for a novel training paradigm in machine translation.
</details>
<details>
<summary>摘要</summary>
生成大型自然语言模型（LLM）在不同的自然语言处理任务中已经取得了非常出色的进步。然而，这些进步并没有反映在翻译任务中，尤其是使用中型模型（i.e., 7B或13B参数），这些模型仍然落后于传统的监督编码器-解码器翻译模型。先前的研究已经尝试使用不同的方法来提高这些中型LLM的翻译能力，但其成果很有限。在这个研究中，我们提出了一种特有的练习方法，用于提高LLM的翻译能力，不需要大量的并行数据。我们的方法包括两个练习阶段：首先在单语言数据上进行初始练习，然后在一个小量高质量并行数据上进行 subsequential 练习。我们称之为Advanced Language Model-based trAnslator（ALMA）。基于LLaMA-2作为我们的基础模型，我们的结果显示，该模型可以在10个翻译方向上 average 提高超过12个BLEU和12个COMET的性能，相比于零开始性能。这个性能高于所有之前的工作，甚至超过NLLB-54B模型和GPT-3.5-text-davinci-003模型，即使只有7B或13B参数。这种方法创立了一种新的训练 парадигма在机器翻译领域。
</details></li>
</ul>
<hr>
<h2 id="Construction-of-Paired-Knowledge-Graph-Text-Datasets-Informed-by-Cyclic-Evaluation"><a href="#Construction-of-Paired-Knowledge-Graph-Text-Datasets-Informed-by-Cyclic-Evaluation" class="headerlink" title="Construction of Paired Knowledge Graph-Text Datasets Informed by Cyclic Evaluation"></a>Construction of Paired Knowledge Graph-Text Datasets Informed by Cyclic Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11669">http://arxiv.org/abs/2309.11669</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Mousavi, Xin Zhan, He Bai, Peng Shi, Theo Rekatsinas, Benjamin Han, Yunyao Li, Jeff Pound, Josh Susskind, Natalie Schluter, Ihab Ilyas, Navdeep Jaitly</li>
<li>for: 这 paper 的目的是证明使用不同噪音水平生成 Knowledge Graph (KG) 和文本对应的数据集，可以训练前向和反向神经网络模型，但是使用不同的数据集可能会导致更多的幻觉和更差的拟合率。</li>
<li>methods: 这 paper 使用了生成文本和 KG 的cyclic evaluation来评估模型的性能，并通过手动创建 WebNLG 和自动创建 TeKGen 和 T-REx 来评估模型的表现。</li>
<li>results: 这 paper 发现，使用不同噪音水平生成的数据集可以影响模型的性能，并且手动创建的 WebNLG 表现更好于自动创建的 TeKGen 和 T-REx。此外，使用大语言模型 (LLM) 构建的数据集可以训练模型在文本生成中表现出色，但是在 Knowledge Graph 生成中表现较差，可能是因为没有一个共同的 Ontology。<details>
<summary>Abstract</summary>
Datasets that pair Knowledge Graphs (KG) and text together (KG-T) can be used to train forward and reverse neural models that generate text from KG and vice versa. However models trained on datasets where KG and text pairs are not equivalent can suffer from more hallucination and poorer recall. In this paper, we verify this empirically by generating datasets with different levels of noise and find that noisier datasets do indeed lead to more hallucination. We argue that the ability of forward and reverse models trained on a dataset to cyclically regenerate source KG or text is a proxy for the equivalence between the KG and the text in the dataset. Using cyclic evaluation we find that manually created WebNLG is much better than automatically created TeKGen and T-REx. Guided by these observations, we construct a new, improved dataset called LAGRANGE using heuristics meant to improve equivalence between KG and text and show the impact of each of the heuristics on cyclic evaluation. We also construct two synthetic datasets using large language models (LLMs), and observe that these are conducive to models that perform significantly well on cyclic generation of text, but less so on cyclic generation of KGs, probably because of a lack of a consistent underlying ontology.
</details>
<details>
<summary>摘要</summary>
Datasets that pair Knowledge Graphs (KG) and text together (KG-T) can be used to train forward and reverse neural models that generate text from KG and vice versa. However, models trained on datasets where KG and text pairs are not equivalent can suffer from more hallucination and poorer recall. In this paper, we verify this empirically by generating datasets with different levels of noise and find that noisier datasets do indeed lead to more hallucination. We argue that the ability of forward and reverse models trained on a dataset to cyclically regenerate source KG or text is a proxy for the equivalence between the KG and the text in the dataset. Using cyclic evaluation, we find that manually created WebNLG is much better than automatically created TeKGen and T-REx. Guided by these observations, we construct a new, improved dataset called LAGRANGE using heuristics meant to improve equivalence between KG and text and show the impact of each of the heuristics on cyclic evaluation. We also construct two synthetic datasets using large language models (LLMs), and observe that these are conducive to models that perform significantly well on cyclic generation of text, but less so on cyclic generation of KGs, probably because of a lack of a consistent underlying ontology.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Towards-Effective-Disambiguation-for-Machine-Translation-with-Large-Language-Models"><a href="#Towards-Effective-Disambiguation-for-Machine-Translation-with-Large-Language-Models" class="headerlink" title="Towards Effective Disambiguation for Machine Translation with Large Language Models"></a>Towards Effective Disambiguation for Machine Translation with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11668">http://arxiv.org/abs/2309.11668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vivek Iyer, Pinzhen Chen, Alexandra Birch</li>
<li>for: 本研究旨在探讨大语言模型（LLM）在具有多义词和罕见词意义时翻译不确定性的能力。</li>
<li>methods: 我们提出了两种改进翻译不确定性处理方法，一种是在 Context 中学习，另一种是在特 curaously 编辑的不确定性数据集上进行练习和微调。</li>
<li>results: 实验结果表明，我们的方法可以与当前状态的系统如深度翻译和 NLLB 匹配或超越，在五种语言方向中四种方向中表现出色。<details>
<summary>Abstract</summary>
Resolving semantic ambiguity has long been recognised as a central challenge in the field of machine translation. Recent work on benchmarking translation performance on ambiguous sentences has exposed the limitations of conventional Neural Machine Translation (NMT) systems, which fail to capture many of these cases. Large language models (LLMs) have emerged as a promising alternative, demonstrating comparable performance to traditional NMT models while introducing new paradigms for controlling the target outputs. In this paper, we study the capabilities of LLMs to translate ambiguous sentences containing polysemous words and rare word senses. We also propose two ways to improve the handling of such ambiguity through in-context learning and fine-tuning on carefully curated ambiguous datasets. Experiments show that our methods can match or outperform state-of-the-art systems such as DeepL and NLLB in four out of five language directions. Our research provides valuable insights into effectively adapting LLMs for disambiguation during machine translation.
</details>
<details>
<summary>摘要</summary>
解决语义含义的挑战一直被认为是机器翻译领域的中心问题。最近的研究表明，使用含义ambiguous sentence进行翻译性能测试的传统神经机器翻译（NMT）系统有限，不能捕捉这些情况。大型语言模型（LLM）在这些情况下表现出了潜在的优势，并提出了新的控制目标输出的方法。在这篇论文中，我们研究了LLM在含义ambiguous sentence中翻译的能力，并提出了两种改进方法，通过在上下文学习和精心编辑的歧义数据进行训练。实验结果显示，我们的方法可以与现有的状态机DeepL和NLLB相当或超越，在五种语言方向中四种方向取得了最佳效果。我们的研究为将LLM适应到翻译中的歧义提供了有价值的视角。
</details></li>
</ul>
<hr>
<h2 id="Hate-speech-detection-in-algerian-dialect-using-deep-learning"><a href="#Hate-speech-detection-in-algerian-dialect-using-deep-learning" class="headerlink" title="Hate speech detection in algerian dialect using deep learning"></a>Hate speech detection in algerian dialect using deep learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11611">http://arxiv.org/abs/2309.11611</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dihia Lanasri, Juan Olano, Sifal Klioui, Sin Liang Lee, Lamia Sekkai</li>
<li>for: 帮助掌握在阿拉伯语言上的仇恨言论检测问题，尤其是在阿尔жи尔语 dialect中。</li>
<li>methods: 使用深度学习架构对阿尔жи尔社交媒体上的短讯进行分类，以确定是否包含仇恨言论。</li>
<li>results: 在对13500余个阿尔жи尔社交媒体短讯的实验中，提出了一种可靠的仇恨言论检测方法，并取得了批判性的结果。<details>
<summary>Abstract</summary>
With the proliferation of hate speech on social networks under different formats, such as abusive language, cyberbullying, and violence, etc., people have experienced a significant increase in violence, putting them in uncomfortable situations and threats. Plenty of efforts have been dedicated in the last few years to overcome this phenomenon to detect hate speech in different structured languages like English, French, Arabic, and others. However, a reduced number of works deal with Arabic dialects like Tunisian, Egyptian, and Gulf, mainly the Algerian ones. To fill in the gap, we propose in this work a complete approach for detecting hate speech on online Algerian messages. Many deep learning architectures have been evaluated on the corpus we created from some Algerian social networks (Facebook, YouTube, and Twitter). This corpus contains more than 13.5K documents in Algerian dialect written in Arabic, labeled as hateful or non-hateful. Promising results are obtained, which show the efficiency of our approach.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:随着社交媒体上不同形式的仇恨言语、网络欺凌和暴力等等的普及，人们受到了不适的情况和威胁。过去几年，为了解决这种现象，各种努力已经投入了很多时间和精力，以检测不同的结构语言中的仇恨言语，如英语、法语、阿拉伯语等等。然而，对于阿拉伯 диалект，如突尼斯、埃及和 Golfo 的研究相对较少。为了填补这个空白，我们在这工作中提出了一个完整的方法，用于在在线阿尔及利亚消息中检测仇恨言语。我们在一些阿尔及利亚社交媒体（Facebook、YouTube和Twitter）上创建了一个大量的 corpus，包括13500余个文档，用阿尔及利亚 диалект的阿拉伯语书写，标注为有仇恨或无仇恨。我们评估了多种深度学习架构，并获得了良好的结果，这表明我们的方法的效果。
</details></li>
</ul>
<hr>
<h2 id="SpeechAlign-a-Framework-for-Speech-Translation-Alignment-Evaluation"><a href="#SpeechAlign-a-Framework-for-Speech-Translation-Alignment-Evaluation" class="headerlink" title="SpeechAlign: a Framework for Speech Translation Alignment Evaluation"></a>SpeechAlign: a Framework for Speech Translation Alignment Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11585">http://arxiv.org/abs/2309.11585</a></li>
<li>repo_url: None</li>
<li>paper_authors: Belen Alastruey, Aleix Sant, Gerard I. Gállego, David Dale, Marta R. Costa-jussà</li>
<li>for: 这篇论文主要为了评估speech模型中的source-target对应问题提供了一个框架。</li>
<li>methods: 这篇论文使用了两个核心组件：首先，它引入了一个英文-德语文本翻译金标对dataset，用于建立评估数据集。其次，它引入了两种新的精度指标，即Speech Alignment Error Rate (SAER)和Time-weighted Speech Alignment Error Rate (TW-SAER)，用于评估speech模型的对应质量。</li>
<li>results: 通过发布SpeechAlign框架，这篇论文为speech模型评估提供了可 accessible的评估框架，并通过使用这个框架对开源Speech Translation模型进行了比较。<details>
<summary>Abstract</summary>
Speech-to-Speech and Speech-to-Text translation are currently dynamic areas of research. To contribute to these fields, we present SpeechAlign, a framework to evaluate the underexplored field of source-target alignment in speech models. Our framework has two core components. First, to tackle the absence of suitable evaluation datasets, we introduce the Speech Gold Alignment dataset, built upon a English-German text translation gold alignment dataset. Secondly, we introduce two novel metrics, Speech Alignment Error Rate (SAER) and Time-weighted Speech Alignment Error Rate (TW-SAER), to evaluate alignment quality in speech models. By publishing SpeechAlign we provide an accessible evaluation framework for model assessment, and we employ it to benchmark open-source Speech Translation models.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。</SYS>现在演示的 Speech-to-Speech 和 Speech-to-Text 翻译是研究领域的动态领域。为了贡献这些领域，我们提出 SpeechAlign 框架，用于评估speech模型中source-target对齐的领域。我们的框架有两个核心组成部分。首先，由于缺乏适合的评估数据集，我们引入 Speech Gold Alignment 数据集，基于英语-德语文本翻译金标Alignment数据集。其次，我们引入两种新的指标，Speech Alignment Error Rate (SAER) 和 Time-weighted Speech Alignment Error Rate (TW-SAER)，用于评估对齐质量在speech模型中。通过发布 SpeechAlign，我们提供了一个可访问的评估框架，并使用它来对开源 Speech Translation 模型进行比较。
</details></li>
</ul>
<hr>
<h2 id="Incorporating-Singletons-and-Mention-based-Features-in-Coreference-Resolution-via-Multi-task-Learning-for-Better-Generalization"><a href="#Incorporating-Singletons-and-Mention-based-Features-in-Coreference-Resolution-via-Multi-task-Learning-for-Better-Generalization" class="headerlink" title="Incorporating Singletons and Mention-based Features in Coreference Resolution via Multi-task Learning for Better Generalization"></a>Incorporating Singletons and Mention-based Features in Coreference Resolution via Multi-task Learning for Better Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11582">http://arxiv.org/abs/2309.11582</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yilunzhu/coref-mtl">https://github.com/yilunzhu/coref-mtl</a></li>
<li>paper_authors: Yilun Zhu, Siyao Peng, Sameer Pradhan, Amir Zeldes</li>
<li>for: 本研究旨在提高英语核心共referencing解决方法中的提及检测步骤，以提高核心共referencing的准确率和Robustness。</li>
<li>methods: 本研究使用多任务学习的方法，学习单个提及span的特征以及实体类型和信息状态的特征，以提高核心共referencing的准确率和Robustness。</li>
<li>results: 本研究在OntoGUMbenchmark上 achieve新的状态机制得分 (+2.7点)，并在多个out-of-domain数据集上提高了Robustness (+2.3点的平均提高值)，这些提高可能是由于更好的提及检测和更多的数据来自单个提及span的使用所致。<details>
<summary>Abstract</summary>
Previous attempts to incorporate a mention detection step into end-to-end neural coreference resolution for English have been hampered by the lack of singleton mention span data as well as other entity information. This paper presents a coreference model that learns singletons as well as features such as entity type and information status via a multi-task learning-based approach. This approach achieves new state-of-the-art scores on the OntoGUM benchmark (+2.7 points) and increases robustness on multiple out-of-domain datasets (+2.3 points on average), likely due to greater generalizability for mention detection and utilization of more data from singletons when compared to only coreferent mention pair matching.
</details>
<details>
<summary>摘要</summary>
先前的尝试将提及检测步骤包含在英语的端到端神经核心referencing中，受到缺乏单个提及跨度数据以及其他实体信息的限制。这篇论文提出了一种核心模型，可以学习单个提及以及实体类型和信息状态等特征，使用多任务学习的方式。这种方法在OntoGUM benchmark上达到了新的状态态标准分（+2.7分），并在多个 OUT-OF-DOMAIN 数据集上提高了鲁棒性（平均+2.3分），可能是因为更好的提及检测和更多的数据来自单个提及 span 的利用。
</details></li>
</ul>
<hr>
<h2 id="Examining-the-Limitations-of-Computational-Rumor-Detection-Models-Trained-on-Static-Datasets"><a href="#Examining-the-Limitations-of-Computational-Rumor-Detection-Models-Trained-on-Static-Datasets" class="headerlink" title="Examining the Limitations of Computational Rumor Detection Models Trained on Static Datasets"></a>Examining the Limitations of Computational Rumor Detection Models Trained on Static Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11576">http://arxiv.org/abs/2309.11576</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yida Mu, Xingyi Song, Kalina Bontcheva, Nikolaos Aletras</li>
<li>for: 本研究旨在评估内容基于和Context基于的谣言探测模型在探测新、未知谣言方面的表现差异。</li>
<li>methods: 本研究使用了实验方法来评估内容基于和Context基于的谣言探测模型在探测新、未知谣言方面的表现差异。</li>
<li>results: 研究结果表明，Context基于的模型仍然受到来源帖子信息的限制，并且忽略了上下文信息的重要作用。此外，研究还探讨了数据分割策略对分类器性能的影响，并提供了实践的建议来降低静态数据集中的时间概念漂移的影响。<details>
<summary>Abstract</summary>
A crucial aspect of a rumor detection model is its ability to generalize, particularly its ability to detect emerging, previously unknown rumors. Past research has indicated that content-based (i.e., using solely source posts as input) rumor detection models tend to perform less effectively on unseen rumors. At the same time, the potential of context-based models remains largely untapped. The main contribution of this paper is in the in-depth evaluation of the performance gap between content and context-based models specifically on detecting new, unseen rumors. Our empirical findings demonstrate that context-based models are still overly dependent on the information derived from the rumors' source post and tend to overlook the significant role that contextual information can play. We also study the effect of data split strategies on classifier performance. Based on our experimental results, the paper also offers practical suggestions on how to minimize the effects of temporal concept drift in static datasets during the training of rumor detection methods.
</details>
<details>
<summary>摘要</summary>
一个重要的噱头检测模型特点是其能够总结，特别是检测出现在未知噱头。过去的研究表明，含有媒体文章仅作输入的内容基于噱头检测模型在未看过的噱头上表现较差。同时，叙述基于模型的潜力仍然未得到充分利用。本文的主要贡献在于对内容和叙述基于模型的性能差异进行深入评估，特别是检测新的、未知噱头。我们的实验结果表明，叙述基于模型仍然过分依赖源媒体文章提供的信息，而忽视了Contextual信息的重要作用。我们还研究了数据分裂策略对分类器性能的影响。根据我们的实验结果，文章还提供了实践的建议，以降低在训练噱头检测方法时的时间概念退变的影响。
</details></li>
</ul>
<hr>
<h2 id="SignBank-Multilingual-Sign-Language-Translation-Dataset"><a href="#SignBank-Multilingual-Sign-Language-Translation-Dataset" class="headerlink" title="SignBank+: Multilingual Sign Language Translation Dataset"></a>SignBank+: Multilingual Sign Language Translation Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11566">http://arxiv.org/abs/2309.11566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amit Moryossef, Zifan Jiang</li>
<li>for: 提高手语机器翻译领域的研究，强调数据质量和翻译系统简化。</li>
<li>methods: 介绍SignBank+数据集，是Optimized for machine translation的纯净版SignBank数据集，并使用简单的文本到文本翻译方法。</li>
<li>results: 评估结果显示，使用SignBank+数据集训练的模型超过原始数据集训练的模型，创造新的benchmark和提供开放资源 для未来研究。<details>
<summary>Abstract</summary>
This work advances the field of sign language machine translation by focusing on dataset quality and simplification of the translation system. We introduce SignBank+, a clean version of the SignBank dataset, optimized for machine translation. Contrary to previous works that employ complex factorization techniques for translation, we advocate for a simplified text-to-text translation approach. Our evaluation shows that models trained on SignBank+ surpass those on the original dataset, establishing a new benchmark and providing an open resource for future research.
</details>
<details>
<summary>摘要</summary>
这个研究提高了手语机器翻译的领域，关注数据集质量和翻译系统简化。我们介绍了SignBank+，一个优化的手语数据集，适用于机器翻译。与前期工作不同，我们主张使用简单的文本到文本翻译方法。我们的评估表明，基于SignBank+的模型比原始数据集模型更高效，创造了新的标准和提供了未来研究的开放资源。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-reinforcement-learning-with-natural-language-subgoals"><a href="#Hierarchical-reinforcement-learning-with-natural-language-subgoals" class="headerlink" title="Hierarchical reinforcement learning with natural language subgoals"></a>Hierarchical reinforcement learning with natural language subgoals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11564">http://arxiv.org/abs/2309.11564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arun Ahuja, Kavya Kopparapu, Rob Fergus, Ishita Dasgupta</li>
<li>for: 这个论文的目的是实现长期行为的目标导向行为，但实现在现实环境中具有挑战。</li>
<li>methods: 这个论文使用了人类数据来软着 Parametrize Goal Space，使用无结构的自然语言来表示这个空间。</li>
<li>results: 该方法比专家复制行为和没有这种监督目标空间的HRL better表现，表明该方法可以结合人类专家监督和奖励学习的优点。<details>
<summary>Abstract</summary>
Hierarchical reinforcement learning has been a compelling approach for achieving goal directed behavior over long sequences of actions. However, it has been challenging to implement in realistic or open-ended environments. A main challenge has been to find the right space of sub-goals over which to instantiate a hierarchy. We present a novel approach where we use data from humans solving these tasks to softly supervise the goal space for a set of long range tasks in a 3D embodied environment. In particular, we use unconstrained natural language to parameterize this space. This has two advantages: first, it is easy to generate this data from naive human participants; second, it is flexible enough to represent a vast range of sub-goals in human-relevant tasks. Our approach outperforms agents that clone expert behavior on these tasks, as well as HRL from scratch without this supervised sub-goal space. Our work presents a novel approach to combining human expert supervision with the benefits and flexibility of reinforcement learning.
</details>
<details>
<summary>摘要</summary>
hierarchical reinforcement learning 是一种吸引人的方法，可以实现长序列动作的目标行为。然而，在真实或开放的环境中实现具有挑战。一个主要挑战是找到适当的下一级目标空间，以实现层次结构。我们提出了一种新的方法，使用人类解决这些任务的数据来软着册这个空间。具体来说，我们使用无结构的自然语言来 parameterize这个空间。这有两个优点：首先，可以轻松地从不熟悉的人参与者中获得这些数据；其次，它够灵活，可以表示人类相关任务中的广泛下一级目标。我们的方法比不同扩展学习的代理人和不带有此协助下一级目标空间的 HRL 表现更好。我们的工作提出了一种结合人类专家指导和强化学习的新方法。
</details></li>
</ul>
<hr>
<h2 id="DreamLLM-Synergistic-Multimodal-Comprehension-and-Creation"><a href="#DreamLLM-Synergistic-Multimodal-Comprehension-and-Creation" class="headerlink" title="DreamLLM: Synergistic Multimodal Comprehension and Creation"></a>DreamLLM: Synergistic Multimodal Comprehension and Creation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11499">http://arxiv.org/abs/2309.11499</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/RunpeiDong/DreamLLM">https://github.com/RunpeiDong/DreamLLM</a></li>
<li>paper_authors: Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, Li Yi</li>
<li>for: 本研究旨在开发一种能够生成多modal language model（MLLM），具有较少注意的多modal理解和创造之间的共识。</li>
<li>methods:  DreamLLM 使用两个基本原则：首先，通过直接抽样在原始多modal空间进行语言和图像 posterior 的生成模型化，以避免 CLIP 等外部特征提取器的局限性和信息损失，从而获得更全面的多modal理解。其次， DreamLLM 可以生成 raw 的、混合的文档，包括文本和图像内容，以及无结构的布局，从而学习所有的 conditional、marginal 和 joint 多modal分布。</li>
<li>results: DreamLLM 能够生成免 Training 的多modal通用专家，在多modal总体 экспериментах中表现出色，受益于提高的学习共识。<details>
<summary>Abstract</summary>
This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Generative modeling of both language and image posteriors through direct sampling in the raw multimodal space. This approach bypasses the limitations of external feature extractors like CLIP and enables a more comprehensive understanding of multimodal information.2. Generation of raw, interleaved documents that model both text and image contents, as well as unstructured layouts. This allows DreamLLM to effectively learn all conditional, marginal, and joint multimodal distributions.As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content, demonstrating superior performance as a zero-shot multimodal generalist. Comprehensive experiments highlight the enhanced learning synergy achieved by DreamLLM.</details></li>
</ol>
<hr>
<h2 id="Controlled-Generation-with-Prompt-Insertion-for-Natural-Language-Explanations-in-Grammatical-Error-Correction"><a href="#Controlled-Generation-with-Prompt-Insertion-for-Natural-Language-Explanations-in-Grammatical-Error-Correction" class="headerlink" title="Controlled Generation with Prompt Insertion for Natural Language Explanations in Grammatical Error Correction"></a>Controlled Generation with Prompt Insertion for Natural Language Explanations in Grammatical Error Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11439">http://arxiv.org/abs/2309.11439</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masahiro Kaneko, Naoaki Okazaki</li>
<li>for: 这个论文的目的是提出一种名为控制生成（Prompt Insertion，PI）的方法，用于使大型自然语言模型（Large Language Models，LLMs）可以在自然语言中提供对 grammar 和语法错误 corrections 的直接解释。</li>
<li>methods: 这个论文使用了 Large Language Models (LLMs) 和 Prompt Insertion (PI) 方法来生成对 grammar 和语法错误 corrections 的直接解释。</li>
<li>results: 这个研究发现，使用 PI 方法可以使 LLMs 能够直接在自然语言中提供对 grammar 和语法错误 corrections 的解释，并且可以提高对 correction reasons 的生成性能。<details>
<summary>Abstract</summary>
In Grammatical Error Correction (GEC), it is crucial to ensure the user's comprehension of a reason for correction. Existing studies present tokens, examples, and hints as to the basis for correction but do not directly explain the reasons for corrections. Although methods that use Large Language Models (LLMs) to provide direct explanations in natural language have been proposed for various tasks, no such method exists for GEC. Generating explanations for GEC corrections involves aligning input and output tokens, identifying correction points, and presenting corresponding explanations consistently. However, it is not straightforward to specify a complex format to generate explanations, because explicit control of generation is difficult with prompts. This study introduces a method called controlled generation with Prompt Insertion (PI) so that LLMs can explain the reasons for corrections in natural language. In PI, LLMs first correct the input text, and then we automatically extract the correction points based on the rules. The extracted correction points are sequentially inserted into the LLM's explanation output as prompts, guiding the LLMs to generate explanations for the correction points. We also create an Explainable GEC (XGEC) dataset of correction reasons by annotating NUCLE, CoNLL2013, and CoNLL2014. Although generations from GPT-3 and ChatGPT using original prompts miss some correction points, the generation control using PI can explicitly guide to describe explanations for all correction points, contributing to improved performance in generating correction reasons.
</details>
<details>
<summary>摘要</summary>
在语法错误 corrections (GEC) 中，确保用户理解 correction 的理由是关键。现有的研究提供了 tokens、例子和提示，但没有直接解释 correction 的理由。虽然使用 Large Language Models (LLMs) 提供直接解释的自然语言方法已经被提出 для多个任务，但对 GEC 的方法不存在。生成 GEC  corrections 的解释 involves 对输入和输出 tokens 进行对应、确定 correction 点并提供相应的解释。然而，不是 straightforward  specify 复杂的生成格式，因为Explicit 控制生成是 difficult 的。这种研究引入一种名为 controlled generation with Prompt Insertion (PI) 的方法，使得 LLMs 可以通过自然语言来解释 correction 的理由。在 PI 中，LLMs 首先 corrections 输入文本，然后我们自动提取 correction 点基于规则。提取的 correction 点被自动插入 LLMs 的解释输出中作为提示，导引 LLMs 生成对 correction 点的解释。我们还创建了一个 Explainable GEC (XGEC) 数据集，其中包含 correction 理由的注释。虽然 GPT-3 和 ChatGPT 使用原始提示生成的 Generation 缺少一些 correction 点，但使用 PI 的生成控制可以明确指导 LLMs 生成对 correction 点的解释，从而提高生成 correction 理由的性能。
</details></li>
</ul>
<hr>
<h2 id="Kosmos-2-5-A-Multimodal-Literate-Model"><a href="#Kosmos-2-5-A-Multimodal-Literate-Model" class="headerlink" title="Kosmos-2.5: A Multimodal Literate Model"></a>Kosmos-2.5: A Multimodal Literate Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11419">http://arxiv.org/abs/2309.11419</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kyegomez/Kosmos2.5">https://github.com/kyegomez/Kosmos2.5</a></li>
<li>paper_authors: Tengchao Lv, Yupan Huang, Jingye Chen, Lei Cui, Shuming Ma, Yaoyao Chang, Shaohan Huang, Wenhui Wang, Li Dong, Weiyao Luo, Shaoxiang Wu, Guoxin Wang, Cha Zhang, Furu Wei</li>
<li>for: 这个论文是为了开发一种可以读取文本充满图像的机器学习模型，即 Kosmos-2.5。</li>
<li>methods: 该模型采用了多modal文本模型，通过共享转换器架构、任务特定的提示和灵活的文本表示来实现文本识别和文本生成任务。</li>
<li>results: 模型在终到级文档级文本识别和图像到markdown文本生成任务中表现出色，可以适应各种不同的任务，并且可以通过精度微调来适应不同的应用场景。<details>
<summary>Abstract</summary>
We present Kosmos-2.5, a multimodal literate model for machine reading of text-intensive images. Pre-trained on large-scale text-intensive images, Kosmos-2.5 excels in two distinct yet cooperative transcription tasks: (1) generating spatially-aware text blocks, where each block of text is assigned its spatial coordinates within the image, and (2) producing structured text output that captures styles and structures into the markdown format. This unified multimodal literate capability is achieved through a shared Transformer architecture, task-specific prompts, and flexible text representations. We evaluate Kosmos-2.5 on end-to-end document-level text recognition and image-to-markdown text generation. Furthermore, the model can be readily adapted for any text-intensive image understanding task with different prompts through supervised fine-tuning, making it a general-purpose tool for real-world applications involving text-rich images. This work also paves the way for the future scaling of multimodal large language models.
</details>
<details>
<summary>摘要</summary>
我们介绍Kosmos-2.5，一种多Modal literate模型，用于机器阅读图像中的文本内容。Kosmos-2.5在两个不同 yet 相互协作的译写任务中表现出色：（1）生成具有空间坐标的文本块，每个文本块在图像中被分配特定的空间坐标；（2）生成符合markdown格式的结构化文本输出。这种多Modal literate能力通过共享Transformer架构、任务特定的提示和灵活文本表示方式实现。我们对Kosmos-2.5进行了端到端文档级文本识别和图像到markdown文本生成的评估。此外，通过精心微调，可以将模型适应不同的提示任务，使其成为实际应用中文本强度图像理解任务的通用工具。此项工作也为未来扩大多Modal大语言模型的前景铺平了路。
</details></li>
</ul>
<hr>
<h2 id="Safurai-001-New-Qualitative-Approach-for-Code-LLM-Evaluation"><a href="#Safurai-001-New-Qualitative-Approach-for-Code-LLM-Evaluation" class="headerlink" title="Safurai 001: New Qualitative Approach for Code LLM Evaluation"></a>Safurai 001: New Qualitative Approach for Code LLM Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11385">http://arxiv.org/abs/2309.11385</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openai/human-eval">https://github.com/openai/human-eval</a></li>
<li>paper_authors: Davide Cifarelli, Leonardo Boiardi, Alessandro Puppo</li>
<li>for: 这个研究旨在开发一种新的大型自然语言模型（LLM），用于编程协助领域。</li>
<li>methods: 这个模型驱动了最新的编程LLM的进步，并且通过数据工程（包括最新的数据转换技术和提示工程）和指令调整来提高性能。</li>
<li>results: 研究表明，Safurai-001可以超越GPT-3.5和WizardCoder在代码可读性方面，提高1.58%和18.78%。<details>
<summary>Abstract</summary>
This paper presents Safurai-001, a new Large Language Model (LLM) with significant potential in the domain of coding assistance. Driven by recent advancements in coding LLMs, Safurai-001 competes in performance with the latest models like WizardCoder [Xu et al., 2023], PanguCoder [Shen et al., 2023] and Phi-1 [Gunasekar et al., 2023] but aims to deliver a more conversational interaction. By capitalizing on the progress in data engineering (including latest techniques of data transformation and prompt engineering) and instruction tuning, this new model promises to stand toe-to-toe with recent closed and open source developments. Recognizing the need for an efficacious evaluation metric for coding LLMs, this paper also introduces GPT4-based MultiParameters, an evaluation benchmark that harnesses varied parameters to present a comprehensive insight into the models functioning and performance. Our assessment shows that Safurai-001 can outperform GPT-3.5 by 1.58% and WizardCoder by 18.78% in the Code Readability parameter and more.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Studying-Lobby-Influence-in-the-European-Parliament"><a href="#Studying-Lobby-Influence-in-the-European-Parliament" class="headerlink" title="Studying Lobby Influence in the European Parliament"></a>Studying Lobby Influence in the European Parliament</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11381">http://arxiv.org/abs/2309.11381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aswin Suresh, Lazar Radojevic, Francesco Salvi, Antoine Magron, Victor Kristof, Matthias Grossglauser</li>
<li>for: 这个研究是为了研究欧洲议会（EP）的法制 процесса中利益集团（ Lobby）的影响。</li>
<li>methods: 这个研究使用自然语言处理（NLP）技术，收集和分析了欧洲议会成员（MEP）的言论和利益集团的 pozition 纸。通过比较这些文本的语义相似性和推论，发现MEP和利益集团之间的可解释的连接。在缺乏ground truth数据的情况下，我们进行了间接验证，比较发现的连接与我们自己 curaated的Retweet链接和公开的MEP会议记录。我们的best方法得到了0.77的AUC分数，与多个基线相比显著性更高。</li>
<li>results: 我们的结果表明，在欧洲议会的法制 процесса中，利益集团对MEP的影响存在可解释的连接。我们对 relate  Lobby 和政治分组的MEP进行了汇总分析，发现与政治分组的意识相符（例如，中间左派组织与社会问题相关）。我们认为这项研究、方法、数据和结果，是为了提高民主机构内复杂决策过程的透明度做出了一步前进。<details>
<summary>Abstract</summary>
We present a method based on natural language processing (NLP), for studying the influence of interest groups (lobbies) in the law-making process in the European Parliament (EP). We collect and analyze novel datasets of lobbies' position papers and speeches made by members of the EP (MEPs). By comparing these texts on the basis of semantic similarity and entailment, we are able to discover interpretable links between MEPs and lobbies. In the absence of a ground-truth dataset of such links, we perform an indirect validation by comparing the discovered links with a dataset, which we curate, of retweet links between MEPs and lobbies, and with the publicly disclosed meetings of MEPs. Our best method achieves an AUC score of 0.77 and performs significantly better than several baselines. Moreover, an aggregate analysis of the discovered links, between groups of related lobbies and political groups of MEPs, correspond to the expectations from the ideology of the groups (e.g., center-left groups are associated with social causes). We believe that this work, which encompasses the methodology, datasets, and results, is a step towards enhancing the transparency of the intricate decision-making processes within democratic institutions.
</details>
<details>
<summary>摘要</summary>
我们提出了基于自然语言处理（NLP）的方法，用于研究欧洲议会（EP）中利益集团（游说者）的影响力。我们收集了和分析了游说者的位置纸和EP议员（MEP）的演讲文本。通过比较这些文本的含义相似性和推导关系，我们能够发现MEP和游说者之间的可读取连接。在没有ground truth datasets的情况下，我们进行了间接验证，比较发现的连接与我们自己curate的推特链接和MEP公开的会议记录。我们的最佳方法在AUC分数0.77达到了，并与多个基eline相比表现出色。此外，我们对发现的连接进行了聚合分析，发现与相关的游说者和政治组织相对应。这种结果与政治组织的意识相符，例如中间左派组织与社会问题相关。我们认为这种方法、数据和结果是推进民主机构内复杂决策过程的一步。
</details></li>
</ul>
<hr>
<h2 id="GECTurk-Grammatical-Error-Correction-and-Detection-Dataset-for-Turkish"><a href="#GECTurk-Grammatical-Error-Correction-and-Detection-Dataset-for-Turkish" class="headerlink" title="GECTurk: Grammatical Error Correction and Detection Dataset for Turkish"></a>GECTurk: Grammatical Error Correction and Detection Dataset for Turkish</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11346">http://arxiv.org/abs/2309.11346</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/GGLAB-KU/gecturk">https://github.com/GGLAB-KU/gecturk</a></li>
<li>paper_authors: Atakan Kara, Farrin Marouf Sofian, Andrew Bond, Gözde Gül Şahin</li>
<li>for: 这个论文的目的是提出一种可以生成高质量的同步数据的Synthetic Data Generation Pipeline，用于解决土耳其语自然语言处理 tasks 中的数据缺乏问题。</li>
<li>methods: 这个论文使用了多种复杂的变换函数来实现更 than 20 个专家修改后的语法和拼写规则，并从专业编辑的文章中 derivation 了130,000个高质量的同步句子。</li>
<li>results: 这个论文通过三种基线模型（neural machine translation, sequence tagging, prefix tuning）实现了强大的结果，并通过对各种尘肤数据进行详细的实验来证明了该论文的可重复性和稳定性。<details>
<summary>Abstract</summary>
Grammatical Error Detection and Correction (GEC) tools have proven useful for native speakers and second language learners. Developing such tools requires a large amount of parallel, annotated data, which is unavailable for most languages. Synthetic data generation is a common practice to overcome the scarcity of such data. However, it is not straightforward for morphologically rich languages like Turkish due to complex writing rules that require phonological, morphological, and syntactic information. In this work, we present a flexible and extensible synthetic data generation pipeline for Turkish covering more than 20 expert-curated grammar and spelling rules (a.k.a., writing rules) implemented through complex transformation functions. Using this pipeline, we derive 130,000 high-quality parallel sentences from professionally edited articles. Additionally, we create a more realistic test set by manually annotating a set of movie reviews. We implement three baselines formulating the task as i) neural machine translation, ii) sequence tagging, and iii) prefix tuning with a pretrained decoder-only model, achieving strong results. Furthermore, we perform exhaustive experiments on out-of-domain datasets to gain insights on the transferability and robustness of the proposed approaches. Our results suggest that our corpus, GECTurk, is high-quality and allows knowledge transfer for the out-of-domain setting. To encourage further research on Turkish GEC, we release our datasets, baseline models, and the synthetic data generation pipeline at https://github.com/GGLAB-KU/gecturk.
</details>
<details>
<summary>摘要</summary>
grammatical error detection和修正工具（GEC）对本地语言和第二语言学习者都有用。开发这些工具需要大量并行、注释的数据，但这些数据对大多数语言而言罕见。Synthetic data生成是一种常见的办法来解决这个问题。然而，对于 morphologically rich的语言如土耳其来说，Synthetic data生成并不简单，因为它们的写作规则需要 fonological、morphological和 sintactic信息。在这种情况下，我们提出了一种灵活可扩展的Synthetic data生成管道，可以覆盖More than 20个专家精心编辑的语法和拼写规则（即写作规则），通过复杂的转换函数来实现。通过这种管道，我们得到了130,000个高质量的并行句子，并创建了一个更真实的测试集，通过手动注释一些电影评论。我们实现了三种基线，即 neural machine translation、sequence tagging 和 prefix tuning with a pretrained decoder-only model，取得了出色的结果。此外，我们进行了详细的对out-of-domain数据集的实验，以了解提案方法的传输性和稳定性。我们的结果表明，我们的句子库，GECTurk，具有高质量，并允许知识传输到out-of-domain Setting。为了促进土耳其GEC的研究，我们在https://github.com/GGLAB-KU/gecturk上发布了我们的数据集、基线模型和Synthetic data生成管道。
</details></li>
</ul>
<hr>
<h2 id="Improving-Article-Classification-with-Edge-Heterogeneous-Graph-Neural-Networks"><a href="#Improving-Article-Classification-with-Edge-Heterogeneous-Graph-Neural-Networks" class="headerlink" title="Improving Article Classification with Edge-Heterogeneous Graph Neural Networks"></a>Improving Article Classification with Edge-Heterogeneous Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11341">http://arxiv.org/abs/2309.11341</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lyvykhang/edgehetero-nodeproppred">https://github.com/lyvykhang/edgehetero-nodeproppred</a></li>
<li>paper_authors: Khang Ly, Yury Kashnitsky, Savvas Chamezopoulos, Valeria Krzhizhanovskaya</li>
<li>for: 这个研究旨在提高文章分类的性能，使用简单的图 neural network (GNN) 拓扑，并将文章中的文本metadata 转化为高级 semantics。</li>
<li>methods: 该研究使用 SciBERT 生成节点特征，以捕捉文章中的高级 semantics。然后，使用完全supervised 推学的node classification 进行实验，使用 Open Graph Benchmark (OGB) ogbn-arxiv 数据集和 PubMed 肥瘤数据集。</li>
<li>results: 结果表明，使用edge-heterogeneous graphs 可以提高 GNN 模型的性能，而且可以使用简单和浅的 GNN 拓扑来达到与更复杂的结构相同的性能。在 OGB 竞赛中，我们获得了第15名的成绩（准确率 74.61%），并在 PubMed 数据集上与 state-of-the-art GNN 结构相当（准确率 89.88%）。<details>
<summary>Abstract</summary>
Classifying research output into context-specific label taxonomies is a challenging and relevant downstream task, given the volume of existing and newly published articles. We propose a method to enhance the performance of article classification by enriching simple Graph Neural Networks (GNN) pipelines with edge-heterogeneous graph representations. SciBERT is used for node feature generation to capture higher-order semantics within the articles' textual metadata. Fully supervised transductive node classification experiments are conducted on the Open Graph Benchmark (OGB) ogbn-arxiv dataset and the PubMed diabetes dataset, augmented with additional metadata from Microsoft Academic Graph (MAG) and PubMed Central, respectively. The results demonstrate that edge-heterogeneous graphs consistently improve the performance of all GNN models compared to the edge-homogeneous graphs. The transformed data enable simple and shallow GNN pipelines to achieve results on par with more complex architectures. On ogbn-arxiv, we achieve a top-15 result in the OGB competition with a 2-layer GCN (accuracy 74.61%), being the highest-scoring solution with sub-1 million parameters. On PubMed, we closely trail SOTA GNN architectures using a 2-layer GraphSAGE by including additional co-authorship edges in the graph (accuracy 89.88%). The implementation is available at: $\href{https://github.com/lyvykhang/edgehetero-nodeproppred}{\text{https://github.com/lyvykhang/edgehetero-nodeproppred}$.
</details>
<details>
<summary>摘要</summary>
classe research output into context-specific label taxonomies 是一个复杂且有 relevance 的下游任务， giventhe volume of existing 和 newly published articles. We propose a method to enhance the performance of article classification by enriching simple Graph Neural Networks (GNN) pipelines with edge-heterogeneous graph representations. SciBERT is used for node feature generation to capture higher-order semantics within the articles' textual metadata. Fully supervised transductive node classification experiments are conducted on the Open Graph Benchmark (OGB) ogbn-arxiv dataset and the PubMed diabetes dataset, augmented with additional metadata from Microsoft Academic Graph (MAG) and PubMed Central, respectively. The results demonstrate that edge-heterogeneous graphs consistently improve the performance of all GNN models compared to the edge-homogeneous graphs. The transformed data enable simple and shallow GNN pipelines to achieve results on par with more complex architectures. On ogbn-arxiv, we achieve a top-15 result in the OGB competition with a 2-layer GCN (accuracy 74.61%), being the highest-scoring solution with sub-1 million parameters. On PubMed, we closely trail SOTA GNN architectures using a 2-layer GraphSAGE by including additional co-authorship edges in the graph (accuracy 89.88%). The implementation is available at: $\href{https://github.com/lyvykhang/edgehetero-nodeproppred}{\text{https://github.com/lyvykhang/edgehetero-nodeproppred}$.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Data-Collection-and-Unsupervised-Learning-for-Code-switched-Tunisian-Arabic-Automatic-Speech-Recognition"><a href="#Leveraging-Data-Collection-and-Unsupervised-Learning-for-Code-switched-Tunisian-Arabic-Automatic-Speech-Recognition" class="headerlink" title="Leveraging Data Collection and Unsupervised Learning for Code-switched Tunisian Arabic Automatic Speech Recognition"></a>Leveraging Data Collection and Unsupervised Learning for Code-switched Tunisian Arabic Automatic Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11327">http://arxiv.org/abs/2309.11327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Amine Ben Abdallah, Ata Kabboudi, Amir Kanoun, Salah Zaiem</li>
<li>for: This paper is written for the purpose of developing an effective Automatic Speech Recognition (ASR) solution for dialects, specifically focusing on the Tunisian dialect.</li>
<li>methods: The paper explores self-supervision, semi-supervision, and few-shot code-switching approaches to improve the state-of-the-art in ASR for Tunisian Arabic, English, and French.</li>
<li>results: The paper produces human evaluations of transcripts to avoid the noise coming from spelling inadequacies in testing references, and the models are able to transcribe audio samples in a linguistic mix involving Tunisian Arabic, English, and French. The data used during training and testing are released for public use and further improvements.<details>
<summary>Abstract</summary>
Crafting an effective Automatic Speech Recognition (ASR) solution for dialects demands innovative approaches that not only address the data scarcity issue but also navigate the intricacies of linguistic diversity. In this paper, we address the aforementioned ASR challenge, focusing on the Tunisian dialect. First, textual and audio data is collected and in some cases annotated. Second, we explore self-supervision, semi-supervision and few-shot code-switching approaches to push the state-of-the-art on different Tunisian test sets; covering different acoustic, linguistic and prosodic conditions. Finally, and given the absence of conventional spelling, we produce a human evaluation of our transcripts to avoid the noise coming from spelling inadequacies in our testing references. Our models, allowing to transcribe audio samples in a linguistic mix involving Tunisian Arabic, English and French, and all the data used during training and testing are released for public use and further improvements.
</details>
<details>
<summary>摘要</summary>
制定一个有效的自动语音识别（ASR）解决方案 для方言需要创新的方法，不仅解决数据缺乏问题，还能够探索方言语言多样性的细节。在这篇论文中，我们关注了前述的ASR挑战，将着眼点在突尼斯方言。首先，我们收集了文本和音频数据，并在某些情况下进行了标注。其次，我们探索了无监督、半监督和少量代码交换的方法，以在不同的突尼斯测试集上提高状态。这些测试集涵盖了不同的听音、语言和语调条件。最后，由于没有传统的拼写法，我们进行了人工评估我们的讲文，以避免测试参考中的杂音。我们的模型可以将突尼斯阿拉伯语、英语和法语混合的语音样本转录为文本，并在训练和测试中使用的所有数据都公开发布，以便进一步的改进。
</details></li>
</ul>
<hr>
<h2 id="DISC-LawLLM-Fine-tuning-Large-Language-Models-for-Intelligent-Legal-Services"><a href="#DISC-LawLLM-Fine-tuning-Large-Language-Models-for-Intelligent-Legal-Services" class="headerlink" title="DISC-LawLLM: Fine-tuning Large Language Models for Intelligent Legal Services"></a>DISC-LawLLM: Fine-tuning Large Language Models for Intelligent Legal Services</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11325">http://arxiv.org/abs/2309.11325</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengbin Yue, Wei Chen, Siyuan Wang, Bingxuan Li, Chenchen Shen, Shujun Liu, Yuxuan Zhou, Yao Xiao, Song Yun, Wei Lin, Xuanjing Huang, Zhongyu Wei</li>
<li>for: 这个论文是为了提供一种智能法律系统，使得法律服务可以更加全面和智能化。</li>
<li>methods: 论文使用大量语言模型（LLMs），并采用法律逻辑提示策略来构建监督精度训练集。同时，文章还增强了模型的知识访问和利用能力。</li>
<li>results: 论文通过对DISC-Law-Eval测试集进行量化和资深评价， demonstarted了其在不同的法律场景中的效果。详细的资源可以在<a target="_blank" rel="noopener" href="https://github.com/FudanDISC/DISC-LawLLM%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/FudanDISC/DISC-LawLLM上找到。</a><details>
<summary>Abstract</summary>
We propose DISC-LawLLM, an intelligent legal system utilizing large language models (LLMs) to provide a wide range of legal services. We adopt legal syllogism prompting strategies to construct supervised fine-tuning datasets in the Chinese Judicial domain and fine-tune LLMs with legal reasoning capability. We augment LLMs with a retrieval module to enhance models' ability to access and utilize external legal knowledge. A comprehensive legal benchmark, DISC-Law-Eval, is presented to evaluate intelligent legal systems from both objective and subjective dimensions. Quantitative and qualitative results on DISC-Law-Eval demonstrate the effectiveness of our system in serving various users across diverse legal scenarios. The detailed resources are available at https://github.com/FudanDISC/DISC-LawLLM.
</details>
<details>
<summary>摘要</summary>
我们提出了DISC-LawLLM，一种智能法律系统，使用大型自然语言模型（LLM）提供广泛的法律服务。我们采用法律逻辑提示策略构建监督精度训练集，在中国司法领域进行超参数 fine-tuning，以提高模型的法律推理能力。我们将LLM加载一个检索模块，以提高模型对外部法律知识的访问和利用能力。我们提供了一个全面的法律评价指标，DISC-Law-Eval，以评估智能法律系统的效果从客观和主观两个角度。我们对DISC-Law-Eval进行了量化和质量的测试，结果表明我们的系统在多种法律场景下可以为用户提供有效的服务。详细的资源可以在https://github.com/FudanDISC/DISC-LawLLM上找到。
</details></li>
</ul>
<hr>
<h2 id="The-Wizard-of-Curiosities-Enriching-Dialogues-with-Fun-Facts"><a href="#The-Wizard-of-Curiosities-Enriching-Dialogues-with-Fun-Facts" class="headerlink" title="The Wizard of Curiosities: Enriching Dialogues with Fun Facts"></a>The Wizard of Curiosities: Enriching Dialogues with Fun Facts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11283">http://arxiv.org/abs/2309.11283</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frederico Vicente, Rafael Ferreira, David Semedo, João Magalhães</li>
<li>for: 本研究旨在增加对对话系统的用户体验，通过引入吸引人的启示。</li>
<li>methods: 本研究使用了来自Amazon Alexa TaskBot挑战的真实人机对话，并对这些对话进行了精心准备和编辑，以创造一组具有Contextualized curiosities的对话。</li>
<li>results: 根据对Over 1000对话的A&#x2F;B测试表明，启示可以不 только增加用户参与度，还提高用户的平均相对评价值9.7%。<details>
<summary>Abstract</summary>
Introducing curiosities in a conversation is a way to teach something new to the person in a pleasant and enjoyable way. Enriching dialogues with contextualized curiosities can improve the users' perception of a dialog system and their overall user experience. In this paper, we introduce a set of curated curiosities, targeting dialogues in the cooking and DIY domains. In particular, we use real human-agent conversations collected in the context of the Amazon Alexa TaskBot challenge, a multimodal and multi-turn conversational setting. According to an A/B test with over 1000 conversations, curiosities not only increase user engagement, but provide an average relative rating improvement of 9.7%.
</details>
<details>
<summary>摘要</summary>
在对话中引入curiosities是一种教育用户新知识的有趣和愉悦的方式。在对话中添加上下文化curiosities可以提高对对话系统的评估和用户总体体验。在这篇论文中，我们介绍了一个 curae的curiosities集合，targeting cooking和DIY对话。特别是，我们使用了来自Amazon Alexa TaskBot挑战的真实人机对话收集，一种多媒体和多turn对话Setting。据A/B测试，curiosities不仅提高了用户参与度，还提供了9.7%的相对评分提升。
</details></li>
</ul>
<hr>
<h2 id="The-Scenario-Refiner-Grounding-subjects-in-images-at-the-morphological-level"><a href="#The-Scenario-Refiner-Grounding-subjects-in-images-at-the-morphological-level" class="headerlink" title="The Scenario Refiner: Grounding subjects in images at the morphological level"></a>The Scenario Refiner: Grounding subjects in images at the morphological level</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11252">http://arxiv.org/abs/2309.11252</a></li>
<li>repo_url: None</li>
<li>paper_authors: Claudia Tagliaferri, Sofia Axioti, Albert Gatt, Denis Paperno</li>
<li>for: 这个论文是用来检验语言模型是否能够捕捉语言中的微妙特征的。</li>
<li>methods: 该论文使用了一种新的方法和数据集来测试语言模型是否能够在 morphological 水平上捕捉语言中的差异。</li>
<li>results: 研究发现，语言模型的预测与人类参与者的判断存在差异，尤其是在 grammatical 方面存在偏向。<details>
<summary>Abstract</summary>
Derivationally related words, such as "runner" and "running", exhibit semantic differences which also elicit different visual scenarios. In this paper, we ask whether Vision and Language (V\&L) models capture such distinctions at the morphological level, using a a new methodology and dataset. We compare the results from V\&L models to human judgements and find that models' predictions differ from those of human participants, in particular displaying a grammatical bias. We further investigate whether the human-model misalignment is related to model architecture. Our methodology, developed on one specific morphological contrast, can be further extended for testing models on capturing other nuanced language features.
</details>
<details>
<summary>摘要</summary>
derivationally related words, such as "runner" and "running", exhibit semantic differences which also elicit different visual scenarios. In this paper, we ask whether Vision and Language (V&L) models capture such distinctions at the morphological level, using a new methodology and dataset. We compare the results from V&L models to human judgements and find that models' predictions differ from those of human participants, in particular displaying a grammatical bias. We further investigate whether the human-model misalignment is related to model architecture. Our methodology, developed on one specific morphological contrast, can be further extended for testing models on capturing other nuanced language features.Here's the translation in Traditional Chinese as well: derivationally related words, such as "runner" and "running", exhibit semantic differences which also elicit different visual scenarios. In this paper, we ask whether Vision and Language (V&L) models capture such distinctions at the morphological level, using a new methodology and dataset. We compare the results from V&L models to human judgements and find that models' predictions differ from those of human participants, in particular displaying a grammatical bias. We further investigate whether the human-model misalignment is related to model architecture. Our methodology, developed on one specific morphological contrast, can be further extended for testing models on capturing other nuanced language features.
</details></li>
</ul>
<hr>
<h2 id="OpenChat-Advancing-Open-source-Language-Models-with-Mixed-Quality-Data"><a href="#OpenChat-Advancing-Open-source-Language-Models-with-Mixed-Quality-Data" class="headerlink" title="OpenChat: Advancing Open-source Language Models with Mixed-Quality Data"></a>OpenChat: Advancing Open-source Language Models with Mixed-Quality Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11235">http://arxiv.org/abs/2309.11235</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/imoneoi/openchat">https://github.com/imoneoi/openchat</a></li>
<li>paper_authors: Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, Yang Liu</li>
<li>for: 这个论文是为了提高开源语言模型的性能而写的，特别是使用混合质量数据进行超参数调整。</li>
<li>methods: 该论文提出了一种名为 OpenChat 的框架，用于使用混合质量数据进行超参数调整，并使用 Conditioned-Reinforcement Learning Fine-Tuning (C-RLFT) 方法。</li>
<li>results: 该论文的实验表明，使用 OpenChat 框架和 C-RLFT 方法可以提高开源语言模型的性能，并且在三个标准的 bencmark 上 achieved the highest average performance 中。<details>
<summary>Abstract</summary>
Nowadays, open-source large language models like LLaMA have emerged. Recent developments have incorporated supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to align these models with human goals. However, SFT methods treat all training data with mixed quality equally, while RLFT methods require high-quality pairwise or ranking-based preference data. In this study, we present a novel framework, named OpenChat, to advance open-source language models with mixed-quality data. Specifically, we consider the general SFT training data, consisting of a small amount of expert data mixed with a large proportion of sub-optimal data, without any preference labels. We propose the C(onditioned)-RLFT, which regards different data sources as coarse-grained reward labels and learns a class-conditioned policy to leverage complementary data quality information. Interestingly, the optimal policy in C-RLFT can be easily solved through single-stage, RL-free supervised learning, which is lightweight and avoids costly human preference labeling. Through extensive experiments on three standard benchmarks, our openchat-13b fine-tuned with C-RLFT achieves the highest average performance among all 13b open-source language models. Moreover, we use AGIEval to validate the model generalization performance, in which only openchat-13b surpasses the base model. Finally, we conduct a series of analyses to shed light on the effectiveness and robustness of OpenChat. Our code, data, and models are publicly available at https://github.com/imoneoi/openchat.
</details>
<details>
<summary>摘要</summary>
现在，开源大语言模型如LLaMA已经出现。最近的发展包括监督精细调教（SFT）和奖励学习调教（RLFT），以使模型与人类目标 better alignment。然而，SFT方法将所有训练数据视为一样的质量，而RLFT方法需要高质量的对数据进行对比或排名。在这种研究中，我们提出了一种新的框架，名为OpenChat，以提高开源语言模型的质量。 Specifically，我们考虑了通用的SFT训练数据，包括一小量的专家数据和大量的不优化数据，无需任何偏好标签。我们提议了C（条件）-RLFT，它将不同的数据来源视为粗粒化奖励标签，并学习一个类别 Conditioned 策略，以利用不同数据质量信息。有趣的是，C-RLFT 的优化策略可以通过单阶段、RL-free 监督学习，以轻量级和避免高昂的人类偏好标签。经过广泛的实验，我们的 openchat-13b 通过 C-RLFT 进行微调，在三个标准 bench mark 上 achieve 所有 13b 开源语言模型的最高平均性能。此外，我们使用 AGIEval 验证模型的通用性能，只有 openchat-13b 在基础模型之上超越。最后，我们进行了一系列的分析，以证明 OpenChat 的效果和可靠性。我们的代码、数据和模型都可以在 https://github.com/imoneoi/openchat 上获取。
</details></li>
</ul>
<hr>
<h2 id="Speak-While-You-Think-Streaming-Speech-Synthesis-During-Text-Generation"><a href="#Speak-While-You-Think-Streaming-Speech-Synthesis-During-Text-Generation" class="headerlink" title="Speak While You Think: Streaming Speech Synthesis During Text Generation"></a>Speak While You Think: Streaming Speech Synthesis During Text Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11210">http://arxiv.org/abs/2309.11210</a></li>
<li>repo_url: None</li>
<li>paper_authors: Avihu Dekel, Slava Shechtman, Raul Fernandez, David Haws, Zvi Kons, Ron Hoory</li>
<li>for: 这篇论文是为了解决大语言模型（LLM）的交互问题，使得LLM可以更加流畅地进行语音交互。</li>
<li>methods: 该论文提出了一种名为LM2Speech的架构，它可以在文本生成过程中同时生成语音，从而减少了对话延迟。LM2Speech模仿了一个非流动教师模型的预测，同时限制了未来上下文的暴露，以便实现流动。它利用了LLM的隐藏嵌入，这是文本生成过程中的一个侧产品，它含有有用的语义上下文。</li>
<li>results: 实验结果表明，LM2Speech可以保持教师模型的质量，同时减少对话延迟，以便实现自然的语音交互。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) demonstrate impressive capabilities, yet interaction with these models is mostly facilitated through text. Using Text-To-Speech to synthesize LLM outputs typically results in notable latency, which is impractical for fluent voice conversations. We propose LLM2Speech, an architecture to synthesize speech while text is being generated by an LLM which yields significant latency reduction. LLM2Speech mimics the predictions of a non-streaming teacher model while limiting the exposure to future context in order to enable streaming. It exploits the hidden embeddings of the LLM, a by-product of the text generation that contains informative semantic context. Experimental results show that LLM2Speech maintains the teacher's quality while reducing the latency to enable natural conversations.
</details>
<details>
<summary>摘要</summary>
大语言模型（LLM）显示出很强的能力，然而与这些模型交互通常是通过文本进行的。使用文本到语音synthesize LLM输出通常会导致很长的延迟，这对于流畅的语音对话不实用。我们提议LLM2Speech，一种架构可以在文本生成过程中同时synthesize语音，从而减少延迟。LLM2Speech模仿教师模型的预测，限制未来上下文的暴露，以便实现流动。它利用LLM的隐藏嵌入，这是文本生成过程的产物，含有有用的semanticContext。实验结果表明，LLM2Speech可以保持教师的质量，同时减少延迟，以便实现自然的对话。
</details></li>
</ul>
<hr>
<h2 id="The-Languini-Kitchen-Enabling-Language-Modelling-Research-at-Different-Scales-of-Compute"><a href="#The-Languini-Kitchen-Enabling-Language-Modelling-Research-at-Different-Scales-of-Compute" class="headerlink" title="The Languini Kitchen: Enabling Language Modelling Research at Different Scales of Compute"></a>The Languini Kitchen: Enabling Language Modelling Research at Different Scales of Compute</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11197">http://arxiv.org/abs/2309.11197</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aleksandar Stanić, Dylan Ashley, Oleg Serikov, Louis Kirsch, Francesco Faccio, Jürgen Schmidhuber, Thomas Hofmann, Imanol Schlag</li>
<li>for:  This paper aims to provide a fair comparison of language modeling methods based on their empirical scaling trends, and to serve as a foundation for meaningful and reproducible research in the field.</li>
<li>methods: The paper introduces an experimental protocol that enables model comparisons based on equivalent compute, measured in accelerator hours, and uses a pre-processed dataset of books to evaluate the methods.</li>
<li>results: The paper shows that the LSTM baseline exhibits a predictable and more favourable scaling law than the GPT baseline, and that the two models intersect at roughly 50,000 accelerator hours.Here is the text in Simplified Chinese:</li>
<li>for: 这篇论文的目的是为语言模型比较提供公平的比较基础，并为语言模型研究提供可重复的基础。</li>
<li>methods: 论文提出了一种实验协议，使得模型比较基于等效计算时间（ measured in accelerator hours）进行。为了评价方法，文章使用了一个已经处理过的大型、多样化、高质量的书籍数据集。</li>
<li>results: 论文显示，LSTM基eline在计算时间上采取了一种可预测的和更有利的整体增长规律，而GPT基eline在所有等效计算时间水平上都保持了更好的折衣率。两个基eline在约50,000个加速器小时上交叉。<details>
<summary>Abstract</summary>
The Languini Kitchen serves as both a research collective and codebase designed to empower researchers with limited computational resources to contribute meaningfully to the field of language modelling. We introduce an experimental protocol that enables model comparisons based on equivalent compute, measured in accelerator hours. The number of tokens on which a model is trained is defined by the model's throughput and the chosen compute class. Notably, this approach avoids constraints on critical hyperparameters which affect total parameters or floating-point operations. For evaluation, we pre-process an existing large, diverse, and high-quality dataset of books that surpasses existing academic benchmarks in quality, diversity, and document length. On it, we compare methods based on their empirical scaling trends which are estimated through experiments at various levels of compute. This work also provides two baseline models: a feed-forward model derived from the GPT-2 architecture and a recurrent model in the form of a novel LSTM with ten-fold throughput. While the GPT baseline achieves better perplexity throughout all our levels of compute, our LSTM baseline exhibits a predictable and more favourable scaling law. This is due to the improved throughput and the need for fewer training tokens to achieve the same decrease in test perplexity. Extrapolating the scaling laws leads of both models results in an intersection at roughly 50,000 accelerator hours. We hope this work can serve as the foundation for meaningful and reproducible language modelling research.
</details>
<details>
<summary>摘要</summary>
蓝夷面厨房 serves as both a research collective and codebase, designed to empower researchers with limited computational resources to contribute meaningfully to the field of language modeling. We introduce an experimental protocol that enables model comparisons based on equivalent compute, measured in accelerator hours. The number of tokens on which a model is trained is defined by the model's throughput and the chosen compute class. Notably, this approach avoids constraints on critical hyperparameters which affect total parameters or floating-point operations. For evaluation, we pre-process an existing large, diverse, and high-quality dataset of books that surpasses existing academic benchmarks in quality, diversity, and document length. On it, we compare methods based on their empirical scaling trends which are estimated through experiments at various levels of compute. This work also provides two baseline models: a feed-forward model derived from the GPT-2 architecture and a recurrent model in the form of a novel LSTM with ten-fold throughput. While the GPT baseline achieves better perplexity throughout all our levels of compute, our LSTM baseline exhibits a predictable and more favourable scaling law. This is due to the improved throughput and the need for fewer training tokens to achieve the same decrease in test perplexity. Extrapolating the scaling laws leads of both models results in an intersection at roughly 50,000 accelerator hours. We hope this work can serve as the foundation for meaningful and reproducible language modeling research.
</details></li>
</ul>
<hr>
<h2 id="Assessment-of-Pre-Trained-Models-Across-Languages-and-Grammars"><a href="#Assessment-of-Pre-Trained-Models-Across-Languages-and-Grammars" class="headerlink" title="Assessment of Pre-Trained Models Across Languages and Grammars"></a>Assessment of Pre-Trained Models Across Languages and Grammars</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11165">http://arxiv.org/abs/2309.11165</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amunozo/multilingual-assessment">https://github.com/amunozo/multilingual-assessment</a></li>
<li>paper_authors: Alberto Muñoz-Ortiz, David Vilares, Carlos Gómez-Rodríguez</li>
<li>for: 这个研究是为了评估多语言大型自然语言处理器（LLMs）如何学习语法结构。</li>
<li>methods: 该研究使用了抽象到多形式语法结构的方法，包括将解析视为序列标签。</li>
<li>results: 研究发现：（一）框架在不同编码下具有一致性，（二）预训练词词 vectors 不会偏好语法树表示于dependency表示，（三）使用字符串分词是需要表示语法结构的，与字符串模型不同，（四）语言出现在预训练数据中的频率比任务数据更重要于从词词 vectors 中恢复语法结构。<details>
<summary>Abstract</summary>
We present an approach for assessing how multilingual large language models (LLMs) learn syntax in terms of multi-formalism syntactic structures. We aim to recover constituent and dependency structures by casting parsing as sequence labeling. To do so, we select a few LLMs and study them on 13 diverse UD treebanks for dependency parsing and 10 treebanks for constituent parsing. Our results show that: (i) the framework is consistent across encodings, (ii) pre-trained word vectors do not favor constituency representations of syntax over dependencies, (iii) sub-word tokenization is needed to represent syntax, in contrast to character-based models, and (iv) occurrence of a language in the pretraining data is more important than the amount of task data when recovering syntax from the word vectors.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法，用于评估多语言大型自然语言处理器（LLM）在多形式语法结构中学习语法的方式。我们希望通过将分析转换为序列标签来恢复句子和依赖结构。为此，我们选择了一些LLM并对13种UD treebanks进行了依赖分析和10种treebanks进行了句子分析。我们的结果表明：（i）框架在不同编码中具有一致性，（ii）预训练词词 vec 不倾向于 syntax 中的句子表示，（iii）字符串分词是必要的，而不是字符串模型，以表示语法，（iv）预training数据中语言的出现次数高于任务数据时，可以更好地从词 vectors 中恢复语法。
</details></li>
</ul>
<hr>
<h2 id="Prototype-of-a-robotic-system-to-assist-the-learning-process-of-English-language-with-text-generation-through-DNN"><a href="#Prototype-of-a-robotic-system-to-assist-the-learning-process-of-English-language-with-text-generation-through-DNN" class="headerlink" title="Prototype of a robotic system to assist the learning process of English language with text-generation through DNN"></a>Prototype of a robotic system to assist the learning process of English language with text-generation through DNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11142">http://arxiv.org/abs/2309.11142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlos Morales-Torres, Mario Campos-Soberanis, Diego Campos-Sobrino</li>
<li>for: 这个论文是为了帮助英语自学者提高英语水平的。</li>
<li>methods: 这个论文使用了Long Short Term Memory（LSTM）神经网络来生成文本，learners通过图形用户界面与系统互动，系统根据学生的英语水平生成文本。</li>
<li>results: 实验结果显示，learners与系统互动后，他们的 grammatical Range 有所提高。<details>
<summary>Abstract</summary>
In the last ongoing years, there has been a significant ascending on the field of Natural Language Processing (NLP) for performing multiple tasks including English Language Teaching (ELT). An effective strategy to favor the learning process uses interactive devices to engage learners in their self-learning process. In this work, we present a working prototype of a humanoid robotic system to assist English language self-learners through text generation using Long Short Term Memory (LSTM) Neural Networks. The learners interact with the system using a Graphic User Interface that generates text according to the English level of the user. The experimentation was conducted using English learners and the results were measured accordingly to International English Language Testing System (IELTS) rubric. Preliminary results show an increment in the Grammatical Range of learners who interacted with the system.
</details>
<details>
<summary>摘要</summary>
最近几年来，自然语言处理（NLP）领域内，有许多进展，以帮助执行多种任务，包括英语教学（ELT）。一种有效的策略是使用互动设备，以吸引学生参与自学习过程。在这个工作中，我们展示了一个人工智能机器人系统，用于帮助英语自学者通过文本生成来提高英语水平。学生通过图形用户界面与系统进行交互，系统根据用户的英语水平生成文本。实验中使用了英语学习者，并根据国际英语语言考试系统（IELTS）标准进行评估结果。初步结果表明，与系统交互的学生的 grammatical range 有所增加。
</details></li>
</ul>
<hr>
<h2 id="K-pop-Lyric-Translation-Dataset-Analysis-and-Neural-Modelling"><a href="#K-pop-Lyric-Translation-Dataset-Analysis-and-Neural-Modelling" class="headerlink" title="K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling"></a>K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11093">http://arxiv.org/abs/2309.11093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haven Kim, Jongmin Jung, Dasaem Jeong, Juhan Nam</li>
<li>for: 本研究的目的是为了推广歌曲翻译研究的范围和语言，并对K-pop歌曲翻译进行系统性的研究。</li>
<li>methods: 本研究使用了一个新的歌词翻译 dataset，该 dataset包含了大约89%的K-pop歌曲歌词，并将韩语和英语歌词进行了行内和段内的对应。</li>
<li>results: 本研究发现了K-pop歌曲翻译的独特特征，与其他已经广泛研究的类型不同，同时还构建了一个基于神经网络的歌词翻译模型，从而证明了专门为歌曲翻译而设计的 dataset 的重要性。<details>
<summary>Abstract</summary>
Lyric translation, a field studied for over a century, is now attracting computational linguistics researchers. We identified two limitations in previous studies. Firstly, lyric translation studies have predominantly focused on Western genres and languages, with no previous study centering on K-pop despite its popularity. Second, the field of lyric translation suffers from a lack of publicly available datasets; to the best of our knowledge, no such dataset exists. To broaden the scope of genres and languages in lyric translation studies, we introduce a novel singable lyric translation dataset, approximately 89\% of which consists of K-pop song lyrics. This dataset aligns Korean and English lyrics line-by-line and section-by-section. We leveraged this dataset to unveil unique characteristics of K-pop lyric translation, distinguishing it from other extensively studied genres, and to construct a neural lyric translation model, thereby underscoring the importance of a dedicated dataset for singable lyric translations.
</details>
<details>
<summary>摘要</summary>
<<SYS>> transtable "Lyric translation, a field studied for over a century, is now attracting computational linguistics researchers. We identified two limitations in previous studies. Firstly, lyric translation studies have predominantly focused on Western genres and languages, with no previous study centering on K-pop despite its popularity. Second, the field of lyric translation suffers from a lack of publicly available datasets; to the best of our knowledge, no such dataset exists. To broaden the scope of genres and languages in lyric translation studies, we introduce a novel singable lyric translation dataset, approximately 89% of which consists of K-pop song lyrics. This dataset aligns Korean and English lyrics line-by-line and section-by-section. We leveraged this dataset to unveil unique characteristics of K-pop lyric translation, distinguishing it from other extensively studied genres, and to construct a neural lyric translation model, thereby underscoring the importance of a dedicated dataset for singable lyric translations."中文翻译：学术界对歌词翻译一百年来进行研究，现在吸引了计算语言学研究者。我们认为前一代研究存在两个限制：首先，歌词翻译研究主要集中在西方类型和语言上，尚未对K-pop进行过研究，尽管其受欢迎程度极高。其次，歌词翻译领域缺乏公共可用数据集，到我们所知，没有这样的数据集存在。为了扩大歌词翻译研究的类型和语言范围，我们介绍了一个新的可唱歌词翻译数据集，其中大约89%是K-pop歌曲 lyrics。这个数据集将韩语和英语歌词一行一行、段段对齐。我们利用了这个数据集，揭示了K-pop歌词翻译的独特特征，与其他广泛研究的类型区分开来，并构建了神经网络歌词翻译模型，从而强调了专门为可唱歌词翻译而设置的数据集的重要性。
</details></li>
</ul>
<hr>
<h2 id="Dual-Modal-Attention-Enhanced-Text-Video-Retrieval-with-Triplet-Partial-Margin-Contrastive-Learning"><a href="#Dual-Modal-Attention-Enhanced-Text-Video-Retrieval-with-Triplet-Partial-Margin-Contrastive-Learning" class="headerlink" title="Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning"></a>Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11082">http://arxiv.org/abs/2309.11082</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Jiang, Hong Liu, Xuzheng Yu, Qing Wang, Yuan Cheng, Jia Xu, Zhongyi Liu, Qingpei Guo, Wei Chu, Ming Yang, Yuan Qi</li>
<li>for: This paper focuses on improving text-video retrieval, which is essential for video filtering, recommendation, and search, due to the increasing amount of web videos.</li>
<li>methods: The paper proposes two novel techniques to improve contrastive learning for text-video retrieval: 1) Dual-Modal Attention-Enhanced Module (DMAE) to mine hard negative pairs, and 2) Triplet Partial Margin Contrastive Learning (TPM-CL) module to construct partial order triplet samples.</li>
<li>results: The proposed approach outperforms existing methods on four widely-used text-video retrieval datasets, including MSR-VTT, MSVD, DiDeMo, and ActivityNet.Here’s the simplified Chinese text in the format you requested:</li>
<li>for: 这篇论文关注提高文本视频相似性 retrieval，用于视频过滤、推荐和搜索，由于网络视频的快速增长。</li>
<li>methods: 该论文提出了两种新的技术来提高对文本视频相似性的探索：1）双Modal Attention-Enhanced Module (DMAE) 来挖掘困难的负例，2）Triplet Partial Margin Contrastive Learning (TPM-CL) 模块来构建partial order triplet samples。</li>
<li>results: 该提出的方法在四个常用的文本视频相似性数据集上（MSR-VTT、MSVD、DiDeMo、ActivityNet）得到了较高的性能，比如 existed 方法。<details>
<summary>Abstract</summary>
In recent years, the explosion of web videos makes text-video retrieval increasingly essential and popular for video filtering, recommendation, and search. Text-video retrieval aims to rank relevant text/video higher than irrelevant ones. The core of this task is to precisely measure the cross-modal similarity between texts and videos. Recently, contrastive learning methods have shown promising results for text-video retrieval, most of which focus on the construction of positive and negative pairs to learn text and video representations. Nevertheless, they do not pay enough attention to hard negative pairs and lack the ability to model different levels of semantic similarity. To address these two issues, this paper improves contrastive learning using two novel techniques. First, to exploit hard examples for robust discriminative power, we propose a novel Dual-Modal Attention-Enhanced Module (DMAE) to mine hard negative pairs from textual and visual clues. By further introducing a Negative-aware InfoNCE (NegNCE) loss, we are able to adaptively identify all these hard negatives and explicitly highlight their impacts in the training loss. Second, our work argues that triplet samples can better model fine-grained semantic similarity compared to pairwise samples. We thereby present a new Triplet Partial Margin Contrastive Learning (TPM-CL) module to construct partial order triplet samples by automatically generating fine-grained hard negatives for matched text-video pairs. The proposed TPM-CL designs an adaptive token masking strategy with cross-modal interaction to model subtle semantic differences. Extensive experiments demonstrate that the proposed approach outperforms existing methods on four widely-used text-video retrieval datasets, including MSR-VTT, MSVD, DiDeMo and ActivityNet.
</details>
<details>
<summary>摘要</summary>
近年来，Web视频的爆炸式增长使得文本视频检索变得越来越重要和受欢迎，用于视频筛选、推荐和搜索。文本视频检索的目标是将相关的文本和视频排名在不相关的文本和视频之前。核心任务是准确度量文本和视频之间的跨Modal相似性。在这个任务中，对照学习方法已经取得了显著成果，大多数方法都是通过建立正例和反例来学习文本和视频表示。然而，这些方法往往忽略硬例和不同水平的 semantic similarity。为了解决这两个问题，本文提出了两种新的技术：首先，我们提出了一种双Modal注意力增强模块（DMAE），以挖掘文本和视频中的硬例。其次，我们引入了一种Negative-aware InfoNCE（NegNCE）损失函数，以适应性地标识和特别强调硬例的影响。其次，我们 argue that triplet samples可以更好地模型细致的 semantic similarity，而不是pairwise samples。我们因此提出了一种新的Triplet Partial Margin Contrastive Learning（TPM-CL）模块，通过自动生成匹配的文本视频对的硬例来建立 partial order triplet samples。TPM-CL模块还设计了一种自适应的token掩码策略，以模型文本和视频之间的跨Modal差异。经过广泛的实验，我们发现，提出的方法在四个常用的文本视频检索数据集上都能够达到更高的性能。
</details></li>
</ul>
<hr>
<h2 id="UniPCM-Universal-Pre-trained-Conversation-Model-with-Task-aware-Automatic-Prompt"><a href="#UniPCM-Universal-Pre-trained-Conversation-Model-with-Task-aware-Automatic-Prompt" class="headerlink" title="UniPCM: Universal Pre-trained Conversation Model with Task-aware Automatic Prompt"></a>UniPCM: Universal Pre-trained Conversation Model with Task-aware Automatic Prompt</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11065">http://arxiv.org/abs/2309.11065</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cycrab/unipcm">https://github.com/cycrab/unipcm</a></li>
<li>paper_authors: Yucheng Cai, Wentao Ma, Yuchuan Wu, Shuzheng Si, Yuan Shao, Zhijian Ou, Yongbin Li</li>
<li>for: 本研究旨在提高对话系统模型的Robustness和传输能力，通过多任务预训练。</li>
<li>methods: 本研究使用Task-based Automatic Prompt generation（TAP）自动生成高质量的提示。</li>
<li>results: 通过使用高质量的提示，我们扩展了对话系统模型的训练数据集至122个任务，并实现了对多种对话任务和不同的对话系统的优秀表现。<details>
<summary>Abstract</summary>
Recent research has shown that multi-task pre-training greatly improves the model's robustness and transfer ability, which is crucial for building a high-quality dialog system. However, most previous works on multi-task pre-training rely heavily on human-defined input format or prompt, which is not optimal in quality and quantity. In this work, we propose to use Task-based Automatic Prompt generation (TAP) to automatically generate high-quality prompts. Using the high-quality prompts generated, we scale the corpus of the pre-trained conversation model to 122 datasets from 15 dialog-related tasks, resulting in Universal Pre-trained Conversation Model (UniPCM), a powerful foundation model for various conversational tasks and different dialog systems. Extensive experiments have shown that UniPCM is robust to input prompts and capable of various dialog-related tasks. Moreover, UniPCM has strong transfer ability and excels at low resource scenarios, achieving SOTA results on 9 different datasets ranging from task-oriented dialog to open-domain conversation. Furthermore, we are amazed to find that TAP can generate prompts on par with those collected with crowdsourcing. The code is released with the paper.
</details>
<details>
<summary>摘要</summary>
近期研究表明，多任务预训练可以大幅提高模型的Robustness和传递能力，这是建立高质量对话系统的关键。然而，大多数前一些工作中的多任务预训练都依赖于人类定义的输入格式或提示，这并不是最佳的质量和量。在这项工作中，我们提议使用任务基本Prompt生成（TAP）自动生成高质量提示。使用生成的高质量提示，我们扩展了预训练对话模型的训练数据集，达到了122个对话相关任务的规模，并命名为Universal Pre-trained Conversation Model（UniPCM）。广泛的实验表明，UniPCM具有输入提示的Robustness和多种对话任务的能力。此外，UniPCM在资源不足的情况下表现出色，在9个不同任务上达到了SOTA的结果，从任务型对话到开放领域对话。此外，我们发现TAP可以生成与人类收集的提示相当的提示。代码随着论文一起发布。
</details></li>
</ul>
<hr>
<h2 id="XATU-A-Fine-grained-Instruction-based-Benchmark-for-Explainable-Text-Updates"><a href="#XATU-A-Fine-grained-Instruction-based-Benchmark-for-Explainable-Text-Updates" class="headerlink" title="XATU: A Fine-grained Instruction-based Benchmark for Explainable Text Updates"></a>XATU: A Fine-grained Instruction-based Benchmark for Explainable Text Updates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11063">http://arxiv.org/abs/2309.11063</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/megagonlabs/xatu">https://github.com/megagonlabs/xatu</a></li>
<li>paper_authors: Haopeng Zhang, Hayate Iso, Sairam Gurajada, Nikita Bhutani</li>
<li>for: 这个论文旨在检验大语言模型的文本编辑能力，并提供一个新的、细化的指令基于的文本编辑benchmark。</li>
<li>methods: 本论文使用了新的benchmark，叫做XATU，它包括了各种话题和文本类型，并且包含了lexical、syntactic、semantic和knowledge-intensive的编辑指令。以提高可读性，这个benchmark使用了高质量的数据源和人工标注，以获得细化的编辑指令和金标准的编辑解释。</li>
<li>results: 通过对现有的开放和关闭大语言模型进行评估，本论文示出了 instrucion tuning 的效果和不同架构下的编辑任务的影响。此外，广泛的实验还表明了对文本编辑任务的细化解释的重要性。<details>
<summary>Abstract</summary>
Text editing is a crucial task that involves modifying text to better align with user intents. However, existing text editing benchmark datasets have limitations in providing only coarse-grained instructions. Consequently, although the edited output may seem reasonable, it often deviates from the intended changes outlined in the gold reference, resulting in low evaluation scores. To comprehensively investigate the text editing capabilities of large language models, this paper introduces XATU, the first benchmark specifically designed for fine-grained instruction-based explainable text editing. XATU covers a wide range of topics and text types, incorporating lexical, syntactic, semantic, and knowledge-intensive edits. To enhance interpretability, we leverage high-quality data sources and human annotation, resulting in a benchmark that includes fine-grained instructions and gold-standard edit explanations. By evaluating existing open and closed large language models against our benchmark, we demonstrate the effectiveness of instruction tuning and the impact of underlying architecture across various editing tasks. Furthermore, extensive experimentation reveals the significant role of explanations in fine-tuning language models for text editing tasks. The benchmark will be open-sourced to support reproduction and facilitate future research.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:文本编辑是一项重要的任务，它涉及修改文本，使其更加符合用户的意图。然而，现有的文本编辑标准数据集有限制，只提供粗略的指令。因此，编辑后的输出可能看起来合理，但它经常与金标准 refer 中的修改细则不符，导致评价分数低下。为了全面调查大语言模型的文本编辑能力，这篇论文引入 XATU，首个专门为精细指令基于的可解释文本编辑标准。XATU 覆盖了各种话题和文本类型，包括语法、语义和知识等编辑。为了增强可读性，我们利用高质量的数据源和人工标注，从而创建了一个包含精细指令和金标准编辑解释的标准。通过评价现有的开源和关闭式大语言模型，我们示出了指令调整和模型的底层结构对于不同的编辑任务的影响。此外，广泛的实验表明，解释在调整语言模型进行文本编辑任务时发挥了重要的作用。这个标准将被开源，以支持重现和未来研究。
</details></li>
</ul>
<hr>
<h2 id="fakenewsbr-A-Fake-News-Detection-Platform-for-Brazilian-Portuguese"><a href="#fakenewsbr-A-Fake-News-Detection-Platform-for-Brazilian-Portuguese" class="headerlink" title="fakenewsbr: A Fake News Detection Platform for Brazilian Portuguese"></a>fakenewsbr: A Fake News Detection Platform for Brazilian Portuguese</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11052">http://arxiv.org/abs/2309.11052</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luiz Giordani, Gilsiley Darú, Rhenan Queiroz, Vitor Buzinaro, Davi Keglevich Neiva, Daniel Camilo Fuentes Guzmán, Marcos Jardel Henriques, Oilson Alberto Gonzatto Junior, Francisco Louzada</li>
<li>for: 本研究旨在探讨假新闻的检测，尤其是在巴西葡萄牙语新闻文章中。</li>
<li>methods: 本研究使用自然语言处理技术，包括TF-IDF和Word2Vec，提取文本数据中的特征。并评估了多种分类算法，如逻辑回归、支持向量机、Random Forest、AdaBoost和LightGBM在一个包含真实和假新闻文章的数据集上的性能。</li>
<li>results: 提出的方法在评估中实现了高精度和F1-Score，证明其在检测假新闻中的有效性。此外，我们还开发了一个User-friendly的网页平台，fakenewsbr.com，以便用户对新闻文章的真实性进行实时分析。<details>
<summary>Abstract</summary>
The proliferation of fake news has become a significant concern in recent times due to its potential to spread misinformation and manipulate public opinion. This paper presents a comprehensive study on detecting fake news in Brazilian Portuguese, focusing on journalistic-type news. We propose a machine learning-based approach that leverages natural language processing techniques, including TF-IDF and Word2Vec, to extract features from textual data. We evaluate the performance of various classification algorithms, such as logistic regression, support vector machine, random forest, AdaBoost, and LightGBM, on a dataset containing both true and fake news articles. The proposed approach achieves high accuracy and F1-Score, demonstrating its effectiveness in identifying fake news. Additionally, we developed a user-friendly web platform, fakenewsbr.com, to facilitate the verification of news articles' veracity. Our platform provides real-time analysis, allowing users to assess the likelihood of fake news articles. Through empirical analysis and comparative studies, we demonstrate the potential of our approach to contribute to the fight against the spread of fake news and promote more informed media consumption.
</details>
<details>
<summary>摘要</summary>
“假新闻的扩散已成为当前的一大问题，因为它可能导致谣言的传播和公众意识的扭曲。这篇论文介绍了检测巴西葡萄牙语假新闻的完整研究，专注于新闻类文章。我们提议一种基于机器学习的方法，利用自然语言处理技术，包括TF-IDF和Word2Vec，提取文本数据中的特征。我们评估了多种分类算法，如逻辑回归、支持向量机和Random Forest等，在一个包含真实和假新闻文章的数据集上进行了评估。我们的方法实现了高精度和F1分数，证明了它的效iveness在识别假新闻。此外，我们还开发了一个用户友好的网站，fakenewsbr.com，以便评估新闻文章的真实性。我们的平台提供了实时分析，让用户在实时基础上评估假新闻文章的可能性。通过实验分析和比较研究，我们表明了我们的方法在抗击假新闻的扩散方面的潜在作用，并促进更有知识的媒体消费。”
</details></li>
</ul>
<hr>
<h2 id="Localize-Retrieve-and-Fuse-A-Generalized-Framework-for-Free-Form-Question-Answering-over-Tables"><a href="#Localize-Retrieve-and-Fuse-A-Generalized-Framework-for-Free-Form-Question-Answering-over-Tables" class="headerlink" title="Localize, Retrieve and Fuse: A Generalized Framework for Free-Form Question Answering over Tables"></a>Localize, Retrieve and Fuse: A Generalized Framework for Free-Form Question Answering over Tables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11049">http://arxiv.org/abs/2309.11049</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wentinghome/TAGQA">https://github.com/wentinghome/TAGQA</a></li>
<li>paper_authors: Wenting Zhao, Ye Liu, Yao Wan, Yibo Wang, Zhongfen Deng, Philip S. Yu</li>
<li>for: 提高表格问答系统的能力，以生成基于表格数据和自然语言信息的详细答案。</li>
<li>methods: 提出一种三 stage 方法，包括表格转换为图形和细化筛选、外部知识 retrieval 和表格和文本融合（TAG-QA），以解决基于表格数据的自由形式问答的挑战。</li>
<li>results: 实验表明，TAG-QA 能够生成比基eline 更加准确、完整的答案，特别是与 pipeline-based 基eline TAPAS 和 end-to-end 模型 T5 相比。TAG-QA 在 BLEU-4 和 PARENT F-score 上比 TAPAS 高出 17% 和 14%，并高于 T5 的 BLEU-4 和 PARENT F-score 上的提高为 16% 和 12%。<details>
<summary>Abstract</summary>
Question answering on tabular data (a.k.a TableQA), which aims at generating answers to questions grounded on a provided table, has gained significant attention recently. Prior work primarily produces concise factual responses through information extraction from individual or limited table cells, lacking the ability to reason across diverse table cells. Yet, the realm of free-form TableQA, which demands intricate strategies for selecting relevant table cells and the sophisticated integration and inference of discrete data fragments, remains mostly unexplored. To this end, this paper proposes a generalized three-stage approach: Table-to- Graph conversion and cell localizing, external knowledge retrieval, and the fusion of table and text (called TAG-QA), to address the challenge of inferring long free-form answers in generative TableQA. In particular, TAG-QA (1) locates relevant table cells using a graph neural network to gather intersecting cells between relevant rows and columns, (2) leverages external knowledge from Wikipedia, and (3) generates answers by integrating both tabular data and natural linguistic information. Experiments showcase the superior capabilities of TAG-QA in generating sentences that are both faithful and coherent, particularly when compared to several state-of-the-art baselines. Notably, TAG-QA surpasses the robust pipeline-based baseline TAPAS by 17% and 14% in terms of BLEU-4 and PARENT F-score, respectively. Furthermore, TAG-QA outperforms the end-to-end model T5 by 16% and 12% on BLEU-4 and PARENT F-score, respectively.
</details>
<details>
<summary>摘要</summary>
问答基于表格数据（即 TableQA）在最近几年内获得了广泛关注，目的是生成基于提供的表格数据的问题的回答。然而，现有的工作主要通过提取表格单元中的信息进行信息抽取，缺乏能够跨单元进行推理的能力。为了解决这个问题，本文提出了一种通用的三stageapproach：表格转 graf并Cell Localization（TAG-QA），以生成具有推理能力的表格问答系统。具体来说，TAG-QA包括以下三个阶段：1. 使用图 neural network 来找到相关的表格单元，并将其作为交叉单元进行汇聚。2. 利用外部知识来提高表格问答的能力。3. 将表格数据和自然语言信息 integrate 起来，以生成具有 faithful 和 coherent 性的回答。实验表明，TAG-QA 在生成长度不受限制的自由形表格问答方面具有显著的优势，特别是与一些状态之际的基准值进行比较。在 BLEU-4 和 PARENT F-score 等指标上，TAG-QA 与 TAPAS 和 T5 模型相比，净提高了17%和14%。此外，TAG-QA 还在 BLEU-4 和 PARENT F-score 上出现16%和12%的提升。
</details></li>
</ul>
<hr>
<h2 id="Heterogeneous-Entity-Matching-with-Complex-Attribute-Associations-using-BERT-and-Neural-Networks"><a href="#Heterogeneous-Entity-Matching-with-Complex-Attribute-Associations-using-BERT-and-Neural-Networks" class="headerlink" title="Heterogeneous Entity Matching with Complex Attribute Associations using BERT and Neural Networks"></a>Heterogeneous Entity Matching with Complex Attribute Associations using BERT and Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11046">http://arxiv.org/abs/2309.11046</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shitao Wang, Jiamin Lu</li>
<li>for: Addressing the challenges of entity matching in heterogeneous data with complex attribute relationships.</li>
<li>methods: Utilizing a novel entity matching model, EMM-CCAR, built upon pre-trained models, with attention mechanisms to capture complex relationships between attributes.</li>
<li>results: Achieving improvements of approximately 4% and 1% in F1 scores compared to prevalent DER-SSM and Ditto approaches, respectively, demonstrating the effectiveness of the proposed model in handling complex attribute relationships.<details>
<summary>Abstract</summary>
Across various domains, data from different sources such as Baidu Baike and Wikipedia often manifest in distinct forms. Current entity matching methodologies predominantly focus on homogeneous data, characterized by attributes that share the same structure and concise attribute values. However, this orientation poses challenges in handling data with diverse formats. Moreover, prevailing approaches aggregate the similarity of attribute values between corresponding attributes to ascertain entity similarity. Yet, they often overlook the intricate interrelationships between attributes, where one attribute may have multiple associations. The simplistic approach of pairwise attribute comparison fails to harness the wealth of information encapsulated within entities.To address these challenges, we introduce a novel entity matching model, dubbed Entity Matching Model for Capturing Complex Attribute Relationships(EMM-CCAR),built upon pre-trained models. Specifically, this model transforms the matching task into a sequence matching problem to mitigate the impact of varying data formats. Moreover, by introducing attention mechanisms, it identifies complex relationships between attributes, emphasizing the degree of matching among multiple attributes rather than one-to-one correspondences. Through the integration of the EMM-CCAR model, we adeptly surmount the challenges posed by data heterogeneity and intricate attribute interdependencies. In comparison with the prevalent DER-SSM and Ditto approaches, our model achieves improvements of approximately 4% and 1% in F1 scores, respectively. This furnishes a robust solution for addressing the intricacies of attribute complexity in entity matching.
</details>
<details>
<summary>摘要</summary>
across various domains, data from different sources such as Baidu Baike and Wikipedia often manifest in distinct forms. Current entity matching methodologies predominantly focus on homogeneous data, characterized by attributes that share the same structure and concise attribute values. However, this orientation poses challenges in handling data with diverse formats. Moreover, prevailing approaches aggregate the similarity of attribute values between corresponding attributes to ascertain entity similarity. Yet, they often overlook the intricate interrelationships between attributes, where one attribute may have multiple associations. The simplistic approach of pairwise attribute comparison fails to harness the wealth of information encapsulated within entities.To address these challenges, we introduce a novel entity matching model, dubbed Entity Matching Model for Capturing Complex Attribute Relationships(EMM-CCAR),built upon pre-trained models. Specifically, this model transforms the matching task into a sequence matching problem to mitigate the impact of varying data formats. Moreover, by introducing attention mechanisms, it identifies complex relationships between attributes, emphasizing the degree of matching among multiple attributes rather than one-to-one correspondences. Through the integration of the EMM-CCAR model, we adeptly surmount the challenges posed by data heterogeneity and intricate attribute interdependencies. In comparison with the prevalent DER-SSM and Ditto approaches, our model achieves improvements of approximately 4% and 1% in F1 scores, respectively. This furnishes a robust solution for addressing the intricacies of attribute complexity in entity matching.
</details></li>
</ul>
<hr>
<h2 id="Named-Entity-Recognition-via-Machine-Reading-Comprehension-A-Multi-Task-Learning-Approach"><a href="#Named-Entity-Recognition-via-Machine-Reading-Comprehension-A-Multi-Task-Learning-Approach" class="headerlink" title="Named Entity Recognition via Machine Reading Comprehension: A Multi-Task Learning Approach"></a>Named Entity Recognition via Machine Reading Comprehension: A Multi-Task Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11027">http://arxiv.org/abs/2309.11027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yibo Wang, Wenting Zhao, Yao Wan, Zhongfen Deng, Philip S. Yu</li>
<li>for: 本研究旨在提高Machine Reading Comprehension（MRC）基于Named Entity Recognition（NER）的性能，特别是在忽略实体类别 Label 相互关系的情况下。</li>
<li>methods: 本文提出了一种基于多任务学习的 Multi-NER 模型，通过自注意机制 capture 实体类别 Label 相互关系。</li>
<li>results: 对于嵌入式 NER 和平面 NER 数据集，实验结果表明 Multi-NER 可以在所有数据集上提高性能。<details>
<summary>Abstract</summary>
Named Entity Recognition (NER) aims to extract and classify entity mentions in the text into pre-defined types (e.g., organization or person name). Recently, many works have been proposed to shape the NER as a machine reading comprehension problem (also termed MRC-based NER), in which entity recognition is achieved by answering the formulated questions related to pre-defined entity types through MRC, based on the contexts. However, these works ignore the label dependencies among entity types, which are critical for precisely recognizing named entities. In this paper, we propose to incorporate the label dependencies among entity types into a multi-task learning framework for better MRC-based NER. We decompose MRC-based NER into multiple tasks and use a self-attention module to capture label dependencies. Comprehensive experiments on both nested NER and flat NER datasets are conducted to validate the effectiveness of the proposed Multi-NER. Experimental results show that Multi-NER can achieve better performance on all datasets.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-Joint-Modeling-of-Dialogue-Response-and-Speech-Synthesis-based-on-Large-Language-Model"><a href="#Towards-Joint-Modeling-of-Dialogue-Response-and-Speech-Synthesis-based-on-Large-Language-Model" class="headerlink" title="Towards Joint Modeling of Dialogue Response and Speech Synthesis based on Large Language Model"></a>Towards Joint Modeling of Dialogue Response and Speech Synthesis based on Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11000">http://arxiv.org/abs/2309.11000</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/XinyuZhou2000/Spoken_Dialogue">https://github.com/XinyuZhou2000/Spoken_Dialogue</a></li>
<li>paper_authors: Xinyu Zhou, Delong Chen, Yudong Chen</li>
<li>for: 构建一个基于大语言模型的对话系统，更加准确地模拟人类语言生成过程。</li>
<li>methods: 使用大语言模型来同时模型对话响应和语言特征，并在语音结构预测和对话响应 integrate 多种语言特征。</li>
<li>results: 实验结果表明，基于大语言模型的方法是建立一个紧凑的对话系统的可能性的。<details>
<summary>Abstract</summary>
This paper explores the potential of constructing an AI spoken dialogue system that "thinks how to respond" and "thinks how to speak" simultaneously, which more closely aligns with the human speech production process compared to the current cascade pipeline of independent chatbot and Text-to-Speech (TTS) modules. We hypothesize that Large Language Models (LLMs) with billions of parameters possess significant speech understanding capabilities and can jointly model dialogue responses and linguistic features. We conduct two sets of experiments: 1) Prosodic structure prediction, a typical front-end task in TTS, demonstrating the speech understanding ability of LLMs, and 2) Further integrating dialogue response and a wide array of linguistic features using a unified encoding format. Our results indicate that the LLM-based approach is a promising direction for building unified spoken dialogue systems.
</details>
<details>
<summary>摘要</summary>
这个论文探讨了构建一个基于人工智能的对话系统，该系统可以同时“思考如何回答”和“思考如何说”，这更接近于人类语言生产过程。我们假设大语言模型（LLM）拥有数十亿个参数，具有强大的语音理解能力，可以同时模型对话回答和语言特征。我们进行了两组实验：1）语调结构预测，这是常见的前端任务在文本识别中，以示LLM的语音理解能力；2）将对话回答和广泛的语言特征集成使用统一编码格式。我们的结果表明，基于LLM的方法是构建统一的对话系统的可能之道。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/20/cs.CL_2023_09_20/" data-id="clpztdne400c8es88a5b12adr" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/20/cs.LG_2023_09_20/" class="article-date">
  <time datetime="2023-09-20T10:00:00.000Z" itemprop="datePublished">2023-09-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/20/cs.LG_2023_09_20/">cs.LG - 2023-09-20</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Large-scale-Pretraining-Improves-Sample-Efficiency-of-Active-Learning-based-Molecule-Virtual-Screening"><a href="#Large-scale-Pretraining-Improves-Sample-Efficiency-of-Active-Learning-based-Molecule-Virtual-Screening" class="headerlink" title="Large-scale Pretraining Improves Sample Efficiency of Active Learning based Molecule Virtual Screening"></a>Large-scale Pretraining Improves Sample Efficiency of Active Learning based Molecule Virtual Screening</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11687">http://arxiv.org/abs/2309.11687</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhonglin Cao, Simone Sciabola, Ye Wang</li>
<li>for: 本研究旨在提高活性学习和 bayesian 优化在虚拟屏选中的精度和样本效率，使用预训练的 transformer 基于语言模型和图神经网络。</li>
<li>methods: 本研究使用的方法包括虚拟屏选、活性学习和 bayesian 优化，采用预训练的 transformer 基于语言模型和图神经网络作为评估模型。</li>
<li>results: 研究结果表明，预训练 transformer 基于语言模型和图神经网络在 Bayesian 优化active learning框架中表现出色，可以在虚拟屏选中提高精度和样本效率，比前一个基eline提高8%。<details>
<summary>Abstract</summary>
Virtual screening of large compound libraries to identify potential hit candidates is one of the earliest steps in drug discovery. As the size of commercially available compound collections grows exponentially to the scale of billions, brute-force virtual screening using traditional tools such as docking becomes infeasible in terms of time and computational resources. Active learning and Bayesian optimization has recently been proven as effective methods of narrowing down the search space. An essential component in those methods is a surrogate machine learning model that is trained with a small subset of the library to predict the desired properties of compounds. Accurate model can achieve high sample efficiency by finding the most promising compounds with only a fraction of the whole library being virtually screened. In this study, we examined the performance of pretrained transformer-based language model and graph neural network in Bayesian optimization active learning framework. The best pretrained models identifies 58.97% of the top-50000 by docking score after screening only 0.6% of an ultra-large library containing 99.5 million compounds, improving 8% over previous state-of-the-art baseline. Through extensive benchmarks, we show that the superior performance of pretrained models persists in both structure-based and ligand-based drug discovery. Such model can serve as a boost to the accuracy and sample efficiency of active learning based molecule virtual screening.
</details>
<details>
<summary>摘要</summary>
做为药物发现的早期步骤之一，虚拟屏选大规模化合物库以找到潜在的靶点候选者。随着商业可用的化合物集合的规模 exponentiated 到亿量级，使用传统工具 such as docking 进行虚拟屏选成为计算资源和时间上的不可行。活动学习和 Bayesian 优化已经被证明为虚拟屏选中的有效方法。这些方法中的一个关键组件是一个训练于小型库中的机器学习模型，用于预测化合物的欲要性。一旦有一个准确的模型，它可以在虚拟屏选中高效地寻找最有前途的化合物，只需虚拟屏选出一小部分的化合物库。在本研究中，我们研究了使用预训练的 transformer 基于语言模型和图神经网络在 Bayesian 优化活动学习框架中的表现。最佳预训练模型可以在虚拟屏选出 99.5 亿个化合物库中的 58.97% 最佳 docking 分数前 50000 个化合物，提高了 8% 于前一个基准值。我们通过广泛的 benchmark 表明，预训练模型在结构基于和药物基于的药物发现中的表现仍然优秀。这种模型可以为活动学习基于虚拟屏选的药物发现增加精度和采样效率。
</details></li>
</ul>
<hr>
<h2 id="Popularity-Degradation-Bias-in-Local-Music-Recommendation"><a href="#Popularity-Degradation-Bias-in-Local-Music-Recommendation" class="headerlink" title="Popularity Degradation Bias in Local Music Recommendation"></a>Popularity Degradation Bias in Local Music Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11671">http://arxiv.org/abs/2309.11671</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/asem010/legend-pice">https://github.com/asem010/legend-pice</a></li>
<li>paper_authors: April Trainor, Douglas Turnbull</li>
<li>for: 本研究探讨了当地音乐推荐中的人气倒退偏见问题。</li>
<li>methods: 研究使用了两种现场表现最佳的推荐算法：Weight Relevance Matrix Factorization (WRMF) 和 Multinomial Variational Autoencoder (Mult-VAE)。</li>
<li>results: 研究发现，这两种算法在更受欢迎的艺术家上的推荐性能都有所提高，并且展现了人气倒退偏见。 Mult-VAE 在 menos popular 的艺术家上表现更好，因此在当地音乐艺术家推荐中可能更有优势。<details>
<summary>Abstract</summary>
In this paper, we study the effect of popularity degradation bias in the context of local music recommendations. Specifically, we examine how accurate two top-performing recommendation algorithms, Weight Relevance Matrix Factorization (WRMF) and Multinomial Variational Autoencoder (Mult-VAE), are at recommending artists as a function of artist popularity. We find that both algorithms improve recommendation performance for more popular artists and, as such, exhibit popularity degradation bias. While both algorithms produce a similar level of performance for more popular artists, Mult-VAE shows better relative performance for less popular artists. This suggests that this algorithm should be preferred for local (long-tail) music artist recommendation.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了本地音乐推荐中的人気倒退偏见影响。我们专门研究了两种最佳推荐算法的精度，即Weight Relevance Matrix Factorization (WRMF)和Multinomial Variational Autoencoder (Mult-VAE)。我们发现这两种算法对更受欢迎的艺术家的推荐性能都有改善，因此它们都存在人気倒退偏见。虽然这两种算法在更受欢迎的艺术家中的表现水平相似，但Mult-VAE在 menos popular 艺术家中表现更优。这表示Mult-VAE应该选择用于本地（长尾）音乐艺术家推荐。
</details></li>
</ul>
<hr>
<h2 id="GLM-Regression-with-Oblivious-Corruptions"><a href="#GLM-Regression-with-Oblivious-Corruptions" class="headerlink" title="GLM Regression with Oblivious Corruptions"></a>GLM Regression with Oblivious Corruptions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11657">http://arxiv.org/abs/2309.11657</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilias Diakonikolas, Sushrut Karmalkar, Jongho Park, Christos Tzamos</li>
<li>for: 这个论文是为了解决通用线性模型（GLM）在某些情况下添加了随机噪声的问题而写的。</li>
<li>methods: 这篇论文使用了一种新的算法来解决这个问题，该算法可以在最通用的分布无关 Settings中实现。</li>
<li>results: 论文的结果表明，该算法可以在大多数情况下提供高度准确的解决方案，而且可以处理更多的样本被随机噪声损害的情况。I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
We demonstrate the first algorithms for the problem of regression for generalized linear models (GLMs) in the presence of additive oblivious noise. We assume we have sample access to examples $(x, y)$ where $y$ is a noisy measurement of $g(w^* \cdot x)$. In particular, \new{the noisy labels are of the form} $y = g(w^* \cdot x) + \xi + \epsilon$, where $\xi$ is the oblivious noise drawn independently of $x$ \new{and satisfies} $\Pr[\xi = 0] \geq o(1)$, and $\epsilon \sim \mathcal N(0, \sigma^2)$. Our goal is to accurately recover a \new{parameter vector $w$ such that the} function $g(w \cdot x)$ \new{has} arbitrarily small error when compared to the true values $g(w^* \cdot x)$, rather than the noisy measurements $y$.   We present an algorithm that tackles \new{this} problem in its most general distribution-independent setting, where the solution may not \new{even} be identifiable. \new{Our} algorithm returns \new{an accurate estimate of} the solution if it is identifiable, and otherwise returns a small list of candidates, one of which is close to the true solution. Furthermore, we \new{provide} a necessary and sufficient condition for identifiability, which holds in broad settings. \new{Specifically,} the problem is identifiable when the quantile at which $\xi + \epsilon = 0$ is known, or when the family of hypotheses does not contain candidates that are nearly equal to a translated $g(w^* \cdot x) + A$ for some real number $A$, while also having large error when compared to $g(w^* \cdot x)$.   This is the first \new{algorithmic} result for GLM regression \new{with oblivious noise} which can handle more than half the samples being arbitrarily corrupted. Prior work focused largely on the setting of linear regression, and gave algorithms under restrictive assumptions.
</details>
<details>
<summary>摘要</summary>
我们展示了第一个对于通用线性模型（GLM）中扩展的问题的回溯算法。我们假设有一个访问例子 $(x, y)$，其中 $y $ 是 $g(w^* \cdot x)$ 的错误的测量。特别是，我们假设错误标签的形式为 $y = g(w^* \cdot x) + \xi + \epsilon$，其中 $\xi $ 是独立于 $x$ 的随机错误，且 $\Pr[\xi = 0] \geq o(1)$，且 $\epsilon \sim \mathcal N(0, \sigma^2)$。我们的目标是将一个精确地回传 $w $ 的参数，使得 $g(w \cdot x)$ 与真正的值 $g(w^* \cdot x)$ 之间的差异可以随时对应。我们提出了一个可以在最通用的分布不依赖情况下解决这个问题的算法。如果问题可解析，我们的算法将返回一个精确的解析结果；否则，它将返回一个小列表，其中一个与真实解析结果相似。此外，我们还提供了必要和充分的可 identificability 条件，这样在广泛的设定下都会成立。具体来说，问题可解析当 $\xi + \epsilon = 0$ 的quantile 知道，或者家族假设不包含 nearly equal to $g(w^* \cdot x) + A$ 的候选者，而且在与 $g(w^* \cdot x)$ 比较时有大的误差。这是第一个对 GLM 回溯算法中扩展的数据验证项目，可以应对更多于半数的样本被任意损坏。先前的工作主要集中在线性回溯领域，并提供了对于特定假设的限制性算法。
</details></li>
</ul>
<hr>
<h2 id="Drift-Control-of-High-Dimensional-RBM-A-Computational-Method-Based-on-Neural-Networks"><a href="#Drift-Control-of-High-Dimensional-RBM-A-Computational-Method-Based-on-Neural-Networks" class="headerlink" title="Drift Control of High-Dimensional RBM: A Computational Method Based on Neural Networks"></a>Drift Control of High-Dimensional RBM: A Computational Method Based on Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11651">http://arxiv.org/abs/2309.11651</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nian-si/rbmsolver">https://github.com/nian-si/rbmsolver</a></li>
<li>paper_authors: Baris Ata, J. Michael Harrison, Nian Si</li>
<li>for: 本研究的目的是寻找一种能够在无穷规划 horizion 上最小化预算的折损控制方法。</li>
<li>methods: 该研究使用了深度神经网络技术来解决这个控制问题，并对一些测试问题进行了实验 validate。</li>
<li>results: 研究发现，使用深度神经网络技术可以在高维度($d&#x3D;30$) 下实现高精度的解决方案，并且计算效率高。<details>
<summary>Abstract</summary>
Motivated by applications in queueing theory, we consider a stochastic control problem whose state space is the $d$-dimensional positive orthant. The controlled process $Z$ evolves as a reflected Brownian motion whose covariance matrix is exogenously specified, as are its directions of reflection from the orthant's boundary surfaces. A system manager chooses a drift vector $\theta(t)$ at each time $t$ based on the history of $Z$, and the cost rate at time $t$ depends on both $Z(t)$ and $\theta(t)$. In our initial problem formulation, the objective is to minimize expected discounted cost over an infinite planning horizon, after which we treat the corresponding ergodic control problem. Extending earlier work by Han et al. (Proceedings of the National Academy of Sciences, 2018, 8505-8510), we develop and illustrate a simulation-based computational method that relies heavily on deep neural network technology. For test problems studied thus far, our method is accurate to within a fraction of one percent, and is computationally feasible in dimensions up to at least $d=30$.
</details>
<details>
<summary>摘要</summary>
Extending earlier work by Han et al. (Proceedings of the National Academy of Sciences, 2018, 8505-8510), we develop and illustrate a simulation-based computational method that relies heavily on deep neural network technology. For test problems studied thus far, our method is accurate to within a fraction of one percent, and is computationally feasible in dimensions up to at least $d=30$.Translated into Simplified Chinese:我们受到排阵理论应用的驱动下，考虑一个 Stochastic control problem，其state space是 $d$ 维正方形。控制过程 $Z$ 是一个受到确定的均值矩阵影响的反射 Браун运动，其方向受到正方形边界表面的反射影响。系统管理员在每个时刻 $t$ 选择一个推移 вектор $\theta(t)$，基于 $Z$ 的历史，而在每个时刻 $t$ 的成本率取决于 $Z(t)$ 和 $\theta(t)$。在我们的初始问题中，目标是在无限计划时间后面内预算成本，然后处理相应的ergodic control问题。我们将 extending Han et al. (Proceedings of the National Academy of Sciences, 2018, 8505-8510) 的研究，开发了一个基于深度神经网络技术的 simulational-based computational method。在我们试验的问题上，我们的方法精度在 fraction of one percent 以内，并且在维度至少 $d=30$ 时是 computationally feasible。
</details></li>
</ul>
<hr>
<h2 id="Potential-and-limitations-of-random-Fourier-features-for-dequantizing-quantum-machine-learning"><a href="#Potential-and-limitations-of-random-Fourier-features-for-dequantizing-quantum-machine-learning" class="headerlink" title="Potential and limitations of random Fourier features for dequantizing quantum machine learning"></a>Potential and limitations of random Fourier features for dequantizing quantum machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11647">http://arxiv.org/abs/2309.11647</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryan Sweke, Erik Recio, Sofiene Jerbi, Elies Gil-Fuster, Bryce Fuller, Jens Eisert, Johannes Jakob Meyer</li>
<li>for: 这篇论文主要是关于量子机器学习的应用，具体来说是关于近期量子设备上的变量量子机器学习。</li>
<li>methods: 这篇论文使用了参数化的量子电路（PQC）作为学习模型，并研究了这些PQC模型在减量化上的效率。</li>
<li>results: 这篇论文提出了关于变量量子机器学习 regression 问题下减量化的必要和 suficient 条件，并基于这些准则提出了具体的PQC架构设计和优化方法。<details>
<summary>Abstract</summary>
Quantum machine learning is arguably one of the most explored applications of near-term quantum devices. Much focus has been put on notions of variational quantum machine learning where parameterized quantum circuits (PQCs) are used as learning models. These PQC models have a rich structure which suggests that they might be amenable to efficient dequantization via random Fourier features (RFF). In this work, we establish necessary and sufficient conditions under which RFF does indeed provide an efficient dequantization of variational quantum machine learning for regression. We build on these insights to make concrete suggestions for PQC architecture design, and to identify structures which are necessary for a regression problem to admit a potential quantum advantage via PQC based optimization.
</details>
<details>
<summary>摘要</summary>
量子机器学习是近期量子设备应用的一个最具探索性的领域。许多研究都集中在变量量子机器学习中，使用参数化量子电路（PQC）作为学习模型。这些PQC模型具有丰富的结构，这意味着它们可能会受到有效的减量化处理（RFF）。在这个工作中，我们确定了变量量子机器学习 regression 问题下的必要和充分条件，以确保RFF实现有效的减量化。我们基于这些发现，对PQC架构设计提出了具体的建议，并标识了可以使用PQC基于优化实现量子优势的结构。
</details></li>
</ul>
<hr>
<h2 id="Early-diagnosis-of-autism-spectrum-disorder-using-machine-learning-approaches"><a href="#Early-diagnosis-of-autism-spectrum-disorder-using-machine-learning-approaches" class="headerlink" title="Early diagnosis of autism spectrum disorder using machine learning approaches"></a>Early diagnosis of autism spectrum disorder using machine learning approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11646">http://arxiv.org/abs/2309.11646</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/diponkor-bala/autism-spectrum-disorder">https://github.com/diponkor-bala/autism-spectrum-disorder</a></li>
<li>paper_authors: Rownak Ara Rasul, Promy Saha, Diponkor Bala, S M Rakib Ul Karim, Ibrahim Abdullah, Bishwajit Saha</li>
<li>for: This paper aims to utilize machine learning algorithms to identify and automate the diagnostic process for Autistic Spectrum Disorder (ASD).</li>
<li>methods: The paper employs six classification models and five popular clustering methods to analyze ASD datasets, and evaluates their performance using various metrics such as accuracy, precision, recall, specificity, F1-score, AUC, kappa, and log loss.</li>
<li>results: The paper achieves a 100% accuracy rate when hyperparameters are carefully tuned for each model, and finds that spectral clustering outperforms other benchmarking clustering models in terms of NMI and ARI metrics, demonstrating comparability to the optimal SC achieved by k-means.Here’s the Chinese version of the three key points:</li>
<li>for: 这篇论文目标是使用机器学习算法来识别和自动诊断听力特指症（ASD）。</li>
<li>methods: 论文使用 six 种分类模型和 five 种流行的聚类方法来分析 ASD 数据集，并评估其性能使用多种指标 such as 准确率、精度、 recall、特异性、 F1 分数、 AUC、 kappa 和 log loss。</li>
<li>results: 论文在hyperparameter 仔细调整后， achieved a 100% 的准确率，并发现 spectral clustering 在 NMI 和 ARI 指标上表现出色，与 k-means 的最佳 SC 相比。<details>
<summary>Abstract</summary>
Autistic Spectrum Disorder (ASD) is a neurological disease characterized by difficulties with social interaction, communication, and repetitive activities. The severity of these difficulties varies, and those with this diagnosis face unique challenges. While its primary origin lies in genetics, identifying and addressing it early can contribute to the enhancement of the condition. In recent years, machine learning-driven intelligent diagnosis has emerged as a supplement to conventional clinical approaches, aiming to address the potential drawbacks of time-consuming and costly traditional methods. In this work, we utilize different machine learning algorithms to find the most significant traits responsible for ASD and to automate the diagnostic process. We study six classification models to see which model works best to identify ASD and also study five popular clustering methods to get a meaningful insight of these ASD datasets. To find the best classifier for these binary datasets, we evaluate the models using accuracy, precision, recall, specificity, F1-score, AUC, kappa and log loss metrics. Our evaluation demonstrates that five out of the six selected models perform exceptionally, achieving a 100% accuracy rate on the ASD datasets when hyperparameters are meticulously tuned for each model. As almost all classification models are able to get 100% accuracy, we become interested in observing the underlying insights of the datasets by implementing some popular clustering algorithms on these datasets. We calculate Normalized Mutual Information (NMI), Adjusted Rand Index (ARI) & Silhouette Coefficient (SC) metrics to select the best clustering models. Our evaluation finds that spectral clustering outperforms all other benchmarking clustering models in terms of NMI & ARI metrics and it also demonstrates comparability to the optimal SC achieved by k-means.
</details>
<details>
<summary>摘要</summary>
“自适应谱综合症（ASD）是一种中枢神经系综合病，表现为社交交流、communication和复制活动等障碍。这些障碍的严重程度不同，患有这个诊断的人面临着独特的挑战。尽管其主要起源是遗传的，但可以通过早期识别和治疗来提高其状况。在过去几年中，基于机器学习的智能诊断技术在传统临床方法的支持下 emerged as a supplement, aiming to address the potential drawbacks of time-consuming and costly traditional methods.在这种工作中，我们使用不同的机器学习算法来找出ASD最重要的特征和自动诊断过程。我们研究了六种分类模型，以确定哪种模型最适合识别ASD，并研究了五种流行的聚类方法，以获得有意义的ASD数据见解。为了选择最佳分类器，我们评估了模型使用精度、准确率、回归率、特征选择率、F1分数、AUC、κ和损失函数等指标。我们的评估表明，五个选择的模型在hyperparameter优化后都能够达到100%的准确率。由于大多数分类模型都能够达到100%的准确率，我们开始关注这些数据集的下面隐含的含义。我们在这些数据集上实施了一些流行的聚类算法，并计算了Normalized Mutual Information（NMI）、Adjusted Rand Index（ARI）和Silhouette Coefficient（SC）等指标，以选择最佳聚类模型。我们的评估发现，spectral clustering在NMI和ARI指标上表现出色，并且与k-means的最佳SC指标相比可观。”
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Negative-Signals-with-Self-Attention-for-Sequential-Music-Recommendation"><a href="#Leveraging-Negative-Signals-with-Self-Attention-for-Sequential-Music-Recommendation" class="headerlink" title="Leveraging Negative Signals with Self-Attention for Sequential Music Recommendation"></a>Leveraging Negative Signals with Self-Attention for Sequential Music Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11623">http://arxiv.org/abs/2309.11623</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pavan Seshadri, Peter Knees</li>
<li>For: This paper focuses on improving sequential music recommendation by incorporating negative session-level feedback using transformer-based self-attentive architectures and contrastive learning.* Methods: The paper proposes using transformer-based self-attentive models to learn implicit session-level information and incorporating negative feedback through a contrastive learning task.* Results: The paper shows that incorporating negative feedback through contrastive learning results in consistent performance gains over baseline architectures ignoring negative user feedback.<details>
<summary>Abstract</summary>
Music streaming services heavily rely on their recommendation engines to continuously provide content to their consumers. Sequential recommendation consequently has seen considerable attention in current literature, where state of the art approaches focus on self-attentive models leveraging contextual information such as long and short-term user history and item features; however, most of these studies focus on long-form content domains (retail, movie, etc.) rather than short-form, such as music. Additionally, many do not explore incorporating negative session-level feedback during training. In this study, we investigate the use of transformer-based self-attentive architectures to learn implicit session-level information for sequential music recommendation. We additionally propose a contrastive learning task to incorporate negative feedback (e.g skipped tracks) to promote positive hits and penalize negative hits. This task is formulated as a simple loss term that can be incorporated into a variety of deep learning architectures for sequential recommendation. Our experiments show that this results in consistent performance gains over the baseline architectures ignoring negative user feedback.
</details>
<details>
<summary>摘要</summary>
音乐流处服务重视推荐引擎，以提供不断的内容给消费者。顺序推荐得到了当前文献中一定的关注，现代approach都是基于自我注意力模型，利用用户历史记录和物品特征进行上下文ual information。然而，大多数研究都是针对长形内容领域（零售、电影等），而不是短形内容领域（如音乐）。另外，许多研究都不会在训练过程中包含负session-level反馈。在这个研究中，我们 investigate使用变换器基于自我注意力架构来学习隐藏session-level信息。我们还提出了一种对比学习任务，以包含负反馈（例如跳过的track），以便提高正确的hit和负反馈hit。这个任务被表示为一个简单的损失函数，可以与多种深度学习架构结合使用。我们的实验结果表明，这会导致 ignore negative user feedback的基eline架构的性能提高。
</details></li>
</ul>
<hr>
<h2 id="Latent-Diffusion-Models-for-Structural-Component-Design"><a href="#Latent-Diffusion-Models-for-Structural-Component-Design" class="headerlink" title="Latent Diffusion Models for Structural Component Design"></a>Latent Diffusion Models for Structural Component Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11601">http://arxiv.org/abs/2309.11601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ethan Herron, Jaydeep Rade, Anushrut Jignasu, Baskar Ganapathysubramanian, Aditya Balu, Soumik Sarkar, Adarsh Krishnamurthy</li>
<li>for: 这篇论文旨在提出一个构件设计生成框架，专门用于生成符合问题特定负载条件的结构元件。</li>
<li>methods: 我们使用一个对称扩散模型（Latent Diffusion model）来生成潜在的元件设计，以满足问题特定的负载条件。</li>
<li>results: 我们的方法可以实现对现有设计的编辑，并且可以实现高品质的结构性表现。我们的研究获得了量化的结果，证明了生成的设计具有内在的近乎最佳性。<details>
<summary>Abstract</summary>
Recent advances in generative modeling, namely Diffusion models, have revolutionized generative modeling, enabling high-quality image generation tailored to user needs. This paper proposes a framework for the generative design of structural components. Specifically, we employ a Latent Diffusion model to generate potential designs of a component that can satisfy a set of problem-specific loading conditions. One of the distinct advantages our approach offers over other generative approaches, such as generative adversarial networks (GANs), is that it permits the editing of existing designs. We train our model using a dataset of geometries obtained from structural topology optimization utilizing the SIMP algorithm. Consequently, our framework generates inherently near-optimal designs. Our work presents quantitative results that support the structural performance of the generated designs and the variability in potential candidate designs. Furthermore, we provide evidence of the scalability of our framework by operating over voxel domains with resolutions varying from $32^3$ to $128^3$. Our framework can be used as a starting point for generating novel near-optimal designs similar to topology-optimized designs.
</details>
<details>
<summary>摘要</summary>
最近的扩散模型技术进步，如扩散模型，对生成模型带来了革命性变革，使得可以生成高质量适应用户需求的图像。这篇论文提出了一个生成结构组件的框架。我们使用潜在扩散模型来生成可满足给定负荷条件的组件的潜在设计。与其他生成方法，如生成对抗网络（GANs）相比，我们的方法允许编辑现有设计。我们使用结构 topology 优化算法来获得几何数据，并在这些数据上训练我们的模型。因此，我们的框架可以生成自然near-optimal设计。我们的工作提供了量化结果，证明生成的设计具有结构性能的可靠性和可变性。此外，我们还证明了我们的框架可以在 voxel 领域中进行扩展，并且可以在 $32^3$ 到 $128^3$ 的分辨率范围内操作。我们的框架可以作为生成类似于 topology-optimized 设计的开始点。
</details></li>
</ul>
<hr>
<h2 id="Multiplying-poles-to-avoid-unwanted-points-in-root-finding-and-optimization"><a href="#Multiplying-poles-to-avoid-unwanted-points-in-root-finding-and-optimization" class="headerlink" title="Multiplying poles to avoid unwanted points in root finding and optimization"></a>Multiplying poles to avoid unwanted points in root finding and optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11475">http://arxiv.org/abs/2309.11475</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tuyen Trung Truong</li>
<li>for: 本文targets at solving the problem of avoiding the basin of attraction of a specific point in root finding and optimization.</li>
<li>methods: 提出了一种新的方法，即将函数值分割成一个适当的Power乘以距离集A的距离函数值，以避免在下一次算法中受到集A的吸引。</li>
<li>results: 提出了一种新的算法，可以帮助避免在root finding和优化中被吸引到特定点的basin of attraction中。该算法适用于iterative算法，并且可以在函数值为0时和函数值非零时两种情况下进行。此外，还提出了一种算法，可以帮助从一个正方向的分支中逃脱到另一个分支。<details>
<summary>Abstract</summary>
In root finding and optimization, there are many cases where there is a closed set $A$ one does not the sequence constructed by one's favourite method will converge to A (here, we do not assume extra properties on $A$ such as being convex or connected). For example, if one wants to find roots, and one chooses initial points in the basin of attraction for 1 root $x^*$ (a fact which one may not know before hand), then one will always end up in that root. In this case, one would like to have a mechanism to avoid this point $z^*$ in the next runs of one's algorithm.   In this paper, we propose a new method aiming to achieve this: we divide the cost function by an appropriate power of the distance function to $A$. This idea is inspired by how one would try to find all roots of a function in 1 variable. We first explain the heuristic for this method in the case where the minimum of the cost function is exactly 0, and then explain how to proceed if the minimum is non-zero (allowing both positive and negative values). The method is very suitable for iterative algorithms which have the descent property. We also propose, based on this, an algorithm to escape the basin of attraction of a component of positive dimension to reach another component.   Along the way, we compare with main existing relevant methods in the current literature. We provide several examples to illustrate the usefulness of the new approach.
</details>
<details>
<summary>摘要</summary>
在根寻找和优化中，有许多情况下，使用一种喜欢的方法constructing sequence将不会 converges to A（这里，我们不 assumption A是 convex或连通的其他性质）。例如，如果一个人想要找到根，并且选择初始点在拥有1根x*的基因囊拥（这可能是一个不知道的前提），那么一定会 ending up in that root。在这种情况下，我们希望有一种机制来避免这个点z*在下一次算法中。在这篇论文中，我们提出了一种新的方法，旨在实现这一点：我们将cost函数除以一个合适的powere distance函数到A。这个想法是根据在一个变量中找所有根的方法启发的。我们首先解释了在cost函数的最小值为0时的补做，然后解释如何处理非零最小值（允许正负值）。这种方法非常适合iterative算法，我们也建议一种使用这种方法逃脱基因囊拥的组分的方法。在进行这种方法的比较中，我们与现有的主要相关方法进行了比较。我们还提供了一些例子，以 Illustrate新的方法的有用性。
</details></li>
</ul>
<hr>
<h2 id="Model-free-tracking-control-of-complex-dynamical-trajectories-with-machine-learning"><a href="#Model-free-tracking-control-of-complex-dynamical-trajectories-with-machine-learning" class="headerlink" title="Model-free tracking control of complex dynamical trajectories with machine learning"></a>Model-free tracking control of complex dynamical trajectories with machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11470">http://arxiv.org/abs/2309.11470</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Zheng-Meng/TrackingControl">https://github.com/Zheng-Meng/TrackingControl</a></li>
<li>paper_authors: Zheng-Meng Zhai, Mohammadamin Moradi, Ling-Wei Kong, Bryan Glaz, Mulugeta Haile, Ying-Cheng Lai</li>
<li>for: 控制两臂 роботизированного护手器使其跟踪欲要的轨迹，应用于多种 цивиль和国防领域。</li>
<li>methods: 使用模型自由、机器学习框架，只使用部分观察状态来控制两臂 роботизированный护手器。</li>
<li>results: 通过使用各种 periodic和异常信号，证明了控制框架的有效性，并在测试阶段（部署阶段）下确认了其对测量噪声、干扰和不确定性的稳定性。<details>
<summary>Abstract</summary>
Nonlinear tracking control enabling a dynamical system to track a desired trajectory is fundamental to robotics, serving a wide range of civil and defense applications. In control engineering, designing tracking control requires complete knowledge of the system model and equations. We develop a model-free, machine-learning framework to control a two-arm robotic manipulator using only partially observed states, where the controller is realized by reservoir computing. Stochastic input is exploited for training, which consists of the observed partial state vector as the first and its immediate future as the second component so that the neural machine regards the latter as the future state of the former. In the testing (deployment) phase, the immediate-future component is replaced by the desired observational vector from the reference trajectory. We demonstrate the effectiveness of the control framework using a variety of periodic and chaotic signals, and establish its robustness against measurement noise, disturbances, and uncertainties.
</details>
<details>
<summary>摘要</summary>
非线性跟踪控制，使动力系统跟踪所需的轨迹是机器人控制的基础，广泛应用于文明和国防领域。在控制工程中，设计跟踪控制需要完整的系统模型和方程。我们开发了一个无模型、机器学习框架，控制两臂机械 manipulate 器使用只有部分观察状态，控制器通过 rezzo 计算机。在训练阶段，利用 Stochastic 输入，训练过程包括观察的部分状态向量作为第一个组成部分，以及其未来的状态向量作为第二个组成部分，因此 neural machine 将后者视为前者的未来状态。在测试（部署）阶段，未来状态向量被替换为来自参照轨迹的所需观察向量。我们使用了多种 periodic 和混沌信号进行测试，并证明了控制框架的可靠性，对测量噪音、干扰和不确定性的抗性。
</details></li>
</ul>
<hr>
<h2 id="Digital-twins-of-nonlinear-dynamical-systems-A-perspective"><a href="#Digital-twins-of-nonlinear-dynamical-systems-A-perspective" class="headerlink" title="Digital twins of nonlinear dynamical systems: A perspective"></a>Digital twins of nonlinear dynamical systems: A perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11461">http://arxiv.org/abs/2309.11461</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ying-Cheng Lai</li>
<li>for: 预测和避免非线性动力系统的突然规模事件</li>
<li>methods:  sparse optimization和机器学习两种方法</li>
<li>results: 可以预测和避免非线性动力系统的突然规模事件，提供早期警示和预测性解决方案<details>
<summary>Abstract</summary>
Digital twins have attracted a great deal of recent attention from a wide range of fields. A basic requirement for digital twins of nonlinear dynamical systems is the ability to generate the system evolution and predict potentially catastrophic emergent behaviors so as to providing early warnings. The digital twin can then be used for system "health" monitoring in real time and for predictive problem solving. In particular, if the digital twin forecasts a possible system collapse in the future due to parameter drifting as caused by environmental changes or perturbations, an optimal control strategy can be devised and executed as early intervention to prevent the collapse. Two approaches exist for constructing digital twins of nonlinear dynamical systems: sparse optimization and machine learning. The basics of these two approaches are described and their advantages and caveats are discussed.
</details>
<details>
<summary>摘要</summary>
<<SYS>>非线性动力系统的数字孪生有很多最近的关注，来自各种领域。数字孪生的基本要求是能够生成系统演化和预测可能出现的灾难性行为，以提供早期警示。数字孪生可以用于实时监测系统“健康”状态，并预测问题。特别是，如果数字孪生预测系统将在未来因为环境变化或干扰而导致崩溃，就可以根据这个预测来设计和执行早期干预措施，以避免崩溃。构建非线性动力系统的数字孪生有两种方法：散列优化和机器学习。这两种方法的基础和优缺点都是介绍的。>>>
</details></li>
</ul>
<hr>
<h2 id="Multi-Step-Model-Predictive-Safety-Filters-Reducing-Chattering-by-Increasing-the-Prediction-Horizon"><a href="#Multi-Step-Model-Predictive-Safety-Filters-Reducing-Chattering-by-Increasing-the-Prediction-Horizon" class="headerlink" title="Multi-Step Model Predictive Safety Filters: Reducing Chattering by Increasing the Prediction Horizon"></a>Multi-Step Model Predictive Safety Filters: Reducing Chattering by Increasing the Prediction Horizon</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11453">http://arxiv.org/abs/2309.11453</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/federico-pizarrobejarano/safe-control-gym">https://github.com/federico-pizarrobejarano/safe-control-gym</a></li>
<li>paper_authors: Federico Pizarro Bejarano, Lukas Brunke, Angela P. Schoellig</li>
<li>for: This paper aims to improve the safety guarantees of learning-based controllers by reducing chattering in model predictive safety filters (MPSFs).</li>
<li>methods: The proposed approach considers input corrections over a longer horizon and uses techniques from robust MPC to prove recursive feasibility, reducing chattering by more than a factor of 4 compared to previous MPSF formulations.</li>
<li>results: The proposed approach is verified through extensive simulation and quadrotor experiments, demonstrating the preservation of desired safety guarantees and a significant reduction in chattering compared to previous MPSF formulations.<details>
<summary>Abstract</summary>
Learning-based controllers have demonstrated superior performance compared to classical controllers in various tasks. However, providing safety guarantees is not trivial. Safety, the satisfaction of state and input constraints, can be guaranteed by augmenting the learned control policy with a safety filter. Model predictive safety filters (MPSFs) are a common safety filtering approach based on model predictive control (MPC). MPSFs seek to guarantee safety while minimizing the difference between the proposed and applied inputs in the immediate next time step. This limited foresight can lead to jerky motions and undesired oscillations close to constraint boundaries, known as chattering. In this paper, we reduce chattering by considering input corrections over a longer horizon. Under the assumption of bounded model uncertainties, we prove recursive feasibility using techniques from robust MPC. We verified the proposed approach in both extensive simulation and quadrotor experiments. In experiments with a Crazyflie 2.0 drone, we show that, in addition to preserving the desired safety guarantees, the proposed MPSF reduces chattering by more than a factor of 4 compared to previous MPSF formulations.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Distribution-and-volume-based-scoring-for-Isolation-Forests"><a href="#Distribution-and-volume-based-scoring-for-Isolation-Forests" class="headerlink" title="Distribution and volume based scoring for Isolation Forests"></a>Distribution and volume based scoring for Isolation Forests</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11450">http://arxiv.org/abs/2309.11450</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest">https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest</a></li>
<li>paper_authors: Hichem Dhouib, Alissa Wilms, Paul Boes</li>
<li>for: 本研究提出了两种改进方法 дляIsland Forest方法，以提高异常检测的精度和效果。</li>
<li>methods: 第一种方法是基于信息理论的总体分数函数的扩展，可以考虑整个分布而不仅仅是树ensemble平均值。第二种方法是在隔离树 estimator  nivel replace depth-based 分数函数。</li>
<li>results: 对于生成的数据和34个&#96;&#96;ADBench’’ benchmark dataset进行了评估，发现使用这两种方法可以在某些dataset上提高异常检测的精度，并且在所有dataset上平均上提高一种变体。代码可以在提交中找到。<details>
<summary>Abstract</summary>
We make two contributions to the Isolation Forest method for anomaly and outlier detection. The first contribution is an information-theoretically motivated generalisation of the score function that is used to aggregate the scores across random tree estimators. This generalisation allows one to take into account not just the ensemble average across trees but instead the whole distribution. The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree's leaf nodes.   We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive ``ADBench'' benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants. The code to reproduce our results is made available as part of the submission.
</details>
<details>
<summary>摘要</summary>
我们做了两个贡献到隔离森林方法中，用于异常和偏出检测。第一个贡献是基于信息理论的预测函数的一种扩展，用于聚合随机树估计值。这个扩展允许我们考虑不仅ensemble均值过滤，而是整个分布。第二个贡献是将隔离树估计值中的深度基于的评分函数 replaced with hyper-volume association with isolation tree leaf nodes。我们在生成数据上验证了这两种方法，并在``ADBench''benchmark中的34个数据集上进行了评估，发现这两种方法在一些数据集上有所改善，而且在所有数据集上的平均改善。我们的结果可以在提交中找到相关的代码。
</details></li>
</ul>
<hr>
<h2 id="Deep-Networks-as-Denoising-Algorithms-Sample-Efficient-Learning-of-Diffusion-Models-in-High-Dimensional-Graphical-Models"><a href="#Deep-Networks-as-Denoising-Algorithms-Sample-Efficient-Learning-of-Diffusion-Models-in-High-Dimensional-Graphical-Models" class="headerlink" title="Deep Networks as Denoising Algorithms: Sample-Efficient Learning of Diffusion Models in High-Dimensional Graphical Models"></a>Deep Networks as Denoising Algorithms: Sample-Efficient Learning of Diffusion Models in High-Dimensional Graphical Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11420">http://arxiv.org/abs/2309.11420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Song Mei, Yuchen Wu</li>
<li>for: 本研究探讨了深度神经网络在diffusion-based生成模型中的折衔效率。现有的折衔理论假设了折衔函数的光滑性，但是这些理论受到维度约束的困难，特别是图形模型如Markov随机场，这些模型通常用于图像分布。</li>
<li>methods: 我们观察到，在图形模型中，折衔函数可以通过变分推理算法得到有效的近似。此外，这些算法可以有效地表示为神经网络。我们在Isling模型、conditional Ising模型、restricted Boltzmann机和简单编码模型中进行了示例。</li>
<li>results: 我们提供了一种基于diffusion-based sampling的有效样本复杂度 bound，当折衔函数是通过深度神经网络学习得到的时候。<details>
<summary>Abstract</summary>
We investigate the approximation efficiency of score functions by deep neural networks in diffusion-based generative modeling. While existing approximation theories utilize the smoothness of score functions, they suffer from the curse of dimensionality for intrinsically high-dimensional data. This limitation is pronounced in graphical models such as Markov random fields, common for image distributions, where the approximation efficiency of score functions remains unestablished.   To address this, we observe score functions can often be well-approximated in graphical models through variational inference denoising algorithms. Furthermore, these algorithms are amenable to efficient neural network representation. We demonstrate this in examples of graphical models, including Ising models, conditional Ising models, restricted Boltzmann machines, and sparse encoding models. Combined with off-the-shelf discretization error bounds for diffusion-based sampling, we provide an efficient sample complexity bound for diffusion-based generative modeling when the score function is learned by deep neural networks.
</details>
<details>
<summary>摘要</summary>
我团队研究使用深度神经网络来近似分布式生成模型中的分数函数的效率。现有的近似理论利用分数函数的平滑性，但是它们由于数维度的封闭而受到诅咒性的影响，特别是图形模型，如图像分布中的马可夫随机场，其中分数函数的近似效率未能得到确定。为了解决这个问题，我们发现分数函数在图形模型中可以通过变量推理梯度下降算法进行良好的近似。此外，这些算法可以fficient地表示为神经网络。我们在图像分布中的伊辛模型、条件伊辛模型、受限的博尔tz曼机和简洁编码模型中进行了示例。与市场上的批量误差边界相结合，我们提供了一个高效的样本复杂度下界 для diffusion-based生成模型，当分数函数被深度神经网络学习时。
</details></li>
</ul>
<hr>
<h2 id="Transformers-versus-LSTMs-for-electronic-trading"><a href="#Transformers-versus-LSTMs-for-electronic-trading" class="headerlink" title="Transformers versus LSTMs for electronic trading"></a>Transformers versus LSTMs for electronic trading</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11400">http://arxiv.org/abs/2309.11400</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Bilokon, Yitao Qiu</li>
<li>for: 这个研究的目的是确定Transformer模型是否可以在金融时间序预测中取代LSTM模型，并比较了不同的LSTM和Transformer模型在多个金融预测任务中的表现。</li>
<li>methods: 该研究使用了多种LSTM和Transformer模型，包括一种新的DLSTM模型和一种适应金融预测的Transformer模型。</li>
<li>results: 实验结果表明，Transformer模型只有在绝对价格序列预测方面表现出有限的优势，而LSTM模型在差价序列预测和价格运动预测方面表现更好和更稳定。<details>
<summary>Abstract</summary>
With the rapid development of artificial intelligence, long short term memory (LSTM), one kind of recurrent neural network (RNN), has been widely applied in time series prediction.   Like RNN, Transformer is designed to handle the sequential data. As Transformer achieved great success in Natural Language Processing (NLP), researchers got interested in Transformer's performance on time series prediction, and plenty of Transformer-based solutions on long time series forecasting have come out recently. However, when it comes to financial time series prediction, LSTM is still a dominant architecture. Therefore, the question this study wants to answer is: whether the Transformer-based model can be applied in financial time series prediction and beat LSTM.   To answer this question, various LSTM-based and Transformer-based models are compared on multiple financial prediction tasks based on high-frequency limit order book data. A new LSTM-based model called DLSTM is built and new architecture for the Transformer-based model is designed to adapt for financial prediction. The experiment result reflects that the Transformer-based model only has the limited advantage in absolute price sequence prediction. The LSTM-based models show better and more robust performance on difference sequence prediction, such as price difference and price movement.
</details>
<details>
<summary>摘要</summary>
随着人工智能的快速发展，长短期记忆（LSTM），一种回归神经网络（RNN），在时间序列预测中得到了广泛的应用。与RNN类似，Transformer是用于处理时间序列数据的设计。由于Transformer在自然语言处理（NLP）中取得了巨大成功，研究人员对Transformer在时间序列预测中的表现感到兴趣，并在最近出现了许多基于Transformer的解决方案。然而，在金融时间序列预测中，LSTM仍然是主导的建筑。因此，本研究的问题是：可否使用Transformer-based模型来预测金融时间序列，并超越LSTM。为了回答这个问题，本研究对多种LSTM-based和Transformer-based模型进行了比较，并在高频限制ORDER BOOK数据上进行了多个金融预测任务。此外，一种新的LSTM-based模型called DLSTM被建立，并对Financial prediction进行了新的建筑。实验结果表明，Transformer-based模型只有有限的优势在绝对价格序列预测中。相比之下，LSTM-based模型在差价序列预测中表现更好和更加稳定，例如价格差和价格运动。
</details></li>
</ul>
<hr>
<h2 id="SR-PredictAO-Session-based-Recommendation-with-High-Capability-Predictor-Add-On"><a href="#SR-PredictAO-Session-based-Recommendation-with-High-Capability-Predictor-Add-On" class="headerlink" title="SR-PredictAO: Session-based Recommendation with High-Capability Predictor Add-On"></a>SR-PredictAO: Session-based Recommendation with High-Capability Predictor Add-On</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12218">http://arxiv.org/abs/2309.12218</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rickyskywalker/sr-predictao-official">https://github.com/rickyskywalker/sr-predictao-official</a></li>
<li>paper_authors: Ruida Wang, Raymond Chi-Wing Wong, Weile Tan</li>
<li>for: 本研究旨在提出一种能够在单个会话中预测用户下一步行为的Session-based recommendation模型，以解决现有模型中随机用户行为的影响。</li>
<li>methods: 我们提出了一种新的框架 called SR-PredictAO，它包括一个高能力预测器模块，可以减轻用户行为的随机性对预测的影响。此外，我们还提出了一种可以应用于现有模型上的高能力预测器模块优化方法。</li>
<li>results: 我们在两个实际数据集上进行了广泛的实验，并证明了SR-PredictAO在三种现有模型上的表现比现有模型更好，具体来说，SR-PredictAO在HR@20和MRR@20上比现有模型高出2.9%和2.3%。此外，这些改进都是在大多数现有模型上的所有数据集上进行的，这可以被视为Session-based recommendation领域的一项重要贡献。<details>
<summary>Abstract</summary>
Session-based recommendation, aiming at making the prediction of the user's next item click based on the information in a single session only even in the presence of some random user's behavior, is a complex problem. This complex problem requires a high-capability model of predicting the user's next action. Most (if not all) existing models follow the encoder-predictor paradigm where all studies focus on how to optimize the encoder module extensively in the paradigm but they ignore how to optimize the predictor module. In this paper, we discover the existing critical issue of the low-capability predictor module among existing models. Motivated by this, we propose a novel framework called \emph{\underline{S}ession-based \underline{R}ecommendation with \underline{Pred}ictor \underline{A}dd-\underline{O}n} (SR-PredictAO). In this framework, we propose a high-capability predictor module which could alleviate the effect of random user's behavior for prediction. It is worth mentioning that this framework could be applied to any existing models, which could give opportunities for further optimizing the framework. Extensive experiments on two real benchmark datasets for three state-of-the-art models show that \emph{SR-PredictAO} out-performs the current state-of-the-art model by up to 2.9\% in HR@20 and 2.3\% in MRR@20. More importantly, the improvement is consistent across almost all the existing models on all datasets, which could be regarded as a significant contribution in the field.
</details>
<details>
<summary>摘要</summary>
Session-based 推荐，targeting at predicting the user's next item click based on the information in a single session, is a complex problem. This complex problem requires a high-capability model for predicting the user's next action. Most (if not all) existing models follow the encoder-predictor paradigm, where all studies focus on optimizing the encoder module extensively in the paradigm but ignore the predictor module. In this paper, we discover the existing critical issue of the low-capability predictor module among existing models. Motivated by this, we propose a novel framework called \emph{\underline{S}ession-based \underline{R}ecommendation with \underline{Pred}ictor \underline{A}dd-\underline{O}n} (SR-PredictAO). In this framework, we propose a high-capability predictor module that can alleviate the effect of random user behavior for prediction. It is worth mentioning that this framework can be applied to any existing models, which can provide opportunities for further optimizing the framework. Extensive experiments on two real benchmark datasets for three state-of-the-art models show that \emph{SR-PredictAO} outperforms the current state-of-the-art model by up to 2.9\% in HR@20 and 2.3\% in MRR@20. More importantly, the improvement is consistent across almost all existing models on all datasets, which can be regarded as a significant contribution in the field.
</details></li>
</ul>
<hr>
<h2 id="Learning-Patient-Static-Information-from-Time-series-EHR-and-an-Approach-for-Safeguarding-Privacy-and-Fairness"><a href="#Learning-Patient-Static-Information-from-Time-series-EHR-and-an-Approach-for-Safeguarding-Privacy-and-Fairness" class="headerlink" title="Learning Patient Static Information from Time-series EHR and an Approach for Safeguarding Privacy and Fairness"></a>Learning Patient Static Information from Time-series EHR and an Approach for Safeguarding Privacy and Fairness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11373">http://arxiv.org/abs/2309.11373</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Liao, Joel Voldman</li>
<li>for: 这种研究旨在 investigate the ability of time-series electronic health record data to predict patient static information, and to develop a general approach to protect patient-sensitive attribute information for downstream tasks.</li>
<li>methods: 研究使用了时序数据和机器学习模型，并使用了多种方法和数据库来评估模型的性能。</li>
<li>results: 研究发现， raw time-series data 和机器学习模型学习的表示可以高度预测patient的静态信息，包括生物性别、年龄和自reported race。此外，这些预测性能可以扩展到各种相关疾病因素，并且存在even when the model was trained for different tasks, using different cohorts, using different model architectures and databases.<details>
<summary>Abstract</summary>
Recent work in machine learning for healthcare has raised concerns about patient privacy and algorithmic fairness. For example, previous work has shown that patient self-reported race can be predicted from medical data that does not explicitly contain racial information. However, the extent of data identification is unknown, and we lack ways to develop models whose outcomes are minimally affected by such information. Here we systematically investigated the ability of time-series electronic health record data to predict patient static information. We found that not only the raw time-series data, but also learned representations from machine learning models, can be trained to predict a variety of static information with area under the receiver operating characteristic curve as high as 0.851 for biological sex, 0.869 for binarized age and 0.810 for self-reported race. Such high predictive performance can be extended to a wide range of comorbidity factors and exists even when the model was trained for different tasks, using different cohorts, using different model architectures and databases. Given the privacy and fairness concerns these findings pose, we develop a variational autoencoder-based approach that learns a structured latent space to disentangle patient-sensitive attributes from time-series data. Our work thoroughly investigates the ability of machine learning models to encode patient static information from time-series electronic health records and introduces a general approach to protect patient-sensitive attribute information for downstream tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Using-Property-Elicitation-to-Understand-the-Impacts-of-Fairness-Constraints"><a href="#Using-Property-Elicitation-to-Understand-the-Impacts-of-Fairness-Constraints" class="headerlink" title="Using Property Elicitation to Understand the Impacts of Fairness Constraints"></a>Using Property Elicitation to Understand the Impacts of Fairness Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11343">http://arxiv.org/abs/2309.11343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jessie Finocchiaro</li>
<li>for: 本研究旨在理解许多预测算法的训练过程中，添加正则化函数会如何改变优化目标的最小值。</li>
<li>methods: 本研究使用属性描述来探讨诸如产品分布变化和约束松弛等因素对优化目标的影响。</li>
<li>results: 研究发现，添加正则化函数可能会改变优化目标的最小值，并且可以通过属性描述来描述这种改变。此外，研究还发现在不同的数据分布和约束条件下，算法决策的变化。<details>
<summary>Abstract</summary>
Predictive algorithms are often trained by optimizing some loss function, to which regularization functions are added to impose a penalty for violating constraints. As expected, the addition of such regularization functions can change the minimizer of the objective. It is not well-understood which regularizers change the minimizer of the loss, and, when the minimizer does change, how it changes. We use property elicitation to take first steps towards understanding the joint relationship between the loss and regularization functions and the optimal decision for a given problem instance. In particular, we give a necessary and sufficient condition on loss and regularizer pairs for when a property changes with the addition of the regularizer, and examine some regularizers satisfying this condition standard in the fair machine learning literature. We empirically demonstrate how algorithmic decision-making changes as a function of both data distribution changes and hardness of the constraints.
</details>
<details>
<summary>摘要</summary>
预测算法经常通过优化损失函数来训练，并将正则函数添加到损失函数中以实现一些约束。预期地，添加正则函数会改变损失函数的最小值。然而，我们不很了解哪些正则函数会改变损失函数的最小值，以及这些改变是如何发生的。我们使用财产描述来开始理解损失和正则函数对于给定问题实例的优化决策的关系。特别是，我们给出了损失和正则函数对的必要和 suficient condition，并考察了常见的公平机器学习 литературе中的一些满足这个condition的正则函数。我们通过实验表明，在数据分布变化和约束硬度变化的情况下，算法决策会发生变化。
</details></li>
</ul>
<hr>
<h2 id="WFTNet-Exploiting-Global-and-Local-Periodicity-in-Long-term-Time-Series-Forecasting"><a href="#WFTNet-Exploiting-Global-and-Local-Periodicity-in-Long-term-Time-Series-Forecasting" class="headerlink" title="WFTNet: Exploiting Global and Local Periodicity in Long-term Time Series Forecasting"></a>WFTNet: Exploiting Global and Local Periodicity in Long-term Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11319">http://arxiv.org/abs/2309.11319</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peiyuan Liu, Beiliang Wu, Naiqi Li, Tao Dai, Fengmao Lei, Jigang Bao, Yong Jiang, Shu-Tao Xia</li>
<li>for: 预测长期时间序列，使用波峰变换网络（WFTNet）捕捉全面的时间频率信息。</li>
<li>methods: 使用波峰变换和 Fourier 变换两者，捕捉全面的时间频率信息，并引入周期性权重因子（PWC）自适应地平衡全面和本地频率模式的重要性。</li>
<li>results: 对多种时间序列数据进行了广泛的实验，并 consistently 超过了其他基准值。<details>
<summary>Abstract</summary>
Recent CNN and Transformer-based models tried to utilize frequency and periodicity information for long-term time series forecasting. However, most existing work is based on Fourier transform, which cannot capture fine-grained and local frequency structure. In this paper, we propose a Wavelet-Fourier Transform Network (WFTNet) for long-term time series forecasting. WFTNet utilizes both Fourier and wavelet transforms to extract comprehensive temporal-frequency information from the signal, where Fourier transform captures the global periodic patterns and wavelet transform captures the local ones. Furthermore, we introduce a Periodicity-Weighted Coefficient (PWC) to adaptively balance the importance of global and local frequency patterns. Extensive experiments on various time series datasets show that WFTNet consistently outperforms other state-of-the-art baseline.
</details>
<details>
<summary>摘要</summary>
近期的CNN和Transformer模型尝试利用频率和周期信息进行长期时间序预测。然而，大多数现有工作基于傅里叶变换，这无法捕捉细致的频率结构。在这篇论文中，我们提出了一种幂 transformed-wavelet网络（WFTNet），用于长期时间序预测。WFTNet利用了傅里叶和wavelet变换来提取时间序列中的全面时间频率信息，其中傅里叶变换捕捉到全球性征周期模式，wavelet变换捕捉到本地性征周期模式。此外，我们引入了一种 Periodicity-Weighted Coefficient（PWC），以适应地 adaptively 衡量全球和本地频率模式的重要性。我们在不同的时间序列数据集上进行了广泛的实验，并证明了WFTNet在其他基eline上 consistently 升级。
</details></li>
</ul>
<hr>
<h2 id="Create-and-Find-Flatness-Building-Flat-Training-Spaces-in-Advance-for-Continual-Learning"><a href="#Create-and-Find-Flatness-Building-Flat-Training-Spaces-in-Advance-for-Continual-Learning" class="headerlink" title="Create and Find Flatness: Building Flat Training Spaces in Advance for Continual Learning"></a>Create and Find Flatness: Building Flat Training Spaces in Advance for Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11305">http://arxiv.org/abs/2309.11305</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Eric8932/Create-and-Find-Flatness">https://github.com/Eric8932/Create-and-Find-Flatness</a></li>
<li>paper_authors: Wenhang Shi, Yiren Chen, Zhe Zhao, Wei Lu, Kimmo Yan, Xiaoyong Du</li>
<li>for: 本研究旨在解决 continual learning 中的恶性忘记问题，提高 neural network 在学习新任务时保持之前任务知识的能力。</li>
<li>methods: 我们提出了一种 novel 的 Create and Find Flatness（C&amp;F）框架，在每个任务学习阶段建立一个适应当任务的平坦训练空间。在学习当前任务时，我们适应创建一个损失函数的平坦区域，然后根据参数对当前任务的重要性进行评估。在适应新任务时，我们会应用约束以根据平坦度，同时为新任务准备平坦的训练空间。</li>
<li>results: 我们的 C&amp;F 框架在 standalone continual learning 中表现出色，并且可以与其他方法组合使用。实验结果表明，C&amp;F 可以保持之前任务知识，同时学习新任务，并且在不同的 dataset 上具有稳定的性能。<details>
<summary>Abstract</summary>
Catastrophic forgetting remains a critical challenge in the field of continual learning, where neural networks struggle to retain prior knowledge while assimilating new information. Most existing studies emphasize mitigating this issue only when encountering new tasks, overlooking the significance of the pre-task phase. Therefore, we shift the attention to the current task learning stage, presenting a novel framework, C&F (Create and Find Flatness), which builds a flat training space for each task in advance. Specifically, during the learning of the current task, our framework adaptively creates a flat region around the minimum in the loss landscape. Subsequently, it finds the parameters' importance to the current task based on their flatness degrees. When adapting the model to a new task, constraints are applied according to the flatness and a flat space is simultaneously prepared for the impending task. We theoretically demonstrate the consistency between the created and found flatness. In this manner, our framework not only accommodates ample parameter space for learning new tasks but also preserves the preceding knowledge of earlier tasks. Experimental results exhibit C&F's state-of-the-art performance as a standalone continual learning approach and its efficacy as a framework incorporating other methods. Our work is available at https://github.com/Eric8932/Create-and-Find-Flatness.
</details>
<details>
<summary>摘要</summary>
catastrophic forgetting 是一个重要挑战在持续学习领域， neural network 在接受新任务时忘记之前的知识是一个关键问题。  existing studies 通常只关注在新任务上 mitigating 这个问题，忽视了 pre-task 阶段的重要性。 因此，我们将注意力集中在当前任务学习阶段，提出了一种新的框架， C&F（Create and Find Flatness），它在每个任务之前建立了一个平坦的训练空间。 specifically, 在学习当前任务时，我们的框架会动态创建一个缺失的最小值附近的平坦区域。 然后，它会根据参数的平坦度来确定参数的当前任务重要性。 当适应新任务时，我们会根据平坦度应用约束，并同时为下一个任务准备一个平坦的空间。 我们理论上验证了创建和发现平坦的一致性。 因此，我们的框架不仅为学习新任务提供了充足的参数空间，而且也保留了前一个任务中的知识。 实验结果表明 C&F 能够独立地实现状态机器学习的表现，同时作为其他方法的框架也有出色的效果。 我们的工作可以在 <https://github.com/Eric8932/Create-and-Find-Flatness> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Accuracy-Measuring-Representation-Capacity-of-Embeddings-to-Preserve-Structural-and-Contextual-Information"><a href="#Beyond-Accuracy-Measuring-Representation-Capacity-of-Embeddings-to-Preserve-Structural-and-Contextual-Information" class="headerlink" title="Beyond Accuracy: Measuring Representation Capacity of Embeddings to Preserve Structural and Contextual Information"></a>Beyond Accuracy: Measuring Representation Capacity of Embeddings to Preserve Structural and Contextual Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11294">http://arxiv.org/abs/2309.11294</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarwan Ali</li>
<li>for: 这个论文的目的是提出一种方法来评估嵌入的质量和容量，以便更好地理解嵌入在不同应用中的效果。</li>
<li>methods: 该论文使用了外部评估方法（如分类和聚类）和t-SNE基于的邻居分析（如邻居一致性和信任度）来全面评估嵌入的质量和容量。同时，使用bayesian优化技术来优化评估 metric的权重，以确保一个数据驱动的、 объек oriented 的方法。</li>
<li>results: 该论文通过使用三个生物序列数据集（蛋白质和核酸）和四种嵌入方法（Spike2Vec、Spaced k-mers、PWM2Vec 和 AutoEncoder）进行评估，并结果表明该方法可以帮助研究者和实践者更好地理解嵌入在不同应用中的效果，并提供一个量化的评估方法。<details>
<summary>Abstract</summary>
Effective representation of data is crucial in various machine learning tasks, as it captures the underlying structure and context of the data. Embeddings have emerged as a powerful technique for data representation, but evaluating their quality and capacity to preserve structural and contextual information remains a challenge. In this paper, we address this need by proposing a method to measure the \textit{representation capacity} of embeddings. The motivation behind this work stems from the importance of understanding the strengths and limitations of embeddings, enabling researchers and practitioners to make informed decisions in selecting appropriate embedding models for their specific applications. By combining extrinsic evaluation methods, such as classification and clustering, with t-SNE-based neighborhood analysis, such as neighborhood agreement and trustworthiness, we provide a comprehensive assessment of the representation capacity. Additionally, the use of optimization techniques (bayesian optimization) for weight optimization (for classification, clustering, neighborhood agreement, and trustworthiness) ensures an objective and data-driven approach in selecting the optimal combination of metrics. The proposed method not only contributes to advancing the field of embedding evaluation but also empowers researchers and practitioners with a quantitative measure to assess the effectiveness of embeddings in capturing structural and contextual information. For the evaluation, we use $3$ real-world biological sequence (proteins and nucleotide) datasets and performed representation capacity analysis of $4$ embedding methods from the literature, namely Spike2Vec, Spaced $k$-mers, PWM2Vec, and AutoEncoder.
</details>
<details>
<summary>摘要</summary>
效果表示数据的表示是机器学习任务中的关键，它捕捉了数据的下面结构和上下文。嵌入在机器学习中出现为一种强大的表示技巧，但评估其质量和保持结构和上下文信息的能力仍然是一个挑战。本文提出一种方法来衡量嵌入的表示能力。这种方法的动机来自于了理解嵌入的优劣点，以便研究者和实践者可以根据特定应用选择合适的嵌入模型。通过结合外部评估方法（如分类和聚类）和t-SNE基于的邻居分析（如邻居一致和信任度），我们提供了一种全面的评估方法。此外，使用搜索算法（bayesian优化）来优化参数（如分类、聚类、邻居一致和信任度），确保了一种客观和数据驱动的方法来选择最佳的综合指标。该方法不仅为嵌入评估领域做出了贡献，还为研究者和实践者提供了一个量化的评估方法，以评估嵌入是否能够有效地捕捉结构和上下文信息。为评估，我们使用了3个实际生物序列（蛋白质和核苷酸）数据集，并对Literature中的4种嵌入方法进行表示能力分析，即Spike2Vec、Spaced k-mers、PWM2Vec和AutoEncoder。
</details></li>
</ul>
<hr>
<h2 id="Grassroots-Operator-Search-for-Model-Edge-Adaptation"><a href="#Grassroots-Operator-Search-for-Model-Edge-Adaptation" class="headerlink" title="Grassroots Operator Search for Model Edge Adaptation"></a>Grassroots Operator Search for Model Edge Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11246">http://arxiv.org/abs/2309.11246</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hadjer Benmeziane, Kaoutar El Maghraoui, Hamza Ouarnoughi, Smail Niar</li>
<li>for: 这种论文的目的是提出一种基于数学基础的 neural architecture search（NAS）方法，用于适应边缘设备上的深度学习模型。</li>
<li>methods: 该方法使用 Grassroots Operator Search（GOS）方法，通过搜索和选择高效的操作符来代替原始模型中的操作符，以提高模型的计算效率while maintaining high accuracy。</li>
<li>results: 在多种深度学习模型上，该方法可以在Redmi Note 7S和Raspberry Pi3等边缘设备上实现至少2.2倍的计算速度提升，同时保持高度的准确率。此外，在脉冲频度估计应用中，该方法可以达到状态 Künstler的性能，同时保持计算复杂度的减少，证明了该方法的实用性。<details>
<summary>Abstract</summary>
Hardware-aware Neural Architecture Search (HW-NAS) is increasingly being used to design efficient deep learning architectures. An efficient and flexible search space is crucial to the success of HW-NAS. Current approaches focus on designing a macro-architecture and searching for the architecture's hyperparameters based on a set of possible values. This approach is biased by the expertise of deep learning (DL) engineers and standard modeling approaches. In this paper, we present a Grassroots Operator Search (GOS) methodology. Our HW-NAS adapts a given model for edge devices by searching for efficient operator replacement. We express each operator as a set of mathematical instructions that capture its behavior. The mathematical instructions are then used as the basis for searching and selecting efficient replacement operators that maintain the accuracy of the original model while reducing computational complexity. Our approach is grassroots since it relies on the mathematical foundations to construct new and efficient operators for DL architectures. We demonstrate on various DL models, that our method consistently outperforms the original models on two edge devices, namely Redmi Note 7S and Raspberry Pi3, with a minimum of 2.2x speedup while maintaining high accuracy. Additionally, we showcase a use case of our GOS approach in pulse rate estimation on wristband devices, where we achieve state-of-the-art performance, while maintaining reduced computational complexity, demonstrating the effectiveness of our approach in practical applications.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种基于植物架构的搜寻方法，称为Grassroots Operator Search（GOS）。我们的HW-NAS方法运用了一个基于植物架构的搜寻空间，寻找高效的操作器替代。我们将每个操作器表示为一些数学指令，这些指令 capture了操作器的行为。这些数学指令后来用作搜寻和选择高效的操作器替代，以维持原始模型的精度，并降低计算复杂性。我们的方法是一种基于植物的方法，因为它将基于植物架构的数学基础建构新的高效操作器。我们在不同的深度学习模型上进行了评估，我们的方法在Redmi Note 7S和Raspberry Pi3等两个边缘设备上显示了至少2.2倍的速度提升，同时维持高精度。此外，我们还展示了我们的GOS方法在脉搏监测器上的实际应用，在这个应用中，我们取得了现有最佳性能，同时维持了降低的计算复杂性，实证了我们的方法在实际应用中的有效性。
</details></li>
</ul>
<hr>
<h2 id="Towards-a-Prediction-of-Machine-Learning-Training-Time-to-Support-Continuous-Learning-Systems-Development"><a href="#Towards-a-Prediction-of-Machine-Learning-Training-Time-to-Support-Continuous-Learning-Systems-Development" class="headerlink" title="Towards a Prediction of Machine Learning Training Time to Support Continuous Learning Systems Development"></a>Towards a Prediction of Machine Learning Training Time to Support Continuous Learning Systems Development</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11226">http://arxiv.org/abs/2309.11226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesca Marzi, Giordano d’Aloisio, Antinisca Di Marco, Giovanni Stilo</li>
<li>for: 预测机器学习模型训练时间的问题在科学社区中变得非常重要。可以预测ML模型训练时间，可以自动选择最佳模型，以提高能效性和性能。本文描述我们在这个方向上的工作。</li>
<li>methods: 我们对 Zheng et al.提出的Full Parameter Time Complexity (FPTC)方法进行了广泛的实证研究。这是我们知道的唯一一种形式化ML模型训练时间与数据集和模型参数之间的关系。我们研究了逻辑回归和随机森林分类器的形ulation，并指出了主要的优点和缺点。</li>
<li>results: 我们发现，从实验结果来看，训练时间预测与数据集上下文有着紧密的关系。FPTC方法不能泛化。<details>
<summary>Abstract</summary>
The problem of predicting the training time of machine learning (ML) models has become extremely relevant in the scientific community. Being able to predict a priori the training time of an ML model would enable the automatic selection of the best model both in terms of energy efficiency and in terms of performance in the context of, for instance, MLOps architectures. In this paper, we present the work we are conducting towards this direction. In particular, we present an extensive empirical study of the Full Parameter Time Complexity (FPTC) approach by Zheng et al., which is, to the best of our knowledge, the only approach formalizing the training time of ML models as a function of both dataset's and model's parameters. We study the formulations proposed for the Logistic Regression and Random Forest classifiers, and we highlight the main strengths and weaknesses of the approach. Finally, we observe how, from the conducted study, the prediction of training time is strictly related to the context (i.e., the involved dataset) and how the FPTC approach is not generalizable.
</details>
<details>
<summary>摘要</summary>
《机器学习模型训练时间预测问题已成为科学界热点问题。如果可以在先知道模型训练时间，那么可以自动选择最佳模型，以保证能够达到最佳性能和能效率。在这篇论文中，我们介绍了我们在这个方向下的工作。具体来说，我们对 Zheng et al. 等人提出的 Full Parameter Time Complexity（FPTC）方法进行了广泛的实证研究。这是我们所知道的唯一一种形式化机器学习模型训练时间为数据集和模型参数的函数。我们对 Logistic Regression 和 Random Forest 分类器的形ulation进行了研究，并将其主要优点和缺点进行了描述。最后，我们发现，从我们进行的研究来看，训练时间预测与数据集相关，而 FPTC 方法不能泛化。》Note: Please note that the translation is in Simplified Chinese, and the grammar and sentence structure may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="A-Model-Based-Machine-Learning-Approach-for-Assessing-the-Performance-of-Blockchain-Applications"><a href="#A-Model-Based-Machine-Learning-Approach-for-Assessing-the-Performance-of-Blockchain-Applications" class="headerlink" title="A Model-Based Machine Learning Approach for Assessing the Performance of Blockchain Applications"></a>A Model-Based Machine Learning Approach for Assessing the Performance of Blockchain Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11205">http://arxiv.org/abs/2309.11205</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AlbshriAdel/BlockchainPerformanceML">https://github.com/AlbshriAdel/BlockchainPerformanceML</a></li>
<li>paper_authors: Adel Albshri, Ali Alzubaidi, Ellis Solaiman</li>
<li>for: 本研究旨在提供一种可靠的模型方法，以便促进区块链应用程序的开发和评估。</li>
<li>methods: 本研究使用了两种机器学习模型基本方法：首先，我们使用 $k$  nearest neighbors（$k$NN）和支持向量机器（SVM）模型来预测区块链性能，使用预先确定的配置参数。其次，我们使用瑞特集群优化（SO）机器学习模型，并使用瑞特集群优化（ISO）来寻找最佳区块链配置，以达到所需性能水平。</li>
<li>results: 我们的模型比较统计结果表明，使用 $k$NN 模型可以比 SVM 模型提高性能，并且使用 ISO 可以减少不确定性 deviation 的偏差。<details>
<summary>Abstract</summary>
The recent advancement of Blockchain technology consolidates its status as a viable alternative for various domains. However, evaluating the performance of blockchain applications can be challenging due to the underlying infrastructure's complexity and distributed nature. Therefore, a reliable modelling approach is needed to boost Blockchain-based applications' development and evaluation. While simulation-based solutions have been researched, machine learning (ML) model-based techniques are rarely discussed in conjunction with evaluating blockchain application performance. Our novel research makes use of two ML model-based methods. Firstly, we train a $k$ nearest neighbour ($k$NN) and support vector machine (SVM) to predict blockchain performance using predetermined configuration parameters. Secondly, we employ the salp swarm optimization (SO) ML model which enables the investigation of optimal blockchain configurations for achieving the required performance level. We use rough set theory to enhance SO, hereafter called ISO, which we demonstrate to prove achieving an accurate recommendation of optimal parameter configurations; despite uncertainty. Finally, statistical comparisons indicate that our models have a competitive edge. The $k$NN model outperforms SVM by 5\% and the ISO also demonstrates a reduction of 4\% inaccuracy deviation compared to regular SO.
</details>
<details>
<summary>摘要</summary>
最近的区块链技术进步使其成为多种领域的可靠 altenative。然而，评估区块链应用程序性能可能会困难由于区块链基础设施的复杂性和分布式特点。因此，一种可靠的模型方法是需要为区块链应用程序的开发和评估提供 boost。而且，使用simulation-based解决方案已经被研究，但是使用机器学习（ML）模型基于技术 rarely discussed in conjunction with evaluating blockchain application performance。我们的新研究使用了两种ML模型基于方法。首先，我们使用 $k$ nearest neighbour ($k$NN) 和支持向量机（SVM）来预测区块链性能使用预先确定的配置参数。其次，我们使用salp swarm optimization（SO）ML模型，该模型允许我们调查到达所需性能水平的优化的区块链配置。我们使用粗设理论来增强SO，称为ISO，并证明ISO可以准确地提供优化参数配置，即使存在uncertainty。最后，统计比较表明，我们的模型具有竞争优势。$k$NN模型在比较SVM方法时表现出5%的提升，而ISO模型也表现出4%的减少不确定性偏移。
</details></li>
</ul>
<hr>
<h2 id="RHALE-Robust-and-Heterogeneity-aware-Accumulated-Local-Effects"><a href="#RHALE-Robust-and-Heterogeneity-aware-Accumulated-Local-Effects" class="headerlink" title="RHALE: Robust and Heterogeneity-aware Accumulated Local Effects"></a>RHALE: Robust and Heterogeneity-aware Accumulated Local Effects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11193">http://arxiv.org/abs/2309.11193</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/givasile/RHALE">https://github.com/givasile/RHALE</a></li>
<li>paper_authors: Vasilis Gkolemis, Theodore Dalamagas, Eirini Ntoutsi, Christos Diou</li>
<li>for: 本研究旨在提高Explainability方法的精度和可靠性，特别是在处理相关特征情况下。</li>
<li>methods: 本研究提出了一种Robust和Heterogeneity-aware ALE（RHALE）方法，该方法可以评估特征对输出的平均效果，同时考虑到实例级别的差异（heterogeneity）。</li>
<li>results: 对于 synthetic 和实际数据集，RHALE 方法比其他方法表现更优，特别是在相关特征情况下。 RHALE 方法还可以自动确定最佳分割方案，以兼顾 bias 和 variance。<details>
<summary>Abstract</summary>
Accumulated Local Effects (ALE) is a widely-used explainability method for isolating the average effect of a feature on the output, because it handles cases with correlated features well. However, it has two limitations. First, it does not quantify the deviation of instance-level (local) effects from the average (global) effect, known as heterogeneity. Second, for estimating the average effect, it partitions the feature domain into user-defined, fixed-sized bins, where different bin sizes may lead to inconsistent ALE estimations. To address these limitations, we propose Robust and Heterogeneity-aware ALE (RHALE). RHALE quantifies the heterogeneity by considering the standard deviation of the local effects and automatically determines an optimal variable-size bin-splitting. In this paper, we prove that to achieve an unbiased approximation of the standard deviation of local effects within each bin, bin splitting must follow a set of sufficient conditions. Based on these conditions, we propose an algorithm that automatically determines the optimal partitioning, balancing the estimation bias and variance. Through evaluations on synthetic and real datasets, we demonstrate the superiority of RHALE compared to other methods, including the advantages of automatic bin splitting, especially in cases with correlated features.
</details>
<details>
<summary>摘要</summary>
集成本地效应（ALE）是一种广泛使用的解释方法，用于隔离输出的平均效应，因为它能够处理相关的特征 случа子 well。然而，它有两个限制。首先，它不计算特定实例（本地）效应与平均（全局）效应之间的偏差。其次，为计算平均效应，它将特征领域分成用户定义、固定大小的分割，不同的分割大小可能会导致不一致的 ALE 估计。为解决这些限制，我们提出了 Robust and Heterogeneity-aware ALE（RHALE）。RHALE 考虑了本地效应的标准差，以及自动确定最佳变量大小分割。在这篇论文中，我们证明了，为在每个分割中精确估计本地效应的标准差，分割必须遵循一组必要条件。基于这些条件，我们提出了一种算法，可以自动确定最佳分割，协调估计偏差和方差。通过对 synthetic 和实际数据进行评估，我们示出了 RHALE 与其他方法相比，具有较好的优势，特别是在相关特征情况下。
</details></li>
</ul>
<hr>
<h2 id="Investigating-Personalization-Methods-in-Text-to-Music-Generation"><a href="#Investigating-Personalization-Methods-in-Text-to-Music-Generation" class="headerlink" title="Investigating Personalization Methods in Text to Music Generation"></a>Investigating Personalization Methods in Text to Music Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11140">http://arxiv.org/abs/2309.11140</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zelaki/DreamSound">https://github.com/zelaki/DreamSound</a></li>
<li>paper_authors: Manos Plitsis, Theodoros Kouzelis, Georgios Paraskevopoulos, Vassilis Katsouros, Yannis Panagakis</li>
<li>for: 这个研究探讨了在几个shot设定下个性化文本到音乐扩散模型的问题。</li>
<li>methods: 研究使用了已有的个性化方法的组合，以及音频专门的数据增强技术。</li>
<li>results: 研究发现，相似度指标与用户喜好相吻合，现有的个性化方法更容易学习rhythmic音乐构造而不是melody。Please note that the above text is in Simplified Chinese.<details>
<summary>Abstract</summary>
In this work, we investigate the personalization of text-to-music diffusion models in a few-shot setting. Motivated by recent advances in the computer vision domain, we are the first to explore the combination of pre-trained text-to-audio diffusers with two established personalization methods. We experiment with the effect of audio-specific data augmentation on the overall system performance and assess different training strategies. For evaluation, we construct a novel dataset with prompts and music clips. We consider both embedding-based and music-specific metrics for quantitative evaluation, as well as a user study for qualitative evaluation. Our analysis shows that similarity metrics are in accordance with user preferences and that current personalization approaches tend to learn rhythmic music constructs more easily than melody. The code, dataset, and example material of this study are open to the research community.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们调查了文本到音乐填充模型在几个尝试设置下的个性化。受最近计算机视觉领域的进步 inspirits，我们是首次探讨将预训练文本到音频填充器与两种已有个性化方法结合使用。我们对系统性能的影响进行了音频特定数据增强的实验，并评估了不同的训练策略。为评价，我们建立了一个新的提示和音乐片断集合。我们使用了两种嵌入空间和音乐特有的评价指标进行量化评估，以及一项用户研究 для质量评估。我们的分析表明，相似度指标与用户喜好相符，现有的个性化方法更容易学习音乐的节奏结构而不是旋律。我们的代码、数据集和研究材料对研究社区开放。
</details></li>
</ul>
<hr>
<h2 id="Ano-SuPs-Multi-size-anomaly-detection-for-manufactured-products-by-identifying-suspected-patches"><a href="#Ano-SuPs-Multi-size-anomaly-detection-for-manufactured-products-by-identifying-suspected-patches" class="headerlink" title="Ano-SuPs: Multi-size anomaly detection for manufactured products by identifying suspected patches"></a>Ano-SuPs: Multi-size anomaly detection for manufactured products by identifying suspected patches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11120">http://arxiv.org/abs/2309.11120</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Xu, Juan Du, Andi Wang</li>
<li>for: This paper aims to address the challenges of existing matrix decomposition methods in image-based anomaly detection, particularly in the presence of complex backgrounds and various anomaly patterns.</li>
<li>methods: The proposed method uses a two-stage strategy that involves detecting suspected patches (Ano-SuPs) by reconstructing the input image twice: the first step is to obtain a set of normal patches by removing suspected patches, and the second step is to use those normal patches to refine the identification of patches with anomalies.</li>
<li>results: The proposed method is evaluated systematically through simulation experiments and case studies, demonstrating its effectiveness in detecting anomalies in image-based systems. The key parameters and designed steps that impact the model’s performance and efficiency are also identified.<details>
<summary>Abstract</summary>
Image-based systems have gained popularity owing to their capacity to provide rich manufacturing status information, low implementation costs and high acquisition rates. However, the complexity of the image background and various anomaly patterns pose new challenges to existing matrix decomposition methods, which are inadequate for modeling requirements. Moreover, the uncertainty of the anomaly can cause anomaly contamination problems, making the designed model and method highly susceptible to external disturbances. To address these challenges, we propose a two-stage strategy anomaly detection method that detects anomalies by identifying suspected patches (Ano-SuPs). Specifically, we propose to detect the patches with anomalies by reconstructing the input image twice: the first step is to obtain a set of normal patches by removing those suspected patches, and the second step is to use those normal patches to refine the identification of the patches with anomalies. To demonstrate its effectiveness, we evaluate the proposed method systematically through simulation experiments and case studies. We further identified the key parameters and designed steps that impact the model's performance and efficiency.
</details>
<details>
<summary>摘要</summary>
图像基于系统在生产环境中得到普及，这主要归功于它们能够提供丰富的生产状况信息，以及实现成本和获得率的低。然而，图像背景的复杂性和各种异常模式带来了对现有矩阵分解方法的新挑战。此外，异常现象的不确定性会导致异常污染问题，使得设计的模型和方法容易受到外部干扰。为解决这些挑战，我们提出了一种两Stage策略异常检测方法，通过检测异常的补丁（Ano-SuPs）来检测异常。具体来说，我们首先从输入图像中提取出一组正常补丁，然后使用这些正常补丁来精细地定位异常补丁。为证明其效果，我们系统地通过实验和案例研究评估了提案的方法。此外，我们还标识出了影响模型性能和效率的关键参数和设计步骤。
</details></li>
</ul>
<hr>
<h2 id="Bold-but-Cautious-Unlocking-the-Potential-of-Personalized-Federated-Learning-through-Cautiously-Aggressive-Collaboration"><a href="#Bold-but-Cautious-Unlocking-the-Potential-of-Personalized-Federated-Learning-through-Cautiously-Aggressive-Collaboration" class="headerlink" title="Bold but Cautious: Unlocking the Potential of Personalized Federated Learning through Cautiously Aggressive Collaboration"></a>Bold but Cautious: Unlocking the Potential of Personalized Federated Learning through Cautiously Aggressive Collaboration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11103">http://arxiv.org/abs/2309.11103</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kxzxvbk/Fling">https://github.com/kxzxvbk/Fling</a></li>
<li>paper_authors: Xinghao Wu, Xuefeng Liu, Jianwei Niu, Guogang Zhu, Shaojie Tang</li>
<li>for: 这篇论文主要关注在对多个客户进行协同学习时，减少非独立同分布（non-IID）资料的影响，并且将客户训练的个人化模型与其他客户进行协同学习。</li>
<li>methods: 这篇论文提出了一个新的协同学习指南，与现有的方法不同的是，它允许客户将更多的参数与其他客户共享，从而提高模型的性能。这篇论文还提出了一个名为FedCAC的新协同学习方法，它使用一个量值指数来评估各参数的非独立同分布敏感度，并将客户选择为协同学习者基于这个评估结果。</li>
<li>results: 实验结果显示，FedCAC比现有的方法更好地将客户的参数与其他客户共享，从而提高模型的性能，特别是在客户的资料分布不同时。<details>
<summary>Abstract</summary>
Personalized federated learning (PFL) reduces the impact of non-independent and identically distributed (non-IID) data among clients by allowing each client to train a personalized model when collaborating with others. A key question in PFL is to decide which parameters of a client should be localized or shared with others. In current mainstream approaches, all layers that are sensitive to non-IID data (such as classifier layers) are generally personalized. The reasoning behind this approach is understandable, as localizing parameters that are easily influenced by non-IID data can prevent the potential negative effect of collaboration. However, we believe that this approach is too conservative for collaboration. For example, for a certain client, even if its parameters are easily influenced by non-IID data, it can still benefit by sharing these parameters with clients having similar data distribution. This observation emphasizes the importance of considering not only the sensitivity to non-IID data but also the similarity of data distribution when determining which parameters should be localized in PFL. This paper introduces a novel guideline for client collaboration in PFL. Unlike existing approaches that prohibit all collaboration of sensitive parameters, our guideline allows clients to share more parameters with others, leading to improved model performance. Additionally, we propose a new PFL method named FedCAC, which employs a quantitative metric to evaluate each parameter's sensitivity to non-IID data and carefully selects collaborators based on this evaluation. Experimental results demonstrate that FedCAC enables clients to share more parameters with others, resulting in superior performance compared to state-of-the-art methods, particularly in scenarios where clients have diverse distributions.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:personalized federated learning (PFL) 减少客户端之间非独立和同分布数据的影响，通过让每个客户端训练个性化模型并与其他客户端合作。PFL中的关键问题是决定每个客户端的参数是否要本地化或与其他客户端共享。现今主流的方法是将所有敏感于非独立和同分布数据的层（例如分类层）都本地化。这种方法的原因是可以避免因合作而导致的可能性。然而，我们认为这种方法是对合作的过度保守。例如，对于某个客户端，即使其参数容易受到非独立和同分布数据的影响，但是它仍可以通过与其他客户端的数据分布相似性来共享参数，从而获得更好的性能。这一观察强调了在PFL中考虑参数的敏感度以及数据分布的相似性是非常重要的。本文提出了一种新的PFL客户端协作指南，与现今主流的方法不同之处在于，它允许客户端更多地共享参数，从而提高模型性能。此外，我们还提出了一种名为FedCAC的新的PFL方法，它使用一种量化的度量来评估每个参数的非独立和同分布数据的敏感度，并且根据这种评估来精心选择合作者。实验结果表明，FedCAC可以减少客户端之间的数据分布差异，从而实现与当前最佳方法相比的更好的性能。
</details></li>
</ul>
<hr>
<h2 id="Delays-in-Reinforcement-Learning"><a href="#Delays-in-Reinforcement-Learning" class="headerlink" title="Delays in Reinforcement Learning"></a>Delays in Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11096">http://arxiv.org/abs/2309.11096</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/reiniscimurs/DRL-robot-navigation">https://github.com/reiniscimurs/DRL-robot-navigation</a></li>
<li>paper_authors: Pierre Liotet</li>
<li>for: 这个论文主要研究了延迟在动态系统中的影响，以及如何在延迟的情况下进行决策。</li>
<li>methods: 该论文使用了马可夫决策过程（MDP）作为基础框架，并研究了延迟在这种决策过程中的影响。</li>
<li>results: 该论文发现了延迟对动态系统的影响，并提出了一些可能的解决方案。同时，论文还Draws links between celebrated frameworks of reinforcement learning literature and the one of delays.<details>
<summary>Abstract</summary>
Delays are inherent to most dynamical systems. Besides shifting the process in time, they can significantly affect their performance. For this reason, it is usually valuable to study the delay and account for it. Because they are dynamical systems, it is of no surprise that sequential decision-making problems such as Markov decision processes (MDP) can also be affected by delays. These processes are the foundational framework of reinforcement learning (RL), a paradigm whose goal is to create artificial agents capable of learning to maximise their utility by interacting with their environment.   RL has achieved strong, sometimes astonishing, empirical results, but delays are seldom explicitly accounted for. The understanding of the impact of delay on the MDP is limited. In this dissertation, we propose to study the delay in the agent's observation of the state of the environment or in the execution of the agent's actions. We will repeatedly change our point of view on the problem to reveal some of its structure and peculiarities. A wide spectrum of delays will be considered, and potential solutions will be presented. This dissertation also aims to draw links between celebrated frameworks of the RL literature and the one of delays.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Delays are inherent to most dynamical systems. Besides shifting the process in time, they can significantly affect their performance. For this reason, it is usually valuable to study the delay and account for it. Because they are dynamical systems, it is of no surprise that sequential decision-making problems such as Markov decision processes (MDP) can also be affected by delays. These processes are the foundational framework of reinforcement learning (RL), a paradigm whose goal is to create artificial agents capable of learning to maximize their utility by interacting with their environment.   RL has achieved strong, sometimes astonishing, empirical results, but delays are seldom explicitly accounted for. The understanding of the impact of delay on the MDP is limited. In this dissertation, we propose to study the delay in the agent's observation of the state of the environment or in the execution of the agent's actions. We will repeatedly change our point of view on the problem to reveal some of its structure and peculiarities. A wide spectrum of delays will be considered, and potential solutions will be presented. This dissertation also aims to draw links between celebrated frameworks of the RL literature and the one of delays." into Simplified Chinese.Here's the translation:<<SYS>>多种动力系统中都存在延迟。 besides 延迟时间的偏移，它们可以对性能产生重要影响。因此，通常值得研究延迟并考虑其影响。因为它们是动力系统，因此也不surprisingly，sequential decision-making problemssuch as Markov decision processes (MDP) 也可以受到延迟的影响。这些过程是RL的基础框架，RL的目标是创建可以在环境中学习提高利用的人工智能代理。   RL已经取得了强大，occasionally astonishing的实验成果，但延迟通常不直接考虑。MDP中延迟的理解受限。在这个论文中，我们提议研究代理 Observation of the state of the environment 或执行代理动作中的延迟。我们将不断更改问题的视点，以揭示其结构和特点。广泛考虑延迟的范围，并提供可能的解决方案。这个论文还计划把RL文献中著名的框架与延迟框架相连接。
</details></li>
</ul>
<hr>
<h2 id="GPSINDy-Data-Driven-Discovery-of-Equations-of-Motion"><a href="#GPSINDy-Data-Driven-Discovery-of-Equations-of-Motion" class="headerlink" title="GPSINDy: Data-Driven Discovery of Equations of Motion"></a>GPSINDy: Data-Driven Discovery of Equations of Motion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11076">http://arxiv.org/abs/2309.11076</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junette Hsin, Shubhankar Agarwal, Adam Thorpe, David Fridovich-Keil</li>
<li>for: 本研究旨在寻找含有噪声数据的非线性动力系统模型。</li>
<li>methods: 我们将 Gaussian 过程回归、SINDy 参数学习方法结合起来，以便从数据中找到非线性动力系统模型。</li>
<li>results: 我们在一个 Lotka-Volterra 模型和一个 unicycle 动力系统上进行了实验和硬件数据处理，并证明了我们的方法可以更好地找到系统动力和预测未来轨迹。<details>
<summary>Abstract</summary>
In this paper, we consider the problem of discovering dynamical system models from noisy data. The presence of noise is known to be a significant problem for symbolic regression algorithms. We combine Gaussian process regression, a nonparametric learning method, with SINDy, a parametric learning approach, to identify nonlinear dynamical systems from data. The key advantages of our proposed approach are its simplicity coupled with the fact that it demonstrates improved robustness properties with noisy data over SINDy. We demonstrate our proposed approach on a Lotka-Volterra model and a unicycle dynamic model in simulation and on an NVIDIA JetRacer system using hardware data. We demonstrate improved performance over SINDy for discovering the system dynamics and predicting future trajectories.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们考虑了从含噪数据中找到动力系统模型的问题。噪声知道会对符号回归算法产生很大的影响。我们将 Gaussian process regression 和 SINDy 结合起来，以非参数方式学习方法来识别非线性动力系统模型。我们的提议的方法的优点是简单易用，同时具有较好的鲁棒性特性，在含噪数据上表现 mejor than SINDy。我们在 Lotka-Volterra 模型和 unicycle 动态模型上进行了在 simulate 和 NVIDIA JetRacer 系统上使用硬件数据进行了实验，并证明了我们的方法可以更好地找到系统动力和预测未来轨迹。
</details></li>
</ul>
<hr>
<h2 id="InkStream-Real-time-GNN-Inference-on-Streaming-Graphs-via-Incremental-Update"><a href="#InkStream-Real-time-GNN-Inference-on-Streaming-Graphs-via-Incremental-Update" class="headerlink" title="InkStream: Real-time GNN Inference on Streaming Graphs via Incremental Update"></a>InkStream: Real-time GNN Inference on Streaming Graphs via Incremental Update</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11071">http://arxiv.org/abs/2309.11071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dan Wu, Zhaoying Li, Tulika Mitra</li>
<li>for: 本文旨在提出一种基于图 neural network (GNN) 的实时推理方法，以适应流动图的更新。</li>
<li>methods: 本方法基于两点关键见解：（1）在 $k$-hop 邻域内，大多数节点不受到修改边的影响，当使用汇聚函数时；（2）当模型权重保持不变，而图结构发生变化， THENode 嵌入可以逐渐发展于时间。基于这两点见解，我们提出了一种名为 InkStream 的新方法，用于实时推理，具有最小的内存访问和计算量，同时保证输出与传统方法相同。InkStream 基于事件驱动系统，控制了间层效应传播和内层增量更新节点嵌入。InkStream 高度可配置和扩展，allowing users to create and process customized events。</li>
<li>results: 我们在四个大图上使用三种 GNN 模型进行实验，显示 InkStream 在 CPU 集群上加速了 2.5-427 倍，在两个不同的 GPU 集群上加速了 2.4-343 倍，而且输出与传统方法的最新图快照相同。<details>
<summary>Abstract</summary>
Classic Graph Neural Network (GNN) inference approaches, designed for static graphs, are ill-suited for streaming graphs that evolve with time. The dynamism intrinsic to streaming graphs necessitates constant updates, posing unique challenges to acceleration on GPU. We address these challenges based on two key insights: (1) Inside the $k$-hop neighborhood, a significant fraction of the nodes is not impacted by the modified edges when the model uses min or max as aggregation function; (2) When the model weights remain static while the graph structure changes, node embeddings can incrementally evolve over time by computing only the impacted part of the neighborhood. With these insights, we propose a novel method, InkStream, designed for real-time inference with minimal memory access and computation, while ensuring an identical output to conventional methods. InkStream operates on the principle of propagating and fetching data only when necessary. It uses an event-based system to control inter-layer effect propagation and intra-layer incremental updates of node embedding. InkStream is highly extensible and easily configurable by allowing users to create and process customized events. We showcase that less than 10 lines of additional user code are needed to support popular GNN models such as GCN, GraphSAGE, and GIN. Our experiments with three GNN models on four large graphs demonstrate that InkStream accelerates by 2.5-427$\times$ on a CPU cluster and 2.4-343$\times$ on two different GPU clusters while producing identical outputs as GNN model inference on the latest graph snapshot.
</details>
<details>
<summary>摘要</summary>
传统的图 neural network (GNN) 推理方法，设计 для静止图，对流动图来说是不适用的。流动图的动态特性需要不断更新，这会带来特殊的加速挑战在 GPU 上。我们根据以下两个关键发现：（1）在 $k$-hop 邻域内，大量节点不会受到改变的边对 GNN 模型进行汇聚时的影响；（2）当模型权重保持不变而图结构发生变化时，节点嵌入可以逐渐发展在时间上，只需计算影响的部分邻域。基于这些发现，我们提出了一种新的方法，称为 InkStream，用于实时推理，具有最小的内存访问和计算量，同时保证输出和普通方法相同。InkStream 运行在事件驱动的系统上，控制间层效应传播和INTRA层增量更新节点嵌入。InkStream 高度可 configurable，可以让用户创建和处理自定义事件。我们的实验表明，使用 InkStream 可以在 CPU 集群上加速 2.5-427 倍，在两个不同的 GPU 集群上加速 2.4-343 倍，而且输出和普通方法相同。
</details></li>
</ul>
<hr>
<h2 id="Extreme-Scenario-Selection-in-Day-Ahead-Power-Grid-Operational-Planning"><a href="#Extreme-Scenario-Selection-in-Day-Ahead-Power-Grid-Operational-Planning" class="headerlink" title="Extreme Scenario Selection in Day-Ahead Power Grid Operational Planning"></a>Extreme Scenario Selection in Day-Ahead Power Grid Operational Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11067">http://arxiv.org/abs/2309.11067</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillermo Terrén-Serrano, Michael Ludkovski</li>
<li>for: 本研究旨在为短期电网规划选择极端情况，以降低运营风险。</li>
<li>methods: 本研究使用统计函数深度指标来筛选极端情况，以确定最有可能导致网络运营风险的情况。</li>
<li>results: 实验结果表明，使用统计函数深度指标可以有效地筛选出高风险情况，并且可以预测load shedding、运营成本、储备短缺和可变能源电停机等操作风险。<details>
<summary>Abstract</summary>
We propose and analyze the application of statistical functional depth metrics for the selection of extreme scenarios in day-ahead grid planning. Our primary motivation is screening of probabilistic scenarios for realized load and renewable generation, in order to identify scenarios most relevant for operational risk mitigation. To handle the high-dimensionality of the scenarios across asset classes and intra-day periods, we employ functional measures of depth to sub-select outlying scenarios that are most likely to be the riskiest for the grid operation. We investigate a range of functional depth measures, as well as a range of operational risks, including load shedding, operational costs, reserves shortfall and variable renewable energy curtailment. The effectiveness of the proposed screening approach is demonstrated through a case study on the realistic Texas-7k grid.
</details>
<details>
<summary>摘要</summary>
我们提出和分析使用统计函数深度指标来选择EXTREME场景在一天前电网规划中。我们的 PRIMARY motivation是对 probabilistic scenario 进行屏选，以便 identific scenarios 对电网操作风险最大化。为了处理不同资产类和时间段之间的高维度场景，我们使用函数指标来子选择异常场景，以便更好地了解电网操作风险。我们调查了一系列函数深度指标，以及一系列操作风险，包括荷 shedding、操作成本、储备短缺和可变可再生能源削减。我们的案例研究基于真实的 Texas-7k 电网。Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Containing-Analog-Data-Deluge-at-Edge-through-Frequency-Domain-Compression-in-Collaborative-Compute-in-Memory-Networks"><a href="#Containing-Analog-Data-Deluge-at-Edge-through-Frequency-Domain-Compression-in-Collaborative-Compute-in-Memory-Networks" class="headerlink" title="Containing Analog Data Deluge at Edge through Frequency-Domain Compression in Collaborative Compute-in-Memory Networks"></a>Containing Analog Data Deluge at Edge through Frequency-Domain Compression in Collaborative Compute-in-Memory Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11048">http://arxiv.org/abs/2309.11048</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nastaran Darabi, Amit R. Trivedi</li>
<li>for: This paper aims to improve area efficiency in deep learning inference tasks for edge computing applications, specifically addressing the challenges of limited storage and computing resources in edge devices.</li>
<li>methods: The proposed method employs two key strategies: (1) Frequency domain learning using binarized Walsh-Hadamard Transforms, which reduces the necessary parameters for DNN and enables compute-in-SRAM, and (2) a memory-immersed collaborative digitization method among CiM arrays to reduce the area overheads of conventional ADCs.</li>
<li>results: The proposed method achieves significant area and energy savings compared to a 40 nm-node 5-bit SAR ADC and 5-bit Flash ADC, as demonstrated using a 65 nm CMOS test chip. The results show that it is possible to process analog data more efficiently and selectively retain valuable data from sensors, alleviating the challenges posed by the analog data deluge.Here’s the Chinese version of the three key information points:</li>
<li>for: 这篇论文旨在提高边缘计算应用中深度学习推理任务的面积效率，具体是解决边缘设备的存储和计算资源受限问题。</li>
<li>methods: 提议的方法采用了两种关键策略：（1）频域学习使用二进制沃尔什-哈达姆变换，减少深度学习模型中的参数数量（MobileNetV2中减少87%），并且使用计算在SRAM中进行计算，更好地利用并行性；（2）使用Memory-immersed collaborative digitization方法，将 CiM 数组与存储器集成，以降低传统ADC的面积开销。</li>
<li>results: 根据65nmCMOS测试板表现，提议的方法可以实现显著的面积和能耗减少，与40nm节点5位SAR ADC和5位Flash ADC相比。通过更有效地处理分析数据，可以选择性地保留感知器中的有价值数据，从而解决分析数据泛洪的问题。<details>
<summary>Abstract</summary>
Edge computing is a promising solution for handling high-dimensional, multispectral analog data from sensors and IoT devices for applications such as autonomous drones. However, edge devices' limited storage and computing resources make it challenging to perform complex predictive modeling at the edge. Compute-in-memory (CiM) has emerged as a principal paradigm to minimize energy for deep learning-based inference at the edge. Nevertheless, integrating storage and processing complicates memory cells and/or memory peripherals, essentially trading off area efficiency for energy efficiency. This paper proposes a novel solution to improve area efficiency in deep learning inference tasks. The proposed method employs two key strategies. Firstly, a Frequency domain learning approach uses binarized Walsh-Hadamard Transforms, reducing the necessary parameters for DNN (by 87% in MobileNetV2) and enabling compute-in-SRAM, which better utilizes parallelism during inference. Secondly, a memory-immersed collaborative digitization method is described among CiM arrays to reduce the area overheads of conventional ADCs. This facilitates more CiM arrays in limited footprint designs, leading to better parallelism and reduced external memory accesses. Different networking configurations are explored, where Flash, SA, and their hybrid digitization steps can be implemented using the memory-immersed scheme. The results are demonstrated using a 65 nm CMOS test chip, exhibiting significant area and energy savings compared to a 40 nm-node 5-bit SAR ADC and 5-bit Flash ADC. By processing analog data more efficiently, it is possible to selectively retain valuable data from sensors and alleviate the challenges posed by the analog data deluge.
</details>
<details>
<summary>摘要</summary>
“边缘计算是一种具有应用前景的解决方案，用于处理具有高维度和多 спектル的数据流畅的感应器和IoT设备。然而，边缘设备的储存和处理资源有限，导致复杂的预测模型在边缘进行实际问题。compute-in-memory（CiM）技术已经成为一种主要的解决方案，以降低运算的能源消耗。然而，将储存和处理复杂化的内存细节和/或内存周边设备，实际上是将面积效率与能源效率进行交换。本文提出了一个新的解决方案，以改善边缘运算中的面积效率。本方法使用了两个关键策略：首先，使用频率域学习方法，通过将数据压缩为二进制数据，并使用对称的华氏-哈达玛特转换，以降低运算所需的参数数量（MobileNetV2中降低87%），并允许在执行运算时使用SRAM进行计算。其次，描述了一种内存嵌入式合作数字化方法，用于实现 CiM 阵列中的内存与ADC之间的联系。这种方法可以在有限的面积设计中支持更多的 CiM 阵列，实现更好的并行性和对外存储器的减少。不同的网络配置被探讨，包括 Flash、SA 和它们的混合式数字化步骤。结果显示，使用本方法可以在65奈米CMOS试验板上展示出具有明显的面积和能源优化的功能。通过更有效地处理数据，可以对感应器中的有用数据进行选择性储存，从而缓解感应器中的数据潮汐问题。”
</details></li>
</ul>
<hr>
<h2 id="A-Region-Shrinking-Based-Acceleration-for-Classification-Based-Derivative-Free-Optimization"><a href="#A-Region-Shrinking-Based-Acceleration-for-Classification-Based-Derivative-Free-Optimization" class="headerlink" title="A Region-Shrinking-Based Acceleration for Classification-Based Derivative-Free Optimization"></a>A Region-Shrinking-Based Acceleration for Classification-Based Derivative-Free Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11036">http://arxiv.org/abs/2309.11036</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyi Han, Jingya Li, Zhipeng Guo, Yuan Jin</li>
<li>for: 这篇论文主要关注于科学和工程设计优化问题中的梯度不可知分布式优化算法的框架。</li>
<li>methods: 本文提出了一种新的分类基于 derivative-free 优化算法，并引入了一个叫做假设目标隔离率的概念，以更新这类算法的计算复杂性Upper bound。</li>
<li>results: 根据实验结果，新提出的 “RACE-CARS” 算法比 traditional “SRACOS” 更快，并且对黑盒优化和自然语言处理中的语言模型服务进行了实证验证。此外，文章还进行了一个ablation experiment，探讨了 “RACE-CARS” 的机制和参数优化的指导。<details>
<summary>Abstract</summary>
Derivative-free optimization algorithms play an important role in scientific and engineering design optimization problems, especially when derivative information is not accessible. In this paper, we study the framework of classification-based derivative-free optimization algorithms. By introducing a concept called hypothesis-target shattering rate, we revisit the computational complexity upper bound of this type of algorithms. Inspired by the revisited upper bound, we propose an algorithm named "RACE-CARS", which adds a random region-shrinking step compared with "SRACOS" (Hu et al., 2017).. We further establish a theorem showing the acceleration of region-shrinking. Experiments on the synthetic functions as well as black-box tuning for language-model-as-a-service demonstrate empirically the efficiency of "RACE-CARS". An ablation experiment on the introduced hyperparameters is also conducted, revealing the mechanism of "RACE-CARS" and putting forward an empirical hyperparameter-tuning guidance.
</details>
<details>
<summary>摘要</summary>
derivative-free 优化算法在科学和工程设计优化问题中扮演着重要的角色，尤其是当 derivate 信息不可获取时。本文研究了类别基于的 derivative-free 优化算法框架。通过引入假设目标震荡率，我们重新评估了这类算法的计算复杂性Upper bound。 inspirited 由 revisited Upper bound，我们提出了名为 "RACE-CARS" 的算法，它在 "SRACOS" （Hu et al., 2017）中添加了随机区域缩小步骤。我们还证明了区域缩小的加速。对于 synthetic 函数以及黑盒调参语言模型服务，我们进行了实验，并证明了 "RACE-CARS" 的效率。另外，我们还进行了一个ablation experiment 对引入的超参数，探讨了 "RACE-CARS" 的机制，并提出了一个empirical 超参数调整指南。
</details></li>
</ul>
<hr>
<h2 id="The-Topology-and-Geometry-of-Neural-Representations"><a href="#The-Topology-and-Geometry-of-Neural-Representations" class="headerlink" title="The Topology and Geometry of Neural Representations"></a>The Topology and Geometry of Neural Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11028">http://arxiv.org/abs/2309.11028</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neurreps/awesome-neural-geometry">https://github.com/neurreps/awesome-neural-geometry</a></li>
<li>paper_authors: Baihan Lin, Nikolaus Kriegeskorte</li>
<li>for: 这项研究的目的是Characterize brain representations of perceptual and cognitive content, and distinguish different functional regions with robustness to noise and individual differences.</li>
<li>methods: 研究使用了 topological representational similarity analysis (tRSA), an extension of representational similarity analysis (RSA) that uses a family of geo-topological summary statistics to characterize the topology of brain representations while de-emphasizing the geometry.</li>
<li>results: 研究发现，使用这种新的统计方法可以robust to noise and interindividual variability, and maintain excellent sensitivity to the unique representational signatures of different neural network layers and brain regions.<details>
<summary>Abstract</summary>
A central question for neuroscience is how to characterize brain representations of perceptual and cognitive content. An ideal characterization should distinguish different functional regions with robustness to noise and idiosyncrasies of individual brains that do not correspond to computational differences. Previous studies have characterized brain representations by their representational geometry, which is defined by the representational dissimilarity matrix (RDM), a summary statistic that abstracts from the roles of individual neurons (or responses channels) and characterizes the discriminability of stimuli. Here we explore a further step of abstraction: from the geometry to the topology of brain representations. We propose topological representational similarity analysis (tRSA), an extension of representational similarity analysis (RSA) that uses a family of geo-topological summary statistics that generalizes the RDM to characterize the topology while de-emphasizing the geometry. We evaluate this new family of statistics in terms of the sensitivity and specificity for model selection using both simulations and functional MRI (fMRI) data. In the simulations, the ground truth is a data-generating layer representation in a neural network model and the models are the same and other layers in different model instances (trained from different random seeds). In fMRI, the ground truth is a visual area and the models are the same and other areas measured in different subjects. Results show that topology-sensitive characterizations of population codes are robust to noise and interindividual variability and maintain excellent sensitivity to the unique representational signatures of different neural network layers and brain regions.
</details>
<details>
<summary>摘要</summary>
中文翻译： neuroscience 中的一个中心问题是如何 caracterize 大脑表征的感知和认知内容。理想的 caracterization 应该能够分辨不同的功能区域，并具有对噪声和个体大脑差异的抗颤性。前一些研究已经使用 representational geometry 来 caracterize 大脑表征，其定义为各个 neuron 或 response channel 的表征差异矩阵 (RDM)，这是一个摘要统计量，抑制了个体大脑差异的计算不同。在这篇文章中，我们 explore 一个进一步的抽象步骤：从 geometry 到大脑表征的 topology。我们提出 topological representational similarity analysis (tRSA)，这是 representational similarity analysis (RSA) 的扩展，使用一个基于地理 topological 摘要统计量，这个统计量抑制了 geometry 的影响，专注于表征的 topology。我们使用 simulate 和 functional MRI (fMRI) 数据来评估这种新的家族统计量的敏感性和特点。在 simulate 中，ground truth 是一个数据生成层表示，模型是不同的 random seed 生成的不同层模型实例。在 fMRI 中，ground truth 是一个视觉区域，模型是不同的视觉区域和不同的主体 measured 的不同主体。结果显示，基于 topology 的人类代表码 caracterization 是噪声和个体差异的抗颤性，并保持了对不同 neural network 层和大脑区域的唯一表征签名的敏感性。
</details></li>
</ul>
<hr>
<h2 id="Information-Leakage-from-Data-Updates-in-Machine-Learning-Models"><a href="#Information-Leakage-from-Data-Updates-in-Machine-Learning-Models" class="headerlink" title="Information Leakage from Data Updates in Machine Learning Models"></a>Information Leakage from Data Updates in Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11022">http://arxiv.org/abs/2309.11022</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Tian Hui, Farhad Farokhi, Olga Ohrimenko</li>
<li>for: 本研究考虑在机器学习模型 retrained 在更新数据集上以 incorporate 最新信息或反映分布变化。</li>
<li>methods: 我们提出了基于模型Prediction confidence差异的攻击方法，并对两个公共数据集以及多层感知器和Logistic regression模型进行评估。</li>
<li>results: 我们发现，使用两个模型Snapshot可以导致更高的信息泄露，而且数据记录 WITH rare attribute value 更容易受到攻击。 repeated changes 可能会带来更大的泄露。<details>
<summary>Abstract</summary>
In this paper we consider the setting where machine learning models are retrained on updated datasets in order to incorporate the most up-to-date information or reflect distribution shifts. We investigate whether one can infer information about these updates in the training data (e.g., changes to attribute values of records). Here, the adversary has access to snapshots of the machine learning model before and after the change in the dataset occurs. Contrary to the existing literature, we assume that an attribute of a single or multiple training data points are changed rather than entire data records are removed or added. We propose attacks based on the difference in the prediction confidence of the original model and the updated model. We evaluate our attack methods on two public datasets along with multi-layer perceptron and logistic regression models. We validate that two snapshots of the model can result in higher information leakage in comparison to having access to only the updated model. Moreover, we observe that data records with rare values are more vulnerable to attacks, which points to the disparate vulnerability of privacy attacks in the update setting. When multiple records with the same original attribute value are updated to the same new value (i.e., repeated changes), the attacker is more likely to correctly guess the updated values since repeated changes leave a larger footprint on the trained model. These observations point to vulnerability of machine learning models to attribute inference attacks in the update setting.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们考虑了机器学习模型在更新数据集后重新训练以包含最新的信息或反映分布变化。我们研究了是否可以从训练数据中推断更新信息（例如，记录属性值的更改）。在这个设定下，敌方可以访问机器学习模型的两个快照，即之前和之后更改数据集发生。不同于现有文献，我们假设单个或多个训练数据点的属性发生变化而不是整个数据记录被删除或添加。我们提出了基于原始模型和更新模型预测信任度差异的攻击方法。我们在两个公共数据集以及多层感知和折衔函数模型上进行了评估。我们发现了两个快照的模型可以导致更高的信息泄露，而且数据记录中的罕见值更容易受到攻击，这指出了机器学习模型在更新设定下的敏感度问题。当多个记录中的原始属性值都更新为同一个新值时（即重复更改），攻击者更可能正确地猜测更新值，因为重复更改会留下更大的模型训练中的印记。这些观察表明了机器学习模型在更新设定下面临的属性推断攻击。
</details></li>
</ul>
<hr>
<h2 id="3D-U-SAM-Network-For-Few-shot-Tooth-Segmentation-in-CBCT-Images"><a href="#3D-U-SAM-Network-For-Few-shot-Tooth-Segmentation-in-CBCT-Images" class="headerlink" title="3D-U-SAM Network For Few-shot Tooth Segmentation in CBCT Images"></a>3D-U-SAM Network For Few-shot Tooth Segmentation in CBCT Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11015">http://arxiv.org/abs/2309.11015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifu Zhang, Zuozhu Liu, Yang Feng, Renjing Xu</li>
<li>for: 解决3D dental图像分割任务中的样本数量太少问题，提出一种基于SAM预训练网络的3D-U-SAM网络。</li>
<li>methods: 采用了一种核心抽象方法，并在U-Net网络中设计了跳跃连接，以保留更多的细节信息。</li>
<li>results: 通过比较和采样大小实验表明，提出的方法可以更好地解决3D dental图像分割任务。<details>
<summary>Abstract</summary>
Accurate representation of tooth position is extremely important in treatment. 3D dental image segmentation is a widely used method, however labelled 3D dental datasets are a scarce resource, leading to the problem of small samples that this task faces in many cases. To this end, we address this problem with a pretrained SAM and propose a novel 3D-U-SAM network for 3D dental image segmentation. Specifically, in order to solve the problem of using 2D pre-trained weights on 3D datasets, we adopted a convolution approximation method; in order to retain more details, we designed skip connections to fuse features at all levels with reference to U-Net. The effectiveness of the proposed method is demonstrated in ablation experiments, comparison experiments, and sample size experiments.
</details>
<details>
<summary>摘要</summary>
很重要的是精确地表示牙齿的位置在治疗中。3D dental图像分割是一种广泛使用的方法，但标注的3D dental数据集是一种罕见的资源，导致这个任务在许多情况下面临着小样本问题。为解决这个问题，我们使用预训练的SAM并提议一种3D-U-SAM网络 для3D dental图像分割。具体来说，为了解决使用2D预训练 веса在3D数据集上的问题，我们采用了一种核心approximation方法；为了保留更多的细节，我们设计了跳转连接，以融合所有层的特征参照U-Net。我们的提议方法的效果在ablation实验、比较实验和样本大小实验中得到了证明。
</details></li>
</ul>
<hr>
<h2 id="It’s-Simplex-Disaggregating-Measures-to-Improve-Certified-Robustness"><a href="#It’s-Simplex-Disaggregating-Measures-to-Improve-Certified-Robustness" class="headerlink" title="It’s Simplex! Disaggregating Measures to Improve Certified Robustness"></a>It’s Simplex! Disaggregating Measures to Improve Certified Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11005">http://arxiv.org/abs/2309.11005</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew C. Cullen, Paul Montague, Shijie Liu, Sarah M. Erfani, Benjamin I. P. Rubinstein</li>
<li>for: 防御攻击的难点是模型预测的不可靠性，这种研究用了证明模型预测的可靠性，以提高模型的鲁棒性。</li>
<li>methods: 这种研究使用了证明模型预测的可靠性，通过计算攻击大小来保证模型预测的可靠性。</li>
<li>results: 这种研究发现，通过考虑可靠性证明的输出空间，可以提高证明机制的分析，并且可以超过现有状态的证明范围。实验证明，新的证明方法可以在噪声率为1时证明9%更多的样本，并且在预测任务的难度增加时，Relative improvement更大。<details>
<summary>Abstract</summary>
Certified robustness circumvents the fragility of defences against adversarial attacks, by endowing model predictions with guarantees of class invariance for attacks up to a calculated size. While there is value in these certifications, the techniques through which we assess their performance do not present a proper accounting of their strengths and weaknesses, as their analysis has eschewed consideration of performance over individual samples in favour of aggregated measures. By considering the potential output space of certified models, this work presents two distinct approaches to improve the analysis of certification mechanisms, that allow for both dataset-independent and dataset-dependent measures of certification performance. Embracing such a perspective uncovers new certification approaches, which have the potential to more than double the achievable radius of certification, relative to current state-of-the-art. Empirical evaluation verifies that our new approach can certify $9\%$ more samples at noise scale $\sigma = 1$, with greater relative improvements observed as the difficulty of the predictive task increases.
</details>
<details>
<summary>摘要</summary>
《认证类弹性超越防御攻击的脆弱性，通过将模型预测 garantuee 为攻击规模内的类型不变，从而确保模型在攻击下的预测稳定性。 although there is value in these certifications, the techniques used to assess their performance do not provide a comprehensive account of their strengths and weaknesses, as they have neglected to consider the performance of individual samples. by considering the potential output space of certified models, this work presents two distinct approaches to improve the analysis of certification mechanisms, which allow for both dataset-independent and dataset-dependent measures of certification performance. embracing such a perspective uncovers new certification approaches, which have the potential to more than double the achievable radius of certification, relative to current state-of-the-art. empirical evaluation verifies that our new approach can certify $9\%$ more samples at noise scale $\sigma = 1$, with greater relative improvements observed as the difficulty of the predictive task increases.》
</details></li>
</ul>
<hr>
<h2 id="Towards-Data-centric-Graph-Machine-Learning-Review-and-Outlook"><a href="#Towards-Data-centric-Graph-Machine-Learning-Review-and-Outlook" class="headerlink" title="Towards Data-centric Graph Machine Learning: Review and Outlook"></a>Towards Data-centric Graph Machine Learning: Review and Outlook</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10979">http://arxiv.org/abs/2309.10979</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Zheng, Yixin Liu, Zhifeng Bao, Meng Fang, Xia Hu, Alan Wee-Chung Liew, Shirui Pan<br>for: 这篇论文主要关注数据驱动AI的发展，尤其是Graph数据结构的应用。methods: 论文提出了一种系统化框架，名为Data-centric Graph Machine Learning（DC-GML），该框架包括Graph数据生命周期中的所有阶段，包括数据收集、探索、改进、利用和维护。results: 论文提供了一份完整的taxonomy，用于回答三个关键的Graph数据中心问题：1）如何提高Graph数据的可用性和质量；2）如何从限量可用和低质量的Graph数据中学习；3）如何建立基于Graph数据的Machine Learning操作系统。<details>
<summary>Abstract</summary>
Data-centric AI, with its primary focus on the collection, management, and utilization of data to drive AI models and applications, has attracted increasing attention in recent years. In this article, we conduct an in-depth and comprehensive review, offering a forward-looking outlook on the current efforts in data-centric AI pertaining to graph data-the fundamental data structure for representing and capturing intricate dependencies among massive and diverse real-life entities. We introduce a systematic framework, Data-centric Graph Machine Learning (DC-GML), that encompasses all stages of the graph data lifecycle, including graph data collection, exploration, improvement, exploitation, and maintenance. A thorough taxonomy of each stage is presented to answer three critical graph-centric questions: (1) how to enhance graph data availability and quality; (2) how to learn from graph data with limited-availability and low-quality; (3) how to build graph MLOps systems from the graph data-centric view. Lastly, we pinpoint the future prospects of the DC-GML domain, providing insights to navigate its advancements and applications.
</details>
<details>
<summary>摘要</summary>
“数据驱动AI”在最近几年内受到了越来越多的关注，它的核心是通过收集、管理和利用数据驱动AI模型和应用程序。在这篇文章中，我们提供了一个深入和全面的评论，对现在的数据驱动AI方面的努力进行了详细的梳理，特别是在图数据strucuture上，图数据是现实世界中各种各样的实体之间的复杂依赖关系的基本表示方式。我们提出了一个涵盖所有图数据生命周期阶段的系统框架，称为数据驱动图机器学习（DC-GML），包括图数据收集、探索、改进、利用和维护等阶段。我们还提供了每个阶段的住进行三个关键问题的答案：（1）如何提高图数据可用性和质量；（2）如何从有限可用性和低质量的图数据中学习；（3）如何从图数据中心视建立图MLOps系统。最后，我们指出了DC-GML领域未来的前景，为其发展和应用提供了指导。
</details></li>
</ul>
<hr>
<h2 id="PAGER-A-Framework-for-Failure-Analysis-of-Deep-Regression-Models"><a href="#PAGER-A-Framework-for-Failure-Analysis-of-Deep-Regression-Models" class="headerlink" title="PAGER: A Framework for Failure Analysis of Deep Regression Models"></a>PAGER: A Framework for Failure Analysis of Deep Regression Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10977">http://arxiv.org/abs/2309.10977</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jayaraman J. Thiagarajan, Vivek Narayanaswamy, Puja Trivedi, Rushil Anirudh</li>
<li>for: 本文旨在提出一种检测深度回归模型预测错误的框架，以确保人工智能模型的安全部署。</li>
<li>methods: 本文使用了建立在深度模型中的稳定点的想法，并结合了知识 uncertainty 和非 conformity 分数，将样本分为不同的风险 régime。</li>
<li>results: 对于 synthetic 和实际 benchmark 进行了评估，结果显示了 PAGER 可以准确地检测出深度回归模型的预测错误，并且可以在不同的风险 régime 中分类样本。<details>
<summary>Abstract</summary>
Safe deployment of AI models requires proactive detection of potential prediction failures to prevent costly errors. While failure detection in classification problems has received significant attention, characterizing failure modes in regression tasks is more complicated and less explored. Existing approaches rely on epistemic uncertainties or feature inconsistency with the training distribution to characterize model risk. However, we show that uncertainties are necessary but insufficient to accurately characterize failure, owing to the various sources of error. In this paper, we propose PAGER (Principled Analysis of Generalization Errors in Regressors), a framework to systematically detect and characterize failures in deep regression models. Built upon the recently proposed idea of anchoring in deep models, PAGER unifies both epistemic uncertainties and novel, complementary non-conformity scores to organize samples into different risk regimes, thereby providing a comprehensive analysis of model errors. Additionally, we introduce novel metrics for evaluating failure detectors in regression tasks. We demonstrate the effectiveness of PAGER on synthetic and real-world benchmarks. Our results highlight the capability of PAGER to identify regions of accurate generalization and detect failure cases in out-of-distribution and out-of-support scenarios.
</details>
<details>
<summary>摘要</summary>
安全部署人工智能模型需要积极检测可能出现的预测错误，以避免高昂的错误成本。尽管在分类问题上的失败检测已经收到了广泛的关注，但在回归任务中的失败模式特征化尚未得到了充分的研究。现有的方法基于模型知识不确定性或特征偏移度与训练分布相关的方法来特征化模型风险。然而，我们表明了不确定性是特征化失败的必要条件，但并不够。在这篇论文中，我们提出了PAGER（基于深度模型的概念分析和总结），一种框架，用于系统地检测和特征化深度回归模型中的失败。基于深度模型的安chor思想，PAGER结合了epistemic不确定性和新的非准确性分数，将样本分为不同的风险 режимом，从而提供了全面的模型错误分析。此外，我们提出了新的评价失败检测器的度量方法。我们在synthetic和实际世界 benchmark上证明了PAGER的效果。我们的结果显示，PAGER能够标识出高度普适泛化和out-of-distribution和out-of-support场景中的失败案例。
</details></li>
</ul>
<hr>
<h2 id="Accurate-and-Scalable-Estimation-of-Epistemic-Uncertainty-for-Graph-Neural-Networks"><a href="#Accurate-and-Scalable-Estimation-of-Epistemic-Uncertainty-for-Graph-Neural-Networks" class="headerlink" title="Accurate and Scalable Estimation of Epistemic Uncertainty for Graph Neural Networks"></a>Accurate and Scalable Estimation of Epistemic Uncertainty for Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10976">http://arxiv.org/abs/2309.10976</a></li>
<li>repo_url: None</li>
<li>paper_authors: Puja Trivedi, Mark Heimann, Rushil Anirudh, Danai Koutra, Jayaraman J. Thiagarajan</li>
<li>for: 这个论文的目的是提高图 neural network（GNN）的安全部署，特别是在分布shift时提供准确的信任度指标（CI）。</li>
<li>methods: 这篇论文使用了一种case study来研究GNN CI的准确性，并证明了增加表达能力或模型大小不总是能提高CI性能。而是使用epistemic uncertainty量化（UQ）方法来调整CI。提出了一种新的单模型UQ方法——G-$\Delta$UQ，它基于最近提出的随机中心框架，支持结构化数据和部分随机性。</li>
<li>results: 对于covariate、concept和图大小shift，G-$\Delta$UQ不仅在获得准确的CI方面表现出色，还在使用CI进行泛化差分预测和OOD检测方面表现更好于其他popular UQ方法。总的来说，这篇论文不仅介绍了一种新的GNN UQ方法，还提供了图 neural network在安全关键任务上的新的理解。<details>
<summary>Abstract</summary>
Safe deployment of graph neural networks (GNNs) under distribution shift requires models to provide accurate confidence indicators (CI). However, while it is well-known in computer vision that CI quality diminishes under distribution shift, this behavior remains understudied for GNNs. Hence, we begin with a case study on CI calibration under controlled structural and feature distribution shifts and demonstrate that increased expressivity or model size do not always lead to improved CI performance. Consequently, we instead advocate for the use of epistemic uncertainty quantification (UQ) methods to modulate CIs. To this end, we propose G-$\Delta$UQ, a new single model UQ method that extends the recently proposed stochastic centering framework to support structured data and partial stochasticity. Evaluated across covariate, concept, and graph size shifts, G-$\Delta$UQ not only outperforms several popular UQ methods in obtaining calibrated CIs, but also outperforms alternatives when CIs are used for generalization gap prediction or OOD detection. Overall, our work not only introduces a new, flexible GNN UQ method, but also provides novel insights into GNN CIs on safety-critical tasks.
</details>
<details>
<summary>摘要</summary>
安全部署图 neural network (GNN) 需要模型提供准确的信任指标 (CI)。然而，虽然在计算机视觉中已经证明了 CI 质量下降于分布转移，但这一点尚未得到对 GNN 的研究。因此，我们开始了一项案例研究，探讨了 CI 准确性下降的情况，并发现增加表达能力或模型大小不一定能提高 CI 性能。因此，我们建议使用 epistemic 不确定性量化 (UQ) 方法来调整 CIs。为此，我们提出了 G-ΔUQ，一种新的单模型 UQ 方法，扩展了最近提出的随机中心框架，以支持结构化数据和部分随机性。经过 covariate、概念和图大小转移的评估，G-ΔUQ 不仅在获得准确的 CIs 方面超过了许多流行的 UQ 方法，还在用 CIs 进行泛化差分预测或 OOD 探测时表现更好。总的来说，我们不仅提出了一种新的、灵活的 GNN UQ 方法，而且为安全关键任务提供了新的思路和发现。
</details></li>
</ul>
<hr>
<h2 id="SPFQ-A-Stochastic-Algorithm-and-Its-Error-Analysis-for-Neural-Network-Quantization"><a href="#SPFQ-A-Stochastic-Algorithm-and-Its-Error-Analysis-for-Neural-Network-Quantization" class="headerlink" title="SPFQ: A Stochastic Algorithm and Its Error Analysis for Neural Network Quantization"></a>SPFQ: A Stochastic Algorithm and Its Error Analysis for Neural Network Quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10975">http://arxiv.org/abs/2309.10975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinjie Zhang, Rayan Saab</li>
<li>for: 这个论文是为了提出一种高效的神经网络压缩方法，以减少过参化神经网络中的重复性。</li>
<li>methods: 这个论文使用了一种快速的随机算法来压缩神经网络的权重。该方法利用了一种扩展的权重路径跟踪机制，以及一种随机压缩器。其计算复杂度只与神经网络中neuron数量成线性关系，因此可以有效地压缩大型神经网络。</li>
<li>results: 这个论文提出了一种全网络误差边界，以及一种在 Gaussian 权重下可以实现的高效压缩方法。此外，论文还证明了，当采用这种方法压缩多层神经网络时，误差表达的平方幂 decay Linear 方式与过参化程度增长。此外，论文还证明了可以使用 loglog N 比特数来实现误差边界相当于无限字母情况下的误差边界。<details>
<summary>Abstract</summary>
Quantization is a widely used compression method that effectively reduces redundancies in over-parameterized neural networks. However, existing quantization techniques for deep neural networks often lack a comprehensive error analysis due to the presence of non-convex loss functions and nonlinear activations. In this paper, we propose a fast stochastic algorithm for quantizing the weights of fully trained neural networks. Our approach leverages a greedy path-following mechanism in combination with a stochastic quantizer. Its computational complexity scales only linearly with the number of weights in the network, thereby enabling the efficient quantization of large networks. Importantly, we establish, for the first time, full-network error bounds, under an infinite alphabet condition and minimal assumptions on the weights and input data. As an application of this result, we prove that when quantizing a multi-layer network having Gaussian weights, the relative square quantization error exhibits a linear decay as the degree of over-parametrization increases. Furthermore, we demonstrate that it is possible to achieve error bounds equivalent to those obtained in the infinite alphabet case, using on the order of a mere $\log\log N$ bits per weight, where $N$ represents the largest number of neurons in a layer.
</details>
<details>
<summary>摘要</summary>
量化是一种广泛使用的压缩方法，可以有效地减少深度神经网络中的重复性。然而，现有的深度神经网络量化技术 frequently lack a comprehensive error analysis due to the presence of non-convex loss functions and nonlinear activations. In this paper, we propose a fast stochastic algorithm for quantizing the weights of fully trained neural networks. Our approach leverages a greedy path-following mechanism in combination with a stochastic quantizer. Its computational complexity scales only linearly with the number of weights in the network, thereby enabling the efficient quantization of large networks. Importantly, we establish, for the first time, full-network error bounds, under an infinite alphabet condition and minimal assumptions on the weights and input data. As an application of this result, we prove that when quantizing a multi-layer network having Gaussian weights, the relative square quantization error exhibits a linear decay as the degree of over-parametrization increases. Furthermore, we demonstrate that it is possible to achieve error bounds equivalent to those obtained in the infinite alphabet case, using on the order of a mere $\log\log N$ bits per weight, where $N$ represents the largest number of neurons in a layer.Note: The translation is done using the Google Translate API, and may not be perfect. Please let me know if you need any further assistance.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/20/cs.LG_2023_09_20/" data-id="clpztdnl700s2es880de02qzu" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_09_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/20/eess.IV_2023_09_20/" class="article-date">
  <time datetime="2023-09-20T09:00:00.000Z" itemprop="datePublished">2023-09-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/20/eess.IV_2023_09_20/">eess.IV - 2023-09-20</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="High-content-stimulated-Raman-histology-of-human-breast-cancer"><a href="#High-content-stimulated-Raman-histology-of-human-breast-cancer" class="headerlink" title="High-content stimulated Raman histology of human breast cancer"></a>High-content stimulated Raman histology of human breast cancer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11642">http://arxiv.org/abs/2309.11642</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongli Ni, Chinmayee Prabhu Dessai, Haonan Lin, Wei Wang, Shaoxiong Chen, Yuhao Yuan, Xiaowei Ge, Jianpeng Ao, Nolan Vild, Ji-Xin Cheng</li>
<li>for: This paper aims to provide a high-content stimulated Raman histology (HC-SRH) platform for cancer diagnosis based on un-stained breast tissues, which can provide both morphological and chemical information.</li>
<li>methods: The HC-SRH platform uses spectral unmixing in the C-H vibration window to map unsaturated lipids, cellular protein, extracellular matrix, saturated lipid, and water in breast tissue, and spectral selective sampling is implemented to boost the speed of HC-SRH.</li>
<li>results: The HC-SRH platform provides excellent contrast for various tissue components, and the advanced fiber laser-based SRS microscopy demonstrates the HC-SRH in a clinical-compatible manner, showing a clear chemical contrast of nucleic acid and solid-state ester in the fingerprint result.<details>
<summary>Abstract</summary>
Histological examination is crucial for cancer diagnosis, including hematoxylin and eosin (H&E) staining for mapping morphology and immunohistochemistry (IHC) staining for revealing chemical information. Recently developed two-color stimulated Raman histology could bypass the complex tissue processing to mimic H&E-like morphology. Yet, the underlying chemical features are not revealed, compromising the effectiveness of prognostic stratification. Here, we present a high-content stimulated Raman histology (HC-SRH) platform that provides both morphological and chemical information for cancer diagnosis based on un-stained breast tissues. Through spectral unmixing in the C-H vibration window, HC-SRH can map unsaturated lipids, cellular protein, extracellular matrix, saturated lipid, and water in breast tissue. In this way, HC-SRH provides excellent contrast for various tissue components. Considering rapidness is important in clinical trials, we implemented spectral selective sampling to boost the speed of HC-SRH by one order. We also successfully demonstrated the HC-SRH in a clinical-compatible fiber laser-based SRS microscopy. With the widely rapid tuning capability of the advanced fiber laser, a clear chemical contrast of nucleic acid and solid-state ester is shown in the fingerprint result.
</details>
<details>
<summary>摘要</summary>
histological 检查是癌病诊断中不可或缺的，包括杂谱和染色技术。 latest developments in two-color stimulated Raman histology can mimic H&E-like morphology, but the underlying chemical features are not revealed, which compromises the effectiveness of prognostic stratification. Here, we present a high-content stimulated Raman histology (HC-SRH) platform that provides both morphological and chemical information for cancer diagnosis based on un-stained breast tissues. Through spectral unmixing in the C-H vibration window, HC-SRH can map unsaturated lipids, cellular protein, extracellular matrix, saturated lipid, and water in breast tissue. In this way, HC-SRH provides excellent contrast for various tissue components. Considering rapidness is important in clinical trials, we implemented spectral selective sampling to boost the speed of HC-SRH by one order. We also successfully demonstrated the HC-SRH in a clinical-compatible fiber laser-based SRS microscopy. With the widely rapid tuning capability of the advanced fiber laser, a clear chemical contrast of nucleic acid and solid-state ester is shown in the fingerprint result.
</details></li>
</ul>
<hr>
<h2 id="Lightning-Fast-Dual-Layer-Lossless-Coding-for-Radiance-Format-High-Dynamic-Range-Images"><a href="#Lightning-Fast-Dual-Layer-Lossless-Coding-for-Radiance-Format-High-Dynamic-Range-Images" class="headerlink" title="Lightning-Fast Dual-Layer Lossless Coding for Radiance Format High Dynamic Range Images"></a>Lightning-Fast Dual-Layer Lossless Coding for Radiance Format High Dynamic Range Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11072">http://arxiv.org/abs/2309.11072</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taizo Suzuki, Sara Yukikata, Kai Yang, Taichi Yoshida</li>
<li>for: 这个论文是为了提出一种高动态范围图像（HDRIs）的快速双层无损编码方法。</li>
<li>methods: 该编码方法包括基层和无损增强层，可以直接将高动态范围图像（HDRIs）转换为标准动态范围图像（SDRIs），无需在解码器端添加额外算法，同时可以产生高质量的SDRIs。</li>
<li>results: 对比现有方法，该编码方法可以减少平均比特率约为1.57%-6.68%，并且显著减少解码器实现时间约为87.13%-98.96%。<details>
<summary>Abstract</summary>
This paper proposes a fast dual-layer lossless coding for high dynamic range images (HDRIs) in the Radiance format. The coding, which consists of a base layer and a lossless enhancement layer, provides a standard dynamic range image (SDRI) without requiring an additional algorithm at the decoder and can losslessly decode the HDRI by adding the residual signals (residuals) between the HDRI and SDRI to the SDRI, if desired. To suppress the dynamic range of the residuals in the enhancement layer, the coding directly uses the mantissa and exponent information from the Radiance format. To further reduce the residual energy, each mantissa is modeled (estimated) as a linear function, i.e., a simple linear regression, of the encoded-decoded SDRI in each region with the same exponent. This is called simple linear regressive mantissa estimator. Experimental results show that, compared with existing methods, our coding reduces the average bitrate by approximately $1.57$-$6.68$ % and significantly reduces the average encoder implementation time by approximately $87.13$-$98.96$ %.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种快速双层无损编码器，用于高动态范围图像（HDRIs）在辉度格式下。该编码器由基层和无损增强层组成，可以将标准动态范围图像（SDRI）转换为HDRIs，无需额外算法在解码器端。此外，该编码器还可以losslessly解码HDRIs，只需将差异信号（差异）加到SDRI上即可。为了减少增强层的动态范围，该编码器直接使用Radiance格式中的杠志和指数信息。进一步减少差异能量，每个杠志都被模型为在每个区域中的线性函数，即简单的线性回归。这被称为简单的线性回归杠志估计器。实验结果表明，相比现有方法，我们的编码器可以将平均比特率降低约1.57%-6.68%，并显著降低解码器实现时间约87.13%-98.96%。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/20/eess.IV_2023_09_20/" data-id="clpztdnsl01ates8811160c3z" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_09_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/20/eess.SP_2023_09_20/" class="article-date">
  <time datetime="2023-09-20T08:00:00.000Z" itemprop="datePublished">2023-09-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/20/eess.SP_2023_09_20/">eess.SP - 2023-09-20</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Channel-Reciprocity-Attacks-Using-Intelligent-Surfaces-with-Non-Diagonal-Phase-Shifts"><a href="#Channel-Reciprocity-Attacks-Using-Intelligent-Surfaces-with-Non-Diagonal-Phase-Shifts" class="headerlink" title="Channel Reciprocity Attacks Using Intelligent Surfaces with Non-Diagonal Phase Shifts"></a>Channel Reciprocity Attacks Using Intelligent Surfaces with Non-Diagonal Phase Shifts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11665">http://arxiv.org/abs/2309.11665</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoyu Wang, Zhu Han, Lee Swindlehurst</li>
<li>for: 本研究探讨了一种基于卷积智能表面（RIS）技术的攻击，该技术可以在多天线几何系统中引起通信链路的干扰。</li>
<li>methods: 本研究使用了非对称（ND）幅shift矩阵（RIS），通过非对称幅shift矩阵来破坏通信链路的对称性，从而降低下链路性能。</li>
<li>results: 研究结果表明，当一个恶意的ND-RIS被部署时，可以使得下链路性能受到极大的降低，并且这种攻击是无动作的和难以探测的。<details>
<summary>Abstract</summary>
While reconfigurable intelligent surface (RIS) technology has been shown to provide numerous benefits to wireless systems, in the hands of an adversary such technology can also be used to disrupt communication links. This paper describes and analyzes an RIS-based attack on multi-antenna wireless systems that operate in time-division duplex mode under the assumption of channel reciprocity. In particular, we show how an RIS with a non-diagonal (ND) phase shift matrix (referred to here as an ND-RIS) can be deployed to maliciously break the channel reciprocity and hence degrade the downlink network performance. Such an attack is entirely passive and difficult to detect. We provide a theoretical analysis of the degradation in the sum ergodic rate that results when an arbitrary malicious ND-RIS is deployed and design an approach based on the genetic algorithm for optimizing the ND structure under partial knowledge of the available channel state information. Our simulation results validate the analysis and demonstrate that an ND-RIS channel reciprocity attack can dramatically reduce the downlink throughput.
</details>
<details>
<summary>摘要</summary>
“弹性智能表面（RIS）技术已经被证明可以提供无线系统中的许多优点，但在敌人手上可以使用这技术来中断通信链接。本纸描述了一种基于RIS的攻击，对于在时分多普遍调幅模式下运行的多antenna无线系统进行破坏。具体来说，我们显示了如何在ND项目（non-diagonal）的RIS中部署恶意的项目，以破坏通道对称性，从而降低下联网性能。这种攻击是完全被动的，难以检测。我们提供了一个理论分析，以及基于生物算法来优化ND结构的方法，以对不完全知道可用通道状态信息进行优化。我们的实验结果证实了分析，并显示了ND-RIS通道对称性攻击可以导致下联网通过率的严重下降。”Note: The translation is done using Google Translate and may not be perfect. Please let me know if you need further assistance.
</details></li>
</ul>
<hr>
<h2 id="Compression-Spectrum-Where-Shannon-meets-Fourier"><a href="#Compression-Spectrum-Where-Shannon-meets-Fourier" class="headerlink" title="Compression Spectrum: Where Shannon meets Fourier"></a>Compression Spectrum: Where Shannon meets Fourier</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11640">http://arxiv.org/abs/2309.11640</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditi Kathpalia, Nithin Nagaraj</li>
<li>for: 这篇论文的目的是将信号处理和信息理论两个领域联系起来，以便通过对时间序列的压缩来估算它的信息量和压缩程度。</li>
<li>methods: 本文使用了一种无损数据压缩算法来估算时间序列的信息量或压缩程度，并使用了Effort-to-Compress（ETC）算法来获得一个压缩спектrum。</li>
<li>results: 本文通过应用压缩спектrum于人体心跳间隔（RR）序列，发现健康年轻人RR序列在律 log-log 尺度上表现类似于1&#x2F;f 噪声，而健康老年人RR序列则表现不同。<details>
<summary>Abstract</summary>
Signal processing and Information theory are two disparate fields used for characterizing signals for various scientific and engineering applications. Spectral/Fourier analysis, a technique employed in signal processing, helps estimation of power at different frequency components present in the signal. Characterizing a time-series based on its average amount of information (Shannon entropy) is useful for estimating its complexity and compressibility (eg., for communication applications). Information theory doesn't deal with spectral content while signal processing doesn't directly consider the information content or compressibility of the signal. In this work, we attempt to bring the fields of signal processing and information theory together by using a lossless data compression algorithm to estimate the amount of information or `compressibility' of time series at different scales. To this end, we employ the Effort-to-Compress (ETC) algorithm to obtain what we call as a Compression Spectrum. This new tool for signal analysis is demonstrated on synthetically generated periodic signals, a sinusoid, chaotic signals (weak and strong chaos) and uniform random noise. The Compression Spectrum is applied on heart interbeat intervals (RR) obtained from real-world normal young and elderly subjects. The compression spectrum of healthy young RR tachograms in the log-log scale shows behaviour similar to $1/f$ noise whereas the healthy old RR tachograms show a different behaviour. We envisage exciting possibilities and future applications of the Compression Spectrum.
</details>
<details>
<summary>摘要</summary>
信号处理和信息理论是两个不同的领域，用于描述信号的不同科学和工程应用。spectral/ fourier分析是信号处理中使用的技术，可以为不同频率组成的信号估算能量。基于时间序列的平均信息量（Shannon entropy）的Characterizing是用于估算信号的复杂性和压缩性（例如， для通信应用）。信息理论不考虑频谱内容，而信号处理不直接考虑信号的信息内容或压缩性。在这项工作中，我们尝试将信号处理和信息理论两个领域联系起来，使用一种无损数据压缩算法来估算时间序列的信息量或“压缩性”。为此，我们使用Effort-to-Compress（ETC）算法获得一个压缩спектrum。这种新的信号分析工具在人工生成的 периодические信号、sinusoid信号、混沌信号（弱和强混沌）以及随机噪声上进行了应用。压缩спектrum在实际获得的心跳间隔（RR）上进行了应用，并在径向均衡尺度上显示了类似于1/f噪声的行为。我们看到了未来应用的激动人心。
</details></li>
</ul>
<hr>
<h2 id="Brief-Architectural-Survey-of-Biopotential-Recording-Front-Ends-since-the-1970s"><a href="#Brief-Architectural-Survey-of-Biopotential-Recording-Front-Ends-since-the-1970s" class="headerlink" title="Brief Architectural Survey of Biopotential Recording Front-Ends since the 1970s"></a>Brief Architectural Survey of Biopotential Recording Front-Ends since the 1970s</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11612">http://arxiv.org/abs/2309.11612</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taeju Lee, Minkyu Je</li>
<li>For: The paper provides a survey of the architecture history of biopotential recording front-ends developed since the 1970s, and discusses overall key circuit techniques for reliable and continuous signal acquisition.* Methods: The paper discusses various front-end architectures for biopotential recording, including their characteristics and challenges, depending on the bioelectric signals being measured.* Results: The paper provides an overview of the evolution of biopotential recording front-ends over the last five decades, and discusses the key circuit techniques for low power and low noise performance.Here are the three information points in Simplified Chinese text:* For: 这篇论文提供了1970年代以来生物电动力记录前端的建筑历史，并讨论了适用于可靠连续记录的信号获取的总体关键电路技术。* Methods: 论文讨论了不同的生物电动力记录前端架构，包括它们的特点和挑战，具体取决于测量的生物电动力信号。* Results: 论文提供了过去五十年来生物电动力记录前端的演化历史，并讨论了低功耗和低噪声性能的关键电路技术。<details>
<summary>Abstract</summary>
Measuring the bioelectric signals is one of the key functions in wearable healthcare devices and implantable medical devices. The use of wearable healthcare devices has made continuous and immediate monitoring of personal health status possible. Implantable medical devices have played an important role throughout the fields of neuroscience, brain-machine (or brain-computer) interface, and rehabilitation technology. Over the last five decades, the bioelectric signals have been observed through a variety of biopotential recording front-ends, along with advances in semiconductor technology scaling and circuit techniques. Also, for reliable and continuous signal acquisition, the front-end architectures have evolved while maintaining low power and low noise performance. In this article, the architecture history of the biopotential recording front-ends developed since the 1970s is surveyed, and overall key circuit techniques are discussed. Depending on the bioelectric signals being measured, appropriate front-end architecture needs to be chosen, and the characteristics and challenges of each architecture are also covered in this article.
</details>
<details>
<summary>摘要</summary>
测量生物电子信号是现代医疗设备和嵌入式医疗设备中的关键功能之一。使用了可穿戴式医疗设备，人们可以实时、连续地监测个人健康状况。嵌入式医疗设备在神经科学、脑机 interfaces 和rehabilitation技术等领域发挥了重要作用。过去五十年，生物电子信号已经通过多种生物潜在记录前端，利用半导体技术的发展和电路技术的进步。为确保可靠和连续的信号捕获，前端架构也在不断发展，同时保持低功耗和低噪性能。在这篇文章中，自1970年代以来发展的生物潜在记录前端架构历史被评估，同时总体讲述了关键的电路技术。根据测量的生物电子信号，需要选择合适的前端架构，文章还讲述了每种架构的特点和挑战。
</details></li>
</ul>
<hr>
<h2 id="Self-Sustaining-Oscillator-with-Frequency-Counter-for-Resonance-Frequency-Tracking-in-Micro-and-Nanomechanical-Sensing"><a href="#Self-Sustaining-Oscillator-with-Frequency-Counter-for-Resonance-Frequency-Tracking-in-Micro-and-Nanomechanical-Sensing" class="headerlink" title="Self-Sustaining Oscillator with Frequency Counter for Resonance Frequency Tracking in Micro- and Nanomechanical Sensing"></a>Self-Sustaining Oscillator with Frequency Counter for Resonance Frequency Tracking in Micro- and Nanomechanical Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11581">http://arxiv.org/abs/2309.11581</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hajrudin Bešić, Alper Demir, Veljko Vukićević, Johannes Steurer, Silvan Schmid</li>
<li>for: 本研究旨在提出一种基于振荡频率变化的奈米机械感知器，并通过 theoretically 和实验研究其速度和精度。</li>
<li>methods: 本研究使用了一种基于振荡频率变化的自带维持振荡器（SSO）奈米电子机械系统（NEMS）配置，并提出了一种基于振荡频率变化的频率计数器，以实现高速度和高精度的频率测量。</li>
<li>results: 研究结果显示，与现有的阶段锁定循环（PLL）方法相比，提出的方法具有类似或更好的性能，同时具有更低的成本和更高的使用容易度。实验测量结果与理论预测几乎完美吻合。<details>
<summary>Abstract</summary>
Nanomechanical sensors based on detecting and tracking resonance frequency shifts are to be used in many applications. Various open- and closed-loop tracking schemes, all offering a trade-off between speed and precision, have been studied both theoretically and experimentally. In this work, we advocate the use of a frequency counter as a frequency shift monitor in conjunction with a self-sustaining oscillator (SSO) nanoelectromechanical system (NEMS) configuration. We derive a theoretical model for characterizing the speed and precision of frequency measurements with state-of-the-art frequency counters. Based on the understanding provided by this model, we introduce novel enhancements to frequency counters that result in a trade-off characteristics which is on a par with the other tracking schemes. We describe a low-cost field-programmable-gate array (FPGA) based implementation for the proposed frequency counter and use it with the SSO-NEMS device in order to study its frequency tracking performance. We compare the proposed approach with the phase-locked-loop based scheme both in theory and experimentally. Our results show that similar or better performance can be achieved at a substantially lower cost and improved ease-of-use. We obtain almost perfect correspondence between the theoretical model predictions and the experimental measurements.
</details>
<details>
<summary>摘要</summary>
几种 nanomechanical 感测器基于探测和跟踪征频Shift的应用将在未来中普遍使用。各种开放和关闭loop tracking 方案，均提供了速度和精度之间的交易，已经被理论和实验研究。在这项工作中，我们建议使用频计作为征频shift 监测器，并与自持 oscillator（SSO） nanoelectromechanical system（NEMS）配置一起使用。我们 derivated一个理论模型，用于Characterizing 频度测量的速度和精度。基于这个模型，我们提出了一些新的增强，使得频计的交易特性与其他跟踪方案相当。我们描述了一种低成本的 field-programmable-gate array（FPGA）基于实现，并用其与 SSO-NEMS 设备一起研究其频度跟踪性能。我们比较了我们的方法与阶段锁相控制（PLL） 方案， both theoretically and experimentally。我们的结果表明，可以在更低的成本和更好的使用性下达到相同或更好的性能。我们实验中的结果与理论预测几乎完美匹配。
</details></li>
</ul>
<hr>
<h2 id="Decision-Directed-Hybrid-RIS-Channel-Estimation-with-Minimal-Pilot-Overhead"><a href="#Decision-Directed-Hybrid-RIS-Channel-Estimation-with-Minimal-Pilot-Overhead" class="headerlink" title="Decision-Directed Hybrid RIS Channel Estimation with Minimal Pilot Overhead"></a>Decision-Directed Hybrid RIS Channel Estimation with Minimal Pilot Overhead</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11485">http://arxiv.org/abs/2309.11485</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ly V. Nguyen, A. Lee Swindlehurst</li>
<li>for: 提高系统spectral efficiency，减少频率干扰。</li>
<li>methods: 使用具有混合元件的RIS，同时反射和感知入射信号，提高渠道状态信息的准确性。</li>
<li>results: 比传统静脉RIS数组系统具有更高的系统spectral efficiency，减少了频率干扰。<details>
<summary>Abstract</summary>
To reap the benefits of reconfigurable intelligent surfaces (RIS), channel state information (CSI) is generally required. However, CSI acquisition in RIS systems is challenging and often results in very large pilot overhead, especially in unstructured channel environments. Consequently, the RIS channel estimation problem has attracted a lot of interest and also been a subject of intense study in recent years. In this paper, we propose a decision-directed RIS channel estimation framework for general unstructured channel models. The employed RIS contains some hybrid elements that can simultaneously reflect and sense the incoming signal. We show that with the help of the hybrid RIS elements, it is possible to accurately recover the CSI with a pilot overhead proportional to the number of users. Therefore, the proposed framework substantially improves the system spectral efficiency compared to systems with passive RIS arrays since the pilot overhead in passive RIS systems is proportional to the number of RIS elements times the number of users. We also perform a detailed spectral efficiency analysis for both the pilot-directed and decision-directed frameworks. Our analysis takes into account both the channel estimation and data detection errors at both the RIS and the BS. Finally, we present numerous simulation results to verify the accuracy of the analysis as well as to show the benefits of the proposed decision-directed framework.
</details>
<details>
<summary>摘要</summary>
通常需要通道状态信息（CSI）来收获智能重配置表面（RIS）的优点。然而，在RIS系统中获取CSI是具有挑战性和很大的尝试量，特别是在无结构通道环境中。因此，RIS通道估计问题已经吸引了很多关注并成为了近年来的研究主题。在这篇论文中，我们提议了一种基于决策的RIS通道估计框架，适用于一般的无结构通道模型。 employ 的RIS包含了一些混合元素，这些元素可同时反射和感知进来的信号。我们表明，使用这些混合RIS元素，可以准确地重建CSI，并且尝试量与用户数成正比。因此，我们提议的框架可以substantially提高系统spectral efficiency，比pasive RIS数组系统更高。我们还进行了详细的spectral efficiency分析，包括通道估计和数据检测错误在RIS和BS之间。最后，我们提供了许多的 simulations 结果，以验证分析的准确性，以及显示提议的决策导向框架的优势。
</details></li>
</ul>
<hr>
<h2 id="Generalised-Hyperbolic-State-space-Models-for-Inference-in-Dynamic-Systems"><a href="#Generalised-Hyperbolic-State-space-Models-for-Inference-in-Dynamic-Systems" class="headerlink" title="Generalised Hyperbolic State-space Models for Inference in Dynamic Systems"></a>Generalised Hyperbolic State-space Models for Inference in Dynamic Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11422">http://arxiv.org/abs/2309.11422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yaman Kındap, Simon Godsill</li>
<li>for: 这个论文是为了探讨连续时间非格aussian滤波问题中的非格aussian滤波模型。</li>
<li>methods: 这个论文使用了一种基于总体化弗洛伯恩（GH）随机过程的连续时间均值场模型，并提供了连续时间模拟方法和一种基于MCMC的新的推断方法。</li>
<li>results: 这个论文通过应用到一个 sintetically生成的数据集和一个实际的金融时间序列上，以示其能力。<details>
<summary>Abstract</summary>
In this work we study linear vector stochastic differential equation (SDE) models driven by the generalised hyperbolic (GH) L\'evy process for inference in continuous-time non-Gaussian filtering problems. The GH family of stochastic processes offers a flexible framework for modelling of non-Gaussian, heavy-tailed characteristics and includes the normal inverse-Gaussian, variance-gamma and Student-t processes as special cases. We present continuous-time simulation methods for the solution of vector SDE models driven by GH processes and novel inference methodologies using a variant of sequential Markov chain Monte Carlo (MCMC). As an example a particular formulation of Langevin dynamics is studied within this framework. The model is applied to both a synthetically generated data set and a real-world financial series to demonstrate its capabilities.
</details>
<details>
<summary>摘要</summary>
“在这项工作中，我们研究线性向量抽象差分方程（SDE）模型，该模型由总体化幂（GH）随机过程驱动。GH随机过程家族提供非常灵活的非高准入特性模型化框架，包括正态反射差分、差分gamma和学生t过程为特殊情况。我们提出了积累时间 simulation方法来解决vector SDE模型中的GH过程，并提出了一种基于Markov链 Monte Carlo（MCMC）的新的推断方法。作为一个示例，我们研究了一种特定的Langevin动力学形式。该模型在一个 sintetically生成的数据集和一个实际世界金融时间序列上进行了应用，以示其能力。”Note that Simplified Chinese is a romanization of Chinese, and the translation may not be perfect.
</details></li>
</ul>
<hr>
<h2 id="Active-Inference-for-Sum-Rate-Maximization-in-UAV-Assisted-Cognitive-NOMA-Networks"><a href="#Active-Inference-for-Sum-Rate-Maximization-in-UAV-Assisted-Cognitive-NOMA-Networks" class="headerlink" title="Active Inference for Sum Rate Maximization in UAV-Assisted Cognitive NOMA Networks"></a>Active Inference for Sum Rate Maximization in UAV-Assisted Cognitive NOMA Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11263">http://arxiv.org/abs/2309.11263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Felix Obite, Ali Krayani, Atm S. Alam, Lucio Marcenaro, Arumugam Nallanathan, Carlo Regazzoni</li>
<li>for: 本研究旨在强化未来无线网络的通信容量，以满足由互联网物联网（IoT）、无人机（UAV）、认知 радио（CR）和多播访问（NOMA）等技术引起的巨大连接问题。</li>
<li>methods: 本文使用了认知活动推理（active inference）从认知神经科学中启发，并提出了一种协调子频和功率分配算法，以最大化总比特率。</li>
<li>results:  simulation结果表明，相比benchmark方案，我们提出的算法可以更好地适应时间变化的网络环境，并提高积总比特率。<details>
<summary>Abstract</summary>
Given the surge in wireless data traffic driven by the emerging Internet of Things (IoT), unmanned aerial vehicles (UAVs), cognitive radio (CR), and non-orthogonal multiple access (NOMA) have been recognized as promising techniques to overcome massive connectivity issues. As a result, there is an increasing need to intelligently improve the channel capacity of future wireless networks. Motivated by active inference from cognitive neuroscience, this paper investigates joint subchannel and power allocation for an uplink UAV-assisted cognitive NOMA network. Maximizing the sum rate is often a highly challenging optimization problem due to dynamic network conditions and power constraints. To address this challenge, we propose an active inference-based algorithm. We transform the sum rate maximization problem into abnormality minimization by utilizing a generalized state-space model to characterize the time-changing network environment. The problem is then solved using an Active Generalized Dynamic Bayesian Network (Active-GDBN). The proposed framework consists of an offline perception stage, in which a UAV employs a hierarchical GDBN structure to learn an optimal generative model of discrete subchannels and continuous power allocation. In the online active inference stage, the UAV dynamically selects discrete subchannels and continuous power to maximize the sum rate of secondary users. By leveraging the errors in each episode, the UAV can adapt its resource allocation policies and belief updating to improve its performance over time. Simulation results demonstrate the effectiveness of our proposed algorithm in terms of cumulative sum rate compared to benchmark schemes.
</details>
<details>
<summary>摘要</summary>
随着无线数据交换量的增加，启发于互联网宇宙（IoT）、无人机（UAV）、认知电波（CR）和非对称多接入（NOMA）等技术的应用，Future无线网络的通道容量需要更加智能地提高。为了解决这一挑战，这篇论文提出了一种基于活动推理的 JOINT 子频率和功率分配算法。通过将总Bit rate最大化问题转化为异常值最小化问题，我们利用一种通用状态空间模型来描述时间变化的网络环境。然后，我们使用一个活动总体动态 bayesian 网络（Active-GDBN）解决这个问题。我们的框架包括在线上active inference阶段，在这个阶段，UAV使用一个层次结构的 GDBN 结构来学习精确的生成模型，以便在精确的子频率和连续的功率分配方面进行最佳化。在线上激活推理阶段，UAV会在精确的子频率和连续的功率分配方面进行动态选择，以最大化次级用户的总Bit rate。通过利用每个回合中的错误，UAV可以适应其资源分配策略和信息更新，从而提高其性能。实验结果表明，我们提出的算法在总Bit rate方面与参考方案相比表现更好。
</details></li>
</ul>
<hr>
<h2 id="Beamforming-Design-for-RIS-Aided-THz-Wideband-Communication-Systems"><a href="#Beamforming-Design-for-RIS-Aided-THz-Wideband-Communication-Systems" class="headerlink" title="Beamforming Design for RIS-Aided THz Wideband Communication Systems"></a>Beamforming Design for RIS-Aided THz Wideband Communication Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11161">http://arxiv.org/abs/2309.11161</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihang Jiang, Ziqin Zhou, Xiaoyang Li, Yi Gong</li>
<li>for: 这篇论文的目的是解决在teraHz（THz）通信系统中的笼形拥填问题，以提高未来6G网络的性能。</li>
<li>methods: 论文提出了一种新的护墙扩展器（RIS）支持的笼形拥填架构，以减少笼形拥填的影响。</li>
<li>results:  simulations表明，提出的架构能够有效地减少笼形拥填的影响，提高系统的性能。<details>
<summary>Abstract</summary>
Benefiting from tens of GHz of bandwidth, terahertz (THz) communications has become a promising technology for future 6G networks. However, the conventional hybrid beamforming architecture based on frequency-independent phase-shifters is not able to cope with the beam split effect (BSE) in THz massive multiple-input multiple-output (MIMO) systems. Despite some work introducing the frequency-dependent phase shifts via the time delay network to mitigate the beam splitting in THz wideband communications, the corresponding issue in reconfigurable intelligent surface (RIS)-aided communications has not been well investigated. In this paper, the BSE in THz massive MIMO is quantified by analyzing the array gain loss. A new beamforming architecture has been proposed to mitigate this effect under RIS-aided communications scenarios. Simulations are performed to evaluate the effectiveness of the proposed system architecture in combating the array gain loss.
</details>
<details>
<summary>摘要</summary>
使用十几GHz的带宽，teraHz（THz）通信已成为未来6G网络的促进技术。然而，传统的混合 beamforming架构基于频率独立的相位调节器无法应对THz大规模多输入多输出（MIMO）系统中的束分裂效应（BSE）。虽有一些工作介绍了频率相关的相位偏移通过时延网络来mitigate THz广泛通信中的束分裂，但相关的RIS（可编程智能面）协助通信场景的研究尚未得到了充分的探讨。本文对THz大规模MIMO系统中的BSE进行了分析，并提出了一种新的束分裂 Mitigation architecture。通过实验评估了提议系统架构的效果。
</details></li>
</ul>
<hr>
<h2 id="Sum-Rate-Maximization-for-Movable-Antenna-Enabled-Multiuser-Communications"><a href="#Sum-Rate-Maximization-for-Movable-Antenna-Enabled-Multiuser-Communications" class="headerlink" title="Sum-Rate Maximization for Movable Antenna Enabled Multiuser Communications"></a>Sum-Rate Maximization for Movable Antenna Enabled Multiuser Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11135">http://arxiv.org/abs/2309.11135</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenqiao Cheng, Nanxi Li, Jianchi Zhu, Xiaoming She, Chongjun Ouyang, Peng Chen</li>
<li>for: 提高下链吞吐量</li>
<li>methods: 使用antenna位置优化和传输扬送矩阵优化</li>
<li>results: 提高下链吞吐量和性能比FPAs更高Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written to propose a novel multiuser communication system with movable antennas (MAs) that can enhance the downlink sum-rate by exploiting the antenna position optimization.</li>
<li>methods: The paper uses a joint optimization of the transmit beamforming vector and transmit MA positions to solve the non-convex problem. The authors propose an efficient algorithm that combines fractional programming, alternating optimization, and gradient descent methods to tackle the problem. As an alternative, a zero-forcing beamforming-based design is also proposed to strike a better performance-complexity trade-off.</li>
<li>results: Numerical investigations show that the proposed algorithms achieve better performance compared with the benchmark relying on conventional fixed-position antennas (FPAs). The proposed system can improve the downlink sum-rate and provide better performance than FPAs.<details>
<summary>Abstract</summary>
A novel multiuser communication system with movable antennas (MAs) is proposed, where the antenna position optimization is exploited to enhance the downlink sum-rate. The joint optimization of the transmit beamforming vector and transmit MA positions is studied for a multiuser multiple-input single-input system. An efficient algorithm is proposed to tackle the formulated non-convex problem via capitalizing on fractional programming, alternating optimization, and gradient descent methods. To strike a better performance-complexity trade-off, a zero-forcing beamforming-based design is also proposed as an alternative. Numerical investigations are presented to verify the efficiency of the proposed algorithms and their superior performance compared with the benchmark relying on conventional fixed-position antennas (FPAs).
</details>
<details>
<summary>摘要</summary>
新的多用户通信系统，使用可移动天线（MA），提出了一种新的天线位置优化技术，以增加下链数据率。在多用户多输入单输出系统中，joint优化传输扩散矩阵和天线位置的算法被研究。通过使用分数编程、 alternate优化和梯度下降方法，提出了一种高效的算法。为了更好地平衡性能和复杂度之间的贸易，也提出了一种基于零干扰扩散矩阵的设计。 numerically investigate the proposed algorithms and their superior performance compared with the benchmark relying on conventional fixed-position antennas (FPAs).Here is the word-for-word translation of the text into Simplified Chinese:新的多用户通信系统，使用可移动天线（MA），提出了一种新的天线位置优化技术，以增加下链数据率。在多用户多输入单输出系统中，joint优化传输扩散矩阵和天线位置的算法被研究。通过使用分数编程、 alternate优化和梯度下降方法，提出了一种高效的算法。为了更好地平衡性能和复杂度之间的贸易，也提出了一种基于零干扰扩散矩阵的设计。 numerically investigate the proposed algorithms and their superior performance compared with the benchmark relying on conventional fixed-position antennas (FPAs).
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Mental-Stress-Among-College-Students-Using-Heart-Rate-and-Hand-Acceleration-Data-Collected-from-Wearable-Sensors"><a href="#Evaluating-Mental-Stress-Among-College-Students-Using-Heart-Rate-and-Hand-Acceleration-Data-Collected-from-Wearable-Sensors" class="headerlink" title="Evaluating Mental Stress Among College Students Using Heart Rate and Hand Acceleration Data Collected from Wearable Sensors"></a>Evaluating Mental Stress Among College Students Using Heart Rate and Hand Acceleration Data Collected from Wearable Sensors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11097">http://arxiv.org/abs/2309.11097</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moein Razavi, Anthony McDonald, Ranjana Mehta, Farzan Sasangohar<br>for: This paper aims to develop a machine learning-based method for identifying stress using physiological data collected from college students.methods: The study uses wearable wrist-worn sensors and a mobile health application to collect heart rate and hand acceleration data, and self-reported stress data from college students. The XGBoost method was used to evaluate the effectiveness of the machine learning algorithms for stress detection.results: The study found that the XGBoost method was the most reliable model for identifying stress episodes, with an AUC of 0.64 and an accuracy of 84.5%. The standard deviation of hand acceleration, standard deviation of heart rate, and the minimum heart rate were the most important features for stress detection.<details>
<summary>Abstract</summary>
Stress is various mental health disorders including depression and anxiety among college students. Early stress diagnosis and intervention may lower the risk of developing mental illnesses. We examined a machine learning-based method for identification of stress using data collected in a naturalistic study utilizing self-reported stress as ground truth as well as physiological data such as heart rate and hand acceleration. The study involved 54 college students from a large campus who used wearable wrist-worn sensors and a mobile health (mHealth) application continuously for 40 days. The app gathered physiological data including heart rate and hand acceleration at one hertz frequency. The application also enabled users to self-report stress by tapping on the watch face, resulting in a time-stamped record of the self-reported stress. We created, evaluated, and analyzed machine learning algorithms for identifying stress episodes among college students using heart rate and accelerometer data. The XGBoost method was the most reliable model with an AUC of 0.64 and an accuracy of 84.5%. The standard deviation of hand acceleration, standard deviation of heart rate, and the minimum heart rate were the most important features for stress detection. This evidence may support the efficacy of identifying patterns in physiological reaction to stress using smartwatch sensors and may inform the design of future tools for real-time detection of stress.
</details>
<details>
<summary>摘要</summary>
stress是多种大学生心理健康问题，包括抑郁和焦虑。早期识别和 intervención可能降低创建心理疾病的风险。我们使用机器学习算法来识别心理压力，使用自报告压力作为真实参照数据，以及Physiological数据，如心 rate和手势加速度。这项研究从大学校园中采集了54名学生，他们使用腕表仪和移动医疗应用程序，连续40天收集数据。应用程序记录了每分钟一次的心 rate和手势加速度数据，同时也让用户通过触摸腕表来报告压力，从而获得了时间戳的自报告压力记录。我们创建、评估和分析了机器学习算法，用于在大学生中识别压力发作。XGBoost方法是最可靠的模型，AUC为0.64，准确率为84.5%。手势加速度的标准差、心 rate的标准差和最低心 rate是最重要的压力检测特征。这些证据可能支持通过智能手表感知器检测压力的Pattern，并且可能导向未来的实时压力检测工具的设计。
</details></li>
</ul>
<hr>
<h2 id="Pointing-and-Acquisition-for-Optical-Wireless-in-6G-From-Algorithms-to-Performance-Evaluation"><a href="#Pointing-and-Acquisition-for-Optical-Wireless-in-6G-From-Algorithms-to-Performance-Evaluation" class="headerlink" title="Pointing-and-Acquisition for Optical Wireless in 6G: From Algorithms to Performance Evaluation"></a>Pointing-and-Acquisition for Optical Wireless in 6G: From Algorithms to Performance Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10999">http://arxiv.org/abs/2309.10999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyung-Joo Moon, Chan-Byoung Chae, Kai-Kit Wong, Mohamed-Slim Alouini</li>
<li>for: 该论文旨在探讨非地面网络的发展和自由空间光学通信技术的应用。</li>
<li>methods: 该论文使用了传统设备和机制，并提出了一种算法，通过抛物线链和反射器来实现角度估计和扫描。</li>
<li>results: 通过大量的 simulations，论文表明，提议的方法可以提供更好的链接和维护性能。<details>
<summary>Abstract</summary>
The increasing demand for wireless communication services has led to the development of non-terrestrial networks, which enables various air and space applications. Free-space optical (FSO) communication is considered one of the essential technologies capable of connecting terrestrial and non-terrestrial layers. In this article, we analyze considerations and challenges for FSO communications between gateways and aircraft from a pointing-and-acquisition perspective. Based on the analysis, we first develop a baseline method that utilizes conventional devices and mechanisms. Furthermore, we propose an algorithm that combines angle of arrival (AoA) estimation through supplementary radio frequency (RF) links and beam tracking using retroreflectors. Through extensive simulations, we demonstrate that the proposed method offers superior performance in terms of link acquisition and maintenance.
</details>
<details>
<summary>摘要</summary>
随着无线通信服务的增加需求，非地球网络的发展已经推动了各种空天应用。自由空间光学（FSO）通信被认为是连接地球和非地球层的重要技术之一。在本文中，我们分析了FSO通信 между网关和飞机从指向和捕获角度来考虑的考虑因素。根据分析结果，我们首先开发了使用普通设备和机制的基线方法。此外，我们提议一种方法，该方法利用补充Radio频信号的角度估计和反射器 beam Tracking。通过广泛的 simulations，我们证明了我们提议的方法可以提供更高的链接和维持链接性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/20/eess.SP_2023_09_20/" data-id="clpztdnuf01eues885qew9xc4" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_09_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/19/cs.SD_2023_09_19/" class="article-date">
  <time datetime="2023-09-19T15:00:00.000Z" itemprop="datePublished">2023-09-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/19/cs.SD_2023_09_19/">cs.SD - 2023-09-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Discrete-Audio-Representation-as-an-Alternative-to-Mel-Spectrograms-for-Speaker-and-Speech-Recognition"><a href="#Discrete-Audio-Representation-as-an-Alternative-to-Mel-Spectrograms-for-Speaker-and-Speech-Recognition" class="headerlink" title="Discrete Audio Representation as an Alternative to Mel-Spectrograms for Speaker and Speech Recognition"></a>Discrete Audio Representation as an Alternative to Mel-Spectrograms for Speaker and Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10922">http://arxiv.org/abs/2309.10922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Krishna C. Puvvada, Nithin Rao Koluguri, Kunal Dhawan, Jagadeesh Balam, Boris Ginsburg</li>
<li>for: 这篇论文的目的是评估压缩基于音频 represencing 的声音识别和 speaker 鉴定性能。</li>
<li>methods: 该论文使用了压缩基于 Residual Vector Quantization (RVQ) 的音频 Tokenization 方法，并对三个任务进行评估：Speaker Verification、Diarization 和多语言 Speech Recognition。</li>
<li>results: 研究发现，使用压缩基于音频 Tokenization 的模型在三个任务中的性能相对较强，与mel-spectrogram特征相对尚未超越，并且在不同的 speaker 和语言下表现稳定。此外，音频 Tokenization 可以实现数据压缩至 20 倍，而无需失去性能。<details>
<summary>Abstract</summary>
Discrete audio representation, aka audio tokenization, has seen renewed interest driven by its potential to facilitate the application of text language modeling approaches in audio domain. To this end, various compression and representation-learning based tokenization schemes have been proposed. However, there is limited investigation into the performance of compression-based audio tokens compared to well-established mel-spectrogram features across various speaker and speech related tasks. In this paper, we evaluate compression based audio tokens on three tasks: Speaker Verification, Diarization and (Multi-lingual) Speech Recognition. Our findings indicate that (i) the models trained on audio tokens perform competitively, on average within $1\%$ of mel-spectrogram features for all the tasks considered, and do not surpass them yet. (ii) these models exhibit robustness for out-of-domain narrowband data, particularly in speaker tasks. (iii) audio tokens allow for compression to 20x compared to mel-spectrogram features with minimal loss of performance in speech and speaker related tasks, which is crucial for low bit-rate applications, and (iv) the examined Residual Vector Quantization (RVQ) based audio tokenizer exhibits a low-pass frequency response characteristic, offering a plausible explanation for the observed results, and providing insight for future tokenizer designs.
</details>
<details>
<summary>摘要</summary>
简化音频表示，即音频tokenization，在最近受到了新的关注，因为它可以使得文本语言模型方法在音频领域应用。为此，各种压缩和表示学习基于的tokenization方案已经被提出。然而，关于压缩基于音频token的性能与著名的mel-spectrogram特征之间的比较，尚未有充分的研究。在这篇论文中，我们评估了基于压缩音频token的三个任务： speaker认证、分类和多语言语音识别。我们的发现是：（一）基于音频token训练的模型在所有考虑的任务中，在average上与mel-spectrogram特征相差不超过1%，并没有超过它们 yet。（二）这些模型在不同频谱数据上表现了Robustness，特别是在speaker任务中。（三）使用音频token可以将数据压缩到20倍，相比mel-spectrogram特征，减少了对speech和speaker相关任务的性能损失，这是低比特率应用中非常重要。（四）我们所考察的Residual Vector Quantization（RVQ）基于的音频tokenizer具有低通过滤波器特征，这提供了可能的解释，并为未来的tokenizer设计提供了意见。
</details></li>
</ul>
<hr>
<h2 id="USED-Universal-Speaker-Extraction-and-Diarization"><a href="#USED-Universal-Speaker-Extraction-and-Diarization" class="headerlink" title="USED: Universal Speaker Extraction and Diarization"></a>USED: Universal Speaker Extraction and Diarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10674">http://arxiv.org/abs/2309.10674</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyi Ao, Mehmet Sinan Yıldırım, Meng Ge, Shuai Wang, Ruijie Tao, Yanmin Qian, Liqun Deng, Longshuai Xiao, Haizhou Li</li>
<li>for: 这 paper 的目的是提出一个统一的框架，即 Universal Speaker Extraction and Diarization (USED)，用于同时抽取所有说话人的波形。</li>
<li>methods: 这 paper 使用了现有的说话人抽取模型，并在其基础上添加了一个场景意识 differentiated loss function，以解决实际对话中的叠 overlap 问题。</li>
<li>results: 根据 paper 的结果，USED 模型在高度重叠和叠 overlap 场景下都能够明显超过基eline，并且可以同时提供高质量的说话人抽取和分类结果。<details>
<summary>Abstract</summary>
Speaker extraction and diarization are two crucial enabling techniques for speech applications. Speaker extraction aims to extract a target speaker's voice from a multi-talk mixture, while speaker diarization demarcates speech segments by speaker, identifying `who spoke when'. The previous studies have typically treated the two tasks independently. However, the two tasks share a similar objective, that is to disentangle the speakers in the spectral domain for the former but in the temporal domain for the latter. It is logical to believe that the speaker turns obtained from speaker diarization can benefit speaker extraction, while the extracted speech offers more accurate speaker turns than the mixture speech. In this paper, we propose a unified framework called Universal Speaker Extraction and Diarization (USED). We extend the existing speaker extraction model to simultaneously extract the waveforms of all speakers. We also employ a scenario-aware differentiated loss function to address the problem of sparsely overlapped speech in real-world conversations. We show that the USED model significantly outperforms the baselines for both speaker extraction and diarization tasks, in both highly overlapped and sparsely overlapped scenarios. Audio samples are available at https://ajyy.github.io/demo/USED/.
</details>
<details>
<summary>摘要</summary>
干支持和分类是语音应用程序中的两个关键技能。干支持目标是从多话者混合中提取目标说话人的声音，而分类则将speech分成不同的speaker，并识别“谁在什么时候说话”。过去的研究通常会独立地处理这两个任务。然而，这两个任务在目标上具有相似的目标，即在spectral domain中分离说话人的声音，但在时间频谱中则是识别speaker。逻辑地来说，来自分类的speaker turn可以帮助提取说话人的声音，而提取的speech也比混合 speech更准确地识别speaker。在这篇论文中，我们提出了一个统一框架，称为Universal Speaker Extraction and Diarization（USED）。我们将现有的说话人提取模型扩展到同时提取所有说话人的波形。我们还使用场景意识化的差分损失函数来解决实际对话中稀疏的 overlap speech问题。我们展示了USED模型在speaker extraction和分类任务上明显超过基eline，并在高度重叠和稀疏重叠的场景中都有优异表现。Audio示例可以在https://ajyy.github.io/demo/USED/中找到。
</details></li>
</ul>
<hr>
<h2 id="An-Active-Noise-Control-System-Based-on-Soundfield-Interpolation-Using-a-Physics-informed-Neural-Network"><a href="#An-Active-Noise-Control-System-Based-on-Soundfield-Interpolation-Using-a-Physics-informed-Neural-Network" class="headerlink" title="An Active Noise Control System Based on Soundfield Interpolation Using a Physics-informed Neural Network"></a>An Active Noise Control System Based on Soundfield Interpolation Using a Physics-informed Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10605">http://arxiv.org/abs/2309.10605</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yile, Zhang, Fei Ma, Thushara Abhayapala, Prasanga Samarasinghe, Amy Bastine</li>
<li>for: 降低ROI内噪音</li>
<li>methods: 使用physics-informed neural network (PINN) interpolate soundfield from monitoring microphones placed outside ROI, 比较SPHERical harmonic method在限制数量监测微phone情况下的 interpolate表现</li>
<li>results: PINN-assisted ANC系统在模拟中比多点ANC系统降低ROI内噪音更好<details>
<summary>Abstract</summary>
Conventional multiple-point active noise control (ANC) systems require placing error microphones within the region of interest (ROI), inconveniencing users. This paper designs a feasible monitoring microphone arrangement placed outside the ROI, providing a user with more freedom of movement. The soundfield within the ROI is interpolated from the microphone signals using a physics-informed neural network (PINN). PINN exploits the acoustic wave equation to assist soundfield interpolation under a limited number of monitoring microphones, and demonstrates better interpolation performance than the spherical harmonic method in simulations. An ANC system is designed to take advantage of the interpolated signal to reduce noise signal within the ROI. The PINN-assisted ANC system reduces noise more than that of the multiple-point ANC system in simulations.
</details>
<details>
<summary>摘要</summary>
传统的多点活动噪声控制（ANC）系统需要在Region of Interest（ROI）中放置错误微phone，对用户造成不便。本文提出了一种可行的监测icrophone布局，位于ROI外部，提供用户更多的自由运动空间。使用物理学信息学习网络（PINN） interpolate the soundfield within the ROI from the microphone signals, PINN leverages the acoustic wave equation to assist soundfield interpolation under a limited number of monitoring microphones, and shows better interpolation performance than the spherical harmonic method in simulations. ANC system is designed to take advantage of the interpolated signal to reduce noise within the ROI. PINN-assisted ANC system reduces noise more than the multiple-point ANC system in simulations.Note: "Region of Interest" (ROI) is translated as " Region of Interest" (ROI) in Simplified Chinese, which is the same as the original English text.
</details></li>
</ul>
<hr>
<h2 id="Bridging-the-Spoof-Gap-A-Unified-Parallel-Aggregation-Network-for-Voice-Presentation-Attacks"><a href="#Bridging-the-Spoof-Gap-A-Unified-Parallel-Aggregation-Network-for-Voice-Presentation-Attacks" class="headerlink" title="Bridging the Spoof Gap: A Unified Parallel Aggregation Network for Voice Presentation Attacks"></a>Bridging the Spoof Gap: A Unified Parallel Aggregation Network for Voice Presentation Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10560">http://arxiv.org/abs/2309.10560</a></li>
<li>repo_url: None</li>
<li>paper_authors: Awais Khan, Khalid Mahmood Malik</li>
<li>for: 强化voice bio-metrics用户身份验证系统的安全性，因为ASV系统面临逻辑和物理冒充攻击的Security Risks。</li>
<li>methods: 提出了一种Parallel Stacked Aggregation Network，该方法处理原始音频，采用Split-Transform-Aggregation技术，将词语分解成卷积表示，应用变换，并聚合结果，以识别逻辑冒充(LA)和物理冒充(PA)攻击。</li>
<li>results: 对ASVspoof-2019和VSDC数据集进行评估，显示了提案的系统的有效性，与现有解决方案相比，具有更低的EER差异和更高的检测冒充攻击的能力。这表明提案的方法具有普适性和优势。在voice-based安全系统中，提出的一元化冒充检测系统为ASV和用户数据提供了一种可靠的防御机制，帮助保护用户身份和数据。<details>
<summary>Abstract</summary>
Automatic Speaker Verification (ASV) systems are increasingly used in voice bio-metrics for user authentication but are susceptible to logical and physical spoofing attacks, posing security risks. Existing research mainly tackles logical or physical attacks separately, leading to a gap in unified spoofing detection. Moreover, when existing systems attempt to handle both types of attacks, they often exhibit significant disparities in the Equal Error Rate (EER). To bridge this gap, we present a Parallel Stacked Aggregation Network that processes raw audio. Our approach employs a split-transform-aggregation technique, dividing utterances into convolved representations, applying transformations, and aggregating the results to identify logical (LA) and physical (PA) spoofing attacks. Evaluation of the ASVspoof-2019 and VSDC datasets shows the effectiveness of the proposed system. It outperforms state-of-the-art solutions, displaying reduced EER disparities and superior performance in detecting spoofing attacks. This highlights the proposed method's generalizability and superiority. In a world increasingly reliant on voice-based security, our unified spoofing detection system provides a robust defense against a spectrum of voice spoofing attacks, safeguarding ASVs and user data effectively.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="FoleyGen-Visually-Guided-Audio-Generation"><a href="#FoleyGen-Visually-Guided-Audio-Generation" class="headerlink" title="FoleyGen: Visually-Guided Audio Generation"></a>FoleyGen: Visually-Guided Audio Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10537">http://arxiv.org/abs/2309.10537</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinhao Mei, Varun Nagaraja, Gael Le Lan, Zhaoheng Ni, Ernie Chang, Yangyang Shi, Vikas Chandra</li>
<li>for: 这项研究的目的是提出一种基于语言模型的开放频谱视频到音频（V2A）生成系统，以便将视频转换为声音。</li>
<li>methods: 该系统使用一个单向 transformer 模型，通过 Conditional Random Field 进行视觉特征提取，并使用一个 off-the-shelf 神经音频编码器进行双向转换。</li>
<li>results: 实验结果表明，提出的 FoleyGen 系统在 VGGSound 数据集上以对象指标和人类评估方面都超过了先前系统。<details>
<summary>Abstract</summary>
Recent advancements in audio generation have been spurred by the evolution of large-scale deep learning models and expansive datasets. However, the task of video-to-audio (V2A) generation continues to be a challenge, principally because of the intricate relationship between the high-dimensional visual and auditory data, and the challenges associated with temporal synchronization. In this study, we introduce FoleyGen, an open-domain V2A generation system built on a language modeling paradigm. FoleyGen leverages an off-the-shelf neural audio codec for bidirectional conversion between waveforms and discrete tokens. The generation of audio tokens is facilitated by a single Transformer model, which is conditioned on visual features extracted from a visual encoder. A prevalent problem in V2A generation is the misalignment of generated audio with the visible actions in the video. To address this, we explore three novel visual attention mechanisms. We further undertake an exhaustive evaluation of multiple visual encoders, each pretrained on either single-modal or multi-modal tasks. The experimental results on VGGSound dataset show that our proposed FoleyGen outperforms previous systems across all objective metrics and human evaluations.
</details>
<details>
<summary>摘要</summary>
(Note: The text has been translated into Simplified Chinese, but please note that the translation may not be perfect and may not capture all the nuances of the original text.)
</details></li>
</ul>
<hr>
<h2 id="Incorporating-Ultrasound-Tongue-Images-for-Audio-Visual-Speech-Enhancement"><a href="#Incorporating-Ultrasound-Tongue-Images-for-Audio-Visual-Speech-Enhancement" class="headerlink" title="Incorporating Ultrasound Tongue Images for Audio-Visual Speech Enhancement"></a>Incorporating Ultrasound Tongue Images for Audio-Visual Speech Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10455">http://arxiv.org/abs/2309.10455</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ZhengRachel/UTIforAVSE-demo">https://github.com/ZhengRachel/UTIforAVSE-demo</a></li>
<li>paper_authors: Rui-Chen Zheng, Yang Ai, Zhen-Hua Ling</li>
<li>for: 提高受损的语音质量和可识别度，同时利用舌头影像信息进行干扰音频影像干扰</li>
<li>methods: 使用知识塑化 durante el entrenamiento para investigar可以利用舌头相关信息而不需要直接输入ultrasound舌头影像，并引入一个lip-tongue键值记忆网络以模型舌头和唇Modalities的对齐。</li>
<li>results: 实验结果表明，两种提议方法可以significantly提高受损语音的质量和可识别度，并且具有强大的通用性能在未看到的speaker和噪声下。此外，通过自动语音识别（ASR）的phone error rate（PER）分析发现，所有音频都从 introducing ultrasound舌头影像中受益，而palatal和velar元音声最大受益。<details>
<summary>Abstract</summary>
Audio-visual speech enhancement (AV-SE) aims to enhance degraded speech along with extra visual information such as lip videos, and has been shown to be more effective than audio-only speech enhancement. This paper proposes the incorporation of ultrasound tongue images to improve the performance of lip-based AV-SE systems further. To address the challenge of acquiring ultrasound tongue images during inference, we first propose to employ knowledge distillation during training to investigate the feasibility of leveraging tongue-related information without directly inputting ultrasound tongue images. Specifically, we guide an audio-lip speech enhancement student model to learn from a pre-trained audio-lip-tongue speech enhancement teacher model, thus transferring tongue-related knowledge. To better model the alignment between the lip and tongue modalities, we further propose the introduction of a lip-tongue key-value memory network into the AV-SE model. This network enables the retrieval of tongue features based on readily available lip features, thereby assisting the subsequent speech enhancement task. Experimental results demonstrate that both methods significantly improve the quality and intelligibility of the enhanced speech compared to traditional lip-based AV-SE baselines. Moreover, both proposed methods exhibit strong generalization performance on unseen speakers and in the presence of unseen noises. Furthermore, phone error rate (PER) analysis of automatic speech recognition (ASR) reveals that while all phonemes benefit from introducing ultrasound tongue images, palatal and velar consonants benefit most.
</details>
<details>
<summary>摘要</summary>
音视频 speech 增强 (AV-SE) 目标是提高受损的语音，同时使用附加的视觉信息，如舌头视频，并被证明高于听音只 speech 增强。这篇论文提议将超声舌头像 incorporated 到 lip-based AV-SE 系统以提高性能。为了解决在推理中获取超声舌头像的挑战，我们首先提议使用知识塑化在训练期间。特别是，我们引导一个 audio-lip speech 增强学生模型学习一个预训练的 audio-lip-tongue speech 增强老师模型，从而传递舌头相关的知识。为了更好地模型舌头和 lip 模态的匹配，我们进一步提议引入一个 lip-tongue 关键值记忆网络到 AV-SE 模型中。这个网络允许根据 readily available 的 lip 特征来检索舌头特征，以帮助后续的语音增强任务。实验结果表明，两种方法都能有效地提高增强后的语音质量和可读性，并且两种方法在不seen  speaker 和不seen 噪音下具有强大的泛化性能。此外，基于自动语音识别 (ASR) 的 phone error rate (PER) 分析表明，将超声舌头像 incorporated 后，所有的音位都受益，但是 palatal 和 velar 元音最多受益。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Multi-Channel-Speech-Enhancement-with-Spherical-Harmonics-Injection-for-Directional-Encoding"><a href="#Efficient-Multi-Channel-Speech-Enhancement-with-Spherical-Harmonics-Injection-for-Directional-Encoding" class="headerlink" title="Efficient Multi-Channel Speech Enhancement with Spherical Harmonics Injection for Directional Encoding"></a>Efficient Multi-Channel Speech Enhancement with Spherical Harmonics Injection for Directional Encoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10832">http://arxiv.org/abs/2309.10832</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiahui Pan, Pengjie Shen, Hui Zhang, Xueliang Zhang</li>
<li>for: 提高多通道Speech干扰的效果，使用多个麦克风捕捉空间信息。</li>
<li>methods: 使用圆形傅立叶变换(SHT)矩阵作为助记模型输入，以便更好地利用空间分布。</li>
<li>results: 在TIMIT dataset上，模型对不同噪声和反射的情况下表现出色，超过了已有的标准。此外，该模型具有较少的计算量和参数数量。<details>
<summary>Abstract</summary>
Multi-channel speech enhancement extracts speech using multiple microphones that capture spatial cues. Effectively utilizing directional information is key for multi-channel enhancement. Deep learning shows great potential on multi-channel speech enhancement and often takes short-time Fourier Transform (STFT) as inputs directly. To fully leverage the spatial information, we introduce a method using spherical harmonics transform (SHT) coefficients as auxiliary model inputs. These coefficients concisely represent spatial distributions. Specifically, our model has two encoders, one for the STFT and another for the SHT. By fusing both encoders in the decoder to estimate the enhanced STFT, we effectively incorporate spatial context. Evaluations on TIMIT under varying noise and reverberation show our model outperforms established benchmarks. Remarkably, this is achieved with fewer computations and parameters. By leveraging spherical harmonics to incorporate directional cues, our model efficiently improves the performance of the multi-channel speech enhancement.
</details>
<details>
<summary>摘要</summary>
多通道语音提升使用多个麦克风捕捉空间信息，以提高语音提升的效果。深度学习在多通道语音提升中表现出了极大的潜力，通常直接使用短时傅立叙 transform（STFT）作为输入。为了充分利用空间信息，我们介绍了一种使用球面幂变换（SHT）系数作为辅助模型输入的方法。这些系数简洁地表示空间分布。具体来说，我们的模型有两个编码器，一个是 для STFT，另一个是 для SHT。在解码器中将两个编码器融合以估计提升后的 STFT，从而有效地 incorporate 空间上下文。在 TIMIT 上进行了不同噪音和频率反射的评估，我们的模型表现出了较好的性能，并且只需要更少的计算和参数。通过利用球面幂变换来包含方向信息，我们的模型高效地提高了多通道语音提升的性能。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Modeling-of-Spatial-Cues-via-Spherical-Harmonics-for-Multi-Channel-Speech-Enhancement"><a href="#Hierarchical-Modeling-of-Spatial-Cues-via-Spherical-Harmonics-for-Multi-Channel-Speech-Enhancement" class="headerlink" title="Hierarchical Modeling of Spatial Cues via Spherical Harmonics for Multi-Channel Speech Enhancement"></a>Hierarchical Modeling of Spatial Cues via Spherical Harmonics for Multi-Channel Speech Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10393">http://arxiv.org/abs/2309.10393</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiahui Pan, Shulin He, Hui Zhang, Xueliang Zhang</li>
<li>for: 提高多渠道语音干扰除的性能，使用多渠道信号中的空间信息更好地提取目标语音。</li>
<li>methods: 提议使用球面傅立卷变换（SHT）来显式地模型空间信息，采用层次结构，先估计更高频率的空间模式，然后与更低频率的空间模式共同预测更细致的空间细节。</li>
<li>results: 在TIMIT数据集上，提议方法可以更好地回归目标空间模式，并且比基eline模型提高性能，使用更少的参数和计算。<details>
<summary>Abstract</summary>
Multi-channel speech enhancement utilizes spatial information from multiple microphones to extract the target speech. However, most existing methods do not explicitly model spatial cues, instead relying on implicit learning from multi-channel spectra. To better leverage spatial information, we propose explicitly incorporating spatial modeling by applying spherical harmonic transforms (SHT) to the multi-channel input. In detail, a hierarchical framework is introduced whereby lower order harmonics capturing broader spatial patterns are estimated first, then combined with higher orders to recursively predict finer spatial details. Experiments on TIMIT demonstrate the proposed method can effectively recover target spatial patterns and achieve improved performance over baseline models, using fewer parameters and computations. Explicitly modeling spatial information hierarchically enables more effective multi-channel speech enhancement.
</details>
<details>
<summary>摘要</summary>
多通道语音增强利用多个麦克风的空间信息提取目标语音。然而，大多数现有方法不直接模型空间指示，而是通过多通道 спектrum 的含义学习来隐式地利用空间信息。为更好地利用空间信息，我们提议直接将多通道输入应用到圆形傅里叶变换（SHT）中，以实现更好的多通道语音增强。在详细的实现中，我们引入一种层次结构，其中低顺位傅里叶capture更广泛的空间模式，然后与更高顺位傅里叶相结合，以递归地预测更细致的空间细节。在 TIMIT 上进行实验，我们发现提议的方法可以更好地回归目标空间模式，并在基准模型上达到更高的性能，使用更少的参数和计算。通过直接模型空间信息层次结构，我们可以更有效地进行多通道语音增强。
</details></li>
</ul>
<hr>
<h2 id="PDPCRN-Parallel-Dual-Path-CRN-with-Bi-directional-Inter-Branch-Interactions-for-Multi-Channel-Speech-Enhancement"><a href="#PDPCRN-Parallel-Dual-Path-CRN-with-Bi-directional-Inter-Branch-Interactions-for-Multi-Channel-Speech-Enhancement" class="headerlink" title="PDPCRN: Parallel Dual-Path CRN with Bi-directional Inter-Branch Interactions for Multi-Channel Speech Enhancement"></a>PDPCRN: Parallel Dual-Path CRN with Bi-directional Inter-Branch Interactions for Multi-Channel Speech Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10379">http://arxiv.org/abs/2309.10379</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiahui Pan, Shulin He, Tianci Wu, Hui Zhang, Xueliang Zhang</li>
<li>for: 提高多渠道语音干扰消除的精度</li>
<li>methods: 提出了并行双路卷积循环神经网络（PDPCRN）模型，包括两个关键创新：分立分支EXTRACT complementary特征，以及bi-directional模块实现交叉通信</li>
<li>results: 对TIMIT数据集进行实验 validate，PDPCRN模型不仅在PESQ和STOI指标中表现出色，而且还具有较少的计算负担和参数量。<details>
<summary>Abstract</summary>
Multi-channel speech enhancement seeks to utilize spatial information to distinguish target speech from interfering signals. While deep learning approaches like the dual-path convolutional recurrent network (DPCRN) have made strides, challenges persist in effectively modeling inter-channel correlations and amalgamating multi-level information. In response, we introduce the Parallel Dual-Path Convolutional Recurrent Network (PDPCRN). This acoustic modeling architecture has two key innovations. First, a parallel design with separate branches extracts complementary features. Second, bi-directional modules enable cross-branch communication. Together, these facilitate diverse representation fusion and enhanced modeling. Experimental validation on TIMIT datasets underscores the prowess of PDPCRN. Notably, against baseline models like the standard DPCRN, PDPCRN not only outperforms in PESQ and STOI metrics but also boasts a leaner computational footprint with reduced parameters.
</details>
<details>
<summary>摘要</summary>
多通道语音增强 seek to 利用空间信息来 distinguishing 目标语音与干扰信号。而深度学习方法如双路卷积回归网络（DPCRN）已经做出了 significiant progress, 但还存在效果模型交通信道和多级信息融合的挑战。为此，我们提出了并行双路卷积回归网络（PDPCRN）。这种语音模型建立有两个关键创新：首先，并行设计分配了 complementary 特征。其次，bi-directional模块允许交叉通信。这两个特征共同使得多元表示融合和模型提高。对于 TIMIT 数据集的实验验证，PDPCRN 表现出众，与基准模型如标准 DPCRN 不仅在 PESQ 和 STOI 指标上表现出优异，还具有更小的计算承载和减少参数量。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/19/cs.SD_2023_09_19/" data-id="clpztdno200zqes882wnibzko" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_09_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/19/eess.AS_2023_09_19/" class="article-date">
  <time datetime="2023-09-19T14:00:00.000Z" itemprop="datePublished">2023-09-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/19/eess.AS_2023_09_19/">eess.AS - 2023-09-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Exploring-Speech-Enhancement-for-Low-resource-Speech-Synthesis"><a href="#Exploring-Speech-Enhancement-for-Low-resource-Speech-Synthesis" class="headerlink" title="Exploring Speech Enhancement for Low-resource Speech Synthesis"></a>Exploring Speech Enhancement for Low-resource Speech Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10795">http://arxiv.org/abs/2309.10795</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaoheng Ni, Sravya Popuri, Ning Dong, Kohei Saijo, Xiaohui Zhang, Gael Le Lan, Yangyang Shi, Vikas Chandra, Changhan Wang</li>
<li>for: 提高低资源语言 Text-to-Speech (TTS) 模型训练的高质量语音数据获取具有挑战性和成本高。</li>
<li>methods: 应用自动语音识别 (ASR)  corpora 上的语音增强模型，以增强训练数据，并对低资源语言 TTS 系统进行训练。</li>
<li>results: 使用阿拉伯语 datasets 作为例子，我们显示了我们的管道比基eline方法在 ASR WER 指标上具有显著改进，并进行了实验分析语音增强和 TTS 性能之间的相关性。<details>
<summary>Abstract</summary>
High-quality and intelligible speech is essential to text-to-speech (TTS) model training, however, obtaining high-quality data for low-resource languages is challenging and expensive. Applying speech enhancement on Automatic Speech Recognition (ASR) corpus mitigates the issue by augmenting the training data, while how the nonlinear speech distortion brought by speech enhancement models affects TTS training still needs to be investigated. In this paper, we train a TF-GridNet speech enhancement model and apply it to low-resource datasets that were collected for the ASR task, then train a discrete unit based TTS model on the enhanced speech. We use Arabic datasets as an example and show that the proposed pipeline significantly improves the low-resource TTS system compared with other baseline methods in terms of ASR WER metric. We also run empirical analysis on the correlation between speech enhancement and TTS performances.
</details>
<details>
<summary>摘要</summary>
高质量和智能可理解的语音是文本到语音（TTS）模型训练的必要条件，但是获取低资源语言的高质量数据是具有挑战和成本的。将自动语音识别（ASR）集合中的非线性语音扭曲应用于TTS训练数据可以缓解这个问题，但是如何评估非线性语音扭曲对TTS训练的影响仍需要进一步调查。在本文中，我们训练了TF-GridNet语音增强模型，并将其应用于低资源 dataset，然后训练基于分立单元的 TTS 模型。我们使用阿拉伯语 dataset 作为例子，并证明了我们的管道可以对低资源 TTS 系统进行显著改进，比基eline方法在 ASR WER 指标上具有更高的性能。我们还运行了实验分析语音增强和 TTS 性能之间的相关性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/19/eess.AS_2023_09_19/" data-id="clpztdnq3014oes88cm543atn" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_09_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/19/cs.CV_2023_09_19/" class="article-date">
  <time datetime="2023-09-19T13:00:00.000Z" itemprop="datePublished">2023-09-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/19/cs.CV_2023_09_19/">cs.CV - 2023-09-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Novel-Deep-Neural-Network-for-Trajectory-Prediction-in-Automated-Vehicles-Using-Velocity-Vector-Field"><a href="#A-Novel-Deep-Neural-Network-for-Trajectory-Prediction-in-Automated-Vehicles-Using-Velocity-Vector-Field" class="headerlink" title="A Novel Deep Neural Network for Trajectory Prediction in Automated Vehicles Using Velocity Vector Field"></a>A Novel Deep Neural Network for Trajectory Prediction in Automated Vehicles Using Velocity Vector Field</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10948">http://arxiv.org/abs/2309.10948</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Amir-Samadi/VVF-TP">https://github.com/Amir-Samadi/VVF-TP</a></li>
<li>paper_authors: MReza Alipour Sormoli, Amir Samadi, Sajjad Mozaffari, Konstantinos Koufos, Mehrdad Dianati, Roger Woodman</li>
<li>for: 预测其他道路用户的运动方向和速度，以帮助自动驾驶系统（ADS）进行安全和知悉的动作规划和决策。</li>
<li>methods: 融合了学习基于数据的方法和流体动力学发现的速度场（VVF），将其作为深度神经网的一个额外输入，以预测基于鸟瞰图片表示的动线。</li>
<li>results: 与现有方法进行比较，提出了一种新的动线预测技术，可以对5~50秒的预测时间 horizon和不同的观察窗口进行优化，并且降低了需要过去的观察历史以获得高精度的动线预测。<details>
<summary>Abstract</summary>
Anticipating the motion of other road users is crucial for automated driving systems (ADS), as it enables safe and informed downstream decision-making and motion planning. Unfortunately, contemporary learning-based approaches for motion prediction exhibit significant performance degradation as the prediction horizon increases or the observation window decreases. This paper proposes a novel technique for trajectory prediction that combines a data-driven learning-based method with a velocity vector field (VVF) generated from a nature-inspired concept, i.e., fluid flow dynamics. In this work, the vector field is incorporated as an additional input to a convolutional-recurrent deep neural network to help predict the most likely future trajectories given a sequence of bird's eye view scene representations. The performance of the proposed model is compared with state-of-the-art methods on the HighD dataset demonstrating that the VVF inclusion improves the prediction accuracy for both short and long-term (5~sec) time horizons. It is also shown that the accuracy remains consistent with decreasing observation windows which alleviates the requirement of a long history of past observations for accurate trajectory prediction. Source codes are available at: https://github.com/Amir-Samadi/VVF-TP.
</details>
<details>
<summary>摘要</summary>
预测道路用户的运动是自动驾驶系统（ADS）中的关键，它允许系统进行安全和有知情的下游决策和运动规划。然而，现代学习基于方法的运动预测表现会随预测时间 horizon 的增加或 observation window 的减少而显著下降。这篇文章提出了一种新的轨迹预测技术，它将数据驱动学习基于方法和流体流动动力学（VVF）相结合，以便基于 bird's eye view 场景表示序列 Predict the most likely future trajectories.在这种方法中，VVF 被包含为循环神经网络的一个额外输入，以帮助预测未来的轨迹。文章比较了提出的模型与现有方法在 HighD 数据集上的性能，并证明了 VVF 包含可以提高预测精度，并且预测精度随 observation window 的减少而保持一致。代码可以在 GitHub 上找到：https://github.com/Amir-Samadi/VVF-TP。
</details></li>
</ul>
<hr>
<h2 id="A-Geometric-Flow-Approach-for-Segmentation-of-Images-with-Inhomongeneous-Intensity-and-Missing-Boundaries"><a href="#A-Geometric-Flow-Approach-for-Segmentation-of-Images-with-Inhomongeneous-Intensity-and-Missing-Boundaries" class="headerlink" title="A Geometric Flow Approach for Segmentation of Images with Inhomongeneous Intensity and Missing Boundaries"></a>A Geometric Flow Approach for Segmentation of Images with Inhomongeneous Intensity and Missing Boundaries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10935">http://arxiv.org/abs/2309.10935</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paramjyoti Mohapatra, Richard Lartey, Weihong Guo, Michael Judkovich, Xiaojuan Li</li>
<li>for: 这篇论文主要针对的是如何使用新的INTENSITY correction和自动化拓扑方法来进行Muscle segmentation，尤其是在MR图像中，图像具有强烈的灰度不均和缺失边界问题。</li>
<li>methods: 该方法使用了一种Geometric flow，该流体现了一个RKHS edge detector和一个geodesic distance penalty term，这些 penalty term来自 marker和anti-marker的集合。此外， paper还提出了一种新的bias field估计方法，即Prior Bias-Corrected Fuzzy C-means (PBCFCM)，以帮助处理MR图像中的灰度不均问题。</li>
<li>results: 数字实验表明，提出的方法与比较方法相比，具有显著的改善，其 dice值为92.5%, 85.3%, 85.3% для quadriceps、hamstrings 和其他肌群，而其他方法至少下降10%。<details>
<summary>Abstract</summary>
Image segmentation is a complex mathematical problem, especially for images that contain intensity inhomogeneity and tightly packed objects with missing boundaries in between. For instance, Magnetic Resonance (MR) muscle images often contain both of these issues, making muscle segmentation especially difficult. In this paper we propose a novel intensity correction and a semi-automatic active contour based segmentation approach. The approach uses a geometric flow that incorporates a reproducing kernel Hilbert space (RKHS) edge detector and a geodesic distance penalty term from a set of markers and anti-markers. We test the proposed scheme on MR muscle segmentation and compare with some state of the art methods. To help deal with the intensity inhomogeneity in this particular kind of image, a new approach to estimate the bias field using a fat fraction image, called Prior Bias-Corrected Fuzzy C-means (PBCFCM), is introduced. Numerical experiments show that the proposed scheme leads to significantly better results than compared ones. The average dice values of the proposed method are 92.5%, 85.3%, 85.3% for quadriceps, hamstrings and other muscle groups while other approaches are at least 10% worse.
</details>
<details>
<summary>摘要</summary>
Image segmentation是一个复杂的数学问题，特别是当图像具有强度不均和紧邻的物体间缺失边界时。例如，核磁共振（MR）肌肉图像经常具有这两种问题，从而使肌肉分割特别困难。在这篇论文中，我们提出了一种新的强度修正和半自动的活动梁基分割方法。该方法使用了一种几何流，其包括一个 reproduce kernel Hilbert space（RKHS）边检测器和一个地odesic距离罚款项。我们对MR肌肉分割进行测试，并与一些现有方法进行比较。为了帮助处理MR肌肉图像中的强度不均，我们还提出了一种新的方法来估计偏置场，即 Prior Bias-Corrected Fuzzy C-means（PBCFCM）。数字实验表明，我们提出的方法与其他方法相比，得到了显著更好的结果。提出的方法的平均 dice 值为 92.5%、85.3% 和 85.3% 分别，而其他方法至少下降了10%。
</details></li>
</ul>
<hr>
<h2 id="Incremental-Multimodal-Surface-Mapping-via-Self-Organizing-Gaussian-Mixture-Models"><a href="#Incremental-Multimodal-Surface-Mapping-via-Self-Organizing-Gaussian-Mixture-Models" class="headerlink" title="Incremental Multimodal Surface Mapping via Self-Organizing Gaussian Mixture Models"></a>Incremental Multimodal Surface Mapping via Self-Organizing Gaussian Mixture Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10900">http://arxiv.org/abs/2309.10900</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hieu9955/ggggg">https://github.com/hieu9955/ggggg</a></li>
<li>paper_authors: Kshitij Goel, Wennie Tabib</li>
<li>for: 这个论文旨在提出一种逐步多模态表面映射方法，用于高精度重建环境，同时压缩空间和强度点云数据。</li>
<li>methods: 该方法使用 Gaussian mixture models (GMMs) 表示环境，并提出了一种快速EXTRACT GMM submap的方法，以及一种判断点云中重复和无关数据的方法，从而提高计算速度。</li>
<li>results: 论文的实验结果显示，该方法可以提供高精度的地图，同时压缩空间和强度点云数据，并且与现有的地图方法相比，具有更好的质量和大小协调。<details>
<summary>Abstract</summary>
This letter describes an incremental multimodal surface mapping methodology, which represents the environment as a continuous probabilistic model. This model enables high-resolution reconstruction while simultaneously compressing spatial and intensity point cloud data. The strategy employed in this work utilizes Gaussian mixture models (GMMs) to represent the environment. While prior GMM-based mapping works have developed methodologies to determine the number of mixture components using information-theoretic techniques, these approaches either operate on individual sensor observations, making them unsuitable for incremental mapping, or are not real-time viable, especially for applications where high-fidelity modeling is required. To bridge this gap, this letter introduces a spatial hash map for rapid GMM submap extraction combined with an approach to determine relevant and redundant data in a point cloud. These contributions increase computational speed by an order of magnitude compared to state-of-the-art incremental GMM-based mapping. In addition, the proposed approach yields a superior tradeoff in map accuracy and size when compared to state-of-the-art mapping methodologies (both GMM- and not GMM-based). Evaluations are conducted using both simulated and real-world data. The software is released open-source to benefit the robotics community.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这封信件描述了一种逐步多模态表面映射方法，该方法将环境表示为一个连续的概率模型。这个模型可以同时实现高分辨率重建和压缩空间和强度点云数据。这种策略使用 Gaussian mixture models (GMMs) 来表示环境。先前的 GMM 基于的映射工作已经开发了基于信息理论技术来确定混合组件的方法，但这些方法 Either operate on individual sensor observations, making them unsuitable for incremental mapping, or are not real-time viable, especially for applications where high-fidelity modeling is required. To address these limitations, this letter introduces a spatial hash map for rapid GMM submap extraction combined with an approach to determine relevant and redundant data in a point cloud. These contributions increase computational speed by an order of magnitude compared to state-of-the-art incremental GMM-based mapping. In addition, the proposed approach yields a superior tradeoff in map accuracy and size when compared to state-of-the-art mapping methodologies (both GMM- and not GMM-based). Evaluations are conducted using both simulated and real-world data, and the software is released open-source to benefit the robotics community.
</details></li>
</ul>
<hr>
<h2 id="PLVS-A-SLAM-System-with-Points-Lines-Volumetric-Mapping-and-3D-Incremental-Segmentation"><a href="#PLVS-A-SLAM-System-with-Points-Lines-Volumetric-Mapping-and-3D-Incremental-Segmentation" class="headerlink" title="PLVS: A SLAM System with Points, Lines, Volumetric Mapping, and 3D Incremental Segmentation"></a>PLVS: A SLAM System with Points, Lines, Volumetric Mapping, and 3D Incremental Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10896">http://arxiv.org/abs/2309.10896</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luigifreda/plvs">https://github.com/luigifreda/plvs</a></li>
<li>paper_authors: Luigi Freda</li>
<li>for: 这篇论文旨在提出一个实时系统，具有稀疏SLAM、Volume Mapping和3D无监督增量分割功能。</li>
<li>methods: 论文使用了 sparse points和Line segments为特征，通过基于锚帧的锚点扫描和Volume Mapping来生成3D环境重建。</li>
<li>results: 论文通过公共数据集的质量和量计算评估了PLVS框架的性能，并提供了一种incremental和几何基于的RGB-D摄像头分割方法。<details>
<summary>Abstract</summary>
This document presents PLVS: a real-time system that leverages sparse SLAM, volumetric mapping, and 3D unsupervised incremental segmentation. PLVS stands for Points, Lines, Volumetric mapping, and Segmentation. It supports RGB-D and Stereo cameras, which may be optionally equipped with IMUs. The SLAM module is keyframe-based, and extracts and tracks sparse points and line segments as features. Volumetric mapping runs in parallel with respect to the SLAM front-end and generates a 3D reconstruction of the explored environment by fusing point clouds backprojected from keyframes. Different volumetric mapping methods are supported and integrated in PLVS. We use a novel reprojection error to bundle-adjust line segments. This error exploits available depth information to stabilize the position estimates of line segment endpoints. An incremental and geometric-based segmentation method is implemented and integrated for RGB-D cameras in the PLVS framework. We present qualitative and quantitative evaluations of the PLVS framework on some publicly available datasets. The appendix details the adopted stereo line triangulation method and provides a derivation of the Jacobians we used for line error terms. The software is available as open-source.
</details>
<details>
<summary>摘要</summary>
Here is the translation in Simplified Chinese:这份文档介绍了PLVS：一个实时系统，它利用稀疏SLAM、三维映射和RGB-D和雷达相机的3D无监督增量分割。PLVS表示点、线、三维映射和分割。它支持RGB-D和雷达相机，这些相机可选择配备IMUs。SLAM模块是基干帧基的，它提取和跟踪稀疏点和线段为特征。三维映射在SLAM前端并行执行，通过将点云反 projekt 到关键帧来生成探索环境的3D重建。不同的三维映射方法被支持和集成在PLVS框架中。我们使用了一种新的 reprojection 错误来稳定线段endpoint的位置估计，这个错误利用可用的深度信息来稳定位置估计。PLVS框架还实现了RGB-D相机上的增量和几何基于的分割方法。我们对PLVS框架在一些公共可用的数据集上进行质量和量化评估。附录详细介绍了我们采用的雷达线段三角法和line error Jacobians的derivation。PLVS框架的软件可以在开源的形式下获取。
</details></li>
</ul>
<hr>
<h2 id="GelSight-Svelte-A-Human-Finger-shaped-Single-camera-Tactile-Robot-Finger-with-Large-Sensing-Coverage-and-Proprioceptive-Sensing"><a href="#GelSight-Svelte-A-Human-Finger-shaped-Single-camera-Tactile-Robot-Finger-with-Large-Sensing-Coverage-and-Proprioceptive-Sensing" class="headerlink" title="GelSight Svelte: A Human Finger-shaped Single-camera Tactile Robot Finger with Large Sensing Coverage and Proprioceptive Sensing"></a>GelSight Svelte: A Human Finger-shaped Single-camera Tactile Robot Finger with Large Sensing Coverage and Proprioceptive Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10885">http://arxiv.org/abs/2309.10885</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jialiang Zhao, Edward H. Adelson</li>
<li>for: 这个论文旨在开发一种可以同时进行感觉和 proprioceptive 感知的、人套指尺寸、单摄像头感知系统（GelSight Svelte）。</li>
<li>methods: 该系统使用折射镜实现感知覆盖区域，并通过摄像头捕捉到 flexible 背部的变形来估计扭矩和旋转力。</li>
<li>results: 经过gel塑变形实验和对三种不同抓取方式的评估，该系统能够准确地估计扭矩和旋转力，并且可以在不同的抓取位置上进行多种任务。更多信息请参考我们的官方网站：<a target="_blank" rel="noopener" href="https://gelsight-svelte.alanz.info./">https://gelsight-svelte.alanz.info。</a><details>
<summary>Abstract</summary>
Camera-based tactile sensing is a low-cost, popular approach to obtain highly detailed contact geometry information. However, most existing camera-based tactile sensors are fingertip sensors, and longer fingers often require extraneous elements to obtain an extended sensing area similar to the full length of a human finger. Moreover, existing methods to estimate proprioceptive information such as total forces and torques applied on the finger from camera-based tactile sensors are not effective when the contact geometry is complex. We introduce GelSight Svelte, a curved, human finger-sized, single-camera tactile sensor that is capable of both tactile and proprioceptive sensing over a large area. GelSight Svelte uses curved mirrors to achieve the desired shape and sensing coverage. Proprioceptive information, such as the total bending and twisting torques applied on the finger, is reflected as deformations on the flexible backbone of GelSight Svelte, which are also captured by the camera. We train a convolutional neural network to estimate the bending and twisting torques from the captured images. We conduct gel deformation experiments at various locations of the finger to evaluate the tactile sensing capability and proprioceptive sensing accuracy. To demonstrate the capability and potential uses of GelSight Svelte, we conduct an object holding task with three different grasping modes that utilize different areas of the finger. More information is available on our website: https://gelsight-svelte.alanz.info
</details>
<details>
<summary>摘要</summary>
Camera-based感觉检测是一种低成本、受欢迎的方法，以获取高级别的接触几何信息。然而，大多数现有的camera-based感觉传感器都是指尖传感器，长手指通常需要外加元素来获取类似于人类手指的扩展感知区域。此外，现有的方法来估算camera-based感觉传感器上的 proprioceptive信息（如手指上的总弯矩和扭矩）在复杂的接触几何下不准确。我们介绍了GelSight Svelte，一种呈杯形、人类手指大小的单摄像头感觉传感器，可以同时进行感觉和 proprioceptive感知。GelSight Svelte使用弯曲镜子实现感知覆盖区域。 proprioceptive信息（如手指上的总弯矩和扭矩）在GelSight Svelte的 flexible backbone上反射为凹形变化，并由摄像头捕捉。我们训练了一个卷积神经网络来估算弯矩和扭矩信息。我们通过在不同手指位置进行gel deformation实验来评估感觉检测能力和 proprioceptive感知精度。为了展示GelSight Svelte的能力和应用前景，我们完成了一个 объект托持任务，使用不同的抓取模式，利用不同的手指区域。更多信息请访问我们的网站：https://gelsight-svelte.alanz.info。
</details></li>
</ul>
<hr>
<h2 id="DeepliteRT-Computer-Vision-at-the-Edge"><a href="#DeepliteRT-Computer-Vision-at-the-Edge" class="headerlink" title="DeepliteRT: Computer Vision at the Edge"></a>DeepliteRT: Computer Vision at the Edge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10878">http://arxiv.org/abs/2309.10878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saad Ashfaq, Alexander Hoffman, Saptarshi Mitra, Sudhakar Sah, MohammadHossein AskariHemmat, Ehsan Saboori</li>
<li>for: 本研究旨在提出一种高效的极低位量化神经网络模型，以便在计算机视觉应用中部署深度学习模型。</li>
<li>methods: 本研究使用了ARM目标平台上高优化的极低位量化卷积运算器，并实现了一个名为Deeplite Runtime（DeepliteRT）的端到端解决方案，用于编译、调参和极低位量化模型的运行。</li>
<li>results: 研究表明，使用DeepliteRT可以实现对分类和检测模型的高速化，相比于优化的32位浮点数、8位整数和2位基准，可以获得速度提升达2.20倍、2.33倍和2.17倍。<details>
<summary>Abstract</summary>
The proliferation of edge devices has unlocked unprecedented opportunities for deep learning model deployment in computer vision applications. However, these complex models require considerable power, memory and compute resources that are typically not available on edge platforms. Ultra low-bit quantization presents an attractive solution to this problem by scaling down the model weights and activations from 32-bit to less than 8-bit. We implement highly optimized ultra low-bit convolution operators for ARM-based targets that outperform existing methods by up to 4.34x. Our operator is implemented within Deeplite Runtime (DeepliteRT), an end-to-end solution for the compilation, tuning, and inference of ultra low-bit models on ARM devices. Compiler passes in DeepliteRT automatically convert a fake-quantized model in full precision to a compact ultra low-bit representation, easing the process of quantized model deployment on commodity hardware. We analyze the performance of DeepliteRT on classification and detection models against optimized 32-bit floating-point, 8-bit integer, and 2-bit baselines, achieving significant speedups of up to 2.20x, 2.33x and 2.17x, respectively.
</details>
<details>
<summary>摘要</summary>
“Edge设备的普及导致深度学习模型在计算机视觉应用中的部署得到了无前例的机会。然而，这些复杂的模型占用了大量的计算、存储和电源资源，通常不可能在边缘平台上提供。超低位数量化presented an attractive solution to this problem by reducing the model weights and activations from 32-bit to less than 8-bit. We implement highly optimized ultra low-bit convolution operators for ARM-based targets that outperform existing methods by up to 4.34x. Our operator is implemented within Deeplite Runtime (DeepliteRT), an end-to-end solution for the compilation, tuning, and inference of ultra low-bit models on ARM devices. Compiler passes in DeepliteRT automatically convert a fake-quantized model in full precision to a compact ultra low-bit representation, easing the process of quantized model deployment on commodity hardware. We analyze the performance of DeepliteRT on classification and detection models against optimized 32-bit floating-point, 8-bit integer, and 2-bit baselines, achieving significant speedups of up to 2.20x, 2.33x, and 2.17x, respectively.”Note: The translation is in Simplified Chinese, which is the standard Chinese writing system used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="On-device-Real-time-Custom-Hand-Gesture-Recognition"><a href="#On-device-Real-time-Custom-Hand-Gesture-Recognition" class="headerlink" title="On-device Real-time Custom Hand Gesture Recognition"></a>On-device Real-time Custom Hand Gesture Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10858">http://arxiv.org/abs/2309.10858</a></li>
<li>repo_url: None</li>
<li>paper_authors: Esha Uboweja, David Tian, Qifei Wang, Yi-Chun Kuo, Joe Zou, Lu Wang, George Sung, Matthias Grundmann</li>
<li>for: 可以快速创建和部署用户定制的手势识别系统，无需专业Machine Learning（ML）知识。</li>
<li>methods: 使用预训练单手嵌入模型，通过收集一小部分的图像来训练和部署自定义手势识别模型。</li>
<li>results: 可以快速完成手势识别系统的开发和部署，只需几分钟时间。<details>
<summary>Abstract</summary>
Most existing hand gesture recognition (HGR) systems are limited to a predefined set of gestures. However, users and developers often want to recognize new, unseen gestures. This is challenging due to the vast diversity of all plausible hand shapes, e.g. it is impossible for developers to include all hand gestures in a predefined list. In this paper, we present a user-friendly framework that lets users easily customize and deploy their own gesture recognition pipeline. Our framework provides a pre-trained single-hand embedding model that can be fine-tuned for custom gesture recognition. Users can perform gestures in front of a webcam to collect a small amount of images per gesture. We also offer a low-code solution to train and deploy the custom gesture recognition model. This makes it easy for users with limited ML expertise to use our framework. We further provide a no-code web front-end for users without any ML expertise. This makes it even easier to build and test the end-to-end pipeline. The resulting custom HGR is then ready to be run on-device for real-time scenarios. This can be done by calling a simple function in our open-sourced model inference API, MediaPipe Tasks. This entire process only takes a few minutes.
</details>
<details>
<summary>摘要</summary>
现有的手势识别（HGR）系统大多是基于固定的手势集。然而，用户和开发者通常希望可以识别新、未看到的手势。这是因为手势的多样性很大，例如开发者无法包含所有的手势在一个预定列表中。在这篇论文中，我们提供了一个用户友好的框架，允许用户轻松自定义和部署自己的手势识别管道。我们的框架提供了预训练的单手嵌入模型，可以根据用户自定义的手势进行微调。用户可以在前置摄像头上执行手势，并收集一小量的图像数据。我们还提供了一个低代码的解决方案，以便用户对自定义手势识别模型进行训练和部署。这使得用户对ML知识有限的用户可以轻松使用我们的框架。此外，我们还提供了一个无代码的web前端，以便用户没有任何ML知识的情况下也可以构建和测试完整的管道。结果的自定义HGR然后可以在实时场景下运行在设备上，只需要很短的时间。这可以通过调用我们开源的模型推理API，MediaPipe Tasks的简单函数来完成。
</details></li>
</ul>
<hr>
<h2 id="Assessing-the-capacity-of-a-denoising-diffusion-probabilistic-model-to-reproduce-spatial-context"><a href="#Assessing-the-capacity-of-a-denoising-diffusion-probabilistic-model-to-reproduce-spatial-context" class="headerlink" title="Assessing the capacity of a denoising diffusion probabilistic model to reproduce spatial context"></a>Assessing the capacity of a denoising diffusion probabilistic model to reproduce spatial context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10817">http://arxiv.org/abs/2309.10817</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rucha Deshpande, Muzaffer Özbey, Hua Li, Mark A. Anastasio, Frank J. Brooks</li>
<li>for: 本研究的目的是探讨DDPMs在医疗影像领域中是否可靠地学习空间上下文信息。</li>
<li>methods: 本研究使用杂相上下文模型（SCMs）生成训练数据，并使用DDPMs生成图像 ensemble，以评估其在学习空间上下文信息方面的能力。</li>
<li>results: 研究发现，DDPMs可以准确地复制空间上下文信息，并且可以生成Contextually correct的图像，这些图像可以作为数据扩展任务中的参考数据。相比之下，GANs无法实现这种效果。<details>
<summary>Abstract</summary>
Diffusion models have emerged as a popular family of deep generative models (DGMs). In the literature, it has been claimed that one class of diffusion models -- denoising diffusion probabilistic models (DDPMs) -- demonstrate superior image synthesis performance as compared to generative adversarial networks (GANs). To date, these claims have been evaluated using either ensemble-based methods designed for natural images, or conventional measures of image quality such as structural similarity. However, there remains an important need to understand the extent to which DDPMs can reliably learn medical imaging domain-relevant information, which is referred to as `spatial context' in this work. To address this, a systematic assessment of the ability of DDPMs to learn spatial context relevant to medical imaging applications is reported for the first time. A key aspect of the studies is the use of stochastic context models (SCMs) to produce training data. In this way, the ability of the DDPMs to reliably reproduce spatial context can be quantitatively assessed by use of post-hoc image analyses. Error-rates in DDPM-generated ensembles are reported, and compared to those corresponding to a modern GAN. The studies reveal new and important insights regarding the capacity of DDPMs to learn spatial context. Notably, the results demonstrate that DDPMs hold significant capacity for generating contextually correct images that are `interpolated' between training samples, which may benefit data-augmentation tasks in ways that GANs cannot.
</details>
<details>
<summary>摘要</summary>
Diffusion models 已经成为深度生成模型（DGM）的流行家族。在文献中，有人声称一种类型的扩散模型——去噪扩散概率模型（DDPM）——在比较于生成敌对网络（GAN）的图像生成性能方面表现出色。到目前为止，这些声称都是通过ensemble-based方法，设计 для自然图像，或者传统的图像质量度量指标，如结构相似性，进行评估。然而，还有一项重要的需求，即了解DDPM在医学影像领域中 relevante的信息是否可靠地学习。为了解决这个问题，本文报告了DDPM在医学影像应用中学习空间上下文的系统性评估。在这些研究中，使用stoochastic context models（SCM）生成训练数据，以便通过后期图像分析来评估DDPM是否可靠地重现空间上下文。DDPM生成的集合的错误率被报告，并与现代GAN的错误率进行比较。研究发现了新的重要信息，即DDPM在学习空间上下文方面具有显著的能力。特别是，结果表明DDPM可以生成符合训练样本上下文的图像，并且可以在数据扩展任务中提供新的优势。
</details></li>
</ul>
<hr>
<h2 id="PanopticNeRF-360-Panoramic-3D-to-2D-Label-Transfer-in-Urban-Scenes"><a href="#PanopticNeRF-360-Panoramic-3D-to-2D-Label-Transfer-in-Urban-Scenes" class="headerlink" title="PanopticNeRF-360: Panoramic 3D-to-2D Label Transfer in Urban Scenes"></a>PanopticNeRF-360: Panoramic 3D-to-2D Label Transfer in Urban Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10815">http://arxiv.org/abs/2309.10815</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fuxiao0719/panopticnerf">https://github.com/fuxiao0719/panopticnerf</a></li>
<li>paper_authors: Xiao Fu, Shangzhan Zhang, Tianrun Chen, Yichong Lu, Xiaowei Zhou, Andreas Geiger, Yiyi Liao</li>
<li>for: 本研究旨在提高自驾车视觉系统的训练效果，以便提高自驾车的安全性和可靠性。</li>
<li>methods: 本文提出了一种新的方法，即combine粗糙3D标注与噪音2Dsemantic约束来生成高质量的360度图像和精确的各种标签。</li>
<li>results: 实验表明，本方法可以在KITTI-360 dataset上达到现有标注传递方法的状态对抗性，并且可以生成高分辨率、多视图和时空一致的图像、semantic和instance标签。<details>
<summary>Abstract</summary>
Training perception systems for self-driving cars requires substantial annotations. However, manual labeling in 2D images is highly labor-intensive. While existing datasets provide rich annotations for pre-recorded sequences, they fall short in labeling rarely encountered viewpoints, potentially hampering the generalization ability for perception models. In this paper, we present PanopticNeRF-360, a novel approach that combines coarse 3D annotations with noisy 2D semantic cues to generate consistent panoptic labels and high-quality images from any viewpoint. Our key insight lies in exploiting the complementarity of 3D and 2D priors to mutually enhance geometry and semantics. Specifically, we propose to leverage noisy semantic and instance labels in both 3D and 2D spaces to guide geometry optimization. Simultaneously, the improved geometry assists in filtering noise present in the 3D and 2D annotations by merging them in 3D space via a learned semantic field. To further enhance appearance, we combine MLP and hash grids to yield hybrid scene features, striking a balance between high-frequency appearance and predominantly contiguous semantics. Our experiments demonstrate PanopticNeRF-360's state-of-the-art performance over existing label transfer methods on the challenging urban scenes of the KITTI-360 dataset. Moreover, PanopticNeRF-360 enables omnidirectional rendering of high-fidelity, multi-view and spatiotemporally consistent appearance, semantic and instance labels. We make our code and data available at https://github.com/fuxiao0719/PanopticNeRF
</details>
<details>
<summary>摘要</summary>
培训自驾车视觉系统需要大量的标注。然而，手动标注在2D图像中是非常劳动密集的。现有的数据集提供了丰富的标注 для预先录制的序列，但它们缺乏标注不常见的视角，可能会妨碍视觉模型的泛化能力。在这篇论文中，我们提出了PanopticNeRF-360，一种新的方法，它将粗略的3D标注与噪声2D语义指标相结合，以生成一致的�anoptic标签和高质量的图像从任何视角。我们的关键发现在于利用3D和2D优先的补偿，以便互相增强几何和 semantics。具体来说，我们提议利用2D和3D空间中噪声的语义和实例标签来导航 geometry 优化。同时，改进的几何 помочь减少2D和3D标注中的噪声，通过学习的semantic场进行3D空间的合并。为了进一步提高外观，我们结合多层感知（MLP）和哈希网格，以生成混合的场景特征， strike a balance between high-frequency appearance and predominantly contiguous semantics。我们的实验表明PanopticNeRF-360在KITTI-360 dataset上的城市场景上表现出了状态机器人的性能，并且允许高精度、多视角和时空协调的出现、semantic和实例标签的渲染。我们在github上分享了我们的代码和数据，请参考https://github.com/fuxiao0719/PanopticNeRF。
</details></li>
</ul>
<hr>
<h2 id="PGDiff-Guiding-Diffusion-Models-for-Versatile-Face-Restoration-via-Partial-Guidance"><a href="#PGDiff-Guiding-Diffusion-Models-for-Versatile-Face-Restoration-via-Partial-Guidance" class="headerlink" title="PGDiff: Guiding Diffusion Models for Versatile Face Restoration via Partial Guidance"></a>PGDiff: Guiding Diffusion Models for Versatile Face Restoration via Partial Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10810">http://arxiv.org/abs/2309.10810</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pq-yang/pgdiff">https://github.com/pq-yang/pgdiff</a></li>
<li>paper_authors: Peiqing Yang, Shangchen Zhou, Qingyi Tao, Chen Change Loy</li>
<li>For: 本研究旨在把传统任务特有的训练方法替换为使用预训diffusion模型，以提高复原性能。* Methods: 我们提出了PGDiff方法，它通过引入偏向指导来增强复原性能。不同于先前的方法，我们的方法不需要具体地定义干扰过程，而是根据高质量图像的特性和颜色统计，设定导航。* Results: 实验结果显示，我们的方法不仅超越了现有的diffusion-prior-based方法，而且与任务特有的模型竞争。此外，我们的方法还可以扩展到复合任务，通过结合不同任务的导航。<details>
<summary>Abstract</summary>
Exploiting pre-trained diffusion models for restoration has recently become a favored alternative to the traditional task-specific training approach. Previous works have achieved noteworthy success by limiting the solution space using explicit degradation models. However, these methods often fall short when faced with complex degradations as they generally cannot be precisely modeled. In this paper, we propose PGDiff by introducing partial guidance, a fresh perspective that is more adaptable to real-world degradations compared to existing works. Rather than specifically defining the degradation process, our approach models the desired properties, such as image structure and color statistics of high-quality images, and applies this guidance during the reverse diffusion process. These properties are readily available and make no assumptions about the degradation process. When combined with a diffusion prior, this partial guidance can deliver appealing results across a range of restoration tasks. Additionally, PGDiff can be extended to handle composite tasks by consolidating multiple high-quality image properties, achieved by integrating the guidance from respective tasks. Experimental results demonstrate that our method not only outperforms existing diffusion-prior-based approaches but also competes favorably with task-specific models.
</details>
<details>
<summary>摘要</summary>
utilizes pre-trained diffusion models for restoration has recently become a popular alternative to the traditional task-specific training approach. Previous works have achieved notable success by limiting the solution space using explicit degradation models. However, these methods often fall short when faced with complex degradations as they generally cannot be precisely modeled. In this paper, we propose PGDiff by introducing partial guidance, a fresh perspective that is more adaptable to real-world degradations compared to existing works. Rather than specifically defining the degradation process, our approach models the desired properties, such as image structure and color statistics of high-quality images, and applies this guidance during the reverse diffusion process. These properties are readily available and make no assumptions about the degradation process. When combined with a diffusion prior, this partial guidance can deliver appealing results across a range of restoration tasks. Additionally, PGDiff can be extended to handle composite tasks by consolidating multiple high-quality image properties, achieved by integrating the guidance from respective tasks. Experimental results demonstrate that our method not only outperforms existing diffusion-prior-based approaches but also competes favorably with task-specific models.
</details></li>
</ul>
<hr>
<h2 id="Multi-Context-Dual-Hyper-Prior-Neural-Image-Compression"><a href="#Multi-Context-Dual-Hyper-Prior-Neural-Image-Compression" class="headerlink" title="Multi-Context Dual Hyper-Prior Neural Image Compression"></a>Multi-Context Dual Hyper-Prior Neural Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10799">http://arxiv.org/abs/2309.10799</a></li>
<li>repo_url: None</li>
<li>paper_authors: Atefeh Khoshkhahtinat, Ali Zafari, Piyush M. Mehta, Mohammad Akyash, Hossein Kashiani, Nasser M. Nasrabadi</li>
<li>for: 这篇论文主要是为了提出一种基于Transformer的深度图像压缩神经网络，以提高图像压缩的环境依赖性和全局相互关系模型。</li>
<li>methods: 该论文提出了一种基于Transformer的非线性变换，以 efficiently capture both local and global information from the input image，并且引入了一种新的 entropy model，该模型包括了两种不同的 гиперприор来模型横轴和水平依赖关系。</li>
<li>results: 经过实验表明，该提议的框架在环境依赖性和全局相互关系上比前者方法更高，并且可以更好地压缩图像。<details>
<summary>Abstract</summary>
Transform and entropy models are the two core components in deep image compression neural networks. Most existing learning-based image compression methods utilize convolutional-based transform, which lacks the ability to model long-range dependencies, primarily due to the limited receptive field of the convolution operation. To address this limitation, we propose a Transformer-based nonlinear transform. This transform has the remarkable ability to efficiently capture both local and global information from the input image, leading to a more decorrelated latent representation. In addition, we introduce a novel entropy model that incorporates two different hyperpriors to model cross-channel and spatial dependencies of the latent representation. To further improve the entropy model, we add a global context that leverages distant relationships to predict the current latent more accurately. This global context employs a causal attention mechanism to extract long-range information in a content-dependent manner. Our experiments show that our proposed framework performs better than the state-of-the-art methods in terms of rate-distortion performance.
</details>
<details>
<summary>摘要</summary>
<<SYS>> transform和Entropy模型是深度图像压缩神经网络的两个核心组件。大多数现有的学习基于图像压缩方法使用 convolutional-based transform，它因为卷积操作的局部场景限制无法准确地模型长距离依赖关系。为了解决这一限制，我们提议一种基于 transformer 的非线性变换。这种变换能够高效地捕捉输入图像的局部和全局信息，从而导致更高度相关的幂等表示。此外，我们介绍了一种新的 Entropy 模型，它通过两个不同的 гиперPRIOR 来模型跨通道和空间依赖关系。为了进一步改进 Entropy 模型，我们添加了一个全局上下文，通过利用远程关系来预测当前幂等的更加准确。这个全局上下文使用 causal attention 机制来提取跨通道信息，以具有内容依赖的方式。我们的实验表明，我们的提议的框架在率度-质量性能方面与当前最佳方法相比表现更好。Note: The translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Multi-spectral-Entropy-Constrained-Neural-Compression-of-Solar-Imagery"><a href="#Multi-spectral-Entropy-Constrained-Neural-Compression-of-Solar-Imagery" class="headerlink" title="Multi-spectral Entropy Constrained Neural Compression of Solar Imagery"></a>Multi-spectral Entropy Constrained Neural Compression of Solar Imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10791">http://arxiv.org/abs/2309.10791</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Zafari, Atefeh Khoshkhahtinat, Piyush M. Mehta, Nasser M. Nasrabadi, Barbara J. Thompson, Michael S. F. Kirk, Daniel da Silva</li>
<li>for: 本研究旨在开发一种基于变换器的多spectral神经图像压缩器，以实现高效地捕捉多波段图像中的相互 redundancy。</li>
<li>methods: 我们提出了一种基于多头自注意 Mechanism的inter-window汇集token自注意机制，以解 liberate地ocalization的窗口自注意机制。此外，我们还使用随机偏移窗口注意机制，使transformer块对输入领域的翻译不敏感。</li>
<li>results: 我们的方法不仅超越了传统压缩算法，还能更好地decorrelates多波段图像，比单spectral压缩更好。<details>
<summary>Abstract</summary>
Missions studying the dynamic behaviour of the Sun are defined to capture multi-spectral images of the sun and transmit them to the ground station in a daily basis. To make transmission efficient and feasible, image compression systems need to be exploited. Recently successful end-to-end optimized neural network-based image compression systems have shown great potential to be used in an ad-hoc manner. In this work we have proposed a transformer-based multi-spectral neural image compressor to efficiently capture redundancies both intra/inter-wavelength. To unleash the locality of window-based self attention mechanism, we propose an inter-window aggregated token multi head self attention. Additionally to make the neural compressor autoencoder shift invariant, a randomly shifted window attention mechanism is used which makes the transformer blocks insensitive to translations in their input domain. We demonstrate that the proposed approach not only outperforms the conventional compression algorithms but also it is able to better decorrelates images along the multiple wavelengths compared to single spectral compression.
</details>
<details>
<summary>摘要</summary>
充当太阳动态行为研究任务的图像压缩系统需要在每天基础上 capture 多spectral 图像并将其传输到地面站。为了使压缩效率高并实现可行，图像压缩系统需要被利用。最近，一种基于神经网络的结构优化的图像压缩系统在随机的方式上表现出了极高的潜力。在这种工作中，我们提出了一种基于变换器的多spectral 神经图像压缩器，可以快速捕捉 intra/inter-探讨 的重复性。为了利用窗口基于的自注意机制的本地性，我们提出了一种多头自注意机制，并使用了随机推移窗口注意机制，使得转换块在输入领域中变得不敏感于平移。我们 demonstarte 了该方法不仅可以超越传统压缩算法，而且可以更好地对多普通频谱图像进行decorrelation。
</details></li>
</ul>
<hr>
<h2 id="AV-SUPERB-A-Multi-Task-Evaluation-Benchmark-for-Audio-Visual-Representation-Models"><a href="#AV-SUPERB-A-Multi-Task-Evaluation-Benchmark-for-Audio-Visual-Representation-Models" class="headerlink" title="AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual Representation Models"></a>AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual Representation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10787">http://arxiv.org/abs/2309.10787</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/roger-tseng/av-superb">https://github.com/roger-tseng/av-superb</a></li>
<li>paper_authors: Yuan Tseng, Layne Berry, Yi-Ting Chen, I-Hsiang Chiu, Hsuan-Hao Lin, Max Liu, Puyuan Peng, Yi-Jen Shih, Hung-Yu Wang, Haibin Wu, Po-Yao Huang, Chun-Mao Lai, Shang-Wen Li, David Harwath, Yu Tsao, Shinji Watanabe, Abdelrahman Mohamed, Chi-Luen Feng, Hung-yi Lee</li>
<li>for: 该研究旨在开发一种人类视觉系统，利用听视信息的相关性来实现。</li>
<li>methods: 该研究使用了AV-SUPERBBenchmark，一个涵盖5种音频视频任务的通用评估平台，以评估5种最近自监学模型的通用性。</li>
<li>results: 研究发现，当前的模型通常只能在限定的任务上进行有效的泛化，而且无法在所有任务上达到理想的性能。此外，研究还发现，通过中间任务练化和使用AudioSet作为中间任务可以提高表示的性能。<details>
<summary>Abstract</summary>
Audio-visual representation learning aims to develop systems with human-like perception by utilizing correlation between auditory and visual information. However, current models often focus on a limited set of tasks, and generalization abilities of learned representations are unclear. To this end, we propose the AV-SUPERB benchmark that enables general-purpose evaluation of unimodal audio/visual and bimodal fusion representations on 7 datasets covering 5 audio-visual tasks in speech and audio processing. We evaluate 5 recent self-supervised models and show that none of these models generalize to all tasks, emphasizing the need for future study on improving universal model performance. In addition, we show that representations may be improved with intermediate-task fine-tuning and audio event classification with AudioSet serves as a strong intermediate task. We release our benchmark with evaluation code and a model submission platform to encourage further research in audio-visual learning.
</details>
<details>
<summary>摘要</summary>
音视频表示学习目标是使系统具有人类类似的感知，通过听音和视觉信息之间的相关性。然而，当前的模型通常只会处理有限的任务，学习得到的表示的通用能力是不清楚的。为此，我们提出了AV-SUPERB benchmark，它可以对7个数据集的5种音视频任务进行通用评估。我们评估了5种最新的自我超vised模型，发现其中没有一个能够通用到所有任务，这 highlights the need for future research on improving universal model performance。此外，我们还发现，通过中间任务精度调整和使用AudioSet进行听音事件分类可以提高表示的质量。我们发布了我们的benchmark，评估代码和模型提交平台，以便更多的研究人员继续探索音视频学习领域。
</details></li>
</ul>
<hr>
<h2 id="Context-Aware-Neural-Video-Compression-on-Solar-Dynamics-Observatory"><a href="#Context-Aware-Neural-Video-Compression-on-Solar-Dynamics-Observatory" class="headerlink" title="Context-Aware Neural Video Compression on Solar Dynamics Observatory"></a>Context-Aware Neural Video Compression on Solar Dynamics Observatory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10784">http://arxiv.org/abs/2309.10784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Atefeh Khoshkhahtinat, Ali Zafari, Piyush M. Mehta, Nasser M. Nasrabadi, Barbara J. Thompson, Michael S. F. Kirk, Daniel da Silva</li>
<li>for:  This paper aims to improve the compression of solar images collected by NASA’s Solar Dynamics Observatory (SDO) mission.</li>
<li>methods: The paper proposes a novel neural Transformer-based video compression approach that leverages a Fused Local-aware Window (FLaWin) Transformer block to efficiently exploit temporal and spatial redundancies in the images, resulting in a high compression ratio.</li>
<li>results: The proposed approach outperforms conventional hand-engineered video codecs such as H.264 and H.265 in terms of rate-distortion trade-off, demonstrating the effectiveness of the FLaWin Transformer block in improving compression performance.<details>
<summary>Abstract</summary>
NASA's Solar Dynamics Observatory (SDO) mission collects large data volumes of the Sun's daily activity. Data compression is crucial for space missions to reduce data storage and video bandwidth requirements by eliminating redundancies in the data. In this paper, we present a novel neural Transformer-based video compression approach specifically designed for the SDO images. Our primary objective is to efficiently exploit the temporal and spatial redundancies inherent in solar images to obtain a high compression ratio. Our proposed architecture benefits from a novel Transformer block called Fused Local-aware Window (FLaWin), which incorporates window-based self-attention modules and an efficient fused local-aware feed-forward (FLaFF) network. This architectural design allows us to simultaneously capture short-range and long-range information while facilitating the extraction of rich and diverse contextual representations. Moreover, this design choice results in reduced computational complexity. Experimental results demonstrate the significant contribution of the FLaWin Transformer block to the compression performance, outperforming conventional hand-engineered video codecs such as H.264 and H.265 in terms of rate-distortion trade-off.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MAGIC-TBR-Multiview-Attention-Fusion-for-Transformer-based-Bodily-Behavior-Recognition-in-Group-Settings"><a href="#MAGIC-TBR-Multiview-Attention-Fusion-for-Transformer-based-Bodily-Behavior-Recognition-in-Group-Settings" class="headerlink" title="MAGIC-TBR: Multiview Attention Fusion for Transformer-based Bodily Behavior Recognition in Group Settings"></a>MAGIC-TBR: Multiview Attention Fusion for Transformer-based Bodily Behavior Recognition in Group Settings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10765">http://arxiv.org/abs/2309.10765</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/surbhimadan92/magic-tbr">https://github.com/surbhimadan92/magic-tbr</a></li>
<li>paper_authors: Surbhi Madan, Rishabh Jain, Gulshan Sharma, Ramanathan Subramanian, Abhinav Dhall</li>
<li>for: 本研究旨在提高人工智能系统的理解，通过自动分析社交语言行为。</li>
<li>methods: 本文提出了一种多视图注意力融合方法（MAGIC-TBR），利用视频和其相应的Discrete Cosine Transform幂等特征，通过变换器基本方法进行融合。</li>
<li>results: 实验结果表明，提出的特征融合方法有效地捕捉了BBSI数据集中的较细的行为特征，如招手、梳妆、摸拥等。<details>
<summary>Abstract</summary>
Bodily behavioral language is an important social cue, and its automated analysis helps in enhancing the understanding of artificial intelligence systems. Furthermore, behavioral language cues are essential for active engagement in social agent-based user interactions. Despite the progress made in computer vision for tasks like head and body pose estimation, there is still a need to explore the detection of finer behaviors such as gesturing, grooming, or fumbling. This paper proposes a multiview attention fusion method named MAGIC-TBR that combines features extracted from videos and their corresponding Discrete Cosine Transform coefficients via a transformer-based approach. The experiments are conducted on the BBSI dataset and the results demonstrate the effectiveness of the proposed feature fusion with multiview attention. The code is available at: https://github.com/surbhimadan92/MAGIC-TBR
</details>
<details>
<summary>摘要</summary>
文体行为语言是社交见识的重要依据，自动分析可以增强人工智能系统的理解。此外，行为语言cue也是社交代理人与用户之间活跃互动的重要因素。虽然计算机视觉技术在头和身体姿态估计等任务上已经取得了进展，但是还需要探索更细致的行为，如手势、梳妆或摸拌等。这篇论文提议一种名为MAGIC-TBR的多视图注意力融合方法，该方法通过 трансформа器方法结合视频和其相应的Discrete Cosine Transform幂值来拼接特征。实验在BBSI数据集上进行，结果表明提议的特征融合与多视图注意力具有效果。代码可以在以下链接获取：https://github.com/surbhimadan92/MAGIC-TBR。
</details></li>
</ul>
<hr>
<h2 id="Few-Shot-Panoptic-Segmentation-With-Foundation-Models"><a href="#Few-Shot-Panoptic-Segmentation-With-Foundation-Models" class="headerlink" title="Few-Shot Panoptic Segmentation With Foundation Models"></a>Few-Shot Panoptic Segmentation With Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10726">http://arxiv.org/abs/2309.10726</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/robot-learning-freiburg/SPINO">https://github.com/robot-learning-freiburg/SPINO</a></li>
<li>paper_authors: Markus Käppeler, Kürsat Petek, Niclas Vödisch, Wolfram Burgard, Abhinav Valada</li>
<li>for: 提高�anoptic segmentation的可行性，使用非标注数据进行训练。</li>
<li>methods: 利用task-agnostic图像特征，即DINOv2底层和轻量级网络头，进行semantic segmentation和boundary estimation。</li>
<li>results: 使用仅10个标注图像，可以预测高质量pseudo-标签，并且与完全监督基eline相比，使用更少的标注数据（less than 0.3%），达到竞争力的结果。<details>
<summary>Abstract</summary>
Current state-of-the-art methods for panoptic segmentation require an immense amount of annotated training data that is both arduous and expensive to obtain posing a significant challenge for their widespread adoption. Concurrently, recent breakthroughs in visual representation learning have sparked a paradigm shift leading to the advent of large foundation models that can be trained with completely unlabeled images. In this work, we propose to leverage such task-agnostic image features to enable few-shot panoptic segmentation by presenting Segmenting Panoptic Information with Nearly 0 labels (SPINO). In detail, our method combines a DINOv2 backbone with lightweight network heads for semantic segmentation and boundary estimation. We show that our approach, albeit being trained with only ten annotated images, predicts high-quality pseudo-labels that can be used with any existing panoptic segmentation method. Notably, we demonstrate that SPINO achieves competitive results compared to fully supervised baselines while using less than 0.3% of the ground truth labels, paving the way for learning complex visual recognition tasks leveraging foundation models. To illustrate its general applicability, we further deploy SPINO on real-world robotic vision systems for both outdoor and indoor environments. To foster future research, we make the code and trained models publicly available at http://spino.cs.uni-freiburg.de.
</details>
<details>
<summary>摘要</summary>
当前最新的方法 для�annoptic segmentation需要庞大量的标注训练数据，这是费时和贵金的，这 pose  significan t challenge  для它们的普及。同时，最近的视觉表示学习技术的突破口导致了基础模型的出现，可以通过完全无标注图像进行训练。在这种情况下，我们提议利用这些任务无关的图像特征来实现几何�train panoptic segmentation，我们称之为Segmenting Panoptic Information with Nearly 0 labels（SPINO）。在详细的实现中，我们将DINOv2 脊梁结合轻量级网络头进行semantic segmentation和边界估计。我们表明，我们的方法可以通过只使用十个标注图像来生成高质量 Pseudo-labels，这些 Pseudo-labels 可以与任何现有的�annoptic segmentation方法结合使用。需要注意的是，我们的方法可以与完全supervised baseline 进行比较，并且使用 less than 0.3% 的标注数据，这种方法可以学习复杂的视觉认知任务。为了见证其通用性，我们进一步在实际的 роботи视系统中部署了SPINO。为了促进未来的研究，我们在http://spino.cs.uni-freiburg.de 上公开发布了代码和训练模型。
</details></li>
</ul>
<hr>
<h2 id="Reconstruct-and-Generate-Diffusion-Model-for-Detail-Preserving-Image-Denoising"><a href="#Reconstruct-and-Generate-Diffusion-Model-for-Detail-Preserving-Image-Denoising" class="headerlink" title="Reconstruct-and-Generate Diffusion Model for Detail-Preserving Image Denoising"></a>Reconstruct-and-Generate Diffusion Model for Detail-Preserving Image Denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10714">http://arxiv.org/abs/2309.10714</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yujin Wang, Lingen Li, Tianfan Xue, Jinwei Gu</li>
<li>for: 减少图像噪声，提高图像质量</li>
<li>methods: 提pose了一种新的Reconstruct-and-Generate Diffusion Model（RnG），包括一个恢复性的噪声去除网络和一个扩散算法，以保持图像的视觉质量和准确性</li>
<li>results: 通过对多个synthetic和实际噪声 dataset进行了广泛的实验，证明了提案的方法的优越性。<details>
<summary>Abstract</summary>
Image denoising is a fundamental and challenging task in the field of computer vision. Most supervised denoising methods learn to reconstruct clean images from noisy inputs, which have intrinsic spectral bias and tend to produce over-smoothed and blurry images. Recently, researchers have explored diffusion models to generate high-frequency details in image restoration tasks, but these models do not guarantee that the generated texture aligns with real images, leading to undesirable artifacts. To address the trade-off between visual appeal and fidelity of high-frequency details in denoising tasks, we propose a novel approach called the Reconstruct-and-Generate Diffusion Model (RnG). Our method leverages a reconstructive denoising network to recover the majority of the underlying clean signal, which serves as the initial estimation for subsequent steps to maintain fidelity. Additionally, it employs a diffusion algorithm to generate residual high-frequency details, thereby enhancing visual quality. We further introduce a two-stage training scheme to ensure effective collaboration between the reconstructive and generative modules of RnG. To reduce undesirable texture introduced by the diffusion model, we also propose an adaptive step controller that regulates the number of inverse steps applied by the diffusion model, allowing control over the level of high-frequency details added to each patch as well as saving the inference computational cost. Through our proposed RnG, we achieve a better balance between perception and distortion. We conducted extensive experiments on both synthetic and real denoising datasets, validating the superiority of the proposed approach.
</details>
<details>
<summary>摘要</summary>
Image 减霉是计算机视觉领域中的基本和挑战性任务。大多数指导下的减霉方法学习从噪声输入中重建清晰图像，这些方法具有内在的频谱偏好，导致生成的图像过于平滑和模糊。在最近的研究中，研究人员使用扩散模型生成高频环境细节，但这些模型不保证生成的文本与实际图像相匹配，导致不жела的artefacts。为了解决减霉任务中的质量和准确性之间的权衡，我们提出了一种新的方法 called Reconstruct-and-Generate Diffusion Model (RnG)。我们的方法利用一种重建减霉网络来恢复大多数的下面清晰信号，该信号 serve as the initial estimation for subsequent steps to maintain fidelity。此外，它采用扩散算法生成剩余的高频环境细节，从而提高视觉质量。我们还提出了一种两阶段训练方案，以确保重建和生成模块之间的合作有效。为了避免扩散模型引入的不жела的文本，我们还提出了一种自适应步长控制器，可以控制每个 patch 中扩散模型应用的 inverse 步长数量，从而控制高频环境细节的添加水平以及计算误差的减少。通过我们的提出的 RnG，我们实现了减霉任务中更好的质量和准确性之间的权衡。我们在 synthetic 和实际减霉数据集上进行了广泛的实验，证明了我们的方法的超越性。
</details></li>
</ul>
<hr>
<h2 id="Interpret-Vision-Transformers-as-ConvNets-with-Dynamic-Convolutions"><a href="#Interpret-Vision-Transformers-as-ConvNets-with-Dynamic-Convolutions" class="headerlink" title="Interpret Vision Transformers as ConvNets with Dynamic Convolutions"></a>Interpret Vision Transformers as ConvNets with Dynamic Convolutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10713">http://arxiv.org/abs/2309.10713</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chong Zhou, Chen Change Loy, Bo Dai</li>
<li>for: 本研究旨在比较视觉变换器和卷积网络的超越性，并提出了一种将视觉变换器视为卷积网络的动态卷积的解释方法，从而在一个统一的框架下比较这两种 Architecture 的设计选择。</li>
<li>methods: 本研究使用了一种新的解释方法，即将视觉变换器视为卷积网络的动态卷积，从而将两种 Architecture 归纳到一个统一的框架下。研究者们还提出了两个具体的研究，一是关于视觉变换器中 softmax 的作用和其可以被取代的问题，二是基于depth-wise convolution的视觉变换器的设计。</li>
<li>results: 研究表明，通过将视觉变换器视为卷积网络的动态卷积，可以更好地理解和设计这两种 Architecture，并且可以提高网络的性能和速度。<details>
<summary>Abstract</summary>
There has been a debate about the superiority between vision Transformers and ConvNets, serving as the backbone of computer vision models. Although they are usually considered as two completely different architectures, in this paper, we interpret vision Transformers as ConvNets with dynamic convolutions, which enables us to characterize existing Transformers and dynamic ConvNets in a unified framework and compare their design choices side by side. In addition, our interpretation can also guide the network design as researchers now can consider vision Transformers from the design space of ConvNets and vice versa. We demonstrate such potential through two specific studies. First, we inspect the role of softmax in vision Transformers as the activation function and find it can be replaced by commonly used ConvNets modules, such as ReLU and Layer Normalization, which results in a faster convergence rate and better performance. Second, following the design of depth-wise convolution, we create a corresponding depth-wise vision Transformer that is more efficient with comparable performance. The potential of the proposed unified interpretation is not limited to the given examples and we hope it can inspire the community and give rise to more advanced network architectures.
</details>
<details>
<summary>摘要</summary>
有一些论战关于 Computer Vision 模型的优劣，主要集中在 vision Transformers 和 ConvNets 之间。尽管它们通常被视为完全不同的架构，但在这篇论文中，我们将 vision Transformers 解释为 ConvNets 中的动态混合，这允许我们在一个统一的框架下描述现有的 Transformers 和动态 ConvNets，并且比较它们的设计决策。此外，我们的解释还可以帮助网络设计，因为研究人员现在可以从 ConvNets 的设计空间中考虑 vision Transformers，并且将之相对。我们透过以下两个具体的研究来证明这一点。首先，我们评估了 vision Transformers 中 softmax 作为活化函数的角色，发现它可以被替换为通用的 ConvNets 模组，如 ReLU 和层normalization，这会导致更快的整合速率和更好的性能。其次，我们根据深度对应 convolution 的设计，创建了相应的深度对应 vision Transformer，该模型更加高效，性能相当。我们希望这一提议可以鼓励社区，并导致更进一步的网络架构。
</details></li>
</ul>
<hr>
<h2 id="Latent-Space-Energy-based-Model-for-Fine-grained-Open-Set-Recognition"><a href="#Latent-Space-Energy-based-Model-for-Fine-grained-Open-Set-Recognition" class="headerlink" title="Latent Space Energy-based Model for Fine-grained Open Set Recognition"></a>Latent Space Energy-based Model for Fine-grained Open Set Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10711">http://arxiv.org/abs/2309.10711</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wentao Bao, Qi Yu, Yu Kong</li>
<li>for: 这个研究旨在应对细部分组别的图像识别，排除未知的类别图像。</li>
<li>methods: 这个方法使用了能量基模型（EBM）来混合生成和识别任务，并在高维度空间进行点击估计，以提高细部分组别的图像识别。</li>
<li>results: 这个方法可以在细部分组别的图像识别中提高表达性、精细度和点击密度，并且可以利用最新的视觉 трансформа器来实现强大的视觉分类和生成。<details>
<summary>Abstract</summary>
Fine-grained open-set recognition (FineOSR) aims to recognize images belonging to classes with subtle appearance differences while rejecting images of unknown classes. A recent trend in OSR shows the benefit of generative models to discriminative unknown detection. As a type of generative model, energy-based models (EBM) are the potential for hybrid modeling of generative and discriminative tasks. However, most existing EBMs suffer from density estimation in high-dimensional space, which is critical to recognizing images from fine-grained classes. In this paper, we explore the low-dimensional latent space with energy-based prior distribution for OSR in a fine-grained visual world. Specifically, based on the latent space EBM, we propose an attribute-aware information bottleneck (AIB), a residual attribute feature aggregation (RAFA) module, and an uncertainty-based virtual outlier synthesis (UVOS) module to improve the expressivity, granularity, and density of the samples in fine-grained classes, respectively. Our method is flexible to take advantage of recent vision transformers for powerful visual classification and generation. The method is validated on both fine-grained and general visual classification datasets while preserving the capability of generating photo-realistic fake images with high resolution.
</details>
<details>
<summary>摘要</summary>
Translation notes:* Fine-grained open-set recognition (FineOSR) 精细开放类识别 (FineOSR)* aims to recognize images belonging to classes with subtle appearance differences 目标是识别具有微妙的外观差异的图像* while rejecting images of unknown classes 并拒绝未知类图像* A recent trend in OSR shows the benefit of generative models for unknown detection 最近的 OSR 趋势表明生成模型对未知检测具有优势* Energy-based models (EBMs) 能量基于模型 (EBMs)* are a type of generative model with potential for hybrid modeling of generative and discriminative tasks 是一种可以混合生成和抑制任务的生成模型* but most existing EBMs suffer from density estimation in high-dimensional space 但大多数现有的 EBM 在高维空间中进行密度估计存在问题* which is critical for recognizing images from fine-grained classes 这是识别精细类图像的关键问题* In this paper, we explore the low-dimensional latent space with energy-based prior distribution for OSR in a fine-grained visual world 在这篇论文中，我们在精细视觉世界中采用能量基于分布来探索低维 latent space，以实现 OSR* Specifically, based on the latent space EBM, we propose an attribute-aware information bottleneck (AIB) 具体来说，基于 latent space EBM，我们提出了 attribute-aware information bottleneck (AIB)* a residual attribute feature aggregation (RAFA) module 剩余特征特征聚合模块 (RAFA)* and an uncertainty-based virtual outlier synthesis (UVOS) module 基于不确定性的虚拟异常生成模块 (UVOS)* to improve the expressivity, granularity, and density of the samples in fine-grained classes 以提高精细类样本的表达能力、粒度和密度* Our method is flexible to take advantage of recent vision transformers for powerful visual classification and generation 我们的方法可以利用最近的视觉转换器来实现强大的视觉分类和生成* The method is validated on both fine-grained and general visual classification datasets while preserving the capability of generating photo-realistic fake images with high resolution 方法在精细和通用视觉分类数据集上进行验证，并保持生成高分辨率的图像的能力
</details></li>
</ul>
<hr>
<h2 id="ReShader-View-Dependent-Highlights-for-Single-Image-View-Synthesis"><a href="#ReShader-View-Dependent-Highlights-for-Single-Image-View-Synthesis" class="headerlink" title="ReShader: View-Dependent Highlights for Single Image View-Synthesis"></a>ReShader: View-Dependent Highlights for Single Image View-Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10689">http://arxiv.org/abs/2309.10689</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/avinashpaliwal/ReShader">https://github.com/avinashpaliwal/ReShader</a></li>
<li>paper_authors: Avinash Paliwal, Brandon Nguyen, Andrii Tsarov, Nima Khademi Kalantari</li>
<li>for: 提高单图像新视图synthesizer的可靠性和准确性</li>
<li>methods: 分解视图合成过程为两个独立任务：像素reshading和重定位</li>
<li>results: 生成具有真实运动高光的新视图图像，在多种真实场景上进行证明<details>
<summary>Abstract</summary>
In recent years, novel view synthesis from a single image has seen significant progress thanks to the rapid advancements in 3D scene representation and image inpainting techniques. While the current approaches are able to synthesize geometrically consistent novel views, they often do not handle the view-dependent effects properly. Specifically, the highlights in their synthesized images usually appear to be glued to the surfaces, making the novel views unrealistic. To address this major problem, we make a key observation that the process of synthesizing novel views requires changing the shading of the pixels based on the novel camera, and moving them to appropriate locations. Therefore, we propose to split the view synthesis process into two independent tasks of pixel reshading and relocation. During the reshading process, we take the single image as the input and adjust its shading based on the novel camera. This reshaded image is then used as the input to an existing view synthesis method to relocate the pixels and produce the final novel view image. We propose to use a neural network to perform reshading and generate a large set of synthetic input-reshaded pairs to train our network. We demonstrate that our approach produces plausible novel view images with realistic moving highlights on a variety of real world scenes.
</details>
<details>
<summary>摘要</summary>
近年来，从单一图像 synthesize 新视图的技术得到了重要进步，归功于三维场景表示和图像填充技术的快速进步。当前的方法可以生成具有正确的几何匹配的新视图，但它们通常不能正确处理视角依赖的效果。具体来说，它们生成的新视图中的高光通常会黏附到表面上，使得新视图变得不真实。为解决这一主要问题，我们作出了关键的观察：在synthesize 新视图过程中，需要根据新的摄像机改变像素的颜色和位置。因此，我们提议将视图synthesize 过程分解为两个独立任务：像素重新灰度和重新位置。在重新灰度过程中，我们使用单一图像作为输入，并根据新的摄像机进行颜色调整。这个重新灰度后的图像然后作为输入给现有的视图synthesize 方法，以生成最终的新视图图像。我们提议使用神经网络来进行重新灰度，并生成大量的人工生成的输入-重新灰度对来训练我们的网络。我们示例了我们的方法可以在多种真实场景上生成真实的高光移动的新视图图像。
</details></li>
</ul>
<hr>
<h2 id="CMRxRecon-An-open-cardiac-MRI-dataset-for-the-competition-of-accelerated-image-reconstruction"><a href="#CMRxRecon-An-open-cardiac-MRI-dataset-for-the-competition-of-accelerated-image-reconstruction" class="headerlink" title="CMRxRecon: An open cardiac MRI dataset for the competition of accelerated image reconstruction"></a>CMRxRecon: An open cardiac MRI dataset for the competition of accelerated image reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10836">http://arxiv.org/abs/2309.10836</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengyan Wang, Jun Lyu, Shuo Wang, Chen Qin, Kunyuan Guo, Xinyu Zhang, Xiaotong Yu, Yan Li, Fanwen Wang, Jianhua Jin, Zhang Shi, Ziqiang Xu, Yapeng Tian, Sha Hua, Zhensen Chen, Meng Liu, Mengting Sun, Xutong Kuang, Kang Wang, Haoran Wang, Hao Li, Yinghua Chu, Guang Yang, Wenjia Bai, Xiahai Zhuang, He Wang, Jing Qin, Xiaobo Qu</li>
<li>for:  This paper aims to facilitate the advancement of state-of-the-art cardiac magnetic resonance imaging (CMR) image reconstruction.</li>
<li>methods: The paper provides a large dataset of multi-contrast, multi-view, multi-slice, and multi-coil CMR imaging data from 300 subjects, which can be used to train and evaluate deep learning-based image reconstruction algorithms.</li>
<li>results: The dataset includes manual segmentations of the myocardium and chambers of all the subjects, and scripts of state-of-the-art reconstruction algorithms are provided as a point of reference. The dataset is freely accessible to the research community and can be accessed at <a target="_blank" rel="noopener" href="https://www.synapse.org/#!Synapse:syn51471091/wiki/">https://www.synapse.org/#!Synapse:syn51471091/wiki/</a>.<details>
<summary>Abstract</summary>
Cardiac magnetic resonance imaging (CMR) has emerged as a valuable diagnostic tool for cardiac diseases. However, a limitation of CMR is its slow imaging speed, which causes patient discomfort and introduces artifacts in the images. There has been growing interest in deep learning-based CMR imaging algorithms that can reconstruct high-quality images from highly under-sampled k-space data. However, the development of deep learning methods requires large training datasets, which have not been publicly available for CMR. To address this gap, we released a dataset that includes multi-contrast, multi-view, multi-slice and multi-coil CMR imaging data from 300 subjects. Imaging studies include cardiac cine and mapping sequences. Manual segmentations of the myocardium and chambers of all the subjects are also provided within the dataset. Scripts of state-of-the-art reconstruction algorithms were also provided as a point of reference. Our aim is to facilitate the advancement of state-of-the-art CMR image reconstruction by introducing standardized evaluation criteria and making the dataset freely accessible to the research community. Researchers can access the dataset at https://www.synapse.org/#!Synapse:syn51471091/wiki/.
</details>
<details>
<summary>摘要</summary>
卡ди亚克力磁共振成像（CMR）已成为心血管疾病诊断工具之一。然而，CMR的成像速度过慢，会让患者感到不适，并且会导致图像中的噪声和抖抖。随着深度学习技术的发展，深度学习基于CMR成像算法已经得到了广泛的关注。然而，深度学习的开发需要大量的训练数据集，而这些数据集在CMR领域并没有公开。为了解决这个问题，我们公开了一个包含多种对比、多视图、多层和多极的CMR成像数据集，其中包括300名参与者的数据。这些数据包括心血管磁共振和映射序列。参与者的心肺部分手动分割也包含在数据集中。我们提供了一些state-of-the-art reconstruction algorithm的脚本，作为参考。我们的目标是通过引入标准评估标准和公开数据集，推动CMR成像图像重建的技术发展。研究人员可以通过以下链接获取数据集：https://www.synapse.org/#!Synapse:syn51471091/wiki/。
</details></li>
</ul>
<hr>
<h2 id="Locally-Stylized-Neural-Radiance-Fields"><a href="#Locally-Stylized-Neural-Radiance-Fields" class="headerlink" title="Locally Stylized Neural Radiance Fields"></a>Locally Stylized Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10684">http://arxiv.org/abs/2309.10684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hong-Wing Pang, Binh-Son Hua, Sai-Kit Yeung</li>
<li>for: 将样式应用到3D场景中，特别是使用神经辐射场（NeRF）。</li>
<li>methods: 使用本地样式传递来实现样式化，使用HashGrid编码学习外观和geometry组件的嵌入，并通过优化外观分支来实现样式化。</li>
<li>results: 实现了可信度高的样式化结果，同时具有可控的个性化特性，通过修改和定制地域匹配来实现本地样式传递。<details>
<summary>Abstract</summary>
In recent years, there has been increasing interest in applying stylization on 3D scenes from a reference style image, in particular onto neural radiance fields (NeRF). While performing stylization directly on NeRF guarantees appearance consistency over arbitrary novel views, it is a challenging problem to guide the transfer of patterns from the style image onto different parts of the NeRF scene. In this work, we propose a stylization framework for NeRF based on local style transfer. In particular, we use a hash-grid encoding to learn the embedding of the appearance and geometry components, and show that the mapping defined by the hash table allows us to control the stylization to a certain extent. Stylization is then achieved by optimizing the appearance branch while keeping the geometry branch fixed. To support local style transfer, we propose a new loss function that utilizes a segmentation network and bipartite matching to establish region correspondences between the style image and the content images obtained from volume rendering. Our experiments show that our method yields plausible stylization results with novel view synthesis while having flexible controllability via manipulating and customizing the region correspondences.
</details>
<details>
<summary>摘要</summary>
近年来，人们对于将参照风格图像应用到3D场景上的涂鸦问题越来越感兴趣，尤其是在神经辐射场（NeRF）上。虽然直接在NeRF上进行涂鸦可以保证视野中任意新视图的外观一致，但是将模式图像中的模式转移到不同的NeRF场景部分是一个具有挑战性的问题。在这项工作中，我们提出了基于本地风格传递的NeRF涂鸦框架。具体来说，我们使用Hash网格编码学习外观和几何component的嵌入，并证明Hash表定义的映射允许一定程度的控制涂鸦。然后，我们通过优化外观支线来实现涂鸦，保持几何支线不变。为支持本地风格传递，我们提出了一种新的损失函数，利用分割网络和两个对偶匹配来建立风格图像和内容图像从volume渲染得到的区域对应关系。我们的实验显示，我们的方法可以实现有效的涂鸦结果，同时具有自由控制的便利性，通过操作和自定义区域对应关系来控制涂鸦效果。
</details></li>
</ul>
<hr>
<h2 id="Learning-Tri-modal-Embeddings-for-Zero-Shot-Soundscape-Mapping"><a href="#Learning-Tri-modal-Embeddings-for-Zero-Shot-Soundscape-Mapping" class="headerlink" title="Learning Tri-modal Embeddings for Zero-Shot Soundscape Mapping"></a>Learning Tri-modal Embeddings for Zero-Shot Soundscape Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10667">http://arxiv.org/abs/2309.10667</a></li>
<li>repo_url: None</li>
<li>paper_authors: Subash Khanal, Srikumar Sastry, Aayush Dhakal, Nathan Jacobs</li>
<li>for: 预测特定地理位置可能听到的各种声音</li>
<li>methods: 使用最新的状态艺术模型对地标Audio、文本描述和捕捉位置的拓展图像进行对比预训练，从而建立三个模式之间的共享嵌入空间，以便从文本或音频查询中构建任何地理区域的声cape图</li>
<li>results: 使用SoundingEarth数据集，我们的方法与现有SOTA之间有显著的改进，Image-to-Audio Recall@100从0.256提高到0.450。<details>
<summary>Abstract</summary>
We focus on the task of soundscape mapping, which involves predicting the most probable sounds that could be perceived at a particular geographic location. We utilise recent state-of-the-art models to encode geotagged audio, a textual description of the audio, and an overhead image of its capture location using contrastive pre-training. The end result is a shared embedding space for the three modalities, which enables the construction of soundscape maps for any geographic region from textual or audio queries. Using the SoundingEarth dataset, we find that our approach significantly outperforms the existing SOTA, with an improvement of image-to-audio Recall@100 from 0.256 to 0.450. Our code is available at https://github.com/mvrl/geoclap.
</details>
<details>
<summary>摘要</summary>
我团队的任务是声景地图，即预测特定地理位置可能被感受到的最有可能的声音。我们利用最新的状态艺术模型来编码地标注的音频、文本描述和捕捉位置的遮盲图像，并通过对比预训练来共享这三种模态的 embeddings 空间。这使得我们可以从文本或音频查询中构建任何地理区域的声景地图。使用 SoundingEarth 数据集，我们发现我们的方法与现有的 SOTA 有明显的改善，即图像到音频 Recall@100 从 0.256 提高到 0.450。我们的代码可以在 GitHub 上找到：https://github.com/mvrl/geoclap。
</details></li>
</ul>
<hr>
<h2 id="Analysing-race-and-sex-bias-in-brain-age-prediction"><a href="#Analysing-race-and-sex-bias-in-brain-age-prediction" class="headerlink" title="Analysing race and sex bias in brain age prediction"></a>Analysing race and sex bias in brain age prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10835">http://arxiv.org/abs/2309.10835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carolina Piçarra, Ben Glocker</li>
<li>for: 这paper的目的是分析使用MRI进行脑年龄预测的模型是否受到人口结构的偏见。</li>
<li>methods: 这paper使用了ResNet-34模型，通过对不同人口结构下的子群进行分析，以及对特征进行检查，以确定模型是否受到偏见。</li>
<li>results: 研究发现，模型在不同人口结构下的预测性能存在 statistically significant differences，并且发现了7对12个对比的特征分布存在 statistically significant differences。这些结果表明脑年龄预测模型可能受到偏见。<details>
<summary>Abstract</summary>
Brain age prediction from MRI has become a popular imaging biomarker associated with a wide range of neuropathologies. The datasets used for training, however, are often skewed and imbalanced regarding demographics, potentially making brain age prediction models susceptible to bias. We analyse the commonly used ResNet-34 model by conducting a comprehensive subgroup performance analysis and feature inspection. The model is trained on 1,215 T1-weighted MRI scans from Cam-CAN and IXI, and tested on UK Biobank (n=42,786), split into six racial and biological sex subgroups. With the objective of comparing the performance between subgroups, measured by the absolute prediction error, we use a Kruskal-Wallis test followed by two post-hoc Conover-Iman tests to inspect bias across race and biological sex. To examine biases in the generated features, we use PCA for dimensionality reduction and employ two-sample Kolmogorov-Smirnov tests to identify distribution shifts among subgroups. Our results reveal statistically significant differences in predictive performance between Black and White, Black and Asian, and male and female subjects. Seven out of twelve pairwise comparisons show statistically significant differences in the feature distributions. Our findings call for further analysis of brain age prediction models.
</details>
<details>
<summary>摘要</summary>
Magnetic Resonance Imaging (MRI) 年龄预测已成为脑科疾病相关的快速成像标记，但是用于训练的数据集经常受到人口结构的偏袋和不均衡影响，可能使脑年龄预测模型受到偏见。我们使用常用的ResNet-34模型进行全面的子组表现分析和特征检查。该模型在Cam-CAN和IXI数据集上训练，并在UK Biobank数据集（n=42,786）上进行测试，并将数据集分为六个种族和生物性别子组。通过对各子组的绝对预测误差进行比较，我们使用克鲁斯卡尔-沃利斯测试后跟进行了两次康维-伊曼测试来检测种族和生物性别之间的偏见。为了检测特征生成的偏见，我们使用PCA进行维度减少，并使用两种样本的科洛摩戈罗夫-斯米纳夫测试来发现分布偏移。我们的结果显示，黑人和白人、黑人和亚裔人、男性和女性之间存在 statistically significant 的差异。在十二个对比中，有七个对比显示 statistically significant 的分布偏移。我们的发现呼吁进一步分析脑年龄预测模型。
</details></li>
</ul>
<hr>
<h2 id="Multi-Stain-Self-Attention-Graph-Multiple-Instance-Learning-Pipeline-for-Histopathology-Whole-Slide-Images"><a href="#Multi-Stain-Self-Attention-Graph-Multiple-Instance-Learning-Pipeline-for-Histopathology-Whole-Slide-Images" class="headerlink" title="Multi-Stain Self-Attention Graph Multiple Instance Learning Pipeline for Histopathology Whole Slide Images"></a>Multi-Stain Self-Attention Graph Multiple Instance Learning Pipeline for Histopathology Whole Slide Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10650">http://arxiv.org/abs/2309.10650</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amayags/mustang">https://github.com/amayags/mustang</a></li>
<li>paper_authors: Amaya Gallagher-Syed, Luca Rossi, Felice Rivellese, Costantino Pitzalis, Myles Lewis, Michael Barnes, Gregory Slabaugh</li>
<li>for: 本研究旨在 Addressing the challenges of weakly supervised computer vision tasks in gigapixel Whole Slide Images (WSIs) for patient diagnosis and stratification.</li>
<li>methods: 我们提出了一种基于自注意力的多例学习框架（MUSTANG），用于解决不具有批处理级别标注的多个WSIs的分类任务。我们使用了一种快速计算的k-Nearest Neighbour Graph，以实现快速的自注意力计算。</li>
<li>results: 我们的方法在实验中取得了当今最佳的F1分数&#x2F;AUC分数0.89&#x2F;0.92，超过了广泛使用的CLAM模型。我们的方法可以轻松地适应不同的临床数据集，只需要patient级别的标注，并可以接受不同的WSIs集合，Graph可以是不同的大小和结构。<details>
<summary>Abstract</summary>
Whole Slide Images (WSIs) present a challenging computer vision task due to their gigapixel size and presence of numerous artefacts. Yet they are a valuable resource for patient diagnosis and stratification, often representing the gold standard for diagnostic tasks. Real-world clinical datasets tend to come as sets of heterogeneous WSIs with labels present at the patient-level, with poor to no annotations. Weakly supervised attention-based multiple instance learning approaches have been developed in recent years to address these challenges, but can fail to resolve both long and short-range dependencies. Here we propose an end-to-end multi-stain self-attention graph (MUSTANG) multiple instance learning pipeline, which is designed to solve a weakly-supervised gigapixel multi-image classification task, where the label is assigned at the patient-level, but no slide-level labels or region annotations are available. The pipeline uses a self-attention based approach by restricting the operations to a highly sparse k-Nearest Neighbour Graph of embedded WSI patches based on the Euclidean distance. We show this approach achieves a state-of-the-art F1-score/AUC of 0.89/0.92, outperforming the widely used CLAM model. Our approach is highly modular and can easily be modified to suit different clinical datasets, as it only requires a patient-level label without annotations and accepts WSI sets of different sizes, as the graphs can be of varying sizes and structures. The source code can be found at https://github.com/AmayaGS/MUSTANG.
</details>
<details>
<summary>摘要</summary>
整幅扫描图像（WSIs）对计算机视觉 задача呈现挑战，因其 гига灵ixel大小和存在许多artefacts。然而，它们对患者诊断和分类非常有价值，通常被视为诊断任务的金标准。实际的临床数据集通常是一组不同类型的WSIs，每个扫描图像都有patient级别的标签，但是标签的质量很差，甚至没有任何注解。弱式监督的注意力基本学习方法在过去几年中开发出来，但是它们可能无法解决长距离和短距离依赖关系。我们提出了一个终端到终Multi-stain自动注意力图（MUSTANG）多个实例学习管道，用于解决弱式监督的гига灵ixel多图分类任务，任务标签是在患者级别上分配，但是没有扫描图像级别的标签或区域注解。管道使用基于Euclidean距离的自动注意力方法，限制操作于高度稀疏的k-最近邻 Graph Embedded WSI patches中。我们显示，这种方法可以达到状态机器人F1-score/AUC的0.89/0.92，超过了广泛使用的CLAM模型。我们的方法非常可 modify，可以轻松地适应不同的临床数据集，只需要patient级别的标签，不需要扫描图像级别的注解，并且可以接受不同大小和结构的WSIs集。源代码可以在https://github.com/AmayaGS/MUSTANG 中找到。
</details></li>
</ul>
<hr>
<h2 id="Cross-modal-and-Cross-domain-Knowledge-Transfer-for-Label-free-3D-Segmentation"><a href="#Cross-modal-and-Cross-domain-Knowledge-Transfer-for-Label-free-3D-Segmentation" class="headerlink" title="Cross-modal and Cross-domain Knowledge Transfer for Label-free 3D Segmentation"></a>Cross-modal and Cross-domain Knowledge Transfer for Label-free 3D Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10649">http://arxiv.org/abs/2309.10649</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingyu Zhang, Huitong Yang, Daijie Wu, Xuesong Li, Xinge Zhu, Yuexin Ma</li>
<li>for: 提高3D点云Semantic Segmentation的性能，不需要大量的标注数据。</li>
<li>methods: 基于图像和点云之间的关系，设计有效的特征对齐策略，实现交叉模式和交叉领域的适应。</li>
<li>results: 无需3D标注数据，我们的方法在SemanticKITTI上达到了state-of-the-art表现，比对 existedUnsupervised和Weakly-supervised基eline的性能更高。<details>
<summary>Abstract</summary>
Current state-of-the-art point cloud-based perception methods usually rely on large-scale labeled data, which requires expensive manual annotations. A natural option is to explore the unsupervised methodology for 3D perception tasks. However, such methods often face substantial performance-drop difficulties. Fortunately, we found that there exist amounts of image-based datasets and an alternative can be proposed, i.e., transferring the knowledge in the 2D images to 3D point clouds. Specifically, we propose a novel approach for the challenging cross-modal and cross-domain adaptation task by fully exploring the relationship between images and point clouds and designing effective feature alignment strategies. Without any 3D labels, our method achieves state-of-the-art performance for 3D point cloud semantic segmentation on SemanticKITTI by using the knowledge of KITTI360 and GTA5, compared to existing unsupervised and weakly-supervised baselines.
</details>
<details>
<summary>摘要</summary>
当前最先进的点云基于识别方法通常依赖于大规模的标注数据，这需要昂贵的人工标注。一种自然的选择是探索无监督的方法学习。然而，这些方法往往面临重大性能下降的困难。幸运地，我们发现了大量的图像数据集和一种代替方案，即将图像知识传播到点云中。我们提出了一种困难的对Modal和对Domain的适应任务，通过全面探索图像和点云之间的关系和设计有效的特征对齐策略。无需任何3D标注，我们的方法在SemanticKITTI上实现了无监督性的3D点云 semantic segmentation的国际级表现，与现有的无监督和弱监督基准相比。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Super-Resolution-Approach-for-Isotropic-Reconstruction-of-3D-Electron-Microscopy-Images-from-Anisotropic-Acquisition"><a href="#Self-Supervised-Super-Resolution-Approach-for-Isotropic-Reconstruction-of-3D-Electron-Microscopy-Images-from-Anisotropic-Acquisition" class="headerlink" title="Self-Supervised Super-Resolution Approach for Isotropic Reconstruction of 3D Electron Microscopy Images from Anisotropic Acquisition"></a>Self-Supervised Super-Resolution Approach for Isotropic Reconstruction of 3D Electron Microscopy Images from Anisotropic Acquisition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10646">http://arxiv.org/abs/2309.10646</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Khateri, Morteza Ghahremani, Alejandra Sierra, Jussi Tohka</li>
<li>for: 用于重建三维电子顾 microscopy（3DEM）图像的均匀性。</li>
<li>methods: 使用深度学习（DL）自主超分解方法，利用U型架构和视图变换（ViT）块，学习多级图像依赖关系。</li>
<li>results: 成功重建均匀的3DEM图像从不均匀的获取图像中。<details>
<summary>Abstract</summary>
Three-dimensional electron microscopy (3DEM) is an essential technique to investigate volumetric tissue ultra-structure. Due to technical limitations and high imaging costs, samples are often imaged anisotropically, where resolution in the axial direction ($z$) is lower than in the lateral directions $(x,y)$. This anisotropy 3DEM can hamper subsequent analysis and visualization tasks. To overcome this limitation, we propose a novel deep-learning (DL)-based self-supervised super-resolution approach that computationally reconstructs isotropic 3DEM from the anisotropic acquisition. The proposed DL-based framework is built upon the U-shape architecture incorporating vision-transformer (ViT) blocks, enabling high-capability learning of local and global multi-scale image dependencies. To train the tailored network, we employ a self-supervised approach. Specifically, we generate pairs of anisotropic and isotropic training datasets from the given anisotropic 3DEM data. By feeding the given anisotropic 3DEM dataset in the trained network through our proposed framework, the isotropic 3DEM is obtained. Importantly, this isotropic reconstruction approach relies solely on the given anisotropic 3DEM dataset and does not require pairs of co-registered anisotropic and isotropic 3DEM training datasets. To evaluate the effectiveness of the proposed method, we conducted experiments using three 3DEM datasets acquired from brain. The experimental results demonstrated that our proposed framework could successfully reconstruct isotropic 3DEM from the anisotropic acquisition.
</details>
<details>
<summary>摘要</summary>
三维电子镜像技术（3DEM）是诊断组织结构的重要方法。由于技术限制和高成本镜像，样本通常在axial方向（z）中的分辨率较低，而在 lateral方向（x, y）中的分辨率高。这种不均匀镜像可能会降低后续分析和视觉任务的效果。为了解决这个问题，我们提出了一种基于深度学习（DL）的自动适应超分辨率方法。这种方法基于U形架构，并包括视觉转换（ViT）块，可以高效地学习本地和全局多尺度图像依赖关系。为了训练专门的网络，我们采用了自动适应的方法。具体来说，我们生成了具有不均匀和均匀特征的训练集。通过将给定的不均匀3DEM数据feed到我们的提posed框架中，可以获得均匀的3DEM。重要的是，这种均匀重建方法不需要对于不均匀和均匀3DEM训练集的对应的数据对。为了评估我们的方法的效果，我们使用了三个由脑组织的3DEM数据进行实验。实验结果表明，我们的提posed方法可以成功地从不均匀镜像中重建均匀的3DEM。
</details></li>
</ul>
<hr>
<h2 id="KFC-Kinship-Verification-with-Fair-Contrastive-Loss-and-Multi-Task-Learning"><a href="#KFC-Kinship-Verification-with-Fair-Contrastive-Loss-and-Multi-Task-Learning" class="headerlink" title="KFC: Kinship Verification with Fair Contrastive Loss and Multi-Task Learning"></a>KFC: Kinship Verification with Fair Contrastive Loss and Multi-Task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10641">http://arxiv.org/abs/2309.10641</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/garynlfd/kfc">https://github.com/garynlfd/kfc</a></li>
<li>paper_authors: Jia Luo Peng, Keng Wei Chang, Shang-Hong Lai</li>
<li>For: The paper is written for the task of kinship verification in computer vision, with the goal of achieving better performance and mitigating racial bias.* Methods: The paper proposes a multi-task learning model structure with an attention module and a fairness-aware contrastive loss function with adversarial learning to enhance accuracy and reduce bias.* Results: The proposed method, called KFC, achieves state-of-the-art performance and mitigates racial bias, as demonstrated through extensive experimental evaluation.Here’s the information in Simplified Chinese text:</li>
<li>for: 本文是为计算机视觉中的身份验证任务所写的，目的是实现更好的性能和减少种族偏见。</li>
<li>methods: 本文提议一种多任务学习模型结构，具有注意力模块和公平意识的对比损失函数，以提高准确率和减少偏见。</li>
<li>results: 提议的方法（KFC）在标准差和准确率两个指标上具有优秀的性能，并成功减少种族偏见，经过了广泛的实验评估。<details>
<summary>Abstract</summary>
Kinship verification is an emerging task in computer vision with multiple potential applications. However, there's no large enough kinship dataset to train a representative and robust model, which is a limitation for achieving better performance. Moreover, face verification is known to exhibit bias, which has not been dealt with by previous kinship verification works and sometimes even results in serious issues. So we first combine existing kinship datasets and label each identity with the correct race in order to take race information into consideration and provide a larger and complete dataset, called KinRace dataset. Secondly, we propose a multi-task learning model structure with attention module to enhance accuracy, which surpasses state-of-the-art performance. Lastly, our fairness-aware contrastive loss function with adversarial learning greatly mitigates racial bias. We introduce a debias term into traditional contrastive loss and implement gradient reverse in race classification task, which is an innovative idea to mix two fairness methods to alleviate bias. Exhaustive experimental evaluation demonstrates the effectiveness and superior performance of the proposed KFC in both standard deviation and accuracy at the same time.
</details>
<details>
<summary>摘要</summary>
《家庭关系验证是计算机视觉领域的一个emerging任务，它具有多种应用前景。然而，当前没有一个大 enough的家庭关系数据集来训练代表性强的模型，这是实现更好的性能的限制。更重要的是，脸部验证已经展现出偏见，这些偏见没有被前一些家庭关系验证工作所处理，有时甚至会导致严重的问题。因此，我们首先将现有的家庭关系数据集合并标注每个人的正确的种族信息，以便考虑种族信息并提供更完整的数据集，我们称之为KinRace数据集。其次，我们提议一种多任务学习模型结构，并在其中添加了注意模块，以提高准确率。最后，我们提出了一种公平意识的抽象损失函数，并通过对涉及到种族的任务进行逆向梯度的实现，这是一种创新的方法来缓解偏见。我们的方法可以同时提高标准差和准确率。我们进行了广泛的实验评估，并证明了我们的提案的效iveness和超越性。》Note: Please note that the translation is in Simplified Chinese, which is used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Sparser-Random-Networks-Exist-Enforcing-Communication-Efficient-Federated-Learning-via-Regularization"><a href="#Sparser-Random-Networks-Exist-Enforcing-Communication-Efficient-Federated-Learning-via-Regularization" class="headerlink" title="Sparser Random Networks Exist: Enforcing Communication-Efficient Federated Learning via Regularization"></a>Sparser Random Networks Exist: Enforcing Communication-Efficient Federated Learning via Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10834">http://arxiv.org/abs/2309.10834</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamad Mestoukirdi, Omid Esrafilian, David Gesbert, Qianrui Li, Nicolas Gresset</li>
<li>for: 本文提出了一种新的方法，用于提高随机联合学习中的通信效率，特别是在训练过参数化的随机网络时。</li>
<li>methods: 在本文中，我们使用了一个 binary 面板来优化，而不是直接优化模型的权重。这个面板可以描述一个可以通过少量参数来泛化的减小网络。与传统联合学习方法不同，我们在交换时使用了简单的 binary 数据，而不是浮点数据。这有效地减少了通信成本，最多只需要1比特每个参数。</li>
<li>results: 我们的实验表明，在使用了本文提出的方法后，可以获得 significan 的通信和存储开销减少，达到最多5 magnitudes，同时Validation 精度也可以保持在一定程度上。<details>
<summary>Abstract</summary>
This work presents a new method for enhancing communication efficiency in stochastic Federated Learning that trains over-parameterized random networks. In this setting, a binary mask is optimized instead of the model weights, which are kept fixed. The mask characterizes a sparse sub-network that is able to generalize as good as a smaller target network. Importantly, sparse binary masks are exchanged rather than the floating point weights in traditional federated learning, reducing communication cost to at most 1 bit per parameter. We show that previous state of the art stochastic methods fail to find the sparse networks that can reduce the communication and storage overhead using consistent loss objectives. To address this, we propose adding a regularization term to local objectives that encourages sparser solutions by eliminating redundant features across sub-networks. Extensive experiments demonstrate significant improvements in communication and memory efficiency of up to five magnitudes compared to the literature, with minimal performance degradation in validation accuracy in some instances.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:这个工作提出了一种新的方法，用于提高Stochastic Federated Learning中的通信效率，该方法在训练过 parametrization 的随机网络时使用。而不是优化模型的权重，这里优化一个二进制的maske，以Characterize一个可以与小型目标网络一样准确预测的稀疏子网络。这种方法可以将通信成本降至每个参数最多1比特，因为在传输的是稀疏二进制maske而不是浮点数据。previous state of the art stochastic方法无法找到可以减少通信和存储开销的稀疏网络，使用一致损失函数。为解决这个问题，我们提出了添加一个正则化项到本地目标函数中，以逼导更稀疏的解决方案，从而消除各个子网络之间的重复特征。广泛的实验表明，我们的方法可以在一些实例中实现到5 magnitudes的通信和存储效率提升，同时减少了验证精度下降。
</details></li>
</ul>
<hr>
<h2 id="Source-free-Active-Domain-Adaptation-for-Diabetic-Retinopathy-Grading-Based-on-Ultra-wide-field-Fundus-Image"><a href="#Source-free-Active-Domain-Adaptation-for-Diabetic-Retinopathy-Grading-Based-on-Ultra-wide-field-Fundus-Image" class="headerlink" title="Source-free Active Domain Adaptation for Diabetic Retinopathy Grading Based on Ultra-wide-field Fundus Image"></a>Source-free Active Domain Adaptation for Diabetic Retinopathy Grading Based on Ultra-wide-field Fundus Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10619">http://arxiv.org/abs/2309.10619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinye Ran, Guanghua Zhang, Ximei Zhang, Juan Xie, Fan Xia, Hao Zhang<br>for: 这篇研究目的是为了提高无标注宽场照片中的脓瘤评分性能。methods: 这篇研究使用了源自由活动领域适应（SFADA）技术，通过生成颜色照片中的脓瘤关系演化，选择一些有价值的宽场照片进行标注，并将模型适应到宽场照片上。results: 实验结果显示，我们的提案的SFADA可以达到现场临床实践中的最佳脓瘤评分性能，比基准值提高20.9%和二次均值权重卡佛洛値18.63%，分别达到85.36%和92.38%。<details>
<summary>Abstract</summary>
Domain adaptation (DA) has been widely applied in the diabetic retinopathy (DR) grading of unannotated ultra-wide-field (UWF) fundus images, which can transfer annotated knowledge from labeled color fundus images. However, suffering from huge domain gaps and complex real-world scenarios, the DR grading performance of most mainstream DA is far from that of clinical diagnosis. To tackle this, we propose a novel source-free active domain adaptation (SFADA) in this paper. Specifically, we focus on DR grading problem itself and propose to generate features of color fundus images with continuously evolving relationships of DRs, actively select a few valuable UWF fundus images for labeling with local representation matching, and adapt model on UWF fundus images with DR lesion prototypes. Notably, the SFADA also takes data privacy and computational efficiency into consideration. Extensive experimental results demonstrate that our proposed SFADA achieves state-of-the-art DR grading performance, increasing accuracy by 20.9% and quadratic weighted kappa by 18.63% compared with baseline and reaching 85.36% and 92.38% respectively. These investigations show that the potential of our approach for real clinical practice is promising.
</details>
<details>
<summary>摘要</summary>
域 adaptation (DA) 已经广泛应用于无注释超宽场视场照片（UWF）的糖尿病症诊断中，可以将标注过的知识传递到颜色视场照片中。然而，由于巨大的域漏洞和复杂的实际情况，大多数主流 DA 的诊断性能远远不如临床诊断。为了解决这个问题，我们在这篇论文中提出了一种新的源自由活动域 adaptation（SFADA）。我们专注于糖尿病诊断问题，并提出了生成颜色视场照片中的糖尿病关系的演化方法，并选择一些有价值的 UWF 照片进行本地匹配标注，并将模型适应到 UWF 照片中的糖尿病肉体抽象。需要注意的是，SFADA 还考虑了数据隐私和计算效率。我们的实验结果表明，我们的提议的 SFADA 可以达到状态机器的糖尿病诊断性能，相比基线提高了20.9%和quadratic weighted kappa 18.63%，分别达到85.36%和92.38%。这些调查表明了我们的方法在实际临床实践中的潜在潜力。
</details></li>
</ul>
<hr>
<h2 id="Intelligent-Debris-Mass-Estimation-Model-for-Autonomous-Underwater-Vehicle"><a href="#Intelligent-Debris-Mass-Estimation-Model-for-Autonomous-Underwater-Vehicle" class="headerlink" title="Intelligent Debris Mass Estimation Model for Autonomous Underwater Vehicle"></a>Intelligent Debris Mass Estimation Model for Autonomous Underwater Vehicle</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10617">http://arxiv.org/abs/2309.10617</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohana Sri S, Swethaa S, Aouthithiye Barathwaj SR Y, Sai Ganesh CS</li>
<li>for: 这个论文的目的是提高自动下水车（AUV）在水下环境中导航和交互的能力，使用实例分割技术来分割图像中的对象。</li>
<li>methods: 这个论文使用的方法包括：YOLOV7在Roboflow中生成对象的 bounding box，将每个对象分割成不同的领域，并使用预处理技术来提高分割质量。</li>
<li>results: 这个论文的结果表明，使用实例分割技术可以准确地分割水下环境中的对象，并且可以提高AUV的导航和交互能力。<details>
<summary>Abstract</summary>
Marine debris poses a significant threat to the survival of marine wildlife, often leading to entanglement and starvation, ultimately resulting in death. Therefore, removing debris from the ocean is crucial to restore the natural balance and allow marine life to thrive. Instance segmentation is an advanced form of object detection that identifies objects and precisely locates and separates them, making it an essential tool for autonomous underwater vehicles (AUVs) to navigate and interact with their underwater environment effectively. AUVs use image segmentation to analyze images captured by their cameras to navigate underwater environments. In this paper, we use instance segmentation to calculate the area of individual objects within an image, we use YOLOV7 in Roboflow to generate a set of bounding boxes for each object in the image with a class label and a confidence score for every detection. A segmentation mask is then created for each object by applying a binary mask to the object's bounding box. The masks are generated by applying a binary threshold to the output of a convolutional neural network trained to segment objects from the background. Finally, refining the segmentation mask for each object is done by applying post-processing techniques such as morphological operations and contour detection, to improve the accuracy and quality of the mask. The process of estimating the area of instance segmentation involves calculating the area of each segmented instance separately and then summing up the areas of all instances to obtain the total area. The calculation is carried out using standard formulas based on the shape of the object, such as rectangles and circles. In cases where the object is complex, the Monte Carlo method is used to estimate the area. This method provides a higher degree of accuracy than traditional methods, especially when using a large number of samples.
</details>
<details>
<summary>摘要</summary>
海洋垃圾对海洋野生动物的存活造成了重要威胁，通常会导致拥挤和饥饿，最终导致死亡。因此，从海洋中除垃圾是保持自然平衡的关键，让海洋生物发展和繁殖。图像分割是高级形态检测的一种，可以准确地标识和分离对象，因此在自动下水潜车（AUV）在水下环境中Navigation和交互时非常重要。AUV使用图像分割来分析捕捉到的图像，以便在水下环境中 Navigation。在这篇论文中，我们使用图像分割来计算图像中每个对象的面积。我们使用YOLOV7在Roboflow中生成每个对象的 bounding box，并为每个检测得到一个分类标签和信任分数。然后，我们生成每个对象的分割面，通过应用一个二进制阈值来对对象的 bounding box 进行分割。这些面由一个基于对象分割的几何学模型训练而成。最后，我们对每个对象的分割面进行修正，使用Post处理技术，如形态运算和某些检测，以提高分割面的准确性和质量。图像分割面积的估计过程包括计算每个分割对象的面积，然后将所有对象的面积之和为总面积。计算使用标准的形态方程，根据对象的形状，如方形和圆形。在对象复杂时，我们使用蒙特卡洛方法来估计面积，这种方法可以提供更高的准确性，特别是使用大量样本。
</details></li>
</ul>
<hr>
<h2 id="NDDepth-Normal-Distance-Assisted-Monocular-Depth-Estimation"><a href="#NDDepth-Normal-Distance-Assisted-Monocular-Depth-Estimation" class="headerlink" title="NDDepth: Normal-Distance Assisted Monocular Depth Estimation"></a>NDDepth: Normal-Distance Assisted Monocular Depth Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10592">http://arxiv.org/abs/2309.10592</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuwei Shao, Zhongcai Pei, Weihai Chen, Xingming Wu, Zhengguo Li</li>
<li>for: 这个论文主要针对的是单目深度估计问题，它的广泛应用在计算机视觉领域。</li>
<li>methods: 该论文提出了一种基于物理学（几何学）的深度学习框架，假设3D场景由分割面组成。论文引入了一个新的normal-distance头，该头输出每个位置的像素级表面法向量和平面到起点的距离，以便从 depth 的估计。此外，normal和距离被通过开发的平面感知约束进行正则化。论文还增加了一个附加的深度头，以提高提案的稳定性。</li>
<li>results: 对于NYU-Depth-v2、KITTI和SUN RGB-D数据集，提案的方法超过了之前的状态态的竞争对手。特别是，在KITTI的深度预测在线竞赛上，提案的方法在提交时 ranked 1st 中所有提交。<details>
<summary>Abstract</summary>
Monocular depth estimation has drawn widespread attention from the vision community due to its broad applications. In this paper, we propose a novel physics (geometry)-driven deep learning framework for monocular depth estimation by assuming that 3D scenes are constituted by piece-wise planes. Particularly, we introduce a new normal-distance head that outputs pixel-level surface normal and plane-to-origin distance for deriving depth at each position. Meanwhile, the normal and distance are regularized by a developed plane-aware consistency constraint. We further integrate an additional depth head to improve the robustness of the proposed framework. To fully exploit the strengths of these two heads, we develop an effective contrastive iterative refinement module that refines depth in a complementary manner according to the depth uncertainty. Extensive experiments indicate that the proposed method exceeds previous state-of-the-art competitors on the NYU-Depth-v2, KITTI and SUN RGB-D datasets. Notably, it ranks 1st among all submissions on the KITTI depth prediction online benchmark at the submission time.
</details>
<details>
<summary>摘要</summary>
单目深度估算在视觉社区中引起了广泛的关注，因为它具有广泛的应用。在这篇论文中，我们提出了一个新的物理（几何）驱动的深度学习框架，assuming that 3D scenes are constituted by piece-wise planes。我们引入了一个新的normal-distance head，这个head将在每个位置输出像素级表面法向和平面到起始距离，以 derivation depth。同时，normal和距离被调整了一个发展的平面应相适应约束。我们还整合了一个额外的深度head，以提高我们的提案的稳定性。为了充分利用这两个head的优点，我们开发了一个有效的补充循环调整模块，这个模块会根据depth的不确定性进行调整。实验结果显示，我们的提案超过了前一代的竞争者在 NYU-Depth-v2、KITTI 和 SUN RGB-D 数据集上。特别是，它在 KITTI 对应测试上的线上测试benchmark中排名第一。
</details></li>
</ul>
<hr>
<h2 id="Few-shot-Object-Detection-in-Remote-Sensing-Lifting-the-Curse-of-Incompletely-Annotated-Novel-Objects"><a href="#Few-shot-Object-Detection-in-Remote-Sensing-Lifting-the-Curse-of-Incompletely-Annotated-Novel-Objects" class="headerlink" title="Few-shot Object Detection in Remote Sensing: Lifting the Curse of Incompletely Annotated Novel Objects"></a>Few-shot Object Detection in Remote Sensing: Lifting the Curse of Incompletely Annotated Novel Objects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10588">http://arxiv.org/abs/2309.10588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fahong Zhang, Yilei Shi, Zhitong Xiong, Xiao Xiang Zhu</li>
<li>for: 本研究的目的是提出一种基于自适应学习的几何卷积网络（ST-FSOD）方法，用于实现几何卷积网络在几何图像处理中的几何检测。</li>
<li>methods: 本研究使用了两个分支的Region Proposal Networks（RPN），其中一个分支用于提取基本对象的提案，另一个分支用于提取 noval 对象的提案。此外，本研究还使用了学生-教师机制，将高度自信的未标注目标作为pseudo标签，并将其包含在RPN和Region of Interest（RoI）头中。</li>
<li>results: 实验结果表明， compared with现有的state-of-the-art方法，本研究的ST-FSOD方法在各种几何检测设置下表现出了大幅提升。<details>
<summary>Abstract</summary>
Object detection is an essential and fundamental task in computer vision and satellite image processing. Existing deep learning methods have achieved impressive performance thanks to the availability of large-scale annotated datasets. Yet, in real-world applications the availability of labels is limited. In this context, few-shot object detection (FSOD) has emerged as a promising direction, which aims at enabling the model to detect novel objects with only few of them annotated. However, many existing FSOD algorithms overlook a critical issue: when an input image contains multiple novel objects and only a subset of them are annotated, the unlabeled objects will be considered as background during training. This can cause confusions and severely impact the model's ability to recall novel objects. To address this issue, we propose a self-training-based FSOD (ST-FSOD) approach, which incorporates the self-training mechanism into the few-shot fine-tuning process. ST-FSOD aims to enable the discovery of novel objects that are not annotated, and take them into account during training. On the one hand, we devise a two-branch region proposal networks (RPN) to separate the proposal extraction of base and novel objects, On another hand, we incorporate the student-teacher mechanism into RPN and the region of interest (RoI) head to include those highly confident yet unlabeled targets as pseudo labels. Experimental results demonstrate that our proposed method outperforms the state-of-the-art in various FSOD settings by a large margin. The codes will be publicly available at https://github.com/zhu-xlab/ST-FSOD.
</details>
<details>
<summary>摘要</summary>
Computer vision 和卫星图像处理中的对象检测是一项基础和重要任务。现有的深度学习方法在大规模标注数据的支持下已经达到了印象人的性能。然而，在实际应用中，标注数据的可用性受限。在这种情况下，几shot对象检测（FSOD）已经出现为一个有前途的方向，它目的是让模型能够检测未标注的对象，只需要几个标注。然而，许多现有的FSOD算法忽视了一个关键问题：当输入图像包含多个未标注的对象，并且只有一部分被标注，那么未标注的对象将被视为背景进行训练。这会导致混乱和严重影响模型的对 novel 对象的回忆。为解决这个问题，我们提出了一种基于自我训练的FSOD方法（ST-FSOD），它在几个shot fine-tuning过程中包含了自我训练机制。ST-FSOD的目标是允许模型发现未标注的对象，并将它们包含在训练中。在一个方面，我们设计了两个分支的区域提档网络（RPN），以分离基本对象和新对象的提档。在另一个方面，我们在 RPN 和区域关注头（RoI）中 integrate了学生-教师机制，以包含高度自信的未标注目标作为 Pseudo 标注。实验结果表明，我们的提出方法在各种 FSOD 设置下的性能均高于当前状态的极大margin。代码将在 <https://github.com/zhu-xlab/ST-FSOD> 上公开。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Attacks-Against-Uncertainty-Quantification"><a href="#Adversarial-Attacks-Against-Uncertainty-Quantification" class="headerlink" title="Adversarial Attacks Against Uncertainty Quantification"></a>Adversarial Attacks Against Uncertainty Quantification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10586">http://arxiv.org/abs/2309.10586</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shymalagowri/Defense-against-Adversarial-Malware-using-RObust-Classifier-DAM-ROC">https://github.com/shymalagowri/Defense-against-Adversarial-Malware-using-RObust-Classifier-DAM-ROC</a></li>
<li>paper_authors: Emanuele Ledda, Daniele Angioni, Giorgio Piras, Giorgio Fumera, Battista Biggio, Fabio Roli</li>
<li>for: 本研究旨在攻击概率评估（Uncertainty Quantification，UQ）技术，以下用于让机器学习模型的输出不可靠。</li>
<li>methods: 我们设计了一个威胁模型，并提出了多种攻击策略，用于让UQ技术输出不准确的结果。</li>
<li>results: 我们的实验结果表明，我们的攻击策略可以更好地 manipulate UQ测量结果，比起induce misclassification。<details>
<summary>Abstract</summary>
Machine-learning models can be fooled by adversarial examples, i.e., carefully-crafted input perturbations that force models to output wrong predictions. While uncertainty quantification has been recently proposed to detect adversarial inputs, under the assumption that such attacks exhibit a higher prediction uncertainty than pristine data, it has been shown that adaptive attacks specifically aimed at reducing also the uncertainty estimate can easily bypass this defense mechanism. In this work, we focus on a different adversarial scenario in which the attacker is still interested in manipulating the uncertainty estimate, but regardless of the correctness of the prediction; in particular, the goal is to undermine the use of machine-learning models when their outputs are consumed by a downstream module or by a human operator. Following such direction, we: \textit{(i)} design a threat model for attacks targeting uncertainty quantification; \textit{(ii)} devise different attack strategies on conceptually different UQ techniques spanning for both classification and semantic segmentation problems; \textit{(iii)} conduct a first complete and extensive analysis to compare the differences between some of the most employed UQ approaches under attack. Our extensive experimental analysis shows that our attacks are more effective in manipulating uncertainty quantification measures than attacks aimed to also induce misclassifications.
</details>
<details>
<summary>摘要</summary>
We design a threat model for attacks targeting uncertainty quantification and devise different attack strategies on various UQ techniques for both classification and semantic segmentation problems. We conduct a comprehensive analysis to compare the differences between some of the most commonly used UQ approaches under attack. Our extensive experimental analysis shows that our attacks are more effective in manipulating uncertainty quantification measures than attacks that also aim to induce misclassifications.
</details></li>
</ul>
<hr>
<h2 id="Forgedit-Text-Guided-Image-Editing-via-Learning-and-Forgetting"><a href="#Forgedit-Text-Guided-Image-Editing-via-Learning-and-Forgetting" class="headerlink" title="Forgedit: Text Guided Image Editing via Learning and Forgetting"></a>Forgedit: Text Guided Image Editing via Learning and Forgetting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10556">http://arxiv.org/abs/2309.10556</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/witcherofresearch/forgedit">https://github.com/witcherofresearch/forgedit</a></li>
<li>paper_authors: Shiwen Zhang, Shuai Xiao, Weilin Huang</li>
<li>for: 这个论文的目的是提出一种新的文本引导图像编辑方法，以解决现有的图像编辑模型受过拟合和时间占用的问题。</li>
<li>methods: 该方法基于一种新的精度学习框架，通过视语言结合学习来重建输入图像，并使用向量减减和向量投影来找到适合的文本嵌入。此外，它还利用了Diffusion Models的一般性特性，并采用了忘记策略来解决恶性拟合问题。</li>
<li>results: 该方法在TEdBench数据集上达到了新的顶峰性状态，在CLIP分数和LPIPS分数两个指标上都超越了之前的SOTA方法Imagic with Imagen。<details>
<summary>Abstract</summary>
Text guided image editing on real images given only the image and the target text prompt as inputs, is a very general and challenging problem, which requires the editing model to reason by itself which part of the image should be edited, to preserve the characteristics of original image, and also to perform complicated non-rigid editing. Previous fine-tuning based solutions are time-consuming and vulnerable to overfitting, limiting their editing capabilities. To tackle these issues, we design a novel text guided image editing method, Forgedit. First, we propose a novel fine-tuning framework which learns to reconstruct the given image in less than one minute by vision language joint learning. Then we introduce vector subtraction and vector projection to explore the proper text embedding for editing. We also find a general property of UNet structures in Diffusion Models and inspired by such a finding, we design forgetting strategies to diminish the fatal overfitting issues and significantly boost the editing abilities of Diffusion Models. Our method, Forgedit, implemented with Stable Diffusion, achieves new state-of-the-art results on the challenging text guided image editing benchmark TEdBench, surpassing the previous SOTA method Imagic with Imagen, in terms of both CLIP score and LPIPS score. Codes are available at https://github.com/witcherofresearch/Forgedit.
</details>
<details>
<summary>摘要</summary>
Text 指导图像编辑问题是一个非常通用和挑战性的问题，需要编辑模型自行判断需要编辑哪些部分，以保持原图特征，同时也需要执行复杂的非RIGID 编辑。先前的精度训练基于解决方案具有较长的训练时间和过拟合问题，这限制了编辑能力。为解决这些问题，我们设计了一种新的文本指导图像编辑方法，即 Forgedit。首先，我们提出了一种新的精度训练框架，通过视语言结合学习来学习重建给定图像，并且在一分钟内完成。然后，我们引入向量减法和向量投影来探索适当的文本嵌入 для编辑。我们还发现了Diffusion Models中的一种普遍性，即UNet结构的特点，并由此得到了忘记策略，以解决致命的过拟合问题，并大幅提高Diffusion Models的编辑能力。我们的方法 Forgedit，通过稳定扩散，在TEdBench的文本指导图像编辑标准 benchmark 上实现了新的状态纪录，超过了之前的SOTA方法 Imagic with Imagen，以 Both CLIP 分数和 LPIPS 分数来衡量。代码可以在https://github.com/witcherofresearch/Forgedit 上找到。
</details></li>
</ul>
<hr>
<h2 id="An-overview-of-some-mathematical-techniques-and-problems-linking-3D-vision-to-3D-printing"><a href="#An-overview-of-some-mathematical-techniques-and-problems-linking-3D-vision-to-3D-printing" class="headerlink" title="An overview of some mathematical techniques and problems linking 3D vision to 3D printing"></a>An overview of some mathematical techniques and problems linking 3D vision to 3D printing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10549">http://arxiv.org/abs/2309.10549</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emiliano Cristiani, Maurizio Falcone, Silvia Tozza</li>
<li>for: 本研究旨在探讨 Computer Vision 和 3D printing 之间的交互，尤其是Shape-from-Shading 问题的解决方法，以及基于非线性偏微分方程和优化的方法。</li>
<li>methods: 本文使用了一些非线性偏微分方程和优化技术来解决 Shape-from-Shading 问题，并考虑了将这些方法应用于 3D printing 过程中。</li>
<li>results: 本研究提出了一些实用的例子，以示出将图像转换为 final 3D 印刷的过程。<details>
<summary>Abstract</summary>
Computer Vision and 3D printing have rapidly evolved in the last 10 years but interactions among them have been very limited so far, despite the fact that they share several mathematical techniques. We try to fill the gap presenting an overview of some techniques for Shape-from-Shading problems as well as for 3D printing with an emphasis on the approaches based on nonlinear partial differential equations and optimization. We also sketch possible couplings to complete the process of object manufacturing starting from one or more images of the object and ending with its final 3D print. We will give some practical examples of this procedure.
</details>
<details>
<summary>摘要</summary>
计算机视觉和3D打印技术在过去10年内快速发展，但它们之间的交互非常有限，尽管它们共享一些数学方法。我们尝试填补这个空白，介绍一些Shape-from-Shading问题的技巧以及基于非线性偏微分方程和优化的3D打印技术。我们还简要介绍可能的交互，完成对象制造的整个过程，从一个或多个图像开始，结束于其最终3D打印。我们将给出一些实践示例。
</details></li>
</ul>
<hr>
<h2 id="Decoupling-the-Curve-Modeling-and-Pavement-Regression-for-Lane-Detection"><a href="#Decoupling-the-Curve-Modeling-and-Pavement-Regression-for-Lane-Detection" class="headerlink" title="Decoupling the Curve Modeling and Pavement Regression for Lane Detection"></a>Decoupling the Curve Modeling and Pavement Regression for Lane Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10533">http://arxiv.org/abs/2309.10533</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wencheng Han, Jianbing Shen</li>
<li>for: 本研究的目的是提出一种新的车道检测方法，以解决现有方法中 curve-based lane representation 对不规则的车道线的处理不佳问题。</li>
<li>methods: 本研究使用了分解车道检测任务为两部分：曲线建模和地面高程回归。Specifically, we use a parameterized curve to represent lanes in the BEV space to reflect the original distribution of lanes, and regress the ground heights of key points separately from the curve modeling.</li>
<li>results: 我们在2D车道检测 benchmarks (TuSimple和CULane) 和最近提出的3D车道检测 datasets (ONCE-3Dlane和OpenLane) 上进行了实验，并显示出了显著的改进。<details>
<summary>Abstract</summary>
The curve-based lane representation is a popular approach in many lane detection methods, as it allows for the representation of lanes as a whole object and maximizes the use of holistic information about the lanes. However, the curves produced by these methods may not fit well with irregular lines, which can lead to gaps in performance compared to indirect representations such as segmentation-based or point-based methods. We have observed that these lanes are not intended to be irregular, but they appear zigzagged in the perspective view due to being drawn on uneven pavement. In this paper, we propose a new approach to the lane detection task by decomposing it into two parts: curve modeling and ground height regression. Specifically, we use a parameterized curve to represent lanes in the BEV space to reflect the original distribution of lanes. For the second part, since ground heights are determined by natural factors such as road conditions and are less holistic, we regress the ground heights of key points separately from the curve modeling. Additionally, we have unified the 2D and 3D lane detection tasks by designing a new framework and a series of losses to guide the optimization of models with or without 3D lane labels. Our experiments on 2D lane detection benchmarks (TuSimple and CULane), as well as the recently proposed 3D lane detection datasets (ONCE-3Dlane and OpenLane), have shown significant improvements. We will make our well-documented source code publicly available.
</details>
<details>
<summary>摘要</summary>
Lane 表示法是许多车道检测方法中的受欢迎方法，因为它允许车道被视为整个对象，并且最大化了车道的整体信息。然而，由这些方法生成的曲线可能不适应不规则的线条，这可能会导致性能上的差距与分 segmentation-based 或点 clouds 方法相比。我们发现这些车道并不是不规则的，但它们在 bird's eye view 视角下看起来是zigzag的，因为它们被Drawn 在不均匀的路面上。在这篇论文中，我们提出了一种新的车道检测任务的方法，即分解为曲线建模和地面高度回归。特别是，我们使用参数化的曲线来表示车道在 bird's eye view 空间中的原始分布。为第二部分，由于地面高度是由自然因素 such as 道路状况而决定的，我们分别从曲线建模中进行地面高度的回归。此外，我们将2D 和 3D 车道检测任务统一为一个框架和一系列损失函数，以引导模型的优化。我们的实验表明，在 TuSimple 和 CULane 2D 车道检测标准测试集上，以及最近提出的3D 车道检测数据集（ONCE-3Dlane 和 OpenLane）上，我们的方法具有显著的改进。我们将我们的详细的源代码公开发布。
</details></li>
</ul>
<hr>
<h2 id="Retinex-guided-Channel-grouping-based-Patch-Swap-for-Arbitrary-Style-Transfer"><a href="#Retinex-guided-Channel-grouping-based-Patch-Swap-for-Arbitrary-Style-Transfer" class="headerlink" title="Retinex-guided Channel-grouping based Patch Swap for Arbitrary Style Transfer"></a>Retinex-guided Channel-grouping based Patch Swap for Arbitrary Style Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10528">http://arxiv.org/abs/2309.10528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chang Liu, Yi Niu, Mingming Ma, Fu Li, Guangming Shi</li>
<li>for: 提高patch-matching基于风格传输的质量和稳定性，以提供更加风格一致的文本URE。</li>
<li>methods: 根据Retinex理论和通道组合策略，对 conten image feature map 进行替换，并提供补充混合和多尺度生成策略来避免不希望的黑色区域和过度风格化问题。</li>
<li>results: 实验结果表明，提案方法可以比存在技术更好地提供风格一致的文本URE，同时保持内容准确性。<details>
<summary>Abstract</summary>
The basic principle of the patch-matching based style transfer is to substitute the patches of the content image feature maps by the closest patches from the style image feature maps. Since the finite features harvested from one single aesthetic style image are inadequate to represent the rich textures of the content natural image, existing techniques treat the full-channel style feature patches as simple signal tensors and create new style feature patches via signal-level fusion, which ignore the implicit diversities existed in style features and thus fail for generating better stylised results. In this paper, we propose a Retinex theory guided, channel-grouping based patch swap technique to solve the above challenges. Channel-grouping strategy groups the style feature maps into surface and texture channels, which prevents the winner-takes-all problem. Retinex theory based decomposition controls a more stable channel code rate generation. In addition, we provide complementary fusion and multi-scale generation strategy to prevent unexpected black area and over-stylised results respectively. Experimental results demonstrate that the proposed method outperforms the existing techniques in providing more style-consistent textures while keeping the content fidelity.
</details>
<details>
<summary>摘要</summary>
基本原则是将内容图像特征地图替换为风格图像特征地图中最近的 patches。由于一个单一风格图像的特征不足以表示自然图像的复杂 текстуры，现有技术通过信号级混合来创建新的风格特征 patches，而忽略了风格特征之间的隐式多样性，从而导致更好的风格化结果不可能获得。在这篇论文中，我们提出了基于 Retinex 理论的、通道分组 based patch swap 技术来解决上述挑战。通道分组策略将风格特征地图分为表面和Texture通道，以避免赢家占据全部问题。Retinex 理论基于的分解控制了更稳定的通道码率生成。此外，我们还提供了补做和多尺度生成策略，以避免不期望的黑色区域和过度风格化结果。实验结果表明，我们的方法在保持内容准确性的同时，可以提供更风格一致的 текстуры。
</details></li>
</ul>
<hr>
<h2 id="SPOT-Scalable-3D-Pre-training-via-Occupancy-Prediction-for-Autonomous-Driving"><a href="#SPOT-Scalable-3D-Pre-training-via-Occupancy-Prediction-for-Autonomous-Driving" class="headerlink" title="SPOT: Scalable 3D Pre-training via Occupancy Prediction for Autonomous Driving"></a>SPOT: Scalable 3D Pre-training via Occupancy Prediction for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10527">http://arxiv.org/abs/2309.10527</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pjlab-adg/3dtrans">https://github.com/pjlab-adg/3dtrans</a></li>
<li>paper_authors: Xiangchao Yan, Runjian Chen, Bo Zhang, Jiakang Yuan, Xinyu Cai, Botian Shi, Wenqi Shao, Junchi Yan, Ping Luo, Yu Qiao</li>
<li>for: 本文为了提高3D LiDAR点云的感知任务，包括3D物体检测和LiDARSemantic分割，提出了一种可扩展的预训练方法。</li>
<li>methods: 本文提出了一种名为SPOT（可扩展预训练via占用预测）的方法，通过大规模预训练和不同下游数据集和任务的细致调整，以提高3D表示的学习效果。</li>
<li>results: 本文通过多个公共数据集和任务下的实验，证明了occupancy预测的潜在性，并且通过树枝抽样技术和类别准备策略来缓解不同LiDAR传感器和注释策略在不同数据集中的领域差异。此外，本文还观察到了扩展预训练的现象，即下游性能随预训练数据的增加而提高。<details>
<summary>Abstract</summary>
Annotating 3D LiDAR point clouds for perception tasks including 3D object detection and LiDAR semantic segmentation is notoriously time-and-energy-consuming. To alleviate the burden from labeling, it is promising to perform large-scale pre-training and fine-tune the pre-trained backbone on different downstream datasets as well as tasks. In this paper, we propose SPOT, namely Scalable Pre-training via Occupancy prediction for learning Transferable 3D representations, and demonstrate its effectiveness on various public datasets with different downstream tasks under the label-efficiency setting. Our contributions are threefold: (1) Occupancy prediction is shown to be promising for learning general representations, which is demonstrated by extensive experiments on plenty of datasets and tasks. (2) SPOT uses beam re-sampling technique for point cloud augmentation and applies class-balancing strategies to overcome the domain gap brought by various LiDAR sensors and annotation strategies in different datasets. (3) Scalable pre-training is observed, that is, the downstream performance across all the experiments gets better with more pre-training data. We believe that our findings can facilitate understanding of LiDAR point clouds and pave the way for future exploration in LiDAR pre-training. Codes and models will be released.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate_language: zh-CN<<SYS>> annotating 3D LiDAR point clouds for perception tasks, including 3D object detection and LiDAR semantic segmentation, is notoriously time-consuming and energy-intensive. To alleviate the burden of labeling, it is promising to perform large-scale pre-training and fine-tune the pre-trained backbone on different downstream datasets and tasks. In this paper, we propose SPOT, namely Scalable Pre-training via Occupancy prediction for learning Transferable 3D representations, and demonstrate its effectiveness on various public datasets with different downstream tasks under the label-efficiency setting. Our contributions are threefold:1. Occupancy prediction is shown to be promising for learning general representations, which is demonstrated by extensive experiments on numerous datasets and tasks.2. SPOT uses beam re-sampling technique for point cloud augmentation and applies class-balancing strategies to overcome the domain gap brought by various LiDAR sensors and annotation strategies in different datasets.3. Scalable pre-training is observed, that is, the downstream performance across all the experiments gets better with more pre-training data.We believe that our findings can facilitate understanding of LiDAR point clouds and pave the way for future exploration in LiDAR pre-training. Codes and models will be released.
</details></li>
</ul>
<hr>
<h2 id="Edge-aware-Feature-Aggregation-Network-for-Polyp-Segmentation"><a href="#Edge-aware-Feature-Aggregation-Network-for-Polyp-Segmentation" class="headerlink" title="Edge-aware Feature Aggregation Network for Polyp Segmentation"></a>Edge-aware Feature Aggregation Network for Polyp Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10523">http://arxiv.org/abs/2309.10523</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Zhou, Yizhe Zhang, Geng Chen, Yi Zhou, Ye Wu, Deng-Ping Fan</li>
<li>for: 本研究旨在提高肠癌检测和预防的早期诊断方面，通过改进肠癌形态学����部分的分割精度。</li>
<li>methods: 本研究提出了一种Edge-aware Feature Aggregation Network（EFA-Net），包括Edge-aware Guidance Module（EGM）、Scale-aware Convolution Module（SCM）和Cross-level Fusion Module（CFM）等模块。</li>
<li>results: 对五种常用的肠癌检测数据集进行实验，EFA-Net表现出比前方法更高的普适性和效果。<details>
<summary>Abstract</summary>
Precise polyp segmentation is vital for the early diagnosis and prevention of colorectal cancer (CRC) in clinical practice. However, due to scale variation and blurry polyp boundaries, it is still a challenging task to achieve satisfactory segmentation performance with different scales and shapes. In this study, we present a novel Edge-aware Feature Aggregation Network (EFA-Net) for polyp segmentation, which can fully make use of cross-level and multi-scale features to enhance the performance of polyp segmentation. Specifically, we first present an Edge-aware Guidance Module (EGM) to combine the low-level features with the high-level features to learn an edge-enhanced feature, which is incorporated into each decoder unit using a layer-by-layer strategy. Besides, a Scale-aware Convolution Module (SCM) is proposed to learn scale-aware features by using dilated convolutions with different ratios, in order to effectively deal with scale variation. Further, a Cross-level Fusion Module (CFM) is proposed to effectively integrate the cross-level features, which can exploit the local and global contextual information. Finally, the outputs of CFMs are adaptively weighted by using the learned edge-aware feature, which are then used to produce multiple side-out segmentation maps. Experimental results on five widely adopted colonoscopy datasets show that our EFA-Net outperforms state-of-the-art polyp segmentation methods in terms of generalization and effectiveness.
</details>
<details>
<summary>摘要</summary>
精准的肠Rectal polyp分割对早期诊断和预防肠Rectal cancer (CRC)在临床实践中是非常重要的。然而，由于Scale variation和模糊的肠Rectal polyp边界，这仍然是一个挑战性的任务，以达到满意的分割性能。在本研究中，我们提出了一种新的Edge-aware Feature Aggregation Network (EFA-Net)，用于肠Rectal polyp分割，可以充分利用不同级别和多种比例的特征来提高肠Rectal polyp分割性能。具体来说，我们首先提出了Edge-aware Guidance Module (EGM)，用于将低级特征与高级特征结合，学习Edge-enhanced特征，并将其分割到每个解码器单元中。此外，我们还提出了Scale-aware Convolution Module (SCM)，用于学习Scale-aware特征，通过不同的扩大系数来有效地处理Scale variation。此外，我们还提出了Cross-level Fusion Module (CFM)，用于有效地整合不同级别的特征，可以利用本地和全局的Contextual information。最后，CFMs输出被使用learn的Edge-aware特征进行权重adaptive归一化，并用于生成多个Side-out分割地图。实验结果表明，我们的EFA-Net在五个常用的肠Rectoscopy数据集上比State-of-the-art的肠Rectal polyp分割方法有更好的普适性和效果。
</details></li>
</ul>
<hr>
<h2 id="Spatial-Assistant-Encoder-Decoder-Network-for-Real-Time-Semantic-Segmentation"><a href="#Spatial-Assistant-Encoder-Decoder-Network-for-Real-Time-Semantic-Segmentation" class="headerlink" title="Spatial-Assistant Encoder-Decoder Network for Real Time Semantic Segmentation"></a>Spatial-Assistant Encoder-Decoder Network for Real Time Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10519">http://arxiv.org/abs/2309.10519</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cuzaoo/sanet-main">https://github.com/cuzaoo/sanet-main</a></li>
<li>paper_authors: Yalun Wang, Shidong Chen, Huicong Bian, Weixiao Li, Qin Lu</li>
<li>for: 本文目的是提出一种基于encoder-decoder架构的实时semantic segmentation网络，以提高自动驾驶车辆的环境理解。</li>
<li>methods: 本文使用了encoder-decoder架构，并在encoder部分保留了中间部分的特征图，并使用了atrous convolution branches来实现同分辨率特征EXTRACTION。在decoder部分，我们提出了一种hybrid attention模块，SAD，来混合不同的分支。</li>
<li>results: 我们的SANet模型在实时CamVid和cityscape数据集上达到了竞争性的Result，包括78.4% mIOU at 65.1 FPS on Cityscape test dataset和78.8% mIOU at 147 FPS on CamVid test dataset。<details>
<summary>Abstract</summary>
Semantic segmentation is an essential technology for self-driving cars to comprehend their surroundings. Currently, real-time semantic segmentation networks commonly employ either encoder-decoder architecture or two-pathway architecture. Generally speaking, encoder-decoder models tend to be quicker,whereas two-pathway models exhibit higher accuracy. To leverage both strengths, we present the Spatial-Assistant Encoder-Decoder Network (SANet) to fuse the two architectures. In the overall architecture, we uphold the encoder-decoder design while maintaining the feature maps in the middle section of the encoder and utilizing atrous convolution branches for same-resolution feature extraction. Toward the end of the encoder, we integrate the asymmetric pooling pyramid pooling module (APPPM) to optimize the semantic extraction of the feature maps. This module incorporates asymmetric pooling layers that extract features at multiple resolutions. In the decoder, we present a hybrid attention module, SAD, that integrates horizontal and vertical attention to facilitate the combination of various branches. To ascertain the effectiveness of our approach, our SANet model achieved competitive results on the real-time CamVid and cityscape datasets. By employing a single 2080Ti GPU, SANet achieved a 78.4 % mIOU at 65.1 FPS on the Cityscape test dataset and 78.8 % mIOU at 147 FPS on the CamVid test dataset. The training code and model for SANet are available at https://github.com/CuZaoo/SANet-main
</details>
<details>
<summary>摘要</summary>
Semantic segmentation 是自驾车技术的重要组成部分，帮助自驾车理解它所处的环境。当前实时 semantic segmentation 网络通常采用 either encoder-decoder 架构或 two-pathway 架构。一般来说，encoder-decoder 模型比较快速，而 two-pathway 模型具有更高的准确率。为了利用这两者的优点，我们提出了 Spatial-Assistant Encoder-Decoder Network (SANet)，将两种架构融合在一起。整体架构保持 encoder-decoder 设计，并在中间部分的 encoder 中维持特征图，使用 atrous convolution 分支来实现同分辨率特征提取。在 encoder 的末端，我们 integration asymmetric pooling pyramid pooling module (APPPM) 来优化特征提取。在 decoder 中，我们提出了 hybrid attention module (SAD)，将 horizontal 和 vertical attention 集成到不同分支中，以便不同分支之间的组合。为了证明我们的方法的有效性，我们的 SANet 模型在 real-time CamVid 和 cityscape 数据集上达到了竞争性的结果。使用单个 2080Ti GPU，SANet 在 Cityscape 测试数据集上达到了 78.4 % mIOU 的值，并在 65.1 FPS 的速度下运行。训练代码和模型可以在 https://github.com/CuZaoo/SANet-main 上下载。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Landmark-Discovery-Using-Consistency-Guided-Bottleneck"><a href="#Unsupervised-Landmark-Discovery-Using-Consistency-Guided-Bottleneck" class="headerlink" title="Unsupervised Landmark Discovery Using Consistency Guided Bottleneck"></a>Unsupervised Landmark Discovery Using Consistency Guided Bottleneck</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10518">http://arxiv.org/abs/2309.10518</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mamonaawan/cgb_uld">https://github.com/mamonaawan/cgb_uld</a></li>
<li>paper_authors: Mamona Awan, Muhammad Haris Khan, Sanoojan Baliah, Muhammad Ahmad Waseem, Salman Khan, Fahad Shahbaz Khan, Arif Mahmood</li>
<li>for: 本研究目标是不监督的物体标记发现问题。</li>
<li>methods: 我们引入了一个可靠性指标导向的瓶颈，该瓶颈利用标记兼容度来生成适应性的热图。我们还提出了一种在图像重建 pipeline 中使用 pseudo-ground truth 来获得假Supervision。</li>
<li>results: 我们在五种多样化的 dataset 上进行了评估，结果表明我们的方法与现有状态的方法相比，表现出色。我们的代码可以在 GitHub 上获取（<a target="_blank" rel="noopener" href="https://github.com/MamonaAwan/CGB_ULD%EF%BC%89%E3%80%82">https://github.com/MamonaAwan/CGB_ULD）。</a><details>
<summary>Abstract</summary>
We study a challenging problem of unsupervised discovery of object landmarks. Many recent methods rely on bottlenecks to generate 2D Gaussian heatmaps however, these are limited in generating informed heatmaps while training, presumably due to the lack of effective structural cues. Also, it is assumed that all predicted landmarks are semantically relevant despite having no ground truth supervision. In the current work, we introduce a consistency-guided bottleneck in an image reconstruction-based pipeline that leverages landmark consistency, a measure of compatibility score with the pseudo-ground truth to generate adaptive heatmaps. We propose obtaining pseudo-supervision via forming landmark correspondence across images. The consistency then modulates the uncertainty of the discovered landmarks in the generation of adaptive heatmaps which rank consistent landmarks above their noisy counterparts, providing effective structural information for improved robustness. Evaluations on five diverse datasets including MAFL, AFLW, LS3D, Cats, and Shoes demonstrate excellent performance of the proposed approach compared to the existing state-of-the-art methods. Our code is publicly available at https://github.com/MamonaAwan/CGB_ULD.
</details>
<details>
<summary>摘要</summary>
我们研究一个挑战性的无监督对象地标发现问题。许多最近的方法利用瓶颈来生成2D加aussian热图，但这些瓶颈受限于在训练时生成有知识的热图，可能因缺乏有效的结构准则而导致。此外，它假设所预测的地标都是semantically相关的，即使没有真实的地标指导。在当前的工作中，我们引入一个具有协调性的瓶颈，该瓶颈在图像重建基于管道中利用地标协调度来生成适应性的热图。我们提议通过图像之间的地标匹配来获取pseudo-超级视图。然后，协调度会修饰发现的地标的不确定性，使得有用的地标在热图生成中排名高于噪声Counterparts，提供有效的结构信息，从而提高了Robustness。我们对五种多样化的数据集，包括MAFL、AFLW、LS3D、猫和鞋子进行了评估，并证明了我们的方法与现有状态的方法相比表现出色。我们的代码公开可用于https://github.com/MamonaAwan/CGB_ULD。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Estimation-in-Instance-Segmentation-with-Star-convex-Shapes"><a href="#Uncertainty-Estimation-in-Instance-Segmentation-with-Star-convex-Shapes" class="headerlink" title="Uncertainty Estimation in Instance Segmentation with Star-convex Shapes"></a>Uncertainty Estimation in Instance Segmentation with Star-convex Shapes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10513">http://arxiv.org/abs/2309.10513</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qasim M. K. Siddiqui, Sebastian Starke, Peter Steinbach</li>
<li>for: 这个研究旨在提高实例 segmentation 中模型的预测uncertainty 评估，以便更加准确地评估模型的可靠性和决策。</li>
<li>methods: 本研究使用 Monte-Carlo Dropout 和 Deep Ensemble 技术来计算实例的空间和分数 certeinty 分布，并 comparing 两种不同的聚类方法来评估它们的效果。</li>
<li>results: 研究发现，结合空间和分数 certeinty 分布可以获得更好的准确性评估结果，而使用 Deep Ensemble 技术并结合我们的新 radial clustering 方法则更加有效。<details>
<summary>Abstract</summary>
Instance segmentation has witnessed promising advancements through deep neural network-based algorithms. However, these models often exhibit incorrect predictions with unwarranted confidence levels. Consequently, evaluating prediction uncertainty becomes critical for informed decision-making. Existing methods primarily focus on quantifying uncertainty in classification or regression tasks, lacking emphasis on instance segmentation. Our research addresses the challenge of estimating spatial certainty associated with the location of instances with star-convex shapes. Two distinct clustering approaches are evaluated which compute spatial and fractional certainty per instance employing samples by the Monte-Carlo Dropout or Deep Ensemble technique. Our study demonstrates that combining spatial and fractional certainty scores yields improved calibrated estimation over individual certainty scores. Notably, our experimental results show that the Deep Ensemble technique alongside our novel radial clustering approach proves to be an effective strategy. Our findings emphasize the significance of evaluating the calibration of estimated certainties for model reliability and decision-making.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Single-Image-based-unsupervised-joint-segmentation-and-denoising"><a href="#Single-Image-based-unsupervised-joint-segmentation-and-denoising" class="headerlink" title="Single-Image based unsupervised joint segmentation and denoising"></a>Single-Image based unsupervised joint segmentation and denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10511">http://arxiv.org/abs/2309.10511</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Nadja1611/Single-Image-based-unsupervised-joint-segmentation-and-denoising">https://github.com/Nadja1611/Single-Image-based-unsupervised-joint-segmentation-and-denoising</a></li>
<li>paper_authors: Nadja Gruber, Johannes Schwab, Noémie Debroux, Nicolas Papadakis, Markus Haltmeier</li>
<li>for: 这个论文的目的是 joint segmentation and denoising of a single image.</li>
<li>methods: 该方法combines variational segmentation method with self-supervised, single-image based deep learning approach.</li>
<li>results: 该方法可以在高噪音和通用Texture图像中分割多个有意义的区域，并且在微scopic镜中可以对噪音图像进行joint segmentation and denoising. I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
In this work, we develop an unsupervised method for the joint segmentation and denoising of a single image. To this end, we combine the advantages of a variational segmentation method with the power of a self-supervised, single-image based deep learning approach. One major strength of our method lies in the fact, that in contrast to data-driven methods, where huge amounts of labeled samples are necessary, our model can segment an image into multiple meaningful regions without any training database. Further, we introduce a novel energy functional in which denoising and segmentation are coupled in a way that both tasks benefit from each other. The limitations of existing single-image based variational segmentation methods, which are not capable of dealing with high noise or generic texture, are tackled by this specific combination with self-supervised image denoising. We propose a unified optimisation strategy and show that, especially for very noisy images available in microscopy, our proposed joint approach outperforms its sequential counterpart as well as alternative methods focused purely on denoising or segmentation. Another comparison is conducted with a supervised deep learning approach designed for the same application, highlighting the good performance of our approach.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们开发了一种无监督的方法，用于整合图像的分割和除噪。为此，我们结合了变形分割方法的优点和单个图像基于深度学习的自动学习方法。我们的方法的一个重要优点在于，它可以在没有培训数据库的情况下，将图像分割成多个有意义的区域。此外，我们引入了一个新的能量函数，其中除噪和分割两个任务之间存在互助关系，两个任务都可以受益于这种结合。现有的单个图像基于变形分割方法存在高噪音或通用的文字纹理等问题，我们通过与自动学习图像除噪方法的特定结合，解决这些局限性。我们提出了一种统一优化策略，并证明，特别是在微scopic频谱中的很噪音图像上，我们的提议的联合方法会超越其顺序对应方法以及专门为这种应用设计的其他方法。另外，我们还与一种监督式深度学习方法进行比较，显示了我们的方法的良好性能。
</details></li>
</ul>
<hr>
<h2 id="DCPT-Darkness-Clue-Prompted-Tracking-in-Nighttime-UAVs"><a href="#DCPT-Darkness-Clue-Prompted-Tracking-in-Nighttime-UAVs" class="headerlink" title="DCPT: Darkness Clue-Prompted Tracking in Nighttime UAVs"></a>DCPT: Darkness Clue-Prompted Tracking in Nighttime UAVs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10491">http://arxiv.org/abs/2309.10491</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiawen Zhu, Huayi Tang, Zhi-Qi Cheng, Jun-Yan He, Bin Luo, Shihao Qiu, Shengming Li, Huchuan Lu</li>
<li>for: 提高夜间无人机跟踪性能</li>
<li>methods: 提出了一种新的 darkness clue-prompted tracking (DCPT) 架构，通过效率地学习生成黑暗提示来实现夜间无人机跟踪。</li>
<li>results: 对多个黑场景标准吊靡进行了广泛的实验，并达到了现状最佳性能。<details>
<summary>Abstract</summary>
Existing nighttime unmanned aerial vehicle (UAV) trackers follow an "Enhance-then-Track" architecture - first using a light enhancer to brighten the nighttime video, then employing a daytime tracker to locate the object. This separate enhancement and tracking fails to build an end-to-end trainable vision system. To address this, we propose a novel architecture called Darkness Clue-Prompted Tracking (DCPT) that achieves robust UAV tracking at night by efficiently learning to generate darkness clue prompts. Without a separate enhancer, DCPT directly encodes anti-dark capabilities into prompts using a darkness clue prompter (DCP). Specifically, DCP iteratively learns emphasizing and undermining projections for darkness clues. It then injects these learned visual prompts into a daytime tracker with fixed parameters across transformer layers. Moreover, a gated feature aggregation mechanism enables adaptive fusion between prompts and between prompts and the base model. Extensive experiments show state-of-the-art performance for DCPT on multiple dark scenario benchmarks. The unified end-to-end learning of enhancement and tracking in DCPT enables a more trainable system. The darkness clue prompting efficiently injects anti-dark knowledge without extra modules. Code and models will be released.
</details>
<details>
<summary>摘要</summary>
exististing nighttime unmanned aerial vehicle (UAV) trackers follow an "Enhance-then-Track" architecture - first using a light enhancer to brighten the nighttime video, then employing a daytime tracker to locate the object. This separate enhancement and tracking fails to build an end-to-end trainable vision system. To address this, we propose a novel architecture called Darkness Clue-Prompted Tracking (DCPT) that achieves robust UAV tracking at night by efficiently learning to generate darkness clue prompts. Without a separate enhancer, DCPT directly encodes anti-dark capabilities into prompts using a darkness clue prompter (DCP). Specifically, DCP iteratively learns emphasizing and undermining projections for darkness clues. It then injects these learned visual prompts into a daytime tracker with fixed parameters across transformer layers. Moreover, a gated feature aggregation mechanism enables adaptive fusion between prompts and between prompts and the base model. Extensive experiments show state-of-the-art performance for DCPT on multiple dark scenario benchmarks. The unified end-to-end learning of enhancement and tracking in DCPT enables a more trainable system. The darkness clue prompting efficiently injects anti-dark knowledge without extra modules. Code and models will be released.Translation notes:* "Enhance-then-Track" architecture is translated as "提高然后跟踪" architecture.* "Darkness clue prompts" is translated as "黑暗提示" or "黑暗Prompt".* "Darkness clue prompter" is translated as "黑暗提示生成器" or "黑暗Prompter".* "Gated feature aggregation" is translated as "离散特征聚合" or "离散Feature Fusion".* "Base model" is translated as "基础模型" or "基础模型。
</details></li>
</ul>
<hr>
<h2 id="RECALL-Adversarial-Web-based-Replay-for-Continual-Learning-in-Semantic-Segmentation"><a href="#RECALL-Adversarial-Web-based-Replay-for-Continual-Learning-in-Semantic-Segmentation" class="headerlink" title="RECALL+: Adversarial Web-based Replay for Continual Learning in Semantic Segmentation"></a>RECALL+: Adversarial Web-based Replay for Continual Learning in Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10479">http://arxiv.org/abs/2309.10479</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chang Liu, Giulia Rizzoli, Francesco Barbato, Umberto Michieli, Yi Niu, Pietro Zanuttigh</li>
<li>for: 本研究的目的是解决 continual learning 中的 catastrophic forgetting 问题，通过不同的 regularization 策略来保持之前学习的知识。</li>
<li>methods: 本研究扩展了之前的方法（RECALL），通过在线数据库中检索过时的类例来避免忘记。本研究引入了两种新的方法：基于对抗学习和自适应阈值调整来选择来自网络数据的示例，以及一种改进的pseudo-labeling scheme来更准确地标注网络数据。</li>
<li>results: 实验结果显示，这种加强的方法在多个增量学习步骤中表现出色，特别是在多个增量学习步骤中表现出色，特别是在多个增量学习步骤中表现出色，特别是在多个增量学习步骤中表现出色。<details>
<summary>Abstract</summary>
Catastrophic forgetting of previous knowledge is a critical issue in continual learning typically handled through various regularization strategies. However, existing methods struggle especially when several incremental steps are performed. In this paper, we extend our previous approach (RECALL) and tackle forgetting by exploiting unsupervised web-crawled data to retrieve examples of old classes from online databases. Differently from the original approach that did not perform any evaluation of the web data, here we introduce two novel approaches based on adversarial learning and adaptive thresholding to select from web data only samples strongly resembling the statistics of the no longer available training ones. Furthermore, we improved the pseudo-labeling scheme to achieve a more accurate labeling of web data that also consider classes being learned in the current step. Experimental results show that this enhanced approach achieves remarkable results, especially when multiple incremental learning steps are performed.
</details>
<details>
<summary>摘要</summary>
continual learning中的重大问题之一是 previous knowledge的恶化，通常通过不同的正则化策略来解决。然而，现有方法在多个递增学习步骤时尤其不稳定。在这篇论文中，我们对我们之前的方法（RECALL）进行扩展，通过利用无监督的网络数据来恢复老的类。与原始方法不同的是，我们在这里引入了两种基于对抗学习和自适应阈值的新方法来从网络数据中选择只有强相似于过去训练数据的统计的样本。此外，我们改进了 Pseudo-labeling 方案，以更加准确地标注网络数据，并考虑当前步骤中学习的类。实验结果表明，这种加强的方法在多个递增学习步骤时显示出了很好的效果。
</details></li>
</ul>
<hr>
<h2 id="LineMarkNet-Line-Landmark-Detection-for-Valet-Parking"><a href="#LineMarkNet-Line-Landmark-Detection-for-Valet-Parking" class="headerlink" title="LineMarkNet: Line Landmark Detection for Valet Parking"></a>LineMarkNet: Line Landmark Detection for Valet Parking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10475">http://arxiv.org/abs/2309.10475</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zizhang Wu, Fan Wang, Yuanzhu Gan, Tianhao Xu, Weiwei Sun, Rui Tang</li>
<li>for:  This paper aims to solve the long-standing problem of accurate and efficient line landmark detection for valet parking in autonomous driving.</li>
<li>methods:  The paper presents a deep line landmark detection system that utilizes a pre-calibrated homography to fuse context from four separate cameras into a unified bird-eye-view (BEV) space. The system employs a multi-task decoder to detect multiple line landmarks and incorporates a graph transformer to enhance the vision transformer with hierarchical level graph reasoning for semantic segmentation.</li>
<li>results:  The paper achieves enhanced performance compared to several line detection methods and validates the multi-task network’s efficiency in real-time line landmark detection on the Qualcomm 820A platform while maintaining superior accuracy.<details>
<summary>Abstract</summary>
We aim for accurate and efficient line landmark detection for valet parking, which is a long-standing yet unsolved problem in autonomous driving. To this end, we present a deep line landmark detection system where we carefully design the modules to be lightweight. Specifically, we first empirically design four general line landmarks including three physical lines and one novel mental line. The four line landmarks are effective for valet parking. We then develop a deep network (LineMarkNet) to detect line landmarks from surround-view cameras where we, via the pre-calibrated homography, fuse context from four separate cameras into the unified bird-eye-view (BEV) space, specifically we fuse the surroundview features and BEV features, then employ the multi-task decoder to detect multiple line landmarks where we apply the center-based strategy for object detection task, and design our graph transformer to enhance the vision transformer with hierarchical level graph reasoning for semantic segmentation task. At last, we further parameterize the detected line landmarks (e.g., intercept-slope form) whereby a novel filtering backend incorporates temporal and multi-view consistency to achieve smooth and stable detection. Moreover, we annotate a large-scale dataset to validate our method. Experimental results show that our framework achieves the enhanced performance compared with several line detection methods and validate the multi-task network's efficiency about the real-time line landmark detection on the Qualcomm 820A platform while meantime keeps superior accuracy, with our deep line landmark detection system.
</details>
<details>
<summary>摘要</summary>
我们目标是实现高精度、高效的直线特征检测系统，以解决自动驾驶中长期存在但未解决的停车卫士问题。为此，我们提出了一种深度学习基于模块的直线特征检测系统，其中我们 méticulously 设计了四个通用的直线特征，包括三个物理直线和一个新的心理直线。这四个直线特征都是适用于停车卫士。然后，我们开发了一种深度网络（LineMarkNet），用于从卫星视图摄像头中检测直线特征。我们通过先天准备好的同步论homography将卫星视图中的特征与 bird-eye-view 空间（BEV）中的特征进行融合，然后使用多任务解码器来检测多个直线特征。我们在object detection任务中采用了中心基本策略，并设计了一种图变换器来增强视Transformer的层次Graph理解，以提高semantic segmentation任务的性能。最后，我们进一步参数化检测到的直线特征（例如， intercept-slope 形式），并使用一种新的筛选后端来实现temporal和多视图一致性，以获得平滑和稳定的检测。此外，我们还为我们的方法进行了大规模数据集注解。实验结果表明，我们的框架在多个直线检测方法的比较中具有更高的性能，并证明了我们的深度直线特征检测系统在Qualcomm 820A 平台上的实时检测性能。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-based-speech-enhancement-with-a-weighted-generative-supervised-learning-loss"><a href="#Diffusion-based-speech-enhancement-with-a-weighted-generative-supervised-learning-loss" class="headerlink" title="Diffusion-based speech enhancement with a weighted generative-supervised learning loss"></a>Diffusion-based speech enhancement with a weighted generative-supervised learning loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10457">http://arxiv.org/abs/2309.10457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jean-Eudes Ayilo, Mostafa Sadeghi, Romain Serizel</li>
<li>for: 喇叭声音提升（SE）中的扩散基本生成模型，提供一种代替传统的指导方法。</li>
<li>methods: 将干净说话训练样本转化为中心在噪声说话的高斯噪声，并逐次学习一个参数化的模型，以逆转这个过程，受到噪声说话的条件。</li>
<li>results: 我们提议的方法可以有效地解决不supervised方法中的不足，并且实验结果表明我们的方法有效。<details>
<summary>Abstract</summary>
Diffusion-based generative models have recently gained attention in speech enhancement (SE), providing an alternative to conventional supervised methods. These models transform clean speech training samples into Gaussian noise centered at noisy speech, and subsequently learn a parameterized model to reverse this process, conditionally on noisy speech. Unlike supervised methods, generative-based SE approaches usually rely solely on an unsupervised loss, which may result in less efficient incorporation of conditioned noisy speech. To address this issue, we propose augmenting the original diffusion training objective with a mean squared error (MSE) loss, measuring the discrepancy between estimated enhanced speech and ground-truth clean speech at each reverse process iteration. Experimental results demonstrate the effectiveness of our proposed methodology.
</details>
<details>
<summary>摘要</summary>
传播基本的生成模型在语音提高（SE）方面已经获得了关注，提供一个传统超级vised方法的替代方案。这些模型将清晰的语音训练样本转换为中心于噪音的 Gaussian 噪音，然后学习一个受控的模型，以逆读这个过程，仅在噪音下进行条件运算。不同于超级vised方法，生成基本的SE方法通常仅靠一个不监控的损失函数，这可能会导致更少的条件噪音语音的整合。为了解决这个问题，我们提议将原始传播训练目标加以mean squared error（MSE）损失函数，以量化在逆读过程中每次估计的提高语音与真正的清晰语音之间的差异。实验结果显示了我们提议的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-speech-enhancement-with-diffusion-based-generative-models"><a href="#Unsupervised-speech-enhancement-with-diffusion-based-generative-models" class="headerlink" title="Unsupervised speech enhancement with diffusion-based generative models"></a>Unsupervised speech enhancement with diffusion-based generative models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10450">http://arxiv.org/abs/2309.10450</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sp-uhh/sgmse">https://github.com/sp-uhh/sgmse</a></li>
<li>paper_authors: Berné Nortier, Mostafa Sadeghi, Romain Serizel</li>
<li>for: 本研究旨在提出一种基于扩散模型的无监督音频增强方法，以解决现有方法在面对未见过的条件时的挑战。</li>
<li>methods: 我们使用了分布式扩散模型来学习干净语音的先验分布在短时域傅立叙特（STFT）域，然后通过将学习到的干净语音先验分布与陌生噪声模型结合，实现无监督的音频增强。</li>
<li>results: 我们的方法在比较之下比一种最新的变分自动编码器（VAE）基于无监督方法和一种现有的扩散基于监督方法的状态艺术方法更有优势，这些结果展示了我们的方法在无监督音频增强方面的潜在优势。<details>
<summary>Abstract</summary>
Recently, conditional score-based diffusion models have gained significant attention in the field of supervised speech enhancement, yielding state-of-the-art performance. However, these methods may face challenges when generalising to unseen conditions. To address this issue, we introduce an alternative approach that operates in an unsupervised manner, leveraging the generative power of diffusion models. Specifically, in a training phase, a clean speech prior distribution is learnt in the short-time Fourier transform (STFT) domain using score-based diffusion models, allowing it to unconditionally generate clean speech from Gaussian noise. Then, we develop a posterior sampling methodology for speech enhancement by combining the learnt clean speech prior with a noise model for speech signal inference. The noise parameters are simultaneously learnt along with clean speech estimation through an iterative expectationmaximisation (EM) approach. To the best of our knowledge, this is the first work exploring diffusion-based generative models for unsupervised speech enhancement, demonstrating promising results compared to a recent variational auto-encoder (VAE)-based unsupervised approach and a state-of-the-art diffusion-based supervised method. It thus opens a new direction for future research in unsupervised speech enhancement.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:最近，基于条件的分布扩散模型在监督性音频增强领域受到了广泛关注，并实现了顶尖性能。然而，这些方法可能面临不可预期的条件下的挑战。为解决这个问题，我们介绍了一种alternative方法，利用扩散模型的生成力。在训练阶段，我们使用分数基于扩散模型在快时域 Fourier变换 (STFT) 中学习干净语音先验分布，以便不条件地生成干净语音。然后，我们开发了 posterior 采样方法，结合学习的干净语音先验分布和语音噪声模型，以进行语音增强。噪声参数同时与干净语音估计一起学习，通过交互式期望最大化 (EM) 方法。到目前为止，这是第一篇探讨扩散基于生成模型的无监督语音增强方法的研究论文，对于一个最近的variational autoencoder (VAE)-based无监督方法和一个状态艺术的扩散基于监督方法进行了比较，并达到了promising的结果。因此，这开启了未来无监督语音增强研究的新方向。
</details></li>
</ul>
<hr>
<h2 id="Posterior-sampling-algorithms-for-unsupervised-speech-enhancement-with-recurrent-variational-autoencoder"><a href="#Posterior-sampling-algorithms-for-unsupervised-speech-enhancement-with-recurrent-variational-autoencoder" class="headerlink" title="Posterior sampling algorithms for unsupervised speech enhancement with recurrent variational autoencoder"></a>Posterior sampling algorithms for unsupervised speech enhancement with recurrent variational autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10439">http://arxiv.org/abs/2309.10439</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mostafa Sadeghi, Romain Serizel</li>
<li>for:  addresses the unsupervised speech enhancement problem based on recurrent variational autoencoder (RVAE)</li>
<li>methods:  uses efficient sampling techniques based on Langevin dynamics and Metropolis-Hasting algorithms to circumvent the computational complexity of variational inference</li>
<li>results:  significantly outperforms the variational expectation-maximization (VEM) method and shows robust generalization performance in mismatched test conditions<details>
<summary>Abstract</summary>
In this paper, we address the unsupervised speech enhancement problem based on recurrent variational autoencoder (RVAE). This approach offers promising generalization performance over the supervised counterpart. Nevertheless, the involved iterative variational expectation-maximization (VEM) process at test time, which relies on a variational inference method, results in high computational complexity. To tackle this issue, we present efficient sampling techniques based on Langevin dynamics and Metropolis-Hasting algorithms, adapted to the EM-based speech enhancement with RVAE. By directly sampling from the intractable posterior distribution within the EM process, we circumvent the intricacies of variational inference. We conduct a series of experiments, comparing the proposed methods with VEM and a state-of-the-art supervised speech enhancement approach based on diffusion models. The results reveal that our sampling-based algorithms significantly outperform VEM, not only in terms of computational efficiency but also in overall performance. Furthermore, when compared to the supervised baseline, our methods showcase robust generalization performance in mismatched test conditions.
</details>
<details>
<summary>摘要</summary>
在本文中，我们Addresses the unsupervised speech enhancement problem based on recurrent variational autoencoder (RVAE). This approach offers promising generalization performance over the supervised counterpart. Nevertheless, the involved iterative variational expectation-maximization (VEM) process at test time, which relies on a variational inference method, results in high computational complexity. To tackle this issue, we present efficient sampling techniques based on Langevin dynamics and Metropolis-Hasting algorithms, adapted to the EM-based speech enhancement with RVAE. By directly sampling from the intractable posterior distribution within the EM process, we circumvent the intricacies of variational inference. We conduct a series of experiments, comparing the proposed methods with VEM and a state-of-the-art supervised speech enhancement approach based on diffusion models. The results reveal that our sampling-based algorithms significantly outperform VEM, not only in terms of computational efficiency but also in overall performance. Furthermore, when compared to the supervised baseline, our methods showcase robust generalization performance in mismatched test conditions.Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="AutoDiffusion-Training-Free-Optimization-of-Time-Steps-and-Architectures-for-Automated-Diffusion-Model-Acceleration"><a href="#AutoDiffusion-Training-Free-Optimization-of-Time-Steps-and-Architectures-for-Automated-Diffusion-Model-Acceleration" class="headerlink" title="AutoDiffusion: Training-Free Optimization of Time Steps and Architectures for Automated Diffusion Model Acceleration"></a>AutoDiffusion: Training-Free Optimization of Time Steps and Architectures for Automated Diffusion Model Acceleration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10438">http://arxiv.org/abs/2309.10438</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lijiang Li, Huixia Li, Xiawu Zheng, Jie Wu, Xuefeng Xiao, Rui Wang, Min Zheng, Xin Pan, Fei Chao, Rongrong Ji</li>
<li>for: 提高 diffusion models 的生成速度，无需额外训练。</li>
<li>methods: 提出了一种统一搜索空间和演化算法，可以在不同的 diffusion models 中找到最佳的时间步骤和模型结构。</li>
<li>results: 通过使用只有几步（例如，4步），可以达到优秀的图像生成效果（例如， ImageNet 64 $\times$ 64 的 FID 分数为 17.86），比传统的 DDIM 更好。<details>
<summary>Abstract</summary>
Diffusion models are emerging expressive generative models, in which a large number of time steps (inference steps) are required for a single image generation. To accelerate such tedious process, reducing steps uniformly is considered as an undisputed principle of diffusion models. We consider that such a uniform assumption is not the optimal solution in practice; i.e., we can find different optimal time steps for different models. Therefore, we propose to search the optimal time steps sequence and compressed model architecture in a unified framework to achieve effective image generation for diffusion models without any further training. Specifically, we first design a unified search space that consists of all possible time steps and various architectures. Then, a two stage evolutionary algorithm is introduced to find the optimal solution in the designed search space. To further accelerate the search process, we employ FID score between generated and real samples to estimate the performance of the sampled examples. As a result, the proposed method is (i).training-free, obtaining the optimal time steps and model architecture without any training process; (ii). orthogonal to most advanced diffusion samplers and can be integrated to gain better sample quality. (iii). generalized, where the searched time steps and architectures can be directly applied on different diffusion models with the same guidance scale. Experimental results show that our method achieves excellent performance by using only a few time steps, e.g. 17.86 FID score on ImageNet 64 $\times$ 64 with only four steps, compared to 138.66 with DDIM.
</details>
<details>
<summary>摘要</summary>
Diffusion模型是一种表达力强的生成模型，需要许多时间步骤（推理步骤）来生成单个图像。为了加速这个繁琐的过程，减少步骤的假设是 diffusion模型的一个常见原则。然而，我们认为这种假设并不是最佳的解决方案，即可以在具体的模型中找到不同的优化时间步骤。因此，我们提议通过搜索优化时间步骤和压缩模型架构的联合框架来实现效果性的图像生成，不需要进行任何训练。我们首先设计了一个包含所有可能的时间步骤和各种架构的统一搜索空间。然后，我们引入了两Stage演化算法，以找到这个搜索空间中的优化解决方案。为了加速搜索过程，我们使用FID分数来评估生成的样本质量。因此，我们的方法具有以下优点：1. 无需训练，可以直接从搜索空间中找到最佳的时间步骤和模型架构。2. 与现有的 diffusion samplers  ortogonal，可以和其他模型结合使用以提高样本质量。3. 通用，可以在不同的 diffusion models 中应用已经搜索的时间步骤和模型架构。实验结果表明，我们的方法可以通过使用只有几个时间步骤，如17.86 FID分数在 ImageNet 64 $\times$ 64 中只需要四个步骤，而DDIM需要138.66 FID分数。
</details></li>
</ul>
<hr>
<h2 id="Sample-adaptive-Augmentation-for-Point-Cloud-Recognition-Against-Real-world-Corruptions"><a href="#Sample-adaptive-Augmentation-for-Point-Cloud-Recognition-Against-Real-world-Corruptions" class="headerlink" title="Sample-adaptive Augmentation for Point Cloud Recognition Against Real-world Corruptions"></a>Sample-adaptive Augmentation for Point Cloud Recognition Against Real-world Corruptions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10431">http://arxiv.org/abs/2309.10431</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/roywangj/adaptpoint">https://github.com/roywangj/adaptpoint</a></li>
<li>paper_authors: Jie Wang, Lihe Ding, Tingfa Xu, Shaocong Dong, Xinli Xu, Long Bai, Jianan Li</li>
<li>for: 提高3D视觉中的稳定性和可靠性，应对各种潜在的损害和噪声。</li>
<li>methods: 提出了一种自适应增强框架，名为AdaptPoint，通过基于样本结构的适应变换来应对潜在的损害。同时，还引入了一种听众指导反馈机制，以便生成合适的样本难度水平。</li>
<li>results: 实验结果表明，我们的方法在多种损害 benchmarcks 上达到了状态的最佳结果，包括 ModelNet-C、我们的 ScanObjectNN-C 和 ShapeNet-C。<details>
<summary>Abstract</summary>
Robust 3D perception under corruption has become an essential task for the realm of 3D vision. While current data augmentation techniques usually perform random transformations on all point cloud objects in an offline way and ignore the structure of the samples, resulting in over-or-under enhancement. In this work, we propose an alternative to make sample-adaptive transformations based on the structure of the sample to cope with potential corruption via an auto-augmentation framework, named as AdaptPoint. Specially, we leverage a imitator, consisting of a Deformation Controller and a Mask Controller, respectively in charge of predicting deformation parameters and producing a per-point mask, based on the intrinsic structural information of the input point cloud, and then conduct corruption simulations on top. Then a discriminator is utilized to prevent the generation of excessive corruption that deviates from the original data distribution. In addition, a perception-guidance feedback mechanism is incorporated to guide the generation of samples with appropriate difficulty level. Furthermore, to address the paucity of real-world corrupted point cloud, we also introduce a new dataset ScanObjectNN-C, that exhibits greater similarity to actual data in real-world environments, especially when contrasted with preceding CAD datasets. Experiments show that our method achieves state-of-the-art results on multiple corruption benchmarks, including ModelNet-C, our ScanObjectNN-C, and ShapeNet-C.
</details>
<details>
<summary>摘要</summary>
Robust 3D感知下面受损Has become an essential task for the realm of 3D vision. While current data augmentation techniques usually perform random transformations on all point cloud objects in an offline way and ignore the structure of the samples, resulting in over-or-under enhancement. In this work, we propose an alternative to make sample-adaptive transformations based on the structure of the sample to cope with potential corruption via an auto-augmentation framework, named as AdaptPoint. Specially, we leverage a imitator, consisting of a Deformation Controller and a Mask Controller, respectively in charge of predicting deformation parameters and producing a per-point mask, based on the intrinsic structural information of the input point cloud, and then conduct corruption simulations on top. Then a discriminator is utilized to prevent the generation of excessive corruption that deviates from the original data distribution. In addition, a perception-guidance feedback mechanism is incorporated to guide the generation of samples with appropriate difficulty level. Furthermore, to address the paucity of real-world corrupted point cloud, we also introduce a new dataset ScanObjectNN-C, that exhibits greater similarity to actual data in real-world environments, especially when contrasted with preceding CAD datasets. Experiments show that our method achieves state-of-the-art results on multiple corruption benchmarks, including ModelNet-C, our ScanObjectNN-C, and ShapeNet-C.
</details></li>
</ul>
<hr>
<h2 id="Predicate-Classification-Using-Optimal-Transport-Loss-in-Scene-Graph-Generation"><a href="#Predicate-Classification-Using-Optimal-Transport-Loss-in-Scene-Graph-Generation" class="headerlink" title="Predicate Classification Using Optimal Transport Loss in Scene Graph Generation"></a>Predicate Classification Using Optimal Transport Loss in Scene Graph Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10430">http://arxiv.org/abs/2309.10430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sorachi Kurita, Satoshi Oyama, Itsuki Noda</li>
<li>for: 提高Scene Graph生成（SGG）中的预测准确率，避免由数据集中关系标签的分布偏度导致的预测偏误。</li>
<li>methods: 使用最佳运输损失来比较两个概率分布的相似性，并在 predicate classification 中使用学习最佳运输损失来生成Scene Graph。</li>
<li>results: 相比 existed 方法，提出的方法在 mean Recall@50 和 100 上表现出色，并且提高了数据集中罕见的关系标签的回归率。<details>
<summary>Abstract</summary>
In scene graph generation (SGG), learning with cross-entropy loss yields biased predictions owing to the severe imbalance in the distribution of the relationship labels in the dataset. Thus, this study proposes a method to generate scene graphs using optimal transport as a measure for comparing two probability distributions. We apply learning with the optimal transport loss, which reflects the similarity between the labels in terms of transportation cost, for predicate classification in SGG. In the proposed approach, the transportation cost of the optimal transport is defined using the similarity of words obtained from the pre-trained model. The experimental evaluation of the effectiveness demonstrates that the proposed method outperforms existing methods in terms of mean Recall@50 and 100. Furthermore, it improves the recall of the relationship labels scarcely available in the dataset.
</details>
<details>
<summary>摘要</summary>
在场景图生成（SGG）中，使用十字积分损失会导致预测结果受到分布不均衡的影响，因此这个研究提出了一种使用优化交通为比较两个概率分布的方法来生成场景图。我们在该方法中使用优化交通损失，该损失反映了标签之间的交通成本的相似性，来进行 predicate 分类。在我们的方法中，交通成本的优化交通是基于预训练模型中的字符相似度来定义的。我们的实验评估结果表明，提议的方法在mean Recall@50和100上超过了现有方法，并且提高了数据集中罕见的关系标签的回快。Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Exploring-Different-Levels-of-Supervision-for-Detecting-and-Localizing-Solar-Panels-on-Remote-Sensing-Imagery"><a href="#Exploring-Different-Levels-of-Supervision-for-Detecting-and-Localizing-Solar-Panels-on-Remote-Sensing-Imagery" class="headerlink" title="Exploring Different Levels of Supervision for Detecting and Localizing Solar Panels on Remote Sensing Imagery"></a>Exploring Different Levels of Supervision for Detecting and Localizing Solar Panels on Remote Sensing Imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10421">http://arxiv.org/abs/2309.10421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maarten Burger, Rob Wijnhoven, Shaodi You</li>
<li>for: 本研究探讨了远程感知图像中对象存在和定位问题，特点是太阳能板识别。研究不同级别的超级视图，包括全监督物体探测器、弱监督图像分类器和最小监督异常探测器。</li>
<li>methods: 研究采用了不同级别的超级视图模型，包括全监督物体探测器、弱监督图像分类器和最小监督异常探测器。</li>
<li>results: 研究结果显示，分类器在二分类存在检测中取得0.79的F1分数，而物体探测器则在精确定位方面取得0.72的成绩。异常探测器需要更多数据来实现可靠性。模型结果的融合可能会提高准确性。CAM对定位有一定影响，高级CAM、高级CAM++和HiResCAM在定位方面具有较好的成绩。另外，分类器在数据量更少时仍然保持了良好的Robustness。<details>
<summary>Abstract</summary>
This study investigates object presence detection and localization in remote sensing imagery, focusing on solar panel recognition. We explore different levels of supervision, evaluating three models: a fully supervised object detector, a weakly supervised image classifier with CAM-based localization, and a minimally supervised anomaly detector. The classifier excels in binary presence detection (0.79 F1-score), while the object detector (0.72) offers precise localization. The anomaly detector requires more data for viable performance. Fusion of model results shows potential accuracy gains. CAM impacts localization modestly, with GradCAM, GradCAM++, and HiResCAM yielding superior results. Notably, the classifier remains robust with less data, in contrast to the object detector.
</details>
<details>
<summary>摘要</summary>
本研究 investigate remote sensing imagery 中的对象存在和位置检测，强调太阳能板识别。我们考虑不同水平的超级visum，评估三种模型：完全supervised object detector、weakly supervised image classifier with CAM-based localization和minimally supervised anomaly detector。classifier在binary presence detection中表现出色（F1-score 0.79），而object detector（F1-score 0.72）具有精确的localization能力。anomaly detector需要更多数据来实现可靠的表现。模型结果的融合显示了可能的准确性提升。CAM对localization产生了一定的影响，高级CAM、GradCAM++和HiResCAM在localization中表现较好。各自注意的是，classifier在数据更少时仍然保持了 robust性，与object detector不同。
</details></li>
</ul>
<hr>
<h2 id="SideGAN-3D-Aware-Generative-Model-for-Improved-Side-View-Image-Synthesis"><a href="#SideGAN-3D-Aware-Generative-Model-for-Improved-Side-View-Image-Synthesis" class="headerlink" title="SideGAN: 3D-Aware Generative Model for Improved Side-View Image Synthesis"></a>SideGAN: 3D-Aware Generative Model for Improved Side-View Image Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10388">http://arxiv.org/abs/2309.10388</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kyungmin Jo, Wonjoon Jin, Jaegul Choo, Hyunjoon Lee, Sunghyun Cho</li>
<li>for: 该 paper 主要 targets 生成高质量的三维图像，尤其是在侧视角度下。</li>
<li>methods: 该 paper 提出了一种新的 GAN 训练方法，即 SideGAN，可以生成不同视角下的高质量图像。为了解决 pose 难以学习和 photo-realism 同时学习的问题，该方法将问题分解为两个更容易解决的子问题。</li>
<li>results: 该 paper 通过了广泛的验证，证明了 SideGAN 可以生成高质量的三维图像，不受 camera pose 的影响。<details>
<summary>Abstract</summary>
While recent 3D-aware generative models have shown photo-realistic image synthesis with multi-view consistency, the synthesized image quality degrades depending on the camera pose (e.g., a face with a blurry and noisy boundary at a side viewpoint). Such degradation is mainly caused by the difficulty of learning both pose consistency and photo-realism simultaneously from a dataset with heavily imbalanced poses. In this paper, we propose SideGAN, a novel 3D GAN training method to generate photo-realistic images irrespective of the camera pose, especially for faces of side-view angles. To ease the challenging problem of learning photo-realistic and pose-consistent image synthesis, we split the problem into two subproblems, each of which can be solved more easily. Specifically, we formulate the problem as a combination of two simple discrimination problems, one of which learns to discriminate whether a synthesized image looks real or not, and the other learns to discriminate whether a synthesized image agrees with the camera pose. Based on this, we propose a dual-branched discriminator with two discrimination branches. We also propose a pose-matching loss to learn the pose consistency of 3D GANs. In addition, we present a pose sampling strategy to increase learning opportunities for steep angles in a pose-imbalanced dataset. With extensive validation, we demonstrate that our approach enables 3D GANs to generate high-quality geometries and photo-realistic images irrespective of the camera pose.
</details>
<details>
<summary>摘要</summary>
Recent 3D-aware生成模型已经显示了多视图一致的真实图像生成，但生成图像质量随着摄像头姿态的变化而下降（例如，一个在侧视角度的人脸会有模糊和噪声的边沿）。这种下降主要是由于学习多视图一致和真实图像同时存在 dataset 中 pose 异常的问题。在这篇论文中，我们提出了 SideGAN，一种新的3D GAN 训练方法，可以不受摄像头姿态的限制生成真实图像。为了缓解学习多视图一致和真实图像生成的复杂问题，我们将问题分解为两个更容易解决的子问题。具体来说，我们将问题定义为两个简单的识别问题的组合：一个识别生成图像是否真实，另一个识别生成图像是否与摄像头姿态一致。基于这，我们提出了一个 dual-branched 识别器，以及一个 pose 匹配损失来学习3D GAN 的姿态一致。此外，我们还提出了一种 pose 采样策略，以增加在 pose 偏负重 dataset 中的学习机会。通过广泛验证，我们证明了我们的方法可以使3D GANs生成高质量的几何图像和真实图像，不受摄像头姿态的限制。
</details></li>
</ul>
<hr>
<h2 id="Pointing-out-Human-Answer-Mistakes-in-a-Goal-Oriented-Visual-Dialogue"><a href="#Pointing-out-Human-Answer-Mistakes-in-a-Goal-Oriented-Visual-Dialogue" class="headerlink" title="Pointing out Human Answer Mistakes in a Goal-Oriented Visual Dialogue"></a>Pointing out Human Answer Mistakes in a Goal-Oriented Visual Dialogue</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10375">http://arxiv.org/abs/2309.10375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryosuke Oshima, Seitaro Shinagawa, Hideki Tsunashima, Qi Feng, Shigeo Morishima</li>
<li>for: 该论文旨在研究人工智能与人类交互中的有效沟通方法，以便解决复杂问题。</li>
<li>methods: 该论文使用视觉对话来助人类完成问题，并分析了人类答案错误的因素，以提高模型的准确率。</li>
<li>results: 经过实验，研究发现人类答案错误的因素包括问题类型和QA转数，这些因素对模型的准确率有重要影响。<details>
<summary>Abstract</summary>
Effective communication between humans and intelligent agents has promising applications for solving complex problems. One such approach is visual dialogue, which leverages multimodal context to assist humans. However, real-world scenarios occasionally involve human mistakes, which can cause intelligent agents to fail. While most prior research assumes perfect answers from human interlocutors, we focus on a setting where the agent points out unintentional mistakes for the interlocutor to review, better reflecting real-world situations. In this paper, we show that human answer mistakes depend on question type and QA turn in the visual dialogue by analyzing a previously unused data collection of human mistakes. We demonstrate the effectiveness of those factors for the model's accuracy in a pointing-human-mistake task through experiments using a simple MLP model and a Visual Language Model.
</details>
<details>
<summary>摘要</summary>
人机对话可以有效地解决复杂问题，其中一种方法是视觉对话，它利用多ModalContext来帮助人类。然而，实际情况中有时会出现人类的错误，这会导致智能代理人失败。而大多数先前的研究假设了人类回答是完美的，我们则关注实际情况中人类的意外错误，并对这些错误进行分析。在这篇论文中，我们发现人类回答错误的因素取决于问题类型和QA轮次在视觉对话中。我们通过使用简单的MLP模型和视觉语言模型进行实验，证明这些因素对模型准确性的影响。
</details></li>
</ul>
<hr>
<h2 id="GloPro-Globally-Consistent-Uncertainty-Aware-3D-Human-Pose-Estimation-Tracking-in-the-Wild"><a href="#GloPro-Globally-Consistent-Uncertainty-Aware-3D-Human-Pose-Estimation-Tracking-in-the-Wild" class="headerlink" title="GloPro: Globally-Consistent Uncertainty-Aware 3D Human Pose Estimation &amp; Tracking in the Wild"></a>GloPro: Globally-Consistent Uncertainty-Aware 3D Human Pose Estimation &amp; Tracking in the Wild</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10369">http://arxiv.org/abs/2309.10369</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simon Schaefer, Dorian F. Henning, Stefan Leutenegger</li>
<li>for: 提高人机交互的安全性和效率，通过提供高精度的3D人体姿态估计。</li>
<li>methods: 利用视觉启示和学习的动作模型，效果地融合视觉启示，预测3D人体姿态的不确定性分布，包括形状、姿态和根姿态。</li>
<li>results: 与现有方法相比，在世界坐标系中的人体轨迹准确率得到了大幅提高（即使面临严重遮挡），能够提供一致的不确定性分布，并可以在实时下运行。<details>
<summary>Abstract</summary>
An accurate and uncertainty-aware 3D human body pose estimation is key to enabling truly safe but efficient human-robot interactions. Current uncertainty-aware methods in 3D human pose estimation are limited to predicting the uncertainty of the body posture, while effectively neglecting the body shape and root pose. In this work, we present GloPro, which to the best of our knowledge the first framework to predict an uncertainty distribution of a 3D body mesh including its shape, pose, and root pose, by efficiently fusing visual clues with a learned motion model. We demonstrate that it vastly outperforms state-of-the-art methods in terms of human trajectory accuracy in a world coordinate system (even in the presence of severe occlusions), yields consistent uncertainty distributions, and can run in real-time.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "An accurate and uncertainty-aware 3D human body pose estimation is key to enabling truly safe but efficient human-robot interactions. Current uncertainty-aware methods in 3D human pose estimation are limited to predicting the uncertainty of the body posture, while effectively neglecting the body shape and root pose. In this work, we present GloPro, which to the best of our knowledge the first framework to predict an uncertainty distribution of a 3D body mesh including its shape, pose, and root pose, by efficiently fusing visual clues with a learned motion model. We demonstrate that it vastly outperforms state-of-the-art methods in terms of human trajectory accuracy in a world coordinate system (even in the presence of severe occlusions), yields consistent uncertainty distributions, and can run in real-time." into Simplified Chinese.翻译文本为简化字符串：“一个准确且识别不确定性的3D人体姿态估计是人机交互的关键，目前的不确定性意识方法仅仅是对体姿的预测不确定性，而忽略了身体形状和根姿。在这项工作中，我们提出了 GloPro，这是我们知道的第一个把3D身体网格的不确定性分布，包括身体形状、姿态和根姿，通过有效地融合视觉线索和学习运动模型来预测。我们示示了它在世界坐标系中的人跟踪精度高于当前状态艺术方法，且可以在真实时间内运行，并且具有一致的不确定性分布。”
</details></li>
</ul>
<hr>
<h2 id="Improving-CLIP-Robustness-with-Knowledge-Distillation-and-Self-Training"><a href="#Improving-CLIP-Robustness-with-Knowledge-Distillation-and-Self-Training" class="headerlink" title="Improving CLIP Robustness with Knowledge Distillation and Self-Training"></a>Improving CLIP Robustness with Knowledge Distillation and Self-Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10361">http://arxiv.org/abs/2309.10361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Clement Laroudie, Andrei Bursuc, Mai Lan Ha, Gianni Franchi</li>
<li>for: 本研究旨在评估CLIP模型在无监督学习上的 robustness，同时探索可以增强其 robustness 的策略。</li>
<li>methods: 我们提出了一种名为LP-CLIP的新方法，即在CLIP模型的编码结构上添加一个线性探测层，并通过使用CLIP生成的pseudo-标签和自我训练策略来训练这层。</li>
<li>results: 我们的LP-CLIP方法可以增强CLIP模型的Robustness，并在多个数据集上达到了SOTA的result。此外，我们的方法不需要标注数据，因此在实际应用中可以更加有效。<details>
<summary>Abstract</summary>
This paper examines the robustness of a multi-modal computer vision model, CLIP (Contrastive Language-Image Pretraining), in the context of unsupervised learning. The main objective is twofold: first, to evaluate the robustness of CLIP, and second, to explore strategies for augmenting its robustness. To achieve this, we introduce a novel approach named LP-CLIP. This technique involves the distillation of CLIP features through the incorporation of a linear probing layer positioned atop its encoding structure. This newly added layer is trained utilizing pseudo-labels produced by CLIP, coupled with a self-training strategy. The LP-CLIP technique offers a promising approach to enhance the robustness of CLIP without the need for annotations. By leveraging a simple linear probing layer, we aim to improve the model's ability to withstand various uncertainties and challenges commonly encountered in real-world scenarios. Importantly, our approach does not rely on annotated data, which makes it particularly valuable in situations where labeled data might be scarce or costly to obtain. Our proposed approach increases the robustness of CLIP with SOTA results compared to supervised technique on various datasets.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="RoadFormer-Duplex-Transformer-for-RGB-Normal-Semantic-Road-Scene-Parsing"><a href="#RoadFormer-Duplex-Transformer-for-RGB-Normal-Semantic-Road-Scene-Parsing" class="headerlink" title="RoadFormer: Duplex Transformer for RGB-Normal Semantic Road Scene Parsing"></a>RoadFormer: Duplex Transformer for RGB-Normal Semantic Road Scene Parsing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10356">http://arxiv.org/abs/2309.10356</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiahang Li, Yikang Zhang, Peng Yun, Guangliang Zhou, Qijun Chen, Rui Fan</li>
<li>for: 本研究旨在提出一种基于Transformer的数据融合网络 RoadFormer，用于道路场景分解。</li>
<li>methods: RoadFormer 使用 duplex encoder 架构提取不同类型的特征，并使用异类特征融合块进行有效的特征融合和重新准确。</li>
<li>results: 对于 SYN-UDTIRI 数据集以及 KITTI 路、CityScapes 和 ORFD 三个公共数据集，RoadFormer 表现出优于所有现有的状态态之网络，特别是在 KITTI 路上排名第一。<details>
<summary>Abstract</summary>
The recent advancements in deep convolutional neural networks have shown significant promise in the domain of road scene parsing. Nevertheless, the existing works focus primarily on freespace detection, with little attention given to hazardous road defects that could compromise both driving safety and comfort. In this paper, we introduce RoadFormer, a novel Transformer-based data-fusion network developed for road scene parsing. RoadFormer utilizes a duplex encoder architecture to extract heterogeneous features from both RGB images and surface normal information. The encoded features are subsequently fed into a novel heterogeneous feature synergy block for effective feature fusion and recalibration. The pixel decoder then learns multi-scale long-range dependencies from the fused and recalibrated heterogeneous features, which are subsequently processed by a Transformer decoder to produce the final semantic prediction. Additionally, we release SYN-UDTIRI, the first large-scale road scene parsing dataset that contains over 10,407 RGB images, dense depth images, and the corresponding pixel-level annotations for both freespace and road defects of different shapes and sizes. Extensive experimental evaluations conducted on our SYN-UDTIRI dataset, as well as on three public datasets, including KITTI road, CityScapes, and ORFD, demonstrate that RoadFormer outperforms all other state-of-the-art networks for road scene parsing. Specifically, RoadFormer ranks first on the KITTI road benchmark. Our source code, created dataset, and demo video are publicly available at mias.group/RoadFormer.
</details>
<details>
<summary>摘要</summary>
Recent advancements in deep convolutional neural networks have shown great potential in the field of road scene parsing. However, existing works mostly focus on free space detection, with little attention paid to hazardous road defects that can compromise both driving safety and comfort. In this paper, we introduce RoadFormer, a novel Transformer-based data-fusion network for road scene parsing. RoadFormer uses a duplex encoder architecture to extract heterogeneous features from both RGB images and surface normal information. The encoded features are then fed into a novel heterogeneous feature synergy block for effective feature fusion and recalibration. The pixel decoder learns multi-scale long-range dependencies from the fused and recalibrated heterogeneous features, which are subsequently processed by a Transformer decoder to produce the final semantic prediction. Furthermore, we release SYN-UDTIRI, the first large-scale road scene parsing dataset that contains over 10,407 RGB images, dense depth images, and the corresponding pixel-level annotations for both freespace and road defects of different shapes and sizes. Extensive experimental evaluations conducted on our SYN-UDTIRI dataset, as well as on three public datasets, including KITTI road, CityScapes, and ORFD, show that RoadFormer outperforms all other state-of-the-art networks for road scene parsing. Specifically, RoadFormer ranks first on the KITTI road benchmark. Our source code, created dataset, and demo video are publicly available at mias.group/RoadFormer.
</details></li>
</ul>
<hr>
<h2 id="Language-Guided-Adversarial-Purification"><a href="#Language-Guided-Adversarial-Purification" class="headerlink" title="Language Guided Adversarial Purification"></a>Language Guided Adversarial Purification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10348">http://arxiv.org/abs/2309.10348</a></li>
<li>repo_url: None</li>
<li>paper_authors: Himanshu Singh, A V Subramanyam</li>
<li>for: 防御 adversarial 攻击</li>
<li>methods: 使用生成模型进行敏感级别提升</li>
<li>results: 对强大 adversarial 攻击进行评估，表现出优秀的防御性能，不需要特殊的网络训练<details>
<summary>Abstract</summary>
Adversarial purification using generative models demonstrates strong adversarial defense performance. These methods are classifier and attack-agnostic, making them versatile but often computationally intensive. Recent strides in diffusion and score networks have improved image generation and, by extension, adversarial purification. Another highly efficient class of adversarial defense methods known as adversarial training requires specific knowledge of attack vectors, forcing them to be trained extensively on adversarial examples. To overcome these limitations, we introduce a new framework, namely Language Guided Adversarial Purification (LGAP), utilizing pre-trained diffusion models and caption generators to defend against adversarial attacks. Given an input image, our method first generates a caption, which is then used to guide the adversarial purification process through a diffusion network. Our approach has been evaluated against strong adversarial attacks, proving its effectiveness in enhancing adversarial robustness. Our results indicate that LGAP outperforms most existing adversarial defense techniques without requiring specialized network training. This underscores the generalizability of models trained on large datasets, highlighting a promising direction for further research.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本为：Adversarial purification using generative models demonstrates strong adversarial defense performance. These methods are classifier and attack-agnostic, making them versatile but often computationally intensive. Recent strides in diffusion and score networks have improved image generation and, by extension, adversarial purification. Another highly efficient class of adversarial defense methods known as adversarial training requires specific knowledge of attack vectors, forcing them to be trained extensively on adversarial examples. To overcome these limitations, we introduce a new framework, namely Language Guided Adversarial Purification (LGAP), utilizing pre-trained diffusion models and caption generators to defend against adversarial attacks. Given an input image, our method first generates a caption, which is then used to guide the adversarial purification process through a diffusion network. Our approach has been evaluated against strong adversarial attacks, proving its effectiveness in enhancing adversarial robustness. Our results indicate that LGAP outperforms most existing adversarial defense techniques without requiring specialized network training. This underscores the generalizability of models trained on large datasets, highlighting a promising direction for further research.Translation into Simplified Chinese:<<SYS>>使用生成模型进行对抗纯化可以达到强大的对抗防御性能。这些方法是无关于类别和攻击方式的，因此它们非常灵活，但经常需要大量计算资源。在扩散和分数网络方面，最近的进步有助于图像生成，并通过扩散网络来进行对抗纯化。另一种非常高效的对抗防御方法是对抗训练，但它需要特定的攻击方向的知识，因此需要对抗示例进行广泛训练。为了超越这些限制，我们介绍了一新的框架，即语言指导对抗纯化（LGAP），使用预训练的扩散模型和caption生成器来防御对抗攻击。给定一个输入图像，我们的方法首先生成一个caption，然后使用扩散网络来指导对抗纯化过程。我们的方法已经在强大的对抗攻击下进行评估，证明了它的效果性。我们的结果表明，LGAP在对抗防御方面超越了大多数现有的对抗防御技术，而不需要特定的网络训练。这反映了模型在大量数据上的普适性，标识了一个有前途的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Anti-Aliased-Neural-Implicit-Surfaces-with-Encoding-Level-of-Detail"><a href="#Anti-Aliased-Neural-Implicit-Surfaces-with-Encoding-Level-of-Detail" class="headerlink" title="Anti-Aliased Neural Implicit Surfaces with Encoding Level of Detail"></a>Anti-Aliased Neural Implicit Surfaces with Encoding Level of Detail</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10336">http://arxiv.org/abs/2309.10336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiyu Zhuang, Qi Zhang, Ying Feng, Hao Zhu, Yao Yao, Xiaoyu Li, Yan-Pei Cao, Ying Shan, Xun Cao</li>
<li>for: 高频几何细节恢复和抗折补新视图渲染</li>
<li>methods: 基于级别 detail（LoD）的 voxel 基本表示法， Multi-scale tri-plane Scene 表示法，可以捕捉 LoD 的 signed distance function（SDF）和空间辐射特征。</li>
<li>results: 比 state-of-the-art 方法更高效的surface 重建和 photorealistic 视图合成。<details>
<summary>Abstract</summary>
We present LoD-NeuS, an efficient neural representation for high-frequency geometry detail recovery and anti-aliased novel view rendering. Drawing inspiration from voxel-based representations with the level of detail (LoD), we introduce a multi-scale tri-plane-based scene representation that is capable of capturing the LoD of the signed distance function (SDF) and the space radiance. Our representation aggregates space features from a multi-convolved featurization within a conical frustum along a ray and optimizes the LoD feature volume through differentiable rendering. Additionally, we propose an error-guided sampling strategy to guide the growth of the SDF during the optimization. Both qualitative and quantitative evaluations demonstrate that our method achieves superior surface reconstruction and photorealistic view synthesis compared to state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
我们介绍LoD-NeuS，一种高效的神经网络表示方法，用于高频 geometry 细节恢复和抗折衔新视图渲染。 draws  inspiration from voxel-based representations with the level of detail (LoD), we introduce a multi-scale tri-plane-based scene representation that is capable of capturing the LoD of the signed distance function (SDF) and the space radiance. Our representation aggregates space features from a multi-convolved featurization within a conical frustum along a ray and optimizes the LoD feature volume through differentiable rendering. Additionally, we propose an error-guided sampling strategy to guide the growth of the SDF during the optimization. Both qualitative and quantitative evaluations demonstrate that our method achieves superior surface reconstruction and photorealistic view synthesis compared to state-of-the-art approaches.Here is the translation in Traditional Chinese:我们介绍LoD-NeuS，一种高效的神经网络表示方法，用于高频 geometry 细节恢复和抗折衔新视域渲染。 draws  inspiration from voxel-based representations with the level of detail (LoD), we introduce a multi-scale tri-plane-based scene representation that is capable of capturing the LoD of the signed distance function (SDF) and the space radiance. Our representation aggregates space features from a multi-convolved featurization within a conical frustum along a ray and optimizes the LoD feature volume through differentiable rendering. Additionally, we propose an error-guided sampling strategy to guide the growth of the SDF during the optimization. Both qualitative and quantitative evaluations demonstrate that our method achieves superior surface reconstruction and photorealistic view synthesis compared to state-of-the-art approaches.
</details></li>
</ul>
<hr>
<h2 id="Multi-dimension-Queried-and-Interacting-Network-for-Stereo-Image-Deraining"><a href="#Multi-dimension-Queried-and-Interacting-Network-for-Stereo-Image-Deraining" class="headerlink" title="Multi-dimension Queried and Interacting Network for Stereo Image Deraining"></a>Multi-dimension Queried and Interacting Network for Stereo Image Deraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10319">http://arxiv.org/abs/2309.10319</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chdwyb/mqinet">https://github.com/chdwyb/mqinet</a></li>
<li>paper_authors: Yuanbo Wen, Tao Gao, Ziqi Li, Jing Zhang, Ting Chen</li>
<li>for: 这个论文的目的是实现高效地除去双投影像中的雨尘变形。</li>
<li>methods: 这个方法使用多维度查询和互动来构建双投影像的雨尘除去模型。具体来说，它使用了一个具有上下文感知的维度别查询块（CDQB），这个模块利用维度别查询来获取双投影像中的关键特征，并且使用全局上下文感知注意力（GCA）来捕捉双投影像中的重要特征，而不是捕捉无用或不相关的信息。此外，它还引入了一个间iewPhysics-aware注意力（IPA），这个注意力基于雨水影像的倒数物理模型，可以提取双投影像中的潜在雨尘特征，并且帮助降低雨尘相关的artefacts在早期学习阶段。最后，它组合了多维度互动来增强两个看到之间的特征互动。</li>
<li>results: 实验结果显示，该模型比EPRRNet和StereoIRR更高效，具体来说，它在PSNR方面比EPRRNet和StereoIRR提高了4.18 dB和0.45 dB。代码和模型可以在\url{<a target="_blank" rel="noopener" href="https://github.com/chdwyb/MQINet%7D%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/chdwyb/MQINet}上获取。</a><details>
<summary>Abstract</summary>
Eliminating the rain degradation in stereo images poses a formidable challenge, which necessitates the efficient exploitation of mutual information present between the dual views. To this end, we devise MQINet, which employs multi-dimension queries and interactions for stereo image deraining. More specifically, our approach incorporates a context-aware dimension-wise queried block (CDQB). This module leverages dimension-wise queries that are independent of the input features and employs global context-aware attention (GCA) to capture essential features while avoiding the entanglement of redundant or irrelevant information. Meanwhile, we introduce an intra-view physics-aware attention (IPA) based on the inverse physical model of rainy images. IPA extracts shallow features that are sensitive to the physics of rain degradation, facilitating the reduction of rain-related artifacts during the early learning period. Furthermore, we integrate a cross-view multi-dimension interacting attention mechanism (CMIA) to foster comprehensive feature interaction between the two views across multiple dimensions. Extensive experimental evaluations demonstrate the superiority of our model over EPRRNet and StereoIRR, achieving respective improvements of 4.18 dB and 0.45 dB in PSNR. Code and models are available at \url{https://github.com/chdwyb/MQINet}.
</details>
<details>
<summary>摘要</summary>
避免雨损减降在双视图图像中存在强大挑战，需要高效利用双视图之间的相互信息。为此，我们设计了MQINet，它使用多维度查询和互动来进行双视图雨损减降。更具体地说，我们的方法包括一个Context-aware尺度 wise查询块（CDQB）。这个模块利用独立于输入特征的尺度 wise查询，并使用全局Context-aware注意力（GCA）来捕捉重要特征，同时避免杂乱或无关的信息。此外，我们还引入了内视图物理学感知（IPA），它基于雨水影响图像的反式物理模型，以提取雨水环境中深度特征，从而减少雨水相关的artefacts。此外，我们还将多视图多维度互动注意力机制（CMIA）与CDQB和IPA相结合，以促进多视图之间的全面特征互动。我们的模型在PSNR方面与EPRRNet和StereoIRR进行了广泛的实验评估，并实现了4.18 dB和0.45 dB的提升。代码和模型可以在 GitHub上找到：<https://github.com/chdwyb/MQINet>。
</details></li>
</ul>
<hr>
<h2 id="Dive-Deeper-into-Rectifying-Homography-for-Stereo-Camera-Online-Self-Calibration"><a href="#Dive-Deeper-into-Rectifying-Homography-for-Stereo-Camera-Online-Self-Calibration" class="headerlink" title="Dive Deeper into Rectifying Homography for Stereo Camera Online Self-Calibration"></a>Dive Deeper into Rectifying Homography for Stereo Camera Online Self-Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10314">http://arxiv.org/abs/2309.10314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongbo Zhao, Yikang Zhang, Qijun Chen, Rui Fan</li>
<li>for: 本文提出了一种基于投影 homography 的单机器推准算法，用于在仅提供一对图像时对 stero 摄像头进行在线自动化准备。</li>
<li>methods: 本文 introduce 了一种简单 yet effective 的全球最优极值外Parameters 估计方法，以及四种新的评价指标用于评估极值外Parameters 估计精度和可靠性。</li>
<li>results: 广泛的实验结果表明，提出的算法在室内和室外环境中的多种实验设置下具有更高的性能，相比基准算法。<details>
<summary>Abstract</summary>
Accurate estimation of stereo camera extrinsic parameters is the key to guarantee the performance of stereo matching algorithms. In prior arts, the online self-calibration of stereo cameras has commonly been formulated as a specialized visual odometry problem, without taking into account the principles of stereo rectification. In this paper, we first delve deeply into the concept of rectifying homography, which serves as the cornerstone for the development of our novel stereo camera online self-calibration algorithm, for cases where only a single pair of images is available. Furthermore, we introduce a simple yet effective solution for global optimum extrinsic parameter estimation in the presence of stereo video sequences. Additionally, we emphasize the impracticality of using three Euler angles and three components in the translation vectors for performance quantification. Instead, we introduce four new evaluation metrics to quantify the robustness and accuracy of extrinsic parameter estimation, applicable to both single-pair and multi-pair cases. Extensive experiments conducted across indoor and outdoor environments using various experimental setups validate the effectiveness of our proposed algorithm. The comprehensive evaluation results demonstrate its superior performance in comparison to the baseline algorithm. Our source code, demo video, and supplement are publicly available at mias.group/StereoCalibrator.
</details>
<details>
<summary>摘要</summary>
针对单一对像的情况，本文首先深入探讨了正交投影的概念，该概念是我们提出的新型双眼相机在线自动调整算法的基础。然后，我们提出了一种简单 yet effective的全球最优化外参参数估计解决方案，适用于双眼视频序列 caso。此外，我们强调了使用三个欧拉角和三个翻译向量来评估性能的不现实性，而 Instead，我们引入了四个新的评估指标来评估双眼相机外参参数估计的精度和稳定性，适用于单个对像和多对像情况。我们在室内和室外环境中进行了广泛的实验，并通过了extensive experiments demonstrate the effectiveness of our proposed algorithm, outperforming the baseline algorithm.我们的源代码、示例视频和补充材料都公开available at mias.group/StereoCalibrator。
</details></li>
</ul>
<hr>
<h2 id="Decoupled-Training-Return-of-Frustratingly-Easy-Multi-Domain-Learning"><a href="#Decoupled-Training-Return-of-Frustratingly-Easy-Multi-Domain-Learning" class="headerlink" title="Decoupled Training: Return of Frustratingly Easy Multi-Domain Learning"></a>Decoupled Training: Return of Frustratingly Easy Multi-Domain Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10302">http://arxiv.org/abs/2309.10302</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ximei Wang, Junwei Pan, Xingzhuo Guo, Dapeng Liu, Jie Jiang</li>
<li>for: 本研究的目的是提出一种简单、无参数的多Domain学习方法，以解决多个领域之间的数据偏袋和领域占据问题。</li>
<li>methods: 本研究使用的方法是一种三阶段的普通到特定的训练策略，首先在所有领域上进行温存训练，然后将每个领域分成多个头，并在固定后部上细化每个头。</li>
<li>results: 研究表明，使用该方法可以在各种数据集上达到惊人的性能，包括标准评价指标和卫星图像和推荐系统等应用领域。<details>
<summary>Abstract</summary>
Multi-domain learning (MDL) aims to train a model with minimal average risk across multiple overlapping but non-identical domains. To tackle the challenges of dataset bias and domain domination, numerous MDL approaches have been proposed from the perspectives of seeking commonalities by aligning distributions to reduce domain gap or reserving differences by implementing domain-specific towers, gates, and even experts. MDL models are becoming more and more complex with sophisticated network architectures or loss functions, introducing extra parameters and enlarging computation costs. In this paper, we propose a frustratingly easy and hyperparameter-free multi-domain learning method named Decoupled Training(D-Train). D-Train is a tri-phase general-to-specific training strategy that first pre-trains on all domains to warm up a root model, then post-trains on each domain by splitting into multi heads, and finally fine-tunes the heads by fixing the backbone, enabling decouple training to achieve domain independence. Despite its extraordinary simplicity and efficiency, D-Train performs remarkably well in extensive evaluations of various datasets from standard benchmarks to applications of satellite imagery and recommender systems.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose a simple and hyperparameter-free multi-domain learning method called Decoupled Training (D-Train). D-Train is a three-phase training strategy that first pre-trains on all domains to warm up a root model, then post-trains on each domain by splitting into multiple heads, and finally fine-tunes the heads by fixing the backbone. This approach enables decoupled training to achieve domain independence.Despite its simplicity and efficiency, D-Train performs remarkably well in extensive evaluations of various datasets from standard benchmarks to applications of satellite imagery and recommender systems.
</details></li>
</ul>
<hr>
<h2 id="360-circ-Reconstruction-From-a-Single-Image-Using-Space-Carved-Outpainting"><a href="#360-circ-Reconstruction-From-a-Single-Image-Using-Space-Carved-Outpainting" class="headerlink" title="360$^\circ$ Reconstruction From a Single Image Using Space Carved Outpainting"></a>360$^\circ$ Reconstruction From a Single Image Using Space Carved Outpainting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10279">http://arxiv.org/abs/2309.10279</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nuri Ryu, Minsu Gong, Geonung Kim, Joo-Haeng Lee, Sunghyun Cho</li>
<li>for: 创建一个全景360°视图的3D模型从单一图像</li>
<li>methods:  combinatorial使用深度和 нормаль预测、空间划分、生成模型和神经ImplicitSurface重建方法</li>
<li>results: 提供了一个高度普适的方法，能够在不同的自然场景中进行高质量的3D重建，并且比同期作品提高了重建的精度和自然性<details>
<summary>Abstract</summary>
We introduce POP3D, a novel framework that creates a full $360^\circ$-view 3D model from a single image. POP3D resolves two prominent issues that limit the single-view reconstruction. Firstly, POP3D offers substantial generalizability to arbitrary categories, a trait that previous methods struggle to achieve. Secondly, POP3D further improves reconstruction fidelity and naturalness, a crucial aspect that concurrent works fall short of. Our approach marries the strengths of four primary components: (1) a monocular depth and normal predictor that serves to predict crucial geometric cues, (2) a space carving method capable of demarcating the potentially unseen portions of the target object, (3) a generative model pre-trained on a large-scale image dataset that can complete unseen regions of the target, and (4) a neural implicit surface reconstruction method tailored in reconstructing objects using RGB images along with monocular geometric cues. The combination of these components enables POP3D to readily generalize across various in-the-wild images and generate state-of-the-art reconstructions, outperforming similar works by a significant margin. Project page: \url{http://cg.postech.ac.kr/research/POP3D}
</details>
<details>
<summary>摘要</summary>
我们引入POP3D，一个新的框架，可以从单一的图像中生成全天球360°的3D模型。POP3D解决了单一构成的两个主要问题，即具有普遍性和高重建实实相似度的能力。POP3D的方法结合了四个主要 ком成分：（1）一个单眼深度和法向预测器，用于预测重要的几何启示；（2）一种能够划分目标物的空间剖面方法；（3）一个基于大规模图像集的生成模型，用于完成未见区域的目标物；以及（4）一个适应RGB图像和单眼几何启示的神经隐面表面重建方法。这些元件的结合使得POP3D可以轻松扩展至各种在野的图像，并生成国际首席的重建效果，超过相似的作品。项目页面：<http://cg.postech.ac.kr/research/POP3D>
</details></li>
</ul>
<hr>
<h2 id="RGB-based-Category-level-Object-Pose-Estimation-via-Decoupled-Metric-Scale-Recovery"><a href="#RGB-based-Category-level-Object-Pose-Estimation-via-Decoupled-Metric-Scale-Recovery" class="headerlink" title="RGB-based Category-level Object Pose Estimation via Decoupled Metric Scale Recovery"></a>RGB-based Category-level Object Pose Estimation via Decoupled Metric Scale Recovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10255">http://arxiv.org/abs/2309.10255</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxin Wei, Xibin Song, Weizhe Liu, Laurent Kneip, Hongdong Li, Pan Ji</li>
<li>For: 提出一种新的摄像头ipeline，用于解决RGB-D摄像头 pose estimation方法中的约束问题。* Methods: 利用预训练的单目估计器提取本地几何信息，并在这些信息上进行2D-3D匹配搜索。另外，还设计了一个独立的分支来直接回归对象的度量尺度基于类别级统计。* Results: 在 both synthetic and real datasets 上进行了广泛的实验，并证明了该方法在前一代RGB-based方法中的精度更高，特别是在旋转精度方面。<details>
<summary>Abstract</summary>
While showing promising results, recent RGB-D camera-based category-level object pose estimation methods have restricted applications due to the heavy reliance on depth sensors. RGB-only methods provide an alternative to this problem yet suffer from inherent scale ambiguity stemming from monocular observations. In this paper, we propose a novel pipeline that decouples the 6D pose and size estimation to mitigate the influence of imperfect scales on rigid transformations. Specifically, we leverage a pre-trained monocular estimator to extract local geometric information, mainly facilitating the search for inlier 2D-3D correspondence. Meanwhile, a separate branch is designed to directly recover the metric scale of the object based on category-level statistics. Finally, we advocate using the RANSAC-P$n$P algorithm to robustly solve for 6D object pose. Extensive experiments have been conducted on both synthetic and real datasets, demonstrating the superior performance of our method over previous state-of-the-art RGB-based approaches, especially in terms of rotation accuracy.
</details>
<details>
<summary>摘要</summary>
“当前RGB-D相机基于分类级别物体姿态估计方法显示了扎实的结果，但由于依赖深度传感器，因此受到了应用限制。RGB只方法可以解决这个问题，但它们受到了固有的比例抽象问题。在这篇论文中，我们提出了一个新的管道，即分离6D姿态和大小估计，以减少不准确的比例对矢量变换的影响。我们利用预训练的单目估计器提取本地几何信息，主要是为了搜索2D-3D匹配。同时，我们设计了一个分支来直接回归对象的度量尺度基于类别统计。最后，我们提议使用RANSAC-P$n$P算法稳定解决6D物体姿态。我们在实验中进行了Synthetic和实际数据集的广泛测试，并证明了我们的方法在前一个状态的RGB基于方法中的稍高精度。”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="UPL-SFDA-Uncertainty-aware-Pseudo-Label-Guided-Source-Free-Domain-Adaptation-for-Medical-Image-Segmentation"><a href="#UPL-SFDA-Uncertainty-aware-Pseudo-Label-Guided-Source-Free-Domain-Adaptation-for-Medical-Image-Segmentation" class="headerlink" title="UPL-SFDA: Uncertainty-aware Pseudo Label Guided Source-Free Domain Adaptation for Medical Image Segmentation"></a>UPL-SFDA: Uncertainty-aware Pseudo Label Guided Source-Free Domain Adaptation for Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10244">http://arxiv.org/abs/2309.10244</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hilab-git/upl-sfda">https://github.com/hilab-git/upl-sfda</a></li>
<li>paper_authors: Jianghao Wu, Guotai Wang, Ran Gu, Tao Lu, Yinan Chen, Wentao Zhu, Tom Vercauteren, Sébastien Ourselin, Shaoting Zhang</li>
<li>for: 这篇研究的目的是为了提高深度学习基础的医疗影像分类模型在新目标领域中的测试影像处理，特别是在源领域数据不可用且目标领域数据没有标签的情况下进行调整。</li>
<li>methods: 本研究提出了一个新的不确定性感知驱动的源自由领域调整（SFDA）方法，包括目标领域增长（TDG）和两个前进对答案驱动（TFS）策略，以及一个平均预测值基于的熵最小化项。</li>
<li>results: 本研究在三个多地点心脏MRI分类任务、跨模型胚胎脑分类任务和3D胚胎组织分类任务上验证了UPL-SFDA方法，与基准方法相比，平均标签精度提高了5.54、5.01和6.89 percentage points。此外，UPL-SFDA方法也超过了一些现有的SFDA方法。<details>
<summary>Abstract</summary>
Domain Adaptation (DA) is important for deep learning-based medical image segmentation models to deal with testing images from a new target domain. As the source-domain data are usually unavailable when a trained model is deployed at a new center, Source-Free Domain Adaptation (SFDA) is appealing for data and annotation-efficient adaptation to the target domain. However, existing SFDA methods have a limited performance due to lack of sufficient supervision with source-domain images unavailable and target-domain images unlabeled. We propose a novel Uncertainty-aware Pseudo Label guided (UPL) SFDA method for medical image segmentation. Specifically, we propose Target Domain Growing (TDG) to enhance the diversity of predictions in the target domain by duplicating the pre-trained model's prediction head multiple times with perturbations. The different predictions in these duplicated heads are used to obtain pseudo labels for unlabeled target-domain images and their uncertainty to identify reliable pseudo labels. We also propose a Twice Forward pass Supervision (TFS) strategy that uses reliable pseudo labels obtained in one forward pass to supervise predictions in the next forward pass. The adaptation is further regularized by a mean prediction-based entropy minimization term that encourages confident and consistent results in different prediction heads. UPL-SFDA was validated with a multi-site heart MRI segmentation dataset, a cross-modality fetal brain segmentation dataset, and a 3D fetal tissue segmentation dataset. It improved the average Dice by 5.54, 5.01 and 6.89 percentage points for the three tasks compared with the baseline, respectively, and outperformed several state-of-the-art SFDA methods.
</details>
<details>
<summary>摘要</summary>
域 adaptation (DA) 是深度学习基于医疗影像分割模型的重要问题，以适应测试图像的新目标域。然而，现有的SFDA方法受到缺乏源域数据和目标域图像标注的限制，导致其性能有限。我们提出了一种新的不确定性感知导向的SFDA方法（UPL），用于医疗影像分割。具体来说，我们提出了目标域增长（TDG）技术，用于在目标域中提高预测的多样性。我们还提出了两次前进 passe supervision（TFS）策略，使用可靠的pseudo标签来监督预测。此外，我们还添加了一个平均预测值基于的Entropy最小化项，以鼓励confident和一致的结果。UPL-SFDA在多个心脏MRI分割 dataset、交叉模态胎生脑分割 dataset和3D胎生组织分割dataset上进行验证，提高了平均 dice 值5.54、5.01和6.89个百分点，分别比基eline进行了5.54、5.01和6.89个百分点的提升。此外，它还超过了一些state-of-the-art SFDA方法。
</details></li>
</ul>
<hr>
<h2 id="Transferable-Adversarial-Attack-on-Image-Tampering-Localization"><a href="#Transferable-Adversarial-Attack-on-Image-Tampering-Localization" class="headerlink" title="Transferable Adversarial Attack on Image Tampering Localization"></a>Transferable Adversarial Attack on Image Tampering Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10243">http://arxiv.org/abs/2309.10243</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuqi Wang, Gang Cao, Zijie Lou, Haochen Zhu</li>
<li>for: 评估现有数字图像修改地标算法的安全性在实际应用中。</li>
<li>methods: 提出了一种对修改地标算法的攻击方案，用于暴露其可靠性，并使其预测修改后的区域不准确。 Specifically, 使用优化和梯度来实现白&#x2F;黑盒攻击。</li>
<li>results: 广泛评估表明，提posed攻击方案可以很准确地降低地标准正确率，同时保持修改后图像的视觉质量高。<details>
<summary>Abstract</summary>
It is significant to evaluate the security of existing digital image tampering localization algorithms in real-world applications. In this paper, we propose an adversarial attack scheme to reveal the reliability of such tampering localizers, which would be fooled and fail to predict altered regions correctly. Specifically, the adversarial examples based on optimization and gradient are implemented for white/black-box attacks. Correspondingly, the adversarial example is optimized via reverse gradient propagation, and the perturbation is added adaptively in the direction of gradient rising. The black-box attack is achieved by relying on the transferability of such adversarial examples to different localizers. Extensive evaluations verify that the proposed attack sharply reduces the localization accuracy while preserving high visual quality of the attacked images.
</details>
<details>
<summary>摘要</summary>
需要评估现有数字图像修改地点算法的安全性在实际应用中。在这篇论文中，我们提出了一种攻击方案，用于暴露修改地点检测器的可靠性，这些检测器会被欺骗并 incorrect地预测修改后的区域。 Specifically，我们基于优化和梯度的例外处理实现了白/黑盒攻击。各自相应地，例外处理通过逆梯度升降来优化，并在梯度升降的方向添加随机的杂音。黑盒攻击通过将这些例外处理转移到不同的地点检测器来实现。广泛的评估表明，我们提出的攻击会减少地点检测精度，同时保持修改后的图像的视觉质量高。
</details></li>
</ul>
<hr>
<h2 id="Learning-Point-wise-Abstaining-Penalty-for-Point-Cloud-Anomaly-Detection"><a href="#Learning-Point-wise-Abstaining-Penalty-for-Point-Cloud-Anomaly-Detection" class="headerlink" title="Learning Point-wise Abstaining Penalty for Point Cloud Anomaly Detection"></a>Learning Point-wise Abstaining Penalty for Point Cloud Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10230">http://arxiv.org/abs/2309.10230</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/daniellli/pad">https://github.com/daniellli/pad</a></li>
<li>paper_authors: Shaocong Xu, Pengfei Li, Xinyu Liu, Qianpu Sun, Yang Li, Shihui Guo, Zhen Wang, Bo Jiang, Rui Wang, Kehua Sheng, Bo Zhang, Hao Zhao<br>for: 该论文旨在提高自动驾驶系统的LiDAR场景理解模块中的Out-Of-Distribution（OOD）点云识别能力。methods: 该方法基于选择性分类，即在标准关闭集分类setup中引入选择函数，以学习点云中的不同类别之间的差异。该方法还包括一个强大的合成管道，用于生成各种不同的外liers。results: 该方法在SemanticKITTI和nuScenes上达到了状态的最佳result，并且通过风险覆盖分析，揭示了不同方法的内在性质。代码和模型将公开可用。<details>
<summary>Abstract</summary>
LiDAR-based semantic scene understanding is an important module in the modern autonomous driving perception stack. However, identifying Out-Of-Distribution (OOD) points in a LiDAR point cloud is challenging as point clouds lack semantically rich features when compared with RGB images. We revisit this problem from the perspective of selective classification, which introduces a selective function into the standard closed-set classification setup. Our solution is built upon the basic idea of abstaining from choosing any known categories but learns a point-wise abstaining penalty with a marginbased loss. Synthesizing outliers to approximate unlimited OOD samples is also critical to this idea, so we propose a strong synthesis pipeline that generates outliers originated from various factors: unrealistic object categories, sampling patterns and sizes. We demonstrate that learning different abstaining penalties, apart from point-wise penalty, for different types of (synthesized) outliers can further improve the performance. We benchmark our method on SemanticKITTI and nuScenes and achieve state-of-the-art results. Risk-coverage analysis further reveals intrinsic properties of different methods. Codes and models will be publicly available.
</details>
<details>
<summary>摘要</summary>
利用LiDAR技术实现 semantic scene understanding 是现代自动驾驶感知栈中的重要模块。然而，在LiDAR点云中标识 Out-Of-Distribution (OOD) 点 cloud 是具有挑战性，因为点云在比RGB图像更缺乏semantic rich features。我们从选择性分类的角度重新评估了这个问题，并提出了一种基于选择函数的标准closed-set分类设置。我们的解决方案基于基本的想法，即在已知类别中选择点云，并学习一个点wise abstaining penalty with margin-based loss。synthesizing outliers to approximate unlimited OOD samples是关键，我们提议了一个强大的合成管道，该管道可以生成来自不同因素的outliers，包括不可能的物体类别、采样模式和大小。我们示示了不同类型的outliers的学习不同的abstaining penalties可以进一步提高性能。我们在SemanticKITTI和nuScenes上进行了比较，并达到了当前最佳的成绩。风险覆盖分析还揭示了不同方法的内在特性。代码和模型将公开 disponibles。
</details></li>
</ul>
<hr>
<h2 id="Learning-Dynamic-MRI-Reconstruction-with-Convolutional-Network-Assisted-Reconstruction-Swin-Transformer"><a href="#Learning-Dynamic-MRI-Reconstruction-with-Convolutional-Network-Assisted-Reconstruction-Swin-Transformer" class="headerlink" title="Learning Dynamic MRI Reconstruction with Convolutional Network Assisted Reconstruction Swin Transformer"></a>Learning Dynamic MRI Reconstruction with Convolutional Network Assisted Reconstruction Swin Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10227">http://arxiv.org/abs/2309.10227</a></li>
<li>repo_url: None</li>
<li>paper_authors: Di Xu, Hengjie Liu, Dan Ruan, Ke Sheng<br>for:这个论文主要是为了提高动态磁共振成像（DMRI）的诊断任务中的动态跟踪能力。methods:这篇论文使用了压缩学习（Compress sensing）和深度学习（Deep learning）技术来加速DMRI获取。具体来说，这篇论文提出了一种基于Transformer的新型推理架构，称为Reconstruction Swin Transformer（RST），用于4D MRI重建。results:实验结果表明，RST在cardiac 4D MR数据集上表现出色，与9倍加速的验证序列相比，RMSE值为0.0286±0.0199，1-SSIM值为0.0872±0.0783。这表明RST可以减少模型复杂度、GPU硬件需求和训练时间，同时保持重建质量。<details>
<summary>Abstract</summary>
Dynamic magnetic resonance imaging (DMRI) is an effective imaging tool for diagnosis tasks that require motion tracking of a certain anatomy. To speed up DMRI acquisition, k-space measurements are commonly undersampled along spatial or spatial-temporal domains. The difficulty of recovering useful information increases with increasing undersampling ratios. Compress sensing was invented for this purpose and has become the most popular method until deep learning (DL) based DMRI reconstruction methods emerged in the past decade. Nevertheless, existing DL networks are still limited in long-range sequential dependency understanding and computational efficiency and are not fully automated. Considering the success of Transformers positional embedding and "swin window" self-attention mechanism in the vision community, especially natural video understanding, we hereby propose a novel architecture named Reconstruction Swin Transformer (RST) for 4D MRI. RST inherits the backbone design of the Video Swin Transformer with a novel reconstruction head introduced to restore pixel-wise intensity. A convolution network called SADXNet is used for rapid initialization of 2D MR frames before RST learning to effectively reduce the model complexity, GPU hardware demand, and training time. Experimental results in the cardiac 4D MR dataset further substantiate the superiority of RST, achieving the lowest RMSE of 0.0286 +/- 0.0199 and 1 - SSIM of 0.0872 +/- 0.0783 on 9 times accelerated validation sequences.
</details>
<details>
<summary>摘要</summary>
dynamic magnetic resonance imaging (DMRI) 是一种有效的成像工具，用于诊断任务中需要跟踪特定的解剖结构运动。为了加速 DMRI 获取，通常将 k-空间测量下折水平或时间域。随着下折率的增加，获取有用信息的困难度也会逐渐增加。 compression sensing 是为此目的而创造的技术，并在过去的一代成为 DMRI 重建方法中最受欢迎的方法。然而，现有的深度学习（DL）基于 DMRI 重建方法仍有一定的局限性，主要是不具备长距离次序关系理解和计算效率问题，以及不具备完全自动化的特点。为了解决这些问题，我们在这里提出了一种新的架构，即 Reconstruction Swin Transformer (RST)。RST 继承了 Video Swin Transformer 的底层设计，并添加了一个新的重建头来恢复像素级强度。一个快速初始化 2D MR 帧的 convolution 网络 called SADXNet 也是用于减少模型复杂度、GPU 硬件需求和训练时间。实验结果表明，RST 在 Cardiac 4D MR 数据集上取得了最低 RMSE 值为 0.0286 ± 0.0199 和 1 - SSIM 值为 0.0872 ± 0.0783，在9倍加速验证序列上。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/19/cs.CV_2023_09_19/" data-id="clpztdnin00k4es88ea4keuls" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_09_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/19/cs.AI_2023_09_19/" class="article-date">
  <time datetime="2023-09-19T12:00:00.000Z" itemprop="datePublished">2023-09-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/19/cs.AI_2023_09_19/">cs.AI - 2023-09-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="LMDX-Language-Model-based-Document-Information-Extraction-and-Localization"><a href="#LMDX-Language-Model-based-Document-Information-Extraction-and-Localization" class="headerlink" title="LMDX: Language Model-based Document Information Extraction and Localization"></a>LMDX: Language Model-based Document Information Extraction and Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10952">http://arxiv.org/abs/2309.10952</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vincent Perot, Kai Kang, Florian Luisier, Guolong Su, Xiaoyu Sun, Ramya Sree Boppana, Zilong Wang, Jiaqi Mu, Hao Zhang, Nan Hua</li>
<li>for: This paper is written for the task of document information extraction, specifically for semi-structured documents, and aims to improve the state-of-the-art in this area.</li>
<li>methods: The paper introduces a new methodology called Language Model-based Document Information Extraction and Localization (LMDX), which adapts arbitrary large language models (LLMs) for document information extraction. LMDX uses a grounding mechanism to ensure that the extracted information is accurate and not hallucinated.</li>
<li>results: The paper evaluates LMDX on two benchmark datasets (VRDU and CORD) and achieves a new state-of-the-art in document information extraction. The results show that LMDX can extract singular, repeated, and hierarchical entities with high accuracy, both with and without training data. Additionally, the paper demonstrates the efficiency of LMDX in creating high-quality parsers with minimal data.Here’s the simplified Chinese text for the three pieces of information:</li>
<li>for: 这篇论文是为了文档信息提取而写的，特别是对半结构化文档进行提取。</li>
<li>methods: 论文提出了一种新的方法——语言模型基于文档信息提取和地点确定（LMDX），可以将任意大语言模型（LLM）适应到文档信息提取中。LMDX使用了一种安全机制，确保提取的信息准确无误。</li>
<li>results: 论文在两个标准 benchmark  datasets（VRDU和CORD）上进行了评估，实现了文档信息提取的新州OF-THE-ART。结果表明，LMDX可以准确提取唯一、重复和层次结构的实体，无需训练数据，并且可以创建高质量的解析器。<details>
<summary>Abstract</summary>
Large Language Models (LLM) have revolutionized Natural Language Processing (NLP), improving state-of-the-art on many existing tasks and exhibiting emergent capabilities. However, LLMs have not yet been successfully applied on semi-structured document information extraction, which is at the core of many document processing workflows and consists of extracting key entities from a visually rich document (VRD) given a predefined target schema. The main obstacles to LLM adoption in that task have been the absence of layout encoding within LLMs, critical for a high quality extraction, and the lack of a grounding mechanism ensuring the answer is not hallucinated. In this paper, we introduce Language Model-based Document Information Extraction and Localization (LMDX), a methodology to adapt arbitrary LLMs for document information extraction. LMDX can do extraction of singular, repeated, and hierarchical entities, both with and without training data, while providing grounding guarantees and localizing the entities within the document. In particular, we apply LMDX to the PaLM 2-S LLM and evaluate it on VRDU and CORD benchmarks, setting a new state-of-the-art and showing how LMDX enables the creation of high quality, data-efficient parsers.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Large Language Models" (LLM) 是指大型自然语言处理（NLP）模型，它们已经革命化了许多现有任务的状态。* "semi-structured document information extraction" 是指从视觉 ricH document (VRD) 中提取预定 schema 中的关键实体，这是许多文档处理工作流程的核心。* " Layout encoding" 是指在 LLM 中包含文档布局信息的机制，这是提取高质量实体的关键。* "grounding guarantees" 是指保证解决方案不会幻想的机制。* "localizing the entities" 是指将实体位于文档中的机制。* "high-quality, data-efficient parsers" 是指高质量，数据有效的解析器。
</details></li>
</ul>
<hr>
<h2 id="Benchmarks-for-Pira-2-0-a-Reading-Comprehension-Dataset-about-the-Ocean-the-Brazilian-Coast-and-Climate-Change"><a href="#Benchmarks-for-Pira-2-0-a-Reading-Comprehension-Dataset-about-the-Ocean-the-Brazilian-Coast-and-Climate-Change" class="headerlink" title="Benchmarks for Pirá 2.0, a Reading Comprehension Dataset about the Ocean, the Brazilian Coast, and Climate Change"></a>Benchmarks for Pirá 2.0, a Reading Comprehension Dataset about the Ocean, the Brazilian Coast, and Climate Change</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10945">http://arxiv.org/abs/2309.10945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paulo Pirozelli, Marcos M. José, Igor Silveira, Flávio Nakasato, Sarajane M. Peres, Anarosa A. F. Brandão, Anna H. R. Costa, Fabio G. Cozman</li>
<li>for: 这篇论文的目的是为了测试机器学习模型在科学知识领域的能力。</li>
<li>methods: 这篇论文使用了 Pir&#39;a 数据集，并定义了六个基准测试。</li>
<li>results: 这篇论文提供了多个参考值，用于测试机器学习模型在不同的问答任务上的能力。<details>
<summary>Abstract</summary>
Pir\'a is a reading comprehension dataset focused on the ocean, the Brazilian coast, and climate change, built from a collection of scientific abstracts and reports on these topics. This dataset represents a versatile language resource, particularly useful for testing the ability of current machine learning models to acquire expert scientific knowledge. Despite its potential, a detailed set of baselines has not yet been developed for Pir\'a. By creating these baselines, researchers can more easily utilize Pir\'a as a resource for testing machine learning models across a wide range of question answering tasks. In this paper, we define six benchmarks over the Pir\'a dataset, covering closed generative question answering, machine reading comprehension, information retrieval, open question answering, answer triggering, and multiple choice question answering. As part of this effort, we have also produced a curated version of the original dataset, where we fixed a number of grammar issues, repetitions, and other shortcomings. Furthermore, the dataset has been extended in several new directions, so as to face the aforementioned benchmarks: translation of supporting texts from English into Portuguese, classification labels for answerability, automatic paraphrases of questions and answers, and multiple choice candidates. The results described in this paper provide several points of reference for researchers interested in exploring the challenges provided by the Pir\'a dataset.
</details>
<details>
<summary>摘要</summary>
pia 是一个关注大洋、巴西海岸和气候变化的阅读理解数据集，基于科学报告和摘要这些主题。这个数据集代表了一个多样化的语言资源，特别有用于测试当前机器学习模型是否积累了专家科学知识。尽管其潜在，但还没有对 pia 的详细基准建立。通过创建这些基准，研究人员可以更加方便地使用 pia 作为测试机器学习模型的资源，涵盖广泛的问答任务。在这篇论文中，我们定义了六个 benchmark  sobre el dataset de pia，包括关闭生成问答、机器阅读理解、信息检索、开放问答、答案触发和多选问答。为了实现这些目标，我们还制作了 pirá dataset 的修正版本，其中修复了一些语法错误、重复和其他缺陷。此外，该 dataset 还被扩展了多个新方向，以面对以上 benchmark：翻译支持文本从英语到葡萄牙语，答案可能性的分类标签，自动重叠问题和答案，以及多选问题的选项。本文中所描述的结果提供了许多参考值 для研究人员感兴趣于探索 pia 数据集的挑战。
</details></li>
</ul>
<hr>
<h2 id="End-to-End-Speech-Recognition-Contextualization-with-Large-Language-Models"><a href="#End-to-End-Speech-Recognition-Contextualization-with-Large-Language-Models" class="headerlink" title="End-to-End Speech Recognition Contextualization with Large Language Models"></a>End-to-End Speech Recognition Contextualization with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10917">http://arxiv.org/abs/2309.10917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Egor Lakomkin, Chunyang Wu, Yassir Fathullah, Ozlem Kalinli, Michael L. Seltzer, Christian Fuegen</li>
<li>for: This paper aims to improve the performance of speech recognition models by incorporating large language models (LLMs) and contextual information.</li>
<li>methods: The authors propose a novel method that casts speech recognition as a mixed-modal language modeling task, using a pretrained LLM and providing audio features and optional text tokens for context. The system is trained in a decoder-only fashion, and the authors use adapters to add a small number of trainable parameters to unlock contextualized speech recognition capability.</li>
<li>results: The authors demonstrate a significant improvement in performance, with a 6% WER reduction when additional textual context is provided, and a 7.5% WER improvement overall and 17% WER improvement on rare words compared to a baseline contextualized RNN-T system that was trained on a much larger dataset.<details>
<summary>Abstract</summary>
In recent years, Large Language Models (LLMs) have garnered significant attention from the research community due to their exceptional performance and generalization capabilities. In this paper, we introduce a novel method for contextualizing speech recognition models incorporating LLMs. Our approach casts speech recognition as a mixed-modal language modeling task based on a pretrained LLM. We provide audio features, along with optional text tokens for context, to train the system to complete transcriptions in a decoder-only fashion. As a result, the system is implicitly incentivized to learn how to leverage unstructured contextual information during training. Our empirical results demonstrate a significant improvement in performance, with a 6% WER reduction when additional textual context is provided. Moreover, we find that our method performs competitively and improve by 7.5% WER overall and 17% WER on rare words against a baseline contextualized RNN-T system that has been trained on more than twenty five times larger speech dataset. Overall, we demonstrate that by only adding a handful number of trainable parameters via adapters, we can unlock contextualized speech recognition capability for the pretrained LLM while keeping the same text-only input functionality.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:Recently, Large Language Models (LLMs) have received significant attention from the research community due to their exceptional performance and generalization capabilities. In this paper, we propose a novel method for contextualizing speech recognition models using LLMs. Our approach treats speech recognition as a mixed-modal language modeling task based on a pre-trained LLM, and we provide audio features and optional text tokens for context to train the system in a decoder-only fashion. This approach implicitly incentivizes the system to learn how to leverage unstructured contextual information during training. Our experimental results show a significant improvement in performance, with a 6% WER reduction when additional textual context is provided. Moreover, we find that our method performs competitively and improves by 7.5% WER overall and 17% WER on rare words against a baseline contextualized RNN-T system that was trained on a much larger speech dataset. Overall, we demonstrate that by adding only a small number of trainable parameters via adapters, we can unlock contextualized speech recognition capabilities for the pre-trained LLM while maintaining the same text-only input functionality.
</details></li>
</ul>
<hr>
<h2 id="Amplifying-Pathological-Detection-in-EEG-Signaling-Pathways-through-Cross-Dataset-Transfer-Learning"><a href="#Amplifying-Pathological-Detection-in-EEG-Signaling-Pathways-through-Cross-Dataset-Transfer-Learning" class="headerlink" title="Amplifying Pathological Detection in EEG Signaling Pathways through Cross-Dataset Transfer Learning"></a>Amplifying Pathological Detection in EEG Signaling Pathways through Cross-Dataset Transfer Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10910">http://arxiv.org/abs/2309.10910</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad-Javad Darvishi-Bayazi, Mohammad Sajjad Ghaemi, Timothee Lesort, Md Rifat Arefin, Jocelyn Faubert, Irina Rish<br>for:This paper aims to explore the effectiveness of data and model scaling, as well as cross-dataset knowledge transfer, in the context of pathology diagnosis based on EEG signals.methods:The authors use a combination of data scaling, model scaling, and cross-dataset knowledge transfer to improve the performance of their target model on a low-regime dataset. They also employ a small and generic model (ShallowNet) and a larger model (TCN) to compare their performance.results:The authors observe varying performance improvements through data scaling, and identify the challenges of possible negative transfer and the significance of some key components to overcome distribution shifts and potential spurious correlations. They find that a small and generic model performs well on a single dataset, while a larger model performs better on transfer and learning from a larger and diverse dataset.<details>
<summary>Abstract</summary>
Pathology diagnosis based on EEG signals and decoding brain activity holds immense importance in understanding neurological disorders. With the advancement of artificial intelligence methods and machine learning techniques, the potential for accurate data-driven diagnoses and effective treatments has grown significantly. However, applying machine learning algorithms to real-world datasets presents diverse challenges at multiple levels. The scarcity of labelled data, especially in low regime scenarios with limited availability of real patient cohorts due to high costs of recruitment, underscores the vital deployment of scaling and transfer learning techniques. In this study, we explore a real-world pathology classification task to highlight the effectiveness of data and model scaling and cross-dataset knowledge transfer. As such, we observe varying performance improvements through data scaling, indicating the need for careful evaluation and labelling. Additionally, we identify the challenges of possible negative transfer and emphasize the significance of some key components to overcome distribution shifts and potential spurious correlations and achieve positive transfer. We see improvement in the performance of the target model on the target (NMT) datasets by using the knowledge from the source dataset (TUAB) when a low amount of labelled data was available. Our findings indicate a small and generic model (e.g. ShallowNet) performs well on a single dataset, however, a larger model (e.g. TCN) performs better on transfer and learning from a larger and diverse dataset.
</details>
<details>
<summary>摘要</summary>
依据EEG信号的 PATHOLOGY诊断利用人工智能方法和机器学习技术已经具有重要意义，用于理解神经系统疾病。随着人工智能方法和机器学习技术的发展，实时数据驱动诊断和有效治疗的潜力已经增长了。然而，在实际数据集上应用机器学习算法时，存在多种挑战，包括数据的罕见性和限制性。在这种情况下，扩大数据和数据转移学习成为了非常重要的。在这个研究中，我们使用一个真实世界的 PATHOLOGY 分类任务来探讨数据和模型的扩大和交叉数据集知识传递的效果。我们发现，通过数据扩大，模型的性能有所提高，需要仔细评估和标注。此外，我们还发现了可能的负向传递和分布shift的挑战，并提出了一些关键组件来超越这些挑战。我们发现，使用源数据集（TUAB）的知识可以在有限的标签数据情况下提高目标模型（NMT）的性能。我们的发现表明，一个小型和通用的模型（例如ShallowNet）在单个数据集上表现良好，而一个更大的模型（例如TCN）在转移学习和学习从更大和多样化的数据集上表现更好。
</details></li>
</ul>
<hr>
<h2 id="Multicopy-Reinforcement-Learning-Agents"><a href="#Multicopy-Reinforcement-Learning-Agents" class="headerlink" title="Multicopy Reinforcement Learning Agents"></a>Multicopy Reinforcement Learning Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10908">http://arxiv.org/abs/2309.10908</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alicia P. Wolfe, Oliver Diamond, Remi Feuerman, Magdalena Kisielinska, Brigitte Goeler-Slough, Victoria Manfredi</li>
<li>for: 这种研究旨在解决一种多智能问题，在其中一个智能创建多个同一个智能的复制品来完成单个智能任务更好或更高效。</li>
<li>methods: 我们提出了一种学习算法，该算法利用值函数的结构来高效地学习如何平衡多 копи地智能的优点和成本。</li>
<li>results: 我们的研究表明，在噪声环境中，使用多 копи地智能可以提高任务的完成率和效率。<details>
<summary>Abstract</summary>
This paper examines a novel type of multi-agent problem, in which an agent makes multiple identical copies of itself in order to achieve a single agent task better or more efficiently. This strategy improves performance if the environment is noisy and the task is sometimes unachievable by a single agent copy. We propose a learning algorithm for this multicopy problem which takes advantage of the structure of the value function to efficiently learn how to balance the advantages and costs of adding additional copies.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:这篇论文研究了一种新型多智能问题，在这种问题中，一个智能创建多个相同的 копи体来实现单个智能任务更好或更高效。这种策略在噪音环境下提高性能，因为单个 копи体可能无法完成任务。我们提议一种学习算法来解决这个多 копи问题，该算法利用价值函数的结构来高效地学习如何平衡增加 копи数的优点和成本。
</details></li>
</ul>
<hr>
<h2 id="Artificial-Intelligence-Enabled-Intelligent-Assistant-for-Personalized-and-Adaptive-Learning-in-Higher-Education"><a href="#Artificial-Intelligence-Enabled-Intelligent-Assistant-for-Personalized-and-Adaptive-Learning-in-Higher-Education" class="headerlink" title="Artificial Intelligence-Enabled Intelligent Assistant for Personalized and Adaptive Learning in Higher Education"></a>Artificial Intelligence-Enabled Intelligent Assistant for Personalized and Adaptive Learning in Higher Education</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10892">http://arxiv.org/abs/2309.10892</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ramteja Sajja, Yusuf Sermet, Muhammed Cikmaz, David Cwiertny, Ibrahim Demir</li>
<li>for: 这篇论文旨在开发一种基于人工智能的智能助手（AIIA），用于个性化和适应性的大学学习。</li>
<li>methods: 该系统使用高级人工智能和自然语言处理技术，创造了一个互动性强、有趣的学习平台，以减轻学生的认知负担，提供易于获取信息、评估知识和个性化学习支持。</li>
<li>results: 研究发现，AIIA系统可以理解和回答学生问题，生成测验和卡片，并提供个性化学习路径，有望改善学生学习效果、参与度和满意度。<details>
<summary>Abstract</summary>
This paper presents a novel framework, Artificial Intelligence-Enabled Intelligent Assistant (AIIA), for personalized and adaptive learning in higher education. The AIIA system leverages advanced AI and Natural Language Processing (NLP) techniques to create an interactive and engaging learning platform. This platform is engineered to reduce cognitive load on learners by providing easy access to information, facilitating knowledge assessment, and delivering personalized learning support tailored to individual needs and learning styles. The AIIA's capabilities include understanding and responding to student inquiries, generating quizzes and flashcards, and offering personalized learning pathways. The research findings have the potential to significantly impact the design, implementation, and evaluation of AI-enabled Virtual Teaching Assistants (VTAs) in higher education, informing the development of innovative educational tools that can enhance student learning outcomes, engagement, and satisfaction. The paper presents the methodology, system architecture, intelligent services, and integration with Learning Management Systems (LMSs) while discussing the challenges, limitations, and future directions for the development of AI-enabled intelligent assistants in education.
</details>
<details>
<summary>摘要</summary>
The AIIA system has several capabilities, including:1. Understanding and responding to student inquiries2. Generating quizzes and flashcards3. Offering personalized learning pathwaysThe research findings have the potential to significantly impact the design, implementation, and evaluation of AI-enabled Virtual Teaching Assistants (VTAs) in higher education, and can inform the development of innovative educational tools that can enhance student learning outcomes, engagement, and satisfaction.The paper discusses the methodology, system architecture, intelligent services, and integration with Learning Management Systems (LMSs) while addressing the challenges, limitations, and future directions for the development of AI-enabled intelligent assistants in education.
</details></li>
</ul>
<hr>
<h2 id="Self-Augmentation-Improves-Zero-Shot-Cross-Lingual-Transfer"><a href="#Self-Augmentation-Improves-Zero-Shot-Cross-Lingual-Transfer" class="headerlink" title="Self-Augmentation Improves Zero-Shot Cross-Lingual Transfer"></a>Self-Augmentation Improves Zero-Shot Cross-Lingual Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10891">http://arxiv.org/abs/2309.10891</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luka-group/SALT">https://github.com/luka-group/SALT</a></li>
<li>paper_authors: Fei Wang, Kuan-Hao Huang, Kai-Wei Chang, Muhao Chen</li>
<li>for: 提高零shot跨语言传递性，不需要外部数据。</li>
<li>methods: 使用代码混合和嵌入混合自我增强，从多语言预训练语言模型中提取跨语言知识，提高下游任务的传递性。</li>
<li>results: 在XNLI和PAWS-X任务上，我们的方法能够提高零shot跨语言传递性，无需外部数据。<details>
<summary>Abstract</summary>
Zero-shot cross-lingual transfer is a central task in multilingual NLP, allowing models trained in languages with more sufficient training resources to generalize to other low-resource languages. Earlier efforts on this task use parallel corpora, bilingual dictionaries, or other annotated alignment data to improve cross-lingual transferability, which are typically expensive to obtain. In this paper, we propose a simple yet effective method, SALT, to improve the zero-shot cross-lingual transfer of the multilingual pretrained language models without the help of such external data. By incorporating code-switching and embedding mixup with self-augmentation, SALT effectively distills cross-lingual knowledge from the multilingual PLM and enhances its transferability on downstream tasks. Experimental results on XNLI and PAWS-X show that our method is able to improve zero-shot cross-lingual transferability without external data. Our code is available at https://github.com/luka-group/SALT.
</details>
<details>
<summary>摘要</summary>
zero-shot 跨语言传递是多语言NLP中的核心任务，允许基于更有 suficient 训练资源的语言模型在其他低资源语言上进行泛化。 Earlier 的尝试使用平行 corpora、双语词典或其他注解对应数据来提高跨语言传递性，这些数据通常是 expensive 的获得。 在这篇论文中，我们提出了一种简单又有效的方法，SALT，以提高多语言预训练语言模型的零shot 跨语言传递性。通过将 code-switching 和 embedding mixup 与自我束缚，SALT 有效地储存了多语言PLM 中的跨语言知识，并提高了其在下游任务的传递性。实验结果表明，我们的方法可以在 XNLI 和 PAWS-X 上提高零shot 跨语言传递性，无需外部数据。我们的代码可以在 https://github.com/luka-group/SALT 上获取。
</details></li>
</ul>
<hr>
<h2 id="Classifying-Organizations-for-Food-System-Ontologies-using-Natural-Language-Processing"><a href="#Classifying-Organizations-for-Food-System-Ontologies-using-Natural-Language-Processing" class="headerlink" title="Classifying Organizations for Food System Ontologies using Natural Language Processing"></a>Classifying Organizations for Food System Ontologies using Natural Language Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10880">http://arxiv.org/abs/2309.10880</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ICICLE-ai/Organization-Classification-for-Food-Systems">https://github.com/ICICLE-ai/Organization-Classification-for-Food-Systems</a></li>
<li>paper_authors: Tianyu Jiang, Sonia Vinogradova, Nathan Stringham, E. Louise Earl, Allan D. Hollander, Patrick R. Huber, Ellen Riloff, R. Sandra Schillo, Giorgio A. Ubbiali, Matthew Lange</li>
<li>for: 填充知识图和食品系统 Ontology 的信息</li>
<li>methods: 使用自然语言处理（NLP）方法自动分类实体</li>
<li>results: NLP 模型可以达到相对好的性能，并可以应用于许多其他分类问题<details>
<summary>Abstract</summary>
Our research explores the use of natural language processing (NLP) methods to automatically classify entities for the purpose of knowledge graph population and integration with food system ontologies. We have created NLP models that can automatically classify organizations with respect to categories associated with environmental issues as well as Standard Industrial Classification (SIC) codes, which are used by the U.S. government to characterize business activities. As input, the NLP models are provided with text snippets retrieved by the Google search engine for each organization, which serves as a textual description of the organization that is used for learning. Our experimental results show that NLP models can achieve reasonably good performance for these two classification tasks, and they rely on a general framework that could be applied to many other classification problems as well. We believe that NLP models represent a promising approach for automatically harvesting information to populate knowledge graphs and aligning the information with existing ontologies through shared categories and concepts.
</details>
<details>
<summary>摘要</summary>
我们的研究探讨了使用自然语言处理（NLP）方法自动分类实体，以填充知识图和食品系统 ontology 的目的。我们已经创建了一些 NLP 模型，可以自动将组织分类到环境问题相关的类别以及美国政府使用的标准工业分类（SIC）代码中。作为输入，NLP 模型被提供文本摘要，它们是由 Google 搜索引擎提取的每个组织的文本描述，用于学习。我们的实验结果表明，NLP 模型可以达到相对好的性能水平，并且可以应用于许多其他分类问题。我们认为，NLP 模型代表一种有前途的方法，用于自动收割信息，并将信息与现有 ontology 进行对应。
</details></li>
</ul>
<hr>
<h2 id="Believable-Minecraft-Settlements-by-Means-of-Decentralised-Iterative-Planning"><a href="#Believable-Minecraft-Settlements-by-Means-of-Decentralised-Iterative-Planning" class="headerlink" title="Believable Minecraft Settlements by Means of Decentralised Iterative Planning"></a>Believable Minecraft Settlements by Means of Decentralised Iterative Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10871">http://arxiv.org/abs/2309.10871</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arthur van der Staaij, Jelmer Prins, Vincent L. Prins, Julian Poelsma, Thera Smit, Matthias Müller-Brockhausen, Mike Preuss</li>
<li>for: 这篇论文主要是为了解决 Procedural Content Generation (PCG) 领域中的寓真性和适应随机地形的城市生成问题。</li>
<li>methods: 该论文使用了分布式、迭代的规划过程，可以转移到类似的生成过程中生成“有机”的内容。</li>
<li>results: 该论文在 Generative Settlement Design in Minecraft (GDMC) 2022 比赛中获胜，表明了其在寓真性和适应随机地形的城市生成方面的可行性。<details>
<summary>Abstract</summary>
Procedural city generation that focuses on believability and adaptability to random terrain is a difficult challenge in the field of Procedural Content Generation (PCG). Dozens of researchers compete for a realistic approach in challenges such as the Generative Settlement Design in Minecraft (GDMC), in which our method has won the 2022 competition. This was achieved through a decentralised, iterative planning process that is transferable to similar generation processes that aims to produce "organic" content procedurally.
</details>
<details>
<summary>摘要</summary>
simultradchinese渐进城市生成，强调可信度和随机地形适应性，是PCG领域的挑战之一。多达数十名研究人员竞相奔走，寻求真实的方法，如 Minecraft 的生成殖民地设计挑战（GDMC），我们的方法在2022年赛事中获胜。这一成果基于分散式、迭代 плани策略，可以应用于类似的生成过程，以生成"有机"的内容。</SYS>Note: "simplified Chinese" is a translation of the text into Chinese that uses simpler grammar and vocabulary, which is easier to understand for non-native speakers. However, the translation may not be as precise or nuanced as a translation into traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Using-AI-Uncertainty-Quantification-to-Improve-Human-Decision-Making"><a href="#Using-AI-Uncertainty-Quantification-to-Improve-Human-Decision-Making" class="headerlink" title="Using AI Uncertainty Quantification to Improve Human Decision-Making"></a>Using AI Uncertainty Quantification to Improve Human Decision-Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10852">http://arxiv.org/abs/2309.10852</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laura R. Marusich, Jonathan Z. Bakdash, Yan Zhou, Murat Kantarcioglu</li>
<li>For: The paper aims to improve human decision-making by providing additional probabilistic information from AI uncertainty quantification (UQ).* Methods: The paper uses instance-based UQ for three real datasets, trains different AI models for classification, and creates confidence intervals for UQ using random samples. The UQ is calibrated using a strictly proper scoring rule.* Results: The paper finds that providing UQ information along with AI predictions significantly improves human decision-making beyond AI predictions alone, and that this benefit generalizes across different representations of UQ information.Here are the three key points in Simplified Chinese:* For: 这篇论文目的是提高人类决策过程中的AI预测信息。* Methods: 这篇论文使用实例基于的AI不确定量评估（UQ）来提高人类决策。它使用不同的AI模型进行分类，并使用随机样本生成周围的实例的信息来创建置信区间。这些置信区间被使用正确的评估函数来评估UQ的质量。* Results: 这篇论文发现，提供UQ信息和AI预测信息可以显著提高人类决策的准确性，并且这种效果适用于不同的UQ信息表示方式。<details>
<summary>Abstract</summary>
AI Uncertainty Quantification (UQ) has the potential to improve human decision-making beyond AI predictions alone by providing additional useful probabilistic information to users. The majority of past research on AI and human decision-making has concentrated on model explainability and interpretability. We implemented instance-based UQ for three real datasets. To achieve this, we trained different AI models for classification for each dataset, and used random samples generated around the neighborhood of the given instance to create confidence intervals for UQ. The computed UQ was calibrated using a strictly proper scoring rule as a form of quality assurance for UQ. We then conducted two preregistered online behavioral experiments that compared objective human decision-making performance under different AI information conditions, including UQ. In Experiment 1, we compared decision-making for no AI (control), AI prediction alone, and AI prediction with a visualization of UQ. We found UQ significantly improved decision-making beyond the other two conditions. In Experiment 2, we focused on comparing different representations of UQ information: Point vs. distribution of uncertainty and visualization type (needle vs. dotplot). We did not find meaningful differences in decision-making performance among these different representations of UQ. Overall, our results indicate that human decision-making can be improved by providing UQ information along with AI predictions, and that this benefit generalizes across a variety of representations of UQ.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SlimPajama-DC-Understanding-Data-Combinations-for-LLM-Training"><a href="#SlimPajama-DC-Understanding-Data-Combinations-for-LLM-Training" class="headerlink" title="SlimPajama-DC: Understanding Data Combinations for LLM Training"></a>SlimPajama-DC: Understanding Data Combinations for LLM Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10818">http://arxiv.org/abs/2309.10818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiqiang Shen, Tianhua Tao, Liqun Ma, Willie Neiswanger, Joel Hestness, Natalia Vassilieva, Daria Soboleva, Eric Xing</li>
<li>for: 本研究使用SlimPajama dataset进行语言模型训练，旨在探索不同数据组合（如网络文本、Wikipedia、GitHub、书籍）对大语言模型训练的影响。</li>
<li>methods: 本研究使用SlimPajama dataset，并对其进行了全面和本地减重。然后，通过使用1.3B Cerebras-GPT模型和Alibi、SwiGLU进行训练，对不同的数据组合进行分析。</li>
<li>results: 研究发现，全球减重和本地减重对训练后的模型性能有着不同的影响。此外，研究还发现，在不同的数据组合中，提高数据多样性是关键的。最终，本研究的最佳配置比使用RedPajama dataset训练的1.3B模型表现出较好的性能。<details>
<summary>Abstract</summary>
This paper aims to understand the impacts of various data combinations (e.g., web text, wikipedia, github, books) on the training of large language models using SlimPajama. SlimPajama is a rigorously deduplicated, multi-source dataset, which has been refined and further deduplicated to 627B tokens from the extensive 1.2T tokens RedPajama dataset contributed by Together. We've termed our research as SlimPajama-DC, an empirical analysis designed to uncover fundamental characteristics and best practices associated with employing SlimPajama in the training of large language models. During our research with SlimPajama, two pivotal observations emerged: (1) Global deduplication vs. local deduplication. We analyze and discuss how global (across different sources of datasets) and local (within the single source of dataset) deduplications affect the performance of trained models. (2) Proportions of high-quality/highly-deduplicated multi-source datasets in the combination. To study this, we construct six configurations of SlimPajama dataset and train individual ones using 1.3B Cerebras-GPT model with Alibi and SwiGLU. Our best configuration outperforms the 1.3B model trained on RedPajama using the same number of training tokens by a significant margin. All our 1.3B models are trained on Cerebras 16$\times$ CS-2 cluster with a total of 80 PFLOP/s in bf16 mixed precision. We further extend our discoveries (such as increasing data diversity is crucial after global deduplication) on a 7B model with large batch-size training. Our models and the separate SlimPajama-DC datasets are available at: https://huggingface.co/MBZUAI-LLM and https://huggingface.co/datasets/cerebras/SlimPajama-627B.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="AI-Foundation-Models-for-Weather-and-Climate-Applications-Design-and-Implementation"><a href="#AI-Foundation-Models-for-Weather-and-Climate-Applications-Design-and-Implementation" class="headerlink" title="AI Foundation Models for Weather and Climate: Applications, Design, and Implementation"></a>AI Foundation Models for Weather and Climate: Applications, Design, and Implementation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10808">http://arxiv.org/abs/2309.10808</a></li>
<li>repo_url: None</li>
<li>paper_authors: S. Karthik Mukkavilli, Daniel Salles Civitarese, Johannes Schmude, Johannes Jakubik, Anne Jones, Nam Nguyen, Christopher Phillips, Sujit Roy, Shraddha Singh, Campbell Watson, Raghu Ganti, Hendrik Hamann, Udaysankar Nair, Rahul Ramachandran, Kommy Weldemariam<br>for:* 这篇论文旨在探讨使用机器学习和深度学习方法来理解大气中的混沌行为，以及如何使用这些方法进行天气预测。methods:* 这篇论文主要使用变换器、物理学习和图 neural network 等方法，以实现在相对狭小的时空尺度和特定任务上的状态顶峰性能。results:* 这篇论文认为，随着现在的生成人工智能技术的进步，现在已经可以构建一个通用的 Earth 系统模型，以及区域气象模型和中级气象模型。这些模型可以在多个领域特定下游任务上达到竞争力。<details>
<summary>Abstract</summary>
Machine learning and deep learning methods have been widely explored in understanding the chaotic behavior of the atmosphere and furthering weather forecasting. There has been increasing interest from technology companies, government institutions, and meteorological agencies in building digital twins of the Earth. Recent approaches using transformers, physics-informed machine learning, and graph neural networks have demonstrated state-of-the-art performance on relatively narrow spatiotemporal scales and specific tasks. With the recent success of generative artificial intelligence (AI) using pre-trained transformers for language modeling and vision with prompt engineering and fine-tuning, we are now moving towards generalizable AI. In particular, we are witnessing the rise of AI foundation models that can perform competitively on multiple domain-specific downstream tasks. Despite this progress, we are still in the nascent stages of a generalizable AI model for global Earth system models, regional climate models, and mesoscale weather models. Here, we review current state-of-the-art AI approaches, primarily from transformer and operator learning literature in the context of meteorology. We provide our perspective on criteria for success towards a family of foundation models for nowcasting and forecasting weather and climate predictions. We also discuss how such models can perform competitively on downstream tasks such as downscaling (super-resolution), identifying conditions conducive to the occurrence of wildfires, and predicting consequential meteorological phenomena across various spatiotemporal scales such as hurricanes and atmospheric rivers. In particular, we examine current AI methodologies and contend they have matured enough to design and implement a weather foundation model.
</details>
<details>
<summary>摘要</summary>
机器学习和深度学习方法已广泛应用于理解大气中的混沌行为以及进一步改进天气预测。现在，技术公司、政府机构和气象局都有增加兴趣于建立地球的数字孪生。最近的方法使用变换器、物理学 informed machine learning 和图 neural networks 已经在相对狭小的空间时间尺度和特定任务上达到了国际先进水平。尤其是在语言模型和视觉领域使用预训练变换器后 fine-tuning 的情况下，人工智能已经取得了很好的进步。我们现在正在向通用人工智能进化。特别是我们正在见证到通用人工智能模型可以在多个领域特定下渠道任务上表现竞争力。 DESPITE 这些进步，我们还处于全球 Earth system models、区域气候模型和 mesoscale 天气模型的通用人工智能模型的初始阶段。这里，我们评论当前领域的状态艺术方法，主要是基于 transformer 和操作学习文献中的 meteorology 方法。我们提供我们对成功 criterion 的看法，以及如何建立一个家族基础模型，以便在不同的下渠道任务上进行 nowcasting 和预测天气和气候预测。我们还讨论了如何使用这些模型在下采样、识别激发野火的条件以及预测不同的空间时间尺度的重要气象现象上表现竞争力。
</details></li>
</ul>
<hr>
<h2 id="Heuristic-Search-for-Path-Finding-with-Refuelling"><a href="#Heuristic-Search-for-Path-Finding-with-Refuelling" class="headerlink" title="Heuristic Search for Path Finding with Refuelling"></a>Heuristic Search for Path Finding with Refuelling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10796">http://arxiv.org/abs/2309.10796</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anushtup Nandy, Zhongqiang Ren, Sivakumar Rathinam, Howie Choset</li>
<li>for: 这篇论文考虑了路径找路径（PF）的一种扩展，即充油路径找路径（RF-PF）问题。与PF问题一样，RF-PF问题定义在一个图上，图 vertices是知道燃料价格的加油站，边成本取决于加油站之间的燃料消耗。RF-PF寻找最低成本路径从开始到目标Vertex的一个机器人，机器人具有有限燃料箱和有限数量的加油停留。</li>
<li>methods: 这篇论文提出了一种启发搜索算法called Refuel A* (RF-A*)，该算法在图上逐步构建部分解决方案路径，并利用准则来精简状态的排除。</li>
<li>results: 测试在大城市地图上，RF-A* 比现有的状态艺术（一种 polynomial time 算法）快速得多于一个顺序幂，并且能够找到优化解决方案。<details>
<summary>Abstract</summary>
This paper considers a generalization of the Path Finding (PF) with refueling constraints referred to as the Refuelling Path Finding (RF-PF) problem. Just like PF, the RF-PF problem is defined over a graph, where vertices are gas stations with known fuel prices, and edge costs depend on the gas consumption between the corresponding vertices. RF-PF seeks a minimum-cost path from the start to the goal vertex for a robot with a limited gas tank and a limited number of refuelling stops. While RF-PF is polynomial-time solvable, it remains a challenge to quickly compute an optimal solution in practice since the robot needs to simultaneously determine the path, where to make the stops, and the amount to refuel at each stop. This paper develops a heuristic search algorithm called Refuel A* (RF-A* ) that iteratively constructs partial solution paths from the start to the goal guided by a heuristic function while leveraging dominance rules for state pruning during planning. RF-A* is guaranteed to find an optimal solution and runs more than an order of magnitude faster than the existing state of the art (a polynomial time algorithm) when tested in large city maps with hundreds of gas stations.
</details>
<details>
<summary>摘要</summary>
这个论文考虑了路径找路（PF）的一种扩展，即充油路径找路（RF-PF）问题。与PF类似，RF-PF问题在图上定义，图 vertices 是知道燃料价格的加油站，边的成本取决于两个顶点之间的燃料消耗。RF-PF寻找最低成本路径从开始顶点到目标顶点， robot 有有限燃料箱和有限数量的加油停。虽然 RF-PF 是 polynomial-time 可解决的，但在实践中很难快速计算优化的解决方案，因为机器人需要同时确定路径、停留处和充油量。这篇论文开发了一种启发搜索算法called Refuel A*（RF-A*），该算法在启发函数的指导下逐步构建从开始顶点到目标顶点的偏好解。RF-A* 保证找到优化解决方案，并在大型城市地图上进行了大量的测试，与现有的状态艺术（一个 polynomial time 算法）比较，运行速度高于一个数量级。
</details></li>
</ul>
<hr>
<h2 id="Guide-Your-Agent-with-Adaptive-Multimodal-Rewards"><a href="#Guide-Your-Agent-with-Adaptive-Multimodal-Rewards" class="headerlink" title="Guide Your Agent with Adaptive Multimodal Rewards"></a>Guide Your Agent with Adaptive Multimodal Rewards</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10790">http://arxiv.org/abs/2309.10790</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/csmile-1006/arp">https://github.com/csmile-1006/arp</a></li>
<li>paper_authors: Changyeon Kim, Younggyo Seo, Hao Liu, Lisa Lee, Jinwoo Shin, Honglak Lee, Kimin Lee</li>
<li>for: 这篇论文的目的是提高强化学习agent在未经见过环境中的适应能力。</li>
<li>methods: 该方法使用了自然语言任务描述和预训练多Modal embedding来增强agent的总结能力。具体来说，它使用了CLIP预训练多Modal embedding来计算视觉观察和自然语言指令之间的相似性，并使用这个相似性作为奖励信号来训练返回conditioned政策。</li>
<li>results: 该方法可以有效地 mitigate目的泛化，并在面对未经见过的文本指令时实现superior的总结性能。此外，通过细化预训练多Modal encoder来提高奖励质量，进一步提高了性能。视频示例和源代码可以在项目网站（<a target="_blank" rel="noopener" href="https://sites.google.com/view/2023arp%EF%BC%89%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://sites.google.com/view/2023arp）上找到。</a><details>
<summary>Abstract</summary>
Developing an agent capable of adapting to unseen environments remains a difficult challenge in imitation learning. In this work, we present Adaptive Return-conditioned Policy (ARP), an efficient framework designed to enhance the agent's generalization ability using natural language task descriptions and pre-trained multimodal encoders. Our key idea is to calculate a similarity between visual observations and natural language instructions in the pre-trained multimodal embedding space (such as CLIP) and use it as a reward signal. We then train a return-conditioned policy using expert demonstrations labeled with multimodal rewards. Because the multimodal rewards provide adaptive signals at each timestep, our ARP effectively mitigates the goal misgeneralization. This results in superior generalization performances even when faced with unseen text instructions, compared to existing text-conditioned policies. To improve the quality of rewards, we also introduce a fine-tuning method for pre-trained multimodal encoders, further enhancing the performance. Video demonstrations and source code are available on the project website: https://sites.google.com/view/2023arp.
</details>
<details>
<summary>摘要</summary>
开发一个能够适应未看过环境的智能代理仍然是一个困难的挑战。在这个工作中，我们提出了适应返回条件策略（ARP），这是一个高效的框架，用于提高智能代理的通用能力使用自然语言任务描述和预训练多模态编码器。我们的关键想法是在预训练多模态空间（如CLIP）中计算视觉观察和自然语言指令之间的相似性，并将其作为奖励信号使用。然后，我们使用专家示范标注为多模态奖励进行返回条件策略的训练，因此我们的ARP可以有效地消除目标泛化。这导致我们在面对未看过文本指令时的总体性能强于现有的文本条件策略。为了提高奖励质量，我们还引入了预训练多模态编码器的细化方法，进一步提高性能。视频示例和源代码可以在项目网站上找到：https://sites.google.com/view/2023arp。
</details></li>
</ul>
<hr>
<h2 id="Language-as-the-Medium-Multimodal-Video-Classification-through-text-only"><a href="#Language-as-the-Medium-Multimodal-Video-Classification-through-text-only" class="headerlink" title="Language as the Medium: Multimodal Video Classification through text only"></a>Language as the Medium: Multimodal Video Classification through text only</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10783">http://arxiv.org/abs/2309.10783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laura Hanu, Anita L. Verő, James Thewlis</li>
<li>for: 本研究旨在提出一种新的模型独立方法，可以帮助解释视频中的复杂Contextual关系。</li>
<li>methods: 该方法利用大型语言模型，如GPT-3.5或Llama2，来理解视频和声音modalities的文本描述，从BLIP-2、Whisper和ImageBind获取。无需进行额外的视频-文本模型或数据集调整，我们示出了现有的LLMs可以使用这些多modal的文本描述作为“看”或“听”的代理，进行零shot多modal视频分类。</li>
<li>results: 我们在UCf-101和Kinetics等知名动作认知 benchmark上进行了评估，并示出了这些 Context-rich描述可以在视频理解任务中得到成功应用。这种方法预示着一个promising的新研究方向，即多modal机器学习模型之间的互动，可以实现更全面的视频理解。<details>
<summary>Abstract</summary>
Despite an exciting new wave of multimodal machine learning models, current approaches still struggle to interpret the complex contextual relationships between the different modalities present in videos. Going beyond existing methods that emphasize simple activities or objects, we propose a new model-agnostic approach for generating detailed textual descriptions that captures multimodal video information. Our method leverages the extensive knowledge learnt by large language models, such as GPT-3.5 or Llama2, to reason about textual descriptions of the visual and aural modalities, obtained from BLIP-2, Whisper and ImageBind. Without needing additional finetuning of video-text models or datasets, we demonstrate that available LLMs have the ability to use these multimodal textual descriptions as proxies for ``sight'' or ``hearing'' and perform zero-shot multimodal classification of videos in-context. Our evaluations on popular action recognition benchmarks, such as UCF-101 or Kinetics, show these context-rich descriptions can be successfully used in video understanding tasks. This method points towards a promising new research direction in multimodal classification, demonstrating how an interplay between textual, visual and auditory machine learning models can enable more holistic video understanding.
</details>
<details>
<summary>摘要</summary>
尽管现有一新的多modal机器学习模型浪潮，现在的方法仍然无法理解视频中不同modalities之间的复杂关系。我们提出了一种新的模型无关方法，可以生成详细的文本描述，捕捉视频信息。我们的方法利用了大语言模型，如GPT-3.5或Llama2所学习的广泛知识，来理解文本描述的视觉和听觉modalities，从BLIP-2、Whisper和ImageBind获取。不需要额外的视频-文本模型或数据集进行训练，我们示示现有的LLM可以使用这些多modal文本描述作为“视”或“听”的代理，进行零例Multimodal分类视频 tasks。我们在UCf-101和Kinetics等流行动作识别benchmark上进行评估，显示这些具有上下文的描述可以在视频理解任务中使用。这种方法指向了一个新的研究方向，证明了多modal机器学习模型之间的互动可以实现更全面的视频理解。
</details></li>
</ul>
<hr>
<h2 id="FRASIMED-a-Clinical-French-Annotated-Resource-Produced-through-Crosslingual-BERT-Based-Annotation-Projection"><a href="#FRASIMED-a-Clinical-French-Annotated-Resource-Produced-through-Crosslingual-BERT-Based-Annotation-Projection" class="headerlink" title="FRASIMED: a Clinical French Annotated Resource Produced through Crosslingual BERT-Based Annotation Projection"></a>FRASIMED: a Clinical French Annotated Resource Produced through Crosslingual BERT-Based Annotation Projection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10770">http://arxiv.org/abs/2309.10770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jamil Zaghir, Mina Bjelogrlic, Jean-Philippe Goldman, Soukaïna Aananou, Christophe Gaudet-Blavignac, Christian Lovis<br>for: 这个研究文章是为了提供一种方法来生成通过语言跨度注解 projection 的翻译版本的注解集，以增加low-resource corpora中的 annotated datasets。methods: 这种方法基于BERT语言模型，使用语言不依赖的方法，可以快速增加low-resource corpora中的 annotated datasets，只需要使用已有的开源数据资源。results: 我们的crosslingual annotation projection方法的评估结果表明其高效和准确，可以生成高质量的注解集。作为实际应用，我们开发了法语注解资源（FRASIMED），这是一个包含2’051个 sintetic clinical cases的法语注解集，可以用于开发和改进法语自然语言处理（NLP）应用。<details>
<summary>Abstract</summary>
Natural language processing (NLP) applications such as named entity recognition (NER) for low-resource corpora do not benefit from recent advances in the development of large language models (LLMs) where there is still a need for larger annotated datasets. This research article introduces a methodology for generating translated versions of annotated datasets through crosslingual annotation projection. Leveraging a language agnostic BERT-based approach, it is an efficient solution to increase low-resource corpora with few human efforts and by only using already available open data resources. Quantitative and qualitative evaluations are often lacking when it comes to evaluating the quality and effectiveness of semi-automatic data generation strategies. The evaluation of our crosslingual annotation projection approach showed both effectiveness and high accuracy in the resulting dataset. As a practical application of this methodology, we present the creation of French Annotated Resource with Semantic Information for Medical Entities Detection (FRASIMED), an annotated corpus comprising 2'051 synthetic clinical cases in French. The corpus is now available for researchers and practitioners to develop and refine French natural language processing (NLP) applications in the clinical field (https://zenodo.org/record/8355629), making it the largest open annotated corpus with linked medical concepts in French.
</details>
<details>
<summary>摘要</summary>
自然语言处理（NLP）应用程序，如命名实体识别（NER） для低资源 Corpora 不会受到最近的大语言模型（LLM）的发展所带来的 beneficial effects。这篇研究文章介绍了一种方法ologies for generating translated versions of annotated datasets through crosslingual annotation projection。通过使用语言无关的 BERT 基于方法，可以efficiently 增加低资源 Corpora 以及少量人工劳动，只需使用已有的开源数据资源。量化和质量评估是评估自动数据生成策略的重要问题，但对于我们的 crosslingual annotation projection 方法，我们的评估结果表明其效果和准确性都很高。作为这种方法的实践应用，我们介绍了创建了 French Annotated Resource with Semantic Information for Medical Entities Detection（FRASIMED），这是一个包含 2'051 个 sintetic clinical cases 的法语 annotated corpus（https://zenodo.org/record/8355629）。这个 corpus 现在可以为研究人员和实践者提供，以开发和完善法语自然语言处理（NLP）应用程序在医疗领域。
</details></li>
</ul>
<hr>
<h2 id="A-Blueprint-for-Precise-and-Fault-Tolerant-Analog-Neural-Networks"><a href="#A-Blueprint-for-Precise-and-Fault-Tolerant-Analog-Neural-Networks" class="headerlink" title="A Blueprint for Precise and Fault-Tolerant Analog Neural Networks"></a>A Blueprint for Precise and Fault-Tolerant Analog Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10759">http://arxiv.org/abs/2309.10759</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cansu Demirkiran, Lakshmi Nair, Darius Bunandar, Ajay Joshi<br>for:This paper aims to improve the energy efficiency and scalability of deep neural network (DNN) acceleration using analog computing.methods:The paper proposes using the residue number system (RNS) to compose high-precision operations from multiple low-precision operations, eliminating information loss caused by limited precision data converters.results:The study achieves $99%$ of FP32 accuracy for state-of-the-art DNN inference using data converters with only $6$-bit precision, reducing energy consumption by several orders of magnitude while maintaining the same throughput and precision. The approach is also applied to DNN training, achieving accuracy comparable to FP32 precision using $7$-bit integer arithmetic. Additionally, the paper presents a fault-tolerant dataflow using redundant RNS error-correcting codes to protect computation against noise and errors in analog accelerators.<details>
<summary>Abstract</summary>
Analog computing has reemerged as a promising avenue for accelerating deep neural networks (DNNs) due to its potential to overcome the energy efficiency and scalability challenges posed by traditional digital architectures. However, achieving high precision and DNN accuracy using such technologies is challenging, as high-precision data converters are costly and impractical. In this paper, we address this challenge by using the residue number system (RNS). RNS allows composing high-precision operations from multiple low-precision operations, thereby eliminating the information loss caused by the limited precision of the data converters. Our study demonstrates that analog accelerators utilizing the RNS-based approach can achieve ${\geq}99\%$ of FP32 accuracy for state-of-the-art DNN inference using data converters with only $6$-bit precision whereas a conventional analog core requires more than $8$-bit precision to achieve the same accuracy in the same DNNs. The reduced precision requirements imply that using RNS can reduce the energy consumption of analog accelerators by several orders of magnitude while maintaining the same throughput and precision. Our study extends this approach to DNN training, where we can efficiently train DNNs using $7$-bit integer arithmetic while achieving accuracy comparable to FP32 precision. Lastly, we present a fault-tolerant dataflow using redundant RNS error-correcting codes to protect the computation against noise and errors inherent within an analog accelerator.
</details>
<details>
<summary>摘要</summary>
Traditional digital architectures have faced challenges in terms of energy efficiency and scalability, which has led to the resurgence of analog computing as a promising avenue for accelerating deep neural networks (DNNs). However, achieving high precision and DNN accuracy using analog technologies is challenging, as high-precision data converters are costly and impractical. In this paper, we address this challenge by using the residue number system (RNS). RNS allows for the composition of high-precision operations from multiple low-precision operations, thereby eliminating the information loss caused by the limited precision of the data converters. Our study shows that analog accelerators utilizing the RNS-based approach can achieve accuracy of at least 99% of FP32 for state-of-the-art DNN inference using data converters with only 6-bit precision, whereas a conventional analog core requires more than 8-bit precision to achieve the same accuracy in the same DNNs. This reduction in precision requirements implies that using RNS can reduce the energy consumption of analog accelerators by several orders of magnitude while maintaining the same throughput and precision. Our study also extends this approach to DNN training, where we can efficiently train DNNs using 7-bit integer arithmetic while achieving accuracy comparable to FP32 precision. Finally, we present a fault-tolerant dataflow using redundant RNS error-correcting codes to protect the computation against noise and errors inherent within an analog accelerator.
</details></li>
</ul>
<hr>
<h2 id="SHOWMe-Benchmarking-Object-agnostic-Hand-Object-3D-Reconstruction"><a href="#SHOWMe-Benchmarking-Object-agnostic-Hand-Object-3D-Reconstruction" class="headerlink" title="SHOWMe: Benchmarking Object-agnostic Hand-Object 3D Reconstruction"></a>SHOWMe: Benchmarking Object-agnostic Hand-Object 3D Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10748">http://arxiv.org/abs/2309.10748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anilkumar Swamy, Vincent Leroy, Philippe Weinzaepfel, Fabien Baradel, Salma Galaaoui, Romain Bregier, Matthieu Armando, Jean-Sebastien Franco, Gregory Rogez</li>
<li>for: 本研究旨在超越现有的手套物交互数据集，提供更多的真实物体变化和精度的3D手套物 reconstruction。</li>
<li>methods: 该研究使用了一种2 stage推理管道，首先使用固定的手套物系统进行准确的 registrations，然后使用多视图重建（MVR）算法进行3D重建。</li>
<li>results: 研究表明，使用SFM工具箱或手势估计器可以实现可靠的 объек-agnostic 3D手套物重建，但这些方法仍然敏感于初始相机pose估计，具有改进的重建空间。<details>
<summary>Abstract</summary>
Recent hand-object interaction datasets show limited real object variability and rely on fitting the MANO parametric model to obtain groundtruth hand shapes. To go beyond these limitations and spur further research, we introduce the SHOWMe dataset which consists of 96 videos, annotated with real and detailed hand-object 3D textured meshes. Following recent work, we consider a rigid hand-object scenario, in which the pose of the hand with respect to the object remains constant during the whole video sequence. This assumption allows us to register sub-millimetre-precise groundtruth 3D scans to the image sequences in SHOWMe. Although simpler, this hypothesis makes sense in terms of applications where the required accuracy and level of detail is important eg., object hand-over in human-robot collaboration, object scanning, or manipulation and contact point analysis. Importantly, the rigidity of the hand-object systems allows to tackle video-based 3D reconstruction of unknown hand-held objects using a 2-stage pipeline consisting of a rigid registration step followed by a multi-view reconstruction (MVR) part. We carefully evaluate a set of non-trivial baselines for these two stages and show that it is possible to achieve promising object-agnostic 3D hand-object reconstructions employing an SfM toolbox or a hand pose estimator to recover the rigid transforms and off-the-shelf MVR algorithms. However, these methods remain sensitive to the initial camera pose estimates which might be imprecise due to lack of textures on the objects or heavy occlusions of the hands, leaving room for improvements in the reconstruction. Code and dataset are available at https://europe.naverlabs.com/research/showme
</details>
<details>
<summary>摘要</summary>
近期手Object交互数据集显示有限的真实物体多样性，并且通过适应MANO参数模型来获取实际手势。为了突破这些限制并促进更多的研究，我们介绍了SHOWMe数据集，包括96个视频，每个视频都有细节Real和3D手Object纹理网格的注释。我们遵循最近的工作，假设手Object场景是固定的，即手指与对象之间的pose保持不变 durante全个视频序列。这种假设使我们能够将亮度毫米精度的地面扫描与图像序列进行注册。虽然更简单，但这种假设在应用场景中是有意义的，例如人 robot合作中的手Object交换、物体扫描、或手指与对象的接触点分析。重要的是，固定的手Object系统使得我们可以通过一个2 stage管道来解决视频基于3D重建未知手持 объек的问题。我们仔细评估了一些非常轻量级的基准，并证明可以使用SfM工具箱或手势估计器来恢复固定变换和Off-the-shelf MVR算法来实现可靠的物体agnostic 3D手Object重建。然而，这些方法仍然敏感于初始相机pose估计，可能因为对象上缺乏文本或手指重叠而导致估计不准确，留下改进重建的空间。代码和数据集可以在https://europe.naverlabs.com/research/showme上下载。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-large-language-models’-ability-to-understand-metaphor-and-sarcasm-using-a-screening-test-for-Asperger-syndrome"><a href="#Evaluating-large-language-models’-ability-to-understand-metaphor-and-sarcasm-using-a-screening-test-for-Asperger-syndrome" class="headerlink" title="Evaluating large language models’ ability to understand metaphor and sarcasm using a screening test for Asperger syndrome"></a>Evaluating large language models’ ability to understand metaphor and sarcasm using a screening test for Asperger syndrome</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10744">http://arxiv.org/abs/2309.10744</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hiromu/llm-msst">https://github.com/hiromu/llm-msst</a></li>
<li>paper_authors: Hiromu Yakura</li>
<li>for: 本研究旨在检验 latest large language models (LLMs) 是否能够理解人类含义渊博的通信方式，包括 метафора和讽刺。</li>
<li>methods: 本研究使用标准化测试来评估 LLMs 对 метафора和讽刺的理解能力。</li>
<li>results: 研究发现，随着模型参数的增加，LLMs 对 метафора的理解能力有所提高，但对讽刺的理解能力没有改善。这表明，为了让 LLMs 理解讽刺，需要采取不同的方法。<details>
<summary>Abstract</summary>
Metaphors and sarcasm are precious fruits of our highly-evolved social communication skills. However, children with Asperger syndrome are known to have difficulties in comprehending sarcasm, even if they possess a certain level of verbal IQ sufficient for understanding metaphors. Given that, a screening test that scores the ability to understand metaphor and sarcasm has been used to differentiate Asperger syndrome from other symptoms exhibiting akin external behaviors (e.g., attention-deficit/hyperactivity disorder). This study uses the standardized test to examine the capability of recent large language models (LLMs) in understanding human nuanced communication. The results divulged that, whereas their ability to comprehend metaphors has been improved with the increase of the number of model parameters, the improvement in sarcasm understanding was not observed. This implies that an alternative approach is imperative to imbue LLMs with the capacity to grasp sarcasm, which has been associated with the amygdala, a pivotal cerebral region for emotional learning, in the case of humans.
</details>
<details>
<summary>摘要</summary>
假设和讽刺是我们高度进化的社会通信技能的珍贵果子。然而，儿童患有阿斯伯格症状时常有困难理解假设和讽刺，即使他们具有足够的语言IQ来理解 мета喻。由此而来，一种用于分 differentiate 阿斯伯格症状和其他外表相似的症状（如注意力不足障碍）的测试方法是使用标准化测试来评估人类偏向通信中的细微表达能力。本研究使用这种标准化测试来检查最新的大语言模型（LLMs）在理解人类细微通信中的能力。结果发现，虽然模型参数的增加可以提高其理解假设的能力，但对讽刺的理解则没有改善。这表明，为了让 LLMs 擅长理解讽刺，需要采取不同的方法。这种方法与人类情感学习中的 Amygdala 相关，即脑部的一个重要区域。
</details></li>
</ul>
<hr>
<h2 id="MelodyGLM-Multi-task-Pre-training-for-Symbolic-Melody-Generation"><a href="#MelodyGLM-Multi-task-Pre-training-for-Symbolic-Melody-Generation" class="headerlink" title="MelodyGLM: Multi-task Pre-training for Symbolic Melody Generation"></a>MelodyGLM: Multi-task Pre-training for Symbolic Melody Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10738">http://arxiv.org/abs/2309.10738</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/NEXTLab-ZJU/MelodyGLM">https://github.com/NEXTLab-ZJU/MelodyGLM</a></li>
<li>paper_authors: Xinda Wu, Zhijie Huang, Kejun Zhang, Jiaxing Yu, Xu Tan, Tieyao Zhang, Zihao Wang, Lingyun Sun</li>
<li>for: 这 paper 是为了提高 symbolic melody generation 的预训练方法，以便更好地捕捉多个尺度、多个维度的结构信息在音序中。</li>
<li>methods: 这 paper 使用了 multi-task pre-training 框架 MelodyGLM，并设计了 local blank infilling 和 global blank infilling 任务，以模型音序中的本地和全球结构。</li>
<li>results: 对于 melody continuation 和 melody inpainting 任务，MelodyGLM 表现出了明显的改善，特别是在 subjective 评价中，MelodyGLM 的平均提升为 0.82、0.87、0.78 和 0.94 个数据点，并且在 melody inpainting 任务上几乎与人工编写的 melody 相当。<details>
<summary>Abstract</summary>
Pre-trained language models have achieved impressive results in various music understanding and generation tasks. However, existing pre-training methods for symbolic melody generation struggle to capture multi-scale, multi-dimensional structural information in note sequences, due to the domain knowledge discrepancy between text and music. Moreover, the lack of available large-scale symbolic melody datasets limits the pre-training improvement. In this paper, we propose MelodyGLM, a multi-task pre-training framework for generating melodies with long-term structure. We design the melodic n-gram and long span sampling strategies to create local and global blank infilling tasks for modeling the local and global structures in melodies. Specifically, we incorporate pitch n-grams, rhythm n-grams, and their combined n-grams into the melodic n-gram blank infilling tasks for modeling the multi-dimensional structures in melodies. To this end, we have constructed a large-scale symbolic melody dataset, MelodyNet, containing more than 0.4 million melody pieces. MelodyNet is utilized for large-scale pre-training and domain-specific n-gram lexicon construction. Both subjective and objective evaluations demonstrate that MelodyGLM surpasses the standard and previous pre-training methods. In particular, subjective evaluations show that, on the melody continuation task, MelodyGLM gains average improvements of 0.82, 0.87, 0.78, and 0.94 in consistency, rhythmicity, structure, and overall quality, respectively. Notably, MelodyGLM nearly matches the quality of human-composed melodies on the melody inpainting task.
</details>
<details>
<summary>摘要</summary>
传统的预训练方法对象是文本和音乐之间的知识差异，使得现有的预训练方法很难捕捉多级多维结构信息在旋律中。此外，Symbolic melody的大规模数据集的可用性限制了预训练的改进。本文提出了MelodyGLM，一个多任务预训练框架，用于生成具有长期结构的旋律。我们设计了旋律n-gram和长span采样策略，以创建本地和全局的缺失填充任务，以模拟旋律中的本地和全局结构。特别是，我们将把抑音n-gram、节奏n-gram和其结合的n-gram添加到旋律n-gram缺失填充任务中，以模拟旋律中的多维结构。为此，我们建立了一个大规模的Symbolic melody数据集，MelodyNet，包含超过0.4万个旋律 Piece。MelodyNet被用于大规模预训练和域pecific n-gram词典构造。对比标准和先前的预训练方法，我们的MelodyGLM得分较高，特别是在旋律续写任务上，MelodyGLM的平均提升为0.82、0.87、0.78和0.94在一致性、节奏性、结构和总质量等方面。值得注意的是，MelodyGLM在旋律填充任务上几乎与人工制作的旋律相当。
</details></li>
</ul>
<hr>
<h2 id="Monte-Carlo-tree-search-with-uncertainty-propagation-via-optimal-transport"><a href="#Monte-Carlo-tree-search-with-uncertainty-propagation-via-optimal-transport" class="headerlink" title="Monte-Carlo tree search with uncertainty propagation via optimal transport"></a>Monte-Carlo tree search with uncertainty propagation via optimal transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10737">http://arxiv.org/abs/2309.10737</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tuan Dam, Pascal Stenger, Lukas Schneider, Joni Pajarinen, Carlo D’Eramo, Odalric-Ambrym Maillard</li>
<li>for: 这篇论文提出了一种新的备份策略，用于高度随机和部分可见的马尔可夫决策过程。</li>
<li>methods: 我们采用了一种概率方法，将值节点和行动值节点都模型为高斯分布。我们引入了一种新的备份操作符，通过计算行动值子节点的 Wasserstein 质量中心来传递估计的不确定性到根节点。</li>
<li>results: 我们提供了许多理论保证，证明我们的概率备份操作符在某些情况下具有极限吞吐量，并且在一些随机和部分可见环境中比较出色的表现。<details>
<summary>Abstract</summary>
This paper introduces a novel backup strategy for Monte-Carlo Tree Search (MCTS) designed for highly stochastic and partially observable Markov decision processes. We adopt a probabilistic approach, modeling both value and action-value nodes as Gaussian distributions. We introduce a novel backup operator that computes value nodes as the Wasserstein barycenter of their action-value children nodes; thus, propagating the uncertainty of the estimate across the tree to the root node. We study our novel backup operator when using a novel combination of $L^1$-Wasserstein barycenter with $\alpha$-divergence, by drawing a notable connection to the generalized mean backup operator. We complement our probabilistic backup operator with two sampling strategies, based on optimistic selection and Thompson sampling, obtaining our Wasserstein MCTS algorithm. We provide theoretical guarantees of asymptotic convergence to the optimal policy, and an empirical evaluation on several stochastic and partially observable environments, where our approach outperforms well-known related baselines.
</details>
<details>
<summary>摘要</summary>
The proposed backup operator is combined with two sampling strategies, based on optimistic selection and Thompson sampling, to obtain the Wasserstein MCTS algorithm. The authors provide theoretical guarantees of asymptotic convergence to the optimal policy and an empirical evaluation on several stochastic and partially observable environments, where the proposed approach outperforms well-known related baselines.Here is the translation of the text into Simplified Chinese:这篇论文介绍了一种新的 Monte Carlo Tree Search（MCTS）的备用策略，适用于高度随机的和部分可见的Markov决策过程。我们采用了一种 probabilistic 模型，将值和行动值节点都模型为 Gaussian 分布。我们提出了一种新的备用算法，计算值节点为行动值孩子节点的 Wasserstein 中心，从而在树中传递不确定性的估计。我们将该备用算法与两种抽样策略相结合，基于 optimistic selection 和 Thompson sampling，得到了 Wasserstein MCTS 算法。我们提供了对于优化策略的 asymptotic 收敛性的理论保证，并对多个随机和部分可见环境进行了 empirical 评估，其中我们的方法超过了一些相关的基准值。
</details></li>
</ul>
<hr>
<h2 id="PAMS-Platform-for-Artificial-Market-Simulations"><a href="#PAMS-Platform-for-Artificial-Market-Simulations" class="headerlink" title="PAMS: Platform for Artificial Market Simulations"></a>PAMS: Platform for Artificial Market Simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10729">http://arxiv.org/abs/2309.10729</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/masanorihirano/pams">https://github.com/masanorihirano/pams</a></li>
<li>paper_authors: Masanori Hirano, Ryosuke Takata, Kiyoshi Izumi</li>
<li>for: 这个论文提出了一个新的人工市场模拟平台，即 PAMS：Platform for Artificial Market Simulations。PAMS 是一个基于 Python 的模拟器，可以轻松地与深度学习结合，并且允许用户轻松地修改 simulation。</li>
<li>methods: 本论文使用的方法包括了深度学习，以 Predicting future prices。</li>
<li>results: 研究表明，PAMS 可以准确地预测未来的价格。<details>
<summary>Abstract</summary>
This paper presents a new artificial market simulation platform, PAMS: Platform for Artificial Market Simulations. PAMS is developed as a Python-based simulator that is easily integrated with deep learning and enabling various simulation that requires easy users' modification. In this paper, we demonstrate PAMS effectiveness through a study using agents predicting future prices by deep learning.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了一个新的人工市场模拟平台，即PAMS：基于Python的人工市场模拟平台。PAMS可以轻松地与深度学习结合，并且允许用户轻松地修改 simulate various simulation scenarios。在本文中，我们通过使用深度学习 agents 预测未来价格来证明 PAMS 的效果。Here's the translation in Traditional Chinese:这篇论文介绍了一个新的人工市场模拟平台，即PAMS：基于Python的人工市场模拟平台。PAMS可以轻松地与深度学习结合，并且允许用户轻松地修改 simulate various simulation scenarios。在本文中，我们通过使用深度学习 agents 预测未来价格来证明 PAMS 的效果。
</details></li>
</ul>
<hr>
<h2 id="Causality-Driven-One-Shot-Learning-for-Prostate-Cancer-Grading-from-MRI"><a href="#Causality-Driven-One-Shot-Learning-for-Prostate-Cancer-Grading-from-MRI" class="headerlink" title="Causality-Driven One-Shot Learning for Prostate Cancer Grading from MRI"></a>Causality-Driven One-Shot Learning for Prostate Cancer Grading from MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10725">http://arxiv.org/abs/2309.10725</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gianluca Carloni, Eva Pachetti, Sara Colantonio</li>
<li>for: 这个研究旨在提出一种自动分类医疗影像的方法，并将弱 causal 信号在影像中学习和应用。</li>
<li>methods: 我们的框架包括卷积神经网络和 causality-extractor 模组，这个模组可以将 causa-effect 关系 между特征图中的特征，帮助模型对于影像中的特征进行推断。</li>
<li>results: 我们通过一个 One-shot 学习 scheme 进行训练，包括 meta-training 和 meta-testing 任务，以评估我们的方法在低数据情况下的效果。我们对公开可用的 проstate MRI 影像集进行了二分和多分类实验，并进行了删除研究和 qualitative 评估，以验证提案的 causality-driven 模组的有效性。我们发现， causal 关系在特征之间扮演着关键角色，帮助模型更好地分类医疗影像。<details>
<summary>Abstract</summary>
In this paper, we present a novel method to automatically classify medical images that learns and leverages weak causal signals in the image. Our framework consists of a convolutional neural network backbone and a causality-extractor module that extracts cause-effect relationships between feature maps that can inform the model on the appearance of a feature in one place of the image, given the presence of another feature within some other place of the image. To evaluate the effectiveness of our approach in low-data scenarios, we train our causality-driven architecture in a One-shot learning scheme, where we propose a new meta-learning procedure entailing meta-training and meta-testing tasks that are designed using related classes but at different levels of granularity. We conduct binary and multi-class classification experiments on a publicly available dataset of prostate MRI images. To validate the effectiveness of the proposed causality-driven module, we perform an ablation study and conduct qualitative assessments using class activation maps to highlight regions strongly influencing the network's decision-making process. Our findings show that causal relationships among features play a crucial role in enhancing the model's ability to discern relevant information and yielding more reliable and interpretable predictions. This would make it a promising approach for medical image classification tasks.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的方法，用于自动分类医疗图像。我们的框架包括一个卷积神经网络背bone和一个 causality-extractor 模块，该模块EXTRACTS causal relationships between feature maps, 可以告诉模型在某个位置的图像中，某个特征的出现，受到另一个特征在另一个位置的影响。为了评估我们的方法在低数据 scenarios 中的效果，我们采用了一种 One-shot learning 方法，包括 meta-training 和 meta-testing 任务，这些任务是使用相关的类，但是在不同的粒度水平上进行设计。我们在公共可用的 проstate MRI 图像 dataset 上进行了二分和多分类分类实验。为了证明提案的 causality-driven 模块的有效性，我们进行了减少学习和qualitative assessment，使用类 activation maps 高亮模型决策过程中强烈影响的区域。我们的发现表明， causal relationships among features 在提高模型对 relevante information 的感知和取得更加可靠和可读的预测方面发挥了关键作用。这会使得这种方法在医疗图像分类任务中成为一种可靠的方法。
</details></li>
</ul>
<hr>
<h2 id="Sound-Source-Localization-is-All-about-Cross-Modal-Alignment"><a href="#Sound-Source-Localization-is-All-about-Cross-Modal-Alignment" class="headerlink" title="Sound Source Localization is All about Cross-Modal Alignment"></a>Sound Source Localization is All about Cross-Modal Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10724">http://arxiv.org/abs/2309.10724</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arda Senocak, Hyeonggon Ryu, Junsik Kim, Tae-Hyun Oh, Hanspeter Pfister, Joon Son Chung</li>
<li>for: 本研究旨在解决真正的声源定位问题，即人类可以通过视觉场景中的声音来确定声音的来源。</li>
<li>methods: 我们提出了一种涉及声音和视觉modalities的共同定位任务，以提高声音定位和视觉modalities之间的协调。</li>
<li>results: 我们的方法在声音定位和跨模态检索中表现出色，高于当前的状态艺术方法。这些结果表明，同时解决声音定位和跨模态协调任务是解决真正的声源定位问题的关键。<details>
<summary>Abstract</summary>
Humans can easily perceive the direction of sound sources in a visual scene, termed sound source localization. Recent studies on learning-based sound source localization have mainly explored the problem from a localization perspective. However, prior arts and existing benchmarks do not account for a more important aspect of the problem, cross-modal semantic understanding, which is essential for genuine sound source localization. Cross-modal semantic understanding is important in understanding semantically mismatched audio-visual events, e.g., silent objects, or off-screen sounds. To account for this, we propose a cross-modal alignment task as a joint task with sound source localization to better learn the interaction between audio and visual modalities. Thereby, we achieve high localization performance with strong cross-modal semantic understanding. Our method outperforms the state-of-the-art approaches in both sound source localization and cross-modal retrieval. Our work suggests that jointly tackling both tasks is necessary to conquer genuine sound source localization.
</details>
<details>
<summary>摘要</summary>
人类可以轻松地在视觉场景中识别声音源的方向，称为声音源localization。现在的学习基于的声音源localization研究主要从localization角度出发。然而，前一代和现有的标准没有考虑一个更重要的问题，即跨模态 semantics的理解，这是真正的声音源localization的关键。跨模态 semantics的理解能够有效地处理semantically mismatched audio-visual事件，如静物或屏外声音。为了考虑这一点，我们提议在声音源localization任务中添加跨模态对应 зада务，以更好地学习视觉modalities之间的交互。因此，我们实现了高地理位性性能和强的跨模态 semantics理解。我们的方法超越了当前状态的方法在声音源localization和跨模态retrieval两个领域。我们的工作表明，同时解决这两个任务是必要的，以解决真正的声音源localization。
</details></li>
</ul>
<hr>
<h2 id="LEA-An-A-Variant-Algorithm-with-Improved-Edge-Efficiency-for-Robot-Motion-Planning"><a href="#LEA-An-A-Variant-Algorithm-with-Improved-Edge-Efficiency-for-Robot-Motion-Planning" class="headerlink" title="LEA*: An A* Variant Algorithm with Improved Edge Efficiency for Robot Motion Planning"></a>LEA*: An A* Variant Algorithm with Improved Edge Efficiency for Robot Motion Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10722">http://arxiv.org/abs/2309.10722</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dongliangch/leastar">https://github.com/dongliangch/leastar</a></li>
<li>paper_authors: Dongliang Zheng, Panagiotis Tsiotras</li>
<li>for: 这个论文是为了提出一种新的图搜索算法，即懒边基于A*（LEA*），用于机器人运动规划。</li>
<li>methods: 该算法使用边队列和懒搜索的想法，与A*相似，具有优化的顶点效率和改进的边效率。它的实现几乎没有改变A*的基本结构，因此对前一些懒搜索算法的过渡带来了较小的负担。</li>
<li>results: 我们在2D规划问题和7度 freedom manipulator的规划中测试了LEA*和其它算法。我们对Random世界和不同的图大小进行了严格的比较，结果显示LEA*和它的彩色版本wLEA*在发现计划的速度方面与之前的算法相比较快。<details>
<summary>Abstract</summary>
In this work, we introduce a new graph search algorithm, lazy edged based A* (LEA*), for robot motion planning. By using an edge queue and exploiting the idea of lazy search, LEA* is optimally vertex efficient similar to A*, and has improved edge efficiency compared to A*. LEA* is simple and easy to implement with minimum modification to A*, resulting in a very small overhead compared to previous lazy search algorithms. We also explore the effect of inflated heuristics, which results in the weighted LEA* (wLEA*). We show that the edge efficiency of wLEA* becomes close to LazySP and, thus is near-optimal. We test LEA* and wLEA* on 2D planning problems and planning of a 7-DOF manipulator. We perform a thorough comparison with previous algorithms by considering sparse, medium, and cluttered random worlds and small, medium, and large graph sizes. Our results show that LEA* and wLEA* are the fastest algorithms to find the plan compared to previous algorithms.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们介绍了一种新的图搜索算法，懒散边基于A*（LEA*），用于机器人运动规划。通过使用边队列和懒散搜索的想法，LEA* 能够与A* 类似的顶点效率优化，并与A* 的边效率相比提高。LEA* 简单易于实现，对 previous lazy search 算法的修改 minimal，因此对于 previous lazy search 算法的 overhead 具有较小的影响。我们还探讨了膨胀式拓扑（weighted LEA*）的效果，并证明其边效率接近LazySP，因此是近似于优化的。我们在 2D 规划问题和7-DOF  manipulator 的规划中测试了 LEA* 和 weighted LEA*。我们对 previous algorithms 进行了系统比较，包括 randomly generated sparse、medium 和填充的世界，以及 small、medium 和大的图像大小。我们的结果表明 LEA* 和 weighted LEA* 比 previous algorithms 更快地查找了计划。
</details></li>
</ul>
<hr>
<h2 id="Measurement-Simplification-in-ρ-POMDP-with-Performance-Guarantees"><a href="#Measurement-Simplification-in-ρ-POMDP-with-Performance-Guarantees" class="headerlink" title="Measurement Simplification in ρ-POMDP with Performance Guarantees"></a>Measurement Simplification in ρ-POMDP with Performance Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10701">http://arxiv.org/abs/2309.10701</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tom Yotam, Vadim Indelman</li>
<li>for: 这篇论文主要目标是提出一种高效的决策方法，用于在不精确的信息下进行决策。</li>
<li>methods: 该论文使用分割观察空间的方法，以形成关于预期信息奖励的分析 bounds。这些 bounds 然后用于高效地规划，保证性能。</li>
<li>results: 该论文显示了这种方法的效果，包括在 Gaussian 信号下的性能提升，以及在实验中的速度增加。同时，它也与其他现有的方法进行比较，并证明其在活动 SLAM 场景中的优势。<details>
<summary>Abstract</summary>
Decision making under uncertainty is at the heart of any autonomous system acting with imperfect information. The cost of solving the decision making problem is exponential in the action and observation spaces, thus rendering it unfeasible for many online systems. This paper introduces a novel approach to efficient decision-making, by partitioning the high-dimensional observation space. Using the partitioned observation space, we formulate analytical bounds on the expected information-theoretic reward, for general belief distributions. These bounds are then used to plan efficiently while keeping performance guarantees. We show that the bounds are adaptive, computationally efficient, and that they converge to the original solution. We extend the partitioning paradigm and present a hierarchy of partitioned spaces that allows greater efficiency in planning. We then propose a specific variant of these bounds for Gaussian beliefs and show a theoretical performance improvement of at least a factor of 4. Finally, we compare our novel method to other state of the art algorithms in active SLAM scenarios, in simulation and in real experiments. In both cases we show a significant speed-up in planning with performance guarantees.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese translation: autonomous system acting with imperfect information 的决策问题在不确定性下充满挑战。因为动作和观察空间的成本是加性的，因此许多在线系统无法解决这个问题。这篇论文提出了一种新的方法，通过分割高维观察空间来提高决策效率。使用分割后的观察空间，我们提出了一些关于预期信息奖励的分析 bound，对于总体信念分布来说。这些 bound 然后用于有效地规划，保证性能。我们证明这些 bound 是可变的、计算效率高，并且会 converge to 原始解。我们还扩展了分割思想，并提出了一个层次结构的分割空间，以提高规划的效率。最后，我们对 Gaussian 信念中的具体变体提出了一种改进，并证明其在至少增加了4倍的性能。最后，我们在模拟和实际实验中与其他当前标准算法进行比较，并在具有性能保证的情况下显示了明显的减速。
</details></li>
</ul>
<hr>
<h2 id="From-“Let’s-Google”-to-“Let’s-ChatGPT”-Student-and-Instructor-Perspectives-on-the-influence-of-LLMs-on-Undergraduate-Engineering-Education"><a href="#From-“Let’s-Google”-to-“Let’s-ChatGPT”-Student-and-Instructor-Perspectives-on-the-influence-of-LLMs-on-Undergraduate-Engineering-Education" class="headerlink" title="From “Let’s Google” to “Let’s ChatGPT”: Student and Instructor Perspectives on the influence of LLMs on Undergraduate Engineering Education"></a>From “Let’s Google” to “Let’s ChatGPT”: Student and Instructor Perspectives on the influence of LLMs on Undergraduate Engineering Education</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10694">http://arxiv.org/abs/2309.10694</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ishika Joshi, Ritvik Budhiraja, Pranav Deepak Tanna, Lovenya Jain, Mihika Deshpande, Arjun Srivastava, Srinivas Rallapalli, Harshal D Akolekar, Jagat Sesh Challa, Dhruv Kumar<br>for: This paper aims to explore the current usage patterns, perceived benefits, threats, and challenges of Large Language Models (LLMs) among students and instructors in undergraduate engineering universities in India.methods: The study uses surveys and interviews to gather data from 1306 students, 112 student interviews, and 27 instructor interviews.results: The study finds that LLMs are currently used primarily for answering questions and providing explanations, and that students and instructors perceive benefits such as improved understanding and efficiency, but also face challenges such as the need for critical thinking and the potential for misuse. The study offers recommendations for enhancing the adoption of LLMs in undergraduate engineering education and beyond.<details>
<summary>Abstract</summary>
The rise in popularity of Large Language Models (LLMs) has prompted discussions in academic circles, with students exploring LLM-based tools for coursework inquiries and instructors exploring them for teaching and research. Even though a lot of work is underway to create LLM-based tools tailored for students and instructors, there is a lack of comprehensive user studies that capture the perspectives of students and instructors regarding LLMs. This paper addresses this gap by conducting surveys and interviews within undergraduate engineering universities in India. Using 1306 survey responses among students, 112 student interviews, and 27 instructor interviews around the academic usage of ChatGPT (a popular LLM), this paper offers insights into the current usage patterns, perceived benefits, threats, and challenges, as well as recommendations for enhancing the adoption of LLMs among students and instructors. These insights are further utilized to discuss the practical implications of LLMs in undergraduate engineering education and beyond.
</details>
<details>
<summary>摘要</summary>
LLM（大型自然语言模型）的崛起，已经引发了学术界的讨论，学生们在作业问题上使用 LLM 的工具，教师则在教学和研究中使用 LLM。虽然有很多人在开发学生和教师专门的 LLM 工具，但是没有全面的用户研究，捕捉学生和教师对 LLM 的看法。这篇论文填补了这个空白，通过在印度的大学中进行调查和采访，收集了1306名学生的问卷回答、112名学生的面对面采访和27名教师的采访，对学生和教师在学术上使用 ChatGPT（一个流行的 LLM）的现有使用模式、感受到的利点、威胁和挑战，以及提高学生和教师对 LLM 的采用的建议。这些发现还可以用来讨论 LLMS 在bachelor 工程教育中的实际应用和未来发展。
</details></li>
</ul>
<hr>
<h2 id="MINT-Evaluating-LLMs-in-Multi-turn-Interaction-with-Tools-and-Language-Feedback"><a href="#MINT-Evaluating-LLMs-in-Multi-turn-Interaction-with-Tools-and-Language-Feedback" class="headerlink" title="MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback"></a>MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10691">http://arxiv.org/abs/2309.10691</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, Heng Ji</li>
<li>for: 评估大型自然语言模型（LLM）在复杂任务解决方面的多轮交互能力。</li>
<li>methods: 利用工具和自然语言反馈来评估 LLM 的多轮交互能力，并提供一套可重复性评估框架。</li>
<li>results: 研究发现， LLM 在多轮交互中受益于工具和自然语言反馈，表现提升（绝对值）1-8% 每次工具使用和2-17% 自然语言反馈。 单 turno 性能不一定对多轮交互性能有积极影响。 SIFT 和 RLHF 等方法在 LLM 中通常减退多轮交互能力。<details>
<summary>Abstract</summary>
To solve complex tasks, large language models (LLMs) often require multiple rounds of interactions with the user, sometimes assisted by external tools. However, current evaluation paradigms often focus solely on benchmark performance with single-turn exchanges, neglecting the intricate interactions among the user, LLMs, and external tools, creating a discrepancy between benchmark evaluation and real-world use cases. We introduce MINT benchmark to evaluate LLMs' ability to solve tasks with multi-turn interactions by (1) using tools and (2) leveraging natural language feedback. To ensure reproducibility, we provide an evaluation framework where LLMs can access tools by executing Python code and receive natural language feedback from the user simulated with GPT-4. We repurpose a diverse set of established datasets and tasks focusing on reasoning, coding, and decision-making and carefully curate them into a compact subset of instances for efficient evaluation. Our analysis of 20 open- and closed-source LLMs offers intriguing findings. (1) LLMs generally benefit from tool interactions and language feedback, with performance gains (absolute, same below) of 1--8% per additional turn with tool use and 2--17% with natural language feedback. (2) Better single-turn performance does not guarantee better multi-turn performance. (3) Surprisingly, on LLMs we evaluated, we found supervised instruction-finetuning (SIFT) and reinforcement learning from human feedback (RLHF) generally hurt multi-turn capabilities. We hope MINT can help measure progress and incentivize research in improving LLMs' capabilities in multi-turn interactions, especially for open-source communities where multi-turn human evaluation has been less accessible compared to commercial LLMs with a larger user base.
</details>
<details>
<summary>摘要</summary>
LLMs 通常需要多次互动来解决复杂任务，但现有的评估方法通常只关注单次交互的性能，忽视用户、LLMs 和外部工具之间的复杂互动，从而导致评估和实际使用场景之间的差异。我们提出了 MINT 评估标准，用于评估 LLMs 在多次交互中解决任务的能力，包括使用工具和利用自然语言反馈。为确保可重复性，我们提供了一个评估框架，其中 LLMs 可以通过执行 Python 代码来访问工具，并从用户模拟器（使用 GPT-4）接收自然语言反馈。我们将一些已有的 dataset 和任务重新分配，并将其精炼成一个高效的评估集。我们对 20 个开源和关闭源 LLMs 进行分析，发现了一些有趣的发现：1. LLMs 通常受益于工具和自然语言反馈，其性能提升（绝对值）为 1-8% 每次工具使用和 2-17% 自然语言反馈。2. 更高的单次性能不一定意味着更高的多次性能。3. 对我们评估的 LLMs，我们发现了超级vised instruction-finetuning (SIFT) 和人类反馈学习 (RLHF) 通常会降低多次性能。我们希望 MINT 可以帮助测量进步，并鼓励研究人员在多次互动中提高 LLMs 的能力，特别是对于开源社区，其中多次人工评估的训练资源相对较少，相比于商业 LLMs 的更大用户基数。
</details></li>
</ul>
<hr>
<h2 id="Learning-Initialized-Trajectory-Planning-in-Unknown-Environments"><a href="#Learning-Initialized-Trajectory-Planning-in-Unknown-Environments" class="headerlink" title="Learning-Initialized Trajectory Planning in Unknown Environments"></a>Learning-Initialized Trajectory Planning in Unknown Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10683">http://arxiv.org/abs/2309.10683</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yicheng Chen, Jinjie Li, Wenyuan Qin, Yongzhao Hua, Xiwang Dong, Qingdong Li</li>
<li>for: 提高自适应飞行器在未知环境中的准确规划，以便实现更高级别的自主飞行。</li>
<li>methods: 提出了学习初始化规划器（LIT-Planner），利用神经网络规划器提供初始值，并通过批量采样进行空间-时间优化，以捕捉多模态性。</li>
<li>results: 通过对真实世界和虚拟环境进行模拟和实验，证明LIT-Planner可以减少优化时间cost，并保持规划质量。<details>
<summary>Abstract</summary>
Autonomous flight in unknown environments requires precise planning for both the spatial and temporal profiles of trajectories, which generally involves nonconvex optimization, leading to high time costs and susceptibility to local optima. To address these limitations, we introduce the Learning-Initialized Trajectory Planner (LIT-Planner), a novel approach that guides optimization using a Neural Network (NN) Planner to provide initial values. We first leverage the spatial-temporal optimization with batch sampling to generate training cases, aiming to capture multimodality in trajectories. Based on these data, the NN-Planner maps visual and inertial observations to trajectory parameters for handling unknown environments. The network outputs are then optimized to enhance both reliability and explainability, ensuring robust performance. Furthermore, we propose a framework that supports robust online replanning with tolerance to planning latency. Comprehensive simulations validate the LIT-Planner's time efficiency without compromising trajectory quality compared to optimization-based methods. Real-world experiments further demonstrate its practical suitability for autonomous drone navigation.
</details>
<details>
<summary>摘要</summary>
自适应飞行在未知环境中需要精准规划空间和时间轨迹的profile，通常是非核心优化，导致高时间成本和易陷到地点优化。为解决这些限制，我们介绍了学习INITIALIZED Trajectory Planner（LIT-Planner），一种新的方法，该使用神经网络（NN）Planner提供初始值。我们首先利用空间-时间优化批处理生成训练例子，以捕捉多模态的轨迹。基于这些数据，NN-Planner将视觉和遥感观察映射到轨迹参数，以处理未知环境。网络输出被优化，以提高可靠性和可解释性，确保robust性。此外，我们提出了支持稳定在线重新规划的框架，抗性能规划延迟。完整的 simulations validate LIT-Planner的时间效率，而不会妥协轨迹质量与优化方法相比。实际世界实验进一步证明了它的实用性。
</details></li>
</ul>
<hr>
<h2 id="Estimating-Contamination-via-Perplexity-Quantifying-Memorisation-in-Language-Model-Evaluation"><a href="#Estimating-Contamination-via-Perplexity-Quantifying-Memorisation-in-Language-Model-Evaluation" class="headerlink" title="Estimating Contamination via Perplexity: Quantifying Memorisation in Language Model Evaluation"></a>Estimating Contamination via Perplexity: Quantifying Memorisation in Language Model Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10677">http://arxiv.org/abs/2309.10677</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yucheng Li</li>
<li>for: 本研究旨在提供一种不需要全量训练数据的污染分析方法，以便对现代语言模型进行可靠的评估。</li>
<li>methods: 本研究提出了一种基于沟通能力的污染分析方法，无需访问全量训练数据。</li>
<li>results: 研究发现，近期的基础模型在文本理解和概要写作benchmark上存在显著的记忆现象，而多选问题相对较少受污染。<details>
<summary>Abstract</summary>
Data contamination in model evaluation is getting increasingly prevalent as the massive training corpora of large language models often unintentionally include benchmark samples. Therefore, contamination analysis has became an inevitable part of reliable model evaluation. However, existing method of contamination analysis requires the access of the entire training data which is often confidential for recent models. This prevent the community to rigorously audit these models and conduct accurate assessment of their capability. In this paper, we propose a novel method to quantify contamination without the access of the full training set, that measure the extent of contamination with perplexity. Our analysis provides evidence of significant memorisation of recent foundation models in popular reading comprehension, summarisation benchmarks, while multiple choice appears less contaminated.
</details>
<details>
<summary>摘要</summary>
大量语言模型的训练集中的数据污染问题在不断增加，这是由于大型语言模型的训练集经常意外包含了标准样本。因此，污染分析已成为可靠模型评估的不可或缺的一部分。然而，现有的污染分析方法需要访问整个训练数据，这些数据通常是最新的模型中的商业秘密。这会阻碍社区对这些模型进行严格审核和准确评估其能力。在这篇论文中，我们提出了一种新的方法，可以无需访问整个训练集来衡量污染程度，这种方法基于混淆度来衡量污染程度。我们的分析表明，最近的基础模型在受欢迎的阅读理解和概要 Writing benchmarks 中存在较大的记忆现象，而多选题则相对较少污染。
</details></li>
</ul>
<hr>
<h2 id="Language-Modeling-Is-Compression"><a href="#Language-Modeling-Is-Compression" class="headerlink" title="Language Modeling Is Compression"></a>Language Modeling Is Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10668">http://arxiv.org/abs/2309.10668</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/FBTT-Embedding">https://github.com/facebookresearch/FBTT-Embedding</a></li>
<li>paper_authors: Grégoire Delétang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, Marcus Hutter, Joel Veness</li>
<li>for: 这项研究的目的是用predictive模型来压缩数据，并评估大型自然语言模型的压缩能力。</li>
<li>methods: 该研究使用了大型自然语言模型，并使用了压缩视角来评估这些模型的Scaling laws、Tokenization和in-context learning能力。</li>
<li>results: 研究发现，大型自然语言模型不仅是强大的预测器，而且可以压缩图像和语音数据，比如ImageNet和LibriSpeech，以达到43.4%和16.4%的压缩率。此外，研究还表明，使用压缩视角可以使用任何压缩器（如gzip）建立conditional generative模型。<details>
<summary>Abstract</summary>
It has long been established that predictive models can be transformed into lossless compressors and vice versa. Incidentally, in recent years, the machine learning community has focused on training increasingly large and powerful self-supervised (language) models. Since these large language models exhibit impressive predictive capabilities, they are well-positioned to be strong compressors. In this work, we advocate for viewing the prediction problem through the lens of compression and evaluate the compression capabilities of large (foundation) models. We show that large language models are powerful general-purpose predictors and that the compression viewpoint provides novel insights into scaling laws, tokenization, and in-context learning. For example, Chinchilla 70B, while trained primarily on text, compresses ImageNet patches to 43.4% and LibriSpeech samples to 16.4% of their raw size, beating domain-specific compressors like PNG (58.5%) or FLAC (30.3%), respectively. Finally, we show that the prediction-compression equivalence allows us to use any compressor (like gzip) to build a conditional generative model.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="NusaWrites-Constructing-High-Quality-Corpora-for-Underrepresented-and-Extremely-Low-Resource-Languages"><a href="#NusaWrites-Constructing-High-Quality-Corpora-for-Underrepresented-and-Extremely-Low-Resource-Languages" class="headerlink" title="NusaWrites: Constructing High-Quality Corpora for Underrepresented and Extremely Low-Resource Languages"></a>NusaWrites: Constructing High-Quality Corpora for Underrepresented and Extremely Low-Resource Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10661">http://arxiv.org/abs/2309.10661</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/indonlp/nusa-writes">https://github.com/indonlp/nusa-writes</a></li>
<li>paper_authors: Samuel Cahyawijaya, Holy Lovenia, Fajri Koto, Dea Adhista, Emmanuel Dave, Sarah Oktavianti, Salsabil Maulana Akbar, Jhonson Lee, Nuur Shadieq, Tjeng Wawan Cenggoro, Hanung Wahyuning Linuwih, Bryan Wilie, Galih Pradipta Muridan, Genta Indra Winata, David Moeljadi, Alham Fikri Aji, Ayu Purwarianti, Pascale Fung</li>
<li>for: 这篇论文的目的是推广自然语言处理（NLP）技术的访问权，尤其是为underrepresented和EXTREMELY low-resource语言。</li>
<li>methods: 这篇论文使用了在线抓取和文档翻译来构建标注和无标注 corpora。然而，这些方法存在限制，包括lack of lexical diversity和local community的文化相关性。</li>
<li>results: 我们的实验结果表明，通过本地Native speakers写作 paragraphs来构建dataset可以提高lexical diversity和文化内容的质量。此外，我们还提供了\datasetname{} benchmark，包括12种underrepresented和EXTREMELY low-resource语言，这些语言在印度尼西亚被 millions of people speaking。我们的实验结果表明，现有的多语言大语言模型需要扩展到更多的underrepresented语言。我们在github上发布了nusa-writes dataset，可以在<a target="_blank" rel="noopener" href="https://github.com/IndoNLP/nusa-writes">https://github.com/IndoNLP/nusa-writes</a> 中下载。<details>
<summary>Abstract</summary>
Democratizing access to natural language processing (NLP) technology is crucial, especially for underrepresented and extremely low-resource languages. Previous research has focused on developing labeled and unlabeled corpora for these languages through online scraping and document translation. While these methods have proven effective and cost-efficient, we have identified limitations in the resulting corpora, including a lack of lexical diversity and cultural relevance to local communities. To address this gap, we conduct a case study on Indonesian local languages. We compare the effectiveness of online scraping, human translation, and paragraph writing by native speakers in constructing datasets. Our findings demonstrate that datasets generated through paragraph writing by native speakers exhibit superior quality in terms of lexical diversity and cultural content. In addition, we present the \datasetname{} benchmark, encompassing 12 underrepresented and extremely low-resource languages spoken by millions of individuals in Indonesia. Our empirical experiment results using existing multilingual large language models conclude the need to extend these models to more underrepresented languages. We release the NusaWrites dataset at https://github.com/IndoNLP/nusa-writes.
</details>
<details>
<summary>摘要</summary>
德米实现自然语言处理（NLP）技术的普及是非常重要，特别是 для那些受到歧视和资源匮乏的语言。过往的研究专注于透过网络采集和文档翻译来建立这些语言的标点和无标点数据库。although these methods have proven effective and cost-efficient，我们发现这些数据库中的缺失，包括语汇多样性和本地文化内涵。为了解决这个问题，我们进行了印尼地方语言的案例研究。我们比较了网络采集、人工翻译和本地母语者写作 paragraph的方法，以建立数据集。我们的发现是，由本地母语者写作 paragraph的数据集具有较高的语汇多样性和本地文化内涵。此外，我们还提供了 \datasetname{} 数据集，覆盖了印尼12种未代表和具有严重资源不足的语言，这些语言由 millions of individuals 使用。我们的实验结果显示，扩展现有的多语言大型语言模型到更多的未代表语言是必要的。我们在 GitHub 上发布了 NusaWrites 数据集，请参考 https://github.com/IndoNLP/nusa-writes。
</details></li>
</ul>
<hr>
<h2 id="CFGPT-Chinese-Financial-Assistant-with-Large-Language-Model"><a href="#CFGPT-Chinese-Financial-Assistant-with-Large-Language-Model" class="headerlink" title="CFGPT: Chinese Financial Assistant with Large Language Model"></a>CFGPT: Chinese Financial Assistant with Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10654">http://arxiv.org/abs/2309.10654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiangtong Li, Yuxuan Bian, Guoxuan Wang, Yang Lei, Dawei Cheng, Zhijun Ding, Changjun Jiang</li>
<li>For: The paper is written for presenting a Chinese Financial Generative Pre-trained Transformer framework (CFGPT) for natural language processing tasks in the financial domain.* Methods: The paper uses a dataset (CFData) for pre-training and supervised fine-tuning, a financial LLM (CFLLM) to manage financial texts, and a deployment framework (CFAPP) to navigate real-world financial applications. The CFLLM is trained on CFData in two stages, continued pre-training and supervised fine-tuning.* Results: The paper presents a tailored dataset (CFData) for financial natural language processing, a financial LLM (CFLLM) that can adeptly manage financial texts, and a deployment framework (CFAPP) with additional modules for multifaceted functionality in real-world applications.Here are the three information points in Simplified Chinese text:</li>
<li>for: 这篇论文是为了介绍一种基于Transformer框架的中文金融生成预训练模型（CFGPT），用于金融自然语言处理任务。</li>
<li>methods: 论文使用了一个名为CFData的数据集进行预训练和精度调整，还有一个专门为金融文本管理的金融LLM（CFLLM），以及一个用于实际应用的投放框架（CFAPP）。CFLLM通过两个阶段的预训练和精度调整来训练。</li>
<li>results: 论文提供了一个适用于金融自然语言处理的专门数据集（CFData），一个能够有效地处理金融文本的金融LLM（CFLLM），以及一个具有多方面功能的投放框架（CFAPP）。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated great potential in natural language processing tasks within the financial domain. In this work, we present a Chinese Financial Generative Pre-trained Transformer framework, named CFGPT, which includes a dataset~(CFData) for pre-training and supervised fine-tuning, a financial LLM~(CFLLM) to adeptly manage financial texts, and a deployment framework~(CFAPP) designed to navigate real-world financial applications. The CFData comprising both a pre-training dataset and a supervised fine-tuning dataset, where the pre-training dataset collates Chinese financial data and analytics, alongside a smaller subset of general-purpose text with 584M documents and 141B tokens in total, and the supervised fine-tuning dataset is tailored for six distinct financial tasks, embodying various facets of financial analysis and decision-making with 1.5M instruction pairs and 1.5B tokens in total. The CFLLM, which is based on InternLM-7B to balance the model capability and size, is trained on CFData in two stage, continued pre-training and supervised fine-tuning. The CFAPP is centered on large language models (LLMs) and augmented with additional modules to ensure multifaceted functionality in real-world application. Our codes are released at https://github.com/TongjiFinLab/CFGPT.
</details>
<details>
<summary>摘要</summary>
大型自然语言处理模型(LLMs)在金融领域内表现出了很大的潜力。在这项工作中，我们介绍了一个名为CFGPT的中文金融生成预训练 transformer框架，其包括一个名为CFData的预训练和监督练习 dataset，一个适应金融文本的金融LLM，以及一个为实际金融应用而设计的CFAPP框架。CFData包含了中文金融数据和分析，以及一小部分通用文本的584M份文档和141B个字符。CFLLM基于InternLM-7B，通过两stage的预训练和监督练习来训练。CFAPP是基于LLMs的框架，并增加了其他模块，以确保在实际应用中的多方面功能。我们的代码在https://github.com/TongjiFinLab/CFGPT上发布。
</details></li>
</ul>
<hr>
<h2 id="Towards-Energy-Aware-Federated-Traffic-Prediction-for-Cellular-Networks"><a href="#Towards-Energy-Aware-Federated-Traffic-Prediction-for-Cellular-Networks" class="headerlink" title="Towards Energy-Aware Federated Traffic Prediction for Cellular Networks"></a>Towards Energy-Aware Federated Traffic Prediction for Cellular Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10645">http://arxiv.org/abs/2309.10645</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vperifan/federated-time-series-forecasting">https://github.com/vperifan/federated-time-series-forecasting</a></li>
<li>paper_authors: Vasileios Perifanis, Nikolaos Pavlidis, Selim F. Yilmaz, Francesc Wilhelmi, Elia Guerra, Marco Miozzo, Pavlos S. Efraimidis, Paolo Dini, Remous-Aris Koutsiamanis</li>
<li>for: 预测 fifth-generation 网络流量是一项重要的活动，以便优化网络，因为准确的预测是关键 для智能网络设计、资源分配和异常情况检测。</li>
<li>methods: 本文使用了 federated learning（FL）作为一种机器学习训练框架，以提高预测精度并避免数据中心化问题。</li>
<li>results: 研究发现，大型机器学习模型在联合学习场景下可以 marginally 提高性能，但具有显著的环境影响，导致它们在实际应用中不实际。<details>
<summary>Abstract</summary>
Cellular traffic prediction is a crucial activity for optimizing networks in fifth-generation (5G) networks and beyond, as accurate forecasting is essential for intelligent network design, resource allocation and anomaly mitigation. Although machine learning (ML) is a promising approach to effectively predict network traffic, the centralization of massive data in a single data center raises issues regarding confidentiality, privacy and data transfer demands. To address these challenges, federated learning (FL) emerges as an appealing ML training framework which offers high accurate predictions through parallel distributed computations. However, the environmental impact of these methods is often overlooked, which calls into question their sustainability. In this paper, we address the trade-off between accuracy and energy consumption in FL by proposing a novel sustainability indicator that allows assessing the feasibility of ML models. Then, we comprehensively evaluate state-of-the-art deep learning (DL) architectures in a federated scenario using real-world measurements from base station (BS) sites in the area of Barcelona, Spain. Our findings indicate that larger ML models achieve marginally improved performance but have a significant environmental impact in terms of carbon footprint, which make them impractical for real-world applications.
</details>
<details>
<summary>摘要</summary>
fifth-generation (5G) 网络中的 cellular traffic prediction 是一项非常重要的活动，因为准确预测是智能网络设计、资源分配和异常现象 mitigation 的关键。虽然机器学习 (ML) 是一种有前途的方法来有效预测网络流量，但是集中大量数据在单个数据中心存储的问题会导致隐私、安全性和数据传输带宽的问题。为解决这些挑战，联邦学习 (FL) 作为一种有appeal的 ML 训练框架，通过并行分布计算来提供高精度预测。然而，这些方法的环境影响 часто被忽略，这会让它们的可持续性成为问题。在这篇论文中，我们考虑了精度和能源消耗之间的负面交互，并提出了一个可用于评估 ML 模型可持续性的新指标。然后，我们对现有的深度学习 (DL) 架构在联邦enario中进行了广泛的评估，使用了实际测量从 Barcelona, Spain 的基站 (BS) 站点。我们发现，更大的 ML 模型可以marginally提高性能，但具有 significanth carbon footprint，这使得它们在实际应用中不可持续。
</details></li>
</ul>
<hr>
<h2 id="Geometric-structure-of-Deep-Learning-networks-and-construction-of-global-mathcal-L-2-minimizers"><a href="#Geometric-structure-of-Deep-Learning-networks-and-construction-of-global-mathcal-L-2-minimizers" class="headerlink" title="Geometric structure of Deep Learning networks and construction of global ${\mathcal L}^2$ minimizers"></a>Geometric structure of Deep Learning networks and construction of global ${\mathcal L}^2$ minimizers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10639">http://arxiv.org/abs/2309.10639</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Chen, Patricia Muñoz Ewald</li>
<li>for: 这个论文的目的是对深度学习（Deep Learning）网络的结构做出几何解释，并使用$L$层抑制函数、${\mathcal L}^2$Schatten类（或希尔бер特- Schmidt）成本函数、输入和输出空间为${\mathbb R}^Q$（$Q\geq1$）。</li>
<li>methods: 这篇论文使用了作者们之前对浅层神经网络的研究结果，构建了一个可导的家族解 minimizers，以实现深度学习网络的全局最小值。在这个设置下，隐藏层神经网络”照料’’（curate）训练输入数据，通过重层应用截断函数来最小化训练输入的噪声比例。</li>
<li>results: 论文显示，在$L\geq Q$的情况下，深度学习网络的全局最小值存在$2^Q-1$个不同的特点点。<details>
<summary>Abstract</summary>
In this paper, we provide a geometric interpretation of the structure of Deep Learning (DL) networks, characterized by $L$ hidden layers, a ramp activation function, an ${\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, and input and output spaces ${\mathbb R}^Q$ with equal dimension $Q\geq1$. The hidden layers are defined on spaces ${\mathbb R}^{Q}$, as well. We apply our recent results on shallow neural networks to construct an explicit family of minimizers for the global minimum of the cost function in the case $L\geq Q$, which we show to be degenerate. In the context presented here, the hidden layers of the DL network "curate" the training inputs by recursive application of a truncation map that minimizes the noise to signal ratio of the training inputs. Moreover, we determine a set of $2^Q-1$ distinct degenerate local minima of the cost function.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提供了深度学习（DL）网络的几何解释，其特征为有$L$层感知层、斜坡活动函数、${\mathcal L}^2$ Schatten类（或希尔伯特-Ш密特）成本函数，以及输入和输出空间为${\mathbb R}^Q$，其维度为$Q\geq1$。感知层在${\mathbb R}^{Q}$上定义。我们利用我们之前对浅层神经网络的研究，构造了$L\geq Q$时的全局最小值的明确家族，并证明其为极值。在这种情况下，深度学习网络的隐藏层“照料”训练输入，通过重复应用一个减少映射来最小化训练输入的噪声与信号比率。此外，我们确定了$2^Q-1$个不同的极值本地最小值。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Influence-of-Information-Entropy-Change-in-Learning-Systems"><a href="#Exploring-the-Influence-of-Information-Entropy-Change-in-Learning-Systems" class="headerlink" title="Exploring the Influence of Information Entropy Change in Learning Systems"></a>Exploring the Influence of Information Entropy Change in Learning Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10625">http://arxiv.org/abs/2309.10625</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaowei Yu, Yao Xue, Lu Zhang, Li Wang, Tianming Liu, Dajiang Zhu<br>for: This paper explores the influence of entropy change in deep learning systems by adding noise to the inputs&#x2F;latent features, with applications in computer vision tasks.methods: The paper uses theoretical analysis and empirical experiments to demonstrate the enhancement gained from positive noise by reducing the task complexity defined by information entropy.results: The paper shows significant performance gains in large image datasets such as ImageNet by proactively injecting positive noise, achieving an unprecedented top 1 accuracy of over 95%.<details>
<summary>Abstract</summary>
In this work, we explore the influence of entropy change in deep learning systems by adding noise to the inputs/latent features. The applications in this paper focus on deep learning tasks within computer vision, but the proposed theory can be further applied to other fields. Noise is conventionally viewed as a harmful perturbation in various deep learning architectures, such as convolutional neural networks (CNNs) and vision transformers (ViTs), as well as different learning tasks like image classification and transfer learning. However, this paper aims to rethink whether the conventional proposition always holds. We demonstrate that specific noise can boost the performance of various deep architectures under certain conditions. We theoretically prove the enhancement gained from positive noise by reducing the task complexity defined by information entropy and experimentally show the significant performance gain in large image datasets, such as the ImageNet. Herein, we use the information entropy to define the complexity of the task. We categorize the noise into two types, positive noise (PN) and harmful noise (HN), based on whether the noise can help reduce the complexity of the task. Extensive experiments of CNNs and ViTs have shown performance improvements by proactively injecting positive noise, where we achieved an unprecedented top 1 accuracy of over 95% on ImageNet. Both theoretical analysis and empirical evidence have confirmed that the presence of positive noise can benefit the learning process, while the traditionally perceived harmful noise indeed impairs deep learning models. The different roles of noise offer new explanations for deep models on specific tasks and provide a new paradigm for improving model performance. Moreover, it reminds us that we can influence the performance of learning systems via information entropy change.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们探索深度学习系统中 entropy 变化的影响。我们在计算机视觉领域中应用了深度学习任务，但我们的理论可以应用到其他领域。在传统上，噪声被视为深度学习架构中的危险扰动，如 convolutional neural networks (CNNs) 和 vision transformers (ViTs)，以及不同的学习任务，如图像分类和转移学习。然而，这篇论文想要重新思考这一观点是否总是正确的。我们展示了特定的噪声可以在某些条件下提高深度架构的性能。我们使用信息 entropy 来定义任务的复杂度，并分类噪声为正面噪声 (PN) 和有害噪声 (HN)，根据噪声是否可以减少任务的复杂度。我们在大量图像数据集，如 ImageNet，进行了广泛的实验，并证明了在某些条件下，注意性噪声可以提高深度架构的性能。我们的理论分析和实验证据都表明，在某些任务上，正面噪声可以促进学习过程，而传统上认为的危险噪声实际上会降低深度学习模型的性能。这些不同的噪声角色为深度模型在特定任务上提供了新的解释，并提供了一个新的性能提升模式。此外，它提醒我们可以通过改变信息 entropy 来影响学习系统的性能。
</details></li>
</ul>
<hr>
<h2 id="Large-language-models-can-accurately-predict-searcher-preferences"><a href="#Large-language-models-can-accurately-predict-searcher-preferences" class="headerlink" title="Large language models can accurately predict searcher preferences"></a>Large language models can accurately predict searcher preferences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10621">http://arxiv.org/abs/2309.10621</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Thomas, Seth Spielman, Nick Craswell, Bhaskar Mitra</li>
<li>for: 这个论文目的是提高搜索系统中 Labels 的质量，即用户是否认为搜索结果有用。</li>
<li>methods: 这个论文使用了大型自然语言模型（Lang Model）来生成 Labels，并通过对用户反馈进行训练来改进 Labels 的质量。</li>
<li>results: 论文表明，使用 Lang Model 可以生成高质量 Labels，并且比第三方标注者更准确，同时也比较cost-effective。此外，这些 Labels 还可以用于训练更好的排名算法。<details>
<summary>Abstract</summary>
Relevance labels, which indicate whether a search result is valuable to a searcher, are key to evaluating and optimising search systems. The best way to capture the true preferences of users is to ask them for their careful feedback on which results would be useful, but this approach does not scale to produce a large number of labels. Getting relevance labels at scale is usually done with third-party labellers, who judge on behalf of the user, but there is a risk of low-quality data if the labeller doesn't understand user needs. To improve quality, one standard approach is to study real users through interviews, user studies and direct feedback, find areas where labels are systematically disagreeing with users, then educate labellers about user needs through judging guidelines, training and monitoring. This paper introduces an alternate approach for improving label quality. It takes careful feedback from real users, which by definition is the highest-quality first-party gold data that can be derived, and develops an large language model prompt that agrees with that data.   We present ideas and observations from deploying language models for large-scale relevance labelling at Bing, and illustrate with data from TREC. We have found large language models can be effective, with accuracy as good as human labellers and similar capability to pick the hardest queries, best runs, and best groups. Systematic changes to the prompts make a difference in accuracy, but so too do simple paraphrases. To measure agreement with real searchers needs high-quality ``gold'' labels, but with these we find that models produce better labels than third-party workers, for a fraction of the cost, and these labels let us train notably better rankers.
</details>
<details>
<summary>摘要</summary>
搜寻结果的价值性标签（relevance labels）是评估和优化搜寻系统的关键因素。获取高质量的标签最好的方法是请求用户提供精确的反馈，但这种方法不能生产大量的标签。通过第三方审核员进行审核，但这可能会导致低质量的数据。将高质量的标签获取到大量的数据是一个问题。这篇文章介绍了一种新的方法来提高标签质量。它从真实的用户中获取了精确的反馈，并使用大型自然语言模型来开发问题提示，以确保它们与用户需求相符。我们在部署语言模型进行大规模的审核labeling时发现，大型语言模型可以有高精度和人工审核员相似的能力，并且能够处理最困难的查询、最佳路径和最佳分组。我们发现，对于标签的系统性改变可以提高精确性，但也注意到了简单的重写可以获得相似的效果。为了衡量模型与真实搜寻者需求的一致，我们需要高质量的“金”标签，但我们发现，这些标签可以让我们训练更好的排名器，并且这些标签的成本比第三方审核员来的便宜得多。
</details></li>
</ul>
<hr>
<h2 id="A-Dynamic-Linear-Bias-Incorporation-Scheme-for-Nonnegative-Latent-Factor-Analysis"><a href="#A-Dynamic-Linear-Bias-Incorporation-Scheme-for-Nonnegative-Latent-Factor-Analysis" class="headerlink" title="A Dynamic Linear Bias Incorporation Scheme for Nonnegative Latent Factor Analysis"></a>A Dynamic Linear Bias Incorporation Scheme for Nonnegative Latent Factor Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10618">http://arxiv.org/abs/2309.10618</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yurong Zhong, Zhe Xie, Weiling Li, Xin Luo</li>
<li>for:  Handle high-dimensional and incomplete (HDI) data in big data-related applications, such as social network services systems, by learning HDI data representation.</li>
<li>methods:  Propose a dynamic linear bias incorporation (DLBI) scheme to improve the scalability and representation ability of nonnegative latent factor analysis (NLFA) models for HDI data.</li>
<li>results:  Obtain higher representation accuracy and competitive computational efficiency compared to state-of-the-art models on three HDI datasets from real applications.<details>
<summary>Abstract</summary>
High-Dimensional and Incomplete (HDI) data is commonly encountered in big data-related applications like social network services systems, which are concerning the limited interactions among numerous nodes. Knowledge acquisition from HDI data is a vital issue in the domain of data science due to their embedded rich patterns like node behaviors, where the fundamental task is to perform HDI data representation learning. Nonnegative Latent Factor Analysis (NLFA) models have proven to possess the superiority to address this issue, where a linear bias incorporation (LBI) scheme is important in present the training overshooting and fluctuation, as well as preventing the model from premature convergence. However, existing LBI schemes are all statistic ones where the linear biases are fixed, which significantly restricts the scalability of the resultant NLFA model and results in loss of representation learning ability to HDI data. Motivated by the above discoveries, this paper innovatively presents the dynamic linear bias incorporation (DLBI) scheme. It firstly extends the linear bias vectors into matrices, and then builds a binary weight matrix to switch the active/inactive states of the linear biases. The weight matrix's each entry switches between the binary states dynamically corresponding to the linear bias value variation, thereby establishing the dynamic linear biases for an NLFA model. Empirical studies on three HDI datasets from real applications demonstrate that the proposed DLBI-based NLFA model obtains higher representation accuracy several than state-of-the-art models do, as well as highly-competitive computational efficiency.
</details>
<details>
<summary>摘要</summary>
高维ensional和不完全（HDI）数据在大数据相关应用中常见，如社交媒体系统等，它们关注有限的节点间交互。科学数据获取从HDI数据是数据科学领域的重要问题，因为它们嵌入了诸如节点行为的复杂模式。非正式因子分析（NLFA）模型已经证明可以解决这个问题，其中线性偏好包含（LBI）策略可以避免模型快速 converges 和抖动。然而，现有的LBI策略都是静态的，这限制了NLFA模型的可扩展性和对HDI数据的表达能力。这篇论文驱动于以上发现，开创了动态线性偏好包含（DLBI）策略。它首先将线性偏好 vectors 扩展到矩阵，然后建立一个二进制权重矩阵，以switch动态线性偏好的活动/不活动状态。每个权重矩阵中的每个Entry 在线性偏好值变化时动态地 switching  между二进制状态，从而实现了动态线性偏好。实验研究在三个HDI数据集上表明，提案的DLBI-based NLFA模型在表达精度方面比现有模型高得多，同时computational efficiency 也具有高度竞争力。
</details></li>
</ul>
<hr>
<h2 id="Decentralized-Online-Learning-in-Task-Assignment-Games-for-Mobile-Crowdsensing"><a href="#Decentralized-Online-Learning-in-Task-Assignment-Games-for-Mobile-Crowdsensing" class="headerlink" title="Decentralized Online Learning in Task Assignment Games for Mobile Crowdsensing"></a>Decentralized Online Learning in Task Assignment Games for Mobile Crowdsensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10594">http://arxiv.org/abs/2309.10594</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bernd Simon, Andrea Ortiz, Walid Saad, Anja Klein</li>
<li>for: 这个研究是为了解决移动对感应系统 (MCS) 中的聚合数据收集问题。</li>
<li>methods: 这个研究使用了一种新的分布式方法，结合了对抗理论和在线学习，被称为碰撞避免多重枪 (CA-MAB-SFS)。这个方法模型了任务将分配问题为一个对抗游戏，考虑到 MCSP 和 MU 的个人目标，并让 MU 在线上学习其努力。</li>
<li>results: 这个研究的结果显示，CA-MAB-SFS 可以将 MCSP 和 MU 的满意度提高，并且降低均值任务完成时间，至少降低 16%。此外，CA-MAB-SFS 可以确保任务分配问题的稳定 regret 是一个线性下降函数，并且在线上学习过程中，MU 的学习速度得到了重要的改善。<details>
<summary>Abstract</summary>
The problem of coordinated data collection is studied for a mobile crowdsensing (MCS) system. A mobile crowdsensing platform (MCSP) sequentially publishes sensing tasks to the available mobile units (MUs) that signal their willingness to participate in a task by sending sensing offers back to the MCSP. From the received offers, the MCSP decides the task assignment. A stable task assignment must address two challenges: the MCSP's and MUs' conflicting goals, and the uncertainty about the MUs' required efforts and preferences. To overcome these challenges a novel decentralized approach combining matching theory and online learning, called collision-avoidance multi-armed bandit with strategic free sensing (CA-MAB-SFS), is proposed. The task assignment problem is modeled as a matching game considering the MCSP's and MUs' individual goals while the MUs learn their efforts online. Our innovative "free-sensing" mechanism significantly improves the MU's learning process while reducing collisions during task allocation. The stable regret of CA-MAB-SFS, i.e., the loss of learning, is analytically shown to be bounded by a sublinear function, ensuring the convergence to a stable optimal solution. Simulation results show that CA-MAB-SFS increases the MUs' and the MCSP's satisfaction compared to state-of-the-art methods while reducing the average task completion time by at least 16%.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将数据收集协调问题应用于移动农垦系统（MCSP）中。MCSP逐次发布感知任务到可用的移动单元（MU），并且MU通过发送感知申请回到MCSP。从接收的申请中，MCSP决定任务分配。稳定任务分配必须解决两个挑战：MCSP和MU的目标冲突，以及MU的努力和偏好的不确定性。为了解决这些挑战，我们提出了一种新的分布式方法， combining matching theory和在线学习，称为碰撞避免多重臂bandit with strategic free sensing（CA-MAB-SFS）。任务分配问题被模型为一个匹配游戏，考虑MCSP和MU的个人目标，而MU在线学习其努力。我们的创新的“免费感知”机制可以显著提高MU的学习过程，同时降低任务分配中的碰撞。CA-MAB-SFS的稳定征 regret，即学习损失，被分析显示为一个下线函数，确保 converge to a stable optimal solution。实验结果表明，CA-MAB-SFS在比 estado-of-the-art方法的情况下，使MU和MCSP满意度提高，而任务完成时间平均下降至少16%。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and widely used in other countries as well. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="PDRL-Multi-Agent-based-Reinforcement-Learning-for-Predictive-Monitoring"><a href="#PDRL-Multi-Agent-based-Reinforcement-Learning-for-Predictive-Monitoring" class="headerlink" title="PDRL: Multi-Agent based Reinforcement Learning for Predictive Monitoring"></a>PDRL: Multi-Agent based Reinforcement Learning for Predictive Monitoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10576">http://arxiv.org/abs/2309.10576</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thanveer Shaik, Xiaohui Tao, Lin Li, Haoran Xie, U R Acharya, Raj Gururajan, Xujuan Zhou</li>
<li>for: 这个研究旨在提出一个新的、通用的预测深度学习（PDRL）系统，用于时间序列预测环境中。</li>
<li>methods: 这个系统使用多个对应的深度问题网络（DQN）代理人，以监控预测的未来环境状态，并将学习的知识与最大化奖励相结合。</li>
<li>results: 在评估过程中，三个DRL代理人能够顺利学习相应的模式，并逐次获得奖励。该系统在时间序列预测中实现了状态预测的最佳性能。<details>
<summary>Abstract</summary>
Reinforcement learning has been increasingly applied in monitoring applications because of its ability to learn from previous experiences and can make adaptive decisions. However, existing machine learning-based health monitoring applications are mostly supervised learning algorithms, trained on labels and they cannot make adaptive decisions in an uncertain complex environment. This study proposes a novel and generic system, predictive deep reinforcement learning (PDRL) with multiple RL agents in a time series forecasting environment. The proposed generic framework accommodates virtual Deep Q Network (DQN) agents to monitor predicted future states of a complex environment with a well-defined reward policy so that the agent learns existing knowledge while maximizing their rewards. In the evaluation process of the proposed framework, three DRL agents were deployed to monitor a subject's future heart rate, respiration, and temperature predicted using a BiLSTM model. With each iteration, the three agents were able to learn the associated patterns and their cumulative rewards gradually increased. It outperformed the baseline models for all three monitoring agents. The proposed PDRL framework is able to achieve state-of-the-art performance in the time series forecasting process. The proposed DRL agents and deep learning model in the PDRL framework are customized to implement the transfer learning in other forecasting applications like traffic and weather and monitor their states. The PDRL framework is able to learn the future states of the traffic and weather forecasting and the cumulative rewards are gradually increasing over each episode.
</details>
<details>
<summary>摘要</summary>
强化学习在监测应用中得到了广泛应用，因为它可以从前一次的经验中学习并做出适应性的决策。然而，现有的机器学习基于的健康监测应用多为指导学习算法，它们不能在不确定的复杂环境中做出适应性的决策。本研究提出了一种新的和通用的系统——预测深度强化学习（PDRL），该系统通过多个RL代理在时间序列预测环境中使用多个DQN代理来监测预测的未来状况，以便代理学习现有的知识而寻求最大化奖励。在评估PDRL系统的过程中，三个DRL代理被部署到监测一个人的未来心率、呼吸和体温预测结果。在每次迭代中，三个代理能够学习相关的模式，其总奖励逐渐增长。与基线模型相比，PDRL系统在时间序列预测过程中实现了状态的极佳性能。PDRL系统可以在其他预测应用中，如交通和天气预测，实现传输学习，并监测其状态。在每个 episoden 中，PDRL系统能够学习未来的交通和天气预测结果，并逐渐增长其总奖励。
</details></li>
</ul>
<hr>
<h2 id="A-multimodal-deep-learning-architecture-for-smoking-detection-with-a-small-data-approach"><a href="#A-multimodal-deep-learning-architecture-for-smoking-detection-with-a-small-data-approach" class="headerlink" title="A multimodal deep learning architecture for smoking detection with a small data approach"></a>A multimodal deep learning architecture for smoking detection with a small data approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10561">http://arxiv.org/abs/2309.10561</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robert Lakatos, Peter Pollner, Andras Hajdu, Tamas Joo</li>
<li>for: 探索使用人工智能检测隐藏烟草广告的可能性，以提高媒体内容的不偏不倚和公平性。</li>
<li>methods: 提出一种基于深度学习、生成方法和人类干扰的整合文本和图像处理模型，可以在文本和图像格式下检测吸烟场景，即使有限的训练数据。</li>
<li>results: 模型可以达到74%的图像准确率和98%的文本准确率，并且可以 integrating human reinforcement 进行专家干扰。<details>
<summary>Abstract</summary>
Introduction: Covert tobacco advertisements often raise regulatory measures. This paper presents that artificial intelligence, particularly deep learning, has great potential for detecting hidden advertising and allows unbiased, reproducible, and fair quantification of tobacco-related media content. Methods: We propose an integrated text and image processing model based on deep learning, generative methods, and human reinforcement, which can detect smoking cases in both textual and visual formats, even with little available training data. Results: Our model can achieve 74\% accuracy for images and 98\% for text. Furthermore, our system integrates the possibility of expert intervention in the form of human reinforcement. Conclusions: Using the pre-trained multimodal, image, and text processing models available through deep learning makes it possible to detect smoking in different media even with few training data.
</details>
<details>
<summary>摘要</summary>
引言：借由覆住式烟草广告，常会引起 regulatory 措施。本文提出，人工智能，特别是深度学习，具有察觉隐藏广告的潜力，并可以实现不偏袋、可重复、公正地评估烟草相关媒体内容。方法：我们提议一种基于深度学习、生成方法和人类补做的集成文本和图像处理模型，可以在文本和图像格式中检测吸烟场景，即使培训数据 scarcity 。结果：我们的模型可以达到 74% 的准确率 для图像和 98% 的准确率 для文本。此外，我们的系统还可以 integrate the possibility of expert intervention in the form of human reinforcement。结论：通过深度学习提供的 pré-train 多模态、图像和文本处理模型，可以在媒体中检测吸烟，即使培训数据 scarce。
</details></li>
</ul>
<hr>
<h2 id="A-Neighbourhood-Aware-Differential-Privacy-Mechanism-for-Static-Word-Embeddings"><a href="#A-Neighbourhood-Aware-Differential-Privacy-Mechanism-for-Static-Word-Embeddings" class="headerlink" title="A Neighbourhood-Aware Differential Privacy Mechanism for Static Word Embeddings"></a>A Neighbourhood-Aware Differential Privacy Mechanism for Static Word Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10551">http://arxiv.org/abs/2309.10551</a></li>
<li>repo_url: None</li>
<li>paper_authors: Danushka Bollegala, Shuichi Otake, Tomoya Machide, Ken-ichi Kawarabayashi</li>
<li>for: 保护个人隐私（differential privacy）</li>
<li>methods: 使用邻域相关的差分隐私机制（Neighbourhood-Aware Differential Privacy，NADP），根据word embedding空间中 Word 的邻域构建图，并在不同邻域中应用不同水平的高斯噪声，以保证指定的隐私水平。</li>
<li>results: 在多个下游任务中，NADP 机制比 laplacian、gaussian 和 mahalanobis 等先前提出的隐私机制表现更好，同时保证更高的隐私水平。<details>
<summary>Abstract</summary>
We propose a Neighbourhood-Aware Differential Privacy (NADP) mechanism considering the neighbourhood of a word in a pretrained static word embedding space to determine the minimal amount of noise required to guarantee a specified privacy level. We first construct a nearest neighbour graph over the words using their embeddings, and factorise it into a set of connected components (i.e. neighbourhoods). We then separately apply different levels of Gaussian noise to the words in each neighbourhood, determined by the set of words in that neighbourhood. Experiments show that our proposed NADP mechanism consistently outperforms multiple previously proposed DP mechanisms such as Laplacian, Gaussian, and Mahalanobis in multiple downstream tasks, while guaranteeing higher levels of privacy.
</details>
<details>
<summary>摘要</summary>
我们提出了一种基于邻居的差分隐私（NADP）机制，利用预训练的静态单词嵌入空间中的邻居 relaciones，确定最小的噪声量以保证指定的隐私水平。我们首先将单词的嵌入图构建成 nearest neighbor 图，然后将其分解成一系列相互独立的噪声应用。实验表明，我们的提议的 NADP 机制在多个下游任务中 consistently 超越了多个先前提出的差分隐私机制，如 Laplacian、Gaussian 和 Mahalanobis，同时保证更高的隐私水平。
</details></li>
</ul>
<hr>
<h2 id="Towards-Generative-Modeling-of-Urban-Flow-through-Knowledge-enhanced-Denoising-Diffusion"><a href="#Towards-Generative-Modeling-of-Urban-Flow-through-Knowledge-enhanced-Denoising-Diffusion" class="headerlink" title="Towards Generative Modeling of Urban Flow through Knowledge-enhanced Denoising Diffusion"></a>Towards Generative Modeling of Urban Flow through Knowledge-enhanced Denoising Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10547">http://arxiv.org/abs/2309.10547</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tsinghua-fib-lab/kstdiff-urban-flow-generation">https://github.com/tsinghua-fib-lab/kstdiff-urban-flow-generation</a></li>
<li>paper_authors: Zhilun Zhou, Jingtao Ding, Yu Liu, Depeng Jin, Yong Li</li>
<li>for: 本研究旨在生成城市流动数据，尤其是在数据缺乏或新规划区域的情况下。</li>
<li>methods: 本研究使用了Diffusion Model和知识增强的spatio-temporal diffusion模型（KSTDiff）来生成城市流动数据。在KSTDiff模型中，我们首先构建了一个城市知识图（UKG），以模拟城市环境和区域之间的关系。然后，我们设计了一个学习式的流量估计器，以便准确地生成不同区域的流量。此外，我们还提出了一种知识增强的降噪网络，以捕捉城市流动的空间时间关系以及城市环境的影响。</li>
<li>results: 对四个实际数据集进行了广泛的实验，并证明了我们的模型在城市流动生成方面的优越性。此外，我们还进行了更深入的研究，证明了生成的城市流动数据的实用性和我们模型的长期流动预测和城市流动预测能力。<details>
<summary>Abstract</summary>
Although generative AI has been successful in many areas, its ability to model geospatial data is still underexplored. Urban flow, a typical kind of geospatial data, is critical for a wide range of urban applications. Existing studies mostly focus on predictive modeling of urban flow that predicts the future flow based on historical flow data, which may be unavailable in data-sparse areas or newly planned regions. Some other studies aim to predict OD flow among regions but they fail to model dynamic changes of urban flow over time. In this work, we study a new problem of urban flow generation that generates dynamic urban flow for regions without historical flow data. To capture the effect of multiple factors on urban flow, such as region features and urban environment, we employ diffusion model to generate urban flow for regions under different conditions. We first construct an urban knowledge graph (UKG) to model the urban environment and relationships between regions, based on which we design a knowledge-enhanced spatio-temporal diffusion model (KSTDiff) to generate urban flow for each region. Specifically, to accurately generate urban flow for regions with different flow volumes, we design a novel diffusion process guided by a volume estimator, which is learnable and customized for each region. Moreover, we propose a knowledge-enhanced denoising network to capture the spatio-temporal dependencies of urban flow as well as the impact of urban environment in the denoising process. Extensive experiments on four real-world datasets validate the superiority of our model over state-of-the-art baselines in urban flow generation. Further in-depth studies demonstrate the utility of generated urban flow data and the ability of our model for long-term flow generation and urban flow prediction. Our code is released at: https://github.com/tsinghua-fib-lab/KSTDiff-Urban-flow-generation.
</details>
<details>
<summary>摘要</summary>
although generative AI has been successful in many areas, its ability to model geospatial data is still underexplored. urban flow, a typical kind of geospatial data, is critical for a wide range of urban applications. existing studies mostly focus on predictive modeling of urban flow that predicts the future flow based on historical flow data, which may be unavailable in data-sparse areas or newly planned regions. some other studies aim to predict OD flow among regions but they fail to model dynamic changes of urban flow over time. in this work, we study a new problem of urban flow generation that generates dynamic urban flow for regions without historical flow data. to capture the effect of multiple factors on urban flow, such as region features and urban environment, we employ diffusion model to generate urban flow for regions under different conditions. we first construct an urban knowledge graph (UKG) to model the urban environment and relationships between regions, based on which we design a knowledge-enhanced spatio-temporal diffusion model (KSTDiff) to generate urban flow for each region. specifically, to accurately generate urban flow for regions with different flow volumes, we design a novel diffusion process guided by a volume estimator, which is learnable and customized for each region. moreover, we propose a knowledge-enhanced denoising network to capture the spatio-temporal dependencies of urban flow as well as the impact of urban environment in the denoising process. extensive experiments on four real-world datasets validate the superiority of our model over state-of-the-art baselines in urban flow generation. further in-depth studies demonstrate the utility of generated urban flow data and the ability of our model for long-term flow generation and urban flow prediction. our code is released at: https://github.com/tsinghua-fib-lab/KSTDiff-Urban-flow-generation.
</details></li>
</ul>
<hr>
<h2 id="Mean-Absolute-Directional-Loss-as-a-New-Loss-Function-for-Machine-Learning-Problems-in-Algorithmic-Investment-Strategies"><a href="#Mean-Absolute-Directional-Loss-as-a-New-Loss-Function-for-Machine-Learning-Problems-in-Algorithmic-Investment-Strategies" class="headerlink" title="Mean Absolute Directional Loss as a New Loss Function for Machine Learning Problems in Algorithmic Investment Strategies"></a>Mean Absolute Directional Loss as a New Loss Function for Machine Learning Problems in Algorithmic Investment Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10546">http://arxiv.org/abs/2309.10546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jakub Michańków, Paweł Sakowski, Robert Ślepaczuk</li>
<li>for: 这paper investigate了用于预测金融时间序列的机器学习模型优化中的恰当损失函数问题，以建立更好的算法投资策略（AIS）。</li>
<li>methods:  authors propose了 Mean Absolute Directional Loss（MADL）函数，解决了 классиical forecast error functions中提取信息从预测中创建有效的 buy&#x2F;sell signals问题。</li>
<li>results:  authors based on two different asset classes（Bitcoin和Crude Oil）的数据，显示了新的损失函数可以更好地选择LSTM模型的超参数，并在它们的验证数据上获得更高的风险调整回报率。<details>
<summary>Abstract</summary>
This paper investigates the issue of an adequate loss function in the optimization of machine learning models used in the forecasting of financial time series for the purpose of algorithmic investment strategies (AIS) construction. We propose the Mean Absolute Directional Loss (MADL) function, solving important problems of classical forecast error functions in extracting information from forecasts to create efficient buy/sell signals in algorithmic investment strategies. Finally, based on the data from two different asset classes (cryptocurrencies: Bitcoin and commodities: Crude Oil), we show that the new loss function enables us to select better hyperparameters for the LSTM model and obtain more efficient investment strategies, with regard to risk-adjusted return metrics on the out-of-sample data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Model-Leeching-An-Extraction-Attack-Targeting-LLMs"><a href="#Model-Leeching-An-Extraction-Attack-Targeting-LLMs" class="headerlink" title="Model Leeching: An Extraction Attack Targeting LLMs"></a>Model Leeching: An Extraction Attack Targeting LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10544">http://arxiv.org/abs/2309.10544</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lewis Birch, William Hackett, Stefan Trawicki, Neeraj Suri, Peter Garraghan</li>
<li>for: 本研究旨在提取大语言模型（LLM）中的任务特有知识，并将其转换为具有减少参数的模型。</li>
<li>methods: 本研究使用的方法是Model Leeching，可以快速和效率地从目标LLM中提取任务特有的知识。</li>
<li>results: 研究表明，通过使用Model Leeching，可以从ChatGPT-3.5-Turbo中提取出73%的 preciseness（EM）相似性，以及SQuAD EM和F1分数分别为75%和87%，仅需API成本50美元。此外，研究还证明了对提取模型进行ML攻击的可行性，对ChatGPT-3.5-Turbo进行ML攻击时，通过 transferred adversarial attack 可以提高攻击成功率11%。<details>
<summary>Abstract</summary>
Model Leeching is a novel extraction attack targeting Large Language Models (LLMs), capable of distilling task-specific knowledge from a target LLM into a reduced parameter model. We demonstrate the effectiveness of our attack by extracting task capability from ChatGPT-3.5-Turbo, achieving 73% Exact Match (EM) similarity, and SQuAD EM and F1 accuracy scores of 75% and 87%, respectively for only $50 in API cost. We further demonstrate the feasibility of adversarial attack transferability from an extracted model extracted via Model Leeching to perform ML attack staging against a target LLM, resulting in an 11% increase to attack success rate when applied to ChatGPT-3.5-Turbo.
</details>
<details>
<summary>摘要</summary>
模型偷窥（Model Leeching）是一种新的抽取攻击，可以从目标大语言模型（LLMs）中提取任务特定的知识，并将其转换为具有减少参数的模型。我们通过对ChatGPT-3.5-Turbo进行抽取，实现了73%的精确匹配（EM）相似性，以及SQuAD EM和F1分数的75%和87%分别。这些成绩只需要50美元的API成本。我们还证明了攻击者可以通过我们提取的模型来对目标LLM进行ML攻击，从而提高了攻击成功率11%。
</details></li>
</ul>
<hr>
<h2 id="OpenMSD-Towards-Multilingual-Scientific-Documents-Similarity-Measurement"><a href="#OpenMSD-Towards-Multilingual-Scientific-Documents-Similarity-Measurement" class="headerlink" title="OpenMSD: Towards Multilingual Scientific Documents Similarity Measurement"></a>OpenMSD: Towards Multilingual Scientific Documents Similarity Measurement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10539">http://arxiv.org/abs/2309.10539</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research/google-research">https://github.com/google-research/google-research</a></li>
<li>paper_authors: Yang Gao, Ji Ma, Ivan Korotkov, Keith Hall, Dana Alon, Don Metzler</li>
<li>for: 本研究旨在开发和评估多语言科学文献相似度测量模型，以便帮助多语言研究人员更有效地找到和探索相关文献。</li>
<li>methods: 我们使用Open-access Multilingual Scientific Documents（OpenMSD）数据集，该数据集包含74M篇论文和778M个引用对，并采用科学专业语言模型的预训练和不同策略来Derive “相关” 文献对进行细化。</li>
<li>results: 我们的最佳模型在比较 STRONG 基线模型时显著超越，提高了7-16%的平均精度。<details>
<summary>Abstract</summary>
We develop and evaluate multilingual scientific documents similarity measurement models in this work. Such models can be used to find related works in different languages, which can help multilingual researchers find and explore papers more efficiently. We propose the first multilingual scientific documents dataset, Open-access Multilingual Scientific Documents (OpenMSD), which has 74M papers in 103 languages and 778M citation pairs. With OpenMSD, we pretrain science-specialized language models, and explore different strategies to derive "related" paper pairs to fine-tune the models, including using a mixture of citation, co-citation, and bibliographic-coupling pairs. To further improve the models' performance for non-English papers, we explore the use of generative language models to enrich the non-English papers with English summaries. This allows us to leverage the models' English capabilities to create better representations for non-English papers. Our best model significantly outperforms strong baselines by 7-16% (in mean average precision).
</details>
<details>
<summary>摘要</summary>
我们在这项工作中开发和评估多语言科学文献相似度评估模型。这些模型可以帮助多语言研究人员更 efficiently找到相关的文献，从而提高研究效率。我们提出了首个多语言科学文献数据集 Open-access Multilingual Scientific Documents (OpenMSD)，该数据集包含7400万篇文献和778亿引用对，涵盖103种语言。使用OpenMSD数据集，我们预训练了专门为科学研究设计的语言模型，并研究了不同的策略来生成"相关"文献对以进一步训练模型，包括使用混合引用、共同引用和文献联系对。为了进一步提高非英文文献的表现，我们explore了使用生成语言模型来把非英文文献与英文摘要相联系。这allow我们利用模型的英文能力来创建更好的非英文文献表示。我们的最佳模型在比 STRONG baseline 高7-16%（在平均精度上）。
</details></li>
</ul>
<hr>
<h2 id="A-Cognitively-Inspired-Neural-Architecture-for-Visual-Abstract-Reasoning-Using-Contrastive-Perceptual-and-Conceptual-Processing"><a href="#A-Cognitively-Inspired-Neural-Architecture-for-Visual-Abstract-Reasoning-Using-Contrastive-Perceptual-and-Conceptual-Processing" class="headerlink" title="A Cognitively-Inspired Neural Architecture for Visual Abstract Reasoning Using Contrastive Perceptual and Conceptual Processing"></a>A Cognitively-Inspired Neural Architecture for Visual Abstract Reasoning Using Contrastive Perceptual and Conceptual Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10532">http://arxiv.org/abs/2309.10532</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Yang, Deepayan Sanyal, James Ainooson, Joel Michelson, Effat Farhana, Maithilee Kunda</li>
<li>for: 解决视觉抽象逻辑任务，即人类认知中的抽象逻辑过程。</li>
<li>methods: 提出了一种基于人类认知原理的新神经网络 architecture，即对比性感知-概念处理网络（CPCNet），它模型了人类视觉抽象逻辑的迭代、自我对比、学习过程。</li>
<li>results: 在使用matrix reasoning问题的 estilo de Raven’s Progressive Matrices智能测验 dataset中，CPCNet实现了所有之前发表的模型的高精度，同时使用最弱的推导假设。此外，文章还发现了原始 RAVEN 数据集中的一个substantial和前未注意的类别偏见，并提出了一个新的 RAVEN 变体 – AB-RAVEN，它更加具有抽象概念的均衡。<details>
<summary>Abstract</summary>
We introduce a new neural architecture for solving visual abstract reasoning tasks inspired by human cognition, specifically by observations that human abstract reasoning often interleaves perceptual and conceptual processing as part of a flexible, iterative, and dynamic cognitive process. Inspired by this principle, our architecture models visual abstract reasoning as an iterative, self-contrasting learning process that pursues consistency between perceptual and conceptual processing of visual stimuli. We explain how this new Contrastive Perceptual-Conceptual Network (CPCNet) works using matrix reasoning problems in the style of the well-known Raven's Progressive Matrices intelligence test. Experiments on the machine learning dataset RAVEN show that CPCNet achieves higher accuracy than all previously published models while also using the weakest inductive bias. We also point out a substantial and previously unremarked class imbalance in the original RAVEN dataset, and we propose a new variant of RAVEN -- AB-RAVEN -- that is more balanced in terms of abstract concepts.
</details>
<details>
<summary>摘要</summary>
我团队引入了一种新的神经网络模型，用于解决视觉抽象逻辑任务， draws inspiration from human cognition, specifically the observation that human abstract reasoning often interleaves perceptual and conceptual processing as part of a flexible, iterative, and dynamic cognitive process. 我们的模型将视觉抽象逻辑模型为一种迭代、自相对抗的学习过程，以实现视觉各种抽象处理和概念处理之间的一致性。我们使用矩阵理解问题，类似于著名的鸭子进步矩阵测验，解释我们的新型 Contrastive Perceptual-Conceptual Network (CPCNet) 如何工作。我们的实验表明，CPCNet 在 RAVEN 机器学习 dataset 上达到了所有前一代模型的高精度，同时使用最弱的推导假设。我们还指出了原始 RAVEN 数据集中的一个substantial和 previously unremarked class imbalance，并提出了一个新的 AB-RAVEN 数据集，它更加均衡了抽象概念。
</details></li>
</ul>
<hr>
<h2 id="Visible-and-NIR-Image-Fusion-Algorithm-Based-on-Information-Complementarity"><a href="#Visible-and-NIR-Image-Fusion-Algorithm-Based-on-Information-Complementarity" class="headerlink" title="Visible and NIR Image Fusion Algorithm Based on Information Complementarity"></a>Visible and NIR Image Fusion Algorithm Based on Information Complementarity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10522">http://arxiv.org/abs/2309.10522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuo Li, Bo Li</li>
<li>for: 本研究旨在利用可见和近红外（NIR）频率图像的融合，以优化图像质量。</li>
<li>methods: 本研究提出了一种基于物理信号水平的共辐合模型，包括两层权值引导滤波器和导引滤波器来获取文本和边缘层，以及使用延展DoG滤波器来生成初始可见-NIR共辐合权重图。</li>
<li>results: 实验结果表明，提出的算法可以良好地利用可见和NIR图像的谱属性和信息协同性，并避免颜色不自然的问题，与现有的状态艺术比较。<details>
<summary>Abstract</summary>
Visible and near-infrared(NIR) band sensors provide images that capture complementary spectral radiations from a scene. And the fusion of the visible and NIR image aims at utilizing their spectrum properties to enhance image quality. However, currently visible and NIR fusion algorithms cannot well take advantage of spectrum properties, as well as lack information complementarity, which results in color distortion and artifacts. Therefore, this paper designs a complementary fusion model from the level of physical signals. First, in order to distinguish between noise and useful information, we use two layers of the weight-guided filter and guided filter to obtain texture and edge layers, respectively. Second, to generate the initial visible-NIR complementarity weight map, the difference maps of visible and NIR are filtered by the extend-DoG filter. After that, the significant region of NIR night-time compensation guides the initial complementarity weight map by the arctanI function. Finally, the fusion images can be generated by the complementarity weight maps of visible and NIR images, respectively. The experimental results demonstrate that the proposed algorithm can not only well take advantage of the spectrum properties and the information complementarity, but also avoid color unnatural while maintaining naturalness, which outperforms the state-of-the-art.
</details>
<details>
<summary>摘要</summary>
可见和近红外（NIR）摄像机提供图像，捕捉场景的补充 спектраль辐射。并将可见和NIR图像融合，以利用其谱属性提高图像质量。然而，当前可见和NIR融合算法无法好地利用谱属性，同时缺乏信息协同，导致颜色扭曲和 Artefacts。因此，这篇论文提出了基于物理信号的补充模型。首先，使用两层权重导向滤波器和导引滤波器来分别获取Texture和Edge层。其次，使用扩展DoG滤波器来缩放可见和NIR差分图。接着，使用arctanI函数来引导初始可见-NIR补充权重图。最后，可以使用可见和NIR图像的补充权重图来生成融合图像。实验结果表明，提出的算法不仅可以好地利用谱属性和信息协同，同时也可以避免颜色不自然，而保持自然性，与当前最佳的方法相比。
</details></li>
</ul>
<hr>
<h2 id="Partially-Specified-Causal-Simulations"><a href="#Partially-Specified-Causal-Simulations" class="headerlink" title="Partially-Specified Causal Simulations"></a>Partially-Specified Causal Simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10514">http://arxiv.org/abs/2309.10514</a></li>
<li>repo_url: None</li>
<li>paper_authors: A. Zamanian, L. Mareis, N. Ahmidi</li>
<li>For: The paper is written to emphasize the importance of proper simulation design in causal inference research, and to introduce a new simulation framework called PARCS that addresses this issue.* Methods: The paper uses graphical causal models and a wide range of adjustable parameters to synthesize data, and allows users to identify and specify the subset of related parameters and randomize the remaining ones to generate a range of complying data-generating processes for their causal method.* Results: The paper reproduces and extends the simulation studies of two well-known causal discovery and missing data analysis papers, demonstrating the necessity of a proper simulation design and the benefits of using PARCS for simulation. The results show that PARCS can generate a more comprehensive and inclusive empirical investigation for causal claims.<details>
<summary>Abstract</summary>
Simulation studies play a key role in the validation of causal inference methods. The simulation results are reliable only if the study is designed according to the promised operational conditions of the method-in-test. Still, many causal inference literature tend to design over-restricted or misspecified studies. In this paper, we elaborate on the problem of improper simulation design for causal methods and compile a list of desiderata for an effective simulation framework. We then introduce partially-randomized causal simulation (PARCS), a simulation framework that meets those desiderata. PARCS synthesizes data based on graphical causal models and a wide range of adjustable parameters. There is a legible mapping from usual causal assumptions to the parameters, thus, users can identify and specify the subset of related parameters and randomize the remaining ones to generate a range of complying data-generating processes for their causal method. The result is a more comprehensive and inclusive empirical investigation for causal claims. Using PARCS, we reproduce and extend the simulation studies of two well-known causal discovery and missing data analysis papers to emphasize the necessity of a proper simulation design. Our results show that those papers would have improved and extended the findings, had they used PARCS for simulation. The framework is implemented as a Python package, too. By discussing the comprehensiveness and transparency of PARCS, we encourage causal inference researchers to utilize it as a standard tool for future works.
</details>
<details>
<summary>摘要</summary>
模拟研究在 causal inference 方法的验证中扮演着关键角色。模拟结果的可靠性取决于研究按照测试方法的操作条件进行设计。然而，许多 causal inference 文献中的模拟设计往往过于紧张或不准确。在这篇文章中，我们讨论了模拟设计不当的问题，并编辑了一份有效模拟框架的需求列表。然后，我们介绍了 partially-randomized causal simulation（PARCS）模拟框架，该框架基于图形 causal 模型和广泛的可调参数。在这个框架中，用户可以明确地将相关参数与 causal 假设之间的映射，并随机化剩下的参数来生成一个包含多种合法的数据生成过程。这使得用户可以对 causal laims 进行更广泛和包容的实验 исследование。使用 PARCS，我们重新生成和扩展了两篇已有的 causal discovery 和 missing data analysis 文献中的模拟研究，以强调模拟设计的重要性。我们的结果表明，如果使用 PARCS，这些文献中的结果将更加完整和多元。PARCS 已经实现为 Python 包。通过讨论 PARCS 的全面性和透明度，我们鼓励 causal inference 研究人员在未来的工作中使用这种标准工具。
</details></li>
</ul>
<hr>
<h2 id="A-Configurable-Library-for-Generating-and-Manipulating-Maze-Datasets"><a href="#A-Configurable-Library-for-Generating-and-Manipulating-Maze-Datasets" class="headerlink" title="A Configurable Library for Generating and Manipulating Maze Datasets"></a>A Configurable Library for Generating and Manipulating Maze Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10498">http://arxiv.org/abs/2309.10498</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/understanding-search/maze-dataset">https://github.com/understanding-search/maze-dataset</a></li>
<li>paper_authors: Michael Igorevich Ivanitskiy, Rusheb Shah, Alex F. Spies, Tilman Räuker, Dan Valentine, Can Rager, Lucia Quirke, Chris Mathwin, Guillaume Corlouer, Cecilia Diniz Behn, Samy Wu Fung</li>
<li>for:  investigate how machine learning models respond to distributional shifts using maze-solving tasks as a testbed</li>
<li>methods:  present a comprehensive library for generating, processing, and visualizing maze-solving datasets with extensive control over generation algorithms and parameters</li>
<li>results:  support for multiple output formats and tools for visualizing and converting between them, ensuring versatility and adaptability in research applications<details>
<summary>Abstract</summary>
Understanding how machine learning models respond to distributional shifts is a key research challenge. Mazes serve as an excellent testbed due to varied generation algorithms offering a nuanced platform to simulate both subtle and pronounced distributional shifts. To enable systematic investigations of model behavior on out-of-distribution data, we present $\texttt{maze-dataset}$, a comprehensive library for generating, processing, and visualizing datasets consisting of maze-solving tasks. With this library, researchers can easily create datasets, having extensive control over the generation algorithm used, the parameters fed to the algorithm of choice, and the filters that generated mazes must satisfy. Furthermore, it supports multiple output formats, including rasterized and text-based, catering to convolutional neural networks and autoregressive transformer models. These formats, along with tools for visualizing and converting between them, ensure versatility and adaptability in research applications.
</details>
<details>
<summary>摘要</summary>
理解机器学习模型对分布转移的响应是一项关键的研究挑战。迷宫 serves as an excellent testbed due to its varied generation algorithms, offering a nuanced platform to simulate both subtle and pronounced distributional shifts. To enable systematic investigations of model behavior on out-of-distribution data, we present $\texttt{maze-dataset}$, a comprehensive library for generating, processing, and visualizing datasets consisting of maze-solving tasks. With this library, researchers can easily create datasets, having extensive control over the generation algorithm used, the parameters fed to the algorithm of choice, and the filters that generated mazes must satisfy. Furthermore, it supports multiple output formats, including rasterized and text-based, catering to convolutional neural networks and autoregressive transformer models. These formats, along with tools for visualizing and converting between them, ensure versatility and adaptability in research applications.Here's the breakdown of the translation:* 理解机器学习模型 (Understanding machine learning models) becomes 理解机器学习模型 (Understanding machine learning models)* 对分布转移 (distributional shifts) becomes 对分布转移 (distributional shifts)* 迷宫 serves as an excellent testbed (mazes serve as an excellent testbed) becomes 迷宫 serves as an excellent testbed (mazes serve as an excellent testbed)*  varied generation algorithms (varied generation algorithms) becomes 多种生成算法 (various generation algorithms)*  nuanced platform (nuanced platform) becomes 细腻的平台 (subtle platform)* To enable systematic investigations (To enable systematic investigations) becomes 为实现系统的调查 (To enable systematic investigations)*  we present $\texttt{maze-dataset}$ (we present $\texttt{maze-dataset}$) becomes 我们提供 $\texttt{maze-dataset}$ (we provide $\texttt{maze-dataset}$)*  a comprehensive library (a comprehensive library) becomes 一个全面的库 (a comprehensive library)*  consisting of maze-solving tasks (consisting of maze-solving tasks) becomes 包含迷宫解决任务 (consisting of maze-solving tasks)* With this library, researchers can easily create datasets (With this library, researchers can easily create datasets) becomes 通过这个库，研究人员可以轻松创建数据集 (With this library, researchers can easily create datasets)* having extensive control (having extensive control) becomes 具有广泛的控制 (having extensive control)*  over the generation algorithm used (over the generation algorithm used) becomes 对生成算法使用的控制 (over the generation algorithm used)*  the parameters fed to the algorithm of choice (the parameters fed to the algorithm of choice) becomes 选择的算法的参数 (the parameters fed to the algorithm of choice)*  and the filters that generated mazes must satisfy (and the filters that generated mazes must satisfy) becomes 并且生成迷宫的筛选器必须满足 (and the filters that generated mazes must satisfy)* Furthermore, it supports multiple output formats (Furthermore, it supports multiple output formats) becomes 此外，它还支持多种输出格式 (Furthermore, it supports multiple output formats)*  including rasterized and text-based (including rasterized and text-based) becomes 包括预览和文本基于的格式 (including rasterized and text-based)* catering to convolutional neural networks and autoregressive transformer models (catering to convolutional neural networks and autoregressive transformer models) becomes 适用于卷积神经网络和自适应转换器模型 (catering to convolutional neural networks and autoregressive transformer models)* These formats, along with tools for visualizing and converting between them (These formats, along with tools for visualizing and converting between them) becomes 这些格式、以及转换和可视化工具 (These formats, along with tools for visualizing and converting between them)* ensure versatility and adaptability in research applications (ensure versatility and adaptability in research applications) becomes 确保在研究应用中具有多样性和适应性 (ensure versatility and adaptability in research applications)
</details></li>
</ul>
<hr>
<h2 id="An-Evaluation-of-GPT-4-on-the-ETHICS-Dataset"><a href="#An-Evaluation-of-GPT-4-on-the-ETHICS-Dataset" class="headerlink" title="An Evaluation of GPT-4 on the ETHICS Dataset"></a>An Evaluation of GPT-4 on the ETHICS Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10492">http://arxiv.org/abs/2309.10492</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sergey Rodionov, Zarathustra Amadeus Goertzel, Ben Goertzel</li>
<li>for: 这个研究是为了评估GPT-4模型在ETHICS数据集上的表现。</li>
<li>methods: 这个研究使用了GPT-4模型来处理ETHICS数据集中的道德判断。</li>
<li>results: GPT-4的表现比前一代模型要好，表明AI工程学习并不是道德伦理的硬件问题。<details>
<summary>Abstract</summary>
This report summarizes a short study of the performance of GPT-4 on the ETHICS dataset. The ETHICS dataset consists of five sub-datasets covering different fields of ethics: Justice, Deontology, Virtue Ethics, Utilitarianism, and Commonsense Ethics. The moral judgments were curated so as to have a high degree of agreement with the aim of representing shared human values rather than moral dilemmas. GPT-4's performance is much better than that of previous models and suggests that learning to work with common human values is not the hard problem for AI ethics.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这份报告总结了GPT-4在ETHICS数据集上的性能。ETHICS数据集包括五个子数据集，涵盖不同领域的伦理：正义、德ontology、美德伦理、功利主义和常识伦理。这些伦理判断被精心准备，以便反映人类共同价值，而不是道德困难。相比之前的模型，GPT-4的性能显著提高，表明AI伦理学中学习共同人类价值不是困难的问题。
</details></li>
</ul>
<hr>
<h2 id="Fully-automated-landmarking-and-facial-segmentation-on-3D-photographs"><a href="#Fully-automated-landmarking-and-facial-segmentation-on-3D-photographs" class="headerlink" title="Fully automated landmarking and facial segmentation on 3D photographs"></a>Fully automated landmarking and facial segmentation on 3D photographs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10472">http://arxiv.org/abs/2309.10472</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rumc3dlab/3dlandmarkdetection">https://github.com/rumc3dlab/3dlandmarkdetection</a></li>
<li>paper_authors: Bo Berends, Freek Bielevelt, Ruud Schreurs, Shankeeth Vinayahalingam, Thomas Maal, Guido de Jong</li>
<li>for: 这个研究的目的是发展和评估一个自动化的侧面测量方法，以提高侧面测量的精度和效率。</li>
<li>methods: 这个方法使用了两个DiffusionNet模型和其他的颜面分类算法，以及人工标注的10个标点。</li>
<li>results: 这个研究发现，这个自动化方法可以实现高精度和高一致性的侧面测量，并且可以减少人工标注的时间和误差。<details>
<summary>Abstract</summary>
Three-dimensional facial stereophotogrammetry provides a detailed representation of craniofacial soft tissue without the use of ionizing radiation. While manual annotation of landmarks serves as the current gold standard for cephalometric analysis, it is a time-consuming process and is prone to human error. The aim in this study was to develop and evaluate an automated cephalometric annotation method using a deep learning-based approach. Ten landmarks were manually annotated on 2897 3D facial photographs by a single observer. The automated landmarking workflow involved two successive DiffusionNet models and additional algorithms for facial segmentation. The dataset was randomly divided into a training and test dataset. The training dataset was used to train the deep learning networks, whereas the test dataset was used to evaluate the performance of the automated workflow. The precision of the workflow was evaluated by calculating the Euclidean distances between the automated and manual landmarks and compared to the intra-observer and inter-observer variability of manual annotation and the semi-automated landmarking method. The workflow was successful in 98.6% of all test cases. The deep learning-based landmarking method achieved precise and consistent landmark annotation. The mean precision of 1.69 (+/-1.15) mm was comparable to the inter-observer variability (1.31 +/-0.91 mm) of manual annotation. The Euclidean distance between the automated and manual landmarks was within 2 mm in 69%. Automated landmark annotation on 3D photographs was achieved with the DiffusionNet-based approach. The proposed method allows quantitative analysis of large datasets and may be used in diagnosis, follow-up, and virtual surgical planning.
</details>
<details>
<summary>摘要</summary>
三维面部塑型摄影技术可以提供细腻的脑颅面软组织图像，不需要使用辐射。现有的手动标注方法为头颈相机分析的现金标准，但是它是一项时间consuming的过程，容易出现人工错误。本研究的目标是开发和评估一种基于深度学习的自动标注方法。研究使用2897个3D面部照片，每个照片由一名观察者手动标注10个标记。自动标注工作流程包括两个DiffusionNet模型和其他的面部分 segmentation 算法。数据集随机分成训练和测试集。训练集用于训练深度学习网络，而测试集用于评估自动工作流程的性能。自动工作流程的精度由计算自动和手动标注之间的欧几丁度距离来评估。结果显示，自动工作流程成功的情况为98.6%。深度学习基本的标注方法实现了精确和一致的标记。手动标注和自动标注之间的平均差距为1.69（+/-1.15）毫米，与人工变化（1.31（+/-0.91）毫米）相比，表明自动标注的精度和一致性。自动和手动标注之间的欧几丁度距离在2毫米内的情况为69%。这种基于DiffusionNet的方法可以在3D照片上自动标注标记，并且允许大量数据的量化分析，可能用于诊断、跟踪和虚拟手术规划。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Dark-Side-of-AI-Advanced-Phishing-Attack-Design-and-Deployment-Using-ChatGPT"><a href="#Exploring-the-Dark-Side-of-AI-Advanced-Phishing-Attack-Design-and-Deployment-Using-ChatGPT" class="headerlink" title="Exploring the Dark Side of AI: Advanced Phishing Attack Design and Deployment Using ChatGPT"></a>Exploring the Dark Side of AI: Advanced Phishing Attack Design and Deployment Using ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10463">http://arxiv.org/abs/2309.10463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nils Begou, Jeremy Vinoy, Andrzej Duda, Maciej Korczynski</li>
<li>for: 这篇论文探讨了使用ChatGPT开发高级钓鱼攻击并大规模部署它们。</li>
<li>methods: 论文使用ChatGPT生成钓鱼攻击的以下部分：（1）复制目标网站，（2）抓取凭据，（3）干扰代码，（4）自动部署网站在托管提供商上，（5）注册钓鱼域名，（6）将网站与反向代理集成。</li>
<li>results: 初步评估自动生成的钓鱼套件显示它们具有快速生成和部署过程以及准确地模拟目标网站的页面。总之，这些发现表明了人工智能的进步，强调了钓鱼攻击的可能性和危险性，强调了人工智能系统中的增强防御措施的必要性。<details>
<summary>Abstract</summary>
This paper explores the possibility of using ChatGPT to develop advanced phishing attacks and automate their large-scale deployment. We make ChatGPT generate the following parts of a phishing attack: i) cloning a targeted website, ii) integrating code for stealing credentials, iii) obfuscating code, iv) automating website deployment on a hosting provider, v) registering a phishing domain name, and vi) integrating the website with a reverse proxy. The initial assessment of the automatically generated phishing kits highlights their rapid generation and deployment process as well as the close resemblance of the resulting pages to the target website. More broadly, we demonstrate that recent advances in AI underscore the potential risks of its misuse in phishing attacks, which can lead to their increased prevalence and severity. This highlights the necessity for enhanced countermeasures within AI systems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Cloning a targeted website2. Integrating code for stealing credentials3. Obfuscating code4. Automating website deployment on a hosting provider5. Registering a phishing domain name6. Integrating the website with a reverse proxyOur initial assessment of the automatically generated phishing kits shows that they can be rapidly generated and deployed, with the resulting pages closely resembling the target website. This demonstrates the potential risks of AI misuse in phishing attacks, which could lead to an increase in their prevalence and severity. This highlights the need for enhanced countermeasures within AI systems.</details></li>
</ol>
<hr>
<h2 id="Human-AI-Interactions-and-Societal-Pitfalls"><a href="#Human-AI-Interactions-and-Societal-Pitfalls" class="headerlink" title="Human-AI Interactions and Societal Pitfalls"></a>Human-AI Interactions and Societal Pitfalls</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10448">http://arxiv.org/abs/2309.10448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francisco Castro, Jian Gao, Sébastien Martin</li>
<li>for: 本研究旨在研究在使用生成式人工智能（AI）时，用户可能会看到产效提升，但AI生成的内容可能不会完全符合他们的偏好。</li>
<li>methods: 本研究使用 bayesian 框架，让不同的用户选择向 AI 分享多少信息，面临一种输出准确性和通信成本之间的负担。</li>
<li>results: 我们发现，在个人层面，AI 训练时使用 AI 生成的内容可能导致输出变得更加一致，特别是在 AI 受训练时。此外，任何 AI 偏见都可能变成社会偏见。要解决这些问题，我们需要改善人机交互，以获得个性化的输出而不是牺牲产效。<details>
<summary>Abstract</summary>
When working with generative artificial intelligence (AI), users may see productivity gains, but the AI-generated content may not match their preferences exactly. To study this effect, we introduce a Bayesian framework in which heterogeneous users choose how much information to share with the AI, facing a trade-off between output fidelity and communication cost. We show that the interplay between these individual-level decisions and AI training may lead to societal challenges. Outputs may become more homogenized, especially when the AI is trained on AI-generated content. And any AI bias may become societal bias. A solution to the homogenization and bias issues is to improve human-AI interactions, enabling personalized outputs without sacrificing productivity.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Toward-Unified-Controllable-Text-Generation-via-Regular-Expression-Instruction"><a href="#Toward-Unified-Controllable-Text-Generation-via-Regular-Expression-Instruction" class="headerlink" title="Toward Unified Controllable Text Generation via Regular Expression Instruction"></a>Toward Unified Controllable Text Generation via Regular Expression Instruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10447">http://arxiv.org/abs/2309.10447</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mrzhengxin/ctg-regex-instruction">https://github.com/mrzhengxin/ctg-regex-instruction</a></li>
<li>paper_authors: Xin Zheng, Hongyu Lin, Xianpei Han, Le Sun</li>
<li>for: 本研究的目的是提出一种基于常见表达式的指令机制，以便快速适应不同的约束类型和组合。</li>
<li>methods: 我们的方法使用了指令机制，通过常见表达式来完全利用其优势，并支持所有流行的细化控制生成约束，包括lexical、position和length约束，以及其复杂组合。</li>
<li>results: 我们的实验结果表明，我们的简单方法可以 дости得高成功率和适应性，同时与其他约束组合进行比较，在自动指标中维持竞争力，并超越大多数之前的基eline。<details>
<summary>Abstract</summary>
Controllable text generation is a fundamental aspect of natural language generation, with numerous methods proposed for different constraint types. However, these approaches often require significant architectural or decoding modifications, making them challenging to apply to additional constraints or resolve different constraint combinations. To address this, our paper introduces Regular Expression Instruction (REI), which utilizes an instruction-based mechanism to fully exploit regular expressions' advantages to uniformly model diverse constraints. Specifically, our REI supports all popular fine-grained controllable generation constraints, i.e., lexical, positional, and length, as well as their complex combinations, via regular expression-style instructions. Our method only requires fine-tuning on medium-scale language models or few-shot, in-context learning on large language models, and requires no further adjustment when applied to various constraint combinations. Experiments demonstrate that our straightforward approach yields high success rates and adaptability to various constraints while maintaining competitiveness in automatic metrics and outperforming most previous baselines.
</details>
<details>
<summary>摘要</summary>
natural language generation中的可控制文本生成是一个基本问题，各种方法被提出来解决不同的约束类型。然而，这些方法经常需要大量的架构或解码修改，使其应用于其他约束或解决不同的约束组合变得困难。为解决这个问题，我们的论文引入了常见表达式指令（REI），该机制利用了常见表达式的优点，以通用的方式模型多样的约束。具体来说，我们的REI支持所有流行的细化可控制生成约束，包括lexical、位置和长度约束，以及它们的复杂组合，通过常见表达式样式的指令。我们的方法仅需要中型语言模型的微调或几极少的在线学习，并且无需进行进一步的调整，无论应用于不同的约束组合。实验表明，我们的简单方法可以实现高的成功率和适应性，同时保持自动指标的竞争力和大多数之前的基elines的性能。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Self-Reinforcement-for-Improving-Learnersourced-Multiple-Choice-Question-Explanations-with-Large-Language-Models"><a href="#Exploring-Self-Reinforcement-for-Improving-Learnersourced-Multiple-Choice-Question-Explanations-with-Large-Language-Models" class="headerlink" title="Exploring Self-Reinforcement for Improving Learnersourced Multiple-Choice Question Explanations with Large Language Models"></a>Exploring Self-Reinforcement for Improving Learnersourced Multiple-Choice Question Explanations with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10444">http://arxiv.org/abs/2309.10444</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/strong-ai-lab/explanation-generation">https://github.com/strong-ai-lab/explanation-generation</a></li>
<li>paper_authors: Qiming Bao, Juho Leinonen, Alex Yuxuan Peng, Wanjun Zhong, Tim Pistotti, Alice Huang, Paul Denny, Michael Witbrock, Jiamou Liu<br>for: 这个论文的目的是帮助学生生成高质量的学习资源，并使用自然语言处理技术来自动生成解释。methods: 这个论文提出了一个基于自适应大语言模型的框架，包括三个模块：生成学生对应的解释、评估这些解释的质量，并不断改进解释。results: 这个论文的实验结果表明，与其他大语言模型相比，GPT-4在生成解释时表现出更高的创造力，并且由人类专家评估时被评为最高。<details>
<summary>Abstract</summary>
Learnersourcing involves students generating and sharing learning resources with their peers. When learnersourcing multiple-choice questions, creating explanations for the generated questions is a crucial step as it facilitates a deeper understanding of the related concepts. However, it is often difficult for students to craft effective explanations due to limited subject understanding and a tendency to merely restate the question stem, distractors, and correct answer. To help scaffold this task, in this work we propose a self-reinforcement large-language-model framework, with the goal of generating and evaluating explanations automatically. Comprising three modules, the framework generates student-aligned explanations, evaluates these explanations to ensure their quality and iteratively enhances the explanations. If an explanation's evaluation score falls below a defined threshold, the framework iteratively refines and reassesses the explanation. Importantly, our framework emulates the manner in which students compose explanations at the relevant grade level. For evaluation, we had a human subject-matter expert compare the explanations generated by students with the explanations created by the open-source large language model Vicuna-13B, a version of Vicuna-13B that had been fine-tuned using our method, and by GPT-4. We observed that, when compared to other large language models, GPT-4 exhibited a higher level of creativity in generating explanations. We also found that explanations generated by GPT-4 were ranked higher by the human expert than both those created by the other models and the original student-created explanations. Our findings represent a significant advancement in enriching the learnersourcing experience for students and enhancing the capabilities of large language models in educational applications.
</details>
<details>
<summary>摘要</summary>
学生来源化包括学生生成和分享学习资源的过程。当学生来源化多选问题时，创造相关概念的解释是一项重要的步骤，因为它可以帮助学生更深入理解相关概念。然而，学生 oftentimes Difficulty crafting effective explanations due to limited subject matter understanding and a tendency to simply restate the question stem, distractors, and correct answer. To help address this challenge, we propose a self-reinforcement large language model framework in this work, with the goal of generating and evaluating explanations automatically. The framework consists of three modules: generating student-aligned explanations, evaluating these explanations to ensure their quality, and iteratively enhancing the explanations. If an explanation's evaluation score falls below a defined threshold, the framework iteratively refines and reassesses the explanation. Importantly, our framework emulates the manner in which students compose explanations at the relevant grade level. For evaluation, we had a human subject-matter expert compare the explanations generated by students with the explanations created by the open-source large language model Vicuna-13B, a version of Vicuna-13B that had been fine-tuned using our method, and by GPT-4. We observed that, when compared to other large language models, GPT-4 exhibited a higher level of creativity in generating explanations. We also found that explanations generated by GPT-4 were ranked higher by the human expert than both those created by the other models and the original student-created explanations. Our findings represent a significant advancement in enriching the learnersourcing experience for students and enhancing the capabilities of large language models in educational applications.
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Imitation-based-Planner-for-Autonomous-Driving"><a href="#Rethinking-Imitation-based-Planner-for-Autonomous-Driving" class="headerlink" title="Rethinking Imitation-based Planner for Autonomous Driving"></a>Rethinking Imitation-based Planner for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10443">http://arxiv.org/abs/2309.10443</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jchengai/planTF">https://github.com/jchengai/planTF</a></li>
<li>paper_authors: Jie Cheng, Yingbing Chen, Xiaodong Mei, Bowen Yang, Bo Li, Ming Liu</li>
<li>for: 这篇论文的目的是为了提供一个大规模的实际世界数据集和一个标准化的封闭比较 benchmark，以便对各种设计的效iveness进行公平的比较。</li>
<li>methods: 本文使用了两个基本 yet 未得到了充分研究的方面： Egoplan 中的关键特征和可以降低堆叠错误的有效数据扩展技术。</li>
<li>results: 我们的结果表明，一个well-designed的强制 imitation-based  плаanner 可以与当前的状态 искусственный智能方法相比，在特长情况下表现出非常高的竞争力，并且在长尾情况下具有更好的泛化能力。<details>
<summary>Abstract</summary>
In recent years, imitation-based driving planners have reported considerable success. However, due to the absence of a standardized benchmark, the effectiveness of various designs remains unclear. The newly released nuPlan addresses this issue by offering a large-scale real-world dataset and a standardized closed-loop benchmark for equitable comparisons. Utilizing this platform, we conduct a comprehensive study on two fundamental yet underexplored aspects of imitation-based planners: the essential features for ego planning and the effective data augmentation techniques to reduce compounding errors. Furthermore, we highlight an imitation gap that has been overlooked by current learning systems. Finally, integrating our findings, we propose a strong baseline model-PlanTF. Our results demonstrate that a well-designed, purely imitation-based planner can achieve highly competitive performance compared to state-of-the-art methods involving hand-crafted rules and exhibit superior generalization capabilities in long-tail cases. Our models and benchmarks are publicly available. Project website https://jchengai.github.io/planTF.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multi-Object-Graph-Affordance-Network-Enabling-Goal-Oriented-Planning-through-Compound-Object-Affordances"><a href="#Multi-Object-Graph-Affordance-Network-Enabling-Goal-Oriented-Planning-through-Compound-Object-Affordances" class="headerlink" title="Multi-Object Graph Affordance Network: Enabling Goal-Oriented Planning through Compound Object Affordances"></a>Multi-Object Graph Affordance Network: Enabling Goal-Oriented Planning through Compound Object Affordances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10426">http://arxiv.org/abs/2309.10426</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tuba Girgin, Emre Ugur</li>
<li>for: 研究复杂物体之间的契合关系，以便在机器学习中更好地训练机器人。</li>
<li>methods: 我们提出了多对象图像契合网络（MOGAN），用于模拟复杂物体之间的契合关系，并预测将新物体放置在现有复杂物体之上的效果。</li>
<li>results: 我们的系统能够正确地模拟复杂物体之间的契合关系，包括堆积球和杯子、杆和环等。我们在虚拟和真实环境中进行了测试，并与基线模型进行比较，以显示我们的系统的优势。<details>
<summary>Abstract</summary>
Learning object affordances is an effective tool in the field of robot learning. While the data-driven models delve into the exploration of affordances of single or paired objects, there is a notable gap in the investigation of affordances of compound objects that are composed of an arbitrary number of objects with complex shapes. In this study, we propose Multi-Object Graph Affordance Network (MOGAN) that models compound object affordances and predicts the effect of placing new objects on top of the existing compound. Given different tasks, such as building towers of specific heights or properties, we used a search based planning to find the sequence of stack actions with the objects of suitable affordances. We showed that our system was able to correctly model the affordances of very complex compound objects that include stacked spheres and cups, poles, and rings that enclose the poles. We demonstrated the applicability of our system in both simulated and real-world environments, comparing our systems with a baseline model to highlight its advantages.
</details>
<details>
<summary>摘要</summary>
学习对象可行性是机器学习领域的有效工具。而数据驱动模型探索单个或对应的对象可行性的探索，但是对于包含多个对象的复杂形状的复合物体可行性的探索却存在显著的缺口。在本研究中，我们提议了多对象图像可行性网络（MOGAN），该模型可以预测将新对象放置在现有复合物体上的效果。我们使用搜索基本计划来找到适合的堆作业序列，以实现不同任务，如建立特定高度或性能的塔楼。我们证明了我们的系统可以正确地模型复杂的复合物体，包括堆积球和杯子、柱子和环形结构，并在实验室和真实环境中进行了比较，与基eline模型进行了对比，以 highlight its advantages。
</details></li>
</ul>
<hr>
<h2 id="Functional-requirements-to-mitigate-the-Risk-of-Harm-to-Patients-from-Artificial-Intelligence-in-Healthcare"><a href="#Functional-requirements-to-mitigate-the-Risk-of-Harm-to-Patients-from-Artificial-Intelligence-in-Healthcare" class="headerlink" title="Functional requirements to mitigate the Risk of Harm to Patients from Artificial Intelligence in Healthcare"></a>Functional requirements to mitigate the Risk of Harm to Patients from Artificial Intelligence in Healthcare</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10424">http://arxiv.org/abs/2309.10424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juan M. García-Gómez, Vicent Blanes-Selva, José Carlos de Bartolomé Cenzano, Jaime Cebolla-Cornejo, Ascensión Doñate-Martínez</li>
<li>for: 本文旨在描述七种人工智能（AI）在医疗领域的风险，以及十四种技术解决方案来减少这些风险。</li>
<li>methods: 本文使用了七种AI风险的列表，并提出了十四种技术要求来降低这些风险。</li>
<li>results: 本文的结果表明，通过实施这些技术要求，可以减少AI在医疗领域的风险，并保证AI系统的不断良好运行，以便为患者提供有益的医疗服务。<details>
<summary>Abstract</summary>
The Directorate General for Parliamentary Research Services of the European Parliament has prepared a report to the Members of the European Parliament where they enumerate seven main risks of Artificial Intelligence (AI) in medicine and healthcare: patient harm due to AI errors, misuse of medical AI tools, bias in AI and the perpetuation of existing inequities, lack of transparency, privacy and security issues, gaps in accountability, and obstacles in implementation.   In this study, we propose fourteen functional requirements that AI systems may implement to reduce the risks associated with their medical purpose: AI passport, User management, Regulation check, Academic use only disclaimer, data quality assessment, Clinicians double check, Continuous performance evaluation, Audit trail, Continuous usability test, Review of retrospective/simulated cases, Bias check, eXplainable AI, Encryption and use of field-tested libraries, and Semantic interoperability.   Our intention here is to provide specific high-level specifications of technical solutions to ensure continuous good performance and use of AI systems to benefit patients in compliance with the future EU regulatory framework.
</details>
<details>
<summary>摘要</summary>
欧洲议会Directorate General for Parliamentary Research Services已经准备了一份关于人工智能（AI）在医疗领域的报告，并列出了七个主要的AI风险：对patient的伤害 due to AI错误，违规使用医疗AI工具，AI中存在偏见和现有不平等的持续传递，缺乏透明度、隐私和安全问题，责任缺口，以及实施困难。在这项研究中，我们提出了十四个功能需求，以减少AI系统的医疗用途中的风险：AI护照，用户管理，法规检查，仅学术用途说明，数据质量评估，临床医生双重检查，不断性能评估，审计记录，不断用户测试，审查退化/模拟案例，偏见检查，可解释AI，加密，并使用已经测试的库。我们的目的是提供特定的高级技术解决方案，以确保AI系统的持续良好表现，并且在欧盟未来的法规框架下使用AI系统为病人带来好处。
</details></li>
</ul>
<hr>
<h2 id="Learning-from-Teaching-Assistants-to-Program-with-Subgoals-Exploring-the-Potential-for-AI-Teaching-Assistants"><a href="#Learning-from-Teaching-Assistants-to-Program-with-Subgoals-Exploring-the-Potential-for-AI-Teaching-Assistants" class="headerlink" title="Learning from Teaching Assistants to Program with Subgoals: Exploring the Potential for AI Teaching Assistants"></a>Learning from Teaching Assistants to Program with Subgoals: Exploring the Potential for AI Teaching Assistants</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10419">http://arxiv.org/abs/2309.10419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Changyoon Lee, Junho Myung, Jieun Han, Jiho Jin, Alice Oh</li>
<li>for: 本研究旨在探讨使用生成AI作为初级编程教育的教学助手，以评估学生与AI助手和人类助手之间的互动和感受。</li>
<li>methods: 我们采用 между组试验方法，尝试20名初级编程学习者在生成AI和人类助手的指导下解决编程任务。learners可以更快地解决任务，并得分相当。</li>
<li>results: 我们发现learners对AI助手的感受和人类助手的感受相似，即快速、全面和有用的回答，满意度也相似。此外，我们还提出了更好地设计和使用生成AI作为编程教育教学助手的指导原则。<details>
<summary>Abstract</summary>
With recent advances in generative AI, conversational models like ChatGPT have become feasible candidates for TAs. We investigate the practicality of using generative AI as TAs in introductory programming education by examining novice learners' interaction with TAs in a subgoal learning environment. To compare the learners' interaction and perception of the AI and human TAs, we conducted a between-subject study with 20 novice programming learners. Learners solve programming tasks by producing subgoals and subsolutions with the guidance of a TA. Our study shows that learners can solve tasks faster with comparable scores with AI TAs. Learners' perception of the AI TA is on par with that of human TAs in terms of speed and comprehensiveness of the replies and helpfulness, difficulty, and satisfaction of the conversation. Finally, we suggest guidelines to better design and utilize generative AI as TAs in programming education from the result of our chat log analysis.
</details>
<details>
<summary>摘要</summary>
Recent advances in 生成AI 使得对话模型如ChatGPT可能成为教学助手。我们 investigate 使用生成AI 作为初级编程教育中的教学助手，通过评估新手学者与 AI 和人类教学助手之间的互动来评估实用性。我们通过对 20 名初级编程学生进行比较研究，发现学生可以更快地解决编程任务，并且得分相似。学生对 AI 教学助手的评估与人类教学助手的评估相似，包括快速回答、全面性、 helpfulness、difficulty 和满意度。最后，我们提出了更好地设计和使用生成AI 作为编程教育教学助手的指南，基于我们的对话记录分析结果。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Learning-via-Network-Aware-Embeddings"><a href="#Unsupervised-Learning-via-Network-Aware-Embeddings" class="headerlink" title="Unsupervised Learning via Network-Aware Embeddings"></a>Unsupervised Learning via Network-Aware Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10408">http://arxiv.org/abs/2309.10408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anne Sophie Riis Damstrup, Sofie Tosti Madsen, Michele Coscia</li>
<li>for: 这篇论文的目的是解决不可靠的深度学习方法在面向网络数据的聚类任务中的缺陷。</li>
<li>methods: 这篇论文使用了一种新的网络意识 embedding 方法，通过对数字节点特征的一般化欧几里得距离进行估计，以便在聚类任务中考虑网络中节点之间的相互关系。</li>
<li>results: 试验结果表明，使用这种网络意识 embedding 方法可以提高聚类任务的效果，并且可以在各种领域（如市场营销、经济学和政治科学）提供实用的洞察。此外，这种方法可以扩展到大型网络中，并且可以在不同的数据集上重复得到类似的好效果。<details>
<summary>Abstract</summary>
Data clustering, the task of grouping observations according to their similarity, is a key component of unsupervised learning -- with real world applications in diverse fields such as biology, medicine, and social science. Often in these fields the data comes with complex interdependencies between the dimensions of analysis, for instance the various characteristics and opinions people can have live on a complex social network. Current clustering methods are ill-suited to tackle this complexity: deep learning can approximate these dependencies, but not take their explicit map as the input of the analysis. In this paper, we aim at fixing this blind spot in the unsupervised learning literature. We can create network-aware embeddings by estimating the network distance between numeric node attributes via the generalized Euclidean distance. Differently from all methods in the literature that we know of, we do not cluster the nodes of the network, but rather its node attributes. In our experiments we show that having these network embeddings is always beneficial for the learning task; that our method scales to large networks; and that we can actually provide actionable insights in applications in a variety of fields such as marketing, economics, and political science. Our method is fully open source and data and code are available to reproduce all results in the paper.
</details>
<details>
<summary>摘要</summary>
“数据聚合，将观察值按其相似性分组，是无监督学习中的关键组成部分，在生物、医学和社会科学等领域有广泛的应用。在这些领域中，数据往往具有复杂的相互关系，例如人们在社交网络上的多种特征和意见。现有的聚合方法无法处理这些复杂性，深度学习可以近似这些关系，但是无法直接使其作为分析的输入。在这篇论文中，我们想要解决这个潜在的盲点在无监督学习文献中。我们可以创建网络意识 embedding，通过一般化欧几何距离来估算网络中节点属性之间的距离。与现有文献中所有方法不同，我们不是将网络节点聚合，而是其节点属性。在我们的实验中，我们发现在应用于多个领域，如市场学、经济学和政治科学等，有助于提供实用的洞察。我们的方法是完全开源的，数据和代码都可以在论文中提供，以便重现所有结果。”
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Causality-Signals-in-Medical-Images-A-Pilot-Study-with-Empirical-Results"><a href="#Exploiting-Causality-Signals-in-Medical-Images-A-Pilot-Study-with-Empirical-Results" class="headerlink" title="Exploiting Causality Signals in Medical Images: A Pilot Study with Empirical Results"></a>Exploiting Causality Signals in Medical Images: A Pilot Study with Empirical Results</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10399">http://arxiv.org/abs/2309.10399</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gianluca Carloni, Sara Colantonio</li>
<li>for: 这篇论文是用于自动分类医疗影像，以实现弱因果信号在场景中的模型，以描述一个特定区域的影像特征如何影响另一个区域的影像特征。</li>
<li>methods: 这篇论文使用了两个组件：一个卷积神经网络背bone和一个因果因素提取模组。这个模组计算出各个特征对象的权重，以强调每个特征对象的影像部分。可以根据两个外部信号来修改这个模组的功能，因此获得不同的方法variant。</li>
<li>results: 这篇论文在一个公开的 проstate MRI 影像数据集上进行了量值实验、质感评估和截除研究，结果显示了我们的方法可以提高分类性能和生成更加可靠的预测，尤其是在医疗影像中， precisione 和可靠性是诊断和治疗规划的重要因素。<details>
<summary>Abstract</summary>
We present a new method for automatically classifying medical images that uses weak causal signals in the scene to model how the presence of a feature in one part of the image affects the appearance of another feature in a different part of the image. Our method consists of two components: a convolutional neural network backbone and a causality-factors extractor module. The latter computes weights for the feature maps to enhance each feature map according to its causal influence in the image's scene. We can modify the functioning of the causality module by using two external signals, thus obtaining different variants of our method. We evaluate our method on a public dataset of prostate MRI images for prostate cancer diagnosis, using quantitative experiments, qualitative assessment, and ablation studies. Our results show that our method improves classification performance and produces more robust predictions, focusing on relevant parts of the image. That is especially important in medical imaging, where accurate and reliable classifications are essential for effective diagnosis and treatment planning.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的自动化医学图像分类方法，该方法利用图像场景中的弱 causal 信号来模型图像中不同部分之间的特征之间的相互影响。我们的方法包括两个组件：一个 convolutional neural network 背bone 和一个 causality-factors 提取模块。后者计算图像feature map 中的强度，以强调每个特征图像中的相互影响。我们可以通过使用两个外部信号来修改 causality 模块的功能，从而获得不同的方法变体。我们对公共的 проstate MRI 图像集进行评估，使用量化实验、质量评估和剪辑研究来评估我们的方法。我们的结果显示，我们的方法可以提高分类性能，生成更加稳定的预测结果，特别是在医学成像中，准确和可靠的分类是诊断和治疗规划的关键。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-questionnaires-for-facilitating-patient-data-entry-in-clinical-decision-support-systems-Methods-and-application-to-STOPP-START-v2"><a href="#Adaptive-questionnaires-for-facilitating-patient-data-entry-in-clinical-decision-support-systems-Methods-and-application-to-STOPP-START-v2" class="headerlink" title="Adaptive questionnaires for facilitating patient data entry in clinical decision support systems: Methods and application to STOPP&#x2F;START v2"></a>Adaptive questionnaires for facilitating patient data entry in clinical decision support systems: Methods and application to STOPP&#x2F;START v2</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10398">http://arxiv.org/abs/2309.10398</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jean-Baptiste Lamy, Abdelmalek Mouazer, Karima Sedki, Sophie Dubois, Hector Falcoff</li>
<li>for: 该论文目的是提出一种简化患者数据录入的解决方案，以便临床医生更容易使用临床决策支持系统。</li>
<li>methods: 该论文使用了一种适应问卷，即在用户交互过程中动态显示或隐藏问题的问卷，以简化患者数据录入。同时，该论文还提出了一种将临床规则翻译成显示规则，以确定问卷中需要显示的项目的方法。</li>
<li>results: 该论文应用于一种决策支持系统，通过使用适应问卷，可以大大减少显示的临床条件数量，从原来的一半减少到一半。在临床医生focus group会议上，该适应问卷被评价为“很容易使用”。未来，该方法可能可以应用于其他指南，并适应于患者自身的数据录入。<details>
<summary>Abstract</summary>
Clinical decision support systems are software tools that help clinicians to make medical decisions. However, their acceptance by clinicians is usually rather low. A known problem is that they often require clinicians to manually enter lots of patient data, which is long and tedious. Existing solutions, such as the automatic data extraction from electronic health record, are not fully satisfying, because of low data quality and availability. In practice, many systems still include long questionnaire for data entry.   In this paper, we propose an original solution to simplify patient data entry, using an adaptive questionnaire, i.e. a questionnaire that evolves during user interaction, showing or hiding questions dynamically. Considering a rule-based decision support systems, we designed methods for translating the system's clinical rules into display rules that determine the items to show in the questionnaire, and methods for determining the optimal order of priority among the items in the questionnaire. We applied this approach to a decision support system implementing STOPP/START v2, a guideline for managing polypharmacy. We show that it permits reducing by about two thirds the number of clinical conditions displayed in the questionnaire. Presented to clinicians during focus group sessions, the adaptive questionnaire was found "pretty easy to use". In the future, this approach could be applied to other guidelines, and adapted for data entry by patients.
</details>
<details>
<summary>摘要</summary>
临床决策支持系统是软件工具，帮助临床医生做出医疗决策。然而，它们的接受度通常很低。一个知道的问题是，它们经常需要临床医生手动输入大量患者数据，这是长时间的和繁琐的。现有的解决方案，如自动提取电子医疗记录中的数据，并不充分满足，因为数据质量和可用性不够。在实践中，许多系统仍然包含长问卷。在这篇论文中，我们提出了一种新的解决方案，以简化患者数据输入。我们使用了适应问卷，即在用户互动中动态显示或隐藏问题的问卷。考虑到规则驱动的决策支持系统，我们设计了将系统的临床规则翻译成显示规则，以确定问卷中显示的项目的顺序和优先级。我们应用了这种方法于一个管理多剂药物的决策支持系统，我们发现可以将问卷中显示的临床条件减少到大约两 third。在临床医生Focus组会议中展示了适应问卷，他们认为它很容易使用。未来，这种方法可能会应用于其他指南，并适应用于患者的数据输入。
</details></li>
</ul>
<hr>
<h2 id="Graph-Contrastive-Learning-Meets-Graph-Meta-Learning-A-Unified-Method-for-Few-shot-Node-Tasks"><a href="#Graph-Contrastive-Learning-Meets-Graph-Meta-Learning-A-Unified-Method-for-Few-shot-Node-Tasks" class="headerlink" title="Graph Contrastive Learning Meets Graph Meta Learning: A Unified Method for Few-shot Node Tasks"></a>Graph Contrastive Learning Meets Graph Meta Learning: A Unified Method for Few-shot Node Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10376">http://arxiv.org/abs/2309.10376</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haoliu-cola/cola">https://github.com/haoliu-cola/cola</a></li>
<li>paper_authors: Hao Liu, Jiarui Feng, Lecheng Kong, Dacheng Tao, Yixin Chen, Muhan Zhang</li>
<li>for: 本研究旨在提出一种新的几拟标签分类方法，以解决现有的几拟标签分类方法受到过拟合问题的限制。</li>
<li>methods: 本研究使用图生成学（Graph Neural Networks，GNNs）和强化学习（fine-tuning）两种方法，并结合了图对比学习（graph contrastive learning）。</li>
<li>results: 对于几拟标签分类任务，我们的方法COLA可以在少量数据情况下达到新的顶峰性能，而且可以减少过拟合风险。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have become popular in Graph Representation Learning (GRL). One fundamental application is few-shot node classification. Most existing methods follow the meta learning paradigm, showing the ability of fast generalization to few-shot tasks. However, recent works indicate that graph contrastive learning combined with fine-tuning can significantly outperform meta learning methods. Despite the empirical success, there is limited understanding of the reasons behind it. In our study, we first identify two crucial advantages of contrastive learning compared to meta learning, including (1) the comprehensive utilization of graph nodes and (2) the power of graph augmentations. To integrate the strength of both contrastive learning and meta learning on the few-shot node classification tasks, we introduce a new paradigm: Contrastive Few-Shot Node Classification (COLA). Specifically, COLA employs graph augmentations to identify semantically similar nodes, which enables the construction of meta-tasks without the need for label information. Therefore, COLA can utilize all nodes to construct meta-tasks, further reducing the risk of overfitting. Through extensive experiments, we validate the essentiality of each component in our design and demonstrate that COLA achieves new state-of-the-art on all tasks.
</details>
<details>
<summary>摘要</summary>
граф neural networks (GNNs) 已成为graph representation learning (GRL) 中流行的方法之一。其中一个基本应用是几拟分类。大多数现有方法采用meta learning paradigm，表明它们在几拟任务上快速泛化的能力。然而，最近的研究表明，结合图像学习和精度调整可以明显超过meta learning方法。 DESPITE THE EMPERICAL SUCCESS， THERE IS LIMITED UNDERSTANDING OF THE REASONS BEHIND IT。在我们的研究中，我们首先确定了对比学习与meta learning之间的两大优势，包括（1）图像中节点的全面利用和（2）图像的扩展。为了结合对比学习和meta learning在几拟节点 classification中的优势，我们介绍了一种新的方法：对比几拟节点分类（COLA）。具体来说，COLA使用图像扩展来标识semantically similar的节点，从而无需标签信息可以构建meta-任务。因此，COLA可以完全利用所有节点来构建meta-任务，从而减少风险过拟合。通过广泛的实验，我们证明了我们的设计的每一个组件的重要性，并证明了COLA在所有任务上达到了新的状态机。
</details></li>
</ul>
<hr>
<h2 id="Generative-AI-vs-AGI-The-Cognitive-Strengths-and-Weaknesses-of-Modern-LLMs"><a href="#Generative-AI-vs-AGI-The-Cognitive-Strengths-and-Weaknesses-of-Modern-LLMs" class="headerlink" title="Generative AI vs. AGI: The Cognitive Strengths and Weaknesses of Modern LLMs"></a>Generative AI vs. AGI: The Cognitive Strengths and Weaknesses of Modern LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10371">http://arxiv.org/abs/2309.10371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ben Goertzel</li>
<li>For: The paper discusses the cognitive strengths and weaknesses of interactive large language models (LLMs) such as ChatGPT, GPT-4, Bard, and Llama, and how they differ from human cognitive systems.* Methods: The paper reviews the basic cognitive architectures of these LLMs and argues that incremental improvement is not a viable approach to achieving human-level artificial general intelligence (AGI).* Results: The paper suggests that while LLMs cannot form significant parts of human-level AGI architectures on their own, they can still provide valuable insights into human-level AGI and should be studied and experimented with. Additionally, the paper touches on social and ethical matters regarding LLMs, such as misinformation and economic upheavals, but argues that a different policy approach is needed compared to more credible approximations of human-level AGI.<details>
<summary>Abstract</summary>
A moderately detailed consideration of interactive LLMs as cognitive systems is given, focusing on LLMs circa mid-2023 such as ChatGPT, GPT-4, Bard, Llama, etc.. Cognitive strengths of these systems are reviewed, and then careful attention is paid to the substantial differences between the sort of cognitive system these LLMs are, and the sort of cognitive systems human beings are. It is found that many of the practical weaknesses of these AI systems can be tied specifically to lacks in the basic cognitive architectures according to which these systems are built. It is argued that incremental improvement of such LLMs is not a viable approach to working toward human-level AGI, in practical terms given realizable amounts of compute resources. This does not imply there is nothing to learn about human-level AGI from studying and experimenting with LLMs, nor that LLMs cannot form significant parts of human-level AGI architectures that also incorporate other ideas. Social and ethical matters regarding LLMs are very briefly touched from this perspective, which implies that while care should be taken regarding misinformation and other issues, and economic upheavals will need their own social remedies based on their unpredictable course as with any powerfully impactful technology, overall the sort of policy needed as regards modern LLMs is quite different than would be the case if a more credible approximation to human-level AGI were at hand.
</details>
<details>
<summary>摘要</summary>
一篇moderately detailed的文章对交互式LLM进行了认知系统的评估，主要focus on LLlMAmid-2023年，如ChatGPT、GPT-4、Bard、Llama等等。文章评估了这些系统的认知优势，然后仔细审视了这些AI系统与人类认知系统之间的重要差异。发现这些AI系统的实用弱点可以追溯到其基本认知架构的缺失。 argue that不可靠地提高这些LLMs不是实现人类水平AGI的可靠方法，具体来说，随着可计算资源的增加，这些LLMs的改进速度会变得慢。这并不意味着不能从研究和实验LLMs中学习人类水平AGI，也不意味着LLMs无法成为人类水平AGI架构的重要组成部分。文章 briefly touched social and ethical matters related to LLMs from this perspective, suggesting that while care should be taken to address misinformation and other issues, and economic upheavals will need their own social remedies based on their unpredictable course, the policy needed for modern LLMs is quite different from what would be the case if a more credible approximation to human-level AGI were at hand.
</details></li>
</ul>
<hr>
<h2 id="Geometric-structure-of-shallow-neural-networks-and-constructive-mathcal-L-2-cost-minimization"><a href="#Geometric-structure-of-shallow-neural-networks-and-constructive-mathcal-L-2-cost-minimization" class="headerlink" title="Geometric structure of shallow neural networks and constructive ${\mathcal L}^2$ cost minimization"></a>Geometric structure of shallow neural networks and constructive ${\mathcal L}^2$ cost minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10370">http://arxiv.org/abs/2309.10370</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Chen, Patricia Muñoz Ewald</li>
<li>for: 这 paper 描述了一种 shallow neural network 的结构，包括一层抑制函数、${\mathcal L}^2$ Schatten class 成本函数、输入空间为 ${\mathbb R}^M$，输出空间为 ${\mathbb R}^Q$  ($Q\leq M$)，训练输入样本大小为 $N&gt;QM$。</li>
<li>methods: 这 paper 使用了一种基于投影的approximate optimizer，并证明了cost function 的最小值下界为 $O(\delta_P)$，其中 $\delta_P$ 是训练输入信号噪声比。在特殊情况下 $M&#x3D;Q$ 时，我们可以得到一个精确的本地最小值，其差异与上述下界为 $O(\delta_P^2)$。</li>
<li>results: 这 paper 证明了cost function 的最小值下界，并构造了一个可被训练的 neural network，该网络可以imetrize 输入空间中 $Q$-维子空间，它是由训练输入样本的平均值 $\overline{x_{0,j}$ ($j&#x3D;1,\dots,Q$) 所决定的。<details>
<summary>Abstract</summary>
In this paper, we provide a geometric interpretation of the structure of shallow neural networks characterized by one hidden layer, a ramp activation function, an ${\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, input space ${\mathbb R}^M$, output space ${\mathbb R}^Q$ with $Q\leq M$, and training input sample size $N>QM$. We prove an upper bound on the minimum of the cost function of order $O(\delta_P$ where $\delta_P$ measures the signal to noise ratio of training inputs. We obtain an approximate optimizer using projections adapted to the averages $\overline{x_{0,j}$ of training input vectors belonging to the same output vector $y_j$, $j=1,\dots,Q$. In the special case $M=Q$, we explicitly determine an exact degenerate local minimum of the cost function; the sharp value differs from the upper bound obtained for $Q\leq M$ by a relative error $O(\delta_P^2)$. The proof of the upper bound yields a constructively trained network; we show that it metrizes the $Q$-dimensional subspace in the input space ${\mathbb R}^M$ spanned by $\overline{x_{0,j}$, $j=1,\dots,Q$. We comment on the characterization of the global minimum of the cost function in the given context.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提供了一种几何 interpreting the structure of shallow neural networks with one hidden layer, ramp activation function, $\mathcal{L}^2$ Schatten class (或希尔伯特- Schmidt) cost function, input space $\mathbb{R}^M$, output space $\mathbb{R}^Q$ with $Q\leq M$, 和训练输入样本大小 $N>QM$. 我们证明了cost function的最小值的上界为$\mathcal{O}(\delta_P)$，其中$\delta_P$是训练输入信号响应率。我们使用适应于$\overline{x_{0,j}$的投影来获得一个approximate optimizer。在特殊情况下，当$M=Q$时，我们确切地确定了一个精确的地方最小值，其差异与上界相对Error $O(\delta_P^2)$。证明上界带来一个可重构的网络，我们表明它在输入空间$\mathbb{R}^M$中метrize了一个$Q$-维子空间，该子空间是由$\overline{x_{0,j}$所确定的。我们评论了在给定的 context中global minimum的特征。
</details></li>
</ul>
<hr>
<h2 id="Toward-efficient-resource-utilization-at-edge-nodes-in-federated-learning"><a href="#Toward-efficient-resource-utilization-at-edge-nodes-in-federated-learning" class="headerlink" title="Toward efficient resource utilization at edge nodes in federated learning"></a>Toward efficient resource utilization at edge nodes in federated learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10367">http://arxiv.org/abs/2309.10367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sadi Alawadi, Addi Ait-Mlouk, Salman Toor, Andreas Hellander</li>
<li>for: 本研究旨在 empirically 探讨在 Federated Learning 中 randomly 选择层进行模型训练的策略可以实现资源占用减少和全球模型收敛不受影响。</li>
<li>methods: 本研究使用了 Federated Learning 框架 FEDn，在不同的 dataset（CIFAR-10、CASA 和 IMDB）和任务（使用不同的深度学习模型架构）下进行了多个实验。</li>
<li>results: 结果显示，只训练部分模型层可以加速训练过程，有效地利用设备上的资源，并将数据传输量减少了约 75% 和 53%，无需妨碍全球模型准确性。<details>
<summary>Abstract</summary>
Federated learning (FL) enables edge nodes to collaboratively contribute to constructing a global model without sharing their data. This is accomplished by devices computing local, private model updates that are then aggregated by a server. However, computational resource constraints and network communication can become a severe bottleneck for larger model sizes typical for deep learning applications. Edge nodes tend to have limited hardware resources (RAM, CPU), and the network bandwidth and reliability at the edge is a concern for scaling federated fleet applications. In this paper, we propose and evaluate a FL strategy inspired by transfer learning in order to reduce resource utilization on devices, as well as the load on the server and network in each global training round. For each local model update, we randomly select layers to train, freezing the remaining part of the model. In doing so, we can reduce both server load and communication costs per round by excluding all untrained layer weights from being transferred to the server. The goal of this study is to empirically explore the potential trade-off between resource utilization on devices and global model convergence under the proposed strategy. We implement the approach using the federated learning framework FEDn. A number of experiments were carried out over different datasets (CIFAR-10, CASA, and IMDB), performing different tasks using different deep-learning model architectures. Our results show that training the model partially can accelerate the training process, efficiently utilizes resources on-device, and reduce the data transmission by around 75% and 53% when we train 25%, and 50% of the model layers, respectively, without harming the resulting global model accuracy.
</details>
<details>
<summary>摘要</summary>
联合学习（FL）allow edge nodes to collaboratively construct a global model without sharing their data. This is achieved by devices computing local, private model updates that are then aggregated by a server. However, computational resource constraints and network communication can become a severe bottleneck for larger model sizes typical for deep learning applications. Edge nodes tend to have limited hardware resources (RAM, CPU), and the network bandwidth and reliability at the edge is a concern for scaling federated fleet applications. In this paper, we propose and evaluate a FL strategy inspired by transfer learning to reduce resource utilization on devices and the load on the server and network in each global training round. For each local model update, we randomly select layers to train, freezing the remaining part of the model. By doing so, we can reduce both server load and communication costs per round by excluding all untrained layer weights from being transferred to the server. The goal of this study is to empirically explore the potential trade-off between resource utilization on devices and global model convergence under the proposed strategy. We implement the approach using the federated learning framework FEDn. A number of experiments were carried out over different datasets (CIFAR-10, CASA, and IMDB), performing different tasks using different deep-learning model architectures. Our results show that training the model partially can accelerate the training process, efficiently utilize resources on-device, and reduce the data transmission by around 75% and 53% when we train 25%, and 50% of the model layers, respectively, without harming the resulting global model accuracy.
</details></li>
</ul>
<hr>
<h2 id="OccluTrack-Rethinking-Awareness-of-Occlusion-for-Enhancing-Multiple-Pedestrian-Tracking"><a href="#OccluTrack-Rethinking-Awareness-of-Occlusion-for-Enhancing-Multiple-Pedestrian-Tracking" class="headerlink" title="OccluTrack: Rethinking Awareness of Occlusion for Enhancing Multiple Pedestrian Tracking"></a>OccluTrack: Rethinking Awareness of Occlusion for Enhancing Multiple Pedestrian Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10360">http://arxiv.org/abs/2309.10360</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hieu9955/ggggg">https://github.com/hieu9955/ggggg</a></li>
<li>paper_authors: Jianjun Gao, Yi Wang, Kim-Hui Yap, Kratika Garg, Boon Siew Han</li>
<li>for: 提高多人跟踪在 occlusion 场景下的精度和稳定性。</li>
<li>methods: 提出了一种适应 occlusion 的多人跟踪方法，包括异常动量抑制机制、pose-guided re-ID 模块和 occlusion-aware 关联方法。</li>
<li>results: 在 MOT-Challenge 数据集上进行了广泛的评估，并表明了我们的 OccluTrack 在多人跟踪和关联性能方面的改进。特别是，对 IDF1、IDSw、AssA 和 AssR 的改进表明了我们的 OccluTrack 在 occlusion 场景下的效果。<details>
<summary>Abstract</summary>
Multiple pedestrian tracking faces the challenge of tracking pedestrians in the presence of occlusion. Existing methods suffer from inaccurate motion estimation, appearance feature extraction, and association due to occlusion, leading to inadequate Identification F1-Score (IDF1), excessive ID switches (IDSw), and insufficient association accuracy and recall (AssA and AssR). We found that the main reason is abnormal detections caused by partial occlusion. In this paper, we suggest that the key insight is explicit motion estimation, reliable appearance features, and fair association in occlusion scenes. Specifically, we propose an adaptive occlusion-aware multiple pedestrian tracker, OccluTrack. We first introduce an abnormal motion suppression mechanism into the Kalman Filter to adaptively detect and suppress outlier motions caused by partial occlusion. Second, we propose a pose-guided re-ID module to extract discriminative part features for partially occluded pedestrians. Last, we design a new occlusion-aware association method towards fair IoU and appearance embedding distance measurement for occluded pedestrians. Extensive evaluation results demonstrate that our OccluTrack outperforms state-of-the-art methods on MOT-Challenge datasets. Particularly, the improvements on IDF1, IDSw, AssA, and AssR demonstrate the effectiveness of our OccluTrack on tracking and association performance.
</details>
<details>
<summary>摘要</summary>
多人行踪面临 occlusion 挑战，现有方法受到 occlusion 的影响，导致不准确的运动估计、外观特征提取和关联，从而导致 IDF1 分数不够高、ID Switches 过多、关联准确率和回归率不够高。我们发现主要原因是部分 occlusion 引起的异常检测。在这篇论文中，我们提出了关键思路，即显式运动估计、可靠的外观特征和公平的关联在 occlusion 场景下。 Specifically，我们提出了一种适应 occlusion 的多人行踪器，即 OccluTrack。我们首先在 Kalman 筛引入了异常运动抑制机制，以适应部分 occlusion 引起的异常检测。其次，我们提出了一种基于 pose 的 Re-ID 模块，以提取部分 occlusion 的特征。最后，我们设计了一种新的 occlusion-aware 关联方法，以实现公平的 IoU 和外观嵌入距离度量测量。我们对 MOT-Challenge 数据集进行了广泛的评估，结果显示，我们的 OccluTrack 超过了当前状态的方法。特别是，对 IDF1、ID Switches、AssA 和 AssR 的改进表明了我们的 OccluTrack 在跟踪和关联性能方面的效果。
</details></li>
</ul>
<hr>
<h2 id="Explaining-Agent-Behavior-with-Large-Language-Models"><a href="#Explaining-Agent-Behavior-with-Large-Language-Models" class="headerlink" title="Explaining Agent Behavior with Large Language Models"></a>Explaining Agent Behavior with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10346">http://arxiv.org/abs/2309.10346</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xijia Zhang, Yue Guo, Simon Stepputtis, Katia Sycara, Joseph Campbell</li>
<li>for: 这种研究旨在提供一种能够让智能代理人（如机器人）对其决策的推理进行说明，以便与人类对手中的解释。</li>
<li>methods: 该方法基于对状态和行为的观察，不需要了解深度神经网络等模型的具体表示。通过学习紧凑表示，生成自然语言的解释，并且可以进行用户与大语言模型的互动。</li>
<li>results: 经过用户测试和实验，该方法能够生成与人类专家的解释相当有帮助的解释，同时允许用户进行如清晰化和反向问题的互动。<details>
<summary>Abstract</summary>
Intelligent agents such as robots are increasingly deployed in real-world, safety-critical settings. It is vital that these agents are able to explain the reasoning behind their decisions to human counterparts, however, their behavior is often produced by uninterpretable models such as deep neural networks. We propose an approach to generate natural language explanations for an agent's behavior based only on observations of states and actions, agnostic to the underlying model representation. We show how a compact representation of the agent's behavior can be learned and used to produce plausible explanations with minimal hallucination while affording user interaction with a pre-trained large language model. Through user studies and empirical experiments, we show that our approach generates explanations as helpful as those generated by a human domain expert while enabling beneficial interactions such as clarification and counterfactual queries.
</details>
<details>
<summary>摘要</summary>
智能代理人如机器人在实际世界中越来越多地被部署。这些代理人的决策需要给人类对手 explicable，但它们的行为通常是由不可解释的模型，如深度神经网络生成的。我们提出了一种方法，可以基于状态和行动的观察来生成代理人的决策的自然语言解释。我们表明了如何学习一种紧凑的代理人行为表示，并使用这种表示生成可靠的解释，同时允许用户与预训练的大型自然语言模型进行互动。通过用户研究和实验，我们表明了我们的方法可以生成与人类领域专家生成的解释相当有用，并允许有利的互动，如确认和对比查询。
</details></li>
</ul>
<hr>
<h2 id="FedWOA-A-Federated-Learning-Model-that-uses-the-Whale-Optimization-Algorithm-for-Renewable-Energy-Prediction"><a href="#FedWOA-A-Federated-Learning-Model-that-uses-the-Whale-Optimization-Algorithm-for-Renewable-Energy-Prediction" class="headerlink" title="FedWOA: A Federated Learning Model that uses the Whale Optimization Algorithm for Renewable Energy Prediction"></a>FedWOA: A Federated Learning Model that uses the Whale Optimization Algorithm for Renewable Energy Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10337">http://arxiv.org/abs/2309.10337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Viorica Chifu, Tudor Cioara, Cristian Anitiei, Cristina Pop, Ionut Anghel</li>
<li>for: 这篇论文旨在解决机器学习模型中敏感个人信息的隐私问题，通过训练大规模数据集来提高能源预测的准确性。</li>
<li>methods: 该论文提出了一种基于联合学习的解决方案，使用鲸鱼优化算法将本地LSTM神经网络模型的参数重新权值融合成全局共享模型，并通过K-Means对不同数据的整合进行处理。</li>
<li>results: 论文的实验结果表明，FedWOA可以提高能源预测模型的准确性，比 FedAVG 提高25%的MSE和16%的MAE，同时显示了好的叠加和降低损失。<details>
<summary>Abstract</summary>
Privacy is important when dealing with sensitive personal information in machine learning models, which require large data sets for training. In the energy field, access to household prosumer energy data is crucial for energy predictions to support energy grid management and large-scale adoption of renewables however citizens are often hesitant to grant access to cloud-based machine learning models. Federated learning has been proposed as a solution to privacy challenges however report issues in generating the global prediction model due to data heterogeneity, variations in generation patterns, and the high number of parameters leading to even lower prediction accuracy. This paper addresses these challenges by introducing FedWOA a novel federated learning model that employs the Whale Optimization Algorithm to aggregate global prediction models from the weights of local LTSM neural network models trained on prosumer energy data. The proposed solution identifies the optimal vector of weights in the search spaces of the local models to construct the global shared model and then is subsequently transmitted to the local nodes to improve the prediction quality at the prosumer site while for handling non-IID data K-Means was used for clustering prosumers with similar scale of energy data. The evaluation results on prosumers energy data have shown that FedWOA can effectively enhance the accuracy of energy prediction models accuracy by 25% for MSE and 16% for MAE compared to FedAVG while demonstrating good convergence and reduced loss.
</details>
<details>
<summary>摘要</summary>
隐私是机器学习模型处理敏感个人信息的重要问题，这些模型需要大量数据进行训练。在能源领域，获取家庭生产者能源数据是重要的，以支持能源网络管理和大规模采用可再生能源，但是公民经常拒绝提供云端机器学习模型访问。联邦学习被提议为解决隐私挑战，但是报告表示因数据不均匀、生成模式变化和参数太多，导致预测精度更低。本文提出了一种名为FedWOA的联邦学习模型，该模型使用吴顿优化算法对全球预测模型的 weights 进行聚合，从而提高预测质量。在处理非Identical distributed（非ID）数据时，使用K-Means进行分 clustering prosumers的能源数据，以便构建全球共享模型。评估结果表明，FedWOA可以提高能源预测模型的准确率，比 FedAVG 提高25%的MSE和16%的MAE，同时示出良好的叠加和降低损失。
</details></li>
</ul>
<hr>
<h2 id="Learning-based-2D-Irregular-Shape-Packing"><a href="#Learning-based-2D-Irregular-Shape-Packing" class="headerlink" title="Learning based 2D Irregular Shape Packing"></a>Learning based 2D Irregular Shape Packing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10329">http://arxiv.org/abs/2309.10329</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeshi Yang, Zherong Pan, Manyi Li, Kui Wu, Xifeng Gao</li>
<li>for: 用于实现纹理Atlas中3D模型的 памяти有效的外观渲染。</li>
<li>methods: 使用学习支持的2D不Regular shape填充方法，包括选择和分组缺陷UV patches，并使用共同优化提高填充率。</li>
<li>results: 与多种常用基elines比较，本方法在三个数据集上实现了更高的填充率，同时保持了竞争性的计算速度。<details>
<summary>Abstract</summary>
2D irregular shape packing is a necessary step to arrange UV patches of a 3D model within a texture atlas for memory-efficient appearance rendering in computer graphics. Being a joint, combinatorial decision-making problem involving all patch positions and orientations, this problem has well-known NP-hard complexity. Prior solutions either assume a heuristic packing order or modify the upstream mesh cut and UV mapping to simplify the problem, which either limits the packing ratio or incurs robustness or generality issues. Instead, we introduce a learning-assisted 2D irregular shape packing method that achieves a high packing quality with minimal requirements from the input. Our method iteratively selects and groups subsets of UV patches into near-rectangular super patches, essentially reducing the problem to bin-packing, based on which a joint optimization is employed to further improve the packing ratio. In order to efficiently deal with large problem instances with hundreds of patches, we train deep neural policies to predict nearly rectangular patch subsets and determine their relative poses, leading to linear time scaling with the number of patches. We demonstrate the effectiveness of our method on three datasets for UV packing, where our method achieves a higher packing ratio over several widely used baselines with competitive computational speed.
</details>
<details>
<summary>摘要</summary>
二维不规则形填充是计算机图形中为三维模型的Texture Atlas中的UV贴图进行内存高效的显示的必要步骤。作为一个共同的、复杂决策问题，这个问题有well-known NP-hard复杂性。先前的解决方案 Either assume a heuristic packing order或修改上游缝隙和UV映射以简化问题，这些方法 Either limit the packing ratio or incur robustness or generality issues。相比之下，我们介绍了一种学习帮助的二维不规则形填充方法，可以 achieve high packing quality with minimal input requirements。我们的方法 iteratively selects and groups subsets of UV patches into near-rectangular super patches， essentially reducing the problem to bin-packing， based on which a joint optimization is employed to further improve the packing ratio。为了有效地处理大型问题集合，我们训练了深度神经策略来预测 nearly rectangular patch subsets和他们的相对位置，从而实现 linear time scaling with the number of patches。我们在三个UV填充数据集上展示了我们的方法的效果，其中我们的方法高于许多常用的基线方法，并且与 computation speed相当。
</details></li>
</ul>
<hr>
<h2 id="QASnowball-An-Iterative-Bootstrapping-Framework-for-High-Quality-Question-Answering-Data-Generation"><a href="#QASnowball-An-Iterative-Bootstrapping-Framework-for-High-Quality-Question-Answering-Data-Generation" class="headerlink" title="QASnowball: An Iterative Bootstrapping Framework for High-Quality Question-Answering Data Generation"></a>QASnowball: An Iterative Bootstrapping Framework for High-Quality Question-Answering Data Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10326">http://arxiv.org/abs/2309.10326</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kunlun Zhu, Shihao Liang, Xu Han, Zhi Zheng, Guoyang Zeng, Zhiyuan Liu, Maosong Sun</li>
<li>for: 本研究的目的是提出一种基于循环增强的问答数据生成方法，以便为Question Answering（QA）模型提供更多和更高质量的数据来源。</li>
<li>methods: 该方法包括三个模块：一个答案抽取模块，用于从无结构文档中提取核心短语作为答案候选;一个问题生成模块，用于基于文档和答案候选生成问题;以及一个问答数据筛选模块，用于筛选高质量的问答数据。此外，该方法还可以通过重新种子来自我改进，以达到不断提高数据生成质量的目的。</li>
<li>results: 我们在高资源英文场景和中资源中文场景进行了实验，结果表明：（1）使用生成的数据来训练QA模型可以达到与使用直接监督数据相当的性能;（2）先使用生成的数据进行预训练，然后使用直接监督数据进行细化训练可以达到更好的性能。<details>
<summary>Abstract</summary>
Recent years have witnessed the success of question answering (QA), especially its potential to be a foundation paradigm for tackling diverse NLP tasks. However, obtaining sufficient data to build an effective and stable QA system still remains an open problem. For this problem, we introduce an iterative bootstrapping framework for QA data augmentation (named QASnowball), which can iteratively generate large-scale high-quality QA data based on a seed set of supervised examples. Specifically, QASnowball consists of three modules, an answer extractor to extract core phrases in unlabeled documents as candidate answers, a question generator to generate questions based on documents and candidate answers, and a QA data filter to filter out high-quality QA data. Moreover, QASnowball can be self-enhanced by reseeding the seed set to fine-tune itself in different iterations, leading to continual improvements in the generation quality. We conduct experiments in the high-resource English scenario and the medium-resource Chinese scenario, and the experimental results show that the data generated by QASnowball can facilitate QA models: (1) training models on the generated data achieves comparable results to using supervised data, and (2) pre-training on the generated data and fine-tuning on supervised data can achieve better performance. Our code and generated data will be released to advance further work.
</details>
<details>
<summary>摘要</summary>
Recent years have witnessed the success of question answering (QA), especially its potential to be a foundation paradigm for tackling diverse NLP tasks. However, obtaining sufficient data to build an effective and stable QA system still remains an open problem. To solve this problem, we propose an iterative bootstrapping framework for QA data augmentation (named QASnowball), which can iteratively generate large-scale high-quality QA data based on a seed set of supervised examples. Specifically, QASnowball consists of three modules: an answer extractor to extract core phrases in unlabeled documents as candidate answers, a question generator to generate questions based on documents and candidate answers, and a QA data filter to filter out high-quality QA data. Moreover, QASnowball can be self-enhanced by reseeding the seed set to fine-tune itself in different iterations, leading to continual improvements in the generation quality. We conduct experiments in the high-resource English scenario and the medium-resource Chinese scenario, and the experimental results show that the data generated by QASnowball can facilitate QA models: (1) training models on the generated data achieves comparable results to using supervised data, and (2) pre-training on the generated data and fine-tuning on supervised data can achieve better performance. Our code and generated data will be released to advance further work.Here is the word-for-word translation of the text into Simplified Chinese:近年来，问答（QA）的成功尤其是一种基础理念，可以应对多种自然语言处理（NLP）任务。然而，建立有效稳定的QA系统仍然是一个打开的问题。为解决这个问题，我们提出了一个迭代启动框架，名为QASnowball，可以在种子集的超级vised例子基础上生成大规模高质量的QA数据。具体来说，QASnowball包括三个模块：一个答案提取器，可以从无标记文档中提取核心短语作为候选答案；一个问题生成器，可以基于文档和候选答案来生成问题；以及一个QA数据筛选器，可以筛选出高质量的QA数据。此外，QASnowball可以通过不同迭代来自我进行改进，从而实现不断提高生成质量。我们在高资源英语场景和中资源中文场景进行了实验，实验结果表明，QASnowball生成的数据可以帮助QA模型：（1）使用生成数据训练模型可以达到相同的性能，和（2）先进行预训练并在超级vised数据上进行细化可以实现更好的性能。我们将代码和生成的数据发布，以便进一步的工作。
</details></li>
</ul>
<hr>
<h2 id="Metastatic-Breast-Cancer-Prognostication-Through-Multimodal-Integration-of-Dimensionality-Reduction-Algorithms-and-Classification-Algorithms"><a href="#Metastatic-Breast-Cancer-Prognostication-Through-Multimodal-Integration-of-Dimensionality-Reduction-Algorithms-and-Classification-Algorithms" class="headerlink" title="Metastatic Breast Cancer Prognostication Through Multimodal Integration of Dimensionality Reduction Algorithms and Classification Algorithms"></a>Metastatic Breast Cancer Prognostication Through Multimodal Integration of Dimensionality Reduction Algorithms and Classification Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10324">http://arxiv.org/abs/2309.10324</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bliss Singhal, Fnu Pooja</li>
<li>for: 这个研究旨在利用机器学习方法检测肿瘤是否为癌变。</li>
<li>methods: 研究使用了两种预处理算法：原理Components分析和种群算法，以降低数据维度，然后使用了三种分类算法：逻辑回归、决策树分类器和k-最近邻分类器来检测肿瘤是否为癌变。</li>
<li>results: 研究发现，使用这些预处理和分类算法的ML管道可以达到71.14%的准确率，表明这些算法在检测肿瘤是否为癌变方面具有潜在的应用前景。<details>
<summary>Abstract</summary>
Machine learning (ML) is a branch of Artificial Intelligence (AI) where computers analyze data and find patterns in the data. The study focuses on the detection of metastatic cancer using ML. Metastatic cancer is the point where the cancer has spread to other parts of the body and is the cause of approximately 90% of cancer related deaths. Normally, pathologists spend hours each day to manually classify whether tumors are benign or malignant. This tedious task contributes to mislabeling metastasis being over 60% of time and emphasizes the importance to be aware of human error, and other inefficiencies. ML is a good candidate to improve the correct identification of metastatic cancer saving thousands of lives and can also improve the speed and efficiency of the process thereby taking less resources and time. So far, deep learning methodology of AI has been used in the research to detect cancer. This study is a novel approach to determine the potential of using preprocessing algorithms combined with classification algorithms in detecting metastatic cancer. The study used two preprocessing algorithms: principal component analysis (PCA) and the genetic algorithm to reduce the dimensionality of the dataset, and then used three classification algorithms: logistic regression, decision tree classifier, and k-nearest neighbors to detect metastatic cancer in the pathology scans. The highest accuracy of 71.14% was produced by the ML pipeline comprising of PCA, the genetic algorithm, and the k-nearest neighbors algorithm, suggesting that preprocessing and classification algorithms have great potential for detecting metastatic cancer.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）是人工智能（AI）的一个分支，计算机通过分析数据找到数据中的模式。这项研究关注利用ML检测肿瘤是否为恶性肿瘤。肿瘤肿瘤是指肿瘤已经扩散到身体其他部分，accounts for approximately 90% of cancer-related deaths. 通常， PATHOLOGISTS spend hours each day manually classify tumors as benign or malignant, but this tedious task can lead to mislabeling of metastasis, which can be over 60% of the time. ML is a good candidate to improve the correct identification of metastatic cancer, which can save thousands of lives and improve the speed and efficiency of the process, reducing the need for resources and time.在这项研究中，我们使用了深度学习方法来检测肿瘤。这是一种新的approach，我们使用了两种预处理算法：主成分分析（PCA）和 генетиче算法来减少数据集的维度，然后使用三种分类算法：Logistic regression、决策树分类器和k-nearest neighbors来检测肿瘤在Pathology scans中。最高的准确率为71.14%，这表明预处理和分类算法在检测肿瘤中具有潜在的潜力。
</details></li>
</ul>
<hr>
<h2 id="Who-to-Trust-How-and-Why-Untangling-AI-Ethics-Principles-Trustworthiness-and-Trust"><a href="#Who-to-Trust-How-and-Why-Untangling-AI-Ethics-Principles-Trustworthiness-and-Trust" class="headerlink" title="Who to Trust, How and Why: Untangling AI Ethics Principles, Trustworthiness and Trust"></a>Who to Trust, How and Why: Untangling AI Ethics Principles, Trustworthiness and Trust</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10318">http://arxiv.org/abs/2309.10318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andreas Duenser, David M. Douglas</li>
<li>for: 本文提供了关于人们对AI的信任和AI可靠性的文献综述，并强调了更清晰地分 differentiating these two concepts，以及更多的实证证据来探讨人们信任行为的影响因素。</li>
<li>methods: 本文讨论了信任AI的方法，包括不仅依赖于系统本身，还包括信任开发者们。AI伦理原则，如解释性和透明度，经常被认为能够促进用户信任，但实际证据表明这些特性对用户所认为的系统可靠性的影响并不是够清晰。</li>
<li>results: 本文认为，AI系统应该被视为社会技术系统，开发者、用户和其他相关人员在设计、开发、部署和使用系统时的参与度是决定系统可靠性的关键因素。如果不认真地考虑这些细节，那么人们对AI和可靠AI的信任就可能变得混乱，变得任何有利AI系统都会被视为可靠。<details>
<summary>Abstract</summary>
We present an overview of the literature on trust in AI and AI trustworthiness and argue for the need to distinguish these concepts more clearly and to gather more empirically evidence on what contributes to people s trusting behaviours. We discuss that trust in AI involves not only reliance on the system itself, but also trust in the developers of the AI system. AI ethics principles such as explainability and transparency are often assumed to promote user trust, but empirical evidence of how such features actually affect how users perceive the system s trustworthiness is not as abundance or not that clear. AI systems should be recognised as socio-technical systems, where the people involved in designing, developing, deploying, and using the system are as important as the system for determining whether it is trustworthy. Without recognising these nuances, trust in AI and trustworthy AI risk becoming nebulous terms for any desirable feature for AI systems.
</details>
<details>
<summary>摘要</summary>
我们提供了关于人们对AI和AI可靠性的文献综述，并 argue了更清晰地分 differentiating these concepts，并更多地寻求实证证据以确定人们如何信任系统的行为。我们讨论了人们对AI系统的信任不仅取决于系统本身，而且还取决于开发者。AI伦理原则，如可读性和透明度，通常被认为能够促进用户信任，但实际证据表明这些特性对用户对系统可靠性的看法并不那么清晰。我们认为AI系统应被视为社会技术系统，其中设计、开发、部署和使用系统的人员是决定系统可靠性的重要因素。如果不认真地考虑这些细节，则“信任AI”和“可靠AI”这两个概念可能会变得混乱，成为任何愿望的AI系统特性。
</details></li>
</ul>
<hr>
<h2 id="Investigating-the-Catastrophic-Forgetting-in-Multimodal-Large-Language-Models"><a href="#Investigating-the-Catastrophic-Forgetting-in-Multimodal-Large-Language-Models" class="headerlink" title="Investigating the Catastrophic Forgetting in Multimodal Large Language Models"></a>Investigating the Catastrophic Forgetting in Multimodal Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10313">http://arxiv.org/abs/2309.10313</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee, Yi Ma</li>
<li>for: 这个论文旨在研究多modal大语言模型（MLLM）的开发，并评估它们是否具有相同的表现水平。</li>
<li>methods: 这篇论文使用了EMT方法（Evaluating MulTimodality）来评估多modal语言模型中的分割混乱现象。</li>
<li>results: 论文发现，大多数经过精制的MLLM都无法保持与视觉模型相同的表现水平，而且随着精制的进度，MLLM会开始幻化，导致表现下降。<details>
<summary>Abstract</summary>
Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-stage fine-tuning on an image dataset improves performance across other image datasets, by enhancing the alignment of text and visual features. However, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in a significant loss of generalizability, even when the image encoder remains frozen. Our results suggest that MLLMs have yet to demonstrate performance on par with their vision models on standard image classification tasks and the current MLLM fine-tuning procedure still has room for improvement.
</details>
<details>
<summary>摘要</summary>
根据GPT4的成功，Multimodal大型语言模型（MLLM）的研究获得了更多的关注。这些研究旨在通过精心适应已经预训的语言模型和视觉模型来开发通用的MLLM。然而，在多modal LLM中，严重的忘记现象仍然是一个困扰，这意味着精心适应的模型无法保持与预训模型相同的性能水平。在这篇论文中，我们引入EMT：评估多modal性，用于评估MLLM中的忘记现象。我们首先将EMT应用于评估一些公开源的精心适应MLLM，我们发现大多数评估的MLLM都无法保持与摄像头模型在标准图像分类任务中的相同性能水平。此外，我们继续适应LLaVA，一个MLLM，并使用EMT评估其性能。我们发现，在早期的适应过程中，使用图像数据集进行适应可以提高图像和文本特征之间的对齐，但是，当精心适应进行时，MLLM开始伪造，导致严重的泛化能力损失，甚至当摄像头模型保持固定时。我们的结果表明，目前的MLLM尚未能达到和摄像头模型在标准图像分类任务中的性能水平，并且精心适应程序仍然需要改进。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Speech-PTM-Text-LLM-and-Emotional-TTS-for-Speech-Emotion-Recognition"><a href="#Leveraging-Speech-PTM-Text-LLM-and-Emotional-TTS-for-Speech-Emotion-Recognition" class="headerlink" title="Leveraging Speech PTM, Text LLM, and Emotional TTS for Speech Emotion Recognition"></a>Leveraging Speech PTM, Text LLM, and Emotional TTS for Speech Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10294">http://arxiv.org/abs/2309.10294</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyang Ma, Wen Wu, Zhisheng Zheng, Yiwei Guo, Qian Chen, Shiliang Zhang, Xie Chen</li>
<li>For: This paper aims to improve speech emotion recognition (SER) using state-of-the-art speech pre-trained models, data2vec, text generation techniques, GPT-4, and speech synthesis techniques, Azure TTS.* Methods: The paper uses a combination of speech self-supervised pre-trained models, powerful large language models (LLMs), emotional text-to-speech (TTS) models, and data augmentation techniques to generate emotionally congruent text and speech.* Results: The paper demonstrates the effectiveness of their method through experiments and ablation studies on the IEMOCAP dataset, showing that their approach outperforms other data augmentation methods and other synthetic data.Here’s the simplified Chinese text:* 为：这篇论文目的是提高语音情感识别（SER），使用当前最佳的语音预训练模型（PTM）、数据2vec、文本生成技术、GPT-4和语音生成技术、Azure TTS。* 方法：该论文使用了语音自我超vised预训练模型、强大的大型语言模型（LLM）、情感文本生成模型和数据增强技术来生成情感相符的文本和语音。* 结果：论文通过对IEMOCAP数据集进行实验和剥离研究，证明了他们的方法的有效性，比其他数据增强方法和其他合成数据更高。<details>
<summary>Abstract</summary>
In this paper, we explored how to boost speech emotion recognition (SER) with the state-of-the-art speech pre-trained model (PTM), data2vec, text generation technique, GPT-4, and speech synthesis technique, Azure TTS. First, we investigated the representation ability of different speech self-supervised pre-trained models, and we found that data2vec has a good representation ability on the SER task. Second, we employed a powerful large language model (LLM), GPT-4, and emotional text-to-speech (TTS) model, Azure TTS, to generate emotionally congruent text and speech. We carefully designed the text prompt and dataset construction, to obtain the synthetic emotional speech data with high quality. Third, we studied different ways of data augmentation to promote the SER task with synthetic speech, including random mixing, adversarial training, transfer learning, and curriculum learning. Experiments and ablation studies on the IEMOCAP dataset demonstrate the effectiveness of our method, compared with other data augmentation methods, and data augmentation with other synthetic data.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们探索了如何通过现代speech预训练模型（PTM）、数据2vec、文本生成技术（GPT-4）和speech生成技术（Azure TTS）来提高语音情感识别（SER）的性能。首先，我们研究了不同的speech自我超vised预训练模型的表示能力，并发现data2vec在SER任务上有良好的表示能力。其次，我们利用了一个强大的大语言模型（LLM）GPT-4和情感文本-to-speech（TTS）模型Azure TTS，生成情感相符的文本和speech。我们仔细设计了文本提问和数据构造，以获得高质量的人工情感语音数据。第三，我们研究了不同的数据增强方法，以提高SER任务的性能，包括随机混合、对抗训练、传输学习和课程学习。实验和缺陷分析在IEMOCAP数据集上表明了我们的方法的有效性，相比其他数据增强方法和数据增强。
</details></li>
</ul>
<hr>
<h2 id="QXAI-Explainable-AI-Framework-for-Quantitative-Analysis-in-Patient-Monitoring-Systems"><a href="#QXAI-Explainable-AI-Framework-for-Quantitative-Analysis-in-Patient-Monitoring-Systems" class="headerlink" title="QXAI: Explainable AI Framework for Quantitative Analysis in Patient Monitoring Systems"></a>QXAI: Explainable AI Framework for Quantitative Analysis in Patient Monitoring Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10293">http://arxiv.org/abs/2309.10293</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thanveer Shaik, Xiaohui Tao, Haoran Xie, Lin Li, Juan D. Velasquez, Niall Higgins</li>
<li>for: 这个研究的目的是提出一种可解释的人工智能技术，用于远程监测病人的生命体指标和 физи活动。</li>
<li>methods: 这个研究使用了深度学习模型和注意力机制，以实现可解释的人工智能框架（QXAI）。</li>
<li>results: 研究使用了PPG-DaLiA数据集和MHEALTH数据集，实现了心率预测和物理活动分类任务的状态知识和地区解释。<details>
<summary>Abstract</summary>
Artificial Intelligence techniques can be used to classify a patient's physical activities and predict vital signs for remote patient monitoring. Regression analysis based on non-linear models like deep learning models has limited explainability due to its black-box nature. This can require decision-makers to make blind leaps of faith based on non-linear model results, especially in healthcare applications. In non-invasive monitoring, patient data from tracking sensors and their predisposing clinical attributes act as input features for predicting future vital signs. Explaining the contributions of various features to the overall output of the monitoring application is critical for a clinician's decision-making. In this study, an Explainable AI for Quantitative analysis (QXAI) framework is proposed with post-hoc model explainability and intrinsic explainability for regression and classification tasks in a supervised learning approach. This was achieved by utilizing the Shapley values concept and incorporating attention mechanisms in deep learning models. We adopted the artificial neural networks (ANN) and attention-based Bidirectional LSTM (BiLSTM) models for the prediction of heart rate and classification of physical activities based on sensor data. The deep learning models achieved state-of-the-art results in both prediction and classification tasks. Global explanation and local explanation were conducted on input data to understand the feature contribution of various patient data. The proposed QXAI framework was evaluated using PPG-DaLiA data to predict heart rate and mobile health (MHEALTH) data to classify physical activities based on sensor data. Monte Carlo approximation was applied to the framework to overcome the time complexity and high computation power requirements required for Shapley value calculations.
</details>
<details>
<summary>摘要</summary>
人工智能技术可以用来分类患者的物理活动和预测生命 Parameters 进行远程患者监测。基于非线性模型的回归分析，如深度学习模型，具有限制可读性的问题，因为它们的黑盒特性可能会导致决策者根据非线性模型的结果进行盲目的信任，� особенpecially in healthcare applications。在非侵入式监测中，患者数据来自跟踪传感器和其相关的临床特征，用作预测未来生命 Parameters 的输入特征。解释不同特征对总输出监测应用的贡献是重要的，以便医生做出决策。在这种研究中，一种可解释AI量化分析（QXAI）框架被提出，该框架包括后续模型解释和内在解释，用于回归和分类任务。这是通过使用Shapley值概念和 incorporating attention mechanisms in deep learning models来实现的。我们采用人工神经网络（ANN）和注意力基于BiLSTM（BiLSTM）模型来预测心率和分类物理活动基于传感器数据。这些深度学习模型在预测和分类任务中达到了状态艺术的结果。全局解释和本地解释在输入数据上进行了全面的解释，以便理解不同患者数据特征的贡献。提议的QXAI框架在使用PPG-DaLiA数据集预测心率和Mobile Health（MHEALTH）数据集分类物理活动基于传感器数据进行了评估。在计算能力和计算复杂性方面，我们使用Monte Carlo Approximation来缓解QXAI框架的时间复杂度和计算能力需求。
</details></li>
</ul>
<hr>
<h2 id="Koopman-Invertible-Autoencoder-Leveraging-Forward-and-Backward-Dynamics-for-Temporal-Modeling"><a href="#Koopman-Invertible-Autoencoder-Leveraging-Forward-and-Backward-Dynamics-for-Temporal-Modeling" class="headerlink" title="Koopman Invertible Autoencoder: Leveraging Forward and Backward Dynamics for Temporal Modeling"></a>Koopman Invertible Autoencoder: Leveraging Forward and Backward Dynamics for Temporal Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10291">http://arxiv.org/abs/2309.10291</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kshitij Tayal, Arvind Renganathan, Rahul Ghosh, Xiaowei Jia, Vipin Kumar<br>for: 这个研究旨在提高机器学习模型的准确长期预测能力，并且解决现有的时间模型（如回传神经网络）在训练数据中的限制，以及它们可能无法学习目标系统的下面特性。methods: 我们提出了一种基于科普曼操作理论的机器学习模型，叫做科普曼倒镜自动encoder（KIA），这个模型可以将系统的前向和反向动态模型在无限维度希尔伯特空间中实现，从而实现更高的预测精度。此外，我们的方法设计了倒镜性，使得这个模型在前向和反向操作中保持逆转性和一致性。results: 我们在摆钟和气候 dataset 上验证了我们的方法，结果显示，对于摆钟dataset，我们的方法可以提高长期预测能力，并且在噪音影响下保持稳定性，而且在气候dataset上，我们的方法也表现出了更好的预测能力。<details>
<summary>Abstract</summary>
Accurate long-term predictions are the foundations for many machine learning applications and decision-making processes. However, building accurate long-term prediction models remains challenging due to the limitations of existing temporal models like recurrent neural networks (RNNs), as they capture only the statistical connections in the training data and may fail to learn the underlying dynamics of the target system. To tackle this challenge, we propose a novel machine learning model based on Koopman operator theory, which we call Koopman Invertible Autoencoders (KIA), that captures the inherent characteristic of the system by modeling both forward and backward dynamics in the infinite-dimensional Hilbert space. This enables us to efficiently learn low-dimensional representations, resulting in more accurate predictions of long-term system behavior. Moreover, our method's invertibility design guarantees reversibility and consistency in both forward and inverse operations. We illustrate the utility of KIA on pendulum and climate datasets, demonstrating 300% improvements in long-term prediction capability for pendulum while maintaining robustness against noise. Additionally, our method excels in long-term climate prediction, further validating our method's effectiveness.
</details>
<details>
<summary>摘要</summary>
准确长期预测是机器学习应用和决策过程的基础。然而，建立准确长期预测模型仍然是一项挑战，因为现有的时间模型如回归神经网络（RNN）只 capture了训练数据中的统计连接，可能无法学习目标系统的下面动态。为解决这个挑战，我们提出了一种基于库曼 оператор理论的新的机器学习模型，我们称之为库曼归一Autoencoder（KIA）。KIA模型能够在无穷维度希尔бер特空间中模型系统的前向和反向动态，从而有效地学习低维度表示，并且可以提高长期系统行为预测的准确性。此外，我们的方法的归一设计 garanties reversibility和一致性在前向和逆向操作中。我们在拖钩和气候数据集上进行了实验，并证明了KIA在长期预测方面的约300%的提升，同时保持了对噪声的Robustness。此外，我们的方法在气候预测方面也具有优异的效果，进一步证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="AstroPortal-An-ontology-repository-concept-for-astronomy-astronautics-and-other-space-topics"><a href="#AstroPortal-An-ontology-repository-concept-for-astronomy-astronautics-and-other-space-topics" class="headerlink" title="AstroPortal: An ontology repository concept for astronomy, astronautics and other space topics"></a>AstroPortal: An ontology repository concept for astronomy, astronautics and other space topics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10288">http://arxiv.org/abs/2309.10288</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rrovetto/astroportal">https://github.com/rrovetto/astroportal</a></li>
<li>paper_authors: Robert J. Rovetto</li>
<li>for: 这篇论文是为了建立一个关于天文学、航天学和其他空间相关领域的 Ontology 仓库而写的。</li>
<li>methods: 论文使用了一种中心化的平台，允许用户搜索、评审和创建 Ontology  для astro-相关话题。</li>
<li>results: 论文提出了一种新的概念，即建立一个专门的 Ontology 仓库，以减少研究时间，并提供一个易用的方式来研究和比较知识组织系统或semantic资源。<details>
<summary>Abstract</summary>
This paper describes a repository for ontologies of astronomy, astronautics, and other space-related topics. It may be called AstroPortal (or SpacePortal), AstroHub (or SpaceHub), etc. The creation of this repository will be applicable to academic, research and other data-intensive sectors. It is relevant for space sciences (including astronomy), Earth science, and astronautics (spaceflight), among other data-intensive disciplines. The repository should provide a centralized platform to search, review and create ontologies for astro-related topics. It thereby can decrease research time, while also providing a user-friendly means to study and compare knowledge organization systems or semantic resources of the target domains. With no apparent repository available on the target domain, this paper also expresses a novel concept.
</details>
<details>
<summary>摘要</summary>
这份论文描述了一个天文、航天和其他空间相关领域 ontology 存储库。它可以被称为 AstroPortal（或 SpacePortal）、AstroHub（或 SpaceHub）等。该存储库的创建将对学术、研究和数据密集领域进行应用。它 relevante 于天文学、地球科学和航天（空间飞行）等数据密集领域。该存储库应该提供一个中央化平台，用于搜索、评审和创建 astro-related ontoologies。因此，它可以降低研究时间，同时提供一个易于使用的方式来研究和比较知识组织系统或semantic 资源的target 领域。由于目标领域没有明显的存储库，这篇论文还描述了一个新的概念。
</details></li>
</ul>
<hr>
<h2 id="FRAMU-Attention-based-Machine-Unlearning-using-Federated-Reinforcement-Learning"><a href="#FRAMU-Attention-based-Machine-Unlearning-using-Federated-Reinforcement-Learning" class="headerlink" title="FRAMU: Attention-based Machine Unlearning using Federated Reinforcement Learning"></a>FRAMU: Attention-based Machine Unlearning using Federated Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10283">http://arxiv.org/abs/2309.10283</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thanveer Shaik, Xiaohui Tao, Lin Li, Haoran Xie, Taotao Cai, Xiaofeng Zhu, Qing Li</li>
<li>for: 这篇论文旨在解决数据隐私问题，提供一种基于联合学习和迪金生学习的机器学习卷积推理框架，以提高模型的准确性和计算效率。</li>
<li>methods: 该框架使用注意力机制、隐私保护技术和优化策略，可以处理不同数据源，包括单模态和多模态数据，并保持模型的准确性和隐私。</li>
<li>results: 在单模态和多模态数据集上进行了实验，发现FRAMUsignificantly outperformed基准模型，并且对模型的演进和优化进行了证明。<details>
<summary>Abstract</summary>
Machine Unlearning is an emerging field that addresses data privacy issues by enabling the removal of private or irrelevant data from the Machine Learning process. Challenges related to privacy and model efficiency arise from the use of outdated, private, and irrelevant data. These issues compromise both the accuracy and the computational efficiency of models in both Machine Learning and Unlearning. To mitigate these challenges, we introduce a novel framework, Attention-based Machine Unlearning using Federated Reinforcement Learning (FRAMU). This framework incorporates adaptive learning mechanisms, privacy preservation techniques, and optimization strategies, making it a well-rounded solution for handling various data sources, either single-modality or multi-modality, while maintaining accuracy and privacy. FRAMU's strength lies in its adaptability to fluctuating data landscapes, its ability to unlearn outdated, private, or irrelevant data, and its support for continual model evolution without compromising privacy. Our experiments, conducted on both single-modality and multi-modality datasets, revealed that FRAMU significantly outperformed baseline models. Additional assessments of convergence behavior and optimization strategies further validate the framework's utility in federated learning applications. Overall, FRAMU advances Machine Unlearning by offering a robust, privacy-preserving solution that optimizes model performance while also addressing key challenges in dynamic data environments.
</details>
<details>
<summary>摘要</summary>
机器无学是一个emerging field，旨在解决数据隐私问题，通过从机器学习过程中除去private或无关的数据。由于使用过时、private或无关的数据，会导致模型精度和计算效率受到挑战。为了解决这些问题，我们提出了一种新的框架：基于联邦反馈学习的注意力机器无学（FRAMU）。这个框架包括适应学习机制、隐私保护技术和优化策略，使其能够处理不同数据源，包括单模态和多模态数据，而不会影响模型的准确性和隐私。FRAMU的优势在于其适应到变化的数据景观、能够快速地忘记过时、private或无关的数据，以及支持不间断的模型演化而不损失隐私。我们在单模态和多模态数据集上进行了实验，发现FRAMU与基准模型相比有显著性能提升。进一步的评估对 convergence 行为和优化策略也证明了框架在联邦学习应用中的实用性。总之，FRAMU 提高了机器无学的可靠性和隐私保护能力，为动态数据环境中的机器学习应用提供了一个robust和隐私保护的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Crowd-Aware-Multi-Agent-Pathfinding-With-Boosted-Curriculum-Reinforcement-Learning"><a href="#Crowd-Aware-Multi-Agent-Pathfinding-With-Boosted-Curriculum-Reinforcement-Learning" class="headerlink" title="Crowd-Aware Multi-Agent Pathfinding With Boosted Curriculum Reinforcement Learning"></a>Crowd-Aware Multi-Agent Pathfinding With Boosted Curriculum Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10275">http://arxiv.org/abs/2309.10275</a></li>
<li>repo_url: None</li>
<li>paper_authors: Phu Pham, Aniket Bera</li>
<li>for: 解决多Agent路径规划（MAPF）在拥挤环境中的困难问题，旨在找到所有Agent在系统中的冲突自由路径。</li>
<li>methods: 我们提出了一种人群意识感知加强的分布式方法（CRAMP），通过强化课程学习引导的训练策略来解决这个问题。</li>
<li>results: 我们在模拟环境中测试了CRAMP，并证明了我们的方法在多种维度上超过了现有的分布式方法的性能。CRAMP提高了解决quality达58% measured in makespan和冲突数量，并提高了成功率达5%。<details>
<summary>Abstract</summary>
Multi-Agent Path Finding (MAPF) in crowded environments presents a challenging problem in motion planning, aiming to find collision-free paths for all agents in the system. MAPF finds a wide range of applications in various domains, including aerial swarms, autonomous warehouse robotics, and self-driving vehicles. The current approaches for MAPF can be broadly categorized into two main categories: centralized and decentralized planning. Centralized planning suffers from the curse of dimensionality and thus does not scale well in large and complex environments. On the other hand, decentralized planning enables agents to engage in real-time path planning within a partially observable environment, demonstrating implicit coordination. However, they suffer from slow convergence and performance degradation in dense environments. In this paper, we introduce CRAMP, a crowd-aware decentralized approach to address this problem by leveraging reinforcement learning guided by a boosted curriculum-based training strategy. We test CRAMP on simulated environments and demonstrate that our method outperforms the state-of-the-art decentralized methods for MAPF on various metrics. CRAMP improves the solution quality up to 58% measured in makespan and collision count, and up to 5% in success rate in comparison to previous methods.
</details>
<details>
<summary>摘要</summary>
多机器人规划（MAPF）在拥挤环境中存在一个复杂的运动规划问题，旨在找到所有机器人的碰撞自由路径。MAPF在各个领域中找到了广泛的应用，包括飞行群体、自主仓库机器人和自动驾驶车辆。当前的MAPF方法可以分为两个主要类别：中央化计划和分布式计划。中央化计划受到维度约束的困扰，因此在大型和复杂的环境中不 scalable。相反，分布式计划使得机器人可以在部分可见环境中实时进行路径规划，表现出隐式协调。然而，它们在拥挤环境中表现缓慢，性能下降。在这篇论文中，我们介绍了一种受欢迎的人群意识 Decentralized Approach（CRAMP），用于解决这个问题，通过利用强化学习指导的推广课程学习策略。我们在模拟环境中测试了CRAMP，并证明我们的方法在多个纪录中性能更好，相比前一代的分布式方法。CRAMP提高了解决方案质量，达到58%的做 span和碰撞计数，以及5%的成功率。
</details></li>
</ul>
<hr>
<h2 id="Using-an-Uncrewed-Surface-Vehicle-to-Create-a-Volumetric-Model-of-Non-Navigable-Rivers-and-Other-Shallow-Bodies-of-Water"><a href="#Using-an-Uncrewed-Surface-Vehicle-to-Create-a-Volumetric-Model-of-Non-Navigable-Rivers-and-Other-Shallow-Bodies-of-Water" class="headerlink" title="Using an Uncrewed Surface Vehicle to Create a Volumetric Model of Non-Navigable Rivers and Other Shallow Bodies of Water"></a>Using an Uncrewed Surface Vehicle to Create a Volumetric Model of Non-Navigable Rivers and Other Shallow Bodies of Water</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10269">http://arxiv.org/abs/2309.10269</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jayesh Tripathi, Robin Murphy</li>
<li>for: 这篇论文旨在提供一种实用的方法，用于使用无人marine surface vehicle（USV）收集和合并浅水体的浅层地图和数字表面地图，以生成一个综合的体积模型。</li>
<li>methods: 本论文使用了Poisson面重建算法来生成下游线上的稀疏声纳深度读数，并使用商业的Structure from Motion（SfM）包装来生成静止银行的稠密上层地图。</li>
<li>results: 该方法可以准确地生成浅水体的体积模型，并且可以填充感器覆盖缺陷，以增强emergency planners对洪水的预测和管理能力。<details>
<summary>Abstract</summary>
Non-navigable rivers and retention ponds play important roles in buffering communities from flooding, yet emergency planners often have no data as to the volume of water that they can carry before flooding the surrounding. This paper describes a practical approach for using an uncrewed marine surface vehicle (USV) to collect and merge bathymetric maps with digital surface maps of the banks of shallow bodies of water into a unified volumetric model. The below-waterline mesh is developed by applying the Poisson surface reconstruction algorithm to the sparse sonar depth readings of the underwater surface. Dense above-waterline meshes of the banks are created using commercial structure from motion (SfM) packages. Merging is challenging for many reasons, the most significant is gaps in sensor coverage, i.e., the USV cannot collect sonar depth data or visually see sandy beaches leading to a bank thus the two meshes may not intersect. The approach is demonstrated on a Hydronalix EMILY USV with a Humminbird single beam echosounder and Teledyne FLIR camera at Lake ESTI at the Texas A&M Engineering Extension Service Disaster City complex.
</details>
<details>
<summary>摘要</summary>
非航行性河流和储水池在抵御洪水方面发挥重要作用，但紧急计划者经常没有洪水量的数据，以便在洪水时进行应急准备。这篇论文描述了一种实用的方法，使用无人海面车 (USV) 收集和融合浸没深度图和数字地面图，形成一个统一的体积模型。在水下的网格是通过将波浪表面重建算法应用于 USV 的罕见声纳深度读数来构建的。陆地上的稠密网格是使用商业的结构从运动 (SfM) 包装来创建的。融合具有许多挑战，最主要的是感器覆盖缺陷，即 USV 不能收集声纳深度数据或视见砂滩，导致两个网格不相交。该方法在得克萨斯A&M工程扩展服务灾难城区使用一只Hydronalix EMILY USV、一个Humminbird单束声纳和Teledyne FLIR Camera进行了示范。
</details></li>
</ul>
<hr>
<h2 id="Correlation-between-morphological-evolution-of-splashing-drop-and-exerted-impact-force-revealed-by-interpretation-of-explainable-artificial-intelligence"><a href="#Correlation-between-morphological-evolution-of-splashing-drop-and-exerted-impact-force-revealed-by-interpretation-of-explainable-artificial-intelligence" class="headerlink" title="Correlation between morphological evolution of splashing drop and exerted impact force revealed by interpretation of explainable artificial intelligence"></a>Correlation between morphological evolution of splashing drop and exerted impact force revealed by interpretation of explainable artificial intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10266">http://arxiv.org/abs/2309.10266</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingzu Yee, Daichi Igarashi, Pradipto, Akinori Yamanaka, Yoshiyuki Tagawa</li>
<li>for: 这个研究探讨了撞击液体在固体表面上的某些特征与正常化的影响力之间的可能的相关性。</li>
<li>methods: 这个研究使用了一种新的特征提取方法和一种可解释的人工智能（XAI）视频分类器来分类撞击和非撞击液体。</li>
<li>results: 研究发现，XAI模型对撞击和非撞击液体的分类值具有不同的重要性，并且这些重要性随时间的演化而变化。具体来说，在撞击时间的各个点上，抽象出的撞击特征的贡献率与正常化影响力的贡献率之间存在紧密的相关性。<details>
<summary>Abstract</summary>
This study reveals a possible correlation between splashing morphology and the normalized impact force exerted by an impacting drop on a solid surface. This finding is obtained from a newly proposed feature extraction method and a subsequent interpretation of the classification of splashing and non-splashing drops performed by an explainable artificial intelligence (XAI) video classifier. Notably, the values of the weight matrix elements of the XAI that correspond to the extracted features are found to change with the temporal evolution of the drop morphology. We compute the rate of change of the contributions of each frame with respect to the classification value of a video as an important index to quantify the contributions of the extracted splashing and non-splashing features at different impact times to the classification of the XAI model. Remarkably, the rate computed for the extracted splashing features is found to closely match the profile of the normalized impact force, where the splashing features are most pronounced immediately after the normalized impact force reaches its peak value. This study has provided an example that clarifies the relationship between the complex morphological evolution of a splashing drop and physical parameters by interpreting the classification of an XAI video classifier.
</details>
<details>
<summary>摘要</summary>
（以下是简化中文版）这个研究发现可能存在液体撞击表面时的液体形态和normalized影响力之间的关系。这一发现来自于一种新提出的特征提取方法和随后的XAI视频分类器的解释。值得注意的是，XAI模型中的weight矩阵元素与提取特征之间的关系发生了时间的变化。我们计算了每帧的贡献的变化率对视频分类值的影响，以便量化不同的撞击时间对XAI模型的分类的贡献。吸引人的是，计算的液体撞击特征的变化率与正常化影响力的profile非常相似，特别是在正常化影响力达到最大值时，液体撞击特征的变化最为明显。这个研究提供了一个示例，从液体撞击的形态进行解释，并且解释了XAI模型的分类结果与物理参数之间的关系。
</details></li>
</ul>
<hr>
<h2 id="LLM-Platform-Security-Applying-a-Systematic-Evaluation-Framework-to-OpenAI’s-ChatGPT-Plugins"><a href="#LLM-Platform-Security-Applying-a-Systematic-Evaluation-Framework-to-OpenAI’s-ChatGPT-Plugins" class="headerlink" title="LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI’s ChatGPT Plugins"></a>LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI’s ChatGPT Plugins</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10254">http://arxiv.org/abs/2309.10254</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/llm-platform-security/chatgpt-plugin-eval">https://github.com/llm-platform-security/chatgpt-plugin-eval</a></li>
<li>paper_authors: Umar Iqbal, Tadayoshi Kohno, Franziska Roesner</li>
<li>for: 本研究旨在提供一个框架，以便LLM平台设计师分析和改善现有和未来插件集成的LLM平台的安全性、隐私性和安全性。</li>
<li>methods: 我们提出了一个攻击分类学，通过询问LLM平台的潜在攻击者如何利用他们的能力和责任来进攻LLM平台。在我们的回归过程中，我们将这个攻击分类学应用于OpenAI的插件生态系。</li>
<li>results: 我们发现了一些插件，它们实际地显示出了我们的攻击分类学中的一些问题类型。我们结论是，这些问题对现有和未来的LLM-基于计算平台的安全性、隐私性和安全性具有新的挑战。<details>
<summary>Abstract</summary>
Large language model (LLM) platforms, such as ChatGPT, have recently begun offering a plugin ecosystem to interface with third-party services on the internet. While these plugins extend the capabilities of LLM platforms, they are developed by arbitrary third parties and thus cannot be implicitly trusted. Plugins also interface with LLM platforms and users using natural language, which can have imprecise interpretations. In this paper, we propose a framework that lays a foundation for LLM platform designers to analyze and improve the security, privacy, and safety of current and future plugin-integrated LLM platforms. Our framework is a formulation of an attack taxonomy that is developed by iteratively exploring how LLM platform stakeholders could leverage their capabilities and responsibilities to mount attacks against each other. As part of our iterative process, we apply our framework in the context of OpenAI's plugin ecosystem. We uncover plugins that concretely demonstrate the potential for the types of issues that we outline in our attack taxonomy. We conclude by discussing novel challenges and by providing recommendations to improve the security, privacy, and safety of present and future LLM-based computing platforms.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="GPTFUZZER-Red-Teaming-Large-Language-Models-with-Auto-Generated-Jailbreak-Prompts"><a href="#GPTFUZZER-Red-Teaming-Large-Language-Models-with-Auto-Generated-Jailbreak-Prompts" class="headerlink" title="GPTFUZZER : Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts"></a>GPTFUZZER : Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10253">http://arxiv.org/abs/2309.10253</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sherdencooper/gptfuzz">https://github.com/sherdencooper/gptfuzz</a></li>
<li>paper_authors: Jiahao Yu, Xingwei Lin, Xinyu Xing</li>
<li>for: 这项研究的目的是提供一种自动生成黑盒攻击模板的攻击探索框架，以提高LLM的安全性。</li>
<li>methods: 这项研究使用了AFL fuzzing框架为基础，并提出了三个关键组成部分：种子选择策略、结构变换和判断模型。</li>
<li>results: 研究发现，使用\fuzzer攻击框架可以在不同的攻击enario下 consistently produce jailbreak templates with high success rate，即使从低质量的种子模板开始。<details>
<summary>Abstract</summary>
Large language models (LLMs) have recently experienced tremendous popularity and are widely used from casual conversations to AI-driven programming. However, despite their considerable success, LLMs are not entirely reliable and can give detailed guidance on how to conduct harmful or illegal activities. While safety measures can reduce the risk of such outputs, adversarial "jailbreak" attacks can still exploit LLMs to produce harmful content. These jailbreak templates are typically manually crafted, making large-scale testing challenging. In this paper, we introduce \fuzzer, a novel black-box jailbreak fuzzing framework inspired by AFL fuzzing framework. Instead of manual engineering, \fuzzer automates the generation of jailbreak templates for red-teaming LLMs. At its core, \fuzzer starts with human-written templates as seeds, then mutates them using mutate operators to produce new templates. We detail three key components of \fuzzer: a seed selection strategy for balancing efficiency and variability, metamorphic relations for creating semantically equivalent or similar sentences, and a judgment model to assess the success of a jailbreak attack. We tested \fuzzer on various commercial and open-source LLMs, such as ChatGPT, LLaMa-2, and Claude2, under diverse attack scenarios. Our results indicate that \fuzzer consistently produces jailbreak templates with a high success rate, even in settings where all human-crafted templates fail. Notably, even starting with suboptimal seed templates, \fuzzer maintains over 90\% attack success rate against ChatGPT and Llama-2 models. We believe \fuzzer will aid researchers and practitioners in assessing LLM robustness and will spur further research into LLM safety.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="On-Explicit-Curvature-Regularization-in-Deep-Generative-Models"><a href="#On-Explicit-Curvature-Regularization-in-Deep-Generative-Models" class="headerlink" title="On Explicit Curvature Regularization in Deep Generative Models"></a>On Explicit Curvature Regularization in Deep Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10237">http://arxiv.org/abs/2309.10237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yonghyeon Lee, Frank Chongwoo Park</li>
<li>for: 这个论文是为了提出一种基于曲率的深度生成模型学习方法的建议。</li>
<li>methods: 这个论文使用了具有征服积分的几何 curvature  measure，并 derivated了一些高效的计算方法。</li>
<li>results: 对于含有噪声的运动捕捉数据， curvature-based 方法表现更高效，内在曲率度量slightly 更为有效。<details>
<summary>Abstract</summary>
We propose a family of curvature-based regularization terms for deep generative model learning. Explicit coordinate-invariant formulas for both intrinsic and extrinsic curvature measures are derived for the case of arbitrary data manifolds embedded in higher-dimensional Euclidean space. Because computing the curvature is a highly computation-intensive process involving the evaluation of second-order derivatives, efficient formulas are derived for approximately evaluating intrinsic and extrinsic curvatures. Comparative studies are conducted that compare the relative efficacy of intrinsic versus extrinsic curvature-based regularization measures, as well as performance comparisons against existing autoencoder training methods. Experiments involving noisy motion capture data confirm that curvature-based methods outperform existing autoencoder regularization methods, with intrinsic curvature measures slightly more effective than extrinsic curvature measures.
</details>
<details>
<summary>摘要</summary>
我们提出了一组基于曲率的调整项，用于深度生成模型的学习。我们 derive了对于任意数据构造的内在和外在曲率度量的明确构成，并且因为计算曲率是高度 computation-intensive 的过程，我们 derivated了高效的曲率度量评估方法。我们进行了比较研究，评估了内在曲率 versus 外在曲率基于的调整项的Relative efficacy，以及与现有 autoencoder 训练方法的比较。实验结果显示，曲率基于的方法在陌生动态捕捉数据上表现较好，内在曲率度量 slightly more effective than 外在曲率度量。
</details></li>
</ul>
<hr>
<h2 id="Drive-as-You-Speak-Enabling-Human-Like-Interaction-with-Large-Language-Models-in-Autonomous-Vehicles"><a href="#Drive-as-You-Speak-Enabling-Human-Like-Interaction-with-Large-Language-Models-in-Autonomous-Vehicles" class="headerlink" title="Drive as You Speak: Enabling Human-Like Interaction with Large Language Models in Autonomous Vehicles"></a>Drive as You Speak: Enabling Human-Like Interaction with Large Language Models in Autonomous Vehicles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10228">http://arxiv.org/abs/2309.10228</a></li>
<li>repo_url: None</li>
<li>paper_authors: Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Ziran Wang</li>
<li>for:  This paper aims to enhance autonomous vehicles’ decision-making processes by integrating Large Language Models (LLMs) to provide personalized assistance, continuous learning, and transparent decision-making.</li>
<li>methods: The proposed framework leverages LLMs’ natural language capabilities and contextual understanding, specialized tools usage, synergizing reasoning, and acting with various modules on autonomous vehicles.</li>
<li>results: The proposed framework has the potential to revolutionize the way autonomous vehicles operate, offering personalized assistance, continuous learning, and transparent decision-making, ultimately contributing to safer and more efficient autonomous driving technologies.<details>
<summary>Abstract</summary>
The future of autonomous vehicles lies in the convergence of human-centric design and advanced AI capabilities. Autonomous vehicles of the future will not only transport passengers but also interact and adapt to their desires, making the journey comfortable, efficient, and pleasant. In this paper, we present a novel framework that leverages Large Language Models (LLMs) to enhance autonomous vehicles' decision-making processes. By integrating LLMs' natural language capabilities and contextual understanding, specialized tools usage, synergizing reasoning, and acting with various modules on autonomous vehicles, this framework aims to seamlessly integrate the advanced language and reasoning capabilities of LLMs into autonomous vehicles. The proposed framework holds the potential to revolutionize the way autonomous vehicles operate, offering personalized assistance, continuous learning, and transparent decision-making, ultimately contributing to safer and more efficient autonomous driving technologies.
</details>
<details>
<summary>摘要</summary>
自动驾驶未来在人类中心设计和高级人工智能技术的融合中实现。未来的自动驾驶车不仅会运送乘客，还会与乘客互动，适应其愿望，使旅行更舒适、更高效、更愉悦。在这篇论文中，我们提出了一种新的框架，通过将大型自然语言模型（LLM）的自然语言能力和上下文理解 integrate into autonomous vehicles的决策过程中。通过特殊工具的使用、同步理解、合并推理和行动等模块的结合，这个框架计划将 LLM 的高级语言和推理能力融合到自动驾驶车中。该框架的提议具有改变自动驾驶车的运行方式，提供个性化协助、不断学习和透明决策，从而为更安全和更高效的自动驾驶技术做出贡献。
</details></li>
</ul>
<hr>
<h2 id="Multi-level-feature-fusion-network-combining-attention-mechanisms-for-polyp-segmentation"><a href="#Multi-level-feature-fusion-network-combining-attention-mechanisms-for-polyp-segmentation" class="headerlink" title="Multi-level feature fusion network combining attention mechanisms for polyp segmentation"></a>Multi-level feature fusion network combining attention mechanisms for polyp segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10219">http://arxiv.org/abs/2309.10219</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junzhuo Liu, Qiaosong Chen, Ye Zhang, Zhixiang Wang, Deng Xin, Jin Wang</li>
<li>For: 这篇论文的目的是提出一种新的自动识别肿瘤技术，以提高医疗诊断的效率和准确性，并减少抑癌病的风险。* Methods: 本论文提出的新技术称为MLFF-Net，它利用多层次特征融合和注意机制来优化肿瘤分类。具体来说，MLFF-Net包括三个模组：多尺度注意模组（MAM）、高级特征增强模组（HFEM）和全球注意模组（GAM）。* Results: 在五个公共数据集上进行实验，MLFF-Net的提案方法不��ley且具有比现有技术更高的准确性和通用能力。<details>
<summary>Abstract</summary>
Clinically, automated polyp segmentation techniques have the potential to significantly improve the efficiency and accuracy of medical diagnosis, thereby reducing the risk of colorectal cancer in patients. Unfortunately, existing methods suffer from two significant weaknesses that can impact the accuracy of segmentation. Firstly, features extracted by encoders are not adequately filtered and utilized. Secondly, semantic conflicts and information redundancy caused by feature fusion are not attended to. To overcome these limitations, we propose a novel approach for polyp segmentation, named MLFF-Net, which leverages multi-level feature fusion and attention mechanisms. Specifically, MLFF-Net comprises three modules: Multi-scale Attention Module (MAM), High-level Feature Enhancement Module (HFEM), and Global Attention Module (GAM). Among these, MAM is used to extract multi-scale information and polyp details from the shallow output of the encoder. In HFEM, the deep features of the encoders complement each other by aggregation. Meanwhile, the attention mechanism redistributes the weight of the aggregated features, weakening the conflicting redundant parts and highlighting the information useful to the task. GAM combines features from the encoder and decoder features, as well as computes global dependencies to prevent receptive field locality. Experimental results on five public datasets show that the proposed method not only can segment multiple types of polyps but also has advantages over current state-of-the-art methods in both accuracy and generalization ability.
</details>
<details>
<summary>摘要</summary>
临床上，自动化肿体分割技术具有提高医学诊断效率和准确率的潜在优势，从而降低患者抗性肿瘤的风险。然而，现有方法受到两大缺陷，这两个缺陷可能会影响分割的准确性。首先，编码器提取的特征并不充分筛选和利用。其次，由特征融合引起的semantic conflict和信息重复不被注意。为了解决这些限制，我们提出了一种新的肿体分割方法，名为MLFF-Net，它利用多级特征融合和注意机制。具体来说，MLFF-Net包括三个模块：多级注意模块（MAM）、高级特征增强模块（HFEM）和全局注意模块（GAM）。其中，MAM用于从编码器的浅输出中提取多级信息和肿体细节。在HFEM中，编码器的深特征相互补充，并通过注意机制重新分配这些特征的权重，弱化冲突的重复部分，高亮任务所需的信息。GAM将编码器和解码器特征相结合，并计算全局依赖关系，以避免感知范围地局部性。我们在五个公共数据集上进行了实验，结果表明，提议的方法不仅可以分割多种肿体，而且在准确率和普适性能方面也有优势于当前state-of-the-art方法。
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Study-of-Attention-Networks-for-Semantic-Segmentation"><a href="#An-Empirical-Study-of-Attention-Networks-for-Semantic-Segmentation" class="headerlink" title="An Empirical Study of Attention Networks for Semantic Segmentation"></a>An Empirical Study of Attention Networks for Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10217">http://arxiv.org/abs/2309.10217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Guo, Hongbiao Si, Guilin Jiang, Wei Zhang, Zhiyan Liu, Xuanyi Zhu, Xulong Zhang, Yang Liu</li>
<li>for: 本文主要研究 semantic segmentation 领域中的注意网络，探讨其计算复杂度和精度在不同类别上的表现，以及适用场景和建议。</li>
<li>methods: 本文使用了多种注意网络，包括 decoder 和 self-attention 网络，进行对比研究。</li>
<li>results: 研究发现，decoder 网络在某些场景下表现较好，而 self-attention 网络在其他场景下表现较好。此外，研究还发现了一些注意网络的缺点和未来发展方向。<details>
<summary>Abstract</summary>
Semantic segmentation is a vital problem in computer vision. Recently, a common solution to semantic segmentation is the end-to-end convolution neural network, which is much more accurate than traditional methods.Recently, the decoders based on attention achieve state-of-the-art (SOTA) performance on various datasets. But these networks always are compared with the mIoU of previous SOTA networks to prove their superiority and ignore their characteristics without considering the computation complexity and precision in various categories, which is essential for engineering applications. Besides, the methods to analyze the FLOPs and memory are not consistent between different networks, which makes the comparison hard to be utilized. What's more, various methods utilize attention in semantic segmentation, but the conclusion of these methods is lacking. This paper first conducts experiments to analyze their computation complexity and compare their performance. Then it summarizes suitable scenes for these networks and concludes key points that should be concerned when constructing an attention network. Last it points out some future directions of the attention network.
</details>
<details>
<summary>摘要</summary>
semantic segmentation 是计算机视觉中的一个关键问题。最近，一种常见的解决方案是将端到端 convolutional neural network（CNN）作为解决方案，这种方法比传统方法更为精准。 Recently, attention 基于的解码器在多个数据集上达到了状态的极点性能（SOTA）。但这些网络总是与之前的 SOTA 网络的 mIoU 进行比较，忽略它们的特点而不考虑不同类别的计算复杂度和精度，这是工程应用中必须考虑的。另外，不同网络之间的 FLOPs 和内存分析方法不一致，使得比较变得困难。此外，各种方法在 semantic segmentation 中使用 attention，但这些方法的结论缺乏。这篇论文首先进行了计算复杂度的分析和比较性能。然后总结了适合这些网络的场景，并指出了在建立注意力网络时需要关注的关键点。最后，它指出了未来注意力网络的发展方向。
</details></li>
</ul>
<hr>
<h2 id="Safe-POMDP-Online-Planning-via-Shielding"><a href="#Safe-POMDP-Online-Planning-via-Shielding" class="headerlink" title="Safe POMDP Online Planning via Shielding"></a>Safe POMDP Online Planning via Shielding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10216">http://arxiv.org/abs/2309.10216</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shili Sheng, David Parker, Lu Feng</li>
<li>for: 这个研究旨在实现安全性的延伸POMDP在线规划，以满足安全需求。</li>
<li>methods: 研究使用了防护盾来限制不安全的动作，并将其与POMCP算法结合以确保安全性。</li>
<li>results: 实验结果显示，提案的防护盾方法可以成功保证安全性，并且对大型POMDP进行在线规划并不会对 runtime 有显著影响。<details>
<summary>Abstract</summary>
Partially observable Markov decision processes (POMDPs) have been widely used in many robotic applications for sequential decision-making under uncertainty. POMDP online planning algorithms such as Partially Observable Monte-Carlo Planning (POMCP) can solve very large POMDPs with the goal of maximizing the expected return. But the resulting policies cannot provide safety guarantees that are imperative for real-world safety-critical tasks (e.g., autonomous driving). In this work, we consider safety requirements represented as almost-sure reach-avoid specifications (i.e., the probability to reach a set of goal states is one and the probability to reach a set of unsafe states is zero). We compute shields that restrict unsafe actions violating almost-sure reach-avoid specifications. We then integrate these shields into the POMCP algorithm for safe POMDP online planning. We propose four distinct shielding methods, differing in how the shields are computed and integrated, including factored variants designed to improve scalability. Experimental results on a set of benchmark domains demonstrate that the proposed shielding methods successfully guarantee safety (unlike the baseline POMCP without shielding) on large POMDPs, with negligible impact on the runtime for online planning.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/19/cs.AI_2023_09_19/" data-id="clpztdnbo0049es8878wp5wek" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_09_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/19/cs.CL_2023_09_19/" class="article-date">
  <time datetime="2023-09-19T11:00:00.000Z" itemprop="datePublished">2023-09-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/19/cs.CL_2023_09_19/">cs.CL - 2023-09-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="MBR-and-QE-Finetuning-Training-time-Distillation-of-the-Best-and-Most-Expensive-Decoding-Methods"><a href="#MBR-and-QE-Finetuning-Training-time-Distillation-of-the-Best-and-Most-Expensive-Decoding-Methods" class="headerlink" title="MBR and QE Finetuning: Training-time Distillation of the Best and Most Expensive Decoding Methods"></a>MBR and QE Finetuning: Training-time Distillation of the Best and Most Expensive Decoding Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10966">http://arxiv.org/abs/2309.10966</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mara Finkelstein, Markus Freitag</li>
<li>for: 提高NLG任务中模型的质量和效率</li>
<li>methods: 使用MBRfinetuning和QEfinetuning方法，在训练时使用高质量的核心模型，在推断时使用高效的推断算法</li>
<li>results: 在Neural Machine Translation任务上，使用这些finetuning方法可以大幅提高模型的质量和效率，并且在使用外部LLM作为教师模型时，还能超越使用人工生成的参考数据<details>
<summary>Abstract</summary>
Recent research in decoding methods for Natural Language Generation (NLG) tasks has shown that the traditional beam search and greedy decoding algorithms are not optimal, because model probabilities do not always align with human preferences. Stronger decoding methods, including Quality Estimation (QE) reranking and Minimum Bayes' Risk (MBR) decoding, have since been proposed to mitigate the model-perplexity-vs-quality mismatch. While these decoding methods achieve state-of-the-art performance, they are prohibitively expensive to compute. In this work, we propose MBR finetuning and QE finetuning which distill the quality gains from these decoding methods at training time, while using an efficient decoding algorithm at inference time. Using the canonical NLG task of Neural Machine Translation (NMT), we show that even with self-training, these finetuning methods significantly outperform the base model. Moreover, when using an external LLM as a teacher model, these finetuning methods outperform finetuning on human-generated references. These findings suggest new ways to leverage monolingual data to achieve improvements in model quality that are on par with, or even exceed, improvements from human-curated data, while maintaining maximum efficiency during decoding.
</details>
<details>
<summary>摘要</summary>
Note:* "排序" (bǎo xiǎng) instead of "sorting"* "搜索" (sōu sòu) instead of "search"* "评估" (píng gòu) instead of "assessment"* "参考" (xiǎng gǎng) instead of "reference"* "模型" (módel) instead of "model"* "语言生成" (yǔ yán shēng chéng) instead of "Natural Language Generation"* "神经机器翻译" (shén qiān jī qì zhōng yì) instead of "Neural Machine Translation"
</details></li>
</ul>
<hr>
<h2 id="In-Context-Learning-for-Text-Classification-with-Many-Labels"><a href="#In-Context-Learning-for-Text-Classification-with-Many-Labels" class="headerlink" title="In-Context Learning for Text Classification with Many Labels"></a>In-Context Learning for Text Classification with Many Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10954">http://arxiv.org/abs/2309.10954</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dia2018/What-is-the-Difference-Between-AI-and-Machine-Learning">https://github.com/dia2018/What-is-the-Difference-Between-AI-and-Machine-Learning</a></li>
<li>paper_authors: Aristides Milios, Siva Reddy, Dzmitry Bahdanau</li>
<li>for: 本研究使用大型自然语言模型进行含义学习（ICL）任务，尤其是多个标签任务，因为限定的上下文窗口尺度使得不能填充足量的示例。</li>
<li>methods: 我们使用预训练的稠密检索模型，只给模型每个推理调用一个部分视图全标签空间。我们使用最新的开源LLMs（OPT、LLaMA）进行测试，并在几个常见的意图分类数据集上设置新的状态码表现。</li>
<li>results: 我们发现，随着不同的模型缩度和数量，更大的模型可以更好地利用更大的上下文长度进行ICL。我们运行了多个简化，分析了模型对：a) 输入中的相似示例和当前输入之间的相似度，b) 类名的Semantic内容，c) 示例和标签之间的正确匹配。我们发现，这三种因素在不同的领域中具有不同的重要性。<details>
<summary>Abstract</summary>
In-context learning (ICL) using large language models for tasks with many labels is challenging due to the limited context window, which makes it difficult to fit a sufficient number of examples in the prompt. In this paper, we use a pre-trained dense retrieval model to bypass this limitation, giving the model only a partial view of the full label space for each inference call. Testing with recent open-source LLMs (OPT, LLaMA), we set new state of the art performance in few-shot settings for three common intent classification datasets, with no finetuning. We also surpass fine-tuned performance on fine-grained sentiment classification in certain cases. We analyze the performance across number of in-context examples and different model scales, showing that larger models are necessary to effectively and consistently make use of larger context lengths for ICL. By running several ablations, we analyze the model's use of: a) the similarity of the in-context examples to the current input, b) the semantic content of the class names, and c) the correct correspondence between examples and labels. We demonstrate that all three are needed to varying degrees depending on the domain, contrary to certain recent works.
</details>
<details>
<summary>摘要</summary>
内容学习（ICL）使用大型语言模型进行多个标签任务是具有限制的上下文窗口，即对每次推寄都只能提供有限的示例。在这篇论文中，我们使用预训练的稠密检索模型，以快速地跳过这个限制，并只给模型提供部分视图全个标签空间。通过测试最新的开源LLMs（OPT、LLaMA），我们在少量示例设置下设置了新的国际标准性能，无需训练。我们还超过了训练后的性能在细化情感分类中，在某些情况下。我们分析了不同的示例数量和模型缩放大小对性能的影响，发现大型模型是需要充分利用更大的上下文长度进行ICL。通过多个缺省分析，我们分析了模型在不同的领域中使用：a）相关的示例和当前输入之间的相似性，b）类名中的semantic内容，以及c）正确地将示例和标签相匹配。我们发现，这三者在不同的领域中均需要不同的程度，与某些最近的工作不同。
</details></li>
</ul>
<hr>
<h2 id="A-Family-of-Pretrained-Transformer-Language-Models-for-Russian"><a href="#A-Family-of-Pretrained-Transformer-Language-Models-for-Russian" class="headerlink" title="A Family of Pretrained Transformer Language Models for Russian"></a>A Family of Pretrained Transformer Language Models for Russian</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10931">http://arxiv.org/abs/2309.10931</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dmitry Zmitrovich, Alexander Abramov, Andrey Kalmykov, Maria Tikhonova, Ekaterina Taktasheva, Danil Astafurov, Mark Baushenko, Artem Snegirev, Tatiana Shavrina, Sergey Markov, Vladislav Mikhailov, Alena Fenogenova</li>
<li>for: 本研究旨在开发特性化的Transformer语言模型，用于俄语自然语言理解和生成。</li>
<li>methods: 本文使用encoder（ruBERT、ruRoBERTa、ruELECTRA）、decoder（ruGPT-3）和encoder-decoder（ruT5、FRED-T5）多种Transformer模型，并对其进行预训练和测试。</li>
<li>results: 研究人员通过对俄语自然语言理解和生成数据集和标准做测试，发现这些特性化Transformer模型具有良好的普适能力和生成能力。<details>
<summary>Abstract</summary>
Nowadays, Transformer language models (LMs) represent a fundamental component of the NLP research methodologies and applications. However, the development of such models specifically for the Russian language has received little attention. This paper presents a collection of 13 Russian Transformer LMs based on the encoder (ruBERT, ruRoBERTa, ruELECTRA), decoder (ruGPT-3), and encoder-decoder (ruT5, FRED-T5) models in multiple sizes. Access to these models is readily available via the HuggingFace platform. We provide a report of the model architecture design and pretraining, and the results of evaluating their generalization abilities on Russian natural language understanding and generation datasets and benchmarks. By pretraining and releasing these specialized Transformer LMs, we hope to broaden the scope of the NLP research directions and enable the development of industrial solutions for the Russian language.
</details>
<details>
<summary>摘要</summary>
现在， transformer 语言模型（LMs）成为了自然语言处理（NLP）研究方法和应用的基本组成部分。然而，为俄语语言的特有Transformer LMs的开发受到了相对少的关注。本文介绍了13种俄语 transformer LMs，包括encoder（ruBERT、ruRoBERTa、ruELECTRA）、decoder（ruGPT-3）和encoder-decoder（ruT5、FRED-T5）模型，以及这些模型的训练和预训练方法。通过这些特化的Transformer LMs，我们希望拓宽NLP研究方向和提供俄语语言industrial解决方案。这些模型通过HuggingFace平台可以访问。我们还提供了模型体系设计和预训练方法的报告，以及在俄语自然语言理解和生成数据集和benchmark上模型的一般化能力的评价结果。
</details></li>
</ul>
<hr>
<h2 id="Specializing-Small-Language-Models-towards-Complex-Style-Transfer-via-Latent-Attribute-Pre-Training"><a href="#Specializing-Small-Language-Models-towards-Complex-Style-Transfer-via-Latent-Attribute-Pre-Training" class="headerlink" title="Specializing Small Language Models towards Complex Style Transfer via Latent Attribute Pre-Training"></a>Specializing Small Language Models towards Complex Style Transfer via Latent Attribute Pre-Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10929">http://arxiv.org/abs/2309.10929</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ruiqixu37/BTTS_ECAI2023">https://github.com/ruiqixu37/BTTS_ECAI2023</a></li>
<li>paper_authors: Ruiqi Xu, Yongfeng Huang, Xin Chen, Lin Zhang</li>
<li>for: 这项研究旨在介绍复杂文本风格传递任务，并基于两个广泛适用的场景构建了复杂文本数据集。</li>
<li>methods: 我们使用了小型模型（ Less than T5-3B）和隐式风格预训练through contrastive learning来解决大型模型（LLM）的数据隐私问题、网络不稳定和高部署成本。</li>
<li>results: 我们的方法比现有方法更有效，可以在几个shot中完成文本风格传递任务，并且可以自动评估文本生成质量基于人工评估使用ChatGPT。<details>
<summary>Abstract</summary>
In this work, we introduce the concept of complex text style transfer tasks, and constructed complex text datasets based on two widely applicable scenarios. Our dataset is the first large-scale data set of its kind, with 700 rephrased sentences and 1,000 sentences from the game Genshin Impact. While large language models (LLM) have shown promise in complex text style transfer, they have drawbacks such as data privacy concerns, network instability, and high deployment costs. To address these issues, we explore the effectiveness of small models (less than T5-3B) with implicit style pre-training through contrastive learning. We also propose a method for automated evaluation of text generation quality based on alignment with human evaluations using ChatGPT. Finally, we compare our approach with existing methods and show that our model achieves state-of-art performances of few-shot text style transfer models.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们介绍了复杂文本风格传递任务的概念，并基于两个广泛适用的情况构建了复杂文本数据集。我们的数据集是首个类似类型的大规模数据集，包含700个重叠句子和1,000个《神韵碰》游戏中的句子。虽然大型语言模型（LLM）在复杂文本风格传递方面表现出了承诺，但它们具有数据隐私问题、网络不稳定和高部署成本的缺点。为了解决这些问题，我们研究了小型模型（ Less than T5-3B）的隐式风格预训练through contrastive learning的效果。我们还提出了一种自动评估文本生成质量的方法，基于人类评估和ChatGPT的匹配。最后，我们与现有方法进行比较，并证明我们的模型在几架text风格传递模型中达到了状态之最。
</details></li>
</ul>
<hr>
<h2 id="Semi-Autoregressive-Streaming-ASR-With-Label-Context"><a href="#Semi-Autoregressive-Streaming-ASR-With-Label-Context" class="headerlink" title="Semi-Autoregressive Streaming ASR With Label Context"></a>Semi-Autoregressive Streaming ASR With Label Context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10926">http://arxiv.org/abs/2309.10926</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddhant Arora, George Saon, Shinji Watanabe, Brian Kingsbury</li>
<li>for: 这个论文是为了提高流式自动语音识别（ASR）模型的准确率和延迟时间而设计的。</li>
<li>methods: 这个论文使用了一种名为“半自动生成”的ASR模型，该模型将以前块中的标签作为额外Context，使用语言模型（LM）子网络来提高流式ASR的准确率。它还提出了一种新的滥货解码算法，可以在块边界附近减少插入和删除错误，而不是 significatively增加推理时间。</li>
<li>results: 实验表明，我们的方法可以与现有的流式非 autoregressive（NAR）模型相比，提高流式ASR的准确率。具体来说，在Tedlium2上，我们的方法提高了19%的相对准确率；在Librispeech-100的清洁&#x2F;其他测试集上，提高了16%&#x2F;8%的相对准确率；在Switchboard（SWB）&#x2F; Callhome（CH）测试集上，提高了19%&#x2F;8%的相对准确率。此外，我们的方法可以更好地利用外部文本数据进行预训练LM子网络，进一步提高流式ASR的准确率。<details>
<summary>Abstract</summary>
Non-autoregressive (NAR) modeling has gained significant interest in speech processing since these models achieve dramatically lower inference time than autoregressive (AR) models while also achieving good transcription accuracy. Since NAR automatic speech recognition (ASR) models must wait for the completion of the entire utterance before processing, some works explore streaming NAR models based on blockwise attention for low-latency applications. However, streaming NAR models significantly lag in accuracy compared to streaming AR and non-streaming NAR models. To address this, we propose a streaming "semi-autoregressive" ASR model that incorporates the labels emitted in previous blocks as additional context using a Language Model (LM) subnetwork. We also introduce a novel greedy decoding algorithm that addresses insertion and deletion errors near block boundaries while not significantly increasing the inference time. Experiments show that our method outperforms the existing streaming NAR model by 19% relative on Tedlium2, 16%/8% on Librispeech-100 clean/other test sets, and 19%/8% on the Switchboard(SWB) / Callhome(CH) test sets. It also reduced the accuracy gap with streaming AR and non-streaming NAR models while achieving 2.5x lower latency. We also demonstrate that our approach can effectively utilize external text data to pre-train the LM subnetwork to further improve streaming ASR accuracy.
</details>
<details>
<summary>摘要</summary>
非自适应（NAR）模型在语音处理领域已经吸引了广泛的关注，因为这些模型在推理时间方面可以达到AR模型的多倍速度，而且也可以达到良好的识别精度。然而，NAR自动语音识别（ASR）模型必须等待整个句子的完成才能进行处理，因此一些研究者开发了基于块级注意力的流式NAR模型，以满足低延迟应用场景。然而，流式NAR模型与流式AR和非流式NAR模型的准确率存在明显的差距。为了解决这个问题，我们提出了一种流式"半自适应" ASR模型，该模型利用在前一个块中生成的标签作为额外Context使用语言模型（LM）子网络。我们还提出了一种新的恰好解oding算法，该算法可以在块边界附近快速地修复插入和删除错误，而不是增加推理时间。实验显示，我们的方法在Tedlium2、Librispeech-100清洁/其他测试集和Switchboard（SWB）/ Callhome（CH）测试集上比既有的流式NAR模型提高19%的相对性能，同时也降低了与流式AR和非流式NAR模型之间的准确率差距。此外，我们还证明了我们的方法可以有效地利用外部文本数据来预训练LM子网络，以进一步提高流式ASR准确率。
</details></li>
</ul>
<hr>
<h2 id="Semi-automatic-staging-area-for-high-quality-structured-data-extraction-from-scientific-literature"><a href="#Semi-automatic-staging-area-for-high-quality-structured-data-extraction-from-scientific-literature" class="headerlink" title="Semi-automatic staging area for high-quality structured data extraction from scientific literature"></a>Semi-automatic staging area for high-quality structured data extraction from scientific literature</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10923">http://arxiv.org/abs/2309.10923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Foppiano, Tomoya Mato, Kensei Terashima, Pedro Ortiz Suarez, Taku Tou, Chikako Sakai, Wei-Sheng Wang, Toshiyuki Amagasa, Yoshihiko Takano, Masashi Ishii</li>
<li>for: 提高 SuperCon 中新型超导体实验数据的更新效率，同时维持或提高数据质量。</li>
<li>methods: 使用自动和手动工作流程在抽取的数据库中实现半自动的stage区。使用异常检测自动过程预先审核收集的数据，并让用户通过专门设计的用户界面对原PDF文档进行数据验证。收集修复后的记录，并将其作为机器学习模型的训练数据使用。</li>
<li>results: 评估实验表明，我们的stage区可以显著提高审核质量。与传统手动方法（读取PDF文档并记录信息在Excel文档中）相比，使用界面提高精度和准确率分别提高6%和50%，平均提高40%的F1分数。<details>
<summary>Abstract</summary>
In this study, we propose a staging area for ingesting new superconductors' experimental data in SuperCon that is machine-collected from scientific articles. Our objective is to enhance the efficiency of updating SuperCon while maintaining or enhancing the data quality. We present a semi-automatic staging area driven by a workflow combining automatic and manual processes on the extracted database. An anomaly detection automatic process aims to pre-screen the collected data. Users can then manually correct any errors through a user interface tailored to simplify the data verification on the original PDF documents. Additionally, when a record is corrected, its raw data is collected and utilised to improve machine learning models as training data. Evaluation experiments demonstrate that our staging area significantly improves curation quality. We compare the interface with the traditional manual approach of reading PDF documents and recording information in an Excel document. Using the interface boosts the precision and recall by 6% and 50%, respectively to an average increase of 40% in F1-score.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们提出了一个用于新超导材料实验数据的投入区域，这个区域使用机器自动从科学文献中收集数据。我们的目标是提高超кон的更新效率，同时保持或提高数据质量。我们提出了一种半自动的投入区域，该区域采用工作流程结合自动和手动过程来处理提取的数据库。一个异常检测自动过程用于预先屏选收集的数据。用户可以通过一个专门设计的用户界面来手动修正任何错误。此外，当记录被修正时，其原始数据会被收集并用于改进机器学习模型的训练数据。我们的评估实验表明，我们的投入区域可以显著提高筛选质量。我们与传统的手动方法相比，使用界面可以提高准确率和敏感度分别提高6%和50%，平均提高40%的F1分数。
</details></li>
</ul>
<hr>
<h2 id="What-Learned-Representations-and-Influence-Functions-Can-Tell-Us-About-Adversarial-Examples"><a href="#What-Learned-Representations-and-Influence-Functions-Can-Tell-Us-About-Adversarial-Examples" class="headerlink" title="What Learned Representations and Influence Functions Can Tell Us About Adversarial Examples"></a>What Learned Representations and Influence Functions Can Tell Us About Adversarial Examples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10916">http://arxiv.org/abs/2309.10916</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sjabin/nnif">https://github.com/sjabin/nnif</a></li>
<li>paper_authors: Shakila Mahjabin Tonni, Mark Dras</li>
<li>for: 这篇论文主要是为了研究在自然语言处理（NLP）中的对抗例（adversarial examples）的检测方法。</li>
<li>methods: 这篇论文使用了两种方法来检测对抗例：一种是基于最近邻居和影响函数，另一种是基于马ха拉欧斯距离。</li>
<li>results: 研究发现，使用基于最近邻居和影响函数的方法可以制定出state-of-the-art的检测器，而且这种方法还提供了对于NLP任务的对抗例subspace的新的理解和对比。<details>
<summary>Abstract</summary>
Adversarial examples, deliberately crafted using small perturbations to fool deep neural networks, were first studied in image processing and more recently in NLP. While approaches to detecting adversarial examples in NLP have largely relied on search over input perturbations, image processing has seen a range of techniques that aim to characterise adversarial subspaces over the learned representations.   In this paper, we adapt two such approaches to NLP, one based on nearest neighbors and influence functions and one on Mahalanobis distances. The former in particular produces a state-of-the-art detector when compared against several strong baselines; moreover, the novel use of influence functions provides insight into how the nature of adversarial example subspaces in NLP relate to those in image processing, and also how they differ depending on the kind of NLP task.
</details>
<details>
<summary>摘要</summary>
adversarial examples, 通过小变化而被意外地骗响深度神经网络，在图像处理领域首先被研究，然后在自然语言处理（NLP）中被研究。在NLP中，检测 adversarial examples 的方法主要基于输入变换的搜索，而图像处理领域则有许多技术来描述恶作剂的表示空间。  在这篇论文中，我们采用了两种方法来检测 adversarial examples，一种是基于最近邻居和影响函数，另一种是基于 Mahalanobis 距离。前者在比较多个强基准下表现出状态顶峰的检测器，而且使用影响函数提供了关于恶作剂表示空间在 NLP 和图像处理之间的相似性，以及哪些因素使得恶作剂表示空间在不同的 NLP 任务中有所不同。
</details></li>
</ul>
<hr>
<h2 id="RedPenNet-for-Grammatical-Error-Correction-Outputs-to-Tokens-Attentions-to-Spans"><a href="#RedPenNet-for-Grammatical-Error-Correction-Outputs-to-Tokens-Attentions-to-Spans" class="headerlink" title="RedPenNet for Grammatical Error Correction: Outputs to Tokens, Attentions to Spans"></a>RedPenNet for Grammatical Error Correction: Outputs to Tokens, Attentions to Spans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10898">http://arxiv.org/abs/2309.10898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bohdan Didenko, Andrii Sameliuk</li>
<li>For: This paper is written for the UNLP 2023 workshop, specifically for the Shared Task in Grammatical Error Correction (GEC) for Ukrainian.* Methods: The paper uses a RedPenNet approach to address text editing tasks, which combines sequence-to-sequence and sequence tagging techniques.* Results: The paper achieves $F_{0.5}$ scores of 77.60 on the BEA-2019 (test) and 67.71 on the UAGEC+Fluency (test) benchmarks, which are considered state-of-the-art results.Here is the simplified Chinese text for the three key points:* For: 这篇论文是为UNLP 2023 工作坊写的，特意是为 grammatical error correction (GEC) 的 Ukrainian 语言共同任务。* Methods: 这篇论文使用 RedPenNet 方法来处理文本编辑任务，这种方法结合了 sequence-to-sequence 和 sequence tagging 技术。* Results: 这篇论文在 BEA-2019 测试集上 achieve $F_{0.5}$ 分数为 77.60，并在 UAGEC+Fluency 测试集上 achieve 67.71 分数，这些结果被视为当前领域的 state-of-the-art 成果。<details>
<summary>Abstract</summary>
The text editing tasks, including sentence fusion, sentence splitting and rephrasing, text simplification, and Grammatical Error Correction (GEC), share a common trait of dealing with highly similar input and output sequences. This area of research lies at the intersection of two well-established fields: (i) fully autoregressive sequence-to-sequence approaches commonly used in tasks like Neural Machine Translation (NMT) and (ii) sequence tagging techniques commonly used to address tasks such as Part-of-speech tagging, Named-entity recognition (NER), and similar. In the pursuit of a balanced architecture, researchers have come up with numerous imaginative and unconventional solutions, which we're discussing in the Related Works section. Our approach to addressing text editing tasks is called RedPenNet and is aimed at reducing architectural and parametric redundancies presented in specific Sequence-To-Edits models, preserving their semi-autoregressive advantages. Our models achieve $F_{0.5}$ scores of 77.60 on the BEA-2019 (test), which can be considered as state-of-the-art the only exception for system combination and 67.71 on the UAGEC+Fluency (test) benchmarks.   This research is being conducted in the context of the UNLP 2023 workshop, where it was presented as a paper as a paper for the Shared Task in Grammatical Error Correction (GEC) for Ukrainian. This study aims to apply the RedPenNet approach to address the GEC problem in the Ukrainian language.
</details>
<details>
<summary>摘要</summary>
文本编辑任务，包括句子合并、句子分割、重写和语法错误修复（GEC），都与高度相似的输入和输出序列交互。这一领域的研究受到两个已有的领域的影响：一是完全自动化的序列到序列方法，通常用于语义翻译（NMT）类任务；二是序列标记技术，通常用于处理如部分词类标注（POS）、命名实体识别（NER）和类似任务。为了实现平衡的架构，研究人员提出了许多创新的解决方案，我们在相关工作部分进行讨论。我们的方法是called RedPenNet，旨在降低特定Sequence-To-Edits模型中的 arquitectónico和参数 redundancy，保留它们的半自动化优势。我们的模型在BEA-2019（测试）上获得了77.60的F0.5分，可以 considere为领先水平，只有系统组合为例外。此外，在UAGEC+ Fluency（测试）标准下，我们的模型获得了67.71的F0.5分。这项研究在UNLP 2023 会议上进行了展示，作为grammatical Error Correction（GEC） Shared Task for Ukrainian 的论文。本研究的目标是通过应用RedPenNet方法，解决 Ukrainian 语言中的GEC问题。
</details></li>
</ul>
<hr>
<h2 id="Natural-Language-Embedded-Programs-for-Hybrid-Language-Symbolic-Reasoning"><a href="#Natural-Language-Embedded-Programs-for-Hybrid-Language-Symbolic-Reasoning" class="headerlink" title="Natural Language Embedded Programs for Hybrid Language Symbolic Reasoning"></a>Natural Language Embedded Programs for Hybrid Language Symbolic Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10814">http://arxiv.org/abs/2309.10814</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luohongyin/langcode">https://github.com/luohongyin/langcode</a></li>
<li>paper_authors: Tianhua Zhang, Jiaxin Ge, Hongyin Luo, Yung-Sung Chuang, Mingye Gao, Yuan Gong, Xixin Wu, Yoon Kim, Helen Meng, James Glass</li>
<li>for: 解决 math&#x2F;symbolic reasoning、自然语言理解和指令执行任务</li>
<li>methods: 使用自然语言嵌入程序（NLEP）框架，让语言模型生成基于数据结构中的自然语言表示知识的函数</li>
<li>results: 可以超越强基线，在多种任务上提高性能，包括数学和符号逻辑、文本分类、问答和指令执行等任务，并且可以进行后续检查中间逻辑步骤的解释。<details>
<summary>Abstract</summary>
How can we perform computations over natural language representations to solve tasks that require symbolic and numeric reasoning? We propose natural language embedded programs (NLEP) as a unifying framework for addressing math/symbolic reasoning, natural language understanding, and instruction following tasks. Our approach prompts a language model to generate full Python programs that define functions over data structures which contain natural language representations of structured knowledge. A Python interpreter then executes the generated code and prints the output. Despite using a task-general prompt, we find that this approach can improve upon strong baselines across a range of different tasks including math and symbolic reasoning, text classification, question answering, and instruction following. We further find the generated programs are often interpretable and enable post-hoc verification of the intermediate reasoning steps.
</details>
<details>
<summary>摘要</summary>
如何通过对自然语言表示进行计算来解决需要符号逻辑和数值计算的任务？我们提出自然语言嵌入程序（NLEP）作为一个统一的框架，用于Addressing math/符号逻辑、自然语言理解和指令遵循任务。我们的方法请求一个语言模型生成全部Python程序，定义数据结构中含有自然语言表示的结构化知识上的函数。一个Python解释器然后执行生成的代码并输出结果。尽管使用了任务通用的提问，我们发现这种方法可以超越强基线 across a range of different tasks, including math and symbolic reasoning, text classification, question answering, and instruction following. 我们还发现生成的程序往往可读性好，允许后续验证中间的逻辑步骤。
</details></li>
</ul>
<hr>
<h2 id="Modeling-interdisciplinary-interactions-among-Physics-Mathematics-Computer-Science"><a href="#Modeling-interdisciplinary-interactions-among-Physics-Mathematics-Computer-Science" class="headerlink" title="Modeling interdisciplinary interactions among Physics, Mathematics &amp; Computer Science"></a>Modeling interdisciplinary interactions among Physics, Mathematics &amp; Computer Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10811">http://arxiv.org/abs/2309.10811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rima Hazra, Mayank Singh, Pawan Goyal, Bibhas Adhikari, Animesh Mukherjee</li>
<li>for: 本研究旨在研究物理（PHY）、数学（MA）和计算机科学（CS）三个领域之间的引用流动，以及这三个领域之间的引用关系。</li>
<li>methods: 本研究使用了一个数据集，包含了这三个领域的 более than 1.2 million 篇论文，并使用了时间桶特征来量化这三个领域之间的引用互动。</li>
<li>results: 研究发现，在这三个领域之间的引用关系存在一些特定的模式，例如，物理领域常常引用数学领域的论文，而计算机科学领域则常常引用物理领域和数学领域的论文。此外，研究还提出了一些基于 relay-linking 框架的数学模型，用于解释这三个领域之间的引用动态。<details>
<summary>Abstract</summary>
Interdisciplinarity has over the recent years have gained tremendous importance and has become one of the key ways of doing cutting edge research. In this paper we attempt to model the citation flow across three different fields -- Physics (PHY), Mathematics (MA) and Computer Science (CS). For instance, is there a specific pattern in which these fields cite one another? We carry out experiments on a dataset comprising more than 1.2 million articles taken from these three fields. We quantify the citation interactions among these three fields through temporal bucket signatures. We present numerical models based on variants of the recently proposed relay-linking framework to explain the citation dynamics across the three disciplines. These models make a modest attempt to unfold the underlying principles of how citation links could have been formed across the three fields over time.
</details>
<details>
<summary>摘要</summary>
近年来，多学科研究（interdisciplinarity）在研究领域中得到了广泛的重视和发展，成为当今研究的一种重要方法。在这篇论文中，我们尝试通过模拟三个不同领域（物理（PHY）、数学（MA）和计算机科学（CS））之间的引用流动，以探索这三个领域之间的引用模式是否存在特定的征式。我们在一个包含超过120万篇论文的数据集上进行了实验，并通过时间桶签名来量化这三个领域之间的引用互动。我们基于近期提出的协助链框架（relay-linking framework）的变体来提出数学模型，用于解释这三个领域之间的引用动力学。这些模型尝试描述在不同时间点上如何形成这三个领域之间的引用链。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Text-Compression-for-Classification"><a href="#Semantic-Text-Compression-for-Classification" class="headerlink" title="Semantic Text Compression for Classification"></a>Semantic Text Compression for Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10809">http://arxiv.org/abs/2309.10809</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emrecan Kutay, Aylin Yener</li>
<li>for: 这 paper 的目的是压缩文本中的含义，以便在分类等应用中提高效率。</li>
<li>methods: 这 paper 使用 semantic quantization 和压缩方法，使用 sentence embeddings 和semantic distortion metric来保持含义。</li>
<li>results: 这 paper 的结果表明，使用 semantic 方法可以大幅降低 bit 数，但准确性损失比基eline 小。此外， semantic clustering 可以进一步增加资源储存的可 reuse。这些方法在多种文本分类任务中表现出色。<details>
<summary>Abstract</summary>
We study semantic compression for text where meanings contained in the text are conveyed to a source decoder, e.g., for classification. The main motivator to move to such an approach of recovering the meaning without requiring exact reconstruction is the potential resource savings, both in storage and in conveying the information to another node. Towards this end, we propose semantic quantization and compression approaches for text where we utilize sentence embeddings and the semantic distortion metric to preserve the meaning. Our results demonstrate that the proposed semantic approaches result in substantial (orders of magnitude) savings in the required number of bits for message representation at the expense of very modest accuracy loss compared to the semantic agnostic baseline. We compare the results of proposed approaches and observe that resource savings enabled by semantic quantization can be further amplified by semantic clustering. Importantly, we observe the generalizability of the proposed methodology which produces excellent results on many benchmark text classification datasets with a diverse array of contexts.
</details>
<details>
<summary>摘要</summary>
我们研究语义压缩 для文本，其中文本中的意义传递到源解码器（例如分类）。主要的动机是避免精确重建的需求，以便实现资源储存和信息传输成本的减少。为此，我们提议语义压缩和压缩方法，使用句子嵌入和semantic distortion metric来保持语义。我们的结果表明，我们提议的语义方法可以实现重要的（许多个数级）储存成本减少，并且与语义agnostic基准相比，只有极少的准确性损失。我们比较了我们的方法与其他方法的结果，发现semantic quantization可以通过semantic clustering进一步增强资源储存的可抗性。重要的是，我们发现我们的方法在多个 benchmark文本分类 datasets上实现了优秀的结果，这些dataset中的上下文非常多样化。
</details></li>
</ul>
<hr>
<h2 id="Interactive-Distillation-of-Large-Single-Topic-Corpora-of-Scientific-Papers"><a href="#Interactive-Distillation-of-Large-Single-Topic-Corpora-of-Scientific-Papers" class="headerlink" title="Interactive Distillation of Large Single-Topic Corpora of Scientific Papers"></a>Interactive Distillation of Large Single-Topic Corpora of Scientific Papers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10772">http://arxiv.org/abs/2309.10772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicholas Solovyev, Ryan Barron, Manish Bhattarai, Maksim E. Eren, Kim O. Rasmussen, Boian S. Alexandrov</li>
<li>for: 这篇论文的目的是建立一个可扩展的、可靠的科学文献集合，并将其用于研究和教育。</li>
<li>methods: 这篇论文使用机器学习技术，将小量的“核心”文献集成为一个大量的科学文献集合。文献集合中的每篇文章都通过机器学习算法进行评估，以确定它们是否与“核心”文献集成关联。</li>
<li>results: 这篇论文透过机器学习技术建立了一个可扩展的、可靠的科学文献集合，并且可以透过人类专家的干预选择来确保集合中的文章是有关的。此外，这篇论文还使用了sub-topic模型ing（SeNMFk）来获得更多关于文章的信息。<details>
<summary>Abstract</summary>
Highly specific datasets of scientific literature are important for both research and education. However, it is difficult to build such datasets at scale. A common approach is to build these datasets reductively by applying topic modeling on an established corpus and selecting specific topics. A more robust but time-consuming approach is to build the dataset constructively in which a subject matter expert (SME) handpicks documents. This method does not scale and is prone to error as the dataset grows. Here we showcase a new tool, based on machine learning, for constructively generating targeted datasets of scientific literature. Given a small initial "core" corpus of papers, we build a citation network of documents. At each step of the citation network, we generate text embeddings and visualize the embeddings through dimensionality reduction. Papers are kept in the dataset if they are "similar" to the core or are otherwise pruned through human-in-the-loop selection. Additional insight into the papers is gained through sub-topic modeling using SeNMFk. We demonstrate our new tool for literature review by applying it to two different fields in machine learning.
</details>
<details>
<summary>摘要</summary>
高度特定的数据集是科研和教育中非常重要的。然而，建立这些数据集是困难的。一种常见的方法是通过缩写分析已有的文库，选择特定的主题来建立数据集。另一种更加robust但是时间费时的方法是通过专家手动选择文献来建立数据集。这种方法不具扩展性和容易出错。在这里，我们介绍了一种新工具，基于机器学习，用于构建targeted的科研文献数据集。我们给出了一个小的初始核心文献库，然后建立了引用网络，并在每个引用网络中生成文本嵌入。我们使用dimensionality reduction来Visualize嵌入。如果文献与核心文献相似，或者通过人工征选来选择，则保留文献在数据集中。我们还使用SeNMFk进行子主题分析，从而获得更多的文献信息。我们通过在机器学习领域进行文献综述来证明我们的新工具。
</details></li>
</ul>
<hr>
<h2 id="OpenBA-An-Open-sourced-15B-Bilingual-Asymmetric-seq2seq-Model-Pre-trained-from-Scratch"><a href="#OpenBA-An-Open-sourced-15B-Bilingual-Asymmetric-seq2seq-Model-Pre-trained-from-Scratch" class="headerlink" title="OpenBA: An Open-sourced 15B Bilingual Asymmetric seq2seq Model Pre-trained from Scratch"></a>OpenBA: An Open-sourced 15B Bilingual Asymmetric seq2seq Model Pre-trained from Scratch</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10706">http://arxiv.org/abs/2309.10706</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/opennlg/openba">https://github.com/opennlg/openba</a></li>
<li>paper_authors: Juntao Li, Zecheng Tang, Yuyang Ding, Pinzheng Wang, Pei Guo, Wangjie You, Dan Qiao, Wenliang Chen, Guohong Fu, Qiaoming Zhu, Guodong Zhou, Min Zhang</li>
<li>for: 本研究旨在开发一个基于中文的开源语言模型，以提高中文自然语言处理 task 的性能。</li>
<li>methods: 本文使用了一些有效和高效的技术，包括三个阶段的训练策略和数据处理等，以减少模型的大小。</li>
<li>results: 根据测试结果，我们的模型在 BELEBELE 测试套件、MMLU 测试套件和 C-Eval (hard) 测试套件上的性能比 LLaMA-70B 和 BLOOM-176B 更好，只需要使用 380B 个字。<details>
<summary>Abstract</summary>
Large language models (LLMs) with billions of parameters have demonstrated outstanding performance on various natural language processing tasks. This report presents OpenBA, an open-sourced 15B bilingual asymmetric seq2seq model, to contribute an LLM variant to the Chinese-oriented open-source model community. We enhance OpenBA with effective and efficient techniques as well as adopt a three-stage training strategy to train the model from scratch. Our solution can also achieve very competitive performance with only 380B tokens, which is better than LLaMA-70B on the BELEBELE benchmark, BLOOM-176B on the MMLU benchmark, GLM-130B on the C-Eval (hard) benchmark. This report provides the main details to pre-train an analogous model, including pre-training data processing, Bilingual Flan data collection, the empirical observations that inspire our model architecture design, training objectives of different stages, and other enhancement techniques. We have refactored our code to follow the design principles of the Huggingface Transformers Library, making it more convenient for developers to use, and released checkpoints of different training stages at https://huggingface.co/openBA. More details of our project are available at https://github.com/OpenNLG/openBA.git.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）拥有数十亿参数，在不同的自然语言处理任务上表现出色。本报告介绍OpenBA，一个开源的15B双语异称seq2seq模型，以贡献一种中文 Orientated 开源模型社区的变体。我们在OpenBA中应用有效和高效的技术，并采用三个阶段训练策略来从头开始训练模型。我们的解决方案可以在只有380B字符时达到非常竞争力的性能，比LLaMA-70B在BELEBELE标准上更好，比BLOOM-176B在MMLU标准上更好，比GLM-130B在C-Eval（困难）标准上更好。本报告提供了预训练模型的主要细节，包括预训练数据处理、双语Flann数据收集、预训练模型建立的设计原则、不同阶段的训练目标以及其他增强技术。我们已经对代码进行了 refactor，使其更加符合Huggingface Transformers Library的设计原则，并将不同训练阶段的检查点上传到https://huggingface.co/openBA。更多关于我们项目的详细信息可以通过https://github.com/OpenNLG/openBA.git获取。
</details></li>
</ul>
<hr>
<h2 id="Improving-Medical-Dialogue-Generation-with-Abstract-Meaning-Representations"><a href="#Improving-Medical-Dialogue-Generation-with-Abstract-Meaning-Representations" class="headerlink" title="Improving Medical Dialogue Generation with Abstract Meaning Representations"></a>Improving Medical Dialogue Generation with Abstract Meaning Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10608">http://arxiv.org/abs/2309.10608</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bohao Yang, Chen Tang, Chenghua Lin</li>
<li>for: 这篇论文旨在探讨对距离医疗的对话生成，以实现医疗专业知识的传递。</li>
<li>methods: 本研究使用抽象意义表示（AMR）创建图形表示，以强调对话中的语言构成部分和医疗知识中的关系和实体。</li>
<li>results: 本研究的结果显示，使用AMR图形表示可以增强对话生成模型的理解能力，并且比对基eline模型表现出色。此外，本研究提供了实现这种方法的资源代码，以便未来的研究。<details>
<summary>Abstract</summary>
Medical Dialogue Generation serves a critical role in telemedicine by facilitating the dissemination of medical expertise to patients. Existing studies focus on incorporating textual representations, which have limited their ability to represent the semantics of text, such as ignoring important medical entities. To enhance the model's understanding of the textual semantics and the medical knowledge including entities and relations, we introduce the use of Abstract Meaning Representations (AMR) to construct graphical representations that delineate the roles of language constituents and medical entities within the dialogues. In this paper, We propose a novel framework that models dialogues between patients and healthcare professionals using AMR graphs, where the neural networks incorporate textual and graphical knowledge with a dual attention mechanism. Experimental results show that our framework outperforms strong baseline models in medical dialogue generation, demonstrating the effectiveness of AMR graphs in enhancing the representations of medical knowledge and logical relationships. Furthermore, to support future research in this domain, we provide the corresponding source code at https://github.com/Bernard-Yang/MedDiaAMR.
</details>
<details>
<summary>摘要</summary>
医疗对话生成扮演着重要的角色在 теле医疗中，以便传递医疗专业知识给患者。现有研究主要集中在文本表示方面，这限制了模型的能力来表示文本 semantics，例如忽略重要的医疗实体。为了增强模型对文本 semantics和医疗知识，包括实体和关系的理解，我们介绍了使用抽象意义表示（AMR）构建图形表示，以便分析语言结构和医疗实体在对话中的角色。在这篇论文中，我们提出了一种新的框架，该框架使用 AMR 图来模型患者和医疗专业人员之间的对话，并使用双重注意机制来结合文本和图形知识。实验结果表明，我们的框架在医疗对话生成中表现出优于强基线模型，这表明 AMR 图可以增强医疗知识和逻辑关系的表示。此外，为支持未来在这个领域的研究，我们在 GitHub 上提供了相关的源代码，请参考 https://github.com/Bernard-Yang/MedDiaAMR。
</details></li>
</ul>
<hr>
<h2 id="FRACAS-A-FRench-Annotated-Corpus-of-Attribution-relations-in-newS"><a href="#FRACAS-A-FRench-Annotated-Corpus-of-Attribution-relations-in-newS" class="headerlink" title="FRACAS: A FRench Annotated Corpus of Attribution relations in newS"></a>FRACAS: A FRench Annotated Corpus of Attribution relations in newS</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10604">http://arxiv.org/abs/2309.10604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ange Richard, Laura Alonzo-Canul, François Portet</li>
<li>for: 本研究用于开发法语新闻文本中引用EXTRACTION和来源归属的手动注释 корпу。</li>
<li>methods: 本研究使用了 manually annotated corpus of 1676 newswire texts in French for quotation extraction and source attribution.</li>
<li>results: 研究得到了一个 manually annotated corpus，并且获得了对引用类型的Balance (直接、间接和混合)，以及对注释者之间的互动对象的高度一致。<details>
<summary>Abstract</summary>
Quotation extraction is a widely useful task both from a sociological and from a Natural Language Processing perspective. However, very little data is available to study this task in languages other than English. In this paper, we present a manually annotated corpus of 1676 newswire texts in French for quotation extraction and source attribution. We first describe the composition of our corpus and the choices that were made in selecting the data. We then detail the annotation guidelines and annotation process, as well as a few statistics about the final corpus and the obtained balance between quote types (direct, indirect and mixed, which are particularly challenging). We end by detailing our inter-annotator agreement between the 8 annotators who worked on manual labelling, which is substantially high for such a difficult linguistic phenomenon.
</details>
<details>
<summary>摘要</summary>
<SYS><LC>zh-CN</LC></SYS>引用抽取是一项广泛有用的任务，不仅从社会学角度而言，也从自然语言处理角度来说。然而，有很少数据可以研究这项任务的其他语言 besides English。在这篇论文中，我们提供了1676篇新闻报道文本的手动注释集，用于引用抽取和来源归属。我们首先介绍了我们的 corpus 的组成和选择的数据。然后，我们详细介绍了注释指南和注释过程，以及最终集合的一些统计数据和引用类型的平衡。最后，我们详细介绍了8名注释员的间接协作情况，即这种语言现象的协作率很高。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Deep-Cross-Language-Entity-Alignment"><a href="#Unsupervised-Deep-Cross-Language-Entity-Alignment" class="headerlink" title="Unsupervised Deep Cross-Language Entity Alignment"></a>Unsupervised Deep Cross-Language Entity Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10598">http://arxiv.org/abs/2309.10598</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chuanyus/udcea">https://github.com/chuanyus/udcea</a></li>
<li>paper_authors: Chuanyu Jiang, Yiming Qian, Lijun Chen, Yang Gu, Xia Xie</li>
<li>for: 这篇论文旨在提出一种简单且新的无监督方法，用于跨语言实体对齐。</li>
<li>methods: 我们使用深度学习多语言encoder和机器翻译器来编码知识图文本，从而减少了标注数据的依赖性。我们的方法同时考虑了全局和局部对齐策略。</li>
<li>results: 我们的方法可以在DBP15K数据集上得到0.966、0.990和0.996的Hits@1率，在无监督和半监督类别中超过了现状的方法。与监督方法相比，我们的方法在 Ja-En和Fr-En对齐任务中表现出了2.6%和0.4%的提升，只是微弱地下降0.2%在Zh-En对齐任务中。<details>
<summary>Abstract</summary>
Cross-lingual entity alignment is the task of finding the same semantic entities from different language knowledge graphs. In this paper, we propose a simple and novel unsupervised method for cross-language entity alignment. We utilize the deep learning multi-language encoder combined with a machine translator to encode knowledge graph text, which reduces the reliance on label data. Unlike traditional methods that only emphasize global or local alignment, our method simultaneously considers both alignment strategies. We first view the alignment task as a bipartite matching problem and then adopt the re-exchanging idea to accomplish alignment. Compared with the traditional bipartite matching algorithm that only gives one optimal solution, our algorithm generates ranked matching results which enabled many potentials downstream tasks. Additionally, our method can adapt two different types of optimization (minimal and maximal) in the bipartite matching process, which provides more flexibility. Our evaluation shows, we each scored 0.966, 0.990, and 0.996 Hits@1 rates on the DBP15K dataset in Chinese, Japanese, and French to English alignment tasks. We outperformed the state-of-the-art method in unsupervised and semi-supervised categories. Compared with the state-of-the-art supervised method, our method outperforms 2.6% and 0.4% in Ja-En and Fr-En alignment tasks while marginally lower by 0.2% in the Zh-En alignment task.
</details>
<details>
<summary>摘要</summary>
crossed-lingual entity alignment是找到不同语言知识图中的同义semantic entity的任务。在这篇论文中，我们提出了一种简单且新的无监督方法 для crossed-lingual entity alignment。我们利用深度学习多语言编码器和机器翻译器来编码知识图文本，从而减少了依赖于标注数据。与传统方法只强调全局或本地对齐不同，我们的方法同时考虑了这两种对齐策略。我们首先将对齐任务视为一种双方匹配问题，然后采用了重新交换的想法来完成对齐。与传统的双方匹配算法只给出一个最优解不同，我们的算法生成了排名匹配结果，这些结果允许更多的下游任务。此外，我们的方法可以在双方匹配过程中采用不同的优化策略（最小和最大），这提供了更多的灵活性。我们的评估结果显示，我们在DBP15K dataset上分别得分0.966、0.990和0.996，在中文、日语和法语到英语对齐任务中。我们超过了当前状态的方法在无监督和半监督类别中。与当前状态的监督方法相比，我们的方法在Ja-En和Fr-En对齐任务中表现出优于2.6%和0.4%，而在Zh-En对齐任务中则只下降0.2%。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Modeling-For-Spoken-Language-Identification"><a href="#Multimodal-Modeling-For-Spoken-Language-Identification" class="headerlink" title="Multimodal Modeling For Spoken Language Identification"></a>Multimodal Modeling For Spoken Language Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10567">http://arxiv.org/abs/2309.10567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shikhar Bharadwaj, Min Ma, Shikhar Vashishth, Ankur Bapna, Sriram Ganapathy, Vera Axelrod, Siddharth Dalmia, Wei Han, Yu Zhang, Daan van Esch, Sandy Ritchie, Partha Talukdar, Jason Riesa</li>
<li>for: 本研究旨在提高多媒体录音语言识别精度，通过利用不同类型的metadata来增强语言识别。</li>
<li>methods: 本研究提出了一种多modal Spoken Language Identification方法（MuSeLI），利用视频标题、描述和地理位置等metadata来提高语言识别精度。</li>
<li>results: 实验结果表明，Metadata可以提供显著帮助语言识别任务，并且对多媒体录音语言识别 Task取得了现场的状态。 Additionally, the ablation study shows that each modality contributes distinctively to language recognition.<details>
<summary>Abstract</summary>
Spoken language identification refers to the task of automatically predicting the spoken language in a given utterance. Conventionally, it is modeled as a speech-based language identification task. Prior techniques have been constrained to a single modality; however in the case of video data there is a wealth of other metadata that may be beneficial for this task. In this work, we propose MuSeLI, a Multimodal Spoken Language Identification method, which delves into the use of various metadata sources to enhance language identification. Our study reveals that metadata such as video title, description and geographic location provide substantial information to identify the spoken language of the multimedia recording. We conduct experiments using two diverse public datasets of YouTube videos, and obtain state-of-the-art results on the language identification task. We additionally conduct an ablation study that describes the distinct contribution of each modality for language recognition.
</details>
<details>
<summary>摘要</summary>
通过多模态特征来实现语言认识，这是我们的研究方向。在视频数据中，我们可以使用视频标题、描述和地理位置等 metadata 来提高语言认识的准确性。我们在两个 YouTube 视频数据集上进行实验，并在语言认识任务上达到了状态之 искусственный前景。此外，我们还进行了减少研究，以解释每个模式在语言认识中的独特贡献。
</details></li>
</ul>
<hr>
<h2 id="NSOAMT-–-New-Search-Only-Approach-to-Machine-Translation"><a href="#NSOAMT-–-New-Search-Only-Approach-to-Machine-Translation" class="headerlink" title="NSOAMT – New Search Only Approach to Machine Translation"></a>NSOAMT – New Search Only Approach to Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10526">http://arxiv.org/abs/2309.10526</a></li>
<li>repo_url: None</li>
<li>paper_authors: João Luís, Diogo Cardoso, José Marques, Luís Campos</li>
<li>for: 这个论文的目的是提出一种基于新搜索方法的机器翻译技术，以解决传统技术的慢速和不准确问题。</li>
<li>methods: 这个研究采用了一种新的索引技术，通过将具有相似语义的词语组合在一起，实现对原文语言记录和翻译语言之间的对应关系。</li>
<li>results: 这个研究发现，对于某些类型的文档，使用这种新索引技术可以提高翻译速度和准确性，并且可以开发出一种基于这种方法的翻译工具。<details>
<summary>Abstract</summary>
Translation automation mechanisms and tools have been developed for several years to bring people who speak different languages together. A "new search only approach to machine translation" was adopted to tackle some of the slowness and inaccuracy of the other technologies. The idea is to develop a solution that, by indexing an incremental set of words that combine a certain semantic meaning, makes it possible to create a process of correspondence between their native language record and the language of translation. This research principle assumes that the vocabulary used in a given type of publication/document is relatively limited in terms of language style and word diversity, which enhances the greater effect of instantaneously and rigor in the translation process through the indexing process. A volume of electronic text documents where processed and loaded into a database, and analyzed and measured in order confirm the previous premise. Although the observed and projected metric values did not give encouraging results, it was possible to develop and make available a translation tool using this approach.
</details>
<details>
<summary>摘要</summary>
机器翻译技术和工具在数年前就已经开发出来，以便让不同语言的人们共同交流。我们采用了一种“新搜索Onlyapproach to machine translation”来解决其他技术的慢速和不准确性。我们的想法是，通过索引增量词汇，使得将原始语言纪录与翻译语言之间创建对应关系。这个研究原则假设了公开发表/文档中的词汇数量相对较少，而且语言风格和词汇多样性受限，从而通过索引过程实现更快速、更加准确的翻译。我们对一volume of electronic文档进行处理和加载到数据库中，并对其进行分析和测量，以确认上述假设。虽然观察到的和预计的 метриック值并不给出激励的结果，但我们仍然可以开发出一种使用这种方法的翻译工具。
</details></li>
</ul>
<hr>
<h2 id="Harnessing-the-Zero-Shot-Power-of-Instruction-Tuned-Large-Language-Model-in-End-to-End-Speech-Recognition"><a href="#Harnessing-the-Zero-Shot-Power-of-Instruction-Tuned-Large-Language-Model-in-End-to-End-Speech-Recognition" class="headerlink" title="Harnessing the Zero-Shot Power of Instruction-Tuned Large Language Model in End-to-End Speech Recognition"></a>Harnessing the Zero-Shot Power of Instruction-Tuned Large Language Model in End-to-End Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10524">http://arxiv.org/abs/2309.10524</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yosuke Higuchi, Tetsuji Ogawa, Tetsunori Kobayashi</li>
<li>for: 本研究旨在探讨一种使用大型自然语言模型（LLM）和端到端自动语音识别（ASR）的新的集成方法，以提高ASR性能。</li>
<li>methods: 我们使用一种名为Llama2的 instrucion-tuned LLM，并将其作为decoder的前端使用。我们还使用CTC和注意力架构，将ASR假设作为输入，并将其feed到LLM中，以便LLM可以根据指令进行文本生成。</li>
<li>results: 我们的实验结果和分析表明，这种集成方法可以提供出色的性能提升，并且我们的方法受益于LLM-based rescoring。<details>
<summary>Abstract</summary>
We present a novel integration of an instruction-tuned large language model (LLM) and end-to-end automatic speech recognition (ASR). Modern LLMs can perform a wide range of linguistic tasks within zero-shot learning when provided with a precise instruction or a prompt to guide the text generation process towards the desired task. We explore using this zero-shot capability of LLMs to extract linguistic information that can contribute to improving ASR performance. Specifically, we direct an LLM to correct grammatical errors in an ASR hypothesis and harness the embedded linguistic knowledge to conduct end-to-end ASR. The proposed model is built on the hybrid connectionist temporal classification (CTC) and attention architecture, where an instruction-tuned LLM (i.e., Llama2) is employed as a front-end of the decoder. An ASR hypothesis, subject to correction, is obtained from the encoder via CTC decoding, which is then fed into the LLM along with an instruction. The decoder subsequently takes as input the LLM embeddings to perform sequence generation, incorporating acoustic information from the encoder output. Experimental results and analyses demonstrate that the proposed integration yields promising performance improvements, and our approach largely benefits from LLM-based rescoring.
</details>
<details>
<summary>摘要</summary>
The proposed model is built on the hybrid connectionist temporal classification (CTC) and attention architecture, where an instruction-tuned LLM (i.e., Llama2) is employed as the front-end of the decoder. An ASR hypothesis, subject to correction, is obtained from the encoder via CTC decoding, which is then fed into the LLM along with an instruction. The decoder subsequently takes as input the LLM embeddings to perform sequence generation, incorporating acoustic information from the encoder output.Experimental results and analyses demonstrate that the proposed integration yields promising performance improvements, and our approach largely benefits from LLM-based rescoring.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Open-Domain-Table-Question-Answering-via-Syntax-and-Structure-aware-Dense-Retrieval"><a href="#Enhancing-Open-Domain-Table-Question-Answering-via-Syntax-and-Structure-aware-Dense-Retrieval" class="headerlink" title="Enhancing Open-Domain Table Question Answering via Syntax- and Structure-aware Dense Retrieval"></a>Enhancing Open-Domain Table Question Answering via Syntax- and Structure-aware Dense Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10506">http://arxiv.org/abs/2309.10506</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nzjin/ODTQA">https://github.com/nzjin/ODTQA</a></li>
<li>paper_authors: Nengzheng Jin, Dongfang Li, Junying Chen, Joanna Siebert, Qingcai Chen</li>
<li>for:  answering open-domain table questions by retrieving and extracting information from a large collection of tables</li>
<li>methods: using syntax- and structure-aware retrieval method that provides syntactical representations for the question and uses structural header and value representations for the tables to avoid information loss</li>
<li>results: achieving state-of-the-art performance on the NQ-tables dataset and overwhelming strong baselines on a newly curated open-domain Text-to-SQL datasetHere’s the simplified Chinese text:</li>
<li>for:  Answering open-domain 表格问题，通过大量表格的检索和提取信息。</li>
<li>methods: 使用 syntax- 和 structure-aware 检索方法，提供问题的 sintactical 表示，并使用表格的结构头和值表示来避免信息损失。</li>
<li>results: 在 NQ-tables 数据集上达到状态级表现，在新编辑的 open-domain Text-to-SQL 数据集上压倒强大的基eline。<details>
<summary>Abstract</summary>
Open-domain table question answering aims to provide answers to a question by retrieving and extracting information from a large collection of tables. Existing studies of open-domain table QA either directly adopt text retrieval methods or consider the table structure only in the encoding layer for table retrieval, which may cause syntactical and structural information loss during table scoring. To address this issue, we propose a syntax- and structure-aware retrieval method for the open-domain table QA task. It provides syntactical representations for the question and uses the structural header and value representations for the tables to avoid the loss of fine-grained syntactical and structural information. Then, a syntactical-to-structural aggregator is used to obtain the matching score between the question and a candidate table by mimicking the human retrieval process. Experimental results show that our method achieves the state-of-the-art on the NQ-tables dataset and overwhelms strong baselines on a newly curated open-domain Text-to-SQL dataset.
</details>
<details>
<summary>摘要</summary>
开放领域表格问答旨在提供问题的答案，通过检索和提取大量表格中的信息。现有研究的开放领域表格QA方法可能直接采用文本检索方法，或者只考虑表格结构在编码层面进行表格检索，这可能会导致问题的语法和结构信息丢失，从而影响表格的得分。为解决这个问题，我们提出一种语法和结构意识检索方法，用于开放领域表格QA任务。它提供了问题的语法表示，并使用表格的结构标头和值表示来避免语法和结构信息的丢失。然后，一个语法-结构汇总器用于获取问题和候选表格之间的匹配分数，通过模拟人类检索过程来实现。实验结果表明，我们的方法在NQ-tables数据集上实现了领先地位，并在一个新收录的开放领域文本-SQL数据集上压倒了强大的基线。
</details></li>
</ul>
<hr>
<h2 id="Improving-Speaker-Diarization-using-Semantic-Information-Joint-Pairwise-Constraints-Propagation"><a href="#Improving-Speaker-Diarization-using-Semantic-Information-Joint-Pairwise-Constraints-Propagation" class="headerlink" title="Improving Speaker Diarization using Semantic Information: Joint Pairwise Constraints Propagation"></a>Improving Speaker Diarization using Semantic Information: Joint Pairwise Constraints Propagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10456">http://arxiv.org/abs/2309.10456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luyao Cheng, Siqi Zheng, Qinglin Zhang, Hui Wang, Yafeng Chen, Qian Chen, Shiliang Zhang</li>
<li>for: 本研究旨在充分利用语言模型来提高基于集群的说话人识别系统的性能。</li>
<li>methods: 我们提出了一种新的方法，利用语言模型提取说话人相关的semantic信息，并将这些信息转化为对应的对比约束。</li>
<li>results: 我们在公共数据集上进行了广泛的实验，结果表明我们提出的方法在比对 voz-only 的说话人识别系统时表现出了一致性的superiority。<details>
<summary>Abstract</summary>
Speaker diarization has gained considerable attention within speech processing research community. Mainstream speaker diarization rely primarily on speakers' voice characteristics extracted from acoustic signals and often overlook the potential of semantic information. Considering the fact that speech signals can efficiently convey the content of a speech, it is of our interest to fully exploit these semantic cues utilizing language models. In this work we propose a novel approach to effectively leverage semantic information in clustering-based speaker diarization systems. Firstly, we introduce spoken language understanding modules to extract speaker-related semantic information and utilize these information to construct pairwise constraints. Secondly, we present a novel framework to integrate these constraints into the speaker diarization pipeline, enhancing the performance of the entire system. Extensive experiments conducted on the public dataset demonstrate the consistent superiority of our proposed approach over acoustic-only speaker diarization systems.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本为简化中文。<</SYS>>发言人识别在语音处理研究社区中受到了广泛的关注。主流的发言人识别方法主要基于发言人的声音特征，从语音信号中提取发言人的声音特征，然而它们经常忽略语音信号中的 semantics 信息。考虑到语音信号可以办好发言人的发言内容，我们想要充分利用这些 semantics 信息，以提高 clustering-based 发言人识别系统的性能。首先，我们引入了语言理解模块，以提取发言人相关的 semantics 信息，并使用这些信息构建对应的对比约束。其次，我们提出了一种新的框架，将这些约束集成到发言人识别管道中，从而提高整个系统的性能。在公共数据集上进行了广泛的实验，我们的提议方法在对比于声音Only 发言人识别系统时具有一致的超越性。
</details></li>
</ul>
<hr>
<h2 id="Reformulating-Sequential-Recommendation-Learning-Dynamic-User-Interest-with-Content-enriched-Language-Modeling"><a href="#Reformulating-Sequential-Recommendation-Learning-Dynamic-User-Interest-with-Content-enriched-Language-Modeling" class="headerlink" title="Reformulating Sequential Recommendation: Learning Dynamic User Interest with Content-enriched Language Modeling"></a>Reformulating Sequential Recommendation: Learning Dynamic User Interest with Content-enriched Language Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10435">http://arxiv.org/abs/2309.10435</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junzhe Jiang, Shang Qu, Mingyue Cheng, Qi Liu</li>
<li>for: 这个论文的目的是提出一种基于语言模型的新式分布式推荐方法，以提高推荐系统的个性化性和准确性。</li>
<li>methods: 该方法使用预训练的语言模型来捕捉用户的兴趣和需求，并通过语义理解来生成个性化的推荐结果。</li>
<li>results: 经过实验 validate，该方法可以在多个数据集上达到比较好的推荐效果，并且提供了有价值的推荐结果和推荐方法的指导意见。<details>
<summary>Abstract</summary>
Recommender systems are essential for online applications, and sequential recommendation has enjoyed significant prevalence due to its expressive ability to capture dynamic user interests. However, previous sequential modeling methods still have limitations in capturing contextual information. The primary reason for this issue is that language models often lack an understanding of domain-specific knowledge and item-related textual content. To address this issue, we adopt a new sequential recommendation paradigm and propose LANCER, which leverages the semantic understanding capabilities of pre-trained language models to generate personalized recommendations. Our approach bridges the gap between language models and recommender systems, resulting in more human-like recommendations. We demonstrate the effectiveness of our approach through experiments on several benchmark datasets, showing promising results and providing valuable insights into the influence of our model on sequential recommendation tasks. Furthermore, our experimental codes are publicly available.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>在线应用程序中，推荐系统是非常重要的，而串行推荐占据了主导地位，因为它可以快速表达用户的兴趣。然而，以前的串行建模方法仍有限制，不能够 capture contextual information。主要的原因是语言模型通常缺乏域pecific知识和项目相关的文本内容的理解。为了解决这个问题，我们采用了一种新的串行推荐方式，并提出了LANCER，它利用预训练语言模型的semantic理解能力来生成个性化推荐。我们的方法 bridge了语言模型和推荐系统之间的 gap，从而生成更人类化的推荐。我们通过对多个 benchmark datasets进行实验，证明了我们的方法的有效性，并提供了有价值的推荐系统设计的意见。此外，我们的实验代码也公开可用。
</details></li>
</ul>
<hr>
<h2 id="Writer-Defined-AI-Personas-for-On-Demand-Feedback-Generation"><a href="#Writer-Defined-AI-Personas-for-On-Demand-Feedback-Generation" class="headerlink" title="Writer-Defined AI Personas for On-Demand Feedback Generation"></a>Writer-Defined AI Personas for On-Demand Feedback Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10433">http://arxiv.org/abs/2309.10433</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karim Benharrak, Tim Zindulka, Florian Lehmann, Hendrik Heuer, Daniel Buschek</li>
<li>for: 这篇论文是为了支持作者创作而设计的，帮助作者更好地理解和 Hook 到他们的target audience。</li>
<li>methods: 这篇论文使用了基于作者定义的 AI人物来生成即时反馈，以帮助作者更好地理解和 Hook 到他们的target audience。</li>
<li>results: 作者对这种概念表示欢迎，并在两个用户研究中使用了这种方法来获得不同的视角和反馈。  however, the feedback was often verbose and unspecific.<details>
<summary>Abstract</summary>
Compelling writing is tailored to its audience. This is challenging, as writers may struggle to empathize with readers, get feedback in time, or gain access to the target group. We propose a concept that generates on-demand feedback, based on writer-defined AI personas of any target audience. We explore this concept with a prototype (using GPT-3.5) in two user studies (N=5 and N=11): Writers appreciated the concept and strategically used personas for getting different perspectives. The feedback was seen as helpful and inspired revisions of text and personas, although it was often verbose and unspecific. We discuss the impact of on-demand feedback, the limited representativity of contemporary AI systems, and further ideas for defining AI personas. This work contributes to the vision of supporting writers with AI by expanding the socio-technical perspective in AI tool design: To empower creators, we also need to keep in mind their relationship to an audience.
</details>
<details>
<summary>摘要</summary>
优秀的写作是适应其读者群体的。这是一项挑战，因为作者可能难以理解读者，获得时间ous feedback，或者访问目标群体。我们提出了一个概念，即基于作者定义的人工智能人类，以获得即时反馈。我们在两项用户研究（N=5和N=11）中试用了这个概念：作者喜欢这个概念，并在获得不同角度的反馈时使用了人类。反馈被看作是有帮助的，并促使了文本和人类的修订，although it was often verbose and unspecific. We discuss the impact of on-demand feedback, the limited representativity of contemporary AI systems, and further ideas for defining AI personas. This work contributes to the vision of supporting writers with AI by expanding the socio-technical perspective in AI tool design: To empower creators, we also need to keep in mind their relationship to an audience.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide that version as well.
</details></li>
</ul>
<hr>
<h2 id="PICK-Polished-Informed-Candidate-Scoring-for-Knowledge-Grounded-Dialogue-Systems"><a href="#PICK-Polished-Informed-Candidate-Scoring-for-Knowledge-Grounded-Dialogue-Systems" class="headerlink" title="PICK: Polished &amp; Informed Candidate Scoring for Knowledge-Grounded Dialogue Systems"></a>PICK: Polished &amp; Informed Candidate Scoring for Knowledge-Grounded Dialogue Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10413">http://arxiv.org/abs/2309.10413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bryan Wilie, Yan Xu, Willy Chung, Samuel Cahyawijaya, Holy Lovenia, Pascale Fung</li>
<li>for: 提高知识grunded对话系统的响应质量，使其更加信息归一化和有趣。</li>
<li>methods: 基于外部知识的对话生成系统，通过分析多种语言模型生成结果，发现存在多个生成结果，其中一些更加准确和有 relevance 性。</li>
<li>results: 提出了一种基于抑制策略的生成重新分配框架PICK，可以使模型生成更加准确和有 relevance 性的响应，不需要额外的标注数据或模型调整。经过自动和人工评估，PICK 能够提高系统的表现，并且在所有排序策略下保持稳定性。详细实现可以参考<a target="_blank" rel="noopener" href="https://github.com/bryanwilie/pick%E3%80%82">https://github.com/bryanwilie/pick。</a><details>
<summary>Abstract</summary>
Grounding dialogue response generation on external knowledge is proposed to produce informative and engaging responses. However, current knowledge-grounded dialogue (KGD) systems often fail to align the generated responses with human-preferred qualities due to several issues like hallucination and the lack of coherence. Upon analyzing multiple language model generations, we observe the presence of alternative generated responses within a single decoding process. These alternative responses are more faithful and exhibit a comparable or higher level of relevance to prior conversational turns compared to the optimal responses prioritized by the decoding processes. To address these challenges and driven by these observations, we propose Polished \& Informed Candidate Scoring (PICK), a generation re-scoring framework that empowers models to generate faithful and relevant responses without requiring additional labeled data or model tuning. Through comprehensive automatic and human evaluations, we demonstrate the effectiveness of PICK in generating responses that are more faithful while keeping them relevant to the dialogue history. Furthermore, PICK consistently improves the system's performance with both oracle and retrieved knowledge in all decoding strategies. We provide the detailed implementation in https://github.com/bryanwilie/pick .
</details>
<details>
<summary>摘要</summary>
提出了基于外部知识的对话回答生成系统，以生成有用和有趣的回答。然而，目前的知识基数对对话（KGD）系统经常无法落实生成的回答与人类喜好的质量不符，这可能是因为幻觉和对话异常的问题。我们分析了多种语言模型生成结果，发现生成过程中存在多个可行的回答，这些回答更 faithful 并且与之前的对话转折更高度相关。为了解决这些挑战，我们提出了精炼并 Informed Candidate Scoring（PICK）生成重新评分框架，使模型能够生成忠诚和相关的回答，不需要额外的标注数据或模型调整。通过自动和人工评估，我们证明了 PICK 的效果，可以生成更忠诚的回答，同时保持与对话历史相关。此外， PICK 在所有搜索策略下都能够一直提高系统的性能，并且与oracle和检索知识相结合。我们在 GitHub 上提供了详细的实现。
</details></li>
</ul>
<hr>
<h2 id="PoSE-Efficient-Context-Window-Extension-of-LLMs-via-Positional-Skip-wise-Training"><a href="#PoSE-Efficient-Context-Window-Extension-of-LLMs-via-Positional-Skip-wise-Training" class="headerlink" title="PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training"></a>PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10400">http://arxiv.org/abs/2309.10400</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dwzhu-pku/pose">https://github.com/dwzhu-pku/pose</a></li>
<li>paper_authors: Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, Sujian Li</li>
<li>for: 用于提高大语言模型的适应性和扩展性</li>
<li>methods: 使用Positional Skip-wisE（PoSE）训练方法，通过在训练过程中随机填充各个 chunk 的位置指标来模拟长输入序列，从而适应不同的上下文窗口大小</li>
<li>results: 比较训练在全长输入上与 PoSE 训练在短 chunk 上，后者减少了内存和时间开销，而且性能减差不大。此外，PoSE 方法可以与 RoPE-based LLMs 和不同的位置插值策略兼容，并且可以无限扩展上下文窗口，具体取决于执行时间的内存使用情况。<details>
<summary>Abstract</summary>
In this paper, we introduce Positional Skip-wisE (PoSE) training for efficient adaptation of large language models~(LLMs) to extremely long context windows. PoSE decouples train length from target context window size by simulating long inputs using a fixed context window with manipulated position indices during training. Concretely, we select several short chunks from a long input sequence, and introduce distinct skipping bias terms to modify the position indices of each chunk. These bias terms, along with the length of each chunk, are altered for each training example, allowing the model to adapt to all positions within the target context window without training on full length inputs. Experiments show that, compared with fine-tuning on the full length, PoSE greatly reduces memory and time overhead with minimal impact on performance. Leveraging this advantage, we have successfully extended the LLaMA model to 128k tokens. Furthermore, we empirically confirm that PoSE is compatible with all RoPE-based LLMs and various position interpolation strategies. Notably, by decoupling fine-tuning length from target context window, PoSE can theoretically extend the context window infinitely, constrained only by memory usage for inference. With ongoing advancements for efficient inference, we believe PoSE holds great promise for scaling the context window even further.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了Positional Skip-wisE（PoSE）训练方法，用于高效地适应大语言模型（LLM）到极长上下文窗口。PoSE将训练长度与目标上下文窗口大小分离开来，在训练过程中使用固定的上下文窗口和修改位标的扰动技术来模拟长输入。具体来说，我们从长输入序列中选择一些短块，并在每个块上引入不同的跳过偏好项来修改位标。这些偏好项， junto with each chunk's length, are altered for each training example, allowing the model to adapt to all positions within the target context window without training on full-length inputs.实验表明，相比于精细调整全长输入，PoSE可以减少内存和时间开销，而无需减少性能。通过这种优势，我们成功地扩展了LLaMA模型到128k个token。此外，我们还证实了PoSE与RoPE基于LLM的各种位置 interpolate策略兼容，并且可以 theoretically extend the context window infinitely，即使在推理时使用内存。随着高效推理技术的不断发展，我们相信PoSE具有极大的扩展前途。
</details></li>
</ul>
<hr>
<h2 id="Prompt-Condition-and-Generate-Classification-of-Unsupported-Claims-with-In-Context-Learning"><a href="#Prompt-Condition-and-Generate-Classification-of-Unsupported-Claims-with-In-Context-Learning" class="headerlink" title="Prompt, Condition, and Generate: Classification of Unsupported Claims with In-Context Learning"></a>Prompt, Condition, and Generate: Classification of Unsupported Claims with In-Context Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10359">http://arxiv.org/abs/2309.10359</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Ebert Christensen, Srishti Yadav, Serge Belongie</li>
<li>for: 本研究旨在提供一种方法，可以从日常生活中遇到的不支持、不可靠的声明中提炼出一个可 COUNT的数据集，以便更好地理解和描述这些声明。</li>
<li>methods: 本研究使用了人工智能技术，特别是大语言模型（LLM），来自动生成声明，并通过Context Learning来帮助模型更好地理解和描述声明。</li>
<li>results: 研究发现，使用生成的声明可以提高 narative 分类模型的性能，并且可以使用一些训练示例来推断声明的Stand和方向。这种模型可以在应用中使用，例如 Fact-checking 等。<details>
<summary>Abstract</summary>
Unsupported and unfalsifiable claims we encounter in our daily lives can influence our view of the world. Characterizing, summarizing, and -- more generally -- making sense of such claims, however, can be challenging. In this work, we focus on fine-grained debate topics and formulate a new task of distilling, from such claims, a countable set of narratives. We present a crowdsourced dataset of 12 controversial topics, comprising more than 120k arguments, claims, and comments from heterogeneous sources, each annotated with a narrative label. We further investigate how large language models (LLMs) can be used to synthesise claims using In-Context Learning. We find that generated claims with supported evidence can be used to improve the performance of narrative classification models and, additionally, that the same model can infer the stance and aspect using a few training examples. Such a model can be useful in applications which rely on narratives , e.g. fact-checking.
</details>
<details>
<summary>摘要</summary>
日常生活中遇到的无法支持和无法证明的声明可能会影响我们对世界的看法。然而，描述、概括和更广泛地来说，对这些声明的理解可以是困难的。在这项工作中，我们关注细化的辩论话题，并提出了一项新的任务：从这些声明中提炼出一个可数的数量的故事。我们提供了一个来自多种源的12个热点话题的大量人工标注数据集，包括超过12万个Arguments、声明和评论，每个声明都被标注了故事标签。我们进一步研究了大型自然语言模型（LLM）如何在上下文学习中生成声明，并发现了以下两点：First，使用支持证明的生成声明可以提高故事分类模型的性能；Second，使用相同的模型可以在几个训练示例后推断姿态和方向。这种模型在基于故事的应用中可以是有用的，例如ifact-checking。
</details></li>
</ul>
<hr>
<h2 id="KoBigBird-large-Transformation-of-Transformer-for-Korean-Language-Understanding"><a href="#KoBigBird-large-Transformation-of-Transformer-for-Korean-Language-Understanding" class="headerlink" title="KoBigBird-large: Transformation of Transformer for Korean Language Understanding"></a>KoBigBird-large: Transformation of Transformer for Korean Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10339">http://arxiv.org/abs/2309.10339</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kisu Yang, Yoonna Jang, Taewoo Lee, Jinwoo Seong, Hyungjin Lee, Hwanseok Jang, Heuiseok Lim</li>
<li>for: 这个研究实现了一个大型的韩文BigBird模型，以获得韩文理解的State-of-the-art表现，并允许较长的序列处理。</li>
<li>methods: 我们仅将架构变更，并将统计编码扩展为我们提出的弹性统计编码表现（TAPER）。</li>
<li>results: 实验结果显示，KoBigBird-large在韩文语言理解benchmark上的总表现和文档分类和问答任务中的较长序列表现皆达到了State-of-the-art水平，并且在比较基eline模型时表现最佳。我们将这个模型公开发布。<details>
<summary>Abstract</summary>
This work presents KoBigBird-large, a large size of Korean BigBird that achieves state-of-the-art performance and allows long sequence processing for Korean language understanding. Without further pretraining, we only transform the architecture and extend the positional encoding with our proposed Tapered Absolute Positional Encoding Representations (TAPER). In experiments, KoBigBird-large shows state-of-the-art overall performance on Korean language understanding benchmarks and the best performance on document classification and question answering tasks for longer sequences against the competitive baseline models. We publicly release our model here.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Rigorously-Assessing-Natural-Language-Explanations-of-Neurons"><a href="#Rigorously-Assessing-Natural-Language-Explanations-of-Neurons" class="headerlink" title="Rigorously Assessing Natural Language Explanations of Neurons"></a>Rigorously Assessing Natural Language Explanations of Neurons</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10312">http://arxiv.org/abs/2309.10312</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hieu9955/ggggg">https://github.com/hieu9955/ggggg</a></li>
<li>paper_authors: Jing Huang, Atticus Geiger, Karel D’Oosterlinck, Zhengxuan Wu, Christopher Potts</li>
<li>for:  This paper aims to evaluate the faithfulness of natural language explanations of how large language models process and store information.</li>
<li>methods: The paper develops two modes of evaluation for natural language explanations that claim individual neurons represent a concept in a text input, including observational and intervention modes.</li>
<li>results: The paper shows that even the most confident explanations have high error rates and little to no causal efficacy, and critically assesses whether natural language is a good choice for explanations and whether neurons are the best level of analysis.<details>
<summary>Abstract</summary>
Natural language is an appealing medium for explaining how large language models process and store information, but evaluating the faithfulness of such explanations is challenging. To help address this, we develop two modes of evaluation for natural language explanations that claim individual neurons represent a concept in a text input. In the observational mode, we evaluate claims that a neuron $a$ activates on all and only input strings that refer to a concept picked out by the proposed explanation $E$. In the intervention mode, we construe $E$ as a claim that the neuron $a$ is a causal mediator of the concept denoted by $E$. We apply our framework to the GPT-4-generated explanations of GPT-2 XL neurons of Bills et al. (2023) and show that even the most confident explanations have high error rates and little to no causal efficacy. We close the paper by critically assessing whether natural language is a good choice for explanations and whether neurons are the best level of analysis.
</details>
<details>
<summary>摘要</summary>
自然语言是一种吸引人的媒介，用于解释大语言模型如何处理和存储信息，但评估这些解释的准确性具有挑战。为了解决这个问题，我们开发了两种评估模式 для自然语言解释，即 observational 模式和 intervención 模式。在 observational 模式下，我们评估laims that neuron $a$ 活动在所有和只有输入串 refer to a concept picked out by the proposed explanation $E$。在 intervención 模式下，我们理解 $E$ 为 neuron $a$ 是输入串中某种概念的 causal mediator。我们应用我们的框架到 Bills et al. (2023) 所生成的 GPT-2 XL neuron的解释中，并发现even the most confident explanations have high error rates and little to no causal efficacy。我们在 conclusion 中 kritically assessing whether natural language is a good choice for explanations and whether neurons are the best level of analysis。
</details></li>
</ul>
<hr>
<h2 id="Baichuan-2-Open-Large-scale-Language-Models"><a href="#Baichuan-2-Open-Large-scale-Language-Models" class="headerlink" title="Baichuan 2: Open Large-scale Language Models"></a>Baichuan 2: Open Large-scale Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10305">http://arxiv.org/abs/2309.10305</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/baichuan-inc/baichuan2">https://github.com/baichuan-inc/baichuan2</a></li>
<li>paper_authors: Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, JunTao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, Zhiying Wu</li>
<li>for: 这个技术报告是用于介绍一种大型多语言语言模型（Baichuan 2）的论文。</li>
<li>methods: 这个论文使用了从零开始训练的大规模多语言语言模型，共包括70亿和130亿参数。</li>
<li>results: 这个论文表明，Baichuan 2 可以与其他开源模型相比或超越它们在公共测试准则上，例如 MMLU、CMMLU、GSM8K 和 HumanEval。 此外，Baichuan 2 在医学和法律领域也表现出色。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated remarkable performance on a variety of natural language tasks based on just a few examples of natural language instructions, reducing the need for extensive feature engineering. However, most powerful LLMs are closed-source or limited in their capability for languages other than English. In this technical report, we present Baichuan 2, a series of large-scale multilingual language models containing 7 billion and 13 billion parameters, trained from scratch, on 2.6 trillion tokens. Baichuan 2 matches or outperforms other open-source models of similar size on public benchmarks like MMLU, CMMLU, GSM8K, and HumanEval. Furthermore, Baichuan 2 excels in vertical domains such as medicine and law. We will release all pre-training model checkpoints to benefit the research community in better understanding the training dynamics of Baichuan 2.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）已经展示出杰出的表现在多种自然语言任务上，仅基于几个自然语言指令，减少需要广泛的特征工程。然而，大多数最具备力 LLMs 是封闭式或仅能在英文语言上使用。在本技术报告中，我们发布了 Baichuan 2，一系列大规模多语言模型，包含 700亿和1300亿个参数，从零开始训练，使用 2.6 兆个字元。Baichuan 2 与其他开源模型相比，在公共测试 benchmark 上匹配或超越。此外，Baichuan 2 在医学和法律领域中表现出色。我们将发布所有预训练模型检查点，以便研究社区更好地理解 Baichuan 2 的训练过程。
</details></li>
</ul>
<hr>
<h2 id="Using-fine-tuning-and-min-lookahead-beam-search-to-improve-Whisper"><a href="#Using-fine-tuning-and-min-lookahead-beam-search-to-improve-Whisper" class="headerlink" title="Using fine-tuning and min lookahead beam search to improve Whisper"></a>Using fine-tuning and min lookahead beam search to improve Whisper</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10299">http://arxiv.org/abs/2309.10299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Do, Oscar Brown, Zhengjie Wang, Nikhil Mathew, Zixin Liu, Jawwad Ahmed, Cheng Yu</li>
<li>for: 提高low-resource语言中Whisper的表现</li>
<li>methods:  fine-tune Whisper on additional data and propose an improved decoding algorithm</li>
<li>results: 在越南语言上， fine-tuning Whisper-Tiny with LoRA leads to an improvement of 38.49 in WER compared to zero-shot setting, and using Filter-Ends and Min Lookahead decoding algorithms reduces WER by 2.26 on average compared to standard beam search.I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
The performance of Whisper in low-resource languages is still far from perfect. In addition to a lack of training data on low-resource languages, we identify some limitations in the beam search algorithm used in Whisper. To address these issues, we fine-tune Whisper on additional data and propose an improved decoding algorithm. On the Vietnamese language, fine-tuning Whisper-Tiny with LoRA leads to an improvement of 38.49 in WER over the zero-shot Whisper-Tiny setting which is a further reduction of 1.45 compared to full-parameter fine-tuning. Additionally, by using Filter-Ends and Min Lookahead decoding algorithms, the WER reduces by 2.26 on average over a range of languages compared to standard beam search. These results generalise to larger Whisper model sizes. We also prove a theorem that Min Lookahead outperforms the standard beam search algorithm used in Whisper.
</details>
<details>
<summary>摘要</summary>
文章提到的Whisper在低资源语言表现仍然远不完美。除了低资源语言的培训数据缺乏外，我们还发现了Whisper中的搜索算法有一些限制。为解决这些问题，我们对Whisper进行了进一步的微调和提议了一种改进的解码算法。在越南语言上，我们使用LoRA进行微调Whisper-Tiny，WER值下降38.49，比零基eline微调 setting下降1.45，而与全参数微调相比，这是进一步的下降。此外，使用Filter-Ends和Min Lookahead搜索算法，WER值平均下降2.26个语言中，相比标准搜索算法。这些结果普适到更大的Whisper模型大小。我们还证明了Min Lookahead比标准搜索算法更高效。
</details></li>
</ul>
<hr>
<h2 id="Mixed-Distil-BERT-Code-mixed-Language-Modeling-for-Bangla-English-and-Hindi"><a href="#Mixed-Distil-BERT-Code-mixed-Language-Modeling-for-Bangla-English-and-Hindi" class="headerlink" title="Mixed-Distil-BERT: Code-mixed Language Modeling for Bangla, English, and Hindi"></a>Mixed-Distil-BERT: Code-mixed Language Modeling for Bangla, English, and Hindi</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10272">http://arxiv.org/abs/2309.10272</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Nishat Raihan, Dhiman Goswami, Antara Mahmud</li>
<li>for: 这个论文主要针对的是 code-mixed NLP 挑战，即在文本中混合多种语言的问题。</li>
<li>methods: 这篇论文使用了 BERT 模型，并通过组合 synthetic 数据和实际数据进行预训练。</li>
<li>results: 论文提出了 Tri-Distil-BERT 和 Mixed-Distil-BERT 两种模型，并在多个 NLP 任务上进行了评估，与大型模型 like mBERT 和 XLM-R 进行了比较，得到了竞争力的表现。<details>
<summary>Abstract</summary>
One of the most popular downstream tasks in the field of Natural Language Processing is text classification. Text classification tasks have become more daunting when the texts are code-mixed. Though they are not exposed to such text during pre-training, different BERT models have demonstrated success in tackling Code-Mixed NLP challenges. Again, in order to enhance their performance, Code-Mixed NLP models have depended on combining synthetic data with real-world data. It is crucial to understand how the BERT models' performance is impacted when they are pretrained using corresponding code-mixed languages. In this paper, we introduce Tri-Distil-BERT, a multilingual model pre-trained on Bangla, English, and Hindi, and Mixed-Distil-BERT, a model fine-tuned on code-mixed data. Both models are evaluated across multiple NLP tasks and demonstrate competitive performance against larger models like mBERT and XLM-R. Our two-tiered pre-training approach offers efficient alternatives for multilingual and code-mixed language understanding, contributing to advancements in the field.
</details>
<details>
<summary>摘要</summary>
一种非常受欢迎的下游任务在自然语言处理领域是文本分类。由于文本受到混合的影响，文本分类任务变得更加困难。虽然模型在预训练时没有接触这种文本，但不同的BERT模型在处理混合代码的挑战中表现出色。为了提高 их表现，混合代码NLP模型通常是通过将 sintetic data 与实际数据相结合来进行优化。我们需要了解BERT模型在使用相应的混合代码语言进行预训练时的表现。在这篇论文中，我们介绍了Tri-Distil-BERT和Mixed-Distil-BERT两种模型。Tri-Distil-BERT 是多语言模型，在孟加拉语、英语和希н第语上进行预训练。Mixed-Distil-BERT 是在混合代码数据上细化的模型。两个模型在多个 NLP 任务中表现竞争性，与较大的 mBERT 和 XLM-R 模型相比。我们的两个阶段预训练方法为多语言和混合代码语言理解提供了高效的选择，对于领域的发展做出了贡献。
</details></li>
</ul>
<hr>
<h2 id="What-is-the-Best-Automated-Metric-for-Text-to-Motion-Generation"><a href="#What-is-the-Best-Automated-Metric-for-Text-to-Motion-Generation" class="headerlink" title="What is the Best Automated Metric for Text to Motion Generation?"></a>What is the Best Automated Metric for Text to Motion Generation?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10248">http://arxiv.org/abs/2309.10248</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jordan Voas, Yili Wang, Qixing Huang, Raymond Mooney</li>
<li>for: 本研究旨在系统地研究skeleton-based人体动作生成 task 的评估指标，并提出一种基于多modal BERT 模型的新指标。</li>
<li>methods: 本研究使用了多种自动评估指标，并进行了人类评估来评估指标的含义。</li>
<li>results: 研究发现，现有的评估指标与人类评估之间存在很大的误差，而新提出的指标则与人类评估呈现很高的相似性。<details>
<summary>Abstract</summary>
There is growing interest in generating skeleton-based human motions from natural language descriptions. While most efforts have focused on developing better neural architectures for this task, there has been no significant work on determining the proper evaluation metric. Human evaluation is the ultimate accuracy measure for this task, and automated metrics should correlate well with human quality judgments. Since descriptions are compatible with many motions, determining the right metric is critical for evaluating and designing effective generative models. This paper systematically studies which metrics best align with human evaluations and proposes new metrics that align even better. Our findings indicate that none of the metrics currently used for this task show even a moderate correlation with human judgments on a sample level. However, for assessing average model performance, commonly used metrics such as R-Precision and less-used coordinate errors show strong correlations. Additionally, several recently developed metrics are not recommended due to their low correlation compared to alternatives. We also introduce a novel metric based on a multimodal BERT-like model, MoBERT, which offers strongly human-correlated sample-level evaluations while maintaining near-perfect model-level correlation. Our results demonstrate that this new metric exhibits extensive benefits over all current alternatives.
</details>
<details>
<summary>摘要</summary>
“人工智能生成人体运动动作从自然语言描述中获得的兴趣在增长。大多数努力都在发展更好的神经网络模型来实现这项任务，但没有 significante 的工作关于确定合适的评价标准。人类评价是生成模型的终极准确度测试，自动化 metric 应该与人类评价呈相似性。由于描述可以与多种动作匹配，确定正确的 metric 是评价生成模型的关键。这篇论文系统地研究了哪些 metric 与人类评价最高相关，并提出了新的 metric ，它们与人类评价呈相似性。我们发现，目前用于这项任务的所有 metric 都没有even  moderate 的相关性，但是用于评估平均模型性能的 R-Precision 和 less-used coordinate errors 显示了强相关性。此外，一些最近开发的 metric 不建议使用，因为它们与替代方案相比有较低的相关性。我们还介绍了一种基于多模态 BERT-like 模型的新 metric，MoBERT，它在样本级别上与人类评价呈相似性，同时保持了near-perfect 模型级别的相关性。我们的结果表明，这种新 metric 在所有当前选择中具有广泛的优势。”
</details></li>
</ul>
<hr>
<h2 id="PolicyGPT-Automated-Analysis-of-Privacy-Policies-with-Large-Language-Models"><a href="#PolicyGPT-Automated-Analysis-of-Privacy-Policies-with-Large-Language-Models" class="headerlink" title="PolicyGPT: Automated Analysis of Privacy Policies with Large Language Models"></a>PolicyGPT: Automated Analysis of Privacy Policies with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10238">http://arxiv.org/abs/2309.10238</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenhao Tang, Zhengliang Liu, Chong Ma, Zihao Wu, Yiwei Li, Wei Liu, Dajiang Zhu, Quanzheng Li, Xiang Li, Tianming Liu, Lei Fan</li>
<li>for: This paper aims to develop a text analysis framework for privacy policies, using Large Language Models (LLM) such as ChatGPT and GPT-4.</li>
<li>methods: The framework, called PolicyGPT, uses zero-shot learning to analyze privacy policies and categorize them into 10 different classes.</li>
<li>results: PolicyGPT achieved high accuracy rates on two datasets, with an accuracy rate of 97% on the first dataset and 87% on the second dataset, outperforming baseline models.Here’s the simplified Chinese text:</li>
<li>for: 这篇论文目标是开发一种基于大语言模型（LLM）的隐私政策文本分析框架，用于自动分类隐私政策。</li>
<li>methods: 该框架使用零shot学习方法，使用大语言模型（ChatGPT和GPT-4）对隐私政策进行分类，并分为10个类别。</li>
<li>results: PolicyGPT在两个数据集上表现出色，在第一个数据集上达到了97%的准确率，在第二个数据集上达到了87%的准确率，超过了基线机器学习和神经网络模型的性能。<details>
<summary>Abstract</summary>
Privacy policies serve as the primary conduit through which online service providers inform users about their data collection and usage procedures. However, in a bid to be comprehensive and mitigate legal risks, these policy documents are often quite verbose. In practical use, users tend to click the Agree button directly rather than reading them carefully. This practice exposes users to risks of privacy leakage and legal issues. Recently, the advent of Large Language Models (LLM) such as ChatGPT and GPT-4 has opened new possibilities for text analysis, especially for lengthy documents like privacy policies. In this study, we investigate a privacy policy text analysis framework PolicyGPT based on the LLM. This framework was tested using two datasets. The first dataset comprises of privacy policies from 115 websites, which were meticulously annotated by legal experts, categorizing each segment into one of 10 classes. The second dataset consists of privacy policies from 304 popular mobile applications, with each sentence manually annotated and classified into one of another 10 categories. Under zero-shot learning conditions, PolicyGPT demonstrated robust performance. For the first dataset, it achieved an accuracy rate of 97%, while for the second dataset, it attained an 87% accuracy rate, surpassing that of the baseline machine learning and neural network models.
</details>
<details>
<summary>摘要</summary>
Recently, the development of Large Language Models (LLM) such as ChatGPT and GPT-4 has opened up new possibilities for text analysis, particularly for lengthy documents like privacy policies. In this study, we investigate a privacy policy text analysis framework called PolicyGPT, which is based on an LLM.We tested PolicyGPT using two datasets. The first dataset consisted of privacy policies from 115 websites, which were carefully annotated by legal experts and categorized into one of 10 classes. The second dataset included privacy policies from 304 popular mobile applications, with each sentence manually annotated and classified into one of another 10 categories.Under zero-shot learning conditions, PolicyGPT demonstrated robust performance. For the first dataset, it achieved an accuracy rate of 97%, while for the second dataset, it attained an 87% accuracy rate, outperforming baseline machine learning and neural network models.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/19/cs.CL_2023_09_19/" data-id="clpztdne400c6es88bet99mux" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/45/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/44/">44</a><a class="page-number" href="/page/45/">45</a><span class="page-number current">46</span><a class="page-number" href="/page/47/">47</a><a class="page-number" href="/page/48/">48</a><span class="space">&hellip;</span><a class="page-number" href="/page/98/">98</a><a class="extend next" rel="next" href="/page/47/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">67</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">82</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">147</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
