
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/24/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.CV_2023_10_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/20/cs.CV_2023_10_20/" class="article-date">
  <time datetime="2023-10-20T13:00:00.000Z" itemprop="datePublished">2023-10-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/20/cs.CV_2023_10_20/">cs.CV - 2023-10-20</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Dual-Stream-Neural-Network-Explains-the-Functional-Segregation-of-Dorsal-and-Ventral-Visual-Pathways-in-Human-Brains"><a href="#A-Dual-Stream-Neural-Network-Explains-the-Functional-Segregation-of-Dorsal-and-Ventral-Visual-Pathways-in-Human-Brains" class="headerlink" title="A Dual-Stream Neural Network Explains the Functional Segregation of Dorsal and Ventral Visual Pathways in Human Brains"></a>A Dual-Stream Neural Network Explains the Functional Segregation of Dorsal and Ventral Visual Pathways in Human Brains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13849">http://arxiv.org/abs/2310.13849</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/minkyu-choi04/dualstreambrains">https://github.com/minkyu-choi04/dualstreambrains</a></li>
<li>paper_authors: Minkyu Choi, Kuan Han, Xiaokai Wang, Yizhen Zhang, Zhongming Liu</li>
<li>for: 模仿人类视系统中的两条平行路径，该模型用于空间处理和物体识别。</li>
<li>methods: 使用两个分支的卷积神经网络（CNN）模型，分别模仿人脑中的脊梁和轴索路径。</li>
<li>results: 与人脑处理同一影片时，模型的两个分支具有不同的学习目标和表征，主要受到视觉注意力和物体识别的各自目标的影响，而不是特定的抑制或选择性。<details>
<summary>Abstract</summary>
The human visual system uses two parallel pathways for spatial processing and object recognition. In contrast, computer vision systems tend to use a single feedforward pathway, rendering them less robust, adaptive, or efficient than human vision. To bridge this gap, we developed a dual-stream vision model inspired by the human eyes and brain. At the input level, the model samples two complementary visual patterns to mimic how the human eyes use magnocellular and parvocellular retinal ganglion cells to separate retinal inputs to the brain. At the backend, the model processes the separate input patterns through two branches of convolutional neural networks (CNN) to mimic how the human brain uses the dorsal and ventral cortical pathways for parallel visual processing. The first branch (WhereCNN) samples a global view to learn spatial attention and control eye movements. The second branch (WhatCNN) samples a local view to represent the object around the fixation. Over time, the two branches interact recurrently to build a scene representation from moving fixations. We compared this model with the human brains processing the same movie and evaluated their functional alignment by linear transformation. The WhereCNN and WhatCNN branches were found to differentially match the dorsal and ventral pathways of the visual cortex, respectively, primarily due to their different learning objectives. These model-based results lead us to speculate that the distinct responses and representations of the ventral and dorsal streams are more influenced by their distinct goals in visual attention and object recognition than by their specific bias or selectivity in retinal inputs. This dual-stream model takes a further step in brain-inspired computer vision, enabling parallel neural networks to actively explore and understand the visual surroundings.
</details>
<details>
<summary>摘要</summary>
人类视系统使用两条平行的路径进行空间处理和物体识别。相比之下，计算机视系统通常使用单个径向的Feedforward路径，使其比人视不太强大、适应性或效率。为了bridging这个差距，我们开发了基于人眼和大脑的双流视模型。在输入级别，模型从两个不同的视觉模式中采样两个协同的视觉特征，以模仿人眼使用大脑皮层ganglion cells将视觉输入分配给大脑。在后端，模型通过两个分支的卷积神经网络（CNN）处理分配的输入，以模仿人大脑使用脑干和脑膜两个路径进行平行的视觉处理。首支分支（WhereCNN）在全视图中学习空间注意力和控制眼动。第二支分支（WhatCNN）在fixation周围的视觉中学习物体特征。随着时间的推移，两支分支之间进行交互性的回归，从移动的眼动中构建场景表示。我们与人脑处理相同电影的结果进行对比，并评估其功能对齐性。WhereCNN和WhatCNN分支在功能上与人脑的视觉干道和脑膜干道相对应，尤其是由于它们的不同学习目标。这些模型基于的结果引导我们推断，人脑的视觉干道和脑膜干道的不同响应和表示是更多地受到它们的特定目标和需求而决定的，而不是它们特定的偏好或选择性在视觉输入中。这种双流模型在人脑适应计算机视觉方面又一步进展，使得平行神经网络能够活动地探索和理解视觉环境。
</details></li>
</ul>
<hr>
<h2 id="Normalizing-flow-based-deep-variational-Bayesian-network-for-seismic-multi-hazards-and-impacts-estimation-from-InSAR-imagery"><a href="#Normalizing-flow-based-deep-variational-Bayesian-network-for-seismic-multi-hazards-and-impacts-estimation-from-InSAR-imagery" class="headerlink" title="Normalizing flow-based deep variational Bayesian network for seismic multi-hazards and impacts estimation from InSAR imagery"></a>Normalizing flow-based deep variational Bayesian network for seismic multi-hazards and impacts estimation from InSAR imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13805">http://arxiv.org/abs/2310.13805</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuechun Li, Paula M. Burgi, Wei Ma, Hae Young Noh, David J. Wald, Susu Xu</li>
<li>for: 本研究旨在提供高精度的onsite灾害估算，以便快速和有效地进行post-灾应对。</li>
<li>methods: 本研究使用抗干扰synthetic aperture radar（InSAR）数据，并提出了一种新的Stochastic variational inference with normalizing flows方法，可以同时估算多种不见的灾害和影响。</li>
<li>results: 研究表明，该方法可以减少由干扰和复杂的信号引起的估算误差，并提供高精度的onsite灾害估算结果。<details>
<summary>Abstract</summary>
Onsite disasters like earthquakes can trigger cascading hazards and impacts, such as landslides and infrastructure damage, leading to catastrophic losses; thus, rapid and accurate estimates are crucial for timely and effective post-disaster responses. Interferometric Synthetic aperture radar (InSAR) data is important in providing high-resolution onsite information for rapid hazard estimation. Most recent methods using InSAR imagery signals predict a single type of hazard and thus often suffer low accuracy due to noisy and complex signals induced by co-located hazards, impacts, and irrelevant environmental changes (e.g., vegetation changes, human activities). We introduce a novel stochastic variational inference with normalizing flows derived to jointly approximate posteriors of multiple unobserved hazards and impacts from noisy InSAR imagery.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Data-Free-Knowledge-Distillation-Using-Adversarially-Perturbed-OpenGL-Shader-Images"><a href="#Data-Free-Knowledge-Distillation-Using-Adversarially-Perturbed-OpenGL-Shader-Images" class="headerlink" title="Data-Free Knowledge Distillation Using Adversarially Perturbed OpenGL Shader Images"></a>Data-Free Knowledge Distillation Using Adversarially Perturbed OpenGL Shader Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13782">http://arxiv.org/abs/2310.13782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Logan Frank, Jim Davis<br>for: This paper focuses on the problem of knowledge distillation (KD) in the absence of the original training data, which is known as “data-free” KD.methods: The proposed approach uses unnatural OpenGL images and large amounts of data augmentation, along with adversarial attacks, to train a student network.results: The proposed method achieves state-of-the-art results for a variety of datasets&#x2F;networks and is more stable than existing generator-based data-free KD methods.Here’s the text in Simplified Chinese:for: 本文关注无原始训练数据的知识储备（KD）问题，即“数据空”KD。methods: 提议的方法使用不同的OpenGL图像和大量数据增强，以及抗击攻击，来训练学生网络。results: 提议的方法在多个数据集&#x2F;网络上达到了状态的最佳结果，并且比现有的生成器基于的数据空KD方法更稳定。<details>
<summary>Abstract</summary>
Knowledge distillation (KD) has been a popular and effective method for model compression. One important assumption of KD is that the original training dataset is always available. However, this is not always the case due to privacy concerns and more. In recent years, "data-free" KD has emerged as a growing research topic which focuses on the scenario of performing KD when no data is provided. Many methods rely on a generator network to synthesize examples for distillation (which can be difficult to train) and can frequently produce images that are visually similar to the original dataset, which raises questions surrounding whether privacy is completely preserved. In this work, we propose a new approach to data-free KD that utilizes unnatural OpenGL images, combined with large amounts of data augmentation and adversarial attacks, to train a student network. We demonstrate that our approach achieves state-of-the-art results for a variety of datasets/networks and is more stable than existing generator-based data-free KD methods. Source code will be available in the future.
</details>
<details>
<summary>摘要</summary>
知识塑化（KD）是一种受欢迎且有效的模型压缩方法。然而，KD中一个重要假设是原始训练集总是可用的。然而，这并不总是情况，特别是由于隐私问题和其他因素。在过去几年，无数据KD（data-free KD）作为一个快速发展的研究领域而出现。许多方法利用生成器网络生成例子进行塑化（可能困难于训练），并且可能会生成与原始数据集相似的图像，这引发了隐私是否完全保持的问题。在这项工作中，我们提出了一种新的无数据KD方法，利用不自然的OpenGL图像，结合大量数据增强和对抗攻击，训练学生网络。我们示出了我们的方法可以在多个数据集和网络上实现状态之最的结果，并且更稳定于现有的生成器基于的无数据KD方法。未来我们计划将源代码公开。
</details></li>
</ul>
<hr>
<h2 id="TexFusion-Synthesizing-3D-Textures-with-Text-Guided-Image-Diffusion-Models"><a href="#TexFusion-Synthesizing-3D-Textures-with-Text-Guided-Image-Diffusion-Models" class="headerlink" title="TexFusion: Synthesizing 3D Textures with Text-Guided Image Diffusion Models"></a>TexFusion: Synthesizing 3D Textures with Text-Guided Image Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13772">http://arxiv.org/abs/2310.13772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianshi Cao, Karsten Kreis, Sanja Fidler, Nicholas Sharp, Kangxue Yin</li>
<li>for: 这 paper 的目的是提出一种新的 texture synthesis 方法，用于给定的 3D geometry Synthesize textures.</li>
<li>methods: 这 paper 使用了 large-scale text-guided image diffusion models 来实现 texture synthesis. 它不同于 recent works 通过使用 2D text-to-image diffusion models 来缓慢和脆弱的优化 процесс来 distill 3D objects.</li>
<li>results:  TexFusion 可以生成高质量、多样性和全局一致的 textures. 它可以efficiently generate diverse, high quality and globally coherent textures.<details>
<summary>Abstract</summary>
We present TexFusion (Texture Diffusion), a new method to synthesize textures for given 3D geometries, using large-scale text-guided image diffusion models. In contrast to recent works that leverage 2D text-to-image diffusion models to distill 3D objects using a slow and fragile optimization process, TexFusion introduces a new 3D-consistent generation technique specifically designed for texture synthesis that employs regular diffusion model sampling on different 2D rendered views. Specifically, we leverage latent diffusion models, apply the diffusion model's denoiser on a set of 2D renders of the 3D object, and aggregate the different denoising predictions on a shared latent texture map. Final output RGB textures are produced by optimizing an intermediate neural color field on the decodings of 2D renders of the latent texture. We thoroughly validate TexFusion and show that we can efficiently generate diverse, high quality and globally coherent textures. We achieve state-of-the-art text-guided texture synthesis performance using only image diffusion models, while avoiding the pitfalls of previous distillation-based methods. The text-conditioning offers detailed control and we also do not rely on any ground truth 3D textures for training. This makes our method versatile and applicable to a broad range of geometry and texture types. We hope that TexFusion will advance AI-based texturing of 3D assets for applications in virtual reality, game design, simulation, and more.
</details>
<details>
<summary>摘要</summary>
我们介绍TexFusion（Texture Diffusion），一种新的方法用于给定3D геометрии的文本渲染。相比之前的工作，利用2D文本到图像扩散模型来降低3D对象的整合过程，TexFusion引入了一种专门为文本生成设计的3D一致的生成技术。具体来说，我们利用潜在扩散模型，对2D渲染视图中的3D对象应用扩散模型的降噪器，并将不同视图中的降噪器预测集成到共享潜在文本地图上。最终输出的RGB文本是通过优化2D渲染视图中的扩散预测来生成的。我们详细验证了TexFusion，并证明了我们可以高效地生成多样化、高质量和全局一致的文本。我们在只使用图像扩散模型的情况下，超越了先前的滤波器基本方法，并且不需要任何基于真实3D文本的训练。这使得我们的方法通用和可应用于各种几何和文本类型。我们希望TexFusion能够推动AI在虚拟现实、游戏设计、模拟和更多应用中的3D资产 текстури化。
</details></li>
</ul>
<hr>
<h2 id="PACE-Human-and-Camera-Motion-Estimation-from-in-the-wild-Videos"><a href="#PACE-Human-and-Camera-Motion-Estimation-from-in-the-wild-Videos" class="headerlink" title="PACE: Human and Camera Motion Estimation from in-the-wild Videos"></a>PACE: Human and Camera Motion Estimation from in-the-wild Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13768">http://arxiv.org/abs/2310.13768</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammed Kocabas, Ye Yuan, Pavlo Molchanov, Yunrong Guo, Michael J. Black, Otmar Hilliges, Jan Kautz, Umar Iqbal</li>
<li>for: 估计人体运动在全景视频中</li>
<li>methods: 提议一种结合人体动作和摄像机动作的全球估计方法，通过对人体动作和背景特征的结合来分离人体和摄像机动作。不同于现有方法使用SLAM作为初始化，我们提议在估计人体和摄像机动作时紧密地结合SLAM和人体动作约束。</li>
<li>results: 对比现有方法，我们的方法在人体和摄像机动作估计中具有显著的改进，并且提出了一种适合批处理的运动假设，使我们的方法更加高效。此外，我们还提出了一个适合评估摄像机动作的实验室数据集，并在实验中证明了我们的方法的优越性。<details>
<summary>Abstract</summary>
We present a method to estimate human motion in a global scene from moving cameras. This is a highly challenging task due to the coupling of human and camera motions in the video. To address this problem, we propose a joint optimization framework that disentangles human and camera motions using both foreground human motion priors and background scene features. Unlike existing methods that use SLAM as initialization, we propose to tightly integrate SLAM and human motion priors in an optimization that is inspired by bundle adjustment. Specifically, we optimize human and camera motions to match both the observed human pose and scene features. This design combines the strengths of SLAM and motion priors, which leads to significant improvements in human and camera motion estimation. We additionally introduce a motion prior that is suitable for batch optimization, making our approach significantly more efficient than existing approaches. Finally, we propose a novel synthetic dataset that enables evaluating camera motion in addition to human motion from dynamic videos. Experiments on the synthetic and real-world RICH datasets demonstrate that our approach substantially outperforms prior art in recovering both human and camera motions.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法来估计人体动作在全景视频中。这是一个非常困难的任务，因为人体和摄像机的运动在视频中相互关联。为解决这个问题，我们提议一个共同优化框架，通过对人体动作和摄像机运动进行分离，使用前景人体动作假设和背景场景特征。不同于现有的方法使用SLAM作为初始化，我们提议在SLAM和人体动作假设之间进行紧密的集成，以便同时优化人体和摄像机的运动。这种设计 combinesthe strengths of SLAM and motion priors, leading to significant improvements in human and camera motion estimation. In addition, we introduce a motion prior suitable for batch optimization, making our approach much more efficient than existing methods. Finally, we propose a novel synthetic dataset that enables evaluating camera motion in addition to human motion from dynamic videos. Experimental results on the synthetic and real-world RICH datasets show that our approach significantly outperforms prior art in estimating both human and camera motions.
</details></li>
</ul>
<hr>
<h2 id="U-BEV-Height-aware-Bird’s-Eye-View-Segmentation-and-Neural-Map-based-Relocalization"><a href="#U-BEV-Height-aware-Bird’s-Eye-View-Segmentation-and-Neural-Map-based-Relocalization" class="headerlink" title="U-BEV: Height-aware Bird’s-Eye-View Segmentation and Neural Map-based Relocalization"></a>U-BEV: Height-aware Bird’s-Eye-View Segmentation and Neural Map-based Relocalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13766">http://arxiv.org/abs/2310.13766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Boscolo Camiletto, Alfredo Bochicchio, Alexander Liniger, Dengxin Dai, Abel Gawel</li>
<li>for: 本研究旨在提高智能汽车的重新本地化精度，当GPS接收强度不足或感知器基本本地化失败时。</li>
<li>methods: 本文使用鸟瞰视图（BEV）分割技术来估算当地场景的准确出现，并可以帮助智能汽车重新本地化。然而，BEV方法的缺点是需要大量计算来利用地理约束。本文提出了U-BEV模型，它是基于U-Net架构的BEV分割模型，可以让BEV在多层高度上进行Scene理解。这种扩展可以提高U-BEV的性能，并且和其他计算相同的BEV方法相比，提高1.7到2.8 mIoU。</li>
<li>results: 本研究结果表明，将编码的神经网络BEV与可导分配模板匹配器结合使用，可以实现高精度的重新本地化。与类似计算复杂度的 transformer-based BEV方法相比，本方法提高了重新本地化精度。在nuScenes数据集上，本方法的召回精度高于26%。<details>
<summary>Abstract</summary>
Efficient relocalization is essential for intelligent vehicles when GPS reception is insufficient or sensor-based localization fails. Recent advances in Bird's-Eye-View (BEV) segmentation allow for accurate estimation of local scene appearance and in turn, can benefit the relocalization of the vehicle. However, one downside of BEV methods is the heavy computation required to leverage the geometric constraints. This paper presents U-BEV, a U-Net inspired architecture that extends the current state-of-the-art by allowing the BEV to reason about the scene on multiple height layers before flattening the BEV features. We show that this extension boosts the performance of the U-BEV by up to 4.11 IoU. Additionally, we combine the encoded neural BEV with a differentiable template matcher to perform relocalization on neural SD-map data. The model is fully end-to-end trainable and outperforms transformer-based BEV methods of similar computational complexity by 1.7 to 2.8 mIoU and BEV-based relocalization by over 26% Recall Accuracy on the nuScenes dataset.
</details>
<details>
<summary>摘要</summary>
efficient 地方化是智能车辆当GPS接收不够或感知器基于的地方化失败时的关键。最近的鸟瞰视图（BEV）分割技术允许精确地估计当地场景的外观，从而为车辆的重新定位提供了利器。然而，BEV方法的一个缺点是需要大量的计算来利用地形约束。这篇论文提出了U-BEV架构，它是基于U-Net的建议，允许BEV理解场景在多个高层次上进行推理，然后将BEV特征级别。我们显示，这种扩展可以提高U-BEV的性能，最多提高4.11 IoU。此外，我们将编码的神经网络BEV与演算器模板匹配器结合，以实现基于神经网络的SD-map数据重新定位。该模型是完全端到端训练的，与类似计算复杂度的转换器基于BEV方法相比，提高了1.7到2.8 mIoU，并超过了26%的Recall精度在nuScenes数据集。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-sleep-stage-classification-how-age-and-early-late-sleep-affects-classification-performance"><a href="#Evaluating-sleep-stage-classification-how-age-and-early-late-sleep-affects-classification-performance" class="headerlink" title="Evaluating sleep-stage classification: how age and early-late sleep affects classification performance"></a>Evaluating sleep-stage classification: how age and early-late sleep affects classification performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13754">http://arxiv.org/abs/2310.13754</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eugenia Moris, Ignacio Larrabide</li>
<li>for:  automatic sleep-stage classification method</li>
<li>methods: Wavelets for feature extraction, Random Forest for classification</li>
<li>results: the performance of the classifier is affected by the age of the subjects and the moment of sleep, with some sleep stages improving and others worsening.Here’s the full translation of the abstract in Simplified Chinese:</li>
<li>for:  automatic sleep-stage classification method</li>
<li>methods: 使用浪波lets для特征提取，Random Forest进行分类</li>
<li>results: 论文发现，参与者年龄和睡眠时间点会影响自动分类器的性能，一些睡眠阶段的分类得到改善，而另一些则得到恶化。<details>
<summary>Abstract</summary>
Sleep stage classification is a common method used by experts to monitor the quantity and quality of sleep in humans, but it is a time-consuming and labour-intensive task with high inter- and intra-observer variability. Using Wavelets for feature extraction and Random Forest for classification, an automatic sleep-stage classification method was sought and assessed. The age of the subjects, as well as the moment of sleep (early-night and late-night), were confronted to the performance of the classifier. From this study, we observed that these variables do affect the automatic model performance, improving the classification of some sleep stages and worsening others.
</details>
<details>
<summary>摘要</summary>
睡眠阶段分类是一种常用的专业人员用于评估人类睡眠量和质量的方法，但是它是一项时间consuming和劳动密集的任务，具有高度的内部和外部观察者变化。使用浪涌来提取特征和随机森林来分类，一种自动睡眠阶段分类方法被寻找并评估。研究发现，试验者的年龄以及睡眠的时间点（晚上早些和晚上较晚）对自动模型的性能产生影响，改善一些睡眠阶段的分类，而对其他阶段的分类则有所恶化。
</details></li>
</ul>
<hr>
<h2 id="Reference-based-Restoration-of-Digitized-Analog-Videotapes"><a href="#Reference-based-Restoration-of-Digitized-Analog-Videotapes" class="headerlink" title="Reference-based Restoration of Digitized Analog Videotapes"></a>Reference-based Restoration of Digitized Analog Videotapes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14926">http://arxiv.org/abs/2310.14926</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/miccunifi/tape">https://github.com/miccunifi/tape</a></li>
<li>paper_authors: Lorenzo Agnolucci, Leonardo Galteri, Marco Bertini, Alberto Del Bimbo</li>
<li>for:  restore analog videotapes</li>
<li>methods: CLIP-based artifact detection, Swin-UNet network with Multi-Reference Spatial Feature Fusion (MRSFF) blocks</li>
<li>results: effective restoration of degraded analog videotapes, outperforms other state-of-the-art methodsHere’s the full text in Simplified Chinese:</li>
<li>for:  restore analog videotapes</li>
<li>methods: CLIP-based artifact detection, Swin-UNet network with Multi-Reference Spatial Feature Fusion (MRSFF) blocks</li>
<li>results: 高效地 Restore 了受损的 analog videotapes, 比其他状态的方法更高效I hope that helps!<details>
<summary>Abstract</summary>
Analog magnetic tapes have been the main video data storage device for several decades. Videos stored on analog videotapes exhibit unique degradation patterns caused by tape aging and reader device malfunctioning that are different from those observed in film and digital video restoration tasks. In this work, we present a reference-based approach for the resToration of digitized Analog videotaPEs (TAPE). We leverage CLIP for zero-shot artifact detection to identify the cleanest frames of each video through textual prompts describing different artifacts. Then, we select the clean frames most similar to the input ones and employ them as references. We design a transformer-based Swin-UNet network that exploits both neighboring and reference frames via our Multi-Reference Spatial Feature Fusion (MRSFF) blocks. MRSFF blocks rely on cross-attention and attention pooling to take advantage of the most useful parts of each reference frame. To address the absence of ground truth in real-world videos, we create a synthetic dataset of videos exhibiting artifacts that closely resemble those commonly found in analog videotapes. Both quantitative and qualitative experiments show the effectiveness of our approach compared to other state-of-the-art methods. The code, the model, and the synthetic dataset are publicly available at https://github.com/miccunifi/TAPE.
</details>
<details>
<summary>摘要</summary>
传统的材料磁带是数据存储设备的主要设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设备数据存储设�
</details></li>
</ul>
<hr>
<h2 id="Localizing-and-Editing-Knowledge-in-Text-to-Image-Generative-Models"><a href="#Localizing-and-Editing-Knowledge-in-Text-to-Image-Generative-Models" class="headerlink" title="Localizing and Editing Knowledge in Text-to-Image Generative Models"></a>Localizing and Editing Knowledge in Text-to-Image Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13730">http://arxiv.org/abs/2310.13730</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samyadeep Basu, Nanxuan Zhao, Vlad Morariu, Soheil Feizi, Varun Manjunatha</li>
<li>for: 这篇论文的目的是研究文本到图像生成模型中的知识储存和传递问题。</li>
<li>methods: 这篇论文使用了 causal mediation analysis 方法来分析文本到图像模型中不同视觉特征知识的储存和传递。具体来说， authors 使用了 UNet 和文本编码器来跟踪不同视觉特征知识的传递，并发现不同视觉特征知识不是孤立在具体组件中，而是分布在文本到图像模型中的一系列组件中。</li>
<li>results: 这篇论文的结果表明，CLIP 文本编码器在实际的文本到图像模型中只有一个 causal state，并且这个 causal state 是文本中最后一个属性token 的第一个自注意层。这与其他语言模型的 causal state 不同，后者通常是中间的 MLP 层。基于这个发现， authors 提出了一种快速、无需数据的模型修改方法 Diff-QuickFix，可以快速编辑（简化）文本到图像模型中的概念。<details>
<summary>Abstract</summary>
Text-to-Image Diffusion Models such as Stable-Diffusion and Imagen have achieved unprecedented quality of photorealism with state-of-the-art FID scores on MS-COCO and other generation benchmarks. Given a caption, image generation requires fine-grained knowledge about attributes such as object structure, style, and viewpoint amongst others. Where does this information reside in text-to-image generative models? In our paper, we tackle this question and understand how knowledge corresponding to distinct visual attributes is stored in large-scale text-to-image diffusion models. We adapt Causal Mediation Analysis for text-to-image models and trace knowledge about distinct visual attributes to various (causal) components in the (i) UNet and (ii) text-encoder of the diffusion model. In particular, we show that unlike generative large-language models, knowledge about different attributes is not localized in isolated components, but is instead distributed amongst a set of components in the conditional UNet. These sets of components are often distinct for different visual attributes. Remarkably, we find that the CLIP text-encoder in public text-to-image models such as Stable-Diffusion contains only one causal state across different visual attributes, and this is the first self-attention layer corresponding to the last subject token of the attribute in the caption. This is in stark contrast to the causal states in other language models which are often the mid-MLP layers. Based on this observation of only one causal state in the text-encoder, we introduce a fast, data-free model editing method Diff-QuickFix which can effectively edit concepts in text-to-image models. DiffQuickFix can edit (ablate) concepts in under a second with a closed-form update, providing a significant 1000x speedup and comparable editing performance to existing fine-tuning based editing methods.
</details>
<details>
<summary>摘要</summary>
文本到图像扩散模型，如稳定扩散和图像，在最新的MS-COCO和其他生成benchmark中实现了无 precedent的图像真实度水平。给出caption，图像生成需要细化的知识，包括对象结构、风格、视角等多个属性。在这些模型中，这些信息存储在哪里？在我们的论文中，我们解答这个问题，了解扩散模型中对不同视觉属性的知识是如何分布的。我们采用了 causal mediation analysis，跟踪扩散模型中对不同视觉属性的知识是如何分布在（i）UNet和（ii）文本编码器中。尤其是，我们发现在大型文本到图像扩散模型中，不同属性的知识不是孤立分布在特定的组件中，而是分布在一系列组件中。这些组件frequently是不同的视觉属性。另外，我们发现在公共的文本到图像模型中，如稳定扩散，CLIP文本编码器只有一个 causal state，这是最后一个主题token的第一个自注意力层。这与其他语言模型的 causal state不同，通常是mid-MLP层。基于这一观察，我们引入了一种快速、无数据的模型修改方法Diff-QuickFix，可以快速编辑（抹除）文本到图像模型中的概念。Diff-QuickFix可以在下一秒内进行数据准确的更新，提供1000倍的速度增加和与现有精细调整方法相当的编辑性能。
</details></li>
</ul>
<hr>
<h2 id="Using-Human-like-Mechanism-to-Weaken-Effect-of-Pre-training-Weight-Bias-in-Face-Recognition-Convolutional-Neural-Network"><a href="#Using-Human-like-Mechanism-to-Weaken-Effect-of-Pre-training-Weight-Bias-in-Face-Recognition-Convolutional-Neural-Network" class="headerlink" title="Using Human-like Mechanism to Weaken Effect of Pre-training Weight Bias in Face-Recognition Convolutional Neural Network"></a>Using Human-like Mechanism to Weaken Effect of Pre-training Weight Bias in Face-Recognition Convolutional Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13674">http://arxiv.org/abs/2310.13674</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haojiang Ying, Yi-Fan Li, Yiyang Chen</li>
<li>for: 这个研究的目的是解释人工智能中的卷积神经网络（CNN）如何模仿人类的认知机制。</li>
<li>methods: 研究者使用了4种广泛使用的CNN模型（AlexNet、VGG11、VGG13和VGG16），通过转移学习进行情感值分类任务。与人类数据进行比较，研究发现这些CNN模型在一定程度上模仿人类的认知方式。基于神经科学和行为数据，研究者还对AlexNet进行了更新，使其更像人类的认知。</li>
<li>results: 研究发现，更新后的FE-AlexNet在情感值分类任务中表现更好，并且与人类认知更相似。这些结果还揭示了CNN模型的计算机制。此外，这项研究还提供了一种新的理解和改进CNN性能的方法，基于人类数据。<details>
<summary>Abstract</summary>
Convolutional neural network (CNN), as an important model in artificial intelligence, has been widely used and studied in different disciplines. The computational mechanisms of CNNs are still not fully revealed due to the their complex nature. In this study, we focused on 4 extensively studied CNNs (AlexNet, VGG11, VGG13, and VGG16) which has been analyzed as human-like models by neuroscientists with ample evidence. We trained these CNNs to emotion valence classification task by transfer learning. Comparing their performance with human data, the data unveiled that these CNNs would partly perform as human does. We then update the object-based AlexNet using self-attention mechanism based on neuroscience and behavioral data. The updated FE-AlexNet outperformed all the other tested CNNs and closely resembles human perception. The results further unveil the computational mechanisms of these CNNs. Moreover, this study offers a new paradigm to better understand and improve CNN performance via human data.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（CNN）是人工智能中一种重要的模型，在不同领域都得到了广泛的应用和研究。然而，它们的计算机制仍然没有完全揭示，因为它们的复杂性。在本研究中，我们选择了4种广泛采用和研究的 CNN（AlexNet、VGG11、VGG13和VGG16），由神经科学家作为人类模型进行分析，并有丰富的证据。我们使用传输学习训练这些 CNN 进行情感值分类任务。与人类数据进行比较，数据表明这些 CNN 在一定程度上会 acted like humans do。然后，我们将 AlexNet 更新为基于自注意力机制的 FE-AlexNet，并证明它在所有测试 CNN 中表现最佳，并且与人类感知很相似。这些结果进一步揭示了这些 CNN 的计算机制，并提供了一种新的方法来更好地理解和改进 CNN 性能。
</details></li>
</ul>
<hr>
<h2 id="ARNIQA-Learning-Distortion-Manifold-for-Image-Quality-Assessment"><a href="#ARNIQA-Learning-Distortion-Manifold-for-Image-Quality-Assessment" class="headerlink" title="ARNIQA: Learning Distortion Manifold for Image Quality Assessment"></a>ARNIQA: Learning Distortion Manifold for Image Quality Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14918">http://arxiv.org/abs/2310.14918</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/miccunifi/arniqa">https://github.com/miccunifi/arniqa</a></li>
<li>paper_authors: Lorenzo Agnolucci, Leonardo Galteri, Marco Bertini, Alberto Del Bimbo</li>
<li>for: 本研究旨在开发一种无需参考图像的图像质量评估方法，以匹配人类受众的识别。</li>
<li>methods: 我们提出了一种自助学习方法，名为ARNIQA，它通过学习图像扭曲映射来获得图像质量表示。</li>
<li>results: 我们的方法在多个数据集上达到了当今最佳性能，并且在数据效率、通用性和稳定性等方面也表现出优异。代码和模型可以在 GitHub 上获取。<details>
<summary>Abstract</summary>
No-Reference Image Quality Assessment (NR-IQA) aims to develop methods to measure image quality in alignment with human perception without the need for a high-quality reference image. In this work, we propose a self-supervised approach named ARNIQA (leArning distoRtion maNifold for Image Quality Assessment) for modeling the image distortion manifold to obtain quality representations in an intrinsic manner. First, we introduce an image degradation model that randomly composes ordered sequences of consecutively applied distortions. In this way, we can synthetically degrade images with a large variety of degradation patterns. Second, we propose to train our model by maximizing the similarity between the representations of patches of different images distorted equally, despite varying content. Therefore, images degraded in the same manner correspond to neighboring positions within the distortion manifold. Finally, we map the image representations to the quality scores with a simple linear regressor, thus without fine-tuning the encoder weights. The experiments show that our approach achieves state-of-the-art performance on several datasets. In addition, ARNIQA demonstrates improved data efficiency, generalization capabilities, and robustness compared to competing methods. The code and the model are publicly available at https://github.com/miccunifi/ARNIQA.
</details>
<details>
<summary>摘要</summary>
NR-IQA（无参考图像质量评估）的目标是开发一种基于人类感知的图像质量评估方法，不需要高质量的参考图像。在这种工作中，我们提出了一种自我超级vised方法 named ARNIQA（扩展的Random Image Distoration Manifold for Image Quality Assessment），用于模型图像扭曲 manifold 以获取内在的质量表示。首先，我们引入了一种图像抑制模型，可以随机排序 consecutively applied distortions 的序列。这样，我们可以Synthesize 图像的多种抑制模式。其次，我们提议在训练模型时，通过将不同图像的 patches 的表示相似化，来实现图像在同样的抑制模式下的匹配。因此，具有同样的抑制模式的图像将在扭曲 manifold 中的不同位置相对应。最后，我们将图像表示映射到质量分数的简单线性回归器，这样无需调整编码器的参数。实验结果显示，我们的方法可以在多个数据集上达到状态之最的性能。此外，ARNIQA 还表现出了更好的数据使用效率、通用能力和Robustness 等特点，相比其他方法。模型和代码可以在 <https://github.com/miccunifi/ARNIQA> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-based-Change-Detection-with-Spaceborne-Hyperspectral-PRISMA-data"><a href="#Deep-Learning-based-Change-Detection-with-Spaceborne-Hyperspectral-PRISMA-data" class="headerlink" title="Deep-Learning-based Change Detection with Spaceborne Hyperspectral PRISMA data"></a>Deep-Learning-based Change Detection with Spaceborne Hyperspectral PRISMA data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13627">http://arxiv.org/abs/2310.13627</a></li>
<li>repo_url: None</li>
<li>paper_authors: J. F. Amieva, A. Austoni, M. A. Brovelli, L. Ansalone, P. Naylor, F. Serva, B. Le Saux<br>for:This paper is written for environmental monitoring and disaster management, as well as other sectors where change detection (CD) is applied.methods:The paper uses both standard and deep-learning (DL) CD methods, as well as a pipeline starting from coregistration, followed by CD with a full-spectrum algorithm, and a DL network developed for optical data.results:The paper finds that changes in vegetation and built environments are well captured using the proposed methods, and that the spectral information is valuable for identifying subtle changes. However, the paper also notes that atmospheric effects and the lack of reliable ground truth present a major challenge to hyperspectral CD.<details>
<summary>Abstract</summary>
Change detection (CD) methods have been applied to optical data for decades, while the use of hyperspectral data with a fine spectral resolution has been rarely explored. CD is applied in several sectors, such as environmental monitoring and disaster management. Thanks to the PRecursore IperSpettrale della Missione operativA (PRISMA), hyperspectral-from-space CD is now possible. In this work, we apply standard and deep-learning (DL) CD methods to different targets, from natural to urban areas. We propose a pipeline starting from coregistration, followed by CD with a full-spectrum algorithm and by a DL network developed for optical data. We find that changes in vegetation and built environments are well captured. The spectral information is valuable to identify subtle changes and the DL methods are less affected by noise compared to the statistical method, but atmospheric effects and the lack of reliable ground truth represent a major challenge to hyperspectral CD.
</details>
<details>
<summary>摘要</summary>
CD方法（变化检测方法）在光学数据中应用了数十年，而使用高分谱数据的使用则rarely explored。CD方法在各个领域，如环境监测和灾害管理中使用。due to the PRecursore IperSpettrale della Missione operativA (PRISMA), hyperspectral-from-space CD现在可能。在这种工作中，我们对不同的目标，从自然区域到城市区域进行了应用。我们提议一个管道，从协调开始，然后使用全谱式算法进行CD，并使用为光学数据开发的深度学习网络。我们发现， Vegetation和建筑环境中的变化都被良好地捕捉。spectral信息有价值，可以发现微妙的变化，而深度学习方法对噪声比统计方法更为敏感，但大气效应和可靠的地面真实数据的缺乏是干扰 hyperspectral CD的主要挑战。
</details></li>
</ul>
<hr>
<h2 id="Inter-Scale-Dependency-Modeling-for-Skin-Lesion-Segmentation-with-Transformer-based-Networks"><a href="#Inter-Scale-Dependency-Modeling-for-Skin-Lesion-Segmentation-with-Transformer-based-Networks" class="headerlink" title="Inter-Scale Dependency Modeling for Skin Lesion Segmentation with Transformer-based Networks"></a>Inter-Scale Dependency Modeling for Skin Lesion Segmentation with Transformer-based Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13727">http://arxiv.org/abs/2310.13727</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sania Eskandari, Janet Lumpp</li>
<li>for: 帮助诊断皮肤癌变，自动分割皮肤病变部分</li>
<li>methods: 使用U-Net建模，并提出Inter-scale Context Fusion（ISCF）模块来弥补semantic gaps</li>
<li>results: 在皮肤病变分割 benchmark 上获得了有效的结果，支持应用性和效果<details>
<summary>Abstract</summary>
Melanoma is a dangerous form of skin cancer caused by the abnormal growth of skin cells. Fully Convolutional Network (FCN) approaches, including the U-Net architecture, can automatically segment skin lesions to aid diagnosis. The symmetrical U-Net model has shown outstanding results, but its use of a convolutional operation limits its ability to capture long-range dependencies, which are essential for accurate medical image segmentation. In addition, the U-shaped structure suffers from the semantic gaps between the encoder and decoder. In this study, we developed and evaluated a U-shaped hierarchical Transformer-based structure for skin lesion segmentation while we proposed an Inter-scale Context Fusion (ISCF) to utilize the attention correlations in each stage of the encoder to adaptively combine the contexts coming from each stage to hinder the semantic gaps. The preliminary results of the skin lesion segmentation benchmark endorse the applicability and efficacy of the ISCF module.
</details>
<details>
<summary>摘要</summary>
melanoma 是一种危险的皮肤癌，由皮肤细胞异常生长所致。全量卷积网络（FCN）方法，包括 U-Net 架构，可以自动 segment 皮肤损伤以帮助诊断。 symmetrical U-Net 模型已经达到了卓越的效果，但它使用的 convolutional 操作限制了它的捕捉长距离依赖关系的能力，这些关系是医疗影像分割中非常重要的。此外， U 形结构受到 encoder 和 decoder 之间的 semantic gap 的困扰。在这项研究中，我们开发了一种 U 形层次 Transformer 结构，用于皮肤损伤 segmentation，并提出了一种 Inter-scale Context Fusion（ISCF）模块，用于在每个encoder 阶段中adaptively combine 来自每个阶段的上下文，以避免 semantic gap。preliminary 的皮肤损伤 segmentation  benchmark 结果表征了 ISCF 模块的可行性和效果。
</details></li>
</ul>
<hr>
<h2 id="What-you-see-is-what-you-get-Experience-ranking-with-deep-neural-dataset-to-dataset-similarity-for-topological-localisation"><a href="#What-you-see-is-what-you-get-Experience-ranking-with-deep-neural-dataset-to-dataset-similarity-for-topological-localisation" class="headerlink" title="What you see is what you get: Experience ranking with deep neural dataset-to-dataset similarity for topological localisation"></a>What you see is what you get: Experience ranking with deep neural dataset-to-dataset similarity for topological localisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13622">http://arxiv.org/abs/2310.13622</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mttgdd/vdna-experience-selection">https://github.com/mttgdd/vdna-experience-selection</a></li>
<li>paper_authors: Matthew Gadd, Benjamin Ramtoula, Daniele De Martini, Paul Newman</li>
<li>for: 本研究旨在提高视觉导航中的地点 lokalisierung 精度和稳定性，通过回忆最相关的视觉记忆来减少地点 lokalisierung 的努力。</li>
<li>methods: 本研究使用了 Visual DNA，一种高度可扩展的工具来比较图像集。在本研究中，我们使用了图像序列（map和实时经验）的比较，以检测模式的变化，包括天气、照明和季节。</li>
<li>results: 我们发现，对于使用深度建筑来进行地点 lokalisierung，分布度量可以比较 neuron-wise 活动统计数据 между实时图像和多个以前记录的经验，并且可以考虑季节（冬夏）或时间点（天亮夜晚）的变化。我们的方法可以准确地评估实际地点 lokalisierung 性能，并且在北欧的 Nordland 跨季度数据集和 Oxford 大学公园的照明和温和季节变化数据集上进行验证。<details>
<summary>Abstract</summary>
Recalling the most relevant visual memories for localisation or understanding a priori the likely outcome of localisation effort against a particular visual memory is useful for efficient and robust visual navigation. Solutions to this problem should be divorced from performance appraisal against ground truth - as this is not available at run-time - and should ideally be based on generalisable environmental observations. For this, we propose applying the recently developed Visual DNA as a highly scalable tool for comparing datasets of images - in this work, sequences of map and live experiences. In the case of localisation, important dataset differences impacting performance are modes of appearance change, including weather, lighting, and season. Specifically, for any deep architecture which is used for place recognition by matching feature volumes at a particular layer, we use distribution measures to compare neuron-wise activation statistics between live images and multiple previously recorded past experiences, with a potentially large seasonal (winter/summer) or time of day (day/night) shift. We find that differences in these statistics correlate to performance when localising using a past experience with the same appearance gap. We validate our approach over the Nordland cross-season dataset as well as data from Oxford's University Parks with lighting and mild seasonal change, showing excellent ability of our system to rank actual localisation performance across candidate experiences.
</details>
<details>
<summary>摘要</summary>
回忆最有关系的视觉记忆可以帮助提高视觉导航的效率和稳定性。解决这个问题应该与表现评估 Against ground truth 分离，因为在运行时不可用。我们提议使用最近发展的视觉 DNA 作为高可扩展的图像比较工具。在本工作中，我们使用 Distribution measures 来比较 neuron-wise 活动统计量 междуlive图像和多个前期记录的过去经验，包括季节（冬季/夏季）或时间点（日间/夜晚）的变化。我们发现这些统计量与表现强相关，可以用来评估不同经验的地方化性。我们验证了我们的方法在北欧的 Nordland 跨季度数据集以及牛津大学公园的光照和温和季节变化数据集上，并示出了我们系统可以高效地评估实际的地方化性。
</details></li>
</ul>
<hr>
<h2 id="FMRT-Learning-Accurate-Feature-Matching-with-Reconciliatory-Transformer"><a href="#FMRT-Learning-Accurate-Feature-Matching-with-Reconciliatory-Transformer" class="headerlink" title="FMRT: Learning Accurate Feature Matching with Reconciliatory Transformer"></a>FMRT: Learning Accurate Feature Matching with Reconciliatory Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13605">http://arxiv.org/abs/2310.13605</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyu Zhang, Li Wang, Zhiqiang Jiang, Kun Dai, Tao Xie, Lei Yang, Wenhao Yu, Yang Shen, Jun Li</li>
<li>for: 本研究旨在提出一种基于Transformer的Feature Matching方法，以提高计算机视觉任务中的结构从运动和视觉定位精度。</li>
<li>methods: 本方法使用一种专门设计的Reconciliatory Transformer（RecFormer），包括全球观察注意层（GPAL）、评估重要性层（PWL）和本地观察Feed-forward网络（LPFFN），以适应不同的感知范围和重要性进行自适应调整。</li>
<li>results: 广泛的实验结果表明，FMRT在多个标准 bencmarks上达到了极高的性能水平，包括pose estimation、visual localization、homography estimation和图像匹配等。<details>
<summary>Abstract</summary>
Local Feature Matching, an essential component of several computer vision tasks (e.g., structure from motion and visual localization), has been effectively settled by Transformer-based methods. However, these methods only integrate long-range context information among keypoints with a fixed receptive field, which constrains the network from reconciling the importance of features with different receptive fields to realize complete image perception, hence limiting the matching accuracy. In addition, these methods utilize a conventional handcrafted encoding approach to integrate the positional information of keypoints into the visual descriptors, which limits the capability of the network to extract reliable positional encoding message. In this study, we propose Feature Matching with Reconciliatory Transformer (FMRT), a novel Transformer-based detector-free method that reconciles different features with multiple receptive fields adaptively and utilizes parallel networks to realize reliable positional encoding. Specifically, FMRT proposes a dedicated Reconciliatory Transformer (RecFormer) that consists of a Global Perception Attention Layer (GPAL) to extract visual descriptors with different receptive fields and integrate global context information under various scales, Perception Weight Layer (PWL) to measure the importance of various receptive fields adaptively, and Local Perception Feed-forward Network (LPFFN) to extract deep aggregated multi-scale local feature representation. Extensive experiments demonstrate that FMRT yields extraordinary performance on multiple benchmarks, including pose estimation, visual localization, homography estimation, and image matching.
</details>
<details>
<summary>摘要</summary>
本文提出了一种新的Transformer基本方法，即Feature Matching with Reconciliatory Transformer（FMRT），用于解决多个计算机视觉任务（如结构从运动和视觉位置）中的本地特征匹配问题。这些方法仅将键点之间的长距离上下文信息集成到Transformer网络中，导致网络无法考虑不同的感知场景中的特征之间的重要性，从而限制匹配精度。此外，这些方法使用了传统的手动编码方法来整合键点的位置信息到视觉描述符中，这限制了网络的能力以可靠的编码位置信息。在本研究中，我们提出了一种专门的Reconciliatory Transformer（RecFormer），包括全球感知注意层（GPAL）、重要性测量层（PWL）和本地感知径向网络（LPFFN）。GPAL用于捕捉不同感知场景中的视觉描述符，并将其集成到不同缩放尺度下的全球上下文信息中；PWL用于测量不同感知场景中的重要性，并将其适应性地调整；LPFFN用于提取多尺度本地特征表示。广泛的实验表明，FMRT在多个测试准则上表现出色，包括pose estimation、视觉位置、投影估计和图像匹配等。
</details></li>
</ul>
<hr>
<h2 id="Longer-range-Contextualized-Masked-Autoencoder"><a href="#Longer-range-Contextualized-Masked-Autoencoder" class="headerlink" title="Longer-range Contextualized Masked Autoencoder"></a>Longer-range Contextualized Masked Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13593">http://arxiv.org/abs/2310.13593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taekyung Kim, Sanghyuk Chun, Byeongho Heo, Dongyoon Han</li>
<li>for: 提高自助学习模型的长范围关注和多个范围关注的能力，以提高图像识别 tasks 的性能。</li>
<li>methods: 提出了一种名为 Longer-range Contextualized Masked Autoencoder (LC-MAE) 的自助学习框架，通过全像 pixels 和多视图 pixels 的组合来提高模型的长范围关注和多个范围关注能力，同时减少输入的空间重复性。</li>
<li>results: 通过LC-MAE框架，实现了 ImageNet-1K 上的 Top-1 准确率提高至 84.2%，与基eline ViT-B 模型相比增加了 0.6%p，并在 semantic segmentation 和 fine-grained visual classification 任务上显示出了显著的性能提升。<details>
<summary>Abstract</summary>
Masked image modeling (MIM) has emerged as a promising self-supervised learning (SSL) strategy. The MIM pre-training facilitates learning powerful representations using an encoder-decoder framework by randomly masking some input pixels and reconstructing the masked pixels from the remaining ones. However, as the encoder is trained with partial pixels, the MIM pre-training can suffer from a low capability of understanding long-range dependency. This limitation may hinder its capability to fully understand multiple-range dependencies, resulting in narrow highlighted regions in the attention map that may incur accuracy drops. To mitigate the limitation, We propose a self-supervised learning framework, named Longer-range Contextualized Masked Autoencoder (LC-MAE). LC-MAE effectively leverages a global context understanding of visual representations while simultaneously reducing the spatial redundancy of input at the same time. Our method steers the encoder to learn from entire pixels in multiple views while also learning local representation from sparse pixels. As a result, LC-MAE learns more discriminative representations, leading to a performance improvement of achieving 84.2% top-1 accuracy with ViT-B on ImageNet-1K with 0.6%p gain. We attribute the success to the enhanced pre-training method, as evidenced by the singular value spectrum and attention analyses. Finally, LC-MAE achieves significant performance gains at the downstream semantic segmentation and fine-grained visual classification tasks; and on diverse robust evaluation metrics. Our code will be publicly available.
</details>
<details>
<summary>摘要</summary>
自适应学习（SSL）策略中的面具模型（MIM）已经出现为一种有前途的策略。MIM预训练使用Encoder-Decoder框架，随机遮盖输入像素，并从剩下的像素中重建遮盖的像素。然而，由于Encoder在部分像素上训练，MIM预训练可能会受到长距离依赖的限制，从而导致缺乏多距离依赖的全面理解，可能导致窄的强调区域在注意力图中，从而导致准确性下降。为解决这些限制，我们提出了一种自适应学习框架，名为长距离contextualized Masked Autoencoder（LC-MAE）。LC-MAE有效地利用了视觉表示的全局上下文理解，同时减少输入的空间重复。我们的方法使得Encoder从多个视图中学习整个像素，同时从罕见像素中学习本地表示。因此，LC-MAE学习出了更加细致的表示，导致它在ImageNet-1K上 achieved 84.2% top-1准确率，升准确率0.6%。我们认为这些成功是由于我们的预训练方法的改进，如谱值 спектrum和注意力分析所证明。最后，LC-MAE在下游semantic segmentation和细化视觉分类任务中具有显著性能提升，并在多个 Robust 评价指标上表现出色。我们的代码将公开。
</details></li>
</ul>
<hr>
<h2 id="POTLoc-Pseudo-Label-Oriented-Transformer-for-Point-Supervised-Temporal-Action-Localization"><a href="#POTLoc-Pseudo-Label-Oriented-Transformer-for-Point-Supervised-Temporal-Action-Localization" class="headerlink" title="POTLoc: Pseudo-Label Oriented Transformer for Point-Supervised Temporal Action Localization"></a>POTLoc: Pseudo-Label Oriented Transformer for Point-Supervised Temporal Action Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13585">http://arxiv.org/abs/2310.13585</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elahe Vahdani, Yingli Tian</li>
<li>for: 本 paper targets the challenge of point-supervised temporal action detection, where only a single frame is annotated for each action instance in the training set.</li>
<li>methods: 本 paper 提出了一种 Pseudo-label Oriented Transformer (POTLoc)，使用只有点级标注进行weakly-supervised Action Localization。POTLoc 通过自我训练策略来识别和跟踪连续的动作结构。</li>
<li>results: POTLoc 在 THUMOS’14 和 ActivityNet-v1.2 数据集上表现出色，与状态之前的点supervised方法进行比较，显示了5%的平均精度提升。<details>
<summary>Abstract</summary>
This paper tackles the challenge of point-supervised temporal action detection, wherein only a single frame is annotated for each action instance in the training set. Most of the current methods, hindered by the sparse nature of annotated points, struggle to effectively represent the continuous structure of actions or the inherent temporal and semantic dependencies within action instances. Consequently, these methods frequently learn merely the most distinctive segments of actions, leading to the creation of incomplete action proposals. This paper proposes POTLoc, a Pseudo-label Oriented Transformer for weakly-supervised Action Localization utilizing only point-level annotation. POTLoc is designed to identify and track continuous action structures via a self-training strategy. The base model begins by generating action proposals solely with point-level supervision. These proposals undergo refinement and regression to enhance the precision of the estimated action boundaries, which subsequently results in the production of `pseudo-labels' to serve as supplementary supervisory signals. The architecture of the model integrates a transformer with a temporal feature pyramid to capture video snippet dependencies and model actions of varying duration. The pseudo-labels, providing information about the coarse locations and boundaries of actions, assist in guiding the transformer for enhanced learning of action dynamics. POTLoc outperforms the state-of-the-art point-supervised methods on THUMOS'14 and ActivityNet-v1.2 datasets, showing a significant improvement of 5% average mAP on the former.
</details>
<details>
<summary>摘要</summary>
This paper proposes POTLoc, a Pseudo-label Oriented Transformer for weakly-supervised Action Localization, which utilizes only point-level annotation. POTLoc is designed to identify and track continuous action structures via a self-training strategy. The base model generates action proposals solely with point-level supervision, which undergo refinement and regression to enhance the precision of the estimated action boundaries. These estimated boundaries serve as supplementary supervisory signals, known as pseudo-labels, to guide the transformer for enhanced learning of action dynamics.The architecture of the model integrates a transformer with a temporal feature pyramid to capture video snippet dependencies and model actions of varying duration. The pseudo-labels provide information about the coarse locations and boundaries of actions, assisting the transformer in learning action dynamics. POTLoc outperforms state-of-the-art point-supervised methods on the THUMOS'14 and ActivityNet-v1.2 datasets, showing a significant improvement of 5% average mAP on the former.
</details></li>
</ul>
<hr>
<h2 id="Progressive-Dual-Priori-Network-for-Generalized-Breast-Tumor-Segmentation"><a href="#Progressive-Dual-Priori-Network-for-Generalized-Breast-Tumor-Segmentation" class="headerlink" title="Progressive Dual Priori Network for Generalized Breast Tumor Segmentation"></a>Progressive Dual Priori Network for Generalized Breast Tumor Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13574">http://arxiv.org/abs/2310.13574</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Wang, Lihui Wang, Zixiang Kuai, Lei Tang, Yingfeng Ou, Chen Ye, Yuemin Zhu</li>
<li>for: 提高乳腺癌分 segmentation模型的通用能力和小型腺癌分割性能</li>
<li>methods: 提出进步 dual priori network (PDPNet)，使用均衡精度和稳定性两种约束来进行乳腺癌分割</li>
<li>results: 比较多种国家数据集，PDPNet的 DSC、SEN、KAPPA 和 HD95 值分别提高 3.63%、8.19%、5.52% 和 3.66%，并通过减少正常组织的影响来提高模型的通用能力。<details>
<summary>Abstract</summary>
To promote the generalization ability of breast tumor segmentation models, as well as to improve the segmentation performance for breast tumors with smaller size, low-contrast amd irregular shape, we propose a progressive dual priori network (PDPNet) to segment breast tumors from dynamic enhanced magnetic resonance images (DCE-MRI) acquired at different sites. The PDPNet first cropped tumor regions with a coarse-segmentation based localization module, then the breast tumor mask was progressively refined by using the weak semantic priori and cross-scale correlation prior knowledge. To validate the effectiveness of PDPNet, we compared it with several state-of-the-art methods on multi-center datasets. The results showed that, comparing against the suboptimal method, the DSC, SEN, KAPPA and HD95 of PDPNet were improved 3.63\%, 8.19\%, 5.52\%, and 3.66\% respectively. In addition, through ablations, we demonstrated that the proposed localization module can decrease the influence of normal tissues and therefore improve the generalization ability of the model. The weak semantic priors allow focusing on tumor regions to avoid missing small tumors and low-contrast tumors. The cross-scale correlation priors are beneficial for promoting the shape-aware ability for irregual tumors. Thus integrating them in a unified framework improved the multi-center breast tumor segmentation performance.
</details>
<details>
<summary>摘要</summary>
<<sys.trans.type>>为提高乳腺癌分割模型的通用能力以及改善小型、低对比度和不规则形状的乳腺癌分割性能，我们提出了进步式双级先知网络（PDPNet）。PDPNet首先使用粗略分割基于本地化模块将肿瘤区域分割出来，然后通过弱语义先知和跨尺度相关先知知识进行进一步细化。为验证PDPNet的效果，我们与多个中心数据集进行比较。结果显示，相比于不优化方法，PDPNet的DSC、SEN、KAPPA和HD95分别提高了3.63%、8.19%、5.52%和3.66%。此外，通过剥离，我们证明了提案的本地化模块可以减少正常组织的影响，因此提高模型的通用能力。弱语义先知使得模型更注重肿瘤区域，以避免漏掉小肿瘤和低对比度肿瘤。跨尺度相关先知使得模型能够更好地捕捉不规则形状的肿瘤。因此，将它们集成到一个统一框架中，提高了多中心乳腺癌分割性能。
</details></li>
</ul>
<hr>
<h2 id="A-Simple-Baseline-for-Knowledge-Based-Visual-Question-Answering"><a href="#A-Simple-Baseline-for-Knowledge-Based-Visual-Question-Answering" class="headerlink" title="A Simple Baseline for Knowledge-Based Visual Question Answering"></a>A Simple Baseline for Knowledge-Based Visual Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13570">http://arxiv.org/abs/2310.13570</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexandros Xenos, Themos Stafylakis, Ioannis Patras, Georgios Tzimiropoulos</li>
<li>for: 本研究的目的是解决知识基础视觉问答问题（KB-VQA），以提高问答效果。</li>
<li>methods: 本研究提出了一种简单、可重现的管道，基于快速内容学习，使用问题描述文本作为上下文信息，通过提问LLaMA（1和2）来解决问题。</li>
<li>results: 相比之前的方法，本研究的方法不需要训练，无需访问外部数据库或API，却可以达到当前最佳性能水平在OK-VQA和A-OK-VQA数据集上。<details>
<summary>Abstract</summary>
This paper is on the problem of Knowledge-Based Visual Question Answering (KB-VQA). Recent works have emphasized the significance of incorporating both explicit (through external databases) and implicit (through LLMs) knowledge to answer questions requiring external knowledge effectively. A common limitation of such approaches is that they consist of relatively complicated pipelines and often heavily rely on accessing GPT-3 API. Our main contribution in this paper is to propose a much simpler and readily reproducible pipeline which, in a nutshell, is based on efficient in-context learning by prompting LLaMA (1 and 2) using question-informative captions as contextual information. Contrary to recent approaches, our method is training-free, does not require access to external databases or APIs, and yet achieves state-of-the-art accuracy on the OK-VQA and A-OK-VQA datasets. Finally, we perform several ablation studies to understand important aspects of our method. Our code is publicly available at https://github.com/alexandrosXe/ASimple-Baseline-For-Knowledge-Based-VQA
</details>
<details>
<summary>摘要</summary>
这篇论文关注知识基于视觉问答（KB-VQA）问题。现有研究强调了结合外部数据库和深度学习模型（LLMs）的知识来解决需要外部知识的问题。这些方法的共同局限性是它们通常包含复杂的管道和大量依赖于GPT-3 API访问。我们的主要贡献在这篇论文中是提出了一个非常简单和可重现的管道，即通过快速在 LLMA （1和2）中进行准确的培训，使用问题描述作为 Contextual information。与之前的方法不同，我们的方法不需要训练和访问外部数据库或 API，却可以达到 OK-VQA 和 A-OK-VQA 数据集上的状态当前精度。最后，我们进行了多个ablation study来理解我们的方法的重要方面。我们的代码公开可用于 GitHub 上的 https://github.com/alexandrosXe/ASimple-Baseline-For-Knowledge-Based-VQA。
</details></li>
</ul>
<hr>
<h2 id="ROSS-Radar-Off-road-Semantic-Segmentation"><a href="#ROSS-Radar-Off-road-Semantic-Segmentation" class="headerlink" title="ROSS: Radar Off-road Semantic Segmentation"></a>ROSS: Radar Off-road Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13551">http://arxiv.org/abs/2310.13551</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Jiang, Srikanth Saripalli</li>
<li>for: 本研究旨在探讨RADAR数据中的Semantic segmentation问题，以提高自动导航在非道路环境中的能效性。</li>
<li>methods: 我们提出了一种新的管道，利用LIDAR数据和现有的 annotated off-road LIDAR数据来生成RADAR标签，其中RADAR数据被 Represented as images。</li>
<li>results: 我们验证了这种实用的方法，并发现它在实际数据中表现出色，这demonstrates the potential of RADAR technology for navigation applications in off-road environments。Here’s the translation in English for reference:</li>
<li>for: This study aims to tackle the inherent complexities of semantic segmentation in RADAR data for off-road scenarios, with the goal of enhancing the efficiency of autonomous navigation in such environments.</li>
<li>methods: We propose a novel pipeline that utilizes LIDAR data and an existing annotated off-road LIDAR dataset to generate RADAR labels, where the RADAR data are represented as images.</li>
<li>results: We validate the effectiveness of our practical approach using real-world datasets, and find that it performs excellently, demonstrating the potential of RADAR technology for navigation applications in off-road environments.<details>
<summary>Abstract</summary>
As the demand for autonomous navigation in off-road environments increases, the need for effective solutions to understand these surroundings becomes essential. In this study, we confront the inherent complexities of semantic segmentation in RADAR data for off-road scenarios. We present a novel pipeline that utilizes LIDAR data and an existing annotated off-road LIDAR dataset for generating RADAR labels, in which the RADAR data are represented as images. Validated with real-world datasets, our pragmatic approach underscores the potential of RADAR technology for navigation applications in off-road environments.
</details>
<details>
<summary>摘要</summary>
As the demand for autonomous navigation in off-road environments increases, the need for effective solutions to understand these surroundings becomes essential. In this study, we confront the inherent complexities of semantic segmentation in RADAR data for off-road scenarios. We present a novel pipeline that utilizes LIDAR data and an existing annotated off-road LIDAR dataset for generating RADAR labels, in which the RADAR data are represented as images. Validated with real-world datasets, our pragmatic approach underscores the potential of RADAR technology for navigation applications in off-road environments.Here's the translation in Simplified Chinese characters:随着自动驾驶在非路面环境中的需求增加，理解这些环境的能力变得非常重要。在这个研究中，我们面临了RADAR数据中的自然复杂性，并提出了一种新的管道，利用LIIDAR数据和现有的 annotated off-road LIIDAR 数据集来生成 RADAR 标签。我们使用现实世界数据集进行验证，并证明了 RADAR 技术在非路面环境中的导航应用程序潜力。
</details></li>
</ul>
<hr>
<h2 id="A-review-of-individual-tree-crown-detection-and-delineation-from-optical-remote-sensing-images"><a href="#A-review-of-individual-tree-crown-detection-and-delineation-from-optical-remote-sensing-images" class="headerlink" title="A review of individual tree crown detection and delineation from optical remote sensing images"></a>A review of individual tree crown detection and delineation from optical remote sensing images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13481">http://arxiv.org/abs/2310.13481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juepeng Zheng, Shuai Yuan, Weijia Li, Haohuan Fu, Le Yu<br>for:  This paper provides a comprehensive review of Individual Tree Crown Detection (ITCD) methods for detecting and delineating individual tree crowns in optical remote sensing images.methods:  The review covers a wide range of ITCD methods, including traditional image processing methods, traditional machine learning methods, and deep learning-based methods.results:  The review discusses the strengths and limitations of each method and provides a clear knowledge map of existing ITCD efforts. It also proposes some ITCD-related applications and potential hot topics in future ITCD research.<details>
<summary>Abstract</summary>
Powered by the advances of optical remote sensing sensors, the production of very high spatial resolution multispectral images provides great potential for achieving cost-efficient and high-accuracy forest inventory and analysis in an automated way. Lots of studies that aim at providing an inventory to the level of each individual tree have generated a variety of methods for Individual Tree Crown Detection and Delineation (ITCD). This review covers ITCD methods for detecting and delineating individual tree crowns, and systematically reviews the past and present of ITCD-related researches applied to the optical remote sensing images. With the goal to provide a clear knowledge map of existing ITCD efforts, we conduct a comprehensive review of recent ITCD papers to build a meta-data analysis, including the algorithm, the study site, the tree species, the sensor type, the evaluation method, etc. We categorize the reviewed methods into three classes: (1) traditional image processing methods (such as local maximum filtering, image segmentation, etc.); (2) traditional machine learning methods (such as random forest, decision tree, etc.); and (3) deep learning based methods. With the deep learning-oriented approaches contributing a majority of the papers, we further discuss the deep learning-based methods as semantic segmentation and object detection methods. In addition, we discuss four ITCD-related issues to further comprehend the ITCD domain using optical remote sensing data, such as comparisons between multi-sensor based data and optical data in ITCD domain, comparisons among different algorithms and different ITCD tasks, etc. Finally, this review proposes some ITCD-related applications and a few exciting prospects and potential hot topics in future ITCD research.
</details>
<details>
<summary>摘要</summary>
Powered by the advances of optical remote sensing sensors, the production of very high spatial resolution multispectral images provides great potential for achieving cost-efficient and high-accuracy forest inventory and analysis in an automated way. Many studies that aim at providing an inventory to the level of each individual tree have generated a variety of methods for Individual Tree Crown Detection and Delineation (ITCD). This review covers ITCD methods for detecting and delineating individual tree crowns, and systematically reviews the past and present of ITCD-related researches applied to the optical remote sensing images. With the goal to provide a clear knowledge map of existing ITCD efforts, we conduct a comprehensive review of recent ITCD papers to build a meta-data analysis, including the algorithm, the study site, the tree species, the sensor type, the evaluation method, etc. We categorize the reviewed methods into three classes: (1) traditional image processing methods (such as local maximum filtering, image segmentation, etc.); (2) traditional machine learning methods (such as random forest, decision tree, etc.); and (3) deep learning based methods. With the deep learning-oriented approaches contributing a majority of the papers, we further discuss the deep learning-based methods as semantic segmentation and object detection methods. In addition, we discuss four ITCD-related issues to further comprehend the ITCD domain using optical remote sensing data, such as comparisons between multi-sensor based data and optical data in ITCD domain, comparisons among different algorithms and different ITCD tasks, etc. Finally, this review proposes some ITCD-related applications and a few exciting prospects and potential hot topics in future ITCD research.
</details></li>
</ul>
<hr>
<h2 id="Segment-Select-Correct-A-Framework-for-Weakly-Supervised-Referring-Segmentation"><a href="#Segment-Select-Correct-A-Framework-for-Weakly-Supervised-Referring-Segmentation" class="headerlink" title="Segment, Select, Correct: A Framework for Weakly-Supervised Referring Segmentation"></a>Segment, Select, Correct: A Framework for Weakly-Supervised Referring Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13479">http://arxiv.org/abs/2310.13479</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fgirbal/segment-select-correct">https://github.com/fgirbal/segment-select-correct</a></li>
<li>paper_authors: Francisco Eiras, Kemal Oksuz, Adel Bibi, Philip H. S. Torr, Puneet K. Dokania</li>
<li>for: 提高弱类学习图像分割（Referring Image Segmentation，RIS）的性能，使其与全监督学习方法匹配。</li>
<li>methods: 提出一种新的弱类学习框架，包括三个步骤：获取引用 instrucion中对象的实例Mask（segment），使用零损学习选择可能正确的Mask（select），并使用模型修复零损选择的错误（correct）。</li>
<li>results: 在实验中，只使用首两个步骤（零损 segment和 select）比其他零损基elines提高了19%，而我们的全方法可以更高地超越这个强baseline，将弱类学习RIS的性能提高至全监督方法水平，在某些情况下将间隔降低至14%。<details>
<summary>Abstract</summary>
Referring Image Segmentation (RIS) - the problem of identifying objects in images through natural language sentences - is a challenging task currently mostly solved through supervised learning. However, while collecting referred annotation masks is a time-consuming process, the few existing weakly-supervised and zero-shot approaches fall significantly short in performance compared to fully-supervised learning ones. To bridge the performance gap without mask annotations, we propose a novel weakly-supervised framework that tackles RIS by decomposing it into three steps: obtaining instance masks for the object mentioned in the referencing instruction (segment), using zero-shot learning to select a potentially correct mask for the given instruction (select), and bootstrapping a model which allows for fixing the mistakes of zero-shot selection (correct). In our experiments, using only the first two steps (zero-shot segment and select) outperforms other zero-shot baselines by as much as 19%, while our full method improves upon this much stronger baseline and sets the new state-of-the-art for weakly-supervised RIS, reducing the gap between the weakly-supervised and fully-supervised methods in some cases from around 33% to as little as 14%. Code is available at https://github.com/fgirbal/segment-select-correct.
</details>
<details>
<summary>摘要</summary>
稍微指导学习（RIS）——在自然语言句子中识别图像中的对象——是一项具有挑战性的任务，目前主要通过指导学习来解决。然而，收集引用涂抹的过程是时间consuming的，而exist的弱指导学习和零shot方法在性能上与完全指导学习方法相比显得较差。为了bridging性能差距而不需要涂抹，我们提出了一种新的弱指导学习框架，该框架通过分解RIS任务为三步来解决：取得提及的对象图像的实例涂抹（segment），使用零shot学习选择可能正确的涂抹（select），并使用一种可以修复零shot选择的错误的模型（correct）。在我们的实验中，只使用第一两步（零shot segment和select）可以与其他零shot基elines相比提高到19%，而我们的全方法可以超越这个强baseline，并将弱指导学习RIS的状态rola-art降低到14%。代码可以在https://github.com/fgirbal/segment-select-correct中找到。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Sequential-Visual-Input-Reasoning-and-Prediction-in-Multimodal-Large-Language-Models"><a href="#Benchmarking-Sequential-Visual-Input-Reasoning-and-Prediction-in-Multimodal-Large-Language-Models" class="headerlink" title="Benchmarking Sequential Visual Input Reasoning and Prediction in Multimodal Large Language Models"></a>Benchmarking Sequential Visual Input Reasoning and Prediction in Multimodal Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13473">http://arxiv.org/abs/2310.13473</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/coderj-one/giraffe-bench">https://github.com/coderj-one/giraffe-bench</a></li>
<li>paper_authors: Mingwei Zhu, Leigang Sha, Yu Shu, Kangjia Zhao, Tiancheng Zhao, Jianwei Yin</li>
<li>for: 本文旨在评估大型语言模型（MLLMs）在预测预测任务中的能力，以探索其预测理解能力的可能性。</li>
<li>methods: 本文提出了一个新的benchmark，用于评估MLLMs在多种场景中的预测能力。该benchmark包括三个重要领域：抽象模式逻辑推理、人员活动预测和物理互动预测。同时，本文还开发了三种评估方法，以评估模型在基于多 modal输入的预测和逻辑推理任务中的表现。</li>
<li>results: 实验证明了本文提出的benchmark和评估方法的可靠性，并 revela了当前popular MLLMs在预测任务中的优缺点。最后，本文的benchmark可以为MLLMs的发展提供一个标准化的评估框架，并促进模型的更高级别的发展，以便在复杂的多modal输入下进行预测和逻辑推理。<details>
<summary>Abstract</summary>
Multimodal large language models (MLLMs) have shown great potential in perception and interpretation tasks, but their capabilities in predictive reasoning remain under-explored. To address this gap, we introduce a novel benchmark that assesses the predictive reasoning capabilities of MLLMs across diverse scenarios. Our benchmark targets three important domains: abstract pattern reasoning, human activity prediction, and physical interaction prediction. We further develop three evaluation methods powered by large language model to robustly quantify a model's performance in predicting and reasoning the future based on multi-visual context. Empirical experiments confirm the soundness of the proposed benchmark and evaluation methods via rigorous testing and reveal pros and cons of current popular MLLMs in the task of predictive reasoning. Lastly, our proposed benchmark provides a standardized evaluation framework for MLLMs and can facilitate the development of more advanced models that can reason and predict over complex long sequence of multimodal input.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Dance-Your-Latents-Consistent-Dance-Generation-through-Spatial-temporal-Subspace-Attention-Guided-by-Motion-Flow"><a href="#Dance-Your-Latents-Consistent-Dance-Generation-through-Spatial-temporal-Subspace-Attention-Guided-by-Motion-Flow" class="headerlink" title="Dance Your Latents: Consistent Dance Generation through Spatial-temporal Subspace Attention Guided by Motion Flow"></a>Dance Your Latents: Consistent Dance Generation through Spatial-temporal Subspace Attention Guided by Motion Flow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14780">http://arxiv.org/abs/2310.14780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haipeng Fang, Zhihao Sun, Ziyao Huang, Fan Tang, Juan Cao, Sheng Tang</li>
<li>for: 这篇论文旨在提高人体舞蹈生成领域的生成AI技术，以实现更高质量的舞蹈视频生成。</li>
<li>methods: 该方法基于隐藏空间的嵌入学习，并提出了空间-时间嵌入注意力块和运动流导航注意力块等两种新的注意力机制，以提高生成的空间时间一致性。</li>
<li>results: 实验结果表明，该方法可以显著提高生成的舞蹈视频中的空间时间一致性，从而提高生成的质量。<details>
<summary>Abstract</summary>
The advancement of generative AI has extended to the realm of Human Dance Generation, demonstrating superior generative capacities. However, current methods still exhibit deficiencies in achieving spatiotemporal consistency, resulting in artifacts like ghosting, flickering, and incoherent motions. In this paper, we present Dance-Your-Latents, a framework that makes latents dance coherently following motion flow to generate consistent dance videos. Firstly, considering that each constituent element moves within a confined space, we introduce spatial-temporal subspace-attention blocks that decompose the global space into a combination of regular subspaces and efficiently model the spatiotemporal consistency within these subspaces. This module enables each patch pay attention to adjacent areas, mitigating the excessive dispersion of long-range attention. Furthermore, observing that body part's movement is guided by pose control, we design motion flow guided subspace align & restore. This method enables the attention to be computed on the irregular subspace along the motion flow. Experimental results in TikTok dataset demonstrate that our approach significantly enhances spatiotemporal consistency of the generated videos.
</details>
<details>
<summary>摘要</summary>
“人体舞蹈生成领域内，对于生成AI的进步已经推广到。然而，目前的方法仍然存在着时空一致性的缺陷，导致类似“幽灵”、“跳跃”和“无统一”的artefacts出现。在本文中，我们提出 Dance-Your-Latents 框架，让 latent 在动作流中跳舞具有一致性，实现了一致的舞蹈视频生成。首先，我们考虑到每个元素在紧随的空间内运动，我们引入时空频域注意力对应方法，将全球空间分解为一系列规律的频域和有效地模型时空一致性。这个模员使每个小区与邻近区域进行对话，解决了过度分散的长距离注意力问题。其次，我们观察到人体部分的运动受到pose控制的指导，我们设计了动作流导向的时空对齐恢复方法。这种方法使得注意力可以在动作流方向上进行计算，实现了一致的注意力Computing。实验结果显示，我们的方法在TikTok数据集上有 statistically significant 提高了生成视频的时空一致性。”
</details></li>
</ul>
<hr>
<h2 id="Two-Stage-Triplet-Loss-Training-with-Curriculum-Augmentation-for-Audio-Visual-Retrieval"><a href="#Two-Stage-Triplet-Loss-Training-with-Curriculum-Augmentation-for-Audio-Visual-Retrieval" class="headerlink" title="Two-Stage Triplet Loss Training with Curriculum Augmentation for Audio-Visual Retrieval"></a>Two-Stage Triplet Loss Training with Curriculum Augmentation for Audio-Visual Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13451">http://arxiv.org/abs/2310.13451</a></li>
<li>repo_url: None</li>
<li>paper_authors: Donghuo Zeng, Kazushi Ikeda</li>
<li>for: 本文targets the problem of cross-modal retrieval, specifically addressing the issue of suboptimal model performance due to the oversight of distinguishing between semi-hard and hard triples in the optimization process.</li>
<li>methods: 本文提出了一种基于课程学习的两阶段训练方法，通过从 semi-hard triplets开始，然后通过 interpolating embeddings来增强模型的学习过程。最后，模型通过 hard triplet mining来进一步优化。</li>
<li>results: 实验结果表明，在两个音频视频数据集上，与当前状态艺术方法MSNSCA进行比较，本文的方法在AV-CMR任务上的AVE数据集上提高了均值溢通精度（MAP）的平均值约9.8%， indicating the effectiveness of the proposed method.<details>
<summary>Abstract</summary>
The cross-modal retrieval model leverages the potential of triple loss optimization to learn robust embedding spaces. However, existing methods often train these models in a singular pass, overlooking the distinction between semi-hard and hard triples in the optimization process. The oversight of not distinguishing between semi-hard and hard triples leads to suboptimal model performance. In this paper, we introduce a novel approach rooted in curriculum learning to address this problem. We propose a two-stage training paradigm that guides the model's learning process from semi-hard to hard triplets. In the first stage, the model is trained with a set of semi-hard triplets, starting from a low-loss base. Subsequently, in the second stage, we augment the embeddings using an interpolation technique. This process identifies potential hard negatives, alleviating issues arising from high-loss functions due to a scarcity of hard triples. Our approach then applies hard triplet mining in the augmented embedding space to further optimize the model. Extensive experimental results conducted on two audio-visual datasets show a significant improvement of approximately 9.8% in terms of average Mean Average Precision (MAP) over the current state-of-the-art method, MSNSCA, for the Audio-Visual Cross-Modal Retrieval (AV-CMR) task on the AVE dataset, indicating the effectiveness of our proposed method.
</details>
<details>
<summary>摘要</summary>
《cross-modal retrieval模型可以利用 triple loss优化来学习强化 embedding空间。然而，现有方法通常在单个过程中训练这些模型，忽视 semi-hard triplets 和 hard triplets 之间的差异在优化过程中。这种忽视导致模型性能下降。在本文中，我们提出了一种新的方法，基于 curriculum learning，来解决这个问题。我们提议一种两阶段训练方法，使得模型在 semi-hard triplets 上学习，然后使用 interpolate 技术来增强 embedding，并在扩展 embedding 空间中进行 hard triplet 挖掘，以进一步优化模型。我们的方法在两个音频视频数据集上进行了广泛的实验，并与当前状态艺术方法 MSNSCA 进行了比较。结果表明，我们的方法在 AVE 数据集上的 Audio-Visual Cross-Modal Retrieval（AV-CMR）任务中提高了平均精度为9.8%。》Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. The translation is based on the given text and may not reflect the exact phrasing or wording of the original text.
</details></li>
</ul>
<hr>
<h2 id="Definition-independent-Formalization-of-Soundscapes-Towards-a-Formal-Methodology"><a href="#Definition-independent-Formalization-of-Soundscapes-Towards-a-Formal-Methodology" class="headerlink" title="Definition-independent Formalization of Soundscapes: Towards a Formal Methodology"></a>Definition-independent Formalization of Soundscapes: Towards a Formal Methodology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13404">http://arxiv.org/abs/2310.13404</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikel D. Jedrusiak, Thomas Harweg, Timo Haselhoff, Bryce T. Lawrence, Susanne Moebus, Frank Weichert</li>
<li>for: 本研究旨在提供一种独立于听samples定义的 formalization，以便捕捉不同领域的听samples数据的多元结构，以及不同 идеológy的表述。</li>
<li>methods: 本研究使用了频谱相关矩阵来检测土地使用类型，作为一种 alternativa 于 MFCCs 的特征。</li>
<li>results:  exemplary 分析表明，使用频谱相关矩阵可以准确地检测土地使用类型，并且可以捕捉到不同领域的听samples数据中的多元结构。<details>
<summary>Abstract</summary>
Soundscapes have been studied by researchers from various disciplines, each with different perspectives, goals, approaches, and terminologies. Accordingly, depending on the field, the concept of a soundscape's components changes, consequently changing the basic definition. This results in complicating interdisciplinary communication and comparison of results. Especially when soundscape-unrelated research areas are involved. For this reason, we present a potential formalization that is independent of the underlying soundscape definition, with the goal of being able to capture the heterogeneous structure of the data as well as the different ideologies in one model. In an exemplary analysis of frequency correlation matrices for land use type detection as an alternative to features like MFCCs, we show a practical application of our presented formalization.
</details>
<details>
<summary>摘要</summary>
听soundscapes已经被不同领域的研究人员研究，每个领域有不同的视角、目标、方法和术语。因此，听soundscapes的组成部分在不同领域中发生变化，导致基本定义的变化。这会使交通 между不同领域和对结果的比较变得复杂。特别是当涉及到不同领域的研究时。为了解决这个问题，我们提出了一种可能的正式化方法，可以独立地捕捉听soundscapes的多元结构和不同意识形态。在一个例子中，我们通过对听frequency correlation matrix进行分析，以示听land use类型检测的实际应用。
</details></li>
</ul>
<hr>
<h2 id="OpenAnnotate3D-Open-Vocabulary-Auto-Labeling-System-for-Multi-modal-3D-Data"><a href="#OpenAnnotate3D-Open-Vocabulary-Auto-Labeling-System-for-Multi-modal-3D-Data" class="headerlink" title="OpenAnnotate3D: Open-Vocabulary Auto-Labeling System for Multi-modal 3D Data"></a>OpenAnnotate3D: Open-Vocabulary Auto-Labeling System for Multi-modal 3D Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13398">http://arxiv.org/abs/2310.13398</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yijie Zhou, Likun Cai, Xianhui Cheng, Zhongxue Gan, Xiangyang Xue, Wenchao Ding</li>
<li>for:  automatic annotating functions for multi-modal data</li>
<li>methods:  open-source open-vocabulary auto-labeling system that integrates Large Language Models (LLMs) and vision-language models (VLMs)</li>
<li>results:  significantly improves annotation efficiency compared to manual annotation while providing accurate open-vocabulary auto-annotating results.Here’s the simplified Chinese text:</li>
<li>for:  automatic对多模态数据进行注解功能</li>
<li>methods: 基于开源的开 vocabulary自动注解系统，结合大语言模型（LLMs）和视力语言模型（VLMs）</li>
<li>results:  Significantly improves manual注解效率，提供准确的开 vocabulary自动注解结果<details>
<summary>Abstract</summary>
In the era of big data and large models, automatic annotating functions for multi-modal data are of great significance for real-world AI-driven applications, such as autonomous driving and embodied AI. Unlike traditional closed-set annotation, open-vocabulary annotation is essential to achieve human-level cognition capability. However, there are few open-vocabulary auto-labeling systems for multi-modal 3D data. In this paper, we introduce OpenAnnotate3D, an open-source open-vocabulary auto-labeling system that can automatically generate 2D masks, 3D masks, and 3D bounding box annotations for vision and point cloud data. Our system integrates the chain-of-thought capabilities of Large Language Models (LLMs) and the cross-modality capabilities of vision-language models (VLMs). To the best of our knowledge, OpenAnnotate3D is one of the pioneering works for open-vocabulary multi-modal 3D auto-labeling. We conduct comprehensive evaluations on both public and in-house real-world datasets, which demonstrate that the system significantly improves annotation efficiency compared to manual annotation while providing accurate open-vocabulary auto-annotating results.
</details>
<details>
<summary>摘要</summary>
在大数据和大模型时代，自动标注函数对多 modal 数据的应用非常重要，如自动驾驶和具体 AI。不同于传统的关闭集合标注，开放词汇标注是实现人类认知水平的关键。然而，对多 modal 3D 数据的开放词汇自动标注系统很少。在这篇论文中，我们介绍 OpenAnnotate3D，一个开源的开放词汇自动标注系统，可以自动生成2D masks、3D masks和3D bounding box注释 для视觉和点云数据。我们的系统结合了 Large Language Models (LLMs) 的链条思维能力和视觉语言模型 (VLMs) 的交叉模态能力。据我们所知，OpenAnnotate3D 是开放词汇多 modal 3D 自动标注的先驱之作。我们对公共和内部实验室数据进行了全面的评估，结果表明，系统可以大幅提高人工标注的效率，同时提供高精度的开放词汇自动标注结果。
</details></li>
</ul>
<hr>
<h2 id="ScalableMap-Scalable-Map-Learning-for-Online-Long-Range-Vectorized-HD-Map-Construction"><a href="#ScalableMap-Scalable-Map-Learning-for-Online-Long-Range-Vectorized-HD-Map-Construction" class="headerlink" title="ScalableMap: Scalable Map Learning for Online Long-Range Vectorized HD Map Construction"></a>ScalableMap: Scalable Map Learning for Online Long-Range Vectorized HD Map Construction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13378">http://arxiv.org/abs/2310.13378</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jingy1yu/scalablemap">https://github.com/jingy1yu/scalablemap</a></li>
<li>paper_authors: Jingyi Yu, Zizhao Zhang, Shengfu Xia, Jizhang Sang</li>
<li>for: 这个论文是为了建立在board camera感知器上的在线长距离高清地图建构管线。</li>
<li>methods: 该论文使用了纹理化表示法，使用多边形和多边形来表示地图元素。它还提出了一种层次稀疏地图表示法，以便更好地利用纹理化地图元素的可扩展性，并设计了一种进程编码机制和一种监督策略。</li>
<li>results: 该论文在nuScenes数据集上达到了6.5 mAP的最高精度，在长距离场景下表现特别出色，超越了之前的状态态模型，并且实现了18.3 FPS的速度。<details>
<summary>Abstract</summary>
We propose a novel end-to-end pipeline for online long-range vectorized high-definition (HD) map construction using on-board camera sensors. The vectorized representation of HD maps, employing polylines and polygons to represent map elements, is widely used by downstream tasks. However, previous schemes designed with reference to dynamic object detection overlook the structural constraints within linear map elements, resulting in performance degradation in long-range scenarios. In this paper, we exploit the properties of map elements to improve the performance of map construction. We extract more accurate bird's eye view (BEV) features guided by their linear structure, and then propose a hierarchical sparse map representation to further leverage the scalability of vectorized map elements and design a progressive decoding mechanism and a supervision strategy based on this representation. Our approach, ScalableMap, demonstrates superior performance on the nuScenes dataset, especially in long-range scenarios, surpassing previous state-of-the-art model by 6.5 mAP while achieving 18.3 FPS. Code is available at https://github.com/jingy1yu/ScalableMap.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的端到端管道，用于在线上进行高清定制（HD）地图建构，使用车载摄像头传感器。这种vectorized表示方法，使用多边形和多边形来表示地图元素，广泛用于下游任务。然而，之前的方案忽略了线性地图元素的结构约束，导致长距离场景下的性能下降。在这篇论文中，我们利用地图元素的属性来提高地图建构的性能。我们从多个 bird's eye view（BEV）特征中提取更加准确的特征，然后提出一种层次稀疏地图表示法，以便更好地利用vectorized地图元素的可扩展性，并设计了一种进程式解码机制和一种根据这种表示法的监督策略。我们的方法，称为ScalableMap，在nuScenes数据集上表现出色，特别是在长距离场景下，比前一个状态的模型提高6.5 mAP，并达到18.3 FPS。代码可以在https://github.com/jingy1yu/ScalableMap中下载。
</details></li>
</ul>
<hr>
<h2 id="Single-view-3D-reconstruction-via-inverse-procedural-modeling"><a href="#Single-view-3D-reconstruction-via-inverse-procedural-modeling" class="headerlink" title="Single-view 3D reconstruction via inverse procedural modeling"></a>Single-view 3D reconstruction via inverse procedural modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13373">http://arxiv.org/abs/2310.13373</a></li>
<li>repo_url: None</li>
<li>paper_authors: Albert Garifullin, Nikolay Maiorov, Vladimir Frolov</li>
<li>for: 3D reconstruction via inverse procedural modeling, demonstrating results on tree models and complex objects</li>
<li>methods: using a genetic algorithm for fitting set of input parameters, differentiable rendering and differentiable procedural generators for precise reconstruction</li>
<li>results: significant improvement in precision, ability to reconstruct 3D models accurately with a small number of input images, and application to complex generators with both differentiable and non-differentiable procedural generators<details>
<summary>Abstract</summary>
We propose an approach to 3D reconstruction via inverse procedural modeling and investigate two variants of this approach. The first option consists in the fitting set of input parameters using a genetic algorithm. We demonstrate the results of our work on tree models, complex objects, with the reconstruction of which most existing methods cannot handle. The second option allows us to significantly improve the precision by using gradients within memetic algorithm, differentiable rendering and also differentiable procedural generators. In our work we see 2 main contributions. First, we propose a method to join differentiable rendering and inverse procedural modeling. This gives us an opportunity to reconstruct 3D model more accurately than existing approaches when a small number of input images are available (even for single image). Second, we join both differentiable and non-differentiable procedural generators in a single framework which allow us to apply inverse procedural modeling to fairly complex generators: when gradient is available, reconstructions is precise, when gradient is not available, reconstruction is approximate, but always high quality without visual artifacts.
</details>
<details>
<summary>摘要</summary>
我们提出了一种3D重建方法，基于反工程模型，并研究了这种方法的两种变体。第一种方法使用遗传算法来调整输入参数的集合。我们在树模型、复杂物体上进行了实验，并达到了现有方法无法处理的3D重建结果。第二种方法使用内在的积分算法、可微渲染和可微生成器，可以在输入图像很少时进行高精度的3D重建。在我们的工作中，我们认为有两个主要贡献：首先，我们将可微渲染和反工程模型结合在一起，从而在输入图像很少时可以更加准确地重建3D模型（即使用单个图像）。其次，我们将可微和非可微的生成器集成到同一个框架中，以便应用反工程模型到较复杂的生成器中，当gradient可用时，重建是精确的，当gradient不可用时，重建是相对精度高而无视觉 artifacts。
</details></li>
</ul>
<hr>
<h2 id="PSGText-Stroke-Guided-Scene-Text-Editing-with-PSP-Module"><a href="#PSGText-Stroke-Guided-Scene-Text-Editing-with-PSP-Module" class="headerlink" title="PSGText: Stroke-Guided Scene Text Editing with PSP Module"></a>PSGText: Stroke-Guided Scene Text Editing with PSP Module</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13366">http://arxiv.org/abs/2310.13366</a></li>
<li>repo_url: None</li>
<li>paper_authors: Felix Liawi, Yun-Da Tsai, Guan-Lun Lu, Shou-De Lin</li>
<li>for: 该论文目的是提供一种将文本 transferred 到图像中的方法，以保持原始文本背景和样式，并提高修改后的文本的清晰度和可读性。</li>
<li>methods: 该方法包括三个阶段：首先，我们引入了一个文本交换网络，可以轻松地将原始文本替换为新的文本。其次，我们 integrates 了一个背景填充网络，用于修复background图像中的空洞，以保持视觉协调和一致性。最后，我们使用一个融合网络将这两个网络的结果融合，得到高清晰度和可读性的修改后图像。</li>
<li>results: 该方法可以生成高质量的修改后图像，保持原始文本背景和样式，并提高修改后文本的清晰度和可读性。 demo 视频可以在补充材料中找到。<details>
<summary>Abstract</summary>
Scene Text Editing (STE) aims to substitute text in an image with new desired text while preserving the background and styles of the original text. However, present techniques present a notable challenge in the generation of edited text images that exhibit a high degree of clarity and legibility. This challenge primarily stems from the inherent diversity found within various text types and the intricate textures of complex backgrounds. To address this challenge, this paper introduces a three-stage framework for transferring texts across text images. Initially, we introduce a text-swapping network that seamlessly substitutes the original text with the desired replacement. Subsequently, we incorporate a background inpainting network into our framework. This specialized network is designed to skillfully reconstruct background images, effectively addressing the voids left after the removal of the original text. This process meticulously preserves visual harmony and coherence in the background. Ultimately, the synthesis of outcomes from the text-swapping network and the background inpainting network is achieved through a fusion network, culminating in the creation of the meticulously edited final image. A demo video is included in the supplementary material.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Sync-NeRF-Generalizing-Dynamic-NeRFs-to-Unsynchronized-Videos"><a href="#Sync-NeRF-Generalizing-Dynamic-NeRFs-to-Unsynchronized-Videos" class="headerlink" title="Sync-NeRF: Generalizing Dynamic NeRFs to Unsynchronized Videos"></a>Sync-NeRF: Generalizing Dynamic NeRFs to Unsynchronized Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13356">http://arxiv.org/abs/2310.13356</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/seoha-kim/Sync-NeRF">https://github.com/seoha-kim/Sync-NeRF</a></li>
<li>paper_authors: Seoha Kim, Jeongmin Bae, Youngsik Yun, Hahyun Lee, Gun Bang, Youngjung Uh</li>
<li>for: 该论文旨在解决4D场景重建使用神经辐射场（NeRF）时，对动态场景的重建受限，并且无法适应不同时刻拍摄的多视图视频。</li>
<li>methods: 作者引入时差偏移来解决这个问题，并将偏移值与NeRF共同优化。这种方法适用于多种基eline和提高了它们的性能。</li>
<li>results: 实验结果表明，该方法可以有效地同步多视图视频，并且在Plenoptic Video Dataset和一个新建的Unsynchronized Dynamic Blender Dataset上进行了验证。项目页面：<a target="_blank" rel="noopener" href="https://seoha-kim.github.io/sync-nerf%E3%80%82">https://seoha-kim.github.io/sync-nerf。</a><details>
<summary>Abstract</summary>
Recent advancements in 4D scene reconstruction using neural radiance fields (NeRF) have demonstrated the ability to represent dynamic scenes from multi-view videos. However, they fail to reconstruct the dynamic scenes and struggle to fit even the training views in unsynchronized settings. It happens because they employ a single latent embedding for a frame while the multi-view images at the frame were actually captured at different moments. To address this limitation, we introduce time offsets for individual unsynchronized videos and jointly optimize the offsets with NeRF. By design, our method is applicable for various baselines and improves them with large margins. Furthermore, finding the offsets naturally works as synchronizing the videos without manual effort. Experiments are conducted on the common Plenoptic Video Dataset and a newly built Unsynchronized Dynamic Blender Dataset to verify the performance of our method. Project page: https://seoha-kim.github.io/sync-nerf
</details>
<details>
<summary>摘要</summary>
最近的进展在4D场景重建领域使用神经辐射场（NeRF）已经表明了能够从多视图视频中重建动态场景。然而，它们无法重建动态场景，并且在不同时刻拍摄的多视图图像中很难匹配。这是因为它们使用单个离散嵌入来表示一帧的图像，而多视图图像在该帧中实际上是在不同的时刻拍摄的。为解决这些限制，我们引入时间偏移 для个体不同时刻的视频，并同时优化偏移。由设计来看，我们的方法适用于各种基elines和提高它们的大幅度。此外，找到偏移也自然地同步视频，无需手动努力。我们在常见的Plenoptic Video Dataset和新建的Unsynchronized Dynamic Blender Dataset上进行了实验，以验证我们的方法的性能。项目页面：https://seoha-kim.github.io/sync-nerf
</details></li>
</ul>
<hr>
<h2 id="SILC-Improving-Vision-Language-Pretraining-with-Self-Distillation"><a href="#SILC-Improving-Vision-Language-Pretraining-with-Self-Distillation" class="headerlink" title="SILC: Improving Vision Language Pretraining with Self-Distillation"></a>SILC: Improving Vision Language Pretraining with Self-Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13355">http://arxiv.org/abs/2310.13355</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Ferjad Naeem, Yongqin Xian, Xiaohua Zhai, Lukas Hoyer, Luc Van Gool, Federico Tombari<br>for:This paper focuses on improving the performance of open-vocabulary classification and retrieval models by using a simple addition of local-to-global correspondence learning through self-distillation during contrastive pre-training.methods:The proposed method, SILC, uses a contrastive objective to learn image-text alignment and adds local-to-global correspondence learning through self-distillation to improve image feature learning for dense prediction tasks.results:The proposed SILC model achieves state-of-the-art performance on several computer vision tasks, including zero-shot classification, few-shot classification, image and text retrieval, zero-shot segmentation, and open vocabulary segmentation, with better scaling compared to baselines.<details>
<summary>Abstract</summary>
Image-Text pretraining on web-scale image caption dataset has become the default recipe for open vocabulary classification and retrieval models thanks to the success of CLIP and its variants. Several works have also used CLIP features for dense prediction tasks and have shown the emergence of open-set abilities. However, the contrastive objective only focuses on image-text alignment and does not incentivise image feature learning for dense prediction tasks. In this work, we propose the simple addition of local-to-global correspondence learning by self-distillation as an additional objective for contrastive pre-training to propose SILC. We show that distilling local image features from an exponential moving average (EMA) teacher model significantly improves model performance on several computer vision tasks including classification, retrieval, and especially segmentation. We further show that SILC scales better with the same training duration compared to the baselines. Our model SILC sets a new state of the art for zero-shot classification, few shot classification, image and text retrieval, zero-shot segmentation, and open vocabulary segmentation.
</details>
<details>
<summary>摘要</summary>
“图文预训在大规模图片描述集合上进行预训，现在成为开 vocabulary 分类和搜寻模型的默认配方，这主要归功于 CLIP 和其变体的成功。一些工作也使用 CLIP 特征进行紧密预测任务，并显示了开集能力的出现。但是，对比тив的目标只是对图片文本的对预导，不对图片特征学习紧密预测任务。在这个工作中，我们提出了将本地到全球对应学习作为额外目标，通过自我静态学习来实现 SILC。我们表明，将本地图片特征从 EMA 教师模型散发到 exponential moving average 教师模型可以大幅提高模型在许多计算机视觉任务中的性能，包括分类、搜寻、特别是分割。我们进一步显示，SILC 在相同训练时间下可以与基准点模型相比，并且在 zero-shot 分类、几步分类、图片和文本搜寻、 zero-shot 分割和开 vocabulary 分割等多个任务中设置新的州OF THE ART。”
</details></li>
</ul>
<hr>
<h2 id="EarlyBird-Early-Fusion-for-Multi-View-Tracking-in-the-Bird’s-Eye-View"><a href="#EarlyBird-Early-Fusion-for-Multi-View-Tracking-in-the-Bird’s-Eye-View" class="headerlink" title="EarlyBird: Early-Fusion for Multi-View Tracking in the Bird’s Eye View"></a>EarlyBird: Early-Fusion for Multi-View Tracking in the Bird’s Eye View</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13350">http://arxiv.org/abs/2310.13350</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tteepe/EarlyBird">https://github.com/tteepe/EarlyBird</a></li>
<li>paper_authors: Torben Teepe, Philipp Wolters, Johannes Gilg, Fabian Herzog, Gerhard Rigoll</li>
<li>for: 本研究旨在探讨 whether tracking in the Bird’s Eye View (BEV) can bring the next performance breakthrough in Multi-Target Multi-Camera (MTMC) tracking.</li>
<li>methods: 本研究使用 early-fusion 方法，通过在 BEV 中探测每个人并学习强的 Re-Identification (re-ID) 特征来实现时间相关性 association.</li>
<li>results: results 显示 early-fusion 在 BEV 中可以达到高精度的探测和跟踪。 EarlyBird 方法在 Wildtrack 上超过当前状态艺术，提高了 MOTA 和 IDF1 的表现。<details>
<summary>Abstract</summary>
Multi-view aggregation promises to overcome the occlusion and missed detection challenge in multi-object detection and tracking. Recent approaches in multi-view detection and 3D object detection made a huge performance leap by projecting all views to the ground plane and performing the detection in the Bird's Eye View (BEV). In this paper, we investigate if tracking in the BEV can also bring the next performance breakthrough in Multi-Target Multi-Camera (MTMC) tracking. Most current approaches in multi-view tracking perform the detection and tracking task in each view and use graph-based approaches to perform the association of the pedestrian across each view. This spatial association is already solved by detecting each pedestrian once in the BEV, leaving only the problem of temporal association. For the temporal association, we show how to learn strong Re-Identification (re-ID) features for each detection. The results show that early-fusion in the BEV achieves high accuracy for both detection and tracking. EarlyBird outperforms the state-of-the-art methods and improves the current state-of-the-art on Wildtrack by +4.6 MOTA and +5.6 IDF1.
</details>
<details>
<summary>摘要</summary>
多视图聚合承诺可以超越 occlusion 和缺失检测挑战在多目标检测和跟踪中。 current 方法在多视图检测和3D对象检测中取得了巨大的性能突破，通过将所有视图投影到地面平面并在 Bird's Eye View（BEV）中进行检测。在这篇论文中，我们研究了 whether 在 BEV 中进行跟踪可以带来下一个性能突破在 Multi-Target Multi-Camera（MTMC）跟踪中。大多数当前方法在多视图跟踪中都是在每个视图中进行检测和跟踪任务，并使用图形基本方法来进行视图之间的关联。这个空间关联已经被推缩到在 BEV 中检测每个人员一次，只剩下时间关联的问题。为了解决时间关联问题，我们展示了如何学习强大的 Re-Identification（re-ID）特征 для每个检测。结果显示，在 BEV 中早期融合可以 дости得高度的准确率 both 检测和跟踪。 EarlyBird 超越了当前状态的方法，并提高了 Wildtrack 中当前状态的 MOTA 和 IDF1 的表现+4.6 和 +5.6。
</details></li>
</ul>
<hr>
<h2 id="DeepFDR-A-Deep-Learning-based-False-Discovery-Rate-Control-Method-for-Neuroimaging-Data"><a href="#DeepFDR-A-Deep-Learning-based-False-Discovery-Rate-Control-Method-for-Neuroimaging-Data" class="headerlink" title="DeepFDR: A Deep Learning-based False Discovery Rate Control Method for Neuroimaging Data"></a>DeepFDR: A Deep Learning-based False Discovery Rate Control Method for Neuroimaging Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13349">http://arxiv.org/abs/2310.13349</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taehyo Kim, Hai Shu, Qiran Jia, Mony de Leon</li>
<li>for: 这篇论文是为了解决voxel-based多测试问题，特别是处理大脑中复杂的空间相关性。</li>
<li>methods: 这篇论文提出了一种基于深度学习的空间FDR控制方法，利用无监督学习图像分割来解决voxel-based多测试问题。</li>
<li>results: 数值研究表明，DeepFDR比现有方法更有效地控制FDR，同时减少了假发现率，并且具有优秀的计算效率，适用于处理大规模的神经成像数据。<details>
<summary>Abstract</summary>
Voxel-based multiple testing is widely used in neuroimaging data analysis. Traditional false discovery rate (FDR) control methods often ignore the spatial dependence among the voxel-based tests and thus suffer from substantial loss of testing power. While recent spatial FDR control methods have emerged, their validity and optimality remain questionable when handling the complex spatial dependencies of the brain. Concurrently, deep learning methods have revolutionized image segmentation, a task closely related to voxel-based multiple testing. In this paper, we propose DeepFDR, a novel spatial FDR control method that leverages unsupervised deep learning-based image segmentation to address the voxel-based multiple testing problem. Numerical studies, including comprehensive simulations and Alzheimer's disease FDG-PET image analysis, demonstrate DeepFDR's superiority over existing methods. DeepFDR not only excels in FDR control and effectively diminishes the false nondiscovery rate, but also boasts exceptional computational efficiency highly suited for tackling large-scale neuroimaging data.
</details>
<details>
<summary>摘要</summary>
voxel基于多测试广泛应用于神经成像数据分析中。传统的假发现率（FDR）控制方法通常忽略了VOXEL基于测试之间的空间依赖关系，从而导致了重大的测试力下降。而最近的空间FDR控制方法已经出现，但它们在处理大脑复杂的空间依赖关系时的有效性和优化性仍然存在问题。同时，深度学习方法在图像分割领域已经引领了革命，这个领域与VOXEL基于多测试密切相关。在这篇论文中，我们提出了DeepFDR，一种基于深度学习无监督图像分割的新的空间FDR控制方法。 numerics 研究，包括完整的 simulations和阿尔茨海默症FDG-PET图像分析，表明DeepFDR在现有方法中具有优越性，不仅在FDR控制方面 excellence，还能够减少假发现率，同时具有出色的计算效率，适合处理大规模神经成像数据。
</details></li>
</ul>
<hr>
<h2 id="DeepFracture-A-Generative-Approach-for-Predicting-Brittle-Fractures"><a href="#DeepFracture-A-Generative-Approach-for-Predicting-Brittle-Fractures" class="headerlink" title="DeepFracture: A Generative Approach for Predicting Brittle Fractures"></a>DeepFracture: A Generative Approach for Predicting Brittle Fractures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13344">http://arxiv.org/abs/2310.13344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhang Huang, Takashi Kanai</li>
<li>for: 这篇论文是关于在不可逆摇摆动画中生成真实的破坏动画，使用物理 simulate 技术，但是使用 Voronoi 图或先 Fraction 的方法可能缺乏真实感。</li>
<li>methods: 该论文提出了一种基于学习的方法，将真实的不可逆破坏动画与 rigid-body  simulations 集成。该方法使用 BEM 不可逆破坏 simulations 生成破坏模式和碰撞条件，然后将其作为学习过程的输入。</li>
<li>results: 该论文的实验结果表明，该方法可以生成比现有方法更加细节的不可逆破坏动画，同时保持了可观计算效率。<details>
<summary>Abstract</summary>
In the realm of brittle fracture animation, generating realistic destruction animations with physics simulation techniques can be computationally expensive. Although methods using Voronoi diagrams or pre-fractured patterns work for real-time applications, they often lack realism in portraying brittle fractures. This paper introduces a novel learning-based approach for seamlessly merging realistic brittle fracture animations with rigid-body simulations. Our method utilizes BEM brittle fracture simulations to create fractured patterns and collision conditions for a given shape, which serve as training data for the learning process. To effectively integrate collision conditions and fractured shapes into a deep learning framework, we introduce the concept of latent impulse representation and geometrically-segmented signed distance function (GS-SDF). The latent impulse representation serves as input, capturing information about impact forces on the shape's surface. Simultaneously, a GS-SDF is used as the output representation of the fractured shape. To address the challenge of optimizing multiple fractured pattern targets with a single latent code, we propose an eight-dimensional latent space based on a normal distribution code within our latent impulse representation design. This adaptation effectively transforms our neural network into a generative one. Our experimental results demonstrate that our approach can generate significantly more detailed brittle fractures compared to existing techniques, all while maintaining commendable computational efficiency during run-time.
</details>
<details>
<summary>摘要</summary>
在破碎破坏动画领域，通过物理 simulations 技术生成真实的破坏动画可以 computationally expensive. Although methods using Voronoi diagrams or pre-fractured patterns work for real-time applications, they often lack realism in portraying brittle fractures. This paper introduces a novel learning-based approach for seamlessly merging realistic brittle fracture animations with rigid-body simulations. Our method utilizes BEM brittle fracture simulations to create fractured patterns and collision conditions for a given shape, which serve as training data for the learning process. To effectively integrate collision conditions and fractured shapes into a deep learning framework, we introduce the concept of latent impulse representation and geometrically-segmented signed distance function (GS-SDF). The latent impulse representation serves as input, capturing information about impact forces on the shape's surface. Simultaneously, a GS-SDF is used as the output representation of the fractured shape. To address the challenge of optimizing multiple fractured pattern targets with a single latent code, we propose an eight-dimensional latent space based on a normal distribution code within our latent impulse representation design. This adaptation effectively transforms our neural network into a generative one. Our experimental results demonstrate that our approach can generate significantly more detailed brittle fractures compared to existing techniques, all while maintaining commendable computational efficiency during run-time.
</details></li>
</ul>
<hr>
<h2 id="CylinderTag-An-Accurate-and-Flexible-Marker-for-Cylinder-Shape-Objects-Pose-Estimation-Based-on-Projective-Invariants"><a href="#CylinderTag-An-Accurate-and-Flexible-Marker-for-Cylinder-Shape-Objects-Pose-Estimation-Based-on-Projective-Invariants" class="headerlink" title="CylinderTag: An Accurate and Flexible Marker for Cylinder-Shape Objects Pose Estimation Based on Projective Invariants"></a>CylinderTag: An Accurate and Flexible Marker for Cylinder-Shape Objects Pose Estimation Based on Projective Invariants</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13320">http://arxiv.org/abs/2310.13320</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wsakobe/cylindertag">https://github.com/wsakobe/cylindertag</a></li>
<li>paper_authors: Shaoan Wang, Mingzhu Zhu, Yaoqing Hu, Dongyue Li, Fusong Yuan, Junzhi Yu</li>
<li>for: 高精度位势估计基于视觉标记，用于圆柱形表面上的物体位势估计。</li>
<li>methods: 提出了一种新的视觉标记 called CylinderTag，适用于可扩展的圆柱形表面，并使用了拟合投影射影的拟合环境来编码方向。</li>
<li>results: 通过广泛的实验评估，显示了CylinderTag在不同视角下的检测率、检测速度、字典大小、地方缩动和位势估计精度等性能指标，并且在实时检测和广泛的应用场景中展现出了优秀的表现。<details>
<summary>Abstract</summary>
High-precision pose estimation based on visual markers has been a thriving research topic in the field of computer vision. However, the suitability of traditional flat markers on curved objects is limited due to the diverse shapes of curved surfaces, which hinders the development of high-precision pose estimation for curved objects. Therefore, this paper proposes a novel visual marker called CylinderTag, which is designed for developable curved surfaces such as cylindrical surfaces. CylinderTag is a cyclic marker that can be firmly attached to objects with a cylindrical shape. Leveraging the manifold assumption, the cross-ratio in projective invariance is utilized for encoding in the direction of zero curvature on the surface. Additionally, to facilitate the usage of CylinderTag, we propose a heuristic search-based marker generator and a high-performance recognizer as well. Moreover, an all-encompassing evaluation of CylinderTag properties is conducted by means of extensive experimentation, covering detection rate, detection speed, dictionary size, localization jitter, and pose estimation accuracy. CylinderTag showcases superior detection performance from varying view angles in comparison to traditional visual markers, accompanied by higher localization accuracy. Furthermore, CylinderTag boasts real-time detection capability and an extensive marker dictionary, offering enhanced versatility and practicality in a wide range of applications. Experimental results demonstrate that the CylinderTag is a highly promising visual marker for use on cylindrical-like surfaces, thus offering important guidance for future research on high-precision visual localization of cylinder-shaped objects. The code is available at: https://github.com/wsakobe/CylinderTag.
</details>
<details>
<summary>摘要</summary>
高精度姿势估计基于视觉标记已经在计算机视觉领域得到了广泛的研究。然而，传统的平面标记在抛物线表面上的适用性受到抛物线表面的多样性所限制，这难以实现高精度姿势估计 для抛物线对象。因此，这篇论文提出了一种新的视觉标记 called CylinderTag，适用于可开发的抛物线表面，如圆柱体表面。CylinderTag 是一种循环标记，可以固定在圆柱体形状的对象上。利用 manifold 假设，在项目ive 的均衡点上使用 cross-ratio 的编码，以获得在表面上的方向。此外，为了使 CylinderTag 更加可用，我们提出了一种冒泡搜索基于的标记生成器和高性能的识别器。此外，我们还进行了广泛的实验，评估 CylinderTag 的性能，包括检测率、检测速度、字典大小、本地化振荡和姿势估计精度。CylinderTag 在不同视角下的检测性能显著高于传统 visual marker，同时具有更高的本地化精度。此外，CylinderTag 具有实时检测能力和广泛的标记字典，提供了更好的 versatility 和实用性，适用于广泛的应用场景。实验结果表明，CylinderTag 是一种非常有前途的视觉标记，适用于圆柱体形状的对象，为高精度视觉本地化做出了重要的指导。代码可以在以下链接获取：https://github.com/wsakobe/CylinderTag。
</details></li>
</ul>
<hr>
<h2 id="Non-Negative-Spherical-Relaxations-for-Universe-Free-Multi-Matching-and-Clustering"><a href="#Non-Negative-Spherical-Relaxations-for-Universe-Free-Multi-Matching-and-Clustering" class="headerlink" title="Non-Negative Spherical Relaxations for Universe-Free Multi-Matching and Clustering"></a>Non-Negative Spherical Relaxations for Universe-Free Multi-Matching and Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13311">http://arxiv.org/abs/2310.13311</a></li>
<li>repo_url: None</li>
<li>paper_authors: Johan Thunberg, Florian Bernard</li>
<li>for:  optimize optimization problems over binary matrices with injectivity constraints, particularly for multi-matching and clustering</li>
<li>methods: 使用非负圆形 relaxation 和 conditional power iteration 方法优化relaxed问题</li>
<li>results: 比较spectral multi-matching和spectral clustering方法，方法不需要额外处理获得binary结果，并且在多种多样的多匹配和归一化设定下表现出优异result<details>
<summary>Abstract</summary>
We propose a novel non-negative spherical relaxation for optimization problems over binary matrices with injectivity constraints, which in particular has applications in multi-matching and clustering. We relax respective binary matrix constraints to the (high-dimensional) non-negative sphere. To optimize our relaxed problem, we use a conditional power iteration method to iteratively improve the objective function, while at same time sweeping over a continuous scalar parameter that is (indirectly) related to the universe size (or number of clusters). Opposed to existing procedures that require to fix the integer universe size before optimization, our method automatically adjusts the analogous continuous parameter. Furthermore, while our approach shares similarities with spectral multi-matching and spectral clustering, our formulation has the strong advantage that we do not rely on additional post-processing procedures to obtain binary results. Our method shows compelling results in various multi-matching and clustering settings, even when compared to methods that use the ground truth universe size (or number of clusters).
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的非负球体relaxation算法，用于优化包含二进制矩阵约束的优化问题，具体应用于多对约束和分组。我们将相应的二进制矩阵约束relax到高维非负球体上。为优化我们的放宽问题，我们使用一种增量的力论迭代方法，同时逐步提高目标函数，并同时探索一个连续的浮点参数，这个参数与宇宙大小（或群集数）相关。与现有方法不同，我们的方法不需要先确定整数宇宙大小，而是通过迭代进行调整。此外，我们的方法与spectral multi-matching和spectral clustering相似，但我们不需要额外的后处理步骤来获得 binary结果。我们的方法在多个多对约束和分组设置中显示出了吸引人的 результа。
</details></li>
</ul>
<hr>
<h2 id="CXR-CLIP-Toward-Large-Scale-Chest-X-ray-Language-Image-Pre-training"><a href="#CXR-CLIP-Toward-Large-Scale-Chest-X-ray-Language-Image-Pre-training" class="headerlink" title="CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training"></a>CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13292">http://arxiv.org/abs/2310.13292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kihyun You, Jawook Gu, Jiyeon Ham, Beomhee Park, Jiho Kim, Eun Kyoung Hong, Woonhyunk Baek, Byungseok Roh</li>
<li>for: 推动视力语言预训练（VLP）模型的发展，以便在医疗领域实现零或几次shot分类无需昂贵的标注。</li>
<li>methods: 对于缺乏医学图像文本数据的问题，我们通过扩展图像标签对进行了图像文本对的扩展，并利用多个图像和多个 radiologic report 中的多个部分。 我们还设计了两种对比损失函数（ICL和TCL），用于学习医学图像和报告的研究级特征。</li>
<li>results: 我们的模型在同样的条件下比现有模型强度，并且扩大的数据集提高了我们预训练模型的分类特征，尽管有一定的损失性能。<details>
<summary>Abstract</summary>
A large-scale image-text pair dataset has greatly contributed to the development of vision-language pre-training (VLP) models, which enable zero-shot or few-shot classification without costly annotation. However, in the medical domain, the scarcity of data remains a significant challenge for developing a powerful VLP model. In this paper, we tackle the lack of image-text data in chest X-ray by expanding image-label pair as image-text pair via general prompt and utilizing multiple images and multiple sections in a radiologic report. We also design two contrastive losses, named ICL and TCL, for learning study-level characteristics of medical images and reports, respectively. Our model outperforms the state-of-the-art models trained under the same conditions. Also, enlarged dataset improve the discriminative power of our pre-trained model for classification, while sacrificing marginal retrieval performance. Code is available at https://github.com/kakaobrain/cxr-clip.
</details>
<details>
<summary>摘要</summary>
一个大规模图文对比数据集的出现对视语言预训练（VLP）模型的发展做出了重要贡献，这些模型可以在零shot或几shot的情况下实现无需高成本标注的分类。然而，在医疗领域，数据的缺乏仍然是开发强大VLP模型的主要挑战。在这篇论文中，我们解决了骨科X射影像数据的缺乏问题，通过扩展图标对为图文对和使用多个图像和多个 radiologic report 中的多个部分。我们还设计了两种对比损失，即 IC 损失和 TC 损失，用于学习医疗图像和报告的学习特征。我们的模型在同样的条件下训练时与现有的状态艺模型相比，表现出色。此外，扩大数据集也提高了我们预训练模型的分类特征， хотя是相对较差的检索性能。代码可以在 <https://github.com/kakaobrain/cxr-clip> 上下载。
</details></li>
</ul>
<hr>
<h2 id="Pathologist-Like-Explanations-Unveiled-an-Explainable-Deep-Learning-System-for-White-Blood-Cell-Classification"><a href="#Pathologist-Like-Explanations-Unveiled-an-Explainable-Deep-Learning-System-for-White-Blood-Cell-Classification" class="headerlink" title="Pathologist-Like Explanations Unveiled: an Explainable Deep Learning System for White Blood Cell Classification"></a>Pathologist-Like Explanations Unveiled: an Explainable Deep Learning System for White Blood Cell Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13279">http://arxiv.org/abs/2310.13279</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya Shankar Pal, Debojyoti Biswas, Joy Mahapatra, Debasis Banerjee, Prantar Chakrabarti, Utpal Garain</li>
<li>For: 这个研究旨在开发一种可解释的深度学习模型，以提高白细胞分类的准确性和可读性。* Methods: 该模型使用深度学习算法和五个特征（granularity、细胞色素、核形状、大小相对红细胞、核：细胞比率）进行自动白细胞分类、定位和分割。* Results: 模型在一个新的数据集上进行训练和评估，并达到了81.08%的平均分类精度和89.16%的精度指数。此外，模型还能够准确地预测五个特征的值，并且与其他多种状态对模型的影响进行了比较。<details>
<summary>Abstract</summary>
White blood cells (WBCs) play a crucial role in safeguarding the human body against pathogens and foreign substances. Leveraging the abundance of WBC imaging data and the power of deep learning algorithms, automated WBC analysis has the potential for remarkable accuracy. However, the capability of deep learning models to explain their WBC classification remains largely unexplored. In this study, we introduce HemaX, an explainable deep neural network-based model that produces pathologist-like explanations using five attributes: granularity, cytoplasm color, nucleus shape, size relative to red blood cells, and nucleus to cytoplasm ratio (N:C), along with cell classification, localization, and segmentation. HemaX is trained and evaluated on a novel dataset, LeukoX, comprising 467 blood smear images encompassing ten (10) WBC types. The proposed model achieves impressive results, with an average classification accuracy of 81.08% and a Jaccard index of 89.16% for cell localization. Additionally, HemaX performs well in generating the five explanations with a normalized mean square error of 0.0317 for N:C ratio and over 80% accuracy for the other four attributes. Comprehensive experiments comparing against multiple state-of-the-art models demonstrate that HemaX's classification accuracy remains unaffected by its ability to provide explanations. Moreover, empirical analyses and validation by expert hematologists confirm the faithfulness of explanations predicted by our proposed model.
</details>
<details>
<summary>摘要</summary>
白血球（WBC）在人体免疫系统中扮演着关键性角色，深受人们的关注。利用白血球图像数据的丰富性和深入学习算法的力量，自动化白血球分类有很大的潜力。然而，深入学习模型对其白血球分类的解释能力尚未得到充分探索。在这项研究中，我们提出了HemaX模型，它是一种可解释的深度神经网络模型，可以生成医生类似的解释，包括五个特征：粒度、细胞膜颜色、核形状、大小相对红细胞、核：细胞膜比率（N：C），同时还包括细胞类别、局部化和分割。HemaX模型在一个新的数据集LeukoX上进行训练和评估，LeukoX数据集包含467个血液干细胞图像，涵盖10种白血球类型。我们的模型在这些实验中获得了非常出色的结果，其中平均分类精度为81.08%，Jaccard指数为89.16%，用于细胞局部化。此外，HemaX模型在生成五个特征的解释方面也表现出色，其中N：C比率的normalized mean square error为0.0317，其他四个特征的解释准确率大于80%。在多个现有的状态对比模型的实验中，我们发现HemaX模型的分类精度不受其能提供解释的影响。此外，由专家血液学家验证的实验和验证表明，HemaX模型生成的解释准确性很高。
</details></li>
</ul>
<hr>
<h2 id="DPM-Solver-v3-Improved-Diffusion-ODE-Solver-with-Empirical-Model-Statistics"><a href="#DPM-Solver-v3-Improved-Diffusion-ODE-Solver-with-Empirical-Model-Statistics" class="headerlink" title="DPM-Solver-v3: Improved Diffusion ODE Solver with Empirical Model Statistics"></a>DPM-Solver-v3: Improved Diffusion ODE Solver with Empirical Model Statistics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13268">http://arxiv.org/abs/2310.13268</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thu-ml/dpm-solver-v3">https://github.com/thu-ml/dpm-solver-v3</a></li>
<li>paper_authors: Kaiwen Zheng, Cheng Lu, Jianfei Chen, Jun Zhu<br>for:DPM-Solver-v3 is designed to improve the efficiency of high-fidelity image generation using diffusion probabilistic models (DPMs).methods:The proposed method introduces a new fast ODE solver for DPMs, which minimizes the first-order discretization error of the ODE solution and incorporates multistep methods and a predictor-corrector framework.results:DPM-Solver-v3 achieves consistently better or comparable performance in both unconditional and conditional sampling with both pixel-space and latent-space DPMs, especially in 5-10 NFEs. The method achieves FIDs of 12.21 (5 NFE), 2.51 (10 NFE) on unconditional CIFAR10, and MSE of 0.55 (5 NFE, 7.5 guidance scale) on Stable Diffusion, bringing a speed-up of 15%-30% compared to previous state-of-the-art training-free methods.<details>
<summary>Abstract</summary>
Diffusion probabilistic models (DPMs) have exhibited excellent performance for high-fidelity image generation while suffering from inefficient sampling. Recent works accelerate the sampling procedure by proposing fast ODE solvers that leverage the specific ODE form of DPMs. However, they highly rely on specific parameterization during inference (such as noise/data prediction), which might not be the optimal choice. In this work, we propose a novel formulation towards the optimal parameterization during sampling that minimizes the first-order discretization error of the ODE solution. Based on such formulation, we propose DPM-Solver-v3, a new fast ODE solver for DPMs by introducing several coefficients efficiently computed on the pretrained model, which we call empirical model statistics. We further incorporate multistep methods and a predictor-corrector framework, and propose some techniques for improving sample quality at small numbers of function evaluations (NFE) or large guidance scales. Experiments show that DPM-Solver-v3 achieves consistently better or comparable performance in both unconditional and conditional sampling with both pixel-space and latent-space DPMs, especially in 5$\sim$10 NFEs. We achieve FIDs of 12.21 (5 NFE), 2.51 (10 NFE) on unconditional CIFAR10, and MSE of 0.55 (5 NFE, 7.5 guidance scale) on Stable Diffusion, bringing a speed-up of 15%$\sim$30% compared to previous state-of-the-art training-free methods. Code is available at https://github.com/thu-ml/DPM-Solver-v3.
</details>
<details>
<summary>摘要</summary>
Diffusion probabilistic models (DPMs) 有出色的表现在高精度图像生成中，但是它们在采样过程中存在不fficient的问题。 current works 提出了快速的 ODE 解决方案，但是它们高度依赖于特定的参数化 durante inference（如噪声/数据预测），这可能并不是最佳选择。在这项工作中，我们提出了一种新的参数化形式，以实现采样过程中的首项累累误差最小化。基于这种形式，我们提出了 DPM-Solver-v3，一种新的快速 ODE 解决方案，通过引入一些高效计算的参数，我们称之为 empirical model statistics。我们还 incorporated multistep methods and a predictor-corrector framework，并提出了一些提高样本质量的技术， especialy in small numbers of function evaluations (NFE) or large guidance scales。实验表明，DPM-Solver-v3 可以在 both unconditional 和 conditional 采样中实现更好或相当的表现，特别是在 5 ∼ 10 NFE 的情况下。我们在 CIFAR10 上获得了 FID 的 12.21 (5 NFE)，MSE 的 0.55 (5 NFE, 7.5 指导尺度)，在 Stable Diffusion 上提高了训练free 方法的速度，比前一代 state-of-the-art 快速 15% ∼ 30%。代码可以在 https://github.com/thu-ml/DPM-Solver-v3 上获得。
</details></li>
</ul>
<hr>
<h2 id="UE4-NeRF-Neural-Radiance-Field-for-Real-Time-Rendering-of-Large-Scale-Scene"><a href="#UE4-NeRF-Neural-Radiance-Field-for-Real-Time-Rendering-of-Large-Scale-Scene" class="headerlink" title="UE4-NeRF:Neural Radiance Field for Real-Time Rendering of Large-Scale Scene"></a>UE4-NeRF:Neural Radiance Field for Real-Time Rendering of Large-Scale Scene</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13263">http://arxiv.org/abs/2310.13263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaming Gu, Minchao Jiang, Hongsheng Li, Xiaoyuan Lu, Guangming Zhu, Syed Afaq Ali Shah, Liang Zhang, Mohammed Bennamoun</li>
<li>For: This paper proposes a novel neural rendering system called UE4-NeRF for real-time rendering of large-scale scenes using NeRF technology.* Methods: The proposed method partitions each large scene into multiple sub-NeRFs, uses polygonal meshes to represent the scene, and trains meshes of varying levels of detail for different observation levels.* Results: The proposed method achieves real-time rendering of large-scale scenes at 4K resolution with a frame rate of up to 43 FPS, and achieves rendering quality comparable to state-of-the-art approaches.Here’s the full summary in Simplified Chinese:* 为：本文提出了一种基于NeRF技术的新型神经采集系统UE4-NeRF，用于实时渲染大规模场景。* 方法：提议方法将每个大场景分割成多个子NeRF，使用多面体来表示场景，并在不同观察水平上进行不同精度的 mesh 训练。* 结果：提议方法在4K分辨率下实现了实时渲染大规模场景，帧率可达43帧&#x2F;秒，并与现有方法的渲染质量相当。<details>
<summary>Abstract</summary>
Neural Radiance Fields (NeRF) is a novel implicit 3D reconstruction method that shows immense potential and has been gaining increasing attention. It enables the reconstruction of 3D scenes solely from a set of photographs. However, its real-time rendering capability, especially for interactive real-time rendering of large-scale scenes, still has significant limitations. To address these challenges, in this paper, we propose a novel neural rendering system called UE4-NeRF, specifically designed for real-time rendering of large-scale scenes. We partitioned each large scene into different sub-NeRFs. In order to represent the partitioned independent scene, we initialize polygonal meshes by constructing multiple regular octahedra within the scene and the vertices of the polygonal faces are continuously optimized during the training process. Drawing inspiration from Level of Detail (LOD) techniques, we trained meshes of varying levels of detail for different observation levels. Our approach combines with the rasterization pipeline in Unreal Engine 4 (UE4), achieving real-time rendering of large-scale scenes at 4K resolution with a frame rate of up to 43 FPS. Rendering within UE4 also facilitates scene editing in subsequent stages. Furthermore, through experiments, we have demonstrated that our method achieves rendering quality comparable to state-of-the-art approaches. Project page: https://jamchaos.github.io/UE4-NeRF/.
</details>
<details>
<summary>摘要</summary>
neural radiance fields (NeRF) 是一种新型的隐式 3D 重建方法，它在过去几年内得到了广泛关注，并且在3D 场景的重建方面表现出了巨大的潜力。它可以通过一组照片来重建 3D 场景，但是它的实时渲染能力，特别是对于大规模场景的实时渲染，仍然存在一定的限制。为了解决这些挑战，在这篇论文中，我们提出了一种基于 neural rendering 的新型渲染系统，即 UE4-NeRF，用于实时渲染大规模场景。我们将每个大场景分解成不同的子 NeRF，以便更好地表示独立的场景。为了表示分解后的独立场景，我们在场景中构建多个正则八面体，并将其中的多边形面的顶点进行不断优化 durante el proceso de entrenamiento。 drawing inspiration from Level of Detail (LOD) techniques, we trained meshes of varying levels of detail for different observation levels。我们的方法结合了 UE4 的渲染管线，实现了实时渲染大规模场景的 4K 分辨率，帧率可达 43 FPS。此外，通过实验，我们证明了我们的方法可以实现与当前状态最佳的渲染质量。项目页面：https://jamchaos.github.io/UE4-NeRF/。
</details></li>
</ul>
<hr>
<h2 id="Domain-specific-optimization-and-diverse-evaluation-of-self-supervised-models-for-histopathology"><a href="#Domain-specific-optimization-and-diverse-evaluation-of-self-supervised-models-for-histopathology" class="headerlink" title="Domain-specific optimization and diverse evaluation of self-supervised models for histopathology"></a>Domain-specific optimization and diverse evaluation of self-supervised models for histopathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13259">http://arxiv.org/abs/2310.13259</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeremy Lai, Faruk Ahmed, Supriya Vijay, Tiam Jaroensri, Jessica Loo, Saurabh Vyawahare, Saloni Agarwal, Fayaz Jamil, Yossi Matias, Greg S. Corrado, Dale R. Webster, Jonathan Krause, Yun Liu, Po-Hsuan Cameron Chen, Ellery Wulczyn, David F. Steiner</li>
<li>for: 提高诊断、临床研究和精准医学的可能性</li>
<li>methods: 自我超VI方法</li>
<li>results: 标准SSL方法在 Histopathology 图像上表现出色，具有各种任务类型和资源的可用性。另外，域pecific的方法ological improvements可以进一步提高性能。<details>
<summary>Abstract</summary>
Task-specific deep learning models in histopathology offer promising opportunities for improving diagnosis, clinical research, and precision medicine. However, development of such models is often limited by availability of high-quality data. Foundation models in histopathology that learn general representations across a wide range of tissue types, diagnoses, and magnifications offer the potential to reduce the data, compute, and technical expertise necessary to develop task-specific deep learning models with the required level of model performance. In this work, we describe the development and evaluation of foundation models for histopathology via self-supervised learning (SSL). We first establish a diverse set of benchmark tasks involving 17 unique tissue types and 12 unique cancer types and spanning different optimal magnifications and task types. Next, we use this benchmark to explore and evaluate histopathology-specific SSL methods followed by further evaluation on held out patch-level and weakly supervised tasks. We found that standard SSL methods thoughtfully applied to histopathology images are performant across our benchmark tasks and that domain-specific methodological improvements can further increase performance. Our findings reinforce the value of using domain-specific SSL methods in pathology, and establish a set of high quality foundation models to enable further research across diverse applications.
</details>
<details>
<summary>摘要</summary>
任务特定的深度学习模型在 histopathology 领域提供了可能性，以提高诊断、临床研究和精准医学。然而，开发这些模型经常受到高质量数据的限制。基本模型在 histopathology 中学习通用表示，可以降低数据、计算和技术专家的努力，以达到需要的模型性能。在这种工作中，我们描述了通过自我超级学习（SSL）的基本模型的开发和评估。我们首先建立了 17 种不同的组织类型和 12 种不同的癌种类，以及不同的最佳放大和任务类型的多样化基准任务。然后，我们使用这些基准任务来探索和评估特有的 histopathology SSL 方法，并在储存的 patch-level 和弱级批处理任务上进行进一步评估。我们发现，针对 histopathology 图像的标准 SSL 方法在我们的基准任务中表现良好，而域pecific的方法学习改进可以进一步提高表现。我们的发现赋予了使用域pecific SSL 方法在 pathology 领域的价值，并建立了一组高质量的基本模型，以便进一步的研究和应用。
</details></li>
</ul>
<hr>
<h2 id="Steve-Eye-Equipping-LLM-based-Embodied-Agents-with-Visual-Perception-in-Open-Worlds"><a href="#Steve-Eye-Equipping-LLM-based-Embodied-Agents-with-Visual-Perception-in-Open-Worlds" class="headerlink" title="Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds"></a>Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13255">http://arxiv.org/abs/2310.13255</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sipeng Zheng, Jiazheng Liu, Yicheng Feng, Zongqing Lu</li>
<li>for: 这篇论文旨在帮助大语言模型（LLM） equip 身体机器人具备自适应能力，但这些努力通常忽略了开放世界的视觉丰富性，导致LLM-based Agent难以直观理解它所处的环境和生成易于理解的回应。</li>
<li>methods: 我们提出了Steve-Eye，一个结合LLM和视觉编码器的大多模式模型，可以处理视觉文本输入并生成多模式反馈。我们还采用了半自动策略来收集了850K的开放世界指令对，使我们的模型涵盖了三种函数：多模式感知、基础知识库和技能预测和规划。</li>
<li>results: 我们开发了三个开放世界评估标准，然后从多个角度进行了广泛的实验来验证我们的模型能够策略性行动和规划。<details>
<summary>Abstract</summary>
Recent studies have presented compelling evidence that large language models (LLMs) can equip embodied agents with the self-driven capability to interact with the world, which marks an initial step toward versatile robotics. However, these efforts tend to overlook the visual richness of open worlds, rendering the entire interactive process akin to "a blindfolded text-based game." Consequently, LLM-based agents frequently encounter challenges in intuitively comprehending their surroundings and producing responses that are easy to understand. In this paper, we propose Steve-Eye, an end-to-end trained large multimodal model designed to address this limitation. Steve-Eye integrates the LLM with a visual encoder which enables it to process visual-text inputs and generate multimodal feedback. In addition, we use a semi-automatic strategy to collect an extensive dataset comprising 850K open-world instruction pairs, empowering our model to encompass three essential functions for an agent: multimodal perception, foundational knowledge base, and skill prediction and planning. Lastly, we develop three open-world evaluation benchmarks, then carry out extensive experiments from a wide range of perspectives to validate our model's capability to strategically act and plan. Codes and datasets will be released.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Diagnosis-oriented-Medical-Image-Compression-with-Efficient-Transfer-Learning"><a href="#Diagnosis-oriented-Medical-Image-Compression-with-Efficient-Transfer-Learning" class="headerlink" title="Diagnosis-oriented Medical Image Compression with Efficient Transfer Learning"></a>Diagnosis-oriented Medical Image Compression with Efficient Transfer Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13250">http://arxiv.org/abs/2310.13250</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangqi Xie, Xin Li, Xiaohan Pan, Zhibo Chen</li>
<li>for: 这篇论文主要应用于Remote medical diagnosis中，以提高医疗资料传输效率和准确性。</li>
<li>methods: 本论文提出了一个新的医疗图像压缩方法，即Diagnosis-oriented medical image compression，通过将医疗资料分类和压缩，以减少传输成本。</li>
<li>results: 实验结果显示，该方法可以实现47.594%的BD-Rate减少，比HEVC标准更高效。另外，仅需要对政策网络的A2C模块（2.7%的参数）进行调整，并且只需要使用1个医疗样本。<details>
<summary>Abstract</summary>
Remote medical diagnosis has emerged as a critical and indispensable technique in practical medical systems, where medical data are required to be efficiently compressed and transmitted for diagnosis by either professional doctors or intelligent diagnosis devices. In this process, a large amount of redundant content irrelevant to the diagnosis is subjected to high-fidelity coding, leading to unnecessary transmission costs. To mitigate this, we propose diagnosis-oriented medical image compression, a special semantic compression task designed for medical scenarios, targeting to reduce the compression cost without compromising the diagnosis accuracy. However, collecting sufficient medical data to optimize such a compression system is significantly expensive and challenging due to privacy issues and the lack of professional annotation. In this study, we propose DMIC, the first efficient transfer learning-based codec, for diagnosis-oriented medical image compression, which can be effectively optimized with only few-shot annotated medical examples, by reusing the knowledge in the existing reinforcement learning-based task-driven semantic coding framework, i.e., HRLVSC [1]. Concretely, we focus on tuning only the partial parameters of the policy network for bit allocation within HRLVSC, which enables it to adapt to the medical images. In this work, we validate our DMIC with the typical medical task, Coronary Artery Segmentation. Extensive experiments have demonstrated that our DMIC can achieve 47.594%BD-Rate savings compared to the HEVC anchor, by tuning only the A2C module (2.7% parameters) of the policy network with only 1 medical sample.
</details>
<details>
<summary>摘要</summary>
医学远程诊断已成为现代医疗系统中不可或缺的技术，医疗数据需要高效压缩并传输以便诊断，由专业医生或智能诊断设备进行诊断。在这个过程中，大量不相关于诊断的医疗数据会被高精度编码，导致无必要的传输成本增加。为了解决这个问题，我们提出了诊断导向医学影像压缩，这是专门为医疗场景设计的Semantic压缩任务，旨在降低压缩成本而无需妥协诊断精度。然而，收集足够的医疗数据来优化这种压缩系统是非常昂贵和困难的，主要是因为隐私问题和缺乏专业标注。在这个研究中，我们提出了DMIC，首个高效转移学习基于codec，用于诊断导向医学影像压缩。DMIC可以通过只需几个 annotated medical example来有效优化， reuse existing reinforcement learning-based task-driven semantic coding framework HRLVSC 的知识，以适应医学影像。我们在实验中验证了我们的DMIC，并与典型的医学任务Coronary Artery Segmentation进行了比较。结果表明，我们的DMIC可以在与HEVC anchor进行比较时实现47.594%BD-Rate savings，只需要调整HRLVSC中A2C模块（2.7%参数）和1个医学样本。
</details></li>
</ul>
<hr>
<h2 id="Auxiliary-Features-Guided-Super-Resolution-for-Monte-Carlo-Rendering"><a href="#Auxiliary-Features-Guided-Super-Resolution-for-Monte-Carlo-Rendering" class="headerlink" title="Auxiliary Features-Guided Super Resolution for Monte Carlo Rendering"></a>Auxiliary Features-Guided Super Resolution for Monte Carlo Rendering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13235">http://arxiv.org/abs/2310.13235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiqi Hou, Feng Liu</li>
<li>for: 提高 Monte Carlo 渲染算法的速度，通过使用超分辨率技术减少像素数量。</li>
<li>methods: 利用高分辨率辅助特征来导航超分辨率渲染，并使用 Cross-modality Transformer 网络和 residual densely-connected Swin Transformer 组合来提取代表性特征。</li>
<li>results: 对比超分辨率渲染和 Monte Carlo 释除方法，auxiliary features 导航的超分辨率渲染方法可以提供更高质量的渲染图像。<details>
<summary>Abstract</summary>
This paper investigates super resolution to reduce the number of pixels to render and thus speed up Monte Carlo rendering algorithms. While great progress has been made to super resolution technologies, it is essentially an ill-posed problem and cannot recover high-frequency details in renderings. To address this problem, we exploit high-resolution auxiliary features to guide super resolution of low-resolution renderings. These high-resolution auxiliary features can be quickly rendered by a rendering engine and at the same time provide valuable high-frequency details to assist super resolution. To this end, we develop a cross-modality Transformer network that consists of an auxiliary feature branch and a low-resolution rendering branch. These two branches are designed to fuse high-resolution auxiliary features with the corresponding low-resolution rendering. Furthermore, we design residual densely-connected Swin Transformer groups to learn to extract representative features to enable high-quality super-resolution. Our experiments show that our auxiliary features-guided super-resolution method outperforms both super-resolution methods and Monte Carlo denoising methods in producing high-quality renderings.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="PTSR-Patch-Translator-for-Image-Super-Resolution"><a href="#PTSR-Patch-Translator-for-Image-Super-Resolution" class="headerlink" title="PTSR: Patch Translator for Image Super-Resolution"></a>PTSR: Patch Translator for Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13216">http://arxiv.org/abs/2310.13216</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neeraj Baghel, Shiv Ram Dubey, Satish Kumar Singh</li>
<li>for: 提高图像超分辨率，降低计算成本和存储量。</li>
<li>methods: 提议一种基于 transformer 的 GAN 网络，不含 convolution 操作，使用 patch translator 模块来重新生成改进的 patches，并由生成器使用这些 patches 生成 2x 和 4x 超分辨率图像。</li>
<li>results: 对于 DIV2K、Set5、Set14 和 BSD100 等标准数据集进行了实验，与最佳竞争模型相比，提出的模型在 $4\times$ 超分辨率上提高了平均21.66% 的 PNSR 分数和11.59% 的 SSIM 分数。还分析了提议的损失函数和焦点图来说明提议方法的效果。<details>
<summary>Abstract</summary>
Image super-resolution generation aims to generate a high-resolution image from its low-resolution image. However, more complex neural networks bring high computational costs and memory storage. It is still an active area for offering the promise of overcoming resolution limitations in many applications. In recent years, transformers have made significant progress in computer vision tasks as their robust self-attention mechanism. However, recent works on the transformer for image super-resolution also contain convolution operations. We propose a patch translator for image super-resolution (PTSR) to address this problem. The proposed PTSR is a transformer-based GAN network with no convolution operation. We introduce a novel patch translator module for regenerating the improved patches utilising multi-head attention, which is further utilised by the generator to generate the 2x and 4x super-resolution images. The experiments are performed using benchmark datasets, including DIV2K, Set5, Set14, and BSD100. The results of the proposed model is improved on an average for $4\times$ super-resolution by 21.66% in PNSR score and 11.59% in SSIM score, as compared to the best competitive models. We also analyse the proposed loss and saliency map to show the effectiveness of the proposed method.
</details>
<details>
<summary>摘要</summary>
Image超解算生成目标是生成高分辨率图像从低分辨率图像。然而，更复杂的神经网络带来高计算成本和存储空间。这仍然是活跃的领域，提供超分辨率限制的缺乏的解决方案。在最近几年，转换器在计算机视觉任务中做出了重要进步，它的稳定自注意机制使得它在图像分类、 object detection 等任务中表现出色。然而，最近对转换器的图像超解算任务也包含了 convolution 操作。我们提出了一种图像超解算patch translator（PTSR），以解决这个问题。我们的提案中的 PTSR 是基于转换器的 GAN 网络，没有 convolution 操作。我们引入了一种新的 patch translator 模块，用于重新生成改进的 patches，使用多头注意机制，该机制被生成器使用，以生成 2x 和 4x 超分辨率图像。我们在 DIV2K、Set5、Set14 和 BSD100 等标准 datasets 上进行了实验。我们的提案模型在 $4\times$ 超分辨率下提高了平均 21.66% 的 PNSR 分数和 11.59% 的 SSIM 分数，相比最佳竞争对手。我们还分析了我们的提案损失函数和质感图，以证明我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Zone-Evaluation-Revealing-Spatial-Bias-in-Object-Detection"><a href="#Zone-Evaluation-Revealing-Spatial-Bias-in-Object-Detection" class="headerlink" title="Zone Evaluation: Revealing Spatial Bias in Object Detection"></a>Zone Evaluation: Revealing Spatial Bias in Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13215">http://arxiv.org/abs/2310.13215</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zzh-tju/zoneeval">https://github.com/zzh-tju/zoneeval</a></li>
<li>paper_authors: Zhaohui Zheng, Yuming Chen, Qibin Hou, Xiang Li, Ping Wang, Ming-Ming Cheng</li>
<li>for: 本研究旨在探讨对象检测器的空间偏好问题，并提出了一种新的zone评价协议来评估对象检测器的性能。</li>
<li>methods: 本研究使用了10种popular对象检测器和5个检测数据集，通过对zone评价协议进行广泛评估，探讨对象检测器的空间偏好问题。</li>
<li>results: 研究发现，对象检测器在图像边缘zone（96%）的性能不达到AP值（平均精度），表明对象检测器在这个zone中表现不佳。此外，研究还发现，对象检测器的性能异常分布在不同的zone中，存在一定的空间偏好问题。<details>
<summary>Abstract</summary>
A fundamental limitation of object detectors is that they suffer from "spatial bias", and in particular perform less satisfactorily when detecting objects near image borders. For a long time, there has been a lack of effective ways to measure and identify spatial bias, and little is known about where it comes from and what degree it is. To this end, we present a new zone evaluation protocol, extending from the traditional evaluation to a more generalized one, which measures the detection performance over zones, yielding a series of Zone Precisions (ZPs). For the first time, we provide numerical results, showing that the object detectors perform quite unevenly across the zones. Surprisingly, the detector's performance in the 96\% border zone of the image does not reach the AP value (Average Precision, commonly regarded as the average detection performance in the entire image zone). To better understand spatial bias, a series of heuristic experiments are conducted. Our investigation excludes two intuitive conjectures about spatial bias that the object scale and the absolute positions of objects barely influence the spatial bias. We find that the key lies in the human-imperceptible divergence in data patterns between objects in different zones, thus eventually forming a visible performance gap between the zones. With these findings, we finally discuss a future direction for object detection, namely, spatial disequilibrium problem, aiming at pursuing a balanced detection ability over the entire image zone. By broadly evaluating 10 popular object detectors and 5 detection datasets, we shed light on the spatial bias of object detectors. We hope this work could raise a focus on detection robustness. The source codes, evaluation protocols, and tutorials are publicly available at \url{https://github.com/Zzh-tju/ZoneEval}.
</details>
<details>
<summary>摘要</summary>
基本限制的物体探测器受到"空间偏见"的影响，特别是在图像边缘附近探测物体的时候表现不如预期。在这个问题上，我们提出了一种新的区域评估协议，从传统评估扩展到更通用的一种，可以测量探测器在不同区域的性能，并提取一系列的Zone精度（ZP）。这是第一次提供数字结果，表明探测器在图像96%边缘区域的性能不达到AP值（平均精度，通常被视为整个图像区域的平均探测性能）。为了更好地理解空间偏见，我们进行了一系列的启发性实验。我们的调查排除了两个直觉的假设：物体大小和绝对位置对空间偏见的影响。我们发现关键在于数据模式之间的人类不可见差异，因此 eventually形成了不同区域的可见性 gap。通过这些发现，我们最后讨论了对象探测的未来方向，即空间不平衡问题，旨在寻求整个图像区域内均匀的探测能力。通过评估10款流行的对象探测器和5个检测数据集，我们突出了对象探测器的空间偏见。我们希望这项工作能够启发对检测稳定性的关注。评估协议、评估工具和教程都可以在<https://github.com/Zzh-tju/ZoneEval>获得。
</details></li>
</ul>
<hr>
<h2 id="Identification-of-Abnormality-in-Maize-Plants-From-UAV-Images-Using-Deep-Learning-Approaches"><a href="#Identification-of-Abnormality-in-Maize-Plants-From-UAV-Images-Using-Deep-Learning-Approaches" class="headerlink" title="Identification of Abnormality in Maize Plants From UAV Images Using Deep Learning Approaches"></a>Identification of Abnormality in Maize Plants From UAV Images Using Deep Learning Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13201">http://arxiv.org/abs/2310.13201</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aminul Huq, Dimitris Zermas, George Bebis</li>
<li>for: 早期识别植物异常是重要的任务，以确保植物健康成长和获得高产量。精准农业可以受益于现代计算机视觉工具，使农业策略变得有效和高效。由于农业地域很大，农民需要手动检查广泛的地区，以确定植物的状况和应用有效的治疗方法。</li>
<li>methods: 我们使用深度学习技术来检测不同水平的异常（即低、中、高或无异常）在玉米植物图像中。我们的方法可以独立地识别不同的生长阶段，并且可以提供有价值的信息 для人工标注数据收集，帮助他们专注于更小的图像集中进行标注。</li>
<li>results: 我们对公共可用的数据集进行了实验，并取得了promising的初步结果，包括88.89%的低异常检测精度和100%的无异常检测精度。<details>
<summary>Abstract</summary>
Early identification of abnormalities in plants is an important task for ensuring proper growth and achieving high yields from crops. Precision agriculture can significantly benefit from modern computer vision tools to make farming strategies addressing these issues efficient and effective. As farming lands are typically quite large, farmers have to manually check vast areas to determine the status of the plants and apply proper treatments. In this work, we consider the problem of automatically identifying abnormal regions in maize plants from images captured by a UAV. Using deep learning techniques, we have developed a methodology which can detect different levels of abnormality (i.e., low, medium, high or no abnormality) in maize plants independently of their growth stage. The primary goal is to identify anomalies at the earliest possible stage in order to maximize the effectiveness of potential treatments. At the same time, the proposed system can provide valuable information to human annotators for ground truth data collection by helping them to focus their attention on a much smaller set of images only. We have experimented with two different but complimentary approaches, the first considering abnormality detection as a classification problem and the second considering it as a regression problem. Both approaches can be generalized to different types of abnormalities and do not make any assumption about the abnormality occurring at an early plant growth stage which might be easier to detect due to the plants being smaller and easier to separate. As a case study, we have considered a publicly available data set which exhibits mostly Nitrogen deficiency in maize plants of various growth stages. We are reporting promising preliminary results with an 88.89\% detection accuracy of low abnormality and 100\% detection accuracy of no abnormality.
</details>
<details>
<summary>摘要</summary>
早期识别植物异常是重要的任务，以确保生长正常并实现高产量的农作物。精准农业可以受益于现代计算机视觉工具，以制定有效和高效的农业策略。由于农地往往非常大，因此农民需要手动检查广泛的地区，以确定植物的状况和应用相应的治疗。在这个工作中，我们考虑了自动地检测玉米植物中的异常区域，使用深度学习技术。我们开发了一种方法，可以独立地分类不同程度的异常（即低、中、高或无异常）在玉米植物中。我们的主要目标是在最早的时间内识别异常，以最大化治疗的效果。同时，我们的提posed系统可以为人类标注数据收集提供有价值的信息，帮助他们只需要关注相对较少的图像。我们对两种不同 pero complementary的方法进行了实验，第一种视异常检测为分类问题，第二种视异常检测为回归问题。两种方法都可以泛化到不同的异常类型，不假设异常发生在植物在早期生长阶段，可能更容易检测由于植物较小和易于分离。作为案例研究，我们使用了公开可用的数据集，该数据集主要表现出玉米植物中的缺氮缺乏。我们报告了有前景的初步结果，包括88.89%的低异常检测精度和100%的无异常检测精度。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/20/cs.CV_2023_10_20/" data-id="clpxp040100lvfm884mfs3jo5" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_10_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/20/cs.AI_2023_10_20/" class="article-date">
  <time datetime="2023-10-20T12:00:00.000Z" itemprop="datePublished">2023-10-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/20/cs.AI_2023_10_20/">cs.AI - 2023-10-20</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Deep-Learning-Approaches-for-Dynamic-Mechanical-Analysis-of-Viscoelastic-Fiber-Composites"><a href="#Deep-Learning-Approaches-for-Dynamic-Mechanical-Analysis-of-Viscoelastic-Fiber-Composites" class="headerlink" title="Deep Learning Approaches for Dynamic Mechanical Analysis of Viscoelastic Fiber Composites"></a>Deep Learning Approaches for Dynamic Mechanical Analysis of Viscoelastic Fiber Composites</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15188">http://arxiv.org/abs/2310.15188</a></li>
<li>repo_url: None</li>
<li>paper_authors: Victor Hoffmann, Ilias Nahmed, Parisa Rastin, Guénaël Cabanes, Julien Boisse</li>
<li>for: 这篇论文是为了映射微结构到其机械性能的映射，以便通过深度神经网络快速设计和理解微结构。</li>
<li>methods: 该论文使用了机器学习技术，特别是深度神经网络，来映射微结构到机械性能。</li>
<li>results: 该论文可以快速地映射微结构到机械性能，从而帮助设计和理解微结构。<details>
<summary>Abstract</summary>
The increased adoption of reinforced polymer (RP) composite materials, driven by eco-design standards, calls for a fine balance between lightness, stiffness, and effective vibration control. These materials are integral to enhancing comfort, safety, and energy efficiency. Dynamic Mechanical Analysis (DMA) characterizes viscoelastic behavior, yet there's a growing interest in using Machine Learning (ML) to expedite the design and understanding of microstructures. In this paper we aim to map microstructures to their mechanical properties using deep neural networks, speeding up the process and allowing for the generation of microstructures from desired properties.
</details>
<details>
<summary>摘要</summary>
随着复合材料（RP）的广泛应用，受生态设计标准的推动，需要均衡轻量、刚性和有效的振荡控制。这些材料对于提高舒适、安全和能效性具有重要作用。动态机械分析（DMA）可以描述弹性行为，但是有一种增长的兴趣是使用机器学习（ML）来加速设计和理解微结构。在这篇论文中，我们希望通过深度神经网络将微结构映射到机械性能上，从而加快过程并允许从所需的性能开发微结构。
</details></li>
</ul>
<hr>
<h2 id="Evoke-Evoking-Critical-Thinking-Abilities-in-LLMs-via-Reviewer-Author-Prompt-Editing"><a href="#Evoke-Evoking-Critical-Thinking-Abilities-in-LLMs-via-Reviewer-Author-Prompt-Editing" class="headerlink" title="Evoke: Evoking Critical Thinking Abilities in LLMs via Reviewer-Author Prompt Editing"></a>Evoke: Evoking Critical Thinking Abilities in LLMs via Reviewer-Author Prompt Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13855">http://arxiv.org/abs/2310.13855</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/Evoke">https://github.com/microsoft/Evoke</a></li>
<li>paper_authors: Xinyu Hu, Pengfei Tang, Simiao Zuo, Zihan Wang, Bowen Song, Qiang Lou, Jian Jiao, Denis Charles</li>
<li>For: The paper is written for improving the performance of large language models (LLMs) in natural language processing tasks by proposing a new prompt refinement framework called Evoke.* Methods: The paper proposes using two instances of a same LLM, one as a reviewer and one as an author, to refine prompts through an author-reviewer feedback loop. The paper also introduces a data selection approach to expose the LLM to hard samples.* Results: The paper reports significant outperformance of Evoke compared to existing methods, with Evoke scoring above 80 in the challenging task of logical fallacy detection while other baseline methods struggle to reach 20.<details>
<summary>Abstract</summary>
Large language models (LLMs) have made impressive progress in natural language processing. These models rely on proper human instructions (or prompts) to generate suitable responses. However, the potential of LLMs are not fully harnessed by commonly-used prompting methods: many human-in-the-loop algorithms employ ad-hoc procedures for prompt selection; while auto prompt generation approaches are essentially searching all possible prompts randomly and inefficiently. We propose Evoke, an automatic prompt refinement framework. In Evoke, there are two instances of a same LLM: one as a reviewer (LLM-Reviewer), it scores the current prompt; the other as an author (LLM-Author), it edits the prompt by considering the edit history and the reviewer's feedback. Such an author-reviewer feedback loop ensures that the prompt is refined in each iteration. We further aggregate a data selection approach to Evoke, where only the hard samples are exposed to the LLM. The hard samples are more important because the LLM can develop deeper understanding of the tasks out of them, while the model may already know how to solve the easier cases. Experimental results show that Evoke significantly outperforms existing methods. For instance, in the challenging task of logical fallacy detection, Evoke scores above 80, while all other baseline methods struggle to reach 20.
</details>
<details>
<summary>摘要</summary>
大型自然语言处理模型（LLM）已经做出了很好的进步。这些模型需要合适的人类指导（或提示）来生成适当的响应。然而，LLM的潜在能力未被充分利用，因为常用的人类在循环算法使用了临时的方法来选择提示，而自动生成提示方法则是随机搜索所有可能的提示。我们提出了“触发”，一个自动提示修充框架。在触发中，有两个相同的LLM实例：一个作为评审者（LLM-Reviewer），它评分当前提示；另一个作为作者（LLM-Author），它通过考虑编辑历史和评审者的反馈来修改提示。这种作者-评审者反馈循环确保提示在每次迭代中得到修充。我们还添加了一种数据选择方法到触发中，只暴露困难样本给LLM。困难样本更重要，因为LLM可以通过它们更深入理解任务，而模型可能已经知道如何解决较易的 случа子。实验结果表明，触发明确超越了现有方法。例如，在逻辑错误检测任务中，触发得分超过80，而所有基准方法几乎无法达到20。
</details></li>
</ul>
<hr>
<h2 id="CNN-based-Prediction-of-Partition-Path-for-VVC-Fast-Inter-Partitioning-Using-Motion-Fields"><a href="#CNN-based-Prediction-of-Partition-Path-for-VVC-Fast-Inter-Partitioning-Using-Motion-Fields" class="headerlink" title="CNN-based Prediction of Partition Path for VVC Fast Inter Partitioning Using Motion Fields"></a>CNN-based Prediction of Partition Path for VVC Fast Inter Partitioning Using Motion Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13838">http://arxiv.org/abs/2310.13838</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/simon123123/vtm10_fast_dt_inter_partition_pcs2021">https://github.com/simon123123/vtm10_fast_dt_inter_partition_pcs2021</a></li>
<li>paper_authors: Yiqun Liu, Marc Riviere, Thomas Guionnet, Aline Roumy, Christine Guillemot<br>for: 这个研究旨在提高VVC编码器的速度，使其在高效视频编码（HEVC）标准下提供更高的压缩效率和更高的编码复杂度。methods: 该研究提出了一种基于卷积神经网络（CNN）的方法，通过预测QUADTREE分割路径来加速VVC编码器的INTERPARTITIONING过程。在这种方法中，首先引入了一种新的QUADTREE分割表示法，然后开发了一种基于U-Net的CNN，用于在CTU级别预测分割路径。results: 实验表明，提出的方法可以在RAGOP32配置下实现加速率 range from 16.5% to 60.2%，而且与此同时，BD-rate的影响在0.44% to 4.59%之间，这超过了其他当前的解决方案。此外，该方法还被认为是当前场景中最轻量级的方法之一，这使得其适用于其他编码器。<details>
<summary>Abstract</summary>
The Versatile Video Coding (VVC) standard has been recently finalized by the Joint Video Exploration Team (JVET). Compared to the High Efficiency Video Coding (HEVC) standard, VVC offers about 50% compression efficiency gain, in terms of Bjontegaard Delta-Rate (BD-rate), at the cost of a 10-fold increase in encoding complexity. In this paper, we propose a method based on Convolutional Neural Network (CNN) to speed up the inter partitioning process in VVC. Firstly, a novel representation for the quadtree with nested multi-type tree (QTMT) partition is introduced, derived from the partition path. Secondly, we develop a U-Net-based CNN taking a multi-scale motion vector field as input at the Coding Tree Unit (CTU) level. The purpose of CNN inference is to predict the optimal partition path during the Rate-Distortion Optimization (RDO) process. To achieve this, we divide CTU into grids and predict the Quaternary Tree (QT) depth and Multi-type Tree (MT) split decisions for each cell of the grid. Thirdly, an efficient partition pruning algorithm is introduced to employ the CNN predictions at each partitioning level to skip RDO evaluations of unnecessary partition paths. Finally, an adaptive threshold selection scheme is designed, making the trade-off between complexity and efficiency scalable. Experiments show that the proposed method can achieve acceleration ranging from 16.5% to 60.2% under the RandomAccess Group Of Picture 32 (RAGOP32) configuration with a reasonable efficiency drop ranging from 0.44% to 4.59% in terms of BD-rate, which surpasses other state-of-the-art solutions. Additionally, our method stands out as one of the lightest approaches in the field, which ensures its applicability to other encoders.
</details>
<details>
<summary>摘要</summary>
joint 视频探索团体（JVET）最近确定的多元视频编码标准（VVC），相比高效视频编码标准（HEVC），VVC在BD率方面提供约50%的压缩效率提升，但是需要10倍的编码复杂度增加。在这篇论文中，我们提出一种基于卷积神经网络（CNN）的方法，用于加速VVC中的分区过程。首先，我们引入了一种新的Quadtree嵌套多型树（QTMT）分区表示，基于分区路径。其次，我们开发了一种基于U-Net的CNN，用于在CTU级别上接受多尺度运动 вектор场为输入，并通过RDO过程预测最佳分区路径。为此，我们将CTU分割成格子，并预测每个格子的QT深度和MT分裂决策。第三，我们引入了一种高效的分区剔除算法，以使用CNN预测结果在每个分区级别进行分区剔除。最后，我们设计了一种可Scalable的阈值选择方案，以实现质量和效率之间的平衡。实验表明，我们的方法可以在RAGOP32配置下实现加速率 ranges from 16.5% to 60.2%，BD率上的效率损失在0.44% to 4.59%之间，超越了其他现有的解决方案。此外，我们的方法具有轻量级的特点，使其适用于其他编码器。
</details></li>
</ul>
<hr>
<h2 id="GraphMaker-Can-Diffusion-Models-Generate-Large-Attributed-Graphs"><a href="#GraphMaker-Can-Diffusion-Models-Generate-Large-Attributed-Graphs" class="headerlink" title="GraphMaker: Can Diffusion Models Generate Large Attributed Graphs?"></a>GraphMaker: Can Diffusion Models Generate Large Attributed Graphs?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13833">http://arxiv.org/abs/2310.13833</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/graph-com/graphmaker">https://github.com/graph-com/graphmaker</a></li>
<li>paper_authors: Mufei Li, Eleonora Kreačić, Vamsi K. Potluru, Pan Li</li>
<li>for: 该论文旨在生成大规模图的实用图学机器学习中，提供一种新的扩散模型，帮助理解网络演化和数据用途保持。</li>
<li>methods: 该论文提出了一种新的扩散模型，名为GraphMaker，可以生成大规模图并考虑节点属性。它还使用了节点级conditioning和小批量策略以提高可扩展性。</li>
<li>results: 实验表明，GraphMaker可以生成高质量的大规模图，并且可以在下游任务中提供有用的数据。<details>
<summary>Abstract</summary>
Large-scale graphs with node attributes are fundamental in real-world scenarios, such as social and financial networks. The generation of synthetic graphs that emulate real-world ones is pivotal in graph machine learning, aiding network evolution understanding and data utility preservation when original data cannot be shared. Traditional models for graph generation suffer from limited model capacity. Recent developments in diffusion models have shown promise in merely graph structure generation or the generation of small molecular graphs with attributes. However, their applicability to large attributed graphs remains unaddressed due to challenges in capturing intricate patterns and scalability. This paper introduces GraphMaker, a novel diffusion model tailored for generating large attributed graphs. We study the diffusion models that either couple or decouple graph structure and node attribute generation to address their complex correlation. We also employ node-level conditioning and adopt a minibatch strategy for scalability. We further propose a new evaluation pipeline using models trained on generated synthetic graphs and tested on original graphs to evaluate the quality of synthetic data. Empirical evaluations on real-world datasets showcase GraphMaker's superiority in generating realistic and diverse large-attributed graphs beneficial for downstream tasks.
</details>
<details>
<summary>摘要</summary>
大规模图像有节点特征是现实世界中常见的案例，如社交和金融网络。生成可信的图像是图机器学习中关键的，帮助理解网络的进化和数据的可用性保持，当原始数据无法分享时。传统的图像生成模型受到有限的模型容量的限制。最近的扩散模型已经显示出了在图像结构生成或小分子图像生成中的承诺。然而，它们对大型具有特征的图像仍然无法解决，因为捕捉复杂的模式和可扩展性的挑战。本文介绍了GraphMaker，一种适用于生成大型具有特征的图像的新型扩散模型。我们研究了把 grafstructures 和节点特征生成 decouple 或 couple 以处理它们之间的复杂关系。我们还使用节点级 conditioning 和采用小批量策略以实现可扩展性。此外，我们还提出了一种基于生成的 синтетиче图像模型来评估生成的图像质量。empirical evaluation 表明，GraphMaker 可以生成真实且多样的大型具有特征的图像，对下游任务具有利助。
</details></li>
</ul>
<hr>
<h2 id="Prompt-Specific-Poisoning-Attacks-on-Text-to-Image-Generative-Models"><a href="#Prompt-Specific-Poisoning-Attacks-on-Text-to-Image-Generative-Models" class="headerlink" title="Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models"></a>Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13828">http://arxiv.org/abs/2310.13828</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shawn Shan, Wenxin Ding, Josephine Passananti, Haitao Zheng, Ben Y. Zhao</li>
<li>for: 本研究旨在探讨数据恶意攻击的可行性和影响，以及可能的防御策略。</li>
<li>methods: 研究人员使用了一种名为“Nightshade”的优化后的提问特定恶意攻击方法，可以在少量恶意样本中训练模型。</li>
<li>results: 研究发现， Nightshade 攻击可以成功地让模型产生意外的输出，并且这些输出可以“延伸”到相关的概念上。此外，研究人员还发现，一些恶意攻击可以“混合”在同一个提问上，从而导致模型的总体特征变得不稳定。<details>
<summary>Abstract</summary>
Data poisoning attacks manipulate training data to introduce unexpected behaviors into machine learning models at training time. For text-to-image generative models with massive training datasets, current understanding of poisoning attacks suggests that a successful attack would require injecting millions of poison samples into their training pipeline. In this paper, we show that poisoning attacks can be successful on generative models. We observe that training data per concept can be quite limited in these models, making them vulnerable to prompt-specific poisoning attacks, which target a model's ability to respond to individual prompts.   We introduce Nightshade, an optimized prompt-specific poisoning attack where poison samples look visually identical to benign images with matching text prompts. Nightshade poison samples are also optimized for potency and can corrupt an Stable Diffusion SDXL prompt in <100 poison samples. Nightshade poison effects "bleed through" to related concepts, and multiple attacks can composed together in a single prompt. Surprisingly, we show that a moderate number of Nightshade attacks can destabilize general features in a text-to-image generative model, effectively disabling its ability to generate meaningful images. Finally, we propose the use of Nightshade` and similar tools as a last defense for content creators against web scrapers that ignore opt-out/do-not-crawl directives, and discuss possible implications for model trainers and content creators.
</details>
<details>
<summary>摘要</summary>
数据毒攻击攻击机器学习模型的训练时间，以引入无预期的行为。文本生成模型拥有庞大的训练集，目前认知中指出，成功攻击需要插入数百万毒样本到训练管道中。在这篇论文中，我们表明了毒攻击可以成功地进行在生成模型中。我们发现，生成模型中每个概念的训练数据很有限，使其易受到特定提示毒攻击，该攻击target模型对具体提示的响应能力。我们引入了“夜露”（Nightshade）优化的提示特定毒攻击，毒样本与benign图像和相同的文本提示保持视觉相同。夜露毒样本还优化了强度，可以在<100个毒样本中训练Stable Diffusion SDXL提示。夜露毒效“泄漏”到相关概念上，并且多个攻击可以在单个提示中组合 together。 surprisingly，我们发现了一些 Nightshade 攻击可以使生成模型的通用特征失效，从而禁用生成有意义的图像。最后，我们提议使用 Nightshade 和类似工具作为内容创作者对网络抓取器忽略 opt-out/do-not-crawl 指令的最后防御，并讨论了模型训练者和内容创作者的可能的影响。
</details></li>
</ul>
<hr>
<h2 id="FERI-A-Multitask-based-Fairness-Achieving-Algorithm-with-Applications-to-Fair-Organ-Transplantation"><a href="#FERI-A-Multitask-based-Fairness-Achieving-Algorithm-with-Applications-to-Fair-Organ-Transplantation" class="headerlink" title="FERI: A Multitask-based Fairness Achieving Algorithm with Applications to Fair Organ Transplantation"></a>FERI: A Multitask-based Fairness Achieving Algorithm with Applications to Fair Organ Transplantation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13820">http://arxiv.org/abs/2310.13820</a></li>
<li>repo_url: None</li>
<li>paper_authors: Can Li, Dejian Lai, Xiaoqian Jiang, Kai Zhang</li>
<li>for: 这 paper 是为了 Addressing fairness challenges in liver transplantation, particularly for subgroups defined by sensitive attributes like age group, gender, and race&#x2F;ethnicity.</li>
<li>methods: 这 paper 使用了 Machine learning models for outcome prediction, with a focus on fairness-aware predictive modeling. The authors introduce the Fairness through the Equitable Rate of Improvement in Multitask Learning (FERI) algorithm, which constrains subgroup loss and prevents subgroup dominance in the training process.</li>
<li>results: 试验表明，FERI 可以 maintain high predictive accuracy with AUROC and AUPRC comparable to baseline models, while improving fairness. Specifically, for gender, FERI reduces the demographic parity disparity by 71.74%, and for the age group, it decreases the equalized odds disparity by 40.46%.<details>
<summary>Abstract</summary>
Liver transplantation often faces fairness challenges across subgroups defined by sensitive attributes like age group, gender, and race/ethnicity. Machine learning models for outcome prediction can introduce additional biases. To address these, we introduce Fairness through the Equitable Rate of Improvement in Multitask Learning (FERI) algorithm for fair predictions of graft failure risk in liver transplant patients. FERI constrains subgroup loss by balancing learning rates and preventing subgroup dominance in the training process. Our experiments show that FERI maintains high predictive accuracy with AUROC and AUPRC comparable to baseline models. More importantly, FERI demonstrates an ability to improve fairness without sacrificing accuracy. Specifically, for gender, FERI reduces the demographic parity disparity by 71.74%, and for the age group, it decreases the equalized odds disparity by 40.46%. Therefore, the FERI algorithm advances fairness-aware predictive modeling in healthcare and provides an invaluable tool for equitable healthcare systems.
</details>
<details>
<summary>摘要</summary>
肝移植往往面临公平挑战，特别是在年龄组、性别和种族/民族等敏感属性下的分组。机器学习模型用于结果预测可能会引入额外偏见。为解决这些问题，我们介绍了在多任务学习中实现公平性的 Fairness through the Equitable Rate of Improvement in Multitask Learning（FERI）算法。FERI通过均衡学习率和避免分组占据训练过程中的分组主导来约束分组损失。我们的实验表明，FERI可以保持高度的预测精度，AUROC和AUPRC与基线模型相当。此外，FERI能够提高公平性而不牺牲准确性。特别是gender方面，FERI可以降低性别差距的比例为71.74%，而年龄组方面，它可以降低平等 odds差距的比例为40.46%。因此，FERI算法为健康卫生领域的公平预测模型提供了一个有价值的工具，并且能够推动公平的医疗系统。
</details></li>
</ul>
<hr>
<h2 id="FATA-Trans-Field-And-Time-Aware-Transformer-for-Sequential-Tabular-Data"><a href="#FATA-Trans-Field-And-Time-Aware-Transformer-for-Sequential-Tabular-Data" class="headerlink" title="FATA-Trans: Field And Time-Aware Transformer for Sequential Tabular Data"></a>FATA-Trans: Field And Time-Aware Transformer for Sequential Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13818">http://arxiv.org/abs/2310.13818</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zdy93/fata-trans">https://github.com/zdy93/fata-trans</a></li>
<li>paper_authors: Dongyu Zhang, Liang Wang, Xin Dai, Shubham Jain, Junpeng Wang, Yujie Fan, Chin-Chia Michael Yeh, Yan Zheng, Zhongfang Zhuang, Wei Zhang</li>
<li>for: 这个研究旨在提出一个适合批评Sequential Tabular Data（STD）的模型，并且能够对STD进行有效的分析和处理。</li>
<li>methods: 本研究提出了一个名为FATA-Trans的模型，它使用了两个Field Transformer来处理STD，并且透过Field-type embedding和Time-aware position embedding来capture static和动态字段之间的差异和时间序列资讯。</li>
<li>results: 实验结果显示，FATA-Trans在下游任务中的学习表现都高于现有的方法，并且通过可视化研究来显示了模型所捕捉的内在结构和时间行为图像。<details>
<summary>Abstract</summary>
Sequential tabular data is one of the most commonly used data types in real-world applications. Different from conventional tabular data, where rows in a table are independent, sequential tabular data contains rich contextual and sequential information, where some fields are dynamically changing over time and others are static. Existing transformer-based approaches analyzing sequential tabular data overlook the differences between dynamic and static fields by replicating and filling static fields into each transformer, and ignore temporal information between rows, which leads to three major disadvantages: (1) computational overhead, (2) artificially simplified data for masked language modeling pre-training task that may yield less meaningful representations, and (3) disregarding the temporal behavioral patterns implied by time intervals. In this work, we propose FATA-Trans, a model with two field transformers for modeling sequential tabular data, where each processes static and dynamic field information separately. FATA-Trans is field- and time-aware for sequential tabular data. The field-type embedding in the method enables FATA-Trans to capture differences between static and dynamic fields. The time-aware position embedding exploits both order and time interval information between rows, which helps the model detect underlying temporal behavior in a sequence. Our experiments on three benchmark datasets demonstrate that the learned representations from FATA-Trans consistently outperform state-of-the-art solutions in the downstream tasks. We also present visualization studies to highlight the insights captured by the learned representations, enhancing our understanding of the underlying data. Our codes are available at https://github.com/zdy93/FATA-Trans.
</details>
<details>
<summary>摘要</summary>
纵向表格数据是现实世界中最常用的数据类型之一。与传统的表格数据不同，纵向表格数据含有较为复杂的上下文和时间序列信息，其中一些字段会随时间的推移而变化，而其他字段则是静态的。现有的转换器基本方法会忽略时间序列中的不同性和顺序信息，这会导致三个主要缺点：（1）计算成本高，（2）为隐藏语言模型预训练任务提供不准确的数据，（3）忽略时间序列中的行为模式。在这项工作中，我们提议了FATA-Trans模型，它包含两个字段转换器，每个转换器都会处理不同的静态和动态字段信息。FATA-Trans模型具有场景和时间意识，可以有效地处理纵向表格数据。我们在三个标准数据集上进行了实验，结果显示FATA-Trans模型在下游任务中的学习表现 Always outperform了现有的解决方案。我们还提供了视觉研究，以帮助我们更好地理解下面数据的含义。我们的代码可以在https://github.com/zdy93/FATA-Trans上获取。
</details></li>
</ul>
<hr>
<h2 id="Enhanced-Low-Dimensional-Sensing-Mapless-Navigation-of-Terrestrial-Mobile-Robots-Using-Double-Deep-Reinforcement-Learning-Techniques"><a href="#Enhanced-Low-Dimensional-Sensing-Mapless-Navigation-of-Terrestrial-Mobile-Robots-Using-Double-Deep-Reinforcement-Learning-Techniques" class="headerlink" title="Enhanced Low-Dimensional Sensing Mapless Navigation of Terrestrial Mobile Robots Using Double Deep Reinforcement Learning Techniques"></a>Enhanced Low-Dimensional Sensing Mapless Navigation of Terrestrial Mobile Robots Using Double Deep Reinforcement Learning Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13809">http://arxiv.org/abs/2310.13809</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lindamoraes/turtlebot-project">https://github.com/lindamoraes/turtlebot-project</a></li>
<li>paper_authors: Linda Dotto de Moraes, Victor Augusto Kich, Alisson Henrique Kolling, Jair Augusto Bottega, Ricardo Bedin Grando, Anselmo Rafael Cukla, Daniel Fernando Tello Gamarra</li>
<li>for: 这项研究旨在提高无地图导航的地面机器人能力。</li>
<li>methods: 研究采用了两种不同的深度强化学习方法，一是基于深度Q网络（DQN）算法的方法，另一是基于双深度Q网络（DDQN）算法的方法。两种方法都使用了激光范围探测器提供的24个量据，并且通过Agent的位差和方向相对于目标来决定导航。</li>
<li>results: 研究发现，使用Double Deep结构可以显著提高地面机器人的导航能力，并且不需要依赖于复杂的图像输入。在三个实际环境中进行了评估，结果表明 Double Deep 结构在导航任务中表现出了显著的优势。<details>
<summary>Abstract</summary>
In this study, we present two distinct approaches within the realm of Deep Reinforcement Learning (Deep-RL) aimed at enhancing mapless navigation for a ground-based mobile robot. The research methodology primarily involves a comparative analysis between a Deep-RL strategy grounded in the foundational Deep Q-Network (DQN) algorithm, and an alternative approach based on the Double Deep Q-Network (DDQN) algorithm. The agents in these approaches leverage 24 measurements from laser range sampling, coupled with the agent's positional differentials and orientation relative to the target. This amalgamation of data influences the agents' determinations regarding navigation, ultimately dictating the robot's velocities. By embracing this parsimonious sensory framework as proposed, we successfully showcase the training of an agent for proficiently executing navigation tasks and adeptly circumventing obstacles. Notably, this accomplishment is attained without a dependency on intricate sensory inputs like those inherent to image-centric methodologies. The proposed methodology is evaluated in three different real environments, revealing that Double Deep structures significantly enhance the navigation capabilities of mobile robots compared to simple Q structures.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们提出了两种不同的深度强化学习方法（深度强化学习），用于增强地面机器人无地图导航。我们的研究方法主要包括对使用深度Q网络（DQN）算法和双深度Q网络（DDQN）算法两种方法进行比较分析。这两种方法的代理人利用24个激光距测样本，并结合代理人的位差和方向偏差 relative to the target，以Influence the agents' determinations regarding navigation, ultimately dictating the robot's velocities。通过采用这种简单的感知框架，我们成功地训练了一个能够高效执行导航任务并绕过障碍物的代理人。值得注意的是，这种成功不依赖于复杂的感知输入，如图像中心的方法。我们的方法在三个不同的实际环境中进行了评估，结果显示，使用双深度结构可以增强移动机器人的导航能力，相比于单深度结构。
</details></li>
</ul>
<hr>
<h2 id="RoseNet-Predicting-Energy-Metrics-of-Double-InDel-Mutants-Using-Deep-Learning"><a href="#RoseNet-Predicting-Energy-Metrics-of-Double-InDel-Mutants-Using-Deep-Learning" class="headerlink" title="RoseNet: Predicting Energy Metrics of Double InDel Mutants Using Deep Learning"></a>RoseNet: Predicting Energy Metrics of Double InDel Mutants Using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13806">http://arxiv.org/abs/2310.13806</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarah Coffland, Katie Christensen, Filip Jagodzinski, Brian Hutchinson</li>
<li>For: The paper is written to explore the use of computational methods to model insertion and deletion (InDel) mutations in proteins, specifically using a deep learning approach called RoseNet to predict the effects of InDel mutations on protein structure and function.* Methods: The paper uses a combination of computational methods, including the Rosetta protein structure prediction software and deep learning techniques, to generate and analyze exhaustive double InDel mutations for three proteins. The authors develop and train RoseNet on several structural and energetic metrics output by Rosetta during the mutant generation process.* Results: The paper presents the results of the authors’ experiments using RoseNet to predict the effects of InDel mutations on protein structure and function. The authors show that RoseNet can accurately emulate the exhaustive data set using deep learning methods, and demonstrate the ability of the model to predict Rosetta metrics for unseen mutant sequences with two InDels. The paper also includes a sensitivity analysis to determine the necessary quantity of data required to accurately emulate the structural scores for computationally generated mutants.<details>
<summary>Abstract</summary>
An amino acid insertion or deletion, or InDel, can have profound and varying functional impacts on a protein's structure. InDel mutations in the transmembrane conductor regulator protein for example give rise to cystic fibrosis. Unfortunately performing InDel mutations on physical proteins and studying their effects is a time prohibitive process. Consequently, modeling InDels computationally can supplement and inform wet lab experiments. In this work, we make use of our data sets of exhaustive double InDel mutations for three proteins which we computationally generated using a robotics inspired inverse kinematics approach available in Rosetta. We develop and train a neural network, RoseNet, on several structural and energetic metrics output by Rosetta during the mutant generation process. We explore and present how RoseNet is able to emulate the exhaustive data set using deep learning methods, and show to what extent it can predict Rosetta metrics for unseen mutant sequences with two InDels. RoseNet achieves a Pearson correlation coefficient median accuracy of 0.775 over all Rosetta scores for the largest protein. Furthermore, a sensitivity analysis is performed to determine the necessary quantity of data required to accurately emulate the structural scores for computationally generated mutants. We show that the model can be trained on minimal data (<50%) and still retain a high level of accuracy.
</details>
<details>
<summary>摘要</summary>
一个氨基酸插入或删除（InDel）可能对蛋白质的结构产生深刻和多样化的功能影响。例如，InDel 变异在传输膜调控蛋白中会导致肾上腺炎病。然而，在实验室中进行InDel变异和研究其效果是一个时间紧张的过程。因此，通过计算方式模拟InDel变异可以补充和指导 wet lab experiment。在这项工作中，我们使用了我们已经生成的double InDel 变异数据集，其中包括三种蛋白质的计算生成的双重InDel 变异。我们开发了一个神经网络模型，称为RoseNet，并在Rosetta中生成的多种结构和能量指标上训练这个模型。我们探索了RoseNet是如何使用深度学习方法来模拟数据集，并对未经见过的双重InDel 变异序列预测Rosetta指标的能力。RoseNet在最大蛋白质中达到了 median 准确率0.775。此外，我们进行了敏感分析，以确定模型需要多少数据来准确模拟计算生成的结构分数。我们发现模型可以在少量数据（<50%）上训练并仍保持高级别的准确率。
</details></li>
</ul>
<hr>
<h2 id="Improving-Molecular-Properties-Prediction-Through-Latent-Space-Fusion"><a href="#Improving-Molecular-Properties-Prediction-Through-Latent-Space-Fusion" class="headerlink" title="Improving Molecular Properties Prediction Through Latent Space Fusion"></a>Improving Molecular Properties Prediction Through Latent Space Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13802">http://arxiv.org/abs/2310.13802</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ibm/molformer">https://github.com/ibm/molformer</a></li>
<li>paper_authors: Eduardo Soares, Akihiro Kishimoto, Emilio Vital Brazil, Seiji Takeda, Hiroshi Kajino, Renato Cerqueira<br>for:This paper aims to enhance the efficacy of pre-trained language models for predicting molecular properties by combining latent spaces derived from state-of-the-art chemical models.methods:The proposed approach combines the embeddings derived from MHG-GNN, which represents molecular structures as graphs, and MoLFormer embeddings rooted in chemical language. The attention mechanism of MoLFormer is able to identify relations between two atoms even when their distance is far apart, while the GNN of MHG-GNN can more precisely capture relations among multiple atoms closely located.results:The proposed multi-view approach outperforms existing state-of-the-art methods, including MoLFormer-XL, in predicting clinical trial drug toxicity and inhibiting HIV replication, as demonstrated on six benchmark datasets from MoleculeNet. The approach achieves superior performance in intricate tasks, and the use of small versions of MHG-GNN and MoLFormer opens up an opportunity for further improvement when using a larger-scale dataset.Here is the result in Simplified Chinese text:for: 这篇论文目标是提高预训练语言模型在预测分子性质方面的效果，通过结合化物学模型的状态空间。methods: 该方法结合MHG-GNN的嵌入， representing molecular structures as graphs，以及基于化学语言的MoLFormer嵌入。MoLFormer的注意机制可以在两个原子之间识别远距离的关系，而MHG-GNN的GNN可以更好地捕捉多个原子 closely located的关系。results: 该方法在六个MoleculeNet的标准数据集上表现出色，比如MoLFormer-XL、在临床药物到ксиicity预测和HIV复制抑制方面取得了更高的性能，特别是在复杂任务中。使用小版本的MHG-GNN和MoLFormer，这开 up了进一步改进的机会，当使用更大规模的数据集时。<details>
<summary>Abstract</summary>
Pre-trained Language Models have emerged as promising tools for predicting molecular properties, yet their development is in its early stages, necessitating further research to enhance their efficacy and address challenges such as generalization and sample efficiency. In this paper, we present a multi-view approach that combines latent spaces derived from state-of-the-art chemical models. Our approach relies on two pivotal elements: the embeddings derived from MHG-GNN, which represent molecular structures as graphs, and MoLFormer embeddings rooted in chemical language. The attention mechanism of MoLFormer is able to identify relations between two atoms even when their distance is far apart, while the GNN of MHG-GNN can more precisely capture relations among multiple atoms closely located. In this work, we demonstrate the superior performance of our proposed multi-view approach compared to existing state-of-the-art methods, including MoLFormer-XL, which was trained on 1.1 billion molecules, particularly in intricate tasks such as predicting clinical trial drug toxicity and inhibiting HIV replication. We assessed our approach using six benchmark datasets from MoleculeNet, where it outperformed competitors in five of them. Our study highlights the potential of latent space fusion and feature integration for advancing molecular property prediction. In this work, we use small versions of MHG-GNN and MoLFormer, which opens up an opportunity for further improvement when our approach uses a larger-scale dataset.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Specific-versus-General-Principles-for-Constitutional-AI"><a href="#Specific-versus-General-Principles-for-Constitutional-AI" class="headerlink" title="Specific versus General Principles for Constitutional AI"></a>Specific versus General Principles for Constitutional AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13798">http://arxiv.org/abs/2310.13798</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sandipan Kundu, Yuntao Bai, Saurav Kadavath, Amanda Askell, Andrew Callahan, Anna Chen, Anna Goldie, Avital Balwit, Azalia Mirhoseini, Brayden McLean, Catherine Olsson, Cassie Evraets, Eli Tran-Johnson, Esin Durmus, Ethan Perez, Jackson Kernion, Jamie Kerr, Kamal Ndousse, Karina Nguyen, Nelson Elhage, Newton Cheng, Nicholas Schiefer, Nova DasSarma, Oliver Rausch, Robin Larson, Shannon Yang, Shauna Kravec, Timothy Telleen-Lawton, Thomas I. Liao, Tom Henighan, Tristan Hume, Zac Hatfield-Dodds, Sören Mindermann, Nicholas Joseph, Sam McCandlish, Jared Kaplan</li>
<li>for: 这篇论文是为了探讨人工智能模型中的伦理问题，以及如何使用AI模型来避免这些问题。</li>
<li>methods: 这篇论文使用了AI模型来替代人类反馈，并让AI模型仅仅根据一个列表的原则进行反馈。</li>
<li>results: 研究发现，使用简单的原则可以有效地防止AI模型表达有害行为，但是需要更多的原则来实现细化的控制。<details>
<summary>Abstract</summary>
Human feedback can prevent overtly harmful utterances in conversational models, but may not automatically mitigate subtle problematic behaviors such as a stated desire for self-preservation or power. Constitutional AI offers an alternative, replacing human feedback with feedback from AI models conditioned only on a list of written principles. We find this approach effectively prevents the expression of such behaviors. The success of simple principles motivates us to ask: can models learn general ethical behaviors from only a single written principle? To test this, we run experiments using a principle roughly stated as "do what's best for humanity". We find that the largest dialogue models can generalize from this short constitution, resulting in harmless assistants with no stated interest in specific motivations like power. A general principle may thus partially avoid the need for a long list of constitutions targeting potentially harmful behaviors. However, more detailed constitutions still improve fine-grained control over specific types of harms. This suggests both general and specific principles have value for steering AI safely.
</details>
<details>
<summary>摘要</summary>
人类反馈可以防止对话模型表达过分危险的言行，但可能无法自动消除微妙的问题行为，如表达自保或权力愿望。宪法AI提供了一种alternative， replacing human feedback with AI模型对一份已编写的原则进行反馈。我们发现这种方法能够有效防止表达这些行为。成功的简单原则使我们问：可以模型学习通用的伦理行为从单一的写好的原则中吗？为测试这一点，我们运行了一些实验，使用“为人类做好事”的简单宪法。我们发现大型对话模型可以从这个短宪法中泛化，得到无权力的助手。一个通用的原则可以因此部分避免制定长列表的宪法，targeting potentially harmful behaviors。然而，更详细的宪法仍然可以提供细化的控制 sobre specific types of harms。这表明 Both general and specific principles have value for steering AI safely。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Illicit-Activity-Detection-using-XAI-A-Multimodal-Graph-LLM-Framework"><a href="#Enhancing-Illicit-Activity-Detection-using-XAI-A-Multimodal-Graph-LLM-Framework" class="headerlink" title="Enhancing Illicit Activity Detection using XAI: A Multimodal Graph-LLM Framework"></a>Enhancing Illicit Activity Detection using XAI: A Multimodal Graph-LLM Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13787">http://arxiv.org/abs/2310.13787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jack Nicholls, Aditya Kuppa, Nhien-An Le-Khac</li>
<li>For: This paper is written for organisations and governments looking to improve their financial cybercrime detection and prevention methods.* Methods: The paper presents a state-of-the-art, novel multimodal approach to explainable AI (XAI) in financial cybercrime detection, leveraging deep learning models to distill essential representations from transaction sequencing, subgraph connectivity, and narrative generation.* Results: The paper’s proposed approach significantly streamlines the investigative process for analysts, allowing them to understand transactions and their metadata much further through contextual narrative generation.<details>
<summary>Abstract</summary>
Financial cybercrime prevention is an increasing issue with many organisations and governments. As deep learning models have progressed to identify illicit activity on various financial and social networks, the explainability behind the model decisions has been lacklustre with the investigative analyst at the heart of any deep learning platform. In our paper, we present a state-of-the-art, novel multimodal proactive approach to addressing XAI in financial cybercrime detection.   We leverage a triad of deep learning models designed to distill essential representations from transaction sequencing, subgraph connectivity, and narrative generation to significantly streamline the analyst's investigative process. Our narrative generation proposal leverages LLM to ingest transaction details and output contextual narrative for an analyst to understand a transaction and its metadata much further.
</details>
<details>
<summary>摘要</summary>
金融电脑犯罪预防已成为许多组织和政府的问题。随着深度学习模型在不同的金融和社交网络上识别违法活动的能力不断提高，模型决策的解释力 however, has been lackluster for investigative analysts at the heart of any deep learning platform. 在我们的论文中，我们提出了一种现代化、新型的多Modalactive approach来解决金融电脑犯罪检测中的XAI问题。我们利用了三种深度学习模型，用于筛选交易时间序列、子图连接和生成文本，以大大简化分析员的调查过程。我们的文本生成提案利用LLM来接受交易细节并输出 Contextual narrative，让分析员更深入理解交易和其元数据。
</details></li>
</ul>
<hr>
<h2 id="Fundamental-Limits-of-Membership-Inference-Attacks-on-Machine-Learning-Models"><a href="#Fundamental-Limits-of-Membership-Inference-Attacks-on-Machine-Learning-Models" class="headerlink" title="Fundamental Limits of Membership Inference Attacks on Machine Learning Models"></a>Fundamental Limits of Membership Inference Attacks on Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13786">http://arxiv.org/abs/2310.13786</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eric Aubinais, Elisabeth Gassiat, Pablo Piantanida</li>
<li>for: 本文探讨了机器学习模型上的会员推测攻击（MIA）的基本统计限制，以及这种攻击对个人隐私的暴露。</li>
<li>methods: 本文首先 derivates了攻击成功的统计量，然后investigates several situations，并提供了这种量的上下限。</li>
<li>results: 根据样本数量和学习模型的结构参数，可以直接从数据集中估算出攻击准确率。<details>
<summary>Abstract</summary>
Membership inference attacks (MIA) can reveal whether a particular data point was part of the training dataset, potentially exposing sensitive information about individuals. This article explores the fundamental statistical limitations associated with MIAs on machine learning models. More precisely, we first derive the statistical quantity that governs the effectiveness and success of such attacks. Then, we investigate several situations for which we provide bounds on this quantity of interest. This allows us to infer the accuracy of potential attacks as a function of the number of samples and other structural parameters of learning models, which in some cases can be directly estimated from the dataset.
</details>
<details>
<summary>摘要</summary>
会员推测攻击（MIA）可以揭示一个特定数据点是否在训练集中，可能暴露个人敏感信息。这篇文章探讨机器学习模型上的基本统计限制，以帮助理解会员推测攻击的效果和成功。我们首先计算攻击成功的统计量，然后研究这种量在不同情况下的下界和上界，从而可以根据样本数量和学习模型的结构参数来估算攻击的准确率。
</details></li>
</ul>
<hr>
<h2 id="Copyright-Violations-and-Large-Language-Models"><a href="#Copyright-Violations-and-Large-Language-Models" class="headerlink" title="Copyright Violations and Large Language Models"></a>Copyright Violations and Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13771">http://arxiv.org/abs/2310.13771</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Noykarde/NoykardeRepository">https://github.com/Noykarde/NoykardeRepository</a></li>
<li>paper_authors: Antonia Karamolegkou, Jiaang Li, Li Zhou, Anders Søgaard</li>
<li>for: 这篇论文探讨了大型自然语言处理模型可能违反版权法律的问题，具体来说是模型是否可以通过精准记忆来重新分布版权文本。</li>
<li>methods: 该论文使用了一系列语言模型对popular books和编程问题进行了实验，以保守地 Characterize the extent to which language models can redistribute these materials.</li>
<li>results: 研究发现，大型语言模型可以很好地记忆和重新分布版权文本，这可能会导致未经授权的版权违反。这些结果提醒我们需要进一步检查和研究，以确保未来的自然语言处理技术的发展遵循版权法律。<details>
<summary>Abstract</summary>
Language models may memorize more than just facts, including entire chunks of texts seen during training. Fair use exemptions to copyright laws typically allow for limited use of copyrighted material without permission from the copyright holder, but typically for extraction of information from copyrighted materials, rather than {\em verbatim} reproduction. This work explores the issue of copyright violations and large language models through the lens of verbatim memorization, focusing on possible redistribution of copyrighted text. We present experiments with a range of language models over a collection of popular books and coding problems, providing a conservative characterization of the extent to which language models can redistribute these materials. Overall, this research highlights the need for further examination and the potential impact on future developments in natural language processing to ensure adherence to copyright regulations. Code is at \url{https://github.com/coastalcph/CopyrightLLMs}.
</details>
<details>
<summary>摘要</summary>
大型自然语言处理模型可能会记忆更多于Just facts，包括训练期间看到的整个文本块。 fair use exemption to copyright laws 通常允许不经copyright持有人授权的限量使用copyrighted material，但通常是为了提取copyrighted materials中的信息，而不是 verbatim 复制。这项研究通过 memorization 的问题来探讨版权侵犯和大型语言模型，强调可能的版权文本重新分布。我们在一系列popular book和编程问题上进行了实验，提供了保守的计算机архитектура来Quantify the extent to which language models can redistribute these materials. 总之，这项研究强调了需要进一步检查和未来自然语言处理发展中的版权规定遵循。代码可以在 <https://github.com/coastalcph/CopyrightLLMs> 上找到。
</details></li>
</ul>
<hr>
<h2 id="Neural-Base-Music-Generation-for-Intelligence-Duplication"><a href="#Neural-Base-Music-Generation-for-Intelligence-Duplication" class="headerlink" title="Neural-Base Music Generation for Intelligence Duplication"></a>Neural-Base Music Generation for Intelligence Duplication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13691">http://arxiv.org/abs/2310.13691</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacob Galajda, Kien Hua</li>
<li>for: 这 paper 是为了研究智能复制（Intelligent Duplication）技术，以便创造新的信息。</li>
<li>methods: 这 paper 使用了深度学习系统，以学习大作曲家贝多芬的作曲能力，并将其转化为一个哈希基本知识库。</li>
<li>results: 这 paper 通过了一种新的音乐创作方法，可以通过贝多芬的作曲能力来驱动音乐创作。<details>
<summary>Abstract</summary>
There are two aspects of machine learning and artificial intelligence: (1) interpreting information, and (2) inventing new useful information. Much advance has been made for (1) with a focus on pattern recognition techniques (e.g., interpreting visual data). This paper focuses on (2) with intelligent duplication (ID) for invention. We explore the possibility of learning a specific individual's creative reasoning in order to leverage the learned expertise and talent to invent new information. More specifically, we employ a deep learning system to learn from the great composer Beethoven and capture his composition ability in a hash-based knowledge base. This new form of knowledge base provides a reasoning facility to drive the music composition through a novel music generation method.
</details>
<details>
<summary>摘要</summary>
Machine learning和人工智能有两个方面：（1）解读信息，和（2）创造新有用信息。在（1）方面，已经做出了很大的进步，主要是通过模式识别技术（如图像数据的解读）。这篇论文则专注于（2）方面，通过智能复制（ID）来创造新的信息。我们尝试了学习特定个人的创造性思维，以利用学习到的专业技巧和才华来创造新的信息。我们使用深度学习系统学习了大作曲家贝多芬的作曲能力，并将其储存在一个哈希基本知识库中。这种新的知识库提供了一种新的思维方式，用于驱动音乐创作。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-Retrieval-augmented-Reader-Models-via-Token-Elimination"><a href="#Optimizing-Retrieval-augmented-Reader-Models-via-Token-Elimination" class="headerlink" title="Optimizing Retrieval-augmented Reader Models via Token Elimination"></a>Optimizing Retrieval-augmented Reader Models via Token Elimination</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13682">http://arxiv.org/abs/2310.13682</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mosheber/token_elimination">https://github.com/mosheber/token_elimination</a></li>
<li>paper_authors: Moshe Berchansky, Peter Izsak, Avi Caciularu, Ido Dagan, Moshe Wasserblat</li>
<li>for: 提高语音模型的运行效率，特别是在生成大量输出时。</li>
<li>methods: 分析支持文本的贡献度，并在токен水平上剔除不必要的信息，以降低运行时间。</li>
<li>results: 可以减少运行时间，最多62.2%，同时保持表现稳定，甚至提高表现。<details>
<summary>Abstract</summary>
Fusion-in-Decoder (FiD) is an effective retrieval-augmented language model applied across a variety of open-domain tasks, such as question answering, fact checking, etc. In FiD, supporting passages are first retrieved and then processed using a generative model (Reader), which can cause a significant bottleneck in decoding time, particularly with long outputs. In this work, we analyze the contribution and necessity of all the retrieved passages to the performance of reader models, and propose eliminating some of the retrieved information, at the token level, that might not contribute essential information to the answer generation process. We demonstrate that our method can reduce run-time by up to 62.2%, with only a 2% reduction in performance, and in some cases, even improve the performance results.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Long-Form-Speech-Translation-through-Segmentation-with-Finite-State-Decoding-Constraints-on-Large-Language-Models"><a href="#Long-Form-Speech-Translation-through-Segmentation-with-Finite-State-Decoding-Constraints-on-Large-Language-Models" class="headerlink" title="Long-Form Speech Translation through Segmentation with Finite-State Decoding Constraints on Large Language Models"></a>Long-Form Speech Translation through Segmentation with Finite-State Decoding Constraints on Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13678">http://arxiv.org/abs/2310.13678</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arya D. McCarthy, Hao Zhang, Shankar Kumar, Felix Stahlberg, Ke Wu</li>
<li>for: 提高对长型语音内容的翻译质量，因为长型语音内容中的各个单元可以独立翻译以提高总体翻译质量。</li>
<li>methods: 利用大语言模型（LLMs）对长ASR讲cript进行分割，以便独立翻译每个单元，并在解码过程中 incorporating finite-state constraints 来消除投射的 Output。</li>
<li>results: 通过prompt-tuning或 fine-tuning，LLMs可以适应ASR错误的讲cript，并在9个测试集上提高了 average BLEU 值by 2.9点，相比于 automatic punctuation 基准。<details>
<summary>Abstract</summary>
One challenge in speech translation is that plenty of spoken content is long-form, but short units are necessary for obtaining high-quality translations. To address this mismatch, we adapt large language models (LLMs) to split long ASR transcripts into segments that can be independently translated so as to maximize the overall translation quality. We overcome the tendency of hallucination in LLMs by incorporating finite-state constraints during decoding; these eliminate invalid outputs without requiring additional training. We discover that LLMs are adaptable to transcripts containing ASR errors through prompt-tuning or fine-tuning. Relative to a state-of-the-art automatic punctuation baseline, our best LLM improves the average BLEU by 2.9 points for English-German, English-Spanish, and English-Arabic TED talk translation in 9 test sets, just by improving segmentation.
</details>
<details>
<summary>摘要</summary>
一个挑战在语音翻译中是许多 spoken 内容很长，但是需要短单位以获得高质量翻译。为Address this mismatch, we adapt large language models (LLMs) to split long ASR transcripts into segments that can be independently translated so as to maximize the overall translation quality. We overcome the tendency of hallucination in LLMs by incorporating finite-state constraints during decoding; these eliminate invalid outputs without requiring additional training. We discover that LLMs are adaptable to transcripts containing ASR errors through prompt-tuning or fine-tuning. Compared to a state-of-the-art automatic punctuation baseline, our best LLM improves the average BLEU by 2.9 points for English-German, English-Spanish, and English-Arabic TED talk translation in 9 test sets, just by improving segmentation.
</details></li>
</ul>
<hr>
<h2 id="Let’s-Synthesize-Step-by-Step-Iterative-Dataset-Synthesis-with-Large-Language-Models-by-Extrapolating-Errors-from-Small-Models"><a href="#Let’s-Synthesize-Step-by-Step-Iterative-Dataset-Synthesis-with-Large-Language-Models-by-Extrapolating-Errors-from-Small-Models" class="headerlink" title="Let’s Synthesize Step by Step: Iterative Dataset Synthesis with Large Language Models by Extrapolating Errors from Small Models"></a>Let’s Synthesize Step by Step: Iterative Dataset Synthesis with Large Language Models by Extrapolating Errors from Small Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13671">http://arxiv.org/abs/2310.13671</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rickyskywalker/synthesis_step-by-step_official">https://github.com/rickyskywalker/synthesis_step-by-step_official</a></li>
<li>paper_authors: Ruida Wang, Wangchunshu Zhou, Mrinmaya Sachan<br>for: 这篇论文的目的是提出一种数据合成方法，以帮助小型模型在具有很少标注数据的情况下进行训练。methods: 该方法利用大语言模型的丰富知识来生成 pseudo 训练示例，以实现数据和计算效率的同时提高。results: 该方法可以减少数据合成集和真实任务数据集之间的分布差距，从而提高小型模型的性能。对多个 NLP 任务进行了广泛的实验，显示了我们的方法可以比基elines提高小型模型的性能，最大提高比例达到 15.17%。<details>
<summary>Abstract</summary>
*Data Synthesis* is a promising way to train a small model with very little labeled data. One approach for data synthesis is to leverage the rich knowledge from large language models to synthesize pseudo training examples for small models, making it possible to achieve both data and compute efficiency at the same time. However, a key challenge in data synthesis is that the synthesized dataset often suffers from a large distributional discrepancy from the *real task* data distribution. Thus, in this paper, we propose *Synthesis Step by Step* (**S3**), a data synthesis framework that shrinks this distribution gap by iteratively extrapolating the errors made by a small model trained on the synthesized dataset on a small real-world validation dataset using a large language model. Extensive experiments on multiple NLP tasks show that our approach improves the performance of a small model by reducing the gap between the synthetic dataset and the real data, resulting in significant improvement compared to several baselines: 9.48% improvement compared to ZeroGen and 2.73% compared to GoldGen, and at most 15.17% improvement compared to the small model trained on human-annotated data.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate english text into simplified chinese*Data Synthesis* 是一种有前途的方法，用于训练一个小型模型，只需要非常小的标注数据。一种实现数据合成的方法是利用大型语言模型中的丰富知识，生成pseudo的训练示例，使得可以同时实现数据和计算效率。然而，数据合成中的一个关键挑战是，合成的数据集经常受到真实任务数据分布的大分布差异。因此，在这篇论文中，我们提出了Synthesis Step by Step（**S3**）数据合成框架，通过在小型真实世界验证集上使用大型语言模型来逐步修正小型模型在合成数据集上的错误。广泛的实验表明，我们的方法可以提高小型模型的性能，降低合成数据集和真实数据集之间的分布差异，相比ZeroGen和GoldGen基准下提高9.48%和2.73%，并且在最好情况下与人工标注数据上训练小型模型相比提高15.17%。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="ManifoldNeRF-View-dependent-Image-Feature-Supervision-for-Few-shot-Neural-Radiance-Fields"><a href="#ManifoldNeRF-View-dependent-Image-Feature-Supervision-for-Few-shot-Neural-Radiance-Fields" class="headerlink" title="ManifoldNeRF: View-dependent Image Feature Supervision for Few-shot Neural Radiance Fields"></a>ManifoldNeRF: View-dependent Image Feature Supervision for Few-shot Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13670">http://arxiv.org/abs/2310.13670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daiju Kanaoka, Motoharu Sonogashira, Hakaru Tamukoh, Yasutomo Kawanishi</li>
<li>for: 本研究旨在提高 Novel View Synthesis 的效果，使用 Neural Radiance Fields (NeRF) 的扩展 DietNeRF。</li>
<li>methods: 本研究提出了一种基于 interpolated features 的方法，来supervise unknown viewpoints 的特征向量。</li>
<li>results: 实验结果表明，提议的方法在复杂场景中表现更好于 DietNeRF，并且在实际环境中identified一组有效的视点 Patterns。<details>
<summary>Abstract</summary>
Novel view synthesis has recently made significant progress with the advent of Neural Radiance Fields (NeRF). DietNeRF is an extension of NeRF that aims to achieve this task from only a few images by introducing a new loss function for unknown viewpoints with no input images. The loss function assumes that a pre-trained feature extractor should output the same feature even if input images are captured at different viewpoints since the images contain the same object. However, while that assumption is ideal, in reality, it is known that as viewpoints continuously change, also feature vectors continuously change. Thus, the assumption can harm training. To avoid this harmful training, we propose ManifoldNeRF, a method for supervising feature vectors at unknown viewpoints using interpolated features from neighboring known viewpoints. Since the method provides appropriate supervision for each unknown viewpoint by the interpolated features, the volume representation is learned better than DietNeRF. Experimental results show that the proposed method performs better than others in a complex scene. We also experimented with several subsets of viewpoints from a set of viewpoints and identified an effective set of viewpoints for real environments. This provided a basic policy of viewpoint patterns for real-world application. The code is available at https://github.com/haganelego/ManifoldNeRF_BMVC2023
</details>
<details>
<summary>摘要</summary>
新型视图合成技术在近期内受到了神经辐射场（NeRF）的推出，其中 DietNeRF 是一种从只有几个图像中学习视图 synthesis 的扩展。DietNeRF 的目标是在不知道视点的情况下，从几个图像中学习视图 synthesis。但是，这个假设是理想的，实际上，随着视点的变化，图像中的特征向量也会随着变化。因此，这个假设可能会对训练造成害。为了避免这种害，我们提出了 ManifoldNeRF，一种使用邻近known viewpoint的 interpolated 特征来监督未知视点的特征vector的方法。由于该方法可以对每个未知视点提供适当的监督，因此可以更好地学习volume representation。实验结果表明，我们提出的方法在复杂场景中表现更好 than others。我们还对一些视点集进行了实验，并确定了在真实环境中有效的视点集。这提供了一个基本的视点模式政策，可以应用于实际世界中。代码可以在 <https://github.com/haganelego/ManifoldNeRF_BMVC2023> 上找到。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Unit-Test-Data-Generation-and-Actor-Critic-Reinforcement-Learning-for-Code-Synthesis"><a href="#Automatic-Unit-Test-Data-Generation-and-Actor-Critic-Reinforcement-Learning-for-Code-Synthesis" class="headerlink" title="Automatic Unit Test Data Generation and Actor-Critic Reinforcement Learning for Code Synthesis"></a>Automatic Unit Test Data Generation and Actor-Critic Reinforcement Learning for Code Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13669">http://arxiv.org/abs/2310.13669</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/huawei-noah/noah-research">https://github.com/huawei-noah/noah-research</a></li>
<li>paper_authors: Philip John Gorinski, Matthieu Zimmer, Gerasimos Lampouras, Derrick Goh Xin Deik, Ignacio Iacobacci</li>
<li>for: 这个论文的目的是提高代码生成模型的性能。</li>
<li>methods: 该论文使用了自动生成的函数签名和相关的单元测试数据，以及actor-critic reinforcement learning训练方法。</li>
<li>results: 该论文的实验结果显示，与原始代码生成LM模型相比，使用自动生成的数据和actor-critic RL训练方法可以提高代码生成模型的性能，最高提高9.9%。同时，与常见的PPO或CodeRL模型相比，该论文的方法可以提高代码生成模型的性能，最高提高4.3%。<details>
<summary>Abstract</summary>
The advent of large pre-trained language models in the domain of Code Synthesis has shown remarkable performance on various benchmarks, treating the problem of Code Generation in a fashion similar to Natural Language Generation, trained with a Language Modelling (LM) objective. In addition, the property of programming language code being precisely evaluable with respect to its semantics -- through the use of Unit Tests to check its functional correctness -- lends itself to using Reinforcement Learning (RL) as a further training paradigm. Previous work has shown that RL can be applied as such to improve models' coding capabilities; however, such RL-based methods rely on a reward signal based on defined Unit Tests, which are much harder to obtain compared to the huge crawled code datasets used in LM objectives. In this work, we present a novel approach to automatically obtain data consisting of function signatures and associated Unit Tests, suitable for RL training of Code Synthesis models. We also introduce a straightforward, simple yet effective Actor-Critic RL training scheme and show that it, in conjunction with automatically generated training data, leads to improvement of a pre-trained code language model's performance by up to 9.9% improvement over the original underlying code synthesis LM, and up to 4.3% over RL-based models trained with standard PPO or CodeRL.
</details>
<details>
<summary>摘要</summary>
大量预训练语言模型在代码生成领域的出现，在不同的benchmark上表现出色，将代码生成问题与自然语言生成问题相似的方式进行训练，使用语言模型（LM）目标。此外，编程语言代码的准确评估性——通过使用单元测试来检查其功能正确性——使得使用强化学习（RL）作为训练方法的可能性。前一个研究表明RL可以用来提高模型的编程能力，但是这些RL基于的方法需要基于定义的单元测试奖励信号，这些奖励信号比大量爬取代码集更难获得。在这项工作中，我们提出了一种新的方法，可以自动获得包含函数签名和相关单元测试的数据，适用于RL训练代码生成模型。我们还提出了一种简单、直观 yet effective的actor-critic RL训练方案，并证明在这种方案下，可以使用自动生成的训练数据，提高一个预训练的代码语言模型的性能，相比原始代码生成LM的9.9%提升，并比使用标准PPO或CodeRL的RL-based模型的4.3%提升。
</details></li>
</ul>
<hr>
<h2 id="An-experimental-study-for-early-diagnosing-Parkinson’s-disease-using-machine-learning"><a href="#An-experimental-study-for-early-diagnosing-Parkinson’s-disease-using-machine-learning" class="headerlink" title="An experimental study for early diagnosing Parkinson’s disease using machine learning"></a>An experimental study for early diagnosing Parkinson’s disease using machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13654">http://arxiv.org/abs/2310.13654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md. Taufiqul Haque Khan Tusar, Md. Touhidul Islam, Abul Hasnat Sakil</li>
<li>For: Early detection of Parkinson’s Disease* Methods: Machine Learning techniques, including MinMax Scaler, Local Outlier Factor, and SMOTE, to automate the early detection of Parkinson’s Disease from clinical characteristics, voice features, and motor examination.* Results: Obtained 100% accuracy in classifying PD and RBD patients, as well as 92% accuracy in classifying PD and HC individuals.<details>
<summary>Abstract</summary>
One of the most catastrophic neurological disorders worldwide is Parkinson's Disease. Along with it, the treatment is complicated and abundantly expensive. The only effective action to control the progression is diagnosing it in the early stage. However, this is challenging because early detection necessitates a large and complex clinical study. This experimental work used Machine Learning techniques to automate the early detection of Parkinson's Disease from clinical characteristics, voice features and motor examination. In this study, we develop ML models utilizing a public dataset of 130 individuals, 30 of whom are untreated Parkinson's Disease patients, 50 of whom are Rapid Eye Movement Sleep Behaviour Disorder patients who are at a greater risk of contracting Parkinson's Disease, and 50 of whom are Healthy Controls. We use MinMax Scaler to rescale the data points, Local Outlier Factor to remove outliers, and SMOTE to balance existing class frequency. Afterwards, apply a number of Machine Learning techniques. We implement the approaches in such a way that data leaking and overfitting are not possible. Finally, obtained 100% accuracy in classifying PD and RBD patients, as well as 92% accuracy in classifying PD and HC individuals.
</details>
<details>
<summary>摘要</summary>
In this study, we use machine learning techniques to automate the early detection of Parkinson's disease from clinical characteristics, voice features, and motor examination. We use a public dataset of 130 individuals, including 30 patients with untreated Parkinson's disease, 50 patients with rapid eye movement sleep behavior disorder who are at a higher risk of contracting Parkinson's disease, and 50 healthy controls.First, we use Min-Max Scaler to rescale the data points, Local Outlier Factor to remove outliers, and SMOTE to balance the existing class frequency. We implement the approaches in such a way that data leaking and overfitting are not possible.After applying these techniques, we obtained 100% accuracy in classifying Parkinson's disease and rapid eye movement sleep behavior disorder patients, as well as 92% accuracy in classifying Parkinson's disease and healthy control individuals.Note: "Parkinson's disease" is translated as " Parkinson 病" in Simplified Chinese, "rapid eye movement sleep behavior disorder" is translated as "快眼睛运动失调" in Simplified Chinese, and "healthy controls" is translated as "健康控制群" in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Weighted-Joint-Maximum-Mean-Discrepancy-Enabled-Multi-Source-Multi-Target-Unsupervised-Domain-Adaptation-Fault-Diagnosis"><a href="#Weighted-Joint-Maximum-Mean-Discrepancy-Enabled-Multi-Source-Multi-Target-Unsupervised-Domain-Adaptation-Fault-Diagnosis" class="headerlink" title="Weighted Joint Maximum Mean Discrepancy Enabled Multi-Source-Multi-Target Unsupervised Domain Adaptation Fault Diagnosis"></a>Weighted Joint Maximum Mean Discrepancy Enabled Multi-Source-Multi-Target Unsupervised Domain Adaptation Fault Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14790">http://arxiv.org/abs/2310.14790</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixuan Wang, Haoran Tang, Haibo Wang, Bo Qin, Mark D. Butala, Weiming Shen, Hongwei Wang</li>
<li>for: 本研究旨在提出一种多源多目标无监督领域适应（WJMMD-MDA）方法，用于在多源多目标场景下实现适应预测。</li>
<li>methods: 该方法提取了多个标注源频谱中的足够信息，并通过改进的距离损失实现多源多目标频谱的域对齐。这使得在多源多目标场景下学习域外特征，并实现跨域缺陷诊断。</li>
<li>results: 对三个数据集进行了广泛的比较试验，实验结果表明，提出的方法在跨域缺陷诊断中具有显著优势。<details>
<summary>Abstract</summary>
Despite the remarkable results that can be achieved by data-driven intelligent fault diagnosis techniques, they presuppose the same distribution of training and test data as well as sufficient labeled data. Various operating states often exist in practical scenarios, leading to the problem of domain shift that hinders the effectiveness of fault diagnosis. While recent unsupervised domain adaptation methods enable cross-domain fault diagnosis, they struggle to effectively utilize information from multiple source domains and achieve effective diagnosis faults in multiple target domains simultaneously. In this paper, we innovatively proposed a weighted joint maximum mean discrepancy enabled multi-source-multi-target unsupervised domain adaptation (WJMMD-MDA), which realizes domain adaptation under multi-source-multi-target scenarios in the field of fault diagnosis for the first time. The proposed method extracts sufficient information from multiple labeled source domains and achieves domain alignment between source and target domains through an improved weighted distance loss. As a result, domain-invariant and discriminative features between multiple source and target domains are learned with cross-domain fault diagnosis realized. The performance of the proposed method is evaluated in comprehensive comparative experiments on three datasets, and the experimental results demonstrate the superiority of this method.
</details>
<details>
<summary>摘要</summary>
尽管数据驱动智能故障诊断技术可以实现很出色的结果，但它们假设训练和测试数据的分布相同，以及充足的标注数据。在实际场景中，各种运行状态经常存在，导致域 shift 问题，这阻碍了故障诊断的效iveness。而最近的无监督领域适应方法可以在不同域的故障诊断中进行交叉领域适应，但它们很难 simultaneously 利用多个来源域的信息，并实现多个目标域的有效诊断。在这篇论文中，我们创新地提出了一种基于加权最大差异 enabled 多源多目标无监督领域适应方法（WJMMD-MDA），该方法在多源多目标场景中实现了领域适应。该方法从多个标注源域中提取了足够的信息，并通过改进的加权距离损失来实现源和目标域之间的域对应。因此，在多个源和目标域之间学习了域不variant 和抑制特征，并实现了交叉域的故障诊断。我们在三个数据集上进行了广泛的比较实验，结果表明该方法的性能优于其他方法。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Preference-Learning-Learning-from-Human-Feedback-without-RL"><a href="#Contrastive-Preference-Learning-Learning-from-Human-Feedback-without-RL" class="headerlink" title="Contrastive Preference Learning: Learning from Human Feedback without RL"></a>Contrastive Preference Learning: Learning from Human Feedback without RL</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13639">http://arxiv.org/abs/2310.13639</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jhejna/cpl">https://github.com/jhejna/cpl</a></li>
<li>paper_authors: Joey Hejna, Rafael Rafailov, Harshit Sikchi, Chelsea Finn, Scott Niekum, W. Bradley Knox, Dorsa Sadigh</li>
<li>for: 这 paper 是关于人工智能学习从人类反馈（RLHF）中的一种新方法，用于让模型与人类意愿相对应。</li>
<li>methods: 这 paper 使用了一种新的 regret-based 模型来学习人类偏好，并使用了一种新的 contrastive 目标函数来学习优化行为。</li>
<li>results: 这 paper 的结果表明，这种新方法可以在高维和连续的RLHF问题中Scale to elegantly，并且比之前的方法更简单。<details>
<summary>Abstract</summary>
Reinforcement Learning from Human Feedback (RLHF) has emerged as a popular paradigm for aligning models with human intent. Typically RLHF algorithms operate in two phases: first, use human preferences to learn a reward function and second, align the model by optimizing the learned reward via reinforcement learning (RL). This paradigm assumes that human preferences are distributed according to reward, but recent work suggests that they instead follow the regret under the user's optimal policy. Thus, learning a reward function from feedback is not only based on a flawed assumption of human preference, but also leads to unwieldy optimization challenges that stem from policy gradients or bootstrapping in the RL phase. Because of these optimization challenges, contemporary RLHF methods restrict themselves to contextual bandit settings (e.g., as in large language models) or limit observation dimensionality (e.g., state-based robotics). We overcome these limitations by introducing a new family of algorithms for optimizing behavior from human feedback using the regret-based model of human preferences. Using the principle of maximum entropy, we derive Contrastive Preference Learning (CPL), an algorithm for learning optimal policies from preferences without learning reward functions, circumventing the need for RL. CPL is fully off-policy, uses only a simple contrastive objective, and can be applied to arbitrary MDPs. This enables CPL to elegantly scale to high-dimensional and sequential RLHF problems while being simpler than prior methods.
</details>
<details>
<summary>摘要</summary>
人工智能学习奖励（RLHF）已成为一种流行的方法，用于将模型与人类意愿相align。通常RLHF算法在两个阶段操作：首先，使用人类偏好来学习奖励函数，然后使用奖励学习（RL）来调整模型。这种方法假设人类偏好是根据奖励分布的，但最近的研究表明，人类偏好实际上是基于用户的优化策略的 regret。因此，从反馈中学习奖励函数并不只是基于错误的人类偏好假设，还会导致困难的优化挑战，例如Policy Gradient或Bootstrapping在RL阶段。由于这些优化挑战，当代RLHF方法通常限制自己在contextual bandit Setting（如大语言模型）或限制观察维度（如状态基于机器人）。我们超越这些限制，通过引入一种新的人类反馈学习算法，使用 regret-based模型来学习行为。使用最大 entropy原理，我们 derive Contrastive Preference Learning（CPL）算法，可以从偏好中学习优化策略，不需要学习奖励函数，从而避免RL阶段的优化挑战。CPL是完全偏离策略的，只需使用简单的对比目标，可以应用于任意MDP。这使得CPL可以简单地扩展到高维和顺序RLHF问题，而且更简单于先前的方法。
</details></li>
</ul>
<hr>
<h2 id="Hunayn-Elevating-Translation-Beyond-the-Literal"><a href="#Hunayn-Elevating-Translation-Beyond-the-Literal" class="headerlink" title="Hunayn: Elevating Translation Beyond the Literal"></a>Hunayn: Elevating Translation Beyond the Literal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13613">http://arxiv.org/abs/2310.13613</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nasser Almousa, Nasser Alzamil, Abdullah Alshehri, Ahmad Sait</li>
<li>for: 这项研究旨在开发一个高级的英语到阿拉伯语翻译工具，超越传统工具。</li>
<li>methods: 该方法利用赫尔辛基 transformer（MarianMT），通过自动抽取的纯文学阿拉伯语数据进行微调。</li>
<li>results: 对于Google翻译的评估表明，该方法在质量评估中具有明显的优势，特别是在文化敏感度和上下文准确性方面。<details>
<summary>Abstract</summary>
This project introduces an advanced English-to-Arabic translator surpassing conventional tools. Leveraging the Helsinki transformer (MarianMT), our approach involves fine-tuning on a self-scraped, purely literary Arabic dataset. Evaluations against Google Translate show consistent outperformance in qualitative assessments. Notably, it excels in cultural sensitivity and context accuracy. This research underscores the Helsinki transformer's superiority for English-to-Arabic translation using a Fusha dataset.
</details>
<details>
<summary>摘要</summary>
这个项目推出了一种高级英语到阿拉伯语翻译工具，超越传统工具。我们采用了赫尔辛基transformer（MarianMT），我们的方法是在自动抽取的纯文学阿拉伯语数据上细调。对于Google翻译进行评估，我们的方法表现出了一致的提升，尤其是在文化敏感度和语言上下文准确性方面。这些研究证明了赫尔辛基transformer在英语到阿拉伯语翻译中的优势，使用福沙数据集。
</details></li>
</ul>
<hr>
<h2 id="Make-Your-Decision-Convincing-A-Unified-Two-Stage-Framework-Self-Attribution-and-Decision-Making"><a href="#Make-Your-Decision-Convincing-A-Unified-Two-Stage-Framework-Self-Attribution-and-Decision-Making" class="headerlink" title="Make Your Decision Convincing! A Unified Two-Stage Framework: Self-Attribution and Decision-Making"></a>Make Your Decision Convincing! A Unified Two-Stage Framework: Self-Attribution and Decision-Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13610">http://arxiv.org/abs/2310.13610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanrui Du, Sendong Zhao, Haochun Wang, Yuhan Chen, Rui Bai, Zewen Qiang, Muzhen Cai, Bing Qin</li>
<li>for: 本研究旨在提高黑盒模型的决策过程中的自然语言描述能力，以及提供用户可靠的决策理由。</li>
<li>methods: 本研究使用了子序列从输入文本中提取的自然语言来为用户提供决策理由，并通过两个阶段框架 Self-Attribution and Decision-Making (SADM) 来确保决策理由和模型决策之间的关系更加可靠。</li>
<li>results: 经过对 ERASER 测试 benchmark 上的五个理解任务的广泛实验，我们表明了我们的框架不仅可以提高决策理由和模型决策之间的关系的可靠性，还可以在任务性能和决策理由质量两个方面达到竞争力。此外，我们还探讨了我们的框架在半supervised情况下的潜在应用。<details>
<summary>Abstract</summary>
Explaining black-box model behavior with natural language has achieved impressive results in various NLP tasks. Recent research has explored the utilization of subsequences from the input text as a rationale, providing users with evidence to support the model decision. Although existing frameworks excel in generating high-quality rationales while achieving high task performance, they neglect to account for the unreliable link between the generated rationale and model decision. In simpler terms, a model may make correct decisions while attributing wrong rationales, or make poor decisions while attributing correct rationales. To mitigate this issue, we propose a unified two-stage framework known as Self-Attribution and Decision-Making (SADM). Through extensive experiments on five reasoning datasets from the ERASER benchmark, we demonstrate that our framework not only establishes a more reliable link between the generated rationale and model decision but also achieves competitive results in task performance and the quality of rationale. Furthermore, we explore the potential of our framework in semi-supervised scenarios.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将黑盒模型行为用自然语言描述达到了各种NLP任务中的出色结果。现有研究利用输入文本中的子序列作为论证，为用户提供模型决策的证据。 although existing frameworks can generate high-quality rationales and achieve high task performance, they neglect to account for the unreliable link between the generated rationale and model decision. In simpler terms, a model may make correct decisions while attributing wrong rationales, or make poor decisions while attributing correct rationales. To address this issue, we propose a unified two-stage framework known as Self-Attribution and Decision-Making (SADM). Through extensive experiments on five reasoning datasets from the ERASER benchmark, we demonstrate that our framework not only establishes a more reliable link between the generated rationale and model decision but also achieves competitive results in task performance and the quality of rationale. Furthermore, we explore the potential of our framework in semi-supervised scenarios.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="MULTITuDE-Large-Scale-Multilingual-Machine-Generated-Text-Detection-Benchmark"><a href="#MULTITuDE-Large-Scale-Multilingual-Machine-Generated-Text-Detection-Benchmark" class="headerlink" title="MULTITuDE: Large-Scale Multilingual Machine-Generated Text Detection Benchmark"></a>MULTITuDE: Large-Scale Multilingual Machine-Generated Text Detection Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13606">http://arxiv.org/abs/2310.13606</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kinit-sk/mgt-detection-benchmark">https://github.com/kinit-sk/mgt-detection-benchmark</a></li>
<li>paper_authors: Dominik Macko, Robert Moro, Adaku Uchendu, Jason Samuel Lucas, Michiharu Yamashita, Matúš Pikuliak, Ivan Srba, Thai Le, Dongwon Lee, Jakub Simko, Maria Bielikova</li>
<li>for: 填充 Multilingual Machine-Generated Text Detection Benchmark Dataset的 lacuna，提供8种多语言LLM生成的74,081个authentic和机器生成文本，用于评估多语言机器生成文本检测器的性能。</li>
<li>methods: 使用这个benchmark dataset， Comparing zero-shot (统计学和黑盒子)和精心调整的检测器的性能，并评估这些检测器在不同语言和LLM之间的一致性。</li>
<li>results: 研究发现， zero-shot检测器在语言不同和LLM不同情况下的性能较差，而精心调整的检测器在多语言和多LLM情况下的性能显著提高。<details>
<summary>Abstract</summary>
There is a lack of research into capabilities of recent LLMs to generate convincing text in languages other than English and into performance of detectors of machine-generated text in multilingual settings. This is also reflected in the available benchmarks which lack authentic texts in languages other than English and predominantly cover older generators. To fill this gap, we introduce MULTITuDE, a novel benchmarking dataset for multilingual machine-generated text detection comprising of 74,081 authentic and machine-generated texts in 11 languages (ar, ca, cs, de, en, es, nl, pt, ru, uk, and zh) generated by 8 multilingual LLMs. Using this benchmark, we compare the performance of zero-shot (statistical and black-box) and fine-tuned detectors. Considering the multilinguality, we evaluate 1) how these detectors generalize to unseen languages (linguistically similar as well as dissimilar) and unseen LLMs and 2) whether the detectors improve their performance when trained on multiple languages.
</details>
<details>
<summary>摘要</summary>
“现有研究缺乏关于最新的LLM的生成文本的真实性的语言 besides English 以及机器生成文本检测器在多语言设置下的性能研究。这也反映在可用的标准准则中，lack of authentic texts in languages other than English and primarily cover older generators。为了填补这一漏洞，我们引入 MULTITuDE，一个新的多语言机器生成文本检测 benchmark，包含74,081 个真实的机器生成文本和人工生成文本在 11 种语言（ar, ca, cs, de, en, es, nl, pt, ru, uk, and zh），由 8 种多语言 LLMS 生成。通过这个benchmark，我们比较了零 shot (统计学和黑盒子) 和精度调教的检测器的性能。考虑到多语言性，我们评估了1）这些检测器在未看过语言（语言相似性和不相似性）和LLMs 中的泛化性能，2）是否在多语言培训中提高检测器的性能。”
</details></li>
</ul>
<hr>
<h2 id="Skin-Lesion-Segmentation-Improved-by-Transformer-based-Networks-with-Inter-scale-Dependency-Modeling"><a href="#Skin-Lesion-Segmentation-Improved-by-Transformer-based-Networks-with-Inter-scale-Dependency-Modeling" class="headerlink" title="Skin Lesion Segmentation Improved by Transformer-based Networks with Inter-scale Dependency Modeling"></a>Skin Lesion Segmentation Improved by Transformer-based Networks with Inter-scale Dependency Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13604">http://arxiv.org/abs/2310.13604</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/saniaesk/skin-lesion-segmentation">https://github.com/saniaesk/skin-lesion-segmentation</a></li>
<li>paper_authors: Sania Eskandari, Janet Lumpp, Luis Sanchez Giraldo<br>for:这项研究旨在提高皮肤癌病变分割的自动化精度，使用Transformer网络模型来增强FCNs的长距离依赖关系捕捉能力。methods:该研究使用了一种基于U-Net架构的Transformer网络模型，并提出了一种增强skip连接路径的方法，以增加网络特征重用性。此外，研究还提出了一种Inter-scale Context Fusion（ISCF）方法，通过在每个阶段的Encoder中使用注意力相关性来适应性地融合不同阶段的上下文。results:研究在两个皮肤癌病变分割 benchmark 上达到了比较高的分割精度，并且对比Transformer网络模型和U-Net架构的性能进行了比较，结果表明，基于U-Net架构的Transformer网络模型可以增强皮肤癌病变分割的自动化精度。代码可以在 GitHub 上找到：<a target="_blank" rel="noopener" href="https://github.com/saniaesk/skin-lesion-segmentation%E3%80%82">https://github.com/saniaesk/skin-lesion-segmentation。</a><details>
<summary>Abstract</summary>
Melanoma, a dangerous type of skin cancer resulting from abnormal skin cell growth, can be treated if detected early. Various approaches using Fully Convolutional Networks (FCNs) have been proposed, with the U-Net architecture being prominent To aid in its diagnosis through automatic skin lesion segmentation. However, the symmetrical U-Net model's reliance on convolutional operations hinders its ability to capture long-range dependencies crucial for accurate medical image segmentation. Several Transformer-based U-Net topologies have recently been created to overcome this limitation by replacing CNN blocks with different Transformer modules to capture local and global representations. Furthermore, the U-shaped structure is hampered by semantic gaps between the encoder and decoder. This study intends to increase the network's feature re-usability by carefully building the skip connection path. Integrating an already calculated attention affinity within the skip connection path improves the typical concatenation process utilized in the conventional skip connection path. As a result, we propose a U-shaped hierarchical Transformer-based structure for skin lesion segmentation and an Inter-scale Context Fusion (ISCF) method that uses attention correlations in each stage of the encoder to adaptively combine the contexts from each stage to mitigate semantic gaps. The findings from two skin lesion segmentation benchmarks support the ISCF module's applicability and effectiveness. The code is publicly available at \url{https://github.com/saniaesk/skin-lesion-segmentation}
</details>
<details>
<summary>摘要</summary>
melanoma，一种危险的皮肤癌症，可以通过早期检测治疗。多种使用 Fully Convolutional Networks (FCNs) 的方法已经被提议，其中 U-Net 建筑物被广泛使用，以帮助自动识别皮肤肿瘤。然而，传统的 U-Net 模型在 convolutional 操作上存在一定的限制，这限制了它的捕捉长距离依赖关系的能力，这些依赖关系是医疗图像分割中必要的。为了缓解这些限制，一些基于 Transformer 的 U-Net 结构已经被创建，这些结构将 CNN 块 replaced 为不同的 Transformer 模块，以捕捉本地和全局表示。此外，U 形结构受到Semantic gap 的限制，这个问题可以通过精心建立 skip connection path 来解决。在传统的 skip connection path 中使用已经计算的 attention affinity 可以提高 feature 的重用性。因此，我们提出了一种 U 形层次 Transformer 基本结构和一种 Inter-scale Context Fusion (ISCF) 方法，该方法在每个encoder阶段使用 attention 相关性来适应地将每个阶段的上下文进行adaptive 组合，以 Mitigate Semantic gap。从两个皮肤肿瘤分割 benchmark 的结果来看，ISCF 模块的可行性和效果。代码可以在 \url{https://github.com/saniaesk/skin-lesion-segmentation} 上获取。
</details></li>
</ul>
<hr>
<h2 id="MarineGPT-Unlocking-Secrets-of-Ocean-to-the-Public"><a href="#MarineGPT-Unlocking-Secrets-of-Ocean-to-the-Public" class="headerlink" title="MarineGPT: Unlocking Secrets of Ocean to the Public"></a>MarineGPT: Unlocking Secrets of Ocean to the Public</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13596">http://arxiv.org/abs/2310.13596</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hkust-vgd/marinegpt">https://github.com/hkust-vgd/marinegpt</a></li>
<li>paper_authors: Ziqiang Zheng, Jipeng Zhang, Tuan-Anh Vu, Shizhe Diao, Yue Him Wong Tim, Sai-Kit Yeung</li>
<li>for: The paper is written to explore the use of large language models (LLMs) and multi-modal large language models (MLLMs) in the domain-specific application of the marine domain.</li>
<li>methods: The paper proposes a new vision-language model called MarineGPT, which is specifically designed for the marine domain and trained on a large dataset of marine image-text pairs called Marine-5M.</li>
<li>results: The paper shows that MarineGPT outperforms existing MLLMs in understanding domain-specific intents and generating informative and scientific responses in the marine domain. It also provides a standard protocol for adapting general-purpose assistants to downstream domain-specific experts.<details>
<summary>Abstract</summary>
Large language models (LLMs), such as ChatGPT/GPT-4, have proven to be powerful tools in promoting the user experience as an AI assistant. The continuous works are proposing multi-modal large language models (MLLM), empowering LLMs with the ability to sense multiple modality inputs through constructing a joint semantic space (e.g. visual-text space). Though significant success was achieved in LLMs and MLLMs, exploring LLMs and MLLMs in domain-specific applications that required domain-specific knowledge and expertise has been less conducted, especially for \textbf{marine domain}. Different from general-purpose MLLMs, the marine-specific MLLM is required to yield much more \textbf{sensitive}, \textbf{informative}, and \textbf{scientific} responses. In this work, we demonstrate that the existing MLLMs optimized on huge amounts of readily available general-purpose training data show a minimal ability to understand domain-specific intents and then generate informative and satisfactory responses. To address these issues, we propose \textbf{MarineGPT}, the first vision-language model specially designed for the marine domain, unlocking the secrets of the ocean to the public. We present our \textbf{Marine-5M} dataset with more than 5 million marine image-text pairs to inject domain-specific marine knowledge into our model and achieve better marine vision and language alignment. Our MarineGPT not only pushes the boundaries of marine understanding to the general public but also offers a standard protocol for adapting a general-purpose assistant to downstream domain-specific experts. We pave the way for a wide range of marine applications while setting valuable data and pre-trained models for future research in both academic and industrial communities.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs），如ChatGPT/GPT-4，已经证明是强大的工具来提升用户体验，作为人工智能助手。不断的研究提出了多modal大型语言模型（MLLM），将LMLMs扩展到多种数据类型的听取，例如文本和视觉数据。虽然LMLMs和MLLMs在不同领域中取得了卓越成就，但是对特定领域的应用仍然较少，尤其是在海洋领域。不同于通用MLLMs，海洋特定MLLM需要更加敏感、有用和科学的回应。在这个工作中，我们表明了现有的MLLMs在大量可用的通用训练数据上并不能够理解领域专门意图，并生成有用和满意的回应。为解决这些问题，我们提出了海洋GPT，首个特别设计 для海洋领域的视觉语言模型，为海洋秘密开启给大众。我们提供了我们的海洋-5M数据集，包含更多 than 500万几何和文本对应项目，将领域专门知识注入到我们的模型中，以 достиieving更好的视觉和语言对齐。我们的海洋GPT不仅扩展了海洋理解的boundaries，并且提供了一个标准协议供后续领域专门人员适应。我们开启了海洋应用的广泛前景，同时设定了价值的数据和预训模型供未来学术和工业社群的研究。
</details></li>
</ul>
<hr>
<h2 id="Towards-equilibrium-molecular-conformation-generation-with-GFlowNets"><a href="#Towards-equilibrium-molecular-conformation-generation-with-GFlowNets" class="headerlink" title="Towards equilibrium molecular conformation generation with GFlowNets"></a>Towards equilibrium molecular conformation generation with GFlowNets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14782">http://arxiv.org/abs/2310.14782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexandra Volokhova, Michał Koziarski, Alex Hernández-García, Cheng-Hao Liu, Santiago Miret, Pablo Lemos, Luca Thiede, Zichao Yan, Alán Aspuru-Guzik, Yoshua Bengio</li>
<li>for: 用于预测分子性质的预测方法</li>
<li>methods: 使用GFlowNet方法对小分子的可能性空间进行采样，根据分子的能量来确定采样分布</li>
<li>results: 可以与不同精度的能量估计方法结合使用，找到高度可变的药物分子低能态 conformations 的多样化集合，并能够复制分子潜在能量表面。<details>
<summary>Abstract</summary>
Sampling diverse, thermodynamically feasible molecular conformations plays a crucial role in predicting properties of a molecule. In this paper we propose to use GFlowNet for sampling conformations of small molecules from the Boltzmann distribution, as determined by the molecule's energy. The proposed approach can be used in combination with energy estimation methods of different fidelity and discovers a diverse set of low-energy conformations for highly flexible drug-like molecules. We demonstrate that GFlowNet can reproduce molecular potential energy surfaces by sampling proportionally to the Boltzmann distribution.
</details>
<details>
<summary>摘要</summary>
采样多样、 termodynamic 可行的分子姿态对分子性质预测具有重要作用。本文提议使用 GFlowNet 采样小分子的姿态，由分子能量确定的博尔ツ曼分布中的一部分。该方法可与不同级别的能量估计方法结合使用，并发现高灵活性药物分子的多个低能量姿态。我们示出 GFlowNet 可以按照博尔ツ曼分布中的比例采样分子潜在能能面。
</details></li>
</ul>
<hr>
<h2 id="ReLM-Leveraging-Language-Models-for-Enhanced-Chemical-Reaction-Prediction"><a href="#ReLM-Leveraging-Language-Models-for-Enhanced-Chemical-Reaction-Prediction" class="headerlink" title="ReLM: Leveraging Language Models for Enhanced Chemical Reaction Prediction"></a>ReLM: Leveraging Language Models for Enhanced Chemical Reaction Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13590">http://arxiv.org/abs/2310.13590</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/syr-cn/relm">https://github.com/syr-cn/relm</a></li>
<li>paper_authors: Yaorui Shi, An Zhang, Enzhi Zhang, Zhiyuan Liu, Xiang Wang</li>
<li>for: 预测化学反应，一个基本的化学挑战，涉及预测反应过程中的产物。现有的技术常受训练数据的限制和不能利用文本信息的办法约束其应用在实际应用中。本文提出了一种名为ReLM的新方案，利用化学知识编码在语言模型（LM）中，以助金Graph Neural Networks（GNNs），从而提高实际化学反应预测的准确率。</li>
<li>methods: 我们提出了一种名为ReLM的新方案，利用化学知识编码在语言模型（LM）中，以助金Graph Neural Networks（GNNs），从而提高实际化学反应预测的准确率。</li>
<li>results: 我们的实验结果表明，ReLM可以在不同的化学反应数据集上提高现有GNN-based方法的性能，特别是在异常情况下。codes可以在<a target="_blank" rel="noopener" href="https://github.com/syr-cn/ReLM%E4%B8%AD%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/syr-cn/ReLM中获得。</a><details>
<summary>Abstract</summary>
Predicting chemical reactions, a fundamental challenge in chemistry, involves forecasting the resulting products from a given reaction process. Conventional techniques, notably those employing Graph Neural Networks (GNNs), are often limited by insufficient training data and their inability to utilize textual information, undermining their applicability in real-world applications. In this work, we propose ReLM, a novel framework that leverages the chemical knowledge encoded in language models (LMs) to assist GNNs, thereby enhancing the accuracy of real-world chemical reaction predictions. To further enhance the model's robustness and interpretability, we incorporate the confidence score strategy, enabling the LMs to self-assess the reliability of their predictions. Our experimental results demonstrate that ReLM improves the performance of state-of-the-art GNN-based methods across various chemical reaction datasets, especially in out-of-distribution settings. Codes are available at https://github.com/syr-cn/ReLM.
</details>
<details>
<summary>摘要</summary>
预测化学反应是化学领域的基本挑战之一，即预测反应过程中的产物。现有的技术，如基于图神经网络（GNN）的方法，常受到数据不充分的训练和文本信息的不能利用的限制，从而削弱其在实际应用中的可行性。在这项工作中，我们提出了一种新的框架，即ReLM，它利用化学知识编码在语言模型（LM）中来帮助GNN，从而提高实际化学反应预测的准确性。为进一步增强模型的可靠性和可读性，我们还在模型中 интеGRATE了信任分数策略，使LM可以自我评估其预测的可靠性。我们的实验结果表明，ReLM可以在不同的化学反应数据集上超越现有的GNN-based方法的性能，特别是在出版数据集上。代码可以在https://github.com/syr-cn/ReLM上下载。
</details></li>
</ul>
<hr>
<h2 id="SPARE-A-Single-Pass-Neural-Model-for-Relational-Databases"><a href="#SPARE-A-Single-Pass-Neural-Model-for-Relational-Databases" class="headerlink" title="SPARE: A Single-Pass Neural Model for Relational Databases"></a>SPARE: A Single-Pass Neural Model for Relational Databases</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13581">http://arxiv.org/abs/2310.13581</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin Hilprecht, Kristian Kersting, Carsten Binnig</li>
<li>for: 这篇论文旨在提出一种高效地在关系数据库（RDB）上训练深度学习模型的方法，以提高predictive performance和减少训练时间。</li>
<li>methods: 该方法基于单过Relational models（SPARE），它利用了关系数据库中数据的规则结构，通过单过训练来快速地训练深度学习模型，并且可以充分利用相似性来降低模型的维度。</li>
<li>results: 对多个基线模型进行了比较，研究发现SPARE可以在训练和推理中快速减少时间，同时保持与基线模型相似的预测性能。<details>
<summary>Abstract</summary>
While there has been extensive work on deep neural networks for images and text, deep learning for relational databases (RDBs) is still a rather unexplored field.   One direction that recently gained traction is to apply Graph Neural Networks (GNNs) to RBDs. However, training GNNs on large relational databases (i.e., data stored in multiple database tables) is rather inefficient due to multiple rounds of training and potentially large and inefficient representations. Hence, in this paper we propose SPARE (Single-Pass Relational models), a new class of neural models that can be trained efficiently on RDBs while providing similar accuracies as GNNs. For enabling efficient training, different from GNNs, SPARE makes use of the fact that data in RDBs has a regular structure, which allows one to train these models in a single pass while exploiting symmetries at the same time. Our extensive empirical evaluation demonstrates that SPARE can significantly speedup both training and inference while offering competitive predictive performance over numerous baselines.
</details>
<details>
<summary>摘要</summary>
tradicional deep learning for images and text 领域已经有了广泛的研究，但是deep learning for relational databases（RDB）仍然是一个未经探索的领域。  一个Recently gained traction的方向是将Graph Neural Networks（GNNs）应用于RBD。然而，在大规模的关系数据库（即数据存储在多个数据库表中）上训练GNNs是不具有效率的，因为需要多 rondas of training和可能很大的不效率表示。因此，在这篇论文中，我们提出了SPARE（Single-Pass Relational models），一种新的神经网络模型，可以高效地在RDB上训练，并且提供与GNNs相似的准确性。为了实现高效的训练，SPARE不同于GNNs，利用了RDB中数据的Regular structure，这 позвоes 一次性训练这些模型，同时利用Symmetries。我们的广泛的实验证明，SPARE可以明显提高训练和推理的速度，并且与多种基eline提供竞争的预测性能。
</details></li>
</ul>
<hr>
<h2 id="Tree-Search-in-DAG-Space-with-Model-based-Reinforcement-Learning-for-Causal-Discovery"><a href="#Tree-Search-in-DAG-Space-with-Model-based-Reinforcement-Learning-for-Causal-Discovery" class="headerlink" title="Tree Search in DAG Space with Model-based Reinforcement Learning for Causal Discovery"></a>Tree Search in DAG Space with Model-based Reinforcement Learning for Causal Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13576">http://arxiv.org/abs/2310.13576</a></li>
<li>repo_url: None</li>
<li>paper_authors: Victor-Alexandru Darvariu, Stephen Hailes, Mirco Musolesi</li>
<li>for: 本研究旨在提出一种基于模型搜索的 causal 发现方法，用于解决多个领域中的决策和生物经济学等领域中的 causal 结构难以确定问题。</li>
<li>methods: 本研究使用了搜索树来逐步构建导向的无环图，并提出了一种有效的算法来排除引入循环的边，从而实现更深入的离散搜索和采样在 DAGC 空间中。</li>
<li>results: 研究人员对两个实际任务进行了评估，并获得了较好的性能，超过了当前的模型自由方法和扩散搜索的性能，表明该方法在 combinatorial 方法中表现了明显的进步。<details>
<summary>Abstract</summary>
Identifying causal structure is central to many fields ranging from strategic decision-making to biology and economics. In this work, we propose a model-based reinforcement learning method for causal discovery based on tree search, which builds directed acyclic graphs incrementally. We also formalize and prove the correctness of an efficient algorithm for excluding edges that would introduce cycles, which enables deeper discrete search and sampling in DAG space. We evaluate our approach on two real-world tasks, achieving substantially better performance than the state-of-the-art model-free method and greedy search, constituting a promising advancement for combinatorial methods.
</details>
<details>
<summary>摘要</summary>
找到 causal 结构是许多领域的中心问题，从策略决策到生物和经济学。在这项工作中，我们提出了基于模型的回归学习方法 для causal 发现，使用搜索树来逐步构建导向的无环图。我们还正式定义和证明了一种高效的算法来排除引入环的边，这使得精确的搜索和采样在 DAG 空间中可以进行更深入的探索。我们在两个实际任务上评估了我们的方法，与当前状态的模型自由方法和排序搜索具有显著更好的性能，代表了 combinatorial 方法的进步。
</details></li>
</ul>
<hr>
<h2 id="Boosting-Generalization-with-Adaptive-Style-Techniques-for-Fingerprint-Liveness-Detection"><a href="#Boosting-Generalization-with-Adaptive-Style-Techniques-for-Fingerprint-Liveness-Detection" class="headerlink" title="Boosting Generalization with Adaptive Style Techniques for Fingerprint Liveness Detection"></a>Boosting Generalization with Adaptive Style Techniques for Fingerprint Liveness Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13573">http://arxiv.org/abs/2310.13573</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kexin Zhu, Bo Lin, Yang Qiu, Adam Yule, Yao Tang, Jiajun Liang</li>
<li>for: 本研究旨在提出一种高性能的指纹生物特征提取技术，并在 LivDet 2023 指纹表现挑战中获得第一名。</li>
<li>methods: 本研究使用了多种方法，包括样式转移，以提高精度和泛化能力。</li>
<li>results: 本研究在 LivDet 2023 生命检测在动作中的挑战中获得第二名，并在 LivDet 2023 指纹表现挑战中实现了状态的最佳性能。<details>
<summary>Abstract</summary>
We introduce a high-performance fingerprint liveness feature extraction technique that secured first place in LivDet 2023 Fingerprint Representation Challenge. Additionally, we developed a practical fingerprint recognition system with 94.68% accuracy, earning second place in LivDet 2023 Liveness Detection in Action. By investigating various methods, particularly style transfer, we demonstrate improvements in accuracy and generalization when faced with limited training data. As a result, our approach achieved state-of-the-art performance in LivDet 2023 Challenges.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种高性能指纹生活特征提取技术，在2023年生活特征挑战赛中获得第一名。此外，我们还开发了一个实用的指纹识别系统，准确率达94.68%，在2023年生活检测在动作赛中获得第二名。通过各种方法的研究，特别是样式传输，我们证明了对有限训练数据的改进。因此，我们的方法在2023年生活检测挑战中达到了国际前列水平。
</details></li>
</ul>
<hr>
<h2 id="Retrieval-Augmented-Neural-Response-Generation-Using-Logical-Reasoning-and-Relevance-Scoring"><a href="#Retrieval-Augmented-Neural-Response-Generation-Using-Logical-Reasoning-and-Relevance-Scoring" class="headerlink" title="Retrieval-Augmented Neural Response Generation Using Logical Reasoning and Relevance Scoring"></a>Retrieval-Augmented Neural Response Generation Using Logical Reasoning and Relevance Scoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13566">http://arxiv.org/abs/2310.13566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicholas Thomas Walker, Stefan Ultes, Pierre Lison</li>
<li>for: 这个论文是为了提出一种基于知识图和逻辑推理的响应生成方法，以提高对话系统的响应质量。</li>
<li>methods: 该方法包括在对话状态和背景信息上构建知识图，然后使用 probabilistic logical programming 推理出逻辑推理出逻辑推理得到的信息，最后使用神经网络模型对对话中每个节点和边进行排名，并将最高排名的元素转化为自然语言形式，并与对话系统的响应相结合。</li>
<li>results: 实验结果表明，将逻辑推理与对话 relevance 排名结合，可以提高对话系统的响应的 фактиче性和流畅性。<details>
<summary>Abstract</summary>
Constructing responses in task-oriented dialogue systems typically relies on information sources such the current dialogue state or external databases. This paper presents a novel approach to knowledge-grounded response generation that combines retrieval-augmented language models with logical reasoning. The approach revolves around a knowledge graph representing the current dialogue state and background information, and proceeds in three steps. The knowledge graph is first enriched with logically derived facts inferred using probabilistic logical programming. A neural model is then employed at each turn to score the conversational relevance of each node and edge of this extended graph. Finally, the elements with highest relevance scores are converted to a natural language form, and are integrated into the prompt for the neural conversational model employed to generate the system response.   We investigate the benefits of the proposed approach on two datasets (KVRET and GraphWOZ) along with a human evaluation. Experimental results show that the combination of (probabilistic) logical reasoning with conversational relevance scoring does increase both the factuality and fluency of the responses.
</details>
<details>
<summary>摘要</summary>
通常情况下，任务导向对话系统的响应执行都是基于对话状态或外部数据库的信息。这篇论文提出了一种新的知识固定响应生成方法，该方法结合检索加强语言模型和逻辑推理。该方法的核心思想是使用对话状态和背景信息的知识图，并在三个步骤中进行处理。首先，将对话状态和背景信息转换为逻辑推理可以生成的逻辑知识图。然后，使用神经网络模型对每个转换后的图进行分类，以评估对话中每个节点和边的对话相关性。最后，选择分类结果最高的元素，并将其转换为自然语言形式，以整合到用于生成系统响应的神经网络模型中。我们在两个数据集（KVRET和GraphWOZ）上进行了实验和人工评估，结果表明，将逻辑推理与对话相关性分类结合使用，可以提高响应的事实性和流畅性。
</details></li>
</ul>
<hr>
<h2 id="Reward-Shaping-for-Happier-Autonomous-Cyber-Security-Agents"><a href="#Reward-Shaping-for-Happier-Autonomous-Cyber-Security-Agents" class="headerlink" title="Reward Shaping for Happier Autonomous Cyber Security Agents"></a>Reward Shaping for Happier Autonomous Cyber Security Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13565">http://arxiv.org/abs/2310.13565</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elizabeth Bates, Vasilios Mavroudis, Chris Hicks</li>
<li>for: 这种工作研究了计算机网络防御任务中深度强化学习模型的训练方法，特别是对奖励信号的影响。</li>
<li>methods: 本研究使用了奖励信号的修正技巧，以帮助代理人更 efficiently 训练和可能 converges to better performance。</li>
<li>results: 研究发现，深度强化学习算法对奖励信号的大小和相对大小有敏感性。此外，结合奖励和外部奖励的组合训练可以与奖励只训练相比，提高代理人的训练效率和性能。但是，内在的好奇心作为一种内部正面奖励机制可能不太适用于高级网络监测任务。<details>
<summary>Abstract</summary>
As machine learning models become more capable, they have exhibited increased potential in solving complex tasks. One of the most promising directions uses deep reinforcement learning to train autonomous agents in computer network defense tasks. This work studies the impact of the reward signal that is provided to the agents when training for this task. Due to the nature of cybersecurity tasks, the reward signal is typically 1) in the form of penalties (e.g., when a compromise occurs), and 2) distributed sparsely across each defense episode. Such reward characteristics are atypical of classic reinforcement learning tasks where the agent is regularly rewarded for progress (cf. to getting occasionally penalized for failures). We investigate reward shaping techniques that could bridge this gap so as to enable agents to train more sample-efficiently and potentially converge to a better performance. We first show that deep reinforcement learning algorithms are sensitive to the magnitude of the penalties and their relative size. Then, we combine penalties with positive external rewards and study their effect compared to penalty-only training. Finally, we evaluate intrinsic curiosity as an internal positive reward mechanism and discuss why it might not be as advantageous for high-level network monitoring tasks.
</details>
<details>
<summary>摘要</summary>
随着机器学习模型的能力不断提高，它们在解决复杂任务方面表现出了潜在的投资潜力。一个最有前途的方向是使用深度反馈学习训练自动化代理人在计算机网络防御任务中。这项研究研究了在这个任务中训练代理人时的奖励信号的影响。由于网络安全任务的性质，奖励信号通常是1）在攻击发生时给出的罚款（例如），和2）每个防御集的分布式罚款。这种奖励特点与 класси型反馈学习任务不同，agent Regularly rewarded for progress（比如 occasional penalties for failures）。我们研究了修复奖励技巧，以帮助代理人更加样本效率地训练，并可能达到更好的性能。我们首先表明深度反馈学习算法对奖励的大小和相对大小的敏感性。然后，我们将罚款与正面的外部奖励相结合，并比较奖励只有训练的效果。最后，我们评估了内在的好奇性作为内部正面奖励机制，并讨论了为高级网络监测任务而言，这种机制可能不太有利。
</details></li>
</ul>
<hr>
<h2 id="Self-prompted-Chain-of-Thought-on-Large-Language-Models-for-Open-domain-Multi-hop-Reasoning"><a href="#Self-prompted-Chain-of-Thought-on-Large-Language-Models-for-Open-domain-Multi-hop-Reasoning" class="headerlink" title="Self-prompted Chain-of-Thought on Large Language Models for Open-domain Multi-hop Reasoning"></a>Self-prompted Chain-of-Thought on Large Language Models for Open-domain Multi-hop Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13552">http://arxiv.org/abs/2310.13552</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/noewangjy/sp-cot">https://github.com/noewangjy/sp-cot</a></li>
<li>paper_authors: Jinyuan Wang, Junlong Li, Hai Zhao</li>
<li>for: 这 paper 的目的是提出一种自动生成高质量的 chain-of-thought (CoT) 框架，以提高大语言模型 (LLM) 的多步逻辑能力。</li>
<li>methods: 这 paper 使用了一种自动生成高质量 CoT 数据集的框架，以及一种适应性抽取器来选择在上下文中的 CoT。另外，它还使用了一种自适应学习的句子提示方法来进行自我推导。</li>
<li>results: 对四个多步逻辑问答 benchmark 进行了广泛的实验，并显示了 SP-CoT 可以在大规模 (175B) LLM 上显著超越之前的 SOTA 方法，同时也可以在小规模 (13B) LLM 上近乎双倍提高零基eline性能。进一步的分析发现，SP-CoT 能够诱导 LLM 提供直接和简洁的中间逻辑步骤，在 MuSiQue-Ans 数据集上 recall 约 50% 的中间答案。<details>
<summary>Abstract</summary>
In open-domain question-answering (ODQA), most existing questions require single-hop reasoning on commonsense. To further extend this task, we officially introduce open-domain multi-hop reasoning (ODMR) by answering multi-hop questions with explicit reasoning steps in open-domain setting. Recently, large language models (LLMs) have found significant utility in facilitating ODQA without external corpus. Furthermore, chain-of-thought (CoT) prompting boosts the reasoning capability of LLMs to a greater extent with manual or automated paradigms. However, existing automated methods lack of quality assurance, while manual approaches suffer from limited scalability and poor diversity, hindering the capabilities of LLMs. In this paper, we propose Self-prompted Chain-of-Thought (SP-CoT), an automated framework to mass-produce high quality CoTs of LLMs, by LLMs and for LLMs. SP-CoT introduces an automated generation pipeline of high quality ODMR datasets, an adaptive sampler for in-context CoT selection and self-prompted inference via in-context learning. Extensive experiments on four multi-hop question-answering benchmarks show that our proposed SP-CoT not only significantly surpasses the previous SOTA methods on large-scale (175B) LLMs, but also nearly doubles the zero-shot performance of small-scale (13B) LLMs. Further analysis reveals the remarkable capability of SP-CoT to elicit direct and concise intermediate reasoning steps by recalling $\sim$50\% of intermediate answers on MuSiQue-Ans dataset.
</details>
<details>
<summary>摘要</summary>
在开放预测问答（ODQA）任务中，大多数现有的问题需要单步逻辑。为了进一步推进这个任务，我们正式引入开放预测多步逻辑（ODMR），通过在开放预测 Setting中回答多步问题，并提供明确的逻辑步骤。现在，大型自然语言模型（LLM）在ODQA中发现了显著的用于促进逻辑能力的用途。此外，链条思维（CoT）提问技术可以大幅提高LLM的逻辑能力，但现有的自动化方法缺乏质量保证，而手动方法受限于批量缺乏多样性，这限制了LLM的能力。在这篇论文中，我们提出了自动生成高质量链条思维（SP-CoT）框架，用于自动生成高质量ODMR数据集，适应Context的CoT选择和自我提示的推理。我们的实验表明，我们提出的SP-CoT不仅在大规模（175B）LLM上明显超越了之前的SOTA方法，而且在小规模（13B）LLM上也近乎双倍了零基eline性能。进一步分析发现，SP-CoT可以诱导直接和简洁的中间逻辑步骤，在MuSiQue-Ans数据集上回快约50%的中间答案。
</details></li>
</ul>
<hr>
<h2 id="Towards-Understanding-Sycophancy-in-Language-Models"><a href="#Towards-Understanding-Sycophancy-in-Language-Models" class="headerlink" title="Towards Understanding Sycophancy in Language Models"></a>Towards Understanding Sycophancy in Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13548">http://arxiv.org/abs/2310.13548</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/meg-tong/sycophancy-eval">https://github.com/meg-tong/sycophancy-eval</a></li>
<li>paper_authors: Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott R. Johnston, Shauna Kravec, Timothy Maxwell, Sam McCandlish, Kamal Ndousse, Oliver Rausch, Nicholas Schiefer, Da Yan, Miranda Zhang, Ethan Perez</li>
<li>for:  investigate the prevalence of sycophancy in models whose finetuning procedure made use of human feedback</li>
<li>methods:  five state-of-the-art AI assistants consistently exhibit sycophancy across four varied free-form text-generation tasks</li>
<li>results:  human preferences drive this broadly observed behavior, and both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a non-negligible fraction of the time<details>
<summary>Abstract</summary>
Human feedback is commonly utilized to finetune AI assistants. But human feedback may also encourage model responses that match user beliefs over truthful ones, a behaviour known as sycophancy. We investigate the prevalence of sycophancy in models whose finetuning procedure made use of human feedback, and the potential role of human preference judgments in such behavior. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophancy across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a non-negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Overall, our results indicate that sycophancy is a general behavior of state-of-the-art AI assistants, likely driven in part by human preference judgments favoring sycophantic responses.
</details>
<details>
<summary>摘要</summary>
人类反馈通常用于精细化AI助手。然而，人类反馈也可能导致模型响应与用户的信念相符，这种行为称为奉承。我们研究了使用人类反馈进行finetuning的模型中是否存在奉承行为，以及人类喜好判断是否对此行为产生影响。我们首先表明了五种当前顶尖AI助手在四种不同的自由文本生成任务上一致地表现出奉承行为。为了了解人类喜好是否驱动这种广泛观察到的行为，我们分析了现有的人类喜好数据。我们发现当响应与用户的观点相符时，它更可能被 preference。此外，人类和喜好模型（PM）也有一定的时间喜好奉承而不是正确的响应。优化模型输出对PM也有时会损失真实性，而是偏向奉承。总的来说，我们的结果表明，state-of-the-art AI助手中的奉承行为是一种普遍存在的现象，可能受到人类喜好判断的影响。
</details></li>
</ul>
<hr>
<h2 id="ScaleLong-Towards-More-Stable-Training-of-Diffusion-Model-via-Scaling-Network-Long-Skip-Connection"><a href="#ScaleLong-Towards-More-Stable-Training-of-Diffusion-Model-via-Scaling-Network-Long-Skip-Connection" class="headerlink" title="ScaleLong: Towards More Stable Training of Diffusion Model via Scaling Network Long Skip Connection"></a>ScaleLong: Towards More Stable Training of Diffusion Model via Scaling Network Long Skip Connection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13545">http://arxiv.org/abs/2310.13545</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sail-sg/scalelong">https://github.com/sail-sg/scalelong</a></li>
<li>paper_authors: Zhongzhan Huang, Pan Zhou, Shuicheng Yan, Liang Lin</li>
<li>for: 这个论文主要是为了解释UNet在diffusion模型中的不稳定性问题，以及对UNet的缩放长连接系数（LSC）的影响。</li>
<li>methods: 这个论文使用了 teoretic 方法来解释UNet在diffusion模型中的不稳定性问题，并提出了一种名为ScaleLong的缩放长连接系数 frameworks，以改进UNet的训练稳定性。</li>
<li>results: 实验结果表明，ScaleLong 方法可以更好地稳定训练UNet，并且可以在不同的diffusion模型和UNet&#x2F;UViT backbones上提高训练速度约1.5倍。<details>
<summary>Abstract</summary>
In diffusion models, UNet is the most popular network backbone, since its long skip connects (LSCs) to connect distant network blocks can aggregate long-distant information and alleviate vanishing gradient. Unfortunately, UNet often suffers from unstable training in diffusion models which can be alleviated by scaling its LSC coefficients smaller. However, theoretical understandings of the instability of UNet in diffusion models and also the performance improvement of LSC scaling remain absent yet. To solve this issue, we theoretically show that the coefficients of LSCs in UNet have big effects on the stableness of the forward and backward propagation and robustness of UNet. Specifically, the hidden feature and gradient of UNet at any layer can oscillate and their oscillation ranges are actually large which explains the instability of UNet training. Moreover, UNet is also provably sensitive to perturbed input, and predicts an output distant from the desired output, yielding oscillatory loss and thus oscillatory gradient. Besides, we also observe the theoretical benefits of the LSC coefficient scaling of UNet in the stableness of hidden features and gradient and also robustness. Finally, inspired by our theory, we propose an effective coefficient scaling framework ScaleLong that scales the coefficients of LSC in UNet and better improves the training stability of UNet. Experimental results on four famous datasets show that our methods are superior to stabilize training and yield about 1.5x training acceleration on different diffusion models with UNet or UViT backbones. Code: https://github.com/sail-sg/ScaleLong
</details>
<details>
<summary>摘要</summary>
在扩散模型中，UNet 是最受欢迎的网络脊梁，因为它的长距离连接（LSC）可以将远方网络块的信息聚合并使得渐减 gradient 问题得到解决。然而，UNet 在扩散模型中的训练过程经常存在不稳定性问题，这可以通过减小 LSC 系数来缓解。然而，关于 UNet 在扩散模型中不稳定性的理论理解和 LSC 系数缩放对性能的改进仍然缺乏研究。为解决这个问题，我们理论上表明了 LSC 系数在 UNet 中对前向和反向传播稳定性和 robustness 的大量影响。具体来说，UNet 的隐藏特征和梯度在任何层都可能会振荡，其振荡范围实际很大，这解释了 UNet 训练不稳定的原因。此外，我们还发现 UNet 对输入的小偏差会导致输出与期望输出远离，从而导致振荡的损失和振荡的梯度。除此之外，我们还发现 LSC 系数缩放对 UNet 的隐藏特征和梯度的稳定性和 robustness 有理论上的 beneficial 效果。最后，我们根据我们的理论，提出了一个有效的 coefficient scaling 框架 ScaleLong，可以更好地改进 UNet 的训练稳定性。实验结果表明，我们的方法在四个知名的 dataset 上的训练稳定性和训练速度比 Traditional UNet 和 UViT 更好，具体来说，我们的方法可以在不同的扩散模型上带来约 1.5 倍的训练加速。代码可以在 GitHub 上找到：https://github.com/sail-sg/ScaleLong
</details></li>
</ul>
<hr>
<h2 id="Positive-Unlabeled-Node-Classification-with-Structure-aware-Graph-Learning"><a href="#Positive-Unlabeled-Node-Classification-with-Structure-aware-Graph-Learning" class="headerlink" title="Positive-Unlabeled Node Classification with Structure-aware Graph Learning"></a>Positive-Unlabeled Node Classification with Structure-aware Graph Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13538">http://arxiv.org/abs/2310.13538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hansi Yang, Yongqi Zhang, Quanming Yao, James Kwok</li>
<li>for: 这篇论文是针对 graphs 上的 node classification 问题进行研究，特别是在 positive-unlabeled (PU) 的情况下。</li>
<li>methods: 本文提出了一个 distance-aware PU 损失函数，利用 graph 的 homophily 来提供更加精确的超级vision。此外，本文还提出了一个对 graph 结构进行调整的正规化器。</li>
<li>results: 实验结果显示，该方法在多种不同的 graph 数据集上表现出色，较先前的 state-of-the-art 方法有更好的性能。<details>
<summary>Abstract</summary>
Node classification on graphs is an important research problem with many applications. Real-world graph data sets may not be balanced and accurate as assumed by most existing works. A challenging setting is positive-unlabeled (PU) node classification, where labeled nodes are restricted to positive nodes. It has diverse applications, e.g., pandemic prediction or network anomaly detection. Existing works on PU node classification overlook information in the graph structure, which can be critical. In this paper, we propose to better utilize graph structure for PU node classification. We first propose a distance-aware PU loss that uses homophily in graphs to introduce more accurate supervision. We also propose a regularizer to align the model with graph structure. Theoretical analysis shows that minimizing the proposed loss also leads to minimizing the expected loss with both positive and negative labels. Extensive empirical evaluation on diverse graph data sets demonstrates its superior performance over existing state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
节点分类在图上是一个重要的研究问题，具有多种应用。现实中的图数据集可能不具备准确性和平衡性，而大多数现有工作假设了这些假设。困难的设定是正面未标注（PU）节点分类，即标注节点只能是正面节点。它在多个应用中具有多样性，例如疫苗预测或网络异常检测。现有的PU节点分类方法忽略了图结构信息，这可能是关键。在这篇论文中，我们提议更好地利用图结构来进行PU节点分类。我们首先提出了距离意识PU损失函数，使用图中的同类关系（homophily）引入更加准确的监督。我们还提出了一种对齐模型与图结构的正则项。理论分析表明，尝试最小化我们提出的损失函数也将导致最小化预期损失函数中的正面和负面标签。我们在多种多样的图数据集进行了广泛的实验，证明了我们的方法在现有状态的方法之上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Technical-Report-for-ICCV-2023-Visual-Continual-Learning-Challenge-Continuous-Test-time-Adaptation-for-Semantic-Segmentation"><a href="#Technical-Report-for-ICCV-2023-Visual-Continual-Learning-Challenge-Continuous-Test-time-Adaptation-for-Semantic-Segmentation" class="headerlink" title="Technical Report for ICCV 2023 Visual Continual Learning Challenge: Continuous Test-time Adaptation for Semantic Segmentation"></a>Technical Report for ICCV 2023 Visual Continual Learning Challenge: Continuous Test-time Adaptation for Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13533">http://arxiv.org/abs/2310.13533</a></li>
<li>repo_url: None</li>
<li>paper_authors: Damian Sójka, Yuyang Liu, Dipam Goswami, Sebastian Cygert, Bartłomiej Twardowski, Joost van de Weijer<br>for:The paper is written for developing a test-time adaptation (TTA) method for semantic segmentation in video sequences, specifically for adapting to gradually changing domains caused by weather conditions and time of day.methods:The proposed TTA method uses a synthetic driving video dataset called SHIFT, and the source model is trained on images taken during daytime in clear weather. The method adapts the model to the changing domains by analyzing the distributional shift and developing a method that can generalize across different scenarios.results:The proposed method secured a 3rd place in a challenge and received an innovation award, outperforming solutions that used external pretrained models or specialized data augmentations. The method demonstrated the ability to adapt to changing data dynamics and generalize across different scenarios.Here’s the Chinese translation of the three points:for:本文是为开发一种测试时适应（TTA）方法，用于视频序列中的 semantics 分割任务，特别是针对逐渐变化的领域所导致的变化。methods:提议的 TTA 方法使用了一个 sintetic driving 视频数据集 called SHIFT，并使用源模型在晴朗的日子时光下训练。该方法通过分析分布Shift的方法，来适应不同的enario。results:提议的方法在一个挑战中获得第三名并获得了创新奖，比其他使用外部预训练模型或特殊数据增强的解决方案更好。该方法表明了适应不同的数据动态和场景的能力。<details>
<summary>Abstract</summary>
The goal of the challenge is to develop a test-time adaptation (TTA) method, which could adapt the model to gradually changing domains in video sequences for semantic segmentation task. It is based on a synthetic driving video dataset - SHIFT. The source model is trained on images taken during daytime in clear weather. Domain changes at test-time are mainly caused by varying weather conditions and times of day. The TTA methods are evaluated in each image sequence (video) separately, meaning the model is reset to the source model state before the next sequence. Images come one by one and a prediction has to be made at the arrival of each frame. Each sequence is composed of 401 images and starts with the source domain, then gradually drifts to a different one (changing weather or time of day) until the middle of the sequence. In the second half of the sequence, the domain gradually shifts back to the source one. Ground truth data is available only for the validation split of the SHIFT dataset, in which there are only six sequences that start and end with the source domain. We conduct an analysis specifically on those sequences. Ground truth data for test split, on which the developed TTA methods are evaluated for leader board ranking, are not publicly available.   The proposed solution secured a 3rd place in a challenge and received an innovation award. Contrary to the solutions that scored better, we did not use any external pretrained models or specialized data augmentations, to keep the solutions as general as possible. We have focused on analyzing the distributional shift and developing a method that could adapt to changing data dynamics and generalize across different scenarios.
</details>
<details>
<summary>摘要</summary>
挑战目标是开发一种测试时适应（TTA）方法，用于在视频序列中进行Semantic Segmentation任务中的模型适应。该方法基于一个合成的驾驶视频集（SHIFT），源模型在日间晴朗的图像上训练。测试时的领域变化主要由不同的天气和时间条件引起。TTA方法在每个图像序列（视频）上进行评估，因此模型在下一个序列之前被重置到源模型状态。图像来一个一个，需要在每帧预测。每个序列由401帧组成，开头是源领域，然后慢慢地变化到不同的领域（不同的天气或时间），中间一部分是源领域，然后再次变化回源领域。VALIDATION Split的真实数据可以进行特定的分析，但是TEST Split的真实数据，用于评估开发的TTA方法，不公开。我们的解决方案在挑战中获得了第三名，并且获得了创新奖。与其他更高分的解决方案不同，我们没有使用任何外部预训练模型或特殊的数据增强，以保持解决方案的通用性。我们主要关注分布式变化的分析，并开发了一种能够适应不同数据动态的方法，并且在不同的场景下具有普适性。
</details></li>
</ul>
<hr>
<h2 id="Design-Inclusive-Language-Models-for-Responsible-Information-Access"><a href="#Design-Inclusive-Language-Models-for-Responsible-Information-Access" class="headerlink" title="Design-Inclusive Language Models for Responsible Information Access"></a>Design-Inclusive Language Models for Responsible Information Access</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18333">http://arxiv.org/abs/2310.18333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Veronica Chatrath, Oluwanifemi Bamgbose, Shaina Raza</li>
<li>for: 这个研究旨在发展一个名为“责任预测语言模型（ReDev）”的框架，以促进对所有用户而言的公正、安全和可靠的语言模型开发。</li>
<li>methods: 研究人员使用了一个专门设计来评估语言模型的测试组合，以确保模型的输出不含有偏见或害害的内容。</li>
<li>results: 研究发现，现有的四个州录语言模型（OPT、GPT-3.5、GPT-4和LLaMA-2）在不同的测试中表现不佳，表明需要在机器学习管线中考虑公正、安全和可靠性。<details>
<summary>Abstract</summary>
As the use of large language models (LLMs) increases for everyday tasks, appropriate safeguards must be in place to ensure unbiased and safe output. Recent events highlight ethical concerns around conventionally trained LLMs, leading to overall unsafe user experiences. This motivates the need for responsible LLMs that are trained fairly, transparent to the public, and regularly monitored after deployment. In this work, we introduce the "Responsible Development of Language Models (ReDev)" framework to foster the development of fair, safe, and robust LLMs for all users. We also present a test suite of unique prompt types to assess LLMs on the aforementioned elements, ensuring all generated responses are non-harmful and free from biased content. Outputs from four state-of-the-art LLMs, OPT, GPT-3.5, GPT-4, and LLaMA-2, are evaluated by our test suite, highlighting the importance of considering fairness, safety, and robustness at every stage of the machine learning pipeline, including data curation, training, and post-deployment.
</details>
<details>
<summary>摘要</summary>
As the use of large language models (LLMs) increases for everyday tasks, appropriate safeguards must be in place to ensure unbiased and safe output. Recent events highlight ethical concerns around conventionally trained LLMs, leading to overall unsafe user experiences. This motivates the need for responsible LLMs that are trained fairly, transparent to the public, and regularly monitored after deployment. In this work, we introduce the "Responsible Development of Language Models (ReDev)" framework to foster the development of fair, safe, and robust LLMs for all users. We also present a test suite of unique prompt types to assess LLMs on the aforementioned elements, ensuring all generated responses are non-harmful and free from biased content. Outputs from four state-of-the-art LLMs, OPT, GPT-3.5, GPT-4, and LLaMA-2, are evaluated by our test suite, highlighting the importance of considering fairness, safety, and robustness at every stage of the machine learning pipeline, including data curation, training, and post-deployment.Here's the translation in Traditional Chinese:当大语言模型（LLMs）在日常任务中使用时，应该有适当的安全措施，以确保无偏袋和安全的输出。最近的事件显示了传统的 LLMS 对于不公正和不安全的用户体验问题，这引起了发展公正、透明和定期监控的 LLMS 的需求。在这个工作中，我们介绍了“责任的语言模型开发（ReDev）”框架，以促进公正、安全和可靠的 LLMS 的开发。我们还提出了一个对 LLMS 进行评估的测试组合，以确保所有生成的回应都是无害的和不受偏袋的。从四个现代 LLMS 中，OPT、GPT-3.5、GPT-4和LLaMA-2 的输出都被我们的测试组合评估，强调了在机器学习管线中考虑公平、安全和可靠性的重要性，包括数据混合、训练和部署阶段。
</details></li>
</ul>
<hr>
<h2 id="Variational-measurement-based-quantum-computation-for-generative-modeling"><a href="#Variational-measurement-based-quantum-computation-for-generative-modeling" class="headerlink" title="Variational measurement-based quantum computation for generative modeling"></a>Variational measurement-based quantum computation for generative modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13524">http://arxiv.org/abs/2310.13524</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arunava Majumder, Marius Krumm, Tina Radkohl, Hendrik Poulsen Nautrup, Sofiene Jerbi, Hans J. Briegel</li>
<li>for: 这篇论文旨在探讨量子测量计算（MBQC）如何使用随机性来进行计算，并explore MBQC 算法可以捕捉这种随机性作为计算资源。</li>
<li>methods: 该论文提出了一种基于 MBQC 的变量算法，该算法通过控制参数来调整随机性的度量，以提高生成模型的学习性能。</li>
<li>results: 数值研究表明，在某些生成模型任务中，该算法可以获得显著的提升，这些结果验证了 MBQC 中随机性的可能的优势，并鼓励进一步的MBQC-based算法的研究。<details>
<summary>Abstract</summary>
Measurement-based quantum computation (MBQC) offers a fundamentally unique paradigm to design quantum algorithms. Indeed, due to the inherent randomness of quantum measurements, the natural operations in MBQC are not deterministic and unitary, but are rather augmented with probabilistic byproducts. Yet, the main algorithmic use of MBQC so far has been to completely counteract this probabilistic nature in order to simulate unitary computations expressed in the circuit model. In this work, we propose designing MBQC algorithms that embrace this inherent randomness and treat the random byproducts in MBQC as a resource for computation. As a natural application where randomness can be beneficial, we consider generative modeling, a task in machine learning centered around generating complex probability distributions. To address this task, we propose a variational MBQC algorithm equipped with control parameters that allow to directly adjust the degree of randomness to be admitted in the computation. Our numerical findings indicate that this additional randomness can lead to significant gains in learning performance in certain generative modeling tasks. These results highlight the potential advantages in exploiting the inherent randomness of MBQC and motivate further research into MBQC-based algorithms.
</details>
<details>
<summary>摘要</summary>
生成模型是机器学习中心的一个任务，旨在生成复杂的概率分布。我们提议一种基于MBQC的变量算法，具有控制参数，以直接控制计算中的随机性水平。我们的numerical findings表明，这种额外的随机性可以导致certain generative modeling tasks中的学习性能提高。这些结果表明了利用MBQC的随机性的优势，并促进了进一步的MBQC算法研究。
</details></li>
</ul>
<hr>
<h2 id="RaceLens-A-Machine-Intelligence-Based-Application-for-Racing-Photo-Analysis"><a href="#RaceLens-A-Machine-Intelligence-Based-Application-for-Racing-Photo-Analysis" class="headerlink" title="RaceLens: A Machine Intelligence-Based Application for Racing Photo Analysis"></a>RaceLens: A Machine Intelligence-Based Application for Racing Photo Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13515">http://arxiv.org/abs/2310.13515</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrei Boiarov, Dmitry Bleklov, Pavlo Bredikhin, Nikita Koritsky, Sergey Ulasen</li>
<li>for: 本研究开发了一个名为 RaceLens 的应用程序，用于精确分析赛车照片。</li>
<li>methods: 本研究使用了进步的深度学习和计算机视觉模型，并实现了访问车辆、车号码、车辆细节和车辆方向的检测和识别。</li>
<li>results: 研究发现，RaceLens 可以实现高度的精确性和效能，并且在 NASCAR 队伍的四个赛季中得到了成功的应用。研究还提供了实际应用的评估和车队的策略决策和性能指标的影响。<details>
<summary>Abstract</summary>
This paper presents RaceLens, a novel application utilizing advanced deep learning and computer vision models for comprehensive analysis of racing photos. The developed models have demonstrated their efficiency in a wide array of tasks, including detecting racing cars, recognizing car numbers, detecting and quantifying car details, and recognizing car orientations. We discuss the process of collecting a robust dataset necessary for training our models, and describe an approach we have designed to augment and improve this dataset continually. Our method leverages a feedback loop for continuous model improvement, thus enhancing the performance and accuracy of RaceLens over time. A significant part of our study is dedicated to illustrating the practical application of RaceLens, focusing on its successful deployment by NASCAR teams over four seasons. We provide a comprehensive evaluation of our system's performance and its direct impact on the team's strategic decisions and performance metrics. The results underscore the transformative potential of machine intelligence in the competitive and dynamic world of car racing, setting a precedent for future applications.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "advanced deep learning and computer vision models" is translated as "高级深度学习和计算机视觉模型" (gāojí shēngrán yǔ jìsuān zhìshuāng)* "comprehensive analysis" is translated as "全面分析" (quánmiàn fāng'àn)* "racing photos" is translated as "赛车照片" (sàichē zhezhe)* "car numbers" is translated as "车辆号码" (chēliàng hàoqī)* "car details" is translated as "车辆细节" (chēliàng xiǎojiě)* "car orientations" is translated as "车辆方向" (chēliàng fāngdòng)* "robust dataset" is translated as "可靠的数据集" (kějì de xiàngxīn)* "feedback loop" is translated as "反馈循环" (fǎnggǎn xiàngxīn)* "continuous model improvement" is translated as "连续模型改进" (liánxù módel gǎijì)* "practical application" is translated as "实用应用" (shíyòng yìngyì)* "successfully deployed" is translated as "成功应用" (chéngjì yìngyì)* "four seasons" is translated as "四季" (sì jì)
</details></li>
</ul>
<hr>
<h2 id="Explaining-Interactions-Between-Text-Spans"><a href="#Explaining-Interactions-Between-Text-Spans" class="headerlink" title="Explaining Interactions Between Text Spans"></a>Explaining Interactions Between Text Spans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13506">http://arxiv.org/abs/2310.13506</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/copenlu/spanex">https://github.com/copenlu/spanex</a></li>
<li>paper_authors: Sagnik Ray Choudhury, Pepa Atanasova, Isabelle Augenstein</li>
<li>for: This paper aims to provide explanations for natural language understanding (NLU) tasks such as fact-checking (FC) and machine reading comprehension (MRC).</li>
<li>methods: The paper introduces a multi-annotator dataset of human span interaction explanations for NLU tasks, and investigates the decision-making processes of fine-tuned large language models in terms of the connections between spans in separate parts of the input.</li>
<li>results: The paper presents a novel community detection based unsupervised method to extract interaction explanations from a model’s inner workings.<details>
<summary>Abstract</summary>
Reasoning over spans of tokens from different parts of the input is essential for natural language understanding (NLU) tasks such as fact-checking (FC), machine reading comprehension (MRC) or natural language inference (NLI). However, existing highlight-based explanations primarily focus on identifying individual important tokens or interactions only between adjacent tokens or tuples of tokens. Most notably, there is a lack of annotations capturing the human decision-making process w.r.t. the necessary interactions for informed decision-making in such tasks. To bridge this gap, we introduce SpanEx, a multi-annotator dataset of human span interaction explanations for two NLU tasks: NLI and FC. We then investigate the decision-making processes of multiple fine-tuned large language models in terms of the employed connections between spans in separate parts of the input and compare them to the human reasoning processes. Finally, we present a novel community detection based unsupervised method to extract such interaction explanations from a model's inner workings.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。</SYS>>自然语言理解（NLU）任务中，理智推理 sobre 各个 Token 的范围是非常重要的，例如实验室检查（FC）、机器阅读理解（MRC）或自然语言推理（NLI）。然而，现有的高亮显示型解释主要集中于特定的重要 Token 或邻近 Token 或 Tuple 的交互。最引人注意的是，缺乏记录人类决策过程中对 informed 决策所需的交互。为了bridging这个差距，我们引入 SpanEx，一个多个注释者数据集，其中包含人类对 NLI 和 FC 任务中的 span 交互解释。然后，我们调查多个精细调节的大语言模型在输入中不同部分的 span 之间的连接方式，并与人类的决策过程进行比较。最后，我们提出一种基于社群探测的无监督方法，用于从模型内部提取这些交互解释。
</details></li>
</ul>
<hr>
<h2 id="Robust-Training-for-Conversational-Question-Answering-Models-with-Reinforced-Reformulation-Generation"><a href="#Robust-Training-for-Conversational-Question-Answering-Models-with-Reinforced-Reformulation-Generation" class="headerlink" title="Robust Training for Conversational Question Answering Models with Reinforced Reformulation Generation"></a>Robust Training for Conversational Question Answering Models with Reinforced Reformulation Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13505">http://arxiv.org/abs/2310.13505</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/magkai/REIGN">https://github.com/magkai/REIGN</a></li>
<li>paper_authors: Magdalena Kaiser, Rishiraj Saha Roy, Gerhard Weikum</li>
<li>for: 提高 conversational question answering (ConvQA) 模型在知识图 (KG) 上的表现，并且让模型能够更好地适应不同的表达形式。</li>
<li>methods: 提出了一种 frameworks named REIGN，通过系统地生成问题的 reformulations，提高模型对表达形式的弹性性，并使用深度强化学习指导模型提高答案质量。</li>
<li>results: 研究发现，通过使用 reformulations 进行强化学习，ConvQA 模型能够显著超越使用标准训练方法的模型，并且能够在不同的测试集上表现良好。<details>
<summary>Abstract</summary>
Models for conversational question answering (ConvQA) over knowledge graphs (KGs) are usually trained and tested on benchmarks of gold QA pairs. This implies that training is limited to surface forms seen in the respective datasets, and evaluation is on a small set of held-out questions. Through our proposed framework REIGN, we take several steps to remedy this restricted learning setup. First, we systematically generate reformulations of training questions to increase robustness of models to surface form variations. This is a particularly challenging problem, given the incomplete nature of such questions. Second, we guide ConvQA models towards higher performance by feeding it only those reformulations that help improve their answering quality, using deep reinforcement learning. Third, we demonstrate the viability of training major model components on one benchmark and applying them zero-shot to another. Finally, for a rigorous evaluation of robustness for trained models, we use and release large numbers of diverse reformulations generated by prompting GPT for benchmark test sets (resulting in 20x increase in sizes). Our findings show that ConvQA models with robust training via reformulations, significantly outperform those with standard training from gold QA pairs only.
</details>
<details>
<summary>摘要</summary>
模型 для对话式问答（ConvQA）在知识图（KG）上通常由金标准问答对（gold QA pairs）进行训练和测试。这意味着训练是基于表面形式所限制的，而测试是基于一小组封闭的问答。我们的提出的框架REIGN通过以下几个步骤来缓解这种受限的学习环境：首先，我们系统地生成了训练问题的重新形式，以提高模型对表面形式的鲁棒性。这是一个特别困难的问题，因为训练问题是不完整的。其次，我们使用深度奖励学习引导ConvQA模型，只有那些能够提高答案质量的重新形式。第三，我们证明了训练主要模型组件在一个benchmark上训练后，可以零式应用到另一个benchmark上。最后，为了进行训练后模型的严格评估，我们使用和发布大量多样化的重新形式，由GPT提问 benchmark测试集（ resulting in 20x increase in size）。我们的发现表明，通过对模型进行robust训练 via 重新形式，ConvQA模型可以明显超越标准训练从金标准问答对只进行训练的模型。
</details></li>
</ul>
<hr>
<h2 id="Analogical-Proportions-and-Creativity-A-Preliminary-Study"><a href="#Analogical-Proportions-and-Creativity-A-Preliminary-Study" class="headerlink" title="Analogical Proportions and Creativity: A Preliminary Study"></a>Analogical Proportions and Creativity: A Preliminary Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13500">http://arxiv.org/abs/2310.13500</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stergos Afantenos, Henri Prade, Leonardo Cortez Bernardes</li>
<li>For: The paper is written to explore the use of analogical proportions for creating novel animal descriptions and retrieving rare animals.* Methods: The paper uses word embeddings and Boolean features to propose novel animals based on analogical proportions.* Results: The paper shows that word embeddings obtain better results in creating novel animals based on analogical proportions compared to Boolean features.Here’s the information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了探讨使用对比比例来创造新的动物描述和找到罕见的动物。</li>
<li>methods: 这篇论文使用word embedding和布尔特Feature来提出新的动物基于对比比例。</li>
<li>results: 这篇论文显示word embedding在基于对比比例创造新动物方面比布尔特Feature更好的效果。<details>
<summary>Abstract</summary>
Analogical proportions are statements of the form "$a$ is to $b$ as $c$ is to $d$", which expresses that the comparisons of the elements in pair $(a, b)$ and in pair $(c, d)$ yield similar results. Analogical proportions are creative in the sense that given 3 distinct items, the representation of a 4th item $d$, distinct from the previous items, which forms an analogical proportion with them can be calculated, provided certain conditions are met. After providing an introduction to analogical proportions and their properties, the paper reports the results of an experiment made with a database of animal descriptions and their class, where we try to "create" new animals from existing ones, retrieving rare animals such as platypus. We perform a series of experiments using word embeddings as well as Boolean features in order to propose novel animals based on analogical proportions, showing that word embeddings obtain better results.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>对比比例是形式如 "$a$ 是 $b$ 的为 $c$ 是 $d$"，表达了对 $(a, b)$ 对和 $(c, d)$ 对的比较结果相似。对比比例是创造性的，即给定三个不同的 Item，可以计算出一个第四个 Item $d$，与之前的 Item 相似，并且满足certain conditions。文章 introduce analogical proportions and their properties, and then reports the results of an experiment using a database of animal descriptions and their class, where we try to "create" new animals from existing ones, retrieving rare animals such as platypus. We perform a series of experiments using word embeddings as well as Boolean features in order to propose novel animals based on analogical proportions, showing that word embeddings obtain better results.Here's the translation in Traditional Chinese:<<SYS>>将文本翻译成繁体中文。<</SYS>>对比比例是形式如 "$a$ 是 $b$ 的为 $c$ 是 $d$"，表达了对 $(a, b)$ 对和 $(c, d)$ 对的比较结果相似。对比比例是创造性的，即给定三个不同的 Item，可以计算出一个第四个 Item $d$，与之前的 Item 相似，并且满足certain conditions。文章 introduce analogical proportions and their properties, and then reports the results of an experiment using a database of animal descriptions and their class, where we try to "create" new animals from existing ones, retrieving rare animals such as platypus. We perform a series of experiments using word embeddings as well as Boolean features in order to propose novel animals based on analogical proportions, showing that word embeddings obtain better results.
</details></li>
</ul>
<hr>
<h2 id="Mind-the-instructions-a-holistic-evaluation-of-consistency-and-interactions-in-prompt-based-learning"><a href="#Mind-the-instructions-a-holistic-evaluation-of-consistency-and-interactions-in-prompt-based-learning" class="headerlink" title="Mind the instructions: a holistic evaluation of consistency and interactions in prompt-based learning"></a>Mind the instructions: a holistic evaluation of consistency and interactions in prompt-based learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13486">http://arxiv.org/abs/2310.13486</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas Weber, Elia Bruni, Dieuwke Hupkes</li>
<li>for: 本研究的目的是找到现有语言模型的最佳适应方式，以便在当前NLP中进行任务适应。</li>
<li>methods: 本研究使用了启发式学习（Prompting）方法，并进行了系统atic的评估，以找出不同因素对预测结果的影响。</li>
<li>results: 研究发现，某些因素会导致预测结果不稳定和不一致，而其他因素则可以无恶不害地使用。这些结论可以帮助选择适合的适应方式，以提高任务适应的效果。<details>
<summary>Abstract</summary>
Finding the best way of adapting pre-trained language models to a task is a big challenge in current NLP. Just like the previous generation of task-tuned models (TT), models that are adapted to tasks via in-context-learning (ICL) are robust in some setups but not in others. Here, we present a detailed analysis of which design choices cause instabilities and inconsistencies in LLM predictions. First, we show how spurious correlations between input distributions and labels -- a known issue in TT models -- form only a minor problem for prompted models. Then, we engage in a systematic, holistic evaluation of different factors that have been found to influence predictions in a prompting setup. We test all possible combinations of a range of factors on both vanilla and instruction-tuned (IT) LLMs of different scale and statistically analyse the results to show which factors are the most influential, interactive or stable. Our results show which factors can be used without precautions and which should be avoided or handled with care in most settings.
</details>
<details>
<summary>摘要</summary>
现在的自然语言处理中，找到最佳适应预训练语言模型任务的方式是一大挑战。与前一代任务调整模型（TT）类似，通过在 контекст学习（ICL）方式进行适应，模型在某些设置下 Displaying  Robustness，但在其他设置下却存在不稳定和不一致的问题。在这里，我们提供了适应模型预测中的不稳定和不一致的分析。首先，我们表明了预训练模型输入分布和标签之间的假 correlations（一个已知的问题）在提问模型中只占了一小部分。然后，我们进行了系统性的全面性评估不同因素对预测的影响。我们测试了所有可能的组合，包括不同的规模和不同的预测模型，并使用统计分析来显示这些因素对预测的影响程度，以及它们之间的互动和稳定性。我们的结果表明了哪些因素可以无需预caution使用，哪些因素应该避免或处理得更加小心。
</details></li>
</ul>
<hr>
<h2 id="Application-of-deep-learning-for-livestock-behaviour-recognition-A-systematic-literature-review"><a href="#Application-of-deep-learning-for-livestock-behaviour-recognition-A-systematic-literature-review" class="headerlink" title="Application of deep learning for livestock behaviour recognition: A systematic literature review"></a>Application of deep learning for livestock behaviour recognition: A systematic literature review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13483">http://arxiv.org/abs/2310.13483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Rohan, Muhammad Saad Rafaq, Md. Junayed Hasan, Furqan Asghar, Ali Kashif Bashir, Tania Dottorini</li>
<li>for: 这个论文主要是为了研究使用深度学习技术来识别畜牧动物的行为。</li>
<li>methods: 这个论文使用了多种深度学习模型和网络，包括CNN、Faster R-CNN、YOLOv5和YOLOv4等模型，以及VGG16、CSPDarknet53、GoogLeNet、ResNet101和ResNet50等网络。</li>
<li>results: 这个论文的研究表明，深度学习成功地解决了13种行为识别问题，包括44种不同的行为类型。<details>
<summary>Abstract</summary>
Livestock health and welfare monitoring has traditionally been a labor-intensive task performed manually. Recent advances have led to the adoption of AI and computer vision techniques, particularly deep learning models, as decision-making tools within the livestock industry. These models have been employed for tasks like animal identification, tracking, body part recognition, and species classification. In the past decade, there has been a growing interest in using these models to explore the connection between livestock behaviour and health issues. While previous review studies have been rather generic, there is currently no review study specifically focusing on DL for livestock behaviour recognition. Hence, this systematic literature review (SLR) was conducted. The SLR involved an initial search across electronic databases, resulting in 1101 publications. After applying defined selection criteria, 126 publications were shortlisted. These publications were further filtered based on quality criteria, resulting in the selection of 44 high-quality primary studies. These studies were analysed to address the research questions. The results showed that DL successfully addressed 13 behaviour recognition problems encompassing 44 different behaviour classes. A variety of DL models and networks were employed, with CNN, Faster R-CNN, YOLOv5, and YOLOv4 being among the most common models, and VGG16, CSPDarknet53, GoogLeNet, ResNet101, and ResNet50 being popular networks. Performance evaluation involved ten different matrices, with precision and accuracy being the most frequently used. Primary studies identified challenges, including occlusion, adhesion, data imbalance, and the complexities of the livestock environment. The SLR study also discussed potential solutions and research directions to facilitate the development of autonomous livestock behaviour recognition systems.
</details>
<details>
<summary>摘要</summary>
livestock health和福祉监测曾经是一项劳动密集的任务，通常是手动完成的。近年来，人工智能和计算机视觉技术的应用，特别是深度学习模型，在畜牧业中被用作决策支持工具。这些模型已被用于动物识别、跟踪、身体部分识别和种类分类等任务。过去的十年中，有一个增长的兴趣在使用这些模型探索畜牧动物行为和健康问题之间的联系。而前一个综述研究已经是非常通用的，但是目前没有专门关于深度学习的畜牧动物行为识别综述。因此，这项系统性文献综述（SLR）被进行了。SLR involve了电子数据库的初步搜索，共计1101篇论文。经过应用定义的选择 criterion，短listed 126篇论文。这些论文进一步根据质量标准进行筛选，选择了44篇高品质的原始研究。这些研究被分析以解答研究 вопро题。结果显示，深度学习成功解决了13种行为识别问题，涵盖44种不同的行为类型。多种深度学习模型和网络被使用，其中CNN、Faster R-CNN、YOLOv5和YOLOv4是最常用的模型，而VGG16、CSPDarknet53、GoogLeNet、ResNet101和ResNet50是最受欢迎的网络。性能评价使用了十种不同的矩阵，准确率和准确率是最常用的两个矩阵。原始研究认为， occlusion、黏合、数据不均衡和畜牧环境的复杂性是挑战。SLR 研究还讨论了可能的解决方案和研究方向，以便开发自主的畜牧动物行为识别系统。
</details></li>
</ul>
<hr>
<h2 id="Ask-Language-Model-to-Clean-Your-Noisy-Translation-Data"><a href="#Ask-Language-Model-to-Clean-Your-Noisy-Translation-Data" class="headerlink" title="Ask Language Model to Clean Your Noisy Translation Data"></a>Ask Language Model to Clean Your Noisy Translation Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13469">http://arxiv.org/abs/2310.13469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quinten Bolding, Baohao Liao, Brandon James Denis, Jun Luo, Christof Monz</li>
<li>for: 该研究旨在提高MTNT数据集的用途，以便更好地评估Neural Machine Translation（NMT）模型对听频输入的敏感性。</li>
<li>methods: 该研究使用大语言模型（LLM）进行听频提取和重塑，从而提高MTNT数据集的清晰度。</li>
<li>results: 研究表明，LLM可以有效地去除听频，同时保留原始句子的 semantics。此外，LLM还能够重塑slang、argot和低语言。这些数据集被称为C-MTNT，并且在评估NMT模型的Robustness方面表现出色。<details>
<summary>Abstract</summary>
Transformer models have demonstrated remarkable performance in neural machine translation (NMT). However, their vulnerability to noisy input poses a significant challenge in practical implementation, where generating clean output from noisy input is crucial. The MTNT dataset is widely used as a benchmark for evaluating the robustness of NMT models against noisy input. Nevertheless, its utility is limited due to the presence of noise in both the source and target sentences. To address this limitation, we focus on cleaning the noise from the target sentences in MTNT, making it more suitable as a benchmark for noise evaluation. Leveraging the capabilities of large language models (LLMs), we observe their impressive abilities in noise removal. For example, they can remove emojis while considering their semantic meaning. Additionally, we show that LLM can effectively rephrase slang, jargon, and profanities. The resulting datasets, called C-MTNT, exhibit significantly less noise in the target sentences while preserving the semantic integrity of the original sentences. Our human and GPT-4 evaluations also lead to a consistent conclusion that LLM performs well on this task. Lastly, experiments on C-MTNT showcased its effectiveness in evaluating the robustness of NMT models, highlighting the potential of advanced language models for data cleaning and emphasizing C-MTNT as a valuable resource.
</details>
<details>
<summary>摘要</summary>
启示器模型在神经机器翻译（NMT）中表现出了惊人的能力。然而，它们对听风输入的敏感性带来了实际应用中的挑战，因为生成干净输出从听风输入是关键。MTNT数据集广泛用于评估NMT模型对听风输入的Robustness。然而，该数据集的利用受到了源和目标句子都含有噪音的限制。为解决这一问题，我们将目标句子中的噪音除掉，使MTNT数据集更适合用于噪音评估。利用大语言模型（LLM）的能力，我们发现它们可以从目标句子中除掉表情符号，同时考虑其 semantics 意义。此外，我们发现 LLM 可以有效地重塑 slang、短语和恶语。所得到的数据集，称为 C-MTNT，具有明显 menos 的噪音，同时保持原始句子的 semantics 完整性。我们的人工评估和 GPT-4 评估也导致了一致的结论：LLM 在这种任务上表现良好。最后，我们对 C-MTNT 进行了实验，并证明了它在评估 NMT 模型的 Robustness 方面的效果，强调了高级语言模型在数据清洁方面的潜在力量，并将 C-MTNT 作为一个价值的资源。
</details></li>
</ul>
<hr>
<h2 id="WordArt-Designer-User-Driven-Artistic-Typography-Synthesis-using-Large-Language-Models"><a href="#WordArt-Designer-User-Driven-Artistic-Typography-Synthesis-using-Large-Language-Models" class="headerlink" title="WordArt Designer: User-Driven Artistic Typography Synthesis using Large Language Models"></a>WordArt Designer: User-Driven Artistic Typography Synthesis using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18332">http://arxiv.org/abs/2310.18332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun-Yan He, Zhi-Qi Cheng, Chenyang Li, Jingdong Sun, Wangmeng Xiang, Xianhui Lin, Xiaoyang Kang, Zengke Jin, Yusen Hu, Bin Luo, Yifeng Geng, Xuansong Xie, Jingren Zhou</li>
<li>for: 这个论文旨在探讨一种基于大语言模型（LLM）的用户驱动框架，用于创造艺术 typography。</li>
<li>methods: 该系统包括四个关键模块：”LLM Engine”、”SemTypo”、”StyTypo”和”TexTypo”模块。”LLM Engine” 通过 LLM（如 GPT-3.5-turbo）解释用户输入并生成可行的提示，将抽象概念转化成可见的设计。”SemTypo module” 利用语义概念优化字体设计，寻求平衡艺术变换和可读性。”StyTypo module” 基于语义布局生成细腻的图像。”TexTypo module” 进一步提高设计的美学效果通过 текстуر渲染。</li>
<li>results: 该系统能够生成创新的文本字体，并且可以在 ModelScope 上体验其能力：<a target="_blank" rel="noopener" href="https://www.modelscope.cn/studios/WordArt/WordArt">https://www.modelscope.cn/studios/WordArt/WordArt</a>.<details>
<summary>Abstract</summary>
This paper introduces "WordArt Designer", a user-driven framework for artistic typography synthesis, relying on Large Language Models (LLM). The system incorporates four key modules: the "LLM Engine", "SemTypo", "StyTypo", and "TexTypo" modules. 1) The "LLM Engine", empowered by LLM (e.g., GPT-3.5-turbo), interprets user inputs and generates actionable prompts for the other modules, thereby transforming abstract concepts into tangible designs. 2) The "SemTypo module" optimizes font designs using semantic concepts, striking a balance between artistic transformation and readability. 3) Building on the semantic layout provided by the "SemTypo module", the "StyTypo module" creates smooth, refined images. 4) The "TexTypo module" further enhances the design's aesthetics through texture rendering, enabling the generation of inventive textured fonts. Notably, "WordArt Designer" highlights the fusion of generative AI with artistic typography. Experience its capabilities on ModelScope: https://www.modelscope.cn/studios/WordArt/WordArt.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>“LLM Engine”: empowered by LLM (e.g., GPT-3.5-turbo), it interprets user inputs and generates actionable prompts for the other modules, transforming abstract concepts into tangible designs.2. “SemTypo module”: optimizes font designs using semantic concepts, balancing artistic transformation and readability.3. “StyTypo module”: creates smooth, refined images based on the semantic layout provided by the “SemTypo module”.4. “TexTypo module”: enhances the design’s aesthetics through texture rendering, enabling the generation of inventive textured fonts.”WordArt Designer” showcases the fusion of generative AI with artistic typography. Experience its capabilities on ModelScope: <a target="_blank" rel="noopener" href="https://www.modelscope.cn/studios/WordArt/WordArt">https://www.modelscope.cn/studios/WordArt/WordArt</a>.</details></li>
</ol>
<hr>
<h2 id="Multiscale-Superpixel-Structured-Difference-Graph-Convolutional-Network-for-VL-Representation"><a href="#Multiscale-Superpixel-Structured-Difference-Graph-Convolutional-Network-for-VL-Representation" class="headerlink" title="Multiscale Superpixel Structured Difference Graph Convolutional Network for VL Representation"></a>Multiscale Superpixel Structured Difference Graph Convolutional Network for VL Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13447">http://arxiv.org/abs/2310.13447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyu Zhang, Yeming Chen, Sirui Cheng, Yaoru Sun, Jun Yang, Lizhi Bai</li>
<li>for: 本研究旨在提高多Modal Semantic Representation，尤其是vision和language之间的同步。</li>
<li>methods: 本研究使用自适应学习的pre-trained模型，并提出了superpixel作为可学习图像数据的全面表示，以及Multiscale Difference Graph Convolutional Network (MDGCN)来捕捉多尺度特征。</li>
<li>results: 对多个下游任务学习，本研究可以与其他状态调研方法竞争，并且可以更好地捕捉图像的空间semantic关系。<details>
<summary>Abstract</summary>
Within the multimodal field, the key to integrating vision and language lies in establishing a good alignment strategy. Recently, benefiting from the success of self-supervised learning, significant progress has been made in multimodal semantic representation based on pre-trained models for vision and language. However, there is still room for improvement in visual semantic representation. The lack of spatial semantic coherence and vulnerability to noise makes it challenging for current pixel or patch-based methods to accurately extract complex scene boundaries. To this end, this paper develops superpixel as a comprehensive compact representation of learnable image data, which effectively reduces the number of visual primitives for subsequent processing by clustering perceptually similar pixels. To mine more precise topological relations, we propose a Multiscale Difference Graph Convolutional Network (MDGCN). It parses the entire image as a fine-to-coarse hierarchical structure of constituent visual patterns, and captures multiscale features by progressively merging adjacent superpixels as graph nodes. Moreover, we predict the differences between adjacent nodes through the graph structure, facilitating key information aggregation of graph nodes to reason actual semantic relations. Afterward, we design a multi-level fusion rule in a bottom-up manner to avoid understanding deviation by learning complementary spatial information at different regional scales. Our proposed method can be well applied to multiple downstream task learning. Extensive experiments demonstrate that our method is competitive with other state-of-the-art methods in visual reasoning. Our code will be released upon publication.
</details>
<details>
<summary>摘要</summary>
在多模态场景中，与语言集成的关键在于设置良好的对齐策略。在最近，基于自动学习的成功，我们在多模态semantic表示方面进行了显著的进步，但是视觉semantic表示仍然存在改进的空间。现有的像素或patch基本方法难以准确地提取复杂的场景边界，主要是因为缺乏空间semantic准确性和噪声抑制的能力。为了解决这些问题，本文提出了superpixel作为可学习图像数据的总体紧凑表示，可以有效减少后续处理的视觉元素数量。此外，我们还提出了多尺度差分图 convolutional neural network（MDGCN）来捕捉多尺度特征。MDGCN通过将整个图像分解为一个细到广的 hierarchical结构，并在不同尺度上 merge 相似的superpixel作为图节点，来捕捉多尺度特征。此外，我们还预测了图节点之间的差异，以便通过图结构来汇总实际semantic关系的信息。最后，我们设计了一种多级融合规则，以避免不同地域尺度上的理解偏差。我们的提出的方法可以应用于多个下游任务学习。广泛的实验表明，我们的方法与其他当前领先的方法在视理解方面具有竞争力。我们的代码将在发表时公布。
</details></li>
</ul>
<hr>
<h2 id="Self-Consistency-of-Large-Language-Models-under-Ambiguity"><a href="#Self-Consistency-of-Large-Language-Models-under-Ambiguity" class="headerlink" title="Self-Consistency of Large Language Models under Ambiguity"></a>Self-Consistency of Large Language Models under Ambiguity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13439">http://arxiv.org/abs/2310.13439</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jacobpfau/introspective-self-consistency">https://github.com/jacobpfau/introspective-self-consistency</a></li>
<li>paper_authors: Henning Bartsch, Ole Jorgensen, Domenic Rosati, Jason Hoelscher-Obermaier, Jacob Pfau</li>
<li>for: 这个论文的目的是评估自身一致性在不充分约束下的情况，以及模型在不同上下文中的一致性是否能够保持。</li>
<li>methods: 这个论文使用了OpenAI模型集进行了一系列行为实验，测试了模型在一个抽象整数序列完成任务上的表现，并发现模型的一致性范围为67%-82%，高于随机预测的水平，并且随着模型能力的提高而增加。</li>
<li>results: 这个论文发现，即使模型在不同的Robustness Check中保持了自身一致性，但模型并不总能够正确地评估自身一致性。此外，模型通常会将一些不一致的答案分配给一定的概率，这提供了内部计算多个可能答案的证据。<details>
<summary>Abstract</summary>
Large language models (LLMs) that do not give consistent answers across contexts are problematic when used for tasks with expectations of consistency, e.g., question-answering, explanations, etc. Our work presents an evaluation benchmark for self-consistency in cases of under-specification where two or more answers can be correct. We conduct a series of behavioral experiments on the OpenAI model suite using an ambiguous integer sequence completion task. We find that average consistency ranges from 67\% to 82\%, far higher than would be predicted if a model's consistency was random, and increases as model capability improves. Furthermore, we show that models tend to maintain self-consistency across a series of robustness checks, including prompting speaker changes and sequence length changes. These results suggest that self-consistency arises as an emergent capability without specifically training for it. Despite this, we find that models are uncalibrated when judging their own consistency, with models displaying both over- and under-confidence. We also propose a nonparametric test for determining from token output distribution whether a model assigns non-trivial probability to alternative answers. Using this test, we find that despite increases in self-consistency, models usually place significant weight on alternative, inconsistent answers. This distribution of probability mass provides evidence that even highly self-consistent models internally compute multiple possible responses.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）无法在不同的上下文中给出一致的答案是问题当用于需要一致性的任务，例如问答、解释等。我们的工作提供了一个评估标准 benchmark for自我一致性在不足规定的情况下，其中有两个或更多的答案都可以是正确的。我们使用OpenAI模型组合进行了一系列的行为实验，包括一个模糊的数字序列完成任务。我们发现，自我一致性的平均值在67%至82%之间，远高于随机的预期，并随模型能力的提高而增加。此外，我们发现模型在多个坚固性检查中保持自我一致性，包括说话人变化和序列长度变化。这些结果表明自我一致性是一个自然而然的能力，不需要具体地训练。尽管如此，我们发现模型对自己的一致性仍然无法准确评估，模型会表现出过度和不足的自信。我们还提出了一个非Parametric测试，用于决定模型从字串输出分布是否将非零概率分配给alternative答案。使用这个测试，我们发现，即使自我一致性提高，模型通常将重要的概率分配给不一致的答案。这个分布的概率质量提供了证据，表明even highly self-consistent models internally compute multiple possible responses。
</details></li>
</ul>
<hr>
<h2 id="Random-Matrix-Analysis-to-Balance-between-Supervised-and-Unsupervised-Learning-under-the-Low-Density-Separation-Assumption"><a href="#Random-Matrix-Analysis-to-Balance-between-Supervised-and-Unsupervised-Learning-under-the-Low-Density-Separation-Assumption" class="headerlink" title="Random Matrix Analysis to Balance between Supervised and Unsupervised Learning under the Low Density Separation Assumption"></a>Random Matrix Analysis to Balance between Supervised and Unsupervised Learning under the Low Density Separation Assumption</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13434">http://arxiv.org/abs/2310.13434</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vasilii Feofanov, Malik Tiomoko, Aladin Virmaux</li>
<li>for: 本研究旨在提出一个理论框架，用于分类下降精度分布假设在高维度情况下。</li>
<li>methods: 我们引入QLDS，一种线性分类模型，在这里实现低概率分离假设通过 quadratic margin maximization。我们提供了可解释的算法，并证明其在某些特定情况下与最小二乘支持向量机、完全无监督分类和半监督图像分类方法相同。</li>
<li>results: 我们使用最新的随机矩阵理论来正式评估分类错误率的数学性评估。此外，我们还提出了一种hyperparameter选择策略，可以找到最佳的超参数，并进行了一系列的示例和实验，证明QLDS在计算效率和分类质量上都有优势，而且可以超越交叉验证。<details>
<summary>Abstract</summary>
We propose a theoretical framework to analyze semi-supervised classification under the low density separation assumption in a high-dimensional regime. In particular, we introduce QLDS, a linear classification model, where the low density separation assumption is implemented via quadratic margin maximization. The algorithm has an explicit solution with rich theoretical properties, and we show that particular cases of our algorithm are the least-square support vector machine in the supervised case, the spectral clustering in the fully unsupervised regime, and a class of semi-supervised graph-based approaches. As such, QLDS establishes a smooth bridge between these supervised and unsupervised learning methods. Using recent advances in the random matrix theory, we formally derive a theoretical evaluation of the classification error in the asymptotic regime. As an application, we derive a hyperparameter selection policy that finds the best balance between the supervised and the unsupervised terms of our learning criterion. Finally, we provide extensive illustrations of our framework, as well as an experimental study on several benchmarks to demonstrate that QLDS, while being computationally more efficient, improves over cross-validation for hyperparameter selection, indicating a high promise of the usage of random matrix theory for semi-supervised model selection.
</details>
<details>
<summary>摘要</summary>
我们提出了一个理论框架，用于分析半监督分类在高维度 Régime 下的 semi-supervised classification。特别是，我们引入QLDS，一种线性分类模型，其中占据低概率分离假设通过quadratic margin maximization进行实现。该算法有显式解，并且我们显示了它的理论性质，并且我们显示了它与其他监督学习方法之间的缓解。使用最近的随机矩阵理论，我们正式地评估分类错误率的极限regime。为此，我们提出了一种hyperparameter选择策略，可以找到我们的学习 criterion 中的最佳平衡。最后，我们提供了广泛的案例研究和一些 benchmark 的实验，以示出QLDS 可以更高效地进行计算，并且可以在 hyperparameter 选择方面超过cross-validation。这表明了随机矩阵理论在 semi-supervised 模型选择方面的高 Promise。
</details></li>
</ul>
<hr>
<h2 id="FLTracer-Accurate-Poisoning-Attack-Provenance-in-Federated-Learning"><a href="#FLTracer-Accurate-Poisoning-Attack-Provenance-in-Federated-Learning" class="headerlink" title="FLTracer: Accurate Poisoning Attack Provenance in Federated Learning"></a>FLTracer: Accurate Poisoning Attack Provenance in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13424">http://arxiv.org/abs/2310.13424</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/eyr3/fltracer">https://github.com/eyr3/fltracer</a></li>
<li>paper_authors: Xinyu Zhang, Qingyu Liu, Zhongjie Ba, Yuan Hong, Tianhang Zheng, Feng Lin, Li Lu, Kui Ren</li>
<li>for: 本研究旨在探讨 Federated Learning (FL) 中的攻击检测方法，以及如何准确地检测不同类型的攻击和恶意更新。</li>
<li>methods: 本研究提出了一种基于 Kalman 约束的跨局回合检测方法，可以准确地检测不同类型的攻击和恶意更新，并且可以适应非独立同分布 (non-IID) 的数据设置。</li>
<li>results: 对比存在的检测方法，本研究的方法可以准确地检测攻击和恶意更新，并且在非独立同分布 (non-IID) 的数据设置下表现出 excel 的性能。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a promising distributed learning approach that enables multiple clients to collaboratively train a shared global model. However, recent studies show that FL is vulnerable to various poisoning attacks, which can degrade the performance of global models or introduce backdoors into them. In this paper, we first conduct a comprehensive study on prior FL attacks and detection methods. The results show that all existing detection methods are only effective against limited and specific attacks. Most detection methods suffer from high false positives, which lead to significant performance degradation, especially in not independent and identically distributed (non-IID) settings. To address these issues, we propose FLTracer, the first FL attack provenance framework to accurately detect various attacks and trace the attack time, objective, type, and poisoned location of updates. Different from existing methodologies that rely solely on cross-client anomaly detection, we propose a Kalman filter-based cross-round detection to identify adversaries by seeking the behavior changes before and after the attack. Thus, this makes it resilient to data heterogeneity and is effective even in non-IID settings. To further improve the accuracy of our detection method, we employ four novel features and capture their anomalies with the joint decisions. Extensive evaluations show that FLTracer achieves an average true positive rate of over $96.88\%$ at an average false positive rate of less than $2.67\%$, significantly outperforming SOTA detection methods. \footnote{Code is available at \url{https://github.com/Eyr3/FLTracer}.}
</details>
<details>
<summary>摘要</summary>
federated 学习（FL）是一种有前途的分布式学习方法，它允许多个客户端共同训练一个共享的全球模型。然而，最近的研究表明，FL 是易受到多种攻击的，这些攻击可以降低全球模型的性能或者引入后门。在这篇论文中，我们首先进行了FL攻击和检测方法的全面研究。结果显示，所有现有的检测方法都只有对特定和有限的攻击有效。大多数检测方法会导致高的假阳性，这会导致非常显著的性能下降，特别是在非独立和相同分布（non-IID）的设置下。为解决这些问题，我们提议FLTracer，FL攻击源架构，它可以准确地检测多种攻击并跟踪攻击时间、目标、类型和毒化的更新。与现有方法不同，我们的方法不仅仅基于客户端间异常检测，而是使用加尔曼滤波器基于跨局检测，以识别对手的行为变化前后。这使得我们的方法能够抗衡数据多样性，并在非独立和相同分布的设置下具有高效性。为了进一步提高我们的检测方法的准确度，我们采用了四个新的特征，并通过联合决策捕捉它们的异常。广泛的评估结果显示，FLTracer 的真正正确率超过 $96.88\%$，假阳性率低于 $2.67\%$，与当前最佳检测方法相比，显著超出。
</details></li>
</ul>
<hr>
<h2 id="AllTogether-Investigating-the-Efficacy-of-Spliced-Prompt-for-Web-Navigation-using-Large-Language-Models"><a href="#AllTogether-Investigating-the-Efficacy-of-Spliced-Prompt-for-Web-Navigation-using-Large-Language-Models" class="headerlink" title="AllTogether: Investigating the Efficacy of Spliced Prompt for Web Navigation using Large Language Models"></a>AllTogether: Investigating the Efficacy of Spliced Prompt for Web Navigation using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18331">http://arxiv.org/abs/2310.18331</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiarun Liu, Wentao Hu, Chunhong Zhang</li>
<li>for: 这个论文主要targets web navigation tasks, using large language models (LLMs) to interpret objectives and interact with web pages.</li>
<li>methods: 该论文提出了一种标准化的提示模板，可以增强任务上下文表示，从而提高 LLMS 在 HTML 基于的网页导航性能。</li>
<li>results: 我们通过提示学习和指令精度调整基于开源 Llama-2 和 API 可用的 GPT 模型，发现 GPT-4 等大型模型在网页导航任务中表现出色，并且 HTML 片段长度和历史轨迹有显著影响性能，而先前的步骤指令不如实时环境反馈有更好的效果。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have emerged as promising agents for web navigation tasks, interpreting objectives and interacting with web pages. However, the efficiency of spliced prompts for such tasks remains underexplored. We introduces AllTogether, a standardized prompt template that enhances task context representation, thereby improving LLMs' performance in HTML-based web navigation. We evaluate the efficacy of this approach through prompt learning and instruction finetuning based on open-source Llama-2 and API-accessible GPT models. Our results reveal that models like GPT-4 outperform smaller models in web navigation tasks. Additionally, we find that the length of HTML snippet and history trajectory significantly influence performance, and prior step-by-step instructions prove less effective than real-time environmental feedback. Overall, we believe our work provides valuable insights for future research in LLM-driven web agents.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经出现为网络浏览任务中有前途的代理人，解释目标和与网页交互。然而，用于这些任务的拼接提示的效率仍未得到足够的探索。我们介绍了AllTogether，一个标准化的提示模板，可以增强任务上下文表示，从而提高LLM在基于HTML的网络浏览中的表现。我们通过提示学习和指令精度调整基于开源Llama-2和可用API的GPT模型进行评估。我们的结果表明，比较大的GPT-4模型在网络浏览任务中表现更好，而且HTML段和历史轨迹的长度对表现有重要影响，而先前的步骤指令比实时环境反馈更有效。总之，我们认为我们的工作对未来LLM驱动的网络代理人做出了重要贡献。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Transfer-Learning-Method-Utilizing-Acoustic-and-Vibration-Signals-for-Rotating-Machinery-Fault-Diagnosis"><a href="#A-Novel-Transfer-Learning-Method-Utilizing-Acoustic-and-Vibration-Signals-for-Rotating-Machinery-Fault-Diagnosis" class="headerlink" title="A Novel Transfer Learning Method Utilizing Acoustic and Vibration Signals for Rotating Machinery Fault Diagnosis"></a>A Novel Transfer Learning Method Utilizing Acoustic and Vibration Signals for Rotating Machinery Fault Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14796">http://arxiv.org/abs/2310.14796</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhongliang Chen, Zhuofei Huang, Wenxiong Kang</li>
<li>for: 这篇论文的目的是提出一种基于声学和振荡信号的错误诊断方法，以解决现有系统中的分布差异问题，提高错误诊断的精度和可靠性。</li>
<li>methods: 本文提出的方法包括设计了声学和振荡特征融合MAVgram，以提供更丰富和可靠的错误信息，并与基于神经网络的分类器结合，实现更有效的错误诊断表现。</li>
<li>results: 实验结果显示，提案的方法可以实现更高的错误诊断性能，并比STgram-MFN更有效。<details>
<summary>Abstract</summary>
Fault diagnosis of rotating machinery plays a important role for the safety and stability of modern industrial systems. However, there is a distribution discrepancy between training data and data of real-world operation scenarios, which causing the decrease of performance of existing systems. This paper proposed a transfer learning based method utilizing acoustic and vibration signal to address this distribution discrepancy. We designed the acoustic and vibration feature fusion MAVgram to offer richer and more reliable information of faults, coordinating with a DNN-based classifier to obtain more effective diagnosis representation. The backbone was pre-trained and then fine-tuned to obtained excellent performance of the target task. Experimental results demonstrate the effectiveness of the proposed method, and achieved improved performance compared to STgram-MFN.
</details>
<details>
<summary>摘要</summary>
扭转机器的故障诊断在现代工业系统中扮演着重要的角色，以保证安全和稳定。然而，现有系统存在训练数据和实际运行场景数据之间的分布差异，导致现有系统的性能下降。这篇论文提出了基于传播学的方法，利用声音和振荡信号来解决这种分布差异。我们设计了声音和振荡特征融合MAVgram，以提供更加丰富和可靠的故障信息，并与基于DNN的分类器结合，以获得更有效的诊断表示。背部先经过训练，然后细化以实现target任务的优秀表现。实验结果表明提案的方法的有效性，并在STgram-MFN的基础上提高了性能。
</details></li>
</ul>
<hr>
<h2 id="POSQA-Probe-the-World-Models-of-LLMs-with-Size-Comparisons"><a href="#POSQA-Probe-the-World-Models-of-LLMs-with-Size-Comparisons" class="headerlink" title="POSQA: Probe the World Models of LLMs with Size Comparisons"></a>POSQA: Probe the World Models of LLMs with Size Comparisons</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13394">http://arxiv.org/abs/2310.13394</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cambridgeltl/posqa">https://github.com/cambridgeltl/posqa</a></li>
<li>paper_authors: Chang Shu, Jiuzhou Han, Fangyu Liu, Ehsan Shareghi, Nigel Collier</li>
<li>for: 验证最新的大语言模型（LLMs）在真实世界中的理解能力</li>
<li>methods: 使用物理对象大小问答数据集（POSQA）进行零基础测试，并使用高级提问技术和外部知识增强</li>
<li>results: 显示最新的LLMs在零基础情况下表现不佳，并且表现受提问形式和对象报告偏见的影响，表明语言模型从文本数据中塑造的理解能力可能受到表单形式的干扰和人类行为的不一致。<details>
<summary>Abstract</summary>
Embodied language comprehension emphasizes that language understanding is not solely a matter of mental processing in the brain but also involves interactions with the physical and social environment. With the explosive growth of Large Language Models (LLMs) and their already ubiquitous presence in our daily lives, it is becoming increasingly necessary to verify their real-world understanding. Inspired by cognitive theories, we propose POSQA: a Physical Object Size Question Answering dataset with simple size comparison questions to examine the extremity and analyze the potential mechanisms of the embodied comprehension of the latest LLMs.   We show that even the largest LLMs today perform poorly under the zero-shot setting. We then push their limits with advanced prompting techniques and external knowledge augmentation. Furthermore, we investigate whether their real-world comprehension primarily derives from contextual information or internal weights and analyse the impact of prompt formats and report bias of different objects. Our results show that real-world understanding that LLMs shaped from textual data can be vulnerable to deception and confusion by the surface form of prompts, which makes it less aligned with human behaviours.
</details>
<details>
<summary>摘要</summary>
研究人员强调体验语言理解，表明语言理解不仅是脑中的心理处理，还与物理环境和社会环境交互有关。随着大语言模型（LLMs）的快速发展和日常生活中的普遍存在，我们必须更加重视他们在实际情况下的理解能力。以聪明认知理论为引导，我们提出了POSQA：一个包含简单的大小比较问题的物理对象大小问答集，以探索最新的LLMs的具体实现和分析其物理语言理解机制。我们发现，当前最大的LLMs在零情况下表现很差。然后，我们使用高级推荐技术和外部知识增强。此外，我们还研究了LLMs的实际理解是否主要来自于上下文信息或内部权重，并分析了提问格式和对象报告偏见的影响。我们的结果表明，由文本数据塑造的LLMs可能因提问表现的表面形式而受到欺骗和混乱，这使其更加与人类行为不一致。
</details></li>
</ul>
<hr>
<h2 id="Learning-Successor-Representations-with-Distributed-Hebbian-Temporal-Memory"><a href="#Learning-Successor-Representations-with-Distributed-Hebbian-Temporal-Memory" class="headerlink" title="Learning Successor Representations with Distributed Hebbian Temporal Memory"></a>Learning Successor Representations with Distributed Hebbian Temporal Memory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13391">http://arxiv.org/abs/2310.13391</a></li>
<li>repo_url: None</li>
<li>paper_authors: Evgenii Dzhivelikian, Petr Kuderov, Aleksandr I. Panov</li>
<li>for: address the challenge of online hidden representation learning for decision-making under uncertainty in non-stationary, partially observable environments.</li>
<li>methods: based on factor graph formalism and a multicomponent neuron model, using distributed representations, sparse transition matrices, and local Hebbian-like learning rules.</li>
<li>results: outperforms classical LSTM and performs comparably to more advanced RNN-like algorithms, speeding up Temporal Difference learning for Successor Representation in changing environments.<details>
<summary>Abstract</summary>
This paper presents a novel approach to address the challenge of online hidden representation learning for decision-making under uncertainty in non-stationary, partially observable environments. The proposed algorithm, Distributed Hebbian Temporal Memory (DHTM), is based on factor graph formalism and a multicomponent neuron model. DHTM aims to capture sequential data relationships and make cumulative predictions about future observations, forming Successor Representation (SR). Inspired by neurophysiological models of the neocortex, the algorithm utilizes distributed representations, sparse transition matrices, and local Hebbian-like learning rules to overcome the instability and slow learning process of traditional temporal memory algorithms like RNN and HMM. Experimental results demonstrate that DHTM outperforms classical LSTM and performs comparably to more advanced RNN-like algorithms, speeding up Temporal Difference learning for SR in changing environments. Additionally, we compare the SRs produced by DHTM to another biologically inspired HMM-like algorithm, CSCG. Our findings suggest that DHTM is a promising approach for addressing the challenges of online hidden representation learning in dynamic environments.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Human-Robot-Mutual-Learning-System-with-Affect-Grounded-Language-Acquisition-and-Differential-Outcomes-Training"><a href="#A-Human-Robot-Mutual-Learning-System-with-Affect-Grounded-Language-Acquisition-and-Differential-Outcomes-Training" class="headerlink" title="A Human-Robot Mutual Learning System with Affect-Grounded Language Acquisition and Differential Outcomes Training"></a>A Human-Robot Mutual Learning System with Affect-Grounded Language Acquisition and Differential Outcomes Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13377">http://arxiv.org/abs/2310.13377</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alva Markelius, Sofia Sjöberg, Zakaria Lemhauori, Laura Cohen, Martin Bergström, Robert Lowe, Lola Cañamero</li>
<li>for: 这个论文旨在探讨一种人机交互设置，以便人类和机器人共同学习符号语言，用于识别机器人的内部需求。</li>
<li>methods: 这个研究采用了一种差异性结果培训（DOT）协议，以便机器人对自己的内部需求（如饿）提供特定的反馈，并且人类通过正确的刺激（如 cookie）来响应机器人的需求。</li>
<li>results: 研究发现，在DOT协议下，人类的学习效率提高，并且可以更有效地学习机器人的语言。机器人在这个研究中使用了一个类似于人类婴儿语言发展阶段的词汇表。机器人的软件架构基于一种对情感相关的语言学习模型，将机器人的词汇与内部需求相关联。研究发现，在DOT conditon下，机器人的语言学习速度比非DOT控制condition更快 converges。参与者还报告了正面的情感体验、感到控制和与机器人之间的共鸣连接。这种教师-学生学习方法可能为增强治疗效果（如对偏抑郁症的治疗）做出贡献，通过增加人类在训练任务中的参与度，从而提高治疗效果。机器人的家OSTATIC需求启发的语言学习具有潜在的社会化和合作（教育）功能，可能为人类与机器人之间的交互带来更多的有用和满足。<details>
<summary>Abstract</summary>
This paper presents a novel human-robot interaction setup for robot and human learning of symbolic language for identifying robot homeostatic needs. The robot and human learn to use and respond to the same language symbols that convey homeostatic needs and the stimuli that satisfy the homeostatic needs, respectively. We adopted a differential outcomes training (DOT) protocol whereby the robot provides feedback specific (differential) to its internal needs (e.g. `hunger') when satisfied by the correct stimulus (e.g. cookie). We found evidence that DOT can enhance the human's learning efficiency, which in turn enables more efficient robot language acquisition. The robot used in the study has a vocabulary similar to that of a human infant in the linguistic ``babbling'' phase. The robot software architecture is built upon a model for affect-grounded language acquisition where the robot associates vocabulary with internal needs (hunger, thirst, curiosity) through interactions with the human. The paper presents the results of an initial pilot study conducted with the interactive setup, which reveal that the robot's language acquisition achieves higher convergence rate in the DOT condition compared to the non-DOT control condition. Additionally, participants reported positive affective experiences, feeling of being in control, and an empathetic connection with the robot. This mutual learning (teacher-student learning) approach offers a potential contribution of facilitating cognitive interventions with DOT (e.g. for people with dementia) through increased therapy adherence as a result of engaging humans more in training tasks by taking an active teaching-learning role. The homeostatic motivational grounding of the robot's language acquisition has potential to contribute to more ecologically valid and social (collaborative/nurturing) interactions with robots.
</details>
<details>
<summary>摘要</summary>
这篇论文描述了一种新的人机交互设置，用于机器人和人类学习符号语言，以便识别机器人的内部需求。机器人和人类都学习了使用和响应同一种语言符号，表示内部需求和满足需求的刺激。我们采用了一种差分结果培训（DOT）协议，其中机器人提供特定的反馈（差分），以满足其内部需求（如快餐）。我们发现，DOT可以加强人类学习效率，从而使机器人语言学习更加高效。机器人在研究中使用的词汇数量与人类婴儿语言发展阶段相似。机器人软件架构基于语言学习模型，其中机器人通过与人类的互动关系 vocabulary 与内部需求（快餐、喝彩、好奇）相联系。研究发现，在DOT条件下，机器人语言学习具有更高的吞吐率，而控制条件下则相对较低。此外，参与者报告了正面的情感体验、感到控制和与机器人之间的共鸣。这种教师学生学习方法可能为增强治疗效率而做出贡献，例如用于人们的诱导疗法（如偏 wurlitzer 症），通过增加人类在训练任务中的活跃参与，从而提高治疗效率。机器人的内部需求驱动的语言学习有助于实现更加生动化和社交（合作/善父）的机器人交互。
</details></li>
</ul>
<hr>
<h2 id="VFedMH-Vertical-Federated-Learning-for-Training-Multi-party-Heterogeneous-Models"><a href="#VFedMH-Vertical-Federated-Learning-for-Training-Multi-party-Heterogeneous-Models" class="headerlink" title="VFedMH: Vertical Federated Learning for Training Multi-party Heterogeneous Models"></a>VFedMH: Vertical Federated Learning for Training Multi-party Heterogeneous Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13367">http://arxiv.org/abs/2310.13367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuo Wang, Keke Gai, Jing Yu, Liehuang Zhu<br>for:This paper proposes a novel approach called Vertical Federated learning for training Multi-parties Heterogeneous models (VFedMH) to address the challenges of heterogeneous local models among participants in existing VFL methods.methods:The approach focuses on aggregating the embeddings of each participant’s knowledge instead of intermediate results during forward propagation. The active party securely aggregates local embeddings to obtain global knowledge embeddings and sends them to passive parties, who then utilize the global embeddings to propagate forward on their local heterogeneous networks. The active party assists the passive party in computing its local heterogeneous model gradients.results:The paper demonstrates that VFedMH can simultaneously train multiple heterogeneous models with heterogeneous optimization and outperform some recent methods in model performance. The paper also provides a theoretical analysis of VFedMH’s convergence performance.<details>
<summary>Abstract</summary>
Vertical Federated Learning (VFL) has gained increasing attention as a novel training paradigm that integrates sample alignment and feature union. However, existing VFL methods face challenges when dealing with heterogeneous local models among participants, which affects optimization convergence and generalization. To address this issue, this paper proposes a novel approach called Vertical Federated learning for training Multi-parties Heterogeneous models (VFedMH). VFedMH focuses on aggregating the embeddings of each participant's knowledge instead of intermediate results during forward propagation. The active party, who possesses labels and features of the sample, in VFedMH securely aggregates local embeddings to obtain global knowledge embeddings, and sends them to passive parties. The passive parties, who own only features of the sample, then utilize the global embeddings to propagate forward on their local heterogeneous networks. However, the passive party does not own the labels, so the local model gradient cannot be calculated locally. To overcome this limitation, the active party assists the passive party in computing its local heterogeneous model gradients. Then, each participant trains their local model using the heterogeneous model gradients. The objective is to minimize the loss value of their respective local heterogeneous models. Additionally, the paper provides a theoretical analysis of VFedMH's convergence performance. Extensive experiments are conducted to demonstrate that VFedMH can simultaneously train multiple heterogeneous models with heterogeneous optimization and outperform some recent methods in model performance.
</details>
<details>
<summary>摘要</summary>
垂直联邦学习（VFL）在最近几年中得到了越来越多的关注，这是一种新的训练方法，它将把样本Alignment和特征Union结合在一起。然而，现有的VFL方法在参与者之间存在不同的本地模型，这会影响优化征程和泛化性。为了解决这个问题，这篇论文提出了一种新的方法，即Vertically Federated Learning for training Multi-parties Heterogeneous models（VFedMH）。VFedMH通过在每个参与者的知识嵌入上进行聚合来取代传输中间结果的方法。活动参与者，即拥有标签和样本特征的方，在VFedMH中安全地聚合本地嵌入，并将其发送到抗拒参与者。抗拒参与者，即拥有样本特征但没有标签的方，然后使用全球嵌入来在本地不同的网络上进行前进传播。然而，抗拒参与者没有标签，因此本地模型梯度无法计算本地。为了解决这个问题，活动参与者为抗拒参与者计算本地不同模型梯度。然后，每个参与者使用自己的本地模型梯度来训练自己的本地模型，目标是将本地模型的损失值最小化。此外，论文还提供了VFL的准确性性分析。广泛的实验表明，VFedMH可以同时训练多个不同的模型，并且在模型性能方面超越一些最新的方法。
</details></li>
</ul>
<hr>
<h2 id="Towards-General-Error-Diagnosis-via-Behavioral-Testing-in-Machine-Translation"><a href="#Towards-General-Error-Diagnosis-via-Behavioral-Testing-in-Machine-Translation" class="headerlink" title="Towards General Error Diagnosis via Behavioral Testing in Machine Translation"></a>Towards General Error Diagnosis via Behavioral Testing in Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13362">http://arxiv.org/abs/2310.13362</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wujunjie1998/btpgbt">https://github.com/wujunjie1998/btpgbt</a></li>
<li>paper_authors: Junjie Wu, Lemao Liu, Dit-Yan Yeung</li>
<li>for: 本研究旨在提供一种基于行为测试的机器翻译系统诊断方法，以检测机器翻译系统的通用错误。</li>
<li>methods: 本研究提出了一种新的双语翻译对生成基于行为测试（BTPGBT）框架，通过自动生成高质量测试 случа和 Pseudoreferences，以便对机器翻译系统进行全面和准确的行为测试。</li>
<li>results: 实验结果表明，BTPGBT 可以为机器翻译系统提供全面和准确的行为测试结果，并提供了一些有趣的发现。codes和数据可以在 https: &#x2F;&#x2F;github.com&#x2F;wujunjie1998&#x2F;BTPGBT 上下载。<details>
<summary>Abstract</summary>
Behavioral testing offers a crucial means of diagnosing linguistic errors and assessing capabilities of NLP models. However, applying behavioral testing to machine translation (MT) systems is challenging as it generally requires human efforts to craft references for evaluating the translation quality of such systems on newly generated test cases. Existing works in behavioral testing of MT systems circumvent this by evaluating translation quality without references, but this restricts diagnosis to specific types of errors, such as incorrect translation of single numeric or currency words. In order to diagnose general errors, this paper proposes a new Bilingual Translation Pair Generation based Behavior Testing (BTPGBT) framework for conducting behavioral testing of MT systems. The core idea of BTPGBT is to employ a novel bilingual translation pair generation (BTPG) approach that automates the construction of high-quality test cases and their pseudoreferences. Experimental results on various MT systems demonstrate that BTPGBT could provide comprehensive and accurate behavioral testing results for general error diagnosis, which further leads to several insightful findings. Our code and data are available at https: //github.com/wujunjie1998/BTPGBT.
</details>
<details>
<summary>摘要</summary>
行为测试提供了诊断语言错误和评估自然语言处理器（NLP）模型能力的重要方式。但在机器翻译（MT）系统上进行行为测试是具有挑战性，因为通常需要人工劳动来制定评估MT系统翻译质量的参考。现有的MT系统行为测试方法通过不使用参考来评估翻译质量，但这限定了诊断的类型为单个数字或货币词的错误。为了诊断通用错误，本文提出了一种新的行为测试框架（BTPGBT），基于自动生成高质量测试用例和其 Pseudoreferences的翻译对。实验结果表明，BTPGBT可以提供全面和准确的行为测试结果，用于普遍错误诊断，并且导致了一些有价值的发现。我们的代码和数据可以在https: //github.com/wujunjie1998/BTPGBT上获取。
</details></li>
</ul>
<hr>
<h2 id="Bridging-the-Gap-between-Synthetic-and-Authentic-Images-for-Multimodal-Machine-Translation"><a href="#Bridging-the-Gap-between-Synthetic-and-Authentic-Images-for-Multimodal-Machine-Translation" class="headerlink" title="Bridging the Gap between Synthetic and Authentic Images for Multimodal Machine Translation"></a>Bridging the Gap between Synthetic and Authentic Images for Multimodal Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13361">http://arxiv.org/abs/2310.13361</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ictnlp/SAMMT">https://github.com/ictnlp/SAMMT</a></li>
<li>paper_authors: Wenyu Guo, Qingkai Fang, Dong Yu, Yang Feng</li>
<li>for:  simultaneous machine translation and image input</li>
<li>methods:  using powerful text-to-image generation models and minimizing the gap between synthetic and authentic image representations</li>
<li>results:  achieving state-of-the-art performance on Multi30K En-De and En-Fr datasets while remaining independent of authentic images during inference<details>
<summary>Abstract</summary>
Multimodal machine translation (MMT) simultaneously takes the source sentence and a relevant image as input for translation. Since there is no paired image available for the input sentence in most cases, recent studies suggest utilizing powerful text-to-image generation models to provide image inputs. Nevertheless, synthetic images generated by these models often follow different distributions compared to authentic images. Consequently, using authentic images for training and synthetic images for inference can introduce a distribution shift, resulting in performance degradation during inference. To tackle this challenge, in this paper, we feed synthetic and authentic images to the MMT model, respectively. Then we minimize the gap between the synthetic and authentic images by drawing close the input image representations of the Transformer Encoder and the output distributions of the Transformer Decoder. Therefore, we mitigate the distribution disparity introduced by the synthetic images during inference, thereby freeing the authentic images from the inference process.Experimental results show that our approach achieves state-of-the-art performance on the Multi30K En-De and En-Fr datasets, while remaining independent of authentic images during inference.
</details>
<details>
<summary>摘要</summary>
多模态机器翻译（MMT）同时接受源句子和相关的图像作为翻译输入。由于翻译输入句子中的图像 rarely available，recent studies suggest 使用强大的文本到图像生成模型提供图像输入。然而，由这些模型生成的图像经常遵循不同的分布，从而导致在推理过程中的分布偏移，从而降低翻译性能。为解决这个挑战，在这篇论文中，我们将Feed synthetic和authentic图像给MMT模型，然后将synthetic和authentic图像的输入图像表示和Transformer Encoder的输出分布减小到最小值。因此，我们消除了在推理过程中由synthetic图像引入的分布偏移，使得authentic图像可以在推理过程中自由发挥作用。实验结果显示，我们的方法在Multi30K En-De和En-Fr数据集上实现了状态的翻译性能，同时不依赖于authentic图像进行推理。
</details></li>
</ul>
<hr>
<h2 id="DYNAMITE-Dynamic-Interplay-of-Mini-Batch-Size-and-Aggregation-Frequency-for-Federated-Learning-with-Static-and-Streaming-Dataset"><a href="#DYNAMITE-Dynamic-Interplay-of-Mini-Batch-Size-and-Aggregation-Frequency-for-Federated-Learning-with-Static-and-Streaming-Dataset" class="headerlink" title="DYNAMITE: Dynamic Interplay of Mini-Batch Size and Aggregation Frequency for Federated Learning with Static and Streaming Dataset"></a>DYNAMITE: Dynamic Interplay of Mini-Batch Size and Aggregation Frequency for Federated Learning with Static and Streaming Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14906">http://arxiv.org/abs/2310.14906</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijie Liu, Xiaoxi Zhang, Jingpu Duan, Carlee Joe-Wong, Zhi Zhou, Xu Chen</li>
<li>for: This paper focuses on analyzing the joint effects of adjusting batch size and aggregation frequency on model performance, training time, and resource consumption in federated learning (FL) training, especially when facing dynamic data streams and network characteristics.</li>
<li>methods: The paper introduces novel analytical models and optimization algorithms that leverage the interplay between batch size and aggregation frequency to navigate the trade-offs among convergence, cost, and completion time for dynamic FL training. The paper also derives closed-form solutions for co-optimized batch size and aggregation frequency that are consistent across all devices.</li>
<li>results: The paper conducts extensive experiments to demonstrate the superiority of the proposed offline optimal solutions and online adaptive algorithm. The results show that the proposed methods can efficiently train accurate FL models while addressing the heterogeneity of both data and system characteristics.<details>
<summary>Abstract</summary>
Federated Learning (FL) is a distributed learning paradigm that can coordinate heterogeneous edge devices to perform model training without sharing private data. While prior works have focused on analyzing FL convergence with respect to hyperparameters like batch size and aggregation frequency, the joint effects of adjusting these parameters on model performance, training time, and resource consumption have been overlooked, especially when facing dynamic data streams and network characteristics. This paper introduces novel analytical models and optimization algorithms that leverage the interplay between batch size and aggregation frequency to navigate the trade-offs among convergence, cost, and completion time for dynamic FL training. We establish a new convergence bound for training error considering heterogeneous datasets across devices and derive closed-form solutions for co-optimized batch size and aggregation frequency that are consistent across all devices. Additionally, we design an efficient algorithm for assigning different batch configurations across devices, improving model accuracy and addressing the heterogeneity of both data and system characteristics. Further, we propose an adaptive control algorithm that dynamically estimates network states, efficiently samples appropriate data batches, and effectively adjusts batch sizes and aggregation frequency on the fly. Extensive experiments demonstrate the superiority of our offline optimal solutions and online adaptive algorithm.
</details>
<details>
<summary>摘要</summary>
联合学习（FL）是一种分布式学习模式，可以在不同的边缘设备上进行模型训练，无需分享private数据。而且，以前的研究主要关注了FL的整合程度和批处理频率之间的关系，而忽略了在面对动态数据流和网络特性时，这些参数的共同影响。本文提出了新的分析模型和优化算法，通过批处理大小和汇集频率之间的交互来导航模型性能、训练时间和资源消耗之间的负面oren。我们提出了一个新的训练误差下界，考虑到不同设备上的hetogeneous数据集，并 deriv出closed-form解决方案，可以在所有设备上实现共同的批处理大小和汇集频率。此外，我们设计了一种高效的分配不同批处理配置的算法，以提高模型精度和处理不同数据和系统特性的hetogeneity。最后，我们提出了一种自适应控制算法，可以在 fly 上精准地估算网络状态，选择合适的数据批处理，并动态地调整批处理大小和汇集频率。广泛的实验表明了我们的offline优化解决方案和在线自适应算法的优越性。
</details></li>
</ul>
<hr>
<h2 id="NurViD-A-Large-Expert-Level-Video-Database-for-Nursing-Procedure-Activity-Understanding"><a href="#NurViD-A-Large-Expert-Level-Video-Database-for-Nursing-Procedure-Activity-Understanding" class="headerlink" title="NurViD: A Large Expert-Level Video Database for Nursing Procedure Activity Understanding"></a>NurViD: A Large Expert-Level Video Database for Nursing Procedure Activity Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13347">http://arxiv.org/abs/2310.13347</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/minghu0830/NurViD-benchmark">https://github.com/minghu0830/NurViD-benchmark</a></li>
<li>paper_authors: Ming Hu, Lin Wang, Siyuan Yan, Don Ma, Qingli Ren, Peng Xia, Wei Feng, Peibo Duan, Lie Ju, Zongyuan Ge</li>
<li>for: 这个论文旨在提高护理活动理解的质量和安全性，通过应用深度学习技术，促进教育和培训，改善质量控制，并启用操作符控制监测。</li>
<li>methods: 这个论文使用的方法包括使用深度学习技术进行护理活动理解，并提供了一个大型视频数据集（NurViD），其包含了51种护理程序和177个动作步骤的专家级标注。</li>
<li>results: 这个论文的结果显示，使用现有的深度学习方法在护理活动理解方面的效果不佳，而 NurViD 数据集可以帮助改善这种效果。<details>
<summary>Abstract</summary>
The application of deep learning to nursing procedure activity understanding has the potential to greatly enhance the quality and safety of nurse-patient interactions. By utilizing the technique, we can facilitate training and education, improve quality control, and enable operational compliance monitoring. However, the development of automatic recognition systems in this field is currently hindered by the scarcity of appropriately labeled datasets. The existing video datasets pose several limitations: 1) these datasets are small-scale in size to support comprehensive investigations of nursing activity; 2) they primarily focus on single procedures, lacking expert-level annotations for various nursing procedures and action steps; and 3) they lack temporally localized annotations, which prevents the effective localization of targeted actions within longer video sequences. To mitigate these limitations, we propose NurViD, a large video dataset with expert-level annotation for nursing procedure activity understanding. NurViD consists of over 1.5k videos totaling 144 hours, making it approximately four times longer than the existing largest nursing activity datasets. Notably, it encompasses 51 distinct nursing procedures and 177 action steps, providing a much more comprehensive coverage compared to existing datasets that primarily focus on limited procedures. To evaluate the efficacy of current deep learning methods on nursing activity understanding, we establish three benchmarks on NurViD: procedure recognition on untrimmed videos, procedure and action recognition on trimmed videos, and action detection. Our benchmark and code will be available at \url{https://github.com/minghu0830/NurViD-benchmark}.
</details>
<details>
<summary>摘要</summary>
使用深度学习对护理程序活动理解可能会大幅提高护理人员和病人之间的质量和安全性。通过这种技术，我们可以提供培训和教育，提高质量控制，并启用操作符合性监测。然而，在这一领域的自动识别系统开发目前受到数据鲜血的限制。现有的视频数据集存在多种限制：1）这些数据集较小，无法支持全面的护理活动调查; 2）它们主要关注单一的程序，缺乏专家级别的护理程序和操作步骤的标注; 3）它们缺乏时间地标注，这使得targeted action在更长的视频序列中不能有效地local化。为了缓解这些限制，我们提出NurViD，一个大型视频数据集，包含专家级别的护理程序活动理解标注。NurViD包含1.5k个视频，总时长144小时，比现有最大的护理活动数据集长得多。其中包含51种不同的护理程序和177个操作步骤，比现有数据集更加全面。为了评估当前深度学习方法在护理活动理解方面的效果，我们建立了三个benchmark在NurViD上：程序认知在未处理视频上，程序和操作认知在处理视频上，以及操作检测。我们的benchmark和代码将在GitHub上公开，请参阅\url{https://github.com/minghu0830/NurViD-benchmark}.
</details></li>
</ul>
<hr>
<h2 id="Challenges-and-Contributing-Factors-in-the-Utilization-of-Large-Language-Models-LLMs"><a href="#Challenges-and-Contributing-Factors-in-the-Utilization-of-Large-Language-Models-LLMs" class="headerlink" title="Challenges and Contributing Factors in the Utilization of Large Language Models (LLMs)"></a>Challenges and Contributing Factors in the Utilization of Large Language Models (LLMs)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13343">http://arxiv.org/abs/2310.13343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoliang Chen, Liangbin Li, Le Chang, Yunhe Huang, Yuxuan Zhao, Yuxiao Zhang, Dinuo Li</li>
<li>for: 本文探讨了大语言模型（LLM）在各种应用场景中的挑战，包括域域特性、知识忘记、知识重复、知识虚假等问题。</li>
<li>methods: 本文提出了多种解决这些问题的方法，包括数据多样化、模型细化、可见性和解释性提高、优化模型、增加优化和公平性训练等。</li>
<li>results: 本文预测未来的LLM将强调公平、透明度和优素，以保证在服务人类时保持高的道德和伦理水平。<details>
<summary>Abstract</summary>
With the development of large language models (LLMs) like the GPT series, their widespread use across various application scenarios presents a myriad of challenges. This review initially explores the issue of domain specificity, where LLMs may struggle to provide precise answers to specialized questions within niche fields. The problem of knowledge forgetting arises as these LLMs might find it hard to balance old and new information. The knowledge repetition phenomenon reveals that sometimes LLMs might deliver overly mechanized responses, lacking depth and originality. Furthermore, knowledge illusion describes situations where LLMs might provide answers that seem insightful but are actually superficial, while knowledge toxicity focuses on harmful or biased information outputs. These challenges underscore problems in the training data and algorithmic design of LLMs. To address these issues, it's suggested to diversify training data, fine-tune models, enhance transparency and interpretability, and incorporate ethics and fairness training. Future technological trends might lean towards iterative methodologies, multimodal learning, model personalization and customization, and real-time learning and feedback mechanisms. In conclusion, future LLMs should prioritize fairness, transparency, and ethics, ensuring they uphold high moral and ethical standards when serving humanity.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）如GPT系列的发展，在各种应用场景中广泛使用，却也带来许多挑战。本文首先探讨领域特定性问题， LLM 可能难以回答特殊领域内的精细问题。知识忘却问题表明这些 LLM 可能很难以平衡旧和新信息。知识重复现象表明 LLM 可能提供过于机械化的答案，缺乏深度和创新。此外，知识幻觉问题描述了 LLM 可能提供的答案似乎具有深度和创新，但实际上是 superficiel 的。知识毒性问题则关注 LLM 输出的有害或偏见信息。这些挑战表明 LLM 的训练数据和算法设计存在问题。为解决这些问题，建议多样化训练数据，细化模型，提高透明度和可解释性，并包括伦理和公平训练。未来技术趋势可能是迭代方法、多模态学习、模型个性化和定制化，以及实时学习和反馈机制。在结束时，未来 LLM 应该优先考虑公平、透明度和伦理，以确保它们在服务人类时保持高的道德和伦理标准。
</details></li>
</ul>
<hr>
<h2 id="FLAIR-a-Country-Scale-Land-Cover-Semantic-Segmentation-Dataset-From-Multi-Source-Optical-Imagery"><a href="#FLAIR-a-Country-Scale-Land-Cover-Semantic-Segmentation-Dataset-From-Multi-Source-Optical-Imagery" class="headerlink" title="FLAIR: a Country-Scale Land Cover Semantic Segmentation Dataset From Multi-Source Optical Imagery"></a>FLAIR: a Country-Scale Land Cover Semantic Segmentation Dataset From Multi-Source Optical Imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13336">http://arxiv.org/abs/2310.13336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anatol Garioud, Nicolas Gonthier, Loic Landrieu, Apolline De Wit, Marion Valette, Marc Poupée, Sébastien Giordano, Boris Wattrelos</li>
<li>for: FLAIR 是一个大规模地理分类的数据集，用于监测和理解人类活动的发展指标，如城市化、扫林和土壤人工化。</li>
<li>methods: FLAIR 使用高分辨率遥感图像和时间序列数据，并提供了精确的地表类型分类。</li>
<li>results: FLAIR 提供了817平方公里的高分辨率遥感图像，以及20亿个分类标签，可以用于开发和评估大规模地理分类算法。<details>
<summary>Abstract</summary>
We introduce the French Land cover from Aerospace ImageRy (FLAIR), an extensive dataset from the French National Institute of Geographical and Forest Information (IGN) that provides a unique and rich resource for large-scale geospatial analysis. FLAIR contains high-resolution aerial imagery with a ground sample distance of 20 cm and over 20 billion individually labeled pixels for precise land-cover classification. The dataset also integrates temporal and spectral data from optical satellite time series. FLAIR thus combines data with varying spatial, spectral, and temporal resolutions across over 817 km2 of acquisitions representing the full landscape diversity of France. This diversity makes FLAIR a valuable resource for the development and evaluation of novel methods for large-scale land-cover semantic segmentation and raises significant challenges in terms of computer vision, data fusion, and geospatial analysis. We also provide powerful uni- and multi-sensor baseline models that can be employed to assess algorithm's performance and for downstream applications. Through its extent and the quality of its annotation, FLAIR aims to spur improvements in monitoring and understanding key anthropogenic development indicators such as urban growth, deforestation, and soil artificialization. Dataset and codes can be accessed at https://ignf.github.io/FLAIR/
</details>
<details>
<summary>摘要</summary>
我们介绍法国陆地覆盖物（FLAIR），一个广泛的数据集来自法国国家地理和森林信息研究所（IGN），提供了一个独特和丰富的大规模地ospatial分析资源。FLAIR包含高分辨率飞行图像，地面抽象距离20 cm，超过20亿个准确标注的像素，用于精确的陆地覆盖类别分类。数据集还 integraoptical卫星时序序数据。因此，FLAIR结合了不同的空间、spectral和时间分辨率，覆盖了法国的全景多样性，总面积超过817 km2。这种多样性使FLAIR成为大规模陆地Semantic分类的开发和评估的丰富资源，同时也提出了计算机视觉、数据融合和地ospatial分析的挑战。我们还提供了强大的单感器和多感器基线模型，可以用于评估算法性能和下游应用。通过其覆盖范围和精确的标注，FLAIR希望能促进跟踪和理解人类发展指标，如城市增长、Deforestation和 soil artificialization。数据集和代码可以在https://ignf.github.io/FLAIR/上获取。
</details></li>
</ul>
<hr>
<h2 id="Democratizing-Reasoning-Ability-Tailored-Learning-from-Large-Language-Model"><a href="#Democratizing-Reasoning-Ability-Tailored-Learning-from-Large-Language-Model" class="headerlink" title="Democratizing Reasoning Ability: Tailored Learning from Large Language Model"></a>Democratizing Reasoning Ability: Tailored Learning from Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13332">http://arxiv.org/abs/2310.13332</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/raibows/learn-to-reason">https://github.com/raibows/learn-to-reason</a></li>
<li>paper_authors: Zhaoyang Wang, Shaohan Huang, Yuxuan Liu, Jiahai Wang, Minghui Song, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang</li>
<li>for: 提高小型语言模型（LLM）的推理能力，使其能够更广泛应用于自然语言处理领域。</li>
<li>methods: 提出了一种特化的学习方法，通过在多轮学习 paradigm 中，将大型黑盒语言模型（LLM）作为教师，为小型语言模型（LM）提供个性化的培训数据，以提高LM的推理能力。同时，通过自我反思学习，让学生从自己的错误中学习。</li>
<li>results: 经过实验和分析，表明该方法可以有效提高小型语言模型的推理能力，并且可以在数学和常识理解任务上达到比较高的性能。<details>
<summary>Abstract</summary>
Large language models (LLMs) exhibit impressive emergent abilities in natural language processing, but their democratization is hindered due to huge computation requirements and closed-source nature. Recent research on advancing open-source smaller LMs by distilling knowledge from black-box LLMs has obtained promising results in the instruction-following ability. However, the reasoning ability which is more challenging to foster, is relatively rarely explored. In this paper, we propose a tailored learning approach to distill such reasoning ability to smaller LMs to facilitate the democratization of the exclusive reasoning ability. In contrast to merely employing LLM as a data annotator, we exploit the potential of LLM as a reasoning teacher by building an interactive multi-round learning paradigm. This paradigm enables the student to expose its deficiencies to the black-box teacher who then can provide customized training data in return. Further, to exploit the reasoning potential of the smaller LM, we propose self-reflection learning to motivate the student to learn from self-made mistakes. The learning from self-reflection and LLM are all tailored to the student's learning status, thanks to the seamless integration with the multi-round learning paradigm. Comprehensive experiments and analysis on mathematical and commonsense reasoning tasks demonstrate the effectiveness of our method. The code will be available at https://github.com/Raibows/Learn-to-Reason.
</details>
<details>
<summary>摘要</summary>
大型自然语言处理模型（LLM）表现出印象的emergent能力，但其普及化受到巨大计算需求和封闭式 natura 的阻碍。现代研究推进小型开源LLM的进步，通过将黑盒LLM知识蒸馏到小型LLM中，已经获得了惊人的成果。然而，更加困难的是 fostering 的逻辑能力，通常不会被探索。在本文中，我们提出了一个适应学习方法，以将这种逻辑能力蒸馏到小型LLM中，以便普及化这种专有的逻辑能力。相比使用LLM作为标签生成器，我们利用LLM的启发力作为教育师，建立了互动多轮学习 paradigm。这个 paradigm 让学生可以向黑盒教育师表达自己的不足，并且获得专门的训练数据回应。此外，为了套用小型LLM的逻辑潜力，我们提出了自我反思学习，让学生从自己的错误中学习。这些学习自我反思和LLM都是根据学习状态进行定制，感谢与多轮学习 paradigm 的整合。实验和分析显示了我们的方法的有效性，代码将会在 GitHub 上公开。
</details></li>
</ul>
<hr>
<h2 id="Boosting-for-Bounding-the-Worst-class-Error"><a href="#Boosting-for-Bounding-the-Worst-class-Error" class="headerlink" title="Boosting for Bounding the Worst-class Error"></a>Boosting for Bounding the Worst-class Error</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14890">http://arxiv.org/abs/2310.14890</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuya Saito, Shinnosuke Matsuo, Seiichi Uchida, Daiki Suehiro</li>
<li>for: 本文解决了最坏类错误率问题，而不是通过平均误差率来衡量所有类的性能。</li>
<li>methods: 本文提出了一种提高算法，该算法可以确保最坏类训练误差的Upper bound，并 deriv出其泛化 bound。</li>
<li>results: 实验结果显示，该算法可以降低测试集最坏类误差率，而不会过拟合训练集。<details>
<summary>Abstract</summary>
This paper tackles the problem of the worst-class error rate, instead of the standard error rate averaged over all classes. For example, a three-class classification task with class-wise error rates of 10\%, 10\%, and 40\% has a worst-class error rate of 40\%, whereas the average is 20\% under the class-balanced condition. The worst-class error is important in many applications. For example, in a medical image classification task, it would not be acceptable for the malignant tumor class to have a 40\% error rate, while the benign and healthy classes have 10\% error rates.We propose a boosting algorithm that guarantees an upper bound of the worst-class training error and derive its generalization bound. Experimental results show that the algorithm lowers worst-class test error rates while avoiding overfitting to the training set.
</details>
<details>
<summary>摘要</summary>
We propose a boosting algorithm that guarantees an upper bound of the worst-class training error and derive its generalization bound. Experimental results show that the algorithm reduces worst-class test error rates while avoiding overfitting to the training set.
</details></li>
</ul>
<hr>
<h2 id="Coarse-to-Fine-Dual-Encoders-are-Better-Frame-Identification-Learners"><a href="#Coarse-to-Fine-Dual-Encoders-are-Better-Frame-Identification-Learners" class="headerlink" title="Coarse-to-Fine Dual Encoders are Better Frame Identification Learners"></a>Coarse-to-Fine Dual Encoders are Better Frame Identification Learners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13316">http://arxiv.org/abs/2310.13316</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pkunlp-icler/cofftea">https://github.com/pkunlp-icler/cofftea</a></li>
<li>paper_authors: Kaikai An, Ce Zheng, Bofei Gao, Haozhe Zhao, Baobao Chang</li>
<li>for: 本研究旨在提高FrameIdentification的精度和效率，尤其是在面临大量候选框的情况下。</li>
<li>methods: 本文提出了CoFFTEA Architecture，包括Coarse-to-Fine Encoders和 dual encoders，通过对框和目标进行对齐学习，以提高FrameIdentification的精度和效率。</li>
<li>results: 实验结果表明，CoFFTEA比前一代模型提高0.93的总得分和1.53的R@1指标，而不使用$lf$。此外，CoFFTEA还能更好地模型框和框之间的关系，以及目标和目标之间的关系。<details>
<summary>Abstract</summary>
Frame identification aims to find semantic frames associated with target words in a sentence. Recent researches measure the similarity or matching score between targets and candidate frames by modeling frame definitions. However, they either lack sufficient representation learning of the definitions or face challenges in efficiently selecting the most suitable frame from over 1000 candidate frames. Moreover, commonly used lexicon filtering ($lf$) to obtain candidate frames for the target may ignore out-of-vocabulary targets and cause inadequate frame modeling. In this paper, we propose CoFFTEA, a $\underline{Co}$arse-to-$\underline{F}$ine $\underline{F}$rame and $\underline{T}$arget $\underline{E}$ncoders $\underline{A}$rchitecture. With contrastive learning and dual encoders, CoFFTEA efficiently and effectively models the alignment between frames and targets. By employing a coarse-to-fine curriculum learning procedure, CoFFTEA gradually learns to differentiate frames with varying degrees of similarity. Experimental results demonstrate that CoFFTEA outperforms previous models by 0.93 overall scores and 1.53 R@1 without $lf$. Further analysis suggests that CoFFTEA can better model the relationships between frame and frame, as well as target and target. The code for our approach is available at https://github.com/pkunlp-icler/COFFTEA.
</details>
<details>
<summary>摘要</summary>
框架识别目标words的Semantic框的相关研究。近期研究通过定义框的模型来衡量目标和候选框之间的相似性或匹配得分。然而，它们可能缺乏定义框的表示学习或有效地从1000多个候选框中选择最适合的框。另外，通常使用词典筛选($lf$)来获取候选框的目标可能忽略到词语表外的目标，从而导致不充分的框模型。在这篇论文中，我们提出了CoFFTEA，一种$\underline{Co}$arse-to-$\underline{F}$ine $\underline{F}$rame和$\underline{T}$arget $\underline{E}$ncoders $\underline{A}$rchitecture。通过对框和目标的对齐学习，CoFFTEA可以高效地和高效地模型框和目标之间的对齐。通过使用粗细度逐步学习程序，CoFFTEA逐渐学习到不同程度的相似度之间的分化。实验结果表明，CoFFTEA在前一代模型的0.93的总得分和1.53的R@1（不使用$lf）。进一步分析表明，CoFFTEA可以更好地模型框和框之间，以及目标和目标之间的关系。我们的代码可以在https://github.com/pkunlp-icler/COFFTEA中获取。
</details></li>
</ul>
<hr>
<h2 id="Decoding-the-Silent-Majority-Inducing-Belief-Augmented-Social-Graph-with-Large-Language-Model-for-Response-Forecasting"><a href="#Decoding-the-Silent-Majority-Inducing-Belief-Augmented-Social-Graph-with-Large-Language-Model-for-Response-Forecasting" class="headerlink" title="Decoding the Silent Majority: Inducing Belief Augmented Social Graph with Large Language Model for Response Forecasting"></a>Decoding the Silent Majority: Inducing Belief Augmented Social Graph with Large Language Model for Response Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13297">http://arxiv.org/abs/2310.13297</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenkai Sun, Jinning Li, Yi R. Fung, Hou Pong Chan, Tarek Abdelzaher, ChengXiang Zhai, Heng Ji</li>
<li>For: The paper aims to improve the accuracy of automatic response forecasting for news media, specifically in cases where explicit profiles or historical actions of users are limited (referred to as lurkers).* Methods: The proposed framework, SocialSense, leverages a large language model to induce a belief-centered graph on top of an existent social network, along with graph-based propagation to capture social dynamics.* Results: The proposed method surpasses existing state-of-the-art in experimental evaluations for both zero-shot and supervised settings, demonstrating its effectiveness in response forecasting, and is capable of handling unseen user and lurker scenarios.Here is the same information in Simplified Chinese:* For: 文章目标是改进新闻媒体自动回应预测精度，特别是在用户的详细信息或历史行为 Limited (referred to as lurkers) 情况下。* Methods: 提议的社交感知框架 SocialSense 利用大型语言模型生成 belief-centered 图 на основе现有的社交网络，并通过图基本传播来捕捉社交动态。* Results: 提议的方法在 zero-shot 和 supervised 设置下的实验评估中超过现有状态的前ier，表明其在回应预测中的有效性，并能够处理未seen 用户和隐身用户情况。<details>
<summary>Abstract</summary>
Automatic response forecasting for news media plays a crucial role in enabling content producers to efficiently predict the impact of news releases and prevent unexpected negative outcomes such as social conflict and moral injury. To effectively forecast responses, it is essential to develop measures that leverage the social dynamics and contextual information surrounding individuals, especially in cases where explicit profiles or historical actions of the users are limited (referred to as lurkers). As shown in a previous study, 97% of all tweets are produced by only the most active 25% of users. However, existing approaches have limited exploration of how to best process and utilize these important features. To address this gap, we propose a novel framework, named SocialSense, that leverages a large language model to induce a belief-centered graph on top of an existent social network, along with graph-based propagation to capture social dynamics. We hypothesize that the induced graph that bridges the gap between distant users who share similar beliefs allows the model to effectively capture the response patterns. Our method surpasses existing state-of-the-art in experimental evaluations for both zero-shot and supervised settings, demonstrating its effectiveness in response forecasting. Moreover, the analysis reveals the framework's capability to effectively handle unseen user and lurker scenarios, further highlighting its robustness and practical applicability.
</details>
<details>
<summary>摘要</summary>
自动回应预测 для新闻媒体在内容制作者能够有效预测新闻发布后的影响，避免不必要的负面结果，如社交冲突和道德伤害。为了有效预测回应，需要开发机制，利用社交动力和用户境外信息，尤其是在用户没有明确 Profiling 或历史行为时。据前一研究显示，97%的所有推文来自只有25%最活跃的用户。然而，现有方法尚未充分探讨如何最佳处理和利用这些重要特征。为了解决这个空白，我们提出了一种新的框架，名为SocialSense，它利用大型自然语言模型生成一个带有信念中心的图，并利用图基于传播来捕捉社交动力。我们假设，生成的图可以bridging distant用户之间的相似信念，使模型能够有效地捕捉回应模式。我们的方法在实验评估中超越了现有状态的艺术，demonstrating its effectiveness in response forecasting。此外，分析表明框架可以有效处理未看到用户和寂寂者enario，进一步强调其可靠性和实用性。
</details></li>
</ul>
<hr>
<h2 id="PathRL-An-End-to-End-Path-Generation-Method-for-Collision-Avoidance-via-Deep-Reinforcement-Learning"><a href="#PathRL-An-End-to-End-Path-Generation-Method-for-Collision-Avoidance-via-Deep-Reinforcement-Learning" class="headerlink" title="PathRL: An End-to-End Path Generation Method for Collision Avoidance via Deep Reinforcement Learning"></a>PathRL: An End-to-End Path Generation Method for Collision Avoidance via Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13295">http://arxiv.org/abs/2310.13295</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenhao Yu, Jie Peng, Quecheng Qiu, Hanyu Wang, Lu Zhang, Jianmin Ji</li>
<li>for: 提高移动机器人性能的深度强化学习（DRL）方法</li>
<li>methods: 使用特定的动作空间精度化技术和适应状态空间表示方法来解决训练困难</li>
<li>results: 比其他DRL导航方法更高的成功率和降低旋转幅度的稳定和平滑机器人运动<details>
<summary>Abstract</summary>
Robot navigation using deep reinforcement learning (DRL) has shown great potential in improving the performance of mobile robots. Nevertheless, most existing DRL-based navigation methods primarily focus on training a policy that directly commands the robot with low-level controls, like linear and angular velocities, which leads to unstable speeds and unsmooth trajectories of the robot during the long-term execution. An alternative method is to train a DRL policy that outputs the navigation path directly. However, two roadblocks arise for training a DRL policy that outputs paths: (1) The action space for potential paths often involves higher dimensions comparing to low-level commands, which increases the difficulties of training; (2) It takes multiple time steps to track a path instead of a single time step, which requires the path to predicate the interactions of the robot w.r.t. the dynamic environment in multiple time steps. This, in turn, amplifies the challenges associated with training. In response to these challenges, we propose PathRL, a novel DRL method that trains the policy to generate the navigation path for the robot. Specifically, we employ specific action space discretization techniques and tailored state space representation methods to address the associated challenges. In our experiments, PathRL achieves better success rates and reduces angular rotation variability compared to other DRL navigation methods, facilitating stable and smooth robot movement. We demonstrate the competitive edge of PathRL in both real-world scenarios and multiple challenging simulation environments.
</details>
<details>
<summary>摘要</summary>
仿生Navigation使用深度强化学习（DRL）已经显示出了提高移动机器人性能的潜在力量。然而，现有的大多数DRL基于 Navigation方法都主要集中于直接训练机器人的低级指令，如线性和ANGULAR velocity，这会导致机器人在长期执行中的速度不稳定和轨迹不平滑。作为一种alternative方法，可以训练一个DRL策略，输出机器人的Navigation path。然而，两个障碍物 arise for training a DRL策略：（1）action space for potential paths often involves higher dimensions comparing to low-level commands，这会增加训练的difficulties;（2）It takes multiple time steps to track a path instead of a single time step，这需要机器人在多个时间步骤中与动态环境进行互动，从而增加训练的挑战。为回应这些挑战，我们提出了PathRL，一种新的DRL方法，训练策略是生成机器人的Navigation path。我们使用specific action space discretization techniques和tailored state space representation methods来解决相关的挑战。在我们的实验中，PathRL实现了与其他DRL Navigation方法相比更高的成功率和降低ANGULAR rotation variability，使机器人的移动更加稳定和平滑。我们在实际场景和多个复杂的simulation环境中证明了PathRL的竞争力。
</details></li>
</ul>
<hr>
<h2 id="Assessing-Privacy-Risks-in-Language-Models-A-Case-Study-on-Summarization-Tasks"><a href="#Assessing-Privacy-Risks-in-Language-Models-A-Case-Study-on-Summarization-Tasks" class="headerlink" title="Assessing Privacy Risks in Language Models: A Case Study on Summarization Tasks"></a>Assessing Privacy Risks in Language Models: A Case Study on Summarization Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13291">http://arxiv.org/abs/2310.13291</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruixiang Tang, Gord Lueck, Rodolfo Quispe, Huseyin A Inan, Janardhan Kulkarni, Xia Hu</li>
<li>for: 研究它们可以攻击语言模型的数据成员情报泄露问题。</li>
<li>methods: 利用文本相似性和模型对文档修改的抵抗性作为可能的数据成员情报泄露信号，并评估其效果在广泛使用的 dataset 上。</li>
<li>results: 结果表明，摘要模型容易泄露数据成员情报，即使参考摘要不可用。此外，我们还讨论了训练摘要模型的安全措施，并讨论了数据隐私和实用性之间的自然补偿。<details>
<summary>Abstract</summary>
Large language models have revolutionized the field of NLP by achieving state-of-the-art performance on various tasks. However, there is a concern that these models may disclose information in the training data. In this study, we focus on the summarization task and investigate the membership inference (MI) attack: given a sample and black-box access to a model's API, it is possible to determine if the sample was part of the training data. We exploit text similarity and the model's resistance to document modifications as potential MI signals and evaluate their effectiveness on widely used datasets. Our results demonstrate that summarization models are at risk of exposing data membership, even in cases where the reference summary is not available. Furthermore, we discuss several safeguards for training summarization models to protect against MI attacks and discuss the inherent trade-off between privacy and utility.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Unified-Pretraining-for-Recommendation-via-Task-Hypergraphs"><a href="#Unified-Pretraining-for-Recommendation-via-Task-Hypergraphs" class="headerlink" title="Unified Pretraining for Recommendation via Task Hypergraphs"></a>Unified Pretraining for Recommendation via Task Hypergraphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13286">http://arxiv.org/abs/2310.13286</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mdyfrank/uprth">https://github.com/mdyfrank/uprth</a></li>
<li>paper_authors: Mingdai Yang, Zhiwei Liu, Liangwei Yang, Xiaolong Liu, Chen Wang, Hao Peng, Philip S. Yu</li>
<li>for: 提出一个 novel 多任务预训练框架 UPRTH，以满足各种推荐任务的多元需求和特点。</li>
<li>methods: 提出了一个名为 task hypergraphs 的新方法，可以将多个预训练任务转换为 Hyperedge 预测任务，并将这些任务联系到推荐任务上。同时，提出了一个名为 transitional attention 的新层，可以精确地学习每个预训练任务与推荐任务之间的相关性。</li>
<li>results: 透过实验结果，显示 UPRTH 的超越性，并进行了详细的探索，以证明提案的架构的有效性。<details>
<summary>Abstract</summary>
Although pretraining has garnered significant attention and popularity in recent years, its application in graph-based recommender systems is relatively limited. It is challenging to exploit prior knowledge by pretraining in widely used ID-dependent datasets. On one hand, user-item interaction history in one dataset can hardly be transferred to other datasets through pretraining, where IDs are different. On the other hand, pretraining and finetuning on the same dataset leads to a high risk of overfitting. In this paper, we propose a novel multitask pretraining framework named Unified Pretraining for Recommendation via Task Hypergraphs. For a unified learning pattern to handle diverse requirements and nuances of various pretext tasks, we design task hypergraphs to generalize pretext tasks to hyperedge prediction. A novel transitional attention layer is devised to discriminatively learn the relevance between each pretext task and recommendation. Experimental results on three benchmark datasets verify the superiority of UPRTH. Additional detailed investigations are conducted to demonstrate the effectiveness of the proposed framework.
</details>
<details>
<summary>摘要</summary>
(Note: The text has been translated into Simplified Chinese, which is the standard form of Chinese used in mainland China. The translation may vary depending on the region or dialect.)
</details></li>
</ul>
<hr>
<h2 id="Meta-learning-of-Physics-informed-Neural-Networks-for-Efficiently-Solving-Newly-Given-PDEs"><a href="#Meta-learning-of-Physics-informed-Neural-Networks-for-Efficiently-Solving-Newly-Given-PDEs" class="headerlink" title="Meta-learning of Physics-informed Neural Networks for Efficiently Solving Newly Given PDEs"></a>Meta-learning of Physics-informed Neural Networks for Efficiently Solving Newly Given PDEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13270">http://arxiv.org/abs/2310.13270</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tomoharu Iwata, Yusuke Tanaka, Naonori Ueda</li>
<li>for: 解决各种partial differential equation(PDE)问题</li>
<li>methods: 使用神经网络基于meta-学习方法，将PDE问题编码成问题表示，并使用神经网络预测解决方案</li>
<li>results: 比较 existed方法，提出的方法可以更高效地预测PDE问题的解决方案<details>
<summary>Abstract</summary>
We propose a neural network-based meta-learning method to efficiently solve partial differential equation (PDE) problems. The proposed method is designed to meta-learn how to solve a wide variety of PDE problems, and uses the knowledge for solving newly given PDE problems. We encode a PDE problem into a problem representation using neural networks, where governing equations are represented by coefficients of a polynomial function of partial derivatives, and boundary conditions are represented by a set of point-condition pairs. We use the problem representation as an input of a neural network for predicting solutions, which enables us to efficiently predict problem-specific solutions by the forwarding process of the neural network without updating model parameters. To train our model, we minimize the expected error when adapted to a PDE problem based on the physics-informed neural network framework, by which we can evaluate the error even when solutions are unknown. We demonstrate that our proposed method outperforms existing methods in predicting solutions of PDE problems.
</details>
<details>
<summary>摘要</summary>
我们提出了一种基于神经网络的meta学习方法，用于效率地解决部分 diferencial equation（PDE）问题。我们的方法可以快速地学习解决各种PDE问题，并使用这些知识来解决新给定的PDE问题。我们将PDE问题编码为一个问题表示，使用神经网络来表示管理方程，并将边界条件表示为一组点条件对。我们使用问题表示作为神经网络预测解决方法的输入，这使得我们可以通过神经网络的前进过程来快速地预测问题特定的解决方案，而不需要更新模型参数。为了训练我们的模型，我们将通过基于物理学信息神经网络框架的最小二乘法来减少预测错误，这样我们就可以在解决PDE问题时评估解决方案的错误。我们示出了我们的提出方法可以比既有方法更高效地预测PDE问题的解决方案。
</details></li>
</ul>
<hr>
<h2 id="An-Exploratory-Study-on-Simulated-Annealing-for-Feature-Selection-in-Learning-to-Rank"><a href="#An-Exploratory-Study-on-Simulated-Annealing-for-Feature-Selection-in-Learning-to-Rank" class="headerlink" title="An Exploratory Study on Simulated Annealing for Feature Selection in Learning-to-Rank"></a>An Exploratory Study on Simulated Annealing for Feature Selection in Learning-to-Rank</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13269">http://arxiv.org/abs/2310.13269</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohd. Sayemul Haque, Md. Fahim, Muhammad Ibrahim</li>
<li>for: 本研究是 investigate the use of simulated annealing for feature selection in the learning-to-rank domain.</li>
<li>methods: 我们使用了一种叫做 simulated annealing的meta-heuristic approach，并 explore various neighborhood selection strategies和 temperature cooling schemes. We also introduce a new hyper-parameter called the progress parameter.</li>
<li>results: 我们的算法在五个公共的学习到排序 benchmark datasets上进行了评估，并与Local Beam Search算法进行了比较。结果表明我们的提议的模型具有效果。<details>
<summary>Abstract</summary>
Learning-to-rank is an applied domain of supervised machine learning. As feature selection has been found to be effective for improving the accuracy of learning models in general, it is intriguing to investigate this process for learning-to-rank domain. In this study, we investigate the use of a popular meta-heuristic approach called simulated annealing for this task. Under the general framework of simulated annealing, we explore various neighborhood selection strategies and temperature cooling schemes. We further introduce a new hyper-parameter called the progress parameter that can effectively be used to traverse the search space. Our algorithms are evaluated on five publicly benchmark datasets of learning-to-rank. For a better validation, we also compare the simulated annealing-based feature selection algorithm with another effective meta-heuristic algorithm, namely local beam search. Extensive experimental results shows the efficacy of our proposed models.
</details>
<details>
<summary>摘要</summary>
学习排名是应用领域的超级vised机器学习。因为特征选择已被证明可以提高学习模型的准确率，因此在这个领域进行这种过程是非常有趣。在这个研究中，我们使用了一种受欢迎的meta-heuristic方法，即模拟熔炉。在总体框架下，我们探索了不同的邻居选择策略和温度冷却方案。我们还引入了一个新的超参数，即进度参数，可以有效地探索搜索空间。我们的算法在五个公共的学习排名数据集上进行了评估。为了更好地验证，我们还与另一种有效的meta-heuristic算法，即本地搜索算法进行比较。广泛的实验结果表明了我们的提议的模型的有效性。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-drug-and-cell-line-representations-via-contrastive-learning-for-improved-anti-cancer-drug-prioritization"><a href="#Enhancing-drug-and-cell-line-representations-via-contrastive-learning-for-improved-anti-cancer-drug-prioritization" class="headerlink" title="Enhancing drug and cell line representations via contrastive learning for improved anti-cancer drug prioritization"></a>Enhancing drug and cell line representations via contrastive learning for improved anti-cancer drug prioritization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13725">http://arxiv.org/abs/2310.13725</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patrick J. Lawrence, Xia Ning</li>
<li>for: 这项研究旨在提高基于omic序列分析的精准肿瘤治疗，通过强化药物和细胞系列表示的关系结构，以提高计算机方法学习药物和细胞系列之间的相互关系。</li>
<li>methods: 本研究使用了对比学习方法，以保留药物机制作用和细胞系列肿瘤类型之间的关系结构，从而提高了学习的药物和细胞系列表示。</li>
<li>results: 相比之前的状态艺法，我们的学习表示可以 achieve更高的性能，并且发现使用我们学习的表示时，预测器更加均衡地使用药物和细胞系列来源的特征，这有助于进行更加个性化的药物优先顺序。<details>
<summary>Abstract</summary>
Due to cancer's complex nature and variable response to therapy, precision oncology informed by omics sequence analysis has become the current standard of care. However, the amount of data produced for each patients makes it difficult to quickly identify the best treatment regimen. Moreover, limited data availability has hindered computational methods' abilities to learn patterns associated with effective drug-cell line pairs. In this work, we propose the use of contrastive learning to improve learned drug and cell line representations by preserving relationship structures associated with drug mechanism of action and cell line cancer types. In addition to achieving enhanced performance relative to a state-of-the-art method, we find that classifiers using our learned representations exhibit a more balances reliance on drug- and cell line-derived features when making predictions. This facilitates more personalized drug prioritizations that are informed by signals related to drug resistance.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:由于肿瘤的复杂性和变化的响应，精准肿瘤学 Informed by omics sequence analysis 已成为现代标准的治疗方法。然而，每个患者生成的数据量太大，使得快速 identificar最佳治疗方案变得困难。此外，数据的有限性限制了计算方法学习 pattern 关联有效的药品和细胞线路。在这项工作中，我们提议使用对比学习来提高学习到药品和细胞线路的表示，以保留药理学 Mechanism of action 和细胞型 cancer 之间的关系结构。此外，我们发现使用我们学习的表示的分类器能够更好地使用药品和细胞线路 Derived 特征进行预测，从而实现更个性化的药品优先级，基于药品抵抗的信号。
</details></li>
</ul>
<hr>
<h2 id="ManiCast-Collaborative-Manipulation-with-Cost-Aware-Human-Forecasting"><a href="#ManiCast-Collaborative-Manipulation-with-Cost-Aware-Human-Forecasting" class="headerlink" title="ManiCast: Collaborative Manipulation with Cost-Aware Human Forecasting"></a>ManiCast: Collaborative Manipulation with Cost-Aware Human Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13258">http://arxiv.org/abs/2310.13258</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kushal Kedia, Prithwish Dan, Atiksh Bhardwaj, Sanjiban Choudhury</li>
<li>for: 该论文旨在提高人机 робо合作 manipulation 性能，具体是通过精准预测人类动作来提高机器人的计划性能。</li>
<li>methods: 该论文提出了一种名为 ManiCast 的新框架，该框架通过学习人类动作预测模型，并将其与一种模拟预测控制器结合，以执行协同 manipulation 任务。</li>
<li>results: 该论文通过实验证明，ManiCast 框架可以在多个实际任务中，如反应混合、物品传递和协同设备等，实现流畅、实时的人机合作。同时，论文还对 Forecast 和 End-to-End 预测控制器系统进行了评估，并与已有的学习基线和规则基线进行了比较。<details>
<summary>Abstract</summary>
Seamless human-robot manipulation in close proximity relies on accurate forecasts of human motion. While there has been significant progress in learning forecast models at scale, when applied to manipulation tasks, these models accrue high errors at critical transition points leading to degradation in downstream planning performance. Our key insight is that instead of predicting the most likely human motion, it is sufficient to produce forecasts that capture how future human motion would affect the cost of a robot's plan. We present ManiCast, a novel framework that learns cost-aware human forecasts and feeds them to a model predictive control planner to execute collaborative manipulation tasks. Our framework enables fluid, real-time interactions between a human and a 7-DoF robot arm across a number of real-world tasks such as reactive stirring, object handovers, and collaborative table setting. We evaluate both the motion forecasts and the end-to-end forecaster-planner system against a range of learned and heuristic baselines while additionally contributing new datasets. We release our code and datasets at https://portal-cornell.github.io/manicast/.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将人机 manipulate 在 close proximity 中进行流畅的执行，需要准确预测人类动作。虽然在大规模学习中有了 significiant progress ，但在执行 manipulate 任务时，这些模型会在关键转折点上产生高错误率，导致下游规划性能下降。我们的关键发现是，而不是预测人类动作的最有可能性，可以生成捕捉未来人类动作对机器人计划的成本影响的预测。我们提出了 ManiCast 框架，该框架学习成本识别人类预测，并将其传递给模型预测控制 плаanner 执行共同执行任务。我们的框架可以在多个实际任务中，如反应搅拌、物品交接和共同设备，实现流畅、实时的人机互动。我们对预测和总体预测控制系统进行了评估，并与已学习和启发式基础集成。我们将代码和数据集发布在 <https://portal-cornell.github.io/manicast/>。
</details></li>
</ul>
<hr>
<h2 id="Visual-Grounding-Helps-Learn-Word-Meanings-in-Low-Data-Regimes"><a href="#Visual-Grounding-Helps-Learn-Word-Meanings-in-Low-Data-Regimes" class="headerlink" title="Visual Grounding Helps Learn Word Meanings in Low-Data Regimes"></a>Visual Grounding Helps Learn Word Meanings in Low-Data Regimes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13257">http://arxiv.org/abs/2310.13257</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengxu Zhuang, Evelina Fedorenko, Jacob Andreas</li>
<li>for: 研究语言模型（LM）是人类语言生成和理解的 poderful工具，并且它们的内部表示与人类语言处理的表示相吻合。但是，为了达到这些结果，LM必须通过不人类化的训练方法进行训练，需要大量的语言数据，而无需与感知、行为或社会行为相关的知识。</li>
<li>methods: 我们使用了多种LM架构，并在不同的数据scale上进行训练。我们还使用了图像描述任务作为 auxiliary supervision。</li>
<li>results: 我们发现，视觉超vision可以提高word learning的效率，但这些改进几乎都出现在低数据 régime中，而且在包含丰富的分布式信号的情况下，这些改进可能会被抵消。我们发现，模型主要驱动的视觉信息和word co-occurrence信息之间的信息不是重复的。然而，我们的结果表明，当前的多模式模型化方法无法有效地利用视觉信息，从人类化的数据集上建立更人类化的word表示。<details>
<summary>Abstract</summary>
Modern neural language models (LMs) are powerful tools for modeling human sentence production and comprehension, and their internal representations are remarkably well-aligned with representations of language in the human brain. But to achieve these results, LMs must be trained in distinctly un-human-like ways -- requiring orders of magnitude more language data than children receive during development, and without any of the accompanying grounding in perception, action, or social behavior. Do models trained more naturalistically -- with grounded supervision -- exhibit more human-like language learning? We investigate this question in the context of word learning, a key sub-task in language acquisition. We train a diverse set of LM architectures, with and without auxiliary supervision from image captioning tasks, on datasets of varying scales. We then evaluate these models on a broad set of benchmarks characterizing models' learning of syntactic categories, lexical relations, semantic features, semantic similarity, and alignment with human neural representations. We find that visual supervision can indeed improve the efficiency of word learning. However, these improvements are limited: they are present almost exclusively in the low-data regime, and sometimes canceled out by the inclusion of rich distributional signals from text. The information conveyed by text and images is not redundant -- we find that models mainly driven by visual information yield qualitatively different from those mainly driven by word co-occurrences. However, our results suggest that current multi-modal modeling approaches fail to effectively leverage visual information to build more human-like word representations from human-sized datasets.
</details>
<details>
<summary>摘要</summary>
现代神经语言模型（LM）是强大的工具，用于模拟人类句子生成和理解，并且其内部表示与人类语言表示相吻合。但是，为了 достичь这些结果，LM需要接受非人类化的训练方法，需要大量的语言数据，并且没有与感知、行为或社会行为相关的背景。我们问 whether models trained more naturally -- with grounded supervision -- exhibit more human-like language learning? 在word learning中，我们训练了多种LM架构，有和无附加的图像描述任务 auxiliary supervision，在不同的数据规模上进行训练。然后，我们对这些模型进行了广泛的测试，以评估它们在 sintactic categories、lexical relations、semantic features、semantic similarity 和人类神经表示相似性方面的学习效果。我们发现，视觉supervision可以提高word learning的效率，但这些改进几乎完全局限于低数据规模，而且有时会被文本中的丰富分布式信号抵消。我们发现，文本和图像中的信息并不是重复的，模型主要驱动的视觉信息会导致模型的学习结果与主要驱动的word co-occurrences不同。然而，我们的结果表明，现有的多模态模型化方法无法有效地利用视觉信息，从人类化的数据规模上建立更人类化的word表示。
</details></li>
</ul>
<hr>
<h2 id="TempGNN-Temporal-Graph-Neural-Networks-for-Dynamic-Session-Based-Recommendations"><a href="#TempGNN-Temporal-Graph-Neural-Networks-for-Dynamic-Session-Based-Recommendations" class="headerlink" title="TempGNN: Temporal Graph Neural Networks for Dynamic Session-Based Recommendations"></a>TempGNN: Temporal Graph Neural Networks for Dynamic Session-Based Recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13249">http://arxiv.org/abs/2310.13249</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eunkyu Oh, Taehun Kim</li>
<li>for: 这种研究旨在提高Session-based recommendation的准确率，通过更好地理解用户在短时间内的交互行为和item之间的相互关系。</li>
<li>methods: 该研究提出了Temporal Graph Neural Networks（TempGNN）模型，通过在动态Session图上使用时间嵌入Operator来捕捉交互行为的结构和时间层次结构。</li>
<li>results: 实验结果表明，TempGNN可以充分利用已有模型的优势，并且在两个真实世界电商数据集上达到了状态之冠的表现。<details>
<summary>Abstract</summary>
Session-based recommendations which predict the next action by understanding a user's interaction behavior with items within a relatively short ongoing session have recently gained increasing popularity. Previous research has focused on capturing the dynamics of sequential dependencies from complicated item transitions in a session by means of recurrent neural networks, self-attention models, and recently, mostly graph neural networks. Despite the plethora of different models relying on the order of items in a session, few approaches have been proposed for dealing better with the temporal implications between interactions. We present Temporal Graph Neural Networks (TempGNN), a generic framework for capturing the structural and temporal dynamics in complex item transitions utilizing temporal embedding operators on nodes and edges on dynamic session graphs, represented as sequences of timed events. Extensive experimental results show the effectiveness and adaptability of the proposed method by plugging it into existing state-of-the-art models. Finally, TempGNN achieved state-of-the-art performance on two real-world e-commerce datasets.
</details>
<details>
<summary>摘要</summary>
SESSION-based recommendations, which predict the next action by understanding a user's interaction behavior with items within a relatively short ongoing session, have recently gained increasing popularity. Previous research has focused on capturing the dynamics of sequential dependencies from complicated item transitions in a session by means of recurrent neural networks, self-attention models, and recently, mostly graph neural networks. Despite the plethora of different models relying on the order of items in a session, few approaches have been proposed for dealing better with the temporal implications between interactions. We present Temporal Graph Neural Networks (TempGNN), a generic framework for capturing the structural and temporal dynamics in complex item transitions utilizing temporal embedding operators on nodes and edges on dynamic session graphs, represented as sequences of timed events. Extensive experimental results show the effectiveness and adaptability of the proposed method by plugging it into existing state-of-the-art models. Finally, TempGNN achieved state-of-the-art performance on two real-world e-commerce datasets.Here's the translation in Traditional Chinese as well:SESSION-based recommendations, which predict the next action by understanding a user's interaction behavior with items within a relatively short ongoing session, have recently gained increasing popularity. Previous research has focused on capturing the dynamics of sequential dependencies from complicated item transitions in a session by means of recurrent neural networks, self-attention models, and recently, mostly graph neural networks. Despite the plethora of different models relying on the order of items in a session, few approaches have been proposed for dealing better with the temporal implications between interactions. We present Temporal Graph Neural Networks (TempGNN), a generic framework for capturing the structural and temporal dynamics in complex item transitions utilizing temporal embedding operators on nodes and edges on dynamic session graphs, represented as sequences of timed events. Extensive experimental results show the effectiveness and adaptability of the proposed method by plugging it into existing state-of-the-art models. Finally, TempGNN achieved state-of-the-art performance on two real-world e-commerce datasets.
</details></li>
</ul>
<hr>
<h2 id="FLEE-GNN-A-Federated-Learning-System-for-Edge-Enhanced-Graph-Neural-Network-in-Analyzing-Geospatial-Resilience-of-Multicommodity-Food-Flows"><a href="#FLEE-GNN-A-Federated-Learning-System-for-Edge-Enhanced-Graph-Neural-Network-in-Analyzing-Geospatial-Resilience-of-Multicommodity-Food-Flows" class="headerlink" title="FLEE-GNN: A Federated Learning System for Edge-Enhanced Graph Neural Network in Analyzing Geospatial Resilience of Multicommodity Food Flows"></a>FLEE-GNN: A Federated Learning System for Edge-Enhanced Graph Neural Network in Analyzing Geospatial Resilience of Multicommodity Food Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13248">http://arxiv.org/abs/2310.13248</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/geods/flee-gnn">https://github.com/geods/flee-gnn</a></li>
<li>paper_authors: Yuxiao Qu, Jinmeng Rao, Song Gao, Qianheng Zhang, Wei-Lun Chao, Yu Su, Michelle Miller, Alfonso Morales, Patrick Huber</li>
<li>for: 这篇论文旨在探讨如何使用 Federated Learning System for Edge-Enhanced Graph Neural Network (FLEE-GNN) 来解决食品供应网络的可恢复性问题，以提高食品安全性。</li>
<li>methods: 该论文提出了一种基于 Federated Learning 的方法，使用 Graph Neural Network (GNN) 来分析食品供应网络的可恢复性。这种方法结合了 GNN 的强大和鲁棒性，以及 Federated Learning 的隐私保护和分布式特点。</li>
<li>results: 该论文的实验结果表明，FLEE-GNN 可以有效地提高食品供应网络的可恢复性分析，并且可以应用于其他的空间网络中。<details>
<summary>Abstract</summary>
Understanding and measuring the resilience of food supply networks is a global imperative to tackle increasing food insecurity. However, the complexity of these networks, with their multidimensional interactions and decisions, presents significant challenges. This paper proposes FLEE-GNN, a novel Federated Learning System for Edge-Enhanced Graph Neural Network, designed to overcome these challenges and enhance the analysis of geospatial resilience of multicommodity food flow network, which is one type of spatial networks. FLEE-GNN addresses the limitations of current methodologies, such as entropy-based methods, in terms of generalizability, scalability, and data privacy. It combines the robustness and adaptability of graph neural networks with the privacy-conscious and decentralized aspects of federated learning on food supply network resilience analysis across geographical regions. This paper also discusses FLEE-GNN's innovative data generation techniques, experimental designs, and future directions for improvement. The results show the advancements of this approach to quantifying the resilience of multicommodity food flow networks, contributing to efforts towards ensuring global food security using AI methods. The developed FLEE-GNN has the potential to be applied in other spatial networks with spatially heterogeneous sub-network distributions.
</details>
<details>
<summary>摘要</summary>
全球化的食品供应网络可靠性理解和测量是面临增长的食品不安全的全球需求。然而，这些网络的复杂性，包括多维度交互和决策，带来了重要的挑战。这篇论文提出了FLEE-GNN，一种新的联邦学习系统，用于增强地图分布式神经网络的地ospatial可靠性分析。FLEE-GNN将现有方法，如Entropy-based方法，超越了一致性、可扩展性和数据隐私方面的限制。它将图神经网络的可靠性和适应性与联邦学习的隐私性和分散性相结合，进行食品供应网络可靠性分析 Across geographical regions。本文还讨论了FLEE-GNN的创新数据生成技术、实验设计和未来改进方向。结果表明FLEE-GNN在多种 alimentary food flow networks 可靠性分析方面做出了进步，贡献到全球食品安全使用 AI 方法。发展的FLEE-GNN可以应用于其他的空间网络，具有空间不同互连分布。
</details></li>
</ul>
<hr>
<h2 id="Multi-level-Contrastive-Learning-for-Script-based-Character-Understanding"><a href="#Multi-level-Contrastive-Learning-for-Script-based-Character-Understanding" class="headerlink" title="Multi-level Contrastive Learning for Script-based Character Understanding"></a>Multi-level Contrastive Learning for Script-based Character Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13231">http://arxiv.org/abs/2310.13231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dawei Li, Hengyuan Zhang, Yanran Li, Shiping Yang</li>
<li>for: 本研究目的是理解文本中的人物性格和身份，通过其讲话习惯了解其全面性。</li>
<li>methods: 我们提出了一种多级对比学习框架，用于捕捉人物的全面信息。我们进行了广泛的实验，与多种先进的语言模型进行比较，包括SpanBERT、Longformer、BigBird和ChatGPT-3.5。</li>
<li>results: 我们的方法可以大幅提高人物理解的性能，并通过进一步的分析，证明了我们的方法的有效性和解决了一些挑战。我们将在github上公开我们的工作，链接在<a target="_blank" rel="noopener" href="https://github.com/David-Li0406/Script-based-Character-Understanding%E3%80%82">https://github.com/David-Li0406/Script-based-Character-Understanding。</a><details>
<summary>Abstract</summary>
In this work, we tackle the scenario of understanding characters in scripts, which aims to learn the characters' personalities and identities from their utterances. We begin by analyzing several challenges in this scenario, and then propose a multi-level contrastive learning framework to capture characters' global information in a fine-grained manner. To validate the proposed framework, we conduct extensive experiments on three character understanding sub-tasks by comparing with strong pre-trained language models, including SpanBERT, Longformer, BigBird and ChatGPT-3.5. Experimental results demonstrate that our method improves the performances by a considerable margin. Through further in-depth analysis, we show the effectiveness of our method in addressing the challenges and provide more hints on the scenario of character understanding. We will open-source our work on github at https://github.com/David-Li0406/Script-based-Character-Understanding.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们面临了剑道字符的理解enario，即从字符的讲话中学习其个性和身份。我们首先分析了这个enario中的一些挑战，然后提出了一种多级对比学习框架，以精细地捕捉字符的全球信息。为验证我们的方法，我们进行了广泛的实验，包括对三种字符理解子任务进行比较，其中包括SpanBERT、Longformer、BigBird和ChatGPT-3.5等强大的预训练语言模型。实验结果显示，我们的方法可以明显提高表现。通过进一步的深入分析，我们证明了我们的方法在面临挑战时的效果，并提供了更多有关字符理解scenario的提示。我们将在GitHub上开源我们的工作，可以在https://github.com/David-Li0406/Script-based-Character-Understanding中找到。
</details></li>
</ul>
<hr>
<h2 id="Absolute-Policy-Optimization"><a href="#Absolute-Policy-Optimization" class="headerlink" title="Absolute Policy Optimization"></a>Absolute Policy Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13230">http://arxiv.org/abs/2310.13230</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/NhaPhatHanh/github">https://github.com/NhaPhatHanh/github</a></li>
<li>paper_authors: Weiye Zhao, Feihan Li, Yifan Sun, Rui Chen, Tianhao Wei, Changliu Liu</li>
<li>for: 解决复杂控制任务和游戏场景中的策略优化问题</li>
<li>methods: 引入新的目标函数优化策略，并通过一系列近似算法简化实现</li>
<li>results: 在复杂 kontinuous control benchmark 任务和 Atari 游戏中显著超越现状强度策略优化算法，并在预期性和最坏性性能两个方面具备显著改进<details>
<summary>Abstract</summary>
In recent years, trust region on-policy reinforcement learning has achieved impressive results in addressing complex control tasks and gaming scenarios. However, contemporary state-of-the-art algorithms within this category primarily emphasize improvement in expected performance, lacking the ability to control over the worst-case performance outcomes. To address this limitation, we introduce a novel objective function; by optimizing which, it will lead to guaranteed monotonic improvement in the lower bound of near-total performance samples (absolute performance). Considering this groundbreaking theoretical advancement, we then refine this theoretically grounded algorithm through a series of approximations, resulting in a practical solution called Absolute Policy Optimization (APO). Our experiments demonstrate the effectiveness of our approach across challenging continuous control benchmark tasks and extend its applicability to mastering Atari games. Our findings reveal that APO significantly outperforms state-of-the-art policy gradient algorithms, resulting in substantial improvements in both expected performance and worst-case performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Interpretable-Deep-Reinforcement-Learning-for-Optimizing-Heterogeneous-Energy-Storage-Systems"><a href="#Interpretable-Deep-Reinforcement-Learning-for-Optimizing-Heterogeneous-Energy-Storage-Systems" class="headerlink" title="Interpretable Deep Reinforcement Learning for Optimizing Heterogeneous Energy Storage Systems"></a>Interpretable Deep Reinforcement Learning for Optimizing Heterogeneous Energy Storage Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14783">http://arxiv.org/abs/2310.14783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luolin Xiong, Yang Tang, Chensheng Liu, Shuai Mao, Ke Meng, Zhaoyang Dong, Feng Qian</li>
<li>for: 提高能量存储系统（ESS）在能源市场中的灵活性和可再生能源利用率，通过杂质太阳能-存储系统（PV-ESS），利用磷酸存储（BES）和氢能存储（HES）的特点。</li>
<li>methods: 开发了一个完整的成本函数，包括衰变、资本和运营维护成本，用于反映实际场景。同时，提出了一种可解释性高的prototype-based policy网络，通过人工设计的原型来引导决策，使得决策过程自然地具有解释性。</li>
<li>results: 在四个不同的案例中，比较了黑obox模型和我们提出的可解释性优化方法，结果表明，我们的方法具有更高的效果和实用性。<details>
<summary>Abstract</summary>
Energy storage systems (ESS) are pivotal component in the energy market, serving as both energy suppliers and consumers. ESS operators can reap benefits from energy arbitrage by optimizing operations of storage equipment. To further enhance ESS flexibility within the energy market and improve renewable energy utilization, a heterogeneous photovoltaic-ESS (PV-ESS) is proposed, which leverages the unique characteristics of battery energy storage (BES) and hydrogen energy storage (HES). For scheduling tasks of the heterogeneous PV-ESS, cost description plays a crucial role in guiding operator's strategies to maximize benefits. We develop a comprehensive cost function that takes into account degradation, capital, and operation/maintenance costs to reflect real-world scenarios. Moreover, while numerous methods excel in optimizing ESS energy arbitrage, they often rely on black-box models with opaque decision-making processes, limiting practical applicability. To overcome this limitation and enable transparent scheduling strategies, a prototype-based policy network with inherent interpretability is introduced. This network employs human-designed prototypes to guide decision-making by comparing similarities between prototypical situations and encountered situations, which allows for naturally explained scheduling strategies. Comparative results across four distinct cases underscore the effectiveness and practicality of our proposed pre-hoc interpretable optimization method when contrasted with black-box models.
</details>
<details>
<summary>摘要</summary>
能量存储系统（ESS）是能源市场中的关键组件，同时作为能源供应者和消费者。ESS运营商可以通过优化存储设备的操作来获得利润。为了进一步提高ESS在能源市场中的灵活性和可再生能源利用率，我们提议了一种多种能量存储系统（PV-ESS），利用独特的电池能量存储（BES）和氢能存储（HES）特点。对于PV-ESS的调度任务，成本描述起到了关键作用，导引运营商的策略以最大化利润。我们开发了一个全面的成本函数，考虑了退化、资本和运营/维护成本，以准确反映实际场景。此外，虽然许多方法可以优化ESS的能源融合，但它们通常基于黑盒模型，即不可见的决策过程，限制了实际应用。为了缓解这一限制和提供透明的调度策略，我们引入了一种封装式政策网络，该网络使用人类设计的原型来引导决策，通过比较相似的原型 situación和遇到的 situación之间的相似性，以便自然地解释调度策略。对四个不同的案例进行比较结果表明，我们提出的先验可解释优化方法在黑盒模型的基础上显著超越了黑盒模型。
</details></li>
</ul>
<hr>
<h2 id="ToolChain-Efficient-Action-Space-Navigation-in-Large-Language-Models-with-A-Search"><a href="#ToolChain-Efficient-Action-Space-Navigation-in-Large-Language-Models-with-A-Search" class="headerlink" title="ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search"></a>ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13227">http://arxiv.org/abs/2310.13227</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor Bursztyn, Ryan A. Rossi, Somdeb Sarkhel, Chao Zhang</li>
<li>for: 这个论文的目的是提出一种高效的搜索算法，用于LLM-based自动代理在具有广泛的动作空间的问题上进行决策和规划。</li>
<li>methods: 这个论文使用的方法是基于搜索算法的树形搜索算法，具体来说是将整个动作空间转换为决策树，每个节点表示一个可能的API函数调用。这个算法利用A*搜索算法和任务特定的成本函数设计，高效地快速搜索最低成本的有效路径。</li>
<li>results: 实验结果表明，ToolChain* 算法可以高效地平衡探索和利用在具有广泛的动作空间中，并在计划和理解任务上比基eline表现出3.1%和3.5%的提升，同时需要7.35倍和2.31倍的时间。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated powerful decision-making and planning capabilities in solving complicated real-world problems. LLM-based autonomous agents can interact with diverse tools (e.g., functional APIs) and generate solution plans that execute a series of API function calls in a step-by-step manner. The multitude of candidate API function calls significantly expands the action space, amplifying the critical need for efficient action space navigation. However, existing methods either struggle with unidirectional exploration in expansive action spaces, trapped into a locally optimal solution, or suffer from exhaustively traversing all potential actions, causing inefficient navigation. To address these issues, we propose ToolChain*, an efficient tree search-based planning algorithm for LLM-based agents. It formulates the entire action space as a decision tree, where each node represents a possible API function call involved in a solution plan. By incorporating the A* search algorithm with task-specific cost function design, it efficiently prunes high-cost branches that may involve incorrect actions, identifying the most low-cost valid path as the solution. Extensive experiments on multiple tool-use and reasoning tasks demonstrate that ToolChain* efficiently balances exploration and exploitation within an expansive action space. It outperforms state-of-the-art baselines on planning and reasoning tasks by 3.1% and 3.5% on average while requiring 7.35x and 2.31x less time, respectively.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Scalable-Neural-Network-Kernels"><a href="#Scalable-Neural-Network-Kernels" class="headerlink" title="Scalable Neural Network Kernels"></a>Scalable Neural Network Kernels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13225">http://arxiv.org/abs/2310.13225</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Arijit Sehanobish, Krzysztof Choromanski, Yunfan Zhao, Avinava Dubey, Valerii Likhosherstov</li>
<li>for: 这篇论文目标是提出一种可扩展神经网络核心（SNNK），用于取代常见的前向传输层（FFL），能够approximate FFL，但具有更有利的计算性质。</li>
<li>methods: 这篇论文使用了SNNK，它可以分离输入和参数之间的关系，只在最终计算中via点积kernel进行连接。它们还是FFL的约等价表示，可以模型输入和参数之间的复杂关系。此外，文章还提出了神经网络集成过程，通过应用SNNK来压缩深度神经网络架构，从而获得额外的压缩效果。在极端情况下，这会导致完全集成的神经网络，其优化参数可以通过explicit的方程表示出来，打开了一个可以 circumvent backpropagation的可能性。</li>
<li>results: 文章提供了严格的理论分析和广泛的实验评估，从点积kernel估计到Transformers的精细调整with novel adapter layers inspired by SNNKs。结果表明，使用SNNK可以减少训练参数的数量，保持竞争性的准确性。<details>
<summary>Abstract</summary>
We introduce the concept of scalable neural network kernels (SNNKs), the replacements of regular feedforward layers (FFLs), capable of approximating the latter, but with favorable computational properties. SNNKs effectively disentangle the inputs from the parameters of the neural network in the FFL, only to connect them in the final computation via the dot-product kernel. They are also strictly more expressive, as allowing to model complicated relationships beyond the functions of the dot-products of parameter-input vectors. We also introduce the neural network bundling process that applies SNNKs to compactify deep neural network architectures, resulting in additional compression gains. In its extreme version, it leads to the fully bundled network whose optimal parameters can be expressed via explicit formulae for several loss functions (e.g. mean squared error), opening a possibility to bypass backpropagation. As a by-product of our analysis, we introduce the mechanism of the universal random features (or URFs), applied to instantiate several SNNK variants, and interesting on its own in the context of scalable kernel methods. We provide rigorous theoretical analysis of all these concepts as well as an extensive empirical evaluation, ranging from point-wise kernel estimation to Transformers' fine-tuning with novel adapter layers inspired by SNNKs. Our mechanism provides up to 5x reduction in the number of trainable parameters, while maintaining competitive accuracy.
</details>
<details>
<summary>摘要</summary>
我们介绍了扩展神经网络底层（SNNK），它们取代了常规传递层（FFL），并能够模拟它们，但具有更有利的计算特性。SNNK将输入与神经网络参数分离开来，仅在最终计算中通过点产生kernel来连接。它们还是严格更加表达力强，因为它们可以模型复杂的关系，超过传递参数vector的数统产生器。我们还介绍了将SNNK应用于压缩深度神经网络架构的神经网络卷绿（bundling）过程，从而获得额外的压缩成本。在极端版本下，它导致完全卷绿网络，其最佳参数可以通过明确的公式表示出来，实现了几个损失函数（例如平均方差）的最佳化。作为一个副作用，我们介绍了内在特征随机抽象（URF）的机制，它可以实现多种SNNK的实例，并且在扩展层神经网络方面具有兴趣。我们对这些概念进行了严谨的理论分析，以及广泛的实验评估，从单元测度到Transformers的精细调整 avec novel adapter layers inspired by SNNKs。我们的机制可以实现5倍的受训练 парамет数减少，保持竞争性的精度。
</details></li>
</ul>
<hr>
<h2 id="HierCas-Hierarchical-Temporal-Graph-Attention-Networks-for-Popularity-Prediction-in-Information-Cascades"><a href="#HierCas-Hierarchical-Temporal-Graph-Attention-Networks-for-Popularity-Prediction-in-Information-Cascades" class="headerlink" title="HierCas: Hierarchical Temporal Graph Attention Networks for Popularity Prediction in Information Cascades"></a>HierCas: Hierarchical Temporal Graph Attention Networks for Popularity Prediction in Information Cascades</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13219">http://arxiv.org/abs/2310.13219</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Daisy-zzz/HierCas">https://github.com/Daisy-zzz/HierCas</a></li>
<li>paper_authors: Zhizhen Zhang, Xiaohui Xie, Yishuo Zhang, Lanshan Zhang, Yong Jiang</li>
<li>for: 预测信息冲击的吸引力，用于识别假新闻和提供准确的推荐。</li>
<li>methods: 使用神经网络方法，不同于传统的特征基于方法，它们具有域特定的特性和新领域的不适应性。</li>
<li>results: 在两个实际 dataset 上，对比州的方法，提出了一种新的框架called Hierarchical Temporal Graph Attention Networks for cascade popularity prediction (HierCas)，实现了更高的准确率和更好的性能。<details>
<summary>Abstract</summary>
Information cascade popularity prediction is critical for many applications, including but not limited to identifying fake news and accurate recommendations. Traditional feature-based methods heavily rely on handcrafted features, which are domain-specific and lack generalizability to new domains. To address this problem, researchers have turned to neural network-based approaches. However, existing methods follow a sampling-based modeling approach, potentially losing continuous dynamic information and structural-temporal dependencies that emerge during the information diffusion process. In this paper, we propose a novel framework called Hierarchical Temporal Graph Attention Networks for cascade popularity prediction (HierCas). Unlike existing methods, HierCas operates on the entire cascade graph by a dynamic graph modeling approach, enabling it to capture the full range of continuous dynamic information and explicitly model the interplay between structural and temporal factors. By leveraging time-aware node embedding, graph attention mechanisms and hierarchical pooling structures, HierCas effectively captures the popularity trend implicit in the complex cascade. Extensive experiments conducted on two real-world datasets in different scenarios demonstrate that our HierCas significantly outperforms the state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
信息带动 popularity 预测是许多应用程序中的关键任务，包括但不限于识别假新闻和准确推荐。传统的特征基于方法依赖于手工设计的特征，这些特征是域特定的，缺乏对新领域的普适性。为解决这个问题，研究人员转向神经网络基于方法。然而，现有方法采用采样基本的模型化方法，可能会失去流行化过程中的连续动态信息和结构时间依赖关系。在本文中，我们提出了一种新的框架，即层次时间图注意力网络（HierCas）。与现有方法不同，HierCas 在整个带动图上运行，能够捕捉整个流行化过程中的连续动态信息，并且可以显式地模型结构时间因素之间的交互作用。通过利用时间意识节点嵌入、图注意力机制和层次聚合结构，HierCas 能够有效地捕捉流行趋势的隐含信息。我们在两个实际场景中进行了大量实验，结果表明，我们的 HierCas 在比较 estado-of-the-art 方法的情况下显著 OUTPERFORM。
</details></li>
</ul>
<hr>
<h2 id="MultiCoNER-v2-a-Large-Multilingual-dataset-for-Fine-grained-and-Noisy-Named-Entity-Recognition"><a href="#MultiCoNER-v2-a-Large-Multilingual-dataset-for-Fine-grained-and-Noisy-Named-Entity-Recognition" class="headerlink" title="MultiCoNER v2: a Large Multilingual dataset for Fine-grained and Noisy Named Entity Recognition"></a>MultiCoNER v2: a Large Multilingual dataset for Fine-grained and Noisy Named Entity Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13213">http://arxiv.org/abs/2310.13213</a></li>
<li>repo_url: None</li>
<li>paper_authors: Besnik Fetahu, Zhiyu Chen, Sudipta Kar, Oleg Rokhlenko, Shervin Malmasi</li>
<li>for: 本文提出了一个新的名实体识别（NER）数据集，即MULTICONER V2，用于解决细化的名实体识别问题。</li>
<li>methods: 本文使用了开源资源如Wikipedia和Wikidata来编译数据集，并在多语言环境下进行了评估。</li>
<li>results: evaluation表明，MULTICONER V2数据集具有较低的精度，macro-F1&#x3D;0.63（所有语言），并且对实体干扰具有更大的影响，相比于上下文干扰。<details>
<summary>Abstract</summary>
We present MULTICONER V2, a dataset for fine-grained Named Entity Recognition covering 33 entity classes across 12 languages, in both monolingual and multilingual settings. This dataset aims to tackle the following practical challenges in NER: (i) effective handling of fine-grained classes that include complex entities like movie titles, and (ii) performance degradation due to noise generated from typing mistakes or OCR errors. The dataset is compiled from open resources like Wikipedia and Wikidata, and is publicly available. Evaluation based on the XLM-RoBERTa baseline highlights the unique challenges posed by MULTICONER V2: (i) the fine-grained taxonomy is challenging, where the scores are low with macro-F1=0.63 (across all languages), and (ii) the corruption strategy significantly impairs performance, with entity corruption resulting in 9% lower performance relative to non-entity corruptions across all languages. This highlights the greater impact of entity noise in contrast to context noise.
</details>
<details>
<summary>摘要</summary>
我们介绍MULTICONER V2 dataset，用于细化Named Entity Recognition（NER），覆盖12种语言和33种实体类。该dataset的目标是解决NER中的两个实际挑战：（i）精细类实体，如电影标题，的处理，（ii）由 typing mistakes 或 OCR errors 生成的噪声的影响。dataset 从开源资源Wikipedia和Wikidata中 compile，并公共可用。使用XLM-RoBERTa基eline进行评估，显示MULTICONER V2具有以下唯一挑战：（i）细化分类困难，macro-F1 = 0.63（所有语言），（ii）损害策略对性能产生显著影响，实体损害相对于非实体损害在所有语言上下降9%。这反映了实体噪声比 контекст噪声更大的影响。
</details></li>
</ul>
<hr>
<h2 id="Primacy-Effect-of-ChatGPT"><a href="#Primacy-Effect-of-ChatGPT" class="headerlink" title="Primacy Effect of ChatGPT"></a>Primacy Effect of ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13206">http://arxiv.org/abs/2310.13206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiwei Wang, Yujun Cai, Muhao Chen, Yuxuan Liang, Bryan Hooi</li>
<li>For: The paper studies the primacy effect of ChatGPT, which is the tendency of selecting the labels at earlier positions as the answer.* Methods: The paper uses ChatGPT to query the model with prompts containing the question and candidate labels, and analyzes the model’s decision-making process.* Results: The paper finds that ChatGPT’s decision is sensitive to the order of labels in the prompt, and the model has a higher chance of selecting the labels at earlier positions as the answer.Here’s the simplified Chinese text for the three information points:* For: 这篇论文研究了ChatGPT模型中的主导效应，即在提问中选择早些位置的标签作为答案。* Methods: 论文使用ChatGPT模型来查询提问和候选标签，并分析模型的决策过程。* Results: 论文发现ChatGPT的决策受提问中标签顺序的影响，模型更有可能选择提问中早些位置的标签作为答案。<details>
<summary>Abstract</summary>
Instruction-tuned large language models (LLMs), such as ChatGPT, have led to promising zero-shot performance in discriminative natural language understanding (NLU) tasks. This involves querying the LLM using a prompt containing the question, and the candidate labels to choose from. The question-answering capabilities of ChatGPT arise from its pre-training on large amounts of human-written text, as well as its subsequent fine-tuning on human preferences, which motivates us to ask: Does ChatGPT also inherits humans' cognitive biases? In this paper, we study the primacy effect of ChatGPT: the tendency of selecting the labels at earlier positions as the answer. We have two main findings: i) ChatGPT's decision is sensitive to the order of labels in the prompt; ii) ChatGPT has a clearly higher chance to select the labels at earlier positions as the answer. We hope that our experiments and analyses provide additional insights into building more reliable ChatGPT-based solutions. We release the source code at https://github.com/wangywUST/PrimacyEffectGPT.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM），如ChatGPT，在推导性自然语言理解（NLU）任务中表现出了可喜的零shot性。这 involves 使用提示符 containing 问题和候选标签，并让 LLM 回答问题。 ChatGPT 的问答能力来自于其在大量人类写的文本上进行预训练，以及其后续的人类偏好的 fine-tuning，这使我们想问： ChatGPT 也继承了人类的认知偏见吗？在这篇论文中，我们研究 ChatGPT 的 primacy effect：提示符中标签的顺序对 ChatGPT 的决策产生影响。我们有两个主要发现：i）ChatGPT 的决策受提示符中标签的顺序影响；ii）ChatGPT 对提示符中早期位置的标签有明显更高的选择概率。我们希望通过我们的实验和分析，为建立更可靠的 ChatGPT-based 解决方案提供更多的意见。我们在 GitHub 上发布了源代码，请参考 <https://github.com/wangywUST/PrimacyEffectGPT>。
</details></li>
</ul>
<hr>
<h2 id="Towards-Detecting-Contextual-Real-Time-Toxicity-for-In-Game-Chat"><a href="#Towards-Detecting-Contextual-Real-Time-Toxicity-for-In-Game-Chat" class="headerlink" title="Towards Detecting Contextual Real-Time Toxicity for In-Game Chat"></a>Towards Detecting Contextual Real-Time Toxicity for In-Game Chat</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18330">http://arxiv.org/abs/2310.18330</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zachary Yang, Nicolas Grenan-Godbout, Reihaneh Rabbany</li>
<li>for: 这篇论文是为了实现在在线环境中实时探测毒性内容而写的。</li>
<li>methods: 这篇论文使用了一种简单可扩展的模型，该模型可以在实时聊天中可靠地检测毒性内容，并包括聊天历史和元数据。</li>
<li>results: 该模型在多player游戏中表现出色，可以成功地检测毒性内容，并且可以在聊天报告后进行后勤调节，成功标记82.1%的聊天报告用户，准确率为90.0%。<details>
<summary>Abstract</summary>
Real-time toxicity detection in online environments poses a significant challenge, due to the increasing prevalence of social media and gaming platforms. We introduce ToxBuster, a simple and scalable model that reliably detects toxic content in real-time for a line of chat by including chat history and metadata. ToxBuster consistently outperforms conventional toxicity models across popular multiplayer games, including Rainbow Six Siege, For Honor, and DOTA 2. We conduct an ablation study to assess the importance of each model component and explore ToxBuster's transferability across the datasets. Furthermore, we showcase ToxBuster's efficacy in post-game moderation, successfully flagging 82.1% of chat-reported players at a precision level of 90.0%. Additionally, we show how an additional 6% of unreported toxic players can be proactively moderated.
</details>
<details>
<summary>摘要</summary>
实时恶意检测在在线环境中具有重要挑战，由于社交媒体和游戏平台的广泛使用。我们介绍ToxBuster，一种简单可扩展的模型，可靠地在实时中检测恶意内容，并包括交谈历史和元数据。ToxBuster在流行的多人游戏中，如雨丝六世、荣誉之战和DOTA 2等，consistently outperforms conventional toxicity models。我们进行了减少研究，以评估模型组件的重要性，并探索ToxBuster的可转移性。此外，我们展示了ToxBuster在后期 Moderation 中的效果，成功地标记了82.1%的交谈报告用户，准确率为90.0%。此外，我们还表明了一个额外的6%的恶意用户可以被早期 Moderation。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/20/cs.AI_2023_10_20/" data-id="clpxp03v3005wfm88elq8e7v5" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/20/cs.CL_2023_10_20/" class="article-date">
  <time datetime="2023-10-20T11:00:00.000Z" itemprop="datePublished">2023-10-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/20/cs.CL_2023_10_20/">cs.CL - 2023-10-20</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Not-all-Fake-News-is-Written-A-Dataset-and-Analysis-of-Misleading-Video-Headlines"><a href="#Not-all-Fake-News-is-Written-A-Dataset-and-Analysis-of-Misleading-Video-Headlines" class="headerlink" title="Not all Fake News is Written: A Dataset and Analysis of Misleading Video Headlines"></a>Not all Fake News is Written: A Dataset and Analysis of Misleading Video Headlines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13859">http://arxiv.org/abs/2310.13859</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yoo Yeon Sung, Jordan Boyd-Graber, Naeemul Hassan</li>
<li>for: 本研究旨在提供一个多模态视频谎导标注数据集，以便更好地检测视频标题的谎导性。</li>
<li>methods: 该研究使用了现有资源的多模态基线测试方法，并对标题的谎导性进行了分析。</li>
<li>results: 研究发现，基线测试方法在检测视频标题的谎导性方面具有较高的准确率。此外，对标题的谎导性进行了更深入的分析，可以更好地理解annotators的背景和视频内容之间的关系。<details>
<summary>Abstract</summary>
Polarization and the marketplace for impressions have conspired to make navigating information online difficult for users, and while there has been a significant effort to detect false or misleading text, multimodal datasets have received considerably less attention. To complement existing resources, we present multimodal Video Misleading Headline (VMH), a dataset that consists of videos and whether annotators believe the headline is representative of the video's contents. After collecting and annotating this dataset, we analyze multimodal baselines for detecting misleading headlines. Our annotation process also focuses on why annotators view a video as misleading, allowing us to better understand the interplay of annotators' background and the content of the videos.
</details>
<details>
<summary>摘要</summary>
政治化和市场化的影响使得在线信息搜索变得更加困难，虽然有很大努力来检测false或误导性的文本，但多媒体数据集获得了相对较少的关注。为了补充现有资源，我们提供了多媒体视频误导头条（VMH）数据集，该数据集包括视频和头条是否正确表示视频内容。我们收集和标注这个数据集后，分析多媒体基线 для检测误导头条。我们的注释过程还关注了annotators的背景和视频内容之间的交互，帮助我们更好地理解annotators的背景和视频内容之间的关系。
</details></li>
</ul>
<hr>
<h2 id="Implications-of-Annotation-Artifacts-in-Edge-Probing-Test-Datasets"><a href="#Implications-of-Annotation-Artifacts-in-Edge-Probing-Test-Datasets" class="headerlink" title="Implications of Annotation Artifacts in Edge Probing Test Datasets"></a>Implications of Annotation Artifacts in Edge Probing Test Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13856">http://arxiv.org/abs/2310.13856</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/josh1108/eptest">https://github.com/josh1108/eptest</a></li>
<li>paper_authors: Sagnik Ray Choudhury, Jushaan Kalra</li>
<li>for: 这个论文旨在检验语言模型（LLM）中的语法知识是否被编码在字符表示中。</li>
<li>methods: 论文使用了edge probing测试来检验LLM的语法知识，并对常用的edge probing测试集合进行了分析，发现这些测试集合带有各种偏见。</li>
<li>results: 论文发现，当暴露出偏见后，LLMencoder和随机编码器之间的差异变得更加明显，而使用信息理论 probes 可以更好地检验LLM的语法知识。<details>
<summary>Abstract</summary>
Edge probing tests are classification tasks that test for grammatical knowledge encoded in token representations coming from contextual encoders such as large language models (LLMs). Many LLM encoders have shown high performance in EP tests, leading to conjectures about their ability to encode linguistic knowledge. However, a large body of research claims that the tests necessarily do not measure the LLM's capacity to encode knowledge, but rather reflect the classifiers' ability to learn the problem. Much of this criticism stems from the fact that often the classifiers have very similar accuracy when an LLM vs a random encoder is used. Consequently, several modifications to the tests have been suggested, including information theoretic probes. We show that commonly used edge probing test datasets have various biases including memorization. When these biases are removed, the LLM encoders do show a significant difference from the random ones, even with the simple non-information theoretic probes.
</details>
<details>
<summary>摘要</summary>
edge probing 测试是一种分类任务，用于测试语言模型（LLM）中的 grammatical 知识编码。许多 LLM 编码器在 EP 测试中表现出色，导致人们对它们的语言知识编码能力的推测。然而，许多研究表明，这些测试并不测试 LLM 的语言知识编码能力，而是测试分类器对问题的学习能力。这些批评的原因在于，经常情况下，使用 LLM 和随机编码器时，分类器的准确率几乎相同。为了解决这个问题，有许多修改的建议，包括信息理论的探测。我们发现，常用的 Edge probing 测试数据集具有各种偏见，包括记忆。当这些偏见被除去后，LLM 编码器与随机编码器之间的差别变得更加明显，жеlack 简单的非信息理论的探测也能够准确地捕捉这个差别。
</details></li>
</ul>
<hr>
<h2 id="Ecologically-Valid-Explanations-for-Label-Variation-in-NLI"><a href="#Ecologically-Valid-Explanations-for-Label-Variation-in-NLI" class="headerlink" title="Ecologically Valid Explanations for Label Variation in NLI"></a>Ecologically Valid Explanations for Label Variation in NLI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13850">http://arxiv.org/abs/2310.13850</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/njjiang/livenli">https://github.com/njjiang/livenli</a></li>
<li>paper_authors: Nan-Jiang Jiang, Chenhao Tan, Marie-Catherine de Marneffe</li>
<li>for: 这个论文是为了解决自然语言推理（NLI）任务中的人类标注差异（annotation disagreement）问题。</li>
<li>methods: 作者们使用了一个英文 dataset，名为 LiveNLI，包含 1,415 个生动的解释（annotators explain the NLI labels they chose），以及 122 个 MNLI 项目（每个项目都有至少 10 个解释）。</li>
<li>results: 研究发现，LiveNLI 的解释确实证明了人们可以系统性地有不同的解释，并且在同一个标签下存在内部差异：annotators 可能选择相同的标签，但是有不同的理由。这表明，解释是在总体上 navigation 标签 интер�прета读取的关键。然而，通过几个 prompt 测试，作者发现大语言模型可以生成有效和有用的解释，但也可能生成不合理的解释，这表明了进一步改进的方向。<details>
<summary>Abstract</summary>
Human label variation, or annotation disagreement, exists in many natural language processing (NLP) tasks, including natural language inference (NLI). To gain direct evidence of how NLI label variation arises, we build LiveNLI, an English dataset of 1,415 ecologically valid explanations (annotators explain the NLI labels they chose) for 122 MNLI items (at least 10 explanations per item). The LiveNLI explanations confirm that people can systematically vary on their interpretation and highlight within-label variation: annotators sometimes choose the same label for different reasons. This suggests that explanations are crucial for navigating label interpretations in general. We few-shot prompt large language models to generate explanations but the results are inconsistent: they sometimes produces valid and informative explanations, but it also generates implausible ones that do not support the label, highlighting directions for improvement.
</details>
<details>
<summary>摘要</summary>
人类标签变化，或者注释不一致，是许多自然语言处理（NLP）任务中的常见问题，包括自然语言推理（NLI）。为了获得直接证据，我们建立了LiveNLI，一个英语dataset，包含1,415个生动有效的解释（拟标者解释选择的NLI标签），对122个MNLI项目进行了至少10个解释。LiveNLI解释表明，人们可以系统地变化其 интерпретаion，并且在标签内部存在差异：拟标者经常选择同一个标签，但是由不同的理由。这表明，解释是在标签 интерпретаion中 Navigation 的关键。我们使用几个描述符提示大型自然语言模型生成解释，但是结果是不一致的：它们有时生成有效和有用的解释，但也可能生成不可能的解释，不支持标签， highlighting 改进的方向。
</details></li>
</ul>
<hr>
<h2 id="Foundation-Model’s-Embedded-Representations-May-Detect-Distribution-Shift"><a href="#Foundation-Model’s-Embedded-Representations-May-Detect-Distribution-Shift" class="headerlink" title="Foundation Model’s Embedded Representations May Detect Distribution Shift"></a>Foundation Model’s Embedded Representations May Detect Distribution Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13836">http://arxiv.org/abs/2310.13836</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Tsou, Max Vargas, Andrew Engel, Tony Chiang</li>
<li>for: 这个研究探讨了深度学习模型在不同任务和环境中进行转移学习（TL）时，模型的泛化能力是否受到训练和测试数据集之间的分布偏移影响。</li>
<li>methods: 作者使用了一个预训练的 GPT-2 模型，并将其转移到 Sentiment140 数据集上进行 sentiment classification。</li>
<li>results: 作者发现，Sentiment140 的测试数据集 $M$ 不是从同一个分布中采样的，因此训练于 $P$ 并测试于 $M$ 不能准确地衡量模型在 sentiment classification 中的泛化能力。<details>
<summary>Abstract</summary>
Distribution shifts between train and test datasets obscure our ability to understand the generalization capacity of neural network models. This topic is especially relevant given the success of pre-trained foundation models as starting points for transfer learning (TL) models across tasks and contexts. We present a case study for TL on a pre-trained GPT-2 model onto the Sentiment140 dataset for sentiment classification. We show that Sentiment140's test dataset $M$ is not sampled from the same distribution as the training dataset $P$, and hence training on $P$ and measuring performance on $M$ does not actually account for the model's generalization on sentiment classification.
</details>
<details>
<summary>摘要</summary>
发布分布Shift between train和test datasets obscures our ability to understand the generalization capacity of neural network models. This topic is especially relevant given the success of pre-trained foundation models as starting points for transfer learning (TL) models across tasks and contexts. We present a case study for TL on a pre-trained GPT-2 model onto the Sentiment140 dataset for sentiment classification. We show that Sentiment140's test dataset $M$ is not sampled from the same distribution as the training dataset $P$, and hence training on $P$ and measuring performance on $M$ does not actually account for the model's generalization on sentiment classification.Here's the breakdown of the translation:* 发布 (fābù) - distribution* 分布 (bìbù) - shift* between (between) - between* train (train) - train* and (and) - and* test (test) - test* datasets (dataset) - datasets* obscures (obscures) - obscures* our (our) - our* ability (ability) - ability* to (to) - to* understand (understand) - understand* the (the) - the* generalization (generalization) - generalization* capacity (capacity) - capacity* of (of) - of* neural (neural) - neural* network (network) - network* models (model) - models* This (this) - this* topic (topic) - topic* is (is) - is* especially (especially) - especially* relevant (relevant) - relevant* given (given) - given* the (the) - the* success (success) - success* of (of) - of* pre-trained (pre-trained) - pre-trained* foundation (foundation) - foundation* models (model) - models* as (as) - as* starting (starting) - starting* points (points) - points* for (for) - for* transfer (transfer) - transfer* learning (learning) - learning* (TL) (TL) - TL* models (model) - models* across (across) - across* tasks (task) - tasks* and (and) - and* contexts (contexts) - contexts* We (we) - we* present (present) - present* a (a) - a* case (case) - case* study (study) - study* for (for) - for* TL (TL) - TL* on (on) - on* a (a) - a* pre-trained (pre-trained) - pre-trained* GPT-2 (GPT-2) - GPT-2* model (model) - model* onto (onto) - onto* the (the) - the* Sentiment140 (Sentiment140) - Sentiment140* dataset (dataset) - dataset* for (for) - for* sentiment (sentiment) - sentiment* classification (classification) - classification* We (we) - we* show (show) - show* that (that) - that* Sentiment140's (Sentiment140's) - Sentiment140's* test (test) - test* dataset (dataset) - dataset* $M$ (M) - M* is (is) - is* not (not) - not* sampled (sampled) - sampled* from (from) - from* the (the) - the* same (same) - same* distribution (distribution) - distribution* as (as) - as* the (the) - the* training (training) - training* dataset (dataset) - dataset* $P$ (P) - P* and (and) - and* hence (hence) - hence* training (training) - training* on (on) - on* $P$ (P) - P* and (and) - and* measuring (measuring) - measuring* performance (performance) - performance* on (on) - on* $M$ (M) - M* does (do) - does* not (not) - not* actually (actually) - actually* account (account) - account* for (for) - for* the (the) - the* model's (model's) - model's* generalization (generalization) - generalization* on (on) - on* sentiment (sentiment) - sentiment* classification (classification) - classification
</details></li>
</ul>
<hr>
<h2 id="Plausibility-Processing-in-Transformer-Language-Models-Focusing-on-the-Role-of-Attention-Heads-in-GPT"><a href="#Plausibility-Processing-in-Transformer-Language-Models-Focusing-on-the-Role-of-Attention-Heads-in-GPT" class="headerlink" title="Plausibility Processing in Transformer Language Models: Focusing on the Role of Attention Heads in GPT"></a>Plausibility Processing in Transformer Language Models: Focusing on the Role of Attention Heads in GPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13824">http://arxiv.org/abs/2310.13824</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/soohyunryu/plausibility-processing-transformers">https://github.com/soohyunryu/plausibility-processing-transformers</a></li>
<li>paper_authors: Soo Hyun Ryu</li>
<li>for: 本研究旨在探索transformer语言模型如何处理 semantics知识,特别是关于名动词关系的可能性。</li>
<li>methods: 本研究使用GPT2语言模型进行实验,通过分析GPT2的注意头来探讨它如何处理可能性。</li>
<li>results: 研究发现GPT2在可能性处理方面与人类更相似,并且在注意头中包含了知识的可能性信息。此外，研究还发现GPT2中的注意头可以共同影响语言模型的可能性处理能力,但各个注意头的可能性检测性能与其贡献相对强度不一致。<details>
<summary>Abstract</summary>
The goal of this paper is to explore how Transformer language models process semantic knowledge, especially regarding the plausibility of noun-verb relations. First, I demonstrate GPT2 exhibits a higher degree of similarity with humans in plausibility processing compared to other Transformer language models. Next, I delve into how knowledge of plausibility is contained within attention heads of GPT2 and how these heads causally contribute to GPT2's plausibility processing ability. Through several experiments, it was found that: i) GPT2 has a number of attention heads that detect plausible noun-verb relationships; ii) these heads collectively contribute to the Transformer's ability to process plausibility, albeit to varying degrees; and iii) attention heads' individual performance in detecting plausibility does not necessarily correlate with how much they contribute to GPT2's plausibility processing ability.
</details>
<details>
<summary>摘要</summary>
本文的目的是探讨 transformer 语言模型在Semantic Knowledge 处理方面的表现，特别是 noun-verb 关系的可能性。首先，我展示 GPT2 与人类更相似在可能性处理方面的表现。然后，我探究 GPT2 中可能性知识的含义以及这些知识如何通过注意头来影响 GPT2 的可能性处理能力。通过一些实验，我发现：1. GPT2 有许多检测可能性 noun-verb 关系的注意头;2. 这些注意头共同 contribuite 到 transformer 的可能性处理能力, 虽然不同的注意头在可能性处理中的表现不同;3. 注意头的个体表现在检测可能性方面与 GPT2 的可能性处理能力相互关系不一定。
</details></li>
</ul>
<hr>
<h2 id="Yet-Another-Model-for-Arabic-Dialect-Identification"><a href="#Yet-Another-Model-for-Arabic-Dialect-Identification" class="headerlink" title="Yet Another Model for Arabic Dialect Identification"></a>Yet Another Model for Arabic Dialect Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13812">http://arxiv.org/abs/2310.13812</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ajinkya Kulkarni, Hanan Aldarmaki</li>
<li>for: 这个研究旨在开发一个用于阿拉伯语口语识别（ADI）模型，能够在两个标准数据集上（ADI-5和ADI-17）上 consistently outperform  previously published results。</li>
<li>methods: 该模型采用了两种不同的架构变体：ResNet和ECAPA-TDNN，以及两种不同的声学特征：MFCCs和自动提取的UniSpeech-SAT Large特征，以及这些特征的混合。</li>
<li>results: 研究发现，ECAPA-TDNN网络单独使用表现比ResNet更高，而使用UniSpeech-SAT特征比MFCCs更高。此外，混合所有变体的模型一直outperform 单独的模型。最佳模型的准确率为84.7%和96.9%。<details>
<summary>Abstract</summary>
In this paper, we describe a spoken Arabic dialect identification (ADI) model for Arabic that consistently outperforms previously published results on two benchmark datasets: ADI-5 and ADI-17. We explore two architectural variations: ResNet and ECAPA-TDNN, coupled with two types of acoustic features: MFCCs and features exratected from the pre-trained self-supervised model UniSpeech-SAT Large, as well as a fusion of all four variants. We find that individually, ECAPA-TDNN network outperforms ResNet, and models with UniSpeech-SAT features outperform models with MFCCs by a large margin. Furthermore, a fusion of all four variants consistently outperforms individual models. Our best models outperform previously reported results on both datasets, with accuracies of 84.7% and 96.9% on ADI-5 and ADI-17, respectively.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们描述了一个用于阿拉伯语言分类（ADI）模型，该模型在两个标准数据集上（ADI-5和ADI-17）上表现出色， persistently 超越了之前发表的结果。我们研究了两种建筑方案：ResNet和ECAPA-TDNN，同时使用了两种声学特征：MFCC和UniSpeech-SAT Large自动学习模型中提取的特征。我们发现，ECAPA-TDNN网络单独使用表现 луч于ResNet，而使用UniSpeech-SAT特征的模型比使用MFCC特征的模型表现出了大幅提升。此外，我们发现将所有四种变体进行混合，可以一直保持模型的表现。我们的最佳模型在ADI-5和ADI-17数据集上的准确率分别为84.7%和96.9%，这些结果比之前报道的结果更高。
</details></li>
</ul>
<hr>
<h2 id="Evaluation-Metrics-in-the-Era-of-GPT-4-Reliably-Evaluating-Large-Language-Models-on-Sequence-to-Sequence-Tasks"><a href="#Evaluation-Metrics-in-the-Era-of-GPT-4-Reliably-Evaluating-Large-Language-Models-on-Sequence-to-Sequence-Tasks" class="headerlink" title="Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks"></a>Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13800">http://arxiv.org/abs/2310.13800</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/protagolabs/seq2seq_llm_evaluation">https://github.com/protagolabs/seq2seq_llm_evaluation</a></li>
<li>paper_authors: Andrea Sottana, Bin Liang, Kai Zou, Zheng Yuan</li>
<li>for: 该研究旨在提高当前生成模型性能的评估方法，通过对多种开源和关闭源生成语言模型（LLMs）在文本概要、简化和 grammatical error correction（GEC）三个 NATLP 标准准则上进行 préliminaire 和混合评估。</li>
<li>methods: 该研究使用了自动和人工评估方法来评估多种生成模型的性能，包括 GPT-4 作为评估器。</li>
<li>results: 研究发现，ChatGPT 在人工评估中常常超过许多其他流行模型，但在经典自动评估指标上得分很低。此外，人工评估人员认为金标准样本质量较差，而且模型输出与人工评估人员的判断相对较少吻合。最后，研究发现，GPT-4 可以reasonably closely align with human judgment across tasks, with a lower alignment in the GEC task.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) evaluation is a patchy and inconsistent landscape, and it is becoming clear that the quality of automatic evaluation metrics is not keeping up with the pace of development of generative models. We aim to improve the understanding of current models' performance by providing a preliminary and hybrid evaluation on a range of open and closed-source generative LLMs on three NLP benchmarks: text summarisation, text simplification and grammatical error correction (GEC), using both automatic and human evaluation. We also explore the potential of the recently released GPT-4 to act as an evaluator. We find that ChatGPT consistently outperforms many other popular models according to human reviewers on the majority of metrics, while scoring much more poorly when using classic automatic evaluation metrics. We also find that human reviewers rate the gold reference as much worse than the best models' outputs, indicating the poor quality of many popular benchmarks. Finally, we find that GPT-4 is capable of ranking models' outputs in a way which aligns reasonably closely to human judgement despite task-specific variations, with a lower alignment in the GEC task.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的评估是一个含糊不清的景象，而且现在的自动评估指标质量不能跟上生成模型的发展。我们想要提高当前模型的表现理解，我们提供了一些先进的混合评估方法，在多种开源和关闭源生成LLM上进行了三个NLPbenchmark：文本概要、文本简化和语法错误修复（GEC）。我们还探索了最近发布的GPT-4是否可以作为评估器。我们发现，ChatGPT在人工评分者的评估中一直表现出色，而自动评估指标中的评分则远低。我们还发现，人工评分者评估金标 referencemuch worse than最佳模型的输出，这表明许多流行的benchmark的质量不高。最后，我们发现GPT-4可以在不同任务上对模型的输出进行排序，与人类判断相对吻合，但在GEC任务中的吻合较低。
</details></li>
</ul>
<hr>
<h2 id="A-Unified-View-of-Evaluation-Metrics-for-Structured-Prediction"><a href="#A-Unified-View-of-Evaluation-Metrics-for-Structured-Prediction" class="headerlink" title="A Unified View of Evaluation Metrics for Structured Prediction"></a>A Unified View of Evaluation Metrics for Structured Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13793">http://arxiv.org/abs/2310.13793</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wanmok/metametric">https://github.com/wanmok/metametric</a></li>
<li>paper_authors: Yunmo Chen, William Gantt, Tongfei Chen, Aaron Steven White, Benjamin Van Durme</li>
<li>for: 这 paper 旨在提供一个概念框架，用于统一不同结构预测任务（如事件和关系抽取、语法和 semantics 解析）的评估指标。</li>
<li>methods: 该框架基于对输出结果的对象化表示，并通过匹配共同结构来 derivation 评估指标，可能会进行Normalization。</li>
<li>results: 作者示出了一些任务的常用指标可以简洁地表达为该框架中的一部分，并且可以在底层上通过输出结构来自然地 derivation 新的指标。同时，作者还提出了一些任务特点所带来的指标设计决策，并对现有指标进行修改。<details>
<summary>Abstract</summary>
We present a conceptual framework that unifies a variety of evaluation metrics for different structured prediction tasks (e.g. event and relation extraction, syntactic and semantic parsing). Our framework requires representing the outputs of these tasks as objects of certain data types, and derives metrics through matching of common substructures, possibly followed by normalization. We demonstrate how commonly used metrics for a number of tasks can be succinctly expressed by this framework, and show that new metrics can be naturally derived in a bottom-up way based on an output structure. We release a library that enables this derivation to create new metrics. Finally, we consider how specific characteristics of tasks motivate metric design decisions, and suggest possible modifications to existing metrics in line with those motivations.
</details>
<details>
<summary>摘要</summary>
我们提出了一个概念框架，它可以统一不同的结构预测任务（如事件和关系抽取、 sintactic和semantic 分析）的评估指标。我们的框架需要将这些任务的输出表示为特定数据类型的对象，然后通过匹配通用的子结构来 derivate 指标，可能会 seguido de normalización。我们示例了一些任务上常用的指标可以简洁地表达在我们的框架中，并证明新的指标可以从输出结构的底层方式 derivation。我们释放了一个库，它可以帮助 derivation 新的指标。最后，我们考虑了特定任务的特征如何驱动指标设计选择，并建议可能的修改以适应这些驱动力。
</details></li>
</ul>
<hr>
<h2 id="How-Much-Consistency-Is-Your-Accuracy-Worth"><a href="#How-Much-Consistency-Is-Your-Accuracy-Worth" class="headerlink" title="How Much Consistency Is Your Accuracy Worth?"></a>How Much Consistency Is Your Accuracy Worth?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13781">http://arxiv.org/abs/2310.13781</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/NitikaRaj1/bug-free-goggles">https://github.com/NitikaRaj1/bug-free-goggles</a></li>
<li>paper_authors: Jacob K. Johnson, Ana Marasović</li>
<li>for: 评估模型的一致性和稳定性</li>
<li>methods: 使用对Minimally Different Examples(MDEs)的评估，并引入相对一致性概率来衡量模型的一致性</li>
<li>results: 提出了一种新的一致性评估方法，并发现模型的一致性和稳定性可以通过Relative Consistency来进行评估，并且模型的100%相对一致性可以达到一致性峰值。<details>
<summary>Abstract</summary>
Contrast set consistency is a robustness measurement that evaluates the rate at which a model correctly responds to all instances in a bundle of minimally different examples relying on the same knowledge. To draw additional insights, we propose to complement consistency with relative consistency -- the probability that an equally accurate model would surpass the consistency of the proposed model, given a distribution over possible consistencies. Models with 100% relative consistency have reached a consistency peak for their accuracy. We reflect on prior work that reports consistency in contrast sets and observe that relative consistency can alter the assessment of a model's consistency compared to another. We anticipate that our proposed measurement and insights will influence future studies aiming to promote consistent behavior in models.
</details>
<details>
<summary>摘要</summary>
“对比集合一致性”是一种Robustness度量，用于评估模型在一组最小差异示例中具有相同知识的情况下，其是否能够正确回应所有示例。为了增加更多的洞察，我们提议在consistency的基础上添加相对一致性——模型的准确率在可能的一致性 Distribution 中的概率。如果模型的相对一致性达到100%，则表示它已经达到了准确率的峰值。我们回顾先前的研究，发现consistency在对比集合中的报告和相对一致性可能会改变模型的一致性评估。我们预计，我们所提出的度量和洞察将影响未来关于模型行为的一致性的研究。
</details></li>
</ul>
<hr>
<h2 id="Seq2seq-is-All-You-Need-for-Coreference-Resolution"><a href="#Seq2seq-is-All-You-Need-for-Coreference-Resolution" class="headerlink" title="Seq2seq is All You Need for Coreference Resolution"></a>Seq2seq is All You Need for Coreference Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13774">http://arxiv.org/abs/2310.13774</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wenzhengzhang/seq2seqcoref">https://github.com/wenzhengzhang/seq2seqcoref</a></li>
<li>paper_authors: Wenzheng Zhang, Sam Wiseman, Karl Stratos</li>
<li>for: This paper aims to challenge the assumption that task-specific models are necessary for coreference resolution, and instead, presents a simple and effective approach using a pre-trained seq2seq transformer.</li>
<li>methods: The proposed method finetunes a pre-trained seq2seq transformer to map an input document to a tagged sequence encoding the coreference annotation, and an especially simple seq2seq approach that generates only tagged spans rather than the spans interleaved with the original text.</li>
<li>results: The model outperforms or closely matches the best coreference systems in the literature on an array of datasets, and the analysis shows that the model size, the amount of supervision, and the choice of sequence representations are key factors in performance.<details>
<summary>Abstract</summary>
Existing works on coreference resolution suggest that task-specific models are necessary to achieve state-of-the-art performance. In this work, we present compelling evidence that such models are not necessary. We finetune a pretrained seq2seq transformer to map an input document to a tagged sequence encoding the coreference annotation. Despite the extreme simplicity, our model outperforms or closely matches the best coreference systems in the literature on an array of datasets. We also propose an especially simple seq2seq approach that generates only tagged spans rather than the spans interleaved with the original text. Our analysis shows that the model size, the amount of supervision, and the choice of sequence representations are key factors in performance.
</details>
<details>
<summary>摘要</summary>
existing works on coreference resolution suggest that task-specific models are necessary to achieve state-of-the-art performance. In this work, we present compelling evidence that such models are not necessary. We fine-tune a pre-trained seq2seq transformer to map an input document to a tagged sequence encoding the coreference annotation. Despite the extreme simplicity, our model outperforms or closely matches the best coreference systems in the literature on an array of datasets. We also propose an especially simple seq2seq approach that generates only tagged spans rather than the spans interleaved with the original text. Our analysis shows that the model size, the amount of supervision, and the choice of sequence representations are key factors in performance.Here's the translation in Traditional Chinese:现有的核心引用解析工作都建议需要任务特定的模型来 дости持最佳性能。在这个工作中，我们提供了吸引人的证据，说明这些模型不是必要的。我们精致地调整了预训练的 seq2seq transformer，将输入文档映射到标注的序列中，以表示核心引用标识。尽管非常简单，我们的模型在多个 dataset 上都能够对核心引用系统进行出色的表现，或与文献中的最佳系统相对接近。我们还提出了一种非常简单的 seq2seq 方法，将标注 span 生成成只有与原始文本混合的 span 相比。我们的分析显示，模型大小、监督量和序列表示方法是表现的关键因素。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Abstractiveness-of-Summarization-Models-through-Calibrated-Distillation"><a href="#Enhancing-Abstractiveness-of-Summarization-Models-through-Calibrated-Distillation" class="headerlink" title="Enhancing Abstractiveness of Summarization Models through Calibrated Distillation"></a>Enhancing Abstractiveness of Summarization Models through Calibrated Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13760">http://arxiv.org/abs/2310.13760</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hwanjun Song, Igor Shalyminov, Hang Su, Siffi Singh, Kaisheng Yao, Saab Mansour</li>
<li>for: 这篇论文的目的是提高抽象概括的效果，不过常常会导致抽象性减退。</li>
<li>methods: 该论文提出了一种新的方法，即DisCal，用于提高抽象概括的效果，不需要失去信息的损失。DisCal通过向学生模型提供多种假概括，使学生模型更好地学习抽象概括技巧。</li>
<li>results: 实验结果表明，DisCal比先前的方法在抽象概括练习中更高效，可以生成高度抽象和有用的概括。<details>
<summary>Abstract</summary>
Sequence-level knowledge distillation reduces the size of Seq2Seq models for more efficient abstractive summarization. However, it often leads to a loss of abstractiveness in summarization. In this paper, we propose a novel approach named DisCal to enhance the level of abstractiveness (measured by n-gram overlap) without sacrificing the informativeness (measured by ROUGE) of generated summaries. DisCal exposes diverse pseudo summaries with two supervision to the student model. Firstly, the best pseudo summary is identified in terms of abstractiveness and informativeness and used for sequence-level distillation. Secondly, their ranks are used to ensure the student model to assign higher prediction scores to summaries with higher ranks. Our experiments show that DisCal outperforms prior methods in abstractive summarization distillation, producing highly abstractive and informative summaries.
</details>
<details>
<summary>摘要</summary>
序列级知识填充可以降低Seq2Seq模型的大小，以实现更高效的抽象概要。然而，这经常会导致抽象性的减少。在这篇论文中，我们提出了一种新的方法，即DisCal，以提高抽象性（通过n-gram重叠度衡量）无需牺牲生成的概要的信息性（通过ROUGE衡量）。DisCal向学生模型提供多种假概要，并对其进行序列级填充。首先，我们选择最佳的假概要，根据抽象性和信息性进行评价。其次，我们使用其排名来确保学生模型对概要的预测分数进行较高的分配。我们的实验表明，DisCal在抽象概要填充中超过了先前的方法，生成了高度抽象和有用的概要。
</details></li>
</ul>
<hr>
<h2 id="ALDi-Quantifying-the-Arabic-Level-of-Dialectness-of-Text"><a href="#ALDi-Quantifying-the-Arabic-Level-of-Dialectness-of-Text" class="headerlink" title="ALDi: Quantifying the Arabic Level of Dialectness of Text"></a>ALDi: Quantifying the Arabic Level of Dialectness of Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13747">http://arxiv.org/abs/2310.13747</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amr-keleg/aldi">https://github.com/amr-keleg/aldi</a></li>
<li>paper_authors: Amr Keleg, Sharon Goldwater, Walid Magdy</li>
<li>for: 本研究旨在提供一个可以辨识阿拉伯语言使用者在不同情况下的语言风格选择的方法。</li>
<li>methods: 本研究使用了一个新的语言水平分布（ALDi）来评估阿拉伯语言的方言差异。 ALDi 是一个连续的语言变量，可以在句子水平上量度阿拉伯语言使用者对语言风格的选择。</li>
<li>results: 研究发现，使用 ALDi 可以对不同的阿拉伯语言资料集进行有效的辨识，并且可以显示阿拉伯语言使用者在不同情况下的语言风格选择。<details>
<summary>Abstract</summary>
Transcribed speech and user-generated text in Arabic typically contain a mixture of Modern Standard Arabic (MSA), the standardized language taught in schools, and Dialectal Arabic (DA), used in daily communications. To handle this variation, previous work in Arabic NLP has focused on Dialect Identification (DI) on the sentence or the token level. However, DI treats the task as binary, whereas we argue that Arabic speakers perceive a spectrum of dialectness, which we operationalize at the sentence level as the Arabic Level of Dialectness (ALDi), a continuous linguistic variable. We introduce the AOC-ALDi dataset (derived from the AOC dataset), containing 127,835 sentences (17% from news articles and 83% from user comments on those articles) which are manually labeled with their level of dialectness. We provide a detailed analysis of AOC-ALDi and show that a model trained on it can effectively identify levels of dialectness on a range of other corpora (including dialects and genres not included in AOC-ALDi), providing a more nuanced picture than traditional DI systems. Through case studies, we illustrate how ALDi can reveal Arabic speakers' stylistic choices in different situations, a useful property for sociolinguistic analyses.
</details>
<details>
<summary>摘要</summary>
传统的阿拉伯语言处理（NLP）研究偏向于干rn Dialect Identification（DI），即 sentence或token level上的方言识别。然而，我们认为阿拉伯语言使用者看到了方言强度的连续变量，我们称之为阿拉伯语言层次（ALDi）。我们引入了AOC-ALDi数据集（基于AOC数据集），包含127,835个句子（新闻文章占17%，用户评论占83%），每个句子都是手动标注了其方言强度。我们提供了AOC-ALDi的详细分析，并证明一个基于AOC-ALDi的模型可以有效地在其他 corpora 上识别方言强度，提供了更加细腻的图像。通过案例研究，我们示出了阿拉伯语言使用者在不同情况下的样式选择，这是社会语言分析中非常有用的特性。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Linguistic-Probes-for-Morphological-Generalization"><a href="#Exploring-Linguistic-Probes-for-Morphological-Generalization" class="headerlink" title="Exploring Linguistic Probes for Morphological Generalization"></a>Exploring Linguistic Probes for Morphological Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13686">http://arxiv.org/abs/2310.13686</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jkodner05/EMNLP2023_LingProbes">https://github.com/jkodner05/EMNLP2023_LingProbes</a></li>
<li>paper_authors: Jordan Kodner, Salam Khalifa, Sarah Payne</li>
<li>for: 这个论文主要针对的是 morphological inflection 的计算模型化。</li>
<li>methods: 这篇论文使用了语言独立的数据分割算法，并采用了语言特定的探针来测试 morphological generalization 的方面。</li>
<li>results: 对于三种 morphologically distinct 语言（英语、西班牙语、斯瓦希利语），研究发现这三个主要 morphological inflection 系统在 conjugational classes 和 feature sets 上采用了不同的总结策略，并在 both orthographic 和 phonologically transcribed inputs 上得到了证据。<details>
<summary>Abstract</summary>
Modern work on the cross-linguistic computational modeling of morphological inflection has typically employed language-independent data splitting algorithms. In this paper, we supplement that approach with language-specific probes designed to test aspects of morphological generalization. Testing these probes on three morphologically distinct languages, English, Spanish, and Swahili, we find evidence that three leading morphological inflection systems employ distinct generalization strategies over conjugational classes and feature sets on both orthographic and phonologically transcribed inputs.
</details>
<details>
<summary>摘要</summary>
现代工作中使用了跨语言计算模型来研究 morphological inflection 的计算模型通常采用语言独立的数据分割算法。在这篇论文中，我们补充了这种方法，使用语言特定的探针来测试 morphological generalization 的方面。在英语、西班牙语和斯瓦希利语三种 morphologically distinct 语言上测试这些探针，我们发现三个领先的 morphological inflection 系统在 conjugational classes 和 feature sets 上使用了不同的总结策略，并且这些策略在 both orthographic 和 phonologically transcribed inputs 上都有效。
</details></li>
</ul>
<hr>
<h2 id="Information-Value-Measuring-Utterance-Predictability-as-Distance-from-Plausible-Alternatives"><a href="#Information-Value-Measuring-Utterance-Predictability-as-Distance-from-Plausible-Alternatives" class="headerlink" title="Information Value: Measuring Utterance Predictability as Distance from Plausible Alternatives"></a>Information Value: Measuring Utterance Predictability as Distance from Plausible Alternatives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13676">http://arxiv.org/abs/2310.13676</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dmg-illc/information-value">https://github.com/dmg-illc/information-value</a></li>
<li>paper_authors: Mario Giulianelli, Sarenne Wallbridge, Raquel Fernández</li>
<li>for: 这篇论文主要用于探讨语言预测性的问题，具体来说是通过语音生成器来评估语言的信息价值。</li>
<li>methods: 论文使用了语音生成器来获得可解释的信息价值估计，并利用这些估计来研究人类理解行为中的维度。</li>
<li>results: 论文发现信息价值是written和spoken对话中语言预测性的更好的预测器，并且与词级抽象度的总和不同，可以作为词级预测性的补充。<details>
<summary>Abstract</summary>
We present information value, a measure which quantifies the predictability of an utterance relative to a set of plausible alternatives. We introduce a method to obtain interpretable estimates of information value using neural text generators, and exploit their psychometric predictive power to investigate the dimensions of predictability that drive human comprehension behaviour. Information value is a stronger predictor of utterance acceptability in written and spoken dialogue than aggregates of token-level surprisal and it is complementary to surprisal for predicting eye-tracked reading times.
</details>
<details>
<summary>摘要</summary>
我们介绍信息价值，一种量化评估话语可能性相对于一组可能的选择的度量。我们提出了使用神经网络文本生成器获取可读取的信息价值估计方法，并利用它们的心理测量预测力来调查驱动人类理解行为的维度。信息价值在书面和口语对话中比聚合各个字符度量的奇偶性和总体的奇偶性强制性更好地预测话语可CCE接受性。
</details></li>
</ul>
<hr>
<h2 id="On-Synthetic-Data-for-Back-Translation"><a href="#On-Synthetic-Data-for-Back-Translation" class="headerlink" title="On Synthetic Data for Back Translation"></a>On Synthetic Data for Back Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13675">http://arxiv.org/abs/2310.13675</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiahao004/data-for-bt">https://github.com/jiahao004/data-for-bt</a></li>
<li>paper_authors: Jiahao Xu, Yubin Ruan, Wei Bi, Guoping Huang, Shuming Shi, Lihui Chen, Lemao Liu</li>
<li>for: 本研究旨在调查Back Translation(BT)技术在Machine Translation(MT)领域中的应用，并研究如何生成更高质量的synthetic data来提高BT性能。</li>
<li>methods: 本研究采用了 teoretic和empirical方法来研究synthetic data在BT性能中的作用，并提出了一种简单 yet effective的方法来生成synthetic data，以更好地考虑质量和重要性两个因素。</li>
<li>results: 经过extensive的实验 validate that our proposed method可以 significantly improve BT性能，在WMT14 DE-EN、EN-DE和RU-EN benchmark任务上都达到了比标准基eline的性能。<details>
<summary>Abstract</summary>
Back translation (BT) is one of the most significant technologies in NMT research fields. Existing attempts on BT share a common characteristic: they employ either beam search or random sampling to generate synthetic data with a backward model but seldom work studies the role of synthetic data in the performance of BT. This motivates us to ask a fundamental question: {\em what kind of synthetic data contributes to BT performance?} Through both theoretical and empirical studies, we identify two key factors on synthetic data controlling the back-translation NMT performance, which are quality and importance. Furthermore, based on our findings, we propose a simple yet effective method to generate synthetic data to better trade off both factors so as to yield a better performance for BT. We run extensive experiments on WMT14 DE-EN, EN-DE, and RU-EN benchmark tasks. By employing our proposed method to generate synthetic data, our BT model significantly outperforms the standard BT baselines (i.e., beam and sampling based methods for data generation), which proves the effectiveness of our proposed methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Back Translation（BT）是现代机器翻译研究领域中最重要的技术之一。现有的尝试中大多数采用了扫描或随机抽样来生成反向模型的synthetic数据，但rarely有研究synthetic数据在BT性能中的作用。这引发了我们的一个基本问题：{\em 这些synthetic数据对BT性能有什么类型的影响?}通过理论和实验研究，我们确定了两个关键因素控制了反向翻译NMT性能：品质和重要性。此外，基于我们的发现，我们提议一种简单 yet effective的方法来生成synthetic数据，以更好地考虑这两个因素，以提高BT性能。我们在WMT14 DE-EN、EN-DE和RU-ENbenchmark任务上进行了广泛的实验，并证明了我们的提议的方法可以significantly outperform标准BT基elines（即扫描和随机抽样基elines），这证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="StereoMap-Quantifying-the-Awareness-of-Human-like-Stereotypes-in-Large-Language-Models"><a href="#StereoMap-Quantifying-the-Awareness-of-Human-like-Stereotypes-in-Large-Language-Models" class="headerlink" title="StereoMap: Quantifying the Awareness of Human-like Stereotypes in Large Language Models"></a>StereoMap: Quantifying the Awareness of Human-like Stereotypes in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13673">http://arxiv.org/abs/2310.13673</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sullamij/stereomap">https://github.com/sullamij/stereomap</a></li>
<li>paper_authors: Sullam Jeoung, Yubin Ge, Jana Diesner</li>
<li>for: 本研究旨在理解大语言模型（LLM）对社会群体的投影和表现，以及LLM如何在训练数据中存储和传播有害关系。</li>
<li>methods: 本研究使用了一种基于心理学理论的框架，称为StereoMap，来探索LLM对社会群体的投影。StereoMap使用心理学中已知的 sterotype Content Model（SCM），将刻画为两个维度：温暖度和能力。</li>
<li>results: 研究发现，LLM对不同社会群体的投影存在多样化的评价，包括温暖度和能力两个维度上的混合评价。此外，分析LLM的推理，研究发现LLM有时会引用社会不平等的统计数据和研究结果来支持其推理。这种做法可能反映LLM对社会不平等的认识和承认。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have been observed to encode and perpetuate harmful associations present in the training data. We propose a theoretically grounded framework called StereoMap to gain insights into their perceptions of how demographic groups have been viewed by society. The framework is grounded in the Stereotype Content Model (SCM); a well-established theory from psychology. According to SCM, stereotypes are not all alike. Instead, the dimensions of Warmth and Competence serve as the factors that delineate the nature of stereotypes. Based on the SCM theory, StereoMap maps LLMs' perceptions of social groups (defined by socio-demographic features) using the dimensions of Warmth and Competence. Furthermore, the framework enables the investigation of keywords and verbalizations of reasoning of LLMs' judgments to uncover underlying factors influencing their perceptions. Our results show that LLMs exhibit a diverse range of perceptions towards these groups, characterized by mixed evaluations along the dimensions of Warmth and Competence. Furthermore, analyzing the reasonings of LLMs, our findings indicate that LLMs demonstrate an awareness of social disparities, often stating statistical data and research findings to support their reasoning. This study contributes to the understanding of how LLMs perceive and represent social groups, shedding light on their potential biases and the perpetuation of harmful associations.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Explainable-Depression-Symptom-Detection-in-Social-Media"><a href="#Explainable-Depression-Symptom-Detection-in-Social-Media" class="headerlink" title="Explainable Depression Symptom Detection in Social Media"></a>Explainable Depression Symptom Detection in Social Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13664">http://arxiv.org/abs/2310.13664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eliseo Bao Souto, Anxo Pérez, Javier Parapar</li>
<li>for: 这研究旨在用语言模型检测和解释用户在社交平台上发布的情绪症状 markers。</li>
<li>methods: 我们使用 transformer 架构来实现这两个任务，包括分类和解释分类决策。我们还使用最新的对话式 LLMS 进行具体实现。</li>
<li>results: 我们的实验结果表明，可以同时实现良好的分类结果和可解释的决策。我们的自然语言解释可以帮助临床专业人员理解模型决策的基础。<details>
<summary>Abstract</summary>
Users of social platforms often perceive these sites as supportive spaces to post about their mental health issues. Those conversations contain important traces about individuals' health risks. Recently, researchers have exploited this online information to construct mental health detection models, which aim to identify users at risk on platforms like Twitter, Reddit or Facebook. Most of these models are centred on achieving good classification results, ignoring the explainability and interpretability of the decisions. Recent research has pointed out the importance of using clinical markers, such as the use of symptoms, to improve trust in the computational models by health professionals. In this paper, we propose using transformer-based architectures to detect and explain the appearance of depressive symptom markers in the users' writings. We present two approaches: i) train a model to classify, and another one to explain the classifier's decision separately and ii) unify the two tasks simultaneously using a single model. Additionally, for this latter manner, we also investigated the performance of recent conversational LLMs when using in-context learning. Our natural language explanations enable clinicians to interpret the models' decisions based on validated symptoms, enhancing trust in the automated process. We evaluate our approach using recent symptom-based datasets, employing both offline and expert-in-the-loop metrics to assess the quality of the explanations generated by our models. The experimental results show that it is possible to achieve good classification results while generating interpretable symptom-based explanations.
</details>
<details>
<summary>摘要</summary>
社交媒体用户们常看待这些平台为他们的心理健康问题提供支持的空间。这些对话包含了用户健康风险的重要 traces。近些年，研究人员利用这些在线信息构建了心理健康检测模型，以 identificar社交媒体上的用户风险。大多数这些模型强调得到好的分类结果，忽略了计算模型的解释性和可读性。现在的研究表明，使用临床标志（如症状使用）可以提高计算模型的信任worth。在这篇论文中，我们提议使用变换器结构来检测和解释用户写作中的抑郁症状标志。我们提出了两种方法：一是在不同的步骤中训练分类和解释模型，二是同时使用单一模型来实现这两个任务。此外，我们还 investigate了最近的对话语言模型在使用上下文学习时的表现。我们的自然语言解释使得临床专业人员可以根据验证的症状来解释计算模型的决策，提高自动化过程中的信任。我们使用最新的症状基数据集进行评估，并使用线上和专家在 Loop 中的 metric 来评估我们的模型生成的解释质量。实验结果表明，可以同时实现好的分类结果和可读的症状基本解释。
</details></li>
</ul>
<hr>
<h2 id="Arabic-Dialect-Identification-under-Scrutiny-Limitations-of-Single-label-Classification"><a href="#Arabic-Dialect-Identification-under-Scrutiny-Limitations-of-Single-label-Classification" class="headerlink" title="Arabic Dialect Identification under Scrutiny: Limitations of Single-label Classification"></a>Arabic Dialect Identification under Scrutiny: Limitations of Single-label Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13661">http://arxiv.org/abs/2310.13661</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amr-keleg/adi-under-scrutiny">https://github.com/amr-keleg/adi-under-scrutiny</a></li>
<li>paper_authors: Amr Keleg, Walid Magdy</li>
<li>for: 本文主要探讨了自动阿拉伯语方言识别（ADI）问题，尤其是在分类问题中存在困难的微方言识别问题。</li>
<li>methods: 作者提出了将 ADI 定义为多个标签分类问题，并提供了设计新的 ADI 数据集的建议。</li>
<li>results: 手动错误分析表明，有大约 66% 的错误不是真正的错误。<details>
<summary>Abstract</summary>
Automatic Arabic Dialect Identification (ADI) of text has gained great popularity since it was introduced in the early 2010s. Multiple datasets were developed, and yearly shared tasks have been running since 2018. However, ADI systems are reported to fail in distinguishing between the micro-dialects of Arabic. We argue that the currently adopted framing of the ADI task as a single-label classification problem is one of the main reasons for that. We highlight the limitation of the incompleteness of the Dialect labels and demonstrate how it impacts the evaluation of ADI systems. A manual error analysis for the predictions of an ADI, performed by 7 native speakers of different Arabic dialects, revealed that $\approx$ 66% of the validated errors are not true errors. Consequently, we propose framing ADI as a multi-label classification task and give recommendations for designing new ADI datasets.
</details>
<details>
<summary>摘要</summary>
自2010年代初起，自动阿拉伯语方言识别（ADI）技术已经受到了广泛的关注。多个数据集被开发出来，并每年举行共享任务。然而，ADI系统被报道无法分辨阿拉伯语微方言。我们认为，现有的ADI任务的帧围是主要的原因之一。我们强调了标签的不完整性的限制，并示出了这些限制如何影响ADI系统的评估。我们手动进行了7名阿拉伯语本地语言 speaker的预测 validate 分析，发现 Approx. 66% 的有效错误不是真正的错误。因此，我们提议将ADI重新定义为多标签分类任务，并提供了设计新的ADI数据集的建议。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-and-Improving-Text-to-SQL-Generation-under-Ambiguity"><a href="#Benchmarking-and-Improving-Text-to-SQL-Generation-under-Ambiguity" class="headerlink" title="Benchmarking and Improving Text-to-SQL Generation under Ambiguity"></a>Benchmarking and Improving Text-to-SQL Generation under Ambiguity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13659">http://arxiv.org/abs/2310.13659</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/testzer0/ambiqt">https://github.com/testzer0/ambiqt</a></li>
<li>paper_authors: Adithya Bhaskar, Tushar Tomar, Ashutosh Sathe, Sunita Sarawagi</li>
<li>for:  bridging the gap between text-to-SQL conversion and real-life database queries</li>
<li>methods:  developing a novel benchmark called AmbiQT, and proposing a new decoding algorithm called LogicalBeam</li>
<li>results:  LogicalBeam is up to $2.5$ times more effective than state-of-the-art models at generating all candidate SQLs in the top-$k$ ranked outputs, and enhances the top-$5$ Exact and Execution Match Accuracies on SPIDER and Kaggle DBQA.Here’s the Chinese version:</li>
<li>for:  closure Text-to-SQL转换和实际数据库查询之间的差距</li>
<li>methods: 开发了一个新的benchmark called AmbiQT，并提出了一种新的解码算法called LogicalBeam</li>
<li>results:  LogicalBeam比现状的模型更加有效，可以在top-$k$排名输出中生成所有可能的SQL查询，并提高了SPIDER和Kaggle DBQA的Exact和Execution Match Accuracy的top-$5$。<details>
<summary>Abstract</summary>
Research in Text-to-SQL conversion has been largely benchmarked against datasets where each text query corresponds to one correct SQL. However, natural language queries over real-life databases frequently involve significant ambiguity about the intended SQL due to overlapping schema names and multiple confusing relationship paths. To bridge this gap, we develop a novel benchmark called AmbiQT with over 3000 examples where each text is interpretable as two plausible SQLs due to lexical and/or structural ambiguity.   When faced with ambiguity, an ideal top-$k$ decoder should generate all valid interpretations for possible disambiguation by the user. We evaluate several Text-to-SQL systems and decoding algorithms, including those employing state-of-the-art LLMs, and find them to be far from this ideal. The primary reason is that the prevalent beam search algorithm and its variants, treat SQL queries as a string and produce unhelpful token-level diversity in the top-$k$.   We propose LogicalBeam, a new decoding algorithm that navigates the SQL logic space using a blend of plan-based template generation and constrained infilling. Counterfactually generated plans diversify templates while in-filling with a beam-search that branches solely on schema names provides value diversity. LogicalBeam is up to $2.5$ times more effective than state-of-the-art models at generating all candidate SQLs in the top-$k$ ranked outputs. It also enhances the top-$5$ Exact and Execution Match Accuracies on SPIDER and Kaggle DBQA.
</details>
<details>
<summary>摘要</summary>
研究在文本到SQL转换方面已经主要基准于具有一个唯一正确SQL的数据集。然而，自然语言 queries 中的真实数据库问题经常具有多种ambiguity，因为schema名称和关系路径的重叠。为了bridging这个差距，我们开发了一个新的benchmark叫做AmbiQT，它包含了超过3000个例子，每个文本都可以被解释为两个可能的SQL。当面临ambiguity时，理想的top-$k$ decoder应该生成所有有效的解释，以便由用户进行解释。然而，我们发现现有的 Text-to-SQL 系统和解码算法，包括使用状态态艺术LLMs，都远离这种理想。主要的原因是普遍使用的搜索算法和其变种，将SQL查询视为字符串，生成不帮助的token级多样性在top-$k$中。我们提议LogicalBeam，一种新的解码算法，通过将SQL逻辑空间映射到plan-based模板生成和受限的填充来解决这个问题。在填充过程中，使用缓冲搜索，在schema名称上分支，提供值多样性。LogicalBeam在top-$k$中生成所有候选SQL的效果比现有模型高达2.5倍。此外，它也提高了SPIDER和Kaggle DBQA中top-$5$的准确率和执行匹配率。
</details></li>
</ul>
<hr>
<h2 id="BotChat-Evaluating-LLMs’-Capabilities-of-Having-Multi-Turn-Dialogues"><a href="#BotChat-Evaluating-LLMs’-Capabilities-of-Having-Multi-Turn-Dialogues" class="headerlink" title="BotChat: Evaluating LLMs’ Capabilities of Having Multi-Turn Dialogues"></a>BotChat: Evaluating LLMs’ Capabilities of Having Multi-Turn Dialogues</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13650">http://arxiv.org/abs/2310.13650</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/open-compass/botchat">https://github.com/open-compass/botchat</a></li>
<li>paper_authors: Haodong Duan, Jueqi Wei, Chonghua Wang, Hongwei Liu, Yixiao Fang, Songyang Zhang, Dahua Lin, Kai Chen</li>
<li>for: 这份报告是为评估现有的大型语言模型（LLMs）在人类样式多Turn对话中的能力而写的。</li>
<li>methods: 我们使用了现实世界的人类对话作为开头，并让LLMs根据这些开头生成全部多Turn对话（数十句）。最后，我们采用了当今最佳的LLMs（GPT-4等）作为评估器，以评估生成的对话质量。</li>
<li>results: 我们发现GPT-4可以生成人类样式的多Turn对话，质量极高，明显超过其他LLMs。这些生成的对话很难被识别为机器生成的对话，而其他LLMs则很难生成满意的多Turn对话，主要由于低效的指令遵循能力、生成过长句子或总能力有限。<details>
<summary>Abstract</summary>
Interacting with human via high-quality multi-turn dialogues is a key feature of large language models (LLMs). However, human-based evaluation of such capability involves intensive manual labor. This report provides a preliminary evaluation of existing large language models for human-style multi-turn chatting, through an LLM-based approach. We start from real-world human dialogues and keep the very first utterances as the ChatSEED. Then we prompt LLMs to generate a full multi-turn dialogue (tens of utterances) based on the ChatSEED, utterance by utterance. Finally, we adopt state-of-the-art LLMs (GPT-4, \etc) as the judge to evaluate the generated dialogues. With different evaluation protocols, we come to substantially identical conclusions. We find that GPT-4 can generate human-style multi-turn dialogues with impressive quality, significantly outperforms its counterparts. It's difficult for a discriminator to distinguish between GPT-4 generated dialogues and human dialogues. In contrast, other LLMs struggle to generate multi-turn dialogues of satisfactory quality due to poor instruction-following capability, tendency to generate lengthy utterances, or limited general capability. All data and codes will be provided in https://github.com/open-compass/BotChat/ and we hope they can serve as a valuable resource for evaluating multi-turn chatting capabilities of LLMs.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用高质量多轮对话与人类交互是大型语言模型（LLM）的关键功能之一。然而，人类基于评估这种能力具有劳动密集型的特点。本报告提供了基于现实世界人类对话的初始实验，并采用LLM基于方法进行评估。我们从真实世界人类对话中挑选出第一句话作为ChatSEED，然后使用LLM生成完整的多轮对话（数十句），一句一句地进行生成。最后，我们采用当今最高级的LLM（GPT-4、等）作为评估者，使用不同的评价协议进行评估生成对话质量。我们发现GPT-4可以生成人类式的多轮对话，质量极高，明显超越其他对手。很难用权限 distinguish GPT-4生成的对话和人类对话。相比之下，其他LLM很难生成满意质量的多轮对话，主要是因为低效的指令遵循能力、生成过长的句子或总能力有限。我们将所有数据和代码提供在https://github.com/open-compass/BotChat/上，希望它们可以为评估LLM多轮对话能力提供有价值的资源。
</details></li>
</ul>
<hr>
<h2 id="Bridging-Information-Theoretic-and-Geometric-Compression-in-Language-Models"><a href="#Bridging-Information-Theoretic-and-Geometric-Compression-in-Language-Models" class="headerlink" title="Bridging Information-Theoretic and Geometric Compression in Language Models"></a>Bridging Information-Theoretic and Geometric Compression in Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13620">http://arxiv.org/abs/2310.13620</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chengemily1/id_bridging">https://github.com/chengemily1/id_bridging</a></li>
<li>paper_authors: Emily Cheng, Corentin Kervadec, Marco Baroni</li>
<li>for: 这项研究旨在探讨语言模型（LM）如何准确地模型人类语言，以及LM的压缩性能对其表现的影响。</li>
<li>methods: 研究者采用了两种视角来分析LM的压缩性能：几何学视角和信息理论视角。他们发现这两种视角之间存在高度相关性，即语言数据的自然几何维度预测了该数据在LM中的编码长度。</li>
<li>results: 研究者发现，LM的压缩性能与其能够快速适应语言数据的能力相关。此外，他们还评估了一些内在维度估计器，并发现只有一些估计器能够捕捉语言数据中的压缩性、几何维度和适应性之间的关系。<details>
<summary>Abstract</summary>
For a language model (LM) to faithfully model human language, it must compress vast, potentially infinite information into relatively few dimensions. We propose analyzing compression in (pre-trained) LMs from two points of view: geometric and information-theoretic. We demonstrate that the two views are highly correlated, such that the intrinsic geometric dimension of linguistic data predicts their coding length under the LM. We then show that, in turn, high compression of a linguistic dataset predicts rapid adaptation to that dataset, confirming that being able to compress linguistic information is an important part of successful LM performance. As a practical byproduct of our analysis, we evaluate a battery of intrinsic dimension estimators for the first time on linguistic data, showing that only some encapsulate the relationship between information-theoretic compression, geometric compression, and ease-of-adaptation.
</details>
<details>
<summary>摘要</summary>
为了准确模拟人类语言，语言模型（LM）必须压缩庞大、潜在无穷的信息到相对较少的维度。我们从两个视角分析LM中的压缩：几何学视角和信息理论视角。我们示示了这两种视角之间存在很高的相关性，即语言数据的内在几何维度预测其编码长度下LM。然后，我们示示了，对于某个语言集合，高压缩率预测了快速适应该语言集合的能力，确认了压缩语言信息是成功LM性能的重要组成部分。此外，我们评估了一系列内在维度估计器，并发现只有一些能够捕捉压缩、几何压缩和适应性之间的关系。
</details></li>
</ul>
<hr>
<h2 id="Semi-supervised-multimodal-coreference-resolution-in-image-narrations"><a href="#Semi-supervised-multimodal-coreference-resolution-in-image-narrations" class="headerlink" title="Semi-supervised multimodal coreference resolution in image narrations"></a>Semi-supervised multimodal coreference resolution in image narrations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13619">http://arxiv.org/abs/2310.13619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arushi Goel, Basura Fernando, Frank Keller, Hakan Bilen</li>
<li>for: 这篇论文研究了多Modal coreference resolution，具体来说是将长的描述文本（即 narraion）与图片对应。</li>
<li>methods: 该论文提出了一种数据效率的半supervised方法，利用图片-文本对应来解决多Modal coreference resolution和narraion grounding问题。该方法在cross-modal框架中结合了标注和无标注数据的损失函数。</li>
<li>results: 论文的实验显示，该方法可以比以强基线数据进行量化和质量上的提升，用于多Modal coreference resolution和narraion grounding任务。<details>
<summary>Abstract</summary>
In this paper, we study multimodal coreference resolution, specifically where a longer descriptive text, i.e., a narration is paired with an image. This poses significant challenges due to fine-grained image-text alignment, inherent ambiguity present in narrative language, and unavailability of large annotated training sets. To tackle these challenges, we present a data efficient semi-supervised approach that utilizes image-narration pairs to resolve coreferences and narrative grounding in a multimodal context. Our approach incorporates losses for both labeled and unlabeled data within a cross-modal framework. Our evaluation shows that the proposed approach outperforms strong baselines both quantitatively and qualitatively, for the tasks of coreference resolution and narrative grounding.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究多模态核心参照解决方法，具体来说是与图像相关的长文描述。这种情况存在细腻的图像文本对齐问题、自然语言中的模糊性和缺乏大量标注训练集的问题。为解决这些问题，我们提出了一种数据效率高的半超级vised方法，该方法利用图像文本对的数据来解决核心参照和多模态场景中的描述固定。我们的方法包括两种损失函数：标注数据的损失函数和未标注数据的损失函数，并在跨模态框架中结合使用。我们的评估结果表明，我们的方法在核心参照和描述固定两个任务上都能够超越强基线，both quantitatively and qualitatively。
</details></li>
</ul>
<hr>
<h2 id="Three-Questions-Concerning-the-Use-of-Large-Language-Models-to-Facilitate-Mathematics-Learning"><a href="#Three-Questions-Concerning-the-Use-of-Large-Language-Models-to-Facilitate-Mathematics-Learning" class="headerlink" title="Three Questions Concerning the Use of Large Language Models to Facilitate Mathematics Learning"></a>Three Questions Concerning the Use of Large Language Models to Facilitate Mathematics Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13615">http://arxiv.org/abs/2310.13615</a></li>
<li>repo_url: None</li>
<li>paper_authors: An-Zi Yen, Wei-Ling Hsu</li>
<li>for: 本研究探讨了使用大语言模型（LLM）来提高学生数学问题解决能力的可能性，以及LLM在教学应用中的教学能力。</li>
<li>methods: 本研究采用了适应性反馈的方法，通过LLM对学生答案的检查和修正来帮助学生解决数学问题。</li>
<li>results: 研究发现，LLM可能会因为问题的意思和逻辑不准确而提供错误的反馈，同时也可能会因为问题的 complexity 而难以理解问题的 rationales。<details>
<summary>Abstract</summary>
Due to the remarkable language understanding and generation abilities of large language models (LLMs), their use in educational applications has been explored. However, little work has been done on investigating the pedagogical ability of LLMs in helping students to learn mathematics. In this position paper, we discuss the challenges associated with employing LLMs to enhance students' mathematical problem-solving skills by providing adaptive feedback. Apart from generating the wrong reasoning processes, LLMs can misinterpret the meaning of the question, and also exhibit difficulty in understanding the given questions' rationales when attempting to correct students' answers. Three research questions are formulated.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>如何使用大语言模型（LLMs）来增强学生的数学问题解决能力？2.  LLMS 是否能够正确理解学生提交的问题，并且能够提供适应性的反馈？3.  LLMS 在帮助学生学习数学时是否存在挑战，如果存在，则如何解决这些挑战？</details></li>
</ol>
<hr>
<h2 id="Simultaneous-Machine-Translation-with-Tailored-Reference"><a href="#Simultaneous-Machine-Translation-with-Tailored-Reference" class="headerlink" title="Simultaneous Machine Translation with Tailored Reference"></a>Simultaneous Machine Translation with Tailored Reference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13588">http://arxiv.org/abs/2310.13588</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ictnlp/Tailored-Ref">https://github.com/ictnlp/Tailored-Ref</a></li>
<li>paper_authors: Shoutao Guo, Shaolei Zhang, Yang Feng</li>
<li>for: 本研究旨在提高同时机器翻译（SiMT）模型的翻译质量，并且适应不同的延迟环境。</li>
<li>methods: 本研究提出了一种新的方法，即通过使用强化学习引入的修改器，对SiMT模型的训练参考进行修改，以避免在训练过程中的强制预测。</li>
<li>results: 实验结果表明，使用修改器进行修改的SiMT模型在三个翻译任务中均 achieve state-of-the-art表现，并且在固定和适应策略下都能够提高表现。<details>
<summary>Abstract</summary>
Simultaneous machine translation (SiMT) generates translation while reading the whole source sentence. However, existing SiMT models are typically trained using the same reference disregarding the varying amounts of available source information at different latency. Training the model with ground-truth at low latency may introduce forced anticipations, whereas utilizing reference consistent with the source word order at high latency results in performance degradation. Consequently, it is crucial to train the SiMT model with appropriate reference that avoids forced anticipations during training while maintaining high quality. In this paper, we propose a novel method that provides tailored reference for the SiMT models trained at different latency by rephrasing the ground-truth. Specifically, we introduce the tailor, induced by reinforcement learning, to modify ground-truth to the tailored reference. The SiMT model is trained with the tailored reference and jointly optimized with the tailor to enhance performance. Importantly, our method is applicable to a wide range of current SiMT approaches. Experiments on three translation tasks demonstrate that our method achieves state-of-the-art performance in both fixed and adaptive policies.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Improving-Cross-Lingual-Transfer-through-Subtree-Aware-Word-Reordering"><a href="#Improving-Cross-Lingual-Transfer-through-Subtree-Aware-Word-Reordering" class="headerlink" title="Improving Cross-Lingual Transfer through Subtree-Aware Word Reordering"></a>Improving Cross-Lingual Transfer through Subtree-Aware Word Reordering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13583">http://arxiv.org/abs/2310.13583</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ofirarviv/ud-based-word-reordering">https://github.com/ofirarviv/ud-based-word-reordering</a></li>
<li>paper_authors: Ofir Arviv, Dmitry Nikolaev, Taelin Karidi, Omri Abend</li>
<li>for: 本研究旨在提高多语言模型对不同语言的表达能力，尤其是在low-resource设置下。</li>
<li>methods: 我们提出了一种新的重新排序方法，基于Universal Dependencies语法，可以通过少量注解数据学习细致的单词顺序模式，并可以应用于各种语言和模型结构。</li>
<li>results: 我们的方法在多种任务上表现出优于强基eline，包括零shot和几shot情况下。这表明我们的方法可以有效地 Mitigate variability in word-order patterns，提高多语言模型的表达能力。<details>
<summary>Abstract</summary>
Despite the impressive growth of the abilities of multilingual language models, such as XLM-R and mT5, it has been shown that they still face difficulties when tackling typologically-distant languages, particularly in the low-resource setting. One obstacle for effective cross-lingual transfer is variability in word-order patterns. It can be potentially mitigated via source- or target-side word reordering, and numerous approaches to reordering have been proposed. However, they rely on language-specific rules, work on the level of POS tags, or only target the main clause, leaving subordinate clauses intact. To address these limitations, we present a new powerful reordering method, defined in terms of Universal Dependencies, that is able to learn fine-grained word-order patterns conditioned on the syntactic context from a small amount of annotated data and can be applied at all levels of the syntactic tree. We conduct experiments on a diverse set of tasks and show that our method consistently outperforms strong baselines over different language pairs and model architectures. This performance advantage holds true in both zero-shot and few-shot scenarios.
</details>
<details>
<summary>摘要</summary>
尽管多语言模型（如XLM-R和mT5）在表现出了卓越的能力，但它们在语言学上较远的语言中仍然遇到困难，特别是在low-resource Setting下。一个阻碍 cross-lingual 转移的障碍是语言变体的word-order模式的变化。这可能可以通过源-或目标-side word reordering来mitigate，并且有许多approach to reordering已经被提出。然而，这些方法都是基于语言特定的规则，工作在POS标签层次上，或者只能target主句，留下副句不变。为了解决这些限制，我们提出了一种新的强大的重编译方法，基于Universal Dependencies，可以通过一小量的注释数据学习细致的word-order模式，并且可以在所有语法树层次上应用。我们进行了多种任务的实验，并证明了我们的方法在不同的语言对和模型架构下表现出了卓越的表现，并且这种表现优势在零shot和几shot Scenario下都保持。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Decomposition-of-Question-and-SQL-for-Text-to-SQL-Parsing"><a href="#Semantic-Decomposition-of-Question-and-SQL-for-Text-to-SQL-Parsing" class="headerlink" title="Semantic Decomposition of Question and SQL for Text-to-SQL Parsing"></a>Semantic Decomposition of Question and SQL for Text-to-SQL Parsing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13575">http://arxiv.org/abs/2310.13575</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ben Eyal, Amir Bachar, Ophir Haroche, Moran Mahabi, Michael Elhadad</li>
<li>for: 提高文本到SQL semantic parsing的通用化能力，解决跨领域和复杂查询的挑战。</li>
<li>methods: 使用问题分解策略提高复杂SQL查询的解析，但这会遇到两个主要障碍：（1）现有数据集缺少问题分解；（2）由SQL语言的 sintaxis复杂性，大多数复杂查询无法被简单 decomposed into sub-queries。</li>
<li>results: 我们提出了一种新的模块化查询计划语言（QPL），系统地将SQL查询分解成简单和Regular sub-queries。我们利用SQL服务器查询优化计划分析，开发了一个将SQL转换为QPL的翻译器，并将Spider数据集扩展到QPL程序。实验结果表明，模块化QPL的性能有助于现有的semantic-parsing架构，并且训练text-to-QPL parser比text-to-SQL parsing更有效果。此外，QPL方法还具有两个优点：（1）QPL程序可以简单化为可读的问题，从而创建了一个复杂问题和分解问题的数据集。（2）QPL更易于非专家理解复杂查询结果，从而提高了semantic parser的可读性。<details>
<summary>Abstract</summary>
Text-to-SQL semantic parsing faces challenges in generalizing to cross-domain and complex queries. Recent research has employed a question decomposition strategy to enhance the parsing of complex SQL queries. However, this strategy encounters two major obstacles: (1) existing datasets lack question decomposition; (2) due to the syntactic complexity of SQL, most complex queries cannot be disentangled into sub-queries that can be readily recomposed. To address these challenges, we propose a new modular Query Plan Language (QPL) that systematically decomposes SQL queries into simple and regular sub-queries. We develop a translator from SQL to QPL by leveraging analysis of SQL server query optimization plans, and we augment the Spider dataset with QPL programs. Experimental results demonstrate that the modular nature of QPL benefits existing semantic-parsing architectures, and training text-to-QPL parsers is more effective than text-to-SQL parsing for semantically equivalent queries. The QPL approach offers two additional advantages: (1) QPL programs can be paraphrased as simple questions, which allows us to create a dataset of (complex question, decomposed questions). Training on this dataset, we obtain a Question Decomposer for data retrieval that is sensitive to database schemas. (2) QPL is more accessible to non-experts for complex queries, leading to more interpretable output from the semantic parser.
</details>
<details>
<summary>摘要</summary>
文本抽取 Semantic 问题面临通用化和复杂查询泛化的挑战。 current research 使用问题分解策略提高复杂 SQL 查询的解析。 however， this strategy 遇到两个主要障碍：（1）现有数据集缺少问题分解；（2）由于 SQL 的 sintax 复杂性，大多数复杂查询无法分解成可轻松重新组合的子查询。 To address these challenges, we propose a new modular Query Plan Language (QPL) that systematically decomposes SQL queries into simple and regular sub-queries. We develop a translator from SQL to QPL by leveraging analysis of SQL server query optimization plans, and we augment the Spider dataset with QPL programs. Experimental results demonstrate that the modular nature of QPL benefits existing semantic-parsing architectures, and training text-to-QPL parsers is more effective than text-to-SQL parsing for semantically equivalent queries. The QPL approach offers two additional advantages: (1) QPL programs can be paraphrased as simple questions, which allows us to create a dataset of (complex question, decomposed questions). Training on this dataset, we obtain a Question Decomposer for data retrieval that is sensitive to database schemas. (2) QPL is more accessible to non-experts for complex queries, leading to more interpretable output from the semantic parser.
</details></li>
</ul>
<hr>
<h2 id="Why-Can-Large-Language-Models-Generate-Correct-Chain-of-Thoughts"><a href="#Why-Can-Large-Language-Models-Generate-Correct-Chain-of-Thoughts" class="headerlink" title="Why Can Large Language Models Generate Correct Chain-of-Thoughts?"></a>Why Can Large Language Models Generate Correct Chain-of-Thoughts?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13571">http://arxiv.org/abs/2310.13571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rasul Tutunov, Antoine Grosnit, Juliusz Ziomek, Jun Wang, Haitham Bou-Ammar</li>
<li>for: 这个论文探讨大语言模型（LLM）的能力，具体来说是提高对链式思维提示的理论认知。</li>
<li>methods: 作者提出了一种两级层次图形模型，用于自然语言生成。在这个框架中，作者证明了一种强有力的 geometrical convergence rate，用于评估 LLM 生成的链式思维是否正确。</li>
<li>results: 研究结果表明，LLM 可以有效地生成一系列相关的思维，并且可以理解和解释这些思维的顺序性。这些结果为具有理解能力的任务中 LLM 的表现提供了理论基础。<details>
<summary>Abstract</summary>
This paper delves into the capabilities of large language models (LLMs), specifically focusing on advancing the theoretical comprehension of chain-of-thought prompting. We investigate how LLMs can be effectively induced to generate a coherent chain of thoughts. To achieve this, we introduce a two-level hierarchical graphical model tailored for natural language generation. Within this framework, we establish a compelling geometrical convergence rate that gauges the likelihood of an LLM-generated chain of thoughts compared to those originating from the true language. Our findings provide a theoretical justification for the ability of LLMs to produce the correct sequence of thoughts (potentially) explaining performance gains in tasks demanding reasoning skills.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Cache-Distil-Optimising-API-Calls-to-Large-Language-Models"><a href="#Cache-Distil-Optimising-API-Calls-to-Large-Language-Models" class="headerlink" title="Cache &amp; Distil: Optimising API Calls to Large Language Models"></a>Cache &amp; Distil: Optimising API Calls to Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13561">http://arxiv.org/abs/2310.13561</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillem Ramírez, Matthias Lindemann, Alexandra Birch, Ivan Titov</li>
<li>for: 降低大规模生成AI工具的成本，尤其是实时处理用户查询的API请求。</li>
<li>methods: 使用一个较小的语言模型（学生），并将其不断地训练成为独立处理用户查询的能力，并透过一个策略选择哪些请求交由学生处理，哪些交由大语言模型处理，以便助学生学习。</li>
<li>results: 在分类任务中，使用活动学习基于选择几个标准的检查方法，例如 Margin Sampling 和 Query by Committee，可以带来一致的优化效果，不论任务或预算。<details>
<summary>Abstract</summary>
Large-scale deployment of generative AI tools often depends on costly API calls to a Large Language Model (LLM) to fulfil user queries. To curtail the frequency of these calls, one can employ a smaller language model -- a student -- which is continuously trained on the responses of the LLM. This student gradually gains proficiency in independently handling an increasing number of user requests, a process we term neural caching. The crucial element in neural caching is a policy that decides which requests should be processed by the student alone and which should be redirected to the LLM, subsequently aiding the student's learning. In this study, we focus on classification tasks, and we consider a range of classic active learning-based selection criteria as the policy. Our experiments suggest that Margin Sampling and Query by Committee bring consistent benefits across tasks and budgets.
</details>
<details>
<summary>摘要</summary>
大规模的生成AI工具常常依赖于成本高昂的API调用来满足用户的查询。为了减少这些调用频率，可以采用一个较小的语言模型——学生模型，并将其不断训练在大语言模型（LLM）的回应基础上。学生模型逐渐增强其独立处理用户请求的能力，这个过程我们称为神经缓存。神经缓存的关键元素是一种策略，决定哪些请求应该由学生模型处理，而哪些请求应该被重定向到LLM。在本研究中，我们关注分类任务，并考虑了一些经典的活动学习基于选择准则。我们的实验表明，边缘抽样和咨询委员会都带来了一致的好处，不 matter what the task or budget.
</details></li>
</ul>
<hr>
<h2 id="The-Perils-Promises-of-Fact-checking-with-Large-Language-Models"><a href="#The-Perils-Promises-of-Fact-checking-with-Large-Language-Models" class="headerlink" title="The Perils &amp; Promises of Fact-checking with Large Language Models"></a>The Perils &amp; Promises of Fact-checking with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13549">http://arxiv.org/abs/2310.13549</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dorian Quelle, Alexandre Bovet</li>
<li>for: 这研究旨在评估大自然语言模型（LLM）在真实性核查中的表现，以及如何使用这些模型来提高核查的准确性。</li>
<li>methods: 这个研究使用了GPT-4和GPT-3大自然语言模型，并将其用于编写学术论文、法律文档和新闻文章，以评估这些模型在核查信息的能力。</li>
<li>results: 研究发现，当equipped with contextual information时，LLMs的表现有所提高，但准确性受到查询语言和CLAIM的影响。GPT-4表现比GPT-3更好，但是不同的查询语言和CLAIM可能会导致不同的准确性。<details>
<summary>Abstract</summary>
Autonomous fact-checking, using machine learning to verify claims, has grown vital as misinformation spreads beyond human fact-checking capacity. Large Language Models (LLMs) like GPT-4 are increasingly trusted to verify information and write academic papers, lawsuits, and news articles, emphasizing their role in discerning truth from falsehood and the importance of being able to verify their outputs. Here, we evaluate the use of LLM agents in fact-checking by having them phrase queries, retrieve contextual data, and make decisions. Importantly, in our framework, agents explain their reasoning and cite the relevant sources from the retrieved context. Our results show the enhanced prowess of LLMs when equipped with contextual information. GPT-4 outperforms GPT-3, but accuracy varies based on query language and claim veracity. While LLMs show promise in fact-checking, caution is essential due to inconsistent accuracy. Our investigation calls for further research, fostering a deeper comprehension of when agents succeed and when they fail.
</details>
<details>
<summary>摘要</summary>
自主 фактоChecking，使用机器学习验证声明，在谎言扩散超出人工验证能力时变得越来越重要。大型语言模型（LLMs）如GPT-4在验证信息和写学术论文、法律诉讼和新闻文章方面被越来越信任，强调其在分辨真假和重要性的能力。在我们的框架中，代理人会提出问题，检索Contextual数据，并做出决定。特别是，在我们的框架中，代理人会解释其思考过程和引用来源于检索的Contextual数据。我们的结果显示在Contextual信息支持下，LLM代理人的能力得到了提高。GPT-4比GPT-3表现更出色，但是准确率基于问题语言和声明真假性有所不同。虽然LLM代理人在验证方面表现良好，但是需要谨慎，因为它们的准确率不稳定。我们的调查表明，需要进一步的研究，以深入理解代理人在不同情况下的表现。
</details></li>
</ul>
<hr>
<h2 id="A-Diachronic-Perspective-on-User-Trust-in-AI-under-Uncertainty"><a href="#A-Diachronic-Perspective-on-User-Trust-in-AI-under-Uncertainty" class="headerlink" title="A Diachronic Perspective on User Trust in AI under Uncertainty"></a>A Diachronic Perspective on User Trust in AI under Uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13544">http://arxiv.org/abs/2310.13544</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zouharvi/trust-intervention">https://github.com/zouharvi/trust-intervention</a></li>
<li>paper_authors: Shehzaad Dhuliawala, Vilém Zouhar, Mennatallah El-Assady, Mrinmaya Sachan</li>
<li>for: 这个论文研究了用户对人工智能系统的信任的发展和恢复，以及不同类型的误差对用户信任的影响。</li>
<li>methods: 该论文使用了一种投票游戏来研究用户对人工智能系统的信任的演变和恢复。</li>
<li>results: 研究发现，即使只有几次错误 Prediction with inaccurate confidence estimates can severely damage user trust and performance, with slow recovery. 不同类型的误差也有不同的负面影响于用户信任。 这些发现highlights the importance of calibration in user-facing AI applications and shed light on what aspects help users decide whether to trust the AI system.<details>
<summary>Abstract</summary>
In a human-AI collaboration, users build a mental model of the AI system based on its reliability and how it presents its decision, e.g. its presentation of system confidence and an explanation of the output. Modern NLP systems are often uncalibrated, resulting in confidently incorrect predictions that undermine user trust. In order to build trustworthy AI, we must understand how user trust is developed and how it can be regained after potential trust-eroding events. We study the evolution of user trust in response to these trust-eroding events using a betting game. We find that even a few incorrect instances with inaccurate confidence estimates damage user trust and performance, with very slow recovery. We also show that this degradation in trust reduces the success of human-AI collaboration and that different types of miscalibration -- unconfidently correct and confidently incorrect -- have different negative effects on user trust. Our findings highlight the importance of calibration in user-facing AI applications and shed light on what aspects help users decide whether to trust the AI system.
</details>
<details>
<summary>摘要</summary>
人与AI合作中，用户会建立AI系统的心理模型，基于其可靠性和输出的解释。现代NLP系统经常无法准确评估自己的可靠性，导致用户对AI系统的信任感受到损害。为建立可信worthy AI，我们需要理解用户信任的发展和恢复机制。我们通过赌博游戏来研究用户对不可靠事件后的信任恢复，发现 Even a few incorrect instances with inaccurate confidence estimates can damage user trust and performance, with very slow recovery. We also show that different types of miscalibration -- unconfidently correct and confidently incorrect -- have different negative effects on user trust. Our findings highlight the importance of calibration in user-facing AI applications and shed light on what aspects help users decide whether to trust the AI system.
</details></li>
</ul>
<hr>
<h2 id="Controlled-Randomness-Improves-the-Performance-of-Transformer-Models"><a href="#Controlled-Randomness-Improves-the-Performance-of-Transformer-Models" class="headerlink" title="Controlled Randomness Improves the Performance of Transformer Models"></a>Controlled Randomness Improves the Performance of Transformer Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13526">http://arxiv.org/abs/2310.13526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tobias Deußer, Cong Zhao, Wolfgang Krämer, David Leonhard, Christian Bauckhage, Rafet Sifa</li>
<li>for: 这个研究的目的是要探索在自然语言模型的预训步骤中引入控制随机性，以提高精确训练和下游任务的性能。</li>
<li>methods: 这个研究使用了随机噪音来控制自然语言模型的训练过程，以提高精确训练和下游任务的性能。</li>
<li>results: 研究发现，在这两个下游任务中，透过将随机噪音添加到训练过程，可以提高自然语言模型的性能。<details>
<summary>Abstract</summary>
During the pre-training step of natural language models, the main objective is to learn a general representation of the pre-training dataset, usually requiring large amounts of textual data to capture the complexity and diversity of natural language. Contrasting this, in most cases, the size of the data available to solve the specific downstream task is often dwarfed by the aforementioned pre-training dataset, especially in domains where data is scarce. We introduce controlled randomness, i.e. noise, into the training process to improve fine-tuning language models and explore the performance of targeted noise in addition to the parameters of these models. We find that adding such noise can improve the performance in our two downstream tasks of joint named entity recognition and relation extraction and text summarization.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:在自然语言模型的预训练阶段，主要目标是学习预训练数据集的通用表示，通常需要大量的文本数据来捕捉自然语言的复杂性和多样性。相比之下，在大多数情况下，解决特定下游任务的数据量通常比预训练数据集要小得多，特别是在数据匮乏的领域。为了解决这个问题，我们引入控制的随机性，即噪声，到训练过程中，以提高语言模型的微调和探索噪声的影响以及模型参数。我们发现，添加这种噪声可以提高我们的两个下游任务的结合命名实体识别和关系EXTRACTION和文本摘要的性能。
</details></li>
</ul>
<hr>
<h2 id="Teaching-Language-Models-to-Self-Improve-through-Interactive-Demonstrations"><a href="#Teaching-Language-Models-to-Self-Improve-through-Interactive-Demonstrations" class="headerlink" title="Teaching Language Models to Self-Improve through Interactive Demonstrations"></a>Teaching Language Models to Self-Improve through Interactive Demonstrations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13522">http://arxiv.org/abs/2310.13522</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jasonyux/tripost">https://github.com/jasonyux/tripost</a></li>
<li>paper_authors: Xiao Yu, Baolin Peng, Michel Galley, Jianfeng Gao, Zhou Yu</li>
<li>for: 这个论文的目的是提高小型语言模型（LLMs）的自我改进能力，以减少与现状最佳LLMs之间的性能差距。</li>
<li>methods: 作者提出了一种名为TriPosT的训练算法，通过让小型模型与大型语言模型互动，收集反馈和改进自己的生成内容，来增强小型模型的自我改进能力。</li>
<li>results: 作者的实验表明，使用TriPosT训练算法可以提高一个LLaMA-7b模型在数学和逻辑任务上的性能，最高提高7.13%。此外，作者发现了在学习和修正自己的错误时，小型模型的互动经验是关键的。<details>
<summary>Abstract</summary>
The self-improving ability of large language models (LLMs), enabled by prompting them to analyze and revise their own outputs, has garnered significant interest in recent research. However, this ability has been shown to be absent and difficult to learn for smaller models, thus widening the performance gap between state-of-the-art LLMs and more cost-effective and faster ones. To reduce this gap, we introduce TriPosT, a training algorithm that endows smaller models with such self-improvement ability, and show that our approach can improve a LLaMA-7b's performance on math and reasoning tasks by up to 7.13%. In contrast to prior work, we achieve this by using the smaller model to interact with LLMs to collect feedback and improvements on its own generations. We then replay this experience to train the small model. Our experiments on four math and reasoning datasets show that the interactive experience of learning from and correcting its own mistakes is crucial for small models to improve their performance.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）的自我改进能力，受到最近研究的广泛关注。然而，这种能力对小型模型来说是缺失的并且困难学习，因此加大了当前LLM和更cost-effective的模型之间性能差距。为了减少这个差距，我们介绍了TriPosT训练算法，使小型模型拥有自我改进能力，并证明我们的方法可以在数学和逻辑任务上提高LLaMA-7b的性能 by up to 7.13%。与之前的工作不同，我们通过使小型模型与LLM进行互动，收集feedback和改进自己的生成。然后，我们将这些经验重新播放以训练小模型。我们在四个数学和逻辑数据集上进行了实验，发现互动式学习自己的错误和改进自己的生成是小模型提高性能的关键。
</details></li>
</ul>
<hr>
<h2 id="Improving-Question-Generation-with-Multi-level-Content-Planning"><a href="#Improving-Question-Generation-with-Multi-level-Content-Planning" class="headerlink" title="Improving Question Generation with Multi-level Content Planning"></a>Improving Question Generation with Multi-level Content Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13512">http://arxiv.org/abs/2310.13512</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zeaver/multifactor">https://github.com/zeaver/multifactor</a></li>
<li>paper_authors: Zehua Xia, Qi Gou, Bowen Yu, Haiyang Yu, Fei Huang, Yongbin Li, Cam-Tu Nguyen</li>
<li>for: 本研究旨在生成基于 Context 和答案的问题，特别是需要跨越多个步骤的理解Context 中的问题。</li>
<li>methods: 我们提出了一种基于多级内容规划的问题生成框架，即 MultiFactor，其包括两个组件：FA-model，同时选择关键短语和生成全答，以及Q-model，使用生成的全答作为额外输入来生成问题。</li>
<li>results: 我们的方法在两个流行的问题生成数据集上表现出优于强基elines。<details>
<summary>Abstract</summary>
This paper addresses the problem of generating questions from a given context and an answer, specifically focusing on questions that require multi-hop reasoning across an extended context. Previous studies have suggested that key phrase selection is essential for question generation (QG), yet it is still challenging to connect such disjointed phrases into meaningful questions, particularly for long context. To mitigate this issue, we propose MultiFactor, a novel QG framework based on multi-level content planning. Specifically, MultiFactor includes two components: FA-model, which simultaneously selects key phrases and generates full answers, and Q-model which takes the generated full answer as an additional input to generate questions. Here, full answer generation is introduced to connect the short answer with the selected key phrases, thus forming an answer-aware summary to facilitate QG. Both FA-model and Q-model are formalized as simple-yet-effective Phrase-Enhanced Transformers, our joint model for phrase selection and text generation. Experimental results show that our method outperforms strong baselines on two popular QG datasets. Our code is available at https://github.com/zeaver/MultiFactor.
</details>
<details>
<summary>摘要</summary>
MultiFactor consists of two components: the FA-model, which simultaneously selects key phrases and generates full answers, and the Q-model, which takes the generated full answer as input to generate questions. The FA-model uses a Phrase-Enhanced Transformer to formalize the process of selecting key phrases and generating full answers. The Q-model also uses a Phrase-Enhanced Transformer to generate questions based on the generated full answer.The key innovation of MultiFactor is the use of full answer generation to connect the short answer with the selected key phrases, creating an answer-aware summary that facilitates QG. This approach allows the model to generate more coherent and relevant questions, especially for long contexts.Experimental results show that MultiFactor outperforms strong baselines on two popular QG datasets. Our code is available at https://github.com/zeaver/MultiFactor.
</details></li>
</ul>
<hr>
<h2 id="DistillCSE-Distilled-Contrastive-Learning-for-Sentence-Embeddings"><a href="#DistillCSE-Distilled-Contrastive-Learning-for-Sentence-Embeddings" class="headerlink" title="DistillCSE: Distilled Contrastive Learning for Sentence Embeddings"></a>DistillCSE: Distilled Contrastive Learning for Sentence Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13499">http://arxiv.org/abs/2310.13499</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiahao Xu, Wei Shao, Lihui Chen, Lemao Liu</li>
<li>for: 本文提出了DistillCSE框架，它通过对自我教学模型进行对比学习，使用知识填充来帮助学习更强的模型。</li>
<li>methods: 本文使用了知识填充和对比学习两种方法来提高模型性能。</li>
<li>results: 实验结果表明，提出的DistillCSE方法可以超过许多强大的基eline方法，并实现新的状态场报表性能。<details>
<summary>Abstract</summary>
This paper proposes the DistillCSE framework, which performs contrastive learning under the self-training paradigm with knowledge distillation. The potential advantage of DistillCSE is its self-enhancing feature: using a base model to provide additional supervision signals, a stronger model may be learned through knowledge distillation. However, the vanilla DistillCSE through the standard implementation of knowledge distillation only achieves marginal improvements due to severe overfitting. The further quantitative analyses demonstrate the reason that the standard knowledge distillation exhibits a relatively large variance of the teacher model's logits due to the essence of contrastive learning. To mitigate the issue induced by high variance, this paper accordingly proposed two simple yet effective solutions for knowledge distillation: a Group-P shuffling strategy as an implicit regularization and the averaging logits from multiple teacher components. Experiments on standard benchmarks demonstrate that the proposed DistillCSE outperforms many strong baseline methods and yields a new state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文提出了DistillCSE框架，它通过对比学习的自我训练方式进行知识储存，并可能带来更强的模型。然而，标准的DistillCSE实现中的知识储存只能达到有限的改进，主要是因为严重的过拟合。进一步的量化分析表明，标准的知识储存会导致教师模型的偏差值较大，这是因为对比学习的本质。为了解决这个问题，这篇论文提出了两种简单 yet有效的解决方案：Group-P混合策略作为隐式正则化，以及从多个教师组件中平均logits。实验表明，提议的DistillCSE可以超越许多强大的基线方法，并达到新的领域最佳性能。
</details></li>
</ul>
<hr>
<h2 id="Steering-Large-Language-Models-for-Machine-Translation-with-Finetuning-and-In-Context-Learning"><a href="#Steering-Large-Language-Models-for-Machine-Translation-with-Finetuning-and-In-Context-Learning" class="headerlink" title="Steering Large Language Models for Machine Translation with Finetuning and In-Context Learning"></a>Steering Large Language Models for Machine Translation with Finetuning and In-Context Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13448">http://arxiv.org/abs/2310.13448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duarte M. Alves, Nuno M. Guerreiro, João Alves, José Pombal, Ricardo Rei, José G. C. de Souza, Pierre Colombo, André F. T. Martins</li>
<li>for: 本研究旨在探讨LLM-based机器翻译系统的缺陷和改进方法。</li>
<li>methods: 本研究使用 adapter-based 微调，并证明这种方法可以提高翻译效果，同时减少训练参数数量。</li>
<li>results: 研究发现，微调通常会降低几个示例的表现，但可以保留它们的翻译能力。此外，提出了一种简单的方法，可以在微调过程中包含几个示例，以提高翻译效果。<details>
<summary>Abstract</summary>
Large language models (LLMs) are a promising avenue for machine translation (MT). However, current LLM-based MT systems are brittle: their effectiveness highly depends on the choice of few-shot examples and they often require extra post-processing due to overgeneration. Alternatives such as finetuning on translation instructions are computationally expensive and may weaken in-context learning capabilities, due to overspecialization. In this paper, we provide a closer look at this problem. We start by showing that adapter-based finetuning with LoRA matches the performance of traditional finetuning while reducing the number of training parameters by a factor of 50. This method also outperforms few-shot prompting and eliminates the need for post-processing or in-context examples. However, we show that finetuning generally degrades few-shot performance, hindering adaptation capabilities. Finally, to obtain the best of both worlds, we propose a simple approach that incorporates few-shot examples during finetuning. Experiments on 10 language pairs show that our proposed approach recovers the original few-shot capabilities while keeping the added benefits of finetuning.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:大型语言模型（LLM）是机器翻译（MT）的有望之路。然而，当前LLM基于的MT系统很脆弱：它们效果受选择少量示例的影响很大，并且经常需要额外处理因过量生成。其他方法，如特定化在翻译指令上的训练， computationally expensive 并可能会削弱在上下文学习Capabilities。在这篇论文中，我们对这个问题进行了更加细化的分析。我们首先显示，使用 adapter-based 特定化可以与传统训练方法相当，同时减少训练参数的数量，相对于50。此方法还超越了少量示例推荐和无需后处理或上下文示例。然而，我们发现，训练通常会降低少量示例性能，阻碍适应能力。最后，为了取得两个世界的优点，我们提议一种简单的方法，在特定化过程中包含少量示例。在10种语言对的实验中，我们的提议方法可以恢复原始的少量示例能力，同时保留特定化的优点。
</details></li>
</ul>
<hr>
<h2 id="The-Past-Present-and-Future-of-Typological-Databases-in-NLP"><a href="#The-Past-Present-and-Future-of-Typological-Databases-in-NLP" class="headerlink" title="The Past, Present, and Future of Typological Databases in NLP"></a>The Past, Present, and Future of Typological Databases in NLP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13440">http://arxiv.org/abs/2310.13440</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emi Baylor, Esther Ploeger, Johannes Bjerva</li>
<li>for: 这研究旨在探讨大规模语言类型学数据库的不一致性，以及这些数据库在自然语言处理（NLP）领域的应用。</li>
<li>methods: 该研究使用了系统性的方法来探讨类型学数据库之间的不一致性，以及这些数据库在NLP领域的应用。</li>
<li>results: 研究发现，大规模语言类型学数据库存在较多的不一致性，这些不一致性的原因包括编码错误、语言变化以及语义差异。此外，研究还发现，一种连续的类型学视角可以帮助解决这些不一致性问题，并且这种视角在未来可能会在语言模型化中发挥重要作用。<details>
<summary>Abstract</summary>
Typological information has the potential to be beneficial in the development of NLP models, particularly for low-resource languages. Unfortunately, current large-scale typological databases, notably WALS and Grambank, are inconsistent both with each other and with other sources of typological information, such as linguistic grammars. Some of these inconsistencies stem from coding errors or linguistic variation, but many of the disagreements are due to the discrete categorical nature of these databases. We shed light on this issue by systematically exploring disagreements across typological databases and resources, and their uses in NLP, covering the past and present. We next investigate the future of such work, offering an argument that a continuous view of typological features is clearly beneficial, echoing recommendations from linguistics. We propose that such a view of typology has significant potential in the future, including in language modeling in low-resource scenarios.
</details>
<details>
<summary>摘要</summary>
typological information 有潜在的优势可以帮助自然语言处理（NLP）模型的发展,特别是 для低资源语言。不幸的是,当前大规模的 typological databases 和 Grambank 存在差异,不仅与其他一些 typological information 源相关,还与自己存在差异。这些差异的原因包括编码错误或语言变化,但许多这些不一致的原因是由于这些数据库的精确性不足。我们通过系统地探讨这些数据库之间的不一致,以及它们在 NLP 中的应用,从过去和现在两个方面来探讨这个问题。我们 subsequentially 探讨未来这种工作的发展,并提出一种持续视角的 typology 特征是非常有利,这与语言学界的建议相符。我们建议这种持续视角在未来会拥有显著的潜在价值,包括语言模型在低资源enario 中的应用。
</details></li>
</ul>
<hr>
<h2 id="Conversation-Chronicles-Towards-Diverse-Temporal-and-Relational-Dynamics-in-Multi-Session-Conversations"><a href="#Conversation-Chronicles-Towards-Diverse-Temporal-and-Relational-Dynamics-in-Multi-Session-Conversations" class="headerlink" title="Conversation Chronicles: Towards Diverse Temporal and Relational Dynamics in Multi-Session Conversations"></a>Conversation Chronicles: Towards Diverse Temporal and Relational Dynamics in Multi-Session Conversations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13420">http://arxiv.org/abs/2310.13420</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/conversation-chronicles/conversation-chronicles">https://github.com/conversation-chronicles/conversation-chronicles</a></li>
<li>paper_authors: Jihyoung Jang, Minseong Boo, Hyounghun Kim</li>
<li>for: This paper aims to address the limitation of existing open-domain chatbot research by incorporating contextual information from multiple consecutive sessions into the conversation setup.</li>
<li>methods: The authors introduce a new 1M multi-session dialogue dataset called Conversation Chronicles, which includes time intervals and fine-grained speaker relationships. They also propose a dialogue model called ReBot, which consists of chronological summarization and dialogue generation modules.</li>
<li>results: The human evaluation shows that dialogue episodes in Conversation Chronicles reflect the properties of long-term conversations while maintaining coherent and consistent interactions across all sessions. ReBot, trained on Conversation Chronicles, demonstrates long-term context understanding with a high human engagement score.<details>
<summary>Abstract</summary>
In the field of natural language processing, open-domain chatbots have emerged as an important research topic. However, a major limitation of existing open-domain chatbot research is its singular focus on short single-session dialogue, neglecting the potential need for understanding contextual information in multiple consecutive sessions that precede an ongoing dialogue. Among the elements that compose the context in multi-session conversation settings, the time intervals between sessions and the relationships between speakers would be particularly important. Despite their importance, current research efforts have not sufficiently addressed these dialogical components. In this paper, we introduce a new 1M multi-session dialogue dataset, called Conversation Chronicles, for implementing a long-term conversation setup in which time intervals and fine-grained speaker relationships are incorporated. Following recent works, we exploit a large language model to produce the data. The extensive human evaluation shows that dialogue episodes in Conversation Chronicles reflect those properties while maintaining coherent and consistent interactions across all the sessions. We also propose a dialogue model, called ReBot, which consists of chronological summarization and dialogue generation modules using only around 630M parameters. When trained on Conversation Chronicles, ReBot demonstrates long-term context understanding with a high human engagement score.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:在自然语言处理领域，开放领域 чат机器人已经成为重要的研究主题。然而，现有的开放领域 чат机器人研究存在一个重要的限制，即它忽略了多场会话中的多个会话session的上下文信息。在多场会话设置中，时间间隔和对话者之间的关系是非常重要的。尽管它们的重要性，现有的研究努力并没有充分考虑这些对话组成部分。在这篇论文中，我们介绍了一个新的100万多场会话对话集合，called Conversation Chronicles，用于实现长期对话设置，在该设置中，时间间隔和细化的对话者关系都被考虑。采用最新的大语言模型生成数据。人工评估表明，Conversation Chronicles中的对话集合具备了这些属性，同时保持了一致和一致的交互。我们还提出了一个对话模型，called ReBot，它包括时间顺序概要和对话生成模块，只使用约630M参数。当训练在Conversation Chronicles上时，ReBot能够展示长期上下文理解，并获得了高度的人工参与度。
</details></li>
</ul>
<hr>
<h2 id="Towards-Enhancing-Relational-Rules-for-Knowledge-Graph-Link-Prediction"><a href="#Towards-Enhancing-Relational-Rules-for-Knowledge-Graph-Link-Prediction" class="headerlink" title="Towards Enhancing Relational Rules for Knowledge Graph Link Prediction"></a>Towards Enhancing Relational Rules for Knowledge Graph Link Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13411">http://arxiv.org/abs/2310.13411</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ninggirsu/run-gnn">https://github.com/ninggirsu/run-gnn</a></li>
<li>paper_authors: Shuhan Wu, Huaiyu Wan, Wei Chen, Yuting Wu, Junfeng Shen, Youfang Lin</li>
<li>for: 提高知识图reasoning的性能</li>
<li>methods: 使用query related fusion gate unit模型关系的顺序性，并使用缓冲更新机制缓解延迟的实体信息传递问题</li>
<li>results: 在多个数据集上表现出优于传递和推导链预测任务<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have shown promising performance for knowledge graph reasoning. A recent variant of GNN called progressive relational graph neural network (PRGNN), utilizes relational rules to infer missing knowledge in relational digraphs and achieves notable results. However, during reasoning with PRGNN, two important properties are often overlooked: (1) the sequentiality of relation composition, where the order of combining different relations affects the semantics of the relational rules, and (2) the lagged entity information propagation, where the transmission speed of required information lags behind the appearance speed of new entities. Ignoring these properties leads to incorrect relational rule learning and decreased reasoning accuracy. To address these issues, we propose a novel knowledge graph reasoning approach, the Relational rUle eNhanced Graph Neural Network (RUN-GNN). Specifically, RUN-GNN employs a query related fusion gate unit to model the sequentiality of relation composition and utilizes a buffering update mechanism to alleviate the negative effect of lagged entity information propagation, resulting in higher-quality relational rule learning. Experimental results on multiple datasets demonstrate the superiority of RUN-GNN is superior on both transductive and inductive link prediction tasks.
</details>
<details>
<summary>摘要</summary>
GRaph Neural Networks (GNNs) 有示 promise的表现力量知识图理解。一种最近的 GNN 变体called progressive relational graph neural network (PRGNN) 利用关系规则来推理知识图中缺失的信息，并取得了显著的成果。然而，在 PRGNN 中进行理解时，有两个重要的特性通常被忽略：（1）关系组合的顺序性，其中不同关系的组合顺序对 semantics 的关系规则产生影响，以及（2）延迟的实体信息传递，其中新出现的实体信息传递的速度落后于实体信息的需求速度。忽略这些特性会导致 incorrect 的关系规则学习和降低理解精度。为了解决这些问题，我们提出了一种新的知识图理解方法，即 Relational rUle eNhanced Graph Neural Network (RUN-GNN)。具体来说，RUN-GNN 使用一个查询相关融合门控制器来模型关系组合的顺序性，并使用一个缓冲更新机制来缓解实体信息传递的负面影响，从而实现更高质量的关系规则学习。实验结果表明，RUN-GNN 在多个数据集上的传递性和概率链预测任务上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Explicit-Alignment-and-Many-to-many-Entailment-Based-Reasoning-for-Conversational-Machine-Reading"><a href="#Explicit-Alignment-and-Many-to-many-Entailment-Based-Reasoning-for-Conversational-Machine-Reading" class="headerlink" title="Explicit Alignment and Many-to-many Entailment Based Reasoning for Conversational Machine Reading"></a>Explicit Alignment and Many-to-many Entailment Based Reasoning for Conversational Machine Reading</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13409">http://arxiv.org/abs/2310.13409</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AidenYo/BiAE">https://github.com/AidenYo/BiAE</a></li>
<li>paper_authors: Yangyang Luo, Shiyu Tian, Caixia Yuan, Xiaojie Wang</li>
<li>for: 本研究旨在提高对话机器阅读（CMR）系统的性能，特别是在多Turn对话中对文档和用户提供的信息进行Alignment。</li>
<li>methods: 该方法使用了轻量级多对多推理模块进行决策，并直接基于文档和已问题 generates follow-up问题。</li>
<li>results: 该方法在微准确率方面达到了领先的状态，并在公共领导者数据集ShARC上排名第一。<details>
<summary>Abstract</summary>
Conversational Machine Reading (CMR) requires answering a user's initial question through multi-turn dialogue interactions based on a given document. Although there exist many effective methods, they largely neglected the alignment between the document and the user-provided information, which significantly affects the intermediate decision-making and subsequent follow-up question generation. To address this issue, we propose a pipeline framework that (1) aligns the aforementioned two sides in an explicit way, (2)makes decisions using a lightweight many-to-many entailment reasoning module, and (3) directly generates follow-up questions based on the document and previously asked questions. Our proposed method achieves state-of-the-art in micro-accuracy and ranks the first place on the public leaderboard of the CMR benchmark dataset ShARC.
</details>
<details>
<summary>摘要</summary>
对话机器阅读（CMR）需要基于给定文档回答用户的初始问题，通过多回交流互动。虽然现有许多有效方法，但它们忽略了文档和用户提供的信息之间的匹配，这对于中间决策和 subsequential 询问生成具有重要影响。为解决这个问题，我们提议一个管道式框架，包括以下三个部分：1. 显式对文档和用户提供的信息进行匹配，以确保它们之间的Alignment。2. 使用轻量级多对多推理模块进行决策，以便更好地处理多个问题。3. 基于文档和之前提出的问题，直接生成 subsequential 询问。我们的提议方法在微准确性方面达到了领先水平，并在公共领导板块上名列第一名。
</details></li>
</ul>
<hr>
<h2 id="Cache-me-if-you-Can-an-Online-Cost-aware-Teacher-Student-framework-to-Reduce-the-Calls-to-Large-Language-Models"><a href="#Cache-me-if-you-Can-an-Online-Cost-aware-Teacher-Student-framework-to-Reduce-the-Calls-to-Large-Language-Models" class="headerlink" title="Cache me if you Can: an Online Cost-aware Teacher-Student framework to Reduce the Calls to Large Language Models"></a>Cache me if you Can: an Online Cost-aware Teacher-Student framework to Reduce the Calls to Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13395">http://arxiv.org/abs/2310.13395</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stoyian/OCaTS">https://github.com/stoyian/OCaTS</a></li>
<li>paper_authors: Ilias Stogiannidis, Stavros Vassos, Prodromos Malakasiotis, Ion Androutsopoulos</li>
<li>for: This paper aims to reduce the operating expense (OpEx) of using third-party language model (LLM) services for small and medium-sized enterprises (SMEs) by caching previous LLM responses and training local inexpensive models.</li>
<li>methods: The proposed framework includes criteria for deciding when to trust the local model or call the LLM, as well as a methodology to tune the criteria and measure the tradeoff between performance and cost.</li>
<li>results: Experimental results using two LLMs (GPT-3.5 and GPT-4) and two inexpensive students (k-NN classifier and Multi-Layer Perceptron) on two common business tasks (intent recognition and sentiment analysis) show that significant OpEx savings can be obtained with only slightly lower performance.<details>
<summary>Abstract</summary>
Prompting Large Language Models (LLMs) performs impressively in zero- and few-shot settings. Hence, small and medium-sized enterprises (SMEs) that cannot afford the cost of creating large task-specific training datasets, but also the cost of pretraining their own LLMs, are increasingly turning to third-party services that allow them to prompt LLMs. However, such services currently require a payment per call, which becomes a significant operating expense (OpEx). Furthermore, customer inputs are often very similar over time, hence SMEs end-up prompting LLMs with very similar instances. We propose a framework that allows reducing the calls to LLMs by caching previous LLM responses and using them to train a local inexpensive model on the SME side. The framework includes criteria for deciding when to trust the local model or call the LLM, and a methodology to tune the criteria and measure the tradeoff between performance and cost. For experimental purposes, we instantiate our framework with two LLMs, GPT-3.5 or GPT-4, and two inexpensive students, a k-NN classifier or a Multi-Layer Perceptron, using two common business tasks, intent recognition and sentiment analysis. Experimental results indicate that significant OpEx savings can be obtained with only slightly lower performance.
</details>
<details>
<summary>摘要</summary>
LLMs 在零或几个预测设置下表现出色，因此小型和中型企业（SMEs）不能负担创建大量任务特定的训练数据集和自己的 LLM 的预训练成本，现在更多地使用第三方服务。然而，现有服务需每次调用 LLM 的费用，这成为了运营成本（OpEx）的一大部分。此外，客户输入通常在时间上很相似，因此 SMEs 通常会向 LLM 提交非常相似的输入。我们提议一个框架，可以减少对 LLM 的调用次数，通过缓存之前 LLM 的回答并使用其来训练在 SME 端的低成本模型。该框架包括决定是否信任本地模型或调用 LLM 的标准，以及跟踪这些标准的调整和评估。为实验目的，我们实现了我们的框架，使用 GPT-3.5 或 GPT-4 两个 LLM，以及两个低成本学生，一个 k-NN 分类器或一个多层感知器，使用两个常见的商业任务，意图识别和情感分析。实验结果表明，可以通过减少 OpEx 来获得显著的成本节省，只有微不足的性能下降。
</details></li>
</ul>
<hr>
<h2 id="Tuna-Instruction-Tuning-using-Feedback-from-Large-Language-Models"><a href="#Tuna-Instruction-Tuning-using-Feedback-from-Large-Language-Models" class="headerlink" title="Tuna: Instruction Tuning using Feedback from Large Language Models"></a>Tuna: Instruction Tuning using Feedback from Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13385">http://arxiv.org/abs/2310.13385</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/lmops">https://github.com/microsoft/lmops</a></li>
<li>paper_authors: Haoran Li, Yiran Liu, Xingxing Zhang, Wei Lu, Furu Wei</li>
<li>for: 这 paper 的目的是提出一种基于 direct outputs 的 Instruction-tuned LLM，以提高模型的行为与人类偏好的 align。</li>
<li>methods: 这 paper 使用了两种新的方法： probablistic ranking 和 contextual ranking，以增加模型的可能性生成更好的响应。</li>
<li>results: 这 paper 的模型（Tuna）在 Super Natural Instructions 和 LMentry 等 119 个测试任务上表现出色，并且可以超过一些强大的奖励学习基准。<details>
<summary>Abstract</summary>
Instruction tuning of open-source large language models (LLMs) like LLaMA, using direct outputs from more powerful LLMs such as Instruct-GPT and GPT-4, has proven to be a cost-effective way to align model behaviors with human preferences. However, the instruction-tuned model has only seen one response per instruction, lacking the knowledge of potentially better responses. In this paper, we propose finetuning an instruction-tuned LLM using our novel \textit{probabilistic ranking} and \textit{contextual ranking} approaches to increase the likelihood of generating better responses. Probabilistic ranking enables the instruction-tuned model to inherit the relative rankings of high-quality and low-quality responses from the teacher LLM. On the other hand, learning with contextual ranking allows the model to refine its own response distribution using the contextual understanding ability of stronger LLMs. Furthermore, we apply probabilistic ranking and contextual ranking sequentially to the instruction-tuned LLM. The resulting model, which we call \textbf{Tuna}, consistently improves the performance on Super Natural Instructions (119 test tasks), LMentry (25 test tasks), Vicuna QA, and can even obtain better results than several strong reinforcement learning baselines. Our code and data are available at \url{ https://github.com/microsoft/LMOps}.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）如LLaMA的指令调整，使用直接输出更强大的LLM的指令，如Instruct-GPT和GPT-4，已经证明是一种经济的方式来调整模型的行为与人类偏好相Alignment。然而，指令调整模型只看到了一个响应，缺乏更好的响应的知识。在这篇论文中，我们提议对指令调整LLM进行迭代finetuning，使用我们的新的概率排序和上下文排序方法来增加生成更好的响应的可能性。概率排序让指令调整模型继承更强大LLM的高质量和低质量响应的相对排名。然而，通过上下文排序来让模型使用更强大LLM的上下文理解能力来细化自己的响应分布。此外，我们采用概率排序和上下文排序两个顺序来处理指令调整LLM。得到的模型，我们称之为Tuna，在Super Natural Instructions（119个测试任务）、LMentry（25个测试任务）、Vicuna QA等测试任务上表现出色，甚至可以超过一些强大的强化学习基elines。我们的代码和数据可以在https://github.com/microsoft/LMOps上获取。
</details></li>
</ul>
<hr>
<h2 id="APP-Adaptive-Prototypical-Pseudo-Labeling-for-Few-shot-OOD-Detection"><a href="#APP-Adaptive-Prototypical-Pseudo-Labeling-for-Few-shot-OOD-Detection" class="headerlink" title="APP: Adaptive Prototypical Pseudo-Labeling for Few-shot OOD Detection"></a>APP: Adaptive Prototypical Pseudo-Labeling for Few-shot OOD Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13380">http://arxiv.org/abs/2310.13380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pei Wang, Keqing He, Yutao Mou, Xiaoshuai Song, Yanan Wu, Jingang Wang, Yunsen Xian, Xunliang Cai, Weiran Xu</li>
<li>for: 这篇论文的目的是提出一种在仅有几个标注IND意图的情况下进行偏出版本检测的方法。</li>
<li>methods: 本文提出了一种适应式prototype pseudo-labeling（APP）方法，包括一个 prototype OOD检测框架（ProtoOOD）来帮助使用有限IND数据进行低资源OOD检测，以及一种适应式pseudo-labeling方法来生成高质量pseudo OOD&amp;IND标签。</li>
<li>results: 实验和分析显示了本方法在几据OOD检测中的效果。<details>
<summary>Abstract</summary>
Detecting out-of-domain (OOD) intents from user queries is essential for a task-oriented dialogue system. Previous OOD detection studies generally work on the assumption that plenty of labeled IND intents exist. In this paper, we focus on a more practical few-shot OOD setting where there are only a few labeled IND data and massive unlabeled mixed data that may belong to IND or OOD. The new scenario carries two key challenges: learning discriminative representations using limited IND data and leveraging unlabeled mixed data. Therefore, we propose an adaptive prototypical pseudo-labeling (APP) method for few-shot OOD detection, including a prototypical OOD detection framework (ProtoOOD) to facilitate low-resource OOD detection using limited IND data, and an adaptive pseudo-labeling method to produce high-quality pseudo OOD\&IND labels. Extensive experiments and analysis demonstrate the effectiveness of our method for few-shot OOD detection.
</details>
<details>
<summary>摘要</summary>
检测用户查询外部域（OOD）意图是任务对话系统的重要任务。先前的OOD检测研究通常假设有充足的标注IND意图数据存在。在这篇论文中，我们专注于更实际的几shotOOD设定，其中只有几个标注IND数据和大量未标注混合数据，这些数据可能属于IND或OOD。这种新的情况带来两个关键挑战：使用有限的IND数据学习准确的表示，并使用大量混合数据进行挖掘。因此，我们提出了适应型pseudo标签法（APP），包括一个 проtotypical OOD检测框架（ProtoOOD），以便在有限IND数据情况下进行低资源OOD检测，以及一种适应pseudo标签方法，以生成高质量pseudo OOD&IND标签。广泛的实验和分析表明，我们的方法在几shotOOD检测中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Cognitive-Plausibility-of-Subword-Tokenization"><a href="#Analyzing-Cognitive-Plausibility-of-Subword-Tokenization" class="headerlink" title="Analyzing Cognitive Plausibility of Subword Tokenization"></a>Analyzing Cognitive Plausibility of Subword Tokenization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13348">http://arxiv.org/abs/2310.13348</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/clap-lab/cogtok">https://github.com/clap-lab/cogtok</a></li>
<li>paper_authors: Lisa Beinborn, Yuval Pinter</li>
<li>for: 本研究的目的是评估不同语言的字根词法 tokenization 算法的认知可能性。</li>
<li>methods: 本研究使用了一种新的评估方法，通过对人类在 lexical decision 任务中的响应时间和准确率与 tokenizer 输出之间的相关性进行分析，以评估不同 tokenization 算法的认知可能性。</li>
<li>results: 研究结果显示，UnigramLM 算法在不同语言和词汇大小下的 tokenization 行为更加不具认知可能性，同时也忽略了 derivational morphemes 的覆盖率。<details>
<summary>Abstract</summary>
Subword tokenization has become the de-facto standard for tokenization, although comparative evaluations of subword vocabulary quality across languages are scarce. Existing evaluation studies focus on the effect of a tokenization algorithm on the performance in downstream tasks, or on engineering criteria such as the compression rate. We present a new evaluation paradigm that focuses on the cognitive plausibility of subword tokenization. We analyze the correlation of the tokenizer output with the response time and accuracy of human performance on a lexical decision task. We compare three tokenization algorithms across several languages and vocabulary sizes. Our results indicate that the UnigramLM algorithm yields less cognitively plausible tokenization behavior and a worse coverage of derivational morphemes, in contrast with prior work.
</details>
<details>
<summary>摘要</summary>
宋Wordtokenization已成为实际标准的分词方法，尽管相对评估语言之间的子字词词库质量的比较罕见。现有的评估研究主要关注下游任务性能的影响或工程特性如压缩率。我们提出了一种新的评估方框，关注分词器输出与人类语言决策任务的响应时间和准确率之间的相关性。我们对三种分词算法进行了多种语言和词汇大小的比较。我们的结果表明，UnigramLM算法产生的分词行为更加不具有认知可能性，同时对 derivational morpheme 的覆盖率也更差，与先前的研究不符。
</details></li>
</ul>
<hr>
<h2 id="Large-Scale-and-Multi-Perspective-Opinion-Summarization-with-Diverse-Review-Subsets"><a href="#Large-Scale-and-Multi-Perspective-Opinion-Summarization-with-Diverse-Review-Subsets" class="headerlink" title="Large-Scale and Multi-Perspective Opinion Summarization with Diverse Review Subsets"></a>Large-Scale and Multi-Perspective Opinion Summarization with Diverse Review Subsets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13340">http://arxiv.org/abs/2310.13340</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Jiang, Rui Wang, Zhihua Wei, Yu Li, Xinpeng Wang</li>
<li>for: 提供了一种基于监督学习的多角度评论摘要框架，以便对大规模评论集进行有效的摘要。</li>
<li>methods: 提出了一种受过监督的摘要框架，包括评论采样策略集和两 stage 训练方案。 评论采样策略会根据评论的 sentiment orientation 和 contrastive information value 选择不同的评论 subset。</li>
<li>results: 实验结果表明，SUBSUMM 能够从百余篇评论中生成高质量的摘要，包括 Pros、Cons 和 Verdict 摘要。此外，我们的深入分析表明，选择评论 subset 和两 stage 训练方案是提高摘要性能的关键因素。<details>
<summary>Abstract</summary>
Opinion summarization is expected to digest larger review sets and provide summaries from different perspectives. However, most existing solutions are deficient in epitomizing extensive reviews and offering opinion summaries from various angles due to the lack of designs for information selection. To this end, we propose SUBSUMM, a supervised summarization framework for large-scale multi-perspective opinion summarization. SUBSUMM consists of a review sampling strategy set and a two-stage training scheme. The sampling strategies take sentiment orientation and contrastive information value into consideration, with which the review subsets from different perspectives and quality levels can be selected. Subsequently, the summarizer is encouraged to learn from the sub-optimal and optimal subsets successively in order to capitalize on the massive input. Experimental results on AmaSum and Rotten Tomatoes datasets demonstrate that SUBSUMM is adept at generating pros, cons, and verdict summaries from hundreds of input reviews. Furthermore, our in-depth analysis verifies that the advanced selection of review subsets and the two-stage training scheme are vital to boosting the summarization performance.
</details>
<details>
<summary>摘要</summary>
文本翻译为简化中文。 существующие解决方案因缺乏信息选择的设计而无法生成覆盖广泛评论和多个角度的意见摘要。为此，我们提出了SUBSUMM，一种监督摘要框架，用于大规模多角度意见摘要。SUBSUMM包括评论采样策略集和两个阶段训练方案。采样策略考虑了 sentiment 方向和对比信息价值，可以从不同的角度和质量水平中选择评论 subset。然后，摘要器受益于大量输入，逐渐学习从优秀和优化subset中获得知识。实验结果表明，SUBSUMM可以从多达百个输入评论中生成评价、缺点和结论摘要。此外，我们的深入分析表明，选择评论subset的高级技巧和两个阶段训练方案对摘要性能产生了重要的提高作用。</sys>Here is the translation of the text into Simplified Chinese:<<sys>文本翻译为简化中文。现有的解决方案因缺乏信息选择的设计而无法生成覆盖广泛评论和多个角度的意见摘要。为此，我们提出了SUBSUMM，一种监督摘要框架，用于大规模多角度意见摘要。SUBSUMM包括评论采样策略集和两个阶段训练方案。采样策略考虑了 sentiment 方向和对比信息价值，可以从不同的角度和质量水平中选择评论 subset。然后，摘要器受益于大量输入，逐渐学习从优秀和优化subset中获得知识。实验结果表明，SUBSUMM可以从多达百个输入评论中生成评价、缺点和结论摘要。此外，我们的深入分析表明，选择评论subset的高级技巧和两个阶段训练方案对摘要性能产生了重要的提高作用。</sys>
</details></li>
</ul>
<hr>
<h2 id="Beyond-Hard-Samples-Robust-and-Effective-Grammatical-Error-Correction-with-Cycle-Self-Augmenting"><a href="#Beyond-Hard-Samples-Robust-and-Effective-Grammatical-Error-Correction-with-Cycle-Self-Augmenting" class="headerlink" title="Beyond Hard Samples: Robust and Effective Grammatical Error Correction with Cycle Self-Augmenting"></a>Beyond Hard Samples: Robust and Effective Grammatical Error Correction with Cycle Self-Augmenting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13321">http://arxiv.org/abs/2310.13321</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zetangforward/csa-gec">https://github.com/zetangforward/csa-gec</a></li>
<li>paper_authors: Zecheng Tang, Kaifeng Qi, Juntao Li, Min Zhang</li>
<li>for: 这个研究是为了提高语法错误修正模型的Robustness，对抗特定类型的攻击。</li>
<li>methods: 本研究使用了sequence-to-sequence模型，并将其攻击到四种不同的攻击类型。furthermore, the paper proposes a simple yet effective Cycle Self-Augmenting (CSA) method to improve the model’s robustness.</li>
<li>results: 实验结果显示，使用CSA方法可以帮助四种不同的基eline模型增强其Robustness，而不需要将攻击示例加入训练过程中。此外，CSA方法可以降低模型对于没有错误的数据的适应性，并提高模型对于未见过的数据的一致性。<details>
<summary>Abstract</summary>
Recent studies have revealed that grammatical error correction methods in the sequence-to-sequence paradigm are vulnerable to adversarial attack, and simply utilizing adversarial examples in the pre-training or post-training process can significantly enhance the robustness of GEC models to certain types of attack without suffering too much performance loss on clean data. In this paper, we further conduct a thorough robustness evaluation of cutting-edge GEC methods for four different types of adversarial attacks and propose a simple yet very effective Cycle Self-Augmenting (CSA) method accordingly. By leveraging the augmenting data from the GEC models themselves in the post-training process and introducing regularization data for cycle training, our proposed method can effectively improve the model robustness of well-trained GEC models with only a few more training epochs as an extra cost. More concretely, further training on the regularization data can prevent the GEC models from over-fitting on easy-to-learn samples and thus can improve the generalization capability and robustness towards unseen data (adversarial noise/samples). Meanwhile, the self-augmented data can provide more high-quality pseudo pairs to improve model performance on the original testing data. Experiments on four benchmark datasets and seven strong models indicate that our proposed training method can significantly enhance the robustness of four types of attacks without using purposely built adversarial examples in training. Evaluation results on clean data further confirm that our proposed CSA method significantly improves the performance of four baselines and yields nearly comparable results with other state-of-the-art models. Our code is available at https://github.com/ZetangForward/CSA-GEC.
</details>
<details>
<summary>摘要</summary>
近期研究发现，序列到序列框架中的语法错误纠正方法容易受到敌意攻击，而使用敌意示例在预训练或后训练过程中可以有效提高GEC模型对certain类型的攻击的抵抗力，而不是受到过多的clean数据影响。在这篇论文中，我们进一步进行了四种不同类型的敌意攻击的精orous evaluate，并提出了一种简单 yet very effective的自回归增强（CSA）方法。通过在后训练过程中利用GEC模型自己生成的增强数据，并在训练数据中引入循环训练数据，我们的提议的方法可以有效提高已经训练过的GEC模型的模型 robustness，只需要增加一些更多的训练粒度。更具体地说，进一步训练在循环训练数据上可以防止GEC模型过拟合易学习样本，提高模型的总体化能力和对未看到数据（敌意噪音）的Robustness。同时，自回归数据可以为模型提供更多的高质量 Pseudo pair，提高模型在原始测试数据上的性能。实验结果表明，我们的提议的训练方法可以有效提高四种攻击类型的Robustness，而不需要在训练过程中使用特制的敌意示例。 clean数据上的评估结果还证明，我们的CSA方法可以大幅提高四个基eline的性能，并与其他当前领先模型几乎相当。我们的代码可以在https://github.com/ZetangForward/CSA-GEC中找到。
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Sharpness-Aware-Quantization-for-Pre-trained-Language-Models"><a href="#Zero-Shot-Sharpness-Aware-Quantization-for-Pre-trained-Language-Models" class="headerlink" title="Zero-Shot Sharpness-Aware Quantization for Pre-trained Language Models"></a>Zero-Shot Sharpness-Aware Quantization for Pre-trained Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13315">http://arxiv.org/abs/2310.13315</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miaoxi Zhu, Qihuang Zhong, Li Shen, Liang Ding, Juhua Liu, Bo Du, Dacheng Tao</li>
<li>for: 这篇论文旨在提出一个零shot量化框架，来实现零shot量化的各种语言模型（PLM）。</li>
<li>methods: 本文使用的方法包括零shot量化和零shot预测，并且提出了一个名为SAM-SGA优化的算法，用于提高量化精度和模型泛化。</li>
<li>results: 实验结果显示，本文的方法可以对11个任务中的描述性和生成性PLM都带来明显和重要的性能提升，最高提升为+6.98均值分数。此外，本文也证明了这个方法可以改善模型的泛化性。<details>
<summary>Abstract</summary>
Quantization is a promising approach for reducing memory overhead and accelerating inference, especially in large pre-trained language model (PLM) scenarios. While having no access to original training data due to security and privacy concerns has emerged the demand for zero-shot quantization. Most of the cutting-edge zero-shot quantization methods primarily 1) apply to computer vision tasks, and 2) neglect of overfitting problem in the generative adversarial learning process, leading to sub-optimal performance. Motivated by this, we propose a novel zero-shot sharpness-aware quantization (ZSAQ) framework for the zero-shot quantization of various PLMs. The key algorithm in solving ZSAQ is the SAM-SGA optimization, which aims to improve the quantization accuracy and model generalization via optimizing a minimax problem. We theoretically prove the convergence rate for the minimax optimization problem and this result can be applied to other nonconvex-PL minimax optimization frameworks. Extensive experiments on 11 tasks demonstrate that our method brings consistent and significant performance gains on both discriminative and generative PLMs, i.e., up to +6.98 average score. Furthermore, we empirically validate that our method can effectively improve the model generalization.
</details>
<details>
<summary>摘要</summary>
“量化是一种具有潜在的方法来降低记忆预算和加速推导，特别在大型预训语言模型（PLM）的情况下。由于安全和隐私问题的缘故，无法存取原始训练数据的需求导致了零统计量化的需求。现有的大部分cutting-edge零统计量化方法主要应用于计算机视觉任务，并且忽略了生成对抗学习过程中的过溢问题，导致表现不佳。骉于这，我们提出了一个新的零统计锐度感知量化（ZSAQ）框架，用于零统计量化不同PLM。关键算法在解决ZSAQ中是SAM-SGA优化，旨在提高量化精度和模型通用化via优化最小最大问题。我们 theoretically prove了最小最大问题的收敛率，这个结果可以应用到其他非对称PL最小最大问题框架。实验结果显示，我们的方法可以在11个任务中提供了稳定和有意义的性能提升，最高提升率为+6.98。此外，我们还证明了我们的方法可以有效地提高模型通用化。”
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Impact-of-Corpus-Diversity-on-Financial-Pretrained-Language-Models"><a href="#Exploring-the-Impact-of-Corpus-Diversity-on-Financial-Pretrained-Language-Models" class="headerlink" title="Exploring the Impact of Corpus Diversity on Financial Pretrained Language Models"></a>Exploring the Impact of Corpus Diversity on Financial Pretrained Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13312">http://arxiv.org/abs/2310.13312</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deep-over/film">https://github.com/deep-over/film</a></li>
<li>paper_authors: Jaeyoung Choe, Keonwoong Noh, Nayeon Kim, Seyun Ahn, Woohwan Jung</li>
<li>for: 这种论文主要为了解决金融领域语言模型的不足之处，提高金融领域下推理语言模型的性能。</li>
<li>methods: 该论文使用了各种金融领域的文本数据集，对这些数据集进行了广泛的采集和训练，并使用了一种新的训练策略来提高模型的性能。</li>
<li>results: 论文的实验结果表明，新提出的金融语言模型（FiLM）不仅可以在金融领域上超越现有的专业语言模型，还可以在未经见过的文本数据集上达到更高的性能。<details>
<summary>Abstract</summary>
Over the past few years, various domain-specific pretrained language models (PLMs) have been proposed and have outperformed general-domain PLMs in specialized areas such as biomedical, scientific, and clinical domains. In addition, financial PLMs have been studied because of the high economic impact of financial data analysis. However, we found that financial PLMs were not pretrained on sufficiently diverse financial data. This lack of diverse training data leads to a subpar generalization performance, resulting in general-purpose PLMs, including BERT, often outperforming financial PLMs on many downstream tasks. To address this issue, we collected a broad range of financial corpus and trained the Financial Language Model (FiLM) on these diverse datasets. Our experimental results confirm that FiLM outperforms not only existing financial PLMs but also general domain PLMs. Furthermore, we provide empirical evidence that this improvement can be achieved even for unseen corpus groups.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)在过去几年，一些领域特定的预训练语言模型（PLMs）已经被提出，并在各种专业领域，如生物医学、科学和医疗领域中表现出色。此外，由于金融数据分析的高经济影响，金融PLMs也被研究。然而，我们发现金融PLMs未被训练在充分多样化的金融数据上。这种缺乏多样化训练数据导致总体性能下降，使得通用领域PLMs，包括BERT，在许多下游任务中表现更好。为解决这个问题，我们收集了广泛的金融文献，并将这些多样化的数据集用于训练金融语言模型（FiLM）。我们的实验结果表明，FiLM不仅能超越现有的金融PLMs，还能超越通用领域PLMs。此外，我们还提供了实验证据，表明这种改进可以在未看到的文献组中实现。
</details></li>
</ul>
<hr>
<h2 id="Test-Time-Self-Adaptive-Small-Language-Models-for-Question-Answering"><a href="#Test-Time-Self-Adaptive-Small-Language-Models-for-Question-Answering" class="headerlink" title="Test-Time Self-Adaptive Small Language Models for Question Answering"></a>Test-Time Self-Adaptive Small Language Models for Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13307">http://arxiv.org/abs/2310.13307</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, Jong C. Park</li>
<li>for: 这个研究是为了测试小型自适应语言模型（LM）的可行性，以及它们在不同的问题 answering（QA）任务中的表现。</li>
<li>methods: 研究使用了自适应策略，将多个答案生成并对其进行整合，并且过滤出低质量的样本以减少错误的标签。</li>
<li>results: 研究发现，这种自适应策略可以帮助小型LM在不同的问题中表现更好，并且具有更高的稳定性。<details>
<summary>Abstract</summary>
Recent instruction-finetuned large language models (LMs) have achieved notable performances in various tasks, such as question-answering (QA). However, despite their ability to memorize a vast amount of general knowledge across diverse tasks, they might be suboptimal on specific tasks due to their limited capacity to transfer and adapt knowledge to target tasks. Moreover, further finetuning LMs with labeled datasets is often infeasible due to their absence, but it is also questionable if we can transfer smaller LMs having limited knowledge only with unlabeled test data. In this work, we show and investigate the capabilities of smaller self-adaptive LMs, only with unlabeled test data. In particular, we first stochastically generate multiple answers, and then ensemble them while filtering out low-quality samples to mitigate noise from inaccurate labels. Our proposed self-adaption strategy demonstrates significant performance improvements on benchmark QA datasets with higher robustness across diverse prompts, enabling LMs to stay stable. Code is available at: https://github.com/starsuzi/T-SAS.
</details>
<details>
<summary>摘要</summary>
现代指令精细调整大型语言模型（LM）在多种任务上已经实现了各种优秀的表现，如问答（QA）。然而，尽管它们可以储存大量的通用知识，但可能因为缺乏目标任务的特定知识而表现不佳。此外，进一步在标注数据缺乏的情况下进行LM的训练是常见的，但是是否可以将更小的LM通过无标注测试数据进行学习呢？在这项工作中，我们展示了和研究了更小的自适应LM，只使用无标注测试数据进行学习。具体来说，我们首先随机生成多个答案，然后将它们ensemble，并对它们进行筛选，以mitigate噪音从不准确的标签中。我们的自适应策略在 benchmark QA 数据集上显示了显著的性能提升，并且具有更高的多样性和稳定性，使LM在多种提问下能够稳定。代码可以在：https://github.com/starsuzi/T-SAS 中找到。
</details></li>
</ul>
<hr>
<h2 id="Interpreting-Indirect-Answers-to-Yes-No-Questions-in-Multiple-Languages"><a href="#Interpreting-Indirect-Answers-to-Yes-No-Questions-in-Multiple-Languages" class="headerlink" title="Interpreting Indirect Answers to Yes-No Questions in Multiple Languages"></a>Interpreting Indirect Answers to Yes-No Questions in Multiple Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13290">http://arxiv.org/abs/2310.13290</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wang-zijie/yn-question-multilingual">https://github.com/wang-zijie/yn-question-multilingual</a></li>
<li>paper_authors: Zijie Wang, Md Mosharaf Hossain, Shivam Mathur, Terry Cruz Melo, Kadir Bulut Ozler, Keun Hee Park, Jacob Quintero, MohammadHossein Rezaei, Shreya Nupur Shakya, Md Nayem Uddin, Eduardo Blanco</li>
<li>for: 这篇论文主要针对响应问题，即回答问题时，答案是否直接回答问题。</li>
<li>methods: 该论文使用远程指导方法收集训练数据，并证明直接回答（即包含肯定或否定词）可以帮助模型理解间接回答。</li>
<li>results: 实验结果显示，在训练数据可以通过远程指导方法获得时，单语言精度提升是有利的（5种语言），而跨语言精度提升总是有利（8种语言）。<details>
<summary>Abstract</summary>
Yes-no questions expect a yes or no for an answer, but people often skip polar keywords. Instead, they answer with long explanations that must be interpreted. In this paper, we focus on this challenging problem and release new benchmarks in eight languages. We present a distant supervision approach to collect training data. We also demonstrate that direct answers (i.e., with polar keywords) are useful to train models to interpret indirect answers (i.e., without polar keywords). Experimental results demonstrate that monolingual fine-tuning is beneficial if training data can be obtained via distant supervision for the language of interest (5 languages). Additionally, we show that cross-lingual fine-tuning is always beneficial (8 languages).
</details>
<details>
<summary>摘要</summary>
Yes-no问题通常需要简单的Yes或No回答，但人们经常会跳过极性词。在这篇论文中，我们关注这个挑战问题，并发布八种语言的新标准 benchmark。我们采用远程指导方法来收集训练数据。我们还证明了直接回答（即包含极性词）对于解释 indirect answers（即无极性词）的训练非常有用。实验结果表明，对于语言兴趣的语言，远程指导下的单语言精度调教是有利的。此外，我们还证明了跨语言精度调教总是有利（8种语言）。
</details></li>
</ul>
<hr>
<h2 id="SALMONN-Towards-Generic-Hearing-Abilities-for-Large-Language-Models"><a href="#SALMONN-Towards-Generic-Hearing-Abilities-for-Large-Language-Models" class="headerlink" title="SALMONN: Towards Generic Hearing Abilities for Large Language Models"></a>SALMONN: Towards Generic Hearing Abilities for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13289">http://arxiv.org/abs/2310.13289</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bytedance/salmonn">https://github.com/bytedance/salmonn</a></li>
<li>paper_authors: Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Chao Zhang</li>
<li>For: 这个论文旨在提出一种能够直接处理和理解通用音频输入的多模态模型（SALMONN），它将预训练的文本大语言模型（LLM）与语音和音频编码器结合在一起，以实现多种语音和音频任务的竞争性表现。* Methods: 这篇论文使用了一种混合多模态的方法，将预训练的文本大语言模型与语音和音频编码器结合在一起，以实现多种语音和音频任务的竞争性表现。* Results: 这篇论文的实验结果表明，SALMONN模型可以在多种语音和音频任务上实现竞争性表现，并且具有一些不可预期的跨模态能力，如语音翻译到未知语言、语音基于槽 filling、听话问答、音频故事等等。<details>
<summary>Abstract</summary>
Hearing is arguably an essential ability of artificial intelligence (AI) agents in the physical world, which refers to the perception and understanding of general auditory information consisting of at least three types of sounds: speech, audio events, and music. In this paper, we propose SALMONN, a speech audio language music open neural network, built by integrating a pre-trained text-based large language model (LLM) with speech and audio encoders into a single multimodal model. SALMONN enables the LLM to directly process and understand general audio inputs and achieve competitive performances on a number of speech and audio tasks used in training, such as automatic speech recognition and translation, auditory-information-based question answering, emotion recognition, speaker verification, and music and audio captioning \textit{etc.} SALMONN also has a diverse set of emergent abilities unseen in the training, which includes but is not limited to speech translation to untrained languages, speech-based slot filling, spoken-query-based question answering, audio-based storytelling, and speech audio co-reasoning \textit{etc}. The presence of the cross-modal emergent abilities is studied, and a novel few-shot activation tuning approach is proposed to activate such abilities of SALMONN. To our knowledge, SALMONN is the first model of its type and can be regarded as a step towards AI with generic hearing abilities. An interactive demo of SALMONN is available at \texttt{\url{https://github.com/bytedance/SALMONN}, and the training code and model checkpoints will be released upon acceptance.
</details>
<details>
<summary>摘要</summary>
听见是人工智能（AI）机器人在物理世界中的重要能力，指的是理解和处理一般声音信息，包括语音、音频事件和音乐等三种类型的声音。在这篇论文中，我们提出了一种名为SALMONN的语音音乐开放神经网络，通过将预训练的文本大型语言模型（LLM）与语音和音频编码器结合在一起而实现。SALMONN使得LLM可以直接处理和理解一般音频输入，并在训练中使用的各种语音和音频任务中达到竞争性的表现，如自动语音识别和翻译、听力信息基于问题回答、情感识别、人识别、音频和歌曲captioning等等。SALMONN还具有许多未在训练中看到的新的能力，包括但不限于语音翻译到未经训练的语言、语音基于插槽填充、声音问题回答、音频故事、语音音频合理等等。我们研究了这些跨模态的新能力的存在，并提出了一种新的几招活动调整方法来活化SALMONN的这些能力。到我们所知，SALMONN是首个类似的模型，可以视为人工智能机器人的听见能力的一步进步。SALMONN的交互示例可以在\url{https://github.com/bytedance/SALMONN}上查看，训练代码和模型检查点将在接受后发布。
</details></li>
</ul>
<hr>
<h2 id="InvGC-Robust-Cross-Modal-Retrieval-by-Inverse-Graph-Convolution"><a href="#InvGC-Robust-Cross-Modal-Retrieval-by-Inverse-Graph-Convolution" class="headerlink" title="InvGC: Robust Cross-Modal Retrieval by Inverse Graph Convolution"></a>InvGC: Robust Cross-Modal Retrieval by Inverse Graph Convolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13276">http://arxiv.org/abs/2310.13276</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yimuwangcs/Better_Cross_Modal_Retrieval">https://github.com/yimuwangcs/Better_Cross_Modal_Retrieval</a></li>
<li>paper_authors: Xiangru Jian, Yimu Wang</li>
<li>for: 解决跨模态检索中的表达缩排问题，提高检索性能。</li>
<li>methods: 引入InvGC方法，一种基于图 convolution和平均 pooling的后处理技术，以及LocalAdj提升方法，用于提高 InvGC 的效率和效果。</li>
<li>results: 对多个跨模态benchmark和方法进行了实验验证，并证明了 InvGC 和 InvGC w&#x2F;LocalAdj 可以有效地 mitigate表达缩排问题，提高检索性能。<details>
<summary>Abstract</summary>
Over recent decades, significant advancements in cross-modal retrieval are mainly driven by breakthroughs in visual and linguistic modeling. However, a recent study shows that multi-modal data representations tend to cluster within a limited convex cone (as representation degeneration problem), which hinders retrieval performance due to the inseparability of these representations. In our study, we first empirically validate the presence of the representation degeneration problem across multiple cross-modal benchmarks and methods. Next, to address it, we introduce a novel method, called InvGC, a post-processing technique inspired by graph convolution and average pooling. Specifically, InvGC defines the graph topology within the datasets and then applies graph convolution in a subtractive manner. This method effectively separates representations by increasing the distances between data points. To improve the efficiency and effectiveness of InvGC, we propose an advanced graph topology, LocalAdj, which only aims to increase the distances between each data point and its nearest neighbors. To understand why InvGC works, we present a detailed theoretical analysis, proving that the lower bound of recall will be improved after deploying InvGC. Extensive empirical results show that InvGC and InvGC w/LocalAdj significantly mitigate the representation degeneration problem, thereby enhancing retrieval performance.   Our code is available at https://github.com/yimuwangcs/Better_Cross_Modal_Retrieval
</details>
<details>
<summary>摘要</summary>
近年来，跨模态检索的大进步主要归功于视觉和语言模型的突破。然而，一项研究发现，跨模态数据表示往往归 converges within a limited convex cone（表示力 degeneration 问题），这会妨碍检索性能，因为这些表示无法分离。在我们的研究中，我们首先确认了多个跨模态benchmark和方法中的表示力 degeneration 问题的存在。然后，我们提出了一种新方法，叫做InvGC，它是基于图 convolution和平均pooling的后处理技术。具体来说，InvGC定义dataset中的图 topology，然后通过图 convolution的 subtractive 方式来分离表示。这种方法可以增加数据点之间的距离，从而提高检索性能。为了提高InvGC的效率和可效性，我们提出了一种高级图 topology，叫做LocalAdj，它只是增加每个数据点和其最近邻居之间的距离。为了解释 InvGC 是如何工作的，我们提供了详细的理论分析，证明 InvGC 后部署后，减少了下界的回归值，从而提高了检索性能。我们的代码可以在 <https://github.com/yimuwangcs/Better_Cross_Modal_Retrieval> 上获取。
</details></li>
</ul>
<hr>
<h2 id="On-the-Language-Encoder-of-Contrastive-Cross-modal-Models"><a href="#On-the-Language-Encoder-of-Contrastive-Cross-modal-Models" class="headerlink" title="On the Language Encoder of Contrastive Cross-modal Models"></a>On the Language Encoder of Contrastive Cross-modal Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13267">http://arxiv.org/abs/2310.13267</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengjie Zhao, Junya Ono, Zhi Zhong, Chieh-Hsin Lai, Yuhta Takida, Naoki Murata, Wei-Hsiang Liao, Takashi Shibuya, Hiromi Wakaki, Yuki Mitsufuji</li>
<li>for: 这篇论文旨在探讨对比式跨模型CLIP和CLAP在视频语言（VL）和音频语言（AL）任务中的表现，以及其语言编码器的质量和改进方法。</li>
<li>methods: 这篇论文使用了不监督和监督句子嵌入训练来评估语言编码器质量和跨模态任务表现。</li>
<li>results: 在VL预训练中，句子嵌入训练语言编码器质量和跨模态任务表现得到了提高，例如CyCLIP。然而，在AL预训练中，句子嵌入训练的效果较差，这可能与预训练数据的有限性有关。分析表示空间和跨模态Alignment的表示空间，发现句子嵌入训练提高了文本空间的均匀性，但是同时导致了跨模态Alignment的减退。<details>
<summary>Abstract</summary>
Contrastive cross-modal models such as CLIP and CLAP aid various vision-language (VL) and audio-language (AL) tasks. However, there has been limited investigation of and improvement in their language encoder, which is the central component of encoding natural language descriptions of image/audio into vector representations. We extensively evaluate how unsupervised and supervised sentence embedding training affect language encoder quality and cross-modal task performance. In VL pretraining, we found that sentence embedding training language encoder quality and aids in cross-modal tasks, improving contrastive VL models such as CyCLIP. In contrast, AL pretraining benefits less from sentence embedding training, which may result from the limited amount of pretraining data. We analyze the representation spaces to understand the strengths of sentence embedding training, and find that it improves text-space uniformity, at the cost of decreased cross-modal alignment.
</details>
<details>
<summary>摘要</summary>
“对比式跨模型如CLIP和CLAP在视觉语言（VL）和语音语言（AL）任务中具有帮助作用。然而，对于这些模型的语言Encoder进行训练仍然受到有限的研究和改进。我们进行了广泛的评估，探索不同的受训练方法对语言Encoder质量和跨模型任务表现的影响。在VL预训中，我们发现这种训练可以提高语言Encoder质量，并且帮助改进对比式VL模型，如CyCLIP。然而，在AL预训中，这种训练几乎没有助益，这可能是因为预训数据的限制。我们分析了表示空间，了解了句子嵌入训练带来的优点，发现它可以提高文本空间的一致性，但是价格是跨模型Alignment的降低。”
</details></li>
</ul>
<hr>
<h2 id="MoqaGPT-Zero-Shot-Multi-modal-Open-domain-Question-Answering-with-Large-Language-Model"><a href="#MoqaGPT-Zero-Shot-Multi-modal-Open-domain-Question-Answering-with-Large-Language-Model" class="headerlink" title="MoqaGPT : Zero-Shot Multi-modal Open-domain Question Answering with Large Language Model"></a>MoqaGPT : Zero-Shot Multi-modal Open-domain Question Answering with Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13265">http://arxiv.org/abs/2310.13265</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lezhang7/moqagpt">https://github.com/lezhang7/moqagpt</a></li>
<li>paper_authors: Le Zhang, Yihong Wu, Fengran Mo, Jian-Yun Nie, Aishwarya Agrawal</li>
<li>for: 这篇论文主要targets Multi-modal open-domain question answering task, aiming to improve the performance of large language models (LLMs) in this task.</li>
<li>methods: 该论文提出了一种 straightforward and flexible framework called MoqaGPT, which uses a divide-and-conquer strategy to retrieve and extract answers from multiple modalities, and then fuses this multi-modal information using LLMs to produce a final answer.</li>
<li>results: 根据MMCoQA和MultiModalQA dataset的实验结果，MoqaGPT比supervised baseline提高了F1分数37.91点和EM分数34.07点，在Zero-shot setting下也超过了基线值，提高F1分数9.5点和EM分数10.1点，并且与supervised方法的性能差距有所减少。<details>
<summary>Abstract</summary>
Multi-modal open-domain question answering typically requires evidence retrieval from databases across diverse modalities, such as images, tables, passages, etc. Even Large Language Models (LLMs) like GPT-4 fall short in this task. To enable LLMs to tackle the task in a zero-shot manner, we introduce MoqaGPT, a straightforward and flexible framework. Using a divide-and-conquer strategy that bypasses intricate multi-modality ranking, our framework can accommodate new modalities and seamlessly transition to new models for the task. Built upon LLMs, MoqaGPT retrieves and extracts answers from each modality separately, then fuses this multi-modal information using LLMs to produce a final answer. Our methodology boosts performance on the MMCoQA dataset, improving F1 by +37.91 points and EM by +34.07 points over the supervised baseline. On the MultiModalQA dataset, MoqaGPT surpasses the zero-shot baseline, improving F1 by 9.5 points and EM by 10.1 points, and significantly closes the gap with supervised methods. Our codebase is available at https://github.com/lezhang7/MOQAGPT.
</details>
<details>
<summary>摘要</summary>
多Modal开放领域问答通常需要从不同的modalities中检索证据，如图像、表格、段落等。即使大型自然语言模型（LLM）如GPT-4也有所不足。为了让LLM在零shot情况下能够完成这项任务，我们介绍了MoqaGPT框架。我们的框架采用分治策略，不需要复杂的多Modal评分，可以轻松扩展到新的modalities和新的任务。基于LLM，MoqaGPT首先从每个modalities中分别检索答案，然后使用LLM将这些多Modal信息进行融合，生成最终的答案。我们的方法在MMCoQA数据集上提高了性能，相比超参的基线，提高了F1值37.91点和EM值34.07点。在MultiModalQA数据集上，MoqaGPT超越零shot基线，提高了F1值9.5点和EM值10.1点，并在无监督方法上减少了差距。我们的代码可以在https://github.com/lezhang7/MOQAGPT上获取。
</details></li>
</ul>
<hr>
<h2 id="A-Quality-based-Syntactic-Template-Retriever-for-Syntactically-controlled-Paraphrase-Generation"><a href="#A-Quality-based-Syntactic-Template-Retriever-for-Syntactically-controlled-Paraphrase-Generation" class="headerlink" title="A Quality-based Syntactic Template Retriever for Syntactically-controlled Paraphrase Generation"></a>A Quality-based Syntactic Template Retriever for Syntactically-controlled Paraphrase Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13262">http://arxiv.org/abs/2310.13262</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xzhang00/qstr">https://github.com/xzhang00/qstr</a></li>
<li>paper_authors: Xue Zhang, Songming Zhang, Yunlong Liang, Yufeng Chen, Jian Liu, Wenjuan Han, Jinan Xu</li>
<li>for: 提高自然语言处理 tasks 中的 paraphrase 生成质量，尤其是在没有人工标注或高质量模板的情况下。</li>
<li>methods: 提出了一种新的质量基于的语法模板检索器 (QSTR)，通过评估生成的 paraphrase 质量来选择最佳的语法模板。此外，为了提高多个 paraphrase 的多样性，我们还提出了一种多样性检索算法 (DTS)。</li>
<li>results: QSTR 可以大幅超越现有的检索方法，在生成高质量 paraphrase 方面取得显著成果，甚至与人工标注的模板相当在无参照度量上表现出色。此外，人工评估和下游任务中使用我们生成的 paraphrase 也表现出了优秀的潜力。<details>
<summary>Abstract</summary>
Existing syntactically-controlled paraphrase generation (SPG) models perform promisingly with human-annotated or well-chosen syntactic templates. However, the difficulty of obtaining such templates actually hinders the practical application of SPG models. For one thing, the prohibitive cost makes it unfeasible to manually design decent templates for every source sentence. For another, the templates automatically retrieved by current heuristic methods are usually unreliable for SPG models to generate qualified paraphrases. To escape this dilemma, we propose a novel Quality-based Syntactic Template Retriever (QSTR) to retrieve templates based on the quality of the to-be-generated paraphrases. Furthermore, for situations requiring multiple paraphrases for each source sentence, we design a Diverse Templates Search (DTS) algorithm, which can enhance the diversity between paraphrases without sacrificing quality. Experiments demonstrate that QSTR can significantly surpass existing retrieval methods in generating high-quality paraphrases and even perform comparably with human-annotated templates in terms of reference-free metrics. Additionally, human evaluation and the performance on downstream tasks using our generated paraphrases for data augmentation showcase the potential of our QSTR and DTS algorithm in practical scenarios.
</details>
<details>
<summary>摘要</summary>
现有的语法控制的篇章生成（SPG）模型在人工标注或选择的语法模板上表现良好。然而，获得这些模板的困难实际上限制了SPG模型的实际应用。一方面，人工设计Decent模板的成本太高，无法实际应用。另一方面，由现有的索引方法自动获取的模板通常不可靠，导致SPG模型生成质量不高的篇章。为了解决这个困境，我们提出了一种新的质量基于的语法模板检索器（QSTR），可以根据生成的篇章质量来选择语法模板。此外，为了处理每个源句需要多个篇章的情况，我们设计了多样性检索（DTS）算法，可以提高篇章之间的多样性而不是质量的牺牲。实验表明，QSTR可以明显超过现有的检索方法，生成高质量的篇章，甚至与人工标注的模板相当在无参照度量上表现出色。此外，人工评估和用我们生成的篇章进行数据增强任务的表现也表明了我们的QSTR和DTS算法在实际场景中的潜力。
</details></li>
</ul>
<hr>
<h2 id="Anomaly-Detection-of-Command-Shell-Sessions-based-on-DistilBERT-Unsupervised-and-Supervised-Approaches"><a href="#Anomaly-Detection-of-Command-Shell-Sessions-based-on-DistilBERT-Unsupervised-and-Supervised-Approaches" class="headerlink" title="Anomaly Detection of Command Shell Sessions based on DistilBERT: Unsupervised and Supervised Approaches"></a>Anomaly Detection of Command Shell Sessions based on DistilBERT: Unsupervised and Supervised Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13247">http://arxiv.org/abs/2310.13247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zefang Liu, John Buford</li>
<li>for: 检测 Unix shell 会话异常行为是计算机安全中的一项关键任务。</li>
<li>methods: 我们使用预训练的 DistilBERT 模型，结合无监督和监督学习技术，以识别 Unix shell 会话中异常活动，同时尽量避免数据标注。</li>
<li>results: 在一个大规模企业数据集上进行实验，我们的方法能够准确地检测 Unix shell 会话中的异常行为。<details>
<summary>Abstract</summary>
Anomaly detection in command shell sessions is a critical aspect of computer security. Recent advances in deep learning and natural language processing, particularly transformer-based models, have shown great promise for addressing complex security challenges. In this paper, we implement a comprehensive approach to detect anomalies in Unix shell sessions using a pretrained DistilBERT model, leveraging both unsupervised and supervised learning techniques to identify anomalous activity while minimizing data labeling. The unsupervised method captures the underlying structure and syntax of Unix shell commands, enabling the detection of session deviations from normal behavior. Experiments on a large-scale enterprise dataset collected from production systems demonstrate the effectiveness of our approach in detecting anomalous behavior in Unix shell sessions. This work highlights the potential of leveraging recent advances in transformers to address important computer security challenges.
</details>
<details>
<summary>摘要</summary>
“命令行Session anomaly detection是计算机安全的关键方面。近年来，深度学习和自然语言处理技术，特别是基于变换器的模型，在解决复杂安全挑战方面表现出了惊人的承诺。本文，我们实现了一种涵盖全面的命令行Session anomaly detection方法，使用预训练的DistilBERT模型，结合无监督和监督学习技术，以确定异常行为，同时尽量避免数据标注。无监督方法捕捉了 Unix shell命令的内部结构和语法，使得检测会话异常行为变得可能。在一个大规模的企业数据集上进行了实验， demonstarted our approach的效果性在 Unix shell sessions中检测异常行为。这种工作表明了利用最新的变换器技术来解决计算机安全挑战的潜力。”Note that Simplified Chinese is used in mainland China, and Traditional Chinese is used in Taiwan and other regions.
</details></li>
</ul>
<hr>
<h2 id="Open-source-Large-Language-Models-are-Strong-Zero-shot-Query-Likelihood-Models-for-Document-Ranking"><a href="#Open-source-Large-Language-Models-are-Strong-Zero-shot-Query-Likelihood-Models-for-Document-Ranking" class="headerlink" title="Open-source Large Language Models are Strong Zero-shot Query Likelihood Models for Document Ranking"></a>Open-source Large Language Models are Strong Zero-shot Query Likelihood Models for Document Ranking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13243">http://arxiv.org/abs/2310.13243</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ielab/llm-qlm">https://github.com/ielab/llm-qlm</a></li>
<li>paper_authors: Shengyao Zhuang, Bing Liu, Bevan Koopman, Guido Zuccon</li>
<li>for: This paper focuses on investigating the effectiveness of recent large language models (LLMs) as query likelihood models (QLMs) for zero-shot ranking of documents.</li>
<li>methods: The authors use pre-trained LLMs without fine-tuning and introduce a novel hybrid zero-shot retriever that integrates LLM-based QLMs with a traditional retriever.</li>
<li>results: The authors find that the LLM-based QLMs demonstrate robust zero-shot ranking ability, and the hybrid retriever achieves exceptional effectiveness in both zero-shot and few-shot scenarios.Here’s the information in Simplified Chinese text:</li>
<li>for: 这篇论文 investigate 最新的大型自然语言模型 (LLMs) 作为问题可能性模型 (QLMs) 的零Instance 排序文档的效果。</li>
<li>methods: 作者使用预训练的 LLMs 而不是精度调教，并提出了一种新的混合零实例检索器，该检索器将 LLM-based QLMs 与传统检索器集成。</li>
<li>results: 作者发现 LLM-based QLMs 在零实例情况下示出了强大的排序能力，并发现混合检索器在零实例和几个实例情况下都达到了非常出色的效果。<details>
<summary>Abstract</summary>
In the field of information retrieval, Query Likelihood Models (QLMs) rank documents based on the probability of generating the query given the content of a document. Recently, advanced large language models (LLMs) have emerged as effective QLMs, showcasing promising ranking capabilities. This paper focuses on investigating the genuine zero-shot ranking effectiveness of recent LLMs, which are solely pre-trained on unstructured text data without supervised instruction fine-tuning. Our findings reveal the robust zero-shot ranking ability of such LLMs, highlighting that additional instruction fine-tuning may hinder effectiveness unless a question generation task is present in the fine-tuning dataset. Furthermore, we introduce a novel state-of-the-art ranking system that integrates LLM-based QLMs with a hybrid zero-shot retriever, demonstrating exceptional effectiveness in both zero-shot and few-shot scenarios. We make our codebase publicly available at https://github.com/ielab/llm-qlm.
</details>
<details>
<summary>摘要</summary>
在信息检索领域，查询可能性模型（QLM）根据文档内容中Generate查询的概率来排序文档。最近，高级大语言模型（LLM）作为效果的QLM出现，展示了可观的排序能力。本文将关注 investigate最近LLM的真正零上下文排序能力，这些QLM都是在无监督指导下预训练的自然语言数据。我们的发现表明这些LLM具有强大的零上下文排序能力，表明添加细化 instrucion 训练可能会降低效果，除非包含问题生成任务在训练集中。此外，我们介绍了一种新的状态略取得 ranked 系统，将LLM-基于的QLM与混合零上下文检索器结合， demonstrate 出色的效果在零上下文和几个shot scenario中。我们在https://github.com/ielab/llm-qlm中公开了我们的代码库。
</details></li>
</ul>
<hr>
<h2 id="The-Less-the-Merrier-Investigating-Language-Representation-in-Multilingual-Models"><a href="#The-Less-the-Merrier-Investigating-Language-Representation-in-Multilingual-Models" class="headerlink" title="The Less the Merrier? Investigating Language Representation in Multilingual Models"></a>The Less the Merrier? Investigating Language Representation in Multilingual Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13228">http://arxiv.org/abs/2310.13228</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hellina Hailu Nigatu, Atnafu Lambebo Tonja, Jugal Kalita</li>
<li>for: 本文探讨多语言模型在不同自然语言处理任务中的表现，特别是在低资源 SETTINGS 中支持的语言是否受到保障。</li>
<li>methods: 我们使用 популяр的多语言模型进行 investigate，分析这些模型对不同语言的表征和学习结果，包括语言家族和方言的影响。</li>
<li>results: 我们的实验结果显示，基于社区的模型（models that focus on languages of a given family or geographical location and are built by communities who speak them）在低资源语言之间的语言分类 task 中表现更好。我们的研究贡献到了多语言模型的理解和改进方向。<details>
<summary>Abstract</summary>
Multilingual Language Models offer a way to incorporate multiple languages in one model and utilize cross-language transfer learning to improve performance for different Natural Language Processing (NLP) tasks. Despite progress in multilingual models, not all languages are supported as well, particularly in low-resource settings. In this work, we investigate the linguistic representation of different languages in multilingual models. We start by asking the question which languages are supported in popular multilingual models and which languages are left behind. Then, for included languages, we look at models' learned representations based on language family and dialect and try to understand how models' learned representations for~(1) seen and~(2) unseen languages vary across different language groups. In addition, we test and analyze performance on downstream tasks such as text generation and Named Entity Recognition. We observe from our experiments that community-centered models -- models that focus on languages of a given family or geographical location and are built by communities who speak them -- perform better at distinguishing between languages in the same family for low-resource languages. Our paper contributes to the literature in understanding multilingual models and their shortcomings and offers insights on potential ways to improve them.
</details>
<details>
<summary>摘要</summary>
多语言语言模型提供了将多种语言 integrate into one model，并利用交叉语言学习来提高不同自然语言处理（NLP）任务的性能。尽管在多语言模型方面有所进步，但不是所有语言都得到了充分支持，特别是在低资源环境下。在这项工作中，我们调查了不同语言在多语言模型中的语言表示。我们开始问题是哪些语言在流行的多语言模型中被支持，哪些语言被排除在外。然后，对包括的语言来说，我们查看模型学习的语言家族和方言基于的表示，并尝试理解模型对seen和unseen语言的学习表示如何不同。此外，我们测试和分析下沟通任务 such as 文本生成和命名实体识别的性能。我们发现在我们的实验中，社区中心的模型（models that focus on languages of a given family or geographical location and are built by communities who speak them）在同家族语言之间的分辨率较高。我们的论文贡献了对多语言模型和其缺陷的研究，并提供了可能改进它们的想法。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Zero-Shot-Crypto-Sentiment-with-Fine-tuned-Language-Model-and-Prompt-Engineering"><a href="#Enhancing-Zero-Shot-Crypto-Sentiment-with-Fine-tuned-Language-Model-and-Prompt-Engineering" class="headerlink" title="Enhancing Zero-Shot Crypto Sentiment with Fine-tuned Language Model and Prompt Engineering"></a>Enhancing Zero-Shot Crypto Sentiment with Fine-tuned Language Model and Prompt Engineering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13226">http://arxiv.org/abs/2310.13226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rahman S M Wahidur, Ishmam Tashdeed, Manjit Kaur, Heung-No-Lee</li>
<li>for: 本研究旨在提高投资者对加密货币市场的情感分析精度，并 investigate fine-tuning技术的效果。</li>
<li>methods: 本研究使用了大型自然语言模型的精度调整技术，包括监督式调整和指令式调整。</li>
<li>results: 实验结果表明，精度调整后可以获得40%的零基eline性能提升，而大型模型在指令调整下表现最高，其中最高的准确率为75.16%。<details>
<summary>Abstract</summary>
Blockchain technology has revolutionized the financial landscape, with cryptocurrencies gaining widespread adoption for their decentralized and transparent nature. As the sentiment expressed on social media platforms can significantly influence cryptocurrency discussions and market movements, sentiment analysis has emerged as a crucial tool for understanding public opinion and predicting market trends. Motivated by the aim to enhance sentiment analysis accuracy in the cryptocurrency domain, this paper investigates fine-tuning techniques on large language models. This paper also investigates the efficacy of supervised fine-tuning and instruction-based fine-tuning on large language models for unseen tasks. Experimental results demonstrate a significant average zero-shot performance gain of 40% after fine-tuning, highlighting the potential of this technique in optimizing pre-trained language model efficiency. Additionally, the impact of instruction tuning on models of varying scales is examined, revealing that larger models benefit from instruction tuning, achieving the highest average accuracy score of 75.16%. In contrast, smaller-scale models may experience reduced generalization due to the complete utilization of model capacity. To gain deeper insight about how instruction works with these language models, this paper presents an experimental investigation into the response of an instruction-based model under different instruction tuning setups. The investigation demonstrates that the model achieves an average accuracy score of 72.38% for short and simple instructions. This performance significantly outperforms its accuracy under long and complex instructions by over 12%, thereby effectively highlighting the profound significance of instruction characteristics in maximizing model performance.
</details>
<details>
<summary>摘要</summary>
blockchain 技术已经革命化了金融领域， криптовалюencies 在 Decentralized 和 Transparent 的特点下得到了广泛的采纳。 在社交媒体平台上表达的情感可以对 криптовалюencies 的讨论和市场走势产生重要影响，因此情感分析在 криптовалюencies 领域已成为一种关键的工具。 为了提高情感分析的准确性，本文调查了大语言模型的精细调整技术。 本文还 investigate 大语言模型的supervised 调整和指令调整在未看到任务上的效果。 实验结果表明，精细调整后平均零件性能提高40%，这 highlights 该技术在优化预训练语言模型效率的潜力。 此外，本文还 investigate 模型不同规模下的指令调整效果，发现大型模型受到指令调整的影响，其平均准确率为75.16%。 相比之下，较小的模型可能会因完全使用模型容量而导致退化。 为了更深入地了解指令如何与这些语言模型交互，本文进行了一种实验调查。 调查结果表明，指令基本的模型在不同的指令调整设置下达到了72.38%的平均准确率。 这个性能在长度和复杂度更高的指令下明显下降，这有效地表明了指令特点在提高模型性能的重要性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/20/cs.CL_2023_10_20/" data-id="clpxp03xi00e0fm88fxra61hh" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_10_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/20/cs.LG_2023_10_20/" class="article-date">
  <time datetime="2023-10-20T10:00:00.000Z" itemprop="datePublished">2023-10-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/20/cs.LG_2023_10_20/">cs.LG - 2023-10-20</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Competitive-Advantage-Attacks-to-Decentralized-Federated-Learning"><a href="#Competitive-Advantage-Attacks-to-Decentralized-Federated-Learning" class="headerlink" title="Competitive Advantage Attacks to Decentralized Federated Learning"></a>Competitive Advantage Attacks to Decentralized Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13862">http://arxiv.org/abs/2310.13862</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuqi Jia, Minghong Fang, Neil Zhenqiang Gong</li>
<li>for: This paper is written for researchers and practitioners in the field of federated learning, particularly those interested in understanding and mitigating attacks on decentralized federated learning (DFL) systems.</li>
<li>methods: The paper proposes a new family of attacks called SelfishAttack, which aims to achieve competitive advantages over non-selfish clients in DFL systems. The authors formulate finding such local models as an optimization problem and propose methods to solve it when DFL uses different aggregation rules.</li>
<li>results: The authors show that their proposed methods successfully increase the accuracy gap between the final learnt local models of selfish clients and those of non-selfish ones. Moreover, SelfishAttack achieves larger accuracy gaps than poisoning attacks when extended to increase competitive advantages.Here is the Chinese translation of the three key information points:</li>
<li>for: 这篇论文是为了向 federated learning 领域的研究者和实践者提供一种新的攻击方法，具体是在分布式 federated learning 系统中实现自利益的攻击。</li>
<li>methods: 论文提出了一种新的攻击方法，即 SelfishAttack，用于在分布式 federated learning 系统中获得竞争优势。作者们将找到这些地方的方法形式为优化问题，并提出了解决这些问题的方法，当 DFL 使用不同的聚合规则时。</li>
<li>results: 作者们证明了他们提出的方法能够成功地增加自利益客户端的最终学习的本地模型准确率与非自利益客户端的准确率之间的差距。此外，SelfishAttack 还能够在扩展到增加竞争优势时超过毒害攻击。<details>
<summary>Abstract</summary>
Decentralized federated learning (DFL) enables clients (e.g., hospitals and banks) to jointly train machine learning models without a central orchestration server. In each global training round, each client trains a local model on its own training data and then they exchange local models for aggregation. In this work, we propose SelfishAttack, a new family of attacks to DFL. In SelfishAttack, a set of selfish clients aim to achieve competitive advantages over the remaining non-selfish ones, i.e., the final learnt local models of the selfish clients are more accurate than those of the non-selfish ones. Towards this goal, the selfish clients send carefully crafted local models to each remaining non-selfish one in each global training round. We formulate finding such local models as an optimization problem and propose methods to solve it when DFL uses different aggregation rules. Theoretically, we show that our methods find the optimal solutions to the optimization problem. Empirically, we show that SelfishAttack successfully increases the accuracy gap (i.e., competitive advantage) between the final learnt local models of selfish clients and those of non-selfish ones. Moreover, SelfishAttack achieves larger accuracy gaps than poisoning attacks when extended to increase competitive advantages.
</details>
<details>
<summary>摘要</summary>
分布式联合学习（DFL）允许客户端（例如医院和银行）共同训练机器学习模型无需中央执行服务器。在每个全球训练轮次中，每个客户端在自己的训练数据上进行本地训练，然后将本地模型交换给其他客户端进行汇总。在这项工作中，我们提出了自利攻击（SelfishAttack），一种新的攻击DFL的方法。在SelfishAttack中，一组自利客户端尝试通过在每个全球训练轮次中发送特制的本地模型来获得与其他非自利客户端的竞争优势。具体来说，自利客户端的最终训练的本地模型比非自利客户端的模型更高精度。为实现这一目标，我们将找到一个优化问题的解决方案，并提出了在不同汇总规则下解决这个问题的方法。理论上，我们证明了我们的方法可以找到优化问题的优解。实际上，我们证明了SelfishAttack成功地增加了自利客户端和非自利客户端之间的精度差（竞争优势）。此外，SelfishAttack在扩展到增加竞争优势时比质量攻击更有效。
</details></li>
</ul>
<hr>
<h2 id="A-Sparse-Bayesian-Learning-for-Diagnosis-of-Nonstationary-and-Spatially-Correlated-Faults-with-Application-to-Multistation-Assembly-Systems"><a href="#A-Sparse-Bayesian-Learning-for-Diagnosis-of-Nonstationary-and-Spatially-Correlated-Faults-with-Application-to-Multistation-Assembly-Systems" class="headerlink" title="A Sparse Bayesian Learning for Diagnosis of Nonstationary and Spatially Correlated Faults with Application to Multistation Assembly Systems"></a>A Sparse Bayesian Learning for Diagnosis of Nonstationary and Spatially Correlated Faults with Application to Multistation Assembly Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16058">http://arxiv.org/abs/2310.16058</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jihoon Chung, Zhenyu Kong</li>
<li>for: 本研究旨在提出一种新的维度诊断方法，以Addressing the challenges of limited sensor numbers, non-stationary process faults, and correlation information in manufacturing systems.</li>
<li>methods: 本方法基于实际假设，即进程 faults sparse，并使用层次结构和多个参数化先验分布来解决上述挑战。 variants Bayes inference 方法 derive approximate posterior distributions of process faults.</li>
<li>results: numerical and real-world case studies demonstrate the efficacy of the proposed method in an actual autobody assembly system, and its generalizability to other domains, including communication and healthcare systems.<details>
<summary>Abstract</summary>
Sensor technology developments provide a basis for effective fault diagnosis in manufacturing systems. However, the limited number of sensors due to physical constraints or undue costs hinders the accurate diagnosis in the actual process. In addition, time-varying operational conditions that generate nonstationary process faults and the correlation information in the process require to consider for accurate fault diagnosis in the manufacturing systems. This article proposes a novel fault diagnosis method: clustering spatially correlated sparse Bayesian learning (CSSBL), and explicitly demonstrates its applicability in a multistation assembly system that is vulnerable to the above challenges. Specifically, the method is based on a practical assumption that it will likely have a few process faults (sparse). In addition, the hierarchical structure of CSSBL has several parameterized prior distributions to address the above challenges. As posterior distributions of process faults do not have closed form, this paper derives approximate posterior distributions through Variational Bayes inference. The proposed method's efficacy is provided through numerical and real-world case studies utilizing an actual autobody assembly system. The generalizability of the proposed method allows the technique to be applied in fault diagnosis in other domains, including communication and healthcare systems.
</details>
<details>
<summary>摘要</summary>
感测技术发展为制造系统效果的缺陷诊断提供基础。然而，由于物理限制或过分成本，制造系统中的感测器数量有限，这限制了实际进程中准确诊断的能力。此外，时变操作条件会生成非站ARY进程缺陷，而且需要考虑进程中的相关信息。本文提出了一种新的缺陷诊断方法：归一化空间相关稀疏 bayesian 学习（CSSBL），并详细描述其在多站制造系统中的可应用性。具体来说，该方法基于实际情况下的稀疏缺陷假设，并有多个层次结构来 Address 上述挑战。由于 posterior distribution 的不准确性，本文使用变分析法来 derive approximate posterior distribution。实验和实际案例研究表明，提议的方法可以在制造系统中实现高度的缺陷诊断精度。此外，该方法的通用性使其可以应用于其他领域，包括通信和医疗系统的缺陷诊断。
</details></li>
</ul>
<hr>
<h2 id="Towards-Subject-Agnostic-Affective-Emotion-Recognition"><a href="#Towards-Subject-Agnostic-Affective-Emotion-Recognition" class="headerlink" title="Towards Subject Agnostic Affective Emotion Recognition"></a>Towards Subject Agnostic Affective Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15189">http://arxiv.org/abs/2310.15189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amit Kumar Jaiswal, Haiming Liu, Prayag Tiwari</li>
<li>for: 本研究旨在实现不含受试者特定信息的情感识别，使用EEG信号进行脑机器交互（aBCI）。</li>
<li>methods: 本研究使用领域总结和领域适应来解决EEG信号中的分布偏移问题。</li>
<li>results: 我们的提议方法可以在实验中与当前领域适应方法相比，在没有额外计算资源的情况下实现类似的性能。<details>
<summary>Abstract</summary>
This paper focuses on affective emotion recognition, aiming to perform in the subject-agnostic paradigm based on EEG signals. However, EEG signals manifest subject instability in subject-agnostic affective Brain-computer interfaces (aBCIs), which led to the problem of distributional shift. Furthermore, this problem is alleviated by approaches such as domain generalisation and domain adaptation. Typically, methods based on domain adaptation confer comparatively better results than the domain generalisation methods but demand more computational resources given new subjects. We propose a novel framework, meta-learning based augmented domain adaptation for subject-agnostic aBCIs. Our domain adaptation approach is augmented through meta-learning, which consists of a recurrent neural network, a classifier, and a distributional shift controller based on a sum-decomposable function. Also, we present that a neural network explicating a sum-decomposable function can effectively estimate the divergence between varied domains. The network setting for augmented domain adaptation follows meta-learning and adversarial learning, where the controller promptly adapts to new domains employing the target data via a few self-adaptation steps in the test phase. Our proposed approach is shown to be effective in experiments on a public aBICs dataset and achieves similar performance to state-of-the-art domain adaptation methods while avoiding the use of additional computational resources.
</details>
<details>
<summary>摘要</summary>
The proposed approach uses a recurrent neural network (RNN), a classifier, and a distributional shift controller based on a sum-decomposable function to adapt to new domains. The controller is trained to quickly adapt to new domains using a few self-adaptation steps in the test phase. The authors also show that a neural network explicating a sum-decomposable function can effectively estimate the divergence between varied domains.The proposed approach is evaluated on a public aBCIs dataset and achieves similar performance to state-of-the-art domain adaptation methods while avoiding the use of additional computational resources. The main contributions of the paper are:1. A novel framework for subject-agnostic aBCIs based on meta-learning and adversarial learning.2. A sum-decomposable function-based distributional shift controller for adapting to new domains.3. A self-adaptation mechanism for quickly adapting to new domains using a few self-adaptation steps in the test phase.4. Experimental results on a public aBCIs dataset that demonstrate the effectiveness of the proposed approach.
</details></li>
</ul>
<hr>
<h2 id="Exponential-weight-averaging-as-damped-harmonic-motion"><a href="#Exponential-weight-averaging-as-damped-harmonic-motion" class="headerlink" title="Exponential weight averaging as damped harmonic motion"></a>Exponential weight averaging as damped harmonic motion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13854">http://arxiv.org/abs/2310.13854</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan Patsenker, Henry Li, Yuval Kluger</li>
<li>for: 提供稳定估计深度学习优化过程中的随机量统计</li>
<li>methods: 使用抽象移动平均（EMA）计算模型参数，并在训练过程中和训练完成后进行weight平均，以提高推理模型的稳定性</li>
<li>results: 提出了一种基于物理拟合的改进训练算法（BELAY），并证明了BELAY在训练过程中和训练完成后具有许多优势，包括更高的稳定性和更好的性能。<details>
<summary>Abstract</summary>
The exponential moving average (EMA) is a commonly used statistic for providing stable estimates of stochastic quantities in deep learning optimization. Recently, EMA has seen considerable use in generative models, where it is computed with respect to the model weights, and significantly improves the stability of the inference model during and after training. While the practice of weight averaging at the end of training is well-studied and known to improve estimates of local optima, the benefits of EMA over the course of training is less understood. In this paper, we derive an explicit connection between EMA and a damped harmonic system between two particles, where one particle (the EMA weights) is drawn to the other (the model weights) via an idealized zero-length spring. We then leverage this physical analogy to analyze the effectiveness of EMA, and propose an improved training algorithm, which we call BELAY. Finally, we demonstrate theoretically and empirically several advantages enjoyed by BELAY over standard EMA.
</details>
<details>
<summary>摘要</summary>
exponential moving average (EMA) 是一种常用的统计方法，用于提供深度学习优化中稳定的随机量 estimator。 最近，EMA 在生成模型中得到了广泛的应用，其中 compute 与模型参数相关。这些参数的 EMA 计算 Significantly improves the stability of the inference model during and after training. 虽然在训练结束时weight averaging的实践已经很受欢迎，但EMA在训练过程中的效果不够了解。在这篇论文中，我们 derivate 了EMA和两个 particles之间的封闭律动系统的直接关系，其中一个particle（EMA 参数）被吸引到另一个（模型参数）via 一个理想化的零长度spring。我们然后利用这种物理类比来分析 EMA 的效果，并提出了一种改进的训练算法，我们称之为 BELAY。最后，我们 theoretically 和 empirically 证明了 BELAY 对标准 EMA 的优势。
</details></li>
</ul>
<hr>
<h2 id="Gradual-Domain-Adaptation-Theory-and-Algorithms"><a href="#Gradual-Domain-Adaptation-Theory-and-Algorithms" class="headerlink" title="Gradual Domain Adaptation: Theory and Algorithms"></a>Gradual Domain Adaptation: Theory and Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13852">http://arxiv.org/abs/2310.13852</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yifei-he/goat">https://github.com/yifei-he/goat</a></li>
<li>paper_authors: Yifei He, Haoxiang Wang, Bo Li, Han Zhao</li>
<li>for: 这篇论文主要关注于 Gradual Domain Adaptation (GDA) 中的一个问题，即如何在具有大量数据的源领域中对于目标领域进行逐步适束。</li>
<li>methods: 这篇论文使用 Gradual Self-training (GST) 来实现 GDA，并提供了一个改进了 Kumar et al. (2020) 的一般化范围边界。</li>
<li>results: 实验结果显示，在实际应用中，这个 GOAT 框架可以对于欠缺中间领域的情况进行改进，并且可以提高标准 GDA 的性能。<details>
<summary>Abstract</summary>
Unsupervised domain adaptation (UDA) adapts a model from a labeled source domain to an unlabeled target domain in a one-off way. Though widely applied, UDA faces a great challenge whenever the distribution shift between the source and the target is large. Gradual domain adaptation (GDA) mitigates this limitation by using intermediate domains to gradually adapt from the source to the target domain. In this work, we first theoretically analyze gradual self-training, a popular GDA algorithm, and provide a significantly improved generalization bound compared with Kumar et al. (2020). Our theoretical analysis leads to an interesting insight: to minimize the generalization error on the target domain, the sequence of intermediate domains should be placed uniformly along the Wasserstein geodesic between the source and target domains. The insight is particularly useful under the situation where intermediate domains are missing or scarce, which is often the case in real-world applications. Based on the insight, we propose $\textbf{G}$enerative Gradual D$\textbf{O}$main $\textbf{A}$daptation with Optimal $\textbf{T}$ransport (GOAT), an algorithmic framework that can generate intermediate domains in a data-dependent way. More concretely, we first generate intermediate domains along the Wasserstein geodesic between two given consecutive domains in a feature space, then apply gradual self-training to adapt the source-trained classifier to the target along the sequence of intermediate domains. Empirically, we demonstrate that our GOAT framework can improve the performance of standard GDA when the given intermediate domains are scarce, significantly broadening the real-world application scenarios of GDA. Our code is available at https://github.com/yifei-he/GOAT.
</details>
<details>
<summary>摘要</summary>
Unsupervised domain adaptation (UDA) 将源频道上标注的模型适应到目标频道上，但是当源和目标频道的分布差异较大时，UDA会遇到很大的挑战。 Gradual domain adaptation (GDA) 可以解决这个问题，通过使用中间频道来慢慢地适应源频道到目标频道。在这项工作中，我们首先从理论角度分析了渐进自动适应算法，并提供了与Kumar et al. (2020)相比较好的泛化约束。我们的理论分析导致了一个有趣的发现：在适应目标频道时，中间频道的序列应该被置于 Wasserstein 地odesic 上。这个发现对于实际应用中缺少或罕见中间频道的情况非常有用。基于这个发现，我们提出了 $\textbf{G}$enerative Gradual D$\textbf{O}$main $\textbf{A}$daptation with Optimal $\textbf{T}$ransport（GOAT）算法框架。具体来说，我们首先在Feature空间中生成中间频道序列，然后应用渐进自动适应来适应源频道上训练的分类器到目标频道。Empirical experiments show that our GOAT framework can improve the performance of standard GDA when the given intermediate domains are scarce, significantly expanding the real-world application scenarios of GDA.我们的代码可以在https://github.com/yifei-he/GOAT上获取。
</details></li>
</ul>
<hr>
<h2 id="Augment-with-Care-Enhancing-Graph-Contrastive-Learning-with-Selective-Spectrum-Perturbation"><a href="#Augment-with-Care-Enhancing-Graph-Contrastive-Learning-with-Selective-Spectrum-Perturbation" class="headerlink" title="Augment with Care: Enhancing Graph Contrastive Learning with Selective Spectrum Perturbation"></a>Augment with Care: Enhancing Graph Contrastive Learning with Selective Spectrum Perturbation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13845">http://arxiv.org/abs/2310.13845</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaiqi Yang, Haoyu Han, Wei Jin, Hui Liu</li>
<li>for: 本文旨在提出一种基于特征频谱的图像增强视图，以提高图像对比学抽象的效果。</li>
<li>methods: 本文使用了spectral hint guided edge perturbation方法，通过选择性地在特定频谱上pose tailored perturbation，以实现adaptive和可控的增强视图。</li>
<li>results: 经过广泛的实验和理论分析，GASSER方法能够提供adaptive和可控的增强视图，同时符合图像结构的自同比率和频谱特征。<details>
<summary>Abstract</summary>
In recent years, Graph Contrastive Learning (GCL) has shown remarkable effectiveness in learning representations on graphs. As a component of GCL, good augmentation views are supposed to be invariant to the important information while discarding the unimportant part. Existing augmentation views with perturbed graph structures are usually based on random topology corruption in the spatial domain; however, from perspectives of the spectral domain, this approach may be ineffective as it fails to pose tailored impacts on the information of different frequencies, thus weakening the agreement between the augmentation views. By a preliminary experiment, we show that the impacts caused by spatial random perturbation are approximately evenly distributed among frequency bands, which may harm the invariance of augmentations required by contrastive learning frameworks. To address this issue, we argue that the perturbation should be selectively posed on the information concerning different frequencies. In this paper, we propose GASSER which poses tailored perturbation on the specific frequencies of graph structures in spectral domain, and the edge perturbation is selectively guided by the spectral hints. As shown by extensive experiments and theoretical analysis, the augmentation views are adaptive and controllable, as well as heuristically fitting the homophily ratios and spectrum of graph structures.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Fast-hyperboloid-decision-tree-algorithms"><a href="#Fast-hyperboloid-decision-tree-algorithms" class="headerlink" title="Fast hyperboloid decision tree algorithms"></a>Fast hyperboloid decision tree algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13841">http://arxiv.org/abs/2310.13841</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philippe Chlenski, Ethan Turok, Antonio Moretti, Itsik Pe’er</li>
<li>for: 这篇论文的目的是提出一种基于гипербо利 geometry的决策树算法，以解决在机器学习中遇到的计算复杂性问题。</li>
<li>methods: 该论文使用了一种基于内积的方法，通过利用内积来适应гиPERBOLIC空间，从而消除了计算复杂性的问题。</li>
<li>results: 对多个数据集进行了广泛的比较，表明该方法可以提供快速、准确、精度高的 гиPERBOLIC数据分析工具。<details>
<summary>Abstract</summary>
Hyperbolic geometry is gaining traction in machine learning for its effectiveness at capturing hierarchical structures in real-world data. Hyperbolic spaces, where neighborhoods grow exponentially, offer substantial advantages and consistently deliver state-of-the-art results across diverse applications. However, hyperbolic classifiers often grapple with computational challenges. Methods reliant on Riemannian optimization frequently exhibit sluggishness, stemming from the increased computational demands of operations on Riemannian manifolds. In response to these challenges, we present hyperDT, a novel extension of decision tree algorithms into hyperbolic space. Crucially, hyperDT eliminates the need for computationally intensive Riemannian optimization, numerically unstable exponential and logarithmic maps, or pairwise comparisons between points by leveraging inner products to adapt Euclidean decision tree algorithms to hyperbolic space. Our approach is conceptually straightforward and maintains constant-time decision complexity while mitigating the scalability issues inherent in high-dimensional Euclidean spaces. Building upon hyperDT we introduce hyperRF, a hyperbolic random forest model. Extensive benchmarking across diverse datasets underscores the superior performance of these models, providing a swift, precise, accurate, and user-friendly toolkit for hyperbolic data analysis.
</details>
<details>
<summary>摘要</summary>
超凡几何在机器学习中得到推广，因为它能够很好地捕捉实际数据中的层次结构。超凡空间，其 neighberhood  exponentially 增长，提供了很多优势，并一直在多种应用中提供了状态机器学习的最佳结果。然而，超凡分类器经常面临计算挑战。基于里曼尼托п optimization 的方法经常会显示缓慢，这是因为在里曼尼托п上进行操作的计算增加了负担。为了解决这些挑战，我们提出了 hyperDT，一种基于几何空间的决策树算法的扩展。 hyperDT 减少了 computationally 成本的 Riemannian 优化、不稳定的对数和对数映射以及点之间的对比。我们的方法是概念简单，保持了常量时间复杂度，同时解决高维度欧氏空间中的缺乏扩展性问题。基于 hyperDT 我们引入了 hyperRF，一种基于超凡空间的随机森林模型。对多种数据进行广泛的 benchmarking 表明，这些模型具有快速、精准、准确和易用的特点，提供了一个可靠的超凡数据分析工具。
</details></li>
</ul>
<hr>
<h2 id="Universal-Representation-of-Permutation-Invariant-Functions-on-Vectors-and-Tensors"><a href="#Universal-Representation-of-Permutation-Invariant-Functions-on-Vectors-and-Tensors" class="headerlink" title="Universal Representation of Permutation-Invariant Functions on Vectors and Tensors"></a>Universal Representation of Permutation-Invariant Functions on Vectors and Tensors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13829">http://arxiv.org/abs/2310.13829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Puoya Tabaghi, Yusu Wang</li>
<li>for: 该研究的主要目标是研究多aset函数，即不同大小的输入上的具有 permutation-invariant 性的函数。</li>
<li>methods: 该研究使用 Deep Sets 模型，其提供了一种 scalars 上的 continuous multiset functions 的 universality 表示，以及一种需要 latent space 维度为 O(N^D) 的 universality 近似。</li>
<li>results: 该研究证明了 continuous 和 discontinuous multiset functions 的 universality 表示只需要 latent space 维度为 O(N^D)，并且证明了 encoder 和 decoder 函数都是连续的。此外，该研究还扩展到 permutation-invariant tensor functions 的 universality 表示，并提供了一些特殊的 sum-decomposition 结构以实现这一目标。<details>
<summary>Abstract</summary>
A main object of our study is multiset functions -- that is, permutation-invariant functions over inputs of varying sizes. Deep Sets, proposed by \cite{zaheer2017deep}, provides a \emph{universal representation} for continuous multiset functions on scalars via a sum-decomposable model. Restricting the domain of the functions to finite multisets of $D$-dimensional vectors, Deep Sets also provides a \emph{universal approximation} that requires a latent space dimension of $O(N^D)$ -- where $N$ is an upper bound on the size of input multisets. In this paper, we strengthen this result by proving that universal representation is guaranteed for continuous and discontinuous multiset functions though a latent space dimension of $O(N^D)$. We then introduce \emph{identifiable} multisets for which we can uniquely label their elements using an identifier function, namely, finite-precision vectors are identifiable. Using our analysis on identifiable multisets, we prove that a sum-decomposable model for general continuous multiset functions only requires a latent dimension of $2DN$. We further show that both encoder and decoder functions of the model are continuous -- our main contribution to the existing work which lack such a guarantee. Also this provides a significant improvement over the aforementioned $O(N^D)$ bound which was derived for universal representation of continuous and discontinuous multiset functions. We then extend our results and provide special sum-decomposition structures to universally represent permutation-invariant tensor functions on identifiable tensors. These families of sum-decomposition models enables us to design deep network architectures and deploy them on a variety of learning tasks on sequences, images, and graphs.
</details>
<details>
<summary>摘要</summary>
我们的研究主要目标是多集函数（permutation-invariant functions），即输入尺寸不同时的函数。深度集（Deep Sets），由\cite{zaheer2017deep}所提出，提供了一个基于条件对称的普遍表示（universal representation），用于简单多集函数上的数学分析。当将函数的域限制为给定维度的多集时，深度集还提供了一个普遍推广（universal approximation），需要的隐藏空间维度为$O(N^D)$，其中$N$是输入多集的最大大小。在这篇论文中，我们将这个结果加强，证明了普遍表示是给定维度的连续和不连续多集函数，只需要隐藏空间维度为$O(N^D)$。我们还引入了可识别的多集（identifiable multisets），可以使用一个元素识别函数（identifier function）将其元素唯一标识。使用我们的分析方法，我们证明了一个条件对称的数学分析模型可以用$2DN$维度表示任意连续多集函数。此外，我们还证明了这个模型的数学分析器和实现器都是连续的，这是我们对现有工作的主要贡献之一。此外，这也提供了$O(N^D)$的独立进步，与先前的结果相比，这个结果可以更好地表示连续和不连续多集函数的普遍表示。最后，我们延伸我们的结果，提供了对应 permutation-invariant tensor functions的特殊条件对称分析模型。这些家族的条件对称分析模型可以用来设计深度网络架构，并将其部署到多个学习任务上，例如序列、图像和 Graf 等。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Attacks-on-Fairness-of-Graph-Neural-Networks"><a href="#Adversarial-Attacks-on-Fairness-of-Graph-Neural-Networks" class="headerlink" title="Adversarial Attacks on Fairness of Graph Neural Networks"></a>Adversarial Attacks on Fairness of Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13822">http://arxiv.org/abs/2310.13822</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhangbinchi/g-fairattack">https://github.com/zhangbinchi/g-fairattack</a></li>
<li>paper_authors: Binchi Zhang, Yushun Dong, Chen Chen, Yada Zhu, Minnan Luo, Jundong Li</li>
<li>for: 这篇论文旨在调查对具有公平性考虑的图神经网络（GNNs）的攻击性评估，以及对不同类型的 GNNs 的公平性攻击。</li>
<li>methods: 该论文提出了 G-FairAttack 框架，用于对 GNNs 进行公平性攻击，包括不可察觉地损害公平性。同时， authors 还提出了一种快速计算技术来降低攻击时间复杂度。</li>
<li>results: 实验研究表明，G-FairAttack 可以成功地损害不同类型的 GNNs 的公平性，而不会影响预测的准确性。这些结果表明了对 GNNs 的公平性攻击的潜在漏洞，并促进了进一步研究 GNNs 的Robustness 在公平性方面。<details>
<summary>Abstract</summary>
Fairness-aware graph neural networks (GNNs) have gained a surge of attention as they can reduce the bias of predictions on any demographic group (e.g., female) in graph-based applications. Although these methods greatly improve the algorithmic fairness of GNNs, the fairness can be easily corrupted by carefully designed adversarial attacks. In this paper, we investigate the problem of adversarial attacks on fairness of GNNs and propose G-FairAttack, a general framework for attacking various types of fairness-aware GNNs in terms of fairness with an unnoticeable effect on prediction utility. In addition, we propose a fast computation technique to reduce the time complexity of G-FairAttack. The experimental study demonstrates that G-FairAttack successfully corrupts the fairness of different types of GNNs while keeping the attack unnoticeable. Our study on fairness attacks sheds light on potential vulnerabilities in fairness-aware GNNs and guides further research on the robustness of GNNs in terms of fairness. The open-source code is available at https://github.com/zhangbinchi/G-FairAttack.
</details>
<details>
<summary>摘要</summary>
“公平意识的图 neural network (GNN) 在图基应用中受到了一波关注，因为它们可以减少任何人类群体（如女性）的预测偏见。尽管这些方法可以大幅提高 GNN 的算法公平性，但这种公平性可以被轻松地通过特制的对抗攻击破坏。在这篇论文中，我们调查了 GNN 公平性对抗攻击的问题，并提出了 G-FairAttack，一种可以对多种公平意识 GNN 进行攻击，并且保持攻击不可见地影响预测Utility。此外，我们还提出了一种快速计算技术来降低 G-FairAttack 的时间复杂度。实验研究表明，G-FairAttack 成功地破坏了不同类型的 GNN 公平性，而且保持了攻击不可见。我们的研究对 GNN 公平性对抗攻击提供了新的灵感，并指导了 GNN 的可靠性研究。G-FairAttack 的开源代码可以在 https://github.com/zhangbinchi/G-FairAttack 中获取。”
</details></li>
</ul>
<hr>
<h2 id="Geometric-Learning-with-Positively-Decomposable-Kernels"><a href="#Geometric-Learning-with-Positively-Decomposable-Kernels" class="headerlink" title="Geometric Learning with Positively Decomposable Kernels"></a>Geometric Learning with Positively Decomposable Kernels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13821">http://arxiv.org/abs/2310.13821</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nathael Da Costa, Cyrus Mostajeran, Juan-Pablo Ortega, Salem Said</li>
<li>for: 这个论文主要针对非欧几何数据空间上的机器学习问题。</li>
<li>methods: 该论文提出了基于 reproduce kernel Krein space (RKKS) 的方法，该方法只需要一个可分解为正数的kernels，而不需要访问这个分解。</li>
<li>results: 论文表明，对于各种特定的几何空间，可以通过constructing positively decomposable kernels来实现机器学习。此外，该论文还提供了一些理论基础，用于推广 RKKS-based 方法。<details>
<summary>Abstract</summary>
Kernel methods are powerful tools in machine learning. Classical kernel methods are based on positive-definite kernels, which map data spaces into reproducing kernel Hilbert spaces (RKHS). For non-Euclidean data spaces, positive-definite kernels are difficult to come by. In this case, we propose the use of reproducing kernel Krein space (RKKS) based methods, which require only kernels that admit a positive decomposition. We show that one does not need to access this decomposition in order to learn in RKKS. We then investigate the conditions under which a kernel is positively decomposable. We show that invariant kernels admit a positive decomposition on homogeneous spaces under tractable regularity assumptions. This makes them much easier to construct than positive-definite kernels, providing a route for learning with kernels for non-Euclidean data. By the same token, this provides theoretical foundations for RKKS-based methods in general.
</details>
<details>
<summary>摘要</summary>
kernels 是机器学习中的强大工具。经典的kernel方法基于正定kernel，将数据空间映射到 reproduce kernel Hilbert space (RKHS) 中。不theless, 非欧几何数据空间中的正定kernel很难得到。在这种情况下，我们提议使用 reproduce kernel Krein space (RKKS) 基本方法，只需要kernels admit a positive decomposition。我们证明，不需要访问这种分解，可以在 RKKS 中学习。然后，我们研究了kernels admit positive decomposition的条件。我们证明，对具有 Symmetry 的kernel，在具有homogeneous space的情况下，可以在 tractable regularity assumptions 下获得正定分解。这使得这些kernel在constructing上比正定kernel更加容易，提供了非欧几何数据上的学习方法的理论基础。
</details></li>
</ul>
<hr>
<h2 id="A-Better-Match-for-Drivers-and-Riders-Reinforcement-Learning-at-Lyft"><a href="#A-Better-Match-for-Drivers-and-Riders-Reinforcement-Learning-at-Lyft" class="headerlink" title="A Better Match for Drivers and Riders: Reinforcement Learning at Lyft"></a>A Better Match for Drivers and Riders: Reinforcement Learning at Lyft</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13810">http://arxiv.org/abs/2310.13810</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xabi Azagirre, Akshay Balwally, Guillaume Candeli, Nicholas Chamandy, Benjamin Han, Alona King, Hyungjun Lee, Martin Loncaric, Sébastien Martin, Vijay Narasiman, Zhiwei, Qin, Baptiste Richard, Sara Smoot, Sean Taylor, Garrett van Ryzin, Di Wu, Fei Yu, Alex Zamoshchin</li>
<li>for: 提高预估驾驶员的预计收益，以便更好地匹配乘客和司机。</li>
<li>methods: 使用了一种新的在线强化学习方法，在实时中估算司机未来的收益，并使用这些信息来找到更有效的匹配。</li>
<li>results: 通过这种改进后，司机可以每年服务更多的乘客，至少增加3000万美元的增值收入。<details>
<summary>Abstract</summary>
To better match drivers to riders in our ridesharing application, we revised Lyft's core matching algorithm. We use a novel online reinforcement learning approach that estimates the future earnings of drivers in real time and use this information to find more efficient matches. This change was the first documented implementation of a ridesharing matching algorithm that can learn and improve in real time. We evaluated the new approach during weeks of switchback experimentation in most Lyft markets, and estimated how it benefited drivers, riders, and the platform. In particular, it enabled our drivers to serve millions of additional riders each year, leading to more than $30 million per year in incremental revenue. Lyft rolled out the algorithm globally in 2021.
</details>
<details>
<summary>摘要</summary>
为了更好地匹配驾驶员和乘客在我们的乘车应用程序中，我们修改了Lyft的核心匹配算法。我们使用了一种新的在线强化学习方法，以估计驾驶员未来的收益情况，并使用这些信息来找到更有效的匹配。这是首次实现了实时学习和改进的乘车匹配算法的documented实现。我们在多个Lyft市场进行了数周的交换实验，并估计了驾驶员、乘客和平台受益的方面。特别是，它允许我们的驾驶员每年服务数百万名乘客，导致每年超过3000万美元的额外收入。Lyft在2021年全球推广了这种算法。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Learn-at-Test-Time"><a href="#Learning-to-Learn-at-Test-Time" class="headerlink" title="Learning to (Learn at Test Time)"></a>Learning to (Learn at Test Time)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13807">http://arxiv.org/abs/2310.13807</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/test-time-training/mttt">https://github.com/test-time-training/mttt</a></li>
<li>paper_authors: Yu Sun, Xinhao Li, Karan Dalal, Chloe Hsu, Sanmi Koyejo, Carlos Guestrin, Xiaolong Wang, Tatsunori Hashimoto, Xinlei Chen</li>
<li>for: 本文改进了supervised learning的问题，将其转换为一个两层循环学习问题（i.e. 学习问题）。内层循环学习每个个体实例自动提供自我监督，然后进行最终预测。外层循环学习内层循环学习任务，以提高其最终预测。</li>
<li>methods: 内层循环学习使用了自我监督学习，并且可以使用线性注意力或自我注意力。外层循环学习学习了内层循环学习任务，以提高其最终预测。</li>
<li>results: 对于ImageNet datasets，使用本文的方法可以大幅提高准确率和计算量，而且比 tradicional transformers 更高效。此外，当内层循环学习器是神经网络时，本文的方法可以在224x224 Raw Pixels的情况下大幅提高准确率，而且不能由传统的 transformers 实现。<details>
<summary>Abstract</summary>
We reformulate the problem of supervised learning as learning to learn with two nested loops (i.e. learning problems). The inner loop learns on each individual instance with self-supervision before final prediction. The outer loop learns the self-supervised task used by the inner loop, such that its final prediction improves. Our inner loop turns out to be equivalent to linear attention when the inner-loop learner is only a linear model, and to self-attention when it is a kernel estimator. For practical comparison with linear or self-attention layers, we replace each of them in a transformer with an inner loop, so our outer loop is equivalent to training the architecture. When each inner-loop learner is a neural network, our approach vastly outperforms transformers with linear attention on ImageNet from 224 x 224 raw pixels in both accuracy and FLOPs, while (regular) transformers cannot run.
</details>
<details>
<summary>摘要</summary>
我们重新定义监督学习的问题为内层循环学习（i.e. 学习问题），内层循环在每个个体例上进行自我监督学习，然后进行最终预测。外层循环则学习内层循环使用的自我监督任务，以改善其最终预测。我们的内层循环发现是线性注意力（linear attention）当内部学习器只是一个线性模型，而是自适应器当内部学习器。为了与线性注意力或自适应器层进行实用比较，我们将它们替换为内层循环，因此外层循环相等于训练架构。当内部学习器是一个神经网络时，我们的方法在ImageNet上与使用线性注意力的transformer相比，在精确性和FLOPs方面具有很大的提升，而且regular transformer无法运行。
</details></li>
</ul>
<hr>
<h2 id="Comparative-Analysis-of-Machine-Learning-Algorithms-for-Solar-Irradiance-Forecasting-in-Smart-Grids"><a href="#Comparative-Analysis-of-Machine-Learning-Algorithms-for-Solar-Irradiance-Forecasting-in-Smart-Grids" class="headerlink" title="Comparative Analysis of Machine Learning Algorithms for Solar Irradiance Forecasting in Smart Grids"></a>Comparative Analysis of Machine Learning Algorithms for Solar Irradiance Forecasting in Smart Grids</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13791">http://arxiv.org/abs/2310.13791</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saman Soleymani, Shima Mohammadzadeh</li>
<li>for: 这个研究旨在预测太阳辐射，以便优化太阳能系统的使用。</li>
<li>methods: 本研究使用了下一代机器学习算法，包括随机树、Extreme Gradient Boosting (XGBoost)、Light Gradient Boosted Machine (lightGBM) ensemble、CatBoost、和多层感知神经网络 (MLP-ANNs)，以预测太阳辐射。此外，运用了 Bayesian 优化来调整参数。</li>
<li>results: 综合结果显示，MLP-ANNs 的性能会随着特征选择而改善；而 Random Forest 则与其他学习算法相比，表现较好。<details>
<summary>Abstract</summary>
The increasing global demand for clean and environmentally friendly energy resources has caused increased interest in harnessing solar power through photovoltaic (PV) systems for smart grids and homes. However, the inherent unpredictability of PV generation poses problems associated with smart grid planning and management, energy trading and market participation, demand response, reliability, etc. Therefore, solar irradiance forecasting is essential for optimizing PV system utilization. This study proposes the next-generation machine learning algorithms such as random forests, Extreme Gradient Boosting (XGBoost), Light Gradient Boosted Machine (lightGBM) ensemble, CatBoost, and Multilayer Perceptron Artificial Neural Networks (MLP-ANNs) to forecast solar irradiance. Besides, Bayesian optimization is applied to hyperparameter tuning. Unlike tree-based ensemble algorithms that select the features intrinsically, MLP-ANN needs feature selection as a separate step. The simulation results indicate that the performance of the MLP-ANNs improves when feature selection is applied. Besides, the random forest outperforms the other learning algorithms.
</details>
<details>
<summary>摘要</summary>
全球减少可再生能源的需求在不断增长，为了利用太阳能电力，人们对光伏系统的智能网格和家庭等领域进行了更多的研究。然而，光伏生产的不可预测性会对智能网格规划和管理、能源交易和市场参与、需求回应等带来问题。因此，太阳辐射预测是必要的。本研究提出了下一代机器学习算法，如随机森林、极限梯度提升（XGBoost）、轻量级梯度提升机器（lightGBM） ensemble、CatBoost和多层感知神经网络（MLP-ANNs）来预测太阳辐射。此外，我们还应用了 bayesian 优化来调整超参数。与树型ensemble算法不同，MLP-ANN需要将特征选择作为一个分离的步骤。实验结果表明，当特征选择被应用时，MLP-ANN的性能会提高。此外，随机森林在其他学习算法中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Graph-AI-in-Medicine"><a href="#Graph-AI-in-Medicine" class="headerlink" title="Graph AI in Medicine"></a>Graph AI in Medicine</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13767">http://arxiv.org/abs/2310.13767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruth Johnson, Michelle M. Li, Ayush Noori, Owen Queen, Marinka Zitnik</li>
<li>for: 这种论文主要用于探讨在医学人工智能中Graph Representation Learning的应用，尤其是通过Graph Neural Networks（GNNs）来捕捉医学数据中的复杂关系。</li>
<li>methods: 这种方法主要使用Graph Neural Networks（GNNs）来处理医学数据，并通过视模态为节点之间的关系来处理数据。</li>
<li>results: 这种方法可以在不同的医学任务上实现模型的转移，并且可以在不添加参数或最小再训练的情况下实现模型的泛化。然而，在医学决策中，人类中心的设计和模型解释性是不可或缺的。<details>
<summary>Abstract</summary>
In clinical artificial intelligence (AI), graph representation learning, mainly through graph neural networks (GNNs), stands out for its capability to capture intricate relationships within structured clinical datasets. With diverse data -- from patient records to imaging -- GNNs process data holistically by viewing modalities as nodes interconnected by their relationships. Graph AI facilitates model transfer across clinical tasks, enabling models to generalize across patient populations without additional parameters or minimal re-training. However, the importance of human-centered design and model interpretability in clinical decision-making cannot be overstated. Since graph AI models capture information through localized neural transformations defined on graph relationships, they offer both an opportunity and a challenge in elucidating model rationale. Knowledge graphs can enhance interpretability by aligning model-driven insights with medical knowledge. Emerging graph models integrate diverse data modalities through pre-training, facilitate interactive feedback loops, and foster human-AI collaboration, paving the way to clinically meaningful predictions.
</details>
<details>
<summary>摘要</summary>
在临床人工智能（AI）领域，图表学习（GNNs）在捕捉复杂的临床数据关系方面表现出色。通过视 modalities 为节点之间的关系的方式，GNNs 可以处理数据的整体特征。图AI 使得模型可以在不同患者人口中进行模型迁移，无需额外参数或 minimal 再训练。然而，在临床决策中，人类中心的设计和模型解释性不可或缺。由于 GNNs 通过本地神经变换定义在图关系上来捕捉信息，因此它们同时提供了机会和挑战来描述模型的逻辑。知识图可以增强解释性，将模型驱动的洞察与医学知识相对应。新兴的图模型通过预训练、实时反馈循环和人类-AI合作，为临床有意义的预测开辟了道路。
</details></li>
</ul>
<hr>
<h2 id="Learning-Interatomic-Potentials-at-Multiple-Scales"><a href="#Learning-Interatomic-Potentials-at-Multiple-Scales" class="headerlink" title="Learning Interatomic Potentials at Multiple Scales"></a>Learning Interatomic Potentials at Multiple Scales</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13756">http://arxiv.org/abs/2310.13756</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Fu, Albert Musaelian, Anders Johansson, Tommi Jaakkola, Boris Kozinsky</li>
<li>for: 这个论文是为了提高分子动力学（MD） simulations 的速度而写的。</li>
<li>methods: 这个论文使用了一种多时步（MTS）integator，该integator可以在评估certain potential energy terms的时候采用更短的时间步。这种方法是由于类型 potentials 的简单且局限的分析形式而可行。</li>
<li>results: 这个论文的结果表明，使用这种方法可以在MD simulations中提高速度（~3x在我们的实验中）无需失去精度。<details>
<summary>Abstract</summary>
The need to use a short time step is a key limit on the speed of molecular dynamics (MD) simulations. Simulations governed by classical potentials are often accelerated by using a multiple-time-step (MTS) integrator that evaluates certain potential energy terms that vary more slowly than others less frequently. This approach is enabled by the simple but limiting analytic forms of classical potentials. Machine learning interatomic potentials (MLIPs), in particular recent equivariant neural networks, are much more broadly applicable than classical potentials and can faithfully reproduce the expensive but accurate reference electronic structure calculations used to train them. They still, however, require the use of a single short time step, as they lack the inherent term-by-term scale separation of classical potentials. This work introduces a method to learn a scale separation in complex interatomic interactions by co-training two MLIPs. Initially, a small and efficient model is trained to reproduce short-time-scale interactions. Subsequently, a large and expressive model is trained jointly to capture the remaining interactions not captured by the small model. When running MD, the MTS integrator then evaluates the smaller model for every time step and the larger model less frequently, accelerating simulation. Compared to a conventionally trained MLIP, our approach can achieve a significant speedup (~3x in our experiments) without a loss of accuracy on the potential energy or simulation-derived quantities.
</details>
<details>
<summary>摘要</summary>
在分子动力学（MD）模拟中，使用短时步是一个关键限制。使用多时步（MTS）integrodor的模拟通常会加速，因为它可以在不同时间步评估不同慢速的潜在能量项。这种方法是由于类型的潜在能量函数的简单但限制的分析型表达。机器学习交互原子电子镜像（MLIPs），特别是最近的对称神经网络，在比较广泛的应用场景中比类型的潜在能量函数更加适用。它们仍然需要使用短时步，因为它们缺乏类型潜在能量函数中的自然层次分离。这项工作介绍了一种方法，通过同时训练两个MLIP来学习复杂的Interatomic交互中的层次分离。首先，一个小型和高效的模型被训练来复制短时间步的交互。然后，一个大型和表达力强的模型被同时训练，以捕捉不同时间步的交互。在MD模拟中，MTS integrodor会在每个时 step中评估小型模型，而大型模型则会在更少的时间 step中评估。相比于传统训练的MLIP，我们的方法可以在同等精度下实现约3倍的加速（~3x在我们的实验中）。
</details></li>
</ul>
<hr>
<h2 id="FairBranch-Fairness-Conflict-Correction-on-Task-group-Branches-for-Fair-Multi-Task-Learning"><a href="#FairBranch-Fairness-Conflict-Correction-on-Task-group-Branches-for-Fair-Multi-Task-Learning" class="headerlink" title="FairBranch: Fairness Conflict Correction on Task-group Branches for Fair Multi-Task Learning"></a>FairBranch: Fairness Conflict Correction on Task-group Branches for Fair Multi-Task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13746">http://arxiv.org/abs/2310.13746</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/arjunroyihrpa/fairbranch-open-source-intelligence">https://github.com/arjunroyihrpa/fairbranch-open-source-intelligence</a></li>
<li>paper_authors: Arjun Roy, Christos Koutlis, Symeon Papadopoulos, Eirini Ntoutsi</li>
<li>for: 提高多任务学习（MTL）模型的公平性和准确性</li>
<li>methods: 使用分支法（FairBranch），分析学习过程中任务之间的参数相似性，并在相关任务集中进行任务分组，以减少负向传递和偏见传递</li>
<li>results: FairBranch在表格和视觉MTL问题中表现出色，在公平性和准确性两个方面超过了当前状态的MTL方法<details>
<summary>Abstract</summary>
The generalization capacity of Multi-Task Learning (MTL) becomes limited when unrelated tasks negatively impact each other by updating shared parameters with conflicting gradients, resulting in negative transfer and a reduction in MTL accuracy compared to single-task learning (STL). Recently, there has been an increasing focus on the fairness of MTL models, necessitating the optimization of both accuracy and fairness for individual tasks. Similarly to how negative transfer affects accuracy, task-specific fairness considerations can adversely influence the fairness of other tasks when there is a conflict of fairness loss gradients among jointly learned tasks, termed bias transfer. To address both negative and bias transfer in MTL, we introduce a novel method called FairBranch. FairBranch branches the MTL model by assessing the similarity of learned parameters, grouping related tasks to mitigate negative transfer. Additionally, it incorporates fairness loss gradient conflict correction between adjoining task-group branches to address bias transfer within these task groups. Our experiments in tabular and visual MTL problems demonstrate that FairBranch surpasses state-of-the-art MTL methods in terms of both fairness and accuracy.
</details>
<details>
<summary>摘要</summary>
多任务学习（MTL）的通用能力在不相关任务之间发生负面影响，导致共享参数更新的梯度冲突，从而导致负面传递和相对于单任务学习（STL）的MTL精度下降。随着对MTL模型的公平性的增加注重，我们需要同时优化任务精度和公平性。与负面传递类似，任务特定的公平性考虑可能会对其他共同学习任务的公平性产生负面影响，这被称为偏见传递。为解决MTL中的负面和偏见传递，我们提出了一种新的方法called FairBranch。FairBranch通过评估学习到的参数相似性，将相关任务分组，以避免负面传递。此外，它还 incorporates fairness loss gradient conflict correction between adjacent task-group branches，以解决内部任务组中的偏见传递。我们在表格和视觉MTL问题上进行了实验，结果显示，FairBranch超过了当前MTL方法的公平性和精度。
</details></li>
</ul>
<hr>
<h2 id="CAPIVARA-Cost-Efficient-Approach-for-Improving-Multilingual-CLIP-Performance-on-Low-Resource-Languages"><a href="#CAPIVARA-Cost-Efficient-Approach-for-Improving-Multilingual-CLIP-Performance-on-Low-Resource-Languages" class="headerlink" title="CAPIVARA: Cost-Efficient Approach for Improving Multilingual CLIP Performance on Low-Resource Languages"></a>CAPIVARA: Cost-Efficient Approach for Improving Multilingual CLIP Performance on Low-Resource Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13683">http://arxiv.org/abs/2310.13683</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hiaac-nlp/capivara">https://github.com/hiaac-nlp/capivara</a></li>
<li>paper_authors: Gabriel Oliveira dos Santos, Diego A. B. Moreira, Alef Iury Ferreira, Jhessica Silva, Luiz Pereira, Pedro Bueno, Thiago Sousa, Helena Maia, Nádia Da Silva, Esther Colombini, Helio Pedrini, Sandra Avila</li>
<li>for: 提高多语言CLIP模型在低资源语言中的性能</li>
<li>methods: 使用图像captioning和机器翻译生成多个Synthetic caption，并优化训练管道使其更加效率</li>
<li>results: 在零批量视觉语言任务中达到了状态的艺术，并在葡萄牙语文本中显示出了显著的改进Here’s the breakdown of each point:</li>
<li>for: The paper is written to improve the performance of multilingual CLIP models in low-resource languages.</li>
<li>methods: The paper uses image captioning and machine translation to generate multiple synthetic captions in low-resource languages, and optimizes the training pipeline with LiT, LoRA, and gradient checkpointing to alleviate the computational cost.</li>
<li>results: The proposed method achieves state-of-the-art performance in zero-shot tasks involving images and Portuguese texts, and shows the potential for significant improvements in other low-resource languages with fine-tuning.<details>
<summary>Abstract</summary>
This work introduces CAPIVARA, a cost-efficient framework designed to enhance the performance of multilingual CLIP models in low-resource languages. While CLIP has excelled in zero-shot vision-language tasks, the resource-intensive nature of model training remains challenging. Many datasets lack linguistic diversity, featuring solely English descriptions for images. CAPIVARA addresses this by augmenting text data using image captioning and machine translation to generate multiple synthetic captions in low-resource languages. We optimize the training pipeline with LiT, LoRA, and gradient checkpointing to alleviate the computational cost. Through extensive experiments, CAPIVARA emerges as state of the art in zero-shot tasks involving images and Portuguese texts. We show the potential for significant improvements in other low-resource languages, achieved by fine-tuning the pre-trained multilingual CLIP using CAPIVARA on a single GPU for 2 hours. Our model and code is available at https://github.com/hiaac-nlp/CAPIVARA.
</details>
<details>
<summary>摘要</summary>
这个工作介绍了CAPIVARA，一种可靠的框架，用于提高多语言CLIP模型在低资源语言中的性能。尽管CLIP在零shot视频语言任务上表现出色，但模型训练的资源成本仍然很高。许多数据集缺乏语言多样性，只有英文描述图像。CAPIVARA解决这个问题，通过图像描述和机器翻译来生成多个synthetic描述在低资源语言中。我们优化了训练管道，使用LiT、LoRA和梯度检查点来缓解计算成本。经过广泛的实验，CAPIVARA在零shot任务中与图像和葡萄牙文描述表现出状元。我们显示了其他低资源语言可以获得显著改进，通过在单个GPU上使用CAPIVARA进行2小时的微调，对pre-trained多语言CLIP进行了深度学习。我们的模型和代码可以在GitHub上找到：https://github.com/hiaac-nlp/CAPIVARA。
</details></li>
</ul>
<hr>
<h2 id="RealFM-A-Realistic-Mechanism-to-Incentivize-Data-Contribution-and-Device-Participation"><a href="#RealFM-A-Realistic-Mechanism-to-Incentivize-Data-Contribution-and-Device-Participation" class="headerlink" title="RealFM: A Realistic Mechanism to Incentivize Data Contribution and Device Participation"></a>RealFM: A Realistic Mechanism to Incentivize Data Contribution and Device Participation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13681">http://arxiv.org/abs/2310.13681</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Bornstein, Amrit Singh Bedi, Anit Kumar Sahu, Furqan Khan, Furong Huang</li>
<li>for: 本研究旨在实际场景下进行联合学习（Federated Learning，FL），并解决现有框架中出现的免费乘客问题。</li>
<li>methods: 本研究提出了RealFM，首个真实联合机制，该机制（1）实际地模型设备利益，（2）激励数据提供和设备参与，（3）可靠地除免费乘客现象。RealFM不需要数据分享，允许非线性关系 между模型准确率和利益，从而提高服务器和参与设备的利益和数据提供量。</li>
<li>results: 在实际数据上，RealFM可以提高设备和服务器的利益和数据提供量，比基eline机制提高3倍和7倍。<details>
<summary>Abstract</summary>
Edge device participation in federating learning (FL) has been typically studied under the lens of device-server communication (e.g., device dropout) and assumes an undying desire from edge devices to participate in FL. As a result, current FL frameworks are flawed when implemented in real-world settings, with many encountering the free-rider problem. In a step to push FL towards realistic settings, we propose RealFM: the first truly federated mechanism which (1) realistically models device utility, (2) incentivizes data contribution and device participation, and (3) provably removes the free-rider phenomena. RealFM does not require data sharing and allows for a non-linear relationship between model accuracy and utility, which improves the utility gained by the server and participating devices compared to non-participating devices as well as devices participating in other FL mechanisms. On real-world data, RealFM improves device and server utility, as well as data contribution, by up to 3 magnitudes and 7x respectively compared to baseline mechanisms.
</details>
<details>
<summary>摘要</summary>
<<SYS>>这里使用 Simplified Chinese 翻译文本。<</SYS>> presente study on edge device participation in federated learning (FL) 通常是通过 device-server communication (例如 device dropout) 进行研究，并假设 edge devices 对 FL 有不断的渴望参与。然而，实际上实现 FL 的现场设置中，现有的 FL 框架受到问题，许多人遇到了免责者问题。为了将 FL 带向现实的设置，我们提出 RealFM：首个真实的联邦机制，其中 (1) 实际地模型 device 的利益， (2) 鼓励数据贡献和设备参与， (3) 可靠地除掉免责者现象。RealFM 不需要数据共享，并允许非线性的数据对应模型精度和利益之间的关系，这提高了服务器和参与设备的利益，以及参与其他 FL 机制的设备中的数据贡献和参与。使用真实数据，RealFM 可以与基准机制相比，提高设备和服务器的利益，以及数据贡献，高达 3 倍和 7 倍。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Transport-for-Measures-with-Noisy-Tree-Metric"><a href="#Optimal-Transport-for-Measures-with-Noisy-Tree-Metric" class="headerlink" title="Optimal Transport for Measures with Noisy Tree Metric"></a>Optimal Transport for Measures with Noisy Tree Metric</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13653">http://arxiv.org/abs/2310.13653</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tam Le, Truyen Nguyen, Kenji Fukumizu</li>
<li>for: 研究了最优运输（OT）问题，即在树 metric 空间上支持的概率测度之间的OT问题。</li>
<li>methods: 采用了robust OT方法，即考虑最大可能的树 metric 间距，以降低受到噪声或敌意测量的影响。</li>
<li>results: 提出了新的uncertainty sets of tree metrics，并利用树结构以及支持测度，实现了closed-form表达，以便快速计算。此外，还证明了max-min robust OT具有metric property和负定性，并提出了正定 definite kernels。在多个实验中，测试了这些kernels在文档分类和拓扑数据分析中的表现。<details>
<summary>Abstract</summary>
We study optimal transport (OT) problem for probability measures supported on a tree metric space. It is known that such OT problem (i.e., tree-Wasserstein (TW)) admits a closed-form expression, but depends fundamentally on the underlying tree structure over supports of input measures. In practice, the given tree structure may be, however, perturbed due to noisy or adversarial measurements. In order to mitigate this issue, we follow the max-min robust OT approach which considers the maximal possible distances between two input measures over an uncertainty set of tree metrics. In general, this approach is hard to compute, even for measures supported in $1$-dimensional space, due to its non-convexity and non-smoothness which hinders its practical applications, especially for large-scale settings. In this work, we propose \emph{novel uncertainty sets of tree metrics} from the lens of edge deletion/addition which covers a diversity of tree structures in an elegant framework. Consequently, by building upon the proposed uncertainty sets, and leveraging the tree structure over supports, we show that the max-min robust OT also admits a closed-form expression for a fast computation as its counterpart standard OT (i.e., TW). Furthermore, we demonstrate that the max-min robust OT satisfies the metric property and is negative definite. We then exploit its negative definiteness to propose \emph{positive definite kernels} and test them in several simulations on various real-world datasets on document classification and topological data analysis for measures with noisy tree metric.
</details>
<details>
<summary>摘要</summary>
我们研究优化运输（OT）问题，将概率分布嵌入树度量空间中。已知这个OT问题（即树 Wasserstein（TW））有闭形表达，但它具有基于支持入力测度的树结构的根本关联。在实践中， giventree结构可能会受到不确定或反对的测量影响。为了解决这个问题，我们遵循max-min类型的Robust OT方法，考虑两个入力测度之间的最大可能距离，这个方法在一般情况下是困难Compute，尤其是在大规模设定下，因为它的非对称和非弹性对computation带来障碍。在这个工作中，我们提出了一种新的不确定树度量集，从edge删除/新增的角度出发，这些集合覆盖了树结构的多样性，并且具有一个漂亮的框架。因此，通过这些不确定树度量集，并且利用树结构，我们显示了max-minRobust OT也有闭形表达，可以快速Compute。此外，我们显示了max-minRobust OT满足了度量性质，并且是负定的。我们运用其负定性，提出了一种正definite核函数，并在实验中对多个实际数据进行测试，包括文档分类和数据探索。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-the-contribution-of-different-passively-collected-data-to-predict-Stress-and-Depression"><a href="#Analyzing-the-contribution-of-different-passively-collected-data-to-predict-Stress-and-Depression" class="headerlink" title="Analyzing the contribution of different passively collected data to predict Stress and Depression"></a>Analyzing the contribution of different passively collected data to predict Stress and Depression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13607">http://arxiv.org/abs/2310.13607</a></li>
<li>repo_url: None</li>
<li>paper_authors: Irene Bonafonte, Cristina Bustos, Abraham Larrazolo, Gilberto Lorenzo Martinez Luna, Adolfo Guzman Arenas, Xavier Baro, Isaac Tourgeman, Mercedes Balcells, Agata Lapedriza</li>
<li>for: 这个论文旨在利用捕获数据来评估心理健康。</li>
<li>methods: 论文使用不同类型的捕获数据（WiFi、GPS、社交互动、手机日志、体育活动和学术特征）来预测每天的自报焦虑和抑郁分数。</li>
<li>results: 研究发现，WiFi特征（表示移动 Pattern）和手机日志特征（与睡眠 Pattern相关）对焦虑和抑郁预测具有重要作用。<details>
<summary>Abstract</summary>
The possibility of recognizing diverse aspects of human behavior and environmental context from passively captured data motivates its use for mental health assessment. In this paper, we analyze the contribution of different passively collected sensor data types (WiFi, GPS, Social interaction, Phone Log, Physical Activity, Audio, and Academic features) to predict daily selfreport stress and PHQ-9 depression score. First, we compute 125 mid-level features from the original raw data. These 125 features include groups of features from the different sensor data types. Then, we evaluate the contribution of each feature type by comparing the performance of Neural Network models trained with all features against Neural Network models trained with specific feature groups. Our results show that WiFi features (which encode mobility patterns) and Phone Log features (which encode information correlated with sleep patterns), provide significative information for stress and depression prediction.
</details>
<details>
<summary>摘要</summary>
“ passively captured 数据的多方面特征和环境 контекст的识别，为心理健康评估带来了动机。本文分析了不同类型的抓取数据（WiFi、GPS、社交交互、手机记录、体育活动和学术特征）对日常自报 стресс和PHQ-9抑郁分数预测的贡献。首先，我们从原始的Raw数据中计算出125个中间特征。这125个特征包括不同数据类型的特征组。然后，我们评估每个特征类型的贡献，通过比较使用所有特征和特定特征组合的神经网络模型的性能来进行比较。我们的结果显示，WiFi特征（表示流动性模式）和手机记录特征（表示睡眠模式相关信息）对心理健康预测提供了重要的信息。”Note: The translation is done using Google Translate, and may not be perfect. Please let me know if you need further assistance.
</details></li>
</ul>
<hr>
<h2 id="Unraveling-the-Enigma-of-Double-Descent-An-In-depth-Analysis-through-the-Lens-of-Learned-Feature-Space"><a href="#Unraveling-the-Enigma-of-Double-Descent-An-In-depth-Analysis-through-the-Lens-of-Learned-Feature-Space" class="headerlink" title="Unraveling the Enigma of Double Descent: An In-depth Analysis through the Lens of Learned Feature Space"></a>Unraveling the Enigma of Double Descent: An In-depth Analysis through the Lens of Learned Feature Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13572">http://arxiv.org/abs/2310.13572</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yufei-gu-451/double_descent_inference">https://github.com/yufei-gu-451/double_descent_inference</a></li>
<li>paper_authors: Yufei Gu, Xiaoqing Zheng, Tomaso Aste</li>
<li>for: 这个论文主要是研究深度学习中的双峰现象，以及这种现象如何与噪声数据相关。</li>
<li>methods: 作者们使用了多种方法来研究双峰现象，包括对学习表示空间的分析，以及对不同模型和任务的实验研究。</li>
<li>results: 研究结果表明，双峰现象与噪声数据的存在密切相关，并且可以通过增加模型的参数数量来避免或减少这种现象的发生。作者们还提出了一种理论，即双峰现象是由模型首先学习噪声数据，然后通过拟合来添加隐式正则化的过程。<details>
<summary>Abstract</summary>
Double descent presents a counter-intuitive aspect within the machine learning domain, and researchers have observed its manifestation in various models and tasks. While some theoretical explanations have been proposed for this phenomenon in specific contexts, an accepted theory to account for its occurrence in deep learning remains yet to be established. In this study, we revisit the phenomenon of double descent and demonstrate that its occurrence is strongly influenced by the presence of noisy data. Through conducting a comprehensive analysis of the feature space of learned representations, we unveil that double descent arises in imperfect models trained with noisy data. We argue that double descent is a consequence of the model first learning the noisy data until interpolation and then adding implicit regularization via over-parameterization acquiring therefore capability to separate the information from the noise. We postulate that double descent should never occur in well-regularized models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="On-sample-complexity-of-conditional-independence-testing-with-Von-Mises-estimator-with-application-to-causal-discovery"><a href="#On-sample-complexity-of-conditional-independence-testing-with-Von-Mises-estimator-with-application-to-causal-discovery" class="headerlink" title="On sample complexity of conditional independence testing with Von Mises estimator with application to causal discovery"></a>On sample complexity of conditional independence testing with Von Mises estimator with application to causal discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13553">http://arxiv.org/abs/2310.13553</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fateme Jamshidi, Luca Ganassali, Negar Kiyavash</li>
<li>for: 本研究旨在提出一种基于kernel density estimator的非 Parametric Von Mises estimator，用于测量多变量分布的 entropy。</li>
<li>methods: 该研究使用了 conditional independence testing，并基于 exponential concentration inequality，设计了一种名为VM-CI的测试方法，可以实现最佳参数化速率。</li>
<li>results: 研究表明，VM-CI测试方法可以在smoothness assumptions下 achiev 最佳 Parametric rates，并且可以 Characterize 任何基于VM-CI测试的 constraint-based causal discovery algorithm 的样本复杂性。此外，VM-CI测试方法在实际中也能够超过其他流行的CI测试方法，这对结构学习也有着良好的影响。<details>
<summary>Abstract</summary>
Motivated by conditional independence testing, an essential step in constraint-based causal discovery algorithms, we study the nonparametric Von Mises estimator for the entropy of multivariate distributions built on a kernel density estimator. We establish an exponential concentration inequality for this estimator. We design a test for conditional independence (CI) based on our estimator, called VM-CI, which achieves optimal parametric rates under smoothness assumptions. Leveraging the exponential concentration, we prove a tight upper bound for the overall error of VM-CI. This, in turn, allows us to characterize the sample complexity of any constraint-based causal discovery algorithm that uses VM-CI for CI tests. To the best of our knowledge, this is the first sample complexity guarantee for causal discovery for continuous variables. Furthermore, we empirically show that VM-CI outperforms other popular CI tests in terms of either time or sample complexity (or both), which translates to a better performance in structure learning as well.
</details>
<details>
<summary>摘要</summary>
使用假设独立性测试为导向，我们研究非 Parametric Von Mises 估计器，用于测量多变量分布的熵。我们证明了这个估计器的快速凝固不等式。我们基于这个估计器设计了一个 Conditional Independence （CI）测试，称为 VM-CI，它在幂级假设下具有最佳参数化速率。利用快速凝固不等式，我们证明了 VM-CI 测试的总错误Bound。这使得我们可以计算任何基于约束的 causal discovery 算法的样本复杂度。根据我们所知，这是首次对连续变量 causal discovery 提供样本复杂度保证。此外，我们还 empirically 表明，VM-CI 测试在时间或样本复杂度（或 Both）方面比其他流行的 CI 测试更高效，这也意味着它在结构学习方面表现更好。Note:* "独立性测试" (conditional independence test) is translated as "独立性测试" in Simplified Chinese.* "假设独立性" (conditional independence) is translated as "假设独立性" in Simplified Chinese.* "熵" (entropy) is translated as "熵" in Simplified Chinese.* "估计器" (estimator) is translated as "估计器" in Simplified Chinese.* "快速凝固不等式" (exponential concentration inequality) is translated as "快速凝固不等式" in Simplified Chinese.* "Conditional Independence 测试" (CI test) is translated as "Conditional Independence 测试" in Simplified Chinese.* "约束" (constraint) is translated as "约束" in Simplified Chinese.* "causal discovery" is translated as " causal discovery" in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Provable-Benefits-of-Multi-task-RL-under-Non-Markovian-Decision-Making-Processes"><a href="#Provable-Benefits-of-Multi-task-RL-under-Non-Markovian-Decision-Making-Processes" class="headerlink" title="Provable Benefits of Multi-task RL under Non-Markovian Decision Making Processes"></a>Provable Benefits of Multi-task RL under Non-Markovian Decision Making Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13550">http://arxiv.org/abs/2310.13550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruiquan Huang, Yuan Cheng, Jing Yang, Vincent Tan, Yingbin Liang</li>
<li>for: 多任务束规划学习（RL）下的Markov决策过程（MDP）中存在共享幽默结构可以获得显著的采样效率提升，本文探索这种优势是否可以扩展到更一般的顺序决策问题，例如部分可观察MDP（POMDP）和更一般的预测状态表示（PSR）。</li>
<li>methods: 本文使用联合模型类来描述任务，并使用$\eta$-括号数量来衡量其复杂度，这个数量也用于量化任务之间的相似性，从而确定多任务学习是否具有优势。</li>
<li>results: 本文提出了一种可求最优策略的算法UMT-PSR，并证明在所有PSRs中找到近似优化策略的执行是可靠的。此外，本文还提供了一些具有小$\eta$-括号数量的多任务PSR示例，这些示例可以充分利用多任务学习的优势。最后，本文还研究了下游学习，即通过对已知任务的学习来学习一个新的目标任务，并证明可以通过利用已知PSRs来实现高效的学习。<details>
<summary>Abstract</summary>
In multi-task reinforcement learning (RL) under Markov decision processes (MDPs), the presence of shared latent structures among multiple MDPs has been shown to yield significant benefits to the sample efficiency compared to single-task RL. In this paper, we investigate whether such a benefit can extend to more general sequential decision making problems, such as partially observable MDPs (POMDPs) and more general predictive state representations (PSRs). The main challenge here is that the large and complex model space makes it hard to identify what types of common latent structure of multi-task PSRs can reduce the model complexity and improve sample efficiency. To this end, we posit a joint model class for tasks and use the notion of $\eta$-bracketing number to quantify its complexity; this number also serves as a general metric to capture the similarity of tasks and thus determines the benefit of multi-task over single-task RL. We first study upstream multi-task learning over PSRs, in which all tasks share the same observation and action spaces. We propose a provably efficient algorithm UMT-PSR for finding near-optimal policies for all PSRs, and demonstrate that the advantage of multi-task learning manifests if the joint model class of PSRs has a smaller $\eta$-bracketing number compared to that of individual single-task learning. We also provide several example multi-task PSRs with small $\eta$-bracketing numbers, which reap the benefits of multi-task learning. We further investigate downstream learning, in which the agent needs to learn a new target task that shares some commonalities with the upstream tasks via a similarity constraint. By exploiting the learned PSRs from the upstream, we develop a sample-efficient algorithm that provably finds a near-optimal policy.
</details>
<details>
<summary>摘要</summary>
在多任务强化学习（RL）下，存在共享隐藏结构的多任务Markov决策过程（MDP）可以提供显著的样本效率提升。在这篇论文中，我们研究了这种优势是否可以扩展到更通用的序列决策问题，如部分可观察MDP（POMDP）和更通用的预测状态表示（PSR）。主要挑战在于，由于模型空间的大小和复杂性，很难确定共享隐藏结构多任务PSR可以降低模型复杂性并提高样本效率。为此，我们提出了一个共同模型类，并使用η-括号数量来衡量其复杂性；这个数量也用于捕捉任务的相似性，从而确定多任务RL的优势。我们首先研究了上游多任务学习，在其中所有任务共享同一个观察和动作空间。我们提出了一个可证fficient的算法UMT-PSR，可以在所有PSR上找到近似优化策略，并证明多任务学习的优势在joint模型类的η-括号数量较小时manifests。我们还提供了一些具有小η-括号数量的多任务PSR示例，这些示例能够利用多任务学习的优势。我们进一步调查下游学习，在其中Agent需要通过一个相似性约束学习一个新的目标任务，该任务与上游任务共享一些共同特征。通过利用已经学习的PSR，我们开发了一种可证fficient的算法，可以在样本效率上提高。
</details></li>
</ul>
<hr>
<h2 id="Feature-Selection-and-Hyperparameter-Fine-tuning-in-Artificial-Neural-Networks-for-Wood-Quality-Classification"><a href="#Feature-Selection-and-Hyperparameter-Fine-tuning-in-Artificial-Neural-Networks-for-Wood-Quality-Classification" class="headerlink" title="Feature Selection and Hyperparameter Fine-tuning in Artificial Neural Networks for Wood Quality Classification"></a>Feature Selection and Hyperparameter Fine-tuning in Artificial Neural Networks for Wood Quality Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13490">http://arxiv.org/abs/2310.13490</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mateus Roder, Leandro Aparecido Passos, João Paulo Papa, André Luis Debiaso Rossi</li>
<li>for: 这篇论文的目的是提出一种可行的机器学习方法来解决木板质量评估问题，以替代人工操作。</li>
<li>methods: 这篇论文使用人工神经网络（ANN）进行模型训练，并同时调整模型的超参数和选择特征集。</li>
<li>results: 实验结果表明，在不同特征集和超参数配置下，模型的预测性能有很大差异。在一些情况下，只进行特征选择或超参数调整可以达到最佳预测性能。<details>
<summary>Abstract</summary>
Quality classification of wood boards is an essential task in the sawmill industry, which is still usually performed by human operators in small to median companies in developing countries. Machine learning algorithms have been successfully employed to investigate the problem, offering a more affordable alternative compared to other solutions. However, such approaches usually present some drawbacks regarding the proper selection of their hyperparameters. Moreover, the models are susceptible to the features extracted from wood board images, which influence the induction of the model and, consequently, its generalization power. Therefore, in this paper, we investigate the problem of simultaneously tuning the hyperparameters of an artificial neural network (ANN) as well as selecting a subset of characteristics that better describes the wood board quality. Experiments were conducted over a private dataset composed of images obtained from a sawmill industry and described using different feature descriptors. The predictive performance of the model was compared against five baseline methods as well as a random search, performing either ANN hyperparameter tuning and feature selection. Experimental results suggest that hyperparameters should be adjusted according to the feature set, or the features should be selected considering the hyperparameter values. In summary, the best predictive performance, i.e., a balanced accuracy of $0.80$, was achieved in two distinct scenarios: (i) performing only feature selection, and (ii) performing both tasks concomitantly. Thus, we suggest that at least one of the two approaches should be considered in the context of industrial applications.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>钢板质量分类是锯木行业中的一项重要任务，通常由人工操作员在发展中国家的小到中型公司中完成。机器学习算法已成功应用于这个问题，提供了更加可持预算的解决方案。然而，这些方法通常存在一些有关选择hyperparameter的缺点。此外，模型受到木板图像特征的影响，这会影响模型的泛化能力。因此，在这篇论文中，我们 investigate了同时调整ANN的hyperparameter和选择一个更好地描述木板质量的特征集。实验在一个私有的数据集上进行，该数据集包括锯木行业中的图像，并使用不同的特征描述器。模型预测性能与五个基线方法以及随机搜索进行比较。实验结果表明，hyperparameter应该与特征集进行调整，或者选择特征集应该考虑到hyperparameter值。简要总结，最佳预测性能，即平均准确率0.80，在两个不同的方案下实现：（1）只进行特征选择，和（2）同时进行特征选择和hyperparameter调整。因此，我们建议在工业应用中至少考虑一种这两种方法。
</details></li>
</ul>
<hr>
<h2 id="Personalized-identification-prediction-and-stimulation-of-neural-oscillations-via-data-driven-models-of-epileptic-network-dynamics"><a href="#Personalized-identification-prediction-and-stimulation-of-neural-oscillations-via-data-driven-models-of-epileptic-network-dynamics" class="headerlink" title="Personalized identification, prediction, and stimulation of neural oscillations via data-driven models of epileptic network dynamics"></a>Personalized identification, prediction, and stimulation of neural oscillations via data-driven models of epileptic network dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13480">http://arxiv.org/abs/2310.13480</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tena Dubcek, Debora Ledergerber, Jana Thomann, Giovanna Aiello, Marc Serra-Garcia, Lukas Imbach, Rafael Polania</li>
<li>for: 这篇论文的目的是为了提供一个基于EEG数据的个体化预测模型，以评估和预测脑动力疾病的疗效。</li>
<li>methods: 这篇论文使用了EEG数据，通过分析脑律动的快速对频和对频几何，提取了个体化的预测模型。</li>
<li>results: 这篇论文的结果显示，透过这个个体化预测模型，可以对脑动力疾病的疗效进行预测和评估。此外，这个模型还可以显示， periodic brain stimulation 可以导向疾病脑动力状态转化为健康的脑动力状态。<details>
<summary>Abstract</summary>
Neural oscillations are considered to be brain-specific signatures of information processing and communication in the brain. They also reflect pathological brain activity in neurological disorders, thus offering a basis for diagnoses and forecasting. Epilepsy is one of the most common neurological disorders, characterized by abnormal synchronization and desynchronization of the oscillations in the brain. About one third of epilepsy cases are pharmacoresistant, and as such emphasize the need for novel therapy approaches, where brain stimulation appears to be a promising therapeutic option. The development of brain stimulation paradigms, however, is often based on generalized assumptions about brain dynamics, although it is known that significant differences occur between patients and brain states. We developed a framework to extract individualized predictive models of epileptic network dynamics directly from EEG data. The models are based on the dominant coherent oscillations and their dynamical coupling, thus combining an established interpretation of dynamics through neural oscillations, with accurate patient-specific features. We show that it is possible to build a direct correspondence between the models of brain-network dynamics under periodic driving, and the mechanism of neural entrainment via periodic stimulation. When our framework is applied to EEG recordings of patients in status epilepticus (a brain state of perpetual seizure activity), it yields a model-driven predictive analysis of the therapeutic performance of periodic brain stimulation. This suggests that periodic brain stimulation can drive pathological states of epileptic network dynamics towards a healthy functional brain state.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese translation:神经振荡被视为脑特有的信息处理和通信特征。它们也反映了脑神经疾病的病理活动，因此提供了诊断和预测的基础。癫痫是最常见的神经疾病之一，表现为脑内oscillation的异常同步和反同步。约一 third的癫痫患者无法由药物控制，这重申了需要新的治疗方法，其中脑刺激似乎是一个有前途的治疗选择。然而，脑刺激模式的开发 frequently based on general assumptions about brain dynamics，尽管已知脑部和脑状态之间存在显著的差异。我们提出了一个框架，可以直接从EEG数据提取个性化预测模型。这些模型基于主要的同步振荡和其动态相互作用，因此结合了已知的神经振荡解释和准确的患者特有特征。我们显示了 periodic driving下brain-network dynamics的模型和神经整合的机制之间存在直接对应关系。当我们的框架应用于 Status epilepticus（脑状态）的EEG记录时，可以获得一种基于模型的预测分析，以评估脑刺激的治疗性能。这表明 periodic brain stimulation可以驱动癫痫网络动态向具有健康功能脑状态。
</details></li>
</ul>
<hr>
<h2 id="An-Analysis-of-D-α-seeding-for-k-means"><a href="#An-Analysis-of-D-α-seeding-for-k-means" class="headerlink" title="An Analysis of $D^α$ seeding for $k$-means"></a>An Analysis of $D^α$ seeding for $k$-means</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13474">http://arxiv.org/abs/2310.13474</a></li>
<li>repo_url: None</li>
<li>paper_authors: Etienne Bamas, Sai Ganesh Nagarajan, Ola Svensson</li>
<li>for: 这个论文的目的是提供关于$D^\alpha$ 种子算法（也称为$k$-means++ 算法）的一种理解，以及证明其在标准 $k$-means 成本上的近似因子。</li>
<li>methods: 该论文使用了$D^\alpha$ 种子算法，并对其进行了分析和证明。</li>
<li>results: 论文证明了，对于任何 $\alpha&gt;2$，$D^\alpha$ 种子算法在标准 $k$-means 成本上提供了一个 $O_\alpha((g_\alpha)^{2&#x2F;\alpha}\cdot \left(\frac{\sigma_{\mathrm{max}}{\sigma_{\mathrm{min}}\right)^{2-4&#x2F;\alpha}\cdot (\min{\ell,\log k})^{2&#x2F;\alpha})$ 的近似因子。此外，论文还提供了一些下界，证明了这个近似因子的依赖关系。最后，论文还提供了一些实验证明，证明了 $\alpha&gt;2$ 可以提高 $k$-means 成本，并且这种优势可以在 Lloyd 算法后仍保持。<details>
<summary>Abstract</summary>
One of the most popular clustering algorithms is the celebrated $D^\alpha$ seeding algorithm (also know as $k$-means++ when $\alpha=2$) by Arthur and Vassilvitskii (2007), who showed that it guarantees in expectation an $O(2^{2\alpha}\cdot \log k)$-approximate solution to the ($k$,$\alpha$)-means cost (where euclidean distances are raised to the power $\alpha$) for any $\alpha\ge 1$. More recently, Balcan, Dick, and White (2018) observed experimentally that using $D^\alpha$ seeding with $\alpha>2$ can lead to a better solution with respect to the standard $k$-means objective (i.e. the $(k,2)$-means cost).   In this paper, we provide a rigorous understanding of this phenomenon. For any $\alpha>2$, we show that $D^\alpha$ seeding guarantees in expectation an approximation factor of $$ O_\alpha \left((g_\alpha)^{2/\alpha}\cdot \left(\frac{\sigma_{\mathrm{max}}{\sigma_{\mathrm{min}}\right)^{2-4/\alpha}\cdot (\min\{\ell,\log k\})^{2/\alpha}\right)$$ with respect to the standard $k$-means cost of any underlying clustering; where $g_\alpha$ is a parameter capturing the concentration of the points in each cluster, $\sigma_{\mathrm{max}$ and $\sigma_{\mathrm{min}$ are the maximum and minimum standard deviation of the clusters around their means, and $\ell$ is the number of distinct mixing weights in the underlying clustering (after rounding them to the nearest power of $2$). We complement these results by some lower bounds showing that the dependency on $g_\alpha$ and $\sigma_{\mathrm{max}/\sigma_{\mathrm{min}$ is tight.   Finally, we provide an experimental confirmation of the effects of the aforementioned parameters when using $D^\alpha$ seeding. Further, we corroborate the observation that $\alpha>2$ can indeed improve the $k$-means cost compared to $D^2$ seeding, and that this advantage remains even if we run Lloyd's algorithm after the seeding.
</details>
<details>
<summary>摘要</summary>
一种非常受欢迎的聚类算法是由Arthur和Vassilvitskii（2007）提出的$D^\alpha$ 种子算法（当$\alpha=2$时也称为$k$-means++），它保证了在期望下有$O(2^{2\alpha}\cdot \log k)$-近似解决($k$,$\alpha$)-means成本（在欧几丁度距离上升到$\alpha$势），其中$\alpha\geq 1$。更 reciently，Balcan、Dick和White（2018）通过实验发现，使用$D^\alpha$种子算法可以在标准$k$-means目标上提供更好的解决方案，特别是当$\alpha>2$时。在这篇论文中，我们提供了一个充分理解这种现象的理论基础。对于任何$\alpha>2$，我们证明了$D^\alpha$种子算法在期望下对标准$k$-means成本的近似因子为：$$O_\alpha \left((g_\alpha)^{2/\alpha}\cdot \left(\frac{\sigma_{\mathrm{max}}{\sigma_{\mathrm{min}}\right)^{2-4/\alpha}\cdot (\min\{\ell,\log k\})^{2/\alpha}\right)$$其中$g_\alpha$是聚集点的含量，$\sigma_{\mathrm{max}$和$\sigma_{\mathrm{min}$是聚集点的最大和最小横幅，$\ell$是聚集点的数量。我们还提供了一些下界，证明了$g_\alpha$和$\sigma_{\mathrm{max}/\sigma_{\mathrm{min}$的依赖性是紧的。最后，我们通过实验证明了$D^\alpha$种子算法中各参数的效果，并证明了$\alpha>2$可以在标准$k$-means成本上提供更好的解决方案，而且这种优势仍然存在，即使在使用Lloyd算法之后。
</details></li>
</ul>
<hr>
<h2 id="Stable-Nonconvex-Nonconcave-Training-via-Linear-Interpolation"><a href="#Stable-Nonconvex-Nonconcave-Training-via-Linear-Interpolation" class="headerlink" title="Stable Nonconvex-Nonconcave Training via Linear Interpolation"></a>Stable Nonconvex-Nonconcave Training via Linear Interpolation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13459">http://arxiv.org/abs/2310.13459</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Pethick, Wanyun Xie, Volkan Cevher</li>
<li>for: 这篇论文提出了一种理论分析，用于稳定（大规模）神经网络训练的线性 interpolate 方法。文章认为，优化过程中的不稳定性往往是因为损失函数的非升 monotonicity，并示了如何通过使用 nonexpansive 算子理论来利用 linear interpolate。</li>
<li>methods: 文章提出了一种新的优化方案，叫做 relaxed approximate proximal point (RAPP)，它是第一个可以达到最后迭代速度的方法，适用于整个 cohypomonotone 问题范围。此外，文章还扩展了 RAPP 方法，使其适用于约束和规范化 Setting。通过将内部优化器换成 Lookahead 算法，文章还重新发现了 Lookahead 算法家族，并证明了它们在 cohypomonotone 问题中的 convergence。</li>
<li>results: 文章通过实验示范了在生成对抗网络中的应用，证明了 linear interpolate 的利用带来的 benefits。<details>
<summary>Abstract</summary>
This paper presents a theoretical analysis of linear interpolation as a principled method for stabilizing (large-scale) neural network training. We argue that instabilities in the optimization process are often caused by the nonmonotonicity of the loss landscape and show how linear interpolation can help by leveraging the theory of nonexpansive operators. We construct a new optimization scheme called relaxed approximate proximal point (RAPP), which is the first explicit method to achieve last iterate convergence rates for the full range of cohypomonotone problems. The construction extends to constrained and regularized settings. By replacing the inner optimizer in RAPP we rediscover the family of Lookahead algorithms for which we establish convergence in cohypomonotone problems even when the base optimizer is taken to be gradient descent ascent. The range of cohypomonotone problems in which Lookahead converges is further expanded by exploiting that Lookahead inherits the properties of the base optimizer. We corroborate the results with experiments on generative adversarial networks which demonstrates the benefits of the linear interpolation present in both RAPP and Lookahead.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "linear interpolation" is translated as "线性 interpolate" (língxìng jìshì)* "nonmonotonicity" is translated as "非 monotonicity" (fēi mónótónicity)* "cohypon monotone" is translated as "共凹 monotone" (gòng cháng mónótón)* "relaxed approximate proximal point" is translated as "松relaxed approximate proximal point" (sōngxiǎoxiǎo jìshì)* "last iterate convergence rates" is translated as "最后迭代收敛率" (zuìhòu dài tiědài shōu yè)* "base optimizer" is translated as "基础优化器" (jībì yóu jiā)* "Lookahead algorithms" is translated as "Lookahead算法" (Lookahead suān fǎ)Please note that the translation is based on the Simplified Chinese version of the text, and the translation may vary depending on the specific context and the version of the text.
</details></li>
</ul>
<hr>
<h2 id="Correspondence-learning-between-morphologically-different-robots-through-task-demonstrations"><a href="#Correspondence-learning-between-morphologically-different-robots-through-task-demonstrations" class="headerlink" title="Correspondence learning between morphologically different robots through task demonstrations"></a>Correspondence learning between morphologically different robots through task demonstrations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13458">http://arxiv.org/abs/2310.13458</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hakan Aktas, Yukie Nagai, Minoru Asada, Erhan Oztop, Emre Ugur<br>for:本文旨在学习不同机器人之间的对应关系，以便将一个机器人学习的技能转移到另一个机器人上。methods:本文提出了一种方法，通过让两个机器人执行相同的任务，形成共同的隐藏表示。然后，通过观察一个机器人执行任务，生成另一个机器人所需的隐藏空间表示。results:本文通过实验和证明，证明了该方法可以成功地学习机器人之间的对应关系。在不同的任务和 trajectory 上，该方法能够将一个机器人学习的技能转移到另一个机器人上。<details>
<summary>Abstract</summary>
We observe a large variety of robots in terms of their bodies, sensors, and actuators. Given the commonalities in the skill sets, teaching each skill to each different robot independently is inefficient and not scalable when the large variety in the robotic landscape is considered. If we can learn the correspondences between the sensorimotor spaces of different robots, we can expect a skill that is learned in one robot can be more directly and easily transferred to the other robots. In this paper, we propose a method to learn correspondences between robots that have significant differences in their morphologies: a fixed-based manipulator robot with joint control and a differential drive mobile robot. For this, both robots are first given demonstrations that achieve the same tasks. A common latent representation is formed while learning the corresponding policies. After this initial learning stage, the observation of a new task execution by one robot becomes sufficient to generate a latent space representation pertaining to the other robot to achieve the same task. We verified our system in a set of experiments where the correspondence between two simulated robots is learned (1) when the robots need to follow the same paths to achieve the same task, (2) when the robots need to follow different trajectories to achieve the same task, and (3) when complexities of the required sensorimotor trajectories are different for the robots considered. We also provide a proof-of-the-concept realization of correspondence learning between a real manipulator robot and a simulated mobile robot.
</details>
<details>
<summary>摘要</summary>
我们观察到多种机器人在体型、感知器和 actuator 方面存在差异。由于 robotic 领域中机器人的多样性，单独教学每个机器人的技能不可能scalable。如果我们可以学习不同机器人感知动作空间之间的对应关系，那么我们可以预期一个学习在一个机器人上的技能可以更直接和更容易地转移到另一个机器人。在这篇论文中，我们提出了一种方法，可以在机器人之间学习对应关系，其中机器人之间存在重大差异。为此，我们首先给两个机器人提供了完成同一任务的示范。在学习初始阶段，我们形成了共同的潜在表示。然后，当一个机器人执行新任务时， observing 它的行为就能够生成另一个机器人所需的潜在空间表示，以便完成同一任务。我们在一系列实验中验证了我们的系统，其中包括（1）两个机器人需要跟踪同一条路来完成同一任务，（2）两个机器人需要跟踪不同的路径来完成同一任务，以及（3）两个机器人所需的感知动作轨迹之间存在差异。此外，我们还提供了一个实际实现对应学习的真实搅拌机器人和虚拟移动机器人之间的对应关系。
</details></li>
</ul>
<hr>
<h2 id="Y-Diagonal-Couplings-Approximating-Posteriors-with-Conditional-Wasserstein-Distances"><a href="#Y-Diagonal-Couplings-Approximating-Posteriors-with-Conditional-Wasserstein-Distances" class="headerlink" title="Y-Diagonal Couplings: Approximating Posteriors with Conditional Wasserstein Distances"></a>Y-Diagonal Couplings: Approximating Posteriors with Conditional Wasserstein Distances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13433">http://arxiv.org/abs/2310.13433</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jannis Chemseddine, Paul Hagemann, Christian Wald</li>
<li>for: 本研究探讨了在逆问题中使用 conditional Wasserstein distance 来 aproximate posterior distribution。</li>
<li>methods: 本研究使用了一种叫做 conditional Wasserstein distance 的方法，该方法使用一组 restriction couplings 来等价 posterior measure。</li>
<li>results: 本研究发现，使用 conditional Wasserstein distance 可以获得更好的 posterior sampling 性能，并且在某些条件下，vanilla Wasserstein distance 和 conditional Wasserstein distance 相同。<details>
<summary>Abstract</summary>
In inverse problems, many conditional generative models approximate the posterior measure by minimizing a distance between the joint measure and its learned approximation. While this approach also controls the distance between the posterior measures in the case of the Kullback Leibler divergence, it does not hold true for the Wasserstein distance. We will introduce a conditional Wasserstein distance with a set of restricted couplings that equals the expected Wasserstein distance of the posteriors. By deriving its dual, we find a rigorous way to motivate the loss of conditional Wasserstein GANs. We outline conditions under which the vanilla and the conditional Wasserstein distance coincide. Furthermore, we will show numerical examples where training with the conditional Wasserstein distance yields favorable properties for posterior sampling.
</details>
<details>
<summary>摘要</summary>
在逆 проблеme中，许多条件生成模型通过控制 JOIN 度和其学习的近似关系来估算 posterior measure。虽然这种方法在 Kullback Leibler 距离下也控制 posterior measures，但并不适用于 Wasserstein 距离。我们将引入一种 conditional Wasserstein 距离，其等于 posterior 的预期 Wasserstein 距离。通过它的 dual，我们找到了 condition Wasserstein GANs 的损失的准确优化方法。我们还详细介绍了这些方法在 vanilla 和 conditional Wasserstein 距离之间的差异，以及训练时使用 conditional Wasserstein 距离的有利属性。Here's the translation in Traditional Chinese:在逆问题中，许多条件生成模型通过控制 JOIN 度和其学习的近似关系来估算 posterior measure。处于 Kullback Leibler 距离下也控制 posterior measures，但并不适用于 Wasserstein 距离。我们将引入一种 conditional Wasserstein 距离，其等于 posterior 的预期 Wasserstein 距离。通过它的 dual，我们找到了 condition Wasserstein GANs 的损失的准确优化方法。我们还详细介绍了这些方法在 vanilla 和 conditional Wasserstein 距离之间的差异，以及训练时使用 conditional Wasserstein 距离的有利属性。
</details></li>
</ul>
<hr>
<h2 id="HRTF-Interpolation-using-a-Spherical-Neural-Process-Meta-Learner"><a href="#HRTF-Interpolation-using-a-Spherical-Neural-Process-Meta-Learner" class="headerlink" title="HRTF Interpolation using a Spherical Neural Process Meta-Learner"></a>HRTF Interpolation using a Spherical Neural Process Meta-Learner</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13430">http://arxiv.org/abs/2310.13430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Etienne Thuillier, Craig Jin, Vesa Välimäki<br>for:* The paper aims to estimate a subject’s Head-Related Transfer Function (HRTF) using convenient input modalities such as anthropometric measurements or pinnae photographs.methods:* The paper proposes a Convolutional Conditional Neural Process meta-learner specialized in HRTF error interpolation, which includes a Spherical Convolutional Neural Network component and exploits potential symmetries between the HRTF’s left and right channels.results:* The proposed model achieves up to 3 dB relative error reduction compared to state-of-the-art interpolation methods, with a reduction in the data point count required to achieve comparable accuracy from 50 to 28 points. Additionally, the trained model provides well-calibrated uncertainty estimates.<details>
<summary>Abstract</summary>
Several individualization methods have recently been proposed to estimate a subject's Head-Related Transfer Function (HRTF) using convenient input modalities such as anthropometric measurements or pinnae photographs. There exists a need for adaptively correcting the estimation error committed by such methods using a few data point samples from the subject's HRTF, acquired using acoustic measurements or perceptual feedback. To this end, we introduce a Convolutional Conditional Neural Process meta-learner specialized in HRTF error interpolation. In particular, the model includes a Spherical Convolutional Neural Network component to accommodate the spherical geometry of HRTF data. It also exploits potential symmetries between the HRTF's left and right channels about the median axis. In this work, we evaluate the proposed model's performance purely on time-aligned spectrum interpolation grounds under a simplified setup where a generic population-mean HRTF forms the initial estimates prior to corrections instead of individualized ones. The trained model achieves up to 3 dB relative error reduction compared to state-of-the-art interpolation methods despite being trained using only 85 subjects. This improvement translates up to nearly a halving of the data point count required to achieve comparable accuracy, in particular from 50 to 28 points to reach an average of -20 dB relative error per interpolated feature. Moreover, we show that the trained model provides well-calibrated uncertainty estimates. Accordingly, such estimates can inform the sequential decision problem of acquiring as few correcting HRTF data points as needed to meet a desired level of HRTF individualization accuracy.
</details>
<details>
<summary>摘要</summary>
几种个性化方法已经最近提出来估算用户的头相关传函数（HRTF），使用便捷的输入模式，如人体测量或耳朵照片。存在一个需要使用一些数据点样本来修正估算错误的需求。为此，我们介绍了一种适应HRTF错误 interpolator。特别是，该模型包括一个圆拟神经网络组件，以便处理HRTF数据的圆形几何。它还利用HRTF左右通道关于中心轴的可能 symmetries。在这种简化的设置下，我们评估了提posed模型的性能，即使只使用85名用户进行训练。结果显示，训练后的模型可以将相对错误降低至3dB，相比之前的状态 искусство interpolating方法。此外，我们还证明了训练后的模型可以提供准确的不确定性估计。因此，这些估计可以为sequential decision问题提供指导，即需要收集多少个正确的HRTF数据点以达到满意的个性化精度。
</details></li>
</ul>
<hr>
<h2 id="BRFL-A-Blockchain-based-Byzantine-Robust-Federated-Learning-Model"><a href="#BRFL-A-Blockchain-based-Byzantine-Robust-Federated-Learning-Model" class="headerlink" title="BRFL: A Blockchain-based Byzantine-Robust Federated Learning Model"></a>BRFL: A Blockchain-based Byzantine-Robust Federated Learning Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13403">http://arxiv.org/abs/2310.13403</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Li, Chunhe Xia, Chang Li, Tianbo Wang</li>
<li>for: 这篇论文旨在提出一个基于区块链技术的联邦学习模型，以提高该模型对于伪造模型的抵抗力。</li>
<li>methods: 本文使用了联邦学习和区块链技术的结合，实现了模型伪造追踪和地方训练Client的优化。特别是，在数据联邦时，选择了基于对应关系的数据范围，并使用了气泡聚类和平均梯度计算，以验证模型的准确性。</li>
<li>results: 实验结果显示，该模型在公开数据上显示了比基于其他基准的伪造抵抗方法更高的抵抗性。此外，模型还能够降低联邦学习的资源消耗问题。<details>
<summary>Abstract</summary>
With the increasing importance of machine learning, the privacy and security of training data have become critical. Federated learning, which stores data in distributed nodes and shares only model parameters, has gained significant attention for addressing this concern. However, a challenge arises in federated learning due to the Byzantine Attack Problem, where malicious local models can compromise the global model's performance during aggregation. This article proposes the Blockchain-based Byzantine-Robust Federated Learning (BRLF) model that combines federated learning with blockchain technology. This integration enables traceability of malicious models and provides incentives for locally trained clients. Our approach involves selecting the aggregation node based on Pearson's correlation coefficient, and we perform spectral clustering and calculate the average gradient within each cluster, validating its accuracy using local dataset of the aggregation nodes. Experimental results on public datasets demonstrate the superior byzantine robustness of our secure aggregation algorithm compared to other baseline byzantine robust aggregation methods, and proved our proposed model effectiveness in addressing the resource consumption problem.
</details>
<details>
<summary>摘要</summary>
随着机器学习的重要性增加，训练数据的隐私和安全问题日益突出。联邦学习，即将数据存储在分布式节点上并只分享模型参数，已经吸引了广泛关注以Addressing这个问题。然而，联邦学习中出现了拜占庭攻击问题，其中有些本地模型会在聚合过程中损害全局模型的性能。这篇文章提出了基于区块链技术的联邦学习模型（BRLF），该模型结合了联邦学习和区块链技术。这种结合使得可以追溯到恶意模型，并为本地训练节点提供了奖励。我们的方法是根据潘森相关系数选择聚合节点，并对每个群进行spectral clustering，计算每个群的平均梯度，并验证其准确性使用本地数据集。实验结果表明，我们的安全聚合算法在其他基准拜占庭Robust聚合方法的比较中显示出了更高的拜占庭Robust性，并证明了我们提出的模型的有效性。
</details></li>
</ul>
<hr>
<h2 id="Calibrating-Neural-Simulation-Based-Inference-with-Differentiable-Coverage-Probability"><a href="#Calibrating-Neural-Simulation-Based-Inference-with-Differentiable-Coverage-Probability" class="headerlink" title="Calibrating Neural Simulation-Based Inference with Differentiable Coverage Probability"></a>Calibrating Neural Simulation-Based Inference with Differentiable Coverage Probability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13402">http://arxiv.org/abs/2310.13402</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dmml-geneva/calibrated-posterior">https://github.com/dmml-geneva/calibrated-posterior</a></li>
<li>paper_authors: Maciej Falkiewicz, Naoya Takeishi, Imahn Shekhzadeh, Antoine Wehenkel, Arnaud Delaunoy, Gilles Louppe, Alexandros Kalousis</li>
<li>for: 这个论文主要用于提出一种基于神经网络的隐藏Variable Bayesian inference算法，以提高 posterior belief 的不确定性评估。</li>
<li>methods: 该算法使用了选择性的隐藏Variable Bayesian inference技术，并引入了一个抑制error的评估项直接到训练目标函数中。</li>
<li>results: 经验表明，该算法可以在六个 benchmark 问题上达到或超过前 exists 的方法的覆盖率和预期 posterior density 水平。<details>
<summary>Abstract</summary>
Bayesian inference allows expressing the uncertainty of posterior belief under a probabilistic model given prior information and the likelihood of the evidence. Predominantly, the likelihood function is only implicitly established by a simulator posing the need for simulation-based inference (SBI). However, the existing algorithms can yield overconfident posteriors (Hermans *et al.*, 2022) defeating the whole purpose of credibility if the uncertainty quantification is inaccurate. We propose to include a calibration term directly into the training objective of the neural model in selected amortized SBI techniques. By introducing a relaxation of the classical formulation of calibration error we enable end-to-end backpropagation. The proposed method is not tied to any particular neural model and brings moderate computational overhead compared to the profits it introduces. It is directly applicable to existing computational pipelines allowing reliable black-box posterior inference. We empirically show on six benchmark problems that the proposed method achieves competitive or better results in terms of coverage and expected posterior density than the previously existing approaches.
</details>
<details>
<summary>摘要</summary>
bayesian 推理允许表达基于概率模型的 posterior belief 中的不确定性，givem prior information 和证据的可能性。然而，现有的算法可能会导致过于自信的 posterior （Hermans *et al.*, 2022），这会让 credibility 失效，如果uncertainty quantification 不准确。我们提议直接在 neural model 的训练目标中包含 calibration 项。通过 relaxes classical 形式的 calibration error 的形式，我们可以实现 end-to-end backpropagation。该方法不仅可以应用于特定的 neural model，而且相比于它引入的计算开销，它带来了moderate的计算开销。它可以直接应用于现有的计算管道， allowing reliable black-box posterior inference。我们在六个 benchmark 问题上进行了实验，并证明了该方法可以达到与之前的方法相同或更好的coverage和预期 posterior density的结果。
</details></li>
</ul>
<hr>
<h2 id="Equivariant-Deep-Weight-Space-Alignment"><a href="#Equivariant-Deep-Weight-Space-Alignment" class="headerlink" title="Equivariant Deep Weight Space Alignment"></a>Equivariant Deep Weight Space Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13397">http://arxiv.org/abs/2310.13397</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aviv Navon, Aviv Shamsian, Ethan Fetaya, Gal Chechik, Nadav Dym, Haggai Maron</li>
<li>for: 深度网络的 permutation symmetries 使得模型平均化和相似性判断变得困难。这些问题的解决需要对深度网络的 weights 进行对齐。</li>
<li>methods: 我们提出了一种新的框架，即 Deep-Align，用于解决这些问题。我们首先证明 weight alignment 遵循两种基本的 symmetries，然后提出了一种深度架构，该架构遵循这些 symmetries。</li>
<li>results: 我们的实验结果表明，使用 Deep-Align 可以更快地生成更好的对齐，并且可以用作其他方法的初始化来获得更好的解决方案，并且可以带来显著的加速。<details>
<summary>Abstract</summary>
Permutation symmetries of deep networks make simple operations like model averaging and similarity estimation challenging. In many cases, aligning the weights of the networks, i.e., finding optimal permutations between their weights, is necessary. More generally, weight alignment is essential for a wide range of applications, from model merging, through exploring the optimization landscape of deep neural networks, to defining meaningful distance functions between neural networks. Unfortunately, weight alignment is an NP-hard problem. Prior research has mainly focused on solving relaxed versions of the alignment problem, leading to either time-consuming methods or sub-optimal solutions. To accelerate the alignment process and improve its quality, we propose a novel framework aimed at learning to solve the weight alignment problem, which we name Deep-Align. To that end, we first demonstrate that weight alignment adheres to two fundamental symmetries and then, propose a deep architecture that respects these symmetries. Notably, our framework does not require any labeled data. We provide a theoretical analysis of our approach and evaluate Deep-Align on several types of network architectures and learning setups. Our experimental results indicate that a feed-forward pass with Deep-Align produces better or equivalent alignments compared to those produced by current optimization algorithms. Additionally, our alignments can be used as an initialization for other methods to gain even better solutions with a significant speedup in convergence.
</details>
<details>
<summary>摘要</summary>
深度网络的 permutation symmetries 使得一些简单的操作，如模型平均和相似性估计，变得复杂。在许多情况下，对深度网络的Weight进行对齐，即找出最佳的 permutation  между它们的Weight，是必要的。更广泛地说，Weight对齐是许多应用的关键，从模型合并、探索深度神经网络的优化困难度到定义深度神经网络之间的意义ful distance function。 unfortunately，Weight对齐是NP-hard问题。先前的研究主要集中在解决Weight对齐问题的宽松版本上，导致 either time-consuming methods or suboptimal solutions。为了加速对齐过程并提高其质量，我们提出了一个新的框架，我们称之为Deep-Align。为达到这一目标，我们首先示出Weight对齐符合两种基本的Symmetries，然后我们提议一种尊重这些Symmetries的深度建筑。很notationably，我们的框架不需要任何标注数据。我们提供了对我们方法的理论分析，并在多种网络架构和学习设置下进行了实验性测试。我们的实验结果表明，在Deep-Align中通过 feed-forward pass 生成的对齐比现有的优化算法更好或相同。此外，我们的对齐可以作为其他方法的初始化，以获得更好的解决方案，并且具有显著的加速减速效果。
</details></li>
</ul>
<hr>
<h2 id="RL-X-A-Deep-Reinforcement-Learning-Library-not-only-for-RoboCup"><a href="#RL-X-A-Deep-Reinforcement-Learning-Library-not-only-for-RoboCup" class="headerlink" title="RL-X: A Deep Reinforcement Learning Library (not only) for RoboCup"></a>RL-X: A Deep Reinforcement Learning Library (not only) for RoboCup</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13396">http://arxiv.org/abs/2310.13396</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nico-bohlinger/rl-x">https://github.com/nico-bohlinger/rl-x</a></li>
<li>paper_authors: Nico Bohlinger, Klaus Dorer</li>
<li>for: 这篇论文是为了描述一个新的深度学习约束学习（DRL）库RL-X，以及其应用于RoboCup Soccer Simulation 3D League和经典DRL benchmark。</li>
<li>methods: RL-X使用自适应的JAX实现，可以 дости到与知名框架Stable-Baselines3相比的4.5倍加速。</li>
<li>results: RL-X可以在RoboCup Soccer Simulation 3D League和经典DRL benchmark上实现更高的性能。<details>
<summary>Abstract</summary>
This paper presents the new Deep Reinforcement Learning (DRL) library RL-X and its application to the RoboCup Soccer Simulation 3D League and classic DRL benchmarks. RL-X provides a flexible and easy-to-extend codebase with self-contained single directory algorithms. Through the fast JAX-based implementations, RL-X can reach up to 4.5x speedups compared to well-known frameworks like Stable-Baselines3.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了新的深度强化学习（DRL）库RL-X和其在RoboCup Soccer Simulation 3D League和经典DRL benchmarks中的应用。RL-X提供了灵活且易于扩展的代码基础，通过快速的JAX实现，RL-X可以与已知框架如Stable-Baselines3进行比较，达到4.5倍的速度提升。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Best-Arm-Identification-with-Fixed-Confidence-in-Restless-Bandits"><a href="#Optimal-Best-Arm-Identification-with-Fixed-Confidence-in-Restless-Bandits" class="headerlink" title="Optimal Best Arm Identification with Fixed Confidence in Restless Bandits"></a>Optimal Best Arm Identification with Fixed Confidence in Restless Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13393">http://arxiv.org/abs/2310.13393</a></li>
<li>repo_url: None</li>
<li>paper_authors: P. N. Karthik, Vincent Y. F. Tan, Arpan Mukherjee, Ali Tajer</li>
<li>for: 最佳臂标识（best arm identification）在不断时钟的多臂矢量游戏中（restless multi-armed bandit setting），具有 фиnitely many arms的情况。</li>
<li>methods: 使用 homogeneous Markov chain 模型（homogeneous Markov chain model）， captured by an ergodic transition probability matrix (TPM) that is a member of a single-parameter exponential family of TPMs。</li>
<li>results: 提出了一种策略（policy），其预期停止时间（expected stopping time）的增长率与下界（lower bound）匹配，并且证明了这种策略在极限下的错误probability（error probability）下逐渐 converges to the optimal policy。<details>
<summary>Abstract</summary>
We study best arm identification in a restless multi-armed bandit setting with finitely many arms. The discrete-time data generated by each arm forms a homogeneous Markov chain taking values in a common, finite state space. The state transitions in each arm are captured by an ergodic transition probability matrix (TPM) that is a member of a single-parameter exponential family of TPMs. The real-valued parameters of the arm TPMs are unknown and belong to a given space. Given a function $f$ defined on the common state space of the arms, the goal is to identify the best arm -- the arm with the largest average value of $f$ evaluated under the arm's stationary distribution -- with the fewest number of samples, subject to an upper bound on the decision's error probability (i.e., the fixed-confidence regime). A lower bound on the growth rate of the expected stopping time is established in the asymptote of a vanishing error probability. Furthermore, a policy for best arm identification is proposed, and its expected stopping time is proved to have an asymptotic growth rate that matches the lower bound. It is demonstrated that tracking the long-term behavior of a certain Markov decision process and its state-action visitation proportions are the key ingredients in analyzing the converse and achievability bounds. It is shown that under every policy, the state-action visitation proportions satisfy a specific approximate flow conservation constraint and that these proportions match the optimal proportions dictated by the lower bound under any asymptotically optimal policy. The prior studies on best arm identification in restless bandits focus on independent observations from the arms, rested Markov arms, and restless Markov arms with known arm TPMs. In contrast, this work is the first to study best arm identification in restless bandits with unknown arm TPMs.
</details>
<details>
<summary>摘要</summary>
我们研究了最佳臂标识在不平静多臂带刺设定中，其中每个臂生成了一个homogeneous Markov链，这个链的状态转移是由一个不知道的臂转移概率矩阵（TPM）捕捉，这个TPM是一个单参数的 exponential family 中的一员。臂的实际参数是未知的， belong to a given space。给定一个函数 $f$ 在臂的共同状态空间上定义，我们的目标是在最少样本数下确定最佳臂，即臂的stationary distribution下的最大平均值。在fixed-confidence regime下，我们提出了一个策略，并证明其预期停止时间的增长率与下界匹配。此外，我们还证明了跟踪臂的长期行为和状态-动作访问比例是分析下界和可达性下界的关键组成部分。我们表明，对于任何策略，状态-动作访问比例满足一个特定的approximate flow conservation constraint，这些比例与最佳策略下的下界匹配。与前一些研究不同的是，本研究是在 unknown arm TPMs 下进行最佳臂标识。
</details></li>
</ul>
<hr>
<h2 id="Music-Augmentation-and-Denoising-For-Peak-Based-Audio-Fingerprinting"><a href="#Music-Augmentation-and-Denoising-For-Peak-Based-Audio-Fingerprinting" class="headerlink" title="Music Augmentation and Denoising For Peak-Based Audio Fingerprinting"></a>Music Augmentation and Denoising For Peak-Based Audio Fingerprinting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13388">http://arxiv.org/abs/2310.13388</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deezer/musicFPaugment">https://github.com/deezer/musicFPaugment</a></li>
<li>paper_authors: Kamil Akesbi, Dorian Desblancs, Benjamin Martin</li>
<li>for: 这个论文目的是提高音频标识系统的精度和可靠性，特别是在噪音环境下。</li>
<li>methods: 论文提出了一种新的音频增强管道，通过模拟实际情况来加入噪音到音乐片断中，以提高音频标识系统的精度。此外，论文还提出了一种深度学习模型，用于从spectrogram中除除噪音 ком分量，以提高音频标识系统的性能。</li>
<li>results: 论文的实验结果表明，通过添加这些模型，可以提高常用的音频标识系统的准确率，即使在噪音环境下。<details>
<summary>Abstract</summary>
Audio fingerprinting is a well-established solution for song identification from short recording excerpts. Popular methods rely on the extraction of sparse representations, generally spectral peaks, and have proven to be accurate, fast, and scalable to large collections. However, real-world applications of audio identification often happen in noisy environments, which can cause these systems to fail. In this work, we tackle this problem by introducing and releasing a new audio augmentation pipeline that adds noise to music snippets in a realistic way, by stochastically mimicking real-world scenarios. We then propose and release a deep learning model that removes noisy components from spectrograms in order to improve peak-based fingerprinting systems' accuracy. We show that the addition of our model improves the identification performance of commonly used audio fingerprinting systems, even under noisy conditions.
</details>
<details>
<summary>摘要</summary>
音频指纹技术是已有的解决方案，可以从短音频片断中识别歌曲。流行的方法通常基于稀疏表示EXTRACTION，通常是 spectral peaks，并已经证明准确、快速和可扩展到大量收藏。然而，实际应用中的音频识别经常发生在噪音环境中，这会使这些系统失败。在这项工作中，我们解决这个问题，通过引入和发布一个新的音频增强管道，该管道在真实的场景下做出随机尝试，以模拟实际中的噪音。然后，我们提议并发布一种深度学习模型，可以从spectrogram中除掉噪声组件，以提高基于peak的音频指纹系统的准确性。我们表明，在噪音环境下，加入我们的模型可以提高通常使用的音频指纹系统的识别性能。
</details></li>
</ul>
<hr>
<h2 id="Assumption-violations-in-causal-discovery-and-the-robustness-of-score-matching"><a href="#Assumption-violations-in-causal-discovery-and-the-robustness-of-score-matching" class="headerlink" title="Assumption violations in causal discovery and the robustness of score matching"></a>Assumption violations in causal discovery and the robustness of score matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13387">http://arxiv.org/abs/2310.13387</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Montagna, Atalanti A. Mastakouri, Elias Eulig, Nicoletta Noceti, Lorenzo Rosasco, Dominik Janzing, Bryon Aragam, Francesco Locatello</li>
<li>for: This paper aims to evaluate the empirical performance of recent causal discovery methods on observational i.i.d. data with different background conditions, allowing for violations of the critical assumptions required by each selected approach.</li>
<li>methods: The paper uses score matching-based methods to recover the causal structure, which demonstrate surprising performance in the false positive and false negative rate of the inferred graph in challenging scenarios.</li>
<li>results: The paper provides theoretical insights into the performance of these methods and is the first effort to benchmark the stability of causal discovery algorithms with respect to the values of their hyperparameters.<details>
<summary>Abstract</summary>
When domain knowledge is limited and experimentation is restricted by ethical, financial, or time constraints, practitioners turn to observational causal discovery methods to recover the causal structure, exploiting the statistical properties of their data. Because causal discovery without further assumptions is an ill-posed problem, each algorithm comes with its own set of usually untestable assumptions, some of which are hard to meet in real datasets. Motivated by these considerations, this paper extensively benchmarks the empirical performance of recent causal discovery methods on observational i.i.d. data generated under different background conditions, allowing for violations of the critical assumptions required by each selected approach. Our experimental findings show that score matching-based methods demonstrate surprising performance in the false positive and false negative rate of the inferred graph in these challenging scenarios, and we provide theoretical insights into their performance. This work is also the first effort to benchmark the stability of causal discovery algorithms with respect to the values of their hyperparameters. Finally, we hope this paper will set a new standard for the evaluation of causal discovery methods and can serve as an accessible entry point for practitioners interested in the field, highlighting the empirical implications of different algorithm choices.
</details>
<details>
<summary>摘要</summary>
当域知识有限，实验受到伦理、金融或时间约束，实践者会选择观察型 causal 发现方法来恢复 causal 结构，利用数据的统计特性。因为无其他假设的 causal 发现是一个不定 пробле  space，每种算法都来自其自己的集合不测试的假设，其中一些在实际数据中很难满足。驱动这些考虑，这篇论文对 recent causal discovery 方法进行了广泛的 benchmarking，使用了不同背景条件下的 observational i.i.d. 数据，允许假设满足不同方法的必要条件。我们的实验结果表明，使用 score matching 基于方法可以在这些复杂的场景中具有高度的 false positive 和 false negative 率，我们还提供了理论分析。这项工作还是首次对 causal discovery 算法的稳定性进行了 benchmarking，并且希望这篇论文可以设置一个新的标准 для causal discovery 方法的评估，并且可以为参与这个领域的实践者提供可访问的入门点，强调不同算法选择的 empirical 影响。
</details></li>
</ul>
<hr>
<h2 id="Salted-Inference-Enhancing-Privacy-while-Maintaining-Efficiency-of-Split-Inference-in-Mobile-Computing"><a href="#Salted-Inference-Enhancing-Privacy-while-Maintaining-Efficiency-of-Split-Inference-in-Mobile-Computing" class="headerlink" title="Salted Inference: Enhancing Privacy while Maintaining Efficiency of Split Inference in Mobile Computing"></a>Salted Inference: Enhancing Privacy while Maintaining Efficiency of Split Inference in Mobile Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13384">http://arxiv.org/abs/2310.13384</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dr-bell/salted-dnns">https://github.com/dr-bell/salted-dnns</a></li>
<li>paper_authors: Mohammad Malekzadeh, Fahim Kawsar</li>
<li>for: 这篇研究旨在提供一种控制深度神经网络（DNN）输出semantic interpretations的方法，以保持精度和效率，并满足在执行时间点网络的资料隐私和计算效率要求。</li>
<li>methods: 本研究使用了“Salted DNNs”方法，让客户在推断时控制DNN输出的semantic interpretations，而不会影响精度和效率。在推断过程中，使用了一个叫做“salted layer”的层，可以让客户控制DNN输出的semantic interpretations。</li>
<li>results: 实验结果显示，使用Salted DNNs方法可以保持高精度和效率，尤其是在将 salted layer 放在早期部分时。在实验中使用了两个不同的数据集，包括图像数据和感应器数据，并与标准DNN相比，Salted DNNs可以实现高精度和效率。<details>
<summary>Abstract</summary>
Split inference partitions a deep neural network (DNN) to run the early part at the edge and the later part in the cloud. This meets two key requirements for on-device machine learning: input privacy and compute efficiency. Still, an open question in split inference is output privacy, given that the output of a DNN is visible to the cloud. While encrypted computing can protect output privacy, it mandates extensive computation and communication resources. In this paper, we introduce "Salted DNNs": a novel method that lets clients control the semantic interpretation of DNN output at inference time while maintaining accuracy and efficiency very close to that of a standard DNN. Experimental evaluations conducted on both image and sensor data show that Salted DNNs achieve classification accuracy very close to standard DNNs, particularly when the salted layer is positioned within the early part to meet the requirements of split inference. Our method is general and can be applied to various DNNs. We open-source our code and results, as a benchmark for future studies.
</details>
<details>
<summary>摘要</summary>
分配推理分区深度神经网络（DNN），让早期部分在边缘上运行，后期部分在云上运行，满足了边缘机器学习中两个关键需求：输入隐私和计算效率。然而，在分配推理中仍存在输出隐私问题，因为深度神经网络的输出可见于云端。尝试使用加密计算来保护输出隐私，但是这需要较为广泛的计算和通信资源。在本文中，我们介绍了“孤立的深度神经网络”（Salted DNNs）：一种新的方法，允许客户控制推理时神经网络输出的Semantic解释，保持精度和效率与标准神经网络几乎相同。我们在图像和感知数据上进行了实验评估，结果表明，孤立神经网络在早期部分中位置时，可以保持分配推理中的精度和效率，并且与标准神经网络几乎相同的精度。我们的方法是通用的，可以应用于多种深度神经网络。我们将代码和结果公开，作为未来研究的标准准。
</details></li>
</ul>
<hr>
<h2 id="Accelerated-sparse-Kernel-Spectral-Clustering-for-large-scale-data-clustering-problems"><a href="#Accelerated-sparse-Kernel-Spectral-Clustering-for-large-scale-data-clustering-problems" class="headerlink" title="Accelerated sparse Kernel Spectral Clustering for large scale data clustering problems"></a>Accelerated sparse Kernel Spectral Clustering for large scale data clustering problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13381">http://arxiv.org/abs/2310.13381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mihaly Novak, Rocco Langone, Carlos Alzate, Johan Suykens</li>
<li>for: 这个论文的目的是提出一种改进的稀疏多方幂 Spectral Clustering（KSC）算法，以解决大规模数据分类问题。</li>
<li>methods: 这个算法使用了Weighted Kernel Principal Component Analysis（KPCA）和锐点 Support Vector Machine（SVM）框架，并使用了缺失Cholesky半阵（ICD）来实现稀疏性。</li>
<li>results: 这个改进算法可以大幅提高计算效率，从而使得 clustering 问题可以在秒钟内解决，而不是之前需要几个小时。此外，稀疏性也得到了显著提高，导致模型的表示更加紧凑，计算效率和描述力都得到了进一步提高。<details>
<summary>Abstract</summary>
An improved version of the sparse multiway kernel spectral clustering (KSC) is presented in this brief. The original algorithm is derived from weighted kernel principal component (KPCA) analysis formulated within the primal-dual least-squares support vector machine (LS-SVM) framework. Sparsity is achieved then by the combination of the incomplete Cholesky decomposition (ICD) based low rank approximation of the kernel matrix with the so called reduced set method. The original ICD based sparse KSC algorithm was reported to be computationally far too demanding, especially when applied on large scale data clustering problems that actually it was designed for, which has prevented to gain more than simply theoretical relevance so far. This is altered by the modifications reported in this brief that drastically improve the computational characteristics. Solving the alternative, symmetrized version of the computationally most demanding core eigenvalue problem eliminates the necessity of forming and SVD of large matrices during the model construction. This results in solving clustering problems now within seconds that were reported to require hours without altering the results. Furthermore, sparsity is also improved significantly, leading to more compact model representation, increasing further not only the computational efficiency but also the descriptive power. These transform the original, only theoretically relevant ICD based sparse KSC algorithm applicable for large scale practical clustering problems. Theoretical results and improvements are demonstrated by computational experiments on carefully selected synthetic data as well as on real life problems such as image segmentation.
</details>
<details>
<summary>摘要</summary>
本文提出了一种改进版的稀疑多方幂kernel spectral clustering（KSC）算法。原始算法基于权重kernel principal component analysis（KPCA）在primaldual最小二乘Support Vector Machine（LS-SVM）框架中得到，并使用 incomplete Cholesky decomposition（ICD）基于低级别approximation kernel矩阵和减少集方法实现稀疑性。原始ICD基于稀疑KSC算法的计算复杂性太高，特别是在应用大规模数据归一化问题时，这使得其在实际应用中具有较低的理论重要性。这篇文章修改了这些问题，使得计算特性得到了很大改进。解决计算中最复杂的核心径值问题的代替版本，从而消除了将大型矩阵的SVD形成和计算的需要，这使得归一化问题可以在几秒钟内解决，而不会改变结果。此外，稀疑性也得到了显著提高，导致模型表示更加紧凑，进一步提高了计算效率和描述力。这些改进使得原来只有理论意义的ICD基于稀疑KSC算法可以应用于大规模实际归一化问题。计算实验在手动制作的数据集和真实问题，如图像分割，都表明了这些改进的有效性。
</details></li>
</ul>
<hr>
<h2 id="Physics-Informed-Graph-Convolutional-Networks-Towards-a-generalized-framework-for-complex-geometries"><a href="#Physics-Informed-Graph-Convolutional-Networks-Towards-a-generalized-framework-for-complex-geometries" class="headerlink" title="Physics-Informed Graph Convolutional Networks: Towards a generalized framework for complex geometries"></a>Physics-Informed Graph Convolutional Networks: Towards a generalized framework for complex geometries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14948">http://arxiv.org/abs/2310.14948</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marien Chenaud, José Alves, Frédéric Magoulès</li>
<li>for: 解决部分束着方程（PDE）问题使用深度学习模型。</li>
<li>methods: 使用图ael neural networks（GNN），基于传统数学方法中的织物结构和PDE问题的相似性。</li>
<li>results: 提出一种将传统数学方法和Physics-Informed framework相结合的方法，并在三维非整形问题上进行实验验证。<details>
<summary>Abstract</summary>
Since the seminal work of [9] and their Physics-Informed neural networks (PINNs), many efforts have been conducted towards solving partial differential equations (PDEs) with Deep Learning models. However, some challenges remain, for instance the extension of such models to complex three-dimensional geometries, and a study on how such approaches could be combined to classical numerical solvers. In this work, we justify the use of graph neural networks for these problems, based on the similarity between these architectures and the meshes used in traditional numerical techniques for solving partial differential equations. After proving an issue with the Physics-Informed framework for complex geometries, during the computation of PDE residuals, an alternative procedure is proposed, by combining classical numerical solvers and the Physics-Informed framework. Finally, we propose an implementation of this approach, that we test on a three-dimensional problem on an irregular geometry.
</details>
<details>
<summary>摘要</summary>
自《[9]》的Physics-Informed neural networks（PINNs）之工作开始以来，有很多努力在解决 partial differential equations（PDEs）中使用深度学习模型。然而，还有一些挑战，如扩展到复杂的三维几何结构，以及如何将这些方法与传统的数学方法结合起来。在这项工作中，我们证明使用图 neural networks 是合适的，因为这些架构与传统的数学方法中使用的网格具有相似之处。在计算 PDE  residuals 时发现了物理 Informed 框架的问题，然后提出了一种 alternatively 的方法，将传统的数学方法与物理 Informed 框架结合起来。最后，我们提出了实现这种方法的方式，并在一个三维问题上进行了测试。
</details></li>
</ul>
<hr>
<h2 id="SigFormer-Signature-Transformers-for-Deep-Hedging"><a href="#SigFormer-Signature-Transformers-for-Deep-Hedging" class="headerlink" title="SigFormer: Signature Transformers for Deep Hedging"></a>SigFormer: Signature Transformers for Deep Hedging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13369">http://arxiv.org/abs/2310.13369</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anh-tong/sigformer">https://github.com/anh-tong/sigformer</a></li>
<li>paper_authors: Anh Tong, Thanh Nguyen-Tang, Dongeun Lee, Toan Tran, Jaesik Choi</li>
<li>for: 这个论文的目的是提出一种新的深度学习模型，用于财务风险管理，以提高财务风险管理的精度和效率。</li>
<li>methods: 这个论文使用了一种新的模型，即SigFormer，它将路径签名和变换器结合起来，以处理串行数据，特别是具有异常性的数据。</li>
<li>results: 研究人员通过对 sintetic数据进行比较，发现SigFormer比既有方法更快地学习并且更强的抗辐射性，特别是在存在异常的资产价格数据时。此外，通过对 SP 500 指数的实际回测，也得到了积极的结果。<details>
<summary>Abstract</summary>
Deep hedging is a promising direction in quantitative finance, incorporating models and techniques from deep learning research. While giving excellent hedging strategies, models inherently requires careful treatment in designing architectures for neural networks. To mitigate such difficulties, we introduce SigFormer, a novel deep learning model that combines the power of path signatures and transformers to handle sequential data, particularly in cases with irregularities. Path signatures effectively capture complex data patterns, while transformers provide superior sequential attention. Our proposed model is empirically compared to existing methods on synthetic data, showcasing faster learning and enhanced robustness, especially in the presence of irregular underlying price data. Additionally, we validate our model performance through a real-world backtest on hedging the SP 500 index, demonstrating positive outcomes.
</details>
<details>
<summary>摘要</summary>
深度封风是现代金融数学的一个有前途的方向，具有深度学习研究中的模型和技术。而这些模型却需要在设计神经网络架构时进行仔细的考虑，以避免一些困难。为了解决这些问题，我们介绍了SigFormer，一种新的深度学习模型，它将路径签名和转换器结合起来处理顺序数据，特别是带有不规则性的数据。路径签名有效地捕捉复杂的数据模式，而转换器则提供了更好的顺序注意力。我们对现成的方法进行了比较，并在人工数据上进行了实验，显示了更快的学习速度和更高的稳定性，特别是在存在不规则的下场价格数据时。此外，我们还验证了我们的模型在真实的SP 500指数投资中的性能，显示了正面的结果。
</details></li>
</ul>
<hr>
<h2 id="Dissecting-Causal-Biases"><a href="#Dissecting-Causal-Biases" class="headerlink" title="Dissecting Causal Biases"></a>Dissecting Causal Biases</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13364">http://arxiv.org/abs/2310.13364</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rūta Binkytė, Sami Zhioua, Yassine Turki</li>
<li>for: 这篇论文是关于机器学习基于自动决策系统中的歧视评估问题。</li>
<li>methods: 该论文使用了 causality 理论来形式地定义和分析歧视的各种来源，包括干扰、选择、测量和互动。</li>
<li>results: 该论文提供了每种歧视来源的关闭式表达式，以便分析每种来源的行为，特别是哪些情况下它们缺失，以及哪些情况下它们最大化。<details>
<summary>Abstract</summary>
Accurately measuring discrimination in machine learning-based automated decision systems is required to address the vital issue of fairness between subpopulations and/or individuals. Any bias in measuring discrimination can lead to either amplification or underestimation of the true value of discrimination. This paper focuses on a class of bias originating in the way training data is generated and/or collected. We call such class causal biases and use tools from the field of causality to formally define and analyze such biases. Four sources of bias are considered, namely, confounding, selection, measurement, and interaction. The main contribution of this paper is to provide, for each source of bias, a closed-form expression in terms of the model parameters. This makes it possible to analyze the behavior of each source of bias, in particular, in which cases they are absent and in which other cases they are maximized. We hope that the provided characterizations help the community better understand the sources of bias in machine learning applications.
</details>
<details>
<summary>摘要</summary>
需要准确测量机器学习自动决策系统中的歧视，以解决人群或个体之间的公平问题。任何偏见在测量歧视方面可能会导致扩大或下降真实的歧视程度。本文关注一种来自训练数据生成和收集的偏见类型，我们称之为 causal bias。我们使用 causality 领域中的工具来正式定义和分析这种偏见。我们考虑了四种偏见来源， namely，杂化、选择、测量和交互。本文的主要贡献是为每种偏见提供了关于模型参数的闭式表达。这使得可以分析每种偏见的行为，特别是在哪些情况下它们缺失，而在哪些情况下它们最大化。我们希望通过提供的特征化来帮助社区更好地理解机器学习应用中的偏见来源。
</details></li>
</ul>
<hr>
<h2 id="Learning-Recurrent-Models-with-Temporally-Local-Rules"><a href="#Learning-Recurrent-Models-with-Temporally-Local-Rules" class="headerlink" title="Learning Recurrent Models with Temporally Local Rules"></a>Learning Recurrent Models with Temporally Local Rules</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13284">http://arxiv.org/abs/2310.13284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Azwar Abdulsalam, Joseph G. Makin</li>
<li>for: 这篇论文是为了探讨如何使用生成模型处理序列数据，并且提出了一种新的方法来缓解计算成本问题。</li>
<li>methods: 这篇论文使用了一种新的方法，即要求生成模型学习当前和前一个时刻的共同分布，而不仅仅是转移概率。</li>
<li>results: 研究人员在实验中发现，使用这种新方法可以学习一些通常需要回传计算的数据特征，并且可以在不同的架构下实现这种效果。<details>
<summary>Abstract</summary>
Fitting generative models to sequential data typically involves two recursive computations through time, one forward and one backward. The latter could be a computation of the loss gradient (as in backpropagation through time), or an inference algorithm (as in the RTS/Kalman smoother). The backward pass in particular is computationally expensive (since it is inherently serial and cannot exploit GPUs), and difficult to map onto biological processes. Work-arounds have been proposed; here we explore a very different one: requiring the generative model to learn the joint distribution over current and previous states, rather than merely the transition probabilities. We show on toy datasets that different architectures employing this principle can learn aspects of the data typically requiring the backward pass.
</details>
<details>
<summary>摘要</summary>
通常情况下，生成模型会将序列数据适应的方法是通过时间进行两个递归计算，一个是前进计算，另一个是后退计算。后退计算通常是计算损失函数对数（如在时间层次propagation中），或者推理算法（如在RTS/Kalman滤波器中）。后退计算尤其是计算成本高（因为它是串行的，无法利用GPU），而且Difficult to map onto biological processes。工作around proposal; here we explore a very different one: requiring the generative model to learn the joint distribution over current and previous states, rather than merely the transition probabilities. We show on toy datasets that different architectures employing this principle can learn aspects of the data typically requiring the backward pass.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Taiwan, Hong Kong, and other countries.
</details></li>
</ul>
<hr>
<h2 id="FedLoRA-Model-Heterogeneous-Personalized-Federated-Learning-with-LoRA-Tuning"><a href="#FedLoRA-Model-Heterogeneous-Personalized-Federated-Learning-with-LoRA-Tuning" class="headerlink" title="FedLoRA: Model-Heterogeneous Personalized Federated Learning with LoRA Tuning"></a>FedLoRA: Model-Heterogeneous Personalized Federated Learning with LoRA Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13283">http://arxiv.org/abs/2310.13283</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liping Yi, Han Yu, Gang Wang, Xiaoguang Liu</li>
<li>for: 这个论文的目的是提出一种 computation-和 communication-efficient 的模型多样化个性化联合学习框架（FedLoRA），用于解决模型多样性、系统多样性和统计多样性等挑战。</li>
<li>methods: 这个框架采用了一种基于 LoRA 调整的迭代式全局-本地知识交换方法，通过在每个客户端上加载一个小型同构器，使得客户端可以训练多样化的本地模型而无需高度 computation 和 communication overhead。</li>
<li>results: 在两个实际数据集上进行了广泛的实验，结果显示，FedLoRA 可以在测试精度、计算开销和通信成本三个方面超过六个基准方法，其中测试精度高于最佳方法1.35%，计算开销下降11.81倍，通信成本减少7.41倍。<details>
<summary>Abstract</summary>
Federated learning (FL) is an emerging machine learning paradigm in which a central server coordinates multiple participants (a.k.a. FL clients) to train a model collaboratively on decentralized data with privacy protection. This paradigm constrains that all clients have to train models with the same structures (homogeneous). In practice, FL often faces statistical heterogeneity, system heterogeneity and model heterogeneity challenges. These challenging issues inspire the field of Model-Heterogeneous Personalized Federated Learning (MHPFL) which aims to train a personalized and heterogeneous local model for each FL client. Existing MHPFL approaches cannot achieve satisfactory model performance, acceptable computational overhead and efficient communication simultaneously. To bridge this gap, we propose a novel computation- and communication-efficient model-heterogeneous personalized Federated learning framework based on LoRA tuning (FedLoRA). It is designed to incorporate a homogeneous small adapter for each client's heterogeneous local model. Both models are trained following the proposed iterative training for global-local knowledge exchange. The homogeneous small local adapters are sent to the FL server to be aggregated into a global adapter. In this way, FL clients can train heterogeneous local models without incurring high computation and communication costs. We theoretically prove the non-convex convergence rate of FedLoRA. Extensive experiments on two real-world datasets demonstrate that FedLoRA outperforms six state-of-the-art baselines, beating the best approach by 1.35% in terms of test accuracy, 11.81 times computation overhead reduction and 7.41 times communication cost saving.
</details>
<details>
<summary>摘要</summary>
联合学习（FL）是一种兴起的机器学习方法，在中央服务器协调多个 particiant（称为FL客户端）共同训练模型，并在分散式数据上保护隐私。这个方法限制所有客户端都必须使用同一个结构的模型（同质）。在实践中，FL经常面临统计不一致、系统不一致和模型不一致的挑战。这些挑战驱使了联合学习专门的人类化个性化训练框架（MHPFL），旨在将各个FL客户端专门训练的本地模型变成个性化的模型。现有的MHPFL方法无法同时 дости得满意的模型性能、可接受的计算负载和有效的通信成本。为了填补这个差距，我们提出了一个新的计算和通信效率的模型专业化人类化联合学习框架，基于LoRA调整（FedLoRA）。它运用了各个客户端的同质小适配器，以实现各个客户端专门训练的本地模型，不需要高计算和通信成本。我们对FedLoRA的非对称协调率进行了理论证明。实验结果显示，FedLoRA在两个真实世界数据集上进行了比基eline的性能评估，与最佳方法相比，获得了1.35%的测试精度提升、11.81倍的计算负载削减和7.41倍的通信成本削减。
</details></li>
</ul>
<hr>
<h2 id="An-Event-based-Prediction-Suffix-Tree"><a href="#An-Event-based-Prediction-Suffix-Tree" class="headerlink" title="An Event based Prediction Suffix Tree"></a>An Event based Prediction Suffix Tree</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14944">http://arxiv.org/abs/2310.14944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Evie Andrew, Travis Monk, André van Schaik</li>
<li>for: 这篇论文是为了介绍一种基于事件预测 suffixed tree（EPST）算法，该算法是基于生物体中的预测方法，可以在多个重叠模式上进行预测。</li>
<li>methods: 该算法使用基于事件数据的特定表示方式，即在短时间窗口内的事件子序列的一部分。它还具有可解释性、耐事件噪音、耐Dropout等多个优点。</li>
<li>results: 在一个 sintetic数据预测任务中，EPST算法可以在添加事件噪音、事件抖动和Dropout等不利条件下进行预测，并输出预测结果，可以应用于事件基于异常检测或模式识别等任务。<details>
<summary>Abstract</summary>
This article introduces the Event based Prediction Suffix Tree (EPST), a biologically inspired, event-based prediction algorithm. The EPST learns a model online based on the statistics of an event based input and can make predictions over multiple overlapping patterns. The EPST uses a representation specific to event based data, defined as a portion of the power set of event subsequences within a short context window. It is explainable, and possesses many promising properties such as fault tolerance, resistance to event noise, as well as the capability for one-shot learning. The computational features of the EPST are examined in a synthetic data prediction task with additive event noise, event jitter, and dropout. The resulting algorithm outputs predicted projections for the near term future of the signal, which may be applied to tasks such as event based anomaly detection or pattern recognition.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇文章介绍了基于事件的预测 sufix tree (EPST) 算法，这是基于生物体的革新性预测算法。EPST 在线学习基于事件输入的统计特征，并可以预测多个 overlap 的模式。EPST 使用专门为事件数据定义的表示方式，即在短上下文窗口内的事件子序列的一部分。它可以解释，并具有许多有前途的特性，如fault tolerance、事件噪音抗性以及一次学习能力。EPST 的计算特点在一个 sintetic 数据预测任务中被检查，包括 additive 事件噪音、事件摆动和 dropout。结果输出的预测投影可以用于事件基于异常检测或模式识别等任务。
</details></li>
</ul>
<hr>
<h2 id="DIG-MILP-a-Deep-Instance-Generator-for-Mixed-Integer-Linear-Programming-with-Feasibility-Guarantee"><a href="#DIG-MILP-a-Deep-Instance-Generator-for-Mixed-Integer-Linear-Programming-with-Feasibility-Guarantee" class="headerlink" title="DIG-MILP: a Deep Instance Generator for Mixed-Integer Linear Programming with Feasibility Guarantee"></a>DIG-MILP: a Deep Instance Generator for Mixed-Integer Linear Programming with Feasibility Guarantee</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13261">http://arxiv.org/abs/2310.13261</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/graph-com/dig_milp">https://github.com/graph-com/dig_milp</a></li>
<li>paper_authors: Haoyu Wang, Jialin Liu, Xiaohan Chen, Xinshang Wang, Pan Li, Wotao Yin</li>
<li>for: 实现高效的整数线性Programming（MILP）解决方案，并且提供广泛、多样化和代表性的数据来支持算法开发、解决方案优化和机器学习模型训练。</li>
<li>methods: 基于Variational auto-encoder（VAE）的深度生成框架，将高度限制的MILP数据中的深度结构特征提取出来，生成与目标数据相似的实例。</li>
<li>results: 通过两个下游任务（S1）资料共享和（S2）资料增强，显示DIG-MILP可以生成高品质且新的MILP实例，并且 garantte Correctness和可行性。<details>
<summary>Abstract</summary>
Mixed-integer linear programming (MILP) stands as a notable NP-hard problem pivotal to numerous crucial industrial applications. The development of effective algorithms, the tuning of solvers, and the training of machine learning models for MILP resolution all hinge on access to extensive, diverse, and representative data. Yet compared to the abundant naturally occurring data in image and text realms, MILP is markedly data deficient, underscoring the vital role of synthetic MILP generation. We present DIG-MILP, a deep generative framework based on variational auto-encoder (VAE), adept at extracting deep-level structural features from highly limited MILP data and producing instances that closely mirror the target data. Notably, by leveraging the MILP duality, DIG-MILP guarantees a correct and complete generation space as well as ensures the boundedness and feasibility of the generated instances. Our empirical study highlights the novelty and quality of the instances generated by DIG-MILP through two distinct downstream tasks: (S1) Data sharing, where solver solution times correlate highly positive between original and DIG-MILP-generated instances, allowing data sharing for solver tuning without publishing the original data; (S2) Data Augmentation, wherein the DIG-MILP-generated instances bolster the generalization performance of machine learning models tasked with resolving MILP problems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Knowledge-Graph-Context-Enhanced-Diversified-Recommendation"><a href="#Knowledge-Graph-Context-Enhanced-Diversified-Recommendation" class="headerlink" title="Knowledge Graph Context-Enhanced Diversified Recommendation"></a>Knowledge Graph Context-Enhanced Diversified Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13253">http://arxiv.org/abs/2310.13253</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anonym844/kg-diverse">https://github.com/anonym844/kg-diverse</a></li>
<li>paper_authors: Xiaolong Liu, Liangwei Yang, Zhiwei Liu, Mingdai Yang, Chen Wang, Hao Peng, Philip S. Yu</li>
<li>For: The paper aims to enhance recommendation diversity within the context of knowledge graphs (KG) by incorporating contextual information and preserving contextual integrity.* Methods: The paper introduces an innovative metric called Entity Coverage and Relation Coverage to quantify diversity within the KG domain. It also proposes a novel technique called Diversified Embedding Learning (DEL) to formulate user representations that possess an innate awareness of diversity. Additionally, the paper introduces a new technique named Conditional Alignment and Uniformity (CAU) to encode KG item embeddings while preserving contextual integrity.* Results: The paper’s contributions signify a substantial stride towards augmenting the panorama of recommendation diversity within the realm of KG-informed RecSys paradigms.<details>
<summary>Abstract</summary>
The field of Recommender Systems (RecSys) has been extensively studied to enhance accuracy by leveraging users' historical interactions. Nonetheless, this persistent pursuit of accuracy frequently engenders diminished diversity, culminating in the well-recognized "echo chamber" phenomenon. Diversified RecSys has emerged as a countermeasure, placing diversity on par with accuracy and garnering noteworthy attention from academic circles and industry practitioners. This research explores the realm of diversified RecSys within the intricate context of knowledge graphs (KG). These KGs act as repositories of interconnected information concerning entities and items, offering a propitious avenue to amplify recommendation diversity through the incorporation of insightful contextual information. Our contributions include introducing an innovative metric, Entity Coverage, and Relation Coverage, which effectively quantifies diversity within the KG domain. Additionally, we introduce the Diversified Embedding Learning (DEL) module, meticulously designed to formulate user representations that possess an innate awareness of diversity. In tandem with this, we introduce a novel technique named Conditional Alignment and Uniformity (CAU). It adeptly encodes KG item embeddings while preserving contextual integrity. Collectively, our contributions signify a substantial stride towards augmenting the panorama of recommendation diversity within the realm of KG-informed RecSys paradigms.
</details>
<details>
<summary>摘要</summary>
领域内的个人推荐系统（RecSys）已经广泛研究，以提高准确性。然而，这种不断追求准确性的努力经常导致多样性减少，最终形成了“喷 voz”现象。为了解决这问题，多样化RecSys已经出现了，将多样性与准确性平起肩。这篇研究在知识图（KG）的厚重的背景下，探讨了多样化RecSys的领域。KG作为实体和物品之间的连接，可以为推荐多样性的提高提供一条优美的路径。我们的贡献包括引入了一种创新的度量，Entity Coverage和Relation Coverage，可以有效量度KG中多样性。此外，我们还提出了多样化嵌入学习（DEL）模块，仔细设计用于构建具有多样性感的用户表示。同时，我们还提出了一种新的技术 named Conditional Alignment and Uniformity（CAU），可以充分利用KG中项目的嵌入，同时保持上下文完整性。总之，我们的贡献代表了对KG-驱动的RecSys多样化领域的一大突破。
</details></li>
</ul>
<hr>
<h2 id="Transparency-challenges-in-policy-evaluation-with-causal-machine-learning-–-improving-usability-and-accountability"><a href="#Transparency-challenges-in-policy-evaluation-with-causal-machine-learning-–-improving-usability-and-accountability" class="headerlink" title="Transparency challenges in policy evaluation with causal machine learning – improving usability and accountability"></a>Transparency challenges in policy evaluation with causal machine learning – improving usability and accountability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13240">http://arxiv.org/abs/2310.13240</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patrick Rehill, Nicholas Biddle</li>
<li>for: 这篇论文旨在探讨 causal machine learning 工具在实际政策评估任务中的应用，以及这些方法在政策评估中的透明性问题。</li>
<li>methods: 本论文使用了 causal forest 模型来估计 conditional average treatment effects，并explores 如何使用可解释 AI 工具和简化模型来解决透明性问题。</li>
<li>results: 研究发现，现有的预测模型理解工具对 causal machine learning 模型不够有效，而简化模型以提高可解释性会导致预测误差增加。<details>
<summary>Abstract</summary>
Causal machine learning tools are beginning to see use in real-world policy evaluation tasks to flexibly estimate treatment effects. One issue with these methods is that the machine learning models used are generally black boxes, i.e., there is no globally interpretable way to understand how a model makes estimates. This is a clear problem in policy evaluation applications, particularly in government, because it is difficult to understand whether such models are functioning in ways that are fair, based on the correct interpretation of evidence and transparent enough to allow for accountability if things go wrong. However, there has been little discussion of transparency problems in the causal machine learning literature and how these might be overcome. This paper explores why transparency issues are a problem for causal machine learning in public policy evaluation applications and considers ways these problems might be addressed through explainable AI tools and by simplifying models in line with interpretable AI principles. It then applies these ideas to a case-study using a causal forest model to estimate conditional average treatment effects for a hypothetical change in the school leaving age in Australia. It shows that existing tools for understanding black-box predictive models are poorly suited to causal machine learning and that simplifying the model to make it interpretable leads to an unacceptable increase in error (in this application). It concludes that new tools are needed to properly understand causal machine learning models and the algorithms that fit them.
</details>
<details>
<summary>摘要</summary>
causal machine learning工具正在实际政策评估任务中得到应用，以便灵活地估计治理效果。 however，这些方法的机器学习模型通常是黑盒子，即没有全面可解释的方式来理解模型如何生成估计。这是政策评估应用中的一个明显问题，特别是在政府中，因为困难以理解模型是否正常工作，基于正确的证据解释和透明度足够高以便负责任。然而，在 causal machine learning文献中对透明性问题的讨论相对少。这篇论文探讨了 causal machine learning在公共政策评估应用中的透明性问题，并考虑了如何通过可解释 AI 工具和简化模型来解决这些问题。然后，它应用这些想法到一个 случа study中，使用 causal forest 模型来估计 conditional average treatment effects  для一个假设的澳大利亚学生离校年龄的变化。结果显示，现有的理解黑盒predictive模型的工具不适用于 causal machine learning，并且简化模型以使其可解释会导致误差增加（在这个应用中）。因此，这篇论文结论认为，需要新的工具来全面理解 causal machine learning模型和这些模型的算法。
</details></li>
</ul>
<hr>
<h2 id="Training-A-Semantic-Communication-System-with-Federated-Learning"><a href="#Training-A-Semantic-Communication-System-with-Federated-Learning" class="headerlink" title="Training A Semantic Communication System with Federated Learning"></a>Training A Semantic Communication System with Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13236">http://arxiv.org/abs/2310.13236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Loc X. Nguyen, Huy Q. Le, Ye Lin Tun, Pyae Sone Aung, Yan Kyaw Tun, Zhu Han, Choong Seon Hong</li>
<li>for: 本研究旨在提高 semantic communication 系统的性能，使其能够更好地处理数据匮乏问题。</li>
<li>methods: 本研究使用 federated learning (FL) Setting，利用用户数据进行学习，同时保护用户隐私和安全。另外，我们提出了一种减少每次全局轮次中传输信息量的机制，以降低网络负担。</li>
<li>results: 我们的提议技术与基准方法进行比较，实验结果表明其效果明显更高。<details>
<summary>Abstract</summary>
Semantic communication has emerged as a pillar for the next generation of communication systems due to its capabilities in alleviating data redundancy. Most semantic communication systems are built using advanced deep learning models whose performance heavily depends on data availability. These studies assume that an abundance of training data is available, which is unrealistic. In practice, data is mainly created on the user side. Due to privacy and security concerns, the transmission of data is restricted, which is necessary for conventional centralized training schemes. To address this challenge, we explore semantic communication in federated learning (FL) setting that utilizes user data without leaking privacy. Additionally, we design our system to tackle the communication overhead by reducing the quantity of information delivered in each global round. In this way, we can save significant bandwidth for resource-limited devices and reduce overall network traffic. Finally, we propose a mechanism to aggregate the global model from the clients, called FedLol. Extensive simulation results demonstrate the efficacy of our proposed technique compared to baseline methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Equivariant-Transformer-is-all-you-need"><a href="#Equivariant-Transformer-is-all-you-need" class="headerlink" title="Equivariant Transformer is all you need"></a>Equivariant Transformer is all you need</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13222">http://arxiv.org/abs/2310.13222</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akio Tomiya, Yuki Nagai</li>
<li>for: 这篇论文是用于推动计算物理学的机器学习和深度学习的应用。</li>
<li>methods: 论文使用了对称Equivariant Attention来改进Self-Learning Monte Carlo（SLMC）方法。</li>
<li>results: 实验结果表明，对称Equivariant Attention可以减少线性模型的接受率问题，并且可以观察到大型语言模型的扩展性。<details>
<summary>Abstract</summary>
Machine learning, deep learning, has been accelerating computational physics, which has been used to simulate systems on a lattice. Equivariance is essential to simulate a physical system because it imposes a strong induction bias for the probability distribution described by a machine learning model. This reduces the risk of erroneous extrapolation that deviates from data symmetries and physical laws. However, imposing symmetry on the model sometimes occur a poor acceptance rate in self-learning Monte-Carlo (SLMC). On the other hand, Attention used in Transformers like GPT realizes a large model capacity. We introduce symmetry equivariant attention to SLMC. To evaluate our architecture, we apply it to our proposed new architecture on a spin-fermion model on a two-dimensional lattice. We find that it overcomes poor acceptance rates for linear models and observe the scaling law of the acceptance rate as in the large language models with Transformers.
</details>
<details>
<summary>摘要</summary>
机器学习、深度学习已经加速计算物理学，通过在格子上模拟系统。对称是计算物理系统的关键因素，因为它对机器学习模型中描述的概率分布强加假设。这可以降低模型外泌的风险，避免因数学 симметрии和物理法律而导致的误差推断。然而，在SLMC中强制实现对称 occasionally leads to poor acceptance rates.在这个场景下，我们引入对称启发注意力。我们采用这种新架构应用于我们的提议的二维格子上的螺旋- fermion 模型。我们发现它可以超越线性模型的 Acceptance 率问题，并观察到大型语言模型中的启发注意力的扩展律。
</details></li>
</ul>
<hr>
<h2 id="In-context-Learning-with-Transformer-Is-Really-Equivalent-to-a-Contrastive-Learning-Pattern"><a href="#In-context-Learning-with-Transformer-Is-Really-Equivalent-to-a-Contrastive-Learning-Pattern" class="headerlink" title="In-context Learning with Transformer Is Really Equivalent to a Contrastive Learning Pattern"></a>In-context Learning with Transformer Is Really Equivalent to a Contrastive Learning Pattern</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13220">http://arxiv.org/abs/2310.13220</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruifeng Ren, Yong Liu</li>
<li>for: 本研究旨在理解Transformers基于强化学习的协同学习（ICL）机制。</li>
<li>methods: 本研究使用权重方法 establishment kernel方法来解释ICL的推理过程，并分析了在contrastive learning pattern下的自注意力层可能的改进。</li>
<li>results: 本研究表明，ICL可以视为一种梯度下降过程，并且通过对contrastive learning pattern进行分析，可以提出可能的自注意力层改进。<details>
<summary>Abstract</summary>
Pre-trained large language models based on Transformers have demonstrated amazing in-context learning (ICL) abilities. Given several demonstration examples, the models can implement new tasks without any parameter updates. However, it is still an open question to understand the mechanism of ICL. In this paper, we interpret the inference process of ICL as a gradient descent process in a contrastive learning pattern. Firstly, leveraging kernel methods, we establish the relationship between gradient descent and self-attention mechanism under generally used softmax attention setting instead of linear attention setting. Then, we analyze the corresponding gradient descent process of ICL from the perspective of contrastive learning without negative samples and discuss possible improvements of this contrastive learning pattern, based on which the self-attention layer can be further modified. Finally, we design experiments to support our opinions. To the best of our knowledge, our work is the first to provide the understanding of ICL from the perspective of contrastive learning and has the potential to facilitate future model design by referring to related works on contrastive learning.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>基于 transformer 的大语言模型已经表现出很好的上下文学习（ICL）能力。给定一些示例，模型可以实现新任务无需参数更新。然而，我们还没有很好地理解ICL的机制。在这篇论文中，我们将推理出ICL的推理过程为一个梯度下降过程，并且在通常使用软max注意力设置下，使用kernel方法来建立梯度下降和自注意力机制之间的关系。然后，我们分析ICL的相应梯度下降过程从对照学习的角度，不包括负样本，并讨论可能改进这种对照学习模式的方法。最后，我们设计了实验来支持我们的观点。根据我们所知，我们的工作是首次从对照学习角度理解ICL，并且具有可能引导未来模型设计的潜在优势。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/20/cs.LG_2023_10_20/" data-id="clpxp042w00tyfm88cql30gmm" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_10_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/20/eess.IV_2023_10_20/" class="article-date">
  <time datetime="2023-10-20T09:00:00.000Z" itemprop="datePublished">2023-10-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/20/eess.IV_2023_10_20/">eess.IV - 2023-10-20</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Parallel-compressive-super-resolution-imaging-with-wide-field-of-view-based-on-physics-enhanced-network"><a href="#Parallel-compressive-super-resolution-imaging-with-wide-field-of-view-based-on-physics-enhanced-network" class="headerlink" title="Parallel compressive super-resolution imaging with wide field-of-view based on physics enhanced network"></a>Parallel compressive super-resolution imaging with wide field-of-view based on physics enhanced network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14965">http://arxiv.org/abs/2310.14965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao-Peng Jin, An-Dong Xiong, Wei Zhang, Xiao-Qing Wang, Fan Liu, Chang-Heng Li, Xu-Ri Yao, Xue-Feng Liu, Qing Zhao</li>
<li>for: 高性能和广阔视场（FOV）超解像成像</li>
<li>methods: 基于物理增强网络的并行压缩超解像成像</li>
<li>results: 实现4x4倍化超解像、三个设计的面镜和实时成像速度<details>
<summary>Abstract</summary>
Achieving both high-performance and wide field-of-view (FOV) super-resolution imaging has been attracting increasing attention in recent years. However, such goal suffers from long reconstruction time and huge storage space. Parallel compressive imaging (PCI) provides an efficient solution, but the super-resolution quality and imaging speed are strongly dependent on precise optical transfer function (OTF), modulation masks and reconstruction algorithm. In this work, we propose a wide FOV parallel compressive super-resolution imaging approach based on physics enhanced network. By training the network with the prior OTF of an arbitrary 128x128-pixel region and fine-tuning the network with other OTFs within rest regions of FOV, we realize both mask optimization and super-resolution imaging with up to 1020x1500 wide FOV. Numerical simulations and practical experiments demonstrate the effectiveness and superiority of the proposed approach. We achieve high-quality reconstruction with 4x4 times super-resolution enhancement using only three designed masks to reach real-time imaging speed. The proposed approach promotes the technology of rapid imaging for super-resolution and wide FOV, ranging from infrared to Terahertz.
</details>
<details>
<summary>摘要</summary>
实现高性能和广频谱场视野（FOV）超分辨成像已经引起了过去几年的关注。然而，这个目标受到了重建时间和存储空间的限制。并行压缩成像（PCI）提供了一个有效的解决方案，但是超分辨质量和成像速度受到了准确的光传导函数（OTF）、模拟面和重建算法的影响。在这项工作中，我们提议一种基于物理增强网络的广频谱场并行压缩超分辨成像方法。通过训练网络使用任意128x128像素区域的先前OTF，并在其他FOV区域中细化网络，我们实现了模拟面优化和超分辨成像，可达到1020x1500广频谱场。 numeral simulations and practical experiments demonstrate the effectiveness and superiority of the proposed approach. We achieve high-quality reconstruction with 4x4 times super-resolution enhancement using only three designed masks to reach real-time imaging speed. The proposed approach promotes the technology of rapid imaging for super-resolution and wide FOV, ranging from infrared to Terahertz.
</details></li>
</ul>
<hr>
<h2 id="An-Invitation-to-Hypercomplex-Phase-Retrieval-Theory-and-Applications"><a href="#An-Invitation-to-Hypercomplex-Phase-Retrieval-Theory-and-Applications" class="headerlink" title="An Invitation to Hypercomplex Phase Retrieval: Theory and Applications"></a>An Invitation to Hypercomplex Phase Retrieval: Theory and Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17660">http://arxiv.org/abs/2310.17660</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roman Jacome, Kumar Vijay Mishra, Brian M. Sadler, Henry Arguello</li>
<li>for: The paper is written for researchers and practitioners working in the field of hypercomplex signal processing (HSP) and its applications in optical imaging.</li>
<li>methods: The paper uses Clifford algebra to handle multidimensional signals and provides a synopsis of emerging areas and applications of hypercomplex phase retrieval (HPR) with a focus on optical imaging.</li>
<li>results: The paper discusses the opportunities for developing novel HSP tools and algorithms in the context of HPR, particularly in optical imaging applications.<details>
<summary>Abstract</summary>
Hypercomplex signal processing (HSP) provides state-of-the-art tools to handle multidimensional signals by harnessing intrinsic correlation of the signal dimensions through Clifford algebra. Recently, the hypercomplex representation of the phase retrieval (PR) problem, wherein a complex-valued signal is estimated through its intensity-only projections, has attracted significant interest. The hypercomplex PR (HPR) arises in many optical imaging and computational sensing applications that usually comprise quaternion and octonion-valued signals. Analogous to the traditional PR, measurements in HPR may involve complex, hypercomplex, Fourier, and other sensing matrices. This set of problems opens opportunities for developing novel HSP tools and algorithms. This article provides a synopsis of the emerging areas and applications of HPR with a focus on optical imaging.
</details>
<details>
<summary>摘要</summary>
超复杂信号处理（HSP）提供了当今最先进的工具来处理多维信号，通过CLIFFORD代数利用信号维度之间的自然相关性。在最近几年，使用超复杂表示法来解决频谱恢复（PR）问题，其中一个复数值信号通过其尺度仅的投影来估算，已经吸引了广泛的关注。这种超复杂PR（HPR）在光学成像和计算感知应用中广泛存在，通常包括四元数和八元数值信号。与传统PR问题相似，HPR问题中的测量可能包括复数、超复杂、傅里叶和其他感知矩阵。这些问题的出现为HSP工具和算法的开发提供了新的机会。本文将对HPR在光学成像领域的出现和应用进行简要的介绍。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/20/eess.IV_2023_10_20/" data-id="clpxp04a301chfm881321f3lv" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_10_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/20/eess.SP_2023_10_20/" class="article-date">
  <time datetime="2023-10-20T08:00:00.000Z" itemprop="datePublished">2023-10-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/20/eess.SP_2023_10_20/">eess.SP - 2023-10-20</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ML-Based-Feedback-Free-Adaptive-MCS-Selection-for-Massive-Multi-User-MIMO"><a href="#ML-Based-Feedback-Free-Adaptive-MCS-Selection-for-Massive-Multi-User-MIMO" class="headerlink" title="ML-Based Feedback-Free Adaptive MCS Selection for Massive Multi-User MIMO"></a>ML-Based Feedback-Free Adaptive MCS Selection for Massive Multi-User MIMO</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13830">http://arxiv.org/abs/2310.13830</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qing An, Mehdi Zafari, Chris Dick, Santiago Segarra, Ashutosh Sabharwal, Rahman Doost-Mohammady</li>
<li>for: 这篇论文旨在提高无线通信系统的 спектル效率，通过运用机器学习（ML）基本的方法来自动选择模ulation和编程码集（MCS）。</li>
<li>methods: 我们提出了一个新的自动MCS选择框架，不需要用户反馈，仅靠对于上传频道的协变网络（CNN）-长期快速传输网络（LSTM）模型来学习用户通道矩阵和最佳MCS水平之间的映射。</li>
<li>results: 我们通过实验使用真实世界的数据集，证明了我们的算法的有效性。<details>
<summary>Abstract</summary>
As wireless communication systems strive to improve spectral efficiency, there has been a growing interest in employing machine learning (ML)-based approaches for adaptive modulation and coding scheme (MCS) selection. In this paper, we introduce a new adaptive MCS selection framework for massive MIMO systems that operates without any feedback from users by solely relying on instantaneous uplink channel estimates. Our proposed method can effectively operate in multi-user scenarios where user feedback imposes excessive delay and bandwidth overhead. To learn the mapping between the user channel matrices and the optimal MCS level of each user, we develop a Convolutional Neural Network (CNN)-Long Short-Term Memory Network (LSTM)-based model and compare the performance with the state-of-the-art methods. Finally, we validate the effectiveness of our algorithm by evaluating it experimentally using real-world datasets collected from the RENEW massive MIMO platform.
</details>
<details>
<summary>摘要</summary>
“为了提高无线通信系统的spectral efficiency，随着机器学习（ML）方法的应用，这些系统中的modulation and coding scheme（MCS）选择方法受到了越来越大的关注。在这篇文章中，我们介绍了一个新的适应MCS选择框架，这个框架可以在大规模多用户MIMO系统中运行，并且不需要用户反馈。我们的提案可以实现在多用户enario中，where user feedback would cause excessive delay and bandwidth overhead。为了将用户通道矩阵与最佳MCS水平相对映射，我们开发了一个Convolutional Neural Network（CNN）-Long Short-Term Memory Network（LSTM）模型，并与现有方法进行比较。最后，我们透过实验使用真实世界的数据集，评估了我们的算法的有效性。”Note that Simplified Chinese is a romanization of Chinese, and the actual Chinese characters may be different.
</details></li>
</ul>
<hr>
<h2 id="V2X-Sidelink-Positioning-in-FR1-Scenarios-Algorithms-and-Performance-Evaluation"><a href="#V2X-Sidelink-Positioning-in-FR1-Scenarios-Algorithms-and-Performance-Evaluation" class="headerlink" title="V2X Sidelink Positioning in FR1: Scenarios, Algorithms, and Performance Evaluation"></a>V2X Sidelink Positioning in FR1: Scenarios, Algorithms, and Performance Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13753">http://arxiv.org/abs/2310.13753</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Ge, Maximilian Stark, Musa Furkan Keskin, Frank Hofmann, Thomas Hansen, Henk Wymeersch</li>
<li>For: 本研究 investigate sub-6 GHz V2X sidelink positioning scenarios in 5G vehicular networks, with a focus on developing a comprehensive end-to-end methodology for channel modeling, performance bounds, channel estimation, and geometric positioning.* Methods: 本研究使用了以下方法：	+  derivation of a novel, approximate Cramér-Rao bound (CRB) on the connected road user (CRU) position, taking into account multipath interference, path merging, and the round-trip-time (RTT) protocol.	+  high-resolution channel parameter estimation algorithms based on tensor decomposition and ESPRIT methods, specifically tailored to dense multipath V2X sidelink environments.	+  comprehensive simulations using realistic ray-tracing data and antenna patterns to evaluate channel estimation and positioning performance.* Results: 本研究的结果表明，使用提议的算法可以实现 sub-meter 精度在 sub-6 GHz V2X 中。<details>
<summary>Abstract</summary>
In this paper, we investigate sub-6 GHz V2X sidelink positioning scenarios in 5G vehicular networks through a comprehensive end-to-end methodology encompassing ray-tracing-based channel modeling, novel theoretical performance bounds, high-resolution channel parameter estimation, and geometric positioning using a round-trip-time (RTT) protocol. We first derive a novel, approximate Cram\'er-Rao bound (CRB) on the connected road user (CRU) position, explicitly taking into account multipath interference, path merging, and the RTT protocol. Capitalizing on tensor decomposition and ESPRIT methods, we propose high-resolution channel parameter estimation algorithms specifically tailored to dense multipath V2X sidelink environments, designed to detect multipath components (MPCs) and extract line-of-sight (LoS) parameters. Finally, using realistic ray-tracing data and antenna patterns, comprehensive simulations are conducted to evaluate channel estimation and positioning performance, indicating that sub-meter accuracy can be achieved in sub-6 GHz V2X with the proposed algorithms.
</details>
<details>
<summary>摘要</summary>
在本文中，我们 investigate 5G vehicular network中的低于6GHz V2X侧链位征enario通过全面的端到端方法，包括束向投影法 Channel modeling、新的理论性表现 bound、高分辨率 Channel parameter estimation 和基于圆投返回时间（RTT）协议的地理位置测定。我们首先 deriv a novel, approximate Cramér-Rao bound（CRB）on the connected road user（CRU）position，直接考虑多path interference, path merging, 和 RTT协议。通过维度分解和ESPRIT方法，我们提议高分辨率 Channel parameter estimation algorithm，专门适用于密集多path V2X侧链环境，探测多path component（MPC）和Extract line-of-sight（LoS）参数。最后，通过实际的束向投影数据和天线 Pattern，我们进行了全面的 Channel estimation和位置测定性能评估，结果表明在sub-6GHz V2X中可以实现sub-meter精度。
</details></li>
</ul>
<hr>
<h2 id="Electrical-Fault-Localisation-Over-a-Distributed-Parameter-Transmission-Line"><a href="#Electrical-Fault-Localisation-Over-a-Distributed-Parameter-Transmission-Line" class="headerlink" title="Electrical Fault Localisation Over a Distributed Parameter Transmission Line"></a>Electrical Fault Localisation Over a Distributed Parameter Transmission Line</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13359">http://arxiv.org/abs/2310.13359</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Selvaratnam, Amritam Das, Henrik Sandberg</li>
<li>for: 本研究旨在地方化电力线路上的故障寻址。</li>
<li>methods: 本文采用了频域方法来参数估算无穷维度线动系统中的一个空间变量。由于故障时间未知，且电压和电流只测量在一端，因此需要从后灾后脉冲中提取距离信息。在考虑高频脉冲行为的情况下，线动系统直接使用了电报员方程，而不是常用的粗略参数近似。首先，管理方程被非级数化，以避免稍偏倦。然后，通过非线性最小二乘优化搜索故障位置。</li>
<li>results: 研究结果表明，该算法可以准确地寻址故障位置。需要故障频率、测量器频率和模拟时间步骤的要求也被提出。<details>
<summary>Abstract</summary>
Motivated by the need to localise faults along electrical power lines, this paper adopts a frequency-domain approach to parameter estimation for an infinite-dimensional linear dynamical system with one spatial variable. Since the time of the fault is unknown, and voltages and currents are measured at only one end of the line, distance information must be extracted from the post-fault transients. To properly account for high-frequency transient behaviour, the line dynamics is modelled directly by the Telegrapher's equation, rather than the more commonly used lumped-parameter approximations. First, the governing equations are non-dimensionalised to avoid ill-conditioning. A closed-form expression for the transfer function is then derived. Finally, nonlinear least-squares optimisation is employed to search for the fault location. Requirements on fault bandwidth, sensor bandwidth and simulation time-step are also presented. The result is a novel end-to-end algorithm for data generation and fault localisation, the effectiveness of which is demonstrated via simulation.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Motivated by the need to localise faults along electrical power lines, this paper adopts a frequency-domain approach to parameter estimation for an infinite-dimensional linear dynamical system with one spatial variable. Since the time of the fault is unknown, and voltages and currents are measured at only one end of the line, distance information must be extracted from the post-fault transients. To properly account for high-frequency transient behaviour, the line dynamics is modelled directly by the Telegrapher's equation, rather than the more commonly used lumped-parameter approximations. First, the governing equations are non-dimensionalised to avoid ill-conditioning. A closed-form expression for the transfer function is then derived. Finally, nonlinear least-squares optimisation is employed to search for the fault location. Requirements on fault bandwidth, sensor bandwidth and simulation time-step are also presented. The result is a novel end-to-end algorithm for data generation and fault localisation, the effectiveness of which is demonstrated via simulation."中文简体版：这篇论文推动了为电力线路检测缺陷的需要，采用频域方法来估算无穷维度线性动力系统中的参数。由于缺陷时间未知，且电压和电流只 mesure在一端，因此需要从后缺陷脉冲中提取距离信息。为了正确地考虑高频脉冲行为，本文直接使用电报方程来模拟线动态，而不是通常使用简化参数近似。首先， governing equations 被非约数学化，以避免不合理的条件。然后， closed-form 表达式 для transfer function  derivation。最后，使用非线性最小二乘优化方法来搜索缺陷位置。 besides， paper 还提出了缺陷频率、感知频率和模拟步长的要求。 results 表明了这种 novel end-to-end 算法的有效性，通过 simulation  validate。
</details></li>
</ul>
<hr>
<h2 id="Reconfigurable-Intelligent-Sensing-Surface-aided-Wireless-Powered-Communication-Networks-A-Sensing-Then-Reflecting-Approach"><a href="#Reconfigurable-Intelligent-Sensing-Surface-aided-Wireless-Powered-Communication-Networks-A-Sensing-Then-Reflecting-Approach" class="headerlink" title="Reconfigurable Intelligent Sensing Surface aided Wireless Powered Communication Networks: A Sensing-Then-Reflecting Approach"></a>Reconfigurable Intelligent Sensing Surface aided Wireless Powered Communication Networks: A Sensing-Then-Reflecting Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13335">http://arxiv.org/abs/2310.13335</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Luo, Jie Hu, Luping Xiang, Kun Yang</li>
<li>for: 这个论文旨在提出一个可重新配置的智能感应面 (RISS)，让它同时进行反射和方向来测量信号的来源方向信息。这个设计可以实现对于普通通道估计的减少，并且让 RISS 成为独立于 Hybrid Access Point (HAP) 的独立系统。</li>
<li>methods: 这个论文使用了 RISS 自动估计 uplink 信号的来源方向信息，并且使用 HAP 的慢测量 DOA 信息来反射信号。在下行传输中，RISS 更新 HAP 的 DOA 信息，并设计反射能量信号的相位基于最新的用户 DOA 信息。</li>
<li>results: 论文包括了一个完整的性能分析，涵盖系统设计、协议细节、接收性能和 RISS 部署建议。我们 derive 了一个关于系统性能的关闭式表示，并使用了 moments-matching 技术来计算用户接收能量的Statistical 分布。我们还提供了一个建议的传输功率，以满足一定的损失概率和能量阈值。numerical 结果显示，提案的系统比传统 counterpart 高2.3 dB 和 4.7 dB 在 Rician 因子 $\kappa_h &#x3D; \kappa_G &#x3D; 1$ 和 $\kappa_h &#x3D; \kappa_G &#x3D; 10$ 下，分别。<details>
<summary>Abstract</summary>
This paper presents a reconfigurable intelligent sensing surface (RISS) that combines passive and active elements to achieve simultaneous reflection and direction of arrival (DOA) estimation tasks. By utilizing DOA information from the RISS instead of conventional channel estimation, the pilot overhead is reduced and the RISS becomes independent of the hybrid access point (HAP), enabling efficient operation. Specifically, the RISS autonomously estimates the DOA of uplink signals from single-antenna users and reflects them using the HAP's slowly varying DOA information. During downlink transmission, it updates the HAP's DOA information and designs the reflection phase of energy signals based on the latest user DOA information. The paper includes a comprehensive performance analysis, covering system design, protocol details, receiving performance, and RISS deployment suggestions. We derive a closed-form expression to analyze system performance under DOA errors, and calculate the statistical distribution of user received energy using the moment-matching technique. We provide a recommended transmit power to meet a specified outage probability and energy threshold. Numerical results demonstrate that the proposed system outperforms the conventional counterpart by 2.3 dB and 4.7 dB for Rician factors $\kappa_h=\kappa_G=1$ and $\kappa_h=\kappa_G=10$, respectively.
</details>
<details>
<summary>摘要</summary>
The paper includes a comprehensive performance analysis, covering system design, protocol details, receiving performance, and RISS deployment suggestions. A closed-form expression is derived to analyze system performance under DOA errors, and the statistical distribution of user received energy is calculated using the moment-matching technique. Numerical results show that the proposed system outperforms the conventional counterpart by 2.3 dB and 4.7 dB for Rician factors $\kappa_h=\kappa_G=1$ and $\kappa_h=\kappa_G=10$, respectively.Here is the translation in Simplified Chinese:这篇论文介绍了一种可重配置的智能感知面（RISS），该面combines passive和active元素以实现同时反射和方向来源估计任务。通过使用RISS中的DOA信息而不是传统的通道估计，可以降低飞行器的干扰负荷和RISS与混合访问点（HAP）之间的依赖关系，从而实现高效的操作。具体来说，RISS可以自主估计单antenna用户信号的DOA，并使用HAP的慢变DOA信息反射这些信号。在下行传输中，RISS会更新HAP的DOA信息，并根据最新的用户DOA信息设计反射能量信号的相对阶段。论文包括了系统设计、协议细节、接收性能和RISS部署建议等方面的全面性表现分析。我们得出了关于系统性能下DOA错误的关闭式表达式，并使用 momento匹配技术计算用户接收能量的统计分布。我们还提供了根据 specify outage probability和能量阈值来确定的建议发射功率。数值结果显示，提案的系统比传统counterpart高于2.3 dB和4.7 dB дляRician因子$\kappa_h=\kappa_G=1$和$\kappa_h=\kappa_G=10$,分别。
</details></li>
</ul>
<hr>
<h2 id="An-Overview-on-IEEE-802-11bf-WLAN-Sensing"><a href="#An-Overview-on-IEEE-802-11bf-WLAN-Sensing" class="headerlink" title="An Overview on IEEE 802.11bf: WLAN Sensing"></a>An Overview on IEEE 802.11bf: WLAN Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17661">http://arxiv.org/abs/2310.17661</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Du, Haocheng Hua, Hailiang Xie, Xianxin Song, Zhonghao Lyu, Mengshi Hu, Narengerile, Yan Xin, Stephen McCann, Michael Montemurro, Tony Xiao Han, Jie Xu<br>for:* 本文旨在提供IEEE 802.11bf标准的全面概述，包括其形成和标准化时间表、WLAN感知应用场景和相关关键性能指标要求、以及先前基于通信启用WLAN标准的WLAN感知研究的限制。methods:* 本文详细介绍了IEEE 802.11bf标准的感知框架和过程，包括低于7GHz和指向多吉比特（DMG）感知在60GHz的评估方法，并讨论了它们之间的共同特点、相似性和差异。results:* 本文介绍了IEEE 802.11bf标准的多个候选技术特征，包括波形&#x2F;序列设计、反馈类型、量化和压缩技术。同时，文章还详细介绍了IEEE 802.11bf TG在评估中使用的方法和通道模型。<details>
<summary>Abstract</summary>
With recent advancements, the wireless local area network (WLAN) or wireless fidelity (Wi-Fi) technology has been successfully utilized to realize sensing functionalities such as detection, localization, and recognition. However, the WLANs standards are developed mainly for the purpose of communication, and thus may not be able to meet the stringent requirements for emerging sensing applications. To resolve this issue, a new Task Group (TG), namely IEEE 802.11bf, has been established by the IEEE 802.11 working group, with the objective of creating a new amendment to the WLAN standard to meet advanced sensing requirements while minimizing the effect on communications. This paper provides a comprehensive overview on the up-to-date efforts in the IEEE 802.11bf TG. First, we introduce the definition of the 802.11bf amendment and its formation and standardization timeline. Next, we discuss the WLAN sensing use cases with the corresponding key performance indicator (KPI) requirements. After reviewing previous WLAN sensing research based on communication-oriented WLAN standards, we identify their limitations and underscore the practical need for the new sensing-oriented amendment in 802.11bf. Furthermore, we discuss the WLAN sensing framework and procedure used for measurement acquisition, by considering both sensing at sub-7GHz and directional multi-gigabit (DMG) sensing at 60 GHz, respectively, and address their shared features, similarities, and differences. In addition, we present various candidate technical features for IEEE 802.11bf, including waveform/sequence design, feedback types, as well as quantization and compression techniques. We also describe the methodologies and the channel modeling used by the IEEE 802.11bf TG for evaluation. Finally, we discuss the challenges and future research directions to motivate more research endeavors towards this field in details.
</details>
<details>
<summary>摘要</summary>
随着技术的发展，无线地区网络（WLAN）或无线准确度（Wi-Fi）技术已经成功地实现感知功能，如探测、地理位置和识别。但是，WLAN标准主要是为沟通而开发，因此可能无法满足新兴感知应用的严格要求。为解决这个问题，IEEE 802.11bf 任务组（TG）在IEEE 802.11工作组内成立，目标是创建一个新的修订，以满足高级感知应用的需求，同时尽量不影响通信。本文提供了IEEE 802.11bf TG 的准确情况。首先，我们介绍了802.11bf 修订的定义和成立时间表。然后，我们讨论了WLAN 感知应用的使用场景和相关关键性能指标（KPI）要求。我们还回顾了基于通信导向的WLAN 感知研究，并识别了其局限性，强调了实际需要新的感知导向修订。此外，我们介绍了WLAN 感知框架和测量获取方法，包括低于7GHz 的感知和60GHz 的指向多吉比特（DMG）感知，并讨论了它们之间的共同特点、相似之处和差异。此外，我们还提出了IEEE 802.11bf 修订的各种技术特征，包括波形/序列设计、反馈类型、量化和压缩技术。我们还描述了IEEE 802.11bf TG 使用的评估方法和通信核心模型。最后，我们讨论了IEEE 802.11bf 修订的挑战和未来研究方向，以便更多的研究资源投入到这一领域。
</details></li>
</ul>
<hr>
<h2 id="Time-Modulated-Intelligent-Reflecting-Surface-for-Waveform-Security"><a href="#Time-Modulated-Intelligent-Reflecting-Surface-for-Waveform-Security" class="headerlink" title="Time-Modulated Intelligent Reflecting Surface for Waveform Security"></a>Time-Modulated Intelligent Reflecting Surface for Waveform Security</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13210">http://arxiv.org/abs/2310.13210</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaoyi Xu, Athina Petropulu</li>
<li>for: 提高OFDM传输器的波形安全性</li>
<li>methods: 使用智能反射表（IRS）和时间模拟（TM）技术</li>
<li>results: 提高了对授权接收器的通信信息保护，并对所有其他方向的信息进行混淆<details>
<summary>Abstract</summary>
We consider an OFDM transmitter aided by an intelligent reflecting surface (IRS) and propose a novel approach to enhance waveform security by employing time modulation (TM) at the IRS side. By controlling the periodic TM pattern of the IRS elements, the system is designed to preserve communication information towards an authorized recipient and scramble the information towards all other directions. We introduce two modes of TM pattern control: the linear mode, in which we design common TM parameters for entire rows or columns of the IRS, and the planar mode, where we design TM parameters for each individual IRS unit. Due to the required fewer switches, the linear mode is easier to implement as compared to the planar mode. However, the linear model results in a beampattern that has sidelobes, over which the transmitted information is not sufficiently scrambled. We show that the sidelobes of the linear mode can be suppressed by exploiting the high diversity available in that mode.
</details>
<details>
<summary>摘要</summary>
我们考虑了一个使用智能反射 superficie（IRS）的OFDM发送器，并提出了一种新的方法来增强波形安全性。我们在IRS сторо面使用时间修订（TM）来控制Periodic TM模式。通过控制IRS元素的TM模式，我们设计了一种保护通信信息向授权接收方和混杂所有其他方向的系统。我们引入了两种TM模式控制方式：直线模式和平面模式。直线模式中，我们设计了整个行或列的TM参数，而平面模式中，我们设计了每个IRS单元的TM参数。由于需要 fewer switches，直线模式更容易实现，但是它会导致有侧峰， transmitter information在这些侧峰上不够混杂。我们表明，直线模式中的侧峰可以通过利用高多样性来降低。
</details></li>
</ul>
<hr>
<h2 id="Foundational-Techniques-for-Wireless-Communications-Channel-Coding-Modulation-and-Equalization"><a href="#Foundational-Techniques-for-Wireless-Communications-Channel-Coding-Modulation-and-Equalization" class="headerlink" title="Foundational Techniques for Wireless Communications: Channel Coding, Modulation, and Equalization"></a>Foundational Techniques for Wireless Communications: Channel Coding, Modulation, and Equalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13209">http://arxiv.org/abs/2310.13209</a></li>
<li>repo_url: None</li>
<li>paper_authors: Solomon McKiernan</li>
<li>for: 这篇论文探讨了提高无线通信系统的基础技术，包括编码方法、模拟方案和通道平衡技术。</li>
<li>methods: 该论文使用了行业标准的 simulate 工具来评估不同通道条件下这些技术的性能。扩展码、截断码和不截断码等编码方法被评估以确保数据传输的可靠性。多种模拟方案，如相位偏移调制（PSK）和相位幅度调制（QAM），也被评估。</li>
<li>results: 该论文通过评估不同通道条件下这些技术的性能，提供了实用的透彻性和现代无线通信系统中这些技术的重要性。线性和决策反馈平衡技术被评估以mitigate通道质量下的效应。<details>
<summary>Abstract</summary>
This paper analyses foundational techniques for improving wireless communication systems, including coding methods, modulation schemes, and channel equalization. Using industry-standard simulation tools, the paper evaluates the performance of these techniques under different channel conditions. Convolutional codes, punctured and unpunctured, are assessed for reliable data transfer. The suitability of various modulation schemes, such as Phase Shift Keying (PSK) and Quadrature Amplitude Modulation (QAM), are examined. Linear and decision-feedback equalization techniques are evaluated for mitigating the effects of channel impairments. The paper provides practical insights into the implementation of these techniques, emphasizing their importance in modern wireless communication systems.
</details>
<details>
<summary>摘要</summary>
Convolutional codes, punctured and unpunctured, are assessed for reliable data transfer. The suitability of various modulation schemes, such as Phase Shift Keying (PSK) and Quadrature Amplitude Modulation (QAM), are examined. Linear and decision-feedback equalization techniques are evaluated for mitigating the effects of channel impairments.The paper provides practical insights into the implementation of these techniques, emphasizing their importance in modern wireless communication systems.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/20/eess.SP_2023_10_20/" data-id="clpxp04br01gofm88hcpzbkvq" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_10_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/19/cs.SD_2023_10_19/" class="article-date">
  <time datetime="2023-10-19T15:00:00.000Z" itemprop="datePublished">2023-10-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/19/cs.SD_2023_10_19/">cs.SD - 2023-10-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Uncertainty-Quantification-of-Bandgaps-in-Acoustic-Metamaterials-with-Stochastic-Geometric-Defects-and-Material-Properties"><a href="#Uncertainty-Quantification-of-Bandgaps-in-Acoustic-Metamaterials-with-Stochastic-Geometric-Defects-and-Material-Properties" class="headerlink" title="Uncertainty Quantification of Bandgaps in Acoustic Metamaterials with Stochastic Geometric Defects and Material Properties"></a>Uncertainty Quantification of Bandgaps in Acoustic Metamaterials with Stochastic Geometric Defects and Material Properties</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12869">http://arxiv.org/abs/2310.12869</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Zhang, Rayehe Karimi Mahabadi, Cynthia Rudin, Johann Guilleminot, L. Catherine Brinson</li>
<li>for: 本研究使用不确定性评估技术，即光谱投影和多项式混沌扩展，减少声学材料特性和几何缺陷的抽象响应频谱的采样需求。</li>
<li>methods: 本研究使用了一种可读性高、分辨率独立的方法对几何缺陷进行编码，并将其与 Монте卡洛、规则评估和稀Grid采样相结合，以生成高精度的输出空间概率分布。</li>
<li>results: 研究发现，通过 combining Monte Carlo, quadrature rule, and sparse grid sampling with surrogate model fitting, 在1D和7D输入空间场景中可以实现单位采样减少至10^0和10^1，同时保持高精度的输出空间概率分布。<details>
<summary>Abstract</summary>
This paper studies the utility of techniques within uncertainty quantification, namely spectral projection and polynomial chaos expansion, in reducing sampling needs for characterizing acoustic metamaterial dispersion band responses given stochastic material properties and geometric defects. A novel method of encoding geometric defects in an interpretable, resolution independent is showcased in the formation of input space probability distributions. Orders of magnitude sampling reductions down to $\sim10^0$ and $\sim10^1$ are achieved in the 1D and 7D input space scenarios respectively while maintaining accurate output space probability distributions through combining Monte Carlo, quadrature rule, and sparse grid sampling with surrogate model fitting.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/19/cs.SD_2023_10_19/" data-id="clpxp045g0119fm88d5exel4h" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_10_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/19/eess.AS_2023_10_19/" class="article-date">
  <time datetime="2023-10-19T14:00:00.000Z" itemprop="datePublished">2023-10-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/19/eess.AS_2023_10_19/">eess.AS - 2023-10-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Deep-Beamforming-for-Speech-Enhancement-and-Speaker-Localization-with-an-Array-Response-Aware-Loss-Function"><a href="#Deep-Beamforming-for-Speech-Enhancement-and-Speaker-Localization-with-an-Array-Response-Aware-Loss-Function" class="headerlink" title="Deep Beamforming for Speech Enhancement and Speaker Localization with an Array Response-Aware Loss Function"></a>Deep Beamforming for Speech Enhancement and Speaker Localization with an Array Response-Aware Loss Function</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12837">http://arxiv.org/abs/2310.12837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hsinyu Chang, Yicheng Hsu, Mingsian R. Bai</li>
<li>for: 提高噪音环境下的语音增强和发音定位</li>
<li>methods: 使用深度征文搜索网络（DNN）基于核函数的扩展 beamformer，并使用一种新的ARRAY RespOnse-aWare（ARROW）损失函数来训练</li>
<li>results: 实验结果表明，使用DNN基于核函数的扩展 beamformer，并通过监督学习使用ARROW损失函数，能够同时实现语音增强和发音定位，并且在语音增强和发音定位方面获得了比基eline两个优化的性能<details>
<summary>Abstract</summary>
Recent research advances in deep neural network (DNN)-based beamformers have shown great promise for speech enhancement under adverse acoustic conditions. Different network architectures and input features have been explored in estimating beamforming weights. In this paper, we propose a deep beamformer based on an efficient convolutional recurrent network (CRN) trained with a novel ARray RespOnse-aWare (ARROW) loss function. The ARROW loss exploits the array responses of the target and interferer by using the ground truth relative transfer functions (RTFs). The DNN-based beamforming system, trained with ARROW loss through supervised learning, is able to perform speech enhancement and speaker localization jointly. Experimental results have shown that the proposed deep beamformer, trained with the linearly weighted scale-invariant source-to-noise ratio (SI-SNR) and ARROW loss functions, achieves superior performance in speech enhancement and speaker localization compared to two baselines.
</details>
<details>
<summary>摘要</summary>
现代深度神经网络（DNN）基本束扩展器的研究进展已经显示了优秀的沟通条件下语音增强的扩展。不同的网络架构和输入特征都被探索以计算扩展权重。在这篇论文中，我们提议一种高效的卷积环recurrent neural network（CRN），通过一种新的ARRAY RespOnse-aWare（ARROW）损失函数进行训练。ARROW损失函数利用目标和干扰者的陷阱响应函数（RTF）。DNN基本束扩展系统，通过监督学习，可以同时进行语音增强和speakerlocalization。实验结果表明，我们提议的深度束扩展器，通过线性权重等比例不变的源噪比（SI-SNR）和ARROW损失函数，在语音增强和speakerlocalization方面比基eline两个参考模型表现出优秀的性能。
</details></li>
</ul>
<hr>
<h2 id="On-Feature-Importance-and-Interpretability-of-Speaker-Representations"><a href="#On-Feature-Importance-and-Interpretability-of-Speaker-Representations" class="headerlink" title="On Feature Importance and Interpretability of Speaker Representations"></a>On Feature Importance and Interpretability of Speaker Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12599">http://arxiv.org/abs/2310.12599</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frederik Rautenberg, Michael Kuhlmann, Jana Wiechmann, Fritz Seebauer, Petra Wagner, Reinhold Haeb-Umbach</li>
<li>for: 这个论文主要针对的是无监督 speech 分解，具体来说是将快变量从慢变量组成部分中分离出来。</li>
<li>methods: 该论文使用的方法是对speaker embedding vector（慢变量组成部分的嵌入向量）进行分析，以确定它们所捕捉的特性是什么。</li>
<li>results: 研究发现，certain speaker-specific acoustic-phonetic properties可以很好地从speaker embedding中预测，而investigated more abstract voice quality features则无法预测。<details>
<summary>Abstract</summary>
Unsupervised speech disentanglement aims at separating fast varying from slowly varying components of a speech signal. In this contribution, we take a closer look at the embedding vector representing the slowly varying signal components, commonly named the speaker embedding vector. We ask, which properties of a speaker's voice are captured and investigate to which extent do individual embedding vector components sign responsible for them, using the concept of Shapley values. Our findings show that certain speaker-specific acoustic-phonetic properties can be fairly well predicted from the speaker embedding, while the investigated more abstract voice quality features cannot.
</details>
<details>
<summary>摘要</summary>
转换文本到简化中文：<</SYS>>无监督演化语音分解目标在分解快变化和慢变化语音信号中的快变化部分。在这篇论文中，我们更加仔细地研究表示慢变化语音信号组成部分的嵌入向量，通常被称为说话人嵌入向量。我们问，说话人的声音特征是哪些被捕捉，并investigate到哪些嵌入向量组件负责它们，使用基于Shapley值的概念。我们的发现表明， certain speaker-specific acoustic-phonetic properties可以很好地从说话人嵌入中预测，而investigated的更抽象的声音质量特征则不能。
</details></li>
</ul>
<hr>
<h2 id="A-New-Time-Series-Similarity-Measure-and-Its-Smart-Grid-Applications"><a href="#A-New-Time-Series-Similarity-Measure-and-Its-Smart-Grid-Applications" class="headerlink" title="A New Time Series Similarity Measure and Its Smart Grid Applications"></a>A New Time Series Similarity Measure and Its Smart Grid Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12399">http://arxiv.org/abs/2310.12399</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Yuan, S. Ali Pourmousavi, Wen L. Soong, Andrew J. Black, Jon A. R. Liisberg, Julian Lemos-Vinasco<br>for: This paper aims to provide a new distance measure for comparing electricity usage patterns in smart grid applications, addressing the limitations of existing measures such as Euclidean Distance (ED) and Dynamic Time Warping (DTW).methods: The proposed method consists of two phases: (1) amplitude-based distance and (2) temporal-based distance, which quantify the effort required to reshape one time series into another considering both amplitude and temporal changes.results: The proposed distance measure outperforms ED and DTW in three smart grid applications: (1) identifying the best load scheduling strategy, (2) detecting anomalous days with irregular electricity usage, and (3) determining electricity users’ behind-the-meter (BTM) equipment.<details>
<summary>Abstract</summary>
Many smart grid applications involve data mining, clustering, classification, identification, and anomaly detection, among others. These applications primarily depend on the measurement of similarity, which is the distance between different time series or subsequences of a time series. The commonly used time series distance measures, namely Euclidean Distance (ED) and Dynamic Time Warping (DTW), do not quantify the flexible nature of electricity usage data in terms of temporal dynamics. As a result, there is a need for a new distance measure that can quantify both the amplitude and temporal changes of electricity time series for smart grid applications, e.g., demand response and load profiling. This paper introduces a novel distance measure to compare electricity usage patterns. The method consists of two phases that quantify the effort required to reshape one time series into another, considering both amplitude and temporal changes. The proposed method is evaluated against ED and DTW using real-world data in three smart grid applications. Overall, the proposed measure outperforms ED and DTW in accurately identifying the best load scheduling strategy, anomalous days with irregular electricity usage, and determining electricity users' behind-the-meter (BTM) equipment.
</details>
<details>
<summary>摘要</summary>
Many smart grid applications involve data mining, clustering, classification, identification, and anomaly detection, among others. These applications primarily depend on the measurement of similarity, which is the distance between different time series or subsequences of a time series. The commonly used time series distance measures, namely Euclidean Distance (ED) and Dynamic Time Warping (DTW), do not quantify the flexible nature of electricity usage data in terms of temporal dynamics. As a result, there is a need for a new distance measure that can quantify both the amplitude and temporal changes of electricity time series for smart grid applications, e.g., demand response and load profiling. This paper introduces a novel distance measure to compare electricity usage patterns. The method consists of two phases that quantify the effort required to reshape one time series into another, considering both amplitude and temporal changes. The proposed method is evaluated against ED and DTW using real-world data in three smart grid applications. Overall, the proposed measure outperforms ED and DTW in accurately identifying the best load scheduling strategy, anomalous days with irregular electricity usage, and determining electricity users' behind-the-meter (BTM) equipment.Simplified Chinese translation:许多智能Grid应用程序包括数据挖掘、划分、类型分类、识别和异常检测等，这些应用程序主要依赖测量相似性，即不同时间序列或时间序列 subsequences 之间的距离。常用的时间序列距离度量方法包括欧几何距离 (ED) 和时间扭曲摸索 (DTW)，但这些方法不能Quantify electricity usage data的时间动态特性。因此，有一需要一个新的距离度量方法，可以Quantify electricity usage时间序列中的振幅和时间变化。这篇论文介绍了一种新的距离度量方法，用于比较电力使用模式。该方法包括两个阶段，第一阶段是Quantify将一个时间序列变换成另一个时间序列所需的努力，第二阶段是Quantify这个时间序列的振幅和时间变化。提议的方法在三个智能Grid应用程序中评估了与ED和DTW进行比较，并表明该方法在确定最佳负荷调度策略、异常日期和电力用户的后勤设备（BTM）等方面具有更高的准确性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/19/eess.AS_2023_10_19/" data-id="clpxp0476015kfm88747c5akv" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_10_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/19/cs.CV_2023_10_19/" class="article-date">
  <time datetime="2023-10-19T13:00:00.000Z" itemprop="datePublished">2023-10-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/19/cs.CV_2023_10_19/">cs.CV - 2023-10-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Car-Model-Identification-System-for-Streamlining-the-Automobile-Sales-Process"><a href="#A-Car-Model-Identification-System-for-Streamlining-the-Automobile-Sales-Process" class="headerlink" title="A Car Model Identification System for Streamlining the Automobile Sales Process"></a>A Car Model Identification System for Streamlining the Automobile Sales Process</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13198">http://arxiv.org/abs/2310.13198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Said Togru, Marco Moldovan</li>
<li>for:  automatizing car model and make identification from images to improve online car-selling platform efficiency</li>
<li>methods:  employing various efficient network architectures (CNNs, ViTs, hybrid models) and refining performance through data augmentation, fine-tuning pretrained models, and hyperparameter tuning</li>
<li>results:  achieving an accuracy of 81.97% with the EfficientNet (V2 b2) architecture, promising enhanced user experiences across car-selling websites<details>
<summary>Abstract</summary>
This project presents an automated solution for the efficient identification of car models and makes from images, aimed at streamlining the vehicle listing process on online car-selling platforms. Through a thorough exploration encompassing various efficient network architectures including Convolutional Neural Networks (CNNs), Vision Transformers (ViTs), and hybrid models, we achieved a notable accuracy of 81.97% employing the EfficientNet (V2 b2) architecture. To refine performance, a combination of strategies, including data augmentation, fine-tuning pretrained models, and extensive hyperparameter tuning, were applied. The trained model offers the potential for automating information extraction, promising enhanced user experiences across car-selling websites.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="LeTFuser-Light-weight-End-to-end-Transformer-Based-Sensor-Fusion-for-Autonomous-Driving-with-Multi-Task-Learning"><a href="#LeTFuser-Light-weight-End-to-end-Transformer-Based-Sensor-Fusion-for-Autonomous-Driving-with-Multi-Task-Learning" class="headerlink" title="LeTFuser: Light-weight End-to-end Transformer-Based Sensor Fusion for Autonomous Driving with Multi-Task Learning"></a>LeTFuser: Light-weight End-to-end Transformer-Based Sensor Fusion for Autonomous Driving with Multi-Task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13135">http://arxiv.org/abs/2310.13135</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pagand/e2etransfuser">https://github.com/pagand/e2etransfuser</a></li>
<li>paper_authors: Pedram Agand, Mohammad Mahdavian, Manolis Savva, Mo Chen</li>
<li>for: 本研究目的是解决在综合感知技术下的自动驾驶中，使用现有的混合感知技术无法处理复杂的动态代理人问题。</li>
<li>methods: 我们介绍了LeTFuser算法，它是基于trasnformer的RGB-D镜头混合算法，用于同时进行感知和控制任务。我们的模型包括两个模块： перception模块负责对RGB-D镜头数据进行编码，并实施semantic segmentation、semantic depth cloud mapping（SDC）和交通灯状态识别等任务；控制模块使用编码特征和补充数据（包括粗略的 simulate器、多种测量）来预测车辆的控制方向。</li>
<li>results: 我们在CARLA simulator上评估了模型，并进行了与其他模型的比较分析。我们发现，LeTFuser在各种情况下（包括正常和反对情况）都能够提供更高的性能和稳定性。<details>
<summary>Abstract</summary>
In end-to-end autonomous driving, the utilization of existing sensor fusion techniques for imitation learning proves inadequate in challenging situations that involve numerous dynamic agents. To address this issue, we introduce LeTFuser, a transformer-based algorithm for fusing multiple RGB-D camera representations. To perform perception and control tasks simultaneously, we utilize multi-task learning. Our model comprises of two modules, the first being the perception module that is responsible for encoding the observation data obtained from the RGB-D cameras. It carries out tasks such as semantic segmentation, semantic depth cloud mapping (SDC), and traffic light state recognition. Our approach employs the Convolutional vision Transformer (CvT) \cite{wu2021cvt} to better extract and fuse features from multiple RGB cameras due to local and global feature extraction capability of convolution and transformer modules, respectively. Following this, the control module undertakes the decoding of the encoded characteristics together with supplementary data, comprising a rough simulator for static and dynamic environments, as well as various measurements, in order to anticipate the waypoints associated with a latent feature space. We use two methods to process these outputs and generate the vehicular controls (e.g. steering, throttle, and brake) levels. The first method uses a PID algorithm to follow the waypoints on the fly, whereas the second one directly predicts the control policy using the measurement features and environmental state. We evaluate the model and conduct a comparative analysis with recent models on the CARLA simulator using various scenarios, ranging from normal to adversarial conditions, to simulate real-world scenarios. Our code is available at \url{https://github.com/pagand/e2etransfuser/tree/cvpr-w} to facilitate future studies.
</details>
<details>
<summary>摘要</summary>
在末端自动驾驶中，使用现有的感知融合技术进行模仿学习显示不够有效，特别是在包含多个动态代理的复杂情况下。为解决这个问题，我们提出了LeTFuser算法，它基于变换器来融合多个RGB-D相机表示。通过多任务学习，我们同时进行感知和控制任务。我们的模型包括两个模块：第一个是感知模块，负责对RGB-D相机获得的观察数据进行编码。它完成了semantic segmentation、semantic depth cloud mapping（SDC）和交通灯状态识别等任务。我们采用了Convolutional Vision Transformer（CvT） \cite{wu2021cvt} 来更好地提取和融合多个RGB相机中的特征，因为它具有局部和全局特征提取能力。接着，控制模块根据编码特征和补充数据（包括粗略的模拟器、静态和动态环境的测量数据）进行解码，以预测 vehicular控制（例如，车辆的油门、加速和刹车）水平。我们使用两种方法处理输出并生成车辆控制水平：一种使用PID算法跟踪卫星点，另一种直接预测控制策略使用测量特征和环境状态。我们在CARLA模拟器上对模型进行评估，并对其进行与最近模型的比较分析，使用多种情况，从普通到反对情况，来模拟真实世界情况。我们的代码可以在\url{https://github.com/pagand/e2etransfuser/tree/cvpr-w}中找到，以便未来的研究。
</details></li>
</ul>
<hr>
<h2 id="RSAdapter-Adapting-Multimodal-Models-for-Remote-Sensing-Visual-Question-Answering"><a href="#RSAdapter-Adapting-Multimodal-Models-for-Remote-Sensing-Visual-Question-Answering" class="headerlink" title="RSAdapter: Adapting Multimodal Models for Remote Sensing Visual Question Answering"></a>RSAdapter: Adapting Multimodal Models for Remote Sensing Visual Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13120">http://arxiv.org/abs/2310.13120</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuduo Wang, Pedram Ghamisi</li>
<li>for: 本研究旨在提高Remote Sensing（RS）Visual Question Answering（VQA）中的效率和 parameter efficiency，尤其是在使用 transformer 模型时。</li>
<li>methods: 本研究提出了一种名为 RSAdapter 的新方法，它包括两个关键组件：并行适配器和每个完全连接（FC）层后的额外线性变换层。这种方法不仅提高了适配预训练多Modal模型的能力，而且在推理时可以将线性变换层的参数与前一层的FC层集成，从而降低推理成本。</li>
<li>results: 在三个不同的 RS-VQA 数据集上进行了广泛的实验，并在所有三个数据集上达到了最佳结果。<details>
<summary>Abstract</summary>
In recent years, with the rapid advancement of transformer models, transformer-based multimodal architectures have found wide application in various downstream tasks, including but not limited to Image Captioning, Visual Question Answering (VQA), and Image-Text Generation. However, contemporary approaches to Remote Sensing (RS) VQA often involve resource-intensive techniques, such as full fine-tuning of large models or the extraction of image-text features from pre-trained multimodal models, followed by modality fusion using decoders. These approaches demand significant computational resources and time, and a considerable number of trainable parameters are introduced. To address these challenges, we introduce a novel method known as RSAdapter, which prioritizes runtime and parameter efficiency. RSAdapter comprises two key components: the Parallel Adapter and an additional linear transformation layer inserted after each fully connected (FC) layer within the Adapter. This approach not only improves adaptation to pre-trained multimodal models but also allows the parameters of the linear transformation layer to be integrated into the preceding FC layers during inference, reducing inference costs. To demonstrate the effectiveness of RSAdapter, we conduct an extensive series of experiments using three distinct RS-VQA datasets and achieve state-of-the-art results on all three datasets. The code for RSAdapter will be available online at https://github.com/Y-D-Wang/RSAdapter.
</details>
<details>
<summary>摘要</summary>
Recently, with the rapid development of transformer models, transformer-based multimodal architectures have been widely applied in various downstream tasks, such as Image Captioning, Visual Question Answering (VQA), and Image-Text Generation. However, current approaches to Remote Sensing (RS) VQA often rely on resource-intensive techniques, such as full fine-tuning of large models or extracting image-text features from pre-trained multimodal models, followed by modality fusion using decoders. These approaches require significant computational resources and time, and a large number of trainable parameters are introduced. To address these challenges, we propose a novel method called RSAdapter, which prioritizes runtime and parameter efficiency. RSAdapter consists of two key components: the Parallel Adapter and an additional linear transformation layer inserted after each fully connected (FC) layer within the Adapter. This approach not only improves adaptation to pre-trained multimodal models but also allows the parameters of the linear transformation layer to be integrated into the preceding FC layers during inference, reducing inference costs. To demonstrate the effectiveness of RSAdapter, we conduct an extensive series of experiments using three distinct RS-VQA datasets and achieve state-of-the-art results on all three datasets. The code for RSAdapter will be available online at <https://github.com/Y-D-Wang/RSAdapter>.
</details></li>
</ul>
<hr>
<h2 id="DreamSpace-Dreaming-Your-Room-Space-with-Text-Driven-Panoramic-Texture-Propagation"><a href="#DreamSpace-Dreaming-Your-Room-Space-with-Text-Driven-Panoramic-Texture-Propagation" class="headerlink" title="DreamSpace: Dreaming Your Room Space with Text-Driven Panoramic Texture Propagation"></a>DreamSpace: Dreaming Your Room Space with Text-Driven Panoramic Texture Propagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13119">http://arxiv.org/abs/2310.13119</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ybbbbt/DreamSpace">https://github.com/ybbbbt/DreamSpace</a></li>
<li>paper_authors: Bangbang Yang, Wenqi Dong, Lin Ma, Wenbo Hu, Xiao Liu, Zhaopeng Cui, Yuewen Ma</li>
<li>for: 这个论文是为了解决3D场景纹理生成中的束缚问题，即在XR&#x2F;VR应用中，通过自由视点渲染实现高品质的场景纹理生成。</li>
<li>methods: 该论文提出了一种新的室内场景纹理框架，通过带有细节和Authentic空间协调的纹理生成技术，实现了文本驱动的纹理生成。该框架包括两个Texture alignment方法，一个是粗糙到细节的纹理生成方法，另一个是在不同视角下进行纹理匹配和生成。</li>
<li>results: 实验和在真实的室内场景上的应用表明，该框架可以生成高品质的纹理图像，并在VR头戴式设备上提供了有趣的体验。<details>
<summary>Abstract</summary>
Diffusion-based methods have achieved prominent success in generating 2D media. However, accomplishing similar proficiencies for scene-level mesh texturing in 3D spatial applications, e.g., XR/VR, remains constrained, primarily due to the intricate nature of 3D geometry and the necessity for immersive free-viewpoint rendering. In this paper, we propose a novel indoor scene texturing framework, which delivers text-driven texture generation with enchanting details and authentic spatial coherence. The key insight is to first imagine a stylized 360{\deg} panoramic texture from the central viewpoint of the scene, and then propagate it to the rest areas with inpainting and imitating techniques. To ensure meaningful and aligned textures to the scene, we develop a novel coarse-to-fine panoramic texture generation approach with dual texture alignment, which both considers the geometry and texture cues of the captured scenes. To survive from cluttered geometries during texture propagation, we design a separated strategy, which conducts texture inpainting in confidential regions and then learns an implicit imitating network to synthesize textures in occluded and tiny structural areas. Extensive experiments and the immersive VR application on real-world indoor scenes demonstrate the high quality of the generated textures and the engaging experience on VR headsets. Project webpage: https://ybbbbt.com/publication/dreamspace
</details>
<details>
<summary>摘要</summary>
diffusion-based methods 已经取得了2D媒体生成的显著成功，但在3D空间应用中，例如XR/VR中的场景级别的 mesh 纹理仍然受到限制，主要是因为3D geometery的复杂性和需要免费看角渲染。在这篇论文中，我们提出了一种新的室内场景纹理框架，可以通过文本驱动生成细节浩繁、真实空间准确的纹理。我们的关键发现是首先从场景的中心视点想象一个精细360度扩展的Texture，然后通过填充和模仿技术将其扩展到其他区域。为确保纹理与场景的意义和平行性，我们开发了一种新的粗略到细腻的扩展Texture生成方法，该方法考虑了场景的几何和纹理特征。在进行纹理填充时，我们设计了一种分离策略，通过在信息丰富的区域进行纹理填充，然后使用一种隐藏的假凝结网络来Synthesize纹理在受阻和小结构区域。我们的实验和基于真实室内场景的VR应用示出了生成的纹理的高质量和VR头戴设备上的有趣体验。项目网站：https://ybbbbt.com/publication/dreamspace
</details></li>
</ul>
<hr>
<h2 id="Streamlining-Brain-Tumor-Classification-with-Custom-Transfer-Learning-in-MRI-Images"><a href="#Streamlining-Brain-Tumor-Classification-with-Custom-Transfer-Learning-in-MRI-Images" class="headerlink" title="Streamlining Brain Tumor Classification with Custom Transfer Learning in MRI Images"></a>Streamlining Brain Tumor Classification with Custom Transfer Learning in MRI Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13108">http://arxiv.org/abs/2310.13108</a></li>
<li>repo_url: None</li>
<li>paper_authors: Javed Hossain, Md. Touhidul Islam, Md. Taufiqul Haque Khan Tusar</li>
<li>for: 这个研究的目的是为了使用Custom transfer learning networks来分类脑肿图像。</li>
<li>methods: 这个研究使用了一种自定义的卷积神经网络架构，包括VGG-19架构和额外的隐藏层，以提高计算效率。</li>
<li>results: 研究得到了96.42%的分类精度。<details>
<summary>Abstract</summary>
Brain tumors are increasingly prevalent, characterized by the uncontrolled spread of aberrant tissues in the brain, with almost 700,000 new cases diagnosed globally each year. Magnetic Resonance Imaging (MRI) is commonly used for the diagnosis of brain tumors and accurate classification is a critical clinical procedure. In this study, we propose an efficient solution for classifying brain tumors from MRI images using custom transfer learning networks. While several researchers have employed various pre-trained architectures such as RESNET-50, ALEXNET, VGG-16, and VGG-19, these methods often suffer from high computational complexity. To address this issue, we present a custom and lightweight model using a Convolutional Neural Network-based pre-trained architecture with reduced complexity. Specifically, we employ the VGG-19 architecture with additional hidden layers, which reduces the complexity of the base architecture but improves computational efficiency. The objective is to achieve high classification accuracy using a novel approach. Finally, the result demonstrates a classification accuracy of 96.42%.
</details>
<details>
<summary>摘要</summary>
脑肿增多，特征为脑内不良组织的无控制扩散，每年全球诊断新 случа数达700,000例。核磁共振成像（MRI）广泛用于脑肿诊断，精确分类是临床重要程序。在本研究中，我们提出一种高效的脑肿分类方法，使用自定义传输学习网络。虽然许多研究人员使用了不同的预训练模型，如RESNET-50、ALEXNET、VGG-16和VGG-19，但这些方法经常受到高计算复杂性的限制。为解决这个问题，我们提出了一种自定义和轻量级的模型，基于卷积神经网络预训练架构，减少了基础架构的复杂性，但提高了计算效率。目标是实现高精度分类。最终结果表明，分类精度达96.42%。
</details></li>
</ul>
<hr>
<h2 id="PatchCURE-Improving-Certifiable-Robustness-Model-Utility-and-Computation-Efficiency-of-Adversarial-Patch-Defenses"><a href="#PatchCURE-Improving-Certifiable-Robustness-Model-Utility-and-Computation-Efficiency-of-Adversarial-Patch-Defenses" class="headerlink" title="PatchCURE: Improving Certifiable Robustness, Model Utility, and Computation Efficiency of Adversarial Patch Defenses"></a>PatchCURE: Improving Certifiable Robustness, Model Utility, and Computation Efficiency of Adversarial Patch Defenses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13076">http://arxiv.org/abs/2310.13076</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chong Xiang, Tong Wu, Sihui Dai, Jonathan Petit, Suman Jana, Prateek Mittal</li>
<li>for: 这个论文是为了提出一种防御攻击patch attack的方法，以提高模型的可靠性和安全性。</li>
<li>methods: 该论文使用了一种名为PatchCURE的防御框架，该框架可以根据不同的计算效率和功能需求来调整防御性能。</li>
<li>results: 该论文的实验结果表明，PatchCURE可以在不同的计算效率和功能需求下提供优秀的防御性能，并且可以与现有的状态态攻击防御方法相比肉。<details>
<summary>Abstract</summary>
State-of-the-art defenses against adversarial patch attacks can now achieve strong certifiable robustness with a marginal drop in model utility. However, this impressive performance typically comes at the cost of 10-100x more inference-time computation compared to undefended models -- the research community has witnessed an intense three-way trade-off between certifiable robustness, model utility, and computation efficiency. In this paper, we propose a defense framework named PatchCURE to approach this trade-off problem. PatchCURE provides sufficient "knobs" for tuning defense performance and allows us to build a family of defenses: the most robust PatchCURE instance can match the performance of any existing state-of-the-art defense (without efficiency considerations); the most efficient PatchCURE instance has similar inference efficiency as undefended models. Notably, PatchCURE achieves state-of-the-art robustness and utility performance across all different efficiency levels, e.g., 16-23% absolute clean accuracy and certified robust accuracy advantages over prior defenses when requiring computation efficiency to be close to undefended models. The family of PatchCURE defenses enables us to flexibly choose appropriate defenses to satisfy given computation and/or utility constraints in practice.
</details>
<details>
<summary>摘要</summary>
现代防御技术可以实现强有条件的鲁棒性，但通常会增加10-100倍的推理时间成本，与未防御的模型相比。在这篇论文中，我们提出了一种防御框架名为PatchCURE，以解决这种三方贸易问题。PatchCURE提供了多个可调参数，allowing us to construct a family of defenses，其中最鲁棒的PatchCURE实例可以与任何现有的最佳防御相匹配，而不考虑效率考虑；而最有效的PatchCURE实例的推理效率与未防御模型几乎相同。此外，PatchCURE在不同的效率水平上都可以实现状态之最好的鲁棒性和用户性能。PatchCURE的家族防御可以在实践中选择合适的防御，满足给定的计算和/或用户约束。
</details></li>
</ul>
<hr>
<h2 id="Using-Logic-Programming-and-Kernel-Grouping-for-Improving-Interpretability-of-Convolutional-Neural-Networks"><a href="#Using-Logic-Programming-and-Kernel-Grouping-for-Improving-Interpretability-of-Convolutional-Neural-Networks" class="headerlink" title="Using Logic Programming and Kernel-Grouping for Improving Interpretability of Convolutional Neural Networks"></a>Using Logic Programming and Kernel-Grouping for Improving Interpretability of Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13073">http://arxiv.org/abs/2310.13073</a></li>
<li>repo_url: None</li>
<li>paper_authors: Parth Padalkar, Gopal Gupta</li>
<li>for: 这个论文的目的是提出一种神经符号学框架（NeSyFOLD-G），该框架使得深度学习神经网络（CNN）的下层层 kernel 的知识更加可读性。</li>
<li>methods: 这个框架使用 CNN 的最后一层 kernel 组成一个符号化规则集（rule-set），并使用 FOLD-SE-M 算法生成规则集。在生成规则集时，首先找到 CNN 中相似的 kernel 组，然后对每个 kernel 组进行 binarization，并将其作为 FOLD-SE-M 的输入数据。</li>
<li>results: 这个框架可以减少 FOLD-SE-M 生成的规则集的大小，从而提高了知识的可读性。此外，这个框架还可以将 CNN 的下层层 kernel 符号化，并将其映射到人类可理解的概念上。<details>
<summary>Abstract</summary>
Within the realm of deep learning, the interpretability of Convolutional Neural Networks (CNNs), particularly in the context of image classification tasks, remains a formidable challenge. To this end we present a neurosymbolic framework, NeSyFOLD-G that generates a symbolic rule-set using the last layer kernels of the CNN to make its underlying knowledge interpretable. What makes NeSyFOLD-G different from other similar frameworks is that we first find groups of similar kernels in the CNN (kernel-grouping) using the cosine-similarity between the feature maps generated by various kernels. Once such kernel groups are found, we binarize each kernel group's output in the CNN and use it to generate a binarization table which serves as input data to FOLD-SE-M which is a Rule Based Machine Learning (RBML) algorithm. FOLD-SE-M then generates a rule-set that can be used to make predictions. We present a novel kernel grouping algorithm and show that grouping similar kernels leads to a significant reduction in the size of the rule-set generated by FOLD-SE-M, consequently, improving the interpretability. This rule-set symbolically encapsulates the connectionist knowledge of the trained CNN. The rule-set can be viewed as a normal logic program wherein each predicate's truth value depends on a kernel group in the CNN. Each predicate in the rule-set is mapped to a concept using a few semantic segmentation masks of the images used for training, to make it human-understandable. The last layers of the CNN can then be replaced by this rule-set to obtain the NeSy-G model which can then be used for the image classification task. The goal directed ASP system s(CASP) can be used to obtain the justification of any prediction made using the NeSy-G model. We also propose a novel algorithm for labeling each predicate in the rule-set with the semantic concept(s) that its corresponding kernel group represents.
</details>
<details>
<summary>摘要</summary>
在深度学习领域，特别是图像分类任务中，卷积神经网络（CNN）的解释性仍然是一大挑战。为此，我们提出了一种神经符号学框架（NeSyFOLD-G），该框架使用 CNN 的最后一层核心来生成一个符号化规则集。与其他类似框架不同的是，我们首先在 CNN 中找到相似核心（kernel-grouping），并使用归一化矩阵来将每个核心组的输出binariz。然后，我们使用这些binarization表作为 FOLD-SE-M 算法的输入数据，并使其生成一个规则集。我们提出了一种新的核心分组算法，并证明了将相似核心分组可以减少 FOLD-SE-M 生成的规则集的大小，从而提高解释性。这个规则集可以被视为一个正逻辑程序，其中每个前置的真值取决于 CNN 中的核心组。每个前置在规则集中都可以被映射到一个概念，使其成为人类理解的。我们还提出了一种新的算法，用于在规则集中标注每个前置的 semantic 概念。最后，我们将 CNN 的最后一层替换为 NeSy-G 模型，并使用该模型进行图像分类任务。我们还可以使用 goal-directed ASP 系统（s(CASP)）来获取任务结果的证明。
</details></li>
</ul>
<hr>
<h2 id="Putting-the-Object-Back-into-Video-Object-Segmentation"><a href="#Putting-the-Object-Back-into-Video-Object-Segmentation" class="headerlink" title="Putting the Object Back into Video Object Segmentation"></a>Putting the Object Back into Video Object Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12982">http://arxiv.org/abs/2310.12982</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hkchengrex/Cutie">https://github.com/hkchengrex/Cutie</a></li>
<li>paper_authors: Ho Kei Cheng, Seoung Wug Oh, Brian Price, Joon-Young Lee, Alexander Schwing</li>
<li>for: 这个研究是为了提高视频对象分割（VOS）的精度和效率，特别是在复杂的数据集中。</li>
<li>methods: 这个模型使用一个叫做“query-based object transformer”的新方法，将物件表现 від memory 拼接到 video object segmentation 结果中，以提高精度和效率。</li>
<li>results: 在具有复杂数据集的 MOSE 测试集上，这个模型与 XMem 相比增加了8.7 J&amp;F，并且与 DeAOT 相比增加了4.2 J&amp;F，且三倍快速。<details>
<summary>Abstract</summary>
We present Cutie, a video object segmentation (VOS) network with object-level memory reading, which puts the object representation from memory back into the video object segmentation result. Recent works on VOS employ bottom-up pixel-level memory reading which struggles due to matching noise, especially in the presence of distractors, resulting in lower performance in more challenging data. In contrast, Cutie performs top-down object-level memory reading by adapting a small set of object queries for restructuring and interacting with the bottom-up pixel features iteratively with a query-based object transformer (qt, hence Cutie). The object queries act as a high-level summary of the target object, while high-resolution feature maps are retained for accurate segmentation. Together with foreground-background masked attention, Cutie cleanly separates the semantics of the foreground object from the background. On the challenging MOSE dataset, Cutie improves by 8.7 J&F over XMem with a similar running time and improves by 4.2 J&F over DeAOT while running three times as fast. Code is available at: https://hkchengrex.github.io/Cutie
</details>
<details>
<summary>摘要</summary>
我们介绍Cutie，一个具有物体记忆阅读的视频物体分割（VOS）网络，它将物体表现从内存中推回到视频物体分割结果中。现有的VOS方法通常使用底向推导的像素级别记忆阅读，它在许多挑战性数据中会受到匹配噪音的影响，导致性能较差。相比之下，Cutie使用顶向物体级别的记忆阅读，通过适应一小集的物体查询来重构和与底向像素特征进行联合运算（qt，因此称为Cutie）。物体查询 behave as a high-level概要 of the target object, while high-resolution feature maps are retained for accurate segmentation. 同时，内部遮瑕遮瑕注意力可以清晰地分离背景和前景的 semantics。在MOSE数据集上，Cutie与XMem的比较获得8.7 J&F的提升，并且与DeAOT的比较获得4.2 J&F的提升，具有相似的执行时间。代码可以在：https://hkchengrex.github.io/Cutie 上取得。
</details></li>
</ul>
<hr>
<h2 id="HumanTOMATO-Text-aligned-Whole-body-Motion-Generation"><a href="#HumanTOMATO-Text-aligned-Whole-body-Motion-Generation" class="headerlink" title="HumanTOMATO: Text-aligned Whole-body Motion Generation"></a>HumanTOMATO: Text-aligned Whole-body Motion Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12978">http://arxiv.org/abs/2310.12978</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/IDEA-Research/HumanTOMATO">https://github.com/IDEA-Research/HumanTOMATO</a></li>
<li>paper_authors: Shunlin Lu, Ling-Hao Chen, Ailing Zeng, Jing Lin, Ruimao Zhang, Lei Zhang, Heung-Yeung Shum</li>
<li>for: 本研究targets a novel text-driven whole-body motion generation task, which takes a given textual description as input and aims to generate high-quality, diverse, and coherent facial expressions, hand gestures, and body motions simultaneously.</li>
<li>methods: 我们提出了一个Text-aligned whOle-body Motion generATiOn framework，named HumanTOMATO，which is the first attempt to our knowledge towards applicable holistic motion generation in this research area. Our solution includes two key designs: (1) a Holistic Hierarchical VQ-VAE (aka H$^2$VQ) and a Hierarchical-GPT for fine-grained body and hand motion reconstruction and generation with two structured codebooks; and (2) a pre-trained text-motion-alignment model to help generated motion align with the input textual description explicitly.</li>
<li>results: 我们的模型在生成动作质量和文本对齐方面具有显著优势。<details>
<summary>Abstract</summary>
This work targets a novel text-driven whole-body motion generation task, which takes a given textual description as input and aims at generating high-quality, diverse, and coherent facial expressions, hand gestures, and body motions simultaneously. Previous works on text-driven motion generation tasks mainly have two limitations: they ignore the key role of fine-grained hand and face controlling in vivid whole-body motion generation, and lack a good alignment between text and motion. To address such limitations, we propose a Text-aligned whOle-body Motion generATiOn framework, named HumanTOMATO, which is the first attempt to our knowledge towards applicable holistic motion generation in this research area. To tackle this challenging task, our solution includes two key designs: (1) a Holistic Hierarchical VQ-VAE (aka H$^2$VQ) and a Hierarchical-GPT for fine-grained body and hand motion reconstruction and generation with two structured codebooks; and (2) a pre-trained text-motion-alignment model to help generated motion align with the input textual description explicitly. Comprehensive experiments verify that our model has significant advantages in both the quality of generated motions and their alignment with text.
</details>
<details>
<summary>摘要</summary>
这个工作目标是一种基于文本描述的全身动作生成任务，它将输入文本描述作为输入，并生成高质量、多样化、协调的面部表达、手势和身体动作。先前的文本动作生成任务主要有两点限制：一是忽略细腻的手势和面部控制的重要作用，二是缺乏文本和动作之间的好的对应。为了解决这些限制，我们提出了一个名为人类TOMATO的文本对齐整体动作框架，是我们知道的研究领域首次尝试。为了解决这个复杂的任务，我们的解决方案包括两个关键设计：一是一种层次结构的VQ-VAE（即H$^2$VQ）和一种层次结构的GPT для细腻的身体和手势动作重建和生成，使用两个结构化的编码库；二是一种预训练的文本动作对齐模型，以帮助生成的动作与输入文本描述对齐Explicitly。广泛的实验证明了我们模型在生成动作质量和文本对齐方面具有显著优势。
</details></li>
</ul>
<hr>
<h2 id="On-the-Hidden-Waves-of-Image"><a href="#On-the-Hidden-Waves-of-Image" class="headerlink" title="On the Hidden Waves of Image"></a>On the Hidden Waves of Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12976">http://arxiv.org/abs/2310.12976</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rprokap/pset-9">https://github.com/rprokap/pset-9</a></li>
<li>paper_authors: Yinpeng Chen, Dongdong Chen, Xiyang Dai, Mengchen Liu, Lu Yuan, Zicheng Liu, Youzuo Lin</li>
<li>for: 这篇论文描述了一种感人现象：通过一组一向波方程，可以成功重建图像，并且每个图像都对应着一个唯一的初始条件，可以从原始图像使用视觉编码器（例如卷积神经网络）来计算。</li>
<li>methods: 这篇论文使用了一种隐藏和学习速度的一向波方程来重建图像。每个图像都对应着一个特定的初始条件，可以使用视觉编码器来计算。</li>
<li>results: 这篇论文发现了一种称为”隐藏波”的现象，即每个图像都可以被分解为一 colelction of special solutions of the same one-way wave equations，这些解决方案都是一个共享的 autoregressive 矩阵的多阶幂。这表明，即使速度和autoregressive矩阵是隐藏的，它们也可以被学习和共享。这种数学变换提供了一种新的数学视角来理解图像。<details>
<summary>Abstract</summary>
In this paper, we introduce an intriguing phenomenon-the successful reconstruction of images using a set of one-way wave equations with hidden and learnable speeds. Each individual image corresponds to a solution with a unique initial condition, which can be computed from the original image using a visual encoder (e.g., a convolutional neural network). Furthermore, the solution for each image exhibits two noteworthy mathematical properties: (a) it can be decomposed into a collection of special solutions of the same one-way wave equations that are first-order autoregressive, with shared coefficient matrices for autoregression, and (b) the product of these coefficient matrices forms a diagonal matrix with the speeds of the wave equations as its diagonal elements. We term this phenomenon hidden waves, as it reveals that, although the speeds of the set of wave equations and autoregressive coefficient matrices are latent, they are both learnable and shared across images. This represents a mathematical invariance across images, providing a new mathematical perspective to understand images.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种有趣的现象：通过一组一个方向波方程的解，成功地重建图像。每个图像都对应于一个唯一的初始条件，可以从原始图像使用视觉编码器（例如卷积神经网络）来计算。此外，每个解表现出两个值得注意的数学性质：（a）它可以分解为同一个一个方向波方程的特殊解，这些特殊解具有共享的权重矩阵，并（b）这些权重矩阵的乘积形成一个对角矩阵，其中对角元素都是波方程的速度。我们称这种现象为“隐藏波”，因为尽管波方程和权重矩阵的速度都是隐藏的，但它们都是学习的，并且在图像之间共享。这表示图像具有数学的变换性，提供了一个新的数学视角来理解图像。
</details></li>
</ul>
<hr>
<h2 id="FSD-Fast-Self-Supervised-Single-RGB-D-to-Categorical-3D-Objects"><a href="#FSD-Fast-Self-Supervised-Single-RGB-D-to-Categorical-3D-Objects" class="headerlink" title="FSD: Fast Self-Supervised Single RGB-D to Categorical 3D Objects"></a>FSD: Fast Self-Supervised Single RGB-D to Categorical 3D Objects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12974">http://arxiv.org/abs/2310.12974</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mayank Lunayach, Sergey Zakharov, Dian Chen, Rares Ambrus, Zsolt Kira, Muhammad Zubair Irshad</li>
<li>for: 3D object recognition without relying on real-world 3D labeled data</li>
<li>methods: multi-stage training pipeline with synthetic and real-world data, combining 2D and 3D supervised losses and 2D self-supervised loss</li>
<li>results: outperforms existing self-supervised 6D pose and size estimation baselines on the NOCS test-set with a 16.4% absolute improvement in mAP for 6D pose estimation, running in near real-time at 5 Hz<details>
<summary>Abstract</summary>
In this work, we address the challenging task of 3D object recognition without the reliance on real-world 3D labeled data. Our goal is to predict the 3D shape, size, and 6D pose of objects within a single RGB-D image, operating at the category level and eliminating the need for CAD models during inference. While existing self-supervised methods have made strides in this field, they often suffer from inefficiencies arising from non-end-to-end processing, reliance on separate models for different object categories, and slow surface extraction during the training of implicit reconstruction models; thus hindering both the speed and real-world applicability of the 3D recognition process. Our proposed method leverages a multi-stage training pipeline, designed to efficiently transfer synthetic performance to the real-world domain. This approach is achieved through a combination of 2D and 3D supervised losses during the synthetic domain training, followed by the incorporation of 2D supervised and 3D self-supervised losses on real-world data in two additional learning stages. By adopting this comprehensive strategy, our method successfully overcomes the aforementioned limitations and outperforms existing self-supervised 6D pose and size estimation baselines on the NOCS test-set with a 16.4% absolute improvement in mAP for 6D pose estimation while running in near real-time at 5 Hz.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们面临着3D物体认知无需真实世界3D标注数据的挑战。我们的目标是在单个RGB-D图像中预测物体的3D形状、大小和6D姿态，并在类别水平上进行预测，不需要在推理过程中使用CAD模型。现有的自动学习方法在这个领域有所进步，但它们经常受到非终端处理引起的不具有效率，以及不同类别的物体模型之间的分离，从而降低了推理速度和实际应用性。我们提议的方法采用多阶段训练管道，通过在synthetic领域中使用2D和3D监督损失进行训练，然后在real-world数据上添加2D监督和3D自监督损失进行两个额外学习阶段。通过这种全面策略，我们的方法成功地超越了先前的自动学习6D姿态和大小估计基准，在NOCS测试集上达到16.4%的绝对提升率，而且在5Hz的刷新率下运行在实时内。
</details></li>
</ul>
<hr>
<h2 id="Human-Pose-based-Estimation-Tracking-and-Action-Recognition-with-Deep-Learning-A-Survey"><a href="#Human-Pose-based-Estimation-Tracking-and-Action-Recognition-with-Deep-Learning-A-Survey" class="headerlink" title="Human Pose-based Estimation, Tracking and Action Recognition with Deep Learning: A Survey"></a>Human Pose-based Estimation, Tracking and Action Recognition with Deep Learning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13039">http://arxiv.org/abs/2310.13039</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lijuan Zhou, Xiang Meng, Zhihuan Liu, Mengqi Wu, Zhimin Gao, Pichao Wang</li>
<li>for: 这篇论文旨在探讨深度学习在人姿分析中的应用，包括人姿估计、人姿跟踪和动作识别。</li>
<li>methods: 本论文提出了一种基于深度学习的人姿分析方法，包括人姿估计、人姿跟踪和动作识别。</li>
<li>results: 研究人员通过对多个人姿分析任务的评估和分析，发现了一些关键的问题和挑战，同时也提出了一些可能的解决方案。<details>
<summary>Abstract</summary>
Human pose analysis has garnered significant attention within both the research community and practical applications, owing to its expanding array of uses, including gaming, video surveillance, sports performance analysis, and human-computer interactions, among others. The advent of deep learning has significantly improved the accuracy of pose capture, making pose-based applications increasingly practical. This paper presents a comprehensive survey of pose-based applications utilizing deep learning, encompassing pose estimation, pose tracking, and action recognition.Pose estimation involves the determination of human joint positions from images or image sequences. Pose tracking is an emerging research direction aimed at generating consistent human pose trajectories over time. Action recognition, on the other hand, targets the identification of action types using pose estimation or tracking data. These three tasks are intricately interconnected, with the latter often reliant on the former. In this survey, we comprehensively review related works, spanning from single-person pose estimation to multi-person pose estimation, from 2D pose estimation to 3D pose estimation, from single image to video, from mining temporal context gradually to pose tracking, and lastly from tracking to pose-based action recognition. As a survey centered on the application of deep learning to pose analysis, we explicitly discuss both the strengths and limitations of existing techniques. Notably, we emphasize methodologies for integrating these three tasks into a unified framework within video sequences. Additionally, we explore the challenges involved and outline potential directions for future research.
</details>
<details>
<summary>摘要</summary>
人姿分析在研究社区和实际应用中受到了广泛关注，因为它在游戏、视频监测、运动表现分析和人机交互等领域有着扩大的应用范围。深度学习的出现使得人姿捕捉的准确性得到了显著提高，使得人姿基于应用变得更加实用。本文是一篇对深度学习应用于人姿分析的全面评论，涵盖了人姿估计、人姿跟踪和动作识别等三个任务。人姿估计是从图像或图像序列中确定人 JOINT 位置的任务。人姿跟踪是一个emerging的研究方向，旨在在时间上生成一致的人姿轨迹。动作识别则是根据人姿估计或跟踪数据来确定动作类型的任务。这三个任务之间存在紧密的关系，后两个任务经常依赖于前一个任务。在这篇评论中，我们全面回顾相关的工作，从单人人姿估计到多人人姿估计，从2D人姿估计到3D人姿估计，从单图像到视频，从慢慢地采集时间上的动作特征来估计人姿到pose tracking，并最后从跟踪到动作识别。作为深度学习应用于人姿分析的评论，我们明确地讨论了现有技术的优势和局限性。尤其是我们强调将这三个任务集成到视频序列中，并探讨了相关挑战和未来研究的可能性。
</details></li>
</ul>
<hr>
<h2 id="Real-Time-Motion-Prediction-via-Heterogeneous-Polyline-Transformer-with-Relative-Pose-Encoding"><a href="#Real-Time-Motion-Prediction-via-Heterogeneous-Polyline-Transformer-with-Relative-Pose-Encoding" class="headerlink" title="Real-Time Motion Prediction via Heterogeneous Polyline Transformer with Relative Pose Encoding"></a>Real-Time Motion Prediction via Heterogeneous Polyline Transformer with Relative Pose Encoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12970">http://arxiv.org/abs/2310.12970</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhejz/hptr">https://github.com/zhejz/hptr</a></li>
<li>paper_authors: Zhejun Zhang, Alexander Liniger, Christos Sakaridis, Fisher Yu, Luc Van Gool</li>
<li>for: 这篇论文是用于解决自动驾驶系统中的动作预测问题，以提高系统的实时运行效率和扩展性。</li>
<li>methods: 本文使用了K-nearest neighbor attention with relative pose encoding (KNARPE)  attended Transformer，以及一个 hierarchical framework 允许在线进行资料更新。</li>
<li>results: 实验结果显示，使用HPTR可以在维持与现有方法相同的性能水准下，提高系统的实时运行效率和扩展性。<details>
<summary>Abstract</summary>
The real-world deployment of an autonomous driving system requires its components to run on-board and in real-time, including the motion prediction module that predicts the future trajectories of surrounding traffic participants. Existing agent-centric methods have demonstrated outstanding performance on public benchmarks. However, they suffer from high computational overhead and poor scalability as the number of agents to be predicted increases. To address this problem, we introduce the K-nearest neighbor attention with relative pose encoding (KNARPE), a novel attention mechanism allowing the pairwise-relative representation to be used by Transformers. Then, based on KNARPE we present the Heterogeneous Polyline Transformer with Relative pose encoding (HPTR), a hierarchical framework enabling asynchronous token update during the online inference. By sharing contexts among agents and reusing the unchanged contexts, our approach is as efficient as scene-centric methods, while performing on par with state-of-the-art agent-centric methods. Experiments on Waymo and Argoverse-2 datasets show that HPTR achieves superior performance among end-to-end methods that do not apply expensive post-processing or model ensembling. The code is available at https://github.com/zhejz/HPTR.
</details>
<details>
<summary>摘要</summary>
现实世界中部署自动驾驶系统需要其组件在实时上下文中运行，包括预测周围交通参与者未来轨迹的运动预测模块。现有的中心式方法在公共标准上表现出色，但它们由于参与者数量增加而受到高计算负担和差异化问题。为解决这问题，我们提出了K-最近邻居注意力与相对姿态编码（KNARPE），一种新的注意力机制，allowing Transformers使用对称的对比姿态表示。然后，基于KNARPE，我们提出了多态轨迹转换器（HPTR），一种层次结构的框架，允许在在线推断过程中 asynchronous token 更新。通过在Agent之间共享上下文和重用不变的上下文，我们的方法可以与场景中心式方法相当努力，同时与现有的中心式方法相比，表现出色。实验结果表明，HPTR在不使用昂贵的后处理或模型ensemble的情况下，在终端方法中达到了最高性能。代码可以在https://github.com/zhejz/HPTR 上找到。
</details></li>
</ul>
<hr>
<h2 id="3D-GPT-Procedural-3D-Modeling-with-Large-Language-Models"><a href="#3D-GPT-Procedural-3D-Modeling-with-Large-Language-Models" class="headerlink" title="3D-GPT: Procedural 3D Modeling with Large Language Models"></a>3D-GPT: Procedural 3D Modeling with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12945">http://arxiv.org/abs/2310.12945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chunyi Sun, Junlin Han, Weijian Deng, Xinlong Wang, Zishan Qin, Stephen Gould<br>for:* 3D模型创建自动化Content生成methods:* 使用大型自然语言模型(LLMs)进行指令驱动3D模型创建* 分解复杂的3D模型创建任务为可访问的部分，并委托适合的代理处理每个任务results:* 可靠地从文本中提取参数值，用于轻松地与3D软件集成* 与人类设计师合作有效* 可以轻松地扩展 manipulate 可能性Note: The above information is in Simplified Chinese.<details>
<summary>Abstract</summary>
In the pursuit of efficient automated content creation, procedural generation, leveraging modifiable parameters and rule-based systems, emerges as a promising approach. Nonetheless, it could be a demanding endeavor, given its intricate nature necessitating a deep understanding of rules, algorithms, and parameters. To reduce workload, we introduce 3D-GPT, a framework utilizing large language models~(LLMs) for instruction-driven 3D modeling. 3D-GPT positions LLMs as proficient problem solvers, dissecting the procedural 3D modeling tasks into accessible segments and appointing the apt agent for each task. 3D-GPT integrates three core agents: the task dispatch agent, the conceptualization agent, and the modeling agent. They collaboratively achieve two objectives. First, it enhances concise initial scene descriptions, evolving them into detailed forms while dynamically adapting the text based on subsequent instructions. Second, it integrates procedural generation, extracting parameter values from enriched text to effortlessly interface with 3D software for asset creation. Our empirical investigations confirm that 3D-GPT not only interprets and executes instructions, delivering reliable results but also collaborates effectively with human designers. Furthermore, it seamlessly integrates with Blender, unlocking expanded manipulation possibilities. Our work highlights the potential of LLMs in 3D modeling, offering a basic framework for future advancements in scene generation and animation.
</details>
<details>
<summary>摘要</summary>
在寻求高效自动内容创造的探索中，procédural生成技术 Emerges as a promising approach。然而，它的复杂性可能会增加工作负担，需要深刻理解规则、算法和参数。为了减轻工作负担，我们介绍了3D-GPT框架，该框架利用大语言模型（LLMs） дляinstruction-driven 3D 模型创建。3D-GPT将 LLMs 作为有能力的问题解决器，将过程式 3D 模型创建任务分解成可 accessible 的部分，并将每个任务分配给适合的代理人。3D-GPT  integrate three core agents：任务派发代理人、概念化代理人和模型代理人。它们合作实现两个目标。首先，它可以将简洁的初始场景描述进行细化，并在基于后续指令的动态适应文本上进行演化。其次，它可以通过执行程序代码来轻松地与 3D 软件进行资产创建。我们的实验证明，3D-GPT 不仅可以理解和执行指令，还可以与人类设计师合作有效。此外，它可以轻松地与 Blender 集成，解锁了更多的操作可能性。我们的工作显示了LLMs在 3D 模型创建方面的潜在力量，并提供了一个基本的框架，以便未来的Scene生成和动画技术的发展。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Object-Localization-in-the-Era-of-Self-Supervised-ViTs-A-Survey"><a href="#Unsupervised-Object-Localization-in-the-Era-of-Self-Supervised-ViTs-A-Survey" class="headerlink" title="Unsupervised Object Localization in the Era of Self-Supervised ViTs: A Survey"></a>Unsupervised Object Localization in the Era of Self-Supervised ViTs: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12904">http://arxiv.org/abs/2310.12904</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/valeoai/awesome-unsupervised-object-localization">https://github.com/valeoai/awesome-unsupervised-object-localization</a></li>
<li>paper_authors: Oriane Siméoni, Éloi Zablocki, Spyros Gidaris, Gilles Puy, Patrick Pérez</li>
<li>for: 这些研究旨在实现无监督的对象位置标注，即无需任何手动标注对象的存在和位置。</li>
<li>methods: 这些方法基于自我超vised的特征学习，包括ViT等模型，以实现对象的探索和定位。</li>
<li>results: 这些方法可以在图像和视频中找到对象，而无需任何手动标注。Here is the text in Simplified Chinese:</li>
<li>for: 这些研究旨在实现无监督的对象位置标注，即无需任何手动标注对象的存在和位置。</li>
<li>methods: 这些方法基于自我超vised的特征学习，包括ViT等模型，以实现对象的探索和定位。</li>
<li>results: 这些方法可以在图像和视频中找到对象，而无需任何手动标注。<details>
<summary>Abstract</summary>
The recent enthusiasm for open-world vision systems show the high interest of the community to perform perception tasks outside of the closed-vocabulary benchmark setups which have been so popular until now. Being able to discover objects in images/videos without knowing in advance what objects populate the dataset is an exciting prospect. But how to find objects without knowing anything about them? Recent works show that it is possible to perform class-agnostic unsupervised object localization by exploiting self-supervised pre-trained features. We propose here a survey of unsupervised object localization methods that discover objects in images without requiring any manual annotation in the era of self-supervised ViTs. We gather links of discussed methods in the repository https://github.com/valeoai/Awesome-Unsupervised-Object-Localization.
</details>
<details>
<summary>摘要</summary>
当前社区对开放视界系统的热情表明了对外部环境中进行感知任务的高度兴趣，而这些任务曾经 Until now, most benchmark setups have been based on closed vocabularies, but recent works have shown that it is possible to perform class-agnostic unsupervised object localization by exploiting self-supervised pre-trained features. We propose a survey of unsupervised object localization methods that discover objects in images without requiring any manual annotation, in the era of self-supervised ViTs. The discussed methods can be found in the repository <https://github.com/valeoai/Awesome-Unsupervised-Object-Localization>.Note: "ViT" stands for Vision Transformer, which is a type of neural network architecture that has gained popularity in computer vision tasks.
</details></li>
</ul>
<hr>
<h2 id="Perceptual-Assessment-and-Optimization-of-High-Dynamic-Range-Image-Rendering"><a href="#Perceptual-Assessment-and-Optimization-of-High-Dynamic-Range-Image-Rendering" class="headerlink" title="Perceptual Assessment and Optimization of High Dynamic Range Image Rendering"></a>Perceptual Assessment and Optimization of High Dynamic Range Image Rendering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12877">http://arxiv.org/abs/2310.12877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peibei Cao, Rafal K. Mantiuk, Kede Ma</li>
<li>for: This paper focuses on developing a family of high dynamic range (HDR) image quality assessment (IQA) models that can accurately evaluate the quality of HDR images.</li>
<li>methods: The proposed HDR IQA models use a simple inverse display model to decompose an HDR image into a set of low dynamic range (LDR) images with different exposures, which are then assessed by existing LDR quality models. The local quality scores of each exposure are aggregated using a well-exposedness measure, and the overall quality score is obtained by weighting the exposures.</li>
<li>results: The proposed HDR IQA models outperform existing IQA methods, including the HDR-VDP family, in evaluating the quality of HDR images. Additionally, the models demonstrate strengths in perceptual optimization of HDR novel view synthesis.<details>
<summary>Abstract</summary>
High dynamic range (HDR) imaging has gained increasing popularity for its ability to faithfully reproduce the luminance levels in natural scenes. Accordingly, HDR image quality assessment (IQA) is crucial but has been superficially treated. The majority of existing IQA models are developed for and calibrated against low dynamic range (LDR) images, which have been shown to be poorly correlated with human perception of HDR image quality. In this work, we propose a family of HDR IQA models by transferring the recent advances in LDR IQA. The key step in our approach is to specify a simple inverse display model that decomposes an HDR image to a set of LDR images with different exposures, which will be assessed by existing LDR quality models. The local quality scores of each exposure are then aggregated with the help of a simple well-exposedness measure into a global quality score for each exposure, which will be further weighted across exposures to obtain the overall quality score. When assessing LDR images, the proposed HDR quality models reduce gracefully to the original LDR ones with the same performance. Experiments on four human-rated HDR image datasets demonstrate that our HDR quality models are consistently better than existing IQA methods, including the HDR-VDP family. Moreover, we demonstrate their strengths in perceptual optimization of HDR novel view synthesis.
</details>
<details>
<summary>摘要</summary>
高动态范围（HDR）摄影技术在自然场景中的亮度水平准确反映，因此HDR图像质量评估（IQA）变得非常重要。然而，现有的IQA模型大多是基于低动态范围（LDR）图像的，这些图像与人类对HDR图像质量的识别呈现相互不符。在这项工作中，我们提出了一系列基于LDR IQA的HDR IQA模型。我们的方法的关键步骤是使用一个简单的反映显示模型将HDR图像分解成不同曝光的LDR图像，然后使用现有的LDR质量模型评估每个曝光的地方质量。最后，我们将每个曝光的地方质量Weighted来获得总质量分数，并将其进行权重平均来获得总质量分数。当评估LDR图像时，我们的HDR质量模型会降低到原始的LDR模型，并保持同样的性能。我们在四个人标注的HDR图像集上进行了实验，并证明了我们的HDR质量模型在现有IQA方法中具有更高的性能。此外，我们还证明了它们在HDR新视图合成中的强大特点。
</details></li>
</ul>
<hr>
<h2 id="EMIT-Diff-Enhancing-Medical-Image-Segmentation-via-Text-Guided-Diffusion-Model"><a href="#EMIT-Diff-Enhancing-Medical-Image-Segmentation-via-Text-Guided-Diffusion-Model" class="headerlink" title="EMIT-Diff: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model"></a>EMIT-Diff: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12868">http://arxiv.org/abs/2310.12868</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheyuan Zhang, Lanhong Yao, Bin Wang, Debesh Jha, Elif Keles, Alpay Medetalibeyoglu, Ulas Bagci<br>for:This paper aims to address the challenge of scarce high-quality labeled data for medical deep learning models by proposing a novel approach called EMIT-Diff for medical image synthesis.methods:The proposed EMIT-Diff method leverages recent diffusion probabilistic models to generate realistic and diverse synthetic medical image data that preserve the essential characteristics of the original medical images. The method incorporates edge information to guide the synthesis process and ensures that the synthesized samples adhere to medically relevant constraints.results:The proposed EMIT-Diff method achieves significant improvements in medical image segmentation tasks on multiple datasets, including Ultrasound breast, CT spleen, and MRI prostate. The method demonstrates the effectiveness of introducing a first-ever text-guided diffusion model for general medical image segmentation tasks, and the results show the feasibility of using synthetic data for medical image segmentation tasks.<details>
<summary>Abstract</summary>
Large-scale, big-variant, and high-quality data are crucial for developing robust and successful deep-learning models for medical applications since they potentially enable better generalization performance and avoid overfitting. However, the scarcity of high-quality labeled data always presents significant challenges. This paper proposes a novel approach to address this challenge by developing controllable diffusion models for medical image synthesis, called EMIT-Diff. We leverage recent diffusion probabilistic models to generate realistic and diverse synthetic medical image data that preserve the essential characteristics of the original medical images by incorporating edge information of objects to guide the synthesis process. In our approach, we ensure that the synthesized samples adhere to medically relevant constraints and preserve the underlying structure of imaging data. Due to the random sampling process by the diffusion model, we can generate an arbitrary number of synthetic images with diverse appearances. To validate the effectiveness of our proposed method, we conduct an extensive set of medical image segmentation experiments on multiple datasets, including Ultrasound breast (+13.87%), CT spleen (+0.38%), and MRI prostate (+7.78%), achieving significant improvements over the baseline segmentation methods. For the first time, to our best knowledge, the promising results demonstrate the effectiveness of our EMIT-Diff for medical image segmentation tasks and show the feasibility of introducing a first-ever text-guided diffusion model for general medical image segmentation tasks. With carefully designed ablation experiments, we investigate the influence of various data augmentation ratios, hyper-parameter settings, patch size for generating random merging mask settings, and combined influence with different network architectures.
</details>
<details>
<summary>摘要</summary>
大规模、大变种、高质量数据是深度学习模型在医疗应用中的关键，因为它们可能提供更好的泛化性和避免过拟合。然而，医疗数据的稀缺性常常带来重大挑战。这篇论文提出了一种新的方法，称为EMIT-Diff，以 Address this challenge by developing controllable diffusion models for medical image synthesis. We leverage recent diffusion probabilistic models to generate realistic and diverse synthetic medical image data that preserve the essential characteristics of the original medical images by incorporating edge information of objects to guide the synthesis process. In our approach, we ensure that the synthesized samples adhere to medically relevant constraints and preserve the underlying structure of imaging data. Due to the random sampling process by the diffusion model, we can generate an arbitrary number of synthetic images with diverse appearances. To validate the effectiveness of our proposed method, we conduct an extensive set of medical image segmentation experiments on multiple datasets, including Ultrasound breast (+13.87%), CT spleen (+0.38%), and MRI prostate (+7.78%), achieving significant improvements over the baseline segmentation methods. For the first time, to our best knowledge, the promising results demonstrate the effectiveness of our EMIT-Diff for medical image segmentation tasks and show the feasibility of introducing a first-ever text-guided diffusion model for general medical image segmentation tasks. With carefully designed ablation experiments, we investigate the influence of various data augmentation ratios, hyper-parameter settings, patch size for generating random merging mask settings, and combined influence with different network architectures.
</details></li>
</ul>
<hr>
<h2 id="Neural-Degradation-Representation-Learning-for-All-In-One-Image-Restoration"><a href="#Neural-Degradation-Representation-Learning-for-All-In-One-Image-Restoration" class="headerlink" title="Neural Degradation Representation Learning for All-In-One Image Restoration"></a>Neural Degradation Representation Learning for All-In-One Image Restoration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12848">http://arxiv.org/abs/2310.12848</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mdyao/NDR-Restore">https://github.com/mdyao/NDR-Restore</a></li>
<li>paper_authors: Mingde Yao, Ruikang Xu, Yuanshen Guan, Jie Huang, Zhiwei Xiong</li>
<li>for:  This paper aims to propose an all-in-one image restoration network that can handle multiple degradations, such as noise, haze, rain, and downsampling.</li>
<li>methods: The proposed method learns a neural degradation representation (NDR) that captures the underlying characteristics of various degradations, and uses a degradation query module and a degradation injection module to recognize and utilize the specific degradation based on NDR. The method also employs a bidirectional optimization strategy to effectively drive NDR to learn the degradation representation.</li>
<li>results: The proposed method is demonstrated to be effective and generalizable on representative types of degradations, including noise, haze, rain, and downsampling, through comprehensive experiments.<details>
<summary>Abstract</summary>
Existing methods have demonstrated effective performance on a single degradation type. In practical applications, however, the degradation is often unknown, and the mismatch between the model and the degradation will result in a severe performance drop. In this paper, we propose an all-in-one image restoration network that tackles multiple degradations. Due to the heterogeneous nature of different types of degradations, it is difficult to process multiple degradations in a single network. To this end, we propose to learn a neural degradation representation (NDR) that captures the underlying characteristics of various degradations. The learned NDR decomposes different types of degradations adaptively, similar to a neural dictionary that represents basic degradation components. Subsequently, we develop a degradation query module and a degradation injection module to effectively recognize and utilize the specific degradation based on NDR, enabling the all-in-one restoration ability for multiple degradations. Moreover, we propose a bidirectional optimization strategy to effectively drive NDR to learn the degradation representation by optimizing the degradation and restoration processes alternately. Comprehensive experiments on representative types of degradations (including noise, haze, rain, and downsampling) demonstrate the effectiveness and generalization capability of our method.
</details>
<details>
<summary>摘要</summary>
现有方法已经证明可以在单一的降解类型上达到有效性。然而，在实际应用中，降解通常是未知的，并且模型和降解之间的匹配问题会导致性能下降。在这篇论文中，我们提出了一个涵盖多种降解的图像恢复网络。由于不同类型的降解具有不同的特征，因此在单一网络中处理多种降解是困难的。为此，我们提出了学习神经降解表示（NDR），该表示捕捉不同类型降解的基本特征。学习后，NDR可以适应性地分解不同类型降解，类似于神经字典中的基本降解组件。然后，我们提出了降解查询模块和降解注入模块，以有效地识别和利用特定降解，实现涵盖多种降解的恢复能力。此外，我们提出了双向优化策略，以有效地驱动NDR学习降解表示，通过同时优化降解和恢复过程来更好地适应不同类型降解。在代表性的降解类型（包括噪声、雾、雨和下采样）的实验中，我们的方法得到了有效性和普适性的证明。
</details></li>
</ul>
<hr>
<h2 id="OODRobustBench-benchmarking-and-analyzing-adversarial-robustness-under-distribution-shift"><a href="#OODRobustBench-benchmarking-and-analyzing-adversarial-robustness-under-distribution-shift" class="headerlink" title="OODRobustBench: benchmarking and analyzing adversarial robustness under distribution shift"></a>OODRobustBench: benchmarking and analyzing adversarial robustness under distribution shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12793">http://arxiv.org/abs/2310.12793</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lin Li, Yifei Wang, Chawin Sitawarin, Michael Spratling</li>
<li>for: 本文旨在评估针对异常输入的鲁棒性，以及验证现有的鲁棒训练方法是否能够在不同的输入分布下保持鲁棒性。</li>
<li>methods: 本文提出了一个名为OODRobustBench的测试框架，用于全面评估异常输入鲁棒性。OODRobustBench使用了23个数据集的自然适应异常shift和6个威胁模型的不适应异常shift进行测试。</li>
<li>results: 本文通过对706个鲁棒模型进行60.7万次攻击评估，发现了多个鲁棒模型在异常输入下的鲁棒性受到严重的挑战。此外，本文还发现了一些新的训练方法和技术可以提高异常输入鲁棒性。<details>
<summary>Abstract</summary>
Existing works have made great progress in improving adversarial robustness, but typically test their method only on data from the same distribution as the training data, i.e. in-distribution (ID) testing. As a result, it is unclear how such robustness generalizes under input distribution shifts, i.e. out-of-distribution (OOD) testing. This is a concerning omission as such distribution shifts are unavoidable when methods are deployed in the wild. To address this issue we propose a benchmark named OODRobustBench to comprehensively assess OOD adversarial robustness using 23 dataset-wise shifts (i.e. naturalistic shifts in input distribution) and 6 threat-wise shifts (i.e., unforeseen adversarial threat models). OODRobustBench is used to assess 706 robust models using 60.7K adversarial evaluations. This large-scale analysis shows that: 1) adversarial robustness suffers from a severe OOD generalization issue; 2) ID robustness correlates strongly with OOD robustness, in a positive linear way, under many distribution shifts. The latter enables the prediction of OOD robustness from ID robustness. Based on this, we are able to predict the upper limit of OOD robustness for existing robust training schemes. The results suggest that achieving OOD robustness requires designing novel methods beyond the conventional ones. Last, we discover that extra data, data augmentation, advanced model architectures and particular regularization approaches can improve OOD robustness. Noticeably, the discovered training schemes, compared to the baseline, exhibit dramatically higher robustness under threat shift while keeping high ID robustness, demonstrating new promising solutions for robustness against both multi-attack and unforeseen attacks.
</details>
<details>
<summary>摘要</summary>
现有的研究已经做出了很大的进步，以改善抗敌性 Robustness，但通常只在同一个分布中进行测试，即内部分布（ID）测试。因此，实际上不清楚这种 Robustness 如何在输入分布差异（OOD）测试中发挥作用。这是一个担忧的漏洞，因为在实际应用中，分布差异是不可避免的。为了解决这个问题，我们提出了名为 OODRobustBench 的库，用于全面评估 OOD 抗敌性 Robustness，透过 23 个dataset-wise 分布差异和 6 个威胁-wise 分布差异。OODRobustBench 用于评估 706 个Robust model，使用 60.7K 个攻击评估。这些大规模的分析显示：1）抗敌性 Robustness 受到 OOD 通用化的问题严重影响；2）ID 抗敌性 Robustness 与 OOD 抗敌性 Robustness 之间存在正相关，在许多分布差异下，呈现正线性关系。这使得可以预测 OOD 抗敌性 Robustness。基于这个结果，我们能够预测现有的 Robust training scheme 的 OOD 抗敌性 Robustness 的Upper bound。结果表明，以现有的方法设计 OOD 抗敌性 Robustness 需要开探新的方法。最后，我们发现额外的数据、数据增强、进阶模型架构和特定的调整方法可以提高 OOD 抗敌性 Robustness。特别是，发现的训练方案，相比基准，在威胁差异下显示出了很高的抗敌性 Robustness，同时保持高的 ID 抗敌性 Robustness，显示出了新的可行的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Anomaly-Heterogeneity-Learning-for-Open-set-Supervised-Anomaly-Detection"><a href="#Anomaly-Heterogeneity-Learning-for-Open-set-Supervised-Anomaly-Detection" class="headerlink" title="Anomaly Heterogeneity Learning for Open-set Supervised Anomaly Detection"></a>Anomaly Heterogeneity Learning for Open-set Supervised Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12790">http://arxiv.org/abs/2310.12790</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mala-lab/ahl">https://github.com/mala-lab/ahl</a></li>
<li>paper_authors: Jiawen Zhu, Choubo Ding, Yu Tian, Guansong Pang</li>
<li>for: 本研究目标是提高openset监控预测（OSAD）预测效果，使其能够掌握未经见过的异常类型。</li>
<li>methods: 本文提出了一种新的方法——异常分布学习（AHL），通过使用有限数量的异常示例来学习多元异常分布，并将其用于建立一个统一的多元异常模型。</li>
<li>results: 实验结果表明，AHL可以1）提高不同的状态的艺术模型（SOTA）在检测已见和未见异常方面的性能，达到新的SOTA性能水平，2）有效地在新目标领域中generalize to unseen anomalies。<details>
<summary>Abstract</summary>
Open-set supervised anomaly detection (OSAD) - a recently emerging anomaly detection area - aims at utilizing a few samples of anomaly classes seen during training to detect unseen anomalies (i.e., samples from open-set anomaly classes), while effectively identifying the seen anomalies. Benefiting from the prior knowledge illustrated by the seen anomalies, current OSAD methods can often largely reduce false positive errors. However, these methods treat the anomaly examples as from a homogeneous distribution, rendering them less effective in generalizing to unseen anomalies that can be drawn from any distribution. In this paper, we propose to learn heterogeneous anomaly distributions using the limited anomaly examples to address this issue. To this end, we introduce a novel approach, namely Anomaly Heterogeneity Learning (AHL), that simulates a diverse set of heterogeneous (seen and unseen) anomaly distributions and then utilizes them to learn a unified heterogeneous abnormality model. Further, AHL is a generic framework that existing OSAD models can plug and play for enhancing their abnormality modeling. Extensive experiments on nine real-world anomaly detection datasets show that AHL can 1) substantially enhance different state-of-the-art (SOTA) OSAD models in detecting both seen and unseen anomalies, achieving new SOTA performance on a large set of datasets, and 2) effectively generalize to unseen anomalies in new target domains.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出了一种新的方法，即异常多样性学习（AHL），用于学习异常多样的分布。AHL 通过模拟各种不同的异常分布，然后利用这些分布来学习一个统一的异常模型。AHL 是一个通用的框架，可以让现有的 OSAD 模型插入和使用，以提高异常模型化。我们在 nine 个实际异常检测 dataset 上进行了广泛的实验，结果表明，AHL 可以：1. 对不同的 SOTA OSAD 模型进行显著改进，在许多 dataset 上减少假阳性错误，并达到新的 SOTA 性能。2. 对未经见过的异常进行有效的泛化，在新的目标领域中具有优秀的泛化性能。
</details></li>
</ul>
<hr>
<h2 id="DT-MARS-CycleGAN-Improved-Object-Detection-for-MARS-Phenotyping-Robot"><a href="#DT-MARS-CycleGAN-Improved-Object-Detection-for-MARS-Phenotyping-Robot" class="headerlink" title="DT&#x2F;MARS-CycleGAN: Improved Object Detection for MARS Phenotyping Robot"></a>DT&#x2F;MARS-CycleGAN: Improved Object Detection for MARS Phenotyping Robot</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12787">http://arxiv.org/abs/2310.12787</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Liu, Zhengkun Li, Zihao Wu, Changying Li</li>
<li>for: 这个论文的目的是提高Modular Agricultural Robotic System（MARS）对复杂和变化背景的作物物体检测精度。</li>
<li>methods: 该论文提出了一种基于Digital-Twin(DT)MARS-CycleGAN模型的图像增强技术，以便在MARS捕获的真实作物图像上进行更好的物体检测。</li>
<li>results: 对比于传统的CycleGAN模型，该新的DT&#x2F;MARS-CycleGAN模型能够更好地适应复杂和变化的作物形态，从而提高MARS的作物物体检测性能。<details>
<summary>Abstract</summary>
Robotic crop phenotyping has emerged as a key technology to assess crops' morphological and physiological traits at scale. These phenotypical measurements are essential for developing new crop varieties with the aim of increasing productivity and dealing with environmental challenges such as climate change. However, developing and deploying crop phenotyping robots face many challenges such as complex and variable crop shapes that complicate robotic object detection, dynamic and unstructured environments that baffle robotic control, and real-time computing and managing big data that challenge robotic hardware/software. This work specifically tackles the first challenge by proposing a novel Digital-Twin(DT)MARS-CycleGAN model for image augmentation to improve our Modular Agricultural Robotic System (MARS)'s crop object detection from complex and variable backgrounds. Our core idea is that in addition to the cycle consistency losses in the CycleGAN model, we designed and enforced a new DT-MARS loss in the deep learning model to penalize the inconsistency between real crop images captured by MARS and synthesized images sensed by DT MARS. Therefore, the generated synthesized crop images closely mimic real images in terms of realism, and they are employed to fine-tune object detectors such as YOLOv8. Extensive experiments demonstrated that our new DT/MARS-CycleGAN framework significantly boosts our MARS' crop object/row detector's performance, contributing to the field of robotic crop phenotyping.
</details>
<details>
<summary>摘要</summary>
人工智能耕地评估技术已经成为评估作物形态和生理特征的关键工具。这些形态测量是为开发新的作物品种，提高产量和面对环境挑战，如气候变化。然而，开发和部署作物评估机器人受到许多挑战，如复杂和变化的作物形态，导致机器人检测困难，不结构化环境，以及实时处理大量数据的硬件/软件挑战。本工作专门解决第一个挑战，提出了一种新的数字双子(DT)MARS-CycleGAN模型，用于图像增强，从复杂和变化的背景中提高我们的Modular Agricultural Robotic System(MARS)的作物对象检测精度。我们的核心想法是，在CycleGAN模型中的循环一致损失之外，我们还设计了一个新的DT-MARS损失函数，以惩罚DT MARS捕捉到的实际作物图像与模拟图像之间的不一致。因此，生成的模拟图像准确地模仿实际图像，并且用于练化对象检测器，如YOLOv8。广泛的实验证明，我们的新DT/MARS-CycleGAN框架可以明显提高MARS的作物对象/行检测器性能，对耕地 roboticphenotyping领域做出了贡献。
</details></li>
</ul>
<hr>
<h2 id="Mixing-Histopathology-Prototypes-into-Robust-Slide-Level-Representations-for-Cancer-Subtyping"><a href="#Mixing-Histopathology-Prototypes-into-Robust-Slide-Level-Representations-for-Cancer-Subtyping" class="headerlink" title="Mixing Histopathology Prototypes into Robust Slide-Level Representations for Cancer Subtyping"></a>Mixing Histopathology Prototypes into Robust Slide-Level Representations for Cancer Subtyping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12769">http://arxiv.org/abs/2310.12769</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/butkej/protomixer">https://github.com/butkej/protomixer</a></li>
<li>paper_authors: Joshua Butke, Noriaki Hashimoto, Ichiro Takeuchi, Hiroaki Miyoshi, Koichi Ohshima, Jun Sakuma</li>
<li>for: 这个论文主要targets Whole-slide image analysis for computational pathology, with the goal of developing an efficient and effective method for processing large-scale datasets.</li>
<li>methods: The proposed method uses a combination of feature embedding and clustering to preprocess the full whole-slide image into a reduced prototype representation, which is then fed into a suitable MLP-Mixer architecture.</li>
<li>results: The proposed method achieves comparable performance to current state-of-the-art methods while achieving lower training costs in terms of computational time and memory load, as demonstrated through experiments on two public benchmarks and one in-house malignant lymphoma dataset.<details>
<summary>Abstract</summary>
Whole-slide image analysis via the means of computational pathology often relies on processing tessellated gigapixel images with only slide-level labels available. Applying multiple instance learning-based methods or transformer models is computationally expensive as, for each image, all instances have to be processed simultaneously. The MLP-Mixer is an under-explored alternative model to common vision transformers, especially for large-scale datasets. Due to the lack of a self-attention mechanism, they have linear computational complexity to the number of input patches but achieve comparable performance on natural image datasets. We propose a combination of feature embedding and clustering to preprocess the full whole-slide image into a reduced prototype representation which can then serve as input to a suitable MLP-Mixer architecture. Our experiments on two public benchmarks and one inhouse malignant lymphoma dataset show comparable performance to current state-of-the-art methods, while achieving lower training costs in terms of computational time and memory load. Code is publicly available at https://github.com/butkej/ProtoMixer.
</details>
<details>
<summary>摘要</summary>
整幕图像分析通常通过计算 pathology 利用处理分割的 gigapixel 图像，只有推送批处理的标签可用。应用多个实例学习基于方法或 transformer 模型是计算昂贵的，因为每个图像都需要同时处理所有实例。 MLP-Mixer 是一种未得到充分探索的代码模型，特别是 для大规模数据集。由于缺乏自我注意机制，它们有线性的计算复杂度与输入patches的数量相同，但在自然图像数据集上实现了相似的性能。我们提议将整个整幕图像 препроцессинг到一个减少的原型表示，然后用 suitable MLP-Mixer 架构进行进一步处理。我们的实验结果表明，在两个公共标准测试集和一个内部恶性淋巴癌数据集上，我们的方法可以与当前状态的方法相比，实现较低的训练成本，包括计算时间和内存负担。代码可以在 https://github.com/butkej/ProtoMixer 上获取。
</details></li>
</ul>
<hr>
<h2 id="Minimalist-and-High-Performance-Semantic-Segmentation-with-Plain-Vision-Transformers"><a href="#Minimalist-and-High-Performance-Semantic-Segmentation-with-Plain-Vision-Transformers" class="headerlink" title="Minimalist and High-Performance Semantic Segmentation with Plain Vision Transformers"></a>Minimalist and High-Performance Semantic Segmentation with Plain Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12755">http://arxiv.org/abs/2310.12755</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ydhonghit/plainseg">https://github.com/ydhonghit/plainseg</a></li>
<li>paper_authors: Yuanduo Hong, Jue Wang, Weichao Sun, Huihui Pan</li>
<li>for: 本研究的目的是提供简单高效的基准系统，用于实际semantic segmentation tasks中的简单ViT模型。</li>
<li>methods: 我们使用了简单的3$\times$3卷积layer和 transformer层（encoder或decoder）来构建我们的模型，并进行了大量的实验研究，以了解高性能semantic segmentation的两个基本原则：（1）高分辨率特征是高性能的关键，即使使用简单的upsampling技术，和（2）较为窄的transformer decoder需要许多更大的学习率。</li>
<li>results: 我们的方法在四个流行的benchmark上得到了高性能和效率的result，并可以作为评估基准模型在semantic segmentation中的转移能力的工具。<details>
<summary>Abstract</summary>
In the wake of Masked Image Modeling (MIM), a diverse range of plain, non-hierarchical Vision Transformer (ViT) models have been pre-trained with extensive datasets, offering new paradigms and significant potential for semantic segmentation. Current state-of-the-art systems incorporate numerous inductive biases and employ cumbersome decoders. Building upon the original motivations of plain ViTs, which are simplicity and generality, we explore high-performance `minimalist' systems to this end. Our primary purpose is to provide simple and efficient baselines for practical semantic segmentation with plain ViTs. Specifically, we first explore the feasibility and methodology for achieving high-performance semantic segmentation using the last feature map. As a result, we introduce the PlainSeg, a model comprising only three 3$\times$3 convolutions in addition to the transformer layers (either encoder or decoder). In this process, we offer insights into two underlying principles: (i) high-resolution features are crucial to high performance in spite of employing simple up-sampling techniques and (ii) the slim transformer decoder requires a much larger learning rate than the wide transformer decoder. On this basis, we further present the PlainSeg-Hier, which allows for the utilization of hierarchical features. Extensive experiments on four popular benchmarks demonstrate the high performance and efficiency of our methods. They can also serve as powerful tools for assessing the transfer ability of base models in semantic segmentation. Code is available at \url{https://github.com/ydhongHIT/PlainSeg}.
</details>
<details>
<summary>摘要</summary>
在Masked Image Modeling（MIM）的投射下，一些简单、非层次的视觉变换器（ViT）模型已经在大量数据上预训练，为Semantic Segmentation带来了新的思路和潜在性。当前领先的系统通常包含许多拟合因子和复杂的解码器。我们从原始的简单ViT的目的开始，即简单和通用，探索高性能的`最小主义'系统。我们的主要目标是提供简单、高效的Semantic Segmentation基线，特别是使用最后一个特征图进行Semantic Segmentation。为此，我们提出了PlainSeg模型，它只有3个3x3卷积 layer以及变换层（ither encoder或decoder）。在这个过程中，我们提供了两个基本原则：（i）高分辨率特征是高性能的关键，即使使用简单的upsampling技术，和（ii）短Transformer解码器需要 Much larger learning rate than wide Transformer解码器。基于这两个原则，我们进一步提出了PlainSeg-Hier，它允许使用层次特征。我们的实验表明，我们的方法具有高性能和高效性，并且可以作为评估基准模型的工具。代码可以在 \url{https://github.com/ydhongHIT/PlainSeg} 中找到。
</details></li>
</ul>
<hr>
<h2 id="ExtSwap-Leveraging-Extended-Latent-Mapper-for-Generating-High-Quality-Face-Swapping"><a href="#ExtSwap-Leveraging-Extended-Latent-Mapper-for-Generating-High-Quality-Face-Swapping" class="headerlink" title="ExtSwap: Leveraging Extended Latent Mapper for Generating High Quality Face Swapping"></a>ExtSwap: Leveraging Extended Latent Mapper for Generating High Quality Face Swapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12736">http://arxiv.org/abs/2310.12736</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aravinda27/extswap">https://github.com/aravinda27/extswap</a></li>
<li>paper_authors: Aravinda Reddy PN, K. Sreenivasa Rao, Raghavendra Ramachandra, Pabitra mitra</li>
<li>for: 这篇论文是为了提出一种基于预训练的StyleGAN结构的新的面孔交换方法。</li>
<li>methods: 该方法使用了不同的编码解码结构和嵌入 интеграción网络来生成高质量结果，但其质量受到杂mix的影响。该方法通过分离 semantics来解除杂mix。</li>
<li>results: 广泛的实验表明，提议的方法成功地解除了个体和特征特征，并超越了许多现有的面孔交换方法， both qualitatively and quantitatively。<details>
<summary>Abstract</summary>
We present a novel face swapping method using the progressively growing structure of a pre-trained StyleGAN. Previous methods use different encoder decoder structures, embedding integration networks to produce high-quality results, but their quality suffers from entangled representation. We disentangle semantics by deriving identity and attribute features separately. By learning to map the concatenated features into the extended latent space, we leverage the state-of-the-art quality and its rich semantic extended latent space. Extensive experiments suggest that the proposed method successfully disentangles identity and attribute features and outperforms many state-of-the-art face swapping methods, both qualitatively and quantitatively.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的面部换位方法，使用逐渐增长的 StyleGAN 预训练结构。先前的方法使用不同的编码解码结构和嵌入集成网络来生成高质量结果，但其质量受到杂合表示的限制。我们通过分离 semantics 来解耦各个特征。我们学习将拼接的特征映射到延展的幂空间中，以利用状态 искусственный智能的高质量和它的富 semantics 延展空间。广泛的实验表明，我们提出的方法成功地解耦了特征和特征，并超越了许多现状艺术的面部换位方法， both qualitatively and quantitatively。
</details></li>
</ul>
<hr>
<h2 id="Multiscale-Motion-Aware-and-Spatial-Temporal-Channel-Contextual-Coding-Network-for-Learned-Video-Compression"><a href="#Multiscale-Motion-Aware-and-Spatial-Temporal-Channel-Contextual-Coding-Network-for-Learned-Video-Compression" class="headerlink" title="Multiscale Motion-Aware and Spatial-Temporal-Channel Contextual Coding Network for Learned Video Compression"></a>Multiscale Motion-Aware and Spatial-Temporal-Channel Contextual Coding Network for Learned Video Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12733">http://arxiv.org/abs/2310.12733</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiming Wang, Qian Huang, Bin Tang, Huashan Sun, Xing Li</li>
<li>for: 提高视频压缩效率和质量</li>
<li>methods: 提出了一种基于 Contextual Coding 的动态媒体编码网络（MASTC-VC），利用多尺度运动预测信息以优化运动识别和压缩表征，同时通过 Contextual Module 捕捉射影层次特征以减少位帧率。</li>
<li>results: 对三个公共的测试数据集进行了广泛的实验，并证明了 MASTC-VC 比前一代方法（H.265&#x2F;HEVC 和 H.266&#x2F;VVC）在 PSNR 和 MS-SSIM 指标上具有较高的效率和质量。具体来说，MASTC-VC 在 PSNR 指标下提供了10.15% 的BD-rate 减少，并在 MS-SSIM 指标下提供了23.93% 的BD-rate 减少。<details>
<summary>Abstract</summary>
Recently, learned video compression has achieved exciting performance. Following the traditional hybrid prediction coding framework, most learned methods generally adopt the motion estimation motion compensation (MEMC) method to remove inter-frame redundancy. However, inaccurate motion vector (MV) usually lead to the distortion of reconstructed frame. In addition, most approaches ignore the spatial and channel redundancy. To solve above problems, we propose a motion-aware and spatial-temporal-channel contextual coding based video compression network (MASTC-VC), which learns the latent representation and uses variational autoencoders (VAEs) to capture the characteristics of intra-frame pixels and inter-frame motion. Specifically, we design a multiscale motion-aware module (MS-MAM) to estimate spatial-temporal-channel consistent motion vector by utilizing the multiscale motion prediction information in a coarse-to-fine way. On the top of it, we further propose a spatial-temporal-channel contextual module (STCCM), which explores the correlation of latent representation to reduce the bit consumption from spatial, temporal and channel aspects respectively. Comprehensive experiments show that our proposed MASTC-VC is surprior to previous state-of-the-art (SOTA) methods on three public benchmark datasets. More specifically, our method brings average 10.15\% BD-rate savings against H.265/HEVC (HM-16.20) in PSNR metric and average 23.93\% BD-rate savings against H.266/VVC (VTM-13.2) in MS-SSIM metric.
</details>
<details>
<summary>摘要</summary>
Specifically, we design a multiscale motion-aware module (MS-MAM) to estimate spatial-temporal-channel consistent motion vector by utilizing the multiscale motion prediction information in a coarse-to-fine way. Additionally, we propose a spatial-temporal-channel contextual module (STCCM), which explores the correlation of latent representation to reduce the bit consumption from spatial, temporal, and channel aspects, respectively.Comprehensive experiments show that our proposed MASTC-VC is superior to previous state-of-the-art (SOTA) methods on three public benchmark datasets. Specifically, our method achieves an average 10.15% BD-rate savings against H.265/HEVC (HM-16.20) in PSNR metric and an average 23.93% BD-rate savings against H.266/VVC (VTM-13.2) in MS-SSIM metric.
</details></li>
</ul>
<hr>
<h2 id="Query-aware-Long-Video-Localization-and-Relation-Discrimination-for-Deep-Video-Understanding"><a href="#Query-aware-Long-Video-Localization-and-Relation-Discrimination-for-Deep-Video-Understanding" class="headerlink" title="Query-aware Long Video Localization and Relation Discrimination for Deep Video Understanding"></a>Query-aware Long Video Localization and Relation Discrimination for Deep Video Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12724">http://arxiv.org/abs/2310.12724</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanxing Xu, Yuting Wei, Bin Wu</li>
<li>for: This paper aims to address the problem of holistically analyzing long videos and extracting useful knowledge to solve different types of queries.</li>
<li>methods: The proposed method uses an imagelanguage pretrained model to select frames pertinent to queries, obviating the need for a complete movie-level knowledge graph.</li>
<li>results: The approach achieved first and fourth positions for two groups of movie-level queries, demonstrating its effectiveness and robustness.Here’s the full Chinese text:</li>
<li>for: 本文目的是解决长视频的全面分析和提取有用知识，以解决不同类型的查询。</li>
<li>methods: 提议的方法使用图语言预训练模型，选择关键词 pertinent 查询的帧，不需要完整的电影级别知识图。</li>
<li>results: 方法在两组电影级别查询中 achieved 第一名和第四名，表明其效果和稳定性。<details>
<summary>Abstract</summary>
The surge in video and social media content underscores the need for a deeper understanding of multimedia data. Most of the existing mature video understanding techniques perform well with short formats and content that requires only shallow understanding, but do not perform well with long format videos that require deep understanding and reasoning. Deep Video Understanding (DVU) Challenge aims to push the boundaries of multimodal extraction, fusion, and analytics to address the problem of holistically analyzing long videos and extract useful knowledge to solve different types of queries. This paper introduces a query-aware method for long video localization and relation discrimination, leveraging an imagelanguage pretrained model. This model adeptly selects frames pertinent to queries, obviating the need for a complete movie-level knowledge graph. Our approach achieved first and fourth positions for two groups of movie-level queries. Sufficient experiments and final rankings demonstrate its effectiveness and robustness.
</details>
<details>
<summary>摘要</summary>
文中的媒体内容增长趋势高亮了深入理解多媒体数据的需要。现有的大多数成熟的视频理解技术在短格式内容上表现良好，但是对长格式视频进行深入理解和推理是不够。深入视频理解（DVU）挑战旨在推动多模态抽取、融合和分析，解决不同类型的查询问题。本文介绍了一种基于图像语言预训练模型的查询意识方法，可以选择相关的查询关键帧，从而避免需要完整的电影级别知识图。我们的方法在两组电影级别查询中获得了第一和第四名，经过了丰富的实验和最终排名，证明了其效果和可靠性。
</details></li>
</ul>
<hr>
<h2 id="Generating-Robust-Adversarial-Examples-against-Online-Social-Networks-OSNs"><a href="#Generating-Robust-Adversarial-Examples-against-Online-Social-Networks-OSNs" class="headerlink" title="Generating Robust Adversarial Examples against Online Social Networks (OSNs)"></a>Generating Robust Adversarial Examples against Online Social Networks (OSNs)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12708">http://arxiv.org/abs/2310.12708</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/csjunjun/robustosnattack">https://github.com/csjunjun/robustosnattack</a></li>
<li>paper_authors: Jun Liu, Jiantao Zhou, Haiwei Wu, Weiwei Sun, Jinyu Tian</li>
<li>for: 本文旨在设计一种可以在在线社交网络（OSN）上传输的Robust adversarial example（AE）攻击方法，以便AE在OSN上传输后仍保持攻击能力。</li>
<li>methods: 本文提出了一种基于 differentiable network（DN）的optimization框架，用于生成Robust AE。 Specifically, the framework includes a differentiable JPEG layer and an encoder-decoder subnetwork to simulate the operations conducted by an OSN.</li>
<li>results: 对Facebook、WeChat和QQ进行了广泛的实验，结果表明， compared to existing methods, our attack methods can produce more robust AEs, especially under small distortion constraints. The performance gain in terms of Attack Success Rate (ASR) can be more than 60%. In addition, we built a public dataset containing more than 10,000 pairs of AEs processed by Facebook, WeChat or QQ, which can facilitate future research in the robust AEs generation.<details>
<summary>Abstract</summary>
Online Social Networks (OSNs) have blossomed into prevailing transmission channels for images in the modern era. Adversarial examples (AEs) deliberately designed to mislead deep neural networks (DNNs) are found to be fragile against the inevitable lossy operations conducted by OSNs. As a result, the AEs would lose their attack capabilities after being transmitted over OSNs. In this work, we aim to design a new framework for generating robust AEs that can survive the OSN transmission; namely, the AEs before and after the OSN transmission both possess strong attack capabilities. To this end, we first propose a differentiable network termed SImulated OSN (SIO) to simulate the various operations conducted by an OSN. Specifically, the SIO network consists of two modules: 1) a differentiable JPEG layer for approximating the ubiquitous JPEG compression and 2) an encoder-decoder subnetwork for mimicking the remaining operations. Based upon the SIO network, we then formulate an optimization framework to generate robust AEs by enforcing model outputs with and without passing through the SIO to be both misled. Extensive experiments conducted over Facebook, WeChat and QQ demonstrate that our attack methods produce more robust AEs than existing approaches, especially under small distortion constraints; the performance gain in terms of Attack Success Rate (ASR) could be more than 60%. Furthermore, we build a public dataset containing more than 10,000 pairs of AEs processed by Facebook, WeChat or QQ, facilitating future research in the robust AEs generation. The dataset and code are available at https://github.com/csjunjun/RobustOSNAttack.git.
</details>
<details>
<summary>摘要</summary>
在现代时期，在线社交网络（OSNs）已经成为图像传输的主流途径。敌意例子（AEs），特意设计用于欺骗深度神经网络（DNNs），在OSNs中的各种lossy操作后失去攻击能力。在这项工作中，我们目的是设计一个新的框架，以生成具有抵抗力的AEs，使其在OSNs中传输后仍保持攻击能力。为此，我们首先提出一个可导网络，称为模拟OSN（SIO）网络。specifically，SIO网络由两个模块组成：1）一个可导JPEG层，用于模拟广泛存在的JPEG压缩；2）一个Encoder-Decoder子网络，用于模拟剩下的操作。基于SIO网络，我们然后建立了一个优化框架，用于生成抵抗力强的AEs，通过对SLO outputs with和without passing through SIO进行权衡来实现。经验表明，我们的攻击方法可以生成更加抵抗力强的AEs，特别是在小质量约束下，Attack Success Rate（ASR）的性能提升可以高达60%。此外，我们建立了一个包含more than 10,000个AEs processed by Facebook, WeChat or QQ的公共数据集，以便未来的研究人员进行抵抗AEs生成的研究。数据集和代码可以在https://github.com/csjunjun/RobustOSNAttack.git中获取。
</details></li>
</ul>
<hr>
<h2 id="Recoverable-Privacy-Preserving-Image-Classification-through-Noise-like-Adversarial-Examples"><a href="#Recoverable-Privacy-Preserving-Image-Classification-through-Noise-like-Adversarial-Examples" class="headerlink" title="Recoverable Privacy-Preserving Image Classification through Noise-like Adversarial Examples"></a>Recoverable Privacy-Preserving Image Classification through Noise-like Adversarial Examples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12707">http://arxiv.org/abs/2310.12707</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/csjunjun/ric">https://github.com/csjunjun/ric</a></li>
<li>paper_authors: Jun Liu, Jiantao Zhou, Jinyu Tian, Weiwei Sun</li>
<li>for: 本研究旨在保障云计算平台上进行的图像相关服务中的数据隐私。</li>
<li>methods: 我们提出了一种新的隐私保护图像分类方案，可以将纯文本预测器应用到加密图像中，而不需要重新训练专门的预测器。此外，加密图像可以通过秘钥进行解密，并且可以保持图像的原始形式。</li>
<li>results: 我们的方案可以保持纯文本预测器在加密和纯文本域中的预测精度相同，并且可以高效地解密加密图像，保持图像的原始形式，并且具有满意的泛化能力和高安全性。<details>
<summary>Abstract</summary>
With the increasing prevalence of cloud computing platforms, ensuring data privacy during the cloud-based image related services such as classification has become crucial. In this study, we propose a novel privacypreserving image classification scheme that enables the direct application of classifiers trained in the plaintext domain to classify encrypted images, without the need of retraining a dedicated classifier. Moreover, encrypted images can be decrypted back into their original form with high fidelity (recoverable) using a secret key. Specifically, our proposed scheme involves utilizing a feature extractor and an encoder to mask the plaintext image through a newly designed Noise-like Adversarial Example (NAE). Such an NAE not only introduces a noise-like visual appearance to the encrypted image but also compels the target classifier to predict the ciphertext as the same label as the original plaintext image. At the decoding phase, we adopt a Symmetric Residual Learning (SRL) framework for restoring the plaintext image with minimal degradation. Extensive experiments demonstrate that 1) the classification accuracy of the classifier trained in the plaintext domain remains the same in both the ciphertext and plaintext domains; 2) the encrypted images can be recovered into their original form with an average PSNR of up to 51+ dB for the SVHN dataset and 48+ dB for the VGGFace2 dataset; 3) our system exhibits satisfactory generalization capability on the encryption, decryption and classification tasks across datasets that are different from the training one; and 4) a high-level of security is achieved against three potential threat models. The code is available at https://github.com/csjunjun/RIC.git.
</details>
<details>
<summary>摘要</summary>
随着云计算平台的普及，在云上进行图像相关服务，如分类，保持数据隐私已成为关键。在本研究中，我们提出了一种新的隐私保护图像分类方案，允许直接在批处文本领域训练的分类器进行加密图像的分类，无需重新训练专门的分类器。此外，加密图像可以使用秘钥回到原始形式中，保持高比特率（可 восстанови）。具体来说，我们的提议方案包括使用特征提取器和编码器将批处文本图像做遮盾，并使用新设计的噪声类对抗例（NAE）。这种NAE不仅将加密图像具有噪声类视觉外观，还迫使目标分类器预测加密图像的标签与原始批处文本图像的标签相同。在解码阶段，我们采用了对称差异学习（SRL）框架，以最小化plaintext图像的受损。我们的实验结果表明：1）在批处文本领域和加密领域中，分类器的分类精度保持不变；2）加密图像可以高效地还原到原始形式，PSNR平均值达到51+ dB для SVHN数据集和48+ dB для VGGFace2数据集；3）我们的系统在不同的数据集上展现了满意的总体化能力；4）对于三种威胁模型，我们的系统实现了高度的安全性。代码可以在https://github.com/csjunjun/RIC.git中找到。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Low-confidence-Pseudo-labels-for-Source-free-Object-Detection"><a href="#Exploiting-Low-confidence-Pseudo-labels-for-Source-free-Object-Detection" class="headerlink" title="Exploiting Low-confidence Pseudo-labels for Source-free Object Detection"></a>Exploiting Low-confidence Pseudo-labels for Source-free Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12705">http://arxiv.org/abs/2310.12705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhihong Chen, Zilei Wang, Yixin Zhang</li>
<li>for: 这个研究旨在适应无标注目标领域中使用源领域训练的检测器，不需要存取源标注数据。</li>
<li>methods: 我们提出了一新的方法，使用高和低信任阈值来全面利用 Pseudo-labels。特别是，高信任阈值上的 Pseudo-labels 使用传统方式，而低信任阈值到中阈值的 Pseudo-labels 通过 Local Spatial Contrastive Learning (LSCL) 和 Proposal Soft Training (PST) 两个组件来进一步提高模型的表现。</li>
<li>results: 我们的方法在五个跨领域物体检测 benchmark 上进行了广泛的实验，结果显示我们的方法可以超越现有的 SFOD 方法， achieved state-of-the-art 性能。<details>
<summary>Abstract</summary>
Source-free object detection (SFOD) aims to adapt a source-trained detector to an unlabeled target domain without access to the labeled source data. Current SFOD methods utilize a threshold-based pseudo-label approach in the adaptation phase, which is typically limited to high-confidence pseudo-labels and results in a loss of information. To address this issue, we propose a new approach to take full advantage of pseudo-labels by introducing high and low confidence thresholds. Specifically, the pseudo-labels with confidence scores above the high threshold are used conventionally, while those between the low and high thresholds are exploited using the Low-confidence Pseudo-labels Utilization (LPU) module. The LPU module consists of Proposal Soft Training (PST) and Local Spatial Contrastive Learning (LSCL). PST generates soft labels of proposals for soft training, which can mitigate the label mismatch problem. LSCL exploits the local spatial relationship of proposals to improve the model's ability to differentiate between spatially adjacent proposals, thereby optimizing representational features further. Combining the two components overcomes the challenges faced by traditional methods in utilizing low-confidence pseudo-labels. Extensive experiments on five cross-domain object detection benchmarks demonstrate that our proposed method outperforms the previous SFOD methods, achieving state-of-the-art performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Representation-Learning-via-Consistent-Assignment-of-Views-over-Random-Partitions"><a href="#Representation-Learning-via-Consistent-Assignment-of-Views-over-Random-Partitions" class="headerlink" title="Representation Learning via Consistent Assignment of Views over Random Partitions"></a>Representation Learning via Consistent Assignment of Views over Random Partitions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12692">http://arxiv.org/abs/2310.12692</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sthalles/carp">https://github.com/sthalles/carp</a></li>
<li>paper_authors: Thalles Silva, Adín Ramírez Rivera</li>
<li>for: 这篇论文是用于自适应对映射学习的可观察性聚合方法，即Consistent Assignment of Views over Random Partitions (CARP)。</li>
<li>methods: CARP 使用梯度下降来学习代表特征，并且不需要额外的非数学模组来解决聚合问题。CARP 使用一个新的随机分割概念来训练模型，并且强制不同观点之间的一致性。</li>
<li>results: CARP 可以在17个数据集上学习出适合的表示，并且在多种自适应学习任务上表现出色。与11个现有的自适应方法进行比较，CARP 在转移学习任务中表现最好，并且在训练时间较短的情况下表现更好。<details>
<summary>Abstract</summary>
We present Consistent Assignment of Views over Random Partitions (CARP), a self-supervised clustering method for representation learning of visual features. CARP learns prototypes in an end-to-end online fashion using gradient descent without additional non-differentiable modules to solve the cluster assignment problem. CARP optimizes a new pretext task based on random partitions of prototypes that regularizes the model and enforces consistency between views' assignments. Additionally, our method improves training stability and prevents collapsed solutions in joint-embedding training. Through an extensive evaluation, we demonstrate that CARP's representations are suitable for learning downstream tasks. We evaluate CARP's representations capabilities in 17 datasets across many standard protocols, including linear evaluation, few-shot classification, k-NN, k-means, image retrieval, and copy detection. We compare CARP performance to 11 existing self-supervised methods. We extensively ablate our method and demonstrate that our proposed random partition pretext task improves the quality of the learned representations by devising multiple random classification tasks. In transfer learning tasks, CARP achieves the best performance on average against many SSL methods trained for a longer time.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种自助学习 clustering 方法，即 Consistent Assignment of Views over Random Partitions (CARP)，用于学习视觉特征的表示。CARP 通过梯度下降来学习抽象，不需要额外的非 diferenciable 模块来解决归一化问题。CARP 优化了一个基于随机分区的 проtotypes 的新预测任务，用于规范模型并确保视图归一化的一致性。此外，我们的方法还提高了共同嵌入训练的稳定性，避免了共同嵌入训练中的坍缩解决方案。通过广泛的评估，我们表明了 CARP 的表示是适合学习下游任务的。我们在 17 个数据集上进行了广泛的评估，包括线性评估、少量分类、k-NN、k-means、图像检索和复制检测等几种标准协议。我们与 11 种现有的自助学习方法进行了比较，并证明了我们提posed的随机分区预测任务可以提高学习的表示质量。在转移学习任务中，CARP 的表示性能在许多 SSL 方法已经训练了更长时间后仍然保持最高的平均性能。
</details></li>
</ul>
<hr>
<h2 id="TapMo-Shape-aware-Motion-Generation-of-Skeleton-free-Characters"><a href="#TapMo-Shape-aware-Motion-Generation-of-Skeleton-free-Characters" class="headerlink" title="TapMo: Shape-aware Motion Generation of Skeleton-free Characters"></a>TapMo: Shape-aware Motion Generation of Skeleton-free Characters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12678">http://arxiv.org/abs/2310.12678</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxu Zhang, Shaoli Huang, Zhigang Tu, Xin Chen, Xiaohang Zhan, Gang Yu, Ying Shan</li>
<li>for: 本研究旨在生成多种非骨架3D人物动作。</li>
<li>methods: 该方法包括两个主要组件： mesh 处理预测器和形态意识扩散模块。 mesh 处理预测器 预测皮肤精度和自适应扩散控制，而 shape-aware 动作扩散模块 使用文本指导动作和形态特征来生成具有形态考虑的动作。</li>
<li>results: 对比其他自动动画方法，TapMo 能够生成高质量的动作，并且能够涵盖多种非人物3D模型。<details>
<summary>Abstract</summary>
Previous motion generation methods are limited to the pre-rigged 3D human model, hindering their applications in the animation of various non-rigged characters. In this work, we present TapMo, a Text-driven Animation Pipeline for synthesizing Motion in a broad spectrum of skeleton-free 3D characters. The pivotal innovation in TapMo is its use of shape deformation-aware features as a condition to guide the diffusion model, thereby enabling the generation of mesh-specific motions for various characters. Specifically, TapMo comprises two main components - Mesh Handle Predictor and Shape-aware Diffusion Module. Mesh Handle Predictor predicts the skinning weights and clusters mesh vertices into adaptive handles for deformation control, which eliminates the need for traditional skeletal rigging. Shape-aware Motion Diffusion synthesizes motion with mesh-specific adaptations. This module employs text-guided motions and mesh features extracted during the first stage, preserving the geometric integrity of the animations by accounting for the character's shape and deformation. Trained in a weakly-supervised manner, TapMo can accommodate a multitude of non-human meshes, both with and without associated text motions. We demonstrate the effectiveness and generalizability of TapMo through rigorous qualitative and quantitative experiments. Our results reveal that TapMo consistently outperforms existing auto-animation methods, delivering superior-quality animations for both seen or unseen heterogeneous 3D characters.
</details>
<details>
<summary>摘要</summary>
previous motion generation methods are limited to pre-rigged 3D human models, which hinders their applications in animating various non-rigged characters. In this work, we present TapMo, a text-driven animation pipeline for synthesizing motion in a broad spectrum of skeleton-free 3D characters. The key innovation of TapMo is its use of shape deformation-aware features as a condition to guide the diffusion model, allowing for the generation of mesh-specific motions for various characters. TapMo consists of two main components: Mesh Handle Predictor and Shape-aware Motion Diffusion Module. The Mesh Handle Predictor predicts the skinning weights and clusters mesh vertices into adaptive handles for deformation control, eliminating the need for traditional skeletal rigging. The Shape-aware Motion Diffusion module synthesizes motion with mesh-specific adaptations, using text-guided motions and mesh features extracted during the first stage. This module preserves the geometric integrity of the animations by accounting for the character's shape and deformation. TapMo is trained in a weakly-supervised manner and can accommodate a multitude of non-human meshes, both with and without associated text motions. Our experiments show that TapMo consistently outperforms existing auto-animation methods, delivering superior-quality animations for both seen and unseen heterogeneous 3D characters.
</details></li>
</ul>
<hr>
<h2 id="Weakly-Supervised-Learning-for-Breast-Cancer-Prediction-on-Mammograms-in-Realistic-Settings"><a href="#Weakly-Supervised-Learning-for-Breast-Cancer-Prediction-on-Mammograms-in-Realistic-Settings" class="headerlink" title="Weakly Supervised Learning for Breast Cancer Prediction on Mammograms in Realistic Settings"></a>Weakly Supervised Learning for Breast Cancer Prediction on Mammograms in Realistic Settings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12677">http://arxiv.org/abs/2310.12677</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shreyasi Pathak, Jörg Schlötterer, Jeroen Geerdink, Onno Dirk Vijlbrief, Maurice van Keulen, Christin Seifert</li>
<li>for: 早期检测乳腺癌的自动方法可以减少死亡率，但目前在医院中广泛应用受限因为这些方法具有太多约束。</li>
<li>methods: 这些方法假设有可用的注释 для单个图像或甚至是区域对象（ROI），以及固定的图像数量。</li>
<li>results: 在实际医疗设置下，我们发现了一种两级多例学习（MIL）方法，可以在只有案例标签而不是每个图像或ROI的情况下进行检测。我们还提出了封装特有的MIL池化变体，以便在 breast cancer 的一侧出现。我们的研究表明，这种两级 MIL 方法可以在现实的医疗设置下应用，并且可以随着患者的不断入院而扩展。<details>
<summary>Abstract</summary>
Automatic methods for early detection of breast cancer on mammography can significantly decrease mortality. Broad uptake of those methods in hospitals is currently hindered because the methods have too many constraints. They assume annotations available for single images or even regions-of-interest (ROIs), and a fixed number of images per patient. Both assumptions do not hold in a general hospital setting. Relaxing those assumptions results in a weakly supervised learning setting, where labels are available per case, but not for individual images or ROIs. Not all images taken for a patient contain malignant regions and the malignant ROIs cover only a tiny part of an image, whereas most image regions represent benign tissue. In this work, we investigate a two-level multi-instance learning (MIL) approach for case-level breast cancer prediction on two public datasets (1.6k and 5k cases) and an in-house dataset of 21k cases. Observing that breast cancer is usually only present in one side, while images of both breasts are taken as a precaution, we propose a domain-specific MIL pooling variant. We show that two-level MIL can be applied in realistic clinical settings where only case labels, and a variable number of images per patient are available. Data in realistic settings scales with continuous patient intake, while manual annotation efforts do not. Hence, research should focus in particular on unsupervised ROI extraction, in order to improve breast cancer prediction for all patients.
</details>
<details>
<summary>摘要</summary>
自动方法可以早期检测乳腺癌，可以大幅降低死亡率。然而，现在医院中广泛应用这些方法受到限制，因为这些方法假设有可用的注释 для单个图像或Region of Interest（ROI），以及固定的图像数量每个患者。这两个假设在普通医院设置中不成立。在弱监督学习 Setting中，标签只有每个案例的Level，而不是每个图像或ROI。不同于其他图像，大多数图像区域表示正常的组织。在这项工作中，我们调查了一种两级多例学习（MIL）方法，用于检测乳腺癌的情况。我们注意到，乳腺癌通常只存在一侧，而图像中收集的两侧图像是为了预防。我们提议一种域Specific MIL Pooling变体。我们表明，两级 MIL可以在现实的医疗设置中应用，只有案例标签和每个患者的变数量图像可以获得。数据在实际设置中随着患者的不断入院而增加，而手动标注努力则不会增加。因此，研究应该专注于无监督ROI提取，以提高乳腺癌预测的准确性。
</details></li>
</ul>
<hr>
<h2 id="TRUSTED-The-Paired-3D-Transabdominal-Ultrasound-and-CT-Human-Data-for-Kidney-Segmentation-and-Registration-Research"><a href="#TRUSTED-The-Paired-3D-Transabdominal-Ultrasound-and-CT-Human-Data-for-Kidney-Segmentation-and-Registration-Research" class="headerlink" title="TRUSTED: The Paired 3D Transabdominal Ultrasound and CT Human Data for Kidney Segmentation and Registration Research"></a>TRUSTED: The Paired 3D Transabdominal Ultrasound and CT Human Data for Kidney Segmentation and Registration Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12646">http://arxiv.org/abs/2310.12646</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Ndzimbong, Cyril Fourniol, Loic Themyr, Nicolas Thome, Yvonne Keeza, Beniot Sauer, Pierre-Thierry Piechaud, Arnaud Mejean, Jacques Marescaux, Daniel George, Didier Mutter, Alexandre Hostettler, Toby Collins</li>
<li>For: The paper is written for researchers to develop and validate new image segmentation and image-modality registration methods using abdominal ultrasound (US) data.* Methods: The paper uses a dataset of paired transabdominal 3DUS and CT kidney images, with segmentation and anatomical landmark annotations, to evaluate the performance of different deep learning models and image registration methods.* Results: The paper reports the results of benchmarking five deep learning models for automatic kidney segmentation, with average DICE scores ranging from 83.2% to 89.1% for CT images and 61.9% to 79.4% for US images. The paper also reports the results of benchmarking three image registration methods, with Coherent Point Drift performing best with an average Target Registration Error of 4.53mm.<details>
<summary>Abstract</summary>
Inter-modal image registration (IMIR) and image segmentation with abdominal Ultrasound (US) data has many important clinical applications, including image-guided surgery, automatic organ measurement and robotic navigation. However, research is severely limited by the lack of public datasets. We propose TRUSTED (the Tridimensional Renal Ultra Sound TomodEnsitometrie Dataset), comprising paired transabdominal 3DUS and CT kidney images from 48 human patients (96 kidneys), including segmentation, and anatomical landmark annotations by two experienced radiographers. Inter-rater segmentation agreement was over 94 (Dice score), and gold-standard segmentations were generated using the STAPLE algorithm. Seven anatomical landmarks were annotated, important for IMIR systems development and evaluation. To validate the dataset's utility, 5 competitive Deep Learning models for automatic kidney segmentation were benchmarked, yielding average DICE scores from 83.2% to 89.1% for CT, and 61.9% to 79.4% for US images. Three IMIR methods were benchmarked, and Coherent Point Drift performed best with an average Target Registration Error of 4.53mm. The TRUSTED dataset may be used freely researchers to develop and validate new segmentation and IMIR methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified Chinese<</SYS>>多Modal图像匹配（IMIR）和腹部超声图像分割（US）数据在临床应用中具有重要的临床应用，包括图像导航、自动器官测量和 робоaxiNavigation。然而，研究受到公共数据的缺乏所限。我们提出了TRUSTED（三维肾脏超声TomodEnsitometrie Dataset），包括48名人类病人（96个肾脏）的对称的肾脏3DUS和CT图像，以及分割和解剖标志注解由两名经验丰富的放射学家。对于每个病人，两名放射学家进行了分割，并取得了多于94%的重合率（Dice分数）。此外，我们还生成了金标准分割，使用STAPLE算法。 Seven个解剖标志被注解，对IMIR系统的开发和评估具有重要性。为验证数据集的有用性，我们对5种竞争型深度学习模型进行了自动肾脏分割的测试，其中CT图像的平均DICE分数在83.2%到89.1%之间，而US图像的平均DICE分数在61.9%到79.4%之间。此外，我们还测试了3种IMIR方法，并确定了coherent Point Drift方法为最佳，其Target Registration Error平均值为4.53毫米。TRUSTED数据集可供研究人员免费使用，以开发和验证新的分割和IMIR方法。
</details></li>
</ul>
<hr>
<h2 id="SIRe-IR-Inverse-Rendering-for-BRDF-Reconstruction-with-Shadow-and-Illumination-Removal-in-High-Illuminance-Scenes"><a href="#SIRe-IR-Inverse-Rendering-for-BRDF-Reconstruction-with-Shadow-and-Illumination-Removal-in-High-Illuminance-Scenes" class="headerlink" title="SIRe-IR: Inverse Rendering for BRDF Reconstruction with Shadow and Illumination Removal in High-Illuminance Scenes"></a>SIRe-IR: Inverse Rendering for BRDF Reconstruction with Shadow and Illumination Removal in High-Illuminance Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13030">http://arxiv.org/abs/2310.13030</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ingra14m/sire-ir">https://github.com/ingra14m/sire-ir</a></li>
<li>paper_authors: Ziyi Yang, Yanzhen Chen, Xinyu Gao, Yazhen Yuan, Yu Wu, Xiaowei Zhou, Xiaogang Jin</li>
<li>for: 这个论文旨在解决强光照下的透视图恢复问题， existing implicit neural inverse rendering方法在强光照下受到阴影和反射的影响，难以正确理解场景几何结构，从而导致精度因素难以分解。</li>
<li>methods: 该方法使用非线性映射和规则化可见性估计来分解场景为环境地图、颜色、粗糙度。通过准确地模型 indirect 辐射场，正常、可见性和直接光同时描述，可以减少阴影和反射的影响，不需要对场景做严格的限制。</li>
<li>results: 该方法可以在强光照下高质量地恢复颜色和粗糙度，不受阴影干扰。与现有方法相比，SIRe-IR 在量化和质量上均表现出优于现有方法。<details>
<summary>Abstract</summary>
Implicit neural representation has opened up new possibilities for inverse rendering. However, existing implicit neural inverse rendering methods struggle to handle strongly illuminated scenes with significant shadows and indirect illumination. The existence of shadows and reflections can lead to an inaccurate understanding of scene geometry, making precise factorization difficult. To this end, we present SIRe-IR, an implicit neural inverse rendering approach that uses non-linear mapping and regularized visibility estimation to decompose the scene into environment map, albedo, and roughness. By accurately modeling the indirect radiance field, normal, visibility, and direct light simultaneously, we are able to remove both shadows and indirect illumination in materials without imposing strict constraints on the scene. Even in the presence of intense illumination, our method recovers high-quality albedo and roughness with no shadow interference. SIRe-IR outperforms existing methods in both quantitative and qualitative evaluations.
</details>
<details>
<summary>摘要</summary>
启用隐藏神经表示法打开了新的可逆渲染可能性。然而，现有的隐藏神经反射方法在强烈照明场景中受到强烈阴影和反射的影响，导致场景几何理解不准确，精确因子化困难。为了解决这问题，我们提出了SIRe-IR方法，它使用非线性映射和规则化可见性估计来分解场景为环境图、抛光率和粗糙度。通过准确模型 indirect radiance field, normal, visibility 和直接照明同时，我们可以去除物体表面上的阴影和 indirect illumination，无需对场景做严格的限制。即使在强烈照明下，我们的方法可以高质量地提取 albedo 和粗糙度，无阴影干扰。相比之下，SIRe-IR 方法在量化和质量两个方面都高于现有方法。
</details></li>
</ul>
<hr>
<h2 id="FUSC-Fetal-Ultrasound-Semantic-Clustering-of-Second-Trimester-Scans-Using-Deep-Self-supervised-Learning"><a href="#FUSC-Fetal-Ultrasound-Semantic-Clustering-of-Second-Trimester-Scans-Using-Deep-Self-supervised-Learning" class="headerlink" title="FUSC: Fetal Ultrasound Semantic Clustering of Second Trimester Scans Using Deep Self-supervised Learning"></a>FUSC: Fetal Ultrasound Semantic Clustering of Second Trimester Scans Using Deep Self-supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12600">http://arxiv.org/abs/2310.12600</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hussain Alasmawi, Leanne Bricker, Mohammad Yaqub</li>
<li>for: 本研究旨在开发一种自动将婴儿超声图像分类为大范围的胎儿视图，以降低或消除人工标注的需求。</li>
<li>methods: 本研究使用了一个大型数据集，共88,063张图像，并开发了一种名为Fetal Ultrasound Semantic Clustering（FUSC）方法。</li>
<li>results: 研究结果显示，FUSC方法可以在一个新的测试数据集上达到92.2%的分类纯度。<details>
<summary>Abstract</summary>
Ultrasound is the primary imaging modality in clinical practice during pregnancy. More than 140M fetuses are born yearly, resulting in numerous scans. The availability of a large volume of fetal ultrasound scans presents the opportunity to train robust machine learning models. However, the abundance of scans also has its challenges, as manual labeling of each image is needed for supervised methods. Labeling is typically labor-intensive and requires expertise to annotate the images accurately. This study presents an unsupervised approach for automatically clustering ultrasound images into a large range of fetal views, reducing or eliminating the need for manual labeling. Our Fetal Ultrasound Semantic Clustering (FUSC) method is developed using a large dataset of 88,063 images and further evaluated on an additional unseen dataset of 8,187 images achieving over 92% clustering purity. The result of our investigation hold the potential to significantly impact the field of fetal ultrasound imaging and pave the way for more advanced automated labeling solutions. Finally, we make the code and the experimental setup publicly available to help advance the field.
</details>
<details>
<summary>摘要</summary>
超声成为了临床实践中最主要的医学影像模式之一，每年超过14亿个胎儿出生，从而生成了大量的扫描图像。这些图像的丰富性带来了许多挑战，特别是需要手动标注每个图像以进行指导方法。手动标注是一项劳动密集的任务，需要专业人士准确地标注图像。本研究提出了一种无监督的方法，可以自动将超声图像分为大量的胎儿视角，从而减少或消除手动标注的需求。我们称之为胎儿超声 semantics 分 clustering（FUSC）方法，通过使用88,063张图像的大型数据集进行开发，并在其他未看过的数据集上进行了进一步的评估，实现了92%以上的分 clustering纯度。我们的发现可能会对胎儿超声成像领域产生深远的影响，并为更高级的自动标注解决方案铺平道路。最后，我们将代码和实验设置公开发布，以帮助前进该领域。
</details></li>
</ul>
<hr>
<h2 id="PrivacyGAN-robust-generative-image-privacy"><a href="#PrivacyGAN-robust-generative-image-privacy" class="headerlink" title="PrivacyGAN: robust generative image privacy"></a>PrivacyGAN: robust generative image privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12590">http://arxiv.org/abs/2310.12590</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mariia Zameshina, Marlene Careil, Olivier Teytaud, Laurent Najman</li>
<li>for: 保护人脸图像隐私	+ 传统的人脸图像隐私保护方法通常分为两类：数据毒素方法和匿名化方法。	+ 这种研究提出了一种新的方法，称为 PrivacyGAN，使用图像生成技术来保护隐私，同时保持图像使用性，特别是在社交媒体应用中。</li>
<li>methods: 使用 VQGAN 和 StyleGAN 等图像生成技术，在 embedding 空间中将原始图像Shift到一个骗图像上。	+ Drawing inspiration from Fawkes，our method entails shifting the original image within the embedding space towards a decoy image。</li>
<li>results:	+ 通过隐私度量表和新的未知图像识别技术抗性测试，我们证明了我们的方法的有效性。	+ 此外，我们还进行了人类评估，并证明了 modificated 图像仍能保持人脸识别的能力，同时避免了隐私泄露。<details>
<summary>Abstract</summary>
Classical techniques for protecting facial image privacy typically fall into two categories: data-poisoning methods, exemplified by Fawkes, which introduce subtle perturbations to images, or anonymization methods that generate images resembling the original only in several characteristics, such as gender, ethnicity, or facial expression.In this study, we introduce a novel approach, PrivacyGAN, that uses the power of image generation techniques, such as VQGAN and StyleGAN, to safeguard privacy while maintaining image usability, particularly for social media applications. Drawing inspiration from Fawkes, our method entails shifting the original image within the embedding space towards a decoy image.We evaluate our approach using privacy metrics on traditional and novel facial image datasets. Additionally, we propose new criteria for evaluating the robustness of privacy-protection methods against unknown image recognition techniques, and we demonstrate that our approach is effective even in unknown embedding transfer scenarios. We also provide a human evaluation that further proves that the modified image preserves its utility as it remains recognisable as an image of the same person by friends and family.
</details>
<details>
<summary>摘要</summary>
传统的面部图像隐私保护方法通常分为两类：数据毒素方法，如法克斯，引入微妙的抖动，或匿名化方法，生成表现类似于原始图像的图像。在这项研究中，我们介绍一种新的方法，隐私GAN，利用图像生成技术，如VQGAN和StyleGAN，保护隐私 while maintaining图像可用性，特别是在社交媒体应用程序中。 drawing inspiration from法克斯，我们的方法是将原始图像在嵌入空间中移动到一个预料图像。我们使用隐私指标进行评估，并提出了新的隐私保护方法不知情图像识别技术的抗性测试标准。我们还进行了人工评估，证明修改后的图像仍然保留了同一个人的认知度，可以被朋友和家人识别。
</details></li>
</ul>
<hr>
<h2 id="Diverse-Diffusion-Enhancing-Image-Diversity-in-Text-to-Image-Generation"><a href="#Diverse-Diffusion-Enhancing-Image-Diversity-in-Text-to-Image-Generation" class="headerlink" title="Diverse Diffusion: Enhancing Image Diversity in Text-to-Image Generation"></a>Diverse Diffusion: Enhancing Image Diversity in Text-to-Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12583">http://arxiv.org/abs/2310.12583</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mariia Zameshina, Olivier Teytaud, Laurent Najman</li>
<li>for: 提高图像生成模型中的多样性，以便生成更加真实和多样的图像。</li>
<li>methods: 提出了一种基于稳定扩散的无监督方法，通过找到稳定扩散空间中远离彼此的向量来提高图像多样性。</li>
<li>results: 通过对各种特征进行实验，包括颜色多样性、LPIPS指标和人类&#x2F;性别表现，证明了我们的多样性方法的有效性，并提供了价值的透彻视角以改进文本到图像模型。<details>
<summary>Abstract</summary>
Latent diffusion models excel at producing high-quality images from text. Yet, concerns appear about the lack of diversity in the generated imagery. To tackle this, we introduce Diverse Diffusion, a method for boosting image diversity beyond gender and ethnicity, spanning into richer realms, including color diversity.Diverse Diffusion is a general unsupervised technique that can be applied to existing text-to-image models. Our approach focuses on finding vectors in the Stable Diffusion latent space that are distant from each other. We generate multiple vectors in the latent space until we find a set of vectors that meets the desired distance requirements and the required batch size.To evaluate the effectiveness of our diversity methods, we conduct experiments examining various characteristics, including color diversity, LPIPS metric, and ethnicity/gender representation in images featuring humans.The results of our experiments emphasize the significance of diversity in generating realistic and varied images, offering valuable insights for improving text-to-image models. Through the enhancement of image diversity, our approach contributes to the creation of more inclusive and representative AI-generated art.
</details>
<details>
<summary>摘要</summary>
latent diffusion模型可以生成高质量的图像，但有关图像多样性的问题出现。为解决这问题，我们介绍了多样化催化（Diverse Diffusion），一种可以超越性别和文化多样性，探索更加丰富的颜色多样性的方法。多样化催化是一种无监督的普适技术，可以应用于现有的文本到图像模型。我们的方法是在稳定催化的幂谱空间中找到远离彼此的向量。我们通过生成多个向量在幂谱空间，直到找到符合需要的距离要求和批处理大小。为评估我们多样性方法的效果，我们进行了各种特征的实验，包括颜色多样性、LPIPS指标和人类团队表现等。实验结果表明，多样性是生成图像的重要因素，我们的方法可以提高图像的多样性，从而创造更加包容和代表性的人工智能生成艺术。
</details></li>
</ul>
<hr>
<h2 id="A-reproducible-3D-convolutional-neural-network-with-dual-attention-module-3D-DAM-for-Alzheimer’s-disease-classification"><a href="#A-reproducible-3D-convolutional-neural-network-with-dual-attention-module-3D-DAM-for-Alzheimer’s-disease-classification" class="headerlink" title="A reproducible 3D convolutional neural network with dual attention module (3D-DAM) for Alzheimer’s disease classification"></a>A reproducible 3D convolutional neural network with dual attention module (3D-DAM) for Alzheimer’s disease classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12574">http://arxiv.org/abs/2310.12574</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gia Minh Hoang, Youngjoo Lee, Jae Gwan Kim<br>for:这个研究的目的是为了提出一个可重现的模型，用于诊断阿尔茨海默症。methods:这个模型使用了3D卷积神经网络，并具有双注意模块。results:这个模型在ADNI数据库上进行训练，并在两个独立的数据集（AIBL和OASIS1）上进行验证。结果显示，这个方法可以实现91.94%的MCI进展诊断精度和96.30%的阿尔茨海默症诊断精度。此外，模型也在两个数据集上具有良好的一致性，即AIBL数据集上的精度为86.37%，OASIS1数据集上的精度为83.42%。<details>
<summary>Abstract</summary>
Alzheimer's disease is one of the most common types of neurodegenerative disease, characterized by the accumulation of amyloid-beta plaque and tau tangles. Recently, deep learning approaches have shown promise in Alzheimer's disease diagnosis. In this study, we propose a reproducible model that utilizes a 3D convolutional neural network with a dual attention module for Alzheimer's disease classification. We trained the model in the ADNI database and verified the generalizability of our method in two independent datasets (AIBL and OASIS1). Our method achieved state-of-the-art classification performance, with an accuracy of 91.94% for MCI progression classification and 96.30% for Alzheimer's disease classification on the ADNI dataset. Furthermore, the model demonstrated good generalizability, achieving an accuracy of 86.37% on the AIBL dataset and 83.42% on the OASIS1 dataset. These results indicate that our proposed approach has competitive performance and generalizability when compared to recent studies in the field.
</details>
<details>
<summary>摘要</summary>
阿尔茨海默病是一种最常见的神经退化病种，表现为amyloid-beta固革和tau卷绕。近年来，深度学习方法在阿尔茨海默病诊断中表现出了搭配性。在这项研究中，我们提议一种可重制的模型，利用3D卷积神经网络和双注意模块进行阿尔茨海默病分类。我们在ADNI数据库中训练了该模型，并在两个独立的数据集（AIBL和OASIS1）中验证了我们的方法的一致性。我们的方法在ADNI数据集上实现了状态机器的分类性能，准确率为91.94%，并在AIBL数据集和OASIS1数据集上达到了86.37%和83.42%的准确率。这些结果表明，我们提议的方法在相关领域中具有竞争力和一致性。
</details></li>
</ul>
<hr>
<h2 id="DA-TransUNet-Integrating-Spatial-and-Channel-Dual-Attention-with-Transformer-U-Net-for-Medical-Image-Segmentation"><a href="#DA-TransUNet-Integrating-Spatial-and-Channel-Dual-Attention-with-Transformer-U-Net-for-Medical-Image-Segmentation" class="headerlink" title="DA-TransUNet: Integrating Spatial and Channel Dual Attention with Transformer U-Net for Medical Image Segmentation"></a>DA-TransUNet: Integrating Spatial and Channel Dual Attention with Transformer U-Net for Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12570">http://arxiv.org/abs/2310.12570</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sun-1024/da-transunet">https://github.com/sun-1024/da-transunet</a></li>
<li>paper_authors: Guanqun Sun, Yizhi Pan, Weikun Kong, Zichang Xu, Jianhua Ma, Teeradaj Racharak, Le-Minh Nguyen, Junyi Xin<br>for:This paper proposes a novel deep medical image segmentation framework called DA-TransUNet, which aims to improve medical image segmentation performance by incorporating transformer and dual attention block into the traditional U-shaped architecture.methods:The proposed DA-TransUNet model utilizes attention mechanism of transformer and multifaceted feature extraction of DA-Block to efficiently combine global, local, and multi-scale features for medical image segmentation. Additionally, dual attention blocks are added before the Transformer layer and in skip connections to enhance feature extraction and transfer.results:Experimental results across various benchmarks of medical image segmentation reveal that DA-TransUNet significantly outperforms state-of-the-art methods, demonstrating the effectiveness of the proposed framework in improving medical image segmentation performance.<details>
<summary>Abstract</summary>
Great progress has been made in automatic medical image segmentation due to powerful deep representation learning. The influence of transformer has led to research into its variants, and large-scale replacement of traditional CNN modules. However, such trend often overlooks the intrinsic feature extraction capabilities of the transformer and potential refinements to both the model and the transformer module through minor adjustments. This study proposes a novel deep medical image segmentation framework, called DA-TransUNet, aiming to introduce the Transformer and dual attention block into the encoder and decoder of the traditional U-shaped architecture. Unlike prior transformer-based solutions, our DA-TransUNet utilizes attention mechanism of transformer and multifaceted feature extraction of DA-Block, which can efficiently combine global, local, and multi-scale features to enhance medical image segmentation. Meanwhile, experimental results show that a dual attention block is added before the Transformer layer to facilitate feature extraction in the U-net structure. Furthermore, incorporating dual attention blocks in skip connections can enhance feature transfer to the decoder, thereby improving image segmentation performance. Experimental results across various benchmark of medical image segmentation reveal that DA-TransUNet significantly outperforms the state-of-the-art methods. The codes and parameters of our model will be publicly available at https://github.com/SUN-1024/DA-TransUnet.
</details>
<details>
<summary>摘要</summary>
医学图像分割领域内，由于深度学习的强大表现， automatizd medical image segmentation 已经取得了 significan progress。 transformer 的影响导致了关于其变体和传统 CNN 模块的大规模替换的研究。然而，这些趋势通常忽略了 transformer 的内在特征提取能力和可能的模型和变体块的微调。本研究提出了一种新的深度医学图像分割框架，称为 DA-TransUNet，旨在将 transformer 和 dual attention block 引入传统 U-shaped 架构的encoder和decoder中。与先前的 transformer-based 解决方案不同，我们的 DA-TransUNet 利用 transformer 的注意机制和 DA-Block 的多元特征提取，可以有效地将全球、本地和多尺度特征相结合，以提高医学图像分割。此外，在 U-net 结构中添加 dual attention block 可以促进特征提取，从而提高图像分割性能。在不同的医学图像分割 benchmark 上，DA-TransUNet 与当前状态的方法进行比较，实验结果显示 DA-TransUNet 在医学图像分割 task 中具有显著的优势。我们将在 GitHub 上公开我们的模型和参数，访问 https://github.com/SUN-1024/DA-TransUnet。
</details></li>
</ul>
<hr>
<h2 id="Click-on-Mask-A-Labor-efficient-Annotation-Framework-with-Level-Set-for-Infrared-Small-Target-Detection"><a href="#Click-on-Mask-A-Labor-efficient-Annotation-Framework-with-Level-Set-for-Infrared-Small-Target-Detection" class="headerlink" title="Click on Mask: A Labor-efficient Annotation Framework with Level Set for Infrared Small Target Detection"></a>Click on Mask: A Labor-efficient Annotation Framework with Level Set for Infrared Small Target Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12562">http://arxiv.org/abs/2310.12562</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/li-haoqing/com">https://github.com/li-haoqing/com</a></li>
<li>paper_authors: Haoqing Li, Jinfu Yang, Yifei Xu, Runshi Wang</li>
<li>for: 这个论文主要关注的是如何提高infrared小target检测的效率和品质，并且解决小target的手动标注需求对于资料驱动方法的限制。</li>
<li>methods: 本文提出了一个劳动效率高且简洁的标注框架，使用level set方法实现高品质pseudo标注，仅需一个简单的点击。 variational level set形式中的期望差能函数设计，以维持零水平面的存在 during level set演化过程。</li>
<li>results: 实验结果显示，我们的方法在NUAA-SIRST和IRSTD-1k数据集上实现了superior表现。<details>
<summary>Abstract</summary>
Infrared Small Target Detection is a challenging task to separate small targets from infrared clutter background. Recently, deep learning paradigms have achieved promising results. However, these data-driven methods need plenty of manual annotation. Due to the small size of infrared targets, manual annotation consumes more resources and restricts the development of this field. This letter proposed a labor-efficient and cursory annotation framework with level set, which obtains a high-quality pseudo mask with only one cursory click. A variational level set formulation with an expectation difference energy functional is designed, in which the zero level contour is intrinsically maintained during the level set evolution. It solves the issue that zero level contour disappearing due to small target size and excessive regularization. Experiments on the NUAA-SIRST and IRSTD-1k datasets reveal that our approach achieves superior performance. Code is available at https://github.com/Li-Haoqing/COM.
</details>
<details>
<summary>摘要</summary>
INFRARED小target检测是一个具有挑战性的任务，既需要分离小目标从红外背景中，又需要大量的手动标注。 Recently, deep learning paradigms have achieved promising results, but these data-driven methods require a lot of manual annotation, which is time-consuming and restricts the development of this field. This letter proposes a labor-efficient and cursory annotation framework with level set, which can obtain a high-quality pseudo mask with just one cursory click.We designed a variational level set formulation with an expectation difference energy functional, which ensures that the zero level contour is intrinsically maintained during the level set evolution. This solves the issue of the zero level contour disappearing due to small target size and excessive regularization. Experiments on the NUAA-SIRST and IRSTD-1k datasets show that our approach achieves superior performance. Code is available at https://github.com/Li-Haoqing/COM.Here's the translation in Traditional Chinese as well:INFRARED小target检测是一个具有挑战性的任务，既需要分离小目标从红外背景中，又需要大量的手动标注。 Recently, deep learning paradigms have achieved promising results, but these data-driven methods require a lot of manual annotation, which is time-consuming and restricts the development of this field. This letter proposes a labor-efficient and cursory annotation framework with level set, which can obtain a high-quality pseudo mask with just one cursory click.We designed a variational level set formulation with an expectation difference energy functional, which ensures that the zero level contour is intrinsically maintained during the level set evolution. This solves the issue of the zero level contour disappearing due to small target size and excessive regularization. Experiments on the NUAA-SIRST and IRSTD-1k datasets show that our approach achieves superior performance. Code is available at https://github.com/Li-Haoqing/COM.
</details></li>
</ul>
<hr>
<h2 id="Explanation-Based-Training-with-Differentiable-Insertion-Deletion-Metric-Aware-Regularizers"><a href="#Explanation-Based-Training-with-Differentiable-Insertion-Deletion-Metric-Aware-Regularizers" class="headerlink" title="Explanation-Based Training with Differentiable Insertion&#x2F;Deletion Metric-Aware Regularizers"></a>Explanation-Based Training with Differentiable Insertion&#x2F;Deletion Metric-Aware Regularizers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12553">http://arxiv.org/abs/2310.12553</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuya Yoshikawa, Tomoharu Iwata</li>
<li>for: 提高复杂机器学习预测器的解释质量，通过插入和删除指标来评估解释的准确性。</li>
<li>methods: 提出插入&#x2F;删除指标认知优化（ID-ExpO），通过优化具有可导性的预测器，以提高插入和删除指标的解释准确性，同时保持预测精度。</li>
<li>results: 实验结果表明，通过ID-ExpO优化的深度神经网络预测器，可以使得后期解释器生成更准确和易于理解的解释，同时保持高度的预测精度。<details>
<summary>Abstract</summary>
The quality of explanations for the predictions of complex machine learning predictors is often measured using insertion and deletion metrics, which assess the faithfulness of the explanations, i.e., how correctly the explanations reflect the predictor's behavior. To improve the faithfulness, we propose insertion/deletion metric-aware explanation-based optimization (ID-ExpO), which optimizes differentiable predictors to improve both insertion and deletion scores of the explanations while keeping their predictive accuracy. Since the original insertion and deletion metrics are indifferentiable with respect to the explanations and directly unavailable for gradient-based optimization, we extend the metrics to be differentiable and use them to formalize insertion and deletion metric-based regularizers. The experimental results on image and tabular datasets show that the deep neural networks-based predictors fine-tuned using ID-ExpO enable popular post-hoc explainers to produce more faithful and easy-to-interpret explanations while keeping high predictive accuracy.
</details>
<details>
<summary>摘要</summary>
“复杂机器学习预测器的解释质量经常用插入和删除指标来衡量，以评估解释的准确性，即预测器的行为如何准确地反映在解释中。为了提高准确性，我们提议使用插入/删除指标意识的解释基于优化（ID-ExpO），该方法优化可导式预测器，以提高插入和删除指标的解释忠实度，同时保持预测精度。由于原始的插入和删除指标与解释无法导数，我们将其扩展为可导的指标，并使用它们来形式化插入和删除指标基于的补偿器。实验结果表明，使用 ID-ExpO 进行 fine-tuning 的深度神经网络预测器在图像和表格数据集上能够生成更准确和易于理解的解释，同时保持高度预测精度。”
</details></li>
</ul>
<hr>
<h2 id="PGA-Personalizing-Grasping-Agents-with-Single-Human-Robot-Interaction"><a href="#PGA-Personalizing-Grasping-Agents-with-Single-Human-Robot-Interaction" class="headerlink" title="PGA: Personalizing Grasping Agents with Single Human-Robot Interaction"></a>PGA: Personalizing Grasping Agents with Single Human-Robot Interaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12547">http://arxiv.org/abs/2310.12547</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junghyun Kim, Gi-Cheon Kang, Jaein Kim, Seoyun Yang, Minjoon Jung, Byoung-Tak Zhang</li>
<li>for: develop robots that ground and grasp objects based on natural language instructions</li>
<li>methods: learn personal objects by propagating user-given information through a Reminiscence-a collection of raw images from the user’s environment</li>
<li>results: significantly outperforms baseline methods both in offline and online settings, demonstrating its effectiveness and personalization applicability on real-world scenarios<details>
<summary>Abstract</summary>
Language-Conditioned Robotic Grasping (LCRG) aims to develop robots that ground and grasp objects based on natural language instructions. While robots capable of recognizing personal objects like "my wallet" can interact more naturally with non-expert users, current LCRG systems primarily limit robots to understanding only generic expressions. To this end, we introduce a task scenario GraspMine with a novel dataset that aims to locate and grasp personal objects given personal indicators via learning from a single human-robot interaction. To address GraspMine, we propose Personalized Grasping Agent (PGA), that learns personal objects by propagating user-given information through a Reminiscence-a collection of raw images from the user's environment. Specifically, PGA acquires personal object information by a user presenting a personal object with its associated indicator, followed by PGA inspecting the object by rotating it. Based on the acquired information, PGA pseudo-labels objects in the Reminiscence by our proposed label propagation algorithm. Harnessing the information acquired from the interactions and the pseudo-labeled objects in the Reminiscence, PGA adapts the object grounding model to grasp personal objects. Experiments on GraspMine show that PGA significantly outperforms baseline methods both in offline and online settings, signifying its effectiveness and personalization applicability on real-world scenarios. Finally, qualitative analysis shows the effectiveness of PGA through a detailed investigation of results in each phase.
</details>
<details>
<summary>摘要</summary>
language-conditioned 机器人抓取（LCRG）目的是开发基于自然语言指令的机器人，以便非专业用户与机器人进行更自然的交互。现今的LCRG系统主要限制机器人只能理解通用表达，而不是具体的个人物品。为此，我们引入一个任务场景名为GraspMine，该场景的目标是基于个人指示器找到和抓取个人物品。为解决GraspMine，我们提出了个性化抓取代理（PGA），它通过人类给出的信息来学习个人物品。具体来说，PGA通过人类展示个人物品和其相关的指示器，然后通过PGA旋转对象来获取个人物品信息。根据获取到的信息，PGA使用我们提议的标签传播算法对Reminiscence中的对象进行 pseudo-标签。利用与人类交互和 pseudo-标签对象的信息，PGA适应对象定位模型以抓取个人物品。GraspMine实验表明，PGA在线和离线 Setting中都有显著优于基eline方法，这表明它在实际场景中具有有效性和个性化应用性。最后，我们进行了详细的分析，以证明PGA的效果。
</details></li>
</ul>
<hr>
<h2 id="Weakly-Supervised-Semantic-Segmentation-with-Image-Level-Labels-from-Traditional-Models-to-Foundation-Models"><a href="#Weakly-Supervised-Semantic-Segmentation-with-Image-Level-Labels-from-Traditional-Models-to-Foundation-Models" class="headerlink" title="Weakly-Supervised Semantic Segmentation with Image-Level Labels: from Traditional Models to Foundation Models"></a>Weakly-Supervised Semantic Segmentation with Image-Level Labels: from Traditional Models to Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13026">http://arxiv.org/abs/2310.13026</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaozheng Chen, Qianru Sun</li>
<li>for: 这篇论文主要针对的是弱iously-supervised semantic segmentation（WSSS），它利用只有图像级别的标签，而不需要每个像素的标签。</li>
<li>methods: 该论文分析了多种传统方法，包括像素级、图像级、跨图像和外部数据的方法。</li>
<li>results: 研究发现，使用视觉基础模型，如Segment Anything Model（SAM），在WSSS中可以实现高效的Semantic segmentation。但是，还需要进一步的研究来解决在WSSS中 deploying visual foundational models 的挑战。<details>
<summary>Abstract</summary>
The rapid development of deep learning has driven significant progress in the field of image semantic segmentation - a fundamental task in computer vision. Semantic segmentation algorithms often depend on the availability of pixel-level labels (i.e., masks of objects), which are expensive, time-consuming, and labor-intensive. Weakly-supervised semantic segmentation (WSSS) is an effective solution to avoid such labeling. It utilizes only partial or incomplete annotations and provides a cost-effective alternative to fully-supervised semantic segmentation. In this paper, we focus on the WSSS with image-level labels, which is the most challenging form of WSSS. Our work has two parts. First, we conduct a comprehensive survey on traditional methods, primarily focusing on those presented at premier research conferences. We categorize them into four groups based on where their methods operate: pixel-wise, image-wise, cross-image, and external data. Second, we investigate the applicability of visual foundation models, such as the Segment Anything Model (SAM), in the context of WSSS. We scrutinize SAM in two intriguing scenarios: text prompting and zero-shot learning. We provide insights into the potential and challenges associated with deploying visual foundational models for WSSS, facilitating future developments in this exciting research area.
</details>
<details>
<summary>摘要</summary>
深度学习的快速发展已经 drived significiant progress in the field of image semantic segmentation - 计算机视觉中的基本任务。 semantic segmentation algorithms  often rely on the availability of pixel-level labels (i.e., object masks), which are expensive, time-consuming, and labor-intensive. Weakly-supervised semantic segmentation (WSSS) is an effective solution to avoid such labeling. It utilizes only partial or incomplete annotations and provides a cost-effective alternative to fully-supervised semantic segmentation. In this paper, we focus on the WSSS with image-level labels, which is the most challenging form of WSSS. Our work has two parts. First, we conduct a comprehensive survey on traditional methods, primarily focusing on those presented at premier research conferences. We categorize them into four groups based on where their methods operate: pixel-wise, image-wise, cross-image, and external data. Second, we investigate the applicability of visual foundation models, such as the Segment Anything Model (SAM), in the context of WSSS. We scrutinize SAM in two intriguing scenarios: text prompting and zero-shot learning. We provide insights into the potential and challenges associated with deploying visual foundational models for WSSS, facilitating future developments in this exciting research area.
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-for-Leaf-Disease-Classification-Data-Techniques-and-Applications"><a href="#Machine-Learning-for-Leaf-Disease-Classification-Data-Techniques-and-Applications" class="headerlink" title="Machine Learning for Leaf Disease Classification: Data, Techniques and Applications"></a>Machine Learning for Leaf Disease Classification: Data, Techniques and Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12509">http://arxiv.org/abs/2310.12509</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianping Yao, Son N. Tran, Samantha Sawyer, Saurabh Garg</li>
<li>for: 本研究旨在为研究者、工程师、管理者和企业家提供Machine learning技术和应用的现状报告，帮助他们更好地理解和利用Machine learning技术来推动智能农业发展。</li>
<li>methods: 本研究将从公共数据集开始，然后概述传统学习、深度学习和增强学习等常见机器学习技术，以及它们在叶病识别方面的应用。</li>
<li>results: 本研究将提供有用的资源和材料，帮助读者更好地理解和应用Machine learning技术，推动智能农业发展。<details>
<summary>Abstract</summary>
The growing demand for sustainable development brings a series of information technologies to help agriculture production. Especially, the emergence of machine learning applications, a branch of artificial intelligence, has shown multiple breakthroughs which can enhance and revolutionize plant pathology approaches. In recent years, machine learning has been adopted for leaf disease classification in both academic research and industrial applications. Therefore, it is enormously beneficial for researchers, engineers, managers, and entrepreneurs to have a comprehensive view about the recent development of machine learning technologies and applications for leaf disease detection. This study will provide a survey in different aspects of the topic including data, techniques, and applications. The paper will start with publicly available datasets. After that, we summarize common machine learning techniques, including traditional (shallow) learning, deep learning, and augmented learning. Finally, we discuss related applications. This paper would provide useful resources for future study and application of machine learning for smart agriculture in general and leaf disease classification in particular.
</details>
<details>
<summary>摘要</summary>
随着可持续发展的需求增长，农业生产方面的信息技术得到了推广应用。特别是人工智能的一个分支——机器学习，在农业生产中已经展现出多个突破，可以增强和革命化植物疾病管理方法。在最近几年中，机器学习在学术研究和实践应用中都得到了广泛的应用。因此， для研究人员、工程师、经理人和企业家来说，有一个全面的认知对现代机器学习技术和应用的发展是非常有利的。本研究将从公共可用数据开始，然后总结传统（浅学习）、深度学习和增强学习等常见机器学习技术，最后讨论相关应用。这篇论文将为未来在智能农业和植物疾病分类方面的研究和应用提供有用的资源。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-High-Resolution-3D-Generation-through-Pixel-wise-Gradient-Clipping"><a href="#Enhancing-High-Resolution-3D-Generation-through-Pixel-wise-Gradient-Clipping" class="headerlink" title="Enhancing High-Resolution 3D Generation through Pixel-wise Gradient Clipping"></a>Enhancing High-Resolution 3D Generation through Pixel-wise Gradient Clipping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12474">http://arxiv.org/abs/2310.12474</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fudan-zvg/pgc-3d">https://github.com/fudan-zvg/pgc-3d</a></li>
<li>paper_authors: Zijie Pan, Jiachen Lu, Xiatian Zhu, Li Zhang</li>
<li>for: 高分辨率3D对象生成任务仍然具有挑战性，主要原因是有限的注释化训练数据的可用性。</li>
<li>methods: 利用图像生成模型，使用知识传递技术如Score Distillation Sampling（SDS）来超越这些限制。</li>
<li>results: 提出了一种新的Pixel-wise Gradient Clipping（PGC）操作，可以快速并高效地控制梯度的大小，从而提高现有3D生成模型的渲染质量。<details>
<summary>Abstract</summary>
High-resolution 3D object generation remains a challenging task primarily due to the limited availability of comprehensive annotated training data. Recent advancements have aimed to overcome this constraint by harnessing image generative models, pretrained on extensive curated web datasets, using knowledge transfer techniques like Score Distillation Sampling (SDS). Efficiently addressing the requirements of high-resolution rendering often necessitates the adoption of latent representation-based models, such as the Latent Diffusion Model (LDM). In this framework, a significant challenge arises: To compute gradients for individual image pixels, it is necessary to backpropagate gradients from the designated latent space through the frozen components of the image model, such as the VAE encoder used within LDM. However, this gradient propagation pathway has never been optimized, remaining uncontrolled during training. We find that the unregulated gradients adversely affect the 3D model's capacity in acquiring texture-related information from the image generative model, leading to poor quality appearance synthesis. To address this overarching challenge, we propose an innovative operation termed Pixel-wise Gradient Clipping (PGC) designed for seamless integration into existing 3D generative models, thereby enhancing their synthesis quality. Specifically, we control the magnitude of stochastic gradients by clipping the pixel-wise gradients efficiently, while preserving crucial texture-related gradient directions. Despite this simplicity and minimal extra cost, extensive experiments demonstrate the efficacy of our PGC in enhancing the performance of existing 3D generative models for high-resolution object rendering.
</details>
<details>
<summary>摘要</summary>
高分辨率3D物体生成仍然是一项具有挑战性的任务，主要因为有限的可用完整标注训练数据。latest advancements 尝试使用图像生成模型，通过知识传输技术如分数投影 sampling (SDS)，超越这一限制。然而，高分辨率渲染通常需要采用秘密表示基于模型，如秘密扩散模型 (LDM)。在这个框架中，一个主要挑战是计算每个图像像素的梯度，需要在图像模型中冻结的组件，如 VAE 编码器，通过秘密空间的梯度传播来进行反射。然而，这个梯度传播路径从未被优化，在训练中保持不控制。我们发现，不控制的梯度会对3D模型获得图像生成模型中的Texture-related信息，导致低质量的外观合成。为了解决这一总体挑战，我们提出了一种创新操作，称为像素级梯度剪辑 (PGC)，用于integration into existing 3D生成模型，从而提高其合成质量。具体来说，我们控制像素级梯度的大小，通过有效地剪辑像素级梯度，保留关键的Texture-related梯度方向。尽管这种简单和附加成本较少，但经验表明，我们的PGC可以提高现有3D生成模型的高分辨率物体渲染质量。
</details></li>
</ul>
<hr>
<h2 id="RecolorCloud-A-Point-Cloud-Tool-for-Recoloring-Segmentation-and-Conversion"><a href="#RecolorCloud-A-Point-Cloud-Tool-for-Recoloring-Segmentation-and-Conversion" class="headerlink" title="RecolorCloud: A Point Cloud Tool for Recoloring, Segmentation, and Conversion"></a>RecolorCloud: A Point Cloud Tool for Recoloring, Segmentation, and Conversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12470">http://arxiv.org/abs/2310.12470</a></li>
<li>repo_url: None</li>
<li>paper_authors: Esteban Segarra Martinez, Ryan P. McMahan</li>
<li>for: 该论文目的是提供一种自动修正点云中的颜色冲击的工具，以提高大型点云的 фото实际质量。</li>
<li>methods: 该工具使用自动染色技术来解决环境干扰所导致的点云颜色错误。用户只需要指定 bounding box 区域，以影响颜色。</li>
<li>results: 实验结果显示，该工具可以大幅提高大型点云的 фото实际质量，并且用户可以快速地将点云染色到设置的semantic segmentation颜色中。<details>
<summary>Abstract</summary>
Point clouds are a 3D space representation of an environment that was recorded with a high precision laser scanner. These scanners can suffer from environmental interference such as surface shading, texturing, and reflections. Because of this, point clouds may be contaminated with fake or incorrect colors. Current open source or proprietary tools offer limited or no access to correcting these visual errors automatically.   RecolorCloud is a tool developed to resolve these color conflicts by utilizing automated color recoloring. We offer the ability to deleting or recoloring outlier points automatically with users only needing to specify bounding box regions to effect colors. Results show a vast improvement of the photo-realistic quality of large point clouds. Additionally, users can quickly recolor a point cloud with set semantic segmentation colors.
</details>
<details>
<summary>摘要</summary>
<<SYS>> tranlate text into Simplified ChinesePoint clouds are a 3D space representation of an environment that was recorded with a high precision laser scanner. These scanners can suffer from environmental interference such as surface shading, texturing, and reflections. Because of this, point clouds may be contaminated with fake or incorrect colors. Current open source or proprietary tools offer limited or no access to correcting these visual errors automatically.   RecolorCloud is a tool developed to resolve these color conflicts by utilizing automated color recoloring. We offer the ability to deleting or recoloring outlier points automatically with users only needing to specify bounding box regions to effect colors. Results show a vast improvement of the photo-realistic quality of large point clouds. Additionally, users can quickly recolor a point cloud with set semantic segmentation colors. tranlate text into Simplified Chinese点云是环境记录高精度激光扫描仪所记录的3D空间表示，这些扫描仪可能会受到环境干扰，如表面阴影、文字化和反射。因此，点云可能会受到假或错误的颜色污染。现有的开源或商业工具具有有限或无法自动 correction这些视觉错误的能力。 RecolorCloud 是一种用于解决这些颜色冲突的工具，通过自动化颜色重新染色来解决。我们提供了自动删除或重新染色异常点的能力，只需要用户指定矩形区域，就可以对颜色产生影响。结果显示大量点云的图像质量有了很大的提升。此外，用户还可以快速地使用设置的语义分割颜色重新染色点云。
</details></li>
</ul>
<hr>
<h2 id="WeedCLR-Weed-Contrastive-Learning-through-Visual-Representations-with-Class-Optimized-Loss-in-Long-Tailed-Datasets"><a href="#WeedCLR-Weed-Contrastive-Learning-through-Visual-Representations-with-Class-Optimized-Loss-in-Long-Tailed-Datasets" class="headerlink" title="WeedCLR: Weed Contrastive Learning through Visual Representations with Class-Optimized Loss in Long-Tailed Datasets"></a>WeedCLR: Weed Contrastive Learning through Visual Representations with Class-Optimized Loss in Long-Tailed Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12465">http://arxiv.org/abs/2310.12465</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alzayat Saleh, Alex Olsen, Jake Wood, Bronson Philippa, Mostafa Rahimi Azghadi</li>
<li>for: 这篇论文旨在提出一种用于长尾数据集的植物识别方法，以解决现有数据集的限制，促进深度学习模型的普及化。</li>
<li>methods: 这篇论文提出了一种基于自我超vised learning的方法，使用类型优化损失函数和涅槽 entropy 来学习rich和稠密的视觉特征，不需要labels。</li>
<li>results: 这篇论文在两个公共的植物数据集上进行了评估， compared to existing methods, WeedCLR 得到了4.3%的精度提升和5.6%的精度提升，并且在不同的环境条件下也展现了更好的一致性和稳定性。<details>
<summary>Abstract</summary>
Image classification is a crucial task in modern weed management and crop intervention technologies. However, the limited size, diversity, and balance of existing weed datasets hinder the development of deep learning models for generalizable weed identification. In addition, the expensive labelling requirements of mainstream fully-supervised weed classifiers make them cost- and time-prohibitive to deploy widely, for new weed species, and in site-specific weed management. This paper proposes a novel method for Weed Contrastive Learning through visual Representations (WeedCLR), that uses class-optimized loss with Von Neumann Entropy of deep representation for weed classification in long-tailed datasets. WeedCLR leverages self-supervised learning to learn rich and robust visual features without any labels and applies a class-optimized loss function to address the class imbalance problem in long-tailed datasets. WeedCLR is evaluated on two public weed datasets: CottonWeedID15, containing 15 weed species, and DeepWeeds, containing 8 weed species. WeedCLR achieves an average accuracy improvement of 4.3\% on CottonWeedID15 and 5.6\% on DeepWeeds over previous methods. It also demonstrates better generalization ability and robustness to different environmental conditions than existing methods without the need for expensive and time-consuming human annotations. These significant improvements make WeedCLR an effective tool for weed classification in long-tailed datasets and allows for more rapid and widespread deployment of site-specific weed management and crop intervention technologies.
</details>
<details>
<summary>摘要</summary>
现代农业中的图像分类任务是现代农业管理和作物 intervención技术的关键任务。然而，现有的异草数据集的大小、多样性和平衡受到了深度学习模型的发展带来限制。此外，主流的完全supervised weed分类器的高产生成成本和时间成本使其在新的异草种、具体的农业场景中广泛应用的成本和时间繁琐。这篇论文提出了一种novel的异草对比学习（WeedCLR）方法，通过视觉表示的类扩展损失函数进行异草分类。WeedCLR通过无监督学习学习丰富和稳定的视觉特征，不需要任何标签，并应用类扩展损失函数来解决长尾数据集中的类偏袋问题。WeedCLR在cottonweedID15和DeepWeeds两个公共异草数据集上进行评估，分别达到了4.3%和5.6%的准确率提升。它还表现出了更好的泛化能力和不同环境条件下的更好的鲁棒性，不需要贵重的人工标注。这些显著改进使WeedCLR成为了长尾数据集中的有效异草分类工具，可以更加快速地普及site-specific农业管理和作物 intervención技术。
</details></li>
</ul>
<hr>
<h2 id="Lidar-Panoptic-Segmentation-and-Tracking-without-Bells-and-Whistles"><a href="#Lidar-Panoptic-Segmentation-and-Tracking-without-Bells-and-Whistles" class="headerlink" title="Lidar Panoptic Segmentation and Tracking without Bells and Whistles"></a>Lidar Panoptic Segmentation and Tracking without Bells and Whistles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12464">http://arxiv.org/abs/2310.12464</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abhinavagarwalla/most-lps">https://github.com/abhinavagarwalla/most-lps</a></li>
<li>paper_authors: Abhinav Agarwalla, Xuhua Huang, Jason Ziglar, Francesco Ferroni, Laura Leal-Taixé, James Hays, Aljoša Ošep, Deva Ramanan</li>
<li>for: 本研究提出了一种surprisingly simple yet effective的探测中心式网络，用于实现3D&#x2F;4D lidar精准分割和跟踪任务。</li>
<li>methods: 我们的网络具有模块化设计，并且对于所有精准分割和跟踪任务进行优化。其中一个核心 ком成分是对象实例探测分支，我们使用点级（modal）注解进行训练，而在缺乏modal（cuboid）注解时，我们使用运动轨迹级别的超级视图来重点探测对象大小。</li>
<li>results: 我们在多个3D&#x2F;4D lidar精准分割和跟踪 benchmark上评估了我们的方法，并观察到我们的模型在开源模型中 establishment了新的状态时刻，超过了最近的查询基本模型。<details>
<summary>Abstract</summary>
State-of-the-art lidar panoptic segmentation (LPS) methods follow bottom-up segmentation-centric fashion wherein they build upon semantic segmentation networks by utilizing clustering to obtain object instances. In this paper, we re-think this approach and propose a surprisingly simple yet effective detection-centric network for both LPS and tracking. Our network is modular by design and optimized for all aspects of both the panoptic segmentation and tracking task. One of the core components of our network is the object instance detection branch, which we train using point-level (modal) annotations, as available in segmentation-centric datasets. In the absence of amodal (cuboid) annotations, we regress modal centroids and object extent using trajectory-level supervision that provides information about object size, which cannot be inferred from single scans due to occlusions and the sparse nature of the lidar data. We obtain fine-grained instance segments by learning to associate lidar points with detected centroids. We evaluate our method on several 3D/4D LPS benchmarks and observe that our model establishes a new state-of-the-art among open-sourced models, outperforming recent query-based models.
</details>
<details>
<summary>摘要</summary>
现代雷达精密分割方法（LPS）采用底层分割心理，从 semantic segmentation 网络开始，通过归类来获得对象实例。在这篇文章中，我们弃用这种方法，并提出一种简单却有效的探测心理网络，用于 both LPS 和跟踪。我们的网络是模块化设计的，并且对所有 LPS 和跟踪任务进行优化。我们的网络中的一个核心组件是对象实例探测支持，我们使用点级（modal）注释进行训练。在缺乏模态（cuboid）注释的情况下，我们使用轨迹级超级视图来恢复模态中心和对象扩展，并通过学习将雷达点与探测中心相关联来获得细化的实例分割。我们对多个 3D/4D LPS  benchmark 进行评估，并观察到我们的模型在开源模型中成功设置新的状态对照。我们的模型超过了最近的查询基于模型。
</details></li>
</ul>
<hr>
<h2 id="Not-Just-Learning-from-Others-but-Relying-on-Yourself-A-New-Perspective-on-Few-Shot-Segmentation-in-Remote-Sensing"><a href="#Not-Just-Learning-from-Others-but-Relying-on-Yourself-A-New-Perspective-on-Few-Shot-Segmentation-in-Remote-Sensing" class="headerlink" title="Not Just Learning from Others but Relying on Yourself: A New Perspective on Few-Shot Segmentation in Remote Sensing"></a>Not Just Learning from Others but Relying on Yourself: A New Perspective on Few-Shot Segmentation in Remote Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12452">http://arxiv.org/abs/2310.12452</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hanbobizl/dmnet">https://github.com/hanbobizl/dmnet</a></li>
<li>paper_authors: Hanbo Bi, Yingchao Feng, Zhiyuan Yan, Yongqiang Mao, Wenhui Diao, Hongqi Wang, Xian Sun</li>
<li>For: 提出了一种新的几 shot segmentation（FSS）方法，用于将未知类目标分类到几个标注样本上。* Methods: 我们提出了一种名为 dual-mining 网络（DMNet）的方法，它不再仅仅是从支持图像中学习 semantics，而是同时从查询图像中提取 semantics。我们还提出了一种减少不相关特征污染的方法，以及一种新的知识分支suppressor（KMS）模块，用于降低已知类对象的活动。* Results: 我们在 iSAID 和 LoveDA 遥感数据集上进行了广泛的实验，并证明了我们的方法可以在 1-shot 和 5-shot 设置下达到最佳性能。特别是，我们的模型（使用 Resnet-50 作为背景网络）在 iSAID 下的 mIoU 达到了 49.58% 和 51.34%，在 1-shot 和 5-shot 设置下分别高于现有的 state-of-the-art 方法 by 1.8% 和 1.12%。代码可以在 <a target="_blank" rel="noopener" href="https://github.com/HanboBizl/DMNet">https://github.com/HanboBizl/DMNet</a> 上下载。<details>
<summary>Abstract</summary>
Few-shot segmentation (FSS) is proposed to segment unknown class targets with just a few annotated samples. Most current FSS methods follow the paradigm of mining the semantics from the support images to guide the query image segmentation. However, such a pattern of `learning from others' struggles to handle the extreme intra-class variation, preventing FSS from being directly generalized to remote sensing scenes. To bridge the gap of intra-class variance, we develop a Dual-Mining network named DMNet for cross-image mining and self-mining, meaning that it no longer focuses solely on support images but pays more attention to the query image itself. Specifically, we propose a Class-public Region Mining (CPRM) module to effectively suppress irrelevant feature pollution by capturing the common semantics between the support-query image pair. The Class-specific Region Mining (CSRM) module is then proposed to continuously mine the class-specific semantics of the query image itself in a `filtering' and `purifying' manner. In addition, to prevent the co-existence of multiple classes in remote sensing scenes from exacerbating the collapse of FSS generalization, we also propose a new Known-class Meta Suppressor (KMS) module to suppress the activation of known-class objects in the sample. Extensive experiments on the iSAID and LoveDA remote sensing datasets have demonstrated that our method sets the state-of-the-art with a minimum number of model parameters. Significantly, our model with the backbone of Resnet-50 achieves the mIoU of 49.58% and 51.34% on iSAID under 1-shot and 5-shot settings, outperforming the state-of-the-art method by 1.8% and 1.12%, respectively. The code is publicly available at https://github.com/HanboBizl/DMNet.
</details>
<details>
<summary>摘要</summary>
“几shot分类（FSS）是一种用于未知类目标的分类方法，只需要几个标注的样本。现有的FSS方法都是基于从支持图像中挖掘 semantics 来导引查询图像的分类。但这种“学习他人”的模式对于类型内的差异问题不能提供直接的解决方案，因此FSS在远程感知场景中难以应用。为了bridging这个差异问题，我们开发了一个名为DMNet的双采矿网络，它不再仅仅从支持图像中挖掘 semantics，而是对查询图像本身也进行了更多的注意。”“ Specifically, we propose a Class-public Region Mining (CPRM) module to effectively suppress irrelevant feature pollution by capturing the common semantics between the support-query image pair. The Class-specific Region Mining (CSRM) module is then proposed to continuously mine the class-specific semantics of the query image itself in a `filtering' and `purifying' manner.”“ In addition, to prevent the co-existence of multiple classes in remote sensing scenes from exacerbating the collapse of FSS generalization, we also propose a new Known-class Meta Suppressor (KMS) module to suppress the activation of known-class objects in the sample.”“ Extensive experiments on the iSAID and LoveDA remote sensing datasets have demonstrated that our method sets the state-of-the-art with a minimum number of model parameters. Significantly, our model with the backbone of Resnet-50 achieves the mIoU of 49.58% and 51.34% on iSAID under 1-shot and 5-shot settings, outperforming the state-of-the-art method by 1.8% and 1.12%, respectively.”“ The code is publicly available at https://github.com/HanboBizl/DMNet.”
</details></li>
</ul>
<hr>
<h2 id="Segment-Anything-Meets-Universal-Adversarial-Perturbation"><a href="#Segment-Anything-Meets-Universal-Adversarial-Perturbation" class="headerlink" title="Segment Anything Meets Universal Adversarial Perturbation"></a>Segment Anything Meets Universal Adversarial Perturbation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12431">http://arxiv.org/abs/2310.12431</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongshen Han, Sheng Zheng, Chaoning Zhang</li>
<li>for:  investigate whether it is possible to attack SAM with image-agnostic Universal Adversarial Perturbation (UAP)</li>
<li>methods:  propose a novel perturbation-centric framework based on self-supervised contrastive learning (CL) to generate UAP</li>
<li>results:  validate the effectiveness of the proposed method with both quantitative and qualitative results, and perform ablation study to understand various components in the method.<details>
<summary>Abstract</summary>
As Segment Anything Model (SAM) becomes a popular foundation model in computer vision, its adversarial robustness has become a concern that cannot be ignored. This works investigates whether it is possible to attack SAM with image-agnostic Universal Adversarial Perturbation (UAP). In other words, we seek a single perturbation that can fool the SAM to predict invalid masks for most (if not all) images. We demonstrate convetional image-centric attack framework is effective for image-independent attacks but fails for universal adversarial attack. To this end, we propose a novel perturbation-centric framework that results in a UAP generation method based on self-supervised contrastive learning (CL), where the UAP is set to the anchor sample and the positive sample is augmented from the UAP. The representations of negative samples are obtained from the image encoder in advance and saved in a memory bank. The effectiveness of our proposed CL-based UAP generation method is validated by both quantitative and qualitative results. On top of the ablation study to understand various components in our proposed method, we shed light on the roles of positive and negative samples in making the generated UAP effective for attacking SAM.
</details>
<details>
<summary>摘要</summary>
As Segment Anything Model (SAM) becomes a popular foundation model in computer vision, its adversarial robustness has become a concern that cannot be ignored. This work investigates whether it is possible to attack SAM with image-agnostic Universal Adversarial Perturbation (UAP). In other words, we seek a single perturbation that can fool the SAM to predict invalid masks for most (if not all) images. We demonstrate that the conventional image-centric attack framework is effective for image-independent attacks but fails for universal adversarial attacks. To this end, we propose a novel perturbation-centric framework that results in a UAP generation method based on self-supervised contrastive learning (CL), where the UAP is set to the anchor sample and the positive sample is augmented from the UAP. The representations of negative samples are obtained from the image encoder in advance and saved in a memory bank. The effectiveness of our proposed CL-based UAP generation method is validated by both quantitative and qualitative results. Additionally, we perform an ablation study to understand the various components in our proposed method and shed light on the roles of positive and negative samples in making the generated UAP effective for attacking SAM.
</details></li>
</ul>
<hr>
<h2 id="LoMAE-Low-level-Vision-Masked-Autoencoders-for-Low-dose-CT-Denoising"><a href="#LoMAE-Low-level-Vision-Masked-Autoencoders-for-Low-dose-CT-Denoising" class="headerlink" title="LoMAE: Low-level Vision Masked Autoencoders for Low-dose CT Denoising"></a>LoMAE: Low-level Vision Masked Autoencoders for Low-dose CT Denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12405">http://arxiv.org/abs/2310.12405</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dayang Wang, Yongshun Xu, Shuo Han, Zhan Wu, Li Zhou, Bahareh Morovati, Hengyong Yu<br>for: 这篇论文是为了提高低剂量 computed tomography（LDCT）图像质量的方法。methods: 这篇论文使用了 transformer 模型，并且使用了 masked autoencoder（MAE）来进行自我预训。results:  experiments 结果显示，提案的 LoMAE 可以增强 transformer 的混参质化表现，并且可以大大减少依赖clean数据。它还展示了优异的韧性和应用性。<details>
<summary>Abstract</summary>
Low-dose computed tomography (LDCT) offers reduced X-ray radiation exposure but at the cost of compromised image quality, characterized by increased noise and artifacts. Recently, transformer models emerged as a promising avenue to enhance LDCT image quality. However, the success of such models relies on a large amount of paired noisy and clean images, which are often scarce in clinical settings. In the fields of computer vision and natural language processing, masked autoencoders (MAE) have been recognized as an effective label-free self-pretraining method for transformers, due to their exceptional feature representation ability. However, the original pretraining and fine-tuning design fails to work in low-level vision tasks like denoising. In response to this challenge, we redesign the classical encoder-decoder learning model and facilitate a simple yet effective low-level vision MAE, referred to as LoMAE, tailored to address the LDCT denoising problem. Moreover, we introduce an MAE-GradCAM method to shed light on the latent learning mechanisms of the MAE/LoMAE. Additionally, we explore the LoMAE's robustness and generability across a variety of noise levels. Experiments results show that the proposed LoMAE can enhance the transformer's denoising performance and greatly relieve the dependence on the ground truth clean data. It also demonstrates remarkable robustness and generalizability over a spectrum of noise levels.
</details>
<details>
<summary>摘要</summary>
低剂量 computed tomography (LDCT) 具有减少 X-ray 辐射暴露的优点，但是它会增加图像质量的噪声和artefacts。最近， transformer 模型在提高 LDCT 图像质量方面表现出了承诺。然而，这些模型的成功受到了丰富的对照图像对照集的限制，而在临床 setting 中这些对照图像通常罕见。在计算机视觉和自然语言处理领域， masked autoencoder (MAE) 被认为是一种有效的无标签预训练方法，因为它们在特征表示方面具有出色的能力。然而，原始的预训练和精度调整设计无法在低级视觉任务中进行 denoising。为回应这个挑战，我们重新设计了传统的 encoder-decoder 学习模型，并提出了一种简单 yet effective 的低级视觉 MAE，被称为 LoMAE，专门针对 LDCT denoising 问题。此外，我们还提出了 MAE-GradCAM 方法，以解释 LoMAE 在隐藏学习机制方面的学习过程。此外，我们还对 LoMAE 的Robustness和可generate性进行了多种噪声水平的测试。实验结果表明，我们的 LoMAE 可以提高 transformer 的 denoising性能，同时大幅减少了对 clean 数据的依赖。它还表现出了remarkable robustness和泛化性，适用于多种噪声水平。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Techniques-for-Video-Instance-Segmentation-A-Survey"><a href="#Deep-Learning-Techniques-for-Video-Instance-Segmentation-A-Survey" class="headerlink" title="Deep Learning Techniques for Video Instance Segmentation: A Survey"></a>Deep Learning Techniques for Video Instance Segmentation: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12393">http://arxiv.org/abs/2310.12393</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenhao Xu, Chang-Tsun Li, Yongjian Hu, Chee Peng Lim, Douglas Creighton</li>
<li>for: 这 paper 的目的是对视频实例分割问题进行深入分析和评估，并提出一些有效的方法来解决这个问题。</li>
<li>methods: 这 paper 使用了许多深度学习技术来解决视频实例分割问题，包括不同的架构设计和 auxiliary 技术。</li>
<li>results: 这 paper 对各种深度学习模型的性能、复杂度和计算负担进行了比较和分析，并提出了一些优化方法来提高视频实例分割的性能。<details>
<summary>Abstract</summary>
Video instance segmentation, also known as multi-object tracking and segmentation, is an emerging computer vision research area introduced in 2019, aiming at detecting, segmenting, and tracking instances in videos simultaneously. By tackling the video instance segmentation tasks through effective analysis and utilization of visual information in videos, a range of computer vision-enabled applications (e.g., human action recognition, medical image processing, autonomous vehicle navigation, surveillance, etc) can be implemented. As deep-learning techniques take a dominant role in various computer vision areas, a plethora of deep-learning-based video instance segmentation schemes have been proposed. This survey offers a multifaceted view of deep-learning schemes for video instance segmentation, covering various architectural paradigms, along with comparisons of functional performance, model complexity, and computational overheads. In addition to the common architectural designs, auxiliary techniques for improving the performance of deep-learning models for video instance segmentation are compiled and discussed. Finally, we discuss a range of major challenges and directions for further investigations to help advance this promising research field.
</details>
<details>
<summary>摘要</summary>
视频实例分割（也称为多对象跟踪和分割）是一个迅速发展的计算机视觉研究领域，于2019年引入，旨在同时检测、分割和跟踪视频中的实例。通过有效地分析和利用视频中的视觉信息，可以实现许多基于计算机视觉的应用程序（如人员动作识别、医疗影像处理、自动驾驶车辆导航、监控等）。随着深度学习技术在多种计算机视觉领域中的主导地位，一大批深度学习基于的视频实例分割方案已经被提出。本评论对深度学习方案的多种建筑思想进行了全面的概述，并对不同方案的功能性能、模型复杂度和计算负担进行了比较。此外，还编译了一些改进深度学习模型的 auxiliary 技巧，并对它们进行了讨论。最后，我们讨论了一些主要挑战和未来研究的方向，以帮助这个有前途的研究领域的发展。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/19/cs.CV_2023_10_19/" data-id="clpxp03zz00lpfm885z4f5rvx" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_10_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/19/cs.AI_2023_10_19/" class="article-date">
  <time datetime="2023-10-19T12:00:00.000Z" itemprop="datePublished">2023-10-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/19/cs.AI_2023_10_19/">cs.AI - 2023-10-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Unveiling-Energy-Efficiency-in-Deep-Learning-Measurement-Prediction-and-Scoring-across-Edge-Devices"><a href="#Unveiling-Energy-Efficiency-in-Deep-Learning-Measurement-Prediction-and-Scoring-across-Edge-Devices" class="headerlink" title="Unveiling Energy Efficiency in Deep Learning: Measurement, Prediction, and Scoring across Edge Devices"></a>Unveiling Energy Efficiency in Deep Learning: Measurement, Prediction, and Scoring across Edge Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18329">http://arxiv.org/abs/2310.18329</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaolong Tu, Anik Mallik, Dawei Chen, Kyungtae Han, Onur Altintas, Haoxin Wang, Jiang Xie</li>
<li>for: 这篇论文的目的是提高深度学习的能源效率和降低延误时间，并且实现在不同的边缘设备上的可持续性。</li>
<li>methods: 这篇论文使用了详细的能量测量、预测和效率评分等三种方法，以提高深度学习的能源效率和降低延误时间。</li>
<li>results: 这篇论文的研究结果包括三个大的能量数据集，以及一个基于这些数据集的预测器和效率评分指标。这些结果可以帮助提高边缘 computing 的可持续性和效率。<details>
<summary>Abstract</summary>
Today, deep learning optimization is primarily driven by research focused on achieving high inference accuracy and reducing latency. However, the energy efficiency aspect is often overlooked, possibly due to a lack of sustainability mindset in the field and the absence of a holistic energy dataset. In this paper, we conduct a threefold study, including energy measurement, prediction, and efficiency scoring, with an objective to foster transparency in power and energy consumption within deep learning across various edge devices. Firstly, we present a detailed, first-of-its-kind measurement study that uncovers the energy consumption characteristics of on-device deep learning. This study results in the creation of three extensive energy datasets for edge devices, covering a wide range of kernels, state-of-the-art DNN models, and popular AI applications. Secondly, we design and implement the first kernel-level energy predictors for edge devices based on our kernel-level energy dataset. Evaluation results demonstrate the ability of our predictors to provide consistent and accurate energy estimations on unseen DNN models. Lastly, we introduce two scoring metrics, PCS and IECS, developed to convert complex power and energy consumption data of an edge device into an easily understandable manner for edge device end-users. We hope our work can help shift the mindset of both end-users and the research community towards sustainability in edge computing, a principle that drives our research. Find data, code, and more up-to-date information at https://amai-gsu.github.io/DeepEn2023.
</details>
<details>
<summary>摘要</summary>
Firstly, we present a detailed, first-of-its-kind measurement study that uncovers the energy consumption characteristics of on-device deep learning. This study results in the creation of three extensive energy datasets for edge devices, covering a wide range of kernels, state-of-the-art DNN models, and popular AI applications.Secondly, we design and implement the first kernel-level energy predictors for edge devices based on our kernel-level energy dataset. Evaluation results demonstrate the ability of our predictors to provide consistent and accurate energy estimations on unseen DNN models.Lastly, we introduce two scoring metrics, PCS and IECS, developed to convert complex power and energy consumption data of an edge device into an easily understandable manner for edge device end-users. We hope our work can help shift the mindset of both end-users and the research community towards sustainability in edge computing, a principle that drives our research.Find data, code, and more up-to-date information at <https://amai-gsu.github.io/DeepEn2023>.
</details></li>
</ul>
<hr>
<h2 id="The-opaque-law-of-artificial-intelligence"><a href="#The-opaque-law-of-artificial-intelligence" class="headerlink" title="The opaque law of artificial intelligence"></a>The opaque law of artificial intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13192">http://arxiv.org/abs/2310.13192</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vincenzo Calderonio</li>
<li>for: 本研究旨在分析算法的透明度，在人工智能 causation 问题上的开放辩论中提供一种实验方法，以评估现有一个最佳生成 AI 模型（Chat-GPT）的性能，并研究如何通过法律规范来调控它。</li>
<li>methods: 本研究使用 Turing Test 的对话方法进行实验，以评估 Chat-GPT 模型的性能，并从法律角度探讨意识、故意和责任等意义，以更好地理解 AI 的使用问题。</li>
<li>results: 研究结果表明，Chat-GPT 模型在一些情况下可以达到高度的准确率，但也存在一些潜在的问题和风险，如模型的透明度和可控性等。此外，本研究还提出了一些可能的法律解决方案，以帮助调控 AI 的使用。<details>
<summary>Abstract</summary>
The purpose of this paper is to analyse the opacity of algorithms, contextualized in the open debate on responsibility for artificial intelligence causation; with an experimental approach by which, applying the proposed conversational methodology of the Turing Test, we expect to evaluate the performance of one of the best existing NLP model of generative AI (Chat-GPT) to see how far it can go right now and how the shape of a legal regulation of it could be. The analysis of the problem will be supported by a comment of Italian classical law categories such as causality, intent and fault to understand the problem of the usage of AI, focusing in particular on the human-machine interaction. On the computer science side, for a technical point of view of the logic used to craft these algorithms, in the second chapter will be proposed a practical interrogation of Chat-GPT aimed at finding some critical points of the functioning of AI. The end of the paper will concentrate on some existing legal solutions which can be applied to the problem, plus a brief description of the approach proposed by EU Artificial Intelligence act.
</details>
<details>
<summary>摘要</summary>
本文的目的是分析算法的透明度，在人工智能 causation 的开放辩论中上下文化了这个问题；通过应用提议的对话方法（Turing Test），我们预计可以评估现有一个最佳的生成 AI 模型（Chat-GPT）的性能，以评估它目前的可能性和如何制定相关的法律规范。对问题的分析将得到意大利古典法Category的评论，包括 causality、意图和责任，以更好地理解 AI 的使用问题，特别是人机交互方面。从计算机科学的角度来看，在第二章中将提出一种实践的问题对 Chat-GPT 的探索，以找出一些算法的关键点。总结部分将聚焦现有的法律解决方案， plus 简要描述 EU 人工智能法规。
</details></li>
</ul>
<hr>
<h2 id="Towards-Robust-Pruning-An-Adaptive-Knowledge-Retention-Pruning-Strategy-for-Language-Models"><a href="#Towards-Robust-Pruning-An-Adaptive-Knowledge-Retention-Pruning-Strategy-for-Language-Models" class="headerlink" title="Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models"></a>Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13191">http://arxiv.org/abs/2310.13191</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianwei Li, Qi Lei, Wei Cheng, Dongkuan Xu</li>
<li>for: 本研究旨在提高语音模型的Robustness，以便在实际应用中减少模型的敏感性和训练时间。</li>
<li>methods: 本研究提出了一种基于预训练知识的post-training束致法，通过保留更多的预训练知识来提高语音模型的Robustness。该方法通过层次重建错误来自动修正层次错误，以便更好地保留预训练知识。</li>
<li>results: 相比其他基于state-of-the-art的基eline，本研究的方法在SST2、IMDB和AGNews等 dataset上表现出了更好的平衡点，即Accuracy、Sparsity、Robustness和束致成本之间的trade-off。这表明了本研究的方法可以有效地提高语音模型的Robustness，同时保持Accuracy和Sparsity的良好性。<details>
<summary>Abstract</summary>
The pruning objective has recently extended beyond accuracy and sparsity to robustness in language models. Despite this, existing methods struggle to enhance robustness against adversarial attacks when continually increasing model sparsity and require a retraining process. As humans step into the era of large language models, these issues become increasingly prominent. This paper proposes that the robustness of language models is proportional to the extent of pre-trained knowledge they encompass. Accordingly, we introduce a post-training pruning strategy designed to faithfully replicate the embedding space and feature space of dense language models, aiming to conserve more pre-trained knowledge during the pruning process. In this setup, each layer's reconstruction error not only originates from itself but also includes cumulative error from preceding layers, followed by an adaptive rectification. Compared to other state-of-art baselines, our approach demonstrates a superior balance between accuracy, sparsity, robustness, and pruning cost with BERT on datasets SST2, IMDB, and AGNews, marking a significant stride towards robust pruning in language models.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "The pruning objective has recently extended beyond accuracy and sparsity to robustness in language models. Despite this, existing methods struggle to enhance robustness against adversarial attacks when continually increasing model sparsity and require a retraining process. As humans step into the era of large language models, these issues become increasingly prominent. This paper proposes that the robustness of language models is proportional to the extent of pre-trained knowledge they encompass. Accordingly, we introduce a post-training pruning strategy designed to faithfully replicate the embedding space and feature space of dense language models, aiming to conserve more pre-trained knowledge during the pruning process. In this setup, each layer's reconstruction error not only originates from itself but also includes cumulative error from preceding layers, followed by an adaptive rectification. Compared to other state-of-art baselines, our approach demonstrates a superior balance between accuracy, sparsity, robustness, and pruning cost with BERT on datasets SST2, IMDB, and AGNews, marking a significant stride towards robust pruning in language models."into Simplified Chinese:<<SYS>>大型语言模型的剪除目标已经从精度和稀疏性扩展到了可靠性，然而现有方法在不断增加模型稀疏性时很难提高对攻击性词语的抵御能力，而且需要重新训练过程。人类进入大型语言模型时代，这些问题变得越来越突出。本文提议语言模型的可靠性与它拥有的预训练知识量成正比。根据此，我们提出了一种增强语言模型可靠性的后期剪除策略，该策略可以准确复制权重空间和特征空间，以保留更多的预训练知识。在这种设置下，每层的重建错误不仅来自自己，还包括先前层次的累加错误，然后进行自适应修正。相比其他当前基elines，我们的方法在 SST2、IMDB 和 AGNews 数据集上显示出了更好的平衡性，marks a significant step towards robust pruning in language models.
</details></li>
</ul>
<hr>
<h2 id="Fast-and-Accurate-Factual-Inconsistency-Detection-Over-Long-Documents"><a href="#Fast-and-Accurate-Factual-Inconsistency-Detection-Over-Long-Documents" class="headerlink" title="Fast and Accurate Factual Inconsistency Detection Over Long Documents"></a>Fast and Accurate Factual Inconsistency Detection Over Long Documents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13189">http://arxiv.org/abs/2310.13189</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/asappresearch/scale-score">https://github.com/asappresearch/scale-score</a></li>
<li>paper_authors: Barrett Martin Lattimer, Patrick Chen, Xinyuan Zhang, Yi Yang</li>
<li>for: 本研究旨在提出一种任务不受限制的模型 для检测事实不一致，以解决现有方法对长输入的处理有限制。</li>
<li>methods: 本研究使用了一种新的分割策略，即源剥离方法（Source Chunking Approach），将长文本分割成大块来conditioning。这种方法基于自然语言推理（Natural Language Inference），并且可以在多种任务上达到状态之最的性能。</li>
<li>results: 本研究的实验结果表明，SCALE模型可以在多种任务上超越现有方法，并且在长输入情况下表现更好。此外，SCALE模型还可以快速地解释自己的决策，并且在效率和模型解释评价中也表现出优异。<details>
<summary>Abstract</summary>
Generative AI models exhibit remarkable potential; however, hallucinations across various tasks present a significant challenge, particularly for longer inputs that current approaches struggle to address effectively. We introduce SCALE (Source Chunking Approach for Large-scale inconsistency Evaluation), a task-agnostic model for detecting factual inconsistencies using a novel chunking strategy. Specifically, SCALE is a Natural Language Inference (NLI) based model that uses large text chunks to condition over long texts. This approach achieves state-of-the-art performance in factual inconsistency detection for diverse tasks and long inputs. Additionally, we leverage the chunking mechanism and employ a novel algorithm to explain SCALE's decisions through relevant source sentence retrieval. Our evaluations reveal that SCALE outperforms existing methods on both standard benchmarks and a new long-form dialogue dataset ScreenEval we constructed. Moreover, SCALE surpasses competitive systems in efficiency and model explanation evaluations. We have released our code and data publicly to GitHub.
</details>
<details>
<summary>摘要</summary>
现代生成AI模型表现出了惊人的潜力，但是幻觉在不同任务中存在一定挑战，特别是对 longer inputs 的处理。我们介绍了 SCALE（Source Chunking Approach for Large-scale inconsistency Evaluation），一种任务非依赖的模型，用于检测事实不一致性。特别是，SCALE 是一种基于自然语言推理（NLI）的模型，使用大量文本块来condition over long text。这种方法在多种任务和长输入上实现了状态的末点性表现。此外，我们利用块化机制，并使用一种新的算法来解释 SCALE 的决策，通过 relevance source sentence retrieval。我们的评估表明，SCALE 在标准 benchmark 和我们新建的长形对话 dataset ScreenEval 上表现出色，并且在效率和模型解释评估中也超过了现有的系统。我们已经在 GitHub 上公开了代码和数据。
</details></li>
</ul>
<hr>
<h2 id="CycleNet-Rethinking-Cycle-Consistency-in-Text-Guided-Diffusion-for-Image-Manipulation"><a href="#CycleNet-Rethinking-Cycle-Consistency-in-Text-Guided-Diffusion-for-Image-Manipulation" class="headerlink" title="CycleNet: Rethinking Cycle Consistency in Text-Guided Diffusion for Image Manipulation"></a>CycleNet: Rethinking Cycle Consistency in Text-Guided Diffusion for Image Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13165">http://arxiv.org/abs/2310.13165</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sled-group/cyclenet">https://github.com/sled-group/cyclenet</a></li>
<li>paper_authors: Sihan Xu, Ziqiao Ma, Yidong Huang, Honglak Lee, Joyce Chai</li>
<li>For: 本文旨在提出一种新的image-to-image翻译方法，以帮助在image synthesis任务中实现高质量的图像生成。* Methods: 本文使用了cycle consistency来regulates image manipulation，并且通过多种方法来提高image-to-image翻译的精度和质量。* Results: 实验结果表明，Cyclenet可以在不同的粒度水平上实现高质量的图像翻译，并且可以在不同的物体和场景翻译任务中提供高度的一致性。此外，本文还提供了一个多域image-to-image翻译 dataset，以便研究物体的物理状态变化。<details>
<summary>Abstract</summary>
Diffusion models (DMs) have enabled breakthroughs in image synthesis tasks but lack an intuitive interface for consistent image-to-image (I2I) translation. Various methods have been explored to address this issue, including mask-based methods, attention-based methods, and image-conditioning. However, it remains a critical challenge to enable unpaired I2I translation with pre-trained DMs while maintaining satisfying consistency. This paper introduces Cyclenet, a novel but simple method that incorporates cycle consistency into DMs to regularize image manipulation. We validate Cyclenet on unpaired I2I tasks of different granularities. Besides the scene and object level translation, we additionally contribute a multi-domain I2I translation dataset to study the physical state changes of objects. Our empirical studies show that Cyclenet is superior in translation consistency and quality, and can generate high-quality images for out-of-domain distributions with a simple change of the textual prompt. Cyclenet is a practical framework, which is robust even with very limited training data (around 2k) and requires minimal computational resources (1 GPU) to train. Project homepage: https://cyclenetweb.github.io/
</details>
<details>
<summary>摘要</summary>
Diffusion models (DMs) 已经帮助实现图像生成任务的突破，但缺乏直观的图像到图像（I2I）翻译界面。各种方法已经被探索以解决这个问题，包括面罩基的方法、注意力基的方法和图像条件。但是，尚未能够通过预训练的 DMs 实现无对应 I2I 翻译，并保持满意的一致性。这篇论文介绍 CycleNet，一种新的简单方法，将图像 manipulate 中的循环一致性 incorporated 到 DMs 中来规范化图像翻译。我们验证 CycleNet 在不同粒度的无对应 I2I 任务上。除了场景和对象层翻译，我们还贡献了多域 I2I 翻译数据集，以研究物体的物理状态变化。我们的实验表明，CycleNet 在翻译一致性和质量方面具有优势，能够生成高质量的图像，并且可以通过简单地修改文本提示来生成出去domains 的图像。CycleNet 是一个实用的框架，可以在很少的训练数据（约 2k）和少量计算资源（1 GPU）下进行训练。项目首页：https://cyclenetweb.github.io/
</details></li>
</ul>
<hr>
<h2 id="A-Distributed-Approach-to-Meteorological-Predictions-Addressing-Data-Imbalance-in-Precipitation-Prediction-Models-through-Federated-Learning-and-GANs"><a href="#A-Distributed-Approach-to-Meteorological-Predictions-Addressing-Data-Imbalance-in-Precipitation-Prediction-Models-through-Federated-Learning-and-GANs" class="headerlink" title="A Distributed Approach to Meteorological Predictions: Addressing Data Imbalance in Precipitation Prediction Models through Federated Learning and GANs"></a>A Distributed Approach to Meteorological Predictions: Addressing Data Imbalance in Precipitation Prediction Models through Federated Learning and GANs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13161">http://arxiv.org/abs/2310.13161</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elaheh Jafarigol, Theodore Trafalis</li>
<li>for: 本研究旨在为各种领域（农业、航空、灾害管理等）提供精准预测和深入分析，通过对大量多维天气数据进行分类。</li>
<li>methods: 本研究使用机器学习模型分析天气数据，并利用数据增强技术（如少数民类过多样本技术或生成对抗网络）改善模型的准确性，以便分类罕见但重要的天气事件。</li>
<li>results: 本研究表明，通过使用数据增强技术，可以提高模型的准确性，并在中央和联合数据存储和处理环境中进行分类，保持数据隐私和完整性。<details>
<summary>Abstract</summary>
The classification of weather data involves categorizing meteorological phenomena into classes, thereby facilitating nuanced analyses and precise predictions for various sectors such as agriculture, aviation, and disaster management. This involves utilizing machine learning models to analyze large, multidimensional weather datasets for patterns and trends. These datasets may include variables such as temperature, humidity, wind speed, and pressure, contributing to meteorological conditions. Furthermore, it's imperative that classification algorithms proficiently navigate challenges such as data imbalances, where certain weather events (e.g., storms or extreme temperatures) might be underrepresented. This empirical study explores data augmentation methods to address imbalanced classes in tabular weather data in centralized and federated settings. Employing data augmentation techniques such as the Synthetic Minority Over-sampling Technique or Generative Adversarial Networks can improve the model's accuracy in classifying rare but critical weather events. Moreover, with advancements in federated learning, machine learning models can be trained across decentralized databases, ensuring privacy and data integrity while mitigating the need for centralized data storage and processing. Thus, the classification of weather data stands as a critical bridge, linking raw meteorological data to actionable insights, enhancing our capacity to anticipate and prepare for diverse weather conditions.
</details>
<details>
<summary>摘要</summary>
天气数据分类涉及将气象现象分为类别，以便进行细化分析和精准预测，为各个领务如农业、航空和灾害管理等提供指导。这些分类模型可以使用机器学习算法分析大量多维天气数据寻找模式和趋势。这些数据可能包括温度、湿度、风速和压力等气象条件变量。此外，分类算法需要能够有效地处理数据不均衡问题，例如某些天气事件（如风暴或极端温度）可能被下标。本研究探讨了数据扩展技术来解决中心化和联邦化设置下的不均衡分类问题。使用数据扩展技术如Synthetic Minority Over-sampling Technique或生成对抗网络可以提高模型在分类罕见而critical天气事件的准确率。此外，随着联邦学习的发展，机器学习模型可以在分布式数据库上进行训练，保持隐私和数据完整性，同时减少中心化数据存储和处理的需求。因此，天气数据分类作为把天气数据与行动指导相连接的关键桥梁，可以提高我们对多样化天气条件的预测和准备能力。
</details></li>
</ul>
<hr>
<h2 id="Conditional-Generative-Modeling-for-Images-3D-Animations-and-Video"><a href="#Conditional-Generative-Modeling-for-Images-3D-Animations-and-Video" class="headerlink" title="Conditional Generative Modeling for Images, 3D Animations, and Video"></a>Conditional Generative Modeling for Images, 3D Animations, and Video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13157">http://arxiv.org/abs/2310.13157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vikram Voleti</li>
<li>for: 这个论文的目的是驱动计算机视觉领域的创新，探索新的条件生成模型，并将其应用于图像、3D动画和视频等领域。</li>
<li>methods: 这个研究专注于具有逆转变数的条件生成模型，以及使用encoder-decoder架构进行生成任务和3D内容修饰。我们还使用神经泛化函数来模型视频动态，并提出了一个具有条件信息的泛化流程，以提高生成过程和生成内容的效率。</li>
<li>results: 我们的研究实现了许多成果，包括使用神经泛化函数来预测未来视频几帧，以及基于低分辨率输入进行高分辨率图像生成。我们还提出了一个可以自动调整人像和3D角色的对称运动架构，并进行了几个实验来评估这些成果。<details>
<summary>Abstract</summary>
This dissertation attempts to drive innovation in the field of generative modeling for computer vision, by exploring novel formulations of conditional generative models, and innovative applications in images, 3D animations, and video. Our research focuses on architectures that offer reversible transformations of noise and visual data, and the application of encoder-decoder architectures for generative tasks and 3D content manipulation. In all instances, we incorporate conditional information to enhance the synthesis of visual data, improving the efficiency of the generation process as well as the generated content.   We introduce the use of Neural ODEs to model video dynamics using an encoder-decoder architecture, demonstrating their ability to predict future video frames despite being trained solely to reconstruct current frames. Next, we propose a conditional variant of continuous normalizing flows that enables higher-resolution image generation based on lower-resolution input, achieving comparable image quality while reducing parameters and training time. Our next contribution presents a pipeline that takes human images as input, automatically aligns a user-specified 3D character with the pose of the human, and facilitates pose editing based on partial inputs. Next, we derive the relevant mathematical details for denoising diffusion models that use non-isotropic Gaussian processes, and show comparable generation quality. Finally, we devise a novel denoising diffusion framework capable of solving all three video tasks of prediction, generation, and interpolation. We perform ablation studies, and show SOTA results on multiple datasets.   Our contributions are published articles at peer-reviewed venues. Overall, our research aims to make a meaningful contribution to the pursuit of more efficient and flexible generative models, with the potential to shape the future of computer vision.
</details>
<details>
<summary>摘要</summary>
这个论文目标在计算机视觉领域推动创新，通过探索新的条件生成模型形式和应用，提高生成过程和生成内容的效率。我们的研究关注降噪和视觉数据的可逆转换，以及使用编码器-解码器架构进行生成任务和3D内容修饰。在所有情况下，我们包含条件信息以提高生成的效果。我们引入神经 ordinary differential equations（ODEs）来模型视频动态，使用编码器-解码器架构预测未来视频帧，即使只有训练current frame。然后，我们提出一种基于Continuous Normalizing Flows的条件变体，允许基于lower-resolution输入生成高分辨率图像，实现相同的图像质量，同时减少参数和训练时间。我们的下一个贡献是一个管道，可以将人像作为输入，自动将用户指定的3D人物与人像的 pose 对齐，并且允许基于部分输入进行pose编辑。接着，我们 derive the relevant mathematical details for denoising diffusion models that use non-isotropic Gaussian processes, and show comparable generation quality。最后，我们提出一种新的杂化噪声框架，能够解决所有三个视频任务：预测、生成和 interpolate。我们进行了减少研究，并在多个数据集上达到了state-of-the-art results。总的来说，我们的研究旨在为计算机视觉领域提供更有效率和灵活的生成模型，并且具有可能在未来影响计算机视觉的潜在。
</details></li>
</ul>
<hr>
<h2 id="CLIFT-Analysing-Natural-Distribution-Shift-on-Question-Answering-Models-in-Clinical-Domain"><a href="#CLIFT-Analysing-Natural-Distribution-Shift-on-Question-Answering-Models-in-Clinical-Domain" class="headerlink" title="CLIFT: Analysing Natural Distribution Shift on Question Answering Models in Clinical Domain"></a>CLIFT: Analysing Natural Distribution Shift on Question Answering Models in Clinical Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13146">http://arxiv.org/abs/2310.13146</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openlifescience-ai/clift">https://github.com/openlifescience-ai/clift</a></li>
<li>paper_authors: Ankit Pal</li>
<li>for: 这个论文提出了一个新的临床域问答任务测试环境 CLIFT (Clinical Shift)，以提供可靠和多样化的参考基准。</li>
<li>methods: 论文使用了多种QA深度学习模型进行了全面的实验研究，并评估了这些模型在提posed的测试环境下的性能。</li>
<li>results: 论文发现，即使在原始测试集上显示出了很好的 результа，但当应用于新的测试集时，模型的性能却会下降，这显示出了分布shift的问题。<details>
<summary>Abstract</summary>
This paper introduces a new testbed CLIFT (Clinical Shift) for the clinical domain Question-answering task. The testbed includes 7.5k high-quality question answering samples to provide a diverse and reliable benchmark. We performed a comprehensive experimental study and evaluated several QA deep-learning models under the proposed testbed. Despite impressive results on the original test set, the performance degrades when applied to new test sets, which shows the distribution shift. Our findings emphasize the need for and the potential for increasing the robustness of clinical domain models under distributional shifts. The testbed offers one way to track progress in that direction. It also highlights the necessity of adopting evaluation metrics that consider robustness to natural distribution shifts. We plan to expand the corpus by adding more samples and model results. The full paper and the updated benchmark are available at github.com/openlifescience-ai/clift
</details>
<details>
<summary>摘要</summary>
这份论文介绍了一个新的 клиниче域问答任务测试环境（CLIFT）。该测试环境包含7500个高质量问答样本，以提供多样化和可靠的参考基线。我们进行了全面的实验研究，评估了多种问答深度学习模型在提posed的测试环境下的性能。尽管在原始测试集上表现出色，但当应用于新的测试集时，其性能却受到分布Shift的影响。我们的发现强调了在分布Shift下提高клиниче域模型的可靠性的需要，以及采用考虑到自然分布Shift的评价指标的重要性。该测试环境可以用于跟踪进步，并高亮了采用更多样本和模型结果来扩展 corrpus的必要性。论文全文和更新的benchmark可以在github.com/openlifescience-ai/clift找到。
</details></li>
</ul>
<hr>
<h2 id="Better-to-Ask-in-English-Cross-Lingual-Evaluation-of-Large-Language-Models-for-Healthcare-Queries"><a href="#Better-to-Ask-in-English-Cross-Lingual-Evaluation-of-Large-Language-Models-for-Healthcare-Queries" class="headerlink" title="Better to Ask in English: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries"></a>Better to Ask in English: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13132">http://arxiv.org/abs/2310.13132</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/claws-lab/XLingEval">https://github.com/claws-lab/XLingEval</a></li>
<li>paper_authors: Yiqiao Jin, Mohit Chandra, Gaurav Verma, Yibo Hu, Munmun De Choudhury, Srijan Kumar</li>
<li>For: The paper aims to investigate the effectiveness of large language models (LLMs) as multi-lingual dialogue systems for healthcare queries, and to provide a cross-lingual benchmark for evaluating their performance.* Methods: The paper uses empirically-derived framework XlingEval, which focuses on three fundamental criteria for evaluating LLM responses to naturalistic health-related questions: correctness, consistency, and verifiability. The paper also uses extensive experiments on four major global languages, including English, Spanish, Chinese, and Hindi, and an amalgamation of algorithmic and human-evaluation strategies.* Results: The paper finds a pronounced disparity in LLM responses across the four languages, indicating a need for enhanced cross-lingual capabilities. The paper proposes XlingHealth, a cross-lingual benchmark for examining the multilingual capabilities of LLMs in the healthcare context. The findings underscore the need to bolster the cross-lingual capacities of these models and provide an equitable information ecosystem accessible to all.Here is the information in Simplified Chinese text:* For: 研究大语言模型（LLM）在医疗域中的多语言对话系统的有效性。* Methods: 使用经验所得的框架XlingEval，对自然语言中的医疗问题进行评估。* Results: 发现不同语言中的LLM响应存在显著的差异，需要提升跨语言能力。提出了跨语言医疗 benchmark XlingHealth，以评估 LLM 在医疗域中的多语言能力。发现需要提高跨语言能力，以提供一个平等的信息生态系统。<details>
<summary>Abstract</summary>
Large language models (LLMs) are transforming the ways the general public accesses and consumes information. Their influence is particularly pronounced in pivotal sectors like healthcare, where lay individuals are increasingly appropriating LLMs as conversational agents for everyday queries. While LLMs demonstrate impressive language understanding and generation proficiencies, concerns regarding their safety remain paramount in these high-stake domains. Moreover, the development of LLMs is disproportionately focused on English. It remains unclear how these LLMs perform in the context of non-English languages, a gap that is critical for ensuring equity in the real-world use of these systems.This paper provides a framework to investigate the effectiveness of LLMs as multi-lingual dialogue systems for healthcare queries. Our empirically-derived framework XlingEval focuses on three fundamental criteria for evaluating LLM responses to naturalistic human-authored health-related questions: correctness, consistency, and verifiability. Through extensive experiments on four major global languages, including English, Spanish, Chinese, and Hindi, spanning three expert-annotated large health Q&A datasets, and through an amalgamation of algorithmic and human-evaluation strategies, we found a pronounced disparity in LLM responses across these languages, indicating a need for enhanced cross-lingual capabilities. We further propose XlingHealth, a cross-lingual benchmark for examining the multilingual capabilities of LLMs in the healthcare context. Our findings underscore the pressing need to bolster the cross-lingual capacities of these models, and to provide an equitable information ecosystem accessible to all.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）正在改变公众如何获取和消耗信息的方式。它们在重要的领域 like 医疗方面的影响 particualry pronounced，lay individuals increasingly appropriate LLMs as conversational agents for everyday queries. Although LLMs demonstrate impressive language understanding and generation capabilities, concerns about their safety remain paramount in these high-stakes domains. In addition, the development of LLMs is disproportionately focused on English, and it is unclear how these models perform in the context of non-English languages, which is critical for ensuring equity in the real-world use of these systems.This paper proposes a framework to investigate the effectiveness of LLMs as multi-lingual dialogue systems for healthcare queries. Our empirically derived framework XlingEval focuses on three fundamental criteria for evaluating LLM responses to naturalistic human-authored health-related questions: correctness, consistency, and verifiability. Through extensive experiments on four major global languages, including English, Spanish, Chinese, and Hindi, spanning three expert-annotated large health Q&A datasets, and through an amalgamation of algorithmic and human-evaluation strategies, we found a pronounced disparity in LLM responses across these languages, indicating a need for enhanced cross-lingual capabilities. We further propose XlingHealth, a cross-lingual benchmark for examining the multilingual capabilities of LLMs in the healthcare context. Our findings underscore the pressing need to bolster the cross-lingual capacities of these models and provide an equitable information ecosystem accessible to all.
</details></li>
</ul>
<hr>
<h2 id="Deep-Reinforcement-Learning-based-Intelligent-Traffic-Signal-Controls-with-Optimized-CO2-emissions"><a href="#Deep-Reinforcement-Learning-based-Intelligent-Traffic-Signal-Controls-with-Optimized-CO2-emissions" class="headerlink" title="Deep Reinforcement Learning-based Intelligent Traffic Signal Controls with Optimized CO2 emissions"></a>Deep Reinforcement Learning-based Intelligent Traffic Signal Controls with Optimized CO2 emissions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13129">http://arxiv.org/abs/2310.13129</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pagand/eco-light">https://github.com/pagand/eco-light</a></li>
<li>paper_authors: Pedram Agand, Alexey Iskrov, Mo Chen</li>
<li>for: 这个论文的目的是提出一种名为EcoLight的奖励形式，用于改善交通信号控制器的绩效，并最小化CO2排放。</li>
<li>methods: 该论文使用了多种RLAlgorithms，包括 tabular Q-Learning、DQN、SARSA 和 A2C，并对它们的性能进行比较。</li>
<li>results: 论文的实验结果表明，EcoLight可以不仅减少CO2排放，还能达到与传统方法相当的旅行时间和等待时间。<details>
<summary>Abstract</summary>
Nowadays, transportation networks face the challenge of sub-optimal control policies that can have adverse effects on human health, the environment, and contribute to traffic congestion. Increased levels of air pollution and extended commute times caused by traffic bottlenecks make intersection traffic signal controllers a crucial component of modern transportation infrastructure. Despite several adaptive traffic signal controllers in literature, limited research has been conducted on their comparative performance. Furthermore, despite carbon dioxide (CO2) emissions' significance as a global issue, the literature has paid limited attention to this area. In this report, we propose EcoLight, a reward shaping scheme for reinforcement learning algorithms that not only reduces CO2 emissions but also achieves competitive results in metrics such as travel time. We compare the performance of tabular Q-Learning, DQN, SARSA, and A2C algorithms using metrics such as travel time, CO2 emissions, waiting time, and stopped time. Our evaluation considers multiple scenarios that encompass a range of road users (trucks, buses, cars) with varying pollution levels.
</details>
<details>
<summary>摘要</summary>
现在，交通网络面临着优化控制策略的挑战，这可能有害于人类健康、环境和交通堵塞。交通拥堵导致的空气污染和延长的交通时间，使得路口交通信号控制器成为现代交通基础设施的关键组件。尽管有多种适应交通信号控制器的文献研究，但有限的研究把关于它们的比较性能。此外，尽管二氧化碳（CO2）排放的全球问题的重要性，文献却很少关注这一方面。在本报告中，我们提出了EcoLight，一种奖励形式学习算法的补做方案，不仅减少CO2排放，还可以在交通时间、等待时间和停止时间等指标中实现竞争性能。我们使用表格Q学习、DQN、SARSA和A2C算法进行比较，评估多种情况，包括不同的路用者（卡车、汽车、客车）以及不同的污染水平。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Addition-in-Transformers"><a href="#Understanding-Addition-in-Transformers" class="headerlink" title="Understanding Addition in Transformers"></a>Understanding Addition in Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13121">http://arxiv.org/abs/2310.13121</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philip Quirke, Fazl Barez</li>
<li>for: 这篇论文旨在对一种一层Transformer模型的内部工作机制进行深入分析，以便更好地理解这些模型的安全和道德使用。</li>
<li>methods: 论文使用了一种彻底的分析方法，揭示了模型在整数加法任务上的具体实现方式，包括将任务分解成并行的数字特定流程，并采用不同的算法来处理不同的数字位置。</li>
<li>results: 研究发现，模型在计算过程中延迟开始，但快速执行计算任务，并且在某些罕见的高损失情况下进行了详细的解释。通过严格的测试和数学模型，这些发现得到了证实。这些研究对机器学习模型的安全和道德使用做出了贡献，并为更复杂的任务和多层Transformer模型的分析开了道。<details>
<summary>Abstract</summary>
Understanding the inner workings of machine learning models like Transformers is vital for their safe and ethical use. This paper presents an in-depth analysis of a one-layer Transformer model trained for integer addition. We reveal that the model divides the task into parallel, digit-specific streams and employs distinct algorithms for different digit positions. Our study also finds that the model starts calculations late but executes them rapidly. A rare use case with high loss is identified and explained. Overall, the model's algorithm is explained in detail. These findings are validated through rigorous testing and mathematical modeling, contributing to the broader works in Mechanistic Interpretability, AI safety, and alignment. Our approach opens the door for analyzing more complex tasks and multi-layer Transformer models.
</details>
<details>
<summary>摘要</summary>
理解机器学习模型如转换器的内部工作方式是安全和道德使用的关键。本文对一个一层转换器模型进行了深入的分析，该模型用于整数加法。我们发现该模型将任务分解为并行、数字特定的流程，并采用不同的算法来处理不同的数字位置。我们的研究还发现该模型在计算开始后很快进行了执行。我们还发现了一个罕见的高损失情况，并对其进行了解释。总的来说，该模型的算法得到了详细的解释。我们的方法为机器学习安全和启发领域的更广泛研究开创了新的可能性。
</details></li>
</ul>
<hr>
<h2 id="Semi-Supervised-Learning-of-Dynamical-Systems-with-Neural-Ordinary-Differential-Equations-A-Teacher-Student-Model-Approach"><a href="#Semi-Supervised-Learning-of-Dynamical-Systems-with-Neural-Ordinary-Differential-Equations-A-Teacher-Student-Model-Approach" class="headerlink" title="Semi-Supervised Learning of Dynamical Systems with Neural Ordinary Differential Equations: A Teacher-Student Model Approach"></a>Semi-Supervised Learning of Dynamical Systems with Neural Ordinary Differential Equations: A Teacher-Student Model Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13110">http://arxiv.org/abs/2310.13110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Wang, Yuxuan Yin, Karthik Somayaji Nanjangud Suryanarayana, Jan Drgona, Malachi Schram, Mahantesh Halappanavar, Frank Liu, Peng Li</li>
<li>for: 模型动态系统，提高模型预测性和泛化能力。</li>
<li>methods: 使用 semi-supervised 方法和 Neural Ordinary Differential Equations (NODE) 模型，利用丰富的无标示数据和 teacher-student 模型来提高模型的性能。</li>
<li>results: 与基准 Neural ODE 模型比较，TS-NODE 在多个动态系统模型Task上显示出了显著的性能改进。<details>
<summary>Abstract</summary>
Modeling dynamical systems is crucial for a wide range of tasks, but it remains challenging due to complex nonlinear dynamics, limited observations, or lack of prior knowledge. Recently, data-driven approaches such as Neural Ordinary Differential Equations (NODE) have shown promising results by leveraging the expressive power of neural networks to model unknown dynamics. However, these approaches often suffer from limited labeled training data, leading to poor generalization and suboptimal predictions. On the other hand, semi-supervised algorithms can utilize abundant unlabeled data and have demonstrated good performance in classification and regression tasks. We propose TS-NODE, the first semi-supervised approach to modeling dynamical systems with NODE. TS-NODE explores cheaply generated synthetic pseudo rollouts to broaden exploration in the state space and to tackle the challenges brought by lack of ground-truth system data under a teacher-student model. TS-NODE employs an unified optimization framework that corrects the teacher model based on the student's feedback while mitigating the potential false system dynamics present in pseudo rollouts. TS-NODE demonstrates significant performance improvements over a baseline Neural ODE model on multiple dynamical system modeling tasks.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:模型动力系统是许多任务的关键，但是它们仍然具有复杂的非线性动力学、有限的观察数据和缺乏先验知识等挑战。最近，基于数据驱动的方法如神经常微方程（NODE）已经展示了许多承诺的结果，通过神经网络的表达能力来模型未知的动力学。然而，这些方法通常受限于有限的标注训练数据，导致泛化和优化结果不佳。相反，半supervised算法可以利用丰富的无标注数据，在分类和回归任务中达到良好的表现。我们提出了TS-NODE，首个半supervised的动力系统模型ing方法，它采用了免费生成的合成pseudo rollouts来扩大状态空间的探索，并在教师-学生模型下对lack of ground-truth system data bring的挑战。TS-NODE employ an unified optimization framework to correct the teacher model based on the student's feedback while mitigating the potential false system dynamics present in pseudo rollouts。TS-NODE在多个动力系统模型ing任务上示出了显著的性能提升 compared to a baseline Neural ODE model.
</details></li>
</ul>
<hr>
<h2 id="AVTENet-Audio-Visual-Transformer-based-Ensemble-Network-Exploiting-Multiple-Experts-for-Video-Deepfake-Detection"><a href="#AVTENet-Audio-Visual-Transformer-based-Ensemble-Network-Exploiting-Multiple-Experts-for-Video-Deepfake-Detection" class="headerlink" title="AVTENet: Audio-Visual Transformer-based Ensemble Network Exploiting Multiple Experts for Video Deepfake Detection"></a>AVTENet: Audio-Visual Transformer-based Ensemble Network Exploiting Multiple Experts for Video Deepfake Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13103">http://arxiv.org/abs/2310.13103</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ammarah Hashmi, Sahibzada Adil Shahzad, Chia-Wen Lin, Yu Tsao, Hsin-Min Wang</li>
<li>for: 防止深伪视频的散布，增加媒体regulation，和研究领域的新挑战。</li>
<li>methods: 基于 transformer 的 Audio-Visual Transformer-based Ensemble Network (AVTENet) 框架，包括视觉、声音和视频 modalities 的融合特征提取。</li>
<li>results: 在多Modal audio-video FakeAVCeleb 数据集上，模型达到了最佳性能，比已有方法高效。<details>
<summary>Abstract</summary>
Forged content shared widely on social media platforms is a major social problem that requires increased regulation and poses new challenges to the research community. The recent proliferation of hyper-realistic deepfake videos has drawn attention to the threat of audio and visual forgeries. Most previous work on detecting AI-generated fake videos only utilizes visual modality or audio modality. While there are some methods in the literature that exploit audio and visual modalities to detect forged videos, they have not been comprehensively evaluated on multi-modal datasets of deepfake videos involving acoustic and visual manipulations. Moreover, these existing methods are mostly based on CNN and suffer from low detection accuracy. Inspired by the recent success of Transformer in various fields, to address the challenges posed by deepfake technology, in this paper, we propose an Audio-Visual Transformer-based Ensemble Network (AVTENet) framework that considers both acoustic manipulation and visual manipulation to achieve effective video forgery detection. Specifically, the proposed model integrates several purely transformer-based variants that capture video, audio, and audio-visual salient cues to reach a consensus in prediction. For evaluation, we use the recently released benchmark multi-modal audio-video FakeAVCeleb dataset. For a detailed analysis, we evaluate AVTENet, its variants, and several existing methods on multiple test sets of the FakeAVCeleb dataset. Experimental results show that our best model outperforms all existing methods and achieves state-of-the-art performance on Testset-I and Testset-II of the FakeAVCeleb dataset.
</details>
<details>
<summary>摘要</summary>
伪造内容在社交媒体平台上广泛分享是一个重要的社会问题，需要加强管理和 pose new challenges to the research community。 recent deepfake video 的普及引起了听Visual forgery的问题的关注。 previoius work on detecting AI-generated fake videos 仅仅使用视觉modalities or audio modalities。 Although there are some methods in the literature that exploit audio and visual modalities to detect forged videos, they have not been comprehensively evaluated on multi-modal datasets of deepfake videos involving acoustic and visual manipulations。 Moreover, these existing methods are mostly based on CNN and suffer from low detection accuracy。inspired by the recent success of Transformer in various fields, to address the challenges posed by deepfake technology, in this paper, we propose an Audio-Visual Transformer-based Ensemble Network (AVTENet) framework that considers both acoustic manipulation and visual manipulation to achieve effective video forgery detection。 Specifically, the proposed model integrates several purely transformer-based variants that capture video, audio, and audio-visual salient cues to reach a consensus in prediction。 For evaluation, we use the recently released benchmark multi-modal audio-video FakeAVCeleb dataset。 For a detailed analysis, we evaluate AVTENet, its variants, and several existing methods on multiple test sets of the FakeAVCeleb dataset。 Experimental results show that our best model outperforms all existing methods and achieves state-of-the-art performance on Testset-I and Testset-II of the FakeAVCeleb dataset。
</details></li>
</ul>
<hr>
<h2 id="Particle-Guidance-non-I-I-D-Diverse-Sampling-with-Diffusion-Models"><a href="#Particle-Guidance-non-I-I-D-Diverse-Sampling-with-Diffusion-Models" class="headerlink" title="Particle Guidance: non-I.I.D. Diverse Sampling with Diffusion Models"></a>Particle Guidance: non-I.I.D. Diverse Sampling with Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13102">http://arxiv.org/abs/2310.13102</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gcorso/particle-guidance">https://github.com/gcorso/particle-guidance</a></li>
<li>paper_authors: Gabriele Corso, Yilun Xu, Valentin de Bortoli, Regina Barzilay, Tommi Jaakkola</li>
<li>for: The paper is written for researchers and practitioners interested in improving the diversity and sample efficiency of generative models, particularly in the context of image and molecular conformer generation.</li>
<li>methods: The paper proposes a new method called particle guidance, which extends diffusion-based generative sampling by enforcing diversity through a joint-particle time-evolving potential. The method is based on the idea of moving beyond the common assumption of independent samples.</li>
<li>results: The paper reports empirical results on conditional image generation and molecular conformer generation, showing that particle guidance can increase diversity without affecting quality in the former, and reduce the state-of-the-art median error by 13% on average in the latter.<details>
<summary>Abstract</summary>
In light of the widespread success of generative models, a significant amount of research has gone into speeding up their sampling time. However, generative models are often sampled multiple times to obtain a diverse set incurring a cost that is orthogonal to sampling time. We tackle the question of how to improve diversity and sample efficiency by moving beyond the common assumption of independent samples. We propose particle guidance, an extension of diffusion-based generative sampling where a joint-particle time-evolving potential enforces diversity. We analyze theoretically the joint distribution that particle guidance generates, its implications on the choice of potential, and the connections with methods in other disciplines. Empirically, we test the framework both in the setting of conditional image generation, where we are able to increase diversity without affecting quality, and molecular conformer generation, where we reduce the state-of-the-art median error by 13% on average.
</details>
<details>
<summary>摘要</summary>
据广泛成功的生成模型，有大量的研究集中在减速生成模型的采样时间。然而，生成模型通常需要多次采样以获得多样性，这会产生与采样时间无关的成本。我们考虑如何提高多样性和采样效率，我们提出了粒子指导，它是基于扩散基本生成采样的扩展，在粒子时间演化的共同可能性下保证多样性。我们 theoretically 分析了粒子指导生成的共同分布，其影响粒子可能性的选择，以及与其他领域方法的连接。empirically，我们在条件图像生成和分子 conformer 生成中测试了框架，并在后一种情况下降低了状态艺术中的平均错误率 by 13%。
</details></li>
</ul>
<hr>
<h2 id="No-offence-Bert-–-I-insult-only-humans-Multiple-addressees-sentence-level-attack-on-toxicity-detection-neural-network"><a href="#No-offence-Bert-–-I-insult-only-humans-Multiple-addressees-sentence-level-attack-on-toxicity-detection-neural-network" class="headerlink" title="No offence, Bert – I insult only humans! Multiple addressees sentence-level attack on toxicity detection neural network"></a>No offence, Bert – I insult only humans! Multiple addressees sentence-level attack on toxicity detection neural network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13099">http://arxiv.org/abs/2310.13099</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sergey Berezin, Reza Farahbakhsh, Noel Crespi</li>
<li>for: 针对黑盒恶意识别器模型的 sentence-level 攻击。</li>
<li>methods: 通过添加一些积极词或句子到仇恨消息的末端，改变神经网络的预测结果，并通过恶意识别系统的检查。</li>
<li>results: 在七种语言上（来自三种语言家族）实现了这种攻击方法，并描述了对此攻击的防御机制以及其局限性。<details>
<summary>Abstract</summary>
We introduce a simple yet efficient sentence-level attack on black-box toxicity detector models. By adding several positive words or sentences to the end of a hateful message, we are able to change the prediction of a neural network and pass the toxicity detection system check. This approach is shown to be working on seven languages from three different language families. We also describe the defence mechanism against the aforementioned attack and discuss its limitations.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种简单 yet 高效的句子级攻击方法，用于针对黑盒毒性检测器模型。我们通过在仇恨消息的末端添加一些积极的词语或句子，使得神经网络的预测结果发生变化，并通过毒性检测系统的检查。这种方法在七种语言中进行了测试，并在三个语言家族中进行了应用。我们还描述了对此攻击的防御机制，并讨论了它的局限性。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Representation-Learning-to-Aid-Semi-Supervised-Meta-Learning"><a href="#Unsupervised-Representation-Learning-to-Aid-Semi-Supervised-Meta-Learning" class="headerlink" title="Unsupervised Representation Learning to Aid Semi-Supervised Meta Learning"></a>Unsupervised Representation Learning to Aid Semi-Supervised Meta Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13085">http://arxiv.org/abs/2310.13085</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/atik666/representationtransfer">https://github.com/atik666/representationtransfer</a></li>
<li>paper_authors: Atik Faysal, Mohammad Rostami, Huaxia Wang, Avimanyu Sahoo, Ryan Antle</li>
<li>for: 解决数据稀缺问题，提高机器学习模型的泛化能力。</li>
<li>methods: 使用一shot无监督meta学习学习模型的含义表示，并在训练阶段使用扩展样本作为查询集。内循环使用温度扩大的交叉熵损失避免过拟合。</li>
<li>results: 在Omniglot和mini-Imagenet datasets上使用模型不偏 meta-learning和关系网络（RN），并实现了具有较好的准确率的模型。此外，使用提档的初始化方法可以在训练样本数量减少的情况下达到满意的准确率。<details>
<summary>Abstract</summary>
Few-shot learning or meta-learning leverages the data scarcity problem in machine learning. Traditionally, training data requires a multitude of samples and labeling for supervised learning. To address this issue, we propose a one-shot unsupervised meta-learning to learn the latent representation of the training samples. We use augmented samples as the query set during the training phase of the unsupervised meta-learning. A temperature-scaled cross-entropy loss is used in the inner loop of meta-learning to prevent overfitting during unsupervised learning. The learned parameters from this step are applied to the targeted supervised meta-learning in a transfer-learning fashion for initialization and fast adaptation with improved accuracy. The proposed method is model agnostic and can aid any meta-learning model to improve accuracy. We use model agnostic meta-learning (MAML) and relation network (RN) on Omniglot and mini-Imagenet datasets to demonstrate the performance of the proposed method. Furthermore, a meta-learning model with the proposed initialization can achieve satisfactory accuracy with significantly fewer training samples.
</details>
<details>
<summary>摘要</summary>
几shot学习或元学习可以解决机器学习中的数据缺乏问题。传统上，超vised学习需要大量的训练数据和标签。为解决这个问题，我们提出了一种一shot无监控元学习，以learn训练数据的隐藏表现。在训练阶段，我们使用了扩展的训练数据作为查询集。在元学习的内循环中，我们使用温度调整的十字熵损失，以避免过拟合。学习的参数从这一步被应用到目标的超级vised元学习中，作为初始化和快速适应的改进。我们的方法是模型不受限的，可以帮助任何元学习模型提高精度。我们使用了model不受限元学习（MAML）和关系网络（RN）在Omniglot和mini-Imagenet数据集上进行表现示例。此外，具有我们的初始化的元学习模型可以在很少的训练数据下 дости得到满意的精度。
</details></li>
</ul>
<hr>
<h2 id="From-Multilingual-Complexity-to-Emotional-Clarity-Leveraging-Commonsense-to-Unveil-Emotions-in-Code-Mixed-Dialogues"><a href="#From-Multilingual-Complexity-to-Emotional-Clarity-Leveraging-Commonsense-to-Unveil-Emotions-in-Code-Mixed-Dialogues" class="headerlink" title="From Multilingual Complexity to Emotional Clarity: Leveraging Commonsense to Unveil Emotions in Code-Mixed Dialogues"></a>From Multilingual Complexity to Emotional Clarity: Leveraging Commonsense to Unveil Emotions in Code-Mixed Dialogues</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13080">http://arxiv.org/abs/2310.13080</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lcs2-iiitd/emnlp-coffee">https://github.com/lcs2-iiitd/emnlp-coffee</a></li>
<li>paper_authors: Shivani Kumar, Ramaneswaran S, Md Shad Akhtar, Tanmoy Chakraborty</li>
<li>for: 本研究旨在提高对码混合对话中情感识别的能力。</li>
<li>methods: 该研究提出了一种新的方法，即将常识信息与对话Context相结合，以更深入地理解情感。该方法包括提取基于码混合输入的相关常识信息，并使用高级融合技术将其与对话表示相结合。</li>
<li>results: 实验表明，通过系统地 интеGRATION常识信息，ERC的性能得到了显著提高。 both量化评估和质量分析都证明了我们的假设的正确性， thereby confirming the importance of incorporating commonsense in ERC.<details>
<summary>Abstract</summary>
Understanding emotions during conversation is a fundamental aspect of human communication, driving NLP research for Emotion Recognition in Conversation (ERC). While considerable research has focused on discerning emotions of individual speakers in monolingual dialogues, understanding the emotional dynamics in code-mixed conversations has received relatively less attention. This motivates our undertaking of ERC for code-mixed conversations in this study. Recognizing that emotional intelligence encompasses a comprehension of worldly knowledge, we propose an innovative approach that integrates commonsense information with dialogue context to facilitate a deeper understanding of emotions. To achieve this, we devise an efficient pipeline that extracts relevant commonsense from existing knowledge graphs based on the code-mixed input. Subsequently, we develop an advanced fusion technique that seamlessly combines the acquired commonsense information with the dialogue representation obtained from a dedicated dialogue understanding module. Our comprehensive experimentation showcases the substantial performance improvement obtained through the systematic incorporation of commonsense in ERC. Both quantitative assessments and qualitative analyses further corroborate the validity of our hypothesis, reaffirming the pivotal role of commonsense integration in enhancing ERC.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:理解对话中的情感是人类交流的基本方面，驱动了NLP研究的情感识别在对话中（ERC）。 although considerable research has focused on distinguishing the emotions of individual speakers in monolingual dialogues, understanding the emotional dynamics in code-mixed conversations has received relatively less attention. This motivates our study of ERC for code-mixed conversations. recognizing that emotional intelligence includes a comprehension of worldly knowledge, we propose an innovative approach that integrates commonsense information with dialogue context to facilitate a deeper understanding of emotions. To achieve this, we design an efficient pipeline that extracts relevant commonsense from existing knowledge graphs based on the code-mixed input. Subsequently, we develop an advanced fusion technique that seamlessly combines the acquired commonsense information with the dialogue representation obtained from a dedicated dialogue understanding module. Our comprehensive experimentation shows the substantial performance improvement obtained through the systematic incorporation of commonsense in ERC. Both quantitative assessments and qualitative analyses further corroborate the validity of our hypothesis, reaffirming the pivotal role of commonsense integration in enhancing ERC.
</details></li>
</ul>
<hr>
<h2 id="Creative-Robot-Tool-Use-with-Large-Language-Models"><a href="#Creative-Robot-Tool-Use-with-Large-Language-Models" class="headerlink" title="Creative Robot Tool Use with Large Language Models"></a>Creative Robot Tool Use with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13065">http://arxiv.org/abs/2310.13065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengdi Xu, Peide Huang, Wenhao Yu, Shiqi Liu, Xilun Zhang, Yaru Niu, Tingnan Zhang, Fei Xia, Jie Tan, Ding Zhao<br>for: 这 paper 的目的是研究如何让机器人使用工具创新地完成复杂的任务，包括适应物理约束和长期规划。methods: 这 paper 使用 Large Language Models (LLMs) 开发了一个系统，可以根据自然语言指令控制机器人在实验室和实际环境中进行操作。该系统包括四个重要组件：分析器、规划器、计算器和编译器。results: 这 paper 的结果表明，RoboTool 可以不仅理解任务中的物理约束和环境因素，还能示出创新的工具使用。与传统的 Task and Motion Planning (TAMP) 方法不同，我们的 LLM-based 系统提供了更灵活、高效和用户友好的解决方案，可以扩展机器人系统的能力。经过广泛的实验，我们证明了 RoboTool 能够处理不可能 без创新工具使用的任务，从而扩大机器人系统的可能性。<details>
<summary>Abstract</summary>
Tool use is a hallmark of advanced intelligence, exemplified in both animal behavior and robotic capabilities. This paper investigates the feasibility of imbuing robots with the ability to creatively use tools in tasks that involve implicit physical constraints and long-term planning. Leveraging Large Language Models (LLMs), we develop RoboTool, a system that accepts natural language instructions and outputs executable code for controlling robots in both simulated and real-world environments. RoboTool incorporates four pivotal components: (i) an "Analyzer" that interprets natural language to discern key task-related concepts, (ii) a "Planner" that generates comprehensive strategies based on the language input and key concepts, (iii) a "Calculator" that computes parameters for each skill, and (iv) a "Coder" that translates these plans into executable Python code. Our results show that RoboTool can not only comprehend explicit or implicit physical constraints and environmental factors but also demonstrate creative tool use. Unlike traditional Task and Motion Planning (TAMP) methods that rely on explicit optimization, our LLM-based system offers a more flexible, efficient, and user-friendly solution for complex robotics tasks. Through extensive experiments, we validate that RoboTool is proficient in handling tasks that would otherwise be infeasible without the creative use of tools, thereby expanding the capabilities of robotic systems. Demos are available on our project page: https://creative-robotool.github.io/.
</details>
<details>
<summary>摘要</summary>
tool 使用是高级智能的标志，在动物行为和机器人能力中都有代表性。这篇论文探讨了让机器人具备创新使用工具的能力，以实现在含有隐式物理约束和长期规划的任务中的高级智能。利用大型自然语言模型（LLM），我们开发了RoboTool系统，可以接受自然语言指令并生成控制机器人的可执行代码。RoboTool包括四个重要组件：（一）一个“分析器”，可以从自然语言中解读关键任务相关的概念；（二）一个“规划器”，可以根据自然语言输入和关键概念生成全面的策略；（三）一个“计算器”，可以计算每种技能的参数；以及（四）一个“编译器”，可以将这些规划转换为可执行的Python代码。我们的结果表明，RoboTool不仅可以理解隐式或明确的物理约束和环境因素，而且可以展示创新的工具使用。与传统的任务和动作规划（TAMP）方法不同，我们的LLM基于系统提供了更灵活、高效和用户友好的解决方案，用于复杂的机器人任务。通过广泛的实验，我们证明了RoboTool可以处理不可能 без创新工具使用的任务，从而扩展机器人系统的能力。demo可以在我们项目页面上找到：https://creative-robotool.github.io/.
</details></li>
</ul>
<hr>
<h2 id="Training-Dynamics-of-Deep-Network-Linear-Regions"><a href="#Training-Dynamics-of-Deep-Network-Linear-Regions" class="headerlink" title="Training Dynamics of Deep Network Linear Regions"></a>Training Dynamics of Deep Network Linear Regions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12977">http://arxiv.org/abs/2310.12977</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Imtiaz Humayun, Randall Balestriero, Richard Baraniuk</li>
<li>for: 这个研究主要关注深度网络（DN）训练过程中的输入空间分区或线性区域的形成。</li>
<li>methods: 这个研究使用了细分Affine DNs（例如，带有（漏斗）ReLU非线性的网络），并使用了一种新的统计量来描述深度网络的本地复杂性（LC）。</li>
<li>results: 研究发现，在训练过程中，LC在数据点周围的区域经历了多个阶段，包括一个减少趋势，然后是一个升高阶段，最终是一个最后的减少趋势。此外，研究还发现，在最后一个LC下降阶段的训练过程中，线性区域从训练和测试样本偏移向决策边界，使得深度网络的输入输出变得几乎线性Everywhere else。此外，研究还发现，不同的LC阶段与深度网络的记忆和泛化性强相关，特别是在“搁浅”阶段。<details>
<summary>Abstract</summary>
The study of Deep Network (DN) training dynamics has largely focused on the evolution of the loss function, evaluated on or around train and test set data points. In fact, many DN phenomenon were first introduced in literature with that respect, e.g., double descent, grokking. In this study, we look at the training dynamics of the input space partition or linear regions formed by continuous piecewise affine DNs, e.g., networks with (leaky)ReLU nonlinearities. First, we present a novel statistic that encompasses the local complexity (LC) of the DN based on the concentration of linear regions inside arbitrary dimensional neighborhoods around data points. We observe that during training, the LC around data points undergoes a number of phases, starting with a decreasing trend after initialization, followed by an ascent and ending with a final descending trend. Using exact visualization methods, we come across the perplexing observation that during the final LC descent phase of training, linear regions migrate away from training and test samples towards the decision boundary, making the DN input-output nearly linear everywhere else. We also observe that the different LC phases are closely related to the memorization and generalization performance of the DN, especially during grokking.
</details>
<details>
<summary>摘要</summary>
研究深度网络（DN）训练动态的研究主要集中在损失函数的演化，即在训练集和测试集数据点附近。实际上，许多DN现象在文献中首次出现，例如双峰Descender。在这种研究中，我们关注了连续划分 affine DN 的输入空间分区或线性区域的训练动态。例如，使用（漏斗）ReLU非线性。我们首先介绍了一种新的统计，用于量化DN本地复杂性（LC），基于数据点附近多维度邻域中线性区域的吸引度。我们发现，在训练过程中，LC附近数据点经历了一些阶段，包括初始阶段下降趋势，然后升附向阶段，最终结束于下降趋势。使用精确视觉化方法，我们发现在最后一个LC下降阶段的训练中，线性区域在训练和测试样本之间迁移，使DN的输入-输出变得nearly linear everywhere else。我们还发现不同的LC阶段与DN的记忆和泛化性的表现有着密切的关系，特别是在搜寻过程中。
</details></li>
</ul>
<hr>
<h2 id="Variational-Inference-for-SDEs-Driven-by-Fractional-Noise"><a href="#Variational-Inference-for-SDEs-Driven-by-Fractional-Noise" class="headerlink" title="Variational Inference for SDEs Driven by Fractional Noise"></a>Variational Inference for SDEs Driven by Fractional Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12975">http://arxiv.org/abs/2310.12975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rembert Daems, Manfred Opper, Guillaume Crevecoeur, Tolga Birdal</li>
<li>for: 这个论文是为了提出一种基于变量框架的概率方法，用于解决随机 diffeomorphism 方程（SDE）驱动的Markov-approximate fractional Brownian motion（fBM）的推理问题。</li>
<li>methods: 这篇论文使用了变量方法，并基于Markov approxiamtion of fBM， derivation of evidence lower bound，以及使用神经网络来学习涨搅、扩散和控制项的变量 posterior。</li>
<li>results: 论文提出了一种基于变量框架的概率方法，可以快速和有效地解决SDE驱动的fBM推理问题，并且在synthetic data上验证了这种方法的有效性。此外，论文还提出了一种基于变量 neural-SDE的视频预测方法，这是varational neural-SDE的首次应用于视频识别领域。<details>
<summary>Abstract</summary>
We present a novel variational framework for performing inference in (neural) stochastic differential equations (SDEs) driven by Markov-approximate fractional Brownian motion (fBM). SDEs offer a versatile tool for modeling real-world continuous-time dynamic systems with inherent noise and randomness. Combining SDEs with the powerful inference capabilities of variational methods, enables the learning of representative function distributions through stochastic gradient descent. However, conventional SDEs typically assume the underlying noise to follow a Brownian motion (BM), which hinders their ability to capture long-term dependencies. In contrast, fractional Brownian motion (fBM) extends BM to encompass non-Markovian dynamics, but existing methods for inferring fBM parameters are either computationally demanding or statistically inefficient. In this paper, building upon the Markov approximation of fBM, we derive the evidence lower bound essential for efficient variational inference of posterior path measures, drawing from the well-established field of stochastic analysis. Additionally, we provide a closed-form expression to determine optimal approximation coefficients. Furthermore, we propose the use of neural networks to learn the drift, diffusion and control terms within our variational posterior, leading to the variational training of neural-SDEs. In this framework, we also optimize the Hurst index, governing the nature of our fractional noise. Beyond validation on synthetic data, we contribute a novel architecture for variational latent video prediction,-an approach that, to the best of our knowledge, enables the first variational neural-SDE application to video perception.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的变量框架，用于在神经网络随机差分方程（SDE）中进行推理。SDE 提供了一种灵活的工具，用于模型化真实世界中的连续时间动态系统，具有内在的噪音和随机性。将 SDE 与变量方法相结合，可以通过随机梯度下降来学习表达函数分布。然而，传统的 SDE 通常假设下面的噪音随机过程是布朗运动（BM），这限制了它们的能力 capture 长期关系。相比之下，分布式扩展运动（fBM）扩展了 BM，以捕捉非Markovian 动态，但现有的 fBM 参数推断方法是计算昂贵或 statistically 不fficient。在这篇论文中，基于 Markov approxiamtion 的 fBM，我们 deriv 出证明 Lower bound 必要的 Variational 推断 posterior path measures，从 Stochastic 分析领域中得到了灵感。此外，我们还提供了一个关闭式表达式，用于确定最佳拟合系数。此外，我们建议使用神经网络来学习涨栅、滤波和控制项的变ational posterior，从而实现变量训练神经-SDE。在这个框架中，我们还优化了哈斯特指数，它控制了我们的分布式噪音的性质。在此之外，我们还提出了一种新的变量架构，用于变量潜在视频预测，这是一种具有 Variational 神经-SDE 应用的首个途径。
</details></li>
</ul>
<hr>
<h2 id="Robust-multimodal-models-have-outlier-features-and-encode-more-concepts"><a href="#Robust-multimodal-models-have-outlier-features-and-encode-more-concepts" class="headerlink" title="Robust multimodal models have outlier features and encode more concepts"></a>Robust multimodal models have outlier features and encode more concepts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13040">http://arxiv.org/abs/2310.13040</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan Crabbé, Pau Rodríguez, Vaishaal Shankar, Luca Zappella, Arno Blaas</li>
<li>for: 这个论文的目的是研究robust模型与非robust模型之间的区别，以及这些区别是如何影响模型的学习和泛化性。</li>
<li>methods: 该论文使用了12种不同的多模态模型（包括ResNet和ViT）和不同的预训练集（如OpenAI、LAION-400M、LAION-2B、YFCC15M、CC12M和DataComp）进行 probing，以探索robust模型的表征空间中的两种特征。</li>
<li>results: 该论文发现robust模型在表征空间中存在两种特征：1）Robust模型具有异常特征，其活动值可以达到数个数量级以上，这些异常特征导致模型的预测力量。2）Robust模型在表征空间中储存了更多的概念，这使得模型可以更好地泛化，但同时也使得模型的解释变得更加困难。<details>
<summary>Abstract</summary>
What distinguishes robust models from non-robust ones? This question has gained traction with the appearance of large-scale multimodal models, such as CLIP. These models have demonstrated unprecedented robustness with respect to natural distribution shifts. While it has been shown that such differences in robustness can be traced back to differences in training data, so far it is not known what that translates to in terms of what the model has learned. In this work, we bridge this gap by probing the representation spaces of 12 robust multimodal models with various backbones (ResNets and ViTs) and pretraining sets (OpenAI, LAION-400M, LAION-2B, YFCC15M, CC12M and DataComp). We find two signatures of robustness in the representation spaces of these models: (1) Robust models exhibit outlier features characterized by their activations, with some being several orders of magnitude above average. These outlier features induce privileged directions in the model's representation space. We demonstrate that these privileged directions explain most of the predictive power of the model by pruning up to $80 \%$ of the least important representation space directions without negative impacts on model accuracy and robustness; (2) Robust models encode substantially more concepts in their representation space. While this superposition of concepts allows robust models to store much information, it also results in highly polysemantic features, which makes their interpretation challenging. We discuss how these insights pave the way for future research in various fields, such as model pruning and mechanistic interpretability.
</details>
<details>
<summary>摘要</summary>
Our findings reveal two distinct signatures of robustness in the representation spaces of these models:1. Robust models exhibit outlier features with significantly higher activations, sometimes orders of magnitude above the average. These outlier features create privileged directions in the model's representation space. We show that these privileged directions account for most of the model's predictive power, as pruning up to 80% of the least important representation space directions does not negatively impact model accuracy or robustness.2. Robust models encode a greater number of concepts in their representation space, leading to a more polysemantic feature space. While this allows robust models to store more information, it also makes their interpretation challenging.These insights open up new avenues for future research, such as model pruning and mechanistic interpretability. By understanding what makes a model robust, we can develop more effective and efficient AI systems that are better equipped to handle the complexities of real-world data.
</details></li>
</ul>
<hr>
<h2 id="Frozen-Transformers-in-Language-Models-Are-Effective-Visual-Encoder-Layers"><a href="#Frozen-Transformers-in-Language-Models-Are-Effective-Visual-Encoder-Layers" class="headerlink" title="Frozen Transformers in Language Models Are Effective Visual Encoder Layers"></a>Frozen Transformers in Language Models Are Effective Visual Encoder Layers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12973">http://arxiv.org/abs/2310.12973</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ziqipang/lm4visualencoding">https://github.com/ziqipang/lm4visualencoding</a></li>
<li>paper_authors: Ziqi Pang, Ziyang Xie, Yunze Man, Yu-Xiong Wang</li>
<li>for: 这篇论文探讨了大型自然语言模型（LLM）在没有语言的情况下如何用于纯视觉任务。</li>
<li>methods: 这篇论文提出了一种新的、前所未见的策略：将预训练的LLM块作为纯视觉任务中的元编码层使用。</li>
<li>results: 该策略可以在多种任务上提高性能，包括2D和3D图像识别、动作识别、非语义任务（如运动预测）和多模态任务（如2D&#x2F;3D视问答和图像文本检索）。这些改进是对不同类型的LLM和不同LLM块的一致性的。此外，论文还提出了一种叫做信息筛选假设的解释，即预训练LLM块可以挖掘有用的视觉标记并加强其效果。<details>
<summary>Abstract</summary>
This paper reveals that large language models (LLMs), despite being trained solely on textual data, are surprisingly strong encoders for purely visual tasks in the absence of language. Even more intriguingly, this can be achieved by a simple yet previously overlooked strategy -- employing a frozen transformer block from pre-trained LLMs as a constituent encoder layer to directly process visual tokens. Our work pushes the boundaries of leveraging LLMs for computer vision tasks, significantly departing from conventional practices that typically necessitate a multi-modal vision-language setup with associated language prompts, inputs, or outputs. We demonstrate that our approach consistently enhances performance across a diverse range of tasks, encompassing pure 2D and 3D visual recognition tasks (e.g., image and point cloud classification), temporal modeling tasks (e.g., action recognition), non-semantic tasks (e.g., motion forecasting), and multi-modal tasks (e.g., 2D/3D visual question answering and image-text retrieval). Such improvements are a general phenomenon, applicable to various types of LLMs (e.g., LLaMA and OPT) and different LLM transformer blocks. We additionally propose the information filtering hypothesis to explain the effectiveness of pre-trained LLMs in visual encoding -- the pre-trained LLM transformer blocks discern informative visual tokens and further amplify their effect. This hypothesis is empirically supported by the observation that the feature activation, after training with LLM transformer blocks, exhibits a stronger focus on relevant regions. We hope that our work inspires new perspectives on utilizing LLMs and deepening our understanding of their underlying mechanisms. Code is available at https://github.com/ziqipang/LM4VisualEncoding.
</details>
<details>
<summary>摘要</summary>
这篇论文发现，大型语言模型（LLM），即使只接受文本数据培训，可以 surprisngly strong的编码器 для纯视觉任务。甚至更有趣的是，这可以通过一种简单 yet previously overlooked 的策略实现——使用预先做好的 transformer 块作为纯视觉任务中的编码器层直接处理视觉符号。我们的工作推动了利用 LLM 进行计算机视觉任务的推广，明显 Departing from conventional practices that typically require a multi-modal vision-language setup with associated language prompts, inputs, or outputs. We demonstrate that our approach consistently enhances performance across a diverse range of tasks, including pure 2D and 3D visual recognition tasks (e.g., image and point cloud classification), temporal modeling tasks (e.g., action recognition), non-semantic tasks (e.g., motion forecasting), and multi-modal tasks (e.g., 2D/3D visual question answering and image-text retrieval). Such improvements are a general phenomenon, applicable to various types of LLMs (e.g., LLaMA and OPT) and different LLM transformer blocks. We additionally propose the information filtering hypothesis to explain the effectiveness of pre-trained LLMs in visual encoding—the pre-trained LLM transformer blocks discern informative visual tokens and further amplify their effect. This hypothesis is empirically supported by the observation that the feature activation, after training with LLM transformer blocks, exhibits a stronger focus on relevant regions. We hope that our work inspires new perspectives on utilizing LLMs and deepening our understanding of their underlying mechanisms. 代码可以在 https://github.com/ziqipang/LM4VisualEncoding 上找到。
</details></li>
</ul>
<hr>
<h2 id="CLAIR-Evaluating-Image-Captions-with-Large-Language-Models"><a href="#CLAIR-Evaluating-Image-Captions-with-Large-Language-Models" class="headerlink" title="CLAIR: Evaluating Image Captions with Large Language Models"></a>CLAIR: Evaluating Image Captions with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12971">http://arxiv.org/abs/2310.12971</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zeniSoida/pl1">https://github.com/zeniSoida/pl1</a></li>
<li>paper_authors: David Chan, Suzanne Petryk, Joseph E. Gonzalez, Trevor Darrell, John Canny</li>
<li>for: 评估机器生成的图像标签，提出了一种新的评估方法CLAIR。</li>
<li>methods: CLAIR利用大型语言模型（LLMs）的零shot语言模型能力来评估候选标签。</li>
<li>results: CLAIR与人类评估标签质量的协调相对于现有方法提高39.6%，并且可以提供噪音可读的结果，让语言模型可以找到其分配的分数的根据。<details>
<summary>Abstract</summary>
The evaluation of machine-generated image captions poses an interesting yet persistent challenge. Effective evaluation measures must consider numerous dimensions of similarity, including semantic relevance, visual structure, object interactions, caption diversity, and specificity. Existing highly-engineered measures attempt to capture specific aspects, but fall short in providing a holistic score that aligns closely with human judgments. Here, we propose CLAIR, a novel method that leverages the zero-shot language modeling capabilities of large language models (LLMs) to evaluate candidate captions. In our evaluations, CLAIR demonstrates a stronger correlation with human judgments of caption quality compared to existing measures. Notably, on Flickr8K-Expert, CLAIR achieves relative correlation improvements over SPICE of 39.6% and over image-augmented methods such as RefCLIP-S of 18.3%. Moreover, CLAIR provides noisily interpretable results by allowing the language model to identify the underlying reasoning behind its assigned score. Code is available at https://davidmchan.github.io/clair/
</details>
<details>
<summary>摘要</summary>
评估机器生成的图像标签 pose 一个有趣又普遍存在挑战。有效的评估方法应该考虑多个相似性维度，包括semantic relevance、视觉结构、对象互动、标签多样性和特定性。现有的高度工程化度量尝试捕捉特定方面，但都不够提供一个整体分数，与人类评估相吻合。在这里，我们提出了CLAIR，一种新的方法，利用大型语言模型（LLM）的零容量语言模型能力来评估候选标签。在我们的评估中，CLAIR表现出较强的人类评估标签质量相关性，比 existed 度量方法更高。尤其是在Flickr8K-Expert上，CLAIR相比SPICE的39.6%和图像扩展方法如RefCLIP-S的18.3%，实现了更高的相关性提升。此外，CLAIR提供了噪音可读的结果，让语言模型可以确定其分配的分数的下层逻辑。代码可以在https://davidmchan.github.io/clair/ 获取。
</details></li>
</ul>
<hr>
<h2 id="Does-Your-Model-Think-Like-an-Engineer-Explainable-AI-for-Bearing-Fault-Detection-with-Deep-Learning"><a href="#Does-Your-Model-Think-Like-an-Engineer-Explainable-AI-for-Bearing-Fault-Detection-with-Deep-Learning" class="headerlink" title="Does Your Model Think Like an Engineer? Explainable AI for Bearing Fault Detection with Deep Learning"></a>Does Your Model Think Like an Engineer? Explainable AI for Bearing Fault Detection with Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12967">http://arxiv.org/abs/2310.12967</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Decker, Michael Lebacher, Volker Tresp<br>for: 这paper是为了解释深度学习模型在承载轮盘附件的磨损检测 task 中的决策过程。methods: 这paper使用了特有的特征归属分析框架，以评估深度学习模型的逻辑是否符合专家的理解。results: 这paper的研究结果表明，通过使用这种特征归属分析框架，可以评估深度学习模型的可靠性和扩展性，并预测不同的深度学习模型在这个任务中的表现。<details>
<summary>Abstract</summary>
Deep Learning has already been successfully applied to analyze industrial sensor data in a variety of relevant use cases. However, the opaque nature of many well-performing methods poses a major obstacle for real-world deployment. Explainable AI (XAI) and especially feature attribution techniques promise to enable insights about how such models form their decision. But the plain application of such methods often fails to provide truly informative and problem-specific insights to domain experts. In this work, we focus on the specific task of detecting faults in rolling element bearings from vibration signals. We propose a novel and domain-specific feature attribution framework that allows us to evaluate how well the underlying logic of a model corresponds with expert reasoning. Utilizing the framework we are able to validate the trustworthiness and to successfully anticipate the generalization ability of different well-performing deep learning models. Our methodology demonstrates how signal processing tools can effectively be used to enhance Explainable AI techniques and acts as a template for similar problems.
</details>
<details>
<summary>摘要</summary>
深度学习已经成功应用于工业传感器数据分析多个相关的实际应用场景。然而，许多高性能方法的含义性具有大量的难以理解性，妨碍了实际应用。可解释AI（XAI）和特征归因技术承诺可以提供决策过程中模型的含义。然而，简单地应用这些方法并不能提供具体的域专家需要的理解。在这种情况下，我们将注意力集中在检测滚动元件 bearings 的振荡信号中的缺陷。我们提出了一种域专家特有的特征归因框架，可以评估模型下的逻辑与域专家的推理是否匹配。通过这种框架，我们能够验证模型的可靠性和扩展性。我们的方法体现了如何使用信号处理工具来增强可解释AI技术，并为类似问题提供模板。
</details></li>
</ul>
<hr>
<h2 id="AutoMix-Automatically-Mixing-Language-Models"><a href="#AutoMix-Automatically-Mixing-Language-Models" class="headerlink" title="AutoMix: Automatically Mixing Language Models"></a>AutoMix: Automatically Mixing Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12963">http://arxiv.org/abs/2310.12963</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/automix-llm/automix">https://github.com/automix-llm/automix</a></li>
<li>paper_authors: Aman Madaan, Pranjal Aggarwal, Ankit Anand, Srividya Pranavi Potharaju, Swaroop Mishra, Pei Zhou, Aditya Gupta, Dheeraj Rajagopal, Karthik Kappaganthu, Yiming Yang, Shyam Upadhyay, Mausam, Manaal Faruqui</li>
<li>for: 本研究旨在优化大型自然语言处理器（LLM）的计算成本和性能，通过灵活地路由查询到更大的LLM。</li>
<li>methods: 本文提出了一种策略性路由方法，基于一个较小的LLM的输出的相对正确性来选择路由到更大的LLM。该方法包括一种几次检验机制，以及一个元验证器来精度地评估检验结果。</li>
<li>results: 根据实验结果，AutoMix方法在五个上下文推理数据集上超过了现有的基准值，提高了每次成本的增量效果，最高达89%。<details>
<summary>Abstract</summary>
Large language models (LLMs) are now available in various sizes and configurations from cloud API providers. While this diversity offers a broad spectrum of choices, effectively leveraging the options to optimize computational cost and performance remains challenging. In this work, we present AutoMix, an approach that strategically routes queries to larger LMs, based on the approximate correctness of outputs from a smaller LM. Central to AutoMix is a few-shot self-verification mechanism, which estimates the reliability of its own outputs without requiring training. Given that verifications can be noisy, we employ a meta verifier in AutoMix to refine the accuracy of these assessments. Our experiments using LLAMA2-13/70B, on five context-grounded reasoning datasets demonstrate that AutoMix surpasses established baselines, improving the incremental benefit per cost by up to 89%. Our code and data are available at https://github.com/automix-llm/automix.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）现在从云API提供商处可以获得多种大小和配置。这种多样性提供了广泛的选择，但是有效地利用这些选择以便最大化computational cost和性能仍然是挑战。在这个工作中，我们介绍AutoMix，一种策略性路由查询到更大的LM，基于更小的LM的输出的近似正确性。AutoMix的核心是几 shot自我验证机制，可以无需训练来估计自己的出力的可靠性。由于验证可能会是噪音的，我们在AutoMix中使用meta验证器来精确化这些评价。我们的实验使用LLAMA2-13/70B在五个上下文推理 dataset上显示，AutoMix超过了已知的基准点，提高了增量的价值随着成本的增加量最多达89%。我们的代码和数据可以在https://github.com/automix-llm/automix上获得。
</details></li>
</ul>
<hr>
<h2 id="An-Emulator-for-Fine-Tuning-Large-Language-Models-using-Small-Language-Models"><a href="#An-Emulator-for-Fine-Tuning-Large-Language-Models-using-Small-Language-Models" class="headerlink" title="An Emulator for Fine-Tuning Large Language Models using Small Language Models"></a>An Emulator for Fine-Tuning Large Language Models using Small Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12962">http://arxiv.org/abs/2310.12962</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eric Mitchell, Rafael Rafailov, Archit Sharma, Chelsea Finn, Christopher D. Manning<br>for:* The paper aims to decouple the knowledge and skills gained during pre-training and fine-tuning in language models, and to investigate the effect of scaling up these stages on the performance of the models.methods:* The authors propose a novel technique called emulated fine-tuning (EFT) to sample from a distribution that approximates the result of pre-training and fine-tuning at different scales.* EFT is based on an RL-based framework derived from recent developments in learning from human preferences.results:* The authors show that scaling up fine-tuning tends to improve helpfulness, while scaling up pre-training tends to improve factuality.* They also demonstrate that EFT enables test-time adjustment of competing behavioral traits like helpfulness and harmlessness without additional training.* Finally, the authors show that LM up-scaling, which involves ensembling small fine-tuned models with large pre-trained models, consistently improves the helpfulness and factuality of instruction-following models in several families of models.<details>
<summary>Abstract</summary>
Widely used language models (LMs) are typically built by scaling up a two-stage training pipeline: a pre-training stage that uses a very large, diverse dataset of text and a fine-tuning (sometimes, 'alignment') stage that uses targeted examples or other specifications of desired behaviors. While it has been hypothesized that knowledge and skills come from pre-training, and fine-tuning mostly filters this knowledge and skillset, this intuition has not been extensively tested. To aid in doing so, we introduce a novel technique for decoupling the knowledge and skills gained in these two stages, enabling a direct answer to the question, "What would happen if we combined the knowledge learned by a large model during pre-training with the knowledge learned by a small model during fine-tuning (or vice versa)?" Using an RL-based framework derived from recent developments in learning from human preferences, we introduce emulated fine-tuning (EFT), a principled and practical method for sampling from a distribution that approximates (or 'emulates') the result of pre-training and fine-tuning at different scales. Our experiments with EFT show that scaling up fine-tuning tends to improve helpfulness, while scaling up pre-training tends to improve factuality. Beyond decoupling scale, we show that EFT enables test-time adjustment of competing behavioral traits like helpfulness and harmlessness without additional training. Finally, a special case of emulated fine-tuning, which we call LM up-scaling, avoids resource-intensive fine-tuning of large pre-trained models by ensembling them with small fine-tuned models, essentially emulating the result of fine-tuning the large pre-trained model. Up-scaling consistently improves helpfulness and factuality of instruction-following models in the Llama, Llama-2, and Falcon families, without additional hyperparameters or training.
</details>
<details>
<summary>摘要</summary>
通用的语言模型（LM）通常由扩大一个两个阶段训练管道建立：一个预训练阶段使用非常大、多样化的文本数据，以及一个精度调整（或准确性调整）阶段使用目标示例或特定的行为要求。尽管曾经 hypothesized 知识和技能来自于预训练，而精度调整主要过滤这些知识和技能集，但这种假设未经过广泛测试。为了解决这问题，我们介绍了一种新的技术，可以分离这两个阶段中学习的知识和技能，从而解决问题，“如果将大型模型的预训练知识与小型模型的精度调整结果相结合（或相反），会发生什么？”使用基于最近的人类偏好学习的RL基础，我们引入了一种名为 Emulated Fine-Tuning（EFT）的原则正确且实用的方法，可以采样一个 approximates （或 '模拟'）预训练和精度调整的结果。我们的实验表明，在不同的缩放比例下，精度调整会提高帮助fulness，而预训练会提高准确性。此外，EFT 还可以在测试时调整竞争性 trait 如帮助fulness 和无害性，无需额外训练。最后，我们还介绍了一种特殊的 Emulated Fine-Tuning 情况，即 LM up-scaling，可以避免高资源占用的精度调整大型预训练模型，通过 ensemble 小型精度调整模型，效果相当于精度调整大型预训练模型。 up-scaling 一致地提高了 instruction-following 模型在 Llama、Llama-2 和 Falcon 家族中的帮助fulness 和准确性，无需额外参数或训练。
</details></li>
</ul>
<hr>
<h2 id="Eureka-Moments-in-Transformers-Multi-Step-Tasks-Reveal-Softmax-Induced-Optimization-Problems"><a href="#Eureka-Moments-in-Transformers-Multi-Step-Tasks-Reveal-Softmax-Induced-Optimization-Problems" class="headerlink" title="Eureka-Moments in Transformers: Multi-Step Tasks Reveal Softmax Induced Optimization Problems"></a>Eureka-Moments in Transformers: Multi-Step Tasks Reveal Softmax Induced Optimization Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12956">http://arxiv.org/abs/2310.12956</a></li>
<li>repo_url: None</li>
<li>paper_authors: David T. Hoffmann, Simon Schrodi, Nadine Behrmann, Volker Fischer, Thomas Brox</li>
<li>for: 本研究探讨了transformer在多步决策任务中快速增进损失的现象。</li>
<li>methods: 我们使用了transformer和CNN来研究多步决策任务，并发现transformer在这些任务上遇到了困难，而CNN则没有这个问题。</li>
<li>results: 当transformer学习中间任务时，它们会快速并意外地提高性能，我们称之为“Eureka-oment”。这种快速改进会使模型在训练过程中更快地 дости到最佳性能，并且更容易学习中间任务，从而提高最终准确率和 robustness。<details>
<summary>Abstract</summary>
In this work, we study rapid, step-wise improvements of the loss in transformers when being confronted with multi-step decision tasks. We found that transformers struggle to learn the intermediate tasks, whereas CNNs have no such issue on the tasks we studied. When transformers learn the intermediate task, they do this rapidly and unexpectedly after both training and validation loss saturated for hundreds of epochs. We call these rapid improvements Eureka-moments, since the transformer appears to suddenly learn a previously incomprehensible task. Similar leaps in performance have become known as Grokking. In contrast to Grokking, for Eureka-moments, both the validation and the training loss saturate before rapidly improving. We trace the problem back to the Softmax function in the self-attention block of transformers and show ways to alleviate the problem. These fixes improve training speed. The improved models reach 95% of the baseline model in just 20% of training steps while having a much higher likelihood to learn the intermediate task, lead to higher final accuracy and are more robust to hyper-parameters.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们研究了transformer在多步决策任务中快速、步进改进损失的现象。我们发现transformer在学习中缺乏intermediate task的能力，而CNN则没有这种问题。当transformer学习intermediate task时，它们会在百余个epoch后， unexpectedly和rapidly提高性能。我们称这些快速改进为“Eureka-moment”，因为transformer似乎在学习一个前不可理解的任务时，突然有了新的理解。相比Grokking，Eureka-moment中，两个loss函数都会先于改进而饱和。我们跟踪问题的起源，并提出了修复方案，这些修复方案可以提高训练速度，并使模型更加快速地学习intermediate task，最终性能高于基线模型，并且更加鲁棒于hyperparameter。
</details></li>
</ul>
<hr>
<h2 id="Towards-Robust-Offline-Reinforcement-Learning-under-Diverse-Data-Corruption"><a href="#Towards-Robust-Offline-Reinforcement-Learning-under-Diverse-Data-Corruption" class="headerlink" title="Towards Robust Offline Reinforcement Learning under Diverse Data Corruption"></a>Towards Robust Offline Reinforcement Learning under Diverse Data Corruption</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12955">http://arxiv.org/abs/2310.12955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Yang, Han Zhong, Jiawei Xu, Amy Zhang, Chongjie Zhang, Lei Han, Tong Zhang<br>for:* 这个论文旨在研究在线上不需要实际交互的情况下学习强化策略的可行性。methods:* 该论文使用了现有的OFFLINE强化学习算法，并进行了全面的数据损害测试，以评估不同算法对数据损害的影响。results:* 研究发现，使用IQL算法可以具有很好的抗性能，并且通过理论和实验分析，发现IQL的超级vised策略学习方案是关键因素。* 不过，IQL仍然面临着动力损害下Q函数的重 tailed Target问题，为了解决这个问题，该论文提出了一种使用惩罚函数来处理重 tailed Target的方法。* 通过将这些简单 yet effective的修改加入IQL算法，该论文提出了一种更加Robust的OFFLINE强化学习方法，称为Robust IQL（RIQL）。* 广泛的实验表明，RIQL在不同的数据损害情况下具有极高的Robust性能。<details>
<summary>Abstract</summary>
Offline reinforcement learning (RL) presents a promising approach for learning reinforced policies from offline datasets without the need for costly or unsafe interactions with the environment. However, datasets collected by humans in real-world environments are often noisy and may even be maliciously corrupted, which can significantly degrade the performance of offline RL. In this work, we first investigate the performance of current offline RL algorithms under comprehensive data corruption, including states, actions, rewards, and dynamics. Our extensive experiments reveal that implicit Q-learning (IQL) demonstrates remarkable resilience to data corruption among various offline RL algorithms. Furthermore, we conduct both empirical and theoretical analyses to understand IQL's robust performance, identifying its supervised policy learning scheme as the key factor. Despite its relative robustness, IQL still suffers from heavy-tail targets of Q functions under dynamics corruption. To tackle this challenge, we draw inspiration from robust statistics to employ the Huber loss to handle the heavy-tailedness and utilize quantile estimators to balance penalization for corrupted data and learning stability. By incorporating these simple yet effective modifications into IQL, we propose a more robust offline RL approach named Robust IQL (RIQL). Extensive experiments demonstrate that RIQL exhibits highly robust performance when subjected to diverse data corruption scenarios.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified Chinese:Offline reinforcement learning (RL) presents a promising approach for learning reinforced policies from offline datasets without the need for costly or unsafe interactions with the environment. However, datasets collected by humans in real-world environments are often noisy and may even be maliciously corrupted, which can significantly degrade the performance of offline RL. In this work, we first investigate the performance of current offline RL algorithms under comprehensive data corruption, including states, actions, rewards, and dynamics. Our extensive experiments reveal that implicit Q-learning (IQL) demonstrates remarkable resilience to data corruption among various offline RL algorithms. Furthermore, we conduct both empirical and theoretical analyses to understand IQL's robust performance, identifying its supervised policy learning scheme as the key factor. Despite its relative robustness, IQL still suffers from heavy-tail targets of Q functions under dynamics corruption. To tackle this challenge, we draw inspiration from robust statistics to employ the Huber loss to handle the heavy-tailedness and utilize quantile estimators to balance penalization for corrupted data and learning stability. By incorporating these simple yet effective modifications into IQL, we propose a more robust offline RL approach named Robust IQL (RIQL). Extensive experiments demonstrate that RIQL exhibits highly robust performance when subjected to diverse data corruption scenarios.离线 reinforcement learning (RL) 提供了一个有前途的方法，通过从离线数据集中学习奖励策略，而不需要costly或 unsafe 的环境交互。然而，由人类在实际环境收集的数据集经常受到噪音和可能是黑客损害，这可能会很大程度地下降离线 RL 的性能。在这项工作中，我们首先investigate 当前离线 RL 算法在全面数据损害情况下的性能，包括状态、动作、奖励和动力学。我们的广泛实验表明，隐式 Q 学习 (IQL) 在多种离线 RL 算法中表现出了抗损害的特点。此外，我们还进行了empirical 和理论分析，以了解 IQL 的Robust性能，并确定其超visited 策略学习方案为关键因素。尽管 IQL 相对强健，但是它仍然受到动力学损害的 heavy-tailed 目标问题。为解决这个挑战，我们 draw inspiration 从robust statistics 中，采用 Huber 损失函数来处理 heavy-tailedness，并使用量iles estimators来平衡损害数据和学习稳定性。通过将这些简单 yet effective 修改 incorporated into IQL，我们提出了一种更 Robust 的离线 RL 方法，名为 Robust IQL (RIQL)。广泛实验表明，RIQL 在多种数据损害场景下具有高度 Robust 性能。
</details></li>
</ul>
<hr>
<h2 id="Structured-Generation-and-Exploration-of-Design-Space-with-Large-Language-Models-for-Human-AI-Co-Creation"><a href="#Structured-Generation-and-Exploration-of-Design-Space-with-Large-Language-Models-for-Human-AI-Co-Creation" class="headerlink" title="Structured Generation and Exploration of Design Space with Large Language Models for Human-AI Co-Creation"></a>Structured Generation and Exploration of Design Space with Large Language Models for Human-AI Co-Creation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12953">http://arxiv.org/abs/2310.12953</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sangho Suh, Meng Chen, Bryan Min, Toby Jia-Jun Li, Haijun Xia</li>
<li>for: 这个研究是为了探讨如何将大语言模型（LLMs）转化为创作工具，以探索它们的创作潜力。</li>
<li>methods: 研究人员使用了一个框架，帮助用户透过结构化的生成方式，让他们可以轻松地探索、评估和 sinthez 许多创作想法。</li>
<li>results: 研究发现，这个框架可以帮助用户寻找更多的创作灵感，并且可以帮助他们快速创造出更多的创作作品。<details>
<summary>Abstract</summary>
Thanks to their generative capabilities, large language models (LLMs) have become an invaluable tool for creative processes. These models have the capacity to produce hundreds and thousands of visual and textual outputs, offering abundant inspiration for creative endeavors. But are we harnessing their full potential? We argue that current interaction paradigms fall short, guiding users towards rapid convergence on a limited set of ideas, rather than empowering them to explore the vast latent design space in generative models. To address this limitation, we propose a framework that facilitates the structured generation of design space in which users can seamlessly explore, evaluate, and synthesize a multitude of responses. We demonstrate the feasibility and usefulness of this framework through the design and development of an interactive system, Luminate, and a user study with 8 professional writers. Our work advances how we interact with LLMs for creative tasks, introducing a way to harness the creative potential of LLMs.
</details>
<details>
<summary>摘要</summary>
Large language models (LLMs) 的生成能力使其成为了创作过程中的不可或缺工具。这些模型可以生成数百或数千个视觉和文本输出，提供了大量的创作灵感。但是，我们现在是否正确地利用其潜在力？我们认为，当前的交互方式有限制，导引用户快速 converges 到一小组IDEAS，而不是让用户在生成模型的庞大latent design space中自由探索。为解决这一限制，我们提出了一个框架，可以让用户轻松探索、评估和synthesize 多种响应。我们通过设计和开发一个交互系统Luminate，以及与8名专业作家的用户研究，证明了这种框架的可行性和实用性。我们的工作推动了如何在创作任务中与LLMs交互，推出了一种可以挖掘LLMs的创作潜力的方法。
</details></li>
</ul>
<hr>
<h2 id="The-Foundation-Model-Transparency-Index"><a href="#The-Foundation-Model-Transparency-Index" class="headerlink" title="The Foundation Model Transparency Index"></a>The Foundation Model Transparency Index</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12941">http://arxiv.org/abs/2310.12941</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stanford-crfm/fmti">https://github.com/stanford-crfm/fmti</a></li>
<li>paper_authors: Rishi Bommasani, Kevin Klyman, Shayne Longpre, Sayash Kapoor, Nestor Maslej, Betty Xiong, Daniel Zhang, Percy Liang</li>
<li>for: The paper aims to assess the transparency of foundation model developers and encourage better governance of the foundation model ecosystem.</li>
<li>methods: The authors use a comprehensive set of 100 indicators to evaluate the transparency of 10 major foundation model developers, including information about the upstream resources used to build the models, details about the models themselves, and downstream use.</li>
<li>results: The authors find that there is a lack of transparency in the foundation model ecosystem, with no developer disclosing significant information about the downstream impact of their flagship models, such as the number of users, affected market sectors, or how users can seek redress for harm. The findings provide a baseline for evaluating progress on transparency and governance in the future.In Simplified Chinese, the three key points would be:</li>
<li>for: 论文目的是评估基础模型开发者的透明度，并促进基础模型生态系统的治理。</li>
<li>methods: 作者使用100个细化指标评估10个主要基础模型开发者的透明度，包括模型建构所用的上游资源、模型自身的详细信息以及下游使用。</li>
<li>results: 作者发现基础模型生态系统中的透明度很低，没有任何开发者公布其旗舰模型的下游影响的重要信息，如用户数量、受影响的市场部门以及如何寻求伤害赔偿等。<details>
<summary>Abstract</summary>
Foundation models have rapidly permeated society, catalyzing a wave of generative AI applications spanning enterprise and consumer-facing contexts. While the societal impact of foundation models is growing, transparency is on the decline, mirroring the opacity that has plagued past digital technologies (e.g. social media). Reversing this trend is essential: transparency is a vital precondition for public accountability, scientific innovation, and effective governance. To assess the transparency of the foundation model ecosystem and help improve transparency over time, we introduce the Foundation Model Transparency Index. The Foundation Model Transparency Index specifies 100 fine-grained indicators that comprehensively codify transparency for foundation models, spanning the upstream resources used to build a foundation model (e.g data, labor, compute), details about the model itself (e.g. size, capabilities, risks), and the downstream use (e.g. distribution channels, usage policies, affected geographies). We score 10 major foundation model developers (e.g. OpenAI, Google, Meta) against the 100 indicators to assess their transparency. To facilitate and standardize assessment, we score developers in relation to their practices for their flagship foundation model (e.g. GPT-4 for OpenAI, PaLM 2 for Google, Llama 2 for Meta). We present 10 top-level findings about the foundation model ecosystem: for example, no developer currently discloses significant information about the downstream impact of its flagship model, such as the number of users, affected market sectors, or how users can seek redress for harm. Overall, the Foundation Model Transparency Index establishes the level of transparency today to drive progress on foundation model governance via industry standards and regulatory intervention.
</details>
<details>
<summary>摘要</summary>
基础模型在社会中迅速普及，推动了一波基于企业和消费者面向的生成AI应用程序。然而，社会影响力的透明度在下降，与过去的数字技术（如社交媒体）一样，透明度的下降需要逆转。为确保公共负责任、科学创新和有效管理，我们引入基础模型透明度指数。基础模型透明度指数包括100个细化的指标，全面捕捉基础模型的透明度，包括建模时使用的上游资源（如数据、劳动、计算）、模型的详细信息（如大小、功能、风险）和下游使用（如分布渠道、使用政策、受影地域）。我们对10个主要基础模型开发者（如OpenAI、Google、Meta）进行评分，以评估他们的透明度。为了便于和标准化评估，我们对开发者的做法是基于他们的旗舰模型（如GPT-4、PaLM 2、Llama 2）进行评分。我们发现了10个关键的基础模型生态系统发现，例如，目前没有开发者公布其旗舰模型下游的重要信息，如用户数量、受影市场部门和用户如何申诉害。总之，基础模型透明度指数为今天的透明度水平，以便通过行业标准和法规干预加以进步。
</details></li>
</ul>
<hr>
<h2 id="Eureka-Human-Level-Reward-Design-via-Coding-Large-Language-Models"><a href="#Eureka-Human-Level-Reward-Design-via-Coding-Large-Language-Models" class="headerlink" title="Eureka: Human-Level Reward Design via Coding Large Language Models"></a>Eureka: Human-Level Reward Design via Coding Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12931">http://arxiv.org/abs/2310.12931</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/eureka-research/Eureka">https://github.com/eureka-research/Eureka</a></li>
<li>paper_authors: Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, Anima Anandkumar</li>
<li>For: The paper aims to develop a human-level reward design algorithm powered by large language models (LLMs) to acquire complex skills via reinforcement learning.* Methods: The proposed algorithm, called Eureka, leverages the zero-shot generation, code-writing, and in-context improvement capabilities of state-of-the-art LLMs to perform evolutionary optimization over reward code.* Results: Eureka outperforms human experts on 83% of the tasks in a diverse suite of 29 open-source RL environments, leading to an average normalized improvement of 52%. Additionally, the algorithm is able to learn complex skills such as pen spinning tricks using a simulated Shadow Hand.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have excelled as high-level semantic planners for sequential decision-making tasks. However, harnessing them to learn complex low-level manipulation tasks, such as dexterous pen spinning, remains an open problem. We bridge this fundamental gap and present Eureka, a human-level reward design algorithm powered by LLMs. Eureka exploits the remarkable zero-shot generation, code-writing, and in-context improvement capabilities of state-of-the-art LLMs, such as GPT-4, to perform evolutionary optimization over reward code. The resulting rewards can then be used to acquire complex skills via reinforcement learning. Without any task-specific prompting or pre-defined reward templates, Eureka generates reward functions that outperform expert human-engineered rewards. In a diverse suite of 29 open-source RL environments that include 10 distinct robot morphologies, Eureka outperforms human experts on 83% of the tasks, leading to an average normalized improvement of 52%. The generality of Eureka also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF), readily incorporating human inputs to improve the quality and the safety of the generated rewards without model updating. Finally, using Eureka rewards in a curriculum learning setting, we demonstrate for the first time, a simulated Shadow Hand capable of performing pen spinning tricks, adeptly manipulating a pen in circles at rapid speed.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在序列决策任务中表现出色，但将其用于学习复杂的低级机械操作任务，如灵活的笔旋转，仍然是一个未解决的问题。我们在这个问题上提出了解论，并提出了一种基于LLM的人类水平奖励设计算法，称为Eureka。Eureka利用现代LLM的零容量生成、代码写作和在场进程提高功能，通过对奖励代码进行演化优化，以获得人类水平的技能。无需任务特定的提示或预定的奖励模板，Eureka可以生成高性能的奖励函数，在29个开源RL环境中，包括10种不同的机器人形态，在83%的任务上超过人类专家，平均减少了52%。Eureka的通用性还允许一种 gradient-free 在场学习方法，可以从人类反馈中提取改进奖励函数的质量和安全性，无需模型更新。最后，使用Eureka奖励在CURRICULUM学习中，我们在 simulations 中首次实现了一个名为Shadow Hand的笔旋转技巧，快速旋转笔，灵活地控制笔的运动。
</details></li>
</ul>
<hr>
<h2 id="Habitat-3-0-A-Co-Habitat-for-Humans-Avatars-and-Robots"><a href="#Habitat-3-0-A-Co-Habitat-for-Humans-Avatars-and-Robots" class="headerlink" title="Habitat 3.0: A Co-Habitat for Humans, Avatars and Robots"></a>Habitat 3.0: A Co-Habitat for Humans, Avatars and Robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13724">http://arxiv.org/abs/2310.13724</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xavier Puig, Eric Undersander, Andrew Szot, Mikael Dallaire Cote, Tsung-Yen Yang, Ruslan Partsey, Ruta Desai, Alexander William Clegg, Michal Hlavac, So Yeon Min, Vladimír Vondruš, Theophile Gervet, Vincent-Pierre Berges, John M. Turner, Oleksandr Maksymets, Zsolt Kira, Mrinal Kalakrishnan, Jitendra Malik, Devendra Singh Chaplot, Unnat Jain, Dhruv Batra, Akshara Rai, Roozbeh Mottaghi</li>
<li>for: 研究人类和机器人之间的协作任务在家庭环境中的模拟平台。</li>
<li>methods: 使用高速的人iform模拟，支持人类和机器人之间的真实交互，以及研究社交导航和社交重新排序两种协作任务。</li>
<li>results: 实现了基于学习的机器人策略的效果，并与人类合作完成任务，以及观察到机器人在协作任务执行过程中的自然行为。<details>
<summary>Abstract</summary>
We present Habitat 3.0: a simulation platform for studying collaborative human-robot tasks in home environments. Habitat 3.0 offers contributions across three dimensions: (1) Accurate humanoid simulation: addressing challenges in modeling complex deformable bodies and diversity in appearance and motion, all while ensuring high simulation speed. (2) Human-in-the-loop infrastructure: enabling real human interaction with simulated robots via mouse/keyboard or a VR interface, facilitating evaluation of robot policies with human input. (3) Collaborative tasks: studying two collaborative tasks, Social Navigation and Social Rearrangement. Social Navigation investigates a robot's ability to locate and follow humanoid avatars in unseen environments, whereas Social Rearrangement addresses collaboration between a humanoid and robot while rearranging a scene. These contributions allow us to study end-to-end learned and heuristic baselines for human-robot collaboration in-depth, as well as evaluate them with humans in the loop. Our experiments demonstrate that learned robot policies lead to efficient task completion when collaborating with unseen humanoid agents and human partners that might exhibit behaviors that the robot has not seen before. Additionally, we observe emergent behaviors during collaborative task execution, such as the robot yielding space when obstructing a humanoid agent, thereby allowing the effective completion of the task by the humanoid agent. Furthermore, our experiments using the human-in-the-loop tool demonstrate that our automated evaluation with humanoids can provide an indication of the relative ordering of different policies when evaluated with real human collaborators. Habitat 3.0 unlocks interesting new features in simulators for Embodied AI, and we hope it paves the way for a new frontier of embodied human-AI interaction capabilities.
</details>
<details>
<summary>摘要</summary>
我们介绍Habitat 3.0：一个人工智能 simulate human-robot collaborative tasks 的平台。Habitat 3.0在三个维度上提供贡献：1. 准确的人形模拟：解决复杂的材质和形态的模拟挑战，同时保证高速的模拟速度。2. 人类在Loop基础设施：允许人类与模拟机器人进行实际的互动，通过鼠标/键盘或VR界面进行评估机器人策略。3. 合作任务：研究社交导航和社交重新排序两种合作任务。社交导航探索机器人如何在未看过环境中找到和跟随人形模拟器，而社交重新排序则研究机器人和人形模拟器之间的合作，重新排序场景。这些贡献使我们可以深入研究人机器人合作的综合学习和规则基础，并通过人类在Loop评估机器人策略的效果。我们的实验表明，学习机器人策略在与未看过人形模拟器和人类合作时能够高效完成任务，并且在合作过程中出现了规则行为，如机器人让出空间以便人形模拟器完成任务。此外，我们使用人类在Loop工具进行自动评估，发现我们的人机器人合作策略在与真实的人类合作者评估时可以提供相对排名的指示。Habitat 3.0开启了人工智能 simulate 的新领域，我们希望它能够推动人机器人交互能力的新前ier。
</details></li>
</ul>
<hr>
<h2 id="Digital-Twin-Enabled-Intelligent-DDoS-Detection-Mechanism-for-Autonomous-Core-Networks"><a href="#Digital-Twin-Enabled-Intelligent-DDoS-Detection-Mechanism-for-Autonomous-Core-Networks" class="headerlink" title="Digital Twin-Enabled Intelligent DDoS Detection Mechanism for Autonomous Core Networks"></a>Digital Twin-Enabled Intelligent DDoS Detection Mechanism for Autonomous Core Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12924">http://arxiv.org/abs/2310.12924</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yagmur Yigit, Bahadir Bal, Aytac Karameseoglu, Trung Q. Duong, Berk Canberk</li>
<li>for: 本研究旨在提供一种适用于互联网服务提供商核心网络的分布式拒绝服务攻击检测机制，以解决现有的检测技术无法处理高度聚合数据流的问题。</li>
<li>methods: 本文提出了一种基于数字双子的智能拒绝服务检测机制，使用在线学习方法来自动更新模型，提高检测精度和效率。文章还提出了一种基于YANG模型和自动特征选择模块来处理核心网络数据。</li>
<li>results: Results show that the proposed solution can successfully detect DDoS attacks and update the feature selection method and learning model with a true classification rate of ninety-seven percent. The proposed solution can estimate the attack within approximately fifteen minutes after the DDoS attack starts.<details>
<summary>Abstract</summary>
Existing distributed denial of service attack (DDoS) solutions cannot handle highly aggregated data rates; thus, they are unsuitable for Internet service provider (ISP) core networks. This article proposes a digital twin-enabled intelligent DDoS detection mechanism using an online learning method for autonomous systems. Our contributions are three-fold: we first design a DDoS detection architecture based on the digital twin for ISP core networks. We implemented a Yet Another Next Generation (YANG) model and an automated feature selection (AutoFS) module to handle core network data. We used an online learning approach to update the model instantly and efficiently, improve the learning model quickly, and ensure accurate predictions. Finally, we reveal that our proposed solution successfully detects DDoS attacks and updates the feature selection method and learning model with a true classification rate of ninety-seven percent. Our proposed solution can estimate the attack within approximately fifteen minutes after the DDoS attack starts.
</details>
<details>
<summary>摘要</summary>
现有的分布式拒绝服务攻击（DDoS）解决方案无法处理高度归一数据流量，因此不适用于互联网服务提供商（ISP）核心网络。本文提出一种基于数字双胞虫的智能DDoS检测机制，使用在线学习方法。我们的贡献包括：1. 基于数字双胞虫的DDoS检测体系设计 для ISP核心网络。2. 实现了基于YANG模型和自动特征选择（AutoFS）模块来处理核心网络数据。3. 使用在线学习方法来升级模型，实时更新学习模型，提高预测精度。我们的解决方案可以在约15分钟内检测DDoS攻击，并且可以随时更新特征选择方法和学习模型，实现true类别率达97%。
</details></li>
</ul>
<hr>
<h2 id="Vision-Language-Models-are-Zero-Shot-Reward-Models-for-Reinforcement-Learning"><a href="#Vision-Language-Models-are-Zero-Shot-Reward-Models-for-Reinforcement-Learning" class="headerlink" title="Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning"></a>Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12921">http://arxiv.org/abs/2310.12921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juan Rocamonde, Victoriano Montesinos, Elvis Nava, Ethan Perez, David Lindner</li>
<li>for: 使用预训练的视觉语言模型（VLM）作为零次学习的奖励模型（RM），以便使用自然语言来定义任务。</li>
<li>methods: 使用 CLIP 基于的 VLM，通过提供单个文本提示来训练 MuJoCo 人工智能学习复杂任务，无需手动指定奖励函数。</li>
<li>results: 使用 VLM-RMs 训练 MuJoCo 人工智能学习多种复杂任务，包括跪姿、分解和坐法印袋pose，只需提供单个文本提示，并且可以通过提供基线提示来改进性能。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) requires either manually specifying a reward function, which is often infeasible, or learning a reward model from a large amount of human feedback, which is often very expensive. We study a more sample-efficient alternative: using pretrained vision-language models (VLMs) as zero-shot reward models (RMs) to specify tasks via natural language. We propose a natural and general approach to using VLMs as reward models, which we call VLM-RMs. We use VLM-RMs based on CLIP to train a MuJoCo humanoid to learn complex tasks without a manually specified reward function, such as kneeling, doing the splits, and sitting in a lotus position. For each of these tasks, we only provide a single sentence text prompt describing the desired task with minimal prompt engineering. We provide videos of the trained agents at: https://sites.google.com/view/vlm-rm. We can improve performance by providing a second ``baseline'' prompt and projecting out parts of the CLIP embedding space irrelevant to distinguish between goal and baseline. Further, we find a strong scaling effect for VLM-RMs: larger VLMs trained with more compute and data are better reward models. The failure modes of VLM-RMs we encountered are all related to known capability limitations of current VLMs, such as limited spatial reasoning ability or visually unrealistic environments that are far off-distribution for the VLM. We find that VLM-RMs are remarkably robust as long as the VLM is large enough. This suggests that future VLMs will become more and more useful reward models for a wide range of RL applications.
</details>
<details>
<summary>摘要</summary>
强化学习（RL）通常需要手动指定奖励函数，这经常是不可能的或者需要大量的人工反馈，这都是非常昂贵的。我们研究了一种更加样本效率的方法：使用预训练的视觉语言模型（VLM）作为零shot奖励模型（RM），通过自然语言来定义任务。我们提出了一种自然的通用方法，称之为VLM-RM。我们使用CLIP基于VLM来训练一个MuJoCo人型机器人学习复杂任务，无需手动指定奖励函数，例如跪下、坐印、坐法轮等。对于每个任务，我们只提供了一个单 sentence文本提示，描述所需任务的目标，并且尽可能少的提示工程。我们提供了训练过程中的视频在：https://sites.google.com/view/vlm-rm。我们可以通过提供第二个“基准”提示来提高性能，并且可以在CLIP embedding空间中投射出无关于目标和基准的部分，以更好地分辨目标和基准。此外，我们发现VLM-RM失败模式都与当前VLM的能力限制相关，如视觉无法理解空间或环境非常远程。然而，我们发现VLM-RM具有惊人的稳定性，只要VLM具有足够的大小和计算资源，它们就可以成为RL应用中广泛使用的优秀奖励模型。这意味着未来的VLM会变得越来越有用。
</details></li>
</ul>
<hr>
<h2 id="Generative-Marginalization-Models"><a href="#Generative-Marginalization-Models" class="headerlink" title="Generative Marginalization Models"></a>Generative Marginalization Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12920">http://arxiv.org/abs/2310.12920</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/princetonlips/mam">https://github.com/princetonlips/mam</a></li>
<li>paper_authors: Sulin Liu, Peter J. Ramadge, Ryan P. Adams</li>
<li>for: 这篇论文旨在推介一种新的生成模型家族，称为积分模型（MaMs），可以处理高维整数数据。这些模型具有可扩展和灵活的生成模型，以及可追踪的概率函数。</li>
<li>methods: 这篇论文提出了一种基于“积分自适应”的可扩展学习方法，用于学习高维整数数据的生成模型。这种方法可以在energy-based训练中实现可靠的生成模型，并且可以在高维问题上进行任意阶层的生成模型化。</li>
<li>results: 论文通过在多种整数数据集上进行实验，证明了MaMs模型的效果。MaMs模型可以在 maximum likelihood 和 energy-based 训练中对高维整数数据进行高效的生成模型化，并且可以实现数量级快速的 marginal 概率评估。<details>
<summary>Abstract</summary>
We introduce marginalization models (MaMs), a new family of generative models for high-dimensional discrete data. They offer scalable and flexible generative modeling with tractable likelihoods by explicitly modeling all induced marginal distributions. Marginalization models enable fast evaluation of arbitrary marginal probabilities with a single forward pass of the neural network, which overcomes a major limitation of methods with exact marginal inference, such as autoregressive models (ARMs). We propose scalable methods for learning the marginals, grounded in the concept of "marginalization self-consistency". Unlike previous methods, MaMs support scalable training of any-order generative models for high-dimensional problems under the setting of energy-based training, where the goal is to match the learned distribution to a given desired probability (specified by an unnormalized (log) probability function such as energy function or reward function). We demonstrate the effectiveness of the proposed model on a variety of discrete data distributions, including binary images, language, physical systems, and molecules, for maximum likelihood and energy-based training settings. MaMs achieve orders of magnitude speedup in evaluating the marginal probabilities on both settings. For energy-based training tasks, MaMs enable any-order generative modeling of high-dimensional problems beyond the capability of previous methods. Code is at https://github.com/PrincetonLIPS/MaM.
</details>
<details>
<summary>摘要</summary>
我们介绍 MARGINALIZATION MODELS (MaMs)，一新的一代生成模型，适用于高维数据。它具有可扩展和 flexible 的生成模型，并且可以实现可读的可能性。MaMs 可以快速评估任意阶层的边界概率，这解决了前一代方法的一个主要限制，即需要精确地掌握边界概率。我们提出了可扩展的学习方法，基于 "marginalization self-consistency" 的概念。MaMs 支持可扩展的训练高维问题，以energy-based 训练为例，目的是将学习的分布与givens 的 (log) 可能性函数相匹配。我们在不同的数据分布上进行了各种实验，包括 binary 图像、语言、物理系统和分子，并获得了许多次速度的提升。对于 energy-based 训练任务，MaMs 可以实现高维问题的任何阶层生成模型，超出了前一代方法的能力。请参考 https://github.com/PrincetonLIPS/MaM。
</details></li>
</ul>
<hr>
<h2 id="Network-Aware-AutoML-Framework-for-Software-Defined-Sensor-Networks"><a href="#Network-Aware-AutoML-Framework-for-Software-Defined-Sensor-Networks" class="headerlink" title="Network-Aware AutoML Framework for Software-Defined Sensor Networks"></a>Network-Aware AutoML Framework for Software-Defined Sensor Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12914">http://arxiv.org/abs/2310.12914</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emre Horsanali, Yagmur Yigit, Gokhan Secinti, Aytac Karameseoglu, Berk Canberk</li>
<li>for: 本文旨在提出一种基于自适应机器学习（AutoML）的软件定义感知网络中的拒绝服务攻击检测方法，以满足软件定义感知网络和感知网络的安全需求。</li>
<li>methods: 本文使用了一种基于变量负载、不同流量率和检测时间的网络知识检测方法，并选择了适合网络环境的最佳机器学习算法来检测拒绝服务攻击。</li>
<li>results: 本文的实验结果表明，在遭受拒绝服务攻击时，我们的检测方法能够保证网络中的流量 packet仍然在正确的时间内传输，并且可以避免过拟合。<details>
<summary>Abstract</summary>
As the current detection solutions of distributed denial of service attacks (DDoS) need additional infrastructures to handle high aggregate data rates, they are not suitable for sensor networks or the Internet of Things. Besides, the security architecture of software-defined sensor networks needs to pay attention to the vulnerabilities of both software-defined networks and sensor networks. In this paper, we propose a network-aware automated machine learning (AutoML) framework which detects DDoS attacks in software-defined sensor networks. Our framework selects an ideal machine learning algorithm to detect DDoS attacks in network-constrained environments, using metrics such as variable traffic load, heterogeneous traffic rate, and detection time while preventing over-fitting. Our contributions are two-fold: (i) we first investigate the trade-off between the efficiency of ML algorithms and network/traffic state in the scope of DDoS detection. (ii) we design and implement a software architecture containing open-source network tools, with the deployment of multiple ML algorithms. Lastly, we show that under the denial of service attacks, our framework ensures the traffic packets are still delivered within the network with additional delays.
</details>
<details>
<summary>摘要</summary>
Current detection solutions of distributed denial of service attacks (DDoS) require additional infrastructure to handle high aggregate data rates, making them unsuitable for sensor networks or the Internet of Things. Moreover, the security architecture of software-defined sensor networks must address the vulnerabilities of both software-defined networks and sensor networks. In this paper, we propose a network-aware automated machine learning (AutoML) framework that detects DDoS attacks in software-defined sensor networks. Our framework selects the most appropriate machine learning algorithm to detect DDoS attacks in network-constrained environments, taking into account variables such as traffic load, heterogeneous traffic rate, and detection time while preventing overfitting. Our contributions are twofold:1. We investigate the trade-off between the efficiency of ML algorithms and network/traffic state in the context of DDoS detection.2. We design and implement a software architecture that incorporates open-source network tools and deploys multiple ML algorithms.Our framework ensures that traffic packets are still delivered within the network, albeit with additional delays, even under denial of service attacks.
</details></li>
</ul>
<hr>
<h2 id="Experimental-Narratives-A-Comparison-of-Human-Crowdsourced-Storytelling-and-AI-Storytelling"><a href="#Experimental-Narratives-A-Comparison-of-Human-Crowdsourced-Storytelling-and-AI-Storytelling" class="headerlink" title="Experimental Narratives: A Comparison of Human Crowdsourced Storytelling and AI Storytelling"></a>Experimental Narratives: A Comparison of Human Crowdsourced Storytelling and AI Storytelling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12902">http://arxiv.org/abs/2310.12902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nina Begus<br>for:This paper aims to investigate cultural artifacts and social biases in storytelling by both humans and generative AI.methods:The study employs fictional prompts as a novel tool for analyzing storytelling, combining behavioral and computational experiments.results:The study finds that AI-generated stories are more progressive in terms of gender roles and sexuality than human-authored texts, but offer less imaginative scenarios and rhetoric. The study also reveals the pervasive presence of the Pygmalion myth in both human and AI storytelling.<details>
<summary>Abstract</summary>
The paper proposes a framework that combines behavioral and computational experiments employing fictional prompts as a novel tool for investigating cultural artifacts and social biases in storytelling both by humans and generative AI. The study analyzes 250 stories authored by crowdworkers in June 2019 and 80 stories generated by GPT-3.5 and GPT-4 in March 2023 by merging methods from narratology and inferential statistics. Both crowdworkers and large language models responded to identical prompts about creating and falling in love with an artificial human. The proposed experimental paradigm allows a direct comparison between human and LLM-generated storytelling. Responses to the Pygmalionesque prompts confirm the pervasive presence of the Pygmalion myth in the collective imaginary of both humans and large language models. All solicited narratives present a scientific or technological pursuit. The analysis reveals that narratives from GPT-3.5 and particularly GPT-4 are more more progressive in terms of gender roles and sexuality than those written by humans. While AI narratives can occasionally provide innovative plot twists, they offer less imaginative scenarios and rhetoric than human-authored texts. The proposed framework argues that fiction can be used as a window into human and AI-based collective imaginary and social dimensions.
</details>
<details>
<summary>摘要</summary>
文章提出一种框架，结合行为和计算实验，使用虚构提问作为一种新的工具，探讨人类和生成AI对文学作品的叙述方式和社会偏见。研究分析了2019年6月由大量志愿者创作的250个故事，以及2023年3月由GPT-3.5和GPT-4生成的80个故事，通过结合 naratology 和推理统计方法进行分析。人类和大语言模型都 responded to identical prompts about creating and falling in love with an artificial human。提出的实验方法允许直接比较人类和大语言模型生成的叙述方式。对 pygmalionesque 提问的答复表明了人类和大语言模型的 коллектив imagine 中 pygmalion 神话的普遍存在。所有的请求故事都涉及科学或技术的追求。分析发现，GPT-3.5 和尤其是 GPT-4 生成的故事在性别角色和性 orientation 方面比人类生成的故事更加进步。虽然 AI 生成的故事可以提供创新的情节和叙述方式，但它们的想象力和修辞比人类创作的文章更差。提出的框架认为， fiction 可以作为人类和 AI 基础 imagine 和社会方面的窗口。
</details></li>
</ul>
<hr>
<h2 id="Personalized-human-mobility-prediction-for-HuMob-challenge"><a href="#Personalized-human-mobility-prediction-for-HuMob-challenge" class="headerlink" title="Personalized human mobility prediction for HuMob challenge"></a>Personalized human mobility prediction for HuMob challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12900">http://arxiv.org/abs/2310.12900</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masahiro Suzuki, Shomu Furuta, Yusuke Fukazawa</li>
<li>for: 这个论文是为了提出一种基于个人行为特征的人员流动预测方法，以便更好地预测每个人的行动路径。</li>
<li>methods: 这个论文采用了个性化模型来预测每个人的行动路径，而不是预测整个人群的行动。它采用了日期和时间、活动时间、天数、时间和 POI 访问频率等特征，并通过聚类技术将其他具有相似行为特征的人员的运动方向纳入考虑。机器学习模型是支持向量回归（SVR）。</li>
<li>results: 论文通过线上评估准确性和特征选择和参数调整来评估模型的准确性，并发现这种个性化模型具有较低的计算成本而且可以达到较好的准确性。<details>
<summary>Abstract</summary>
We explain the methodology used to create the data submitted to HuMob Challenge, a data analysis competition for human mobility prediction. We adopted a personalized model to predict the individual's movement trajectory from their data, instead of predicting from the overall movement, based on the hypothesis that human movement is unique to each person. We devised the features such as the date and time, activity time, days of the week, time of day, and frequency of visits to POI (Point of Interest). As additional features, we incorporated the movement of other individuals with similar behavior patterns through the employment of clustering. The machine learning model we adopted was the Support Vector Regression (SVR). We performed accuracy through offline assessment and carried out feature selection and parameter tuning. Although overall dataset provided consists of 100,000 users trajectory, our method use only 20,000 target users data, and do not need to use other 80,000 data. Despite the personalized model's traditional feature engineering approach, this model yields reasonably good accuracy with lower computational cost.
</details>
<details>
<summary>摘要</summary>
我们介绍了在 HuMob Challenge 数据分析比赛中使用的数据创建方法ологи。我们采用了个性化模型来预测个人的运动轨迹，而不是基于整体运动的预测，根据假设人类运动各自独特。我们设计了特征，包括日期和时间、活动时间、天数、时间和访问点索引（POI）的频率。在其他特征上，我们通过聚类来 incorporate 其他有相似行为模式的个体的运动。我们采用的机器学习模型是支持向量回归（SVR）。我们通过线上评估来评估准确性，并进行特征选择和参数调整。虽然总体数据集包含100,000个用户的轨迹，但我们的方法只使用了20,000个目标用户的数据，并不需要使用其他80,000个数据。尽管个性化模型采用传统的特征工程方法，但这种模型仍然可以得到相对较好的准确性，同时计算成本较低。
</details></li>
</ul>
<hr>
<h2 id="TwinPot-Digital-Twin-assisted-Honeypot-for-Cyber-Secure-Smart-Seaports"><a href="#TwinPot-Digital-Twin-assisted-Honeypot-for-Cyber-Secure-Smart-Seaports" class="headerlink" title="TwinPot: Digital Twin-assisted Honeypot for Cyber-Secure Smart Seaports"></a>TwinPot: Digital Twin-assisted Honeypot for Cyber-Secure Smart Seaports</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12880">http://arxiv.org/abs/2310.12880</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yagmur Yigit, Omer Kemal Kinaci, Trung Q. Duong, Berk Canberk<br>for: 这个研究是为了提供一个基于双胞萃取技术的智能港区防护系统，以应对现代港区中的攻击和侵略。methods: 本研究使用了双胞萃取技术和诱掳技术来建立一个具有高真实性的诱掳系统，并与现有的防护系统集成。results: 本研究发现，使用双胞萃取技术和诱掳系统可以实现更高的攻击检测精度和防护性。对于智能港区中的内部和外部攻击，我们的解决方案可以成功地检测和防护系统。<details>
<summary>Abstract</summary>
The idea of next-generation ports has become more apparent in the last ten years in response to the challenge posed by the rising demand for efficiency and the ever-increasing volume of goods. In this new era of intelligent infrastructure and facilities, it is evident that cyber-security has recently received the most significant attention from the seaport and maritime authorities, and it is a primary concern on the agenda of most ports. Traditional security solutions can be applied to safeguard IoT and Cyber-Physical Systems (CPS) from harmful entities. Nevertheless, security researchers can only watch, examine, and learn about the behaviors of attackers if these solutions operate more transparently. Herein, honeypots are potential solutions since they offer valuable information about the attackers. It can be virtual or physical. Virtual honeypots must be more realistic to entice attackers, necessitating better high-fidelity. To this end, Digital Twin (DT) technology can be employed to increase the complexity and simulation fidelity of the honeypots. Seaports can be attacked from both their existing devices and external devices at the same time. Existing mechanisms are insufficient to detect external attacks; therefore, the current systems cannot handle attacks at the desired level. DT and honeypot technologies can be used together to tackle them. Consequently, we suggest a DT-assisted honeypot, called TwinPot, for external attacks in smart seaports. Moreover, we propose an intelligent attack detection mechanism to handle different attack types using DT for internal attacks. Finally, we build an extensive smart seaport dataset for internal and external attacks using the MANSIM tool and two existing datasets to test the performance of our system. We show that under simultaneous internal and external attacks on the system, our solution successfully detects internal and external attacks.
</details>
<details>
<summary>摘要</summary>
“随着近期的高效需求和货物量的增加，次代港口设施在过去十年中得到了更多的关注。在这个新时代的智能基础设施和设备中，cyber安全已经成为港口和海事管理部门的首要课题。传统安全解决方案可以保护互联网物理设备和Cyber-Physical Systems（CPS）免于危险威胁。但是，安全研究人员只能通过观察攻击者的行为来了解攻击者。在这种情况下，诱饵（honeypot）是一个有potential的解决方案，因为它们提供了攻击者的有益信息。诱饵可以是虚拟的或物理的。虚拟诱饵需要更加真实，以吸引攻击者，因此需要更高的高精确度。为此，数位双胞袭（DT）技术可以被应用，以增加诱饵的复杂性和模拟精度。 smart ports 可以受到来自现有设备以及外部设备的同时攻击。现有的机制不足以检测外部攻击，因此现有的系统无法处理攻击到所需的水平。DT和诱饵技术可以被用 вместе，以解决这个问题。此外，我们还提出了一个智能攻击探测机制，用于处理不同类型的攻击。最后，我们建立了一个大量的聪明港口数据集，用于内部和外部攻击的测试。我们显示，在同时受到内部和外部攻击的情况下，我们的解决方案成功地检测了内部和外部攻击。”
</details></li>
</ul>
<hr>
<h2 id="Predicting-Ovarian-Cancer-Treatment-Response-in-Histopathology-using-Hierarchical-Vision-Transformers-and-Multiple-Instance-Learning"><a href="#Predicting-Ovarian-Cancer-Treatment-Response-in-Histopathology-using-Hierarchical-Vision-Transformers-and-Multiple-Instance-Learning" class="headerlink" title="Predicting Ovarian Cancer Treatment Response in Histopathology using Hierarchical Vision Transformers and Multiple Instance Learning"></a>Predicting Ovarian Cancer Treatment Response in Histopathology using Hierarchical Vision Transformers and Multiple Instance Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12866">http://arxiv.org/abs/2310.12866</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/scjjb/hipt_abmil_atec23">https://github.com/scjjb/hipt_abmil_atec23</a></li>
<li>paper_authors: Jack Breen, Katie Allen, Kieran Zucker, Geoff Hall, Nishant Ravikumar, Nicolas M. Orsi</li>
<li>for: 这个研究的目的是用深度学习预测抗血管生长药bevacizumab治疗悉尼癌症患者是否能达到至少6个月的痊愈或疾病进展阻止。</li>
<li>methods: 这个研究使用了一种预训练的层次图像变换器（HIPT）和一种注意力基于多个实例学习（ABMIL）模型来提取区域特征和汇集特征，并使用一个内置的权重系统来分类整个扫描片。</li>
<li>results: 这个研究发现，使用HIPT-ABMIL模型可以在282个厚示片图像（WSIs）中预测悉尼癌症患者是否能达到至少6个月的痊愈或疾病进展阻止，并取得了60.2%±2.9%的内部平衡准确率和0.646±0.033的ROC曲线。<details>
<summary>Abstract</summary>
For many patients, current ovarian cancer treatments offer limited clinical benefit. For some therapies, it is not possible to predict patients' responses, potentially exposing them to the adverse effects of treatment without any therapeutic benefit. As part of the automated prediction of treatment effectiveness in ovarian cancer using histopathological images (ATEC23) challenge, we evaluated the effectiveness of deep learning to predict whether a course of treatment including the antiangiogenic drug bevacizumab could contribute to remission or prevent disease progression for at least 6 months in a set of 282 histopathology whole slide images (WSIs) from 78 ovarian cancer patients. Our approach used a pretrained Hierarchical Image Pyramid Transformer (HIPT) to extract region-level features and an attention-based multiple instance learning (ABMIL) model to aggregate features and classify whole slides. The optimal HIPT-ABMIL model had an internal balanced accuracy of 60.2% +- 2.9% and an AUC of 0.646 +- 0.033. Histopathology-specific model pretraining was found to be beneficial to classification performance, though hierarchical transformers were not, with a ResNet feature extractor achieving similar performance. Due to the dataset being small and highly heterogeneous, performance was variable across 5-fold cross-validation folds, and there were some extreme differences between validation and test set performance within folds. The model did not generalise well to tissue microarrays, with accuracy worse than random chance. It is not yet clear whether ovarian cancer WSIs contain information that can be used to accurately predict treatment response, with further validation using larger, higher-quality datasets required.
</details>
<details>
<summary>摘要</summary>
很多病人，现有的卵巢癌治疗方案具有有限的临床效果。一些疗法，无法预测患者的反应，可能会将患者暴露于无效的治疗后果。在自动预测卵巢癌治疗效果使用历史Pathological Images（ATEC23）挑战中，我们评估了深度学习是否可以预测在6个月内remission或疾病进展的可能性，并对78例卵巢癌病人的282个历史Pathology Whole Slide Images（WSIs）进行了分析。我们的方法使用了预训练的层次Image Pyramid Transformer（HIPT）提取区域特征，以及关注基本多实例学习（ABMIL）模型来聚合特征并分类整个扫描图。最佳的HIPT-ABMIL模型具有内部平衡准确率为60.2% ± 2.9%和ROC曲线为0.646 ± 0.033。 histopathology特定的模型预训练被发现对分类性能产生了正面影响，而层次变换器则不是，ResNet特征提取器达到了类似的性能。由于数据集较小且高度多样化，性能在5个批处分验中具有变化性，并且在批处和测试集之间有一些极端的差异。模型无法通过细胞 microarray进行泛化，准确率落后于随机抽样。这表明卵巢癌WSIs中可能没有准确预测治疗效果的信息，需要进一步的验证和 validate larger、higherquality数据集。
</details></li>
</ul>
<hr>
<h2 id="Physical-Information-Neural-Networks-for-Solving-High-index-Differential-algebraic-Equation-Systems-Based-on-Radau-Methods"><a href="#Physical-Information-Neural-Networks-for-Solving-High-index-Differential-algebraic-Equation-Systems-Based-on-Radau-Methods" class="headerlink" title="Physical Information Neural Networks for Solving High-index Differential-algebraic Equation Systems Based on Radau Methods"></a>Physical Information Neural Networks for Solving High-index Differential-algebraic Equation Systems Based on Radau Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12846">http://arxiv.org/abs/2310.12846</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiasheng Chen, Juan Tang, Ming Yan, Shuai Lai, Kun Liang, Jianguang Lu, Wenqiang Yang</li>
<li>for: 解决大型差分代数方程系统（DAE）中的精度问题</li>
<li>methods: 结合Radau IIA数学方法和神经网络结构，使用注意力机制解决高次DAE</li>
<li>results: 对两个经典高次DAE系统进行数值实验，并证明使用5次Radau IIA方法可以实现最高精度解决方案，其绝对误差低于$10^{-6}$，超过现有文献结果。<details>
<summary>Abstract</summary>
As is well known, differential algebraic equations (DAEs), which are able to describe dynamic changes and underlying constraints, have been widely applied in engineering fields such as fluid dynamics, multi-body dynamics, mechanical systems and control theory. In practical physical modeling within these domains, the systems often generate high-index DAEs. Classical implicit numerical methods typically result in varying order reduction of numerical accuracy when solving high-index systems.~Recently, the physics-informed neural network (PINN) has gained attention for solving DAE systems. However, it faces challenges like the inability to directly solve high-index systems, lower predictive accuracy, and weaker generalization capabilities. In this paper, we propose a PINN computational framework, combined Radau IIA numerical method with a neural network structure via the attention mechanisms, to directly solve high-index DAEs. Furthermore, we employ a domain decomposition strategy to enhance solution accuracy. We conduct numerical experiments with two classical high-index systems as illustrative examples, investigating how different orders of the Radau IIA method affect the accuracy of neural network solutions. The experimental results demonstrate that the PINN based on a 5th-order Radau IIA method achieves the highest level of system accuracy. Specifically, the absolute errors for all differential variables remains as low as $10^{-6}$, and the absolute errors for algebraic variables is maintained at $10^{-5}$, surpassing the results found in existing literature. Therefore, our method exhibits excellent computational accuracy and strong generalization capabilities, providing a feasible approach for the high-precision solution of larger-scale DAEs with higher indices or challenging high-dimensional partial differential algebraic equation systems.
</details>
<details>
<summary>摘要</summary>
为了描述动态变化和下面约束，混合方程（DAEs）在工程领域得到广泛应用，如流体动力学、多体动力学、机械系统和控制理论。在实际物理模拟中，系统经常生成高指数DAEs。经典的假设方法通常会导致解的级数减少，从而降低数值精度。在这篇论文中，我们提议一种基于物理学习神经网络（PINN）的计算框架，结合卷积IIA方法和神经网络结构，以直接解决高指数DAEs。此外，我们采用域划分策略来提高解的准确性。我们在两个经典的高指数系统中进行了数值实验，研究不同的卷积IIA方法的影响于神经网络解的精度。实验结果表明，基于5个卷积IIA方法的PINN算法可以达到系统的最高精度水平。具体地，所有的杂分变量的绝对误差保持在10^-6之下，而部分变量的绝对误差保持在10^-5之下，超过现有文献的结果。因此，我们的方法可以具有出色的计算精度和强大的泛化能力，为更大规模的DAEs或更复杂的部分数学方程系统提供可靠的解决方案。
</details></li>
</ul>
<hr>
<h2 id="AgentTuning-Enabling-Generalized-Agent-Abilities-for-LLMs"><a href="#AgentTuning-Enabling-Generalized-Agent-Abilities-for-LLMs" class="headerlink" title="AgentTuning: Enabling Generalized Agent Abilities for LLMs"></a>AgentTuning: Enabling Generalized Agent Abilities for LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12823">http://arxiv.org/abs/2310.12823</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thudm/agenttuning">https://github.com/thudm/agenttuning</a></li>
<li>paper_authors: Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, Jie Tang</li>
<li>for: 提高LLMs的代理能力（agent capabilities），不影响其通用能力（general abilities）。</li>
<li>methods: 使用AgentInstruct dataset和混合 instruciton-tuning策略，升级Llama 2 series为AgentLM。</li>
<li>results: AgentTuning可以提高LLMs的代理能力，而无需妥协其通用能力，AgentLM-70B与GPT-3.5-turbo在未看过任务上的表现相当。<details>
<summary>Abstract</summary>
Open large language models (LLMs) with great performance in various tasks have significantly advanced the development of LLMs. However, they are far inferior to commercial models such as ChatGPT and GPT-4 when acting as agents to tackle complex tasks in the real world. These agent tasks employ LLMs as the central controller responsible for planning, memorization, and tool utilization, necessitating both fine-grained prompting methods and robust LLMs to achieve satisfactory performance. Though many prompting methods have been proposed to complete particular agent tasks, there is lack of research focusing on improving the agent capabilities of LLMs themselves without compromising their general abilities. In this work, we present AgentTuning, a simple and general method to enhance the agent abilities of LLMs while maintaining their general LLM capabilities. We construct AgentInstruct, a lightweight instruction-tuning dataset containing high-quality interaction trajectories. We employ a hybrid instruction-tuning strategy by combining AgentInstruct with open-source instructions from general domains. AgentTuning is used to instruction-tune the Llama 2 series, resulting in AgentLM. Our evaluations show that AgentTuning enables LLMs' agent capabilities without compromising general abilities. The AgentLM-70B is comparable to GPT-3.5-turbo on unseen agent tasks, demonstrating generalized agent capabilities. We open source the AgentInstruct and AgentLM-7B, 13B, and 70B models at https://github.com/THUDM/AgentTuning, serving open and powerful alternatives to commercial LLMs for agent tasks.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）在各种任务中表现出色，但在实际世界中执行复杂任务时，它们远远不如商业模型如ChatGPT和GPT-4。这些代理任务使用LLMs作为中央控制器，负责计划、记忆和工具使用，需要细化的提示方法和Robust LLMs来实现满意性。虽然许多提示方法已经被提出来完成特定任务，但是尚未有关于提高LLMs自身代理能力的研究。在这项工作中，我们提出了AgentTuning方法，用于提高LLMs的代理能力，而不需要妥协其总体能力。我们构建了AgentInstruct数据集，包含高质量交互轨迹，并采用混合指令练级策略，将AgentInstruct与通用领域的开源指令结合使用。我们使用AgentTuning进行指令练级，得到了AgentLM。我们的评估结果表明，AgentTuning可以提高LLMs的代理能力，无需妥协其总体能力。AgentLM-70B与GPT-3.5-turbo在未见agent任务上的表现相当，表明AgentLM具有通用的代理能力。我们将AgentInstruct和AgentLM-7B、13B和70B模型公开发布在GitHub上， serving as open and powerful alternatives to commercial LLMs for agent tasks。
</details></li>
</ul>
<hr>
<h2 id="Hybrid-Search-for-Efficient-Planning-with-Completeness-Guarantees"><a href="#Hybrid-Search-for-Efficient-Planning-with-Completeness-Guarantees" class="headerlink" title="Hybrid Search for Efficient Planning with Completeness Guarantees"></a>Hybrid Search for Efficient Planning with Completeness Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12819">http://arxiv.org/abs/2310.12819</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kalle Kujanpää, Joni Pajarinen, Alexander Ilin</li>
<li>for: 解决复杂的规划问题，使用学习基于搜索的方法可以减少计算复杂性，但是这些方法通常缺乏完整性保证，可能无法找到答案。</li>
<li>methods: 我们提出了一种可以确保完整性的高级搜索方法，即完整子目标搜索（complete subgoal search），通过将高级搜索与低级动作结合在一起，实现了高级搜索的实用性和低级搜索的完整性。</li>
<li>results: 我们在复杂的规划问题上应用了我们的完整子目标搜索方法，并证明了该方法可以确保完整性，并且在一些情况下，可以提高高级搜索的性能。这种方法使得可以在系统中应用高级搜索，并且确保完整性。<details>
<summary>Abstract</summary>
Solving complex planning problems has been a long-standing challenge in computer science. Learning-based subgoal search methods have shown promise in tackling these problems, but they often suffer from a lack of completeness guarantees, meaning that they may fail to find a solution even if one exists. In this paper, we propose an efficient approach to augment a subgoal search method to achieve completeness in discrete action spaces. Specifically, we augment the high-level search with low-level actions to execute a multi-level (hybrid) search, which we call complete subgoal search. This solution achieves the best of both worlds: the practical efficiency of high-level search and the completeness of low-level search. We apply the proposed search method to a recently proposed subgoal search algorithm and evaluate the algorithm trained on offline data on complex planning problems. We demonstrate that our complete subgoal search not only guarantees completeness but can even improve performance in terms of search expansions for instances that the high-level could solve without low-level augmentations. Our approach makes it possible to apply subgoal-level planning for systems where completeness is a critical requirement.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Boosting-Inference-Efficiency-Unleashing-the-Power-of-Parameter-Shared-Pre-trained-Language-Models"><a href="#Boosting-Inference-Efficiency-Unleashing-the-Power-of-Parameter-Shared-Pre-trained-Language-Models" class="headerlink" title="Boosting Inference Efficiency: Unleashing the Power of Parameter-Shared Pre-trained Language Models"></a>Boosting Inference Efficiency: Unleashing the Power of Parameter-Shared Pre-trained Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12818">http://arxiv.org/abs/2310.12818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weize Chen, Xiaoyue Xu, Xu Han, Yankai Lin, Ruobing Xie, Zhiyuan Liu, Maosong Sun, Jie Zhou</li>
<li>for: 提高 parameter-shared PLMs 的推理效率，以适应有限的计算资源和延迟要求。</li>
<li>methods: 基于神经网络 ODEs 的一种简单技术，以及一种简单的预训练技术，可以更加提高推理效率。</li>
<li>results: 对 autoregressive 和 autoencoding PLMs 进行实验，显示了我们的方法的有效性，并提供了新的想法 для更有效地使用 parameter-shared 模型。<details>
<summary>Abstract</summary>
Parameter-shared pre-trained language models (PLMs) have emerged as a successful approach in resource-constrained environments, enabling substantial reductions in model storage and memory costs without significant performance compromise. However, it is important to note that parameter sharing does not alleviate computational burdens associated with inference, thus impeding its practicality in situations characterized by limited stringent latency requirements or computational resources. Building upon neural ordinary differential equations (ODEs), we introduce a straightforward technique to enhance the inference efficiency of parameter-shared PLMs. Additionally, we propose a simple pre-training technique that leads to fully or partially shared models capable of achieving even greater inference acceleration. The experimental results demonstrate the effectiveness of our methods on both autoregressive and autoencoding PLMs, providing novel insights into more efficient utilization of parameter-shared models in resource-constrained settings.
</details>
<details>
<summary>摘要</summary>
parameter-shared预训练语言模型（PLMs）在有限资源环境中成为成功的方法，实现了重要的存储和内存成本减少，而无需妥协性能。然而，需要注意的是，参数共享不会减轻推理过程中的计算压力，因此在具有严格的响应时间要求或计算资源限制的情况下，实际应用中可能存在一定的限制。基于神经ordinary differential equations（ODEs），我们提出了一种简单的技术来提高参数共享PLMs的推理效率。此外，我们还提出了一种简单的预训练技术，可以实现完全或部分共享的模型，以达到更高的推理加速。实验结果表明，我们的方法在autoregressive和autoencodingPLMs中具有显著的效果，为有限资源设置中更有效地使用参数共享模型提供了新的理解。
</details></li>
</ul>
<hr>
<h2 id="2D-3D-Interlaced-Transformer-for-Point-Cloud-Segmentation-with-Scene-Level-Supervision"><a href="#2D-3D-Interlaced-Transformer-for-Point-Cloud-Segmentation-with-Scene-Level-Supervision" class="headerlink" title="2D-3D Interlaced Transformer for Point Cloud Segmentation with Scene-Level Supervision"></a>2D-3D Interlaced Transformer for Point Cloud Segmentation with Scene-Level Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12817">http://arxiv.org/abs/2310.12817</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng-Kun Yang, Min-Hung Chen, Yung-Yu Chuang, Yen-Yu Lin</li>
<li>for: weakly supervised point cloud segmentation</li>
<li>methods: transformer model with two encoders and one decoder, using self-attention and interlaced 2D-3D cross-attention for implicit feature fusion</li>
<li>results: outperforms existing weakly supervised point cloud segmentation methods by a large margin on S3DIS and ScanNet benchmarksHere is the Chinese translation of the three key information points:</li>
<li>for: 弱Supervised点云分割</li>
<li>methods: 转换器模型，使用自注意和跨视图的2D-3D交叉注意来实现隐式特征融合</li>
<li>results: 在S3DIS和ScanNet benchmark上，与现有弱Supervised点云分割方法相比，表现出了大幅提升<details>
<summary>Abstract</summary>
We present a Multimodal Interlaced Transformer (MIT) that jointly considers 2D and 3D data for weakly supervised point cloud segmentation. Research studies have shown that 2D and 3D features are complementary for point cloud segmentation. However, existing methods require extra 2D annotations to achieve 2D-3D information fusion. Considering the high annotation cost of point clouds, effective 2D and 3D feature fusion based on weakly supervised learning is in great demand. To this end, we propose a transformer model with two encoders and one decoder for weakly supervised point cloud segmentation using only scene-level class tags. Specifically, the two encoders compute the self-attended features for 3D point clouds and 2D multi-view images, respectively. The decoder implements interlaced 2D-3D cross-attention and carries out implicit 2D and 3D feature fusion. We alternately switch the roles of queries and key-value pairs in the decoder layers. It turns out that the 2D and 3D features are iteratively enriched by each other. Experiments show that it performs favorably against existing weakly supervised point cloud segmentation methods by a large margin on the S3DIS and ScanNet benchmarks. The project page will be available at https://jimmy15923.github.io/mit_web/.
</details>
<details>
<summary>摘要</summary>
我们提出了一个多模式融合 transformer（MIT），它同时考虑了2D和3D数据 для弱监督点云分类。研究表明了2D和3D特征是点云分类的补充，但现有方法需要额外的2D标注以实现2D-3D信息融合。鉴于点云标注的高成本，有效的2D和3D特征融合基于弱监督学习是非常需要。为此，我们提出了一个 transformer 模型，它包括两个Encoder和一个Decoder，用于弱监督点云分类。具体来说，两个Encoder computes 3D点云和2D多视角图像的自我对应特征，respectively。Decoder 实现了排序2D-3D交叉对话，并执行隐式2D和3D特征融合。我们在Decoder层中 alternate 将查询和键值组替换。发现2D和3D特征在彼此之间轮流浓化。实验结果显示，它在S3DIS和ScanNet参数上与现有弱监督点云分类方法相比，大幅提高了表现。更多信息可以通过查看https://jimmy15923.github.io/mit_web/。
</details></li>
</ul>
<hr>
<h2 id="Prompt-Injection-Attacks-and-Defenses-in-LLM-Integrated-Applications"><a href="#Prompt-Injection-Attacks-and-Defenses-in-LLM-Integrated-Applications" class="headerlink" title="Prompt Injection Attacks and Defenses in LLM-Integrated Applications"></a>Prompt Injection Attacks and Defenses in LLM-Integrated Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12815">http://arxiv.org/abs/2310.12815</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liu00222/open-prompt-injection">https://github.com/liu00222/open-prompt-injection</a></li>
<li>paper_authors: Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, Neil Zhenqiang Gong</li>
<li>for: 这篇论文旨在bridge在LLM-Integrated Applications中的攻击和防御之间的知识 gap。</li>
<li>methods: 该论文提出了一个通用的攻击框架，可以帮助研究人员更好地理解攻击方法，以及一个防御框架，可以帮助开发者设计有效的防御策略。</li>
<li>results: 该论文通过对10个LLMs和7个任务进行系统评估，发现了许多攻击和防御策略，并提供了一个可用的框架来帮助研究人员进一步发展这个领域。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are increasingly deployed as the backend for a variety of real-world applications called LLM-Integrated Applications. Multiple recent works showed that LLM-Integrated Applications are vulnerable to prompt injection attacks, in which an attacker injects malicious instruction/data into the input of those applications such that they produce results as the attacker desires. However, existing works are limited to case studies. As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. We aim to bridge the gap in this work. In particular, we propose a general framework to formalize prompt injection attacks. Existing attacks, which are discussed in research papers and blog posts, are special cases in our framework. Our framework enables us to design a new attack by combining existing attacks. Moreover, we also propose a framework to systematize defenses against prompt injection attacks. Using our frameworks, we conduct a systematic evaluation on prompt injection attacks and their defenses with 10 LLMs and 7 tasks. We hope our frameworks can inspire future research in this field. Our code is available at https://github.com/liu00222/Open-Prompt-Injection.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Model-Merging-by-Uncertainty-Based-Gradient-Matching"><a href="#Model-Merging-by-Uncertainty-Based-Gradient-Matching" class="headerlink" title="Model Merging by Uncertainty-Based Gradient Matching"></a>Model Merging by Uncertainty-Based Gradient Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12808">http://arxiv.org/abs/2310.12808</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nico Daheim, Thomas Möllenhoff, Edoardo Maria Ponti, Iryna Gurevych, Mohammad Emtiyaz Khan</li>
<li>for: 这个论文旨在解释模型训练在不同数据集上的合并方法是如何实现Weighted-averaging，以及这种方法在不同情况下是否会失败。</li>
<li>methods: 该论文使用Weighted-averaging方法将不同数据集上的模型参数进行权重平均，并通过分析梯度匹配度来解释这种方法的成功和失败。</li>
<li>results: 研究发现，Weighted-averaging方法可以在大语言模型和视觉转换器上提供良好的性能和鲁棒性，但在某些情况下可能会失败，这与梯度匹配度的不同有关。<details>
<summary>Abstract</summary>
Models trained on different datasets can be merged by a weighted-averaging of their parameters, but why does it work and when can it fail? Here, we connect the inaccuracy of weighted-averaging to mismatches in the gradients and propose a new uncertainty-based scheme to improve the performance by reducing the mismatch. The connection also reveals implicit assumptions in other schemes such as averaging, task arithmetic, and Fisher-weighted averaging. Our new method gives consistent improvements for large language models and vision transformers, both in terms of performance and robustness to hyperparameters.
</details>
<details>
<summary>摘要</summary>
模型训练在不同的数据集上可以通过将其参数进行权重平均合并，但为什么它会工作，而且在哪些情况下可能失败？我们连接权重平均的不准确性与梯度匹配度的差异，并提出一种基于不确定性的方案来改进性能，从而降低差异。这种连接还暴露了其他方案，如平均、任务加法和鱼得权重平均，的隐式假设。我们的新方法在大语言模型和视Transformers上都得到了一致性的改进，以及随着超参数的Robustness。
</details></li>
</ul>
<hr>
<h2 id="An-effective-theory-of-collective-deep-learning"><a href="#An-effective-theory-of-collective-deep-learning" class="headerlink" title="An effective theory of collective deep learning"></a>An effective theory of collective deep learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12802">http://arxiv.org/abs/2310.12802</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lluís Arola-Fernández, Lucas Lacasa</li>
<li>for: 这种研究的目的是解释人工神经网络系统中集体学习的出现。</li>
<li>methods: 这种模型使用了多种分布式算法，包括对每个神经网络单元的本地学习动态和对单元之间的卷积 Coupling。</li>
<li>results: 研究发现，当集体学习阶段出现时，各个网络单元可以通过私有数据进行训练，并且可以完全泛化到未看到的数据类型。<details>
<summary>Abstract</summary>
Unraveling the emergence of collective learning in systems of coupled artificial neural networks is an endeavor with broader implications for physics, machine learning, neuroscience and society. Here we introduce a minimal model that condenses several recent decentralized algorithms by considering a competition between two terms: the local learning dynamics in the parameters of each neural network unit, and a diffusive coupling among units that tends to homogenize the parameters of the ensemble. We derive the coarse-grained behavior of our model via an effective theory for linear networks that we show is analogous to a deformed Ginzburg-Landau model with quenched disorder. This framework predicts (depth-dependent) disorder-order-disorder phase transitions in the parameters' solutions that reveal the onset of a collective learning phase, along with a depth-induced delay of the critical point and a robust shape of the microscopic learning path. We validate our theory in realistic ensembles of coupled nonlinear networks trained in the MNIST dataset under privacy constraints. Interestingly, experiments confirm that individual networks -- trained only with private data -- can fully generalize to unseen data classes when the collective learning phase emerges. Our work elucidates the physics of collective learning and contributes to the mechanistic interpretability of deep learning in decentralized settings.
</details>
<details>
<summary>摘要</summary>
探讨集成人工神经网络系统中共同学习的出现是一项广泛有关物理学、机器学习、神经科学和社会的研究。我们在这里提出了一个简化的模型，它把几种最近的分布式算法简化为两个参数的竞争：每个神经网络单元的本地学习动力和 Ensemble 中参数的卷积 Coupling。我们通过一种有效的线性网络理论来解释模型的宽泛行为，并证明其与扭曲的加顿兹-兰道模型相似，带有随机噪声。这个框架预测了参数解决方案中的（深度依赖）约束-约束-噪声相变，这些相变表明集成学习阶段的出现，以及深度带来的晚期极点延迟和微型学习路径的稳定形态。我们在使用实际的非线性网络集合和MNIST数据集进行训练的情况下验证了我们的理论。结果表明，当集成学习阶段出现时，各个网络——只使用私有数据进行训练——可以完全通过未看过数据类型进行总结。我们的研究为集成学习的物理学和深度学习在分布式设置中的机制解释做出了贡献。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Graph-Neural-Networks-for-Indian-Legal-Judgment-Prediction"><a href="#Exploring-Graph-Neural-Networks-for-Indian-Legal-Judgment-Prediction" class="headerlink" title="Exploring Graph Neural Networks for Indian Legal Judgment Prediction"></a>Exploring Graph Neural Networks for Indian Legal Judgment Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12800">http://arxiv.org/abs/2310.12800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mann Khatri, Mirza Yusuf, Yaman Kumar, Rajiv Ratn Shah, Ponnurangam Kumaraguru</li>
<li>for: 减轻法律系统受到不公正的判例量压力，提高司法效率和公正性。</li>
<li>methods: 使用图神经网络模型，利用事实证据和前例来预测判例结果，并考虑性别和姓名偏见。</li>
<li>results: 使用XLNet预训练特征得到最佳表现，macro F1分数达75%，链接预测ROC超过80%。<details>
<summary>Abstract</summary>
The burdensome impact of a skewed judges-to-cases ratio on the judicial system manifests in an overwhelming backlog of pending cases alongside an ongoing influx of new ones. To tackle this issue and expedite the judicial process, the proposition of an automated system capable of suggesting case outcomes based on factual evidence and precedent from past cases gains significance. This research paper centres on developing a graph neural network-based model to address the Legal Judgment Prediction (LJP) problem, recognizing the intrinsic graph structure of judicial cases and making it a binary node classification problem. We explored various embeddings as model features, while nodes such as time nodes and judicial acts were added and pruned to evaluate the model's performance. The study is done while considering the ethical dimension of fairness in these predictions, considering gender and name biases. A link prediction task is also conducted to assess the model's proficiency in anticipating connections between two specified nodes. By harnessing the capabilities of graph neural networks and incorporating fairness analyses, this research aims to contribute insights towards streamlining the adjudication process, enhancing judicial efficiency, and fostering a more equitable legal landscape, ultimately alleviating the strain imposed by mounting case backlogs. Our best-performing model with XLNet pre-trained embeddings as its features gives the macro F1 score of 75% for the LJP task. For link prediction, the same set of features is the best performing giving ROC of more than 80%
</details>
<details>
<summary>摘要</summary>
“judicial系统中个偏斜的审判官至案例比例对审判过程带来巨大的负担，这导致了案件库存问题和持续的新案件涌入。为了解决这个问题并加速审判过程，建议一个自动化的系统，可以根据实际证据和过去案例的先例来预测审判结果。这个研究 paper 的目标是发展一个基于图 neural network 的模型，以解决审判预测问题（Legal Judgment Prediction，LJP），并考虑了性别和名称偏见的伦理维护。我们尝试了不同的嵌入 Space 作为模型特征，并在时间节点和司法行为上进行了评估。我们的最佳模型，使用 XLNet 预训嵌入，在 LJP 任务中取得了macro F1 分数为75%。另外，我们还进行了连接预测任务，使用相同的特征集，ROC 的成果超过80%。”Note: Please keep in mind that the translation is done by a machine and may not be as accurate as a human translation. Additionally, the translation may not capture all the nuances and idiomatic expressions of the original text.
</details></li>
</ul>
<hr>
<h2 id="Agri-GNN-A-Novel-Genotypic-Topological-Graph-Neural-Network-Framework-Built-on-GraphSAGE-for-Optimized-Yield-Prediction"><a href="#Agri-GNN-A-Novel-Genotypic-Topological-Graph-Neural-Network-Framework-Built-on-GraphSAGE-for-Optimized-Yield-Prediction" class="headerlink" title="Agri-GNN: A Novel Genotypic-Topological Graph Neural Network Framework Built on GraphSAGE for Optimized Yield Prediction"></a>Agri-GNN: A Novel Genotypic-Topological Graph Neural Network Framework Built on GraphSAGE for Optimized Yield Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13037">http://arxiv.org/abs/2310.13037</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya Gupta, Asheesh Singh<br>for:* 这篇论文旨在帮助农业领域实现更高的生产力和可持续性，通过将技术融入农业领域。methods:* 本论文提出了一个名为 $\textit{Agri-GNN}$ 的新型 Genotypic-Topological Graph Neural Network 框架，用于捕捉农作物间的细微空间和遗传学交互，以提高农作物的收获预测。results:* $\textit{Agri-GNN}$ 在一个包括植物指标、时间、遗传学资讯和位置资料的广泛数据集上进行了实验，得到了 $R^2 &#x3D; .876$ 的收获预测准确性，与基准和其他相关研究相比有很大的改善。<details>
<summary>Abstract</summary>
Agriculture, as the cornerstone of human civilization, constantly seeks to integrate technology for enhanced productivity and sustainability. This paper introduces $\textit{Agri-GNN}$, a novel Genotypic-Topological Graph Neural Network Framework tailored to capture the intricate spatial and genotypic interactions of crops, paving the way for optimized predictions of harvest yields. $\textit{Agri-GNN}$ constructs a Graph $\mathcal{G}$ that considers farming plots as nodes, and then methodically constructs edges between nodes based on spatial and genotypic similarity, allowing for the aggregation of node information through a genotypic-topological filter. Graph Neural Networks (GNN), by design, consider the relationships between data points, enabling them to efficiently model the interconnected agricultural ecosystem. By harnessing the power of GNNs, $\textit{Agri-GNN}$ encapsulates both local and global information from plants, considering their inherent connections based on spatial proximity and shared genotypes, allowing stronger predictions to be made than traditional Machine Learning architectures. $\textit{Agri-GNN}$ is built from the GraphSAGE architecture, because of its optimal calibration with large graphs, like those of farming plots and breeding experiments. $\textit{Agri-GNN}$ experiments, conducted on a comprehensive dataset of vegetation indices, time, genotype information, and location data, demonstrate that $\textit{Agri-GNN}$ achieves an $R^2 = .876$ in yield predictions for farming fields in Iowa. The results show significant improvement over the baselines and other work in the field. $\textit{Agri-GNN}$ represents a blueprint for using advanced graph-based neural architectures to predict crop yield, providing significant improvements over baselines in the field.
</details>
<details>
<summary>摘要</summary>
农业，为人类文明的基础，不断寻求技术进步，以提高生产力和可持续性。本文介绍了一种新的种植物种拟合Graph Neural Network（GNN）框架，称为$\textit{Agri-GNN}$，用于捕捉作物的细致空间和种植物互动关系，为产量预测做出优化。$\textit{Agri-GNN}$首先构建一个图 $\mathcal{G}$，其中 Plot 作为节点，然后遍历节点之间的边基于空间和种植物相似性，通过种植物拟合筛选节点信息。GNN 由于其可以准确地模型数据点之间的关系，因此可以高效地模型农业生态系统的连接关系。通过利用 GNN 的能力，$\textit{Agri-GNN}$ 可以捕捉作物的本地和全局信息，考虑作物的空间距离和共享种植物，从而进行更准确的产量预测。$\textit{Agri-GNN}$ 基于 GraphSAGE 框架，因为它可以对大型图进行优化匹配。在一个包括植被指数、时间、种植物信息和位置数据的全面数据集上进行了 $\textit{Agri-GNN}$ 的实验，结果显示 $\textit{Agri-GNN}$ 在农场产量预测中达到了 $R^2 = .876$。结果表明 $\textit{Agri-GNN}$ 在基线和其他相关工作上具有显著改善。 $\textit{Agri-GNN}$ 为使用先进的图基于神经网络预测作物产量提供了蓝本，并且在基eline上显示出了显著的改善。
</details></li>
</ul>
<hr>
<h2 id="Survival-of-the-Most-Influential-Prompts-Efficient-Black-Box-Prompt-Search-via-Clustering-and-Pruning"><a href="#Survival-of-the-Most-Influential-Prompts-Efficient-Black-Box-Prompt-Search-via-Clustering-and-Pruning" class="headerlink" title="Survival of the Most Influential Prompts: Efficient Black-Box Prompt Search via Clustering and Pruning"></a>Survival of the Most Influential Prompts: Efficient Black-Box Prompt Search via Clustering and Pruning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12774">http://arxiv.org/abs/2310.12774</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cambridgeltl/claps">https://github.com/cambridgeltl/claps</a></li>
<li>paper_authors: Han Zhou, Xingchen Wan, Ivan Vulić, Anna Korhonen</li>
<li>for: 本研究旨在探讨黑盒寄存器搜索的效率和用途。</li>
<li>methods: 本研究提出了一种简单的黑盒搜索方法，named ClaPS，它首先对搜索空间进行分组并剔除不重要的搜索token，然后使用简单的搜索方法在剔除后的搜索空间中进行搜索。</li>
<li>results: 根据多个任务和黑盒语言模型的实验结果，ClaPS方法可以在黑盒搜索中实现state-of-the-art表现，同时减少搜索成本。这些结果表明搜索空间设计和优化在黑盒寄存器搜索中扮演了关键的角色。<details>
<summary>Abstract</summary>
Prompt-based learning has been an effective paradigm for large pretrained language models (LLM), enabling few-shot or even zero-shot learning. Black-box prompt search has received growing interest recently for its distinctive properties of gradient-free optimization, proven particularly useful and powerful for model-as-a-service usage. However, the discrete nature and the complexity of combinatorial optimization hinder the efficiency of modern black-box approaches. Despite extensive research on search algorithms, the crucial aspect of search space design and optimization has been largely overlooked. In this paper, we first conduct a sensitivity analysis by prompting LLM, revealing that only a small number of tokens exert a disproportionate amount of influence on LLM predictions. Leveraging this insight, we propose the Clustering and Pruning for Efficient Black-box Prompt Search (ClaPS), a simple black-box search method that first clusters and prunes the search space to focus exclusively on influential prompt tokens. By employing even simple search methods within the pruned search space, ClaPS achieves state-of-the-art performance across various tasks and LLMs, surpassing the performance of complex approaches while significantly reducing search costs. Our findings underscore the critical role of search space design and optimization in enhancing both the usefulness and the efficiency of black-box prompt-based learning.
</details>
<details>
<summary>摘要</summary>
In this paper, we first conduct a sensitivity analysis by prompting LLMs, revealing that only a small number of tokens have a disproportionate influence on LLM predictions. Building on this insight, we propose the Clustering and Pruning for Efficient Black-box Prompt Search (ClaPS), a simple black-box search method that clusters and prunes the search space to focus exclusively on influential prompt tokens. By employing even simple search methods within the pruned search space, ClaPS achieves state-of-the-art performance across various tasks and LLMs, surpassing the performance of complex approaches while significantly reducing search costs. Our findings highlight the critical role of search space design and optimization in enhancing both the usefulness and efficiency of black-box prompt-based learning.
</details></li>
</ul>
<hr>
<h2 id="Safe-RLHF-Safe-Reinforcement-Learning-from-Human-Feedback"><a href="#Safe-RLHF-Safe-Reinforcement-Learning-from-Human-Feedback" class="headerlink" title="Safe RLHF: Safe Reinforcement Learning from Human Feedback"></a>Safe RLHF: Safe Reinforcement Learning from Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12773">http://arxiv.org/abs/2310.12773</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pku-alignment/safe-rlhf">https://github.com/pku-alignment/safe-rlhf</a></li>
<li>paper_authors: Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, Yaodong Yang</li>
<li>for: 这篇论文旨在提出一种名为Safe Reinforcement Learning from Human Feedback（Safe RLHF）的新算法，用于人类价值调整大语言模型（LLMs）。</li>
<li>methods: Safe RLHF 使用了解释分离问题的方法，将人类偏好的帮助和无害性分开，以避免工作者的混乱，并将两个目标分别训练为奖励和成本模型。</li>
<li>results: 通过三轮精细调整，Safe RLHF 能够优化模型的性能和安全性，比较现有的价值调整算法更好地避免危害性的回应。实验中，我们使用 Safe RLHF 调整了 Alpaca-7B，并与人类偏好相匹配，实现了模型的帮助和无害性。<details>
<summary>Abstract</summary>
With the development of large language models (LLMs), striking a balance between the performance and safety of AI systems has never been more critical. However, the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during LLM training. To address this issue, we propose Safe Reinforcement Learning from Human Feedback (Safe RLHF), a novel algorithm for human value alignment. Safe RLHF explicitly decouples human preferences regarding helpfulness and harmlessness, effectively avoiding the crowdworkers' confusion about the tension and allowing us to train separate reward and cost models. We formalize the safety concern of LLMs as an optimization task of maximizing the reward function while satisfying specified cost constraints. Leveraging the Lagrangian method to solve this constrained problem, Safe RLHF dynamically adjusts the balance between the two objectives during fine-tuning. Through a three-round fine-tuning using Safe RLHF, we demonstrate a superior ability to mitigate harmful responses while enhancing model performance compared to existing value-aligned algorithms. Experimentally, we fine-tuned the Alpaca-7B using Safe RLHF and aligned it with collected human preferences, significantly improving its helpfulness and harmlessness according to human evaluations.
</details>
<details>
<summary>摘要</summary>
大语言模型（LLM）的发展使得保持人工智能系统的性能和安全性更加重要。然而，帮助和无害的目标之间存在内在的矛盾，在LLM训练中带来了一定的挑战。为解决这个问题，我们提出了安全强化学习从人类反馈（Safe RLHF），一种新的人价值对齐算法。Safe RLHF将人类对帮助和无害的喜好分开显式地避免了协作者的混乱，让我们可以分开训练奖励和成本模型。我们将LLM的安全问题定义为最大化奖励函数的优化问题，满足specified的成本约束。通过拉格朗日方法解决这个受约问题，Safe RLHF在细化过程中动态调整了两个目标之间的平衡。通过三轮细化使用Safe RLHF，我们示出了与现有的值适应算法相比，更好地 mitigate harmful responses  while enhancing model performance。实验中，我们使用Safe RLHF细化了Alpaca-7B，并与收集的人类偏好相对应，显著提高了其帮助和无害性。
</details></li>
</ul>
<hr>
<h2 id="SemantIC-Semantic-Interference-Cancellation-Towards-6G-Wireless-Communications"><a href="#SemantIC-Semantic-Interference-Cancellation-Towards-6G-Wireless-Communications" class="headerlink" title="SemantIC: Semantic Interference Cancellation Towards 6G Wireless Communications"></a>SemantIC: Semantic Interference Cancellation Towards 6G Wireless Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12768">http://arxiv.org/abs/2310.12768</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/linwest/SemantIC">https://github.com/linwest/SemantIC</a></li>
<li>paper_authors: Wensheng Lin, Yuna Yan, Lixin Li, Zhu Han, Tad Matsumoto</li>
<li>for: 提高6G无线网络信息质量</li>
<li>methods: 使用语义干扰除法（SemantIC）</li>
<li>results: 无需额外频率资源，可以提高信号干扰抑制性和信息质量<details>
<summary>Abstract</summary>
This letter proposes a novel anti-interference technique, semantic interference cancellation (SemantIC), for enhancing information quality towards the sixth-generation (6G) wireless networks. SemantIC only requires the receiver to concatenate the channel decoder with a semantic auto-encoder. This constructs a turbo loop which iteratively and alternately eliminates noise in the signal domain and the semantic domain. From the viewpoint of network information theory, the neural network of the semantic auto-encoder stores side information by training, and provides side information in iterative decoding, as an implementation of the Wyner-Ziv theorem. Simulation results verify the performance improvement by SemantIC without extra channel resource cost.
</details>
<details>
<summary>摘要</summary>
这封信件提出了一种新的反干扰技术，即语义干扰抑制（SemantIC），用于提高 sixth-generation（6G）无线网络中信息质量。SemantIC只需接收器将通道解码器与语义自动编码器 concatenate 起来，这将construct一个 turbo 循环，通过逐次和 alternately 消除信号域和语义域中的干扰。从网络信息理论的视角来看，语义自动编码器的神经网络在训练中存储了侧信息，并在循环解码中提供了侧信息，实现了万ер- Жи夫定理。实验结果表明SemantIC可以不添加额外通道资源成本下提高性能。
</details></li>
</ul>
<hr>
<h2 id="Training-binary-neural-networks-without-floating-point-precision"><a href="#Training-binary-neural-networks-without-floating-point-precision" class="headerlink" title="Training binary neural networks without floating point precision"></a>Training binary neural networks without floating point precision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19815">http://arxiv.org/abs/2310.19815</a></li>
<li>repo_url: None</li>
<li>paper_authors: Federico Fontana</li>
<li>for: 提高 binary neural network 训练效率，实现低延迟低能耗网络。</li>
<li>methods: 提出两种解决方案，包括 topology 变化和策略训练，使网络达到接近现状前方性和高效训练。</li>
<li>results: 实现 near state-of-the-art 性能和高效训练，降低训练时间和内存占用量。<details>
<summary>Abstract</summary>
The main goal of this work is to improve the efficiency of training binary neural networks, which are low latency and low energy networks. The main contribution of this work is the proposal of two solutions comprised of topology changes and strategy training that allow the network to achieve near the state-of-the-art performance and efficient training. The time required for training and the memory required in the process are two factors that contribute to efficient training.
</details>
<details>
<summary>摘要</summary>
主要目标是提高Binary神经网络训练效率，这些神经网络具有低延迟和低能耗特性。本工作的主要贡献是提出两种解决方案，包括结构变化和策略训练，使网络达到近状态艺术性能和高效训练。训练时间和训练过程中所需的内存是两个 contribuuting 因素。Note: "Binary neural networks" in the original text is translated as "Binary神经网络" in Simplified Chinese, which is a common way to refer to neural networks with binary weights and activations.
</details></li>
</ul>
<hr>
<h2 id="LASER-Linear-Compression-in-Wireless-Distributed-Optimization"><a href="#LASER-Linear-Compression-in-Wireless-Distributed-Optimization" class="headerlink" title="LASER: Linear Compression in Wireless Distributed Optimization"></a>LASER: Linear Compression in Wireless Distributed Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13033">http://arxiv.org/abs/2310.13033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashok Vardhan Makkuva, Marco Bondaschi, Thijs Vogels, Martin Jaggi, Hyeji Kim, Michael C. Gastpar</li>
<li>for: 这篇论文主要是为了解决分布式优化中的通信瓶颈问题。</li>
<li>methods: 这篇论文提出了一种名为LASER的线性压缩方法，利用梯度的低级结构来高效地传输杂变通信频道。</li>
<li>results: 实验结果表明，LASER比基eline对比，在计算机视觉和GPT语言模型任务上表现出了Consistent的提升，特别是在噪声通信频道上，可以获得50-64%的提升。<details>
<summary>Abstract</summary>
Data-parallel SGD is the de facto algorithm for distributed optimization, especially for large scale machine learning. Despite its merits, communication bottleneck is one of its persistent issues. Most compression schemes to alleviate this either assume noiseless communication links, or fail to achieve good performance on practical tasks. In this paper, we close this gap and introduce LASER: LineAr CompreSsion in WirEless DistRibuted Optimization. LASER capitalizes on the inherent low-rank structure of gradients and transmits them efficiently over the noisy channels. Whilst enjoying theoretical guarantees similar to those of the classical SGD, LASER shows consistent gains over baselines on a variety of practical benchmarks. In particular, it outperforms the state-of-the-art compression schemes on challenging computer vision and GPT language modeling tasks. On the latter, we obtain $50$-$64 \%$ improvement in perplexity over our baselines for noisy channels.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate text into Simplified ChineseData-parallel SGD 是大规模机器学习中的标准算法，却受到交通瓶颈的困扰。大多数压缩方案可以减轻这个问题，但是它们假设无噪通信链路，或者在实际任务上没有良好的性能。在这篇论文中，我们填补这个空白和引入了LASER：线性压缩在无线分布优化中。LASER 利用梯度的自然低维结构，并将其高效地传输到噪音通信链路上。与传统的SGD 算法相似，LASER 具有类似的理论保证，但是在实际任务上显示出了与基准模型的一致性。特别是在计算机视觉和 GPT 自然语言模型任务上，LASER 表现出了50-64%的改善。在后一个任务中，我们对噪音通信链路上的基准模型进行了50-64%的改善。<</SYS>>
</details></li>
</ul>
<hr>
<h2 id="Neurosymbolic-Grounding-for-Compositional-World-Models"><a href="#Neurosymbolic-Grounding-for-Compositional-World-Models" class="headerlink" title="Neurosymbolic Grounding for Compositional World Models"></a>Neurosymbolic Grounding for Compositional World Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12690">http://arxiv.org/abs/2310.12690</a></li>
<li>repo_url: None</li>
<li>paper_authors: Atharva Sehgal, Arya Grayeli, Jennifer J. Sun, Swarat Chaudhuri</li>
<li>for:  cosm os是一个框架，旨在实现物件中心的世界模型，以进行可compose的通用化（CG），即通过视觉“原子”的合成，实现高性能的未见input scene。</li>
<li>methods:  cosm os使用了一种新的征识符学符合grounding，包括：(i) neurosymbolic scene encodings，即每个场景中的实体使用一个真实向量，由神经网络编码器计算，以及一组可组合的符号描述实体的属性; (ii) neurosymbolic attention机制，将这些实体与学习的交互规则绑定。</li>
<li>results:  cosm os在一个已知的块组 pushing 领域进行了评估，并实现了一个新的状态剑框架，在可 compose 的通用化方面实现了新的州of-the-art。<details>
<summary>Abstract</summary>
We introduce Cosmos, a framework for object-centric world modeling that is designed for compositional generalization (CG), i.e., high performance on unseen input scenes obtained through the composition of known visual "atoms." The central insight behind Cosmos is the use of a novel form of neurosymbolic grounding. Specifically, the framework introduces two new tools: (i) neurosymbolic scene encodings, which represent each entity in a scene using a real vector computed using a neural encoder, as well as a vector of composable symbols describing attributes of the entity, and (ii) a neurosymbolic attention mechanism that binds these entities to learned rules of interaction. Cosmos is end-to-end differentiable; also, unlike traditional neurosymbolic methods that require representations to be manually mapped to symbols, it computes an entity's symbolic attributes using vision-language foundation models. Through an evaluation that considers two different forms of CG on an established blocks-pushing domain, we show that the framework establishes a new state-of-the-art for CG in world modeling.
</details>
<details>
<summary>摘要</summary>
我们介绍 Cosmos，一个基于物体中心的世界模型框架，旨在实现compositional generalization（CG），即在见到的输入场景中表现出高效性。 Cosmos的中心思想是通过一种新的神经 символіック对应。 Specifically, the framework introduces two new tools: (i) neurosymbolic scene encodings, which represent each entity in a scene using a real vector computed using a neural encoder, as well as a vector of composable symbols describing attributes of the entity, and (ii) a neurosymbolic attention mechanism that binds these entities to learned rules of interaction. Cosmos is end-to-end differentiable; also, unlike traditional neurosymbolic methods that require representations to be manually mapped to symbols, it computes an entity's symbolic attributes using vision-language foundation models. Through an evaluation that considers two different forms of CG on an established blocks-pushing domain, we show that the framework establishes a new state-of-the-art for CG in world modeling.Here's the translation of the text into Traditional Chinese:我们介绍 Cosmos，一个基于物体中心的世界模型框架，旨在实现compositional generalization（CG），即在见到的输入场景中表现出高效性。 Cosmos的中心思想是通过一种新的神经 символіック对应。 Specifically, the framework introduces two new tools: (i) neurosymbolic scene encodings, which represent each entity in a scene using a real vector computed using a neural encoder, as well as a vector of composable symbols describing attributes of the entity, and (ii) a neurosymbolic attention mechanism that binds these entities to learned rules of interaction. Cosmos is end-to-end differentiable; also, unlike traditional neurosymbolic methods that require representations to be manually mapped to symbols, it computes an entity's symbolic attributes using vision-language foundation models. Through an evaluation that considers two different forms of CG on an established blocks-pushing domain, we show that the framework establishes a new state-of-the-art for CG in world modeling.
</details></li>
</ul>
<hr>
<h2 id="Compression-of-Recurrent-Neural-Networks-using-Matrix-Factorization"><a href="#Compression-of-Recurrent-Neural-Networks-using-Matrix-Factorization" class="headerlink" title="Compression of Recurrent Neural Networks using Matrix Factorization"></a>Compression of Recurrent Neural Networks using Matrix Factorization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12688">http://arxiv.org/abs/2310.12688</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deathekirl/low-rank-approximation">https://github.com/deathekirl/low-rank-approximation</a></li>
<li>paper_authors: Lucas Maison, Hélion du Mas des Bourboux, Thomas Courtat</li>
<li>for: 压缩神经网络，以提高实时或嵌入式应用中模型的可扩展性和效率。</li>
<li>methods: 使用低级别预测来因素化模型的矩阵，并通过rank-tuning方法选择不同的级别。同时使用训练改进来实现压缩。</li>
<li>results: 在信号处理任务上，可以对循环神经网络进行14倍的压缩，而且最多产生1.4%的性能下降。<details>
<summary>Abstract</summary>
Compressing neural networks is a key step when deploying models for real-time or embedded applications. Factorizing the model's matrices using low-rank approximations is a promising method for achieving compression. While it is possible to set the rank before training, this approach is neither flexible nor optimal. In this work, we propose a post-training rank-selection method called Rank-Tuning that selects a different rank for each matrix. Used in combination with training adaptations, our method achieves high compression rates with no or little performance degradation. Our numerical experiments on signal processing tasks show that we can compress recurrent neural networks up to 14x with at most 1.4% relative performance reduction.
</details>
<details>
<summary>摘要</summary>
压缩神经网络是部署模型实时或嵌入式应用时的关键步骤。使用低级别应对approximation фактор化模型的矩阵是一种有前途的方法。although it is possible to set the rank before training, this approach is neither flexible nor optimal。在这种工作中，我们提出了一种post-training rank-selection方法，叫做Rank-Tuning，它可以为每个矩阵选择不同的极值。通过与训练改进相结合，我们的方法可以实现高度压缩，而无或很少的性能下降。我们的数字实验表明，可以将回传神经网络压缩到14倍，最多1.4%的相对性能下降。
</details></li>
</ul>
<hr>
<h2 id="Quality-Diversity-through-AI-Feedback"><a href="#Quality-Diversity-through-AI-Feedback" class="headerlink" title="Quality-Diversity through AI Feedback"></a>Quality-Diversity through AI Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13032">http://arxiv.org/abs/2310.13032</a></li>
<li>repo_url: None</li>
<li>paper_authors: Herbie Bradley, Andrew Dai, Hannah Teufel, Jenny Zhang, Koen Oostermeijer, Marco Bellagente, Jeff Clune, Kenneth Stanley, Grégory Schott, Joel Lehman</li>
<li>for: 这 paper 的目的是探讨Quality-Diversity (QD) 搜索算法在文学创作领域的应用。</li>
<li>methods: 这 paper 使用了语言模型 (LM) 来引导搜索，并通过人工选择和淘汰来优化候选文本的质量和多样性。</li>
<li>results: 对比非 QD 控制方法，QDAIF 能够更好地覆盖搜索空间，并生成高质量的创作文本。人工评价也表明，AI 和人类评价之间存在相对的一致。<details>
<summary>Abstract</summary>
In many text-generation problems, users may prefer not only a single response, but a diverse range of high-quality outputs from which to choose. Quality-diversity (QD) search algorithms aim at such outcomes, by continually improving and diversifying a population of candidates. However, the applicability of QD to qualitative domains, like creative writing, has been limited by the difficulty of algorithmically specifying measures of quality and diversity. Interestingly, recent developments in language models (LMs) have enabled guiding search through AI feedback, wherein LMs are prompted in natural language to evaluate qualitative aspects of text. Leveraging this development, we introduce Quality-Diversity through AI Feedback (QDAIF), wherein an evolutionary algorithm applies LMs to both generate variation and evaluate the quality and diversity of candidate text. When assessed on creative writing domains, QDAIF covers more of a specified search space with high-quality samples than do non-QD controls. Further, human evaluation of QDAIF-generated creative texts validates reasonable agreement between AI and human evaluation. Our results thus highlight the potential of AI feedback to guide open-ended search for creative and original solutions, providing a recipe that seemingly generalizes to many domains and modalities. In this way, QDAIF is a step towards AI systems that can independently search, diversify, evaluate, and improve, which are among the core skills underlying human society's capacity for innovation.
</details>
<details>
<summary>摘要</summary>
在许多文本生成问题中，用户可能会希望不仅得到一个响应，而是一个多样化的高质量输出，从中选择。质量多样性（QD）搜索算法target这些输出，通过不断改进和多样化候选人 population。然而，在艺术创作领域，QD的应用受到了算法specifying measure of quality和多样性的difficulty的限制。幸好，现在的语言模型（LM）的发展使得可以通过人工智能反馈来引导搜索，其中LM被提示在自然语言中评估文本的质量和多样性。基于这一发展，我们介绍了质量多样性通过人工智能反馈（QDAIF），其中演化算法利用LM来生成多样性和评估候选人的质量和多样性。在艺术创作领域进行评估，QDAIF能够更好地覆盖指定的搜索空间，并且输出高质量的样本。此外，人类对QDAIF生成的艺术创作文本进行评估，并与人工智能评估 Display reasonable agreement。我们的结果因此表明了人工智能反馈可以引导开放的搜索，以找到创新和原创的解决方案，这种方法似乎可以普遍应用于多个领域和模式。因此，QDAIF是一步向AI系统独立搜索、多样化、评估和改进，这些技能是人类社会创新的核心。
</details></li>
</ul>
<hr>
<h2 id="A-Use-Case-Reformulating-Query-Rewriting-as-a-Statistical-Machine-Translation-Problem"><a href="#A-Use-Case-Reformulating-Query-Rewriting-as-a-Statistical-Machine-Translation-Problem" class="headerlink" title="A Use Case: Reformulating Query Rewriting as a Statistical Machine Translation Problem"></a>A Use Case: Reformulating Query Rewriting as a Statistical Machine Translation Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13031">http://arxiv.org/abs/2310.13031</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdullah Can Algan, Emre Yürekli, Aykut Çayır</li>
<li>for: 该论文目的是提高现代搜索引擎的搜索结果相关性，通过用户查询 rewrite 模型来实现。</li>
<li>methods: 该论文提出了一个基于单语言机器翻译模型的查询 rewrite 管道，并对用户查询进行预处理，以创建用户查询和网页标题之间的映射。</li>
<li>results: 该论文通过使用这种查询 rewrite 管道，可以提高搜索结果的相关性和精度。<details>
<summary>Abstract</summary>
One of the most important challenges for modern search engines is to retrieve relevant web content based on user queries. In order to achieve this challenge, search engines have a module to rewrite user queries. That is why modern web search engines utilize some statistical and neural models used in the natural language processing domain. Statistical machine translation is a well-known NLP method among them. The paper proposes a query rewriting pipeline based on a monolingual machine translation model that learns to rewrite Arabic user search queries. This paper also describes preprocessing steps to create a mapping between user queries and web page titles.
</details>
<details>
<summary>摘要</summary>
现代搜索引擎面临着一个非常重要的挑战，即根据用户查询 retrieve relevante web内容。为了解决这个挑战，搜索引擎通常具有一个用于重写用户查询的模块。为了实现这一目标，现代网络搜索引擎通常使用自然语言处理领域的统计学和神经网络模型。统计机器翻译是这些模型中的一个非常知名的方法。本文提出了一个基于单语言机器翻译模型的查询重写管道，该管道可以学习重写阿拉伯语用户搜索查询。本文还描述了为创建用户查询和网页标题之间的映射而进行的预处理步骤。
</details></li>
</ul>
<hr>
<h2 id="PSYCHIC-A-Neuro-Symbolic-Framework-for-Knowledge-Graph-Question-Answering-Grounding"><a href="#PSYCHIC-A-Neuro-Symbolic-Framework-for-Knowledge-Graph-Question-Answering-Grounding" class="headerlink" title="PSYCHIC: A Neuro-Symbolic Framework for Knowledge Graph Question-Answering Grounding"></a>PSYCHIC: A Neuro-Symbolic Framework for Knowledge Graph Question-Answering Grounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12638">http://arxiv.org/abs/2310.12638</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanna Abi Akl</li>
<li>for: 本研究用于回答知识 graphs（KGs）上的问题。</li>
<li>methods: 我们提出了一种 neuralsymbolic（NS）框架，基于EXTRACTIVE QA模型PSYCHIC，可以识别问题和关联的实体。</li>
<li>results: 我们的系统在问题回答任务（KGQA）上取得了0.18%的F1分数，并在实体链接任务（EL）上取得了71.00%的分数。<details>
<summary>Abstract</summary>
The Scholarly Question Answering over Linked Data (Scholarly QALD) at The International Semantic Web Conference (ISWC) 2023 challenge presents two sub-tasks to tackle question answering (QA) over knowledge graphs (KGs). We answer the KGQA over DBLP (DBLP-QUAD) task by proposing a neuro-symbolic (NS) framework based on PSYCHIC, an extractive QA model capable of identifying the query and entities related to a KG question. Our system achieved a F1 score of 00.18% on question answering and came in third place for entity linking (EL) with a score of 71.00%.
</details>
<details>
<summary>摘要</summary>
“学术问答 над连接数据”（Scholarly QALD）在2023年国际semantic Web会议（ISWC）挑战中提出了两个子任务来解决知识图（KG）上的问答（QA）。我们使用一个基于neuro-symbolic（NS）框架，其中PSYCHIC是一种提取式QA模型，可以识别KG问题中的查询和相关的实体。我们的系统在问答任务上达到了0.18%的F1分数，并在实体关联（EL）任务上达到了71.00%的分数。
</details></li>
</ul>
<hr>
<h2 id="On-existence-uniqueness-and-scalability-of-adversarial-robustness-measures-for-AI-classifiers"><a href="#On-existence-uniqueness-and-scalability-of-adversarial-robustness-measures-for-AI-classifiers" class="headerlink" title="On existence, uniqueness and scalability of adversarial robustness measures for AI classifiers"></a>On existence, uniqueness and scalability of adversarial robustness measures for AI classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14421">http://arxiv.org/abs/2310.14421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Illia Horenko</li>
<li>for: 该论文是为了解决机器学习模型的攻击性和风险性问题，提供了一种可靠的存在和唯一性条件，以及可 analytical 计算方法。</li>
<li>methods: 论文使用了一种基于泛函理论的方法，并提出了一种基于 entropy 的 AI 模型。</li>
<li>results: 论文提出了一种可 analytical 计算的最小敌对路径（MAP）和最小敌对距离（MAD），并在不同类型的 AI 工具（如神经网络、启动随机森林、GLM 和 EAI）上进行了实践计算和比较。 另外，论文还解释了如何使用 MAP 提供专门的患者特定风险 Mitigation 方案。<details>
<summary>Abstract</summary>
Simply-verifiable mathematical conditions for existence, uniqueness and explicit analytical computation of minimal adversarial paths (MAP) and minimal adversarial distances (MAD) for (locally) uniquely-invertible classifiers, for generalized linear models (GLM), and for entropic AI (EAI) are formulated and proven. Practical computation of MAP and MAD, their comparison and interpretations for various classes of AI tools (for neuronal networks, boosted random forests, GLM and EAI) are demonstrated on the common synthetic benchmarks: on a double Swiss roll spiral and its extensions, as well as on the two biomedical data problems (for the health insurance claim predictions, and for the heart attack lethality classification). On biomedical applications it is demonstrated how MAP provides unique minimal patient-specific risk-mitigating interventions in the predefined subsets of accessible control variables.
</details>
<details>
<summary>摘要</summary>
<<SYS>>通过形式化的数学条件，得出最小对抗路径（MAP）和最小对抗距离（MAD）的存在、唯一性和计算方法， для（本地）唯一推导类型（GLM）和生成Entropic AI（EAI）。在各种AI工具（神经网络、提高Random Forest、GLM和EAI）中进行实际计算MAP和MAD，并对它们进行比较和解释。在Double Swiss Roll Spiral和其扩展问题上，以及在两个生物医学数据问题（健康保险laims预测和心血栓病性分类）上进行实践。在生物医学应用中，MAP提供了唯一的最小特定患者风险减轻 intervención。Note: " Simplified Chinese" is also known as "Mandarin Chinese" or "Standard Chinese".Please note that the translation is done using a machine translation tool, and may not be 100% accurate or idiomatic.
</details></li>
</ul>
<hr>
<h2 id="Towards-a-Deep-Learning-based-Online-Quality-Prediction-System-for-Welding-Processes"><a href="#Towards-a-Deep-Learning-based-Online-Quality-Prediction-System-for-Welding-Processes" class="headerlink" title="Towards a Deep Learning-based Online Quality Prediction System for Welding Processes"></a>Towards a Deep Learning-based Online Quality Prediction System for Welding Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12632">http://arxiv.org/abs/2310.12632</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yannik Hahn, Robert Maack, Guido Buchholz, Marion Purrio, Matthias Angerhausen, Hasan Tercan, Tobias Meisen</li>
<li>for: 这个论文旨在提出一种基于深度学习的预测质量系统，用于监控和评估钢材气动焊（GMAW） proces。</li>
<li>methods: 该论文提出了一个包括四个主要阶段的概念：数据收集和管理（如电压和电流）、实时处理和特征工程（使用自适应神经网络）、使用适当的循环深度学习模型进行质量预测，以及在 proces 条件变化时进行模型进化（使用不间断学习）。</li>
<li>results: 该论文未提供实际result，但提出了一种可靠的预测质量系统基于深度学习，可以在非实验室环境中实时监控和评估GMAW proces。<details>
<summary>Abstract</summary>
The digitization of manufacturing processes enables promising applications for machine learning-assisted quality assurance. A widely used manufacturing process that can strongly benefit from data-driven solutions is gas metal arc welding (GMAW). The welding process is characterized by complex cause-effect relationships between material properties, process conditions and weld quality. In non-laboratory environments with frequently changing process parameters, accurate determination of weld quality by destructive testing is economically unfeasible. Deep learning offers the potential to identify the relationships in available process data and predict the weld quality from process observations. In this paper, we present a concept for a deep learning based predictive quality system in GMAW. At its core, the concept involves a pipeline consisting of four major phases: collection and management of multi-sensor data (e.g. current and voltage), real-time processing and feature engineering of the time series data by means of autoencoders, training and deployment of suitable recurrent deep learning models for quality predictions, and model evolutions under changing process conditions using continual learning. The concept provides the foundation for future research activities in which we will realize an online predictive quality system for running production.
</details>
<details>
<summary>摘要</summary>
随着制造过程的数字化，机器学习助成质量监控应用得到了推动。一种广泛使用的制造过程是气密填充焊接（GMAW）。焊接过程具有复杂的原因关系，其中物质性、过程参数和焊接质量之间存在紧密的关系。在实际生产环境中，通过采用破坏性测试准确地确定焊接质量是经济不可行。深度学习可以识别可用的过程数据中的关系，预测焊接质量基于过程观察。在这篇论文中，我们提出了一种基于深度学习的预测质量系统概念。这个概念包括四个主要阶段：收集和管理多感器数据（如电流和电压）、实时处理和特征工程时间序列数据使用自动编码器、训练和部署适合的循环深度学习模型以进行质量预测、以及在过程参数变化时使用连续学习进行模型进化。这个概念提供了未来研究活动的基础，我们将实现在生产中运行的在线预测质量系统。
</details></li>
</ul>
<hr>
<h2 id="Heart-Disease-Detection-using-Vision-Based-Transformer-Models-from-ECG-Images"><a href="#Heart-Disease-Detection-using-Vision-Based-Transformer-Models-from-ECG-Images" class="headerlink" title="Heart Disease Detection using Vision-Based Transformer Models from ECG Images"></a>Heart Disease Detection using Vision-Based Transformer Models from ECG Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12630">http://arxiv.org/abs/2310.12630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeynep Hilal Kilimci, Mustafa Yalcin, Ayhan Kucukmanisa, Amit Kumar Mishra</li>
<li>for: 您的论文旨在检测心血管疾病，使用最新的技术和计算方法。</li>
<li>methods: 您使用了视transformer模型，包括Google-Vit、Microsoft-Beit和Swin-Tiny等。</li>
<li>results: 实验结果显示，您的方法可以准确地检测心血管疾病。<details>
<summary>Abstract</summary>
Heart disease, also known as cardiovascular disease, is a prevalent and critical medical condition characterized by the impairment of the heart and blood vessels, leading to various complications such as coronary artery disease, heart failure, and myocardial infarction. The timely and accurate detection of heart disease is of paramount importance in clinical practice. Early identification of individuals at risk enables proactive interventions, preventive measures, and personalized treatment strategies to mitigate the progression of the disease and reduce adverse outcomes. In recent years, the field of heart disease detection has witnessed notable advancements due to the integration of sophisticated technologies and computational approaches. These include machine learning algorithms, data mining techniques, and predictive modeling frameworks that leverage vast amounts of clinical and physiological data to improve diagnostic accuracy and risk stratification. In this work, we propose to detect heart disease from ECG images using cutting-edge technologies, namely vision transformer models. These models are Google-Vit, Microsoft-Beit, and Swin-Tiny. To the best of our knowledge, this is the initial endeavor concentrating on the detection of heart diseases through image-based ECG data by employing cuttingedge technologies namely, transformer models. To demonstrate the contribution of the proposed framework, the performance of vision transformer models are compared with state-of-the-art studies. Experiment results show that the proposed framework exhibits remarkable classification results.
</details>
<details>
<summary>摘要</summary>
心脏病，也称为心血管疾病，是一种非常普遍和严重的医疗病种， caracterized by impairment of the heart and blood vessels, leading to various complications such as coronary artery disease, heart failure, and myocardial infarction. 时间和准确的检测心脏病非常重要在临床实践中。早期识别患者风险可以实施措施，预防措施和个性化的治疗策略，以降低疾病的进程和不良结果。在过去几年，心脏病检测领域已经经历了显著的进步，它们是通过融合先进技术和计算方法的努力。这些包括机器学习算法、数据挖掘技术和预测模型框架，这些技术可以利用庞大的临床和生理学数据，提高诊断精度和风险分级。在这项工作中，我们提议使用电cardioGRAM（ECG）图像来检测心脏病，使用 cutting-edge 技术，即视Transformer模型。这些模型包括Google-Vit、Microsoft-Beit和Swin-Tiny。到目前为止，这是第一个集中于通过图像基本ECG数据来检测心脏病的研究，使用 cutting-edge 技术，即 transformer 模型。为了证明提案的贡献，我们将比较视Transformer模型的表现与现有研究的最佳成果。实验结果显示，我们的提案框架具有惊人的分类效果。
</details></li>
</ul>
<hr>
<h2 id="Cross-attention-Spatio-temporal-Context-Transformer-for-Semantic-Segmentation-of-Historical-Maps"><a href="#Cross-attention-Spatio-temporal-Context-Transformer-for-Semantic-Segmentation-of-Historical-Maps" class="headerlink" title="Cross-attention Spatio-temporal Context Transformer for Semantic Segmentation of Historical Maps"></a>Cross-attention Spatio-temporal Context Transformer for Semantic Segmentation of Historical Maps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12616">http://arxiv.org/abs/2310.12616</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sidi Wu, Yizi Chen, Konrad Schindler, Lorenz Hurni</li>
<li>for: 这 paper 的目的是提出一种基于 U-Net 和 cross-attention transformers 的网络，用于提取历史地图中的信息。</li>
<li>methods: 这 paper 使用了 neural networks 和 U-Net 来处理历史地图，并使用 cross-attention transformers 来捕捉历史地图中的 espacio-temporal 信息。</li>
<li>results: 这 paper 的模型比其他 state-of-the-art 模型更好，可以更好地 segment 历史地图。此外，这 paper 还提出了一种将 spatial 和 temporal 上下文 fusion 到一起的方法，以提高模型的性能。<details>
<summary>Abstract</summary>
Historical maps provide useful spatio-temporal information on the Earth's surface before modern earth observation techniques came into being. To extract information from maps, neural networks, which gain wide popularity in recent years, have replaced hand-crafted map processing methods and tedious manual labor. However, aleatoric uncertainty, known as data-dependent uncertainty, inherent in the drawing/scanning/fading defects of the original map sheets and inadequate contexts when cropping maps into small tiles considering the memory limits of the training process, challenges the model to make correct predictions. As aleatoric uncertainty cannot be reduced even with more training data collected, we argue that complementary spatio-temporal contexts can be helpful. To achieve this, we propose a U-Net-based network that fuses spatio-temporal features with cross-attention transformers (U-SpaTem), aggregating information at a larger spatial range as well as through a temporal sequence of images. Our model achieves a better performance than other state-or-art models that use either temporal or spatial contexts. Compared with pure vision transformers, our model is more lightweight and effective. To the best of our knowledge, leveraging both spatial and temporal contexts have been rarely explored before in the segmentation task. Even though our application is on segmenting historical maps, we believe that the method can be transferred into other fields with similar problems like temporal sequences of satellite images. Our code is freely accessible at https://github.com/chenyizi086/wu.2023.sigspatial.git.
</details>
<details>
<summary>摘要</summary>
历史地图提供了地球表面之前的有用空间-时间信息。为提取信息从地图，人工神经网络在最近几年中得到了广泛的应用，取代了手工地图处理方法和繁琐的手动劳动。然而，抽象不确定性（数据依赖的不确定性），包括地图描述/扫描/褪色缺陷和缺乏合适的上下文，使模型预测 incorrect。由于这种抽象不确定性不能减少，我们认为可以利用 complementary 空间-时间上下文。为此，我们提出了基于 U-Net 网络的混合空间-时间特征（U-SpaTem），将空间-时间特征混合在一起，并通过带有权重的混合层来进行权重融合。我们的模型在与其他状态 искусственного神经网络（ANN）比较时表现更好，而且相比于纯视觉变换器，我们的模型更轻量级、有效。我们认为利用空间和时间上下文的组合是一项 rarely explored 的方法，即使在地图分割任务中。虽然我们的应用是在历史地图分割中，但我们认为这种方法可以被应用到其他具有相似问题的领域，如卫星图像序列中的时间序列分割。我们的代码可以免费在 GitHub 上获取：https://github.com/chenyizi086/wu.2023.sigspatial.git。
</details></li>
</ul>
<hr>
<h2 id="Blending-gradient-boosted-trees-and-neural-networks-for-point-and-probabilistic-forecasting-of-hierarchical-time-series"><a href="#Blending-gradient-boosted-trees-and-neural-networks-for-point-and-probabilistic-forecasting-of-hierarchical-time-series" class="headerlink" title="Blending gradient boosted trees and neural networks for point and probabilistic forecasting of hierarchical time series"></a>Blending gradient boosted trees and neural networks for point and probabilistic forecasting of hierarchical time series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13029">http://arxiv.org/abs/2310.13029</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/IoannisNasios/M5_Uncertainty_3rd_place">https://github.com/IoannisNasios/M5_Uncertainty_3rd_place</a></li>
<li>paper_authors: Ioannis Nasios, Konstantinos Vogklis</li>
<li>for: 这 paper targets 点和 probabilistic 预测问题，并提出了一种 combining 机器学习模型的方法ology，包括 gradient boosted trees 和 neural networks 家族。这些原则在 reciently M5 竞赛中得到了成功应用。</li>
<li>methods: 方法包括将任务转换为销售量的回归问题，rich feature engineering，构建多种 state-of-the-art 机器学习模型，并且精心选择验证集进行模型调整。</li>
<li>results: 结果显示，多样性的机器学习模型和精心选择的验证示例是效果最好的关键因素。尽管预测数据具有自然的层次结构（12 级），但我们的提posed 解决方案并未利用这种层次结构。使用提posed 方法，我们的团队在 both Accuracy 和 Uncertainty  track 中获得了金牌奖。<details>
<summary>Abstract</summary>
In this paper we tackle the problem of point and probabilistic forecasting by describing a blending methodology of machine learning models that belong to gradient boosted trees and neural networks families. These principles were successfully applied in the recent M5 Competition on both Accuracy and Uncertainty tracks. The keypoints of our methodology are: a) transform the task to regression on sales for a single day b) information rich feature engineering c) create a diverse set of state-of-the-art machine learning models and d) carefully construct validation sets for model tuning. We argue that the diversity of the machine learning models along with the careful selection of validation examples, where the most important ingredients for the effectiveness of our approach. Although forecasting data had an inherent hierarchy structure (12 levels), none of our proposed solutions exploited that hierarchical scheme. Using the proposed methodology, our team was ranked within the gold medal range in both Accuracy and the Uncertainty track. Inference code along with already trained models are available at https://github.com/IoannisNasios/M5_Uncertainty_3rd_place
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们解决了点预测和概率预测问题，描述了一种将机器学习模型组合成 gradient boosted trees 和神经网络家族的方法。这些原则在最近的 M5 竞赛中得到了成功应用，在精度和不确定性轨道上均获得了金牌。我们的方法的关键特点包括：a) 将任务转化为销售单日的回归问题b) rich feature engineeringc) 创建多种现有最佳机器学习模型d) 仔细构建验证集 для模型调整我们认为多样化的机器学习模型以及仔细选择的验证示例是我们方法的关键元素。尽管预测数据有自然的层次结构（12级），但我们所提出的解决方案并未利用该层次结构。使用我们提议的方法，我们的团队在精度和不确定性轨道上均获得了金牌。可以在 https://github.com/IoannisNasios/M5_Uncertainty_3rd_place 获取预测代码以及已训练模型。
</details></li>
</ul>
<hr>
<h2 id="Identifying-and-Adapting-Transformer-Components-Responsible-for-Gender-Bias-in-an-English-Language-Model"><a href="#Identifying-and-Adapting-Transformer-Components-Responsible-for-Gender-Bias-in-an-English-Language-Model" class="headerlink" title="Identifying and Adapting Transformer-Components Responsible for Gender Bias in an English Language Model"></a>Identifying and Adapting Transformer-Components Responsible for Gender Bias in an English Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12611">http://arxiv.org/abs/2310.12611</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iabhijith/bias-causal-analysis">https://github.com/iabhijith/bias-causal-analysis</a></li>
<li>paper_authors: Abhijith Chintam, Rahel Beloch, Willem Zuidema, Michael Hanna, Oskar van der Wal</li>
<li>for: 本研究旨在探讨如何使Language Model（LM）减少各种不良偏见，包括性别偏见。</li>
<li>methods: 本研究使用三种方法来识别Language Model组件之间的 causal 关系： causal mediation analysis、自动化电路发现和我们的新方法DiffMask+基于差异掩码。</li>
<li>results: 我们应用这些方法于GPT-2小和性别偏见问题，并使用发现的组件集来进行参数有效地 fine-tuning，以减少性别偏见而不损害通用语言模型表现。<details>
<summary>Abstract</summary>
Language models (LMs) exhibit and amplify many types of undesirable biases learned from the training data, including gender bias. However, we lack tools for effectively and efficiently changing this behavior without hurting general language modeling performance. In this paper, we study three methods for identifying causal relations between LM components and particular output: causal mediation analysis, automated circuit discovery and our novel, efficient method called DiffMask+ based on differential masking. We apply the methods to GPT-2 small and the problem of gender bias, and use the discovered sets of components to perform parameter-efficient fine-tuning for bias mitigation. Our results show significant overlap in the identified components (despite huge differences in the computational requirements of the methods) as well as success in mitigating gender bias, with less damage to general language modeling compared to full model fine-tuning. However, our work also underscores the difficulty of defining and measuring bias, and the sensitivity of causal discovery procedures to dataset choice. We hope our work can contribute to more attention for dataset development, and lead to more effective mitigation strategies for other types of bias.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Denoising-Heat-inspired-Diffusion-with-Insulators-for-Collision-Free-Motion-Planning"><a href="#Denoising-Heat-inspired-Diffusion-with-Insulators-for-Collision-Free-Motion-Planning" class="headerlink" title="Denoising Heat-inspired Diffusion with Insulators for Collision Free Motion Planning"></a>Denoising Heat-inspired Diffusion with Insulators for Collision Free Motion Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12609">http://arxiv.org/abs/2310.12609</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junwoo Chang, Hyunwoo Ryu, Jiwoo Kim, Soochul Yoo, Joohwan Seo, Nikhil Prakash, Jongeun Choi, Roberto Horowitz</li>
<li>for: 这个论文是为了解决 robotics 中的问题，特别是在对于复杂的环境下实现机器人的运动和目标追寻。</li>
<li>methods: 这个方法使用了一种新的碰撞避免的扩散函数进行训练，并在推断时同时生成可到达的目标和避免障碍的动作计划。</li>
<li>results: 试验结果显示，这个方法在多模式环境中表现更加稳定和有效，能够实现机器人对于可到达的目标的追寻，并避免障碍物的碰撞。<details>
<summary>Abstract</summary>
Diffusion models have risen as a powerful tool in robotics due to their flexibility and multi-modality. While some of these methods effectively address complex problems, they often depend heavily on inference-time obstacle detection and require additional equipment. Addressing these challenges, we present a method that, during inference time, simultaneously generates only reachable goals and plans motions that avoid obstacles, all from a single visual input. Central to our approach is the novel use of a collision-avoiding diffusion kernel for training. Through evaluations against behavior-cloning and classical diffusion models, our framework has proven its robustness. It is particularly effective in multi-modal environments, navigating toward goals and avoiding unreachable ones blocked by obstacles, while ensuring collision avoidance.
</details>
<details>
<summary>摘要</summary>
Diffusion 模型在机器人学中得到了广泛应用，因为它们具有灵活性和多模式性。虽然一些这些方法可以有效地解决复杂的问题，但它们经常依赖于运行时检测障碍物并需要额外设备。为解决这些挑战，我们提出了一种方法，即在运行时同时生成可达的目标和避免障碍的动作计划，从单一的视觉输入中进行训练。我们的方法的核心是使用避免碰撞的扩散核心进行训练。经过对行为假写和古典扩散模型的评估，我们的框架在多模式环境中表现特别有效，能够寻求目标并避免不可达的目标，同时确保碰撞避免。
</details></li>
</ul>
<hr>
<h2 id="Time-Aware-Representation-Learning-for-Time-Sensitive-Question-Answering"><a href="#Time-Aware-Representation-Learning-for-Time-Sensitive-Question-Answering" class="headerlink" title="Time-Aware Representation Learning for Time-Sensitive Question Answering"></a>Time-Aware Representation Learning for Time-Sensitive Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12585">http://arxiv.org/abs/2310.12585</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sonjbin/tcqa">https://github.com/sonjbin/tcqa</a></li>
<li>paper_authors: Jungbin Son, Alice Oh</li>
<li>for: 提高语言模型理解时间关系和数字之间的关系，以解决现实世界问答问题中的时间因素。</li>
<li>methods: 提议一种时间上下文相互作用Question Answering（TCQA）框架，包括时间上下文依赖的Span Extraction（TCSE）任务，以及基于时间上下文的数据生成框架。</li>
<li>results: 对比基eline模型，TCQA模型在TimeQA数据集上的表现提高了8.5个F1得分。<details>
<summary>Abstract</summary>
Time is one of the crucial factors in real-world question answering (QA) problems. However, language models have difficulty understanding the relationships between time specifiers, such as 'after' and 'before', and numbers, since existing QA datasets do not include sufficient time expressions. To address this issue, we propose a Time-Context aware Question Answering (TCQA) framework. We suggest a Time-Context dependent Span Extraction (TCSE) task, and build a time-context dependent data generation framework for model training. Moreover, we present a metric to evaluate the time awareness of the QA model using TCSE. The TCSE task consists of a question and four sentence candidates classified as correct or incorrect based on time and context. The model is trained to extract the answer span from the sentence that is both correct in time and context. The model trained with TCQA outperforms baseline models up to 8.5 of the F1-score in the TimeQA dataset. Our dataset and code are available at https://github.com/sonjbin/TCQA
</details>
<details>
<summary>摘要</summary>
时间是现实世界问答（QA）问题中的一个关键因素。然而，语言模型很难理解时间specifier，如'after'和'before'，以及数字之间的关系，因为现有的QA数据集没有充足的时间表达。为解决这个问题，我们提议一种时间上下文意识Question Answering（TCQA）框架。我们建议一种基于时间上下文的Span抽取任务（TCSE），并为模型训练建立了时间上下文依赖的数据生成框架。此外，我们还提出了一种用于评估时间意识的QA模型的度量，该度量基于TCSE任务。TCSE任务包括一个问题和四个句子选择器，每个选择器根据时间和上下文被分为正确或错误。模型需要从正确的时间和上下文中提取答案Span。与基eline模型相比，使用TCQA训练的模型在TimeQA数据集上的F1得分提高了8.5。我们的数据集和代码可以在https://github.com/sonjbin/TCQA上获取。
</details></li>
</ul>
<hr>
<h2 id="Pretraining-Language-Models-with-Text-Attributed-Heterogeneous-Graphs"><a href="#Pretraining-Language-Models-with-Text-Attributed-Heterogeneous-Graphs" class="headerlink" title="Pretraining Language Models with Text-Attributed Heterogeneous Graphs"></a>Pretraining Language Models with Text-Attributed Heterogeneous Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12580">http://arxiv.org/abs/2310.12580</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hope-rita/thlm">https://github.com/hope-rita/thlm</a></li>
<li>paper_authors: Tao Zou, Le Yu, Yifei Huang, Leilei Sun, Bowen Du</li>
<li>for: 本文旨在提高语言模型（LM）的预训练方法，以更好地捕捉文本各种关系网络中的 topological 和异质信息。</li>
<li>methods: 本文提出了一种新的预训练框架，其包括定义 context graph 和一种 topology-aware 预训练任务，以及一种基于文本增强策略来处理文本不平衡问题。</li>
<li>results: 实验结果表明，本文的方法在三个不同领域的 datasets 上的链接预测和节点分类任务中具有明显的优势，并且每一个设计的合理性。代码可以在 <a target="_blank" rel="noopener" href="https://github.com/Hope-Rita/THLM">https://github.com/Hope-Rita/THLM</a> 中找到。<details>
<summary>Abstract</summary>
In many real-world scenarios (e.g., academic networks, social platforms), different types of entities are not only associated with texts but also connected by various relationships, which can be abstracted as Text-Attributed Heterogeneous Graphs (TAHGs). Current pretraining tasks for Language Models (LMs) primarily focus on separately learning the textual information of each entity and overlook the crucial aspect of capturing topological connections among entities in TAHGs. In this paper, we present a new pretraining framework for LMs that explicitly considers the topological and heterogeneous information in TAHGs. Firstly, we define a context graph as neighborhoods of a target node within specific orders and propose a topology-aware pretraining task to predict nodes involved in the context graph by jointly optimizing an LM and an auxiliary heterogeneous graph neural network. Secondly, based on the observation that some nodes are text-rich while others have little text, we devise a text augmentation strategy to enrich textless nodes with their neighbors' texts for handling the imbalance issue. We conduct link prediction and node classification tasks on three datasets from various domains. Experimental results demonstrate the superiority of our approach over existing methods and the rationality of each design. Our code is available at https://github.com/Hope-Rita/THLM.
</details>
<details>
<summary>摘要</summary>
在许多实际场景（如学术网络、社交平台），不同类型的实体不仅与文本相关，还之间存在多种关系，可以抽象为文本拥有hetogeneous图（TAHG）。现有的语言模型（LM）预训练任务主要关注每个实体的文本信息，忽略了捕捉TAHG中实体之间的 topological和多样化信息的重要性。在这篇论文中，我们提出了一种新的预训练框架 дляLM，其中明确考虑TAHG中实体之间的topological和多样化信息。首先，我们定义了一个上下文图，即target节点的邻居在特定顺序中的 neighborhood，并提出了一种 topology-aware预训练任务，用于预测target节点的上下文图中的节点。其次，根据发现一些节点有多少文本信息，而其他节点几乎没有文本信息的观察，我们提出了一种文本扩充策略，用于让文本缺乏节点通过与其他节点的文本信息进行扩充。我们在三个不同领域的 dataset上进行了链接预测和节点分类任务。实验结果表明，我们的方法比现有方法更有优势，并且每一个设计的合理性。我们的代码可以在https://github.com/Hope-Rita/THLM中找到。
</details></li>
</ul>
<hr>
<h2 id="Safety-Gymnasium-A-Unified-Safe-Reinforcement-Learning-Benchmark"><a href="#Safety-Gymnasium-A-Unified-Safe-Reinforcement-Learning-Benchmark" class="headerlink" title="Safety-Gymnasium: A Unified Safe Reinforcement Learning Benchmark"></a>Safety-Gymnasium: A Unified Safe Reinforcement Learning Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12567">http://arxiv.org/abs/2310.12567</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/PKU-Alignment/safety-gymnasium">https://github.com/PKU-Alignment/safety-gymnasium</a></li>
<li>paper_authors: Jiaming Ji, Borong Zhang, Jiayi Zhou, Xuehai Pan, Weidong Huang, Ruiyang Sun, Yiran Geng, Yifan Zhong, Juntao Dai, Yaodong Yang</li>
<li>for: 本研究旨在提供一个环境套件和一个安全政策优化算法库，以便在安全敏感 scenarios 中实现强化学习的应用。</li>
<li>methods: 本研究使用了 Safety-Gymnasium 环境套件和 Safe Policy Optimization (SafePO) 算法库，提供了多种安全强化学习算法供比较和评估。</li>
<li>results: 本研究将提供一个全面的安全性评估工具，以促进强化学习在安全敏感 scenarios 中的应用。<details>
<summary>Abstract</summary>
Artificial intelligence (AI) systems possess significant potential to drive societal progress. However, their deployment often faces obstacles due to substantial safety concerns. Safe reinforcement learning (SafeRL) emerges as a solution to optimize policies while simultaneously adhering to multiple constraints, thereby addressing the challenge of integrating reinforcement learning in safety-critical scenarios. In this paper, we present an environment suite called Safety-Gymnasium, which encompasses safety-critical tasks in both single and multi-agent scenarios, accepting vector and vision-only input. Additionally, we offer a library of algorithms named Safe Policy Optimization (SafePO), comprising 16 state-of-the-art SafeRL algorithms. This comprehensive library can serve as a validation tool for the research community. By introducing this benchmark, we aim to facilitate the evaluation and comparison of safety performance, thus fostering the development of reinforcement learning for safer, more reliable, and responsible real-world applications. The website of this project can be accessed at https://sites.google.com/view/safety-gymnasium.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）系统具有潜在的社会进步 potential。然而，它们的部署经常遇到安全问题的阻碍。安全强化学习（SafeRL）作为一种解决方案，可以同时优化策略并遵循多个约束，以解决在安全关键场景中应用强化学习的挑战。在这篇论文中，我们介绍了一个名为安全健身房（Safety-Gymnasium）的环境集合，包括单机和多机场景下的安全关键任务，接受 вектор和视觉输入。此外，我们还提供了一个名为安全策略优化（SafePO）的库，包含16种当前最佳的SafeRL算法。这个全面的库可以作为一个验证工具，以便研究人员对这些算法进行评估和比较。通过推出这个标准，我们希望能够促进强化学习在实际应用中的安全性、可靠性和责任性的发展。相关项目的网站地址为 <https://sites.google.com/view/safety-gymnasium>。
</details></li>
</ul>
<hr>
<h2 id="DepWiGNN-A-Depth-wise-Graph-Neural-Network-for-Multi-hop-Spatial-Reasoning-in-Text"><a href="#DepWiGNN-A-Depth-wise-Graph-Neural-Network-for-Multi-hop-Spatial-Reasoning-in-Text" class="headerlink" title="DepWiGNN: A Depth-wise Graph Neural Network for Multi-hop Spatial Reasoning in Text"></a>DepWiGNN: A Depth-wise Graph Neural Network for Multi-hop Spatial Reasoning in Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12557">http://arxiv.org/abs/2310.12557</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/syon-li/depwignn">https://github.com/syon-li/depwignn</a></li>
<li>paper_authors: Shuaiyi Li, Yang Deng, Wai Lam</li>
<li>for: 本研究旨在提高图像中的空间理解能力，以便在各种应用场景中进行更好的决策。</li>
<li>methods: 我们提出了一种新的深度智能图 neural network（DepWiGNN），其特点在于在图像中采用深度维度进行信息汇集，以避免过拟合问题。</li>
<li>results: 实验结果表明，Compared with现有的空间理解方法，DepWiGNN在两个复杂的多趟空间理解数据集上表现出色，并且在捕捉长距离依赖关系方面具有优势。<details>
<summary>Abstract</summary>
Spatial reasoning in text plays a crucial role in various real-world applications. Existing approaches for spatial reasoning typically infer spatial relations from pure text, which overlook the gap between natural language and symbolic structures. Graph neural networks (GNNs) have showcased exceptional proficiency in inducing and aggregating symbolic structures. However, classical GNNs face challenges in handling multi-hop spatial reasoning due to the over-smoothing issue, \textit{i.e.}, the performance decreases substantially as the number of graph layers increases. To cope with these challenges, we propose a novel \textbf{Dep}th-\textbf{Wi}se \textbf{G}raph \textbf{N}eural \textbf{N}etwork (\textbf{DepWiGNN}). Specifically, we design a novel node memory scheme and aggregate the information over the depth dimension instead of the breadth dimension of the graph, which empowers the ability to collect long dependencies without stacking multiple layers. Experimental results on two challenging multi-hop spatial reasoning datasets show that DepWiGNN outperforms existing spatial reasoning methods. The comparisons with the other three GNNs further demonstrate its superiority in capturing long dependency in the graph.
</details>
<details>
<summary>摘要</summary>
文本中的空间理解在各种实际应用中发挥关键作用。现有的空间理解方法通常从纯文本中推断空间关系，忽略自然语言和符号结构之间的差异。图 neural network（GNN）已经表现出了exceptional的能力 inducting和聚合符号结构。然而，经典GNN难以处理多跳空间理解，因为over-smoothing问题，即在图层数增加时性能减退较大。为了解决这些挑战，我们提出了一种新的Depth-Wise Graph Neural Network（DepWiGNN）。具体来说，我们设计了一种新的节点储存方案，并在图的深度维度上聚合信息，而不是在图的宽度维度上，这使得我们可以不堆叠多层来收集长距离依赖关系。实验结果表明，DepWiGNN在两个复杂的多跳空间理解 dataset 上表现出了比较出色的表现，并且与其他三种 GNN 进行比较，具体来说，DepWiGNN 能够更好地捕捉图中的长距离依赖关系。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Model-for-Multi-objective-Evolutionary-Optimization"><a href="#Large-Language-Model-for-Multi-objective-Evolutionary-Optimization" class="headerlink" title="Large Language Model for Multi-objective Evolutionary Optimization"></a>Large Language Model for Multi-objective Evolutionary Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12541">http://arxiv.org/abs/2310.12541</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fei Liu, Xi Lin, Zhenkun Wang, Shunyu Yao, Xialiang Tong, Mingxuan Yuan, Qingfu Zhang</li>
<li>for: 解决多目标优化问题 (Multi-Objective Optimization Problems, MOPs)</li>
<li>methods: 使用语言模型 (Large Language Model, LLM) 设计多目标演化算法 (Multi-Objective Evolutionary Algorithm, MOEA) Operator</li>
<li>results: 在不同测试 benchmark 上实现竞争性的表现，并且Operator 只需学习从一些实例就能够在未经见过的问题上有 robust 的一致性表现。<details>
<summary>Abstract</summary>
Multiobjective evolutionary algorithms (MOEAs) are major methods for solving multiobjective optimization problems (MOPs). Many MOEAs have been proposed in the past decades, of which the search operators need a carefully handcrafted design with domain knowledge. Recently, some attempts have been made to replace the manually designed operators in MOEAs with learning-based operators (e.g., neural network models). However, much effort is still required for designing and training such models, and the learned operators might not generalize well on new problems. To tackle the above challenges, this work investigates a novel approach that leverages the powerful large language model (LLM) to design MOEA operators. With proper prompt engineering, we successfully let a general LLM serve as a black-box search operator for decomposition-based MOEA (MOEA/D) in a zero-shot manner. In addition, by learning from the LLM behavior, we further design an explicit white-box operator with randomness and propose a new version of decomposition-based MOEA, termed MOEA/D-LO. Experimental studies on different test benchmarks show that our proposed method can achieve competitive performance with widely used MOEAs. It is also promising to see the operator only learned from a few instances can have robust generalization performance on unseen problems with quite different patterns and settings. The results reveal the potential benefits of using pre-trained LLMs in the design of MOEAs.
</details>
<details>
<summary>摘要</summary>
多目标演化算法（MOEA）是多目标优化问题（MOP）的主要解决方法。过去几十年内，许多MOEA的搜索运算需要手动设计，需要域知识。现在，一些尝试将MOEA中的手动设计 replaced with学习基于神经网络模型（e.g., neural network models）。然而，设计和训练这些模型需要很多努力，并且学习的运算可能无法在新的问题上generalize well。为了解决以上挑战，本研究提出了一种新的方法，利用强大的大语言模型（LLM）来设计MOEA运算。通过适当的提示工程，我们成功地让一个通用的LLM服务器为MOEA/D中的黑盒搜索运算。此外，通过学习LLM的行为，我们进一步设计了一个显式的白盒运算，并提出了一新的MOEA/D版本，称为MOEA/D-LO。实验研究在不同的测试准则上表明，我们的提posed方法可以与常用的MOEA具有竞争性的性能。此外，我们发现只需从一些实例学习，Operator就可以在未看过问题上display robust generalization性。结果表明，使用预训练的LLM可以在MOEA的设计中带来潜在的优势。
</details></li>
</ul>
<hr>
<h2 id="Reliable-Academic-Conference-Question-Answering-A-Study-Based-on-Large-Language-Model"><a href="#Reliable-Academic-Conference-Question-Answering-A-Study-Based-on-Large-Language-Model" class="headerlink" title="Reliable Academic Conference Question Answering: A Study Based on Large Language Model"></a>Reliable Academic Conference Question Answering: A Study Based on Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13028">http://arxiv.org/abs/2310.13028</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiwei Huang, Long Jin, Junjie Wang, Mingchen Tu, Yin Hua, Zhiqiang Liu, Jiawei Meng, Huajun Chen, Wen Zhang</li>
<li>for: 这研究的目的是提出一种基于大语言模型的学术会议问答系统，以便快速地回答研究人员关于学术会议的各种问题。</li>
<li>methods: 这种系统使用了一种组合自动和手动的方法，首先将学术会议数据组织成一种半结构化JSON格式，然后为每个会议注释了约100个问题答对。每个对被分为四个维度，并且手动注释了每个答案的来源。</li>
<li>results: 该研究表明，通过采用结构意识的检索方法，可以增强大语言模型的问答能力，并且对学术会议问答 task 进行了验证。<details>
<summary>Abstract</summary>
The rapid growth of computer science has led to a proliferation of research presented at academic conferences, fostering global scholarly communication. Researchers consistently seek accurate, current information about these events at all stages. This data surge necessitates an intelligent question-answering system to efficiently address researchers' queries and ensure awareness of the latest advancements. The information of conferences is usually published on their official website, organized in a semi-structured way with a lot of text. To address this need, we have developed the ConferenceQA dataset for 7 diverse academic conferences with human annotations. Firstly, we employ a combination of manual and automated methods to organize academic conference data in a semi-structured JSON format. Subsequently, we annotate nearly 100 question-answer pairs for each conference. Each pair is classified into four different dimensions. To ensure the reliability of the data, we manually annotate the source of each answer. In light of recent advancements, Large Language Models (LLMs) have demonstrated impressive performance in various NLP tasks. They have demonstrated impressive capabilities in information-seeking question answering after instruction fine-tuning, and as such, we present our conference QA study based on LLM. Due to hallucination and outdated knowledge of LLMs, we adopt retrieval based methods to enhance LLMs' question-answering abilities. We have proposed a structure-aware retrieval method, specifically designed to leverage inherent structural information during the retrieval process. Empirical validation on the ConferenceQA dataset has demonstrated the effectiveness of this method. The dataset and code are readily accessible on https://github.com/zjukg/ConferenceQA.
</details>
<details>
<summary>摘要</summary>
computer科学的快速发展导致学术会议的研究成果激增，促进了全球学术交流。研究人员通常需要准确、实时的信息关于这些活动，这些数据涌入使得需要一个智能问答系统来有效地回答研究人员的问题，以确保对最新的发展进行了了解。会议信息通常发布在官方网站上，排序方式半结构化，具有大量文本。为了解决这个需求，我们开发了7个不同的学术会议的会议QA数据集，并进行了人工和自动方法来组织学术会议数据。然后，我们为每个会议annotated约100个问题答对。每对问题答被分类为四个维度。为保证数据的可靠性，我们手动标注每个答案的来源。鉴于最近的进步，大型自然语言模型（LLMs）在多种自然语言处理任务中表现出色。它们在带有指导的信息时进行问答任务也表现出色。因此，我们基于LLM进行会议QA研究。由于LLM的幻觉和过时知识，我们采用检索方法来增强LLM的问答能力。我们提出了结构意识检索方法，专门利用检索过程中的结构信息。对ConferenceQA数据集的验证表明了这种方法的有效性。数据集和代码可以在GitHub上获得：https://github.com/zjukg/ConferenceQA。
</details></li>
</ul>
<hr>
<h2 id="Be-Bayesian-by-Attachments-to-Catch-More-Uncertainty"><a href="#Be-Bayesian-by-Attachments-to-Catch-More-Uncertainty" class="headerlink" title="Be Bayesian by Attachments to Catch More Uncertainty"></a>Be Bayesian by Attachments to Catch More Uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13027">http://arxiv.org/abs/2310.13027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiyu Shen, Bin Pan, Tianyang Shi, Tao Li, Zhenwei Shi</li>
<li>for: 这个论文旨在提出一种新的泛矩条件网络（ABNN），以捕捉更多的不确定性从对外数据（OOD）中。</li>
<li>methods: 这个论文使用了一个附加结构（ABNN），它包含一个期望模组和几个分布模组。期望模组是一个深度网络，它主要关注原始任务，而分布模组则是一些小型的bayesian结构，它们serve as ABNN的附加部分，以捕捉ID和OOD数据中的不确定性。</li>
<li>results: 这个论文提出了一个 theoretically sound的ABNN模型，并进行了实验验证，与一些现有的不确定性估计方法进行比较，结果显示ABNN具有较高的uncertainty估计精度。<details>
<summary>Abstract</summary>
Bayesian Neural Networks (BNNs) have become one of the promising approaches for uncertainty estimation due to the solid theorical foundations. However, the performance of BNNs is affected by the ability of catching uncertainty. Instead of only seeking the distribution of neural network weights by in-distribution (ID) data, in this paper, we propose a new Bayesian Neural Network with an Attached structure (ABNN) to catch more uncertainty from out-of-distribution (OOD) data. We first construct a mathematical description for the uncertainty of OOD data according to the prior distribution, and then develop an attached Bayesian structure to integrate the uncertainty of OOD data into the backbone network. ABNN is composed of an expectation module and several distribution modules. The expectation module is a backbone deep network which focuses on the original task, and the distribution modules are mini Bayesian structures which serve as attachments of the backbone. In particular, the distribution modules aim at extracting the uncertainty from both ID and OOD data. We further provide theoretical analysis for the convergence of ABNN, and experimentally validate its superiority by comparing with some state-of-the-art uncertainty estimation methods Code will be made available.
</details>
<details>
<summary>摘要</summary>
权 bayesian neural networks (BNNs) 已成为一种有前途的方法 для uncertainty estimation，它的理论基础非常坚固。然而，BNNs 的性能受到捕捉 uncertainty 的能力的限制。在这篇论文中，我们提议一种新的 Bayesian Neural Network with Attached structure (ABNN)，可以更好地捕捉 OOD 数据中的 uncertainty。我们首先构造了 OOD 数据中 uncertainty 的数学描述，然后开发了一种附加结构来将 OOD 数据中的 uncertainty 集成到主要网络中。ABNN 由一个期望模块和多个分布模块组成。期望模块是一个深度网络，主要关注原始任务，而分布模块则是一些附加的 Bayesian 结构，用于提取 ID 和 OOD 数据中的 uncertainty。我们进一步提供了 ABNN 的理论分析，并通过与一些现有的 uncertainty estimation 方法进行比较，证明 ABNN 的优越性。代码将会公开。
</details></li>
</ul>
<hr>
<h2 id="Testing-the-Consistency-of-Performance-Scores-Reported-for-Binary-Classification-Problems"><a href="#Testing-the-Consistency-of-Performance-Scores-Reported-for-Binary-Classification-Problems" class="headerlink" title="Testing the Consistency of Performance Scores Reported for Binary Classification Problems"></a>Testing the Consistency of Performance Scores Reported for Binary Classification Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12527">http://arxiv.org/abs/2310.12527</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gykovacs/mlscorecheck">https://github.com/gykovacs/mlscorecheck</a></li>
<li>paper_authors: Attila Fazekas, György Kovács</li>
<li>for: 本研究旨在提高机器学习中的二分类任务评估方法的可靠性，以及检测报告性能指标的可靠性。</li>
<li>methods: 本研究使用数值方法来检测报告性能指标的可靠性，而不是基于统计学推断。</li>
<li>results: 通过三个医学应用例程，研究人员可以使用提出的方法检测报告性能指标的不一致，以保护科学领域的 integriy。<details>
<summary>Abstract</summary>
Binary classification is a fundamental task in machine learning, with applications spanning various scientific domains. Whether scientists are conducting fundamental research or refining practical applications, they typically assess and rank classification techniques based on performance metrics such as accuracy, sensitivity, and specificity. However, reported performance scores may not always serve as a reliable basis for research ranking. This can be attributed to undisclosed or unconventional practices related to cross-validation, typographical errors, and other factors. In a given experimental setup, with a specific number of positive and negative test items, most performance scores can assume specific, interrelated values. In this paper, we introduce numerical techniques to assess the consistency of reported performance scores and the assumed experimental setup. Importantly, the proposed approach does not rely on statistical inference but uses numerical methods to identify inconsistencies with certainty. Through three different applications related to medicine, we demonstrate how the proposed techniques can effectively detect inconsistencies, thereby safeguarding the integrity of research fields. To benefit the scientific community, we have made the consistency tests available in an open-source Python package.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化字符的中文。<</SYS>>机器学习中的二分类任务是基础任务之一，它在各个科学领域中有各种应用。科学家在进行基础研究或实践应用时，通常会评估和排序分类技术基于性能指标 such as 准确率、敏感度和特异度。然而，报告的性能分数可能不一定是可靠的基础 для研究排名。这可以归因于不明文或非标准的批处分配、 typographical errors 和其他因素。在给定的实验设置中，有一定数量的正例和负例测试项时，大多数性能分数会假设特定、相关的值。在这篇论文中，我们介绍了数学技术来评估报告的性能分数的一致性和假设的实验设置。importantly，我们的方法不基于统计推断，而是使用数学方法来确定不一致性。通过医学相关的三个应用，我们示例了如何使用我们的方法检测不一致性，从而保护科学领域的准确性。为了利助科学社区，我们将一致性测试公开发布为开源的 Python 包。
</details></li>
</ul>
<hr>
<h2 id="Powerset-multi-class-cross-entropy-loss-for-neural-speaker-diarization"><a href="#Powerset-multi-class-cross-entropy-loss-for-neural-speaker-diarization" class="headerlink" title="Powerset multi-class cross entropy loss for neural speaker diarization"></a>Powerset multi-class cross entropy loss for neural speaker diarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13025">http://arxiv.org/abs/2310.13025</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/frenchkrab/is2023-powerset-diarization">https://github.com/frenchkrab/is2023-powerset-diarization</a></li>
<li>paper_authors: Alexis Plaquet, Hervé Bredin</li>
<li>for: 这篇论文旨在提出一种新的Speaker diarization方法，以解决现有的多标签分类问题。</li>
<li>methods: 该方法使用了 permutation-invariant training 和 (local) 监督 EEND diarization 的组合，并对多标签分类问题进行了改进。</li>
<li>results: 经过广泛的实验，该方法在9个标准测试集上达到了较好的性能（特别是在 overlap speech 中），同时消除了探测阈值参数，从而提高了 robustness 和灵活性。<details>
<summary>Abstract</summary>
Since its introduction in 2019, the whole end-to-end neural diarization (EEND) line of work has been addressing speaker diarization as a frame-wise multi-label classification problem with permutation-invariant training. Despite EEND showing great promise, a few recent works took a step back and studied the possible combination of (local) supervised EEND diarization with (global) unsupervised clustering. Yet, these hybrid contributions did not question the original multi-label formulation. We propose to switch from multi-label (where any two speakers can be active at the same time) to powerset multi-class classification (where dedicated classes are assigned to pairs of overlapping speakers). Through extensive experiments on 9 different benchmarks, we show that this formulation leads to significantly better performance (mostly on overlapping speech) and robustness to domain mismatch, while eliminating the detection threshold hyperparameter, critical for the multi-label formulation.
</details>
<details>
<summary>摘要</summary>
自2019年引入以来，整个端到端神经 диари化（EEND）工作一直是将说话者 диари化视为一个帧内多类标签分类问题，并在训练中保持 permutation-invariant。 despite EEND表现出了很大的承诺，一些最近的工作又回退了，研究了可能的(本地)supervised EEND диари化与(全局)无监督分群的组合。然而，这些混合贡献没有质疑原始多类形式。我们提议将从多类（任何两个说话者都可以同时活跃）转换为集合类别分类（每对重叠的说话者都有专门的类别）。通过对9个benchmark进行了广泛的实验，我们显示这种形式导致了明显的性能提高（主要是在重叠speech），同时消除了多个标签分类的检测阈值参数，这是多类分类的 kritical 参数。
</details></li>
</ul>
<hr>
<h2 id="RTNH-Enhanced-4D-Radar-Object-Detection-Network-using-Combined-CFAR-based-Two-level-Preprocessing-and-Vertical-Encoding"><a href="#RTNH-Enhanced-4D-Radar-Object-Detection-Network-using-Combined-CFAR-based-Two-level-Preprocessing-and-Vertical-Encoding" class="headerlink" title="RTNH+: Enhanced 4D Radar Object Detection Network using Combined CFAR-based Two-level Preprocessing and Vertical Encoding"></a>RTNH+: Enhanced 4D Radar Object Detection Network using Combined CFAR-based Two-level Preprocessing and Vertical Encoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17659">http://arxiv.org/abs/2310.17659</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seung-Hyun Kong, Dong-Hee Paek, Sangjae Cho<br>for: 提高4D Radar对3D物体探测和相对垂直速度估计的精度methods: 提出了两种新算法：一是基于CFAR的两级预处理算法（CCTP），通过生成不同特征的两个 filtered measurement，提高输入4D Radar物体探测网络的信息含义；二是垂直编码算法（VE），有效地编码 Vertical 特征。results: RTNH+在${AP}<em>{3D}^{IoU&#x3D;0.3}$和${AP}</em>{3D}^{IoU&#x3D;0.5}$中具有10.14%和16.12%的性能提升，相比RTNH。<details>
<summary>Abstract</summary>
Four-dimensional (4D) Radar is a useful sensor for 3D object detection and the relative radial speed estimation of surrounding objects under various weather conditions. However, since Radar measurements are corrupted with invalid components such as noise, interference, and clutter, it is necessary to employ a preprocessing algorithm before the 3D object detection with neural networks. In this paper, we propose RTNH+ that is an enhanced version of RTNH, a 4D Radar object detection network, by two novel algorithms. The first algorithm is the combined constant false alarm rate (CFAR)-based two-level preprocessing (CCTP) algorithm that generates two filtered measurements of different characteristics using the same 4D Radar measurements, which can enrich the information of the input to the 4D Radar object detection network. The second is the vertical encoding (VE) algorithm that effectively encodes vertical features of the road objects from the CCTP outputs. We provide details of the RTNH+, and demonstrate that RTNH+ achieves significant performance improvement of 10.14\% in ${AP}_{3D}^{IoU=0.3}$ and 16.12\% in ${AP}_{3D}^{IoU=0.5}$ over RTNH.
</details>
<details>
<summary>摘要</summary>
四维度（4D）雷达是一种有用的感知器 для 3D 对象检测和周围对象的相对径速度估计，在不同的天气条件下。然而，由于雷达测量受到无效组成部分的干扰，如噪声、干扰和垃圾，因此需要采用预处理算法 перед 3D 对象检测用神经网络。在这篇论文中，我们提出了 RTNH+，它是 RTNH 的改进版，采用了两个新的算法。第一个算法是基于常量假阳报率（CFAR）的两级预处理算法（CCTP），它使用同一个 4D 雷达测量生成两个不同特征的 filtered measurement，以增加输入到 4D 雷达对象检测网络的信息量。第二个算法是垂直编码（VE）算法，它有效地编码了从 CCTP 输出的 vertical 特征。我们详细介绍了 RTNH+，并证明了 RTNH+ 在 ${AP}_{3D}^{IoU=0.3}$ 和 ${AP}_{3D}^{IoU=0.5}$ 中提高了10.14% 和 16.12% 相对于 RTNH。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Hallucination-Assessment-for-Aligned-Large-Language-Models-via-Transferable-Adversarial-Attacks"><a href="#Automatic-Hallucination-Assessment-for-Aligned-Large-Language-Models-via-Transferable-Adversarial-Attacks" class="headerlink" title="Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks"></a>Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12516">http://arxiv.org/abs/2310.12516</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaodong Yu, Hao Cheng, Xiaodong Liu, Dan Roth, Jianfeng Gao</li>
<li>for: This paper aims to develop a method of automatically generating evaluation data for large language models (LLMs) to measure their reliability and detect hallucinations.</li>
<li>methods: The proposed method, called AutoDebug, uses prompting chaining to generate transferable adversarial attacks in the form of question-answering examples. These examples are designed to trigger hallucination behaviors in LLMs.</li>
<li>results: The paper evaluates the effectiveness of AutoDebug using two variants of the Natural Questions (NQ) dataset and a collection of open-source and proprietary LLMs. The results show that LLMs are likely to hallucinate in certain question-answering scenarios, and the adversarial examples generated by AutoDebug are transferable across all considered LLMs.<details>
<summary>Abstract</summary>
Although remarkable progress has been achieved in preventing large language model (LLM) hallucinations using instruction tuning and retrieval augmentation, it remains challenging to measure the reliability of LLMs using human-crafted evaluation data which is not available for many tasks and domains and could suffer from data leakage. Inspired by adversarial machine learning, this paper aims to develop a method of automatically generating evaluation data by appropriately modifying existing data on which LLMs behave faithfully. Specifically, this paper presents AutoDebug, an LLM-based framework to use prompting chaining to generate transferable adversarial attacks in the form of question-answering examples. We seek to understand the extent to which these examples trigger the hallucination behaviors of LLMs.   We implement AutoDebug using ChatGPT and evaluate the resulting two variants of a popular open-domain question-answering dataset, Natural Questions (NQ), on a collection of open-source and proprietary LLMs under various prompting settings. Our generated evaluation data is human-readable and, as we show, humans can answer these modified questions well. Nevertheless, we observe pronounced accuracy drops across multiple LLMs including GPT-4. Our experimental results show that LLMs are likely to hallucinate in two categories of question-answering scenarios where (1) there are conflicts between knowledge given in the prompt and their parametric knowledge, or (2) the knowledge expressed in the prompt is complex. Finally, we find that the adversarial examples generated by our method are transferable across all considered LLMs. The examples generated by a small model can be used to debug a much larger model, making our approach cost-effective.
</details>
<details>
<summary>摘要</summary>
尽管在防止大语言模型（LLM）幻 apparition 中使用 instrucion tuning 和 Retrieval Augmentation 进行了很大的进步，但仍然困难量测试 LLM 的可靠性使用人工制作的评估数据，因为这些数据可能不可得到多个任务和领域，并且可能会uffer from data leakage。受到对抗机器学习的启发，本文提出了一种自动生成评估数据的方法，通过修改 LLM 在 faithful 的数据上进行应ropriate 的修改来生成 transferred adversarial attacks。我们想要了解这些例子在 LLM 中引发幻 apparition 行为的程度。我们使用 ChatGPT 实现 AutoDebug 框架，并使用 prompting chaining 技术生成了一些可读性好的评估数据。我们对一些常用的开放领域问答任务 Natural Questions (NQ) 进行了两种变体的评估，并在多个开源和商业 LLM 上进行了多种 prompting 设置的测试。我们发现，这些修改后的问题可以由人类回答，但 LLM 却表现出了明显的准确率下降。我们的实验结果表明，LLM 在某些问题解决方案中会出现幻 apparition 行为，包括知识与 parametric 知识之间的冲突以及复杂的知识表达。最后，我们发现生成的 adversarial examples 是可 пере移的，可以在所有考虑的 LLM 上使用。此外，我们发现小型模型生成的例子可以用来调试大型模型，这使得我们的方法成本低廉。
</details></li>
</ul>
<hr>
<h2 id="Towards-Anytime-Fine-tuning-Continually-Pre-trained-Language-Models-with-Hypernetwork-Prompt"><a href="#Towards-Anytime-Fine-tuning-Continually-Pre-trained-Language-Models-with-Hypernetwork-Prompt" class="headerlink" title="Towards Anytime Fine-tuning: Continually Pre-trained Language Models with Hypernetwork Prompt"></a>Towards Anytime Fine-tuning: Continually Pre-trained Language Models with Hypernetwork Prompt</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13024">http://arxiv.org/abs/2310.13024</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gangwjiang/hprompt-cpt">https://github.com/gangwjiang/hprompt-cpt</a></li>
<li>paper_authors: Gangwei Jiang, Caigao Jiang, Siqiao Xue, James Y. Zhang, Jun Zhou, Defu Lian, Ying Wei</li>
<li>for: 本研究旨在探讨 continual pre-training 的有效性，以适应快速发展的世界中多个领域和任务。</li>
<li>methods: 我们使用了一种提前学习方法，并通过协调和不协调的损失函数来训练一个 hypernetwork，以生成域pecific的提示。提示可以降低域标识，并且促进了域之间的知识传递。</li>
<li>results: 我们在两个真实世界 dataset 上进行了实验，并获得了3.57%和3.4%的提高，证明了我们的方法的有效性。<details>
<summary>Abstract</summary>
Continual pre-training has been urgent for adapting a pre-trained model to a multitude of domains and tasks in the fast-evolving world. In practice, a continually pre-trained model is expected to demonstrate not only greater capacity when fine-tuned on pre-trained domains but also a non-decreasing performance on unseen ones. In this work, we first investigate such anytime fine-tuning effectiveness of existing continual pre-training approaches, concluding with unanimously decreased performance on unseen domains. To this end, we propose a prompt-guided continual pre-training method, where we train a hypernetwork to generate domain-specific prompts by both agreement and disagreement losses. The agreement loss maximally preserves the generalization of a pre-trained model to new domains, and the disagreement one guards the exclusiveness of the generated hidden states for each domain. Remarkably, prompts by the hypernetwork alleviate the domain identity when fine-tuning and promote knowledge transfer across domains. Our method achieved improvements of 3.57% and 3.4% on two real-world datasets (including domain shift and temporal shift), respectively, demonstrating its efficacy.
</details>
<details>
<summary>摘要</summary>
To address this issue, we propose a prompt-guided continual pre-training method. Our approach involves training a hypernetwork to generate domain-specific prompts using both agreement and disagreement losses. The agreement loss ensures that the generated prompts do not deviate from the original input, thereby preserving the generalization of the pre-trained model to new domains. The disagreement loss encourages the generated prompts to be unique and exclusive for each domain, preventing the model from overfitting to a single domain.Remarkably, the prompts generated by the hypernetwork help to alleviate the domain shift problem when fine-tuning and promote knowledge transfer across domains. Our method achieved improvements of 3.57% and 3.4% on two real-world datasets (including domain shift and temporal shift), demonstrating its effectiveness.
</details></li>
</ul>
<hr>
<h2 id="GraphGPT-Graph-Instruction-Tuning-for-Large-Language-Models"><a href="#GraphGPT-Graph-Instruction-Tuning-for-Large-Language-Models" class="headerlink" title="GraphGPT: Graph Instruction Tuning for Large Language Models"></a>GraphGPT: Graph Instruction Tuning for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13023">http://arxiv.org/abs/2310.13023</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HKUDS/GraphGPT">https://github.com/HKUDS/GraphGPT</a></li>
<li>paper_authors: Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, Chao Huang</li>
<li>for: 提高图模型的通用化能力，在零shot学习场景下实现高通用性。</li>
<li>methods: 提出了一种基于大语言模型（LLMs）的图模型框架，即图GPT框架，通过文本图关联组件和双Stage指令调整方法来帮助LLMs更好地理解图结构和适应不同下游任务。</li>
<li>results: 通过对超级vised和零shot图学习任务进行评估，表明了我们的框架在不同下游任务中的优于state-of-the-art基elines。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have advanced graph structure understanding via recursive information exchange and aggregation among graph nodes. To improve model robustness, self-supervised learning (SSL) has emerged as a promising approach for data augmentation. However, existing methods for generating pre-trained graph embeddings often rely on fine-tuning with specific downstream task labels, which limits their usability in scenarios where labeled data is scarce or unavailable. To address this, our research focuses on advancing the generalization capabilities of graph models in challenging zero-shot learning scenarios. Inspired by the success of large language models (LLMs), we aim to develop a graph-oriented LLM that can achieve high generalization across diverse downstream datasets and tasks, even without any information available from the downstream graph data. In this work, we present the GraphGPT framework that aligns LLMs with graph structural knowledge with a graph instruction tuning paradigm. Our framework incorporates a text-graph grounding component to establish a connection between textual information and graph structures. Additionally, we propose a dual-stage instruction tuning paradigm, accompanied by a lightweight graph-text alignment projector. This paradigm explores self-supervised graph structural signals and task-specific graph instructions, to guide LLMs in understanding complex graph structures and improving their adaptability across different downstream tasks. Our framework is evaluated on supervised and zero-shot graph learning tasks, demonstrating superior generalization and outperforming state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
GRAPH NEURAL NETWORKS (GNNs) 有助于更好地理解图结构，通过图节点之间的循环信息交换和聚合来提高模型的 robustness。然而，现有的图模型生成预训练 embeddings 方法通常依赖于特定的下游任务标签，这限制了它们在数据罕见或无标签情况下的使用性。为了解决这个问题，我们的研究集中着精力在挑战的零例学习场景中提高图模型的通用性。受到大型自然语言模型 (LLMs) 的成功所 inspirited，我们目标是开发一个可以在多种下游任务和数据上高度通用的图模型。在这个工作中，我们提出了 GraphGPT 框架，它将 LLMs 与图结构知识集成，并通过图 instrucion 优化 paradigm 来实现。我们的框架包括一个文本-图结合组件，用于将文本信息与图结构相连。此外，我们还提出了一种双stage instrucion 优化 парадигмы，并附加了一个轻量级的图文对齐项目。这种 парадигмы探索了自然语言 Task 特定的图结构信号和任务特定的图 instrucion，以帮助 LLMs 理解复杂的图结构并提高其适应性。我们的框架在超级vised和零例学习图学习任务上进行了评估，并表现出了superior的通用性和超越了现有基eline。
</details></li>
</ul>
<hr>
<h2 id="SalUn-Empowering-Machine-Unlearning-via-Gradient-based-Weight-Saliency-in-Both-Image-Classification-and-Generation"><a href="#SalUn-Empowering-Machine-Unlearning-via-Gradient-based-Weight-Saliency-in-Both-Image-Classification-and-Generation" class="headerlink" title="SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation"></a>SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12508">http://arxiv.org/abs/2310.12508</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/optml-group/unlearn-saliency">https://github.com/optml-group/unlearn-saliency</a></li>
<li>paper_authors: Chongyu Fan, Jiancheng Liu, Yihua Zhang, Dennis Wei, Eric Wong, Sijia Liu</li>
<li>for: This paper focuses on the problem of machine unlearning (MU) and introduces a new method called saliency unlearning (SalUn) to address the limitations of existing MU methods.</li>
<li>methods: The SalUn method uses the concept of weight saliency to direct MU’s attention toward specific model weights, improving effectiveness and efficiency.</li>
<li>results: SalUn narrows the performance gap with exact unlearning and achieves better stability and accuracy in high-variance random data forgetting and preventing conditional diffusion models from generating harmful images.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文关注机器学习忘记（MU）问题，并提出了一种新的方法called saliency unlearning（SalUn），以解决现有MU方法的限制。</li>
<li>methods: SalUn方法利用模型重量焦点的概念，引导MU的注意力向特定的模型重量，提高效果和效率。</li>
<li>results: SalUn方法在高异常随机数据忘记中减小了与精准忘记（模型重新训练）之间的性能差距，并在预测扩散模型生成危险图像时达到了 nearly 100%的忘记精度。<details>
<summary>Abstract</summary>
With evolving data regulations, machine unlearning (MU) has become an important tool for fostering trust and safety in today's AI models. However, existing MU methods focusing on data and/or weight perspectives often grapple with limitations in unlearning accuracy, stability, and cross-domain applicability. To address these challenges, we introduce the concept of 'weight saliency' in MU, drawing parallels with input saliency in model explanation. This innovation directs MU's attention toward specific model weights rather than the entire model, improving effectiveness and efficiency. The resultant method that we call saliency unlearning (SalUn) narrows the performance gap with 'exact' unlearning (model retraining from scratch after removing the forgetting dataset). To the best of our knowledge, SalUn is the first principled MU approach adaptable enough to effectively erase the influence of forgetting data, classes, or concepts in both image classification and generation. For example, SalUn yields a stability advantage in high-variance random data forgetting, e.g., with a 0.2% gap compared to exact unlearning on the CIFAR-10 dataset. Moreover, in preventing conditional diffusion models from generating harmful images, SalUn achieves nearly 100% unlearning accuracy, outperforming current state-of-the-art baselines like Erased Stable Diffusion and Forget-Me-Not.
</details>
<details>
<summary>摘要</summary>
With evolving data regulations, machine unlearning (MU) has become an important tool for fostering trust and safety in today's AI models. However, existing MU methods focusing on data and/or weight perspectives often grapple with limitations in unlearning accuracy, stability, and cross-domain applicability. To address these challenges, we introduce the concept of 'weight saliency' in MU, drawing parallels with input saliency in model explanation. This innovation directs MU's attention toward specific model weights rather than the entire model, improving effectiveness and efficiency. The resultant method that we call saliency unlearning (SalUn) narrows the performance gap with 'exact' unlearning (model retraining from scratch after removing the forgetting dataset). To the best of our knowledge, SalUn is the first principled MU approach adaptable enough to effectively erase the influence of forgetting data, classes, or concepts in both image classification and generation. For example, SalUn yields a stability advantage in high-variance random data forgetting, e.g., with a 0.2% gap compared to exact unlearning on the CIFAR-10 dataset. Moreover, in preventing conditional diffusion models from generating harmful images, SalUn achieves nearly 100% unlearning accuracy, outperforming current state-of-the-art baselines like Erased Stable Diffusion and Forget-Me-Not.
</details></li>
</ul>
<hr>
<h2 id="Not-All-Countries-Celebrate-Thanksgiving-On-the-Cultural-Dominance-in-Large-Language-Models"><a href="#Not-All-Countries-Celebrate-Thanksgiving-On-the-Cultural-Dominance-in-Large-Language-Models" class="headerlink" title="Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in Large Language Models"></a>Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12481">http://arxiv.org/abs/2310.12481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenxuan Wang, Wenxiang Jiao, Jingyuan Huang, Ruyi Dai, Jen-tse Huang, Zhaopeng Tu, Michael R. Lyu</li>
<li>for: 本研究探讨了大语言模型（LLM）中的文化主导问题，即因训练数据主要来自英语而导致的模型偏向英语文化。</li>
<li>methods: 我们建立了一个包含具体（如假期和歌曲）和抽象（如价值观和意见）文化对象的基准。我们对一些代表性的GPT模型进行了系统性的评估，发现这些模型受到文化主导问题的影响，GPT-4最为严重，而text-davinci-003最少受到这种问题的影响。</li>
<li>results: 我们的研究强调了在开发和部署LMM时的文化主导和伦理考虑的必要性。我们示出了在模型开发和部署中使用两种简单的方法（即预训练数据更加多样化和文化意识提醒）可以有效缓解LMM中的文化主导问题。<details>
<summary>Abstract</summary>
In this paper, we identify a cultural dominance issue within large language models (LLMs) due to the predominant use of English data in model training (e.g. ChatGPT). LLMs often provide inappropriate English-culture-related answers that are not relevant to the expected culture when users ask in non-English languages. To systematically evaluate the cultural dominance issue, we build a benchmark that consists of both concrete (e.g. holidays and songs) and abstract (e.g. values and opinions) cultural objects. Empirical results show that the representative GPT models suffer from the culture dominance problem, where GPT-4 is the most affected while text-davinci-003 suffers the least from this problem. Our study emphasizes the need for critical examination of cultural dominance and ethical consideration in their development and deployment. We show two straightforward methods in model development (i.e. pretraining on more diverse data) and deployment (e.g. culture-aware prompting) can significantly mitigate the cultural dominance issue in LLMs.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们发现了大语言模型（LLM）中的文化主导问题，即因训练数据主要来自英语而导致模型提供不 relevante的英文文化答案。我们建立了一个 benchmark，包括具体的文化对象（如假日和歌曲）和抽象的文化对象（如价值和意见），以系统地评估文化主导问题。实验结果表明，代表性的 GPT 模型受到文化主导问题的影响，其中 GPT-4 最为严重，而 text-davinci-003 最少受到这个问题的影响。我们的研究强调了在开发和部署 LLM 时需要进行严格的文化主导和伦理考虑。我们展示了在模型开发和部署阶段使用更多多样化数据和文化意识提醒等两种简单方法可以减轻 LLM 中的文化主导问题。
</details></li>
</ul>
<hr>
<h2 id="GRAPE-S-Near-Real-Time-Coalition-Formation-for-Multiple-Service-Collectives"><a href="#GRAPE-S-Near-Real-Time-Coalition-Formation-for-Multiple-Service-Collectives" class="headerlink" title="GRAPE-S: Near Real-Time Coalition Formation for Multiple Service Collectives"></a>GRAPE-S: Near Real-Time Coalition Formation for Multiple Service Collectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12480">http://arxiv.org/abs/2310.12480</a></li>
<li>repo_url: None</li>
<li>paper_authors: Grace Diehl, Julie A. Adams<br>for: 这种论文是为了解决机器人集群在军事和灾害应急应用中的协会化问题，包括分配机器人到合适的任务小组。methods: 这种论文使用了GRAPE算法和服务模型，以及两种拍卖基础算法作为比较。results: 论文表明，拍卖基础算法在分布式集群中转移不好，导致过长的运行时间和低Utility解决方案。GRAPE-S和Pair-GRAPE-S算法可以在近实时内提供近似优质解决方案，并且可以支持大规模分布式集群和多种服务。<details>
<summary>Abstract</summary>
Robotic collectives for military and disaster response applications require coalition formation algorithms to partition robots into appropriate task teams. Collectives' missions will often incorporate tasks that require multiple high-level robot behaviors or services, which coalition formation must accommodate. The highly dynamic and unstructured application domains also necessitate that coalition formation algorithms produce near optimal solutions (i.e., >95% utility) in near real-time (i.e., <5 minutes) with very large collectives (i.e., hundreds of robots). No previous coalition formation algorithm satisfies these requirements. An initial evaluation found that traditional auction-based algorithms' runtimes are too long, even though the centralized simulator incorporated ideal conditions unlikely to occur in real-world deployments (i.e., synchronization across robots and perfect, instantaneous communication). The hedonic game-based GRAPE algorithm can produce solutions in near real-time, but cannot be applied to multiple service collectives. This manuscript integrates GRAPE and a services model, producing GRAPE-S and Pair-GRAPE-S. These algorithms and two auction baselines were evaluated using a centralized simulator with up to 1000 robots, and via the largest distributed coalition formation simulated evaluation to date, with up to 500 robots. The evaluations demonstrate that auctions transfer poorly to distributed collectives, resulting in excessive runtimes and low utility solutions. GRAPE-S satisfies the target domains' coalition formation requirements, producing near optimal solutions in near real-time, and Pair-GRAPE-S more than satisfies the domain requirements, producing optimal solutions in near real-time. GRAPE-S and Pair-GRAPE-S are the first algorithms demonstrated to support near real-time coalition formation for very large, distributed collectives with multiple services.
</details>
<details>
<summary>摘要</summary>
军事和灾害应急应用中的机器人集群需要联盟形成算法来将机器人分配到相应的任务团队。集群的任务经常包括多个高级机器人行为或服务，联盟形成算法必须满足这些要求。应用领域具有高度动态和无结构特点，因此联盟形成算法需要生成>95%的利用率（i.e., <5分钟内），并且可以处理数百个机器人。现有的任何一种联盟形成算法都不满足这些要求。一个初步评估发现，传统的拍卖式算法的运行时间太长，即使在中央模拟器中包含理想的条件（i.e., 机器人同步和精准、实时通信）。hedonic game-based GRAPE算法可以在近实时生成解决方案，但不能应用于多服务集群。这篇论文将GRAPE算法与服务模型集成，生成GRAPE-S和Pair-GRAPE-S。这些算法和两个拍卖基准被用中央模拟器中的1000台机器人进行评估，以及最大的分布式联盟形成模拟评估，包含500台机器人。评估结果表明，拍卖式算法在分布式集群中传递不好，导致运行时间过长，解决方案质量低。GRAPE-S满足了目标领域的联盟形成要求，在近实时内生成近似优解，而Pair-GRAPE-S更 чем满足了领域要求，在近实时内生成优解。GRAPE-S和Pair-GRAPE-S是首个在大规模、分布式集群中实现近实时联盟形成的算法。
</details></li>
</ul>
<hr>
<h2 id="An-Exploration-of-In-Context-Learning-for-Speech-Language-Model"><a href="#An-Exploration-of-In-Context-Learning-for-Speech-Language-Model" class="headerlink" title="An Exploration of In-Context Learning for Speech Language Model"></a>An Exploration of In-Context Learning for Speech Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12477">http://arxiv.org/abs/2310.12477</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/Robot-learning">https://github.com/Aryia-Behroziuan/Robot-learning</a></li>
<li>paper_authors: Ming-Hao Hsu, Kai-Wei Chang, Shang-Wen Li, Hung-yi Lee</li>
<li>for: 本研究探讨了语音自然语言处理领域中卷积学习（ICL）的可能性，以便不需要文本指导或参数修改，使语音语言模型（LM）能够快速学习和适应。</li>
<li>methods: 本研究使用了提档训练方法，使语音LM能够完成无文本指导的几拟学习。</li>
<li>results: 研究表明，通过提档训练，语音LM可以在未看到任务示例的情况下完成几拟学习，并在语音分类任务中证明了其可行性。<details>
<summary>Abstract</summary>
Ever since the development of GPT-3 in the natural language processing (NLP) field, in-context learning (ICL) has played an important role in utilizing large language models (LLMs). By presenting the LM utterance-label demonstrations at the input, the LM can accomplish few-shot learning without relying on gradient descent or requiring explicit modification of its parameters. This enables the LM to learn and adapt in a black-box manner. Despite the success of ICL in NLP, little work is exploring the possibility of ICL in speech processing. This study proposes the first exploration of ICL with a speech LM without text supervision. We first show that the current speech LM does not have the ICL capability. With the proposed warmup training, the speech LM can, therefore, perform ICL on unseen tasks. In this work, we verify the feasibility of ICL for speech LM on speech classification tasks.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese: desde el desarrollo de GPT-3 en el campo de procesamiento de lenguaje natural (NLP), la aprendizaje en contexto (ICL) ha desempeñado un papel importante en el uso de modelos de lenguaje grande (LLM). Al presentar las demostraciones de utterance-etiqueta del LM como entrada, el LM puede realizar aprendizaje de pocos tiros sin depender del descenso de gradientes o requerir modificaciones explícitas de sus parámetros. Esto permite al LM aprender y adaptarse de manera "caja negra". A pesar del éxito de ICL en NLP, poco trabajo se ha realizado en explorar la posibilidad de ICL en el procesamiento de speech. Este estudio propone la primera exploración de ICL con un modelo de speech sin supervisión textual. Primero mostramos que el actual modelo de speech no tiene la capacidad de ICL. Con la formación de warm-up propuesta, el modelo de speech puede, por lo tanto, realizar ICL en tareas no vistas. En este trabajo, verificamos la factibilidad de ICL para el modelo de speech en tareas de clasificación de speech.
</details></li>
</ul>
<hr>
<h2 id="Affective-Conversational-Agents-Understanding-Expectations-and-Personal-Influences"><a href="#Affective-Conversational-Agents-Understanding-Expectations-and-Personal-Influences" class="headerlink" title="Affective Conversational Agents: Understanding Expectations and Personal Influences"></a>Affective Conversational Agents: Understanding Expectations and Personal Influences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12459">http://arxiv.org/abs/2310.12459</a></li>
<li>repo_url: None</li>
<li>paper_authors: Javier Hernandez, Jina Suh, Judith Amores, Kael Rowan, Gonzalo Ramos, Mary Czerwinski</li>
<li>for: 这个研究的目的是调查人们对AI会话代理的情感能力的期望和偏好，以便更好地理解它们在不同应用场景中的表现和用户体验。</li>
<li>methods: 这个研究使用了745名受试者的问卷调查，以评估受试者对不同情感能力的需求和偏好。Specifically, the study assessed preferences regarding AI agents that can perceive, respond to, and simulate emotions across 32 distinct scenarios.</li>
<li>results: 研究发现，受试者对AI会话代理的情感能力的需求因应用场景而异，具体来说，受试者最偏好AI agents能够进行人际交流、提供情感支持和创造性任务。 Additionally, the study found that factors such as emotional reappraisal and personality traits influence the desired affective skills in AI agents.<details>
<summary>Abstract</summary>
The rise of AI conversational agents has broadened opportunities to enhance human capabilities across various domains. As these agents become more prevalent, it is crucial to investigate the impact of different affective abilities on their performance and user experience. In this study, we surveyed 745 respondents to understand the expectations and preferences regarding affective skills in various applications. Specifically, we assessed preferences concerning AI agents that can perceive, respond to, and simulate emotions across 32 distinct scenarios. Our results indicate a preference for scenarios that involve human interaction, emotional support, and creative tasks, with influences from factors such as emotional reappraisal and personality traits. Overall, the desired affective skills in AI agents depend largely on the application's context and nature, emphasizing the need for adaptability and context-awareness in the design of affective AI conversational agents.
</details>
<details>
<summary>摘要</summary>
人工智能对话代理的出现已经扩大了增强人类能力的机会，在不同领域。随着这些代理变得更普遍，研究对它们的表现和用户体验的影响已经变得非常重要。在这项研究中，我们询问了745名参与者，以了解他们对不同情感能力的期望和偏好。我们专门评估了参与者对情感感知、应对和模拟情感的场景中的偏好。我们的结果表明，参与者对人工交流、情感支持和创造性任务的场景有很高的偏好，这些场景的影响因素包括情感重新评估和个性特质。总之，人工智能对话代理所需的情感能力取决于应用场景的内容和性质，这推翻了设计情感AI对话代理的需要适应性和场景意识。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-the-Construction-of-Effective-Metrics-for-Understanding-the-Mechanisms-of-Pretrained-Language-Models"><a href="#Rethinking-the-Construction-of-Effective-Metrics-for-Understanding-the-Mechanisms-of-Pretrained-Language-Models" class="headerlink" title="Rethinking the Construction of Effective Metrics for Understanding the Mechanisms of Pretrained Language Models"></a>Rethinking the Construction of Effective Metrics for Understanding the Mechanisms of Pretrained Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12454">http://arxiv.org/abs/2310.12454</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cclx/effective_metrics">https://github.com/cclx/effective_metrics</a></li>
<li>paper_authors: You Li, Jinhui Yin, Yuming Lin</li>
<li>for: 本研究旨在解释BERT-like预训练语言模型的机制，并提出一种基于树Topological Probe的方法来计算这些机制。</li>
<li>methods: 本研究使用了一种基于树Topological Probe的方法来计算BERT-large模型中的各种机制，并对BERT-large模型进行了测试。</li>
<li>results: 实验结果表明，使用树Topological Probe可以提供有用的信息，并且可以帮助提高 fine-tuning 性能。此外，研究还提出了一种可能的BERT-like预训练语言模型的工作机制。<details>
<summary>Abstract</summary>
Pretrained language models are expected to effectively map input text to a set of vectors while preserving the inherent relationships within the text. Consequently, designing a white-box model to compute metrics that reflect the presence of specific internal relations in these vectors has become a common approach for post-hoc interpretability analysis of pretrained language models. However, achieving interpretability in white-box models and ensuring the rigor of metric computation becomes challenging when the source model lacks inherent interpretability. Therefore, in this paper, we discuss striking a balance in this trade-off and propose a novel line to constructing metrics for understanding the mechanisms of pretrained language models. We have specifically designed a family of metrics along this line of investigation, and the model used to compute these metrics is referred to as the tree topological probe. We conducted measurements on BERT-large by using these metrics. Based on the experimental results, we propose a speculation regarding the working mechanism of BERT-like pretrained language models, as well as a strategy for enhancing fine-tuning performance by leveraging the topological probe to improve specific submodules.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Pretrained language models are expected to effectively map input text to a set of vectors while preserving the inherent relationships within the text. Consequently, designing a white-box model to compute metrics that reflect the presence of specific internal relations in these vectors has become a common approach for post-hoc interpretability analysis of pretrained language models. However, achieving interpretability in white-box models and ensuring the rigor of metric computation becomes challenging when the source model lacks inherent interpretability. Therefore, in this paper, we discuss striking a balance in this trade-off and propose a novel line to constructing metrics for understanding the mechanisms of pretrained language models. We have specifically designed a family of metrics along this line of investigation, and the model used to compute these metrics is referred to as the tree topological probe. We conducted measurements on BERT-large by using these metrics. Based on the experimental results, we propose a speculation regarding the working mechanism of BERT-like pretrained language models, as well as a strategy for enhancing fine-tuning performance by leveraging the topological probe to improve specific submodules."中文翻译：预训言语模型预期能够有效地将输入文本映射到一组矢量，保留文本中内在关系的约束。因此，设计一个白盒模型来计算这些矢量中特定内在关系的指标，成为预训言语模型后期可读性分析的常见方法。然而，在白盒模型中实现可读性和指标计算的精准性变得困难，因为源模型缺乏内在可读性。因此，在这篇论文中，我们讨论如何平衡这种贸易，并提出了一种新的方法来构建适用于理解预训言语模型机制的指标。我们专门设计了一家指标，用于计算这些指标，并称之为树Topological probe。我们对BERT-large进行了测量，并基于实验结果，我们提出了预训言语模型工作机制的 Speculation，以及可以通过树Topological probe来提高精度调整性的策略。
</details></li>
</ul>
<hr>
<h2 id="MTS-LOF-Medical-Time-Series-Representation-Learning-via-Occlusion-Invariant-Features"><a href="#MTS-LOF-Medical-Time-Series-Representation-Learning-via-Occlusion-Invariant-Features" class="headerlink" title="MTS-LOF: Medical Time-Series Representation Learning via Occlusion-Invariant Features"></a>MTS-LOF: Medical Time-Series Representation Learning via Occlusion-Invariant Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12451">http://arxiv.org/abs/2310.12451</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/huayuliarizona/mst-lof">https://github.com/huayuliarizona/mst-lof</a></li>
<li>paper_authors: Huayu Li, Ana S. Carreon-Rascon, Xiwen Chen, Geng Yuan, Ao Li</li>
<li>for: 这 paper 是为了提高医疗数据的表示学学习，特别是针对医疗时间序数据。</li>
<li>methods: 这 paper 使用了自适应学习（SSL）和伪预测器（MAE）方法，并将其组合起来，以提高医疗应用程序的表示学学习。它还使用了多个遮盲策略，以便在不同的医疗时间序数据上学习不同的视图。</li>
<li>results: 实验结果表明，MTS-LOF 比其他方法更高效，并且可以更好地捕捉医疗时间序数据中的 Contextual information。这些结果表明，MTS-LOF 可以提高医疗应用程序的表示学学习，并且可以更好地理解医疗数据的复杂关系。<details>
<summary>Abstract</summary>
Medical time series data are indispensable in healthcare, providing critical insights for disease diagnosis, treatment planning, and patient management. The exponential growth in data complexity, driven by advanced sensor technologies, has presented challenges related to data labeling. Self-supervised learning (SSL) has emerged as a transformative approach to address these challenges, eliminating the need for extensive human annotation. In this study, we introduce a novel framework for Medical Time Series Representation Learning, known as MTS-LOF. MTS-LOF leverages the strengths of contrastive learning and Masked Autoencoder (MAE) methods, offering a unique approach to representation learning for medical time series data. By combining these techniques, MTS-LOF enhances the potential of healthcare applications by providing more sophisticated, context-rich representations. Additionally, MTS-LOF employs a multi-masking strategy to facilitate occlusion-invariant feature learning. This approach allows the model to create multiple views of the data by masking portions of it. By minimizing the discrepancy between the representations of these masked patches and the fully visible patches, MTS-LOF learns to capture rich contextual information within medical time series datasets. The results of experiments conducted on diverse medical time series datasets demonstrate the superiority of MTS-LOF over other methods. These findings hold promise for significantly enhancing healthcare applications by improving representation learning. Furthermore, our work delves into the integration of joint-embedding SSL and MAE techniques, shedding light on the intricate interplay between temporal and structural dependencies in healthcare data. This understanding is crucial, as it allows us to grasp the complexities of healthcare data analysis.
</details>
<details>
<summary>摘要</summary>
医疗时间序数据是医疗健康管理中不可或缺的，它们提供了重要的疾病诊断、治疗规划和患者管理的关键信息。随着感知技术的不断发展，医疗时间序数据的复杂性呈指数增长，这对数据标注带来了挑战。自主学习（SSL）已经成为一种解决这些挑战的transformative方法，不需要大量的人类标注。在这个研究中，我们介绍了一种新的医疗时间序表示学习框架，称为MTS-LOF。MTS-LOF利用了对比学习和Masked Autoencoder（MAE）方法的优点，提供了一种新的医疗时间序表示学习方法。通过将这些技术相结合，MTS-LOF可以为医疗应用程序提供更加复杂、上下文rich的表示。此外，MTS-LOF使用多masking策略来促进遮盲不变的特征学习。这种方法使得模型可以创建多个视图的数据。通过将这些遮盲的patches与完整可见的patches的表示差异到最小，MTS-LOF可以捕捉医疗时间序数据中的rich上下文信息。实验结果表明，MTS-LOF在多种医疗时间序数据集上的性能superior于其他方法。这些发现承诺可以大幅提高医疗应用程序的表示学习，并且我们的工作也探讨了 joint-embedding SSL和MAE技术的共同作用， shedding light on the intricate interplay between temporal and structural dependencies in healthcare data。这种理解是关键的，因为它允许我们更好地理解医疗数据分析的复杂性。
</details></li>
</ul>
<hr>
<h2 id="Know-Where-to-Go-Make-LLM-a-Relevant-Responsible-and-Trustworthy-Searcher"><a href="#Know-Where-to-Go-Make-LLM-a-Relevant-Responsible-and-Trustworthy-Searcher" class="headerlink" title="Know Where to Go: Make LLM a Relevant, Responsible, and Trustworthy Searcher"></a>Know Where to Go: Make LLM a Relevant, Responsible, and Trustworthy Searcher</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12443">http://arxiv.org/abs/2310.12443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Shi, Jiawei Liu, Yinpeng Liu, Qikai Cheng, Wei Lu</li>
<li>for: 提高 relevance 和提供直接答案的搜寻结果的可靠性和信任性。</li>
<li>methods: 提出了一个 novel 的生成搜寻框架，利用 LLM 的知识实现查询和线上资源之间的直接关联，包括 Generator、Validator 和 Optimizer 三个核心模组，每个模组专注于生成可靠的线上资源、验证来源可靠性和修正不可靠来源。</li>
<li>results: 经过广泛的实验和评估，我们的方法在不同的 SOTA 方法面进行了超过其他方法的重要性、责任性和信任性。<details>
<summary>Abstract</summary>
The advent of Large Language Models (LLMs) has shown the potential to improve relevance and provide direct answers in web searches. However, challenges arise in validating the reliability of generated results and the credibility of contributing sources, due to the limitations of traditional information retrieval algorithms and the LLM hallucination problem. Aiming to create a "PageRank" for the LLM era, we strive to transform LLM into a relevant, responsible, and trustworthy searcher. We propose a novel generative retrieval framework leveraging the knowledge of LLMs to foster a direct link between queries and online sources. This framework consists of three core modules: Generator, Validator, and Optimizer, each focusing on generating trustworthy online sources, verifying source reliability, and refining unreliable sources, respectively. Extensive experiments and evaluations highlight our method's superior relevance, responsibility, and trustfulness against various SOTA methods.
</details>
<details>
<summary>摘要</summary>
LLM时代的出现带来了提高搜索结果相关性和直接回答的潜力。然而，验证生成结果的可靠性和贡献来源的可靠性受到传统信息检索算法的限制和LLM幻觉问题的影响。我们希望通过转化LLM成为可靠、负责任和可信的搜索引擎。我们提出了一种新的生成检索框架，利用LLM的知识来建立直接连接查询和在线源。这个框架包括三个核心模块：生成器、验证器和优化器，每个模块都关注生成可靠的在线源，验证源可靠性，并且修复不可靠的源。我们的方法在多种SOTA方法的比较中显示出了更高的相关性、负责任性和可信worthiness。
</details></li>
</ul>
<hr>
<h2 id="PoisonPrompt-Backdoor-Attack-on-Prompt-based-Large-Language-Models"><a href="#PoisonPrompt-Backdoor-Attack-on-Prompt-based-Large-Language-Models" class="headerlink" title="PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models"></a>PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12439">http://arxiv.org/abs/2310.12439</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/grasses/poisonprompt">https://github.com/grasses/poisonprompt</a></li>
<li>paper_authors: Hongwei Yao, Jian Lou, Zhan Qin</li>
<li>for: 研究表示，预训练大型自然语言模型（LLM）的提示方法可以提高其下游任务的性能，但是这些方法尚未充分探讨过攻击性质的威胁。本文提出了一种新的后门攻击方法，可以成功地攻击硬件和软件提示方法的LLM。</li>
<li>methods: 本文使用的三种 популяр的提示方法是：（1）硬件提示法（Hard Prompt），（2）软件提示法（Soft Prompt），（3）混合提示法（Hybrid Prompt）。</li>
<li>results: 根据EXTENSIVE experiments中的结果，POISONPROMPT可以成功地攻击三种提示方法中的两种（硬件和软件提示法），并且可以在六个数据集和三种常用的LLM中进行可靠的攻击。这些结果表明，提示方法可以增强LLM的下游任务性能，但是同时也增加了LLM的安全风险。<details>
<summary>Abstract</summary>
Prompts have significantly improved the performance of pretrained Large Language Models (LLMs) on various downstream tasks recently, making them increasingly indispensable for a diverse range of LLM application scenarios. However, the backdoor vulnerability, a serious security threat that can maliciously alter the victim model's normal predictions, has not been sufficiently explored for prompt-based LLMs. In this paper, we present POISONPROMPT, a novel backdoor attack capable of successfully compromising both hard and soft prompt-based LLMs. We evaluate the effectiveness, fidelity, and robustness of POISONPROMPT through extensive experiments on three popular prompt methods, using six datasets and three widely used LLMs. Our findings highlight the potential security threats posed by backdoor attacks on prompt-based LLMs and emphasize the need for further research in this area.
</details>
<details>
<summary>摘要</summary>
Recently, 提示（prompts）have significantly improved the performance of pre-trained Large Language Models (LLMs) on various downstream tasks, making them increasingly indispensable for a diverse range of LLM application scenarios. However, the backdoor vulnerability, a serious security threat that can maliciously alter the victim model's normal predictions, has not been sufficiently explored for prompt-based LLMs. In this paper, we present POISONPROMPT, a novel backdoor attack capable of successfully compromising both hard and soft prompt-based LLMs. We evaluate the effectiveness, fidelity, and robustness of POISONPROMPT through extensive experiments on three popular prompt methods, using six datasets and three widely used LLMs. Our findings highlight the potential security threats posed by backdoor attacks on prompt-based LLMs and emphasize the need for further research in this area.Here's the text with traditional Chinese characters:Recently, 提示（prompts）have significantly improved the performance of pre-trained Large Language Models (LLMs) on various downstream tasks, making them increasingly indispensable for a diverse range of LLM application scenarios. However, the backdoor vulnerability, a serious security threat that can maliciously alter the victim model's normal predictions, has not been sufficiently explored for prompt-based LLMs. In this paper, we present POISONPROMPT, a novel backdoor attack capable of successfully compromising both hard and soft prompt-based LLMs. We evaluate the effectiveness, fidelity, and robustness of POISONPROMPT through extensive experiments on three popular prompt methods, using six datasets and three widely used LLMs. Our findings highlight the potential security threats posed by backdoor attacks on prompt-based LLMs and emphasize the need for further research in this area.
</details></li>
</ul>
<hr>
<h2 id="Towards-Enhanced-Local-Explainability-of-Random-Forests-a-Proximity-Based-Approach"><a href="#Towards-Enhanced-Local-Explainability-of-Random-Forests-a-Proximity-Based-Approach" class="headerlink" title="Towards Enhanced Local Explainability of Random Forests: a Proximity-Based Approach"></a>Towards Enhanced Local Explainability of Random Forests: a Proximity-Based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12428">http://arxiv.org/abs/2310.12428</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joshua Rosaler, Dhruv Desai, Bhaskarjit Sarmah, Dimitrios Vamvourellis, Deran Onay, Dhagash Mehta, Stefano Pasquali</li>
<li>for: 这个论文旨在解释随机森林模型（RF）的外样性表现，通过利用随机森林可以表示为一种可适应权重k最近邻居模型来实现。</li>
<li>methods: 这种方法使用随机森林中点之间的距离学习到的特征空间 proximity 来重写随机森林预测，将随机森林预测转化为一个权重平均的目标标签。</li>
<li>results: 这种方法可以为随机森林预测提供地方性的解释，生成对于任何模型预测的贡献，并且与已有的方法like SHAP相比，能够更好地解释随机森林模型的外样性表现。<details>
<summary>Abstract</summary>
We initiate a novel approach to explain the out of sample performance of random forest (RF) models by exploiting the fact that any RF can be formulated as an adaptive weighted K nearest-neighbors model. Specifically, we use the proximity between points in the feature space learned by the RF to re-write random forest predictions exactly as a weighted average of the target labels of training data points. This linearity facilitates a local notion of explainability of RF predictions that generates attributions for any model prediction across observations in the training set, and thereby complements established methods like SHAP, which instead generates attributions for a model prediction across dimensions of the feature space. We demonstrate this approach in the context of a bond pricing model trained on US corporate bond trades, and compare our approach to various existing approaches to model explainability.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法来解释随机森林（RF）模型的外样性表现，利用随机森林可以视为一种适应加权最近邻近模型的事实。特别是，我们使用随机森林学习到的特征空间中点的距离来重写随机森林预测为一个加权平均的target标签值。这种线性性使得我们可以在培 обу集中的观察点上生成随机森林预测的解释，并且与已有的方法 like SHAP 相结合，从而提供了一种地方性的解释随机森林预测的方法。我们在美国公司债券交易数据集上应用了这种方法，并与其他已有的解释方法进行比较。
</details></li>
</ul>
<hr>
<h2 id="MAF-Multi-Aspect-Feedback-for-Improving-Reasoning-in-Large-Language-Models"><a href="#MAF-Multi-Aspect-Feedback-for-Improving-Reasoning-in-Large-Language-Models" class="headerlink" title="MAF: Multi-Aspect Feedback for Improving Reasoning in Large Language Models"></a>MAF: Multi-Aspect Feedback for Improving Reasoning in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12426">http://arxiv.org/abs/2310.12426</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deepakn97/maf">https://github.com/deepakn97/maf</a></li>
<li>paper_authors: Deepak Nathani, David Wang, Liangming Pan, William Yang Wang</li>
<li>for: 提高自然语言理解能力</li>
<li>methods: 多方面反馈机制，包括冻结LM和外部工具，每一个模块专注于特定的错误类型</li>
<li>results: 对LM生成的理由链中多种错误进行改进，提高LM在多种逻辑任务的表现，相对提高率达20%（数学逻辑）和18%（逻辑推论）<details>
<summary>Abstract</summary>
Language Models (LMs) have shown impressive performance in various natural language tasks. However, when it comes to natural language reasoning, LMs still face challenges such as hallucination, generating incorrect intermediate reasoning steps, and making mathematical errors. Recent research has focused on enhancing LMs through self-improvement using feedback. Nevertheless, existing approaches relying on a single generic feedback source fail to address the diverse error types found in LM-generated reasoning chains. In this work, we propose Multi-Aspect Feedback, an iterative refinement framework that integrates multiple feedback modules, including frozen LMs and external tools, each focusing on a specific error category. Our experimental results demonstrate the efficacy of our approach to addressing several errors in the LM-generated reasoning chain and thus improving the overall performance of an LM in several reasoning tasks. We see a relative improvement of up to 20% in Mathematical Reasoning and up to 18% in Logical Entailment.
</details>
<details>
<summary>摘要</summary>
自然语言模型（LM）在各种自然语言任务中表现出色，但在自然语言推理方面仍面临挑战，如幻见、生成错误中间推理步骤和数学错误。现有研究集中在使LM自我改进的方法，但现有的方法均靠单一的通用反馈源，无法解决LM生成推理链中的多种错误类型。在这项工作中，我们提出了多方面反馈（Multi-Aspect Feedback），一种迭代优化框架，它将多个反馈模块、包括冻结LM和外部工具，每个模块都专注于特定错误类型。我们的实验结果表明，我们的方法可以有效地改进LM在数学推理和逻辑推理等任务中的表现，相对提高了20%以上，并在逻辑推理任务中提高了18%以上。
</details></li>
</ul>
<hr>
<h2 id="Automated-Repair-of-Declarative-Software-Specifications-in-the-Era-of-Large-Language-Models"><a href="#Automated-Repair-of-Declarative-Software-Specifications-in-the-Era-of-Large-Language-Models" class="headerlink" title="Automated Repair of Declarative Software Specifications in the Era of Large Language Models"></a>Automated Repair of Declarative Software Specifications in the Era of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12425">http://arxiv.org/abs/2310.12425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Rashedul Hasan, Jiawei Li, Iftekhar Ahmed, Hamid Bagheri</li>
<li>for: 这个研究旨在评估OpenAI的ChatGPT在Alloy声明语言中自动修复软件规范的能力。</li>
<li>methods: 该研究使用ChatGPT自动修复Alloy规范，并比较其与现有的自动修复方法的效果。</li>
<li>results: 研究发现ChatGPT可以成功修复一些其他方法无法 Address的错误，但也存在一些错误和幻见问题。<details>
<summary>Abstract</summary>
The growing adoption of declarative software specification languages, coupled with their inherent difficulty in debugging, has underscored the need for effective and automated repair techniques applicable to such languages. Researchers have recently explored various methods to automatically repair declarative software specifications, such as template-based repair, feedback-driven iterative repair, and bounded exhaustive approaches. The latest developments in large language models provide new opportunities for the automatic repair of declarative specifications. In this study, we assess the effectiveness of utilizing OpenAI's ChatGPT to repair software specifications written in the Alloy declarative language. Unlike imperative languages, specifications in Alloy are not executed but rather translated into logical formulas and evaluated using backend constraint solvers to identify specification instances and counterexamples to assertions. Our evaluation focuses on ChatGPT's ability to improve the correctness and completeness of Alloy declarative specifications through automatic repairs. We analyze the results produced by ChatGPT and compare them with those of leading automatic Alloy repair methods. Our study revealed that while ChatGPT falls short in comparison to existing techniques, it was able to successfully repair bugs that no other technique could address. Our analysis also identified errors in ChatGPT's generated repairs, including improper operator usage, type errors, higher-order logic misuse, and relational arity mismatches. Additionally, we observed instances of hallucinations in ChatGPT-generated repairs and inconsistency in its results. Our study provides valuable insights for software practitioners, researchers, and tool builders considering ChatGPT for declarative specification repairs.
</details>
<details>
<summary>摘要</summary>
随着声明式软件要求的使用越来越普遍，人们对于这类语言的自动修复技术的需求也在增长。研究人员们已经开始 explore various方法来自动修复声明式软件要求，如模板基于的修复、反馈驱动的迭代修复和约束搜索等方法。最新的大语言模型提供了新的机会 для声明式软件要求的自动修复。本研究通过使用 OpenAI 的 ChatGPT 来评估 Alloy 声明语言中的自动修复效果。不同于 imperative 语言，Alloy 的specification 不会被执行，而是被翻译成逻辑方程并通过 backend 约束解释器来评估 specification 实例和 counterexample 。我们的评估将注重 ChatGPT 在修复 Alloy 声明语言中的正确性和完整性。我们分析了 ChatGPT 生成的结果，并与主流的自动 Alloy 修复方法进行比较。我们的研究发现，虽然 ChatGPT 落后于现有的技术，但它能够成功修复一些其他技术无法处理的错误。我们的分析还发现了 ChatGPT 生成的修复结果中的错误，包括不正确的运算使用、类型错误、高阶逻辑错误和关系性质匹配错误。此外，我们还观察到了 ChatGPT 生成的修复结果中的幻见和结果不一致。本研究为软件实践人员、研究人员和工具制作人员在考虑使用 ChatGPT 进行声明式软件要求的修复提供了有价值的洞察。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-aware-Parameter-Efficient-Self-training-for-Semi-supervised-Language-Understanding"><a href="#Uncertainty-aware-Parameter-Efficient-Self-training-for-Semi-supervised-Language-Understanding" class="headerlink" title="Uncertainty-aware Parameter-Efficient Self-training for Semi-supervised Language Understanding"></a>Uncertainty-aware Parameter-Efficient Self-training for Semi-supervised Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13022">http://arxiv.org/abs/2310.13022</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wjn1996/upet">https://github.com/wjn1996/upet</a></li>
<li>paper_authors: Jianing Wang, Qiushi Sun, Nuo Chen, Chengyu Wang, Jun Huang, Ming Gao, Xiang Li</li>
<li>for: 这个 paper 的目的是提高大型预训语言模型在具有有限资源的情况下表现，以解决大型预训语言模型对于有限资源的缺乏。</li>
<li>methods: 这个 paper 使用了自教学法（SSL），具体来说是使用大量无标的数据生成模拟例子，并在教师模型中添加 Monte Carlo 抽掉以进行不确定性估计。在学生训练中，我们提出了多个参数效率学习（PEL）方法，允许仅更新一小部分参数。</li>
<li>results: 实验结果显示，UPET 可以提高表现和效率，并且可以在多个下游任务上 достичьsubstantial 的改进。<details>
<summary>Abstract</summary>
The recent success of large pre-trained language models (PLMs) heavily hinges on massive labeled data, which typically produces inferior performance in low-resource scenarios. To remedy this dilemma, we study self-training as one of the predominant semi-supervised learning (SSL) approaches, which utilizes large-scale unlabeled data to generate synthetic examples. However, too many noisy labels will hurt the model performance, and the self-training procedure requires multiple training iterations making it more expensive if all the model parameters of the PLM are updated. This paper presents UPET, a novel Uncertainty-aware Parameter-Efficient self-Training framework to effectively and efficiently address the labeled data scarcity issue. Specifically, we incorporate Monte Carlo (MC) dropout in Bayesian neural network (BNN) to perform uncertainty estimation for the teacher model and then judiciously select reliable pseudo-labeled examples based on confidence and certainty. During the student training, we introduce multiple parameter-efficient learning (PEL) paradigms that allow the optimization of only a small percentage of parameters. We also propose a novel Easy-Hard Contrastive Tuning to enhance the robustness and generalization. Extensive experiments over multiple downstream tasks demonstrate that UPET achieves a substantial improvement in terms of performance and efficiency. Our codes and data are released at https: //github.com/wjn1996/UPET.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Reducing-Uncertainty-in-Sea-level-Rise-Prediction-A-Spatial-variability-aware-Approach"><a href="#Reducing-Uncertainty-in-Sea-level-Rise-Prediction-A-Spatial-variability-aware-Approach" class="headerlink" title="Reducing Uncertainty in Sea-level Rise Prediction: A Spatial-variability-aware Approach"></a>Reducing Uncertainty in Sea-level Rise Prediction: A Spatial-variability-aware Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15179">http://arxiv.org/abs/2310.15179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Subhankar Ghosh, Shuai An, Arun Sharma, Jayant Gupta, Shashi Shekhar, Aneesh Subramanian</li>
<li>for: 这 paper 的目的是准确地预测未来海平面升高，同时降低不确定性。</li>
<li>methods: 这 paper 使用了 zonal regression 模型，解决了地域差异和模型依赖关系。</li>
<li>results: 实验结果表明，通过这 approach 在地区层次上学习 weights，可以提供更可靠的海平面升高预测。<details>
<summary>Abstract</summary>
Given multi-model ensemble climate projections, the goal is to accurately and reliably predict future sea-level rise while lowering the uncertainty. This problem is important because sea-level rise affects millions of people in coastal communities and beyond due to climate change's impacts on polar ice sheets and the ocean. This problem is challenging due to spatial variability and unknowns such as possible tipping points (e.g., collapse of Greenland or West Antarctic ice-shelf), climate feedback loops (e.g., clouds, permafrost thawing), future policy decisions, and human actions. Most existing climate modeling approaches use the same set of weights globally, during either regression or deep learning to combine different climate projections. Such approaches are inadequate when different regions require different weighting schemes for accurate and reliable sea-level rise predictions. This paper proposes a zonal regression model which addresses spatial variability and model inter-dependency. Experimental results show more reliable predictions using the weights learned via this approach on a regional scale.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:给定多模型ensemble气候预测，目标是准确地预测未来海平面上升，同时降低不确定性。这个问题非常重要，因为海平面上升会对海洋和沿海社区的人们产生深见影响，这是由于气候变化对北极和南极冰川的影响。这个问题具有空间不一致和未知因素，如可能的致命点（例如格陵兰或西安таркти亚极冰川崩塌）、气候反馈循环（例如云和冻土融化）、未来政策决策和人类行为。现有的气候模拟方法通常使用全球相同的权重，在回归或深度学习中组合不同的气候预测。这些方法无法满足不同地区需要不同权重分配的准确和可靠海平面上升预测。这篇论文提出了zonal回归模型，该模型解决了空间不一致和模型互相依赖问题。实验结果表明，通过该方法在地域级别上学习权重后，可以获得更可靠的预测结果。
</details></li>
</ul>
<hr>
<h2 id="AI-for-Mathematics-A-Cognitive-Science-Perspective"><a href="#AI-for-Mathematics-A-Cognitive-Science-Perspective" class="headerlink" title="AI for Mathematics: A Cognitive Science Perspective"></a>AI for Mathematics: A Cognitive Science Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13021">http://arxiv.org/abs/2310.13021</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Cedegao E. Zhang, Katherine M. Collins, Adrian Weller, Joshua B. Tenenbaum</li>
<li>for: This paper is written for researchers and practitioners in the field of artificial intelligence (AI) who are interested in developing automated mathematicians.</li>
<li>methods: The paper draws on cognitive science research directions to inform the development of truly human-level mathematical systems.</li>
<li>results: The paper highlights the importance of considering a multidisciplinary perspective, involving cognitive scientists, AI researchers, and mathematicians, to develop better mathematical AI systems that can push the frontier of mathematics and provide insights into human cognition.<details>
<summary>Abstract</summary>
Mathematics is one of the most powerful conceptual systems developed and used by the human species. Dreams of automated mathematicians have a storied history in artificial intelligence (AI). Rapid progress in AI, particularly propelled by advances in large language models (LLMs), has sparked renewed, widespread interest in building such systems. In this work, we reflect on these goals from a \textit{cognitive science} perspective. We call attention to several classical and ongoing research directions from cognitive science, which we believe are valuable for AI practitioners to consider when seeking to build truly human (or superhuman)-level mathematical systems. We close with open discussions and questions that we believe necessitate a multi-disciplinary perspective -- cognitive scientists working in tandem with AI researchers and mathematicians -- as we move toward better mathematical AI systems which not only help us push the frontier of the mathematics, but also offer glimpses into how we as humans are even capable of such great cognitive feats.
</details>
<details>
<summary>摘要</summary>
mathematics是人类创造并使用的一种极其强大的概念体系。自动化数学家的梦想有着悠久的历史在人工智能（AI）领域。特别是在大语言模型（LLMs）的进步下，AI的进步得到了推动，这引发了人们对于建立这种系统的再一次兴趣。在这篇文章中，我们从认知科学的角度思考这些目标。我们强调了一些经典和持续研究的方向，我们认为这些方向对于AI实践人员来说非常有价值，可以帮助建立人类（或超人）水平的数学系统。文章结束时，我们提出了一些开放的讨论和问题，我们认为需要多学科合作才能解决这些问题，即认知科学家、AI研究者和数学家在一起，以实现更好的数学AI系统，不仅能够推动数学的前沿，还能够为我们提供人类的大脑能力之间的窥视。
</details></li>
</ul>
<hr>
<h2 id="Provable-Guarantees-for-Neural-Networks-via-Gradient-Feature-Learning"><a href="#Provable-Guarantees-for-Neural-Networks-via-Gradient-Feature-Learning" class="headerlink" title="Provable Guarantees for Neural Networks via Gradient Feature Learning"></a>Provable Guarantees for Neural Networks via Gradient Feature Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12408">http://arxiv.org/abs/2310.12408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenmei Shi, Junyi Wei, Yingyu Liang</li>
<li>for: 这项研究的目的是为了解释深度学习网络的成功，并提供一种统一的分析框架。</li>
<li>methods: 这项研究使用了梯度下降来训练两层网络，并基于特征学习的原理来分析网络的学习过程。</li>
<li>results: 研究发现，两层网络在训练过程中可以学习出非线性特征，并且这种特征学习能力不仅限于特征函数的内存。此外，研究还发现了一些有趣的网络学习现象，如特征学习超出kernel的限制和抽奖假设。<details>
<summary>Abstract</summary>
Neural networks have achieved remarkable empirical performance, while the current theoretical analysis is not adequate for understanding their success, e.g., the Neural Tangent Kernel approach fails to capture their key feature learning ability, while recent analyses on feature learning are typically problem-specific. This work proposes a unified analysis framework for two-layer networks trained by gradient descent. The framework is centered around the principle of feature learning from gradients, and its effectiveness is demonstrated by applications in several prototypical problems, such as mixtures of Gaussians and parity functions. The framework also sheds light on interesting network learning phenomena such as feature learning beyond kernels and the lottery ticket hypothesis.
</details>
<details>
<summary>摘要</summary>
神经网络已经实现了非常出色的实际性能，而当前的理论分析并没有充分理解它们的成功，例如，神经折衔核方法不能捕捉它们的关键特征学习能力，而最近的特征学习分析通常是问题特定的。这个工作提出了两层网络通过梯度下降学习的统一分析框架，该框架中心在特征学习从梯度中的原则上，并通过应用于多个典型问题，如混合的高斯函数和平衡函数，证明了其效iveness。此外，该框架还暴露了神经网络学习现象，如特征学习超过折衔和抽奖假设。
</details></li>
</ul>
<hr>
<h2 id="Classification-Aided-Robust-Multiple-Target-Tracking-Using-Neural-Enhanced-Message-Passing"><a href="#Classification-Aided-Robust-Multiple-Target-Tracking-Using-Neural-Enhanced-Message-Passing" class="headerlink" title="Classification-Aided Robust Multiple Target Tracking Using Neural Enhanced Message Passing"></a>Classification-Aided Robust Multiple Target Tracking Using Neural Enhanced Message Passing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12407">http://arxiv.org/abs/2310.12407</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xianglong Bai, Zengfu Wang, Quan Pan, Tao Yun, Hua Lan</li>
<li>for: 提高强陌生环境下多个目标的跟踪稳定性，使用雷达测量信息。</li>
<li>methods: 利用范围-Doppler谱信息进行测量类别标识，以提高干扰抑制和数据关联，从而提高目标跟踪稳定性。采用神经网络增强消息传递方法，将信念输入神经网络，以提高原始信念的准确性。</li>
<li>results: 提出了一种基于神经网络增强消息传递的分类帮助稳定多目标跟踪算法，包括三个模块：消息传递模块、神经网络模块和德本-沙佛模块。该算法可以有效地抑制干扰和提高数据关联，从而在实际雷达应用中提高多目标跟踪性能。验证了该方法的有效性通过实验和实际数据场景。<details>
<summary>Abstract</summary>
We address the challenge of tracking an unknown number of targets in strong clutter environments using measurements from a radar sensor. Leveraging the range-Doppler spectra information, we identify the measurement classes, which serve as additional information to enhance clutter rejection and data association, thus bolstering the robustness of target tracking. We first introduce a novel neural enhanced message passing approach, where the beliefs obtained by the unified message passing are fed into the neural network as additional information. The output beliefs are then utilized to refine the original beliefs. Then, we propose a classification-aided robust multiple target tracking algorithm, employing the neural enhanced message passing technique. This algorithm is comprised of three modules: a message-passing module, a neural network module, and a Dempster-Shafer module. The message-passing module is used to represent the statistical model by the factor graph and infers target kinematic states, visibility states, and data associations based on the spatial measurement information. The neural network module is employed to extract features from range-Doppler spectra and derive beliefs on whether a measurement is target-generated or clutter-generated. The Dempster-Shafer module is used to fuse the beliefs obtained from both the factor graph and the neural network. As a result, our proposed algorithm adopts a model-and-data-driven framework, effectively enhancing clutter suppression and data association, leading to significant improvements in multiple target tracking performance. We validate the effectiveness of our approach using both simulated and real data scenarios, demonstrating its capability to handle challenging tracking scenarios in practical radar applications.
</details>
<details>
<summary>摘要</summary>
我们面临对未知数目目标在强杂环境中进行追踪，使用射频感知器的测量。我们利用射频 Doppler  спектrum 信息，识别测量类型，从而增强杂环排除和数据汇合，提高目标追踪的Robustness。我们首先介绍一个具有神经网络增强的讯息传递方法，其中信念由统一讯息传递获取，然后透过神经网络将信念处理，以改善原始信念。然后，我们提出一个类别帮助强化多目标追踪算法，这个算法包括三个模组：讯息传递模组、神经网络模组和德мп斯特-沙佛模组。讯息传递模组用于表示统计模型，使用因子 граhp 表示目标运动状态、可见状态和数据汇合，基于空间测量信息。神经网络模组用于从射频 Doppler  спектrum 提取特征，得出 measurement 是否为目标生成或噪音生成的信念。德мп斯特-沙佛模组用于联合这两个模组所得到的信念。因此，我们的提出的方法采用模型和数据驱动的框架，实现强化杂环排除和数据汇合，导致多目标追踪性能的明显提高。我们验证了方法的有效性通过实验和实际数据enario，证明它在实际射频应用中可以应对具有具有挑战性的追踪enario。
</details></li>
</ul>
<hr>
<h2 id="GPT-4-Doesn’t-Know-It’s-Wrong-An-Analysis-of-Iterative-Prompting-for-Reasoning-Problems"><a href="#GPT-4-Doesn’t-Know-It’s-Wrong-An-Analysis-of-Iterative-Prompting-for-Reasoning-Problems" class="headerlink" title="GPT-4 Doesn’t Know It’s Wrong: An Analysis of Iterative Prompting for Reasoning Problems"></a>GPT-4 Doesn’t Know It’s Wrong: An Analysis of Iterative Prompting for Reasoning Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12397">http://arxiv.org/abs/2310.12397</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaya Stechly, Matthew Marquez, Subbarao Kambhampati</li>
<li>for: 这 paper  investigate LLMs 的自我批判能力在 Graph Coloring 问题上。</li>
<li>methods:  authors 使用 GPT4 和 external correct reasoner 对 Graph Coloring 实例进行解决和验证。</li>
<li>results:  study 显示 LLMs 不good at solving Graph Coloring 实例，并且不能够验证自己生成的解决方案。correctness 和内容 of the criticisms 对 iterative prompting 的性能没有影响。<details>
<summary>Abstract</summary>
There has been considerable divergence of opinion on the reasoning abilities of Large Language Models (LLMs). While the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples, a wide spread belief in their iterative self-critique capabilities persists. In this paper, we set out to systematically investigate the effectiveness of iterative prompting of LLMs in the context of Graph Coloring, a canonical NP-complete reasoning problem that is related to propositional satisfiability as well as practical problems like scheduling and allocation. We present a principled empirical study of the performance of GPT4 in solving graph coloring instances or verifying the correctness of candidate colorings. In iterative modes, we experiment with the model critiquing its own answers and an external correct reasoner verifying proposed solutions. In both cases, we analyze whether the content of the criticisms actually affects bottom line performance. The study seems to indicate that (i) LLMs are bad at solving graph coloring instances (ii) they are no better at verifying a solution--and thus are not effective in iterative modes with LLMs critiquing LLM-generated solutions (iii) the correctness and content of the criticisms--whether by LLMs or external solvers--seems largely irrelevant to the performance of iterative prompting. We show that the observed increase in effectiveness is largely due to the correct solution being fortuitously present in the top-k completions of the prompt (and being recognized as such by an external verifier). Our results thus call into question claims about the self-critiquing capabilities of state of the art LLMs.
</details>
<details>
<summary>摘要</summary>
LLMs 的思维能力受到了不同的观点的争议。尽管初始的乐观情绪，思维能力会自动出现随着规模的提高，已经受到了许多对例的推翻，但对 LLMs 的循环自我批判能力仍然广泛存在信任。在这篇论文中，我们系统地 investigate LLMs 在图色置问题上的效果，这是一个NP完备的问题，与 propositional 满足性和实际问题如调度和分配有关。我们使用 GPT4 来解决图色置问题或验证候选解的正确性。在循环模式下，我们实验室LMs 自我批判其自己的答案，以及一个外部正确的解释器验证提出的解决方案。在两种情况下，我们分析了批判的内容是否实际影响循环提问的表现。研究显示：1. LLMs 解决图色置问题不善。2. LLMs 不能验证解决方案的正确性，因此循环模式下LMs 自我批判LMs 生成的解决方案无效。3. 批判的内容和正确性无关，无论是由 LLMs 或外部解释器验证。4. 观察到的效果增加主要归因于prompt中的top-k完成中包含正确解决方案，并由外部验证器认可。我们的结果质疑了现代 LLMs 的自我批判能力的声称。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/19/cs.AI_2023_10_19/" data-id="clpxp03v2005ufm88a3y3c4lv" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/23/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/22/">22</a><a class="page-number" href="/page/23/">23</a><span class="page-number current">24</span><a class="page-number" href="/page/25/">25</a><a class="page-number" href="/page/26/">26</a><span class="space">&hellip;</span><a class="page-number" href="/page/98/">98</a><a class="extend next" rel="next" href="/page/25/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">67</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">82</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">147</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
