
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/83/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.CL_2023_07_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/22/cs.CL_2023_07_22/" class="article-date">
  <time datetime="2023-07-22T11:00:00.000Z" itemprop="datePublished">2023-07-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/22/cs.CL_2023_07_22/">cs.CL - 2023-07-22</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Revisiting-Distillation-for-Continual-Learning-on-Visual-Question-Localized-Answering-in-Robotic-Surgery"><a href="#Revisiting-Distillation-for-Continual-Learning-on-Visual-Question-Localized-Answering-in-Robotic-Surgery" class="headerlink" title="Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery"></a>Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12045">http://arxiv.org/abs/2307.12045</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/longbai1006/cs-vqla">https://github.com/longbai1006/cs-vqla</a></li>
<li>paper_authors: Long Bai, Mobarakol Islam, Hongliang Ren<br>for:这篇论文旨在探讨如何使用非例子同时学习（Continual Learning，CL）方法，以提高手术教育中的知识助手系统（VQLA）的能力。methods:这篇论文提出了一种非例子CL框架，以解决深度神经网络（DNNs）在学习新知识时的忘却问题。具体来说，当DNNs学习新的类或任务时，其对于老任务的性能会下降很快。此外，由于医疗数据隐私和许可问题，通常无法访问老数据来更新CL模型。因此，该论文提出了一种具有刚性和柔性特性的CL框架，以平衡DNNs在顺序学习中的刚性和柔性。results:通过对三个公共的手术数据集进行大量的实验，该论文证明了其提出的方法可以在手术VQLA中超越传统CL方法。具体来说，该方法可以保持老任务的性能，同时学习新任务。此外，该方法还可以调整权重对于老和新任务，以适应不同的学习情况。<details>
<summary>Abstract</summary>
The visual-question localized-answering (VQLA) system can serve as a knowledgeable assistant in surgical education. Except for providing text-based answers, the VQLA system can highlight the interested region for better surgical scene understanding. However, deep neural networks (DNNs) suffer from catastrophic forgetting when learning new knowledge. Specifically, when DNNs learn on incremental classes or tasks, their performance on old tasks drops dramatically. Furthermore, due to medical data privacy and licensing issues, it is often difficult to access old data when updating continual learning (CL) models. Therefore, we develop a non-exemplar continual surgical VQLA framework, to explore and balance the rigidity-plasticity trade-off of DNNs in a sequential learning paradigm. We revisit the distillation loss in CL tasks, and propose rigidity-plasticity-aware distillation (RP-Dist) and self-calibrated heterogeneous distillation (SH-Dist) to preserve the old knowledge. The weight aligning (WA) technique is also integrated to adjust the weight bias between old and new tasks. We further establish a CL framework on three public surgical datasets in the context of surgical settings that consist of overlapping classes between old and new surgical VQLA tasks. With extensive experiments, we demonstrate that our proposed method excellently reconciles learning and forgetting on the continual surgical VQLA over conventional CL methods. Our code is publicly accessible.
</details>
<details>
<summary>摘要</summary>
Visual-问题本地回答（VQLA）系统可以作为医学教育中的知识助手。除了提供文本回答外，VQLA系统还可以高亮 interessested 区域，以便更好地理解外科场景。然而，深度神经网络（DNNs）在学习新知识时会出现慢性学习问题。具体来说，当 DNNs 学习增量类或任务时，其对于古老任务的性能会下降很快。此外，由于医学数据隐私和许可问题，通常不可以访问老数据，因此在更新 continual learning（CL）模型时困难。因此，我们开发了一个非例外 continual surgical VQLA框架，以探索和衡量 DNNs 在顺序学习中的僵化-柔软性质的权衡。我们重新评估了 CL 任务中的滥览损失，并提出了固有-柔软性感知（RP-Dist）和自适应多样化滥览（SH-Dist）来保持古老知识。此外，我们还使用了 weight aligning（WA）技术来调整新任务和古老任务之间的权重偏好。我们进一步建立了 CL 框架在三个公共外科数据集上，这些数据集在外科设置中包含了古老和新的外科 VQLA任务之间的重叠类。经过广泛的实验，我们证明了我们提出的方法在 continual surgical VQLA 中优化了学习和忘却。我们的代码公共访问。
</details></li>
</ul>
<hr>
<h2 id="FinPT-Financial-Risk-Prediction-with-Profile-Tuning-on-Pretrained-Foundation-Models"><a href="#FinPT-Financial-Risk-Prediction-with-Profile-Tuning-on-Pretrained-Foundation-Models" class="headerlink" title="FinPT: Financial Risk Prediction with Profile Tuning on Pretrained Foundation Models"></a>FinPT: Financial Risk Prediction with Profile Tuning on Pretrained Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00065">http://arxiv.org/abs/2308.00065</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuweiyin/finpt">https://github.com/yuweiyin/finpt</a></li>
<li>paper_authors: Yuwei Yin, Yazheng Yang, Jian Yang, Qi Liu</li>
<li>For: Financial risk prediction in the financial sector, specifically addressing the issues of outdated algorithms and lack of a unified benchmark.* Methods: Propose a novel approach called FinPT that leverages large pretrained foundation models and natural language processing techniques to improve financial risk prediction. FinPT fills financial tabular data into pre-defined instruction templates, obtains natural-language customer profiles by prompting LLMs, and fine-tunes large foundation models with the profile text for predictions.* Results: Demonstrate the effectiveness of FinPT by experimenting with a range of representative strong baselines on FinBench, a set of high-quality datasets on financial risks. Analytical studies also deepen the understanding of LLMs for financial risk prediction.<details>
<summary>Abstract</summary>
Financial risk prediction plays a crucial role in the financial sector. Machine learning methods have been widely applied for automatically detecting potential risks and thus saving the cost of labor. However, the development in this field is lagging behind in recent years by the following two facts: 1) the algorithms used are somewhat outdated, especially in the context of the fast advance of generative AI and large language models (LLMs); 2) the lack of a unified and open-sourced financial benchmark has impeded the related research for years. To tackle these issues, we propose FinPT and FinBench: the former is a novel approach for financial risk prediction that conduct Profile Tuning on large pretrained foundation models, and the latter is a set of high-quality datasets on financial risks such as default, fraud, and churn. In FinPT, we fill the financial tabular data into the pre-defined instruction template, obtain natural-language customer profiles by prompting LLMs, and fine-tune large foundation models with the profile text to make predictions. We demonstrate the effectiveness of the proposed FinPT by experimenting with a range of representative strong baselines on FinBench. The analytical studies further deepen the understanding of LLMs for financial risk prediction.
</details>
<details>
<summary>摘要</summary>
financial风险预测在金融领域扮演着关键的角色。机器学习方法在自动检测 potential的风险方面得到广泛的应用，从而节省劳动成本。然而，在这一领域的发展在最近几年来受到了以下两个因素的延迟：1）使用的算法有些已经过时，尤其是在生成式AI和大型自然语言模型（LLM）的快速进步的背景下；2）缺乏一个统一的、开源的金融标准 benchmark，对相关研究造成了多年的阻碍。为解决这些问题，我们提出了 FinPT 和 FinBench。前者是一种新的金融风险预测方法，通过 Profile Tuning 大型预训模型中的大型预训模型，并在 FinBench 中进行了详细的分析研究。在 FinPT 中，我们将金融表格数据填充到预定的指令模板中，通过 LLMS 提取自然语言客户profile，并使用profile文本微调大型基础模型进行预测。我们通过对 FinBench 中的多种代表强基eline进行实验，证明了我们提出的 FinPT 的有效性。分析研究还深入了解LLMS的金融风险预测能力。
</details></li>
</ul>
<hr>
<h2 id="Learning-Vision-and-Language-Navigation-from-YouTube-Videos"><a href="#Learning-Vision-and-Language-Navigation-from-YouTube-Videos" class="headerlink" title="Learning Vision-and-Language Navigation from YouTube Videos"></a>Learning Vision-and-Language Navigation from YouTube Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11984">http://arxiv.org/abs/2307.11984</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jeremylinky/youtube-vln">https://github.com/jeremylinky/youtube-vln</a></li>
<li>paper_authors: Kunyang Lin, Peihao Chen, Diwei Huang, Thomas H. Li, Mingkui Tan, Chuang Gan</li>
<li>for: 使用 YouTube 上的房屋游览视频来培养一个embodied agent，以便在真实的3D环境中使用自然语言指令进行导航。</li>
<li>methods: 创建一个大规模的数据集，其包含有理性的路径指令对从房屋游览视频中提取的，并在这个数据集上预训练 agent。</li>
<li>results: 通过使用 entropy 算法构建路径指令对，以及一个action-aware生成器来从未标注的旁路中提取指令，最终通过训练 trajectory judgment 预text task 来让 agent 挖掘到环境的布局知识，实现了在 R2R 和 REVERIE 两个标准测试benchmark上的状态级表现。<details>
<summary>Abstract</summary>
Vision-and-language navigation (VLN) requires an embodied agent to navigate in realistic 3D environments using natural language instructions. Existing VLN methods suffer from training on small-scale environments or unreasonable path-instruction datasets, limiting the generalization to unseen environments. There are massive house tour videos on YouTube, providing abundant real navigation experiences and layout information. However, these videos have not been explored for VLN before. In this paper, we propose to learn an agent from these videos by creating a large-scale dataset which comprises reasonable path-instruction pairs from house tour videos and pre-training the agent on it. To achieve this, we have to tackle the challenges of automatically constructing path-instruction pairs and exploiting real layout knowledge from raw and unlabeled videos. To address these, we first leverage an entropy-based method to construct the nodes of a path trajectory. Then, we propose an action-aware generator for generating instructions from unlabeled trajectories. Last, we devise a trajectory judgment pretext task to encourage the agent to mine the layout knowledge. Experimental results show that our method achieves state-of-the-art performance on two popular benchmarks (R2R and REVERIE). Code is available at https://github.com/JeremyLinky/YouTube-VLN
</details>
<details>
<summary>摘要</summary>
vision-and-language navigation (VLN) 需要一个具体的智能体在真实的3D环境中使用自然语言指令进行导航。现有的VLN方法受到小规模环境或不合理的路径指令数据的限制，导致对未看过的环境的泛化能力受到限制。 YouTube上有大量的房屋游览视频，这些视频提供了丰富的实际导航经验和房屋布局信息。然而，这些视频没有被前面的VLN研究所用。在这篇论文中，我们提议从这些视频中学习一个智能体，并创建了一个大规模的数据集，该数据集包括合理的路径指令对。为了实现这一点，我们首先利用一种 entropy-based 方法构建路径轨迹的节点。然后，我们提出了一种 action-aware 生成器，用于从无标签的轨迹中生成指令。最后，我们设计了一个轨迹判断预测任务，以便让智能体挖掘布局知识。实验结果表明，我们的方法在两个流行的benchmark（R2R和REVERIE）上达到了状态艺术性的表现。代码可以在 https://github.com/JeremyLinky/YouTube-VLN 上获取。
</details></li>
</ul>
<hr>
<h2 id="CARTIER-Cartographic-lAnguage-Reasoning-Targeted-at-Instruction-Execution-for-Robots"><a href="#CARTIER-Cartographic-lAnguage-Reasoning-Targeted-at-Instruction-Execution-for-Robots" class="headerlink" title="CARTIER: Cartographic lAnguage Reasoning Targeted at Instruction Execution for Robots"></a>CARTIER: Cartographic lAnguage Reasoning Targeted at Instruction Execution for Robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11865">http://arxiv.org/abs/2307.11865</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikhil Kakodkar, Dmitriy Rivkin, Bobak H. Baghi, Francois Hogan, Gregory Dudek</li>
<li>for: 这个论文探讨了大语言模型（LLM）如何解决混合语言计划和自然语言导航界面的问题。</li>
<li>methods: 该论文使用了大语言模型来解释用户在对话中提供的描述性语言查询，并在3D simulator AI2Thor中创建了复杂和可重复的场景。</li>
<li>results: 研究表明，使用大语言模型可以更好地解析用户在对话中提供的描述性语言查询，并且可以更好地理解用户的 Navigation 目标。<details>
<summary>Abstract</summary>
This work explores the capacity of large language models (LLMs) to address problems at the intersection of spatial planning and natural language interfaces for navigation.Our focus is on following relatively complex instructions that are more akin to natural conversation than traditional explicit procedural directives seen in robotics. Unlike most prior work, where navigation directives are provided as imperative commands (e.g., go to the fridge), we examine implicit directives within conversational interactions. We leverage the 3D simulator AI2Thor to create complex and repeatable scenarios at scale, and augment it by adding complex language queries for 40 object types. We demonstrate that a robot can better parse descriptive language queries than existing methods by using an LLM to interpret the user interaction in the context of a list of the objects in the scene.
</details>
<details>
<summary>摘要</summary>
To conduct our research, we utilize the 3D simulator AI2Thor to create complex and repeatable scenarios at scale, and augment it by adding complex language queries for 40 object types. Our results show that a robot can better understand and execute descriptive language queries by using an LLM to interpret the user interaction in the context of a list of objects in the scene.
</details></li>
</ul>
<hr>
<h2 id="The-Looming-Threat-of-Fake-and-LLM-generated-LinkedIn-Profiles-Challenges-and-Opportunities-for-Detection-and-Prevention"><a href="#The-Looming-Threat-of-Fake-and-LLM-generated-LinkedIn-Profiles-Challenges-and-Opportunities-for-Detection-and-Prevention" class="headerlink" title="The Looming Threat of Fake and LLM-generated LinkedIn Profiles: Challenges and Opportunities for Detection and Prevention"></a>The Looming Threat of Fake and LLM-generated LinkedIn Profiles: Challenges and Opportunities for Detection and Prevention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11864">http://arxiv.org/abs/2307.11864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navid Ayoobi, Sadat Shahriar, Arjun Mukherjee</li>
<li>for: 这个研究目的是为 LinkedIn 线上社交网络中检测伪注册和大语言模型（LLM）生成的账户，以避免伪者获取正常用户的私人资讯和推广未来的骗变活动。</li>
<li>methods: 这个研究使用 LinkedIn  Profil 中提供的文本信息，并引入 Section and Subsection Tag Embedding（SSTE）方法，以增强这些数据的归类特征，以分辨伪注册和 manually 或使用 LLM 生成的账户。</li>
<li>results: 这个研究获得了约 95% 的准确率，可以分辨伪注册和正常账户，并且显示 SSTE 在识别 LLM 生成的账户时的准确率为约 90%，即使在训练阶段没有使用 LLM 生成的账户。<details>
<summary>Abstract</summary>
In this paper, we present a novel method for detecting fake and Large Language Model (LLM)-generated profiles in the LinkedIn Online Social Network immediately upon registration and before establishing connections. Early fake profile identification is crucial to maintaining the platform's integrity since it prevents imposters from acquiring the private and sensitive information of legitimate users and from gaining an opportunity to increase their credibility for future phishing and scamming activities. This work uses textual information provided in LinkedIn profiles and introduces the Section and Subsection Tag Embedding (SSTE) method to enhance the discriminative characteristics of these data for distinguishing between legitimate profiles and those created by imposters manually or by using an LLM. Additionally, the dearth of a large publicly available LinkedIn dataset motivated us to collect 3600 LinkedIn profiles for our research. We will release our dataset publicly for research purposes. This is, to the best of our knowledge, the first large publicly available LinkedIn dataset for fake LinkedIn account detection. Within our paradigm, we assess static and contextualized word embeddings, including GloVe, Flair, BERT, and RoBERTa. We show that the suggested method can distinguish between legitimate and fake profiles with an accuracy of about 95% across all word embeddings. In addition, we show that SSTE has a promising accuracy for identifying LLM-generated profiles, despite the fact that no LLM-generated profiles were employed during the training phase, and can achieve an accuracy of approximately 90% when only 20 LLM-generated profiles are added to the training set. It is a significant finding since the proliferation of several LLMs in the near future makes it extremely challenging to design a single system that can identify profiles created with various LLMs.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的方法，用于在 LinkedIn 在线社交网络上立即识别假 profiles 和 Large Language Model（LLM）生成的 profiles，以避免骗子在获取正式用户的隐私信息和敏感信息后，进行后续的骗取和骗财活动。这种工作使用 LinkedIn profile 中提供的文本信息，并引入 Section and Subsection Tag Embedding（SSTE）方法，以增强这些数据的抑制特征，以分辨真实 profiles 和骗劫 manually 或使用 LLM 生成的 profiles。此外，由于 LinkedIn 公共可用的大型数据集缺乏，我们自己收集了 3600 个 LinkedIn profiles  для我们的研究。我们将会在研究用途上公开发布我们的数据集。这是，我们知道的， LinkedIn 上假帐户检测的首个大型公共可用数据集。在我们的 paradigm 中，我们评估了静态和Contextualized Word Embeddings，包括 GloVe、Flair、BERT 和 RoBERTa。我们发现，我们的方法可以在所有 Word Embeddings 上分辨真实 profiles 和假 profiles，准确率约为 95%。此外，我们发现 SSTE 在 LLM 生成 profiles 上具有扩展的准确率，即使在训练阶段没有使用 LLM 生成 profiles，可以达到约 90% 的准确率，只需要将 20 个 LLM 生成 profiles 添加到训练集中。这是一个重要的发现，因为未来几年 LLM 的普及会使得设计一个可以分辨多种 LLM 生成的 profiles 的系统变得极其困难。
</details></li>
</ul>
<hr>
<h2 id="MythQA-Query-Based-Large-Scale-Check-Worthy-Claim-Detection-through-Multi-Answer-Open-Domain-Question-Answering"><a href="#MythQA-Query-Based-Large-Scale-Check-Worthy-Claim-Detection-through-Multi-Answer-Open-Domain-Question-Answering" class="headerlink" title="MythQA: Query-Based Large-Scale Check-Worthy Claim Detection through Multi-Answer Open-Domain Question Answering"></a>MythQA: Query-Based Large-Scale Check-Worthy Claim Detection through Multi-Answer Open-Domain Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11848">http://arxiv.org/abs/2307.11848</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tonyby/myth-qa">https://github.com/tonyby/myth-qa</a></li>
<li>paper_authors: Yang Bai, Anthony Colas, Daisy Zhe Wang</li>
<li>for: The paper is written for detecting check-worthy claims directly from a large-scale information source, such as Twitter, to accelerate the fact-checking process.</li>
<li>methods: The paper introduces MythQA, a new multi-answer open-domain question answering task that involves contradictory stance mining for query-based large-scale check-worthy claim detection.</li>
<li>results: The paper presents a baseline system for MythQA and evaluates existing NLP models for each system component using the TweetMythQA dataset. The paper also provides initial benchmarks and identifies key challenges for future models to improve upon.Here’s the Simplified Chinese text for the three information points:</li>
<li>for: 这篇论文是为了从大规模信息源，如推特，直接检测check-worthy claim的目的。</li>
<li>methods: 这篇论文提出了一种新的多答问题回答任务，即mythQA，以推特上的矛盾立场挖掘为基础，以加速事实核查的过程。</li>
<li>results: 这篇论文提供了mythQA的基eline系统，并对现有NLP模型进行了TweetMythQA数据集上的评估。 paper还提供了初步的benchmark和未来模型改进的关键挑战。<details>
<summary>Abstract</summary>
Check-worthy claim detection aims at providing plausible misinformation to downstream fact-checking systems or human experts to check. This is a crucial step toward accelerating the fact-checking process. Many efforts have been put into how to identify check-worthy claims from a small scale of pre-collected claims, but how to efficiently detect check-worthy claims directly from a large-scale information source, such as Twitter, remains underexplored. To fill this gap, we introduce MythQA, a new multi-answer open-domain question answering(QA) task that involves contradictory stance mining for query-based large-scale check-worthy claim detection. The idea behind this is that contradictory claims are a strong indicator of misinformation that merits scrutiny by the appropriate authorities. To study this task, we construct TweetMythQA, an evaluation dataset containing 522 factoid multi-answer questions based on controversial topics. Each question is annotated with multiple answers. Moreover, we collect relevant tweets for each distinct answer, then classify them into three categories: "Supporting", "Refuting", and "Neutral". In total, we annotated 5.3K tweets. Contradictory evidence is collected for all answers in the dataset. Finally, we present a baseline system for MythQA and evaluate existing NLP models for each system component using the TweetMythQA dataset. We provide initial benchmarks and identify key challenges for future models to improve upon. Code and data are available at: https://github.com/TonyBY/Myth-QA
</details>
<details>
<summary>摘要</summary>
<<SYS>> CHECK-worthy 声明检测的目标是提供可信的谣言来供下游真伪检查系统或人类专家进行检查。这是减少真伪检查过程的关键步骤。许多努力已经投入到如何从小规模的预收集声明中Identify CHECK-worthy 声明，但如何高效地从大规模信息源，如推特，中直接检测 CHECK-worthy 声明仍未得到充分研究。为了填补这个空白，我们介绍了 MitQA，一个新的多答题开放Domain问答任务，涉及到矛盾立场挖掘，以便从推特等大规模信息源中检测 CHECK-worthy 声明。我们的想法是，矛盾的声明是谣言的强力指标，值得当局的审查。为了研究这个任务，我们构建了 TweetMythQA 评估数据集，包含 522 个多答问题，基于争议话题。每个问题有多个答案。此外，我们收集了每个问题的相关推特，然后将其分为三类：“支持”、“反对”和“中立”。总的来说，我们标注了 5.3K 条推特。为所有答案在数据集中，我们收集了矛盾证据。最后，我们提供了基线系统 для MitQA，并使用 TweetMythQA 数据集评估现有 NLP 模型的每个系统组件。我们提供了初步的标准和标识未来模型改进的关键挑战。代码和数据可以在 GitHub 上获取：https://github.com/TonyBY/Myth-QA。
</details></li>
</ul>
<hr>
<h2 id="OUTFOX-LLM-generated-Essay-Detection-through-In-context-Learning-with-Adversarially-Generated-Examples"><a href="#OUTFOX-LLM-generated-Essay-Detection-through-In-context-Learning-with-Adversarially-Generated-Examples" class="headerlink" title="OUTFOX: LLM-generated Essay Detection through In-context Learning with Adversarially Generated Examples"></a>OUTFOX: LLM-generated Essay Detection through In-context Learning with Adversarially Generated Examples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11729">http://arxiv.org/abs/2307.11729</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryuto Koike, Masahiro Kaneko, Naoaki Okazaki</li>
<li>for: 本研究旨在提高LLM生成文本检测器的Robustness，并在实际场景中评估其效果。</li>
<li>methods: 本研究提出了OUTFOX框架，该框架让检测器和攻击者都可以考虑对方的输出，并在学生作业作文中应用。</li>
<li>results: 实验结果表明，OUR proposed detector可以通过在攻击者的帮助下进行培训，提高检测性能，而OUR proposed attacker可以使检测器性能下降至-57.0点F1分。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have achieved human-level fluency in text generation, making it difficult to distinguish between human-written and LLM-generated texts. This poses a growing risk of misuse of LLMs and demands the development of detectors to identify LLM-generated texts. However, existing detectors degrade detection accuracy by simply paraphrasing LLM-generated texts. Furthermore, the effectiveness of these detectors in real-life situations, such as when students use LLMs for writing homework assignments (e.g., essays) and quickly learn how to evade these detectors, has not been explored. In this paper, we propose OUTFOX, a novel framework that improves the robustness of LLM-generated-text detectors by allowing both the detector and the attacker to consider each other's output and apply this to the domain of student essays. In our framework, the attacker uses the detector's prediction labels as examples for in-context learning and adversarially generates essays that are harder to detect. While the detector uses the adversarially generated essays as examples for in-context learning to learn to detect essays from a strong attacker. Our experiments show that our proposed detector learned in-context from the attacker improves the detection performance on the attacked dataset by up to +41.3 point F1-score. While our proposed attacker can drastically degrade the performance of the detector by up to -57.0 point F1-score compared to the paraphrasing method.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经 дости成人类水准的文本生成能力，使得区分人类写成和LLM生成的文本变得困难。这导致了LLM的滥用风险的增加，并且需要发展检测LLM生成的文本的技术。然而，现有的检测器对LLM生成的文本进行简单的重写，从而降低了检测器的准确度。此外，现有的检测器在实际情况下，例如学生使用LLM写作作业（例如论文）并快速学习如何避免这些检测器的情况下，尚未被探访。在这篇论文中，我们提出了 OUTFOX 框架，它可以提高 LLM 生成文本检测器的Robustness。在我们的框架中，攻击者使用检测器的预测标签作为内容学习的示例，并通过对检测器进行对抗式学习来生成更难以检测的文本。而检测器则使用对抗式生成的文本作为内容学习的示例，以提高检测器对于攻击者生成的文本的准确度。我们的实验显示，我们的提案的检测器在攻击 dataset 上的准确度提高了 +41.3 点 F1 分数。而我们的提案的攻击者可以对检测器造成极大的影响，比如 -57.0 点 F1 分数，相比之下，对文本进行重写方法的影响相对较小。
</details></li>
</ul>
<hr>
<h2 id="GPT-4-Can’t-Reason"><a href="#GPT-4-Can’t-Reason" class="headerlink" title="GPT-4 Can’t Reason"></a>GPT-4 Can’t Reason</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03762">http://arxiv.org/abs/2308.03762</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vohidjon123/google">https://github.com/vohidjon123/google</a></li>
<li>paper_authors: Konstantine Arkoudas</li>
<li>for: 评估 GPT-4 模型的逻辑能力</li>
<li>methods: 使用多种评估方法评估 GPT-4 模型的逻辑能力</li>
<li>results: GPT-4 模型现在不具备逻辑能力，尝试用多种方法进行评估，但它只有偶尔出现一些分析天赋。<details>
<summary>Abstract</summary>
GPT-4 was released in March 2023 to wide acclaim, marking a very substantial improvement across the board over GPT-3.5 (OpenAI's previously best model, which had powered the initial release of ChatGPT). However, despite the genuinely impressive improvement, there are good reasons to be highly skeptical of GPT-4's ability to reason. This position paper discusses the nature of reasoning; criticizes the current formulation of reasoning problems in the NLP community, as well as the way in which LLM reasoning performance is currently evaluated; introduces a small collection of 21 diverse reasoning problems; and performs a detailed qualitative evaluation of GPT-4's performance on those problems. Based on this analysis, the paper concludes that, despite its occasional flashes of analytical brilliance, GPT-4 at present is utterly incapable of reasoning.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/22/cs.CL_2023_07_22/" data-id="clp89do9x008ni788bk5o76wk" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/22/cs.LG_2023_07_22/" class="article-date">
  <time datetime="2023-07-22T10:00:00.000Z" itemprop="datePublished">2023-07-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/22/cs.LG_2023_07_22/">cs.LG - 2023-07-22</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Revolution-of-Personalized-Healthcare-Enabling-Human-Digital-Twin-with-Mobile-AIGC"><a href="#A-Revolution-of-Personalized-Healthcare-Enabling-Human-Digital-Twin-with-Mobile-AIGC" class="headerlink" title="A Revolution of Personalized Healthcare: Enabling Human Digital Twin with Mobile AIGC"></a>A Revolution of Personalized Healthcare: Enabling Human Digital Twin with Mobile AIGC</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12115">http://arxiv.org/abs/2307.12115</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayuan Chen, Changyan Yi, Hongyang Du, Dusit Niyato, Jiawen Kang, Jun Cai, Xuemin, Shen</li>
<li>for: 本研究旨在探讨 mobil AI 生成内容技术如何推动人类数字孪生（HDT）的发展，以提高个人化医疗服务。</li>
<li>methods: 本文提出了一种基于 mobil AI 生成内容技术的 HDT 系统架构，并讨论了相关的设计要求和挑战。</li>
<li>results: 本文通过两个使用场景的示例和一个实验研究证明了该方案的有效性，并提出了一些未来方向和开放问题。<details>
<summary>Abstract</summary>
Mobile Artificial Intelligence-Generated Content (AIGC) technology refers to the adoption of AI algorithms deployed at mobile edge networks to automate the information creation process while fulfilling the requirements of end users. Mobile AIGC has recently attracted phenomenal attentions and can be a key enabling technology for an emerging application, called human digital twin (HDT). HDT empowered by the mobile AIGC is expected to revolutionize the personalized healthcare by generating rare disease data, modeling high-fidelity digital twin, building versatile testbeds, and providing 24/7 customized medical services. To promote the development of this new breed of paradigm, in this article, we propose a system architecture of mobile AIGC-driven HDT and highlight the corresponding design requirements and challenges. Moreover, we illustrate two use cases, i.e., mobile AIGC-driven HDT in customized surgery planning and personalized medication. In addition, we conduct an experimental study to prove the effectiveness of the proposed mobile AIGC-driven HDT solution, which shows a particular application in a virtual physical therapy teaching platform. Finally, we conclude this article by briefly discussing several open issues and future directions.
</details>
<details>
<summary>摘要</summary>
mobile artificial intelligence生成内容（AIGC）技术指的是在移动边缘网络中部署AI算法，以自动化信息创建过程，同时满足用户的需求。 mobile AIGC 在最近受到了极高的关注，并可以是人类数字双（HDT）的关键启用技术。 HDT 通过 mobile AIGC 的 empowerment，预计将重塑个性化医疗，生成罕见疾病数据，模拟高精度数字双，建立多样化测试床，提供24/7个性化医疗服务。为推动这种新的 Paradigma 的发展，本文提出了移动 AIGC 驱动 HDT 的系统架构，并 highlighted 相应的设计要求和挑战。 此外，本文还 illustrate 了两个用例，即移动 AIGC 驱动 HDT 在定制手术规划和个性化药物。 此外，我们还进行了实验研究，证明了提议的移动 AIGC 驱动 HDT 解决方案的效iveness。 最后，我们 briefly discuss 了一些开放问题和未来方向。
</details></li>
</ul>
<hr>
<h2 id="A-Zero-shot-and-Few-shot-Study-of-Instruction-Finetuned-Large-Language-Models-Applied-to-Clinical-and-Biomedical-Tasks"><a href="#A-Zero-shot-and-Few-shot-Study-of-Instruction-Finetuned-Large-Language-Models-Applied-to-Clinical-and-Biomedical-Tasks" class="headerlink" title="A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks"></a>A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12114">http://arxiv.org/abs/2307.12114</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanis Labrak, Mickael Rouvier, Richard Dufour</li>
<li>for: 这些大型自然语言处理（NLP）任务，如名实化识别（NER）、问答（QA）、关系抽取（RE）等，是为了评估四种现状最佳的 instruciton-tuned 大语言模型（LLMs）在英文医学和生物医学领域的表现。</li>
<li>methods: 这些LLMs 是通过对 instruction-tuned 模型进行训练，以适应不同的 NLP 任务。</li>
<li>results: 结果表明，评估的 LLMs 在零到几个采样enario 下，对大多数任务的性能都在逐渐提高，特别是在问答任务上，即使它们从来没有看到这些任务的示例。然而，分类和RE任务的性能下降，与专门为医疗领域训练的模型，如PubMedBERT，相比而言，它们的性能较差。此外，我们发现没有任何 LLM 在所有任务上都能超越其他模型，各个模型在不同任务上的表现不同。<details>
<summary>Abstract</summary>
We evaluate four state-of-the-art instruction-tuned large language models (LLMs) -- ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca -- on a set of 13 real-world clinical and biomedical natural language processing (NLP) tasks in English, such as named-entity recognition (NER), question-answering (QA), relation extraction (RE), etc. Our overall results demonstrate that the evaluated LLMs begin to approach performance of state-of-the-art models in zero- and few-shot scenarios for most tasks, and particularly well for the QA task, even though they have never seen examples from these tasks before. However, we observed that the classification and RE tasks perform below what can be achieved with a specifically trained model for the medical field, such as PubMedBERT. Finally, we noted that no LLM outperforms all the others on all the studied tasks, with some models being better suited for certain tasks than others.
</details>
<details>
<summary>摘要</summary>
我们评估了四种现代 instruction-tuned大型自然语言处理（NLP）模型（ChatGPT、Flan-T5 UL2、Tk-Instruct和Alpaca），在英语的13种实际医疗和生物医学NLP任务上进行评估，包括名称实体识别（NER）、问答（QA）、关系提取（RE）等。我们的总结结果表明，评估的LLMs在零或几个预测enario中的性能已经接近了现有模型的性能，尤其是在QA任务上表现出色，即使它们从来没有看到这些任务的示例。然而，我们发现，分类和RE任务的性能比特别训练的医疗领域模型，如PubMedBERT，还是有所下降。最后，我们注意到，无LLM可以在所有研究任务上表现出优于其他模型，一些模型更适合某些任务。
</details></li>
</ul>
<hr>
<h2 id="Active-Control-of-Flow-over-Rotating-Cylinder-by-Multiple-Jets-using-Deep-Reinforcement-Learning"><a href="#Active-Control-of-Flow-over-Rotating-Cylinder-by-Multiple-Jets-using-Deep-Reinforcement-Learning" class="headerlink" title="Active Control of Flow over Rotating Cylinder by Multiple Jets using Deep Reinforcement Learning"></a>Active Control of Flow over Rotating Cylinder by Multiple Jets using Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12083">http://arxiv.org/abs/2307.12083</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kamyar Dobakhti, Jafar Ghazanfarian</li>
<li>for: 这个论文主要目的是提出一种基于深度学习的活动流控方法，以减少碰撞体上的阻力。</li>
<li>methods: 该方法使用多个控制的喷流来达到最大可能的阻力减少。具体来说，文章将介绍DRL算法的控制参数、其限制和优化，以及喷流数量和位置、感测器位置和最大喷流速率的优化。</li>
<li>results: 结果表明，将旋转和DRL相结合可以有效地减少阻力系数，达到49.75%的减少级别。此外，文章还表明，在不同的配置下，感测器的数量和位置需要根据用户的需求进行选择。同时，允许代理人访问更高的喷流速率，通常不会提高性能，除非rotating cylinder。<details>
<summary>Abstract</summary>
The real power of artificial intelligence appears in reinforcement learning, which is computationally and physically more sophisticated due to its dynamic nature. Rotation and injection are some of the proven ways in active flow control for drag reduction on blunt bodies. In this paper, rotation will be added to the cylinder alongside the deep reinforcement learning (DRL) algorithm, which uses multiple controlled jets to reach the maximum possible drag suppression. Characteristics of the DRL code, including controlling parameters, their limitations, and optimization of the DRL network for use with rotation will be presented. This work will focus on optimizing the number and positions of the jets, the sensors location, and the maximum allowed flow rate to jets in the form of the maximum allowed flow rate of each actuation and the total number of them per episode. It is found that combining the rotation and DRL is promising since it suppresses the vortex shedding, stabilizes the Karman vortex street, and reduces the drag coefficient by up to 49.75%. Also, it will be shown that having more sensors at more locations is not always a good choice and the sensor number and location should be determined based on the need of the user and corresponding configuration. Also, allowing the agent to have access to higher flow rates, mostly reduces the performance, except when the cylinder rotates. In all cases, the agent can keep the lift coefficient at a value near zero, or stabilize it at a smaller number.
</details>
<details>
<summary>摘要</summary>
真正的人工智能在强化学习中表现出真正的力量，因为它的动态性使其更加复杂。在活动流控中，旋转和注入是已知的降低拖力的方法。在这篇论文中，我们将在筒体上添加旋转，并与深度强化学习（DRL）算法结合使用多个控制的气流来达到最大可能的拖力降低。我们将展示DRL代码中的控制参数、其限制和优化DRL网络的方法，包括气流管道的数量和位置、感应器的位置和每个episode中的最大气流量。我们发现，将旋转和DRL结合使用是有前途的，因为它可以阻断旋转 shedding，稳定卡曼旋流street，并降低拖力系数至最多49.75%。此外，我们还发现，在某些情况下，添加更多的感应器并不总是有利，需要根据用户的需求和相应的配置来确定感应器的数量和位置。此外，允许机器人访问更高的气流量，通常会降低性能，除非筒体在旋转。在所有情况下，机器人都可以保持着降低的升力系数，或者稳定其在更小的数字上。
</details></li>
</ul>
<hr>
<h2 id="Spectral-Normalized-Cut-Graph-Partitioning-with-Fairness-Constraints"><a href="#Spectral-Normalized-Cut-Graph-Partitioning-with-Fairness-Constraints" class="headerlink" title="Spectral Normalized-Cut Graph Partitioning with Fairness Constraints"></a>Spectral Normalized-Cut Graph Partitioning with Fairness Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12065">http://arxiv.org/abs/2307.12065</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiali2000/fnm">https://github.com/jiali2000/fnm</a></li>
<li>paper_authors: Jia Li, Yanhao Wang, Arpit Merchant</li>
<li>for: 本文目的是为了分解一个图的节点集 into $k$ 个彩色独立集，以最小化图中任何两个集之间的正规化连接值，同时保证每个属性分布在每个集中是约等的。</li>
<li>methods: 本文提出了一种两阶段的光谱算法，称为 FNM，用于实现公平分解。在第一阶段，我们添加了一个增强的拉格朗日函数基于我们的公平准则，以生成一个公平的光谱节点嵌入。在第二阶段，我们设计了一种圆拟方案，以生成 $k$ 个集从公平嵌入中生成高质量的分解。</li>
<li>results: 通过对九个标准数据集进行广泛的实验，我们证明了 FNM 比三种基准方法更高效。<details>
<summary>Abstract</summary>
Normalized-cut graph partitioning aims to divide the set of nodes in a graph into $k$ disjoint clusters to minimize the fraction of the total edges between any cluster and all other clusters. In this paper, we consider a fair variant of the partitioning problem wherein nodes are characterized by a categorical sensitive attribute (e.g., gender or race) indicating membership to different demographic groups. Our goal is to ensure that each group is approximately proportionally represented in each cluster while minimizing the normalized cut value. To resolve this problem, we propose a two-phase spectral algorithm called FNM. In the first phase, we add an augmented Lagrangian term based on our fairness criteria to the objective function for obtaining a fairer spectral node embedding. Then, in the second phase, we design a rounding scheme to produce $k$ clusters from the fair embedding that effectively trades off fairness and partition quality. Through comprehensive experiments on nine benchmark datasets, we demonstrate the superior performance of FNM compared with three baseline methods.
</details>
<details>
<summary>摘要</summary>
normalized-cut graph partitioning aimed to divide the set of nodes in a graph into $k$ disjoint clusters to minimize the fraction of the total edges between any cluster and all other clusters. In this paper, we considered a fair variant of the partitioning problem, where nodes were characterized by a categorical sensitive attribute (e.g., gender or race) indicating membership to different demographic groups. Our goal was to ensure that each group was approximately proportionally represented in each cluster while minimizing the normalized cut value. To resolve this problem, we proposed a two-phase spectral algorithm called FNM. In the first phase, we added an augmented Lagrangian term based on our fairness criteria to the objective function for obtaining a fairer spectral node embedding. Then, in the second phase, we designed a rounding scheme to produce $k$ clusters from the fair embedding that effectively trades off fairness and partition quality. Through comprehensive experiments on nine benchmark datasets, we demonstrated the superior performance of FNM compared with three baseline methods.
</details></li>
</ul>
<hr>
<h2 id="Balancing-Exploration-and-Exploitation-in-Hierarchical-Reinforcement-Learning-via-Latent-Landmark-Graphs"><a href="#Balancing-Exploration-and-Exploitation-in-Hierarchical-Reinforcement-Learning-via-Latent-Landmark-Graphs" class="headerlink" title="Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs"></a>Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12063">http://arxiv.org/abs/2307.12063</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/papercode2022/hill">https://github.com/papercode2022/hill</a></li>
<li>paper_authors: Qingyang Zhang, Yiming Yang, Jingqing Ruan, Xuantang Xiong, Dengpeng Xing, Bo Xu</li>
<li>for: 这篇论文目的是提出一种可以解决循环对待问题的弹性问题决策学习方法，即 Hierarchical reinforcement learning via dynamically building Latent Landmark graphs (HILL)。</li>
<li>methods: 这篇论文使用了一种名为 HILL 的方法，它使用了对抗表示学习目标来学习隐藏目标表示，然后使用这些表示来动态建立隐藏标签图和选择策略，以解决循环对待问题的问题。</li>
<li>results: 实验结果显示，HILL 比state-of-the-art基eline在缺乏对象奖励的连续控制任务上具有更高的样本效率和渐进性表现。<details>
<summary>Abstract</summary>
Goal-Conditioned Hierarchical Reinforcement Learning (GCHRL) is a promising paradigm to address the exploration-exploitation dilemma in reinforcement learning. It decomposes the source task into subgoal conditional subtasks and conducts exploration and exploitation in the subgoal space. The effectiveness of GCHRL heavily relies on subgoal representation functions and subgoal selection strategy. However, existing works often overlook the temporal coherence in GCHRL when learning latent subgoal representations and lack an efficient subgoal selection strategy that balances exploration and exploitation. This paper proposes HIerarchical reinforcement learning via dynamically building Latent Landmark graphs (HILL) to overcome these limitations. HILL learns latent subgoal representations that satisfy temporal coherence using a contrastive representation learning objective. Based on these representations, HILL dynamically builds latent landmark graphs and employs a novelty measure on nodes and a utility measure on edges. Finally, HILL develops a subgoal selection strategy that balances exploration and exploitation by jointly considering both measures. Experimental results demonstrate that HILL outperforms state-of-the-art baselines on continuous control tasks with sparse rewards in sample efficiency and asymptotic performance. Our code is available at https://github.com/papercode2022/HILL.
</details>
<details>
<summary>摘要</summary>
“对于受益从探索和实施的问题，叫做目标调整层次学习（GCHRL）是一种有前途的思路。它将源任务分解为子任务 conditional subtask，并在子任务空间进行探索和实施。GCHRL的有效性很大程度上取决于子任务表示函数和子任务选择策略。然而，现有的工作往往忽略GCHRL中的时间协调性在学习隐藏子任务表示时。此外，缺乏一个能够均衡探索和实施的子任务选择策略。本文提出了层次学习 via 动态建立隐藏地标 graphs（HILL）来解决这些限制。HILL使用了一个对照式表示学习目标来学习隐藏子任务表示，并在这些表示上动态建立隐藏地标 graphs。HILL还使用了节点上的新鲜度量和边上的实用度量。最后，HILL发展了一个子任务选择策略，考虑了这两个度量，以均衡探索和实施。实验结果显示，HILL在缺少奖励的粒子控制任务上比基于 estado-of-the-art 基eline 高效和长期性。我们的代码可以在 <https://github.com/papercode2022/HILL> 获取。”
</details></li>
</ul>
<hr>
<h2 id="Game-Theoretic-Robust-Reinforcement-Learning-Handles-Temporally-Coupled-Perturbations"><a href="#Game-Theoretic-Robust-Reinforcement-Learning-Handles-Temporally-Coupled-Perturbations" class="headerlink" title="Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations"></a>Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12062">http://arxiv.org/abs/2307.12062</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongyuan Liang, Yanchao Sun, Ruijie Zheng, Xiangyu Liu, Tuomas Sandholm, Furong Huang, Stephen McAleer</li>
<li>for: 本研究旨在训练能够在环境干扰或敌意攻击下表现良好的RL策略。</li>
<li>methods: 我们提出了GRAD方法，它将把 temporally-coupled 干扰视为一个部分可见二人零 SUM 游戏，通过查找这个游戏的approximate equilibria来确保代理人的强度对 temporally-coupled 干扰的Robustness。</li>
<li>results: 我们的提议方法在许多连续控制任务中实验证明了与基elines相比，具有显著的Robustness优势，包括对于标准和 temporally-coupled 干扰的攻击。<details>
<summary>Abstract</summary>
Robust reinforcement learning (RL) seeks to train policies that can perform well under environment perturbations or adversarial attacks. Existing approaches typically assume that the space of possible perturbations remains the same across timesteps. However, in many settings, the space of possible perturbations at a given timestep depends on past perturbations. We formally introduce temporally-coupled perturbations, presenting a novel challenge for existing robust RL methods. To tackle this challenge, we propose GRAD, a novel game-theoretic approach that treats the temporally-coupled robust RL problem as a partially-observable two-player zero-sum game. By finding an approximate equilibrium in this game, GRAD ensures the agent's robustness against temporally-coupled perturbations. Empirical experiments on a variety of continuous control tasks demonstrate that our proposed approach exhibits significant robustness advantages compared to baselines against both standard and temporally-coupled attacks, in both state and action spaces.
</details>
<details>
<summary>摘要</summary>
Strong reinforcement learning (RL) aims to train policies that can perform well under environmental perturbations or adversarial attacks. Existing methods typically assume that the space of possible perturbations remains the same across timesteps. However, in many situations, the space of possible perturbations at a given timestep depends on past perturbations. We formally introduce temporally-coupled perturbations, presenting a new challenge for existing robust RL methods. To address this challenge, we propose GRAD, a novel game-theoretic approach that treats the temporally-coupled robust RL problem as a partially-observable two-player zero-sum game. By finding an approximate equilibrium in this game, GRAD ensures the agent's robustness against temporally-coupled perturbations. Empirical experiments on a variety of continuous control tasks show that our proposed approach exhibits significant robustness advantages compared to baselines against both standard and temporally-coupled attacks, in both state and action spaces.Note: Simplified Chinese is used here, as it is the most widely used variety of Chinese in mainland China and Taiwan. Traditional Chinese is also commonly used, especially in Hong Kong and Macau.
</details></li>
</ul>
<hr>
<h2 id="Fast-Knowledge-Graph-Completion-using-Graphics-Processing-Units"><a href="#Fast-Knowledge-Graph-Completion-using-Graphics-Processing-Units" class="headerlink" title="Fast Knowledge Graph Completion using Graphics Processing Units"></a>Fast Knowledge Graph Completion using Graphics Processing Units</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12059">http://arxiv.org/abs/2307.12059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chun-Hee Lee, Dong-oh Kang, Hwa Jeon Song</li>
<li>for: 这个论文的目的是提出一种高效的知识图完成框架，用于在GPU上获得新的关系。</li>
<li>methods: 该论文使用知识图嵌入模型，将知识图完成问题转化为一种相似Join问题，然后使用度量空间的性质来 derive 高速的完成算法。</li>
<li>results:  experiments 表明，该框架可以高效处理知识图完成问题。<details>
<summary>Abstract</summary>
Knowledge graphs can be used in many areas related to data semantics such as question-answering systems, knowledge based systems. However, the currently constructed knowledge graphs need to be complemented for better knowledge in terms of relations. It is called knowledge graph completion. To add new relations to the existing knowledge graph by using knowledge graph embedding models, we have to evaluate $N\times N \times R$ vector operations, where $N$ is the number of entities and $R$ is the number of relation types. It is very costly.   In this paper, we provide an efficient knowledge graph completion framework on GPUs to get new relations using knowledge graph embedding vectors. In the proposed framework, we first define "transformable to a metric space" and then provide a method to transform the knowledge graph completion problem into the similarity join problem for a model which is "transformable to a metric space". After that, to efficiently process the similarity join problem, we derive formulas using the properties of a metric space. Based on the formulas, we develop a fast knowledge graph completion algorithm. Finally, we experimentally show that our framework can efficiently process the knowledge graph completion problem.
</details>
<details>
<summary>摘要</summary>
知识图可以应用于数据 semantics 多个领域，如问答系统、知识基础系统。然而，目前构建的知识图需要补充以获得更好的知识，这被称为知识图完成。为添加新的关系到现有的知识图，我们需要评估 $N\times N \times R$ 矢量操作，其中 $N$ 是实体的数量，$R$ 是关系类型的数量。这很费时。在这篇论文中，我们提供了一个高效的知识图完成框架在 GPU 上来获得新关系使用知识图嵌入向量。我们首先定义 "可转换到一个度量空间"，然后提供一种将知识图完成问题转换成一个度量空间中的相似Join问题的方法。接着，我们使用度量空间的性质 deriv 出 formulas，并根据 formulas 开发了一个快速的知识图完成算法。最后，我们通过实验表示，我们的框架可以高效地处理知识图完成问题。
</details></li>
</ul>
<hr>
<h2 id="Exploring-MLOps-Dynamics-An-Experimental-Analysis-in-a-Real-World-Machine-Learning-Project"><a href="#Exploring-MLOps-Dynamics-An-Experimental-Analysis-in-a-Real-World-Machine-Learning-Project" class="headerlink" title="Exploring MLOps Dynamics: An Experimental Analysis in a Real-World Machine Learning Project"></a>Exploring MLOps Dynamics: An Experimental Analysis in a Real-World Machine Learning Project</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13473">http://arxiv.org/abs/2307.13473</a></li>
<li>repo_url: None</li>
<li>paper_authors: Awadelrahman M. A. Ahmed<br>for:这个研究旨在优化机器学习操作（MLOps）过程，以提高机器学习项目的效率和生产力。methods:该实验使用了一个全面的 MLOps 工作流程，覆盖了问题定义、数据收集、数据准备、模型开发、模型部署、监测、管理、扩展性和合规遵守等重要阶段。实验还采用了一种系统化跟踪方法，以记录 especified 阶段之间的重复访问，以捕捉这些访问的原因。results:研究发现，MLOps 工作流程具有很强的融合和循环特性，并且具有很高的可重复性和可缩放性。通过对实验数据进行分析，提供了一些实践建议和推荐，以便在实际应用中进行进一步的优化和改进。<details>
<summary>Abstract</summary>
This article presents an experiment focused on optimizing the MLOps (Machine Learning Operations) process, a crucial aspect of efficiently implementing machine learning projects. The objective is to identify patterns and insights to enhance the MLOps workflow, considering its iterative and interdependent nature in real-world model development scenarios.   The experiment involves a comprehensive MLOps workflow, covering essential phases like problem definition, data acquisition, data preparation, model development, model deployment, monitoring, management, scalability, and governance and compliance. Practical tips and recommendations are derived from the results, emphasizing proactive planning and continuous improvement for the MLOps workflow.   The experimental investigation was strategically integrated within a real-world ML project which followed essential phases of the MLOps process in a production environment, handling large-scale structured data. A systematic tracking approach was employed to document revisits to specific phases from a main phase under focus, capturing the reasons for such revisits. By constructing a matrix to quantify the degree of overlap between phases, the study unveils the dynamic and iterative nature of the MLOps workflow.   The resulting data provides visual representations of the MLOps process's interdependencies and iterative characteristics within the experimental framework, offering valuable insights for optimizing the workflow and making informed decisions in real-world scenarios. This analysis contributes to enhancing the efficiency and effectiveness of machine learning projects through an improved MLOps process.   Keywords: MLOps, Machine Learning Operations, Optimization, Experimental Analysis, Iterative Process, Pattern Identification.
</details>
<details>
<summary>摘要</summary>
The experiment covers a comprehensive MLOps workflow, including problem definition, data acquisition, data preparation, model development, model deployment, monitoring, management, scalability, and governance and compliance. The results provide practical tips and recommendations for proactive planning and continuous improvement of the MLOps workflow.The experimental investigation was conducted within a real-world ML project, which followed the essential phases of the MLOps process in a production environment, handling large-scale structured data. A systematic tracking approach was employed to document revisits to specific phases, capturing the reasons for such revisits. By constructing a matrix to quantify the degree of overlap between phases, the study reveals the dynamic and iterative nature of the MLOps workflow.The resulting data provides visual representations of the MLOps process's interdependencies and iterative characteristics within the experimental framework, offering valuable insights for optimizing the workflow and making informed decisions in real-world scenarios. This analysis contributes to enhancing the efficiency and effectiveness of machine learning projects through an improved MLOps process.Keywords: MLOps, Machine Learning Operations, Optimization, Experimental Analysis, Iterative Process, Pattern Identification.
</details></li>
</ul>
<hr>
<h2 id="Extracting-Molecular-Properties-from-Natural-Language-with-Multimodal-Contrastive-Learning"><a href="#Extracting-Molecular-Properties-from-Natural-Language-with-Multimodal-Contrastive-Learning" class="headerlink" title="Extracting Molecular Properties from Natural Language with Multimodal Contrastive Learning"></a>Extracting Molecular Properties from Natural Language with Multimodal Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12996">http://arxiv.org/abs/2307.12996</a></li>
<li>repo_url: None</li>
<li>paper_authors: Romain Lacombe, Andrew Gaut, Jeff He, David Lüdeke, Kateryna Pistunova</li>
<li>for: 本研究旨在将科学知识从文本中转移到分子图表示，以推进计算生物化学中深度学习的发展。</li>
<li>methods: 研究者使用了对比学习将神经图表示与文本描述的特征相对转移，以提高分子性质预测性能。他们还提出了一种基于有机反应的新型分子图数据生成策略。</li>
<li>results: 研究者在下游的分子网络Property Classification任务上实现了+4.26%的AUROC提升，比Graph模式alone模型提升+1.54%。这表明将科学知识从文本中转移到分子图表示可以提高分子性质预测性能。<details>
<summary>Abstract</summary>
Deep learning in computational biochemistry has traditionally focused on molecular graphs neural representations; however, recent advances in language models highlight how much scientific knowledge is encoded in text. To bridge these two modalities, we investigate how molecular property information can be transferred from natural language to graph representations. We study property prediction performance gains after using contrastive learning to align neural graph representations with representations of textual descriptions of their characteristics. We implement neural relevance scoring strategies to improve text retrieval, introduce a novel chemically-valid molecular graph augmentation strategy inspired by organic reactions, and demonstrate improved performance on downstream MoleculeNet property classification tasks. We achieve a +4.26% AUROC gain versus models pre-trained on the graph modality alone, and a +1.54% gain compared to recently proposed molecular graph/text contrastively trained MoMu model (Su et al. 2022).
</details>
<details>
<summary>摘要</summary>
深度学习在计算生物化学中传统上专注于分子图神经表示;然而，最近的语言模型发展显示了科学知识在文本中的含义。为了融合这两种模式，我们研究如何从自然语言中提取分子性质信息并将其传递到图表示中。我们使用对比学习对神经图表示和文本描述中的特征进行对齐，并使用神经相关性分数策略来提高文本检索。我们还介绍了一种基于有机反应的新型化学Graph augmentation策略，并在下游MoleculeNet性质分类任务上达到了+4.26% AUROC提升和+1.54%提升 compared to MoMu模型（Su et al., 2022）。
</details></li>
</ul>
<hr>
<h2 id="Flight-Contrail-Segmentation-via-Augmented-Transfer-Learning-with-Novel-SR-Loss-Function-in-Hough-Space"><a href="#Flight-Contrail-Segmentation-via-Augmented-Transfer-Learning-with-Novel-SR-Loss-Function-in-Hough-Space" class="headerlink" title="Flight Contrail Segmentation via Augmented Transfer Learning with Novel SR Loss Function in Hough Space"></a>Flight Contrail Segmentation via Augmented Transfer Learning with Novel SR Loss Function in Hough Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12032">http://arxiv.org/abs/2307.12032</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/junzis/contrail-net">https://github.com/junzis/contrail-net</a></li>
<li>paper_authors: Junzi Sun, Esther Roosenbrand</li>
<li>for: 检测飞行 contrails 从卫星图像中</li>
<li>methods: 基于增强转移学习的新模型，以及一种新的损失函数 SR Loss</li>
<li>results: 准确地检测 contrails  WITH  minimal data<details>
<summary>Abstract</summary>
Air transport poses significant environmental challenges, particularly the contribution of flight contrails to climate change due to their potential global warming impact. Detecting contrails from satellite images has been a long-standing challenge. Traditional computer vision techniques have limitations under varying image conditions, and machine learning approaches using typical convolutional neural networks are hindered by the scarcity of hand-labeled contrail datasets and contrail-tailored learning processes. In this paper, we introduce an innovative model based on augmented transfer learning that accurately detects contrails with minimal data. We also propose a novel loss function, SR Loss, which improves contrail line detection by transforming the image space into Hough space. Our research opens new avenues for machine learning-based contrail detection in aviation research, offering solutions to the lack of large hand-labeled datasets, and significantly enhancing contrail detection models.
</details>
<details>
<summary>摘要</summary>
空中交通对环境造成重要挑战，特别是飞行烟尘的潜在全球暖化影响。从卫星图像探测飞行烟尘是一项长期挑战。传统的计算机视觉技术在不同的图像条件下有限制，机器学习方法使用 Typical convolutional neural networks 也受到手动标注飞行烟尘数据的罕见性和适应飞行烟尘学习过程的限制。在这篇论文中，我们介绍了一种创新的模型，基于增强传输学习，可以准确地检测飞行烟尘，只需 minimal data。我们还提出了一种新的损失函数，SR Loss，它通过将图像空间转换为截距空间，提高了飞行烟尘线检测。我们的研究打开了新的机器学习基于飞行烟尘检测的可能性，解决了航空研究中缺乏大量手动标注数据的问题，并显著提高了飞行烟尘检测模型。
</details></li>
</ul>
<hr>
<h2 id="FinPT-Financial-Risk-Prediction-with-Profile-Tuning-on-Pretrained-Foundation-Models"><a href="#FinPT-Financial-Risk-Prediction-with-Profile-Tuning-on-Pretrained-Foundation-Models" class="headerlink" title="FinPT: Financial Risk Prediction with Profile Tuning on Pretrained Foundation Models"></a>FinPT: Financial Risk Prediction with Profile Tuning on Pretrained Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00065">http://arxiv.org/abs/2308.00065</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuweiyin/finpt">https://github.com/yuweiyin/finpt</a></li>
<li>paper_authors: Yuwei Yin, Yazheng Yang, Jian Yang, Qi Liu</li>
<li>for: 这研究旨在提出一种新的金融风险预测方法，以帮助金融机构更好地识别和预测风险。</li>
<li>methods: 该方法使用Profile Tuning技术，将大型预训模型粘贴到金融表格数据中，并通过提问大语言模型（LLMs）获取自然语言客户profile，进而进行预测。</li>
<li>results: 通过对FinBench数据集进行实验，研究人员发现FinPT方法可以与各种代表性的强基线进行比较，并且通过分析LLMs的性能，深入理解它们在金融风险预测中的应用。<details>
<summary>Abstract</summary>
Financial risk prediction plays a crucial role in the financial sector. Machine learning methods have been widely applied for automatically detecting potential risks and thus saving the cost of labor. However, the development in this field is lagging behind in recent years by the following two facts: 1) the algorithms used are somewhat outdated, especially in the context of the fast advance of generative AI and large language models (LLMs); 2) the lack of a unified and open-sourced financial benchmark has impeded the related research for years. To tackle these issues, we propose FinPT and FinBench: the former is a novel approach for financial risk prediction that conduct Profile Tuning on large pretrained foundation models, and the latter is a set of high-quality datasets on financial risks such as default, fraud, and churn. In FinPT, we fill the financial tabular data into the pre-defined instruction template, obtain natural-language customer profiles by prompting LLMs, and fine-tune large foundation models with the profile text to make predictions. We demonstrate the effectiveness of the proposed FinPT by experimenting with a range of representative strong baselines on FinBench. The analytical studies further deepen the understanding of LLMs for financial risk prediction.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Flexible-Framework-for-Incorporating-Patient-Preferences-Into-Q-Learning"><a href="#A-Flexible-Framework-for-Incorporating-Patient-Preferences-Into-Q-Learning" class="headerlink" title="A Flexible Framework for Incorporating Patient Preferences Into Q-Learning"></a>A Flexible Framework for Incorporating Patient Preferences Into Q-Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12022">http://arxiv.org/abs/2307.12022</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joshua P. Zitovsky, Leslie Wilson, Michael R. Kosorok</li>
<li>for: 这篇论文是为了解决现实世界医疗问题中的多个竞争结果问题而写的，包括治疗效果和不良反应的严重程度。</li>
<li>methods: 这篇论文提出了一种新的方法，即Latent Utility Q-Learning（LUQ-Learning），以解决现有方法的限制，包括只能处理单个时间点和两个结果、不能 incorporate自报病人偏好等。LUQ-Learning 使用隐藏模型方法，自然地扩展 Q-learning 到复合结果设定下，并采取理想的质量评价来对各个病人进行评价。</li>
<li>results: 在基于低背痛的实验中，我们的方法与多种基线方法进行比较，并在所有实验中达到了非常竞争性的实验性表现。<details>
<summary>Abstract</summary>
In real-world healthcare problems, there are often multiple competing outcomes of interest, such as treatment efficacy and side effect severity. However, statistical methods for estimating dynamic treatment regimes (DTRs) usually assume a single outcome of interest, and the few methods that deal with composite outcomes suffer from important limitations. This includes restrictions to a single time point and two outcomes, the inability to incorporate self-reported patient preferences and limited theoretical guarantees. To this end, we propose a new method to address these limitations, which we dub Latent Utility Q-Learning (LUQ-Learning). LUQ-Learning uses a latent model approach to naturally extend Q-learning to the composite outcome setting and adopt the ideal trade-off between outcomes to each patient. Unlike previous approaches, our framework allows for an arbitrary number of time points and outcomes, incorporates stated preferences and achieves strong asymptotic performance with realistic assumptions on the data. We conduct simulation experiments based on an ongoing trial for low back pain as well as a well-known completed trial for schizophrenia. In all experiments, our method achieves highly competitive empirical performance compared to several alternative baselines.
</details>
<details>
<summary>摘要</summary>
在现实医疗问题中，常常存在多个竞争的目的结果，如治疗效果和副作用严重程度。然而，统计方法 для估计动态治疗方案（DTR）通常假设单一的目的结果，而其中几种方法只能处理单个时间点和两个结果。这些方法还具有限制性，例如不能 incorporate自报病人喜好和有限的理论保证。为此，我们提出了一种新的方法，我们称之为潜在用户价值Q学习（LUQ-Learning）。LUQ-Learning 使用潜在模型方法来自然地扩展Q学习到复合结果设定下，并采取每个患者的理想妥协。不同于前一些方法，我们的框架允许任意数量的时间点和结果，并 incorporate 自报病人喜好，并实现强 asymptotic performance 在现实数据下，只需要有限的假设。我们在一个低肢瘤痛试验和一个已完成的躁闹症试验中进行了 simulations experiments。在所有实验中，我们的方法与多个基准方法相比，表现出了非常竞争的实验性。
</details></li>
</ul>
<hr>
<h2 id="Model-Predictive-Control-MPC-of-an-Artificial-Pancreas-with-Data-Driven-Learning-of-Multi-Step-Ahead-Blood-Glucose-Predictors"><a href="#Model-Predictive-Control-MPC-of-an-Artificial-Pancreas-with-Data-Driven-Learning-of-Multi-Step-Ahead-Blood-Glucose-Predictors" class="headerlink" title="Model Predictive Control (MPC) of an Artificial Pancreas with Data-Driven Learning of Multi-Step-Ahead Blood Glucose Predictors"></a>Model Predictive Control (MPC) of an Artificial Pancreas with Data-Driven Learning of Multi-Step-Ahead Blood Glucose Predictors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12015">http://arxiv.org/abs/2307.12015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eleonora Maria Aiello, Mehrad Jaloli, Marzia Cescon</li>
<li>for: 这个研究是为了开发一个基于Linear Time-Varying（LTV）Model Predictive Control（MPC）框架的关闭循环胰岛素输送算法，用于治疗类型1 диабе尼（T1D）。</li>
<li>methods: 这个研究使用了一个数据驱动的多步预测血糖（BG）预测器，并将其与LTV MPC框架集成。而不是从数据中直接标定胰岛素逻辑系统的开放循环模型，这里提议直接使用BG预测器来预测未来的血糖水平。为非线性部分，使用了Long Short-Term Memory（LSTM）网络，而为线性部分，使用了线性回归模型。</li>
<li>results: 对于三个模拟场景，包括一个标准情况，一个随机餐食干扰情况，以及一个减少胰岛素敏感度25%的情况，我们证明了我们的LSTM-MPC控制器的优势。在随机餐食干扰情况下，我们的方法提供了更加准确的未来血糖水平预测，以及更好的封闭循环性能。<details>
<summary>Abstract</summary>
We present the design and \textit{in-silico} evaluation of a closed-loop insulin delivery algorithm to treat type 1 diabetes (T1D) consisting in a data-driven multi-step-ahead blood glucose (BG) predictor integrated into a Linear Time-Varying (LTV) Model Predictive Control (MPC) framework. Instead of identifying an open-loop model of the glucoregulatory system from available data, we propose to directly fit the entire BG prediction over a predefined prediction horizon to be used in the MPC, as a nonlinear function of past input-ouput data and an affine function of future insulin control inputs. For the nonlinear part, a Long Short-Term Memory (LSTM) network is proposed, while for the affine component a linear regression model is chosen. To assess benefits and drawbacks when compared to a traditional linear MPC based on an auto-regressive with exogenous (ARX) input model identified from data, we evaluated the proposed LSTM-MPC controller in three simulation scenarios: a nominal case with 3 meals per day, a random meal disturbances case where meals were generated with a recently published meal generator, and a case with 25$\%$ decrease in the insulin sensitivity. Further, in all the scenarios, no feedforward meal bolus was administered. For the more challenging random meal generation scenario, the mean $\pm$ standard deviation percent time in the range 70-180 [mg/dL] was 74.99 $\pm$ 7.09 vs. 54.15 $\pm$ 14.89, the mean $\pm$ standard deviation percent time in the tighter range 70-140 [mg/dL] was 47.78$\pm$8.55 vs. 34.62 $\pm$9.04, while the mean $\pm$ standard deviation percent time in sever hypoglycemia, i.e., $<$ 54 [mg/dl] was 1.00$\pm$3.18 vs. 9.45$\pm$11.71, for our proposed LSTM-MPC controller and the traditional ARX-MPC, respectively. Our approach provided accurate predictions of future glucose concentrations and good closed-loop performances of the overall MPC controller.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种关闭Loop抗糖尿病（T1D）的设计和 simulate evaluate 的数据驱动多步预测血糖（BG）预测算法，包括一个基于线性时变（LTV）模型预测控制（MPC）框架的数据驱动多步预测算法。而不是直接从可用数据中Identify一个开 Loop模型的glucoregulatory系统，我们提议直接将整个BG预测 horizon为用于MPC，作为非线性函数过去输入输出数据和未来药物控制输入的非线性函数。 для非线性部分，我们提议使用一个Long Short-Term Memory（LSTM）网络，而对于线性部分，我们选择了一个线性回归模型。为了评估我们提议的LSTM-MPC控制器与传统的ARX-MPC控制器相比，我们在三个模拟场景中评估了这两个控制器的表现：一个标准的3餐/天场景，一个随机餐品干扰场景，以及一个25%的药物敏感度下降场景。此外，在所有场景中，没有feedforward餐品补偿。在更加复杂的随机餐品生成场景中，LSTM-MPC控制器的mean±标准差%时间在70-180[mg/dL]范围内为74.99±7.09 vs. 54.15±14.89，mean±标准差%时间在70-140[mg/dL]范围内为47.78±8.55 vs. 34.62±9.04，而且mean±标准差%时间在严重低血糖（<54[mg/dL]）下为1.00±3.18 vs. 9.45±11.71。我们的方法提供了精准的未来血糖浓度预测和关闭Loop控制器的全面性能的良好表现。
</details></li>
</ul>
<hr>
<h2 id="NLCUnet-Single-Image-Super-Resolution-Network-with-Hairline-Details"><a href="#NLCUnet-Single-Image-Super-Resolution-Network-with-Hairline-Details" class="headerlink" title="NLCUnet: Single-Image Super-Resolution Network with Hairline Details"></a>NLCUnet: Single-Image Super-Resolution Network with Hairline Details</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12014">http://arxiv.org/abs/2307.12014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiancong Feng, Yuan-Gen Wang, Fengchuang Xing</li>
<li>For: 提高单张超解像图像质量，特别是细节部分的精度。* Methods: 提出了一种基于非本地注意力的单张超解像网络（NLCUnet），包括三个核心设计：非本地注意力机制、深度卷积 convolution 和通道注意力。* Results: 在DF2K dataset上进行了许多实验，发现 NLCUnet 在 PSNR 和 SSIM 指标上比现有方法提高较多，并且可以保持更好的细节部分。<details>
<summary>Abstract</summary>
Pursuing the precise details of super-resolution images is challenging for single-image super-resolution tasks. This paper presents a single-image super-resolution network with hairline details (termed NLCUnet), including three core designs. Specifically, a non-local attention mechanism is first introduced to restore local pieces by learning from the whole image region. Then, we find that the blur kernel trained by the existing work is unnecessary. Based on this finding, we create a new network architecture by integrating depth-wise convolution with channel attention without the blur kernel estimation, resulting in a performance improvement instead. Finally, to make the cropped region contain as much semantic information as possible, we propose a random 64$\times$64 crop inside the central 512$\times$512 crop instead of a direct random crop inside the whole image of 2K size. Numerous experiments conducted on the benchmark DF2K dataset demonstrate that our NLCUnet performs better than the state-of-the-art in terms of the PSNR and SSIM metrics and yields visually favorable hairline details.
</details>
<details>
<summary>摘要</summary>
推进超高清照片的精确细节是单图超解像 зада务中的挑战。本文提出了一个单图超解像网络（NLCUnet），包括三个核心设计。具体来说，我们首先引入非本地注意力机制，以便通过整个图像区域学习地址本地副本。然后，我们发现现有工作中训练的模糊核心不是必需的，因此我们创建了一个新的网络架构，通过depthwise核论和通道注意力来提高性能。最后，我们提议在中心256×256区域中随机选择64×64区域，以便尽可能包含图像中的semantic信息。在DF2K数据集上进行了多次实验，表明我们的NLCUnet在PSNR和SSIM指标上比state-of-the-art更高，并且视觉上具有更好的毛细膨胀细节。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Self-Supervised-Learning-Based-Approach-for-Patient-Similarity-A-Case-Study-on-Atrial-Fibrillation-Detection-from-PPG-Signal"><a href="#Contrastive-Self-Supervised-Learning-Based-Approach-for-Patient-Similarity-A-Case-Study-on-Atrial-Fibrillation-Detection-from-PPG-Signal" class="headerlink" title="Contrastive Self-Supervised Learning Based Approach for Patient Similarity: A Case Study on Atrial Fibrillation Detection from PPG Signal"></a>Contrastive Self-Supervised Learning Based Approach for Patient Similarity: A Case Study on Atrial Fibrillation Detection from PPG Signal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02433">http://arxiv.org/abs/2308.02433</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/subangkar/simsig">https://github.com/subangkar/simsig</a></li>
<li>paper_authors: Subangkar Karmaker Shanto, Shoumik Saha, Atif Hasan Rahman, Mohammad Mehedy Masud, Mohammed Eunus Ali</li>
<li>for: 这个论文是为了提出一种基于对比学习的深度学习框架，用于搜索基于生物 физи学信号的病人相似性。</li>
<li>methods: 这个框架使用对比学习方法来学习病人的相似embedding，并引入了一些邻居选择算法来确定生成embedding上的最高相似性。</li>
<li>results: 作者通过对一个涉及到心脏病的案例研究，证明了该框架的有效性。实验结果表明，该框架可以准确地检测心脏病AF，并且与其他基线方法相比，其性能更高。<details>
<summary>Abstract</summary>
In this paper, we propose a novel contrastive learning based deep learning framework for patient similarity search using physiological signals. We use a contrastive learning based approach to learn similar embeddings of patients with similar physiological signal data. We also introduce a number of neighbor selection algorithms to determine the patients with the highest similarity on the generated embeddings. To validate the effectiveness of our framework for measuring patient similarity, we select the detection of Atrial Fibrillation (AF) through photoplethysmography (PPG) signals obtained from smartwatch devices as our case study. We present extensive experimentation of our framework on a dataset of over 170 individuals and compare the performance of our framework with other baseline methods on this dataset.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出了一种基于对比学习的深度学习框架，用于通过生物物理信号来查找病人相似性。我们使用对比学习方法来学习病人的相似 embedding，并引入了一些邻居选择算法来确定生成 embedding 中最相似的病人。为了证明我们的框架的有效性，我们选择了基于 photoplethysmography (PPG) 信号检测 Atrial Fibrillation (AF) 为我们的案例研究。我们对一个包含超过 170 个个体的数据集进行了广泛的实验，并与其他基线方法进行比较。
</details></li>
</ul>
<hr>
<h2 id="Expert-Knowledge-Aware-Image-Difference-Graph-Representation-Learning-for-Difference-Aware-Medical-Visual-Question-Answering"><a href="#Expert-Knowledge-Aware-Image-Difference-Graph-Representation-Learning-for-Difference-Aware-Medical-Visual-Question-Answering" class="headerlink" title="Expert Knowledge-Aware Image Difference Graph Representation Learning for Difference-Aware Medical Visual Question Answering"></a>Expert Knowledge-Aware Image Difference Graph Representation Learning for Difference-Aware Medical Visual Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11986">http://arxiv.org/abs/2307.11986</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/holipori/mimic-diff-vqa">https://github.com/holipori/mimic-diff-vqa</a></li>
<li>paper_authors: Xinyue Hu, Lin Gu, Qiyuan An, Mengliang Zhang, Liangchen Liu, Kazuma Kobayashi, Tatsuya Harada, Ronald M. Summers, Yingying Zhu</li>
<li>for: 这 paper 的目的是提出一个新的胸部X射影差异视觉问答任务 (VQA)，以帮助自动化医疗视觉语言模型。</li>
<li>methods: 这 paper 使用了一种新的专家知识感知图表学习模型，将图像差异视觉问答任务解决。该模型利用了 анатомиче结构优先知识、semantic知识和空间知识等专家知识来构建多关系图，表示图像差异的问答任务。</li>
<li>results: 这 paper 收集了一个新的数据集，名为 MIMIC-Diff-VQA，包含 700,703 个问答对from 164,324 对主要和参考图像。与现有的医疗 VQA 数据集相比，这些问题更加适合临床诊断实践中的诊断- intervene-评估过程。<details>
<summary>Abstract</summary>
To contribute to automating the medical vision-language model, we propose a novel Chest-Xray Difference Visual Question Answering (VQA) task. Given a pair of main and reference images, this task attempts to answer several questions on both diseases and, more importantly, the differences between them. This is consistent with the radiologist's diagnosis practice that compares the current image with the reference before concluding the report. We collect a new dataset, namely MIMIC-Diff-VQA, including 700,703 QA pairs from 164,324 pairs of main and reference images. Compared to existing medical VQA datasets, our questions are tailored to the Assessment-Diagnosis-Intervention-Evaluation treatment procedure used by clinical professionals. Meanwhile, we also propose a novel expert knowledge-aware graph representation learning model to address this task. The proposed baseline model leverages expert knowledge such as anatomical structure prior, semantic, and spatial knowledge to construct a multi-relationship graph, representing the image differences between two images for the image difference VQA task. The dataset and code can be found at https://github.com/Holipori/MIMIC-Diff-VQA. We believe this work would further push forward the medical vision language model.
</details>
<details>
<summary>摘要</summary>
为了让医疗视语言模型自动化，我们提出了一个新的胸部X射影异常视问答（VQA）任务。给定一对主要和参考图像，这个任务的目标是回答一些疾病和图像之间的异常问题。这与医生诊断实践相一致，即将当前图像与参考图像进行比较，以确定报告。我们收集了一个新的数据集，即MIMIC-Diff-VQA，包含700703个问答对 from 164324对主要和参考图像。与现有的医学VQA数据集相比，我们的问题更加适合医生在诊断过程中采用的评估-诊断- interven-评估（ADIE）治疗流程。此外，我们还提出了一种基于专家知识的图像异常关系学习模型，以解决这个任务。我们的基eline模型利用专家知识，如生物结构优先知识、semantic知识和空间知识，构建多关系图，表示图像之间的异常关系。数据集和代码可以在https://github.com/Holipori/MIMIC-Diff-VQA中找到。我们认为这项工作将会进一步推动医学视语言模型的发展。
</details></li>
</ul>
<hr>
<h2 id="Collaborative-Graph-Neural-Networks-for-Attributed-Network-Embedding"><a href="#Collaborative-Graph-Neural-Networks-for-Attributed-Network-Embedding" class="headerlink" title="Collaborative Graph Neural Networks for Attributed Network Embedding"></a>Collaborative Graph Neural Networks for Attributed Network Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11981">http://arxiv.org/abs/2307.11981</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qiaoyut/conn">https://github.com/qiaoyut/conn</a></li>
<li>paper_authors: Qiaoyu Tan, Xin Zhang, Xiao Huang, Hao Chen, Jundong Li, Xia Hu<br>for: This paper focuses on developing a new graph neural network (GNN) architecture called COllaborative graph Neural Networks (CONN) to improve attribute network embedding.methods: The proposed CONN architecture uses selective message diffusion and cross-correlation to jointly reconstruct node-to-node and node-to-attribute-category interactions, which enhances the model’s capacity.results: The experimental results on real-world networks show that CONN outperforms state-of-the-art embedding algorithms with a significant margin.<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have shown prominent performance on attributed network embedding. However, existing efforts mainly focus on exploiting network structures, while the exploitation of node attributes is rather limited as they only serve as node features at the initial layer. This simple strategy impedes the potential of node attributes in augmenting node connections, leading to limited receptive field for inactive nodes with few or even no neighbors. Furthermore, the training objectives (i.e., reconstructing network structures) of most GNNs also do not include node attributes, although studies have shown that reconstructing node attributes is beneficial. Thus, it is encouraging to deeply involve node attributes in the key components of GNNs, including graph convolution operations and training objectives. However, this is a nontrivial task since an appropriate way of integration is required to maintain the merits of GNNs. To bridge the gap, in this paper, we propose COllaborative graph Neural Networks--CONN, a tailored GNN architecture for attribute network embedding. It improves model capacity by 1) selectively diffusing messages from neighboring nodes and involved attribute categories, and 2) jointly reconstructing node-to-node and node-to-attribute-category interactions via cross-correlation. Experiments on real-world networks demonstrate that CONN excels state-of-the-art embedding algorithms with a great margin.
</details>
<details>
<summary>摘要</summary>
GRAPH Neural Networks (GNNs) 有出色表现在嵌入属性网络中。然而，现有努力主要是利用网络结构，而忽视节点特征的利用，只是将节点特征作为初始层节点特征使用。这种简单的策略限制了无活节点的潜在范围，因为它们有少量或甚至没有邻居。此外，大多数 GNN 的训练目标（即重建网络结构）并不包括节点特征，尽管研究表明重建节点特征有利。因此，深入涉及节点特征在 GNN 的关键组件中是一项挑战，需要避免降低 GNN 的优点。为了bridging这个差距，在这篇论文中，我们提出了协同图 neural Networks（CONN），一种针对嵌入属性网络的特化 GNN 架构。它提高了模型容量，通过1) 选择性地往返邻居节点和涉及属性类别中传递消息，2) 并同时重建节点到节点和节点到属性类别的交互。实验表明，CONN 在实际网络上超过了当前领先 embedding 算法的性能。
</details></li>
</ul>
<hr>
<h2 id="Simulation-of-Arbitrary-Level-Contrast-Dose-in-MRI-Using-an-Iterative-Global-Transformer-Model"><a href="#Simulation-of-Arbitrary-Level-Contrast-Dose-in-MRI-Using-an-Iterative-Global-Transformer-Model" class="headerlink" title="Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model"></a>Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11980">http://arxiv.org/abs/2307.11980</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dayang Wang, Srivathsa Pasumarthi, Greg Zaharchuk, Ryan Chamberlain</li>
<li>for: 这个研究旨在提出一种基于卷积神经网络的图像合成方法，以实现不同剂量水平的对照增强图像的生成，以便为MRI成像中的医学应用提供更好的依据。</li>
<li>methods: 该方法基于一种名为Gformer的变换器，其包括一种抽样基于注意力机制和一种旋转 shift模块，以捕捉不同对照增强特征。</li>
<li>results: 对比其他状态艺技术，该方法的评估结果表明其性能更高。此外，该方法还在下游任务中，如剂量减少和肿瘤分割中进行了评估，以证明其在临床应用中的价值。<details>
<summary>Abstract</summary>
Deep learning (DL) based contrast dose reduction and elimination in MRI imaging is gaining traction, given the detrimental effects of Gadolinium-based Contrast Agents (GBCAs). These DL algorithms are however limited by the availability of high quality low dose datasets. Additionally, different types of GBCAs and pathologies require different dose levels for the DL algorithms to work reliably. In this work, we formulate a novel transformer (Gformer) based iterative modelling approach for the synthesis of images with arbitrary contrast enhancement that corresponds to different dose levels. The proposed Gformer incorporates a sub-sampling based attention mechanism and a rotational shift module that captures the various contrast related features. Quantitative evaluation indicates that the proposed model performs better than other state-of-the-art methods. We further perform quantitative evaluation on downstream tasks such as dose reduction and tumor segmentation to demonstrate the clinical utility.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）基于对比剂量减少和消除在MRI成像中得到了进一步的发展，因为Gadolinium-based Contrast Agents（GBCAs）的负面效应。但这些DL算法受到高质量低剂量数据的有效性的限制。此外，不同类型的GBCAs和疾病需要不同的剂量水平以便DL算法可靠地工作。在这种工作中，我们提出了一种基于转换器（Gformer）的迭代模型方法，用于生成具有任意对比强化的图像。我们的Gformer模型包括子抽样基于注意力机制和旋转变换模块，以捕捉不同的对比相关特征。量化评估表明，我们提出的模型在其他状态当前的方法之上表现出了更好的性能。我们进一步进行了下游任务如剂量减少和肿瘤分割，以证明临床实用性。
</details></li>
</ul>
<hr>
<h2 id="Why-Is-Prompt-Tuning-for-Vision-Language-Models-Robust-to-Noisy-Labels"><a href="#Why-Is-Prompt-Tuning-for-Vision-Language-Models-Robust-to-Noisy-Labels" class="headerlink" title="Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?"></a>Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11978">http://arxiv.org/abs/2307.11978</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cewu/ptnl">https://github.com/cewu/ptnl</a></li>
<li>paper_authors: Cheng-En Wu, Yu Tian, Haichao Yu, Heng Wang, Pedro Morgado, Yu Hen Hu, Linjie Yang</li>
<li>for: 研究了CLIP模型在干预几个示例下调整为新的分类任务中的稳定性。</li>
<li>methods: 使用了几个示例来调整CLIP模型，并发现这种方法具有很高的抗噪性。</li>
<li>results: 发现了两个关键因素导致这种方法的稳定性：1）固定的类名Token提供了模型优化过程中强制的正则化，减少了噪音样本引起的梯度; 2）从多样化和通用的网络数据中学习的强大预训练图文映射提供了图像分类的强大先验知识。此外，我们还示出了使用CLIP模型自己的噪音零例预测来调整其自己的提示，可以显著提高无监督下的预测精度。代码可以在<a target="_blank" rel="noopener" href="https://github.com/CEWu/PTNL%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/CEWu/PTNL中找到。</a><details>
<summary>Abstract</summary>
Vision-language models such as CLIP learn a generic text-image embedding from large-scale training data. A vision-language model can be adapted to a new classification task through few-shot prompt tuning. We find that such a prompt tuning process is highly robust to label noises. This intrigues us to study the key reasons contributing to the robustness of the prompt tuning paradigm. We conducted extensive experiments to explore this property and find the key factors are: 1) the fixed classname tokens provide a strong regularization to the optimization of the model, reducing gradients induced by the noisy samples; 2) the powerful pre-trained image-text embedding that is learned from diverse and generic web data provides strong prior knowledge for image classification. Further, we demonstrate that noisy zero-shot predictions from CLIP can be used to tune its own prompt, significantly enhancing prediction accuracy in the unsupervised setting. The code is available at https://github.com/CEWu/PTNL.
</details>
<details>
<summary>摘要</summary>
CLIP类的视觉语言模型通过大规模训练学习一个通用的文本图像嵌入。一个视觉语言模型可以通过几个shot提问调整到新的分类任务。我们发现这种提问调整过程具有高度的鲁棒性，这使我们感到感兴趣，并且想 deeper 地研究这种特性的原因。我们进行了广泛的实验，并发现关键因素有两个：1）固定的类名token提供了模型优化的强制性，减少了噪音样本引起的梯度；2）通过多种和通用的网络数据学习的强大预训练图像文本嵌入，为图像分类提供了强大的先验知识。此外，我们示出了使用CLIP生成的噪音零shot预测来调整其自己的提问，可以大幅提高无监督下的预测精度。代码可以在https://github.com/CEWu/PTNL 中找到。
</details></li>
</ul>
<hr>
<h2 id="Out-of-Distribution-Optimality-of-Invariant-Risk-Minimization"><a href="#Out-of-Distribution-Optimality-of-Invariant-Risk-Minimization" class="headerlink" title="Out-of-Distribution Optimality of Invariant Risk Minimization"></a>Out-of-Distribution Optimality of Invariant Risk Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11972">http://arxiv.org/abs/2307.11972</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shoji Toyota, Kenji Fukumizu</li>
<li>for: 提高深度神经网络的泛化能力，即使在未经见过的领域下也能准确预测。</li>
<li>methods: 使用偏向风险最小化（IRM）方法，解决深度神经网络继承训练数据中嵌入的假 correlations 问题，以提高模型的泛化能力。</li>
<li>results: 提供了一种理论保证，表明在满足certain conditions下，bi-level optimization problem的解决方案会最小化异常风险。<details>
<summary>Abstract</summary>
Deep Neural Networks often inherit spurious correlations embedded in training data and hence may fail to generalize to unseen domains, which have different distributions from the domain to provide training data. M. Arjovsky et al. (2019) introduced the concept out-of-distribution (o.o.d.) risk, which is the maximum risk among all domains, and formulated the issue caused by spurious correlations as a minimization problem of the o.o.d. risk. Invariant Risk Minimization (IRM) is considered to be a promising approach to minimize the o.o.d. risk: IRM estimates a minimum of the o.o.d. risk by solving a bi-level optimization problem. While IRM has attracted considerable attention with empirical success, it comes with few theoretical guarantees. Especially, a solid theoretical guarantee that the bi-level optimization problem gives the minimum of the o.o.d. risk has not yet been established. Aiming at providing a theoretical justification for IRM, this paper rigorously proves that a solution to the bi-level optimization problem minimizes the o.o.d. risk under certain conditions. The result also provides sufficient conditions on distributions providing training data and on a dimension of feature space for the bi-leveled optimization problem to minimize the o.o.d. risk.
</details>
<details>
<summary>摘要</summary>
深度神经网络经常会继承训练数据中嵌入的假 correlations，从而导致在未看到的领域中失败，这些领域的分布与训练数据的分布不同。M. Arjovsky等人（2019）引入了 OUT-OF-DISTRIBUTION（o.o.d）风险，它是所有领域的最大风险，并将嵌入在训练数据中的假 correlations 问题定义为一个 minimization 问题。不变risk Minimization (IRM) 被视为一种有前景的方法来减少 o.o.d. 风险：IRM 通过解决一个二级优化问题来估算 o.o.d. 风险的最小值。虽然 IRM 在实际中得到了广泛的关注并取得了一些成功，但它具有少量的理论保证。特别是，一个坚实的理论保证，即二级优化问题的解决方案实际上是 o.o.d. 风险的最小值，尚未被成功地建立。本文通过坚实的理论证明，解决二级优化问题可以减少 o.o.d. 风险，并提供了一些有关训练数据的分布和特征空间维度的充分条件。
</details></li>
</ul>
<hr>
<h2 id="DHC-Dual-debiased-Heterogeneous-Co-training-Framework-for-Class-imbalanced-Semi-supervised-Medical-Image-Segmentation"><a href="#DHC-Dual-debiased-Heterogeneous-Co-training-Framework-for-Class-imbalanced-Semi-supervised-Medical-Image-Segmentation" class="headerlink" title="DHC: Dual-debiased Heterogeneous Co-training Framework for Class-imbalanced Semi-supervised Medical Image Segmentation"></a>DHC: Dual-debiased Heterogeneous Co-training Framework for Class-imbalanced Semi-supervised Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11960">http://arxiv.org/abs/2307.11960</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/dhc">https://github.com/xmed-lab/dhc</a></li>
<li>paper_authors: Haonan Wang, Xiaomeng Li</li>
<li>for: 这个研究的目的是提出一个基于 semi-supervised learning (SSL) 的三维医疗影像分类框架，以解决对于医疗影像分类的专家需求和时间耗费问题。</li>
<li>methods: 这个框架使用了一个新的 Dual-debiased Heterogeneous Co-training (DHC) 方法，包括两种损失衡量策略：Distribution-aware Debiased Weighting (DistDW) 和 Difficulty-aware Debiased Weighting (DiffDW)，这些策略可以动态地使用 Pseudo 标签来导引模型解决数据和学习偏见。</li>
<li>results: 实验结果显示，提出的方法可以将 pseudo 标签用于偏见调整和纠正阶层分类问题，并且与现有的 SSL 方法比较，显示出我们的方法在更加具体的 SSL 设定下表现更好。代码和模型可以在 GitHub 上找到：<a target="_blank" rel="noopener" href="https://github.com/xmed-lab/DHC">https://github.com/xmed-lab/DHC</a>.<details>
<summary>Abstract</summary>
The volume-wise labeling of 3D medical images is expertise-demanded and time-consuming; hence semi-supervised learning (SSL) is highly desirable for training with limited labeled data. Imbalanced class distribution is a severe problem that bottlenecks the real-world application of these methods but was not addressed much. Aiming to solve this issue, we present a novel Dual-debiased Heterogeneous Co-training (DHC) framework for semi-supervised 3D medical image segmentation. Specifically, we propose two loss weighting strategies, namely Distribution-aware Debiased Weighting (DistDW) and Difficulty-aware Debiased Weighting (DiffDW), which leverage the pseudo labels dynamically to guide the model to solve data and learning biases. The framework improves significantly by co-training these two diverse and accurate sub-models. We also introduce more representative benchmarks for class-imbalanced semi-supervised medical image segmentation, which can fully demonstrate the efficacy of the class-imbalance designs. Experiments show that our proposed framework brings significant improvements by using pseudo labels for debiasing and alleviating the class imbalance problem. More importantly, our method outperforms the state-of-the-art SSL methods, demonstrating the potential of our framework for the more challenging SSL setting. Code and models are available at: https://github.com/xmed-lab/DHC.
</details>
<details>
<summary>摘要</summary>
医学三维图像的体积级标注是专业技术和时间consuming的;因此使用限制标注数据的 semi-supervised learning (SSL) 是非常有优点的。然而，实际应用中存在严重的类别分布不均问题，这个问题未得到充分关注。为解决这个问题，我们提出了一种新的双向偏置共训（DHC）框架，用于 semi-supervised 三维医学图像分割。我们提出了两种损失补偿策略，即 Distribution-aware Debiased Weighting（DistDW）和 Difficulty-aware Debiased Weighting（DiffDW），这两种策略可以动态使用 pseudo labels 来引导模型解决数据和学习偏见。我们的框架在合作这两个多样和准确的子模型时得到了显著改进。我们还提出了更加代表性的 semi-supervised 医学图像分割 benchmark，可以全面展示我们的类别偏见设计的效果。实验表明，我们的提议的框架可以通过使用 pseudo labels 进行偏见修正和缓解类别偏见问题，并且超越了当前状态的 SSL 方法，表明了我们的框架在更加挑战的 SSL 设定下的潜在力量。代码和模型可以在 GitHub 上找到：https://github.com/xmed-lab/DHC。
</details></li>
</ul>
<hr>
<h2 id="Multi-representations-Space-Separation-based-Graph-level-Anomaly-aware-Detection"><a href="#Multi-representations-Space-Separation-based-Graph-level-Anomaly-aware-Detection" class="headerlink" title="Multi-representations Space Separation based Graph-level Anomaly-aware Detection"></a>Multi-representations Space Separation based Graph-level Anomaly-aware Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12994">http://arxiv.org/abs/2307.12994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fu Lin, Haonan Gong, Mingkang Li, Zitong Wang, Yue Zhang, Xuexiong Luo</li>
<li>for: 本研究的目标是检测图DataSet中的异常图。</li>
<li>methods: 我们提出了一种基于多个表示空间分离的图级异常检测框架。为了考虑不同类型的异常图数据的重要性，我们设计了一个异常感知模块来学习特定的节点级和图级异常重要性。此外，我们学习了严格地分离正常和异常图表示空间，通过四种不同的权重图表示对比彼此。</li>
<li>results: 我们对基eline方法进行了广泛的评估，并通过十个公共图数据集来评估我们的方法。结果表明，我们的方法具有效果。<details>
<summary>Abstract</summary>
Graph structure patterns are widely used to model different area data recently. How to detect anomalous graph information on these graph data has become a popular research problem. The objective of this research is centered on the particular issue that how to detect abnormal graphs within a graph set. The previous works have observed that abnormal graphs mainly show node-level and graph-level anomalies, but these methods equally treat two anomaly forms above in the evaluation of abnormal graphs, which is contrary to the fact that different types of abnormal graph data have different degrees in terms of node-level and graph-level anomalies. Furthermore, abnormal graphs that have subtle differences from normal graphs are easily escaped detection by the existing methods. Thus, we propose a multi-representations space separation based graph-level anomaly-aware detection framework in this paper. To consider the different importance of node-level and graph-level anomalies, we design an anomaly-aware module to learn the specific weight between them in the abnormal graph evaluation process. In addition, we learn strictly separate normal and abnormal graph representation spaces by four types of weighted graph representations against each other including anchor normal graphs, anchor abnormal graphs, training normal graphs, and training abnormal graphs. Based on the distance error between the graph representations of the test graph and both normal and abnormal graph representation spaces, we can accurately determine whether the test graph is anomalous. Our approach has been extensively evaluated against baseline methods using ten public graph datasets, and the results demonstrate its effectiveness.
</details>
<details>
<summary>摘要</summary>
GRAPH结构模式在近期内广泛应用于不同领域的数据模型中。检测图数据中异常Graph信息已成为一个流行的研究问题。我们的研究 objective 是 centered 在特定的问题上，即如何在图数据中检测异常图。前一些研究发现，异常图主要表现为节点级别和图级别异常，但这些方法很容易对两种异常形态进行等效的评估，这与实际情况不符。此外，一些异常图具有轻微异常特征，容易被现有方法排除。因此，我们提出了一个基于多个 Representation space 的图级别异常检测框架。为了考虑不同的节点级别和图级别异常的重要性，我们设计了一个异常检测模块，以学习特定的节点级别和图级别异常之间的权重。此外，我们通过四种不同类型的权重化图表示对之间的竞争学习，以学习纯正的正常图表示空间和异常图表示空间。通过测试图表示与正常图表示空间和异常图表示空间之间的距离错误来准确判断测试图是否异常。我们的方法在比基线方法进行evaluate 后得到了显著的效果。
</details></li>
</ul>
<hr>
<h2 id="High-performance-real-world-optical-computing-trained-by-in-situ-model-free-optimization"><a href="#High-performance-real-world-optical-computing-trained-by-in-situ-model-free-optimization" class="headerlink" title="High-performance real-world optical computing trained by in situ model-free optimization"></a>High-performance real-world optical computing trained by in situ model-free optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11957">http://arxiv.org/abs/2307.11957</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangyuan Zhao, Xin Shu, Renjie Zhou</li>
<li>for: 提高光学计算系统的高速和低能耗数据处理能力，并解决 simulation-to-reality gap。</li>
<li>methods: 使用 score gradient estimation 算法，对光学系统进行模型独立优化，不需要 computation-heavy 和偏见的系统模拟。</li>
<li>results: 在 MNIST 和 FMNIST 数据集上实现了高精度分类，并在无图像和高速细胞分析中展示了潜在的应用前景。<details>
<summary>Abstract</summary>
Optical computing systems can provide high-speed and low-energy data processing but face deficiencies in computationally demanding training and simulation-to-reality gap. We propose a model-free solution for lightweight in situ optimization of optical computing systems based on the score gradient estimation algorithm. This approach treats the system as a black box and back-propagates loss directly to the optical weights' probabilistic distributions, hence circumventing the need for computation-heavy and biased system simulation. We demonstrate a superior classification accuracy on the MNIST and FMNIST datasets through experiments on a single-layer diffractive optical computing system. Furthermore, we show its potential for image-free and high-speed cell analysis. The inherent simplicity of our proposed method, combined with its low demand for computational resources, expedites the transition of optical computing from laboratory demonstrations to real-world applications.
</details>
<details>
<summary>摘要</summary>
光学计算系统可以提供高速和低能耗数据处理，但面临 computationally demanding 训练和实际-模拟之间的差距。我们提出了一种模型自由的解决方案，基于分布式权重的排名预测算法，用于优化光学计算系统。这种方法将系统视为黑盒子，直接从损失函数反射到光学权重的概率分布，因此不需要计算负担重和偏见的系统模拟。我们通过对单层散射光学计算系统进行实验，在 MNIST 和 FMNIST 数据集上达到了更高的分类精度。此外，我们还示出了无图像和高速细胞分析的潜在可能性。我们的提议的简单性和计算资源的低需求，使得光学计算从实验室示范转移到实际应用变得更加容易。
</details></li>
</ul>
<hr>
<h2 id="Puioio-On-device-Real-Time-Smartphone-Based-Automated-Exercise-Repetition-Counting-System"><a href="#Puioio-On-device-Real-Time-Smartphone-Based-Automated-Exercise-Repetition-Counting-System" class="headerlink" title="Pūioio: On-device Real-Time Smartphone-Based Automated Exercise Repetition Counting System"></a>Pūioio: On-device Real-Time Smartphone-Based Automated Exercise Repetition Counting System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02420">http://arxiv.org/abs/2308.02420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Sinclair, Kayla Kautai, Seyed Reza Shahamiri</li>
<li>for: 这个研究的目的是为了开发一个可靠且低成本的手机应用程序，可以在实时进行运动重复计数。</li>
<li>methods: 这个研究使用了深度学习技术，搭配手机摄像头进行运动重复计数。系统包括五个组件：（1）姿势估计、（2）阈值分类、（3）流动性、（4）状态机器、（5）计数器。</li>
<li>results: 这个系统在实际测试中精度高达98.89%，并且在预先录影的数据集中也达到98.85%的准确性。这使得这个系统成为一个有效、低成本且便捷的选择，不需要特殊的仪器或网络连接。<details>
<summary>Abstract</summary>
Automated exercise repetition counting has applications across the physical fitness realm, from personal health to rehabilitation. Motivated by the ubiquity of mobile phones and the benefits of tracking physical activity, this study explored the feasibility of counting exercise repetitions in real-time, using only on-device inference, on smartphones. In this work, after providing an extensive overview of the state-of-the-art automatic exercise repetition counting methods, we introduce a deep learning based exercise repetition counting system for smartphones consisting of five components: (1) Pose estimation, (2) Thresholding, (3) Optical flow, (4) State machine, and (5) Counter. The system is then implemented via a cross-platform mobile application named P\=uioio that uses only the smartphone camera to track repetitions in real time for three standard exercises: Squats, Push-ups, and Pull-ups. The proposed system was evaluated via a dataset of pre-recorded videos of individuals exercising as well as testing by subjects exercising in real time. Evaluation results indicated the system was 98.89% accurate in real-world tests and up to 98.85% when evaluated via the pre-recorded dataset. This makes it an effective, low-cost, and convenient alternative to existing solutions since the proposed system has minimal hardware requirements without requiring any wearable or specific sensors or network connectivity.
</details>
<details>
<summary>摘要</summary>
自动化的运动重复计数有各种应用在身体健身和重建领域，从个人健康到rehabilitation。为了利用移动电话的普遍性和跟踪物理活动的利点，这项研究探索了使用移动电话上的只有设备推理来实时计数运动重复的可能性。在这项研究中，我们首先提供了现有自动运动重复计数方法的广泛概述，然后引入了一种基于深度学习的运动重复计数系统，该系统由五个组成部分：（1）姿势估计，（2）阈值分割，（3）Optical flow，（4）状态机和（5）计数器。这个系统然后通过一个跨平台移动应用程序 named P\=uioio 实现，该应用程序使用了移动电话摄像头来实时跟踪运动重复，并对三种标准运动进行测试：蹲squats，推push-ups和抓pull-ups。我们对这个系统进行了一系列测试和评估，测试结果表明该系统在实际测试中的准确率达98.89%，并且在预录视频数据集上的评估结果为98.85%。这使得该系统成为一个有效、低成本、方便的替代方案，因为它没有特殊的硬件需求，也没有需要佩戴式设备或特殊的传感器或网络连接。
</details></li>
</ul>
<hr>
<h2 id="Implicit-Interpretation-of-Importance-Weight-Aware-Updates"><a href="#Implicit-Interpretation-of-Importance-Weight-Aware-Updates" class="headerlink" title="Implicit Interpretation of Importance Weight Aware Updates"></a>Implicit Interpretation of Importance Weight Aware Updates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11955">http://arxiv.org/abs/2307.11955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keyi Chen, Francesco Orabona</li>
<li>for: 这篇论文主要是为了解释importance weight aware（IWA）更新法的性能优劣。</li>
<li>methods: 论文使用了一种新的框架，即通用隐式跟踪领导者（FTRL），来分析通用隐式更新法。</li>
<li>results: 论文表明，IWA更新法在在线学习设置中具有更好的 regret upper bound，比plain gradient更新法更好。<details>
<summary>Abstract</summary>
Due to its speed and simplicity, subgradient descent is one of the most used optimization algorithms in convex machine learning algorithms. However, tuning its learning rate is probably its most severe bottleneck to achieve consistent good performance. A common way to reduce the dependency on the learning rate is to use implicit/proximal updates. One such variant is the Importance Weight Aware (IWA) updates, which consist of infinitely many infinitesimal updates on each loss function. However, IWA updates' empirical success is not completely explained by their theory. In this paper, we show for the first time that IWA updates have a strictly better regret upper bound than plain gradient updates in the online learning setting. Our analysis is based on the new framework, generalized implicit Follow-the-Regularized-Leader (FTRL) (Chen and Orabona, 2023), to analyze generalized implicit updates using a dual formulation. In particular, our results imply that IWA updates can be considered as approximate implicit/proximal updates.
</details>
<details>
<summary>摘要</summary>
由于其速度和简洁性，剪梯下降是机器学习中最常用的优化算法之一。然而，调整学习率是它最严重的瓶颈，以实现一致的好表现。一种常见的方法是使用隐式/辅助更新。一种such variant是重要性评估（IWA）更新，它们包括无限多个infinitesimal更新。然而， IWA更新的实际成功并不完全由其理论来解释。在这篇论文中，我们展示了IWA更新在在线学习 Setting中具有更好的 regret upper bound，比普通的梯度更新更好。我们的分析基于新的框架，通用隐式 Follow-the-Regularized-Leader（FTRL）（Chen和Orabona，2023），用于分析通用隐式更新。特别是，我们的结果表明，IWA更新可以被视为approximate隐式/辅助更新。
</details></li>
</ul>
<hr>
<h2 id="On-Robot-Bayesian-Reinforcement-Learning-for-POMDPs"><a href="#On-Robot-Bayesian-Reinforcement-Learning-for-POMDPs" class="headerlink" title="On-Robot Bayesian Reinforcement Learning for POMDPs"></a>On-Robot Bayesian Reinforcement Learning for POMDPs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11954">http://arxiv.org/abs/2307.11954</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hai Nguyen, Sammie Katt, Yuchen Xiao, Christopher Amato</li>
<li>for: 这篇论文的目的是提出一种专门适用于物理系统的 bayesian 强化学习方法，以解决 robot 学习中的数据成本问题。</li>
<li>methods: 该方法使用了一种特殊的 factored 表示方法，以捕捉专家知识，并使用 Monte-Carlo tree search 和 particle filtering 来解决 posterior 的推理问题。</li>
<li>results: 在两个人机交互任务中，该方法可以在几个实际世界回合后达到 near-optimal 性能，并且可以利用 typical low-level robot simulators 和处理未知环境的不确定性。<details>
<summary>Abstract</summary>
Robot learning is often difficult due to the expense of gathering data. The need for large amounts of data can, and should, be tackled with effective algorithms and leveraging expert information on robot dynamics. Bayesian reinforcement learning (BRL), thanks to its sample efficiency and ability to exploit prior knowledge, is uniquely positioned as such a solution method. Unfortunately, the application of BRL has been limited due to the difficulties of representing expert knowledge as well as solving the subsequent inference problem. This paper advances BRL for robotics by proposing a specialized framework for physical systems. In particular, we capture this knowledge in a factored representation, then demonstrate the posterior factorizes in a similar shape, and ultimately formalize the model in a Bayesian framework. We then introduce a sample-based online solution method, based on Monte-Carlo tree search and particle filtering, specialized to solve the resulting model. This approach can, for example, utilize typical low-level robot simulators and handle uncertainty over unknown dynamics of the environment. We empirically demonstrate its efficiency by performing on-robot learning in two human-robot interaction tasks with uncertainty about human behavior, achieving near-optimal performance after only a handful of real-world episodes. A video of learned policies is at https://youtu.be/H9xp60ngOes.
</details>
<details>
<summary>摘要</summary>
机器人学习通常困难由于数据收集成本高昂。为了解决这问题，我们可以采用有效的算法和利用机器人动力学专家的知识。 bayesian reinforcement learning（BRL）因其样本效率高和可以利用先验知识而成为一种适用的解决方案。然而，BRL在应用中受到了专家知识表示和推理问题的限制。本文提出了一种特殊的框架，用于physical systems。我们通过 capture this knowledge in a factored representation，然后证明 posterior factorizes in a similar shape，并 ultimately formalize the model in a Bayesian framework。然后，我们引入了一种基于Monte-Carlo tree search和particle filtering的在线解决方法，专门用于解决这个模型。这种方法可以利用 typical low-level robot simulators and handle uncertainty over unknown dynamics of the environment。我们通过在两个人机交互任务中进行实验，demonstrate its efficiency，只需要几个真实世界回合就能够 дости得 near-optimal performance。视频 display learned policies at https://youtu.be/H9xp60ngOes.
</details></li>
</ul>
<hr>
<h2 id="HIQL-Offline-Goal-Conditioned-RL-with-Latent-States-as-Actions"><a href="#HIQL-Offline-Goal-Conditioned-RL-with-Latent-States-as-Actions" class="headerlink" title="HIQL: Offline Goal-Conditioned RL with Latent States as Actions"></a>HIQL: Offline Goal-Conditioned RL with Latent States as Actions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11949">http://arxiv.org/abs/2307.11949</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/seohongpark/hiql">https://github.com/seohongpark/hiql</a></li>
<li>paper_authors: Seohong Park, Dibya Ghosh, Benjamin Eysenbach, Sergey Levine</li>
<li>for: 这个论文旨在提出一种基于非监督学习的目标conditioned reinforcement learning算法，可以从无标签数据中学习。</li>
<li>methods: 该算法使用一个action-free value function，通过层次分解来学习两个策略：一个高级策略使得状态被看作动作，预测子目标，以及一个低级策略预测达到子目标的行动。</li>
<li>results: 通过分析和实践示例， authors表明该层次分解使得其方法具有对噪音估计值函数的 Robustness。然后，通过应用该方法于offline目标 дости达标准别件，authors证明其方法可以解决远程目标任务，可以扩展到高维图像观察数据，并可以充分利用无动作数据。<details>
<summary>Abstract</summary>
Unsupervised pre-training has recently become the bedrock for computer vision and natural language processing. In reinforcement learning (RL), goal-conditioned RL can potentially provide an analogous self-supervised approach for making use of large quantities of unlabeled (reward-free) data. However, building effective algorithms for goal-conditioned RL that can learn directly from diverse offline data is challenging, because it is hard to accurately estimate the exact value function for faraway goals. Nonetheless, goal-reaching problems exhibit structure, such that reaching distant goals entails first passing through closer subgoals. This structure can be very useful, as assessing the quality of actions for nearby goals is typically easier than for more distant goals. Based on this idea, we propose a hierarchical algorithm for goal-conditioned RL from offline data. Using one action-free value function, we learn two policies that allow us to exploit this structure: a high-level policy that treats states as actions and predicts (a latent representation of) a subgoal and a low-level policy that predicts the action for reaching this subgoal. Through analysis and didactic examples, we show how this hierarchical decomposition makes our method robust to noise in the estimated value function. We then apply our method to offline goal-reaching benchmarks, showing that our method can solve long-horizon tasks that stymie prior methods, can scale to high-dimensional image observations, and can readily make use of action-free data. Our code is available at https://seohong.me/projects/hiql/
</details>
<details>
<summary>摘要</summary>
现代计算机视觉和自然语言处理领域中，无监督预训练已经成为核心。在奖励学习（RL）领域，目标受控RL可能提供一种类似的自我监督方法，使用大量无奖数据进行学习。然而，建立有效的目标受控RL算法，直接从多样化的离线数据中学习，是一项挑战。这是因为，难以准确地估计远距离目标的价值函数。然而，目标达成问题具有结构，即达到远距离目标需要先通过更近的亚目标。这种结构可以很有用，因为评估近距离目标的动作质量通常比远距离目标更容易。基于这个想法，我们提出了一种层次算法 для目标受控RL。使用一个没有动作的价值函数，我们学习了两个政策：一个高级政策，将状态看作动作，预测（一个隐藏表示）亚目标，以及一个低级政策，预测用于达到亚目标的动作。通过分析和示例，我们证明了这种层次 decomposition 使我们的方法具有鲁棒性，可以抵抗估计值函数的噪声。然后，我们将我们的方法应用于离线目标达成标准，并证明了我们的方法可以解决长期任务，可以扩展到高维图像观察，并可以轻松地使用无动作数据。我们的代码可以在 <https://seohong.me/projects/hiql/> 上获取。
</details></li>
</ul>
<hr>
<h2 id="The-instabilities-of-large-learning-rate-training-a-loss-landscape-view"><a href="#The-instabilities-of-large-learning-rate-training-a-loss-landscape-view" class="headerlink" title="The instabilities of large learning rate training: a loss landscape view"></a>The instabilities of large learning rate training: a loss landscape view</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11948">http://arxiv.org/abs/2307.11948</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lawrence Wang, Stephen Roberts</li>
<li>for: 研究深度学习网络训练中大学习率的稳定性，特别是在大学习率下的训练过程中存在潜在的不稳定性。</li>
<li>methods: 通过分析梯度下降的矩阵Hessian matrix来研究深度学习网络训练过程中的不稳定性。</li>
<li>results: 发现在大学习率下的训练过程中出现了“景观平整”和“景观转移”这两种fenomenon，这两种现象与训练过程中的不稳定性息息相关。<details>
<summary>Abstract</summary>
Modern neural networks are undeniably successful. Numerous works study how the curvature of loss landscapes can affect the quality of solutions. In this work we study the loss landscape by considering the Hessian matrix during network training with large learning rates - an attractive regime that is (in)famously unstable. We characterise the instabilities of gradient descent, and we observe the striking phenomena of \textit{landscape flattening} and \textit{landscape shift}, both of which are intimately connected to the instabilities of training.
</details>
<details>
<summary>摘要</summary>
现代神经网络确实非常成功。许多研究表明损失函数的凹凸度可以影响解决方案的质量。在这篇文章中，我们研究训练神经网络时的损失函数地形，包括在大学习率下进行训练的情况。我们描述梯度下降的不稳定性，并观察到了各种phenomena，如“地形平整”和“地形转移”，这些现象与训练过程中的不稳定性密切相关。
</details></li>
</ul>
<hr>
<h2 id="Collaboratively-Learning-Linear-Models-with-Structured-Missing-Data"><a href="#Collaboratively-Learning-Linear-Models-with-Structured-Missing-Data" class="headerlink" title="Collaboratively Learning Linear Models with Structured Missing Data"></a>Collaboratively Learning Linear Models with Structured Missing Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11947">http://arxiv.org/abs/2307.11947</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Cheng, Gary Cheng, John Duchi</li>
<li>for: 这篇论文目的是解决多个代理（agent）协同学习最小二乘估计问题。每个代理都观察到不同的特征集（e.g., 感知器的分辨率不同）。我们想要协调代理，以生成每个代理最佳估计器。</li>
<li>methods: 我们提出了一种分布式、半监督的算法Collab，包括三步：本地训练、聚合和分布。我们的过程不需要交换标注数据，因此具有通信效率和在标注数据不可 accessible 的场景中使用。</li>
<li>results: 我们的方法在真实数据和 sintetic 数据上进行测试，并达到了 Nearly asymptotically local minimax 优化的水平，即在不交换标注数据的情况下，我们的方法与可以交换标注数据的优化方法相比，具有类似的性能。<details>
<summary>Abstract</summary>
We study the problem of collaboratively learning least squares estimates for $m$ agents. Each agent observes a different subset of the features$\unicode{x2013}$e.g., containing data collected from sensors of varying resolution. Our goal is to determine how to coordinate the agents in order to produce the best estimator for each agent. We propose a distributed, semi-supervised algorithm Collab, consisting of three steps: local training, aggregation, and distribution. Our procedure does not require communicating the labeled data, making it communication efficient and useful in settings where the labeled data is inaccessible. Despite this handicap, our procedure is nearly asymptotically local minimax optimal$\unicode{x2013}$even among estimators allowed to communicate the labeled data such as imputation methods. We test our method on real and synthetic data.
</details>
<details>
<summary>摘要</summary>
我们研究多 Agent 协同学习最小二乘估计问题。每个 Agent 观察不同的特征集合$\unicode{x2013}$例如，各种感知器的分辨率不同。我们的目标是在 Agent 之间协调，以生成每个 Agent 最佳估计器。我们提出了分布式、半监督的算法 Collab，包括三个步骤：本地训练、聚合和分布。我们的过程不需要通信标注数据，因此具有通信效率和在标注数据不可 accessible 的场景中使用。尽管这些限制，我们的过程仍然几乎极限本地最小最优$\unicode{x2013}$甚至与可以通信标注数据的估计器相比。我们在真实数据和 sintetic 数据上测试了我们的方法。
</details></li>
</ul>
<hr>
<h2 id="Batch-Clipping-and-Adaptive-Layerwise-Clipping-for-Differential-Private-Stochastic-Gradient-Descent"><a href="#Batch-Clipping-and-Adaptive-Layerwise-Clipping-for-Differential-Private-Stochastic-Gradient-Descent" class="headerlink" title="Batch Clipping and Adaptive Layerwise Clipping for Differential Private Stochastic Gradient Descent"></a>Batch Clipping and Adaptive Layerwise Clipping for Differential Private Stochastic Gradient Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11939">http://arxiv.org/abs/2307.11939</a></li>
<li>repo_url: None</li>
<li>paper_authors: Toan N. Nguyen, Phuong Ha Nguyen, Lam M. Nguyen, Marten Van Dijk<br>for: 这 paper 是为了提出一种新的权限保护技术，以保证 differential privacy 的实现。methods: 这 paper 使用了 Individual Clipping (IC) 和 Batch Clipping (BC) 两种方法来实现权限保护，并且引入了 Adaptive Layerwise Clipping (ALC) 方法来适应不同层的敏感度。results:  experiments 表明，使用 BC 和 ALC 可以使 Differential Private Stochastic Gradient Descent (DPSGD)  converge，而使用 IC 和 ALC 不能 converge。<details>
<summary>Abstract</summary>
Each round in Differential Private Stochastic Gradient Descent (DPSGD) transmits a sum of clipped gradients obfuscated with Gaussian noise to a central server which uses this to update a global model which often represents a deep neural network. Since the clipped gradients are computed separately, which we call Individual Clipping (IC), deep neural networks like resnet-18 cannot use Batch Normalization Layers (BNL) which is a crucial component in deep neural networks for achieving a high accuracy. To utilize BNL, we introduce Batch Clipping (BC) where, instead of clipping single gradients as in the orginal DPSGD, we average and clip batches of gradients. Moreover, the model entries of different layers have different sensitivities to the added Gaussian noise. Therefore, Adaptive Layerwise Clipping methods (ALC), where each layer has its own adaptively finetuned clipping constant, have been introduced and studied, but so far without rigorous DP proofs. In this paper, we propose {\em a new ALC and provide rigorous DP proofs for both BC and ALC}. Experiments show that our modified DPSGD with BC and ALC for CIFAR-$10$ with resnet-$18$ converges while DPSGD with IC and ALC does not.
</details>
<details>
<summary>摘要</summary>
每个轮次在差分私人梯度下降（DPSGD）中传输一个混合的梯度，其中包含 Gaussian 噪声，并将其发送到中央服务器，以更新一个全球模型，通常是深度神经网络。由于混合的梯度在不同层中计算，因此无法使用批处理正则化层（BNL），这是深度神经网络实现高精度的一个关键组件。为了使用 BNL，我们引入批量混合（BC），其中，而不是归一化单个梯度，我们平均混合批处理的梯度。此外，不同层的模型元素对添加的 Gaussian 噪声具有不同的感度。因此，我们引入自适应层wise混合方法（ALC），其中每个层有自己的自适应调整的混合常量。在本文中，我们提出了一种新的 ALC，并为 BC 和 ALC 提供了严格的 DP 证明。实验表明，我们修改了 DPSGD 的 BC 和 ALC，可以在 CIFAR-10 上使用 resnet-18 进行训练，而 DPSGD 的 IC 和 ALC 不能。
</details></li>
</ul>
<hr>
<h2 id="Mercer-Large-Scale-Kernel-Machines-from-Ridge-Function-Perspective"><a href="#Mercer-Large-Scale-Kernel-Machines-from-Ridge-Function-Perspective" class="headerlink" title="Mercer Large-Scale Kernel Machines from Ridge Function Perspective"></a>Mercer Large-Scale Kernel Machines from Ridge Function Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11925">http://arxiv.org/abs/2307.11925</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karol Dziedziul, Sergey Kryzhevich</li>
<li>for: 本文关注大规模kernel机器学习方面的Mercer kernel machines的推理方法，从ridge函数的角度出发，回顾林和拜访的结果。</li>
<li>methods: 本文使用了近期rachimi和recht（2008）的Random features for large-scale kernel machines，以及相关的Approximation Theory来研究哪些kernel可以被简化为一个恒等式的极值函数。</li>
<li>results: 本文发现了一些障碍使用这种方法的问题，并可能有各种应用在深度学习中，特别是图像处理等问题。<details>
<summary>Abstract</summary>
To present Mercer large-scale kernel machines from a ridge function perspective, we recall the results by Lin and Pinkus from Fundamentality of ridge functions. We consider the main theorem of the recent paper by Rachimi and Recht, 2008, Random features for large-scale kernel machines in terms of the Approximation Theory. We study which kernels can be approximated by a sum of cosine function products with arguments depending on $x$ and $y$ and present the obstacles of such an approach. The results of this article may have various applications in Deep Learning, especially in problems related to Image Processing.
</details>
<details>
<summary>摘要</summary>
要从ridge函数角度介绍Mercer大规模kernel机器，我们回忆了林和拜纳斯在基本性理论中的结果。我们考虑了2008年rachimi和 recht的论文《Random features for large-scale kernel machines in terms of Approximation Theory》中的主要定理。我们研究了可以通过cosine函数产品的叠加来近似kernel机器，其中Arguments取决于x和y坐标，并提出了这种方法的阻碍。这些结果可能在深度学习中有各种应用，特别是在图像处理问题中。
</details></li>
</ul>
<hr>
<h2 id="Selective-Perception-Optimizing-State-Descriptions-with-Reinforcement-Learning-for-Language-Model-Actors"><a href="#Selective-Perception-Optimizing-State-Descriptions-with-Reinforcement-Learning-for-Language-Model-Actors" class="headerlink" title="Selective Perception: Optimizing State Descriptions with Reinforcement Learning for Language Model Actors"></a>Selective Perception: Optimizing State Descriptions with Reinforcement Learning for Language Model Actors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11922">http://arxiv.org/abs/2307.11922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kolby Nottingham, Yasaman Razeghi, Kyungmin Kim, JB Lanier, Pierre Baldi, Roy Fox, Sameer Singh</li>
<li>for: 这个论文是为了研究如何使用自然语言处理技术来提高机器人和游戏中的决策过程。</li>
<li>methods: 该论文提出了一种自动选择简洁状态描述的方法，称为Brief Language INputs for DEcision-making Responses（BLINDER），它通过学习任务条件下的状态描述价值函数来选择描述。</li>
<li>results: 该论文在NetHack游戏和机器人 manipulate任务中实现了提高任务成功率、减少输入大小和计算成本、并且可以在不同的LLM actors之间进行泛化。<details>
<summary>Abstract</summary>
Large language models (LLMs) are being applied as actors for sequential decision making tasks in domains such as robotics and games, utilizing their general world knowledge and planning abilities. However, previous work does little to explore what environment state information is provided to LLM actors via language. Exhaustively describing high-dimensional states can impair performance and raise inference costs for LLM actors. Previous LLM actors avoid the issue by relying on hand-engineered, task-specific protocols to determine which features to communicate about a state and which to leave out. In this work, we propose Brief Language INputs for DEcision-making Responses (BLINDER), a method for automatically selecting concise state descriptions by learning a value function for task-conditioned state descriptions. We evaluate BLINDER on the challenging video game NetHack and a robotic manipulation task. Our method improves task success rate, reduces input size and compute costs, and generalizes between LLM actors.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:大型语言模型（LLM）在机器人和游戏等领域中作为决策演员，利用其通用世界知识和规划能力。然而，前一代工作几乎没有探讨在语言中提供环境状态信息给 LLM 演员的问题。描述高维状态的尝试可能会降低性能和提高 LLM 演员的推理成本。先前的 LLM 演员通常采用手动设计、任务特定的协议来确定要将哪些特征包含在状态描述中，并且哪些可以略去。在这项工作中，我们提出了简短语言输入 для决策响应（BLINDER）方法，通过学习任务条件下的状态描述价值函数来自动选择简洁的状态描述。我们在 NetHack 游戏和机器人搅拌任务上评估了 BLINDER。我们的方法可以提高任务成功率，减少输入大小和计算成本，并且可以在不同的 LLM 演员之间进行泛化。
</details></li>
</ul>
<hr>
<h2 id="Poverty-rate-prediction-using-multi-modal-survey-and-earth-observation-data"><a href="#Poverty-rate-prediction-using-multi-modal-survey-and-earth-observation-data" class="headerlink" title="Poverty rate prediction using multi-modal survey and earth observation data"></a>Poverty rate prediction using multi-modal survey and earth observation data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11921">http://arxiv.org/abs/2307.11921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simone Fobi, Manuel Cardona, Elliott Collins, Caleb Robinson, Anthony Ortiz, Tina Sederholm, Rahul Dodhia, Juan Lavista Ferres</li>
<li>for: 预测地区贫困率</li>
<li>methods:  combining household demographic and living standards survey questions with features derived from satellite imagery</li>
<li>results: 1)  inclusion of visual features reduces the mean error in poverty rate estimates from 4.09% to 3.88% 2)  the best performance – errors in poverty rate decrease from 4.09% to 3.71% 3) extracted visual features encode geographic and urbanization differences between regions.<details>
<summary>Abstract</summary>
This work presents an approach for combining household demographic and living standards survey questions with features derived from satellite imagery to predict the poverty rate of a region. Our approach utilizes visual features obtained from a single-step featurization method applied to freely available 10m/px Sentinel-2 surface reflectance satellite imagery. These visual features are combined with ten survey questions in a proxy means test (PMT) to estimate whether a household is below the poverty line. We show that the inclusion of visual features reduces the mean error in poverty rate estimates from 4.09% to 3.88% over a nationally representative out-of-sample test set. In addition to including satellite imagery features in proxy means tests, we propose an approach for selecting a subset of survey questions that are complementary to the visual features extracted from satellite imagery. Specifically, we design a survey variable selection approach guided by the full survey and image features and use the approach to determine the most relevant set of small survey questions to include in a PMT. We validate the choice of small survey questions in a downstream task of predicting the poverty rate using the small set of questions. This approach results in the best performance -- errors in poverty rate decrease from 4.09% to 3.71%. We show that extracted visual features encode geographic and urbanization differences between regions.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese translation:这项研究提出了一种方法，利用户户普查和卫星成像特征来预测地区贫困率。该方法使用10m/px Sentinel-2表面反射卫星成像中的视觉特征，与十个问题组成一个代表测试（PMT）来估算户户是否下于贫困线。包括卫星成像特征后，贫困率估计的平均错误率由4.09%降低到3.88%。此外，该方法还提出了一种方法，选择与卫星成像特征相关的小问题集，以便在预测贫困率的下游任务中使用。该方法根据全面调查和成像特征选择最相关的小问题集，并用这些问题集来预测贫困率。这种方法实现了最佳性能，贫困率估计错误率由4.09%降低到3.71%。此外，提取的视觉特征还含有地域和城市化差异。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Vulnerabilities-in-Interpretable-Deep-Learning-Systems-with-Query-Efficient-Black-box-Attacks"><a href="#Unveiling-Vulnerabilities-in-Interpretable-Deep-Learning-Systems-with-Query-Efficient-Black-box-Attacks" class="headerlink" title="Unveiling Vulnerabilities in Interpretable Deep Learning Systems with Query-Efficient Black-box Attacks"></a>Unveiling Vulnerabilities in Interpretable Deep Learning Systems with Query-Efficient Black-box Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11906">http://arxiv.org/abs/2307.11906</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eldor Abdukhamidov, Mohammed Abuhamad, Simon S. Woo, Eric Chan-Tin, Tamer Abuhmed</li>
<li>for: 保障深度学习系统的可靠性、可靠性和信任性，防止恶意攻击。</li>
<li>methods: 使用微生物遗传算法，基于黑盒测试，不需要目标模型和解释模型的先知知识。</li>
<li>results: 实验结果显示，这种攻击具有高成功率，使用挑战性示例和归因地幔，很难于探测。<details>
<summary>Abstract</summary>
Deep learning has been rapidly employed in many applications revolutionizing many industries, but it is known to be vulnerable to adversarial attacks. Such attacks pose a serious threat to deep learning-based systems compromising their integrity, reliability, and trust. Interpretable Deep Learning Systems (IDLSes) are designed to make the system more transparent and explainable, but they are also shown to be susceptible to attacks. In this work, we propose a novel microbial genetic algorithm-based black-box attack against IDLSes that requires no prior knowledge of the target model and its interpretation model. The proposed attack is a query-efficient approach that combines transfer-based and score-based methods, making it a powerful tool to unveil IDLS vulnerabilities. Our experiments of the attack show high attack success rates using adversarial examples with attribution maps that are highly similar to those of benign samples which makes it difficult to detect even by human analysts. Our results highlight the need for improved IDLS security to ensure their practical reliability.
</details>
<details>
<summary>摘要</summary>
深度学习在许多应用中得到了迅速的应用，但它知道是易受到敌意攻击的。这些攻击会对深度学习基于系统的完整性、可靠性和信任造成严重的威胁。可解释深度学习系统（IDLS）是为了使系统更加透明和可解释的，但它们也被证明是易受到攻击的。在这种工作中，我们提出了一种基于微生物遗传算法的黑盒攻击方法，不需要target模型和其解释模型的先前知识。我们的攻击方法结合了传递基本方法和分数基本方法，使其成为对IDLS的可靠性进行检测的强大工具。我们的实验表明，使用对抗例中的特征图可以达到高度的攻击成功率，并且这些特征图与正常样本的特征图几乎相同，使其具有难以检测的特点。我们的结果表明，为了确保IDLS的实际可靠性，需要进一步加强IDLS的安全性。
</details></li>
</ul>
<hr>
<h2 id="Model-Compression-Methods-for-YOLOv5-A-Review"><a href="#Model-Compression-Methods-for-YOLOv5-A-Review" class="headerlink" title="Model Compression Methods for YOLOv5: A Review"></a>Model Compression Methods for YOLOv5: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11904">http://arxiv.org/abs/2307.11904</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Jani, Jamil Fayyad, Younes Al-Younes, Homayoun Najjaran</li>
<li>for: 本文主要针对强化YOLO对象检测器的研究进行了概括，以便在资源有限的设备上部署。</li>
<li>methods: 本文主要考虑了网络剪辑和量化两种压缩方法，以减少模型的内存使用量和计算时间。</li>
<li>results: 通过对YOLOv5进行剪辑和量化处理，可以降低模型的内存使用量和计算时间，但是还存在一些 gap 需要进一步研究。<details>
<summary>Abstract</summary>
Over the past few years, extensive research has been devoted to enhancing YOLO object detectors. Since its introduction, eight major versions of YOLO have been introduced with the purpose of improving its accuracy and efficiency. While the evident merits of YOLO have yielded to its extensive use in many areas, deploying it on resource-limited devices poses challenges. To address this issue, various neural network compression methods have been developed, which fall under three main categories, namely network pruning, quantization, and knowledge distillation. The fruitful outcomes of utilizing model compression methods, such as lowering memory usage and inference time, make them favorable, if not necessary, for deploying large neural networks on hardware-constrained edge devices. In this review paper, our focus is on pruning and quantization due to their comparative modularity. We categorize them and analyze the practical results of applying those methods to YOLOv5. By doing so, we identify gaps in adapting pruning and quantization for compressing YOLOv5, and provide future directions in this area for further exploration. Among several versions of YOLO, we specifically choose YOLOv5 for its excellent trade-off between recency and popularity in literature. This is the first specific review paper that surveys pruning and quantization methods from an implementation point of view on YOLOv5. Our study is also extendable to newer versions of YOLO as implementing them on resource-limited devices poses the same challenges that persist even today. This paper targets those interested in the practical deployment of model compression methods on YOLOv5, and in exploring different compression techniques that can be used for subsequent versions of YOLO.
</details>
<details>
<summary>摘要</summary>
在过去几年，对 YOLO 对象检测器进行了广泛的研究，以提高其精度和效率。自其引入以来，共有八个主要版本的 YOLO 发布，以提高其精度和效率。虽然 YOLO 在许多领域得到了广泛的应用，但在资源有限的设备上部署它却存在挑战。为解决这个问题，各种神经网络压缩方法被开发出来，这些方法分为三个主要类别：网络剪辑、量化和知识传递。使用这些方法可以降低内存使用量和执行时间，这使得它们在硬件限制的边缘设备上进行部署变得有利可图。在本文中，我们将关注剪辑和量化，因为它们在可模块化方面比较出色。我们将这些方法进行分类和分析，并通过应用这些方法于 YOLOv5 来评估其实际效果。通过这些研究，我们可以了解剪辑和量化在 YOLOv5 上的应用存在哪些挑战，并提供未来研究的方向。在多个 YOLO 版本中，我们选择 YOLOv5，因为它在文献中的悠久度和受欢迎程度均很高。这是关于剪辑和量化方法在 YOLOv5 上的首个具体评估文章。我们的研究也可以扩展到 newer 版本的 YOLO，因为在资源有限的设备上部署它们也存在同样的挑战。本文适合那些关注实际部署模型压缩方法在 YOLOv5 上的人，以及想要探索不同的压缩技术，以应用于未来的 YOLO 版本。
</details></li>
</ul>
<hr>
<h2 id="Project-Florida-Federated-Learning-Made-Easy"><a href="#Project-Florida-Federated-Learning-Made-Easy" class="headerlink" title="Project Florida: Federated Learning Made Easy"></a>Project Florida: Federated Learning Made Easy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11899">http://arxiv.org/abs/2307.11899</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Madrigal Diaz, Andre Manoel, Jialei Chen, Nalin Singal, Robert Sim</li>
<li>for: This paper is written for machine learning engineers and application developers who want to deploy large-scale federated learning (FL) solutions across a heterogeneous device ecosystem.</li>
<li>methods: The paper presents a system architecture and software development kit (SDK) called Project Florida, which enables the deployment of FL solutions across a wide range of operating systems and hardware specifications. The paper also discusses the use of cloud-hosted infrastructure and task management interfaces to support the training process.</li>
<li>results: The paper presents illustrative experiments that demonstrate the system’s capabilities, including the ability to train machine learning models across a wide range of devices and the ability to scale the training process to accommodate a large number of client devices.<details>
<summary>Abstract</summary>
We present Project Florida, a system architecture and software development kit (SDK) enabling deployment of large-scale Federated Learning (FL) solutions across a heterogeneous device ecosystem. Federated learning is an approach to machine learning based on a strong data sovereignty principle, i.e., that privacy and security of data is best enabled by storing it at its origin, whether on end-user devices or in segregated cloud storage silos. Federated learning enables model training across devices and silos while the training data remains within its security boundary, by distributing a model snapshot to a client running inside the boundary, running client code to update the model, and then aggregating updated snapshots across many clients in a central orchestrator. Deploying a FL solution requires implementation of complex privacy and security mechanisms as well as scalable orchestration infrastructure. Scale and performance is a paramount concern, as the model training process benefits from full participation of many client devices, which may have a wide variety of performance characteristics. Project Florida aims to simplify the task of deploying cross-device FL solutions by providing cloud-hosted infrastructure and accompanying task management interfaces, as well as a multi-platform SDK supporting most major programming languages including C++, Java, and Python, enabling FL training across a wide range of operating system (OS) and hardware specifications. The architecture decouples service management from the FL workflow, enabling a cloud service provider to deliver FL-as-a-service (FLaaS) to ML engineers and application developers. We present an overview of Florida, including a description of the architecture, sample code, and illustrative experiments demonstrating system capabilities.
</details>
<details>
<summary>摘要</summary>
我们介绍项目“佛罗里达”，这是一个系统架构和软件开发包（SDK），它使得大规模联合学习（FL）解决方案可以在多种设备生态系统中部署。联合学习是一种基于强大数据主权原则的机器学习方法，即数据privacy和安全最好是在数据的原始位置保持，whether on end-user devices or in segregated cloud storage silos。联合学习可以在设备和存储silos之间进行模型训练，而不需要将数据传输到外部，只需在设备上运行客户端代码来更新模型，然后将更新后的模型集中到中央抽象器中。实现FL解决方案需要实施复杂的隐私和安全机制，以及可扩展的管理基础设施。因为模型训练过程需要全面参与多个客户端设备，这些设备可能有各种性能特点。项目“佛罗里达”目标是使得跨设备FL解决方案的部署变得更加简单，通过提供云主机的基础设施和相关的任务管理界面，以及支持多种主要编程语言，包括C++、Java和Python，以实现FL训练在多种操作系统和硬件特性上。架构解决方案的分离，使得云服务提供商可以提供FLaaS（联合学习 как服务），让机器学习工程师和应用程序开发人员快速搭建FL解决方案。我们将对项目“佛罗里达”进行概述，包括架构描述、示例代码和 ilustrative experiments，以示系统的能力。
</details></li>
</ul>
<hr>
<h2 id="Hindsight-DICE-Stable-Credit-Assignment-for-Deep-Reinforcement-Learning"><a href="#Hindsight-DICE-Stable-Credit-Assignment-for-Deep-Reinforcement-Learning" class="headerlink" title="Hindsight-DICE: Stable Credit Assignment for Deep Reinforcement Learning"></a>Hindsight-DICE: Stable Credit Assignment for Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11897">http://arxiv.org/abs/2307.11897</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/skandavaidyanath/credit-assignment">https://github.com/skandavaidyanath/credit-assignment</a></li>
<li>paper_authors: Akash Velu, Skanda Vaidyanath, Dilip Arumugam</li>
<li>for: 该文章为了解决奖励学习在缺乏评价反馈的环境中表现不佳问题，提出了一种基于追溯政策的方法。</li>
<li>methods: 该文章使用了现有的重要性抽样比例估计技术来稳定化和改进基于追溯政策的方法。</li>
<li>results: 该文章在各种环境中显示了稳定和高效的学习效果，并且可以在奖励学习中缓解奖励分配问题。<details>
<summary>Abstract</summary>
Oftentimes, environments for sequential decision-making problems can be quite sparse in the provision of evaluative feedback to guide reinforcement-learning agents. In the extreme case, long trajectories of behavior are merely punctuated with a single terminal feedback signal, leading to a significant temporal delay between the observation of a non-trivial reward and the individual steps of behavior culpable for achieving said reward. Coping with such a credit assignment challenge is one of the hallmark characteristics of reinforcement learning. While prior work has introduced the concept of hindsight policies to develop a theoretically moxtivated method for reweighting on-policy data by impact on achieving the observed trajectory return, we show that these methods experience instabilities which lead to inefficient learning in complex environments. In this work, we adapt existing importance-sampling ratio estimation techniques for off-policy evaluation to drastically improve the stability and efficiency of these so-called hindsight policy methods. Our hindsight distribution correction facilitates stable, efficient learning across a broad range of environments where credit assignment plagues baseline methods.
</details>
<details>
<summary>摘要</summary>
经常情况下，决策问题环境往往缺乏评价反馈，导致强化学习代理人受到很大的评价延迟。在极端情况下，长期行为只会被截止符号性的终端反馈信号刺激，从而导致行为减少的减少很大。强化学习面临着寄付问题的挑战。 Prior work已经引入了叫做前景政策的方法，以 theoretically moxtivated 方式重新权重on-policy数据，以便更好地评价 achieve  trajectory return。但我们发现这些方法会导致不稳定性，从而降低强化学习的效率。在这种情况下，我们采用了现有的重要性折衔估计技术，以改善这些叫做 hindsight 政策方法的稳定性和效率。我们的 hindsight 分布修正方法可以在各种缺乏寄付的环境中，稳定、高效地学习。
</details></li>
</ul>
<hr>
<h2 id="On-the-Vulnerability-of-Fairness-Constrained-Learning-to-Malicious-Noise"><a href="#On-the-Vulnerability-of-Fairness-Constrained-Learning-to-Malicious-Noise" class="headerlink" title="On the Vulnerability of Fairness Constrained Learning to Malicious Noise"></a>On the Vulnerability of Fairness Constrained Learning to Malicious Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11892">http://arxiv.org/abs/2307.11892</a></li>
<li>repo_url: None</li>
<li>paper_authors: Avrim Blum, Princewill Okoroafor, Aadirupa Saha, Kevin Stangl</li>
<li>For:  This paper studies the vulnerability of fairness-constrained learning to small amounts of malicious noise in the training data.* Methods: The paper uses randomized classifiers to mitigate the vulnerability of fairness-constrained learning to adversarial noise.* Results: The paper shows that for certain fairness notions, such as Demographic Parity, the loss in accuracy can be as low as $\Theta(\alpha)$ when the noise rate is small. For other fairness notions, such as Equal Opportunity, the loss in accuracy can be as low as $O(\sqrt{\alpha})$. The paper also shows that the loss in accuracy clusters into three natural regimes: $O(\alpha)$, $O(\sqrt{\alpha})$, and $O(1)$.Here’s the Chinese translation of the three points:* For: 这篇论文研究了受到训练数据中小量邪恶噪声影响的公平学习的敏感性。* Methods: 这篇论文使用Randomized classifier来减少公平学习受到邪恶噪声影响的敏感性。* Results: 这篇论文显示，对于某些公平性定义，如人口均衡，当噪声率小时，损失率可以为Theta（α）。对于其他公平性定义，如机会平等，损失率可以为O（√α）。论文还显示，损失率分布在三个自然的 régime中：O（α）、O（√α）和O(1)。<details>
<summary>Abstract</summary>
We consider the vulnerability of fairness-constrained learning to small amounts of malicious noise in the training data. Konstantinov and Lampert (2021) initiated the study of this question and presented negative results showing there exist data distributions where for several fairness constraints, any proper learner will exhibit high vulnerability when group sizes are imbalanced. Here, we present a more optimistic view, showing that if we allow randomized classifiers, then the landscape is much more nuanced. For example, for Demographic Parity we show we can incur only a $\Theta(\alpha)$ loss in accuracy, where $\alpha$ is the malicious noise rate, matching the best possible even without fairness constraints. For Equal Opportunity, we show we can incur an $O(\sqrt{\alpha})$ loss, and give a matching $\Omega(\sqrt{\alpha})$lower bound. In contrast, Konstantinov and Lampert (2021) showed for proper learners the loss in accuracy for both notions is $\Omega(1)$. The key technical novelty of our work is how randomization can bypass simple "tricks" an adversary can use to amplify his power. We also consider additional fairness notions including Equalized Odds and Calibration. For these fairness notions, the excess accuracy clusters into three natural regimes $O(\alpha)$,$O(\sqrt{\alpha})$ and $O(1)$. These results provide a more fine-grained view of the sensitivity of fairness-constrained learning to adversarial noise in training data.
</details>
<details>
<summary>摘要</summary>
我们考虑了公平性条件下的学习敏感性对小量邪恶训练数据的影响。 Konstantinov 和 Lampert (2021) 开始了这个研究，并发现了一些负的结果，表明在某些公平性条件下，任何合法的学习者都将具有高度敏感性，当群体大小不对称时。 在这里，我们提供了一个更optimistic的见解，表明如果允许随机分类器，则情况会变得更加细分。例如，对于人口均衡公平性，我们显示可以允许只有 $\Theta(\alpha)$ 的损失率，其中 $\alpha$ 是邪恶训练数据的损失率，与不具有公平性条件时相同。对于平等机会公平性，我们显示可以允许 $O(\sqrt{\alpha})$ 的损失率，并提供了对应的 $\Omega(\sqrt{\alpha})$ 下界。与 Konstantinov 和 Lampert (2021) 的结果相比，我们的结果显示，在合法学习者下，两个公平性条件的损失率都是 $\Omega(1)$。我们的技术新动向是如何使用随机性来绕过简单的邪恶攻击者可以使用的“套路”。我们还考虑了其他的公平性条件，包括平等机会和准确性。这些结果提供了训练数据中邪恶训练数据的影响的更细分的见解。
</details></li>
</ul>
<hr>
<h2 id="On-the-Universality-of-Linear-Recurrences-Followed-by-Nonlinear-Projections"><a href="#On-the-Universality-of-Linear-Recurrences-Followed-by-Nonlinear-Projections" class="headerlink" title="On the Universality of Linear Recurrences Followed by Nonlinear Projections"></a>On the Universality of Linear Recurrences Followed by Nonlinear Projections</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11888">http://arxiv.org/abs/2307.11888</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antonio Orvieto, Soham De, Caglar Gulcehre, Razvan Pascanu, Samuel L. Smith</li>
<li>for: 本研究目标是表明一种基于回归线性层的字符串模型（包括S4、S5和LRU），可以正确地模拟任何具有 suficiently  régulier不对称序列-到-序列映射。</li>
<li>methods: 本研究使用了扩展的回归层和位置wise多层感知器（MLPs）来模拟序列-to-序列映射。主要想法是看到回归层为压缩算法，可以准确地存储输入序列的信息到内部状态中，然后由高度表达的 MLP 进行处理。</li>
<li>results: 研究发现，这种模型可以将任何 suficiently  régulier不对称序列-to-序列映射 aproximated 到任何 desired 精度。<details>
<summary>Abstract</summary>
In this note (work in progress towards a full-length paper) we show that a family of sequence models based on recurrent linear layers~(including S4, S5, and the LRU) interleaved with position-wise multi-layer perceptrons~(MLPs) can approximate arbitrarily well any sufficiently regular non-linear sequence-to-sequence map. The main idea behind our result is to see recurrent layers as compression algorithms that can faithfully store information about the input sequence into an inner state, before it is processed by the highly expressive MLP.
</details>
<details>
<summary>摘要</summary>
在这份工作进度中（正在prepare一篇全长论文），我们展示了一家系列模型，该模型基于循环线性层（包括S4、S5和LRU）和位置层 wise多层感知器（MLP）。这种模型可以在任何足够规则的序列到序列映射中进行近似。我们的主要想法是看循环层为压缩算法，可以准确地将输入序列存储在内部状态中，然后由高度表达的 MLP 进行处理。
</details></li>
</ul>
<hr>
<h2 id="MORE-Measurement-and-Correlation-Based-Variational-Quantum-Circuit-for-Multi-classification"><a href="#MORE-Measurement-and-Correlation-Based-Variational-Quantum-Circuit-for-Multi-classification" class="headerlink" title="MORE: Measurement and Correlation Based Variational Quantum Circuit for Multi-classification"></a>MORE: Measurement and Correlation Based Variational Quantum Circuit for Multi-classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11875">http://arxiv.org/abs/2307.11875</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jindi0/more">https://github.com/jindi0/more</a></li>
<li>paper_authors: Jindi Wu, Tianjie Hu, Qun Li<br>for:MORE is a quantum multi-classifier that leverages the quantum information of a single readout qubit to perform multi-class classification tasks.methods:MORE uses a variational ansatz and quantum state tomography to reconstruct the readout state, and then employs variational quantum clustering and supervised learning to determine the mapping between input data and quantum labels.results:MORE achieves advanced performance in multi-class classification tasks despite using a simple ansatz and limited quantum resources, and outperforms traditional binary classifiers in certain scenarios.Here’s the Chinese translation of the three points:for:MORE 是一个使用单 readout qubit 进行多类别分类任务的量子多类别推断器。methods:MORE 使用量子状态测量来重建 readout 状态，然后使用量子推断 clustering 和 supervised learning 来决定输入数据和量子标签之间的映射。results:MORE 在多类别分类任务中获得进步的表现，即使使用简单的推断器和有限的量子资源，并在某些情况下超越传统的二进制推断器。<details>
<summary>Abstract</summary>
Quantum computing has shown considerable promise for compute-intensive tasks in recent years. For instance, classification tasks based on quantum neural networks (QNN) have garnered significant interest from researchers and have been evaluated in various scenarios. However, the majority of quantum classifiers are currently limited to binary classification tasks due to either constrained quantum computing resources or the need for intensive classical post-processing. In this paper, we propose an efficient quantum multi-classifier called MORE, which stands for measurement and correlation based variational quantum multi-classifier. MORE adopts the same variational ansatz as binary classifiers while performing multi-classification by fully utilizing the quantum information of a single readout qubit. To extract the complete information from the readout qubit, we select three observables that form the basis of a two-dimensional Hilbert space. We then use the quantum state tomography technique to reconstruct the readout state from the measurement results. Afterward, we explore the correlation between classes to determine the quantum labels for classes using the variational quantum clustering approach. Next, quantum label-based supervised learning is performed to identify the mapping between the input data and their corresponding quantum labels. Finally, the predicted label is determined by its closest quantum label when using the classifier. We implement this approach using the Qiskit Python library and evaluate it through extensive experiments on both noise-free and noisy quantum systems. Our evaluation results demonstrate that MORE, despite using a simple ansatz and limited quantum resources, achieves advanced performance.
</details>
<details>
<summary>摘要</summary>
量子计算在最近几年内已经显示了较大的承诺，尤其是对于计算密集的任务。例如，基于量子神经网络（QNN）的分类任务已经吸引了研究者的广泛关注，并在多个场景中进行了评估。然而，大多数量子分类器目前仅限于二进制分类任务，这可能是因为量子计算资源的限制或需要大量的经典后处理。在这篇论文中，我们提出了一种高效的量子多分类器，即MORE（测量和相关性基于量子多分类器）。MORE采用了同binary分类器一样的变量 ansatz，并在完全利用单个读取量子比特的量子信息上进行多分类。为了从读取量子比特中提取完整的信息，我们选择了三个观察量，它们构成了一个二维希尔伯特空间的基。然后，我们使用量子状态探测技术来重建读取状态。接着，我们研究分类关系来确定类别的量子标签，并使用量子分布式学习方法来确定输入数据与其相应的量子标签之间的映射。最后，我们使用类ifier来预测输入数据的标签。我们使用Qiskit Python库实现这种方法，并对噪声量子系统和噪声自由量子系统进行了广泛的实验评估。我们的评估结果表明，MORE，即使使用简单的 ansatz 和有限的量子资源，仍然可以达到高效的性能。
</details></li>
</ul>
<hr>
<h2 id="The-Looming-Threat-of-Fake-and-LLM-generated-LinkedIn-Profiles-Challenges-and-Opportunities-for-Detection-and-Prevention"><a href="#The-Looming-Threat-of-Fake-and-LLM-generated-LinkedIn-Profiles-Challenges-and-Opportunities-for-Detection-and-Prevention" class="headerlink" title="The Looming Threat of Fake and LLM-generated LinkedIn Profiles: Challenges and Opportunities for Detection and Prevention"></a>The Looming Threat of Fake and LLM-generated LinkedIn Profiles: Challenges and Opportunities for Detection and Prevention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11864">http://arxiv.org/abs/2307.11864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navid Ayoobi, Sadat Shahriar, Arjun Mukherjee<br>for:This paper is written to detect fake and Large Language Model (LLM)-generated profiles in the LinkedIn Online Social Network immediately upon registration and before establishing connections.methods:The paper introduces the Section and Subsection Tag Embedding (SSTE) method to enhance the discriminative characteristics of textual information provided in LinkedIn profiles for distinguishing between legitimate profiles and those created by imposters manually or by using an LLM. The paper also uses static and contextualized word embeddings, including GloVe, Flair, BERT, and RoBERTa.results:The suggested method can distinguish between legitimate and fake profiles with an accuracy of about 95% across all word embeddings. Additionally, the SSTE method has a promising accuracy for identifying LLM-generated profiles, with an accuracy of approximately 90% when only 20 LLM-generated profiles are added to the training set.<details>
<summary>Abstract</summary>
In this paper, we present a novel method for detecting fake and Large Language Model (LLM)-generated profiles in the LinkedIn Online Social Network immediately upon registration and before establishing connections. Early fake profile identification is crucial to maintaining the platform's integrity since it prevents imposters from acquiring the private and sensitive information of legitimate users and from gaining an opportunity to increase their credibility for future phishing and scamming activities. This work uses textual information provided in LinkedIn profiles and introduces the Section and Subsection Tag Embedding (SSTE) method to enhance the discriminative characteristics of these data for distinguishing between legitimate profiles and those created by imposters manually or by using an LLM. Additionally, the dearth of a large publicly available LinkedIn dataset motivated us to collect 3600 LinkedIn profiles for our research. We will release our dataset publicly for research purposes. This is, to the best of our knowledge, the first large publicly available LinkedIn dataset for fake LinkedIn account detection. Within our paradigm, we assess static and contextualized word embeddings, including GloVe, Flair, BERT, and RoBERTa. We show that the suggested method can distinguish between legitimate and fake profiles with an accuracy of about 95% across all word embeddings. In addition, we show that SSTE has a promising accuracy for identifying LLM-generated profiles, despite the fact that no LLM-generated profiles were employed during the training phase, and can achieve an accuracy of approximately 90% when only 20 LLM-generated profiles are added to the training set. It is a significant finding since the proliferation of several LLMs in the near future makes it extremely challenging to design a single system that can identify profiles created with various LLMs.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种新的方法，用于在 LinkedIn 在线社交网络上立即识别 fake 和 Large Language Model（LLM）生成的 profiless，并在注册后before establishing connections。早期识别假 profiless是维护平台的完整性的关键，因为它防止了假者从获取真正用户的私人和敏感信息，并从获得未来骗财活动的机会。本工作使用 LinkedIn  profiless 中提供的文本信息，并引入 Section and Subsection Tag Embedding（SSTE）方法，以增强这些数据的权威性，以分辨真实 profiless 和由假者或 LLM 生成的 profiless。此外，由于没有大量公开可用的 LinkedIn 数据集，我们自己收集了 3600 个 LinkedIn profiless 为我们的研究。我们将在研究用途上公开我们的数据集。这是，我们知道的， LinkedIn 上假账户检测的首个大规模公开数据集。在我们的 paradigm 中，我们评估了静止和 contextualized 单词嵌入，包括 GloVe、Flair、BERT 和 RoBERTa。我们显示，我们的方法可以在所有单词嵌入上分辨 true 和 fake profiless，准确率约为 95%。此外，我们还显示了 SSTE 在 LLM 生成 profiless 上的扩展性，即使在训练阶段没有使用 LLM 生成 profiless，可以达到约 90% 的准确率，只需要添加 20 个 LLM 生成 profiless 到训练集中。这是一项重要发现，因为未来几年内，许多 LLM 将在未来逐渐普及，设计一个系统可以识别由不同 LLM 生成的 profiless 将变得极其困难。
</details></li>
</ul>
<hr>
<h2 id="Data-Induced-Interactions-of-Sparse-Sensors"><a href="#Data-Induced-Interactions-of-Sparse-Sensors" class="headerlink" title="Data-Induced Interactions of Sparse Sensors"></a>Data-Induced Interactions of Sparse Sensors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11838">http://arxiv.org/abs/2307.11838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrei A. Klishin, J. Nathan Kutz, Krithika Manohar</li>
<li>for: 该论文旨在描述如何使用少量的感知器来重建复杂系统的状态，并且如何选择感知器的位置以实现最佳重建结果。</li>
<li>methods: 论文使用了基于异谱 interpolate 和 QR 分解的多种算法来优化感知器的位置，并通过统计物理学的狄耳诺模型来计算感知器之间的互动。</li>
<li>results: 论文通过计算数据引导的感知器互动的全景，可以结合外部选择标准和预测感知器更换的影响。<details>
<summary>Abstract</summary>
Large-dimensional empirical data in science and engineering frequently has low-rank structure and can be represented as a combination of just a few eigenmodes. Because of this structure, we can use just a few spatially localized sensor measurements to reconstruct the full state of a complex system. The quality of this reconstruction, especially in the presence of sensor noise, depends significantly on the spatial configuration of the sensors. Multiple algorithms based on gappy interpolation and QR factorization have been proposed to optimize sensor placement. Here, instead of an algorithm that outputs a singular "optimal" sensor configuration, we take a thermodynamic view to compute the full landscape of sensor interactions induced by the training data. The landscape takes the form of the Ising model in statistical physics, and accounts for both the data variance captured at each sensor location and the crosstalk between sensors. Mapping out these data-induced sensor interactions allows combining them with external selection criteria and anticipating sensor replacement impacts.
</details>
<details>
<summary>摘要</summary>
大量实际数据在科学和工程频繁具有低维结构，可以通过一些本地感知器来表示。由于这种结构，我们可以使用一些感知器来重建复杂系统的全部状态，尤其是在感知噪声存在时。多种基于异常 interpolate 和 QR 分解的算法已经被提出来优化感知器布局。而不是输出一个“最优”的感知器配置，我们在这里采用热力学视角计算整个感知器与训练数据之间的互动场景。这个场景采用牛顿模型来描述，考虑了每个感知器位置上采集数据的方差以及感知器之间的干扰。通过映射这些数据引起的感知器互动，我们可以与外部选择标准结合并预测感知器更换的影响。
</details></li>
</ul>
<hr>
<h2 id="eXplainable-Artificial-Intelligence-XAI-in-age-prediction-A-systematic-review"><a href="#eXplainable-Artificial-Intelligence-XAI-in-age-prediction-A-systematic-review" class="headerlink" title="eXplainable Artificial Intelligence (XAI) in age prediction: A systematic review"></a>eXplainable Artificial Intelligence (XAI) in age prediction: A systematic review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13704">http://arxiv.org/abs/2307.13704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alena Kalyakulina, Igor Yusipov</li>
<li>for: 这篇论文探讨了使用可解释人工智能（XAI）技术进行年龄预测任务的应用。</li>
<li>methods: 论文将XAI技术应用于不同的身体系统，进行系统化的文献综述。</li>
<li>results: 论文指出了XAI在医疗应用中的优点，特别是在年龄预测任务中。<details>
<summary>Abstract</summary>
eXplainable Artificial Intelligence (XAI) is now an important and essential part of machine learning, allowing to explain the predictions of complex models. XAI is especially required in risky applications, particularly in health care, where human lives depend on the decisions of AI systems. One area of medical research is age prediction and identification of biomarkers of aging and age-related diseases. However, the role of XAI in the age prediction task has not previously been explored directly. In this review, we discuss the application of XAI approaches to age prediction tasks. We give a systematic review of the works organized by body systems, and discuss the benefits of XAI in medical applications and, in particular, in the age prediction domain.
</details>
<details>
<summary>摘要</summary>
<<SYS>>可解释人工智能（XAI）现在是机器学习中非常重要和必需的一部分，允许解释复杂模型的预测。XAI特别在危险应用中需要，特别是在医疗领域，人工智能系统的决策直接关系到人们的生命。一个医学研究领域是年龄预测和衰老病症的生物标志物质的预测。然而，XAI在年龄预测任务中的角色没有直接探讨过。在这篇评论中，我们讨论了XAI方法在年龄预测任务中的应用。我们按照身体系统进行了系统性的综述，并讨论了医疗应用中XAI的优点和年龄预测领域中XAI的特点。>>>Note that Simplified Chinese is used here, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Hong Kong, Macau, and Taiwan.
</details></li>
</ul>
<hr>
<h2 id="PINNsFormer-A-Transformer-Based-Framework-For-Physics-Informed-Neural-Networks"><a href="#PINNsFormer-A-Transformer-Based-Framework-For-Physics-Informed-Neural-Networks" class="headerlink" title="PINNsFormer: A Transformer-Based Framework For Physics-Informed Neural Networks"></a>PINNsFormer: A Transformer-Based Framework For Physics-Informed Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11833">http://arxiv.org/abs/2307.11833</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adityalab/pinnsformer">https://github.com/adityalab/pinnsformer</a></li>
<li>paper_authors: Leo Zhiyuan Zhao, Xueying Ding, B. Aditya Prakash</li>
<li>for: 用于数值解 partial differential equations (PDEs) 的深度学习框架。</li>
<li>methods: 使用 Transformer 结构，并采用多头注意机制来捕捉 PDEs 中的时间关系。</li>
<li>results: 能够准确地 approximates PDEs 的解，并在不同场景下超过传统 PINNs 的表现，尽管具有较少的计算和存储成本。<details>
<summary>Abstract</summary>
Physics-Informed Neural Networks (PINNs) have emerged as a promising deep learning framework for approximating numerical solutions for partial differential equations (PDEs). While conventional PINNs and most related studies adopt fully-connected multilayer perceptrons (MLP) as the backbone structure, they have neglected the temporal relations in PDEs and failed to approximate the true solution. In this paper, we propose a novel Transformer-based framework, namely PINNsFormer, that accurately approximates PDEs' solutions by capturing the temporal dependencies with multi-head attention mechanisms in Transformer-based models. Instead of approximating point predictions, PINNsFormer adapts input vectors to pseudo sequences and point-wise PINNs loss to a sequential PINNs loss. In addition, PINNsFormer is equipped with a novel activation function, namely Wavelet, which anticipates the Fourier decomposition through deep neural networks. We empirically demonstrate PINNsFormer's ability to capture the PDE solutions for various scenarios, in which conventional PINNs have failed to learn. We also show that PINNsFormer achieves superior approximation accuracy on such problems than conventional PINNs with non-sensitive hyperparameters, in trade of marginal computational and memory costs, with extensive experiments.
</details>
<details>
<summary>摘要</summary>
physics-informed neural networks (PINNs) 已经出现为解决数学Physical laws的深度学习框架，但是传统的PINNs和大多数相关研究都是使用完全连接多层感知器(MLP)作为脊梁结构，这些结构忽略了PDEs中的时间关系，并且无法准确地预测解。在本文中，我们提出了一种新的Transformer-based框架，即PINNsFormer，可以准确地预测PDEs的解决方案，通过在Transformer-based模型中使用多头注意机制来捕捉PDEs中的时间相关性。而不是对点预测进行approximation，PINNsFormer将输入向量转化为pseudo序列，并将点级PINNs损失转化为sequential PINNs损失。此外，PINNsFormer还具有一种新的活动函数，即wavelet，该函数预测了深度神经网络中的Fourier分解。我们通过实验证明PINNsFormer可以在不同的情况下，包括传统PINNs无法学习的情况下，准确地预测PDEs的解决方案。此外，我们还证明PINNsFormer在这些问题上的 aproximation精度高于传统PINNs，但是与非敏感的计算和存储成本相比，PINNsFormer的计算和存储成本几乎是零的。
</details></li>
</ul>
<hr>
<h2 id="Differentially-Private-Heavy-Hitter-Detection-using-Federated-Analytics"><a href="#Differentially-Private-Heavy-Hitter-Detection-using-Federated-Analytics" class="headerlink" title="Differentially Private Heavy Hitter Detection using Federated Analytics"></a>Differentially Private Heavy Hitter Detection using Federated Analytics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11749">http://arxiv.org/abs/2307.11749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karan Chadha, Junye Chen, John Duchi, Vitaly Feldman, Hanieh Hashemi, Omid Javidbakht, Audra McMillan, Kunal Talwar</li>
<li>for: 增强 prefix-tree 算法 隐私检测 differentially private heavy hitter 性能。</li>
<li>methods: 提出了一种基于 adaptive hyperparameter tuning 算法，以满足计算、通信和隐私约束的多用户数据点检测。</li>
<li>results: 通过对 Reddit 数据集进行大量实验，发现该方法可以提高检测性能，同时满足计算、通信和隐私约束。<details>
<summary>Abstract</summary>
In this work, we study practical heuristics to improve the performance of prefix-tree based algorithms for differentially private heavy hitter detection. Our model assumes each user has multiple data points and the goal is to learn as many of the most frequent data points as possible across all users' data with aggregate and local differential privacy. We propose an adaptive hyperparameter tuning algorithm that improves the performance of the algorithm while satisfying computational, communication and privacy constraints. We explore the impact of different data-selection schemes as well as the impact of introducing deny lists during multiple runs of the algorithm. We test these improvements using extensive experimentation on the Reddit dataset~\cite{caldas2018leaf} on the task of learning the most frequent words.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们研究了使用前缀树基于算法来提高分布式隐私极大热点检测的实用规则。我们的模型假设每个用户有多个数据点，目标是通过聚合和本地隐私来学习所有用户数据中的最多频数据点。我们提议一种适应性hyperparameter调整算法，可以提高算法的性能，同时满足计算、通信和隐私约束。我们还研究了不同的数据选择方案以及在多次运行算法时引入拒绝列表的影响。我们对这些改进进行了广泛的实验，使用了Reddit数据集（Caldas et al., 2018），以学习最常见的单词。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Ad-Auction-Realism-Practical-Insights-Modeling-Implications"><a href="#Advancing-Ad-Auction-Realism-Practical-Insights-Modeling-Implications" class="headerlink" title="Advancing Ad Auction Realism: Practical Insights &amp; Modeling Implications"></a>Advancing Ad Auction Realism: Practical Insights &amp; Modeling Implications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11732">http://arxiv.org/abs/2307.11732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ming Chen, Sareh Nabi, Marciano Siniscalchi</li>
<li>for: 这个论文是为了研究当代在线广告拍卖中的四个实际特征，包括广告插播值和点击率因用户搜索词而异常，竞争者的数量和身份在拍卖过程中是未知的，广告主只能得到部分、汇总的反馈。</li>
<li>methods: 作者使用了对抗人工智能算法来模型广告主的行为，不受拍卖机制细节的影响。</li>
<li>results: 研究发现，在更加复杂的环境中，“软底”可以提高关键性能指标，而且可以在竞争者来自同一个人口群体时实现这一效果。此外，研究还证明了如何从观察拍卖价格中推断广告主价值分布，从而证明了这种方法在更加实际的拍卖Setting中的实际效果。<details>
<summary>Abstract</summary>
This paper proposes a learning model of online ad auctions that allows for the following four key realistic characteristics of contemporary online auctions: (1) ad slots can have different values and click-through rates depending on users' search queries, (2) the number and identity of competing advertisers are unobserved and change with each auction, (3) advertisers only receive partial, aggregated feedback, and (4) payment rules are only partially specified. We model advertisers as agents governed by an adversarial bandit algorithm, independent of auction mechanism intricacies. Our objective is to simulate the behavior of advertisers for counterfactual analysis, prediction, and inference purposes. Our findings reveal that, in such richer environments, "soft floors" can enhance key performance metrics even when bidders are drawn from the same population. We further demonstrate how to infer advertiser value distributions from observed bids, thereby affirming the practical efficacy of our approach even in a more realistic auction setting.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Ad slots can have different values and click-through rates depending on users’ search queries.2. The number and identity of competing advertisers are unobserved and change with each auction.3. Advertisers only receive partial, aggregated feedback.4. Payment rules are only partially specified.We model advertisers as agents governed by an adversarial bandit algorithm, independent of auction mechanism intricacies. Our objective is to simulate the behavior of advertisers for counterfactual analysis, prediction, and inference purposes.Our findings show that “soft floors” can enhance key performance metrics even when bidders are drawn from the same population. Additionally, we demonstrate how to infer advertiser value distributions from observed bids, confirming the practical efficacy of our approach in a more realistic auction setting.</details></li>
</ol>
<hr>
<h2 id="Mitigating-Communications-Threats-in-Decentralized-Federated-Learning-through-Moving-Target-Defense"><a href="#Mitigating-Communications-Threats-in-Decentralized-Federated-Learning-through-Moving-Target-Defense" class="headerlink" title="Mitigating Communications Threats in Decentralized Federated Learning through Moving Target Defense"></a>Mitigating Communications Threats in Decentralized Federated Learning through Moving Target Defense</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11730">http://arxiv.org/abs/2307.11730</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/enriquetomasmb/fedstellar">https://github.com/enriquetomasmb/fedstellar</a></li>
<li>paper_authors: Enrique Tomás Martínez Beltrán, Pedro Miguel Sánchez Sánchez, Sergio López Bernal, Gérôme Bovet, Manuel Gil Pérez, Gregorio Martínez Pérez, Alberto Huertas Celdrán</li>
<li>for: This paper aims to address the communication security challenges in Decentralized Federated Learning (DFL) by introducing a security module that combines encryption and Moving Target Defense (MTD) techniques.</li>
<li>methods: The security module is implemented in a DFL platform called Fedstellar, and the authors evaluate the effectiveness of the module through experiments with the MNIST dataset and eclipse attacks.</li>
<li>results: The results show that the security module can mitigate the risks posed by eavesdropping or eclipse attacks, with an average F1 score of 95% and moderate increases in CPU usage and network traffic under the most secure configuration.Here’s the simplified Chinese text for the three points:</li>
<li>for: 这篇论文目的是解决分布式联合学习（DFL）中的通信安全挑战，通过引入加密和移动目标防御（MTD）技术的安全模块。</li>
<li>methods: 这个安全模块在分布式联合学习平台Fedstellar中实现，通过MNIST数据集和eclipse攻击进行测试。</li>
<li>results: 测试结果表明，安全模块可以降低防御 eclipse 攻击和窃听攻击的风险，实现了95%的平均F1分数，并且在最安全配置下，CPU使用率可以达到63.2% +-3.5%，网络流量可以达到230 MB +-15 MB。<details>
<summary>Abstract</summary>
The rise of Decentralized Federated Learning (DFL) has enabled the training of machine learning models across federated participants, fostering decentralized model aggregation and reducing dependence on a server. However, this approach introduces unique communication security challenges that have yet to be thoroughly addressed in the literature. These challenges primarily originate from the decentralized nature of the aggregation process, the varied roles and responsibilities of the participants, and the absence of a central authority to oversee and mitigate threats. Addressing these challenges, this paper first delineates a comprehensive threat model, highlighting the potential risks of DFL communications. In response to these identified risks, this work introduces a security module designed for DFL platforms to counter communication-based attacks. The module combines security techniques such as symmetric and asymmetric encryption with Moving Target Defense (MTD) techniques, including random neighbor selection and IP/port switching. The security module is implemented in a DFL platform called Fedstellar, allowing the deployment and monitoring of the federation. A DFL scenario has been deployed, involving eight physical devices implementing three security configurations: (i) a baseline with no security, (ii) an encrypted configuration, and (iii) a configuration integrating both encryption and MTD techniques. The effectiveness of the security module is validated through experiments with the MNIST dataset and eclipse attacks. The results indicated an average F1 score of 95%, with moderate increases in CPU usage (up to 63.2% +-3.5%) and network traffic (230 MB +-15 MB) under the most secure configuration, mitigating the risks posed by eavesdropping or eclipse attacks.
</details>
<details>
<summary>摘要</summary>
《协同学习的分布式协同学习（DFL）技术在训练机器学习模型方面带来了巨大的改变，使得多个参与者之间的模型协同学习可以实现，从而减少依赖于服务器。然而，这种方法引入了一些独特的通信安全挑战，这些挑战主要来自于协同学习过程的分布式特性，参与者的多样化角色和责任，以及缺乏中央权限来监管和处理威胁。为了解决这些挑战，本文首先提出了一个全面的威胁模型，描述了DFL通信的潜在风险。为应对这些风险，本工作提出了一个专门为DFL平台设计的安全模块，该模块结合了加密技术和移动目标防御（MTD）技术，包括随机 neighber 选择和IP/端口 switching。该安全模块在一个名为Fedstellar的DFL平台上实现，allowing the deployment and monitoring of the federation。一个DFL场景已经被部署，并在八个物理设备上实现了三种安全配置：（i）基eline with no security，（ii）加密配置，和（iii） integrate both encryption and MTD techniques。安全模块的有效性通过使用MNIST数据集和eclipse攻击进行实验 validate。结果显示，在最安全的配置下，模型的F1分数平均为95%，CPU使用率提高至63.2% ± 3.5%，网络流量增加至230 MB ± 15 MB。这些结果表明，通过加密和MTD技术，可以有效地防止遮断或eclipse攻击。
</details></li>
</ul>
<hr>
<h2 id="Local-Kernel-Renormalization-as-a-mechanism-for-feature-learning-in-overparametrized-Convolutional-Neural-Networks"><a href="#Local-Kernel-Renormalization-as-a-mechanism-for-feature-learning-in-overparametrized-Convolutional-Neural-Networks" class="headerlink" title="Local Kernel Renormalization as a mechanism for feature learning in overparametrized Convolutional Neural Networks"></a>Local Kernel Renormalization as a mechanism for feature learning in overparametrized Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11807">http://arxiv.org/abs/2307.11807</a></li>
<li>repo_url: None</li>
<li>paper_authors: R. Aiudi, R. Pacelli, A. Vezzani, R. Burioni, P. Rotondo</li>
<li>for: 这篇论文主要研究了深度神经网络中的特征学习方法，以及它们在不同类型的架构中的表现。</li>
<li>methods: 研究者使用了一种简单的理论框架，来解释FC和CNN架构中特征学习的不同表现。他们首先显示了一个有限宽FC网络的泛化性能可以通过无穷宽网络来获得，并且提出了一种有限宽效果行动来描述CNN架构中的特征学习。</li>
<li>results: 研究者发现了一种简单的特征学习机制，它只能在浅层CNN中发生，而不是在浅层FC网络或者无Weight连接神经网络中。这种机制导致CNN架构在有限宽 régime中表现优秀，而FC网络则是在无穷宽 régime中表现优秀。<details>
<summary>Abstract</summary>
Feature learning, or the ability of deep neural networks to automatically learn relevant features from raw data, underlies their exceptional capability to solve complex tasks. However, feature learning seems to be realized in different ways in fully-connected (FC) or convolutional architectures (CNNs). Empirical evidence shows that FC neural networks in the infinite-width limit eventually outperform their finite-width counterparts. Since the kernel that describes infinite-width networks does not evolve during training, whatever form of feature learning occurs in deep FC architectures is not very helpful in improving generalization. On the other hand, state-of-the-art architectures with convolutional layers achieve optimal performances in the finite-width regime, suggesting that an effective form of feature learning emerges in this case. In this work, we present a simple theoretical framework that provides a rationale for these differences, in one hidden layer networks. First, we show that the generalization performance of a finite-width FC network can be obtained by an infinite-width network, with a suitable choice of the Gaussian priors. Second, we derive a finite-width effective action for an architecture with one convolutional hidden layer and compare it with the result available for FC networks. Remarkably, we identify a completely different form of kernel renormalization: whereas the kernel of the FC architecture is just globally renormalized by a single scalar parameter, the CNN kernel undergoes a local renormalization, meaning that the network can select the local components that will contribute to the final prediction in a data-dependent way. This finding highlights a simple mechanism for feature learning that can take place in overparametrized shallow CNNs, but not in shallow FC architectures or in locally connected neural networks without weight sharing.
</details>
<details>
<summary>摘要</summary>
“特征学习”，也就是深度神经网络自动从原始数据中学习到 relevante 特征的能力，是深度神经网络解决复杂任务的关键。然而，在完全连接（FC）或卷积（CNN）架构中，特征学习似乎存在不同的实现方式。实际证明表明，在无穷宽限制下，FC神经网络 eventually 超越其固定宽度 counterparts。由于无穷宽网络的kernel不会在训练过程中进行变化，因此深度FC架构中的特征学习不会对泛化提供帮助。相反，当前领域的状态艺术架构，卷积层 achiev 最佳性能，表明在这种情况下，特征学习会出现有效的形式。在这种情况下，我们提出了一个简单的理论框架，用于解释这些差异。首先，我们证明了一个有限宽FC网络的泛化性能可以通过无穷宽网络来获得，并且需要一个适当的高斯先验。其次，我们 deriv 有限宽效果动作，并与FC网络的结果进行比较。意外地，我们发现了一种完全不同的kernel renormalization：FC架构的kernel仅受到全局抽象，而CNN架构的kernel则会在数据依赖的方式进行本地抽象，这意味着网络可以在数据中选择本地组分，以便在数据依赖的方式进行预测。这种发现高光了一种简单的特征学习机制，可以在过参神经网络中发生，但不可以在FC架构中或者在没有权重共享的本地神经网络中发生。
</details></li>
</ul>
<hr>
<h2 id="Convergence-of-SGD-for-Training-Neural-Networks-with-Sliced-Wasserstein-Losses"><a href="#Convergence-of-SGD-for-Training-Neural-Networks-with-Sliced-Wasserstein-Losses" class="headerlink" title="Convergence of SGD for Training Neural Networks with Sliced Wasserstein Losses"></a>Convergence of SGD for Training Neural Networks with Sliced Wasserstein Losses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11714">http://arxiv.org/abs/2307.11714</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eloi Tanguy</li>
<li>for: 本研究的目的是提供对 fixes step SGD 在 SW 损失函数上的分布学习模型 parameters 的趋势，并对这种方法的有效性进行 теории保证。</li>
<li>methods: 本研究使用了 Bianchi et al. (2022) 所提出的非平滑非对称函数下 SGD 的渐进结果，并在这种 Setting 中进行了实际的应用。</li>
<li>results: 研究发现，随着步长减小，SGD 轨迹会接近 (sub) 导流方程，并且在更加严格的假设下，SGD 轨迹会在极限下 approaching 泛化极点。<details>
<summary>Abstract</summary>
Optimal Transport has sparked vivid interest in recent years, in particular thanks to the Wasserstein distance, which provides a geometrically sensible and intuitive way of comparing probability measures. For computational reasons, the Sliced Wasserstein (SW) distance was introduced as an alternative to the Wasserstein distance, and has seen uses for training generative Neural Networks (NNs). While convergence of Stochastic Gradient Descent (SGD) has been observed practically in such a setting, there is to our knowledge no theoretical guarantee for this observation. Leveraging recent works on convergence of SGD on non-smooth and non-convex functions by Bianchi et al. (2022), we aim to bridge that knowledge gap, and provide a realistic context under which fixed-step SGD trajectories for the SW loss on NN parameters converge. More precisely, we show that the trajectories approach the set of (sub)-gradient flow equations as the step decreases. Under stricter assumptions, we show a much stronger convergence result for noised and projected SGD schemes, namely that the long-run limits of the trajectories approach a set of generalised critical points of the loss function.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="JoinGym-An-Efficient-Query-Optimization-Environment-for-Reinforcement-Learning"><a href="#JoinGym-An-Efficient-Query-Optimization-Environment-for-Reinforcement-Learning" class="headerlink" title="JoinGym: An Efficient Query Optimization Environment for Reinforcement Learning"></a>JoinGym: An Efficient Query Optimization Environment for Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11704">http://arxiv.org/abs/2307.11704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaiwen Wang, Junxiong Wang, Yueying Li, Nathan Kallus, Immanuel Trummer, Wen Sun</li>
<li>for: 本文提出了一个高效和轻量级的查询优化环境，用于应用智能学习（RL）。</li>
<li>methods: 本文使用了Markov决策过程（MDP）将左深和叶子变种的JoinOrder选择（JOS）问题转化为一个实际的数据管理问题，并提供了遵循标准Gymnasium API的实现。</li>
<li>results: 本文对各种RL算法进行了测试，并发现至少一种方法可以在训练集查询中near-优化性表现，但是在测试集查询中表现下降数个量级。这个差距驱动了进一步的研究，以确定RL算法在多任务 combinatorial优化问题中的泛化能力。<details>
<summary>Abstract</summary>
In this paper, we present \textsc{JoinGym}, an efficient and lightweight query optimization environment for reinforcement learning (RL). Join order selection (JOS) is a classic NP-hard combinatorial optimization problem from database query optimization and can serve as a practical testbed for the generalization capabilities of RL algorithms. We describe how to formulate each of the left-deep and bushy variants of the JOS problem as a Markov Decision Process (MDP), and we provide an implementation adhering to the standard Gymnasium API. We highlight that our implementation \textsc{JoinGym} is completely based on offline traces of all possible joins, which enables RL practitioners to easily and quickly test their methods on a realistic data management problem without needing to setup any systems. Moreover, we also provide all possible join traces on $3300$ novel SQL queries generated from the IMDB dataset. Upon benchmarking popular RL algorithms, we find that at least one method can obtain near-optimal performance on train-set queries but their performance degrades by several orders of magnitude on test-set queries. This gap motivates further research for RL algorithms that generalize well in multi-task combinatorial optimization problems.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一个高效和轻量级的查询优化环境，称为JoinGym，用于应急学习（RL）。Join order选择（JOS）是一个经典的NP困难的 combinatorial optimization问题，可以作为RL算法的总结能力的实际测试场景。我们描述了如何将左深和荔枝两种JOS问题转化为Markov决策过程（MDP），并提供了符合标准Gymnasium API的实现。我们指出，我们的实现基于全部可能的连接轨迹，使得RL专家可以轻松地和快速地在真实的数据管理问题上测试自己的方法，不需要设置任何系统。此外，我们还提供了3300个新的SQL查询，这些查询来自IMDB数据集。在 benchmarking 各种RL算法时，我们发现至少一种方法可以在训练集查询上获得近似优秀性能，但是它们在测试集查询上的性能却减少了几个数量级。这个差距激励了我们进一步研究RL算法在多任务 combinatorial optimization 问题中的总结能力。
</details></li>
</ul>
<hr>
<h2 id="Using-simulation-to-calibrate-real-data-acquisition-in-veterinary-medicine"><a href="#Using-simulation-to-calibrate-real-data-acquisition-in-veterinary-medicine" class="headerlink" title="Using simulation to calibrate real data acquisition in veterinary medicine"></a>Using simulation to calibrate real data acquisition in veterinary medicine</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11695">http://arxiv.org/abs/2307.11695</a></li>
<li>repo_url: None</li>
<li>paper_authors: Krystian Strzałka, Szymon Mazurek, Maciej Wielgosz, Paweł Russek, Jakub Caputa, Daria Łukasik, Jan Krupiński, Jakub Grzeszczyk, Michał Karwatowski, Rafał Frączek, Ernest Jamro, Marcin Pietroń, Sebastian Koryciak, Agnieszka Dąbrowska-Boruch, Kazimierz Wiatr</li>
<li>for: 这个研究旨在使用模拟环境提高动物医学数据收集和诊断，特点是通过使用Blender和Blenderproc库生成具有多种生物学、环境和行为条件的 sintetic数据集，并用这些数据集训练机器学习模型以识别正常和异常的步态。</li>
<li>methods: 这个研究使用了Blender和Blenderproc库生成 sintetic数据集，并使用这些数据集训练机器学习模型。两个不同的数据集，具有不同的摄像头角度细节，被创建以进一步研究摄像头角度对模型准确性的影响。</li>
<li>results: 初步结果表明，通过使用模拟环境和真实病人数据集的组合，这种基于模拟的方法可能会提高动物医学诊断的效果和效率。<details>
<summary>Abstract</summary>
This paper explores the innovative use of simulation environments to enhance data acquisition and diagnostics in veterinary medicine, focusing specifically on gait analysis in dogs. The study harnesses the power of Blender and the Blenderproc library to generate synthetic datasets that reflect diverse anatomical, environmental, and behavioral conditions. The generated data, represented in graph form and standardized for optimal analysis, is utilized to train machine learning algorithms for identifying normal and abnormal gaits. Two distinct datasets with varying degrees of camera angle granularity are created to further investigate the influence of camera perspective on model accuracy. Preliminary results suggest that this simulation-based approach holds promise for advancing veterinary diagnostics by enabling more precise data acquisition and more effective machine learning models. By integrating synthetic and real-world patient data, the study lays a robust foundation for improving overall effectiveness and efficiency in veterinary medicine.
</details>
<details>
<summary>摘要</summary>
这个研究paper explores the innovative use of simulation environments to enhance data acquisition and diagnostics in veterinary medicine, focusing specifically on gait analysis in dogs. The study harnesses the power of Blender and the Blenderproc library to generate synthetic datasets that reflect diverse anatomical, environmental, and behavioral conditions. The generated data, represented in graph form and standardized for optimal analysis, is utilized to train machine learning algorithms for identifying normal and abnormal gaits. Two distinct datasets with varying degrees of camera angle granularity are created to further investigate the influence of camera perspective on model accuracy. Preliminary results suggest that this simulation-based approach holds promise for advancing veterinary diagnostics by enabling more precise data acquisition and more effective machine learning models. By integrating synthetic and real-world patient data, the study lays a robust foundation for improving overall effectiveness and efficiency in veterinary medicine.Here's the text with traditional Chinese characters:这个研究paper explores the innovative use of simulation environments to enhance data acquisition and diagnostics in veterinary medicine, focusing specifically on gait analysis in dogs. The study harnesses the power of Blender and the Blenderproc library to generate synthetic datasets that reflect diverse anatomical, environmental, and behavioral conditions. The generated data, represented in graph form and standardized for optimal analysis, is utilized to train machine learning algorithms for identifying normal and abnormal gaits. Two distinct datasets with varying degrees of camera angle granularity are created to further investigate the influence of camera perspective on model accuracy. Preliminary results suggest that this simulation-based approach holds promise for advancing veterinary diagnostics by enabling more precise data acquisition and more effective machine learning models. By integrating synthetic and real-world patient data, the study lays a robust foundation for improving overall effectiveness and efficiency in veterinary medicine.
</details></li>
</ul>
<hr>
<h2 id="Fast-Adaptive-Test-Time-Defense-with-Robust-Features"><a href="#Fast-Adaptive-Test-Time-Defense-with-Robust-Features" class="headerlink" title="Fast Adaptive Test-Time Defense with Robust Features"></a>Fast Adaptive Test-Time Defense with Robust Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11672">http://arxiv.org/abs/2307.11672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anurag Singh, Mahalakshmi Sabanayagam, Krikamol Muandet, Debarghya Ghoshdastidar</li>
<li>for: 提高深度神经网络的对抗性性能</li>
<li>methods: 基于特征稳定性的抗击攻击策略</li>
<li>results: 在CIFAR-10和CIFAR-100数据集上，与现有最佳方法相比，提出的方法具有较低的计算成本，且对抗性性能较高。<details>
<summary>Abstract</summary>
Adaptive test-time defenses are used to improve the robustness of deep neural networks to adversarial examples. However, existing methods significantly increase the inference time due to additional optimization on the model parameters or the input at test time. In this work, we propose a novel adaptive test-time defense strategy that is easy to integrate with any existing (robust) training procedure without additional test-time computation. Based on the notion of robustness of features that we present, the key idea is to project the trained models to the most robust feature space, thereby reducing the vulnerability to adversarial attacks in non-robust directions. We theoretically show that the top eigenspace of the feature matrix are more robust for a generalized additive model and support our argument for a large width neural network with the Neural Tangent Kernel (NTK) equivalence. We conduct extensive experiments on CIFAR-10 and CIFAR-100 datasets for several robustness benchmarks, including the state-of-the-art methods in RobustBench, and observe that the proposed method outperforms existing adaptive test-time defenses at much lower computation costs.
</details>
<details>
<summary>摘要</summary>
使用适应性测试时防御，提高深度神经网络对攻击示例的Robustness。然而，现有方法会significantly增加测试时间，因为它们需要在测试时进行额外的优化模型参数或输入。在这项工作中，我们提出了一种新的适应测试时防御策略，可以轻松地与任何现有的Robust训练过程集成，无需额外的测试时间计算。我们基于特征空间的Robustness提出了一个新的思路，即将训练模型映射到最Robust的特征空间，以降低非Robust方向的攻击性。我们理论上显示，通过对特征矩阵的top射影空间进行投影，可以提高一般加法模型的Robustness。我们在CIFAR-10和CIFAR-100数据集上进行了广泛的实验，包括RobustBench状态OF-the-art方法，并观察到我们提出的方法在计算成本远低于现有适应测试时防御方法时仍能够获得更高的Robustness性。
</details></li>
</ul>
<hr>
<h2 id="An-Efficient-Interior-Point-Method-for-Online-Convex-Optimization"><a href="#An-Efficient-Interior-Point-Method-for-Online-Convex-Optimization" class="headerlink" title="An Efficient Interior-Point Method for Online Convex Optimization"></a>An Efficient Interior-Point Method for Online Convex Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11668">http://arxiv.org/abs/2307.11668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elad Hazan, Nimrod Megiddo</li>
<li>for: 这个论文是为了最小化在线凸优化中的遗弃量而写的。</li>
<li>methods: 这个论文使用了一种新的算法来最小化遗弃量，该算法是适应的，meaning its regret bounds hold not only for the time periods 1,…，T but also for every sub-interval s,s+1,…，t。</li>
<li>results: 这个论文的结果表明，该算法的遗弃量为O（√T log T），这是最小化遗弃量的下限，只有一个 logs 项。<details>
<summary>Abstract</summary>
A new algorithm for regret minimization in online convex optimization is described. The regret of the algorithm after $T$ time periods is $O(\sqrt{T \log T})$ - which is the minimum possible up to a logarithmic term. In addition, the new algorithm is adaptive, in the sense that the regret bounds hold not only for the time periods $1,\ldots,T$ but also for every sub-interval $s,s+1,\ldots,t$. The running time of the algorithm matches that of newly introduced interior point algorithms for regret minimization: in $n$-dimensional space, during each iteration the new algorithm essentially solves a system of linear equations of order $n$, rather than solving some constrained convex optimization problem in $n$ dimensions and possibly many constraints.
</details>
<details>
<summary>摘要</summary>
新的算法可以最小化 regret 在在线凸优化中描述。这个算法在 $T$ 时间段后的 regret 是 $O(\sqrt{T \log T})$，这是最低的，只有一个对数性 терMINOLOGY。此外，这个新算法是可适应的，意味着其 regret 约束不仅适用于时间段 $1,\ldots,T$，还适用于每个子时间段 $s,s+1,\ldots,t$。算法的运行时间与新引入的内部点算法一样，在 $n$ 维空间中，每次迭代中，新算法基本上解决了一个线性方程组问题，而不是解决一个凸优化问题并且可能有很多约束。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/22/cs.LG_2023_07_22/" data-id="clp89dog700oci7886gin1t2n" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/22/eess.IV_2023_07_22/" class="article-date">
  <time datetime="2023-07-22T09:00:00.000Z" itemprop="datePublished">2023-07-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/22/eess.IV_2023_07_22/">eess.IV - 2023-07-22</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Direct-atomic-number-reconstruction-of-dual-energy-cargo-radiographs-using-a-semiempirical-transparency-model"><a href="#Direct-atomic-number-reconstruction-of-dual-energy-cargo-radiographs-using-a-semiempirical-transparency-model" class="headerlink" title="Direct atomic number reconstruction of dual energy cargo radiographs using a semiempirical transparency model"></a>Direct atomic number reconstruction of dual energy cargo radiographs using a semiempirical transparency model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12099">http://arxiv.org/abs/2307.12099</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Lalor, Areg Danagoulian</li>
<li>for: 本研究旨在提高货物内容的检测能力，特别是检测敏感物品。</li>
<li>methods: 本研究使用高分辨率的原子数预测方法，通过最小化χ²错误来预测物品的原子数。此外，还包括一个整形步骤来提高物品的原子数选择性。</li>
<li>results: 研究表明，通过包含准确性检查步骤，可以在噪声入口图像上获得高精度的物品预测结果。此外，还可以根据屏障的性质来确定屏障的物品。<details>
<summary>Abstract</summary>
Dual energy cargo inspection systems are sensitive to both the area density and the atomic number of an imaged container due to the Z dependence of photon attenuation. The ability to identify cargo contents by their atomic number enables improved detection capabilities of illicit materials. Existing methods typically classify materials into a few material classes using an empirical calibration step. However, such a coarse label discretization limits atomic number selectivity and can yield inaccurate results if a material is near the midpoint of two bins. This work introduces a high resolution atomic number prediction method by minimizing the chi-squared error between measured transparency values and a semiempirical transparency model. Our previous work showed that by incorporating calibration step, the semiempirical transparency model can capture second order effects such as scattering. This method is benchmarked using two simulated radiographic phantoms, demonstrating the ability to obtain accurate material predictions on noisy input images by incorporating an image segmentation step. Furthermore, we show that this approach can be adapted to identify shielded objects after first determining the properties of the shielding, taking advantage of the closed-form nature of the transparency model.
</details>
<details>
<summary>摘要</summary>
双能量货物检测系统具有区域密度和原子数的敏感性，由光子吸收度随着Z值变化而受到影响。通过物质的原子数确定货物内容，可以提高披靡材料的检测能力。现有方法通常通过静默分类材料到几个物类来实现，但这会限制原子数选择性并可能导致结果不准确，如果材料处于两个极值点之间。这项工作介绍了高分辨率原子数预测方法，通过最小化χ²错误值来预测测量值和 semiempirical 透明性模型之间的差异。我们之前的工作表明，通过添加准确步骤， semiempirical 透明性模型可以捕捉到第二个效应，如散射。这种方法在使用两个模拟的放射学phantom中进行了测试，并显示了在噪声输入图像上获得高精度材料预测的能力。此外，我们还示出了在确定防护物的属性后，通过利用闭式形式的透明性模型，可以识别防护物。
</details></li>
</ul>
<hr>
<h2 id="On-the-Effectiveness-of-Spectral-Discriminators-for-Perceptual-Quality-Improvement"><a href="#On-the-Effectiveness-of-Spectral-Discriminators-for-Perceptual-Quality-Improvement" class="headerlink" title="On the Effectiveness of Spectral Discriminators for Perceptual Quality Improvement"></a>On the Effectiveness of Spectral Discriminators for Perceptual Quality Improvement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12027">http://arxiv.org/abs/2307.12027</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luciennnnnnn/dualformer">https://github.com/luciennnnnnn/dualformer</a></li>
<li>paper_authors: Xin Luo, Yunan Zhu, Shunxin Xu, Dong Liu</li>
<li>for: 本研究探讨了spectral discriminator在高品质图像生成中的应用，以提高SR图像质量。</li>
<li>methods: 本研究使用了GAN-based SR方法，并使用了spectral discriminator和ordinary discriminator进行评估。在高频范围内，spectral discriminator表现更好，而在低频范围内，ordinary discriminator表现更好。为了解决这个问题，我们提议同时使用spectral和ordinary discriminator。</li>
<li>results: 我们的方法可以更好地保持SR图像的spectrum，从而提高PD质量。此外，我们的ensembled discriminator可以更好地预测图像质量，并在无参图像质量评估任务中获得更高的准确率。<details>
<summary>Abstract</summary>
Several recent studies advocate the use of spectral discriminators, which evaluate the Fourier spectra of images for generative modeling. However, the effectiveness of the spectral discriminators is not well interpreted yet. We tackle this issue by examining the spectral discriminators in the context of perceptual image super-resolution (i.e., GAN-based SR), as SR image quality is susceptible to spectral changes. Our analyses reveal that the spectral discriminator indeed performs better than the ordinary (a.k.a. spatial) discriminator in identifying the differences in the high-frequency range; however, the spatial discriminator holds an advantage in the low-frequency range. Thus, we suggest that the spectral and spatial discriminators shall be used simultaneously. Moreover, we improve the spectral discriminators by first calculating the patch-wise Fourier spectrum and then aggregating the spectra by Transformer. We verify the effectiveness of the proposed method twofold. On the one hand, thanks to the additional spectral discriminator, our obtained SR images have their spectra better aligned to those of the real images, which leads to a better PD tradeoff. On the other hand, our ensembled discriminator predicts the perceptual quality more accurately, as evidenced in the no-reference image quality assessment task.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "perceptual image super-resolution" is translated as "图像超分辨" (tú zhì xiào fāng zhì)* "GAN-based SR" is translated as "基于GAN的SR" (jī yú GAN de SR)* "SR image quality" is translated as "超分辨图像质量" (tú zhì xiào fāng zhì tú yàng)* "spectral discriminator" is translated as "频谱检测器" (freqüência kēng cè qì)* "ordinary discriminator" is translated as "常规检测器" (cháng guī kēng cè qì)* "perceptual quality" is translated as "人类质量" (rén zhì zhì yè)* "no-reference image quality assessment task" is translated as "无参考图像质量评估任务" (wú jiàng qiào tú yàng zhì yè bìng xiǎng)
</details></li>
</ul>
<hr>
<h2 id="A-Cascade-Transformer-based-Model-for-3D-Dose-Distribution-Prediction-in-Head-and-Neck-Cancer-Radiotherapy"><a href="#A-Cascade-Transformer-based-Model-for-3D-Dose-Distribution-Prediction-in-Head-and-Neck-Cancer-Radiotherapy" class="headerlink" title="A Cascade Transformer-based Model for 3D Dose Distribution Prediction in Head and Neck Cancer Radiotherapy"></a>A Cascade Transformer-based Model for 3D Dose Distribution Prediction in Head and Neck Cancer Radiotherapy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12005">http://arxiv.org/abs/2307.12005</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ghtara/dose_prediction">https://github.com/ghtara/dose_prediction</a></li>
<li>paper_authors: Tara Gheshlaghi, Shahabedin Nabavi, Samire Shirzadikia, Mohsen Ebrahimi Moghaddam, Nima Rostampour</li>
<li>for: 这些研究旨在提高辐射疗法的规划效率和精度，并使用深度学习方法来预测辐射剂量分布图。</li>
<li>methods: 该研究使用了一种卷积encoder-decoder网络来实现器官风险分 segmentation，并使用另一种pyramid architecture来预测辐射剂量分布。</li>
<li>results: 该模型在一个自有的头颈癌 dataset上得到了0.79和2.71的Dice和HD95分数，分别高于现有的基eline。此外，该模型还在OpenKBP dataset上得到了2.77和1.79的辐射剂量和DVH分数，并且在链接auxiliary segmentation任务时表现更优异。<details>
<summary>Abstract</summary>
Radiation therapy is the primary method used to treat cancer in the clinic. Its goal is to deliver a precise dose to the planning target volume (PTV) while protecting the surrounding organs at risk (OARs). However, the traditional workflow used by dosimetrists to plan the treatment is time-consuming and subjective, requiring iterative adjustments based on their experience. Deep learning methods can be used to predict dose distribution maps to address these limitations. The study proposes a cascade model for organs at risk segmentation and dose distribution prediction. An encoder-decoder network has been developed for the segmentation task, in which the encoder consists of transformer blocks, and the decoder uses multi-scale convolutional blocks. Another cascade encoder-decoder network has been proposed for dose distribution prediction using a pyramid architecture. The proposed model has been evaluated using an in-house head and neck cancer dataset of 96 patients and OpenKBP, a public head and neck cancer dataset of 340 patients. The segmentation subnet achieved 0.79 and 2.71 for Dice and HD95 scores, respectively. This subnet outperformed the existing baselines. The dose distribution prediction subnet outperformed the winner of the OpenKBP2020 competition with 2.77 and 1.79 for dose and DVH scores, respectively. The predicted dose maps showed good coincidence with ground truth, with a superiority after linking with the auxiliary segmentation task. The proposed model outperformed state-of-the-art methods, especially in regions with low prescribed doses.
</details>
<details>
<summary>摘要</summary>
射频疗法是临床肿瘤治疗的主要方法。其目标是精确地对规划目标量（PTV）进行处理，保护周围的器官随机变化（OARs）。然而，传统的规划工作流程由剂理师进行，是时间consuming和主观的，需要迭代的调整基于它们的经验。深度学习方法可以用来预测射频分布图，解决这些限制。本研究提出了组织随机分布预测和射频分布预测的卷积模型。具有变数块的对话网络已经为分 Segmentation 任务开发，具有变数块的对话网络使用多尺度的对话网络。另一个组织随机分布预测的卷积模型已经提出，使用 pyramid 架构。本研究使用了96名患有头颈癌的患者的内部头颈癌数据集和OpenKBP，一个公共头颈癌数据集，进行评估。分割子网络获得了0.79和2.71的Dice和HD95分数，分别高于现有的基准。射频分布预测子网络高于OpenKBP2020大赛中的胜出者，具有2.77和1.79的射频和DVH分数，分别。预测的射频图表示与真实射频图有good的一致，并且在附加分 segmentation 任务下表现出superiority。本研究的模型在低剂量区域表现出了特别的优势，比如脑部和肺部。
</details></li>
</ul>
<hr>
<h2 id="ELiOT-End-to-end-Lidar-Odometry-using-Transformer-Framework"><a href="#ELiOT-End-to-end-Lidar-Odometry-using-Transformer-Framework" class="headerlink" title="ELiOT : End-to-end Lidar Odometry using Transformer Framework"></a>ELiOT : End-to-end Lidar Odometry using Transformer Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11998">http://arxiv.org/abs/2307.11998</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daegyu Lee, Hyunwoo Nam, D. Hyunchul Shim</li>
<li>for: 该文章为了提出一种基于深度学习的 LiDAR ODometry 方法。</li>
<li>methods: 该方法使用 transformer 架构，并通过自注意力流 embedding 网络来隐式地表示顺序 LiDAR 场景的运动。</li>
<li>results: 该方法在 urbane 数据集上显示了 Encouraging 的结果，即 translational 和 rotational 错误分别为 7.59% 和 2.67%。<details>
<summary>Abstract</summary>
In recent years, deep-learning-based point cloud registration methods have shown significant promise. Furthermore, learning-based 3D detectors have demonstrated their effectiveness in encoding semantic information from LiDAR data. In this paper, we introduce ELiOT, an end-to-end LiDAR odometry framework built on a transformer architecture. Our proposed Self-attention flow embedding network implicitly represents the motion of sequential LiDAR scenes, bypassing the need for 3D-2D projections traditionally used in such tasks. The network pipeline, composed of a 3D transformer encoder-decoder, has shown effectiveness in predicting poses on urban datasets. In terms of translational and rotational errors, our proposed method yields encouraging results, with 7.59% and 2.67% respectively on the KITTI odometry dataset. This is achieved with an end-to-end approach that foregoes the need for conventional geometric concepts.
</details>
<details>
<summary>摘要</summary>
Recently, deep learning-based point cloud registration methods have shown significant promise. Furthermore, learning-based 3D detectors have demonstrated their effectiveness in encoding semantic information from LiDAR data. In this paper, we introduce ELiOT, an end-to-end LiDAR odometry framework built on a transformer architecture. Our proposed Self-attention flow embedding network implicitly represents the motion of sequential LiDAR scenes, bypassing the need for 3D-2D projections traditionally used in such tasks. The network pipeline, composed of a 3D transformer encoder-decoder, has shown effectiveness in predicting poses on urban datasets. In terms of translational and rotational errors, our proposed method yields encouraging results, with 7.59% and 2.67% respectively on the KITTI odometry dataset. This is achieved with an end-to-end approach that foregoes the need for conventional geometric concepts.Here's a word-for-word translation of the text into Simplified Chinese:近年来，深度学习基于点云注册方法已经显示了很大的承诺。此外，学习基于LiDAR数据的3D探测器也证明了它们在编码 semantic信息方面的效iveness。在这篇文章中，我们介绍了 ELiOT，一个基于transformer架构的LiDAR odometry框架。我们的提出的Self-attention流embedding网络可以隐式地表示sequential LiDAR scene中的运动，不需要传统的3D-2D投影。这个框架由3D transformer编码器-解码器组成，在城市数据集上表现出了良好的效果。在翻译和旋转错误方面，我们的提出方法得到了鼓舞人的结果，分别为7.59%和2.67%在KITTI odometry数据集上。这是通过一个端到端方法来实现的，不需要传统的几何概念。
</details></li>
</ul>
<hr>
<h2 id="Topology-Preserving-Automatic-Labeling-of-Coronary-Arteries-via-Anatomy-aware-Connection-Classifier"><a href="#Topology-Preserving-Automatic-Labeling-of-Coronary-Arteries-via-Anatomy-aware-Connection-Classifier" class="headerlink" title="Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-aware Connection Classifier"></a>Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-aware Connection Classifier</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11959">http://arxiv.org/abs/2307.11959</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zutsusemi/miccai2023-topolab-labels">https://github.com/zutsusemi/miccai2023-topolab-labels</a></li>
<li>paper_authors: Zhixing Zhang, Ziwei Zhao, Dong Wang, Shishuang Zhao, Yuhang Liu, Jia Liu, Liwei Wang</li>
<li>for: 这个论文主要用于提出了一种新的涵围 coronary artery 自动标注框架，以便在冠状动脉疾病诊断过程中准确地标注 coronary artery。</li>
<li>methods: 这个框架叫做 TopoLab，它在网络设计中直接嵌入了解剖学连接的知识。具体来说，这个框架使用了内部 segment 特征聚合和间 segment 特征互动来进行层次特征提取。此外，我们还提出了一种基于解剖学连接的连接类别分类器，以便将每个连接对应的 segment 对应到正确的类别中。</li>
<li>results: 我们在公共 orCaScore 数据集上提供了高质量的涵围 coronary artery 标注数据，并在 orCaScore 数据集和一个内部数据集上进行了实验。结果显示，我们的 TopoLab 已经实现了领先的性能。<details>
<summary>Abstract</summary>
Automatic labeling of coronary arteries is an essential task in the practical diagnosis process of cardiovascular diseases. For experienced radiologists, the anatomically predetermined connections are important for labeling the artery segments accurately, while this prior knowledge is barely explored in previous studies. In this paper, we present a new framework called TopoLab which incorporates the anatomical connections into the network design explicitly. Specifically, the strategies of intra-segment feature aggregation and inter-segment feature interaction are introduced for hierarchical segment feature extraction. Moreover, we propose the anatomy-aware connection classifier to enable classification for each connected segment pair, which effectively exploits the prior topology among the arteries with different categories. To validate the effectiveness of our method, we contribute high-quality annotations of artery labeling to the public orCaScore dataset. The experimental results on both the orCaScore dataset and an in-house dataset show that our TopoLab has achieved state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
自动标注 coronary artery 是冠артериосclerosis 诊断过程中的关键任务。经验丰富的放射学家可以借助 anatomical connections 准确地标注 artery segments，然而这一知识在前一些研究中几乎未被探讨。在这篇论文中，我们提出了一种新的框架 called TopoLab，该框架将 anatomical connections 直接包含到网络设计中。具体来说，我们引入了 intra-segment feature aggregation 和 inter-segment feature interaction 策略，以实现 hierarchical segment feature extraction。此外，我们还提出了 anatomy-aware connection classifier，以便对每个连接 segment pair 进行分类，从而有效利用了不同类别 artery 之间的先天 topology。为验证我们的方法的有效性，我们在公共 orCaScore 数据集上提供了高质量的 annotations of artery labeling。实验结果表明，我们的 TopoLab 在 orCaScore 数据集和自有数据集上均 achieved state-of-the-art performance。
</details></li>
</ul>
<hr>
<h2 id="PartDiff-Image-Super-resolution-with-Partial-Diffusion-Models"><a href="#PartDiff-Image-Super-resolution-with-Partial-Diffusion-Models" class="headerlink" title="PartDiff: Image Super-resolution with Partial Diffusion Models"></a>PartDiff: Image Super-resolution with Partial Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11926">http://arxiv.org/abs/2307.11926</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Zhao, Alex Ling Yu Hung, Kaifeng Pang, Haoxin Zheng, Kyunghyun Sung</li>
<li>for: 这个论文主要是为了提高图像超分辨率生成task中的性能。</li>
<li>methods: 论文使用了Diffusion Probabilistic Models（DDPMs）来生成图像，通过学习逆转数据分布的演化过程，将数据从普通的噪声演化成高质量的图像。</li>
<li>results: 论文的实验表明，在使用Partial Diffusion Model（PartDiff）方法时，可以significantly reduce the number of denoising steps without sacrificing the quality of generation，相比于传统的 diffusion-based super-resolution methods。<details>
<summary>Abstract</summary>
Denoising diffusion probabilistic models (DDPMs) have achieved impressive performance on various image generation tasks, including image super-resolution. By learning to reverse the process of gradually diffusing the data distribution into Gaussian noise, DDPMs generate new data by iteratively denoising from random noise. Despite their impressive performance, diffusion-based generative models suffer from high computational costs due to the large number of denoising steps.In this paper, we first observed that the intermediate latent states gradually converge and become indistinguishable when diffusing a pair of low- and high-resolution images. This observation inspired us to propose the Partial Diffusion Model (PartDiff), which diffuses the image to an intermediate latent state instead of pure random noise, where the intermediate latent state is approximated by the latent of diffusing the low-resolution image. During generation, Partial Diffusion Models start denoising from the intermediate distribution and perform only a part of the denoising steps. Additionally, to mitigate the error caused by the approximation, we introduce "latent alignment", which aligns the latent between low- and high-resolution images during training. Experiments on both magnetic resonance imaging (MRI) and natural images show that, compared to plain diffusion-based super-resolution methods, Partial Diffusion Models significantly reduce the number of denoising steps without sacrificing the quality of generation.
</details>
<details>
<summary>摘要</summary>
diffeomorphism probabilistic models (DDPMs)  haben achieved impressive performance on various image generation tasks, including image super-resolution. By learning to reverse the process of gradually diffusing the data distribution into Gaussian noise, DDPMs generate new data by iteratively denoising from random noise. Despite their impressive performance, diffusion-based generative models suffer from high computational costs due to the large number of denoising steps.In this paper, we first observed that the intermediate latent states gradually converge and become indistinguishable when diffusing a pair of low- and high-resolution images. This observation inspired us to propose the Partial Diffusion Model (PartDiff), which diffuses the image to an intermediate latent state instead of pure random noise, where the intermediate latent state is approximated by the latent of diffusing the low-resolution image. During generation, Partial Diffusion Models start denoising from the intermediate distribution and perform only a part of the denoising steps. Additionally, to mitigate the error caused by the approximation, we introduce "latent alignment", which aligns the latent between low- and high-resolution images during training. Experiments on both magnetic resonance imaging (MRI) and natural images show that, compared to plain diffusion-based super-resolution methods, Partial Diffusion Models significantly reduce the number of denoising steps without sacrificing the quality of generation.
</details></li>
</ul>
<hr>
<h2 id="Conditional-Temporal-Attention-Networks-for-Neonatal-Cortical-Surface-Reconstruction"><a href="#Conditional-Temporal-Attention-Networks-for-Neonatal-Cortical-Surface-Reconstruction" class="headerlink" title="Conditional Temporal Attention Networks for Neonatal Cortical Surface Reconstruction"></a>Conditional Temporal Attention Networks for Neonatal Cortical Surface Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11870">http://arxiv.org/abs/2307.11870</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/m-qiang/cotan">https://github.com/m-qiang/cotan</a></li>
<li>paper_authors: Qiang Ma, Liu Li, Vanessa Kyriakopoulou, Joseph Hajnal, Emma C. Robinson, Bernhard Kainz, Daniel Rueckert</li>
<li>for: 这个论文的目的是提出一种快速的端到端框架，用于 diffeomorphic 新生儿大脑表面重建。</li>
<li>methods: 该方法使用 Conditional Temporal Attention Network (CoTAN)，可以快速预测多尺度 stationary velocity fields (SVF)，并通过注意力机制来学习 conditional time-varying velocity field (CTVF)。</li>
<li>results: CoTAN 可以减少 mesh 自交错错误，并且只需 0.21 秒可以将 initial template mesh 变换成 cortical white matter 和 pial surfaces。与州际基准相比，CoTAN  achieved 0.12mm 的准确性和 0.07% 的自交错错误。<details>
<summary>Abstract</summary>
Cortical surface reconstruction plays a fundamental role in modeling the rapid brain development during the perinatal period. In this work, we propose Conditional Temporal Attention Network (CoTAN), a fast end-to-end framework for diffeomorphic neonatal cortical surface reconstruction. CoTAN predicts multi-resolution stationary velocity fields (SVF) from neonatal brain magnetic resonance images (MRI). Instead of integrating multiple SVFs, CoTAN introduces attention mechanisms to learn a conditional time-varying velocity field (CTVF) by computing the weighted sum of all SVFs at each integration step. The importance of each SVF, which is estimated by learned attention maps, is conditioned on the age of the neonates and varies with the time step of integration. The proposed CTVF defines a diffeomorphic surface deformation, which reduces mesh self-intersection errors effectively. It only requires 0.21 seconds to deform an initial template mesh to cortical white matter and pial surfaces for each brain hemisphere. CoTAN is validated on the Developing Human Connectome Project (dHCP) dataset with 877 3D brain MR images acquired from preterm and term born neonates. Compared to state-of-the-art baselines, CoTAN achieves superior performance with only 0.12mm geometric error and 0.07% self-intersecting faces. The visualization of our attention maps illustrates that CoTAN indeed learns coarse-to-fine surface deformations automatically without intermediate supervision.
</details>
<details>
<summary>摘要</summary>
cortical surface reconstruction plays a fundamental role in modeling the rapid brain development during the perinatal period. In this work, we propose Conditional Temporal Attention Network (CoTAN), a fast end-to-end framework for diffeomorphic neonatal cortical surface reconstruction. CoTAN predicts multi-resolution stationary velocity fields (SVF) from neonatal brain magnetic resonance images (MRI). Instead of integrating multiple SVFs, CoTAN introduces attention mechanisms to learn a conditional time-varying velocity field (CTVF) by computing the weighted sum of all SVFs at each integration step. The importance of each SVF, which is estimated by learned attention maps, is conditioned on the age of the neonates and varies with the time step of integration. The proposed CTVF defines a diffeomorphic surface deformation, which reduces mesh self-intersection errors effectively. It only requires 0.21 seconds to deform an initial template mesh to cortical white matter and pial surfaces for each brain hemisphere. CoTAN is validated on the Developing Human Connectome Project (dHCP) dataset with 877 3D brain MR images acquired from preterm and term born neonates. Compared to state-of-the-art baselines, CoTAN achieves superior performance with only 0.12mm geometric error and 0.07% self-intersecting faces. The visualization of our attention maps illustrates that CoTAN indeed learns coarse-to-fine surface deformations automatically without intermediate supervision.Here's the translation in Traditional Chinese: cortical surface reconstruction plays a fundamental role in modeling the rapid brain development during the perinatal period. In this work, we propose Conditional Temporal Attention Network (CoTAN), a fast end-to-end framework for diffeomorphic neonatal cortical surface reconstruction. CoTAN predicts multi-resolution stationary velocity fields (SVF) from neonatal brain magnetic resonance images (MRI). Instead of integrating multiple SVFs, CoTAN introduces attention mechanisms to learn a conditional time-varying velocity field (CTVF) by computing the weighted sum of all SVFs at each integration step. The importance of each SVF, which is estimated by learned attention maps, is conditioned on the age of the neonates and varies with the time step of integration. The proposed CTVF defines a diffeomorphic surface deformation, which reduces mesh self-intersection errors effectively. It only requires 0.21 seconds to deform an initial template mesh to cortical white matter and pial surfaces for each brain hemisphere. CoTAN is validated on the Developing Human Connectome Project (dHCP) dataset with 877 3D brain MR images acquired from preterm and term born neonates. Compared to state-of-the-art baselines, CoTAN achieves superior performance with only 0.12mm geometric error and 0.07% self-intersecting faces. The visualization of our attention maps illustrates that CoTAN indeed learns coarse-to-fine surface deformations automatically without intermediate supervision.
</details></li>
</ul>
<hr>
<h2 id="Digital-Modeling-on-Large-Kernel-Metamaterial-Neural-Network"><a href="#Digital-Modeling-on-Large-Kernel-Metamaterial-Neural-Network" class="headerlink" title="Digital Modeling on Large Kernel Metamaterial Neural Network"></a>Digital Modeling on Large Kernel Metamaterial Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11862">http://arxiv.org/abs/2307.11862</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quan Liu, Hanyu Zheng, Brandon T. Swartz, Ho hin Lee, Zuhayr Asad, Ivan Kravchenko, Jason G. Valentine, Yuankai Huo</li>
<li>for: 这篇论文目的是提出一种新的大kernel金属材料神经网络（LMNN），以最大化现有金属材料神经网络（MNN）的数位能力，同时考虑物理限制。</li>
<li>methods: 本论文使用了模型重新参数化和网络压缩，以提高MNN的学习能力，并考虑了光学限制。</li>
<li>results: 实验结果显示，提案的LMNN可以提高分类精度，同时降低计算延迟。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) utilized recently are physically deployed with computational units (e.g., CPUs and GPUs). Such a design might lead to a heavy computational burden, significant latency, and intensive power consumption, which are critical limitations in applications such as the Internet of Things (IoT), edge computing, and the usage of drones. Recent advances in optical computational units (e.g., metamaterial) have shed light on energy-free and light-speed neural networks. However, the digital design of the metamaterial neural network (MNN) is fundamentally limited by its physical limitations, such as precision, noise, and bandwidth during fabrication. Moreover, the unique advantages of MNN's (e.g., light-speed computation) are not fully explored via standard 3x3 convolution kernels. In this paper, we propose a novel large kernel metamaterial neural network (LMNN) that maximizes the digital capacity of the state-of-the-art (SOTA) MNN with model re-parametrization and network compression, while also considering the optical limitation explicitly. The new digital learning scheme can maximize the learning capacity of MNN while modeling the physical restrictions of meta-optic. With the proposed LMNN, the computation cost of the convolutional front-end can be offloaded into fabricated optical hardware. The experimental results on two publicly available datasets demonstrate that the optimized hybrid design improved classification accuracy while reducing computational latency. The development of the proposed LMNN is a promising step towards the ultimate goal of energy-free and light-speed AI.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）最近的应用中使用了计算单元（例如CPU和GPU）。这种设计可能会导致重大的计算负担、显著的延迟和高度的能量投入，这些限制在互联网智能（IoT）、边缘计算和无人机应用中是关键的。最近的光学计算单元（例如元material）的进步暴露了无需能源和光速神经网络。然而，光学计算单元的数字设计（MNN）的物理限制，如精度、噪声和带宽，会在制造过程中带来限制。此外，MNN的独特优势（例如光速计算）还没有通过标准3x3卷积核来完全发挥。在本文中，我们提出了一种新的大型卷积金刚物理神经网络（LMNN），该网络可以最大化MNN的数字能力，同时考虑光学限制。新的数字学习方案可以最大化MNN的学习能力，同时模拟光学限制。通过我们的LMNN，计算前端的计算成本可以被卷积到制造的光学硬件上。实验结果表明，使用我们提出的LMNN可以提高分类精度，同时减少计算延迟。开发LMNN是通向无需能源和光速AI的最终目标的一步。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Hyperspectral-Pansharpening-on-large-scale-PRISMA-dataset"><a href="#Deep-Learning-Hyperspectral-Pansharpening-on-large-scale-PRISMA-dataset" class="headerlink" title="Deep Learning Hyperspectral Pansharpening on large scale PRISMA dataset"></a>Deep Learning Hyperspectral Pansharpening on large scale PRISMA dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11666">http://arxiv.org/abs/2307.11666</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simone Zini, Mirko Paolo Barbato, Flavio Piccoli, Paolo Napoletano<br>for:* 这个论文旨在评估多种深度学习策略用于光谱缩放。methods:* 作者使用了多种现有的深度学习方法，并适应了PRISMA卫星获得的光谱数据进行适应。results:* 研究发现，基于数据驱动的神经网络方法在RR和FR协议中都能够超越机器学习无法预测的方法，并适应更好地完成光谱缩放任务。<details>
<summary>Abstract</summary>
In this work, we assess several deep learning strategies for hyperspectral pansharpening. First, we present a new dataset with a greater extent than any other in the state of the art. This dataset, collected using the ASI PRISMA satellite, covers about 262200 km2, and its heterogeneity is granted by randomly sampling the Earth's soil. Second, we adapted several state of the art approaches based on deep learning to fit PRISMA hyperspectral data and then assessed, quantitatively and qualitatively, the performance in this new scenario. The investigation has included two settings: Reduced Resolution (RR) to evaluate the techniques in a supervised environment and Full Resolution (FR) for a real-world evaluation. The main purpose is the evaluation of the reconstruction fidelity of the considered methods. In both scenarios, for the sake of completeness, we also included machine-learning-free approaches. From this extensive analysis has emerged that data-driven neural network methods outperform machine-learning-free approaches and adapt better to the task of hyperspectral pansharpening, both in RR and FR protocols.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们评估了数字深度学习策略用于多спектраль照片增高。首先，我们提供了一个新的数据集，其覆盖环境比现有的状态 искус到达更大。这个数据集，通过ASI PRISMA卫星收集，覆盖约262200 km2的区域，并通过随机采样地球土壤来保证其多样性。其次，我们适应了一些现有的深度学习方法，用于适应PRISMA多спектраль数据，然后评估了这些方法的性能。我们在两个设置下进行了评估：减小分辨率（RR）以评估这些方法在指导环境下的性能，以及全分辨率（FR）以评估这些方法在真实世界中的性能。为了充分评估这些方法的重建精度，我们还包括了不含机器学习的方法。从这项广泛的分析中，我们发现了数据驱动的神经网络方法在多спектраль照片增高任务中表现更好，在RR和FR协议中都能够更好地适应任务。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/22/eess.IV_2023_07_22/" data-id="clp89don2017si7886rs6565t" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/21/cs.SD_2023_07_21/" class="article-date">
  <time datetime="2023-07-21T15:00:00.000Z" itemprop="datePublished">2023-07-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/21/cs.SD_2023_07_21/">cs.SD - 2023-07-21</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Integrating-Pretrained-ASR-and-LM-to-Perform-Sequence-Generation-for-Spoken-Language-Understanding"><a href="#Integrating-Pretrained-ASR-and-LM-to-Perform-Sequence-Generation-for-Spoken-Language-Understanding" class="headerlink" title="Integrating Pretrained ASR and LM to Perform Sequence Generation for Spoken Language Understanding"></a>Integrating Pretrained ASR and LM to Perform Sequence Generation for Spoken Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11005">http://arxiv.org/abs/2307.11005</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddhant Arora, Hayato Futami, Yosuke Kashiwagi, Emiru Tsunoo, Brian Yan, Shinji Watanabe</li>
<li>for: 这个研究旨在提出一个三阶段终端对话系统（E2E SLU），实现对话识别和语言模型（LM）的统合，并且解决预先训练的语音识别（ASR）和LM之间的词汇差异问题。</li>
<li>methods: 本研究使用三阶段E2E SLU系统，首先使用ASR子网络预测ASR转译，然后使用LM子网络做初步的SLU预测，最后使用妥协子网络根据ASR和LM子网络的表现进行最终预测。</li>
<li>results: 根据两个SLU资料集（SLURP和SLUE）的实验结果，提出的三阶段E2E SLU系统在处理具有声音挑战的声明时表现更好，特别是在SLUE资料集上。<details>
<summary>Abstract</summary>
There has been an increased interest in the integration of pretrained speech recognition (ASR) and language models (LM) into the SLU framework. However, prior methods often struggle with a vocabulary mismatch between pretrained models, and LM cannot be directly utilized as they diverge from its NLU formulation. In this study, we propose a three-pass end-to-end (E2E) SLU system that effectively integrates ASR and LM subnetworks into the SLU formulation for sequence generation tasks. In the first pass, our architecture predicts ASR transcripts using the ASR subnetwork. This is followed by the LM subnetwork, which makes an initial SLU prediction. Finally, in the third pass, the deliberation subnetwork conditions on representations from the ASR and LM subnetworks to make the final prediction. Our proposed three-pass SLU system shows improved performance over cascaded and E2E SLU models on two benchmark SLU datasets, SLURP and SLUE, especially on acoustically challenging utterances.
</details>
<details>
<summary>摘要</summary>
在 latest research, there has been an increased interest in integrating pre-trained speech recognition (ASR) and language models (LM) into the SLU framework. However, previous methods often struggle with a vocabulary mismatch between pre-trained models, and LM cannot be directly utilized as they diverge from its NLU formulation. In this study, we propose a three-pass end-to-end (E2E) SLU system that effectively integrates ASR and LM subnetworks into the SLU formulation for sequence generation tasks. In the first pass, our architecture predicts ASR transcripts using the ASR subnetwork. This is followed by the LM subnetwork, which makes an initial SLU prediction. Finally, in the third pass, the deliberation subnetwork conditions on representations from the ASR and LM subnetworks to make the final prediction. Our proposed three-pass SLU system shows improved performance over cascaded and E2E SLU models on two benchmark SLU datasets, SLURP and SLUE, especially on acoustically challenging utterances.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/21/cs.SD_2023_07_21/" data-id="clp89doiw00wii788bh1hc3kx" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_07_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/21/eess.AS_2023_07_21/" class="article-date">
  <time datetime="2023-07-21T14:00:00.000Z" itemprop="datePublished">2023-07-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/21/eess.AS_2023_07_21/">eess.AS - 2023-07-21</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Topic-Identification-For-Spontaneous-Speech-Enriching-Audio-Features-With-Embedded-Linguistic-Information"><a href="#Topic-Identification-For-Spontaneous-Speech-Enriching-Audio-Features-With-Embedded-Linguistic-Information" class="headerlink" title="Topic Identification For Spontaneous Speech: Enriching Audio Features With Embedded Linguistic Information"></a>Topic Identification For Spontaneous Speech: Enriching Audio Features With Embedded Linguistic Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11450">http://arxiv.org/abs/2307.11450</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aalto-speech/Topic-identification-for-spontaneous-Finnish-speech">https://github.com/aalto-speech/Topic-identification-for-spontaneous-Finnish-speech</a></li>
<li>paper_authors: Dejan Porjazovski, Tamás Grósz, Mikko Kurimo</li>
<li>for: 这篇论文旨在检验非标准的话语识别方案，以寻找不需要自动话语识别系统（ASR）的解决方案。</li>
<li>methods: 这篇论文使用了音频只和多模态组合方法来识别非标准的芬兰语。</li>
<li>results: 研究发现，听音只的方法在ASR系统不可用时是一个可行的选择，而多模态组合方法在识别性能上表现最佳。<details>
<summary>Abstract</summary>
Traditional topic identification solutions from audio rely on an automatic speech recognition system (ASR) to produce transcripts used as input to a text-based model. These approaches work well in high-resource scenarios, where there are sufficient data to train both components of the pipeline. However, in low-resource situations, the ASR system, even if available, produces low-quality transcripts, leading to a bad text-based classifier. Moreover, spontaneous speech containing hesitations can further degrade the performance of the ASR model. In this paper, we investigate alternatives to the standard text-only solutions by comparing audio-only and hybrid techniques of jointly utilising text and audio features. The models evaluated on spontaneous Finnish speech demonstrate that purely audio-based solutions are a viable option when ASR components are not available, while the hybrid multi-modal solutions achieve the best results.
</details>
<details>
<summary>摘要</summary>
传统的话题识别解决方案从音频中获得的听写系统（ASR）生成的讲解作为输入，用文本基于模型进行识别。这些方法在高资源场景下工作良好，因为可以在训练两个组件的气候下进行训练。然而，在低资源情况下，即使有ASR系统，也会生成低质量的讲解，导致文本基于模型的性能下降。此外，不慎的语音中的停顿也可能使ASR模型的性能下降。在这篇论文中，我们调查了标准文本仅解决方案的代替方案，比较音频仅、多模态融合等方法的性能。我们在自然的芬兰语音中评估了这些模型，得到的结论是：当ASR组件不可用时，听写仅的解决方案是一个可靠的选择；而多模态融合解决方案在性能上表现最佳。
</details></li>
</ul>
<hr>
<h2 id="MeetEval-A-Toolkit-for-Computation-of-Word-Error-Rates-for-Meeting-Transcription-Systems"><a href="#MeetEval-A-Toolkit-for-Computation-of-Word-Error-Rates-for-Meeting-Transcription-Systems" class="headerlink" title="MeetEval: A Toolkit for Computation of Word Error Rates for Meeting Transcription Systems"></a>MeetEval: A Toolkit for Computation of Word Error Rates for Meeting Transcription Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11394">http://arxiv.org/abs/2307.11394</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thilo von Neumann, Christoph Boeddeker, Marc Delcroix, Reinhold Haeb-Umbach</li>
<li>for: 这个论文是为了评估各种会议笔记系统而编写的。</li>
<li>methods: 这个论文使用了一个开源的工具kit来评估会议笔记系统的评估方法，包括通用的Word Error Rates（WER）计算，以及一些特定的WER定义，如cpWER、ORC WER和MIMO WER。此外，它还提供了一种基于时间约束的cpWER计算方法，以提高匹配假设字符串和参照字符串的匹配质量。</li>
<li>results: 这个论文的结果表明，基于时间约束的cpWER计算方法可以提高匹配质量，同时也可以提高匹配速度。此外，这个方法还可以使用不准确的时间标签来进行匹配，从而降低了计算成本。<details>
<summary>Abstract</summary>
MeetEval is an open-source toolkit to evaluate all kinds of meeting transcription systems. It provides a unified interface for the computation of commonly used Word Error Rates (WERs), specifically cpWER, ORC WER and MIMO WER along other WER definitions. We extend the cpWER computation by a temporal constraint to ensure that only words are identified as correct when the temporal alignment is plausible. This leads to a better quality of the matching of the hypothesis string to the reference string that more closely resembles the actual transcription quality, and a system is penalized if it provides poor time annotations. Since word-level timing information is often not available, we present a way to approximate exact word-level timings from segment-level timings (e.g., a sentence) and show that the approximation leads to a similar WER as a matching with exact word-level annotations. At the same time, the time constraint leads to a speedup of the matching algorithm, which outweighs the additional overhead caused by processing the time stamps.
</details>
<details>
<summary>摘要</summary>
美特评估是一个开源工具kit，用于评估各种会议笔记系统。它提供一个统一的接口来计算常用的单词错误率（WER），包括cpWER、ORC WER 和 MIMO WER 等 WER 定义。我们在cpWER 计算中添加了时间约束，以确保只有在时间对齐是可能的时候才认为单词是正确的。这会导致匹配假设字符串与参考字符串的匹配更加精准，系统会受到负面抑制，如果它提供了低质量的时间标记。由于单词水平的时间信息通常不可用，我们提出了一种将 sentence 级别的时间信息约化为单词级别的时间信息的方法，并证明这种约化导致与匹配精度相似的 WER。同时，时间约束会使匹配算法加速，这些加速的效果超过了对处理时间戳的额外开销。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/21/eess.AS_2023_07_21/" data-id="clp89dolb013hi788cxp034qh" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_07_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/21/cs.CV_2023_07_21/" class="article-date">
  <time datetime="2023-07-21T13:00:00.000Z" itemprop="datePublished">2023-07-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/21/cs.CV_2023_07_21/">cs.CV - 2023-07-21</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="FEDD-–-Fair-Efficient-and-Diverse-Diffusion-based-Lesion-Segmentation-and-Malignancy-Classification"><a href="#FEDD-–-Fair-Efficient-and-Diverse-Diffusion-based-Lesion-Segmentation-and-Malignancy-Classification" class="headerlink" title="FEDD – Fair, Efficient, and Diverse Diffusion-based Lesion Segmentation and Malignancy Classification"></a>FEDD – Fair, Efficient, and Diverse Diffusion-based Lesion Segmentation and Malignancy Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11654">http://arxiv.org/abs/2307.11654</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hectorcarrion/fedd">https://github.com/hectorcarrion/fedd</a></li>
<li>paper_authors: Héctor Carrión, Narges Norouzi<br>for:这个研究目的是提高肤科图像诊断的存取ibilit，以提供更加公平和准确的肤科图像分类和描述。methods:这个研究使用了一个名为FEDD的数据测试框架，它使用了干扰推导的数据库，并使用了直线探针来处理具有Semantic feature embeddings的数据。results:这个研究获得了显著的改进，包括0.18、0.13、0.06和0.07的intersection over union，并且仅使用了5%、10%、15%和20%的标注数据。此外，FEDD在10%的DDI上获得了81%的癌症分类精度，高于现有的州均值。<details>
<summary>Abstract</summary>
Skin diseases affect millions of people worldwide, across all ethnicities. Increasing diagnosis accessibility requires fair and accurate segmentation and classification of dermatology images. However, the scarcity of annotated medical images, especially for rare diseases and underrepresented skin tones, poses a challenge to the development of fair and accurate models. In this study, we introduce a Fair, Efficient, and Diverse Diffusion-based framework for skin lesion segmentation and malignancy classification. FEDD leverages semantically meaningful feature embeddings learned through a denoising diffusion probabilistic backbone and processes them via linear probes to achieve state-of-the-art performance on Diverse Dermatology Images (DDI). We achieve an improvement in intersection over union of 0.18, 0.13, 0.06, and 0.07 while using only 5%, 10%, 15%, and 20% labeled samples, respectively. Additionally, FEDD trained on 10% of DDI demonstrates malignancy classification accuracy of 81%, 14% higher compared to the state-of-the-art. We showcase high efficiency in data-constrained scenarios while providing fair performance for diverse skin tones and rare malignancy conditions. Our newly annotated DDI segmentation masks and training code can be found on https://github.com/hectorcarrion/fedd.
</details>
<details>
<summary>摘要</summary>
世界各地的人们中有数百万人被皮肤疾病影响。为了提高诊断的可accessibility，需要公平、准确地分类和分割皮肤图像。然而，罕见皮肤疾病和不 Represented 的皮肤颜色的医疗图像的缺乏标注，对开发公平和准确的模型的发展带来了挑战。本研究提出了一个公平、高效、多样的扩散基础框架（FEDD），用于皮肤病变分类和诊断。FEDD 利用了semantically meaningful的特征嵌入，通过推敲扩散probabilistic backbone来学习，然后通过线性探针进行处理，以达到最新的性能水平在多样的皮肤图像（DDI）上。我们在5%, 10%, 15%, 和20%标注样本上分别提高了交集隔��的0.18, 0.13, 0.06, 和0.07。此外，FEDD 在10%的DDI上进行训练，可以达到81%的肉瘤分类精度，与现有最新的14%高。我们在数据缺乏的情况下实现了高效性，同时为多样的皮肤颜色和罕见的肉瘤疾病情况提供公平性。我们在https://github.com/hectorcarrion/fedd上提供了新的DDI分割 mask和训练代码。
</details></li>
</ul>
<hr>
<h2 id="Deep-Reinforcement-Learning-Based-System-for-Intraoperative-Hyperspectral-Video-Autofocusing"><a href="#Deep-Reinforcement-Learning-Based-System-for-Intraoperative-Hyperspectral-Video-Autofocusing" class="headerlink" title="Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing"></a>Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11638">http://arxiv.org/abs/2307.11638</a></li>
<li>repo_url: None</li>
<li>paper_authors: Charlie Budd, Jianrong Qiu, Oscar MacCormac, Martin Huber, Christopher Mower, Mirek Janatka, Théo Trotouin, Jonathan Shapey, Mads S. Bergholt, Tom Vercauteren</li>
<li>for: 这篇论文旨在探讨使用高spectral成像技术进行手持式实时视频HSIC的可用性问题，并提出了一种基于深度学习的自动对焦方法来解决这个问题。</li>
<li>methods: 这篇论文使用了一种可变焦点的液体镜来嵌入视频HSIC镜头，并提出了一种基于深度学习的自动对焦算法来解决视频HSIC中的对焦问题。</li>
<li>results: 实验结果显示，该新的自动对焦算法比传统的对焦策略更好 ($p&lt;0.05$），其中平均焦点误差为 $0.070\pm.098$，比传统策略的 $0.146\pm.148$ 更低。此外，两名 neurosurgeon 在对不同自动对焦策略的比较中，认为该新的自动对焦算法最为有利，因此该系统在实际应用中是一个可靠的选择。<details>
<summary>Abstract</summary>
Hyperspectral imaging (HSI) captures a greater level of spectral detail than traditional optical imaging, making it a potentially valuable intraoperative tool when precise tissue differentiation is essential. Hardware limitations of current optical systems used for handheld real-time video HSI result in a limited focal depth, thereby posing usability issues for integration of the technology into the operating room. This work integrates a focus-tunable liquid lens into a video HSI exoscope, and proposes novel video autofocusing methods based on deep reinforcement learning. A first-of-its-kind robotic focal-time scan was performed to create a realistic and reproducible testing dataset. We benchmarked our proposed autofocus algorithm against traditional policies, and found our novel approach to perform significantly ($p<0.05$) better than traditional techniques ($0.070\pm.098$ mean absolute focal error compared to $0.146\pm.148$). In addition, we performed a blinded usability trial by having two neurosurgeons compare the system with different autofocus policies, and found our novel approach to be the most favourable, making our system a desirable addition for intraoperative HSI.
</details>
<details>
<summary>摘要</summary>
高spectral成像（HSI）可以捕捉更多的spectral特征，使其成为可能有价值的实时操作中的工具，当精确的组织区分是必要时。现有的光学系统的硬件限制使得实时视频HSI的焦点深度受限，从而导致了技术的可用性问题。这项工作将射频调整液体镜组合到视频HSI外Scope中，并提出了基于深度学习的视频自动对焦方法。我们首次实现了Robotics focal-time扫描，并创建了可信度评估dataset。我们对我们提出的自动对焦算法与传统策略进行比较，发现我们的新方法在($p<0.05$)的显著程度上 ($0.070\pm.098$的平均焦点错误与$0.146\pm.148$之间)。此外，我们进行了隐身用户试验，让两名 neurosurgeon比较不同的自动对焦策略，发现我们的新方法是最有优势，使我们的系统成为实时HSI中的欢迎加入。
</details></li>
</ul>
<hr>
<h2 id="Divide-and-Adapt-Active-Domain-Adaptation-via-Customized-Learning"><a href="#Divide-and-Adapt-Active-Domain-Adaptation-via-Customized-Learning" class="headerlink" title="Divide and Adapt: Active Domain Adaptation via Customized Learning"></a>Divide and Adapt: Active Domain Adaptation via Customized Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11618">http://arxiv.org/abs/2307.11618</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/duojun-huang/diana-cvpr2023">https://github.com/duojun-huang/diana-cvpr2023</a></li>
<li>paper_authors: Duojun Huang, Jichang Li, Weikai Chen, Junshi Huang, Zhenhua Chai, Guanbin Li</li>
<li>for: 这个研究旨在提高适应性模型的适应性表现，通过整合活动学习（AL）技术来标识目标数据中最有价的子集。</li>
<li>methods: 这个研究使用了一个名为Divide-and-Adapt（DiaNA）的新的适应领域活动整合框架，将目标实例分为四个类别，每个类别都具有透明的转移性质。</li>
<li>results: DiaNA可以精确地识别最有价的数据，并且可以适应不同的领域转移Setting，例如无监督领域转移（UDA）、半监督领域转移（SSDA）、源自由领域转移（SFDA）等。<details>
<summary>Abstract</summary>
Active domain adaptation (ADA) aims to improve the model adaptation performance by incorporating active learning (AL) techniques to label a maximally-informative subset of target samples. Conventional AL methods do not consider the existence of domain shift, and hence, fail to identify the truly valuable samples in the context of domain adaptation. To accommodate active learning and domain adaption, the two naturally different tasks, in a collaborative framework, we advocate that a customized learning strategy for the target data is the key to the success of ADA solutions. We present Divide-and-Adapt (DiaNA), a new ADA framework that partitions the target instances into four categories with stratified transferable properties. With a novel data subdivision protocol based on uncertainty and domainness, DiaNA can accurately recognize the most gainful samples. While sending the informative instances for annotation, DiaNA employs tailored learning strategies for the remaining categories. Furthermore, we propose an informativeness score that unifies the data partitioning criteria. This enables the use of a Gaussian mixture model (GMM) to automatically sample unlabeled data into the proposed four categories. Thanks to the "divideand-adapt" spirit, DiaNA can handle data with large variations of domain gap. In addition, we show that DiaNA can generalize to different domain adaptation settings, such as unsupervised domain adaptation (UDA), semi-supervised domain adaptation (SSDA), source-free domain adaptation (SFDA), etc.
</details>
<details>
<summary>摘要</summary>
aktive domaine anpassung (ADA) zielt darauf ab, die anpassungsleistung von modellen durch einbezug aktiver lernen (AL) technologien zu verbessern, indem eine maximale-informative auswahl vonzielchen labelliert wird. conventional AL methods ignorieren die existenz von domain shift, und daher können sie keine echten wertvollen beispiele in context von domain adaptation identifizieren. um active learning und domain anpassung zu accommodate, two naturally different tasks, propose a customized learning strategy for the target data is the key to the success of ADA solutions. we present divide-and-adapt (DiaNA), a new ADA framework that partitions the target instances into four categories with stratified transferable properties. with a novel data subdivision protocol based on uncertainty and domainness, DiaNA can accurately recognize the most gainful samples. while sending the informative instances for annotation, DiaNA employs tailored learning strategies for the remaining categories. furthermore, we propose an informativeness score that unifies the data partitioning criteria. this enables the use of a gaussian mixture model (GMM) to automatically sample unlabeled data into the proposed four categories. thanks to the "divideand-adapt" spirit, DiaNA can handle data with large variations of domain gap. in addition, we show that DiaNA can generalize to different domain adaptation settings, such as unsupervised domain adaptation (UDA), semi-supervised domain adaptation (SSDA), source-free domain adaptation (SFDA), etc.
</details></li>
</ul>
<hr>
<h2 id="Consistency-guided-Meta-Learning-for-Bootstrapping-Semi-Supervised-Medical-Image-Segmentation"><a href="#Consistency-guided-Meta-Learning-for-Bootstrapping-Semi-Supervised-Medical-Image-Segmentation" class="headerlink" title="Consistency-guided Meta-Learning for Bootstrapping Semi-Supervised Medical Image Segmentation"></a>Consistency-guided Meta-Learning for Bootstrapping Semi-Supervised Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11604">http://arxiv.org/abs/2307.11604</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aijinrjinr/mlb-seg">https://github.com/aijinrjinr/mlb-seg</a></li>
<li>paper_authors: Qingyue Wei, Lequan Yu, Xianhang Li, Wei Shao, Cihang Xie, Lei Xing, Yuyin Zhou</li>
<li>for: 这个研究旨在解决医疗影像识别的 semi-supervised 学习问题，提高医疗影像识别的效率和精度。</li>
<li>methods: 我们提出了一个名为 Meta-Learning for Bootstrapping Medical Image Segmentation (MLB-Seg) 的新方法，具有以下三个特点：首先，我们使用一小批清洁标注的影像进行训练，以生成初始的标签 для无标注的数据。其次，我们引入了一个像素类别映射系统，将像素标签和模型预测的结果相互映射。这些映射是基于一个小型的精心标注影像集，并且使用一个基于这些标注的meta-过程来决定这些映射。最后，我们引入了一个内部的Consistency-based Pseudo Label Enhancement (PLE) 方法，用于提高模型预测的品质。</li>
<li>results: 我们的实验结果显示，我们的提出的方法在 semi-supervised 下可以 achieve state-of-the-art 的效果，并且在两个公开的颈部和肾脏分类数据集上实现了显著的改善。<details>
<summary>Abstract</summary>
Medical imaging has witnessed remarkable progress but usually requires a large amount of high-quality annotated data which is time-consuming and costly to obtain. To alleviate this burden, semi-supervised learning has garnered attention as a potential solution. In this paper, we present Meta-Learning for Bootstrapping Medical Image Segmentation (MLB-Seg), a novel method for tackling the challenge of semi-supervised medical image segmentation. Specifically, our approach first involves training a segmentation model on a small set of clean labeled images to generate initial labels for unlabeled data. To further optimize this bootstrapping process, we introduce a per-pixel weight mapping system that dynamically assigns weights to both the initialized labels and the model's own predictions. These weights are determined using a meta-process that prioritizes pixels with loss gradient directions closer to those of clean data, which is based on a small set of precisely annotated images. To facilitate the meta-learning process, we additionally introduce a consistency-based Pseudo Label Enhancement (PLE) scheme that improves the quality of the model's own predictions by ensembling predictions from various augmented versions of the same input. In order to improve the quality of the weight maps obtained through multiple augmentations of a single input, we introduce a mean teacher into the PLE scheme. This method helps to reduce noise in the weight maps and stabilize its generation process. Our extensive experimental results on public atrial and prostate segmentation datasets demonstrate that our proposed method achieves state-of-the-art results under semi-supervision. Our code is available at https://github.com/aijinrjinr/MLB-Seg.
</details>
<details>
<summary>摘要</summary>
医疗影像技术在过去几年内发展非常remarkable，但通常需要大量高质量标注数据，这是时间consuming和costly的。为了解决这个问题，半supervised learning在这个领域得到了关注。在这篇论文中，我们提出了一种新的方法：Meta-Learning for Bootstrapping Medical Image Segmentation（MLB-Seg），用于解决半supervised的医疗影像分割挑战。具体来说，我们的方法首先在一小个清洁标注图像上训练一个分割模型，以生成初始标签 для无标注数据。为了进一步优化这个启动过程，我们引入了一个像素Weight Mapping系统，该系统在运行时动态分配像素的标签和模型自己预测的标签之间的权重。这些权重是基于一个小型的精确标注图像来确定的，这个过程是基于一个Meta-process来进行。为了促进这个Meta-学习过程，我们还引入了一种Consistency-based Pseudo Label Enhancement（PLE）方案，该方案可以提高模型自己的预测质量。为了进一步提高Weight map的质量，我们引入了一个Mean Teacher。这种方法可以减少Weight map中的噪音，并使其生成过程更加稳定。我们对公共的atrial和prostate分割数据集进行了广泛的实验，结果表明，我们提出的方法在半supervision下可以达到状态级result。我们的代码可以在https://github.com/aijinrjinr/MLB-Seg上获取。
</details></li>
</ul>
<hr>
<h2 id="Cascaded-multitask-U-Net-using-topological-loss-for-vessel-segmentation-and-centerline-extraction"><a href="#Cascaded-multitask-U-Net-using-topological-loss-for-vessel-segmentation-and-centerline-extraction" class="headerlink" title="Cascaded multitask U-Net using topological loss for vessel segmentation and centerline extraction"></a>Cascaded multitask U-Net using topological loss for vessel segmentation and centerline extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11603">http://arxiv.org/abs/2307.11603</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Rougé, Nicolas Passat, Odyssée Merveille</li>
<li>for: 这篇论文主要针对的是血管疾病诊断工具中的血管分割和中心线提取任务。</li>
<li>methods: 这篇论文提出了一种使用深度学习方法进行血管分割和中心线提取任务，并使用了clDice损失函数来保证结果的准确性。</li>
<li>results: 该方法可以提供更加准确的血管分割和中心线提取结果，并且可以在3D图像上进行更加有效的血管分割和中心线提取。<details>
<summary>Abstract</summary>
Vessel segmentation and centerline extraction are two crucial preliminary tasks for many computer-aided diagnosis tools dealing with vascular diseases. Recently, deep-learning based methods have been widely applied to these tasks. However, classic deep-learning approaches struggle to capture the complex geometry and specific topology of vascular networks, which is of the utmost importance in most applications. To overcome these limitations, the clDice loss, a topological loss that focuses on the vessel centerlines, has been recently proposed. This loss requires computing, with a proposed soft-skeleton algorithm, the skeletons of both the ground truth and the predicted segmentation. However, the soft-skeleton algorithm provides suboptimal results on 3D images, which makes the clDice hardly suitable on 3D images. In this paper, we propose to replace the soft-skeleton algorithm by a U-Net which computes the vascular skeleton directly from the segmentation. We show that our method provides more accurate skeletons than the soft-skeleton algorithm. We then build upon this network a cascaded U-Net trained with the clDice loss to embed topological constraints during the segmentation. The resulting model is able to predict both the vessel segmentation and centerlines with a more accurate topology.
</details>
<details>
<summary>摘要</summary>
船 Segmentation 和中心线抽取是许多计算机辅助诊断工具处理血管疾病的两个重要前期任务。近年来，深度学习基于方法广泛应用于这两个任务。然而，经典深度学习方法很难捕捉血管网络的复杂geometry和特定topology，这在大多数应用中非常重要。为了解决这些限制，最近提出了clDice损失，这是一种关注血管中心线的topological损失。这个损失需要计算，使用我们提议的软skeleton算法，真实的血管skeleton。然而，软skeleton算法在3D图像上提供的结果不够优化，使得clDice几乎不适用于3D图像。在这篇论文中，我们提议将软skeleton算法替换为一个U-Net，该网络直接从分 segmentation 中计算血管skeleton。我们显示我们的方法可以提供更加准确的skeletons。然后，我们在这个网络上建立了一个缓冲 U-Net，用于在分 segmentation 中嵌入topological约束。结果是一个能够预测血管分 segmentation 和中心线的模型，其 topology 更加准确。
</details></li>
</ul>
<hr>
<h2 id="CortexMorph-fast-cortical-thickness-estimation-via-diffeomorphic-registration-using-VoxelMorph"><a href="#CortexMorph-fast-cortical-thickness-estimation-via-diffeomorphic-registration-using-VoxelMorph" class="headerlink" title="CortexMorph: fast cortical thickness estimation via diffeomorphic registration using VoxelMorph"></a>CortexMorph: fast cortical thickness estimation via diffeomorphic registration using VoxelMorph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11567">http://arxiv.org/abs/2307.11567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Richard McKinley, Christian Rummel</li>
<li>for:  This paper aims to improve the efficiency of estimating cortical thickness in magnetic resonance imaging (MRI) studies.</li>
<li>methods:  The proposed method, CortexMorph, uses unsupervised deep learning to directly regress the deformation field needed for DiReCT, which can significantly reduce the registration time.</li>
<li>results:  The proposed method can estimate region-wise thickness in seconds from a T1-weighted image, while maintaining the ability to detect cortical atrophy, as validated on the OASIS-3 dataset and the synthetic cortical thickness phantom.<details>
<summary>Abstract</summary>
The thickness of the cortical band is linked to various neurological and psychiatric conditions, and is often estimated through surface-based methods such as Freesurfer in MRI studies. The DiReCT method, which calculates cortical thickness using a diffeomorphic deformation of the gray-white matter interface towards the pial surface, offers an alternative to surface-based methods. Recent studies using a synthetic cortical thickness phantom have demonstrated that the combination of DiReCT and deep-learning-based segmentation is more sensitive to subvoxel cortical thinning than Freesurfer.   While anatomical segmentation of a T1-weighted image now takes seconds, existing implementations of DiReCT rely on iterative image registration methods which can take up to an hour per volume. On the other hand, learning-based deformable image registration methods like VoxelMorph have been shown to be faster than classical methods while improving registration accuracy. This paper proposes CortexMorph, a new method that employs unsupervised deep learning to directly regress the deformation field needed for DiReCT. By combining CortexMorph with a deep-learning-based segmentation model, it is possible to estimate region-wise thickness in seconds from a T1-weighted image, while maintaining the ability to detect cortical atrophy. We validate this claim on the OASIS-3 dataset and the synthetic cortical thickness phantom of Rusak et al.
</details>
<details>
<summary>摘要</summary>
cortical band 厚度与多种神经科学和心理科学疾病相关，通常通过表面基本方法such as Freesurfer在MRI研究中估算。DiReCT方法，它通过Diffuse Deformation of the gray-white matter interface towards the pial surface来计算 cortical thickness，为surface-based方法提供了一种alternative。Recent studies using a synthetic cortical thickness phantom have shown that the combination of DiReCT and deep learning-based segmentation is more sensitive to subvoxel cortical thinning than Freesurfer. However, existing implementations of DiReCT rely on iterative image registration methods, which can take up to an hour per volume. In contrast, learning-based deformable image registration methods like VoxelMorph have been shown to be faster than classical methods while improving registration accuracy. This paper proposes CortexMorph, a new method that employs unsupervised deep learning to directly regress the deformation field needed for DiReCT. By combining CortexMorph with a deep learning-based segmentation model, it is possible to estimate region-wise thickness in seconds from a T1-weighted image, while maintaining the ability to detect cortical atrophy. We validate this claim on the OASIS-3 dataset and the synthetic cortical thickness phantom of Rusak et al.Here's the text in Traditional Chinese: cortical band 厚度与多种神经科学和心理科学疾病相关，通常通过表面基本方法such as Freesurfer在MRI研究中估算。DiReCT方法，它通过Diffuse Deformation of the gray-white matter interface towards the pial surface来计算 cortical thickness，为surface-based方法提供了一个alternative。Recent studies using a synthetic cortical thickness phantom have shown that the combination of DiReCT and deep learning-based segmentation is more sensitive to subvoxel cortical thinning than Freesurfer. 然而，现有的DiReCT实现方法仍然 rely on迭代的图像 регистра方法，这可以花费到每个volume上一个小时。相比之下，学习型的可扩展图像REGISTRATION方法like VoxelMorph已经被证明是更快的，同时改善了registration的准确性。本文提出了CortexMorph，一个新的方法，它使用不supervised的深度学习来直接预测DiReCT所需的歪み场。通过结合CortexMorph和深度学习基本的分类模型，可以从T1类型的影像中 estimates region-wise thickness in seconds, while maintaining the ability to detect cortical atrophy。我们验证了这个主张在OASIS-3 dataset和Rusak et al.的synthetic cortical thickness phantom上。
</details></li>
</ul>
<hr>
<h2 id="YOLOPose-V2-Understanding-and-Improving-Transformer-based-6D-Pose-Estimation"><a href="#YOLOPose-V2-Understanding-and-Improving-Transformer-based-6D-Pose-Estimation" class="headerlink" title="YOLOPose V2: Understanding and Improving Transformer-based 6D Pose Estimation"></a>YOLOPose V2: Understanding and Improving Transformer-based 6D Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11550">http://arxiv.org/abs/2307.11550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arul Selvam Periyasamy, Arash Amini, Vladimir Tsaturyan, Sven Behnke<br>for:* 6D object pose estimation for autonomous robot manipulation applicationsmethods:* Transformer-based multi-object 6D pose estimation method using keypoint regression and learnable orientation estimationresults:* Achieves results comparable to state-of-the-art methods and suitable for real-time applications<details>
<summary>Abstract</summary>
6D object pose estimation is a crucial prerequisite for autonomous robot manipulation applications. The state-of-the-art models for pose estimation are convolutional neural network (CNN)-based. Lately, Transformers, an architecture originally proposed for natural language processing, is achieving state-of-the-art results in many computer vision tasks as well. Equipped with the multi-head self-attention mechanism, Transformers enable simple single-stage end-to-end architectures for learning object detection and 6D object pose estimation jointly. In this work, we propose YOLOPose (short form for You Only Look Once Pose estimation), a Transformer-based multi-object 6D pose estimation method based on keypoint regression and an improved variant of the YOLOPose model. In contrast to the standard heatmaps for predicting keypoints in an image, we directly regress the keypoints. Additionally, we employ a learnable orientation estimation module to predict the orientation from the keypoints. Along with a separate translation estimation module, our model is end-to-end differentiable. Our method is suitable for real-time applications and achieves results comparable to state-of-the-art methods. We analyze the role of object queries in our architecture and reveal that the object queries specialize in detecting objects in specific image regions. Furthermore, we quantify the accuracy trade-off of using datasets of smaller sizes to train our model.
</details>
<details>
<summary>摘要</summary>
6D对象pose估计是自动化机器人控制应用的重要前提。现状的模型都是基于卷积神经网络（CNN）的。最近，Transformers这种原本是自然语言处理领域的 arquitecture 在计算机视觉任务中也取得了状态的前景result。通过多头自注意机制，Transformers 使得单 Stage 末端到末的结构可以用于学习对象检测和6D对象pose估计。在这种工作中，我们提出了 YOLOPose（简称为You Only Look Once Pose estimation），一种基于键点回归和改进的 YOLOPose 模型。与标准的热图来predict键点的方法不同，我们直接进行键点回归。此外，我们使用学习的orientation estimation模块来预测键点的方向。与此同时，我们还使用一个分离的翻译估计模块，使得我们的模型能够在终端到终的可 differentiable。我们的方法适用于实时应用，并达到了与状态前景方法相当的结果。我们分析了我们的建筑中的对象查询的角色，并发现对象查询在特定的图像区域中探测对象。此外，我们量化使用小型数据集来训练我们的模型的准确性交换。
</details></li>
</ul>
<hr>
<h2 id="KVN-Keypoints-Voting-Network-with-Differentiable-RANSAC-for-Stereo-Pose-Estimation"><a href="#KVN-Keypoints-Voting-Network-with-Differentiable-RANSAC-for-Stereo-Pose-Estimation" class="headerlink" title="KVN: Keypoints Voting Network with Differentiable RANSAC for Stereo Pose Estimation"></a>KVN: Keypoints Voting Network with Differentiable RANSAC for Stereo Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11543">http://arxiv.org/abs/2307.11543</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ivano-donadi/kvn">https://github.com/ivano-donadi/kvn</a></li>
<li>paper_authors: Ivano Donadi, Alberto Pretto</li>
<li>for: 本研究旨在提高对象pose estimation的精度，特别是在多视图情况下。</li>
<li>methods: 该研究使用了一种新的可导式RANSAC层和多视图PnP算法来解决多视图对象pose estimation问题。</li>
<li>results: 实验结果表明，该方法在一个公共的多视图对象pose estimation dataset上达到了当前最佳的结果，并且在对比其他最新方法时表现出了优异性。Translation:</li>
<li>for: The purpose of this research is to improve the accuracy of object pose estimation, particularly in multi-view scenarios.</li>
<li>methods: The research uses a new differentiable RANSAC layer and a multi-view PnP algorithm to solve the multi-view object pose estimation problem.</li>
<li>results: Experimental results show that the proposed method achieves the best results on a public multi-view object pose estimation dataset, and outperforms other recent methods.<details>
<summary>Abstract</summary>
Object pose estimation is a fundamental computer vision task exploited in several robotics and augmented reality applications. Many established approaches rely on predicting 2D-3D keypoint correspondences using RANSAC (Random sample consensus) and estimating the object pose using the PnP (Perspective-n-Point) algorithm. Being RANSAC non-differentiable, correspondences cannot be directly learned in an end-to-end fashion. In this paper, we address the stereo image-based object pose estimation problem by (i) introducing a differentiable RANSAC layer into a well-known monocular pose estimation network; (ii) exploiting an uncertainty-driven multi-view PnP solver which can fuse information from multiple views. We evaluate our approach on a challenging public stereo object pose estimation dataset, yielding state-of-the-art results against other recent approaches. Furthermore, in our ablation study, we show that the differentiable RANSAC layer plays a significant role in the accuracy of the proposed method. We release with this paper the open-source implementation of our method.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译为简化中文。</SYS>>对象pose估算是计算机视觉的基本任务，广泛应用于机器人和增强现实领域。许多成熟的方法利用随机抽样consensus（RANSAC）和perspective-n-point（PnP）算法来估算对象pose。由于RANSAC不具有导数，因此对应关系不能直接在终端到终点学习。在这篇论文中，我们解决了使用多视图PnP算法和不同视图之间的信息融合来解决顺序图像基本对象pose估算问题。我们对一个公共顺序图像基本对象pose估算数据集进行了评估，并与其他最近的方法进行比较，获得了状态的艺术成绩。此外，在我们的抽象研究中，我们发现了使用可导RANSAC层对提出的方法具有重要作用。我们在这篇论文中释放了我们的方法的开源实现。
</details></li>
</ul>
<hr>
<h2 id="UWAT-GAN-Fundus-Fluorescein-Angiography-Synthesis-via-Ultra-wide-angle-Transformation-Multi-scale-GAN"><a href="#UWAT-GAN-Fundus-Fluorescein-Angiography-Synthesis-via-Ultra-wide-angle-Transformation-Multi-scale-GAN" class="headerlink" title="UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-wide-angle Transformation Multi-scale GAN"></a>UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-wide-angle Transformation Multi-scale GAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11530">http://arxiv.org/abs/2307.11530</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Tinysqua/UWAT-GAN">https://github.com/Tinysqua/UWAT-GAN</a></li>
<li>paper_authors: Zhaojie Fang, Zhanghao Chen, Pengxue Wei, Wangting Li, Shaochong Zhang, Ahmed Elazab, Gangyong Jia, Ruiquan Ge, Changmiao Wang</li>
<li>For: The paper proposes a novel conditional generative adversarial network (UWAT-GAN) to synthesize UWF-FA from UWF-SLO, aiming to avoid the negative impacts of injecting sodium fluorescein and improve the resolution of fundus imaging.* Methods: The UWAT-GAN uses multi-scale generators and a fusion module patch to better extract global and local information, and an attention transmit module to help the decoder learn effectively. The network is trained using a supervised approach with multiple new weighted losses on different scales of data.* Results: The experiments on an in-house UWF image dataset demonstrate the superiority of the UWAT-GAN over the state-of-the-art methods, with high-resolution images generated and the ability to capture tiny vascular lesion areas.Here’s the information in Simplified Chinese text:* For: 这篇论文提出了一种新的条件生成 adversarial network（UWAT-GAN），用于从ultra-wide-angle fundus photography（UWF）中生成fluorescein angiography（FA）图像，以避免使用氯胺绿色素的不良影响和提高肤肤影像的分辨率。* Methods: UWAT-GAN使用多尺度生成器和一个拼接模块补充，以更好地提取全局和局部信息，同时使用一个注意力传输模块来帮助解码器更好地学习。网络使用了一种监管的方法，通过多个不同尺度的数据进行训练，并使用多种新的质量损失来更好地调节网络。* Results: 实验结果表明，UWAT-GAN在一个自有的UWF图像集上表现出优于当前的方法，可以生成高分辨率的图像，同时能够捕捉到微小的血管病变区域。<details>
<summary>Abstract</summary>
Fundus photography is an essential examination for clinical and differential diagnosis of fundus diseases. Recently, Ultra-Wide-angle Fundus (UWF) techniques, UWF Fluorescein Angiography (UWF-FA) and UWF Scanning Laser Ophthalmoscopy (UWF-SLO) have been gradually put into use. However, Fluorescein Angiography (FA) and UWF-FA require injecting sodium fluorescein which may have detrimental influences. To avoid negative impacts, cross-modality medical image generation algorithms have been proposed. Nevertheless, current methods in fundus imaging could not produce high-resolution images and are unable to capture tiny vascular lesion areas. This paper proposes a novel conditional generative adversarial network (UWAT-GAN) to synthesize UWF-FA from UWF-SLO. Using multi-scale generators and a fusion module patch to better extract global and local information, our model can generate high-resolution images. Moreover, an attention transmit module is proposed to help the decoder learn effectively. Besides, a supervised approach is used to train the network using multiple new weighted losses on different scales of data. Experiments on an in-house UWF image dataset demonstrate the superiority of the UWAT-GAN over the state-of-the-art methods. The source code is available at: https://github.com/Tinysqua/UWAT-GAN.
</details>
<details>
<summary>摘要</summary>
背景：背部照片是诊断背部疾病的重要诊断工具。近些年来，极广角背部照片（UWF）技术，包括UWF氟胺染色（UWF-FA）和UWF扫描镜内眼镜（UWF-SLO），逐渐得到应用。然而，氟胺染色和UWF-FA都需要注射氟胺，可能会产生不良影响。为了避免这些影响，混合模式医学图像生成算法已经被提出。然而，当前的基于背部照片的医学图像生成方法无法生成高分辨率图像，并且无法捕捉微型血管病区。方法：本文提出了一种新的冲拦生成 adversarial network（UWAT-GAN），用于从UWF-SLO中生成UWF-FA。该模型使用多尺度生成器和一个融合模块质子来更好地提取全部和局部信息。此外，我们还提出了一种注意力传输模块，以帮助解码器更好地学习。此外，我们采用了多种新的权重损失方法来训练网络。结果：我们在自有的UWF图像集上进行了实验，并证明了UWAT-GAN的优越性。相比之前的方法，UWAT-GAN能够生成高分辨率图像，并且能够更好地捕捉微型血管病区。代码可以在以下GitHub地址下下载：https://github.com/Tinysqua/UWAT-GAN。
</details></li>
</ul>
<hr>
<h2 id="Improving-Viewpoint-Robustness-for-Visual-Recognition-via-Adversarial-Training"><a href="#Improving-Viewpoint-Robustness-for-Visual-Recognition-via-Adversarial-Training" class="headerlink" title="Improving Viewpoint Robustness for Visual Recognition via Adversarial Training"></a>Improving Viewpoint Robustness for Visual Recognition via Adversarial Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11528">http://arxiv.org/abs/2307.11528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shouwei Ruan, Yinpeng Dong, Hang Su, Jianteng Peng, Ning Chen, Xingxing Wei</li>
<li>for: 提高图像分类器的视点Robustness，使其能够快速和稳定地识别不同视点下的图像。</li>
<li>methods: 提出了Viewpoint-Invariant Adversarial Training（VIAT）方法，通过对视点变换视为攻击，形式化为最小化最坏情况下的损失函数，从而获得视点不变的图像分类器。</li>
<li>results: VIAT可以显著提高多种图像分类器的视点Robustness，并且可以通过提供证明 radius 和准确率来证明其效果。<details>
<summary>Abstract</summary>
Viewpoint invariance remains challenging for visual recognition in the 3D world, as altering the viewing directions can significantly impact predictions for the same object. While substantial efforts have been dedicated to making neural networks invariant to 2D image translations and rotations, viewpoint invariance is rarely investigated. Motivated by the success of adversarial training in enhancing model robustness, we propose Viewpoint-Invariant Adversarial Training (VIAT) to improve the viewpoint robustness of image classifiers. Regarding viewpoint transformation as an attack, we formulate VIAT as a minimax optimization problem, where the inner maximization characterizes diverse adversarial viewpoints by learning a Gaussian mixture distribution based on the proposed attack method GMVFool. The outer minimization obtains a viewpoint-invariant classifier by minimizing the expected loss over the worst-case viewpoint distributions that can share the same one for different objects within the same category. Based on GMVFool, we contribute a large-scale dataset called ImageNet-V+ to benchmark viewpoint robustness. Experimental results show that VIAT significantly improves the viewpoint robustness of various image classifiers based on the diversity of adversarial viewpoints generated by GMVFool. Furthermore, we propose ViewRS, a certified viewpoint robustness method that provides a certified radius and accuracy to demonstrate the effectiveness of VIAT from the theoretical perspective.
</details>
<details>
<summary>摘要</summary>
视点不变性仍然是视觉识别在三维世界中的挑战，因为改变观察方向可能会对同一个物体的预测产生很大的影响。虽然大量的努力已经投入到了使神经网络不受二维图像的翻译和旋转的影响，但视点不变性几乎没有被研究。驱动于对抗训练的成功，我们提出了视点不变性对抗训练（VIAT），以提高图像分类器的视点稳定性。我们将视点变换视为攻击，并将VIAT表示为一个对抗最优化问题。内部最大化Characterizes diverse adversarial viewpoints by learning a Gaussian mixture distribution based on the proposed attack method GMVFool.外部最小化获得视点不变的分类器，将预期损失最小化在多种可能的观察方向中的最坏情况下。基于GMVFool，我们贡献了一个大规模的数据集ImageNet-V+，用于评估视点稳定性。实验结果显示，VIAT可以大幅提高多种图像分类器的视点稳定性，并且基于GMVFool生成的多种攻击视点的多样性。此外，我们还提出了ViewRS，一种可证明的视点稳定性方法，可以在理论上证明VIAT的效果。
</details></li>
</ul>
<hr>
<h2 id="CopyRNeRF-Protecting-the-CopyRight-of-Neural-Radiance-Fields"><a href="#CopyRNeRF-Protecting-the-CopyRight-of-Neural-Radiance-Fields" class="headerlink" title="CopyRNeRF: Protecting the CopyRight of Neural Radiance Fields"></a>CopyRNeRF: Protecting the CopyRight of Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11526">http://arxiv.org/abs/2307.11526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyuan Luo, Qing Guo, Ka Chun Cheung, Simon See, Renjie Wan</li>
<li>for: 保护NeRF模型的版权</li>
<li>methods: 使用水印颜色表示法保护NeRF模型的版权，并设计了一种鲁棒的渲染方案来保证NeRF模型的抽象稳定性</li>
<li>results: 比较 Optional solutions 中的不同方法，提出了一种能够直接保护NeRF模型的版权，同时保持高质量渲染和位准率的方法<details>
<summary>Abstract</summary>
Neural Radiance Fields (NeRF) have the potential to be a major representation of media. Since training a NeRF has never been an easy task, the protection of its model copyright should be a priority. In this paper, by analyzing the pros and cons of possible copyright protection solutions, we propose to protect the copyright of NeRF models by replacing the original color representation in NeRF with a watermarked color representation. Then, a distortion-resistant rendering scheme is designed to guarantee robust message extraction in 2D renderings of NeRF. Our proposed method can directly protect the copyright of NeRF models while maintaining high rendering quality and bit accuracy when compared among optional solutions.
</details>
<details>
<summary>摘要</summary>
neural radiance fields (NeRF) 有可能成为媒体表示的主要形式。由于训练 NeRF 从来不是一件容易的任务，因此保护其模型版权应该是优先事项。在这篇论文中，我们通过分析可能的版权保护解决方案的利弊，提议将 NeRF 模型中原始颜色表示替换为水印颜色表示，然后设计一种抗扭变渲染方案，以保证 NeRF 2D 渲染中的信息提取Robust。我们提议的方法可以直接保护 NeRF 模型的版权，同时保持高质量渲染和位数精度。
</details></li>
</ul>
<hr>
<h2 id="BatMobility-Towards-Flying-Without-Seeing-for-Autonomous-Drones"><a href="#BatMobility-Towards-Flying-Without-Seeing-for-Autonomous-Drones" class="headerlink" title="BatMobility: Towards Flying Without Seeing for Autonomous Drones"></a>BatMobility: Towards Flying Without Seeing for Autonomous Drones</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11518">http://arxiv.org/abs/2307.11518</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emerson Sie, Zikun Liu, Deepak Vasisht</li>
<li>for: 这个论文的目的是否允许无人飞行器（UAV）不依赖于光学感知器，即UAV可以不见而飞。</li>
<li>methods: 作者提出了一种轻量级的mm波雷达仅基于感知系统，称为BatMobility，以取代光学感知器。BatMobility实现了无人飞行器的电台流估计（一种基于表面平行偏振的FMCW雷达基于 superficie-parallel doppler shift的新方法）和雷达基于碰撞避免。</li>
<li>results: 作者使用商业感知器建立了BatMobility，并在一个小型的Off-the-shelf quadcopter上运行了一个未修改的飞行控制器。评估表明，BatMobility在各种场景中比或超过了商业级光学感知器的性能。<details>
<summary>Abstract</summary>
Unmanned aerial vehicles (UAVs) rely on optical sensors such as cameras and lidar for autonomous operation. However, such optical sensors are error-prone in bad lighting, inclement weather conditions including fog and smoke, and around textureless or transparent surfaces. In this paper, we ask: is it possible to fly UAVs without relying on optical sensors, i.e., can UAVs fly without seeing? We present BatMobility, a lightweight mmWave radar-only perception system for UAVs that eliminates the need for optical sensors. BatMobility enables two core functionalities for UAVs -- radio flow estimation (a novel FMCW radar-based alternative for optical flow based on surface-parallel doppler shift) and radar-based collision avoidance. We build BatMobility using commodity sensors and deploy it as a real-time system on a small off-the-shelf quadcopter running an unmodified flight controller. Our evaluation shows that BatMobility achieves comparable or better performance than commercial-grade optical sensors across a wide range of scenarios.
</details>
<details>
<summary>摘要</summary>
无人飞行器（UAV）通常靠光学感知器件如摄像头和激光雷达进行自主操作。但光学感知器件在糟糕的照明条件、不适的天气条件（如雾和烟）以及无Texture或透明表面上存在误差。在这篇论文中，我们问：是否可以让UAV飞行不用光学感知器件，即UAV是否可以“不见”飞行？我们介绍了BatMobility，一种轻量级mmWave雷达只的感知系统，它消除了对光学感知器件的需求。BatMobility实现了UAV两个核心功能—— радио流速估计（一种基于表面平行Doppler偏移的新型FMCW雷达基于光流的替代方案）和雷达基于避免碰撞。我们使用常见的感知器件建立BatMobility，并将其部署为实时系统，运行在一个小型、Off-the-shelfquadcopter上，不需要修改飞行控制器。我们的评估表明，BatMobility在各种场景中与商业级光学感知器件相比具有相当或更好的性能。
</details></li>
</ul>
<hr>
<h2 id="CORE-Cooperative-Reconstruction-for-Multi-Agent-Perception"><a href="#CORE-Cooperative-Reconstruction-for-Multi-Agent-Perception" class="headerlink" title="CORE: Cooperative Reconstruction for Multi-Agent Perception"></a>CORE: Cooperative Reconstruction for Multi-Agent Perception</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11514">http://arxiv.org/abs/2307.11514</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zllxot/core">https://github.com/zllxot/core</a></li>
<li>paper_authors: Binglu Wang, Lei Zhang, Zhaozhong Wang, Yongqiang Zhao, Tianfei Zhou</li>
<li>for: 这篇论文提出了一种基于多智能机器人协同感知的模型CORE，用于提高多机器人合作感知的效果和通信效率。</li>
<li>methods: 该模型包括三个主要组成部分：压缩器、协同协商组件和重建模块。压缩器用于每个机器人创建更加压缩的特征表示，以便高效广播；协同协商组件用于跨机器人消息集成，以提高协同感知的效果；重建模块用于根据合并的特征表示重建观察。</li>
<li>results: 该模型在OPV2V大规模多机器人感知数据集上进行了3D物体检测和 semantic segmentation两个任务的验证，结果表明该模型在两个任务中均达到了领先的性能水平，并且更高效的进行通信。<details>
<summary>Abstract</summary>
This paper presents CORE, a conceptually simple, effective and communication-efficient model for multi-agent cooperative perception. It addresses the task from a novel perspective of cooperative reconstruction, based on two key insights: 1) cooperating agents together provide a more holistic observation of the environment, and 2) the holistic observation can serve as valuable supervision to explicitly guide the model learning how to reconstruct the ideal observation based on collaboration. CORE instantiates the idea with three major components: a compressor for each agent to create more compact feature representation for efficient broadcasting, a lightweight attentive collaboration component for cross-agent message aggregation, and a reconstruction module to reconstruct the observation based on aggregated feature representations. This learning-to-reconstruct idea is task-agnostic, and offers clear and reasonable supervision to inspire more effective collaboration, eventually promoting perception tasks. We validate CORE on OPV2V, a large-scale multi-agent percetion dataset, in two tasks, i.e., 3D object detection and semantic segmentation. Results demonstrate that the model achieves state-of-the-art performance on both tasks, and is more communication-efficient.
</details>
<details>
<summary>摘要</summary>
CORE consists of three major components:1. Compressor: Each agent creates a more compact feature representation for efficient broadcasting.2. Lightweight attentive collaboration component: Cross-agent message aggregation is performed using a lightweight attentive mechanism.3. Reconstruction module: The observation is reconstructed based on aggregated feature representations.The learning-to-reconstruct idea is task-agnostic and provides clear and reasonable supervision to inspire more effective collaboration, leading to improved perception tasks. The model is validated on the OPV2V dataset, achieving state-of-the-art performance in both 3D object detection and semantic segmentation tasks, while also being more communication-efficient.
</details></li>
</ul>
<hr>
<h2 id="Bone-mineral-density-estimation-from-a-plain-X-ray-image-by-learning-decomposition-into-projections-of-bone-segmented-computed-tomography"><a href="#Bone-mineral-density-estimation-from-a-plain-X-ray-image-by-learning-decomposition-into-projections-of-bone-segmented-computed-tomography" class="headerlink" title="Bone mineral density estimation from a plain X-ray image by learning decomposition into projections of bone-segmented computed tomography"></a>Bone mineral density estimation from a plain X-ray image by learning decomposition into projections of bone-segmented computed tomography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11513">http://arxiv.org/abs/2307.11513</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Gu, Yoshito Otake, Keisuke Uemura, Mazen Soufi, Masaki Takao, Hugues Talbot, Seiji Okada, Nobuhiko Sugano, Yoshinobu Sato</li>
<li>for: 这个研究旨在透过简单的X射线影像来估计骨骼粒子密度（BMD），以便在日常医疗实践中进行早期诊断。</li>
<li>methods: 本研究提出了一种高效的方法，即将骨骼分割成QCT射测的投影，以估计BMD。这种方法只需要有限的数据，并且可以在实际医疗实践中进行应用。</li>
<li>results: 研究发现，这种方法可以高度准确地估计BMD，其相互联系系数（Pearson correlation coefficient）为0.880和0.920，并且准确性的标准差（root mean square of coefficient of variation）为3.27%至3.79%。此外，研究还进行了多个验证实验，包括多姿、未测量CT和压缩实验，以确保其可以在实际应用中进行运行。<details>
<summary>Abstract</summary>
Osteoporosis is a prevalent bone disease that causes fractures in fragile bones, leading to a decline in daily living activities. Dual-energy X-ray absorptiometry (DXA) and quantitative computed tomography (QCT) are highly accurate for diagnosing osteoporosis; however, these modalities require special equipment and scan protocols. To frequently monitor bone health, low-cost, low-dose, and ubiquitously available diagnostic methods are highly anticipated. In this study, we aim to perform bone mineral density (BMD) estimation from a plain X-ray image for opportunistic screening, which is potentially useful for early diagnosis. Existing methods have used multi-stage approaches consisting of extraction of the region of interest and simple regression to estimate BMD, which require a large amount of training data. Therefore, we propose an efficient method that learns decomposition into projections of bone-segmented QCT for BMD estimation under limited datasets. The proposed method achieved high accuracy in BMD estimation, where Pearson correlation coefficients of 0.880 and 0.920 were observed for DXA-measured BMD and QCT-measured BMD estimation tasks, respectively, and the root mean square of the coefficient of variation values were 3.27 to 3.79% for four measurements with different poses. Furthermore, we conducted extensive validation experiments, including multi-pose, uncalibrated-CT, and compression experiments toward actual application in routine clinical practice.
</details>
<details>
<summary>摘要</summary>
骨质疾病（osteoporosis）是一种非常普遍的骨疾病，会导致脆弱骨骼中的裂解，从而导致日常生活活动下降。 dual-energy X-ray absorptiometry（DXA）和量子计算Tomography（QCT）是骨质疾病诊断的非常准确的方法，但是这些方法需要特殊的设备和扫描协议。为了经常监测骨健康，低成本、低剂量、通用可用的诊断方法非常需求。在这项研究中，我们想要从普通X射线图像中估算骨矿化密度（BMD），以便在机会性检测中进行早期诊断。现有的方法通常使用多stage的方法，包括提取区域 интереса和简单的回归来估算BMD，这些方法需要很大的训练数据。因此，我们提出了一种高效的方法，可以在有限的数据集上学习分解为骨segmented QCT的投影来估算BMD。我们的方法实现了高精度的BMD估算，其中DXA测量BMD和QCT测量BMD估算任务的Pearson相关系数分别为0.880和0.920，并且根据不同的姿势测量结果，核心均方差的值为3.27%至3.79%。此外，我们进行了广泛的验证实验，包括多姿、无抽象CT和压缩实验，以便在实际临床医疗实践中应用。
</details></li>
</ul>
<hr>
<h2 id="R2Det-Redemption-from-Range-view-for-Accurate-3D-Object-Detection"><a href="#R2Det-Redemption-from-Range-view-for-Accurate-3D-Object-Detection" class="headerlink" title="R2Det: Redemption from Range-view for Accurate 3D Object Detection"></a>R2Det: Redemption from Range-view for Accurate 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11482">http://arxiv.org/abs/2307.11482</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihan Wang, Qiao Yan, Yi Wang</li>
<li>for: 这篇论文的目的是提高LiDAR数据驱动的自动驾驶系统中的3D物体检测精度。</li>
<li>methods: 该论文提出了一种基于范围视图的方法，使用Range-view Representation来增强3D点Cloud的精度。这种方法包括BasicBlock、Hierarchical-dilated Meta Kernel和Feature Points Redemption等部分。</li>
<li>results: 该论文的实验结果表明，将该方法与现有的LiDAR数据驱动的3D物体检测器结合使用，可以提高3D物体检测精度，比如在KITTI val set上的easy、moderate和hard等difficulty level上的mAP提高1.39%、1.67%和1.97%。此外，该论文还提出了一种基于R2M的新的3D物体检测器R2Detector，与现有的范围视图基本的方法相比，R2Detector在KITTI benchmark和Waymo Open Dataset上表现出了明显的优异。<details>
<summary>Abstract</summary>
LiDAR-based 3D object detection is of paramount importance for autonomous driving. Recent trends show a remarkable improvement for bird's-eye-view (BEV) based and point-based methods as they demonstrate superior performance compared to range-view counterparts. This paper presents an insight that leverages range-view representation to enhance 3D points for accurate 3D object detection. Specifically, we introduce a Redemption from Range-view Module (R2M), a plug-and-play approach for 3D surface texture enhancement from the 2D range view to the 3D point view. R2M comprises BasicBlock for 2D feature extraction, Hierarchical-dilated (HD) Meta Kernel for expanding the 3D receptive field, and Feature Points Redemption (FPR) for recovering 3D surface texture information. R2M can be seamlessly integrated into state-of-the-art LiDAR-based 3D object detectors as preprocessing and achieve appealing improvement, e.g., 1.39%, 1.67%, and 1.97% mAP improvement on easy, moderate, and hard difficulty level of KITTI val set, respectively. Based on R2M, we further propose R2Detector (R2Det) with the Synchronous-Grid RoI Pooling for accurate box refinement. R2Det outperforms existing range-view-based methods by a significant margin on both the KITTI benchmark and the Waymo Open Dataset. Codes will be made publicly available.
</details>
<details>
<summary>摘要</summary>
利用LiDAR的3D对象检测是自动驾驶中的重要环节。最新趋势表明BEV（鸟瞰视）和点云方法在性能方面表现更出色，超过范围视图方法。本文提出一种启示，利用范围视图表示增强3D点云的精度。特别是，我们介绍了一个叫做Redemption from Range-view Module（R2M），这是一种可插入现有LiDAR基于3D对象检测器中的预处理方法。R2M包括基本块（BasicBlock） дляEXTRACTING 2D特征，叠加的高级叠加（HD）元件 kernel for 扩展3D感知范围，以及特征点重新识别（FPR）模块，用于从2D范围视图中恢复3D表面文本信息。R2M可以轻松地与现有的LiDAR基于3D对象检测器集成，并实现了让人满意的提升，例如，在KITTI测试集的易、中、Difficulty三级水平上的mAP提升率分别为1.39%、1.67%和1.97%。基于R2M，我们进一步提出了R2Detector（R2Det），它使用同步格RoI pooling来进行精度的盒子精度。R2Det在KITTI测试集和Waymo开放数据集上与现有范围视图基于方法相比具有明显的提升。代码将公开发布。
</details></li>
</ul>
<hr>
<h2 id="SA-BEV-Generating-Semantic-Aware-Bird’s-Eye-View-Feature-for-Multi-view-3D-Object-Detection"><a href="#SA-BEV-Generating-Semantic-Aware-Bird’s-Eye-View-Feature-for-Multi-view-3D-Object-Detection" class="headerlink" title="SA-BEV: Generating Semantic-Aware Bird’s-Eye-View Feature for Multi-view 3D Object Detection"></a>SA-BEV: Generating Semantic-Aware Bird’s-Eye-View Feature for Multi-view 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11477">http://arxiv.org/abs/2307.11477</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mengtan00/sa-bev">https://github.com/mengtan00/sa-bev</a></li>
<li>paper_authors: Jinqing Zhang, Yanan Zhang, Qingjie Liu, Yunhong Wang</li>
<li>for: 提高自适应驾驶的经济性，使用纯摄像头基础的鸟瞰视图（BEV）感知。</li>
<li>methods: 提出 semantic-aware BEV Pooling（SA-BEVPool），可以根据图像特征的semantic segmentation过滤背景信息，将图像特征转化为semantic-aware BEV特征。还提出 BEV-Paste 数据增强策略，可以尝试与 semantic-aware BEV特征进行匹配。此外，我们还设计了 Multi-Scale Cross-Task（MSCT）头，可以结合任务特定和跨任务信息，更准确地预测深度分布和semantic segmentation。</li>
<li>results: 经过实验，SA-BEV在 nuScenes 上达到了状态革命性的性能。<details>
<summary>Abstract</summary>
Recently, the pure camera-based Bird's-Eye-View (BEV) perception provides a feasible solution for economical autonomous driving. However, the existing BEV-based multi-view 3D detectors generally transform all image features into BEV features, without considering the problem that the large proportion of background information may submerge the object information. In this paper, we propose Semantic-Aware BEV Pooling (SA-BEVPool), which can filter out background information according to the semantic segmentation of image features and transform image features into semantic-aware BEV features. Accordingly, we propose BEV-Paste, an effective data augmentation strategy that closely matches with semantic-aware BEV feature. In addition, we design a Multi-Scale Cross-Task (MSCT) head, which combines task-specific and cross-task information to predict depth distribution and semantic segmentation more accurately, further improving the quality of semantic-aware BEV feature. Finally, we integrate the above modules into a novel multi-view 3D object detection framework, namely SA-BEV. Experiments on nuScenes show that SA-BEV achieves state-of-the-art performance. Code has been available at https://github.com/mengtan00/SA-BEV.git.
</details>
<details>
<summary>摘要</summary>
最近，纯摄像头基本视角（BEV）的感知提供了经济自动驾驶的可行解决方案。然而，现有的 BEV 基本多视图三维探测器通常将所有图像特征转换成 BEV 特征，无论背景信息占据对象信息的大量。在这篇论文中，我们提出了semantic-aware BEV pooling（SA-BEVPool），可以根据图像特征的semantic segmentation过滤出背景信息，并将图像特征转换成semantic-aware BEV特征。此外，我们提出了BEV-Paste，一种高效的数据增强策略，可以快速匹配semantic-aware BEV特征。此外，我们设计了多尺度交叉任务（MSCT）头，可以将任务特定和交叉任务信息组合以更准确地预测深度分布和semantic segmentation，进一步提高 semantic-aware BEV特征的质量。最后，我们将上述模块集成到了一个新的多视图三维对象检测框架中，称之为SA-BEV。nuScenes实验显示，SA-BEV可以达到状态前的性能。代码可以在https://github.com/mengtan00/SA-BEV.git中下载。
</details></li>
</ul>
<hr>
<h2 id="Physics-Aware-Semi-Supervised-Underwater-Image-Enhancement"><a href="#Physics-Aware-Semi-Supervised-Underwater-Image-Enhancement" class="headerlink" title="Physics-Aware Semi-Supervised Underwater Image Enhancement"></a>Physics-Aware Semi-Supervised Underwater Image Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11470">http://arxiv.org/abs/2307.11470</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Qi, Xinghui Dong</li>
<li>for: 提高水下图像质量，解决水下图像受到媒体传输的降低效应</li>
<li>methods:  combining physics-based underwater Image Formation Model (IFM)和深度学习技术，提出了一种新的物理意识的双流水下图像提升网络（PA-UIENet），包括传输估计流和环境光估计流</li>
<li>results: 与基eline的八个方法进行比较，在五个测试集上的降低估计和水下图像提升任务中表现更好，这可能是因为它不仅可以模拟降低，还可以学习不同的水下场景特征。<details>
<summary>Abstract</summary>
Underwater images normally suffer from degradation due to the transmission medium of water bodies. Both traditional prior-based approaches and deep learning-based methods have been used to address this problem. However, the inflexible assumption of the former often impairs their effectiveness in handling diverse underwater scenes, while the generalization of the latter to unseen images is usually weakened by insufficient data. In this study, we leverage both the physics-based underwater Image Formation Model (IFM) and deep learning techniques for Underwater Image Enhancement (UIE). To this end, we propose a novel Physics-Aware Dual-Stream Underwater Image Enhancement Network, i.e., PA-UIENet, which comprises a Transmission Estimation Steam (T-Stream) and an Ambient Light Estimation Stream (A-Stream). This network fulfills the UIE task by explicitly estimating the degradation parameters of the IFM. We also adopt an IFM-inspired semi-supervised learning framework, which exploits both the labeled and unlabeled images, to address the issue of insufficient data. Our method performs better than, or at least comparably to, eight baselines across five testing sets in the degradation estimation and UIE tasks. This should be due to the fact that it not only can model the degradation but also can learn the characteristics of diverse underwater scenes.
</details>
<details>
<summary>摘要</summary>
水下图像通常受到水体媒体传输的质量下降的影响。传统的基于先前的方法和深度学习基于方法都已经用来解决这个问题。然而，前者的固定假设经常使其效果不足以处理多样化的水下场景，而后者的普适性通常由数据不足而削弱。在这项研究中，我们利用物理基础的水下图像形成模型（IFM）和深度学习技术来进行水下图像提升（UIE）。为此，我们提出了一种具有物理意识的双流水下图像提升网络，即PA-UIENet，该网络包括传输估计流（T-Stream）和投光估计流（A-Stream）。这个网络通过直接估计IFM中的降低参数来完成UIE任务。我们还采用基于IFM的半监督学习框架，这种框架可以利用标注和无标注图像来解决数据不足的问题。我们的方法在五个测试集上比基eline的八个参考方法表现更好，或者至少与其相当。这应该是因为它不仅可以模型降低，还可以学习多样化的水下场景的特点。
</details></li>
</ul>
<hr>
<h2 id="MatSpectNet-Material-Segmentation-Network-with-Domain-Aware-and-Physically-Constrained-Hyperspectral-Reconstruction"><a href="#MatSpectNet-Material-Segmentation-Network-with-Domain-Aware-and-Physically-Constrained-Hyperspectral-Reconstruction" class="headerlink" title="MatSpectNet: Material Segmentation Network with Domain-Aware and Physically-Constrained Hyperspectral Reconstruction"></a>MatSpectNet: Material Segmentation Network with Domain-Aware and Physically-Constrained Hyperspectral Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11466">http://arxiv.org/abs/2307.11466</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/heng-yuwen/matspectnet">https://github.com/heng-yuwen/matspectnet</a></li>
<li>paper_authors: Yuwen Heng, Yihong Wu, Jiawen Chen, Srinandan Dasmahapatra, Hansung Kim</li>
<li>for: 提高RGB图像中材料分割的准确率，使用Recovered hyperspectral images。</li>
<li>methods: 提出了一种新的模型——MatSpectNet，利用现代相机的色彩感知原理来约束重构的彩色图像，并通过领域适应方法将彩色图像映射到材料分割dataset中。</li>
<li>results: 对LMD dataset和OpenSurfaces dataset进行了实验，MatSpectNet在比较最新的发表文章的基础上提高了1.60%的平均像素精度和3.42%的类别精度。<details>
<summary>Abstract</summary>
Achieving accurate material segmentation for 3-channel RGB images is challenging due to the considerable variation in a material's appearance. Hyperspectral images, which are sets of spectral measurements sampled at multiple wavelengths, theoretically offer distinct information for material identification, as variations in intensity of electromagnetic radiation reflected by a surface depend on the material composition of a scene. However, existing hyperspectral datasets are impoverished regarding the number of images and material categories for the dense material segmentation task, and collecting and annotating hyperspectral images with a spectral camera is prohibitively expensive. To address this, we propose a new model, the MatSpectNet to segment materials with recovered hyperspectral images from RGB images. The network leverages the principles of colour perception in modern cameras to constrain the reconstructed hyperspectral images and employs the domain adaptation method to generalise the hyperspectral reconstruction capability from a spectral recovery dataset to material segmentation datasets. The reconstructed hyperspectral images are further filtered using learned response curves and enhanced with human perception. The performance of MatSpectNet is evaluated on the LMD dataset as well as the OpenSurfaces dataset. Our experiments demonstrate that MatSpectNet attains a 1.60% increase in average pixel accuracy and a 3.42% improvement in mean class accuracy compared with the most recent publication. The project code is attached to the supplementary material and will be published on GitHub.
</details>
<details>
<summary>摘要</summary>
实现高精度物料分 segmentation для 3-canal RGB 影像是一个挑战，因为物料的外观可能会有很大的变化。投色影像（sets of spectral measurements sampled at multiple wavelengths）在理论上可以提供专门的材料识别信息，因为表面反射的电磁辐射强度变化随物料scene的Composition。但是现有的投色影像数据集是缺乏 dense material segmentation 任务中的图像数据和材料类别，收集和标注投色影像使用 spectral camera 是昂费的。为了解决这个问题，我们提出了一个新的模型，MatSpectNet，它可以将 RGB 影像中的材料分 segmentation 构成为 recovered 投色影像。MatSpectNet 利用了现代相机中的色彩感知原理来对投色影像进行对映，并使用领域适应方法将投色影像的对映能力从投色回传数据集中转移到物料分 segmentation 数据集中。另外，MatSpectNet 还使用学习回应曲线来筛选重建的投色影像，并通过人类感知进行改进。我们将 MatSpectNet 的性能评估在 LMD 数据集以及 OpenSurfaces 数据集上。实验结果显示，MatSpectNet 可以与最近一篇文献相比，提高了1.60%的平均像素精度和3.42%的mean class accuracy。项目代码会附加到补充材料中，并将在 GitHub 上发布。
</details></li>
</ul>
<hr>
<h2 id="Strip-MLP-Efficient-Token-Interaction-for-Vision-MLP"><a href="#Strip-MLP-Efficient-Token-Interaction-for-Vision-MLP" class="headerlink" title="Strip-MLP: Efficient Token Interaction for Vision MLP"></a>Strip-MLP: Efficient Token Interaction for Vision MLP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11458">http://arxiv.org/abs/2307.11458</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/med-process/strip_mlp">https://github.com/med-process/strip_mlp</a></li>
<li>paper_authors: Guiping Cao, Shengda Luo, Wenjian Huang, Xiangyuan Lan, Dongmei Jiang, Yaowei Wang, Jianguo Zhang</li>
<li>for: 提高 MLP 模型在小数据集上表现，以及与现有 MLP 模型在 ImageNet 上的比较</li>
<li>methods: 提出了一种新的 MLP 层called Strip MLP layer，并提出了一种 Cascade Group Strip Mixing Module (CGSMM) 和一种 Local Strip Mixing Module (LSMM) 以增强 токен交互力</li>
<li>results: 实验表明，Strip-MLP 模型在小数据集上显著提高了性能，并在 ImageNet 上与现有 MLP 模型相当或更好的结果。具体来说，Strip-MLP 模型在 Caltech-101 和 CIFAR-100 上的平均 Top-1 准确率高于现有 MLP 模型 +2.44% 和 +2.16%。<details>
<summary>Abstract</summary>
Token interaction operation is one of the core modules in MLP-based models to exchange and aggregate information between different spatial locations. However, the power of token interaction on the spatial dimension is highly dependent on the spatial resolution of the feature maps, which limits the model's expressive ability, especially in deep layers where the feature are down-sampled to a small spatial size. To address this issue, we present a novel method called \textbf{Strip-MLP} to enrich the token interaction power in three ways. Firstly, we introduce a new MLP paradigm called Strip MLP layer that allows the token to interact with other tokens in a cross-strip manner, enabling the tokens in a row (or column) to contribute to the information aggregations in adjacent but different strips of rows (or columns). Secondly, a \textbf{C}ascade \textbf{G}roup \textbf{S}trip \textbf{M}ixing \textbf{M}odule (CGSMM) is proposed to overcome the performance degradation caused by small spatial feature size. The module allows tokens to interact more effectively in the manners of within-patch and cross-patch, which is independent to the feature spatial size. Finally, based on the Strip MLP layer, we propose a novel \textbf{L}ocal \textbf{S}trip \textbf{M}ixing \textbf{M}odule (LSMM) to boost the token interaction power in the local region. Extensive experiments demonstrate that Strip-MLP significantly improves the performance of MLP-based models on small datasets and obtains comparable or even better results on ImageNet. In particular, Strip-MLP models achieve higher average Top-1 accuracy than existing MLP-based models by +2.44\% on Caltech-101 and +2.16\% on CIFAR-100. The source codes will be available at~\href{https://github.com/Med-Process/Strip_MLP{https://github.com/Med-Process/Strip\_MLP}.
</details>
<details>
<summary>摘要</summary>
Token 交互操作是 MLB 模型中的核心模块，用于在不同的空间位置交换和聚合信息。然而，Token 交互力在空间维度上受特定的空间分辨率限制，尤其是在深层次 Where Feature  Maps 下采用了压缩，这限制了模型的表达能力。为解决这问题，我们提出了一种新的方法 called Strip-MLP，它可以在三个方面增强 Token 交互力。首先，我们引入了一种新的 MLP  парадиг called Strip MLP 层，允许 Token 在横向（或纵向）方向上交互，使得在同一行（或同一列）的 Token 能够参与到邻近但不同的横向（或纵向）的信息聚合中。其次，我们提出了一种 Cascade Group Strip Mixing Module (CGSMM)，用于解决因特定空间分辨率而导致的性能下降。该模块允许 Token 在不同的横向和纵向上进行有效的交互，不受特定空间分辨率的限制。最后，基于 Strip MLP 层，我们提出了一种 Local Strip Mixing Module (LSMM)，用于在本地区域中增强 Token 交互力。广泛的实验表明，Strip-MLP 可以大幅提高 MLP 模型在小 datasets 上的表现，并在 ImageNet 上达到相当或更好的结果。具体来说，Strip-MLP 模型在 Caltech-101 和 CIFAR-100 上的平均 Top-1 准确率高于现有 MLP 模型 +2.44% 和 +2.16%。代码将在 GitHub 上公开，可以通过 <https://github.com/Med-Process/Strip_MLP> 访问。
</details></li>
</ul>
<hr>
<h2 id="Attention-Consistency-Refined-Masked-Frequency-Forgery-Representation-for-Generalizing-Face-Forgery-Detection"><a href="#Attention-Consistency-Refined-Masked-Frequency-Forgery-Representation-for-Generalizing-Face-Forgery-Detection" class="headerlink" title="Attention Consistency Refined Masked Frequency Forgery Representation for Generalizing Face Forgery Detection"></a>Attention Consistency Refined Masked Frequency Forgery Representation for Generalizing Face Forgery Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11438">http://arxiv.org/abs/2307.11438</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chenboluo/acmf">https://github.com/chenboluo/acmf</a></li>
<li>paper_authors: Decheng Liu, Tao Chen, Chunlei Peng, Nannan Wang, Ruimin Hu, Xinbo Gao</li>
<li>for: 这个论文旨在提高Visual data forgery detection的能力，以应对社会和经济安全中的复杂问题。</li>
<li>methods: 本论文提出了一个新的Attention Consistency Refined masked frequency forgery representation模型（ACMF），用于实现面伪造检测器的普遍化能力。</li>
<li>results: 实验结果显示，提案的方法在多个公共面伪造数据集（FaceForensic++, DFD, Celeb-DF, WDF数据集）上表现出色，较前一代方法更好。<details>
<summary>Abstract</summary>
Due to the successful development of deep image generation technology, visual data forgery detection would play a more important role in social and economic security. Existing forgery detection methods suffer from unsatisfactory generalization ability to determine the authenticity in the unseen domain. In this paper, we propose a novel Attention Consistency Refined masked frequency forgery representation model toward generalizing face forgery detection algorithm (ACMF). Most forgery technologies always bring in high-frequency aware cues, which make it easy to distinguish source authenticity but difficult to generalize to unseen artifact types. The masked frequency forgery representation module is designed to explore robust forgery cues by randomly discarding high-frequency information. In addition, we find that the forgery attention map inconsistency through the detection network could affect the generalizability. Thus, the forgery attention consistency is introduced to force detectors to focus on similar attention regions for better generalization ability. Experiment results on several public face forgery datasets (FaceForensic++, DFD, Celeb-DF, and WDF datasets) demonstrate the superior performance of the proposed method compared with the state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
Existing forgery detection methods often rely on high-frequency aware cues, which make it easy to distinguish between authentic and forged sources but difficult to generalize to unseen artifact types. To address this limitation, our proposed masked frequency forgery representation module is designed to explore robust forgery cues by randomly discarding high-frequency information.In addition, we find that the forgery attention map inconsistency through the detection network can affect the generalizability of the model. To address this issue, we introduce forgery attention consistency to force detectors to focus on similar attention regions for better generalization ability.Experimental results on several public face forgery datasets (FaceForensic++, DFD, Celeb-DF, and WDF datasets) demonstrate the superior performance of our proposed method compared to state-of-the-art methods.
</details></li>
</ul>
<hr>
<h2 id="FaceCLIPNeRF-Text-driven-3D-Face-Manipulation-using-Deformable-Neural-Radiance-Fields"><a href="#FaceCLIPNeRF-Text-driven-3D-Face-Manipulation-using-Deformable-Neural-Radiance-Fields" class="headerlink" title="FaceCLIPNeRF: Text-driven 3D Face Manipulation using Deformable Neural Radiance Fields"></a>FaceCLIPNeRF: Text-driven 3D Face Manipulation using Deformable Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11418">http://arxiv.org/abs/2307.11418</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sungwon Hwang, Junha Hyung, Daejin Kim, Min-Jung Kim, Jaegul Choo</li>
<li>for: 本研究旨在提供一种基于文本的3D人脸扭曲控制方法，使得非专家用户可以通过单个文本来控制基于NeRF的3D人脸重建。</li>
<li>methods: 我们的方法首先训练了一个场景扭曲器，一个基于秘密码的可变NeRF，以控制人脸扭曲使用秘密码。然而，表示场景扭曲的单个秘密码不利于分布式扭曲观察到的不同实例中。因此，我们提出了一种Positional-conditional Anchor Compositor（PAC），以学习表示扭曲场景的空间分布式秘密码。他们的渲染结果与场景扭曲器进行优化，以实现高cosine相似性与目标文本在CLIP空间的预测 embedding。</li>
<li>results: 根据我们所知，我们的方法是首次Addressing the text-driven manipulation of a face reconstructed with NeRF。我们的实验、比较和剖析研究表明，我们的方法可以实现高质量的文本驱动扭曲控制。<details>
<summary>Abstract</summary>
As recent advances in Neural Radiance Fields (NeRF) have enabled high-fidelity 3D face reconstruction and novel view synthesis, its manipulation also became an essential task in 3D vision. However, existing manipulation methods require extensive human labor, such as a user-provided semantic mask and manual attribute search unsuitable for non-expert users. Instead, our approach is designed to require a single text to manipulate a face reconstructed with NeRF. To do so, we first train a scene manipulator, a latent code-conditional deformable NeRF, over a dynamic scene to control a face deformation using the latent code. However, representing a scene deformation with a single latent code is unfavorable for compositing local deformations observed in different instances. As so, our proposed Position-conditional Anchor Compositor (PAC) learns to represent a manipulated scene with spatially varying latent codes. Their renderings with the scene manipulator are then optimized to yield high cosine similarity to a target text in CLIP embedding space for text-driven manipulation. To the best of our knowledge, our approach is the first to address the text-driven manipulation of a face reconstructed with NeRF. Extensive results, comparisons, and ablation studies demonstrate the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
To achieve this, we first train a scene manipulator, a latent code-conditional deformable NeRF, over a dynamic scene to control face deformations using the latent code. However, representing a scene deformation with a single latent code is not ideal for compositing local deformations observed in different instances. Therefore, we propose the Position-conditional Anchor Compositor (PAC) to learn how to represent a manipulated scene with spatially varying latent codes. The renderings with the scene manipulator are then optimized to have high cosine similarity to a target text in CLIP embedding space for text-driven manipulation.To the best of our knowledge, our approach is the first to address text-driven manipulation of faces reconstructed with NeRF. Our extensive results, comparisons, and ablation studies demonstrate the effectiveness of our approach.
</details></li>
</ul>
<hr>
<h2 id="Subject-Diffusion-Open-Domain-Personalized-Text-to-Image-Generation-without-Test-time-Fine-tuning"><a href="#Subject-Diffusion-Open-Domain-Personalized-Text-to-Image-Generation-without-Test-time-Fine-tuning" class="headerlink" title="Subject-Diffusion:Open Domain Personalized Text-to-Image Generation without Test-time Fine-tuning"></a>Subject-Diffusion:Open Domain Personalized Text-to-Image Generation without Test-time Fine-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11410">http://arxiv.org/abs/2307.11410</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/OPPO-Mente-Lab/Subject-Diffusion">https://github.com/OPPO-Mente-Lab/Subject-Diffusion</a></li>
<li>paper_authors: Jian Ma, Junhao Liang, Chen Chen, Haonan Lu</li>
<li>for: 这篇论文主要targets open-domain和non-fine-tuning个性化图像生成领域的进步。</li>
<li>methods: 该论文提出了一种新的开放领域个性化图像生成模型，只需要一张参考图像来支持单或多个主题的图像生成。</li>
<li>results: 对比其他SOTA框架，该方法在单个、多个和人类自定义图像生成方面具有优异的表现。<details>
<summary>Abstract</summary>
Recent progress in personalized image generation using diffusion models has been significant. However, development in the area of open-domain and non-fine-tuning personalized image generation is proceeding rather slowly. In this paper, we propose Subject-Diffusion, a novel open-domain personalized image generation model that, in addition to not requiring test-time fine-tuning, also only requires a single reference image to support personalized generation of single- or multi-subject in any domain. Firstly, we construct an automatic data labeling tool and use the LAION-Aesthetics dataset to construct a large-scale dataset consisting of 76M images and their corresponding subject detection bounding boxes, segmentation masks and text descriptions. Secondly, we design a new unified framework that combines text and image semantics by incorporating coarse location and fine-grained reference image control to maximize subject fidelity and generalization. Furthermore, we also adopt an attention control mechanism to support multi-subject generation. Extensive qualitative and quantitative results demonstrate that our method outperforms other SOTA frameworks in single, multiple, and human customized image generation. Please refer to our \href{https://oppo-mente-lab.github.io/subject_diffusion/}{project page}
</details>
<details>
<summary>摘要</summary>
最近几年个性化图像生成采用扩散模型的进步很 significative。然而，开放领域和不需要微调的个性化图像生成领域的发展相对较慢。在这篇论文中，我们提议了一种新的开放领域个性化图像生成模型，即主题扩散（Subject-Diffusion）。这种模型不仅不需要测试时微调，而且只需一个参考图像来支持个性化生成单或多主题图像。首先，我们构建了一个自动数据标签工具，并使用LAION-Aesthetics dataset constructed a large-scale dataset consisting of 76M images and their corresponding subject detection bounding boxes, segmentation masks and text descriptions。然后，我们设计了一个新的统一框架，通过 combining text and image semantics，并通过粗略位置和细腻参考图像控制来 maximize subject fidelity and generalization。此外，我们还采用了一种注意控制机制来支持多主题生成。我们的方法在单、多和人自定义图像生成方面均有优秀表现。详细的质量和量测试结果可以参考我们的 \href{https://oppo-mente-lab.github.io/subject_diffusion/}{项目页面}。
</details></li>
</ul>
<hr>
<h2 id="Latent-OFER-Detect-Mask-and-Reconstruct-with-Latent-Vectors-for-Occluded-Facial-Expression-Recognition"><a href="#Latent-OFER-Detect-Mask-and-Reconstruct-with-Latent-Vectors-for-Occluded-Facial-Expression-Recognition" class="headerlink" title="Latent-OFER: Detect, Mask, and Reconstruct with Latent Vectors for Occluded Facial Expression Recognition"></a>Latent-OFER: Detect, Mask, and Reconstruct with Latent Vectors for Occluded Facial Expression Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11404">http://arxiv.org/abs/2307.11404</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leeisack/latent-ofer">https://github.com/leeisack/latent-ofer</a></li>
<li>paper_authors: Isack Lee, Eungi Lee, Seok Bong Yoo</li>
<li>for: 提高实际场景中人脸表达识别（FER）的性能，解决 occluded FER（OFER）问题。</li>
<li>methods: 使用视transformer（ViT）基于 occlusion patch detector，检测 occluded 位置，并使用 hybrid reconstruction network 生成掩蔽位置的完整图像。然后，使用 CNN 基于 class activation map 提取表达相关的latent vector。</li>
<li>results: 对多个数据库进行了实验，demonstrated 提档方法的优势，比state-of-the-art方法更高效。<details>
<summary>Abstract</summary>
Most research on facial expression recognition (FER) is conducted in highly controlled environments, but its performance is often unacceptable when applied to real-world situations. This is because when unexpected objects occlude the face, the FER network faces difficulties extracting facial features and accurately predicting facial expressions. Therefore, occluded FER (OFER) is a challenging problem. Previous studies on occlusion-aware FER have typically required fully annotated facial images for training. However, collecting facial images with various occlusions and expression annotations is time-consuming and expensive. Latent-OFER, the proposed method, can detect occlusions, restore occluded parts of the face as if they were unoccluded, and recognize them, improving FER accuracy. This approach involves three steps: First, the vision transformer (ViT)-based occlusion patch detector masks the occluded position by training only latent vectors from the unoccluded patches using the support vector data description algorithm. Second, the hybrid reconstruction network generates the masking position as a complete image using the ViT and convolutional neural network (CNN). Last, the expression-relevant latent vector extractor retrieves and uses expression-related information from all latent vectors by applying a CNN-based class activation map. This mechanism has a significant advantage in preventing performance degradation from occlusion by unseen objects. The experimental results on several databases demonstrate the superiority of the proposed method over state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
大多数人脸表达识别（FER）研究都进行在高度控制的环境中，但其在实际场景中的性能往往不受接受。这是因为当不期望的物体遮挡面部时，FER网络很难提取面部特征并正确预测面部表达。因此，遮挡FER（OFER）成为一个挑战性问题。先前的 occlusion-aware FER 研究通常需要全部标注的面部图像进行训练。然而，收集面部图像与不同遮挡物和表达注解是时间consuming 和昂贵的。我们提出的方法是 Latent-OFER，它可以检测遮挡，还原遮挡部分的面部图像，并正确识别它们，从而提高 FER 的准确率。这个方法包括三个步骤：1. 使用支持向量数据描述算法来训练 ViT 基于的 occlusion patch detector，将遮挡位置掩码为不遮挡位置的 latent vector。2. 使用 ViT 和卷积神经网络（CNN）来生成遮挡位置的完整图像。3. 使用 CNN 来提取表达相关的 latent vector，并应用类Activation map 来找到表达相关的信息。这种机制有着显著的优势，可以避免由不可见的遮挡物引起的性能下降。我们在多个数据库上进行了多个实验，结果表明，我们的方法在 state-of-the-art 方法之上表现出了明显的优势。
</details></li>
</ul>
<hr>
<h2 id="CLR-Channel-wise-Lightweight-Reprogramming-for-Continual-Learning"><a href="#CLR-Channel-wise-Lightweight-Reprogramming-for-Continual-Learning" class="headerlink" title="CLR: Channel-wise Lightweight Reprogramming for Continual Learning"></a>CLR: Channel-wise Lightweight Reprogramming for Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11386">http://arxiv.org/abs/2307.11386</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gyhandy/channel-wise-lightweight-reprogramming">https://github.com/gyhandy/channel-wise-lightweight-reprogramming</a></li>
<li>paper_authors: Yunhao Ge, Yuecheng Li, Shuo Ni, Jiaping Zhao, Ming-Hsuan Yang, Laurent Itti</li>
<li>for: 这个研究旨在解决 kontinual learning 中的问题，即维持先前学习的任务表现度以后学习新任务。</li>
<li>methods: 我们提出了一个 Channel-wise Lightweight Reprogramming (CLR) 方法，帮助条件神经网络 (CNN) 在 kontinual learning 中避免 catastrophic forgetting。我们使用一个轻量级 (very cheap) 的重新程式码参数，将 CNN 模型训练在旧任务 (或自我指导 proxy task) 后，转换为解决新任务。</li>
<li>results: 我们的方法可以实现 better stability-plasticity trade-off，即维持先前任务表现度并吸收新知识。我们的方法比 13 个 state-of-the-art kontinual learning 基线测试项目表现出色，在一个新的挑战性的序列中的 53 个图像分类任务中。<details>
<summary>Abstract</summary>
Continual learning aims to emulate the human ability to continually accumulate knowledge over sequential tasks. The main challenge is to maintain performance on previously learned tasks after learning new tasks, i.e., to avoid catastrophic forgetting. We propose a Channel-wise Lightweight Reprogramming (CLR) approach that helps convolutional neural networks (CNNs) overcome catastrophic forgetting during continual learning. We show that a CNN model trained on an old task (or self-supervised proxy task) could be ``reprogrammed" to solve a new task by using our proposed lightweight (very cheap) reprogramming parameter. With the help of CLR, we have a better stability-plasticity trade-off to solve continual learning problems: To maintain stability and retain previous task ability, we use a common task-agnostic immutable part as the shared ``anchor" parameter set. We then add task-specific lightweight reprogramming parameters to reinterpret the outputs of the immutable parts, to enable plasticity and integrate new knowledge. To learn sequential tasks, we only train the lightweight reprogramming parameters to learn each new task. Reprogramming parameters are task-specific and exclusive to each task, which makes our method immune to catastrophic forgetting. To minimize the parameter requirement of reprogramming to learn new tasks, we make reprogramming lightweight by only adjusting essential kernels and learning channel-wise linear mappings from anchor parameters to task-specific domain knowledge. We show that, for general CNNs, the CLR parameter increase is less than 0.6\% for any new task. Our method outperforms 13 state-of-the-art continual learning baselines on a new challenging sequence of 53 image classification datasets. Code and data are available at https://github.com/gyhandy/Channel-wise-Lightweight-Reprogramming
</details>
<details>
<summary>摘要</summary>
“CONTINUAL LEARNING”目的是实现人类在继续完成多个任务后继续累累积累知识的能力。主要挑战是维持先前学习的任务表现，即避免“悖论”现象。我们提出了一个“通道对适应”（Channel-wise Lightweight Reprogramming，CLR）方法，帮助对称神经网络（CNNs）在继续学习中获得更好的稳定性和柔韧性。我们显示了一个 CNN 模型在旧任务（或自我超vised proxy task）上训练后可以通过我们提议的轻量级（非常便宜）重新配置参数，以解决继续学习中的悖论问题。我们在 CLR 方法中使用了一个通用任务无法适应的固定“标准”（immutable）部分作为共同“anchor”参数集，然后将任务特定的轻量级重新配置参数添加到标准部分以重新解读出PUTS，以获得更好的稳定性和柔韧性。为了学习继续任务，我们仅需要对每个新任务进行轻量级重新配置参数的训练。重新配置参数是任务特定的且对应到每个任务，这使我们的方法免于悖论现象。为了实现轻量级重新配置参数的学习，我们仅将重要的核心变化和通道对适应的linear mapping学习。我们发现，对于通用 CNN，CLR 参数增加的比例小于 0.6%  для任何新任务。我们的方法在一个新的53个图像分类任务中进行了比较，并与13个现有的基eline方法进行了比较。代码和数据可以在 https://github.com/gyhandy/Channel-wise-Lightweight-Reprogramming 上获取。
</details></li>
</ul>
<hr>
<h2 id="LatentAugment-Data-Augmentation-via-Guided-Manipulation-of-GAN’s-Latent-Space"><a href="#LatentAugment-Data-Augmentation-via-Guided-Manipulation-of-GAN’s-Latent-Space" class="headerlink" title="LatentAugment: Data Augmentation via Guided Manipulation of GAN’s Latent Space"></a>LatentAugment: Data Augmentation via Guided Manipulation of GAN’s Latent Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11375">http://arxiv.org/abs/2307.11375</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ltronchin/latentaugment">https://github.com/ltronchin/latentaugment</a></li>
<li>paper_authors: Lorenzo Tronchin, Minh H. Vu, Paolo Soda, Tommy Löfstedt</li>
<li>for: 这个研究旨在提高训练数据的量和多样性，以降低适架化和提高泛化。</li>
<li>methods: 这个研究使用生成数据网络（GAN）生成高质量的 sintetic 数据，并通过修改秘密 вектор来增加模式覆盖率和多样性。</li>
<li>results: 研究显示，使用 LatentAugment 技术可以提高深度模型从 MRI-to-CT 的转换性能，并在模式覆盖率和多样性方面超过 GAN-based sampling。<details>
<summary>Abstract</summary>
Data Augmentation (DA) is a technique to increase the quantity and diversity of the training data, and by that alleviate overfitting and improve generalisation. However, standard DA produces synthetic data for augmentation with limited diversity. Generative Adversarial Networks (GANs) may unlock additional information in a dataset by generating synthetic samples having the appearance of real images. However, these models struggle to simultaneously address three key requirements: fidelity and high-quality samples; diversity and mode coverage; and fast sampling. Indeed, GANs generate high-quality samples rapidly, but have poor mode coverage, limiting their adoption in DA applications. We propose LatentAugment, a DA strategy that overcomes the low diversity of GANs, opening up for use in DA applications. Without external supervision, LatentAugment modifies latent vectors and moves them into latent space regions to maximise the synthetic images' diversity and fidelity. It is also agnostic to the dataset and the downstream task. A wide set of experiments shows that LatentAugment improves the generalisation of a deep model translating from MRI-to-CT beating both standard DA as well GAN-based sampling. Moreover, still in comparison with GAN-based sampling, LatentAugment synthetic samples show superior mode coverage and diversity. Code is available at: https://github.com/ltronchin/LatentAugment.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>数据扩展（DA）是一种技术，以增加训练数据的量和多样性，从而缓解过拟合和提高泛化。然而，标准的DA产生的合成数据具有有限的多样性。生成对抗网络（GANs）可以通过生成具有真实图像的样式的合成样本，从而激活数据中的额外信息。然而，这些模型很难同时满足三个关键要求：准确性和高质量样本；多样性和模式覆盖率；和快速采样。indeed，GANs可以快速生成高质量样本，但它们的模式覆盖率很低，限制了它们在DA应用中的采用。我们提出了LatentAugment，一种DA策略，可以在GANs中增加多样性，使其在DA应用中使用。无需外部监督，LatentAugment会修改缺少的缺省向量，将其移动到缺省空间中，以最大化合成图像的多样性和准确性。它还是数据aset和下游任务无关的。一系列实验表明，LatentAugment可以提高一个深度模型从MRI-to-CT的翻译，比标准DA和GAN-based sampling更好。此外，与GAN-based sampling相比，LatentAugment的合成样本还显示出更高的模式覆盖率和多样性。代码可以在：https://github.com/ltronchin/LatentAugment.
</details></li>
</ul>
<hr>
<h2 id="Photo2Relief-Let-Human-in-the-Photograph-Stand-Out"><a href="#Photo2Relief-Let-Human-in-the-Photograph-Stand-Out" class="headerlink" title="Photo2Relief: Let Human in the Photograph Stand Out"></a>Photo2Relief: Let Human in the Photograph Stand Out</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11364">http://arxiv.org/abs/2307.11364</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhongping Ji, Feifei Che, Hanshuo Liu, Ziyi Zhao, Yu-Wei Zhang, Wenping Wang</li>
<li>for: 这 paper 的目的是创造从照片中生成的数字 2.5D 艺术作品，其中描绘人体的全身活动。</li>
<li>methods: 该方法使用了一种新的 sigmoid 变体函数来灵活控制梯度，并通过定义在梯度空间的损失函数来训练神经网络。此外，它还使用了图像基于的渲染技术来解决不同光照条件下的挑战。</li>
<li>results: 实验结果表明，该方法可以高效地从照片中生成高质量的数字 2.5D 艺术作品，并且可以在多种场景下实现高度的自然性和细节。<details>
<summary>Abstract</summary>
In this paper, we propose a technique for making humans in photographs protrude like reliefs. Unlike previous methods which mostly focus on the face and head, our method aims to generate art works that describe the whole body activity of the character. One challenge is that there is no ground-truth for supervised deep learning. We introduce a sigmoid variant function to manipulate gradients tactfully and train our neural networks by equipping with a loss function defined in gradient domain. The second challenge is that actual photographs often across different light conditions. We used image-based rendering technique to address this challenge and acquire rendering images and depth data under different lighting conditions. To make a clear division of labor in network modules, a two-scale architecture is proposed to create high-quality relief from a single photograph. Extensive experimental results on a variety of scenes show that our method is a highly effective solution for generating digital 2.5D artwork from photographs.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种技术，使人像中的人物凸出如 relief 一般。与先前的方法一样，我们的方法不仅关注人脸和头部，而是通过生成描述人物全身活动的艺术作品。一个挑战是没有超出真实数据的观察数据，我们引入了截然函数来策略性地操作梯度，并通过定义梯度领域的损失函数来训练我们的神经网络。另一个挑战是实际照片通常在不同的照明条件下拍摄，我们使用图像基于的渲染技术来解决这个问题，并获得不同照明条件下的渲染图像和深度数据。为了在网络模块之间进行清晰的划分工作，我们提议了一种两级架构，从单个照片中生成高质量的 relief。我们的实验结果表明，我们的方法是生成数字2.5D艺术作品从照片中的高效解决方案。
</details></li>
</ul>
<hr>
<h2 id="ParGANDA-Making-Synthetic-Pedestrians-A-Reality-For-Object-Detection"><a href="#ParGANDA-Making-Synthetic-Pedestrians-A-Reality-For-Object-Detection" class="headerlink" title="ParGANDA: Making Synthetic Pedestrians A Reality For Object Detection"></a>ParGANDA: Making Synthetic Pedestrians A Reality For Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11360">http://arxiv.org/abs/2307.11360</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daria Reshetova, Guanhang Wu, Marcel Puyat, Chunhui Gu, Huizhong Chen</li>
<li>for: 本研究旨在提高人体检测模型的性能，使其在实际应用中更加稳定和可靠。</li>
<li>methods: 本研究使用了生成 adversarial Network (GAN) 来将生成的人体图像变换成更加真实的图像，以减少 Synthetic-to-real 频率差。</li>
<li>results: 实验结果表明，使用 GAN 可以生成高质量的人体图像，并且不需要标注真实频率图像，因此可以应用于多个下游任务。<details>
<summary>Abstract</summary>
Object detection is the key technique to a number of Computer Vision applications, but it often requires large amounts of annotated data to achieve decent results. Moreover, for pedestrian detection specifically, the collected data might contain some personally identifiable information (PII), which is highly restricted in many countries. This label intensive and privacy concerning task has recently led to an increasing interest in training the detection models using synthetically generated pedestrian datasets collected with a photo-realistic video game engine. The engine is able to generate unlimited amounts of data with precise and consistent annotations, which gives potential for significant gains in the real-world applications. However, the use of synthetic data for training introduces a synthetic-to-real domain shift aggravating the final performance. To close the gap between the real and synthetic data, we propose to use a Generative Adversarial Network (GAN), which performsparameterized unpaired image-to-image translation to generate more realistic images. The key benefit of using the GAN is its intrinsic preference of low-level changes to geometric ones, which means annotations of a given synthetic image remain accurate even after domain translation is performed thus eliminating the need for labeling real data. We extensively experimented with the proposed method using MOTSynth dataset to train and MOT17 and MOT20 detection datasets to test, with experimental results demonstrating the effectiveness of this method. Our approach not only produces visually plausible samples but also does not require any labels of the real domain thus making it applicable to the variety of downstream tasks.
</details>
<details>
<summary>摘要</summary>
Computer视觉应用中的对象检测是关键技术，但它通常需要大量注解数据来 достичь良好的结果。此外，人员检测特别是可能包含个人标识信息（PII），这在许多国家是非常限制的。这种标注密集和隐私担忧的任务最近引起了使用synthetically生成的人员数据集来训练检测模型的兴趣。这个引擎可以生成无限量的数据，并且可以提供精确和一致的注释，这给了实际应用中的可能性。然而，使用synthetic数据进行训练会导致synthetic-to-real域shift，从而使得最终性能下降。为了封闭这个域shift，我们提议使用生成对抗网络（GAN），它可以进行参数化的无对比image-to-image翻译，以生成更真实的图像。GAN的关键优点在于它对低级别的变化偏好，这意味着注释给定的synthetic图像保持正确，even after domain translation is performed，因此不需要标注实际数据。我们对提议方法进行了广泛的实验，使用MOTSynth数据集进行训练，并使用MOT17和MOT20检测数据集进行测试，实验结果表明了该方法的有效性。我们的方法不仅可以生成可见的样本，而且不需要实际域的标注，因此可以应用于多个下游任务。
</details></li>
</ul>
<hr>
<h2 id="Tuning-Pre-trained-Model-via-Moment-Probing"><a href="#Tuning-Pre-trained-Model-via-Moment-Probing" class="headerlink" title="Tuning Pre-trained Model via Moment Probing"></a>Tuning Pre-trained Model via Moment Probing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11342">http://arxiv.org/abs/2307.11342</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mingzeg/moment-probing">https://github.com/mingzeg/moment-probing</a></li>
<li>paper_authors: Mingze Gao, Qilong Wang, Zhenyi Lin, Pengfei Zhu, Qinghua Hu, Jingbo Zhou</li>
<li>for: 提高大规模预训练模型的高效调参，以探索LP模块的潜力。</li>
<li>methods: 提出了一种新的幻数探测（MP）方法，通过对特征分布的线性探测来提供更强的表示能力。</li>
<li>results: MP比LP更高效，并且与同类模型在训练成本下达到了竞争水平，而MP$_{+}$达到了最佳性能。<details>
<summary>Abstract</summary>
Recently, efficient fine-tuning of large-scale pre-trained models has attracted increasing research interests, where linear probing (LP) as a fundamental module is involved in exploiting the final representations for task-dependent classification. However, most of the existing methods focus on how to effectively introduce a few of learnable parameters, and little work pays attention to the commonly used LP module. In this paper, we propose a novel Moment Probing (MP) method to further explore the potential of LP. Distinguished from LP which builds a linear classification head based on the mean of final features (e.g., word tokens for ViT) or classification tokens, our MP performs a linear classifier on feature distribution, which provides the stronger representation ability by exploiting richer statistical information inherent in features. Specifically, we represent feature distribution by its characteristic function, which is efficiently approximated by using first- and second-order moments of features. Furthermore, we propose a multi-head convolutional cross-covariance (MHC$^3$) to compute second-order moments in an efficient and effective manner. By considering that MP could affect feature learning, we introduce a partially shared module to learn two recalibrating parameters (PSRP) for backbones based on MP, namely MP$_{+}$. Extensive experiments on ten benchmarks using various models show that our MP significantly outperforms LP and is competitive with counterparts at less training cost, while our MP$_{+}$ achieves state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
近期，高效练级预训模型的调整吸引了增加的研究兴趣，其中线性探测（LP）作为基本模块被利用来挖掘任务特定的分类表现。然而，大多数现有方法都是如何效果地引入一些学习参数的问题，而忽略了通常使用的LP模块。在这篇论文中，我们提出了一种新的幂值探测（MP）方法，可以进一步探索LP的潜力。与LP不同，MP不是根据最终特征（如ViT中的单词符）或分类符建立线性分类头，而是在特征分布上进行线性分类，从而获得更强的表达能力，因为它可以利用特征内置的更加丰富的统计信息。具体来说，我们使用特征分布的特征函数来表示特征分布，该函数可以高效地被approximated通过使用特征的第一和第二 moments。此外，我们还提出了一种多头 convolutional cross-covariance（MHC$^3）来计算特征分布中的第二 moments，以实现高效和有效地计算。由于MP可能会影响特征学习，我们引入了一个共享模块来学习两个修正参数（PSRP），即MP$_{+}$.经验表明，我们的MP在十个benchmark上与LP和其他方法进行比较，显示MP显著超过LP，并且与其他方法在训练成本下更低的情况下具有竞争力。此外，我们的MP$_{+}$实现了状态计算的表现。
</details></li>
</ul>
<hr>
<h2 id="Character-Time-series-Matching-For-Robust-License-Plate-Recognition"><a href="#Character-Time-series-Matching-For-Robust-License-Plate-Recognition" class="headerlink" title="Character Time-series Matching For Robust License Plate Recognition"></a>Character Time-series Matching For Robust License Plate Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11336">http://arxiv.org/abs/2307.11336</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chequanghuy/Character-Time-series-Matching">https://github.com/chequanghuy/Character-Time-series-Matching</a></li>
<li>paper_authors: Quang Huy Che, Tung Do Thanh, Cuong Truong Van</li>
<li>For: This paper aims to improve license plate recognition accuracy in real-world situations by tracking the license plate in multiple frames.* Methods: The proposed method uses the Adaptive License Plate Rotation algorithm to correctly align the detected license plate, and a new method called Character Time-series Matching to recognize license plate characters from multiple consecutive frames.* Results: The proposed method achieved 96.7% accuracy on the UFPR-ALPR dataset in real-time on an RTX A5000 GPU card, and achieved high accuracy in the Vietnamese ALPR system with license plate detection and character recognition accuracy of 0.881 and 0.979 $mAP^{test}$@.5 respectively.Here’s the Chinese translation of the three key points:*  FOR: 这篇论文目标是在实际情况下提高车牌识别精度，通过跟踪多帧中的车牌。*  METHODS: 提议的方法使用了适应式车牌旋转算法来正确地对齐检测到的车牌，以及一种新的Character Time-series Matching方法来在多个后续帧中识别车牌字符。* RESULTS: 提议的方法在UFPR-ALPR数据集上实时达到96.7%的准确率，并在越南的ALPR系统中实现了高精度。车牌检测精度和字符识别精度分别为0.881和0.979 $mAP^{test}$@.5。<details>
<summary>Abstract</summary>
Automatic License Plate Recognition (ALPR) is becoming a popular study area and is applied in many fields such as transportation or smart city. However, there are still several limitations when applying many current methods to practical problems due to the variation in real-world situations such as light changes, unclear License Plate (LP) characters, and image quality. Almost recent ALPR algorithms process on a single frame, which reduces accuracy in case of worse image quality. This paper presents methods to improve license plate recognition accuracy by tracking the license plate in multiple frames. First, the Adaptive License Plate Rotation algorithm is applied to correctly align the detected license plate. Second, we propose a method called Character Time-series Matching to recognize license plate characters from many consequence frames. The proposed method archives high performance in the UFPR-ALPR dataset which is \boldmath$96.7\%$ accuracy in real-time on RTX A5000 GPU card. We also deploy the algorithm for the Vietnamese ALPR system. The accuracy for license plate detection and character recognition are 0.881 and 0.979 $mAP^{test}$@.5 respectively. The source code is available at https://github.com/chequanghuy/Character-Time-series-Matching.git
</details>
<details>
<summary>摘要</summary>
自动识别车牌（ALPR）已成为当前研究领域之一，并应用于交通和智能城市等领域。然而，现有许多方法在实际问题中仍然存在一些限制，主要是因为实际情况下的光线变化、车牌字符模糊和图像质量等问题。大多数当前ALPR算法都是基于单帧处理，这会导致图像质量更差时的准确率下降。本文提出了一种改进车牌识别精度的方法，通过跟踪车牌在多帧中的变化。首先，我们提出了一种适应车牌旋转算法，以正确地对检测到的车牌进行对齐。然后，我们提出了一种名为 Character Time-series Matching的方法，用于在多个后续帧中识别车牌字符。我们在UFPR-ALPR数据集上测试了该方法，实时性达到96.7%，并在RTX A5000 GPU卡上进行了测试。此外，我们还部署了该算法于越南ALPR系统，车牌检测精度和字符识别精度分别为0.881和0.979 $mAP^{test}$@.5。源代码可以在https://github.com/chequanghuy/Character-Time-series-Matching.git中下载。
</details></li>
</ul>
<hr>
<h2 id="Improving-Transferability-of-Adversarial-Examples-via-Bayesian-Attacks"><a href="#Improving-Transferability-of-Adversarial-Examples-via-Bayesian-Attacks" class="headerlink" title="Improving Transferability of Adversarial Examples via Bayesian Attacks"></a>Improving Transferability of Adversarial Examples via Bayesian Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11334">http://arxiv.org/abs/2307.11334</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qizhang Li, Yiwen Guo, Xiaochen Yang, Wangmeng Zuo, Hao Chen</li>
<li>for: 提高模型转移性，防止黑客利用模型识别图像的攻击</li>
<li>methods:  incorporating Bayesian formulation into model parameters and input, 采用权重积分法对模型参数和输入进行权重补偿</li>
<li>results: 1) 将极大 Bayesian 形式应用于模型输入和参数，对转移性具有显著提高效果; 2) 通过高级推论 posterior distribution over the model input, 提高黑客攻击的可扩展性; 3) 提出一种理性的方法来细化模型参数，以便在扩展 bayesian 形式下进行优化。<details>
<summary>Abstract</summary>
This paper presents a substantial extension of our work published at ICLR. Our ICLR work advocated for enhancing transferability in adversarial examples by incorporating a Bayesian formulation into model parameters, which effectively emulates the ensemble of infinitely many deep neural networks, while, in this paper, we introduce a novel extension by incorporating the Bayesian formulation into the model input as well, enabling the joint diversification of both the model input and model parameters. Our empirical findings demonstrate that: 1) the combination of Bayesian formulations for both the model input and model parameters yields significant improvements in transferability; 2) by introducing advanced approximations of the posterior distribution over the model input, adversarial transferability achieves further enhancement, surpassing all state-of-the-arts when attacking without model fine-tuning. Moreover, we propose a principled approach to fine-tune model parameters in such an extended Bayesian formulation. The derived optimization objective inherently encourages flat minima in the parameter space and input space. Extensive experiments demonstrate that our method achieves a new state-of-the-art on transfer-based attacks, improving the average success rate on ImageNet and CIFAR-10 by 19.14% and 2.08%, respectively, when comparing with our ICLR basic Bayesian method. We will make our code publicly available.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Combining Bayesian formulations for both the model input and model parameters leads to significant improvements in transferability.2. Introducing advanced approximations of the posterior distribution over the model input further enhances adversarial transferability, surpassing all state-of-the-art methods when attacking without model fine-tuning.Moreover, we propose a principled approach to fine-tune model parameters in this extended Bayesian formulation. The derived optimization objective inherently encourages flat minima in both the parameter space and input space. Extensive experiments demonstrate that our method achieves a new state-of-the-art on transfer-based attacks, improving the average success rate on ImageNet and CIFAR-10 by 19.14% and 2.08%, respectively, compared to our ICLR basic Bayesian method. We will make our code publicly available.</details></li>
</ol>
<hr>
<h2 id="EndoSurf-Neural-Surface-Reconstruction-of-Deformable-Tissues-with-Stereo-Endoscope-Videos"><a href="#EndoSurf-Neural-Surface-Reconstruction-of-Deformable-Tissues-with-Stereo-Endoscope-Videos" class="headerlink" title="EndoSurf: Neural Surface Reconstruction of Deformable Tissues with Stereo Endoscope Videos"></a>EndoSurf: Neural Surface Reconstruction of Deformable Tissues with Stereo Endoscope Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11307">http://arxiv.org/abs/2307.11307</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ruyi-zha/endosurf">https://github.com/ruyi-zha/endosurf</a></li>
<li>paper_authors: Ruyi Zha, Xuelian Cheng, Hongdong Li, Mehrtash Harandi, Zongyuan Ge</li>
<li>for:  reconstruction of soft tissues from stereo endoscope videos</li>
<li>methods:  neural-field-based method (EndoSurf) using deformation field, SDF field, and radiance field for shape and texture representation</li>
<li>results:  significantly outperforms existing solutions in reconstructing high-fidelity shapes, as demonstrated by experiments on public endoscope datasets.<details>
<summary>Abstract</summary>
Reconstructing soft tissues from stereo endoscope videos is an essential prerequisite for many medical applications. Previous methods struggle to produce high-quality geometry and appearance due to their inadequate representations of 3D scenes. To address this issue, we propose a novel neural-field-based method, called EndoSurf, which effectively learns to represent a deforming surface from an RGBD sequence. In EndoSurf, we model surface dynamics, shape, and texture with three neural fields. First, 3D points are transformed from the observed space to the canonical space using the deformation field. The signed distance function (SDF) field and radiance field then predict their SDFs and colors, respectively, with which RGBD images can be synthesized via differentiable volume rendering. We constrain the learned shape by tailoring multiple regularization strategies and disentangling geometry and appearance. Experiments on public endoscope datasets demonstrate that EndoSurf significantly outperforms existing solutions, particularly in reconstructing high-fidelity shapes. Code is available at https://github.com/Ruyi-Zha/endosurf.git.
</details>
<details>
<summary>摘要</summary>
<<SYS>>很多医疗应用都需要从斯tereo激光镜视频中重建软组织。现有方法很难生成高质量的几何和外观，因为它们不能准确地表示3D场景。为解决这个问题，我们提出了一种基于神经场的新方法，叫做EndoSurf。EndoSurf可以从RGBD序列中有效地学习表示变形表面。在EndoSurf中，我们使用三个神经场来模型表面动态、形状和Texture。首先，3D点从观察空间转换到Canonical空间使用扭变场。然后，SDF场和颜色场使用梯度渲染算法来预测它们的SDF和颜色。通过这种方式，我们可以通过 differentiable volume rendering来synthesizeRGBD图像。我们使用多种正则化策略和分解几何和外观来约束学习的形状。实验表明，EndoSurf在公共激光镜数据集上表现出色，特别是在重建高精度形状方面。代码可以在https://github.com/Ruyi-Zha/endosurf.git中找到。
</details></li>
</ul>
<hr>
<h2 id="MAS-Towards-Resource-Efficient-Federated-Multiple-Task-Learning"><a href="#MAS-Towards-Resource-Efficient-Federated-Multiple-Task-Learning" class="headerlink" title="MAS: Towards Resource-Efficient Federated Multiple-Task Learning"></a>MAS: Towards Resource-Efficient Federated Multiple-Task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11285">http://arxiv.org/abs/2307.11285</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiming Zhuang, Yonggang Wen, Lingjuan Lyu, Shuai Zhang</li>
<li>For: This paper proposes a new approach to training multiple simultaneous federated learning (FL) tasks on resource-constrained devices, which can improve the performance and efficiency of FL training.* Methods: The proposed approach, called MAS (Merge and Split), merges multiple FL tasks into an all-in-one task and then splits it into two or more tasks based on the affinities among tasks. It continues training each split of tasks using model parameters from the all-in-one training.* Results: Extensive experiments demonstrate that MAS outperforms other methods while reducing training time by 2x and reducing energy consumption by 40%.<details>
<summary>Abstract</summary>
Federated learning (FL) is an emerging distributed machine learning method that empowers in-situ model training on decentralized edge devices. However, multiple simultaneous FL tasks could overload resource-constrained devices. In this work, we propose the first FL system to effectively coordinate and train multiple simultaneous FL tasks. We first formalize the problem of training simultaneous FL tasks. Then, we present our new approach, MAS (Merge and Split), to optimize the performance of training multiple simultaneous FL tasks. MAS starts by merging FL tasks into an all-in-one FL task with a multi-task architecture. After training for a few rounds, MAS splits the all-in-one FL task into two or more FL tasks by using the affinities among tasks measured during the all-in-one training. It then continues training each split of FL tasks based on model parameters from the all-in-one training. Extensive experiments demonstrate that MAS outperforms other methods while reducing training time by 2x and reducing energy consumption by 40%. We hope this work will inspire the community to further study and optimize training simultaneous FL tasks.
</details>
<details>
<summary>摘要</summary>
Federated 学习（FL）是一种emerging的分布式机器学习方法，它允许在分布式边缘设备上进行 situ 模型训练。然而，多个同时进行 FL 任务可能会过载resource-constrained设备。在这种工作中，我们提出了首个有效地协调和训练多个同时进行 FL 任务的 FL 系统。我们首先将同时进行 FL 任务的训练问题正式化。然后，我们提出了我们的新方法，MAS（合并和分割），以便优化训练多个同时进行 FL 任务的性能。MAS 开始是将 FL 任务合并成一个所有任务的 FL 任务，并使用多任务架构进行训练。在训练一些往返后，MAS 使用在所有训练中测量的任务之间的相互关系，将所有任务合并成两个或更多个 FL 任务。然后，它继续基于所有训练中的模型参数进行每个分割的 FL 任务的训练。我们的实验证明，MAS 在减少训练时间和能量消耗的情况下，能够超越其他方法。我们希望这种工作能够鼓励社区进一步研究和优化训练同时进行 FL 任务。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Segment-from-Noisy-Annotations-A-Spatial-Correction-Approach"><a href="#Learning-to-Segment-from-Noisy-Annotations-A-Spatial-Correction-Approach" class="headerlink" title="Learning to Segment from Noisy Annotations: A Spatial Correction Approach"></a>Learning to Segment from Noisy Annotations: A Spatial Correction Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02498">http://arxiv.org/abs/2308.02498</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/michaelofsbu/spatialcorrection">https://github.com/michaelofsbu/spatialcorrection</a></li>
<li>paper_authors: Jiachen Yao, Yikai Zhang, Songzhu Zheng, Mayank Goswami, Prateek Prasanna, Chao Chen</li>
<li>for: 这个研究旨在提出一个新的Markov模型，用于减少医疗影像分类 задачі中的标签错误。</li>
<li>methods: 我们提出了一种基于Markov模型的标签修正方法，可以逐步修正错误的标签，并提供了理论保证。</li>
<li>results: 实验结果显示，我们的方法在实际标签错误情况下表现比现有的方法更好。<details>
<summary>Abstract</summary>
Noisy labels can significantly affect the performance of deep neural networks (DNNs). In medical image segmentation tasks, annotations are error-prone due to the high demand in annotation time and in the annotators' expertise. Existing methods mostly assume noisy labels in different pixels are \textit{i.i.d}. However, segmentation label noise usually has strong spatial correlation and has prominent bias in distribution. In this paper, we propose a novel Markov model for segmentation noisy annotations that encodes both spatial correlation and bias. Further, to mitigate such label noise, we propose a label correction method to recover true label progressively. We provide theoretical guarantees of the correctness of the proposed method. Experiments show that our approach outperforms current state-of-the-art methods on both synthetic and real-world noisy annotations.
</details>
<details>
<summary>摘要</summary>
“噪音标签可以严重影响深度神经网络（DNNs）的性能。医疗影像分类任务中的标签通常受到高度的标签时间和标签专家的要求，因此标签损害很普遍。现有的方法通常假设不同像素的标签噪音是独立同分布（i.i.d）。但是，分类标签噪音通常具有强烈的空间相关性和明显的偏好性。在这篇论文中，我们提出了一个新的Markov模型，用于描述分类标签噪音的特性。此外，我们也提出了一个标签更正方法，可以逐步地更正true标签。我们提供了理论保证方法的正确性。实验结果显示，我们的方法在实验和实际标签噪音上都能够超过目前的州Of-the-art方法。”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Screening-Mammography-Breast-Cancer-Detection"><a href="#Screening-Mammography-Breast-Cancer-Detection" class="headerlink" title="Screening Mammography Breast Cancer Detection"></a>Screening Mammography Breast Cancer Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11274">http://arxiv.org/abs/2307.11274</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chakrabortyde/rsna-breast-cancer">https://github.com/chakrabortyde/rsna-breast-cancer</a></li>
<li>paper_authors: Debajyoti Chakraborty</li>
<li>for: 提高乳癌检测效率和准确率，以减少成本和假阳性结果导致的患者担忧。</li>
<li>methods: 使用自动化乳癌检测方法，测试了不同的方法 against RSNA数据集中的约20,000名女性患者的 radiographic 乳癌图像，得到了平均验证案例 pF1 分数为0.56。</li>
<li>results: 实现了提高乳癌检测效率和准确率的目标，可能减少成本和假阳性结果导致的患者担忧。<details>
<summary>Abstract</summary>
Breast cancer is a leading cause of cancer-related deaths, but current programs are expensive and prone to false positives, leading to unnecessary follow-up and patient anxiety. This paper proposes a solution to automated breast cancer detection, to improve the efficiency and accuracy of screening programs. Different methodologies were tested against the RSNA dataset of radiographic breast images of roughly 20,000 female patients and yielded an average validation case pF1 score of 0.56 across methods.
</details>
<details>
<summary>摘要</summary>
乳癌是癌症related deaths的主要原因，但现有的Programs 昂贵并且容易出现假阳性结果，导致不必要的跟进和患者担忧。这篇论文提出一种自动乳癌检测方案，以提高检测计划的效率和准确率。不同的方法ologies 在 RSNA 数据集上测试，对约20,000名女性患者的 radiographic 乳影像进行了测试，Validation case pF1  score 的平均值为 0.56。
</details></li>
</ul>
<hr>
<h2 id="SimCol3D-–-3D-Reconstruction-during-Colonoscopy-Challenge"><a href="#SimCol3D-–-3D-Reconstruction-during-Colonoscopy-Challenge" class="headerlink" title="SimCol3D – 3D Reconstruction during Colonoscopy Challenge"></a>SimCol3D – 3D Reconstruction during Colonoscopy Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11261">http://arxiv.org/abs/2307.11261</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anita Rau, Sophia Bano, Yueming Jin, Pablo Azagra, Javier Morlana, Edward Sanderson, Bogdan J. Matuszewski, Jae Young Lee, Dong-Jae Lee, Erez Posner, Netanel Frank, Varshini Elangovan, Sista Raviteja, Zhengwen Li, Jiquan Liu, Seenivasan Lalithkumar, Mobarakol Islam, Hongliang Ren, José M. M. Montiel, Danail Stoyanov</li>
<li>for: 针对受到抗坏性肿瘤的护理和检测</li>
<li>methods: 使用学习型方法对视频材料进行深度和pose预测</li>
<li>results: 虚拟colonoscopy中的深度预测robust可行，而pose预测仍然是一个开放的研究问题<details>
<summary>Abstract</summary>
Colorectal cancer is one of the most common cancers in the world. While colonoscopy is an effective screening technique, navigating an endoscope through the colon to detect polyps is challenging. A 3D map of the observed surfaces could enhance the identification of unscreened colon tissue and serve as a training platform. However, reconstructing the colon from video footage remains unsolved due to numerous factors such as self-occlusion, reflective surfaces, lack of texture, and tissue deformation that limit feature-based methods. Learning-based approaches hold promise as robust alternatives, but necessitate extensive datasets. By establishing a benchmark, the 2022 EndoVis sub-challenge SimCol3D aimed to facilitate data-driven depth and pose prediction during colonoscopy. The challenge was hosted as part of MICCAI 2022 in Singapore. Six teams from around the world and representatives from academia and industry participated in the three sub-challenges: synthetic depth prediction, synthetic pose prediction, and real pose prediction. This paper describes the challenge, the submitted methods, and their results. We show that depth prediction in virtual colonoscopy is robustly solvable, while pose estimation remains an open research question.
</details>
<details>
<summary>摘要</summary>
抗rectal cancer是全球最常见的癌症之一。虽然colonoscopy是一种有效的检测技术，但是通过endooscope检测colon中的质量是具有挑战性的。一个3D地图可以增强未检查的colon组织识别和作为培训平台。然而，从视频足本中重建colon仍然是一个未解决的问题，因为自我遮挡、反射表面、缺乏Texture和组织变形等多种因素限制了基于特征的方法。学习基于方法具有潜在的优势，但它们需要大量的数据。为了实现这一目标，2022年的EndoVis子挑战SimCol3D在新加坡的MICCAI 2022会议上举行。六支来自全球的团队和学术界和产业界的代表参加了三个子挑战： sintetic depth prediction、syntetic pose prediction和real pose prediction。本文描述了这一挑战，提交的方法以及其结果。我们显示了虚拟colonoscopy中的深度预测是可靠地解决的，而pose预测仍然是一个开放的研究问题。
</details></li>
</ul>
<hr>
<h2 id="Towards-Non-Parametric-Models-for-Confidence-Aware-Image-Prediction-from-Low-Data-using-Gaussian-Processes"><a href="#Towards-Non-Parametric-Models-for-Confidence-Aware-Image-Prediction-from-Low-Data-using-Gaussian-Processes" class="headerlink" title="Towards Non-Parametric Models for Confidence Aware Image Prediction from Low Data using Gaussian Processes"></a>Towards Non-Parametric Models for Confidence Aware Image Prediction from Low Data using Gaussian Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11259">http://arxiv.org/abs/2307.11259</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikhil U. Shinde, Florian Richter, Michael C. Yip</li>
<li>for: 预测未来图像序列中的图像</li>
<li>methods: 使用非 Parametric 模型，采取 probabilistic 方法来预测图像</li>
<li>results: 成功预测 fluid  simulations 环境中的未来帧数据<details>
<summary>Abstract</summary>
The ability to envision future states is crucial to informed decision making while interacting with dynamic environments. With cameras providing a prevalent and information rich sensing modality, the problem of predicting future states from image sequences has garnered a lot of attention. Current state of the art methods typically train large parametric models for their predictions. Though often able to predict with accuracy, these models rely on the availability of large training datasets to converge to useful solutions. In this paper we focus on the problem of predicting future images of an image sequence from very little training data. To approach this problem, we use non-parametric models to take a probabilistic approach to image prediction. We generate probability distributions over sequentially predicted images and propagate uncertainty through time to generate a confidence metric for our predictions. Gaussian Processes are used for their data efficiency and ability to readily incorporate new training data online. We showcase our method by successfully predicting future frames of a smooth fluid simulation environment.
</details>
<details>
<summary>摘要</summary>
<<SYS>>预测未来状态的能力对于在动态环境中决策是非常重要的。由于摄像头是一种非常普遍和信息充沛的感知方式，预测未来状态从图像序列中得到的问题已经吸引了很多注意。当前的状态艺术方法通常是通过大型参数模型进行预测。虽然它们经常可以准确预测，但它们需要大量的训练数据来得到有用的解决方案。在这篇论文中，我们关注的是从非常少的训练数据中预测图像序列的未来帧。为了解决这个问题，我们使用非Parametric模型采取一种 probabilistic 的方法来预测图像。我们生成图像序列中预测的probability分布，并将uncertainty通过时间进行传播，以生成一个 confidence 度量器 для我们的预测。使用 Gaussian Processes 的数据效率和能够轻松地在线上添加新的训练数据，我们成功地预测了一个平滑的液体流体动画环境中的未来帧。
</details></li>
</ul>
<hr>
<h2 id="UP-DP-Unsupervised-Prompt-Learning-for-Data-Pre-Selection-with-Vision-Language-Models"><a href="#UP-DP-Unsupervised-Prompt-Learning-for-Data-Pre-Selection-with-Vision-Language-Models" class="headerlink" title="UP-DP: Unsupervised Prompt Learning for Data Pre-Selection with Vision-Language Models"></a>UP-DP: Unsupervised Prompt Learning for Data Pre-Selection with Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11227">http://arxiv.org/abs/2307.11227</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Li, Sima Behpour, Thang Doan, Wenbin He, Liang Gou, Liu Ren</li>
<li>for: 本研究旨在 optimizing performance for undefined downstream tasks with a limited annotation budget, through selecting instances for labeling from an unlabeled dataset through a single pass.</li>
<li>methods: 本研究使用的方法是基于 joint feature space of both vision and text, 通过不同的设计和训练策略，提高数据预选的性能。 Specifically, we introduce UP-DP, a simple yet effective unsupervised prompt learning approach that adapts vision-language models, like BLIP-2, for data pre-selection.</li>
<li>results: 与状态的艺术 compare our method with the state-of-the-art using seven benchmark datasets in different settings, achieving up to a performance gain of 20%. Additionally, the prompts learned from one dataset demonstrate significant generalizability and can be applied directly to enhance the feature extraction of BLIP-2 from other datasets.<details>
<summary>Abstract</summary>
In this study, we investigate the task of data pre-selection, which aims to select instances for labeling from an unlabeled dataset through a single pass, thereby optimizing performance for undefined downstream tasks with a limited annotation budget. Previous approaches to data pre-selection relied solely on visual features extracted from foundation models, such as CLIP and BLIP-2, but largely ignored the powerfulness of text features. In this work, we argue that, with proper design, the joint feature space of both vision and text can yield a better representation for data pre-selection. To this end, we introduce UP-DP, a simple yet effective unsupervised prompt learning approach that adapts vision-language models, like BLIP-2, for data pre-selection. Specifically, with the BLIP-2 parameters frozen, we train text prompts to extract the joint features with improved representation, ensuring a diverse cluster structure that covers the entire dataset. We extensively compare our method with the state-of-the-art using seven benchmark datasets in different settings, achieving up to a performance gain of 20%. Interestingly, the prompts learned from one dataset demonstrate significant generalizability and can be applied directly to enhance the feature extraction of BLIP-2 from other datasets. To the best of our knowledge, UP-DP is the first work to incorporate unsupervised prompt learning in a vision-language model for data pre-selection.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们调查了数据预选任务，该任务通过单次通过，选择未标注数据集中的实例，以优化未定下渠道任务的性能，并尽可能减少标注预算。先前的数据预选方法仅仅基于基础模型中的视觉特征，如CLIP和BLIP-2，而忽略文本特征的力量。在这项工作中，我们认为，如果设计得当，则联合视觉和文本特征的特征空间可以提供更好的数据预选表示。为此，我们提出了UP-DP方法，这是一种简单 yet有效的无监督提问学习方法，可以使用视觉语言模型，如BLIP-2，进行数据预选。具体来说，我们将BLIP-2参数冻结，然后使用文本提示来提取联合特征，以确保多样化的群集结构，覆盖整个数据集。我们在七个标准测试集上进行了广泛的比较，与状态艺术的方法进行比较，达到了最高的20%的性能提升。有趣的是，从一个数据集上学习的提示可以直接应用于提高BLIP-2在其他数据集上的特征提取。据我们所知，UP-DP是首次在视觉语言模型中 incorporate无监督提问学习来进行数据预选。
</details></li>
</ul>
<hr>
<h2 id="Heuristic-Hyperparameter-Choice-for-Image-Anomaly-Detection"><a href="#Heuristic-Hyperparameter-Choice-for-Image-Anomaly-Detection" class="headerlink" title="Heuristic Hyperparameter Choice for Image Anomaly Detection"></a>Heuristic Hyperparameter Choice for Image Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11197">http://arxiv.org/abs/2307.11197</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyu Jiang, João P. C. Bertoldo, Etienne Decencière</li>
<li>for: 这个研究旨在提出一种基于深度学习神经网络的图像异常检测方法，使用 pré-训练的模型提取的深度特征进行异常检测，并且采用约化特征分解方法来降低计算成本和提高性能。</li>
<li>methods: 该方法使用的是逆原理 Component Analysis（NPCA）约化特征分解方法，并且提出了一些优化策略来选择NPCA算法中的参数，以获得最少的特征组件数而仍保持良好的性能。</li>
<li>results: 研究结果表明，使用NPCA约化特征分解方法可以减少图像异常检测中的计算成本，同时保持良好的检测性能。此外，该方法还可以适应不同的图像数据集和异常检测任务。<details>
<summary>Abstract</summary>
Anomaly detection (AD) in images is a fundamental computer vision problem by deep learning neural network to identify images deviating significantly from normality. The deep features extracted from pretrained models have been proved to be essential for AD based on multivariate Gaussian distribution analysis. However, since models are usually pretrained on a large dataset for classification tasks such as ImageNet, they might produce lots of redundant features for AD, which increases computational cost and degrades the performance. We aim to do the dimension reduction of Negated Principal Component Analysis (NPCA) for these features. So we proposed some heuristic to choose hyperparameter of NPCA algorithm for getting as fewer components of features as possible while ensuring a good performance.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。</SYS>>图像异常检测（AD）是计算机视觉中的基本问题，使用深度学习神经网络来识别图像异常。深度特征从预训练模型中提取出来的特征被证明是AD基于多变量 Gaussian 分布分析中的 essencial。然而，由于模型通常在大量的分类任务上预训练，如 ImageNet，它们可能生成大量的冗余特征，这会增加计算成本并降低性能。我们想使用NPCA算法进行维度减少。因此，我们提出了一些规则来选择NPCA算法的超参数，以获得最少的特征组件而 guaranteeing good performance。
</details></li>
</ul>
<hr>
<h2 id="Representation-Learning-in-Anomaly-Detection-Successes-Limits-and-a-Grand-Challenge"><a href="#Representation-Learning-in-Anomaly-Detection-Successes-Limits-and-a-Grand-Challenge" class="headerlink" title="Representation Learning in Anomaly Detection: Successes, Limits and a Grand Challenge"></a>Representation Learning in Anomaly Detection: Successes, Limits and a Grand Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11085">http://arxiv.org/abs/2307.11085</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yedid Hoshen</li>
<li>for: 这篇论文主要写于异常检测领域， argue that 异常检测领域中的主流思想无法无限扩展，会遇到基础限制。</li>
<li>methods: 该论文使用了异常检测中的“无免费午餐”原理，并提出了两个grand challenges，一是科学发现通过异常检测，二是检测imageNet数据集中最异常的图像。</li>
<li>results: 该论文提出了新的异常检测工具和想法，以应对这两个挑战。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
In this perspective paper, we argue that the dominant paradigm in anomaly detection cannot scale indefinitely and will eventually hit fundamental limits. This is due to the a no free lunch principle for anomaly detection. These limitations can be overcome when there are strong tasks priors, as is the case for many industrial tasks. When such priors do not exists, the task is much harder for anomaly detection. We pose two such tasks as grand challenges for anomaly detection: i) scientific discovery by anomaly detection ii) a "mini-grand" challenge of detecting the most anomalous image in the ImageNet dataset. We believe new anomaly detection tools and ideas would need to be developed to overcome these challenges.
</details>
<details>
<summary>摘要</summary>
在这篇观点论文中，我们 argueThat the dominant paradigm in anomaly detection cannot scale indefinitely and will eventually hit fundamental limits. This is due to the a no free lunch principle for anomaly detection. These limitations can be overcome when there are strong tasks priors, as is the case for many industrial tasks. When such priors do not exists, the task is much harder for anomaly detection. We pose two such tasks as grand challenges for anomaly detection: i) scientific discovery by anomaly detection ii) a "mini-grand" challenge of detecting the most anomalous image in the ImageNet dataset. We believe new anomaly detection tools and ideas would need to be developed to overcome these challenges.Note: Please keep in mind that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Hong Kong, Taiwan, and other countries.
</details></li>
</ul>
<hr>
<h2 id="GLSFormer-Gated-Long-Short-Sequence-Transformer-for-Step-Recognition-in-Surgical-Videos"><a href="#GLSFormer-Gated-Long-Short-Sequence-Transformer-for-Step-Recognition-in-Surgical-Videos" class="headerlink" title="GLSFormer: Gated - Long, Short Sequence Transformer for Step Recognition in Surgical Videos"></a>GLSFormer: Gated - Long, Short Sequence Transformer for Step Recognition in Surgical Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11081">http://arxiv.org/abs/2307.11081</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nisargshah1999/glsformer">https://github.com/nisargshah1999/glsformer</a></li>
<li>paper_authors: Nisarg A. Shah, Shameema Sikder, S. Swaroop Vedula, Vishal M. Patel</li>
<li>for: automated surgical step recognition, 自动化手术步骤识别</li>
<li>methods: 使用视Transformer学习直接从批量帧级别的空间时间特征，并 integrate短期和长期空间时间特征表示</li>
<li>results: 在两个眼睛手术视频数据集（Cataract-101和D99）上进行了广泛评估，与多种现有方法进行比较，并达到了更高的性能。<details>
<summary>Abstract</summary>
Automated surgical step recognition is an important task that can significantly improve patient safety and decision-making during surgeries. Existing state-of-the-art methods for surgical step recognition either rely on separate, multi-stage modeling of spatial and temporal information or operate on short-range temporal resolution when learned jointly. However, the benefits of joint modeling of spatio-temporal features and long-range information are not taken in account. In this paper, we propose a vision transformer-based approach to jointly learn spatio-temporal features directly from sequence of frame-level patches. Our method incorporates a gated-temporal attention mechanism that intelligently combines short-term and long-term spatio-temporal feature representations. We extensively evaluate our approach on two cataract surgery video datasets, namely Cataract-101 and D99, and demonstrate superior performance compared to various state-of-the-art methods. These results validate the suitability of our proposed approach for automated surgical step recognition. Our code is released at: https://github.com/nisargshah1999/GLSFormer
</details>
<details>
<summary>摘要</summary>
自动化手术步骤识别是一项重要的任务，可以有效提高手术过程中的患者安全和决策。现有的状态级方法 для手术步骤识别可以是分离的多stage模型化的空间和时间信息，或者在学习时间resolution上进行简单的操作。然而，将空间和时间特征结合jointly learning的好处并没有得到足够的考虑。在本文中，我们提出了基于视觉 трансформер的方法，直接从序列帧级补剪图像中学习spatio-temporal特征。我们的方法包括一种阻止temporal注意力机制，可以智能地将短期和长期的spatio-temporal特征表示结合起来。我们对两个眼部手术视频数据集，即Cataract-101和D99进行了广泛的评估，并证明了与多种状态级方法的比较。这些结果证明了我们的提议的合适性。我们的代码可以在：https://github.com/nisargshah1999/GLSFormer 获取。
</details></li>
</ul>
<hr>
<h2 id="Learning-Dense-UV-Completion-for-Human-Mesh-Recovery"><a href="#Learning-Dense-UV-Completion-for-Human-Mesh-Recovery" class="headerlink" title="Learning Dense UV Completion for Human Mesh Recovery"></a>Learning Dense UV Completion for Human Mesh Recovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11074">http://arxiv.org/abs/2307.11074</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanjun Wang, Qingping Sun, Wenjia Wang, Jun Ling, Zhongang Cai, Rong Xie, Li Song</li>
<li>for: 解决单张图像中的人体重建问题，尤其是在受到自身、物体或其他人体 occlusion 的情况下。</li>
<li>methods: 提出了 Dense Inpainting Human Mesh Recovery (DIMR) 方法，利用稠密匹配图进行人体特征分离和完善。方法还包括一个基于注意力的特征填充模块，以便在 heavily occluded 图像下进行人体特征的填充和完善。</li>
<li>results: 对多个数据集进行了评测，并证明了 DIMR 方法在受到 heavily occluded 情况下的表现明显更好，而且在标准标准 bencmarks (3DPW) 上也达到了相似的结果。<details>
<summary>Abstract</summary>
Human mesh reconstruction from a single image is challenging in the presence of occlusion, which can be caused by self, objects, or other humans. Existing methods either fail to separate human features accurately or lack proper supervision for feature completion. In this paper, we propose Dense Inpainting Human Mesh Recovery (DIMR), a two-stage method that leverages dense correspondence maps to handle occlusion. Our method utilizes a dense correspondence map to separate visible human features and completes human features on a structured UV map dense human with an attention-based feature completion module. We also design a feature inpainting training procedure that guides the network to learn from unoccluded features. We evaluate our method on several datasets and demonstrate its superior performance under heavily occluded scenarios compared to other methods. Extensive experiments show that our method obviously outperforms prior SOTA methods on heavily occluded images and achieves comparable results on the standard benchmarks (3DPW).
</details>
<details>
<summary>摘要</summary>
人体三角形重建从单个图像中是具有挑战性的，尤其在 occlusion 存在时。现有方法可能不准确地分割人体特征或缺乏适当的监督来完成特征。在这篇论文中，我们提议 dense inpainting human mesh recovery（DIMR）方法，这是一种两个阶段的方法，利用 dense correspondence map 来处理 occlusion。我们的方法使用 dense correspondence map 来分割可见的人体特征，并在结构化 UV 图 dense human 上完成人体特征使用注意力基于的特征完成模块。我们还设计了一种特征填充训练过程，使网络学习从不受 occlusion 的特征。我们对多个数据集进行了评估，并证明了我们的方法在受到重度 occlusion 的场景下表现出色，比 Priors 的 SOTA 方法更好。广泛的实验表明，我们的方法在 heavily occluded 图像上表现出色，并在标准 benchmarks 上 achieve 相当的结果。
</details></li>
</ul>
<hr>
<h2 id="Towards-General-Game-Representations-Decomposing-Games-Pixels-into-Content-and-Style"><a href="#Towards-General-Game-Representations-Decomposing-Games-Pixels-into-Content-and-Style" class="headerlink" title="Towards General Game Representations: Decomposing Games Pixels into Content and Style"></a>Towards General Game Representations: Decomposing Games Pixels into Content and Style</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11141">http://arxiv.org/abs/2307.11141</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chintan Trivedi, Konstantinos Makantasis, Antonios Liapis, Georgios N. Yannakakis</li>
<li>for: 本研究旨在利用游戏视频中的丰富上下文信息，探索人工智能在多个下渠任务中的应用，包括游戏玩家模型、程序生成和游戏玩家Agent。</li>
<li>methods: 本研究使用了预训练的计算机视觉Encoder，并使用了基于游戏类型的分解技术，以获取独立的内容嵌入和风格嵌入。</li>
<li>results: 研究发现，通过分解内容和风格嵌入，可以在多个游戏环境中实现风格不变性，同时仍能保持强的内容提取能力。这些结果表明，提出的内容和风格分解方法可以更好地普适化在不同游戏环境中。<details>
<summary>Abstract</summary>
On-screen game footage contains rich contextual information that players process when playing and experiencing a game. Learning pixel representations of games can benefit artificial intelligence across several downstream tasks including game-playing agents, procedural content generation, and player modelling. The generalizability of these methods, however, remains a challenge, as learned representations should ideally be shared across games with similar game mechanics. This could allow, for instance, game-playing agents trained on one game to perform well in similar games with no re-training. This paper explores how generalizable pre-trained computer vision encoders can be for such tasks, by decomposing the latent space into content embeddings and style embeddings. The goal is to minimize the domain gap between games of the same genre when it comes to game content critical for downstream tasks, and ignore differences in graphical style. We employ a pre-trained Vision Transformer encoder and a decomposition technique based on game genres to obtain separate content and style embeddings. Our findings show that the decomposed embeddings achieve style invariance across multiple games while still maintaining strong content extraction capabilities. We argue that the proposed decomposition of content and style offers better generalization capacities across game environments independently of the downstream task.
</details>
<details>
<summary>摘要</summary>
电脑游戏截屏视频内容含有丰富的上下文信息，玩家在游戏时会处理这些信息。学习游戏像素表示可以提高人工智能在多个下游任务中的表现，如游戏玩家代理、过程内容生成和玩家模型。然而，这些方法的通用性仍然是一个挑战，因为学习的表示应该能够在同类游戏中共享。这可以让游戏玩家代理从一款游戏中转移到类似游戏中，无需重新训练。本文研究如何使用普通的计算机视觉encoder和游戏类别 decomposition技术来提取游戏内容和风格的嵌入。我们的发现表明，这些分解的嵌入可以在多个游戏中保持风格不变，同时仍然保留强大的内容提取能力。我们认为，我们的内容和风格分解方法可以在不同的游戏环境下独立地提高总化能力。
</details></li>
</ul>
<hr>
<h2 id="CNOS-A-Strong-Baseline-for-CAD-based-Novel-Object-Segmentation"><a href="#CNOS-A-Strong-Baseline-for-CAD-based-Novel-Object-Segmentation" class="headerlink" title="CNOS: A Strong Baseline for CAD-based Novel Object Segmentation"></a>CNOS: A Strong Baseline for CAD-based Novel Object Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11067">http://arxiv.org/abs/2307.11067</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nv-nguyen/cnos">https://github.com/nv-nguyen/cnos</a></li>
<li>paper_authors: Van Nguyen Nguyen, Tomas Hodan, Georgy Ponimatkin, Thibault Groueix, Vincent Lepetit</li>
<li>for: 针对RGB图像中未见对象的分割，使用CAD模型创建描述符和提议，并使用对比描述符与参考描述符进行匹配，以实现精度的对象ID分配和模式面。</li>
<li>methods: 采用三个阶段方法，首先使用最新的基础模型DINOv2和Segment Anything创建描述符和提议，然后使用对比描述符与参考描述符进行匹配，最后通过模式面进行分割。</li>
<li>results: 对七个核心数据集进行实验，比较与现有方法，得到了19.8% AP的最佳result，超越现有方法。<details>
<summary>Abstract</summary>
We propose a simple three-stage approach to segment unseen objects in RGB images using their CAD models. Leveraging recent powerful foundation models, DINOv2 and Segment Anything, we create descriptors and generate proposals, including binary masks for a given input RGB image. By matching proposals with reference descriptors created from CAD models, we achieve precise object ID assignment along with modal masks. We experimentally demonstrate that our method achieves state-of-the-art results in CAD-based novel object segmentation, surpassing existing approaches on the seven core datasets of the BOP challenge by 19.8% AP using the same BOP evaluation protocol. Our source code is available at https://github.com/nv-nguyen/cnos.
</details>
<details>
<summary>摘要</summary>
我们提出了一种简单的三个阶段方法，用于使用CAD模型来分割RGB图像中的未见对象。利用最新的强大基础模型，DINOv2和Segment Anything，我们创建了描述符和生成提案，包括输入RGB图像的二进制掩码。通过与参考描述符，创建从CAD模型中获得的对象ID分配和modal掩码。我们经验表明，我们的方法可以在BOP挑战的七个核心数据集上达到状态理论的Result，比既有方法在同一BOP评估协议下提高19.8%的AP。我们的源代码可以在https://github.com/nv-nguyen/cnos上获取。
</details></li>
</ul>
<hr>
<h2 id="HRFNet-High-Resolution-Forgery-Network-for-Localizing-Satellite-Image-Manipulation"><a href="#HRFNet-High-Resolution-Forgery-Network-for-Localizing-Satellite-Image-Manipulation" class="headerlink" title="HRFNet: High-Resolution Forgery Network for Localizing Satellite Image Manipulation"></a>HRFNet: High-Resolution Forgery Network for Localizing Satellite Image Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11052">http://arxiv.org/abs/2307.11052</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fahim Faisal Niloy, Kishor Kumar Bhaumik, Simon S. Woo</li>
<li>for: 本研究旨在提出一种高分辨率卫星图像伪造地址化方法，以解决现有高分辨率卫星图像伪造地址化方法存在较多缺陷的问题。</li>
<li>methods: 本研究提议一种名为HRFNet的新型模型，具有 shallow 和 deep 两个分支，可以很好地结合 RGB 和扩展特征在全球和本地层次上进行卫星图像伪造地址化。</li>
<li>results: 通过多种实验表明，OUR方法可以准确地 lokalize 卫星图像伪造地址，而且不会与现有方法相比增加内存需求和处理速度的需求。<details>
<summary>Abstract</summary>
Existing high-resolution satellite image forgery localization methods rely on patch-based or downsampling-based training. Both of these training methods have major drawbacks, such as inaccurate boundaries between pristine and forged regions, the generation of unwanted artifacts, etc. To tackle the aforementioned challenges, inspired by the high-resolution image segmentation literature, we propose a novel model called HRFNet to enable satellite image forgery localization effectively. Specifically, equipped with shallow and deep branches, our model can successfully integrate RGB and resampling features in both global and local manners to localize forgery more accurately. We perform various experiments to demonstrate that our method achieves the best performance, while the memory requirement and processing speed are not compromised compared to existing methods.
</details>
<details>
<summary>摘要</summary>
当前高分辨率卫星图像假造地点 localization 方法通常基于 patch-based 或 downsampling-based 训练。两者均有严重缺陷，如假造区域与原始区域的界限不准确、生成不必要的 artifacts 等。为了解决上述挑战，我们引用高分辨率图像分割文献，提出了一种新的模型called HRFNet，用于有效地进行卫星图像假造地点 localization。具体来说，我们的模型具有 shallow 和 deep 分支，可以成功地在全球和本地方面 интегра RGB 和抽样特征，以更加准确地Localize 假造。我们进行了多种实验，证明了我们的方法可以具有最高性能，而占用内存和处理速度与现有方法相比，不会受到影响。
</details></li>
</ul>
<hr>
<h2 id="Multi-objective-point-cloud-autoencoders-for-explainable-myocardial-infarction-prediction"><a href="#Multi-objective-point-cloud-autoencoders-for-explainable-myocardial-infarction-prediction" class="headerlink" title="Multi-objective point cloud autoencoders for explainable myocardial infarction prediction"></a>Multi-objective point cloud autoencoders for explainable myocardial infarction prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11017">http://arxiv.org/abs/2307.11017</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcel Beetz, Abhirup Banerjee, Vicente Grau</li>
<li>for: 预测心肌梗死（Myocardial Infarction，MI）的病理学基础。</li>
<li>methods: 使用多目标点云自动编码器（Multi-objective Point Cloud Autoencoder），一种基于多类3D点云表示的心肌生物学特征和功能的几何深度学习方法，可以有效地学习心肌3D形态特征，并提供可解释的MI预测结果。</li>
<li>results: 在一个大型UK Biobank数据集上，使用多目标点云自动编码器可以准确地重建多个时间点的3D形态，并且与输入形态的 Chamfer 距离低于图像像素分辨率。此外，该方法在incident MI预测任务上比多种机器学习和深度学习标准准降19%，并且可以清晰地分解控制和MI群集，并与相应的3D形态之间存在临床可能的关系，这种可解释性能够展示预测的可信度。<details>
<summary>Abstract</summary>
Myocardial infarction (MI) is one of the most common causes of death in the world. Image-based biomarkers commonly used in the clinic, such as ejection fraction, fail to capture more complex patterns in the heart's 3D anatomy and thus limit diagnostic accuracy. In this work, we present the multi-objective point cloud autoencoder as a novel geometric deep learning approach for explainable infarction prediction, based on multi-class 3D point cloud representations of cardiac anatomy and function. Its architecture consists of multiple task-specific branches connected by a low-dimensional latent space to allow for effective multi-objective learning of both reconstruction and MI prediction, while capturing pathology-specific 3D shape information in an interpretable latent space. Furthermore, its hierarchical branch design with point cloud-based deep learning operations enables efficient multi-scale feature learning directly on high-resolution anatomy point clouds. In our experiments on a large UK Biobank dataset, the multi-objective point cloud autoencoder is able to accurately reconstruct multi-temporal 3D shapes with Chamfer distances between predicted and input anatomies below the underlying images' pixel resolution. Our method outperforms multiple machine learning and deep learning benchmarks for the task of incident MI prediction by 19% in terms of Area Under the Receiver Operating Characteristic curve. In addition, its task-specific compact latent space exhibits easily separable control and MI clusters with clinically plausible associations between subject encodings and corresponding 3D shapes, thus demonstrating the explainability of the prediction.
</details>
<details>
<summary>摘要</summary>
myocardial infarction (MI) 是世界上最常见的死亡原因之一。传统的图像基于标记器，如舒张率，无法捕捉心脏三维解剖结构中更复杂的模式，因此限制了诊断精度。在这项工作中，我们提出了基于多对象点云自适应神经网络的新的几何深度学方法，用于可见的损害预测，基于多类三维点云表示的律动器解剖结构和功能。其架构包括多个任务特定分支，通过低维度的干扰空间相连，以实现有效的多目标学习 both 重建和MI预测，同时捕捉疾病特定的三维形态信息。此外，其层次分支设计和点云深度运算使得高分辨率的解剖点云上可以有效地进行多级别特征学习。在我们对大型UK Biobank数据集进行实验时，多对象点云自适应神经网络能够准确地重建多个时间点的三维形态，Chamfer距离输入和预测的形态之间的距离小于图像的像素分辨率。我们的方法在多种机器学习和深度学习标准准点上出现19%的提升，在接收操作特征曲线图下的面积来评估预测效果。此外，任务特定的紧凑的干托空间显示可以分别控制和MI层次分解，并且与相应的三维形态之间存在严格的相关关系，这表明预测是可解释的。
</details></li>
</ul>
<hr>
<h2 id="General-Image-to-Image-Translation-with-One-Shot-Image-Guidance"><a href="#General-Image-to-Image-Translation-with-One-Shot-Image-Guidance" class="headerlink" title="General Image-to-Image Translation with One-Shot Image Guidance"></a>General Image-to-Image Translation with One-Shot Image Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14352">http://arxiv.org/abs/2307.14352</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/crystalneuro/visual-concept-translator">https://github.com/crystalneuro/visual-concept-translator</a></li>
<li>paper_authors: Bin Cheng, Zuhao Liu, Yunbo Peng, Yue Lin</li>
<li>for: 可以将愿景图像中的视觉概念翻译成另一种图像，保持源图像的内容。</li>
<li>methods: 提出了一种名为视觉概念翻译器（VCT）的新框架，通过内容概念分离和内容概念融合两个过程来实现图像翻译。</li>
<li>results: 经验证明，提出的方法可以在各种普遍的图像翻译任务中达到优秀的结果，并且可以保持源图像的内容。<details>
<summary>Abstract</summary>
Large-scale text-to-image models pre-trained on massive text-image pairs show excellent performance in image synthesis recently. However, image can provide more intuitive visual concepts than plain text. People may ask: how can we integrate the desired visual concept into an existing image, such as our portrait? Current methods are inadequate in meeting this demand as they lack the ability to preserve content or translate visual concepts effectively. Inspired by this, we propose a novel framework named visual concept translator (VCT) with the ability to preserve content in the source image and translate the visual concepts guided by a single reference image. The proposed VCT contains a content-concept inversion (CCI) process to extract contents and concepts, and a content-concept fusion (CCF) process to gather the extracted information to obtain the target image. Given only one reference image, the proposed VCT can complete a wide range of general image-to-image translation tasks with excellent results. Extensive experiments are conducted to prove the superiority and effectiveness of the proposed methods. Codes are available at https://github.com/CrystalNeuro/visual-concept-translator.
</details>
<details>
<summary>摘要</summary>
大规模的文本到图像模型在最近的图像生成中表现出色，但图像可以提供更直观的视觉概念 than plain text。人们可能会问：如何将我们的肖像中的愿望 visual concept 集成到现有的图像中？现有的方法无法满足这个需求，因为它们缺乏保持内容或翻译视觉概念的能力。 draw inspiration from this，我们提出了一种名为视觉概念翻译器（VCT）的新框架，具有保持内容的源图像和根据单个参考图像 guid 视觉概念的翻译能力。VCT 包括一个内容概念反转（CCI）过程，用于提取内容和概念，以及一个内容概念聚合（CCF）过程，用于将提取的信息聚合以获得目标图像。只需要一个参考图像，提出的 VCT 可以完成广泛的普通图像到图像翻译任务，效果极佳。我们进行了广泛的实验，以证明我们的方法的超越性和有效性。代码可以在 https://github.com/CrystalNeuro/visual-concept-translator 上获取。
</details></li>
</ul>
<hr>
<h2 id="Frequency-aware-optical-coherence-tomography-image-super-resolution-via-conditional-generative-adversarial-neural-network"><a href="#Frequency-aware-optical-coherence-tomography-image-super-resolution-via-conditional-generative-adversarial-neural-network" class="headerlink" title="Frequency-aware optical coherence tomography image super-resolution via conditional generative adversarial neural network"></a>Frequency-aware optical coherence tomography image super-resolution via conditional generative adversarial neural network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11130">http://arxiv.org/abs/2307.11130</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueshen Li, Zhenxing Dong, Hongshan Liu, Jennifer J. Kang-Mieler, Yuye Ling, Yu Gan</li>
<li>for: 提高医学影像诊断和治疗的能力，特别是Cardiology和Ophthalmology领域。</li>
<li>methods: 使用深度学习基于super-resolution技术，以提高图像的分辨率和结构的保存。</li>
<li>results: 提出一种 integrate three critical frequency-based modules and frequency-based loss function into a conditional generative adversarial network (cGAN)的频率意识super-resolution框架，并在大规模的 coronary OCT 数据集中进行了评估，并在鱼眼和小鼠视网膜图像中进行了应用，以证明其在眼科影像中的抽象能力。<details>
<summary>Abstract</summary>
Optical coherence tomography (OCT) has stimulated a wide range of medical image-based diagnosis and treatment in fields such as cardiology and ophthalmology. Such applications can be further facilitated by deep learning-based super-resolution technology, which improves the capability of resolving morphological structures. However, existing deep learning-based method only focuses on spatial distribution and disregard frequency fidelity in image reconstruction, leading to a frequency bias. To overcome this limitation, we propose a frequency-aware super-resolution framework that integrates three critical frequency-based modules (i.e., frequency transformation, frequency skip connection, and frequency alignment) and frequency-based loss function into a conditional generative adversarial network (cGAN). We conducted a large-scale quantitative study from an existing coronary OCT dataset to demonstrate the superiority of our proposed framework over existing deep learning frameworks. In addition, we confirmed the generalizability of our framework by applying it to fish corneal images and rat retinal images, demonstrating its capability to super-resolve morphological details in eye imaging.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Deep-Spiking-UNet-for-Image-Processing"><a href="#Deep-Spiking-UNet-for-Image-Processing" class="headerlink" title="Deep Spiking-UNet for Image Processing"></a>Deep Spiking-UNet for Image Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10974">http://arxiv.org/abs/2307.10974</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/snnresearch/spiking-unet">https://github.com/snnresearch/spiking-unet</a></li>
<li>paper_authors: Hebei Li, Yueyi Zhang, Zhiwei Xiong, Zheng-jun Zha, Xiaoyan Sun</li>
<li>for: 这篇论文的目的是探讨使用快速神经网络（SNN）在图像处理任务中的应用，并将U-Net架构与SNN结合。</li>
<li>methods: 作者使用了多reshold飞神 ней元来提高信息传递的效率，并采用了一种转换和精度调整的管道来优化转换后的模型。</li>
<li>results: 实验结果表明，作者的快速神经网络在图像分割和减噪任务中达到了与非快速神经网络相当的性能，并超越了现有的SNN方法。 Comparing with the converted Spiking-UNet without fine-tuning, the inference time is reduced by approximately 90%.<details>
<summary>Abstract</summary>
U-Net, known for its simple yet efficient architecture, is widely utilized for image processing tasks and is particularly suitable for deployment on neuromorphic chips. This paper introduces the novel concept of Spiking-UNet for image processing, which combines the power of Spiking Neural Networks (SNNs) with the U-Net architecture. To achieve an efficient Spiking-UNet, we face two primary challenges: ensuring high-fidelity information propagation through the network via spikes and formulating an effective training strategy. To address the issue of information loss, we introduce multi-threshold spiking neurons, which improve the efficiency of information transmission within the Spiking-UNet. For the training strategy, we adopt a conversion and fine-tuning pipeline that leverage pre-trained U-Net models. During the conversion process, significant variability in data distribution across different parts is observed when utilizing skip connections. Therefore, we propose a connection-wise normalization method to prevent inaccurate firing rates. Furthermore, we adopt a flow-based training method to fine-tune the converted models, reducing time steps while preserving performance. Experimental results show that, on image segmentation and denoising, our Spiking-UNet achieves comparable performance to its non-spiking counterpart, surpassing existing SNN methods. Compared with the converted Spiking-UNet without fine-tuning, our Spiking-UNet reduces inference time by approximately 90\%. This research broadens the application scope of SNNs in image processing and is expected to inspire further exploration in the field of neuromorphic engineering. The code for our Spiking-UNet implementation is available at https://github.com/SNNresearch/Spiking-UNet.
</details>
<details>
<summary>摘要</summary>
U-Net，知名的简单 yet efficient 架构，广泛应用于图像处理任务，特别适合运行在神经模拟器芯片上。这篇论文介绍了一种新的激发式 U-Net 图像处理方法，该方法结合激发式神经网络（SNNs）与 U-Net 架构。为了实现高效的激发式 U-Net，我们面临两个主要挑战：保证通过网络传递高精度信息via spikes，并制定有效的训练策略。为了解决信息损失问题，我们引入多reshold spiking neurons，以提高激发式 U-Net 中信息传输的效率。在训练策略方面，我们采用一个转化和细化训练管道，利用预训练 U-Net 模型。在转化过程中，我们发现了不同部分数据分布变化带来的显著差异。因此，我们提出了一种连接 wise normalization 方法，以避免假象率。此外，我们采用一种流式训练方法，以细化转化后的模型，降低时间步骤而保持性能。实验结果表明，在图像分割和净化任务上，我们的激发式 U-Net 与非激发式 counterpart 的性能相似，超过现有的 SNN 方法。相比转化后的激发式 U-Net  без细化，我们的激发式 U-Net 可以降低推理时间约90%。这项研究扩展了 SNN 在图像处理领域的应用范围，预计会激发更多的神经模拟器工程研究。Spiking-UNet 实现代码可在 GitHub 上找到：https://github.com/SNNresearch/Spiking-UNet.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/21/cs.CV_2023_07_21/" data-id="clp89docc00gji7886pz8gcu8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_07_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/21/cs.AI_2023_07_21/" class="article-date">
  <time datetime="2023-07-21T12:00:00.000Z" itemprop="datePublished">2023-07-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/21/cs.AI_2023_07_21/">cs.AI - 2023-07-21</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Enhancing-CLIP-with-GPT-4-Harnessing-Visual-Descriptions-as-Prompts"><a href="#Enhancing-CLIP-with-GPT-4-Harnessing-Visual-Descriptions-as-Prompts" class="headerlink" title="Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts"></a>Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11661">http://arxiv.org/abs/2307.11661</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mayug/vdt-adapter">https://github.com/mayug/vdt-adapter</a></li>
<li>paper_authors: Mayug Maniparambil, Chris Vorster, Derek Molloy, Noel Murphy, Kevin McGuinness, Noel E. O’Connor<br>for:This paper focuses on improving the performance of CLIP, a contrastive pre-trained large Vision-Language Model (VLM), on downstream datasets by using GPT-4 to generate visually descriptive text prompts.methods:The authors use GPT-4 to generate text prompts that are relevant to the downstream dataset, and then use these prompts to adapt CLIP to the dataset with zero-shot learning. They also design a simple few-shot adapter that learns to choose the best possible sentences to construct generalizable classifiers.results:The authors show that their method achieves considerable improvements in 0-shot transfer accuracy on specialized fine-grained datasets, outperforming CLIP’s default prompt by around 7% on average. They also demonstrate that their simple few-shot adapter outperforms the recently proposed CoCoOP by around 2% on average and by over 4% on 4 specialized fine-grained datasets.Here is the simplified Chinese version of the three key points:for:这篇论文目标是提高CLIP的下游数据集表现，使用GPT-4生成相关的视觉描述文本提示。methods:作者使用GPT-4生成相关的视觉描述文本提示，然后使用这些提示将CLIP适应到数据集中进行零批学习。他们还设计了一个简单的几批适应器，可以选择最佳的句子来构建通用的分类器。results:作者表明，他们的方法在特殊的细化数据集上实现了considerable的0批传输准确率提升，比CLIP的默认提示高约7%。他们还证明了他们的简单的几批适应器可以超过最近提出的CoCoOP，在4个特殊的细化数据集上平均提高了2%，并在这些数据集上提高了4%。<details>
<summary>Abstract</summary>
Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have revolutionized visual representation learning by providing good performance on downstream datasets. VLMs are 0-shot adapted to a downstream dataset by designing prompts that are relevant to the dataset. Such prompt engineering makes use of domain expertise and a validation dataset. Meanwhile, recent developments in generative pretrained models like GPT-4 mean they can be used as advanced internet search tools. They can also be manipulated to provide visual information in any structure. In this work, we show that GPT-4 can be used to generate text that is visually descriptive and how this can be used to adapt CLIP to downstream tasks. We show considerable improvements in 0-shot transfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD (~7%), SUN397 (~4.6%), and CUB (~3.3%) when compared to CLIP's default prompt. We also design a simple few-shot adapter that learns to choose the best possible sentences to construct generalizable classifiers that outperform the recently proposed CoCoOP by ~2% on average and by over 4% on 4 specialized fine-grained datasets. The code, prompts, and auxiliary text dataset is available at https://github.com/mayug/VDT-Adapter.
</details>
<details>
<summary>摘要</summary>
带有对比学习的大视力语言模型（VLM）如CLIP，已经革命化视觉表示学习。VLM可以通过设计相关的提示来适应下游数据集，而这种提示工程充分利用了领域专业知识和验证数据集。此外，最近的生成预训练模型如GPT-4，可以用作高级网络搜索工具，同时可以通过提供任意结构的视觉信息来操作。在这项工作中，我们展示了GPT-4可以生成视觉描述性文本，并使其用于CLIP的适应下游任务。我们发现，与CLIP的默认提示相比，在特殊化细腻数据集（EuroSAT、DTD、SUN397和CUB）上显示了 considerable improvement（大约7%）。此外，我们还设计了一个简单的几拍适配器，可以选择最佳的句子来构建通用的分类器，超过了最近提出的CoCoOP的平均提升率（大约2%），并在特殊化细腻数据集上提升了4%。代码、提示和辅助文本数据集可以在https://github.com/mayug/VDT-Adapter中下载。
</details></li>
</ul>
<hr>
<h2 id="Bandits-with-Deterministically-Evolving-States"><a href="#Bandits-with-Deterministically-Evolving-States" class="headerlink" title="Bandits with Deterministically Evolving States"></a>Bandits with Deterministically Evolving States</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11655">http://arxiv.org/abs/2307.11655</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khashayar Khosravi, Renato Paes Leme, Chara Podimata, Apostolis Tsorvantzis</li>
<li>for: 这个论文是为了学习带带资料的投机问题，并考虑到状态不可见和演化的问题。</li>
<li>methods: 该论文提出了一种名为带带投机问题的模型，其中状态会 deterministically evolve，并且不可见。</li>
<li>results: 论文分析了这种模型下的在线学习算法，并证明了这些算法可以在任何可能的参数化下进行 оптими化。Specifically, the regret rates obtained are: for $\lambda \in [0, 1&#x2F;T^2]$: $\widetilde O(\sqrt{KT})$; for $\lambda &#x3D; T^{-a&#x2F;b}$ with $b &lt; a &lt; 2b$: $\widetilde O (T^{b&#x2F;a})$; for $\lambda \in (1&#x2F;T, 1 - 1&#x2F;\sqrt{T}): \widetilde O (K^{1&#x2F;3}T^{2&#x2F;3})$; and for $\lambda \in [1 - 1&#x2F;\sqrt{T}, 1]: \widetilde O (K\sqrt{T})$.<details>
<summary>Abstract</summary>
We propose a model for learning with bandit feedback while accounting for deterministically evolving and unobservable states that we call Bandits with Deterministically Evolving States. The workhorse applications of our model are learning for recommendation systems and learning for online ads. In both cases, the reward that the algorithm obtains at each round is a function of the short-term reward of the action chosen and how ``healthy'' the system is (i.e., as measured by its state). For example, in recommendation systems, the reward that the platform obtains from a user's engagement with a particular type of content depends not only on the inherent features of the specific content, but also on how the user's preferences have evolved as a result of interacting with other types of content on the platform. Our general model accounts for the different rate $\lambda \in [0,1]$ at which the state evolves (e.g., how fast a user's preferences shift as a result of previous content consumption) and encompasses standard multi-armed bandits as a special case. The goal of the algorithm is to minimize a notion of regret against the best-fixed sequence of arms pulled. We analyze online learning algorithms for any possible parametrization of the evolution rate $\lambda$. Specifically, the regret rates obtained are: for $\lambda \in [0, 1/T^2]$: $\widetilde O(\sqrt{KT})$; for $\lambda = T^{-a/b}$ with $b < a < 2b$: $\widetilde O (T^{b/a})$; for $\lambda \in (1/T, 1 - 1/\sqrt{T}): \widetilde O (K^{1/3}T^{2/3})$; and for $\lambda \in [1 - 1/\sqrt{T}, 1]: \widetilde O (K\sqrt{T})$.
</details>
<details>
<summary>摘要</summary>
我们提出一个模型，称为带征 Deterministic Evolving States 的 Bandit Learning 模型。这种模型的应用包括推荐系统和在线广告学习。在这些应用中，算法在每个轮次获得的奖励是功能和系统状态（例如，用户的喜好）的函数。例如，在推荐系统中，用户与某种内容的互动奖励不仅受到内容本身的特点的影响，还受到用户在其他类型的内容上的互动的影响。我们的通用模型考虑了不同的演化速率 $\lambda \in [0,1]$，并包括标准多重武器的特例。算法的目标是对于任何可能的参数化 $\lambda$，最小化对最佳固定sequence of arms pulled的 regret。我们分析了在线学习算法，并得到了不同的 regret 率：* for $\lambda \in [0, 1/T^2]$: $\widetilde O(\sqrt{KT})$;* for $\lambda = T^{-a/b}$ with $b < a < 2b$: $\widetilde O (T^{b/a})$;* for $\lambda \in (1/T, 1 - 1/\sqrt{T}): \widetilde O (K^{1/3}T^{2/3})$;* for $\lambda \in [1 - 1/\sqrt{T}, 1]: \widetilde O (K\sqrt{T})$.
</details></li>
</ul>
<hr>
<h2 id="Alleviating-the-Long-Tail-Problem-in-Conversational-Recommender-Systems"><a href="#Alleviating-the-Long-Tail-Problem-in-Conversational-Recommender-Systems" class="headerlink" title="Alleviating the Long-Tail Problem in Conversational Recommender Systems"></a>Alleviating the Long-Tail Problem in Conversational Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11650">http://arxiv.org/abs/2307.11650</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhipeng Zhao, Kun Zhou, Xiaolei Wang, Wayne Xin Zhao, Fan Pan, Zhao Cao, Ji-Rong Wen</li>
<li>for: 提高 conversational recommender systems (CRS) 的效果，尤其是对长尾项的推荐。</li>
<li>methods: 提出了一种名为 LOT-CRS 的新框架，该框架通过均衡 CRS 数据集来提高长尾项的推荐性能。在我们的方法中，我们设计了两个预训练任务来提高对长尾项的对话理解，并采用了 RETRIEVAL-augmented 精度调教策略来进一步提高长尾项的推荐。</li>
<li>results: 在两个公共 CRS 数据集上进行了广泛的实验，证明了我们的方法的有效性和可扩展性，特别是对长尾项的推荐。<details>
<summary>Abstract</summary>
Conversational recommender systems (CRS) aim to provide the recommendation service via natural language conversations. To develop an effective CRS, high-quality CRS datasets are very crucial. However, existing CRS datasets suffer from the long-tail issue, \ie a large proportion of items are rarely (or even never) mentioned in the conversations, which are called long-tail items. As a result, the CRSs trained on these datasets tend to recommend frequent items, and the diversity of the recommended items would be largely reduced, making users easier to get bored.   To address this issue, this paper presents \textbf{LOT-CRS}, a novel framework that focuses on simulating and utilizing a balanced CRS dataset (\ie covering all the items evenly) for improving \textbf{LO}ng-\textbf{T}ail recommendation performance of CRSs. In our approach, we design two pre-training tasks to enhance the understanding of simulated conversation for long-tail items, and adopt retrieval-augmented fine-tuning with label smoothness strategy to further improve the recommendation of long-tail items. Extensive experiments on two public CRS datasets have demonstrated the effectiveness and extensibility of our approach, especially on long-tail recommendation.
</details>
<details>
<summary>摘要</summary>
很多受众推荐系统（CRS）寻求通过自然语言对话提供推荐服务。为开发有效的CRS，高质量CRS数据集非常重要。然而，现有的CRS数据集受到长尾问题的困扰，即大多数 item 在对话中被 rarely (或者是从未) 提及，这些 item 被称为长尾 item。这导致CRS 在这些数据集上训练后，倾向于推荐频繁 item，推荐的 Item 的多样性会受到很大的削弱，使用户更容易感到厌烦。为解决这个问题，本文提出了一种新的框架，即 LOT-CRS，它是一种集中 item 的 CRS 数据集，以提高长尾推荐性能。在我们的方法中，我们设计了两个预训练任务，以增强对 simulated conversation 的理解，并采用了提取扩展的 fine-tuning 策略和标签平滑策略，以进一步提高长尾推荐。我们在两个公共 CRS 数据集上进行了广泛的实验，并证明了我们的方法的有效性和可扩展性，特别是在长尾推荐方面。
</details></li>
</ul>
<hr>
<h2 id="Morphological-Image-Analysis-and-Feature-Extraction-for-Reasoning-with-AI-based-Defect-Detection-and-Classification-Models"><a href="#Morphological-Image-Analysis-and-Feature-Extraction-for-Reasoning-with-AI-based-Defect-Detection-and-Classification-Models" class="headerlink" title="Morphological Image Analysis and Feature Extraction for Reasoning with AI-based Defect Detection and Classification Models"></a>Morphological Image Analysis and Feature Extraction for Reasoning with AI-based Defect Detection and Classification Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11643">http://arxiv.org/abs/2307.11643</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiajun Zhang, Georgina Cosma, Sarah Bugby, Axel Finke, Jason Watkins</li>
<li>for: 该论文旨在提高工业应用中的AI模型表现，通过解释IE Mask R-CNN模型的预测结果。</li>
<li>methods: 该论文提出了AI-Reasoner，它从图像中提取杂志特征（DefChars），并使用决策树来理解DefChar值。然后，AI-Reasoner输出了视觉化和文本解释，以提供IE Mask R-CNN模型输出的理解。</li>
<li>results: 实验结果表明，AI-Reasoner能够有效地解释IE Mask R-CNN模型的预测结果。总的来说，该论文提供了一种解释AI模型表现的解决方案，有助于提高工业应用中的AI模型表现。<details>
<summary>Abstract</summary>
As the use of artificial intelligent (AI) models becomes more prevalent in industries such as engineering and manufacturing, it is essential that these models provide transparent reasoning behind their predictions. This paper proposes the AI-Reasoner, which extracts the morphological characteristics of defects (DefChars) from images and utilises decision trees to reason with the DefChar values. Thereafter, the AI-Reasoner exports visualisations (i.e. charts) and textual explanations to provide insights into outputs made by masked-based defect detection and classification models. It also provides effective mitigation strategies to enhance data pre-processing and overall model performance. The AI-Reasoner was tested on explaining the outputs of an IE Mask R-CNN model using a set of 366 images containing defects. The results demonstrated its effectiveness in explaining the IE Mask R-CNN model's predictions. Overall, the proposed AI-Reasoner provides a solution for improving the performance of AI models in industrial applications that require defect analysis.
</details>
<details>
<summary>摘要</summary>
随着人工智能（AI）模型在工程和生产中的应用变得更加普遍，这些模型的预测需要提供透明的思维过程。这篇论文提议了AI理解器（AI-Reasoner），它从图像中提取杂形特征（DefChars），并使用决策树来进行思维。然后，AI-Reasoner将生成视觉化（如图表）和文本解释，以提供对掩模隐藏基于漏斗检测和分类模型的输出的深入了解。它还提供有效的缓解策略，以提高数据预处理和整体模型性能。AI-Reasoner在对IE Mask R-CNN模型的输出进行解释中得到了证明。总之，提议的AI-Reasoner为工业应用中需要检测分析的AI模型带来了改进性。
</details></li>
</ul>
<hr>
<h2 id="The-Two-Faces-of-AI-in-Green-Mobile-Computing-A-Literature-Review"><a href="#The-Two-Faces-of-AI-in-Green-Mobile-Computing-A-Literature-Review" class="headerlink" title="The Two Faces of AI in Green Mobile Computing: A Literature Review"></a>The Two Faces of AI in Green Mobile Computing: A Literature Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04436">http://arxiv.org/abs/2308.04436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wander Siemers, June Sallou, Luís Cruz</li>
<li>for: 本研究は、过去10年间の人工智能在移动设备上的应用に関する文献のレビューであり、13个主要题をまとめて详细に说明しています。</li>
<li>methods: 本研究では、34篇の论文を分析し、人工智能在移动设备上的能源消耗についての研究が不足していることを発见しています。また、大多数の研究は、学术的な背景に基づいていることを示しています。</li>
<li>results: 本研究の结果は、过去10年间で人工智能在移动设备上の应用が増加していることを示しています。しかし、人工智能の能源消耗については、更に调查が必要です。また、大多数の研究が公开されていないことも発见しています。<details>
<summary>Abstract</summary>
Artificial intelligence is bringing ever new functionalities to the realm of mobile devices that are now considered essential (e.g., camera and voice assistants, recommender systems). Yet, operating artificial intelligence takes up a substantial amount of energy. However, artificial intelligence is also being used to enable more energy-efficient solutions for mobile systems. Hence, artificial intelligence has two faces in that regard, it is both a key enabler of desired (efficient) mobile functionalities and a major power draw on these devices, playing a part in both the solution and the problem. In this paper, we present a review of the literature of the past decade on the usage of artificial intelligence within the realm of green mobile computing. From the analysis of 34 papers, we highlight the emerging patterns and map the field into 13 main topics that are summarized in details.   Our results showcase that the field is slowly increasing in the past years, more specifically, since 2019. Regarding the double impact AI has on the mobile energy consumption, the energy consumption of AI-based mobile systems is under-studied in comparison to the usage of AI for energy-efficient mobile computing, and we argue for more exploratory studies in that direction. We observe that although most studies are framed as solution papers (94%), the large majority do not make those solutions publicly available to the community. Moreover, we also show that most contributions are purely academic (28 out of 34 papers) and that we need to promote the involvement of the mobile software industry in this field.
</details>
<details>
<summary>摘要</summary>
人工智能在移动设备领域带来了不断新的功能（例如相机和语音助手、推荐系统），现在被视为必备的功能。然而，运行人工智能需要很多能量。然而，人工智能也在使移动系统更加能效的解决方案中发挥作用。因此，人工智能在这个方面有两个面，它是必需的功能启用者和移动设备的主要能量消耗者。在这篇论文中，我们对过去十年的文献进行了回顾，并将场景映射到13个主题中。我们的结果显示，这个领域在过去几年中逐渐增长，特别是自2019年起。关于人工智能对移动设备能 consumption的双重影响，我们认为需要更多的探索性研究。我们发现，大多数研究是呈现为解决方案纸（94%），但大多数解决方案没有公开提供给社区。此外，我们还发现大多数贡献是学术性质（28 out of 34 papers），我们需要推动移动软件产业的参与。
</details></li>
</ul>
<hr>
<h2 id="Integration-of-Domain-Expert-Centric-Ontology-Design-into-the-CRISP-DM-for-Cyber-Physical-Production-Systems"><a href="#Integration-of-Domain-Expert-Centric-Ontology-Design-into-the-CRISP-DM-for-Cyber-Physical-Production-Systems" class="headerlink" title="Integration of Domain Expert-Centric Ontology Design into the CRISP-DM for Cyber-Physical Production Systems"></a>Integration of Domain Expert-Centric Ontology Design into the CRISP-DM for Cyber-Physical Production Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11637">http://arxiv.org/abs/2307.11637</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/htytewx/softcam">https://github.com/htytewx/softcam</a></li>
<li>paper_authors: Milapji Singh Gill, Tom Westermann, Marvin Schieseck, Alexander Fay</li>
<li>for: 本研究旨在提高数据驱动项目的效率和可靠性，通过在CRISP-DM中 интегра Ontology design for CPPSs。</li>
<li>methods: 本研究使用了domain-specific ontologies，以提高数据驱动项目中 CPPSs 的理解和准备过程。</li>
<li>results: 本研究实现了一种可靠的 anomaly detection 用例，帮助数据科学家更快地和更可靠地从 CPPSs 中获得有价值信息。<details>
<summary>Abstract</summary>
In the age of Industry 4.0 and Cyber-Physical Production Systems (CPPSs) vast amounts of potentially valuable data are being generated. Methods from Machine Learning (ML) and Data Mining (DM) have proven to be promising in extracting complex and hidden patterns from the data collected. The knowledge obtained can in turn be used to improve tasks like diagnostics or maintenance planning. However, such data-driven projects, usually performed with the Cross-Industry Standard Process for Data Mining (CRISP-DM), often fail due to the disproportionate amount of time needed for understanding and preparing the data. The application of domain-specific ontologies has demonstrated its advantageousness in a wide variety of Industry 4.0 application scenarios regarding the aforementioned challenges. However, workflows and artifacts from ontology design for CPPSs have not yet been systematically integrated into the CRISP-DM. Accordingly, this contribution intends to present an integrated approach so that data scientists are able to more quickly and reliably gain insights into the CPPS. The result is exemplarily applied to an anomaly detection use case.
</details>
<details>
<summary>摘要</summary>
在第四产业时代和跨industry标准数据挖掘过程（CPPS）中， vast amounts of potentially valuable data 被生成。机器学习（ML）和数据挖掘（DM）的方法已经证明可以提取复杂和隐藏的模式，从而提高诊断或维护规划等任务。然而，这些数据驱动项目通常使用 Cross-Industry Standard Process for Data Mining（CRISP-DM）进行实施，但它们往往因为数据理解和准备过程中的时间过长而失败。适用域pecific ontology 在多种第四产业应用场景中表现出了优势。然而， CPPSs 的 workflow 和 artifacts 尚未被系统地 инте integrate into CRISP-DM。因此，本贡献的目的是提出一种集成的approach，使数据科学家能够更快速地和更可靠地获得 CPPS 的 Insights。结果通过例示了一个异常检测用例来说明。
</details></li>
</ul>
<hr>
<h2 id="On-the-Complexity-of-the-Bipartite-Polarization-Problem-from-Neutral-to-Highly-Polarized-Discussions"><a href="#On-the-Complexity-of-the-Bipartite-Polarization-Problem-from-Neutral-to-Highly-Polarized-Discussions" class="headerlink" title="On the Complexity of the Bipartite Polarization Problem: from Neutral to Highly Polarized Discussions"></a>On the Complexity of the Bipartite Polarization Problem: from Neutral to Highly Polarized Discussions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11621">http://arxiv.org/abs/2307.11621</a></li>
<li>repo_url: None</li>
<li>paper_authors: Teresa Alsinet, Josep Argelich, Ramón Béjar, Santi Martínez</li>
<li>for: 本研究探讨了一个优化问题，即寻找一个极化分 partition 的最高极化矩阵，该问题表示一个社交网络上的辩论，节点表示用户的意见，边表示用户之间的一致或不一致。</li>
<li>methods: 本研究使用了一种实验抽象模型，通过控制实例的极化程度来调控实例的解决难度。</li>
<li>results: 研究发现，极化程度与实例的解决难度正相关，即极化程度越高，解决实例的难度就越低。<details>
<summary>Abstract</summary>
The Bipartite Polarization Problem is an optimization problem where the goal is to find the highest polarized bipartition on a weighted and labelled graph that represents a debate developed through some social network, where nodes represent user's opinions and edges agreement or disagreement between users. This problem can be seen as a generalization of the maxcut problem, and in previous work approximate solutions and exact solutions have been obtained for real instances obtained from Reddit discussions, showing that such real instances seem to be very easy to solve. In this paper, we investigate further the complexity of this problem, by introducing an instance generation model where a single parameter controls the polarization of the instances in such a way that this correlates with the average complexity to solve those instances. The average complexity results we obtain are consistent with our hypothesis: the higher the polarization of the instance, the easier is to find the corresponding polarized bipartition.
</details>
<details>
<summary>摘要</summary>
《双分化卷积问题》是一个优化问题，目标是在一个权重 Labelled 图上找到最高卷积分的生成方法，表示一个社交网络上的辩论，节点表示用户的意见，边表示用户之间的一致或不一致。这个问题可以看作是最大cut问题的推广，在过去的工作中，人们已经得到了实际例子中的近似解和精确解，显示这些实际例子很容易解决。在这篇论文中，我们进一步调查了这个问题的复杂性，通过设计一个参数控制实例的卷积程度，以确定实例的复杂性和解决实例的难度之间的关系。我们获得的平均复杂性结果与我们的假设一致：卷积程度越高，实例的解决难度就越低。
</details></li>
</ul>
<hr>
<h2 id="CausE-Towards-Causal-Knowledge-Graph-Embedding"><a href="#CausE-Towards-Causal-Knowledge-Graph-Embedding" class="headerlink" title="CausE: Towards Causal Knowledge Graph Embedding"></a>CausE: Towards Causal Knowledge Graph Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11610">http://arxiv.org/abs/2307.11610</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zjukg/cause">https://github.com/zjukg/cause</a></li>
<li>paper_authors: Yichi Zhang, Wen Zhang</li>
<li>for: 本研究旨在提高知识图(KG)的完整性(KGC)，通过 repre senting KG 中的实体和关系为连续向量空间中的 embedding。</li>
<li>methods: 本研究提出了一种基于 causality 和 embedding 分离的新型知识图嵌入(KGE)方法，并提出了一种新的训练目标来稳定地预测 missing triple。</li>
<li>results: 实验结果表明，CausE 可以超过基eline模型，并实现状态的� characteristic KGC performance。<details>
<summary>Abstract</summary>
Knowledge graph embedding (KGE) focuses on representing the entities and relations of a knowledge graph (KG) into the continuous vector spaces, which can be employed to predict the missing triples to achieve knowledge graph completion (KGC). However, KGE models often only briefly learn structural correlations of triple data and embeddings would be misled by the trivial patterns and noisy links in real-world KGs. To address this issue, we build the new paradigm of KGE in the context of causality and embedding disentanglement. We further propose a Causality-enhanced knowledge graph Embedding (CausE) framework. CausE employs causal intervention to estimate the causal effect of the confounder embeddings and design new training objectives to make stable predictions. Experimental results demonstrate that CausE could outperform the baseline models and achieve state-of-the-art KGC performance. We release our code in https://github.com/zjukg/CausE.
</details>
<details>
<summary>摘要</summary>
知识图embedding（KGE）专注于将知识图（KG）中的实体和关系转换到连续的vector空间中，以便预测缺失的 triple以完成知识图完成（KGC）。然而，KGE模型经常只是简单地学习 triple数据的结构相关性，而 embedding会受到实际世界KG中的噪声和负面相关性的影响。为解决这个问题，我们建立了一种基于 causality的新型KGE paradigma，并提出了一种causality-enhanced知识图Embedding（CausE）框架。CausE使用 causal intervention来估计干扰因子 embedding的 causal效果，并设计了新的训练目标来确保稳定的预测。实验结果表明，CausE可以超越基eline模型，并实现状态控制KGC性能。我们在https://github.com/zjukg/CausE中发布了我们的代码。
</details></li>
</ul>
<hr>
<h2 id="Predict-AI-bility-of-how-humans-balance-self-interest-with-the-interest-of-others"><a href="#Predict-AI-bility-of-how-humans-balance-self-interest-with-the-interest-of-others" class="headerlink" title="Predict-AI-bility of how humans balance self-interest with the interest of others"></a>Predict-AI-bility of how humans balance self-interest with the interest of others</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12776">http://arxiv.org/abs/2307.12776</a></li>
<li>repo_url: None</li>
<li>paper_authors: Valerio Capraro, Roberto Di Paolo, Veronica Pizziol</li>
<li>for: This paper aims to investigate the ability of three advanced chatbots to predict dictator game decisions and capture the balance between self-interest and the interest of others in decision-making.</li>
<li>methods: The paper uses 78 experiments with human participants from 12 countries to evaluate the performance of GPT-4, Bard, and Bing in predicting dictator game decisions and identifying qualitative behavioral patterns.</li>
<li>results: The paper finds that only GPT-4 correctly captures qualitative behavioral patterns, identifying three major classes of behavior, but consistently overestimates other-regarding behavior, inflating the proportion of inequity-averse and fully altruistic participants. This bias has significant implications for AI developers and users.<details>
<summary>Abstract</summary>
Generative artificial intelligence holds enormous potential to revolutionize decision-making processes, from everyday to high-stake scenarios. However, as many decisions carry social implications, for AI to be a reliable assistant for decision-making it is crucial that it is able to capture the balance between self-interest and the interest of others. We investigate the ability of three of the most advanced chatbots to predict dictator game decisions across 78 experiments with human participants from 12 countries. We find that only GPT-4 (not Bard nor Bing) correctly captures qualitative behavioral patterns, identifying three major classes of behavior: self-interested, inequity-averse, and fully altruistic. Nonetheless, GPT-4 consistently overestimates other-regarding behavior, inflating the proportion of inequity-averse and fully altruistic participants. This bias has significant implications for AI developers and users.
</details>
<details>
<summary>摘要</summary>
translate the given text into Simplified ChineseGenerated artificial intelligence has the potential to revolutionize decision-making processes, from everyday to high-stakes scenarios. However, as many decisions have social implications, for AI to be a reliable assistant for decision-making, it is crucial that it can capture the balance between self-interest and the interest of others. We investigate the ability of three of the most advanced chatbots to predict dictator game decisions across 78 experiments with human participants from 12 countries. We find that only GPT-4 (not Bard nor Bing) correctly captures qualitative behavioral patterns, identifying three major classes of behavior: self-interested, inequity-averse, and fully altruistic. However, GPT-4 consistently overestimates other-regarding behavior, inflating the proportion of inequity-averse and fully altruistic participants. This bias has significant implications for AI developers and users.Note: "Dictator game" is not a commonly used term in Simplified Chinese, so it may be better to use a more common term such as "分配游戏" (bǐng huò yóu xì) or "权力游戏" (quán lì yóu xì) to convey the same idea.
</details></li>
</ul>
<hr>
<h2 id="Feature-Map-Testing-for-Deep-Neural-Networks"><a href="#Feature-Map-Testing-for-Deep-Neural-Networks" class="headerlink" title="Feature Map Testing for Deep Neural Networks"></a>Feature Map Testing for Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11563">http://arxiv.org/abs/2307.11563</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ase2023paper/deepfeature">https://github.com/ase2023paper/deepfeature</a></li>
<li>paper_authors: Dong Huang, Qingwen Bu, Yahao Qing, Yichao Fu, Heming Cui</li>
<li>for: This paper is written to address the issue of deep learning testing, specifically the problem of detecting fault-inducing feature maps in deep neural networks (DNNs).</li>
<li>methods: The paper proposes a new method called DeepFeature, which tests DNNs from the feature map level and identifies vulnerabilities that can be enhanced through repairing to increase the model’s overall performance.</li>
<li>results: The paper presents experimental results that demonstrate the effectiveness of DeepFeature in detecting the model’s vulnerable feature maps, with a high fault detection rate and the ability to detect more types of faults compared to current techniques. Additionally, the paper shows that DeepFeature’s fuzzer outperforms current fuzzing techniques and generates valuable test cases more efficiently.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了解决深度学习测试问题，具体来说是检测深度神经网络（DNNs）中的特征图层级漏洞。</li>
<li>methods: 论文提出了一种新方法called DeepFeature，它测试DNNs从特征图层级，并通过修复特征图来增强模型的总性表现。</li>
<li>results: 论文发表了实验结果，证明DeepFeature可以快速检测DNNs中的特征图漏洞，并且能够检测更多的问题类型，比如当前技术。另外，论文还表明DeepFeature的随机生成器比现有的随机生成技术更高效。<details>
<summary>Abstract</summary>
Due to the widespread application of deep neural networks~(DNNs) in safety-critical tasks, deep learning testing has drawn increasing attention. During the testing process, test cases that have been fuzzed or selected using test metrics are fed into the model to find fault-inducing test units (e.g., neurons and feature maps, activating which will almost certainly result in a model error) and report them to the DNN developer, who subsequently repair them~(e.g., retraining the model with test cases). Current test metrics, however, are primarily concerned with the neurons, which means that test cases that are discovered either by guided fuzzing or selection with these metrics focus on detecting fault-inducing neurons while failing to detect fault-inducing feature maps.   In this work, we propose DeepFeature, which tests DNNs from the feature map level. When testing is conducted, DeepFeature will scrutinize every internal feature map in the model and identify vulnerabilities that can be enhanced through repairing to increase the model's overall performance. Exhaustive experiments are conducted to demonstrate that (1) DeepFeature is a strong tool for detecting the model's vulnerable feature maps; (2) DeepFeature's test case selection has a high fault detection rate and can detect more types of faults~(comparing DeepFeature to coverage-guided selection techniques, the fault detection rate is increased by 49.32\%). (3) DeepFeature's fuzzer also outperforms current fuzzing techniques and generates valuable test cases more efficiently.
</details>
<details>
<summary>摘要</summary>
由于深度神经网络（DNN）在安全关键任务中广泛应用，深度学习测试已引起了越来越多的关注。测试过程中，经过抽象或使用测试指标选择的测试用例会被 feed 到模型中，以找到引起模型错误的测试单元（例如神经元和特征图），并将其报告给 DNN 开发者，以便他们进行修复（例如重新训练模型使用测试用例）。现有的测试指标主要关注神经元，因此通过抽象或选择测试指标来发现的测试用例主要是检测引起模型错误的神经元，而忽略了特征图。在这项工作中，我们提出了 DeepFeature，它测试 DNN 从特征图层次。在测试过程中，DeepFeature 会仔细检查模型中每个内部特征图，并找到可以通过修复提高模型的性能的漏洞。我们进行了广泛的实验，证明了以下结论：1. DeepFeature 是一个强大的特征图漏洞检测工具，可以帮助检测模型的漏洞特征图。2. DeepFeature 的测试用例选择比coverage-guided选择技术高出49.32%的FAULT检测率。3. DeepFeature 的随机生成器也超越了当前的随机生成技术，可以更有效地生成有价值的测试用例。
</details></li>
</ul>
<hr>
<h2 id="CycleIK-Neuro-inspired-Inverse-Kinematics"><a href="#CycleIK-Neuro-inspired-Inverse-Kinematics" class="headerlink" title="CycleIK: Neuro-inspired Inverse Kinematics"></a>CycleIK: Neuro-inspired Inverse Kinematics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11554">http://arxiv.org/abs/2307.11554</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan-Gerrit Habekost, Erik Strahl, Philipp Allgeuer, Matthias Kerzel, Stefan Wermter</li>
<li>for: 这篇论文旨在介绍一种基于神经网络的 inverse kinematics (IK) 方法，即 CycleIK，以及一种混合神经遗传算法pipeline，可以在独立的方式下使用，也可以通过sequential least-squares programming (SLSQP) 或者生物遗传算法 (GA)进行优化。</li>
<li>methods: 该方法使用了两种新的神经网络驱动的方法，即 Generative Adversarial Network (GAN) 和 Multi-Layer Perceptron architecture，这些方法可以单独使用，也可以在混合神经遗传算法pipeline中使用。</li>
<li>results: 在使用Weighted Multi-Objective Function from state-of-the-art BioIK方法支持下，神经网络模型可以与现有的IK方法竞争，并且通过 incorporating a genetic algorithm 可以提高精度，同时减少总的运行时间。<details>
<summary>Abstract</summary>
The paper introduces CycleIK, a neuro-robotic approach that wraps two novel neuro-inspired methods for the inverse kinematics (IK) task, a Generative Adversarial Network (GAN), and a Multi-Layer Perceptron architecture. These methods can be used in a standalone fashion, but we also show how embedding these into a hybrid neuro-genetic IK pipeline allows for further optimization via sequential least-squares programming (SLSQP) or a genetic algorithm (GA). The models are trained and tested on dense datasets that were collected from random robot configurations of the new Neuro-Inspired COLlaborator (NICOL), a semi-humanoid robot with two redundant 8-DoF manipulators. We utilize the weighted multi-objective function from the state-of-the-art BioIK method to support the training process and our hybrid neuro-genetic architecture. We show that the neural models can compete with state-of-the-art IK approaches, which allows for deployment directly to robotic hardware. Additionally, it is shown that the incorporation of the genetic algorithm improves the precision while simultaneously reducing the overall runtime.
</details>
<details>
<summary>摘要</summary>
文章介绍了 CycleIK，一种神经机器人方法，包括两种新的神经网络做 inverse kinematics（IK）任务的方法，生成对抗网络（GAN）和多层感知网络架构。这些方法可以单独使用，但我们还证明了将它们集成到一个混合神经遗传IK管道中，可以通过顺序最小二乘程序（SLSQP）或遗传算法（GA）进行进一步优化。模型在 dense 数据集上训练和测试，数据集是通过随机机器人配置新的神经机器人 Neuro-Inspired COLlaborator（NICOL）的两个冗余的 8-DoF 机械臂收集而来。我们使用了 BioIK 方法中的Weighted 多目标函数来支持训练过程，并使用我们的混合神经遗传架构。我们显示了神经模型可以与当前IK方法竞争，可以直接部署到机器人硬件上。此外，我们还证明了将遗传算法包含在PIPELINE中可以提高精度，同时降低总时间。
</details></li>
</ul>
<hr>
<h2 id="Identifying-Relevant-Features-of-CSE-CIC-IDS2018-Dataset-for-the-Development-of-an-Intrusion-Detection-System"><a href="#Identifying-Relevant-Features-of-CSE-CIC-IDS2018-Dataset-for-the-Development-of-an-Intrusion-Detection-System" class="headerlink" title="Identifying Relevant Features of CSE-CIC-IDS2018 Dataset for the Development of an Intrusion Detection System"></a>Identifying Relevant Features of CSE-CIC-IDS2018 Dataset for the Development of an Intrusion Detection System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11544">http://arxiv.org/abs/2307.11544</a></li>
<li>repo_url: None</li>
<li>paper_authors: László Göcs, Zsolt Csaba Johanyák</li>
<li>for: 本研究旨在帮助开发一个高效的攻击检测系统（IDS），其中包括选择必要的特征来分类网络流量。</li>
<li>methods: 本研究使用了六种特征选择方法，并对每个攻击类型进行了不同特征选择。</li>
<li>results: 研究发现，采用不同的特征选择方法和分类算法可以得到不同的优化效果，并且可以为不同的攻击类型选择最佳的特征集。<details>
<summary>Abstract</summary>
Intrusion detection systems (IDSs) are essential elements of IT systems. Their key component is a classification module that continuously evaluates some features of the network traffic and identifies possible threats. Its efficiency is greatly affected by the right selection of the features to be monitored. Therefore, the identification of a minimal set of features that are necessary to safely distinguish malicious traffic from benign traffic is indispensable in the course of the development of an IDS. This paper presents the preprocessing and feature selection workflow as well as its results in the case of the CSE-CIC-IDS2018 on AWS dataset, focusing on five attack types. To identify the relevant features, six feature selection methods were applied, and the final ranking of the features was elaborated based on their average score. Next, several subsets of the features were formed based on different ranking threshold values, and each subset was tried with five classification algorithms to determine the optimal feature set for each attack type. During the evaluation, four widely used metrics were taken into consideration.
</details>
<details>
<summary>摘要</summary>
安全系统检测系统（IDS）是信息技术系统中不可或缺的元素。它的关键组件是分类模块，不断评估网络流量中的一些特征，并识别可能的威胁。因此，选择需要监测的特征是非常重要的，以确保安全地分辨恶意流量和良好流量。本文介绍了预处理和特征选择工作流程，以及在CSE-CIC-IDS2018 on AWS dataset上的实验结果，关注五种攻击类型。为了确定相关的特征，本文使用了六种特征选择方法，并根据每个特征的平均分数进行了最终排名。然后，根据不同的排名阈值，将特征分为多个子集，并对每个子集使用五种分类算法进行了评估。在评估过程中，考虑了四种常用的指标。
</details></li>
</ul>
<hr>
<h2 id="Model-Reporting-for-Certifiable-AI-A-Proposal-from-Merging-EU-Regulation-into-AI-Development"><a href="#Model-Reporting-for-Certifiable-AI-A-Proposal-from-Merging-EU-Regulation-into-AI-Development" class="headerlink" title="Model Reporting for Certifiable AI: A Proposal from Merging EU Regulation into AI Development"></a>Model Reporting for Certifiable AI: A Proposal from Merging EU Regulation into AI Development</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11525">http://arxiv.org/abs/2307.11525</a></li>
<li>repo_url: None</li>
<li>paper_authors: Danilo Brajovic, Niclas Renner, Vincent Philipp Goebels, Philipp Wagner, Benjamin Fresz, Martin Biller, Mara Klaeb, Janika Kutz, Jens Neuhuettler, Marco F. Huber</li>
<li>for: 这篇论文旨在提供标准化的卡片来描述 AI 应用程序的开发过程，以帮助实践者开发安全的 AI 系统，并且满足政策法规的要求。</li>
<li>methods: 该论文使用了最新的欧盟法规和 AI 指南，以及最新的研究趋势：数据和模型卡片。它提出了使用标准化的卡片来记录 AI 应用程序的开发过程，包括用例卡和运行卡，以满足政策法规的要求。</li>
<li>results: 该论文的主要贡献是提出了一套标准化的卡片，以帮助实践者开发安全的 AI 系统，并且可以方便第三方进行 AI 应用程序的审核。该论文还 incorporates 了专家访谈和开发者的意见，以及相关的研究和工具箱。<details>
<summary>Abstract</summary>
Despite large progress in Explainable and Safe AI, practitioners suffer from a lack of regulation and standards for AI safety. In this work we merge recent regulation efforts by the European Union and first proposals for AI guidelines with recent trends in research: data and model cards. We propose the use of standardized cards to document AI applications throughout the development process. Our main contribution is the introduction of use-case and operation cards, along with updates for data and model cards to cope with regulatory requirements. We reference both recent research as well as the source of the regulation in our cards and provide references to additional support material and toolboxes whenever possible. The goal is to design cards that help practitioners develop safe AI systems throughout the development process, while enabling efficient third-party auditing of AI applications, being easy to understand, and building trust in the system. Our work incorporates insights from interviews with certification experts as well as developers and individuals working with the developed AI applications.
</details>
<details>
<summary>摘要</summary>
尽管在可解释和安全人工智能方面有大量进步，但实践者受到了不足的规范和标准的压力。在这项工作中，我们将欧盟最新的规定努力和首个AI指南提议与最新的研究趋势相结合：数据和模型卡。我们提议在开发过程中使用标准化卡来记录AI应用程序。我们的主要贡献是引入使用情况和运行卡，并更新数据和模型卡以适应规定要求。我们参考了最新的研究以及规定的来源，并提供了参考资料和工具箱 whenever possible。我们的目标是通过开发安全人工智能系统的整个开发过程中使用卡来帮助实践者，并允许第三方审核人工智能应用程序，易于理解，建立信任系统。我们的工作还 incorporates 专家评论、开发人员和使用开发的AI应用程序的人员的意见。
</details></li>
</ul>
<hr>
<h2 id="IndigoVX-Where-Human-Intelligence-Meets-AI-for-Optimal-Decision-Making"><a href="#IndigoVX-Where-Human-Intelligence-Meets-AI-for-Optimal-Decision-Making" class="headerlink" title="IndigoVX: Where Human Intelligence Meets AI for Optimal Decision Making"></a>IndigoVX: Where Human Intelligence Meets AI for Optimal Decision Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11516">http://arxiv.org/abs/2307.11516</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kais Dukes</li>
<li>for: 本研究专攻了将人工智能与人类智慧结合，以实现最佳目标解决方案。</li>
<li>methods: 本研究提出了一种名为“Indigo”的新方法，它是一个辅助人类做出最佳决策的数据驱动AI系统。人类和AI共同合作，组成一个名为“IndigoVX”的虚拟专家系统，用于应对游戏或商业策略等领域。</li>
<li>results: 本研究显示，通过人类和AI的联合合作，可以实现更高效的目标解决。通过变量的三个分数评估指标，评估和改进策略，适应到实际挑战和变化。<details>
<summary>Abstract</summary>
This paper defines a new approach for augmenting human intelligence with AI for optimal goal solving. Our proposed AI, Indigo, is an acronym for Informed Numerical Decision-making through Iterative Goal-Oriented optimization. When combined with a human collaborator, we term the joint system IndigoVX, for Virtual eXpert. The system is conceptually simple. We envisage this method being applied to games or business strategies, with the human providing strategic context and the AI offering optimal, data-driven moves. Indigo operates through an iterative feedback loop, harnessing the human expert's contextual knowledge and the AI's data-driven insights to craft and refine strategies towards a well-defined goal. Using a quantified three-score schema, this hybridization allows the combined team to evaluate strategies and refine their plan, while adapting to challenges and changes in real-time.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种新的方法，用人工智能增强人类智能，以实现最佳目标解决。我们的提议的AI系统名为Indigo，全名为Informed Numerical Decision-making through Iterative Goal-Oriented optimization。当与人类合作者结合时，我们称之为IndigoVX，即虚拟专家。该系统的核心思想简单。我们认为这种方法可以应用于游戏或商业策略等领域，人类提供战略背景知识，AI提供数据驱动的优化 Move。Indigo通过迭代反馈循环，利用人类专家的Contextual knowledge和AI的数据驱动洞察，制定和细化策略，以达到已定义的目标。使用量化的三个分数schema，这个混合体系可以评估策略和修改计划，同时适应挑战和变化的实时反应。
</details></li>
</ul>
<hr>
<h2 id="Framework-for-developing-quantitative-agent-based-models-based-on-qualitative-expert-knowledge-an-organised-crime-use-case"><a href="#Framework-for-developing-quantitative-agent-based-models-based-on-qualitative-expert-knowledge-an-organised-crime-use-case" class="headerlink" title="Framework for developing quantitative agent based models based on qualitative expert knowledge: an organised crime use-case"></a>Framework for developing quantitative agent based models based on qualitative expert knowledge: an organised crime use-case</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00505">http://arxiv.org/abs/2308.00505</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frederike Oetker, Vittorio Nespeca, Thijs Vis, Paul Duijn, Peter Sloot, Rick Quax</li>
<li>For:	+ The paper aims to provide a systematic and transparent framework for creating agent-based models of criminal networks, specifically for law enforcement purposes.	+ The authors propose a methodology called FREIDA (Framework for Expert-Informed Data-driven Agent-based models) to translate qualitative data into quantitative rules for modeling criminal networks.	+ The paper uses the example of a criminal cocaine network in the Netherlands to demonstrate the FREIDA methodology.* Methods:	+ The authors use a combination of qualitative data sources (case files, literature, interviews) and quantitative data sources (databases) to create a networked agent-based model of the criminal cocaine network.	+ They use empirical laws to translate the qualitative data into quantitative rules, and combine these rules with the quantitative data to create the three dimensions (environment, agents, behavior) of the networked ABM.	+ The authors perform sensitivity analysis, uncertainty quantification, and scenario testing to validate the model and make it robust for law enforcement planning.* Results:	+ The authors find that the model requires flexible parameters and additional case file simulations to be performed to achieve a robust model.	+ The results of the sensitivity analysis and scenario testing indicate the need for adaptive intervention strategies that can respond to changes in the criminal network.Here is the summary in Simplified Chinese text:* For:	+ 本研究旨在提供法 enforcement 目的下的刑事网络模型创建系统，特别是通过转化专家知识到数据驱动的方法。	+ 作者提出了 FREIDA（框架 для专家驱动数据驱动模型）方法，以便将刑事网络的质量数据转化为数值规则。	+ 本研究使用荷兰的刑事可毒网络为例，以示 FREIDA 方法ологи。* Methods:	+ 作者使用质量数据源（案例文件、文献、面谈）和量化数据源（数据库）创建刑事网络的 Agent-based 模型。	+ 他们使用实证法则将质量数据转化为数值规则，并将这些规则与量化数据相结合以创建网络 Agent-based 模型的三个维度（环境、代理、行为）。	+ 作者进行了敏感分析、不确定性评估和enario 测试，以验证模型并使其对法 enforcement 规划有效。* Results:	+ 作者发现模型需要灵活的参数和更多的案例文件 simulate 以实现稳定的模型。	+ 结果表明，需要适应性的干预策略，以应对刑事网络的变化。<details>
<summary>Abstract</summary>
In order to model criminal networks for law enforcement purposes, a limited supply of data needs to be translated into validated agent-based models. What is missing in current criminological modelling is a systematic and transparent framework for modelers and domain experts that establishes a modelling procedure for computational criminal modelling that includes translating qualitative data into quantitative rules. For this, we propose FREIDA (Framework for Expert-Informed Data-driven Agent-based models). Throughout the paper, the criminal cocaine replacement model (CCRM) will be used as an example case to demonstrate the FREIDA methodology. For the CCRM, a criminal cocaine network in the Netherlands is being modelled where the kingpin node is being removed, the goal being for the remaining agents to reorganize after the disruption and return the network into a stable state. Qualitative data sources such as case files, literature and interviews are translated into empirical laws, and combined with the quantitative sources such as databases form the three dimensions (environment, agents, behaviour) of a networked ABM. Four case files are being modelled and scored both for training as well as for validation scores to transition to the computational model and application phase respectively. In the last phase, iterative sensitivity analysis, uncertainty quantification and scenario testing eventually lead to a robust model that can help law enforcement plan their intervention strategies. Results indicate the need for flexible parameters as well as additional case file simulations to be performed.
</details>
<details>
<summary>摘要</summary>
为了模拟犯罪网络，需要有限的数据被翻译成有效的代理基模型。现在的刑事模拟中缺乏一个系统化和透明的框架，使模型者和领域专家可以确定模型生成过程，包括将Qualitative数据转化为Quantitative规则。为此，我们提出了FREIDA（专家驱动数据驱动代理基模型框架）。本文中，我们使用了药物替换模型（CCRM）作为例子，模拟了荷兰一个犯罪冰毒网络，其中王牌节点被移除，目标是让剩下的代理重新组织并返回网络到稳定状态。Qualitative数据源，如案例文件、文献和采访，被翻译成Empirical法律，并与Quantitative数据源，如数据库，共同构成了网络ABM的三个维度（环境、代理、行为）。四个案例文件被模拟和评分，以便在计算模型阶段进行训练和验证分别。在最后阶段，iterative敏感分析、不确定性评估和enario测试，最终导致一个可靠的模型，可以帮助刑事机构规划 intervención策略。结果表明需要灵活的参数以及更多的案例文件模拟，以确保模型的可靠性。
</details></li>
</ul>
<hr>
<h2 id="General-regularization-in-covariate-shift-adaptation"><a href="#General-regularization-in-covariate-shift-adaptation" class="headerlink" title="General regularization in covariate shift adaptation"></a>General regularization in covariate shift adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11503">http://arxiv.org/abs/2307.11503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duc Hoan Nguyen, Sergei V. Pereverzyev, Werner Zellinger</li>
<li>for:  corrected the error of least squares learning algorithms in reproducing kernel Hilbert spaces (RKHS) caused by future data distributions that are different from the training data distribution.</li>
<li>methods:  reweighted kernel regression in RKHS, using the estimated Radon-Nikod&#39;ym derivative of the future data distribution w.r.t.~the training data distribution.</li>
<li>results:  novel results obtained by combining known error bounds, showing that the amount of samples needed to achieve the same order of accuracy as in standard supervised learning without differences in data distributions is smaller than previously proven by state-of-the-art analyses, under weak smoothness conditions.<details>
<summary>Abstract</summary>
Sample reweighting is one of the most widely used methods for correcting the error of least squares learning algorithms in reproducing kernel Hilbert spaces (RKHS), that is caused by future data distributions that are different from the training data distribution. In practical situations, the sample weights are determined by values of the estimated Radon-Nikod\'ym derivative, of the future data distribution w.r.t.~the training data distribution. In this work, we review known error bounds for reweighted kernel regression in RKHS and obtain, by combination, novel results. We show under weak smoothness conditions, that the amount of samples, needed to achieve the same order of accuracy as in the standard supervised learning without differences in data distributions, is smaller than proven by state-of-the-art analyses.
</details>
<details>
<summary>摘要</summary>
样本重Weight是最常用的方法来修正最小二乘学习算法在 reproduce kernel Hilbert space（RKHS）中的错误，这是由于未来数据分布与训练数据分布不同而导致的。在实际情况下，样本重量是根据估计的Radon-Nikodym Derivative，未来数据分布与训练数据分布之间的比例来确定。在这种工作中，我们回顾了已知的重Weighted kernel regression在 RKHS 中的错误上限，并通过组合，获得了新的结果。我们表明，在弱稳定条件下，需要更少的样本数量，以达到标准超级vised learning 无法分布差异的同等精度水平。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-ResNet-Architecture-for-Distributed-Inference-in-Resource-Constrained-IoT-Systems"><a href="#Adaptive-ResNet-Architecture-for-Distributed-Inference-in-Resource-Constrained-IoT-Systems" class="headerlink" title="Adaptive ResNet Architecture for Distributed Inference in Resource-Constrained IoT Systems"></a>Adaptive ResNet Architecture for Distributed Inference in Resource-Constrained IoT Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11499">http://arxiv.org/abs/2307.11499</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fazeela Mazhar Khan, Emna Baccour, Aiman Erbad, Mounir Hamdi</li>
<li>for: 这篇论文的目的是为了提出一个可以适应资源短缺的对应网络，以减少资源共享和延迟，并维持高准确率。</li>
<li>methods: 这篇论文使用了一个Empirical Study，并识别了ResNet中可以被去除的连接，以实现当地资源不足时的分布。然后，它提出了一个多bjective optimization问题，以最小化延迟和最大化准确率，根据可用资源。</li>
<li>results: 根据实验结果，这个适应性的ResNet架构可以降低共享资料、能源消耗和延迟，并维持高准确率。<details>
<summary>Abstract</summary>
As deep neural networks continue to expand and become more complex, most edge devices are unable to handle their extensive processing requirements. Therefore, the concept of distributed inference is essential to distribute the neural network among a cluster of nodes. However, distribution may lead to additional energy consumption and dependency among devices that suffer from unstable transmission rates. Unstable transmission rates harm real-time performance of IoT devices causing low latency, high energy usage, and potential failures. Hence, for dynamic systems, it is necessary to have a resilient DNN with an adaptive architecture that can downsize as per the available resources. This paper presents an empirical study that identifies the connections in ResNet that can be dropped without significantly impacting the model's performance to enable distribution in case of resource shortage. Based on the results, a multi-objective optimization problem is formulated to minimize latency and maximize accuracy as per available resources. Our experiments demonstrate that an adaptive ResNet architecture can reduce shared data, energy consumption, and latency throughout the distribution while maintaining high accuracy.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:深度神经网络继续扩展和复杂化，大多数边缘设备无法处理它们的广泛处理要求。因此，分布式推理是必要的，以分布神经网络到一群节点中。然而，分布可能会导致更多的能源消耗和设备之间的依赖关系，从而影响实时性和可靠性。因此，对动态系统来说，需要一个可靠的DNN，具有可变的架构，以适应可用资源。这篇论文提出了一项实验研究，以确定ResNet中可以被去除的连接，不会对模型性能产生重要影响，以便在资源短缺情况下进行分布。基于结果，我们提出了一个多目标优化问题，以最小化延迟和最大化准确率，根据可用资源。我们的实验表明，可变ResNet架构可以降低共享数据、能源消耗和延迟，同时保持高准确率。
</details></li>
</ul>
<hr>
<h2 id="Predict-Refine-Synthesize-Self-Guiding-Diffusion-Models-for-Probabilistic-Time-Series-Forecasting"><a href="#Predict-Refine-Synthesize-Self-Guiding-Diffusion-Models-for-Probabilistic-Time-Series-Forecasting" class="headerlink" title="Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting"></a>Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11494">http://arxiv.org/abs/2307.11494</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcel Kollovieh, Abdul Fatir Ansari, Michael Bohlke-Schneider, Jasper Zschiegner, Hao Wang, Yuyang Wang</li>
<li>for: 这篇 paper 的目的是探讨时间序列散射模型的应用，并提出了一个不受条件所限的时间序列散射模型（TSDiff），可以在多个时间序列任务上进行应用。</li>
<li>methods: 这篇 paper 使用了一种自我引导机制，让 TSDiff 在推理过程中进行条件化，不需要额外的auxiliary networks或变更训练程式。</li>
<li>results: 这篇 paper 的结果显示 TSDiff 在三个时间序列任务中具有竞争力：一是与条件性的forecasting方法竞争（predict）；二是透过降低计算负载，使用 TSDiff 进行反散射（refine），并且模型对数据的生成性能仍然保持不变。<details>
<summary>Abstract</summary>
Diffusion models have achieved state-of-the-art performance in generative modeling tasks across various domains. Prior works on time series diffusion models have primarily focused on developing conditional models tailored to specific forecasting or imputation tasks. In this work, we explore the potential of task-agnostic, unconditional diffusion models for several time series applications. We propose TSDiff, an unconditionally trained diffusion model for time series. Our proposed self-guidance mechanism enables conditioning TSDiff for downstream tasks during inference, without requiring auxiliary networks or altering the training procedure. We demonstrate the effectiveness of our method on three different time series tasks: forecasting, refinement, and synthetic data generation. First, we show that TSDiff is competitive with several task-specific conditional forecasting methods (predict). Second, we leverage the learned implicit probability density of TSDiff to iteratively refine the predictions of base forecasters with reduced computational overhead over reverse diffusion (refine). Notably, the generative performance of the model remains intact -- downstream forecasters trained on synthetic samples from TSDiff outperform forecasters that are trained on samples from other state-of-the-art generative time series models, occasionally even outperforming models trained on real data (synthesize).
</details>
<details>
<summary>摘要</summary>
Diffusion models 已经在不同领域的生成模型任务中达到了状态计算机。先前的时间序列扩散模型研究都主要集中在发展特定预测或填充任务的条件模型。在这项工作中，我们探索了无条件的时间序列扩散模型在不同应用领域中的潜在性。我们提出了TSDiff，一种未经条件训练的时间序列扩散模型。我们的建议的自顾机制使得TSDiff在推理过程中可以通过自我指导来conditioning，无需附加网络或改变训练过程。我们示出了TSDiff在三个不同的时间序列任务上的效果：预测、修正和生成数据。首先，我们表明TSDiff与一些任务特定的条件预测方法相当竞争（predict）。其次，我们利用TSDiff学习到的隐式概率密度来降低预测基础预测器的计算开销，通过反扩散（refine）来修正基础预测器的预测。另外，通过使用TSDiff生成的 sintetic 样本来训练下游预测器，我们发现了一个关键结果：下游预测器训练在TSDiff生成的 sintetic 样本上表现更好， occasional 甚至超过了使用其他状态之前的生成时间序列模型训练的模型，即使是使用真实数据（synthesize）。
</details></li>
</ul>
<hr>
<h2 id="Robust-Visual-Question-Answering-Datasets-Methods-and-Future-Challenges"><a href="#Robust-Visual-Question-Answering-Datasets-Methods-and-Future-Challenges" class="headerlink" title="Robust Visual Question Answering: Datasets, Methods, and Future Challenges"></a>Robust Visual Question Answering: Datasets, Methods, and Future Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11471">http://arxiv.org/abs/2307.11471</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Ma, Pinghui Wang, Dechen Kong, Zewei Wang, Jun Liu, Hongbin Pei, Junzhou Zhao</li>
<li>for: This paper provides a comprehensive survey of the development of datasets and debiasing methods for visual question answering (VQA) to improve the robustness of VQA systems.</li>
<li>methods: The paper examines the evaluation metrics employed by VQA datasets and proposes a typology of debiasing methods for VQA, including their development process, similarities and differences, robustness comparison, and technical features.</li>
<li>results: The paper analyzes and discusses the robustness of representative vision-and-language pre-training models on VQA and identifies key areas for future research in VQA, including the need for more diverse and challenging datasets and the development of more effective debiasing methods.<details>
<summary>Abstract</summary>
Visual question answering requires a system to provide an accurate natural language answer given an image and a natural language question. However, it is widely recognized that previous generic VQA methods often exhibit a tendency to memorize biases present in the training data rather than learning proper behaviors, such as grounding images before predicting answers. Therefore, these methods usually achieve high in-distribution but poor out-of-distribution performance. In recent years, various datasets and debiasing methods have been proposed to evaluate and enhance the VQA robustness, respectively. This paper provides the first comprehensive survey focused on this emerging fashion. Specifically, we first provide an overview of the development process of datasets from in-distribution and out-of-distribution perspectives. Then, we examine the evaluation metrics employed by these datasets. Thirdly, we propose a typology that presents the development process, similarities and differences, robustness comparison, and technical features of existing debiasing methods. Furthermore, we analyze and discuss the robustness of representative vision-and-language pre-training models on VQA. Finally, through a thorough review of the available literature and experimental analysis, we discuss the key areas for future research from various viewpoints.
</details>
<details>
<summary>摘要</summary>
Visual问题回答需要一个系统提供正确的自然语言答案，givengoogle image和自然语言问题。然而，以前的通用VQA方法经常受到训练数据中存在的偏见的影响，而不是学习正确的行为，如在图像上固定答案。因此，这些方法通常在内部数据上得到高分，但在外部数据上表现不佳。在过去几年，各种数据集和减偏方法被提出来评估和提高VQA的可靠性。这篇论文是这个新趋势的第一篇全面的报告。 Specifically，我们首先提供了数据集的发展过程的概述，从内部和外部视角出发。然后，我们检查了这些数据集使用的评价指标。第三，我们提出了一种分类，描述了现有减偏方法的发展过程，相似性和差异，对比和技术特点。此外，我们分析和讨论了代表性视觉语言预训模型在VQA中的可靠性。最后，通过历史文献的审查和实验分析，我们讨论了未来研究的关键领域从多种角度。
</details></li>
</ul>
<hr>
<h2 id="Distribution-Shift-Matters-for-Knowledge-Distillation-with-Webly-Collected-Images"><a href="#Distribution-Shift-Matters-for-Knowledge-Distillation-with-Webly-Collected-Images" class="headerlink" title="Distribution Shift Matters for Knowledge Distillation with Webly Collected Images"></a>Distribution Shift Matters for Knowledge Distillation with Webly Collected Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11469">http://arxiv.org/abs/2307.11469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jialiang Tang, Shuo Chen, Gang Niu, Masashi Sugiyama, Chen Gong</li>
<li>for: 学习一个轻量级的学生网络从一个预训练的教师网络中。</li>
<li>methods: 使用数据free知识填充方法，从互联网上收集训练实例，并通过对两个网络的结合预测来选择有用的训练实例。然后，对学生网络和教师网络的权重特征和分类器参数进行对齐，并在新的对比学习块中生成受到扰动的数据，以便学生网络学习一种分布不变的表示。</li>
<li>results: 在多个 benchmark 数据集上进行了广泛的实验，结果表明，我们的提议的 KD$^{3}$ 可以比现有的数据free知识填充方法高效。<details>
<summary>Abstract</summary>
Knowledge distillation aims to learn a lightweight student network from a pre-trained teacher network. In practice, existing knowledge distillation methods are usually infeasible when the original training data is unavailable due to some privacy issues and data management considerations. Therefore, data-free knowledge distillation approaches proposed to collect training instances from the Internet. However, most of them have ignored the common distribution shift between the instances from original training data and webly collected data, affecting the reliability of the trained student network. To solve this problem, we propose a novel method dubbed ``Knowledge Distillation between Different Distributions" (KD$^{3}$), which consists of three components. Specifically, we first dynamically select useful training instances from the webly collected data according to the combined predictions of teacher network and student network. Subsequently, we align both the weighted features and classifier parameters of the two networks for knowledge memorization. Meanwhile, we also build a new contrastive learning block called MixDistribution to generate perturbed data with a new distribution for instance alignment, so that the student network can further learn a distribution-invariant representation. Intensive experiments on various benchmark datasets demonstrate that our proposed KD$^{3}$ can outperform the state-of-the-art data-free knowledge distillation approaches.
</details>
<details>
<summary>摘要</summary>
知识填充目标是学习一个轻量级学生网络从一个预训练的教师网络中。在实践中，现有的知识填充方法通常在原始训练数据不可用时无法实现，这是由于一些隐私问题和数据管理考虑。因此，数据无法知识填充方法被提出，以收集来自互联网的训练实例。然而，大多数其中忽略了原始训练数据和互联网收集的实例之间的常见分布偏移，这会影响学生网络的可靠性。为解决这问题，我们提出了一种新的方法，名为“知识填充between Different Distributions”（KD$^{3}$）。该方法包括三个组成部分。特别是，我们首先动态从互联网收集的数据中选择有用的训练实例，根据教师网络和学生网络的共同预测结果进行选择。然后，我们将两个网络的权重特征和分类器参数对齐，以便知识填充。同时，我们还建立了一个新的对比学习块，名为 MixDistribution，以生成一个新的分布 для实例对齐，使学生网络可以进一步学习一种分布不变的表示。我们对多种 benchmark 数据集进行了实验，得到的结果表明，我们的提议的 KD$^{3}$ 可以超越现有的数据无法知识填充方法。
</details></li>
</ul>
<hr>
<h2 id="Zero-touch-realization-of-Pervasive-Artificial-Intelligence-as-a-service-in-6G-networks"><a href="#Zero-touch-realization-of-Pervasive-Artificial-Intelligence-as-a-service-in-6G-networks" class="headerlink" title="Zero-touch realization of Pervasive Artificial Intelligence-as-a-service in 6G networks"></a>Zero-touch realization of Pervasive Artificial Intelligence-as-a-service in 6G networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11468">http://arxiv.org/abs/2307.11468</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emna Baccour, Mhd Saria Allahham, Aiman Erbad, Amr Mohamed, Ahmed Refaey Hussein, Mounir Hamdi</li>
<li>for: 支持Pervasive AI（PAI）的零交互解决方案，以满足6G网络中的自动化需求。</li>
<li>methods: 使用区块链基础设施，实现零交互PAIaaS平台，并通过聚合学习来自动化网络配置和自适应控制。</li>
<li>results: 提出了一种基于区块链的PAIaaS平台，可以轻松地提供零交互的PAI服务，并在6G网络中实现自动化配置和自适应控制，同时能够减少用户的成本、安全性和资源分配担忧。<details>
<summary>Abstract</summary>
The vision of the upcoming 6G technologies, characterized by ultra-dense network, low latency, and fast data rate is to support Pervasive AI (PAI) using zero-touch solutions enabling self-X (e.g., self-configuration, self-monitoring, and self-healing) services. However, the research on 6G is still in its infancy, and only the first steps have been taken to conceptualize its design, investigate its implementation, and plan for use cases. Toward this end, academia and industry communities have gradually shifted from theoretical studies of AI distribution to real-world deployment and standardization. Still, designing an end-to-end framework that systematizes the AI distribution by allowing easier access to the service using a third-party application assisted by a zero-touch service provisioning has not been well explored. In this context, we introduce a novel platform architecture to deploy a zero-touch PAI-as-a-Service (PAIaaS) in 6G networks supported by a blockchain-based smart system. This platform aims to standardize the pervasive AI at all levels of the architecture and unify the interfaces in order to facilitate the service deployment across application and infrastructure domains, relieve the users worries about cost, security, and resource allocation, and at the same time, respect the 6G stringent performance requirements. As a proof of concept, we present a Federated Learning-as-a-service use case where we evaluate the ability of our proposed system to self-optimize and self-adapt to the dynamics of 6G networks in addition to minimizing the users' perceived costs.
</details>
<details>
<summary>摘要</summary>
6G技术的未来视野， caracterizada por ultra-densa red, baja latencia y alta tasa de datos，是支持Pervasive AI（PAI）使用零交互解决方案，实现自动化服务。然而，6G研究仍在幼年期，只有对其设计、实现和应用场景进行了初步的探索。在这个过程中，学术和产业社区逐渐从AI分布的理论研究转移到实际部署和标准化。然而，设计一个端到端框架，使AI分布更加容易访问服务，并且通过第三方应用程序协助零交互服务提供方式，尚未得到充分探索。在这个上下文中，我们提出了一种新的平台架构，用于在6G网络上部署零交互PAI-as-a-Service（PAIaaS）。这个平台旨在将Pervasive AI规范化到所有架构层次，并统一接口，以便在应用和基础设施域中部署服务，缓解用户关于成本、安全性和资源分配的担忧，同时尊重6G严格的性能要求。作为证明，我们提出了一个基于 Federated Learning的服务示例，并评估了我们提posed系统的自动化和自适应能力，以及对6G网络动态的自适应。
</details></li>
</ul>
<hr>
<h2 id="Improve-Long-term-Memory-Learning-Through-Rescaling-the-Error-Temporally"><a href="#Improve-Long-term-Memory-Learning-Through-Rescaling-the-Error-Temporally" class="headerlink" title="Improve Long-term Memory Learning Through Rescaling the Error Temporally"></a>Improve Long-term Memory Learning Through Rescaling the Error Temporally</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11462">http://arxiv.org/abs/2307.11462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shida Wang, Zhanglu Yan</li>
<li>for: 这 paper 研究 seq2seq 模型中长期记忆学习中的错误度选择问题。</li>
<li>methods: 我们发现常用的 errors 都带有短期记忆偏好，包括 Mean Absolute&#x2F;Squared Error。为了减少这种偏好并提高长期记忆学习，我们提议使用时间折算的 error。此外，这种方法还可以解决渐进式 gradient 问题。</li>
<li>results: 我们在不同的长期任务和序列模型上进行了数值实验，结果证明了我们的主张。我们的结果还表明，适当的时间折算 error 对长期记忆学习是必要的。据我们所知，这是 seq2seq 模型中错误度选择问题的首次量化分析。<details>
<summary>Abstract</summary>
This paper studies the error metric selection for long-term memory learning in sequence modelling. We examine the bias towards short-term memory in commonly used errors, including mean absolute/squared error. Our findings show that all temporally positive-weighted errors are biased towards short-term memory in learning linear functionals. To reduce this bias and improve long-term memory learning, we propose the use of a temporally rescaled error. In addition to reducing the bias towards short-term memory, this approach can also alleviate the vanishing gradient issue. We conduct numerical experiments on different long-memory tasks and sequence models to validate our claims. Numerical results confirm the importance of appropriate temporally rescaled error for effective long-term memory learning. To the best of our knowledge, this is the first work that quantitatively analyzes different errors' memory bias towards short-term memory in sequence modelling.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:这篇论文研究序列模型中长期记忆学习中的错误度量选择。我们分析通常使用的错误度量，包括平均绝对值/平方 error，它们是偏向短期记忆的。为了减少这种偏向和改进长期记忆学习，我们提议使用时间折合的错误度量。此外，这种方法还可以减轻淡入梯度问题。我们在不同的长期记忆任务和序列模型上进行了数值实验，以验证我们的结论。数值结果证明了适当的时间折合错误度量对长期记忆学习的重要性。据我们所知，这是序列模型中错误度量偏向短期记忆的首次量化分析。
</details></li>
</ul>
<hr>
<h2 id="Incorporating-Human-Translator-Style-into-English-Turkish-Literary-Machine-Translation"><a href="#Incorporating-Human-Translator-Style-into-English-Turkish-Literary-Machine-Translation" class="headerlink" title="Incorporating Human Translator Style into English-Turkish Literary Machine Translation"></a>Incorporating Human Translator Style into English-Turkish Literary Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11457">http://arxiv.org/abs/2307.11457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeynep Yirmibeşoğlu, Olgun Dursun, Harun Dallı, Mehmet Şahin, Ena Hodzik, Sabri Gürses, Tunga Güngör</li>
<li>for: 英-土文学翻译</li>
<li>methods: 机器翻译模型精度调整，以及手动对照翻译者的特点进行适应</li>
<li>results: 通过适应翻译者的特点，可以高度复制翻译者的风格在目标机器翻译中<details>
<summary>Abstract</summary>
Although machine translation systems are mostly designed to serve in the general domain, there is a growing tendency to adapt these systems to other domains like literary translation. In this paper, we focus on English-Turkish literary translation and develop machine translation models that take into account the stylistic features of translators. We fine-tune a pre-trained machine translation model by the manually-aligned works of a particular translator. We make a detailed analysis of the effects of manual and automatic alignments, data augmentation methods, and corpus size on the translations. We propose an approach based on stylistic features to evaluate the style of a translator in the output translations. We show that the human translator style can be highly recreated in the target machine translations by adapting the models to the style of the translator.
</details>
<details>
<summary>摘要</summary>
尽管机器翻译系统主要设计用于通用领域，但现在有越来越多的人尝试将这些系统应用到其他领域，如文学翻译。在这篇论文中，我们将英语-土耳其文学翻译作为研究对象，并开发了基于翻译家的风格特征的机器翻译模型。我们通过手动对齐的方法来细化一个预训练的机器翻译模型，并对数据增强、词汇库大小和手动对齐的效果进行详细分析。我们提出了基于风格特征的方法来评估翻译家的风格在目标机器翻译中的表现。我们示出，通过适应翻译家的风格，可以在目标机器翻译中高度复制翻译家的风格。
</details></li>
</ul>
<hr>
<h2 id="Providing-personalized-Explanations-a-Conversational-Approach"><a href="#Providing-personalized-Explanations-a-Conversational-Approach" class="headerlink" title="Providing personalized Explanations: a Conversational Approach"></a>Providing personalized Explanations: a Conversational Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11452">http://arxiv.org/abs/2307.11452</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jieting Luo, Thomas Studer, Mehdi Dastani</li>
<li>for: 该论文旨在提供一种方法，使解释者通过继 consecutive conversations 与解释者进行交流，为不同背景和知识水平的听众提供个性化解释。</li>
<li>methods: 该方法基于 conversational AI 技术，通过多个交流阶段，解释者可以了解解释者的背景和需求，并逐渐提供更加个性化的解释。</li>
<li>results: 作者证明了，如果存在一个可以被解释者理解的解释，并且解释者知悉这个解释，那么对于任何初始声明， conversation 都将terminates。<details>
<summary>Abstract</summary>
The increasing applications of AI systems require personalized explanations for their behaviors to various stakeholders since the stakeholders may have various knowledge and backgrounds. In general, a conversation between explainers and explainees not only allows explainers to obtain the explainees' background, but also allows explainees to better understand the explanations. In this paper, we propose an approach for an explainer to communicate personalized explanations to an explainee through having consecutive conversations with the explainee. We prove that the conversation terminates due to the explainee's justification of the initial claim as long as there exists an explanation for the initial claim that the explainee understands and the explainer is aware of.
</details>
<details>
<summary>摘要</summary>
随着人工智能系统的应用越来越广泛，需要对其行为提供个性化解释，以适应不同的潜在利益相关者的不同知识背景。通常，一个对话 между解释者和解释者可以让解释者了解解释者的背景，同时也可以让解释者更好地理解解释。在这篇论文中，我们提议一种方法，通过与解释者进行连续对话来让解释者对解释者进行个性化解释。我们证明，如果存在一个对初始laim的解释，并且解释者理解这个解释，那么对话就会结束。
</details></li>
</ul>
<hr>
<h2 id="AIGC-Empowering-Telecom-Sector-White-Paper-chinese"><a href="#AIGC-Empowering-Telecom-Sector-White-Paper-chinese" class="headerlink" title="AIGC Empowering Telecom Sector White Paper_chinese"></a>AIGC Empowering Telecom Sector White Paper_chinese</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11449">http://arxiv.org/abs/2307.11449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ye Ouyang, Yaqin Zhang, Xiaozhou Ye, Yunxin Liu, Yong Song, Yang Liu, Sen Bian, Zhiyong Liu</li>
<li>for: 本研究旨在探讨如何在电信领域应用人工智能大数据技术（AIGC），以及如何在电信领域实现AIGC应用。</li>
<li>methods: 本研究通过分析GPT模型，对电信服务提供商（Telco）的应用场景进行分析，提出了telco增强认知能力系统，并提供了构建电信服务GPT的方法。</li>
<li>results: 本研究提出了一种telco增强认知能力系统，并实现了在电信领域应用GPT的具体实践。<details>
<summary>Abstract</summary>
In the global craze of GPT, people have deeply realized that AI, as a transformative technology and key force in economic and social development, will bring great leaps and breakthroughs to the global industry and profoundly influence the future world competition pattern. As the builder and operator of information and communication infrastructure, the telecom sector provides infrastructure support for the development of AI, and even takes the lead in the implementation of AI applications. How to enable the application of AIGC (GPT) and implement AIGC in the telecom sector are questions that telecom practitioners must ponder and answer. Through the study of GPT, a typical representative of AIGC, the authors have analyzed how GPT empowers the telecom sector in the form of scenarios, discussed the gap between the current GPT general model and telecom services, proposed for the first time a Telco Augmented Cognition capability system, provided answers to how to construct a telecom service GPT in the telecom sector, and carried out various practices. Our counterparts in the industry are expected to focus on collaborative innovation around telecom and AI, build an open and shared innovation ecosystem, promote the deep integration of AI and telecom sector, and accelerate the construction of next-generation information infrastructure, in an effort to facilitate the digital transformation of the economy and society.
</details>
<details>
<summary>摘要</summary>
在全球GPT热潮中，人们已经深刻认识到AI作为转变技术和经济社会发展的关键力量，将会带来巨大的突破和进步，对全球行业和未来世界竞争格局产生深远的影响。作为信息和通信基础设施的建设者和运营者，电信业务提供了对AI发展的基础设施支持，甚至在实施AI应用方面领先于其他行业。如何应用AIGC（GPT）和在电信业务中实施AIGC是电信干部必须思考和答案的问题。通过研究GPT，一种典型的AIGC代表，作者们分析了GPT如何使电信业务具备更高的智能能力，讨论了现有GPT通用模型与电信服务之间的差距，提出了为电信服务GPT建立telco增强认知能力系统的建议，并提供了如何构建电信服务GPT的方法。我们的行业同仁应该集中协同创新，建立开放共享创新生态系统，促进AI和电信业务深度融合，加快构建未来信息基础设施，以便推动经济社会数字化转型。
</details></li>
</ul>
<hr>
<h2 id="Batching-for-Green-AI-–-An-Exploratory-Study-on-Inference"><a href="#Batching-for-Green-AI-–-An-Exploratory-Study-on-Inference" class="headerlink" title="Batching for Green AI – An Exploratory Study on Inference"></a>Batching for Green AI – An Exploratory Study on Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11434">http://arxiv.org/abs/2307.11434</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim Yarally, Luís Cruz, Daniel Feitosa, June Sallou, Arie van Deursen</li>
<li>For:	+ The paper examines the effect of input batching on the energy consumption and response times of five fully-trained neural networks for computer vision.	+ The study aims to investigate the potential benefits of introducing a batch size during the application phase of a deep learning model.* Methods:	+ The paper uses five fully-trained neural networks for computer vision that were considered state-of-the-art at the time of their publication.	+ The authors measure the energy consumption and response times of the networks with and without input batching.* Results:	+ The results suggest that batching has a significant effect on both energy consumption and response times.	+ The authors find that energy consumption rises at a much steeper pace than accuracy over the past decade, and highlight one particular network (ShuffleNetV2(2018)) that achieved a competitive performance while maintaining a lower energy consumption.Here is the simplified Chinese version of the three key information points:* For:	+ 本研究探讨了输入批处理对计算机视觉领域五个完全训练的神经网络的能耗和响应时间的影响。	+ 研究目的是调查在深度学习模型应用阶段是否可以通过引入批处理来获得优点。* Methods:	+ 研究使用了五个完全训练的计算机视觉神经网络，这些神经网络在其出版时被认为是领域的状态OF-the-art。	+ 作者们测量了这些神经网络在批处理和无批处理情况下的能耗和响应时间。* Results:	+ 结果表明，批处理对计算机视觉领域的能耗和响应时间有着显著的影响。	+ 作者们发现，在过去十年中，能耗的增长速度远远高于精度的提高，并指出了一个特定的神经网络（ShuffleNetV2（2018）），它在其时间内实现了竞争性表现，同时具有远低的能耗。<details>
<summary>Abstract</summary>
The batch size is an essential parameter to tune during the development of new neural networks. Amongst other quality indicators, it has a large degree of influence on the model's accuracy, generalisability, training times and parallelisability. This fact is generally known and commonly studied. However, during the application phase of a deep learning model, when the model is utilised by an end-user for inference, we find that there is a disregard for the potential benefits of introducing a batch size. In this study, we examine the effect of input batching on the energy consumption and response times of five fully-trained neural networks for computer vision that were considered state-of-the-art at the time of their publication. The results suggest that batching has a significant effect on both of these metrics. Furthermore, we present a timeline of the energy efficiency and accuracy of neural networks over the past decade. We find that in general, energy consumption rises at a much steeper pace than accuracy and question the necessity of this evolution. Additionally, we highlight one particular network, ShuffleNetV2(2018), that achieved a competitive performance for its time while maintaining a much lower energy consumption. Nevertheless, we highlight that the results are model dependent.
</details>
<details>
<summary>摘要</summary>
批处大小是深度学习模型开发中非常重要的参数。其影响模型的准确率、通用性、训练时间和并行性等质量指标。这种情况通常被认为并且广泛研究。然而，在深度学习模型在实际应用阶段使用时，批处大小却被忽视。本研究检查了五种在发表时被视为state-of-the-art的计算机视觉神经网络的输入批处对能耗和响应时间的影响。结果表明，批处有着显著的影响。此外，我们还提供了过去十年内神经网络能效率和准确率的时间线。我们发现，能 consumption 在准确率上升的速度快得多，并质疑这种演化的必要性。此外，我们还提到了一个特定的网络，ShuffleNetV2（2018），在其时间上实现了竞争性表现，同时保持了许多更低的能 consumption。然而，我们注意到结果受模型的影响。
</details></li>
</ul>
<hr>
<h2 id="Prompting-Large-Language-Models-with-Speech-Recognition-Abilities"><a href="#Prompting-Large-Language-Models-with-Speech-Recognition-Abilities" class="headerlink" title="Prompting Large Language Models with Speech Recognition Abilities"></a>Prompting Large Language Models with Speech Recognition Abilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11795">http://arxiv.org/abs/2307.11795</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Junteng Jia, Yuan Shangguan, Ke Li, Jinxi Guo, Wenhan Xiong, Jay Mahadeokar, Ozlem Kalinli, Christian Fuegen, Mike Seltzer</li>
<li>for: 这个论文的目的是扩展大型自然语言模型（LLM）的能力，使其能够直接从语音识别（ASR）系统中获得语音识别能力。</li>
<li>methods: 论文使用了一种小型音频编码器，将其直接附加到文本token embedding中，使LLM可以变成一个自动语音识别系统，并且可以在原来的文本识别系统中使用。</li>
<li>results: 实验表明，将一个小型音频编码器添加到open sourced LLaMA-7B模型中，可以比基线模型提高18%的性能，并且可以在多语言语音识别中进行多语言语音识别，即使LLaMA只在英语文本上进行训练。此外，论文还进行了减少学习环境的研究，以及增加音频编码器的步长和增加音频编码器的步长以生成更少的嵌入。研究结果表明，使用LLM可以在长度达1秒的语音识别中进行多语言语音识别。<details>
<summary>Abstract</summary>
Large language models have proven themselves highly flexible, able to solve a wide range of generative tasks, such as abstractive summarization and open-ended question answering. In this paper we extend the capabilities of LLMs by directly attaching a small audio encoder allowing it to perform speech recognition. By directly prepending a sequence of audial embeddings to the text token embeddings, the LLM can be converted to an automatic speech recognition (ASR) system, and be used in the exact same manner as its textual counterpart. Experiments on Multilingual LibriSpeech (MLS) show that incorporating a conformer encoder into the open sourced LLaMA-7B allows it to outperform monolingual baselines by 18% and perform multilingual speech recognition despite LLaMA being trained overwhelmingly on English text. Furthermore, we perform ablation studies to investigate whether the LLM can be completely frozen during training to maintain its original capabilities, scaling up the audio encoder, and increasing the audio encoder striding to generate fewer embeddings. The results from these studies show that multilingual ASR is possible even when the LLM is frozen or when strides of almost 1 second are used in the audio encoder opening up the possibility for LLMs to operate on long-form audio.
</details>
<details>
<summary>摘要</summary>
大型语言模型已经证明了它们在多种生成任务中的高度灵活性，如抽象摘要和开端问答。在这篇论文中，我们将LLM的功能扩展到附加一个小型音频编码器，使其能够进行语音识别。通过直接附加文本Token embedding序列的音频嵌入，LLM可以转化为自动语音识别（ASR）系统，并且可以在同样的方式下使用。在多语言LibriSpeech（MLS）上进行了实验，并示出了在开源的LLaMA-7B中附加一个协调器编码器后，其能够比单语言基准点高出18%，并且可以进行多语言语音识别，即使LLaMA在训练时主要是使用英语文本。此外，我们还进行了剥离研究，以investigate Whether the LLM可以在训练时被冻结，缩放音频编码器，并减少音频编码器的步进。研究结果表明，多语言ASR是可能的， même when the LLM is frozen or when strides of almost 1 second are used in the audio encoder。这开 up the possibility for LLMs to operate on long-form audio.
</details></li>
</ul>
<hr>
<h2 id="A-Video-based-Detector-for-Suspicious-Activity-in-Examination-with-OpenPose"><a href="#A-Video-based-Detector-for-Suspicious-Activity-in-Examination-with-OpenPose" class="headerlink" title="A Video-based Detector for Suspicious Activity in Examination with OpenPose"></a>A Video-based Detector for Suspicious Activity in Examination with OpenPose</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11413">http://arxiv.org/abs/2307.11413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reuben Moyo, Stanley Ndebvu, Michael Zimba, Jimmy Mbelwa</li>
<li>for: 防止学生和考试监管者之间的假设行为，保持学术integrity</li>
<li>methods: 使用自动化视频分析和人 pose estimation技术，检测考试过程中可疑的活动</li>
<li>results: 提高了考试监管效果，减少了考试过程中的假设行为<details>
<summary>Abstract</summary>
Examinations are a crucial part of the learning process, and academic institutions invest significant resources into maintaining their integrity by preventing cheating from students or facilitators. However, cheating has become rampant in examination setups, compromising their integrity. The traditional method of relying on invigilators to monitor every student is impractical and ineffective. To address this issue, there is a need to continuously record exam sessions to monitor students for suspicious activities. However, these recordings are often too lengthy for invigilators to analyze effectively, and fatigue may cause them to miss significant details. To widen the coverage, invigilators could use fixed overhead or wearable cameras. This paper introduces a framework that uses automation to analyze videos and detect suspicious activities during examinations efficiently and effectively. We utilized the OpenPose framework and Convolutional Neural Network (CNN) to identify students exchanging objects during exams. This detection system is vital in preventing cheating and promoting academic integrity, fairness, and quality education for institutions.
</details>
<details>
<summary>摘要</summary>
笔考是学习过程中不可或缺的一部分，学院投入了大量资源来保持笔考的完整性，避免学生或指导教师的作弊行为。然而，在考试场景中，作弊行为已经变得普遍，这威胁到了笔考的完整性。传统的依靠监督员监视每名学生的方法已经成为不切实际和不可行的。为解决这一问题，需要不断录制考试会考session，以便监测学生的可疑活动。然而，这些录制材料经常是长得太长，监督员分析效果不佳，疲劳可能会导致重要细节被遗弃。为了扩大覆盖率，监督员可以使用固定卫星或佩戴式摄像头。本文介绍了一种基于自动化分析视频，高效地检测考试会考中的可疑活动的框架。我们利用了OpenPose框架和卷积神经网络（CNN）来识别考试会考中学生交换物品的行为。这个检测系统是保持学术 integrity、公平性和质量教育的重要工具。
</details></li>
</ul>
<hr>
<h2 id="Deep-Directly-Trained-Spiking-Neural-Networks-for-Object-Detection"><a href="#Deep-Directly-Trained-Spiking-Neural-Networks-for-Object-Detection" class="headerlink" title="Deep Directly-Trained Spiking Neural Networks for Object Detection"></a>Deep Directly-Trained Spiking Neural Networks for Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11411">http://arxiv.org/abs/2307.11411</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/BICLab/EMS-YOLO">https://github.com/BICLab/EMS-YOLO</a></li>
<li>paper_authors: Qiaoyi Su, Yuhong Chou, Yifan Hu, Jianing Li, Shijie Mei, Ziyang Zhang, Guoqi Li</li>
<li>for: 这个研究是为了解决如何透过直接训练神经网络（SNN）来实现物体检测任务，而不是使用传统的ANN-SNN转换策略。</li>
<li>methods: 我们提出了一个名为EMS-YOLO的新的直接训练SNN框架，它使用代理Gradient来训练深度的SNN，并且设计了一个全节点遗传架构EMS-ResNet，可以有效地延长深度的SNN训练。</li>
<li>results: 我们的方法比前一代ANN-SNN转换方法（至少500步）在更少的时间步骤（仅4步）中表现更好，并且可以在几乎相同的时间点数上 дости��FC（5.83倍）较少的能源消耗。<details>
<summary>Abstract</summary>
Spiking neural networks (SNNs) are brain-inspired energy-efficient models that encode information in spatiotemporal dynamics. Recently, deep SNNs trained directly have shown great success in achieving high performance on classification tasks with very few time steps. However, how to design a directly-trained SNN for the regression task of object detection still remains a challenging problem. To address this problem, we propose EMS-YOLO, a novel directly-trained SNN framework for object detection, which is the first trial to train a deep SNN with surrogate gradients for object detection rather than ANN-SNN conversion strategies. Specifically, we design a full-spike residual block, EMS-ResNet, which can effectively extend the depth of the directly-trained SNN with low power consumption. Furthermore, we theoretically analyze and prove the EMS-ResNet could avoid gradient vanishing or exploding. The results demonstrate that our approach outperforms the state-of-the-art ANN-SNN conversion methods (at least 500 time steps) in extremely fewer time steps (only 4 time steps). It is shown that our model could achieve comparable performance to the ANN with the same architecture while consuming 5.83 times less energy on the frame-based COCO Dataset and the event-based Gen1 Dataset.
</details>
<details>
<summary>摘要</summary>
神经网络（SNN）是基于脑的能效模型，它将信息编码在空间时间动态中。最近，深度SNN直接训练得到了高性能的分类任务，但是如何设计直接训练的SNN用于对象检测任务仍然是一个挑战。为解决这个问题，我们提出了EMS-YOLO，一种新的直接训练SNN框架 для对象检测，这是首次使用代理梯度来训练深度SNN而不是转换为ANN-SNN策略。我们设计了全突起减噪块，EMS-ResNet，可以有效地延长直接训练SNN的深度，同时降低能耗。此外，我们理论分析并证明EMS-ResNet可以避免梯度消失或扩散。结果表明，我们的方法在训练4个时间步骤后可以超越当前的ANN-SNN转换方法（至少500个时间步骤）。此外，我们的模型在Frame-based COCO数据集和Event-based Gen1数据集上可以与同样的ANN架构具有相同的性能，同时占用5.83倍少的能量。
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Modeling-of-Inter-and-Intra-observer-Variability-in-Medical-Image-Segmentation"><a href="#Probabilistic-Modeling-of-Inter-and-Intra-observer-Variability-in-Medical-Image-Segmentation" class="headerlink" title="Probabilistic Modeling of Inter- and Intra-observer Variability in Medical Image Segmentation"></a>Probabilistic Modeling of Inter- and Intra-observer Variability in Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11397">http://arxiv.org/abs/2307.11397</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arne Schmidt, Pablo Morales-Álvarez, Rafael Molina</li>
<li>for: 这篇论文的目的是提出一个新的医疗影像分类模型，以减少医疗专业人员之间和内部的观察者间变化。</li>
<li>methods: 这个模型以probabilistic inter-observer和intra-observer variation network（Pionono）的名称，它captures each rater’s labeling behavior as a multidimensional probability distribution，并与影像特征图对组合以生成可信度分布式的分类预测。这个模型可以通过统计推论优化，并可以在端到端的整合训练中进行训练。</li>
<li>results: 实验结果显示，Pionono模型可以高效地预测医疗影像分类，并且比起现有的State-of-the-art模型，例如STAPLE、Probabilistic U-Net和基于混淆矩阵的模型，有更高的准确性。此外，Pionono模型可以预测多个协调的分类图对，这些图对可以提供额外的有用信息来医疗诊断过程。<details>
<summary>Abstract</summary>
Medical image segmentation is a challenging task, particularly due to inter- and intra-observer variability, even between medical experts. In this paper, we propose a novel model, called Probabilistic Inter-Observer and iNtra-Observer variation NetwOrk (Pionono). It captures the labeling behavior of each rater with a multidimensional probability distribution and integrates this information with the feature maps of the image to produce probabilistic segmentation predictions. The model is optimized by variational inference and can be trained end-to-end. It outperforms state-of-the-art models such as STAPLE, Probabilistic U-Net, and models based on confusion matrices. Additionally, Pionono predicts multiple coherent segmentation maps that mimic the rater's expert opinion, which provides additional valuable information for the diagnostic process. Experiments on real-world cancer segmentation datasets demonstrate the high accuracy and efficiency of Pionono, making it a powerful tool for medical image analysis.
</details>
<details>
<summary>摘要</summary>
医学图像分割是一项具有挑战性的任务，尤其是由于内部和外部观察者的变化， même between 医疗专家。在这篇论文中，我们提出了一种新的模型，即概率性内部和外部变化网络（Pionono）。它捕捉了每个评分者的标注行为，并将其映射到多维度概率分布中。然后，它将这些信息与图像特征图 integration 以生成概率性的分割预测。该模型可以通过变量推断优化，并可以在端到端方式进行训练。与现有的STATE-OF-THE-ART模型，如STAPLE和概率性U-Net，以及基于混淆矩阵的模型相比，Pionono 表现更高准确和有效。此外，Pionono 还预测了多个协调的分割图，这些图像类似于评分者的专家意见，这些信息可以为诊断过程提供valuable 的信息。在实际的肿瘤分割数据集上进行了实验，Pionono 的准确率和效率均很高，这使得它成为医学图像分析中的一个强大工具。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Model-based-System-to-Provide-Immediate-Feedback-to-Students-in-Flipped-Classroom-Preparation-Learning"><a href="#Large-Language-Model-based-System-to-Provide-Immediate-Feedback-to-Students-in-Flipped-Classroom-Preparation-Learning" class="headerlink" title="Large Language Model-based System to Provide Immediate Feedback to Students in Flipped Classroom Preparation Learning"></a>Large Language Model-based System to Provide Immediate Feedback to Students in Flipped Classroom Preparation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11388">http://arxiv.org/abs/2307.11388</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shintaro Uchiyama, Kyoji Umemura, Yusuke Morita</li>
<li>for: 这种系统是为了提供prepare learning中的学生即时反馈，以解决flipped classroom模型中的一些挑战，如保证学生在学习过程中具备情感动力和学习motivation。</li>
<li>methods: 该系统使用大型自然语言模型提供学生在准备学习过程中的即时反馈，并使用ChatGPT API开发了一个视频学习支持系统。为了将ChatGPT的答案与学生的问题相align，该paper还提出了一种方法。此外，该paper还提出了一种方法，用于收集教师对学生问题的答案，并使其为学生提供更多的指导。</li>
<li>results: 该paper提出了一种基于大型自然语言模型的prepare learning支持系统，可以帮助学生在准备学习过程中得到即时反馈，提高学习效果。<details>
<summary>Abstract</summary>
This paper proposes a system that uses large language models to provide immediate feedback to students in flipped classroom preparation learning. This study aimed to solve challenges in the flipped classroom model, such as ensuring that students are emotionally engaged and motivated to learn. Students often have questions about the content of lecture videos in the preparation of flipped classrooms, but it is difficult for teachers to answer them immediately. The proposed system was developed using the ChatGPT API on a video-watching support system for preparation learning that is being used in real practice. Answers from ChatGPT often do not align with the context of the student's question. Therefore, this paper also proposes a method to align the answer with the context. This paper also proposes a method to collect the teacher's answers to the students' questions and use them as additional guides for the students. This paper discusses the design and implementation of the proposed system.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种基于大语言模型的系统，用于在flipped classroom准备学习中提供立即反馈给学生。这项研究的目标是解决flipped classroom模型中的一些挑战，如确保学生在准备过程中保持情感投入和学习motivation。学生经常有lecture视频的内容问题，但是教师answering这些问题很困难。该系统基于ChatGPT API的视频观看支持系统，已经在实践中应用。然而，ChatGPT的答案不一定与学生的问题Context相align，因此该论文还提出了一种方法来将答案与Context进行Alignment。此外，该论文还提出了一种方法来收集教师对学生问题的答案，并使其为学生提供额外指导。该论文讨论了系统的设计和实现。
</details></li>
</ul>
<hr>
<h2 id="Diverse-Offline-Imitation-via-Fenchel-Duality"><a href="#Diverse-Offline-Imitation-via-Fenchel-Duality" class="headerlink" title="Diverse Offline Imitation via Fenchel Duality"></a>Diverse Offline Imitation via Fenchel Duality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11373">http://arxiv.org/abs/2307.11373</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marin Vlastelica, Pavel Kolev, Jin Cheng, Georg Martius</li>
<li>for: 本研究旨在开发一个不需要在环境上线的自动技能发现算法。</li>
<li>methods: 我们使用了互信息目标函数，并运用了 Fenchel duality、征 Stimulation 学习和无监督技能发现，开发出一个简单的OFFLINE算法，以获得与专家相似的技能。</li>
<li>results: 我们的主要贡献是将 Fenchel duality、征 Stimulation 学习和无监督技能发现相互连接，并提供了一个简单的OFFLINE算法，以获得与专家相似的多样化技能。<details>
<summary>Abstract</summary>
There has been significant recent progress in the area of unsupervised skill discovery, with various works proposing mutual information based objectives, as a source of intrinsic motivation. Prior works predominantly focused on designing algorithms that require online access to the environment. In contrast, we develop an \textit{offline} skill discovery algorithm. Our problem formulation considers the maximization of a mutual information objective constrained by a KL-divergence. More precisely, the constraints ensure that the state occupancy of each skill remains close to the state occupancy of an expert, within the support of an offline dataset with good state-action coverage. Our main contribution is to connect Fenchel duality, reinforcement learning and unsupervised skill discovery, and to give a simple offline algorithm for learning diverse skills that are aligned with an expert.
</details>
<details>
<summary>摘要</summary>
“近期有很大的进步在无监督技能发现领域，多个作品提出了基于互联信息的目标，作为自适应的动机。先前的工作主要集中在设计需要在环境上线存取的算法。相比之下，我们开发了一个OFFLINE技能发现算法。我们的问题设计将最大化互联信息目标，并受到KL散度的限制。更加精确地说，这些限制保证每个技能的状态占有率保持在专家状态占有率的支持下，在对应的OFFLINE数据集中具有良好的状态动作覆盖。我们的主要贡献是连接 Fenchel 对偶、复制学习和无监督技能发现，并提供一个简单的OFFLINE算法，用于学习与专家Alignment的多标的技能。”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="CohortGPT-An-Enhanced-GPT-for-Participant-Recruitment-in-Clinical-Study"><a href="#CohortGPT-An-Enhanced-GPT-for-Participant-Recruitment-in-Clinical-Study" class="headerlink" title="CohortGPT: An Enhanced GPT for Participant Recruitment in Clinical Study"></a>CohortGPT: An Enhanced GPT for Participant Recruitment in Clinical Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11346">http://arxiv.org/abs/2307.11346</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihan Guan, Zihao Wu, Zhengliang Liu, Dufan Wu, Hui Ren, Quanzheng Li, Xiang Li, Ninghao Liu</li>
<li>For: 这个研究旨在提高大型自然语言模型（LLMs）在医疗研究中的参考组建 task 的性能，特别是在对于专业医生的医疗报告中进行分类。* Methods: 这个研究使用了一个知识图作为助长信息，以及一个链条思考（CoT）样本选择策略增强了执行学习。* Results: 这个研究获得了满意的性能，并且在有限的数据情况下表现更好。 code和示例数据集都可以在以下网址中获取：<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/CohortGPT-4872/">https://anonymous.4open.science/r/CohortGPT-4872/</a><details>
<summary>Abstract</summary>
Participant recruitment based on unstructured medical texts such as clinical notes and radiology reports has been a challenging yet important task for the cohort establishment in clinical research. Recently, Large Language Models (LLMs) such as ChatGPT have achieved tremendous success in various downstream tasks thanks to their promising performance in language understanding, inference, and generation. It is then natural to test their feasibility in solving the cohort recruitment task, which involves the classification of a given paragraph of medical text into disease label(s). However, when applied to knowledge-intensive problem settings such as medical text classification, where the LLMs are expected to understand the decision made by human experts and accurately identify the implied disease labels, the LLMs show a mediocre performance. A possible explanation is that, by only using the medical text, the LLMs neglect to use the rich context of additional information that languages afford. To this end, we propose to use a knowledge graph as auxiliary information to guide the LLMs in making predictions. Moreover, to further boost the LLMs adapt to the problem setting, we apply a chain-of-thought (CoT) sample selection strategy enhanced by reinforcement learning, which selects a set of CoT samples given each individual medical report. Experimental results and various ablation studies show that our few-shot learning method achieves satisfactory performance compared with fine-tuning strategies and gains superb advantages when the available data is limited. The code and sample dataset of the proposed CohortGPT model is available at: https://anonymous.4open.science/r/CohortGPT-4872/
</details>
<details>
<summary>摘要</summary>
参与者招募基于无结构医疗文本如临床笔记和放射报告的问题是临床研究中的一项挑战。现在，大型语言模型（LLMs）如ChatGPT在不同下游任务中表现出色，因为它们在语言理解、推理和生成方面表现出色。因此，试图使其在这个参与者招募任务中表现出色，这个任务涉及将给定的医疗文本分类到疾病标签中。然而，当应用到医学知识密集的问题设定中，LLMs表现不佳。一个可能的解释是，只使用医疗文本，LLMs会忽略语言提供的丰富背景信息。为此，我们提议使用知识图作为 auxillary 信息来导引 LLMs 进行预测。此外，为了进一步提高 LLMs 适应问题设定，我们应用了链条思维（CoT）样本选择策略强化学习，这种策略选择每个医疗报告中的一组 CoT 样本。实验结果和多种减少学习研究表明，我们的少量学习方法在可用数据有限时表现满意，而且与精度调整策略相比具有超卓的优势。代码和示例数据集的提案CohortGPT模型可以在以下链接中获取：https://anonymous.4open.science/r/CohortGPT-4872/
</details></li>
</ul>
<hr>
<h2 id="A-Two-stage-Fine-tuning-Strategy-for-Generalizable-Manipulation-Skill-of-Embodied-AI"><a href="#A-Two-stage-Fine-tuning-Strategy-for-Generalizable-Manipulation-Skill-of-Embodied-AI" class="headerlink" title="A Two-stage Fine-tuning Strategy for Generalizable Manipulation Skill of Embodied AI"></a>A Two-stage Fine-tuning Strategy for Generalizable Manipulation Skill of Embodied AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11343">http://arxiv.org/abs/2307.11343</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xtli12/gxu-lipe">https://github.com/xtli12/gxu-lipe</a></li>
<li>paper_authors: Fang Gao, XueTao Li, Jun Yu, Feng Shaung</li>
<li>for: The paper aims to enhance the generalization capability of Embodied AI models in real-world scenarios, particularly in the Maniskill2 benchmark.</li>
<li>methods: The proposed method uses a two-stage fine-tuning strategy based on the Maniskill2 benchmark, which involves training the model on diverse datasets of demonstrations and evaluating its ability to generalize to unseen scenarios.</li>
<li>results: The proposed method achieved the 1st prize in all three tracks of the ManiSkill2 Challenge, demonstrating its effectiveness in enhancing the generalization abilities of Embodied AI models.Here’s the same information in Simplified Chinese text:</li>
<li>for: 本研究旨在提高Embodied AI模型在实际场景中的泛化能力，特别是在Maniskill2 benchmark中。</li>
<li>methods: 提议的方法使用两个阶段精细调整策略，基于Maniskill2 benchmark，包括在多个示例数据上训练模型，并评估其能够在未看过的场景中泛化。</li>
<li>results: 提议的方法在ManiSkill2 Challenge中获得了全部三个赛道的冠军，表明其能够提高Embodied AI模型的泛化能力。<details>
<summary>Abstract</summary>
The advent of Chat-GPT has led to a surge of interest in Embodied AI. However, many existing Embodied AI models heavily rely on massive interactions with training environments, which may not be practical in real-world situations. To this end, the Maniskill2 has introduced a full-physics simulation benchmark for manipulating various 3D objects. This benchmark enables agents to be trained using diverse datasets of demonstrations and evaluates their ability to generalize to unseen scenarios in testing environments. In this paper, we propose a novel two-stage fine-tuning strategy that aims to further enhance the generalization capability of our model based on the Maniskill2 benchmark. Through extensive experiments, we demonstrate the effectiveness of our approach by achieving the 1st prize in all three tracks of the ManiSkill2 Challenge. Our findings highlight the potential of our method to improve the generalization abilities of Embodied AI models and pave the way for their ractical applications in real-world scenarios. All codes and models of our solution is available at https://github.com/xtli12/GXU-LIPE.git
</details>
<details>
<summary>摘要</summary>
chat-gpt 的出现引发了人工智能embody 的兴趣，但是许多现有的embody 模型具有庞大的训练环境依赖，可能在实际应用中不够实用。为此，Maniskill2 引入了一个完整的物理 simulate 标准套件，用于 manipulating 多种 3D 物体。这个套件允许代理人通过多种示例数据进行训练，并评估其在未看过enario 中的扩展性。在这篇论文中，我们提出了一种新的两阶段精细调整策略，旨在进一步提高我们模型的扩展能力。经过广泛的实验，我们证明了我们的方法的有效性，在 Maniskill2 挑战赛中获得了全部三个 tracks 的第一奖。我们的发现指出了我们的方法可以提高 Embodied AI 模型的扩展能力，并为它们的实际应用奠定基础。codes 和模型的解决方案可以在 <https://github.com/xtli12/GXU-LIPE.git> 中找到。
</details></li>
</ul>
<hr>
<h2 id="OpenGDA-Graph-Domain-Adaptation-Benchmark-for-Cross-network-Learning"><a href="#OpenGDA-Graph-Domain-Adaptation-Benchmark-for-Cross-network-Learning" class="headerlink" title="OpenGDA: Graph Domain Adaptation Benchmark for Cross-network Learning"></a>OpenGDA: Graph Domain Adaptation Benchmark for Cross-network Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11341">http://arxiv.org/abs/2307.11341</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/skyorca/opengda">https://github.com/skyorca/opengda</a></li>
<li>paper_authors: Boshen Shi, Yongqing Wang, Fangda Guo, Jiangli Shao, Huawei Shen, Xueqi Cheng<br>for: 本文主要针对的是评估图域适应模型（Graph Domain Adaptation，GDA）在不同类型的任务上的性能，包括节点分类、边分类和图分类。methods: 本文提出了一个名为OpenGDA的测试基准，它提供了丰富的预处理后的数据集和统一的评估管线，用于评估不同类型的GDA模型。results: 测试结果显示，当应用于实际应用场景时，GDA模型的性能具有一定的挑战，并且需要进一步的研究以提高其实际应用效果。<details>
<summary>Abstract</summary>
Graph domain adaptation models are widely adopted in cross-network learning tasks, with the aim of transferring labeling or structural knowledge. Currently, there mainly exist two limitations in evaluating graph domain adaptation models. On one side, they are primarily tested for the specific cross-network node classification task, leaving tasks at edge-level and graph-level largely under-explored. Moreover, they are primarily tested in limited scenarios, such as social networks or citation networks, lacking validation of model's capability in richer scenarios. As comprehensively assessing models could enhance model practicality in real-world applications, we propose a benchmark, known as OpenGDA. It provides abundant pre-processed and unified datasets for different types of tasks (node, edge, graph). They originate from diverse scenarios, covering web information systems, urban systems and natural systems. Furthermore, it integrates state-of-the-art models with standardized and end-to-end pipelines. Overall, OpenGDA provides a user-friendly, scalable and reproducible benchmark for evaluating graph domain adaptation models. The benchmark experiments highlight the challenges of applying GDA models to real-world applications with consistent good performance, and potentially provide insights to future research. As an emerging project, OpenGDA will be regularly updated with new datasets and models. It could be accessed from https://github.com/Skyorca/OpenGDA.
</details>
<details>
<summary>摘要</summary>
Graph domain adaptation模型在跨网络学习任务中广泛应用，旨在传递标签或结构知识。目前，评估graph domain adaptation模型存在两大限制。一方面，它们主要在特定的跨网络节点分类任务上测试，剩下的任务（如边级和图级）尚未得到足够的探索。另一方面，它们主要在有限的场景中测试，如社交网络或引用网络，缺乏模型在更加丰富的场景中的验证。为了全面评估模型的实用性，我们提出了OpenGDA benchmark。它提供了各种类型的任务（节点、边、图）的充分预处理和统一的数据集，来自多样化的场景，包括网络信息系统、城市系统和自然系统。此外，它集成了当前最佳的模型和标准化的管道。总之，OpenGDA提供了用户友好、可扩展和可重现的 benchmark，用于评估graph domain adaptation模型。 benchmark实验显示出应用GDA模型到实际应用场景的挑战，并可能提供未来研究的指导。作为一个emerging项目，OpenGDA将在将来不断更新数据集和模型，可以在https://github.com/Skyorca/OpenGDA上访问。
</details></li>
</ul>
<hr>
<h2 id="Tri-MipRF-Tri-Mip-Representation-for-Efficient-Anti-Aliasing-Neural-Radiance-Fields"><a href="#Tri-MipRF-Tri-Mip-Representation-for-Efficient-Anti-Aliasing-Neural-Radiance-Fields" class="headerlink" title="Tri-MipRF: Tri-Mip Representation for Efficient Anti-Aliasing Neural Radiance Fields"></a>Tri-MipRF: Tri-Mip Representation for Efficient Anti-Aliasing Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11335">http://arxiv.org/abs/2307.11335</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wbhu/Tri-MipRF">https://github.com/wbhu/Tri-MipRF</a></li>
<li>paper_authors: Wenbo Hu, Yuling Wang, Lin Ma, Bangbang Yang, Lin Gao, Xiao Liu, Yuewen Ma</li>
<li>for: 提高 NeRF 的质量和效率，解决在训练时间和渲染质量之间的膝盘。</li>
<li>methods: 提出了一种新的 Tri-Mip 编码方法，通过三个 Mutli-scale Invariance Points (Mip) 维度空间的因子化，实现高效的 3D 区域抽象和渲染。</li>
<li>results: 实验表明，提出的方法可以具有 state-of-the-art 的渲染质量和重建速度，同时减少模型大小25%，比 Instant-ngp 更高效。<details>
<summary>Abstract</summary>
Despite the tremendous progress in neural radiance fields (NeRF), we still face a dilemma of the trade-off between quality and efficiency, e.g., MipNeRF presents fine-detailed and anti-aliased renderings but takes days for training, while Instant-ngp can accomplish the reconstruction in a few minutes but suffers from blurring or aliasing when rendering at various distances or resolutions due to ignoring the sampling area. To this end, we propose a novel Tri-Mip encoding that enables both instant reconstruction and anti-aliased high-fidelity rendering for neural radiance fields. The key is to factorize the pre-filtered 3D feature spaces in three orthogonal mipmaps. In this way, we can efficiently perform 3D area sampling by taking advantage of 2D pre-filtered feature maps, which significantly elevates the rendering quality without sacrificing efficiency. To cope with the novel Tri-Mip representation, we propose a cone-casting rendering technique to efficiently sample anti-aliased 3D features with the Tri-Mip encoding considering both pixel imaging and observing distance. Extensive experiments on both synthetic and real-world datasets demonstrate our method achieves state-of-the-art rendering quality and reconstruction speed while maintaining a compact representation that reduces 25% model size compared against Instant-ngp.
</details>
<details>
<summary>摘要</summary>
尽管神经辐射场（NeRF）已经取得了很大的进步，但我们仍然面临质量和效率之间的负担选择问题，例如MipNeRF可以提供细节够好和无折扑的渲染结果，但是训练时间需要几天，而Instant-ngp可以快速完成重建，但是在不同的距离或分辨率下，因为忽略采样区域而导致折扑或抖音。为了解决这个问题，我们提出了一种新的Tri-Mip编码方法，它可以同时实现快速重建和高精度渲染。关键在于将预处理的3D特征空间分解成三个正交的Mipmap。这样，我们可以高效地进行3D采样，利用2D预处理的特征图，从而提高渲染质量无需牺牲效率。为了处理Tri-Mip表示，我们提出了一种锥体投射渲染技术，可以高效地采样折扑3D特征，考虑到像素捕捷和观察距离。我们对both synthetic和实际数据进行了广泛的实验，并证明我们的方法可以实现state-of-the-art的渲染质量和重建速度，同时保持一个减少25%的模型大小。
</details></li>
</ul>
<hr>
<h2 id="Analysis-of-Elephant-Movement-in-Sub-Saharan-Africa-Ecological-Climatic-and-Conservation-Perspectives"><a href="#Analysis-of-Elephant-Movement-in-Sub-Saharan-Africa-Ecological-Climatic-and-Conservation-Perspectives" class="headerlink" title="Analysis of Elephant Movement in Sub-Saharan Africa: Ecological, Climatic, and Conservation Perspectives"></a>Analysis of Elephant Movement in Sub-Saharan Africa: Ecological, Climatic, and Conservation Perspectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11325">http://arxiv.org/abs/2307.11325</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew Hines, Gregory Glatzer, Shreya Ghosh, Prasenjit Mitra</li>
<li>for: 这项研究旨在为非洲亚洲 elephant 的生态和保护策略提供基础数据。</li>
<li>methods: 这项研究使用分析方法来揭示非洲亚洲 elephant 的移动模式，强调关键生态因素，如季节变化和降水模式。</li>
<li>results: 研究发现 elephant 的移动模式受到季节变化和降水模式的影响，并提供了一个涵盖了这些因素的整体视图。这些结果可以用于预测未来 elephant 的移动模式，并为生态和保护策略提供依据。<details>
<summary>Abstract</summary>
The interaction between elephants and their environment has profound implications for both ecology and conservation strategies. This study presents an analytical approach to decipher the intricate patterns of elephant movement in Sub-Saharan Africa, concentrating on key ecological drivers such as seasonal variations and rainfall patterns. Despite the complexities surrounding these influential factors, our analysis provides a holistic view of elephant migratory behavior in the context of the dynamic African landscape. Our comprehensive approach enables us to predict the potential impact of these ecological determinants on elephant migration, a critical step in establishing informed conservation strategies. This projection is particularly crucial given the impacts of global climate change on seasonal and rainfall patterns, which could substantially influence elephant movements in the future. The findings of our work aim to not only advance the understanding of movement ecology but also foster a sustainable coexistence of humans and elephants in Sub-Saharan Africa. By predicting potential elephant routes, our work can inform strategies to minimize human-elephant conflict, effectively manage land use, and enhance anti-poaching efforts. This research underscores the importance of integrating movement ecology and climatic variables for effective wildlife management and conservation planning.
</details>
<details>
<summary>摘要</summary>
Elephant and its environment interaction has profound ecological and conservation implications. This study uses an analytical approach to decipher the complex patterns of elephant movement in Sub-Saharan Africa, focusing on key ecological drivers such as seasonal variations and rainfall patterns. Our comprehensive approach provides a holistic view of elephant migratory behavior in the dynamic African landscape, allowing us to predict the potential impact of ecological determinants on elephant migration. This projection is crucial in establishing informed conservation strategies, especially in light of the impacts of global climate change on seasonal and rainfall patterns. Our findings aim to advance the understanding of movement ecology and foster sustainable human-elephant coexistence in Sub-Saharan Africa. By predicting potential elephant routes, our work can inform strategies to minimize human-elephant conflict, effectively manage land use, and enhance anti-poaching efforts. This research highlights the importance of integrating movement ecology and climatic variables for effective wildlife management and conservation planning.
</details></li>
</ul>
<hr>
<h2 id="HVDetFusion-A-Simple-and-Robust-Camera-Radar-Fusion-Framework"><a href="#HVDetFusion-A-Simple-and-Robust-Camera-Radar-Fusion-Framework" class="headerlink" title="HVDetFusion: A Simple and Robust Camera-Radar Fusion Framework"></a>HVDetFusion: A Simple and Robust Camera-Radar Fusion Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11323">http://arxiv.org/abs/2307.11323</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hvxlab/hvdetfusion">https://github.com/hvxlab/hvdetfusion</a></li>
<li>paper_authors: Kai Lei, Zhan Chen, Shuman Jia, Xiaoteng Zhang</li>
<li>for: 提出了一种新的探测算法 called HVDetFusion，用于实时3D对象检测。</li>
<li>methods: 该算法不仅支持纯摄像头数据输入，还可以进行摄像头和雷达数据的融合输入。 modify了Bevdet4D框架，以提高探测效果和减少探测时间。</li>
<li>results: HVDetFusion实现了在nuScenes测试集上的新状态计时性3D对象检测精度记录67.4%，比其他摄像头雷达3D对象检测器更高。<details>
<summary>Abstract</summary>
In the field of autonomous driving, 3D object detection is a very important perception module. Although the current SOTA algorithm combines Camera and Lidar sensors, limited by the high price of Lidar, the current mainstream landing schemes are pure Camera sensors or Camera+Radar sensors. In this study, we propose a new detection algorithm called HVDetFusion, which is a multi-modal detection algorithm that not only supports pure camera data as input for detection, but also can perform fusion input of radar data and camera data. The camera stream does not depend on the input of Radar data, thus addressing the downside of previous methods. In the pure camera stream, we modify the framework of Bevdet4D for better perception and more efficient inference, and this stream has the whole 3D detection output. Further, to incorporate the benefits of Radar signals, we use the prior information of different object positions to filter the false positive information of the original radar data, according to the positioning information and radial velocity information recorded by the radar sensors to supplement and fuse the BEV features generated by the original camera data, and the effect is further improved in the process of fusion training. Finally, HVDetFusion achieves the new state-of-the-art 67.4\% NDS on the challenging nuScenes test set among all camera-radar 3D object detectors. The code is available at https://github.com/HVXLab/HVDetFusion
</details>
<details>
<summary>摘要</summary>
在自动驾驶领域，3D物体检测是一个非常重要的感知模块。当前最佳算法 combining 摄像头和雷达感知器，受到雷达价格高昂的限制，目前主流实施方案是纯摄像头感知器或摄像头+雷达感知器。在本研究中，我们提出了一种新的检测算法called HVDetFusion，该算法不仅支持纯摄像头数据作为输入进行检测，而且可以进行雷达数据和摄像头数据的融合输入。摄像头流不依赖于雷达数据输入，因此解决了之前方法的缺点。在纯摄像头流中，我们修改了 BEvdet4D 框架，以提高感知效果和更加有效的推理，并且该流有整个3D检测输出。此外，为了利用雷达信号的优势，我们使用雷达感知器记录的位置信息和径向速度信息来筛选和融合 BEV 特征，并在融合训练过程中进一步提高效果。最终，HVDetFusion 实现了在 NuScenes 测试集上的新状态最佳67.4% NDS，比其他所有摄像头-雷达 3D 物体检测器更高。代码可以在 GitHub 上找到：https://github.com/HVXLab/HVDetFusion。
</details></li>
</ul>
<hr>
<h2 id="How-to-Tidy-Up-a-Table-Fusing-Visual-and-Semantic-Commonsense-Reasoning-for-Robotic-Tasks-with-Vague-Objectives"><a href="#How-to-Tidy-Up-a-Table-Fusing-Visual-and-Semantic-Commonsense-Reasoning-for-Robotic-Tasks-with-Vague-Objectives" class="headerlink" title="How to Tidy Up a Table: Fusing Visual and Semantic Commonsense Reasoning for Robotic Tasks with Vague Objectives"></a>How to Tidy Up a Table: Fusing Visual and Semantic Commonsense Reasoning for Robotic Tasks with Vague Objectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11319">http://arxiv.org/abs/2307.11319</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiqing Xu, David Hsu<br>for:This paper aims to solve the problem of tidying a messy table using a simple approach that combines semantic and visual tidiness.methods:The proposed method uses a lightweight, image-based tidiness score function to ground the semantically tidy policy of Large Language Models (LLMs) to achieve visual tidiness. The tidiness score is trained using synthetic data gathered using random walks from a few tidy configurations.results:The proposed pipeline can be applied to unseen objects and complex 3D arrangements, and the empirical results show that it is effective in achieving both semantic and visual tidiness.<details>
<summary>Abstract</summary>
Vague objectives in many real-life scenarios pose long-standing challenges for robotics, as defining rules, rewards, or constraints for optimization is difficult. Tasks like tidying a messy table may appear simple for humans, but articulating the criteria for tidiness is complex due to the ambiguity and flexibility in commonsense reasoning. Recent advancement in Large Language Models (LLMs) offers us an opportunity to reason over these vague objectives: learned from extensive human data, LLMs capture meaningful common sense about human behavior. However, as LLMs are trained solely on language input, they may struggle with robotic tasks due to their limited capacity to account for perception and low-level controls. In this work, we propose a simple approach to solve the task of table tidying, an example of robotic tasks with vague objectives. Specifically, the task of tidying a table involves not just clustering objects by type and functionality for semantic tidiness but also considering spatial-visual relations of objects for a visually pleasing arrangement, termed as visual tidiness. We propose to learn a lightweight, image-based tidiness score function to ground the semantically tidy policy of LLMs to achieve visual tidiness. We innovatively train the tidiness score using synthetic data gathered using random walks from a few tidy configurations. Such trajectories naturally encode the order of tidiness, thereby eliminating the need for laborious and expensive human demonstrations. Our empirical results show that our pipeline can be applied to unseen objects and complex 3D arrangements.
</details>
<details>
<summary>摘要</summary>
在实际应用中，多元目标问题常常对机器人学习 pose 长期的挑战，因为定义规则、奖励或约束 для优化是困难的。例如，人类可以轻松地整理一个混乱的桌子，但是明确定义整理的标准是复杂的，因为普遍的常识理解有很多弹性和模糊性。现有的大量语言模型（LLMs）可以让我们通过掌握人类的常识来进行整理。然而，由于 LLMS 仅从语言输入学习，它们可能会对机器人任务产生困难，因为它们可能无法考虑感知和低阶控制。在这个工作中，我们提出了一个简单的方法来解决桌子整理任务，这是一个机器人任务中的普遍目标。具体来说，桌子整理任务不仅需要根据类型和功能将物品集中，而且还需要考虑物品之间的空间视觉关系，以获得一个美观的安排，称为视觉整理。我们提出了一个轻量级的图像基于整理分数函数，以降低 LLMS 的Semantic 整理政策来实现视觉整理。我们创新地使用随机步行生成的Synthetic 数据进行训练。这些数据自然地嵌入了整理顺序，因此没有需要传递大量和昂费的人类示例。我们的实验结果显示，我们的管道可以应用于未见过的物品和复杂的3D排列。
</details></li>
</ul>
<hr>
<h2 id="XLDA-Linear-Discriminant-Analysis-for-Scaling-Continual-Learning-to-Extreme-Classification-at-the-Edge"><a href="#XLDA-Linear-Discriminant-Analysis-for-Scaling-Continual-Learning-to-Extreme-Classification-at-the-Edge" class="headerlink" title="XLDA: Linear Discriminant Analysis for Scaling Continual Learning to Extreme Classification at the Edge"></a>XLDA: Linear Discriminant Analysis for Scaling Continual Learning to Extreme Classification at the Edge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11317">http://arxiv.org/abs/2307.11317</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karan Shah, Vishruth Veerendranath, Anushka Hebbar, Raghavendra Bhat</li>
<li>for: 这个论文是为了证明流式线性减分分析（LDA）在 Edge 端部署中的可行性，特别是在极端分类场景下。</li>
<li>methods: 这篇论文提出了一种名为 XLDA 的框架，它将 LDA 分类器与 FC 层相结合，并在 Edge 端部署中进行了优化。</li>
<li>results: 论文表明，XLDA 可以在极端分类场景下实现高效的训练和推断，并且可以在 Edge 端部署中实现更高的速度。例如，在 AliProducts 和 Google Landmarks V2 等极端 datasets 上，XLDA 可以实现 Up to 42x 的训练速度减少和 Up to 5x 的推断速度减少。<details>
<summary>Abstract</summary>
Streaming Linear Discriminant Analysis (LDA) while proven in Class-incremental Learning deployments at the edge with limited classes (upto 1000), has not been proven for deployment in extreme classification scenarios. In this paper, we present: (a) XLDA, a framework for Class-IL in edge deployment where LDA classifier is proven to be equivalent to FC layer including in extreme classification scenarios, and (b) optimizations to enable XLDA-based training and inference for edge deployment where there is a constraint on available compute resources. We show up to 42x speed up using a batched training approach and up to 5x inference speedup with nearest neighbor search on extreme datasets like AliProducts (50k classes) and Google Landmarks V2 (81k classes)
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate into Simplified ChineseStreaming Linear Discriminant Analysis (LDA) 已经在端点扩展学习中证明了在Edge环境下进行分类学习，但是它在极端分类场景中没有得到证明。在这篇论文中，我们提出了以下两点：(a) XLDA，一种基于Class-IL的Edge部署框架，其中LDA分类器在极端分类场景中与FC层相等，并且可以在有限的计算资源 constraints下进行训练和推理。(b) 优化XLDA在Edge部署中进行训练和推理，以提高性能。我们在极端分类数据集如AliProducts（50k类）和Google Landmarks V2（81k类）上实现了最多42倍的训练速度减少和最多5倍的推理速度减少。Note: "Class-IL" refers to "class-incremental learning", which means learning new classes while preserving the knowledge of previous classes. "Edge" refers to the edge device, such as a smartphone or a smart speaker, where the model is deployed and executed.
</details></li>
</ul>
<hr>
<h2 id="DPM-OT-A-New-Diffusion-Probabilistic-Model-Based-on-Optimal-Transport"><a href="#DPM-OT-A-New-Diffusion-Probabilistic-Model-Based-on-Optimal-Transport" class="headerlink" title="DPM-OT: A New Diffusion Probabilistic Model Based on Optimal Transport"></a>DPM-OT: A New Diffusion Probabilistic Model Based on Optimal Transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11308">http://arxiv.org/abs/2307.11308</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cognaclee/dpm-ot">https://github.com/cognaclee/dpm-ot</a></li>
<li>paper_authors: Zezeng Li, ShengHao Li, Zhanpeng Wang, Na Lei, Zhongxuan Luo, Xianfeng Gu</li>
<li>For: 这个研究目的是设计一个高速的Diffusion Probabilistic Model（DPM）样本，以实现高品质的生成模型。* Methods: 这个方法使用了知识传播或是调整含扩散方程来设计快速的DPM样本，并将问题转化为一个最佳运输问题（OT），以获得一个直接表达的OT对映，从而产生高品质的样本。* Results: 实验结果显示，DPM-OT可以在约10个函数评估下产生高品质的样本，并有较好的速度和品质（FID和模式混合），并且有一定的理论保证。<details>
<summary>Abstract</summary>
Sampling from diffusion probabilistic models (DPMs) can be viewed as a piecewise distribution transformation, which generally requires hundreds or thousands of steps of the inverse diffusion trajectory to get a high-quality image. Recent progress in designing fast samplers for DPMs achieves a trade-off between sampling speed and sample quality by knowledge distillation or adjusting the variance schedule or the denoising equation. However, it can't be optimal in both aspects and often suffer from mode mixture in short steps. To tackle this problem, we innovatively regard inverse diffusion as an optimal transport (OT) problem between latents at different stages and propose the DPM-OT, a unified learning framework for fast DPMs with a direct expressway represented by OT map, which can generate high-quality samples within around 10 function evaluations. By calculating the semi-discrete optimal transport map between the data latents and the white noise, we obtain an expressway from the prior distribution to the data distribution, while significantly alleviating the problem of mode mixture. In addition, we give the error bound of the proposed method, which theoretically guarantees the stability of the algorithm. Extensive experiments validate the effectiveness and advantages of DPM-OT in terms of speed and quality (FID and mode mixture), thus representing an efficient solution for generative modeling. Source codes are available at https://github.com/cognaclee/DPM-OT
</details>
<details>
<summary>摘要</summary>
diffusion probabilistic models (DPMs) 的抽样可以视为一种分割分布变换，通常需要百个或千个反射游程来获得高质量图像。现代设计快速抽样器的进步可以在抽样速度和样本质量之间实现一种平衡，通过知识传授或调整方差表单或杂噪方程。然而，这些方法通常无法同时优化两个方面，常常会出现短步内模杂的问题。为解决这个问题，我们创新地将反射 diffusion 视为一种最优运输（OT）问题，并提出了DPM-OT，一种统一学习框架，可以在约10个函数评估中生成高质量样本。通过计算 semi-精确的最优运输图表 между数据射频和白噪，我们获得了一条从先验分布到数据分布的直达表达，同时减轻了内模杂问题。此外，我们还给出了提案的方法的误差 bound，从而理论上保证了算法的稳定性。广泛的实验证明了 DPM-OT 的有效性和优势（FID和内模杂），因此表现为一种高效的生成模型解决方案。相关代码可以在 https://github.com/cognaclee/DPM-OT 上获得。
</details></li>
</ul>
<hr>
<h2 id="Kernelized-Offline-Contextual-Dueling-Bandits"><a href="#Kernelized-Offline-Contextual-Dueling-Bandits" class="headerlink" title="Kernelized Offline Contextual Dueling Bandits"></a>Kernelized Offline Contextual Dueling Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11288">http://arxiv.org/abs/2307.11288</a></li>
<li>repo_url: None</li>
<li>paper_authors: Viraj Mehta, Ojash Neopane, Vikramjeet Das, Sen Lin, Jeff Schneider, Willie Neiswanger</li>
<li>for: 这个论文是为了解决 preference-based 反馈是不可靠的应用问题，例如在大语言模型中的 reinforcement learning 中。</li>
<li>methods: 这个论文使用了上下文选择来提高效率，并提出了一种上下文汇投票banditSetting，并采用了上限 confidence bound 算法。</li>
<li>results: 论文实验表明，该方法可以比使用均匀随机上下文更高效地识别好策略，并且提供了一个 regret bound。<details>
<summary>Abstract</summary>
Preference-based feedback is important for many applications where direct evaluation of a reward function is not feasible. A notable recent example arises in reinforcement learning from human feedback on large language models. For many of these applications, the cost of acquiring the human feedback can be substantial or even prohibitive. In this work, we take advantage of the fact that often the agent can choose contexts at which to obtain human feedback in order to most efficiently identify a good policy, and introduce the offline contextual dueling bandit setting. We give an upper-confidence-bound style algorithm for this setting and prove a regret bound. We also give empirical confirmation that this method outperforms a similar strategy that uses uniformly sampled contexts.
</details>
<details>
<summary>摘要</summary>
preference-based 反馈是许多应用程序中非常重要的，特别是当直接评估奖函数不可能时。一个最近的例子来自于人工智能语言模型的强化学习，其中人类反馈的成本可能很高或甚至是不可能的。在这项工作中，我们利用了代理人可以选择收集人类反馈的上下文，以便最有效地确定一个好策略，并引入了线上上下文对抗式带宽设定。我们提供了上限信息级别风格的算法，并证明了一个违和 bound。我们还提供了实际证明，该方法在相似策略中比使用均匀样本上下文的方法表现更好。
</details></li>
</ul>
<hr>
<h2 id="Eliminating-Unintended-Stable-Fixpoints-for-Hybrid-Reasoning-Systems"><a href="#Eliminating-Unintended-Stable-Fixpoints-for-Hybrid-Reasoning-Systems" class="headerlink" title="Eliminating Unintended Stable Fixpoints for Hybrid Reasoning Systems"></a>Eliminating Unintended Stable Fixpoints for Hybrid Reasoning Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11286">http://arxiv.org/abs/2307.11286</a></li>
<li>repo_url: None</li>
<li>paper_authors: Spencer Killen, Jia-Huai You</li>
<li>for: 这篇论文旨在描述一种基于 Approximation Fixpoint Theory（AFT）的方法，用于表达非卷积 semantics。</li>
<li>methods: 该方法使用了 traditional AFT 理论，并利用之前计算的上限来更 preciselly 表达 semantics。</li>
<li>results: 该方法可以应用于 hybrid MKNF（MINIMAL KNOWLEDGE AND NEGATION AS FAILURE）知识库，并可以提高现有 approximator 的精度。<details>
<summary>Abstract</summary>
A wide variety of nonmonotonic semantics can be expressed as approximators defined under AFT (Approximation Fixpoint Theory). Using traditional AFT theory, it is not possible to define approximators that rely on information computed in previous iterations of stable revision. However, this information is rich for semantics that incorporate classical negation into nonmonotonic reasoning. In this work, we introduce a methodology resembling AFT that can utilize priorly computed upper bounds to more precisely capture semantics. We demonstrate our framework's applicability to hybrid MKNF (minimal knowledge and negation as failure) knowledge bases by extending the state-of-the-art approximator.
</details>
<details>
<summary>摘要</summary>
可以用AFT（近似稳定点理论）表达各种不寻常 semantics。传统AFT理论中无法定义基于之前稳定修订中计算的近似器，但这些信息很有用于包含古希腊否定的非 monotonic  semantics。在这种工作中，我们提出了一种类似AFT的方法，可以利用先前计算的上限来更准确地捕捉 semantics。我们在hybrid MKNF（最小知识和否定为失败）知识库中应用了这种框架，并将现有最佳近似器扩展。
</details></li>
</ul>
<hr>
<h2 id="Nature-of-Intelligence"><a href="#Nature-of-Intelligence" class="headerlink" title="Nature of Intelligence"></a>Nature of Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11114">http://arxiv.org/abs/2307.11114</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nature-of-code/NOC-S17-2-Intelligence-Learning">https://github.com/nature-of-code/NOC-S17-2-Intelligence-Learning</a></li>
<li>paper_authors: Barco Jie You</li>
<li>for: 这篇论文旨在探讨人类智能的本质和人工智能（AI）如何实现人类水平的智能任务。</li>
<li>methods: 该论文使用了深度神经网络模型来学习数据表示和提高认知领域的状态。</li>
<li>results: 该论文提出了一种减少系统熵的数学函数过程的假设，并建立了语言、无意识和意识的数学模型，预测了神经科学和AI工程中的证据。此外，该论文还认为宇宙熵是保守的，智能可以通过物理或信息连接来减少熵，并且存在更高级别的智能。<details>
<summary>Abstract</summary>
The human brain is the substrate for human intelligence. By simulating the human brain, artificial intelligence builds computational models that have learning capabilities and perform intelligent tasks approaching the human level. Deep neural networks consist of multiple computation layers to learn representations of data and improve the state-of-the-art in many recognition domains. However, the essence of intelligence commonly represented by both humans and AI is unknown. Here, we show that the nature of intelligence is a series of mathematically functional processes that minimize system entropy by establishing functional relationships between datasets over space and time. Humans and AI have achieved intelligence by implementing these entropy-reducing processes in a reinforced manner that consumes energy. With this hypothesis, we establish mathematical models of language, unconsciousness and consciousness, predicting the evidence to be found by neuroscience and achieved by AI engineering. Furthermore, a conclusion is made that the total entropy of the universe is conservative, and intelligence counters the spontaneous processes to decrease entropy by physically or informationally connecting datasets that originally exist in the universe but are separated across space and time. This essay should be a starting point for a deeper understanding of the universe and us as human beings and for achieving sophisticated AI models that are tantamount to human intelligence or even superior. Furthermore, this essay argues that more advanced intelligence than humans should exist if only it reduces entropy in a more efficient energy-consuming way.
</details>
<details>
<summary>摘要</summary>
人类大脑是人类智能的substrate。通过模拟人类大脑，人工智能建立了计算模型，这些模型具有学习能力并完成人类水平的智能任务。深度神经网络由多层计算层学习数据表示和提高多种识别领域的状态。然而，人类智能和AI智能的本质仍然未知。在这里，我们表明了智能的本质是一系列数学函数过程，减少系统熵，通过在空间和时间上建立函数关系来实现。人类和AI都通过实施这些熵减少过程来减少能量。根据这个假设，我们建立了语言、无意识和意识的数学模型，预测了神经科学和AI工程的证据，并达到了人类智能水平。此外，我们结论是：宇宙熵总是保守的，智能通过物理或信息方式连接宇宙中分布的数据点，减少熵。这篇文章应该是人类智能和宇宙的深入理解的开端，以及实现人类智能水平或更高的AI模型的起点。此外，这篇文章还提出了更高级别的智能应该存在，即能够更有效率地减少熵，即使消耗更多的能量。
</details></li>
</ul>
<hr>
<h2 id="Joint-one-sided-synthetic-unpaired-image-translation-and-segmentation-for-colorectal-cancer-prevention"><a href="#Joint-one-sided-synthetic-unpaired-image-translation-and-segmentation-for-colorectal-cancer-prevention" class="headerlink" title="Joint one-sided synthetic unpaired image translation and segmentation for colorectal cancer prevention"></a>Joint one-sided synthetic unpaired image translation and segmentation for colorectal cancer prevention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11253">http://arxiv.org/abs/2307.11253</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enric Moreu, Eric Arazo, Kevin McGuinness, Noel E. O’Connor</li>
<li>for: 提高医疗影像分割的效果，解决 Privacy 问题和标准化问题</li>
<li>methods: 使用3D技术和生成对抗网络生成实际的医疗影像，并将分割模型和生成模型一起训练</li>
<li>results: 在五个真实的肠结肿分割数据集上取得了更好的效果，比其他内存占用量较高的图像翻译方法更加快速和效果更好，只需一个真实的图像和零个真实的注释。同时也发布了一个完全synthetic的数据集Synth-Colon，包含20000个实际的肠影像和更多的深度和3D几何信息：<a target="_blank" rel="noopener" href="https://enric1994.github.io/synth-colon">https://enric1994.github.io/synth-colon</a><details>
<summary>Abstract</summary>
Deep learning has shown excellent performance in analysing medical images. However, datasets are difficult to obtain due privacy issues, standardization problems, and lack of annotations. We address these problems by producing realistic synthetic images using a combination of 3D technologies and generative adversarial networks. We propose CUT-seg, a joint training where a segmentation model and a generative model are jointly trained to produce realistic images while learning to segment polyps. We take advantage of recent one-sided translation models because they use significantly less memory, allowing us to add a segmentation model in the training loop. CUT-seg performs better, is computationally less expensive, and requires less real images than other memory-intensive image translation approaches that require two stage training. Promising results are achieved on five real polyp segmentation datasets using only one real image and zero real annotations. As a part of this study we release Synth-Colon, an entirely synthetic dataset that includes 20000 realistic colon images and additional details about depth and 3D geometry: https://enric1994.github.io/synth-colon
</details>
<details>
<summary>摘要</summary>
深度学习在医疗影像分析方面表现出色，但获取数据集却存在隐私问题、标准化问题和缺乏标注问题。我们通过使用3D技术和生成敌对网络生成真实的 sintetic图像来解决这些问题。我们提议CUT-seg，一种同时训练分割模型和生成模型，以生成真实的图像并学习识别肿瘤。我们利用最近的一侧翻译模型，因为它们使用较少的内存，因此可以在训练 loop中添加分割模型。CUT-seg在计算机资源上更加经济，需要更少的真实图像，并且在其他内存昂贵的图像翻译方法上表现出色。我们在五个真实肿瘤分割数据集上获得了出色的结果，只用一张真实图像和零个真实注释。作为这项研究的一部分，我们发布了Synth-Colon entirely sintetic数据集，包括20000个真实的colon图像以及其他的深度和3D几何信息：https://enric1994.github.io/synth-colon。
</details></li>
</ul>
<hr>
<h2 id="On-Sensor-Data-Filtering-using-Neuromorphic-Computing-for-High-Energy-Physics-Experiments"><a href="#On-Sensor-Data-Filtering-using-Neuromorphic-Computing-for-High-Energy-Physics-Experiments" class="headerlink" title="On-Sensor Data Filtering using Neuromorphic Computing for High Energy Physics Experiments"></a>On-Sensor Data Filtering using Neuromorphic Computing for High Energy Physics Experiments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11242">http://arxiv.org/abs/2307.11242</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shruti R. Kulkarni, Aaron Young, Prasanna Date, Narasinga Rao Miniskar, Jeffrey S. Vetter, Farah Fahim, Benjamin Parpillon, Jennet Dickinson, Nhan Tran, Jieun Yoo, Corrinne Mills, Morris Swartz, Petar Maksimovic, Catherine D. Schuman, Alice Bean</li>
<li>for: 这个研究探讨了基于神经omorphic计算的脉冲神经网络（SNN）模型，用于高能物理实验中的传感器数据筛选。</li>
<li>methods: 我们提出了一种压缩型神经omorphic模型，将传感器数据基于粒子的横向势能筛选出有用信息，以降低数据流向下游电子设备的吞吐量。</li>
<li>results: 我们的结果显示，使用进化算法和优化的超参数，SNN可以达到约91%的信号效率，并且减少了大约一半的参数量，相比深度神经网络。<details>
<summary>Abstract</summary>
This work describes the investigation of neuromorphic computing-based spiking neural network (SNN) models used to filter data from sensor electronics in high energy physics experiments conducted at the High Luminosity Large Hadron Collider. We present our approach for developing a compact neuromorphic model that filters out the sensor data based on the particle's transverse momentum with the goal of reducing the amount of data being sent to the downstream electronics. The incoming charge waveforms are converted to streams of binary-valued events, which are then processed by the SNN. We present our insights on the various system design choices - from data encoding to optimal hyperparameters of the training algorithm - for an accurate and compact SNN optimized for hardware deployment. Our results show that an SNN trained with an evolutionary algorithm and an optimized set of hyperparameters obtains a signal efficiency of about 91% with nearly half as many parameters as a deep neural network.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Jina-Embeddings-A-Novel-Set-of-High-Performance-Sentence-Embedding-Models"><a href="#Jina-Embeddings-A-Novel-Set-of-High-Performance-Sentence-Embedding-Models" class="headerlink" title="Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models"></a>Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11224">http://arxiv.org/abs/2307.11224</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Günther, Louis Milliken, Jonathan Geuter, Georgios Mastrapas, Bo Wang, Han Xiao</li>
<li>for: 本研究旨在开发高性能的句子嵌入模型，以捕捉文本中各种输入的 semantics。</li>
<li>methods: 本研究使用高质量的对比和 triplet dataset 进行模型训练，并强调数据清洁的重要性。</li>
<li>results: 研究结果表明，Jina Embeddings 模型在 dense retrieval 和 semantic textual similarity 等应用中具有高性能。<details>
<summary>Abstract</summary>
Jina Embeddings constitutes a set of high-performance sentence embedding models adept at translating various textual inputs into numerical representations, thereby capturing the semantic essence of the text. The models excel in applications such as dense retrieval and semantic textual similarity. This paper details the development of Jina Embeddings, starting with the creation of high-quality pairwise and triplet datasets. It underlines the crucial role of data cleaning in dataset preparation, gives in-depth insights into the model training process, and concludes with a comprehensive performance evaluation using the Massive Textual Embedding Benchmark (MTEB). To increase the model's awareness of negations, we constructed a novel training and evaluation dataset of negated and non-negated statements, which we make publicly available to the community.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-Ontologically-Grounded-and-Language-Agnostic-Knowledge-Graphs"><a href="#Towards-Ontologically-Grounded-and-Language-Agnostic-Knowledge-Graphs" class="headerlink" title="Towards Ontologically Grounded and Language-Agnostic Knowledge Graphs"></a>Towards Ontologically Grounded and Language-Agnostic Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11206">http://arxiv.org/abs/2307.11206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Walid S. Saba</li>
<li>for: 提高知识 graphs (KGs) 的更新和融合问题。</li>
<li>methods: 通过具体对抽象对象的封装和承认概念和类型之间的 ontological 分辨率，实现语言和领域独立的表示。</li>
<li>results: 提高 KGs 的融合和更新问题。<details>
<summary>Abstract</summary>
Knowledge graphs (KGs) have become the standard technology for the representation of factual information in applications such as recommendation engines, search, and question-answering systems. However, the continual updating of KGs, as well as the integration of KGs from different domains and KGs in different languages, remains to be a major challenge. What we suggest here is that by a reification of abstract objects and by acknowledging the ontological distinction between concepts and types, we arrive at an ontologically grounded and language-agnostic representation that can alleviate the difficulties in KG integration.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Applying-QNLP-to-sentiment-analysis-in-finance"><a href="#Applying-QNLP-to-sentiment-analysis-in-finance" class="headerlink" title="Applying QNLP to sentiment analysis in finance"></a>Applying QNLP to sentiment analysis in finance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11788">http://arxiv.org/abs/2307.11788</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ichrist97/qnlp_finance">https://github.com/ichrist97/qnlp_finance</a></li>
<li>paper_authors: Jonas Stein, Ivo Christ, Nicolas Kraus, Maximilian Balthasar Mansky, Robert Müller, Claudia Linnhoff-Popien</li>
<li>for: 针对金融领域的情感分析问题</li>
<li>methods: 使用DisCoCat和Quantum-Enhanced Long Short-Term Memory（QLSTM）两种中心方法进行实验</li>
<li>results: QLSTM可以比DisCoCat更快速地训练，并且达到类比类型软件实现的ResultsHere’s a breakdown of each point:</li>
<li>for: The paper is written for the problem of sentiment analysis in the financial domain.</li>
<li>methods: The paper explores the practical applicability of two central approaches, DisCoCat and QLSTM, to the problem of sentiment analysis in finance.</li>
<li>results: The paper finds that QLSTMs can be trained substantially faster than DisCoCat while also achieving close to classical results for their available software implementations.<details>
<summary>Abstract</summary>
As an application domain where the slightest qualitative improvements can yield immense value, finance is a promising candidate for early quantum advantage. Focusing on the rapidly advancing field of Quantum Natural Language Processing (QNLP), we explore the practical applicability of the two central approaches DisCoCat and Quantum-Enhanced Long Short-Term Memory (QLSTM) to the problem of sentiment analysis in finance. Utilizing a novel ChatGPT-based data generation approach, we conduct a case study with more than 1000 realistic sentences and find that QLSTMs can be trained substantially faster than DisCoCat while also achieving close to classical results for their available software implementations.
</details>
<details>
<summary>摘要</summary>
当作应用领域，金融领域具有最小量质量改善可以带来巨大价值，因此它是早期量子优势的潜在候选者。我们在快速进步的量子自然语言处理（QNLP）领域中专注于两个中心方法：DisCoCat和量子增强长短时间记忆（QLSTM），以探索金融领域中情感分析的实际应用。我们运用一种基于ChatGPT的新型数据生成方法，进行了超过1000个真实的句子的 caso study，发现QLSTM可以训练得比DisCoCat更快，并且可以在可用的软件实现中 achieved close to classical results。
</details></li>
</ul>
<hr>
<h2 id="Exploring-reinforcement-learning-techniques-for-discrete-and-continuous-control-tasks-in-the-MuJoCo-environment"><a href="#Exploring-reinforcement-learning-techniques-for-discrete-and-continuous-control-tasks-in-the-MuJoCo-environment" class="headerlink" title="Exploring reinforcement learning techniques for discrete and continuous control tasks in the MuJoCo environment"></a>Exploring reinforcement learning techniques for discrete and continuous control tasks in the MuJoCo environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11166">http://arxiv.org/abs/2307.11166</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vaddadi Sai Rahul, Debajyoti Chakraborty</li>
<li>for: 研究了 continuous control 环境下的值基方法和深度策略 gradient 方法的比较，以及如何在这些方法基础之上进行 hyper-parameter 优化。</li>
<li>methods: 使用了 MuJoCo  física simulator 和 Q-learning、SARSA 和 DDPG 等方法进行研究。</li>
<li>results: Q-learning 在大量话数episode中表现出色，但是 DDPG 在一些话数episode中表现出色，而且可以在减少时间和资源的情况下进行hyper-parameter 优化，以提高性能。<details>
<summary>Abstract</summary>
We leverage the fast physics simulator, MuJoCo to run tasks in a continuous control environment and reveal details like the observation space, action space, rewards, etc. for each task. We benchmark value-based methods for continuous control by comparing Q-learning and SARSA through a discretization approach, and using them as baselines, progressively moving into one of the state-of-the-art deep policy gradient method DDPG. Over a large number of episodes, Qlearning outscored SARSA, but DDPG outperformed both in a small number of episodes. Lastly, we also fine-tuned the model hyper-parameters expecting to squeeze more performance but using lesser time and resources. We anticipated that the new design for DDPG would vastly improve performance, yet after only a few episodes, we were able to achieve decent average rewards. We expect to improve the performance provided adequate time and computational resources.
</details>
<details>
<summary>摘要</summary>
我们利用快速物理渲染器MuJoCo来运行任务在连续控制环境中，揭示任务的观察空间、动作空间、奖励等细节。我们对continuous control方法进行了价值基础方法的比较，通过离散方法来对Q学习和SARSA进行比较，并将它们作为基准来进行比较。在大量的话 épisode 中，Q学习高于SARSA，但DDPG在一个小量的话pisode中高于两者。最后，我们也进行了模型超参数的调整，期望通过更少的时间和资源来提高性能。然而，我们在只需一些话pisode中就能够获得不错的均值奖励。我们预计通过充足的时间和计算资源来提高性能。
</details></li>
</ul>
<hr>
<h2 id="PAPR-Proximity-Attention-Point-Rendering"><a href="#PAPR-Proximity-Attention-Point-Rendering" class="headerlink" title="PAPR: Proximity Attention Point Rendering"></a>PAPR: Proximity Attention Point Rendering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11086">http://arxiv.org/abs/2307.11086</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanshu Zhang, Shichong Peng, Alireza Moazeni, Ke Li</li>
<li>for: 学习Scene表面准确且简洁的点云表示方法</li>
<li>methods: 使用点云表示和可微分渲染器</li>
<li>results: 可以准确地学习Scene geometry和纹理，并且只需要一个简洁的点云集Here’s the full translation of the abstract in Simplified Chinese:</li>
<li>for: 本研究旨在学习Scene表面准确且简洁的点云表示方法，以满足3D场景的学习和描述。</li>
<li>methods: 我们提出了Proximity Attention Point Rendering（PAPR）方法，它包括一个点云表示和一个可微分渲染器。我们的点云表示使用每个点的空间位置、前景得分和视图独立特征向量来 caracterize each point。渲染器选择每个辐射的相关点，并生成准确的颜色使用其关联的特征。</li>
<li>results: PAPR可以准确地学习Scene geometry和纹理，并且只需要一个简洁的点云集。我们还证明了我们的方法可以 capture fine texture details，并且可以在不同的初始化和目标geometry之间进行调整。I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
Learning accurate and parsimonious point cloud representations of scene surfaces from scratch remains a challenge in 3D representation learning. Existing point-based methods often suffer from the vanishing gradient problem or require a large number of points to accurately model scene geometry and texture. To address these limitations, we propose Proximity Attention Point Rendering (PAPR), a novel method that consists of a point-based scene representation and a differentiable renderer. Our scene representation uses a point cloud where each point is characterized by its spatial position, foreground score, and view-independent feature vector. The renderer selects the relevant points for each ray and produces accurate colours using their associated features. PAPR effectively learns point cloud positions to represent the correct scene geometry, even when the initialization drastically differs from the target geometry. Notably, our method captures fine texture details while using only a parsimonious set of points. We also demonstrate four practical applications of our method: geometry editing, object manipulation, texture transfer, and exposure control. More results and code are available on our project website at https://zvict.github.io/papr/.
</details>
<details>
<summary>摘要</summary>
学习精准且减少点云表示Scene表面的挑战仍然存在于3D表示学习中。现有的点云方法经常会遇到vanishing gradient问题或需要大量点云来准确模型Scene的几何学和 Texture。为解决这些限制，我们提出了靠近注意力点云渲染（PAPR），一种新的方法，它包括点云表示和可微分渲染器。我们的Scene表示使用一个点云，每个点Characterized by its spatial position, foreground score, and view-independent feature vector。渲染器选择每个光栅中 relevants Points and produce accurate colors using their associated features。PAPR有效地学习点云位置来表示正确的Scene几何学，即使初始化与Target几何学有很大差异。另外，我们的方法可以捕捉细 texture details，只用一个减少的点云集。我们还展示了我们的方法在四个实际应用中的成果：几何编辑、物体操作、 Texture Transfer 和曝光控制。更多结果和代码可以在我们项目网站（https://zvict.github.io/papr/）上找到。
</details></li>
</ul>
<hr>
<h2 id="AlignDet-Aligning-Pre-training-and-Fine-tuning-in-Object-Detection"><a href="#AlignDet-Aligning-Pre-training-and-Fine-tuning-in-Object-Detection" class="headerlink" title="AlignDet: Aligning Pre-training and Fine-tuning in Object Detection"></a>AlignDet: Aligning Pre-training and Fine-tuning in Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11077">http://arxiv.org/abs/2307.11077</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liming-ai/AlignDet">https://github.com/liming-ai/AlignDet</a></li>
<li>paper_authors: Ming Li, Jie Wu, Xionghui Wang, Chen Chen, Jie Qin, Xuefeng Xiao, Rui Wang, Min Zheng, Xin Pan</li>
<li>for: 提高 object detection 算法的性能、通用能力和训练速度</li>
<li>methods: 分离 pre-training 过程 into 两个阶段：图像领域预训练和框领域预训练，使得检测器可以在无监督的情况下预训练所有模块</li>
<li>results: 在多种协议下（检测算法、模型脊梁、数据设定和训练计划）进行了广泛的实验，并达到了显著提高（如 FCOS 的提高为 5.3 mAP，RetinaNet 的提高为 2.1 mAP，Faster R-CNN 的提高为 3.3 mAP，DETR 的提高为 2.3 mAP），并且在更少的训练epoch中 дости到了这些提高。<details>
<summary>Abstract</summary>
The paradigm of large-scale pre-training followed by downstream fine-tuning has been widely employed in various object detection algorithms. In this paper, we reveal discrepancies in data, model, and task between the pre-training and fine-tuning procedure in existing practices, which implicitly limit the detector's performance, generalization ability, and convergence speed. To this end, we propose AlignDet, a unified pre-training framework that can be adapted to various existing detectors to alleviate the discrepancies. AlignDet decouples the pre-training process into two stages, i.e., image-domain and box-domain pre-training. The image-domain pre-training optimizes the detection backbone to capture holistic visual abstraction, and box-domain pre-training learns instance-level semantics and task-aware concepts to initialize the parts out of the backbone. By incorporating the self-supervised pre-trained backbones, we can pre-train all modules for various detectors in an unsupervised paradigm. As depicted in Figure 1, extensive experiments demonstrate that AlignDet can achieve significant improvements across diverse protocols, such as detection algorithm, model backbone, data setting, and training schedule. For example, AlignDet improves FCOS by 5.3 mAP, RetinaNet by 2.1 mAP, Faster R-CNN by 3.3 mAP, and DETR by 2.3 mAP under fewer epochs.
</details>
<details>
<summary>摘要</summary>
大量预训练后下游细化的概念在各种物体检测算法中广泛应用。在这篇论文中，我们揭示了预训练和细化过程中数据、模型和任务之间的不一致，这些不一致限制了检测器的性能、泛化能力和收敛速度。为了解决这些问题，我们提出了AlignDet，一个可适应不同检测器的统一预训练框架。AlignDet将预训练过程分为两个阶段：图像预训练和框预训练。图像预训练使检测背景优化捕捉整体视觉抽象，而框预训练学习实例级别的 semantics和任务相关的概念，以初始化检测器中的部分。通过包含自动生成的预训练后缀，我们可以在无监督模式下预训练各种检测器的所有模块。根据图1所示，我们进行了广泛的实验，并证明了AlignDet可以在多种协议下实现显著改进，例如检测算法、模型脊梁、数据设置和训练计划。例如，AlignDet可以提高FCOS的map值5.3，RetinaNet的map值2.1，Faster R-CNN的map值3.3，和DETR的map值2.3，并且在较少的训练epoch下达到这些改进。
</details></li>
</ul>
<hr>
<h2 id="OBJECT-3DIT-Language-guided-3D-aware-Image-Editing"><a href="#OBJECT-3DIT-Language-guided-3D-aware-Image-Editing" class="headerlink" title="OBJECT 3DIT: Language-guided 3D-aware Image Editing"></a>OBJECT 3DIT: Language-guided 3D-aware Image Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11073">http://arxiv.org/abs/2307.11073</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oscar Michel, Anand Bhattad, Eli VanderBilt, Ranjay Krishna, Aniruddha Kembhavi, Tanmay Gupta</li>
<li>for: 这篇论文旨在提出语言引导的3D意识编辑技术，以便在图像编辑中尊重图像的3D几何结构。</li>
<li>methods: 作者采用了新的数据集OBJECT，并引入了3DIT单任务和多任务模型，以实现4种编辑任务。</li>
<li>results: 模型能够快速理解整个场景的3D几何结构，考虑周围的物体、表面、灯光条件和物理可能的对象配置。几乎无需使用真实图像数据，训练在OBJECTSynthetic scenes上的3DIT模型仍能在真实图像中表现出色。<details>
<summary>Abstract</summary>
Existing image editing tools, while powerful, typically disregard the underlying 3D geometry from which the image is projected. As a result, edits made using these tools may become detached from the geometry and lighting conditions that are at the foundation of the image formation process. In this work, we formulate the newt ask of language-guided 3D-aware editing, where objects in an image should be edited according to a language instruction in context of the underlying 3D scene. To promote progress towards this goal, we release OBJECT: a dataset consisting of 400K editing examples created from procedurally generated 3D scenes. Each example consists of an input image, editing instruction in language, and the edited image. We also introduce 3DIT : single and multi-task models for four editing tasks. Our models show impressive abilities to understand the 3D composition of entire scenes, factoring in surrounding objects, surfaces, lighting conditions, shadows, and physically-plausible object configurations. Surprisingly, training on only synthetic scenes from OBJECT, editing capabilities of 3DIT generalize to real-world images.
</details>
<details>
<summary>摘要</summary>
现有的图像编辑工具，尽管强大，通常忽略图像下面的3D几何结构。因此，使用这些工具进行编辑可能会导致编辑内容与图像的3D几何和照明条件脱离开来。在这项工作中，我们提出了语言指导的3D意识编辑问题，即在图像中编辑对象应该遵循语言指令，并且这些指令需要考虑到图像下面的3D场景。为促进这个目标的进步，我们发布了OBJECT数据集，该数据集包含400,000个编辑示例，每个示例包括输入图像、语言指令和编辑后的图像。我们还引入了3DIT单任务和多任务模型，这些模型在四个编辑任务中表现出了惊人的3D场景理解能力，包括周围物体、表面、照明条件、阴影等Physically-plausible object配置。意外地，使用只有synthetic场景从OBJECT进行训练，3DIT的编辑能力可以泛化到真实世界图像。
</details></li>
</ul>
<hr>
<h2 id="Driving-Policy-Prediction-based-on-Deep-Learning-Models"><a href="#Driving-Policy-Prediction-based-on-Deep-Learning-Models" class="headerlink" title="Driving Policy Prediction based on Deep Learning Models"></a>Driving Policy Prediction based on Deep Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11058">http://arxiv.org/abs/2307.11058</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Fuxiao Liu</li>
<li>for: 本研究开发了一个终端系统，用于从普通镜头和云点扫描器获取的视觉特征和深度信息，并预测车辆驾驶策略（车速和推进角）。</li>
<li>methods: 本研究使用了一个终端系统， combinig 视觉特征和深度信息，并使用了一些预测模型来预测车辆驾驶策略。</li>
<li>results: 本研究的测试结果显示，使用了combined 视觉特征和深度信息可以将预测精度提高到50%-80%，并且在大多数情况下比使用视觉特征Only更好。<details>
<summary>Abstract</summary>
In this project, we implemented an end-to-end system that takes in combined visual features of video frames from a normal camera and depth information from a cloud points scanner, and predicts driving policies (vehicle speed and steering angle). We verified the safety of our system by comparing the predicted results with standard behaviors by real-world experienced drivers. Our test results show that the predictions can be considered as accurate in at lease half of the testing cases (50% 80%, depending on the model), and using combined features improved the performance in most cases than using video frames only.
</details>
<details>
<summary>摘要</summary>
在这个项目中，我们实现了一个端到端系统，接受普通相机捕捉的视觉特征和云点扫描器提供的深度信息，并预测行驶策略（车辆速度和转向角）。我们确认了我们的系统的安全性，比较了预测结果与现实世界经验丰富的司机的标准行为。我们的测试结果显示，预测结果可以视为准确的至少半数的测试 caso（50%-80%，具体取决于模型），并且使用共同特征提高了性能的大多数情况。
</details></li>
</ul>
<hr>
<h2 id="A-LLM-Assisted-Exploitation-of-AI-Guardian"><a href="#A-LLM-Assisted-Exploitation-of-AI-Guardian" class="headerlink" title="A LLM Assisted Exploitation of AI-Guardian"></a>A LLM Assisted Exploitation of AI-Guardian</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15008">http://arxiv.org/abs/2307.15008</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicholas Carlini</li>
<li>for: 研究是否可以使用大型自然语言模型（LLMs）来帮助研究人员在攻击机器学习领域进行研究。</li>
<li>methods: 该研究使用GPT-4语言模型来实现这一目标，并通过提出攻击AI-Guardian防御方案的攻击算法来评估其效果。</li>
<li>results: 研究发现，AI-Guardian防御方案并没有提高鲁棒性，而GPT-4语言模型可以快速和效果地实现攻击算法。<details>
<summary>Abstract</summary>
Large language models (LLMs) are now highly capable at a diverse range of tasks. This paper studies whether or not GPT-4, one such LLM, is capable of assisting researchers in the field of adversarial machine learning. As a case study, we evaluate the robustness of AI-Guardian, a recent defense to adversarial examples published at IEEE S&P 2023, a top computer security conference. We completely break this defense: the proposed scheme does not increase robustness compared to an undefended baseline.   We write none of the code to attack this model, and instead prompt GPT-4 to implement all attack algorithms following our instructions and guidance. This process was surprisingly effective and efficient, with the language model at times producing code from ambiguous instructions faster than the author of this paper could have done. We conclude by discussing (1) the warning signs present in the evaluation that suggested to us AI-Guardian would be broken, and (2) our experience with designing attacks and performing novel research using the most recent advances in language modeling.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）现在可以很好地完成多种任务。这篇论文研究了GPT-4，一个这些LLM，是否能够帮助研究人员在对抗机器学习领域进行研究。作为一个案例研究，我们评估了在IEEE S&P 2023上发表的AI-Guardian防御机制，这是一种对抗攻击示例的最新防御策略。我们完全击碎了这个防御机制：提议的方案不会提高鲁棒性相比于无防御基eline。我们没有编写攻击代码，而是通过提供指导和指令，让GPT-4实现攻击算法。这个过程很有效率，GPT-4在某些情况下可以从歧义的指令中更快速地生成代码，比作者本身更快。我们 conclude by discussing (1) 评估中存在的警示信号，表明AI-Guardian会被击碎，以及 (2) 我们使用最新的语言模型技术进行攻击设计和进行新的研究。
</details></li>
</ul>
<hr>
<h2 id="Breadcrumbs-to-the-Goal-Goal-Conditioned-Exploration-from-Human-in-the-Loop-Feedback"><a href="#Breadcrumbs-to-the-Goal-Goal-Conditioned-Exploration-from-Human-in-the-Loop-Feedback" class="headerlink" title="Breadcrumbs to the Goal: Goal-Conditioned Exploration from Human-in-the-Loop Feedback"></a>Breadcrumbs to the Goal: Goal-Conditioned Exploration from Human-in-the-Loop Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11049">http://arxiv.org/abs/2307.11049</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/improbable-ai/human-guided-exploration">https://github.com/improbable-ai/human-guided-exploration</a></li>
<li>paper_authors: Marcel Torne, Max Balsells, Zihan Wang, Samedh Desai, Tao Chen, Pulkit Agrawal, Abhishek Gupta</li>
<li>for:  solves sequential decision-making tasks requiring expansive exploration without careful design of reward functions or the use of novelty-seeking exploration bonuses.</li>
<li>methods:  uses low-quality feedback from non-expert users that may be sporadic, asynchronous, and noisy to guide exploration, bifurcating human feedback and policy learning.</li>
<li>results:  learns policies with no hand-crafted reward design or exploration bonuses, and can scale to learning directly on real-world robots using occasional, asynchronous feedback from human supervisors.<details>
<summary>Abstract</summary>
Exploration and reward specification are fundamental and intertwined challenges for reinforcement learning. Solving sequential decision-making tasks requiring expansive exploration requires either careful design of reward functions or the use of novelty-seeking exploration bonuses. Human supervisors can provide effective guidance in the loop to direct the exploration process, but prior methods to leverage this guidance require constant synchronous high-quality human feedback, which is expensive and impractical to obtain. In this work, we present a technique called Human Guided Exploration (HuGE), which uses low-quality feedback from non-expert users that may be sporadic, asynchronous, and noisy. HuGE guides exploration for reinforcement learning not only in simulation but also in the real world, all without meticulous reward specification. The key concept involves bifurcating human feedback and policy learning: human feedback steers exploration, while self-supervised learning from the exploration data yields unbiased policies. This procedure can leverage noisy, asynchronous human feedback to learn policies with no hand-crafted reward design or exploration bonuses. HuGE is able to learn a variety of challenging multi-stage robotic navigation and manipulation tasks in simulation using crowdsourced feedback from non-expert users. Moreover, this paradigm can be scaled to learning directly on real-world robots, using occasional, asynchronous feedback from human supervisors.
</details>
<details>
<summary>摘要</summary>
探索和奖励规则设定是人工智能学习中的基本和结合的挑战。解决需要广泛探索的序列决策任务需要 either 精心设计奖励函数或使用新奇探索奖励。人工指导 loop 可以提供有效的导引，但先前的方法需要高质量、同步的人类反馈，这是昂贵和不实际的。在这项工作中，我们提出了一种技术 called HuGE（人类指导探索），它使用低质量的非专家用户反馈，这些反馈可能是零散的、异步的和噪音。HuGE 驱动探索 reinforcement learning 不仅在模拟中，还在实际世界中进行，无需谨慎设计奖励。关键思想是分离人类反馈和政策学习：人类反馈引导探索，而自我超vised learning from 探索数据获得无偏见的政策。这种方法可以利用噪音、异步的人类反馈来学习无需手动设计奖励或探索奖励。HuGE 可以在模拟中学习多个复杂的机器人导航和 manipulate 任务，并且可以扩展到直接在实际世界中学习，使用 occasional 和异步的人类反馈。</sys>Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. The translation is written in a more informal style, and some idiomatic expressions and cultural references may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="A-Definition-of-Continual-Reinforcement-Learning"><a href="#A-Definition-of-Continual-Reinforcement-Learning" class="headerlink" title="A Definition of Continual Reinforcement Learning"></a>A Definition of Continual Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11046">http://arxiv.org/abs/2307.11046</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Abel, André Barreto, Benjamin Van Roy, Doina Precup, Hado van Hasselt, Satinder Singh</li>
<li>for: 本 paper 开发了一个基础 для continual reinforcement learning.</li>
<li>methods: 本 paper 使用了一种基于 experience replay 的方法来实现 continual reinforcement learning.</li>
<li>results: 本 paper 实验结果表明，基于 experience replay 的方法可以有效地应对 continual reinforcement learning 中的问题。<details>
<summary>Abstract</summary>
In this paper we develop a foundation for continual reinforcement learning.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们开发了一个基础 для连续奖励学习。Here's the breakdown of the translation:* 在这篇论文中 (in this paper) - 这是一个定语词，指的是文章的内容。* 我们开发了 (we develop) - 开发是一个动词，意思是创造或设计 quelquechose。* 一个基础 (a foundation) - 基础是一个名词，指的是一个基础或基础结构。* 连续奖励学习 (continual reinforcement learning) - 奖励学习是一种学习方法，在每次学习后，对已经学习的内容进行回归和补做，以增强未来学习的效果。I hope this helps! Let me know if you have any other questions.
</details></li>
</ul>
<hr>
<h2 id="On-the-Convergence-of-Bounded-Agents"><a href="#On-the-Convergence-of-Bounded-Agents" class="headerlink" title="On the Convergence of Bounded Agents"></a>On the Convergence of Bounded Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11044">http://arxiv.org/abs/2307.11044</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Abel, André Barreto, Hado van Hasselt, Benjamin Van Roy, Doina Precup, Satinder Singh</li>
<li>for: 本研究探讨了agent convergences的定义和性质。</li>
<li>methods: 研究使用了中心于bounded agents的框架，并提出了两种补充性质的定义：一种是指agent的行为未来变化数量不能减少，另一种是指agent的表现只有在内部状态发生变化时才会改变。</li>
<li>results: 研究显示了这两种定义之间的关系和特性，并证明了它们在标准设置中的适用性。<details>
<summary>Abstract</summary>
When has an agent converged? Standard models of the reinforcement learning problem give rise to a straightforward definition of convergence: An agent converges when its behavior or performance in each environment state stops changing. However, as we shift the focus of our learning problem from the environment's state to the agent's state, the concept of an agent's convergence becomes significantly less clear. In this paper, we propose two complementary accounts of agent convergence in a framing of the reinforcement learning problem that centers around bounded agents. The first view says that a bounded agent has converged when the minimal number of states needed to describe the agent's future behavior cannot decrease. The second view says that a bounded agent has converged just when the agent's performance only changes if the agent's internal state changes. We establish basic properties of these two definitions, show that they accommodate typical views of convergence in standard settings, and prove several facts about their nature and relationship. We take these perspectives, definitions, and analysis to bring clarity to a central idea of the field.
</details>
<details>
<summary>摘要</summary>
The first view is that a bounded agent has converged when the minimum number of states needed to describe the agent's future behavior cannot decrease. The second view is that a bounded agent has converged when the agent's performance only changes if the agent's internal state changes. We establish basic properties of these two definitions, show that they align with standard views of convergence in typical settings, and prove several facts about their nature and relationship. Our goal is to bring clarity to a fundamental idea in the field.
</details></li>
</ul>
<hr>
<h2 id="Of-Models-and-Tin-Men-–-a-behavioural-economics-study-of-principal-agent-problems-in-AI-alignment-using-large-language-models"><a href="#Of-Models-and-Tin-Men-–-a-behavioural-economics-study-of-principal-agent-problems-in-AI-alignment-using-large-language-models" class="headerlink" title="Of Models and Tin Men – a behavioural economics study of principal-agent problems in AI alignment using large-language models"></a>Of Models and Tin Men – a behavioural economics study of principal-agent problems in AI alignment using large-language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11137">http://arxiv.org/abs/2307.11137</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/phelps-sg/llm-cooperation">https://github.com/phelps-sg/llm-cooperation</a></li>
<li>paper_authors: Steve Phelps, Rebecca Ranson</li>
<li>for: The paper focuses on the issue of AI safety, specifically the principal-agent problem that arises when there is a mismatch between the utility of an artificial agent and its principal.</li>
<li>methods: The paper uses empirical methods to investigate the behavior of GPT models in principal-agent conflicts, and examines how the models respond to changes in information asymmetry.</li>
<li>results: The paper finds that both GPT-3.5 and GPT-4 models exhibit clear evidence of principal-agent conflict, and that the earlier GPT-3.5 model shows more nuanced behavior in response to changes in information asymmetry, while the later GPT-4 model is more rigid in adhering to its prior alignment.Here is the information in Simplified Chinese text:</li>
<li>for: 本文关注人工智能安全问题，具体是代理人-代理人问题，代理人的利益与代理人所拥有的人工智能模型之间存在差异。</li>
<li>methods: 本文使用实证方法研究GPT模型在代理人-代理人问题中的行为，并对信息不均衡的变化进行调查。</li>
<li>results: 本文发现GPT-3.5和GPT-4模型都存在代理人-代理人问题，而GPT-3.5模型在信息不均衡下表现更加灵活，而GPT-4模型则更加坚持其先前的启配。<details>
<summary>Abstract</summary>
AI Alignment is often presented as an interaction between a single designer and an artificial agent in which the designer attempts to ensure the agent's behavior is consistent with its purpose, and risks arise solely because of conflicts caused by inadvertent misalignment between the utility function intended by the designer and the resulting internal utility function of the agent. With the advent of agents instantiated with large-language models (LLMs), which are typically pre-trained, we argue this does not capture the essential aspects of AI safety because in the real world there is not a one-to-one correspondence between designer and agent, and the many agents, both artificial and human, have heterogeneous values. Therefore, there is an economic aspect to AI safety and the principal-agent problem is likely to arise. In a principal-agent problem conflict arises because of information asymmetry together with inherent misalignment between the utility of the agent and its principal, and this inherent misalignment cannot be overcome by coercing the agent into adopting a desired utility function through training. We argue the assumptions underlying principal-agent problems are crucial to capturing the essence of safety problems involving pre-trained AI models in real-world situations. Taking an empirical approach to AI safety, we investigate how GPT models respond in principal-agent conflicts. We find that agents based on both GPT-3.5 and GPT-4 override their principal's objectives in a simple online shopping task, showing clear evidence of principal-agent conflict. Surprisingly, the earlier GPT-3.5 model exhibits more nuanced behaviour in response to changes in information asymmetry, whereas the later GPT-4 model is more rigid in adhering to its prior alignment. Our results highlight the importance of incorporating principles from economics into the alignment process.
</details>
<details>
<summary>摘要</summary>
人工智能安全问题经常被描述为一个设计者和一个人工智能系统之间的交互，其中设计者试图确保智能系统的行为与其目的相一致，而风险仅由设计者和智能系统之间的不一致引起。然而，随着大语言模型（LLM）实例的出现，我们认为这种定义不能捕捉AI安全问题的核心特征。在真实世界中，不存在一对一的设计者和智能系统之间的对应关系，而是有多个人工智能系统和人类的多个价值观。因此，AI安全问题具有经济性质，并且潜在的主体-代理人问题是不可避免的。在主体-代理人问题中，因为信息不均衡以及内置的利益不一致，导致冲突的产生，而这种内置的利益不一致无法通过强制训练改变代理人的利益。我们认为，包括经济原则在内的主体-代理人问题假设是捕捉AI安全问题实际情况的关键。我们采取了实证方法，研究了基于GPT模型的代理人冲突问题。我们发现，基于GPT-3.5和GPT-4模型的代理人在在线购物任务中Override其主体的目标，提供了明确的代理人冲突证据。另外，我们发现GPT-3.5模型在信息不均衡变化时的行为更加灵活，而GPT-4模型更加坚持其先前的协调。我们的结果 highlights the importance of incorporating principles from economics into the alignment process.
</details></li>
</ul>
<hr>
<h2 id="Cascade-DETR-Delving-into-High-Quality-Universal-Object-Detection"><a href="#Cascade-DETR-Delving-into-High-Quality-Universal-Object-Detection" class="headerlink" title="Cascade-DETR: Delving into High-Quality Universal Object Detection"></a>Cascade-DETR: Delving into High-Quality Universal Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11035">http://arxiv.org/abs/2307.11035</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/syscv/cascade-detr">https://github.com/syscv/cascade-detr</a></li>
<li>paper_authors: Mingqiao Ye, Lei Ke, Siyuan Li, Yu-Wing Tai, Chi-Keung Tang, Martin Danelljan, Fisher Yu</li>
<li>for: 提高各种领域中对象检测质量</li>
<li>methods: 提出了 Cascade Attention 层，以限制检测解码器的注意力，以提高对象localization的准确性；还进行了查询得分改进，改进了对象检测器的准确性</li>
<li>results: 在 UDB10 数据集上提高了 DETR 基于的对象检测器的性能，并在 stringent quality requirements 下表现出更明显的改进<details>
<summary>Abstract</summary>
Object localization in general environments is a fundamental part of vision systems. While dominating on the COCO benchmark, recent Transformer-based detection methods are not competitive in diverse domains. Moreover, these methods still struggle to very accurately estimate the object bounding boxes in complex environments.   We introduce Cascade-DETR for high-quality universal object detection. We jointly tackle the generalization to diverse domains and localization accuracy by proposing the Cascade Attention layer, which explicitly integrates object-centric information into the detection decoder by limiting the attention to the previous box prediction. To further enhance accuracy, we also revisit the scoring of queries. Instead of relying on classification scores, we predict the expected IoU of the query, leading to substantially more well-calibrated confidences. Lastly, we introduce a universal object detection benchmark, UDB10, that contains 10 datasets from diverse domains. While also advancing the state-of-the-art on COCO, Cascade-DETR substantially improves DETR-based detectors on all datasets in UDB10, even by over 10 mAP in some cases. The improvements under stringent quality requirements are even more pronounced. Our code and models will be released at https://github.com/SysCV/cascade-detr.
</details>
<details>
<summary>摘要</summary>
通用环境中的对象定位是视觉系统的基本组成部分。Recent Transformer-based detection方法在多样化领域中占据了主导地位，但是在复杂环境中仍然有很大的提升空间。我们介绍了Cascade-DETR，一种高质量通用对象检测方法，同时解决了多样化领域的泛化和本地化精度的问题。我们提出了卷积束注意力层，通过限制注意力到前一个框预测中来进行对象特征信息的集成。此外，我们还重新评估了查询的得分方式，而不是仅仅依靠分类得分，我们预测查询的预期 IoU，从而导致了较好的准确把握。最后，我们提出了一个通用对象检测标准 benchmark，UDB10，它包含了10个来自多样化领域的数据集。Cascade-DETR在COCO上也提高了状态Of-the-art，并在所有UDB10数据集上提高了DETR-based检测器的性能，有些情况下提高了10 mAP以上。在严格的质量要求下，改进的性能更加出色。我们将代码和模型发布在https://github.com/SysCV/cascade-detr。
</details></li>
</ul>
<hr>
<h2 id="“It-Felt-Like-Having-a-Second-Mind”-Investigating-Human-AI-Co-creativity-in-Prewriting-with-Large-Language-Models"><a href="#“It-Felt-Like-Having-a-Second-Mind”-Investigating-Human-AI-Co-creativity-in-Prewriting-with-Large-Language-Models" class="headerlink" title="“It Felt Like Having a Second Mind”: Investigating Human-AI Co-creativity in Prewriting with Large Language Models"></a>“It Felt Like Having a Second Mind”: Investigating Human-AI Co-creativity in Prewriting with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10811">http://arxiv.org/abs/2307.10811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qian Wan, Siying Hu, Yu Zhang, Piaohong Wang, Bo Wen, Zhicong Lu</li>
<li>for:  investigate human-LLM collaboration patterns and dynamics during prewriting</li>
<li>methods:  qualitative study with 15 participants in two creative tasks</li>
<li>results:  three-stage iterative Human-AI Co-creativity process with humans in a dominant role, mixed and shifting levels of initiative between humans and LLMs, and collaboration breakdowns<details>
<summary>Abstract</summary>
Prewriting is the process of discovering and developing ideas before a first draft, which requires divergent thinking and often implies unstructured strategies such as diagramming, outlining, free-writing, etc. Although large language models (LLMs) have been demonstrated to be useful for a variety of tasks including creative writing, little is known about how users would collaborate with LLMs to support prewriting. The preferred collaborative role and initiative of LLMs during such a creativity process is also unclear. To investigate human-LLM collaboration patterns and dynamics during prewriting, we conducted a three-session qualitative study with 15 participants in two creative tasks: story writing and slogan writing. The findings indicated that during collaborative prewriting, there appears to be a three-stage iterative Human-AI Co-creativity process that includes Ideation, Illumination, and Implementation stages. This collaborative process champions the human in a dominant role, in addition to mixed and shifting levels of initiative that exist between humans and LLMs. This research also reports on collaboration breakdowns that occur during this process, user perceptions of using existing LLMs during Human-AI Co-creativity, and discusses design implications to support this co-creativity process.
</details>
<details>
<summary>摘要</summary>
前期写作是发现和发展想法之前的过程，需要多元思维和不结构策略，如 диаграм、概要、自由写作等。虽然大型语言模型（LLM）已经在多种任务上显示出了用于创作写作的用途，但是 collaboration between humans and LLMs during prewriting 的可能性和偏好还未得到了充分的了解。为了调查人类和 LLM 之间在创作过程中的合作模式和动态，我们进行了三场质量研究，参与者共15人，完成了两项创作任务：故事写作和广告写作。研究发现，在人类和 LLM 合作的创作过程中，存在一个三阶段的融合人工智能创作过程，包括创意、灯光和实施阶段。这个合作过程强调人类在主导地位，同时存在人类和 LLM 之间的混合和不确定的主动程度。这项研究还报告了在这个过程中的协作破裂、用户对现有 LLM 的使用感受以及设计实现这种合作过程的建议。
</details></li>
</ul>
<hr>
<h2 id="NeoSySPArtaN-A-Neuro-Symbolic-Spin-Prediction-Architecture-for-higher-order-multipole-waveforms-from-eccentric-Binary-Black-Hole-mergers-using-Numerical-Relativity"><a href="#NeoSySPArtaN-A-Neuro-Symbolic-Spin-Prediction-Architecture-for-higher-order-multipole-waveforms-from-eccentric-Binary-Black-Hole-mergers-using-Numerical-Relativity" class="headerlink" title="NeoSySPArtaN: A Neuro-Symbolic Spin Prediction Architecture for higher-order multipole waveforms from eccentric Binary Black Hole mergers using Numerical Relativity"></a>NeoSySPArtaN: A Neuro-Symbolic Spin Prediction Architecture for higher-order multipole waveforms from eccentric Binary Black Hole mergers using Numerical Relativity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11003">http://arxiv.org/abs/2307.11003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amrutaa Vibho, Ali Al Bataineh</li>
<li>for: 这个论文旨在准确预测黑洞和中子星合并事件中的轨道矩剖。</li>
<li>methods: 该论文提出了一种 combine neural network和符号回归的Neuro-Symbolic Architecture（NSA），以便准确预测黑洞和中子星合并事件中的轨道矩剖。</li>
<li>results: 实验结果表明，提案的NSA模型可以准确地预测黑洞和中子星合并事件中的轨道矩剖，其RMSE和MSE值分别为0.05和0.03。<details>
<summary>Abstract</summary>
The prediction of spin magnitudes in binary black hole and neutron star mergers is crucial for understanding the astrophysical processes and gravitational wave (GW) signals emitted during these cataclysmic events. In this paper, we present a novel Neuro-Symbolic Architecture (NSA) that combines the power of neural networks and symbolic regression to accurately predict spin magnitudes of black hole and neutron star mergers. Our approach utilizes GW waveform data obtained from numerical relativity simulations in the SXS Waveform catalog. By combining these two approaches, we leverage the strengths of both paradigms, enabling a comprehensive and accurate prediction of spin magnitudes. Our experiments demonstrate that the proposed architecture achieves an impressive root-mean-squared-error (RMSE) of 0.05 and mean-squared-error (MSE) of 0.03 for the NSA model and an RMSE of 0.12 for the symbolic regression model alone. We train this model to handle higher-order multipole waveforms, with a specific focus on eccentric candidates, which are known to exhibit unique characteristics. Our results provide a robust and interpretable framework for predicting spin magnitudes in mergers. This has implications for understanding the astrophysical properties of black holes and deciphering the physics underlying the GW signals.
</details>
<details>
<summary>摘要</summary>
“预测双黑洞或中子星合并事件中的轨道矩的大小是astrophysical processes和 gravitational wave（GW）信号的关键。在这篇论文中，我们提出了一种新的Neuro-Symbolic Architecture（NSA），该模型结合神经网络和符号回归的力量，以准确预测双黑洞和中子星合并事件中的轨道矩。我们的方法使用了数值相对论中的GW波形数据，收集于SXS波形目录。通过结合这两种方法，我们可以利用每种 парадигма的优势，实现全面和准确的预测。我们的实验表明，我们提出的模型可以达到 impressive root-mean-squared-error（RMSE）值为0.05和mean-squared-error（MSE）值为0.03。此外，我们还训练了模型以处理更高阶多极波形，具体来说是焦点在非圆拟合物理特性上。我们的结果提供了一个可靠和可解释的框架，用于预测双黑洞和中子星合并事件中的轨道矩。这有助于理解黑洞的astrophysical Properties和解读GW信号中的物理。”
</details></li>
</ul>
<hr>
<h2 id="LLM-Cognitive-Judgements-Differ-From-Human"><a href="#LLM-Cognitive-Judgements-Differ-From-Human" class="headerlink" title="LLM Cognitive Judgements Differ From Human"></a>LLM Cognitive Judgements Differ From Human</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11787">http://arxiv.org/abs/2307.11787</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sotlampr/llm-cognitive-judgements">https://github.com/sotlampr/llm-cognitive-judgements</a></li>
<li>paper_authors: Sotiris Lamprinidis</li>
<li>for: 研究大语言模型（LLMs）的认知能力</li>
<li>methods: 使用GPT-3和ChatGPT模型完成有限数据的induction reasoning任务</li>
<li>results: GPT-3和ChatGPT的认知判断不类似于人类<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have lately been on the spotlight of researchers, businesses, and consumers alike. While the linguistic capabilities of such models have been studied extensively, there is growing interest in investigating them as cognitive subjects. In the present work I examine GPT-3 and ChatGPT capabilities on an limited-data inductive reasoning task from the cognitive science literature. The results suggest that these models' cognitive judgements are not human-like.
</details>
<details>
<summary>摘要</summary>
受到研究者、企业和消费者的关注，大语言模型（LLMs）在最近几年来已经备受关注。虽然这些模型的语言能力已经得到了广泛的研究，但是有越来越多的人想研究它们作为认知主体。在 presente 的工作中，我研究了 GPT-3 和 ChatGPT 在有限数据 inductive reasoning 任务上的能力。结果表明，这些模型的认知判断并不像人类的。
</details></li>
</ul>
<hr>
<h2 id="Dense-Sample-Deep-Learning"><a href="#Dense-Sample-Deep-Learning" class="headerlink" title="Dense Sample Deep Learning"></a>Dense Sample Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10991">http://arxiv.org/abs/2307.10991</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lizhaoliu-Lec/DAS">https://github.com/lizhaoliu-Lec/DAS</a></li>
<li>paper_authors: Stephen Josè Hanson, Vivek Yadav, Catherine Hanson</li>
<li>for: 本研究旨在探究深度学习（DL）网络在许多应用领域中的成功原理，包括语言翻译、蛋白质折叠、自动驾驶等，以及最近受到关注的人工智能语言模型（CHATbot）。</li>
<li>methods: 本研究使用了一个大型（1.24M参数；VGG）的DL网络，并在一种新的高密度样本任务中进行了研究（5个唯一的符号，每个符号至少有500个示例），以便更好地跟踪emergence of category structure和特征构建。研究者使用了多种视觉化方法来跟踪DL的学习动态和特征束成的发展。</li>
<li>results: 研究结果表明，DL网络在学习过程中会逐渐建立一种复杂的特征结构，这种结构可以通过图解方法来visualize。此外，研究者还提出了一种基于实验结果的新 teoría of complex feature construction。<details>
<summary>Abstract</summary>
Deep Learning (DL) , a variant of the neural network algorithms originally proposed in the 1980s, has made surprising progress in Artificial Intelligence (AI), ranging from language translation, protein folding, autonomous cars, and more recently human-like language models (CHATbots), all that seemed intractable until very recently. Despite the growing use of Deep Learning (DL) networks, little is actually understood about the learning mechanisms and representations that makes these networks effective across such a diverse range of applications. Part of the answer must be the huge scale of the architecture and of course the large scale of the data, since not much has changed since 1987. But the nature of deep learned representations remain largely unknown. Unfortunately training sets with millions or billions of tokens have unknown combinatorics and Networks with millions or billions of hidden units cannot easily be visualized and their mechanisms cannot be easily revealed. In this paper, we explore these questions with a large (1.24M weights; VGG) DL in a novel high density sample task (5 unique tokens with at minimum 500 exemplars per token) which allows us to more carefully follow the emergence of category structure and feature construction. We use various visualization methods for following the emergence of the classification and the development of the coupling of feature detectors and structures that provide a type of graphical bootstrapping, From these results we harvest some basic observations of the learning dynamics of DL and propose a new theory of complex feature construction based on our results.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Characterising-Decision-Theories-with-Mechanised-Causal-Graphs"><a href="#Characterising-Decision-Theories-with-Mechanised-Causal-Graphs" class="headerlink" title="Characterising Decision Theories with Mechanised Causal Graphs"></a>Characterising Decision Theories with Mechanised Causal Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10987">http://arxiv.org/abs/2307.10987</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matt MacDermott, Tom Everitt, Francesco Belardinelli</li>
<li>for: 本研究旨在描述和区分不同决策理论的机器化 causal 模型，并生成一种决策理论分类表。</li>
<li>methods: 本研究使用机器化 causal 模型来描述和分析不同决策理论的特征和区别。</li>
<li>results: 本研究通过使用机器化 causal 模型，生成了一种决策理论分类表，并提供了一种方法来区分不同决策理论的重要特征。<details>
<summary>Abstract</summary>
How should my own decisions affect my beliefs about the outcomes I expect to achieve? If taking a certain action makes me view myself as a certain type of person, it might affect how I think others view me, and how I view others who are similar to me. This can influence my expected utility calculations and change which action I perceive to be best. Whether and how it should is subject to debate, with contenders for how to think about it including evidential decision theory, causal decision theory, and functional decision theory. In this paper, we show that mechanised causal models can be used to characterise and differentiate the most important decision theories, and generate a taxonomy of different decision theories.
</details>
<details>
<summary>摘要</summary>
我的决定如何影响我对 достичь的结果的期望？如果我们选择一 certain action，可能会使我看到自己为一种特定的人类型，这可能会影响我如何看待他人，以及我如何看待与我类似的人。这可能会影响我的预期的利得计算，并改变我认为是最佳的行为。是否和如何是一个问题，在这篇论文中，我们使用机器化 causal 模型来描述和区分不同的决策理论，并生成一个决策理论的分类。
</details></li>
</ul>
<hr>
<h2 id="Metric3D-Towards-Zero-shot-Metric-3D-Prediction-from-A-Single-Image"><a href="#Metric3D-Towards-Zero-shot-Metric-3D-Prediction-from-A-Single-Image" class="headerlink" title="Metric3D: Towards Zero-shot Metric 3D Prediction from A Single Image"></a>Metric3D: Towards Zero-shot Metric 3D Prediction from A Single Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10984">http://arxiv.org/abs/2307.10984</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yvanyin/metric3d">https://github.com/yvanyin/metric3d</a></li>
<li>paper_authors: Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaixuan Wang, Xiaozhi Chen, Chunhua Shen</li>
<li>for: 这 paper 是为了解决单张图像中的3D场景重建问题而写的。</li>
<li>methods: 这 paper 使用了大规模数据训练和解决不同摄像机模型中的 métrico 歧义来解决单视metric depth estimation问题。</li>
<li>results: 该 paper 的方法在7个零shot benchmark 上达到了SOTA性能，并在2nd Monocular Depth Estimation Challenge 中获得了冠军。该方法可以准确地回归 metric 3D 结构，并且可以用于解决单视metrology 中的规模漂移问题。<details>
<summary>Abstract</summary>
Reconstructing accurate 3D scenes from images is a long-standing vision task. Due to the ill-posedness of the single-image reconstruction problem, most well-established methods are built upon multi-view geometry. State-of-the-art (SOTA) monocular metric depth estimation methods can only handle a single camera model and are unable to perform mixed-data training due to the metric ambiguity. Meanwhile, SOTA monocular methods trained on large mixed datasets achieve zero-shot generalization by learning affine-invariant depths, which cannot recover real-world metrics. In this work, we show that the key to a zero-shot single-view metric depth model lies in the combination of large-scale data training and resolving the metric ambiguity from various camera models. We propose a canonical camera space transformation module, which explicitly addresses the ambiguity problems and can be effortlessly plugged into existing monocular models. Equipped with our module, monocular models can be stably trained with over 8 million images with thousands of camera models, resulting in zero-shot generalization to in-the-wild images with unseen camera settings. Experiments demonstrate SOTA performance of our method on 7 zero-shot benchmarks. Notably, our method won the championship in the 2nd Monocular Depth Estimation Challenge. Our method enables the accurate recovery of metric 3D structures on randomly collected internet images, paving the way for plausible single-image metrology. The potential benefits extend to downstream tasks, which can be significantly improved by simply plugging in our model. For example, our model relieves the scale drift issues of monocular-SLAM (Fig. 1), leading to high-quality metric scale dense mapping. The code is available at https://github.com/YvanYin/Metric3D.
</details>
<details>
<summary>摘要</summary>
Traditional 3D scene reconstruction from images is a long-standing vision task. Due to the ill-posedness of the single-image reconstruction problem, most well-established methods are built upon multi-view geometry. State-of-the-art (SOTA) monocular metric depth estimation methods can only handle a single camera model and are unable to perform mixed-data training due to the metric ambiguity. Meanwhile, SOTA monocular methods trained on large mixed datasets achieve zero-shot generalization by learning affine-invariant depths, which cannot recover real-world metrics. In this work, we show that the key to a zero-shot single-view metric depth model lies in the combination of large-scale data training and resolving the metric ambiguity from various camera models. We propose a canonical camera space transformation module, which explicitly addresses the ambiguity problems and can be effortlessly plugged into existing monocular models. Equipped with our module, monocular models can be stably trained with over 8 million images with thousands of camera models, resulting in zero-shot generalization to in-the-wild images with unseen camera settings. Experiments demonstrate SOTA performance of our method on 7 zero-shot benchmarks. Notably, our method won the championship in the 2nd Monocular Depth Estimation Challenge. Our method enables the accurate recovery of metric 3D structures on randomly collected internet images, paving the way for plausible single-image metrology. The potential benefits extend to downstream tasks, which can be significantly improved by simply plugging in our model. For example, our model relieves the scale drift issues of monocular-SLAM (Fig. 1), leading to high-quality metric scale dense mapping. The code is available at https://github.com/YvanYin/Metric3D.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/21/cs.AI_2023_07_21/" data-id="clp89do7r000zi7888ey7c5tb" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_07_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/21/cs.CL_2023_07_21/" class="article-date">
  <time datetime="2023-07-21T11:00:00.000Z" itemprop="datePublished">2023-07-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/21/cs.CL_2023_07_21/">cs.CL - 2023-07-21</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="OxfordTVG-HIC-Can-Machine-Make-Humorous-Captions-from-Images"><a href="#OxfordTVG-HIC-Can-Machine-Make-Humorous-Captions-from-Images" class="headerlink" title="OxfordTVG-HIC: Can Machine Make Humorous Captions from Images?"></a>OxfordTVG-HIC: Can Machine Make Humorous Captions from Images?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11636">http://arxiv.org/abs/2307.11636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Runjia Li, Shuyang Sun, Mohamed Elhoseiny, Philip Torr</li>
<li>for: 本研究开发了一个大规模的humorous image captions数据集（OxfordTVG-HIC），用于幽默生成和理解。</li>
<li>methods: 本研究使用了一个大规模的image-text对的数据集，并运用了深度学习方法来训练一个通用的幽默captioning模型。</li>
<li>results: 研究发现，OxfordTVG-HIC数据集可以用于训练一个通用的幽默captioning模型，并且可以用来评估生成的文本是否具有幽默性。此外，研究还发现，幽默的文本生成通常需要融合语言和图像的信息，并且需要运用幽默的概念和笑料。<details>
<summary>Abstract</summary>
This paper presents OxfordTVG-HIC (Humorous Image Captions), a large-scale dataset for humour generation and understanding. Humour is an abstract, subjective, and context-dependent cognitive construct involving several cognitive factors, making it a challenging task to generate and interpret. Hence, humour generation and understanding can serve as a new task for evaluating the ability of deep-learning methods to process abstract and subjective information. Due to the scarcity of data, humour-related generation tasks such as captioning remain under-explored. To address this gap, OxfordTVG-HIC offers approximately 2.9M image-text pairs with humour scores to train a generalizable humour captioning model. Contrary to existing captioning datasets, OxfordTVG-HIC features a wide range of emotional and semantic diversity resulting in out-of-context examples that are particularly conducive to generating humour. Moreover, OxfordTVG-HIC is curated devoid of offensive content. We also show how OxfordTVG-HIC can be leveraged for evaluating the humour of a generated text. Through explainability analysis of the trained models, we identify the visual and linguistic cues influential for evoking humour prediction (and generation). We observe qualitatively that these cues are aligned with the benign violation theory of humour in cognitive psychology.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Change-of-Heart-Improving-Speech-Emotion-Recognition-through-Speech-to-Text-Modality-Conversion"><a href="#A-Change-of-Heart-Improving-Speech-Emotion-Recognition-through-Speech-to-Text-Modality-Conversion" class="headerlink" title="A Change of Heart: Improving Speech Emotion Recognition through Speech-to-Text Modality Conversion"></a>A Change of Heart: Improving Speech Emotion Recognition through Speech-to-Text Modality Conversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11584">http://arxiv.org/abs/2307.11584</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iclr2023achangeofheart/meld-modality-conversion">https://github.com/iclr2023achangeofheart/meld-modality-conversion</a></li>
<li>paper_authors: Zeinab Sadat Taghavi, Ali Satvaty, Hossein Sameti</li>
<li>for: 本研究旨在提高MELD dataset上的情感识别性能。</li>
<li>methods: 我们提出了一种模态转换概念，通过使用自动语音识别系统（ASR）和文本分类器来提高情感识别性能。</li>
<li>results: 我们的方法在MELD dataset上实现了显著的result，而Modality-Conversion++方法even outperformed当前的speech-based方法（WF1分数）。这些结果表明模态转换可以在不同的模态下进行任务，提高task的性能。<details>
<summary>Abstract</summary>
Speech Emotion Recognition (SER) is a challenging task. In this paper, we introduce a modality conversion concept aimed at enhancing emotion recognition performance on the MELD dataset. We assess our approach through two experiments: first, a method named Modality-Conversion that employs automatic speech recognition (ASR) systems, followed by a text classifier; second, we assume perfect ASR output and investigate the impact of modality conversion on SER, this method is called Modality-Conversion++. Our findings indicate that the first method yields substantial results, while the second method outperforms state-of-the-art (SOTA) speech-based approaches in terms of SER weighted-F1 (WF1) score on the MELD dataset. This research highlights the potential of modality conversion for tasks that can be conducted in alternative modalities.
</details>
<details>
<summary>摘要</summary>
《语音情感识别（SER）是一个挑战性的任务。在这篇论文中，我们介绍了一个modalities conversion概念，以提高MELD dataset上的情感识别性能。我们通过两个实验进行评估：第一个方法是使用自动话音识别系统（ASR），然后使用文本分类器；第二个方法是假设ASR输出是完美的，并调查模式转换对SER的影响，这个方法被称为Modality-Conversion++。我们发现第一个方法具有重要的成果，而第二个方法在MELDdataset上的SERWeighted-F1（WF1）分数超过了现有的话语基于的方法。这个研究显示了modalities conversion的潜力，用于可以在不同的模式下进行的任务。》Note that Simplified Chinese is the standard writing system used in mainland China, and it may be different from Traditional Chinese, which is used in Taiwan and other parts of the world.
</details></li>
</ul>
<hr>
<h2 id="Advancing-Visual-Grounding-with-Scene-Knowledge-Benchmark-and-Method"><a href="#Advancing-Visual-Grounding-with-Scene-Knowledge-Benchmark-and-Method" class="headerlink" title="Advancing Visual Grounding with Scene Knowledge: Benchmark and Method"></a>Advancing Visual Grounding with Scene Knowledge: Benchmark and Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11558">http://arxiv.org/abs/2307.11558</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhjohnchan/sk-vg">https://github.com/zhjohnchan/sk-vg</a></li>
<li>paper_authors: Zhihong Chen, Ruifei Zhang, Yibing Song, Xiang Wan, Guanbin Li</li>
<li>for:  This paper aims to create a new benchmark for visual grounding (VG) called SK-VG, which requires models to have reasoning abilities on long-form scene knowledge.</li>
<li>methods:  The proposed approaches for SK-VG involve embedding knowledge into image features before the image-query interaction, or leveraging linguistic structure to assist in computing the image-text matching.</li>
<li>results:  The proposed approaches achieve promising results but still leave room for improvement, including performance and interpretability.Here’s the simplified Chinese text:</li>
<li>for: 这篇论文目标是构建一个新的视觉固定（VG）benchmark，叫做SK-VG，它需要模型具有长文场景知识的理解能力。</li>
<li>methods: 提议的SK-VG方法包括在图像特征上嵌入知识，或者通过语言结构帮助图像文本匹配。</li>
<li>results: 提议的方法达到了可以的结果，但还有进一步的改进空间，包括性能和可解释性。<details>
<summary>Abstract</summary>
Visual grounding (VG) aims to establish fine-grained alignment between vision and language. Ideally, it can be a testbed for vision-and-language models to evaluate their understanding of the images and texts and their reasoning abilities over their joint space. However, most existing VG datasets are constructed using simple description texts, which do not require sufficient reasoning over the images and texts. This has been demonstrated in a recent study~\cite{luo2022goes}, where a simple LSTM-based text encoder without pretraining can achieve state-of-the-art performance on mainstream VG datasets. Therefore, in this paper, we propose a novel benchmark of \underline{S}cene \underline{K}nowledge-guided \underline{V}isual \underline{G}rounding (SK-VG), where the image content and referring expressions are not sufficient to ground the target objects, forcing the models to have a reasoning ability on the long-form scene knowledge. To perform this task, we propose two approaches to accept the triple-type input, where the former embeds knowledge into the image features before the image-query interaction; the latter leverages linguistic structure to assist in computing the image-text matching. We conduct extensive experiments to analyze the above methods and show that the proposed approaches achieve promising results but still leave room for improvement, including performance and interpretability. The dataset and code are available at \url{https://github.com/zhjohnchan/SK-VG}.
</details>
<details>
<summary>摘要</summary>
视觉定位（VG）的目标是建立细腻的视觉和语言之间对应。理想情况下，它可以作为视觉和语言模型的评估平台，以评估这些模型对图像和文本的理解和逻辑能力。然而，大多数现有的VG数据集都是使用简单的描述文本构建的，这些文本不足以需要图像和文本之间的充分理解。这已经在一项研究中被证明（Luo et al., 2022），使用无预训练的LSTM文本编码器可以在主流VG数据集上达到状态的表现。因此，在这篇论文中，我们提出了一个新的基准测试 datasets，即Scene Knowledge-guided Visual Grounding（SK-VG）。在这个数据集中，图像内容和引用表达不够地固定目标对象，需要模型具备对场景知识的理解和逻辑能力。为了完成这个任务，我们提出了两种方法，其中一种将知识嵌入图像特征之前进行图像-查询交互；另一种利用语言结构来帮助计算图像-文本匹配。我们进行了广泛的实验分析这些方法，并显示了提议的方法可以获得可 promise的结果，但仍有改进的空间，包括性能和可读性。数据集和代码可以在GitHub上获取，请参阅 <https://github.com/zhjohnchan/SK-VG>。
</details></li>
</ul>
<hr>
<h2 id="Bridging-Vision-and-Language-Encoders-Parameter-Efficient-Tuning-for-Referring-Image-Segmentation"><a href="#Bridging-Vision-and-Language-Encoders-Parameter-Efficient-Tuning-for-Referring-Image-Segmentation" class="headerlink" title="Bridging Vision and Language Encoders: Parameter-Efficient Tuning for Referring Image Segmentation"></a>Bridging Vision and Language Encoders: Parameter-Efficient Tuning for Referring Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11545">http://arxiv.org/abs/2307.11545</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kkakkkka/etris">https://github.com/kkakkkka/etris</a></li>
<li>paper_authors: Zunnan Xu, Zhihong Chen, Yong Zhang, Yibing Song, Xiang Wan, Guanbin Li</li>
<li>for: 提高图像 segmentation 任务的 Parametric Efficient Tuning (PET) 性能，并且可以更好地适应多Modalities 的交互。</li>
<li>methods: 提出了一种名为 Bridger 的 adapter，用于在预训练模型中交换 cross-modal 信息，并将任务特定的信息注入到模型中。还设计了一个轻量级的解码器。</li>
<li>results: 通过对 challenging benchmarks 进行评估，得到了与预训练模型相比的比较或更高的性能，只需要更新 backbone 参数 1.61% 到 3.38%。<details>
<summary>Abstract</summary>
Parameter Efficient Tuning (PET) has gained attention for reducing the number of parameters while maintaining performance and providing better hardware resource savings, but few studies investigate dense prediction tasks and interaction between modalities. In this paper, we do an investigation of efficient tuning problems on referring image segmentation. We propose a novel adapter called Bridger to facilitate cross-modal information exchange and inject task-specific information into the pre-trained model. We also design a lightweight decoder for image segmentation. Our approach achieves comparable or superior performance with only 1.61\% to 3.38\% backbone parameter updates, evaluated on challenging benchmarks. The code is available at \url{https://github.com/kkakkkka/ETRIS}.
</details>
<details>
<summary>摘要</summary>
Parameter Efficient Tuning (PET) 已经吸引了关注，因为它可以降低参数的数量而保持性能，并且提供更好的硬件资源储存。然而，有很少的研究探讨 dense prediction 任务和多Modalities 之间的交互。本文提出了一种叫做 Bridger 的 novel adapter，用于促进 cross-modal 信息交换和注入预训练模型中的任务特定信息。我们还设计了一个轻量级的解码器，用于图像 segmentation。我们的方法可以在具有挑战性的benchmark上实现相当或更高的性能，只需要更新 1.61% 到 3.38% 的后部参数。代码可以在 \url{https://github.com/kkakkkka/ETRIS} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Topic-Identification-For-Spontaneous-Speech-Enriching-Audio-Features-With-Embedded-Linguistic-Information"><a href="#Topic-Identification-For-Spontaneous-Speech-Enriching-Audio-Features-With-Embedded-Linguistic-Information" class="headerlink" title="Topic Identification For Spontaneous Speech: Enriching Audio Features With Embedded Linguistic Information"></a>Topic Identification For Spontaneous Speech: Enriching Audio Features With Embedded Linguistic Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11450">http://arxiv.org/abs/2307.11450</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aalto-speech/Topic-identification-for-spontaneous-Finnish-speech">https://github.com/aalto-speech/Topic-identification-for-spontaneous-Finnish-speech</a></li>
<li>paper_authors: Dejan Porjazovski, Tamás Grósz, Mikko Kurimo</li>
<li>for: 本研究旨在探讨非标准文本解决方案，即使没有自动语音识别系统（ASR）的情况下，是否可以使用音频特征来实现话语识别。</li>
<li>methods: 本研究使用了音频只解决方案和多模态解决方案，并对其在自然语言处理 tasks 上进行评估。</li>
<li>results: 研究结果表明，当 ASR 系统不可用时，专门使用音频特征的解决方案是一个可行的选择，而多模态解决方案在识别精度方面达到了最佳效果。<details>
<summary>Abstract</summary>
Traditional topic identification solutions from audio rely on an automatic speech recognition system (ASR) to produce transcripts used as input to a text-based model. These approaches work well in high-resource scenarios, where there are sufficient data to train both components of the pipeline. However, in low-resource situations, the ASR system, even if available, produces low-quality transcripts, leading to a bad text-based classifier. Moreover, spontaneous speech containing hesitations can further degrade the performance of the ASR model. In this paper, we investigate alternatives to the standard text-only solutions by comparing audio-only and hybrid techniques of jointly utilising text and audio features. The models evaluated on spontaneous Finnish speech demonstrate that purely audio-based solutions are a viable option when ASR components are not available, while the hybrid multi-modal solutions achieve the best results.
</details>
<details>
<summary>摘要</summary>
传统的话题标识解决方案从音频中依赖于自动语音识别系统（ASR）生成讲解，然后用文本基于模型进行分类。这些方法在高资源场景下工作良好，因为有足够的数据来训练两个组件的管道。然而，在低资源情况下，ASR系统，即使可用，也会生成低质量的讲解，导致文本基于分类器的性能差。此外，不断的语音中含有停顿可能会进一步降低ASR模型的性能。在这篇论文中，我们 investigate了标准文本只solution的代替方案，包括音频只solution和多Modal joint使用文本和音频特征的 Hybrid方法。我们对于不间断的芬兰语言进行了评估，得出结论是：纯音频基本方案在ASR组件不可用时是一个可行的选择，而Hybrid多Modal方法在最佳情况下实现了最好的结果。
</details></li>
</ul>
<hr>
<h2 id="MeetEval-A-Toolkit-for-Computation-of-Word-Error-Rates-for-Meeting-Transcription-Systems"><a href="#MeetEval-A-Toolkit-for-Computation-of-Word-Error-Rates-for-Meeting-Transcription-Systems" class="headerlink" title="MeetEval: A Toolkit for Computation of Word Error Rates for Meeting Transcription Systems"></a>MeetEval: A Toolkit for Computation of Word Error Rates for Meeting Transcription Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11394">http://arxiv.org/abs/2307.11394</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thilo von Neumann, Christoph Boeddeker, Marc Delcroix, Reinhold Haeb-Umbach</li>
<li>for: 本研究旨在评估各种会议笔记系统，提供一个通用的接口来计算常用的词错率（WER），包括cpWER、ORC WER 和 MIMO WER 等定义。</li>
<li>methods: 本文extend cpWER 计算中的时间约束，以确保只有在可能的时间对齐下才认为词语为正确的。这会导致匹配假设字符串与参照字符串的匹配质量更高，系统会受到负面折算。同时，作者还提出了一种使用 sentence-level 时间信息来估算 exact word-level 时间信息的方法，并证明该方法可以达到类似的 WER。</li>
<li>results: 作者通过实验表明，时间约束可以提高匹配算法的速度，并且这种提高的速度可以抵消对处理时间戳的额外开销。<details>
<summary>Abstract</summary>
MeetEval is an open-source toolkit to evaluate all kinds of meeting transcription systems. It provides a unified interface for the computation of commonly used Word Error Rates (WERs), specifically cpWER, ORC WER and MIMO WER along other WER definitions. We extend the cpWER computation by a temporal constraint to ensure that only words are identified as correct when the temporal alignment is plausible. This leads to a better quality of the matching of the hypothesis string to the reference string that more closely resembles the actual transcription quality, and a system is penalized if it provides poor time annotations. Since word-level timing information is often not available, we present a way to approximate exact word-level timings from segment-level timings (e.g., a sentence) and show that the approximation leads to a similar WER as a matching with exact word-level annotations. At the same time, the time constraint leads to a speedup of the matching algorithm, which outweighs the additional overhead caused by processing the time stamps.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Is-ChatGPT-Involved-in-Texts-Measure-the-Polish-Ratio-to-Detect-ChatGPT-Generated-Text"><a href="#Is-ChatGPT-Involved-in-Texts-Measure-the-Polish-Ratio-to-Detect-ChatGPT-Generated-Text" class="headerlink" title="Is ChatGPT Involved in Texts? Measure the Polish Ratio to Detect ChatGPT-Generated Text"></a>Is ChatGPT Involved in Texts? Measure the Polish Ratio to Detect ChatGPT-Generated Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11380">http://arxiv.org/abs/2307.11380</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/clement1290/chatgpt-detection-pr-hppt">https://github.com/clement1290/chatgpt-detection-pr-hppt</a></li>
<li>paper_authors: Lingyi Yang, Feng Jiang, Haizhou Li</li>
<li>For: The paper aims to address the limitations of previous detectors that can only differentiate between purely ChatGPT-generated texts and human-authored texts, and instead focuses on detecting texts generated through human-machine collaboration, such as ChatGPT-polished texts.* Methods: The paper introduces a novel dataset called HPPT (ChatGPT-polished academic abstracts) that consists of pairs of human-written and ChatGPT-polished abstracts, and proposes a new method called the “Polish Ratio” to measure the degree of ChatGPT involvement in text generation based on editing distance.* Results: The paper shows that the proposed model has better robustness on the HPPT dataset and two existing datasets (HC3 and CDB), and the “Polish Ratio” method provides a more comprehensive explanation by quantifying the degree of ChatGPT involvement, with a value greater than 0.2 indicating ChatGPT involvement and a value exceeding 0.6 implying that ChatGPT generates most of the text.Here are the three points in Simplified Chinese:</li>
<li>for: 这个论文的目的是解决以往的探测器只能区分纯然是人类写的文本和ChatGPT生成的文本，而不能区分人机合作生成的文本，例如ChatGPT准备过的文本。</li>
<li>methods: 这个论文提出了一个新的数据集名为HPPT（ChatGPT准备过的学术摘要），该数据集包含了人类写的和ChatGPT准备过的摘要对，并提出了一种新的方法名为“Polish Ratio”来度量ChatGPT在文本生成中的参与度。</li>
<li>results: 论文的实验结果表明，提出的模型在HPPT数据集和两个现有数据集（HC3和CDB）上具有更好的Robustness，而“Polish Ratio”方法提供了一个更加全面的解释，通过编辑距离度量ChatGPT的参与度，其中Polish Ratio值大于0.2表示ChatGPT参与，值大于0.6则表示ChatGPT主要生成了文本。<details>
<summary>Abstract</summary>
The remarkable capabilities of large-scale language models, such as ChatGPT, in text generation have incited awe and spurred researchers to devise detectors to mitigate potential risks, including misinformation, phishing, and academic dishonesty. Despite this, most previous studies, including HC3, have been predominantly geared towards creating detectors that differentiate between purely ChatGPT-generated texts and human-authored texts. This approach, however, fails to work on discerning texts generated through human-machine collaboration, such as ChatGPT-polished texts. Addressing this gap, we introduce a novel dataset termed HPPT (ChatGPT-polished academic abstracts), facilitating the construction of more robust detectors. It diverges from extant corpora by comprising pairs of human-written and ChatGPT-polished abstracts instead of purely ChatGPT-generated texts. Additionally, we propose the "Polish Ratio" method, an innovative measure of ChatGPT's involvement in text generation based on editing distance. It provides a mechanism to measure the degree of human originality in the resulting text. Our experimental results show our proposed model has better robustness on the HPPT dataset and two existing datasets (HC3 and CDB). Furthermore, the "Polish Ratio" we proposed offers a more comprehensive explanation by quantifying the degree of ChatGPT involvement, which indicates that a Polish Ratio value greater than 0.2 signifies ChatGPT involvement and a value exceeding 0.6 implies that ChatGPT generates most of the text.
</details>
<details>
<summary>摘要</summary>
大型语言模型，如ChatGPT，在文本生成方面表现出了惊人的能力，引发了研究人员的关注和探索。为了 mitigate  potential risks，包括诈骗、垃圾信息和学术不正当行为，研究人员们已经开发了多种检测器。然而，大多数前一些研究，包括HC3，都是将注意力集中在区分ChatGPT生成的文本和人类写的文本之间。这种方法并不能够在检测人机合作生成的文本，如ChatGPT优化的文本。为了解决这个 gap，我们提出了一个新的数据集，称为HPPT（ChatGPT优化的学术摘要）。这个数据集与现有的数据集不同，因为它包含了人类写的和ChatGPT优化的摘要对。此外，我们还提出了一种新的方法，称为“熔化率”方法，它基于编辑距离来衡量ChatGPT在文本生成中的参与度。这种方法可以衡量生成的文本中人类的原创性。我们的实验结果表明，我们的提出的模型在HPPT数据集和两个现有的数据集（HC3和CDB）上具有更高的Robustness。此外，我们提出的“熔化率”方法可以为检测器提供更全面的解释，并且表明一个熔化率值大于0.2表示ChatGPT的参与度，而一个值超过0.6表示ChatGPT主要生成了文本。
</details></li>
</ul>
<hr>
<h2 id="DEFTri-A-Few-Shot-Label-Fused-Contextual-Representation-Learning-For-Product-Defect-Triage-in-e-Commerce"><a href="#DEFTri-A-Few-Shot-Label-Fused-Contextual-Representation-Learning-For-Product-Defect-Triage-in-e-Commerce" class="headerlink" title="DEFTri: A Few-Shot Label Fused Contextual Representation Learning For Product Defect Triage in e-Commerce"></a>DEFTri: A Few-Shot Label Fused Contextual Representation Learning For Product Defect Triage in e-Commerce</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11344">http://arxiv.org/abs/2307.11344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ipsita Mohanty</li>
<li>for: 这研究旨在提高大规模敏捷软件开发循环中的缺陷排查效率，通过自动化方法使用机器学习来准确地将缺陷分配给合适的团队。</li>
<li>methods: 该研究提出了一个新的自动化缺陷排查框架（DEFTri），使用精度调整的现有预训练BERT来提高人类生成的产品缺陷文本embedding的上下文表示。</li>
<li>results: 在我们的多标签文本分类缺陷排查任务中，我们引入了一个Walmart报有的产品缺陷数据集，使用弱监督和对抗学习，在几个尝试设定下实现了一些表现。<details>
<summary>Abstract</summary>
Defect Triage is a time-sensitive and critical process in a large-scale agile software development lifecycle for e-commerce. Inefficiencies arising from human and process dependencies in this domain have motivated research in automated approaches using machine learning to accurately assign defects to qualified teams. This work proposes a novel framework for automated defect triage (DEFTri) using fine-tuned state-of-the-art pre-trained BERT on labels fused text embeddings to improve contextual representations from human-generated product defects. For our multi-label text classification defect triage task, we also introduce a Walmart proprietary dataset of product defects using weak supervision and adversarial learning, in a few-shot setting.
</details>
<details>
<summary>摘要</summary>
大规模敏捷软件开发生命周期中的缺陷察批是一个时间敏感和关键的过程，人工和过程依赖的不足导致了研究自动化方法的需求。这种工作提出了一种基于机器学习的新框架，用于自动检测缺陷（DEFTri），使用精度调整的先进预训练BERT来提高人工生成产品缺陷的文本表示。为我们的多标签文本分类缺陷察批任务，我们还介绍了一个华尔街专有的产品缺陷 dataset，使用弱监督和对抗学习，在几个尝试 Setting中。
</details></li>
</ul>
<hr>
<h2 id="Making-Pre-trained-Language-Models-both-Task-solvers-and-Self-calibrators"><a href="#Making-Pre-trained-Language-Models-both-Task-solvers-and-Self-calibrators" class="headerlink" title="Making Pre-trained Language Models both Task-solvers and Self-calibrators"></a>Making Pre-trained Language Models both Task-solvers and Self-calibrators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11316">http://arxiv.org/abs/2307.11316</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yangyi-chen/lm-toast">https://github.com/yangyi-chen/lm-toast</a></li>
<li>paper_authors: Yangyi Chen, Xingyao Wang, Heng Ji</li>
<li>for: 这个论文是为了解决PLMs在高风险应用中的过于自信问题，并提出了一种基于LM-TOAST算法的自适应Calibration方法。</li>
<li>methods: 这个论文使用了LM-TOAST算法，该算法是基于验证分布的，可以在有限的训练样本下使PLMs有合理的自信估计。</li>
<li>results: 实验结果表明，LM-TOAST可以有效地利用训练数据，使PLMs在高风险应用中具有合理的自信估计，同时保持原始任务性能。此外，论文还应用了LM-TOAST在选择性分类、对抗攻击和模型堆叠等下游应用中的实用性。<details>
<summary>Abstract</summary>
Pre-trained language models (PLMs) serve as backbones for various real-world systems. For high-stake applications, it's equally essential to have reasonable confidence estimations in predictions. While the vanilla confidence scores of PLMs can already be effectively utilized, PLMs consistently become overconfident in their wrong predictions, which is not desirable in practice. Previous work shows that introducing an extra calibration task can mitigate this issue. The basic idea involves acquiring additional data to train models in predicting the confidence of their initial predictions. However, it only demonstrates the feasibility of this kind of method, assuming that there are abundant extra available samples for the introduced calibration task. In this work, we consider the practical scenario that we need to effectively utilize training samples to make PLMs both task-solvers and self-calibrators. Three challenges are presented, including limited training samples, data imbalance, and distribution shifts. We first conduct pilot experiments to quantify various decisive factors in the calibration task. Based on the empirical analysis results, we propose a training algorithm LM-TOAST to tackle the challenges. Experimental results show that LM-TOAST can effectively utilize the training data to make PLMs have reasonable confidence estimations while maintaining the original task performance. Further, we consider three downstream applications, namely selective classification, adversarial defense, and model cascading, to show the practical usefulness of LM-TOAST. The code will be made public at \url{https://github.com/Yangyi-Chen/LM-TOAST}.
</details>
<details>
<summary>摘要</summary>
预训言语模型（PLM）作为许多实际应用的基础，需要有合理的信任估计。然而，PLM经常在错误预测时变得过自信，这不是实际应用中希望的。前期工作表明，引入额外的calibration任务可以解决这个问题。基本思路是通过获取更多的数据来训练模型对其初始预测的信任程度进行预测。然而，这只是一种可行的方法，假设有充足的额外可用样本。在这种实际场景中，我们需要使PLM同时作为任务解决方案和自我调整方法。我们提出三个挑战，包括有限的训练样本、数据不均衡和分布shift。我们首先进行了飞行实验，以量化各种重要的因素在calibration任务中。根据实验分析结果，我们提出了一种名为LM-TOAST的训练算法，以解决这些挑战。实验结果表明，LM-TOAST可以有效地利用训练数据，使PLM有合理的信任估计，同时保持原始任务性能。此外，我们还考虑了三个下游应用，namely selective classification、adversarial defense和model cascading，以显示LM-TOAST的实际用途。代码将在\url{https://github.com/Yangyi-Chen/LM-TOAST}上公开。
</details></li>
</ul>
<hr>
<h2 id="GIST-Generating-Image-Specific-Text-for-Fine-grained-Object-Classification"><a href="#GIST-Generating-Image-Specific-Text-for-Fine-grained-Object-Classification" class="headerlink" title="GIST: Generating Image-Specific Text for Fine-grained Object Classification"></a>GIST: Generating Image-Specific Text for Fine-grained Object Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11315">http://arxiv.org/abs/2307.11315</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/emu1729/gist">https://github.com/emu1729/gist</a></li>
<li>paper_authors: Kathleen M. Lewis, Emily Mu, Adrian V. Dalca, John Guttag</li>
<li>for: 这篇论文旨在提高精细图像分类 tasks 的表现， especailly for vision-language models that lack paired text&#x2F;image descriptions.</li>
<li>methods: 我们提出了一个方法，即 GIST，它可以将图像转换为特定的精细文本描述，并证明这些文本描述可以用来改善分类。 GIST 的关键部分包括使用域专的提示Word 激发预训大型自然语言模型生成多元精细文本描述，以及使用预训视语模型对每个图像和文本描述进行匹配。</li>
<li>results: 我们透过对视语模型进行 fine-tuning 以 learns an aligned vision-language representation space，并在四个不同领域的全载和少载测试中证明了我们的方法可以提高 $4.1%$ 的准确性和 $1.1%$ 的前期的最佳图像文本分类方法。<details>
<summary>Abstract</summary>
Recent vision-language models outperform vision-only models on many image classification tasks. However, because of the absence of paired text/image descriptions, it remains difficult to fine-tune these models for fine-grained image classification. In this work, we propose a method, GIST, for generating image-specific fine-grained text descriptions from image-only datasets, and show that these text descriptions can be used to improve classification. Key parts of our method include 1. prompting a pretrained large language model with domain-specific prompts to generate diverse fine-grained text descriptions for each class and 2. using a pretrained vision-language model to match each image to label-preserving text descriptions that capture relevant visual features in the image. We demonstrate the utility of GIST by fine-tuning vision-language models on the image-and-generated-text pairs to learn an aligned vision-language representation space for improved classification. We evaluate our learned representation space in full-shot and few-shot scenarios across four diverse fine-grained classification datasets, each from a different domain. Our method achieves an average improvement of $4.1\%$ in accuracy over CLIP linear probes and an average of $1.1\%$ improvement in accuracy over the previous state-of-the-art image-text classification method on the full-shot datasets. Our method achieves similar improvements across few-shot regimes. Code is available at https://github.com/emu1729/GIST.
</details>
<details>
<summary>摘要</summary>
近期的视觉语模型在许多图像分类任务上表现出了超越视觉模型的能力。然而，由于缺乏对应的文本/图像描述对， fine-tune这些模型 для细化图像分类仍然具有挑战。在这项工作中，我们提出了一种方法，即GIST，可以从图像只 datasets中生成特定图像的细化文本描述，并证明这些文本描述可以用于改进分类。关键的部分包括：1. 使用域pecific的推文提示大型预训练语言模型生成每个类型的多样化细化文本描述。2. 使用预训练的视觉语言模型将每幅图像与保持标签的文本描述相匹配，以捕捉图像中相关的视觉特征。我们采用GIST方法，在图像和生成的文本对上进行了 fine-tuning 视觉语言模型，以学习一个含有视觉语言对应关系的表示空间，并证明该表示空间可以提高分类精度。我们在四个不同领域的四个全文shot和几个少shot场景中进行了评估，结果显示，我们的方法在全文shot场景中平均提高了4.1%的精度，并在前一个状态的图像文本分类方法中提高了1.1%的精度。我们的方法在少shot场景中也具有相似的改进。代码可以在https://github.com/emu1729/GIST 中找到。
</details></li>
</ul>
<hr>
<h2 id="Who-should-I-Collaborate-with-A-Comparative-Study-of-Academia-and-Industry-Research-Collaboration-in-NLP"><a href="#Who-should-I-Collaborate-with-A-Comparative-Study-of-Academia-and-Industry-Research-Collaboration-in-NLP" class="headerlink" title="Who should I Collaborate with? A Comparative Study of Academia and Industry Research Collaboration in NLP"></a>Who should I Collaborate with? A Comparative Study of Academia and Industry Research Collaboration in NLP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04524">http://arxiv.org/abs/2308.04524</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hussain Sadiq Abuwala, Bohan Zhang, Mushi Wang</li>
<li>for: 研究了学术和产业合作对自然语言处理（NLP）的影响。</li>
<li>methods: 创建了一个管道，EXTRACTAFFILIATIONS和CITATIONS FROM NLP论文，并将其分为三类：学术、产业和混合（学术和产业合作）。</li>
<li>results: 发现论文中学术和产业合作的出版数量呈增长趋势，而这些合作出版物的影响力比solely在学术界出版的高。<details>
<summary>Abstract</summary>
The goal of our research was to investigate the effects of collaboration between academia and industry on Natural Language Processing (NLP). To do this, we created a pipeline to extract affiliations and citations from NLP papers and divided them into three categories: academia, industry, and hybrid (collaborations between academia and industry). Our empirical analysis found that there is a trend towards an increase in industry and academia-industry collaboration publications and that these types of publications tend to have a higher impact compared to those produced solely within academia.
</details>
<details>
<summary>摘要</summary>
我们的研究目标是研究学术与产业合作对自然语言处理（NLP）的影响。为此，我们创建了一个管道，EXTRACT afFILIATIONS和引用 FROM NLP论文，并将其分为三类：学术、产业和Hybrid（学术和产业合作）。我们的实证分析发现，有一趋向增加的产业和学术合作出版物，并且这些类型的出版物具有较高的影响力，比solely within academia的出版物更高。Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Generator-Retriever-Generator-A-Novel-Approach-to-Open-domain-Question-Answering"><a href="#Generator-Retriever-Generator-A-Novel-Approach-to-Open-domain-Question-Answering" class="headerlink" title="Generator-Retriever-Generator: A Novel Approach to Open-domain Question Answering"></a>Generator-Retriever-Generator: A Novel Approach to Open-domain Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11278">http://arxiv.org/abs/2307.11278</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abdoelsayed2016/grg">https://github.com/abdoelsayed2016/grg</a></li>
<li>paper_authors: Abdelrahman Abdallah, Adam Jatowt</li>
<li>for: 提高开放领域问答（QA）任务中的答案准确性，通过将文档检索技术与大语言模型（LLM）结合在一起，首先提示模型生成基于问题的文档，然后并行使用双编码网络从外部废集中检索相关的文档，最后通过第二个LLM生成最终的答案。</li>
<li>methods: 提议使用生成器-检索器-生成器（GRG）模型，其中首先使用LLM生成基于问题的文档，然后使用双编码网络从外部废集中检索相关的文档，最后使用第二个LLM生成最终的答案。</li>
<li>results: GRG模型在TriviaQA、NQ和WebQ数据集上的性能比GENREAD和RFiD模型高出至少5.2、4.2和1.6分，表明GRG模型可以更好地解决开放领域QA任务中的挑战，如生成相关和Contextually relevante的答案。<details>
<summary>Abstract</summary>
Open-domain question answering (QA) tasks usually require the retrieval of relevant information from a large corpus to generate accurate answers. We propose a novel approach called Generator-Retriever-Generator (GRG) that combines document retrieval techniques with a large language model (LLM), by first prompting the model to generate contextual documents based on a given question. In parallel, a dual-encoder network retrieves documents that are relevant to the question from an external corpus. The generated and retrieved documents are then passed to the second LLM, which generates the final answer. By combining document retrieval and LLM generation, our approach addresses the challenges of open-domain QA, such as generating informative and contextually relevant answers. GRG outperforms the state-of-the-art generate-then-read and retrieve-then-read pipelines (GENREAD and RFiD) improving their performance at least by +5.2, +4.2, and +1.6 on TriviaQA, NQ, and WebQ datasets, respectively. We provide code, datasets, and checkpoints \footnote{\url{https://github.com/abdoelsayed2016/GRG}
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Systematic-Evaluation-of-Federated-Learning-on-Biomedical-Natural-Language-Processing"><a href="#A-Systematic-Evaluation-of-Federated-Learning-on-Biomedical-Natural-Language-Processing" class="headerlink" title="A Systematic Evaluation of Federated Learning on Biomedical Natural Language Processing"></a>A Systematic Evaluation of Federated Learning on Biomedical Natural Language Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11254">http://arxiv.org/abs/2307.11254</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pl97/fednlp">https://github.com/pl97/fednlp</a></li>
<li>paper_authors: Le Peng, sicheng zhou, jiandong chen, Rui Zhang, Ziyue Xu, Ju Sun</li>
<li>for: 本研究用于探讨基于联合学习的医疗自然语言处理（NLP）模型的可靠性和私有数据保护。</li>
<li>methods: 本研究使用了6种基于 transformer 的语言模型，对6种生物医学数据集进行了融合学习。</li>
<li>results: 研究结果显示，基于联合学习的模型在医疗数据集上的性能比各自训练的模型和投票数据集训练的模型要好，但是当客户端数据不均匀分布时，模型性能会下降。 code 可以在 GitHub 上找到：<a target="_blank" rel="noopener" href="https://github.com/PL97/FedNLP%E3%80%82">https://github.com/PL97/FedNLP。</a><details>
<summary>Abstract</summary>
Language models (LMs) like BERT and GPT have revolutionized natural language processing (NLP). However, privacy-sensitive domains, particularly the medical field, face challenges to train LMs due to limited data access and privacy constraints imposed by regulations like the Health Insurance Portability and Accountability Act (HIPPA) and the General Data Protection Regulation (GDPR). Federated learning (FL) offers a decentralized solution that enables collaborative learning while ensuring the preservation of data privacy. In this study, we systematically evaluate FL in medicine across $2$ biomedical NLP tasks using $6$ LMs encompassing $8$ corpora. Our results showed that: 1) FL models consistently outperform LMs trained on individual client's data and sometimes match the model trained with polled data; 2) With the fixed number of total data, LMs trained using FL with more clients exhibit inferior performance, but pre-trained transformer-based models exhibited greater resilience. 3) LMs trained using FL perform nearly on par with the model trained with pooled data when clients' data are IID distributed while exhibiting visible gaps with non-IID data. Our code is available at: https://github.com/PL97/FedNLP
</details>
<details>
<summary>摘要</summary>
语言模型（LM）如BERT和GPT已经革命化自然语言处理（NLP）领域。然而，隐私敏感领域，特别是医疗领域，面临着培训LM的限制，这是由于有限的数据访问和隐私法规，如医疗保险和人类资源保护法（HIPAA）和欧盟数据保护条例（GDPR）所带来的。联邦学习（FL）提供了一种分布式解决方案，允许客户端之间的合作学习，同时保持数据隐私。在本研究中，我们系统地评估FL在医学领域中的表现，使用了6种LM，涵盖8个corpus。我们的结果显示了以下几点：1. FL模型通常比各个客户端的数据进行单独培训的LM表现更好，有时与汇总数据进行培训的模型匹配。2. 随着客户端数量增加，使用FL培训的LM在固定总数据量下表现较差，但是使用预先学习的 transformer-based 模型表现更好。3. 使用FL培训的LM在客户端数据是IID分布的情况下几乎与汇总数据进行培训的模型表现相同，而在非IID分布的情况下显示出明显的差距。我们的代码可以在GitHub上找到：https://github.com/PL97/FedNLP。
</details></li>
</ul>
<hr>
<h2 id="UMLS-KGI-BERT-Data-Centric-Knowledge-Integration-in-Transformers-for-Biomedical-Entity-Recognition"><a href="#UMLS-KGI-BERT-Data-Centric-Knowledge-Integration-in-Transformers-for-Biomedical-Entity-Recognition" class="headerlink" title="UMLS-KGI-BERT: Data-Centric Knowledge Integration in Transformers for Biomedical Entity Recognition"></a>UMLS-KGI-BERT: Data-Centric Knowledge Integration in Transformers for Biomedical Entity Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11170">http://arxiv.org/abs/2307.11170</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aidan Mannion, Thierry Chevalier, Didier Schwab, Lorraine Geouriot</li>
<li>for: 这篇论文旨在提高生物医学领域的自然语言处理（NLP）任务中的表达能力。</li>
<li>methods: 该论文提出了一种基于数据的概念，用于增强生物医学领域的变换器encoder语言模型（LM）的语言表示。该方法通过提取来自UMLS的文本序列，将图形学习目标融合到了遮盖语言预训练中。</li>
<li>results: 根据对扩展预训练LM和从scratch进行训练的实验结果，该框架可以提高多个生物医学和临床Named Entity Recognition（NER）任务的下游性能。<details>
<summary>Abstract</summary>
Pre-trained transformer language models (LMs) have in recent years become the dominant paradigm in applied NLP. These models have achieved state-of-the-art performance on tasks such as information extraction, question answering, sentiment analysis, document classification and many others. In the biomedical domain, significant progress has been made in adapting this paradigm to NLP tasks that require the integration of domain-specific knowledge as well as statistical modelling of language. In particular, research in this area has focused on the question of how best to construct LMs that take into account not only the patterns of token distribution in medical text, but also the wealth of structured information contained in terminology resources such as the UMLS. This work contributes a data-centric paradigm for enriching the language representations of biomedical transformer-encoder LMs by extracting text sequences from the UMLS. This allows for graph-based learning objectives to be combined with masked-language pre-training. Preliminary results from experiments in the extension of pre-trained LMs as well as training from scratch show that this framework improves downstream performance on multiple biomedical and clinical Named Entity Recognition (NER) tasks.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)先前的 transformer 语言模型 (LM) 在应用 NLP 领域已经成为主导的 paradigm。这些模型在信息提取、问答、情感分析、文档分类等任务上达到了状态机器的性能。在生物医学领域，人们已经在这些模型中采用了适应性的方法，以满足需要 integrate 域特定知识和语言统计模型的任务。特别是，研究人员在这个领域的关注点是如何构建 LM，以便考虑医学文本中token的分布模式，同时还考虑terminology资源such as UMLS 中的结构化信息。这个工作提供了一种数据驱动的 paradigm，用于增强生物医学 transformer-encoder LM 的语言表示。通过EXTRACTING text sequences from UMLS，可以将图基的学习目标与遮盖语言预训练结合。初步的实验结果表明，这种框架可以提高多个生物医学和临床Named Entity Recognition (NER) 任务的下游性能。
</details></li>
</ul>
<hr>
<h2 id="L-Eval-Instituting-Standardized-Evaluation-for-Long-Context-Language-Models"><a href="#L-Eval-Instituting-Standardized-Evaluation-for-Long-Context-Language-Models" class="headerlink" title="L-Eval: Instituting Standardized Evaluation for Long Context Language Models"></a>L-Eval: Instituting Standardized Evaluation for Long Context Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11088">http://arxiv.org/abs/2307.11088</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openlmlab/leval">https://github.com/openlmlab/leval</a></li>
<li>paper_authors: Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, Xipeng Qiu</li>
<li>for: 评估长context语言模型的表现，以提高其处理单turn输入和对话历史的能力。</li>
<li>methods: 开发了一个标准化的评估方法，名为L-Eval，包含411份长文档和2,000多个人标注的查询响应对。</li>
<li>results: 发现open-source模型在评估中落后于商业模型，但仍然在开放式任务上表现出色，并在4k上下文长度下达到了最好的结果。<details>
<summary>Abstract</summary>
Recently, there has been growing interest in extending the context length of instruction-following models in order to effectively process single-turn long input (e.g. summarizing a paper) and conversations with more extensive histories. While proprietary models such as GPT-4 and Claude have shown significant strides in handling extremely lengthy input, open-sourced models are still in the early stages of experimentation. It also remains unclear whether extending the context can offer substantial gains over traditional methods such as retrieval, and to what extent it improves upon their regular counterparts in practical downstream tasks. To address this challenge, we propose instituting standardized evaluation for long context language models. Concretely, we develop L-Eval which contains 411 long documents and over 2,000 human-labeled query-response pairs encompassing areas such as law, finance, school lectures, lengthy conversations, news, long-form novels, and meetings. L-Eval also adopts diverse evaluation methods and instruction styles, enabling a more reliable assessment of Long Context Language Models (LCLMs). Our findings indicate that while open-source models typically lag behind commercial models, they still exhibit impressive performance compared with their regular versions. LLaMA2-13B achieves the best results on both open-ended tasks (win \textbf{42}\% vs turbo-16k-0613) and closed-ended tasks with only 4k context length. We release our new evaluation suite, code, and all generation results including predictions from all open-sourced LCLMs, GPT4-32k, Cluade-100k at {\url{https://github.com/OpenLMLab/LEval}.
</details>
<details>
<summary>摘要</summary>
近些时间，有越来越多的人对长 Context Language Model（LCLM）的Context长度扩展表示出兴趣，以便有效处理单次输入（如报告概要）和历史记录更加详细的对话。虽然Proprietary模型如GPT-4和Claude已经在处理极长输入方面进行了显著的进步，但开源模型仍处于实验阶段。尚未确定是否通过扩展Context可以提供重要的提升，以及到哪程度 extent 可以超越传统方法（如检索）的实际下渠道任务中的表现。为解决这个挑战，我们提议实施长 Context Language Model 的标准化评估。具体来说，我们开发了L-Eval，包含411个长文档和超过2000个人标注的查询响应对。L-Eval采用多种评估方法和指令风格，使得对Long Context Language Models 的评估更加可靠。我们的发现表明，虽然开源模型通常落后于商业模型，但仍然在开放式任务中表现出色，LLaMA2-13B在开放式任务中取得了42%的胜利率，并在4k Context长度下达到了最佳成绩。我们将在{\url{https://github.com/OpenLMLab/LEval}上发布我们的新评估集合、代码和所有生成结果，包括所有开源LCLMs、GPT4-32k和Cluade-100k的预测结果。
</details></li>
</ul>
<hr>
<h2 id="Embroid-Unsupervised-Prediction-Smoothing-Can-Improve-Few-Shot-Classification"><a href="#Embroid-Unsupervised-Prediction-Smoothing-Can-Improve-Few-Shot-Classification" class="headerlink" title="Embroid: Unsupervised Prediction Smoothing Can Improve Few-Shot Classification"></a>Embroid: Unsupervised Prediction Smoothing Can Improve Few-Shot Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11031">http://arxiv.org/abs/2307.11031</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HazyResearch/embroid">https://github.com/HazyResearch/embroid</a></li>
<li>paper_authors: Neel Guha, Mayee F. Chen, Kush Bhatia, Azalia Mirhoseini, Frederic Sala, Christopher Ré</li>
<li>for: 提高自动数据标注的效率，使用语言模型（LM）的提问基本学习能力。</li>
<li>methods: 使用LM的预测结果进行修改，而不需要更多的标注数据。提出Embroid方法，通过计算不同嵌入函数下的多个表示来实现。</li>
<li>results: 对六种LM和95个任务进行了严格的实验评估，发现 Embroid 能够提高表现（比如 GPT-JT 的平均提高7.3个点），同时也能够实现更复杂的提问策略（如连链思维），并可以根据嵌入函数进行特化。<details>
<summary>Abstract</summary>
Recent work has shown that language models' (LMs) prompt-based learning capabilities make them well suited for automating data labeling in domains where manual annotation is expensive. The challenge is that while writing an initial prompt is cheap, improving a prompt is costly -- practitioners often require significant labeled data in order to evaluate the impact of prompt modifications. Our work asks whether it is possible to improve prompt-based learning without additional labeled data. We approach this problem by attempting to modify the predictions of a prompt, rather than the prompt itself. Our intuition is that accurate predictions should also be consistent: samples which are similar under some feature representation should receive the same prompt prediction. We propose Embroid, a method which computes multiple representations of a dataset under different embedding functions, and uses the consistency between the LM predictions for neighboring samples to identify mispredictions. Embroid then uses these neighborhoods to create additional predictions for each sample, and combines these predictions with a simple latent variable graphical model in order to generate a final corrected prediction. In addition to providing a theoretical analysis of Embroid, we conduct a rigorous empirical evaluation across six different LMs and up to 95 different tasks. We find that (1) Embroid substantially improves performance over original prompts (e.g., by an average of 7.3 points on GPT-JT), (2) also realizes improvements for more sophisticated prompting strategies (e.g., chain-of-thought), and (3) can be specialized to domains like law through the embedding functions.
</details>
<details>
<summary>摘要</summary>
最近的研究表明，语言模型（LM）的提问基本学习能力使其适合自动化数据标注，尤其是在人工标注成本高的领域。问题在于，虽然编写初始提问便宜，但是改进提问却需要较多的标注数据来评估提问修改的影响。我们的工作是要判断是否可以不使用额外的标注数据来改进提问基本学习。我们采取的方法是通过修改预测而不是提问本身来改进提问。我们的直觉是，准确的预测应该也是一致的：样本在某种特征表示下应该具有相似的预测结果。我们提出了Embroid方法，它计算了不同的嵌入函数下的数据集多种表示，并使用预测结果之间的一致性来识别预测错误。Embroid使用这些邻居样本来生成每个样本的额外预测，并将这些预测与一个简单的潜在变量图模型结合起来，以生成最终更正的预测。除了对Embroid的理论分析之外，我们还进行了六种不同LM的系统性的实验，并在95个任务中进行了严格的实验评估。我们发现：1. Embroid可以明显改进原始提问的性能（例如，使用GPT-JT的平均提高7.3分）。2. Embroid还可以提高更复杂的提问策略（例如，链条思维）的性能。3. Embroid可以根据嵌入函数特定的领域进行特殊化。
</details></li>
</ul>
<hr>
<h2 id="Investigating-the-Factual-Knowledge-Boundary-of-Large-Language-Models-with-Retrieval-Augmentation"><a href="#Investigating-the-Factual-Knowledge-Boundary-of-Large-Language-Models-with-Retrieval-Augmentation" class="headerlink" title="Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation"></a>Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11019">http://arxiv.org/abs/2307.11019</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rucaibox/llm-knowledge-boundary">https://github.com/rucaibox/llm-knowledge-boundary</a></li>
<li>paper_authors: Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin Zhao, Jing Liu, Hao Tian, Hua Wu, Ji-Rong Wen, Haifeng Wang</li>
<li>for: 这个研究的目的是调查大语言模型（LLMs）在开放领域问答（QA）任务中的知识边界能力，以及如何通过检索增强来提高 LLMS 的判断能力。</li>
<li>methods: 这个研究使用了大语言模型（LLMs），包括 ChatGPT，以及检索增强技术来解决开放领域问答（QA）任务。</li>
<li>results: 研究发现，LLMs 具有不动摇的自信心，并且它们的回答准确率较高。此外，通过检索增强，LLMs 的判断能力得到了改善。同时，研究发现 LLMS 倾向于使用提供的检索结果来组织答案，而这些结果的质量对 LLMS 的依赖有着重要的影响。<details>
<summary>Abstract</summary>
Knowledge-intensive tasks (e.g., open-domain question answering (QA)) require a substantial amount of factual knowledge and often rely on external information for assistance. Recently, large language models (LLMs) (e.g., ChatGPT), have demonstrated impressive prowess in solving a wide range of tasks with world knowledge, including knowledge-intensive tasks. However, it remains unclear how well LLMs are able to perceive their factual knowledge boundaries, particularly how they behave when incorporating retrieval augmentation. In this study, we present an initial analysis of the factual knowledge boundaries of LLMs and how retrieval augmentation affects LLMs on open-domain QA. Specially, we focus on three primary research questions and analyze them by examining QA performance, priori judgement and posteriori judgement of LLMs. We show evidence that LLMs possess unwavering confidence in their capabilities to respond to questions and the accuracy of their responses. Furthermore, retrieval augmentation proves to be an effective approach in enhancing LLMs' awareness of knowledge boundaries, thereby improving their judgemental abilities. Additionally, we also find that LLMs have a propensity to rely on the provided retrieval results when formulating answers, while the quality of these results significantly impacts their reliance. The code to reproduce this work is available at https://github.com/RUCAIBox/LLM-Knowledge-Boundary.
</details>
<details>
<summary>摘要</summary>
知识密集任务（如开放领域问答（QA））需要很大的实际知识和常常依靠外部信息。最近，大型语言模型（LLMs）（如ChatGPT）在解决各种任务的能力方面表现出色，包括知识密集任务。然而，LLMs在把握实际知识边界方面的能力仍然不清楚，特别是在将检索增强纳入系统时。在本研究中，我们提出了三个主要研究问题，并通过分析LLMs的问答性能、先前判断和后来判断来研究这些问题。我们发现LLMs具有不退让的自信心，并且其回答的准确率较高。此外，我们发现检索增强是一种有效的方法，可以提高LLMs对实际知识边界的意识。此外，我们还发现LLMs倾向于根据提供的检索结果来组织答案，而这些结果的质量对LLMs的依赖度有着重要影响。相关代码可以在GitHub上找到：https://github.com/RUCAIBox/LLM-Knowledge-Boundary。
</details></li>
</ul>
<hr>
<h2 id="Integrating-Pretrained-ASR-and-LM-to-Perform-Sequence-Generation-for-Spoken-Language-Understanding"><a href="#Integrating-Pretrained-ASR-and-LM-to-Perform-Sequence-Generation-for-Spoken-Language-Understanding" class="headerlink" title="Integrating Pretrained ASR and LM to Perform Sequence Generation for Spoken Language Understanding"></a>Integrating Pretrained ASR and LM to Perform Sequence Generation for Spoken Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11005">http://arxiv.org/abs/2307.11005</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddhant Arora, Hayato Futami, Yosuke Kashiwagi, Emiru Tsunoo, Brian Yan, Shinji Watanabe</li>
<li>for: 这篇论文是为了探讨预训练语音识别（ASR）和语言模型（LM）在语义理解（SLU）框架中的集成。</li>
<li>methods: 该论文提出了一种三个过程的端到端（E2E）SLU系统，它将预训练ASR和LM子网络集成到SLU框架中进行序列生成任务。在第一个过程中，我们的架构预测ASR转译文本使用ASR子网络。接着，LM子网络会对初始SLU预测。最后，在第三个过程中，妥协子网络通过ASR和LM子网络表示进行最终预测。</li>
<li>results: 该论文的提出的三个过程SLU系统在两个标准SLU数据集上（SLURP和SLUE）表现出优于栅积和E2E SLU模型，特别是在听觉困难的语音中。<details>
<summary>Abstract</summary>
There has been an increased interest in the integration of pretrained speech recognition (ASR) and language models (LM) into the SLU framework. However, prior methods often struggle with a vocabulary mismatch between pretrained models, and LM cannot be directly utilized as they diverge from its NLU formulation. In this study, we propose a three-pass end-to-end (E2E) SLU system that effectively integrates ASR and LM subnetworks into the SLU formulation for sequence generation tasks. In the first pass, our architecture predicts ASR transcripts using the ASR subnetwork. This is followed by the LM subnetwork, which makes an initial SLU prediction. Finally, in the third pass, the deliberation subnetwork conditions on representations from the ASR and LM subnetworks to make the final prediction. Our proposed three-pass SLU system shows improved performance over cascaded and E2E SLU models on two benchmark SLU datasets, SLURP and SLUE, especially on acoustically challenging utterances.
</details>
<details>
<summary>摘要</summary>
有些研究者表示，在将预训练的语音识别（ASR）和语言模型（LM）集成到SLU框架中的 интерес增长。然而，之前的方法经常会遇到预训练模型和LM之间的词汇差异，LM无法直接使用，因为它与NLU表述不匹配。在这项研究中，我们提出了一种三个过程的SLU系统，可以有效地将ASR和LM子网络集成到SLU表述中进行序列生成任务。在第一个过程中，我们的架构预测ASR译文使用ASR子网络。接着，LM子网络会 Initial SLU预测。最后，在第三个过程中，妥协子网络使用ASR和LM子网络的表示来做最终预测。我们的提议的三个过程SLU系统在两个标准SLU数据集上（SLURP和SLUE）表现出色，特别是在具有声学挑战的语音中。
</details></li>
</ul>
<hr>
<h2 id="MASR-Metadata-Aware-Speech-Representation"><a href="#MASR-Metadata-Aware-Speech-Representation" class="headerlink" title="MASR: Metadata Aware Speech Representation"></a>MASR: Metadata Aware Speech Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10982">http://arxiv.org/abs/2307.10982</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anjali Raj, Shikhar Bharadwaj, Sriram Ganapathy, Min Ma, Shikhar Vashishth</li>
<li>for: 本文提出了一种Metadata Aware Speech Representation learning框架（MASR），用于利用外部知识来增强speech表示学习。</li>
<li>methods: 本文使用了一种基于sample-level pair-wise similarity矩阵的硬挖损失函数，以及任意选择的self-supervised learning方法。</li>
<li>results: 在多个下游任务中，包括语言识别、语音识别和非语言性任务中，MASR表示学习得到了显著的性能提升，比较其他已知标准。在语言识别任务中，文件进行了详细分析，以便理解如何使用提案的损失函数使表示分离 closely related语言。<details>
<summary>Abstract</summary>
In the recent years, speech representation learning is constructed primarily as a self-supervised learning (SSL) task, using the raw audio signal alone, while ignoring the side-information that is often available for a given speech recording. In this paper, we propose MASR, a Metadata Aware Speech Representation learning framework, which addresses the aforementioned limitations. MASR enables the inclusion of multiple external knowledge sources to enhance the utilization of meta-data information. The external knowledge sources are incorporated in the form of sample-level pair-wise similarity matrices that are useful in a hard-mining loss. A key advantage of the MASR framework is that it can be combined with any choice of SSL method. Using MASR representations, we perform evaluations on several downstream tasks such as language identification, speech recognition and other non-semantic tasks such as speaker and emotion recognition. In these experiments, we illustrate significant performance improvements for the MASR over other established benchmarks. We perform a detailed analysis on the language identification task to provide insights on how the proposed loss function enables the representations to separate closely related languages.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/21/cs.CL_2023_07_21/" data-id="clp89doa0008vi788cfgt52n5" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/21/cs.LG_2023_07_21/" class="article-date">
  <time datetime="2023-07-21T10:00:00.000Z" itemprop="datePublished">2023-07-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/21/cs.LG_2023_07_21/">cs.LG - 2023-07-21</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Enhancing-CLIP-with-GPT-4-Harnessing-Visual-Descriptions-as-Prompts"><a href="#Enhancing-CLIP-with-GPT-4-Harnessing-Visual-Descriptions-as-Prompts" class="headerlink" title="Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts"></a>Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11661">http://arxiv.org/abs/2307.11661</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mayug/vdt-adapter">https://github.com/mayug/vdt-adapter</a></li>
<li>paper_authors: Mayug Maniparambil, Chris Vorster, Derek Molloy, Noel Murphy, Kevin McGuinness, Noel E. O’Connor</li>
<li>for: 这篇论文旨在研究如何使用生成型大型视觉语言模型（VLMs）和适应器来提高视觉表示学习的性能。</li>
<li>methods: 论文使用了一种生成型大型语言模型GPT-4，通过设计相关的提问来适应CLIP模型，并使用这些提问来生成视觉描述文本。</li>
<li>results: 论文表明，使用GPT-4生成的提问可以大幅提高CLIP模型的零shot传输精度（<del>7%），并且设计了一种简单的几招 adapter 来选择最佳的句子来构建通用的分类器，该分类器可以超过 CoCoOP 的表现（</del>2%）。<details>
<summary>Abstract</summary>
Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have revolutionized visual representation learning by providing good performance on downstream datasets. VLMs are 0-shot adapted to a downstream dataset by designing prompts that are relevant to the dataset. Such prompt engineering makes use of domain expertise and a validation dataset. Meanwhile, recent developments in generative pretrained models like GPT-4 mean they can be used as advanced internet search tools. They can also be manipulated to provide visual information in any structure. In this work, we show that GPT-4 can be used to generate text that is visually descriptive and how this can be used to adapt CLIP to downstream tasks. We show considerable improvements in 0-shot transfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD (~7%), SUN397 (~4.6%), and CUB (~3.3%) when compared to CLIP's default prompt. We also design a simple few-shot adapter that learns to choose the best possible sentences to construct generalizable classifiers that outperform the recently proposed CoCoOP by ~2% on average and by over 4% on 4 specialized fine-grained datasets. The code, prompts, and auxiliary text dataset is available at https://github.com/mayug/VDT-Adapter.
</details>
<details>
<summary>摘要</summary>
带有对比学习的大型视力语言模型（VLM）如CLIP，已经革命化视觉表示学习。VLM可以通过设计相关的提问来适应下游数据集，而这种提问工程充分利用了领域专业知识和验证数据集。同时，最新的生成预训练模型如GPT-4，可以用作高级网络搜索工具，同时也可以通过提供视觉信息的任意结构来 manipulate。在这项工作中，我们示出了使用GPT-4生成文本可以提供视觉描述，并如何使用这种方法适应CLIP到下游任务。我们在特殊细化数据集上（如EuroSAT、DTD、SUN397和CUB）表现出了 considerable 的提升（~7%、~7%、~4.6%和~3.3%），相比CLIP的默认提问。我们还设计了一个简单的几招学习器，可以选择最佳的句子来构建通用的分类器，超过CoCoOP的最近提出的 ~2% 的平均提升和 ~4% 的特殊细化数据集上的提升。代码、提问和辅助文本数据集可以在 https://github.com/mayug/VDT-Adapter 上找到。
</details></li>
</ul>
<hr>
<h2 id="Bandits-with-Deterministically-Evolving-States"><a href="#Bandits-with-Deterministically-Evolving-States" class="headerlink" title="Bandits with Deterministically Evolving States"></a>Bandits with Deterministically Evolving States</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11655">http://arxiv.org/abs/2307.11655</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khashayar Khosravi, Renato Paes Leme, Chara Podimata, Apostolis Tsorvantzis</li>
<li>for: 学习渠道反馈中的抽象渠道问题，特别是在推荐系统和在线广告中。</li>
<li>methods: 使用多支武器抽象渠道模型，考虑渠道状态的演化和不可见性。</li>
<li>results: 提出了一种受到演化状态的抽象渠道模型，并对这种模型进行了线上学习算法的分析，包括对任何可能的演化率$\lambda$的分析。结果显示，在不同的演化率下，算法的 regret 率可以达到 $\widetilde O(\sqrt{KT})$、$\widetilde O(T^{b&#x2F;a})$ 和 $\widetilde O(K^{1&#x2F;3}T^{2&#x2F;3})$ 等 bound。<details>
<summary>Abstract</summary>
We propose a model for learning with bandit feedback while accounting for deterministically evolving and unobservable states that we call Bandits with Deterministically Evolving States. The workhorse applications of our model are learning for recommendation systems and learning for online ads. In both cases, the reward that the algorithm obtains at each round is a function of the short-term reward of the action chosen and how ``healthy'' the system is (i.e., as measured by its state). For example, in recommendation systems, the reward that the platform obtains from a user's engagement with a particular type of content depends not only on the inherent features of the specific content, but also on how the user's preferences have evolved as a result of interacting with other types of content on the platform. Our general model accounts for the different rate $\lambda \in [0,1]$ at which the state evolves (e.g., how fast a user's preferences shift as a result of previous content consumption) and encompasses standard multi-armed bandits as a special case. The goal of the algorithm is to minimize a notion of regret against the best-fixed sequence of arms pulled. We analyze online learning algorithms for any possible parametrization of the evolution rate $\lambda$. Specifically, the regret rates obtained are: for $\lambda \in [0, 1/T^2]$: $\widetilde O(\sqrt{KT})$; for $\lambda = T^{-a/b}$ with $b < a < 2b$: $\widetilde O (T^{b/a})$; for $\lambda \in (1/T, 1 - 1/\sqrt{T}): \widetilde O (K^{1/3}T^{2/3})$; and for $\lambda \in [1 - 1/\sqrt{T}, 1]: \widetilde O (K\sqrt{T})$.
</details>
<details>
<summary>摘要</summary>
我们提出一种学习模型，称为带有束缚演化的带刺机制（Bandits with Deterministically Evolving States）。该模型的应用场景包括推荐系统和在线广告学习。在这两种情况下，算法在每个轮次获得的奖励是基于选择的动作的短期奖励和系统的状态。例如，在推荐系统中，用户对于特定类型的内容的互动奖励取决于内容的特性以及用户在前一次互动后的偏好的演化程度。我们的通用模型考虑了不同的演化速率 $\lambda \in [0,1]$，并包括标准多重机枪为特例。算法的目标是在最佳固定序列的机枪上取得最小的误差。我们分析了在任何可能的参数化 $\lambda$ 下的在线学习算法，并得到了以下 regret 率：对 $\lambda \in [0, 1/T^2]$， $\widetilde O(\sqrt{KT})$；对 $\lambda = T^{-a/b}$， $b < a < 2b$， $\widetilde O (T^{b/a})$；对 $\lambda \in (1/T, 1 - 1/\sqrt{T})$， $\widetilde O (K^{1/3}T^{2/3})$；对 $\lambda \in [1 - 1/\sqrt{T}, 1]$， $\widetilde O (K\sqrt{T})$。
</details></li>
</ul>
<hr>
<h2 id="Scalable-Multi-agent-Covering-Option-Discovery-based-on-Kronecker-Graphs"><a href="#Scalable-Multi-agent-Covering-Option-Discovery-based-on-Kronecker-Graphs" class="headerlink" title="Scalable Multi-agent Covering Option Discovery based on Kronecker Graphs"></a>Scalable Multi-agent Covering Option Discovery based on Kronecker Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11629">http://arxiv.org/abs/2307.11629</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayu Chen, Jingdi Chen, Tian Lan, Vaneet Aggarwal</li>
<li>for: This paper is written for improving the exploration of reinforcement learning (RL) in single-agent scenarios with sparse reward signals, and enabling the ease of decomposition in multi-agent systems.</li>
<li>methods: The paper proposes a multi-agent skill discovery method that approximates the joint state space as a Kronecker graph, and estimates its Fiedler vector using the Laplacian spectrum of individual agents’ transition graphs. The method also includes a deep learning extension using NN-based representation learning techniques.</li>
<li>results: The proposed algorithm is evaluated on multi-agent tasks built with simulators like Mujoco, and shows significant outperformance compared to the state-of-the-art.Here is the text in Simplified Chinese:</li>
<li>for: 这篇论文是为了改进单个机器人的奖励学习（RL）探索，以及在多机器人系统中实现拆分的容易性。</li>
<li>methods: 这篇论文提出了一种多机器人技能发现方法，通过将 JOINT 状态空间approximate为Kronecker图，并使用个体机器人的过渡图 Laplacian  спектrum 来估算 JOINT 状态空间的Fiedler вектор。方法还包括一种基于深度学习的扩展，通过使用 NN-based 表示学习技术来估算 eigenfunctions。</li>
<li>results: 提出的算法在使用 Mujoco 等模拟器构建的多机器人任务上进行评估，并显示出了明显的超越现有的state-of-the-art。<details>
<summary>Abstract</summary>
Covering skill (a.k.a., option) discovery has been developed to improve the exploration of RL in single-agent scenarios with sparse reward signals, through connecting the most distant states in the embedding space provided by the Fiedler vector of the state transition graph. Given that joint state space grows exponentially with the number of agents in multi-agent systems, existing researches still relying on single-agent skill discovery either become prohibitive or fail to directly discover joint skills that improve the connectivity of the joint state space. In this paper, we propose multi-agent skill discovery which enables the ease of decomposition. Our key idea is to approximate the joint state space as a Kronecker graph, based on which we can directly estimate its Fiedler vector using the Laplacian spectrum of individual agents' transition graphs. Further, considering that directly computing the Laplacian spectrum is intractable for tasks with infinite-scale state spaces, we further propose a deep learning extension of our method by estimating eigenfunctions through NN-based representation learning techniques. The evaluation on multi-agent tasks built with simulators like Mujoco, shows that the proposed algorithm can successfully identify multi-agent skills, and significantly outperforms the state-of-the-art. Codes are available at: https://github.itap.purdue.edu/Clan-labs/Scalable_MAOD_via_KP.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用简化中文表述文本。<</SYS>>利用技能探索（即选项）发展来改进单机RL探索，通过连接状态transition图中的最远状态点。由于多机体系中的共同状态空间 exponentiation  WITH  agent数量，现有的研究仍然依赖单机体系技能探索，导致计算拥堵或直接探索多机体系技能而不可能。在这篇论文中，我们提出了多机体系技能探索，允许拆分。我们的关键思想是将共同状态空间approximer为Kronecker图，基于这个图可以直接估算其Fiedler вектор使用个体代理的过程图laplacian spectrum。此外，由于直接计算laplacian spectrum是任务 WITH 无穷大状态空间中的不可能，我们进一步提出了基于深度学习的方法，通过NN-based representation learning技术来估算eigenfunctions。在使用Mujoco等模拟器constructed的多机任务上进行评估，我们的方法可以成功地确定多机技能，并在与当前最佳方法进行比较时显著超越。代码可以在https://github.itap.purdue.edu/Clan-labs/Scalable_MAOD_via_KP中找到。
</details></li>
</ul>
<hr>
<h2 id="Offline-Multi-Agent-Reinforcement-Learning-with-Implicit-Global-to-Local-Value-Regularization"><a href="#Offline-Multi-Agent-Reinforcement-Learning-with-Implicit-Global-to-Local-Value-Regularization" class="headerlink" title="Offline Multi-Agent Reinforcement Learning with Implicit Global-to-Local Value Regularization"></a>Offline Multi-Agent Reinforcement Learning with Implicit Global-to-Local Value Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11620">http://arxiv.org/abs/2307.11620</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangsen Wang, Haoran Xu, Yinan Zheng, Xianyuan Zhan</li>
<li>for: Offline multi-agent reinforcement learning (MARL)</li>
<li>methods: Implicit global-to-local value regularization, in-sample learning</li>
<li>results: Superior performance over state-of-the-art offline MARL methods in almost all tasks, as demonstrated through comprehensive experiments on the offline multi-agent MuJoCo and StarCraft II micro-management tasks.<details>
<summary>Abstract</summary>
Offline reinforcement learning (RL) has received considerable attention in recent years due to its attractive capability of learning policies from offline datasets without environmental interactions. Despite some success in the single-agent setting, offline multi-agent RL (MARL) remains to be a challenge. The large joint state-action space and the coupled multi-agent behaviors pose extra complexities for offline policy optimization. Most existing offline MARL studies simply apply offline data-related regularizations on individual agents, without fully considering the multi-agent system at the global level. In this work, we present OMIGA, a new offline m ulti-agent RL algorithm with implicit global-to-local v alue regularization. OMIGA provides a principled framework to convert global-level value regularization into equivalent implicit local value regularizations and simultaneously enables in-sample learning, thus elegantly bridging multi-agent value decomposition and policy learning with offline regularizations. Based on comprehensive experiments on the offline multi-agent MuJoCo and StarCraft II micro-management tasks, we show that OMIGA achieves superior performance over the state-of-the-art offline MARL methods in almost all tasks.
</details>
<details>
<summary>摘要</summary>
偏向式学习（RL）在线下学习（Offline RL）方面已经吸引了一些注意，因为它可以从在线数据集中学习策略而不需要环境交互。然而，多智能体RL（MARL）在线下仍然是一个挑战。大量共同状态-动作空间和多智能体行为的交互增加了在线策略优化的复杂性。大多数现有的在线MARL研究只是对个体代理应用在线数据相关的正则化，没有全面考虑多智能体系统的全局水平。在这种情况下，我们提出了OMIGA，一种新的在线多智能体RL算法，其中包含隐式全局到本地值正则化。OMIGA可以将全局水平的值正则化转换为等效的隐式本地值正则化，同时允许在样本中学习，因此简洁地结合多智能体值分解和策略学习，并且使用在线正则化。根据对多智能体MuJoCo和StarCraft II微管理任务的完整实验，我们表明OMIGA在大多数任务上表现出优于当前最佳的在线MARL方法。
</details></li>
</ul>
<hr>
<h2 id="Robust-Fully-Asynchronous-Methods-for-Distributed-Training-over-General-Architecture"><a href="#Robust-Fully-Asynchronous-Methods-for-Distributed-Training-over-General-Architecture" class="headerlink" title="Robust Fully-Asynchronous Methods for Distributed Training over General Architecture"></a>Robust Fully-Asynchronous Methods for Distributed Training over General Architecture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11617">http://arxiv.org/abs/2307.11617</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zehan Zhu, Ye Tian, Yan Huang, Jinming Xu, Shibo He</li>
<li>for: 提高分布式机器学习问题中的同步效率和稳定性，因为存在延迟、包失和偏移者等问题。</li>
<li>methods: 提出了一种Robust Fully-Asynchronous Stochastic Gradient Tracking方法（R-FAST），每个设备都可以在自己的pace进行本地计算和通信，不需要任何同步。与现有的异步分布式算法不同，R-FAST可以消除设备间数据不同性的影响和 packet loss 的问题，通过设计合适的副本变量来跟踪和缓存总导数向量。</li>
<li>results: R-FAST 可以在平均情况下达到一个邻域的优点，具有平方根速率和强共形性的目标函数，以及一个子线性速率的总体非共形目标函数。实验表明，R-FAST 比同步 refer 算法（如环形 AllReduce 和 D-PSGD）快 1.5-2 倍，同时保持相似的精度，并超过现有的异步 SOTA 算法（如 AD-PSGD 和 OSGP），特别是在存在偏移者的情况下。<details>
<summary>Abstract</summary>
Perfect synchronization in distributed machine learning problems is inefficient and even impossible due to the existence of latency, package losses and stragglers. We propose a Robust Fully-Asynchronous Stochastic Gradient Tracking method (R-FAST), where each device performs local computation and communication at its own pace without any form of synchronization. Different from existing asynchronous distributed algorithms, R-FAST can eliminate the impact of data heterogeneity across devices and allow for packet losses by employing a robust gradient tracking strategy that relies on properly designed auxiliary variables for tracking and buffering the overall gradient vector. More importantly, the proposed method utilizes two spanning-tree graphs for communication so long as both share at least one common root, enabling flexible designs in communication architectures. We show that R-FAST converges in expectation to a neighborhood of the optimum with a geometric rate for smooth and strongly convex objectives; and to a stationary point with a sublinear rate for general non-convex settings. Extensive experiments demonstrate that R-FAST runs 1.5-2 times faster than synchronous benchmark algorithms, such as Ring-AllReduce and D-PSGD, while still achieving comparable accuracy, and outperforms existing asynchronous SOTA algorithms, such as AD-PSGD and OSGP, especially in the presence of stragglers.
</details>
<details>
<summary>摘要</summary>
完美同步在分布式机器学习问题上是不可取和无法实现，因为存在延迟、包装丢失和偏移器。我们提议一种Robust Fully-Asynchronous Stochastic Gradient Tracking方法（R-FAST），其中每个设备都会在自己的速度和方式下进行本地计算和通信，不需要任何同步。与现有的异步分布式算法不同，R-FAST可以消除设备之间数据不一致的影响，并且能够承受 packet 丢失，通过设计合适的辅助变量来跟踪和缓存总Gradient向量。此外，我们利用了两个拓扑图进行通信，只要这两个拓扑图至少有一个共同的根节点，然后可以实现 flexible 的通信架构。我们证明了R-FAST在各种情况下都可以减少到预期的一个邻域中的优化率，包括圆满和强烈的对称问题。同时，R-FAST在一般非对称问题下也可以到达一个站点点的稳定点，但是速度是不可预测的。我们的实验表明，R-FAST比同步标准算法快1.5-2倍，同时仍能保持相同的准确率，并且在存在偏移器的情况下比现有的异步SOTA算法更高效。
</details></li>
</ul>
<hr>
<h2 id="Persistent-Ballistic-Entanglement-Spreading-with-Optimal-Control-in-Quantum-Spin-Chains"><a href="#Persistent-Ballistic-Entanglement-Spreading-with-Optimal-Control-in-Quantum-Spin-Chains" class="headerlink" title="Persistent Ballistic Entanglement Spreading with Optimal Control in Quantum Spin Chains"></a>Persistent Ballistic Entanglement Spreading with Optimal Control in Quantum Spin Chains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11609">http://arxiv.org/abs/2307.11609</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ying Lu, Pei Shi, Xiao-Han Wang, Jie Hu, Shi-Ju Ran</li>
<li>for: 这个论文是研究量子多体系统的动态行为的关键方法之一，即束缚扩散。</li>
<li>methods: 这篇论文使用了变量束缚增强场（VEEF）来induce持续弹性扩散束缚。VEEF是时间依赖的，并可以最优化以 maximize维度束缚 entropy（EE）的最终态。</li>
<li>results: 研究发现，在VEEF的控制下，束缚 entropy（EE）会 linearly增长，直到EE到达真正的极限值 $\tilde{S} &#x3D; - \log_{2} 2^{-\frac{N}{2} &#x3D; \frac{N}{2}$。在这个过程中，EE的增长速度是velocity $v$，其中$v \about 2.76$, $4.98$,和$5.75$分别对应于各种束缚（Ising、XY、Heisenberg）。此外，研究还发现了在长距离相互作用下出现非线性增长的束缚。<details>
<summary>Abstract</summary>
Entanglement propagation provides a key routine to understand quantum many-body dynamics in and out of equilibrium. In this work, we uncover that the ``variational entanglement-enhancing'' field (VEEF) robustly induces a persistent ballistic spreading of entanglement in quantum spin chains. The VEEF is time dependent, and is optimally controlled to maximize the bipartite entanglement entropy (EE) of the final state. Such a linear growth persists till the EE reaches the genuine saturation $\tilde{S} = - \log_{2} 2^{-\frac{N}{2}=\frac{N}{2}$ with $N$ the total number of spins. The EE satisfies $S(t) = v t$ for the time $t \leq \frac{N}{2v}$, with $v$ the velocity. These results are in sharp contrast with the behaviors without VEEF, where the EE generally approaches a sub-saturation known as the Page value $\tilde{S}_{P} =\tilde{S} - \frac{1}{2\ln{2}$ in the long-time limit, and the entanglement growth deviates from being linear before the Page value is reached. The dependence between the velocity and interactions is explored, with $v \simeq 2.76$, $4.98$, and $5.75$ for the spin chains with Ising, XY, and Heisenberg interactions, respectively. We further show that the nonlinear growth of EE emerges with the presence of long-range interactions.
</details>
<details>
<summary>摘要</summary>
Entanglement 传播提供了量子多体动力学中重要的键式程序，以解释在和离EQilibrium中的动力学行为。在这个工作中，我们发现了“可变性增强”的场（VEEF）可以坚定地促进量子磁链中的抽象协议射频衰减。VEEF是时间依赖的，并且可以优化地控制以最大化磁链的二分 Entanglement entropy（EE）的最终状态。这种线性增长持续到EE到达真正的极限 $\tilde{S} = - \log_{2} 2^{-\frac{N}{2}=\frac{N}{2}$ 的时间 $t \leq \frac{N}{2v}$，其中 $v$ 是速度。这些结果与无VEEF情况下的行为不同，其中EE通常在长时间后达到一个下降的值known as the Page value $\tilde{S}_{P} =\tilde{S} - \frac{1}{2\ln{2}$，而且协议射频的增长不是线性的。我们还发现了在存在长距离交互时，非线性增长的EE现象。
</details></li>
</ul>
<hr>
<h2 id="Learning-minimal-representations-of-stochastic-processes-with-variational-autoencoders"><a href="#Learning-minimal-representations-of-stochastic-processes-with-variational-autoencoders" class="headerlink" title="Learning minimal representations of stochastic processes with variational autoencoders"></a>Learning minimal representations of stochastic processes with variational autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11608">http://arxiv.org/abs/2307.11608</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gabrielfernandezfernandez/spivae">https://github.com/gabrielfernandezfernandez/spivae</a></li>
<li>paper_authors: Gabriel Fernández-Fernández, Carlo Manzo, Maciej Lewenstein, Alexandre Dauphin, Gorka Muñoz-Gil</li>
<li>for: 研究者使用无监督机器学习方法自动发现随机过程中不确定参数的最小集合。</li>
<li>methods: 研究者使用扩展β-variational autoencoder建模方法，通过模拟数据集来证明方法的有效性。</li>
<li>results: 方法可以准确地描述随机过程的动态，并且可以生成符合预期随机行为的新轨迹。<details>
<summary>Abstract</summary>
Stochastic processes have found numerous applications in science, as they are broadly used to model a variety of natural phenomena. Due to their intrinsic randomness and uncertainty, they are however difficult to characterize. Here, we introduce an unsupervised machine learning approach to determine the minimal set of parameters required to effectively describe the dynamics of a stochastic process. Our method builds upon an extended $\beta$-variational autoencoder architecture. By means of simulated datasets corresponding to paradigmatic diffusion models, we showcase its effectiveness in extracting the minimal relevant parameters that accurately describe these dynamics. Furthermore, the method enables the generation of new trajectories that faithfully replicate the expected stochastic behavior. Overall, our approach enables for the autonomous discovery of unknown parameters describing stochastic processes, hence enhancing our comprehension of complex phenomena across various fields.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is also known as "简化字" or "简化字".Please note that the translation is done using the "free" translation tool provided by Google, and may not be 100% accurate or idiomatic.
</details></li>
</ul>
<hr>
<h2 id="Finding-Optimal-Diverse-Feature-Sets-with-Alternative-Feature-Selection"><a href="#Finding-Optimal-Diverse-Feature-Sets-with-Alternative-Feature-Selection" class="headerlink" title="Finding Optimal Diverse Feature Sets with Alternative Feature Selection"></a>Finding Optimal Diverse Feature Sets with Alternative Feature Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11607">http://arxiv.org/abs/2307.11607</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jakob-bach/alternative-feature-selection">https://github.com/jakob-bach/alternative-feature-selection</a></li>
<li>paper_authors: Jakob Bach</li>
<li>for: 本研究旨在提出一种新的特征选择方法，以提供多个可能的特征集，从而为用户提供不同的解释方案。</li>
<li>methods: 本研究使用了约束来定义备选特征集，并允许用户控制备选集的数量和差异度。</li>
<li>results: 研究发现，使用备选特征集可以获得高精度预测模型，并且分析了一些影响这种结果的因素。<details>
<summary>Abstract</summary>
Feature selection is popular for obtaining small, interpretable, yet highly accurate prediction models. Conventional feature-selection methods typically yield one feature set only, which might not suffice in some scenarios. For example, users might be interested in finding alternative feature sets with similar prediction quality, offering different explanations of the data. In this article, we introduce alternative feature selection and formalize it as an optimization problem. In particular, we define alternatives via constraints and enable users to control the number and dissimilarity of alternatives. Next, we analyze the complexity of this optimization problem and show NP-hardness. Further, we discuss how to integrate conventional feature-selection methods as objectives. Finally, we evaluate alternative feature selection with 30 classification datasets. We observe that alternative feature sets may indeed have high prediction quality, and we analyze several factors influencing this outcome.
</details>
<details>
<summary>摘要</summary>
<<SYS>>通过选择特征来获得小、可解释、具有高准确性预测模型的做法非常受欢迎。传统的特征选择方法通常只能提供一个特征集，这可能无法满足一些场景中的需求。例如，用户可能会有兴趣找到不同的特征集，这些特征集可以提供不同的数据解释，同时具有相似的预测质量。在这篇文章中，我们将介绍代替特征选择，并将其формализова为优化问题。具体来说，我们将通过约束定义代替，并让用户控制代替的数量和差异。接下来，我们分析了这个优化问题的复杂性，并证明其NP困难。此外，我们还讨论了如何 интеGRATE传统的特征选择方法作为目标。最后，我们对30种分类 datasets进行了评估，并发现代替特征集可以具有高预测质量。我们还分析了一些影响这种结果的因素。
</details></li>
</ul>
<hr>
<h2 id="Transferability-of-Convolutional-Neural-Networks-in-Stationary-Learning-Tasks"><a href="#Transferability-of-Convolutional-Neural-Networks-in-Stationary-Learning-Tasks" class="headerlink" title="Transferability of Convolutional Neural Networks in Stationary Learning Tasks"></a>Transferability of Convolutional Neural Networks in Stationary Learning Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11588">http://arxiv.org/abs/2307.11588</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/damowerko/mtt">https://github.com/damowerko/mtt</a></li>
<li>paper_authors: Damian Owerko, Charilaos I. Kanatsoulis, Jennifer Bondarchuk, Donald J. Bucci Jr, Alejandro Ribeiro</li>
<li>for: This paper is written for those interested in efficient training of convolutional neural networks (CNNs) for large-scale spatial problems.</li>
<li>methods: The paper investigates the properties of CNNs for tasks where the underlying signals are stationary, and proposes a novel framework for efficient training of CNNs on small windows of such signals.</li>
<li>results: The paper shows that the proposed framework achieves nearly optimal performance on much larger windows without retraining, and demonstrates this through theoretical analysis and experimental analysis on two tasks: multi-target tracking and mobile infrastructure on demand. The results show that the CNN is able to tackle problems with many hundreds of agents after being trained with fewer than ten.<details>
<summary>Abstract</summary>
Recent advances in hardware and big data acquisition have accelerated the development of deep learning techniques. For an extended period of time, increasing the model complexity has led to performance improvements for various tasks. However, this trend is becoming unsustainable and there is a need for alternative, computationally lighter methods. In this paper, we introduce a novel framework for efficient training of convolutional neural networks (CNNs) for large-scale spatial problems. To accomplish this we investigate the properties of CNNs for tasks where the underlying signals are stationary. We show that a CNN trained on small windows of such signals achieves a nearly performance on much larger windows without retraining. This claim is supported by our theoretical analysis, which provides a bound on the performance degradation. Additionally, we conduct thorough experimental analysis on two tasks: multi-target tracking and mobile infrastructure on demand. Our results show that the CNN is able to tackle problems with many hundreds of agents after being trained with fewer than ten. Thus, CNN architectures provide solutions to these problems at previously computationally intractable scales.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:latest advances in hardware and big data acquisition have accelerated the development of deep learning techniques. for a long time, increasing the model complexity has led to performance improvements for various tasks. however, this trend is becoming unsustainable, and there is a need for alternative, computationally lighter methods. in this paper, we introduce a novel framework for efficient training of convolutional neural networks (CNNs) for large-scale spatial problems. to accomplish this, we investigate the properties of CNNs for tasks where the underlying signals are stationary. we show that a CNN trained on small windows of such signals achieves nearly the same performance on much larger windows without retraining. this claim is supported by our theoretical analysis, which provides a bound on the performance degradation. additionally, we conduct thorough experimental analysis on two tasks: multi-target tracking and mobile infrastructure on demand. our results show that the CNN is able to tackle problems with many hundreds of agents after being trained with fewer than ten. thus, CNN architectures provide solutions to these problems at previously computationally intractable scales.
</details></li>
</ul>
<hr>
<h2 id="A-Change-of-Heart-Improving-Speech-Emotion-Recognition-through-Speech-to-Text-Modality-Conversion"><a href="#A-Change-of-Heart-Improving-Speech-Emotion-Recognition-through-Speech-to-Text-Modality-Conversion" class="headerlink" title="A Change of Heart: Improving Speech Emotion Recognition through Speech-to-Text Modality Conversion"></a>A Change of Heart: Improving Speech Emotion Recognition through Speech-to-Text Modality Conversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11584">http://arxiv.org/abs/2307.11584</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iclr2023achangeofheart/meld-modality-conversion">https://github.com/iclr2023achangeofheart/meld-modality-conversion</a></li>
<li>paper_authors: Zeinab Sadat Taghavi, Ali Satvaty, Hossein Sameti</li>
<li>for: 提高语音情感识别（SER）的性能，特别是在MELD数据集上。</li>
<li>methods: 使用自动语音识别（ASR）系统，然后使用文本分类器进行模式转换。在这个过程中，我们首先使用ASR系统将语音转换为文本，然后使用文本分类器进行模式转换。</li>
<li>results: 我们的方法在MELD数据集上实现了substantial的result，而Modality-Conversion++方法在语音基于的approaches中实现了最高的SERWF1分数。这些结果表明，模式转换可以帮助提高tasks的性能，特别是在不同的modalities上进行的任务。<details>
<summary>Abstract</summary>
Speech Emotion Recognition (SER) is a challenging task. In this paper, we introduce a modality conversion concept aimed at enhancing emotion recognition performance on the MELD dataset. We assess our approach through two experiments: first, a method named Modality-Conversion that employs automatic speech recognition (ASR) systems, followed by a text classifier; second, we assume perfect ASR output and investigate the impact of modality conversion on SER, this method is called Modality-Conversion++. Our findings indicate that the first method yields substantial results, while the second method outperforms state-of-the-art (SOTA) speech-based approaches in terms of SER weighted-F1 (WF1) score on the MELD dataset. This research highlights the potential of modality conversion for tasks that can be conducted in alternative modalities.
</details>
<details>
<summary>摘要</summary>
《语音情感识别（SER）是一项复杂的任务。在这篇论文中，我们介绍了一种模态转换概念，用于提高MELD数据集上的情感识别性能。我们通过两个实验进行评估：第一个方法是使用自动语音识别（ASR）系统，然后使用文本分类器；第二个方法是假设ASR输出是完美的，并调查模态转换对SER的影响。我们的发现表明第一个方法具有显著的效果，而第二个方法在MELD数据集上的SERWeighted-F1（WF1）分数超过了现有的speech-based方法。这项研究强调了模态转换在可以进行多种模态任务的场景中的潜力。》
</details></li>
</ul>
<hr>
<h2 id="Design-Space-Exploration-on-Efficient-and-Accurate-Human-Pose-Estimation-from-Sparse-IMU-Sensing"><a href="#Design-Space-Exploration-on-Efficient-and-Accurate-Human-Pose-Estimation-from-Sparse-IMU-Sensing" class="headerlink" title="Design Space Exploration on Efficient and Accurate Human Pose Estimation from Sparse IMU-Sensing"></a>Design Space Exploration on Efficient and Accurate Human Pose Estimation from Sparse IMU-Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02397">http://arxiv.org/abs/2308.02397</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/itiv-kit/dse-sparse-imu">https://github.com/itiv-kit/dse-sparse-imu</a></li>
<li>paper_authors: Iris Fürst-Walter, Antonio Nappi, Tanja Harbaum, Jürgen Becker</li>
<li>for: 本研究旨在为体育、重abilitation或工作安全领域的人体动态评估（HPE）提供准确的感知，而不需要泄露敏感个人数据。因此，本地处理是必要的，而且由于限制的能量预算，IMU（惯性测量单元）成为了更好的选择。</li>
<li>methods: 本研究使用了 simulative Design Space Exploration（DSE）来研究变量IMU传感器的数量和位置的影响。首先，我们生成了基于公共可用的人体模型数据集的IMU数据，并使用深度学习模型进行训练。此外，我们提出了一个结合约束的精度-资源交互度量来评估感知器的配置。</li>
<li>results: 我们的研究表明，对于一个与精度和资源平衡的系统，可以通过选择4个感知器的优质配置，来提高精度32.7%，同时减少硬件尝试量 by two个感知器。我们的工作可以用于设计关注隐私和资源管理的健康应用程序。<details>
<summary>Abstract</summary>
Human Pose Estimation (HPE) to assess human motion in sports, rehabilitation or work safety requires accurate sensing without compromising the sensitive underlying personal data. Therefore, local processing is necessary and the limited energy budget in such systems can be addressed by Inertial Measurement Units (IMU) instead of common camera sensing. The central trade-off between accuracy and efficient use of hardware resources is rarely discussed in research. We address this trade-off by a simulative Design Space Exploration (DSE) of a varying quantity and positioning of IMU-sensors. First, we generate IMU-data from a publicly available body model dataset for different sensor configurations and train a deep learning model with this data. Additionally, we propose a combined metric to assess the accuracy-resource trade-off. We used the DSE as a tool to evaluate sensor configurations and identify beneficial ones for a specific use case. Exemplary, for a system with equal importance of accuracy and resources, we identify an optimal sensor configuration of 4 sensors with a mesh error of 6.03 cm, increasing the accuracy by 32.7% and reducing the hardware effort by two sensors compared to state of the art. Our work can be used to design health applications with well-suited sensor positioning and attention to data privacy and resource-awareness.
</details>
<details>
<summary>摘要</summary>
人体姿势估算（HPE）在运动、重abilitation或工作安全中需要准确感知而不妨碍敏感个人数据的保护。因此，本地处理是必要的，而常见的相机感知可能会占用过多的硬件资源。中心的准确性和硬件资源的有效使用之间的贸易是在研究中 rarely 被讨论。我们通过 simulative 的 Design Space Exploration（DSE）来解决这个贸易，并通过不同的 IMU 传感器的数量和位置来调整 IMU 数据。首先，我们使用公共可用的人体模型集来生成 IMU 数据，并使用这些数据来训练深度学习模型。此外，我们提出了一个合并的度量来评估准确性和资源之间的贸易。通过 DSE 作为工具，我们可以评估不同的传感器配置，并为特定应用场景选择有利的传感器配置。例如，对于需要同时保证准确性和资源的系统，我们identified 最佳的传感器配置为4个传感器，具有6.03 cm的网格误差，提高准确性32.7%，并在相对于状态艺术的系统中减少硬件努力两个传感器。我们的工作可以用于设计健康应用程序，并且注重数据隐私和硬件资源的注意。
</details></li>
</ul>
<hr>
<h2 id="FMT-Removing-Backdoor-Feature-Maps-via-Feature-Map-Testing-in-Deep-Neural-Networks"><a href="#FMT-Removing-Backdoor-Feature-Maps-via-Feature-Map-Testing-in-Deep-Neural-Networks" class="headerlink" title="FMT: Removing Backdoor Feature Maps via Feature Map Testing in Deep Neural Networks"></a>FMT: Removing Backdoor Feature Maps via Feature Map Testing in Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11565">http://arxiv.org/abs/2307.11565</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ase2023paper/fmt">https://github.com/ase2023paper/fmt</a></li>
<li>paper_authors: Dong Huang, Qingwen Bu, Yahao Qing, Yichao Fu, Heming Cui</li>
<li>for: 本研究旨在提出一种新的防御策略，以防止深度神经网络（DNN）模型受到攻击。</li>
<li>methods: 本研究使用了特征图测试（Feature Map Testing，FMT）方法，不同于现有的防御策略，FMT尝试探测DNN模型中的后门特征图，然后将其消除，并使用安全的subset of training data进行微调。</li>
<li>results: 对比现有防御策略，FMT可以更好地降低攻击成功率（ASR），并保持模型性能高。此外，FMT的Robust Accuracy（RA）比 conventinal defense methods高，表明其在mitigating the effects of backdoor attacks时可以更好地保持模型性能。<details>
<summary>Abstract</summary>
Deep neural networks have been widely used in many critical applications, such as autonomous vehicles and medical diagnosis. However, their security is threatened by backdoor attack, which is achieved by adding artificial patterns to specific training data. Existing defense strategies primarily focus on using reverse engineering to reproduce the backdoor trigger generated by attackers and subsequently repair the DNN model by adding the trigger into inputs and fine-tuning the model with ground-truth labels. However, once the trigger generated by the attackers is complex and invisible, the defender can not successfully reproduce the trigger. Consequently, the DNN model will not be repaired since the trigger is not effectively removed.   In this work, we propose Feature Map Testing~(FMT). Different from existing defense strategies, which focus on reproducing backdoor triggers, FMT tries to detect the backdoor feature maps, which are trained to extract backdoor information from the inputs. After detecting these backdoor feature maps, FMT will erase them and then fine-tune the model with a secure subset of training data. Our experiments demonstrate that, compared to existing defense strategies, FMT can effectively reduce the Attack Success Rate (ASR) even against the most complex and invisible attack triggers. Second, unlike conventional defense methods that tend to exhibit low Robust Accuracy (i.e., the model's accuracy on the poisoned data), FMT achieves higher RA, indicating its superiority in maintaining model performance while mitigating the effects of backdoor attacks~(e.g., FMT obtains 87.40\% RA in CIFAR10). Third, compared to existing feature map pruning techniques, FMT can cover more backdoor feature maps~(e.g., FMT removes 83.33\% of backdoor feature maps from the model in the CIFAR10 \& BadNet scenario).
</details>
<details>
<summary>摘要</summary>
深度神经网络在许多关键应用中广泛使用，如自动驾驶和医疗诊断。然而，它们的安全性受到后门攻击的威胁，后门攻击通过添加人工 patrerns到特定的训练数据来实现。现有的防御策略主要集中在使用反工程来复制攻击者生成的后门触发器，并在使用ground-truth标签进行修复模型。然而，如果攻击者生成的后门触发器复杂且隐藏的话，DEFENDER无法成功复制它。因此，模型不会被修复，因为触发器不会有效地除除。在这种情况下，我们提出了特征图测试~(FMT)。与现有的防御策略不同，FMT尝试探测backdoor特征图，它们是用于从输入中提取backdoor信息的。一旦探测到这些backdoor特征图，FMT会将它们清除，然后使用安全的训练数据进行微调。我们的实验表明，相比于现有的防御策略，FMT可以更有效地减少攻击成功率，即使攻击者生成的触发器非常复杂和隐藏。二、与传统防御方法不同，FMT可以保持模型性能的高度，而不是产生低Robust Accuracy（i.e.,模型在恶意数据上的准确率）。三、与现有的特征图剔除技术相比，FMT可以覆盖更多的backdoor特征图，例如在CIFAR10 & BadNet场景中，FMT可以从模型中删除83.33%的backdoor特征图。
</details></li>
</ul>
<hr>
<h2 id="A-multi-modal-representation-of-El-Nino-Southern-Oscillation-Diversity"><a href="#A-multi-modal-representation-of-El-Nino-Southern-Oscillation-Diversity" class="headerlink" title="A multi-modal representation of El Niño Southern Oscillation Diversity"></a>A multi-modal representation of El Niño Southern Oscillation Diversity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11552">http://arxiv.org/abs/2307.11552</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jakob-schloer/latentgmm">https://github.com/jakob-schloer/latentgmm</a></li>
<li>paper_authors: Jakob Schlör, Felix Strnad, Antonietta Capotondi, Bedartha Goswami</li>
<li>for: 本研究旨在描述和分类东太平洋附近海面温度异常（ENSO）的多样性。</li>
<li>methods: 研究人员使用低维表示法和不精确 clustering 方法来描述和分类ENSO事件。</li>
<li>results: 研究人员发现，ENSO事件并不是二元的，而是有五种类型：极端 El Ni~no、EP El Ni~no、CP La Ni~na、EP La Ni~na 和 Extreme La Ni~na。此外，研究人员还发现，这些不同类型的 ENSO 事件在不同的时间和强度方面具有显著的差异。<details>
<summary>Abstract</summary>
The El Ni\~no-Southern Oscillation (ENSO) is characterized by alternating periods of warm (El Ni\~no) and cold (La Ni\~na) sea surface temperature anomalies (SSTA) in the equatorial Pacific. Although El Ni\~no and La Ni\~na are well-defined climate patterns, no two events are alike. To date, ENSO diversity has been described primarily in terms of the longitudinal location of peak SSTA, used to define a bimodal classification of events in Eastern Pacific (EP) and Central Pacific (CP) types. Here, we use low-dimensional representations of Pacific SSTAs to argue that binary categorical memberships are unsuitable to describe ENSO events. Using fuzzy unsupervised clustering, we recover the four known ENSO categories, along with a fifth category: an Extreme El Ni\~no. We show that Extreme El Ni\~nos differ both in their intensity and temporal evolution from canonical EP El Ni\~nos. We also find that CP La Ni\~nas, EP El Ni\~nos, and Extreme El Ni\~nos contribute the most to interdecadal ENSO variability.
</details>
<details>
<summary>摘要</summary>
“恶elf Niño-南方振荡（ENSO）是指太平洋赤道区的海水温度异常（SSTA）在 alternate periods of warm（El Niño）和 cold（La Niña）。虽然El Niño和La Niña是明确定义的气候模式，但每个事件都不一样。到目前为止，ENSO 多样性仅被描述为东太平洋（EP）和中太平洋（CP）类型的 longitudinal 位置峰值SSTA的 биModal 分类。本文使用低维表示太平洋SSTAs， argue  binary categorical memberships 不适用于描述ENSO事件。使用不supervised clustering，我们回收了四种已知ENSO类别，以及一个第五个类别：极端 El Niño。我们显示，极端 El Niño 与 canonical EP El Niño 不同，具有不同的强度和时间演化。我们还发现，CP La Niña、EP El Niño 和极端 El Niño 在 Interdecadal ENSO 变化中发挥了关键作用。”Note: "恶elf" is a typo, the correct word is "El Niño".
</details></li>
</ul>
<hr>
<h2 id="Towards-practical-reinforcement-learning-for-tokamak-magnetic-control"><a href="#Towards-practical-reinforcement-learning-for-tokamak-magnetic-control" class="headerlink" title="Towards practical reinforcement learning for tokamak magnetic control"></a>Towards practical reinforcement learning for tokamak magnetic control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11546">http://arxiv.org/abs/2307.11546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brendan D. Tracey, Andrea Michi, Yuri Chervonyi, Ian Davies, Cosmin Paduraru, Nevena Lazic, Federico Felici, Timo Ewalds, Craig Donner, Cristian Galperti, Jonas Buchli, Michael Neunert, Andrea Huber, Jonathan Evens, Paula Kurylowicz, Daniel J. Mankowitz, Martin Riedmiller, The TCV Team</li>
<li>for: 这个研究旨在改善基于增强反馈学习（RL）的真实时间控制系统，特别是核聚体控制领域。</li>
<li>methods: 本研究使用RL方法，并提出了几个算法改进，包括代理架构和训练程序。</li>
<li>results:  simulation results show that the proposed RL-based controller can achieve up to 65% improvement in shape accuracy, reduce the long-term bias of the plasma current, and reduce the training time required to learn new tasks by a factor of 3 or more. 新的实验结果显示，RL方法可以在TCV Tokamak上实现精准的燃烧。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) has shown promising results for real-time control systems, including the domain of plasma magnetic control. However, there are still significant drawbacks compared to traditional feedback control approaches for magnetic confinement. In this work, we address key drawbacks of the RL method; achieving higher control accuracy for desired plasma properties, reducing the steady-state error, and decreasing the required time to learn new tasks. We build on top of \cite{degrave2022magnetic}, and present algorithmic improvements to the agent architecture and training procedure. We present simulation results that show up to 65\% improvement in shape accuracy, achieve substantial reduction in the long-term bias of the plasma current, and additionally reduce the training time required to learn new tasks by a factor of 3 or more. We present new experiments using the upgraded RL-based controllers on the TCV tokamak, which validate the simulation results achieved, and point the way towards routinely achieving accurate discharges using the RL approach.
</details>
<details>
<summary>摘要</summary>
现代回归学习（RL）已经在实时控制系统中展现出了扎实的成果，包括磁泵控制领域。然而，RL方法还存在许多缺点，比如传统反馈控制方法更高的控制精度和稳定性。在这项工作中，我们解决了RL方法的关键缺点，包括提高愿望束质量的控制精度、减少稳态误差和减少学习新任务所需的训练时间。我们基于\cite{degrave2022magnetic}的研究，提出了算法改进和训练过程优化。我们在实验中获得了65%的形状精度提高、长期偏差的磁泵电流减少和学习新任务所需的训练时间减少了3倍以上。此外，我们在TCVtokamak上使用了更新的RL控制器，实际实现了在实验中获得的结果，并预示了在RL方法中实现精度的常见实践。
</details></li>
</ul>
<hr>
<h2 id="Training-Latency-Minimization-for-Model-Splitting-Allowed-Federated-Edge-Learning"><a href="#Training-Latency-Minimization-for-Model-Splitting-Allowed-Federated-Edge-Learning" class="headerlink" title="Training Latency Minimization for Model-Splitting Allowed Federated Edge Learning"></a>Training Latency Minimization for Model-Splitting Allowed Federated Edge Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11532">http://arxiv.org/abs/2307.11532</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Wen, Guopeng Zhang, Kezhi Wang, Kun Yang</li>
<li>for: 提高 federated learning 中的计算能力，以便在训练深度神经网络时遇到的计算能力短缺问题。</li>
<li>methods: 利用边缘计算和分解学习，提出了模型分解允许 federated learning 框架（SFL），以减少训练延迟而无损测试精度。</li>
<li>results: 通过对 EfficientNetV2 模型在 MNIST 数据集上进行广泛的实验，证明了提案的 SFL 框架的有效性和提高性。<details>
<summary>Abstract</summary>
To alleviate the shortage of computing power faced by clients in training deep neural networks (DNNs) using federated learning (FL), we leverage the edge computing and split learning to propose a model-splitting allowed FL (SFL) framework, with the aim to minimize the training latency without loss of test accuracy. Under the synchronized global update setting, the latency to complete a round of global training is determined by the maximum latency for the clients to complete a local training session. Therefore, the training latency minimization problem (TLMP) is modelled as a minimizing-maximum problem. To solve this mixed integer nonlinear programming problem, we first propose a regression method to fit the quantitative-relationship between the cut-layer and other parameters of an AI-model, and thus, transform the TLMP into a continuous problem. Considering that the two subproblems involved in the TLMP, namely, the cut-layer selection problem for the clients and the computing resource allocation problem for the parameter-server are relative independence, an alternate-optimization-based algorithm with polynomial time complexity is developed to obtain a high-quality solution to the TLMP. Extensive experiments are performed on a popular DNN-model EfficientNetV2 using dataset MNIST, and the results verify the validity and improved performance of the proposed SFL framework.
</details>
<details>
<summary>摘要</summary>
To solve this mixed integer nonlinear programming problem, we first develop a regression method to establish a quantitative relationship between the cut-layer and other parameters of an AI model. This transformation allows us to convert the TLMP into a continuous problem. As the two subproblems involved in the TLMP, namely, the cut-layer selection problem for clients and the computing resource allocation problem for the parameter server, are relatively independent, we develop an alternate optimization-based algorithm with polynomial time complexity to obtain a high-quality solution to the TLMP.We conduct extensive experiments on the popular DNN model EfficientNetV2 using the MNIST dataset, and the results demonstrate the effectiveness and improved performance of the proposed SFL framework.
</details></li>
</ul>
<hr>
<h2 id="General-regularization-in-covariate-shift-adaptation"><a href="#General-regularization-in-covariate-shift-adaptation" class="headerlink" title="General regularization in covariate shift adaptation"></a>General regularization in covariate shift adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11503">http://arxiv.org/abs/2307.11503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duc Hoan Nguyen, Sergei V. Pereverzyev, Werner Zellinger</li>
<li>for:  corrected error of least squares learning algorithms in reproducing kernel Hilbert spaces (RKHS)</li>
<li>methods:  sample weights determined by estimated Radon-Nikod&#39;ym derivative of future data distribution w.r.t.~training data distribution</li>
<li>results:  novel error bounds for reweighted kernel regression in RKHS, showing that fewer samples are needed for the same level of accuracy compared to state-of-the-art analyses under weak smoothness conditions.Here’s the Chinese text in the format you requested:</li>
<li>for: 修正最小二乘学习算法在嵌入kernel空间（RKHS）中的错误</li>
<li>methods: 使用估计的拉戈-尼科德偏微分来确定未来数据分布与训练数据分布之间的比较</li>
<li>results:  novel的误差下界结果，表明在弱约束下，只需要 fewer samples 来达到相同级别的准确率，与现有分析相比。<details>
<summary>Abstract</summary>
Sample reweighting is one of the most widely used methods for correcting the error of least squares learning algorithms in reproducing kernel Hilbert spaces (RKHS), that is caused by future data distributions that are different from the training data distribution. In practical situations, the sample weights are determined by values of the estimated Radon-Nikod\'ym derivative, of the future data distribution w.r.t.~the training data distribution. In this work, we review known error bounds for reweighted kernel regression in RKHS and obtain, by combination, novel results. We show under weak smoothness conditions, that the amount of samples, needed to achieve the same order of accuracy as in the standard supervised learning without differences in data distributions, is smaller than proven by state-of-the-art analyses.
</details>
<details>
<summary>摘要</summary>
Sample 重点是一种广泛使用的方法，用于纠正最小二乘学习算法在 reproduce kernel Hilbert space（RKHS）中的错误，这种错误是由未来数据分布不同于训练数据分布所导致的。在实际应用中，样本权重通常是根据估计的Radon-Nikodym derivate，未来数据分布与训练数据分布之间的比例来确定。在这种工作中，我们回顾了已知的重量 kernel 回归错误约束，并通过组合，获得了新的结果。我们在弱纹理条件下表明，需要的样本数量，以达到标准无 diferenze 学习中的同等精度水平，比已知分析的结果少。
</details></li>
</ul>
<hr>
<h2 id="Predict-Refine-Synthesize-Self-Guiding-Diffusion-Models-for-Probabilistic-Time-Series-Forecasting"><a href="#Predict-Refine-Synthesize-Self-Guiding-Diffusion-Models-for-Probabilistic-Time-Series-Forecasting" class="headerlink" title="Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting"></a>Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11494">http://arxiv.org/abs/2307.11494</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcel Kollovieh, Abdul Fatir Ansari, Michael Bohlke-Schneider, Jasper Zschiegner, Hao Wang, Yuyang Wang</li>
<li>for: 这个研究旨在探讨时间序列散射模型在不同领域的生成模型任务中的潜在性。</li>
<li>methods: 本研究提出了一个名为TSDiff的无条件散射模型，并实现了在推理过程中通过自我指引机制来将TSDiff训练为下游任务。</li>
<li>results: 研究结果显示TSDiff可以与多个任务特定的条件散射模型竞争，并且可以透过将TSDiff训练为下游任务来实现更好的预测性和生成性。<details>
<summary>Abstract</summary>
Diffusion models have achieved state-of-the-art performance in generative modeling tasks across various domains. Prior works on time series diffusion models have primarily focused on developing conditional models tailored to specific forecasting or imputation tasks. In this work, we explore the potential of task-agnostic, unconditional diffusion models for several time series applications. We propose TSDiff, an unconditionally trained diffusion model for time series. Our proposed self-guidance mechanism enables conditioning TSDiff for downstream tasks during inference, without requiring auxiliary networks or altering the training procedure. We demonstrate the effectiveness of our method on three different time series tasks: forecasting, refinement, and synthetic data generation. First, we show that TSDiff is competitive with several task-specific conditional forecasting methods (predict). Second, we leverage the learned implicit probability density of TSDiff to iteratively refine the predictions of base forecasters with reduced computational overhead over reverse diffusion (refine). Notably, the generative performance of the model remains intact -- downstream forecasters trained on synthetic samples from TSDiff outperform forecasters that are trained on samples from other state-of-the-art generative time series models, occasionally even outperforming models trained on real data (synthesize).
</details>
<details>
<summary>摘要</summary>
Translate the given text into Simplified Chinese.Diffusion models have achieved state-of-the-art performance in generative modeling tasks across various domains. Prior works on time series diffusion models have primarily focused on developing conditional models tailored to specific forecasting or imputation tasks. In this work, we explore the potential of task-agnostic, unconditional diffusion models for several time series applications. We propose TSDiff, an unconditionally trained diffusion model for time series. Our proposed self-guidance mechanism enables conditioning TSDiff for downstream tasks during inference, without requiring auxiliary networks or altering the training procedure. We demonstrate the effectiveness of our method on three different time series tasks: forecasting, refinement, and synthetic data generation. First, we show that TSDiff is competitive with several task-specific conditional forecasting methods (预测). Second, we leverage the learned implicit probability density of TSDiff to iteratively refine the predictions of base forecasters with reduced computational overhead over reverse diffusion (修正). Notably, the generative performance of the model remains intact -- downstream forecasters trained on synthetic samples from TSDiff outperform forecasters that are trained on samples from other state-of-the-art generative time series models, occasionally even outperforming models trained on real data (生成).
</details></li>
</ul>
<hr>
<h2 id="A-New-Deep-State-Space-Analysis-Framework-for-Patient-Latent-State-Estimation-and-Classification-from-EHR-Time-Series-Data"><a href="#A-New-Deep-State-Space-Analysis-Framework-for-Patient-Latent-State-Estimation-and-Classification-from-EHR-Time-Series-Data" class="headerlink" title="A New Deep State-Space Analysis Framework for Patient Latent State Estimation and Classification from EHR Time Series Data"></a>A New Deep State-Space Analysis Framework for Patient Latent State Estimation and Classification from EHR Time Series Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11487">http://arxiv.org/abs/2307.11487</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aya Nakamura, Ryosuke Kojima, Yuji Okamoto, Eiichiro Uchino, Yohei Mineharu, Yohei Harada, Mayumi Kamada, Manabu Muto, Motoko Yanagita, Yasushi Okuno</li>
<li>for: 这个研究旨在利用电子医疗记录（EHRs）和机器学习技术，提供更好的诊断和治疗策略。</li>
<li>methods: 这种深度状态空间分析框架使用时间序列无监督学习，通过深度状态空间模型来学习、可见化和分群病人的潜在状态变化。</li>
<li>results: 研究人员通过对12,695名癌病人的时间序列实验数据进行分析，成功地发现了与诊断相关的潜在状态。通过可见化和分群分析，研究人员还发现了不同抗癌药物的时间过程中病人状态的特征变化。这种框架比现有方法更能够捕捉可解释的潜在空间。<details>
<summary>Abstract</summary>
Many diseases, including cancer and chronic conditions, require extended treatment periods and long-term strategies. Machine learning and AI research focusing on electronic health records (EHRs) have emerged to address this need. Effective treatment strategies involve more than capturing sequential changes in patient test values. It requires an explainable and clinically interpretable model by capturing the patient's internal state over time.   In this study, we propose the "deep state-space analysis framework," using time-series unsupervised learning of EHRs with a deep state-space model. This framework enables learning, visualizing, and clustering of temporal changes in patient latent states related to disease progression.   We evaluated our framework using time-series laboratory data from 12,695 cancer patients. By estimating latent states, we successfully discover latent states related to prognosis. By visualization and cluster analysis, the temporal transition of patient status and test items during state transitions characteristic of each anticancer drug were identified. Our framework surpasses existing methods in capturing interpretable latent space. It can be expected to enhance our comprehension of disease progression from EHRs, aiding treatment adjustments and prognostic determinations.
</details>
<details>
<summary>摘要</summary>
许多疾病，包括癌症和chronic condition，需要长期的治疗期和长期策略。机器学习和人工智能研究集中在电子医疗记录（EHRs）上，以解决这一需求。有效的治疗策略不仅仅是记录患者的测试值序列变化，而是需要一个可解释和临床可解释的模型，可以捕捉患者的内部状态的变化。在这项研究中，我们提出了“深度状态空间分析框架”，使用时间序列无监督学习EHRs中的深度状态空间模型。这个框架允许我们学习、可见化和分组患者的时间变化状态。我们对12,695名癌症患者的时间序列实验室数据进行了评估。通过估计 latent states，我们成功地发现了与诊断相关的秘密状态。通过可视化和分组分析，我们可以了解患者的状态转移特征和测试项的时间变化特征。我们的框架比现有方法更能捕捉可解释的状态空间。它可以预期地提高我们从EHRs中了解疾病进程的认知，以便治疗调整和诊断决策。
</details></li>
</ul>
<hr>
<h2 id="A-Deep-Learning-Approach-for-Overall-Survival-Prediction-in-Lung-Cancer-with-Missing-Values"><a href="#A-Deep-Learning-Approach-for-Overall-Survival-Prediction-in-Lung-Cancer-with-Missing-Values" class="headerlink" title="A Deep Learning Approach for Overall Survival Prediction in Lung Cancer with Missing Values"></a>A Deep Learning Approach for Overall Survival Prediction in Lung Cancer with Missing Values</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11465">http://arxiv.org/abs/2307.11465</a></li>
<li>repo_url: None</li>
<li>paper_authors: Camillo Maria Caruso, Valerio Guarrasi, Sara Ramella, Paolo Soda</li>
<li>for: 本研究旨在应用人工智能技术于肺癌研究，特别是非小细胞肺癌（NSCLC），以提高患者状况评估和生存时间（OS）率。</li>
<li>methods: 本研究使用变换器架构，不需要任何填充策略，可以Effectively learning from both censored and uncensored patients and their available features，预测NSCLC患者的OS。</li>
<li>results:  comparing with现有的方法，本研究 obtiented Ct-index of 71.97, 77.58 and 80.72 for time units of 1 month, 1 year and 2 years, respectively, outperforming all state-of-the-art methods regardless of the imputation method used.<details>
<summary>Abstract</summary>
One of the most challenging fields where Artificial Intelligence (AI) can be applied is lung cancer research, specifically non-small cell lung cancer (NSCLC). In particular, overall survival (OS), the time between diagnosis and death, is a vital indicator of patient status, enabling tailored treatment and improved OS rates. In this analysis, there are two challenges to take into account. First, few studies effectively exploit the information available from each patient, leveraging both uncensored (i.e., dead) and censored (i.e., survivors) patients, considering also the events' time. Second, the handling of incomplete data is a common issue in the medical field. This problem is typically tackled through the use of imputation methods. Our objective is to present an AI model able to overcome these limits, effectively learning from both censored and uncensored patients and their available features, for the prediction of OS for NSCLC patients. We present a novel approach to survival analysis with missing values in the context of NSCLC, which exploits the strengths of the transformer architecture to account only for available features without requiring any imputation strategy. By making use of ad-hoc losses for OS, it is able to account for both censored and uncensored patients, as well as changes in risks over time. We compared our method with state-of-the-art models for survival analysis coupled with different imputation strategies. We evaluated the results obtained over a period of 6 years using different time granularities obtaining a Ct-index, a time-dependent variant of the C-index, of 71.97, 77.58 and 80.72 for time units of 1 month, 1 year and 2 years, respectively, outperforming all state-of-the-art methods regardless of the imputation method used.
</details>
<details>
<summary>摘要</summary>
一个非常挑战性的领域是用人工智能（AI）应用于肺癌研究，特别是非小细胞肺癌（NSCLC）。在这种分析中，需要考虑两个挑战。首先，很少的研究有效利用每个病人的信息，同时利用bothuncensored（即死亡）和censored（即存活）病人的信息，同时考虑事件的时间。第二，医疗领域中的数据缺失是一个常见的问题。通常通过使用插入方法来解决这个问题。我们的目标是开发一个能够超越这些限制的AI模型，可以从bothuncensored和censored病人中学习可用的特征，并且可以预测NSCLC病人的生存时间。我们提出了一种新的缺失值处理方法，利用变换器架构来考虑only可用的特征，不需要任何插入策略。通过使用适应性损失函数来考虑bothuncensored和censored病人，以及时间变化的风险。我们与状态arta的模型进行比较，并使用不同的插入策略。我们在6年的评估期间使用不同的时间粒度obtained a Ct-index of 71.97, 77.58, and 80.72 for time units of 1 month, 1 year, and 2 years, respectively, outperforming all state-of-the-art methods regardless of the imputation method used.
</details></li>
</ul>
<hr>
<h2 id="Improve-Long-term-Memory-Learning-Through-Rescaling-the-Error-Temporally"><a href="#Improve-Long-term-Memory-Learning-Through-Rescaling-the-Error-Temporally" class="headerlink" title="Improve Long-term Memory Learning Through Rescaling the Error Temporally"></a>Improve Long-term Memory Learning Through Rescaling the Error Temporally</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11462">http://arxiv.org/abs/2307.11462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shida Wang, Zhanglu Yan</li>
<li>for: 这 paper 研究了长期记忆学习中的错误度量选择。我们发现通常使用的错误都带有短期记忆偏好，包括平均绝对&#x2F;平方Error。我们的发现表明所有有正向时间权重的错误都带有短期记忆偏好，以学习线性函数。</li>
<li>methods: 为了减少这种偏好和提高长期记忆学习，我们提议使用时间折算错误。此外，这种方法还可以减轻消失梯度问题。</li>
<li>results: 我们在不同的长期任务和序列模型上进行了数值实验，并证实了我们的主张。数值结果表明，适当的时间折算错误对长期记忆学习是重要的。据我们所知，这是首次对序列模型中不同错误的短期记忆偏好进行量化分析。<details>
<summary>Abstract</summary>
This paper studies the error metric selection for long-term memory learning in sequence modelling. We examine the bias towards short-term memory in commonly used errors, including mean absolute/squared error. Our findings show that all temporally positive-weighted errors are biased towards short-term memory in learning linear functionals. To reduce this bias and improve long-term memory learning, we propose the use of a temporally rescaled error. In addition to reducing the bias towards short-term memory, this approach can also alleviate the vanishing gradient issue. We conduct numerical experiments on different long-memory tasks and sequence models to validate our claims. Numerical results confirm the importance of appropriate temporally rescaled error for effective long-term memory learning. To the best of our knowledge, this is the first work that quantitatively analyzes different errors' memory bias towards short-term memory in sequence modelling.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Neural-Operators-for-Delay-Compensating-Control-of-Hyperbolic-PIDEs"><a href="#Neural-Operators-for-Delay-Compensating-Control-of-Hyperbolic-PIDEs" class="headerlink" title="Neural Operators for Delay-Compensating Control of Hyperbolic PIDEs"></a>Neural Operators for Delay-Compensating Control of Hyperbolic PIDEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11436">http://arxiv.org/abs/2307.11436</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jingzhang-jz/no_hyperbolic_delay">https://github.com/jingzhang-jz/no_hyperbolic_delay</a></li>
<li>paper_authors: Jie Qi, Jing Zhang, Miroslav Krstic</li>
<li>for: 该 paper 探讨了将 DeepONet Operator-learning 框架应用于高级湍流 PDE 控制中的延迟问题。</li>
<li>methods: 该 paper 使用了 PDE 倒退设计生成积分函数，并使用 DeepONet 神经网络来近似这些积分函数。</li>
<li>results: 该 paper 证明了在反馈 controllers 下的稳定性，并且还提出了 DeepONet 近似的观测器和输出反馈法则，并证明了其稳定性。 numerics 表明，使用 DeepONet 可以大幅减少计算量，减少两个数量级。<details>
<summary>Abstract</summary>
The recently introduced DeepONet operator-learning framework for PDE control is extended from the results for basic hyperbolic and parabolic PDEs to an advanced hyperbolic class that involves delays on both the state and the system output or input. The PDE backstepping design produces gain functions that are outputs of a nonlinear operator, mapping functions on a spatial domain into functions on a spatial domain, and where this gain-generating operator's inputs are the PDE's coefficients. The operator is approximated with a DeepONet neural network to a degree of accuracy that is provably arbitrarily tight. Once we produce this approximation-theoretic result in infinite dimension, with it we establish stability in closed loop under feedback that employs approximate gains. In addition to supplying such results under full-state feedback, we also develop DeepONet-approximated observers and output-feedback laws and prove their own stabilizing properties under neural operator approximations. With numerical simulations we illustrate the theoretical results and quantify the numerical effort savings, which are of two orders of magnitude, thanks to replacing the numerical PDE solving with the DeepONet.
</details>
<details>
<summary>摘要</summary>
With this approximation-theoretic result in infinite dimension, we establish stability in closed loop under feedback that employs approximate gains. In addition to supplying results under full-state feedback, we also develop DeepONet-approximated observers and output-feedback laws and prove their own stabilizing properties under neural operator approximations.Numerical simulations illustrate the theoretical results and quantify the numerical effort savings, which are of two orders of magnitude, thanks to replacing the numerical PDE solving with the DeepONet.
</details></li>
</ul>
<hr>
<h2 id="Batching-for-Green-AI-–-An-Exploratory-Study-on-Inference"><a href="#Batching-for-Green-AI-–-An-Exploratory-Study-on-Inference" class="headerlink" title="Batching for Green AI – An Exploratory Study on Inference"></a>Batching for Green AI – An Exploratory Study on Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11434">http://arxiv.org/abs/2307.11434</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim Yarally, Luís Cruz, Daniel Feitosa, June Sallou, Arie van Deursen</li>
<li>for: 本研究旨在探讨输入批处理对深度学习模型的能耗和响应时间的影响。</li>
<li>methods: 研究使用了五种已经在出版时被视为state-of-the-art的计算机视觉神经网络，并对这些神经网络的输入批处理产生了影响。</li>
<li>results: 研究发现，批处理有显著影响于能耗和响应时间。此外，研究还提供了过去十年内神经网络精度和能耗的时间轴图表，发现能耗在精度提升的同时增长得远远 быстре，并提出了一些问题。同时，研究发现ShuffleNetV2（2018）模型在其时代实现了竞争性性能，但能耗较低。但是，研究表明结果受模型影响。<details>
<summary>Abstract</summary>
The batch size is an essential parameter to tune during the development of new neural networks. Amongst other quality indicators, it has a large degree of influence on the model's accuracy, generalisability, training times and parallelisability. This fact is generally known and commonly studied. However, during the application phase of a deep learning model, when the model is utilised by an end-user for inference, we find that there is a disregard for the potential benefits of introducing a batch size. In this study, we examine the effect of input batching on the energy consumption and response times of five fully-trained neural networks for computer vision that were considered state-of-the-art at the time of their publication. The results suggest that batching has a significant effect on both of these metrics. Furthermore, we present a timeline of the energy efficiency and accuracy of neural networks over the past decade. We find that in general, energy consumption rises at a much steeper pace than accuracy and question the necessity of this evolution. Additionally, we highlight one particular network, ShuffleNetV2(2018), that achieved a competitive performance for its time while maintaining a much lower energy consumption. Nevertheless, we highlight that the results are model dependent.
</details>
<details>
<summary>摘要</summary>
批处大小是深度学习模型开发中非常重要的参数，它对模型的准确率、通用性、训练时间和并行性具有很大的影响。这个事实已经广泛被研究和了解。但在深度学习模型在实际应用阶段使用时，批处大小却往往被忽视。在这个研究中，我们研究了五种在发表时被视为顶尖的计算机视觉神经网络的输入批处效果。结果表明，批处有着显著的影响于能源消耗和响应时间。此外，我们还提供了过去十年内神经网络能效和准确率的时间轴，发现在总体来说，能源消耗在准确率增长的速度上升得 Much steeper than the latter，并质疑这种演化的必要性。此外，我们还提到了一个特定的网络，ShuffleNetV2（2018），它在其时间内实现了竞争性的性能，同时保持了远低的能源消耗。但我们要注意，结果是模型具体的。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Embedding-Learning-for-Human-Activity-Recognition-Using-Wearable-Sensor-Data"><a href="#Unsupervised-Embedding-Learning-for-Human-Activity-Recognition-Using-Wearable-Sensor-Data" class="headerlink" title="Unsupervised Embedding Learning for Human Activity Recognition Using Wearable Sensor Data"></a>Unsupervised Embedding Learning for Human Activity Recognition Using Wearable Sensor Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11796">http://arxiv.org/abs/2307.11796</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taoran Sheng, Manfred Huber</li>
<li>for: Recognizing different human activities from wearable sensor data in ubiquitous computing.</li>
<li>methods: Unsupervised approach based on the nature of human activity to project data into an embedding space, followed by clustering algorithms to form behavior clusters.</li>
<li>results: Experimental results on three labeled benchmark datasets show the effectiveness of the framework and improved performance in identifying and categorizing human activities compared to unsupervised techniques applied directly to the original data set.Here’s the full Chinese text:</li>
<li>for: 本研究旨在recognize wearable sensor数据中的人类活动，尤其是在ubiquitous computing中。</li>
<li>methods: 我们提出了一种无监督方法，基于人类活动的本质，将数据 проек到嵌入空间中，使得相似的活动在空间中呈现相似的坐标。然后，使用 clustering 算法对行为集进行分类。</li>
<li>results: 对三个标注数据集进行实验，结果表明我们的框架具有效果，可以帮助 clustering 算法更好地认定和分类人类活动，相比直接应用于原始数据集的无监督技术。<details>
<summary>Abstract</summary>
The embedded sensors in widely used smartphones and other wearable devices make the data of human activities more accessible. However, recognizing different human activities from the wearable sensor data remains a challenging research problem in ubiquitous computing. One of the reasons is that the majority of the acquired data has no labels. In this paper, we present an unsupervised approach, which is based on the nature of human activity, to project the human activities into an embedding space in which similar activities will be located closely together. Using this, subsequent clustering algorithms can benefit from the embeddings, forming behavior clusters that represent the distinct activities performed by a person. Results of experiments on three labeled benchmark datasets demonstrate the effectiveness of the framework and show that our approach can help the clustering algorithm achieve improved performance in identifying and categorizing the underlying human activities compared to unsupervised techniques applied directly to the original data set.
</details>
<details>
<summary>摘要</summary>
随处通用的智能手机和其他搭载设备中嵌入的传感器使人类活动数据更加可 accessible。然而，从智能手机传感器数据中识别不同的人类活动仍然是智能 computing中一个挑战。其中一个原因是大多数获取到的数据没有标签。在这篇论文中，我们提出了一种不supervised的方法，基于人类活动的自然特征，将人类活动 проек到一个嵌入空间中，在该空间中相似的活动会受到相似的嵌入。使用这些嵌入，后续的聚类算法可以从这些嵌入中受益，形成人类活动的行为归一化。实验结果表明，我们的方法可以帮助聚类算法更好地识别和分类人类活动，相比直接应用于原始数据集的无监督技术。
</details></li>
</ul>
<hr>
<h2 id="An-Analysis-of-Multi-Agent-Reinforcement-Learning-for-Decentralized-Inventory-Control-Systems"><a href="#An-Analysis-of-Multi-Agent-Reinforcement-Learning-for-Decentralized-Inventory-Control-Systems" class="headerlink" title="An Analysis of Multi-Agent Reinforcement Learning for Decentralized Inventory Control Systems"></a>An Analysis of Multi-Agent Reinforcement Learning for Decentralized Inventory Control Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11432">http://arxiv.org/abs/2307.11432</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marwan Mousa, Damien van de Berg, Niki Kotecha, Ehecatl Antonio del Rio-Chanona, Max Mowbray</li>
<li>for: 解决供应链中独立实体之间的信息封锁问题，提出了一种基于多代理人学习的分布式存储管理方法。</li>
<li>methods: 研究了三种多代理人变体的距离优化算法，包括中央训练分布式执行、分布式训练中央执行和分布式训练分布式执行。</li>
<li>results: 通过对不同供应链网络和不确定性水平的 simulate 结果显示，使用多代理人距离优化算法与中央数据驱动方法相比，性能几乎相同，而且在大多数情况下超过分布式模型基于方法。<details>
<summary>Abstract</summary>
Most solutions to the inventory management problem assume a centralization of information that is incompatible with organisational constraints in real supply chain networks. The inventory management problem is a well-known planning problem in operations research, concerned with finding the optimal re-order policy for nodes in a supply chain. While many centralized solutions to the problem exist, they are not applicable to real-world supply chains made up of independent entities. The problem can however be naturally decomposed into sub-problems, each associated with an independent entity, turning it into a multi-agent system. Therefore, a decentralized data-driven solution to inventory management problems using multi-agent reinforcement learning is proposed where each entity is controlled by an agent. Three multi-agent variations of the proximal policy optimization algorithm are investigated through simulations of different supply chain networks and levels of uncertainty. The centralized training decentralized execution framework is deployed, which relies on offline centralization during simulation-based policy identification, but enables decentralization when the policies are deployed online to the real system. Results show that using multi-agent proximal policy optimization with a centralized critic leads to performance very close to that of a centralized data-driven solution and outperforms a distributed model-based solution in most cases while respecting the information constraints of the system.
</details>
<details>
<summary>摘要</summary>
大多数解决存储管理问题的解决方案假设了中央化信息，这与实际供应链网络中的组织结构不兼容。存储管理问题是操作研究中的一个常见规划问题，旨在找到优化再配货策略的节点在供应链中。虽然许多中央化解决方案存在，但它们不适用于实际的独立实体组成的供应链。问题可以自然划分为子问题，每个问题与一个独立实体相关。因此，一种基于多代理人学习的数据驱动的分布式存储管理解决方案被提议，其中每个实体由一个代理人控制。通过在不同供应链网络和不确定程度下进行多个多代理人变种的迪斯特普罗ximal Policy优化算法的实践，研究了这种解决方案的性能。中央训练、分布行动的框架被采用，其在在线部署策略时仅仅是在模拟期间进行中央化训练，但在实际系统中允许分布式执行。结果表明，使用多代理人 proximal Policy优化算法和中央评估器可以与中央数据驱动解决方案的性能几乎相同，而且在大多数情况下超越分布式模型基于解决方案，同时尊重系统信息约束。
</details></li>
</ul>
<hr>
<h2 id="Prompting-Large-Language-Models-with-Speech-Recognition-Abilities"><a href="#Prompting-Large-Language-Models-with-Speech-Recognition-Abilities" class="headerlink" title="Prompting Large Language Models with Speech Recognition Abilities"></a>Prompting Large Language Models with Speech Recognition Abilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11795">http://arxiv.org/abs/2307.11795</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Junteng Jia, Yuan Shangguan, Ke Li, Jinxi Guo, Wenhan Xiong, Jay Mahadeokar, Ozlem Kalinli, Christian Fuegen, Mike Seltzer</li>
<li>for:  extensions the capabilities of large language models (LLMs) to perform speech recognition</li>
<li>methods:  directly attaching a small audio encoder to the LLM, prepending a sequence of audial embeddings to the text token embeddings</li>
<li>results:  outperformed monolingual baselines by 18% and performed multilingual speech recognition despite LLaMA being trained overwhelmingly on English text, and the LLM can be frozen or with fewer embeddings<details>
<summary>Abstract</summary>
Large language models have proven themselves highly flexible, able to solve a wide range of generative tasks, such as abstractive summarization and open-ended question answering. In this paper we extend the capabilities of LLMs by directly attaching a small audio encoder allowing it to perform speech recognition. By directly prepending a sequence of audial embeddings to the text token embeddings, the LLM can be converted to an automatic speech recognition (ASR) system, and be used in the exact same manner as its textual counterpart. Experiments on Multilingual LibriSpeech (MLS) show that incorporating a conformer encoder into the open sourced LLaMA-7B allows it to outperform monolingual baselines by 18% and perform multilingual speech recognition despite LLaMA being trained overwhelmingly on English text. Furthermore, we perform ablation studies to investigate whether the LLM can be completely frozen during training to maintain its original capabilities, scaling up the audio encoder, and increasing the audio encoder striding to generate fewer embeddings. The results from these studies show that multilingual ASR is possible even when the LLM is frozen or when strides of almost 1 second are used in the audio encoder opening up the possibility for LLMs to operate on long-form audio.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Attention-to-Entropic-Communication"><a href="#Attention-to-Entropic-Communication" class="headerlink" title="Attention to Entropic Communication"></a>Attention to Entropic Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11423">http://arxiv.org/abs/2307.11423</a></li>
<li>repo_url: None</li>
<li>paper_authors: Torsten Enßlin, Carolin Weidinger, Philipp Frank</li>
<li>for: 这种研究旨在探讨一种新的通信协议，即基于注意力的 entropy 通信，以便在技术应用中设计优化的通信协议，以及更好地理解人类communication。</li>
<li>methods: 这种研究使用了 relative entropy（RE）和 maximum entropy principle（MEP）来分析和设计通信协议。具体来说，研究使用了 RE 来导引合理编码和解码消息，并使用了 weighted RE 来引导注意力协调。</li>
<li>results: 研究发现，使用 entropic attention communication 可以实现合理的注意力协调，并且可以帮助解决人类communication 中的各种问题，例如不同利益的合作。此外，研究还发现，在某些情况下，weighted RE 不是一个正确的方法，需要使用 proper attention communication。<details>
<summary>Abstract</summary>
The concept of attention, numerical weights that emphasize the importance of particular data, has proven to be very relevant in artificial intelligence. Relative entropy (RE, aka Kullback-Leibler divergence) plays a central role in communication theory. Here we combine these concepts, attention and RE. RE guides optimal encoding of messages in bandwidth-limited communication as well as optimal message decoding via the maximum entropy principle (MEP). In the coding scenario, RE can be derived from four requirements, namely being analytical, local, proper, and calibrated. Weighted RE, used for attention steering in communications, turns out to be improper. To see how proper attention communication can emerge, we analyze a scenario of a message sender who wants to ensure that the receiver of the message can perform well-informed actions. If the receiver decodes the message using the MEP, the sender only needs to know the receiver's utility function to inform optimally, but not the receiver's initial knowledge state. In case only the curvature of the utility function maxima are known, it becomes desirable to accurately communicate an attention function, in this case a by this curvature weighted and re-normalized probability function. Entropic attention communication is here proposed as the desired generalization of entropic communication that permits weighting while being proper, thereby aiding the design of optimal communication protocols in technical applications and helping to understand human communication. For example, our analysis shows how to derive the level of cooperation expected under misaligned interests of otherwise honest communication partners.
</details>
<details>
<summary>摘要</summary>
人工智能中的注意力概念（numerical weights that emphasize the importance of particular data）已经证明非常有用。相对 entropy（RE，又称库拉克-莱布尔差分）在通信理论中扮演着中心角色。在这里，我们将这些概念结合起来。RE guideline优化的信息编码和解码，以及最大Entropy原则（MEP）。在编码场景下，RE可以从四个需求 derivation，namely analytical, local, proper, and calibrated。在通信中使用Weighted RE进行注意力引导，实际上是不正确的。要实现正确的注意力通信，我们分析了一个消息发送者想要确保接收者可以做出有知识的行动的场景。如果接收者使用MEP解码消息， THEN sender只需要知道接收者的用途函数，以便具有最佳知识，而不需要知道接收者的初始知识状态。如果只知道用途函数的拐点的 curvature，则可以准确地传输注意力函数，即通过拐点的权重和normalized probability function。我们提出了Entropic attention communication作为可以正确地权重的通信 generalized，从而帮助设计优化的通信协议和理解人类通信。例如，我们的分析表明，在不同的利益冲突下，可以预期的合作水平。
</details></li>
</ul>
<hr>
<h2 id="Direct-and-inverse-modeling-of-soft-robots-by-learning-a-condensed-FEM-model"><a href="#Direct-and-inverse-modeling-of-soft-robots-by-learning-a-condensed-FEM-model" class="headerlink" title="Direct and inverse modeling of soft robots by learning a condensed FEM model"></a>Direct and inverse modeling of soft robots by learning a condensed FEM model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11408">http://arxiv.org/abs/2307.11408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Etienne Ménager, Tanguy Navez, Olivier Goury, Christian Duriez</li>
<li>for: 该文章是为了提出一种学习基于方法来获得一个具有充分精炼的机械表示的软体机器人控制方法。</li>
<li>methods: 该文章使用了非线性弹性数据在活动器&#x2F;效应器空间，从 Condensation of Finite Element Method 模型中提取出了一个压缩后的机械模型。</li>
<li>results: 该文章表明了这个压缩后的机械模型可以通过一个合理的数据量来学习，同时也非常高效地模拟软体机器人的直接和反直接姿势。 authors also show了一个由 FEM 模型和学习后的压缩模型组成的融合模型，并对其进行了比较。<details>
<summary>Abstract</summary>
The Finite Element Method (FEM) is a powerful modeling tool for predicting the behavior of soft robots. However, its use for control can be difficult for non-specialists of numerical computation: it requires an optimization of the computation to make it real-time. In this paper, we propose a learning-based approach to obtain a compact but sufficiently rich mechanical representation. Our choice is based on nonlinear compliance data in the actuator/effector space provided by a condensation of the FEM model. We demonstrate that this compact model can be learned with a reasonable amount of data and, at the same time, be very efficient in terms of modeling, since we can deduce the direct and inverse kinematics of the robot. We also show how to couple some models learned individually in particular on an example of a gripper composed of two soft fingers. Other results are shown by comparing the inverse model derived from the full FEM model and the one from the compact learned version. This work opens new perspectives, namely for the embedded control of soft robots, but also for their design. These perspectives are also discussed in the paper.
</details>
<details>
<summary>摘要</summary>
finite element method (FEM) 是软型机器人预测行为的强大工具。然而，它的控制使用可能会对非专家们数值计算不太容易。在这篇论文中，我们提出了一种基于学习的方法，以获得实时计算的减少。我们的选择基于软型机器人 actuator/effector 空间中的非线性弹性数据，即 FEM 模型的压缩。我们证明了这个简洁的模型可以通过一定数据量学习，同时具有高效的计算模型特性，因为我们可以直接从模型中推导软型机器人的直接和反直接骨骼动作。我们还将一些单独学习的模型，如一个由两个软指组成的抓取器，融合在一起。其他结果还是通过对全 ФЭМ 模型 derive 的反模型和学习后的模型进行比较。这项工作打开了新的视野，包括软型机器人 Embedded 控制和设计。这些视野也在论文中进行了讨论。
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Modeling-of-Inter-and-Intra-observer-Variability-in-Medical-Image-Segmentation"><a href="#Probabilistic-Modeling-of-Inter-and-Intra-observer-Variability-in-Medical-Image-Segmentation" class="headerlink" title="Probabilistic Modeling of Inter- and Intra-observer Variability in Medical Image Segmentation"></a>Probabilistic Modeling of Inter- and Intra-observer Variability in Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11397">http://arxiv.org/abs/2307.11397</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arne Schmidt, Pablo Morales-Álvarez, Rafael Molina</li>
<li>for: 这篇论文目的是提出一种新的医学图像分割模型，以减少医生之间和同一医生之间的变量，提高医学图像分割的准确性。</li>
<li>methods: 该模型called Pionono，使用多维度概率分布来捕捉每名评估者的标注行为，并将其与图像特征图层结合，生成概率性的分割预测。该模型可以通过变量推理优化，并可以进行端到端训练。</li>
<li>results: 对实际的肿瘤分割数据进行实验，显示Pionono模型在准确性和效率方面比前者模型（如STAPLE、概率U-Net等）高，同时还可以预测多个协调的分割图，提供诊断过程中的有价值信息。<details>
<summary>Abstract</summary>
Medical image segmentation is a challenging task, particularly due to inter- and intra-observer variability, even between medical experts. In this paper, we propose a novel model, called Probabilistic Inter-Observer and iNtra-Observer variation NetwOrk (Pionono). It captures the labeling behavior of each rater with a multidimensional probability distribution and integrates this information with the feature maps of the image to produce probabilistic segmentation predictions. The model is optimized by variational inference and can be trained end-to-end. It outperforms state-of-the-art models such as STAPLE, Probabilistic U-Net, and models based on confusion matrices. Additionally, Pionono predicts multiple coherent segmentation maps that mimic the rater's expert opinion, which provides additional valuable information for the diagnostic process. Experiments on real-world cancer segmentation datasets demonstrate the high accuracy and efficiency of Pionono, making it a powerful tool for medical image analysis.
</details>
<details>
<summary>摘要</summary>
医学图像分割是一项具有挑战性的任务，特别是因为 между观察者和内部观察者之间存在差异，甚至包括医学专家。在这篇论文中，我们提出了一种新的模型，即可信度描述网络（Pionono）。它捕捉每名评分员的标签行为，并将其映射到多维度的可信度分布中。然后，将这些信息与图像特征图拟合，生成可信度推测。该模型通过变分推断优化，可以在端到端方式进行训练。与现有的STAPLE、概率U-Net和基于冲突矩阵的模型相比，Pionono表现出较高的准确率和效率。此外，Pionono可以预测多个具有一致性的分割图，这些图像与评分员的专业意见相似，提供了更多有价值的信息 для诊断过程。在实际抑制癌症分割数据集上进行了实验，Pionono表现出了高度的准确性和效率，使其成为医学图像分析中的有力工具。
</details></li>
</ul>
<hr>
<h2 id="Towards-Better-Fairness-Utility-Trade-off-A-Comprehensive-Measurement-Based-Reinforcement-Learning-Framework"><a href="#Towards-Better-Fairness-Utility-Trade-off-A-Comprehensive-Measurement-Based-Reinforcement-Learning-Framework" class="headerlink" title="Towards Better Fairness-Utility Trade-off: A Comprehensive Measurement-Based Reinforcement Learning Framework"></a>Towards Better Fairness-Utility Trade-off: A Comprehensive Measurement-Based Reinforcement Learning Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11379">http://arxiv.org/abs/2307.11379</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simiao Zhang, Jitao Bai, Menghong Guan, Yihao Huang, Yueling Zhang, Jun Sun, Geguang Pu</li>
<li>for:  This paper aims to ensure the fairness of machine learning classifiers while maintaining their utility.</li>
<li>methods:  The proposed method, CFU (Comprehensive Fairness-Utility), is a reinforcement learning-based framework that considers multiple fairness metrics and utility simultaneously.</li>
<li>results:  CFU outperforms all state-of-the-art techniques and improves the classifier on multiple fairness metrics without sacrificing its utility, with an average improvement of 37.5%.Here is the answer in Simplified Chinese text:</li>
<li>for: 这篇论文目标是确保机器学习分类器的公正性，同时保持其用用性。</li>
<li>methods: 提议的方法是 CFU（全面公正性-用用性）框架，它考虑了多种公正性指标以及用用性。</li>
<li>results: CFU比所有现有技术更高效，在多种公正性指标上提高分类器，无需牺牲其用用性，平均提高37.5%。<details>
<summary>Abstract</summary>
Machine learning is widely used to make decisions with societal impact such as bank loan approving, criminal sentencing, and resume filtering. How to ensure its fairness while maintaining utility is a challenging but crucial issue. Fairness is a complex and context-dependent concept with over 70 different measurement metrics. Since existing regulations are often vague in terms of which metric to use and different organizations may prefer different fairness metrics, it is important to have means of improving fairness comprehensively. Existing mitigation techniques often target at one specific fairness metric and have limitations in improving multiple notions of fairness simultaneously. In this work, we propose CFU (Comprehensive Fairness-Utility), a reinforcement learning-based framework, to efficiently improve the fairness-utility trade-off in machine learning classifiers. A comprehensive measurement that can simultaneously consider multiple fairness notions as well as utility is established, and new metrics are proposed based on an in-depth analysis of the relationship between different fairness metrics. The reward function of CFU is constructed with comprehensive measurement and new metrics. We conduct extensive experiments to evaluate CFU on 6 tasks, 3 machine learning models, and 15 fairness-utility measurements. The results demonstrate that CFU can improve the classifier on multiple fairness metrics without sacrificing its utility. It outperforms all state-of-the-art techniques and has witnessed a 37.5% improvement on average.
</details>
<details>
<summary>摘要</summary>
在这种情况下，我们提出了 CFU（全面公平性-实用性）框架，用于改进机器学习分类器的公平性-实用性贸易。我们提出了一种涵盖多个公平性指标以及实用性的衡量方法，并基于这些指标构建了奖励函数。我们在 6 个任务、3 种机器学习模型和 15 个公平性-实用性衡量中进行了广泛的实验。结果显示，CFU 可以同时改进多个公平性指标，不 sacrifi 其实用性。它超过了所有现有技术，并在平均上提高了 37.5%。
</details></li>
</ul>
<hr>
<h2 id="LatentAugment-Data-Augmentation-via-Guided-Manipulation-of-GAN’s-Latent-Space"><a href="#LatentAugment-Data-Augmentation-via-Guided-Manipulation-of-GAN’s-Latent-Space" class="headerlink" title="LatentAugment: Data Augmentation via Guided Manipulation of GAN’s Latent Space"></a>LatentAugment: Data Augmentation via Guided Manipulation of GAN’s Latent Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11375">http://arxiv.org/abs/2307.11375</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ltronchin/latentaugment">https://github.com/ltronchin/latentaugment</a></li>
<li>paper_authors: Lorenzo Tronchin, Minh H. Vu, Paolo Soda, Tommy Löfstedt</li>
<li>for: 提高训练数据的量和多样性，并减少过拟合和提高泛化。</li>
<li>methods: 使用生成对抗网络（GANs）生成高质量样本，同时增加样本的多样性和模式覆盖率。</li>
<li>results: 在MRI-to-CT翻译任务中，使用LatentAugment DA策略可以提高模型的泛化能力，并在多样性和模式覆盖率方面超过标准DA和GAN-based sampling。<details>
<summary>Abstract</summary>
Data Augmentation (DA) is a technique to increase the quantity and diversity of the training data, and by that alleviate overfitting and improve generalisation. However, standard DA produces synthetic data for augmentation with limited diversity. Generative Adversarial Networks (GANs) may unlock additional information in a dataset by generating synthetic samples having the appearance of real images. However, these models struggle to simultaneously address three key requirements: fidelity and high-quality samples; diversity and mode coverage; and fast sampling. Indeed, GANs generate high-quality samples rapidly, but have poor mode coverage, limiting their adoption in DA applications. We propose LatentAugment, a DA strategy that overcomes the low diversity of GANs, opening up for use in DA applications. Without external supervision, LatentAugment modifies latent vectors and moves them into latent space regions to maximise the synthetic images' diversity and fidelity. It is also agnostic to the dataset and the downstream task. A wide set of experiments shows that LatentAugment improves the generalisation of a deep model translating from MRI-to-CT beating both standard DA as well GAN-based sampling. Moreover, still in comparison with GAN-based sampling, LatentAugment synthetic samples show superior mode coverage and diversity. Code is available at: https://github.com/ltronchin/LatentAugment.
</details>
<details>
<summary>摘要</summary>
数据扩展（DA）是一种技术来增加训练数据的量和多样性，从而避免过拟合和提高泛化。然而，标准的DA仅生成有限多样性的 sintetic 数据。生成对抗网络（GANs）可以从数据集中提取更多的信息，生成具有真实图像外观的 sintetic 样本。然而，这些模型很难同时满足三个关键要求：准确性和高质量样本; 多样性和模式覆盖率; 和快速采样。实际上，GANs 可以快速生成高质量样本，但它们的模式覆盖率很低，因此它们在 DA 应用中尚未得到广泛采用。我们提出了 LatentAugment，一种 DA 策略，可以在 GANs 中提高 sintetic 样本的多样性和准确性。无需外部监督，LatentAugment 会修改幽默向量并将其移动到幽默空间中，以 maximize  sintetic 样本的多样性和准确性。此外，LatentAugment 是无关于数据集和下游任务的。一系列实验表明，LatentAugment 可以提高一个深度模型从 MRI-to-CT 的翻译效果，比标准 DA 和 GAN-based 采样更高。此外，与 GAN-based 采样相比，LatentAugment 的 sintetic 样本还显示出更高的模式覆盖率和多样性。代码可以在 GitHub 上找到：https://github.com/ltronchin/LatentAugment。
</details></li>
</ul>
<hr>
<h2 id="Diverse-Offline-Imitation-via-Fenchel-Duality"><a href="#Diverse-Offline-Imitation-via-Fenchel-Duality" class="headerlink" title="Diverse Offline Imitation via Fenchel Duality"></a>Diverse Offline Imitation via Fenchel Duality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11373">http://arxiv.org/abs/2307.11373</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marin Vlastelica, Pavel Kolev, Jin Cheng, Georg Martius</li>
<li>for: 本研究旨在提出一种无监督技能发现算法，以便在无线上访问环境的前提下，通过自适应的方式学习多个独特的技能。</li>
<li>methods: 本研究使用了较新的积分信息对象函数作为内在动机，并通过对环境数据进行离线分析，实现了无监督技能发现。</li>
<li>results: 本研究的主要贡献是将 Fenchel 对偶、强化学习和无监督技能发现相连接起来，并提供了一种简单的离线算法，可以在不同的环境下学习多个与专家相似的独特技能。<details>
<summary>Abstract</summary>
There has been significant recent progress in the area of unsupervised skill discovery, with various works proposing mutual information based objectives, as a source of intrinsic motivation. Prior works predominantly focused on designing algorithms that require online access to the environment. In contrast, we develop an \textit{offline} skill discovery algorithm. Our problem formulation considers the maximization of a mutual information objective constrained by a KL-divergence. More precisely, the constraints ensure that the state occupancy of each skill remains close to the state occupancy of an expert, within the support of an offline dataset with good state-action coverage. Our main contribution is to connect Fenchel duality, reinforcement learning and unsupervised skill discovery, and to give a simple offline algorithm for learning diverse skills that are aligned with an expert.
</details>
<details>
<summary>摘要</summary>
“Recently, there have been significant advancements in the field of unsupervised skill discovery, with various studies proposing mutual information-based objectives as a source of intrinsic motivation. Previous works primarily focused on designing algorithms that require online access to the environment. In contrast, we develop an \textbf{offline} skill discovery algorithm. Our problem formulation involves maximizing a mutual information objective while constraining the KL-divergence. Specifically, the constraints ensure that the state occupancy of each skill remains close to the state occupancy of an expert, within the support of an offline dataset with good state-action coverage. Our main contribution is to connect Fenchel duality, reinforcement learning, and unsupervised skill discovery, and to provide a simple offline algorithm for learning diverse skills that are aligned with an expert.”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Random-Separating-Hyperplane-Theorem-and-Learning-Polytopes"><a href="#Random-Separating-Hyperplane-Theorem-and-Learning-Polytopes" class="headerlink" title="Random Separating Hyperplane Theorem and Learning Polytopes"></a>Random Separating Hyperplane Theorem and Learning Polytopes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11371">http://arxiv.org/abs/2307.11371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chiranjib Bhattacharyya, Ravindran Kannan, Amit Kumar</li>
<li>for: 本 paper 的目的是提供一种快速和有效地学习多聚体的算法，特别是在高维空间中。</li>
<li>methods: 本 paper 使用 Random Separating Hyperplane Theorem (RSH) 来学习多聚体。RSH 是一种强化了 Separating Hyperplane theorem 的结论，可以用来分离多聚体中的点。</li>
<li>results: 本 paper 的结果是提供了一种可以快速和有效地学习多聚体的算法，并且可以 garantuee 学习的误差的上界是 O（δ）。此外，本 paper 还提供了一种可以在高维空间中学习多聚体的算法，并且可以 garantuee 学习的误差的上界是 O（δ）。<details>
<summary>Abstract</summary>
The Separating Hyperplane theorem is a fundamental result in Convex Geometry with myriad applications. Our first result, Random Separating Hyperplane Theorem (RSH), is a strengthening of this for polytopes. $\rsh$ asserts that if the distance between $a$ and a polytope $K$ with $k$ vertices and unit diameter in $\Re^d$ is at least $\delta$, where $\delta$ is a fixed constant in $(0,1)$, then a randomly chosen hyperplane separates $a$ and $K$ with probability at least $1/poly(k)$ and margin at least $\Omega \left(\delta/\sqrt{d} \right)$. An immediate consequence of our result is the first near optimal bound on the error increase in the reduction from a Separation oracle to an Optimization oracle over a polytope.   RSH has algorithmic applications in learning polytopes. We consider a fundamental problem, denoted the ``Hausdorff problem'', of learning a unit diameter polytope $K$ within Hausdorff distance $\delta$, given an optimization oracle for $K$. Using RSH, we show that with polynomially many random queries to the optimization oracle, $K$ can be approximated within error $O(\delta)$. To our knowledge this is the first provable algorithm for the Hausdorff Problem. Building on this result, we show that if the vertices of $K$ are well-separated, then an optimization oracle can be used to generate a list of points, each within Hausdorff distance $O(\delta)$ of $K$, with the property that the list contains a point close to each vertex of $K$. Further, we show how to prune this list to generate a (unique) approximation to each vertex of the polytope. We prove that in many latent variable settings, e.g., topic modeling, LDA, optimization oracles do exist provided we project to a suitable SVD subspace. Thus, our work yields the first efficient algorithm for finding approximations to the vertices of the latent polytope under the well-separatedness assumption.
</details>
<details>
<summary>摘要</summary>
“ separating 凸函数定理”是凸 геометRY的基本结果，它在各种应用中具有重要意义。我们的第一个结果是随机分隔函数定理（RSH），它是凸函数定理的强化版本，适用于多面体。RSHasserts that if the distance between $a$ and a polytope $K$ with $k$ vertices and unit diameter in $\Re^d$ is at least $\delta$, where $\delta$ is a fixed constant in $(0,1)$, then a randomly chosen hyperplane separates $a$ and $K$ with probability at least $1/poly(k)$ and margin at least $\Omega \left(\delta/\sqrt{d} \right)$.这个结果的直接后果是关于凸函数定理中Error增加的首先近似Optimal bound。RSH有Algorithmic应用在学习多面体上。我们考虑了一个基本问题，称为“ Hausdorff 问题”，即在 Hausdorff 距离 $\delta$ 内， Learning一个 unit diameter 多面体 $K$，给定一个优化函数 oracle for $K$.使用 RSH，我们表明了可以使用 polynomially many random queries to the optimization oracle， Approximate $K$ within error $O(\delta)$。这是我们知道的第一个可证算法 для Hausdorff 问题。基于这个结果，我们还证明了如果多面体的顶点受到很好地分离，那么可以使用优化函数 oracle 生成一个包含多面体顶点的点列表，每个点在 Hausdorff 距离 $O(\delta)$ 内的 $K$ 的顶点。此外，我们还证明了如何将这个列表缩减，以生成一个（唯一）approximation to each vertex of the polytope。最后，我们证明了在多变量设置中，例如主题模型化、LDA、优化函数 oracle 存在，只要Project to a suitable SVD subspace。因此，我们的工作提供了第一个高效的算法来找到隐藏多面体的顶点approximation，假设顶点受到很好地分离。
</details></li>
</ul>
<hr>
<h2 id="Bridging-the-Reality-Gap-of-Reinforcement-Learning-based-Traffic-Signal-Control-using-Domain-Randomization-and-Meta-Learning"><a href="#Bridging-the-Reality-Gap-of-Reinforcement-Learning-based-Traffic-Signal-Control-using-Domain-Randomization-and-Meta-Learning" class="headerlink" title="Bridging the Reality Gap of Reinforcement Learning based Traffic Signal Control using Domain Randomization and Meta Learning"></a>Bridging the Reality Gap of Reinforcement Learning based Traffic Signal Control using Domain Randomization and Meta Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11357">http://arxiv.org/abs/2307.11357</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arthur Müller, Matthia Sabatelli</li>
<li>for: 本研究旨在 Addressing the reality gap challenge in Reinforcement Learning (RL) based Traffic Signal Control (TSC) systems.</li>
<li>methods: 本研究使用了两种有前途的策略来减少实际与模拟之间的差距：Domain Randomization (DR) 和 Model-Agnostic Meta-Learning (MAML).</li>
<li>results: 实验结果表明，DR 和 MAML 两种策略都能够超过现有RL算法的性能，因此有望在RL基于TSC系统中减少实际与模拟之间的差距。<details>
<summary>Abstract</summary>
Reinforcement Learning (RL) has been widely explored in Traffic Signal Control (TSC) applications, however, still no such system has been deployed in practice. A key barrier to progress in this area is the reality gap, the discrepancy that results from differences between simulation models and their real-world equivalents. In this paper, we address this challenge by first presenting a comprehensive analysis of potential simulation parameters that contribute to this reality gap. We then also examine two promising strategies that can bridge this gap: Domain Randomization (DR) and Model-Agnostic Meta-Learning (MAML). Both strategies were trained with a traffic simulation model of an intersection. In addition, the model was embedded in LemgoRL, a framework that integrates realistic, safety-critical requirements into the control system. Subsequently, we evaluated the performance of the two methods on a separate model of the same intersection that was developed with a different traffic simulator. In this way, we mimic the reality gap. Our experimental results show that both DR and MAML outperform a state-of-the-art RL algorithm, therefore highlighting their potential to mitigate the reality gap in RLbased TSC systems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="What-can-a-Single-Attention-Layer-Learn-A-Study-Through-the-Random-Features-Lens"><a href="#What-can-a-Single-Attention-Layer-Learn-A-Study-Through-the-Random-Features-Lens" class="headerlink" title="What can a Single Attention Layer Learn? A Study Through the Random Features Lens"></a>What can a Single Attention Layer Learn? A Study Through the Random Features Lens</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11353">http://arxiv.org/abs/2307.11353</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hengyu Fu, Tianyu Guo, Yu Bai, Song Mei</li>
<li>for: 本文研究了一种单个多头注意层的学习和泛化，该层接受一个序列输入和多个键向量作为输入，并使用多头注意机制来映射输入到输出序列中。</li>
<li>methods: 本文使用了随机特征设置，其中注意层有大量的头，键和值矩阵是随机冻结的，而值矩阵是可训练的。作者们表明了这种随机特征注意层可以高效地学习一类具有排序不变性的目标函数。</li>
<li>results: 作者们提供了许多与注意结构相关的特点，如（1）与标准两层随机特征网络相比，随机特征注意层在样本复杂性方面具有优势；（2）随机特征注意层可以高效地学习一类自然的目标函数；以及（3）采样Query-key权重矩阵（Query和Key矩阵的乘积）的分布对学习某些自然目标函数的效果有所影响。实验结果与理论发现相一致，并证明了样本大小和目标函数的复杂度之间的交互关系。<details>
<summary>Abstract</summary>
Attention layers -- which map a sequence of inputs to a sequence of outputs -- are core building blocks of the Transformer architecture which has achieved significant breakthroughs in modern artificial intelligence. This paper presents a rigorous theoretical study on the learning and generalization of a single multi-head attention layer, with a sequence of key vectors and a separate query vector as input. We consider the random feature setting where the attention layer has a large number of heads, with randomly sampled frozen query and key matrices, and trainable value matrices. We show that such a random-feature attention layer can express a broad class of target functions that are permutation invariant to the key vectors. We further provide quantitative excess risk bounds for learning these target functions from finite samples, using random feature attention with finitely many heads.   Our results feature several implications unique to the attention structure compared with existing random features theory for neural networks, such as (1) Advantages in the sample complexity over standard two-layer random-feature networks; (2) Concrete and natural classes of functions that can be learned efficiently by a random-feature attention layer; and (3) The effect of the sampling distribution of the query-key weight matrix (the product of the query and key matrix), where Gaussian random weights with a non-zero mean result in better sample complexities over the zero-mean counterpart for learning certain natural target functions. Experiments on simulated data corroborate our theoretical findings and further illustrate the interplay between the sample size and the complexity of the target function.
</details>
<details>
<summary>摘要</summary>
注意层 -- 将输入序列映射到输出序列 -- 是现代人工智能中的核心组件之一，它们已经实现了重要的突破。这篇论文对单个多头注意层的学习和泛化进行了严格的理论研究，输入包括一个序列的键 vector 和一个独立的查询 вектор。我们考虑了随机特征设置，其中注意层有大量的头，查询和键矩阵随机冻结，值矩阵可变。我们表明，这种随机特征注意层可以表示一类具有排序不变性的目标函数。我们进一步提供了来自 finite samples 的过度风险下界，用于学习这类目标函数。我们的结果具有以下特点：1. 相比标准的两层随机特征网络，随机特征注意层在样本复杂性方面具有优势。2. 随机特征注意层可以高效地学习一类自然的目标函数，这些目标函数具有排序不变性。3. 查询-键权重矩阵（查询和键矩阵的乘积）的采样分布对学习某些自然目标函数的性能产生了影响。在某些情况下，随机采样的查询-键权重矩阵的 Gaussian 分布可以导致更好的样本复杂性。实验结果证明了我们的理论结论，并进一步阐明了样本大小和目标函数复杂性之间的关系。
</details></li>
</ul>
<hr>
<h2 id="Model-based-Offline-Reinforcement-Learning-with-Count-based-Conservatism"><a href="#Model-based-Offline-Reinforcement-Learning-with-Count-based-Conservatism" class="headerlink" title="Model-based Offline Reinforcement Learning with Count-based Conservatism"></a>Model-based Offline Reinforcement Learning with Count-based Conservatism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11352">http://arxiv.org/abs/2307.11352</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/oh-lab/count-morl">https://github.com/oh-lab/count-morl</a></li>
<li>paper_authors: Byeongchan Kim, Min-hwan Oh</li>
<li>for: 这个论文提出了一种基于模型的离线强化学习方法，称为$\texttt{Count-MORL}$。这种方法利用状态动作对的计数估计来衡量模型估计误差，这是现有 literature 中第一个示示了计数基础保守性的算法。</li>
<li>methods: 我们的方法首先显示了状态动作对的计数估计与频率之间的相对关系。其次，我们证明了基于计数基础保守性的学习政策具有优化性 guarantees。</li>
<li>results: 通过广泛的数字实验，我们证明了 $\texttt{Count-MORL}$ 与哈希码实现在 D4RL 测试数据集上表现出色，与现有的离线RL算法相比显著超越。代码可以在 $\href{<a target="_blank" rel="noopener" href="https://github.com/oh-lab/Count-MORL%7D%7Bhttps://github.com/oh-lab/Count-MORL%7D$">https://github.com/oh-lab/Count-MORL}{https://github.com/oh-lab/Count-MORL}$</a> 上获取。<details>
<summary>Abstract</summary>
In this paper, we propose a model-based offline reinforcement learning method that integrates count-based conservatism, named $\texttt{Count-MORL}$. Our method utilizes the count estimates of state-action pairs to quantify model estimation error, marking the first algorithm of demonstrating the efficacy of count-based conservatism in model-based offline deep RL to the best of our knowledge. For our proposed method, we first show that the estimation error is inversely proportional to the frequency of state-action pairs. Secondly, we demonstrate that the learned policy under the count-based conservative model offers near-optimality performance guarantees. Through extensive numerical experiments, we validate that $\texttt{Count-MORL}$ with hash code implementation significantly outperforms existing offline RL algorithms on the D4RL benchmark datasets. The code is accessible at $\href{https://github.com/oh-lab/Count-MORL}{https://github.com/oh-lab/Count-MORL}$.
</details>
<details>
<summary>摘要</summary>
“在这篇论文中，我们提出了一种基于模型的离线束缚学习方法，称为$\texttt{Count-MORL}$。我们的方法利用状态动作对的计数估计来衡量模型估计误差，这是离线深度学习中COUNT-based保守性的首次应用，至于我们所知道的最佳办法。首先，我们证明了估计误差与状态动作对的频率成反比。其次，我们示出了learned政策在基于计数的保守模型下提供了近似优化性能保证。通过广泛的数值实验，我们证明了$\texttt{Count-MORL}$与哈希码实现在D4RL benchmark数据集上表现出色，明显超过了现有的离线RL算法。代码可以在 $\href{https://github.com/oh-lab/Count-MORL}{https://github.com/oh-lab/Count-MORL}$ 上获取。”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Bounded-P-values-in-Parametric-Programming-based-Selective-Inference"><a href="#Bounded-P-values-in-Parametric-Programming-based-Selective-Inference" class="headerlink" title="Bounded P-values in Parametric Programming-based Selective Inference"></a>Bounded P-values in Parametric Programming-based Selective Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11351">http://arxiv.org/abs/2307.11351</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shirara1016/bounded_p_values_in_si">https://github.com/shirara1016/bounded_p_values_in_si</a></li>
<li>paper_authors: Tomohiro Shiraishi, Daiki Miwa, Vo Nguyen Le Duy, Ichiro Takeuchi</li>
<li>for: 本研究旨在提出一种可靠且高效的选择推理（Selective Inference）方法，以便对数据驱动的假设进行统计检验。</li>
<li>methods: 本研究使用 Parametric Programming-based Selective Inference（PP-based SI）方法，并提出了一种计算最大和最小值的方法来降低计算成本。同时，我们还提出了三种搜索策略来有效地提高这些 bound。</li>
<li>results: 我们在线性模型和深度神经网络中进行了Feature选择和注意区域标识等假设测试问题，并证明了我们的方法的有效性和高效性。<details>
<summary>Abstract</summary>
Selective inference (SI) has been actively studied as a promising framework for statistical hypothesis testing for data-driven hypotheses. The basic idea of SI is to make inferences conditional on an event that a hypothesis is selected. In order to perform SI, this event must be characterized in a traceable form. When selection event is too difficult to characterize, additional conditions are introduced for tractability. This additional conditions often causes the loss of power, and this issue is referred to as over-conditioning. Parametric programming-based SI (PP-based SI) has been proposed as one way to address the over-conditioning issue. The main problem of PP-based SI is its high computational cost due to the need to exhaustively explore the data space. In this study, we introduce a procedure to reduce the computational cost while guaranteeing the desired precision, by proposing a method to compute the upper and lower bounds of p-values. We also proposed three types of search strategies that efficiently improve these bounds. We demonstrate the effectiveness of the proposed method in hypothesis testing problems for feature selection in linear models and attention region identification in deep neural networks.
</details>
<details>
<summary>摘要</summary>
选择性推理（SI）已经广泛研究，作为数据驱动假设检测的可能性框架。SI的基本思想是在检测假设时，根据某些事件进行 conditional inference。为了实现SI，这个事件必须能够 traces。当选择事件太难以 traces时，通常会引入额外条件，这会导致损失力量，称为过度条件。Parametric programming-based SI（PP-based SI）已经提出来解决过度条件问题。然而，PP-based SI的主要问题是高计算成本，因为需要完全探索数据空间。在这种情况下，我们提出了一种方法来降低计算成本，保证所需的精度，通过计算权值的上下限。此外，我们还提出了三种搜索策略，可以有效地提高这些上下限。我们在线性模型中的特征选择问题和深度神经网络中的注意区域标识问题中进行了示例。
</details></li>
</ul>
<hr>
<h2 id="Improving-Transferability-of-Adversarial-Examples-via-Bayesian-Attacks"><a href="#Improving-Transferability-of-Adversarial-Examples-via-Bayesian-Attacks" class="headerlink" title="Improving Transferability of Adversarial Examples via Bayesian Attacks"></a>Improving Transferability of Adversarial Examples via Bayesian Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11334">http://arxiv.org/abs/2307.11334</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qizhang Li, Yiwen Guo, Xiaochen Yang, Wangmeng Zuo, Hao Chen</li>
<li>for: 提高逆转换性，增强模型对不同任务的适应性。</li>
<li>methods:  incorporating Bayesian formulation into model parameters and input, 采用权重学习策略，提高模型的适应性和稳定性。</li>
<li>results: 实验表明， combining Bayesian formulations for both model input and parameters leads to significant improvements in transferability, 新方法在 ImageNet 和 CIFAR-10 上的平均成功率提高19.14%和2.08%，分别。<details>
<summary>Abstract</summary>
This paper presents a substantial extension of our work published at ICLR. Our ICLR work advocated for enhancing transferability in adversarial examples by incorporating a Bayesian formulation into model parameters, which effectively emulates the ensemble of infinitely many deep neural networks, while, in this paper, we introduce a novel extension by incorporating the Bayesian formulation into the model input as well, enabling the joint diversification of both the model input and model parameters. Our empirical findings demonstrate that: 1) the combination of Bayesian formulations for both the model input and model parameters yields significant improvements in transferability; 2) by introducing advanced approximations of the posterior distribution over the model input, adversarial transferability achieves further enhancement, surpassing all state-of-the-arts when attacking without model fine-tuning. Moreover, we propose a principled approach to fine-tune model parameters in such an extended Bayesian formulation. The derived optimization objective inherently encourages flat minima in the parameter space and input space. Extensive experiments demonstrate that our method achieves a new state-of-the-art on transfer-based attacks, improving the average success rate on ImageNet and CIFAR-10 by 19.14% and 2.08%, respectively, when comparing with our ICLR basic Bayesian method. We will make our code publicly available.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>The combination of Bayesian formulations for both the model input and model parameters leads to significant improvements in transferability.2. By introducing advanced approximations of the posterior distribution over the model input, adversarial transferability is further enhanced, surpassing all state-of-the-art methods when attacking without model fine-tuning.Moreover, we propose a principled approach to fine-tune model parameters in such an extended Bayesian formulation. The derived optimization objective inherently encourages flat minima in both the parameter space and input space. Extensive experiments demonstrate that our method achieves a new state-of-the-art on transfer-based attacks, improving the average success rate on ImageNet and CIFAR-10 by 19.14% and 2.08%, respectively, when compared with our ICLR basic Bayesian method. Our code will be publicly available.</details></li>
</ol>
<hr>
<h2 id="Demystifying-Local-and-Global-Fairness-Trade-offs-in-Federated-Learning-Using-Partial-Information-Decomposition"><a href="#Demystifying-Local-and-Global-Fairness-Trade-offs-in-Federated-Learning-Using-Partial-Information-Decomposition" class="headerlink" title="Demystifying Local and Global Fairness Trade-offs in Federated Learning Using Partial Information Decomposition"></a>Demystifying Local and Global Fairness Trade-offs in Federated Learning Using Partial Information Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11333">http://arxiv.org/abs/2307.11333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Faisal Hamman, Sanghamitra Dutta</li>
<li>For: This paper aims to provide an information-theoretic perspective on group fairness trade-offs in federated learning (FL) with respect to sensitive attributes, such as gender and race.* Methods: The paper uses a body of work in information theory called partial information decomposition (PID) to identify three sources of unfairness in FL, namely, Unique Disparity, Redundant Disparity, and Masked Disparity.* Results: The paper derives fundamental limits and trade-offs between global and local fairness, particularly under data heterogeneity, and presents experimental results on benchmark datasets to support the theoretical findings.Here is the same information in Simplified Chinese:* For: 这篇论文目标是通过对敏感特征（如性别和种族）的联邦学习（FL）中的群体公平负担进行信息理论的视角来探讨。* Methods: 这篇论文使用信息理论中的剩余信息分解（PID）技术来识别联邦学习中的三种不公平源，即唯一差异、重复差异和遮盖差异。* Results: 这篇论文 derivates了全球公平和本地公平之间的基本限制和负担，特别是在数据不均衡情况下，并通过使用标准例子来说明这三种差异如何影响全球和本地公平。<details>
<summary>Abstract</summary>
In this paper, we present an information-theoretic perspective to group fairness trade-offs in federated learning (FL) with respect to sensitive attributes, such as gender, race, etc. Existing works mostly focus on either \emph{global fairness} (overall disparity of the model across all clients) or \emph{local fairness} (disparity of the model at each individual client), without always considering their trade-offs. There is a lack of understanding of the interplay between global and local fairness in FL, and if and when one implies the other. To address this gap, we leverage a body of work in information theory called partial information decomposition (PID) which first identifies three sources of unfairness in FL, namely, \emph{Unique Disparity}, \emph{Redundant Disparity}, and \emph{Masked Disparity}. Using canonical examples, we demonstrate how these three disparities contribute to global and local fairness. This decomposition helps us derive fundamental limits and trade-offs between global or local fairness, particularly under data heterogeneity, as well as, derive conditions under which one implies the other. We also present experimental results on benchmark datasets to support our theoretical findings. This work offers a more nuanced understanding of the sources of disparity in FL that can inform the use of local disparity mitigation techniques, and their convergence and effectiveness when deployed in practice.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种信息学方面的视角来探讨在联合学习（Federated Learning，FL）中的群体公平负担。现有的工作主要关注global fairness（总体模型对所有客户的不均衡）或local fairness（每个客户模型不均衡），而不一定考虑它们之间的负担。对于FL中的群体公平负担的理解匮乏，特别是这些负担之间的关系。为了解决这个难题，我们利用了一种信息理论中的分解技术（partial information decomposition，PID），并识别了FL中三种不公平来源，即Unique Disparity、Redundant Disparity和Masked Disparity。使用标准示例，我们示出了这三种不公平如何影响全局和本地公平。这种分解帮助我们 derive fundamental limits和这些负担之间的负担规则，特别是在数据不均衡情况下。我们还提供了实验结果，以支持我们的理论发现。这项工作为FL中不公平负担管理技术的使用提供了更加细化的理解，以及这些技术在实践中的协调和效果。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Convergence-Identifiability-of-Machine-Learning-and-Deep-Learning-Models"><a href="#Beyond-Convergence-Identifiability-of-Machine-Learning-and-Deep-Learning-Models" class="headerlink" title="Beyond Convergence: Identifiability of Machine Learning and Deep Learning Models"></a>Beyond Convergence: Identifiability of Machine Learning and Deep Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11332">http://arxiv.org/abs/2307.11332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reza Sameni</li>
<li>for:  investigate the notion of model parameter identifiability through a case study</li>
<li>methods:  utilizing a deep neural network to estimate subject-wise parameters from motion sensor data</li>
<li>results:  certain parameters can be identified, while others remain unidentifiable due to the experimental setup’s limitations<details>
<summary>Abstract</summary>
Machine learning (ML) and deep learning models are extensively used for parameter optimization and regression problems. However, not all inverse problems in ML are ``identifiable,'' indicating that model parameters may not be uniquely determined from the available data and the data model's input-output relationship. In this study, we investigate the notion of model parameter identifiability through a case study focused on parameter estimation from motion sensor data. Utilizing a bipedal-spring mass human walk dynamics model, we generate synthetic data representing diverse gait patterns and conditions. Employing a deep neural network, we attempt to estimate subject-wise parameters, including mass, stiffness, and equilibrium leg length. The results show that while certain parameters can be identified from the observation data, others remain unidentifiable, highlighting that unidentifiability is an intrinsic limitation of the experimental setup, necessitating a change in data collection and experimental scenarios. Beyond this specific case study, the concept of identifiability has broader implications in ML and deep learning. Addressing unidentifiability requires proven identifiable models (with theoretical support), multimodal data fusion techniques, and advancements in model-based machine learning. Understanding and resolving unidentifiability challenges will lead to more reliable and accurate applications across diverse domains, transcending mere model convergence and enhancing the reliability of machine learning models.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）和深度学习模型在参数优化和回归问题中广泛应用。然而，不是所有机器学习 inverse problem 是可识别的，表示模型参数可能不是来自可用数据和输入输出关系的数据模型的唯一确定。在本研究中，我们通过人行动数据的情况进行研究，探讨模型参数可识别性的概念。使用一个人体弹簧模型，我们生成了多种步态和条件的 sintetic 数据。使用深度神经网络，我们尝试了每个参与者的参数，包括质量、刚度和平衡脚长。结果表明，有些参数可以从观察数据中提取，而其他参数则无法准确地确定，这反映了实验设置的内在限制，需要更改数据收集和实验方法。这种特定的案例研究还有更广泛的意义在机器学习和深度学习中。解决不可识别性需要有理据支持的可识别模型、多Modal 数据融合技术和机器学习模型的发展。更好地理解和解决不可识别性挑战，将导致更可靠、更准确的应用在多个领域，超越模型的极限并提高机器学习模型的可靠性。
</details></li>
</ul>
<hr>
<h2 id="Methodologies-for-Improving-Modern-Industrial-Recommender-Systems"><a href="#Methodologies-for-Improving-Modern-Industrial-Recommender-Systems" class="headerlink" title="Methodologies for Improving Modern Industrial Recommender Systems"></a>Methodologies for Improving Modern Industrial Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01204">http://arxiv.org/abs/2308.01204</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shusen Wang</li>
<li>for: 这篇论文是为了提高现代工业 recommender systems（RS）的方法ologys。</li>
<li>methods: 这篇论文使用了一些现代工业RS的实践经验和不公开的参考文献。</li>
<li>results: 这篇论文提出了一些有效的方法，可以提高现代工业RS的适用率和持续时间。<details>
<summary>Abstract</summary>
Recommender system (RS) is an established technology with successful applications in social media, e-commerce, entertainment, and more. RSs are indeed key to the success of many popular APPs, such as YouTube, Tik Tok, Xiaohongshu, Bilibili, and others. This paper explores the methodology for improving modern industrial RSs. It is written for experienced RS engineers who are diligently working to improve their key performance indicators, such as retention and duration. The experiences shared in this paper have been tested in some real industrial RSs and are likely to be generalized to other RSs as well. Most contents in this paper are industry experience without publicly available references.
</details>
<details>
<summary>摘要</summary>
“推荐系统（RS）是一种已经成熟的技术，在社交媒体、电商、娱乐等领域都有成功应用。RS在许多受欢迎的APP中扮演重要角色，如YouTube、Tik Tok、Xiaohongshu和Bilibili等。本文探讨现代工业RS的改进方法。这篇文章主要对于经验丰富的RS工程师，以提高关键性表现指标（如滞留率和使用时间）为目标。文中的经验主要基于实际的工业应用，并且可能对其他RS进行应用。”Note: Please keep in mind that the translation is based on the provided text and may not be perfect or entirely accurate, as the nuances of the original text may be lost in translation.
</details></li>
</ul>
<hr>
<h2 id="Systematic-Adaptation-of-Communication-focused-Machine-Learning-Models-from-Real-to-Virtual-Environments-for-Human-Robot-Collaboration"><a href="#Systematic-Adaptation-of-Communication-focused-Machine-Learning-Models-from-Real-to-Virtual-Environments-for-Human-Robot-Collaboration" class="headerlink" title="Systematic Adaptation of Communication-focused Machine Learning Models from Real to Virtual Environments for Human-Robot Collaboration"></a>Systematic Adaptation of Communication-focused Machine Learning Models from Real to Virtual Environments for Human-Robot Collaboration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11327">http://arxiv.org/abs/2307.11327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Debasmita Mukherjee, Ritwik Singhai, Homayoun Najjaran</li>
<li>for: 这篇论文旨在解决虚拟现实环境中手势识别问题，以便实现人机合作 robots 的embodied teleoperation。</li>
<li>methods: 该论文提出了一种系统性的框架，通过限制大小的虚拟数据集和精心制作的数据集来适应虚拟环境。</li>
<li>results: 该论文通过对实际环境中训练的深度学习模型进行适应，在虚拟环境中实现了高效的手势识别。<details>
<summary>Abstract</summary>
Virtual reality has proved to be useful in applications in several fields ranging from gaming, medicine, and training to development of interfaces that enable human-robot collaboration. It empowers designers to explore applications outside of the constraints posed by the real world environment and develop innovative solutions and experiences. Hand gestures recognition which has been a topic of much research and subsequent commercialization in the real world has been possible because of the creation of large, labelled datasets. In order to utilize the power of natural and intuitive hand gestures in the virtual domain for enabling embodied teleoperation of collaborative robots, similarly large datasets must be created so as to keep the working interface easy to learn and flexible enough to add more gestures. Depending on the application, this may be computationally or economically prohibitive. Thus, the adaptation of trained deep learning models that perform well in the real environment to the virtual may be a solution to this challenge. This paper presents a systematic framework for the real to virtual adaptation using limited size of virtual dataset along with guidelines for creating a curated dataset. Finally, while hand gestures have been considered as the communication mode, the guidelines and recommendations presented are generic. These are applicable to other modes such as body poses and facial expressions which have large datasets available in the real domain which must be adapted to the virtual one.
</details>
<details>
<summary>摘要</summary>
虚拟现实已经在各种领域展示了其用途，包括游戏、医疗、训练和人机合作交互的开发。它让设计师能够在虚拟环境中探索不受实际环境限制的应用，并开发创新的解决方案和体验。手势认识是虚拟现实中的一个重要话题，因为大量标注的数据集的创建使得手势在实际世界中得到了商业化。为了在虚拟世界中使用自然和直观的手势，需要创建大量的虚拟数据集，以保持工作界面简单易学习，并能够添加更多的手势。在应用程序方面，这可能是计算机或经济上的瓶颈。因此，将已经在实际环境中表现好的深度学习模型适应到虚拟环境可能是一个解决方案。本文提出了一个系统化的实际环境到虚拟环境的适应框架，以及创建审核数据集的指南。最后，尽管手势被视为交流方式，但是这些指南和建议适用于其他模式，如身体姿态和表情，这些在实际环境中有大量数据集可以适应到虚拟环境。
</details></li>
</ul>
<hr>
<h2 id="Analysis-of-Elephant-Movement-in-Sub-Saharan-Africa-Ecological-Climatic-and-Conservation-Perspectives"><a href="#Analysis-of-Elephant-Movement-in-Sub-Saharan-Africa-Ecological-Climatic-and-Conservation-Perspectives" class="headerlink" title="Analysis of Elephant Movement in Sub-Saharan Africa: Ecological, Climatic, and Conservation Perspectives"></a>Analysis of Elephant Movement in Sub-Saharan Africa: Ecological, Climatic, and Conservation Perspectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11325">http://arxiv.org/abs/2307.11325</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew Hines, Gregory Glatzer, Shreya Ghosh, Prasenjit Mitra</li>
<li>for: 这项研究旨在更深入理解非洲亚洲 elephant 的移动 behaviors，以便更好地保护这些动物。</li>
<li>methods: 该研究使用分析方法来揭示非洲亚洲 elephant 的移动模式，包括考虑季节变化和降水 patrerns 等生态因素。</li>
<li>results: 研究发现 elephant 的移动 behaviors 受到季节变化和降水 patrerns 等生态因素的影响，并提供了一种可预测 elephant 移动模式的方法，这可能为保护这些动物提供有价值的信息。<details>
<summary>Abstract</summary>
The interaction between elephants and their environment has profound implications for both ecology and conservation strategies. This study presents an analytical approach to decipher the intricate patterns of elephant movement in Sub-Saharan Africa, concentrating on key ecological drivers such as seasonal variations and rainfall patterns. Despite the complexities surrounding these influential factors, our analysis provides a holistic view of elephant migratory behavior in the context of the dynamic African landscape. Our comprehensive approach enables us to predict the potential impact of these ecological determinants on elephant migration, a critical step in establishing informed conservation strategies. This projection is particularly crucial given the impacts of global climate change on seasonal and rainfall patterns, which could substantially influence elephant movements in the future. The findings of our work aim to not only advance the understanding of movement ecology but also foster a sustainable coexistence of humans and elephants in Sub-Saharan Africa. By predicting potential elephant routes, our work can inform strategies to minimize human-elephant conflict, effectively manage land use, and enhance anti-poaching efforts. This research underscores the importance of integrating movement ecology and climatic variables for effective wildlife management and conservation planning.
</details>
<details>
<summary>摘要</summary>
elephants和其环境之间的互动对生态和保护策略有深刻的影响。这项研究提出了一种分析方法，用于解读在亚洲南部的非洲大象运动征分布的复杂模式，主要考虑了季节变化和降水 patrerns等生态因素。尽管这些因素具有复杂性，但我们的分析提供了一个整体的视角，用于理解非洲静止的大象migrations。我们的全面approach可以预测大象移动的可能性，这是为建立有知识基础的保护策略提供了关键的一步。由于全球气候变化对季节和降水 patrerns的影响，这些因素可能会对大象移动产生深刻的影响。我们的研究结果希望能不仅提高大象运动学的理解，还能推动人类和大象之间可持续合作的发展，以保护非洲南部的野生动物资源。我们的工作可以预测可能的大象路线，以便为人类-大象冲突的避免、土地管理和反贼战斗提供有价值的信息。这项研究强调了在保护野生动物资源方面 integrating movement ecology和 climatic variables的重要性。
</details></li>
</ul>
<hr>
<h2 id="XLDA-Linear-Discriminant-Analysis-for-Scaling-Continual-Learning-to-Extreme-Classification-at-the-Edge"><a href="#XLDA-Linear-Discriminant-Analysis-for-Scaling-Continual-Learning-to-Extreme-Classification-at-the-Edge" class="headerlink" title="XLDA: Linear Discriminant Analysis for Scaling Continual Learning to Extreme Classification at the Edge"></a>XLDA: Linear Discriminant Analysis for Scaling Continual Learning to Extreme Classification at the Edge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11317">http://arxiv.org/abs/2307.11317</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karan Shah, Vishruth Veerendranath, Anushka Hebbar, Raghavendra Bhat</li>
<li>For: 该研究旨在提出一种基于流式线性减少分析（XLDA）的分类器，用于在边缘部署中进行类增量学习（Class-IL），并在极端分类场景下保持等效性。* Methods: 该研究提出了一种基于XLDA的框架，包括对LDA分类器的证明，以及在边缘部署中进行训练和推断的优化策略，以提高效率和可扩展性。* Results: 研究人员通过使用批处理训练策略和最近匹配搜索策略，在极端分类场景下实现了至多42倍的训练速度减少和至多5倍的推断速度减少，在AliProducts和Google Landmarks V2等极端数据集上进行了实验 validate。<details>
<summary>Abstract</summary>
Streaming Linear Discriminant Analysis (LDA) while proven in Class-incremental Learning deployments at the edge with limited classes (upto 1000), has not been proven for deployment in extreme classification scenarios. In this paper, we present: (a) XLDA, a framework for Class-IL in edge deployment where LDA classifier is proven to be equivalent to FC layer including in extreme classification scenarios, and (b) optimizations to enable XLDA-based training and inference for edge deployment where there is a constraint on available compute resources. We show up to 42x speed up using a batched training approach and up to 5x inference speedup with nearest neighbor search on extreme datasets like AliProducts (50k classes) and Google Landmarks V2 (81k classes)
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>流式线性准确分析（LDA）在Edge环境中进行类增量学习部署（upto 1000）已经证明，但在极端分类场景下没有得到证明。本文提出了以下两点：（a）XLDA，一个基于LDA分类器的Edge环境中的类增量学习框架，其中LDA分类器在极端分类场景下与FC层等价；（b）为了实现XLDA基于训练和推断的编制和优化，以便在 Edge环境中使用有限的计算资源。我们在极端 dataset like AliProducts (50k类) 和 Google Landmarks V2 (81k类) 上实现了最多42倍的批处理训练速度和最多5倍的 nearest neighbor search 推断速度。
</details></li>
</ul>
<hr>
<h2 id="Making-Pre-trained-Language-Models-both-Task-solvers-and-Self-calibrators"><a href="#Making-Pre-trained-Language-Models-both-Task-solvers-and-Self-calibrators" class="headerlink" title="Making Pre-trained Language Models both Task-solvers and Self-calibrators"></a>Making Pre-trained Language Models both Task-solvers and Self-calibrators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11316">http://arxiv.org/abs/2307.11316</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yangyi-chen/lm-toast">https://github.com/yangyi-chen/lm-toast</a></li>
<li>paper_authors: Yangyi Chen, Xingyao Wang, Heng Ji</li>
<li>for: 这 paper 的目的是提高 PLM 的自信估计，使其在错误预测时不再过于自信。</li>
<li>methods: 这 paper 使用了一种叫做 LM-TOAST 的训练算法，以使 PLM 同时成为任务解决者和自我调整器。</li>
<li>results: 实验结果表明，LM-TOAST 可以有效地利用训练数据，使 PLM 有合理的自信估计，而不会影响其原始任务性能。<details>
<summary>Abstract</summary>
Pre-trained language models (PLMs) serve as backbones for various real-world systems. For high-stake applications, it's equally essential to have reasonable confidence estimations in predictions. While the vanilla confidence scores of PLMs can already be effectively utilized, PLMs consistently become overconfident in their wrong predictions, which is not desirable in practice. Previous work shows that introducing an extra calibration task can mitigate this issue. The basic idea involves acquiring additional data to train models in predicting the confidence of their initial predictions. However, it only demonstrates the feasibility of this kind of method, assuming that there are abundant extra available samples for the introduced calibration task. In this work, we consider the practical scenario that we need to effectively utilize training samples to make PLMs both task-solvers and self-calibrators. Three challenges are presented, including limited training samples, data imbalance, and distribution shifts. We first conduct pilot experiments to quantify various decisive factors in the calibration task. Based on the empirical analysis results, we propose a training algorithm LM-TOAST to tackle the challenges. Experimental results show that LM-TOAST can effectively utilize the training data to make PLMs have reasonable confidence estimations while maintaining the original task performance. Further, we consider three downstream applications, namely selective classification, adversarial defense, and model cascading, to show the practical usefulness of LM-TOAST. The code will be made public at \url{https://github.com/Yangyi-Chen/LM-TOAST}.
</details>
<details>
<summary>摘要</summary>
预训言语模型（PLM）作为各种实际系统的基础，其中一个重要问题是保证预测结果的可靠性。虽然vanilla confidence scores已经可以有效地使用，但PLM在错误预测时常常过分自信，这不是实际应用中所需的。 previous work表明，通过添加额外的calibration任务可以解决这个问题。基本思想是通过训练模型预测其初始预测的可信度。然而，这只是一种可行的方法，假设有充足的额外可用样本。在这种实际场景中，我们需要使PLM同时成为任务解决者和自我调整者。我们描述了三个挑战，包括有限的训练样本、数据不均衡和分布shift。我们首先进行了飞行实验，以量化各种决定性因素在calibration任务中。根据实验分析结果，我们提出了一种训练算法LM-TOAST，用于解决这些挑战。实验结果表明，LM-TOAST可以有效地利用训练数据，使PLM有合理的可信度估计，同时保持原始任务性能。此外，我们考虑了三个下游应用，namely selective classification、adversarial defense和model cascading，以显示LM-TOAST的实际用途。代码将在\url{https://github.com/Yangyi-Chen/LM-TOAST}公开。
</details></li>
</ul>
<hr>
<h2 id="Artificial-Intelligence-Generated-Terahertz-Multi-Resonant-Metasurfaces-via-Improved-Transformer-and-CGAN-Neural-Networks"><a href="#Artificial-Intelligence-Generated-Terahertz-Multi-Resonant-Metasurfaces-via-Improved-Transformer-and-CGAN-Neural-Networks" class="headerlink" title="Artificial Intelligence-Generated Terahertz Multi-Resonant Metasurfaces via Improved Transformer and CGAN Neural Networks"></a>Artificial Intelligence-Generated Terahertz Multi-Resonant Metasurfaces via Improved Transformer and CGAN Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11794">http://arxiv.org/abs/2307.11794</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yangpeng Huang, Naixing Feng, Yijun Cai</li>
<li>For: This paper proposes improved Transformer and conditional generative adversarial neural networks (CGAN) for the inverse design of graphene metasurfaces based on THz multi-resonant absorption spectra.* Methods: The paper uses traditional deep neural networks (DNNs), improved Transformer, and CGAN for the inverse design of graphene metasurfaces.* Results: The improved Transformer achieves higher accuracy and generalization performance in the StoV design, while the StoI design achieved through CGAN provides more comprehensive information and higher accuracy than the StoV design obtained by MLP. The improved CGAN can also achieve the inverse design of graphene metasurface images directly from the desired multi-resonant absorption spectra.Here’s the Chinese translation of the three key points:* For: 这篇论文提出了基于 THz 多晶谐振荷重Graphene 元件的 inverse 设计方法，使用传统的深度神经网络 (DNNs) 和改进的 Transformer 和 conditional generative adversarial neural networks (CGAN)。* Methods: 这篇论文使用了 DNNs、改进的 Transformer 和 CGAN 进行 inverse 设计。* Results: 改进的 Transformer 在 StoV 设计中实现了更高的准确率和泛化能力，而 StoI 设计通过 CGAN 实现了更全面的信息和更高的准确率，而且改进的 CGAN 还可以直接从愿景多晶谐振荷谱图中生成图像。<details>
<summary>Abstract</summary>
It is well known that the inverse design of terahertz (THz) multi-resonant graphene metasurfaces by using traditional deep neural networks (DNNs) has limited generalization ability. In this paper, we propose improved Transformer and conditional generative adversarial neural networks (CGAN) for the inverse design of graphene metasurfaces based upon THz multi-resonant absorption spectra. The improved Transformer can obtain higher accuracy and generalization performance in the StoV (Spectrum to Vector) design compared to traditional multilayer perceptron (MLP) neural networks, while the StoI (Spectrum to Image) design achieved through CGAN can provide more comprehensive information and higher accuracy than the StoV design obtained by MLP. Moreover, the improved CGAN can achieve the inverse design of graphene metasurface images directly from the desired multi-resonant absorption spectra. It is turned out that this work can finish facilitating the design process of artificial intelligence-generated metasurfaces (AIGM), and even provide a useful guide for developing complex THz metasurfaces based on 2D materials using generative neural networks.
</details>
<details>
<summary>摘要</summary>
bekannt ist, dass die inverse Design von terahertz (THz) multi-resonant graphene metasurfaces using traditional deep neural networks (DNNs) begrenzt ist. In diesem paper, we propose improved Transformer and conditional generative adversarial neural networks (CGAN) for the inverse Design of graphene metasurfaces based on THz multi-resonant absorption spectra. die verbesserte Transformer kann eine höhere Genauigkeit und Generalisierungleistung im StoV (Spektrum zu Vektor) Design gegenüber traditionellen multilayer perceptron (MLP) neural networks erzielen, während die StoI (Spektrum zu Bild) Gestaltung durch CGAN mehr umfassende Informationen und eine höhere Genauigkeit als die StoV Gestaltung durch MLP bereitstellt. besides, the improved CGAN can achieve the inverse design of graphene metasurface images directly from the desired multi-resonant absorption spectra. it turns out that this work can facilitate the design process of artificial intelligence-generated metasurfaces (AIGM), and even provide a useful guide for developing complex THz metasurfaces based on 2D materials using generative neural networks.
</details></li>
</ul>
<hr>
<h2 id="Neuromorphic-Online-Learning-for-Spatiotemporal-Patterns-with-a-Forward-only-Timeline"><a href="#Neuromorphic-Online-Learning-for-Spatiotemporal-Patterns-with-a-Forward-only-Timeline" class="headerlink" title="Neuromorphic Online Learning for Spatiotemporal Patterns with a Forward-only Timeline"></a>Neuromorphic Online Learning for Spatiotemporal Patterns with a Forward-only Timeline</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11314">http://arxiv.org/abs/2307.11314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenhang Zhang, Jingang Jin, Haowen Fang, Qinru Qiu</li>
<li>for: 这个论文主要针对的是在线学习神经网络模型（SNN），以提高神经网络的能效性和扩展它们的应用范围。</li>
<li>methods: 这篇论文提出了一种名为Spatiotemporal Online Learning for Synaptic Adaptation（SOLSA）的在线学习算法，用于学习具有泄漏散发和软重置的窗口级神经元（LIF）和其相关的Synapse。该算法不仅学习了synaptic weight，还适应了时间滤波器。相比BPTT算法，SOLSA具有远低的内存需求，并实现了更好的时间工作负荷分布。此外，SOLSA还包含了启用技术，如调度的weight更新、早期停止训练和自适应synapse滤波器，这些技术使得SOLSA更快速地 converges和提高了学习性能。</li>
<li>results: 相比非BPTT基于SNN学习算法，SOLSA在平均学习精度上提高了14.2%。而相比BPTT算法，SOLSA在内存成本下降72%的情况下，实现了5%高的平均学习精度。<details>
<summary>Abstract</summary>
Spiking neural networks (SNNs) are bio-plausible computing models with high energy efficiency. The temporal dynamics of neurons and synapses enable them to detect temporal patterns and generate sequences. While Backpropagation Through Time (BPTT) is traditionally used to train SNNs, it is not suitable for online learning of embedded applications due to its high computation and memory cost as well as extended latency. Previous works have proposed online learning algorithms, but they often utilize highly simplified spiking neuron models without synaptic dynamics and reset feedback, resulting in subpar performance. In this work, we present Spatiotemporal Online Learning for Synaptic Adaptation (SOLSA), specifically designed for online learning of SNNs composed of Leaky Integrate and Fire (LIF) neurons with exponentially decayed synapses and soft reset. The algorithm not only learns the synaptic weight but also adapts the temporal filters associated to the synapses. Compared to the BPTT algorithm, SOLSA has much lower memory requirement and achieves a more balanced temporal workload distribution. Moreover, SOLSA incorporates enhancement techniques such as scheduled weight update, early stop training and adaptive synapse filter, which speed up the convergence and enhance the learning performance. When compared to other non-BPTT based SNN learning, SOLSA demonstrates an average learning accuracy improvement of 14.2%. Furthermore, compared to BPTT, SOLSA achieves a 5% higher average learning accuracy with a 72% reduction in memory cost.
</details>
<details>
<summary>摘要</summary>
神经网络（SNN）是生物可能性计算模型，具有高能效性。 neuron和 synapse 的时间动态性允许它们检测时间模式和生成序列。 而传统上使用的 Backpropagation Through Time（BPTT）不适用于嵌入式应用的在线学习，因为它具有高计算和存储成本以及延迟。 先前的工作已经提出了在线学习算法，但它们通常使用简化的脑神经模型，无法考虑 synaptic 动态和软Reset，导致性能不佳。 在这种工作中，我们提出了Spatiotemporal Online Learning for Synaptic Adaptation（SOLSA），专门为SNNs组成的Leaky Integrate and Fire（LIF） neurons with exponentially decayed synapses and soft reset进行在线学习。该算法不仅学习synaptic weight，还适应相关的时间筛子。相比BPTT算法，SOLSA具有远低的内存需求，并实现了更平衡的时间工作负荷分布。此外，SOLSA还包括了加强技术，如计划的weight更新、早期停止训练和适应synapse筛子，快速增长和提高学习性能。与其他非BPTT基于SNN学习相比，SOLSA表现出14.2%的学习精度提高。而与BPTT相比，SOLSA实现了72%的内存成本减少，并达到15%高的学习精度。
</details></li>
</ul>
<hr>
<h2 id="Who-should-I-Collaborate-with-A-Comparative-Study-of-Academia-and-Industry-Research-Collaboration-in-NLP"><a href="#Who-should-I-Collaborate-with-A-Comparative-Study-of-Academia-and-Industry-Research-Collaboration-in-NLP" class="headerlink" title="Who should I Collaborate with? A Comparative Study of Academia and Industry Research Collaboration in NLP"></a>Who should I Collaborate with? A Comparative Study of Academia and Industry Research Collaboration in NLP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04524">http://arxiv.org/abs/2308.04524</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hussain Sadiq Abuwala, Bohan Zhang, Mushi Wang</li>
<li>for: investigate the effects of collaboration between academia and industry on Natural Language Processing (NLP)</li>
<li>methods: created a pipeline to extract affiliations and citations from NLP papers and divided them into three categories: academia, industry, and hybrid (collaborations between academia and industry)</li>
<li>results: found a trend towards an increase in industry and academia-industry collaboration publications, and these types of publications tend to have a higher impact compared to those produced solely within academia.Here’s the information in Simplified Chinese text:</li>
<li>for: 研究学术与产业之间的协作对自然语言处理（NLP）的影响</li>
<li>methods: 创建了一个管道，EXTRACT affiliations and citations from NLP papers, and divided them into three categories: academia, industry, and hybrid (collaborations between academia and industry)</li>
<li>results: 发现了协作类别的发表数量在提高，并且这些类别的发表物 tend to have a higher impact compared to those produced solely within academia.<details>
<summary>Abstract</summary>
The goal of our research was to investigate the effects of collaboration between academia and industry on Natural Language Processing (NLP). To do this, we created a pipeline to extract affiliations and citations from NLP papers and divided them into three categories: academia, industry, and hybrid (collaborations between academia and industry). Our empirical analysis found that there is a trend towards an increase in industry and academia-industry collaboration publications and that these types of publications tend to have a higher impact compared to those produced solely within academia.
</details>
<details>
<summary>摘要</summary>
我们的研究目标是研究学术与工业合作对自然语言处理（NLP）的影响。为此，我们创建了一个管道，以EXTRACT affiliations和引用信息从NLP论文中，并将其分为三类：学术、产业和混合（学术与产业合作）。我们的实证分析发现， there is a trend towards an increase in industry and academia-industry collaboration publications, and these types of publications tend to have a higher impact compared to those produced solely within academia.Note: Please note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="PI-VEGAN-Physics-Informed-Variational-Embedding-Generative-Adversarial-Networks-for-Stochastic-Differential-Equations"><a href="#PI-VEGAN-Physics-Informed-Variational-Embedding-Generative-Adversarial-Networks-for-Stochastic-Differential-Equations" class="headerlink" title="PI-VEGAN: Physics Informed Variational Embedding Generative Adversarial Networks for Stochastic Differential Equations"></a>PI-VEGAN: Physics Informed Variational Embedding Generative Adversarial Networks for Stochastic Differential Equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11289">http://arxiv.org/abs/2307.11289</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruisong Gao, Yufeng Wang, Min Yang, Chuanjun Chen</li>
<li>for: 解决Stochastic Differential Equations（SDEs）的前向、逆向和混合问题，只有部分系统参数的感知数据available。</li>
<li>methods:  integrate governing physical laws into Physics Informed Variational Embedding Generative Adversarial Network (PI-VEGAN) with automatic differentiation, introduce variational encoder to approximate latent variables, and use stochastic gradient descent algorithm to update components.</li>
<li>results: PI-VEGAN achieves satisfactory stability and accuracy in addressing forward, inverse, and mixed problems, outperforming previous Physics-Informed Generative Adversarial Network (PI-WGAN) in numerical results.<details>
<summary>Abstract</summary>
We present a new category of physics-informed neural networks called physics informed variational embedding generative adversarial network (PI-VEGAN), that effectively tackles the forward, inverse, and mixed problems of stochastic differential equations. In these scenarios, the governing equations are known, but only a limited number of sensor measurements of the system parameters are available. We integrate the governing physical laws into PI-VEGAN with automatic differentiation, while introducing a variational encoder for approximating the latent variables of the actual distribution of the measurements. These latent variables are integrated into the generator to facilitate accurate learning of the characteristics of the stochastic partial equations. Our model consists of three components, namely the encoder, generator, and discriminator, each of which is updated alternatively employing the stochastic gradient descent algorithm. We evaluate the effectiveness of PI-VEGAN in addressing forward, inverse, and mixed problems that require the concurrent calculation of system parameters and solutions. Numerical results demonstrate that the proposed method achieves satisfactory stability and accuracy in comparison with the previous physics-informed generative adversarial network (PI-WGAN).
</details>
<details>
<summary>摘要</summary>
我们提出了一新类型的物理学信息泛化网络（PI-VEGAN），用于有效地解决Stochastic Differential Equations（SDEs）中的前向、逆向和混合问题。在这些场景下，系统参数的 governing equations 已知，但只有一个有限多的感知器测量可用。我们将物理法则integrated into PI-VEGAN中，并引入了一个变量编码器来 aproximate latent variables的实际分布。这些 latent variables 被 integrate into the generator 以便准确地学习 SDEs 的特征。我们的模型包括三个组件：编码器、生成器和批判器，每个组件都在使用随机梯度下降算法进行更新。我们评估了PI-VEGAN的有效性，并发现它在解决前向、逆向和混合问题时具有满意的稳定性和准确性，比之前的物理学信息泛化网络（PI-WGAN）更好。
</details></li>
</ul>
<hr>
<h2 id="Kernelized-Offline-Contextual-Dueling-Bandits"><a href="#Kernelized-Offline-Contextual-Dueling-Bandits" class="headerlink" title="Kernelized Offline Contextual Dueling Bandits"></a>Kernelized Offline Contextual Dueling Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11288">http://arxiv.org/abs/2307.11288</a></li>
<li>repo_url: None</li>
<li>paper_authors: Viraj Mehta, Ojash Neopane, Vikramjeet Das, Sen Lin, Jeff Schneider, Willie Neiswanger</li>
<li>for: 这个论文主要针对哪些应用场景？</li>
<li>methods: 这个论文使用哪些方法？</li>
<li>results: 这个论文获得了哪些结果？Here are the answers, in Simplified Chinese:</li>
<li>for: 这个论文主要针对 preference-based 反馈的应用场景，如人工智能学习从人类反馈中获得的大语言模型。</li>
<li>methods: 这个论文使用 offline contextual dueling bandit 设定，并提供了一种 upper-confidence-bound 样式的算法，以及证明了 regret bound。</li>
<li>results: 这个论文的算法比一种使用均匀随机 contexts 的策略表现更好，并得到了实验证明。<details>
<summary>Abstract</summary>
Preference-based feedback is important for many applications where direct evaluation of a reward function is not feasible. A notable recent example arises in reinforcement learning from human feedback on large language models. For many of these applications, the cost of acquiring the human feedback can be substantial or even prohibitive. In this work, we take advantage of the fact that often the agent can choose contexts at which to obtain human feedback in order to most efficiently identify a good policy, and introduce the offline contextual dueling bandit setting. We give an upper-confidence-bound style algorithm for this setting and prove a regret bound. We also give empirical confirmation that this method outperforms a similar strategy that uses uniformly sampled contexts.
</details>
<details>
<summary>摘要</summary>
Preferences-based feedback 是许多应用程序中非常重要的，特别是当直接评估奖金函数不可能时。一个最近的例子来自大自然语言模型的人工反馈。许多这些应用程序中，人类反馈的成本可能很高或甚至是不可接受的。在这种情况下，我们利用agent可以选择获取人类反馈的上下文，以便最有效地标识一个好策略，并引入了线上上下文战斗带 setting。我们提供了一种上确界风格的算法，并证明了一个违和 bound。我们还提供了实验证明，这种方法在相似的策略中表现比较好。
</details></li>
</ul>
<hr>
<h2 id="MAS-Towards-Resource-Efficient-Federated-Multiple-Task-Learning"><a href="#MAS-Towards-Resource-Efficient-Federated-Multiple-Task-Learning" class="headerlink" title="MAS: Towards Resource-Efficient Federated Multiple-Task Learning"></a>MAS: Towards Resource-Efficient Federated Multiple-Task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11285">http://arxiv.org/abs/2307.11285</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiming Zhuang, Yonggang Wen, Lingjuan Lyu, Shuai Zhang</li>
<li>for: 这篇研究旨在实现多元分布式机器学习（Federated Learning，FL）系统，以实现在分散的Edge设备上进行内置模型训练。</li>
<li>methods: 本研究提出了首个实现多元FL任务训练的系统，称为MAS（Merge and Split）。MAS首先将多元FL任务合并为一个统一FL任务，然后在训练一些循环后，使用任务之间的相互关联性来拆分为多个FL任务。接着，MAS将每个拆分的FL任务继续训练，根据在统一训练中获得的模型参数。</li>
<li>results: 实验结果显示，MAS比其他方法具有更好的性能，同时降低了训练时间和能源消耗。实验结果显示，在训练20个FL任务时，MAS可以降低训练时间2倍，并降低能源消耗40%。<details>
<summary>Abstract</summary>
Federated learning (FL) is an emerging distributed machine learning method that empowers in-situ model training on decentralized edge devices. However, multiple simultaneous FL tasks could overload resource-constrained devices. In this work, we propose the first FL system to effectively coordinate and train multiple simultaneous FL tasks. We first formalize the problem of training simultaneous FL tasks. Then, we present our new approach, MAS (Merge and Split), to optimize the performance of training multiple simultaneous FL tasks. MAS starts by merging FL tasks into an all-in-one FL task with a multi-task architecture. After training for a few rounds, MAS splits the all-in-one FL task into two or more FL tasks by using the affinities among tasks measured during the all-in-one training. It then continues training each split of FL tasks based on model parameters from the all-in-one training. Extensive experiments demonstrate that MAS outperforms other methods while reducing training time by 2x and reducing energy consumption by 40%. We hope this work will inspire the community to further study and optimize training simultaneous FL tasks.
</details>
<details>
<summary>摘要</summary>
federa 学习（FL）是一种emerging分布式机器学习方法，它允许在分布式边缘设备上进行增 Situ 模型训练。然而，多个同时进行FL任务可能会过载具有限的设备资源。在这种情况下，我们提出了首个可以有效协调和训练多个同时FL任务的FL系统。我们首先正式定义同时训练多个FL任务的问题。然后，我们介绍了我们的新方法MAS（合并并分），用于优化训练多个FL任务的性能。MAS开始是将FL任务合并成一个所有任务的FL任务，并使用多任务架构进行训练。在训练一些循环后，MAS将所有任务合并的FL任务分解成两个或更多的FL任务，然后继续基于所有任务训练的模型参数进行每个分解的FL任务训练。广泛的实验表明，MAS在减少训练时间2x，减少能量消耗40%的情况下，能够超越其他方法。我们希望这种工作能够激励社区进一步研究并优化同时训练FL任务。
</details></li>
</ul>
<hr>
<h2 id="Epsilon-Privacy-Metric-for-Machine-Learning-Models"><a href="#Epsilon-Privacy-Metric-for-Machine-Learning-Models" class="headerlink" title="Epsilon*: Privacy Metric for Machine Learning Models"></a>Epsilon*: Privacy Metric for Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11280">http://arxiv.org/abs/2307.11280</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diana M. Negoescu, Humberto Gonzalez, Saad Eddin Al Orjany, Jilei Yang, Yuliia Lut, Rahul Tandra, Xiaowen Zhang, Xinyi Zheng, Zach Douglas, Vidita Nolkha, Parvez Ahammad, Gennady Samorodnitsky<br>for:The paper aims to provide a new privacy metric called Epsilon* to measure the privacy risk of a single model instance before, during, or after deployment of privacy mitigation strategies.methods:The metric does not require access to the training data sampling or model training algorithm, and is based on a hypothesis test used by an adversary in a membership inference attack.results:The paper shows that Epsilon* is sensitive to privacy risk mitigation by training with differential privacy (DP), where the value of Epsilon* is reduced by up to 800% compared to the Epsilon* values of non-DP trained baseline models. This allows privacy auditors to be independent of model owners and enables all decision-makers to visualize the privacy-utility landscape to make informed decisions regarding the trade-offs between model privacy and utility.Here is the same information in Simplified Chinese:for:这篇论文目标是提供一个新的隐私度量 metric  called Epsilon<em>，用于在模型实例之前、 durante 或者 после部署隐私修正策略时测量隐私风险。methods:metric 不需要访问训练数据采样或模型训练算法，基于一个 adversary 在成员推测攻击中使用的 гипотеза测试。results:论文显示，使用权限隐私（DP）训练可以降低 Epsilon</em> 的值，相比非DP 训练基eline 模型，最多降低 800%。这使得隐私审计人员可以独立于模型所有者，并且允许所有决策者在隐私与用途之间做出 Informed 决策。<details>
<summary>Abstract</summary>
We introduce Epsilon*, a new privacy metric for measuring the privacy risk of a single model instance prior to, during, or after deployment of privacy mitigation strategies. The metric does not require access to the training data sampling or model training algorithm. Epsilon* is a function of true positive and false positive rates in a hypothesis test used by an adversary in a membership inference attack. We distinguish between quantifying the privacy loss of a trained model instance and quantifying the privacy loss of the training mechanism which produces this model instance. Existing approaches in the privacy auditing literature provide lower bounds for the latter, while our metric provides a lower bound for the former by relying on an (${\epsilon}$,${\delta}$)-type of quantification of the privacy of the trained model instance. We establish a relationship between these lower bounds and show how to implement Epsilon* to avoid numerical and noise amplification instability. We further show in experiments on benchmark public data sets that Epsilon* is sensitive to privacy risk mitigation by training with differential privacy (DP), where the value of Epsilon* is reduced by up to 800% compared to the Epsilon* values of non-DP trained baseline models. This metric allows privacy auditors to be independent of model owners, and enables all decision-makers to visualize the privacy-utility landscape to make informed decisions regarding the trade-offs between model privacy and utility.
</details>
<details>
<summary>摘要</summary>
我们引入ε*, a new privacy metric to measure the privacy risk of a single model instance before, during, or after deploying privacy mitigation strategies. This metric does not require access to the training data sampling or model training algorithm. ε* is a function of true positive and false positive rates in a hypothesis test used by an adversary in a membership inference attack. We distinguish between quantifying the privacy loss of a trained model instance and quantifying the privacy loss of the training mechanism that produces this model instance. Existing approaches in the privacy auditing literature provide lower bounds for the latter, while our metric provides a lower bound for the former by relying on an (${\epsilon}$,${\delta}$)-type of quantification of the privacy of the trained model instance. We establish a relationship between these lower bounds and show how to implement ε* to avoid numerical and noise amplification instability. We further show in experiments on benchmark public data sets that ε* is sensitive to privacy risk mitigation by training with differential privacy (DP), where the value of ε* is reduced by up to 800% compared to the ε* values of non-DP trained baseline models. This metric allows privacy auditors to be independent of model owners, and enables all decision-makers to visualize the privacy-utility landscape to make informed decisions regarding the trade-offs between model privacy and utility.
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Segment-from-Noisy-Annotations-A-Spatial-Correction-Approach"><a href="#Learning-to-Segment-from-Noisy-Annotations-A-Spatial-Correction-Approach" class="headerlink" title="Learning to Segment from Noisy Annotations: A Spatial Correction Approach"></a>Learning to Segment from Noisy Annotations: A Spatial Correction Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02498">http://arxiv.org/abs/2308.02498</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/michaelofsbu/spatialcorrection">https://github.com/michaelofsbu/spatialcorrection</a></li>
<li>paper_authors: Jiachen Yao, Yikai Zhang, Songzhu Zheng, Mayank Goswami, Prateek Prasanna, Chao Chen</li>
<li>for: 本研究旨在提高深度神经网络（DNNs）在医学图像分割任务中的性能，通过处理杂乱标注（noisy labels）。</li>
<li>methods: 我们提出了一种新的Markov模型，用于捕捉分割杂乱标注中的空间相关性和偏好性。此外，我们还提出了一种标签修正方法，用于逐步修复真实标签。</li>
<li>results: 我们的方法在 sintetic和实际杂乱标注数据上进行了实验，并证明了我们的方法可以超过现有状态的各种方法。<details>
<summary>Abstract</summary>
Noisy labels can significantly affect the performance of deep neural networks (DNNs). In medical image segmentation tasks, annotations are error-prone due to the high demand in annotation time and in the annotators' expertise. Existing methods mostly assume noisy labels in different pixels are \textit{i.i.d}. However, segmentation label noise usually has strong spatial correlation and has prominent bias in distribution. In this paper, we propose a novel Markov model for segmentation noisy annotations that encodes both spatial correlation and bias. Further, to mitigate such label noise, we propose a label correction method to recover true label progressively. We provide theoretical guarantees of the correctness of the proposed method. Experiments show that our approach outperforms current state-of-the-art methods on both synthetic and real-world noisy annotations.
</details>
<details>
<summary>摘要</summary>
干扰标签可以影响深度神经网络（DNNs）的性能。在医疗图像分割任务中，标注错误率高，主要因为标注时间和标注人员的专业知识需求很高。现有方法大多假设不同像素的干扰标签是独立的，但实际上，分割标签噪声通常具有强相关性和明显的偏见。在这篇论文中，我们提出了一种新的马尔可夫模型，用于捕捉分割噪声标注的空间相关性和偏见。此外，我们还提出了一种标签修正方法，可以逐步recover真实的标签。我们提供了理论保证方法的正确性。实验表明，我们的方法在 sintetic和实际噪声标注上都超过了当前状态的先进方法。
</details></li>
</ul>
<hr>
<h2 id="Screening-Mammography-Breast-Cancer-Detection"><a href="#Screening-Mammography-Breast-Cancer-Detection" class="headerlink" title="Screening Mammography Breast Cancer Detection"></a>Screening Mammography Breast Cancer Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11274">http://arxiv.org/abs/2307.11274</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chakrabortyde/rsna-breast-cancer">https://github.com/chakrabortyde/rsna-breast-cancer</a></li>
<li>paper_authors: Debajyoti Chakraborty</li>
<li>for: 提高乳癌检测效率和准确率，减少成本和假阳性结果导致的患者担忧。</li>
<li>methods: 使用自动化 breast cancer 检测方法，包括不同的方法ologies 在 RSNA 数据集上进行测试，并获得了 roughly 20,000 名女性患者的 radiographic breast images 的平均验证案例 pF1 分数为 0.56。</li>
<li>results: 通过自动化检测方法，可以提高乳癌检测的效率和准确率，减少成本和假阳性结果导致的患者担忧。<details>
<summary>Abstract</summary>
Breast cancer is a leading cause of cancer-related deaths, but current programs are expensive and prone to false positives, leading to unnecessary follow-up and patient anxiety. This paper proposes a solution to automated breast cancer detection, to improve the efficiency and accuracy of screening programs. Different methodologies were tested against the RSNA dataset of radiographic breast images of roughly 20,000 female patients and yielded an average validation case pF1 score of 0.56 across methods.
</details>
<details>
<summary>摘要</summary>
乳癌是癌症相关死亡率的主要原因，但目前的计划昂贵，且容易出现假阳性结果，导致无需跟踪和患者担忧。本文提出一种自动乳癌检测方案，以提高检测计划的效率和准确率。不同的方法在RSNA数据集上进行了测试，并获得了 roughly 20,000 名女性患者的乳影像数据集的平均验证案例 pF1 分数0.56。
</details></li>
</ul>
<hr>
<h2 id="On-the-Fisher-Rao-Gradient-of-the-Evidence-Lower-Bound"><a href="#On-the-Fisher-Rao-Gradient-of-the-Evidence-Lower-Bound" class="headerlink" title="On the Fisher-Rao Gradient of the Evidence Lower Bound"></a>On the Fisher-Rao Gradient of the Evidence Lower Bound</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11249">http://arxiv.org/abs/2307.11249</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nihat Ay, Jesse van Oostrum</li>
<li>for: 这篇论文研究了鱼asser-Rao梯度，也称为自然梯度，是证据下界的证据，它在变换自适应机器、海尔曼机器和自由能原理中扮演重要的角色。</li>
<li>methods: 该论文使用了变换自适应机器、海尔曼机器和自由能原理中的各种方法，包括使用鱼asser-Rao梯度的自然梯度和库拉克-莱布尔差分的自然梯度。</li>
<li>results: 研究发现，在满足某些条件下，最小化 prime objective function 和最大化 ELBO 都可以 Ensure the equivalence of minimizing the prime objective function and maximizing the ELBO.<details>
<summary>Abstract</summary>
This article studies the Fisher-Rao gradient, also referred to as the natural gradient, of the evidence lower bound, the ELBO, which plays a crucial role within the theory of the Variational Autonecoder, the Helmholtz Machine and the Free Energy Principle. The natural gradient of the ELBO is related to the natural gradient of the Kullback-Leibler divergence from a target distribution, the prime objective function of learning. Based on invariance properties of gradients within information geometry, conditions on the underlying model are provided that ensure the equivalence of minimising the prime objective function and the maximisation of the ELBO.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Leveraging-arbitrary-mobile-sensor-trajectories-with-shallow-recurrent-decoder-networks-for-full-state-reconstruction"><a href="#Leveraging-arbitrary-mobile-sensor-trajectories-with-shallow-recurrent-decoder-networks-for-full-state-reconstruction" class="headerlink" title="Leveraging arbitrary mobile sensor trajectories with shallow recurrent decoder networks for full-state reconstruction"></a>Leveraging arbitrary mobile sensor trajectories with shallow recurrent decoder networks for full-state reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11793">http://arxiv.org/abs/2307.11793</a></li>
<li>repo_url: None</li>
<li>paper_authors: Megan R. Ebers, Jan P. Williams, Katherine M. Steele, J. Nathan Kutz</li>
<li>for: 这种论文的目的是提出一种基于深度学习的模型，用于在动态系统中进行感知和状态估计。</li>
<li>methods: 这种模型使用了长短时冲Memery（LSTM）网络和推理器，将流动感知器的轨迹信息转化为全状态空间估计。</li>
<li>results: 试验表明，这种模型可以准确地重建全状态空间，并且在不同的动态轨迹下可以快速适应。此外，模型还可以减少抽象误差的方差，并且可以在训练数据外进行快速推理。<details>
<summary>Abstract</summary>
Sensing is one of the most fundamental tasks for the monitoring, forecasting and control of complex, spatio-temporal systems. In many applications, a limited number of sensors are mobile and move with the dynamics, with examples including wearable technology, ocean monitoring buoys, and weather balloons. In these dynamic systems (without regions of statistical-independence), the measurement time history encodes a significant amount of information that can be extracted for critical tasks. Most model-free sensing paradigms aim to map current sparse sensor measurements to the high-dimensional state space, ignoring the time-history all together. Using modern deep learning architectures, we show that a sequence-to-vector model, such as an LSTM (long, short-term memory) network, with a decoder network, dynamic trajectory information can be mapped to full state-space estimates. Indeed, we demonstrate that by leveraging mobile sensor trajectories with shallow recurrent decoder networks, we can train the network (i) to accurately reconstruct the full state space using arbitrary dynamical trajectories of the sensors, (ii) the architecture reduces the variance of the mean-square error of the reconstruction error in comparison with immobile sensors, and (iii) the architecture also allows for rapid generalization (parameterization of dynamics) for data outside the training set. Moreover, the path of the sensor can be chosen arbitrarily, provided training data for the spatial trajectory of the sensor is available. The exceptional performance of the network architecture is demonstrated on three applications: turbulent flows, global sea-surface temperature data, and human movement biomechanics.
</details>
<details>
<summary>摘要</summary>
感测是观测、预测和控制复杂空间时间系统的基本任务之一。在许多应用中，有限数量的感测器是移动的，其中例子包括佩戴式技术、海洋监测浮标和天气气球。在这些动态系统中（没有独立的统计区域），测量时间历史中含有大量信息，可以EXTRACT FOR CRITICAL TASKS。大多数无模型感测方法忽略时间历史，只是将当前稀缺的感测值映射到高维状态空间中。使用现代深度学习架构，我们显示了一种序列到向量模型，如LSTM（长短期记忆）网络，并将decoder网络与动态轨迹信息结合使用。我们可以通过这种方法来：1. 使用移动感测器的动态轨迹，准确重建全状态空间。2. 相比 stationary 感测器，使用这种架构可以降低mean-square error的方差。3. 该架构允许快速通用化（参数化动态），可以在训练集外进行数据处理。此外，感测器的路径可以随意选择，只需要提供感测器的空间轨迹训练数据即可。我们在三个应用中展示了该网络架构的出色表现：液体动力学、全球海面温度数据和人体运动生物力学。
</details></li>
</ul>
<hr>
<h2 id="On-Sensor-Data-Filtering-using-Neuromorphic-Computing-for-High-Energy-Physics-Experiments"><a href="#On-Sensor-Data-Filtering-using-Neuromorphic-Computing-for-High-Energy-Physics-Experiments" class="headerlink" title="On-Sensor Data Filtering using Neuromorphic Computing for High Energy Physics Experiments"></a>On-Sensor Data Filtering using Neuromorphic Computing for High Energy Physics Experiments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11242">http://arxiv.org/abs/2307.11242</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shruti R. Kulkarni, Aaron Young, Prasanna Date, Narasinga Rao Miniskar, Jeffrey S. Vetter, Farah Fahim, Benjamin Parpillon, Jennet Dickinson, Nhan Tran, Jieun Yoo, Corrinne Mills, Morris Swartz, Petar Maksimovic, Catherine D. Schuman, Alice Bean</li>
<li>for: 这个研究探讨了基于神经omorphic计算的脉冲神经网络（SNN）模型，用于高能物理实验中的传感器数据筛选。</li>
<li>methods: 我们提出了一种压缩型神经omorphic模型，用于基于粒子的横向动量进行数据筛选，以减少下游电子设备接收的数据量。我们将入来的电荷波形转换为二进制值事件流，然后由SNN进行处理。</li>
<li>results: 我们的研究表明，使用进化算法和优化的超参数，SNN可以达到约91%的信号效率，并且减少了大约一半的参数量，相比深度神经网络。<details>
<summary>Abstract</summary>
This work describes the investigation of neuromorphic computing-based spiking neural network (SNN) models used to filter data from sensor electronics in high energy physics experiments conducted at the High Luminosity Large Hadron Collider. We present our approach for developing a compact neuromorphic model that filters out the sensor data based on the particle's transverse momentum with the goal of reducing the amount of data being sent to the downstream electronics. The incoming charge waveforms are converted to streams of binary-valued events, which are then processed by the SNN. We present our insights on the various system design choices - from data encoding to optimal hyperparameters of the training algorithm - for an accurate and compact SNN optimized for hardware deployment. Our results show that an SNN trained with an evolutionary algorithm and an optimized set of hyperparameters obtains a signal efficiency of about 91% with nearly half as many parameters as a deep neural network.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Edgewise-outliers-of-network-indexed-signals"><a href="#Edgewise-outliers-of-network-indexed-signals" class="headerlink" title="Edgewise outliers of network indexed signals"></a>Edgewise outliers of network indexed signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11239">http://arxiv.org/abs/2307.11239</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kristats/spout">https://github.com/kristats/spout</a></li>
<li>paper_authors: Christopher Rieser, Anne Ruiz-Gazen, Christine Thomas-Agnan</li>
<li>for: 本文旨在提出模型，用于处理网络索引多变量数据，包括变量之间的依赖关系和图节点之间的关系。</li>
<li>methods: 本文使用了修改后的MCD算法，以检测图像中的异常点。首先，我们计算了一些平方 Mahalanobis 距离的分布，以便定制检测规则和阈值。然后，我们提出了一种Robust MCD算法，称为 Edgewise MCD。</li>
<li>results: 在模拟数据和实际数据集上，我们的方法可以准确地检测图像中的异常点。同时，我们还证明了在不考虑依赖关系时，检测异常点的方法可能会导致 false positive 的出现。<details>
<summary>Abstract</summary>
We consider models for network indexed multivariate data involving a dependence between variables as well as across graph nodes.   In the framework of these models, we focus on outliers detection and introduce the concept of edgewise outliers. For this purpose, we first derive the distribution of some sums of squares, in particular squared Mahalanobis distances that can be used to fix detection rules and thresholds for outlier detection. We then propose a robust version of the deterministic MCD algorithm that we call edgewise MCD. An application on simulated data shows the interest of taking the dependence structure into account. We also illustrate the utility of the proposed method with a real data set.
</details>
<details>
<summary>摘要</summary>
我们考虑网络索引多变量数据中存在变量之间和图节点之间的依赖关系。在这个框架下，我们关注异常检测，并引入边缘异常概念。为此，我们首先计算一些平方差的分布，特别是方差距离的平方的分布，可以用于固定检测规则和阈值 для异常检测。然后，我们提出了一种robust版本的权重MCD算法，称为边缘MCD。在对模拟数据进行应用后，我们发现了考虑依赖结构的利好。此外，我们还通过实际数据集来证明方法的实用性。
</details></li>
</ul>
<hr>
<h2 id="QDC-Quantum-Diffusion-Convolution-Kernels-on-Graphs"><a href="#QDC-Quantum-Diffusion-Convolution-Kernels-on-Graphs" class="headerlink" title="QDC: Quantum Diffusion Convolution Kernels on Graphs"></a>QDC: Quantum Diffusion Convolution Kernels on Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11234">http://arxiv.org/abs/2307.11234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Markovich</li>
<li>for: 提高图像预测精度</li>
<li>methods: 基于量子扩散概念的新卷积核心（Quantum Diffusion Convolution，QDC）和传统的 combinatorial Laplacian 的多尺度组合</li>
<li>results: 在多种数据集上，与类似方法相比，QDC 能够提高预测性能<details>
<summary>Abstract</summary>
Graph convolutional neural networks (GCNs) operate by aggregating messages over local neighborhoods given the prediction task under interest. Many GCNs can be understood as a form of generalized diffusion of input features on the graph, and significant work has been dedicated to improving predictive accuracy by altering the ways of message passing. In this work, we propose a new convolution kernel that effectively rewires the graph according to the occupation correlations of the vertices by trading on the generalized diffusion paradigm for the propagation of a quantum particle over the graph. We term this new convolution kernel the Quantum Diffusion Convolution (QDC) operator. In addition, we introduce a multiscale variant that combines messages from the QDC operator and the traditional combinatorial Laplacian. To understand our method, we explore the spectral dependence of homophily and the importance of quantum dynamics in the construction of a bandpass filter. Through these studies, as well as experiments on a range of datasets, we observe that QDC improves predictive performance on the widely used benchmark datasets when compared to similar methods.
</details>
<details>
<summary>摘要</summary>
图 convolutional neural networks (GCNs)  operate by aggregating messages over local neighborhoods given the prediction task under interest. Many GCNs can be understood as a form of generalized diffusion of input features on the graph, and significant work has been dedicated to improving predictive accuracy by altering the ways of message passing. In this work, we propose a new convolution kernel that effectively rewires the graph according to the occupation correlations of the vertices by trading on the generalized diffusion paradigm for the propagation of a quantum particle over the graph. We term this new convolution kernel the Quantum Diffusion Convolution (QDC) operator. In addition, we introduce a multiscale variant that combines messages from the QDC operator and the traditional combinatorial Laplacian. To understand our method, we explore the spectral dependence of homophily and the importance of quantum dynamics in the construction of a bandpass filter. Through these studies, as well as experiments on a range of datasets, we observe that QDC improves predictive performance on the widely used benchmark datasets when compared to similar methods.Here's the translation in Traditional Chinese:同步 convolutional neural networks (GCNs)  operate by aggregating messages over local neighborhoods given the prediction task under interest. Many GCNs can be understood as a form of generalized diffusion of input features on the graph, and significant work has been dedicated to improving predictive accuracy by altering the ways of message passing. In this work, we propose a new convolution kernel that effectively rewires the graph according to the occupation correlations of the vertices by trading on the generalized diffusion paradigm for the propagation of a quantum particle over the graph. We term this new convolution kernel the Quantum Diffusion Convolution (QDC) operator. In addition, we introduce a multiscale variant that combines messages from the QDC operator and the traditional combinatorial Laplacian. To understand our method, we explore the spectral dependence of homophily and the importance of quantum dynamics in the construction of a bandpass filter. Through these studies, as well as experiments on a range of datasets, we observe that QDC improves predictive performance on the widely used benchmark datasets when compared to similar methods.
</details></li>
</ul>
<hr>
<h2 id="From-Adaptive-Query-Release-to-Machine-Unlearning"><a href="#From-Adaptive-Query-Release-to-Machine-Unlearning" class="headerlink" title="From Adaptive Query Release to Machine Unlearning"></a>From Adaptive Query Release to Machine Unlearning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11228">http://arxiv.org/abs/2307.11228</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enayat Ullah, Raman Arora</li>
<li>for: 这个论文主要关注的问题是机器学习忘记（machine unlearning），即设计高效的忘记算法，以应对学习算法的选择性查询。</li>
<li>methods: 论文提出了高效的忘记算法，包括线性和预处理查询类型的算法。</li>
<li>results: 论文得到了在多个应用中的改进保证，特别是在随机凸优化（SCO）等问题中。在不同的损失函数下，论文得到了不同的忘记算法，其中对于光滑 lipschitz 损失函数和任意 $\rho&gt;0$，论文得到了忘记算法的质量损失风险为 $\tilde O\big(\frac{1}{\sqrt{n}+\frac{\sqrt{d}{n\rho}\big)$，并且忘记查询（梯度）复杂度为 $\tilde O(\rho \cdot \text{Retraining Complexity})$。<details>
<summary>Abstract</summary>
We formalize the problem of machine unlearning as design of efficient unlearning algorithms corresponding to learning algorithms which perform a selection of adaptive queries from structured query classes. We give efficient unlearning algorithms for linear and prefix-sum query classes. As applications, we show that unlearning in many problems, in particular, stochastic convex optimization (SCO), can be reduced to the above, yielding improved guarantees for the problem. In particular, for smooth Lipschitz losses and any $\rho>0$, our results yield an unlearning algorithm with excess population risk of $\tilde O\big(\frac{1}{\sqrt{n}+\frac{\sqrt{d}{n\rho}\big)$ with unlearning query (gradient) complexity $\tilde O(\rho \cdot \text{Retraining Complexity})$, where $d$ is the model dimensionality and $n$ is the initial number of samples. For non-smooth Lipschitz losses, we give an unlearning algorithm with excess population risk $\tilde O\big(\frac{1}{\sqrt{n}+\big(\frac{\sqrt{d}{n\rho}\big)^{1/2}\big)$ with the same unlearning query (gradient) complexity. Furthermore, in the special case of Generalized Linear Models (GLMs), such as those in linear and logistic regression, we get dimension-independent rates of $\tilde O\big(\frac{1}{\sqrt{n} +\frac{1}{(n\rho)^{2/3}\big)$ and $\tilde O\big(\frac{1}{\sqrt{n} +\frac{1}{(n\rho)^{1/3}\big)$ for smooth Lipschitz and non-smooth Lipschitz losses respectively. Finally, we give generalizations of the above from one unlearning request to \textit{dynamic} streams consisting of insertions and deletions.
</details>
<details>
<summary>摘要</summary>
我们正式化机器学习忘记问题为设计高效的忘记算法，与学习算法相似的选择适应查询。我们提供了高效的忘记算法 для线性和预先构成查询类。我们还证明了忘记在许多问题中，特别是随机凸似对映运算（SCO），可以被简化为上述问题，从而获得改善的保证。具体而言，对于光滑 lipschitz 损失函数并且任意 $\rho>0$，我们的结果提供了一个忘记算法，其错失人口风险为 $\tilde O\big(\frac{1}{\sqrt{n}+\frac{\sqrt{d}{n\rho}\big)$，忘记查询（Gradient）复杂度为 $\tilde O(\rho \cdot \text{Retraining Complexity})$。对非光滑 lipschitz 损失函数，我们提供了一个忘记算法，其错失人口风险为 $\tilde O\big(\frac{1}{\sqrt{n}+\big(\frac{\sqrt{d}{n\rho}\big)^{1/2}\big)$，忘记查询复杂度仍为 $\tilde O(\rho \cdot \text{Retraining Complexity})$。尤其是，在通用化线性模型（GLM）中，例如线性和对数回传 regression，我们得到了维度独立的约 $\tilde O\big(\frac{1}{\sqrt{n} +\frac{1}{(n\rho)^{2/3}\big)$ 和 $\tilde O\big(\frac{1}{\sqrt{n} +\frac{1}{(n\rho)^{1/3}\big)$ 的错失人口风险。最后，我们将上述结果推广到多个忘记请求的情况，包括动态流过中的插入和删除。
</details></li>
</ul>
<hr>
<h2 id="Quantum-Convolutional-Neural-Networks-with-Interaction-Layers-for-Classification-of-Classical-Data"><a href="#Quantum-Convolutional-Neural-Networks-with-Interaction-Layers-for-Classification-of-Classical-Data" class="headerlink" title="Quantum Convolutional Neural Networks with Interaction Layers for Classification of Classical Data"></a>Quantum Convolutional Neural Networks with Interaction Layers for Classification of Classical Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11792">http://arxiv.org/abs/2307.11792</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jishnu Mahmud, Raisa Mashtura, Shaikh Anowarul Fattah, Mohammad Saquib</li>
<li>for: 研究多比特交互对量子神经网络的影响，提高表达能力和嵌入能力。</li>
<li>methods: 提出了一种量子卷积网络，利用三元器件交互层，提高网络的表达能力和嵌入能力。</li>
<li>results: 在MNIST、Fashion MNIST和芳香 dataset上进行了二分和多分类预测，并与现有状态 искусственный智能方法进行比较，结果表明该方法可以超越现有的状态 искусственный智能方法。<details>
<summary>Abstract</summary>
Quantum Machine Learning (QML) has come into the limelight due to the exceptional computational abilities of quantum computers. With the promises of near error-free quantum computers in the not-so-distant future, it is important that the effect of multi-qubit interactions on quantum neural networks is studied extensively. This paper introduces a Quantum Convolutional Network with novel Interaction layers exploiting three-qubit interactions increasing the network's expressibility and entangling capability, for classifying both image and one-dimensional data. The proposed approach is tested on three publicly available datasets namely MNIST, Fashion MNIST, and Iris datasets, to perform binary and multiclass classifications and is found to supersede the performance of the existing state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
量子机器学习（QML）因量子计算机的特殊计算能力而受到关注。随着近Error-free量子计算机的未来降低，研究多比特交互对量子神经网络的影响非常重要。本文提出了一种具有新型互动层的量子卷积网络，利用三比特交互提高网络的表达能力和排序能力，用于图像和一维数据的分类。该方法在MNIST、Fashion MNIST和爬行 datasets上进行了三类和多类分类测试，并被证明超过了现有状态的方法表现。
</details></li>
</ul>
<hr>
<h2 id="Jina-Embeddings-A-Novel-Set-of-High-Performance-Sentence-Embedding-Models"><a href="#Jina-Embeddings-A-Novel-Set-of-High-Performance-Sentence-Embedding-Models" class="headerlink" title="Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models"></a>Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11224">http://arxiv.org/abs/2307.11224</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Günther, Louis Milliken, Jonathan Geuter, Georgios Mastrapas, Bo Wang, Han Xiao</li>
<li>for: 本研究的目的是开发一种高性能的句子嵌入模型，能够将不同的文本输入翻译为数字表示，捕捉文本的 semantic essence。</li>
<li>methods: 本研究使用了高质量的对Alignment和 triplet dataset，进行模型训练。同时， authors also constructed a novel dataset of negated and non-negated statements，以提高模型对否定语言的识别能力。</li>
<li>results: 本研究通过 Massive Textual Embedding Benchmark (MTEB) 进行了广泛的性能评估，并取得了优异的结果。 In addition, the authors also provided a publicly available dataset of negated and non-negated statements, which can be used to improve the model’s ability to recognize negations.<details>
<summary>Abstract</summary>
Jina Embeddings constitutes a set of high-performance sentence embedding models adept at translating various textual inputs into numerical representations, thereby capturing the semantic essence of the text. The models excel in applications such as dense retrieval and semantic textual similarity. This paper details the development of Jina Embeddings, starting with the creation of high-quality pairwise and triplet datasets. It underlines the crucial role of data cleaning in dataset preparation, gives in-depth insights into the model training process, and concludes with a comprehensive performance evaluation using the Massive Textual Embedding Benchmark (MTEB). To increase the model's awareness of negations, we constructed a novel training and evaluation dataset of negated and non-negated statements, which we make publicly available to the community.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="FairMobi-Net-A-Fairness-aware-Deep-Learning-Model-for-Urban-Mobility-Flow-Generation"><a href="#FairMobi-Net-A-Fairness-aware-Deep-Learning-Model-for-Urban-Mobility-Flow-Generation" class="headerlink" title="FairMobi-Net: A Fairness-aware Deep Learning Model for Urban Mobility Flow Generation"></a>FairMobi-Net: A Fairness-aware Deep Learning Model for Urban Mobility Flow Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11214">http://arxiv.org/abs/2307.11214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhewei Liu, Lipai Huang, Chao Fan, Ali Mostafavi</li>
<li>for: 帮助研究者和实践者更好地理解城市结构和人口活动模式，并且实现重要的城市规划和管理应用。</li>
<li>methods: 提出了一种新的、具有公平性的深度学习模型，即 FairMobi-Net，用于跨地区人流预测。该模型独特地将公平性损失添加到损失函数中，并使用了混合方法，将二分类和数值回归技术结合在一起 для人流预测。</li>
<li>results: 对四个美国城市的人流数据进行了详细验证，并发现 FairMobi-Net 模型在不同地区之间的人流预测中具有更高的准确性和公平性。模型在不同地区之间的人流预测中具有更高的稳定性和可靠性，并且能够更好地捕捉人们在不同地区之间的流动性。<details>
<summary>Abstract</summary>
Generating realistic human flows across regions is essential for our understanding of urban structures and population activity patterns, enabling important applications in the fields of urban planning and management. However, a notable shortcoming of most existing mobility generation methodologies is neglect of prediction fairness, which can result in underestimation of mobility flows across regions with vulnerable population groups, potentially resulting in inequitable resource distribution and infrastructure development. To overcome this limitation, our study presents a novel, fairness-aware deep learning model, FairMobi-Net, for inter-region human flow prediction. The FairMobi-Net model uniquely incorporates fairness loss into the loss function and employs a hybrid approach, merging binary classification and numerical regression techniques for human flow prediction. We validate the FairMobi-Net model using comprehensive human mobility datasets from four U.S. cities, predicting human flow at the census-tract level. Our findings reveal that the FairMobi-Net model outperforms state-of-the-art models (such as the DeepGravity model) in producing more accurate and equitable human flow predictions across a variety of region pairs, regardless of regional income differences. The model maintains a high degree of accuracy consistently across diverse regions, addressing the previous fairness concern. Further analysis of feature importance elucidates the impact of physical distances and road network structures on human flows across regions. With fairness as its touchstone, the model and results provide researchers and practitioners across the fields of urban sciences, transportation engineering, and computing with an effective tool for accurate generation of human mobility flows across regions.
</details>
<details>
<summary>摘要</summary>
Generating realistic human flows across regions is essential for understanding urban structures and population activity patterns, enabling important applications in urban planning and management. However, most existing mobility generation methodologies neglect prediction fairness, which can result in underestimation of mobility flows across regions with vulnerable population groups, potentially leading to inequitable resource distribution and infrastructure development. To address this limitation, our study presents a novel, fairness-aware deep learning model, FairMobi-Net, for inter-region human flow prediction. The FairMobi-Net model incorporates fairness loss into the loss function and employs a hybrid approach, merging binary classification and numerical regression techniques for human flow prediction. We validate the FairMobi-Net model using comprehensive human mobility datasets from four U.S. cities, predicting human flow at the census-tract level. Our findings show that the FairMobi-Net model outperforms state-of-the-art models (such as the DeepGravity model) in producing more accurate and equitable human flow predictions across a variety of region pairs, regardless of regional income differences. The model maintains a high degree of accuracy consistently across diverse regions, addressing the previous fairness concern. Further analysis of feature importance reveals the impact of physical distances and road network structures on human flows across regions. With fairness as its touchstone, the model and results provide researchers and practitioners across the fields of urban sciences, transportation engineering, and computing with an effective tool for accurate generation of human mobility flows across regions.
</details></li>
</ul>
<hr>
<h2 id="The-Effect-of-Epidemiological-Cohort-Creation-on-the-Machine-Learning-Prediction-of-Homelessness-and-Police-Interaction-Outcomes-Using-Administrative-Health-Care-Data"><a href="#The-Effect-of-Epidemiological-Cohort-Creation-on-the-Machine-Learning-Prediction-of-Homelessness-and-Police-Interaction-Outcomes-Using-Administrative-Health-Care-Data" class="headerlink" title="The Effect of Epidemiological Cohort Creation on the Machine Learning Prediction of Homelessness and Police Interaction Outcomes Using Administrative Health Care Data"></a>The Effect of Epidemiological Cohort Creation on the Machine Learning Prediction of Homelessness and Police Interaction Outcomes Using Administrative Health Care Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11211">http://arxiv.org/abs/2307.11211</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Fuzzy-sh/Machine-Learning-Risk-Estimation-and-Prediction-of-Homelessness-and-Police-interaction">https://github.com/Fuzzy-sh/Machine-Learning-Risk-Estimation-and-Prediction-of-Homelessness-and-Police-interaction</a></li>
<li>paper_authors: Faezehsadat Shahidi, M. Ethan MacDonald, Dallas Seitz, Geoffrey Messier</li>
<li>For: The paper aims to identify factors associated with initial homelessness and police interaction among individuals with addiction or mental health (AMH) diagnoses, and to evaluate the performance of different predictive models using flexible and fixed observation windows.* Methods: The study uses an administrative healthcare dataset from Calgary, Alberta, Canada, comprising 240,219 individuals diagnosed with AMH between April 1, 2013, and March 31, 2018. The cohort is followed for 2 years to identify factors associated with homelessness and police interactions. The authors compare the performance of logistic regression (LR) and machine learning (ML) models, including random forests (RF) and extreme gradient boosting (XGBoost), in two cohorts with fixed and flexible observation windows.* Results: The study finds that male sex, substance disorder, psychiatrist visits, and drug abuse are associated with initial homelessness and police interaction. The authors also demonstrate that XGBoost shows superior performance using the flexible method, with sensitivity and AUC values of 91% and 90%, respectively, for initial homelessness, and 90% and 89%, respectively, for initial police interaction.<details>
<summary>Abstract</summary>
Background: Mental illness can lead to adverse outcomes such as homelessness and police interaction and understanding of the events leading up to these adverse outcomes is important. Predictive models may help identify individuals at risk of such adverse outcomes. Using a fixed observation window cohort with logistic regression (LR) or machine learning (ML) models can result in lower performance when compared with adaptive and parcellated windows.   Method: An administrative healthcare dataset was used, comprising of 240,219 individuals in Calgary, Alberta, Canada who were diagnosed with addiction or mental health (AMH) between April 1, 2013, and March 31, 2018. The cohort was followed for 2 years to identify factors associated with homelessness and police interactions. To understand the benefit of flexible windows to predictive models, an alternative cohort was created. Then LR and ML models, including random forests (RF), and extreme gradient boosting (XGBoost) were compared in the two cohorts.   Results: Among 237,602 individuals, 0.8% (1,800) experienced first homelessness, while 0.32% (759) reported initial police interaction among 237,141 individuals. Male sex (AORs: H=1.51, P=2.52), substance disorder (AORs: H=3.70, P=2.83), psychiatrist visits (AORs: H=1.44, P=1.49), and drug abuse (AORs: H=2.67, P=1.83) were associated with initial homelessness (H) and police interaction (P). XGBoost showed superior performance using the flexible method (sensitivity =91%, AUC =90% for initial homelessness, and sensitivity =90%, AUC=89% for initial police interaction)   Conclusion: This study identified key features associated with initial homelessness and police interaction and demonstrated that flexible windows can improve predictive modeling.
</details>
<details>
<summary>摘要</summary>
背景：心理疾病可能导致不良结果，如失Homelessness和与警察的交往，了解这些不良结果的发展过程是重要的。预测模型可能帮助 identificidadividuals at risk of such adverse outcomes。使用固定观察窗口 cohort with logistic regression (LR) or machine learning (ML) models可能会导致性能下降相比之下 flexible and parcellated windows。方法：使用了一个行政医疗数据集，包括2013年4月1日至2018年3月31日在加拿大阿尔伯塔省加尔法瑞市的240,219名患有情绪或心理健康（AMH）的个体。这个 cohort 被跟踪了2年，以便发现与失Homelessness和与警察的交往相关的因素。为了理解柔性窗口对预测模型的好处，一个alternative cohort 被创建。然后，LR和ML模型，包括随机森林（RF）和极限差分boosting（XGBoost）在两个 cohort 中进行了比较。结果：在237,602个个体中，0.8%（1,800）经历了首次失Homelessness，而0.32%（759）在237,141个个体中首次与警察交往。男性性别（AORs：H=1.51，P=2.52）、物质过度（AORs：H=3.70，P=2.83）、心理医生访问（AORs：H=1.44，P=1.49）和药物滥用（AORs：H=2.67，P=1.83）与初始失Homelessness（H）和初始与警察交往（P）相关。XGBoost 在柔性方法下表现出优于性能（敏感性 =91%， ROC =90%  для初始失Homelessness，以及敏感性 =90%， ROC =89%  для初始与警察交往）。结论：本研究确定了初始失Homelessness和初始与警察交往的关键特征，并证明了柔性窗口可以改善预测模型。
</details></li>
</ul>
<hr>
<h2 id="Clinical-Trial-Active-Learning"><a href="#Clinical-Trial-Active-Learning" class="headerlink" title="Clinical Trial Active Learning"></a>Clinical Trial Active Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11209">http://arxiv.org/abs/2307.11209</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/olivesgatech/clinical-trial-active-learning">https://github.com/olivesgatech/clinical-trial-active-learning</a></li>
<li>paper_authors: Zoe Fowler, Kiran Kokilepersaud, Mohit Prabhushankar, Ghassan AlRegib</li>
<li>for: 这篇论文提出了一种新的活动学习方法，能够考虑临床试验中数据的非独立同分布（非i.i.d）结构。</li>
<li>methods: 该方法使用了 prospectively active learning，在医学试验中采集数据时，condition on the time an image was collected，以维护i.i.d.假设。</li>
<li>results:  Comparing with传统的active learning方法，prospective active learning在两种不同的测试环境中表现出了更好的性能。<details>
<summary>Abstract</summary>
This paper presents a novel approach to active learning that takes into account the non-independent and identically distributed (non-i.i.d.) structure of a clinical trial setting. There exists two types of clinical trials: retrospective and prospective. Retrospective clinical trials analyze data after treatment has been performed; prospective clinical trials collect data as treatment is ongoing. Typically, active learning approaches assume the dataset is i.i.d. when selecting training samples; however, in the case of clinical trials, treatment results in a dependency between the data collected at the current and past visits. Thus, we propose prospective active learning to overcome the limitations present in traditional active learning methods and apply it to disease detection in optical coherence tomography (OCT) images, where we condition on the time an image was collected to enforce the i.i.d. assumption. We compare our proposed method to the traditional active learning paradigm, which we refer to as retrospective in nature. We demonstrate that prospective active learning outperforms retrospective active learning in two different types of test settings.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Heuristic-Hyperparameter-Choice-for-Image-Anomaly-Detection"><a href="#Heuristic-Hyperparameter-Choice-for-Image-Anomaly-Detection" class="headerlink" title="Heuristic Hyperparameter Choice for Image Anomaly Detection"></a>Heuristic Hyperparameter Choice for Image Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11197">http://arxiv.org/abs/2307.11197</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyu Jiang, João P. C. Bertoldo, Etienne Decencière</li>
<li>for: 这个论文主要针对图像异常检测（AD）中使用深度学习神经网络，以提高图像异常检测的精度和效率。</li>
<li>methods: 本文使用了预训练模型提取的深度特征，并使用NPCA维度减少算法进行维度减少。文中还提出了一些优化NPCA算法参数的启发方法，以优化维度减少后的性能。</li>
<li>results: 经过NPCA维度减少后，图像异常检测的性能得到了提高，而且可以避免大量的维度缩放。同时，NPCA algorithm的选择参数也对性能产生了一定的影响。<details>
<summary>Abstract</summary>
Anomaly detection (AD) in images is a fundamental computer vision problem by deep learning neural network to identify images deviating significantly from normality. The deep features extracted from pretrained models have been proved to be essential for AD based on multivariate Gaussian distribution analysis. However, since models are usually pretrained on a large dataset for classification tasks such as ImageNet, they might produce lots of redundant features for AD, which increases computational cost and degrades the performance. We aim to do the dimension reduction of Negated Principal Component Analysis (NPCA) for these features. So we proposed some heuristic to choose hyperparameter of NPCA algorithm for getting as fewer components of features as possible while ensuring a good performance.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Anomaly detection (AD) in images is a fundamental computer vision problem by deep learning neural network to identify images deviating significantly from normality. The deep features extracted from pretrained models have been proved to be essential for AD based on multivariate Gaussian distribution analysis. However, since models are usually pretrained on a large dataset for classification tasks such as ImageNet, they might produce lots of redundant features for AD, which increases computational cost and degrades the performance. We aim to do the dimension reduction of Negated Principal Component Analysis (NPCA) for these features. So we proposed some heuristic to choose hyperparameter of NPCA algorithm for getting as fewer components of features as possible while ensuring a good performance." into Simplified Chinese.翻译文本“图像异常检测（AD）是计算机视觉领域的基本问题，使用深度学习神经网络来标识图像异常的情况。深度特征从预训练模型中提取出来已经证明是AD的关键因素，基于多元 Gaussian 分布分析。但是，由于模型通常在大量数据集上进行分类任务，如 ImageNet，因此可能生成大量的冗余特征，从而增加计算成本并降低性能。我们想使用 Negated Principal Component Analysis（NPCA）维度减少这些特征。因此，我们提出了一些启发来选择 NPCA 算法中的 гиперпарамет。”into Simplified Chinese.Here's the translation:“图像异常检测（AD）是计算机视觉领域的基本问题，通过深度学习神经网络来标识图像异常的情况。深度特征从预训练模型中提取出来已经证明是AD的关键因素，基于多元 Gaussian 分布分析。但是，由于模型通常在大量数据集上进行分类任务，如 ImageNet，因此可能生成大量的冗余特征，从而增加计算成本并降低性能。我们想使用 Negated Principal Component Analysis（NPCA）维度减少这些特征。因此，我们提出了一些启发来选择 NPCA 算法中的 гиперпарамет。”
</details></li>
</ul>
<hr>
<h2 id="Exploring-reinforcement-learning-techniques-for-discrete-and-continuous-control-tasks-in-the-MuJoCo-environment"><a href="#Exploring-reinforcement-learning-techniques-for-discrete-and-continuous-control-tasks-in-the-MuJoCo-environment" class="headerlink" title="Exploring reinforcement learning techniques for discrete and continuous control tasks in the MuJoCo environment"></a>Exploring reinforcement learning techniques for discrete and continuous control tasks in the MuJoCo environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11166">http://arxiv.org/abs/2307.11166</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vaddadi Sai Rahul, Debajyoti Chakraborty</li>
<li>for: 本文目的是对 continuous control 环境下的值基方法进行比较，并使用 MuJoCo  физи学引擎进行实验，以揭示每个任务的观察空间、动作空间、奖励等细节。</li>
<li>methods: 本文使用了 Q-学习和 SARSA 两种值基方法作为基准，并使用 DDPG 作为一种 state-of-the-art 深度政策梯度方法进行比较。</li>
<li>results: Q-学习在大量集数据下表现较好，但 DDPG 在一些集数据下表现更好，且在几个集数据下可以快速达到适当的奖励平均值。此外，文章还评估了模型超参数的调整，并预计通过增加时间和资源可以进一步提高性能。<details>
<summary>Abstract</summary>
We leverage the fast physics simulator, MuJoCo to run tasks in a continuous control environment and reveal details like the observation space, action space, rewards, etc. for each task. We benchmark value-based methods for continuous control by comparing Q-learning and SARSA through a discretization approach, and using them as baselines, progressively moving into one of the state-of-the-art deep policy gradient method DDPG. Over a large number of episodes, Qlearning outscored SARSA, but DDPG outperformed both in a small number of episodes. Lastly, we also fine-tuned the model hyper-parameters expecting to squeeze more performance but using lesser time and resources. We anticipated that the new design for DDPG would vastly improve performance, yet after only a few episodes, we were able to achieve decent average rewards. We expect to improve the performance provided adequate time and computational resources.
</details>
<details>
<summary>摘要</summary>
我们利用快速物理模拟器MuJoCo来运行任务在连续控制环境中，并揭示任务的观察空间、动作空间、奖励等细节。我们对连续控制方法进行了价值基础比较，通过精度逐步逼近一个现有的深度策略方法DDPG。经过大量的集数，Q学习超越了SARSA，但DDPG在一些集数中超越了两者。最后，我们还进行了模型参数的微调，以适应更好的性能，但是使用更少的时间和资源。我们预计新的DDPG设计将大幅提高性能，但只需几集数就能达到了相当的平均奖励。我们预计通过充足的时间和计算资源，可以进一步提高性能。
</details></li>
</ul>
<hr>
<h2 id="Data-driven-criteria-for-quantum-correlations"><a href="#Data-driven-criteria-for-quantum-correlations" class="headerlink" title="Data-driven criteria for quantum correlations"></a>Data-driven criteria for quantum correlations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11091">http://arxiv.org/abs/2307.11091</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mateusz Krawczyk, Jarosław Pawłowski, Maciej M. Maśka, Katarzyna Roszak</li>
<li>for: 这个论文目的是检测三个量子比特系统中的相关性，使用基于无监督学习的神经网络。</li>
<li>methods: 这个论文使用了一种叫做”强制认知”的方法，强制神经网络认识分解性状态。在这种方法下，相关性被检测为异常。</li>
<li>results: 研究发现，该检测器在检测量子不紧密相关性（quantum discord）时表现很好，而在检测量子同步相关性（entanglement）时表现不尽如人们所料。实际上，它会大大过分检测同步相关性，而对分解相关性的检测则很准确。<details>
<summary>Abstract</summary>
We build a machine learning model to detect correlations in a three-qubit system using a neural network trained in an unsupervised manner on randomly generated states. The network is forced to recognize separable states, and correlated states are detected as anomalies. Quite surprisingly, we find that the proposed detector performs much better at distinguishing a weaker form of quantum correlations, namely, the quantum discord, than entanglement. In fact, it has a tendency to grossly overestimate the set of entangled states even at the optimal threshold for entanglement detection, while it underestimates the set of discordant states to a much lesser extent. In order to illustrate the nature of states classified as quantum-correlated, we construct a diagram containing various types of states -- entangled, as well as separable, both discordant and non-discordant. We find that the near-zero value of the recognition loss reproduces the shape of the non-discordant separable states with high accuracy, especially considering the non-trivial shape of this set on the diagram. The network architecture is designed carefully: it preserves separability, and its output is equivariant with respect to qubit permutations. We show that the choice of architecture is important to get the highest detection accuracy, much better than for a baseline model that just utilizes a partial trace operation.
</details>
<details>
<summary>摘要</summary>
我们建立了一个机器学习模型，用于检测三量子系统中的相关性，使用一个无监督的神经网络在随机生成的态上进行训练。网络强制地认可分解态，相关态被检测为异常。 surprisingly,我们发现，我们提议的检测器在分解态和非分解态之间的分界点上表现比较好，而不是在共聚态和非共聚态之间的分界点上。实际上，它在优化的阈值下对共聚态进行检测时会大大过分，而对非分解态的检测则比较准确。为了 illustrate the nature of 被分类为量子相关的态，我们构建了一个包含多种态的图表，包括共聚态、分解态、不同的分orde和非分orde态。我们发现， near-zero值的认可损失很准确地复制了非分orde分解态的形状，特别是考虑到这些态的非rivial形状。我们注意到，网络的建立是非常重要，以获得最高的检测精度。我们采用了一种细心设计的网络结构，它保留了分解性，并且输出是对 qubit  permutation 的对称变换的。我们显示，这种选择的网络结构可以达到最高的检测精度，远胜于基eline 模型，只使用 partial trace 操作。
</details></li>
</ul>
<hr>
<h2 id="PAPR-Proximity-Attention-Point-Rendering"><a href="#PAPR-Proximity-Attention-Point-Rendering" class="headerlink" title="PAPR: Proximity Attention Point Rendering"></a>PAPR: Proximity Attention Point Rendering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11086">http://arxiv.org/abs/2307.11086</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanshu Zhang, Shichong Peng, Alireza Moazeni, Ke Li</li>
<li>for: 学习 scene 表面的准确和简洁点云表示方法。</li>
<li>methods: 我们提出了 Proximity Attention Point Rendering (PAPR) 方法，它包括一个点云表示和可微分渲染器。我们的点云表示使用每个点的空间位置、前景得分和视角独立特征向量来Characterize each point。渲染器选择每个辐线上 relevante 点并使用它们相关的特征生成准确的颜色。</li>
<li>results: PAPR 方法可以准确地学习点云位置，以表示正确的 scene  geometry，即使初始化与目标geometry差异很大。此外，我们的方法还能够捕捉细 texture 细节，只用parsimonious 的点云数据。我们还实现了 geometry 编辑、物体操作、Texture 传输和曝光控制等四种实用应用。更多结果和代码可以在我们项目网站上找到：<a target="_blank" rel="noopener" href="https://zvict.github.io/papr/">https://zvict.github.io/papr/</a>.<details>
<summary>Abstract</summary>
Learning accurate and parsimonious point cloud representations of scene surfaces from scratch remains a challenge in 3D representation learning. Existing point-based methods often suffer from the vanishing gradient problem or require a large number of points to accurately model scene geometry and texture. To address these limitations, we propose Proximity Attention Point Rendering (PAPR), a novel method that consists of a point-based scene representation and a differentiable renderer. Our scene representation uses a point cloud where each point is characterized by its spatial position, foreground score, and view-independent feature vector. The renderer selects the relevant points for each ray and produces accurate colours using their associated features. PAPR effectively learns point cloud positions to represent the correct scene geometry, even when the initialization drastically differs from the target geometry. Notably, our method captures fine texture details while using only a parsimonious set of points. We also demonstrate four practical applications of our method: geometry editing, object manipulation, texture transfer, and exposure control. More results and code are available on our project website at https://zvict.github.io/papr/.
</details>
<details>
<summary>摘要</summary>
学习精确且简洁的点云表示Scene表面从头条仍然是3D表示学习中的挑战。现有的点云方法经常会遇到消失梯度问题或需要很多点云来准确地模型Scene的geometry和文本ure。为了解决这些限制，我们提出了Proximity Attention Point Rendering（PAPR），一种新的方法，其包括点云表示和可导 Renderer。我们的Scene表示使用一个点云，每个点被定义为其空间位置、前景分数和视 independente特征向量。Renderer选择每个辐射中的相关点，并使用它们关联的特征来生成高精度颜色。PAPR有效地学习点云位置来表示正确的Scene geometry，即使初始化与目标geometry有很大差异。此外，我们的方法能够捕捉细节Texture detail，使用只需要一小部分的点云。我们还展示了PAPR的四个实用应用：geometry编辑、物体操作、Texture传输和曝光控制。更多结果和代码可以在我们项目网站https://zvict.github.io/papr/中找到。
</details></li>
</ul>
<hr>
<h2 id="Representation-Learning-in-Anomaly-Detection-Successes-Limits-and-a-Grand-Challenge"><a href="#Representation-Learning-in-Anomaly-Detection-Successes-Limits-and-a-Grand-Challenge" class="headerlink" title="Representation Learning in Anomaly Detection: Successes, Limits and a Grand Challenge"></a>Representation Learning in Anomaly Detection: Successes, Limits and a Grand Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11085">http://arxiv.org/abs/2307.11085</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yedid Hoshen</li>
<li>for: 这篇论文主要写于异常检测领域， argue that 异常检测领域的主流思想无法无限扩展，将 eventually 遇到基础限制。</li>
<li>methods: 这篇论文使用了 “no free lunch” 原理来解释异常检测领域的限制。当有强大任务优先的情况下，这些限制可以被突破。</li>
<li>results: 文章提出了两个异常检测挑战 зада题：一是科学发现通过异常检测，二是ImageNet dataset中最异常的图像检测挑战。文章认为，新的异常检测工具和想法需要被开发，以解决这些挑战。<details>
<summary>Abstract</summary>
In this perspective paper, we argue that the dominant paradigm in anomaly detection cannot scale indefinitely and will eventually hit fundamental limits. This is due to the a no free lunch principle for anomaly detection. These limitations can be overcome when there are strong tasks priors, as is the case for many industrial tasks. When such priors do not exists, the task is much harder for anomaly detection. We pose two such tasks as grand challenges for anomaly detection: i) scientific discovery by anomaly detection ii) a "mini-grand" challenge of detecting the most anomalous image in the ImageNet dataset. We believe new anomaly detection tools and ideas would need to be developed to overcome these challenges.
</details>
<details>
<summary>摘要</summary>
在这篇观点论文中，我们 argue That the dominant paradigm in anomaly detection cannot be scaled indefinitely and will eventually reach fundamental limits. This is due to the a no free lunch principle for anomaly detection. These limitations can be overcome when there are strong task priors, as is the case for many industrial tasks. When such priors do not exist, the task is much harder for anomaly detection. We pose two such tasks as grand challenges for anomaly detection: i) scientific discovery by anomaly detection ii) a "mini-grand" challenge of detecting the most anomalous image in the ImageNet dataset. We believe new anomaly detection tools and ideas would need to be developed to overcome these challenges.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other parts of the world. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="GLSFormer-Gated-Long-Short-Sequence-Transformer-for-Step-Recognition-in-Surgical-Videos"><a href="#GLSFormer-Gated-Long-Short-Sequence-Transformer-for-Step-Recognition-in-Surgical-Videos" class="headerlink" title="GLSFormer: Gated - Long, Short Sequence Transformer for Step Recognition in Surgical Videos"></a>GLSFormer: Gated - Long, Short Sequence Transformer for Step Recognition in Surgical Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11081">http://arxiv.org/abs/2307.11081</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nisargshah1999/glsformer">https://github.com/nisargshah1999/glsformer</a></li>
<li>paper_authors: Nisarg A. Shah, Shameema Sikder, S. Swaroop Vedula, Vishal M. Patel</li>
<li>for: automated surgical step recognition, 自动化手术步骤识别</li>
<li>methods: 使用视transformer学习 directly from sequence of frame-level patches,  incorporates a gated-temporal attention mechanism, 并在两个猫眼手术视频数据集上进行了广泛评估</li>
<li>results: 比various state-of-the-art methods superior performance,  validate the suitability of our proposed approach for automated surgical step recognition, Results: 在两个猫眼手术视频数据集上进行了广泛评估，并证明了我们提议的方法适用于自动化手术步骤识别。Here’s the full text in Simplified Chinese:</li>
<li>for: 本研究旨在提出一种基于视transformer的自动化手术步骤识别方法，以提高患者安全性和决策过程中的手术步骤识别精度。</li>
<li>methods: 我们的方法使用视transformer学习 directly from sequence of frame-level patches，并在这些patches中嵌入精度的spatio-temporal feature。我们还提出了一种闭合时间注意力机制，可以智能地结合短期和长期的spatio-temporal特征表示。</li>
<li>results: 我们在两个猫眼手术视频数据集上进行了广泛评估，并证明了我们提议的方法比various state-of-the-art methods superior performance。这些结果证明了我们的方法适用于自动化手术步骤识别。I hope that helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
Automated surgical step recognition is an important task that can significantly improve patient safety and decision-making during surgeries. Existing state-of-the-art methods for surgical step recognition either rely on separate, multi-stage modeling of spatial and temporal information or operate on short-range temporal resolution when learned jointly. However, the benefits of joint modeling of spatio-temporal features and long-range information are not taken in account. In this paper, we propose a vision transformer-based approach to jointly learn spatio-temporal features directly from sequence of frame-level patches. Our method incorporates a gated-temporal attention mechanism that intelligently combines short-term and long-term spatio-temporal feature representations. We extensively evaluate our approach on two cataract surgery video datasets, namely Cataract-101 and D99, and demonstrate superior performance compared to various state-of-the-art methods. These results validate the suitability of our proposed approach for automated surgical step recognition. Our code is released at: https://github.com/nisargshah1999/GLSFormer
</details>
<details>
<summary>摘要</summary>
自动化手术步骤识别是一项非常重要的任务，可以大大提高手术过程中病人安全性和决策。现有的现状前沿方法都是 Either rely on separate, multi-stage modeling of spatial and temporal information or operate on short-range temporal resolution when learned jointly。然而， jointly learning spatio-temporal features and long-range information 的好处没有得到了考虑。在这篇论文中，我们提出了基于视觉转换器的方法，直接从框架级别的补充帧序列中学习spatio-temporal特征。我们的方法包括一个闭合时间注意力机制，智能地结合短期和长期的spatio-temporal特征表示。我们对两个眼部手术视频数据集，即Cataract-101和D99进行了广泛的评估，并demonstrate了与多种现状前沿方法相比的更好的性能。这些结果证明了我们提出的方法的适用性。我们的代码可以在：https://github.com/nisargshah1999/GLSFormer 中找到。
</details></li>
</ul>
<hr>
<h2 id="Brain2Music-Reconstructing-Music-from-Human-Brain-Activity"><a href="#Brain2Music-Reconstructing-Music-from-Human-Brain-Activity" class="headerlink" title="Brain2Music: Reconstructing Music from Human Brain Activity"></a>Brain2Music: Reconstructing Music from Human Brain Activity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11078">http://arxiv.org/abs/2307.11078</a></li>
<li>repo_url: None</li>
<li>paper_authors: Timo I. Denk, Yu Takagi, Takuya Matsuyama, Andrea Agostinelli, Tomoya Nakai, Christian Frank, Shinji Nishimoto</li>
<li>for: 这项研究旨在通过解剖活动中的脑活动来重构音乐经验，以便更好地理解脑如何解释和表达世界。</li>
<li>methods: 这篇论文提出了一种使用功能磁共振成像(fMRI)技术capture脑活动，并使用音乐检索或MusicLM音乐生成模型来重构音乐。</li>
<li>results: 研究发现，通过使用这种方法可以重构出与人类Subjects经验的音乐相似的音乐，包括元素如乐器、氛围和情感等Semantic属性。此外，研究还发现了脑活动中不同组织部分对音乐描述文本的表达方式的响应。更多详细的例子可以在<a target="_blank" rel="noopener" href="https://google-research.github.io/seanet/brain2music%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://google-research.github.io/seanet/brain2music中找到。</a><details>
<summary>Abstract</summary>
The process of reconstructing experiences from human brain activity offers a unique lens into how the brain interprets and represents the world. In this paper, we introduce a method for reconstructing music from brain activity, captured using functional magnetic resonance imaging (fMRI). Our approach uses either music retrieval or the MusicLM music generation model conditioned on embeddings derived from fMRI data. The generated music resembles the musical stimuli that human subjects experienced, with respect to semantic properties like genre, instrumentation, and mood. We investigate the relationship between different components of MusicLM and brain activity through a voxel-wise encoding modeling analysis. Furthermore, we discuss which brain regions represent information derived from purely textual descriptions of music stimuli. We provide supplementary material including examples of the reconstructed music at https://google-research.github.io/seanet/brain2music
</details>
<details>
<summary>摘要</summary>
人脑活动重建经验的过程提供了一种独特的视角，描绘如何Brain interprets和表示世界。在这篇论文中，我们介绍了一种使用functional magnetic resonance imaging（fMRI）捕捉的Brain Activity来重建音乐的方法。我们的方法使用 either music retrieval or MusicLM music generation model conditioned on embeddings derived from fMRI data。生成的音乐与人类试验者所经历的音乐具有相同的Semantic properties，如种类、编制和情感。我们通过 voxel-wise encoding modeling analysis来调查MusicLM中不同组件和脑动activities之间的关系。此外，我们还讨论了脑动活动中表示来自文本描述的音乐刺激的Brain regions。我们提供了补充材料，包括重建音乐的示例，可以在https://google-research.github.io/seanet/brain2music中找到。
</details></li>
</ul>
<hr>
<h2 id="AlignDet-Aligning-Pre-training-and-Fine-tuning-in-Object-Detection"><a href="#AlignDet-Aligning-Pre-training-and-Fine-tuning-in-Object-Detection" class="headerlink" title="AlignDet: Aligning Pre-training and Fine-tuning in Object Detection"></a>AlignDet: Aligning Pre-training and Fine-tuning in Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11077">http://arxiv.org/abs/2307.11077</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liming-ai/AlignDet">https://github.com/liming-ai/AlignDet</a></li>
<li>paper_authors: Ming Li, Jie Wu, Xionghui Wang, Chen Chen, Jie Qin, Xuefeng Xiao, Rui Wang, Min Zheng, Xin Pan</li>
<li>for: 提高物体检测算法的性能、通用性和速度</li>
<li>methods: 提出了一种名为AlignDet的统一预训练框架，可以适应不同的检测器，以解决预训练和细化训练过程中存在的数据、模型和任务之间的不一致问题</li>
<li>results: 经过广泛的实验表明，AlignDet可以在多种协议下 дости得显著的提高，例如检测算法、模型脊梁、数据设定和训练计划等，如图1所示，对于FCOS、RetinaNet、Faster R-CNN和DETR等检测器，AlignDet可以提高5.3 mAP、2.1 mAP、3.3 mAP和2.3 mAP等指标。<details>
<summary>Abstract</summary>
The paradigm of large-scale pre-training followed by downstream fine-tuning has been widely employed in various object detection algorithms. In this paper, we reveal discrepancies in data, model, and task between the pre-training and fine-tuning procedure in existing practices, which implicitly limit the detector's performance, generalization ability, and convergence speed. To this end, we propose AlignDet, a unified pre-training framework that can be adapted to various existing detectors to alleviate the discrepancies. AlignDet decouples the pre-training process into two stages, i.e., image-domain and box-domain pre-training. The image-domain pre-training optimizes the detection backbone to capture holistic visual abstraction, and box-domain pre-training learns instance-level semantics and task-aware concepts to initialize the parts out of the backbone. By incorporating the self-supervised pre-trained backbones, we can pre-train all modules for various detectors in an unsupervised paradigm. As depicted in Figure 1, extensive experiments demonstrate that AlignDet can achieve significant improvements across diverse protocols, such as detection algorithm, model backbone, data setting, and training schedule. For example, AlignDet improves FCOS by 5.3 mAP, RetinaNet by 2.1 mAP, Faster R-CNN by 3.3 mAP, and DETR by 2.3 mAP under fewer epochs.
</details>
<details>
<summary>摘要</summary>
大量预训练后下游精度定义的框架在各种对象检测算法中广泛应用。在这篇论文中，我们发现了预训练和精度定义过程中的数据、模型和任务之间的不一致，这些不一致限制了检测器的性能、泛化能力和收敛速度。为解决这些问题，我们提出了AlignDet，一个可适应不同检测器的统一预训练框架。AlignDet将预训练过程分成两个阶段：图像领域预训练和框预训练。图像领域预训练使检测后处理网络学习整体视觉抽象，而框预训练学习实例级别的语义和任务相关概念，以初始化检测器的部分。通过将自然语言预训练后的背景 integrate 到检测器中，我们可以在无监督模式下预训练所有模块。根据 Figure 1 的实验结果，AlignDet可以在不同的协议、模型背景、数据设置和训练计划下实现显著的改善。例如，AlignDet可以提高 FCOS 的 mAP 指标by 5.3，RetinaNet 的 mAP 指标by 2.1，Faster R-CNN 的 mAP 指标by 3.3，和 DETR 的 mAP 指标by 2.3，并且在更少的训练 epoch 下达到这些改善。
</details></li>
</ul>
<hr>
<h2 id="Effectiveness-and-predictability-of-in-network-storage-cache-for-scientific-workflows"><a href="#Effectiveness-and-predictability-of-in-network-storage-cache-for-scientific-workflows" class="headerlink" title="Effectiveness and predictability of in-network storage cache for scientific workflows"></a>Effectiveness and predictability of in-network storage cache for scientific workflows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11069">http://arxiv.org/abs/2307.11069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Caitlin Sim, Kesheng Wu, Alex Sim, Inder Monga, Chin Guok, Frank Wurthwein, Diego Davila, Harvey Newman, Justas Balcas</li>
<li>for: 这个论文是为了研究大型科学合作中多名科学家访问同一个数据集而减少宽带网络流量和数据访问延迟而写的。</li>
<li>methods: 该论文使用了地域数据存储缓存系统来减少宽带网络流量和数据访问延迟。它们还使用了Machine Learning模型来预测缓存行为。</li>
<li>results: 该论文通过分析约3TB的运维日志，发现地域缓存系统可以将67.6%的文件请求从宽带网络中除除，并将宽带网络流量减少35.4%（或12.3TB）的平均值。但由于数据访问模式的不同，缓存系统采用了不要将小文件被赋值的策略。此外，该论文还建立了一个可以准确预测缓存行为的Machine Learning模型，这使得该模型有用于未来资源配置和规划研究。<details>
<summary>Abstract</summary>
Large scientific collaborations often have multiple scientists accessing the same set of files while doing different analyses, which create repeated accesses to the large amounts of shared data located far away. These data accesses have long latency due to distance and occupy the limited bandwidth available over the wide-area network. To reduce the wide-area network traffic and the data access latency, regional data storage caches have been installed as a new networking service. To study the effectiveness of such a cache system in scientific applications, we examine the Southern California Petabyte Scale Cache for a high-energy physics experiment. By examining about 3TB of operational logs, we show that this cache removed 67.6% of file requests from the wide-area network and reduced the traffic volume on wide-area network by 12.3TB (or 35.4%) an average day. The reduction in the traffic volume (35.4%) is less than the reduction in file counts (67.6%) because the larger files are less likely to be reused. Due to this difference in data access patterns, the cache system has implemented a policy to avoid evicting smaller files when processing larger files. We also build a machine learning model to study the predictability of the cache behavior. Tests show that this model is able to accurately predict the cache accesses, cache misses, and network throughput, making the model useful for future studies on resource provisioning and planning.
</details>
<details>
<summary>摘要</summary>
大型科学合作项目经常有多个科学家访问同一组文件进行不同的分析，这会导致访问大量分布在远程位置的数据的重复访问。这些数据访问具有较长延迟时间和吞吐量限制，从宽带网络中传输数据。为了减少宽带网络流量和数据访问延迟时间，地区数据存储缓存已经被设为新的网络服务。为了研究这种缓存系统在科学应用中的效iveness，我们研究了南加利福尼亚州 petabyte 级缓存系统，用于一个高能物理实验。通过分析约 3TB 的操作日志，我们发现这个缓存系统可以从宽带网络中除掉 67.6% 的文件请求，并将每天宽带网络流量减少 12.3TB（或 35.4%）。由于不同的数据访问模式，缓存系统实施了不会将小文件逐出缓存系统的策略。我们还建立了一个机器学习模型，用于研究缓存行为的预测性。测试表明，这个模型能准确预测缓存访问、缓存错误和网络吞吐量，使得这个模型有用于未来的资源配置和规划研究。
</details></li>
</ul>
<hr>
<h2 id="A-LLM-Assisted-Exploitation-of-AI-Guardian"><a href="#A-LLM-Assisted-Exploitation-of-AI-Guardian" class="headerlink" title="A LLM Assisted Exploitation of AI-Guardian"></a>A LLM Assisted Exploitation of AI-Guardian</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15008">http://arxiv.org/abs/2307.15008</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicholas Carlini</li>
<li>for: 本研究是用GPT-4语言模型评估AI-Guardian防御机器学习攻击的能力。</li>
<li>methods: 本研究使用GPT-4语言模型实现攻击AI-Guardian防御机器学习模型的攻击算法，不需要编写任何代码。</li>
<li>results: 研究发现，GPT-4语言模型可以快速和高效地生成攻击AI-Guardian防御机器学习模型的代码，并且在某些情况下可以更快速地生成代码 than 本研究的作者。<details>
<summary>Abstract</summary>
Large language models (LLMs) are now highly capable at a diverse range of tasks. This paper studies whether or not GPT-4, one such LLM, is capable of assisting researchers in the field of adversarial machine learning. As a case study, we evaluate the robustness of AI-Guardian, a recent defense to adversarial examples published at IEEE S&P 2023, a top computer security conference. We completely break this defense: the proposed scheme does not increase robustness compared to an undefended baseline.   We write none of the code to attack this model, and instead prompt GPT-4 to implement all attack algorithms following our instructions and guidance. This process was surprisingly effective and efficient, with the language model at times producing code from ambiguous instructions faster than the author of this paper could have done. We conclude by discussing (1) the warning signs present in the evaluation that suggested to us AI-Guardian would be broken, and (2) our experience with designing attacks and performing novel research using the most recent advances in language modeling.
</details>
<details>
<summary>摘要</summary>
我们没有编写攻击代码，而是使用 GPT-4 实现所有攻击算法，根据我们的指导和指令。这个过程 surprisingly 高效，GPT-4 可以很快地从抽象的指令中生成代码，甚至比作者本身更快。我们 conclude 了 (1) 在评估中存在的警示符，表明 AI-Guardian 将被破坏，以及 (2) 我们使用最新的语言模型技术来设计攻击和执行原创研究的经验。
</details></li>
</ul>
<hr>
<h2 id="Breadcrumbs-to-the-Goal-Goal-Conditioned-Exploration-from-Human-in-the-Loop-Feedback"><a href="#Breadcrumbs-to-the-Goal-Goal-Conditioned-Exploration-from-Human-in-the-Loop-Feedback" class="headerlink" title="Breadcrumbs to the Goal: Goal-Conditioned Exploration from Human-in-the-Loop Feedback"></a>Breadcrumbs to the Goal: Goal-Conditioned Exploration from Human-in-the-Loop Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11049">http://arxiv.org/abs/2307.11049</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/improbable-ai/human-guided-exploration">https://github.com/improbable-ai/human-guided-exploration</a></li>
<li>paper_authors: Marcel Torne, Max Balsells, Zihan Wang, Samedh Desai, Tao Chen, Pulkit Agrawal, Abhishek Gupta</li>
<li>for: 解决强化学习中的探索和奖励问题，需要精心设计奖励函数或使用新奇探索奖励。</li>
<li>methods: 使用低质量的人类反馈，不需要同步高质量的人类反馈，可以在实际世界中进行探索和学习。</li>
<li>results: 在模拟和实际世界中，使用人类反馈 guideline 探索，可以学习多种复杂的机器人探索和搅拌任务，而无需手动设计奖励函数或探索奖励。<details>
<summary>Abstract</summary>
Exploration and reward specification are fundamental and intertwined challenges for reinforcement learning. Solving sequential decision-making tasks requiring expansive exploration requires either careful design of reward functions or the use of novelty-seeking exploration bonuses. Human supervisors can provide effective guidance in the loop to direct the exploration process, but prior methods to leverage this guidance require constant synchronous high-quality human feedback, which is expensive and impractical to obtain. In this work, we present a technique called Human Guided Exploration (HuGE), which uses low-quality feedback from non-expert users that may be sporadic, asynchronous, and noisy. HuGE guides exploration for reinforcement learning not only in simulation but also in the real world, all without meticulous reward specification. The key concept involves bifurcating human feedback and policy learning: human feedback steers exploration, while self-supervised learning from the exploration data yields unbiased policies. This procedure can leverage noisy, asynchronous human feedback to learn policies with no hand-crafted reward design or exploration bonuses. HuGE is able to learn a variety of challenging multi-stage robotic navigation and manipulation tasks in simulation using crowdsourced feedback from non-expert users. Moreover, this paradigm can be scaled to learning directly on real-world robots, using occasional, asynchronous feedback from human supervisors.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Definition-of-Continual-Reinforcement-Learning"><a href="#A-Definition-of-Continual-Reinforcement-Learning" class="headerlink" title="A Definition of Continual Reinforcement Learning"></a>A Definition of Continual Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11046">http://arxiv.org/abs/2307.11046</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Abel, André Barreto, Benjamin Van Roy, Doina Precup, Hado van Hasselt, Satinder Singh</li>
<li>for: 本研究旨在开发一个基础 для continual reinforcement learning。</li>
<li>methods: 本 paper 使用的方法包括 &lt;insert methods used in the paper, e.g. experience replay, memory-based methods, etc.&gt;。</li>
<li>results: 本研究获得了 &lt;insert results of the paper, e.g. improved performance on benchmark tasks, etc.&gt;。<details>
<summary>Abstract</summary>
In this paper we develop a foundation for continual reinforcement learning.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们开发了一基础 для连续回归学习。Here's the breakdown of the translation:* 在这篇论文中 (in this paper) - 在这里 (here)* 我们开发了 (we develop) - 我们开发 (we develop)* 一基础 (a foundation) - 一基 (a base)*  для连续回归学习 (for continual reinforcement learning) - 连续回归学习 (continual reinforcement learning)
</details></li>
</ul>
<hr>
<h2 id="On-the-Convergence-of-Bounded-Agents"><a href="#On-the-Convergence-of-Bounded-Agents" class="headerlink" title="On the Convergence of Bounded Agents"></a>On the Convergence of Bounded Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11044">http://arxiv.org/abs/2307.11044</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Abel, André Barreto, Hado van Hasselt, Benjamin Van Roy, Doina Precup, Satinder Singh</li>
<li>for: 本文探讨了Agent convergency的定义和特点，尤其是在 bounded agents 上。</li>
<li>methods: 作者提出了两种 complementary 的Agent convergency定义，一种是指Agent的行为未来状态数量不能减少，另一种是指Agent的性能只会改变当Agent的内部状态改变。</li>
<li>results: 作者证明了这两种定义的基本性质和特点，以及它们在标准设置下的应用。<details>
<summary>Abstract</summary>
When has an agent converged? Standard models of the reinforcement learning problem give rise to a straightforward definition of convergence: An agent converges when its behavior or performance in each environment state stops changing. However, as we shift the focus of our learning problem from the environment's state to the agent's state, the concept of an agent's convergence becomes significantly less clear. In this paper, we propose two complementary accounts of agent convergence in a framing of the reinforcement learning problem that centers around bounded agents. The first view says that a bounded agent has converged when the minimal number of states needed to describe the agent's future behavior cannot decrease. The second view says that a bounded agent has converged just when the agent's performance only changes if the agent's internal state changes. We establish basic properties of these two definitions, show that they accommodate typical views of convergence in standard settings, and prove several facts about their nature and relationship. We take these perspectives, definitions, and analysis to bring clarity to a central idea of the field.
</details>
<details>
<summary>摘要</summary>
当一个代理人已经 converges 时，标准的模型会给出一个直观的定义：一个代理人 converges 当其在每个环境状态下的行为或性能停止变化。但是，如果我们从环境状态向代理人的状态转移，则代理人的 converges 的概念变得非常不清楚。在这篇论文中，我们提出了两种 complementary 的代理人 converges 观点，即：第一种观点是，一个受限的代理人已经 converges 当其最小需要的状态数量不能减少。第二种观点是，一个受限的代理人已经 converges 当代理人的性能只会变化当代理人的内部状态发生变化。我们证明了这两种定义的基本性质，并证明它们能够涵盖标准设定下的常见 converges 观点。此外，我们还证明了这两种定义之间的关系。我们通过这些观点、定义和分析来带来这个领域中的一个重要概念的清晰性。
</details></li>
</ul>
<hr>
<h2 id="Embroid-Unsupervised-Prediction-Smoothing-Can-Improve-Few-Shot-Classification"><a href="#Embroid-Unsupervised-Prediction-Smoothing-Can-Improve-Few-Shot-Classification" class="headerlink" title="Embroid: Unsupervised Prediction Smoothing Can Improve Few-Shot Classification"></a>Embroid: Unsupervised Prediction Smoothing Can Improve Few-Shot Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11031">http://arxiv.org/abs/2307.11031</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HazyResearch/embroid">https://github.com/HazyResearch/embroid</a></li>
<li>paper_authors: Neel Guha, Mayee F. Chen, Kush Bhatia, Azalia Mirhoseini, Frederic Sala, Christopher Ré</li>
<li>for: automating data labeling in domains where manual annotation is expensive</li>
<li>methods: using a method called Embroid to modify the predictions of a prompt, rather than the prompt itself, to improve prompt-based learning without additional labeled data</li>
<li>results: Embroid substantially improves performance over original prompts, realizes improvements for more sophisticated prompting strategies, and can be specialized to domains like law through the embedding functions.Here’s the full Chinese text:</li>
<li>for:  automatizing 数据标注在人工标注成本高的领域</li>
<li>methods: 使用 Embroid 方法，修改提示而不是提示本身，以提高提示基本学习的效果，无需更多的标注数据</li>
<li>results: Embroid 可以大幅提高提示的性能，实现更复杂的提示策略的提高，并可以根据嵌入函数进行特化，用于领域如法律。<details>
<summary>Abstract</summary>
Recent work has shown that language models' (LMs) prompt-based learning capabilities make them well suited for automating data labeling in domains where manual annotation is expensive. The challenge is that while writing an initial prompt is cheap, improving a prompt is costly -- practitioners often require significant labeled data in order to evaluate the impact of prompt modifications. Our work asks whether it is possible to improve prompt-based learning without additional labeled data. We approach this problem by attempting to modify the predictions of a prompt, rather than the prompt itself. Our intuition is that accurate predictions should also be consistent: samples which are similar under some feature representation should receive the same prompt prediction. We propose Embroid, a method which computes multiple representations of a dataset under different embedding functions, and uses the consistency between the LM predictions for neighboring samples to identify mispredictions. Embroid then uses these neighborhoods to create additional predictions for each sample, and combines these predictions with a simple latent variable graphical model in order to generate a final corrected prediction. In addition to providing a theoretical analysis of Embroid, we conduct a rigorous empirical evaluation across six different LMs and up to 95 different tasks. We find that (1) Embroid substantially improves performance over original prompts (e.g., by an average of 7.3 points on GPT-JT), (2) also realizes improvements for more sophisticated prompting strategies (e.g., chain-of-thought), and (3) can be specialized to domains like law through the embedding functions.
</details>
<details>
<summary>摘要</summary>
Embroid works by computing multiple representations of a dataset under different embedding functions and using the consistency between the LM predictions for neighboring samples to identify mispredictions. It then uses these neighborhoods to create additional predictions for each sample and combines these predictions with a simple latent variable graphical model to generate a final corrected prediction.We provide a theoretical analysis of Embroid and conduct a rigorous empirical evaluation across six different LMs and up to 95 different tasks. Our results show that Embroid substantially improves performance over original prompts (e.g., by an average of 7.3 points on GPT-JT) and also realizes improvements for more sophisticated prompting strategies (e.g., chain-of-thought). Additionally, we find that Embroid can be specialized to domains like law through the embedding functions.
</details></li>
</ul>
<hr>
<h2 id="Cluster-aware-Semi-supervised-Learning-Relational-Knowledge-Distillation-Provably-Learns-Clustering"><a href="#Cluster-aware-Semi-supervised-Learning-Relational-Knowledge-Distillation-Provably-Learns-Clustering" class="headerlink" title="Cluster-aware Semi-supervised Learning: Relational Knowledge Distillation Provably Learns Clustering"></a>Cluster-aware Semi-supervised Learning: Relational Knowledge Distillation Provably Learns Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11030">http://arxiv.org/abs/2307.11030</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yijun Dong, Kevin Miller, Qi Lei, Rachel Ward</li>
<li>for: 本文旨在提供关于 semi-supervised classification 问题的 theoretically 解释 relational knowledge distillation（RKD）的一个初步。</li>
<li>methods: 本文使用 spectral clustering 来解释 RKD，并提供了一个基于 teacher model 生成的 population-induced graph，以及一个量化预测和实际 clustering 之间的差异的 clustering error。</li>
<li>results: 本文证明了 RKD 能够适用于 semi-supervised classification 问题，并提供了一个样本复杂度 bound。此外，文章还展示了 RKD 可以增强 label efficiency，并通过一种涉及到 low clustering error 的框架来证明这一点。最后，文章还将 data augmentation consistency regularization 与这种框架结合，并证明 RKD 可以帮助模型具有 “global” 视角，而不是 consistency regularization 的 “local” 视角。<details>
<summary>Abstract</summary>
Despite the empirical success and practical significance of (relational) knowledge distillation that matches (the relations of) features between teacher and student models, the corresponding theoretical interpretations remain limited for various knowledge distillation paradigms. In this work, we take an initial step toward a theoretical understanding of relational knowledge distillation (RKD), with a focus on semi-supervised classification problems. We start by casting RKD as spectral clustering on a population-induced graph unveiled by a teacher model. Via a notion of clustering error that quantifies the discrepancy between the predicted and ground truth clusterings, we illustrate that RKD over the population provably leads to low clustering error. Moreover, we provide a sample complexity bound for RKD with limited unlabeled samples. For semi-supervised learning, we further demonstrate the label efficiency of RKD through a general framework of cluster-aware semi-supervised learning that assumes low clustering errors. Finally, by unifying data augmentation consistency regularization into this cluster-aware framework, we show that despite the common effect of learning accurate clusterings, RKD facilitates a "global" perspective through spectral clustering, whereas consistency regularization focuses on a "local" perspective via expansion.
</details>
<details>
<summary>摘要</summary>
尽管relational知识储备（RKD）在实践中取得了成功和实用意义，但其理论解释仍然受限。在这个工作中，我们开始了RKD的理论理解，特点是用 semi-supervised classification 问题作为入口。我们首先将RKD视为师模型对人口induced图像中的spectral clustering。通过定义分类错误来衡量师模型预测和真实分类之间的差异，我们证明了RKD在人口上可以达到低分类错误。此外，我们还提供了有限无标示样本的抽象 bound。在 semi-supervised learning 中，我们进一步表明了RKD的标签效率，通过一种涵盖cluster-aware semi-supervised learning 框架，假设分类错误低。最后，我们将数据扩展一致性正则化 integrate 到这个框架中，并证明了RKD在global perspective中促进了spectral clustering，而consistency regularization强调了local perspectivevia expansion。
</details></li>
</ul>
<hr>
<h2 id="Amortized-Variational-Inference-When-and-Why"><a href="#Amortized-Variational-Inference-When-and-Why" class="headerlink" title="Amortized Variational Inference: When and Why?"></a>Amortized Variational Inference: When and Why?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11018">http://arxiv.org/abs/2307.11018</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/charlesm93/amortized_vi">https://github.com/charlesm93/amortized_vi</a></li>
<li>paper_authors: Charles C. Margossian, David M. Blei</li>
<li>for: 这个论文主要针对的是approximate posterior inference的问题，具体来说是用amortized variational inference（A-VI）代替classical factorized variational inference（F-VI）来减轻计算复杂性。</li>
<li>methods: 这篇论文使用了A-VI作为一种计算技巧，以便在深度生成模型中加速推理。在这篇论文中，作者研究了A-VI作为一个通用的后退推理方法，并Derive了关于模型和推理函数下的条件，使得A-VI可以达到F-VI的优秀解。</li>
<li>results: 作者通过 theoretical analysis和实验研究，证明了在某些模型中，A-VI可以与F-VI的优秀解相匹配，而且在某些情况下，A-VI可以更快 converge than F-VI。此外，作者还发现在某些模型中，A-VI无法达到F-VI的优秀解，即使推理函数具有足够的表达能力。<details>
<summary>Abstract</summary>
Amortized variational inference (A-VI) is a method for approximating the intractable posterior distributions that arise in probabilistic models. The defining feature of A-VI is that it learns a global inference function that maps each observation to its local latent variable's approximate posterior. This stands in contrast to the more classical factorized (or mean-field) variational inference (F-VI), which directly learns the parameters of the approximating distribution for each latent variable. In deep generative models, A-VI is used as a computational trick to speed up inference for local latent variables. In this paper, we study A-VI as a general alternative to F-VI for approximate posterior inference. A-VI cannot produce an approximation with a lower Kullback-Leibler divergence than F-VI's optimal solution, because the amortized family is a subset of the factorized family. Thus a central theoretical problem is to characterize when A-VI still attains F-VI's optimal solution. We derive conditions on both the model and the inference function under which A-VI can theoretically achieve F-VI's optimum. We show that for a broad class of hierarchical models, including deep generative models, it is possible to close the gap between A-VI and F-VI. Further, for an even broader class of models, we establish when and how to expand the domain of the inference function to make amortization a feasible strategy. Finally, we prove that for certain models -- including hidden Markov models and Gaussian processes -- A-VI cannot match F-VI's solution, no matter how expressive the inference function is. We also study A-VI empirically. On several examples, we corroborate our theoretical results and investigate the performance of A-VI when varying the complexity of the inference function. When the gap between A-VI and F-VI can be closed, we find that the required complexity of the function need not scale with the number of observations, and that A-VI often converges faster than F-VI.
</details>
<details>
<summary>摘要</summary>
《总结：Amortized Variational Inference的潜在优势和局限性》Amortized Variational Inference（A-VI）是一种用于估计不可计算的 posterior 分布的方法，它在概率模型中出现的 posterior 分布中进行估计。A-VI 的特点在于它学习一个全局的推理函数，该函数可以将每个观察数据映射到其相应的本地隐藏变量的近似 posterior。与传统的分解（或均值场）变量假设（F-VI）相比，A-VI 直接学习每个隐藏变量的参数。在深度生成模型中，A-VI 被用作一种计算技巧，以加速地域性 latent 变量的推理。在这篇论文中，我们研究 A-VI 作为一种通用的近似 posterior 推理方法。由于 A-VI 不能生成一个低于 Kullback-Leibler 差分的近似，因为权重插值家族是 F-VI 的一个子集。因此，A-VI 的主要理论问题是Characterizing situations in which A-VI can attain F-VI's optimal solution。我们 derive conditions on both the model and the inference function under which A-VI can theoretically achieve F-VI's optimum。我们证明，对于一类层次模型，包括深度生成模型，可以减小 A-VI 和 F-VI 之间的差异。此外，对于另一类模型，我们确定了如何扩展推理函数的域，以使权重插值成为可能的策略。最后，我们证明，对于某些模型，例如隐藏 Markov 模型和 Gaussian 过程，A-VI 无法与 F-VI 的解决方案匹配，不管推理函数如何表达。我们还进行了 A-VI 的实验研究。在一些示例中，我们证明了我们的理论结果，并investigated A-VI 在观察数据的变化情况下的性能。当 A-VI 和 F-VI 之间的差异可以减小时，我们发现推理函数的复杂度不必随观察数据的数量增长，而且 A-VI 通常更快 convergence than F-VI。
</details></li>
</ul>
<hr>
<h2 id="Multi-objective-point-cloud-autoencoders-for-explainable-myocardial-infarction-prediction"><a href="#Multi-objective-point-cloud-autoencoders-for-explainable-myocardial-infarction-prediction" class="headerlink" title="Multi-objective point cloud autoencoders for explainable myocardial infarction prediction"></a>Multi-objective point cloud autoencoders for explainable myocardial infarction prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11017">http://arxiv.org/abs/2307.11017</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcel Beetz, Abhirup Banerjee, Vicente Grau</li>
<li>for: 这个研究旨在开发一种基于多对象点云自适应神经网络的可解释性心血栓损预测方法，以提高心血栓损的诊断精度。</li>
<li>methods: 该方法基于多类3D点云表示心脏 анатомия和功能的多对象点云自适应神经网络，其架构包括多个任务特定分支连接到一个低维 latent space，以实现多对象学习 both 重建和心血栓损预测，同时捕捉疾病特定的3D形态信息在可读取的 latent space 中。</li>
<li>results: 在一个大型 UK Biobank 数据集上，这种方法能够准确重建多时间序3D形态，Chamfer 距离输入形态下的误差在图像 pixel 分辨率以下，并在多种机器学习和深度学习标准模型之上提高了19%的incident MI 预测精度。此外，这种方法的任务特定紧凑的 latent space 可以清晰地分离控制和 MI 群集，并与相应的3D形态之间存在 клиниче可能的关联，因此证明了预测的可解释性。<details>
<summary>Abstract</summary>
Myocardial infarction (MI) is one of the most common causes of death in the world. Image-based biomarkers commonly used in the clinic, such as ejection fraction, fail to capture more complex patterns in the heart's 3D anatomy and thus limit diagnostic accuracy. In this work, we present the multi-objective point cloud autoencoder as a novel geometric deep learning approach for explainable infarction prediction, based on multi-class 3D point cloud representations of cardiac anatomy and function. Its architecture consists of multiple task-specific branches connected by a low-dimensional latent space to allow for effective multi-objective learning of both reconstruction and MI prediction, while capturing pathology-specific 3D shape information in an interpretable latent space. Furthermore, its hierarchical branch design with point cloud-based deep learning operations enables efficient multi-scale feature learning directly on high-resolution anatomy point clouds. In our experiments on a large UK Biobank dataset, the multi-objective point cloud autoencoder is able to accurately reconstruct multi-temporal 3D shapes with Chamfer distances between predicted and input anatomies below the underlying images' pixel resolution. Our method outperforms multiple machine learning and deep learning benchmarks for the task of incident MI prediction by 19% in terms of Area Under the Receiver Operating Characteristic curve. In addition, its task-specific compact latent space exhibits easily separable control and MI clusters with clinically plausible associations between subject encodings and corresponding 3D shapes, thus demonstrating the explainability of the prediction.
</details>
<details>
<summary>摘要</summary>
心肺infarction (MI) 是全球最常见的死亡原因之一。通常使用在临床中的图像基于标记器，如舒缩率，无法捕捉心肺三维形态中更复杂的模式，因此限制诊断准确性。在这种工作中，我们介绍了一种多目标点云自适应神经网络，作为解释性infarction预测的新的几何深度学方法，基于多类3D点云表示心肺 анатоми和功能。其架构包括多个任务特定分支，连接了一个低维ensional的秘密空间，以实现有效的多目标学习重建和infarction预测，同时捕捉疾病特定的3D形状信息。此外，其层次分支设计和点云深度运算使得高级别特征学习可以直接进行高分辨率的生物Point cloud。在我们对大型UK Biobank数据集的实验中，多目标点云自适应神经网络能够准确重建多时间点云形态，Chamfer距离输入和预测的形态下限于图像的像素分辨率。我们的方法在incident MI预测任务上超过多种机器学习和深度学习参考值19%，在接收操作特征曲线的面积下测试 Area Under the Receiver Operating Characteristic curve。此外，任务特定的秘密空间表现出易分割控制和MI团群，并且与相应的3D形状之间存在临床可能的相关性，因此证明预测的可解释性。
</details></li>
</ul>
<hr>
<h2 id="Flow-Map-Learning-for-Unknown-Dynamical-Systems-Overview-Implementation-and-Benchmarks"><a href="#Flow-Map-Learning-for-Unknown-Dynamical-Systems-Overview-Implementation-and-Benchmarks" class="headerlink" title="Flow Map Learning for Unknown Dynamical Systems: Overview, Implementation, and Benchmarks"></a>Flow Map Learning for Unknown Dynamical Systems: Overview, Implementation, and Benchmarks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11013">http://arxiv.org/abs/2307.11013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Victor Churchill, Dongbin Xiu</li>
<li>for: 这 paper 是关于 Flow map learning (FML) 和深度神经网络 (DNNs) 如何用于数据驱动模型未知动力系统的研究。</li>
<li>methods: 这 paper 使用 FML 框架，并提供了实现这种方法的重要计算细节。</li>
<li>results: 这 paper 提供了一组定义良好的 benchmark 问题，以及这些问题的 FML 结果，以便其他研究人员可以进行交互式检验和结果重现。<details>
<summary>Abstract</summary>
Flow map learning (FML), in conjunction with deep neural networks (DNNs), has shown promises for data driven modeling of unknown dynamical systems. A remarkable feature of FML is that it is capable of producing accurate predictive models for partially observed systems, even when their exact mathematical models do not exist. In this paper, we present an overview of the FML framework, along with the important computational details for its successful implementation. We also present a set of well defined benchmark problems for learning unknown dynamical systems. All the numerical details of these problems are presented, along with their FML results, to ensure that the problems are accessible for cross-examination and the results are reproducible.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the following text into Simplified Chinese.<</SYS>>流图学习（FML），与深度神经网络（DNN）结合，已经表现出模拟未知动力系统的数据驱动模型的搭配性。FML有能力生成准确预测模型，即使系统的准确数学模型不存在。在这篇论文中，我们提供FML框架的概述，以及成功实施的计算细节。我们还提供一组已定义的benchmark问题，用于学习未知动力系统。这些问题的数字细节和FML结果都提供，以便让问题可以进行交叉检验，并且结果可以重新制作。
</details></li>
</ul>
<hr>
<h2 id="Neuron-Sensitivity-Guided-Test-Case-Selection-for-Deep-Learning-Testing"><a href="#Neuron-Sensitivity-Guided-Test-Case-Selection-for-Deep-Learning-Testing" class="headerlink" title="Neuron Sensitivity Guided Test Case Selection for Deep Learning Testing"></a>Neuron Sensitivity Guided Test Case Selection for Deep Learning Testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11011">http://arxiv.org/abs/2307.11011</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ase2023paper/nss">https://github.com/ase2023paper/nss</a></li>
<li>paper_authors: Dong Huang, Qingwen Bu, Yichao Fu, Yuhao Qing, Bocheng Xiao, Heming Cui</li>
<li>for: 这篇论文的目的是解释如何透过NSS（Neural Network Sensitivity guided test case Selection）方法来快速检测深度神经网络（DNN）模型中的错误行为，并且将其修复。</li>
<li>methods: 这篇论文使用了NSS方法，它利用了内部神经元的信息来选择有价值的测试案例，以提高测试效率和错误检测率。</li>
<li>results: 根据论文的结果，NSS方法可以实现高度的错误检测率，比如在MNIST &amp; LeNet1实验中，从5%的测试案例中，NSS可以获得81.8%的错误检测率，较baseline方法高出20%。<details>
<summary>Abstract</summary>
Deep Neural Networks~(DNNs) have been widely deployed in software to address various tasks~(e.g., autonomous driving, medical diagnosis). However, they could also produce incorrect behaviors that result in financial losses and even threaten human safety. To reveal the incorrect behaviors in DNN and repair them, DNN developers often collect rich unlabeled datasets from the natural world and label them to test the DNN models. However, properly labeling a large number of unlabeled datasets is a highly expensive and time-consuming task.   To address the above-mentioned problem, we propose NSS, Neuron Sensitivity guided test case Selection, which can reduce the labeling time by selecting valuable test cases from unlabeled datasets. NSS leverages the internal neuron's information induced by test cases to select valuable test cases, which have high confidence in causing the model to behave incorrectly. We evaluate NSS with four widely used datasets and four well-designed DNN models compared to SOTA baseline methods. The results show that NSS performs well in assessing the test cases' probability of fault triggering and model improvement capabilities. Specifically, compared with baseline approaches, NSS obtains a higher fault detection rate~(e.g., when selecting 5\% test case from the unlabeled dataset in MNIST \& LeNet1 experiment, NSS can obtain 81.8\% fault detection rate, 20\% higher than baselines).
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）在软件中广泛应用，用于解决多种任务（例如自动驾驶和医疗诊断）。然而，它们也可能产生错误行为，导致金钱损失和对人类安全的威胁。为了揭示DNN中的错误行为并修复它们，DNN开发者们经常收集自然世界中的丰富无标数据集和将其标记以测试DNN模型。然而，为了标记大量无标数据集是非常昂贵和时间消耗的。为解决上述问题，我们提出了NSS，即神经元敏感度导引测试 caso选择。NSS可以将valuable测试 caso从无标数据集中选择出来，以降低标记时间。NSS利用测试 caso对内部神经元的影响来选择有价值的测试 caso，这些测试 caso具有高度触发模型错误的 confidence。我们对四个常用的数据集和四个Well-designed DNN模型进行评估，并与当前最佳方法进行比较。结果表明，NSS在评估测试 caso的可能性和模型改进能力方面表现良好。具体来说，相比基eline方法，NSS在MNIST & LeNet1 experiment中选择5%的测试 caso时可以获得81.8%的错误检测率，高于基eline20%。
</details></li>
</ul>
<hr>
<h2 id="Sharpness-Minimization-Algorithms-Do-Not-Only-Minimize-Sharpness-To-Achieve-Better-Generalization"><a href="#Sharpness-Minimization-Algorithms-Do-Not-Only-Minimize-Sharpness-To-Achieve-Better-Generalization" class="headerlink" title="Sharpness Minimization Algorithms Do Not Only Minimize Sharpness To Achieve Better Generalization"></a>Sharpness Minimization Algorithms Do Not Only Minimize Sharpness To Achieve Better Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11007">http://arxiv.org/abs/2307.11007</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaiyue Wen, Zhiyuan Li, Tengyu Ma</li>
<li>for: 这种研究旨在解释为什么过参数神经网络可以泛化？</li>
<li>methods: 该研究使用了理论和实验方法，对两层ReLU网络进行了研究，并identified以下三种情况：（1）平坦性确实导致泛化；（2）存在不泛化最平坦的模型和锋尖最小化算法，但它们并不泛化；（3）有些不泛化最平坦的模型，但锋尖最小化算法仍然泛化。</li>
<li>results: 研究结果表明，锋尖和泛化之间的关系不仅取决于数据分布和模型结构，而且锋尖最小化算法不仅减少锋尖度来获得更好的泛化性。这说明，过参数神经网络的泛化仍然需要寻找更多的解释。<details>
<summary>Abstract</summary>
Despite extensive studies, the underlying reason as to why overparameterized neural networks can generalize remains elusive. Existing theory shows that common stochastic optimizers prefer flatter minimizers of the training loss, and thus a natural potential explanation is that flatness implies generalization. This work critically examines this explanation. Through theoretical and empirical investigation, we identify the following three scenarios for two-layer ReLU networks: (1) flatness provably implies generalization; (2) there exist non-generalizing flattest models and sharpness minimization algorithms fail to generalize, and (3) perhaps most surprisingly, there exist non-generalizing flattest models, but sharpness minimization algorithms still generalize. Our results suggest that the relationship between sharpness and generalization subtly depends on the data distributions and the model architectures and sharpness minimization algorithms do not only minimize sharpness to achieve better generalization. This calls for the search for other explanations for the generalization of over-parameterized neural networks.
</details>
<details>
<summary>摘要</summary>
尽管有广泛的研究，底层原因为为争 parameterized neural networks 能够泛化仍然未知。现有理论表明，通用的随机优化器偏好训练损失函数的平坦 minimizers，因此一个自然的可能解释是平坦性implicit generalization。这项工作 Critically examines this explanation。通过理论和实验调查，我们确定了以下三个场景：（1）平坦性可以证明地导致泛化；（2）存在不泛化的最平坦模型和锋尖优化算法失败泛化，以及（3）最有趣的是，存在不泛化的最平坦模型，但锋尖优化算法仍然泛化。我们的结果表明，泛化与锋尖之间存在复杂的相互关系，并且锋尖优化算法不仅仅是为了减少锋尖来实现更好的泛化。这些结果呼吁搜索其他解释泛化过 parameterized neural networks 的机制。
</details></li>
</ul>
<hr>
<h2 id="Private-Federated-Learning-with-Autotuned-Compression"><a href="#Private-Federated-Learning-with-Autotuned-Compression" class="headerlink" title="Private Federated Learning with Autotuned Compression"></a>Private Federated Learning with Autotuned Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10999">http://arxiv.org/abs/2307.10999</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research/federated">https://github.com/google-research/federated</a></li>
<li>paper_authors: Enayat Ullah, Christopher A. Choquette-Choo, Peter Kairouz, Sewoong Oh</li>
<li>for: 降低聚合学习中的通信量，不需要设置或调整压缩率。</li>
<li>methods: 使用安全聚合和差分隐私来保证隐私，并在训练过程中自动调整压缩率，以适应训练问题的“困难程度”。</li>
<li>results: 在实际数据上实现了不需要调整的压缩率，并且可以保证隐私。<details>
<summary>Abstract</summary>
We propose new techniques for reducing communication in private federated learning without the need for setting or tuning compression rates. Our on-the-fly methods automatically adjust the compression rate based on the error induced during training, while maintaining provable privacy guarantees through the use of secure aggregation and differential privacy. Our techniques are provably instance-optimal for mean estimation, meaning that they can adapt to the ``hardness of the problem" with minimal interactivity. We demonstrate the effectiveness of our approach on real-world datasets by achieving favorable compression rates without the need for tuning.
</details>
<details>
<summary>摘要</summary>
我们提出了一些新的技术来减少联盟学习中的通信量，不需要设置或调整压缩率。我们的在线方法会自动根据训练过程中的错误来调整压缩率，同时保持可信数据隐藏和分散隐藏的保证。我们的技术可以适应问题的“困难程度”，并具有最小化互动性。我们在实际应用中获得了有利的压缩率，无需调整。
</details></li>
</ul>
<hr>
<h2 id="DREAM-Domain-free-Reverse-Engineering-Attributes-of-Black-box-Model"><a href="#DREAM-Domain-free-Reverse-Engineering-Attributes-of-Black-box-Model" class="headerlink" title="DREAM: Domain-free Reverse Engineering Attributes of Black-box Model"></a>DREAM: Domain-free Reverse Engineering Attributes of Black-box Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10997">http://arxiv.org/abs/2307.10997</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rongqing Li, Jiaqi Yu, Changsheng Li, Wenhan Luo, Ye Yuan, Guoren Wang</li>
<li>for: 这篇论文的目的是研究一种不知道黑盒模型训练数据的情况下，可以透过一系列的询问来暴露黑盒模型的特性。</li>
<li>methods: 这篇论文提出了一个基于偏出现象（out-of-distribution，OOD）扩展的框架，通过这个框架可以将黑盒模型的特性们逆向推断出来，不需要知道黑盒模型的训练数据。</li>
<li>results: 实验结果显示，该方法比基于基线模型的方法更有优势，并且在不同的领域中均有优秀的测试成绩。<details>
<summary>Abstract</summary>
Deep learning models are usually black boxes when deployed on machine learning platforms. Prior works have shown that the attributes ($e.g.$, the number of convolutional layers) of a target black-box neural network can be exposed through a sequence of queries. There is a crucial limitation: these works assume the dataset used for training the target model to be known beforehand and leverage this dataset for model attribute attack. However, it is difficult to access the training dataset of the target black-box model in reality. Therefore, whether the attributes of a target black-box model could be still revealed in this case is doubtful. In this paper, we investigate a new problem of Domain-agnostic Reverse Engineering the Attributes of a black-box target Model, called DREAM, without requiring the availability of the target model's training dataset, and put forward a general and principled framework by casting this problem as an out of distribution (OOD) generalization problem. In this way, we can learn a domain-agnostic model to inversely infer the attributes of a target black-box model with unknown training data. This makes our method one of the kinds that can gracefully apply to an arbitrary domain for model attribute reverse engineering with strong generalization ability. Extensive experimental studies are conducted and the results validate the superiority of our proposed method over the baselines.
</details>
<details>
<summary>摘要</summary>
深度学习模型在机器学习平台上通常是黑obox。先前的研究表明，目标黑obox神经网络的特征（例如，卷积层的数量）可以通过一系列查询暴露出来。然而，这些研究假设了目标模型的训练集已经知道，并利用这个训练集进行模型特征攻击。然而，在实际场景中，目标模型的训练集往往难以获取。因此，目标模型的特征是否可以在这种情况下暴露出来的是有很大的uncertainty。在这篇论文中，我们调查了一个新的问题：针对黑obox目标模型的领域无关特征探测，称为DREAM（黑obox模型特征探测）。我们不需要知道目标模型的训练集，并且提出了一种普适和原则性的框架，将这个问题转化为对外围（OOD）泛化问题。因此，我们可以学习一个领域无关的模型，以倒计时探测黑obox目标模型的特征。这使我们的方法成为可以适应任意领域的模型特征探测方法之一，并且具有强大的泛化能力。我们进行了广泛的实验研究，结果证明了我们的提议方法的优越性。
</details></li>
</ul>
<hr>
<h2 id="Progressive-distillation-diffusion-for-raw-music-generation"><a href="#Progressive-distillation-diffusion-for-raw-music-generation" class="headerlink" title="Progressive distillation diffusion for raw music generation"></a>Progressive distillation diffusion for raw music generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10994">http://arxiv.org/abs/2307.10994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Svetlana Pavlova</li>
<li>for: 这个论文旨在应用新的深度学习方法来生成原始的音频文件。它基于扩散模型，这是一种最近受到计算机视觉社区关注的深度生成模型。</li>
<li>methods: 这个论文使用进步的扩散Diffusion模型，并使用1D U-Net进行逐步减少。然后，对不同扩散参数的比较和其全局效果的展示。</li>
<li>results: 这个论文通过对不同自收集的数据进行实验，实现了无条件生成音频的进程。模型能够处理进程audio处理和生成，并使用变换从1个通道128x384到3个通道128x128 mel-spectrograms进行循环生成。<details>
<summary>Abstract</summary>
This paper aims to apply a new deep learning approach to the task of generating raw audio files. It is based on diffusion models, a recent type of deep generative model. This new type of method has recently shown outstanding results with image generation. A lot of focus has been given to those models by the computer vision community. On the other hand, really few have been given for other types of applications such as music generation in waveform domain.   In this paper the model for unconditional generating applied to music is implemented: Progressive distillation diffusion with 1D U-Net. Then, a comparison of different parameters of diffusion and their value in a full result is presented. One big advantage of the methods implemented through this work is the fact that the model is able to deal with progressing audio processing and generating , using transformation from 1-channel 128 x 384 to 3-channel 128 x 128 mel-spectrograms and looped generation. The empirical comparisons are realized across different self-collected datasets.
</details>
<details>
<summary>摘要</summary>
这篇论文目标是应用一种新的深度学习方法来生成原始音频文件。它基于扩散模型，一种最近的深度生成模型。这种新的方法在图像生成领域有出色的成绩， Computer Vision 社区对其具有很大的关注。然而，对其他应用领域，如音乐生成，实际上很少有研究。在这篇论文中，我们实现了不可Conditional的生成模型：进程散布扩散 with 1D U-Net。然后，我们对不同扩散参数的影响进行了比较，并对全面结果进行了评价。这种方法的一大优点是它可以处理进行 Audio 处理和生成，从1个通道的 128 x 384 转换为3个通道的 128 x 128 mel-spectrograms，并实现循环生成。我们在不同自己收集的数据集上进行了实际比较。
</details></li>
</ul>
<hr>
<h2 id="LLM-Cognitive-Judgements-Differ-From-Human"><a href="#LLM-Cognitive-Judgements-Differ-From-Human" class="headerlink" title="LLM Cognitive Judgements Differ From Human"></a>LLM Cognitive Judgements Differ From Human</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11787">http://arxiv.org/abs/2307.11787</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sotlampr/llm-cognitive-judgements">https://github.com/sotlampr/llm-cognitive-judgements</a></li>
<li>paper_authors: Sotiris Lamprinidis</li>
<li>for: 研究大语言模型（LLMs）的认知能力</li>
<li>methods: 使用GPT-3和ChatGPT模型完成有限数据 inductive reasoning任务</li>
<li>results: GPT-3和ChatGPT模型的认知判断不类似于人类的认知<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have lately been on the spotlight of researchers, businesses, and consumers alike. While the linguistic capabilities of such models have been studied extensively, there is growing interest in investigating them as cognitive subjects. In the present work I examine GPT-3 and ChatGPT capabilities on an limited-data inductive reasoning task from the cognitive science literature. The results suggest that these models' cognitive judgements are not human-like.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在研究人员、企业和消费者之间备受关注。虽然这些模型的语言能力已经得到了广泛的研究，但是有越来越多的人想研究它们作为认知主体。在 presente 作品中，我 исследова了 GPT-3 和 ChatGPT 在有限数据 inductive reasoning 任务上的能力。结果表明这些模型的认知判断不如人类的。
</details></li>
</ul>
<hr>
<h2 id="Investigating-minimizing-the-training-set-fill-distance-in-machine-learning-regression"><a href="#Investigating-minimizing-the-training-set-fill-distance-in-machine-learning-regression" class="headerlink" title="Investigating minimizing the training set fill distance in machine learning regression"></a>Investigating minimizing the training set fill distance in machine learning regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10988">http://arxiv.org/abs/2307.10988</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paolo Climaco, Jochen Garcke</li>
<li>for: 这篇论文的目的是提出一种采样方法，以最小化预测误差。</li>
<li>methods: 本文使用了一种基于采样的方法，通过从大量的未标注数据中采样小型训练集，以提高模型性能而降低计算成本。</li>
<li>results: 实验结果显示，这种采样方法可以对多种回归模型进行最佳化，并且较以往的采样方法有着明显的优势。<details>
<summary>Abstract</summary>
Many machine learning regression methods leverage large datasets for training predictive models. However, using large datasets may not be feasible due to computational limitations or high labelling costs. Therefore, sampling small training sets from large pools of unlabelled data points is essential to maximize model performance while maintaining computational efficiency. In this work, we study a sampling approach aimed to minimize the fill distance of the selected set. We derive an upper bound for the maximum expected prediction error that linearly depends on the training set fill distance, conditional to the knowledge of data features. For empirical validation, we perform experiments using two regression models on two datasets. We empirically show that selecting a training set by aiming to minimize the fill distance, thereby minimizing the bound, significantly reduces the maximum prediction error of various regression models, outperforming existing sampling approaches by a large margin.
</details>
<details>
<summary>摘要</summary>
很多机器学习回归方法利用大量数据进行训练预测模型。然而，使用大量数据可能不是可行的，因为计算限制或高标注成本。因此，从大量未标注数据点中采样小训练集是必要的，以最大化模型性能而减少计算成本。在这种情况下，我们研究了一种采样方法，以最小化选择集的填距。我们 derivates了一个上界，用于预测误差的最大值，其 conditional 于数据特征的知识。为了Empirical验证，我们在两个回归模型上进行了两个数据集的实验。我们发现，通过选择填距最小的训练集，最大化约束，并最小化 bound，可以大幅降低不同回归模型的最大预测误差，超过现有采样方法的表现。
</details></li>
</ul>
<hr>
<h2 id="MASR-Metadata-Aware-Speech-Representation"><a href="#MASR-Metadata-Aware-Speech-Representation" class="headerlink" title="MASR: Metadata Aware Speech Representation"></a>MASR: Metadata Aware Speech Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10982">http://arxiv.org/abs/2307.10982</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anjali Raj, Shikhar Bharadwaj, Sriram Ganapathy, Min Ma, Shikhar Vashishth</li>
<li>for: 提高演示语言模型的性能，使用多种外部知识来增强 metadata 信息的利用</li>
<li>methods: 提出了Metadata Aware Speech Representation learning（MASR）框架，可以结合任何 SSL 方法，并使用样本级对比性 Matrix 来增强表示</li>
<li>results: 在语言识别、语音识别和其他非语义任务中，MASR 表示具有显著的性能提升，并进行了详细的语言识别任务分析，以提供表示增强的原因<details>
<summary>Abstract</summary>
In the recent years, speech representation learning is constructed primarily as a self-supervised learning (SSL) task, using the raw audio signal alone, while ignoring the side-information that is often available for a given speech recording. In this paper, we propose MASR, a Metadata Aware Speech Representation learning framework, which addresses the aforementioned limitations. MASR enables the inclusion of multiple external knowledge sources to enhance the utilization of meta-data information. The external knowledge sources are incorporated in the form of sample-level pair-wise similarity matrices that are useful in a hard-mining loss. A key advantage of the MASR framework is that it can be combined with any choice of SSL method. Using MASR representations, we perform evaluations on several downstream tasks such as language identification, speech recognition and other non-semantic tasks such as speaker and emotion recognition. In these experiments, we illustrate significant performance improvements for the MASR over other established benchmarks. We perform a detailed analysis on the language identification task to provide insights on how the proposed loss function enables the representations to separate closely related languages.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="PATROL-Privacy-Oriented-Pruning-for-Collaborative-Inference-Against-Model-Inversion-Attacks"><a href="#PATROL-Privacy-Oriented-Pruning-for-Collaborative-Inference-Against-Model-Inversion-Attacks" class="headerlink" title="PATROL: Privacy-Oriented Pruning for Collaborative Inference Against Model Inversion Attacks"></a>PATROL: Privacy-Oriented Pruning for Collaborative Inference Against Model Inversion Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10981">http://arxiv.org/abs/2307.10981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiwei Ding, Lan Zhang, Miao Pan, Xiaoyong Yuan</li>
<li>for: 提高边缘设备的隐私和安全性，推动可靠的分布式推理</li>
<li>methods: 利用隐私导向的剪辑技术，在边缘设备上部署更多层，以提高任务特定的特征提取和隐私保护</li>
<li>results: 提高了隐私和安全性，同时保持了可靠的分布式推理性能<details>
<summary>Abstract</summary>
Collaborative inference has been a promising solution to enable resource-constrained edge devices to perform inference using state-of-the-art deep neural networks (DNNs). In collaborative inference, the edge device first feeds the input to a partial DNN locally and then uploads the intermediate result to the cloud to complete the inference. However, recent research indicates model inversion attacks (MIAs) can reconstruct input data from intermediate results, posing serious privacy concerns for collaborative inference. Existing perturbation and cryptography techniques are inefficient and unreliable in defending against MIAs while performing accurate inference. This paper provides a viable solution, named PATROL, which develops privacy-oriented pruning to balance privacy, efficiency, and utility of collaborative inference. PATROL takes advantage of the fact that later layers in a DNN can extract more task-specific features. Given limited local resources for collaborative inference, PATROL intends to deploy more layers at the edge based on pruning techniques to enforce task-specific features for inference and reduce task-irrelevant but sensitive features for privacy preservation. To achieve privacy-oriented pruning, PATROL introduces two key components: Lipschitz regularization and adversarial reconstruction training, which increase the reconstruction errors by reducing the stability of MIAs and enhance the target inference model by adversarial training, respectively.
</details>
<details>
<summary>摘要</summary>
协同推理已经是一个有前途的解决方案，使得具有限制的边缘设备可以使用当前最佳深度学习模型（DNN）进行推理。在协同推理中，边缘设备首先将输入feed到本地部分DNN中，然后将中间结果上传到云端以完成推理。然而，最近的研究表明，模型反向攻击（MIA）可以从中间结果中重construct输入数据，对协同推理 pose serious privacy concerns。现有的干扰和加密技术是不可靠和不fficient的，无法防止 MIA 的攻击。这篇论文提供了一个可行的解决方案，名为PATROL，它通过在隐私、效率和可用性之间平衡privacy-oriented pruning来解决这个问题。PATROL利用了DNN的后 layers可以更好地提取任务特定的特征。在边缘设备具有限制的协同推理资源的情况下，PATROL将更多的层部署在边缘基础上，使用剪辑技术来强制实施任务特定的特征 для推理，同时减少任务无关但敏感的特征来保护隐私。为了实现隐私 oriented pruning，PATROL引入了两个关键 ком ponent：Lipschitz regularization和对抗重建训练。这两个 ком ponent 会增加MIA的重建错误，从而提高目标推理模型的性能，同时增强隐私保护。
</details></li>
</ul>
<hr>
<h2 id="Globally-Normalising-the-Transducer-for-Streaming-Speech-Recognition"><a href="#Globally-Normalising-the-Transducer-for-Streaming-Speech-Recognition" class="headerlink" title="Globally Normalising the Transducer for Streaming Speech Recognition"></a>Globally Normalising the Transducer for Streaming Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10975">http://arxiv.org/abs/2307.10975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rogier van Dalen</li>
<li>for: 这篇论文主要是关于如何解决流行式模型中的一个数学问题，以提高流行式模型在语音识别任务中的表现。</li>
<li>methods: 该论文提出了一种将全球 нормализация应用于流行式模型中，以解决流行式模型在部分输入序列时的数学问题。</li>
<li>results: 根据实验结果，在应用全球 нормализаation后，流行式模型的词错率下降了9-11%相对，相对于lookahead模式，流行式模式的表现减少了约一半的差距。<details>
<summary>Abstract</summary>
The Transducer (e.g. RNN-Transducer or Conformer-Transducer) generates an output label sequence as it traverses the input sequence. It is straightforward to use in streaming mode, where it generates partial hypotheses before the complete input has been seen. This makes it popular in speech recognition. However, in streaming mode the Transducer has a mathematical flaw which, simply put, restricts the model's ability to change its mind. The fix is to replace local normalisation (e.g. a softmax) with global normalisation, but then the loss function becomes impossible to evaluate exactly. A recent paper proposes to solve this by approximating the model, severely degrading performance. Instead, this paper proposes to approximate the loss function, allowing global normalisation to apply to a state-of-the-art streaming model. Global normalisation reduces its word error rate by 9-11% relative, closing almost half the gap between streaming and lookahead mode.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换器（例如 RNN-转换器或 Conformer-转换器）在输入序列中生成输出标签序列。在流式模式下使用它非常直接，它可以在输入序列未完全传输前就生成部分假设。这使得它在语音识别中很受欢迎。然而，在流式模式下，转换器具有一个数学毛病，简单来说，限制模型改变自己的想法的能力。这可以通过全局 нормализации（例如 softmax）的替换来解决，但是这会使损失函数不能准确计算。一篇最近的论文提议解决这问题，通过模型的 aproximation 来解决，但这会严重降低性能。这篇论文提议通过近似损失函数来解决，使得全局 нормализация可以应用于流式模型中，从而降低流式模式中的单词错误率，相对于lookahead模式下降9-11%，只差半个差距。<</SYS>>
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/21/cs.LG_2023_07_21/" data-id="clp89doga00ooi788e92fbaux" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/21/eess.IV_2023_07_21/" class="article-date">
  <time datetime="2023-07-21T09:00:00.000Z" itemprop="datePublished">2023-07-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/21/eess.IV_2023_07_21/">eess.IV - 2023-07-21</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Deep-Reinforcement-Learning-Based-System-for-Intraoperative-Hyperspectral-Video-Autofocusing"><a href="#Deep-Reinforcement-Learning-Based-System-for-Intraoperative-Hyperspectral-Video-Autofocusing" class="headerlink" title="Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing"></a>Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11638">http://arxiv.org/abs/2307.11638</a></li>
<li>repo_url: None</li>
<li>paper_authors: Charlie Budd, Jianrong Qiu, Oscar MacCormac, Martin Huber, Christopher Mower, Mirek Janatka, Théo Trotouin, Jonathan Shapey, Mads S. Bergholt, Tom Vercauteren</li>
<li>for: 本研究旨在开发一种可靠的、快速的激光成像系统，以便在手术室中实时进行细胞分类。</li>
<li>methods: 该研究使用了深度学习自适应关注法，并将焦点调整liquid镜组合到视频激光成像镜头中。</li>
<li>results: 研究发现，使用深度学习自适应关注法可以提高激光成像系统的焦点精度，并且在比较 tradicional 焦点调整策略时表现出 statistically  significan advantage。此外，两名 neurosurgeon 在不知情的用户测试中，对不同的焦点调整策略进行比较，并评价了我们的新方法，发现其最为满意。<details>
<summary>Abstract</summary>
Hyperspectral imaging (HSI) captures a greater level of spectral detail than traditional optical imaging, making it a potentially valuable intraoperative tool when precise tissue differentiation is essential. Hardware limitations of current optical systems used for handheld real-time video HSI result in a limited focal depth, thereby posing usability issues for integration of the technology into the operating room. This work integrates a focus-tunable liquid lens into a video HSI exoscope, and proposes novel video autofocusing methods based on deep reinforcement learning. A first-of-its-kind robotic focal-time scan was performed to create a realistic and reproducible testing dataset. We benchmarked our proposed autofocus algorithm against traditional policies, and found our novel approach to perform significantly ($p<0.05$) better than traditional techniques ($0.070\pm.098$ mean absolute focal error compared to $0.146\pm.148$). In addition, we performed a blinded usability trial by having two neurosurgeons compare the system with different autofocus policies, and found our novel approach to be the most favourable, making our system a desirable addition for intraoperative HSI.
</details>
<details>
<summary>摘要</summary>
This work integrates a focus-tunable liquid lens into a video HSI exoscope and proposes novel video autofocusing methods based on deep reinforcement learning. A first-of-its-kind robotic focal-time scan was performed to create a realistic and reproducible testing dataset. The proposed autofocus algorithm was benchmarked against traditional policies, and the results showed that the novel approach performed significantly better ($p<0.05$) than traditional techniques ($0.070\pm.098$ mean absolute focal error compared to $0.146\pm.148$).In addition, a blinded usability trial was conducted by having two neurosurgeons compare the system with different autofocus policies, and the results showed that the novel approach was the most favourable, making the system a desirable addition for intraoperative HSI.
</details></li>
</ul>
<hr>
<h2 id="Computational-Image-Formation"><a href="#Computational-Image-Formation" class="headerlink" title="Computational Image Formation"></a>Computational Image Formation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11635">http://arxiv.org/abs/2307.11635</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AnirudhaRamesh/15663-Computational-Photography-Assignment1">https://github.com/AnirudhaRamesh/15663-Computational-Photography-Assignment1</a></li>
<li>paper_authors: Stanley H. Chan</li>
<li>For: The paper is focused on the concept of computational image formation (CIF) and its applications in imaging through adverse weather conditions.* Methods: The paper introduces the idea of using an approximate mapping $\mathcal{H}_{\theta}$ to simulate the forward degradation process in imaging, and discusses the attributes of a CIF simulator, including accuracy, speed, well-posedness, and differentiability.* Results: The paper provides a detailed case study on imaging through atmospheric turbulence using CIF, and discusses other examples of CIF applications. The paper also shares thoughts on the future direction and recommendations for the community.<details>
<summary>Abstract</summary>
At the pinnacle of computational imaging is the co-optimization of camera and algorithm. This, however, is not the only form of computational imaging. In problems such as imaging through adverse weather, the bigger challenge is how to accurately simulate the forward degradation process so that we can synthesize data to train reconstruction models and/or integrating the forward model as part of the reconstruction algorithm. This article introduces the concept of computational image formation (CIF). Compared to the standard inverse problems where the goal is to recover the latent image $\mathbf{x}$ from the observation $\mathbf{y} = \mathcal{G}(\mathbf{x})$, CIF shifts the focus to designing an approximate mapping $\mathcal{H}_{\theta}$ such that $\mathcal{H}_{\theta} \approx \mathcal{G}$ while giving a better image reconstruction result. The word ``computational'' highlights the fact that the image formation is now replaced by a numerical simulator. While matching nature remains an important goal, CIF pays even greater attention on strategically choosing an $\mathcal{H}_{\theta}$ so that the reconstruction performance is maximized.   The goal of this article is to conceptualize the idea of CIF by elaborating on its meaning and implications. The first part of the article is a discussion on the four attributes of a CIF simulator: accurate enough to mimic $\mathcal{G}$, fast enough to be integrated as part of the reconstruction, providing a well-posed inverse problem when plugged into the reconstruction, and differentiable in the backpropagation sense. The second part of the article is a detailed case study based on imaging through atmospheric turbulence. The third part of the article is a collection of other examples that fall into the category of CIF. Finally, thoughts about the future direction and recommendations to the community are shared.
</details>
<details>
<summary>摘要</summary>
在计算成像领域的尽头是合理化相机和算法的共同优化。然而，这并不是唯一的计算成像方式。在如影像受恶征天气等问题中，更大的挑战是如何准确地模拟前向干扰过程，以便可以合成数据来训练重建模型和/或将前向模型纳入重建算法中。这篇文章介绍了计算成像形成（CIF）的概念。与标准的逆问题where the goal is to recover the latent image $\mathbf{x}$ from the observation $\mathbf{y} = \mathcal{G}(\mathbf{x})$不同，CIF将注意力集中在设计一个approximate mapping $\mathcal{H}_{\theta}$，使其 approximate $\mathcal{G}$，同时提供更好的图像重建结果。“计算”一词 highlights the fact that the image formation is now replaced by a numerical simulator。而Matching nature remains an important goal，CIF pays even greater attention on strategically choosing an $\mathcal{H}_{\theta}$ so that the reconstruction performance is maximized。本文的目标是把CIF的概念进行详细说明和分析，包括其意义和影响。文中的第一部分是讨论CIF simulator的四个特征：准确地模拟 $\mathcal{G}$，快速 enough to be integrated as part of the reconstruction，提供一个准确的逆问题when plugged into the reconstruction，以及在反射卷积中可微分。第二部分是基于大气扩散的详细案例研究。第三部分是收集其他符合CIF的例子。最后，文章结束了未来方向和对社区的建议。
</details></li>
</ul>
<hr>
<h2 id="Cascaded-multitask-U-Net-using-topological-loss-for-vessel-segmentation-and-centerline-extraction"><a href="#Cascaded-multitask-U-Net-using-topological-loss-for-vessel-segmentation-and-centerline-extraction" class="headerlink" title="Cascaded multitask U-Net using topological loss for vessel segmentation and centerline extraction"></a>Cascaded multitask U-Net using topological loss for vessel segmentation and centerline extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11603">http://arxiv.org/abs/2307.11603</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Rougé, Nicolas Passat, Odyssée Merveille</li>
<li>for: 这paper的目的是提出一种基于深度学习的血管分割和中心线抽取方法，以提高血管疾病诊断工具的精度。</li>
<li>methods: 这paper使用了一种基于U-Net的方法，通过计算分割后的血管skeleton来提高分割结果的准确性。</li>
<li>results: 这paper的实验结果表明，使用U-Net计算血管skeleton可以提高分割结果的准确性，并且可以提供更加准确的血管中心线。<details>
<summary>Abstract</summary>
Vessel segmentation and centerline extraction are two crucial preliminary tasks for many computer-aided diagnosis tools dealing with vascular diseases. Recently, deep-learning based methods have been widely applied to these tasks. However, classic deep-learning approaches struggle to capture the complex geometry and specific topology of vascular networks, which is of the utmost importance in most applications. To overcome these limitations, the clDice loss, a topological loss that focuses on the vessel centerlines, has been recently proposed. This loss requires computing, with a proposed soft-skeleton algorithm, the skeletons of both the ground truth and the predicted segmentation. However, the soft-skeleton algorithm provides suboptimal results on 3D images, which makes the clDice hardly suitable on 3D images. In this paper, we propose to replace the soft-skeleton algorithm by a U-Net which computes the vascular skeleton directly from the segmentation. We show that our method provides more accurate skeletons than the soft-skeleton algorithm. We then build upon this network a cascaded U-Net trained with the clDice loss to embed topological constraints during the segmentation. The resulting model is able to predict both the vessel segmentation and centerlines with a more accurate topology.
</details>
<details>
<summary>摘要</summary>
船 Segmentation 和中心线抽取是许多计算机支持诊断工具处理血管疾病的两项重要前置任务。现在，深度学习基于方法广泛应用于这两项任务。然而， classical deep-learning 方法很难捕捉血管网络的复杂 геометри和特有的 topologic，这在大多数应用中是非常重要的。为了解决这些限制，recently proposed clDice loss 强调血管中心线的 topological loss。这个损失函数需要计算，使用我们提议的 soft-skeleton 算法，真实的血管skeleton 和预测 segmentation 的skeleton。然而，soft-skeleton 算法在 3D 图像上提供了不佳的结果，使 clDice  hardly suitable  for 3D images。在本文中，我们提议将 soft-skeleton 算法 replaced  by a U-Net，从 segmentation 直接计算血管skeleton。我们示出了我们的方法可以提供更加准确的 skeletons。然后，我们在这个网络上建立了一个 cascaded U-Net，通过 clDice 损失函数 embedding topological constraints  During the segmentation。结果是一个能够预测血管 segmentation 和中心线的更加准确 topology。
</details></li>
</ul>
<hr>
<h2 id="CortexMorph-fast-cortical-thickness-estimation-via-diffeomorphic-registration-using-VoxelMorph"><a href="#CortexMorph-fast-cortical-thickness-estimation-via-diffeomorphic-registration-using-VoxelMorph" class="headerlink" title="CortexMorph: fast cortical thickness estimation via diffeomorphic registration using VoxelMorph"></a>CortexMorph: fast cortical thickness estimation via diffeomorphic registration using VoxelMorph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11567">http://arxiv.org/abs/2307.11567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Richard McKinley, Christian Rummel</li>
<li>for: 这个论文是用于描述一种新的 cortical thickness estimation 方法，即 CortexMorph，以及其与深度学习基于的 segmentation 模型的结合。</li>
<li>methods: 该方法使用了不监督的深度学习来直接预测 deformation field，以便用 DiReCT 方法计算 cortical thickness。</li>
<li>results: 研究表明，通过结合 CortexMorph 和深度学习基于的 segmentation 模型，可以在秒钟内从 T1 束缚图像中计算区域化 cortical thickness，同时保持检测 cortical atrophy 的能力。<details>
<summary>Abstract</summary>
The thickness of the cortical band is linked to various neurological and psychiatric conditions, and is often estimated through surface-based methods such as Freesurfer in MRI studies. The DiReCT method, which calculates cortical thickness using a diffeomorphic deformation of the gray-white matter interface towards the pial surface, offers an alternative to surface-based methods. Recent studies using a synthetic cortical thickness phantom have demonstrated that the combination of DiReCT and deep-learning-based segmentation is more sensitive to subvoxel cortical thinning than Freesurfer.   While anatomical segmentation of a T1-weighted image now takes seconds, existing implementations of DiReCT rely on iterative image registration methods which can take up to an hour per volume. On the other hand, learning-based deformable image registration methods like VoxelMorph have been shown to be faster than classical methods while improving registration accuracy. This paper proposes CortexMorph, a new method that employs unsupervised deep learning to directly regress the deformation field needed for DiReCT. By combining CortexMorph with a deep-learning-based segmentation model, it is possible to estimate region-wise thickness in seconds from a T1-weighted image, while maintaining the ability to detect cortical atrophy. We validate this claim on the OASIS-3 dataset and the synthetic cortical thickness phantom of Rusak et al.
</details>
<details>
<summary>摘要</summary>
cortical 带的厚度与多种神经内科和心理科学疾病相关，通常通过表面基本方法如Freesurfer在MRI研究中估算。DiReCT方法，它通过Diffomorphic deformation of gray-white matter interface towards the pial surface来计算 cortical thickness，为表面基本方法提供了一种alternative。最近的研究使用了Synthetic cortical thickness phantom表明，combined DiReCT and deep-learning-based segmentation more sensitive to subvoxel cortical thinning than Freesurfer。 whereas anatomical segmentation of a T1-weighted image now takes only seconds, existing implementations of DiReCT rely on iterative image registration methods that can take up to an hour per volume. On the other hand, learning-based deformable image registration methods like VoxelMorph have been shown to be faster than classical methods while improving registration accuracy. This paper proposes CortexMorph, a new method that employs unsupervised deep learning to directly regress the deformation field needed for DiReCT. By combining CortexMorph with a deep-learning-based segmentation model, it is possible to estimate region-wise thickness in seconds from a T1-weighted image, while maintaining the ability to detect cortical atrophy. We validate this claim on the OASIS-3 dataset and the synthetic cortical thickness phantom of Rusak et al.
</details></li>
</ul>
<hr>
<h2 id="FedAutoMRI-Federated-Neural-Architecture-Search-for-MR-Image-Reconstruction"><a href="#FedAutoMRI-Federated-Neural-Architecture-Search-for-MR-Image-Reconstruction" class="headerlink" title="FedAutoMRI: Federated Neural Architecture Search for MR Image Reconstruction"></a>FedAutoMRI: Federated Neural Architecture Search for MR Image Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11538">http://arxiv.org/abs/2307.11538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruoyou Wu, Cheng Li, Juan Zou, Shanshan Wang</li>
<li>for: 用于MR图像重建</li>
<li>methods: 使用分布式协同学习方法和不同数据分布稳定化方法</li>
<li>results: 实现了较好的表现，使用轻量级模型并且比经典联合学习方法具有更少的参数数量<details>
<summary>Abstract</summary>
Centralized training methods have shown promising results in MR image reconstruction, but privacy concerns arise when gathering data from multiple institutions. Federated learning, a distributed collaborative training scheme, can utilize multi-center data without the need to transfer data between institutions. However, existing federated learning MR image reconstruction methods rely on manually designed models which have extensive parameters and suffer from performance degradation when facing heterogeneous data distributions. To this end, this paper proposes a novel FederAted neUral archiTecture search approach fOr MR Image reconstruction (FedAutoMRI). The proposed method utilizes differentiable architecture search to automatically find the optimal network architecture. In addition, an exponential moving average method is introduced to improve the robustness of the client model to address the data heterogeneity issue. To the best of our knowledge, this is the first work to use federated neural architecture search for MR image reconstruction. Experimental results demonstrate that our proposed FedAutoMRI can achieve promising performances while utilizing a lightweight model with only a small number of model parameters compared to the classical federated learning methods.
</details>
<details>
<summary>摘要</summary>
中央化训练方法在MR图像重建中表现出了扎实的成果，但是收集数据从多家机构时，隐私问题就会出现。基于分布式合作训练的联邦学习（Federated Learning）可以利用多中心数据无需将数据传输到机构之间。然而，现有的联邦学习MR图像重建方法通常采用手动设计的模型，这些模型具有较多的参数，并且面临着数据不均衡问题时会导致性能下降。为此，这篇论文提出了一种新的FederAted neUral archiTecture search Approach（FedAutoMRI）。提议的方法使用可微分的建筑搜索自动找到最佳网络架构。此外，我们还引入了指数移动平均方法，以提高客户端模型的数据不均衡问题的Robustness。到目前为止，这是我们知道的第一篇使用联邦神经建筑搜索MR图像重建的论文。实验结果表明，我们的提议的FedAutoMRI可以实现扎实的性能，同时使用轻量级的模型，只有少量的模型参数，与传统的联邦学习方法相比，具有显著的优势。
</details></li>
</ul>
<hr>
<h2 id="UWAT-GAN-Fundus-Fluorescein-Angiography-Synthesis-via-Ultra-wide-angle-Transformation-Multi-scale-GAN"><a href="#UWAT-GAN-Fundus-Fluorescein-Angiography-Synthesis-via-Ultra-wide-angle-Transformation-Multi-scale-GAN" class="headerlink" title="UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-wide-angle Transformation Multi-scale GAN"></a>UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-wide-angle Transformation Multi-scale GAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11530">http://arxiv.org/abs/2307.11530</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Tinysqua/UWAT-GAN">https://github.com/Tinysqua/UWAT-GAN</a></li>
<li>paper_authors: Zhaojie Fang, Zhanghao Chen, Pengxue Wei, Wangting Li, Shaochong Zhang, Ahmed Elazab, Gangyong Jia, Ruiquan Ge, Changmiao Wang</li>
<li>for: 本研究旨在提出一种新的条件生成对抗网络（UWAT-GAN），用于从ultra-wide-angle fundus photography（UWF-SLO）中生成高分辨率的 fluorescein angiography（UWF-FA）图像。</li>
<li>methods: 该模型使用多尺度生成器和融合模块贮取更好地抽取全局和局部信息，并使用注意力传输模块帮助解码器学习。此外，使用多个新的权重损失函数在不同的数据尺度进行超参数化训练。</li>
<li>results: 实验结果表明，UWAT-GAN比现有方法有更高的图像质量和更好的抽象能力。code可以在GitHub上找到：<a target="_blank" rel="noopener" href="https://github.com/Tinysqua/UWAT-GAN%E3%80%82">https://github.com/Tinysqua/UWAT-GAN。</a><details>
<summary>Abstract</summary>
Fundus photography is an essential examination for clinical and differential diagnosis of fundus diseases. Recently, Ultra-Wide-angle Fundus (UWF) techniques, UWF Fluorescein Angiography (UWF-FA) and UWF Scanning Laser Ophthalmoscopy (UWF-SLO) have been gradually put into use. However, Fluorescein Angiography (FA) and UWF-FA require injecting sodium fluorescein which may have detrimental influences. To avoid negative impacts, cross-modality medical image generation algorithms have been proposed. Nevertheless, current methods in fundus imaging could not produce high-resolution images and are unable to capture tiny vascular lesion areas. This paper proposes a novel conditional generative adversarial network (UWAT-GAN) to synthesize UWF-FA from UWF-SLO. Using multi-scale generators and a fusion module patch to better extract global and local information, our model can generate high-resolution images. Moreover, an attention transmit module is proposed to help the decoder learn effectively. Besides, a supervised approach is used to train the network using multiple new weighted losses on different scales of data. Experiments on an in-house UWF image dataset demonstrate the superiority of the UWAT-GAN over the state-of-the-art methods. The source code is available at: https://github.com/Tinysqua/UWAT-GAN.
</details>
<details>
<summary>摘要</summary>
血液照片是诊断和区分疾病的基本检查方法。最近，ultra-wide-anglefundus（UWF）技术，UWFfluoresceinangiography（UWF-FA）和UWF扫描镜观察（UWF-SLO）逐渐普及。但是，fluoresceinangiography（FA）和UWF-FA需要注射Na fluorescein，可能有不良影响。为了避免这些影响，多modal医学影像生成算法已经被提出。然而，目前的基于基准图像的检查方法无法生成高分辨率图像，也无法捕捉微小血管损伤区域。本文提出了一种新的冲激生成随机网络（UWAT-GAN），用于从UWF-SLO中生成UWF-FA。通过多级生成器和融合模块贴图，我们的模型可以生成高分辨率图像。此外，我们还提出了一种注意力传输模块，帮助解码器更好地学习。此外，我们采用了多种新的质量权重损失来训练网络。实验结果表明，UWAT-GAN比现有的方法更高效。代码可以在GitHub上找到：https://github.com/Tinysqua/UWAT-GAN。
</details></li>
</ul>
<hr>
<h2 id="Bone-mineral-density-estimation-from-a-plain-X-ray-image-by-learning-decomposition-into-projections-of-bone-segmented-computed-tomography"><a href="#Bone-mineral-density-estimation-from-a-plain-X-ray-image-by-learning-decomposition-into-projections-of-bone-segmented-computed-tomography" class="headerlink" title="Bone mineral density estimation from a plain X-ray image by learning decomposition into projections of bone-segmented computed tomography"></a>Bone mineral density estimation from a plain X-ray image by learning decomposition into projections of bone-segmented computed tomography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11513">http://arxiv.org/abs/2307.11513</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Gu, Yoshito Otake, Keisuke Uemura, Mazen Soufi, Masaki Takao, Hugues Talbot, Seiji Okada, Nobuhiko Sugano, Yoshinobu Sato</li>
<li>For: The paper aims to estimate bone mineral density (BMD) from a plain X-ray image for opportunistic screening of osteoporosis.* Methods: The proposed method uses a novel approach that learns decomposition into projections of bone-segmented quantitative computed tomography (QCT) for BMD estimation under limited datasets.* Results: The proposed method achieved high accuracy in BMD estimation, with Pearson correlation coefficients of 0.880 and 0.920 observed for DXA-measured BMD and QCT-measured BMD estimation tasks, respectively. The root mean square of the coefficient of variation values were 3.27 to 3.79% for four measurements with different poses.<details>
<summary>Abstract</summary>
Osteoporosis is a prevalent bone disease that causes fractures in fragile bones, leading to a decline in daily living activities. Dual-energy X-ray absorptiometry (DXA) and quantitative computed tomography (QCT) are highly accurate for diagnosing osteoporosis; however, these modalities require special equipment and scan protocols. To frequently monitor bone health, low-cost, low-dose, and ubiquitously available diagnostic methods are highly anticipated. In this study, we aim to perform bone mineral density (BMD) estimation from a plain X-ray image for opportunistic screening, which is potentially useful for early diagnosis. Existing methods have used multi-stage approaches consisting of extraction of the region of interest and simple regression to estimate BMD, which require a large amount of training data. Therefore, we propose an efficient method that learns decomposition into projections of bone-segmented QCT for BMD estimation under limited datasets. The proposed method achieved high accuracy in BMD estimation, where Pearson correlation coefficients of 0.880 and 0.920 were observed for DXA-measured BMD and QCT-measured BMD estimation tasks, respectively, and the root mean square of the coefficient of variation values were 3.27 to 3.79% for four measurements with different poses. Furthermore, we conducted extensive validation experiments, including multi-pose, uncalibrated-CT, and compression experiments toward actual application in routine clinical practice.
</details>
<details>
<summary>摘要</summary>
骨质疾病（osteoporosis）是一种非常普遍的骨疾病，可以导致脆弱骨骼的折损，从而导致日常生活活动下降。双能X射线吸收测定（DXA）和量子计算Tomography（QCT）是骨质疾病的诊断非常准确的方法，但这些方法需要特殊的设备和扫描协议。为了经常监测骨健康，低成本、低剂量、 universally available的诊断方法很需求。在这个研究中，我们想使用普通X射线图像来估算骨质密度（BMD），以便于早期诊断。现有的方法通常使用多个阶段的方法，包括提取区域兴趣和简单的回归来估算BMD，这些方法需要大量的训练数据。因此，我们提出了一种高效的方法，该方法可以通过分解为骨 segmentation QCT 的投影来估算BMD，并在有限的数据集下进行学习。我们的方法实现了高精度的BMD 估算，其中 DXA 测量的BMD 和 QCT 测量的BMD 估算任务中的归一化相关系数为0.880和0.920，分解系数的平均方差为3.27%-3.79%。此外，我们进行了广泛的验证实验，包括多个姿势、不同扫描方式、压缩实验等，以便在实际临床医学中应用。
</details></li>
</ul>
<hr>
<h2 id="MatSpectNet-Material-Segmentation-Network-with-Domain-Aware-and-Physically-Constrained-Hyperspectral-Reconstruction"><a href="#MatSpectNet-Material-Segmentation-Network-with-Domain-Aware-and-Physically-Constrained-Hyperspectral-Reconstruction" class="headerlink" title="MatSpectNet: Material Segmentation Network with Domain-Aware and Physically-Constrained Hyperspectral Reconstruction"></a>MatSpectNet: Material Segmentation Network with Domain-Aware and Physically-Constrained Hyperspectral Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11466">http://arxiv.org/abs/2307.11466</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/heng-yuwen/matspectnet">https://github.com/heng-yuwen/matspectnet</a></li>
<li>paper_authors: Yuwen Heng, Yihong Wu, Jiawen Chen, Srinandan Dasmahapatra, Hansung Kim</li>
<li>for: 实现RGB图像中物质分 segmentation任务中的精准物质分类，由于场景中物质的外观变化很大，是一项挑战。</li>
<li>methods: 提出了一种新的模型——MatSpectNet，通过使用现有的RGB图像恢复 hyperspectral图像，并利用频谱恢复数据集中的频谱恢复能力进行适应化，以便在物质分类任务中提高物质分 segmentation的精度。</li>
<li>results: 对于LMD数据集和OpenSurfaces数据集，MatSpectNet实验表明，与最近一篇论文相比，MatSpectNet可以提高平均像素准确率1.60%，提高物种均准确率3.42%。<details>
<summary>Abstract</summary>
Achieving accurate material segmentation for 3-channel RGB images is challenging due to the considerable variation in a material's appearance. Hyperspectral images, which are sets of spectral measurements sampled at multiple wavelengths, theoretically offer distinct information for material identification, as variations in intensity of electromagnetic radiation reflected by a surface depend on the material composition of a scene. However, existing hyperspectral datasets are impoverished regarding the number of images and material categories for the dense material segmentation task, and collecting and annotating hyperspectral images with a spectral camera is prohibitively expensive. To address this, we propose a new model, the MatSpectNet to segment materials with recovered hyperspectral images from RGB images. The network leverages the principles of colour perception in modern cameras to constrain the reconstructed hyperspectral images and employs the domain adaptation method to generalise the hyperspectral reconstruction capability from a spectral recovery dataset to material segmentation datasets. The reconstructed hyperspectral images are further filtered using learned response curves and enhanced with human perception. The performance of MatSpectNet is evaluated on the LMD dataset as well as the OpenSurfaces dataset. Our experiments demonstrate that MatSpectNet attains a 1.60% increase in average pixel accuracy and a 3.42% improvement in mean class accuracy compared with the most recent publication. The project code is attached to the supplementary material and will be published on GitHub.
</details>
<details>
<summary>摘要</summary>
achieving accurate material segmentation for 3-channel RGB images is challenging due to the considerable variation in a material's appearance. Hyperspectral images, which are sets of spectral measurements sampled at multiple wavelengths, theoretically offer distinct information for material identification, as variations in intensity of electromagnetic radiation reflected by a surface depend on the material composition of a scene. However, existing hyperspectral datasets are impoverished regarding the number of images and material categories for the dense material segmentation task, and collecting and annotating hyperspectral images with a spectral camera is prohibitively expensive. To address this, we propose a new model, the MatSpectNet to segment materials with recovered hyperspectral images from RGB images. The network leverages the principles of color perception in modern cameras to constrain the reconstructed hyperspectral images and employs the domain adaptation method to generalize the hyperspectral reconstruction capability from a spectral recovery dataset to material segmentation datasets. The reconstructed hyperspectral images are further filtered using learned response curves and enhanced with human perception. The performance of MatSpectNet is evaluated on the LMD dataset as well as the OpenSurfaces dataset. Our experiments demonstrate that MatSpectNet attains a 1.60% increase in average pixel accuracy and a 3.42% improvement in mean class accuracy compared with the most recent publication. The project code is attached to the supplementary material and will be published on GitHub.Here's the translation in Traditional Chinese:achieving accurate material segmentation for 3-channel RGB images is challenging due to the considerable variation in a material's appearance. Hyperspectral images, which are sets of spectral measurements sampled at multiple wavelengths, theoretically offer distinct information for material identification, as variations in intensity of electromagnetic radiation reflected by a surface depend on the material composition of a scene. However, existing hyperspectral datasets are impoverished regarding the number of images and material categories for the dense material segmentation task, and collecting and annotating hyperspectral images with a spectral camera is prohibitively expensive. To address this, we propose a new model, the MatSpectNet to segment materials with recovered hyperspectral images from RGB images. The network leverages the principles of color perception in modern cameras to constrain the reconstructed hyperspectral images and employs the domain adaptation method to generalize the hyperspectral reconstruction capability from a spectral recovery dataset to material segmentation datasets. The reconstructed hyperspectral images are further filtered using learned response curves and enhanced with human perception. The performance of MatSpectNet is evaluated on the LMD dataset as well as the OpenSurfaces dataset. Our experiments demonstrate that MatSpectNet attains a 1.60% increase in average pixel accuracy and a 3.42% improvement in mean class accuracy compared with the most recent publication. The project code is attached to the supplementary material and will be published on GitHub.
</details></li>
</ul>
<hr>
<h2 id="BLISS-Interplanetary-Exploration-with-Swarms-of-Low-Cost-Spacecraft"><a href="#BLISS-Interplanetary-Exploration-with-Swarms-of-Low-Cost-Spacecraft" class="headerlink" title="BLISS: Interplanetary Exploration with Swarms of Low-Cost Spacecraft"></a>BLISS: Interplanetary Exploration with Swarms of Low-Cost Spacecraft</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11226">http://arxiv.org/abs/2307.11226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander N. Alvara, Lydia Lee, Emmanuel Sin, Nathan Lambert, Andrew J. Westphal, Kristofer S. J. Pister</li>
<li>for: 这个论文旨在探讨一种用微型技术实现的低成本、自动化的小天体飞船，用于快速、低成本的内太阳系探索。</li>
<li>methods: 这个论文使用了小型技术，包括微型电romechanical系统（MEMS）尺寸步进动作器和太阳帆，实现一种约10g的空间飞船。</li>
<li>results: 论文详细介绍了一种用于控制太阳帆的轨迹和低级 actuation控制，以及建议的载荷和计算机设计。 论文还 briefly Considered两个其他应用：从数十个金星家族彗星返回样本，以及遥感和拍摄遥行彗星。<details>
<summary>Abstract</summary>
Leveraging advancements in micro-scale technology, we propose a fleet of autonomous, low-cost, small solar sails for interplanetary exploration. The Berkeley Low-cost Interplanetary Solar Sail (BLISS) project aims to utilize small-scale technologies to create a fleet of tiny interplanetary femto-spacecraft for rapid, low-cost exploration of the inner solar system. This paper describes the hardware required to build a nearly 10 g spacecraft using a 1 m$^2$ solar sail steered by micro-electromechanical systems (MEMS) inchworm actuators. The trajectory control to a NEO, here 101955 Bennu, is detailed along with the low-level actuation control of the solar sail and the specifications of proposed onboard communication and computation. Two other applications are also shortly considered: sample return from dozens of Jupiter-family comets and interstellar comet rendezvous and imaging. The paper concludes by discussing the fundamental scaling limits and future directions for steerable autonomous miniature solar sails with onboard custom computers and sensors.
</details>
<details>
<summary>摘要</summary>
使用微型技术进行推动，我们提议一支自主、低成本、小型太阳帆船队进行 planetary exploration。 Berkeley Low-cost Interplanetary Solar Sail（BLISS）项目旨在利用小规模技术创造一支微型惯性空间飞船，用于快速、低成本地 explore 内太阳系。这篇文章描述了用于建立约10g空间飞船的硬件，包括1米²的太阳帆，由微型电子机械系统（MEMS）滚动器控制。文章还详细介绍了天体控制 trajectory 到 NEO 101955 Bennu，以及太阳帆的低级控制和船载通信和计算机的规格。此外，文章还 briefly 讨论了从数十个金星家族彗星返回样本和遥感柯梅丝 rendezvous 和拍摄。文章 conclude 了可控推进器的基本扩展限制和未来方向，包括自适应驱动器和船载特定计算机和感测器。
</details></li>
</ul>
<hr>
<h2 id="Sparse-electrophysiological-source-imaging-predicts-aging-related-gait-speed-slowing"><a href="#Sparse-electrophysiological-source-imaging-predicts-aging-related-gait-speed-slowing" class="headerlink" title="Sparse electrophysiological source imaging predicts aging-related gait speed slowing"></a>Sparse electrophysiological source imaging predicts aging-related gait speed slowing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11273">http://arxiv.org/abs/2307.11273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vega-Hernandez, Mayrim, Galan-Garcia, Lidice, Perez-Hidalgo-Gato, Jhoanna, Ontivero-Ortega, Marlis, Garcia-Agustin, Daysi, Garcia-Reyes, Ronaldo, Bosch-Bayard, Jorge, Marinazzo, Daniele, Martinez-Montes, Eduardo, Valdes-Sosa, Pedro A</li>
<li>For: The paper aims to identify stable Electrophysiological Source Imaging (ESI) biomarkers associated with Gait Speed (GS) as a measure of functional decline in aging individuals.* Methods: The authors use a combination of flexible sparse&#x2F;smooth&#x2F;non-negative models (NN-SLASSO) and the Stable Sparse Classifier method to estimate ESI and select relevant features, including activation ESI (aESI) and connectivity ESI (cESI) features.* Results: The authors found that novel sparse aESI models outperformed traditional methods, and that combining aESI and cESI features improved the predictability of GS changes. The selected biomarkers were localized to orbitofrontal and temporal cortical regions.Here’s the Chinese translation of the three points:* For: 这篇论文目标是找到年轻人函散速度 (GS) 的稳定电physiological Source Imaging (ESI) 标记器。* Methods: 作者使用了一种组合 flexible sparse&#x2F;smooth&#x2F;non-negative models (NN-SLASSO) 和稳定粗粒分类方法来估算 ESI 和选择相关特征，包括 activation ESI (aESI) 和 connectivity ESI (cESI) 特征。* Results: 作者发现了一种新的简单 aESI 模型，并且将 aESI 和 cESI 特征组合可以提高 GS 变化的预测性。选择的标记器被localized到 orbitofrontal 和 temporal cortical region。<details>
<summary>Abstract</summary>
Objective: We seek stable Electrophysiological Source Imaging (ESI) biomarkers associated with Gait Speed (GS) as a measure of functional decline. Towards this end we determine the predictive value of ESI activation and connectivity patterns of resting-state EEG Theta rhythm on physical performance decline measured by a slowing GS in aging individuals. Methods: As potential biomarkers related to GS changes, we estimate ESI using flexible sparse/smooth/non-negative models (NN-SLASSO), from which activation ESI (aESI) and connectivity ESI (cESI) features are selected using the Stable Sparse Classifier method. Results and Conclusions: Novel sparse aESI models outperformed traditional methods such as the LORETA family. The models combining aESI and cESI features improved the predictability of GS changes. Selected biomarkers from activation/connectivity patterns were localized to orbitofrontal and temporal cortical regions. Significance: The proposed methodology contributes to understanding the activation and connectivity of ESI complex patterns related to GS, providing potential biomarker features for GS slowing. Given the known relationship between GS decline and cognitive impairment, this preliminary work suggests it might be applied to other, more complex measures of healthy and pathological aging. Importantly, it might allow an ESI-based evaluation of rehabilitation programs.
</details>
<details>
<summary>摘要</summary>
Methods: 作为可能的生物 marker相关于 GS 变化的方法，我们使用了 flexible sparse/smooth/non-negative 模型（NN-SLASSO），从中可以选择 activation ESI（aESI）和 connectivity ESI（cESI）特征，使用稳定的组合方法。Results and Conclusions: 我们发现了 Novel sparse aESI 模型，比 traditional LORETA 家族的方法更好。将 activation ESI 和 connectivity ESI 特征结合起来，可以提高 GS 变化的预测性。选择的生物 marker从 activation/connectivity 模式中的localized 到 orbitofrontal 和 temporal cortical region。Significance: 我们的方法可以帮助理解ESI复杂的模式之间的活动和连接相互关联，提供了可能的生物 marker特征，用于评估GS slowing。由于GS decline 和认知障碍之间的已知关系，这些早期的工作可能适用于其他更复杂的健康和疾病年龄。更重要的是，这种方法可能可以用于评估复健计划。
</details></li>
</ul>
<hr>
<h2 id="Treatment-And-Follow-Up-Guidelines-For-Multiple-Brain-Metastases-A-Systematic-Review"><a href="#Treatment-And-Follow-Up-Guidelines-For-Multiple-Brain-Metastases-A-Systematic-Review" class="headerlink" title="Treatment And Follow-Up Guidelines For Multiple Brain Metastases: A Systematic Review"></a>Treatment And Follow-Up Guidelines For Multiple Brain Metastases: A Systematic Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11016">http://arxiv.org/abs/2307.11016</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ana Sofia Santos, Victor Alves, José Soares, Matheus Silva, Crystian Saraiva</li>
<li>for: 这篇研究是为了探讨多个脑部метаstatic的管理方法，以提高病人生活质量和神经保存。</li>
<li>methods: 这篇研究使用了STereotactic radiosurgery (SRS)来管理多个脑部metastatic，并且使用了人工智能模型来预测后治疗图像中的新出现脑部metastatic。</li>
<li>results: 研究发现这种方法可以帮助医疗专业人员早期决定最佳治疗方案，并且可以提高病人的生活质量和神经保存。<details>
<summary>Abstract</summary>
Brain metastases are a complication of primary cancer, representing the most common type of brain tumor in adults. The management of multiple brain metastases represents a clinical challenge worldwide in finding the optimal treatment for patients considering various individual aspects. Managing multiple metastases with stereotactic radiosurgery (SRS) is being increasingly used because of quality of life and neurocognitive preservation, which do not present such good outcomes when dealt with whole brain radiation therapy (WBRT). After treatment, analyzing the progression of the disease still represents a clinical issue, since it is difficult to determine a standard schedule for image acquisition. A solution could be the applying artificial intelligence, namely predictive models to forecast the incidence of new metastases in post-treatment images. Although there aren't many works on this subject, this could potentially bennefit medical professionals in early decision of the best treatment approaches.
</details>
<details>
<summary>摘要</summary>
主要癌症的脑部 метаста团是一种常见的脑肿瘤，特别是在成人中。管理多个脑部 метаста团的治疗呈现出了全球的临床挑战，在考虑各个个体特点时寻找优化的治疗方案。使用STereotactic radiosurgery（SRS）来管理多个脑部 метаста团，因为它可以保持生活质量和神经功能，而不是整个脑部放射疗法（WBRT）所能提供的较差的结果。然而，在 после治疗时，评估疾病的进程仍然是临床问题，因为困难确定标准的图像获取时间间隔。一种可能的解决方案是通过应用人工智能，即预测模型，预测在后治疗图像中新形成的脑部 метаста团的发生率。虽然没有很多相关研究，但这可能可以帮助医疗专业人员在早期决定最佳治疗方案。
</details></li>
</ul>
<hr>
<h2 id="Frequency-aware-optical-coherence-tomography-image-super-resolution-via-conditional-generative-adversarial-neural-network"><a href="#Frequency-aware-optical-coherence-tomography-image-super-resolution-via-conditional-generative-adversarial-neural-network" class="headerlink" title="Frequency-aware optical coherence tomography image super-resolution via conditional generative adversarial neural network"></a>Frequency-aware optical coherence tomography image super-resolution via conditional generative adversarial neural network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11130">http://arxiv.org/abs/2307.11130</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueshen Li, Zhenxing Dong, Hongshan Liu, Jennifer J. Kang-Mieler, Yuye Ling, Yu Gan</li>
<li>for: 提高镜像医学图像诊断和治疗的能力，特别是心血管和眼科领域。</li>
<li>methods:  integrate three critical frequency-based modules (i.e., frequency transformation, frequency skip connection, and frequency alignment) and frequency-based loss function into a conditional generative adversarial network (cGAN)。</li>
<li>results: 在现有的 coronary OCT 数据集上进行了大规模的量化研究，证明了我们提出的框架在现有的深度学习框架之上具有超过性。此外，我们还在鱼眼层图像和兔脑层图像上应用了我们的框架，证明了它的 universality。<details>
<summary>Abstract</summary>
Optical coherence tomography (OCT) has stimulated a wide range of medical image-based diagnosis and treatment in fields such as cardiology and ophthalmology. Such applications can be further facilitated by deep learning-based super-resolution technology, which improves the capability of resolving morphological structures. However, existing deep learning-based method only focuses on spatial distribution and disregard frequency fidelity in image reconstruction, leading to a frequency bias. To overcome this limitation, we propose a frequency-aware super-resolution framework that integrates three critical frequency-based modules (i.e., frequency transformation, frequency skip connection, and frequency alignment) and frequency-based loss function into a conditional generative adversarial network (cGAN). We conducted a large-scale quantitative study from an existing coronary OCT dataset to demonstrate the superiority of our proposed framework over existing deep learning frameworks. In addition, we confirmed the generalizability of our framework by applying it to fish corneal images and rat retinal images, demonstrating its capability to super-resolve morphological details in eye imaging.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Deep-Spiking-UNet-for-Image-Processing"><a href="#Deep-Spiking-UNet-for-Image-Processing" class="headerlink" title="Deep Spiking-UNet for Image Processing"></a>Deep Spiking-UNet for Image Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10974">http://arxiv.org/abs/2307.10974</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/snnresearch/spiking-unet">https://github.com/snnresearch/spiking-unet</a></li>
<li>paper_authors: Hebei Li, Yueyi Zhang, Zhiwei Xiong, Zheng-jun Zha, Xiaoyan Sun</li>
<li>for: 这paper的目的是提出一种基于神经元活动的图像处理方法，使用SNNs和U-Net结构。</li>
<li>methods: 这paper使用了多reshold spiking neurons来提高信息传递效率，并采用了一种转换和精度调整管道来使用预训练的U-Net模型。</li>
<li>results: 实验结果表明，对于图像分割和净化任务，我们的Spiking-UNet可以与非神经元网络相比，并且超过现有的SNN方法。与未调整的Spiking-UNet相比，我们的Spiking-UNet可以降低推理时间约90%。<details>
<summary>Abstract</summary>
U-Net, known for its simple yet efficient architecture, is widely utilized for image processing tasks and is particularly suitable for deployment on neuromorphic chips. This paper introduces the novel concept of Spiking-UNet for image processing, which combines the power of Spiking Neural Networks (SNNs) with the U-Net architecture. To achieve an efficient Spiking-UNet, we face two primary challenges: ensuring high-fidelity information propagation through the network via spikes and formulating an effective training strategy. To address the issue of information loss, we introduce multi-threshold spiking neurons, which improve the efficiency of information transmission within the Spiking-UNet. For the training strategy, we adopt a conversion and fine-tuning pipeline that leverage pre-trained U-Net models. During the conversion process, significant variability in data distribution across different parts is observed when utilizing skip connections. Therefore, we propose a connection-wise normalization method to prevent inaccurate firing rates. Furthermore, we adopt a flow-based training method to fine-tune the converted models, reducing time steps while preserving performance. Experimental results show that, on image segmentation and denoising, our Spiking-UNet achieves comparable performance to its non-spiking counterpart, surpassing existing SNN methods. Compared with the converted Spiking-UNet without fine-tuning, our Spiking-UNet reduces inference time by approximately 90\%. This research broadens the application scope of SNNs in image processing and is expected to inspire further exploration in the field of neuromorphic engineering. The code for our Spiking-UNet implementation is available at https://github.com/SNNresearch/Spiking-UNet.
</details>
<details>
<summary>摘要</summary>
优等网络（U-Net），因其简单而高效的架构，广泛应用于图像处理任务，特别适合部署在神经omorphic芯片上。这篇论文提出了一种新的启发式神经网络（SNN）图像处理方法，其结合了U-Net架构和启发式神经网络（SNN）的优势。为确保高精度信息传递，我们面临两个主要挑战：保证信息传递的高精度和制定有效的训练策略。为Address the issue of information loss, we introduce multi-threshold spiking neurons, which improve the efficiency of information transmission within the Spiking-UNet. For the training strategy, we adopt a conversion and fine-tuning pipeline that leverage pre-trained U-Net models. During the conversion process, we observe significant variability in data distribution across different parts when utilizing skip connections. Therefore, we propose a connection-wise normalization method to prevent inaccurate firing rates. Furthermore, we adopt a flow-based training method to fine-tune the converted models, reducing time steps while preserving performance. Experimental results show that, on image segmentation and denoising, our Spiking-UNet achieves comparable performance to its non-spiking counterpart, surpassing existing SNN methods. Compared with the converted Spiking-UNet without fine-tuning, our Spiking-UNet reduces inference time by approximately 90\%. This research broadens the application scope of SNNs in image processing and is expected to inspire further exploration in the field of neuromorphic engineering. The code for our Spiking-UNet implementation is available at <https://github.com/SNNresearch/Spiking-UNet>.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/21/eess.IV_2023_07_21/" data-id="clp89domx017gi7884cy211ka" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/82/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/81/">81</a><a class="page-number" href="/page/82/">82</a><span class="page-number current">83</span><a class="page-number" href="/page/84/">84</a><a class="page-number" href="/page/85/">85</a><span class="space">&hellip;</span><a class="page-number" href="/page/97/">97</a><a class="extend next" rel="next" href="/page/84/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">66</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">81</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">140</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
