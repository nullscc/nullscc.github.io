
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/83/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.CL_2023_07_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/10/cs.CL_2023_07_10/" class="article-date">
  <time datetime="2023-07-10T11:00:00.000Z" itemprop="datePublished">2023-07-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/10/cs.CL_2023_07_10/">cs.CL - 2023-07-10</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="BeaverTails-Towards-Improved-Safety-Alignment-of-LLM-via-a-Human-Preference-Dataset"><a href="#BeaverTails-Towards-Improved-Safety-Alignment-of-LLM-via-a-Human-Preference-Dataset" class="headerlink" title="BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset"></a>BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04657">http://arxiv.org/abs/2307.04657</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Chi Zhang, Ruiyang Sun, Yizhou Wang, Yaodong Yang</li>
<li>for: 这个论文旨在鼓励大语言模型（LLM）的安全对齐研究，提供了一个名为BeaverTails的数据集。</li>
<li>methods: 这个数据集独特地将问答对的注释分为帮助性和无害性两个方面，因此可以为这两个关键特征提供多个视角。</li>
<li>results: 作者们编译了30,207个问答对的安全元标和30,144个专家对比数据，并在内容审核和人工反馈学习（RLHF）中应用了BeaverTails，证明了其在LLM的实际安全措施方面的潜在价值。<details>
<summary>Abstract</summary>
In this paper, we introduce the BeaverTails dataset, aimed at fostering research on safety alignment in large language models (LLMs). This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes. In total, we have compiled safety meta-labels for 30,207 question-answer (QA) pairs and gathered 30,144 pairs of expert comparison data for both the helpfulness and harmlessness metrics. We further showcase applications of BeaverTails in content moderation and reinforcement learning with human feedback (RLHF), emphasizing its potential for practical safety measures in LLMs. We believe this dataset provides vital resources for the community, contributing towards the safe development and deployment of LLMs. Our project page is available at the following URL: https://sites.google.com/view/pku-beavertails.
</details>
<details>
<summary>摘要</summary>
在本文中，我们介绍了BeaverTails数据集，用于推动大语言模型（LLM）的安全对齐研究。这个数据集独特地将问答对的有用性和无害性标注分开，因此可以提供不同的视角。总共，我们对30,207个问答对进行了安全元标注，并收集了30,144对专家比较数据，以便在有用性和无害性metric上进行评估。我们还展示了在内容审核和人工回馈学习（RLHF）中使用BeaverTails的应用，强调其在LLM的安全实施中的潜在价值。我们认为这个数据集为研究者提供了重要的资源，帮助开发和部署LLM的安全。项目页面的URL为：https://sites.google.com/view/pku-beavertails。
</details></li>
</ul>
<hr>
<h2 id="Measuring-Lexical-Diversity-in-Texts-The-Twofold-Length-Problem"><a href="#Measuring-Lexical-Diversity-in-Texts-The-Twofold-Length-Problem" class="headerlink" title="Measuring Lexical Diversity in Texts: The Twofold Length Problem"></a>Measuring Lexical Diversity in Texts: The Twofold Length Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04626">http://arxiv.org/abs/2307.04626</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yves Bestgen</li>
<li>for: 评估语言学习者文本 lexical diversity 的方法和问题</li>
<li>methods: 使用probabilistic或algorithmic方法来缩短文本长度，但都未能解决第二个问题：敏感度参数的影响</li>
<li>results: 三个英语语言学习者文本 dataset 的分析显示，使用这些方法可以解决长度问题，但都未能解决第二个问题<details>
<summary>Abstract</summary>
The impact of text length on the estimation of lexical diversity has captured the attention of the scientific community for more than a century. Numerous indices have been proposed, and many studies have been conducted to evaluate them, but the problem remains. This methodological review provides a critical analysis not only of the most commonly used indices in language learning studies, but also of the length problem itself, as well as of the methodology for evaluating the proposed solutions. The analysis of three datasets of English language-learners' texts revealed that indices that reduce all texts to the same length using a probabilistic or an algorithmic approach solve the length dependency problem; however, all these indices failed to address the second problem, which is their sensitivity to the parameter that determines the length to which the texts are reduced. The paper concludes with recommendations for optimizing lexical diversity analysis.
</details>
<details>
<summary>摘要</summary>
Text length的影响对语言多样性的估计已经在科学社区内引起了超过一个世纪的关注。许多指标已经被提出，但问题仍然存在。本方法评论提供了不只是语言学研究中最常用的指标的重要分析，还包括长度问题本身以及评估提出的解决方案的方法学分析。分析三个英语学习者的文本数据表明，使用概率或算法方法减小所有文本到同一个长度可以解决长度依赖问题，但所有这些指标都无法解决第二个问题，即它们对参数的敏感性。文章结束于优化语言多样性分析的建议。
</details></li>
</ul>
<hr>
<h2 id="On-the-Computational-Modeling-of-Meaning-Embodied-Cognition-Intertwined-with-Emotion"><a href="#On-the-Computational-Modeling-of-Meaning-Embodied-Cognition-Intertwined-with-Emotion" class="headerlink" title="On the Computational Modeling of Meaning: Embodied Cognition Intertwined with Emotion"></a>On the Computational Modeling of Meaning: Embodied Cognition Intertwined with Emotion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04518">http://arxiv.org/abs/2307.04518</a></li>
<li>repo_url: None</li>
<li>paper_authors: Casey Kennington</li>
<li>for: 本文探讨了语言意义如何形成，尤其关注儿童语言学习。</li>
<li>methods: 本文使用了历史风格，汇总了作者在不同时间发现的想法，并描述了儿童语言学习的设置、身体知识和情感知识的重要性。</li>
<li>results: 本文提出了语言学习机器人需要满足的一些要求，以及对未来语言模型的建议。<details>
<summary>Abstract</summary>
This document chronicles this author's attempt to explore how words come to mean what they do, with a particular focus on child language acquisition and what that means for models of language understanding.\footnote{I say \emph{historical} because I synthesize the ideas based on when I discovered them and how those ideas influenced my later thinking.} I explain the setting for child language learning, how embodiment -- being able to perceive and enact in the world, including knowledge of concrete and abstract concepts -- is crucial, and how emotion and cognition relate to each other and the language learning process. I end with what I think are some of the requirements for a language-learning agent that learns language in a setting similar to that of children. This paper can act as a potential guide for ongoing and future work in modeling language.
</details>
<details>
<summary>摘要</summary>
这份文档记录作者对语言意义的探索，尤其是儿童语言学习的过程。作者提出了一些关于语言理解的模型，并在文档中解释了儿童语言学习的背景和embodiment的重要性。文档还讨论了情感和认知之间的关系，以及语言学习过程中的感知和行为。最后，作者提出了一些对于模拟语言学习的agent来说的需求。这份文档可以作为未来语言模型研究的指南。
</details></li>
</ul>
<hr>
<h2 id="Detecting-LLM-Generated-Text-in-Computing-Education-A-Comparative-Study-for-ChatGPT-Cases"><a href="#Detecting-LLM-Generated-Text-in-Computing-Education-A-Comparative-Study-for-ChatGPT-Cases" class="headerlink" title="Detecting LLM-Generated Text in Computing Education: A Comparative Study for ChatGPT Cases"></a>Detecting LLM-Generated Text in Computing Education: A Comparative Study for ChatGPT Cases</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07411">http://arxiv.org/abs/2307.07411</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Sheinman Orenstrakh, Oscar Karnalim, Carlos Anibal Suarez, Michael Liut</li>
<li>for: This paper aims to evaluate the effectiveness of eight publicly-available LLM-generated text detectors in detecting LLM-generated text in computer science submissions.</li>
<li>methods: The authors collected 124 submissions from computer science students and generated 40 ChatGPT submissions to evaluate the eight LLM-generated text detectors using accuracy, false positives, and resilience measures.</li>
<li>results: The results show that CopyLeaks is the most accurate LLM-generated text detector, GPTKit is the best LLM-generated text detector to reduce false positives, and GLTR is the most resilient LLM-generated text detector. However, the authors also note that all LLM-generated text detectors are less accurate with code, other languages, and after the use of paraphrasing tools.Here’s the same information in Simplified Chinese:</li>
<li>for: 这个论文目的是评估现有的8种公共可用LLM生成文本检测器在计算机科学提交中检测LLM生成文本的有效性。</li>
<li>methods: 作者收集了124名计算机科学学生的提交和生成40个ChatGPT提交来评估8种LLM生成文本检测器，使用准确率、假阳性和抗耗力三个指标评估。</li>
<li>results: 结果显示，CopyLeaks是LLM生成文本检测器中最准确的，GPTKit可以减少假阳性，而GLTR是最有抗耗力的LLM生成文本检测器。然而，作者还注意到，所有LLM生成文本检测器在代码、其他语言和使用篇章工具（如QuillBot）后都有减少准确性的问题。<details>
<summary>Abstract</summary>
Due to the recent improvements and wide availability of Large Language Models (LLMs), they have posed a serious threat to academic integrity in education. Modern LLM-generated text detectors attempt to combat the problem by offering educators with services to assess whether some text is LLM-generated. In this work, we have collected 124 submissions from computer science students before the creation of ChatGPT. We then generated 40 ChatGPT submissions. We used this data to evaluate eight publicly-available LLM-generated text detectors through the measures of accuracy, false positives, and resilience. The purpose of this work is to inform the community of what LLM-generated text detectors work and which do not, but also to provide insights for educators to better maintain academic integrity in their courses. Our results find that CopyLeaks is the most accurate LLM-generated text detector, GPTKit is the best LLM-generated text detector to reduce false positives, and GLTR is the most resilient LLM-generated text detector. We also express concerns over 52 false positives (of 114 human written submissions) generated by GPTZero. Finally, we note that all LLM-generated text detectors are less accurate with code, other languages (aside from English), and after the use of paraphrasing tools (like QuillBot). Modern detectors are still in need of improvements so that they can offer a full-proof solution to help maintain academic integrity. Further, their usability can be improved by facilitating a smooth API integration, providing clear documentation of their features and the understandability of their model(s), and supporting more commonly used languages.
</details>
<details>
<summary>摘要</summary>
因为最近的大语言模型（LLM）的改进和普遍可用性，它们对教育的学术 integrity 造成了严重的威胁。现代 LLM 生成文本检测器尝试通过为教师提供检测 LLM 生成文本的服务，以确保学术 integrity 的维护。在这项工作中，我们收集了 124 篇计算机科学学生的作业，然后生成了 40 篇 ChatGPT 作业。我们使用这些数据来评估 eight 个公共可用的 LLM 生成文本检测器，通过准确率、假阳性和抗耗能力三个指标进行评估。本研究的目的是通过检测器的评估，了解哪些 LLM 生成文本检测器效果好、哪些需要改进，以便为教育行业提供更好的学术 integrity 维护方案。我们的结果显示，CopyLeaks 是最准确的 LLM 生成文本检测器，GPTKit 是减少假阳性的最佳选择，GLTR 是最有抗耗能力的 LLM 生成文本检测器。此外，我们还发现 GPTZero 对 114 篇人工写作中的 52 个假阳性存在问题。最后，我们注意到所有 LLM 生成文本检测器都对代码、其他语言（除英语外）和使用副作业工具（如 QuillBot）后的文本准确率较低。现代检测器仍需进一步改进，以提供不可攻击的解决方案，并且可以提高使用者体验，例如通过简单的 API 集成、清晰的功能和模型文档、以及支持更常用的语言。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Biomedical-Text-Summarization-and-Question-Answering-On-the-Utility-of-Domain-Specific-Pre-Training"><a href="#Enhancing-Biomedical-Text-Summarization-and-Question-Answering-On-the-Utility-of-Domain-Specific-Pre-Training" class="headerlink" title="Enhancing Biomedical Text Summarization and Question-Answering: On the Utility of Domain-Specific Pre-Training"></a>Enhancing Biomedical Text Summarization and Question-Answering: On the Utility of Domain-Specific Pre-Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04412">http://arxiv.org/abs/2307.04412</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dima Galat, Marian-Andrei Rizoiu</li>
<li>for: 这个论文旨在解决生物医学摘要 generation 需要大量数据训练的问题。</li>
<li>methods: 论文使用了转移学习，并证明了在 BioASQ 摘要任务中，不一定需要域专的预训练。作者提出了一种适合模型体系，并在这种模型体系下进行了任务特定细化，从而提出了一种三步细化方法，只需要千个域内示例即可。</li>
<li>results: 研究结果表明，没有域专预训练的大语言模型在某些域pecific生物医学文本生成任务中可以具有显著优势。<details>
<summary>Abstract</summary>
Biomedical summarization requires large datasets to train for text generation. We show that while transfer learning offers a viable option for addressing this challenge, an in-domain pre-training does not always offer advantages in a BioASQ summarization task. We identify a suitable model architecture and use it to show a benefit of a general-domain pre-training followed by a task-specific fine-tuning in the context of a BioASQ summarization task, leading to a novel three-step fine-tuning approach that works with only a thousand in-domain examples. Our results indicate that a Large Language Model without domain-specific pre-training can have a significant edge in some domain-specific biomedical text generation tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="TIM-Teaching-Large-Language-Models-to-Translate-with-Comparison"><a href="#TIM-Teaching-Large-Language-Models-to-Translate-with-Comparison" class="headerlink" title="TIM: Teaching Large Language Models to Translate with Comparison"></a>TIM: Teaching Large Language Models to Translate with Comparison</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04408">http://arxiv.org/abs/2307.04408</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lemon0830/tim">https://github.com/lemon0830/tim</a></li>
<li>paper_authors: Jiali Zeng, Fandong Meng, Yongjing Yin, Jie Zhou</li>
<li>for: 提高大型自然语言模型（LLM）在翻译任务中的表现</li>
<li>methods: 使用比较例子来教育LLM学习翻译</li>
<li>results: 比较例子学习翻译的方法可以超越现有的方法，并提高LLM在翻译任务中的表现<details>
<summary>Abstract</summary>
Open-sourced large language models (LLMs) have demonstrated remarkable efficacy in various tasks with instruction tuning. However, these models can sometimes struggle with tasks that require more specialized knowledge such as translation. One possible reason for such deficiency is that instruction tuning aims to generate fluent and coherent text that continues from a given instruction without being constrained by any task-specific requirements. Moreover, it can be more challenging for tuning smaller LLMs with lower-quality training data. To address this issue, we propose a novel framework using examples in comparison to teach LLMs to learn translation. Our approach involves presenting the model with examples of correct and incorrect translations and using a preference loss to guide the model's learning. We evaluate our method on WMT2022 test sets and show that it outperforms existing methods. Our findings offer a new perspective on fine-tuning LLMs for translation tasks and provide a promising solution for generating high-quality translations. Please refer to Github for more details: https://github.com/lemon0830/TIM.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Enhancing-Cross-lingual-Transfer-via-Phonemic-Transcription-Integration"><a href="#Enhancing-Cross-lingual-Transfer-via-Phonemic-Transcription-Integration" class="headerlink" title="Enhancing Cross-lingual Transfer via Phonemic Transcription Integration"></a>Enhancing Cross-lingual Transfer via Phonemic Transcription Integration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04361">http://arxiv.org/abs/2307.04361</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nhhoang96/phonemic_xlingual">https://github.com/nhhoang96/phonemic_xlingual</a></li>
<li>paper_authors: Hoang H. Nguyen, Chenwei Zhang, Tao Zhang, Eugene Rohrbaugh, Philip S. Yu</li>
<li>for: 本研究旨在提高cross-lingual transfer的效果，特别是为了 bridging the gap among Chinese-Japanese-Korean-Vietnamese (CJKV) languages.</li>
<li>methods: 本研究使用了一个叫PhoneXL的框架，它将phonemic transcriptions作为cross-lingual transfer中的一个额外语言特征，以便优化cross-lingual transfer。PhoneXL使用了不同的对Alignment objectives，包括本地一对一对 alignment、多模态上下文对 alignment以及多语言上下文对 alignment。</li>
<li>results: 本研究的试验结果表明，PhoneXL可以提高cross-lingual transfer的效果，特别是在CJKV语言之间。在Named Entity Recognition和Part-of-Speech Tagging两个token-level任务上，PhoneXL可以实现了Consistent improvements over orthographic-based multilingual PLMs。<details>
<summary>Abstract</summary>
Previous cross-lingual transfer methods are restricted to orthographic representation learning via textual scripts. This limitation hampers cross-lingual transfer and is biased towards languages sharing similar well-known scripts. To alleviate the gap between languages from different writing scripts, we propose PhoneXL, a framework incorporating phonemic transcriptions as an additional linguistic modality beyond the traditional orthographic transcriptions for cross-lingual transfer. Particularly, we propose unsupervised alignment objectives to capture (1) local one-to-one alignment between the two different modalities, (2) alignment via multi-modality contexts to leverage information from additional modalities, and (3) alignment via multilingual contexts where additional bilingual dictionaries are incorporated. We also release the first phonemic-orthographic alignment dataset on two token-level tasks (Named Entity Recognition and Part-of-Speech Tagging) among the understudied but interconnected Chinese-Japanese-Korean-Vietnamese (CJKV) languages. Our pilot study reveals phonemic transcription provides essential information beyond the orthography to enhance cross-lingual transfer and bridge the gap among CJKV languages, leading to consistent improvements on cross-lingual token-level tasks over orthographic-based multilingual PLMs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Local one-to-one alignment between the two modalities2. Alignment via multi-modality contexts to leverage information from additional modalities3. Alignment via multilingual contexts using bilingual dictionariesWe also release the first phonemic-orthographic alignment dataset on two token-level tasks (Named Entity Recognition and Part-of-Speech Tagging) among the understudied but interconnected Chinese-Japanese-Korean-Vietnamese (CJKV) languages. Our pilot study shows that phonemic transcriptions provide essential information beyond the orthography to enhance cross-lingual transfer and bridge the gap among CJKV languages, leading to consistent improvements on cross-lingual token-level tasks over orthographic-based multilingual PLMs.Translation notes:* “orthographic representation” is translated as “文字表示” (wén zì biǎo yì)* “phonemic transcriptions” is translated as “phoneme  транскрипции” (fōnēm yīn xiǎng)* “alignment” is translated as “对应” (duì yì)* “modality” is translated as “modalities” is translated as “语言 modalities” (yǔ yán modalities)* “token-level tasks” is translated as “токен级任务” (tuō kēn jīn yè)* “multilingual PLMs” is translated as “多语言 PLMs” (duō yǔ yán PLMs)</details></li>
</ol>
<hr>
<h2 id="Event-Extraction-as-Question-Generation-and-Answering"><a href="#Event-Extraction-as-Question-Generation-and-Answering" class="headerlink" title="Event Extraction as Question Generation and Answering"></a>Event Extraction as Question Generation and Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05567">http://arxiv.org/abs/2307.05567</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dataminr-ai/event-extraction-as-question-generation-and-answering">https://github.com/dataminr-ai/event-extraction-as-question-generation-and-answering</a></li>
<li>paper_authors: Di Lu, Shihao Ran, Joel Tetreault, Alejandro Jaimes</li>
<li>for: 这篇论文的目的是提出一种基于问答模型的事件抽取方法，以提高事件抽取的准确率和效率。</li>
<li>methods: 这篇论文使用了问题生成模型（QG）和动态模板来生成更加具有Contextual information的问题，从而提高事件抽取的准确率。</li>
<li>results: 实验表明， compared to prior single-task-based models, QGA-EE在ACE05英语 dataset上的表现更高， indicating that the proposed method can effectively improve the accuracy and efficiency of event extraction.<details>
<summary>Abstract</summary>
Recent work on Event Extraction has reframed the task as Question Answering (QA), with promising results. The advantage of this approach is that it addresses the error propagation issue found in traditional token-based classification approaches by directly predicting event arguments without extracting candidates first. However, the questions are typically based on fixed templates and they rarely leverage contextual information such as relevant arguments. In addition, prior QA-based approaches have difficulty handling cases where there are multiple arguments for the same role. In this paper, we propose QGA-EE, which enables a Question Generation (QG) model to generate questions that incorporate rich contextual information instead of using fixed templates. We also propose dynamic templates to assist the training of QG model. Experiments show that QGA-EE outperforms all prior single-task-based models on the ACE05 English dataset.
</details>
<details>
<summary>摘要</summary>
最近的Event Extraction研究已经将任务重新定义为问题回答（QA），并取得了良好的结果。这种方法可以直接预测事件参数而不是首先提取候选人选。然而，问题通常基于固定模板，rarely leveraging上下文信息如相关参数。此外，先前的QA-based方法很难处理多个参数的同一个角色情况。在这篇论文中，我们提议QGA-EE，即使用问题生成（QG）模型生成含有丰富上下文信息的问题，而不是使用固定模板。我们还提出了动态模板，以帮助QG模型的训练。实验显示，QGA-EE在ACE05英语数据集上的单任务模型都超过了所有之前的模型。
</details></li>
</ul>
<hr>
<h2 id="HistRED-A-Historical-Document-Level-Relation-Extraction-Dataset"><a href="#HistRED-A-Historical-Document-Level-Relation-Extraction-Dataset" class="headerlink" title="HistRED: A Historical Document-Level Relation Extraction Dataset"></a>HistRED: A Historical Document-Level Relation Extraction Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04285">http://arxiv.org/abs/2307.04285</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/Soyoung/HistRED">https://huggingface.co/datasets/Soyoung/HistRED</a></li>
<li>paper_authors: Soyoung Yang, Minseok Choi, Youngwoo Cho, Jaegul Choo</li>
<li>for: 本研究的目的是促进历史关系抽取（RE）研究，探讨历史数据中的潜在应用场景。</li>
<li>methods: 该研究使用了 Yeonhaengnok 集成了的 HistRED 数据集，该数据集包含了 Hanja 和 Korean 文本的双语注释，以支持历史 RE 任务的研究。</li>
<li>results: 研究提出了一种双语 RE 模型，利用了 Korean 和 Hanja 文本上的上下文来预测实体之间的关系。模型在 HistRED 数据集上表现出色，超过了单语基elines，表明使用多语言上下文可以补充 RE 预测。Here’s the simplified Chinese text for each point:</li>
<li>for: 这个研究的目的是推动历史关系抽取（RE）研究，探索历史数据中的潜在应用场景。</li>
<li>methods: 该研究使用了《연향록》集成的 HistRED 数据集，该数据集包含了汉字和韩语文本的双语注释，以支持历史 RE 任务的研究。</li>
<li>results: 研究提出了一种双语 RE 模型，利用了韩语和汉字文本上的上下文来预测实体之间的关系。模型在 HistRED 数据集上表现出色，超过了单语基elines，表明使用多语言上下文可以补充 RE 预测。<details>
<summary>Abstract</summary>
Despite the extensive applications of relation extraction (RE) tasks in various domains, little has been explored in the historical context, which contains promising data across hundreds and thousands of years. To promote the historical RE research, we present HistRED constructed from Yeonhaengnok. Yeonhaengnok is a collection of records originally written in Hanja, the classical Chinese writing, which has later been translated into Korean. HistRED provides bilingual annotations such that RE can be performed on Korean and Hanja texts. In addition, HistRED supports various self-contained subtexts with different lengths, from a sentence level to a document level, supporting diverse context settings for researchers to evaluate the robustness of their RE models. To demonstrate the usefulness of our dataset, we propose a bilingual RE model that leverages both Korean and Hanja contexts to predict relations between entities. Our model outperforms monolingual baselines on HistRED, showing that employing multiple language contexts supplements the RE predictions. The dataset is publicly available at: https://huggingface.co/datasets/Soyoung/HistRED under CC BY-NC-ND 4.0 license.
</details>
<details>
<summary>摘要</summary>
尽管关系提取（RE）任务在不同领域得到了广泛应用，但历史上的应用尚未得到了充分的研究。为推动历史RE研究，我们现在提出了 HistRED，它是基于《연해нг록》的一个建构。《연해нг록》是一种原始写于汉字的记录，后来被翻译成朝鲜语。HistRED提供了双语注释，使得RE可以在朝鲜语和汉字文本之间进行。此外，HistRED支持多种自 contenido Subtexts，其中 lengths 从句子级到文档级，以支持多种上下文设置，以便研究人员可以评估其RE模型的可靠性。为了证明我们的数据集的有用性，我们提出了一种双语RE模型，该模型利用了朝鲜语和汉字上下文来预测实体之间的关系。我们的模型在 HistRED 上表现出色，超过了单语基eline，显示了employna多语言上下文可以补充RE预测。该数据集现在可以在以下链接获取：https://huggingface.co/datasets/Soyoung/HistRED， unter CC BY-NC-ND 4.0 license。
</details></li>
</ul>
<hr>
<h2 id="Automated-Essay-Scoring-in-Argumentative-Writing-DeBERTeachingAssistant"><a href="#Automated-Essay-Scoring-in-Argumentative-Writing-DeBERTeachingAssistant" class="headerlink" title="Automated Essay Scoring in Argumentative Writing: DeBERTeachingAssistant"></a>Automated Essay Scoring in Argumentative Writing: DeBERTeachingAssistant</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04276">http://arxiv.org/abs/2307.04276</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yann Hicke, Tonghua Tian, Karan Jha, Choong Hee Kim</li>
<li>for: This paper aims to improve the assessment of argumentative writing by developing a transformer-based architecture that can annotate discourse elements for their persuasiveness quality.</li>
<li>methods: The proposed method uses a transformer-based architecture to analyze argumentative writing and provide annotations for the persuasiveness quality of various discourse elements.</li>
<li>results: The proposed method achieved above-human accuracy in annotating argumentative writing discourse elements for their persuasiveness quality.Here’s the text in Simplified Chinese:</li>
<li>for: 这篇论文的目的是改进口头写作评价，通过开发基于转换器的架构，对口头写作中的语言元素进行评估，并提供有关语言元素的评价结果。</li>
<li>methods: 提议的方法使用基于转换器的架构来分析口头写作，并提供对语言元素的评价结果。</li>
<li>results: 提议的方法在评估口头写作中的语言元素评价上达到了人类以上的准确率。<details>
<summary>Abstract</summary>
Automated Essay scoring has been explored as a research and industry problem for over 50 years. It has drawn a lot of attention from the NLP community because of its clear educational value as a research area that can engender the creation of valuable time-saving tools for educators around the world. Yet, these tools are generally focused on detecting good grammar, spelling mistakes, and organization quality but tend to fail at incorporating persuasiveness features in their final assessment. The responsibility to give actionable feedback to the student to improve the strength of their arguments is left solely on the teacher's shoulders. In this work, we present a transformer-based architecture capable of achieving above-human accuracy in annotating argumentative writing discourse elements for their persuasiveness quality and we expand on planned future work investigating the explainability of our model so that actionable feedback can be offered to the student and thus potentially enable a partnership between the teacher's advice and the machine's advice.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Augmenters-at-SemEval-2023-Task-1-Enhancing-CLIP-in-Handling-Compositionality-and-Ambiguity-for-Zero-Shot-Visual-WSD-through-Prompt-Augmentation-and-Text-To-Image-Diffusion"><a href="#Augmenters-at-SemEval-2023-Task-1-Enhancing-CLIP-in-Handling-Compositionality-and-Ambiguity-for-Zero-Shot-Visual-WSD-through-Prompt-Augmentation-and-Text-To-Image-Diffusion" class="headerlink" title="Augmenters at SemEval-2023 Task 1: Enhancing CLIP in Handling Compositionality and Ambiguity for Zero-Shot Visual WSD through Prompt Augmentation and Text-To-Image Diffusion"></a>Augmenters at SemEval-2023 Task 1: Enhancing CLIP in Handling Compositionality and Ambiguity for Zero-Shot Visual WSD through Prompt Augmentation and Text-To-Image Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05564">http://arxiv.org/abs/2307.05564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie S. Li, Yow-Ting Shiue, Yong-Siang Shih, Jonas Geiping</li>
<li>for: 这篇论文主要是为了解决英语视觉单词意思异步问题。</li>
<li>methods: 论文提出了两种解决方案：增强CLIP和稳定扩散抽样（SD Sampling）。增强CLIP通过使用大语言模型（LLMs）生成句子，以增强CLIP模型对文本中词语的理解能力。SD Sampling使用文本到图像的稳定扩散来生成多个图像，提高图像和文本的匹配机会。</li>
<li>results: 实验结果表明，增强CLIP和SD Sampling可以提高图像和文本的匹配率，并且可以减少多对多的问题。<details>
<summary>Abstract</summary>
This paper describes our zero-shot approaches for the Visual Word Sense Disambiguation (VWSD) Task in English. Our preliminary study shows that the simple approach of matching candidate images with the phrase using CLIP suffers from the many-to-many nature of image-text pairs. We find that the CLIP text encoder may have limited abilities in capturing the compositionality in natural language. Conversely, the descriptive focus of the phrase varies from instance to instance. We address these issues in our two systems, Augment-CLIP and Stable Diffusion Sampling (SD Sampling). Augment-CLIP augments the text prompt by generating sentences that contain the context phrase with the help of large language models (LLMs). We further explore CLIP models in other languages, as the an ambiguous word may be translated into an unambiguous one in the other language. SD Sampling uses text-to-image Stable Diffusion to generate multiple images from the given phrase, increasing the likelihood that a subset of images match the one that paired with the text.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Assessing-the-efficacy-of-large-language-models-in-generating-accurate-teacher-responses"><a href="#Assessing-the-efficacy-of-large-language-models-in-generating-accurate-teacher-responses" class="headerlink" title="Assessing the efficacy of large language models in generating accurate teacher responses"></a>Assessing the efficacy of large language models in generating accurate teacher responses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04274">http://arxiv.org/abs/2307.04274</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yann Hicke, Abhishek Masand, Wentao Guo, Tushaar Gangavarapu</li>
<li>for: 这个研究是为了评估大语言模型在教育对话中的生成能力，以便模拟知gable teacher。</li>
<li>methods: 研究使用了多种标准的生成模型，包括GPT-4（少量学习、在场景学习）、精度调整的GPT-2和DialoGPT，并使用了强化学习来优化Flan-T5模型。</li>
<li>results: 研究发现GPT-4在Techer-Student Chatroom Corpus子集上表现出色， measured using BERTScore和DialogRPT。  Additionally, the study found that certain dataset characteristics, such as sampling, representativeness, and dialog completeness, can pose challenges to fine-tuning and contribute to the poor generalizability of the fine-tuned models.<details>
<summary>Abstract</summary>
(Tack et al., 2023) organized the shared task hosted by the 18th Workshop on Innovative Use of NLP for Building Educational Applications on generation of teacher language in educational dialogues. Following the structure of the shared task, in this study, we attempt to assess the generative abilities of large language models in providing informative and helpful insights to students, thereby simulating the role of a knowledgeable teacher. To this end, we present an extensive evaluation of several benchmarking generative models, including GPT-4 (few-shot, in-context learning), fine-tuned GPT-2, and fine-tuned DialoGPT. Additionally, to optimize for pedagogical quality, we fine-tuned the Flan-T5 model using reinforcement learning. Our experimental findings on the Teacher-Student Chatroom Corpus subset indicate the efficacy of GPT-4 over other fine-tuned models, measured using BERTScore and DialogRPT.   We hypothesize that several dataset characteristics, including sampling, representativeness, and dialog completeness, pose significant challenges to fine-tuning, thus contributing to the poor generalizability of the fine-tuned models. Finally, we note the need for these generative models to be evaluated with a metric that relies not only on dialog coherence and matched language modeling distribution but also on the model's ability to showcase pedagogical skills.
</details>
<details>
<summary>摘要</summary>
我们认为， dataset 特性，包括采样、 representativeness 和对话完整性，对于调整带来了 significi cant 挑战，这些挑战对于调整模型的泛化性具有负面影响。最后，我们注意到，为了评估这些生成模型，需要使用一种指标，不仅考虑对话 coherence 和模型语言分布的匹配，还需要考虑模型在教学技巧方面的表现。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Coding-at-Scale-Design-and-Deployment-of-a-Nationwide-System-for-Normalizing-Referrals-in-the-Chilean-Public-Healthcare-System"><a href="#Automatic-Coding-at-Scale-Design-and-Deployment-of-a-Nationwide-System-for-Normalizing-Referrals-in-the-Chilean-Public-Healthcare-System" class="headerlink" title="Automatic Coding at Scale: Design and Deployment of a Nationwide System for Normalizing Referrals in the Chilean Public Healthcare System"></a>Automatic Coding at Scale: Design and Deployment of a Nationwide System for Normalizing Referrals in the Chilean Public Healthcare System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05560">http://arxiv.org/abs/2307.05560</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabián Villena, Matías Rojas, Felipe Arias, Jorge Pacheco, Paulina Vera, Jocelyn Dunstan</li>
<li>for: 该论文主要目的是提出一种自动将疾病提取出 из医疗文档，以便进行 epidemiological studies 等研究。</li>
<li>methods: 该论文提出了一种两步方法，首先使用 state-of-the-art NER 模型来识别疾病提到，然后使用基于 Elasticsearch 的搜索引擎系统将最相关的疾病代码分配给疾病提到。</li>
<li>results: 论文的实验结果表明，该系统可以准确地自动分配疾病代码，MAP 得分为 0.63 和 0.83 分别在 subcategory 和 category 两个水平上。<details>
<summary>Abstract</summary>
The disease coding task involves assigning a unique identifier from a controlled vocabulary to each disease mentioned in a clinical document. This task is relevant since it allows information extraction from unstructured data to perform, for example, epidemiological studies about the incidence and prevalence of diseases in a determined context. However, the manual coding process is subject to errors as it requires medical personnel to be competent in coding rules and terminology. In addition, this process consumes a lot of time and energy, which could be allocated to more clinically relevant tasks. These difficulties can be addressed by developing computational systems that automatically assign codes to diseases. In this way, we propose a two-step system for automatically coding diseases in referrals from the Chilean public healthcare system. Specifically, our model uses a state-of-the-art NER model for recognizing disease mentions and a search engine system based on Elasticsearch for assigning the most relevant codes associated with these disease mentions. The system's performance was evaluated on referrals manually coded by clinical experts. Our system obtained a MAP score of 0.63 for the subcategory level and 0.83 for the category level, close to the best-performing models in the literature. This system could be a support tool for health professionals, optimizing the coding and management process. Finally, to guarantee reproducibility, we publicly release the code of our models and experiments.
</details>
<details>
<summary>摘要</summary>
疾病编码任务是将每个在临床文档中提到的疾病分配一个从控制词汇中获取的唯一标识符。这项任务非常重要，因为它允许从无结构数据中提取信息，以进行例如，疾病发生率和患病率的评估。然而，手动编码过程受到误差的影响，因为医疗人员需要熟悉编码规则和术语。此外，这个过程需要很多时间和能量，这些资源可以用于更有价值的临床任务。为解决这些困难，我们提出了一种自动将疾病编码为疾病名称的两步系统。具体来说，我们的模型使用了当前领域的最佳NER模型，以识别疾病提到的文本，并使用基于Elasticsearch的搜索引擎系统，将疾病提到的最相关的编码词归类。我们的系统在专家手动编码的referral上进行评估，我们的系统在分类层次上获得了MAP分数为0.63，在类别层次上获得了MAP分数为0.83，与文献中最佳模型几乎相同。这个系统可以作为医疗专业人员的支持工具，优化编码和管理过程。最后，为保证可重现性，我们在线上公开发布了我们的模型和实验。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/10/cs.CL_2023_07_10/" data-id="closbrolh007l0g8800yu1yay" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/10/cs.LG_2023_07_10/" class="article-date">
  <time datetime="2023-07-10T10:00:00.000Z" itemprop="datePublished">2023-07-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/10/cs.LG_2023_07_10/">cs.LG - 2023-07-10</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="On-the-power-of-graph-neural-networks-and-the-role-of-the-activation-function"><a href="#On-the-power-of-graph-neural-networks-and-the-role-of-the-activation-function" class="headerlink" title="On the power of graph neural networks and the role of the activation function"></a>On the power of graph neural networks and the role of the activation function</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04661">http://arxiv.org/abs/2307.04661</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sammy Khalife, Amitabh Basu</li>
<li>for: 本研究探讨了图 neural network (GNN) 的表达能力。</li>
<li>methods: 作者使用了对凑分多项式活化函数的研究，证明了任何具有固定大小的 GNN 无法在任意数量的迭代过程中分辨两个非同构的根树。</li>
<li>results: 研究发现，允许Activation function 不是固定多项式的 GNN 在两轮迭代过程中可以分辨任何两个非同构的根树。此外，研究还回答了 [Grohe, 2021] 提出的一个开问，并证明了 bounded 和 unbounded 大小 GNN 之间存在势必的分化。<details>
<summary>Abstract</summary>
In this article we present new results about the expressivity of Graph Neural Networks (GNNs). We prove that for any GNN with piecewise polynomial activations, whose architecture size does not grow with the graph input sizes, there exists a pair of non-isomorphic rooted trees of depth two such that the GNN cannot distinguish their root vertex up to an arbitrary number of iterations. The proof relies on tools from the algebra of symmetric polynomials. In contrast, it was already known that unbounded GNNs (those whose size is allowed to change with the graph sizes) with piecewise polynomial activations can distinguish these vertices in only two iterations. Our results imply a strict separation between bounded and unbounded size GNNs, answering an open question formulated by [Grohe, 2021]. We next prove that if one allows activations that are not piecewise polynomial, then in two iterations a single neuron perceptron can distinguish the root vertices of any pair of nonisomorphic trees of depth two (our results hold for activations like the sigmoid, hyperbolic tan and others). This shows how the power of graph neural networks can change drastically if one changes the activation function of the neural networks. The proof of this result utilizes the Lindemann-Weierstrauss theorem from transcendental number theory.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出了新的结果关于图神经网络（GNNs）的表达能力。我们证明了，对任何具有分割 polynomials 活化函数的 GNN，其架构大小不随图像大小增长，那么存在两个非同构的根树，其中根节点不可以在无限多轮 iterations 中被 GNN  отличить。证明基于同态多项式代数的工具。在对比之下，已知无穷 GNNs（允许架构大小随图像大小变化）可以在两轮 iterations 中分辨这两个根节点。我们的结果表明，有穷 GNNs 和无穷 GNNs 之间存在彻底的分化，解答了 [Grohe, 2021] 提出的开问。我们接着证明，允许非分割 polynomials 活化函数的情况下，在两轮 iterations 中，单个神经元某元素权重网络可以分辨任何两个非同构的深度两个根树的根节点（我们的结果适用于如 сиги模、恒下弯和其他活化函数）。这示cases how the power of graph neural networks can change drastically if one changes the activation function of the neural networks. The proof of this result utilizes the Lindemann-Weierstrauss theorem from transcendental number theory.
</details></li>
</ul>
<hr>
<h2 id="Active-Learning-for-Video-Classification-with-Frame-Level-Queries"><a href="#Active-Learning-for-Video-Classification-with-Frame-Level-Queries" class="headerlink" title="Active Learning for Video Classification with Frame Level Queries"></a>Active Learning for Video Classification with Frame Level Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05587">http://arxiv.org/abs/2307.05587</a></li>
<li>repo_url: None</li>
<li>paper_authors: Debanjan Goswami, Shayok Chakraborty</li>
<li>for: 这个研究目的是为了更进一步削减人工标注员的努力，使得将机器学习模型训练需要的标注数量更加少。</li>
<li>methods: 我们提出了一个新的活动学习框架，使用了不确定度和多样性的标准来选择具有代表性的视频和具有价值的几帧帧。</li>
<li>results: 我们的方法可以将人工标注员的努力削减至只需要审核几帧帧，而不是观看整个视频。这些结果表明了我们的方法可以帮助将机器学习模型训练需要的标注数量降低，并且可以更好地利用人工标注员的时间和努力。<details>
<summary>Abstract</summary>
Deep learning algorithms have pushed the boundaries of computer vision research and have depicted commendable performance in a variety of applications. However, training a robust deep neural network necessitates a large amount of labeled training data, acquiring which involves significant time and human effort. This problem is even more serious for an application like video classification, where a human annotator has to watch an entire video end-to-end to furnish a label. Active learning algorithms automatically identify the most informative samples from large amounts of unlabeled data; this tremendously reduces the human annotation effort in inducing a machine learning model, as only the few samples that are identified by the algorithm, need to be labeled manually. In this paper, we propose a novel active learning framework for video classification, with the goal of further reducing the labeling onus on the human annotators. Our framework identifies a batch of exemplar videos, together with a set of informative frames for each video; the human annotator needs to merely review the frames and provide a label for each video. This involves much less manual work than watching the complete video to come up with a label. We formulate a criterion based on uncertainty and diversity to identify the informative videos and exploit representative sampling techniques to extract a set of exemplar frames from each video. To the best of our knowledge, this is the first research effort to develop an active learning framework for video classification, where the annotators need to inspect only a few frames to produce a label, rather than watching the end-to-end video.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multimodal-brain-age-estimation-using-interpretable-adaptive-population-graph-learning"><a href="#Multimodal-brain-age-estimation-using-interpretable-adaptive-population-graph-learning" class="headerlink" title="Multimodal brain age estimation using interpretable adaptive population-graph learning"></a>Multimodal brain age estimation using interpretable adaptive population-graph learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04639">http://arxiv.org/abs/2307.04639</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bintsi/adaptive-graph-learning">https://github.com/bintsi/adaptive-graph-learning</a></li>
<li>paper_authors: Kyriaki-Margarita Bintsi, Vasileios Baltatzis, Rolandos Alexandros Potamias, Alexander Hammers, Daniel Rueckert</li>
<li>for: 本研究旨在提出一种基于图 convolutional neural network (GCN) 的人脑年龄估计方法，以便在诊断阿尔ц海默病等nehrodegenerative疾病中提供有价值的信息。</li>
<li>methods: 本研究使用了图建构机制（GCN）和注意力机制（Attention），通过学习人脑图的结构，从而提高人脑年龄估计的准确性。</li>
<li>results: 比较static图建构方法和其他适应方法，本研究的方法在人脑年龄估计和分类任务上表现出色，并且通过图像和非图像特征（phenotypes）的权重赋值，提高了图建构的可读性。<details>
<summary>Abstract</summary>
Brain age estimation is clinically important as it can provide valuable information in the context of neurodegenerative diseases such as Alzheimer's. Population graphs, which include multimodal imaging information of the subjects along with the relationships among the population, have been used in literature along with Graph Convolutional Networks (GCNs) and have proved beneficial for a variety of medical imaging tasks. A population graph is usually static and constructed manually using non-imaging information. However, graph construction is not a trivial task and might significantly affect the performance of the GCN, which is inherently very sensitive to the graph structure. In this work, we propose a framework that learns a population graph structure optimized for the downstream task. An attention mechanism assigns weights to a set of imaging and non-imaging features (phenotypes), which are then used for edge extraction. The resulting graph is used to train the GCN. The entire pipeline can be trained end-to-end. Additionally, by visualizing the attention weights that were the most important for the graph construction, we increase the interpretability of the graph. We use the UK Biobank, which provides a large variety of neuroimaging and non-imaging phenotypes, to evaluate our method on brain age regression and classification. The proposed method outperforms competing static graph approaches and other state-of-the-art adaptive methods. We further show that the assigned attention scores indicate that there are both imaging and non-imaging phenotypes that are informative for brain age estimation and are in agreement with the relevant literature.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用Graph Convolutional Networks (GCNs)在文献中使用人口图（population graph），其中包括多Modal imaging信息和人口之间的关系，已经证明是有用的 для多种医学影像任务。然而，人口图的建构并不是一个轻松的任务，可能会影响GCN的性能，GCN本身是非常敏感于图结构。在这种情况下，我们提出了一个框架，可以学习优化的人口图结构，用于下游任务。我们使用一个注意力机制，将多Modal imaging和非 imaging特征（phenotypes）分配给权重，然后用这些权重进行边EXTRACTION。得到的图可以用来训练GCN。整个管道可以被训练END-to-END。此外，通过Visual化注意力权重，我们提高了图的可读性。我们使用UK Biobank，该提供了丰富的神经成像和非成像特征，来评估我们的方法在脑年龄回归和分类任务中的性能。我们的方法超过了相同的静止图方法和其他状态的适应方法。我们进一步表明，分配的注意力分数指示，有 both imaging和非 imaging特征是脑年龄估计中有用的，与相关文献一致。<</SYS>>
</details></li>
</ul>
<hr>
<h2 id="Weakly-supervised-positional-contrastive-learning-application-to-cirrhosis-classification"><a href="#Weakly-supervised-positional-contrastive-learning-application-to-cirrhosis-classification" class="headerlink" title="Weakly-supervised positional contrastive learning: application to cirrhosis classification"></a>Weakly-supervised positional contrastive learning: application to cirrhosis classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04617">http://arxiv.org/abs/2307.04617</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/guerbet-ai/wsp-contrastive">https://github.com/guerbet-ai/wsp-contrastive</a></li>
<li>paper_authors: Emma Sarfati, Alexandre Bône, Marc-Michel Rohé, Pietro Gori, Isabelle Bloch</li>
<li>for: 这个研究是为了提出一种高效的弱监督位置学习（WSP）对称学习策略，以搭配弱标签和空间上的排序信息来提高医学影像分类的精度。</li>
<li>methods: 本研究使用了一种具有内部积分函数的通用核心损失函数，将弱标签和空间上的排序信息融合到一起，以提高医学影像分类的精度。</li>
<li>results: 研究结果显示，与基准模型相比，提案的模型在内部数据集上提高了分类AUC的值 by 5%，并在公共的LIHC数据集上提高了分类AUC的值 by 26%。<details>
<summary>Abstract</summary>
Large medical imaging datasets can be cheaply and quickly annotated with low-confidence, weak labels (e.g., radiological scores). Access to high-confidence labels, such as histology-based diagnoses, is rare and costly. Pretraining strategies, like contrastive learning (CL) methods, can leverage unlabeled or weakly-annotated datasets. These methods typically require large batch sizes, which poses a difficulty in the case of large 3D images at full resolution, due to limited GPU memory. Nevertheless, volumetric positional information about the spatial context of each 2D slice can be very important for some medical applications. In this work, we propose an efficient weakly-supervised positional (WSP) contrastive learning strategy where we integrate both the spatial context of each 2D slice and a weak label via a generic kernel-based loss function. We illustrate our method on cirrhosis prediction using a large volume of weakly-labeled images, namely radiological low-confidence annotations, and small strongly-labeled (i.e., high-confidence) datasets. The proposed model improves the classification AUC by 5% with respect to a baseline model on our internal dataset, and by 26% on the public LIHC dataset from the Cancer Genome Atlas. The code is available at: https://github.com/Guerbet-AI/wsp-contrastive.
</details>
<details>
<summary>摘要</summary>
大量医学成像数据集可以便宜地和快速地进行低信度、弱标注（例如，放射学分数）的标注。高信度标签，如 histology-based 诊断，则非常罕见和昂贵。预训练策略，如对冲学习（CL）方法，可以利用无标签或弱标签数据集。这些方法通常需要大批量大小，但是在大量3D图像的全分辨率下，由于 GPU 内存限制，这可能会增加困难。然而，三维位势信息可以在某些医学应用中非常重要。在这种情况下，我们提出了一种高效的弱指导位势对比（WSP）对比学习策略，该策略通过一种通用的 kernel-based 损失函数来整合每个2D slice 的空间上下文和弱标签。我们在 cirrhosis 预测 task 上使用了一大量的弱标签图像，即放射学低信度标注，以及一些小型、强标签（即高信度标签）数据集。我们的模型与基线模型在我们的内部数据集上提高了分类 AUC 值5%，并在 LIHC 数据集上提高了26%。代码可以在：https://github.com/Guerbet-AI/wsp-contrastive 中找到。
</details></li>
</ul>
<hr>
<h2 id="MiVOLO-Multi-input-Transformer-for-Age-and-Gender-Estimation"><a href="#MiVOLO-Multi-input-Transformer-for-Age-and-Gender-Estimation" class="headerlink" title="MiVOLO: Multi-input Transformer for Age and Gender Estimation"></a>MiVOLO: Multi-input Transformer for Age and Gender Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04616">http://arxiv.org/abs/2307.04616</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wildchlamydia/mivolo">https://github.com/wildchlamydia/mivolo</a></li>
<li>paper_authors: Maksim Kuprashevich, Irina Tolstykh</li>
<li>for: 这篇论文旨在提出一种基于视觉变换器的年龄和性别认知方法，以便在野外环境中进行年龄和性别预测。</li>
<li>methods: 该方法使用最新的视觉变换器，并将年龄和性别两个任务集成到一个共同的双输入&#x2F;输出模型中，以利用人脸信息和人像数据。</li>
<li>results: 经过实验表明，该模型在四个流行的标准benchmark上达到了状态机器人性能，并且在实时处理方面表现出色。此外，我们还引入了一个基于Open Images Dataset的新的benchmark，并提供了高精度的人工筛选的标注数据。最终，我们比较了该模型的年龄认知性能与人类水平，并证明它在大多数年龄范围内有显著优势。<details>
<summary>Abstract</summary>
Age and gender recognition in the wild is a highly challenging task: apart from the variability of conditions, pose complexities, and varying image quality, there are cases where the face is partially or completely occluded. We present MiVOLO (Multi Input VOLO), a straightforward approach for age and gender estimation using the latest vision transformer. Our method integrates both tasks into a unified dual input/output model, leveraging not only facial information but also person image data. This improves the generalization ability of our model and enables it to deliver satisfactory results even when the face is not visible in the image. To evaluate our proposed model, we conduct experiments on four popular benchmarks and achieve state-of-the-art performance, while demonstrating real-time processing capabilities. Additionally, we introduce a novel benchmark based on images from the Open Images Dataset. The ground truth annotations for this benchmark have been meticulously generated by human annotators, resulting in high accuracy answers due to the smart aggregation of votes. Furthermore, we compare our model's age recognition performance with human-level accuracy and demonstrate that it significantly outperforms humans across a majority of age ranges. Finally, we grant public access to our models, along with the code for validation and inference. In addition, we provide extra annotations for used datasets and introduce our new benchmark.
</details>
<details>
<summary>摘要</summary>
在野外中，年龄和性别识别是一项非常具有挑战性的任务：除了条件的变化、姿势复杂度和图像质量的变化外，还有情况下面部或完全被遮盖。我们介绍了 MiVOLO（多输入VOLO），一种简单的方法，使用最新的视觉变换器进行年龄和性别估算。我们的方法将这两个任务集成到一个统一的双输入/输出模型中，利用人像数据以及脸部信息。这会提高我们的模型的总体化能力，使其能够在图像中不可见的脸部时还能达到满意的结果。为评估我们提出的模型，我们进行了四个流行的benchmark测试，并实现了实时处理能力。此外，我们还创建了一个基于Open Images Dataset的新的benchmark，其中的ground truth标注由人工标注员仔细生成，因此得到了高度的准确率。此外，我们比较了我们的年龄识别性能与人类水平的准确率，并证明它在大多数年龄范围内Significantly Outperform humans。最后，我们向公众开放了我们的模型，同时提供了验证和推理代码。此外，我们还提供了其他的标注，并引入了我们的新benchmark。
</details></li>
</ul>
<hr>
<h2 id="EchoVest-Real-Time-Sound-Classification-and-Depth-Perception-Expressed-through-Transcutaneous-Electrical-Nerve-Stimulation"><a href="#EchoVest-Real-Time-Sound-Classification-and-Depth-Perception-Expressed-through-Transcutaneous-Electrical-Nerve-Stimulation" class="headerlink" title="EchoVest: Real-Time Sound Classification and Depth Perception Expressed through Transcutaneous Electrical Nerve Stimulation"></a>EchoVest: Real-Time Sound Classification and Depth Perception Expressed through Transcutaneous Electrical Nerve Stimulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04604">http://arxiv.org/abs/2307.04604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jesse Choe, Siddhant Sood, Ryan Park</li>
<li>for: The paper aims to develop an assistive device for blind&#x2F;deaf individuals to enhance their awareness of their environment, with a focus on sound classification and localization.</li>
<li>methods: The paper employs a novel audio pipeline that combines the Audio Spectrogram Transformer (AST) model and Fast Fourier Transforms for noise reduction, as well as Otsu’s Method for background noise sound filtering and Complex Time Difference of Arrival algorithms for direction and depth calculation.</li>
<li>results: The final algorithm achieved state-of-the-art results on numerous checkpoints, including a 95.7% accuracy on the ESC-50 dataset for environmental sound classification.Here’s the simplified Chinese text for the three key points:</li>
<li>for: 这篇论文目标是开发一种助听设备，帮助失明聋听人更好地了解环境，特色在声音分类和本地化。</li>
<li>methods: 这篇论文采用了一种新的音频管线，将Audio Spectrogram Transformer（AST）模型和快速傅里埃变换（FFT）组合用于声音干扰reduction，以及OTSU的方法来过滤背景噪声，并使用复杂时差到达算法来计算方向和深度。</li>
<li>results: 最终算法在多个检查点上达到了顶尖的结果，包括ESC-50数据集上的声音分类准确率95.7%。<details>
<summary>Abstract</summary>
Over 1.5 billion people worldwide live with hearing impairment. Despite various technologies that have been created for individuals with such disabilities, most of these technologies are either extremely expensive or inaccessible for everyday use in low-medium income countries. In order to combat this issue, we have developed a new assistive device, EchoVest, for blind/deaf people to intuitively become more aware of their environment. EchoVest transmits vibrations to the user's body by utilizing transcutaneous electric nerve stimulation (TENS) based on the source of the sounds. EchoVest also provides various features, including sound localization, sound classification, noise reduction, and depth perception. We aimed to outperform CNN-based machine-learning models, the most commonly used machine learning model for classification tasks, in accuracy and computational costs. To do so, we developed and employed a novel audio pipeline that adapts the Audio Spectrogram Transformer (AST) model, an attention-based model, for our sound classification purposes, and Fast Fourier Transforms for noise reduction. The application of Otsu's Method helped us find the optimal thresholds for background noise sound filtering and gave us much greater accuracy. In order to calculate direction and depth accurately, we applied Complex Time Difference of Arrival algorithms and SOTA localization. Our last improvement was to use blind source separation to make our algorithms applicable to multiple microphone inputs. The final algorithm achieved state-of-the-art results on numerous checkpoints, including a 95.7\% accuracy on the ESC-50 dataset for environmental sound classification.
</details>
<details>
<summary>摘要</summary>
全球1.5亿人口中有听力障碍，尽管有许多为这些人群开发的技术，但大多数这些技术是非常昂贵或在低中收入国家中不可 accessing。为解决这个问题，我们开发了一个新的助手设备——EchoVest，用于听力障碍人群更好地了解环境。EchoVest通过使用皮肤电刺激（TENS）技术，将声音传递给用户的身体，并提供了多种功能，如声音地图、声音分类、干扰reduction和深度感知。我们想要在精度和计算成本两个方面超越基于Convolutional Neural Networks（CNN）的机器学习模型，因此我们开发了一个新的音频管道，使用Audio Spectrogram Transformer（AST）模型，并使用快速傅立叶变换来减少干扰。使用欧氏方法，我们可以查找最佳背景声音滤波的阈值，从而提高了准确性。为了计算方向和深度，我们应用了复杂时间差分解算法和当今最佳地址算法。最后，我们使用盲源分离来使我们的算法适用于多个麦克风输入。最终算法达到了现有最佳结果，包括ESC-50数据集上的95.7%准确率。
</details></li>
</ul>
<hr>
<h2 id="DBFed-Debiasing-Federated-Learning-Framework-based-on-Domain-Independent"><a href="#DBFed-Debiasing-Federated-Learning-Framework-based-on-Domain-Independent" class="headerlink" title="DBFed: Debiasing Federated Learning Framework based on Domain-Independent"></a>DBFed: Debiasing Federated Learning Framework based on Domain-Independent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05582">http://arxiv.org/abs/2307.05582</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiale Li, Zhixin Li, Yibo Wang, Yao Li, Lei Wang</li>
<li>for: 本研究旨在提出一种基于领域独立的 federated learning 框架，以解决多个主体数据岛化问题，并 mitigate 模型偏见问题。</li>
<li>methods: 本研究使用了客户端生成的敏感特征来Explicitly 编码敏感特征，以避免模型偏见问题。</li>
<li>results: 实验结果显示，DBFed 比三种比较方法中的大多数指标都高于，这 fully  demonstrably 表明 DBFed 的偏见纠正效果。<details>
<summary>Abstract</summary>
As digital transformation continues, enterprises are generating, managing, and storing vast amounts of data, while artificial intelligence technology is rapidly advancing. However, it brings challenges in information security and data security. Data security refers to the protection of digital information from unauthorized access, damage, theft, etc. throughout its entire life cycle. With the promulgation and implementation of data security laws and the emphasis on data security and data privacy by organizations and users, Privacy-preserving technology represented by federated learning has a wide range of application scenarios. Federated learning is a distributed machine learning computing framework that allows multiple subjects to train joint models without sharing data to protect data privacy and solve the problem of data islands. However, the data among multiple subjects are independent of each other, and the data differences in quality may cause fairness issues in federated learning modeling, such as data bias among multiple subjects, resulting in biased and discriminatory models. Therefore, we propose DBFed, a debiasing federated learning framework based on domain-independent, which mitigates model bias by explicitly encoding sensitive attributes during client-side training. This paper conducts experiments on three real datasets and uses five evaluation metrics of accuracy and fairness to quantify the effect of the model. Most metrics of DBFed exceed those of the other three comparative methods, fully demonstrating the debiasing effect of DBFed.
</details>
<details>
<summary>摘要</summary>
为数统计 револю过程中，企业创生、管理和储存大量数据，同时人工智能技术快速发展。然而，这带来数据安全和资讯安全的挑战。数据安全指的是保护数据 digitization 过程中的数据免被未经授权的存取、损坏、窃取等行为。随着数据安全法规的推广和组织和用户对数据安全和隐私的重视，隐私保持技术如联邦学习被应用在各个应用场景中。联邦学习是一种分布式机器学习计算框架，允许多个主题共同训练无需分享数据，以保护数据隐私和解决数据岛问题。然而，各个主题的数据独立于对方，并且各个数据质量的不同可能导致该课程优化问题，如主题间的数据偏见，从而导致不公正和歧视的模型。因此，我们提出了DBFed，一个基于领域独立的偏见调整联邦学习框架，通过客边端训练中明确地编码敏感特征，以 Mitigate 模型偏见。本文对三个真实数据集进行实验，使用五个评估 метри来衡量模型的影响。大多数DBFed的 métriques exceed 其他三种比较方法的 métriques，全面展示了DBFed的偏见调整效果。
</details></li>
</ul>
<hr>
<h2 id="AnyTeleop-A-General-Vision-Based-Dexterous-Robot-Arm-Hand-Teleoperation-System"><a href="#AnyTeleop-A-General-Vision-Based-Dexterous-Robot-Arm-Hand-Teleoperation-System" class="headerlink" title="AnyTeleop: A General Vision-Based Dexterous Robot Arm-Hand Teleoperation System"></a>AnyTeleop: A General Vision-Based Dexterous Robot Arm-Hand Teleoperation System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04577">http://arxiv.org/abs/2307.04577</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuzhe Qin, Wei Yang, Binghao Huang, Karl Van Wyk, Hao Su, Xiaolong Wang, Yu-Wei Chao, Dieter Fox</li>
<li>for: 这篇论文目的是提出一个通用的视觉控制系统，以支持多种不同的机器人臂、手、现实和摄像头配置，并且可以在单个系统中实现。</li>
<li>methods: 该系统使用了一种通用的视觉控制方法，可以在多种不同的simulator和实际环境中进行操作。</li>
<li>results: 实际实验和虚拟实验中，AnyTeleop系统可以达到更高的成功率和更好的模仿学习性能，相比之前特定设计 для该机器人硬件的系统。<details>
<summary>Abstract</summary>
Vision-based teleoperation offers the possibility to endow robots with human-level intelligence to physically interact with the environment, while only requiring low-cost camera sensors. However, current vision-based teleoperation systems are designed and engineered towards a particular robot model and deploy environment, which scales poorly as the pool of the robot models expands and the variety of the operating environment increases. In this paper, we propose AnyTeleop, a unified and general teleoperation system to support multiple different arms, hands, realities, and camera configurations within a single system. Although being designed to provide great flexibility to the choice of simulators and real hardware, our system can still achieve great performance. For real-world experiments, AnyTeleop can outperform a previous system that was designed for a specific robot hardware with a higher success rate, using the same robot. For teleoperation in simulation, AnyTeleop leads to better imitation learning performance, compared with a previous system that is particularly designed for that simulator. Project page: http://anyteleop.com/.
</details>
<details>
<summary>摘要</summary>
“视觉基于的 теле操作可以赋予机器人人类水平的智能，同时只需要低成本的摄像头感知器。然而，当前的视觉基于的 теле操作系统是为特定的机器人模型和部署环境设计和工程，这会随着机器人模型池的扩展和运行环境的多样化而扩展不佳。在这篇论文中，我们提议了 AnyTeleop，一个通用和普适的 теле操作系统，可以支持多种不同的臂、手、现实和摄像头配置。尽管我们的系统设计了大量的灵活性，但它仍然可以实现高性能。在实际实验中， AnyTeleop 可以比一个特定机器人硬件设计的系统更高的成功率，使用同样的机器人。在模拟中， AnyTeleop 比特定适用于该模拟器的系统更好的学习效果。项目页面：http://anyteleop.com/。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="A-Semi-Automated-Solution-Approach-Selection-Tool-for-Any-Use-Case-via-Scopus-and-OpenAI-a-Case-Study-for-AI-ML-in-Oncology"><a href="#A-Semi-Automated-Solution-Approach-Selection-Tool-for-Any-Use-Case-via-Scopus-and-OpenAI-a-Case-Study-for-AI-ML-in-Oncology" class="headerlink" title="A Semi-Automated Solution Approach Selection Tool for Any Use Case via Scopus and OpenAI: a Case Study for AI&#x2F;ML in Oncology"></a>A Semi-Automated Solution Approach Selection Tool for Any Use Case via Scopus and OpenAI: a Case Study for AI&#x2F;ML in Oncology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04573">http://arxiv.org/abs/2307.04573</a></li>
<li>repo_url: None</li>
<li>paper_authors: Deniz Kenan Kılıç, Alex Elkjær Vasegaard, Aurélien Desoeuvres, Peter Nielsen</li>
<li>for: 本研究提出了一个半自动化的工具，用于方案方法评估和选择，供研究者、实践者和决策者使用，同时作为未来研究的benchmark。</li>
<li>methods: 本工具包括三个模组：(1) 文献选择和分数计算，使用关键词选择方案进行Scopus API查询和 Compute relevancy; (2) 方案方法提取在文献中使用OpenAI API; (3) 敏感分析和后analyzes。</li>
<li>results: 研究显示，这个工具可以实现半自动化的方案方法评估和选择，并提供了各种应用场景的敏感分析和后analyzes。在肿瘤领域的 caso study 和多个使用案例中，该工具获得了有前提的结果，与手动真实比较。<details>
<summary>Abstract</summary>
In today's vast literature landscape, a manual review is very time-consuming. To address this challenge, this paper proposes a semi-automated tool for solution method review and selection. It caters to researchers, practitioners, and decision-makers while serving as a benchmark for future work. The tool comprises three modules: (1) paper selection and scoring, using a keyword selection scheme to query Scopus API and compute relevancy; (2) solution method extraction in papers utilizing OpenAI API; (3) sensitivity analysis and post-analyzes. It reveals trends, relevant papers, and methods. AI in the oncology case study and several use cases are presented with promising results, comparing the tool to manual ground truth.
</details>
<details>
<summary>摘要</summary>
今天的文献景观中， manual review 非常时间consuming。为解决这个挑战，这篇论文提出了一种半自动化工具 для方法评估和选择。它适用于研究人员、实践者和决策者，同时也作为未来工作的标准。工具包括三个模块：（1）文献选择和分数计算，使用关键词选择方案查询Scopus API，计算相关性；（2）解决方法提取在论文中使用OpenAI API；（3）敏感分析和后置分析。它揭示了趋势、相关论文和方法。在肿瘤 случа子研究和一些实践案例中，提出了有前提的结果，与手动参照相比。
</details></li>
</ul>
<hr>
<h2 id="Unraveling-the-Age-Estimation-Puzzle-Comparative-Analysis-of-Deep-Learning-Approaches-for-Facial-Age-Estimation"><a href="#Unraveling-the-Age-Estimation-Puzzle-Comparative-Analysis-of-Deep-Learning-Approaches-for-Facial-Age-Estimation" class="headerlink" title="Unraveling the Age Estimation Puzzle: Comparative Analysis of Deep Learning Approaches for Facial Age Estimation"></a>Unraveling the Age Estimation Puzzle: Comparative Analysis of Deep Learning Approaches for Facial Age Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04570">http://arxiv.org/abs/2307.04570</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/paplhjak/facial-age-estimation-benchmark">https://github.com/paplhjak/facial-age-estimation-benchmark</a></li>
<li>paper_authors: Jakub Paplham, Vojtech Franc</li>
<li>for: 本研究旨在解决不同年龄估计方法的比较带来的挑战，即发布结果的不可靠性，归因于测试过程中的不一致性。先前的研究报告了过去十年内特殊方法的不断改进，但我们的发现证明这些报告是不准确的。</li>
<li>methods: 本研究使用了跨Entropy损失函数作为标准方法，并系统分析了影响年龄估计结果的多种因素，包括脸部对齐、脸部覆盖率、图像分辨率、图像表示方式、模型架构和数据量。</li>
<li>results: 我们发现，这些因素通常对年龄估计结果产生更大的影响，而不是选择年龄估计方法本身。我们还评估了每种方法的泛化能力，通过评估每种方法在公共可用的年龄估计数据集上的跨数据集性能。结果强调了使用一致的数据处理方法和建立标准 benchmarks，以确保可靠和意义的比较。<details>
<summary>Abstract</summary>
Comparing different age estimation methods poses a challenge due to the unreliability of published results, stemming from inconsistencies in the benchmarking process. Previous studies have reported continuous performance improvements over the past decade using specialized methods; however, our findings challenge these claims. We argue that, for age estimation tasks outside of the low-data regime, designing specialized methods is unnecessary, and the standard approach of utilizing cross-entropy loss is sufficient. This paper aims to address the benchmark shortcomings by evaluating state-of-the-art age estimation methods in a unified and comparable setting. We systematically analyze the impact of various factors, including facial alignment, facial coverage, image resolution, image representation, model architecture, and the amount of data on age estimation results. Surprisingly, these factors often exert a more significant influence than the choice of the age estimation method itself. We assess the generalization capability of each method by evaluating the cross-dataset performance for publicly available age estimation datasets. The results emphasize the importance of using consistent data preprocessing practices and establishing standardized benchmarks to ensure reliable and meaningful comparisons. The source code is available at https://github.com/paplhjak/Facial-Age-Estimation-Benchmark.
</details>
<details>
<summary>摘要</summary>
This paper aims to address benchmark shortcomings by evaluating state-of-the-art age estimation methods in a unified and comparable setting. We systematically analyze the impact of various factors, including facial alignment, facial coverage, image resolution, image representation, model architecture, and the amount of data on age estimation results. Surprisingly, these factors often have a more significant influence than the choice of age estimation method.We assess the generalization capability of each method by evaluating cross-dataset performance for publicly available age estimation datasets. The results emphasize the importance of using consistent data preprocessing practices and establishing standardized benchmarks to ensure reliable and meaningful comparisons. The source code is available at https://github.com/paplhjak/Facial-Age-Estimation-Benchmark.
</details></li>
</ul>
<hr>
<h2 id="Interpreting-and-generalizing-deep-learning-in-physics-based-problems-with-functional-linear-models"><a href="#Interpreting-and-generalizing-deep-learning-in-physics-based-problems-with-functional-linear-models" class="headerlink" title="Interpreting and generalizing deep learning in physics-based problems with functional linear models"></a>Interpreting and generalizing deep learning in physics-based problems with functional linear models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04569">http://arxiv.org/abs/2307.04569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirhossein Arzani, Lingxiao Yuan, Pania Newell, Bei Wang</li>
<li>for: 本研究旨在提供一种可解释的深度学习模型，以提高其在不同数据集上的泛化能力和可读性。</li>
<li>methods: 我们提议使用函数数据分析（FDA）的概念，设计一种通用函数线性模型来替代已经训练过的深度学习模型。我们可以使用不同的kernel函数库，并使用稀疏回归来找到一个可解释的替代模型。</li>
<li>results: 我们的模型可以与深度学习模型具有相同的准确率，同时提高对于不同数据集的泛化能力，并且可以提供更多的可读性和可解释性。我们在固体力学、流体力学和运输等领域进行了测试，并得到了良好的结果。<details>
<summary>Abstract</summary>
Although deep learning has achieved remarkable success in various scientific machine learning applications, its black-box nature poses concerns regarding interpretability and generalization capabilities beyond the training data. Interpretability is crucial and often desired in modeling physical systems. Moreover, acquiring extensive datasets that encompass the entire range of input features is challenging in many physics-based learning tasks, leading to increased errors when encountering out-of-distribution (OOD) data. In this work, motivated by the field of functional data analysis (FDA), we propose generalized functional linear models as an interpretable surrogate for a trained deep learning model. We demonstrate that our model could be trained either based on a trained neural network (post-hoc interpretation) or directly from training data (interpretable operator learning). A library of generalized functional linear models with different kernel functions is considered and sparse regression is used to discover an interpretable surrogate model that could be analytically presented. We present test cases in solid mechanics, fluid mechanics, and transport. Our results demonstrate that our model can achieve comparable accuracy to deep learning and can improve OOD generalization while providing more transparency and interpretability. Our study underscores the significance of interpretability in scientific machine learning and showcases the potential of functional linear models as a tool for interpreting and generalizing deep learning.
</details>
<details>
<summary>摘要</summary>
Inspired by the field of functional data analysis (FDA), we propose generalized functional linear models as an interpretable surrogate for a trained deep learning model. Our model can be trained either based on a trained neural network (post-hoc interpretation) or directly from training data (interpretable operator learning). We consider a library of generalized functional linear models with different kernel functions and use sparse regression to discover an interpretable surrogate model that can be analytically presented.We demonstrate our model's effectiveness through test cases in solid mechanics, fluid mechanics, and transport. Our results show that our model can achieve comparable accuracy to deep learning and can improve OOD generalization while providing more transparency and interpretability. Our study highlights the importance of interpretability in scientific machine learning and showcases the potential of functional linear models as a tool for interpreting and generalizing deep learning.
</details></li>
</ul>
<hr>
<h2 id="Automatically-detecting-activities-of-daily-living-from-in-home-sensors-as-indicators-of-routine-behaviour-in-an-older-population"><a href="#Automatically-detecting-activities-of-daily-living-from-in-home-sensors-as-indicators-of-routine-behaviour-in-an-older-population" class="headerlink" title="Automatically detecting activities of daily living from in-home sensors as indicators of routine behaviour in an older population"></a>Automatically detecting activities of daily living from in-home sensors as indicators of routine behaviour in an older population</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04563">http://arxiv.org/abs/2307.04563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Claire M. Timon, Pamela Hussey, Hyowon Lee, Catriona Murphy, Harsh Vardan Rai, and Alan F. Smeaton</li>
<li>for: 这个研究是为了开发一个基于互联网物 Things（IoT）系统，与数据分析，来提供不干扰的健康和待遇监控，以支持年长者在家中独立生活。</li>
<li>methods: 这个研究使用了一个Action Research Cycle（ARC）试验，试验了23名参与者，每个参与者有约20个IoT仪器在家中。在ARC试验中，参与者参加了两次数据告诉会，每次都会显示参与者的家中活动。这些会议也收集了参与者对检测活动的准确性的反馈。</li>
<li>results: 这个研究发现，使用了联合规则探索，可以独立地检测参与者的每天生活活动（ADL），并且可以使用单一的规则来检测各个参与者的ADL。这意味着，可以降低参与者提供训练数据的必要性，并且可以让更多的参与者加入系统。<details>
<summary>Abstract</summary>
Objective: The NEX project has developed an integrated Internet of Things (IoT) system coupled with data analytics to offer unobtrusive health and wellness monitoring supporting older adults living independently at home. Monitoring {currently} involves visualising a set of automatically detected activities of daily living (ADLs) for each participant. The detection of ADLs is achieved {} to allow the incorporation of additional participants whose ADLs are detected without re-training the system.   Methods: Following an extensive User Needs and Requirements study involving 426 participants, a pilot trial and a friendly trial of the deployment, an Action Research Cycle (ARC) trial was completed. This involved 23 participants over a 10-week period each with c.20 IoT sensors in their homes. During the ARC trial, participants each took part in two data-informed briefings which presented visualisations of their own in-home activities. The briefings also gathered training data on the accuracy of detected activities. Association rule mining was then used on the combination of data from sensors and participant feedback to improve the automatic detection of ADLs.   Results: Association rule mining was used to detect a range of ADLs for each participant independently of others and was then used to detect ADLs across participants using a single set of rules {for each ADL}. This allows additional participants to be added without the necessity of them providing training data.   Conclusions: Additional participants can be added to the NEX system without the necessity to re-train the system for automatic detection of the set of their activities of daily living.
</details>
<details>
<summary>摘要</summary>
目标：NEX项目已经开发了一个集成互联网智能（IoT）系统，并结合数据分析，以提供不间断的健康和休闲监测，支持年轻者在家中独立生活。监测当前包括图示每名参与者的自动检测的日常生活活动（ADLs）的集成。方法：经过了426名参与者的用户需求和要求研究，飞行试验和友好试验的部署，完成了一个Action Research Cycle（ARC）试验。这 involve了23名参与者，每名参与者在10周期内有约20个IoT传感器在家中。在ARC试验期间，参与者每人参加了两次数据驱动的会议，其中显示了每名参与者的家中活动的视觉化。这些会议还收集了参与者对检测的准确性的反馈，以便进一步改进自动检测ADLs的方法。结论：通过对传感器和参与者反馈的组合使用关联规则挖掘，可以独立地检测每名参与者的ADLs，并且可以在所有参与者之间共享同一组则。这意味着可以在不需要更多的训练数据的情况下，将更多的参与者添加到NEX系统中。
</details></li>
</ul>
<hr>
<h2 id="Gradient-Surgery-for-One-shot-Unlearning-on-Generative-Model"><a href="#Gradient-Surgery-for-One-shot-Unlearning-on-Generative-Model" class="headerlink" title="Gradient Surgery for One-shot Unlearning on Generative Model"></a>Gradient Surgery for One-shot Unlearning on Generative Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04550">http://arxiv.org/abs/2307.04550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seohui Bae, Seoyoon Kim, Hyemin Jung, Woohyung Lim</li>
<li>for: 这篇论文目的是为了解释如何将数据的影响力除去 deep generative model 中。</li>
<li>methods: 本文提出了一个简单又有效的方法，通过将条件变数调整到正常方向，以干扰条件变数的影响。</li>
<li>results: 本文比较了现有的基eline，并提供了理论分析，证明了本方法可以高效地删除数据的影响。<details>
<summary>Abstract</summary>
Recent regulation on right-to-be-forgotten emerges tons of interest in unlearning pre-trained machine learning models. While approximating a straightforward yet expensive approach of retrain-from-scratch, recent machine unlearning methods unlearn a sample by updating weights to remove its influence on the weight parameters. In this paper, we introduce a simple yet effective approach to remove a data influence on the deep generative model. Inspired by works in multi-task learning, we propose to manipulate gradients to regularize the interplay of influence among samples by projecting gradients onto the normal plane of the gradients to be retained. Our work is agnostic to statistics of the removal samples, outperforming existing baselines while providing theoretical analysis for the first time in unlearning a generative model.
</details>
<details>
<summary>摘要</summary>
最近的强制忘记法规则促使大量关注在强制忘记前训练机器学习模型上。而 aproximating一个直接又昂贵的重新训练方法，现代机器忘记方法在一个样本上忘记，通过更新权重来消除其影响于权重参数。在这篇论文中，我们介绍了一种简单又有效的方法，用于从深度生成模型中移除数据的影响。受到多任务学习的启发，我们提议在欠拟合的情况下，对权重参数进行抑制，以 regularize各个样本之间的影响相互作用。我们的方法不依赖于废弃样本的统计特征，超越现有的基eline，并提供了理论分析。
</details></li>
</ul>
<hr>
<h2 id="StyleGAN2-based-Out-of-Distribution-Detection-for-Medical-Imaging"><a href="#StyleGAN2-based-Out-of-Distribution-Detection-for-Medical-Imaging" class="headerlink" title="StyleGAN2-based Out-of-Distribution Detection for Medical Imaging"></a>StyleGAN2-based Out-of-Distribution Detection for Medical Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10193">http://arxiv.org/abs/2307.10193</a></li>
<li>repo_url: None</li>
<li>paper_authors: McKell Woodland, John Wood, Caleb O’Connor, Ankit B. Patel, Kristy K. Brock</li>
<li>for: 这个研究旨在探讨一种基于生成敌方网络（GAN）的方法，用于检测显像处理器中的非常训练分布（out-of-distribution，OOD）图像。</li>
<li>methods: 这个研究使用了StyleGAN2-ADA架构，并使用了反射推导来重建图像。测试数据包括了脑、头颈、肺、阴道和异常肝脏的 Computed Tomography（CT）图像。</li>
<li>results: 研究发现，这个方法可以对CT图像进行非常好的分类，并且完全无法重建肝脏异常，如针和痰液。AUROC值高于90%，这表明这个方法可以实现非常好的OOD检测。<details>
<summary>Abstract</summary>
One barrier to the clinical deployment of deep learning-based models is the presence of images at runtime that lie far outside the training distribution of a given model. We aim to detect these out-of-distribution (OOD) images with a generative adversarial network (GAN). Our training dataset was comprised of 3,234 liver-containing computed tomography (CT) scans from 456 patients. Our OOD test data consisted of CT images of the brain, head and neck, lung, cervix, and abnormal livers. A StyleGAN2-ADA architecture was employed to model the training distribution. Images were reconstructed using backpropagation. Reconstructions were evaluated using the Wasserstein distance, mean squared error, and the structural similarity index measure. OOD detection was evaluated with the area under the receiver operating characteristic curve (AUROC). Our paradigm distinguished between liver and non-liver CT with greater than 90% AUROC. It was also completely unable to reconstruct liver artifacts, such as needles and ascites.
</details>
<details>
<summary>摘要</summary>
一个阻碍深度学习模型在临床应用的问题是运行时存在远离训练分布的图像。我们使用生成对抗网络（GAN）来探测这些外部分布（OOD）图像。我们的训练集包括3234个liver-包含 computed tomography（CT）扫描图像，来自456名患者。我们的OOD测试数据包括CT图像的脑、头颈、肺、颈部和异常liver。我们采用了StyleGAN2-ADA架构来模型训练分布。图像使用反射推导重建。重建图像的评估方法包括 Wasserstein 距离、平均方差和结构相似度指标。OOD检测的评估方法包括接受操作特征分布曲线（AUROC）。我们的方法可以在liver和非liver CT之间进行分类，AUROC高于90%。此外，我们的方法完全无法重建liver残留物，如针和液体。
</details></li>
</ul>
<hr>
<h2 id="Pathway-toward-prior-knowledge-integrated-machine-learning-in-engineering"><a href="#Pathway-toward-prior-knowledge-integrated-machine-learning-in-engineering" class="headerlink" title="Pathway toward prior knowledge-integrated machine learning in engineering"></a>Pathway toward prior knowledge-integrated machine learning in engineering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.06950">http://arxiv.org/abs/2307.06950</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xia Chen, Philipp Geyer</li>
<li>for: 本研究旨在整合多学科领域专业知识和数据驱动过程，以便将域知识传递到机器可识别的数据驱动过程中。</li>
<li>methods: 本研究使用多学科领域专业知识的整合和数据驱动技术，包括信息不确定性源的检查和知识归约三层知识集成机器学习平台。</li>
<li>results: 本研究可以均衡全景和分解视角，满足不同领域专业人员的需求，同时也能够利用域知识来提高数据驱动过程的精度和可靠性。<details>
<summary>Abstract</summary>
Despite the digitalization trend and data volume surge, first-principles models (also known as logic-driven, physics-based, rule-based, or knowledge-based models) and data-driven approaches have existed in parallel, mirroring the ongoing AI debate on symbolism versus connectionism. Research for process development to integrate both sides to transfer and utilize domain knowledge in the data-driven process is rare. This study emphasizes efforts and prevailing trends to integrate multidisciplinary domain professions into machine acknowledgeable, data-driven processes in a two-fold organization: examining information uncertainty sources in knowledge representation and exploring knowledge decomposition with a three-tier knowledge-integrated machine learning paradigm. This approach balances holist and reductionist perspectives in the engineering domain.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:不withstanding数字化趋势和数据量的急剧增长，首 принциples模型（也称为逻辑驱动、物理学驱动、规则驱动或知识驱动模型）和数据驱动方法在平行的方式存在，反映了人工智能中符号主义VS连接主义的讨论。对于过程发展来 integrate both sides的研究 rare。这种研究强调在机器可识别的数据驱动过程中 Multidisciplinary domain professions的集成，使用 two-fold 组织方式：对知识表示中的信息不确定源进行检查，并 explore knowledge decomposition with a three-tier knowledge-integrated machine learning paradigm。这种方法平衡了整体和分解的视角在工程领域。
</details></li>
</ul>
<hr>
<h2 id="DADO-–-Low-Cost-Selection-Strategies-for-Deep-Active-Design-Optimization"><a href="#DADO-–-Low-Cost-Selection-Strategies-for-Deep-Active-Design-Optimization" class="headerlink" title="DADO – Low-Cost Selection Strategies for Deep Active Design Optimization"></a>DADO – Low-Cost Selection Strategies for Deep Active Design Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04536">http://arxiv.org/abs/2307.04536</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jens Decke, Christian Gruhl, Lukas Rauch, Bernhard Sick</li>
<li>for: 降低计算成本的数值仿真 simulate computationally expensive numerical simulations.</li>
<li>methods: 使用深度学习自动学习来优化设计。</li>
<li>results: 提高设计优化的效率，减少计算成本。<details>
<summary>Abstract</summary>
In this experience report, we apply deep active learning to the field of design optimization to reduce the number of computationally expensive numerical simulations. We are interested in optimizing the design of structural components, where the shape is described by a set of parameters. If we can predict the performance based on these parameters and consider only the promising candidates for simulation, there is an enormous potential for saving computing power. We present two selection strategies for self-optimization to reduce the computational cost in multi-objective design optimization problems. Our proposed methodology provides an intuitive approach that is easy to apply, offers significant improvements over random sampling, and circumvents the need for uncertainty estimation. We evaluate our strategies on a large dataset from the domain of fluid dynamics and introduce two new evaluation metrics to determine the model's performance. Findings from our evaluation highlights the effectiveness of our selection strategies in accelerating design optimization. We believe that the introduced method is easily transferable to other self-optimization problems.
</details>
<details>
<summary>摘要</summary>
在这份经验报告中，我们运用深度活动学来降低计算成本的数字实验 simulation 的数量。我们关注设计结构元件的优化，其形状由一组参数描述。如果我们可以根据这些参数预测性能，只考虑计算成本较低的候选者，那么可以很大减少计算力量。我们提出了两种选择策略来减少多目标设计优化问题中的计算成本。我们的提出的方法是INTUITIVE的，易于实施，比Random Sampling更好，无需 uncertainty estimation。我们对大量的流体动力学数据集进行了评估，并引入了两种新的评价指标来评估模型的性能。我们的评估结果表明，我们的选择策略在加速设计优化方面具有显著的效果。我们认为引入的方法可以轻松地应用于其他自动优化问题。
</details></li>
</ul>
<hr>
<h2 id="QBitOpt-Fast-and-Accurate-Bitwidth-Reallocation-during-Training"><a href="#QBitOpt-Fast-and-Accurate-Bitwidth-Reallocation-during-Training" class="headerlink" title="QBitOpt: Fast and Accurate Bitwidth Reallocation during Training"></a>QBitOpt: Fast and Accurate Bitwidth Reallocation during Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04535">http://arxiv.org/abs/2307.04535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jorn Peters, Marios Fournarakis, Markus Nagel, Mart van Baalen, Tijmen Blankevoort</li>
<li>for: 提高 mobil 和嵌入式设备上的准确性，量化神经网络是一种非常有效的方法。特别是混合精度量化（MPQ）网络，其层可以被量化到不同的位宽，可以实现对同等资源限制下的任务性能更好的表现。</li>
<li>methods: 我们提出了一种名为QBitOpt的新算法，用于在量化感知训练（QAT）中更新位宽。我们将位宽分配问题解释为一个约束优化问题，并通过将快速计算敏感度与高效的解决器结合在一起，可以在QAT中生成混合精度网络，保证任务性能和严格的资源约束之间具有高度的匹配。</li>
<li>results: 我们在ImageNet上测试了QBitOpt，并证明了我们可以比存在fixed和混合精度方法的情况下，在常见的位宽约束下表现更好。<details>
<summary>Abstract</summary>
Quantizing neural networks is one of the most effective methods for achieving efficient inference on mobile and embedded devices. In particular, mixed precision quantized (MPQ) networks, whose layers can be quantized to different bitwidths, achieve better task performance for the same resource constraint compared to networks with homogeneous bitwidths. However, finding the optimal bitwidth allocation is a challenging problem as the search space grows exponentially with the number of layers in the network. In this paper, we propose QBitOpt, a novel algorithm for updating bitwidths during quantization-aware training (QAT). We formulate the bitwidth allocation problem as a constraint optimization problem. By combining fast-to-compute sensitivities with efficient solvers during QAT, QBitOpt can produce mixed-precision networks with high task performance guaranteed to satisfy strict resource constraints. This contrasts with existing mixed-precision methods that learn bitwidths using gradients and cannot provide such guarantees. We evaluate QBitOpt on ImageNet and confirm that we outperform existing fixed and mixed-precision methods under average bitwidth constraints commonly found in the literature.
</details>
<details>
<summary>摘要</summary>
“量化神经网络是一种最有效的方法来实现移动和嵌入式设备上快速的推理。特别是杂比例量化（MPQ）网络，它的层可以被量化到不同的比特宽，可以在同样的资源约束下提高任务性能。然而，寻找最佳比特宽分配是一个复杂的问题，因为搜索空间随着网络层数的增加而呈指数增长。在这篇论文中，我们提出了QBitOpt算法，它是一种在量化推理期间更新比特宽的算法。我们将比特宽分配问题形式化为一个约束优化问题。通过将快速计算的敏感度与高效的解决方案结合在一起，QBitOpt可以生成杂比例网络，保证任务性能高，同时也能够遵守常见的比特宽约束。这与现有的杂比例方法不同，它们通过梯度来学习比特宽，不能提供相同的保证。我们对ImageNet进行了评估，并证明了我们在平均比特宽约束下出perform existing固定和杂比例方法。”
</details></li>
</ul>
<hr>
<h2 id="Self-Expanding-Neural-Networks"><a href="#Self-Expanding-Neural-Networks" class="headerlink" title="Self Expanding Neural Networks"></a>Self Expanding Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04526">http://arxiv.org/abs/2307.04526</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ferzaad/Diabetes">https://github.com/ferzaad/Diabetes</a></li>
<li>paper_authors: Rupert Mitchell, Martin Mundt, Kristian Kersting</li>
<li>for: 这个研究旨在提出一种自适应的神经网络架构，可以灵活地调整网络的大小和深度，以减少训练过程中的损失。</li>
<li>methods: 本研究使用自然几何基于的方法，将网络的宽度和深度适当地调整，以降低训练过程中的损失。 authors 并证明了网络扩展的速率和扩展分数的下界。</li>
<li>results: 研究人员透过实验证明了自适应神经网络的有效性，在分类和回授问题中均有好的表现。此外， authors 还证明了这种自适应神经网络可以在网络的大小和深度不确定情况下表现良好。<details>
<summary>Abstract</summary>
The results of training a neural network are heavily dependent on the architecture chosen; and even a modification of only the size of the network, however small, typically involves restarting the training process. In contrast to this, we begin training with a small architecture, only increase its capacity as necessary for the problem, and avoid interfering with previous optimization while doing so. We thereby introduce a natural gradient based approach which intuitively expands both the width and depth of a neural network when this is likely to substantially reduce the hypothetical converged training loss. We prove an upper bound on the "rate" at which neurons are added, and a computationally cheap lower bound on the expansion score. We illustrate the benefits of such Self-Expanding Neural Networks in both classification and regression problems, including those where the appropriate architecture size is substantially uncertain a priori.
</details>
<details>
<summary>摘要</summary>
training一个神经网络的结果很受网络结构的影响，而 même一些小的修改会导致重新开始训练。相比之下，我们开始训练时选择小的网络 architecture，逐渐增加其容量，并避免在过程中对前一个优化器进行干扰。我们提出一种自然偏导的方法，其可以在可能减少假设的训练损失的情况下，自然地扩展神经网络的宽度和深度。我们证明了某种准则下的神经元数目增加的Upper bound，以及一种计算效率低的下界。我们还证明了这种自适应神经网络在分类和回归问题中的优势，包括一些预先不确定适应网络大小的问题。
</details></li>
</ul>
<hr>
<h2 id="Cluster-Induced-Mask-Transformers-for-Effective-Opportunistic-Gastric-Cancer-Screening-on-Non-contrast-CT-Scans"><a href="#Cluster-Induced-Mask-Transformers-for-Effective-Opportunistic-Gastric-Cancer-Screening-on-Non-contrast-CT-Scans" class="headerlink" title="Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans"></a>Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04525">http://arxiv.org/abs/2307.04525</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingze Yuan, Yingda Xia, Xin Chen, Jiawen Yao, Junli Wang, Mingyan Qiu, Hexin Dong, Jingren Zhou, Bin Dong, Le Lu, Li Zhang, Zaiyi Liu, Ling Zhang</li>
<li>for: 寻找一种可靠、低成本、质量高的胃癌检测方法，以帮助提高胃癌检测率和降低误 диагности Ratio。</li>
<li>methods: 我们提出了一种基于深度学习的方法，使用非对照CT扫描图像进行胃癌检测。我们的模型具有学习的团集，可以快速识别胃癌的特征，并且可以减少误分类的风险。</li>
<li>results: 我们的方法在一个包含100名癌症患者和148名正常人的测试集上实现了感知率85.0%和特征率92.6%。与两名医生的平均感知率（73.5%）和特征率（84.3%）相比，我们的方法表现更好。此外，我们在一个外部测试集上实现了特征率97.7%。这表明我们的方法可以作为一种新、不侵入、低成本、高精度的胃癌检测方法。<details>
<summary>Abstract</summary>
Gastric cancer is the third leading cause of cancer-related mortality worldwide, but no guideline-recommended screening test exists. Existing methods can be invasive, expensive, and lack sensitivity to identify early-stage gastric cancer. In this study, we explore the feasibility of using a deep learning approach on non-contrast CT scans for gastric cancer detection. We propose a novel cluster-induced Mask Transformer that jointly segments the tumor and classifies abnormality in a multi-task manner. Our model incorporates learnable clusters that encode the texture and shape prototypes of gastric cancer, utilizing self- and cross-attention to interact with convolutional features. In our experiments, the proposed method achieves a sensitivity of 85.0% and specificity of 92.6% for detecting gastric tumors on a hold-out test set consisting of 100 patients with cancer and 148 normal. In comparison, two radiologists have an average sensitivity of 73.5% and specificity of 84.3%. We also obtain a specificity of 97.7% on an external test set with 903 normal cases. Our approach performs comparably to established state-of-the-art gastric cancer screening tools like blood testing and endoscopy, while also being more sensitive in detecting early-stage cancer. This demonstrates the potential of our approach as a novel, non-invasive, low-cost, and accurate method for opportunistic gastric cancer screening.
</details>
<details>
<summary>摘要</summary>
Gastic cancer 是全球第三大的肿瘤癌症死亡原因，但没有任何指南推荐的检测试验。现有的方法可能是侵入的，昂贵的，并且缺乏感知力来检测早期肿瘤。在这项研究中，我们探索了使用深度学习方法来非侵入性的检测肿瘤。我们提出了一种新的帧Transformer，它同时分割肿瘤和识别异常。我们的模型包含学习的团队，这些团队编码了肿瘤癌症的文本和形状原型，并利用自我和交叉关注来交互 WITH  convolutional 特征。在我们的实验中，我们的方法实现了检测肿瘤的感知率为85.0%，特征率为92.6%。与两名放射学家的平均感知率（73.5%）和特征率（84.3%）相比，我们的方法表现出色。此外，我们在一个外部测试集上获得了特征率为97.7%，这个测试集包含903个正常的案例。我们的方法与已知的肿瘤癌症检测工具如血液测试和endoscopic一样有效，同时具有更高的感知力和更早的识别能力。这表明了我们的方法的潜在优势，作为一种新、非侵入的、低成本的、准确的肿瘤检测方法。
</details></li>
</ul>
<hr>
<h2 id="SAGC-A68-a-space-access-graph-dataset-for-the-classification-of-spaces-and-space-elements-in-apartment-buildings"><a href="#SAGC-A68-a-space-access-graph-dataset-for-the-classification-of-spaces-and-space-elements-in-apartment-buildings" class="headerlink" title="SAGC-A68: a space access graph dataset for the classification of spaces and space elements in apartment buildings"></a>SAGC-A68: a space access graph dataset for the classification of spaces and space elements in apartment buildings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04515">http://arxiv.org/abs/2307.04515</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/a2amir/sagc-a68">https://github.com/a2amir/sagc-a68</a></li>
<li>paper_authors: Amir Ziaee, Georg Suter</li>
<li>for: The paper is written for researchers and practitioners who are interested in developing Graph Deep Learning (GDL) models for space function and space element classification in the context of building design and analysis.</li>
<li>methods: The paper introduces a new dataset, SAGC-A68, which comprises access graphs automatically generated from 68 digital 3D models of space layouts of apartment buildings. The authors use this dataset to train and evaluate a graph attention network (GAT) that predicts 22 space function and 6 space element classes.</li>
<li>results: The authors demonstrate the potential of the dataset and the GAT model by achieving high accuracy rates on the test set. They also show that the GAT model outperforms other baseline models, indicating the effectiveness of using GDL methods for space function and space element classification.<details>
<summary>Abstract</summary>
The analysis of building models for usable area, building safety, and energy use requires accurate classification data of spaces and space elements. To reduce input model preparation effort and errors, automated classification of spaces and space elements is desirable. A barrier hindering the utilization of Graph Deep Learning (GDL) methods to space function and space element classification is a lack of suitable datasets. To bridge this gap, we introduce a dataset, SAGC-A68, which comprises access graphs automatically generated from 68 digital 3D models of space layouts of apartment buildings. This graph-based dataset is well-suited for developing GDL models for space function and space element classification. To demonstrate the potential of the dataset, we employ it to train and evaluate a graph attention network (GAT) that predicts 22 space function and 6 space element classes. The dataset and code used in the experiment are available online. https://doi.org/10.5281/zenodo.7805872, https://github.com/A2Amir/SAGC-A68.
</details>
<details>
<summary>摘要</summary>
analysis of building models for usable area, building safety, and energy use requires accurate classification data of spaces and space elements. to reduce input model preparation effort and errors, automated classification of spaces and space elements is desirable. a barrier hindering the utilization of graph deep learning (gdl) methods to space function and space element classification is a lack of suitable datasets. to bridge this gap, we introduce a dataset, sagc-a68, which comprises access graphs automatically generated from 68 digital 3d models of space layouts of apartment buildings. this graph-based dataset is well-suited for developing gdl models for space function and space element classification. to demonstrate the potential of the dataset, we employ it to train and evaluate a graph attention network (gat) that predicts 22 space function and 6 space element classes. the dataset and code used in the experiment are available online. https://doi.org/10.5281/zenodo.7805872, https://github.com/A2Amir/SAGC-A68.Here's the translation in Traditional Chinese:分析建筑模型的用able面积、建筑安全和能源使用需要准确的分类数据空间和空间元素。为了减少输入模型准备的劳动和错误，自动分类空间和空间元素是可能的。 however，使用图深度学习（GDL）方法进行空间功能和空间元素分类存在一个障碍：缺乏适合的数据集。为了跨越这一障碍，我们介绍了一个数据集，SAGC-A68，该数据集包括自动生成的68个数字3D模型的空间布局的访问图。这个图基本数据集非常适合开发GDL模型进行空间功能和空间元素分类。为了证明该数据集的潜力，我们使用它来训练和评估一个图注意力网络（GAT），该网络预测22个空间功能和6个空间元素类型。该数据集和实验代码在线可用。https://doi.org/10.5281/zenodo.7805872, https://github.com/A2Amir/SAGC-A68。
</details></li>
</ul>
<hr>
<h2 id="Improving-Heterogeneous-Graph-Learning-with-Weighted-Mixed-Curvature-Product-Manifold"><a href="#Improving-Heterogeneous-Graph-Learning-with-Weighted-Mixed-Curvature-Product-Manifold" class="headerlink" title="Improving Heterogeneous Graph Learning with Weighted Mixed-Curvature Product Manifold"></a>Improving Heterogeneous Graph Learning with Weighted Mixed-Curvature Product Manifold</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04514">http://arxiv.org/abs/2307.04514</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sharecodesubmission/weighted_product_manifold">https://github.com/sharecodesubmission/weighted_product_manifold</a></li>
<li>paper_authors: Tuc Nguyen-Van, Dung D. Le, The-Anh Ta</li>
<li>for: 该论文主要研究 graphs 的 embedding 问题，即如何将 graphs 转换为低维度的 vector 空间中的表示，以便进行后续的计算和分析。</li>
<li>methods: 该论文提出了一种基于 weighted product manifolds 的方法，即 WEIGHTED-PM，来解决 graphs 的 embedding 问题。该方法利用输入 graph 的 topological 信息自动地确定每个 ком成分空间在 product spaces 中的权重，从而更好地表示输入 graph 的复杂结构。</li>
<li>results: 该论文通过 extensively 的实验表明，WEIGHTED-PM 方法可以从输入数据中学习更好的 graph 表示，并且在多个下游任务中表现更好，如 word similarity learning、top-$k$ recommendation 和 knowledge graph embedding。<details>
<summary>Abstract</summary>
In graph representation learning, it is important that the complex geometric structure of the input graph, e.g. hidden relations among nodes, is well captured in embedding space. However, standard Euclidean embedding spaces have a limited capacity in representing graphs of varying structures. A promising candidate for the faithful embedding of data with varying structure is product manifolds of component spaces of different geometries (spherical, hyperbolic, or euclidean). In this paper, we take a closer look at the structure of product manifold embedding spaces and argue that each component space in a product contributes differently to expressing structures in the input graph, hence should be weighted accordingly. This is different from previous works which consider the roles of different components equally. We then propose WEIGHTED-PM, a data-driven method for learning embedding of heterogeneous graphs in weighted product manifolds. Our method utilizes the topological information of the input graph to automatically determine the weight of each component in product spaces. Extensive experiments on synthetic and real-world graph datasets demonstrate that WEIGHTED-PM is capable of learning better graph representations with lower geometric distortion from input data, and performs better on multiple downstream tasks, such as word similarity learning, top-$k$ recommendation, and knowledge graph embedding.
</details>
<details>
<summary>摘要</summary>
在图表示学中，capturing复杂的图形结构，如隐藏的节点关系，在嵌入空间中是非常重要的。然而，标准的欧几何嵌入空间有限的表示能力，无法捕捉不同结构的图。一种有前途的方法是使用不同geometry的组件空间的产品拓扑（球面、尖锥形、欧几何）。在这篇文章中，我们做了产品拓扑 embedding 空间的结构分析，并 argue that每个组件空间在产品中都有不同的表达效果，因此应该给予不同的权重。这与之前的作品不同，一般认为所有组件都应该具有相同的角色。我们然后提出了WEIGHTED-PM，一种数据驱动的方法，用于学习异类图的嵌入。我们的方法利用输入图的topological信息来自动确定每个组件在产品空间中的权重。我们的实验表明，WEIGHTED-PM可以从输入数据中学习更好的图表示，并且在多个下游任务中表现更好，如word相似度学习、top-$k$推荐和知识图嵌入。
</details></li>
</ul>
<hr>
<h2 id="An-Algorithm-with-Optimal-Dimension-Dependence-for-Zero-Order-Nonsmooth-Nonconvex-Stochastic-Optimization"><a href="#An-Algorithm-with-Optimal-Dimension-Dependence-for-Zero-Order-Nonsmooth-Nonconvex-Stochastic-Optimization" class="headerlink" title="An Algorithm with Optimal Dimension-Dependence for Zero-Order Nonsmooth Nonconvex Stochastic Optimization"></a>An Algorithm with Optimal Dimension-Dependence for Zero-Order Nonsmooth Nonconvex Stochastic Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04504">http://arxiv.org/abs/2307.04504</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guy Kornowski, Ohad Shamir</li>
<li>for: 本研究探讨了使用杂变函数评估的$( \delta, \epsilon )$-稳定点优化问题的复杂性，并提出了一种 faster algorithm，其复杂度为 $O(d\delta^{-1}\epsilon^{-3})$，这是对维度 $d$ 和精度参数 $\delta,\epsilon$ 的优化。</li>
<li>methods: 本研究使用了随机零次算法来解决该问题，并提出了一种新的方法，该方法的复杂度为 $O(d\delta^{-1}\epsilon^{-3})$，这是对维度 $d$ 和精度参数 $\delta,\epsilon$ 的优化。</li>
<li>results: 本研究的结果表明，我们的算法可以在几乎所有情况下达到最优的 convergence rate，并且在满足certain condition下，我们的算法可以在随机扰动下达到最优的 convergence rate。此外，我们的分析还证明了在非 convex 随机零次设定下，nonsmooth 优化与 smooth 优化之间存在等价关系。<details>
<summary>Abstract</summary>
We study the complexity of producing $(\delta,\epsilon)$-stationary points of Lipschitz objectives which are possibly neither smooth nor convex, using only noisy function evaluations. Recent works proposed several stochastic zero-order algorithms that solve this task, all of which suffer from a dimension-dependence of $\Omega(d^{3/2})$ where $d$ is the dimension of the problem, which was conjectured to be optimal. We refute this conjecture by providing a faster algorithm that has complexity $O(d\delta^{-1}\epsilon^{-3})$, which is optimal (up to numerical constants) with respect to $d$ and also optimal with respect to the accuracy parameters $\delta,\epsilon$, thus solving an open question due to Lin et al. (NeurIPS'22). Moreover, the convergence rate achieved by our algorithm is also optimal for smooth objectives, proving that in the nonconvex stochastic zero-order setting, nonsmooth optimization is as easy as smooth optimization. We provide algorithms that achieve the aforementioned convergence rate in expectation as well as with high probability. Our analysis is based on a simple yet powerful geometric lemma regarding the Goldstein-subdifferential set, which allows utilizing recent advancements in first-order nonsmooth nonconvex optimization.
</details>
<details>
<summary>摘要</summary>
我们研究生成($\delta$, $\epsilon$)-稳定点的复杂性，这些目标可能不对称，并且可能不 глад。我们使用仅有抽象函数评估来进行。近期的研究提出了一些测量零次法，解决了这个任务，但是所有这些方法都受到了维度($d$)的依赖性，即$\Omega(d^{3/2})$。我们推翻了这个推论，提出了一个更快的算法，其复杂度为$O(d\delta^{-1}\epsilon^{-3})$，它在$d$和精度参数$\delta,\epsilon$方面都是最佳（即对应的常数），解决了林等人（NeurIPS'22）所提出的开问题。此外，我们的算法还可以在光滑目标上 достичь最佳的凝固率，证明了在非对称零次法设定中，非光滑优化是与光滑优化一样容易。我们提供了在预期中和高概率下实现的算法，我们的分析基于一个简单又强大的几何学 lemma，允许我们利用最近的非光滑非对称非凝固优化的进步。
</details></li>
</ul>
<hr>
<h2 id="Geometric-Constraints-in-Probabilistic-Manifolds-A-Bridge-from-Molecular-Dynamics-to-Structured-Diffusion-Processes"><a href="#Geometric-Constraints-in-Probabilistic-Manifolds-A-Bridge-from-Molecular-Dynamics-to-Structured-Diffusion-Processes" class="headerlink" title="Geometric Constraints in Probabilistic Manifolds: A Bridge from Molecular Dynamics to Structured Diffusion Processes"></a>Geometric Constraints in Probabilistic Manifolds: A Bridge from Molecular Dynamics to Structured Diffusion Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04493">http://arxiv.org/abs/2307.04493</a></li>
<li>repo_url: None</li>
<li>paper_authors: Justin Diamond, Markus Lill</li>
<li>for: 这篇论文是为了解决生物复杂体系的宏观特征特征模型中的精度和特点问题，具体来说是在Euclidean空间中采样特定子集的问题。</li>
<li>methods: 该论文提出了一种将约束 projetion 算法纳入杜伦抑杂概率模型中的方法，以便在Euclidean空间中采样受约束的分布。</li>
<li>results: 该方法可以帮助实现在深度学习基础上的药物设计中维护特定分子对话的需要，以确保治疗效果和安全性。<details>
<summary>Abstract</summary>
Understanding the macroscopic characteristics of biological complexes demands precision and specificity in statistical ensemble modeling. One of the primary challenges in this domain lies in sampling from particular subsets of the state-space, driven either by existing structural knowledge or specific areas of interest within the state-space. We propose a method that enables sampling from distributions that rigorously adhere to arbitrary sets of geometric constraints in Euclidean spaces. This is achieved by integrating a constraint projection operator within the well-regarded architecture of Denoising Diffusion Probabilistic Models, a framework founded in generative modeling and probabilistic inference. The significance of this work becomes apparent, for instance, in the context of deep learning-based drug design, where it is imperative to maintain specific molecular profile interactions to realize the desired therapeutic outcomes and guarantee safety.
</details>
<details>
<summary>摘要</summary>
理解生物复杂体系的宏观特征需要精度和特点的统计ensemble模型。这个领域的主要挑战在于采样特定的子集state-space，由于现有的结构知识或特定区域的 интересы。我们提出了一种方法，可以准确地采样符合任意的几何约束的分布在欧几何空间中。这是通过将约束投影Operator интеGRATE到了Denosing Diffusion Probabilistic Models的可靠的建筑基础上实现的。这种方法的重要性在深度学习基础设计中 particualry evident，因为需要保持特定的分子 profil interactio来实现所需的治疗效果和安全性。
</details></li>
</ul>
<hr>
<h2 id="Invertible-Low-Dimensional-Modelling-of-X-ray-Absorption-Spectra-for-Potential-Applications-in-Spectral-X-ray-Imaging"><a href="#Invertible-Low-Dimensional-Modelling-of-X-ray-Absorption-Spectra-for-Potential-Applications-in-Spectral-X-ray-Imaging" class="headerlink" title="Invertible Low-Dimensional Modelling of X-ray Absorption Spectra for Potential Applications in Spectral X-ray Imaging"></a>Invertible Low-Dimensional Modelling of X-ray Absorption Spectra for Potential Applications in Spectral X-ray Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04484">http://arxiv.org/abs/2307.04484</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raziye Kubra Kumrular, Thomas Blumensath</li>
<li>for: 这个论文主要是为了提出一种新的非线性模型，用于 spectral X-ray imaging 中的数据压缩、噪声除除、spectral 估计和材料组成的量化测量。</li>
<li>methods: 该论文提出了一种新的模型，它combines一个深度神经网络自编器和一个最佳的线性模型基于 Singular Value Decomposition (SVD)。</li>
<li>results: 作者比较了这种新方法与其他线性和非线性方法，包括一种简单的模型和一种另一种深度学习模型。结果显示，该新方法在模拟 X-ray 吸收谱中的 K-edge 区域时表现更优异。<details>
<summary>Abstract</summary>
X-ray interaction with matter is an energy-dependent process that is contingent on the atomic structure of the constituent material elements. The most advanced models to capture this relationship currently rely on Monte Carlo (MC) simulations. Whilst these very accurate models, in many problems in spectral X-ray imaging, such as data compression, noise removal, spectral estimation, and the quantitative measurement of material compositions, these models are of limited use, as these applications typically require the efficient inversion of the model, that is, they require the estimation of the best model parameters for a given spectral measurement. Current models that can be easily inverted however typically only work when modelling spectra in regions away from their K-edges, so they have limited utility when modelling a wider range of materials. In this paper, we thus propose a novel, non-linear model that combines a deep neural network autoencoder with an optimal linear model based on the Singular Value Decomposition (SVD). We compare our new method to other alternative linear and non-linear approaches, a sparse model and an alternative deep learning model. We demonstrate the advantages of our method over traditional models, especially when modelling X-ray absorption spectra that contain K-edges in the energy range of interest.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们因此提出了一种新的、非线性模型，它结合了深度神经网络自适应器和基于Singular Value Decomposition（SVD）的最佳线性模型。我们与其他的线性和非线性方法进行比较，包括一种稀疏模型和一种另一种深度学习模型。我们示出了我们的方法在传统模型的基础上具有优势，特别是当模型X射线吸收谱在能量范围内时。
</details></li>
</ul>
<hr>
<h2 id="Badgers-generating-data-quality-deficits-with-Python"><a href="#Badgers-generating-data-quality-deficits-with-Python" class="headerlink" title="Badgers: generating data quality deficits with Python"></a>Badgers: generating data quality deficits with Python</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04468">http://arxiv.org/abs/2307.04468</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fraunhofer-iese/badgers">https://github.com/fraunhofer-iese/badgers</a></li>
<li>paper_authors: Julien Siebert, Daniel Seifert, Patricia Kelbert, Michael Kläs, Adam Trendowicz</li>
<li>for: 这篇论文的目的是为了实验性评估数据驱动（人工智能（AI）或机器学习（ML））应用中的数据质量问题。</li>
<li>methods: 这篇论文使用了一个名为“badgers”的Python库，该库可以生成不同类型的数据质量缺陷（例如异常值、数据不均衡、漂移等），以便对数据驱动应用的数据质量进行实验性评估。</li>
<li>results: 这篇论文通过使用“badgers”库，可以生成不同类型的数据质量缺陷，以便对数据驱动应用的数据质量进行实验性评估。 documentation 可以在<a target="_blank" rel="noopener" href="https://fraunhofer-iese.github.io/badgers/">https://fraunhofer-iese.github.io/badgers/</a> 中找到，源代码可以在<a target="_blank" rel="noopener" href="https://github.com/Fraunhofer-IESE/badgers">https://github.com/Fraunhofer-IESE/badgers</a> 中找到。<details>
<summary>Abstract</summary>
Generating context specific data quality deficits is necessary to experimentally assess data quality of data-driven (artificial intelligence (AI) or machine learning (ML)) applications. In this paper we present badgers, an extensible open-source Python library to generate data quality deficits (outliers, imbalanced data, drift, etc.) for different modalities (tabular data, time-series, text, etc.). The documentation is accessible at https://fraunhofer-iese.github.io/badgers/ and the source code at https://github.com/Fraunhofer-IESE/badgers
</details>
<details>
<summary>摘要</summary>
生成模式特定数据质量缺陷是评估数据驱动（人工智能（AI）或机器学习（ML））应用的实验需求。本文介绍badgers，一个可扩展的开源Python库，用于生成不同类型的数据质量缺陷（异常值、不均衡数据、漂移等）。文档可以在https://fraunhofer-iese.github.io/badgers/中查看，代码在https://github.com/Fraunhofer-IESE/badgers上可以查看。
</details></li>
</ul>
<hr>
<h2 id="Multi-modal-Graph-Learning-over-UMLS-Knowledge-Graphs"><a href="#Multi-modal-Graph-Learning-over-UMLS-Knowledge-Graphs" class="headerlink" title="Multi-modal Graph Learning over UMLS Knowledge Graphs"></a>Multi-modal Graph Learning over UMLS Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04461">http://arxiv.org/abs/2307.04461</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manuel Burger, Gunnar Rätsch, Rita Kuznetsova</li>
<li>for: 预测病人患病进程的多次医院访问（Multiple Hospital Visits）</li>
<li>methods: 使用图 neural network 学习医学概念的有意义表示，并将其组合为代表整个病人访问的表示。</li>
<li>results: 比较于现有的建筑方案，提高了表示多模态医学概念的性能，并且证明了在医学知识的支持下，多模态医学概念表示具有重要的意义。In English, this means:</li>
<li>for: Predicting the progression of patient illnesses across multiple hospital visits</li>
<li>methods: Using graph neural networks to learn meaningful representations of medical concepts, and combining them to represent entire patient visits.</li>
<li>results: Outperforming existing architectures in representing multiple modalities of medical concepts, and demonstrating the significance of incorporating prior medical knowledge.<details>
<summary>Abstract</summary>
Clinicians are increasingly looking towards machine learning to gain insights about patient evolutions. We propose a novel approach named Multi-Modal UMLS Graph Learning (MMUGL) for learning meaningful representations of medical concepts using graph neural networks over knowledge graphs based on the unified medical language system. These representations are aggregated to represent entire patient visits and then fed into a sequence model to perform predictions at the granularity of multiple hospital visits of a patient. We improve performance by incorporating prior medical knowledge and considering multiple modalities. We compare our method to existing architectures proposed to learn representations at different granularities on the MIMIC-III dataset and show that our approach outperforms these methods. The results demonstrate the significance of multi-modal medical concept representations based on prior medical knowledge.
</details>
<details>
<summary>摘要</summary>
临床医生们正在寻求机器学习技术以获取病人发展情况的洞察。我们提出了一种新的方法名为多modal UMLS图像学习（MMUGL），该方法利用知识图库基于统一医学语言系统（UMLS）中的医学概念进行图神经网络学习，以获取有用的医学概念表示。这些表示被聚合以表示整个病人访问，然后通过顺序模型进行预测，以达到多个医院访问病人的级别。我们通过 incorporating 先前的医学知识和考虑多种感知modalities来提高性能。我们对MIMIC-III数据集中的不同层次的表示学习体系进行比较，并显示了我们的方法在这些方法之上出performances。结果表明多modal医学概念表示基于先前的医学知识具有重要性。
</details></li>
</ul>
<hr>
<h2 id="Invex-Programs-First-Order-Algorithms-and-Their-Convergence"><a href="#Invex-Programs-First-Order-Algorithms-and-Their-Convergence" class="headerlink" title="Invex Programs: First Order Algorithms and Their Convergence"></a>Invex Programs: First Order Algorithms and Their Convergence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04456">http://arxiv.org/abs/2307.04456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adarsh Barik, Suvrit Sra, Jean Honorio</li>
<li>for: 解决非对称问题，即具有全局最小值的每个站点点。</li>
<li>methods: 提出了新的一代首选法，并证明其 converge 的条件和速率。</li>
<li>results: 提供了一种新的投影法，可以解决约束的非对称问题，并提供了速率 guarantees。<details>
<summary>Abstract</summary>
Invex programs are a special kind of non-convex problems which attain global minima at every stationary point. While classical first-order gradient descent methods can solve them, they converge very slowly. In this paper, we propose new first-order algorithms to solve the general class of invex problems. We identify sufficient conditions for convergence of our algorithms and provide rates of convergence. Furthermore, we go beyond unconstrained problems and provide a novel projected gradient method for constrained invex programs with convergence rate guarantees. We compare and contrast our results with existing first-order algorithms for a variety of unconstrained and constrained invex problems. To the best of our knowledge, our proposed algorithm is the first algorithm to solve constrained invex programs.
</details>
<details>
<summary>摘要</summary>
“inx problems是一种特殊的非凸问题，它们在每个稳定点上取得全球最小值。 classical first-order gradient descent方法可以解决它们，但是它们对应的速度很慢。在这篇论文中，我们提出了一些新的first-order算法来解决一般的inx问题。我们识别出了它们的充分条件，并提供了速度 guarantee。此外，我们不仅处理不受限制的问题，而且提出了一个新的对应投影法，它具有速度 guarantee。我们与现有的first-order算法进行比较和对照，并证明了我们的提案是第一个可以解决约束inx问题的算法。”Note: The translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Graph-Convolutional-Networks-for-Simulating-Multi-phase-Flow-and-Transport-in-Porous-Media"><a href="#Graph-Convolutional-Networks-for-Simulating-Multi-phase-Flow-and-Transport-in-Porous-Media" class="headerlink" title="Graph Convolutional Networks for Simulating Multi-phase Flow and Transport in Porous Media"></a>Graph Convolutional Networks for Simulating Multi-phase Flow and Transport in Porous Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04449">http://arxiv.org/abs/2307.04449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiamin Jiang, Bo Guo</li>
<li>for:  numerical simulation of multi-phase fluid dynamics in porous media</li>
<li>methods:  data-driven surrogate modeling using Graph Convolutional Networks (GCNs)</li>
<li>results:  high accuracy in predicting pressure and saturation states, and generalization to irregular domain geometries and unstructured meshes.Here is the same information in Simplified Chinese:</li>
<li>for:  numerical simulation of多相流体动力学在材料孔隙中</li>
<li>methods: 基于图像 convolutional neural networks (GCNs) 的数据驱动模拟</li>
<li>results: 高精度地预测压力和湿度状态的演变，并可以通过不同的域几何和粗粒度网格来扩展。<details>
<summary>Abstract</summary>
Numerical simulation of multi-phase fluid dynamics in porous media is critical for many subsurface applications. Data-driven surrogate modeling provides computationally inexpensive alternatives to high-fidelity numerical simulators. While the commonly used convolutional neural networks (CNNs) are powerful in approximating partial differential equation solutions, it remains challenging for CNNs to handle irregular and unstructured simulation meshes. However, subsurface simulation models often involve unstructured meshes with complex mesh geometries, which limits the application of CNNs. To address this challenge, here we construct surrogate models based on Graph Convolutional Networks (GCNs) to approximate the spatial-temporal solutions of multi-phase flow and transport processes. We propose a new GCN architecture suited to the hyperbolic character of the coupled PDE system, to better capture the saturation dynamics. Results of 2D heterogeneous test cases show that our surrogates predict the evolutions of the pressure and saturation states with high accuracy, and the predicted rollouts remain stable for multiple timesteps. Moreover, the GCN-based models generalize well to irregular domain geometries and unstructured meshes that are unseen in the training dataset.
</details>
<details>
<summary>摘要</summary>
numerically simulating multi-phase fluid dynamics in porous media is crucial for many subsurface applications. data-driven surrogate modeling provides computationally inexpensive alternatives to high-fidelity numerical simulators. although commonly used convolutional neural networks (CNNs) are powerful in approximating partial differential equation solutions, it remains challenging for CNNs to handle irregular and unstructured simulation meshes. however, subsurface simulation models often involve unstructured meshes with complex mesh geometries, which limits the application of CNNs. to address this challenge, we construct surrogate models based on graph convolutional networks (GCNs) to approximate the spatial-temporal solutions of multi-phase flow and transport processes. we propose a new GCN architecture suited to the hyperbolic character of the coupled PDE system, to better capture the saturation dynamics. results of 2D heterogeneous test cases show that our surrogates predict the evolutions of the pressure and saturation states with high accuracy, and the predicted rollouts remain stable for multiple timesteps. moreover, the GCN-based models generalize well to irregular domain geometries and unstructured meshes that are unseen in the training dataset.
</details></li>
</ul>
<hr>
<h2 id="Learning-Behavioral-Representations-of-Routines-From-Large-scale-Unlabeled-Wearable-Time-series-Data-Streams-using-Hawkes-Point-Process"><a href="#Learning-Behavioral-Representations-of-Routines-From-Large-scale-Unlabeled-Wearable-Time-series-Data-Streams-using-Hawkes-Point-Process" class="headerlink" title="Learning Behavioral Representations of Routines From Large-scale Unlabeled Wearable Time-series Data Streams using Hawkes Point Process"></a>Learning Behavioral Representations of Routines From Large-scale Unlabeled Wearable Time-series Data Streams using Hawkes Point Process</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04445">http://arxiv.org/abs/2307.04445</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiantian Feng, Brandon M Booth, Shrikanth Narayanan</li>
<li>for: 这篇论文旨在探讨如何通过不带任何标签的穿戴式传感器数据来探索人们的日常活动模式。</li>
<li>methods: 该论文提出了一种基于时间序列 clustering 和 Hawkes 点过程学习算法的新方法，可以从无标签时间序列数据中探索人们的日常行为模式。</li>
<li>results: 研究人员通过使用该方法，成功地探索了100多名参与者的日常活动模式，并发现了每天的活动转换关系。此外，该方法还能够捕捉到各个人的个性特征和情绪变化。<details>
<summary>Abstract</summary>
Continuously-worn wearable sensors enable researchers to collect copious amounts of rich bio-behavioral time series recordings of real-life activities of daily living, offering unprecedented opportunities to infer novel human behavior patterns during daily routines. Existing approaches to routine discovery through bio-behavioral data rely either on pre-defined notions of activities or use additional non-behavioral measurements as contexts, such as GPS location or localization within the home, presenting risks to user privacy. In this work, we propose a novel wearable time-series mining framework, Hawkes point process On Time series clusters for ROutine Discovery (HOT-ROD), for uncovering behavioral routines from completely unlabeled wearable recordings. We utilize a covariance-based method to generate time-series clusters and discover routines via the Hawkes point process learning algorithm. We empirically validate our approach for extracting routine behaviors using a completely unlabeled time-series collected continuously from over 100 individuals both in and outside of the workplace during a period of ten weeks. Furthermore, we demonstrate this approach intuitively captures daily transitional relationships between physical activity states without using prior knowledge. We also show that the learned behavioral patterns can assist in illuminating an individual's personality and affect.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用不间断搭配的便携式传感器，研究人员可以收集大量的丰富的生物行为时序记录，以获得前无之例的人类行为模式发现机会。现有的 Routine 发现方法通过生物行为数据，都是基于先前定义的活动或者使用其他非行为测量，如 GPS 位置或家庭内部的位置，这些方法存在用户隐私风险。在这项工作中，我们提出了一种基于 Hawkes 点过程的时序序分 clustering 框架，称为 HOT-ROD，用于从未标记的便携式记录中探索行为 Routine。我们使用 covariance 基本方法生成时序序 clusters，通过 Hawkes 点过程学习算法发现 Routine。我们经验 Validate 我们的方法可以从未标记的时序记录中提取行为 Routine。我们使用了来自100名参与者的完全无标记时序记录，在工作场所和外部环境中进行了10周的观察。此外，我们还示出了我们的方法可以直观地捕捉日常physical activity状态之间的关系，无需使用先前知识。最后，我们还示出了学习的行为模式可以帮助描述个体的人性和情感。
</details></li>
</ul>
<hr>
<h2 id="Search-time-Efficient-Device-Constraints-Aware-Neural-Architecture-Search"><a href="#Search-time-Efficient-Device-Constraints-Aware-Neural-Architecture-Search" class="headerlink" title="Search-time Efficient Device Constraints-Aware Neural Architecture Search"></a>Search-time Efficient Device Constraints-Aware Neural Architecture Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04443">http://arxiv.org/abs/2307.04443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oshin Dutta, Tanu Kanvar, Sumeet Agarwal</li>
<li>for: 这篇论文的目的是提出一种自动化建立适合边缘设备的对应深度学习架构，以减少边缘设备的资源需求。</li>
<li>methods: 这篇论文使用了对应深度学习架构搜索（Neural Architecture Search，NAS）来自动化建立适合边缘设备的架构。它还使用了Weight sharing和Channel bottleneck技术来加速搜索过程。</li>
<li>results: 根据实验结果，DCA-NAS比对应深度学习架构更好地适应边缘设备的硬件限制，并且与流行的 mobil 架构相比，在不同的图像识别 datasets 上具有相似的表现。此外，DCA-NAS 还可以在 Hardware-NAS-Bench 上进行硬件特定的架构搜索，实现低 inference 延迟和现场表现。<details>
<summary>Abstract</summary>
Edge computing aims to enable edge devices, such as IoT devices, to process data locally instead of relying on the cloud. However, deep learning techniques like computer vision and natural language processing can be computationally expensive and memory-intensive. Creating manual architectures specialized for each device is infeasible due to their varying memory and computational constraints. To address these concerns, we automate the construction of task-specific deep learning architectures optimized for device constraints through Neural Architecture Search (NAS). We present DCA-NAS, a principled method of fast neural network architecture search that incorporates edge-device constraints such as model size and floating-point operations. It incorporates weight sharing and channel bottleneck techniques to speed up the search time. Based on our experiments, we see that DCA-NAS outperforms manual architectures for similar sized models and is comparable to popular mobile architectures on various image classification datasets like CIFAR-10, CIFAR-100, and Imagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the generalization capabilities of DCA-NAS. On further evaluating our approach on Hardware-NAS-Bench, device-specific architectures with low inference latency and state-of-the-art performance were discovered.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)边计算旨在让边缘设备，如物联网设备，直接处理本地数据而不依赖于云端。然而，深度学习技术如计算机视觉和自然语言处理可能具有计算成本和内存占用的问题。为了解决这些问题，我们自动构建任务特定的深度学习架构，以适应边缘设备的约束。我们提出了DCA-NAS，一种基于约束的快速神经网络架构搜索方法，该方法包括边缘设备的模型大小和浮点运算数量等约束。它还包括权重共享和通道瓶颈技术来加速搜索时间。根据我们的实验，DCA-NAS在相同大小的模型中表现更好，与流行的移动设备架构相当，并在多个图像分类 datasets  like CIFAR-10, CIFAR-100, Imagenet-1k 上达到了类似的性能。我们的方法在 DARTS 和 NAS-Bench-201 的搜索空间上进行了广泛的测试，并证明了DCA-NAS的普适性。在进一步评估我们的方法在 Hardware-NAS-Bench 上时，我们发现了适用于具体的设备的低延迟和状态公平的设备特定架构。
</details></li>
</ul>
<hr>
<h2 id="Designing-Novel-Cognitive-Diagnosis-Models-via-Evolutionary-Multi-Objective-Neural-Architecture-Search"><a href="#Designing-Novel-Cognitive-Diagnosis-Models-via-Evolutionary-Multi-Objective-Neural-Architecture-Search" class="headerlink" title="Designing Novel Cognitive Diagnosis Models via Evolutionary Multi-Objective Neural Architecture Search"></a>Designing Novel Cognitive Diagnosis Models via Evolutionary Multi-Objective Neural Architecture Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04429">http://arxiv.org/abs/2307.04429</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/devilyangs/emo-nas-cd">https://github.com/devilyangs/emo-nas-cd</a></li>
<li>paper_authors: Shangshang Yang, Haiping Ma, Cheng Zhen, Ye Tian, Limiao Zhang, Yaochu Jin, Xingyi Zhang</li>
<li>for: 提高现代智能教育平台中学生知识概念掌握的精度和效率，通过自动设计新的认知诊断模型。</li>
<li>methods: 使用进化多目标神经网络架构搜索（NAS）自动设计新的认知诊断模型，并maximize模型性能和可解释性。</li>
<li>results: 通过实验表明，提posed方法可以寻找比现有模型更好的认知诊断模型，同时保持与人类设计的模型相同的可解释性。<details>
<summary>Abstract</summary>
Cognitive diagnosis plays a vital role in modern intelligent education platforms to reveal students' proficiency in knowledge concepts for subsequent adaptive tasks. However, due to the requirement of high model interpretability, existing manually designed cognitive diagnosis models hold too simple architectures to meet the demand of current intelligent education systems, where the bias of human design also limits the emergence of effective cognitive diagnosis models. In this paper, we propose to automatically design novel cognitive diagnosis models by evolutionary multi-objective neural architecture search (NAS). Specifically, we observe existing models can be represented by a general model handling three given types of inputs and thus first design an expressive search space for the NAS task in cognitive diagnosis. Then, we propose multi-objective genetic programming (MOGP) to explore the NAS task's search space by maximizing model performance and interpretability. In the MOGP design, each architecture is transformed into a tree architecture and encoded by a tree for easy optimization, and a tailored genetic operation based on four sub-genetic operations is devised to generate offspring effectively. Besides, an initialization strategy is also suggested to accelerate the convergence by evolving half of the population from existing models' variants. Experiments on two real-world datasets demonstrate that the cognitive diagnosis models searched by the proposed approach exhibit significantly better performance than existing models and also hold as good interpretability as human-designed models.
</details>
<details>
<summary>摘要</summary>
现代智能教育平台中，认知诊断发挥了关键作用，以揭示学生知识概念的熟练程度，以便进行后续适应任务。然而，由于需要高度的模型解释性，现有的手动设计的认知诊断模型具有过于简单的结构，无法满足当前智能教育系统的需求。在这篇论文中，我们提议使用EVOLUTIONARY MULTI-OBJECTIVE NEURAL ARCHITECTURE SEARCH（NAS）自动设计新的认知诊断模型。 Specifically, we observe that existing models can be represented by a general model handling three given types of inputs, and thus first design an expressive search space for the NAS task in cognitive diagnosis. Then, we propose multi-objective genetic programming (MOGP) to explore the NAS task's search space by maximizing model performance and interpretability. In the MOGP design, each architecture is transformed into a tree architecture and encoded by a tree for easy optimization, and a tailored genetic operation based on four sub-genetic operations is devised to generate offspring effectively. Besides, an initialization strategy is also suggested to accelerate the convergence by evolving half of the population from existing models' variants. Experiments on two real-world datasets demonstrate that the cognitive diagnosis models searched by the proposed approach exhibit significantly better performance than existing models and also hold as good interpretability as human-designed models.
</details></li>
</ul>
<hr>
<h2 id="Observation-of-high-energy-neutrinos-from-the-Galactic-plane"><a href="#Observation-of-high-energy-neutrinos-from-the-Galactic-plane" class="headerlink" title="Observation of high-energy neutrinos from the Galactic plane"></a>Observation of high-energy neutrinos from the Galactic plane</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04427">http://arxiv.org/abs/2307.04427</a></li>
<li>repo_url: None</li>
<li>paper_authors: R. Abbasi, M. Ackermann, J. Adams, J. A. Aguilar, M. Ahlers, M. Ahrens, J. M. Alameddine, A. A. Alves Jr., N. M. Amin, K. Andeen, T. Anderson, G. Anton, C. Argüelles, Y. Ashida, S. Athanasiadou, S. Axani, X. Bai, A. Balagopal V., S. W. Barwick, V. Basu, S. Baur, R. Bay, J. J. Beatty, K. -H. Becker, J. Becker Tjus, J. Beise, C. Bellenghi, S. Benda, S. BenZvi, D. Berley, E. Bernardini, D. Z. Besson, G. Binder, D. Bindig, E. Blaufuss, S. Blot, M. Boddenberg, F. Bontempo, J. Y. Book, J. Borowka, S. Böser, O. Botner, J. Böttcher, E. Bourbeau, F. Bradascio, J. Braun, B. Brinson, S. Bron, J. Brostean-Kaiser, R. T. Burley, R. S. Busse, M. A. Campana, E. G. Carnie-Bronca, C. Chen, Z. Chen, D. Chirkin, K. Choi, B. A. Clark, K. Clark, L. Classen, A. Coleman, G. H. Collin, A. Connolly, J. M. Conrad, P. Coppin, P. Correa, D. F. Cowen, R. Cross, C. Dappen, P. Dave, C. De Clercq, J. J. DeLaunay, D. Delgado López, H. Dembinski, K. Deoskar, A. Desai, P. Desiati, K. D. de Vries, G. de Wasseige, T. DeYoung, A. Diaz, J. C. Díaz-Vélez, M. Dittmer, H. Dujmovic, M. Dunkman, M. A. DuVernois, T. Ehrhardt, P. Eller, R. Engel, H. Erpenbeck, J. Evans, P. A. Evenson, K. L. Fan, A. R. Fazely, A. Fedynitch, N. Feigl, S. Fiedlschuster, A. T. Fienberg, C. Finley, L. Fischer, D. Fox, A. Franckowiak, E. Friedman, A. Fritz, P. Fürst, T. K. Gaisser, J. Gallagher, E. Ganster, A. Garcia, S. Garrappa, L. Gerhardt, A. Ghadimi, C. Glaser, T. Glauch, T. Glüsenkamp, N. Goehlke, A. Goldschmidt, J. G. Gonzalez, S. Goswami, D. Grant, T. Grégoire, S. Griswold, C. Günther, P. Gutjahr, C. Haack, A. Hallgren, R. Halliday, L. Halve, F. Halzen, M. Ha Minh, K. Hanson, J. Hardin, A. A. Harnisch, A. Haungs, K. Helbing, F. Henningsen, E. C. Hettinger, S. Hickford, J. Hignight, C. Hill, G. C. Hill, K. D. Hoffman, K. Hoshina, W. Hou, F. Huang, M. Huber, T. Huber, K. Hultqvist, M. Hünnefeld, R. Hussain, K. Hymon, S. In, N. Iovine, A. Ishihara, M. Jansson, G. S. Japaridze, M. Jeong, M. Jin, B. J. P. Jones, D. Kang, W. Kang, X. Kang, A. Kappes, D. Kappesser, L. Kardum, T. Karg, M. Karl, A. Karle, U. Katz, M. Kauer, M. Kellermann, J. L. Kelley, A. Kheirandish, K. Kin, J. Kiryluk, S. R. Klein, A. Kochocki, R. Koirala, H. Kolanoski, T. Kontrimas, L. Köpke, C. Kopper, S. Kopper, D. J. Koskinen, P. Koundal, M. Kovacevich, M. Kowalski, T. Kozynets, E. Krupczak, E. Kun, N. Kurahashi, N. Lad, C. Lagunas Gualda, J. L. Lanfranchi, M. J. Larson, F. Lauber, J. P. Lazar, J. W. Lee, K. Leonard, A. Leszczyńska, Y. Li, M. Lincetto, Q. R. Liu, M. Liubarska, E. Lohfink, C. J. Lozano Mariscal, L. Lu, F. Lucarelli, A. Ludwig, W. Luszczak, Y. Lyu, W. Y. Ma, J. Madsen, K. B. M. Mahn, Y. Makino, S. Mancina, I. C. Mariş, I. Martinez-Soler, R. Maruyama, S. McCarthy, T. McElroy, F. McNally, J. V. Mead, K. Meagher, S. Mechbal, A. Medina, M. Meier, S. Meighen-Berger, Y. Merckx, J. Micallef, D. Mockler, T. Montaruli, R. W. Moore, K. Morik, R. Morse, M. Moulai, T. Mukherjee, R. Naab, R. Nagai, R. Nahnhauer, U. Naumann, J. Necker, L. V. Nguyen, H. Niederhausen, M. U. Nisa, S. C. Nowicki, D. Nygren, A. Obertacke Pollmann, M. Oehler, B. Oeyen, A. Olivas, E. O’Sullivan, H. Pandya, D. V. Pankova, N. Park, G. K. Parker, E. N. Paudel, L. Paul, C. Pérez de los Heros, L. Peters, J. Peterson, S. Philippen, S. Pieper, A. Pizzuto, M. Plum, Y. Popovych, A. Porcelli, M. Prado Rodriguez, B. Pries, G. T. Przybylski, C. Raab, J. Rack-Helleis, A. Raissi, M. Rameez, K. Rawlins, I. C. Rea, Z. Rechav, A. Rehman, P. Reichherzer, R. Reimann, G. Renzi, E. Resconi, S. Reusch, W. Rhode, M. Richman, B. Riedel, E. J. Roberts, S. Robertson, G. Roellinghoff, M. Rongen, C. Rott, T. Ruhe, D. Ryckbosch, D. Rysewyk Cantu, I. Safa, J. Saffer, D. Salazar-Gallegos, P. Sampathkumar, S. E. Sanchez Herrera, A. Sandrock, M. Santander, S. Sarkar, S. Sarkar, K. Satalecka, M. Schaufel, H. Schieler, S. Schindler, T. Schmidt, A. Schneider, J. Schneider, F. G. Schröder, L. Schumacher, G. Schwefer, S. Sclafani, D. Seckel, S. Seunarine, A. Sharma, S. Shefali, N. Shimizu, M. Silva, B. Skrzypek, B. Smithers, R. Snihur, J. Soedingrekso, A. Sogaard, D. Soldin, C. Spannfellner, G. M. Spiczak, C. Spiering, M. Stamatikos, T. Stanev, R. Stein, J. Stettner, T. Stezelberger, B. Stokstad, T. Stürwald, T. Stuttard, G. W. Sullivan, I. Taboada, S. Ter-Antonyan, J. Thwaites, S. Tilav, F. Tischbein, K. Tollefson, C. Tönnis, S. Toscano, D. Tosi, A. Trettin, M. Tselengidou, C. F. Tung, A. Turcati, R. Turcotte, C. F. Turley, J. P. Twagirayezu, B. Ty, M. A. Unland Elorrieta, N. Valtonen-Mattila, J. Vandenbroucke, N. van Eijndhoven, D. Vannerom, J. van Santen, J. Veitch-Michaelis, S. Verpoest, C. Walck, W. Wang, T. B. Watson, C. Weaver, P. Weigel, A. Weindl, M. J. Weiss, J. Weldert, C. Wendt, J. Werthebach, M. Weyrauch, N. Whitehorn, C. H. Wiebusch, N. Willey, D. R. Williams, M. Wolf, G. Wrede, J. Wulff, X. W. Xu, J. P. Yanez, E. Yildizci, S. Yoshida, S. Yu, T. Yuan, Z. Zhang, P. Zhelnin</li>
<li>for: 本研究的目的是找到高能宇宙射线的起源。</li>
<li>methods: 这个研究使用机器学习技术分析了十年的冰穹神 neutrino telescope 数据，寻找宇宙射线的辐射。</li>
<li>results: 研究发现了来自银河平面的 neutrino 辐射，stats 相对于背景只是4.5$\sigma$ 的水平，但这些辐射也可能来自一群未能解决的点源。<details>
<summary>Abstract</summary>
The origin of high-energy cosmic rays, atomic nuclei that continuously impact Earth's atmosphere, has been a mystery for over a century. Due to deflection in interstellar magnetic fields, cosmic rays from the Milky Way arrive at Earth from random directions. However, near their sources and during propagation, cosmic rays interact with matter and produce high-energy neutrinos. We search for neutrino emission using machine learning techniques applied to ten years of data from the IceCube Neutrino Observatory. We identify neutrino emission from the Galactic plane at the 4.5$\sigma$ level of significance, by comparing diffuse emission models to a background-only hypothesis. The signal is consistent with modeled diffuse emission from the Galactic plane, but could also arise from a population of unresolved point sources.
</details>
<details>
<summary>摘要</summary>
高能宇宙射线的起源已经是一个谜题超过一个世纪。由于介电场的折射，宇宙射线从银河系 arrive at Earth from random directions。然而，在源头附近和传播过程中，宇宙射线与物质进行交互，生成高能中微子。我们使用机器学习技术对 IceCube 中微子观测器十年的数据进行搜索。我们在background-only假设下，在4.5σ水平上发现了来自银河平面的中微子发射。该信号与模型的散发辐射相一致，但也可能来自一群未解决的点源。
</details></li>
</ul>
<hr>
<h2 id="Handling-Group-Fairness-in-Federated-Learning-Using-Augmented-Lagrangian-Approach"><a href="#Handling-Group-Fairness-in-Federated-Learning-Using-Augmented-Lagrangian-Approach" class="headerlink" title="Handling Group Fairness in Federated Learning Using Augmented Lagrangian Approach"></a>Handling Group Fairness in Federated Learning Using Augmented Lagrangian Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04417">http://arxiv.org/abs/2307.04417</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gerry Windiarto Mohamad Dunda, Shenghui Song</li>
<li>for: 提高 Federated Learning 系统中群体公平性的方法</li>
<li>methods: 提出了一种基于 local differential privacy 和 client-side aggregation 的新 Federated Learning 算法，可以有效地提高群体公平性，同时具有较少的通信成本和负载。</li>
<li>results: 在 CelebA 和 ImSitu 数据集上实验表明，提出的方法可以在统计上不同的客户端数量和随机性下，Quantitatively 和 Qualitatively 提高群体公平性，同时具有较少的减少精度损失。<details>
<summary>Abstract</summary>
Federated learning (FL) has garnered considerable attention due to its privacy-preserving feature. Nonetheless, the lack of freedom in managing user data can lead to group fairness issues, where models might be biased towards sensitive factors such as race or gender, even if they are trained using a legally compliant process. To redress this concern, this paper proposes a novel FL algorithm designed explicitly to address group fairness issues. We show empirically on CelebA and ImSitu datasets that the proposed method can improve fairness both quantitatively and qualitatively with minimal loss in accuracy in the presence of statistical heterogeneity and with different numbers of clients. Besides improving fairness, the proposed FL algorithm is compatible with local differential privacy (LDP), has negligible communication costs, and results in minimal overhead when migrating existing FL systems from the common FL protocol such as FederatedAveraging (FedAvg). We also provide the theoretical convergence rate guarantee for the proposed algorithm and the required noise level of the Gaussian mechanism to achieve desired LDP. This innovative approach holds significant potential to enhance the fairness and effectiveness of FL systems, particularly in sensitive applications such as healthcare or criminal justice.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 已经引起了广泛的关注，主要是因为它具有隐私保护的特点。然而，由于用户数据的管理不具有完全的自由性，这可能会导致群体公平问题，模型可能会偏爱敏感因素 such as 种族或性别，即使使用了合法的过程进行训练。为了解决这个问题，这篇论文提出了一种新的 Federated Learning 算法，用于直接地 Addressing Group Fairness Issues。我们通过实验表明，在 celebA 和 ImSitu 数据集上，我们的方法可以改善公平性 both quantitatively and qualitatively，即使在统计不同性和不同客户端数量的情况下。此外，我们的 FL 算法兼容地方 differential privacy (LDP)，通信成本很低，并且在将现有 FL 系统从 Common FL Protocol such as FederatedAveraging (FedAvg) 迁移到我们的方法时，不会带来很大的干扰。我们还提供了对我们的算法的理论收敛率保证和需要的 Gaussian 机制的噪音水平来实现 Desired LDP。这种创新的方法可能会提高 FL 系统的公平性和效果，特别是在敏感应用 such as 医疗或刑事正义领域。
</details></li>
</ul>
<hr>
<h2 id="Episodic-Gaussian-Process-Based-Learning-Control-with-Vanishing-Tracking-Errors"><a href="#Episodic-Gaussian-Process-Based-Learning-Control-with-Vanishing-Tracking-Errors" class="headerlink" title="Episodic Gaussian Process-Based Learning Control with Vanishing Tracking Errors"></a>Episodic Gaussian Process-Based Learning Control with Vanishing Tracking Errors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04415">http://arxiv.org/abs/2307.04415</a></li>
<li>repo_url: None</li>
<li>paper_authors: Armin Lederer, Jonas Umlauft, Sandra Hirche</li>
<li>for: 这个论文是为了解决技术系统中减少精细模型的问题，通过超visirotting机器学习来推断模型。</li>
<li>methods: 这篇论文使用了 Gaussian process regression，具有高数据效率和明确的uncertainty表示，从而 derivation of prediction error bounds。</li>
<li>results: 论文提出了一种 Bayesian prediction error bound，其与数据密度相关，并证明了时间变化的跟踪准确性保证。通过这种 bound，可以实现vanishing tracking error with increasing data density。<details>
<summary>Abstract</summary>
Due to the increasing complexity of technical systems, accurate first principle models can often not be obtained. Supervised machine learning can mitigate this issue by inferring models from measurement data. Gaussian process regression is particularly well suited for this purpose due to its high data-efficiency and its explicit uncertainty representation, which allows the derivation of prediction error bounds. These error bounds have been exploited to show tracking accuracy guarantees for a variety of control approaches, but their direct dependency on the training data is generally unclear. We address this issue by deriving a Bayesian prediction error bound for GP regression, which we show to decay with the growth of a novel, kernel-based measure of data density. Based on the prediction error bound, we prove time-varying tracking accuracy guarantees for learned GP models used as feedback compensation of unknown nonlinearities, and show to achieve vanishing tracking error with increasing data density. This enables us to develop an episodic approach for learning Gaussian process models, such that an arbitrary tracking accuracy can be guaranteed. The effectiveness of the derived theory is demonstrated in several simulations.
</details>
<details>
<summary>摘要</summary>
To address this issue, we derive a Bayesian prediction error bound for GP regression, which we show to decay with the growth of a novel, kernel-based measure of data density. Based on this bound, we prove time-varying tracking accuracy guarantees for learned GP models used as feedback compensation of unknown nonlinearities, and show that the tracking error vanishes as the data density increases. This enables us to develop an episodic approach for learning Gaussian process models, such that an arbitrary tracking accuracy can be guaranteed. The effectiveness of the derived theory is demonstrated through several simulations.
</details></li>
</ul>
<hr>
<h2 id="Comparison-of-Point-Cloud-and-Image-based-Models-for-Calorimeter-Fast-Simulation"><a href="#Comparison-of-Point-Cloud-and-Image-based-Models-for-Calorimeter-Fast-Simulation" class="headerlink" title="Comparison of Point Cloud and Image-based Models for Calorimeter Fast Simulation"></a>Comparison of Point Cloud and Image-based Models for Calorimeter Fast Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04780">http://arxiv.org/abs/2307.04780</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/viniciusmikuni/calo4eic">https://github.com/viniciusmikuni/calo4eic</a></li>
<li>paper_authors: Fernando Torales Acosta, Vinicius Mikuni, Benjamin Nachman, Miguel Arratia, Bishnu Karki, Ryan Milton, Piyush Karande, Aaron Angerami</li>
<li>for: 这个论文是为了研究Score基于生成模型，以准确地生成高维度calorimeter数据集。</li>
<li>methods: 这两个状态对比的Score基于模型都是使用同一个calorimeter simulate数据集训练的。</li>
<li>results: 研究发现，使用点云来表示calorimeter shower比使用3D voxels更加自然地处理稀疏数据集，并可以使用更加紧凑的模型和数据文件。<details>
<summary>Abstract</summary>
Score based generative models are a new class of generative models that have been shown to accurately generate high dimensional calorimeter datasets. Recent advances in generative models have used images with 3D voxels to represent and model complex calorimeter showers. Point clouds, however, are likely a more natural representation of calorimeter showers, particularly in calorimeters with high granularity. Point clouds preserve all of the information of the original simulation, more naturally deal with sparse datasets, and can be implemented with more compact models and data files. In this work, two state-of-the-art score based models are trained on the same set of calorimeter simulation and directly compared.
</details>
<details>
<summary>摘要</summary>
分数基于的生成模型是一种新的生成模型，已经证明可以准确地生成高维度的椭圆仪数据。最近的生成模型使用图像的3D矩阵来表示和模型复杂的椭圆仪涨潮。然而，点云更可能是椭圆仪涨潮的自然表示，特别是高精度的椭圆仪中。点云保留原始模拟中的所有信息，更自然地处理稀疏数据，并可以通过更 компакт的模型和数据文件实现。在这项工作中，两种当前领先的分数基于模型被直接对比训练。
</details></li>
</ul>
<hr>
<h2 id="CT-based-Subchondral-Bone-Microstructural-Analysis-in-Knee-Osteoarthritis-via-MR-Guided-Distillation-Learning"><a href="#CT-based-Subchondral-Bone-Microstructural-Analysis-in-Knee-Osteoarthritis-via-MR-Guided-Distillation-Learning" class="headerlink" title="CT-based Subchondral Bone Microstructural Analysis in Knee Osteoarthritis via MR-Guided Distillation Learning"></a>CT-based Subchondral Bone Microstructural Analysis in Knee Osteoarthritis via MR-Guided Distillation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04390">http://arxiv.org/abs/2307.04390</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jackhu-bme/srrd">https://github.com/jackhu-bme/srrd</a></li>
<li>paper_authors: Yuqi Hu, Xiangyu Zhao, Gaowei Qing, Kai Xie, Chenglei Liu, Lichi Zhang</li>
<li>For: The paper aims to develop a novel method for subchondral bone microstructural analysis using easily-acquired CT images, which can help diagnose knee osteoarthritis.* Methods: The proposed method, named SRRD, uses a distillation-learning-based approach to transfer MR structural information to a CT-based model, and leverages paired MR images to enhance the CT-based analysis model during training.* Results: The proposed method achieved high reliability and validity in MR-CT registration, regression, and knee osteoarthritis classification, with an AUC score of 0.767 (95% CI, 0.681-0.853) compared to 0.658 (95% CI, 0.574-0.742) using the CNN approach.<details>
<summary>Abstract</summary>
Background: MR-based subchondral bone effectively predicts knee osteoarthritis. However, its clinical application is limited by the cost and time of MR. Purpose: We aim to develop a novel distillation-learning-based method named SRRD for subchondral bone microstructural analysis using easily-acquired CT images, which leverages paired MR images to enhance the CT-based analysis model during training. Materials and Methods: Knee joint images of both CT and MR modalities were collected from October 2020 to May 2021. Firstly, we developed a GAN-based generative model to transform MR images into CT images, which was used to establish the anatomical correspondence between the two modalities. Next, we obtained numerous patches of subchondral bone regions of MR images, together with their trabecular parameters (BV / TV, Tb. Th, Tb. Sp, Tb. N) from the corresponding CT image patches via regression. The distillation-learning technique was used to train the regression model and transfer MR structural information to the CT-based model. The regressed trabecular parameters were further used for knee osteoarthritis classification. Results: A total of 80 participants were evaluated. CT-based regression results of trabecular parameters achieved intra-class correlation coefficients (ICCs) of 0.804, 0.773, 0.711, and 0.622 for BV / TV, Tb. Th, Tb. Sp, and Tb. N, respectively. The use of distillation learning significantly improved the performance of the CT-based knee osteoarthritis classification method using the CNN approach, yielding an AUC score of 0.767 (95% CI, 0.681-0.853) instead of 0.658 (95% CI, 0.574-0.742) (p<.001). Conclusions: The proposed SRRD method showed high reliability and validity in MR-CT registration, regression, and knee osteoarthritis classification, indicating the feasibility of subchondral bone microstructural analysis based on CT images.
</details>
<details>
<summary>摘要</summary>
背景：MR基于subchondral骨的效果可以预测膝关节炎，但其临床应用受到MR成本和时间的限制。目的：我们希望开发一种基于截然学习的方法，名为SRRD，用于CT图像上的subchondral骨微结构分析，利用MR图像来增强CT图像分析模型的训练。材料和方法：膝关节图像 Both CT和MR模式自2020年10月至2021年5月收集。首先，我们开发了一个GAN基于生成模型，将MR图像转换成CT图像，以建立两种模态之间的解剖相对性。接着，我们从MR图像中提取了许多subchondral骨区域的补丁，并将其与CT图像中的相应区域进行对比，以获取 trabecular参数（BV / TV、Tb. Th、Tb. Sp、Tb. N）的 regression。使用截然学习技术来训练回归模型，以将MR结构信息传递给CT基本的模型。重新计算的 trabecular参数被用于膝关节炎分类。结果：总共评估了80名参与者。CT基于 regression 结果中的 trabecular参数 achieved intra-class correlation coefficients (ICCs) of 0.804, 0.773, 0.711, and 0.622 for BV / TV, Tb. Th, Tb. Sp, and Tb. N, respectively。使用截然学习对CT基本的膝关节炎分类方法进行了显著改进，其AUC score为0.767（95% CI, 0.681-0.853），比之前的0.658（95% CI, 0.574-0.742）（p<.001）。结论：我们的SRRD方法在MR-CT регистраción、回归和膝关节炎分类中表现了高可靠性和有效性，表明基于CT图像的subchondral骨微结构分析是可能的。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Identify-Graphs-from-Node-Trajectories-in-Multi-Robot-Networks"><a href="#Learning-to-Identify-Graphs-from-Node-Trajectories-in-Multi-Robot-Networks" class="headerlink" title="Learning to Identify Graphs from Node Trajectories in Multi-Robot Networks"></a>Learning to Identify Graphs from Node Trajectories in Multi-Robot Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04374">http://arxiv.org/abs/2307.04374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eduardo Sebastian, Thai Duong, Nikolay Atanasov, Eduardo Montijano, Carlos Sagues</li>
<li>for:  Identifying the interactions among nodes in a network given their state&#x2F;feature trajectories.</li>
<li>methods: Combines a strongly convex program with a self-attention encoder to learn the graph topology and appropriate regularizers for optimization.</li>
<li>results: Can identify the graph topology of unseen networks with new configurations in terms of number of nodes, connectivity, or state trajectories, and demonstrates effectiveness in multi-robot formation and flocking tasks.Here’s the text in Simplified Chinese:</li>
<li>for:  Identifying网络中节点之间的互动关系，给出状态&#x2F;特征轨迹。</li>
<li>methods: 组合强型凸程程序和自注意编码器，学习网络架构和优化适当的正则化项。</li>
<li>results: 可以 Identify未看到的网络架构，包括节点数量、连接性和状态轨迹等新配置，并在多机器formation和群集任务中表现效果。<details>
<summary>Abstract</summary>
The graph identification problem consists of discovering the interactions among nodes in a network given their state/feature trajectories. This problem is challenging because the behavior of a node is coupled to all the other nodes by the unknown interaction model. Besides, high-dimensional and nonlinear state trajectories make difficult to identify if two nodes are connected. Current solutions rely on prior knowledge of the graph topology and the dynamic behavior of the nodes, and hence, have poor generalization to other network configurations. To address these issues, we propose a novel learning-based approach that combines (i) a strongly convex program that efficiently uncovers graph topologies with global convergence guarantees and (ii) a self-attention encoder that learns to embed the original state trajectories into a feature space and predicts appropriate regularizers for the optimization program. In contrast to other works, our approach can identify the graph topology of unseen networks with new configurations in terms of number of nodes, connectivity or state trajectories. We demonstrate the effectiveness of our approach in identifying graphs in multi-robot formation and flocking tasks.
</details>
<details>
<summary>摘要</summary>
《网络图标识问题》的解决方法是找出网络中节点之间的交互关系， giventheir状态/特征轨迹。这个问题具有挑战性，因为每个节点的行为都是所有其他节点的未知交互模型的各自响应。此外，高维和非线性的状态轨迹使得判断两个节点是否连接的很困难。现有的解决方案均基于节点的动态行为和网络拓扑结构的先验知识，因此具有poor泛化性。为了解决这些问题，我们提出了一种新的学习型方法，其包括（i）一个强 convex 程序，可以高效地揭示网络拓扑结构，并且具有全局收敛保证；（ii）一个自注意编码器，可以将原始状态轨迹编码到特征空间中，并预测适当的正则化项来优化程序。与其他工作不同，我们的方法可以在未看到的网络配置下，适应新的节点数、连接性和状态轨迹。我们在多机器人formation和群集任务中证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Recent-Advancements-in-End-to-End-Autonomous-Driving-using-Deep-Learning-A-Survey"><a href="#Recent-Advancements-in-End-to-End-Autonomous-Driving-using-Deep-Learning-A-Survey" class="headerlink" title="Recent Advancements in End-to-End Autonomous Driving using Deep Learning: A Survey"></a>Recent Advancements in End-to-End Autonomous Driving using Deep Learning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04370">http://arxiv.org/abs/2307.04370</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pranav-chib/recent-advancements-in-end-to-end-autonomous-driving-using-deep-learning">https://github.com/pranav-chib/recent-advancements-in-end-to-end-autonomous-driving-using-deep-learning</a></li>
<li>paper_authors: Pranav Singh Chib, Pravendra Singh</li>
<li>for: 本研究旨在提供一份涵盖整个自动驾驶栈的全面回顾，探讨了深度学习在自动驾驶中的应用，以及适应实际应用环境中的挑战。</li>
<li>methods: 本研究使用了深度学习来实现自动驾驶栈的全面控制，包括感知到控制的全流程，并Addressed key challenges in real-world applications, such as explainability and safety aspects.</li>
<li>results: 本研究对自动驾驶领域的最新发展进行了分类和评估，并提供了一个包含最新开源实现的GitHub仓库。<details>
<summary>Abstract</summary>
End-to-End driving is a promising paradigm as it circumvents the drawbacks associated with modular systems, such as their overwhelming complexity and propensity for error propagation. Autonomous driving transcends conventional traffic patterns by proactively recognizing critical events in advance, ensuring passengers' safety and providing them with comfortable transportation, particularly in highly stochastic and variable traffic settings. This paper presents a comprehensive review of the End-to-End autonomous driving stack. It provides a taxonomy of automated driving tasks wherein neural networks have been employed in an End-to-End manner, encompassing the entire driving process from perception to control, while addressing key challenges encountered in real-world applications. Recent developments in End-to-End autonomous driving are analyzed, and research is categorized based on underlying principles, methodologies, and core functionality. These categories encompass sensorial input, main and auxiliary output, learning approaches ranging from imitation to reinforcement learning, and model evaluation techniques. The survey incorporates a detailed discussion of the explainability and safety aspects. Furthermore, it assesses the state-of-the-art, identifies challenges, and explores future possibilities. We maintained the latest advancements and their corresponding open-source implementations at https://github.com/Pranav-chib/Recent-Advancements-in-End-to-End-Autonomous-Driving-using-Deep-Learning.
</details>
<details>
<summary>摘要</summary>
END-TO-END自驾系统是一种优秀的思想，因为它绕过模块化系统的缺点，如它们的复杂性和错误卷积。自驾系统超越了传统的交通模式，通过积极地预测重要事件，确保乘客的安全和提供舒适的交通方式，特别是在高度随机和变化的交通环境中。这篇论文提供了END-TO-END自驾栈的全面回顾。它提供了自动驾驶任务中使用神经网络的End-to-End方式，涵盖整个驾驶过程从感知到控制，并解决了实际应用中遇到的主要挑战。最新的END-TO-END自驾技术发展分析，并按照基本原理、方法论和核心功能分类。这些类别包括感知输入、主要和辅助输出、学习方法从模仿到强化学习，以及模型评估技术。文章还包括对解释性和安全方面的详细讨论，以及现状、挑战和未来可能性的评估。我们将latestadvances和它们相应的开源实现存储在https://github.com/Pranav-chib/Recent-Advancements-in-End-to-End-Autonomous-Driving-using-Deep-Learning上。
</details></li>
</ul>
<hr>
<h2 id="ECS-–-an-Interactive-Tool-for-Data-Quality-Assurance"><a href="#ECS-–-an-Interactive-Tool-for-Data-Quality-Assurance" class="headerlink" title="ECS – an Interactive Tool for Data Quality Assurance"></a>ECS – an Interactive Tool for Data Quality Assurance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04368">http://arxiv.org/abs/2307.04368</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Sieberichs, Simon Geerkens, Alexander Braun, Thomas Waschulzik</li>
<li>for: 这项研究旨在提高机器学习系统的可靠性和安全性，通过确保数据质量高。</li>
<li>methods: 该paper提出了一种新的数据质量保证方法，通过数学基础和多个示例来检测数据中可能有害的特性。</li>
<li>results: 该方法可以检测出可能对安全系统具有害的数据点，从而提高机器学习系统的可靠性和安全性。<details>
<summary>Abstract</summary>
With the increasing capabilities of machine learning systems and their potential use in safety-critical systems, ensuring high-quality data is becoming increasingly important. In this paper we present a novel approach for the assurance of data quality. For this purpose, the mathematical basics are first discussed and the approach is presented using multiple examples. This results in the detection of data points with potentially harmful properties for the use in safety-critical systems.
</details>
<details>
<summary>摘要</summary>
随着机器学习系统的能力不断提高，它们的潜在应用在安全关键系统中也在增加。为此，保证数据质量的重要性也在增加。在这篇论文中，我们提出了一种新的数据质量保证方法。为此，我们首先介绍了数学基础，然后通过多个示例介绍了该方法。这将导致检测数据点中可能有害的特性，因此不适用于安全关键系统的使用。
</details></li>
</ul>
<hr>
<h2 id="One-Shot-Pruning-for-Fast-adapting-Pre-trained-Models-on-Devices"><a href="#One-Shot-Pruning-for-Fast-adapting-Pre-trained-Models-on-Devices" class="headerlink" title="One-Shot Pruning for Fast-adapting Pre-trained Models on Devices"></a>One-Shot Pruning for Fast-adapting Pre-trained Models on Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04365">http://arxiv.org/abs/2307.04365</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haiyan Zhao, Guodong Long</li>
<li>for: 这篇论文旨在解决大规模预训练模型在低能力设备上部署的问题，通过实现一种可扩展的一击适束方法。</li>
<li>methods: 方法是使用相似任务中删除过的模型删除知识，创建任务特有的权重对，以对原始模型进行删除，从而提取适当大小的子网络，并将其适束到新任务。</li>
<li>results: 实验分析显示，提案方法在各种数据集和任务下，与各种删除基准方法进行比较，具有较高的精度和效率。<details>
<summary>Abstract</summary>
Large-scale pre-trained models have been remarkably successful in resolving downstream tasks. Nonetheless, deploying these models on low-capability devices still requires an effective approach, such as model pruning. However, pruning the model from scratch can pose a practical challenge given the limited resources of each downstream task or device. To tackle this issue, we present a scalable one-shot pruning method that leverages pruned knowledge of similar tasks to extract a sub-network from the pre-trained model for a new task. Specifically, we create a score mask using the pruned models of similar tasks to identify task-specific filters/nodes in the pre-trained model for the new task. Based on this mask, we conduct a single round of pruning to extract a suitably-sized sub-network that can quickly adapt to the new task with only a few training iterations. Our experimental analysis demonstrates the effectiveness of the proposed method on the convolutional neural networks (CNNs) and vision transformers (ViT) with various datasets. The proposed method consistently outperforms popular pruning baseline methods in terms of accuracy and efficiency when dealing with diverse downstream tasks with different memory constraints.
</details>
<details>
<summary>摘要</summary>
Specifically, we create a score mask using the pruned models of similar tasks to identify task-specific filters/nodes in the pre-trained model for the new task. Based on this mask, we conduct a single round of pruning to extract a suitably-sized sub-network that can quickly adapt to the new task with only a few training iterations. Our experimental analysis demonstrates the effectiveness of the proposed method on convolutional neural networks (CNNs) and vision transformers (ViT) with various datasets. The proposed method consistently outperforms popular pruning baseline methods in terms of accuracy and efficiency when dealing with diverse downstream tasks with different memory constraints.
</details></li>
</ul>
<hr>
<h2 id="False-Sense-of-Security-Leveraging-XAI-to-Analyze-the-Reasoning-and-True-Performance-of-Context-less-DGA-Classifiers"><a href="#False-Sense-of-Security-Leveraging-XAI-to-Analyze-the-Reasoning-and-True-Performance-of-Context-less-DGA-Classifiers" class="headerlink" title="False Sense of Security: Leveraging XAI to Analyze the Reasoning and True Performance of Context-less DGA Classifiers"></a>False Sense of Security: Leveraging XAI to Analyze the Reasoning and True Performance of Context-less DGA Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04358">http://arxiv.org/abs/2307.04358</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://gitlab.com/rwth-itsec/explainability-analyzed-dga-models">https://gitlab.com/rwth-itsec/explainability-analyzed-dga-models</a></li>
<li>paper_authors: Arthur Drichel, Ulrike Meyer</li>
<li>for: 本研究旨在解决使用Domain Generation Algorithm（DGA）检测 botnet 活动时存在的假 positives 问题，通过使用深度学习分类器达到精度超过 99.9%。</li>
<li>methods: 本研究使用 Explainable Artificial Intelligence（XAI）方法分析深度学习分类器的 reasoning，并系统地揭露了这些分类器的偏见。</li>
<li>results:  eliminating 偏见后，DGA 分类器的性能明显下降，但我们提出了一种 context-aware 检测系统，能够维持 state-of-the-art 深度学习分类器的检测率，并提供了一种可视化分析系统，帮助更好地理解分类器的 reasoning，从而提高检测方法的信任和透明度。<details>
<summary>Abstract</summary>
The problem of revealing botnet activity through Domain Generation Algorithm (DGA) detection seems to be solved, considering that available deep learning classifiers achieve accuracies of over 99.9%. However, these classifiers provide a false sense of security as they are heavily biased and allow for trivial detection bypass. In this work, we leverage explainable artificial intelligence (XAI) methods to analyze the reasoning of deep learning classifiers and to systematically reveal such biases. We show that eliminating these biases from DGA classifiers considerably deteriorates their performance. Nevertheless we are able to design a context-aware detection system that is free of the identified biases and maintains the detection rate of state-of-the art deep learning classifiers. In this context, we propose a visual analysis system that helps to better understand a classifier's reasoning, thereby increasing trust in and transparency of detection methods and facilitating decision-making.
</details>
<details>
<summary>摘要</summary>
“botnet活动探测透过类别生成算法（DGA）探测的问题似乎已经解决了，因为可用的深度学习分类器可以达到99.9%以上的准确率。但是这些分类器具有严重的偏见，容易被轻松地逃脱检测。在这个工作中，我们利用可解释人工智能（XAI）方法来分析深度学习分类器的思考过程，并系统地揭露这些偏见。我们发现，从DGA分类器中除掉这些偏见后，其性能会很差。但我们能够设计一个具有上述偏见的 контекст感知检测系统，并维持现有深度学习分类器的检测率。在这个上下文中，我们提出了一个可观分析系统，帮助更好地理解分类器的思考过程，从而增加检测方法的信任和透明度，并且方便决策。”
</details></li>
</ul>
<hr>
<h2 id="Formulating-A-Strategic-Plan-Based-On-Statistical-Analyses-And-Applications-For-Financial-Companies-Through-A-Real-World-Use-Case"><a href="#Formulating-A-Strategic-Plan-Based-On-Statistical-Analyses-And-Applications-For-Financial-Companies-Through-A-Real-World-Use-Case" class="headerlink" title="Formulating A Strategic Plan Based On Statistical Analyses And Applications For Financial Companies Through A Real-World Use Case"></a>Formulating A Strategic Plan Based On Statistical Analyses And Applications For Financial Companies Through A Real-World Use Case</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04778">http://arxiv.org/abs/2307.04778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saman Sarraf</li>
<li>for: 这篇论文是为了提出一个基于统计分析的战略计划，以帮助金融公司LendingClub实施数据驱动的决策过程。</li>
<li>methods: 论文使用了不同的统计分析方法，包括机器学习技术，来构建更好的总体数据驱动预测模型。</li>
<li>results: 研究发现，贷款数量对borrower债务抵押率有很大影响，而提出的战略计划可以帮助LendingClub提高收入，同时减少贷款风险。<details>
<summary>Abstract</summary>
Business statistics play a crucial role in implementing a data-driven strategic plan at the enterprise level to employ various analytics where the outcomes of such a plan enable an enterprise to enhance the decision-making process or to mitigate risks to the organization. In this work, a strategic plan informed by the statistical analysis is introduced for a financial company called LendingClub, where the plan is comprised of exploring the possibility of onboarding a big data platform along with advanced feature selection capacities. The main objectives of such a plan are to increase the company's revenue while reducing the risks of granting loans to borrowers who cannot return their loans. In this study, different hypotheses formulated to address the company's concerns are studied, where the results reveal that the amount of loans profoundly impacts the number of borrowers charging off their loans. Also, the proposed strategic plan includes onboarding advanced analytics such as machine learning technologies that allow the company to build better generalized data-driven predictive models.
</details>
<details>
<summary>摘要</summary>
企业统计在实施数据驱动策略方面发挥关键作用，以使用不同的统计分析来帮助企业做出更好的决策或减少对组织的风险。在这个工作中，我们将介绍一份基于统计分析的战略计划，用于一家名为LendingClub的金融公司，以增加公司的收入，同时减少向借款者发放贷款的风险。在这个研究中，我们提出了一些用于解决公司的问题的假设，其结果表明，贷款的数量对借款者养成贷款的数量产生很大的影响。此外，我们的战略计划还包括在boarding高级分析工具，如机器学习技术，以帮助公司建立更好的通用数据驱动预测模型。
</details></li>
</ul>
<hr>
<h2 id="Policy-Finetuning-in-Reinforcement-Learning-via-Design-of-Experiments-using-Offline-Data"><a href="#Policy-Finetuning-in-Reinforcement-Learning-via-Design-of-Experiments-using-Offline-Data" class="headerlink" title="Policy Finetuning in Reinforcement Learning via Design of Experiments using Offline Data"></a>Policy Finetuning in Reinforcement Learning via Design of Experiments using Offline Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04354">http://arxiv.org/abs/2307.04354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruiqi Zhang, Andrea Zanette</li>
<li>for: 本研究是用于提高强化学习策略质量的数据采集和处理方法。</li>
<li>methods: 本研究使用了一种可证明的算法，利用已有的离线数据集来设计单一不反应的探索策略。</li>
<li>results: 研究表明，该算法可以在离线数据集的本地覆盖率和额外数据量的情况下提供可证明的质量保证。<details>
<summary>Abstract</summary>
In some applications of reinforcement learning, a dataset of pre-collected experience is already available but it is also possible to acquire some additional online data to help improve the quality of the policy. However, it may be preferable to gather additional data with a single, non-reactive exploration policy and avoid the engineering costs associated with switching policies.   In this paper we propose an algorithm with provable guarantees that can leverage an offline dataset to design a single non-reactive policy for exploration. We theoretically analyze the algorithm and measure the quality of the final policy as a function of the local coverage of the original dataset and the amount of additional data collected.
</details>
<details>
<summary>摘要</summary>
在某些应用中的强化学习中，已经有一个准备好的经验数据集，但也可以获得一些在线数据来提高政策质量。然而，可能更好的选择是使用单一、不反应的探索策略来收集更多数据，而不是在政策之间切换。在这篇论文中，我们提出了一个可证明的 garantía 的算法，可以利用已有的离线数据集来设计单一的不反应策略。我们对算法进行了理论分析，并测量了最终政策质量与原始数据集的地方覆盖率以及额外收集的数据量之间的关系。
</details></li>
</ul>
<hr>
<h2 id="On-Sufficient-Graphical-Models"><a href="#On-Sufficient-Graphical-Models" class="headerlink" title="On Sufficient Graphical Models"></a>On Sufficient Graphical Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04353">http://arxiv.org/abs/2307.04353</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Beuleup93/ProjetPythonSISE">https://github.com/Beuleup93/ProjetPythonSISE</a></li>
<li>paper_authors: Bing Li, Kyongwon Kim</li>
<li>For: This paper is written for researchers and practitioners who work with high-dimensional data and are interested in evaluating conditional independence in a nonparametric manner.* Methods: The paper uses recently developed nonlinear sufficient dimension reduction techniques to introduce a sufficient graphical model for evaluating conditional independence. The model is nonparametric and does not make distributional assumptions, but it is based on conditional independence given a set of sufficient predictors with a reduced dimension.* Results: The paper demonstrates that the proposed method outperforms existing methods when the Gaussian or copula Gaussian assumptions are violated, and its performance remains excellent in high-dimensional settings. The method is also shown to be consistent in variable selection.<details>
<summary>Abstract</summary>
We introduce a sufficient graphical model by applying the recently developed nonlinear sufficient dimension reduction techniques to the evaluation of conditional independence. The graphical model is nonparametric in nature, as it does not make distributional assumptions such as the Gaussian or copula Gaussian assumptions. However, unlike a fully nonparametric graphical model, which relies on the high-dimensional kernel to characterize conditional independence, our graphical model is based on conditional independence given a set of sufficient predictors with a substantially reduced dimension. In this way we avoid the curse of dimensionality that comes with a high-dimensional kernel. We develop the population-level properties, convergence rate, and variable selection consistency of our estimate. By simulation comparisons and an analysis of the DREAM 4 Challenge data set, we demonstrate that our method outperforms the existing methods when the Gaussian or copula Gaussian assumptions are violated, and its performance remains excellent in the high-dimensional setting.
</details>
<details>
<summary>摘要</summary>
我们引入一种充分的图形模型，通过最近发展的非线性充分维度减少技术来评估条件独立性。这个图形模型是非 Parametric 性的，意味着它不会对条件独立性进行分布假设，如 Gaussian 或 copula Gaussian 假设。然而，与完全非 Parametric 图形模型不同，我们的图形模型基于条件独立性给定一组充分的预测变量，具有减少维度的优点。这样可以避免高维度 kernel 中的困惑。我们研究这个方法的人口级特性、整体速度和变量选择一致性。通过对比实验和分析 DREAM 4 Challenge 数据集，我们示出了我们的方法在假设不满足 Gaussian 或 copula Gaussian 假设时表现出色，并在高维度设置中保持出色的表现。
</details></li>
</ul>
<hr>
<h2 id="MD-HIT-Machine-learning-for-materials-property-prediction-with-dataset-redundancy-control"><a href="#MD-HIT-Machine-learning-for-materials-property-prediction-with-dataset-redundancy-control" class="headerlink" title="MD-HIT: Machine learning for materials property prediction with dataset redundancy control"></a>MD-HIT: Machine learning for materials property prediction with dataset redundancy control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04351">http://arxiv.org/abs/2307.04351</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/usccolumbia/md-hit">https://github.com/usccolumbia/md-hit</a></li>
<li>paper_authors: Qin Li, Nihang Fu, Sadman Sadeed Omee, Jianjun Hu</li>
<li>For: 本研究旨在解决物料数据集中的重复样本问题，以提高物料预测性能的准确性。* Methods: 本文提出了一种物料数据集重复样本减少算法（MD-HIT），并对其进行了评估。* Results: 研究表明，通过使用 MD-HIT 减少样本重复，可以更好地反映物料预测性能的准确性。<details>
<summary>Abstract</summary>
Materials datasets are usually featured by the existence of many redundant (highly similar) materials due to the tinkering material design practice over the history of materials research. For example, the materials project database has many perovskite cubic structure materials similar to SrTiO$_3$. This sample redundancy within the dataset makes the random splitting of machine learning model evaluation to fail so that the ML models tend to achieve over-estimated predictive performance which is misleading for the materials science community. This issue is well known in the field of bioinformatics for protein function prediction, in which a redundancy reduction procedure (CD-Hit) is always applied to reduce the sample redundancy by ensuring no pair of samples has a sequence similarity greater than a given threshold. This paper surveys the overestimated ML performance in the literature for both composition based and structure based material property prediction. We then propose a material dataset redundancy reduction algorithm called MD-HIT and evaluate it with several composition and structure based distance threshold sfor reducing data set sample redundancy. We show that with this control, the predicted performance tends to better reflect their true prediction capability. Our MD-hit code can be freely accessed at https://github.com/usccolumbia/MD-HIT
</details>
<details>
<summary>摘要</summary>
In this paper, we survey the overestimated ML performance in the literature for both composition-based and structure-based material property prediction. We then propose a material dataset redundancy reduction algorithm called MD-HIT and evaluate it with several composition and structure-based distance thresholds for reducing data set sample redundancy. Our results show that with this control, the predicted performance tends to better reflect their true prediction capability. The MD-hit code can be freely accessed at https://github.com/usccolumbia/MD-HIT.
</details></li>
</ul>
<hr>
<h2 id="RLTF-Reinforcement-Learning-from-Unit-Test-Feedback"><a href="#RLTF-Reinforcement-Learning-from-Unit-Test-Feedback" class="headerlink" title="RLTF: Reinforcement Learning from Unit Test Feedback"></a>RLTF: Reinforcement Learning from Unit Test Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04349">http://arxiv.org/abs/2307.04349</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zyq-scut/rltf">https://github.com/zyq-scut/rltf</a></li>
<li>paper_authors: Jiate Liu, Yiqin Zhu, Kaiwen Xiao, Qiang Fu, Xiao Han, Wei Yang, Deheng Ye</li>
<li>for: 提高大型自然语言模型（LLM）的代码生成性能。</li>
<li>methods: 使用强化学习（RL）方法，并在训练过程中接受多级别单元测试反馈。</li>
<li>results: 在APPS和MBPPbenchmark上实现了状态的最佳性能。<details>
<summary>Abstract</summary>
The goal of program synthesis, or code generation, is to generate executable code based on given descriptions. Recently, there has been an increasing number of studies employing reinforcement learning (RL) to improve the performance of large language models (LLMs) for code. However, these RL methods have only used offline frameworks, limiting their exploration of new sample spaces. Additionally, current approaches that utilize unit test signals are rather simple, not accounting for specific error locations within the code. To address these issues, we proposed RLTF, i.e., Reinforcement Learning from Unit Test Feedback, a novel online RL framework with unit test feedback of multi-granularity for refining code LLMs. Our approach generates data in real-time during training and simultaneously utilizes fine-grained feedback signals to guide the model towards producing higher-quality code. Extensive experiments show that RLTF achieves state-of-the-art performance on the APPS and the MBPP benchmarks. Our code can be found at: https://github.com/Zyq-scut/RLTF.
</details>
<details>
<summary>摘要</summary>
目标是使程序生成器、代码生成器或代码生成器可以根据给定的描述生成可执行的代码。近些年来，有越来越多的研究使用强化学习（RL）来提高大型自然语言模型（LLM）的代码生成性能。然而，这些RL方法只使用了离线框架，限制了新样本空间的探索。此外，现有的使用单元测试信号的方法相对简单，没有考虑特定的错误位置在代码中。为解决这些问题，我们提出了RLTF，即基于单元测试反馈的强化学习框架。我们的方法在训练过程中生成数据并同时使用多级别的反馈信号来引导模型生成更高质量的代码。我们的实验表明，RLTF可以在APPS和MBPPbenchmark上达到当前最佳性能。我们的代码可以在github上找到：https://github.com/Zyq-scut/RLTF。
</details></li>
</ul>
<hr>
<h2 id="Injecting-Logical-Constraints-into-Neural-Networks-via-Straight-Through-Estimators"><a href="#Injecting-Logical-Constraints-into-Neural-Networks-via-Straight-Through-Estimators" class="headerlink" title="Injecting Logical Constraints into Neural Networks via Straight-Through Estimators"></a>Injecting Logical Constraints into Neural Networks via Straight-Through Estimators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04347">http://arxiv.org/abs/2307.04347</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/azreasoners/cl-ste">https://github.com/azreasoners/cl-ste</a></li>
<li>paper_authors: Zhun Yang, Joohyung Lee, Chiyoun Park</li>
<li>for: 将逻辑约束直接注入到神经网络学习中</li>
<li>methods: 使用直通估计器将逻辑约束转化为损失函数，使神经网络通过梯度下降更新参数，以满足逻辑约束</li>
<li>results: 在使用GPU和批处理训练时，方法可以规模更好地比较现有的神经符号计算方法，同时可以在不同类型的神经网络上学习，无需或减少标注数据<details>
<summary>Abstract</summary>
Injecting discrete logical constraints into neural network learning is one of the main challenges in neuro-symbolic AI. We find that a straight-through-estimator, a method introduced to train binary neural networks, could effectively be applied to incorporate logical constraints into neural network learning. More specifically, we design a systematic way to represent discrete logical constraints as a loss function; minimizing this loss using gradient descent via a straight-through-estimator updates the neural network's weights in the direction that the binarized outputs satisfy the logical constraints. The experimental results show that by leveraging GPUs and batch training, this method scales significantly better than existing neuro-symbolic methods that require heavy symbolic computation for computing gradients. Also, we demonstrate that our method applies to different types of neural networks, such as MLP, CNN, and GNN, making them learn with no or fewer labeled data by learning directly from known constraints.
</details>
<details>
<summary>摘要</summary>
injecting discrete logical constraints into neural network learning is one of the main challenges in neuro-symbolic AI. we find that a straight-through-estimator, a method introduced to train binary neural networks, could effectively be applied to incorporate logical constraints into neural network learning. more specifically, we design a systematic way to represent discrete logical constraints as a loss function; minimizing this loss using gradient descent via a straight-through-estimator updates the neural network's weights in the direction that the binarized outputs satisfy the logical constraints. the experimental results show that by leveraging GPUs and batch training, this method scales significantly better than existing neuro-symbolic methods that require heavy symbolic computation for computing gradients. also, we demonstrate that our method applies to different types of neural networks, such as MLP, CNN, and GNN, making them learn with no or fewer labeled data by learning directly from known constraints.Here's the translation in Traditional Chinese as well:injecting discrete logical constraints into neural network learning is one of the main challenges in neuro-symbolic AI. we find that a straight-through-estimator, a method introduced to train binary neural networks, could effectively be applied to incorporate logical constraints into neural network learning. more specifically, we design a systematic way to represent discrete logical constraints as a loss function; minimizing this loss using gradient descent via a straight-through-estimator updates the neural network's weights in the direction that the binarized outputs satisfy the logical constraints. the experimental results show that by leveraging GPUs and batch training, this method scales significantly better than existing neuro-symbolic methods that require heavy symbolic computation for computing gradients. also, we demonstrate that our method applies to different types of neural networks, such as MLP, CNN, and GNN, making them learn with no or fewer labeled data by learning directly from known constraints.
</details></li>
</ul>
<hr>
<h2 id="Continual-Learning-as-Computationally-Constrained-Reinforcement-Learning"><a href="#Continual-Learning-as-Computationally-Constrained-Reinforcement-Learning" class="headerlink" title="Continual Learning as Computationally Constrained Reinforcement Learning"></a>Continual Learning as Computationally Constrained Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04345">http://arxiv.org/abs/2307.04345</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saurabh Kumar, Henrik Marklund, Ashish Rao, Yifan Zhu, Hong Jun Jeon, Yueyang Liu, Benjamin Van Roy</li>
<li>for: 本研究旨在开发一种能够高效储存知识，逐渐提高人工智能能力的智能代理人。</li>
<li>methods: 本文提出了一种概念和工具集，用于探讨智能代理人的持续学习问题。</li>
<li>results: 本研究提供了一个明确和正式的定义和框架，以促进持续学习领域的进一步研究。<details>
<summary>Abstract</summary>
An agent that efficiently accumulates knowledge to develop increasingly sophisticated skills over a long lifetime could advance the frontier of artificial intelligence capabilities. The design of such agents, which remains a long-standing challenge of artificial intelligence, is addressed by the subject of continual learning. This monograph clarifies and formalizes concepts of continual learning, introducing a framework and set of tools to stimulate further research.
</details>
<details>
<summary>摘要</summary>
一个智能代理人，能够有效地积累知识，开发越来越复杂的技能，可能会推动人工智能技能的前沿。该代理人的设计，仍然是人工智能领域的长期挑战。本论文将 clarify和正式化持续学习的概念，提出一个框架和一组工具，以促进更多的研究。
</details></li>
</ul>
<hr>
<h2 id="Privacy-Preserving-Graph-Machine-Learning-from-Data-to-Computation-A-Survey"><a href="#Privacy-Preserving-Graph-Machine-Learning-from-Data-to-Computation-A-Survey" class="headerlink" title="Privacy-Preserving Graph Machine Learning from Data to Computation: A Survey"></a>Privacy-Preserving Graph Machine Learning from Data to Computation: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04338">http://arxiv.org/abs/2307.04338</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongqi Fu, Wenxuan Bao, Ross Maciejewski, Hanghang Tong, Jingrui He</li>
<li>for: 这篇论文主要关注于保护敏感信息的图机器学习隐私技术。</li>
<li>methods: 论文系统地回顾了图数据生成的隐私技术，以及在多方合作下传输隐私保护的图模型参数的方法。</li>
<li>results: 论文详细介绍了现有的隐私保护技术和软件工具，同时也提出了未来研究的挑战和可能性。最终，论文概述了一个简单、通用的安全图机器学习系统。<details>
<summary>Abstract</summary>
In graph machine learning, data collection, sharing, and analysis often involve multiple parties, each of which may require varying levels of data security and privacy. To this end, preserving privacy is of great importance in protecting sensitive information. In the era of big data, the relationships among data entities have become unprecedentedly complex, and more applications utilize advanced data structures (i.e., graphs) that can support network structures and relevant attribute information. To date, many graph-based AI models have been proposed (e.g., graph neural networks) for various domain tasks, like computer vision and natural language processing. In this paper, we focus on reviewing privacy-preserving techniques of graph machine learning. We systematically review related works from the data to the computational aspects. We first review methods for generating privacy-preserving graph data. Then we describe methods for transmitting privacy-preserved information (e.g., graph model parameters) to realize the optimization-based computation when data sharing among multiple parties is risky or impossible. In addition to discussing relevant theoretical methodology and software tools, we also discuss current challenges and highlight several possible future research opportunities for privacy-preserving graph machine learning. Finally, we envision a unified and comprehensive secure graph machine learning system.
</details>
<details>
<summary>摘要</summary>
在图机器学习中，数据采集、分享和分析经常涉及多方面，每个方面可能需要不同的数据安全和隐私保护。因此，保护隐私是图机器学习中非常重要的一环。在大数据时代，数据之间的关系变得无前例地复杂，更多的应用利用高级数据结构（即图）来支持网络结构和相关属性信息。到目前为止，许多基于图的AI模型（如图神经网络）已经被提出，用于各种领域任务，如计算机视觉和自然语言处理。在本文中，我们关注图机器学习中的隐私保护技术的评审。我们系统地回顾相关的数据和计算方面的方法。我们首先回顾如何生成隐私保护的图数据。然后我们描述如何将隐私保持的信息（如图模型参数）传输，以实现在多方共享数据时的优化计算。除了讨论相关的理论方法和软件工具外，我们还讨论当前的挑战和可能的未来研究机会。最后，我们拟定了一个统一和完整的安全图机器学习系统。
</details></li>
</ul>
<hr>
<h2 id="Source-Aware-Embedding-Training-on-Heterogeneous-Information-Networks"><a href="#Source-Aware-Embedding-Training-on-Heterogeneous-Information-Networks" class="headerlink" title="Source-Aware Embedding Training on Heterogeneous Information Networks"></a>Source-Aware Embedding Training on Heterogeneous Information Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04336">http://arxiv.org/abs/2307.04336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tsai Hor Chan, Chi Ho Wong, Jiajun Shen, Guosheng Yin</li>
<li>for: 这篇论文旨在提出一种可扩展的无监督多源异构信息网络嵌入方法，以解决现有的异构信息网络嵌入方法忽略多个来源子图的分布差异问题。</li>
<li>methods: 该方法基于一种新的嵌入空间分解技术，可以适应不同来源的异构信息网络，并通过一种扩展的聚合推理来将各个来源的嵌入空间调整到同一个空间中。</li>
<li>results: 实验结果表明，SUMSHINE方法可以在实际世界数据集上达到现有方法的性能水平，同时具有更好的扩展性和灵活性。<details>
<summary>Abstract</summary>
Heterogeneous information networks (HINs) have been extensively applied to real-world tasks, such as recommendation systems, social networks, and citation networks. While existing HIN representation learning methods can effectively learn the semantic and structural features in the network, little awareness was given to the distribution discrepancy of subgraphs within a single HIN. However, we find that ignoring such distribution discrepancy among subgraphs from multiple sources would hinder the effectiveness of graph embedding learning algorithms. This motivates us to propose SUMSHINE (Scalable Unsupervised Multi-Source Heterogeneous Information Network Embedding) -- a scalable unsupervised framework to align the embedding distributions among multiple sources of an HIN. Experimental results on real-world datasets in a variety of downstream tasks validate the performance of our method over the state-of-the-art heterogeneous information network embedding algorithms.
</details>
<details>
<summary>摘要</summary>
各种多源多类网络（HINs）已经广泛应用于现实世界任务中，如推荐系统、社交网络和引用网络。 existing HIN表示学习方法可以有效学习网络的semantic和结构特征，但是忽略了各个子图在单个HIN中的分布差异。然而，我们发现忽略这些分布差异会降低图像学习算法的效果。这种情况 Motivates我们提出 SUMSHINE（可扩展无监督多源多类网络嵌入）——一种可扩展无监督框架，用于对多个HIN的嵌入分布进行对齐。实验结果表明，我们的方法在真实世界数据上的多种下游任务中表现出色，胜过现状的多种多样化网络嵌入算法。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Adversarial-Robustness-via-Score-Based-Optimization"><a href="#Enhancing-Adversarial-Robustness-via-Score-Based-Optimization" class="headerlink" title="Enhancing Adversarial Robustness via Score-Based Optimization"></a>Enhancing Adversarial Robustness via Score-Based Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04333">http://arxiv.org/abs/2307.04333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boya Zhang, Weijian Luo, Zhihua Zhang</li>
<li>for: 本研究旨在提出一种新的防御机制，以提高深度神经网络分类器对势力攻击的抵抗能力。</li>
<li>methods: 该方法使用分Diffusion模型来优化对抗样本，以便在测试时更好地抵抗势力攻击。</li>
<li>results: 实验结果表明，ScoreOpt方法可以在多个数据集上（包括CIFAR10、CIFAR100和ImageNet）击败现有的防御方法，both in terms of robustness performance和推理速度。<details>
<summary>Abstract</summary>
Adversarial attacks have the potential to mislead deep neural network classifiers by introducing slight perturbations. Developing algorithms that can mitigate the effects of these attacks is crucial for ensuring the safe use of artificial intelligence. Recent studies have suggested that score-based diffusion models are effective in adversarial defenses. However, existing diffusion-based defenses rely on the sequential simulation of the reversed stochastic differential equations of diffusion models, which are computationally inefficient and yield suboptimal results. In this paper, we introduce a novel adversarial defense scheme named ScoreOpt, which optimizes adversarial samples at test-time, towards original clean data in the direction guided by score-based priors. We conduct comprehensive experiments on multiple datasets, including CIFAR10, CIFAR100 and ImageNet. Our experimental results demonstrate that our approach outperforms existing adversarial defenses in terms of both robustness performance and inference speed.
</details>
<details>
<summary>摘要</summary>
深度神经网络分类器可能会被抗击攻击诱导，引入微小的扰动。为确保人工智能的安全使用，开发有效的抗击攻击算法是非常重要。latest studies suggest that score-based diffusion models are effective in adversarial defenses. However, existing diffusion-based defenses rely on the sequential simulation of the reversed stochastic differential equations of diffusion models, which are computationally inefficient and yield suboptimal results. In this paper, we introduce a novel adversarial defense scheme named ScoreOpt, which optimizes adversarial samples at test-time, towards original clean data in the direction guided by score-based priors. We conduct comprehensive experiments on multiple datasets, including CIFAR10, CIFAR100 and ImageNet. Our experimental results demonstrate that our approach outperforms existing adversarial defenses in terms of both robustness performance and inference speed.Here's the translation in Traditional Chinese:深度神经网络分类器可能会被抗击攻击诱导，引入微小的扰动。为确保人工智能的安全使用，开发有效的抗击攻击算法是非常重要。latest studies suggest that score-based diffusion models are effective in adversarial defenses. However, existing diffusion-based defenses rely on the sequential simulation of the reversed stochastic differential equations of diffusion models, which are computationally inefficient and yield suboptimal results. In this paper, we introduce a novel adversarial defense scheme named ScoreOpt, which optimizes adversarial samples at test-time, towards original clean data in the direction guided by score-based priors. We conduct comprehensive experiments on multiple datasets, including CIFAR10, CIFAR100 and ImageNet. Our experimental results demonstrate that our approach outperforms existing adversarial defenses in terms of both robustness performance and inference speed.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Multiple-Descriptive-Features-for-Robust-Few-shot-Image-Learning"><a href="#Leveraging-Multiple-Descriptive-Features-for-Robust-Few-shot-Image-Learning" class="headerlink" title="Leveraging Multiple Descriptive Features for Robust Few-shot Image Learning"></a>Leveraging Multiple Descriptive Features for Robust Few-shot Image Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04317">http://arxiv.org/abs/2307.04317</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhili Feng, Anna Bair, J. Zico Kolter</li>
<li>for: 这篇论文旨在提出一种新的图像分类方法，以便更好地评估图像分类决策中的视觉特征。</li>
<li>methods: 这篇论文使用了一种大型语言模型（LLM）来自动生成每个分类的多个视觉描述，然后使用视图图像模型将这些描述翻译成每个图像的多个视觉特征。最后，使用稀疏逻辑回归选择每个图像的相关特征进行分类。</li>
<li>results:  compared to标准方法such as linear probing, this method outperforms in the few-shot learning setting, and when combined with fine-tuning, it also outperforms existing state-of-the-art finetuning approaches on both in-distribution and out-of-distribution performance.<details>
<summary>Abstract</summary>
Modern image classification is based upon directly predicting model classes via large discriminative networks, making it difficult to assess the intuitive visual ``features'' that may constitute a classification decision. At the same time, recent works in joint visual language models such as CLIP provide ways to specify natural language descriptions of image classes but typically focus on providing single descriptions for each class. In this work, we demonstrate that an alternative approach, arguably more akin to our understanding of multiple ``visual features'' per class, can also provide compelling performance in the robust few-shot learning setting. In particular, we automatically enumerate multiple visual descriptions of each class -- via a large language model (LLM) -- then use a vision-image model to translate these descriptions to a set of multiple visual features of each image; we finally use sparse logistic regression to select a relevant subset of these features to classify each image. This both provides an ``intuitive'' set of relevant features for each class, and in the few-shot learning setting, outperforms standard approaches such as linear probing. When combined with finetuning, we also show that the method is able to outperform existing state-of-the-art finetuning approaches on both in-distribution and out-of-distribution performance.
</details>
<details>
<summary>摘要</summary>
现代图像分类基于直接预测模型类via大量推理网络，这使得评估视觉“特征”的概念变得更加困难。同时，现有的 JOINT 视觉语言模型，如 CLIP，可以提供每个类型的自然语言描述，但通常只focus on提供每个类型的单一描述。在这个工作中，我们展示了一种alternative方法，更加类似于我们对多个“视觉特征”的认知，可以在robust few-shot learning setting中提供出色的表现。具体来说，我们使用大型语言模型（LLM）自动生成每个类型的多个视觉描述，然后使用视觉图像模型将这些描述翻译成每个图像的多个视觉特征；最后，我们使用稀疏逻辑回归选择每个图像的相关子集特征进行分类。这种方法不仅提供了每个类型的“直观”相关特征，而且在few-shot learning setting中，也超越了标准方法such as linear probing。当与finetuning结合使用时，我们还证明了该方法可以超越现有的state-of-the-art finetuning方法，包括在分布式和非分布式情况下的表现。
</details></li>
</ul>
<hr>
<h2 id="Data-driven-Nonlinear-Parametric-Model-Order-Reduction-Framework-using-Deep-Hierarchical-Variational-Autoencoder"><a href="#Data-driven-Nonlinear-Parametric-Model-Order-Reduction-Framework-using-Deep-Hierarchical-Variational-Autoencoder" class="headerlink" title="Data-driven Nonlinear Parametric Model Order Reduction Framework using Deep Hierarchical Variational Autoencoder"></a>Data-driven Nonlinear Parametric Model Order Reduction Framework using Deep Hierarchical Variational Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.06816">http://arxiv.org/abs/2307.06816</a></li>
<li>repo_url: None</li>
<li>paper_authors: SiHun Lee, Sangmin Lee, Kijoo Jang, Haeseong Cho, SangJoon Shin</li>
<li>for: 提出了一种基于深度人工神经网络的数据驱动参数缩放方法，用于非线性动力系统的参数 interpolating。</li>
<li>methods: 使用了一种叫做Least-Squares Hierarchical Variational Autoencoder（LSH-VAE）的深度人工神经网络，该网络能够进行非线性参数缩放。LSH-VAE 通过两种主要改进：嵌入了一个嵌入的深度结构和一个权重加权概率损失函数。这两种改进使得LSH-VAE 的准确性和稳定性与传统的非线性参数缩放方法、自动Encoder和Variational Autoencoder相比有了显著提高。</li>
<li>results: 基于LSH-VAE，提出了一种基于圆拟 interpolating 的参数缩放框架。该框架在三个非线性多物理动力系统上进行了验证和评估，并与传统的非线性参数缩放方法进行了比较。结果显示，LSH-VAE 能够在准确性和速度两个方面与传统方法相比有显著优势。<details>
<summary>Abstract</summary>
A data-driven parametric model order reduction (MOR) method using a deep artificial neural network is proposed. The present network, which is the least-squares hierarchical variational autoencoder (LSH-VAE), is capable of performing nonlinear MOR for the parametric interpolation of a nonlinear dynamic system with a significant number of degrees of freedom. LSH-VAE exploits two major changes to the existing networks: a hierarchical deep structure and a hybrid weighted, probabilistic loss function. The enhancements result in a significantly improved accuracy and stability compared against the conventional nonlinear MOR methods, autoencoder, and variational autoencoder. Upon LSH-VAE, a parametric MOR framework is presented based on the spherically linear interpolation of the latent manifold. The present framework is validated and evaluated on three nonlinear and multiphysics dynamic systems. First, the present framework is evaluated on the fluid-structure interaction benchmark problem to assess its efficiency and accuracy. Then, a highly nonlinear aeroelastic phenomenon, limit cycle oscillation, is analyzed. Finally, the present framework is applied to a three-dimensional fluid flow to demonstrate its capability of efficiently analyzing a significantly large number of degrees of freedom. The performance of LSH-VAE is emphasized by comparing its results against that of the widely used nonlinear MOR methods, convolutional autoencoder, and $\beta$-VAE. The present framework exhibits a significantly enhanced accuracy to the conventional methods while still exhibiting a large speed-up factor.
</details>
<details>
<summary>摘要</summary>
提出了一种基于深度人工神经网络的数据驱动参数化模型简化方法（MOR）。该网络为Least-Squares Hierarchical Variational Autoencoder（LSH-VAE），可以实现非线性MOR，用于 interpolating非线性动力系统中的多个自由度。LSH-VAE利用了两个主要改进：层次深度结构和权重加权概率损失函数。这些改进使得MOR的准确性和稳定性得到了明显提高，相比于传统的非线性MOR方法、自动encoder和variational autoencoder。基于LSH-VAE，一种基于圆拟 interpolating latent manifold的参数化MOR框架被提出。该框架在三个非线性和多物理动力系统上进行了验证和评估。首先，该框架在流体-结构交互问题上进行了效率和准确性的评估。然后，一种高度非线性的风动现象，限цик尔振荡，进行了分析。最后，该框架在三维流体流中进行了efficient地分析一个相对较大的自由度。LSH-VAE的性能被与传统的非线性MOR方法、卷积自动encoder和β-VAE进行了比较，其准确性与传统方法相比具有明显提高，同时仍然具有大快速因子。
</details></li>
</ul>
<hr>
<h2 id="CT-BERT-Learning-Better-Tabular-Representations-Through-Cross-Table-Pre-training"><a href="#CT-BERT-Learning-Better-Tabular-Representations-Through-Cross-Table-Pre-training" class="headerlink" title="CT-BERT: Learning Better Tabular Representations Through Cross-Table Pre-training"></a>CT-BERT: Learning Better Tabular Representations Through Cross-Table Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04308">http://arxiv.org/abs/2307.04308</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Ye, Guoshan Lu, Haobo Wang, Liyao Li, Sai Wu, Gang Chen, Junbo Zhao<br>for: 这个论文的目的是为了提出一种可以在大规模表格数据上预训练表格数据的方法，以便在表格数据上实现通用表示。methods: 这个论文使用了一种名为 CT-BERT 的新框架，该框架可以在跨表格上进行预训练。 CT-BERT 可以与 both 监督学习和自监学习方法一起使用，并且提出了一种基于对比学习的表格模型目标函数。results: 论文的实验结果显示，CT-BERT 在 15 个 dataset 上达到了州际前进的性能，其中包括监督学习和自监学习两种不同的设置。 CT-BERT 的性能都高于先前的方法。<details>
<summary>Abstract</summary>
Tabular data -- also known as structured data -- is one of the most common data forms in existence, thanks to the stable development and scaled deployment of database systems in the last few decades. At present however, despite the blast brought by large pre-trained models in other domains such as ChatGPT or SAM, how can we extract common knowledge across tables at a scale that may eventually lead to generalizable representation for tabular data remains a full blank. Indeed, there have been a few works around this topic. Most (if not all) of them are limited in the scope of a single table or fixed form of a schema. In this work, we first identify the crucial research challenges behind tabular data pre-training, particularly towards the cross-table scenario. We position the contribution of this work in two folds: (i)-we collect and curate nearly 2k high-quality tabular datasets, each of which is guaranteed to possess clear semantics, clean labels, and other necessary meta information. (ii)-we propose a novel framework that allows cross-table pre-training dubbed as CT-BERT. Noticeably, in light of pioneering the scaled cross-table training, CT-BERT is fully compatible with both supervised and self-supervised schemes, where the specific instantiation of CT-BERT is very much dependent on the downstream tasks. We further propose and implement a contrastive-learning-based and masked table modeling (MTM) objective into CT-BERT, that is inspired from computer vision and natural language processing communities but sophistically tailored to tables. The extensive empirical results on 15 datasets demonstrate CT-BERT's state-of-the-art performance, where both its supervised and self-supervised setups significantly outperform the prior approaches.
</details>
<details>
<summary>摘要</summary>
表格数据 -- 也称为结构化数据 -- 是现代数据的最常见形式，这主要归功于过去几十年内Database系统的稳定发展和大规模部署。然而，当前，即使大型预训模型在其他领域，如ChatGPT或SAM，所带来的冲击，总之，如何在大规模的表格数据中提取通用知识，以达到可generalizable的表格数据表示尚未得到解决。事实上，有一些相关的研究工作。大多数（如果不是所有）这些工作都受到单个表格或固定的表格Schema的限制。在这种工作中，我们首先 indentified the crucial research challenges behind tabular data pre-training, particularly in the cross-table scenario。我们的贡献在两个方面：（i）我们收集和精心整理了nearly 2k高质量的表格数据集，每个数据集都具有明确的 semantics、clean labels和其他必要的元信息。（ii）我们提出了一种新的框架，称为 CT-BERT，可以在跨表格上进行预训。另外，我们还提出了一种基于对比学习和遮盖表格模型（MTM）的目标函数，这种目标函数是从计算机视觉和自然语言处理社区中吸取的，但是它在表格上进行了精心修改。我们的实验结果表明，CT-BERT在15个数据集上 display state-of-the-art performance，其中包括supervised和self-supervised设置。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Piano-Transcription-with-Hierarchical-Frequency-Time-Transformer"><a href="#Automatic-Piano-Transcription-with-Hierarchical-Frequency-Time-Transformer" class="headerlink" title="Automatic Piano Transcription with Hierarchical Frequency-Time Transformer"></a>Automatic Piano Transcription with Hierarchical Frequency-Time Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04305">http://arxiv.org/abs/2307.04305</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sony/hft-transformer">https://github.com/sony/hft-transformer</a></li>
<li>paper_authors: Keisuke Toyama, Taketo Akama, Yukara Ikemiya, Yuhta Takida, Wei-Hsiang Liao, Yuki Mitsufuji</li>
<li>for:  automatic piano transcription, especially for determining the precise onset and offset of each note in polyphonic piano content.</li>
<li>methods:  hFT-Transformer, a two-level hierarchical frequency-time Transformer architecture that captures long-term dependencies in the frequency and time axes using self-attention mechanism.</li>
<li>results:  state-of-the-art performance on all F1-scores of metrics among Frame, Note, Note with Offset, and Note with Offset and Velocity estimations, as demonstrated on the widely used MAPS and MAESTRO v3.0.0 datasets.<details>
<summary>Abstract</summary>
Taking long-term spectral and temporal dependencies into account is essential for automatic piano transcription. This is especially helpful when determining the precise onset and offset for each note in the polyphonic piano content. In this case, we may rely on the capability of self-attention mechanism in Transformers to capture these long-term dependencies in the frequency and time axes. In this work, we propose hFT-Transformer, which is an automatic music transcription method that uses a two-level hierarchical frequency-time Transformer architecture. The first hierarchy includes a convolutional block in the time axis, a Transformer encoder in the frequency axis, and a Transformer decoder that converts the dimension in the frequency axis. The output is then fed into the second hierarchy which consists of another Transformer encoder in the time axis. We evaluated our method with the widely used MAPS and MAESTRO v3.0.0 datasets, and it demonstrated state-of-the-art performance on all the F1-scores of the metrics among Frame, Note, Note with Offset, and Note with Offset and Velocity estimations.
</details>
<details>
<summary>摘要</summary>
需要考虑长期 spectral 和 temporal 依赖性，以便自动识别钢琴乐谱。特别是在确定每个乐谱中的精确开始和结束时间点时，长期依赖性对于多重钢琴内容非常重要。在这种情况下，我们可以利用 Transformer 模型中的自我注意力机制，以捕捉在频谱和时间轴上的长期依赖性。在这项工作中，我们提出了 hFT-Transformer，这是一种使用两级叠加频谱-时间 Transformer 架构的自动音乐识别方法。第一层包括时间轴中的卷积块，频谱轴中的 Transformer 编码器，以及将频谱维度转换为时间轴的 Transformer 解码器。输出然后被 fed 到第二层，该层包括另一个时间轴中的 Transformer 编码器。我们对 widely 使用的 MAPS 和 MAESTRO v3.0.0 数据集进行了评估，并示出了所有 F1-scores 的 metric 中的状态顶峰性能。
</details></li>
</ul>
<hr>
<h2 id="Edge-Storage-Management-Recipe-with-Zero-Shot-Data-Compression-for-Road-Anomaly-Detection"><a href="#Edge-Storage-Management-Recipe-with-Zero-Shot-Data-Compression-for-Road-Anomaly-Detection" class="headerlink" title="Edge Storage Management Recipe with Zero-Shot Data Compression for Road Anomaly Detection"></a>Edge Storage Management Recipe with Zero-Shot Data Compression for Road Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04298">http://arxiv.org/abs/2307.04298</a></li>
<li>repo_url: None</li>
<li>paper_authors: YeongHyeon Park, Uju Gim, Myung Jin Kim</li>
<li>for: 本研究旨在提出一种高效的存储管理方法，以便在Edge计算环境中进行噪声检测系统的实现。</li>
<li>methods: 本研究提出了一种使用预训练 autoencoder 进行数据压缩的方法，以便减少数据的存储和传输成本。</li>
<li>results: 经过对比性测试，我们发现该方法可以保持噪声检测性能，同时提高存储和传输效率。<details>
<summary>Abstract</summary>
Recent studies show edge computing-based road anomaly detection systems which may also conduct data collection simultaneously. However, the edge computers will have small data storage but we need to store the collected audio samples for a long time in order to update existing models or develop a novel method. Therefore, we should consider an approach for efficient storage management methods while preserving high-fidelity audio. A hardware-perspective approach, such as using a low-resolution microphone, is an intuitive way to reduce file size but is not recommended because it fundamentally cuts off high-frequency components. On the other hand, a computational file compression approach that encodes collected high-resolution audio into a compact code should be recommended because it also provides a corresponding decoding method. Motivated by this, we propose a way of simple yet effective pre-trained autoencoder-based data compression method. The pre-trained autoencoder is trained for the purpose of audio super-resolution so it can be utilized to encode or decode any arbitrary sampling rate. Moreover, it will reduce the communication cost for data transmission from the edge to the central server. Via the comparative experiments, we confirm that the zero-shot audio compression and decompression highly preserve anomaly detection performance while enhancing storage and transmission efficiency.
</details>
<details>
<summary>摘要</summary>
近期研究显示基于边缘计算的公路异常检测系统可能同时进行数据收集。然而，边缘计算机器具有小容量数据存储，我们需要长期保存收集的音频采样以更新现有模型或开发新方法。因此，我们应该考虑一种高效存储管理方法，同时保持高质量音频。一种硬件视角的方法，如使用低分辨率 Mikrofon，不建议使用，因为它基本上切断高频组件。相反，一种计算机件压缩方法，通过编码收集的高分辨率音频为紧凑的编码，应该得到推荐。受到这一点的激励，我们提出了简单又有效的预训练自动编码器基于数据压缩方法。预训练自动编码器是为了音频超分辨率而训练的，因此可以用于编码或解码任何采样率。此外，它还会降低边缘到中央服务器的数据传输成本。通过比较实验，我们证明了零aser音频压缩和解压缩可以高度保持异常检测性能，同时提高存储和传输效率。
</details></li>
</ul>
<hr>
<h2 id="Online-Ad-Procurement-in-Non-stationary-Autobidding-Worlds"><a href="#Online-Ad-Procurement-in-Non-stationary-Autobidding-Worlds" class="headerlink" title="Online Ad Procurement in Non-stationary Autobidding Worlds"></a>Online Ad Procurement in Non-stationary Autobidding Worlds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05698">http://arxiv.org/abs/2307.05698</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jason Cheuk Nam Liang, Haihao Lu, Baoyu Zhou</li>
<li>for: 本研究旨在帮助在线广告主动ynamically优化广告平台的杠杆决策，以便更好地管理媒体购买预算和竞争对手的影响，并在长期环境下减少 regret。</li>
<li>methods: 本研究使用了在线学习框架，并引入了 primal-dual 算法来解决多维决策变量、飞行反馈和长期不确定约束的问题。</li>
<li>results: 本研究表明，我们的算法在不知道飞行过程的情况下可以 achieve low regret in many worlds，并且可以在不同类型的飞行过程中提供优化的杠杆决策。<details>
<summary>Abstract</summary>
Today's online advertisers procure digital ad impressions through interacting with autobidding platforms: advertisers convey high level procurement goals via setting levers such as budget, target return-on-investment, max cost per click, etc.. Then ads platforms subsequently procure impressions on advertisers' behalf, and report final procurement conversions (e.g. click) to advertisers. In practice, advertisers may receive minimal information on platforms' procurement details, and procurement outcomes are subject to non-stationary factors like seasonal patterns, occasional system corruptions, and market trends which make it difficult for advertisers to optimize lever decisions effectively. Motivated by this, we present an online learning framework that helps advertisers dynamically optimize ad platform lever decisions while subject to general long-term constraints in a realistic bandit feedback environment with non-stationary procurement outcomes. In particular, we introduce a primal-dual algorithm for online decision making with multi-dimension decision variables, bandit feedback and long-term uncertain constraints. We show that our algorithm achieves low regret in many worlds when procurement outcomes are generated through procedures that are stochastic, adversarial, adversarially corrupted, periodic, and ergodic, respectively, without having to know which procedure is the ground truth. Finally, we emphasize that our proposed algorithm and theoretical results extend beyond the applications of online advertising.
</details>
<details>
<summary>摘要</summary>
今天的在线广告主通过交互式自动拍卖平台购买数字广告印象：广告主通过设置杠杆如预算、目标回报率、最高单击成本等来传递高级购买目标。然后广告平台会在广告主的 behalf 购买印象，并将最终购买转化（例如点击）报告给广告主。在实践中，广告主可能会收到最小的平台购买详细信息，并且购买结果受到不同因素的影响，如季节性模式、 occasional 系统腐蚀和市场趋势，这使得广告主难以有效地优化杠杆决策。为了解决这个问题，我们提出了一个在线学习框架，帮助广告主在面临实际的链接环境下动态优化广告平台杠杆决策，同时遵循长期不确定的约束。具体来说，我们引入了 primal-dual 算法，用于在线决策中的多维决策变量、链接反馈和长期不确定约束。我们证明了我们的算法在许多世界中具有低念悟，无需知道采用哪种程序是真实的。最后，我们强调了我们的提出的算法和理论结果超出了在线广告应用的限制，可以应用于其他领域。
</details></li>
</ul>
<hr>
<h2 id="Generalizing-Graph-ODE-for-Learning-Complex-System-Dynamics-across-Environments"><a href="#Generalizing-Graph-ODE-for-Learning-Complex-System-Dynamics-across-Environments" class="headerlink" title="Generalizing Graph ODE for Learning Complex System Dynamics across Environments"></a>Generalizing Graph ODE for Learning Complex System Dynamics across Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04287">http://arxiv.org/abs/2307.04287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zijie Huang, Yizhou Sun, Wei Wang</li>
<li>for: 学习多体系统动力学，特别是在生物学中的分子动力学，以便更好地预测系统的未来轨迹。</li>
<li>methods: 我们提出了一种机器学习框架，名为GG-ODE（通用图Ordinary Differential Equations），可以学习不同环境中系统动力学的连续性。我们使用图神经网络（GNNs）参数化神经普通 diferencial equations（ODEs），以捕捉多体系统之间的连续交互。为了实现模型泛化，我们假设不同环境下的动力学都受到共同的物理法则控制，可以通过学习共享的ODE函数来捕捉。</li>
<li>results: 我们的模型可以准确预测系统动力学，特别是在长距离上，并且可以将新系统中的少量观测数据泛化到整个系统。在多种物理 simulate 中进行了实验，我们的模型能够准确预测系统动力学，特别是在长距离上，并且可以将新系统中的少量观测数据泛化到整个系统。<details>
<summary>Abstract</summary>
Learning multi-agent system dynamics has been extensively studied for various real-world applications, such as molecular dynamics in biology. Most of the existing models are built to learn single system dynamics from observed historical data and predict the future trajectory. In practice, however, we might observe multiple systems that are generated across different environments, which differ in latent exogenous factors such as temperature and gravity. One simple solution is to learn multiple environment-specific models, but it fails to exploit the potential commonalities among the dynamics across environments and offers poor prediction results where per-environment data is sparse or limited. Here, we present GG-ODE (Generalized Graph Ordinary Differential Equations), a machine learning framework for learning continuous multi-agent system dynamics across environments. Our model learns system dynamics using neural ordinary differential equations (ODE) parameterized by Graph Neural Networks (GNNs) to capture the continuous interaction among agents. We achieve the model generalization by assuming the dynamics across different environments are governed by common physics laws that can be captured via learning a shared ODE function. The distinct latent exogenous factors learned for each environment are incorporated into the ODE function to account for their differences. To improve model performance, we additionally design two regularization losses to (1) enforce the orthogonality between the learned initial states and exogenous factors via mutual information minimization; and (2) reduce the temporal variance of learned exogenous factors within the same system via contrastive learning. Experiments over various physical simulations show that our model can accurately predict system dynamics, especially in the long range, and can generalize well to new systems with few observations.
</details>
<details>
<summary>摘要</summary>
学习多智能体系统动态已经广泛研究了各种现实世界应用，如生物分子动力学。大多数现有模型都是建立来学习单个系统动态从观察到数据，预测未来轨迹。然而，在实践中，我们可能会观察到来自不同环境的多个系统，这些系统之间的潜在因素不同，如温度和重力。一个简单的解决方案是学习每个环境特定的模型，但这会忽略系统动态之间的共同特征，并且预测结果在每个环境数据稀缺或有限时会不佳。我们提出了GG-ODE（通用图 ordininary differential equations）机器学习框架，用于学习连续多智能体系统动态。我们的模型通过使用图神经网络（GNNs）参数化神经ordinary differential equations（ODE）来捕捉连续智能体之间的交互。我们通过假设不同环境的动态都受到共同的物理法则所控制，以学习共同的ODE函数来泛化模型。每个环境的潜在隐藏因素被学习到ODE函数中，以考虑它们之间的差异。为了提高模型性能，我们还设计了两种常见化loss，即（1）通过对学习的初始状态和隐藏因素进行相互信息减少来保持对隐藏因素的正交性; 和（2）在同一个系统中减少学习的时间异谱。通过对各种物理 simulate experiment 进行测试，我们发现我们的模型可以准确预测系统动态，特别是在长距离内，并且可以良好地泛化到新系统。
</details></li>
</ul>
<hr>
<h2 id="Assessing-the-efficacy-of-large-language-models-in-generating-accurate-teacher-responses"><a href="#Assessing-the-efficacy-of-large-language-models-in-generating-accurate-teacher-responses" class="headerlink" title="Assessing the efficacy of large language models in generating accurate teacher responses"></a>Assessing the efficacy of large language models in generating accurate teacher responses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04274">http://arxiv.org/abs/2307.04274</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yann Hicke, Abhishek Masand, Wentao Guo, Tushaar Gangavarapu</li>
<li>for: 这个研究旨在评估大语言模型在提供教育性帮助和指导中的生成能力，以采用知识able teacher的角色进行 simulate。</li>
<li>methods: 研究使用了多个标准的生成模型，包括GPT-4（少量示例学习）、精度调整GPT-2和DialogGPT，以及通过强化学习来优化Flan-T5模型。</li>
<li>results: 实验结果表明，GPT-4在BERTScore和DialogRPT上的表现较高，而其他精度调整模型表现较差，这些结果提示了 dataset 特性的影响，如采样、代表性和对话完整性，对于精度调整具有 significanteffect。<details>
<summary>Abstract</summary>
(Tack et al., 2023) organized the shared task hosted by the 18th Workshop on Innovative Use of NLP for Building Educational Applications on generation of teacher language in educational dialogues. Following the structure of the shared task, in this study, we attempt to assess the generative abilities of large language models in providing informative and helpful insights to students, thereby simulating the role of a knowledgeable teacher. To this end, we present an extensive evaluation of several benchmarking generative models, including GPT-4 (few-shot, in-context learning), fine-tuned GPT-2, and fine-tuned DialoGPT. Additionally, to optimize for pedagogical quality, we fine-tuned the Flan-T5 model using reinforcement learning. Our experimental findings on the Teacher-Student Chatroom Corpus subset indicate the efficacy of GPT-4 over other fine-tuned models, measured using BERTScore and DialogRPT.   We hypothesize that several dataset characteristics, including sampling, representativeness, and dialog completeness, pose significant challenges to fine-tuning, thus contributing to the poor generalizability of the fine-tuned models. Finally, we note the need for these generative models to be evaluated with a metric that relies not only on dialog coherence and matched language modeling distribution but also on the model's ability to showcase pedagogical skills.
</details>
<details>
<summary>摘要</summary>
我们认为，共同任务的 dataset 特点，包括采样、代表性和对话完整性，对于调整 pose  significiant 挑战，从而导致调整模型的泛化性不佳。最后，我们注意到，为了评估这些生成模型，需要使用一个指标，不仅考虑对话 coherence 和语言模型的匹配分布，还需要考虑模型的教学技能展示能力。
</details></li>
</ul>
<hr>
<h2 id="MentalHealthAI-Utilizing-Personal-Health-Device-Data-to-Optimize-Psychiatry-Treatment"><a href="#MentalHealthAI-Utilizing-Personal-Health-Device-Data-to-Optimize-Psychiatry-Treatment" class="headerlink" title="MentalHealthAI: Utilizing Personal Health Device Data to Optimize Psychiatry Treatment"></a>MentalHealthAI: Utilizing Personal Health Device Data to Optimize Psychiatry Treatment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04777">http://arxiv.org/abs/2307.04777</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manan Shukla, Oshani Seneviratne</li>
<li>for: 提供个性化心理健康跟踪和情绪预测系统，以帮助心理医生更好地了解患有心理疾病的患者的情况，并提供更有效的心理疾病治疗和管理方法。</li>
<li>methods: 利用个体 физиологи数据，combines transfer和 federated机器学习概念，使用智能合约，保持数据在用户设备上，实现隐私感知和负责任的心理健康跟踪。</li>
<li>results: 使用流行的心理健康数据集，实现了Promising results，表明我们的方法可以提供有效的心理健康跟踪和预测。<details>
<summary>Abstract</summary>
Mental health disorders remain a significant challenge in modern healthcare, with diagnosis and treatment often relying on subjective patient descriptions and past medical history. To address this issue, we propose a personalized mental health tracking and mood prediction system that utilizes patient physiological data collected through personal health devices. Our system leverages a decentralized learning mechanism that combines transfer and federated machine learning concepts using smart contracts, allowing data to remain on users' devices and enabling effective tracking of mental health conditions for psychiatric treatment and management in a privacy-aware and accountable manner. We evaluate our model using a popular mental health dataset that demonstrates promising results. By utilizing connected health systems and machine learning models, our approach offers a novel solution to the challenge of providing psychiatrists with further insight into their patients' mental health outside of traditional office visits.
</details>
<details>
<summary>摘要</summary>
精神健康问题仍然是现代医疗中的主要挑战，诊断和治疗往往基于患者主观描述和医疗历史。为解决这一问题，我们提出了个性化精神健康跟踪和情绪预测系统，该系统利用通过个人健康设备收集的患者生物数据。我们的系统采用分布式学习机制，结合了转移和联邦机器学习概念，使得数据能够留在用户设备上，并且有效地跟踪精神健康状况，供心理医生进行精神疾病治疗和管理，同时具有隐私和负责任的特点。我们使用了一个流行的精神健康数据集，并得到了批判性的结果。通过连接医疗系统和机器学习模型，我们的方法提供了一种新的解决方案，即为心理医生在传统办公室访问之外提供更多的精神健康情况的信息。
</details></li>
</ul>
<hr>
<h2 id="RidgeBase-A-Cross-Sensor-Multi-Finger-Contactless-Fingerprint-Dataset"><a href="#RidgeBase-A-Cross-Sensor-Multi-Finger-Contactless-Fingerprint-Dataset" class="headerlink" title="RidgeBase: A Cross-Sensor Multi-Finger Contactless Fingerprint Dataset"></a>RidgeBase: A Cross-Sensor Multi-Finger Contactless Fingerprint Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05563">http://arxiv.org/abs/2307.05563</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bhavinjawade/RidgeBase_Fingerprint_Camera_App">https://github.com/bhavinjawade/RidgeBase_Fingerprint_Camera_App</a></li>
<li>paper_authors: Bhavin Jawade, Deen Dayal Mohan, Srirangaraj Setlur, Nalini Ratha, Venu Govindaraju</li>
<li>For: 这 paper 的目的是提出一个大规模实际数据集，以促进无接触指纹识别技术的进一步发展。* Methods: 这 paper 使用了两种摄像头和一个平板式接触传感器来收集 contactless 和 contact-based 指纹图像，并提出了一种基于 facial recognition 数据集的集成匹配协议来处理 intra-sample 差异。* Results: 这 paper 的结果显示，使用 COTS 指纹匹配器和深度 CNN 方法在 RidgeBase 数据集上实现了高度准确的指纹识别。<details>
<summary>Abstract</summary>
Contactless fingerprint matching using smartphone cameras can alleviate major challenges of traditional fingerprint systems including hygienic acquisition, portability and presentation attacks. However, development of practical and robust contactless fingerprint matching techniques is constrained by the limited availability of large scale real-world datasets. To motivate further advances in contactless fingerprint matching across sensors, we introduce the RidgeBase benchmark dataset. RidgeBase consists of more than 15,000 contactless and contact-based fingerprint image pairs acquired from 88 individuals under different background and lighting conditions using two smartphone cameras and one flatbed contact sensor. Unlike existing datasets, RidgeBase is designed to promote research under different matching scenarios that include Single Finger Matching and Multi-Finger Matching for both contactless- to-contactless (CL2CL) and contact-to-contactless (C2CL) verification and identification. Furthermore, due to the high intra-sample variance in contactless fingerprints belonging to the same finger, we propose a set-based matching protocol inspired by the advances in facial recognition datasets. This protocol is specifically designed for pragmatic contactless fingerprint matching that can account for variances in focus, polarity and finger-angles. We report qualitative and quantitative baseline results for different protocols using a COTS fingerprint matcher (Verifinger) and a Deep CNN based approach on the RidgeBase dataset. The dataset can be downloaded here: https://www.buffalo.edu/cubs/research/datasets/ridgebase-benchmark-dataset.html
</details>
<details>
<summary>摘要</summary>
请注意，以下文本将使用简化中文表示法。无接触指纹识别使用智能手机镜头可以解决传统指纹系统中的主要挑战，包括医疗安全、可移植性和展示攻击。然而，发展实用且可靠的无接触指纹识别技术受到实际数据的有限可用性所限制。为了鼓励进一步的无接触指纹识别技术发展，我们介绍了RidgeBase参考 dataset。RidgeBase 包含了15,000多个无接触和接触基于指纹图像的对照项目，来自88名参与者，在不同的背景和照明条件下使用两款智能手机镜头和一款平板式接触感应器所取得。与现有数据集不同的是，RidgeBase 是设计来鼓励研究不同的匹配enario，包括单指纹匹配和多指纹匹配，并且包括CL2CL和C2CL验证和识别。此外，由于无接触指纹内部的同 fingers 的标本之间的高同一样性，我们提出了一个基于人脸识别数据集的集合匹配协议。这个协议特别适用于实用无接触指纹识别，可以考虑到专注、极化和手均角度的变化。我们将在RidgeBase dataset上报告基于不同协议的 qualitative 和量化基eline结果，使用商业 fingerprint 匹配软件（Verifinger）和深度 CNN 基础的方法。数据可以在以下网址下载：https://www.buffalo.edu/cubs/research/datasets/ridgebase-benchmark-dataset.html。
</details></li>
</ul>
<hr>
<h2 id="The-Future-of-Fundamental-Science-Led-by-Generative-Closed-Loop-Artificial-Intelligence"><a href="#The-Future-of-Fundamental-Science-Led-by-Generative-Closed-Loop-Artificial-Intelligence" class="headerlink" title="The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence"></a>The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07522">http://arxiv.org/abs/2307.07522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hector Zenil, Jesper Tegnér, Felipe S. Abrahão, Alexander Lavin, Vipin Kumar, Jeremy G. Frey, Adrian Weller, Larisa Soldatova, Alan R. Bundy, Nicholas R. Jennings, Koichi Takahashi, Lawrence Hunter, Saso Dzeroski, Andrew Briggs, Frederick D. Gregory, Carla P. Gomes, Christopher K. I. Williams, Jon Rowe, James Evans, Hiroaki Kitano, Joshua B. Tenenbaum, Ross King</li>
<li>For: The paper explores the potential of AI-driven, automated, closed-loop approach to scientific discovery, including self-driven hypothesis generation and open-ended autonomous exploration of the hypothesis space.* Methods: The paper discusses the use of Generative AI and Large Language Models to augment and accelerate the scientific discovery of fundamental deep science with quantitative models.* Results: The paper suggests that integrating AI-driven automation into the practice of science could mitigate current problems, including the replication of findings, systematic production of data, and ultimately democratise the scientific process.<details>
<summary>Abstract</summary>
Recent advances in machine learning and AI, including Generative AI and LLMs, are disrupting technological innovation, product development, and society as a whole. AI's contribution to technology can come from multiple approaches that require access to large training data sets and clear performance evaluation criteria, ranging from pattern recognition and classification to generative models. Yet, AI has contributed less to fundamental science in part because large data sets of high-quality data for scientific practice and model discovery are more difficult to access. Generative AI, in general, and Large Language Models in particular, may represent an opportunity to augment and accelerate the scientific discovery of fundamental deep science with quantitative models. Here we explore and investigate aspects of an AI-driven, automated, closed-loop approach to scientific discovery, including self-driven hypothesis generation and open-ended autonomous exploration of the hypothesis space. Integrating AI-driven automation into the practice of science would mitigate current problems, including the replication of findings, systematic production of data, and ultimately democratisation of the scientific process. Realising these possibilities requires a vision for augmented AI coupled with a diversity of AI approaches able to deal with fundamental aspects of causality analysis and model discovery while enabling unbiased search across the space of putative explanations. These advances hold the promise to unleash AI's potential for searching and discovering the fundamental structure of our world beyond what human scientists have been able to achieve. Such a vision would push the boundaries of new fundamental science rather than automatize current workflows and instead open doors for technological innovation to tackle some of the greatest challenges facing humanity today.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ChatGPT-in-the-Age-of-Generative-AI-and-Large-Language-Models-A-Concise-Survey"><a href="#ChatGPT-in-the-Age-of-Generative-AI-and-Large-Language-Models-A-Concise-Survey" class="headerlink" title="ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey"></a>ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04251">http://arxiv.org/abs/2307.04251</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iamgmujtaba/scholar_search">https://github.com/iamgmujtaba/scholar_search</a></li>
<li>paper_authors: Salman Mohamadi, Ghulam Mujtaba, Ngan Le, Gianfranco Doretto, Donald A. Adjeroh<br>for: 这 paper 的主要目的是为了提供一份简洁的survey关于 ChatGPT 的当前研究进展和演化。methods: 这 paper 使用了两种视角来研究 ChatGPT：玻璃盒视角（glass box view）和黑盒视角（black box view）。玻璃盒视角专注于理解技术的内部工作方式，而黑盒视角则视其为一个复杂系统，从输入、输出和效果的角度进行研究。results: 这 paper 提供了一份全面的探究 ChatGPT 技术的研究进展和应用前景，同时也提出了进一步研究的必要性和潜在应用领域。此外， paper 还 shed light on  LLMS 和GAI 的基础文献，并探讨了这些技术在教育、研究、医疗、金融等领域的应用和关键问题。<details>
<summary>Abstract</summary>
ChatGPT is a large language model (LLM) created by OpenAI that has been carefully trained on a large amount of data. It has revolutionized the field of natural language processing (NLP) and has pushed the boundaries of LLM capabilities. ChatGPT has played a pivotal role in enabling widespread public interaction with generative artificial intelligence (GAI) on a large scale. It has also sparked research interest in developing similar technologies and investigating their applications and implications. In this paper, our primary goal is to provide a concise survey on the current lines of research on ChatGPT and its evolution. We considered both the glass box and black box views of ChatGPT, encompassing the components and foundational elements of the technology, as well as its applications, impacts, and implications. The glass box approach focuses on understanding the inner workings of the technology, and the black box approach embraces it as a complex system, and thus examines its inputs, outputs, and effects. This paves the way for a comprehensive exploration of the technology and provides a road map for further research and experimentation. We also lay out essential foundational literature on LLMs and GAI in general and their connection with ChatGPT. This overview sheds light on existing and missing research lines in the emerging field of LLMs, benefiting both public users and developers. Furthermore, the paper delves into the broad spectrum of applications and significant concerns in fields such as education, research, healthcare, finance, etc.
</details>
<details>
<summary>摘要</summary>
chatGPT是一个大型自然语言模型（LLM），由OpenAI精心训练了大量数据。它对自然语言处理（NLP）领域产生了革命性的变革，并推动了LLM的可能性的极限。chatGPT使得大规模的人工智能生成（GAI）与公众进行交互，并且引发了研究人员对类似技术的开发和应用的兴趣。在这篇论文中，我们的主要目标是提供对chatGPT的当前研究进展和演化的简短概述。我们包括了“玻璃盒”和“黑盒”两种视角，即理解技术的内部结构和行为，以及对其输入、输出和影响的研究。这种方法使得我们可以全面探索技术，并为进一步的研究和实验提供了道路图。此外，我们还提供了LLM和GAI的基础文献，这些文献对于公众和开发者都是必读的。此概述照明了LLM领域的现有和缺失的研究方向，并且探讨了GAI在各个领域的应用和关注点，如教育、研究、医疗、金融等。
</details></li>
</ul>
<hr>
<h2 id="Ensemble-learning-for-blending-gridded-satellite-and-gauge-measured-precipitation-data"><a href="#Ensemble-learning-for-blending-gridded-satellite-and-gauge-measured-precipitation-data" class="headerlink" title="Ensemble learning for blending gridded satellite and gauge-measured precipitation data"></a>Ensemble learning for blending gridded satellite and gauge-measured precipitation data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.06840">http://arxiv.org/abs/2307.06840</a></li>
<li>repo_url: None</li>
<li>paper_authors: Georgia Papacharalampous, Hristos Tyralis, Nikolaos Doulamis, Anastasios Doulamis</li>
<li>for: 提高卫星降水产品的准确性</li>
<li>methods: 使用11种新的ensemble学习算法，包括多变量适应回归splines、多变量适应多项式splines、随机森林、梯度提升机、极大梯度提升和 bayesian常量化神经网络</li>
<li>results: 对整个美国陆地区和15年时间段进行了广泛的比较，并发现ensemble学习算法可以提高卫星降水产品的准确性。<details>
<summary>Abstract</summary>
Regression algorithms are regularly used for improving the accuracy of satellite precipitation products. In this context, ground-based measurements are the dependent variable and the satellite data are the predictor variables, together with topography factors. Alongside this, it is increasingly recognised in many fields that combinations of algorithms through ensemble learning can lead to substantial predictive performance improvements. Still, a sufficient number of ensemble learners for improving the accuracy of satellite precipitation products and their large-scale comparison are currently missing from the literature. In this work, we fill this specific gap by proposing 11 new ensemble learners in the field and by extensively comparing them for the entire contiguous United States and for a 15-year period. We use monthly data from the PERSIANN (Precipitation Estimation from Remotely Sensed Information using Artificial Neural Networks) and IMERG (Integrated Multi-satellitE Retrievals for GPM) gridded datasets. We also use gauge-measured precipitation data from the Global Historical Climatology Network monthly database, version 2 (GHCNm). The ensemble learners combine the predictions by six regression algorithms (base learners), namely the multivariate adaptive regression splines (MARS), multivariate adaptive polynomial splines (poly-MARS), random forests (RF), gradient boosting machines (GBM), extreme gradient boosting (XGBoost) and Bayesian regularized neural networks (BRNN), and each of them is based on a different combiner. The combiners include the equal-weight combiner, the median combiner, two best learners and seven variants of a sophisticated stacking method. The latter stacks a regression algorithm on the top of the base learners to combine their independent predictions...
</details>
<details>
<summary>摘要</summary>
干涉算法常用于提高卫星降水产品的准确性。在这个上下文中，地面测量是dependent变量，而卫星数据和地形因素是预测变量。此外，逐渐认识到，将算法组合在ensemble学习中可以导致重要的预测性能提高。然而，卫星降水产品的精度 improvemen要求足够多的ensemble学习者，而现有文献中缺乏这些学习者。在这项工作中，我们填充这个空白，并提出11种新的ensemble学习者，并对整个大陆和15年时间进行了广泛的比较。我们使用PERSIANN（Precipitation Estimation from Remotely Sensed Information using Artificial Neural Networks）和IMERG（Integrated Multi-satellitE Retrievals for GPM）的月度数据，以及GHCNm（Global Historical Climatology Network monthly database, version 2）中的 gauge-measured precipitation数据。ensemble学习者将base learner（六种回归算法）的预测结果进行组合，其中包括平均值 combiner、 median combiner、两个best learner和七种堆叠方法的七种变种。这些堆叠方法都基于不同的combiner。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Bayesian-travel-time-tomography-with-geologically-complex-priors-using-sensitivity-informed-polynomial-chaos-expansion-and-deep-generative-networks"><a href="#Efficient-Bayesian-travel-time-tomography-with-geologically-complex-priors-using-sensitivity-informed-polynomial-chaos-expansion-and-deep-generative-networks" class="headerlink" title="Efficient Bayesian travel-time tomography with geologically-complex priors using sensitivity-informed polynomial chaos expansion and deep generative networks"></a>Efficient Bayesian travel-time tomography with geologically-complex priors using sensitivity-informed polynomial chaos expansion and deep generative networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04228">http://arxiv.org/abs/2307.04228</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giovanni Angelo Meles, Macarena Amaya, Shiran Levy, Stefano Marelli, Niklas Linde</li>
<li>for: This paper focuses on developing a strategy for Bayesian travel-time tomography using Monte Carlo Markov Chain (MCMC) methods, which can accurately characterize the prior distribution and efficiently evaluate the likelihood.</li>
<li>methods: The paper combines the use of principal component analysis (PCA) and polynomial chaos expansion (PCE) to develop a surrogate model for the forward problem, and leverages variational autoencoders (VAEs) to represent the prior distribution.</li>
<li>results: The proposed method enables accurate reconstruction of the true travel times and provides a viable alternative to traditional MCMC methods, which can be computationally expensive and challenging to implement.<details>
<summary>Abstract</summary>
Monte Carlo Markov Chain (MCMC) methods commonly confront two fundamental challenges: the accurate characterization of the prior distribution and the efficient evaluation of the likelihood. In the context of Bayesian studies on tomography, principal component analysis (PCA) can in some cases facilitate the straightforward definition of the prior distribution, while simultaneously enabling the implementation of accurate surrogate models based on polynomial chaos expansion (PCE) to replace computationally intensive full-physics forward solvers. When faced with scenarios where PCA does not offer a direct means of easily defining the prior distribution alternative methods like deep generative models (e.g., variational autoencoders (VAEs)), can be employed as viable options. However, accurately producing a surrogate capable of capturing the intricate non-linear relationship between the latent parameters of a VAE and the outputs of forward modeling presents a notable challenge. Indeed, while PCE models provide high accuracy when the input-output relationship can be effectively approximated by relatively low-degree multivariate polynomials, this condition is typically unmet when utilizing latent variables derived from deep generative models. In this contribution, we present a strategy that combines the excellent reconstruction performances of VAE in terms of prio representation with the accuracy of PCA-PCE surrogate modeling in the context of Bayesian ground penetrating radar (GPR) travel-time tomography. Within the MCMC process, the parametrization of the VAE is leveraged for prior exploration and sample proposal. Concurrently, modeling is conducted using PCE, which operates on either globally or locally defined principal components of the VAE samples under examination.
</details>
<details>
<summary>摘要</summary>
Monte Carlo Markov Chain（MCMC）方法常遇到两个基本挑战：准确地 characteryrization of the prior distribution和高效地评估 likelihood。在扩展学中的Tomography研究中，主成分分析（PCA）可以在一些情况下使得归一化分布的定义变得更加直观，同时允许通过多项式混合（PCE）来替代 computationally intensive full-physics forward solvers。然而，在PCA无法直接定义归一化分布的情况下，可以使用深度生成模型（例如变量自动编码器（VAEs））作为可行的选择。然而，生成高精度的surrogate模型，以capture latent parameters of VAE和前向模型之间的复杂非线性关系，则成为一大挑战。实际上，PCE模型在输入-输出关系可以高度有效地被近似为低度多ivariate polynomials时，具有高准确性。然而，这种条件通常不满足使用来自深度生成模型 derivated的latent variables。在这篇论文中，我们提出一种策略，将VAE的很好的重建性与PCA-PCE模型的准确性相结合，以进行 Bayesian ground penetrating radar（GPR） travel-time tomography。在MCMC过程中，VAE的 Parametrization被用于归一化和样本提议。同时，使用PCE进行模型化， operate on either globally or locally defined principal components of VAE samples under examination。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Autoencoder-based-Lossy-Compression-for-Large-scale-High-resolution-Scientific-Data"><a href="#Hierarchical-Autoencoder-based-Lossy-Compression-for-Large-scale-High-resolution-Scientific-Data" class="headerlink" title="Hierarchical Autoencoder-based Lossy Compression for Large-scale High-resolution Scientific Data"></a>Hierarchical Autoencoder-based Lossy Compression for Large-scale High-resolution Scientific Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04216">http://arxiv.org/abs/2307.04216</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hieutrungle/data-slim">https://github.com/hieutrungle/data-slim</a></li>
<li>paper_authors: Hieu Le, Hernan Santos, Jian Tao<br>for:  compress large-scale scientific data while maintaining high reconstruction qualitymethods:  uses a neural network-based approach with Autoencoder architectureresults:  achieves a compression ratio of 140 on several benchmark data sets, and 200 on simulation data from the High-Resolution Community Earth System Model (CESM) Version 1.3 with negligible reconstruction error.<details>
<summary>Abstract</summary>
Lossy compression has become an important technique to reduce data size in many domains. This type of compression is especially valuable for large-scale scientific data, whose size ranges up to several petabytes. Although Autoencoder-based models have been successfully leveraged to compress images and videos, such neural networks have not widely gained attention in the scientific data domain. Our work presents a neural network that not only significantly compresses large-scale scientific data but also maintains high reconstruction quality. The proposed model is tested with scientific benchmark data available publicly and applied to a large-scale high-resolution climate modeling data set. Our model achieves a compression ratio of 140 on several benchmark data sets without compromising the reconstruction quality. Simulation data from the High-Resolution Community Earth System Model (CESM) Version 1.3 over 500 years are also being compressed with a compression ratio of 200 while the reconstruction error is negligible for scientific analysis.
</details>
<details>
<summary>摘要</summary>
lossy compression技术在许多领域中已成为重要的数据压缩方法。特别是在大规模科学数据领域，数据的大小可以达到数十个petabyte级别。虽然基于Autoencoder的模型已成功地压缩图像和视频，但这些神经网络在科学数据领域尚未得到广泛关注。我们的工作推出了一种能够高效压缩大规模科学数据，同时保持高重建质量的神经网络模型。我们的模型在公共预测数据集上进行测试，并应用于大规模高分辨率气候模拟数据集。我们的模型在多个 benchmark 数据集上实现了压缩率为140，而且重建质量几乎不受影响。同时，我们还对高分辨率社区地球系统模型（CESM）版本1.3的500年 simultion 数据进行压缩，压缩率达200，重建错误几乎可以忽略不计。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Action-based-Ball-Recovery-Model-using-360-circ-data"><a href="#Generalized-Action-based-Ball-Recovery-Model-using-360-circ-data" class="headerlink" title="Generalized Action-based Ball Recovery Model using 360$^\circ$ data"></a>Generalized Action-based Ball Recovery Model using 360$^\circ$ data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04215">http://arxiv.org/abs/2307.04215</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ricardo Furbino Marques do Nascimento, Hugo M. R. Rios-Neto</li>
<li>for: 本研究的目的是答 answer the question of what actions lead to a change in possession, and how a team’s positioning affects ball recovery.</li>
<li>methods: 本研究使用了Statsbomb 360$^\circ$ 数据创建了一个通用的动作基于球 Possession 模型（GABR）。</li>
<li>results: 研究发现，在各种不同的防御方式下，球 Possession 的变化是由几种不同的动作引起的，而这些动作与防御方式之间存在着正相关关系。此外，研究还发现了一些球队的位置对球 Possession 的影响。<details>
<summary>Abstract</summary>
Even though having more possession does not necessarily lead to winning, teams like Manchester City, Liverpool, and Leeds United notably have tried to recover the ball quickly after they lost it over the past few years. Nowadays, some of the top managers in the world apply high-pressing styles, and concepts such as the five-second rule, usually credited to Guardiola, have been spreading out [9][10], becoming a fundamental part of how lots of teams have played over the recent years. Expressions like "don't let them breathe" and "get the ball back as soon as possible" are often heard in the media [4][5][6], but what are the actions that most lead to a change in possession? What is the influence of a team's positioning on the ball recovery? Which are the players that more often collapse when under pressure? Can we evaluate the defensive dynamics of teams that do not necessarily press the player in possession as intensely as those mentioned above? We try to answer those and other questions in this paper by creating a Generalized Action based Ball Recovery model (GABR) using Statsbomb 360$^\circ$ data.
</details>
<details>
<summary>摘要</summary>
即使 possessed 更多不一定会导致赢球，但是球队如曼城、利物浦和利兹联等在过去几年来尝试快速回夺球的情况仍然很常见。目前，世界上许多顶尖教练会应用高压风格，并且概念如五秒规则，通常被归功于加多达（Guardiola），在过去几年间成为了许多队伍的基本战斗方式。媒体中经常听到“不让他呼吸”和“尽快回夺球”的表达（4][5][6），但是哪些行为最可能导致 possession 的变化？队伍的位置如何影响球回夺？哪些球员在压力下更容易塌陷？我们通过创建一个通用行动基于球回夺模型（GABR），使用Statsbomb 360$^\circ$ 数据来回答这些问题。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/10/cs.LG_2023_07_10/" data-id="closbroq100m30g88844ohqbw" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/10/eess.IV_2023_07_10/" class="article-date">
  <time datetime="2023-07-10T09:00:00.000Z" itemprop="datePublished">2023-07-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/10/eess.IV_2023_07_10/">eess.IV - 2023-07-10</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="DWA-Differential-Wavelet-Amplifier-for-Image-Super-Resolution"><a href="#DWA-Differential-Wavelet-Amplifier-for-Image-Super-Resolution" class="headerlink" title="DWA: Differential Wavelet Amplifier for Image Super-Resolution"></a>DWA: Differential Wavelet Amplifier for Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04593">http://arxiv.org/abs/2307.04593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brian B. Moser, Stanislav Frolov, Federico Raue, Sebastian Palacio, Andreas Dengel</li>
<li>for: 这篇论文是为了提出一种 Drop-in 模块，即 diferencial wavelet amplifier (DWA)，用于提高wavelet-based image Super-Resolution (SR)。</li>
<li>methods: DWA 使用了Discrete Wavelet Transformation (DWT)，一种已经受到较少关注的方法，来实现高效的图像表示和减少输入空间大小。</li>
<li>results: 根据实验结果，DWA 可以提高wavelet-based SR 模型的性能，特别是在经典的 SR 任务中，如 DWSR 和 MWCNN。此外，DWA 可以直接应用于输入图像空间，从而避免了传统 DWT 的频道化表示。<details>
<summary>Abstract</summary>
This work introduces Differential Wavelet Amplifier (DWA), a drop-in module for wavelet-based image Super-Resolution (SR). DWA invigorates an approach recently receiving less attention, namely Discrete Wavelet Transformation (DWT). DWT enables an efficient image representation for SR and reduces the spatial area of its input by a factor of 4, the overall model size, and computation cost, framing it as an attractive approach for sustainable ML. Our proposed DWA model improves wavelet-based SR models by leveraging the difference between two convolutional filters to refine relevant feature extraction in the wavelet domain, emphasizing local contrasts and suppressing common noise in the input signals. We show its effectiveness by integrating it into existing SR models, e.g., DWSR and MWCNN, and demonstrate a clear improvement in classical SR tasks. Moreover, DWA enables a direct application of DWSR and MWCNN to input image space, reducing the DWT representation channel-wise since it omits traditional DWT.
</details>
<details>
<summary>摘要</summary>
Our proposed DWA model enhances wavelet-based SR models by leveraging the difference between two convolutional filters to refine relevant feature extraction in the wavelet domain. This approach emphasizes local contrasts and suppresses common noise in the input signals. We demonstrate the effectiveness of DWA by integrating it into existing SR models, such as DWSR and MWCNN, and show a clear improvement in classical SR tasks.Moreover, DWA enables a direct application of DWSR and MWCNN to the input image space, reducing the DWT representation channel-wise since it omits traditional DWT. This simplifies the model architecture and reduces the computational cost, making it more sustainable and efficient.
</details></li>
</ul>
<hr>
<h2 id="TFR-Texture-Defect-Detection-with-Fourier-Transform-using-Normal-Reconstructed-Template-of-Simple-Autoencoder"><a href="#TFR-Texture-Defect-Detection-with-Fourier-Transform-using-Normal-Reconstructed-Template-of-Simple-Autoencoder" class="headerlink" title="TFR: Texture Defect Detection with Fourier Transform using Normal Reconstructed Template of Simple Autoencoder"></a>TFR: Texture Defect Detection with Fourier Transform using Normal Reconstructed Template of Simple Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04574">http://arxiv.org/abs/2307.04574</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jongwook Si, Sungyoung Kim</li>
<li>for:  detection of texture defects in real-world images</li>
<li>methods:  simple autoencoder + Fourier transform analysis</li>
<li>results:  effective and accurate defect detection, demonstrated through experimental results<details>
<summary>Abstract</summary>
Texture is an essential information in image representation, capturing patterns and structures. As a result, texture plays a crucial role in the manufacturing industry and is extensively studied in the fields of computer vision and pattern recognition. However, real-world textures are susceptible to defects, which can degrade image quality and cause various issues. Therefore, there is a need for accurate and effective methods to detect texture defects. In this study, a simple autoencoder and Fourier transform are employed for texture defect detection. The proposed method combines Fourier transform analysis with the reconstructed template obtained from the simple autoencoder. Fourier transform is a powerful tool for analyzing the frequency domain of images and signals. Moreover, since texture defects often exhibit characteristic changes in specific frequency ranges, analyzing the frequency domain enables effective defect detection. The proposed method demonstrates effectiveness and accuracy in detecting texture defects. Experimental results are presented to evaluate its performance and compare it with existing approaches.
</details>
<details>
<summary>摘要</summary>
Texture 是图像表示中的重要信息，捕捉pattern和结构。由于这些Texture在生产业中扮演着重要的角色，因此在计算机视觉和模式识别领域中进行了广泛的研究。然而，实际世界中的Texture受到defect的影响，这会导致图像质量下降和多种问题。因此，需要一种准确和有效的方法来检测Texture defect。在本研究中，使用了简单的自适应神经网络和快推trasform来检测Texture defect。该方法将快推trasform分析与自适应神经网络重建的模板结合使用。快推trasform是图像和信号频谱分析的powerful工具，而Texture defects通常在特定频谱范围内表现出 caracteristic 变化，因此在频谱分析中可以实现有效的检测。该方法的实验结果表明其效果和准确性在检测Texture defect方面具有优势。与现有方法进行比较的实验结果也是如此。
</details></li>
</ul>
<hr>
<h2 id="CoactSeg-Learning-from-Heterogeneous-Data-for-New-Multiple-Sclerosis-Lesion-Segmentation"><a href="#CoactSeg-Learning-from-Heterogeneous-Data-for-New-Multiple-Sclerosis-Lesion-Segmentation" class="headerlink" title="CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation"></a>CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04513">http://arxiv.org/abs/2307.04513</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ycwu1997/coactseg">https://github.com/ycwu1997/coactseg</a></li>
<li>paper_authors: Yicheng Wu, Zhonghua Wu, Hengcan Shi, Bjoern Picker, Winston Chong, Jianfei Cai</li>
<li>for: 提高多发形股病（MS）临床治疗中新出现的肿瘤分割精度，以估计疾病进程和治疗效果。</li>
<li>methods: 利用不同时点样本的异类数据（新出现肿瘤注解两个时点样本和全部肿瘤注解单个时点样本），提出了一种协作分割（CoactSeg）框架，以提高新肿瘤分割精度。</li>
<li>results: 对于新肿瘤和全部肿瘤分割任务，通过利用不同时点样本和提出的关系常量约束，可以显著提高分割精度。同时，还提供了一个MS-23v1 dataset，包括38个澳大利亚单个时点样本的全部肿瘤标签。<details>
<summary>Abstract</summary>
New lesion segmentation is essential to estimate the disease progression and therapeutic effects during multiple sclerosis (MS) clinical treatments. However, the expensive data acquisition and expert annotation restrict the feasibility of applying large-scale deep learning models. Since single-time-point samples with all-lesion labels are relatively easy to collect, exploiting them to train deep models is highly desirable to improve new lesion segmentation. Therefore, we proposed a coaction segmentation (CoactSeg) framework to exploit the heterogeneous data (i.e., new-lesion annotated two-time-point data and all-lesion annotated single-time-point data) for new MS lesion segmentation. The CoactSeg model is designed as a unified model, with the same three inputs (the baseline, follow-up, and their longitudinal brain differences) and the same three outputs (the corresponding all-lesion and new-lesion predictions), no matter which type of heterogeneous data is being used. Moreover, a simple and effective relation regularization is proposed to ensure the longitudinal relations among the three outputs to improve the model learning. Extensive experiments demonstrate that utilizing the heterogeneous data and the proposed longitudinal relation constraint can significantly improve the performance for both new-lesion and all-lesion segmentation tasks. Meanwhile, we also introduce an in-house MS-23v1 dataset, including 38 Oceania single-time-point samples with all-lesion labels. Codes and the dataset are released at https://github.com/ycwu1997/CoactSeg.
</details>
<details>
<summary>摘要</summary>
新的肿瘤分割是 Multiple Sclerosis (MS) 诊断和治疗中的关键，但是收集大规模的数据和专家标注的成本限制了应用大规模深度学习模型的可能性。由于单个时间点样本中的所有肿瘤标签比较容易获得，因此可以利用它们来训练深度模型以提高新的肿瘤分割。为此，我们提出了一个合作分割（CoactSeg）框架，可以利用不同类型的数据（新的肿瘤标签的两个时间点数据和所有肿瘤标签的单个时间点数据）来进行新的肿瘤分割。CoactSeg 模型设计为一个统一的模型，三个输入（基线、追视和它们之间的脑部差异）和三个输出（相应的所有肿瘤和新肿瘤预测），不管使用哪种不同类型的数据。此外，我们还提出了一种简单而有效的时间相关约束，以保证新肿瘤和所有肿瘤之间的长期关系，从而提高模型学习。我们的实验表明，利用不同类型的数据和提议的时间相关约束可以大幅提高新肿瘤和所有肿瘤分割任务的表现。此外，我们还释放了一个 MS-23v1 数据集，包括 38 个澳大利亚单个时间点样本，每个样本均有所有肿瘤标签。代码和数据可以在 <https://github.com/ycwu1997/CoactSeg> 上下载。
</details></li>
</ul>
<hr>
<h2 id="SAM-IQA-Can-Segment-Anything-Boost-Image-Quality-Assessment"><a href="#SAM-IQA-Can-Segment-Anything-Boost-Image-Quality-Assessment" class="headerlink" title="SAM-IQA: Can Segment Anything Boost Image Quality Assessment?"></a>SAM-IQA: Can Segment Anything Boost Image Quality Assessment?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04455">http://arxiv.org/abs/2307.04455</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hedlen/sam-iqa">https://github.com/hedlen/sam-iqa</a></li>
<li>paper_authors: Xinpeng Li, Ting Jiang, Haoqiang Fan, Shuaicheng Liu</li>
<li>for: 本研究旨在提高图像质量评估（IQA） task 的准确性，通过使用大量数据进行训练。</li>
<li>methods: 本研究使用 Segment Anything 模型的encoder部分进行高级别 semantic feature extraction，并利用 Fourier 和标准卷积来提取频域特征。</li>
<li>results: 对四个代表性的数据集进行了广泛的实验，结果表明我们的方法可以比STATE-OF-THE-ART 高效， both qualitatively 和 quantitatively。<details>
<summary>Abstract</summary>
Image Quality Assessment (IQA) is a challenging task that requires training on massive datasets to achieve accurate predictions. However, due to the lack of IQA data, deep learning-based IQA methods typically rely on pre-trained networks trained on massive datasets as feature extractors to enhance their generalization ability, such as the ResNet network trained on ImageNet. In this paper, we utilize the encoder of Segment Anything, a recently proposed segmentation model trained on a massive dataset, for high-level semantic feature extraction. Most IQA methods are limited to extracting spatial-domain features, while frequency-domain features have been shown to better represent noise and blur. Therefore, we leverage both spatial-domain and frequency-domain features by applying Fourier and standard convolutions on the extracted features, respectively. Extensive experiments are conducted to demonstrate the effectiveness of all the proposed components, and results show that our approach outperforms the state-of-the-art (SOTA) in four representative datasets, both qualitatively and quantitatively. Our experiments confirm the powerful feature extraction capabilities of Segment Anything and highlight the value of combining spatial-domain and frequency-domain features in IQA tasks. Code: https://github.com/Hedlen/SAM-IQA
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Identification-of-Hemorrhage-and-Infarct-Lesions-on-Brain-CT-Images-using-Deep-Learning"><a href="#Identification-of-Hemorrhage-and-Infarct-Lesions-on-Brain-CT-Images-using-Deep-Learning" class="headerlink" title="Identification of Hemorrhage and Infarct Lesions on Brain CT Images using Deep Learning"></a>Identification of Hemorrhage and Infarct Lesions on Brain CT Images using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04425">http://arxiv.org/abs/2307.04425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arunkumar Govindarajan, Arjun Agarwal, Subhankar Chattoraj, Dennis Robert, Satish Golla, Ujjwal Upadhyay, Swetha Tanamala, Aarthi Govindarajan</li>
<li>for: 鉴定非contrast computed tomography（NCCT）头部影像的潜在疾病</li>
<li>methods: 使用深度学习（DL）基本的计算机助临 diagnosis（CAD）模型</li>
<li>results: 对头部NCCT影像的自动识别出血肿和损伤的可能性和局限性<details>
<summary>Abstract</summary>
Head Non-contrast computed tomography (NCCT) scan remain the preferred primary imaging modality due to their widespread availability and speed. However, the current standard for manual annotations of abnormal brain tissue on head NCCT scans involves significant disadvantages like lack of cutoff standardization and degeneration identification. The recent advancement of deep learning-based computer-aided diagnostic (CAD) models in the multidisciplinary domain has created vast opportunities in neurological medical imaging. Significant literature has been published earlier in the automated identification of brain tissue on different imaging modalities. However, determining Intracranial hemorrhage (ICH) and infarct can be challenging due to image texture, volume size, and scan quality variability. This retrospective validation study evaluated a DL-based algorithm identifying ICH and infarct from head-NCCT scans. The head-NCCT scans dataset was collected consecutively from multiple diagnostic imaging centers across India. The study exhibits the potential and limitations of such DL-based software for introduction in routine workflow in extensive healthcare facilities.
</details>
<details>
<summary>摘要</summary>
head non-contrast computed tomography (NCCT) 扫描仍然是 primary imaging modality 的首选方式，因为它们在可用性和速度方面具有广泛的优势。然而，现有的手动标注病理脑组织在 head NCCT 扫描中存在一些缺点，如标准化标注的缺乏和衰变识别。Recent Advances 在多学科领域的 computer-aided diagnostic (CAD) 模型中，已经创造了巨大的机会，特别是在神经科医学影像领域。 Earlier literature 已经发表了自动识别不同 imaging modalities 中的脑组织。然而，由于图像 текстура、体积大小和扫描质量的变化，识别Intracranial hemorrhage (ICH) 和衰变可以是困难的。本回顾验证研究检查了一种基于深度学习 (DL) 的算法，可以从 head-NCCT 扫描中识别 ICH 和衰变。该 dataset 是从印度多个诊断影像中心收集的 consecutively。研究表明了这种 DL-based 软件的潜在和局限性，并探讨了在广泛的医疗设施中的应用前景。
</details></li>
</ul>
<hr>
<h2 id="Towards-Enabling-Cardiac-Digital-Twins-of-Myocardial-Infarction-Using-Deep-Computational-Models-for-Inverse-Inference"><a href="#Towards-Enabling-Cardiac-Digital-Twins-of-Myocardial-Infarction-Using-Deep-Computational-Models-for-Inverse-Inference" class="headerlink" title="Towards Enabling Cardiac Digital Twins of Myocardial Infarction Using Deep Computational Models for Inverse Inference"></a>Towards Enabling Cardiac Digital Twins of Myocardial Infarction Using Deep Computational Models for Inverse Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04421">http://arxiv.org/abs/2307.04421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Li, Julia Camps, Zhinuo, Wang, Abhirup Banerjee, Marcel Beetz, Blanca Rodriguez, Vicente Grau</li>
<li>for: 这个论文的目的是开发一个基于电cardiac digital twins (CDTs)的个性化诊断和治疗规划系统，以便更准确地诊断心肺病。</li>
<li>methods: 该论文使用了多Modal数据，包括心肺成像和电cardiacogram (ECG)，以提高推断肉粉组织特性的准确性和可靠性。具体来说，该论文采用了一种深度计算模型，通过对QRS信号和相应的损肉区域之间的复杂关系进行捕捉，来推断损肉的位置和分布。</li>
<li>results: 在计算实验中，该模型能够有效地捕捉QRS信号和相应的损肉区域之间的复杂关系，并且在未来的临床应用中具有扎实的潜在性。<details>
<summary>Abstract</summary>
Myocardial infarction (MI) demands precise and swift diagnosis. Cardiac digital twins (CDTs) have the potential to offer individualized evaluation of cardiac function in a non-invasive manner, making them a promising approach for personalized diagnosis and treatment planning of MI. The inference of accurate myocardial tissue properties is crucial in creating a reliable CDT platform, and particularly in the context of studying MI. In this work, we investigate the feasibility of inferring myocardial tissue properties from the electrocardiogram (ECG), focusing on the development of a comprehensive CDT platform specifically designed for MI. The platform integrates multi-modal data, such as cardiac MRI and ECG, to enhance the accuracy and reliability of the inferred tissue properties. We perform a sensitivity analysis based on computer simulations, systematically exploring the effects of infarct location, size, degree of transmurality, and electrical activity alteration on the simulated QRS complex of ECG, to establish the limits of the approach. We subsequently propose a deep computational model to infer infarct location and distribution from the simulated QRS. The in silico experimental results show that our model can effectively capture the complex relationships between the QRS signals and the corresponding infarct regions, with promising potential for clinical application in the future. The code will be released publicly once the manuscript is accepted for publication.
</details>
<details>
<summary>摘要</summary>
In this study, we investigate the feasibility of inferring myocardial tissue properties from the electrocardiogram (ECG) to develop a comprehensive CDT platform specifically designed for MI. Our platform integrates multi-modal data, such as cardiac MRI and ECG, to enhance the accuracy and reliability of the inferred tissue properties.We performed a sensitivity analysis based on computer simulations to explore the effects of infarct location, size, degree of transmurality, and electrical activity alteration on the simulated QRS complex of ECG. Our results show that our approach has limitations, but our deep computational model can effectively capture the complex relationships between the QRS signals and the corresponding infarct regions, with promising potential for clinical application in the future. The code will be publicly released once the manuscript is accepted for publication.
</details></li>
</ul>
<hr>
<h2 id="K-Space-Aware-Cross-Modality-Score-for-Synthesized-Neuroimage-Quality-Assessment"><a href="#K-Space-Aware-Cross-Modality-Score-for-Synthesized-Neuroimage-Quality-Assessment" class="headerlink" title="K-Space-Aware Cross-Modality Score for Synthesized Neuroimage Quality Assessment"></a>K-Space-Aware Cross-Modality Score for Synthesized Neuroimage Quality Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04296">http://arxiv.org/abs/2307.04296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinbao Wang, Guoyang Xie, Yawen Huang, Jiayi Lyu, Feng Zheng, Yefeng Zheng, Yaochu Jin</li>
<li>for: This paper aims to address the problem of assessing cross-modality medical image synthesis, which has been largely unexplored and neglected by existing measures such as PSNR and SSIM.</li>
<li>methods: The proposed method, called K-CROSS, uses a pre-trained multi-modality segmentation network to predict lesion locations, together with a tumor encoder to represent features such as texture details and brightness intensities. Both k-space features and vision features are obtained and employed in comprehensive encoders with a frequency reconstruction penalty. The structure-shared encoders are designed and constrained with a similarity loss to capture the intrinsic common structural information for both modalities.</li>
<li>results: The proposed method outperforms other metrics, especially when compared with radiologists on a large-scale cross-modality neuroimaging perceptual similarity (NIRPS) dataset with 6,000 radiologist judgments.<details>
<summary>Abstract</summary>
The problem of how to assess cross-modality medical image synthesis has been largely unexplored. The most used measures like PSNR and SSIM focus on analyzing the structural features but neglect the crucial lesion location and fundamental k-space speciality of medical images. To overcome this problem, we propose a new metric K-CROSS to spur progress on this challenging problem. Specifically, K-CROSS uses a pre-trained multi-modality segmentation network to predict the lesion location, together with a tumor encoder for representing features, such as texture details and brightness intensities. To further reflect the frequency-specific information from the magnetic resonance imaging principles, both k-space features and vision features are obtained and employed in our comprehensive encoders with a frequency reconstruction penalty. The structure-shared encoders are designed and constrained with a similarity loss to capture the intrinsic common structural information for both modalities. As a consequence, the features learned from lesion regions, k-space, and anatomical structures are all captured, which serve as our quality evaluators. We evaluate the performance by constructing a large-scale cross-modality neuroimaging perceptual similarity (NIRPS) dataset with 6,000 radiologist judgments. Extensive experiments demonstrate that the proposed method outperforms other metrics, especially in comparison with the radiologists on NIRPS.
</details>
<details>
<summary>摘要</summary>
医疗影像合成的跨Modalidad评估问题一直受到了相对少数研究。现有的度量方法，如PSNR和SSIM，主要关注医疗影像的结构特征，而忽略了重要的疾病位置和基本的k-空间特点。为了解决这个问题，我们提出了一个新的度量方法，即K-CROSS。Specifically，K-CROSS使用一个预训练的多Modalidad分割网络预测疾病位置，并使用一个恶性编码器来表示特征，如纹理细节和明亮度。为了更好地反映医疗影像的频率特征，我们在我们的全面编码器中使用频率重建罚款。同时，我们设计了结构共享编码器，并使用一个相似损失来捕捉两Modalidad之间的共同结构信息。因此，我们可以从疾病区域、k-空间和解剖结构中 capture所有的特征，这些特征作为我们的质量评估器。我们通过构建一个大规模的跨Modalidad神经成像相似度（NIRPS）数据集，并对6,000名医生判断进行评估，来评估性能。广泛的实验表明，我们提出的方法在比较其他度量方法时，尤其是与医生在NIRPS上的评估中，表现出了优异。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/10/eess.IV_2023_07_10/" data-id="closbroww013l0g887nolfjzn" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/09/cs.SD_2023_07_09/" class="article-date">
  <time datetime="2023-07-09T15:00:00.000Z" itemprop="datePublished">2023-07-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/09/cs.SD_2023_07_09/">cs.SD - 2023-07-09</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Can-Generative-Large-Language-Models-Perform-ASR-Error-Correction"><a href="#Can-Generative-Large-Language-Models-Perform-ASR-Error-Correction" class="headerlink" title="Can Generative Large Language Models Perform ASR Error Correction?"></a>Can Generative Large Language Models Perform ASR Error Correction?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04172">http://arxiv.org/abs/2307.04172</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rao Ma, Mengjie Qian, Potsawee Manakul, Mark Gales, Kate Knill</li>
<li>for: 提高 ASR 系统的表现，使其更加准确和有效。</li>
<li>methods: 使用 ChatGPT 大语言模型进行零次或一次学习，对 ASR N-best 列表进行错误修正。</li>
<li>results: 对 Conformer-Transducer 模型和预训练的 Whisper 模型进行错误修正，可以大幅提高 ASR 系统的表现。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
ASR error correction continues to serve as an important part of post-processing for speech recognition systems. Traditionally, these models are trained with supervised training using the decoding results of the underlying ASR system and the reference text. This approach is computationally intensive and the model needs to be re-trained when switching the underlying ASR model. Recent years have seen the development of large language models and their ability to perform natural language processing tasks in a zero-shot manner. In this paper, we take ChatGPT as an example to examine its ability to perform ASR error correction in the zero-shot or 1-shot settings. We use the ASR N-best list as model input and propose unconstrained error correction and N-best constrained error correction methods. Results on a Conformer-Transducer model and the pre-trained Whisper model show that we can largely improve the ASR system performance with error correction using the powerful ChatGPT model.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本为简化中文。<</SYS>>ASR错误修正仍然serve as重要的后处理步骤 для语音识别系统。传统上，这些模型通过指导学习使用下面ASR系统的解码结果和参考文本进行训练。这种方法是计算机程序昂贵，模型需要在Switching beneath ASR模型时重新训练。 recent years have seen the development of large language models and their ability to perform natural language processing tasks in a zero-shot manner. In this paper, we take ChatGPT as an example to examine its ability to perform ASR error correction in the zero-shot or 1-shot settings. We use the ASR N-best list as model input and propose unconstrained error correction and N-best constrained error correction methods. Results on a Conformer-Transducer model and the pre-trained Whisper model show that we can largely improve the ASR system performance with error correction using the powerful ChatGPT model.Note: The translation is done using Google Translate, which may not be perfect, but it should give you a good idea of the content in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Emotion-Guided-Music-Accompaniment-Generation-Based-on-Variational-Autoencoder"><a href="#Emotion-Guided-Music-Accompaniment-Generation-Based-on-Variational-Autoencoder" class="headerlink" title="Emotion-Guided Music Accompaniment Generation Based on Variational Autoencoder"></a>Emotion-Guided Music Accompaniment Generation Based on Variational Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04015">http://arxiv.org/abs/2307.04015</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/duoluoluos/emotion-guided-music-accompaniment-generation">https://github.com/duoluoluos/emotion-guided-music-accompaniment-generation</a></li>
<li>paper_authors: Qi Wang, Shubing Zhang, Li Zhou</li>
<li>for: 本研究旨在解决人工智能在音乐创作过程中困难地 integrate 人类情感创作美妙的伴奏乐曲。</li>
<li>methods: 我们提议使用一种易于表示情感流程的曲线模型，即Valence&#x2F;Arousal Curve，以实现模型内部情感信息的兼容，并使用变量自动编码器作为模型结构以提高情感因素的解释性。 而Relative Self-Attention 技术也用于保持音乐句子水平结构和生成更加丰富的伴奏乐曲。</li>
<li>results: 我们的方法可以增强音乐创作过程中 AI 的情感创作能力，并生成更加美妙的伴奏乐曲。<details>
<summary>Abstract</summary>
Music accompaniment generation is a crucial aspect in the composition process. Deep neural networks have made significant strides in this field, but it remains a challenge for AI to effectively incorporate human emotions to create beautiful accompaniments. Existing models struggle to effectively characterize human emotions within neural network models while composing music. To address this issue, we propose the use of an easy-to-represent emotion flow model, the Valence/Arousal Curve, which allows for the compatibility of emotional information within the model through data transformation and enhances interpretability of emotional factors by utilizing a Variational Autoencoder as the model structure. Further, we used relative self-attention to maintain the structure of the music at music phrase level and to generate a richer accompaniment when combined with the rules of music theory.
</details>
<details>
<summary>摘要</summary>
音乐伴奏生成是作曲过程中的一个重要方面。深度神经网络在这个领域已经做出了很大的进步，但是AI还未能够有效地涵盖人类情感以创造美妙的伴奏。现有的模型很难准确地捕捉人类情感信息在神经网络模型中，而且通常会导致模型难以理解和描述情感因素。为解决这个问题，我们提议使用一种容易表达情感流程的曲线模型，即情感值/刺激曲线，该模型通过数据转换来兼容情感信息，并通过使用变量自动编码器结构来提高情感因素的解释性。此外，我们使用相对自注意力来保持音乐段级结构和生成更加丰富的伴奏，并与音乐理论规则相结合。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/09/cs.SD_2023_07_09/" data-id="closbrosj00t50g88hxt98lfy" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_07_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/09/eess.AS_2023_07_09/" class="article-date">
  <time datetime="2023-07-09T14:00:00.000Z" itemprop="datePublished">2023-07-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/09/eess.AS_2023_07_09/">eess.AS - 2023-07-09</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="IANS-Intelligibility-aware-Null-steering-Beamforming-for-Dual-Microphone-Arrays"><a href="#IANS-Intelligibility-aware-Null-steering-Beamforming-for-Dual-Microphone-Arrays" class="headerlink" title="IANS: Intelligibility-aware Null-steering Beamforming for Dual-Microphone Arrays"></a>IANS: Intelligibility-aware Null-steering Beamforming for Dual-Microphone Arrays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04179">http://arxiv.org/abs/2307.04179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wen-Yuan Ting, Syu-Siang Wang, Yu Tsao, Borching Su</li>
<li>for: 提高杂音干扰 speech 的 intelligibility</li>
<li>methods: 使用 STOI-Net 智能预测模型，并结合 null-steering  beamformer (NSBF) 生成一系列 beamformed 输出，以提高 speech 的 intelligibility</li>
<li>results: 实验结果表明，IANS 可以使用小型双麦克麦array 生成有 inteligibility 提高的 signal，与 known DOAs 的 null-steering beamformers 的结果相似<details>
<summary>Abstract</summary>
Beamforming techniques are popular in speech-related applications due to their effective spatial filtering capabilities. Nonetheless, conventional beamforming techniques generally depend heavily on either the target's direction-of-arrival (DOA), relative transfer function (RTF) or covariance matrix. This paper presents a new approach, the intelligibility-aware null-steering (IANS) beamforming framework, which uses the STOI-Net intelligibility prediction model to improve speech intelligibility without prior knowledge of the speech signal parameters mentioned earlier. The IANS framework combines a null-steering beamformer (NSBF) to generate a set of beamformed outputs, and STOI-Net, to determine the optimal result. Experimental results indicate that IANS can produce intelligibility-enhanced signals using a small dual-microphone array. The results are comparable to those obtained by null-steering beamformers with given knowledge of DOAs.
</details>
<details>
<summary>摘要</summary>
<<SYS>> simultrare il testo in Cinese semplificato.<</SYS>>Beamforming 技术在语音相关应用中很受欢迎，因为它们可以提供有效的空间滤波功能。然而，传统的 beamforming 技术通常具有依赖于目标的方向 arrival (DOA)、相对转移函数 (RTF) 或 covariance matrix 的缺点。这篇论文提出了一种新的方法——智能可识别 beamforming 框架 (IANS)，它使用 STOI-Net 智能可识别模型来提高语音可识别性，无需先知道语音信号参数。IANS 框架将 null-steering beamformer (NSBF) 与 STOI-Net 结合，生成一组扩展出的输出，并使用 STOI-Net 确定最佳结果。实验结果表明，IANS 可以使用小型双 микрофон阵列生成具有可识别性的信号，与 null-steering beamformers 的结果相似。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/09/eess.AS_2023_07_09/" data-id="closbrovi00zv0g88fobbeild" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_07_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/09/cs.CV_2023_07_09/" class="article-date">
  <time datetime="2023-07-09T13:00:00.000Z" itemprop="datePublished">2023-07-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/09/cs.CV_2023_07_09/">cs.CV - 2023-07-09</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Histopathology-Whole-Slide-Image-Analysis-with-Heterogeneous-Graph-Representation-Learning"><a href="#Histopathology-Whole-Slide-Image-Analysis-with-Heterogeneous-Graph-Representation-Learning" class="headerlink" title="Histopathology Whole Slide Image Analysis with Heterogeneous Graph Representation Learning"></a>Histopathology Whole Slide Image Analysis with Heterogeneous Graph Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04189">http://arxiv.org/abs/2307.04189</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hku-medai/wsi-hgnn">https://github.com/hku-medai/wsi-hgnn</a></li>
<li>paper_authors: Tsai Hor Chan, Fernando Julio Cendra, Lan Ma, Guosheng Yin, Lequan Yu</li>
<li>for: 本研究旨在提出一种基于非同质图的抽象方法，以利用染色体图像中不同类型的核体之间的复杂结构关系来进行抽象分析。</li>
<li>methods: 本研究提出了一种基于非同质图的抽象方法，包括形成WTSI为非同质图，使用HEAT模型进行消息协同汇聚，并提出一种假标签基于含义相似性的 pooling 机制来获取图级特征。</li>
<li>results: 对三个TCGA公共数据集进行了广泛的实验，并证明了该方法可以在不同任务上具有显著的优势，比如识别率、抑阻率等。<details>
<summary>Abstract</summary>
Graph-based methods have been extensively applied to whole-slide histopathology image (WSI) analysis due to the advantage of modeling the spatial relationships among different entities. However, most of the existing methods focus on modeling WSIs with homogeneous graphs (e.g., with homogeneous node type). Despite their successes, these works are incapable of mining the complex structural relations between biological entities (e.g., the diverse interaction among different cell types) in the WSI. We propose a novel heterogeneous graph-based framework to leverage the inter-relationships among different types of nuclei for WSI analysis. Specifically, we formulate the WSI as a heterogeneous graph with "nucleus-type" attribute to each node and a semantic similarity attribute to each edge. We then present a new heterogeneous-graph edge attribute transformer (HEAT) to take advantage of the edge and node heterogeneity during massage aggregating. Further, we design a new pseudo-label-based semantic-consistent pooling mechanism to obtain graph-level features, which can mitigate the over-parameterization issue of conventional cluster-based pooling. Additionally, observing the limitations of existing association-based localization methods, we propose a causal-driven approach attributing the contribution of each node to improve the interpretability of our framework. Extensive experiments on three public TCGA benchmark datasets demonstrate that our framework outperforms the state-of-the-art methods with considerable margins on various tasks. Our codes are available at https://github.com/HKU-MedAI/WSI-HGNN.
</details>
<details>
<summary>摘要</summary>
渐变图方法已广泛应用于整个染色体图像（WSI）分析，这是因为渐变图可以模型染色体图像中的空间关系。然而，大多数现有方法都是使用同质graph（例如，具有同质节点类型）来模型WSI。尽管它们在一定程度上取得了成功，但它们无法捕捉染色体图像中不同生物实体之间复杂的结构关系（例如，不同细胞类型之间的多样化互动）。我们提议一种新的多态渐变图基于框架，以利用染色体图像中不同类型细胞的关系。具体来说，我们将WSI转化为一个多态渐变图，其中每个节点具有“细胞类型”特性，以及每个边具有semantic similarity特性。然后，我们提出一种新的多态渐变图边属性变换器（HEAT），以利用边和节点多样性进行消息汇聚。此外，我们设计了一种新的 pseudo-label-based semantic-consistent pooling机制，以获取图 уров减少过拟合问题。此外，我们发现现有的协同localization方法存在局限性，我们提出一种 causal-driven 方法，以解释我们的框架的解释性。我们的实验结果表明，我们的框架在三个公共 TCGA 测试数据集上比现状态方法具有较大的优势，在不同任务上具有显著的提升。我们的代码可以在 <https://github.com/HKU-MedAI/WSI-HGNN> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Predictive-Coding-For-Animation-Based-Video-Compression"><a href="#Predictive-Coding-For-Animation-Based-Video-Compression" class="headerlink" title="Predictive Coding For Animation-Based Video Compression"></a>Predictive Coding For Animation-Based Video Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04187">http://arxiv.org/abs/2307.04187</a></li>
<li>repo_url: None</li>
<li>paper_authors: Goluck Konuko, Stéphane Lathuilière, Giuseppe Valenzise</li>
<li>for: 提高视频压缩效率，适用于视频会议类应用。</li>
<li>methods: 基于图像动画的新方法，使用预测编码，对待标题帧进行很好的重建。</li>
<li>results: 对比HEVC和VVC，实现了70%以上的比特率减少和30%以上的比特率减少，在语音视频数据集上。<details>
<summary>Abstract</summary>
We address the problem of efficiently compressing video for conferencing-type applications. We build on recent approaches based on image animation, which can achieve good reconstruction quality at very low bitrate by representing face motions with a compact set of sparse keypoints. However, these methods encode video in a frame-by-frame fashion, i.e. each frame is reconstructed from a reference frame, which limits the reconstruction quality when the bandwidth is larger. Instead, we propose a predictive coding scheme which uses image animation as a predictor, and codes the residual with respect to the actual target frame. The residuals can be in turn coded in a predictive manner, thus removing efficiently temporal dependencies. Our experiments indicate a significant bitrate gain, in excess of 70% compared to the HEVC video standard and over 30% compared to VVC, on a datasetof talking-head videos
</details>
<details>
<summary>摘要</summary>
我们处理对 conferencing 型应用程序进行高效压缩影像的问题。我们基于最近的图像动画方法，可以在非常低比特率下 achieve good 重建质量。但这些方法在每帧基于参考帧进行重建，因此在带宽较大时会限制重建质量。我们提议一种预测编码方案，使用图像动画作为预测器，并将差分码到目标帧。这些差分可以在预测性下进行高效地删除时间相依性。我们的实验结果显示，与 HEVC 影像标准和 VVC 相比，我们的方法可以获得高达70% 以上的比特率优化，在 talking-head 影像集上。
</details></li>
</ul>
<hr>
<h2 id="Reducing-False-Alarms-in-Video-Surveillance-by-Deep-Feature-Statistical-Modeling"><a href="#Reducing-False-Alarms-in-Video-Surveillance-by-Deep-Feature-Statistical-Modeling" class="headerlink" title="Reducing False Alarms in Video Surveillance by Deep Feature Statistical Modeling"></a>Reducing False Alarms in Video Surveillance by Deep Feature Statistical Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04159">http://arxiv.org/abs/2307.04159</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xavier Bou, Aitor Artola, Thibaud Ehret, Gabriele Facciolo, Jean-Michel Morel, Rafael Grompone von Gioi</li>
<li>for: 降低视频监测中False Alarm的数量</li>
<li>methods: 基于深度特征高维统计模型的无监督可靠性验证过程</li>
<li>results: 对六种方法和多个数据集中的多个序列进行Pixel和Object级别的评估，实验结果表明提议的a-contrario验证可以大幅减少False Alarm数量。<details>
<summary>Abstract</summary>
Detecting relevant changes is a fundamental problem of video surveillance. Because of the high variability of data and the difficulty of properly annotating changes, unsupervised methods dominate the field. Arguably one of the most critical issues to make them practical is to reduce their false alarm rate. In this work, we develop a method-agnostic weakly supervised a-contrario validation process, based on high dimensional statistical modeling of deep features, to reduce the number of false alarms of any change detection algorithm. We also raise the insufficiency of the conventionally used pixel-wise evaluation, as it fails to precisely capture the performance needs of most real applications. For this reason, we complement pixel-wise metrics with object-wise metrics and evaluate the impact of our approach at both pixel and object levels, on six methods and several sequences from different datasets. Experimental results reveal that the proposed a-contrario validation is able to largely reduce the number of false alarms at both pixel and object levels.
</details>
<details>
<summary>摘要</summary>
检测有关变化是视频监测领域的基本问题。由于数据的高度变化和难以正确地标注变化，因此无监督方法在该领域占据主导地位。然而，减少假警告率是实现这些方法的实用性的核心问题。在这项工作中，我们开发了一种方法不偏的弱监睹验证过程，基于深度特征的高维统计模型，以减少任何变化检测算法的假警告率。此外，我们指出了通用的像素精度评价方法的不足，因为它无法准确地捕捉实际应用中的性能需求。因此，我们补充了像素精度 metric 的对象精度 metric，并对六种方法和多个数据集中的多个序列进行了评估。实验结果表明，我们的弱监睹验证方法能够大幅减少像素和对象级别的假警告率。
</details></li>
</ul>
<hr>
<h2 id="DIFF-NST-Diffusion-Interleaving-For-deFormable-Neural-Style-Transfer"><a href="#DIFF-NST-Diffusion-Interleaving-For-deFormable-Neural-Style-Transfer" class="headerlink" title="DIFF-NST: Diffusion Interleaving For deFormable Neural Style Transfer"></a>DIFF-NST: Diffusion Interleaving For deFormable Neural Style Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04157">http://arxiv.org/abs/2307.04157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dan Ruta, Gemma Canet Tarrés, Andrew Gilbert, Eli Shechtman, Nicholas Kolkin, John Collomosse</li>
<li>for: 本研究探讨如何使用神经网络技术来修改内容图像的艺术外观，以符合参照样式图像的风格。</li>
<li>methods: 本研究使用了新型的扩散模型，如稳定扩散，可以访问更强大的图像生成技术，以实现新的可能性。</li>
<li>results: 本研究提出了一种新的方法，可以在扩散模型的基础之上实现可变式风格传输，这是前一代模型无法实现的。我们还证明了在推理时可以通过模型的假设来获得新的艺术控制，并在这个新方向下进行了探索。<details>
<summary>Abstract</summary>
Neural Style Transfer (NST) is the field of study applying neural techniques to modify the artistic appearance of a content image to match the style of a reference style image. Traditionally, NST methods have focused on texture-based image edits, affecting mostly low level information and keeping most image structures the same. However, style-based deformation of the content is desirable for some styles, especially in cases where the style is abstract or the primary concept of the style is in its deformed rendition of some content. With the recent introduction of diffusion models, such as Stable Diffusion, we can access far more powerful image generation techniques, enabling new possibilities. In our work, we propose using this new class of models to perform style transfer while enabling deformable style transfer, an elusive capability in previous models. We show how leveraging the priors of these models can expose new artistic controls at inference time, and we document our findings in exploring this new direction for the field of style transfer.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Survey-on-Figure-Classification-Techniques-in-Scientific-Documents"><a href="#A-Survey-on-Figure-Classification-Techniques-in-Scientific-Documents" class="headerlink" title="A Survey on Figure Classification Techniques in Scientific Documents"></a>A Survey on Figure Classification Techniques in Scientific Documents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05694">http://arxiv.org/abs/2307.05694</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anurag Dhote, Mohammed Javed, David S Doermann</li>
<li>for: 本文主要用于系统地把图像分类为五类：表格、照片、图表、地图和图表，并对现有的方法和数据集进行报告和评论。</li>
<li>methods: 本文使用了不同的人工智能和机器学习技术来从图像中提取数据，包括图像分类、图像描述、图像识别等方法。</li>
<li>results: 本文对现有的方法和数据集进行了批判性评估，并发现了一些研究缺失，提出了可能的未来研究方向。<details>
<summary>Abstract</summary>
Figures visually represent an essential piece of information and provide an effective means to communicate scientific facts. Recently there have been many efforts toward extracting data directly from figures, specifically from tables, diagrams, and plots, using different Artificial Intelligence and Machine Learning techniques. This is because removing information from figures could lead to deeper insights into the concepts highlighted in the scientific documents. In this survey paper, we systematically categorize figures into five classes - tables, photos, diagrams, maps, and plots, and subsequently present a critical review of the existing methodologies and data sets that address the problem of figure classification. Finally, we identify the current research gaps and provide possible directions for further research on figure classification.
</details>
<details>
<summary>摘要</summary>
figuress 可以视觉表达重要信息，提供有效的科学信息传递方式。近些年来，有许多努力在抽取图表数据方面，特别是从表格、图表、图例和地图等方面，使用不同的人工智能和机器学习技术。这是因为从图表中提取信息可能会导致更深入的理解科学文献中所描述的概念。在本评论 paper中，我们系统地分类图表为五类 - 表格、照片、图例、地图和图表，并随后提供了现有方法和数据集的批判性评审。最后，我们确定了当前的研究漏洞和提供了进一步研究图表分类的可能方向。
</details></li>
</ul>
<hr>
<h2 id="ECL-Class-Enhancement-Contrastive-Learning-for-Long-tailed-Skin-Lesion-Classification"><a href="#ECL-Class-Enhancement-Contrastive-Learning-for-Long-tailed-Skin-Lesion-Classification" class="headerlink" title="ECL: Class-Enhancement Contrastive Learning for Long-tailed Skin Lesion Classification"></a>ECL: Class-Enhancement Contrastive Learning for Long-tailed Skin Lesion Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04136">http://arxiv.org/abs/2307.04136</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zylbuaa/ecl">https://github.com/zylbuaa/ecl</a></li>
<li>paper_authors: Yilan Zhang, Jianqi Chen, Ke Wang, Fengying Xie<br>for: 这个研究旨在解决肤色图像数据集中的数据分布偏好问题，使得计算机支持的皮肤疾病诊断更加困难。methods: 我们提出了一种叫做类增强对照学习（ECL）的方法，它可以增强少数类别中的信息，并对不同类别进行平等对待。为了实现信息增强，我们设计了一种混合代理模型，并提出了一种循环更新策略来优化参数。我们还设计了一种类别依赖的混合代理损失函数，以利用样本和代理之间的关系，并对不同类别进行平等对待。results: 我们的方法在处理肤色图像数据集中的分类任务中达到了最高的性能。我们还通过评估学习曲线来证明我们的方法可以适应不同的学习环境，并且在不同的数据分布下都能够保持高度的稳定性。<details>
<summary>Abstract</summary>
Skin image datasets often suffer from imbalanced data distribution, exacerbating the difficulty of computer-aided skin disease diagnosis. Some recent works exploit supervised contrastive learning (SCL) for this long-tailed challenge. Despite achieving significant performance, these SCL-based methods focus more on head classes, yet ignoring the utilization of information in tail classes. In this paper, we propose class-Enhancement Contrastive Learning (ECL), which enriches the information of minority classes and treats different classes equally. For information enhancement, we design a hybrid-proxy model to generate class-dependent proxies and propose a cycle update strategy for parameters optimization. A balanced-hybrid-proxy loss is designed to exploit relations between samples and proxies with different classes treated equally. Taking both "imbalanced data" and "imbalanced diagnosis difficulty" into account, we further present a balanced-weighted cross-entropy loss following curriculum learning schedule. Experimental results on the classification of imbalanced skin lesion data have demonstrated the superiority and effectiveness of our method.
</details>
<details>
<summary>摘要</summary>
皮肤图像数据集经常受到数据分布不均衡的影响，使计算机辅助皮肤病诊断变得更加困难。一些最近的研究利用Supervised Contrastive Learning（SCL）来解决这个长尾挑战。尽管这些SCL基于方法达到了显著的性能，但是它们更关注头等级类，忽略了使用尾等级类信息。在这篇论文中，我们提出了类增强对比学习（ECL）方法，它可以增强少数量级类信息并对不同类型进行平等对待。为了增强信息，我们设计了一种混合代理模型，生成类具有不同代理模型，并提出了一种循环更新策略来优化参数。为了利用样本和代理之间的关系，我们设计了一种权重平衡损失函数。考虑到“不均衡数据”和“不均衡诊断难度”两个因素，我们还提出了一种平衡权重十进制架构。实验结果表明，我们的方法在皮肤病患数据分类任务中具有superiority和有效性。
</details></li>
</ul>
<hr>
<h2 id="Ultrasonic-Image’s-Annotation-Removal-A-Self-supervised-Noise2Noise-Approach"><a href="#Ultrasonic-Image’s-Annotation-Removal-A-Self-supervised-Noise2Noise-Approach" class="headerlink" title="Ultrasonic Image’s Annotation Removal: A Self-supervised Noise2Noise Approach"></a>Ultrasonic Image’s Annotation Removal: A Self-supervised Noise2Noise Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04133">http://arxiv.org/abs/2307.04133</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/grandarth/ultrasonicimage-n2n-approach">https://github.com/grandarth/ultrasonicimage-n2n-approach</a></li>
<li>paper_authors: Yuanheng Zhang, Nan Jiang, Zhaoheng Xie, Junying Cao, Yueyang Teng</li>
<li>for: 这篇论文的目的是创建一个自动标注医疗影像的方法，以减少医疗影像标注的人工审核时间。</li>
<li>methods: 这篇论文使用了一个自我指定预备任务，将标注视为噪音，并使用了一个基于噪音的模型，将影像重新构成为清洁的形式。</li>
<li>results: 这篇论文的结果显示，使用了自我指定预备任务和噪音的模型，可以对医疗影像进行高精度的标注，并且比使用噪音-清洁数据对的模型更好。<details>
<summary>Abstract</summary>
Accurately annotated ultrasonic images are vital components of a high-quality medical report. Hospitals often have strict guidelines on the types of annotations that should appear on imaging results. However, manually inspecting these images can be a cumbersome task. While a neural network could potentially automate the process, training such a model typically requires a dataset of paired input and target images, which in turn involves significant human labour. This study introduces an automated approach for detecting annotations in images. This is achieved by treating the annotations as noise, creating a self-supervised pretext task and using a model trained under the Noise2Noise scheme to restore the image to a clean state. We tested a variety of model structures on the denoising task against different types of annotation, including body marker annotation, radial line annotation, etc. Our results demonstrate that most models trained under the Noise2Noise scheme outperformed their counterparts trained with noisy-clean data pairs. The costumed U-Net yielded the most optimal outcome on the body marker annotation dataset, with high scores on segmentation precision and reconstruction similarity. We released our code at https://github.com/GrandArth/UltrasonicImage-N2N-Approach.
</details>
<details>
<summary>摘要</summary>
高品质医疗报告中的精准阴影图像是不可或缺的元素。医院通常有严格的指引，要求医疗影像报告中的标注项目。然而，手动检查这些图像可能是一个费时的任务。 neural network 可能可以自动进行这个任务，但是训练这种模型通常需要一个对应的数据集，这需要大量的人工劳动。本研究提出了一个自动标注图像的方法。这是通过将标注视为噪音，创建一个自我监督任务，并使用以噪音为学习目标的模型来恢复图像的清洁状态。我们对恢复任务进行了多种模型结构的测试，包括体部标注、径向线标注等。我们的结果显示，大多数以噪音为学习目标的模型比以噪音-清洁数据对的模型来得到更好的结果。自适应U-Net传播网络在体部标注数据集上获得最佳效果，高于分类精度和重建相似性。我们将我们的代码公开在 GitHub 上，请参考 https://github.com/GrandArth/UltrasonicImage-N2N-Approach.
</details></li>
</ul>
<hr>
<h2 id="Cross-modal-Orthogonal-High-rank-Augmentation-for-RGB-Event-Transformer-trackers"><a href="#Cross-modal-Orthogonal-High-rank-Augmentation-for-RGB-Event-Transformer-trackers" class="headerlink" title="Cross-modal Orthogonal High-rank Augmentation for RGB-Event Transformer-trackers"></a>Cross-modal Orthogonal High-rank Augmentation for RGB-Event Transformer-trackers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04129">http://arxiv.org/abs/2307.04129</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ZHU-Zhiyu/High-Rank_RGB-Event_Tracker">https://github.com/ZHU-Zhiyu/High-Rank_RGB-Event_Tracker</a></li>
<li>paper_authors: Zhiyu Zhu, Junhui Hou, Dapeng Oliver Wu</li>
<li>for: 这个论文解决了RGB视频和事件数据之间的跨模态对象跟踪问题。</li>
<li>methods: 我们不是构建复杂的跨模态融合网络，而是探索pre-trained视觉转换器（ViT）的潜在能力。我们特别是通过插件和游戏训练增强技术来鼓励ViTbridging两种模态之间的巨大分布差，从而启用全面的跨模态信息互动，提高其能力。我们提议一种面积模型策略，随机遮盖某些批处理的特定模式，以促进不同模式之间的互动。</li>
<li>results: 我们的插件和游戏训练增强技术可以明显提高现有的一气道和两气道跟踪器的跟踪精度和成功率。我们的新视角和发现可能会为跨模态数据模型领域带来新的思路和发现。代码将公开发布。<details>
<summary>Abstract</summary>
This paper addresses the problem of cross-modal object tracking from RGB videos and event data. Rather than constructing a complex cross-modal fusion network, we explore the great potential of a pre-trained vision Transformer (ViT). Particularly, we delicately investigate plug-and-play training augmentations that encourage the ViT to bridge the vast distribution gap between the two modalities, enabling comprehensive cross-modal information interaction and thus enhancing its ability. Specifically, we propose a mask modeling strategy that randomly masks a specific modality of some tokens to enforce the interaction between tokens from different modalities interacting proactively. To mitigate network oscillations resulting from the masking strategy and further amplify its positive effect, we then theoretically propose an orthogonal high-rank loss to regularize the attention matrix. Extensive experiments demonstrate that our plug-and-play training augmentation techniques can significantly boost state-of-the-art one-stream and twostream trackers to a large extent in terms of both tracking precision and success rate. Our new perspective and findings will potentially bring insights to the field of leveraging powerful pre-trained ViTs to model cross-modal data. The code will be publicly available.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Marine-Debris-Detection-in-Satellite-Surveillance-using-Attention-Mechanisms"><a href="#Marine-Debris-Detection-in-Satellite-Surveillance-using-Attention-Mechanisms" class="headerlink" title="Marine Debris Detection in Satellite Surveillance using Attention Mechanisms"></a>Marine Debris Detection in Satellite Surveillance using Attention Mechanisms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04128">http://arxiv.org/abs/2307.04128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ao Shen, Yijie Zhu, Richard Jiang</li>
<li>for: 本研究旨在提高marine debris的位置Localization效率和应用范围，通过结合YOLOv7的实例分割和不同的注意机制。</li>
<li>methods: 本研究使用了一个标注 dataset，包括含有海洋垃圾的卫星图像，并评估了三种注意模型，包括轻量级坐标注意、CBAM（结合空间和通道注意）以及瓶颈transformer（基于自注意）。</li>
<li>results:  Box detection 评估显示，CBAM 得到了最好的结果（F1 分数为 77%），比 coordinate attention （F1 分数为 71%）和 YOLOv7&#x2F;瓶颈transformer （两者 F1 分数为 around 66%）更好。Mask 评估显示 CBAM 再次领先，F1 分数为 73%，而 coordinate attention 和 YOLOv7 的表现相似（around F1 分数为 68%&#x2F;69%），瓶颈transformer 落后，F1 分数为 56%。这些结果表明，CBAM 适合 Marine debris 的检测。但是，瓶颈transformer 可能具有更好的实际性能，因为它检测到了一些人工标注 missed 的区域，并且具有较好的面积精度。<details>
<summary>Abstract</summary>
Marine debris is an important issue for environmental protection, but current methods for locating marine debris are yet limited. In order to achieve higher efficiency and wider applicability in the localization of Marine debris, this study tries to combine the instance segmentation of YOLOv7 with different attention mechanisms and explores the best model. By utilizing a labelled dataset consisting of satellite images containing ocean debris, we examined three attentional models including lightweight coordinate attention, CBAM (combining spatial and channel focus), and bottleneck transformer (based on self-attention). Box detection assessment revealed that CBAM achieved the best outcome (F1 score of 77%) compared to coordinate attention (F1 score of 71%) and YOLOv7/bottleneck transformer (both F1 scores around 66%). Mask evaluation showed CBAM again leading with an F1 score of 73%, whereas coordinate attention and YOLOv7 had comparable performances (around F1 score of 68%/69%) and bottleneck transformer lagged behind at F1 score of 56%. These findings suggest that CBAM offers optimal suitability for detecting marine debris. However, it should be noted that the bottleneck transformer detected some areas missed by manual annotation and displayed better mask precision for larger debris pieces, signifying potentially superior practical performance.
</details>
<details>
<summary>摘要</summary>
海洋垃圾是环境保护的重要问题，但当前用于找到海洋垃圾的方法仍有限制。为了提高效率和应用范围，这项研究尝试将YOLOv7的实例 segmentation技术与不同的注意机制结合，并探索最佳模型。通过使用含有海洋垃圾的卫星图像标注 datasets，我们评估了三种注意模型，包括轻量级坐标注意、CBAM（结合空间和通道注意）和瓶颈变换器（基于自注意）。 Box 检测评估表明，CBAM得到了最佳结果（F1分数为77%），比 coordinate attention（F1分数为71%）和 YOLOv7/瓶颈变换器（两者F1分数都在66%左右）更高。面 mask 评估表明，CBAM再次领先，F1分数为73%；coordinate attention 和 YOLOv7 的表现相似（约F1分数为68%/69%），而瓶颈变换器则落后，F1分数为56%。这些结果表明，CBAM 是最适合检测海洋垃圾的选择。然而，瓶颈变换器可能在实际应用中表现更好，因为它检测了一些人工标注 missed 的区域，并且对大型垃圾件的面 mask 准确率较高。
</details></li>
</ul>
<hr>
<h2 id="HA-ViD-A-Human-Assembly-Video-Dataset-for-Comprehensive-Assembly-Knowledge-Understanding"><a href="#HA-ViD-A-Human-Assembly-Video-Dataset-for-Comprehensive-Assembly-Knowledge-Understanding" class="headerlink" title="HA-ViD: A Human Assembly Video Dataset for Comprehensive Assembly Knowledge Understanding"></a>HA-ViD: A Human Assembly Video Dataset for Comprehensive Assembly Knowledge Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05721">http://arxiv.org/abs/2307.05721</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Zheng, Regina Lee, Yuqian Lu</li>
<li>for: 这 paper 是为了掌握现代制造业中的人工 Assembly 知识而做的，以便实现技术突破。</li>
<li>methods: 这 paper 使用了人工视频数据集 HA-ViD，该数据集包含了真实世界中的 Assembly 场景，以及人类在 Assembly 过程中的自然行为和学习过程。</li>
<li>results: 这 paper 通过分析不同的 Assembly 场景、人类行为和学习过程，对现代制造业中的 Assembly 知识进行了全面的掌握和分析。<details>
<summary>Abstract</summary>
Understanding comprehensive assembly knowledge from videos is critical for futuristic ultra-intelligent industry. To enable technological breakthrough, we present HA-ViD - the first human assembly video dataset that features representative industrial assembly scenarios, natural procedural knowledge acquisition process, and consistent human-robot shared annotations. Specifically, HA-ViD captures diverse collaboration patterns of real-world assembly, natural human behaviors and learning progression during assembly, and granulate action annotations to subject, action verb, manipulated object, target object, and tool. We provide 3222 multi-view, multi-modality videos (each video contains one assembly task), 1.5M frames, 96K temporal labels and 2M spatial labels. We benchmark four foundational video understanding tasks: action recognition, action segmentation, object detection and multi-object tracking. Importantly, we analyze their performance for comprehending knowledge in assembly progress, process efficiency, task collaboration, skill parameters and human intention. Details of HA-ViD is available at: https://iai-hrc.github.io/ha-vid.
</details>
<details>
<summary>摘要</summary>
理解全面 montage 知识从视频是未来超智能工业的关键。为实现技术突破，我们提出了HA-ViD，首个人工 montage 视频集，它包含了真实工业 assembly 场景，自然的学习过程，以及人机共享标注。HA-ViD 捕捉了真实 assembly 中的多样化合作模式，人类行为和学习过程，并为每个动作分别提供了主体、动作词、操作对象、目标对象和工具的精细标注。我们提供了3222个多视角、多种 modalities 视频（每个视频包含一个 assembly 任务），150万帧，96000个时间标签和200万个空间标签。我们对四个基本视频理解任务进行了测试：行动识别、动作分割、对象检测和多对象跟踪。我们还分析了它们在理解 assembly 进度、生产效率、任务合作、技能参数和人类意图方面的性能。HA-ViD 的详细信息可以在以下网址中找到：https://iai-hrc.github.io/ha-vid。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Low-Light-Images-Using-Infrared-Encoded-Images"><a href="#Enhancing-Low-Light-Images-Using-Infrared-Encoded-Images" class="headerlink" title="Enhancing Low-Light Images Using Infrared-Encoded Images"></a>Enhancing Low-Light Images Using Infrared-Encoded Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04122">http://arxiv.org/abs/2307.04122</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wyf0912/ELIEI">https://github.com/wyf0912/ELIEI</a></li>
<li>paper_authors: Shulin Tian, Yufei Wang, Renjie Wan, Wenhan Yang, Alex C. Kot, Bihan Wen</li>
<li>For: 提高低光照图像的可见度和细节表示，增强低光照图像的感知效果。* Methods: 根据各个像素的值，除去各个像素的IR滤波器，从而提高图像的信号吞吐量和细节表示。* Results: 经验结果表明，提议的方法可以更好地提高低光照图像的可见度和细节表示，并且和参考图像的对比表明，提议的方法可以更好地保留图像的细节和含义。<details>
<summary>Abstract</summary>
Low-light image enhancement task is essential yet challenging as it is ill-posed intrinsically. Previous arts mainly focus on the low-light images captured in the visible spectrum using pixel-wise loss, which limits the capacity of recovering the brightness, contrast, and texture details due to the small number of income photons. In this work, we propose a novel approach to increase the visibility of images captured under low-light environments by removing the in-camera infrared (IR) cut-off filter, which allows for the capture of more photons and results in improved signal-to-noise ratio due to the inclusion of information from the IR spectrum. To verify the proposed strategy, we collect a paired dataset of low-light images captured without the IR cut-off filter, with corresponding long-exposure reference images with an external filter. The experimental results on the proposed dataset demonstrate the effectiveness of the proposed method, showing better performance quantitatively and qualitatively. The dataset and code are publicly available at https://wyf0912.github.io/ELIEI/
</details>
<details>
<summary>摘要</summary>
低光照图像增强任务是必备又挑战的，因为它是内在无法定义的。先前的艺术主要在可见光谱上使用像素损失来处理低光照图像，这限制了恢复照度、对比度和Texture详细的能力，因为可见光谱中的光子数量太少。在这种工作中，我们提出了一种新的方法，通过从增强摄像头中除掉内部红外（IR）遮盖器，以获取更多的光子信息，从而提高信号噪声比。为验证提议的策略，我们收集了一个对应的低光照图像和长暂曝光参考图像的对照数据集。实验结果表明，提议的方法具有较好的量化和质量性能。数据集和代码在https://wyf0912.github.io/ELIEI/可公共下载。
</details></li>
</ul>
<hr>
<h2 id="Mitosis-Detection-from-Partial-Annotation-by-Dataset-Generation-via-Frame-Order-Flipping"><a href="#Mitosis-Detection-from-Partial-Annotation-by-Dataset-Generation-via-Frame-Order-Flipping" class="headerlink" title="Mitosis Detection from Partial Annotation by Dataset Generation via Frame-Order Flipping"></a>Mitosis Detection from Partial Annotation by Dataset Generation via Frame-Order Flipping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04113">http://arxiv.org/abs/2307.04113</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/naivete5656/mdpafof">https://github.com/naivete5656/mdpafof</a></li>
<li>paper_authors: Kazuya Nishimura, Ami Katanaya, Shinichiro Chuma, Ryoma Bise</li>
<li>for: 提高生物医学研究中的细胞分化检测精度</li>
<li>methods: 使用部分标注序列训练深度学习模型</li>
<li>results: 对四个数据集进行测试，比较其性能和其他比较方法，得到了更高的检测精度<details>
<summary>Abstract</summary>
Detection of mitosis events plays an important role in biomedical research. Deep-learning-based mitosis detection methods have achieved outstanding performance with a certain amount of labeled data. However, these methods require annotations for each imaging condition. Collecting labeled data involves time-consuming human labor. In this paper, we propose a mitosis detection method that can be trained with partially annotated sequences. The base idea is to generate a fully labeled dataset from the partial labels and train a mitosis detection model with the generated dataset. First, we generate an image pair not containing mitosis events by frame-order flipping. Then, we paste mitosis events to the image pair by alpha-blending pasting and generate a fully labeled dataset. We demonstrate the performance of our method on four datasets, and we confirm that our method outperforms other comparisons which use partially labeled sequences.
</details>
<details>
<summary>摘要</summary>
检测细胞分裂事件在生物医学研究中扮演着重要的角色。基于深度学习的细胞分裂检测方法已经实现了非常出色的表现，但这些方法需要每种检测条件的注释。收集标注数据需要很长时间的人工劳动。在本文中，我们提出了一种不需要完全标注的细胞分裂检测方法。我们的基本想法是通过将部分标注序列转化成完全标注序列，然后使用生成的序列来训练细胞分裂检测模型。我们的方法包括将帧顺序翻转生成不含细胞分裂事件的图像对，然后使用alpha拟合粘贴细胞分裂事件到图像对中，并生成了一个完全标注的数据集。我们在四个数据集上展示了我们的方法的性能，并证明了我们的方法在与其他部分标注序列比较的情况下表现更好。
</details></li>
</ul>
<hr>
<h2 id="Parametric-Depth-Based-Feature-Representation-Learning-for-Object-Detection-and-Segmentation-in-Bird’s-Eye-View"><a href="#Parametric-Depth-Based-Feature-Representation-Learning-for-Object-Detection-and-Segmentation-in-Bird’s-Eye-View" class="headerlink" title="Parametric Depth Based Feature Representation Learning for Object Detection and Segmentation in Bird’s Eye View"></a>Parametric Depth Based Feature Representation Learning for Object Detection and Segmentation in Bird’s Eye View</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04106">http://arxiv.org/abs/2307.04106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayu Yang, Enze Xie, Miaomiao Liu, Jose M. Alvarez</li>
<li>for: 本研究旨在提高视觉 только perceive autonomous driving模型的性能，通过编码多视图图像特征到 Bird’s-Eye-View（BEV）空间中。</li>
<li>methods: 我们提出使用parametric depth distribution modeling来模型特征转换。我们首先通过每个像素在每个视图中预测parametric depth distribution来提升2D图像特征到3D空间中，然后将3D特征体系归一化到BEV幂。</li>
<li>results: 我们的方法在object detection和semantic segmentation任务上表现出色，比existings方法更高效。此外，我们还提出了一种新的可视性感知评价指标，可以减少halucination问题的影响。<details>
<summary>Abstract</summary>
Recent vision-only perception models for autonomous driving achieved promising results by encoding multi-view image features into Bird's-Eye-View (BEV) space. A critical step and the main bottleneck of these methods is transforming image features into the BEV coordinate frame. This paper focuses on leveraging geometry information, such as depth, to model such feature transformation. Existing works rely on non-parametric depth distribution modeling leading to significant memory consumption, or ignore the geometry information to address this problem. In contrast, we propose to use parametric depth distribution modeling for feature transformation. We first lift the 2D image features to the 3D space defined for the ego vehicle via a predicted parametric depth distribution for each pixel in each view. Then, we aggregate the 3D feature volume based on the 3D space occupancy derived from depth to the BEV frame. Finally, we use the transformed features for downstream tasks such as object detection and semantic segmentation. Existing semantic segmentation methods do also suffer from an hallucination problem as they do not take visibility information into account. This hallucination can be particularly problematic for subsequent modules such as control and planning. To mitigate the issue, our method provides depth uncertainty and reliable visibility-aware estimations. We further leverage our parametric depth modeling to present a novel visibility-aware evaluation metric that, when taken into account, can mitigate the hallucination problem. Extensive experiments on object detection and semantic segmentation on the nuScenes datasets demonstrate that our method outperforms existing methods on both tasks.
</details>
<details>
<summary>摘要</summary>
近期无视图识别模型在自动驾驶领域取得了有前途的结果，其中包括编码多视图图像特征到 Bird's-Eye-View（BEV）空间。这个步骤是无视图识别模型的关键步骤，同时也是主要的瓶颈。现有的方法通过非 Parametric depth 分布模型来解决这个问题，导致巨大的内存占用，或者完全忽略geometry信息。相比之下，我们提议使用 Parametric depth 分布模型来模型特征转换。我们首先通过每个像素在每个视图中预测的 Parametric depth 分布将二dimensional的图像特征提升到 egovehicle 定义的三维空间中。然后，我们根据 depth 空间占用 derivation 集成三维特征体volume到 BEV 帧中。最后，我们使用转换后的特征来进行下游任务，如物体检测和semantic segmentation。现有的semantic segmentation方法也受到了一种halucination问题的困扰，他们不考虑视ibilty信息。这种halucination可能对后续模块，如控制和规划，造成 particualrly problematic。为了缓解这个问题，我们的方法提供了depth uncertainty和可靠的视ibilty感知。此外，我们还利用我们的 Parametric depth 模型来提出一种新的可见性感知 metric，当被考虑时，可以缓解halucination问题。我们在 nuScenes 数据集上进行了广泛的对象检测和semantic segmentation实验，结果表明我们的方法在两个任务上都高于现有方法。
</details></li>
</ul>
<hr>
<h2 id="CA-CentripetalNet-A-novel-anchor-free-deep-learning-framework-for-hardhat-wearing-detection"><a href="#CA-CentripetalNet-A-novel-anchor-free-deep-learning-framework-for-hardhat-wearing-detection" class="headerlink" title="CA-CentripetalNet: A novel anchor-free deep learning framework for hardhat wearing detection"></a>CA-CentripetalNet: A novel anchor-free deep learning framework for hardhat wearing detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04103">http://arxiv.org/abs/2307.04103</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhijian Liu, Nian Cai, Wensheng Ouyang, Chengbin Zhang, Nili Tian, Han Wang</li>
<li>for: 强化建筑工地安全管理，提高建筑工人安全度</li>
<li>methods: 使用CA-CentripetalNet深度学习框架，并提出了两种新的策略，即垂直水平角块挤压和约束中心注意力，以提高特征提取和利用能力</li>
<li>results: 实验结果显示，CA-CentripetalNet在准确率和内存占用之间取得了更好的平衡，具体来说是86.63%的MAP值，特别是在小型帽子和非穿着帽子的情况下表现更佳，而且比既有深度学习方法更快速、更低占用内存。<details>
<summary>Abstract</summary>
Automatic hardhat wearing detection can strengthen the safety management in construction sites, which is still challenging due to complicated video surveillance scenes. To deal with the poor generalization of previous deep learning based methods, a novel anchor-free deep learning framework called CA-CentripetalNet is proposed for hardhat wearing detection. Two novel schemes are proposed to improve the feature extraction and utilization ability of CA-CentripetalNet, which are vertical-horizontal corner pooling and bounding constrained center attention. The former is designed to realize the comprehensive utilization of marginal features and internal features. The latter is designed to enforce the backbone to pay attention to internal features, which is only used during the training rather than during the detection. Experimental results indicate that the CA-CentripetalNet achieves better performance with the 86.63% mAP (mean Average Precision) with less memory consumption at a reasonable speed than the existing deep learning based methods, especially in case of small-scale hardhats and non-worn-hardhats.
</details>
<details>
<summary>摘要</summary>
自动帽子穿戴检测可以增强建筑现场的安全管理，但是由于复杂的视频监测场景，这还是一项挑战。为了解决先前深度学习基于方法的泛化不佳问题，一种新的无锚点深度学习框架called CA-CentripetalNet被提议用于帽子穿戴检测。两种新的方案被提出来提高CA-CentripetalNet的特征提取和利用能力，即水平垂直角池和约束中心注意力。前者是为了实现全面利用边缘特征和内部特征。后者是为了让底层在训练时强制注意内部特征，但是只在训练中使用，而不是在检测中使用。实验结果显示，CA-CentripetalNet可以在小规模帽子和非穿戴帽子情况下达到86.63%的MAP（平均准确率），并且具有较低的内存占用和合理的速度，比先前的深度学习基于方法更好。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Building-Semantic-Segmentation-Accuracy-with-Super-Resolution-and-Deep-Learning-Investigating-the-Impact-of-Spatial-Resolution-on-Various-Datasets"><a href="#Enhancing-Building-Semantic-Segmentation-Accuracy-with-Super-Resolution-and-Deep-Learning-Investigating-the-Impact-of-Spatial-Resolution-on-Various-Datasets" class="headerlink" title="Enhancing Building Semantic Segmentation Accuracy with Super Resolution and Deep Learning: Investigating the Impact of Spatial Resolution on Various Datasets"></a>Enhancing Building Semantic Segmentation Accuracy with Super Resolution and Deep Learning: Investigating the Impact of Spatial Resolution on Various Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04101">http://arxiv.org/abs/2307.04101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiling Guo, Xiaodan Shi, Haoran Zhang, Dou Huang, Xiaoya Song, Jinyue Yan, Ryosuke Shibasaki<br>for: 本研究旨在 investigate the impact of spatial resolution on deep learning based building semantic segmentation, and provide insights for data selection and preparation.methods: 本研究使用 remote sensing images 在三个研究区域中创造多个空间分辨率，通过超分辨和降采样。然后，选择了 two 个代表性的深度学习架构：UNet 和 FPN，进行模型训练和测试。results: 实验结果显示，空间分辨率对建筑分类结果产生很大影响，0.3米级别的空间分辨率具有更高的成本效果。<details>
<summary>Abstract</summary>
The development of remote sensing and deep learning techniques has enabled building semantic segmentation with high accuracy and efficiency. Despite their success in different tasks, the discussions on the impact of spatial resolution on deep learning based building semantic segmentation are quite inadequate, which makes choosing a higher cost-effective data source a big challenge. To address the issue mentioned above, in this study, we create remote sensing images among three study areas into multiple spatial resolutions by super-resolution and down-sampling. After that, two representative deep learning architectures: UNet and FPN, are selected for model training and testing. The experimental results obtained from three cities with two deep learning models indicate that the spatial resolution greatly influences building segmentation results, and with a better cost-effectiveness around 0.3m, which we believe will be an important insight for data selection and preparation.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Remote sensing技术和深度学习技术的发展，使得建筑 semantic segmentation 的准确率和效率得到了大幅提高。 DESPITE 这些成功在不同任务中，关于深度学习基于建筑 semantic segmentation 的空间分辨率影响的讨论相对较少，这使得选择更高成本效益的数据源变得具有挑战性。为解决上述问题，在本研究中，我们将Remote sensing 图像分割成多个空间分辨率，通过超分辨和降分辨。然后，我们选择了两种代表性的深度学习架构：UNet 和 FPN，进行模型训练和测试。实验结果表明，在三个城市的两种深度学习模型下，空间分辨率对建筑分割结果产生了很大影响，并且在约0.3米的成本效益下，我们认为这将成为数据选择和准备的重要视角。Note: The text has been translated using Google Translate, and some minor adjustments may be necessary to ensure accuracy.
</details></li>
</ul>
<hr>
<h2 id="Visible-and-infrared-self-supervised-fusion-trained-on-a-single-example"><a href="#Visible-and-infrared-self-supervised-fusion-trained-on-a-single-example" class="headerlink" title="Visible and infrared self-supervised fusion trained on a single example"></a>Visible and infrared self-supervised fusion trained on a single example</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04100">http://arxiv.org/abs/2307.04100</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nati Ofir</li>
<li>for: 这个论文解决了RGB和 Near-Infrared（NIR）图像 fusión问题。</li>
<li>methods: 该方法使用了一个Convolutional-Neural-Network（CNN），通过Self-Supervised-Learning（SSL）在一个示例上训练。</li>
<li>results: 该方法可以 preserve each spectral channel的相关细节，而不需要大量的训练过程。在实验部分，该方法比其他最近的方法得到了更好的量化和质量的多спектраль融合结果。<details>
<summary>Abstract</summary>
This paper addresses the problem of visible (RGB) to Near-Infrared (NIR) image fusion. Multispectral imaging is an important task relevant to image processing and computer vision, even more, since the development of the RGBT sensor. While the visible image sees color and suffers from noise, haze, and clouds, the NIR channel captures a clearer picture and it is significantly required by applications such as dehazing or object detection. The proposed approach fuses these two aligned channels by training a Convolutional-Neural-Network (CNN) by a Self-Supervised-Learning (SSL) on a single example. For each such pair, RGB and IR, the network is trained for seconds to deduce the final fusion. The SSL is based on Sturcture-of-Similarity (SSIM) loss combined with Edge-Preservation (EP) loss. The labels for the SSL are the input channels themselves. This fusion preserves the relevant detail of each spectral channel while not based on a heavy training process. In the experiments section, the proposed approach achieves better qualitative and quantitative multispectral fusion results with respect to other recent methods, that are not based on large dataset training.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="GNP-Attack-Transferable-Adversarial-Examples-via-Gradient-Norm-Penalty"><a href="#GNP-Attack-Transferable-Adversarial-Examples-via-Gradient-Norm-Penalty" class="headerlink" title="GNP Attack: Transferable Adversarial Examples via Gradient Norm Penalty"></a>GNP Attack: Transferable Adversarial Examples via Gradient Norm Penalty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04099">http://arxiv.org/abs/2307.04099</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Wu, Tie Luo, Donald C. Wunsch</li>
<li>for: 增强敌意例的跨模型可转移性，使得实际黑盒攻击可以在不同的目标模型上进行。</li>
<li>methods: 使用Gradient Norm Penalty（GNP）进行攻击，GNP会让优化过程逐渐落入丢失函数的平坦区域，从而提高敌意例的可转移性。</li>
<li>results: 通过对11个state-of-the-art深度学习模型和6个高级防御方法进行攻击，实证表明GNP可以非常有效地生成高可转移性的敌意例。此外，GNP也可以与其他梯度基于方法结合使用，以实现更强大的跨模型攻击。<details>
<summary>Abstract</summary>
Adversarial examples (AE) with good transferability enable practical black-box attacks on diverse target models, where insider knowledge about the target models is not required. Previous methods often generate AE with no or very limited transferability; that is, they easily overfit to the particular architecture and feature representation of the source, white-box model and the generated AE barely work for target, black-box models. In this paper, we propose a novel approach to enhance AE transferability using Gradient Norm Penalty (GNP). It drives the loss function optimization procedure to converge to a flat region of local optima in the loss landscape. By attacking 11 state-of-the-art (SOTA) deep learning models and 6 advanced defense methods, we empirically show that GNP is very effective in generating AE with high transferability. We also demonstrate that it is very flexible in that it can be easily integrated with other gradient based methods for stronger transfer-based attacks.
</details>
<details>
<summary>摘要</summary>
“敌对例”（AE）具有良好的转移性，可以实现实际的黑盒攻击，不需要内部知识关于目标模型。先前的方法往往将AE生成成为特定架构和特征表示的适应器，即使生成的AE对目标黑盒模型具有很少或根本无效的转移性。在本文中，我们提出了一种增强AE转移性的方法，使用梯度 нор penalty（GNP）。它驱动损失函数优化程序落在损失图形中的极小区域中落点。我们透过对11个现代深度学习模型和6个高级防护方法进行实验，证明了GNP具有很高的转移性。我们还证明了它可以与其他梯度基本方法结合使用，以实现更强的转移基于攻击。
</details></li>
</ul>
<hr>
<h2 id="CMDFusion-Bidirectional-Fusion-Network-with-Cross-modality-Knowledge-Distillation-for-LIDAR-Semantic-Segmentation"><a href="#CMDFusion-Bidirectional-Fusion-Network-with-Cross-modality-Knowledge-Distillation-for-LIDAR-Semantic-Segmentation" class="headerlink" title="CMDFusion: Bidirectional Fusion Network with Cross-modality Knowledge Distillation for LIDAR Semantic Segmentation"></a>CMDFusion: Bidirectional Fusion Network with Cross-modality Knowledge Distillation for LIDAR Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04091">http://arxiv.org/abs/2307.04091</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Jun-CEN/CMDFusion">https://github.com/Jun-CEN/CMDFusion</a></li>
<li>paper_authors: Jun Cen, Shiwei Zhang, Yixuan Pei, Kun Li, Hang Zheng, Maochun Luo, Yingya Zhang, Qifeng Chen</li>
<li>for: 本研究提出了一种bidirectional fusion network with cross-modality knowledge distillation（CMDFusion），用于解决自动驾驶车辆的LIDAR semantic segmentation任务中的2D和3D混合问题。</li>
<li>methods: 我们的CMDFusion方法有两个贡献：首先，我们的对称混合方案可以同时利用2D和3D信息，从而超越单个混合方案；其次，我们通过知识传授来帮助3D网络生成2D信息，从而解决RGB图像不可预知的问题。</li>
<li>results: 我们在SemanticKITTI和nuScenes数据集上测试了CMDFusion方法，并证明其在所有混合基于方法中达到最好的性能。<details>
<summary>Abstract</summary>
2D RGB images and 3D LIDAR point clouds provide complementary knowledge for the perception system of autonomous vehicles. Several 2D and 3D fusion methods have been explored for the LIDAR semantic segmentation task, but they suffer from different problems. 2D-to-3D fusion methods require strictly paired data during inference, which may not be available in real-world scenarios, while 3D-to-2D fusion methods cannot explicitly make full use of the 2D information. Therefore, we propose a Bidirectional Fusion Network with Cross-Modality Knowledge Distillation (CMDFusion) in this work. Our method has two contributions. First, our bidirectional fusion scheme explicitly and implicitly enhances the 3D feature via 2D-to-3D fusion and 3D-to-2D fusion, respectively, which surpasses either one of the single fusion schemes. Second, we distillate the 2D knowledge from a 2D network (Camera branch) to a 3D network (2D knowledge branch) so that the 3D network can generate 2D information even for those points not in the FOV (field of view) of the camera. In this way, RGB images are not required during inference anymore since the 2D knowledge branch provides 2D information according to the 3D LIDAR input. We show that our CMDFusion achieves the best performance among all fusion-based methods on SemanticKITTI and nuScenes datasets. The code will be released at https://github.com/Jun-CEN/CMDFusion.
</details>
<details>
<summary>摘要</summary>
二维RGB图像和三维激光雷达点云提供补充知识 для自动驾驶车辆的识别系统。许多二维和三维融合方法已经为LIDAR语义分割任务进行研究，但它们受到不同的问题困扰。二维到三维融合方法需要在推理过程中具有匹配的数据，而三维到二维融合方法无法直接充分利用二维信息。因此，我们在本工作中提出了一种架构协同融合网络（CMDFusion）。我们的方法有两个贡献。首先，我们的对称融合方案可以同时利用二维和三维信息，通过二维到三维融合和三维到二维融合来强化三维特征，这超过了单一的融合方案。其次，我们将二维知识从二维网络（Camera分支）传承给三维网络（二维知识分支），以便三维网络可以根据三维激光雷达输入生成二维信息，不需要RGB图像在推理过程中 anymore。我们表明，我们的CMDFusion在SemanticKITTI和nuScenes数据集上的性能比所有融合基于方法更好。代码将于https://github.com/Jun-CEN/CMDFusion中发布。
</details></li>
</ul>
<hr>
<h2 id="SVIT-Scaling-up-Visual-Instruction-Tuning"><a href="#SVIT-Scaling-up-Visual-Instruction-Tuning" class="headerlink" title="SVIT: Scaling up Visual Instruction Tuning"></a>SVIT: Scaling up Visual Instruction Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04087">http://arxiv.org/abs/2307.04087</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/baai-dcai/visual-instruction-tuning">https://github.com/baai-dcai/visual-instruction-tuning</a></li>
<li>paper_authors: Bo Zhao, Boya Wu, Tiejun Huang</li>
<li>for: 提高多Modal模型的可视理解和计划能力</li>
<li>methods: 使用Visual Instruction Tuning（SVIT）集成3.2万个视觉指令调整数据，包括1.6万对话问答（QA）对和1.6万个复杂的推理QA对和106个详细的图像描述。</li>
<li>results: 训练多Modal模型 на SVIT可以显著提高多Modal性能，包括视觉理解、推理和规划能力。<details>
<summary>Abstract</summary>
Thanks to the emerging of foundation models, the large language and vision models are integrated to acquire the multimodal ability of visual captioning, dialogue, question answering, etc. Although existing multimodal models present impressive performance of visual understanding and reasoning, their limits are still largely under-explored due to the scarcity of high-quality instruction tuning data. To push the limits of multimodal capability, we Sale up Visual Instruction Tuning (SVIT) by constructing a dataset of 3.2 million visual instruction tuning data including 1.6M conversation question-answer (QA) pairs and 1.6M complex reasoning QA pairs and 106K detailed image descriptions. Besides the volume, the proposed dataset is also featured by the high quality and rich diversity, which is generated by prompting GPT-4 with the abundant manual annotations of images. We empirically verify that training multimodal models on SVIT can significantly improve the multimodal performance in terms of visual perception, reasoning and planing.
</details>
<details>
<summary>摘要</summary>
Thanks to the emergence of foundation models, large language and vision models are integrated to acquire multimodal capabilities such as visual captioning, dialogue, question answering, etc. Although existing multimodal models have shown impressive performance in visual understanding and reasoning, their limitations are still largely unexplored due to the scarcity of high-quality instruction tuning data. To push the limits of multimodal capability, we constructed a dataset of 3.2 million visual instruction tuning data, including 1.6 million conversation question-answer (QA) pairs and 1.6 million complex reasoning QA pairs, as well as 106,000 detailed image descriptions. Besides the volume, the proposed dataset is also characterized by high quality and rich diversity, generated by prompting GPT-4 with abundant manual annotations of images. We empirically verified that training multimodal models on SVIT can significantly improve multimodal performance in terms of visual perception, reasoning, and planning.
</details></li>
</ul>
<hr>
<h2 id="Score-based-Conditional-Generation-with-Fewer-Labeled-Data-by-Self-calibrating-Classifier-Guidance"><a href="#Score-based-Conditional-Generation-with-Fewer-Labeled-Data-by-Self-calibrating-Classifier-Guidance" class="headerlink" title="Score-based Conditional Generation with Fewer Labeled Data by Self-calibrating Classifier Guidance"></a>Score-based Conditional Generation with Fewer Labeled Data by Self-calibrating Classifier Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04081">http://arxiv.org/abs/2307.04081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Kuo-Ming Huang, Si-An Chen, Hsuan-Tien Lin</li>
<li>for: 提高基于分类器的生成模型（SGMs）的 conditional generation 质量，特别是使用 fewer labeled data。</li>
<li>methods: 利用 energy-based models 将分类器视为另一种视角，然后使用现有的损失函数来准确地调整分类器。</li>
<li>results: 提出的方法可以在不同百分比的标注数据量下提高 conditional generation 质量，并且在使用 fewer labeled data 时比其他 conditional SGMs 表现更佳。<details>
<summary>Abstract</summary>
Score-based Generative Models (SGMs) are a popular family of deep generative models that achieves leading image generation quality. Earlier studies have extended SGMs to tackle class-conditional generation by coupling an unconditional SGM with the guidance of a trained classifier. Nevertheless, such classifier-guided SGMs do not always achieve accurate conditional generation, especially when trained with fewer labeled data. We argue that the issue is rooted in unreliable gradients of the classifier and the inability to fully utilize unlabeled data during training. We then propose to improve classifier-guided SGMs by letting the classifier calibrate itself. Our key idea is to use principles from energy-based models to convert the classifier as another view of the unconditional SGM. Then, existing loss for the unconditional SGM can be adopted to calibrate the classifier using both labeled and unlabeled data. Empirical results validate that the proposed approach significantly improves the conditional generation quality across different percentages of labeled data. The improved performance makes the proposed approach consistently superior to other conditional SGMs when using fewer labeled data. The results confirm the potential of the proposed approach for generative modeling with limited labeled data.
</details>
<details>
<summary>摘要</summary>
Score-based生成模型（SGM）是一种深度生成模型，其可以实现领先的图像生成质量。早期研究者将SGM扩展到实现类别条件生成，通过与训练过的分类器结合来实现。然而，这种类器导向的SGM并不总能够实现准确的条件生成，特别是当使用少量标注数据训练时。我们认为这问题的根本原因是分类器的梯度不可靠和无法完全利用无标注数据 during training。我们然后提议通过让分类器自己准确化来改进类器导向的SGM。我们的关键思想是使用能量基模型的原理来转换分类器，然后使用现有的损失函数来准确化分类器，并使用标注数据和无标注数据进行准确化。实验结果表明，我们的方法可以明显提高条件生成质量，并在使用少量标注数据时保持优越性。这些结果证明了我们的方法在生成模型中具有有限标注数据的潜在优势。
</details></li>
</ul>
<hr>
<h2 id="Random-Position-Adversarial-Patch-for-Vision-Transformers"><a href="#Random-Position-Adversarial-Patch-for-Vision-Transformers" class="headerlink" title="Random Position Adversarial Patch for Vision Transformers"></a>Random Position Adversarial Patch for Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04066">http://arxiv.org/abs/2307.04066</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingzhen Shao<br>for: This paper aims to propose a novel method for generating adversarial patches that can launch targeted attacks on vision transformers, overcoming the alignment constraint of previous studies.methods: The proposed method employs a GAN-like structure to generate the adversarial patch, instead of directly optimizing the patch using gradients.results: The generated adversarial patch exhibits effectiveness in achieving universal attacks on vision transformers in both digital and physical-world scenarios, and shows robustness to brightness restriction, color transfer, and random noise. Real-world attack experiments validate the effectiveness of the proposed method.Here’s the Chinese version:for: 这篇论文目的是提出一种新的对视转换器进行攻击的方法，使得攻击可以在任何位置进行。methods: 该方法使用GAN-like结构生成攻击 patch，而不是直接使用梯度来优化 patch。results: 生成的攻击 patch能够在数字和物理世界中实现对视转换器的通用攻击，并且具有对比明亮、颜色转换和随机噪声的Robustness。实际攻击实验证明了提案的有效性。<details>
<summary>Abstract</summary>
Previous studies have shown the vulnerability of vision transformers to adversarial patches, but these studies all rely on a critical assumption: the attack patches must be perfectly aligned with the patches used for linear projection in vision transformers. Due to this stringent requirement, deploying adversarial patches for vision transformers in the physical world becomes impractical, unlike their effectiveness on CNNs. This paper proposes a novel method for generating an adversarial patch (G-Patch) that overcomes the alignment constraint, allowing the patch to launch a targeted attack at any position within the field of view. Specifically, instead of directly optimizing the patch using gradients, we employ a GAN-like structure to generate the adversarial patch. Our experiments show the effectiveness of the adversarial patch in achieving universal attacks on vision transformers, both in digital and physical-world scenarios. Additionally, further analysis reveals that the generated adversarial patch exhibits robustness to brightness restriction, color transfer, and random noise. Real-world attack experiments validate the effectiveness of the G-Patch to launch robust attacks even under some very challenging conditions.
</details>
<details>
<summary>摘要</summary>
Here's the Simplified Chinese translation:先前的研究已经显示了视transformer的易受到攻击的潜在性，但这些研究都基于一个严格的假设：攻击 patches必须与视transformer中使用的 linear projection patches完全匹配。由于这个严格的要求，在实际世界中部署攻击 patches for vision transformers是不实际的，与 CNNs 不同。这篇论文提出了一种新的方法来生成攻击 patch (G-Patch)，以 overcome 这个匹配 constraint，让 patch 可以在视野中任意位置发起攻击。具体来说，我们不直接使用梯度来优化 patch，而是使用 GAN 结构来生成攻击 patch。我们的实验显示，G-Patch 可以在数字和物理世界中实现对 vision transformers 的通用攻击。此外，进一步的分析发现，生成的攻击 patch 具有对 brightness restriction、color transfer 和随机噪声的Robustness。实际攻击实验证明 G-Patch 可以在一些非常困难的条件下发起Robust攻击。
</details></li>
</ul>
<hr>
<h2 id="Combining-transmission-speckle-photography-and-convolutional-neural-network-for-determination-of-fat-content-in-cow’s-milk-–-an-exercise-in-classification-of-parameters-of-a-complex-suspension"><a href="#Combining-transmission-speckle-photography-and-convolutional-neural-network-for-determination-of-fat-content-in-cow’s-milk-–-an-exercise-in-classification-of-parameters-of-a-complex-suspension" class="headerlink" title="Combining transmission speckle photography and convolutional neural network for determination of fat content in cow’s milk – an exercise in classification of parameters of a complex suspension"></a>Combining transmission speckle photography and convolutional neural network for determination of fat content in cow’s milk – an exercise in classification of parameters of a complex suspension</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15069">http://arxiv.org/abs/2307.15069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kwasi Nyandey, Daniel Jakubczyk</li>
<li>for: direct classification and recognition of milk fat content classes</li>
<li>methods: combined transmission speckle photography and machine learning</li>
<li>results: achieved 100% test and independent classification accuracies<details>
<summary>Abstract</summary>
We have combined transmission speckle photography and machine learning for direct classification and recognition of milk fat content classes. Our aim was hinged on the fact that parameters of scattering particles (and the dispersion medium) can be linked to the intensity distribution (speckle) observed when coherent light is transmitted through a scattering medium. For milk, it is primarily the size distribution and concentration of fat globules, which constitutes the total fat content. Consequently, we trained convolutional neural network to recognise and classify laser speckle from different fat content classes (0.5, 1.5, 2.0 and 3.2%). We investigated four exposure-time protocols and obtained the highest performance for shorter exposure times, in which the intensity histograms are kept similar for all images and the most probable intensity in the speckle pattern is close to zero. Our neural network was able to recognize the milk fat content classes unambiguously and we obtained the highest test and independent classification accuracies of 100 and ~99% respectively. It indicates that the parameters of other complex realistic suspensions could be classified with similar methods.
</details>
<details>
<summary>摘要</summary>
我们将传输扑杂照相与机器学习结合用于直接分类和识别牛奶脂肪含量类别。我们的目标是基于散射体（以及杂散媒体）参数与干扰光束传输过程中观察到的INTENSITY分布（扑杂），以确定牛奶中脂肪含量的总体分布。因此，我们训练了卷积神经网络，以识别和分类不同脂肪含量类别（0.5、1.5、2.0和3.2%）的扑杂照片。我们研究了四种不同的曝光时间协议，并获得了最高性能，其中短曝光时间下，图像的INTENSITY分布保持相似，而最有可能的扑杂干扰强度在图像中几乎为零。我们的神经网络能够无ambiguously识别牛奶脂肪含量类别，并在测试和独立分类任务中获得了100%和~99%的准确率。这表示我们可以使用类似方法来分类其他复杂的实际涂杂体系。
</details></li>
</ul>
<hr>
<h2 id="Deep-Unsupervised-Learning-Using-Spike-Timing-Dependent-Plasticity"><a href="#Deep-Unsupervised-Learning-Using-Spike-Timing-Dependent-Plasticity" class="headerlink" title="Deep Unsupervised Learning Using Spike-Timing-Dependent Plasticity"></a>Deep Unsupervised Learning Using Spike-Timing-Dependent Plasticity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04054">http://arxiv.org/abs/2307.04054</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sen Lu, Abhronil Sengupta</li>
<li>for: 这个论文是为了提出一种基于STDP的深度学习框架，以提高SNNs的性能和可扩展性。</li>
<li>methods: 该论文使用了一种组合了STDP clustering和深度学习的方法，通过在网络输出上生成pseudo-标签来训练深度网络。</li>
<li>results: 相比于使用$k$-means clustering方法，该方法可以达到$24.56%$的高精度和$3.5\times$快的 convergespeed，在Tiny ImageNet dataset上的10类子集上实现。<details>
<summary>Abstract</summary>
Spike-Timing-Dependent Plasticity (STDP) is an unsupervised learning mechanism for Spiking Neural Networks (SNNs) that has received significant attention from the neuromorphic hardware community. However, scaling such local learning techniques to deeper networks and large-scale tasks has remained elusive. In this work, we investigate a Deep-STDP framework where a convolutional network is trained in tandem with pseudo-labels generated by the STDP clustering process on the network outputs. We achieve $24.56\%$ higher accuracy and $3.5\times$ faster convergence speed at iso-accuracy on a 10-class subset of the Tiny ImageNet dataset in contrast to a $k$-means clustering approach.
</details>
<details>
<summary>摘要</summary>
短时间依赖形变学习（STDP）是一种无监督学习机制，用于神经网络（SNN），它在神经机器学习社区中受到了重视。但是，将这种本地学习技术扩展到更深的网络和大规模任务中，仍然是一个棘手的问题。在这项工作中，我们调查了一种含有深度学习的STDP框架，其中一个卷积网络与STDP归类过程的输出生成的 Pseudo-标签一起进行训练。我们在一个Tiny ImageNet数据集的10类子集上达到了$24.56\%$的高精度和$3.5\times$快于iso-精度的整合速度。相比之下，使用$k$-means归类法，我们的精度提高了$24.56\%$，并且需要$3.5\times$多的训练时间。
</details></li>
</ul>
<hr>
<h2 id="Calibration-Aware-Margin-Loss-Pushing-the-Accuracy-Calibration-Consistency-Pareto-Frontier-for-Deep-Metric-Learning"><a href="#Calibration-Aware-Margin-Loss-Pushing-the-Accuracy-Calibration-Consistency-Pareto-Frontier-for-Deep-Metric-Learning" class="headerlink" title="Calibration-Aware Margin Loss: Pushing the Accuracy-Calibration Consistency Pareto Frontier for Deep Metric Learning"></a>Calibration-Aware Margin Loss: Pushing the Accuracy-Calibration Consistency Pareto Frontier for Deep Metric Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04047">http://arxiv.org/abs/2307.04047</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qin Zhang, Linghan Xu, Qingming Tang, Jun Fang, Ying Nian Wu, Joe Tighe, Yifan Xing</li>
<li>for: 本文旨在提出一种新的评估 metric learning 模型准确性和一致性的方法，以便在不同的测试分布中实现轻松的部署。</li>
<li>methods: 本文提出了一种名为 Operating-Point-Incosistency-Score (OPIS) 的新指标，用于衡量不同类别在目标准化范围内的运行特性之间的差异。同时，本文还提出了一种名为 Calibration-Aware Margin (CAM) 的新正则项，用于在训练过程中鼓励类别间的表示结构更加一致。</li>
<li>results: 实验结果表明，CAM 正则项可以提高模型的准确性和一致性，并且可以在保持或者超越现有深度 metric learning 方法的情况下，提高模型的一致性。<details>
<summary>Abstract</summary>
The ability to use the same distance threshold across different test classes / distributions is highly desired for a frictionless deployment of commercial image retrieval systems. However, state-of-the-art deep metric learning losses often result in highly varied intra-class and inter-class embedding structures, making threshold calibration a non-trivial process in practice. In this paper, we propose a novel metric named Operating-Point-Incosistency-Score (OPIS) that measures the variance in the operating characteristics across different classes in a target calibration range, and demonstrate that high accuracy of a metric learning embedding model does not guarantee calibration consistency for both seen and unseen classes. We find that, in the high-accuracy regime, there exists a Pareto frontier where accuracy improvement comes at the cost of calibration consistency. To address this, we develop a novel regularization, named Calibration-Aware Margin (CAM) loss, to encourage uniformity in the representation structures across classes during training. Extensive experiments demonstrate CAM's effectiveness in improving calibration-consistency while retaining or even enhancing accuracy, outperforming state-of-the-art deep metric learning methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>使用同一个距离阈值来部署商业图像检索系统是非常愿望的。然而，现状的深度度量学损失经常导致类之间和类之间的嵌入结构具有很高的变化程度，从而使得阈值调整成为实践中的非常困难问题。在这篇论文中，我们提出了一个新的度量名为操作点不一致分数（OPIS），该度量测量不同类别在目标调整范围内的操作特性的变化程度，并证明了高精度度量学嵌入模型不一定能够保证所见和未见类别之间的均衡一致。我们发现，在高精度 régime 下，存在一个Pareto前沿，其中精度提高来到了均衡一致的代价。为解决这个问题，我们开发了一种新的正则化方法，即均衡感知损失（CAM），以促进类别之间的表示结构具有更好的一致性。广泛的实验证明了CAM的效果，可以提高均衡一致性，同时保持或者提高精度，超越当前的深度度量学方法。
</details></li>
</ul>
<hr>
<h2 id="High-Fidelity-3D-Hand-Shape-Reconstruction-via-Scalable-Graph-Frequency-Decomposition"><a href="#High-Fidelity-3D-Hand-Shape-Reconstruction-via-Scalable-Graph-Frequency-Decomposition" class="headerlink" title="High Fidelity 3D Hand Shape Reconstruction via Scalable Graph Frequency Decomposition"></a>High Fidelity 3D Hand Shape Reconstruction via Scalable Graph Frequency Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05541">http://arxiv.org/abs/2307.05541</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tyluann/freqhand">https://github.com/tyluann/freqhand</a></li>
<li>paper_authors: Tianyu Luan, Yuanhao Zhai, Jingjing Meng, Zhong Li, Zhang Chen, Yi Xu, Junsong Yuan</li>
<li>for: 高精度手模型生成</li>
<li>methods: 频率拆分网络+频域分解损失</li>
<li>results:  preserved 高频精度个性化手模型Here’s a brief explanation of each point:* “高精度手模型生成” (high-fidelity hand modeling) - The paper aims to generate highly detailed 3D hand models using a novel network architecture.* “频率拆分网络+” (frequency split network) - The proposed network uses a coarse-to-fine strategy to generate 3D hand meshes, with different frequency bands used in each resolution level.* “频域分解损失” (frequency decomposition loss) - The network uses a novel loss function to supervise each frequency component, allowing it to capture high-frequency personalized details.<details>
<summary>Abstract</summary>
Despite the impressive performance obtained by recent single-image hand modeling techniques, they lack the capability to capture sufficient details of the 3D hand mesh. This deficiency greatly limits their applications when high-fidelity hand modeling is required, e.g., personalized hand modeling. To address this problem, we design a frequency split network to generate 3D hand mesh using different frequency bands in a coarse-to-fine manner. To capture high-frequency personalized details, we transform the 3D mesh into the frequency domain, and propose a novel frequency decomposition loss to supervise each frequency component. By leveraging such a coarse-to-fine scheme, hand details that correspond to the higher frequency domain can be preserved. In addition, the proposed network is scalable, and can stop the inference at any resolution level to accommodate different hardware with varying computational powers. To quantitatively evaluate the performance of our method in terms of recovering personalized shape details, we introduce a new evaluation metric named Mean Signal-to-Noise Ratio (MSNR) to measure the signal-to-noise ratio of each mesh frequency component. Extensive experiments demonstrate that our approach generates fine-grained details for high-fidelity 3D hand reconstruction, and our evaluation metric is more effective for measuring mesh details compared with traditional metrics.
</details>
<details>
<summary>摘要</summary>
尽管最近的单图手模型技术具有印象人的表现，但它们缺乏能够捕捉足够细节的3D手模型的能力。这个问题限制了它们在需要高精度手模型时的应用。为解决这个问题，我们设计了一个频分网络，通过不同的频谱带来生成3D手模型。为了捕捉高频个性化细节，我们将3D网格转换到频域中，并提出了一种新的频分loss来监督每个频谱成分。通过这种层次结构，可以保持手中的高频细节。此外，我们的网络可扩展，可以根据不同的硬件计算能力 stopping inference 在不同的分辨率级别。为了量化评估我们方法在个性化手模型中恢复细节的性能，我们引入了一个新的评估指标，即 Mean Signal-to-Noise Ratio (MSNR)，来度量每个频谱成分的信号噪比。我们的实验表明，我们的方法可以生成高精度的3D手模型，而我们引入的评估指标比传统指标更有效地度量网格细节。
</details></li>
</ul>
<hr>
<h2 id="Novel-Pipeline-for-Diagnosing-Acute-Lymphoblastic-Leukemia-Sensitive-to-Related-Biomarkers"><a href="#Novel-Pipeline-for-Diagnosing-Acute-Lymphoblastic-Leukemia-Sensitive-to-Related-Biomarkers" class="headerlink" title="Novel Pipeline for Diagnosing Acute Lymphoblastic Leukemia Sensitive to Related Biomarkers"></a>Novel Pipeline for Diagnosing Acute Lymphoblastic Leukemia Sensitive to Related Biomarkers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04014">http://arxiv.org/abs/2307.04014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirhossein Askari-Farsangi, Ali Sharifi-Zarchi, Mohammad Hossein Rohban</li>
<li>for: 这份研究的目的是为了提供一个基于深度学习的ALL诊断方法，并且解决了小型医疗训练数据所导致的模型简化现象。</li>
<li>methods: 我们的方法是基于专家的工作流程，使用多个图像进行诊断，并且将模型组织成一个多个例子学习问题，以提高诊断精度。</li>
<li>results: 我们的模型在ALL IDB 1上取得了96.15%的准确率，94.24%的F1分数，97.56%的感知率和90.91%的特征率，并且在对应外部数据集进行评估时也有了可接受的表现。<details>
<summary>Abstract</summary>
Acute Lymphoblastic Leukemia (ALL) is one of the most common types of childhood blood cancer. The quick start of the treatment process is critical to saving the patient's life, and for this reason, early diagnosis of this disease is essential. Examining the blood smear images of these patients is one of the methods used by expert doctors to diagnose this disease. Deep learning-based methods have numerous applications in medical fields, as they have significantly advanced in recent years. ALL diagnosis is not an exception in this field, and several machine learning-based methods for this problem have been proposed. In previous methods, high diagnostic accuracy was reported, but our work showed that this alone is not sufficient, as it can lead to models taking shortcuts and not making meaningful decisions. This issue arises due to the small size of medical training datasets. To address this, we constrained our model to follow a pipeline inspired by experts' work. We also demonstrated that, since a judgement based on only one image is insufficient, redefining the problem as a multiple-instance learning problem is necessary for achieving a practical result. Our model is the first to provide a solution to this problem in a multiple-instance learning setup. We introduced a novel pipeline for diagnosing ALL that approximates the process used by hematologists, is sensitive to disease biomarkers, and achieves an accuracy of 96.15%, an F1-score of 94.24%, a sensitivity of 97.56%, and a specificity of 90.91% on ALL IDB 1. Our method was further evaluated on an out-of-distribution dataset, which posed a challenging test and had acceptable performance. Notably, our model was trained on a relatively small dataset, highlighting the potential for our approach to be applied to other medical datasets with limited data availability.
</details>
<details>
<summary>摘要</summary>
急性limephoblastic leukemia（ALL）是儿童血液癌的最常见种类。快速开始治疗过程是患者生存的关键，因此早期诊断这种疾病非常重要。 examine the blood smear images of these patients is one of the methods used by expert doctors to diagnose this disease。 Deep learning-based methods have numerous applications in medical fields, and ALL diagnosis is no exception. However, previous machine learning-based methods for this problem have been criticized for relying too heavily on shortcuts and not making meaningful decisions. This issue arises due to the small size of medical training datasets. To address this, we constrained our model to follow a pipeline inspired by experts' work. We also demonstrated that redefining the problem as a multiple-instance learning problem is necessary for achieving a practical result. Our model is the first to provide a solution to this problem in a multiple-instance learning setup. We introduced a novel pipeline for diagnosing ALL that approximates the process used by hematologists, is sensitive to disease biomarkers, and achieves an accuracy of 96.15%, an F1-score of 94.24%, a sensitivity of 97.56%, and a specificity of 90.91% on ALL IDB 1. Our method was further evaluated on an out-of-distribution dataset, which posed a challenging test and had acceptable performance. Notably, our model was trained on a relatively small dataset, highlighting the potential for our approach to be applied to other medical datasets with limited data availability.
</details></li>
</ul>
<hr>
<h2 id="BPNet-Bezier-Primitive-Segmentation-on-3D-Point-Clouds"><a href="#BPNet-Bezier-Primitive-Segmentation-on-3D-Point-Clouds" class="headerlink" title="BPNet: Bézier Primitive Segmentation on 3D Point Clouds"></a>BPNet: Bézier Primitive Segmentation on 3D Point Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04013">http://arxiv.org/abs/2307.04013</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bizerfr/bpnet">https://github.com/bizerfr/bpnet</a></li>
<li>paper_authors: Rao Fu, Cheng Wen, Qian Li, Xiao Xiao, Pierre Alliez</li>
<li>for: 本文提出了BPNet，一种基于深度学习的端到端框架，用于在3D点云上学习B&#39;ezier基本形态分割。现有的方法往往只处理不同的基本类型分割，因此它们只能处理有限的形态类型。为了解决这个问题，我们寻求一种通用的点云基本分割方法。</li>
<li>methods: 我们将B&#39;ezier分解 transferred to point cloud segmentation，并在缓冲 architecture 上进行联合优化来学习B&#39;ezier基本分割和形态适应同时。我们还引入了一个软投票正则化来提高基本分割，并提出了一个自适应嵌入模块来聚合点Cloud特征，使网络更加稳定和通用。</li>
<li>results: 我们在ABC dataset和实际扫描数据集上进行了广泛的实验，并与不同的基准方法进行比较。实验结果表明，我们的方法在基本分割方面具有superior performance，并且在推理速度方面表现出了显著的提高。<details>
<summary>Abstract</summary>
This paper proposes BPNet, a novel end-to-end deep learning framework to learn B\'ezier primitive segmentation on 3D point clouds. The existing works treat different primitive types separately, thus limiting them to finite shape categories. To address this issue, we seek a generalized primitive segmentation on point clouds. Taking inspiration from B\'ezier decomposition on NURBS models, we transfer it to guide point cloud segmentation casting off primitive types. A joint optimization framework is proposed to learn B\'ezier primitive segmentation and geometric fitting simultaneously on a cascaded architecture. Specifically, we introduce a soft voting regularizer to improve primitive segmentation and propose an auto-weight embedding module to cluster point features, making the network more robust and generic. We also introduce a reconstruction module where we successfully process multiple CAD models with different primitives simultaneously. We conducted extensive experiments on the synthetic ABC dataset and real-scan datasets to validate and compare our approach with different baseline methods. Experiments show superior performance over previous work in terms of segmentation, with a substantially faster inference speed.
</details>
<details>
<summary>摘要</summary>
Inspired by B\'ezier decomposition on NURBS models, we transfer this concept to guide point cloud segmentation and eliminate the need for separate treatment of different primitive types. Our approach uses a joint optimization framework to learn B\'ezier primitive segmentation and geometric fitting simultaneously on a cascaded architecture.To improve primitive segmentation, we introduce a soft voting regularizer and an auto-weight embedding module to cluster point features. These components make our network more robust and generic, allowing it to handle a wide range of shapes and scenarios.In addition, we introduce a reconstruction module that enables us to process multiple CAD models with different primitives simultaneously. Our approach is validated through extensive experiments on the synthetic ABC dataset and real-scan datasets, demonstrating superior performance over previous methods in terms of segmentation and inference speed.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/09/cs.CV_2023_07_09/" data-id="closbrons00es0g885i12bvj4" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_07_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/09/cs.AI_2023_07_09/" class="article-date">
  <time datetime="2023-07-09T12:00:00.000Z" itemprop="datePublished">2023-07-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/09/cs.AI_2023_07_09/">cs.AI - 2023-07-09</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="On-the-Challenges-of-Deploying-Privacy-Preserving-Synthetic-Data-in-the-Enterprise"><a href="#On-the-Challenges-of-Deploying-Privacy-Preserving-Synthetic-Data-in-the-Enterprise" class="headerlink" title="On the Challenges of Deploying Privacy-Preserving Synthetic Data in the Enterprise"></a>On the Challenges of Deploying Privacy-Preserving Synthetic Data in the Enterprise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04208">http://arxiv.org/abs/2307.04208</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lauren Arthur, Jason Costello, Jonathan Hardy, Will O’Brien, James Rea, Gareth Rees, Georgi Ganev</li>
<li>for: 本研究旨在探讨生成AI技术在企业部署中所遇到的挑战，尤其是由于巨量个人敏感数据的隐私问题。</li>
<li>methods: 本研究系统化了40多个挑战，并将其分为五大类：生成、基础设施与架构、治理、合规与法规、并 adopt。</li>
<li>results: 本研究提出了一种战略和系统的方法，可以帮助企业有效地解决挑战，并在实施解决方案时建立信任。<details>
<summary>Abstract</summary>
Generative AI technologies are gaining unprecedented popularity, causing a mix of excitement and apprehension through their remarkable capabilities. In this paper, we study the challenges associated with deploying synthetic data, a subfield of Generative AI. Our focus centers on enterprise deployment, with an emphasis on privacy concerns caused by the vast amount of personal and highly sensitive data. We identify 40+ challenges and systematize them into five main groups -- i) generation, ii) infrastructure & architecture, iii) governance, iv) compliance & regulation, and v) adoption. Additionally, we discuss a strategic and systematic approach that enterprises can employ to effectively address the challenges and achieve their goals by establishing trust in the implemented solutions.
</details>
<details>
<summary>摘要</summary>
<<SYS>>通用AI技术在当前时期得到了历史上无 precedent的普及度，这引发了一种混乱的感觉，同时也带来了一些担忧。在这篇论文中，我们研究了生成数据的投入问题，这是通用AI技术的一个子领域。我们的研究重点在于企业部署，强调个人隐私权和敏感数据的巨大数量所引起的隐私问题。我们识别了40多个挑战，并将它们分为五个主要组：一、生成；二、基础设施与架构；三、管理；四、合规与法规；五、采用。此外，我们还讨论了企业可以采用的策略和系统性的方法，以确保在实施解决方案时建立信任。
</details></li>
</ul>
<hr>
<h2 id="Natural-Language-Instructions-for-Intuitive-Human-Interaction-with-Robotic-Assistants-in-Field-Construction-Work"><a href="#Natural-Language-Instructions-for-Intuitive-Human-Interaction-with-Robotic-Assistants-in-Field-Construction-Work" class="headerlink" title="Natural Language Instructions for Intuitive Human Interaction with Robotic Assistants in Field Construction Work"></a>Natural Language Instructions for Intuitive Human Interaction with Robotic Assistants in Field Construction Work</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04195">http://arxiv.org/abs/2307.04195</a></li>
<li>repo_url: None</li>
<li>paper_authors: Somin Park, Xi Wang, Carol C. Menassa, Vineet R. Kamat, Joyce Y. Chai</li>
<li>For: This paper aims to provide a framework for human workers to interact with construction robots based on natural language instructions, enabling intuitive and familiar communication and improving teamwork and supervision in field construction.* Methods: The proposed method consists of three stages: Natural Language Understanding (NLU), Information Mapping (IM), and Robot Control (RC). The NLU module uses a language model to predict a tag for each word in the input natural language instruction. The IM module generates the final instructional output essential for the robot to acknowledge and perform the construction task, based on the result of the NLU module and building component information.* Results: A case study for drywall installation is conducted to evaluate the proposed approach, and the obtained results highlight the potential of using natural language-based interaction to replicate the communication that occurs between human workers within the context of human-robot teams.<details>
<summary>Abstract</summary>
The introduction of robots is widely considered to have significant potential of alleviating the issues of worker shortage and stagnant productivity that afflict the construction industry. However, it is challenging to use fully automated robots in complex and unstructured construction sites. Human-Robot Collaboration (HRC) has shown promise of combining human workers' flexibility and robot assistants' physical abilities to jointly address the uncertainties inherent in construction work. When introducing HRC in construction, it is critical to recognize the importance of teamwork and supervision in field construction and establish a natural and intuitive communication system for the human workers and robotic assistants. Natural language-based interaction can enable intuitive and familiar communication with robots for human workers who are non-experts in robot programming. However, limited research has been conducted on this topic in construction. This paper proposes a framework to allow human workers to interact with construction robots based on natural language instructions. The proposed method consists of three stages: Natural Language Understanding (NLU), Information Mapping (IM), and Robot Control (RC). Natural language instructions are input to a language model to predict a tag for each word in the NLU module. The IM module uses the result of the NLU module and building component information to generate the final instructional output essential for a robot to acknowledge and perform the construction task. A case study for drywall installation is conducted to evaluate the proposed approach. The obtained results highlight the potential of using natural language-based interaction to replicate the communication that occurs between human workers within the context of human-robot teams.
</details>
<details>
<summary>摘要</summary>
introduce robots 广泛认为可以有效缓解建筑业的工作人员短缺和低效 Productivity 问题。然而，在复杂和无结构的建筑 Site 中使用完全自动化 Robot 很具挑战性。人 robot合作 (HRC) 表现出了将人工作者的灵活性和机器助手的物理能力结合起来解决建筑工作中的不确定性的潜力。在将 HRC 引入建筑时，需要认可场地建筑 Teamwork 和监督的重要性，并建立一个自然和直观的沟通系统，以便人工作者和机器助手之间能够协作。使用自然语言基于的交互可以让人工作者成为不熟悉机器程序的情况下，与机器助手进行直观和熟悉的交互。然而，建筑业中对这个主题的研究较少。这篇论文提出了一种框架，以便人工作者通过自然语言指令与建筑机器人进行交互。该方法包括三个阶段：自然语言理解 (NLU)、信息映射 (IM) 和机器人控制 (RC)。自然语言指令将输入到语言模型中，以便预测每个词的标签。IM模块使用NLU模块的结果和建筑元件信息，生成构建任务所需的最终指令输出，以便机器人认可并执行构建任务。一个关于墙壁安装的实验研究被进行，以评估提议的方法。研究结果表明，使用自然语言基于的交互可以复制人工作者之间的通信，在人机合作团队中。
</details></li>
</ul>
<hr>
<h2 id="SAS-Video-QA-Self-Adaptive-Sampling-for-Efficient-Video-Question-Answering"><a href="#SAS-Video-QA-Self-Adaptive-Sampling-for-Efficient-Video-Question-Answering" class="headerlink" title="SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering"></a>SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04192">http://arxiv.org/abs/2307.04192</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/declare-lab/sas-vqa">https://github.com/declare-lab/sas-vqa</a></li>
<li>paper_authors: Wei Han, Hui Chen, Min-Yen Kan, Soujanya Poria</li>
<li>for: 提高视频理解模型的效果和可靠性，尤其是在实时应用场景中。</li>
<li>methods: 提出了两种框架采样策略：最域幂frames（MDF）和最含义frames（MIF），以最大化保留关键帧。MDF通过循环采样来减少风险，而MIF通过辅助模型来搜索个性化的关键帧。</li>
<li>results: 在三个公共数据集上（CLIP、GIT和All-in-one）进行了实验，结果表明，提出的采样策略可以提高图文预训练模型的性能。<details>
<summary>Abstract</summary>
Video question--answering is a fundamental task in the field of video understanding. Although current vision--language models (VLMs) equipped with Video Transformers have enabled temporal modeling and yielded superior results, they are at the cost of huge computational power and thus too expensive to deploy in real-time application scenarios. An economical workaround only samples a small portion of frames to represent the main content of that video and tune an image--text model on these sampled frames. Recent video understanding models usually randomly sample a set of frames or clips, regardless of internal correlations between their visual contents, nor their relevance to the problem. We argue that such kinds of aimless sampling may omit the key frames from which the correct answer can be deduced, and the situation gets worse when the sampling sparsity increases, which always happens as the video lengths increase. To mitigate this issue, we propose two frame sampling strategies, namely the most domain frames (MDF) and most implied frames (MIF), to maximally preserve those frames that are most likely vital to the given questions. MDF passively minimizes the risk of key frame omission in a bootstrap manner, while MIS actively searches key frames customized for each video--question pair with the assistance of auxiliary models. The experimental results on three public datasets from three advanced VLMs (CLIP, GIT and All-in-one) demonstrate that our proposed strategies can boost the performance for image--text pretrained models. The source codes pertaining to the method proposed in this paper are publicly available at https://github.com/declare-lab/sas-vqa.
</details>
<details>
<summary>摘要</summary>
视频问答是视频理解领域的基本任务。尽管当前的视频语言模型（VLM）搭配视频变换器已经实现了时间模型化，并且获得了更高的性能，但是这些模型却需要很大的计算能力，因此在实时应用场景中成本太高。为了解决这个问题，我们提出了两种帧 sampling 策略：最域帧（MDF）和最含义帧（MIF），以最大化保留关键帧。MDF 采用 bootstrap 方式减少风险 omitted 关键帧，而 MIF 通过辅助模型自动搜索每个视频问题对应的关键帧。我们在三个公共数据集上（CLIP、GIT 和 All-in-one）进行了实验，结果显示，我们提出的策略可以提高图文预训练模型的性能。关于我们提出的方法的源代码，可以在 GitHub 上获取：https://github.com/declare-lab/sas-vqa。
</details></li>
</ul>
<hr>
<h2 id="Review-of-feedback-in-Automated-Essay-Scoring"><a href="#Review-of-feedback-in-Automated-Essay-Scoring" class="headerlink" title="Review of feedback in Automated Essay Scoring"></a>Review of feedback in Automated Essay Scoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05553">http://arxiv.org/abs/2307.05553</a></li>
<li>repo_url: None</li>
<li>paper_authors: You-Jin Jong, Yong-Jin Kim, Ok-Chol Ri</li>
<li>for: 这篇论文主要是为了探讨自动化论文评分系统的发展和其在写作技己提升方面的应用。</li>
<li>methods: 本论文通过审查已有的研究和最新的案例研究，探讨了不同类型的反馈和论文特征在自动化论文评分系统中的应用。</li>
<li>results: 研究发现，反馈是自动化论文评分系统的关键因素，可以帮助用户提升写作技己。<details>
<summary>Abstract</summary>
The first automated essay scoring system was developed 50 years ago. Automated essay scoring systems are developing into systems with richer functions than the previous simple scoring systems. Its purpose is not only to score essays but also as a learning tool to improve the writing skill of users. Feedback is the most important aspect of making an automated essay scoring system useful in real life. The importance of feedback was already emphasized in the first AES system. This paper reviews research on feedback including different feedback types and essay traits on automated essay scoring. We also reviewed the latest case studies of the automated essay scoring system that provides feedback.
</details>
<details>
<summary>摘要</summary>
50 年前开发出了第一个自动化评分系统，现在自动化评分系统在功能上不断提高，不再是简单的评分系统。它的目的不只是评分文章，更重要的是作为学习工具，帮助用户提高写作技巧。回馈是自动化评分系统在实际应用中最重要的一部分。这篇文章检视了不同类型的回馈和文章特征在自动化评分系统中的应用，同时也检视了最新的案例研究。
</details></li>
</ul>
<hr>
<h2 id="Latent-Graph-Attention-for-Enhanced-Spatial-Context"><a href="#Latent-Graph-Attention-for-Enhanced-Spatial-Context" class="headerlink" title="Latent Graph Attention for Enhanced Spatial Context"></a>Latent Graph Attention for Enhanced Spatial Context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04149">http://arxiv.org/abs/2307.04149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ayush Singh, Yash Bhambhu, Himanshu Buckchash, Deepak K. Gupta, Dilip K. Prasad</li>
<li>for: 这个论文的目的是提出一种 computationally inexpensive 和稳定的全局 контекст模型，用于提高图像转换 зада务中的性能。</li>
<li>methods: 该论文使用了一种名为 Latent Graph Attention (LGA) 的模型，它利用一种分布式图像网络来传递信息，并通过自适应的深度设置来控制全局 контекст的扩展。</li>
<li>results: 该论文通过在三个复杂的应用中（透明物体分割、雾度修复和光流估计）的实验，表明 incorporating LGA 可以提高性能，而且可以使小型结构更接近大型结构的性能。<details>
<summary>Abstract</summary>
Global contexts in images are quite valuable in image-to-image translation problems. Conventional attention-based and graph-based models capture the global context to a large extent, however, these are computationally expensive. Moreover, the existing approaches are limited to only learning the pairwise semantic relation between any two points on the image. In this paper, we present Latent Graph Attention (LGA) a computationally inexpensive (linear to the number of nodes) and stable, modular framework for incorporating the global context in the existing architectures, especially empowering small-scale architectures to give performance closer to large size architectures, thus making the light-weight architectures more useful for edge devices with lower compute power and lower energy needs. LGA propagates information spatially using a network of locally connected graphs, thereby facilitating to construct a semantically coherent relation between any two spatially distant points that also takes into account the influence of the intermediate pixels. Moreover, the depth of the graph network can be used to adapt the extent of contextual spread to the target dataset, thereby being able to explicitly control the added computational cost. To enhance the learning mechanism of LGA, we also introduce a novel contrastive loss term that helps our LGA module to couple well with the original architecture at the expense of minimal additional computational load. We show that incorporating LGA improves the performance on three challenging applications, namely transparent object segmentation, image restoration for dehazing and optical flow estimation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Survey-and-Approach-to-Chart-Classification"><a href="#A-Survey-and-Approach-to-Chart-Classification" class="headerlink" title="A Survey and Approach to Chart Classification"></a>A Survey and Approach to Chart Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04147">http://arxiv.org/abs/2307.04147</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anurag Dhote, Mohammed Javed, David S Doermann</li>
<li>for: 这篇论文主要针对的是自动化图表分类问题，即通过分类图表的类型和特征来理解图表中含义的问题。</li>
<li>methods: 这篇论文主要介绍了现有的图表分类技术，包括传统的机器学习方法、卷积神经网络和变换器等。</li>
<li>results: 论文中提出了一种基于视觉变换器的图表分类模型，并在CHARTINFO UB-UNITECH PMC数据集上进行了比较性表现分析，实现了图表分类领域的状态机器。<details>
<summary>Abstract</summary>
Charts represent an essential source of visual information in documents and facilitate a deep understanding and interpretation of information typically conveyed numerically. In the scientific literature, there are many charts, each with its stylistic differences. Recently the document understanding community has begun to address the problem of automatic chart understanding, which begins with chart classification. In this paper, we present a survey of the current state-of-the-art techniques for chart classification and discuss the available datasets and their supported chart types. We broadly classify these contributions as traditional approaches based on ML, CNN, and Transformers. Furthermore, we carry out an extensive comparative performance analysis of CNN-based and transformer-based approaches on the recently published CHARTINFO UB-UNITECH PMC dataset for the CHART-Infographics competition at ICPR 2022. The data set includes 15 different chart categories, including 22,923 training images and 13,260 test images. We have implemented a vision-based transformer model that produces state-of-the-art results in chart classification.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Emotion-Analysis-on-EEG-Signal-Using-Machine-Learning-and-Neural-Network"><a href="#Emotion-Analysis-on-EEG-Signal-Using-Machine-Learning-and-Neural-Network" class="headerlink" title="Emotion Analysis on EEG Signal Using Machine Learning and Neural Network"></a>Emotion Analysis on EEG Signal Using Machine Learning and Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05375">http://arxiv.org/abs/2307.05375</a></li>
<li>repo_url: None</li>
<li>paper_authors: S. M. Masrur Ahmed, Eshaan Tanzim Sabur</li>
<li>for: 本研究的目的是提高使用脑波识别情绪的性能。</li>
<li>methods: 本研究使用了EEG信号处理技术和人工神经网络模型，包括SVM、KNN和RNN等，以提高情绪识别性能。</li>
<li>results: 研究在DEAP数据集上进行了多种情绪状态的分类和测试，并获得了较高的识别精度。<details>
<summary>Abstract</summary>
Emotion has a significant influence on how one thinks and interacts with others. It serves as a link between how a person feels and the actions one takes, or it could be said that it influences one's life decisions on occasion. Since the patterns of emotions and their reflections vary from person to person, their inquiry must be based on approaches that are effective over a wide range of population regions. To extract features and enhance accuracy, emotion recognition using brain waves or EEG signals requires the implementation of efficient signal processing techniques. Various approaches to human-machine interaction technologies have been ongoing for a long time, and in recent years, researchers have had great success in automatically understanding emotion using brain signals. In our research, several emotional states were classified and tested on EEG signals collected from a well-known publicly available dataset, the DEAP Dataset, using SVM (Support Vector Machine), KNN (K-Nearest Neighbor), and an advanced neural network model, RNN (Recurrent Neural Network), trained with LSTM (Long Short Term Memory). The main purpose of this study is to improve ways to improve emotion recognition performance using brain signals. Emotions, on the other hand, can change with time. As a result, the changes in emotion over time are also examined in our research.
</details>
<details>
<summary>摘要</summary>
感情对人们的思维和社交互动产生重要影响。它是人们的情感和行为之间的联系，也可以说是影响人们的生活决策的一种因素。由于人们的情感模式和表达方式不同，因此对于不同人群的情感识别需要采用有效的方法。为了提取特征和提高准确性，使用大脑电声信号进行情感识别需要实施有效的信号处理技术。在人机交互技术方面，研究人员已经在过去几年里取得了很大的成功，通过自动理解大脑电声信号来识别情感。在我们的研究中，我们使用了DEAP数据集，使用SVM、KNN和RNN（长短期 памя存）模型，并在LSTM（长短期 памя存）模型中进行训练，以提高情感识别性能。本研究的主要目标是提高情感识别性能使用大脑电声信号。同时，我们还对情感的变化考查了时间的影响。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Explainable-Artificial-Intelligence-Model-in-Image-Classification-problem"><a href="#A-Novel-Explainable-Artificial-Intelligence-Model-in-Image-Classification-problem" class="headerlink" title="A Novel Explainable Artificial Intelligence Model in Image Classification problem"></a>A Novel Explainable Artificial Intelligence Model in Image Classification problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04137">http://arxiv.org/abs/2307.04137</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quoc Hung Cao, Truong Thanh Hung Nguyen, Vo Thanh Khang Nguyen, Xuan Phong Nguyen</li>
<li>for: 本研究旨在提供一种新的图像分类模型解释方法，以帮助AI科学家和实际应用者更深入地理解模型内部的工作机制。</li>
<li>methods: 本研究使用了LIME、CAM和GradCAM等多种现有的解释算法，并将其综合使用以提高解释效果。同时，本方法还实现了提高解释效果的时间和空间约束。</li>
<li>results: 对于ILSVRC数据集中的多种图像分类模型，包括ResNet50、Inception-v3和VGG16等，本方法在准确率和解释效果两个方面均取得了出色的结果。<details>
<summary>Abstract</summary>
In recent years, artificial intelligence is increasingly being applied widely in many different fields and has a profound and direct impact on human life. Following this is the need to understand the principles of the model making predictions. Since most of the current high-precision models are black boxes, neither the AI scientist nor the end-user deeply understands what's going on inside these models. Therefore, many algorithms are studied for the purpose of explaining AI models, especially those in the problem of image classification in the field of computer vision such as LIME, CAM, GradCAM. However, these algorithms still have limitations such as LIME's long execution time and CAM's confusing interpretation of concreteness and clarity. Therefore, in this paper, we propose a new method called Segmentation - Class Activation Mapping (SeCAM) that combines the advantages of these algorithms above, while at the same time overcoming their disadvantages. We tested this algorithm with various models, including ResNet50, Inception-v3, VGG16 from ImageNet Large Scale Visual Recognition Challenge (ILSVRC) data set. Outstanding results when the algorithm has met all the requirements for a specific explanation in a remarkably concise time.
</details>
<details>
<summary>摘要</summary>
Recently,人工智能在各个领域广泛应用，对人类生活产生深远的影响。随着模型预测的需求，需要理解模型的原理。然而，现有的高精度模型大多是黑obox，科学家和用户都无法深入了解模型内部的工作原理。为此，许多算法被研究以解释AI模型，特别是计算视觉领域的图像分类问题中的LIME、CAM和GradCAM等算法。然而，这些算法仍有局限性，如LIME的执行时间过长和CAM的抽象和明确性的含糊不清。因此，在本文中，我们提出了一种新的方法 called Segmentation - Class Activation Mapping（SeCAM），该方法结合了上述算法的优点，同时超越了它们的缺点。我们在不同的模型，包括ResNet50、Inception-v3和VGG16等，在ImageNet大规模视觉识别挑战（ILSVRC）数据集上进行了测试，结果很出色，可以快速和准确地解释特定的模型预测结果。
</details></li>
</ul>
<hr>
<h2 id="Reasoning-over-the-Behaviour-of-Objects-in-Video-Clips-for-Adverb-Type-Recognition"><a href="#Reasoning-over-the-Behaviour-of-Objects-in-Video-Clips-for-Adverb-Type-Recognition" class="headerlink" title="Reasoning over the Behaviour of Objects in Video-Clips for Adverb-Type Recognition"></a>Reasoning over the Behaviour of Objects in Video-Clips for Adverb-Type Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04132">http://arxiv.org/abs/2307.04132</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amrit Diggavi Seshadri, Alessandra Russo</li>
<li>for: 本研究旨在Recognize scene adverbs from raw video clips, without assuming knowledge of the clips’ underlying action types.</li>
<li>methods: 提议一个新的框架，利用对 Raw video clips 中对象行为的抽象来认识clip的相应adverb-types。该框架包括一个新的数据采集管道和一种基于符号和转换器的推理方法。</li>
<li>results: 实验结果表明，提议的方法可以与之前的状态OF-THE-ART技术进行比较，而且支持符号视频处理的努力。此外，我们还发布了两个新的数据集，以支持符号视频处理：MSR-VTT-ASP和ActivityNet-ASP数据集。<details>
<summary>Abstract</summary>
In this work, following the intuition that adverbs describing scene-sequences are best identified by reasoning over high-level concepts of object-behavior, we propose the design of a new framework that reasons over object-behaviours extracted from raw-video-clips to recognize the clip's corresponding adverb-types. Importantly, while previous works for general scene adverb-recognition assume knowledge of the clips underlying action-types, our method is directly applicable in the more general problem setting where the action-type of a video-clip is unknown. Specifically, we propose a novel pipeline that extracts human-interpretable object-behaviour-facts from raw video clips and propose novel symbolic and transformer based reasoning methods that operate over these extracted facts to identify adverb-types. Experiment results demonstrate that our proposed methods perform favourably against the previous state-of-the-art. Additionally, to support efforts in symbolic video-processing, we release two new datasets of object-behaviour-facts extracted from raw video clips - the MSR-VTT-ASP and ActivityNet-ASP datasets.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们采用直觉，认为Scene-sequences中的副词可以通过对物体行为高级概念的逻辑来识别。因此，我们提出了一种新的框架，可以从 raw-video-clip 中提取 object-behaviour-facts，并使用符号和变换器来进行逻辑和计算来识别 adverb-types。与前一些Scene adverb-recognition工作不同，我们的方法不需要知道clip的 action-type。具体来说，我们提出了一个新的管道，可以从 raw video clips 中提取人类可解释的 object-behaviour-facts，并提出了一种新的符号和变换器来进行逻辑和计算来识别 adverb-types。实验结果表明，我们的提出的方法在比较prevailing state-of-the-art方法之上表现出色。此外，为支持符号视频处理，我们释放了两个新的对象行为事实数据集 - MSR-VTT-ASP 和 ActivityNet-ASP 数据集。
</details></li>
</ul>
<hr>
<h2 id="Carbon-Efficient-Neural-Architecture-Search"><a href="#Carbon-Efficient-Neural-Architecture-Search" class="headerlink" title="Carbon-Efficient Neural Architecture Search"></a>Carbon-Efficient Neural Architecture Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04131">http://arxiv.org/abs/2307.04131</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiyang Zhao, Tian Guo</li>
<li>for: 降低神经网络设计过程中的能耗成本和碳负担</li>
<li>methods: 提出了一种基于碳效率的神经建筑搜索方法（CE-NAS），包括碳效率评估算法、多目标优化器和启发式GPU分配策略</li>
<li>results: 使用最新的NASbenchmark数据集和两个碳轨迹进行跟踪驱动的模拟结果显示，CE-NAS在碳负担和搜索效率方面比基eline三个基线更好<details>
<summary>Abstract</summary>
This work presents a novel approach to neural architecture search (NAS) that aims to reduce energy costs and increase carbon efficiency during the model design process. The proposed framework, called carbon-efficient NAS (CE-NAS), consists of NAS evaluation algorithms with different energy requirements, a multi-objective optimizer, and a heuristic GPU allocation strategy. CE-NAS dynamically balances energy-efficient sampling and energy-consuming evaluation tasks based on current carbon emissions. Using a recent NAS benchmark dataset and two carbon traces, our trace-driven simulations demonstrate that CE-NAS achieves better carbon and search efficiency than the three baselines.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="FILM-How-can-Few-Shot-Image-Classification-Benefit-from-Pre-Trained-Language-Models"><a href="#FILM-How-can-Few-Shot-Image-Classification-Benefit-from-Pre-Trained-Language-Models" class="headerlink" title="FILM: How can Few-Shot Image Classification Benefit from Pre-Trained Language Models?"></a>FILM: How can Few-Shot Image Classification Benefit from Pre-Trained Language Models?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04114">http://arxiv.org/abs/2307.04114</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Jiang, Yunkai Dang, Dong Pang, Huishuai Zhang, Weiran Huang</li>
<li>for: 增强少样本学习的方法，使模型可以通过少量样本进行泛化。</li>
<li>methods: 使用预训练语言模型，基于对比学习来提取 semantic information，并通过metric模块进行对visual特征和文本嵌入的Alignment。</li>
<li>results: 通过对多个 benchmark 进行广泛的实验，证明了我们的方法的有效性。<details>
<summary>Abstract</summary>
Few-shot learning aims to train models that can be generalized to novel classes with only a few samples. Recently, a line of works are proposed to enhance few-shot learning with accessible semantic information from class names. However, these works focus on improving existing modules such as visual prototypes and feature extractors of the standard few-shot learning framework. This limits the full potential use of semantic information. In this paper, we propose a novel few-shot learning framework that uses pre-trained language models based on contrastive learning. To address the challenge of alignment between visual features and textual embeddings obtained from text-based pre-trained language model, we carefully design the textual branch of our framework and introduce a metric module to generalize the cosine similarity. For better transferability, we let the metric module adapt to different few-shot tasks and adopt MAML to train the model via bi-level optimization. Moreover, we conduct extensive experiments on multiple benchmarks to demonstrate the effectiveness of our method.
</details>
<details>
<summary>摘要</summary>
少量学习目标是训练模型能够通过少量样本泛化到新类。最近，一些工作提出了使用可 accessible  semantic information from class names 进行增强少量学习。然而，这些工作通常是修改标准少量学习框架中的视觉原型和特征提取器。这限制了semantic information的全部潜力。在本文中，我们提出了一种新的少量学习框架，使用基于对比学习的预训练语言模型。为了解决视觉特征和文本嵌入获得自文本预训练语言模型之间的对应挑战，我们在文本分支中进行了精心的设计，并引入了一个度量模块来泛化cosine相似性。为了提高trasferability，我们让度量模块适应不同的少量任务，并采用MAML来训练模型viabi-level优化。此外，我们在多个benchmark上进行了广泛的实验，以证明我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="A-User-Study-on-Explainable-Online-Reinforcement-Learning-for-Adaptive-Systems"><a href="#A-User-Study-on-Explainable-Online-Reinforcement-Learning-for-Adaptive-Systems" class="headerlink" title="A User Study on Explainable Online Reinforcement Learning for Adaptive Systems"></a>A User Study on Explainable Online Reinforcement Learning for Adaptive Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04098">http://arxiv.org/abs/2307.04098</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andreas Metzger, Jan Laufer, Felix Feit, Klaus Pohl</li>
<li>For: The paper aims to evaluate the effectiveness and usability of an explainable reinforcement learning technique (XRL-DINE) for software engineers to understand and debug adaptive systems.* Methods: The paper uses an empirical user study involving 54 software engineers to assess the performance of software engineers when performing different tasks using XRL-DINE, and to evaluate the perceived usefulness and ease of use of XRL-DINE.* Results: The study finds that XRL-DINE provides visual insights into why certain decisions were made at important time points, and that software engineers perceive XRL-DINE as useful and easy to use.<details>
<summary>Abstract</summary>
Online reinforcement learning (RL) is increasingly used for realizing adaptive systems in the presence of design time uncertainty. Online RL facilitates learning from actual operational data and thereby leverages feedback only available at runtime. However, Online RL requires the definition of an effective and correct reward function, which quantifies the feedback to the RL algorithm and thereby guides learning. With Deep RL gaining interest, the learned knowledge is no longer explicitly represented, but is represented as a neural network. For a human, it becomes practically impossible to relate the parametrization of the neural network to concrete RL decisions. Deep RL thus essentially appears as a black box, which severely limits the debugging of adaptive systems. We previously introduced the explainable RL technique XRL-DINE, which provides visual insights into why certain decisions were made at important time points. Here, we introduce an empirical user study involving 54 software engineers from academia and industry to assess (1) the performance of software engineers when performing different tasks using XRL-DINE and (2) the perceived usefulness and ease of use of XRL-DINE.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DebateKG-Automatic-Policy-Debate-Case-Creation-with-Semantic-Knowledge-Graphs"><a href="#DebateKG-Automatic-Policy-Debate-Case-Creation-with-Semantic-Knowledge-Graphs" class="headerlink" title="DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge Graphs"></a>DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04090">http://arxiv.org/abs/2307.04090</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hellisotherpeople/debatekg">https://github.com/hellisotherpeople/debatekg</a></li>
<li>paper_authors: Allen Roush</li>
<li>for: 本研究的目的是使用自然语言处理系统解决竞赛辩论中的问题，特别是构建高质量辩论案例。</li>
<li>methods: 本研究使用受限短 PATH 搜索在Argumentative Semantic Knowledge Graphs上进行了实现。</li>
<li>results: 研究人员在Policy Debate中的一种美国竞赛辩论中，使用这种方法可以大幅提高DebateSum数据集的质量，并且开发了一种新的评价方法来评估不同的知识图。<details>
<summary>Abstract</summary>
Recent work within the Argument Mining community has shown the applicability of Natural Language Processing systems for solving problems found within competitive debate. One of the most important tasks within competitive debate is for debaters to create high quality debate cases. We show that effective debate cases can be constructed using constrained shortest path traversals on Argumentative Semantic Knowledge Graphs. We study this potential in the context of a type of American Competitive Debate, called Policy Debate, which already has a large scale dataset targeting it called DebateSum. We significantly improve upon DebateSum by introducing 53180 new examples, as well as further useful metadata for every example, to the dataset. We leverage the txtai semantic search and knowledge graph toolchain to produce and contribute 9 semantic knowledge graphs built on this dataset. We create a unique method for evaluating which knowledge graphs are better in the context of producing policy debate cases. A demo which automatically generates debate cases, along with all other code and the Knowledge Graphs, are open-sourced and made available to the public here: https://github.com/Hellisotherpeople/DebateKG
</details>
<details>
<summary>摘要</summary>
近期在论证挖掘社区中的工作表明，自然语言处理系统可以解决竞论中的问题。竞论中最重要的任务之一是论者创造高质量辩论案例。我们表明，使用限制最短路径搜索的方法可以构建有效的辩论案例。我们在美国竞论中的一种类型，即政策辩论，已经有了大规模的数据集，即DebateSum。我们在DebateSum上进行了大幅改进，并将53180个新的例子和每个例子的更多有用的元数据添加到数据集中。我们利用txtai的 semantic search和知识图工具链来生成和投入9个基于这个数据集的Semantic Knowledge Graph。我们还创造了一种用于评估知识图的评价方法。一个可以自动生成辩论案例的 demo，以及所有代码和知识图，都被开源并公开提供于公众，可以在以下链接中找到：https://github.com/Hellisotherpeople/DebateKG。
</details></li>
</ul>
<hr>
<h2 id="Semi-Supervised-Meta-Learning-for-Spatiotemporal-Learning"><a href="#Semi-Supervised-Meta-Learning-for-Spatiotemporal-Learning" class="headerlink" title="Semi Supervised Meta Learning for Spatiotemporal Learning"></a>Semi Supervised Meta Learning for Spatiotemporal Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01916">http://arxiv.org/abs/2308.01916</a></li>
<li>repo_url: None</li>
<li>paper_authors: Faraz Waseem, Pratyush Muthukumar</li>
<li>for: 这个论文的目的是应用元学习到自动编码器中进行空间时间学习。</li>
<li>methods: 这个论文使用的方法是将元学习搅入现有的状态艺术架构中。他们通过三个步骤来实现这一目标：首先，他们使用隐藏状态搅入网络（MANN）架构来应用元学习到他们的小规模空间时间数据集中进行视频重建任务。其次，他们使用一个预训练的MAE编码器并在其上添加一个分类头进行动作分类任务。最后，他们使用一个预训练的MAE编码器并与Mann架构结合来进行动作分类任务。</li>
<li>results: 这个论文的结果表明，通过应用元学习到现有的状态艺术架构中，可以提高空间时间学习的性能。<details>
<summary>Abstract</summary>
We approached the goal of applying meta-learning to self-supervised masked autoencoders for spatiotemporal learning in three steps. Broadly, we seek to understand the impact of applying meta-learning to existing state-of-the-art representation learning architectures. Thus, we test spatiotemporal learning through: a meta-learning architecture only, a representation learning architecture only, and an architecture applying representation learning alongside a meta learning architecture. We utilize the Memory Augmented Neural Network (MANN) architecture to apply meta-learning to our framework. Specifically, we first experiment with applying a pre-trained MAE and fine-tuning on our small-scale spatiotemporal dataset for video reconstruction tasks. Next, we experiment with training an MAE encoder and applying a classification head for action classification tasks. Finally, we experiment with applying a pre-trained MAE and fine-tune with MANN backbone for action classification tasks.
</details>
<details>
<summary>摘要</summary>
我们通过三步来应用meta学到自我超visedMasked autoencoders中的空间时间学习。大致来说，我们想要了解将meta学应用到现有的状态艺术 repreentation learning架构中的影响。因此，我们通过以下三种方式进行测试：1. 仅使用meta学架构，2. 仅使用 repreentation learning架构，3. 将 repreentation learning架构与meta学架构相结合。我们使用Memory Augmented Neural Network（MANN）架构来应用meta学到我们的小规模空间时间数据集中。具体来说，我们首先试验使用预训练的MAE和我们小规模空间时间数据集进行视频重建任务。然后，我们试验将MAE编码器训练并应用分类头进行动作分类任务。最后，我们试验使用预训练的MAE和Mann架构进行动作分类任务。
</details></li>
</ul>
<hr>
<h2 id="Disentangling-Societal-Inequality-from-Model-Biases-Gender-Inequality-in-Divorce-Court-Proceedings"><a href="#Disentangling-Societal-Inequality-from-Model-Biases-Gender-Inequality-in-Divorce-Court-Proceedings" class="headerlink" title="Disentangling Societal Inequality from Model Biases: Gender Inequality in Divorce Court Proceedings"></a>Disentangling Societal Inequality from Model Biases: Gender Inequality in Divorce Court Proceedings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10200">http://arxiv.org/abs/2307.10200</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sujan Dutta, Parth Srivastava, Vaishnavi Solunke, Swaprava Nath, Ashiqur R. KhudaBukhsh</li>
<li>For: This paper uses a large corpus of court proceedings to investigate gender inequality in the context of divorce in India.* Methods: The authors use natural language processing (NLP) techniques to analyze the court proceedings and quantify societal inequalities. They also modify existing NLP resources to better suit their research goals.* Results: The authors find that while there may be changing norms in India with more women challenging patriarchy, there is still striking gender inequality in the context of divorce, with women often experiencing domestic violence.Here’s the same information in Simplified Chinese text:* For: 这篇论文通过废除婚姻的法律分裂的法院记录，调查印度妇女在婚姻中的不平等。* Methods: 作者使用自然语言处理（NLP）技术分析法院记录，量化社会不平等。他们还对现有的NLP资源进行了一些修改，以更好地适应他们的研究目标。* Results: 作者发现，尽管印度可能存在变革，但在废除婚姻中，女性仍然面临着家庭暴力的问题，是一种 striking 的性别不平等。<details>
<summary>Abstract</summary>
Divorce is the legal dissolution of a marriage by a court. Since this is usually an unpleasant outcome of a marital union, each party may have reasons to call the decision to quit which is generally documented in detail in the court proceedings. Via a substantial corpus of 17,306 court proceedings, this paper investigates gender inequality through the lens of divorce court proceedings. While emerging data sources (e.g., public court records) on sensitive societal issues hold promise in aiding social science research, biases present in cutting-edge natural language processing (NLP) methods may interfere with or affect such studies. We thus require a thorough analysis of potential gaps and limitations present in extant NLP resources. In this paper, on the methodological side, we demonstrate that existing NLP resources required several non-trivial modifications to quantify societal inequalities. On the substantive side, we find that while a large number of court cases perhaps suggest changing norms in India where women are increasingly challenging patriarchy, AI-powered analyses of these court proceedings indicate striking gender inequality with women often subjected to domestic violence.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Personalized-Reinforcement-Learning-Summarization-Service-for-Learning-Structure-from-Unstructured-Data"><a href="#A-Personalized-Reinforcement-Learning-Summarization-Service-for-Learning-Structure-from-Unstructured-Data" class="headerlink" title="A Personalized Reinforcement Learning Summarization Service for Learning Structure from Unstructured Data"></a>A Personalized Reinforcement Learning Summarization Service for Learning Structure from Unstructured Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05696">http://arxiv.org/abs/2307.05696</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samira Ghodratnama, Amin Beheshti, Mehrdad Zakershahrak</li>
<li>for: 提供个性化文摘服务，帮助用户从大量文档中提取有意义的信息。</li>
<li>methods: 使用强化学习算法生成个性化文摘，并将文摘映射到一个层次结构中，以便用户更好地理解和浏览文档。</li>
<li>results: 提高了用户的理解和 Navigation 能力，帮助用户从文档中提取有意义的信息。<details>
<summary>Abstract</summary>
The exponential growth of textual data has created a crucial need for tools that assist users in extracting meaningful insights. Traditional document summarization approaches often fail to meet individual user requirements and lack structure for efficient information processing. To address these limitations, we propose Summation, a hierarchical personalized concept-based summarization approach. It synthesizes documents into a concise hierarchical concept map and actively engages users by learning and adapting to their preferences. Using a Reinforcement Learning algorithm, Summation generates personalized summaries for unseen documents on specific topics. This framework enhances comprehension, enables effective navigation, and empowers users to extract meaningful insights from large document collections aligned with their unique requirements.
</details>
<details>
<summary>摘要</summary>
文本数据的指数增长带来了提取有意义信息的重要需求。传统文摘方法frequently fail to meet individual user requirements and lack structure for efficient information processing. To address these limitations, we propose Summation, a hierarchical personalized concept-based summarization approach. It synthesizes documents into a concise hierarchical concept map and actively engages users by learning and adapting to their preferences. Using a Reinforcement Learning algorithm, Summation generates personalized summaries for unseen documents on specific topics. This framework enhances comprehension, enables effective navigation, and empowers users to extract meaningful insights from large document collections aligned with their unique requirements.Here's a word-for-word translation of the text into Simplified Chinese:文本数据的指数增长带来了提取有意义信息的重要需求。传统文摘方法frequently fail to meet individual user requirements and lack structure for efficient information processing. To address these limitations, we propose Summation, a hierarchical personalized concept-based summarization approach. It synthesizes documents into a concise hierarchical concept map and actively engages users by learning and adapting to their preferences. Using a Reinforcement Learning algorithm, Summation generates personalized summaries for unseen documents on specific topics. This framework enhances comprehension, enables effective navigation, and empowers users to extract meaningful insights from large document collections aligned with their unique requirements.
</details></li>
</ul>
<hr>
<h2 id="Multi-Head-Attention-Mechanism-Learning-for-Cancer-New-Subtypes-and-Treatment-Based-on-Cancer-Multi-Omics-Data"><a href="#Multi-Head-Attention-Mechanism-Learning-for-Cancer-New-Subtypes-and-Treatment-Based-on-Cancer-Multi-Omics-Data" class="headerlink" title="Multi-Head Attention Mechanism Learning for Cancer New Subtypes and Treatment Based on Cancer Multi-Omics Data"></a>Multi-Head Attention Mechanism Learning for Cancer New Subtypes and Treatment Based on Cancer Multi-Omics Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04075">http://arxiv.org/abs/2307.04075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liangrui Pan, Dazhen Liu, Yutao Dou, Lian Wang, Zhichao Feng, Pengfei Rong, Liwen Xu, Shaoliang Peng<br>for:The paper aims to identify and characterize cancer subtypes using unsupervised contrastive learning on multi-omics data.methods:The proposed method uses a generalization framework based on attention mechanisms for unsupervised contrastive learning (AMUCL), which includes a decoupled contrastive learning model (DMACL) based on a multi-head attention mechanism to deeply extract multi-omics data features and identify new cancer subtypes.results:The DMACL model achieved the most reliable cancer subtype clustering results on a single-cell multi-omics dataset and a cancer multi-omics dataset, with a C-index of 0.002, a Silhouette score of 0.801, and a Davies Bouldin Score of 0.38 on the single-cell dataset, and a C-index of 0.016, a Silhouette score of 0.688, and a Davies Bouldin Score of 0.46 on the cancer dataset. The results also revealed six cancer subtypes of AML, which were validated through GO functional enrichment, subtype-specific biological functions, and GSEA.<details>
<summary>Abstract</summary>
Due to the high heterogeneity and clinical characteristics of cancer, there are significant differences in multi-omics data and clinical features among subtypes of different cancers. Therefore, the identification and discovery of cancer subtypes are crucial for the diagnosis, treatment, and prognosis of cancer. In this study, we proposed a generalization framework based on attention mechanisms for unsupervised contrastive learning (AMUCL) to analyze cancer multi-omics data for the identification and characterization of cancer subtypes. AMUCL framework includes a unsupervised multi-head attention mechanism, which deeply extracts multi-omics data features. Importantly, a decoupled contrastive learning model (DMACL) based on a multi-head attention mechanism is proposed to learn multi-omics data features and clusters and identify new cancer subtypes. This unsupervised contrastive learning method clusters subtypes by calculating the similarity between samples in the feature space and sample space of multi-omics data. Compared to 11 other deep learning models, the DMACL model achieved a C-index of 0.002, a Silhouette score of 0.801, and a Davies Bouldin Score of 0.38 on a single-cell multi-omics dataset. On a cancer multi-omics dataset, the DMACL model obtained a C-index of 0.016, a Silhouette score of 0.688, and a Davies Bouldin Score of 0.46, and obtained the most reliable cancer subtype clustering results for each type of cancer. Finally, we used the DMACL model in the AMUCL framework to reveal six cancer subtypes of AML. By analyzing the GO functional enrichment, subtype-specific biological functions, and GSEA of AML, we further enhanced the interpretability of cancer subtype analysis based on the generalizable AMUCL framework.
</details>
<details>
<summary>摘要</summary>
因为癌症的高度多样性和临床特征，不同类型的癌症在多Omics数据和临床特征上存在显著的差异。因此，癌症类型的识别和描述是诊断、治疗和预 afterwards 的关键。在本研究中，我们提出了基于注意力机制的通用泛化学习（AMUCL）框架，用于分析癌症多Omics数据，以识别和描述癌症类型。AMUCL框架包括一种多头注意力机制，深度提取多Omics数据特征。另外，基于多头注意力机制的异步对比学习模型（DMACL）被提出，以学习多Omics数据特征和群集，并Identify新的癌症类型。这种无监督对比学习方法，通过计算样本在特征空间和样本空间的相似度，对癌症类型进行归类。与11种深度学习模型相比，DMACL模型在单个细胞多Omics数据集上达到了C指数0.002，Silhouette分数0.801和Davies Bouldin分数0.38。在癌症多Omics数据集上，DMACL模型获得了C指数0.016，Silhouette分数0.688和Davies Bouldin分数0.46，并获得了每种癌症类型的最可靠归类结果。最后，我们使用DMACL模型在AMUCL框架中，揭示了急性骨髓癌（AML）中的六种癌症类型。通过分析GO功能强化、癌症类型特有的生物功能和GSEA，我们进一步增强了癌症类型分析的可读性，基于通用的AMUCL框架。
</details></li>
</ul>
<hr>
<h2 id="Contextual-Dynamic-Pricing-with-Strategic-Buyers"><a href="#Contextual-Dynamic-Pricing-with-Strategic-Buyers" class="headerlink" title="Contextual Dynamic Pricing with Strategic Buyers"></a>Contextual Dynamic Pricing with Strategic Buyers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04055">http://arxiv.org/abs/2307.04055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pangpang Liu, Zhuoran Yang, Zhaoran Wang, Will Wei Sun</li>
<li>for: 本研究旨在解决在优化销售价格时，买家可以通过 manipulate 自己的特征数据来获得更低的价格，这会增加销售商的损失。</li>
<li>methods: 本研究使用了 contextual dynamic pricing 问题，即销售商不知道买家真实的特征，只能根据买家提供的报告来决定价格。同时，销售商也不知道买家对产品的评价，只能根据买家的回答来判断销售成功或失败。</li>
<li>results: 研究人员提出了一种策略性动态价格策略，可以考虑到买家的策略行为，以 maximize 销售商的总收入。此策略不仅不比随机价格策略差，而且可以同时考虑到买家的策略行为和不确定的 manipulate 成本。实验结果表明，这种策略可以与其他不考虑买家策略行为的价格策略相比，具有更高的效果。<details>
<summary>Abstract</summary>
Personalized pricing, which involves tailoring prices based on individual characteristics, is commonly used by firms to implement a consumer-specific pricing policy. In this process, buyers can also strategically manipulate their feature data to obtain a lower price, incurring certain manipulation costs. Such strategic behavior can hinder firms from maximizing their profits. In this paper, we study the contextual dynamic pricing problem with strategic buyers. The seller does not observe the buyer's true feature, but a manipulated feature according to buyers' strategic behavior. In addition, the seller does not observe the buyers' valuation of the product, but only a binary response indicating whether a sale happens or not. Recognizing these challenges, we propose a strategic dynamic pricing policy that incorporates the buyers' strategic behavior into the online learning to maximize the seller's cumulative revenue. We first prove that existing non-strategic pricing policies that neglect the buyers' strategic behavior result in a linear $\Omega(T)$ regret with $T$ the total time horizon, indicating that these policies are not better than a random pricing policy. We then establish that our proposed policy achieves a sublinear regret upper bound of $O(\sqrt{T})$. Importantly, our policy is not a mere amalgamation of existing dynamic pricing policies and strategic behavior handling algorithms. Our policy can also accommodate the scenario when the marginal cost of manipulation is unknown in advance. To account for it, we simultaneously estimate the valuation parameter and the cost parameter in the online pricing policy, which is shown to also achieve an $O(\sqrt{T})$ regret bound. Extensive experiments support our theoretical developments and demonstrate the superior performance of our policy compared to other pricing policies that are unaware of the strategic behaviors.
</details>
<details>
<summary>摘要</summary>
企业通常采用个性化价格策略，根据消费者个人特点来调整价格。在这个过程中，消费者也可以通过操作自己的特征数据来获得更低的价格，这会产生一定的操作成本。这种战略性行为可能会妨碍企业实现最大利润。本文研究了Contextual Dynamic Pricing问题，在这个问题中，卖方不知道买方的真实特征，只知道买方通过战略行为 manipulate的特征。此外，卖方也不知道买方对产品的评估价值，只知道一个二分类回应，表示是否成交或者不成交。识别这些挑战，我们提出了一种战略动态价格策略，该策略将买方的战略行为纳入在线学习中，以最大化卖方的总收益。我们首先证明了忽略买方战略行为的非战略价格策略会在总时间周期T上得到线性Ω(T)的 regret，这表明这些策略与随机价格策略相当。然后，我们证明了我们提出的策略可以达到O（√T）的 regret上限，这表明我们的策略不仅不 inferior于随机价格策略，还可以在不知道 manipulate 成本的情况下进行优化。为了考虑这种情况，我们同时估算价格参数和成本参数，并证明该策略可以达到O（√T）的 regret上限。实验结果支持我们的理论发展，并证明了我们的策略比其他不考虑买方战略行为的价格策略更高效。
</details></li>
</ul>
<hr>
<h2 id="A-Physics-Informed-Low-Shot-Learning-For-sEMG-Based-Estimation-of-Muscle-Force-and-Joint-Kinematics"><a href="#A-Physics-Informed-Low-Shot-Learning-For-sEMG-Based-Estimation-of-Muscle-Force-and-Joint-Kinematics" class="headerlink" title="A Physics-Informed Low-Shot Learning For sEMG-Based Estimation of Muscle Force and Joint Kinematics"></a>A Physics-Informed Low-Shot Learning For sEMG-Based Estimation of Muscle Force and Joint Kinematics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05361">http://arxiv.org/abs/2307.05361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Shi, Shuhao Ma, Yihui Zhao, Zhiqiang Zhang<br>for:This paper aims to improve the estimation of muscle force and joint kinematics from surface electromyography (sEMG) data using a physics-informed low-shot learning method.methods:The proposed method integrates Lagrange’s equation of motion and an inverse dynamic muscle model into a generative adversarial network (GAN) framework for structured feature decoding and extrapolated estimation from small sample data.results:The proposed method outperforms selected benchmark methods, including physics-informed convolution neural network (PI-CNN), vallina GAN, and multi-layer extreme learning machine (ML-ELM), in estimating muscle forces and joint kinematics. The estimations are also unbiased compared to physics-based inverse dynamics.<details>
<summary>Abstract</summary>
Muscle force and joint kinematics estimation from surface electromyography (sEMG) are essential for real-time biomechanical analysis of the dynamic interplay among neural muscle stimulation, muscle dynamics, and kinetics. Recent advances in deep neural networks (DNNs) have shown the potential to improve biomechanical analysis in a fully automated and reproducible manner. However, the small sample nature and physical interpretability of biomechanical analysis limit the applications of DNNs. This paper presents a novel physics-informed low-shot learning method for sEMG-based estimation of muscle force and joint kinematics. This method seamlessly integrates Lagrange's equation of motion and inverse dynamic muscle model into the generative adversarial network (GAN) framework for structured feature decoding and extrapolated estimation from the small sample data. Specifically, Lagrange's equation of motion is introduced into the generative model to restrain the structured decoding of the high-level features following the laws of physics. And a physics-informed policy gradient is designed to improve the adversarial learning efficiency by rewarding the consistent physical representation of the extrapolated estimations and the physical references. Experimental validations are conducted on two scenarios (i.e. the walking trials and wrist motion trials). Results indicate that the estimations of the muscle forces and joint kinematics are unbiased compared to the physics-based inverse dynamics, which outperforms the selected benchmark methods, including physics-informed convolution neural network (PI-CNN), vallina generative adversarial network (GAN), and multi-layer extreme learning machine (ML-ELM).
</details>
<details>
<summary>摘要</summary>
Muscle force和关节动态观测从表面电omyography（sEMG）是生动机动分析中的关键因素。最近的深度神经网络（DNNs）技术已经表现出可以自动化和复制生动机动分析的潜在力量。然而，生动机动分析的小样本特征和物理解释性限制了DNNs的应用。本文提出了一种新的物理学习低精度学习方法，用于sEMG基于的肌力和关节动态观测。这种方法将拉格朗日方程组入生成对抗网络（GAN）框架中，用于结构化特征解码和推断预测。具体来说，拉格朗日方程组入生成模型中，以便结构化解码高级特征遵循物理法律。此外，我们还设计了一种物理学习策略，以提高对抗学习效率，通过奖励遵循物理表述的推断估计和物理参考。实验 validate 在两个场景（即行走试验和手部运动试验）。结果表明，肌力和关节动态观测的估计不受偏见，与物理反向动力学相符，超越了选择的参考方法，包括物理学习核lear neural network（PI-CNN）、 vallina GAN 和多层极限学习机（ML-ELM）。
</details></li>
</ul>
<hr>
<h2 id="Optimization-based-Learning-for-Dynamic-Load-Planning-in-Trucking-Service-Networks"><a href="#Optimization-based-Learning-for-Dynamic-Load-Planning-in-Trucking-Service-Networks" class="headerlink" title="Optimization-based Learning for Dynamic Load Planning in Trucking Service Networks"></a>Optimization-based Learning for Dynamic Load Planning in Trucking Service Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04050">http://arxiv.org/abs/2307.04050</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ritesh Ojha, Wenbo Chen, Hanyu Zhang, Reem Khir, Alan Erera, Pascal Van Hentenryck</li>
<li>For: This paper aims to develop a decision-support tool for parcel carriers to optimize their service network design and load planning.* Methods: The paper formulates the Dynamic Load Planning Problem (DLPP) as a Mixed-Integer Programming (MIP) model and proposes a Goal-Directed Optimization method to eliminate symmetries and improve the quality of solutions. The paper also introduces an optimization proxy that combines a machine learning model and a feasibility restoration model to address computational challenges.* Results: The proposed approach is tested on industrial instances and shows significant improvements in terms of computational efficiency and solution quality compared to a commercial solver. The approach also demonstrates the benefits of load consolidation and the potential for significant cost savings through the combination of machine learning and optimization.Here is the summary in Simplified Chinese:* 为：这篇论文目标是为快递公司开发一种决策支持工具，以优化其服务网络设计和货运规划。* 方法：该论文将动态货运规划问题（DLPP）формализова为杂程式（MIP）模型，并提出了一种目标导向优化方法，以消除对称性并提高解的质量。论文还介绍了一种优化代理，该代理结合机器学习模型和可行性修复模型，以解决优化模型的计算挑战。* 结果：提出的方法在工业实例中进行了广泛的计算研究，并显示出了明显的计算效率和解质量的改善，相比于商业 solve。该方法还 demonstarted了货物集中和加工成本的减少，以及机器学习和优化的结合可以带来的显著经济效益。<details>
<summary>Abstract</summary>
The load planning problem is a critical challenge in service network design for parcel carriers: it decides how many trailers (or loads) to assign for dispatch over time between pairs of terminals. Another key challenge is to determine a flow plan, which specifies how parcel volumes are assigned to planned loads. This paper considers the Dynamic Load Planning Problem (DLPP) that considers both flow and load planning challenges jointly to adjust loads and flows as the demand forecast changes over time before the day of operations. The paper aims at developing a decision-support tool to inform planners making these decisions at terminals across the network. The paper formulates the DLPP as a MIP and shows that it admits a large number of symmetries in a network where each commodity can be routed through primary and alternate paths. As a result, an optimization solver may return fundamentally different solutions to closely related problems, confusing planners and reducing trust in optimization. To remedy this limitation, the paper proposes a Goal-Directed Optimization that eliminates those symmetries by generating optimal solutions staying close to a reference plan. The paper also proposes an optimization proxy to address the computational challenges of the optimization models. The proxy combines a machine learning model and a feasibility restoration model and finds solutions that satisfy real-time constraints imposed by planners-in-the-loop. An extensive computational study on industrial instances shows that the optimization proxy is around 10 times faster than the commercial solver in obtaining the same quality solutions and orders of magnitude faster for generating solutions that are consistent with each other. The proposed approach also demonstrates the benefits of the DLPP for load consolidation, and the significant savings obtained from combining machine learning and optimization.
</details>
<details>
<summary>摘要</summary>
服务网络设计中的负载观念问题是一个扮演性的挑战，决定在不同终点之间分配多少货车（或负载），以及将货物量分配到计划中的负载上。这篇文章考虑了时间流动负载观念问题（DLPP），考虑了流动和负载观念问题的共同挑战，以适应需求预测在时间上的变化。文章的目标是发展一个帮助计划人员做出决策的决策支持工具。文章将DLPP表述为一个内部数据流过程（MIP），并证明了这个问题在网络中的各种商品可以通过主要和备用路径进行路由。因此，优化 solver 可能会返回 closely related 问题的不同解，导致计划人员误解和依靠优化减少。为解决这个限制，文章提出了一个目标导向优化，删除了这些对称性。文章还提出了一个优化代理， combinates 机器学习模型和可行性修复模型，寻找满足实时约束的解决方案。一系列的 Computational 研究显示，优化代理 比商业 solver 在获得相同质量解决方案上约 10 倍快，而且在生成相容的解决方案上具有数量级的优化。提案的方法也显示了 DLPP 的负载整合和机器学习优化的重要性。
</details></li>
</ul>
<hr>
<h2 id="The-Value-of-Chess-Squares"><a href="#The-Value-of-Chess-Squares" class="headerlink" title="The Value of Chess Squares"></a>The Value of Chess Squares</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05330">http://arxiv.org/abs/2307.05330</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Dpay123/chess">https://github.com/Dpay123/chess</a></li>
<li>paper_authors: Aditya Gupta, Shiva Maharaj, Nicholas Polson, Vadim Sokolov</li>
<li>for: 这个研究的目的是确定棋盘上的棋子和位置的价值，以及用于评估棋盘上的位置的精度。</li>
<li>methods: 本研究使用了新的评估方法，其中包括对棋子和平方的边缘价值的引入。</li>
<li>results: 研究发现，使用新的评估方法可以更好地评估棋盘上的位置和棋子的价值，并提供了有价值的棋盘结构和棋子评估的新视角。<details>
<summary>Abstract</summary>
Valuing chess squares and determining the placement of pieces on the board are the main objectives of our study. With the emergence of chess AI, it has become possible to accurately assess the worth of positions in a game of chess. The conventional approach assigns fixed values to pieces $(\symking=\infty, \symqueen=9, \symrook=5, \symbishop=3, \symknight=3, \sympawn=1)$. We enhance this analysis by introducing marginal valuations for both pieces and squares. We demonstrate our method by examining the positioning of Knights and Bishops, and also provide valuable insights into the valuation of pawns. Notably, Nimzowitsch was among the pioneers in advocating for the significance of Pawn structure and valuation. Finally, we conclude by suggesting potential avenues for future research.
</details>
<details>
<summary>摘要</summary>
我们的研究的主要目标是评估棋盘上的棋子和坐标的价值。随着棋盘智能的出现，可以准确评估棋盘上的位置价值。传统方法将棋子的价值分别设置为($\symking=\infty, \symqueen=9, \symrook=5, \symbishop=3, \symknight=3, \sympawn=1)$。我们增强了这种分析，通过引入棋子和坐标的边缘价值。我们通过研究夜莺和主教的位置问题，并提供了关于坐标价值的有益信息。值得注意的是，尼莫迪奇（Nimzowitsch）是棋盘价值和结构的先驱者之一，他认为Pawn结构的价值很重要。最后，我们 conclude by suggesting potential future research directions.Here's the translation of the text into Traditional Chinese:我们的研究的主要目标是评估棋盘上的棋子和坐标的价值。随着棋盘智能的出现，可以准确评估棋盘上的位置价值。传统方法将棋子的价值分别设置为($\symking=\infty, \symqueen=9, \symrook=5, \symbishop=3, \symknight=3, \sympawn=1)$。我们增强了这种分析，通过引入棋子和坐标的边缘价值。我们通过研究夜莺和主教的位置问题，并提供了关于坐标价值的有益信息。值得注意的是，尼莫迪奇（Nimzowitsch）是棋盘价值和结构的先驱者之一，他认为Pawn结构的价值很重要。最后，我们 conclude by suggesting potential future research directions.
</details></li>
</ul>
<hr>
<h2 id="Designing-a-Direct-Feedback-Loop-between-Humans-and-Convolutional-Neural-Networks-through-Local-Explanations"><a href="#Designing-a-Direct-Feedback-Loop-between-Humans-and-Convolutional-Neural-Networks-through-Local-Explanations" class="headerlink" title="Designing a Direct Feedback Loop between Humans and Convolutional Neural Networks through Local Explanations"></a>Designing a Direct Feedback Loop between Humans and Convolutional Neural Networks through Local Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04036">http://arxiv.org/abs/2307.04036</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tongstevensun/deepfuse">https://github.com/tongstevensun/deepfuse</a></li>
<li>paper_authors: Tong Steven Sun, Yuyang Gao, Shubham Khaladkar, Sijia Liu, Liang Zhao, Young-Ho Kim, Sungsoo Ray Hong</li>
<li>for: 本研究旨在提供一种实时Feedback loopbetween用户和Convolutional Neural Networks (CNNs)，以便在诊断和修复CNNs的漏洞方面提供直接反馈。</li>
<li>methods: 本研究使用了Local explanation方法，通过可见的直观性，帮助ML工程师更好地理解CNNs的输出。同时，本研究还提出了一种实时反馈机制，使得用户可以在执行CNNs时，对其输出进行修改和调整。</li>
<li>results: 本研究通过一个两天的实验（S2），证明了DeepFuse可以帮助参与者创建一个更加准确和合理的CNN模型，同时也提高了参与者对CNNs的理解和修复能力。此外，本研究还发现，通过DeepFuse的指导，参与者可以更加准确地诊断和修复CNNs的漏洞。<details>
<summary>Abstract</summary>
The local explanation provides heatmaps on images to explain how Convolutional Neural Networks (CNNs) derive their output. Due to its visual straightforwardness, the method has been one of the most popular explainable AI (XAI) methods for diagnosing CNNs. Through our formative study (S1), however, we captured ML engineers' ambivalent perspective about the local explanation as a valuable and indispensable envision in building CNNs versus the process that exhausts them due to the heuristic nature of detecting vulnerability. Moreover, steering the CNNs based on the vulnerability learned from the diagnosis seemed highly challenging. To mitigate the gap, we designed DeepFuse, the first interactive design that realizes the direct feedback loop between a user and CNNs in diagnosing and revising CNN's vulnerability using local explanations. DeepFuse helps CNN engineers to systemically search "unreasonable" local explanations and annotate the new boundaries for those identified as unreasonable in a labor-efficient manner. Next, it steers the model based on the given annotation such that the model doesn't introduce similar mistakes. We conducted a two-day study (S2) with 12 experienced CNN engineers. Using DeepFuse, participants made a more accurate and "reasonable" model than the current state-of-the-art. Also, participants found the way DeepFuse guides case-based reasoning can practically improve their current practice. We provide implications for design that explain how future HCI-driven design can move our practice forward to make XAI-driven insights more actionable.
</details>
<details>
<summary>摘要</summary>
本地解释提供图像上的热图，以解释卷积神经网络（CNN）的输出来源。由于其视觉直观，这种方法在诊断CNN方面得到了广泛的应用。但是，根据我们的首次研究（S1），机器学习工程师对本地解释持有折衔的看法，认为它作为CNN建模的重要工具，但同时也会带来劳动iously检测漏洞的困难。此外，通过检测的漏洞来调整CNN也显然具有困难。为了缓解这一问题，我们设计了深度融合（DeepFuse），第一个实现用户与CNN之间的直接反馈循环的交互设计。DeepFuse帮助CNN工程师系统地搜索"不合理"的本地解释，并在劳动效率高的情况下注释新的边界。然后，它将模型按照给出的注释进行调整，以避免模型 introduce 类似的错误。我们进行了两天的研究（S2），与12名经验丰富的CNN工程师进行了合作。使用DeepFuse，参与者创建了更加准确和"合理"的模型，并认为DeepFuse的指导方法可以实际地改善他们当前的做法。我们提供了设计方面的推荐，解释了未来HCID驱动的设计如何将XAI驱动的洞察力变得更加操作化。
</details></li>
</ul>
<hr>
<h2 id="Learning-Variational-Neighbor-Labels-for-Test-Time-Domain-Generalization"><a href="#Learning-Variational-Neighbor-Labels-for-Test-Time-Domain-Generalization" class="headerlink" title="Learning Variational Neighbor Labels for Test-Time Domain Generalization"></a>Learning Variational Neighbor Labels for Test-Time Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04033">http://arxiv.org/abs/2307.04033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sameer Ambekar, Zehao Xiao, Jiayi Shen, Xiantong Zhen, Cees G. M. Snoek</li>
<li>for: 本研究努力实现领域总结，即模型在训练于源频道后在未见目标频道上部署。我们遵循严格的源训练和目标测试的分离，但是利用目标频道自身的无标注数据来进行推理。</li>
<li>methods: 我们提出了三个贡献。首先，我们提出了使用概率 Pseudo-labeling 将目标样本泛化到目标频道上，以使源频道训练的模型在测试时能够泛化到目标频道。其次，我们学习了variational neighbor labels，以使用邻居目标样本的信息来生成更加Robust的 Pseudo labels。第三，我们引入了一个元总结阶段，以在训练中模拟泛化过程，以学习更好地泛化目标信息。</li>
<li>results: 我们在六个广泛使用的 dataset 上进行了实验，结果表明我们的提议具有优势、能力和有效性。<details>
<summary>Abstract</summary>
This paper strives for domain generalization, where models are trained exclusively on source domains before being deployed at unseen target domains. We follow the strict separation of source training and target testing but exploit the value of the unlabeled target data itself during inference. We make three contributions. First, we propose probabilistic pseudo-labeling of target samples to generalize the source-trained model to the target domain at test time. We formulate the generalization at test time as a variational inference problem by modeling pseudo labels as distributions to consider the uncertainty during generalization and alleviate the misleading signal of inaccurate pseudo labels. Second, we learn variational neighbor labels that incorporate the information of neighboring target samples to generate more robust pseudo labels. Third, to learn the ability to incorporate more representative target information and generate more precise and robust variational neighbor labels, we introduce a meta-generalization stage during training to simulate the generalization procedure. Experiments on six widely-used datasets demonstrate the benefits, abilities, and effectiveness of our proposal.
</details>
<details>
<summary>摘要</summary>
这篇论文努力实现领域通用化，即在训练时仅使用源领域，然后在未见目标领域进行部署。我们遵循严格的源训练和目标测试的分离，但是利用目标数据本身的价值进行推理。我们提出了三个贡献：1. 我们提议在测试时使用目标样本的概率 Pseudo-标签来泛化源训练模型到目标领域。我们将泛化视为测试时的变量推理问题，并在泛化过程中考虑 pseudo labels 的不确定性，以避免 pseudo labels 的不准确信号。2. 我们学习了基于邻域目标样本的变量邻域标签，以生成更加稳健的 pseudo labels。3. 为了学习更好地汇集更多的目标信息并生成更加精准和稳定的变量邻域标签，我们引入了一个元泛化阶段在训练中进行模拟泛化过程。我们在六个广泛使用的 dataset 上进行了实验，并证明了我们的提议的优点、能力和有效性。
</details></li>
</ul>
<hr>
<h2 id="On-“Indifference”-and-Backward-Induction-in-Games-with-Perfect-Information"><a href="#On-“Indifference”-and-Backward-Induction-in-Games-with-Perfect-Information" class="headerlink" title="On “Indifference” and Backward Induction in Games with Perfect Information"></a>On “Indifference” and Backward Induction in Games with Perfect Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04029">http://arxiv.org/abs/2307.04029</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nimrod Megiddo</li>
<li>for: 这 paper written for?	+ 这 paper 探讨了在游戏中不同结果间的偏袋不可小 perturbations 问题。</li>
<li>methods: 这 paper 使用了哪些方法?	+ 这 paper 使用了 rational choice 概念和其他玩家的利得来解决了偏袋不可小 perturbations 问题。</li>
<li>results: 这 paper 得到了哪些结果?	+ 这 paper 得到了一种基于其他玩家的利得的 rationality 概念来解决偏袋不可小 perturbations 问题的方法，即 Tit-for-Tat。<details>
<summary>Abstract</summary>
Indifference of a player with respect to two distinct outcomes of a game cannot be handled by small perturbations, because the actual choice may have significant impact on other players, and cause them to act in a way that has significant impact of the indifferent player. It is argued that ties among rational choices can be resolved by refinements of the concept of rationality based on the utilities of other players. One such refinement is the concept of Tit-for-Tat.
</details>
<details>
<summary>摘要</summary>
“玩家对两个游戏结果的不在焦虑不能通过小幅度的改变来处理，因为他的实际选择可能会对其他玩家造成重要影响，使其发生重要影响。有一种解决方案是基于其他玩家的利益来修改 rationality 概念，例如 Tit-for-Tat。”Note: "玩家" (wán jiā) in Chinese refers to "player" in English.
</details></li>
</ul>
<hr>
<h2 id="Measuring-the-Success-of-Diffusion-Models-at-Imitating-Human-Artists"><a href="#Measuring-the-Success-of-Diffusion-Models-at-Imitating-Human-Artists" class="headerlink" title="Measuring the Success of Diffusion Models at Imitating Human Artists"></a>Measuring the Success of Diffusion Models at Imitating Human Artists</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04028">http://arxiv.org/abs/2307.04028</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Casper, Zifan Guo, Shreya Mogulothu, Zachary Marinov, Chinmay Deshpande, Rui-Jie Yew, Zheng Dai, Dylan Hadfield-Menell</li>
<li>for: 这个论文旨在研究现代扩散模型是否可以模仿人类艺术家的作品。</li>
<li>methods: 这个论文使用了 Contrastive Language-Image Pretrained (CLIP) 算法来测试模型是否可以模仿特定艺术家的风格。</li>
<li>results: 研究发现，当模型被请求模仿某个艺术家的作品时，CLIP 可以很准确地将这些作品归类回原始艺术家。此外，研究还发现，这些模仿作品与艺术家的原始作品之间存在高度的统计学相似性。<details>
<summary>Abstract</summary>
Modern diffusion models have set the state-of-the-art in AI image generation. Their success is due, in part, to training on Internet-scale data which often includes copyrighted work. This prompts questions about the extent to which these models learn from, imitate, or copy the work of human artists. This work suggests that tying copyright liability to the capabilities of the model may be useful given the evolving ecosystem of generative models. Specifically, much of the legal analysis of copyright and generative systems focuses on the use of protected data for training. As a result, the connections between data, training, and the system are often obscured. In our approach, we consider simple image classification techniques to measure a model's ability to imitate specific artists. Specifically, we use Contrastive Language-Image Pretrained (CLIP) encoders to classify images in a zero-shot fashion. Our process first prompts a model to imitate a specific artist. Then, we test whether CLIP can be used to reclassify the artist (or the artist's work) from the imitation. If these tests match the imitation back to the original artist, this suggests the model can imitate that artist's expression. Our approach is simple and quantitative. Furthermore, it uses standard techniques and does not require additional training. We demonstrate our approach with an audit of Stable Diffusion's capacity to imitate 70 professional digital artists with copyrighted work online. When Stable Diffusion is prompted to imitate an artist from this set, we find that the artist can be identified from the imitation with an average accuracy of 81.0%. Finally, we also show that a sample of the artist's work can be matched to these imitation images with a high degree of statistical reliability. Overall, these results suggest that Stable Diffusion is broadly successful at imitating individual human artists.
</details>
<details>
<summary>摘要</summary>
现代扩散模型已经设置了人工智能图像生成的 estado-del-arte。其成功部分归功于训练在互联网规模的数据上，这些数据经常包含版权工作。这些问题引发了关于模型从人类艺术家的作品中学习、模仿或复制的问题。这个研究建议将版权责任与模型的能力相关联可能是有用的， giventhe evolving ecosystem of generative models。具体来说，法律分析对于版权和生成系统的关系经常集中在使用保护的数据进行训练。因此，数据、训练和系统之间的连接经常被隐藏。我们的方法是通过使用语义相似性来衡量模型是否能够模仿特定艺术家。我们使用语言-图像预训练（CLIP）编码器来在零扩展方式进行图像分类。我们的过程首先要让模型模仿特定艺术家。然后，我们测试是否可以使用CLIP来重新分类艺术家（或艺术家的作品）。如果这些测试匹配艺术家（或艺术家的作品）与模仿，这表示模型可以模仿这位艺术家的表达。我们的方法是简单而量化的，并且不需要额外训练。我们通过对Stable Diffusion的可行性进行审核，发现它可以模仿70名职业数字艺术家的版权作品。当Stable Diffusion被让模仿这些艺术家时，我们发现这些艺术家可以从模仿中匹配出来，均为81.0%。此外，我们还证明了这些模仿图像和艺术家的作品之间存在高度统计学的相互关联。总之，这些结果表明Stable Diffusion能够广泛地模仿人类艺术家。
</details></li>
</ul>
<hr>
<h2 id="GP-guided-MPPI-for-Efficient-Navigation-in-Complex-Unknown-Cluttered-Environments"><a href="#GP-guided-MPPI-for-Efficient-Navigation-in-Complex-Unknown-Cluttered-Environments" class="headerlink" title="GP-guided MPPI for Efficient Navigation in Complex Unknown Cluttered Environments"></a>GP-guided MPPI for Efficient Navigation in Complex Unknown Cluttered Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04019">http://arxiv.org/abs/2307.04019</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ihab S. Mohamed, Mahmoud Ali, Lantao Liu</li>
<li>For: The paper is written for robotic navigation in unknown, cluttered environments with limited sensing capabilities.* Methods: The paper uses local trajectory optimization methods, specifically Model Predictive Path Intergal (MPPI), and integrates it with a local perception model based on Sparse Gaussian Process (SGP) to learn about the navigable space surrounding the robot and identify suggested subgoals.* Results: The proposed control strategy, called GP-MPPI, is validated through both simulated and real-world experiments of 2D autonomous navigation tasks in complex unknown environments, demonstrating its efficiency and robustness in guiding the robot safely towards its desired goal while avoiding obstacles and escaping entrapment in local minima.Here is the information in Simplified Chinese text:* 为：论文写作的目的是Robotic Navigation在未知、拥堵的环境中进行路径规划，具有限制的感知能力。* 方法：论文使用本地规划方法，即Model Predictive Path Intergal (MPPI)，并将其与本地感知模型基于Sparse Gaussian Process (SGP)相结合，以学习环境中可行的空间，并提供了一系列建议的目标，以便MPPI计划器选择最优化的目标。* 结果：论文提出的控制策略，即GP-MPPI，通过在实验室和真实环境中进行的2D自主导航任务的实验 validate了其效率和可靠性，证明了它在避免障碍物和脱险地逃脱局部最优化的情况下安全地导航到目标。<details>
<summary>Abstract</summary>
Robotic navigation in unknown, cluttered environments with limited sensing capabilities poses significant challenges in robotics. Local trajectory optimization methods, such as Model Predictive Path Intergal (MPPI), are a promising solution to this challenge. However, global guidance is required to ensure effective navigation, especially when encountering challenging environmental conditions or navigating beyond the planning horizon. This study presents the GP-MPPI, an online learning-based control strategy that integrates MPPI with a local perception model based on Sparse Gaussian Process (SGP). The key idea is to leverage the learning capability of SGP to construct a variance (uncertainty) surface, which enables the robot to learn about the navigable space surrounding it, identify a set of suggested subgoals, and ultimately recommend the optimal subgoal that minimizes a predefined cost function to the local MPPI planner. Afterward, MPPI computes the optimal control sequence that satisfies the robot and collision avoidance constraints. Such an approach eliminates the necessity of a global map of the environment or an offline training process. We validate the efficiency and robustness of our proposed control strategy through both simulated and real-world experiments of 2D autonomous navigation tasks in complex unknown environments, demonstrating its superiority in guiding the robot safely towards its desired goal while avoiding obstacles and escaping entrapment in local minima. The GPU implementation of GP-MPPI, including the supplementary video, is available at https://github.com/IhabMohamed/GP-MPPI.
</details>
<details>
<summary>摘要</summary>
人工智能导航在未知、杂乱环境中具有限制的感知能力 pose significant challenges in robotics. Local trajectory optimization methods, such as Model Predictive Path Intergal (MPPI), are a promising solution to this challenge. However, global guidance is required to ensure effective navigation, especially when encountering challenging environmental conditions or navigating beyond the planning horizon. This study presents the GP-MPPI, an online learning-based control strategy that integrates MPPI with a local perception model based on Sparse Gaussian Process (SGP). The key idea is to leverage the learning capability of SGP to construct a variance (uncertainty) surface, which enables the robot to learn about the navigable space surrounding it, identify a set of suggested subgoals, and ultimately recommend the optimal subgoal that minimizes a predefined cost function to the local MPPI planner. Afterward, MPPI computes the optimal control sequence that satisfies the robot and collision avoidance constraints. Such an approach eliminates the necessity of a global map of the environment or an offline training process. We validate the efficiency and robustness of our proposed control strategy through both simulated and real-world experiments of 2D autonomous navigation tasks in complex unknown environments, demonstrating its superiority in guiding the robot safely towards its desired goal while avoiding obstacles and escaping entrapment in local minima. The GPU implementation of GP-MPPI, including the supplementary video, is available at https://github.com/IhabMohamed/GP-MPPI.
</details></li>
</ul>
<hr>
<h2 id="Proceedings-Nineteenth-conference-on-Theoretical-Aspects-of-Rationality-and-Knowledge"><a href="#Proceedings-Nineteenth-conference-on-Theoretical-Aspects-of-Rationality-and-Knowledge" class="headerlink" title="Proceedings Nineteenth conference on Theoretical Aspects of Rationality and Knowledge"></a>Proceedings Nineteenth conference on Theoretical Aspects of Rationality and Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04005">http://arxiv.org/abs/2307.04005</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rineke Verbrugge</li>
</ul>
<details>
<summary>Abstract</summary>
The TARK conference (Theoretical Aspects of Rationality and Knowledge) is a conference that aims to bring together researchers from a wide variety of fields, including computer science, artificial intelligence, game theory, decision theory, philosophy, logic, linguistics, and cognitive science. Its goal is to further our understanding of interdisciplinary issues involving reasoning about rationality and knowledge.   Previous conferences have been held biennially around the world since 1986, on the initiative of Joe Halpern (Cornell University). Topics of interest include, but are not limited to, semantic models for knowledge, belief, awareness and uncertainty, bounded rationality and resource-bounded reasoning, commonsense epistemic reasoning, epistemic logic, epistemic game theory, knowledge and action, applications of reasoning about knowledge and other mental states, belief revision, computational social choice, algorithmic game theory, and foundations of multi-agent systems. Information about TARK, including conference proceedings, is available at http://www.tark.org/   These proceedings contain the papers that have been accepted for presentation at the Nineteenth Conference on Theoretical Aspects of Rationality and Knowledge (TARK 2023), held between June 28 and June 30, 2023, at the University of Oxford, United Kingdom. The conference website can be found at https://sites.google.com/view/tark-2023
</details>
<details>
<summary>摘要</summary>
TARK conference（理性和知识的理论方面）是一个会议，旨在让不同领域的研究人员（包括计算机科学、人工智能、游戏理论、决策理论、哲学、逻辑、语言科学和认知科学）共同分享他们的研究成果。会议的目标是深入了解跨学科问题，有关理性和知识的推理。自1986年以来，TARK会议每两年在世界各地举行，由 Джо·哈尔佩恩（科内尔大学）发起。会议的主题包括，但不限于：知识、信念、意识和不确定性的semantic模型，有限智能和资源有限的推理，通常的epistemic推理，epistemic逻辑、epistemic游戏理论、知识和行动、理性和知识之间的关系，以及应用推理知识和其他心理状态的问题。以下是TARK会议的论文集，包括2023年6月28日-6月30日在英国牛津大学举行的第十九届TARK会议（TARK 2023） Accepted Papers。会议网站的地址为<https://sites.google.com/view/tark-2023>。
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/09/cs.AI_2023_07_09/" data-id="closbroj800080g883h2wczfv" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_07_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/09/cs.CL_2023_07_09/" class="article-date">
  <time datetime="2023-07-09T11:00:00.000Z" itemprop="datePublished">2023-07-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/09/cs.CL_2023_07_09/">cs.CL - 2023-07-09</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Can-Generative-Large-Language-Models-Perform-ASR-Error-Correction"><a href="#Can-Generative-Large-Language-Models-Perform-ASR-Error-Correction" class="headerlink" title="Can Generative Large Language Models Perform ASR Error Correction?"></a>Can Generative Large Language Models Perform ASR Error Correction?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04172">http://arxiv.org/abs/2307.04172</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rao Ma, Mengjie Qian, Potsawee Manakul, Mark Gales, Kate Knill</li>
<li>for: 这paper是为了提高ASR系统的性能而写的，具体来说是通过采用zero-shot或1-shot的方式进行语言模型ChatGPT的应用，以实现ASR错误修正。</li>
<li>methods: 这paper使用了ChatGPT模型作为ASR错误修正的基础模型，并提出了无约束和N-best约束两种方法来进行修正。</li>
<li>results: 试验结果表明，使用ChatGPT模型进行ASR错误修正可以大幅提高ASR系统的性能，特别是在1-shot setting下。<details>
<summary>Abstract</summary>
ASR error correction continues to serve as an important part of post-processing for speech recognition systems. Traditionally, these models are trained with supervised training using the decoding results of the underlying ASR system and the reference text. This approach is computationally intensive and the model needs to be re-trained when switching the underlying ASR model. Recent years have seen the development of large language models and their ability to perform natural language processing tasks in a zero-shot manner. In this paper, we take ChatGPT as an example to examine its ability to perform ASR error correction in the zero-shot or 1-shot settings. We use the ASR N-best list as model input and propose unconstrained error correction and N-best constrained error correction methods. Results on a Conformer-Transducer model and the pre-trained Whisper model show that we can largely improve the ASR system performance with error correction using the powerful ChatGPT model.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本为简化中文。<</SYS>>ASR错误修正仍然serve为speech recognition系统 posterior 处理中的重要部分。传统上，这些模型通过supervised training使用underlying ASR系统的解码结果和参考文本进行训练。这种方法是计算昂贵的，并且模型需要在Switching underlying ASR模型时重新训练。 recent years have seen the development of large language models and their ability to perform natural language processing tasks in a zero-shot manner. 在这篇论文中，我们使用ChatGPT作为例子，检查它是否可以在zero-shot或1-shot settings中进行ASR错误修正。我们使用ASR N-best list作为模型输入，并提出了不受限制的错误修正方法和N-best受限制的错误修正方法。 results on a Conformer-Transducer模型和预训练Whisper模型表明，我们可以使用powerful ChatGPT模型来大幅提高ASR系统性能。
</details></li>
</ul>
<hr>
<h2 id="Dream-Content-Discovery-from-Reddit-with-an-Unsupervised-Mixed-Method-Approach"><a href="#Dream-Content-Discovery-from-Reddit-with-an-Unsupervised-Mixed-Method-Approach" class="headerlink" title="Dream Content Discovery from Reddit with an Unsupervised Mixed-Method Approach"></a>Dream Content Discovery from Reddit with an Unsupervised Mixed-Method Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04167">http://arxiv.org/abs/2307.04167</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anubhab Das, Sanja Šćepanović, Luca Maria Aiello, Remington Mallett, Deirdre Barrett, Daniele Quercia</li>
<li>for: This paper aims to develop a new, data-driven approach for analyzing dream reports and understanding the topics and themes that appear in dreams.</li>
<li>methods: The authors use natural language processing techniques to identify topics in free-form dream reports and group them into larger themes. They also compare their results to the Hall and van de Castle scale to validate their findings.</li>
<li>results: The authors analyze 44,213 dream reports from Reddit’s r&#x2F;Dreams subreddit and identify 217 topics, grouped into 22 larger themes. They also show how their method can be used to understand changes in collective dream experiences over time and around major events like the COVID-19 pandemic and the Russo-Ukrainian war.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文的目的是开发一种基于自然语言处理技术的新方法，用于分析梦境报告和了解梦境中出现的主题和主题。</li>
<li>methods: 作者使用自然语言处理技术来 identificar梦境报告中的主题，并将其分组成更大的主题。他们还与哈尔和德卡斯特LE scale进行比较，以验证其结果。</li>
<li>results: 作者分析了 Reddit r&#x2F;梦境 subreddit上的44,213个梦境报告，并发现了217个主题，分组成22个更大的主题。他们还表明了他们的方法可以用于理解时间和大事件的影响，如COVID-19疫苗和俄乌战争。<details>
<summary>Abstract</summary>
Dreaming is a fundamental but not fully understood part of human experience that can shed light on our thought patterns. Traditional dream analysis practices, while popular and aided by over 130 unique scales and rating systems, have limitations. Mostly based on retrospective surveys or lab studies, they struggle to be applied on a large scale or to show the importance and connections between different dream themes. To overcome these issues, we developed a new, data-driven mixed-method approach for identifying topics in free-form dream reports through natural language processing. We tested this method on 44,213 dream reports from Reddit's r/Dreams subreddit, where we found 217 topics, grouped into 22 larger themes: the most extensive collection of dream topics to date. We validated our topics by comparing it to the widely-used Hall and van de Castle scale. Going beyond traditional scales, our method can find unique patterns in different dream types (like nightmares or recurring dreams), understand topic importance and connections, and observe changes in collective dream experiences over time and around major events, like the COVID-19 pandemic and the recent Russo-Ukrainian war. We envision that the applications of our method will provide valuable insights into the intricate nature of dreaming.
</details>
<details>
<summary>摘要</summary>
梦境是人类经验中的基本 Component, but it is not yet fully understood. 传统的梦境分析方法，虽然受欢迎且有130多个专门的Scale和评分系统，但它们有限制。大多数是基于回忆题或实验室实验，难以应用于大规模或显示梦境主题之间的重要性和连接。为了解决这些问题，我们开发了一个新的数据驱动混合方法，通过自然语言处理来识别自由形式的梦境报告中的主题。我们对网络上的Reddit的r/Dreams子区域上的44,213个梦境报告进行了测试，发现了217个主题，分为22个更大的主题：迄今为止最大的梦境主题收集。我们验证了我们的主题，与广泛使用的哈尔和van de Castle专业Scale进行比较。我们的方法可以找到不同类型的梦境（如夜恶梦或重复的梦境）中的独特模式，理解主题的重要性和连接，并观察时间和主要事件（如COVID-19疫情和最近的俄乌战略）的影响。我们觉得这些应用将提供价值的关于梦境的深入了解。
</details></li>
</ul>
<hr>
<h2 id="Towards-cross-language-prosody-transfer-for-dialog"><a href="#Towards-cross-language-prosody-transfer-for-dialog" class="headerlink" title="Towards cross-language prosody transfer for dialog"></a>Towards cross-language prosody transfer for dialog</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04123">http://arxiv.org/abs/2307.04123</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/joneavila/dral">https://github.com/joneavila/dral</a></li>
<li>paper_authors: Jonathan E. Avila, Nigel G. Ward</li>
<li>for: 这个论文的目的是为了解决现代语音译语系统中对对话用途的不足。特别是在译语过程中略过 speaker intention 和态度的细节。</li>
<li>methods: 作者采用了一种数据采集协议，让双语说话者重新重复之前的对话中的语言，并使用这些数据采集了一个英语-西班牙语词库，目前包含1871个匹配的语音对。此外，作者还开发了一个简单的距离度量，基于一组广泛的语音特征。</li>
<li>results: 作者的发现可以指导未来关于 cross-language 语音特征的研究，以及设计能够有效地传递语音特征的语音译语系统。<details>
<summary>Abstract</summary>
Speech-to-speech translation systems today do not adequately support use for dialog purposes. In particular, nuances of speaker intent and stance can be lost due to improper prosody transfer. We present an exploration of what needs to be done to overcome this. First, we developed a data collection protocol in which bilingual speakers re-enact utterances from an earlier conversation in their other language, and used this to collect an English-Spanish corpus, so far comprising 1871 matched utterance pairs. Second, we developed a simple prosodic dissimilarity metric based on Euclidean distance over a broad set of prosodic features. We then used these to investigate cross-language prosodic differences, measure the likely utility of three simple baseline models, and identify phenomena which will require more powerful modeling. Our findings should inform future research on cross-language prosody and the design of speech-to-speech translation systems capable of effective prosody transfer.
</details>
<details>
<summary>摘要</summary>
当今的语音到语音翻译系统不充分支持对话用途。特别是，speaker的意图和态度可能会在不正确的语速传递中丢失。我们提出了一种解决方案，包括发展一种数据采集协议，由双语说话人重新表演之前的对话中的语音，并使用这些数据采集了一个英语-西班牙语词库，目前已经有1871个匹配的语音对。second, we developed a simple prosody dissimilarity metric based on Euclidean distance over a broad set of prosodic features. we then used these to investigate cross-language prosodic differences, measure the likely utility of three simple baseline models, and identify phenomena which will require more powerful modeling. our findings should inform future research on cross-language prosody and the design of speech-to-speech translation systems capable of effective prosody transfer.
</details></li>
</ul>
<hr>
<h2 id="Optimal-Transport-Posterior-Alignment-for-Cross-lingual-Semantic-Parsing"><a href="#Optimal-Transport-Posterior-Alignment-for-Cross-lingual-Semantic-Parsing" class="headerlink" title="Optimal Transport Posterior Alignment for Cross-lingual Semantic Parsing"></a>Optimal Transport Posterior Alignment for Cross-lingual Semantic Parsing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04096">http://arxiv.org/abs/2307.04096</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tom Sherborne, Tom Hosking, Mirella Lapata</li>
<li>for: 这则论文旨在探讨跨语言Semantic parsing的问题，即从高资源语言（例如英文）传递分析能力到低资源语言。</li>
<li>methods: 本文提出了一新的方法，即使用Optimal Transport来降低跨语言分布差异，以提高从自然语言中的分析。</li>
<li>results: 本文在两个数据集MTOP和MultiATIS++SQL上进行了评估，成功地在少数例下lingshot regime中实现了顶尖的结果。<details>
<summary>Abstract</summary>
Cross-lingual semantic parsing transfers parsing capability from a high-resource language (e.g., English) to low-resource languages with scarce training data. Previous work has primarily considered silver-standard data augmentation or zero-shot methods, however, exploiting few-shot gold data is comparatively unexplored. We propose a new approach to cross-lingual semantic parsing by explicitly minimizing cross-lingual divergence between probabilistic latent variables using Optimal Transport. We demonstrate how this direct guidance improves parsing from natural languages using fewer examples and less training. We evaluate our method on two datasets, MTOP and MultiATIS++SQL, establishing state-of-the-art results under a few-shot cross-lingual regime. Ablation studies further reveal that our method improves performance even without parallel input translations. In addition, we show that our model better captures cross-lingual structure in the latent space to improve semantic representation similarity.
</details>
<details>
<summary>摘要</summary>
通用语言semantic parsing是一种将分析能力从高资源语言（如英语）传递到低资源语言的技术。先前的工作主要集中在银色标准数据增强或零例目标方法上，然而受限于数据量的几个例行金融方法尚未得到了充分利用。我们提出了一种新的交通优化方法来实现跨语言semantic parsing，通过最优交通来减少跨语言差异。我们通过这种直接导向来提高自然语言的分析，需要 fewer examples和更少的训练。我们在MTOP和MultiATIS++SQL两个 datasets上进行了评估，并在几个例行跨语言 режиме下创造了状态机器人的结果。剖分研究表明，我们的方法可以提高性能，甚至没有平行输入翻译。此外，我们的模型更好地捕捉了跨语言结构，以提高 semantic representation的相似性。
</details></li>
</ul>
<hr>
<h2 id="Bidirectional-Attention-as-a-Mixture-of-Continuous-Word-Experts"><a href="#Bidirectional-Attention-as-a-Mixture-of-Continuous-Word-Experts" class="headerlink" title="Bidirectional Attention as a Mixture of Continuous Word Experts"></a>Bidirectional Attention as a Mixture of Continuous Word Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04057">http://arxiv.org/abs/2307.04057</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yixinw-lab/attention-uai">https://github.com/yixinw-lab/attention-uai</a></li>
<li>paper_authors: Kevin Christian Wibisono, Yixin Wang</li>
<li>for: 这篇论文研究了bidirectional attention的统计基础，即bidirectional attention是如何适应hetroogeneous数据的。</li>
<li>methods: 该论文使用了单层单头bidirectional attention，并对其进行了梯度下降优化。</li>
<li>results: 研究发现，bidirectional attention可以在out-of-distribution泛化中表现出优于现有的表示器扩展。此外，该论文还提出了一种基于MoE的对 categorical tabular data的扩展，并证明了bidirectional attention中的线性词 analogies的存在可能性。<details>
<summary>Abstract</summary>
Bidirectional attention $\unicode{x2013}$ composed of self-attention with positional encodings and the masked language model (MLM) objective $\unicode{x2013}$ has emerged as a key component of modern large language models (LLMs). Despite its empirical success, few studies have examined its statistical underpinnings: What statistical model is bidirectional attention implicitly fitting? What sets it apart from its non-attention predecessors? We explore these questions in this paper. The key observation is that fitting a single-layer single-head bidirectional attention, upon reparameterization, is equivalent to fitting a continuous bag of words (CBOW) model with mixture-of-experts (MoE) weights. Further, bidirectional attention with multiple heads and multiple layers is equivalent to stacked MoEs and a mixture of MoEs, respectively. This statistical viewpoint reveals the distinct use of MoE in bidirectional attention, which aligns with its practical effectiveness in handling heterogeneous data. It also suggests an immediate extension to categorical tabular data, if we view each word location in a sentence as a tabular feature. Across empirical studies, we find that this extension outperforms existing tabular extensions of transformers in out-of-distribution (OOD) generalization. Finally, this statistical perspective of bidirectional attention enables us to theoretically characterize when linear word analogies are present in its word embeddings. These analyses show that bidirectional attention can require much stronger assumptions to exhibit linear word analogies than its non-attention predecessors.
</details>
<details>
<summary>摘要</summary>
bidirectional attention $\unicode{x2013}$ 由自我注意与位置编码和伪语言模型（MLM）目标组成， $\unicode{x2013}$ 在现代大语言模型（LLM）中发展为关键组成部分。尽管其实际成功，但很少研究其统计基础：bidirectional attention隐式地适应哪种统计模型？与非注意前辈相比，它有什么特点？我们在这篇论文中进行了这些问题的探索。关键观察结果表明，单层单头bidirectional attention经重parameterization等价于单个单元词语（CBOW）模型中的混合专家（MoE）权重。此外，bidirectional attention有多个头和多层，等价于堆叠的MoE和混合MoE。这种统计视角显示bidirectional attention中MoE的独特使用，与其实际效果在处理不同数据有关。它还建议了将每个句子中的单词位置视为一个表格特征，对 tabular数据进行扩展。在实验研究中，我们发现这种扩展在不同数据集上的OOD泛化性能高于现有的表transformer扩展。最后，这种统计观点对bidirectional attention的word embeddings中的线性单词 analogies进行了理论 caracterization。这些分析表明，bidirectional attention可能需要更强的假设，以便在其word embeddings中出现线性单词 analogies，与非注意前辈相比。
</details></li>
</ul>
<hr>
<h2 id="How-is-Fatherhood-Framed-Online-in-Singapore"><a href="#How-is-Fatherhood-Framed-Online-in-Singapore" class="headerlink" title="How is Fatherhood Framed Online in Singapore?"></a>How is Fatherhood Framed Online in Singapore?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04053">http://arxiv.org/abs/2307.04053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tran Hien Van, Abhay Goyal, Muhammad Siddique, Lam Yin Cheung, Nimay Parekh, Jonathan Y Huang, Keri McCrickerd, Edson C Tandoc Jr., Gerard Chung, Navin Kumar</li>
<li>for: 研究 singapore 父亲身份的框架，以帮助制定有关父亲政策。</li>
<li>methods: 使用 NLP 技术分析 singapore 在线环境中 fatherhood 的框架，包括新闻报道、Parenting 讨论区和 Twitter。</li>
<li>results: 发现 singapore 父亲在线环境中的框架并不仅仅是中心的家庭单位。<details>
<summary>Abstract</summary>
The proliferation of discussion about fatherhood in Singapore attests to its significance, indicating the need for an exploration of how fatherhood is framed, aiding policy-making around fatherhood in Singapore. Sound and holistic policy around fatherhood in Singapore may reduce stigma and apprehension around being a parent, critical to improving the nations flagging birth rate. We analyzed 15,705 articles and 56,221 posts to study how fatherhood is framed in Singapore across a range of online platforms (news outlets, parenting forums, Twitter). We used NLP techniques to understand these differences. While fatherhood was framed in a range of ways on the Singaporean online environment, it did not seem that fathers were framed as central to the Singaporean family unit. A strength of our work is how the different techniques we have applied validate each other.
</details>
<details>
<summary>摘要</summary>
《父亲权利在新加坡的普及和讨论，证明了它的重要性，表明了需要对父亲权利的框架进行研究，以便在新加坡制定有效的父亲权利政策。如果制定有效的父亲权利政策，可能会减轻父母身份的偏见和担忧，从而改善新加坡的出生率。我们分析了15705篇文章和56221个微博，以研究新加坡在线环境中 fatherhood 的框架。我们使用自然语言处理技术来理解这些差异。虽然在新加坡的在线环境中 fatherhood 被框架在不同的方式，但是父亲并不被视为新加坡家庭单位的中心。我们的工作的一个优点是我们所应用的不同技术都能够证实Each other。》
</details></li>
</ul>
<hr>
<h2 id="Can-LLMs-be-Good-Financial-Advisors-An-Initial-Study-in-Personal-Decision-Making-for-Optimized-Outcomes"><a href="#Can-LLMs-be-Good-Financial-Advisors-An-Initial-Study-in-Personal-Decision-Making-for-Optimized-Outcomes" class="headerlink" title="Can LLMs be Good Financial Advisors?: An Initial Study in Personal Decision Making for Optimized Outcomes"></a>Can LLMs be Good Financial Advisors?: An Initial Study in Personal Decision Making for Optimized Outcomes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07422">http://arxiv.org/abs/2307.07422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kausik Lakkaraju, Sai Krishna Revanth Vuruma, Vishal Pallagani, Bharath Muppasani, Biplav Srivastava</li>
<li>for: 本研究旨在 investigate LLM-based chatbot在个人财务领域的表现，即银行在推广金融包容方面的着点。</li>
<li>methods: 我们问了13个问题，代表银行个人财务产品，包括银行账户、信用卡和证券，以及这些产品之间的交互和决策。我们还使用不同的方言和语言（英语、非裔美洲语言和telugu）进行询问。</li>
<li>results: 我们发现，虽然chatbot的输出具有流畅和可能性，但还存在提供准确和可靠的金融信息的重要缺陷。<details>
<summary>Abstract</summary>
Increasingly powerful Large Language Model (LLM) based chatbots, like ChatGPT and Bard, are becoming available to users that have the potential to revolutionize the quality of decision-making achieved by the public. In this context, we set out to investigate how such systems perform in the personal finance domain, where financial inclusion has been an overarching stated aim of banks for decades. We asked 13 questions representing banking products in personal finance: bank account, credit card, and certificate of deposits and their inter-product interactions, and decisions related to high-value purchases, payment of bank dues, and investment advice, and in different dialects and languages (English, African American Vernacular English, and Telugu). We find that although the outputs of the chatbots are fluent and plausible, there are still critical gaps in providing accurate and reliable financial information using LLM-based chatbots.
</details>
<details>
<summary>摘要</summary>
增强的大语言模型（LLM）基于的聊天机器人，如ChatGPT和Bard，正在用户处得到普及，它们有可能革命化公众做出决策的质量。在这个上下文下，我们进行了评估这些系统在个人财务领域的表现，其中财务包括银行产品的问题和 dialects and languages（英语、非洲美国黑话和泰卢固）。我们发现虽然聊天机器人的输出具有流畅和合理的特点，但仍然存在提供准确和可靠的金融信息的关键缺失。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Cross-Lingual-Summarization-A-Corpus-based-Study-and-A-New-Benchmark-with-Improved-Annotation"><a href="#Revisiting-Cross-Lingual-Summarization-A-Corpus-based-Study-and-A-New-Benchmark-with-Improved-Annotation" class="headerlink" title="Revisiting Cross-Lingual Summarization: A Corpus-based Study and A New Benchmark with Improved Annotation"></a>Revisiting Cross-Lingual Summarization: A Corpus-based Study and A New Benchmark with Improved Annotation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04018">http://arxiv.org/abs/2307.04018</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cylnlp/convsumx">https://github.com/cylnlp/convsumx</a></li>
<li>paper_authors: Yulong Chen, Huajian Zhang, Yijie Zhou, Xuefeng Bai, Yueguan Wang, Ming Zhong, Jianhao Yan, Yafu Li, Judy Li, Michael Zhu, Yue Zhang</li>
<li>for: 这个论文的目的是提出一个新的跨语言概要标准 benchmark，以及一种基于人工标注的 Two-Step 方法，以优化跨语言概要的生成。</li>
<li>methods: 这个论文使用了一种新的注意力机制，以及一种基于 conversation 和概要的 Two-Step 方法，来模型跨语言概要。</li>
<li>results: 实验结果显示，Two-Step 方法在 ConvSumX 下表现出色，并且在人工评估中也获得了好几个比较高的评分。分析表明，输入文本和概要都是模型跨语言概要的关键因素。<details>
<summary>Abstract</summary>
Most existing cross-lingual summarization (CLS) work constructs CLS corpora by simply and directly translating pre-annotated summaries from one language to another, which can contain errors from both summarization and translation processes. To address this issue, we propose ConvSumX, a cross-lingual conversation summarization benchmark, through a new annotation schema that explicitly considers source input context. ConvSumX consists of 2 sub-tasks under different real-world scenarios, with each covering 3 language directions. We conduct thorough analysis on ConvSumX and 3 widely-used manually annotated CLS corpora and empirically find that ConvSumX is more faithful towards input text. Additionally, based on the same intuition, we propose a 2-Step method, which takes both conversation and summary as input to simulate human annotation process. Experimental results show that 2-Step method surpasses strong baselines on ConvSumX under both automatic and human evaluation. Analysis shows that both source input text and summary are crucial for modeling cross-lingual summaries.
</details>
<details>
<summary>摘要</summary>
现有跨语言概要（CLS）工作通常通过直接翻译已经翻译过的概要来构建CLS corpora，这可能会包含翻译和概要过程中的错误。为解决这个问题，我们提出了ConvSumX，一个跨语言对话概要benchmark，通过一种新的注释schema来考虑原始输入上下文。ConvSumX包括2个子任务，每个覆盖3种语言方向，并进行了对ConvSumX和3种手动注释CLS corpora的 thorought的分析。我们发现ConvSumX更 faithful于输入文本。此外，基于同一种INTUITION，我们提出了2步方法，该方法接受对话和概要作为输入，以模拟人工注释过程。实验结果显示，2步方法在ConvSumX上超过了强基eline。分析表明，原始输入文本和概要都是模型跨语言概要的关键因素。
</details></li>
</ul>
<hr>
<h2 id="Toward-Interactive-Dictation"><a href="#Toward-Interactive-Dictation" class="headerlink" title="Toward Interactive Dictation"></a>Toward Interactive Dictation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04008">http://arxiv.org/abs/2307.04008</a></li>
<li>repo_url: None</li>
<li>paper_authors: Belinda Z. Li, Jason Eisner, Adam Pauls, Sam Thomson</li>
<li>for: 这个论文旨在研究让用户在 dictation 模式下可以通过语音编辑命令来修改文本。</li>
<li>methods: 该论文使用了大型预训练语言模型，并在实时模式下对 spoken 语音进行分类和 segmentation，以便在 dictation 模式下进行语音编辑。</li>
<li>results: 实验显示，使用大型模型可以 дости得30%的终态准确率，但是也会增加延迟。同时，使用较小的模型可以降低延迟，但是终态准确率也会降低至55%。<details>
<summary>Abstract</summary>
Voice dictation is an increasingly important text input modality. Existing systems that allow both dictation and editing-by-voice restrict their command language to flat templates invoked by trigger words. In this work, we study the feasibility of allowing users to interrupt their dictation with spoken editing commands in open-ended natural language. We introduce a new task and dataset, TERTiUS, to experiment with such systems. To support this flexibility in real-time, a system must incrementally segment and classify spans of speech as either dictation or command, and interpret the spans that are commands. We experiment with using large pre-trained language models to predict the edited text, or alternatively, to predict a small text-editing program. Experiments show a natural trade-off between model accuracy and latency: a smaller model achieves 30% end-state accuracy with 1.3 seconds of latency, while a larger model achieves 55% end-state accuracy with 7 seconds of latency.
</details>
<details>
<summary>摘要</summary>
“对话输入模式在不断增长的重要性。现有系统允许Dictation和Speech editing Command的混合使用，但是它们仅允许使用特定的 trigger word invoked flat templates。在这个工作中，我们研究了允许用户在Dictation中间点击 spoken editing Command的可能性。我们引入了一个新的任务和数据集TERTIUS，以便实验这些系统。为了在实时支持这种自由性，一个系统需要逐条分析和识别语音为Dictation或Command，并将这些条件转换为编辑文本。我们尝试使用大型预训语言模型来预测编辑文本，或者alternatively，预测小的文本编辑程式。实验结果表明，这种自然的贸易在精度和延迟之间：一个较小的模型可以在1.3秒的延迟时间下达到30%的终端精度，而一个较大的模型可以在7秒的延迟时间下达到55%的终端精度。”
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/09/cs.CL_2023_07_09/" data-id="closbrolf007h0g88hzs67bhb" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/09/cs.LG_2023_07_09/" class="article-date">
  <time datetime="2023-07-09T10:00:00.000Z" itemprop="datePublished">2023-07-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/09/cs.LG_2023_07_09/">cs.LG - 2023-07-09</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Investigating-the-Edge-of-Stability-Phenomenon-in-Reinforcement-Learning"><a href="#Investigating-the-Edge-of-Stability-Phenomenon-in-Reinforcement-Learning" class="headerlink" title="Investigating the Edge of Stability Phenomenon in Reinforcement Learning"></a>Investigating the Edge of Stability Phenomenon in Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04210">http://arxiv.org/abs/2307.04210</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rares Iordan, Marc Peter Deisenroth, Mihaela Rosca</li>
<li>for: 这个论文研究了深度学习 Reinforcement Learning（RL）中的优化动力学特性，具体来说是Off-policy Q-learning算法在不同数据域中的表现。</li>
<li>methods: 该论文使用了全批量梯度下降with momentum方法，并对不同的损失函数进行了比较。</li>
<li>results: 研究发现，尽管RL和supervised learning在一些方面存在差异，但是在某些情况下可以看到类似的优化动力学特性，例如Edge of stability现象。此外，研究发现DQN使用的Huber损失函数会导致更强的Edge of stability效应，而C51使用的cross entropy损失函数则没有这种效应。<details>
<summary>Abstract</summary>
Recent progress has been made in understanding optimisation dynamics in neural networks trained with full-batch gradient descent with momentum with the uncovering of the edge of stability phenomenon in supervised learning. The edge of stability phenomenon occurs as the leading eigenvalue of the Hessian reaches the divergence threshold of the underlying optimisation algorithm for a quadratic loss, after which it starts oscillating around the threshold, and the loss starts to exhibit local instability but decreases over long time frames. In this work, we explore the edge of stability phenomenon in reinforcement learning (RL), specifically off-policy Q-learning algorithms across a variety of data regimes, from offline to online RL. Our experiments reveal that, despite significant differences to supervised learning, such as non-stationarity of the data distribution and the use of bootstrapping, the edge of stability phenomenon can be present in off-policy deep RL. Unlike supervised learning, however, we observe strong differences depending on the underlying loss, with DQN -- using a Huber loss -- showing a strong edge of stability effect that we do not observe with C51 -- using a cross entropy loss. Our results suggest that, while neural network structure can lead to optimisation dynamics that transfer between problem domains, certain aspects of deep RL optimisation can differentiate it from domains such as supervised learning.
</details>
<details>
<summary>摘要</summary>
In this work, we explore the edge of stability phenomenon in reinforcement learning (RL), specifically off-policy Q-learning algorithms, across various data regimes, from offline to online RL. Our experiments show that, despite significant differences between supervised learning and RL, such as non-stationarity of the data distribution and the use of bootstrapping, the edge of stability phenomenon can still be present in off-policy deep RL.However, we observe strong differences depending on the underlying loss function. Specifically, we find that DQN, which uses a Huber loss, exhibits a strong edge of stability effect, while C51, which uses a cross entropy loss, does not. Our results suggest that while neural network structure can lead to optimization dynamics that transfer between problem domains, certain aspects of deep RL optimization can differentiate it from domains such as supervised learning.
</details></li>
</ul>
<hr>
<h2 id="On-the-Challenges-of-Deploying-Privacy-Preserving-Synthetic-Data-in-the-Enterprise"><a href="#On-the-Challenges-of-Deploying-Privacy-Preserving-Synthetic-Data-in-the-Enterprise" class="headerlink" title="On the Challenges of Deploying Privacy-Preserving Synthetic Data in the Enterprise"></a>On the Challenges of Deploying Privacy-Preserving Synthetic Data in the Enterprise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04208">http://arxiv.org/abs/2307.04208</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lauren Arthur, Jason Costello, Jonathan Hardy, Will O’Brien, James Rea, Gareth Rees, Georgi Ganev</li>
<li>for: 本研究探讨了在企业环境中部署生成数据所附带的挑战，特别是隐私问题。</li>
<li>methods: 本研究使用了5个主要类别的挑战系统化分类：生成、基础设施与架构、治理、合规与法规、采用。</li>
<li>results: 研究提出了一种策略性和系统性的方法，可以帮助企业有效地解决挑战并实现目标，同时建立生成数据解决方案的信任。<details>
<summary>Abstract</summary>
Generative AI technologies are gaining unprecedented popularity, causing a mix of excitement and apprehension through their remarkable capabilities. In this paper, we study the challenges associated with deploying synthetic data, a subfield of Generative AI. Our focus centers on enterprise deployment, with an emphasis on privacy concerns caused by the vast amount of personal and highly sensitive data. We identify 40+ challenges and systematize them into five main groups -- i) generation, ii) infrastructure & architecture, iii) governance, iv) compliance & regulation, and v) adoption. Additionally, we discuss a strategic and systematic approach that enterprises can employ to effectively address the challenges and achieve their goals by establishing trust in the implemented solutions.
</details>
<details>
<summary>摘要</summary>
<<SYS>>随着生成AI技术的普及，人们对其卓越的能力感到激动和略有拘懑。在这篇论文中，我们研究了部署生成数据的挑战，这是生成AI技术的一个子领域。我们的关注点是企业部署，强调个人隐私问题，由于巨量的个人隐私数据。我们识别出40多个挑战，并将它们分为五个主要类别：一、生成；二、基础设施与架构；三、管理；四、合规与法规；五、采纳。此外，我们还讨论了企业可以采用的战略和系统性的方法，以确保实施解决方案的可靠性和效果。
</details></li>
</ul>
<hr>
<h2 id="Extending-the-Forward-Forward-Algorithm"><a href="#Extending-the-Forward-Forward-Algorithm" class="headerlink" title="Extending the Forward Forward Algorithm"></a>Extending the Forward Forward Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04205">http://arxiv.org/abs/2307.04205</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ads-cmu/forwardforward">https://github.com/ads-cmu/forwardforward</a></li>
<li>paper_authors: Saumya Gandhi, Ritu Gala, Jonah Kornberg, Advaith Sridhar</li>
<li>For: The paper is written to propose and experiment with the Forward Forward algorithm, a novel method for training neural networks as an alternative to backpropagation.* Methods: The paper uses the MNIST dataset to replicate Hinton’s experiments and extend the scope of the method with two significant contributions: establishing a baseline performance on the IMDb movie reviews dataset for sentiment analysis, and introducing a novel pyramidal optimization strategy for the loss threshold.* Results: The paper shows that the Forward Forward network achieves a good performance on the sentiment analysis task, with a test error difference of up to 8% compared to the baseline method. Additionally, the paper visualizes the trained parameters and derives several significant insights, such as a notably larger mean and variance in the weights acquired by the Forward Forward network.Here is the same information in Simplified Chinese text:* For: 该文章是为了介绍和实验Forward Forward算法，一种用于训练神经网络的新方法，作为对backpropagation的替代方案。* Methods: 文章使用MNIST数据集来重现Hinton的实验，并对方法进行了两项重要贡献：在IMDb电影评论数据集上建立了一个基准性表现，并介绍了一种新的pyramidal优化策略来处理损失阈值。* Results: 文章显示，Forward Forward网络在情感分析任务上达到了一个不错的性能，与基准方法相比，测试错误差异可达8%。文章还可视化了训练参数并得出了一些重要的发现，如Forward Forward网络的 weights 的平均值和标准差较大（10-20倍）。<details>
<summary>Abstract</summary>
The Forward Forward algorithm, proposed by Geoffrey Hinton in November 2022, is a novel method for training neural networks as an alternative to backpropagation. In this project, we replicate Hinton's experiments on the MNIST dataset, and subsequently extend the scope of the method with two significant contributions. First, we establish a baseline performance for the Forward Forward network on the IMDb movie reviews dataset. As far as we know, our results on this sentiment analysis task marks the first instance of the algorithm's extension beyond computer vision. Second, we introduce a novel pyramidal optimization strategy for the loss threshold - a hyperparameter specific to the Forward Forward method. Our pyramidal approach shows that a good thresholding strategy causes a difference of up to 8% in test error. Lastly, we perform visualizations of the trained parameters and derived several significant insights, such as a notably larger (10-20x) mean and variance in the weights acquired by the Forward Forward network.   Repository: https://github.com/Ads-cmu/ForwardForward
</details>
<details>
<summary>摘要</summary>
“对前进对方法”，由 Geoffrey Hinton 在2022年11月提出，是一种可以作为后向传播的训练神经网络的新方法。在这个项目中，我们重现了 Hinton 的实验，并将其推广到两个重要的贡献。首先，我们建立了 Forward Forward 网络在 IMDb 电影评论数据集上的基准性能。我们知道，这是 Forward Forward 方法在 Computer Vision 以外的第一个应用。其次，我们导入了一个新的 pyramidal 优化策略，用于损失阈值（一个特有的 Forward Forward 方法参数）。我们的 pyramidal 方法显示，一个好的阈值选择可以导致测试错误下降8%。最后，我们进行了网络参数的训练和分析，获得了一些重要的见解，例如 Forward Forward 网络获得的平均和方差都是10-20倍于传统的神经网络。Repository：https://github.com/Ads-cmu/ForwardForward
</details></li>
</ul>
<hr>
<h2 id="Trajectory-Alignment-Understanding-the-Edge-of-Stability-Phenomenon-via-Bifurcation-Theory"><a href="#Trajectory-Alignment-Understanding-the-Edge-of-Stability-Phenomenon-via-Bifurcation-Theory" class="headerlink" title="Trajectory Alignment: Understanding the Edge of Stability Phenomenon via Bifurcation Theory"></a>Trajectory Alignment: Understanding the Edge of Stability Phenomenon via Bifurcation Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04204">http://arxiv.org/abs/2307.04204</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minhak Song, Chulhee Yun</li>
<li>for: 本研究 empirically studies the evolution of the largest eigenvalue of the loss Hessian during gradient descent (GD) training, and observes a phenomenon called Edge of Stability (EoS).</li>
<li>methods: 本研究使用 empirical studies and rigorous proof to demonstrate the phenomenon of trajectory alignment on a specific bifurcation diagram, independent of initialization, when EoS occurs.</li>
<li>results: 研究发现，在进行GD训练时，大estenvalue of loss Hessian在初期phasereceives a sharp increase (referred to as progressive sharpening), eventually saturating close to the threshold of $2 &#x2F; \text{(step size)}$. Additionally, the study establishes the trajectory alignment phenomenon for a two-layer fully-connected linear network and a single-neuron nonlinear network trained with a single data point.<details>
<summary>Abstract</summary>
Cohen et al. (2021) empirically study the evolution of the largest eigenvalue of the loss Hessian, also known as sharpness, along the gradient descent (GD) trajectory and observe a phenomenon called the Edge of Stability (EoS). The sharpness increases at the early phase of training (referred to as progressive sharpening), and eventually saturates close to the threshold of $2 / \text{(step size)}$. In this paper, we start by demonstrating through empirical studies that when the EoS phenomenon occurs, different GD trajectories (after a proper reparameterization) align on a specific bifurcation diagram independent of initialization. We then rigorously prove this trajectory alignment phenomenon for a two-layer fully-connected linear network and a single-neuron nonlinear network trained with a single data point. Our trajectory alignment analysis establishes both progressive sharpening and EoS phenomena, encompassing and extending recent findings in the literature.
</details>
<details>
<summary>摘要</summary>
科恩等（2021）employs empirical studies to investigate the evolution of the largest eigenvalue of the loss Hessian, also known as sharpness, along the gradient descent (GD) trajectory, and observe a phenomenon called the Edge of Stability (EoS). The sharpness initially increases during the early phase of training (referred to as progressive sharpening) and eventually saturates near the threshold of $2 / (\text{step size})$.在本文中，我们首先通过实验研究发现，当EoS现象出现时，不同的GD轨迹（经过适当的重parameterization）在独立于初始化的情况下归一化到特定的分岔图表上。然后，我们正式证明了这种轨迹归一化现象，并且覆盖了 reciently的 literatura 中的发现。我们的轨迹归一化分析确立了进攻性增强和EoS现象，这包括并推广了现有的 literatura 中的发现。
</details></li>
</ul>
<hr>
<h2 id="On-the-sample-complexity-of-estimation-in-logistic-regression"><a href="#On-the-sample-complexity-of-estimation-in-logistic-regression" class="headerlink" title="On the sample complexity of estimation in logistic regression"></a>On the sample complexity of estimation in logistic regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04191">http://arxiv.org/abs/2307.04191</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Hsu, Arya Mazumdar</li>
<li>for: 这 paper 的目的是研究逻辑回归模型在噪声二分类问题中的样本复杂度。</li>
<li>methods: 这 paper 使用了标准正态分布的covariates，并研究了逻辑回归模型参数的估计样本复杂度，以及逻辑回归模型参数的估计与错误和 inverse temperature 之间的关系。</li>
<li>results: 这 paper 发现了逻辑回归模型参数估计的样本复杂度curve 有两个变点（或 kritical points），这些变点可以清晰地分类三个温度范围：低温、中温和高温。<details>
<summary>Abstract</summary>
The logistic regression model is one of the most popular data generation model in noisy binary classification problems. In this work, we study the sample complexity of estimating the parameters of the logistic regression model up to a given $\ell_2$ error, in terms of the dimension and the inverse temperature, with standard normal covariates. The inverse temperature controls the signal-to-noise ratio of the data generation process. While both generalization bounds and asymptotic performance of the maximum-likelihood estimator for logistic regression are well-studied, the non-asymptotic sample complexity that shows the dependence on error and the inverse temperature for parameter estimation is absent from previous analyses. We show that the sample complexity curve has two change-points (or critical points) in terms of the inverse temperature, clearly separating the low, moderate, and high temperature regimes.
</details>
<details>
<summary>摘要</summary>
“逻辑回传模型是数据生成模型中最受欢迎的一种，尤其在噪音 binary 分类问题中。在这个工作中，我们研究了逻辑回传模型参数的估计，在给定的 $\ell_2$ 误差下，与维度和逆温度的关系。使用标准正常分布的 covariates。逆温度控制了数据生成过程中的信号噪音比例。 Although the generalization bounds and asymptotic performance of the maximum-likelihood estimator for logistic regression have been well-studied, the non-asymptotic sample complexity that shows the dependence on error and the inverse temperature for parameter estimation is absent from previous analyses. We show that the sample complexity curve has two change-points (or critical points) in terms of the inverse temperature, clearly separating the low, moderate, and high temperature regimes.”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Review-of-feedback-in-Automated-Essay-Scoring"><a href="#Review-of-feedback-in-Automated-Essay-Scoring" class="headerlink" title="Review of feedback in Automated Essay Scoring"></a>Review of feedback in Automated Essay Scoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05553">http://arxiv.org/abs/2307.05553</a></li>
<li>repo_url: None</li>
<li>paper_authors: You-Jin Jong, Yong-Jin Kim, Ok-Chol Ri</li>
<li>for: 这篇论文主要写于自动评分系统的发展和其在改善写作技巧方面的应用。</li>
<li>methods: 这篇论文主要采用了对已有的自动评分系统研究的回顾和对各种反馈类型和文章特点的分析。</li>
<li>results: 论文认为反馈是自动评分系统的关键组成部分，可以帮助用户改善写作技巧。<details>
<summary>Abstract</summary>
The first automated essay scoring system was developed 50 years ago. Automated essay scoring systems are developing into systems with richer functions than the previous simple scoring systems. Its purpose is not only to score essays but also as a learning tool to improve the writing skill of users. Feedback is the most important aspect of making an automated essay scoring system useful in real life. The importance of feedback was already emphasized in the first AES system. This paper reviews research on feedback including different feedback types and essay traits on automated essay scoring. We also reviewed the latest case studies of the automated essay scoring system that provides feedback.
</details>
<details>
<summary>摘要</summary>
50 年前开发出了首个自动评分文章系统。现在的自动评分系统不仅可以评分文章，还可以作为学习工具，帮助用户提升写作技巧。回馈是自动评分系统在实际应用中的重要方面。这篇文章检视了不同类型的回馈和文章特征在自动评分系统中的研究。我们也检视了最新的自动评分系统提供的回馈的案例研究。
</details></li>
</ul>
<hr>
<h2 id="Latent-Graph-Attention-for-Enhanced-Spatial-Context"><a href="#Latent-Graph-Attention-for-Enhanced-Spatial-Context" class="headerlink" title="Latent Graph Attention for Enhanced Spatial Context"></a>Latent Graph Attention for Enhanced Spatial Context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04149">http://arxiv.org/abs/2307.04149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ayush Singh, Yash Bhambhu, Himanshu Buckchash, Deepak K. Gupta, Dilip K. Prasad</li>
<li>for: 提高小规模 architecture 的表现，使其 closer to large size architecture，以满足边缘设备的低计算能力和低能耗要求。</li>
<li>methods: 使用 Latent Graph Attention (LGA) 模型，该模型通过构建一个具有Semantic coherence的网络来传递信息，以及引入一种新的对比损失函数来增强学习机制。</li>
<li>results: 在透明物体分割、雾度修复和动态流体估计等三个挑战性应用中，通过 incorporating LGA 提高表现。<details>
<summary>Abstract</summary>
Global contexts in images are quite valuable in image-to-image translation problems. Conventional attention-based and graph-based models capture the global context to a large extent, however, these are computationally expensive. Moreover, the existing approaches are limited to only learning the pairwise semantic relation between any two points on the image. In this paper, we present Latent Graph Attention (LGA) a computationally inexpensive (linear to the number of nodes) and stable, modular framework for incorporating the global context in the existing architectures, especially empowering small-scale architectures to give performance closer to large size architectures, thus making the light-weight architectures more useful for edge devices with lower compute power and lower energy needs. LGA propagates information spatially using a network of locally connected graphs, thereby facilitating to construct a semantically coherent relation between any two spatially distant points that also takes into account the influence of the intermediate pixels. Moreover, the depth of the graph network can be used to adapt the extent of contextual spread to the target dataset, thereby being able to explicitly control the added computational cost. To enhance the learning mechanism of LGA, we also introduce a novel contrastive loss term that helps our LGA module to couple well with the original architecture at the expense of minimal additional computational load. We show that incorporating LGA improves the performance on three challenging applications, namely transparent object segmentation, image restoration for dehazing and optical flow estimation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Survey-on-Figure-Classification-Techniques-in-Scientific-Documents"><a href="#A-Survey-on-Figure-Classification-Techniques-in-Scientific-Documents" class="headerlink" title="A Survey on Figure Classification Techniques in Scientific Documents"></a>A Survey on Figure Classification Techniques in Scientific Documents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05694">http://arxiv.org/abs/2307.05694</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anurag Dhote, Mohammed Javed, David S Doermann</li>
<li>for: This paper is written to provide a systematic review of existing methodologies and data sets for figure classification, with the goal of identifying current research gaps and providing possible directions for future research.</li>
<li>methods: The paper uses a categorization framework to classify figures into five classes - tables, photos, diagrams, maps, and plots - and presents a critical review of existing methodologies and data sets for figure classification.</li>
<li>results: The paper identifies current research gaps in figure classification and provides possible directions for future research, including the need for more diverse and annotated data sets, the development of more sophisticated machine learning algorithms, and the integration of figure classification with other NLP tasks.<details>
<summary>Abstract</summary>
Figures visually represent an essential piece of information and provide an effective means to communicate scientific facts. Recently there have been many efforts toward extracting data directly from figures, specifically from tables, diagrams, and plots, using different Artificial Intelligence and Machine Learning techniques. This is because removing information from figures could lead to deeper insights into the concepts highlighted in the scientific documents. In this survey paper, we systematically categorize figures into five classes - tables, photos, diagrams, maps, and plots, and subsequently present a critical review of the existing methodologies and data sets that address the problem of figure classification. Finally, we identify the current research gaps and provide possible directions for further research on figure classification.
</details>
<details>
<summary>摘要</summary>
Figures 可以用来表示重要信息，并提供一种有效的方式来传递科学知识。近年来，有很多努力在抽取图表中的数据，特别是图表、图形、地图和图表，使用不同的人工智能和机器学习技术。这是因为从图表中抽取信息可以带来更深入的理解科学文献中所提出的概念。在这篇评论文中，我们系统地分类图表为五种类别：表格、照片、图形、地图和图表，然后给出现有的方法和数据集，并评论现有的方法和数据集。最后，我们识别了现有的研究潜在问题，并提供可能的研究方向。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-and-Approach-to-Chart-Classification"><a href="#A-Survey-and-Approach-to-Chart-Classification" class="headerlink" title="A Survey and Approach to Chart Classification"></a>A Survey and Approach to Chart Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04147">http://arxiv.org/abs/2307.04147</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anurag Dhote, Mohammed Javed, David S Doermann</li>
<li>for: 本研究旨在 automatic chart understanding 问题上进行报告和分析，即从chart的类型来自动分类。</li>
<li>methods: 本文使用了传统的机器学习方法、卷积神经网络和变换器来实现chart classification。</li>
<li>results: 我们在使用chartINFO UB-UNITECH PMC数据集进行实验后，发现使用视觉基于 transformer 模型可以达到 chart classification 的state-of-the-art result。<details>
<summary>Abstract</summary>
Charts represent an essential source of visual information in documents and facilitate a deep understanding and interpretation of information typically conveyed numerically. In the scientific literature, there are many charts, each with its stylistic differences. Recently the document understanding community has begun to address the problem of automatic chart understanding, which begins with chart classification. In this paper, we present a survey of the current state-of-the-art techniques for chart classification and discuss the available datasets and their supported chart types. We broadly classify these contributions as traditional approaches based on ML, CNN, and Transformers. Furthermore, we carry out an extensive comparative performance analysis of CNN-based and transformer-based approaches on the recently published CHARTINFO UB-UNITECH PMC dataset for the CHART-Infographics competition at ICPR 2022. The data set includes 15 different chart categories, including 22,923 training images and 13,260 test images. We have implemented a vision-based transformer model that produces state-of-the-art results in chart classification.
</details>
<details>
<summary>摘要</summary>
图表是文档中重要的视觉信息来源，可以帮助人们更深入理解和解释通常用数字形式表示的信息。科学文献中有很多图表，每个图表都有不同的风格。在文档理解社区中，人们已经开始努力解决自动图表理解的问题，开始于图表分类。在这篇论文中，我们提供了当前状态的技术进展和数据集，以及它们支持的图表类型。我们将这些贡献分为传统的机器学习、CNN和Transformers三大类别。此外，我们还进行了对CNN和Transformers两种方法在ICPR 2022年CHARTINFO UB-UNITECH PMC数据集上的比较性能分析。该数据集包括15种不同的图表类别，包括22,923个训练图像和13,260个测试图像。我们实现了一种基于视觉的Transformers模型，可以在图表分类中达到状态之 Art。
</details></li>
</ul>
<hr>
<h2 id="On-The-Impact-of-Machine-Learning-Randomness-on-Group-Fairness"><a href="#On-The-Impact-of-Machine-Learning-Randomness-on-Group-Fairness" class="headerlink" title="On The Impact of Machine Learning Randomness on Group Fairness"></a>On The Impact of Machine Learning Randomness on Group Fairness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04138">http://arxiv.org/abs/2307.04138</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Prakhar Ganesh, Hongyan Chang, Martin Strobel, Reza Shokri</li>
<li>for: 这篇论文探讨了机器学习算法中的群体公平性指标，以及这些指标在不同实验实例中的不稳定性。</li>
<li>methods: 作者研究了不同训练实例中机器学习算法的学习过程中的随机性对群体公平性指标的影响。</li>
<li>results: 研究发现，群体公平性指标的不稳定性主要来自于训练过程中数据顺序的随机性，并且可以通过改变数据顺序的一个epoch来控制群体级别准确率，无需影响模型的总性能。<details>
<summary>Abstract</summary>
Statistical measures for group fairness in machine learning reflect the gap in performance of algorithms across different groups. These measures, however, exhibit a high variance between different training instances, which makes them unreliable for empirical evaluation of fairness. What causes this high variance? We investigate the impact on group fairness of different sources of randomness in training neural networks. We show that the variance in group fairness measures is rooted in the high volatility of the learning process on under-represented groups. Further, we recognize the dominant source of randomness as the stochasticity of data order during training. Based on these findings, we show how one can control group-level accuracy (i.e., model fairness), with high efficiency and negligible impact on the model's overall performance, by simply changing the data order for a single epoch.
</details>
<details>
<summary>摘要</summary>
machine learning中的统计方法可以反映不同群体的性能差异。然而，这些度量值具有高度的变化性，使其在实际评估公平性方面不可靠。我们调查了训练神经网络时不同来源的随机性对集体公平性的影响。我们发现，这种变化源于训练过程中少数群体的学习过程的高度波动性。此外，我们发现主要的随机性来源是训练过程中数据顺序的随机性。基于这些发现，我们展示了如何通过单个轮数据的重新排序来控制集体精度（即模型公平性），并且可以高效地、无损到模型总性能。
</details></li>
</ul>
<hr>
<h2 id="Graph-Neural-Network-enabled-Terahertz-based-Flow-guided-Nanoscale-Localization"><a href="#Graph-Neural-Network-enabled-Terahertz-based-Flow-guided-Nanoscale-Localization" class="headerlink" title="Graph Neural Network-enabled Terahertz-based Flow-guided Nanoscale Localization"></a>Graph Neural Network-enabled Terahertz-based Flow-guided Nanoscale Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05551">http://arxiv.org/abs/2307.05551</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gerard Calvo Bartra, Filip Lemic, Sergi Abadal, Xavier Costa Perez</li>
<li>for: 这篇论文旨在提出一种基于图神经网络（GNN）的流动导向地方化方法，以提高心血管系统中事件的地点确定精度和覆盖率。</li>
<li>methods: 该方法使用GNN来地图化流动数据，并通过图神经网络的学习来实现事件的地点确定。</li>
<li>results: 相比现有状态的方法，该方法可以提高地点确定精度和覆盖率，并且提供了一些设计指南 дляGNN启用的流动导向地方化。<details>
<summary>Abstract</summary>
Scientific advancements in nanotechnology and advanced materials are paving the way toward nanoscale devices for in-body precision medicine; comprising integrated sensing, computing, communication, data and energy storage capabilities. In the human cardiovascular system, such devices are envisioned to be passively flowing and continuously sensing for detecting events of diagnostic interest. The diagnostic value of detecting such events can be enhanced by assigning to them their physical locations (e.g., body region), which is the main proposition of flow-guided localization. Current flow-guided localization approaches suffer from low localization accuracy and they are by-design unable to localize events within the entire cardiovascular system. Toward addressing this issue, we propose the utilization of Graph Neural Networks (GNNs) for this purpose, and demonstrate localization accuracy and coverage enhancements of our proposal over the existing State of the Art (SotA) approaches. Based on our evaluation, we provide several design guidelines for GNN-enabled flow-guided localization.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Carbon-Efficient-Neural-Architecture-Search"><a href="#Carbon-Efficient-Neural-Architecture-Search" class="headerlink" title="Carbon-Efficient Neural Architecture Search"></a>Carbon-Efficient Neural Architecture Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04131">http://arxiv.org/abs/2307.04131</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiyang Zhao, Tian Guo</li>
<li>for: 提高模型设计过程中的碳效率和能效性</li>
<li>methods: 使用不同能量需求的 NAS评估算法、多目标优化器和启发式GPU分配策略</li>
<li>results: 在使用最新的 NASbenchmark数据集和两个碳轨迹下，CE-NAS在碳效率和搜索效率方面表现更好 than三个基eline<details>
<summary>Abstract</summary>
This work presents a novel approach to neural architecture search (NAS) that aims to reduce energy costs and increase carbon efficiency during the model design process. The proposed framework, called carbon-efficient NAS (CE-NAS), consists of NAS evaluation algorithms with different energy requirements, a multi-objective optimizer, and a heuristic GPU allocation strategy. CE-NAS dynamically balances energy-efficient sampling and energy-consuming evaluation tasks based on current carbon emissions. Using a recent NAS benchmark dataset and two carbon traces, our trace-driven simulations demonstrate that CE-NAS achieves better carbon and search efficiency than the three baselines.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这个工作提出了一种新的神经建筑搜索（NAS）方法，旨在降低能耗成本和提高碳素效率 durante 模型设计过程。提出的框架，叫做碳效 NAS（CE-NAS），包括 NAS 评估算法不同的能耗要求，多目标优化器，和一种启发式 GPU 分配策略。CE-NAS 动态均衡能效取样和能耗评估任务基于当前碳排放。使用最新的 NAS benchmark 数据集和两个碳轨迹，我们的轨迹驱动 simulations 显示，CE-NAS 可以比基准三个实现更好的碳和搜索效率。
</details></li>
</ul>
<hr>
<h2 id="A-Deep-Learning-Framework-for-Solving-Hyperbolic-Partial-Differential-Equations-Part-I"><a href="#A-Deep-Learning-Framework-for-Solving-Hyperbolic-Partial-Differential-Equations-Part-I" class="headerlink" title="A Deep Learning Framework for Solving Hyperbolic Partial Differential Equations: Part I"></a>A Deep Learning Framework for Solving Hyperbolic Partial Differential Equations: Part I</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04121">http://arxiv.org/abs/2307.04121</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rajat Arora</li>
<li>for: 这个研究旨在开发一种基于物理知识的深度学习框架，用于精确地近似非线性偏微分方程（PDE）的解。</li>
<li>methods: 该框架采用了基于物理的偏微分方程（PDE）的近似方法，并利用了finite element法来解决问题。</li>
<li>results: 经过numerical experiment和analytical解的验证，该框架能够准确地近似非线性偏微分方程（PDE）的解，并能够自动处理boundary condition和entropy condition等约束。<details>
<summary>Abstract</summary>
Physics informed neural networks (PINNs) have emerged as a powerful tool to provide robust and accurate approximations of solutions to partial differential equations (PDEs). However, PINNs face serious difficulties and challenges when trying to approximate PDEs with dominant hyperbolic character. This research focuses on the development of a physics informed deep learning framework to approximate solutions to nonlinear PDEs that can develop shocks or discontinuities without any a-priori knowledge of the solution or the location of the discontinuities. The work takes motivation from finite element method that solves for solution values at nodes in the discretized domain and use these nodal values to obtain a globally defined solution field. Built on the rigorous mathematical foundations of the discontinuous Galerkin method, the framework naturally handles imposition of boundary conditions (Neumann/Dirichlet), entropy conditions, and regularity requirements. Several numerical experiments and validation with analytical solutions demonstrate the accuracy, robustness, and effectiveness of the proposed framework.
</details>
<details>
<summary>摘要</summary>
物理学 Informed neural networks (PINNs) 已成为解决部分偏微分方程 (PDEs) 的强大工具，但 PINNs 对具有主要浮点特征的 PDEs 颇受阻碍和挑战。这些研究将发展一种基于物理学 Informed 深度学习框架，用于非线性 PDEs 的解析，无需任何先验知识或解析解的位置。这个框架取得了 finite element 方法的灵感，解决方案值在分解区域中的节点，然后使用这些节点值获得全局定义的解场。建立在精确的数学基础上的步骤，该框架自然处理边界条件（Neumann/Dirichlet）、热力学条件和常数性要求。数学实验和与分析解的验证表明该提案的准确性、稳定性和效果。
</details></li>
</ul>
<hr>
<h2 id="FILM-How-can-Few-Shot-Image-Classification-Benefit-from-Pre-Trained-Language-Models"><a href="#FILM-How-can-Few-Shot-Image-Classification-Benefit-from-Pre-Trained-Language-Models" class="headerlink" title="FILM: How can Few-Shot Image Classification Benefit from Pre-Trained Language Models?"></a>FILM: How can Few-Shot Image Classification Benefit from Pre-Trained Language Models?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04114">http://arxiv.org/abs/2307.04114</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Jiang, Yunkai Dang, Dong Pang, Huishuai Zhang, Weiran Huang</li>
<li>for: 增强少样本学习，使模型可以通过几个样本进行扩展到新的类别。</li>
<li>methods: 使用预训练的自然语言模型，基于对比学习来提高模型的泛化能力。</li>
<li>results: 提出了一种新的几shot学习框架，通过精心设计文本分支和 metric 模块，可以更好地利用 semantic 信息，并通过 MAML 进行训练，实现更好的扩展性和传输性。<details>
<summary>Abstract</summary>
Few-shot learning aims to train models that can be generalized to novel classes with only a few samples. Recently, a line of works are proposed to enhance few-shot learning with accessible semantic information from class names. However, these works focus on improving existing modules such as visual prototypes and feature extractors of the standard few-shot learning framework. This limits the full potential use of semantic information. In this paper, we propose a novel few-shot learning framework that uses pre-trained language models based on contrastive learning. To address the challenge of alignment between visual features and textual embeddings obtained from text-based pre-trained language model, we carefully design the textual branch of our framework and introduce a metric module to generalize the cosine similarity. For better transferability, we let the metric module adapt to different few-shot tasks and adopt MAML to train the model via bi-level optimization. Moreover, we conduct extensive experiments on multiple benchmarks to demonstrate the effectiveness of our method.
</details>
<details>
<summary>摘要</summary>
几个样本学习目标是训练模型可以通过几个样本来泛化到新的类型。最近，一些工作提出了使用可 accessible 的 semantic information from class names 来增强几个样本学习。然而，这些工作通常是对现有模块，如视觉原型和特征提取器，进行改进。这限制了使用 semantic information 的全部潜力。在这篇论文中，我们提出了一种新的几个样本学习框架，使用基于对比学习的预训练语言模型。为 Address the challenge of 对 visual features 和 textual embeddings 的Alignment，我们仔细设计了文本分支我们的框架，并引入了一个 metric module 来泛化cosine similarity。为了提高转移性，我们让 metric module 适应不同的几个样本任务，并采用 MAML 来训练模型via bi-level optimization。此外，我们在多个 benchmark 上进行了广泛的实验，以证明我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Learning-Space-Time-Continuous-Neural-PDEs-from-Partially-Observed-States"><a href="#Learning-Space-Time-Continuous-Neural-PDEs-from-Partially-Observed-States" class="headerlink" title="Learning Space-Time Continuous Neural PDEs from Partially Observed States"></a>Learning Space-Time Continuous Neural PDEs from Partially Observed States</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04110">http://arxiv.org/abs/2307.04110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Valerii Iakovlev, Markus Heinonen, Harri Lähdesmäki</li>
<li>for: 学习受损和异常观测的 partial differential equations (PDEs) 模型</li>
<li>methods: 提议了一种基于 continous latent neural PDE 模型的可靠概率框架和一种改进的编码设计，以提高数据效率和网格独立性</li>
<li>results: 模型在复杂的synthetic和实际数据集上达到了状态顶峰性，超越了先前的方法和有效地处理受限观测数据，示出其可能性进一步推动数据驱动的 PDE 模型和可靠网格独立的模型化复杂动态过程。<details>
<summary>Abstract</summary>
We introduce a novel grid-independent model for learning partial differential equations (PDEs) from noisy and partial observations on irregular spatiotemporal grids. We propose a space-time continuous latent neural PDE model with an efficient probabilistic framework and a novel encoder design for improved data efficiency and grid independence. The latent state dynamics are governed by a PDE model that combines the collocation method and the method of lines. We employ amortized variational inference for approximate posterior estimation and utilize a multiple shooting technique for enhanced training speed and stability. Our model demonstrates state-of-the-art performance on complex synthetic and real-world datasets, overcoming limitations of previous approaches and effectively handling partially-observed data. The proposed model outperforms recent methods, showing its potential to advance data-driven PDE modeling and enabling robust, grid-independent modeling of complex partially-observed dynamic processes.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的网格独立模型，用于从不稳定和受限的观测数据集上学习部分梯度方程（PDE）。我们提议了一种空间时间连续的潜在神经PDE模型，并使用了一种高效的概率框架和一种新的编码设计以提高数据效率和网格独立性。 latent state动态被由PDE模型控制，这个模型结合了 colocated method和方程eline方法。我们使用了启动变量推理来估计近似 posterior，并使用多个射击技术来提高训练速度和稳定性。我们的模型在复杂的 sintetic和实际数据集上达到了状态艺术性的表现，超越了过去的方法，有效地处理了部分观测数据。我们的模型比最近的方法表现更佳，这表明其可以推进数据驱动PDE模型化，并提供了可靠、网格独立的模型复杂部分观测动态过程。
</details></li>
</ul>
<hr>
<h2 id="Towards-Assumption-free-Bias-Mitigation"><a href="#Towards-Assumption-free-Bias-Mitigation" class="headerlink" title="Towards Assumption-free Bias Mitigation"></a>Towards Assumption-free Bias Mitigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04105">http://arxiv.org/abs/2307.04105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chia-Yuan Chang, Yu-Neng Chuang, Kwei-Herng Lai, Xiaotian Han, Xia Hu, Na Zou</li>
<li>for: 降低机器学习模型的不公正预测行为</li>
<li>methods: 利用自动检测偏聚特征交互来减少不公正预测行为</li>
<li>results: 实验结果表明，提议的框架可以在四个实际 datasets 上减少不公正预测行为，并且不需要假设非敏感特征之间的相关性。<details>
<summary>Abstract</summary>
Despite the impressive prediction ability, machine learning models show discrimination towards certain demographics and suffer from unfair prediction behaviors. To alleviate the discrimination, extensive studies focus on eliminating the unequal distribution of sensitive attributes via multiple approaches. However, due to privacy concerns, sensitive attributes are often either unavailable or missing in real-world scenarios. Therefore, several existing works alleviate the bias without sensitive attributes. Those studies face challenges, either in inaccurate predictions of sensitive attributes or the need to mitigate unequal distribution of manually defined non-sensitive attributes related to bias. The latter requires strong assumptions about the correlation between sensitive and non-sensitive attributes. As data distribution and task goals vary, the strong assumption on non-sensitive attributes may not be valid and require domain expertise. In this work, we propose an assumption-free framework to detect the related attributes automatically by modeling feature interaction for bias mitigation. The proposed framework aims to mitigate the unfair impact of identified biased feature interactions. Experimental results on four real-world datasets demonstrate that our proposed framework can significantly alleviate unfair prediction behaviors by considering biased feature interactions.
</details>
<details>
<summary>摘要</summary>
尽管机器学习模型表现出色，但它们仍然存在对某些人群的歧视行为，并且受到不公正预测行为的影响。为了解决这问题，广泛的研究努力于消除敏感特征的不均衡分布，使用多种方法。然而，由于隐私问题，敏感特征在实际场景中 oftentimes 缺失或未知。因此，现有的研究往往需要快速假设敏感特征的替代方案，以减少不公正的预测行为。然而，这些研究面临着两个挑战：一是不准确预测敏感特征，二是需要调整不公正的非敏感特征，这需要强制假设敏感特征和非敏感特征之间的相关性。由于数据分布和任务目标因素的变化，这些假设可能不正确，需要培训领域专家。在这种情况下，我们提出一种假设自由框架，通过模型特性互动来检测相关特征，以消除不公正的影响。我们的提议框架可以减少不公正预测行为，并且实际试验结果表明，在四个真实的数据集上，我们的框架可以明显减少不公正预测行为。
</details></li>
</ul>
<hr>
<h2 id="A-generative-flow-for-conditional-sampling-via-optimal-transport"><a href="#A-generative-flow-for-conditional-sampling-via-optimal-transport" class="headerlink" title="A generative flow for conditional sampling via optimal transport"></a>A generative flow for conditional sampling via optimal transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04102">http://arxiv.org/abs/2307.04102</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/giuliotrigila/moonexample">https://github.com/giuliotrigila/moonexample</a></li>
<li>paper_authors: Jason Alfonso, Ricardo Baptista, Anupam Bhakta, Noam Gal, Alfin Hou, Isa Lyubimova, Daniel Pocklington, Josef Sajonz, Giulio Trigila, Ryan Tsai</li>
<li>for: 这个论文是用来解决Conditional sampling的问题的，即在bayesian inference和density estimation中对于非正态分布的问题。</li>
<li>methods: 这个论文使用了一种非 Parametric generative model，它通过iteratively mapping reference samples to the target distribution来描述conditionals。这个模型使用了块三角形的交通地图，其中每个块的组件都可以描述目标分布中的conditionals。这个地图由解决一个最优交通问题来获得，其中cost函数是一个weighted $L^2$ cost function。</li>
<li>results: 这个论文的实验结果表明，这种方法可以成功地描述许多非正态的问题，并且比传统的正态流和生成敌对网络更加稳定和可靠。<details>
<summary>Abstract</summary>
Sampling conditional distributions is a fundamental task for Bayesian inference and density estimation. Generative models, such as normalizing flows and generative adversarial networks, characterize conditional distributions by learning a transport map that pushes forward a simple reference (e.g., a standard Gaussian) to a target distribution. While these approaches successfully describe many non-Gaussian problems, their performance is often limited by parametric bias and the reliability of gradient-based (adversarial) optimizers to learn these transformations. This work proposes a non-parametric generative model that iteratively maps reference samples to the target. The model uses block-triangular transport maps, whose components are shown to characterize conditionals of the target distribution. These maps arise from solving an optimal transport problem with a weighted $L^2$ cost function, thereby extending the data-driven approach in [Trigila and Tabak, 2016] for conditional sampling. The proposed approach is demonstrated on a two dimensional example and on a parameter inference problem involving nonlinear ODEs.
</details>
<details>
<summary>摘要</summary>
采样 conditional distributions 是 bayesian inference 和 density estimation 的基本任务之一。生成模型，如 нормализацион流和生成对抗网络，可以通过学习一个传输图来描述 conditional distributions，其中传输图将一个简单的参考（例如标准正态）推进到目标分布中。although these approaches have been successful in solving many non-Gaussian problems, their performance is often limited by parametric bias and the reliability of gradient-based (adversarial) optimizers to learn these transformations.本文提出了一种非Parametric生成模型，可以逐步将参考样本映射到目标分布中。该模型使用块三角形的传输图，其中每个组件可以表示目标分布的 conditionals。这些传输图来自于解一个最优运输问题，其中的cost函数是 weighted $L^2$ 的，从而扩展了 [Trigila and Tabak, 2016] 中的数据驱动方法。提出的方法在二维示例和非线性 ODE 参数推断中进行了示例。
</details></li>
</ul>
<hr>
<h2 id="GNP-Attack-Transferable-Adversarial-Examples-via-Gradient-Norm-Penalty"><a href="#GNP-Attack-Transferable-Adversarial-Examples-via-Gradient-Norm-Penalty" class="headerlink" title="GNP Attack: Transferable Adversarial Examples via Gradient Norm Penalty"></a>GNP Attack: Transferable Adversarial Examples via Gradient Norm Penalty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04099">http://arxiv.org/abs/2307.04099</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Wu, Tie Luo, Donald C. Wunsch</li>
<li>for: 这篇论文的目的是提高敌意例的传输性，使其可以对各种目标模型进行实际的黑盒攻击，不需要内部知识。</li>
<li>methods: 这篇论文提出了一种新的方法，即Gradient Norm Penalty（GNP），用于提高敌意例的传输性。GNP使得优化过程 converges to a flat region of local optima in the loss landscape，从而提高了敌意例的通用性。</li>
<li>results: 通过对11种state-of-the-art深度学习模型和6种高级防御方法进行实验，这篇论文证明了GNP的高效性和灵活性。GNP可以轻松地与其他梯度基本方法结合使用，以实现更强大的传输基本攻击。<details>
<summary>Abstract</summary>
Adversarial examples (AE) with good transferability enable practical black-box attacks on diverse target models, where insider knowledge about the target models is not required. Previous methods often generate AE with no or very limited transferability; that is, they easily overfit to the particular architecture and feature representation of the source, white-box model and the generated AE barely work for target, black-box models. In this paper, we propose a novel approach to enhance AE transferability using Gradient Norm Penalty (GNP). It drives the loss function optimization procedure to converge to a flat region of local optima in the loss landscape. By attacking 11 state-of-the-art (SOTA) deep learning models and 6 advanced defense methods, we empirically show that GNP is very effective in generating AE with high transferability. We also demonstrate that it is very flexible in that it can be easily integrated with other gradient based methods for stronger transfer-based attacks.
</details>
<details>
<summary>摘要</summary>
“敌对例”（AE） WITH 良好的传播能力允许实际的黑盒攻击多种目标模型，不需要内部知识 About the target models. Previous methods often generate AE with no or very limited transferability; that is, they easily overfit to the particular architecture and feature representation of the source, white-box model, and the generated AE barely work for target, black-box models. In this paper, we propose a novel approach to enhance AE transferability using Gradient Norm Penalty (GNP). It drives the loss function optimization procedure to converge to a flat region of local optima in the loss landscape. By attacking 11 state-of-the-art (SOTA) deep learning models and 6 advanced defense methods, we empirically show that GNP is very effective in generating AE with high transferability. We also demonstrate that it is very flexible in that it can be easily integrated with other gradient-based methods for stronger transfer-based attacks.Note: Please note that the translation is in Simplified Chinese, which is one of the two standardized Chinese writing systems.
</details></li>
</ul>
<hr>
<h2 id="SpreadNUTS-–-Moderate-Dynamic-Extension-of-Paths-for-No-U-Turn-Sampling-Partitioning-Visited-Regions"><a href="#SpreadNUTS-–-Moderate-Dynamic-Extension-of-Paths-for-No-U-Turn-Sampling-Partitioning-Visited-Regions" class="headerlink" title="SpreadNUTS – Moderate Dynamic Extension of Paths for No-U-Turn Sampling &amp; Partitioning Visited Regions"></a>SpreadNUTS – Moderate Dynamic Extension of Paths for No-U-Turn Sampling &amp; Partitioning Visited Regions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.06279">http://arxiv.org/abs/2307.06279</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fareed Sheriff</li>
<li>for: This paper aims to improve the efficiency and speed of convergence of Hamiltonian Monte Carlo (HMC) methods for sampling distributions.</li>
<li>methods: The paper introduces modifications to the no-U-turn sampler (NUTS) algorithm to explore the sample space faster and achieve faster convergence to the true distribution.</li>
<li>results: The modified NUTS algorithm is shown to have faster convergence to the true distribution than the original NUTS algorithm.<details>
<summary>Abstract</summary>
Markov chain Monte Carlo (MCMC) methods have existed for a long time and the field is well-explored. The purpose of MCMC methods is to approximate a distribution through repeated sampling; most MCMC algorithms exhibit asymptotically optimal behavior in that they converge to the true distribution at the limit. However, what differentiates these algorithms are their practical convergence guarantees and efficiency. While a sampler may eventually approximate a distribution well, because it is used in the real world it is necessary that the point at which the sampler yields a good estimate of the distribution is reachable in a reasonable amount of time. Similarly, if it is computationally difficult or intractable to produce good samples from a distribution for use in estimation, then there is no real-world utility afforded by the sampler. Thus, most MCMC methods these days focus on improving efficiency and speeding up convergence. However, many MCMC algorithms suffer from random walk behavior and often only mitigate such behavior as outright erasing random walks is difficult. Hamiltonian Monte Carlo (HMC) is a class of MCMC methods that theoretically exhibit no random walk behavior because of properties related to Hamiltonian dynamics. This paper introduces modifications to a specific HMC algorithm known as the no-U-turn sampler (NUTS) that aims to explore the sample space faster than NUTS, yielding a sampler that has faster convergence to the true distribution than NUTS.
</details>
<details>
<summary>摘要</summary>
Hamiltonian Monte Carlo (HMC) 是一种 MCMC 方法，它们在理论上不会受到随机步行行为的影响，因为它们具有相关的 Hamiltonian dynamics 性质。这篇文章介绍了一种对 NUTS 算法（No-U-turn sampler）进行修改，以实现更快地探索样本空间，并实现一个更快速地趋向真实分布的抽样器。
</details></li>
</ul>
<hr>
<h2 id="Restricted-Generative-Projection-for-One-Class-Classification-and-Anomaly-Detection"><a href="#Restricted-Generative-Projection-for-One-Class-Classification-and-Anomaly-Detection" class="headerlink" title="Restricted Generative Projection for One-Class Classification and Anomaly Detection"></a>Restricted Generative Projection for One-Class Classification and Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04097">http://arxiv.org/abs/2307.04097</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feng Xiao, Ruoyu Sun, Jicong Fan</li>
<li>for: 一篇关于一类分类和异常检测的论文，核心思想是将未知训练数据的分布映射到一个已知目标分布上。</li>
<li>methods: 我们提出使用截断 Gaussian、均匀在卷积体、均匀在卷积体或均匀 между卷积体作为目标分布。然后我们寻找将数据分布映射到目标分布的最佳方法，以保持原始数据的重建错误小。</li>
<li>results: 对多个基准数据集进行比较研究，我们的方法与基准方法相比，显示更高的效果。<details>
<summary>Abstract</summary>
We present a simple framework for one-class classification and anomaly detection. The core idea is to learn a mapping to transform the unknown distribution of training (normal) data to a known target distribution. Crucially, the target distribution should be sufficiently simple, compact, and informative. The simplicity is to ensure that we can sample from the distribution easily, the compactness is to ensure that the decision boundary between normal data and abnormal data is clear and reliable, and the informativeness is to ensure that the transformed data preserve the important information of the original data. Therefore, we propose to use truncated Gaussian, uniform in hypersphere, uniform on hypersphere, or uniform between hyperspheres, as the target distribution. We then minimize the distance between the transformed data distribution and the target distribution while keeping the reconstruction error for the original data small enough. Comparative studies on multiple benchmark datasets verify the effectiveness of our methods in comparison to baselines.
</details>
<details>
<summary>摘要</summary>
我们提出了一个简单的框架 для一类分类和异常检测。核心思想是学习将未知的训练数据分布映射到已知的目标分布上。重要的是，目标分布应该够简单、够 компакт、够有信息。简单是以便我们可以轻松地采样分布， compactness 是以便决策边界 между 正常数据和异常数据清晰可靠，有信息是以便保留原始数据的重要信息。因此，我们提议使用 truncated Gaussian， uniform in hypersphere， uniform on hypersphere，或 uniform between hyperspheres 作为目标分布。然后我们尝试将数据分布与目标分布的距离降为最小化，保持原始数据的重建错误小 enough。多种比较研究在多个 benchmark 数据集上验证了我们的方法的有效性，与基eline 相比。
</details></li>
</ul>
<hr>
<h2 id="Class-Incremental-Mixture-of-Gaussians-for-Deep-Continual-Learning"><a href="#Class-Incremental-Mixture-of-Gaussians-for-Deep-Continual-Learning" class="headerlink" title="Class-Incremental Mixture of Gaussians for Deep Continual Learning"></a>Class-Incremental Mixture of Gaussians for Deep Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04094">http://arxiv.org/abs/2307.04094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukasz Korycki, Bartosz Krawczyk</li>
<li>for: 这篇论文专门针对静态数据的连续学习模型，旨在学习和保持来自顺序推出的概念。</li>
<li>methods: 该论文提出了一种基于中心点驱动方法的综合含气球模型，并在连续学习框架中进行了端到端的整合。</li>
<li>results: 实验表明，该模型在内存免除的场景下可以有效地学习，并与现有的连续学习基eline相比较竞争力强。<details>
<summary>Abstract</summary>
Continual learning models for stationary data focus on learning and retaining concepts coming to them in a sequential manner. In the most generic class-incremental environment, we have to be ready to deal with classes coming one by one, without any higher-level grouping. This requirement invalidates many previously proposed methods and forces researchers to look for more flexible alternative approaches. In this work, we follow the idea of centroid-driven methods and propose end-to-end incorporation of the mixture of Gaussians model into the continual learning framework. By employing the gradient-based approach and designing losses capable of learning discriminative features while avoiding degenerate solutions, we successfully combine the mixture model with a deep feature extractor allowing for joint optimization and adjustments in the latent space. Additionally, we show that our model can effectively learn in memory-free scenarios with fixed extractors. In the conducted experiments, we empirically demonstrate the effectiveness of the proposed solutions and exhibit the competitiveness of our model when compared with state-of-the-art continual learning baselines evaluated in the context of image classification problems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Properly-Learning-Decision-Trees-with-Queries-Is-NP-Hard"><a href="#Properly-Learning-Decision-Trees-with-Queries-Is-NP-Hard" class="headerlink" title="Properly Learning Decision Trees with Queries Is NP-Hard"></a>Properly Learning Decision Trees with Queries Is NP-Hard</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04093">http://arxiv.org/abs/2307.04093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Caleb Koch, Carmen Strassle, Li-Yang Tan</li>
<li>for: 本研究证明了在使用查询学习时，正确地学习决策树是NP困难的，解决了长期存在的开放问题在学习理论中（Bshouty 1993；Guijarro-Lavin-Raghavan 1999；Mehta-Raghavan 2002；Feldman 2016）。</li>
<li>methods: 我们引入了一种called hardness distillation，用于研究决策树复杂性的函数。我们的技术可以应用于任何复杂度度量，并可以排除常量错误的查询学习器。</li>
<li>results: 我们的结果，与最近的几乎多项式时间查询算法（Blanc-Lange-Qiao-Tan 2022）一起，表明了分布假设对问题的影响。<details>
<summary>Abstract</summary>
We prove that it is NP-hard to properly PAC learn decision trees with queries, resolving a longstanding open problem in learning theory (Bshouty 1993; Guijarro-Lavin-Raghavan 1999; Mehta-Raghavan 2002; Feldman 2016). While there has been a long line of work, dating back to (Pitt-Valiant 1988), establishing the hardness of properly learning decision trees from random examples, the more challenging setting of query learners necessitates different techniques and there were no previous lower bounds. En route to our main result, we simplify and strengthen the best known lower bounds for a different problem of Decision Tree Minimization (Zantema-Bodlaender 2000; Sieling 2003).   On a technical level, we introduce the notion of hardness distillation, which we study for decision tree complexity but can be considered for any complexity measure: for a function that requires large decision trees, we give a general method for identifying a small set of inputs that is responsible for its complexity. Our technique even rules out query learners that are allowed constant error. This contrasts with existing lower bounds for the setting of random examples which only hold for inverse-polynomial error.   Our result, taken together with a recent almost-polynomial time query algorithm for properly learning decision trees under the uniform distribution (Blanc-Lange-Qiao-Tan 2022), demonstrates the dramatic impact of distributional assumptions on the problem.
</details>
<details>
<summary>摘要</summary>
我们证明了对问题进行PAC学习是NP困难的，解决了学习理论中长期存在的开问题（Bshouty 1993；Guijarro-Lavin-Raghavan 1999；Mehta-Raghavan 2002；Feldman 2016）。尽管在过去的工作中（Pitt-Valiant 1988）已经证明了从随机例子学习决策树的困难性，但是问题的设定是查询学习需要不同的技术，没有过去的下界。我们在证明过程中简化了和加强了最好的下界，并将其应用于决策树问题。在技术上，我们引入了困难炼煮的概念，它可以应用于任何复杂度度量。对于需要大型决策树的函数，我们提供了一个通用的方法，可以从小批量的输入中获得问题的复杂性。我们的技术甚至可以排除查询学习器，即允许常数错误。这与以往的下界，仅适用于随机例子中的问题，不同之处。我们的结果，加上最近的几乎多项时间查询算法（Blanc-Lange-Qiao-Tan 2022），显示出分布假设的影响。
</details></li>
</ul>
<hr>
<h2 id="DebateKG-Automatic-Policy-Debate-Case-Creation-with-Semantic-Knowledge-Graphs"><a href="#DebateKG-Automatic-Policy-Debate-Case-Creation-with-Semantic-Knowledge-Graphs" class="headerlink" title="DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge Graphs"></a>DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04090">http://arxiv.org/abs/2307.04090</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hellisotherpeople/debatekg">https://github.com/hellisotherpeople/debatekg</a></li>
<li>paper_authors: Allen Roush</li>
<li>for: 这篇论文主要针对竞议辩护中的问题，即如何使用自然语言处理系统解决竞议辩护中的问题。</li>
<li>methods: 该论文使用了受限短路征服算法在 Argumentative Semantic Knowledge Graphs 上进行搜索，以构建高质量的辩护案例。</li>
<li>results: 该论文在 Policy Debate 中的一种美国竞议辩护中，使用了53180个新的例子和更多的有用metadata，并使用了 txtai semantic search 和知识图工具链生成了9个Semantic Knowledge Graphs。这些知识图可以评估哪些知识图在生成政策辩护案例方面更好。<details>
<summary>Abstract</summary>
Recent work within the Argument Mining community has shown the applicability of Natural Language Processing systems for solving problems found within competitive debate. One of the most important tasks within competitive debate is for debaters to create high quality debate cases. We show that effective debate cases can be constructed using constrained shortest path traversals on Argumentative Semantic Knowledge Graphs. We study this potential in the context of a type of American Competitive Debate, called Policy Debate, which already has a large scale dataset targeting it called DebateSum. We significantly improve upon DebateSum by introducing 53180 new examples, as well as further useful metadata for every example, to the dataset. We leverage the txtai semantic search and knowledge graph toolchain to produce and contribute 9 semantic knowledge graphs built on this dataset. We create a unique method for evaluating which knowledge graphs are better in the context of producing policy debate cases. A demo which automatically generates debate cases, along with all other code and the Knowledge Graphs, are open-sourced and made available to the public here: https://github.com/Hellisotherpeople/DebateKG
</details>
<details>
<summary>摘要</summary>
最近在辩论挖掘社区中的工作表明，自然语言处理系统可以解决竞赛辩论中的问题。辩论中最重要的任务之一是创建高质量辩论案例。我们表明，可以使用受限短路径搜索在辩论Semantic Knowledge Graphs中构建高效的辩论案例。我们在美国竞赛辩论（Policy Debate）的上下文中研究这一潜力，使用DebateSum数据集进行研究。我们在DebateSum数据集上进行了大规模的扩展和补充，增加了53180个新的示例，以及每个示例的更多有用的元数据。我们使用txtai的semantic搜索和知识图工具链生成和提交了9个基于这些数据集的semantic知识图。我们还创建了一种用于评估这些知识图在生成政策辩论案例中的优劣的评价方法。一个自动生成辩论案例的demo，以及所有代码和知识图，都是公开开源的，可以在以下链接中获取：https://github.com/Hellisotherpeople/DebateKG。
</details></li>
</ul>
<hr>
<h2 id="Semi-Supervised-Meta-Learning-for-Spatiotemporal-Learning"><a href="#Semi-Supervised-Meta-Learning-for-Spatiotemporal-Learning" class="headerlink" title="Semi Supervised Meta Learning for Spatiotemporal Learning"></a>Semi Supervised Meta Learning for Spatiotemporal Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01916">http://arxiv.org/abs/2308.01916</a></li>
<li>repo_url: None</li>
<li>paper_authors: Faraz Waseem, Pratyush Muthukumar</li>
<li>for: 这个论文的目的是应用元学习到自我超visedMasked autoencoders中进行空间时间学习。</li>
<li>methods: 这个论文使用的方法包括：使用Memory Augmented Neural Network（MANN）架构应用元学习到我们的小规模空间时间 dataset上进行视频重建任务，以及在MAE encoder上进行动作分类任务。</li>
<li>results: 这个论文的结果显示，通过应用元学习到MAE架构中，可以提高视频重建和动作分类的性能。<details>
<summary>Abstract</summary>
We approached the goal of applying meta-learning to self-supervised masked autoencoders for spatiotemporal learning in three steps. Broadly, we seek to understand the impact of applying meta-learning to existing state-of-the-art representation learning architectures. Thus, we test spatiotemporal learning through: a meta-learning architecture only, a representation learning architecture only, and an architecture applying representation learning alongside a meta learning architecture. We utilize the Memory Augmented Neural Network (MANN) architecture to apply meta-learning to our framework. Specifically, we first experiment with applying a pre-trained MAE and fine-tuning on our small-scale spatiotemporal dataset for video reconstruction tasks. Next, we experiment with training an MAE encoder and applying a classification head for action classification tasks. Finally, we experiment with applying a pre-trained MAE and fine-tune with MANN backbone for action classification tasks.
</details>
<details>
<summary>摘要</summary>
我们在三个步骤中尝试了应用元学习到自动编码器中进行空间时间学习。我们的目标是理解将元学习应用到现有的状态艺术表示学习架构中的影响。因此，我们通过以下三种方法进行测试：一种仅使用元学习架构，一种仅使用表示学习架构，以及一种同时使用表示学习和元学习架构。我们使用带有记忆增强的神经网络（MANN）架构来应用元学习到我们的小规模空间时间数据集中。首先，我们试验了使用预训练的MAE（自动编码器）并在我们的小规模空间时间数据集中细化 reconstruction 任务。然后，我们试验了训练MAE编码器并应用一个分类头进行动作分类任务。最后，我们试验了使用预训练的MAE并在Mann架构中进行细化，以便进行动作分类任务。
</details></li>
</ul>
<hr>
<h2 id="Disentangling-Societal-Inequality-from-Model-Biases-Gender-Inequality-in-Divorce-Court-Proceedings"><a href="#Disentangling-Societal-Inequality-from-Model-Biases-Gender-Inequality-in-Divorce-Court-Proceedings" class="headerlink" title="Disentangling Societal Inequality from Model Biases: Gender Inequality in Divorce Court Proceedings"></a>Disentangling Societal Inequality from Model Biases: Gender Inequality in Divorce Court Proceedings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10200">http://arxiv.org/abs/2307.10200</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sujan Dutta, Parth Srivastava, Vaishnavi Solunke, Swaprava Nath, Ashiqur R. KhudaBukhsh</li>
<li>for: This paper uses court proceedings to investigate gender inequality in divorce cases in India.</li>
<li>methods: The paper uses natural language processing (NLP) techniques to analyze the court proceedings, but also acknowledges the limitations and biases present in these methods.</li>
<li>results: The paper finds that while there may be changing social norms in India, with more women challenging patriarchy, the court proceedings reveal striking gender inequality, with women often experiencing domestic violence.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文通过印度离婚法院记录来研究妇女不平等。</li>
<li>methods: 这篇论文使用自然语言处理（NLP）技术分析法院记录，但也承认这些方法中存在限制和偏见。</li>
<li>results: 这篇论文发现，虽然印度社会规范可能在变化，但法院记录表明，妇女经常遭受家庭暴力。<details>
<summary>Abstract</summary>
Divorce is the legal dissolution of a marriage by a court. Since this is usually an unpleasant outcome of a marital union, each party may have reasons to call the decision to quit which is generally documented in detail in the court proceedings. Via a substantial corpus of 17,306 court proceedings, this paper investigates gender inequality through the lens of divorce court proceedings. While emerging data sources (e.g., public court records) on sensitive societal issues hold promise in aiding social science research, biases present in cutting-edge natural language processing (NLP) methods may interfere with or affect such studies. We thus require a thorough analysis of potential gaps and limitations present in extant NLP resources. In this paper, on the methodological side, we demonstrate that existing NLP resources required several non-trivial modifications to quantify societal inequalities. On the substantive side, we find that while a large number of court cases perhaps suggest changing norms in India where women are increasingly challenging patriarchy, AI-powered analyses of these court proceedings indicate striking gender inequality with women often subjected to domestic violence.
</details>
<details>
<summary>摘要</summary>
决aroof是法律解除婚姻的法院程序。由于这通常是婚姻合作的不愉快结果，每个方可能有理由来称呼这个决定，通常会在法律程序中详细记录。通过17,306起法律程序的庞大资料库，这篇论文研究了妇女不平等问题，通过婚姻法律程序的视角。虽然新的数据源（如公共法律记录）在社会问题上具有潜在的研究价值，但是现有的自然语言处理（NLP）技术可能会对或affect这些研究。因此，我们需要进行深入的分析，检查现有NLP资源中的潜在差距和局限性。在方法ológico side，我们示出了现有NLP资源的修改，以便量化社会不平等。在 substantiál side，我们发现，尽管有很多法律案件，但AI对这些法律案件的分析表明，妇女在婚姻中 frequently subjected to domestic violence。
</details></li>
</ul>
<hr>
<h2 id="Score-based-Conditional-Generation-with-Fewer-Labeled-Data-by-Self-calibrating-Classifier-Guidance"><a href="#Score-based-Conditional-Generation-with-Fewer-Labeled-Data-by-Self-calibrating-Classifier-Guidance" class="headerlink" title="Score-based Conditional Generation with Fewer Labeled Data by Self-calibrating Classifier Guidance"></a>Score-based Conditional Generation with Fewer Labeled Data by Self-calibrating Classifier Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04081">http://arxiv.org/abs/2307.04081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Kuo-Ming Huang, Si-An Chen, Hsuan-Tien Lin</li>
<li>for: 提高基于标签数据的少量数据下的生成模型质量</li>
<li>methods: 使用能量基模型改进类别导向的深度生成模型，并通过使用标签和未标签数据进行准确的均衡</li>
<li>results: 提高了基于少量标签数据的生成质量，并在使用不同百分比的标签数据时表现出优异性，证明了提案的方法在生成模型化中具有广泛的应用前景<details>
<summary>Abstract</summary>
Score-based Generative Models (SGMs) are a popular family of deep generative models that achieves leading image generation quality. Earlier studies have extended SGMs to tackle class-conditional generation by coupling an unconditional SGM with the guidance of a trained classifier. Nevertheless, such classifier-guided SGMs do not always achieve accurate conditional generation, especially when trained with fewer labeled data. We argue that the issue is rooted in unreliable gradients of the classifier and the inability to fully utilize unlabeled data during training. We then propose to improve classifier-guided SGMs by letting the classifier calibrate itself. Our key idea is to use principles from energy-based models to convert the classifier as another view of the unconditional SGM. Then, existing loss for the unconditional SGM can be adopted to calibrate the classifier using both labeled and unlabeled data. Empirical results validate that the proposed approach significantly improves the conditional generation quality across different percentages of labeled data. The improved performance makes the proposed approach consistently superior to other conditional SGMs when using fewer labeled data. The results confirm the potential of the proposed approach for generative modeling with limited labeled data.
</details>
<details>
<summary>摘要</summary>
Score-based生成模型（SGM）是一种深度生成模型，其可以实现领先的图像生成质量。 Earlier studies have extended SGMs to tackle class-conditional generation by coupling an unconditional SGM with the guidance of a trained classifier. However, such classifier-guided SGMs do not always achieve accurate conditional generation, especially when trained with fewer labeled data. We argue that the issue is rooted in unreliable gradients of the classifier and the inability to fully utilize unlabeled data during training. We then propose to improve classifier-guided SGMs by letting the classifier calibrate itself. Our key idea is to use principles from energy-based models to convert the classifier as another view of the unconditional SGM. Then, existing loss for the unconditional SGM can be adopted to calibrate the classifier using both labeled and unlabeled data. Empirical results validate that the proposed approach significantly improves the conditional generation quality across different percentages of labeled data. The improved performance makes the proposed approach consistently superior to other conditional SGMs when using fewer labeled data. The results confirm the potential of the proposed approach for generative modeling with limited labeled data.
</details></li>
</ul>
<hr>
<h2 id="Towards-Fast-and-Scalable-Private-Inference"><a href="#Towards-Fast-and-Scalable-Private-Inference" class="headerlink" title="Towards Fast and Scalable Private Inference"></a>Towards Fast and Scalable Private Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04077">http://arxiv.org/abs/2307.04077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianqiao Mo, Karthik Garimella, Negar Neda, Austin Ebel, Brandon Reagen</li>
<li>for: 本文旨在探讨如何通过隐私保护计算（Privacy-Preserving Computation，PPC）技术来提高用户数据的安全性和隐私性。</li>
<li>methods: 本文使用了许多现有的隐私保护计算技术，包括同知加密（Homomorphic Encryption，HE）、秘密分享（Secret Sharing，SS）、卷积阵列（Garbled Circuits，GCs）和无知传输（Oblivious Transfer，OT）等。</li>
<li>results: 本文对这些技术的使用 overhead 进行了Characterization，并提出了一些加速GCs和HE加速器的解决方案，包括HAAC和RPU。最后，本文还讨论了未来工作的需要，以减少PPC的开销。<details>
<summary>Abstract</summary>
Privacy and security have rapidly emerged as first order design constraints. Users now demand more protection over who can see their data (confidentiality) as well as how it is used (control). Here, existing cryptographic techniques for security fall short: they secure data when stored or communicated but must decrypt it for computation. Fortunately, a new paradigm of computing exists, which we refer to as privacy-preserving computation (PPC). Emerging PPC technologies can be leveraged for secure outsourced computation or to enable two parties to compute without revealing either users' secret data. Despite their phenomenal potential to revolutionize user protection in the digital age, the realization has been limited due to exorbitant computational, communication, and storage overheads.   This paper reviews recent efforts on addressing various PPC overheads using private inference (PI) in neural network as a motivating application. First, the problem and various technologies, including homomorphic encryption (HE), secret sharing (SS), garbled circuits (GCs), and oblivious transfer (OT), are introduced. Next, a characterization of their overheads when used to implement PI is covered. The characterization motivates the need for both GCs and HE accelerators. Then two solutions are presented: HAAC for accelerating GCs and RPU for accelerating HE. To conclude, results and effects are shown with a discussion on what future work is needed to overcome the remaining overheads of PI.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>隐私和安全已经迅速emerge为首要的设计约束。用户现在要求更多的保护，包括谁可以看到他们的数据（保密）以及如何使用它们（控制）。在这里，现有的 криптографические技术 для安全 fallen short：它们可以保护数据在存储或传输时，但必须解密它们进行计算。幸运的是，一种新的计算模式存在，我们称之为隐私保持计算（PPC）。这些技术可以用于安全的外部计算或让两个方面计算而不把用户的秘密数据泄露出来。尽管它们在数字时代中保护用户的潜在潜力很大，但实现却受到了极高的计算、通信和存储开销的限制。这篇文章介绍了在实现隐私保持计算时不同技术的开销。首先，问题和不同技术，包括同质加密（HE）、分 sharing（SS）、拟合圈（GCs）和无意识传输（OT），是介绍的。接着，对这些技术在实现隐私保持计算时的开销进行了描述。这种描述驱动了GCs和HE加速器的需求。然后，文章介绍了两个解决方案：HAAC用于加速GCs，和RPU用于加速HE。最后，文章显示了结果和影响，并进行了未来工作的讨论，以便继续减少隐私保持计算的开销。
</details></li>
</ul>
<hr>
<h2 id="Multi-Head-Attention-Mechanism-Learning-for-Cancer-New-Subtypes-and-Treatment-Based-on-Cancer-Multi-Omics-Data"><a href="#Multi-Head-Attention-Mechanism-Learning-for-Cancer-New-Subtypes-and-Treatment-Based-on-Cancer-Multi-Omics-Data" class="headerlink" title="Multi-Head Attention Mechanism Learning for Cancer New Subtypes and Treatment Based on Cancer Multi-Omics Data"></a>Multi-Head Attention Mechanism Learning for Cancer New Subtypes and Treatment Based on Cancer Multi-Omics Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04075">http://arxiv.org/abs/2307.04075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liangrui Pan, Dazhen Liu, Yutao Dou, Lian Wang, Zhichao Feng, Pengfei Rong, Liwen Xu, Shaoliang Peng<br>for:* 这个研究旨在透过非监督学习的对照学习方法来分类不同种类的肝癌，以提高肝癌诊断、治疗和预后预测。methods:* 这个研究使用了一个普遍化 Framework 基于注意力机制（AMUCL），并提出了一个基于注意力机制的多头注意力对照学习模型（DMACL），以深入探索肝癌多种数据的特点和分类。results:* 相比11个深度学习模型，DMACL 模型在单细胞多种数据集上取得了 C-指数0.002、Silhouette 分数0.801和Davies Bouldin 分数0.38的最佳结果，并在肝癌多种数据集上取得了最可靠的肝癌乱型分类结果。<details>
<summary>Abstract</summary>
Due to the high heterogeneity and clinical characteristics of cancer, there are significant differences in multi-omics data and clinical features among subtypes of different cancers. Therefore, the identification and discovery of cancer subtypes are crucial for the diagnosis, treatment, and prognosis of cancer. In this study, we proposed a generalization framework based on attention mechanisms for unsupervised contrastive learning (AMUCL) to analyze cancer multi-omics data for the identification and characterization of cancer subtypes. AMUCL framework includes a unsupervised multi-head attention mechanism, which deeply extracts multi-omics data features. Importantly, a decoupled contrastive learning model (DMACL) based on a multi-head attention mechanism is proposed to learn multi-omics data features and clusters and identify new cancer subtypes. This unsupervised contrastive learning method clusters subtypes by calculating the similarity between samples in the feature space and sample space of multi-omics data. Compared to 11 other deep learning models, the DMACL model achieved a C-index of 0.002, a Silhouette score of 0.801, and a Davies Bouldin Score of 0.38 on a single-cell multi-omics dataset. On a cancer multi-omics dataset, the DMACL model obtained a C-index of 0.016, a Silhouette score of 0.688, and a Davies Bouldin Score of 0.46, and obtained the most reliable cancer subtype clustering results for each type of cancer. Finally, we used the DMACL model in the AMUCL framework to reveal six cancer subtypes of AML. By analyzing the GO functional enrichment, subtype-specific biological functions, and GSEA of AML, we further enhanced the interpretability of cancer subtype analysis based on the generalizable AMUCL framework.
</details>
<details>
<summary>摘要</summary>
due to the high heterogeneity and clinical characteristics of cancer, there are significant differences in multi-omics data and clinical features among subtypes of different cancers. therefore, the identification and discovery of cancer subtypes are crucial for the diagnosis, treatment, and prognosis of cancer. in this study, we proposed a generalization framework based on attention mechanisms for unsupervised contrastive learning (AMUCL) to analyze cancer multi-omics data for the identification and characterization of cancer subtypes. AMUCL framework includes a unsupervised multi-head attention mechanism, which deeply extracts multi-omics data features. importantly, a decoupled contrastive learning model (DMACL) based on a multi-head attention mechanism is proposed to learn multi-omics data features and clusters and identify new cancer subtypes. this unsupervised contrastive learning method clusters subtypes by calculating the similarity between samples in the feature space and sample space of multi-omics data. compared to 11 other deep learning models, the DMACL model achieved a C-index of 0.002, a silhouette score of 0.801, and a davies bouldin score of 0.38 on a single-cell multi-omics dataset. on a cancer multi-omics dataset, the DMACL model obtained a C-index of 0.016, a silhouette score of 0.688, and a davies bouldin score of 0.46, and obtained the most reliable cancer subtype clustering results for each type of cancer. finally, we used the DMACL model in the AMUCL framework to reveal six cancer subtypes of AML. by analyzing the go functional enrichment, subtype-specific biological functions, and gsea of AML, we further enhanced the interpretability of cancer subtype analysis based on the generalizable AMUCL framework.
</details></li>
</ul>
<hr>
<h2 id="Large-scale-global-optimization-of-ultra-high-dimensional-non-convex-landscapes-based-on-generative-neural-networks"><a href="#Large-scale-global-optimization-of-ultra-high-dimensional-non-convex-landscapes-based-on-generative-neural-networks" class="headerlink" title="Large-scale global optimization of ultra-high dimensional non-convex landscapes based on generative neural networks"></a>Large-scale global optimization of ultra-high dimensional non-convex landscapes based on generative neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04065">http://arxiv.org/abs/2307.04065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaqi Jiang, Jonathan A. Fan</li>
<li>For: 这个论文是关于非凸优化问题的一种基于深度生成网络的算法 мета希略，用于在维度非常高的搜索空间中寻找优化答案。* Methods: 该算法使用了一个特制的损失函数和一个适应性的深度网络 architecture，通过训练这个网络来进行搜索和优化。* Results: 在一些标准的优化问题中，该算法能够比对State-of-the-art algorithm benchmarks更好地性能，并且需要 fewer function evaluations。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
We present a non-convex optimization algorithm metaheuristic, based on the training of a deep generative network, which enables effective searching within continuous, ultra-high dimensional landscapes. During network training, populations of sampled local gradients are utilized within a customized loss function to evolve the network output distribution function towards one peak at high-performing optima. The deep network architecture is tailored to support progressive growth over the course of training, which allows the algorithm to manage the curse of dimensionality characteristic of high-dimensional landscapes. We apply our concept to a range of standard optimization problems with dimensions as high as one thousand and show that our method performs better with fewer function evaluations compared to state-of-the-art algorithm benchmarks. We also discuss the role of deep network over-parameterization, loss function engineering, and proper network architecture selection in optimization, and why the required batch size of sampled local gradients is independent of problem dimension. These concepts form the foundation for a new class of algorithms that utilize customizable and expressive deep generative networks to solve non-convex optimization problems.
</details>
<details>
<summary>摘要</summary>
我们提出了一种基于深度生成网络的非 convex 优化算法metaheuristic，可以有效地在维度非常高的连续空间中寻找优点。在网络训练中，通过自定义损失函数来使用射程数据集的人工 popula-tion，逐渐提高网络输出分布函数的优化。我们的网络架构是通过训练进程来支持不断增长，以适应高维度空间的特点。我们在一些标准的优化问题中使用了这种方法，并证明我们的方法可以在 fewer function evaluations 下比 benchmark 更好的性能。我们还讨论了深度网络过parameterization、损失函数工程和网络架构选择对优化的影响，以及批处理大小是独立于问题维度的。这些概念形成了一种新的类型的算法，可以使用可定制和表达ive的深度生成网络来解决非 convex 优化问题。
</details></li>
</ul>
<hr>
<h2 id="Bidirectional-Attention-as-a-Mixture-of-Continuous-Word-Experts"><a href="#Bidirectional-Attention-as-a-Mixture-of-Continuous-Word-Experts" class="headerlink" title="Bidirectional Attention as a Mixture of Continuous Word Experts"></a>Bidirectional Attention as a Mixture of Continuous Word Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04057">http://arxiv.org/abs/2307.04057</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yixinw-lab/attention-uai">https://github.com/yixinw-lab/attention-uai</a></li>
<li>paper_authors: Kevin Christian Wibisono, Yixin Wang</li>
<li>for: This paper aims to examine the statistical underpinnings of bidirectional attention in large language models (LLMs), specifically exploring the relationship between bidirectional attention and mixture-of-experts (MoE) weights.</li>
<li>methods: The paper uses a combination of theoretical analysis and empirical studies to investigate the statistical properties of bidirectional attention. The authors reparameterize bidirectional attention as a continuous bag of words (CBOW) model with MoE weights, and show that this allows for a deeper understanding of the model’s behavior.</li>
<li>results: The paper finds that bidirectional attention with multiple heads and multiple layers is equivalent to stacked MoEs and a mixture of MoEs, respectively. Additionally, the authors extend the model to categorical tabular data and find that it outperforms existing tabular extensions of transformers in out-of-distribution (OOD) generalization. Finally, the paper theoretically characterizes when linear word analogies are present in the word embeddings of bidirectional attention.<details>
<summary>Abstract</summary>
Bidirectional attention $\unicode{x2013}$ composed of self-attention with positional encodings and the masked language model (MLM) objective $\unicode{x2013}$ has emerged as a key component of modern large language models (LLMs). Despite its empirical success, few studies have examined its statistical underpinnings: What statistical model is bidirectional attention implicitly fitting? What sets it apart from its non-attention predecessors? We explore these questions in this paper. The key observation is that fitting a single-layer single-head bidirectional attention, upon reparameterization, is equivalent to fitting a continuous bag of words (CBOW) model with mixture-of-experts (MoE) weights. Further, bidirectional attention with multiple heads and multiple layers is equivalent to stacked MoEs and a mixture of MoEs, respectively. This statistical viewpoint reveals the distinct use of MoE in bidirectional attention, which aligns with its practical effectiveness in handling heterogeneous data. It also suggests an immediate extension to categorical tabular data, if we view each word location in a sentence as a tabular feature. Across empirical studies, we find that this extension outperforms existing tabular extensions of transformers in out-of-distribution (OOD) generalization. Finally, this statistical perspective of bidirectional attention enables us to theoretically characterize when linear word analogies are present in its word embeddings. These analyses show that bidirectional attention can require much stronger assumptions to exhibit linear word analogies than its non-attention predecessors.
</details>
<details>
<summary>摘要</summary>
bidirectional attention $\unicode{x2013}$ 由自我注意和位置编码组成，并且与屏蔽语言模型（MLM）目标相结合，已经成为现代大语言模型（LLM）的关键组成部分。Despite its empirical success, few studies have examined its statistical underpinnings: What statistical model is bidirectional attention implicitly fitting? What sets it apart from its non-attention predecessors? We explore these questions in this paper. The key observation is that fitting a single-layer single-head bidirectional attention, upon reparameterization, is equivalent to fitting a continuous bag of words (CBOW) model with mixture-of-experts (MoE) weights. Further, bidirectional attention with multiple heads and multiple layers is equivalent to stacked MoEs and a mixture of MoEs, respectively. This statistical viewpoint reveals the distinct use of MoE in bidirectional attention, which aligns with its practical effectiveness in handling heterogeneous data. It also suggests an immediate extension to categorical tabular data, if we view each word location in a sentence as a tabular feature. Across empirical studies, we find that this extension outperforms existing tabular extensions of transformers in out-of-distribution (OOD) generalization. Finally, this statistical perspective of bidirectional attention enables us to theoretically characterize when linear word analogies are present in its word embeddings. These analyses show that bidirectional attention can require much stronger assumptions to exhibit linear word analogies than its non-attention predecessors.
</details></li>
</ul>
<hr>
<h2 id="Manifold-Filter-Combine-Networks"><a href="#Manifold-Filter-Combine-Networks" class="headerlink" title="Manifold Filter-Combine Networks"></a>Manifold Filter-Combine Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04056">http://arxiv.org/abs/2307.04056</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/krishnaswamylab/mfcn">https://github.com/krishnaswamylab/mfcn</a></li>
<li>paper_authors: Joyce Chew, Edward De Brouwer, Smita Krishnaswamy, Deanna Needell, Michael Perlmutter</li>
<li>for: 本研究旨在更深入理解 manifold neural networks (MNNs)， analogous to how the aggregate-combine framework helps with the understanding of graph neural networks (GNNs)。</li>
<li>methods: 该类含有多种 subclass，可以看作 manifold 的 аналоги。furthermore, the authors propose a method for implementing such networks when one does not have global knowledge of the manifold, but merely has access to finitely many sample points.</li>
<li>results: the authors provide sufficient conditions for the network to provably converge to its continuum limit as the number of sample points tends to infinity. unlike previous work (which focused on specific graph constructions), the rate of convergence does not directly depend on the number of filters used, and exhibits linear dependence on the depth of the network rather than the exponential dependence obtained previously. additionally, the authors provide several examples of interesting subclasses of MFCNs and of the rates of convergence that are obtained under specific graph constructions.<details>
<summary>Abstract</summary>
We introduce a class of manifold neural networks (MNNs) that we call Manifold Filter-Combine Networks (MFCNs), that aims to further our understanding of MNNs, analogous to how the aggregate-combine framework helps with the understanding of graph neural networks (GNNs). This class includes a wide variety of subclasses that can be thought of as the manifold analog of various popular GNNs. We then consider a method, based on building a data-driven graph, for implementing such networks when one does not have global knowledge of the manifold, but merely has access to finitely many sample points. We provide sufficient conditions for the network to provably converge to its continuum limit as the number of sample points tends to infinity. Unlike previous work (which focused on specific graph constructions), our rate of convergence does not directly depend on the number of filters used. Moreover, it exhibits linear dependence on the depth of the network rather than the exponential dependence obtained previously. Additionally, we provide several examples of interesting subclasses of MFCNs and of the rates of convergence that are obtained under specific graph constructions.
</details>
<details>
<summary>摘要</summary>
我们引入一种类型的拟合神经网络（MNN），称之为拟合筛合网络（MFCN），以深入理解MNN，类似于如何使用集合合并框架理解图神经网络（GNN）。这个类型包括许多子类，可以看作拟合 manifold 中的各种受欢迎 GNN 的拟合。我们 THEN 考虑一种基于构建数据驱动图的方法，实现这些网络，只有 finite 数据点的知识，而不是全局 manifold 的知识。我们提供了足够的条件，使网络可靠地趋向于维度随着数据点的数量增加而减少。与前一个工作不同，我们的速度不直接取决于使用的筛刷数量。此外，它展现出线性取决于网络的深度，而不是之前所获得的对数靠渐增长。我们还提供了一些有趣的 MFCN 的 subclass 和特定图构造下的速度减少率。
</details></li>
</ul>
<hr>
<h2 id="Contextual-Dynamic-Pricing-with-Strategic-Buyers"><a href="#Contextual-Dynamic-Pricing-with-Strategic-Buyers" class="headerlink" title="Contextual Dynamic Pricing with Strategic Buyers"></a>Contextual Dynamic Pricing with Strategic Buyers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04055">http://arxiv.org/abs/2307.04055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pangpang Liu, Zhuoran Yang, Zhaoran Wang, Will Wei Sun</li>
<li>For: This paper is written to study the contextual dynamic pricing problem with strategic buyers, where buyers can manipulate their feature data to obtain a lower price.* Methods: The paper proposes a strategic dynamic pricing policy that incorporates the buyers’ strategic behavior into the online learning to maximize the seller’s cumulative revenue. The policy uses a combination of dynamic pricing and strategic behavior handling algorithms to account for the buyers’ manipulation of their feature data.* Results: The paper achieves a sublinear regret upper bound of $O(\sqrt{T})$ and is shown to be superior to other pricing policies that are unaware of the strategic behaviors through extensive experiments.Here’s the Chinese translation of the three points:* For: 这篇论文是研究Contextual Dynamic Pricing问题，在这个问题中，买家可以通过操纵自己的特征数据来获得更低的价格。* Methods: 论文提出了一种战略性动态价格策略，它将买家的战略行为包含在在线学习中，以最大化卖家的累积收益。这种策略使用了动态价格和战略行为处理算法的组合来考虑买家的操纵行为。* Results: 论文实现了一个下限为$O(\sqrt{T})$的非线性 regret bound，并通过广泛的实验证明了其在其他不知情的价格策略比较优秀。<details>
<summary>Abstract</summary>
Personalized pricing, which involves tailoring prices based on individual characteristics, is commonly used by firms to implement a consumer-specific pricing policy. In this process, buyers can also strategically manipulate their feature data to obtain a lower price, incurring certain manipulation costs. Such strategic behavior can hinder firms from maximizing their profits. In this paper, we study the contextual dynamic pricing problem with strategic buyers. The seller does not observe the buyer's true feature, but a manipulated feature according to buyers' strategic behavior. In addition, the seller does not observe the buyers' valuation of the product, but only a binary response indicating whether a sale happens or not. Recognizing these challenges, we propose a strategic dynamic pricing policy that incorporates the buyers' strategic behavior into the online learning to maximize the seller's cumulative revenue. We first prove that existing non-strategic pricing policies that neglect the buyers' strategic behavior result in a linear $\Omega(T)$ regret with $T$ the total time horizon, indicating that these policies are not better than a random pricing policy. We then establish that our proposed policy achieves a sublinear regret upper bound of $O(\sqrt{T})$. Importantly, our policy is not a mere amalgamation of existing dynamic pricing policies and strategic behavior handling algorithms. Our policy can also accommodate the scenario when the marginal cost of manipulation is unknown in advance. To account for it, we simultaneously estimate the valuation parameter and the cost parameter in the online pricing policy, which is shown to also achieve an $O(\sqrt{T})$ regret bound. Extensive experiments support our theoretical developments and demonstrate the superior performance of our policy compared to other pricing policies that are unaware of the strategic behaviors.
</details>
<details>
<summary>摘要</summary>
企业通常采用个性化价格策略，根据消费者特点来定价。在这个过程中，消费者可以通过滥用自己的特征数据来获得较低的价格，并且会付出一定的操作成本。这种策略性行为可能会妨碍企业实现最高利润。本文研究了上述情境下的动态价格问题，其中买方不仅不见商家的实际特征，而且也不知道自己对产品的评估价值。为了应对这些挑战，我们提出了一种战略性动态价格策略，该策略将买方的策略行为纳入在线学习中，以最大化卖家的总收入。我们首先证明了忽略买方策略行为的非战略价格策略会在总时间 horizon $T$ 上导致一个线性的 $\Omega(T)$ 违和，这表明这些策略不比随机价格策略更好。然后，我们证明了我们提出的策略可以达到一个幂函数Bound $O(\sqrt{T})$ 的违和上限，这表明我们的策略比其他不考虑买方策略行为的价格策略更高效。另外，我们的策略还能够考虑到 manipulate 成本未知的情况，通过在线学习来同时估算买方评估价值和 manipulate 成本参数，并且也可以达到 $O(\sqrt{T})$ 的违和上限。实际实验支持我们的理论发展，并证明了我们的策略在与其他不考虑买方策略行为的价格策略进行比较时具有更高的性能。
</details></li>
</ul>
<hr>
<h2 id="A-Physics-Informed-Low-Shot-Learning-For-sEMG-Based-Estimation-of-Muscle-Force-and-Joint-Kinematics"><a href="#A-Physics-Informed-Low-Shot-Learning-For-sEMG-Based-Estimation-of-Muscle-Force-and-Joint-Kinematics" class="headerlink" title="A Physics-Informed Low-Shot Learning For sEMG-Based Estimation of Muscle Force and Joint Kinematics"></a>A Physics-Informed Low-Shot Learning For sEMG-Based Estimation of Muscle Force and Joint Kinematics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05361">http://arxiv.org/abs/2307.05361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Shi, Shuhao Ma, Yihui Zhao, Zhiqiang Zhang</li>
<li>for: 这篇论文旨在提出一种基于surface electromyography（sEMG）的肌力和关节动态分析方法，以便在实时生物机器学中进行无需人工干预的生物机器分析。</li>
<li>methods: 该方法基于深度神经网络（DNNs），并将拉格朗日方程和反向动态肌肉模型纳入生成对抗网络（GAN）框架中，以实现结构化特征编码和距离抽象估计。</li>
<li>results: 实验结果表明，与物理反向动态肌肉模型相比，该方法的估计结果具有较小的偏差，并且在两个试验（走行和手部运动）中都有较高的准确率。<details>
<summary>Abstract</summary>
Muscle force and joint kinematics estimation from surface electromyography (sEMG) are essential for real-time biomechanical analysis of the dynamic interplay among neural muscle stimulation, muscle dynamics, and kinetics. Recent advances in deep neural networks (DNNs) have shown the potential to improve biomechanical analysis in a fully automated and reproducible manner. However, the small sample nature and physical interpretability of biomechanical analysis limit the applications of DNNs. This paper presents a novel physics-informed low-shot learning method for sEMG-based estimation of muscle force and joint kinematics. This method seamlessly integrates Lagrange's equation of motion and inverse dynamic muscle model into the generative adversarial network (GAN) framework for structured feature decoding and extrapolated estimation from the small sample data. Specifically, Lagrange's equation of motion is introduced into the generative model to restrain the structured decoding of the high-level features following the laws of physics. And a physics-informed policy gradient is designed to improve the adversarial learning efficiency by rewarding the consistent physical representation of the extrapolated estimations and the physical references. Experimental validations are conducted on two scenarios (i.e. the walking trials and wrist motion trials). Results indicate that the estimations of the muscle forces and joint kinematics are unbiased compared to the physics-based inverse dynamics, which outperforms the selected benchmark methods, including physics-informed convolution neural network (PI-CNN), vallina generative adversarial network (GAN), and multi-layer extreme learning machine (ML-ELM).
</details>
<details>
<summary>摘要</summary>
muscle force和关节动态分析从表面电omyography(sEMG)是生物机器学分析中的关键因素，它们描述了神经肌肉刺激、肌肉动态和动力学的 dynamically interplay。 recent advances in deep neural networks (DNNs) have shown the potential to improve biomechanical analysis in a fully automated and reproducible manner. However, the small sample nature and physical interpretability of biomechanical analysis limit the applications of DNNs. This paper presents a novel physics-informed low-shot learning method for sEMG-based estimation of muscle force and joint kinematics. This method seamlessly integrates Lagrange's equation of motion and inverse dynamic muscle model into the generative adversarial network (GAN) framework for structured feature decoding and extrapolated estimation from the small sample data. Specifically, Lagrange's equation of motion is introduced into the generative model to restrain the structured decoding of the high-level features following the laws of physics. And a physics-informed policy gradient is designed to improve the adversarial learning efficiency by rewarding the consistent physical representation of the extrapolated estimations and the physical references. Experimental validations are conducted on two scenarios (i.e. the walking trials and wrist motion trials). Results indicate that the estimations of the muscle forces and joint kinematics are unbiased compared to the physics-based inverse dynamics, which outperforms the selected benchmark methods, including physics-informed convolution neural network (PI-CNN), vallina generative adversarial network (GAN), and multi-layer extreme learning machine (ML-ELM).
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Group-Auxiliary-Datasets-for-Molecule"><a href="#Learning-to-Group-Auxiliary-Datasets-for-Molecule" class="headerlink" title="Learning to Group Auxiliary Datasets for Molecule"></a>Learning to Group Auxiliary Datasets for Molecule</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04052">http://arxiv.org/abs/2307.04052</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tinglin Huang, Ziniu Hu, Rex Ying</li>
<li>for: 提高小分子数据集中机器学习模型的性能，因为小分子数据集的数据有限制，通常采用与其他小分子数据集合作，但不一定能够提高模型性能。</li>
<li>methods: 提出了一种基于路径优化和二级优化框架的方法，即MolGroup，可以预测每个目标数据集的最佳auxiliary数据集组合，并通过路径优化和meta gradient来优化路径。</li>
<li>results: 通过广泛的实验，显示MolGroup可以提高GIN和Graphormer在11个目标数据集上的性能，增加4.41%和3.47%。<details>
<summary>Abstract</summary>
The limited availability of annotations in small molecule datasets presents a challenge to machine learning models. To address this, one common strategy is to collaborate with additional auxiliary datasets. However, having more data does not always guarantee improvements. Negative transfer can occur when the knowledge in the target dataset differs or contradicts that of the auxiliary molecule datasets. In light of this, identifying the auxiliary molecule datasets that can benefit the target dataset when jointly trained remains a critical and unresolved problem. Through an empirical analysis, we observe that combining graph structure similarity and task similarity can serve as a more reliable indicator for identifying high-affinity auxiliary datasets. Motivated by this insight, we propose MolGroup, which separates the dataset affinity into task and structure affinity to predict the potential benefits of each auxiliary molecule dataset. MolGroup achieves this by utilizing a routing mechanism optimized through a bi-level optimization framework. Empowered by the meta gradient, the routing mechanism is optimized toward maximizing the target dataset's performance and quantifies the affinity as the gating score. As a result, MolGroup is capable of predicting the optimal combination of auxiliary datasets for each target dataset. Our extensive experiments demonstrate the efficiency and effectiveness of MolGroup, showing an average improvement of 4.41%/3.47% for GIN/Graphormer trained with the group of molecule datasets selected by MolGroup on 11 target molecule datasets.
</details>
<details>
<summary>摘要</summary>
因为小分子数据集的标注有限，机器学习模型受到挑战。为了解决这个问题，一种常见的策略是与其他辅助数据集合作。然而，更多的数据不总是能带来改善。当目标数据集和辅助分子数据集之间存在知识差异或矛盾时，可能会出现负面传递。因此，确定可以帮助目标数据集的辅助分子数据集仍是一个关键和未解决的问题。经验分析发现，将分子结构相似性和任务相似性组合起来可以 servir como一个可靠的指标来选择高价值的辅助分子数据集。鼓动了这一点，我们提出了MolGroup，它将数据集的亲和力分解为任务亲和力和结构亲和力，以预测每个目标数据集的最佳辅助分子数据集。MolGroup使用优化的 Routing 机制和meta gradient来实现这一点，并通过最大化目标数据集的性能来衡量亲和力。因此，MolGroup可以预测每个目标数据集的优化辅助分子数据集。我们的广泛实验表明，MolGroup可以提高 GIN 和 Graphormer 在 11 个目标分子数据集上的性能，平均提高4.41%/3.47%。
</details></li>
</ul>
<hr>
<h2 id="Parallel-Algorithms-Align-with-Neural-Execution"><a href="#Parallel-Algorithms-Align-with-Neural-Execution" class="headerlink" title="Parallel Algorithms Align with Neural Execution"></a>Parallel Algorithms Align with Neural Execution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04049">http://arxiv.org/abs/2307.04049</a></li>
<li>repo_url: None</li>
<li>paper_authors: Valerie Engelmayer, Dobrik Georgiev, Petar Veličković</li>
<li>for: 学习并使用并行算法，以优化神经算法逻辑推理器的性能。</li>
<li>methods: 使用并行算法，例如搜索、排序和寻找强连接组件，来利用神经算法逻辑推理器的完全计算能力，从而减少训练时间和提高预测性能。</li>
<li>results: 相比顺序实现，并行实现可以减少训练时间，并且在大多数情况下，并行版本可以达到更高的预测性能。<details>
<summary>Abstract</summary>
Neural algorithmic reasoners are parallel processors. Teaching them sequential algorithms contradicts this nature, rendering a significant share of their computations redundant. Parallel algorithms however may exploit their full computational power, therefore requiring fewer layers to be executed. This drastically reduces training times, as we observe when comparing parallel implementations of searching, sorting and finding strongly connected components to their sequential counterparts on the CLRS framework. Additionally, parallel versions achieve strongly superior predictive performance in most cases.
</details>
<details>
<summary>摘要</summary>
Note:* "Neural algorithmic reasoners" is translated as "神经算法推理器" (shénxīn xiàngxíng suǒyì) in Simplified Chinese.* "Parallel processors" is translated as "并行处理器" (héxìng chùxíng) in Simplified Chinese.* "Sequential algorithms" is translated as "顺序算法" (shùxìng suāfāng) in Simplified Chinese.* "Parallel algorithms" is translated as "并行算法" (héxìng suāfāng) in Simplified Chinese.* "CLRS framework" is translated as "CLRS框架" (C-L-R-S kuàiwā) in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Optimization-based-Learning-for-Dynamic-Load-Planning-in-Trucking-Service-Networks"><a href="#Optimization-based-Learning-for-Dynamic-Load-Planning-in-Trucking-Service-Networks" class="headerlink" title="Optimization-based Learning for Dynamic Load Planning in Trucking Service Networks"></a>Optimization-based Learning for Dynamic Load Planning in Trucking Service Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04050">http://arxiv.org/abs/2307.04050</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ritesh Ojha, Wenbo Chen, Hanyu Zhang, Reem Khir, Alan Erera, Pascal Van Hentenryck</li>
<li>for: 这种论文是为了解决货物配送服务网络设计中的货物规划问题，即如何在不同的终端之间分配货物和流量，以及如何在不同的时间点进行货物和流量的划分。</li>
<li>methods: 这篇论文使用了动态货物规划问题（DLPP）来 JOINTLY 考虑货物和流量规划问题，并提出了一种决策支持工具来帮助终端执行这些决策。论文还提出了一种目标指导优化方法，以消除网络中每个商品可以通过主要和备用路径的Symmetry，从而提高优化的可靠性和可行性。</li>
<li>results: 论文的计算研究表明，使用这种目标指导优化方法可以在约10倍速度下获得同质性的优化解决方案，并且可以在几个数量级快的速度下生成与实际终端决策兼容的解决方案。此外，论文还表明，通过结合机器学习和优化，可以获得货物整合的重要经济效益。<details>
<summary>Abstract</summary>
The load planning problem is a critical challenge in service network design for parcel carriers: it decides how many trailers (or loads) to assign for dispatch over time between pairs of terminals. Another key challenge is to determine a flow plan, which specifies how parcel volumes are assigned to planned loads. This paper considers the Dynamic Load Planning Problem (DLPP) that considers both flow and load planning challenges jointly to adjust loads and flows as the demand forecast changes over time before the day of operations. The paper aims at developing a decision-support tool to inform planners making these decisions at terminals across the network. The paper formulates the DLPP as a MIP and shows that it admits a large number of symmetries in a network where each commodity can be routed through primary and alternate paths. As a result, an optimization solver may return fundamentally different solutions to closely related problems, confusing planners and reducing trust in optimization. To remedy this limitation, the paper proposes a Goal-Directed Optimization that eliminates those symmetries by generating optimal solutions staying close to a reference plan. The paper also proposes an optimization proxy to address the computational challenges of the optimization models. The proxy combines a machine learning model and a feasibility restoration model and finds solutions that satisfy real-time constraints imposed by planners-in-the-loop. An extensive computational study on industrial instances shows that the optimization proxy is around 10 times faster than the commercial solver in obtaining the same quality solutions and orders of magnitude faster for generating solutions that are consistent with each other. The proposed approach also demonstrates the benefits of the DLPP for load consolidation, and the significant savings obtained from combining machine learning and optimization.
</details>
<details>
<summary>摘要</summary>
服务网络设计中的负载观念问题是货运公司的核心挑战：它决定在时间上将多少货车（或货量）分配给发送。另一个关键挑战是决定流程计划，它决定了各种货物的分配量。这篇文章考虑了时间统计负载观念问题（DLPP），考虑了流程和负载观念问题的 JOINT 解决方案，以适应变化的需求预测。文章的目标是为终端站规划人创建一个决策支持工具，以帮助他们在网络中做出这些决策。文章使用了数理统计方法来形式化 DLPP，并证明了网络中每个商品可以通过主要和备用路径进行 routed。这导致优化 solver 可能会返回对应的解，导致计划师和优化模型之间的不一致，从而减少优化的信任度。为解决这个限制，文章提出了目标导向优化方法，删除网络中的 symmetries，从而确保优化模型返回的解是固定的。文章还提出了一个优化代理，将数理统计方法与可行性修复模型结合，以确保解决方案满足了现场实际的时间限制。一系列的 Computational Study 表明，该优化代理比商业 solver 更快，可以在获得相同质量解决方案的情况下节省大量时间。文章还证明了 DLPP 的负载整合和机器学习优化的好处，并获得了负载观念问题的答案。
</details></li>
</ul>
<hr>
<h2 id="Sup-Norm-Convergence-of-Deep-Neural-Network-Estimator-for-Nonparametric-Regression-by-Adversarial-Training"><a href="#Sup-Norm-Convergence-of-Deep-Neural-Network-Estimator-for-Nonparametric-Regression-by-Adversarial-Training" class="headerlink" title="Sup-Norm Convergence of Deep Neural Network Estimator for Nonparametric Regression by Adversarial Training"></a>Sup-Norm Convergence of Deep Neural Network Estimator for Nonparametric Regression by Adversarial Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04042">http://arxiv.org/abs/2307.04042</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masaaki Imaizumi</li>
<li>for: 这个论文主要探讨了深度神经网络估计器的sup-norm收敛性，并提出了一种新的对抗训练方案来实现这一目标。</li>
<li>methods: 作者使用了深度神经网络模型来解决非参数统计问题，并通过对抗训练方案来实现sup-norm收敛性。</li>
<li>results: 研究发现，通过对抗训练方案，深度神经网络估计器可以在$L2$-norm上达到更好的性能，而且可以在sup-norm上收敛。此外，作者还扩展了对抗训练方案到更通用的损失函数和数据生成函数上。实验结果支持了理论发现。<details>
<summary>Abstract</summary>
We show the sup-norm convergence of deep neural network estimators with a novel adversarial training scheme. For the nonparametric regression problem, it has been shown that an estimator using deep neural networks can achieve better performances in the sense of the $L2$-norm. In contrast, it is difficult for the neural estimator with least-squares to achieve the sup-norm convergence, due to the deep structure of neural network models. In this study, we develop an adversarial training scheme and investigate the sup-norm convergence of deep neural network estimators. First, we find that ordinary adversarial training makes neural estimators inconsistent. Second, we show that a deep neural network estimator achieves the optimal rate in the sup-norm sense by the proposed adversarial training with correction. We extend our adversarial training to general setups of a loss function and a data-generating function. Our experiments support the theoretical findings.
</details>
<details>
<summary>摘要</summary>
我们展示了深度神经网络估计器的sup-norm收敛性，使用一个新的反抗训练方案。在非 Parametric 回授问题中，已经证明了使用深度神经网络可以实现更好的性能在 $L2$-norm 的意义上。然而，使用最小二乘训练的神经网络估计器很难实现sup-norm收敛性，因为神经网络模型的深度结构。在这个研究中，我们开发了一个反抗训练方案，并调查深度神经网络估计器的sup-norm收敛性。首先，我们发现了普通的反抗训练会使神经网络估计器不一致。其次，我们显示了一个深度神经网络估计器可以通过我们的反抗训练方案和修正得到最佳的sup-norm收敛性。我们将我们的反抗训练扩展到普通的损失函数和数据生成函数的情况下。我们的实验支持了理论的结论。
</details></li>
</ul>
<hr>
<h2 id="The-Value-of-Chess-Squares"><a href="#The-Value-of-Chess-Squares" class="headerlink" title="The Value of Chess Squares"></a>The Value of Chess Squares</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05330">http://arxiv.org/abs/2307.05330</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Dpay123/chess">https://github.com/Dpay123/chess</a></li>
<li>paper_authors: Aditya Gupta, Shiva Maharaj, Nicholas Polson, Vadim Sokolov<br>for: 研究棋盘上棋子的分布和评估棋盘的价值。methods: 引入杂入valuation的方法，包括对棋子和棋盘进行评估。results: 研究发现， Knight和Bishop的位置有着重要的影响，而Pawn的价值也需要考虑棋盘结构。同时，研究还提供了有价值的 Pawn 评估方法。<details>
<summary>Abstract</summary>
Valuing chess squares and determining the placement of pieces on the board are the main objectives of our study. With the emergence of chess AI, it has become possible to accurately assess the worth of positions in a game of chess. The conventional approach assigns fixed values to pieces $(\symking=\infty, \symqueen=9, \symrook=5, \symbishop=3, \symknight=3, \sympawn=1)$. We enhance this analysis by introducing marginal valuations for both pieces and squares. We demonstrate our method by examining the positioning of Knights and Bishops, and also provide valuable insights into the valuation of pawns. Notably, Nimzowitsch was among the pioneers in advocating for the significance of Pawn structure and valuation. Finally, we conclude by suggesting potential avenues for future research.
</details>
<details>
<summary>摘要</summary>
我们的研究目标是评估棋盘上的棋子和棋子的位置。随着棋盘AI的出现，我们可以准确评估棋盘上的位置价值。传统方法将棋子分别赋予固定价值（♔∞、♕9、♖5、♗3、♘3、♟1）。我们增强这种分析方法，引入棋子和棋盘的边缘价值。我们通过审查夜报和主教的位置，提供有价值的反馈，并探讨了它们的价值。值得注意的是，尼莫雷维茨（Nimzowitsch）是棋盘结构和价值评估的先驱者之一。最后，我们提出了未来研究的可能性。Note that the piece names in Simplified Chinese are:♔ 皇后 (queen)♕ 王后 (rook)♖  bishop♗ night♘ knight♟ Pawn
</details></li>
</ul>
<hr>
<h2 id="Designing-a-Direct-Feedback-Loop-between-Humans-and-Convolutional-Neural-Networks-through-Local-Explanations"><a href="#Designing-a-Direct-Feedback-Loop-between-Humans-and-Convolutional-Neural-Networks-through-Local-Explanations" class="headerlink" title="Designing a Direct Feedback Loop between Humans and Convolutional Neural Networks through Local Explanations"></a>Designing a Direct Feedback Loop between Humans and Convolutional Neural Networks through Local Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04036">http://arxiv.org/abs/2307.04036</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tongstevensun/deepfuse">https://github.com/tongstevensun/deepfuse</a></li>
<li>paper_authors: Tong Steven Sun, Yuyang Gao, Shubham Khaladkar, Sijia Liu, Liang Zhao, Young-Ho Kim, Sungsoo Ray Hong</li>
<li>For: This paper aims to improve the explainability of Convolutional Neural Networks (CNNs) by providing a more interactive and labor-efficient design for diagnosing and revising CNN vulnerabilities using local explanations.* Methods: The paper proposes an interactive design called DeepFuse, which realizes a direct feedback loop between a user and CNNs in diagnosing and revising CNN vulnerabilities using local explanations.* Results: The paper reports the results of a two-day study with 12 experienced CNN engineers, showing that DeepFuse helps participants create more accurate and “reasonable” models than the current state-of-the-art, and that participants found the way DeepFuse guides case-based reasoning to be practically useful for their current practice.Here’s the Chinese translation of the three points:* For: 这篇论文目的是提高卷积神经网络（CNN）的可解释性，通过提供更有优化的交互式和劳动效率的设计，以便用户更好地诊断和修复 CNN 的潜在漏洞。* Methods: 论文提出了一种交互式的设计方案called DeepFuse，它实现了用户和 CNN 之间的直接反馈循环，以便更好地诊断和修复 CNN 的潜在漏洞。* Results: 论文报告了一项两天的研究， involvement 12名有经验的 CNN 工程师，结果显示，使用 DeepFuse 可以帮助参与者创建更准确和 “合理” 的模型，并且参与者认为 DeepFuse 引导的案例分析方法是实际上有用的 для他们当前的做法。<details>
<summary>Abstract</summary>
The local explanation provides heatmaps on images to explain how Convolutional Neural Networks (CNNs) derive their output. Due to its visual straightforwardness, the method has been one of the most popular explainable AI (XAI) methods for diagnosing CNNs. Through our formative study (S1), however, we captured ML engineers' ambivalent perspective about the local explanation as a valuable and indispensable envision in building CNNs versus the process that exhausts them due to the heuristic nature of detecting vulnerability. Moreover, steering the CNNs based on the vulnerability learned from the diagnosis seemed highly challenging. To mitigate the gap, we designed DeepFuse, the first interactive design that realizes the direct feedback loop between a user and CNNs in diagnosing and revising CNN's vulnerability using local explanations. DeepFuse helps CNN engineers to systemically search "unreasonable" local explanations and annotate the new boundaries for those identified as unreasonable in a labor-efficient manner. Next, it steers the model based on the given annotation such that the model doesn't introduce similar mistakes. We conducted a two-day study (S2) with 12 experienced CNN engineers. Using DeepFuse, participants made a more accurate and "reasonable" model than the current state-of-the-art. Also, participants found the way DeepFuse guides case-based reasoning can practically improve their current practice. We provide implications for design that explain how future HCI-driven design can move our practice forward to make XAI-driven insights more actionable.
</details>
<details>
<summary>摘要</summary>
当地解释提供了图像上的热图，以解释卷积神经网络（CNN）的输出。由于其可见直观，这种方法在诊断CNN方面变得非常流行。然而，我们在第一个研究（S1）中发现，机器学习工程师对当地解释持有两样的看法：一方面，他们认为这种方法是非常有价值和必不可少的视角，另一方面，由于探测漏洞的做法是尝试性的，因此导致了困惑。此外，根据诊断得到的漏洞来控制CNN也显得很困难。为了弥合这个差距，我们设计了深度融合（DeepFuse），这是第一个可互动的设计，可以在诊断和修复CNN漏洞中直接将用户和CNN之间建立反馈循环。深度融合帮助CNN工程师系统地搜索"不合理"的当地解释，并在劳动效率高的情况下注释新的边界。然后，它会根据给出的注释来导向模型，以避免模型 introduce类似的错误。我们在第二个研究（S2）中与12名经验丰富的CNN工程师进行了两天的研究。使用深度融合，参与者可以建立更加准确和"合理"的模型，并发现了深度融合在诊断过程中的指导作用。我们提供了设计意味着，解释如何未来驱动的设计可以将XAI驱动的洞察力转化为实际行动。
</details></li>
</ul>
<hr>
<h2 id="Learning-Variational-Neighbor-Labels-for-Test-Time-Domain-Generalization"><a href="#Learning-Variational-Neighbor-Labels-for-Test-Time-Domain-Generalization" class="headerlink" title="Learning Variational Neighbor Labels for Test-Time Domain Generalization"></a>Learning Variational Neighbor Labels for Test-Time Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04033">http://arxiv.org/abs/2307.04033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sameer Ambekar, Zehao Xiao, Jiayi Shen, Xiantong Zhen, Cees G. M. Snoek</li>
<li>for: 本研究探讨域间泛化问题，即将模型在不同域面上进行训练后在未知目标域上进行测试。</li>
<li>methods: 我们提出了三种贡献：首先，我们提出了 probabilistic pseudo-labeling 方法，通过模型在测试时对目标样本的推测来对源域训练过的模型进行泛化。其次，我们学习了variational neighbor labels，将邻域目标样本的信息integrated到pseudo标签中，以生成更加稳定和robust的pseudo标签。最后，我们引入了元泛化阶段，通过模拟泛化过程来帮助模型更好地integrate更多的目标信息和生成更加精准和robust的variational neighbor labels。</li>
<li>results: 我们在六个常用的数据集上进行了实验，结果表明我们的提议可以提高模型的泛化能力，并且可以在不同的域面上进行更好的泛化。<details>
<summary>Abstract</summary>
This paper strives for domain generalization, where models are trained exclusively on source domains before being deployed at unseen target domains. We follow the strict separation of source training and target testing but exploit the value of the unlabeled target data itself during inference. We make three contributions. First, we propose probabilistic pseudo-labeling of target samples to generalize the source-trained model to the target domain at test time. We formulate the generalization at test time as a variational inference problem by modeling pseudo labels as distributions to consider the uncertainty during generalization and alleviate the misleading signal of inaccurate pseudo labels. Second, we learn variational neighbor labels that incorporate the information of neighboring target samples to generate more robust pseudo labels. Third, to learn the ability to incorporate more representative target information and generate more precise and robust variational neighbor labels, we introduce a meta-generalization stage during training to simulate the generalization procedure. Experiments on six widely-used datasets demonstrate the benefits, abilities, and effectiveness of our proposal.
</details>
<details>
<summary>摘要</summary>
这篇论文努力实现领域总结，即模型在训练后在未见目标领域中进行部署。我们遵循严格的源训练和目标测试的分离，但是利用目标数据本身的价值进行推断。我们提出了三项贡献：一、在测试时对目标样本进行 probabilistic pseudo-标签，以使源训练模型在目标领域进行普适化。我们将总结问题形式为变量推理问题，模型 pseudo labels 为分布来考虑不确定性，以避免 pseudo labels 的不准确信号。二、我们学习了变量邻域标签，以包含邻近目标样本的信息生成更加稳健的 pseudo labels。三、为了学习更好地包含更多的目标信息并生成更精准和稳定的变量邻域标签，我们引入了元总结阶段在训练中进行模拟总结过程。我们在六种广泛使用的数据集上进行了实验，并证明了我们的提议的优点、能力和有效性。
</details></li>
</ul>
<hr>
<h2 id="Measuring-the-Success-of-Diffusion-Models-at-Imitating-Human-Artists"><a href="#Measuring-the-Success-of-Diffusion-Models-at-Imitating-Human-Artists" class="headerlink" title="Measuring the Success of Diffusion Models at Imitating Human Artists"></a>Measuring the Success of Diffusion Models at Imitating Human Artists</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04028">http://arxiv.org/abs/2307.04028</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Casper, Zifan Guo, Shreya Mogulothu, Zachary Marinov, Chinmay Deshpande, Rui-Jie Yew, Zheng Dai, Dylan Hadfield-Menell</li>
<li>for: 这个论文旨在检查生成模型是否可以模仿人类艺术家的作品。</li>
<li>methods: 这个论文使用了语义映射学习模型CLIP来测试生成模型是否可以模仿特定艺术家的作品。</li>
<li>results: 研究发现，当生成模型被训练时，可以很准确地模仿70名艺术家的作品，并且可以在图像上匹配这些作品的特征。<details>
<summary>Abstract</summary>
Modern diffusion models have set the state-of-the-art in AI image generation. Their success is due, in part, to training on Internet-scale data which often includes copyrighted work. This prompts questions about the extent to which these models learn from, imitate, or copy the work of human artists. This work suggests that tying copyright liability to the capabilities of the model may be useful given the evolving ecosystem of generative models. Specifically, much of the legal analysis of copyright and generative systems focuses on the use of protected data for training. As a result, the connections between data, training, and the system are often obscured. In our approach, we consider simple image classification techniques to measure a model's ability to imitate specific artists. Specifically, we use Contrastive Language-Image Pretrained (CLIP) encoders to classify images in a zero-shot fashion. Our process first prompts a model to imitate a specific artist. Then, we test whether CLIP can be used to reclassify the artist (or the artist's work) from the imitation. If these tests match the imitation back to the original artist, this suggests the model can imitate that artist's expression. Our approach is simple and quantitative. Furthermore, it uses standard techniques and does not require additional training. We demonstrate our approach with an audit of Stable Diffusion's capacity to imitate 70 professional digital artists with copyrighted work online. When Stable Diffusion is prompted to imitate an artist from this set, we find that the artist can be identified from the imitation with an average accuracy of 81.0%. Finally, we also show that a sample of the artist's work can be matched to these imitation images with a high degree of statistical reliability. Overall, these results suggest that Stable Diffusion is broadly successful at imitating individual human artists.
</details>
<details>
<summary>摘要</summary>
现代扩散模型已经提高了人工智能图像生成的水平。这些模型的成功部分归功于训练在互联网规模的数据上，这些数据经常包含版权保护的作品。这种情况引发了关于模型学习、模仿或复制人类艺术家的作品的问题。本研究表明，将版权责任与模型的能力相关可能是有用的，尤其在生成模型生态系统不断演化的情况下。现在的法律分析对版权和生成系统的关系都集中在使用保护数据进行训练上。因此，数据、训练和系统之间的连接经常被隐藏。我们的方法是通过使用语言-图像预训练（CLIP）编码器来测试模型是否可以模仿特定艺术家。我们的过程是先让模型模仿特定艺术家，然后使用CLIP来重新分类艺术家（或者艺术家的作品）。如果这些测试与模仿相匹配，这表明模型可以模仿该艺术家的表达。我们的方法简单、量化，并且不需要额外训练。我们通过对Stable Diffusion的可imitability进行审核，发现这些模型可以成功地模仿70名职业数字艺术家的版权作品。当Stable Diffusion被要求模仿这些艺术家时，我们发现这些艺术家可以通过模仿的方式与模型的作品相匹配，平均准确率为81.0%。此外，我们还发现这些模仿作品与艺术家的作品之间存在高度的统计相似性。总之，这些结果表明Stable Diffusion可以广泛地模仿人类艺术家。
</details></li>
</ul>
<hr>
<h2 id="Robust-Ranking-Explanations"><a href="#Robust-Ranking-Explanations" class="headerlink" title="Robust Ranking Explanations"></a>Robust Ranking Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04024">http://arxiv.org/abs/2307.04024</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Chen, Chenghua Guo, Guixiang Ma, Ming Zeng, Xi Zhang, Sihong Xie</li>
<li>for: 提高机器学习模型的解释可靠性，以便建立人类对模型的信任。</li>
<li>methods: 使用 explanation thickness 度量精要特征的排名稳定性，并 derivate tractable surrogate bounds 来设计 \textit{R2ET} 算法，以高效地提高精要特征的鲁棒性。</li>
<li>results: 对各种网络架构和数据模式，包括大脑网络，实验表明 R2ET 可以在恶意攻击下增强解释鲁棒性，保持准确性。<details>
<summary>Abstract</summary>
Robust explanations of machine learning models are critical to establish human trust in the models. Due to limited cognition capability, most humans can only interpret the top few salient features. It is critical to make top salient features robust to adversarial attacks, especially those against the more vulnerable gradient-based explanations. Existing defense measures robustness using $\ell_p$-norms, which have weaker protection power. We define explanation thickness for measuring salient features ranking stability, and derive tractable surrogate bounds of the thickness to design the \textit{R2ET} algorithm to efficiently maximize the thickness and anchor top salient features. Theoretically, we prove a connection between R2ET and adversarial training. Experiments with a wide spectrum of network architectures and data modalities, including brain networks, demonstrate that R2ET attains higher explanation robustness under stealthy attacks while retaining accuracy.
</details>
<details>
<summary>摘要</summary>
机器学习模型的强健解释是建立人类信任的关键。由于人类认知能力有限，大多数人只能理解最重要的几个突出特征。因此，使得最重要的突出特征对抗攻击，特别是对于更脆弱的梯度基于解释而言，是非常重要的。现有的防御措施通过 $\ell_p$-norm 来实现强健性，但这些措施具有较弱的保护力。我们定义解释厚度来衡量突出特征排名稳定性，并 deriv 出可观测的代理 bound 来设计 \textit{R2ET} 算法，以高效地提高厚度和固定最重要的突出特征。理论上，我们证明 R2ET 与反击训练之间存在联系。实验结果表明，R2ET 在隐蔽攻击下具有更高的解释强度，同时保持准确性。
</details></li>
</ul>
<hr>
<h2 id="Robust-Learning-Based-Incipient-Slip-Detection-using-the-PapillArray-Optical-Tactile-Sensor-for-Improved-Robotic-Gripping"><a href="#Robust-Learning-Based-Incipient-Slip-Detection-using-the-PapillArray-Optical-Tactile-Sensor-for-Improved-Robotic-Gripping" class="headerlink" title="Robust Learning-Based Incipient Slip Detection using the PapillArray Optical Tactile Sensor for Improved Robotic Gripping"></a>Robust Learning-Based Incipient Slip Detection using the PapillArray Optical Tactile Sensor for Improved Robotic Gripping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04011">http://arxiv.org/abs/2307.04011</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiang Wang, Pablo Martinez Ulloa, Robert Burke, David Cordova Bulens, Stephen J. Redmond</li>
<li>for:  This paper aims to detect incipient slip in robotic gripping tasks using a learning-based approach with the PapillArray tactile sensor.</li>
<li>methods: The proposed approach uses a machine learning model to identify patterns associated with incipient slip, and the model is trained using data augmentation techniques to enhance its robustness.</li>
<li>results: The proposed approach achieved a high detection success rate of 95.6% when tested with an offline dataset, and maintained robust performance with a success rate of 96.8% when transferred to a robotic gripping environment distinct from where the training data was collected.Here is the information in Simplified Chinese text:</li>
<li>for: 本研究旨在使用学习基本法探测机器人抓取任务中的incipient slip。</li>
<li>methods: 提议的方法使用机器学习模型识别incipient slip的模式，并通过数据增强技术提高模型的可靠性。</li>
<li>results: 提议的方法在测试数据集上达到了95.6%的检测成功率，并在将训练数据集 transferred to robotic gripping环境中保持了96.8%的成功率。<details>
<summary>Abstract</summary>
The ability to detect slip, particularly incipient slip, enables robotic systems to take corrective measures to prevent a grasped object from being dropped. Therefore, slip detection can enhance the overall security of robotic gripping. However, accurately detecting incipient slip remains a significant challenge. In this paper, we propose a novel learning-based approach to detect incipient slip using the PapillArray (Contactile, Australia) tactile sensor. The resulting model is highly effective in identifying patterns associated with incipient slip, achieving a detection success rate of 95.6% when tested with an offline dataset. Furthermore, we introduce several data augmentation methods to enhance the robustness of our model. When transferring the trained model to a robotic gripping environment distinct from where the training data was collected, our model maintained robust performance, with a success rate of 96.8%, providing timely feedback for stabilizing several practical gripping tasks. Our project website: https://sites.google.com/view/incipient-slip-detection.
</details>
<details>
<summary>摘要</summary>
“感知滑动，特别是潜在滑动，可以让机器人系统采取正确的措施以防止握住的物品被掉落。因此，滑动检测可以提高机器人握住的安全性。但是，准确地检测潜在滑动仍然是一项重要的挑战。在这篇论文中，我们提出了一种基于学习的方法来检测潜在滑动，使用Contactile（澳大利亚）的PapillArray感知器。我们的模型可以高效地识别潜在滑动的模式，在测试集上达到95.6%的检测成功率。此外，我们还提出了一些数据增强方法来提高我们的模型的稳定性。当将训练模型应用于机器人握住环境中，与训练数据集不同的环境下，我们的模型保持了96.8%的成功率，提供了实时反馈，以稳定许多实际的握住任务。更多信息请访问我们的项目网站：https://sites.google.com/view/incipient-slip-detection。”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know and I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Understanding-the-Efficacy-of-U-Net-Vision-Transformer-for-Groundwater-Numerical-Modelling"><a href="#Understanding-the-Efficacy-of-U-Net-Vision-Transformer-for-Groundwater-Numerical-Modelling" class="headerlink" title="Understanding the Efficacy of U-Net &amp; Vision Transformer for Groundwater Numerical Modelling"></a>Understanding the Efficacy of U-Net &amp; Vision Transformer for Groundwater Numerical Modelling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04010">http://arxiv.org/abs/2307.04010</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maria Luisa Taccari, Oded Ovadia, He Wang, Adar Kahana, Xiaohui Chen, Peter K. Jimack</li>
<li>for: 本研究比较了不同机器学习模型（U-Net、U-Net+ViT和FNO）在地下水系统中的时间依赖前向模型。</li>
<li>methods: 本研究使用了Synthetic数据进行测试，并证明了U-Net和U-Net+ViT模型在精度和效率方面在缺少数据情况下表现出色，特别是在缺少数据情况下。</li>
<li>results: 研究发现，U-Net和U-Net+ViT模型在缺少数据情况下的准确率和效率较高，这些结果表明U-Net基于模型在实际应用中的地下水模拟中具有潜力。<details>
<summary>Abstract</summary>
This paper presents a comprehensive comparison of various machine learning models, namely U-Net, U-Net integrated with Vision Transformers (ViT), and Fourier Neural Operator (FNO), for time-dependent forward modelling in groundwater systems. Through testing on synthetic datasets, it is demonstrated that U-Net and U-Net + ViT models outperform FNO in accuracy and efficiency, especially in sparse data scenarios. These findings underscore the potential of U-Net-based models for groundwater modelling in real-world applications where data scarcity is prevalent.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Polynomial-Width-is-Sufficient-for-Set-Representation-with-High-dimensional-Features"><a href="#Polynomial-Width-is-Sufficient-for-Set-Representation-with-High-dimensional-Features" class="headerlink" title="Polynomial Width is Sufficient for Set Representation with High-dimensional Features"></a>Polynomial Width is Sufficient for Set Representation with High-dimensional Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04001">http://arxiv.org/abs/2307.04001</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peihao Wang, Shenghao Yang, Shu Li, Zhangyang Wang, Pan Li</li>
<li>for: 研究深度学习中集合表示的维度$L$的影响，以确定最小的$L$值可以实现足够的表达力。</li>
<li>methods: 使用两种集合元素嵌入层：(a) 线性+指数活化（LP）和(b) 线性+指数活化（LE），并证明$L$可以是$N$和$D$的乘积。</li>
<li>results: 表示$L$可以是$N$和$D$的乘积，并提供了对LP嵌入层的下界。此外，我们还扩展了结果到卷积Equivariant集函数和复数域。<details>
<summary>Abstract</summary>
Set representation has become ubiquitous in deep learning for modeling the inductive bias of neural networks that are insensitive to the input order. DeepSets is the most widely used neural network architecture for set representation. It involves embedding each set element into a latent space with dimension $L$, followed by a sum pooling to obtain a whole-set embedding, and finally mapping the whole-set embedding to the output. In this work, we investigate the impact of the dimension $L$ on the expressive power of DeepSets. Previous analyses either oversimplified high-dimensional features to be one-dimensional features or were limited to analytic activations, thereby diverging from practical use or resulting in $L$ that grows exponentially with the set size $N$ and feature dimension $D$. To investigate the minimal value of $L$ that achieves sufficient expressive power, we present two set-element embedding layers: (a) linear + power activation (LP) and (b) linear + exponential activations (LE). We demonstrate that $L$ being poly$(N, D)$ is sufficient for set representation using both embedding layers. We also provide a lower bound of $L$ for the LP embedding layer. Furthermore, we extend our results to permutation-equivariant set functions and the complex field.
</details>
<details>
<summary>摘要</summary>
设 Representation 已经成为深度学习中广泛使用的 inductive bias，用于建立不受输入顺序影响的神经网络。DeepSets 是最广泛使用的集合表示方法之一，它包括将每个集合元素embedding到一个维度为 $L$ 的隐藏空间中，然后使用总池化以获取整个集合的权重，最后将整个集合权重映射到输出。在这项工作中，我们研究了 $L$ 的缩放对 DeepSets 的表达力的影响。先前的分析都是将高维特征简化为一维特征，或者只是对分析性活动进行了限制，从而与实际应用不符或者 $L$ 与集合大小 $N$ 和特征维度 $D$ 相乘 exponential 增长。为了研究最小的 $L$ 可以 достичь足够的表达力，我们提出了两种集合元素嵌入层：（a）线性 + 力活动（LP）和（b）线性 + 指数活动（LE）。我们示出 $L$ 是 poly $(N, D)$ 是 sufficient for set representation 的。我们还提供了 LP 嵌入层的下界。此外，我们将结果推广到卷积同态函数和复数域。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/09/cs.LG_2023_07_09/" data-id="closbroq000lz0g88gvp39sov" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/09/eess.IV_2023_07_09/" class="article-date">
  <time datetime="2023-07-09T09:00:00.000Z" itemprop="datePublished">2023-07-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/09/eess.IV_2023_07_09/">eess.IV - 2023-07-09</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Ultrasonic-Image’s-Annotation-Removal-A-Self-supervised-Noise2Noise-Approach"><a href="#Ultrasonic-Image’s-Annotation-Removal-A-Self-supervised-Noise2Noise-Approach" class="headerlink" title="Ultrasonic Image’s Annotation Removal: A Self-supervised Noise2Noise Approach"></a>Ultrasonic Image’s Annotation Removal: A Self-supervised Noise2Noise Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04133">http://arxiv.org/abs/2307.04133</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/grandarth/ultrasonicimage-n2n-approach">https://github.com/grandarth/ultrasonicimage-n2n-approach</a></li>
<li>paper_authors: Yuanheng Zhang, Nan Jiang, Zhaoheng Xie, Junying Cao, Yueyang Teng</li>
<li>for: 提高医疗报告质量的高级医疗图像标注数据集的自动化处理。</li>
<li>methods: 使用噪声作为预 Text task，使用Noise2Noise scheme进行模型训练，以恢复图像到干净状态。</li>
<li>results: 对不同类型的标注数据进行测试，大多数基于Noise2Noise scheme的模型在噪声恢复任务中表现出色，特别是使用自定义U-Net结构在Body marker标注数据集上得到了最佳效果，具有高精度和高重建相似性。<details>
<summary>Abstract</summary>
Accurately annotated ultrasonic images are vital components of a high-quality medical report. Hospitals often have strict guidelines on the types of annotations that should appear on imaging results. However, manually inspecting these images can be a cumbersome task. While a neural network could potentially automate the process, training such a model typically requires a dataset of paired input and target images, which in turn involves significant human labour. This study introduces an automated approach for detecting annotations in images. This is achieved by treating the annotations as noise, creating a self-supervised pretext task and using a model trained under the Noise2Noise scheme to restore the image to a clean state. We tested a variety of model structures on the denoising task against different types of annotation, including body marker annotation, radial line annotation, etc. Our results demonstrate that most models trained under the Noise2Noise scheme outperformed their counterparts trained with noisy-clean data pairs. The costumed U-Net yielded the most optimal outcome on the body marker annotation dataset, with high scores on segmentation precision and reconstruction similarity. We released our code at https://github.com/GrandArth/UltrasonicImage-N2N-Approach.
</details>
<details>
<summary>摘要</summary>
高品质医疗报告中的准确标注图像是关键组成部分。医院通常有严格的图像标注规范，但手动检查这些图像可以是一项繁琐的任务。使用神经网络自动化这个过程可能是一个解决方案，但是训练这种模型通常需要一个包含输入和目标图像的 paired 数据集，这需要很多人工劳动。本研究提出了一种自动化图像标注检测方法。这是通过将标注视为噪声，创建一种自我超vised pretext task，并使用基于 Noise2Noise 方案训练的模型来还原图像为一个清晰的状态来实现的。我们测试了不同类型的标注，包括体 markers 标注和径向线标注等，并评估了不同模型结构的性能。我们的结果表明，大多数基于 Noise2Noise 方案训练的模型在杂噪clean 数据对比下表现出色，并且用于体 markers 标注集上的 customized U-Net 得到了最佳的结果，其segmentation精度和重建相似度均达到了高水平。我们的代码可以在 <https://github.com/GrandArth/UltrasonicImage-N2N-Approach> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Low-Light-Images-Using-Infrared-Encoded-Images"><a href="#Enhancing-Low-Light-Images-Using-Infrared-Encoded-Images" class="headerlink" title="Enhancing Low-Light Images Using Infrared-Encoded Images"></a>Enhancing Low-Light Images Using Infrared-Encoded Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04122">http://arxiv.org/abs/2307.04122</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wyf0912/ELIEI">https://github.com/wyf0912/ELIEI</a></li>
<li>paper_authors: Shulin Tian, Yufei Wang, Renjie Wan, Wenhan Yang, Alex C. Kot, Bihan Wen</li>
<li>for: 提高低光照图像的可见度和细节表示</li>
<li>methods: 移除卡口内的折射光镜Filter，使用更多的照明信息从近infraredpectrum中获取更高的信噪比</li>
<li>results: 对比 referencetest dataset，提出的方法能够更好地提高低光照图像的可见度和细节表示，并且量化和 каче地比较好<details>
<summary>Abstract</summary>
Low-light image enhancement task is essential yet challenging as it is ill-posed intrinsically. Previous arts mainly focus on the low-light images captured in the visible spectrum using pixel-wise loss, which limits the capacity of recovering the brightness, contrast, and texture details due to the small number of income photons. In this work, we propose a novel approach to increase the visibility of images captured under low-light environments by removing the in-camera infrared (IR) cut-off filter, which allows for the capture of more photons and results in improved signal-to-noise ratio due to the inclusion of information from the IR spectrum. To verify the proposed strategy, we collect a paired dataset of low-light images captured without the IR cut-off filter, with corresponding long-exposure reference images with an external filter. The experimental results on the proposed dataset demonstrate the effectiveness of the proposed method, showing better performance quantitatively and qualitatively. The dataset and code are publicly available at https://wyf0912.github.io/ELIEI/
</details>
<details>
<summary>摘要</summary>
低光照图像提升任务是必备又挑战的，因为它是一个内在不定的问题。先前的艺术主要通过像素精度损失来处理低光照图像，这限制了恢复图像的亮度、对比度和细节的能力，因为可得到的光子数很少。在这个工作中，我们提议一种新的方法，利用取消相机内置的红外（IR）剔除filter，以获取更多的光子数，从而提高信号噪声比。为验证提议的策略，我们收集了一个对应的低光照图像集和长曝光参照图像集，使用外部滤镜获取。实验结果表明，提议的方法具有更好的数量和质量性能。数据集和代码在https://wyf0912.github.io/ELIEI/上公开。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Building-Semantic-Segmentation-Accuracy-with-Super-Resolution-and-Deep-Learning-Investigating-the-Impact-of-Spatial-Resolution-on-Various-Datasets"><a href="#Enhancing-Building-Semantic-Segmentation-Accuracy-with-Super-Resolution-and-Deep-Learning-Investigating-the-Impact-of-Spatial-Resolution-on-Various-Datasets" class="headerlink" title="Enhancing Building Semantic Segmentation Accuracy with Super Resolution and Deep Learning: Investigating the Impact of Spatial Resolution on Various Datasets"></a>Enhancing Building Semantic Segmentation Accuracy with Super Resolution and Deep Learning: Investigating the Impact of Spatial Resolution on Various Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04101">http://arxiv.org/abs/2307.04101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiling Guo, Xiaodan Shi, Haoran Zhang, Dou Huang, Xiaoya Song, Jinyue Yan, Ryosuke Shibasaki</li>
<li>for: 本研究旨在探讨深度学习基于远程感知技术的建筑 semantics 分割中 spatial resolution 的影响。</li>
<li>methods: 本研究使用了 super-resolution 和 down-sampling 技术将 remote sensing 图像转化为多个空间分辨率，然后选择了 UNet 和 FPN 两种深度学习模型进行训练和测试。</li>
<li>results: 实验结果显示，建筑 semantics 分割结果受到空间分辨率的影响，并且在约 0.3m 的空间分辨率下达到最佳成本效果。<details>
<summary>Abstract</summary>
The development of remote sensing and deep learning techniques has enabled building semantic segmentation with high accuracy and efficiency. Despite their success in different tasks, the discussions on the impact of spatial resolution on deep learning based building semantic segmentation are quite inadequate, which makes choosing a higher cost-effective data source a big challenge. To address the issue mentioned above, in this study, we create remote sensing images among three study areas into multiple spatial resolutions by super-resolution and down-sampling. After that, two representative deep learning architectures: UNet and FPN, are selected for model training and testing. The experimental results obtained from three cities with two deep learning models indicate that the spatial resolution greatly influences building segmentation results, and with a better cost-effectiveness around 0.3m, which we believe will be an important insight for data selection and preparation.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "The development of remote sensing and deep learning techniques has enabled building semantic segmentation with high accuracy and efficiency. Despite their success in different tasks, the discussions on the impact of spatial resolution on deep learning based building semantic segmentation are quite inadequate, which makes choosing a higher cost-effective data source a big challenge. To address the issue mentioned above, in this study, we create remote sensing images among three study areas into multiple spatial resolutions by super-resolution and down-sampling. After that, two representative deep learning architectures: UNet and FPN, are selected for model training and testing. The experimental results obtained from three cities with two deep learning models indicate that the spatial resolution greatly influences building segmentation results, and with a better cost-effectiveness around 0.3m, which we believe will be an important insight for data selection and preparation." into Chinese (Simplified)📝发展远程感知和深度学习技术，使得建筑 semantic segmentation 的准确率和效率得到了大幅提高。然而，关于深度学习基于建筑 semantic segmentation 的空间分辨率影响的讨论，尚不充分，这使得选择更高效果和经济的数据源成为一大挑战。为解决上述问题，本研究中将Remote sensing 图像在三个研究区域中进行多个空间分辨率的创建，通过超分辨和降采样。然后，选择了两种代表性的深度学习架构：UNet 和 FPN。通过三座城市的两个深度学习模型的实验结果显示，空间分辨率对建筑 segmentation 结果产生了极大的影响，并且在约0.3米的成本效果上表现较好。我们认为这将成为数据选择和准备的重要意见。
</details></li>
</ul>
<hr>
<h2 id="Combining-transmission-speckle-photography-and-convolutional-neural-network-for-determination-of-fat-content-in-cow’s-milk-–-an-exercise-in-classification-of-parameters-of-a-complex-suspension"><a href="#Combining-transmission-speckle-photography-and-convolutional-neural-network-for-determination-of-fat-content-in-cow’s-milk-–-an-exercise-in-classification-of-parameters-of-a-complex-suspension" class="headerlink" title="Combining transmission speckle photography and convolutional neural network for determination of fat content in cow’s milk – an exercise in classification of parameters of a complex suspension"></a>Combining transmission speckle photography and convolutional neural network for determination of fat content in cow’s milk – an exercise in classification of parameters of a complex suspension</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15069">http://arxiv.org/abs/2307.15069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kwasi Nyandey, Daniel Jakubczyk</li>
<li>for: direct classification and recognition of milk fat content classes</li>
<li>methods: combined transmission speckle photography and machine learning</li>
<li>results: unambiguous recognition of milk fat content classes with high accuracy (100% and ~99%)<details>
<summary>Abstract</summary>
We have combined transmission speckle photography and machine learning for direct classification and recognition of milk fat content classes. Our aim was hinged on the fact that parameters of scattering particles (and the dispersion medium) can be linked to the intensity distribution (speckle) observed when coherent light is transmitted through a scattering medium. For milk, it is primarily the size distribution and concentration of fat globules, which constitutes the total fat content. Consequently, we trained convolutional neural network to recognise and classify laser speckle from different fat content classes (0.5, 1.5, 2.0 and 3.2%). We investigated four exposure-time protocols and obtained the highest performance for shorter exposure times, in which the intensity histograms are kept similar for all images and the most probable intensity in the speckle pattern is close to zero. Our neural network was able to recognize the milk fat content classes unambiguously and we obtained the highest test and independent classification accuracies of 100 and ~99% respectively. It indicates that the parameters of other complex realistic suspensions could be classified with similar methods.
</details>
<details>
<summary>摘要</summary>
我们将传输扫描光学和机器学习结合，以直接分类和识别牛奶脂肪含量类别。我们的目标是利用散射体粒子（以及散射媒体）的参数与扫描光通过散射媒体所观察到的INTENSITY分布（扫描光斑）之间存在关系。在牛奶中，主要是脂肪球体大小分布和总脂肪含量。因此，我们将 convolutional neural network 训练来识别和分类不同脂肪含量类别（0.5%, 1.5%, 2.0%和3.2%）。我们研究了四种曝光时间协议，并获得了最高性能，其中曝光时间较短，图像INTENSITY频谱均匀，最 probable intensity 在扫描光斑模式中接近零。我们的神经网络能够不 ambiguously 识别牛奶脂肪含量类别，并在测试和独立分类准确率达100%和~99%。这表明可以使用类似方法来分类其他复杂的真实涂敷。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/09/eess.IV_2023_07_09/" data-id="closbrowv013j0g889hkzedz9" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/82/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/81/">81</a><a class="page-number" href="/page/82/">82</a><span class="page-number current">83</span><a class="page-number" href="/page/84/">84</a><a class="page-number" href="/page/85/">85</a><span class="space">&hellip;</span><a class="page-number" href="/page/89/">89</a><a class="extend next" rel="next" href="/page/84/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">60</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">118</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">58</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
