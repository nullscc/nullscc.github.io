
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/59/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.CL_2023_08_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/16/cs.CL_2023_08_16/" class="article-date">
  <time datetime="2023-08-16T11:00:00.000Z" itemprop="datePublished">2023-08-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/16/cs.CL_2023_08_16/">cs.CL - 2023-08-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Mitigating-the-Exposure-Bias-in-Sentence-Level-Grapheme-to-Phoneme-G2P-Transduction"><a href="#Mitigating-the-Exposure-Bias-in-Sentence-Level-Grapheme-to-Phoneme-G2P-Transduction" class="headerlink" title="Mitigating the Exposure Bias in Sentence-Level Grapheme-to-Phoneme (G2P) Transduction"></a>Mitigating the Exposure Bias in Sentence-Level Grapheme-to-Phoneme (G2P) Transduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08442">http://arxiv.org/abs/2308.08442</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eunseop Yoon, Hee Suk Yoon, Dhananjaya Gowda, SooHwan Eom, Daehyeok Kim, John Harvill, Heting Gao, Mark Hasegawa-Johnson, Chanwoo Kim, Chang D. Yoo</li>
<li>for: 这篇论文旨在提高文本中字符串到音素的转化率，具体来说是在使用Text-to-Text Transfer Transformer（T5）和tokenizer-free byte-level模型（ByT5）进行 Grapheme-to-Phoneme（G2P）转化时。</li>
<li>methods: 这篇论文使用了ByT5模型，并提出了一种基于损失函数的采样方法来 Mitigate exposure bias，以提高文本中 sentence-level和paragraph-level G2P 的性能。</li>
<li>results: 研究发现，通过使用该采样方法，可以有效地提高 sentence-level和paragraph-level G2P 的性能，从而提高文本中的可用性和可读性。<details>
<summary>Abstract</summary>
Text-to-Text Transfer Transformer (T5) has recently been considered for the Grapheme-to-Phoneme (G2P) transduction. As a follow-up, a tokenizer-free byte-level model based on T5 referred to as ByT5, recently gave promising results on word-level G2P conversion by representing each input character with its corresponding UTF-8 encoding. Although it is generally understood that sentence-level or paragraph-level G2P can improve usability in real-world applications as it is better suited to perform on heteronyms and linking sounds between words, we find that using ByT5 for these scenarios is nontrivial. Since ByT5 operates on the character level, it requires longer decoding steps, which deteriorates the performance due to the exposure bias commonly observed in auto-regressive generation models. This paper shows that the performance of sentence-level and paragraph-level G2P can be improved by mitigating such exposure bias using our proposed loss-based sampling method.
</details>
<details>
<summary>摘要</summary>
文本转换transformer（T5）最近被考虑用于字母到音素（G2P）转化。作为续作，一个不需要分词的字节级模型基于T5，称为ByT5，最近在单词级G2P转化中达到了promising的结果，通过对输入字符的UTF-8编码进行表示。虽然一般认为 sentence-level或 paragraph-level G2P可以提高实际应用中的可用性，因为更适合处理同义词和 слова之间的声音连接，但是使用ByT5进行这些场景是非常困难。由于ByT5 operate在字符级别，需要更长的解码步骤，这会导致性能下降，这是因为自动生成模型中通常存在的曝露偏见。本文显示，可以通过我们提出的损失样本方法来改善 sentence-level和 paragraph-level G2P的性能。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Enhanced-Multi-Label-Few-Shot-Product-Attribute-Value-Extraction"><a href="#Knowledge-Enhanced-Multi-Label-Few-Shot-Product-Attribute-Value-Extraction" class="headerlink" title="Knowledge-Enhanced Multi-Label Few-Shot Product Attribute-Value Extraction"></a>Knowledge-Enhanced Multi-Label Few-Shot Product Attribute-Value Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08413">http://arxiv.org/abs/2308.08413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaying Gong, Wei-Te Chen, Hoda Eldardiry</li>
<li>for: 该研究旨在提高 attribute-value extraction (AVE) 模型的训练效果，以满足实际电商中每天新增的新产品和新的 attribute-value 对。</li>
<li>methods: 该研究使用 multi-label few-shot learning (FSL) 方法，并提出了一个 Knowledge-Enhanced Attentive Framework (KEAF)，基于 prototypical networks 和 hybrid attention 技术，以提高模型的拟合能力和精度。</li>
<li>results: 实验结果显示，KEAF 在两个 dataset 上的表现均较之前的 State-of-the-Art (SOTA) 模型更高，并且可以减少 Label Noise 和提高模型的稳定性。<details>
<summary>Abstract</summary>
Existing attribute-value extraction (AVE) models require large quantities of labeled data for training. However, new products with new attribute-value pairs enter the market every day in real-world e-Commerce. Thus, we formulate AVE in multi-label few-shot learning (FSL), aiming to extract unseen attribute value pairs based on a small number of training examples. We propose a Knowledge-Enhanced Attentive Framework (KEAF) based on prototypical networks, leveraging the generated label description and category information to learn more discriminative prototypes. Besides, KEAF integrates with hybrid attention to reduce noise and capture more informative semantics for each class by calculating the label-relevant and query-related weights. To achieve multi-label inference, KEAF further learns a dynamic threshold by integrating the semantic information from both the support set and the query set. Extensive experiments with ablation studies conducted on two datasets demonstrate that KEAF outperforms other SOTA models for information extraction in FSL. The code can be found at: https://github.com/gjiaying/KEAF
</details>
<details>
<summary>摘要</summary>
现有的属性值提取（AVE）模型需要大量的标注数据进行训练。然而，实际世界电商中每天都会出现新的产品，带有新的属性值对。因此，我们将 AVE 转化为多标签少数例学习（FSL），以提取未经见过的属性值对。我们提议一种基于原型网络的知识增强框架（KEAF），利用生成的标签描述和类别信息来学习更有刺激的原型。此外，KEAF 还 integrate 了杂合注意 Mechanism 来减少噪音和捕捉更有意义的语义信息。为实现多标签推断，KEAF 进一步学习一个动态阈值，通过将支持集和查询集的语义信息集成在一起。经验表明，KEAF 在两个数据集上实现了对信息提取的最佳性能，超过其他 SOTA 模型。代码可以在 GitHub 上找到：https://github.com/gjiaying/KEAF。
</details></li>
</ul>
<hr>
<h2 id="Advancing-continual-lifelong-learning-in-neural-information-retrieval-definition-dataset-framework-and-empirical-evaluation"><a href="#Advancing-continual-lifelong-learning-in-neural-information-retrieval-definition-dataset-framework-and-empirical-evaluation" class="headerlink" title="Advancing continual lifelong learning in neural information retrieval: definition, dataset, framework, and empirical evaluation"></a>Advancing continual lifelong learning in neural information retrieval: definition, dataset, framework, and empirical evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08378">http://arxiv.org/abs/2308.08378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingrui Hou, Georgina Cosma, Axel Finke</li>
<li>for: 本研究旨在提出一个具体的定义，以便探讨机器学习模型在新信息探索中的持续学习能力。</li>
<li>methods: 本研究使用了多种常见的学习策略，包括嵌入式探索模型和预训练模型，以测试它们在持续学习中的表现。</li>
<li>results: 研究结果显示，提出的框架可以成功防止 neural information retrieval 中的恐慌性遗传，并提高之前学习的任务表现。 embedding-based Retrieval 模型在持续学习中受到主题转移距离和新任务集大小的影响，而 pretraining-based 模型则不受此影响。适当的学习策略可以 Mitigate 这些影响。<details>
<summary>Abstract</summary>
Continual learning refers to the capability of a machine learning model to learn and adapt to new information, without compromising its performance on previously learned tasks. Although several studies have investigated continual learning methods for information retrieval tasks, a well-defined task formulation is still lacking, and it is unclear how typical learning strategies perform in this context. To address this challenge, a systematic task formulation of continual neural information retrieval is presented, along with a multiple-topic dataset that simulates continuous information retrieval. A comprehensive continual neural information retrieval framework consisting of typical retrieval models and continual learning strategies is then proposed. Empirical evaluations illustrate that the proposed framework can successfully prevent catastrophic forgetting in neural information retrieval and enhance performance on previously learned tasks. The results indicate that embedding-based retrieval models experience a decline in their continual learning performance as the topic shift distance and dataset volume of new tasks increase. In contrast, pretraining-based models do not show any such correlation. Adopting suitable learning strategies can mitigate the effects of topic shift and data augmentation.
</details>
<details>
<summary>摘要</summary>
（简化中文）持续学习指的是机器学习模型能够在新的信息上学习和适应，而不会对之前学习的任务的性能产生负面影响。虽然一些研究已经研究过持续学习方法，但是对于信息检索任务的明确定义仍然缺失，而且不清楚 Typical learning strategies 在这个上下文中的性能。为了解决这个挑战，本文提出了一个系统的持续神经信息检索任务定义，以及一个多主题的数据集，用于模拟连续的信息检索。然后，一个包含 Typical retrieval models 和持续学习策略的完整持续神经信息检索框架被提出。实验证明，提议的框架可以成功避免神经信息检索中的恶化学习现象，并提高之前学习任务的性能。结果表明，基于嵌入的检索模型，随着主题偏移距离和新任务数据量增加，其持续学习性能会下降。相比之下，基于预训练的模型不会出现这种趋势。采用适当的学习策略可以 Mitigate 主题偏移和数据扩展的影响。
</details></li>
</ul>
<hr>
<h2 id="SummHelper-Collaborative-Human-Computer-Summarization"><a href="#SummHelper-Collaborative-Human-Computer-Summarization" class="headerlink" title="SummHelper: Collaborative Human-Computer Summarization"></a>SummHelper: Collaborative Human-Computer Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08363">http://arxiv.org/abs/2308.08363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aviv Slobodkin, Niv Nachum, Shmuel Amar, Ori Shapira, Ido Dagan</li>
<li>for: 本研究旨在提供一种人机合作的文本概要生成方法，以增强用户对概要生成过程的控制和参与度。</li>
<li>methods: 本研究使用了两个阶段的方法：内容选择阶段和内容整合阶段。内容选择阶段，系统提供了可能的内容选择，用户可以接受、修改或添加更多的选择。内容整合阶段，系统生成了一份具有一致性的概要，用户可以通过视觉映射来修改和完善概要。</li>
<li>results: 小规模用户研究表明，本应用程序具有效果，用户特别是appreciate系统提供的自动指导和个性化输入的平衡。<details>
<summary>Abstract</summary>
Current approaches for text summarization are predominantly automatic, with rather limited space for human intervention and control over the process. In this paper, we introduce SummHelper, a 2-phase summarization assistant designed to foster human-machine collaboration. The initial phase involves content selection, where the system recommends potential content, allowing users to accept, modify, or introduce additional selections. The subsequent phase, content consolidation, involves SummHelper generating a coherent summary from these selections, which users can then refine using visual mappings between the summary and the source text. Small-scale user studies reveal the effectiveness of our application, with participants being especially appreciative of the balance between automated guidance and opportunities for personal input.
</details>
<details>
<summary>摘要</summary>
当前的文本摘要方法都是自动的，具有有限的人类参与和控制能力。在这篇论文中，我们介绍了SummHelper，一种两个阶段的摘要助手，旨在激发人机合作。第一阶段是内容选择，系统会提供可能的内容选择，用户可以接受、修改或添加自己的选择。第二阶段是内容归约，系统会基于这些选择生成一份有 coherence 的摘要，用户可以通过视觉映射来修改摘要和原始文本之间的关系。小规模用户研究表明了我们的应用的有效性，用户尤其喜欢SummHelper的自动导航和个性化输入的平衡。
</details></li>
</ul>
<hr>
<h2 id="Detoxify-Language-Model-Step-by-Step"><a href="#Detoxify-Language-Model-Step-by-Step" class="headerlink" title="Detoxify Language Model Step-by-Step"></a>Detoxify Language Model Step-by-Step</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08295">http://arxiv.org/abs/2308.08295</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/codinnlg/detox-cot">https://github.com/codinnlg/detox-cot</a></li>
<li>paper_authors: Zecheng Tang, Keyan Zhou, Pinzheng Wang, Yuyang Ding, Juntao Li, Minzhang</li>
<li>for:  detoxifying language models (LLMs) to avoid generating harmful content while maintaining generation capability.</li>
<li>methods: decomposing the detoxification process into sub-steps, calibrating the strong reasoning ability of LLMs using a Detox-Chain, and training with the non-toxic prompt.</li>
<li>results: significant detoxification and generation improvement for six LLMs scaling from 1B to 33B, as demonstrated by automatic and human evaluation on two benchmarks.Here’s the simplified Chinese text:</li>
<li>for: 这研究旨在对语言模型（LLMs）进行净化，以避免它们产生危险内容的同时保留生成能力。</li>
<li>methods: 将净化过程分解为不同的步骤，使用Detox-Chain来联结这些步骤，并在非危险提示上进行续生成。</li>
<li>results: 透过对六个LLMs（1B至33B）进行训练，在两个标准 benchmark 上展示了重要的净化和生成改善。<details>
<summary>Abstract</summary>
Detoxification for LLMs is challenging since it requires models to avoid generating harmful content while maintaining the generation capability. To ensure the safety of generations, previous detoxification methods detoxify the models by changing the data distributions or constraining the generations from different aspects in a single-step manner. However, these approaches will dramatically affect the generation quality of LLMs, e.g., discourse coherence and semantic consistency, since language models tend to generate along the toxic prompt while detoxification methods work in the opposite direction. To handle such a conflict, we decompose the detoxification process into different sub-steps, where the detoxification is concentrated in the input stage and the subsequent continual generation is based on the non-toxic prompt. Besides, we also calibrate the strong reasoning ability of LLMs by designing a Detox-Chain to connect the above sub-steps in an orderly manner, which allows LLMs to detoxify the text step-by-step. Automatic and human evaluation on two benchmarks reveals that by training with Detox-Chain, six LLMs scaling from 1B to 33B can obtain significant detoxification and generation improvement. Our code and data are available at https://github.com/CODINNLG/Detox-CoT. Warning: examples in the paper may contain uncensored offensive content.
</details>
<details>
<summary>摘要</summary>
LLM 的净化是一个挑战，因为它们需要避免生成有害内容，同时保持生成能力。为确保生成的安全，以前的净化方法通常是改变数据分布或限制生成从不同方面，这些方法会很快地影响 LLM 的生成质量，例如文本连贯性和Semantic 一致性。这是因为语言模型在生成过程中很可能会遵循有害提示，而净化方法则在对向的方向上工作。为解决这种冲突，我们将净化过程分解成不同的子步骤，其中净化在输入阶段进行，而后续的不断生成则基于非有害提示。此外，我们还使用 Detox-Chain 来连接这些子步骤，使 LLM 可以逐步净化文本。在自动和人工评估的基础上，我们发现通过训练 Detox-Chain，6个 LLM 缩放到 1B 和 33B 的模型都可以获得显著的净化和生成改进。我们的代码和数据可以在 GitHub 上找到，链接在 https://github.com/CODINNLG/Detox-CoT 上。请注意，文章中的示例可能包含不цензурирован的有害内容。
</details></li>
</ul>
<hr>
<h2 id="Pre-training-with-Large-Language-Model-based-Document-Expansion-for-Dense-Passage-Retrieval"><a href="#Pre-training-with-Large-Language-Model-based-Document-Expansion-for-Dense-Passage-Retrieval" class="headerlink" title="Pre-training with Large Language Model-based Document Expansion for Dense Passage Retrieval"></a>Pre-training with Large Language Model-based Document Expansion for Dense Passage Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08285">http://arxiv.org/abs/2308.08285</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangyuan Ma, Xing Wu, Peng Wang, Zijia Lin, Songlin Hu</li>
<li>for: 这个论文主要是为了研究使用大语言模型（LLM）基于文档扩展来提高紧凑段 Retrieval的潜在性。</li>
<li>methods: 这个论文使用了LLM的能力进行文档扩展，即生成查询，并使用适应扩展知识的预训练策略来传输知识到检索器。这些策略包括对比学习和瓶颈查询生成。此外，我们还采用了课程学习策略来减少LLM的推断依赖。</li>
<li>results: 实验结果表明，通过预训练 LLM-基于文档扩展，可以significantly提高大规模网络检索任务的检索性能。我们的工作具有强的零基数和外部预测能力，使其更适用于没有人工标注数据的检索。<details>
<summary>Abstract</summary>
In this paper, we systematically study the potential of pre-training with Large Language Model(LLM)-based document expansion for dense passage retrieval. Concretely, we leverage the capabilities of LLMs for document expansion, i.e. query generation, and effectively transfer expanded knowledge to retrievers using pre-training strategies tailored for passage retrieval. These strategies include contrastive learning and bottlenecked query generation. Furthermore, we incorporate a curriculum learning strategy to reduce the reliance on LLM inferences. Experimental results demonstrate that pre-training with LLM-based document expansion significantly boosts the retrieval performance on large-scale web-search tasks. Our work shows strong zero-shot and out-of-domain retrieval abilities, making it more widely applicable for retrieval when initializing with no human-labeled data.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们系统地研究了使用大语言模型(LLM)-基于文档扩展的预训练对紧凑段 retrieval的潜力。具体来说，我们利用 LLM 的扩展能力，即查询生成，并将扩展知识有效地传递给 Retriever 使用预训练策略。这些策略包括对比学习和瓶颈查询生成。此外，我们采用了课程学习策略来减少 LLM 的推断依赖度。实验结果表明，预训练与 LLM-基于文档扩展可以大幅提高大规模网络搜索任务中的 Retrieval 性能。我们的工作具有强大的零shot和 OUT-OF-DOMAIN 检索能力，使其更适用于没有人类标注数据的检索。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Neural-Network-Generalization-for-Grammar-Induction"><a href="#Benchmarking-Neural-Network-Generalization-for-Grammar-Induction" class="headerlink" title="Benchmarking Neural Network Generalization for Grammar Induction"></a>Benchmarking Neural Network Generalization for Grammar Induction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08253">http://arxiv.org/abs/2308.08253</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/taucompling/bliss">https://github.com/taucompling/bliss</a></li>
<li>paper_authors: Nur Lan, Emmanuel Chemla, Roni Katzir</li>
<li>for: 这 paper 是为了测试神经网络模型的泛化能力而写的。</li>
<li>methods: 这 paper 使用了一种基于正式语言的测试方法来评估神经网络模型的泛化能力。</li>
<li>results: 研究发现，使用 Minimum Description Length 目标函数（MDL）来训练神经网络模型可以提高其泛化能力，并使用更少的数据。<details>
<summary>Abstract</summary>
How well do neural networks generalize? Even for grammar induction tasks, where the target generalization is fully known, previous works have left the question open, testing very limited ranges beyond the training set and using different success criteria. We provide a measure of neural network generalization based on fully specified formal languages. Given a model and a formal grammar, the method assigns a generalization score representing how well a model generalizes to unseen samples in inverse relation to the amount of data it was trained on. The benchmark includes languages such as $a^nb^n$, $a^nb^nc^n$, $a^nb^mc^{n+m}$, and Dyck-1 and 2. We evaluate selected architectures using the benchmark and find that networks trained with a Minimum Description Length objective (MDL) generalize better and using less data than networks trained using standard loss functions. The benchmark is available at https://github.com/taucompling/bliss.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>previous works 未能很好地回答了 neural network 的泛化能力问题，即使是 grammar induction 任务，target generalization 完全知道。我们提供了一种基于完全指定的形式语言的 neural network 泛化评价方法。给定一个模型和一个形式语法，该方法将分配一个泛化分数，表示模型在未经见过样本数据上的泛化能力，与训练数据量之间存在 inverse 关系。 benchmark 包括 $a^nb^n$, $a^nb^nc^n$, $a^nb^mc^{n+m}$, Dyck-1 和 Dyck-2 等语言。我们使用这些 benchmark 评估选择的 architecture，并发现使用 MDL 目标函数（Minimum Description Length）训练的网络在使用更少的数据量下可以更好地泛化。 benchmark 可以在 GitHub 上找到：https://github.com/taucompling/bliss。
</details></li>
</ul>
<hr>
<h2 id="MemoChat-Tuning-LLMs-to-Use-Memos-for-Consistent-Long-Range-Open-Domain-Conversation"><a href="#MemoChat-Tuning-LLMs-to-Use-Memos-for-Consistent-Long-Range-Open-Domain-Conversation" class="headerlink" title="MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation"></a>MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08239">http://arxiv.org/abs/2308.08239</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lujunru/memochat">https://github.com/lujunru/memochat</a></li>
<li>paper_authors: Junru Lu, Siyu An, Mingbao Lin, Gabriele Pergola, Yulan He, Di Yin, Xing Sun, Yunsheng Wu</li>
<li>for: 这个论文旨在帮助大型自然语言模型（LLM）在开放领域对话中维护一致性，通过自我组合的备忘录来实现。</li>
<li>methods: 这篇论文提出了一个名为MemoChat的管道，用于修正 instrucions，使得 LLM 可以有效地使用自我组合的备忘录来维护长距离开放领域对话的一致性。</li>
<li>results: 实验结果表明，MemoChat 可以在三个不同的测试场景中超越强基eline，并且可以在大规模的 open-source 和 API 可达的 chatbot 上实现高效的长距离开放领域对话。<details>
<summary>Abstract</summary>
We propose MemoChat, a pipeline for refining instructions that enables large language models (LLMs) to effectively employ self-composed memos for maintaining consistent long-range open-domain conversations. We demonstrate a long-range open-domain conversation through iterative "memorization-retrieval-response" cycles. This requires us to carefully design tailored tuning instructions for each distinct stage. The instructions are reconstructed from a collection of public datasets to teach the LLMs to memorize and retrieve past dialogues with structured memos, leading to enhanced consistency when participating in future conversations. We invite experts to manually annotate a test set designed to evaluate the consistency of long-range conversations questions. Experiments on three testing scenarios involving both open-source and API-accessible chatbots at scale verify the efficacy of MemoChat, which outperforms strong baselines. Our codes, data and models are available here: https://github.com/LuJunru/MemoChat.
</details>
<details>
<summary>摘要</summary>
我们提出了 MemoChat，一个用于精细调整 instrucion 的管道，使大语言模型 (LLM) 可以有效地利用自己编写的笔记来维护长范围开放领域对话的一致性。我们通过迭代“记忆-检索-响应”  cycles 来实现长范围开放领域对话。这需要我们仔细设计适应每个特定阶段的调整 instruction。这些 instruction 从公共数据集中搜集并教育 LLM 记忆和检索过去对话，从而在未来对话中提高一致性。我们邀请专家手动标注一组用于评估长范围对话一致性的测试集。我们在三个测试场景中使用了开源和 API 访问ible 的 chatbot，并在大规模上进行了实验，以证明 MemoChat 的有效性，超越了强大的基线。我们的代码、数据和模型可以在以下链接中找到：https://github.com/LuJunru/MemoChat。
</details></li>
</ul>
<hr>
<h2 id="MoCoSA-Momentum-Contrast-for-Knowledge-Graph-Completion-with-Structure-Augmented-Pre-trained-Language-Models"><a href="#MoCoSA-Momentum-Contrast-for-Knowledge-Graph-Completion-with-Structure-Augmented-Pre-trained-Language-Models" class="headerlink" title="MoCoSA: Momentum Contrast for Knowledge Graph Completion with Structure-Augmented Pre-trained Language Models"></a>MoCoSA: Momentum Contrast for Knowledge Graph Completion with Structure-Augmented Pre-trained Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08204">http://arxiv.org/abs/2308.08204</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiabang He, Liu Jia, Lei Wang, Xiyao Li, Xing Xu</li>
<li>for: 本研究旨在提高知识图完成任务的性能，使得模型能够更好地进行知识图中的推理和推出逻辑结论。</li>
<li>methods: 本研究提出了一种基于structure-augmented pre-trained language models (MoCoSA)的方法，通过适应结构编码器让PLM能够更好地感知结构信息，以提高学习效率。同时，我们还提出了很重要的势能负采样和内部关系负采样来提高学习效率。</li>
<li>results: 实验结果显示，我们的方法可以在Wn18RR和OpenBG500上实现state-of-the-art的性能，MRR分数提高2.5%和21%。<details>
<summary>Abstract</summary>
Knowledge Graph Completion (KGC) aims to conduct reasoning on the facts within knowledge graphs and automatically infer missing links. Existing methods can mainly be categorized into structure-based or description-based. On the one hand, structure-based methods effectively represent relational facts in knowledge graphs using entity embeddings. However, they struggle with semantically rich real-world entities due to limited structural information and fail to generalize to unseen entities. On the other hand, description-based methods leverage pre-trained language models (PLMs) to understand textual information. They exhibit strong robustness towards unseen entities. However, they have difficulty with larger negative sampling and often lag behind structure-based methods. To address these issues, in this paper, we propose Momentum Contrast for knowledge graph completion with Structure-Augmented pre-trained language models (MoCoSA), which allows the PLM to perceive the structural information by the adaptable structure encoder. To improve learning efficiency, we proposed momentum hard negative and intra-relation negative sampling. Experimental results demonstrate that our approach achieves state-of-the-art performance in terms of mean reciprocal rank (MRR), with improvements of 2.5% on WN18RR and 21% on OpenBG500.
</details>
<details>
<summary>摘要</summary>
知识图完成（KGC）的目标是通过知识图中的事实进行推理，自动填充缺失的链接。现有方法主要可以分为结构基于的和描述基于的两类。一方面，结构基于的方法可以有效地表示知识图中的关系事实使用实体嵌入。然而，它们在semantic rich的实体上遇到限制，容易受到未知实体的影响，并且难以泛化到未看过的实体。另一方面，描述基于的方法可以利用预训练语言模型（PLM）来理解文本信息。它们在面对未看过的实体时具有强的鲁棒性，但是它们在大量负样本下表现不佳，经常落后于结构基于的方法。为了解决这些问题，在这篇论文中，我们提出了带有结构扩充适应器的摘要凝聚（MoCoSA），使PLM能够通过适应器感知结构信息。此外，我们还提出了振荡负样本和内部关系负样本的提高学习效率。实验结果表明，我们的方法在MRR指标上达到了领先水平，与WN18RR和OpenBG500上的提高分别为2.5%和21%。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Explainable-AI-to-Analyze-Researchers’-Aspect-Based-Sentiment-about-ChatGPT"><a href="#Leveraging-Explainable-AI-to-Analyze-Researchers’-Aspect-Based-Sentiment-about-ChatGPT" class="headerlink" title="Leveraging Explainable AI to Analyze Researchers’ Aspect-Based Sentiment about ChatGPT"></a>Leveraging Explainable AI to Analyze Researchers’ Aspect-Based Sentiment about ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11001">http://arxiv.org/abs/2308.11001</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shilpa Lakhanpal, Ajay Gupta, Rajeev Agrawal</li>
<li>for: 本研究旨在分析研究者对ChatGPT的看法，以帮助更好地理解其 Correctness 和使用伦理。</li>
<li>methods: 本研究使用 Explainable AI 方法来分析研究数据，以拓展 Aspect-Based Sentiment Analysis 的状态艺术。</li>
<li>results: 本研究提供了valuable insights into extending the state of the art of Aspect-Based Sentiment Analysis on newer datasets，where such analysis is not hampered by the length of the text data。<details>
<summary>Abstract</summary>
The groundbreaking invention of ChatGPT has triggered enormous discussion among users across all fields and domains. Among celebration around its various advantages, questions have been raised with regards to its correctness and ethics of its use. Efforts are already underway towards capturing user sentiments around it. But it begs the question as to how the research community is analyzing ChatGPT with regards to various aspects of its usage. It is this sentiment of the researchers that we analyze in our work. Since Aspect-Based Sentiment Analysis has usually only been applied on a few datasets, it gives limited success and that too only on short text data. We propose a methodology that uses Explainable AI to facilitate such analysis on research data. Our technique presents valuable insights into extending the state of the art of Aspect-Based Sentiment Analysis on newer datasets, where such analysis is not hampered by the length of the text data.
</details>
<details>
<summary>摘要</summary>
<<SYS>> chatgpt 的创新性发明已经引发了各界用户的广泛讨论。虽然有很多人对其多方面的优点表示欢迎，但也有人提出了关于其正确性和使用道德性的问题。为了捕捉用户情绪，已经有努力在进行。但问题来临，研究者们如何对 chatgpt 进行不同方面的分析呢？我们的研究团队在这个问题上进行了分析。由于 aspect-based sentiment analysis 通常仅在一些数据集上得到有限的成功，尤其是在短文本数据上。我们提出了一种使用可解释AI来实现这种分析的方法。我们的技术可以为 newer datasets 提供有价值的发现，使 aspect-based sentiment analysis 不受文本数据的长度所限制。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format instead.
</details></li>
</ul>
<hr>
<h2 id="ChinaTelecom-System-Description-to-VoxCeleb-Speaker-Recognition-Challenge-2023"><a href="#ChinaTelecom-System-Description-to-VoxCeleb-Speaker-Recognition-Challenge-2023" class="headerlink" title="ChinaTelecom System Description to VoxCeleb Speaker Recognition Challenge 2023"></a>ChinaTelecom System Description to VoxCeleb Speaker Recognition Challenge 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08181">http://arxiv.org/abs/2308.08181</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengjie Du, Xiang Fang, Jie Li</li>
<li>for: 这份报告描述了参加VoxCeleb2023 Speaker Recognition Challenge（VoxSRC 2023）的中国电信系统（Track 1）。</li>
<li>methods: 该系统包括多种ResNet变种，只使用VoxCeleb2进行训练，并将它们进行了融合以提高性能。在融合系统中，还应用了分数调整。</li>
<li>results: 最终提交得分为0.1066和EER为1.980%。<details>
<summary>Abstract</summary>
This technical report describes ChinaTelecom system for Track 1 (closed) of the VoxCeleb2023 Speaker Recognition Challenge (VoxSRC 2023). Our system consists of several ResNet variants trained only on VoxCeleb2, which were fused for better performance later. Score calibration was also applied for each variant and the fused system. The final submission achieved minDCF of 0.1066 and EER of 1.980%.
</details>
<details>
<summary>摘要</summary>
这份技术报告描述了我们在VoxCeleb2023演说识别挑战（VoxSRC 2023）的Track 1（关闭）系统。我们的系统包括了多种ResNet变种，只在VoxCeleb2上进行训练。这些变种之后被 fusion 以提高性能。在每个变种和混合系统上进行了分数均衡。最终提交得分为0.1066%和1.980%。
</details></li>
</ul>
<hr>
<h2 id="RSpell-Retrieval-augmented-Framework-for-Domain-Adaptive-Chinese-Spelling-Check"><a href="#RSpell-Retrieval-augmented-Framework-for-Domain-Adaptive-Chinese-Spelling-Check" class="headerlink" title="RSpell: Retrieval-augmented Framework for Domain Adaptive Chinese Spelling Check"></a>RSpell: Retrieval-augmented Framework for Domain Adaptive Chinese Spelling Check</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08176">http://arxiv.org/abs/2308.08176</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/47777777/rspell">https://github.com/47777777/rspell</a></li>
<li>paper_authors: Siqi Song, Qi Lv, Lei Geng, Ziqiang Cao, Guohong Fu</li>
<li>for: 提高中文拼写检查（CSC）模型在不同领域中的拼写错误检测和修复能力。</li>
<li>methods: 提出了一种基于检索的CSC模型框架，称为RSpell，通过使用拼音模糊匹配搜索相关领域词汇，并将其与输入结合使用CSC模型进行检测和修复。另外，还引入了一种可控的外部知识 dynamically 调整机制，以便在不同领域中进行最佳化。</li>
<li>results: 通过在法律、医学和官方文书写作三个领域的CSC数据集上进行实验，显示RSpell在零shot和 fine-tuning  scenarios 中均达到了当前最佳性能，证明了基于检索的CSC模型框架的效果。<details>
<summary>Abstract</summary>
Chinese Spelling Check (CSC) refers to the detection and correction of spelling errors in Chinese texts. In practical application scenarios, it is important to make CSC models have the ability to correct errors across different domains. In this paper, we propose a retrieval-augmented spelling check framework called RSpell, which searches corresponding domain terms and incorporates them into CSC models. Specifically, we employ pinyin fuzzy matching to search for terms, which are combined with the input and fed into the CSC model. Then, we introduce an adaptive process control mechanism to dynamically adjust the impact of external knowledge on the model. Additionally, we develop an iterative strategy for the RSpell framework to enhance reasoning capabilities. We conducted experiments on CSC datasets in three domains: law, medicine, and official document writing. The results demonstrate that RSpell achieves state-of-the-art performance in both zero-shot and fine-tuning scenarios, demonstrating the effectiveness of the retrieval-augmented CSC framework. Our code is available at https://github.com/47777777/Rspell.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="AffectEcho-Speaker-Independent-and-Language-Agnostic-Emotion-and-Affect-Transfer-for-Speech-Synthesis"><a href="#AffectEcho-Speaker-Independent-and-Language-Agnostic-Emotion-and-Affect-Transfer-for-Speech-Synthesis" class="headerlink" title="AffectEcho: Speaker Independent and Language-Agnostic Emotion and Affect Transfer for Speech Synthesis"></a>AffectEcho: Speaker Independent and Language-Agnostic Emotion and Affect Transfer for Speech Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08577">http://arxiv.org/abs/2308.08577</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hrishikesh Viswanath, Aneesh Bhattacharya, Pascal Jutras-Dubé, Prerit Gupta, Mridu Prashanth, Yashvardhan Khaitan, Aniket Bera</li>
<li>for: 这篇论文旨在提出一种基于Vector Quantized codebook的情感翻译模型，用于控制生成的语音中的情感强度和多样性。</li>
<li>methods: 该模型使用一个Vector Quantized codebook来模型情感，从而消除了使用强制 embedding 或者显式强制 embedding 的需要。实验结果表明，该方法可以控制生成的语音中的情感强度和多样性，同时保持语音特点和情感 Cadence 的唯一性。</li>
<li>results: 该研究通过一系列实验表明，使用Vector Quantized codebook来模型情感可以达到语言独立的情感模型化能力，并且在评价 metrics 上取得了顶尖的结果。<details>
<summary>Abstract</summary>
Affect is an emotional characteristic encompassing valence, arousal, and intensity, and is a crucial attribute for enabling authentic conversations. While existing text-to-speech (TTS) and speech-to-speech systems rely on strength embedding vectors and global style tokens to capture emotions, these models represent emotions as a component of style or represent them in discrete categories. We propose AffectEcho, an emotion translation model, that uses a Vector Quantized codebook to model emotions within a quantized space featuring five levels of affect intensity to capture complex nuances and subtle differences in the same emotion. The quantized emotional embeddings are implicitly derived from spoken speech samples, eliminating the need for one-hot vectors or explicit strength embeddings. Experimental results demonstrate the effectiveness of our approach in controlling the emotions of generated speech while preserving identity, style, and emotional cadence unique to each speaker. We showcase the language-independent emotion modeling capability of the quantized emotional embeddings learned from a bilingual (English and Chinese) speech corpus with an emotion transfer task from a reference speech to a target speech. We achieve state-of-art results on both qualitative and quantitative metrics.
</details>
<details>
<summary>摘要</summary>
情感是一种情感特征，包括价值、刺激和强度，它是对话的真实化的关键属性。现有的文本到语音（TTS）和语音到语音系统（STTS）都 rely on 强度 embedding vector 和全局风格标识符来捕捉情感，但这些模型表现情感为样式的一部分或用 discrete category 表示。我们提议 AffectEcho，一种情感翻译模型，使用量化码字表示情感在量化空间中的五级情感强度，以捕捉复杂的细节和同一种情感中的微妙差异。量化情感嵌入在不需要一hot вектор或显式强度 embedding 的基础上，从而消除了一些不必要的缺失。实验结果表明我们的方法可以控制生成的语音中的情感，保留每个说话人的个性、风格和情感节奏。我们还展示了基于英语和中文 speech 集合的语言无关情感模型的能力，通过将 refer 语音中的情感传递到 target 语音中。我们在质量和量度 metrics 上获得了领先的结果。
</details></li>
</ul>
<hr>
<h2 id="Sarcasm-Detection-in-a-Disaster-Context"><a href="#Sarcasm-Detection-in-a-Disaster-Context" class="headerlink" title="Sarcasm Detection in a Disaster Context"></a>Sarcasm Detection in a Disaster Context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08156">http://arxiv.org/abs/2308.08156</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiberiu Sosea, Junyi Jessy Li, Cornelia Caragea</li>
<li>for: 这个论文是为了研究在自然灾害时人们使用社交媒体平台上表达蔑视的语言模式。</li>
<li>methods: 该论文使用了预训练语言模型进行蔑视检测，并提供了一个包含15,000条推文的数据集（HurricaneSARC），以及一系列的实验结果来evaluate这些模型的性能。</li>
<li>results: 研究人员的最佳模型在HurricaneSARC数据集上的性能为0.70 F1，而使用中间任务转移学习可以提高该数据集上的性能。<details>
<summary>Abstract</summary>
During natural disasters, people often use social media platforms such as Twitter to ask for help, to provide information about the disaster situation, or to express contempt about the unfolding event or public policies and guidelines. This contempt is in some cases expressed as sarcasm or irony. Understanding this form of speech in a disaster-centric context is essential to improving natural language understanding of disaster-related tweets. In this paper, we introduce HurricaneSARC, a dataset of 15,000 tweets annotated for intended sarcasm, and provide a comprehensive investigation of sarcasm detection using pre-trained language models. Our best model is able to obtain as much as 0.70 F1 on our dataset. We also demonstrate that the performance on HurricaneSARC can be improved by leveraging intermediate task transfer learning. We release our data and code at https://github.com/tsosea2/HurricaneSarc.
</details>
<details>
<summary>摘要</summary>
在自然灾害事件中，人们经常通过社交媒体平台如推特请求帮助、提供灾害情况信息或表达对事件或公共政策的负面态度。这种负面态度在一些情况下会表现为讽刺或反意。在这篇论文中，我们介绍了飓风SARC数据集，该数据集包含15,000条推特帖子，每个帖子都被标注为意图 sarcastic。我们进行了全面的讽刺检测研究，使用预训练的语言模型。我们的最佳模型在我们的数据集上可以获得0.70的F1分。此外，我们还证明了可以通过中间任务传承学习提高HurricaneSARC的性能。我们在 GitHub上发布了数据集和代码，请参考https://github.com/tsosea2/HurricaneSarc。
</details></li>
</ul>
<hr>
<h2 id="Fast-Training-of-NMT-Model-with-Data-Sorting"><a href="#Fast-Training-of-NMT-Model-with-Data-Sorting" class="headerlink" title="Fast Training of NMT Model with Data Sorting"></a>Fast Training of NMT Model with Data Sorting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08153">http://arxiv.org/abs/2308.08153</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniela N. Rim, Kimera Richard, Heeyoul Choi</li>
<li>for: 提高Transformer模型的计算效率和准确率，解决针对空token的计算浪费。</li>
<li>methods:  sorting translation sentence pairs based on length before batching, partially sorting data to maintain i.i.d data assumption.</li>
<li>results: 在英-韩语和英-刚拉语机器翻译任务上实现了计算时间减少的目标，而不失去性能水平。<details>
<summary>Abstract</summary>
The Transformer model has revolutionized Natural Language Processing tasks such as Neural Machine Translation, and many efforts have been made to study the Transformer architecture, which increased its efficiency and accuracy. One potential area for improvement is to address the computation of empty tokens that the Transformer computes only to discard them later, leading to an unnecessary computational burden. To tackle this, we propose an algorithm that sorts translation sentence pairs based on their length before batching, minimizing the waste of computing power. Since the amount of sorting could violate the independent and identically distributed (i.i.d) data assumption, we sort the data partially. In experiments, we apply the proposed method to English-Korean and English-Luganda language pairs for machine translation and show that there are gains in computational time while maintaining the performance. Our method is independent of architectures, so that it can be easily integrated into any training process with flexible data lengths.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language Simplified Chinese;The Transformer model has revolutionized Natural Language Processing tasks such as Neural Machine Translation, and many efforts have been made to study the Transformer architecture, which increased its efficiency and accuracy. One potential area for improvement is to address the computation of empty tokens that the Transformer computes only to discard them later, leading to an unnecessary computational burden. To tackle this, we propose an algorithm that sorts translation sentence pairs based on their length before batching, minimizing the waste of computing power. Since the amount of sorting could violate the independent and identically distributed (i.i.d) data assumption, we sort the data partially. In experiments, we apply the proposed method to English-Korean and English-Luganda language pairs for machine translation and show that there are gains in computational time while maintaining the performance. Our method is independent of architectures, so that it can be easily integrated into any training process with flexible data lengths.Translate_done
</details></li>
</ul>
<hr>
<h2 id="MDDial-A-Multi-turn-Differential-Diagnosis-Dialogue-Dataset-with-Reliability-Evaluation"><a href="#MDDial-A-Multi-turn-Differential-Diagnosis-Dialogue-Dataset-with-Reliability-Evaluation" class="headerlink" title="MDDial: A Multi-turn Differential Diagnosis Dialogue Dataset with Reliability Evaluation"></a>MDDial: A Multi-turn Differential Diagnosis Dialogue Dataset with Reliability Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08147">http://arxiv.org/abs/2308.08147</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/srijamacherla24/mddial">https://github.com/srijamacherla24/mddial</a></li>
<li>paper_authors: Srija Macherla, Man Luo, Mihir Parmar, Chitta Baral</li>
<li>for: 这个论文的目的是提供一个英语自动分级诊断（ADD）对话系统的建立和评估所需的对话训练数据集。</li>
<li>methods: 该论文使用了一个新的对话数据集 called MDDial，该数据集包含了英语的分级诊断对话。此外，论文还引入了一个统一的分级诊断分数，该分数考虑了症状和诊断之间的关系，并且反映了系统的可靠性。</li>
<li>results: 实验表明，使用现有的语言模型在MDDial上进行训练后，其表现不佳，因为它们无法关联相关的症状和疾病。这表明，为了建立高效的ADD对话系统，还需要进一步的研究和改进。<details>
<summary>Abstract</summary>
Dialogue systems for Automatic Differential Diagnosis (ADD) have a wide range of real-life applications. These dialogue systems are promising for providing easy access and reducing medical costs. Building end-to-end ADD dialogue systems requires dialogue training datasets. However, to the best of our knowledge, there is no publicly available ADD dialogue dataset in English (although non-English datasets exist). Driven by this, we introduce MDDial, the first differential diagnosis dialogue dataset in English which can aid to build and evaluate end-to-end ADD dialogue systems. Additionally, earlier studies present the accuracy of diagnosis and symptoms either individually or as a combined weighted score. This method overlooks the connection between the symptoms and the diagnosis. We introduce a unified score for the ADD system that takes into account the interplay between symptoms and diagnosis. This score also indicates the system's reliability. To the end, we train two moderate-size of language models on MDDial. Our experiments suggest that while these language models can perform well on many natural language understanding tasks, including dialogue tasks in the general domain, they struggle to relate relevant symptoms and disease and thus have poor performance on MDDial. MDDial will be released publicly to aid the study of ADD dialogue research.
</details>
<details>
<summary>摘要</summary>
对话系统 для自动差异诊断（ADD）有很广泛的实际应用。这些对话系统有承诺提供容易访问和降低医疗成本。建立终到终ADD对话系统需要对话训练数据集。然而，到我们所知，英语的ADD对话数据集没有公共可用。我们驱动了这，我们引入了MDDial，英语中的第一个差异诊断对话数据集，可以帮助建立和评估终到终ADD对话系统。此外，先前的研究表明诊断和症状的准确率，或者是将其合并为权重加权分数。这种方法忽视了症状和诊断之间的连接。我们引入了一个统一的ADD系统分数，考虑了症状和诊断之间的互动。这个分数还指示系统的可靠性。为了实现这一点，我们训练了两个中型语言模型在MDDial上。我们的实验表明，虽然这两个语言模型可以在许多自然语言理解任务上表现良好，包括对话任务在通用领域，但它们在MDDial上很难关联相关的症状和疾病，因此它们在MDDial上表现不佳。MDDial将被公开发布，以便ADD对话研究的支持。
</details></li>
</ul>
<hr>
<h2 id="Radio2Text-Streaming-Speech-Recognition-Using-mmWave-Radio-Signals"><a href="#Radio2Text-Streaming-Speech-Recognition-Using-mmWave-Radio-Signals" class="headerlink" title="Radio2Text: Streaming Speech Recognition Using mmWave Radio Signals"></a>Radio2Text: Streaming Speech Recognition Using mmWave Radio Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08125">http://arxiv.org/abs/2308.08125</a></li>
<li>repo_url: None</li>
<li>paper_authors: Running Zhao, Jiangtao Yu, Hang Zhao, Edith C. H. Ngai</li>
<li>for: 这篇论文旨在提出一种基于 millimeter wave (mmWave) 的自动语音识别系统，以提高语音相关应用的可能性，如会议语音笔记和间谍listen。</li>
<li>methods: 该系统基于一种适应流处理的Transformer，可以有效地学习语音相关特征表示，为流式自动语音识别带来更多的可能性。此外， authors还提出了一种帮助初始化方法，可以在流处理网络无法访问整个未来输入的情况下，传递feature知识相关的全局上下文。</li>
<li>results: 实验结果显示，Radio2Text可以实现一个字符错误率为5.7%和一个词错率为9.4%，用于识别一个 vocabulary 包含超过13,000个词的语音。<details>
<summary>Abstract</summary>
Millimeter wave (mmWave) based speech recognition provides more possibility for audio-related applications, such as conference speech transcription and eavesdropping. However, considering the practicality in real scenarios, latency and recognizable vocabulary size are two critical factors that cannot be overlooked. In this paper, we propose Radio2Text, the first mmWave-based system for streaming automatic speech recognition (ASR) with a vocabulary size exceeding 13,000 words. Radio2Text is based on a tailored streaming Transformer that is capable of effectively learning representations of speech-related features, paving the way for streaming ASR with a large vocabulary. To alleviate the deficiency of streaming networks unable to access entire future inputs, we propose the Guidance Initialization that facilitates the transfer of feature knowledge related to the global context from the non-streaming Transformer to the tailored streaming Transformer through weight inheritance. Further, we propose a cross-modal structure based on knowledge distillation (KD), named cross-modal KD, to mitigate the negative effect of low quality mmWave signals on recognition performance. In the cross-modal KD, the audio streaming Transformer provides feature and response guidance that inherit fruitful and accurate speech information to supervise the training of the tailored radio streaming Transformer. The experimental results show that our Radio2Text can achieve a character error rate of 5.7% and a word error rate of 9.4% for the recognition of a vocabulary consisting of over 13,000 words.
</details>
<details>
<summary>摘要</summary>
高频振荡（mmWave）基于的语音识别系统提供了更多的音频相关应用，如会议语音笔录和窃听。然而，在实际场景中，延迟和可识别词汇数是两个不可或缺的因素。在这篇论文中，我们提出了Radio2Text，第一个 mmWave 基于的流动自动语音识别（ASR）系统，可以处理超过 13,000 个词的词汇库。Radio2Text 基于一个适应流动 Transformer，可以有效地学习语音相关特征的表示，为流动 ASR 开辟了新的可能性。为了解决流动网络无法访问整个未来输入的问题，我们提出了导航初始化，使得非流动 Transformer 的特征知识相关的全局上下文特征被传递给适应流动 Transformer  через 重量继承。此外，我们提出了基于知识储存（KD）的交叉模态结构，称为交叉模态 KD，以 Mitigate 低质量 mmWave 信号对识别性的负面影响。在交叉模态 KD 中，音频流动 Transformer 提供了特征和回应指导，将有用和准确的语音信息继承到supervise 适应流动 Transformer 的训练。实验结果显示，我们的 Radio2Text 可以实现字符错误率为 5.7% 和词错率为 9.4%  для 识别超过 13,000 个词的词汇库。
</details></li>
</ul>
<hr>
<h2 id="Separate-the-Wheat-from-the-Chaff-Model-Deficiency-Unlearning-via-Parameter-Efficient-Module-Operation"><a href="#Separate-the-Wheat-from-the-Chaff-Model-Deficiency-Unlearning-via-Parameter-Efficient-Module-Operation" class="headerlink" title="Separate the Wheat from the Chaff: Model Deficiency Unlearning via Parameter-Efficient Module Operation"></a>Separate the Wheat from the Chaff: Model Deficiency Unlearning via Parameter-Efficient Module Operation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08090">http://arxiv.org/abs/2308.08090</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinshuo Hu, Dongfang Li, Zihao Zheng, Zhenyu Liu, Baotian Hu, Min Zhang</li>
<li>for: 提高LLM的真实性和干净性</li>
<li>methods: 使用Extraction-before-Subtraction（Ext-Sub）方法，结合“专家”PEM和“反专家”PEM，以提高LLM的核心功能和扩展能力</li>
<li>results: 经过广泛的实验表明，我们的方法可以有效提高LLM的真实性和干净性，同时保留LLM的基本功能。<details>
<summary>Abstract</summary>
Large language models (LLMs) have been widely used in various applications but are known to suffer from issues related to untruthfulness and toxicity. While parameter-efficient modules (PEMs) have demonstrated their effectiveness in equipping models with new skills, leveraging PEMs for deficiency unlearning remains underexplored. In this work, we propose a PEMs operation approach, namely Extraction-before-Subtraction (Ext-Sub), to enhance the truthfulness and detoxification of LLMs through the integration of ``expert'' PEM and ``anti-expert'' PEM. Remarkably, even anti-expert PEM possess valuable capabilities due to their proficiency in generating fabricated content, which necessitates language modeling and logical narrative competence. Rather than merely negating the parameters, our approach involves extracting and eliminating solely the deficiency capability within anti-expert PEM while preserving the general capabilities. To evaluate the effectiveness of our approach in terms of truthfulness and detoxification, we conduct extensive experiments on LLMs, encompassing additional abilities such as language modeling and mathematical reasoning. Our empirical results demonstrate that our approach effectively improves truthfulness and detoxification, while largely preserving the fundamental abilities of LLMs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Improving-Detection-of-ChatGPT-Generated-Fake-Science-Using-Real-Publication-Text-Introducing-xFakeBibs-a-Supervised-Learning-Network-Algorithm"><a href="#Improving-Detection-of-ChatGPT-Generated-Fake-Science-Using-Real-Publication-Text-Introducing-xFakeBibs-a-Supervised-Learning-Network-Algorithm" class="headerlink" title="Improving Detection of ChatGPT-Generated Fake Science Using Real Publication Text: Introducing xFakeBibs a Supervised-Learning Network Algorithm"></a>Improving Detection of ChatGPT-Generated Fake Science Using Real Publication Text: Introducing xFakeBibs a Supervised-Learning Network Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11767">http://arxiv.org/abs/2308.11767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Abdeen Hamed, Xindong Wu</li>
<li>for: 本研究旨在分辨ChatGPT生成的学术论文和科学家生成的论文。</li>
<li>methods: 本研究使用一种新型的监督式机器学习算法来检测ChatGPT生成的论文和科学家生成的论文。</li>
<li>results: 研究发现，ChatGPT仅生成了23%的大文本内容，落后于其他10个拟合分布中的任何一个。这种技术性的差异使得ChatGPT难以与真正的科学研究相匹配。该算法可以准确地标识98 out of 100篇论文为假文献，但是还需要进一步的研究来检测所有假记录。<details>
<summary>Abstract</summary>
ChatGPT is becoming a new reality. In this paper, we show how to distinguish ChatGPT-generated publications from counterparts produced by scientists. Using a newly designed supervised Machine Learning algorithm, we demonstrate how to detect machine-generated publications from those produced by scientists. The algorithm was trained using 100 real publication abstracts, followed by a 10-fold calibration approach to establish a lower-upper bound range of acceptance. In the comparison with ChatGPT content, it was evident that ChatGPT contributed merely 23\% of the bigram content, which is less than 50\% of any of the other 10 calibrating folds. This analysis highlights a significant disparity in technical terms where ChatGPT fell short of matching real science. When categorizing the individual articles, the xFakeBibs algorithm accurately identified 98 out of 100 publications as fake, with 2 articles incorrectly classified as real publications. Though this work introduced an algorithmic approach that detected the ChatGPT-generated fake science with a high degree of accuracy, it remains challenging to detect all fake records. This work is indeed a step in the right direction to counter fake science and misinformation.
</details>
<details>
<summary>摘要</summary>
chatgpt是一种新的现实 becoming。在这篇论文中，我们展示了如何分辨chatgpt生成的出版物和科学家生成的对应出版物。使用一种新的监督式机器学习算法，我们示示了如何检测机器生成的出版物和科学家生成的出版物。这个算法在100个真实出版摘要上训练，然后采用10次核对方法来确定下限上限范围。与chatgpt内容进行比较，显然chatgpt仅占bigram内容的23%，这比任何其他10个核对叶下降的50%少。这一分析表明chatgpt在技术性方面存在显著的差距，它未能与真正的科学相匹配。当分类个别文章时，xFakeBibs算法正确地标识了98个出版物为假，2个文章被误将作为真正的出版物分类。虽然这项工作提出了一种算法方法来检测chatgpt生成的假科学，但是仍然存在一定的检测所有假记录的挑战。这项工作确实是对假科学和误信息的一步进击。
</details></li>
</ul>
<hr>
<h2 id="The-Costly-Dilemma-Generalization-Evaluation-and-Cost-Optimal-Deployment-of-Large-Language-Models"><a href="#The-Costly-Dilemma-Generalization-Evaluation-and-Cost-Optimal-Deployment-of-Large-Language-Models" class="headerlink" title="The Costly Dilemma: Generalization, Evaluation and Cost-Optimal Deployment of Large Language Models"></a>The Costly Dilemma: Generalization, Evaluation and Cost-Optimal Deployment of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08061">http://arxiv.org/abs/2308.08061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abi Aryan, Aakash Kumar Nain, Andrew McMahon, Lucas Augusto Meyer, Harpreet Singh Sahota</li>
<li>for: 这 paper 是为了探讨大语言模型在企业级别上的应用和投资问题。</li>
<li>methods: 该 paper 使用了一种框架，用于评估大语言模型的泛化、评价和成本优化。</li>
<li>results: 该 paper 表明，在大语言模型的开发、部署和管理方面，泛化、评价和成本优化是可以相互独立地考虑的三个因素。<details>
<summary>Abstract</summary>
When deploying machine learning models in production for any product/application, there are three properties that are commonly desired. First, the models should be generalizable, in that we can extend it to further use cases as our knowledge of the domain area develops. Second they should be evaluable, so that there are clear metrics for performance and the calculation of those metrics in production settings are feasible. Finally, the deployment should be cost-optimal as far as possible. In this paper we propose that these three objectives (i.e. generalization, evaluation and cost-optimality) can often be relatively orthogonal and that for large language models, despite their performance over conventional NLP models, enterprises need to carefully assess all the three factors before making substantial investments in this technology. We propose a framework for generalization, evaluation and cost-modeling specifically tailored to large language models, offering insights into the intricacies of development, deployment and management for these large language models.
</details>
<details>
<summary>摘要</summary>
当部署机器学习模型在生产环境中时，通常有三个属性被希望拥有。第一，模型应该泛化，以便在知识域领域的发展中扩展其使用场景。第二，模型应该可评估，以便在生产环境中有明确的表现指标和计算这些指标的能力。最后，部署应该是可最小化成本的。在这篇论文中，我们提出了这三个目标（即泛化、评估和成本优化）可以相对独立，并且对于大语言模型来说，企业需要仔细评估这三个因素才能够进行大规模投资。我们提出了特制化于大语言模型的泛化、评估和成本模型，以便更好地理解这些模型的开发、部署和管理。
</details></li>
</ul>
<hr>
<h2 id="Using-Artificial-Populations-to-Study-Psychological-Phenomena-in-Neural-Models"><a href="#Using-Artificial-Populations-to-Study-Psychological-Phenomena-in-Neural-Models" class="headerlink" title="Using Artificial Populations to Study Psychological Phenomena in Neural Models"></a>Using Artificial Populations to Study Psychological Phenomena in Neural Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08032">http://arxiv.org/abs/2308.08032</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jesse Roberts, Kyle Moore, Drew Wilenzick, Doug Fisher</li>
<li>for: 本研究旨在检测基于转换器的自然语言处理模型中是否存在人类认知行为。</li>
<li>methods: 该研究使用了uncertainty estimation的新方法，开发了一个名为PopulationLM的开源工具，以 theoretically 和现有的 cognitive 研究为基础，以及其他科学共同体的方法学 Lessons。</li>
<li>results: 通过人工 популяции的实验，发现语言模型在高度表现 Category 的情况下展现典型效应，但是不具有结构激活效应。 通过这些结果，我们发现单个模型通常会过度估计 neural 模型中的认知行为。<details>
<summary>Abstract</summary>
The recent proliferation of research into transformer based natural language processing has led to a number of studies which attempt to detect the presence of human-like cognitive behavior in the models. We contend that, as is true of human psychology, the investigation of cognitive behavior in language models must be conducted in an appropriate population of an appropriate size for the results to be meaningful. We leverage work in uncertainty estimation in a novel approach to efficiently construct experimental populations. The resultant tool, PopulationLM, has been made open source. We provide theoretical grounding in the uncertainty estimation literature and motivation from current cognitive work regarding language models. We discuss the methodological lessons from other scientific communities and attempt to demonstrate their application to two artificial population studies. Through population based experimentation we find that language models exhibit behavior consistent with typicality effects among categories highly represented in training. However, we find that language models don't tend to exhibit structural priming effects. Generally, our results show that single models tend to over estimate the presence of cognitive behaviors in neural models.
</details>
<details>
<summary>摘要</summary>
近期，研究基于转移器的自然语言处理技术的普及，导致了许多研究，试图探测模型中是否存在人类智能行为。我们认为，与人类心理学一样，研究语言模型的认知行为应该在适当的人口规模下进行。我们利用了不确定性估计的工作，开发了一种新的方法，名为PopulationLM，并将其开源。我们提供了不确定性估计的理论基础和当前语言模型认知工作的动机。我们也学习了其他科学社区的方法，尝试应用它们到两个人工人口研究中。通过人口基本实验，我们发现语言模型在高度表现Category的情况下展现典型效果，但是它们不往往展现结构激活效应。总的来说，我们的结果表明，单个模型通常会过度估计 neural models 中的认知行为。
</details></li>
</ul>
<hr>
<h2 id="End-to-End-Open-Vocabulary-Keyword-Search-With-Multilingual-Neural-Representations"><a href="#End-to-End-Open-Vocabulary-Keyword-Search-With-Multilingual-Neural-Representations" class="headerlink" title="End-to-End Open Vocabulary Keyword Search With Multilingual Neural Representations"></a>End-to-End Open Vocabulary Keyword Search With Multilingual Neural Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08027">http://arxiv.org/abs/2308.08027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bolaji Yusuf, Jan Cernocky, Murat Saraclar</li>
<li>for: 提高keyword搜寻系统的效能和简化搜寻过程</li>
<li>methods: 使用神经网络编码器对查询和文档进行编码，并将编码值进行点积 multiplication 以实现关键词搜寻</li>
<li>results: 对于长 queries 和不在训练数据中出现的 queries，提出的模型表现比ASR-based系统更佳，而对于短 queries 和在训练数据中出现的 queries，模型表现相对落后 ASR-based系统<details>
<summary>Abstract</summary>
Conventional keyword search systems operate on automatic speech recognition (ASR) outputs, which causes them to have a complex indexing and search pipeline. This has led to interest in ASR-free approaches to simplify the search procedure. We recently proposed a neural ASR-free keyword search model which achieves competitive performance while maintaining an efficient and simplified pipeline, where queries and documents are encoded with a pair of recurrent neural network encoders and the encodings are combined with a dot-product. In this article, we extend this work with multilingual pretraining and detailed analysis of the model. Our experiments show that the proposed multilingual training significantly improves the model performance and that despite not matching a strong ASR-based conventional keyword search system for short queries and queries comprising in-vocabulary words, the proposed model outperforms the ASR-based system for long queries and queries that do not appear in the training data.
</details>
<details>
<summary>摘要</summary>
传统的关键词搜索系统通常基于自动语音识别（ASR）输出，这导致了搜索过程中的复杂的索引和搜索管道。这有利于无需ASR的方法，以简化搜索过程。我们最近提出了一种基于神经网络的无ASR关键词搜索模型，该模型在竞争性能和高效的搜索管道之间寻找平衡。在这篇文章中，我们将这种工作扩展到多语言预训练和详细分析模型。我们的实验表明，论文中提出的多语言预训练显著提高了模型性能，而且尽管短 queries 和包含 vocabulary 词的 queries 不能与强大的 ASR-based 传统关键词搜索系统匹配，但是模型仍然超越了 ASR-based 系统，对于长 queries 和没有在训练数据中出现的 queries。
</details></li>
</ul>
<hr>
<h2 id="Anaphoric-Structure-Emerges-Between-Neural-Networks"><a href="#Anaphoric-Structure-Emerges-Between-Neural-Networks" class="headerlink" title="Anaphoric Structure Emerges Between Neural Networks"></a>Anaphoric Structure Emerges Between Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07984">http://arxiv.org/abs/2308.07984</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hcoxec/emerge">https://github.com/hcoxec/emerge</a></li>
<li>paper_authors: Nicholas Edwards, Hannah Rohde, Henry Conklin</li>
<li>for: 本研究探讨了自然语言中 ellipsis 和 anaphora 等结构是如何影响语言学习的。</li>
<li>methods: 研究者使用人工神经网络来解决交流任务，并观察了这些网络中是否会自然地出现 anaphoric 结构。</li>
<li>results: 研究发现，尽管 anaphoric 结构可能会增加ambiguity，但是这些结构仍然可以被人工神经网络学习，并且这些结构会自然地出现在网络之间。此外，在加入了效率压力的情况下，anaphoric 结构的出现变得更加普遍。研究结论是，certain pragmatic structures 可以 straightforwardly emerge between neural networks，但是speakers 和 listeners 之间的竞争需要conditions the degree and nature of their emergence.<details>
<summary>Abstract</summary>
Pragmatics is core to natural language, enabling speakers to communicate efficiently with structures like ellipsis and anaphora that can shorten utterances without loss of meaning. These structures require a listener to interpret an ambiguous form - like a pronoun - and infer the speaker's intended meaning - who that pronoun refers to. Despite potential to introduce ambiguity, anaphora is ubiquitous across human language. In an effort to better understand the origins of anaphoric structure in natural language, we look to see if analogous structures can emerge between artificial neural networks trained to solve a communicative task. We show that: first, despite the potential for increased ambiguity, languages with anaphoric structures are learnable by neural models. Second, anaphoric structures emerge between models 'naturally' without need for additional constraints. Finally, introducing an explicit efficiency pressure on the speaker increases the prevalence of these structures. We conclude that certain pragmatic structures straightforwardly emerge between neural networks, without explicit efficiency pressures, but that the competing needs of speakers and listeners conditions the degree and nature of their emergence.
</details>
<details>
<summary>摘要</summary>
Pragmatics 是自然语言的核心，允许发言人通过缩短语句而不产生意义损失的结构进行efficient的交流。这些结构需要听众可以解释模糊的形式，如pronoun，并从speaker的意图中归纳出true meaning。虽然可能引入模糊性，但是anaphora在人类语言中 ubique。为了更好地理解自然语言中anaphoric structure的起源，我们尝试看看人工神经网络在解决交流任务时是否可以学习这些结构。我们发现：一、despite the potential for increased ambiguity, languages with anaphoric structures can be learned by neural models; two、anaphoric structures emerge between models naturally, without the need for additional constraints; three、introducing an explicit efficiency pressure on the speaker increases the prevalence of these structures. 我们 conclude that certain pragmatic structures can emerge between neural networks without explicit efficiency pressures, but the competing needs of speakers and listeners condition the degree and nature of their emergence.
</details></li>
</ul>
<hr>
<h2 id="“Beware-of-deception”-Detecting-Half-Truth-and-Debunking-it-through-Controlled-Claim-Editing"><a href="#“Beware-of-deception”-Detecting-Half-Truth-and-Debunking-it-through-Controlled-Claim-Editing" class="headerlink" title="“Beware of deception”: Detecting Half-Truth and Debunking it through Controlled Claim Editing"></a>“Beware of deception”: Detecting Half-Truth and Debunking it through Controlled Claim Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07973">http://arxiv.org/abs/2308.07973</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sandeep Singamsetty, Nishtha Madaan, Sameep Mehta, Varad Bhatnagar, Pushpak Bhattacharyya</li>
<li>for: 帮助解决受伪信息侵害的问题，提高网络上信息的可靠性和准确性。</li>
<li>methods: 提出了一个全面的检测半真话模型和修订声明模型的管道，利用T5模型进行控制的声明修订。</li>
<li>results: 实现了对修订后的声明的BLEU分数0.88，对受检测的半真话的混淆分数85%，并在与其他语言模型比较中表现出了显著的优势。<details>
<summary>Abstract</summary>
The prevalence of half-truths, which are statements containing some truth but that are ultimately deceptive, has risen with the increasing use of the internet. To help combat this problem, we have created a comprehensive pipeline consisting of a half-truth detection model and a claim editing model. Our approach utilizes the T5 model for controlled claim editing; "controlled" here means precise adjustments to select parts of a claim. Our methodology achieves an average BLEU score of 0.88 (on a scale of 0-1) and a disinfo-debunk score of 85% on edited claims. Significantly, our T5-based approach outperforms other Language Models such as GPT2, RoBERTa, PEGASUS, and Tailor, with average improvements of 82%, 57%, 42%, and 23% in disinfo-debunk scores, respectively. By extending the LIAR PLUS dataset, we achieve an F1 score of 82% for the half-truth detection model, setting a new benchmark in the field. While previous attempts have been made at half-truth detection, our approach is, to the best of our knowledge, the first to attempt to debunk half-truths.
</details>
<details>
<summary>摘要</summary>
“半真话”的流行率在互联网时代增加了，为了解决这问题，我们创建了一个全面的管道，包括半真话检测模型和声明编辑模型。我们的方法使用 T5 模型进行控制的声明编辑，其中“控制”指的是精确地调整选择部分声明的部分。我们的方法在编辑声明后获得的 BLEU 分数为 0.88（分数范围为 0-1），并达到了对编辑声明的85%的误信驳斥分数。与其他语言模型相比，如 GPT2、RoBERTa、PEGASUS 和 Tailor，我们的 T5 基本法提供了82%、57%、42% 和 23% 的平均提高在误信驳斥分数上。通过扩展 LIAR PLUS 数据集，我们实现了半真话检测模型的 F1 分数为 82%，创造了新的benchmark。相比之前的尝试，我们的方法是我们知道的首次尝试用于驳斥半真话。
</details></li>
</ul>
<hr>
<h2 id="MultiSChuBERT-Effective-Multimodal-Fusion-for-Scholarly-Document-Quality-Prediction"><a href="#MultiSChuBERT-Effective-Multimodal-Fusion-for-Scholarly-Document-Quality-Prediction" class="headerlink" title="MultiSChuBERT: Effective Multimodal Fusion for Scholarly Document Quality Prediction"></a>MultiSChuBERT: Effective Multimodal Fusion for Scholarly Document Quality Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07971">http://arxiv.org/abs/2308.07971</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gideon Maillette de Buy Wenniger, Thomas van Dongen, Lambert Schomaker</li>
<li>for: 这 paper 的目的是提出一种多模态预测模型 MultiSChuBERT，用于学术文献质量预测（SDQP）任务。</li>
<li>methods: 这 paper 使用了一种 combining 文本和视觉模型，其中文本模型基于 chunking 全文本并计算BERT chunk-encodings（SChuBERT），而视觉模型基于 Inception V3。</li>
<li>results: 这 paper 的实验结果表明，combining 文本和视觉模型可以显著提高 SDQP 任务的结果。此外，paper 还证明了逐渐冰结 Visual 子模型的重量可以降低它的预测误差，并且使用不同的文本嵌入模型可以提高结果。<details>
<summary>Abstract</summary>
Automatic assessment of the quality of scholarly documents is a difficult task with high potential impact. Multimodality, in particular the addition of visual information next to text, has been shown to improve the performance on scholarly document quality prediction (SDQP) tasks. We propose the multimodal predictive model MultiSChuBERT. It combines a textual model based on chunking full paper text and aggregating computed BERT chunk-encodings (SChuBERT), with a visual model based on Inception V3.Our work contributes to the current state-of-the-art in SDQP in three ways. First, we show that the method of combining visual and textual embeddings can substantially influence the results. Second, we demonstrate that gradual-unfreezing of the weights of the visual sub-model, reduces its tendency to ovefit the data, improving results. Third, we show the retained benefit of multimodality when replacing standard BERT$_{\textrm{BASE}$ embeddings with more recent state-of-the-art text embedding models.   Using BERT$_{\textrm{BASE}$ embeddings, on the (log) number of citations prediction task with the ACL-BiblioMetry dataset, our MultiSChuBERT (text+visual) model obtains an $R^{2}$ score of 0.454 compared to 0.432 for the SChuBERT (text only) model. Similar improvements are obtained on the PeerRead accept/reject prediction task. In our experiments using SciBERT, scincl, SPECTER and SPECTER2.0 embeddings, we show that each of these tailored embeddings adds further improvements over the standard BERT$_{\textrm{BASE}$ embeddings, with the SPECTER2.0 embeddings performing best.
</details>
<details>
<summary>摘要</summary>
自动评估学术文献质量是一项复杂的任务，具有高度可能的影响。多模态，即在文献中添加视觉信息，已经被证明可以提高学术文献质量预测（SDQP）任务的性能。我们提出了多模态预测模型MultiSChuBERT。它将文本模型基于块化全篇文献文本和计算BERT块编码（SChuBERT），与视觉模型基于Inception V3.0相结合。我们的工作对现状领域的SDQP做出了三个贡献。首先，我们显示了将视觉和文本嵌入结合的方法可以具有显著的影响。其次，我们证明了逐渐冰封视觉子模型的重量的方法可以降低它的预测数据倾斜现象，提高结果。最后，我们显示了在使用现代文本嵌入模型代替标准BERT$_{\textrm{BASE}$嵌入时，多模态的优势仍然保留。使用BERT$_{\textrm{BASE}$嵌入，我们在ACL-BiblioMetry数据集上的（对数）引用数预测任务中，MultiSChuBERT（文本+视觉）模型的$R^{2}$分数为0.454，比SChuBERT（文本只）模型的0.432分数高。类似的改进也在PeerReadAccept/拒绝预测任务中得到。在我们使用SciBERT、scincl、SPECTER和SPECTER2.0嵌入时，我们显示了每个适应嵌入都添加了进一步的改进，SPECTER2.0嵌入表现最佳。
</details></li>
</ul>
<hr>
<h2 id="Teach-LLMs-to-Personalize-–-An-Approach-inspired-by-Writing-Education"><a href="#Teach-LLMs-to-Personalize-–-An-Approach-inspired-by-Writing-Education" class="headerlink" title="Teach LLMs to Personalize – An Approach inspired by Writing Education"></a>Teach LLMs to Personalize – An Approach inspired by Writing Education</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07968">http://arxiv.org/abs/2308.07968</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Li, Mingyang Zhang, Qiaozhu Mei, Yaqing Wang, Spurthi Amba Hombaiah, Yi Liang, Michael Bendersky</li>
<li>for: 本研究旨在提出一种通用的文本生成方法，用于个性化文本生成。</li>
<li>methods: 我们提出了一种多stage和多任务框架，用于教学大语言模型（LLMs）进行个性化文本生成。我们的方法受到写作教学中的写作任务的各个阶段启发，包括找到、评估、概括、结合和生成等。</li>
<li>results: 我们在三个公共数据集上进行了评估，并与多种基eline进行比较。我们的结果表明，我们的方法可以获得显著的提升。<details>
<summary>Abstract</summary>
Personalized text generation is an emerging research area that has attracted much attention in recent years. Most studies in this direction focus on a particular domain by designing bespoke features or models. In this work, we propose a general approach for personalized text generation using large language models (LLMs). Inspired by the practice of writing education, we develop a multistage and multitask framework to teach LLMs for personalized generation. In writing instruction, the task of writing from sources is often decomposed into multiple steps that involve finding, evaluating, summarizing, synthesizing, and integrating information. Analogously, our approach to personalized text generation consists of multiple stages: retrieval, ranking, summarization, synthesis, and generation. In addition, we introduce a multitask setting that helps the model improve its generation ability further, which is inspired by the observation in education that a student's reading proficiency and writing ability are often correlated. We evaluate our approach on three public datasets, each of which covers a different and representative domain. Our results show significant improvements over a variety of baselines.
</details>
<details>
<summary>摘要</summary>
personnalized text generation 是一个崛起的研究领域，Recently, much attention has been paid to this field. Most studies in this direction focus on a particular domain by designing bespoke features or models. In this work, we propose a general approach for personalized text generation using large language models (LLMs). Inspired by the practice of writing education, we develop a multistage and multitask framework to teach LLMs for personalized generation. In writing instruction, the task of writing from sources is often decomposed into multiple steps that involve finding, evaluating, summarizing, synthesizing, and integrating information. Analogously, our approach to personalized text generation consists of multiple stages: retrieval, ranking, summarization, synthesis, and generation. In addition, we introduce a multitask setting that helps the model improve its generation ability further, which is inspired by the observation in education that a student's reading proficiency and writing ability are often correlated. We evaluate our approach on three public datasets, each of which covers a different and representative domain. Our results show significant improvements over a variety of baselines.Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="The-Regular-Expression-Inference-Challenge"><a href="#The-Regular-Expression-Inference-Challenge" class="headerlink" title="The Regular Expression Inference Challenge"></a>The Regular Expression Inference Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07899">http://arxiv.org/abs/2308.07899</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mojtaba Valizadeh, Philip John Gorinski, Ignacio Iacobacci, Martin Berger</li>
<li>for: 这个论文是关于正则表达式推理（REI）的挑战，它是一种监督式机器学习（ML）和程序生成任务。</li>
<li>methods: 这篇论文使用了程序生成技术和GPU来实现REI问题的解决方案。</li>
<li>results: 这篇论文首次生成了大规模的REI问题数据集，并提出了一些初步的机器学习基线。<details>
<summary>Abstract</summary>
We propose \emph{regular expression inference (REI)} as a challenge for code/language modelling, and the wider machine learning community. REI is a supervised machine learning (ML) and program synthesis task, and poses the problem of finding minimal regular expressions from examples: Given two finite sets of strings $P$ and $N$ and a cost function $\text{cost}(\cdot)$, the task is to generate an expression $r$ that accepts all strings in $P$ and rejects all strings in $N$, while no other such expression $r'$ exists with $\text{cost}(r')<\text{cost}(r)$.   REI has advantages as a challenge problem: (i) regular expressions are well-known, widely used, and a natural idealisation of code; (ii) REI's asymptotic worst-case complexity is well understood; (iii) REI has a small number of easy to understand parameters (e.g.~$P$ or $N$ cardinality, string lengths of examples, or the cost function); this lets us easily finetune REI-hardness; (iv) REI is an unsolved problem for deep learning based ML.   Recently, an REI solver was implemented on GPUs, using program synthesis techniques. This enabled, for the first time, fast generation of minimal expressions for complex REI instances. Building on this advance, we generate and publish the first large-scale datasets for REI, and devise and evaluate several initial heuristic and machine learning baselines.   We invite the community to participate and explore ML methods that learn to solve REI problems. We believe that progress in REI directly translates to code/language modelling.
</details>
<details>
<summary>摘要</summary>
我们提出了几何表达推理（REI）作为代码/语言模型化挑战，并且广泛的机器学习社群。 REI 是一个监督式机器学习（ML）和程式生成任务，问题是找到最简的几何表达，使得所有 $P$ 中的字串都被接受，而所有 $N$ 中的字串都被拒绝，而且没有其他任何几何表达 $r'$ 存在，使得 $\text{cost}(r') < \text{cost}(r)$。REI 有以下优点作为挑战问题：1. 几何表达是通范知的，广泛使用的，并且是代码的自然化 идеalization。2. REI 的扩展最差情况的复杂度良好理解。3. REI 只有一小部分容易理解的参数（例如 $P$ 或 $N$ 的卡丁大小、字串示例的长度、或成本函数），这让我们可以轻松地调整 REI 的困难度。4. REI 是深度学习基础的未解问题。最近，一个 REI 解决方案在 GPU 上实现，使用程式生成技术。这允许了，如 never before，快速生成复杂的几何表达。基于这个进步，我们产生了第一个大规模的 REI 数据集，并设计了多个初步的调和和机器学习基线。我们邀请社区参与，探索机器学习方法可以解决 REI 问题。我们相信，进步在 REI 直接对代码/语言模型化有着影响。
</details></li>
</ul>
<hr>
<h2 id="Link-Context-Learning-for-Multimodal-LLMs"><a href="#Link-Context-Learning-for-Multimodal-LLMs" class="headerlink" title="Link-Context Learning for Multimodal LLMs"></a>Link-Context Learning for Multimodal LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07891">http://arxiv.org/abs/2308.07891</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/isekai-portal/Link-Context-Learning">https://github.com/isekai-portal/Link-Context-Learning</a></li>
<li>paper_authors: Yan Tai, Weichen Fan, Zhao Zhang, Feng Zhu, Rui Zhao, Ziwei Liu</li>
<li>for: 本研究旨在强化大型语言模型（MLLMs）的学习能力，使其能够在无需训练的情况下理解新的概念和图像。</li>
<li>methods: 本研究提出了链接上下文学习（LCL）方法，它在限定任务的情况下强制模型“学习学习”，并通过提供 causal 链接来增强模型对数据点之间的 causal 关系。</li>
<li>results: 对比 vanilla MLLMs，LCL-MLLM 在Recognizing unseen images和理解 novel concepts 方面表现出色，并且在ISEKAI dataset上进行了广泛的实验评估。<details>
<summary>Abstract</summary>
The ability to learn from context with novel concepts, and deliver appropriate responses are essential in human conversations. Despite current Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being trained on mega-scale datasets, recognizing unseen images or understanding novel concepts in a training-free manner remains a challenge. In-Context Learning (ICL) explores training-free few-shot learning, where models are encouraged to ``learn to learn" from limited tasks and generalize to unseen tasks. In this work, we propose link-context learning (LCL), which emphasizes "reasoning from cause and effect" to augment the learning capabilities of MLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causal relationship between the support set and the query set. By providing demonstrations with causal links, LCL guides the model to discern not only the analogy but also the underlying causal associations between data points, which empowers MLLMs to recognize unseen images and understand novel concepts more effectively. To facilitate the evaluation of this novel approach, we introduce the ISEKAI dataset, comprising exclusively of unseen generated image-label pairs designed for link-context learning. Extensive experiments show that our LCL-MLLM exhibits strong link-context learning capabilities to novel concepts over vanilla MLLMs. Code and data will be released at https://github.com/isekai-portal/Link-Context-Learning.
</details>
<details>
<summary>摘要</summary>
人工智能沟通中，学习从文本上下文中理解新概念并提供相应的响应是非常重要的。尽管当前的多模态大型自然语言模型（MLLM）和大型自然语言模型（LLM）在巨大数据集上训练，仍然识别未看过的图像或理解未经训练的新概念是一大挑战。无需训练的内容学习（ICL）探索了几shot学习，使模型能够“学习学习”从有限任务中吸取知识并通过未经训练的任务进行推理。在这种工作中，我们提出了链接上下文学习（LCL），强调“因果关系”来增强模型的学习能力。LCL比传统的ICL更加广泛，通过显示 causal 链接，使模型能够不仅理解 analogies，还能够捕捉数据点之间的 causal 关系，从而使得 MLLMs 能够更好地识别未看过的图像和理解新概念。为了便于这种新的方法的评估，我们提出了 ISEKAI 数据集，包括一系列未经训练的生成图像标签对，供 link-context 学习使用。经过广泛的实验，我们发现我们的 LCL-MLLM 在对 novel 概念的链接上下文学习中表现出色，胜过普通的 MLLMs。代码和数据将在 GitHub 上发布，请参考 https://github.com/isekai-portal/Link-Context-Learning。
</details></li>
</ul>
<hr>
<h2 id="A-Trustable-LSTM-Autoencoder-Network-for-Cyberbullying-Detection-on-Social-Media-Using-Synthetic-Data"><a href="#A-Trustable-LSTM-Autoencoder-Network-for-Cyberbullying-Detection-on-Social-Media-Using-Synthetic-Data" class="headerlink" title="A Trustable LSTM-Autoencoder Network for Cyberbullying Detection on Social Media Using Synthetic Data"></a>A Trustable LSTM-Autoencoder Network for Cyberbullying Detection on Social Media Using Synthetic Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09722">http://arxiv.org/abs/2308.09722</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mst Shapna Akter, Hossain Shahriar, Alfredo Cuzzocrea</li>
<li>for: The paper aims to detect cyberbullying on social media, specifically using a trustable LSTM-Autoencoder Network with synthetic data to address data availability issues.</li>
<li>methods: The proposed method uses a combination of LSTM and Autoencoder networks to identify aggressive comments on social media, and the authors also compare the performance of their model with other state-of-the-art models such as LSTM, BiLSTM, Word2vec, BERT, and GPT-2.</li>
<li>results: The proposed model outperforms all the other models on all datasets, achieving an accuracy of 95%. The authors also demonstrate the state-of-the-art results of their model compared to previous works on the dataset used in this paper.Here’s the simplified Chinese version of the three key points:</li>
<li>for: 这篇论文目标是在社交媒体上探测циberbullying，使用可靠的LSTM-Autoencoder网络来解决数据可用性问题。</li>
<li>methods: 该方法使用LSTM和Autoencoder网络组合来识别社交媒体上的侵略性评论，并与其他现有的state-of-the-art模型进行比较，包括LSTM、BiLSTM、Word2vec、BERT和GPT-2模型。</li>
<li>results: 该方法在所有数据集上都有最高的准确率（95%），并且比前一些工作在该数据集上的结果更为出色。<details>
<summary>Abstract</summary>
Social media cyberbullying has a detrimental effect on human life. As online social networking grows daily, the amount of hate speech also increases. Such terrible content can cause depression and actions related to suicide. This paper proposes a trustable LSTM-Autoencoder Network for cyberbullying detection on social media using synthetic data. We have demonstrated a cutting-edge method to address data availability difficulties by producing machine-translated data. However, several languages such as Hindi and Bangla still lack adequate investigations due to a lack of datasets. We carried out experimental identification of aggressive comments on Hindi, Bangla, and English datasets using the proposed model and traditional models, including Long Short-Term Memory (LSTM), Bidirectional Long Short-Term Memory (BiLSTM), LSTM-Autoencoder, Word2vec, Bidirectional Encoder Representations from Transformers (BERT), and Generative Pre-trained Transformer 2 (GPT-2) models. We employed evaluation metrics such as f1-score, accuracy, precision, and recall to assess the models performance. Our proposed model outperformed all the models on all datasets, achieving the highest accuracy of 95%. Our model achieves state-of-the-art results among all the previous works on the dataset we used in this paper.
</details>
<details>
<summary>摘要</summary>
社交媒体网络欺凌对人类生活产生负面影响。随着在线社交网络的日常增长，讨厌言语的数量也在增加。这些丑陋的内容可以导致抑郁和自杀行为。这篇论文提议一种可靠的LSTM-自动encoder网络，用于社交媒体上的欺凌检测。我们通过生成机器翻译数据解决了数据可用性的问题。然而，一些语言，如希ن第和孟加拉语仍然缺乏足够的研究，因为缺乏数据集。我们通过实验标识了攻击性评论在希нд语、孟加拉语和英语数据集上的表现。我们使用了评价指标，如f1分数、准确率、精度和回归率来评价模型的表现。我们的提议模型在所有数据集上都达到了最高的准确率95%。我们的模型在所有之前的工作中 achieved state-of-the-art 结果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/16/cs.CL_2023_08_16/" data-id="clorjzl3x009rf1880mxyaar3" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/16/cs.LG_2023_08_16/" class="article-date">
  <time datetime="2023-08-16T10:00:00.000Z" itemprop="datePublished">2023-08-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/16/cs.LG_2023_08_16/">cs.LG - 2023-08-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Accurate-synthesis-of-Dysarthric-Speech-for-ASR-data-augmentation"><a href="#Accurate-synthesis-of-Dysarthric-Speech-for-ASR-data-augmentation" class="headerlink" title="Accurate synthesis of Dysarthric Speech for ASR data augmentation"></a>Accurate synthesis of Dysarthric Speech for ASR data augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08438">http://arxiv.org/abs/2308.08438</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Soleymanpour, Michael T. Johnson, Rahim Soleymanpour, Jeffrey Berry</li>
<li>For: This paper is written for the purpose of developing a new dysarthric speech synthesis method for use in Automatic Speech Recognition (ASR) training data augmentation.* Methods: The paper uses a modified neural multi-talker Text-to-Speech (TTS) system, which includes a dysarthria severity level coefficient and a pause insertion model, to synthesize dysarthric speech for varying severity levels. The paper also uses a DNN-HMM model for dysarthria-specific speech recognition.* Results: The paper shows that the addition of dysarthric speech synthesis to ASR training data improves the accuracy of dysarthric speech recognition by 12.2%, and that the addition of severity level and pause insertion controls decreases WER by 6.5%. The subjective evaluation shows that the synthesized speech is perceived as similar to true dysarthric speech, especially for higher levels of dysarthria.Here is the simplified Chinese translation of the three key information points:* For: 这篇论文是为了开发一种新的嗜睡术语合成方法，以便用于自动语音识别（ASR）训练数据增强。* Methods: 这篇论文使用一种修改后的神经网络多个人Text-to-Speech（TTS）系统，包括嗜睡度量级别和插入停顿模型，以生成嗜睡术语。* Results: 这篇论文显示，通过将嗜睡术语合成添加到ASR训练数据中，可以提高嗜睡术语识别精度，相比基eline，增强精度12.2%。此外，添加严重度和插入停顿控制可以降低WER值6.5%。主观评估表明，生成的合成语音与真正的嗜睡术语相似，尤其是高度嗜睡术语。<details>
<summary>Abstract</summary>
Dysarthria is a motor speech disorder often characterized by reduced speech intelligibility through slow, uncoordinated control of speech production muscles. Automatic Speech recognition (ASR) systems can help dysarthric talkers communicate more effectively. However, robust dysarthria-specific ASR requires a significant amount of training speech, which is not readily available for dysarthric talkers. This paper presents a new dysarthric speech synthesis method for the purpose of ASR training data augmentation. Differences in prosodic and acoustic characteristics of dysarthric spontaneous speech at varying severity levels are important components for dysarthric speech modeling, synthesis, and augmentation. For dysarthric speech synthesis, a modified neural multi-talker TTS is implemented by adding a dysarthria severity level coefficient and a pause insertion model to synthesize dysarthric speech for varying severity levels. To evaluate the effectiveness for synthesis of training data for ASR, dysarthria-specific speech recognition was used. Results show that a DNN-HMM model trained on additional synthetic dysarthric speech achieves WER improvement of 12.2% compared to the baseline, and that the addition of the severity level and pause insertion controls decrease WER by 6.5%, showing the effectiveness of adding these parameters. Overall results on the TORGO database demonstrate that using dysarthric synthetic speech to increase the amount of dysarthric-patterned speech for training has significant impact on the dysarthric ASR systems. In addition, we have conducted a subjective evaluation to evaluate the dysarthric-ness and similarity of synthesized speech. Our subjective evaluation shows that the perceived dysartrhic-ness of synthesized speech is similar to that of true dysarthric speech, especially for higher levels of dysarthria
</details>
<details>
<summary>摘要</summary>
�� Dyasarthria 是一种运动性语言障碍，常characterized by reduced speech intelligibility through slow, uncoordinated control of speech production muscles. Automatic Speech recognition (ASR) systems can help dysarthric talkers communicate more effectively. However, robust dysarthria-specific ASR requires a significant amount of training speech, which is not readily available for dysarthric talkers. This paper presents a new dysarthric speech synthesis method for the purpose of ASR training data augmentation. Differences in prosodic and acoustic characteristics of dysarthric spontaneous speech at varying severity levels are important components for dysarthric speech modeling, synthesis, and augmentation. For dysarthric speech synthesis, a modified neural multi-talker TTS is implemented by adding a dysarthria severity level coefficient and a pause insertion model to synthesize dysarthric speech for varying severity levels. To evaluate the effectiveness for synthesis of training data for ASR, dysarthria-specific speech recognition was used. Results show that a DNN-HMM model trained on additional synthetic dysarthric speech achieves WER improvement of 12.2% compared to the baseline, and that the addition of the severity level and pause insertion controls decrease WER by 6.5%, showing the effectiveness of adding these parameters. Overall results on the TORGO database demonstrate that using dysarthric synthetic speech to increase the amount of dysarthric-patterned speech for training has significant impact on the dysarthric ASR systems. In addition, we have conducted a subjective evaluation to evaluate the dysarthric-ness and similarity of synthesized speech. Our subjective evaluation shows that the perceived dysartrhic-ness of synthesized speech is similar to that of true dysarthric speech, especially for higher levels of dysarthria.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Eliciting-Risk-Aversion-with-Inverse-Reinforcement-Learning-via-Interactive-Questioning"><a href="#Eliciting-Risk-Aversion-with-Inverse-Reinforcement-Learning-via-Interactive-Questioning" class="headerlink" title="Eliciting Risk Aversion with Inverse Reinforcement Learning via Interactive Questioning"></a>Eliciting Risk Aversion with Inverse Reinforcement Learning via Interactive Questioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08427">http://arxiv.org/abs/2308.08427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziteng Cheng, Anthony Coache, Sebastian Jaimungal</li>
<li>for: 这个研究旨在使用互动问答方法来识别代理人的风险偏好。</li>
<li>methods: 我们在一期和无穷远期两种场景中进行研究，在一期场景中，我们假设代理人的风险偏好是由状态的成本函数和风险评估函数来描述。在无穷远期场景中，我们模型风险偏好带有一个折衡因子。我们假设可以访问一组finite的候选人，其中包含代理人的真实风险偏好，然后通过询问代理人的优化政策在不同环境中来识别代理人的风险偏好。</li>
<li>results: 我们证明，询问代理人的优化政策在不同环境中是一种有效的方法来识别代理人的风险偏好。具体来说，我们证明代理人的风险偏好可以通过问题的数量增长和问题的随机设计来识别出来，并且我们开发了一种算法来设计优化问题。在 simulations 中，我们发现我们的方法可以快速地学习代理人的风险偏好，比Randomly 设计的问题更快。这种方法在 robo-advising 中有重要应用，并提供了一种新的风险偏好识别方法。<details>
<summary>Abstract</summary>
This paper proposes a novel framework for identifying an agent's risk aversion using interactive questioning. Our study is conducted in two scenarios: a one-period case and an infinite horizon case. In the one-period case, we assume that the agent's risk aversion is characterized by a cost function of the state and a distortion risk measure. In the infinite horizon case, we model risk aversion with an additional component, a discount factor. Assuming the access to a finite set of candidates containing the agent's true risk aversion, we show that asking the agent to demonstrate her optimal policies in various environment, which may depend on their previous answers, is an effective means of identifying the agent's risk aversion. Specifically, we prove that the agent's risk aversion can be identified as the number of questions tends to infinity, and the questions are randomly designed. We also develop an algorithm for designing optimal questions and provide empirical evidence that our method learns risk aversion significantly faster than randomly designed questions in simulations. Our framework has important applications in robo-advising and provides a new approach for identifying an agent's risk preferences.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "risk aversion" is translated as "风险偏好" (fēngxǐn tiěndòng)* "cost function" is translated as "成本函数" (chéngbèng fāngxìn)* "distortion risk measure" is translated as "偏移风险度量" (diānchōng fēngxǐn duōliàng)* "infinite horizon" is translated as "无限距离" (wúxìn jiǔdì)* "discount factor" is translated as "折扣因子" (diǎnkē yīn zhī)* "finite set" is translated as "有限集" (yǒujiàn jítè)* "optimal policies" is translated as "最佳策略" (zuìjiā cuòlüè)* "randomly designed questions" is translated as "随机设计的问题" (suījī jièdǎo de wèn tí)* "empirical evidence" is translated as "实验证据" (shíyàn zhèngjiā)
</details></li>
</ul>
<hr>
<h2 id="Digital-twinning-of-cardiac-electrophysiology-models-from-the-surface-ECG-a-geodesic-backpropagation-approach"><a href="#Digital-twinning-of-cardiac-electrophysiology-models-from-the-surface-ECG-a-geodesic-backpropagation-approach" class="headerlink" title="Digital twinning of cardiac electrophysiology models from the surface ECG: a geodesic backpropagation approach"></a>Digital twinning of cardiac electrophysiology models from the surface ECG: a geodesic backpropagation approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08410">http://arxiv.org/abs/2308.08410</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Grandits, Jan Verhülsdonk, Gundolf Haase, Alexander Effland, Simone Pezzuto</li>
<li>for: 用于建立个性化的心脏电physiology模型，以便在临床时间Constraint中提供个性化的cardiac models。</li>
<li>methods: 使用Geodesic-BP方法，一种基于GPU加速的机器学习框架，来优化eikonal方程的参数，以达到模拟cardiac activation的高精度。</li>
<li>results: 在一个人工测试 caso中，Geodesic-BP方法可以很准确地重construct一个模拟的cardiac activation，包括在模型不准确性的情况下。此外，我们还应用了该算法于一个公共可用的兔子模型数据集，得到了非常正面的结果。<details>
<summary>Abstract</summary>
The eikonal equation has become an indispensable tool for modeling cardiac electrical activation accurately and efficiently. In principle, by matching clinically recorded and eikonal-based electrocardiograms (ECGs), it is possible to build patient-specific models of cardiac electrophysiology in a purely non-invasive manner. Nonetheless, the fitting procedure remains a challenging task. The present study introduces a novel method, Geodesic-BP, to solve the inverse eikonal problem. Geodesic-BP is well-suited for GPU-accelerated machine learning frameworks, allowing us to optimize the parameters of the eikonal equation to reproduce a given ECG. We show that Geodesic-BP can reconstruct a simulated cardiac activation with high accuracy in a synthetic test case, even in the presence of modeling inaccuracies. Furthermore, we apply our algorithm to a publicly available dataset of a rabbit model, with very positive results. Given the future shift towards personalized medicine, Geodesic-BP has the potential to help in future functionalizations of cardiac models meeting clinical time constraints while maintaining the physiological accuracy of state-of-the-art cardiac models.
</details>
<details>
<summary>摘要</summary>
《椭圆方程》已成为心脏电动力学模型的不可或缺工具。在原理上，通过对临床记录和椭圆方程基于的电cardiogram（ECG）进行匹配，可以建立个性化的心脏电physiology模型，无需侵入性的干预。然而，匹配过程仍然是一项复杂的任务。本研究提出了一种新方法，即Geodesic-BP，以解决反椭圆问题。Geodesic-BP适用于加速机器学习框架的GPU，可以优化椭圆方程的参数，以复制给定的ECG。我们在synthetic测试案例中显示，Geodesic-BP可以高精度地重建模拟的cardiac activation，即使在模型不准确的情况下。此外，我们对公共可用的兔子模型数据集进行了应用，得到了非常正面的结果。随着未来的个性化医疗转型，Geodesic-BP具有帮助将cardiac模型功能化，遵循临床时间约束，保持现有的心脏模型physiological accuracy的潜在力量。
</details></li>
</ul>
<hr>
<h2 id="Explainable-AI-for-clinical-risk-prediction-a-survey-of-concepts-methods-and-modalities"><a href="#Explainable-AI-for-clinical-risk-prediction-a-survey-of-concepts-methods-and-modalities" class="headerlink" title="Explainable AI for clinical risk prediction: a survey of concepts, methods, and modalities"></a>Explainable AI for clinical risk prediction: a survey of concepts, methods, and modalities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08407">http://arxiv.org/abs/2308.08407</a></li>
<li>repo_url: None</li>
<li>paper_authors: Munib Mesinovic, Peter Watkinson, Tingting Zhu</li>
<li>for: 这篇论文的目的是解释AI医疗应用中的关键概念，包括解释性、可读性、公平性、信任和透明度，以及它们如何在临床预测中实现。</li>
<li>methods: 这篇论文使用了许多解释模型的发展，包括使用生成器和潜在隐藏的数据来提高解释性，以及使用多种解释方法来增强信任和公平性。</li>
<li>results: 这篇论文的结果显示，这些解释模型可以在临床预测中提高解释性和信任度，并且可以实现在多种模式下的透明度和公平性。<details>
<summary>Abstract</summary>
Recent advancements in AI applications to healthcare have shown incredible promise in surpassing human performance in diagnosis and disease prognosis. With the increasing complexity of AI models, however, concerns regarding their opacity, potential biases, and the need for interpretability. To ensure trust and reliability in AI systems, especially in clinical risk prediction models, explainability becomes crucial. Explainability is usually referred to as an AI system's ability to provide a robust interpretation of its decision-making logic or the decisions themselves to human stakeholders. In clinical risk prediction, other aspects of explainability like fairness, bias, trust, and transparency also represent important concepts beyond just interpretability. In this review, we address the relationship between these concepts as they are often used together or interchangeably. This review also discusses recent progress in developing explainable models for clinical risk prediction, highlighting the importance of quantitative and clinical evaluation and validation across multiple common modalities in clinical practice. It emphasizes the need for external validation and the combination of diverse interpretability methods to enhance trust and fairness. Adopting rigorous testing, such as using synthetic datasets with known generative factors, can further improve the reliability of explainability methods. Open access and code-sharing resources are essential for transparency and reproducibility, enabling the growth and trustworthiness of explainable research. While challenges exist, an end-to-end approach to explainability in clinical risk prediction, incorporating stakeholders from clinicians to developers, is essential for success.
</details>
<details>
<summary>摘要</summary>
近期人工智能在医疗领域的应用显示了很大的承诺，可以超越人类的诊断和疾病预测。然而，随着人工智能模型的复杂度的增加，关于它们的不透明度、潜在偏见和解释性的问题也在关注。为确保人工智能系统的可靠性和可信worthiness，特别是在临床风险预测模型中，解释性变得非常重要。解释性通常指人工智能系统能够提供人类权利者可靠的决策逻辑或决策结果的强有力的解释。在临床风险预测中，其他方面的解释性，如公平、偏见、信任和透明度，也是重要的概念，不仅是解释性。本文评论了这些概念之间的关系，并讨论了最新的开发可解释模型的进展，强调临床实践中多种普遍的Modalities的数据量的评估和验证。它强调需要外部验证和多种解释性方法的组合，以增强可靠性和公平。采用严格的测试，如使用已知生成因素的 sintetic 数据集，可以进一步提高解释性方法的可靠性。开放访问和代码分享资源是必要的，以便透明度和可重现性。虽然存在挑战，但是综合approach，从临床医生到开发者，是必要的 для成功。
</details></li>
</ul>
<hr>
<h2 id="Content-based-Recommendation-Engine-for-Video-Streaming-Platform"><a href="#Content-based-Recommendation-Engine-for-Video-Streaming-Platform" class="headerlink" title="Content-based Recommendation Engine for Video Streaming Platform"></a>Content-based Recommendation Engine for Video Streaming Platform</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08406">http://arxiv.org/abs/2308.08406</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Puskal Khadka, Prabhav Lamichhane</li>
<li>for: 提供视频推荐服务，根据用户的前一个 interess和选择。</li>
<li>methods: 使用机器学习算法和TF-IDF文本矩阵化方法，确定文档中的相关性。计算每个内容之间的cosinusimilarity值，以确定视频的相似性。</li>
<li>results: 提出一种基于内容的推荐引擎，可以为用户提供适合其兴趣和选择的视频建议。测试结果显示，提出的引擎性能比较好，精度、回归率和F1核心均达到了预期水平。<details>
<summary>Abstract</summary>
Recommendation engine suggest content, product or services to the user by using machine learning algorithm. This paper proposed a content-based recommendation engine for providing video suggestion to the user based on their previous interests and choices. We will use TF-IDF text vectorization method to determine the relevance of words in a document. Then we will find out the similarity between each content by calculating cosine similarity between them. Finally, engine will recommend videos to the users based on the obtained similarity score value. In addition, we will measure the engine's performance by computing precision, recall, and F1 core of the proposed system.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate into Simplified Chinese推荐引擎使用机器学习算法提供内容、产品或服务给用户。这篇论文提出了基于用户之前的兴趣和选择的视频推荐引擎。我们使用TF-IDF文本向量化方法确定文档中的相关性。然后我们计算每个内容之间的cosine相似性，并根据得到的相似性分值来推荐视频给用户。此外，我们还会测算推荐引擎的性能，包括精度、准确率和F1分值。Note: "TF-IDF" stands for "Term Frequency-Inverse Document Frequency", which is a text vectorization method used to determine the importance of words in a document.
</details></li>
</ul>
<hr>
<h2 id="Fast-Uncertainty-Quantification-of-Spent-Nuclear-Fuel-with-Neural-Networks"><a href="#Fast-Uncertainty-Quantification-of-Spent-Nuclear-Fuel-with-Neural-Networks" class="headerlink" title="Fast Uncertainty Quantification of Spent Nuclear Fuel with Neural Networks"></a>Fast Uncertainty Quantification of Spent Nuclear Fuel with Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08391">http://arxiv.org/abs/2308.08391</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arnau Albà, Andreas Adelmann, Lucas Münster, Dimitri Rochman, Romana Boiger</li>
<li>for: 这 paper 是为了快速评估核电燃料（SNF）的特性而写的。</li>
<li>methods: 这 paper 使用神经网络（NN）来模拟 SNF 的特性，减少计算成本。</li>
<li>results: NN 可以准确地预测 decay heat 和 nuclide 浓度，响应关键输入参数的变化。模型被验证了，并且可以减少计算成本。<details>
<summary>Abstract</summary>
The accurate calculation and uncertainty quantification of the characteristics of spent nuclear fuel (SNF) play a crucial role in ensuring the safety, efficiency, and sustainability of nuclear energy production, waste management, and nuclear safeguards. State of the art physics-based models, while reliable, are computationally intensive and time-consuming. This paper presents a surrogate modeling approach using neural networks (NN) to predict a number of SNF characteristics with reduced computational costs compared to physics-based models. An NN is trained using data generated from CASMO5 lattice calculations. The trained NN accurately predicts decay heat and nuclide concentrations of SNF, as a function of key input parameters, such as enrichment, burnup, cooling time between cycles, mean boron concentration and fuel temperature. The model is validated against physics-based decay heat simulations and measurements of different uranium oxide fuel assemblies from two different pressurized water reactors. In addition, the NN is used to perform sensitivity analysis and uncertainty quantification. The results are in very good alignment to CASMO5, while the computational costs (taking into account the costs of generating training samples) are reduced by a factor of 10 or more. Our findings demonstrate the feasibility of using NNs as surrogate models for fast characterization of SNF, providing a promising avenue for improving computational efficiency in assessing nuclear fuel behavior and associated risks.
</details>
<details>
<summary>摘要</summary>
现代物理模型可靠但计算成本高。这篇论文提出了使用神经网络（NN）模型来快速预测核电燃料（SNF）特性。通过训练NN模型使用CASMO5网格计算数据，可以准确预测燃料衰变热和核lide浓度，具体取决于一些关键输入参数，如浓缩度、燃烧时间、循环冷却时间、平均氧化物含量和燃料温度。模型被验证了基于物理模型的衰变热计算和不同氧化物燃料聚集体的测量数据。此外，NN模型还可以进行敏感分析和不确定性评估。结果与CASMO5很相似，计算成本（包括生成训练样本的成本）被降低了一倍或更多。这些发现表明使用NN模型可以快速预测SNF特性，提供了改善核电燃料行为和相关风险评估计算效率的可能性。
</details></li>
</ul>
<hr>
<h2 id="Continuous-Sweep-an-improved-binary-quantifier"><a href="#Continuous-Sweep-an-improved-binary-quantifier" class="headerlink" title="Continuous Sweep: an improved, binary quantifier"></a>Continuous Sweep: an improved, binary quantifier</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08387">http://arxiv.org/abs/2308.08387</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kevin Kloos, Julian D. Karch, Quinten A. Meertens, Mark de Rooij</li>
<li>for: 估计数据集中类别的总比例（quantification learning）</li>
<li>methods: 使用参数化二分类分布（parametric binary quantifier），改进决策边界，并计算mean而不是median</li>
<li>results: 在各种情况下，Continuous Sweep比Median Sweep表现更好，并且可以通过分析表达来找到最佳决策边界。<details>
<summary>Abstract</summary>
Quantification is a supervised machine learning task, focused on estimating the class prevalence of a dataset rather than labeling its individual observations. We introduce Continuous Sweep, a new parametric binary quantifier inspired by the well-performing Median Sweep. Median Sweep is currently one of the best binary quantifiers, but we have changed this quantifier on three points, namely 1) using parametric class distributions instead of empirical distributions, 2) optimizing decision boundaries instead of applying discrete decision rules, and 3) calculating the mean instead of the median. We derive analytic expressions for the bias and variance of Continuous Sweep under general model assumptions. This is one of the first theoretical contributions in the field of quantification learning. Moreover, these derivations enable us to find the optimal decision boundaries. Finally, our simulation study shows that Continuous Sweep outperforms Median Sweep in a wide range of situations.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Precision-and-Recall-Reject-Curves-for-Classification"><a href="#Precision-and-Recall-Reject-Curves-for-Classification" class="headerlink" title="Precision and Recall Reject Curves for Classification"></a>Precision and Recall Reject Curves for Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08381">http://arxiv.org/abs/2308.08381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lydia Fischer, Patricia Wollstadt</li>
<li>for: 用于评估模型的置信度评价</li>
<li>methods: 使用 prototype-based classifiers from learning vector quantization</li>
<li>results: 提供了一种新的评估方法，可以更 accurately 评估模型的性能， especialy for 数据异常分布的场景。<details>
<summary>Abstract</summary>
For some classification scenarios, it is desirable to use only those classification instances that a trained model associates with a high certainty. To obtain such high-certainty instances, previous work has proposed accuracy-reject curves. Reject curves allow to evaluate and compare the performance of different certainty measures over a range of thresholds for accepting or rejecting classifications. However, the accuracy may not be the most suited evaluation metric for all applications, and instead precision or recall may be preferable. This is the case, for example, for data with imbalanced class distributions. We therefore propose reject curves that evaluate precision and recall, the recall-reject curve and the precision-reject curve. Using prototype-based classifiers from learning vector quantization, we first validate the proposed curves on artificial benchmark data against the accuracy reject curve as a baseline. We then show on imbalanced benchmarks and medical, real-world data that for these scenarios, the proposed precision- and recall-curves yield more accurate insights into classifier performance than accuracy reject curves.
</details>
<details>
<summary>摘要</summary>
有些分类场景中，您可能想使用已经训练好的模型对分类结果进行高度确定性的评估。为了获得这些高度确定性的分类实例，前一些工作提出了准确率拒绝曲线。拒绝曲线可以评估和比较不同确定度度量在不同的阈值上Accept或拒绝分类的性能。但是，准确率可能不是所有应用场景中最适合的评估度量，特别是数据具有不均匀的类别分布。我们因此提议使用精度和准确率的拒绝曲线。使用学习 вектор量化的原型基 classifier，我们首先验证提议的曲线在人工 benchmark 数据上对准确率 reject 曲线作为基准进行验证。然后，我们在具有不均匀分布的 benchmark 和医学实际数据上示出，在这些场景中，提议的精度和准确率曲线可以更准确地评估分类器的性能，而不是准确率 reject 曲线。
</details></li>
</ul>
<hr>
<h2 id="A-distributed-neural-network-architecture-for-dynamic-sensor-selection-with-application-to-bandwidth-constrained-body-sensor-networks"><a href="#A-distributed-neural-network-architecture-for-dynamic-sensor-selection-with-application-to-bandwidth-constrained-body-sensor-networks" class="headerlink" title="A distributed neural network architecture for dynamic sensor selection with application to bandwidth-constrained body-sensor networks"></a>A distributed neural network architecture for dynamic sensor selection with application to bandwidth-constrained body-sensor networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08379">http://arxiv.org/abs/2308.08379</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Strypsteen, Alexander Bertrand</li>
<li>for: 这个论文旨在提出一种动态侦测器选择方法，以便在深度神经网络（DNN）中选择最佳的侦测器子集，并将这个选择与任务模型一起学习，以提高侦测器选择的精确性。</li>
<li>methods: 这个方法使用Gumbel-Softmax点子来允许数字的决策被通过标准的反射调变学习。它还包括一个动态空间范 filter，使任务-DNN更加具有耐腐性，以便能够处理多个可能的node subset。</li>
<li>results: 这个方法可以将选择最佳通道分布到不同的节点上，并且可以对实际的体内侦测网络（WSN）进行验证，并分析传输负载和任务准确性之间的交易。<details>
<summary>Abstract</summary>
We propose a dynamic sensor selection approach for deep neural networks (DNNs), which is able to derive an optimal sensor subset selection for each specific input sample instead of a fixed selection for the entire dataset. This dynamic selection is jointly learned with the task model in an end-to-end way, using the Gumbel-Softmax trick to allow the discrete decisions to be learned through standard backpropagation. We then show how we can use this dynamic selection to increase the lifetime of a wireless sensor network (WSN) by imposing constraints on how often each node is allowed to transmit. We further improve performance by including a dynamic spatial filter that makes the task-DNN more robust against the fact that it now needs to be able to handle a multitude of possible node subsets. Finally, we explain how the selection of the optimal channels can be distributed across the different nodes in a WSN. We validate this method on a use case in the context of body-sensor networks, where we use real electroencephalography (EEG) sensor data to emulate an EEG sensor network. We analyze the resulting trade-offs between transmission load and task accuracy.
</details>
<details>
<summary>摘要</summary>
我们提出了一种动态感知选择方法，用于深度神经网络（DNN），可以在每个特定输入样本上选择最佳感知subset，而不是整个数据集中固定的选择。这种动态选择与任务模型在综合的方式中同时学习，使用Gumbel-Softmax技巧，以使得柔性决策可以通过标准反馈来学习。然后，我们介绍了如何使用这种动态选择来增加无线传感器网络（WSN）的寿命，通过限制每个节点发送的次数。此外，我们还提高了性能，通过包括动态空间筛选器，使任务-DNN更加抗性能于面临多个可能的节点subset。最后，我们解释了如何选择优化的通道。我们验证了这种方法，使用了真实的电enzephalography（EEG）感知数据，模拟了EEG感知网络。我们分析了结果的平衡 Trade-offs between transmission load和任务准确率。
</details></li>
</ul>
<hr>
<h2 id="PDPK-A-Framework-to-Synthesise-Process-Data-and-Corresponding-Procedural-Knowledge-for-Manufacturing"><a href="#PDPK-A-Framework-to-Synthesise-Process-Data-and-Corresponding-Procedural-Knowledge-for-Manufacturing" class="headerlink" title="PDPK: A Framework to Synthesise Process Data and Corresponding Procedural Knowledge for Manufacturing"></a>PDPK: A Framework to Synthesise Process Data and Corresponding Procedural Knowledge for Manufacturing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08371">http://arxiv.org/abs/2308.08371</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/0x14d/embedding-operator-knowledge">https://github.com/0x14d/embedding-operator-knowledge</a></li>
<li>paper_authors: Richard Nordsieck, André Schweizer, Michael Heider, Jörg Hähner<br>for: 本研究的目的是提供一个框架，可以生成基于不同领域的 sintetic 数据集，以便模拟实际中的程序�beroject knowledge。methods: 本研究使用的方法包括：(1) 基于 Resource Description Framework (RDF) 的知识 graphs 的设计，(2) 模拟 parametrisation  проце�cess，(3) 使用现有的嵌入方法来表示程序�beroject knowledge。results: 本研究的结果包括：(1) 一个可以适应不同领域的 sintetic 数据集，(2) 一个基于 RDF 的知识 graphs 的表示方法，(3) 一个可以评估嵌入方法的基本比较结果。<details>
<summary>Abstract</summary>
Procedural knowledge describes how to accomplish tasks and mitigate problems. Such knowledge is commonly held by domain experts, e.g. operators in manufacturing who adjust parameters to achieve quality targets. To the best of our knowledge, no real-world datasets containing process data and corresponding procedural knowledge are publicly available, possibly due to corporate apprehensions regarding the loss of knowledge advances. Therefore, we provide a framework to generate synthetic datasets that can be adapted to different domains. The design choices are inspired by two real-world datasets of procedural knowledge we have access to. Apart from containing representations of procedural knowledge in Resource Description Framework (RDF)-compliant knowledge graphs, the framework simulates parametrisation processes and provides consistent process data. We compare established embedding methods on the resulting knowledge graphs, detailing which out-of-the-box methods have the potential to represent procedural knowledge. This provides a baseline which can be used to increase the comparability of future work. Furthermore, we validate the overall characteristics of a synthesised dataset by comparing the results to those achievable on a real-world dataset. The framework and evaluation code, as well as the dataset used in the evaluation, are available open source.
</details>
<details>
<summary>摘要</summary>
“程序性知识”描述了如何完成任务和解决问题。这种知识通常由领域专家所拥有，例如制造业中的操作员，他们会调整参数以达到质量目标。据我们所知，没有公开可用的实际世界数据集，可能因为企业对知识前进的担忧。因此，我们提供了一个框架，可以生成可靠的Synthetic数据集，可以适应不同领域。这个框架基于我们可以获得的两个实际世界数据集的知识，并且模拟参数化过程，提供一致的处理数据。我们使用已有的嵌入方法对知识图进行评估，并详细介绍这些方法的潜在表现。此外，我们还验证了生成的数据集的总性特征，并与实际世界数据集进行比较。框架和评估代码以及使用于评估的数据集都可以公开获取。
</details></li>
</ul>
<hr>
<h2 id="Dual-Branch-Temperature-Scaling-Calibration-for-Long-Tailed-Recognition"><a href="#Dual-Branch-Temperature-Scaling-Calibration-for-Long-Tailed-Recognition" class="headerlink" title="Dual-Branch Temperature Scaling Calibration for Long-Tailed Recognition"></a>Dual-Branch Temperature Scaling Calibration for Long-Tailed Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08366">http://arxiv.org/abs/2308.08366</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jialin Guo, Zhenyu Wu, Zhiqiang Zhan, Yang Ji</li>
<li>for: 本研究旨在解决深度神经网络的调整问题，尤其是在面对长条形分布数据时，模型受到调整问题的影响，导致模型过度自信。</li>
<li>methods: 本研究使用温度扩展（TS）方法，设计了多支分支温度扩展模型（Dual-TS）， simultaneously considering the diversity of temperature parameters of different categories and the non-generalizability of temperature parameters for rare samples in minority classes.</li>
<li>results: 通过实验，我们示出了我们的模型在传统ECE和Esbin-ECE评价指标上均达到了顶尖性。<details>
<summary>Abstract</summary>
The calibration for deep neural networks is currently receiving widespread attention and research. Miscalibration usually leads to overconfidence of the model. While, under the condition of long-tailed distribution of data, the problem of miscalibration is more prominent due to the different confidence levels of samples in minority and majority categories, and it will result in more serious overconfidence. To address this problem, some current research have designed diverse temperature coefficients for different categories based on temperature scaling (TS) method. However, in the case of rare samples in minority classes, the temperature coefficient is not generalizable, and there is a large difference between the temperature coefficients of the training set and the validation set. To solve this challenge, this paper proposes a dual-branch temperature scaling calibration model (Dual-TS), which considers the diversities in temperature parameters of different categories and the non-generalizability of temperature parameters for rare samples in minority classes simultaneously. Moreover, we noticed that the traditional calibration evaluation metric, Excepted Calibration Error (ECE), gives a higher weight to low-confidence samples in the minority classes, which leads to inaccurate evaluation of model calibration. Therefore, we also propose Equal Sample Bin Excepted Calibration Error (Esbin-ECE) as a new calibration evaluation metric. Through experiments, we demonstrate that our model yields state-of-the-art in both traditional ECE and Esbin-ECE metrics.
</details>
<details>
<summary>摘要</summary>
Currently, 深度神经网络的准确性调整 receiving extensive attention and research. 不准确的情况可能导致模型过于自信。而在长条形分布的数据下，偏好类和少数类之间的样本准确性差异更为明显，这会导致更严重的过自信。为解决这个问题，一些当前的研究已经设计了不同类别的温度系数。然而，在罕见的少数类中的样本中，温度系数不一致，Validation集和训练集的温度系数存在大的差异。为解决这个挑战，本文提出了双支temperature scaling calibration模型（Dual-TS），该模型考虑了不同类别的温度参数的多样性以及少数类中样本的非一致性。此外，我们发现传统的准确性评价指标excepted Calibration Error（ECE）会将低信度的少数类样本更加重视，这会导致模型准确性的误估。因此，我们也提出了Equal Sample Bin Excepted Calibration Error（Esbin-ECE）作为一个新的准确性评价指标。经过实验，我们展示了我们的模型在传统ECE和Esbin-ECE指标下达到了国际一流的性能。
</details></li>
</ul>
<hr>
<h2 id="KernelWarehouse-Towards-Parameter-Efficient-Dynamic-Convolution"><a href="#KernelWarehouse-Towards-Parameter-Efficient-Dynamic-Convolution" class="headerlink" title="KernelWarehouse: Towards Parameter-Efficient Dynamic Convolution"></a>KernelWarehouse: Towards Parameter-Efficient Dynamic Convolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08361">http://arxiv.org/abs/2308.08361</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/osvai/kernelwarehouse">https://github.com/osvai/kernelwarehouse</a></li>
<li>paper_authors: Chao Li, Anbang Yao</li>
<li>for: 本研究旨在提高动态核函数的表现，并提出了一种更通用的动态核函数模型——$KernelWarehouse$.</li>
<li>methods: 本研究使用了一种新的基本思路，即通过减少核 dimension 和增加核数来提高动态核函数的表现。这种方法通过策略性地分割核和共享库藏来增强层内参数的相互关系和逻辑相互作用，从而提供更多的自由度来适应不同的参数预算。</li>
<li>results: 研究人员通过对 ImageNet 和 MS-COCO 数据集使用不同的 ConvNet 架构，并使用 KernelWarehouse 模型，达到了当前最佳的结果。例如，使用 KernelWarehouse 模型在 ImageNet 上训练 ResNet18|ResNet50|MobileNetV2|ConvNeXt-Tiny 模型，可以达到 76.05%|81.05%|75.52%|82.51% 的 top-1 准确率。此外，由于 KernelWarehouse 的灵活设计，可以将 ConvNet 模型的大小减小，同时提高准确率，例如，我们的 ResNet18 模型在 36.45%|65.10% 参数减少后，对比基eline 模型，可以提高 2.89%|2.29% 的绝对准确率。<details>
<summary>Abstract</summary>
Dynamic convolution learns a linear mixture of $n$ static kernels weighted with their sample-dependent attentions, demonstrating superior performance compared to normal convolution. However, existing designs are parameter-inefficient: they increase the number of convolutional parameters by $n$ times. This and the optimization difficulty lead to no research progress in dynamic convolution that can allow us to use a significant large value of $n$ (e.g., $n>100$ instead of typical setting $n<10$) to push forward the performance boundary. In this paper, we propose $KernelWarehouse$, a more general form of dynamic convolution, which can strike a favorable trade-off between parameter efficiency and representation power. Its key idea is to redefine the basic concepts of "$kernels$" and "$assembling$ $kernels$" in dynamic convolution from the perspective of reducing kernel dimension and increasing kernel number significantly. In principle, KernelWarehouse enhances convolutional parameter dependencies within the same layer and across successive layers via tactful kernel partition and warehouse sharing, yielding a high degree of freedom to fit a desired parameter budget. We validate our method on ImageNet and MS-COCO datasets with different ConvNet architectures, and show that it attains state-of-the-art results. For instance, the ResNet18|ResNet50|MobileNetV2|ConvNeXt-Tiny model trained with KernelWarehouse on ImageNet reaches 76.05%|81.05%|75.52%|82.51% top-1 accuracy. Thanks to its flexible design, KernelWarehouse can even reduce the model size of a ConvNet while improving the accuracy, e.g., our ResNet18 model with 36.45%|65.10% parameter reduction to the baseline shows 2.89%|2.29% absolute improvement to top-1 accuracy.
</details>
<details>
<summary>摘要</summary>
“动态核函数学习一种线性混合的 $n$ 个静止核函数，显示出比普通核函数更高的性能。然而，现有设计是参数不充分利用的：它们将核函数参数数量增加 $n$ 倍。这和优化困难导致无法进行大量 $n$ 的研究进步，而 $n$ 通常设置在 10 左右。在这篇论文中，我们提出了 $KernelWarehouse$，一种更通用的动态核函数设计，可以在参数效率和表示力之间做出一个平衡。它的关键思想是通过重新定义动态核函数中 "$核函数" 和 "$核函数组合" 的概念，从减少核函数维度和增加核函数数量的角度来提高卷积参数的相互依赖性和层之间的参数共享，从而获得一定的参数预算的自由度。我们在 ImageNet 和 MS-COCO 数据集上验证了我们的方法，并显示其可以达到领先的Result。例如，我们在 ImageNet 上使用 KernelWarehouse 训练 ResNet18|ResNet50|MobileNetV2|ConvNeXt-Tiny 模型，达到 76.05%|81.05%|75.52%|82.51% 的 top-1 准确率。另外，由于 KernelWarehouse 的灵活设计，它可以减少 ConvNet 模型的大小，同时提高准确率，例如，我们的 ResNet18 模型，在参数减少 36.45%|65.10% 后，对比基eline的准确率提高 2.89%|2.29%。”
</details></li>
</ul>
<hr>
<h2 id="Independent-Distribution-Regularization-for-Private-Graph-Embedding"><a href="#Independent-Distribution-Regularization-for-Private-Graph-Embedding" class="headerlink" title="Independent Distribution Regularization for Private Graph Embedding"></a>Independent Distribution Regularization for Private Graph Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08360">http://arxiv.org/abs/2308.08360</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hkust-knowcomp/privategraphencoder">https://github.com/hkust-knowcomp/privategraphencoder</a></li>
<li>paper_authors: Qi Hu, Yangqiu Song</li>
<li>for: 本研究旨在提出一种名为Private Variational Graph AutoEncoders（PVGAE）的新方法，以保护图像数据的隐私性。</li>
<li>methods: PVGAE使用了独立分布罚款来规范各个编码器之间的独立性，并通过在训练过程中添加独立分布罚款来保证隐私性。</li>
<li>results: 实验结果表明，PVGAE在三个实际数据集上的性能和隐私保护性都比基eline方法更高。<details>
<summary>Abstract</summary>
Learning graph embeddings is a crucial task in graph mining tasks. An effective graph embedding model can learn low-dimensional representations from graph-structured data for data publishing benefiting various downstream applications such as node classification, link prediction, etc. However, recent studies have revealed that graph embeddings are susceptible to attribute inference attacks, which allow attackers to infer private node attributes from the learned graph embeddings. To address these concerns, privacy-preserving graph embedding methods have emerged, aiming to simultaneously consider primary learning and privacy protection through adversarial learning. However, most existing methods assume that representation models have access to all sensitive attributes in advance during the training stage, which is not always the case due to diverse privacy preferences. Furthermore, the commonly used adversarial learning technique in privacy-preserving representation learning suffers from unstable training issues. In this paper, we propose a novel approach called Private Variational Graph AutoEncoders (PVGAE) with the aid of independent distribution penalty as a regularization term. Specifically, we split the original variational graph autoencoder (VGAE) to learn sensitive and non-sensitive latent representations using two sets of encoders. Additionally, we introduce a novel regularization to enforce the independence of the encoders. We prove the theoretical effectiveness of regularization from the perspective of mutual information. Experimental results on three real-world datasets demonstrate that PVGAE outperforms other baselines in private embedding learning regarding utility performance and privacy protection.
</details>
<details>
<summary>摘要</summary>
学习图像抽象是图像挖掘任务中关键的任务。一个有效的图像抽象模型可以从图像结构数据中学习低维度表示，为数据发布带来多个下游应用程序的 beneficial effects，如节点分类、链接预测等。然而， latest studies have shown that graph embeddings are vulnerable to attribute inference attacks, which allow attackers to infer private node attributes from the learned graph embeddings. To address these concerns, privacy-preserving graph embedding methods have emerged, aiming to simultaneously consider primary learning and privacy protection through adversarial learning. However, most existing methods assume that representation models have access to all sensitive attributes in advance during the training stage, which is not always the case due to diverse privacy preferences. Furthermore, the commonly used adversarial learning technique in privacy-preserving representation learning suffers from unstable training issues. In this paper, we propose a novel approach called Private Variational Graph AutoEncoders (PVGAE) with the aid of independent distribution penalty as a regularization term. Specifically, we split the original variational graph autoencoder (VGAE) to learn sensitive and non-sensitive latent representations using two sets of encoders. Additionally, we introduce a novel regularization to enforce the independence of the encoders. We prove the theoretical effectiveness of regularization from the perspective of mutual information. Experimental results on three real-world datasets demonstrate that PVGAE outperforms other baselines in private embedding learning regarding utility performance and privacy protection.
</details></li>
</ul>
<hr>
<h2 id="Convergence-of-Two-Layer-Regression-with-Nonlinear-Units"><a href="#Convergence-of-Two-Layer-Regression-with-Nonlinear-Units" class="headerlink" title="Convergence of Two-Layer Regression with Nonlinear Units"></a>Convergence of Two-Layer Regression with Nonlinear Units</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08358">http://arxiv.org/abs/2308.08358</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yichuan Deng, Zhao Song, Shenghao Xie</li>
<li>for: 这个论文主要是为了解决一个基于Softmax和ReLU单元的归一化问题。</li>
<li>methods: 作者提出了一种基于粗略新颖方法的搜索算法，用于解决这个归一化问题。</li>
<li>results: 作者计算了一个准确的表示形式 для损失函数的资深特征，并证明了这个表示形式下的梯度是 lipschitz 连续和归一化的。<details>
<summary>Abstract</summary>
Large language models (LLMs), such as ChatGPT and GPT4, have shown outstanding performance in many human life task. Attention computation plays an important role in training LLMs. Softmax unit and ReLU unit are the key structure in attention computation. Inspired by them, we put forward a softmax ReLU regression problem. Generally speaking, our goal is to find an optimal solution to the regression problem involving the ReLU unit. In this work, we calculate a close form representation for the Hessian of the loss function. Under certain assumptions, we prove the Lipschitz continuous and the PSDness of the Hessian. Then, we introduce an greedy algorithm based on approximate Newton method, which converges in the sense of the distance to optimal solution. Last, We relax the Lipschitz condition and prove the convergence in the sense of loss value.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM），如ChatGPT和GPT4，在许多人类生活任务中表现出色。注意计算在训练 LLM 中扮演重要角色。Softmax单元和ReLU单元是关键结构在注意计算中。受到他们的灵感，我们提出了Softmax ReLU  regression问题。通过该问题，我们的目标是找到一个最佳解决方案。在这个工作中，我们计算了一个圆滑函数表示的条件下的条件下的条件下的Hessian数值。在某些假设下，我们证明了Hessian 的 lipschitz 连续和对偶性。接着，我们引入了一个类似于扩展新тон方法的探索算法，它在几乎所有情况下将 convergence 到最佳解决方案。最后，我们宽松了 lipschitz 条件，并证明了在损失值方面的对应。
</details></li>
</ul>
<hr>
<h2 id="Is-Meta-Learning-the-Right-Approach-for-the-Cold-Start-Problem-in-Recommender-Systems"><a href="#Is-Meta-Learning-the-Right-Approach-for-the-Cold-Start-Problem-in-Recommender-Systems" class="headerlink" title="Is Meta-Learning the Right Approach for the Cold-Start Problem in Recommender Systems?"></a>Is Meta-Learning the Right Approach for the Cold-Start Problem in Recommender Systems?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08354">http://arxiv.org/abs/2308.08354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davide Buffelli, Ashish Gupta, Agnieszka Strzalka, Vassilis Plachouras</li>
<li>for: 本研究旨在解决现代 Онлайн产品和服务中的推荐系统中的冷启动问题。</li>
<li>methods: 本研究使用了标准的深度学习模型，并证明了这些模型可以在冷启动 setting中达到同等或更高的性能。</li>
<li>results: 研究表明，当正确地调整深度学习模型时，它们可以与更新的 meta-learning 模型相比，在常用的benchmark上达到同等或更高的性能。此外，研究还表明了一种非常简单的模块化方法，可以在实际应用中更易地实现。<details>
<summary>Abstract</summary>
Recommender systems have become fundamental building blocks of modern online products and services, and have a substantial impact on user experience. In the past few years, deep learning methods have attracted a lot of research, and are now heavily used in modern real-world recommender systems. Nevertheless, dealing with recommendations in the cold-start setting, e.g., when a user has done limited interactions in the system, is a problem that remains far from solved. Meta-learning techniques, and in particular optimization-based meta-learning, have recently become the most popular approaches in the academic research literature for tackling the cold-start problem in deep learning models for recommender systems. However, current meta-learning approaches are not practical for real-world recommender systems, which have billions of users and items, and strict latency requirements. In this paper we show that it is possible to obtaining similar, or higher, performance on commonly used benchmarks for the cold-start problem without using meta-learning techniques. In more detail, we show that, when tuned correctly, standard and widely adopted deep learning models perform just as well as newer meta-learning models. We further show that an extremely simple modular approach using common representation learning techniques, can perform comparably to meta-learning techniques specifically designed for the cold-start setting while being much more easily deployable in real-world applications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Graph-Out-of-Distribution-Generalization-with-Controllable-Data-Augmentation"><a href="#Graph-Out-of-Distribution-Generalization-with-Controllable-Data-Augmentation" class="headerlink" title="Graph Out-of-Distribution Generalization with Controllable Data Augmentation"></a>Graph Out-of-Distribution Generalization with Controllable Data Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08344">http://arxiv.org/abs/2308.08344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bin Lu, Xiaoying Gan, Ze Zhao, Shiyu Liang, Luoyi Fu, Xinbing Wang, Chenghu Zhou<br>for:  This paper aims to address the issue of distribution deviation in graph neural network (GNN) training, specifically the problem of hybrid structure distribution shift, which can lead to spurious correlations and degrade the performance of GNN methods.methods:  The proposed method, called \texttt{OOD-GMixup}, uses controllable data augmentation in the metric space to manipulate the training distribution and alleviate the distribution deviation problem. Specifically, the method involves extracting graph rationales to eliminate spurious correlations, generating virtual samples with perturbation on the graph rationale representation domain, and using OOD calibration to measure the distribution deviation of virtual samples.results:  The proposed method outperforms state-of-the-art baselines on several real-world datasets for graph classification tasks, demonstrating its effectiveness in addressing the distribution deviation problem in GNN training.<details>
<summary>Abstract</summary>
Graph Neural Network (GNN) has demonstrated extraordinary performance in classifying graph properties. However, due to the selection bias of training and testing data (e.g., training on small graphs and testing on large graphs, or training on dense graphs and testing on sparse graphs), distribution deviation is widespread. More importantly, we often observe \emph{hybrid structure distribution shift} of both scale and density, despite of one-sided biased data partition. The spurious correlations over hybrid distribution deviation degrade the performance of previous GNN methods and show large instability among different datasets. To alleviate this problem, we propose \texttt{OOD-GMixup} to jointly manipulate the training distribution with \emph{controllable data augmentation} in metric space. Specifically, we first extract the graph rationales to eliminate the spurious correlations due to irrelevant information. Secondly, we generate virtual samples with perturbation on graph rationale representation domain to obtain potential OOD training samples. Finally, we propose OOD calibration to measure the distribution deviation of virtual samples by leveraging Extreme Value Theory, and further actively control the training distribution by emphasizing the impact of virtual OOD samples. Extensive studies on several real-world datasets on graph classification demonstrate the superiority of our proposed method over state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
图 нейрон网络（GNN）在分类图属性方面表现出色，但由于训练和测试数据的选择偏见（例如训练小图测试大图，或训练密集图测试稀疏图），流行范围偏差。更重要的是，我们经常观察到 hybrid 结构分布偏移，即一种同时具有规模和密度偏移的分布偏移，尽管数据分区是一种一侧偏见。这种偏移导致过去的 GNN 方法表现不稳定，并且在不同的数据集上显示出大的变化。为解决这个问题，我们提出了 \texttt{OOD-GMixup}，它通过在度量空间进行可控的数据增强来同时 manipulate 训练分布。具体来说，我们首先提取图理由来消除由无关信息引起的假 positives。然后，我们通过对图理由表示域进行扰动来生成可能的 OOD 训练样本。最后，我们提出了 OOD 校准，通过沿用极值理论来测量虚拟 OOD 样本的分布偏移，并且通过强调虚拟 OOD 样本的影响来活动控制训练分布。我们在多个实际 dataset 上进行了广泛的研究，并证明了我们的提议方法的超越性。
</details></li>
</ul>
<hr>
<h2 id="Learning-Logic-Programs-by-Discovering-Higher-Order-Abstractions"><a href="#Learning-Logic-Programs-by-Discovering-Higher-Order-Abstractions" class="headerlink" title="Learning Logic Programs by Discovering Higher-Order Abstractions"></a>Learning Logic Programs by Discovering Higher-Order Abstractions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08334">http://arxiv.org/abs/2308.08334</a></li>
<li>repo_url: None</li>
<li>paper_authors: Céline Hocquette, Sebastijan Dumančić, Andrew Cropper</li>
<li>for: 本研究旨在找到人类水平的AI需要的新抽象，以提高预测精度和学习效率。</li>
<li>methods: 本研究使用逻辑编程，从示例和背景知识中逻辑程序的induction。我们引入高阶 refactoring 问题，寻找可以压缩逻辑程序的高阶抽象。</li>
<li>results: 我们在多个领域，包括程序生成和视觉理解，进行了实验。结果显示，相比无 refactoring，STEVIE可以提高预测精度27%，降低学习时间47%。此外，STEVIE还可以找到可以在不同领域中传递的抽象。<details>
<summary>Abstract</summary>
Discovering novel abstractions is important for human-level AI. We introduce an approach to discover higher-order abstractions, such as map, filter, and fold. We focus on inductive logic programming, which induces logic programs from examples and background knowledge. We introduce the higher-order refactoring problem, where the goal is to compress a logic program by introducing higher-order abstractions. We implement our approach in STEVIE, which formulates the higher-order refactoring problem as a constraint optimisation problem. Our experimental results on multiple domains, including program synthesis and visual reasoning, show that, compared to no refactoring, STEVIE can improve predictive accuracies by 27% and reduce learning times by 47%. We also show that STEVIE can discover abstractions that transfer to different domains
</details>
<details>
<summary>摘要</summary>
发现新的抽象是人类水平AI的关键。我们介绍了一种方法，用于发现更高级别的抽象，如地图、筛选和折叠。我们关注了逻辑编程，它从示例和背景知识中生成逻辑程序。我们定义了更高级别的重构问题，其目标是通过引入更高级别的抽象来压缩逻辑程序。我们在STEVIE中实现了我们的方法，它将重构问题转化为约束优化问题。我们的实验结果在多个领域，包括程序生成和视觉理解，表明，相比无 refactoring，STEVIE可以提高预测精度 by 27%，并将学习时间减少 by 47%。我们还表明STEVIE可以找到可以在不同领域传递的抽象。
</details></li>
</ul>
<hr>
<h2 id="Warped-geometric-information-on-the-optimisation-of-Euclidean-functions"><a href="#Warped-geometric-information-on-the-optimisation-of-Euclidean-functions" class="headerlink" title="Warped geometric information on the optimisation of Euclidean functions"></a>Warped geometric information on the optimisation of Euclidean functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08305">http://arxiv.org/abs/2308.08305</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcelo Hartmann, Bernardo Williams, Hanlin Yu, Mark Girolami, Alessandro Barp, Arto Klami</li>
<li>for: 优化一个在高维欧几何空间中定义的实数函数，如机器学习任务中的损函数或统计推断中的LOG probability distribution。</li>
<li>methods: 使用扭曲的里曼尼安geometry的概念来重新定义欧几何空间上的函数优化问题，然后在拓扑学上找到函数的最优点。</li>
<li>results: 使用第三阶 Taylor级别的地odesic曲线approximation来提高优化效率，并且比标准欧几何Gradient-basedcounterparts更快 converging。<details>
<summary>Abstract</summary>
We consider the fundamental task of optimizing a real-valued function defined in a potentially high-dimensional Euclidean space, such as the loss function in many machine-learning tasks or the logarithm of the probability distribution in statistical inference. We use the warped Riemannian geometry notions to redefine the optimisation problem of a function on Euclidean space to a Riemannian manifold with a warped metric, and then find the function's optimum along this manifold. The warped metric chosen for the search domain induces a computational friendly metric-tensor for which optimal search directions associate with geodesic curves on the manifold becomes easier to compute. Performing optimization along geodesics is known to be generally infeasible, yet we show that in this specific manifold we can analytically derive Taylor approximations up to third-order. In general these approximations to the geodesic curve will not lie on the manifold, however we construct suitable retraction maps to pull them back onto the manifold. Therefore, we can efficiently optimize along the approximate geodesic curves. We cover the related theory, describe a practical optimization algorithm and empirically evaluate it on a collection of challenging optimisation benchmarks. Our proposed algorithm, using third-order approximation of geodesics, outperforms standard Euclidean gradient-based counterparts in term of number of iterations until convergence and an alternative method for Hessian-based optimisation routines.
</details>
<details>
<summary>摘要</summary>
我们考虑了在高维欧几学空间中优化实数函数的基本任务，例如机器学习中的损函数或统计推断中的Logarithm的概率分布。我们使用扭曲的里曼尼geometry来重新定义在欧几学空间上的函数优化问题，然后在这个拓扑上寻找函数的最优点。选择的扭曲度metric在搜索区域上引入了一个计算友好的度量牛顿，使得优化问题的解决变得更加容易。尽管在欧几学空间上进行优化是不可能的，但我们可以 analytically derived Taylor approximation up to third-order。这些approximation不会在拓扑上，但我们可以构建适当的Retraction map pull them back onto the manifold。因此，我们可以高效地优化third-order approximation of geodesics。我们涵盖相关理论，描述了一种实用的优化算法，并对一组具有挑战性的优化benchmark进行了实验性评估。我们的提议的算法，使用第三阶 Taylor approximation of geodesics，在迭代次数 Until convergence和一种基于Hessian的优化方法之间占据了优势。
</details></li>
</ul>
<hr>
<h2 id="Robust-Bayesian-Satisficing"><a href="#Robust-Bayesian-Satisficing" class="headerlink" title="Robust Bayesian Satisficing"></a>Robust Bayesian Satisficing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08291">http://arxiv.org/abs/2308.08291</a></li>
<li>repo_url: None</li>
<li>paper_authors: Artun Saday, Yaşar Cahit Yıldırım, Cem Tekin</li>
<li>for: 本研究旨在解决当代机器学习中存在分布变化的问题，以实现Robust Satisficing（RS）的目标。</li>
<li>methods: 本文提出了一种基于 bayesian 优化的 robust Bayesian satisficing 算法（RoBOS），用于帮助解决上述问题。</li>
<li>results: 本研究显示，RoBOS 算法可以在不同的学习问题中具有抑 Linear 的宽松偏误，并且可以独立于分布变化的量来控制偏误。<details>
<summary>Abstract</summary>
Distributional shifts pose a significant challenge to achieving robustness in contemporary machine learning. To overcome this challenge, robust satisficing (RS) seeks a robust solution to an unspecified distributional shift while achieving a utility above a desired threshold. This paper focuses on the problem of RS in contextual Bayesian optimization when there is a discrepancy between the true and reference distributions of the context. We propose a novel robust Bayesian satisficing algorithm called RoBOS for noisy black-box optimization. Our algorithm guarantees sublinear lenient regret under certain assumptions on the amount of distribution shift. In addition, we define a weaker notion of regret called robust satisficing regret, in which our algorithm achieves a sublinear upper bound independent of the amount of distribution shift. To demonstrate the effectiveness of our method, we apply it to various learning problems and compare it to other approaches, such as distributionally robust optimization.
</details>
<details>
<summary>摘要</summary>
<SYS> translate("Distributional shifts pose a significant challenge to achieving robustness in contemporary machine learning. To overcome this challenge, robust satisficing (RS) seeks a robust solution to an unspecified distributional shift while achieving a utility above a desired threshold. This paper focuses on the problem of RS in contextual Bayesian optimization when there is a discrepancy between the true and reference distributions of the context. We propose a novel robust Bayesian satisficing algorithm called RoBOS for noisy black-box optimization. Our algorithm guarantees sublinear lenient regret under certain assumptions on the amount of distribution shift. In addition, we define a weaker notion of regret called robust satisficing regret, in which our algorithm achieves a sublinear upper bound independent of the amount of distribution shift. To demonstrate the effectiveness of our method, we apply it to various learning problems and compare it to other approaches, such as distributionally robust optimization.")</SYS>Here's the translation in Simplified Chinese:Distributional shifts pose a significant challenge to achieving robustness in contemporary machine learning. To overcome this challenge, robust satisficing (RS) seeks a robust solution to an unspecified distributional shift while achieving a utility above a desired threshold. This paper focuses on the problem of RS in contextual Bayesian optimization when there is a discrepancy between the true and reference distributions of the context. We propose a novel robust Bayesian satisficing algorithm called RoBOS for noisy black-box optimization. Our algorithm guarantees sublinear lenient regret under certain assumptions on the amount of distribution shift. In addition, we define a weaker notion of regret called robust satisficing regret, in which our algorithm achieves a sublinear upper bound independent of the amount of distribution shift. To demonstrate the effectiveness of our method, we apply it to various learning problems and compare it to other approaches, such as distributionally robust optimization.
</details></li>
</ul>
<hr>
<h2 id="DFedADMM-Dual-Constraints-Controlled-Model-Inconsistency-for-Decentralized-Federated-Learning"><a href="#DFedADMM-Dual-Constraints-Controlled-Model-Inconsistency-for-Decentralized-Federated-Learning" class="headerlink" title="DFedADMM: Dual Constraints Controlled Model Inconsistency for Decentralized Federated Learning"></a>DFedADMM: Dual Constraints Controlled Model Inconsistency for Decentralized Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08290">http://arxiv.org/abs/2308.08290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinglun Li, Li Shen, Guanghao Li, Quanjun Yin, Dacheng Tao</li>
<li>for: 提高 Federated Learning (FL) 中的通信负担问题，增进 Decentralized Federated Learning (DFL) 的性能。</li>
<li>methods: 提出了两种新的 DFL 算法：DFedADMM 和其改进版 DFedADMM-SAM，使用 primal-dual 优化 (ADMM) 和 Sharpness-Aware Minimization (SAM) 优化器来解决本地不一致和本地不同程度适应问题。</li>
<li>results: 在非对称 Setting 中，DFedADMM 和 DFedADMM-SAM 算法的渐进速度为 $\mathcal{O}\Big(\frac{1}{\sqrt{KT}+\frac{1}{KT(1-\psi)^2}\Big)$ 和 $\mathcal{O}\Big(\frac{1}{\sqrt{KT}+\frac{1}{KT(1-\psi)^2}+ \frac{1}{T^{3&#x2F;2}K^{1&#x2F;2}\Big)$，并且在 MNIST、CIFAR10 和 CIFAR100 数据集上实验证明了我们的算法在一致性和渐进速度方面与现有 SOTA 优化器相比具有显著优势。<details>
<summary>Abstract</summary>
To address the communication burden issues associated with federated learning (FL), decentralized federated learning (DFL) discards the central server and establishes a decentralized communication network, where each client communicates only with neighboring clients. However, existing DFL methods still suffer from two major challenges: local inconsistency and local heterogeneous overfitting, which have not been fundamentally addressed by existing DFL methods. To tackle these issues, we propose novel DFL algorithms, DFedADMM and its enhanced version DFedADMM-SAM, to enhance the performance of DFL. The DFedADMM algorithm employs primal-dual optimization (ADMM) by utilizing dual variables to control the model inconsistency raised from the decentralized heterogeneous data distributions. The DFedADMM-SAM algorithm further improves on DFedADMM by employing a Sharpness-Aware Minimization (SAM) optimizer, which uses gradient perturbations to generate locally flat models and searches for models with uniformly low loss values to mitigate local heterogeneous overfitting. Theoretically, we derive convergence rates of $\small \mathcal{O}\Big(\frac{1}{\sqrt{KT}+\frac{1}{KT(1-\psi)^2}\Big)$ and $\small \mathcal{O}\Big(\frac{1}{\sqrt{KT}+\frac{1}{KT(1-\psi)^2}+ \frac{1}{T^{3/2}K^{1/2}\Big)$ in the non-convex setting for DFedADMM and DFedADMM-SAM, respectively, where $1 - \psi$ represents the spectral gap of the gossip matrix. Empirically, extensive experiments on MNIST, CIFAR10 and CIFAR100 datesets demonstrate that our algorithms exhibit superior performance in terms of both generalization and convergence speed compared to existing state-of-the-art (SOTA) optimizers in DFL.
</details>
<details>
<summary>摘要</summary>
To address the communication burden issues associated with federated learning (FL), decentralized federated learning (DFL) discards the central server and establishes a decentralized communication network, where each client communicates only with neighboring clients. However, existing DFL methods still suffer from two major challenges: local inconsistency and local heterogeneous overfitting, which have not been fundamentally addressed by existing DFL methods. To tackle these issues, we propose novel DFL algorithms, DFedADMM and its enhanced version DFedADMM-SAM, to enhance the performance of DFL. The DFedADMM algorithm employs primal-dual optimization (ADMM) by utilizing dual variables to control the model inconsistency raised from the decentralized heterogeneous data distributions. The DFedADMM-SAM algorithm further improves on DFedADMM by employing a Sharpness-Aware Minimization (SAM) optimizer, which uses gradient perturbations to generate locally flat models and searches for models with uniformly low loss values to mitigate local heterogeneous overfitting. Theoretically, we derive convergence rates of $\small \mathcal{O}\Big(\frac{1}{\sqrt{KT}+\frac{1}{KT(1-\psi)^2}\Big)$ and $\small \mathcal{O}\Big(\frac{1}{\sqrt{KT}+\frac{1}{KT(1-\psi)^2}+ \frac{1}{T^{3/2}K^{1/2}\Big)$ in the non-convex setting for DFedADMM and DFedADMM-SAM, respectively, where $1 - \psi$ represents the spectral gap of the gossip matrix. Empirically, extensive experiments on MNIST, CIFAR10 and CIFAR100 datesets demonstrate that our algorithms exhibit superior performance in terms of both generalization and convergence speed compared to existing state-of-the-art (SOTA) optimizers in DFL.
</details></li>
</ul>
<hr>
<h2 id="CARE-A-Large-Scale-CT-Image-Dataset-and-Clinical-Applicable-Benchmark-Model-for-Rectal-Cancer-Segmentation"><a href="#CARE-A-Large-Scale-CT-Image-Dataset-and-Clinical-Applicable-Benchmark-Model-for-Rectal-Cancer-Segmentation" class="headerlink" title="CARE: A Large Scale CT Image Dataset and Clinical Applicable Benchmark Model for Rectal Cancer Segmentation"></a>CARE: A Large Scale CT Image Dataset and Clinical Applicable Benchmark Model for Rectal Cancer Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08283">http://arxiv.org/abs/2308.08283</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hantao Zhang, Weidong Guo, Chenyang Qiu, Shouhong Wan, Bingbing Zou, Wanqin Wang, Peiquan Jin</li>
<li>for:  Rectal cancer segmentation of CT images for timely clinical diagnosis, radiotherapy treatment, and follow-up.</li>
<li>methods:  A novel large-scale rectal cancer CT image dataset (CARE) with pixel-level annotations, and a novel medical cancer lesion segmentation benchmark model (U-SAM) that incorporates prompt information to tackle the challenges of intricate anatomical structures.</li>
<li>results:  The proposed U-SAM outperforms state-of-the-art methods on the CARE dataset and the WORD dataset, demonstrating its effectiveness in rectal cancer segmentation.<details>
<summary>Abstract</summary>
Rectal cancer segmentation of CT image plays a crucial role in timely clinical diagnosis, radiotherapy treatment, and follow-up. Although current segmentation methods have shown promise in delineating cancerous tissues, they still encounter challenges in achieving high segmentation precision. These obstacles arise from the intricate anatomical structures of the rectum and the difficulties in performing differential diagnosis of rectal cancer. Additionally, a major obstacle is the lack of a large-scale, finely annotated CT image dataset for rectal cancer segmentation. To address these issues, this work introduces a novel large scale rectal cancer CT image dataset CARE with pixel-level annotations for both normal and cancerous rectum, which serves as a valuable resource for algorithm research and clinical application development. Moreover, we propose a novel medical cancer lesion segmentation benchmark model named U-SAM. The model is specifically designed to tackle the challenges posed by the intricate anatomical structures of abdominal organs by incorporating prompt information. U-SAM contains three key components: promptable information (e.g., points) to aid in target area localization, a convolution module for capturing low-level lesion details, and skip-connections to preserve and recover spatial information during the encoding-decoding process. To evaluate the effectiveness of U-SAM, we systematically compare its performance with several popular segmentation methods on the CARE dataset. The generalization of the model is further verified on the WORD dataset. Extensive experiments demonstrate that the proposed U-SAM outperforms state-of-the-art methods on these two datasets. These experiments can serve as the baseline for future research and clinical application development.
</details>
<details>
<summary>摘要</summary>
CT 影像中的肛门癌分 segmentation 在临床诊断、放疗治疗和跟踪中扮演着关键的角色。 although current segmentation methods have shown promise in delineating cancerous tissues, they still encounter challenges in achieving high segmentation precision. These obstacles arise from the intricate anatomical structures of the rectum and the difficulties in performing differential diagnosis of rectal cancer. Additionally, a major obstacle is the lack of a large-scale, finely annotated CT image dataset for rectal cancer segmentation. To address these issues, this work introduces a novel large scale rectal cancer CT image dataset CARE with pixel-level annotations for both normal and cancerous rectum, which serves as a valuable resource for algorithm research and clinical application development. Moreover, we propose a novel medical cancer lesion segmentation benchmark model named U-SAM. The model is specifically designed to tackle the challenges posed by the intricate anatomical structures of abdominal organs by incorporating prompt information. U-SAM contains three key components: promptable information (e.g., points) to aid in target area localization, a convolution module for capturing low-level lesion details, and skip-connections to preserve and recover spatial information during the encoding-decoding process. To evaluate the effectiveness of U-SAM, we systematically compare its performance with several popular segmentation methods on the CARE dataset. The generalization of the model is further verified on the WORD dataset. Extensive experiments demonstrate that the proposed U-SAM outperforms state-of-the-art methods on these two datasets. These experiments can serve as the baseline for future research and clinical application development.Here's the text with some additional information about the Simplified Chinese translation:The Simplified Chinese translation is written in a formal and objective style, which is appropriate for a scientific or technical document. The translation uses proper nouns and phrases to convey the meaning of the original text accurately. The use of 大量 (dàliàng) to describe the dataset CARE emphasizes the scale of the dataset, while the use of 精心 (jīngxīn) to describe the annotations highlights the precision of the annotations. The translation also uses technical terms such as 肛门 (fèngmén) for "rectum" and 癌 (gān) for "cancer" to ensure accuracy in the medical context.Please note that the translation is provided as a courtesy and may not be perfect or entirely accurate. If you have any further questions or need more information, please feel free to ask!
</details></li>
</ul>
<hr>
<h2 id="It-Ain’t-That-Bad-Understanding-the-Mysterious-Performance-Drop-in-OOD-Generalization-for-Generative-Transformer-Models"><a href="#It-Ain’t-That-Bad-Understanding-the-Mysterious-Performance-Drop-in-OOD-Generalization-for-Generative-Transformer-Models" class="headerlink" title="It Ain’t That Bad: Understanding the Mysterious Performance Drop in OOD Generalization for Generative Transformer Models"></a>It Ain’t That Bad: Understanding the Mysterious Performance Drop in OOD Generalization for Generative Transformer Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08268">http://arxiv.org/abs/2308.08268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingcheng Xu, Zihao Pan, Haipeng Zhang, Yanqing Yang</li>
<li>for: 研究基于Transformer模型的生成模型在解决多种问题上表现出色，但其泛化能力还不够了解。</li>
<li>methods: 研究人员通过使用基本数学任务，如n进位加法或乘法，来研究这些模型的泛化行为。</li>
<li>results: 研究发现，当训练n进位操作（如加法）时，模型在未seen n进位输入上能够成功泛化（ID泛化），但在长度更长的未seen输入上表现很差（OOD泛化）。研究人员尝试使用工具such as修改位嵌入、细化和priming来bridging这个差距，但这些解决方案并没有解决核心机制，因此无法保证其稳定性。<details>
<summary>Abstract</summary>
Generative Transformer-based models have achieved remarkable proficiency on solving diverse problems. However, their generalization ability is not fully understood and not always satisfying. Researchers take basic mathematical tasks like n-digit addition or multiplication as important perspectives for investigating their generalization behaviors. Curiously, it is observed that when training on n-digit operations (e.g., additions) in which both input operands are n-digit in length, models generalize successfully on unseen n-digit inputs (in-distribution (ID) generalization), but fail miserably and mysteriously on longer, unseen cases (out-of-distribution (OOD) generalization). Studies try to bridge this gap with workarounds such as modifying position embedding, fine-tuning, and priming with more extensive or instructive data. However, without addressing the essential mechanism, there is hardly any guarantee regarding the robustness of these solutions. We bring this unexplained performance drop into attention and ask whether it is purely from random errors. Here we turn to the mechanistic line of research which has notable successes in model interpretability. We discover that the strong ID generalization stems from structured representations, while behind the unsatisfying OOD performance, the models still exhibit clear learned algebraic structures. Specifically, these models map unseen OOD inputs to outputs with equivalence relations in the ID domain. These highlight the potential of the models to carry useful information for improved generalization.
</details>
<details>
<summary>摘要</summary>
transformer-based models have achieved remarkable proficiency in solving diverse problems, but their generalization ability is not fully understood and is not always satisfying. researchers take basic mathematical tasks like n-digit addition or multiplication as important perspectives for investigating their generalization behaviors. curiously, it is observed that when training on n-digit operations (e.g., additions) in which both input operands are n-digit in length, models generalize successfully on unseen n-digit inputs (in-distribution (ID) generalization), but fail miserably and mysteriously on longer, unseen cases (out-of-distribution (OOD) generalization). studies try to bridge this gap with workarounds such as modifying position embedding, fine-tuning, and priming with more extensive or instructive data. however, without addressing the essential mechanism, there is hardly any guarantee regarding the robustness of these solutions. we bring this unexplained performance drop into attention and ask whether it is purely from random errors. here we turn to the mechanistic line of research which has notable successes in model interpretability. we discover that the strong ID generalization stems from structured representations, while behind the unsatisfying OOD performance, the models still exhibit clear learned algebraic structures. specifically, these models map unseen OOD inputs to outputs with equivalence relations in the ID domain. these highlight the potential of the models to carry useful information for improved generalization.
</details></li>
</ul>
<hr>
<h2 id="Graph-Relation-Aware-Continual-Learning"><a href="#Graph-Relation-Aware-Continual-Learning" class="headerlink" title="Graph Relation Aware Continual Learning"></a>Graph Relation Aware Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08259">http://arxiv.org/abs/2308.08259</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinghua Shen, Weijieying Ren, Wei Qin</li>
<li>for: 这个论文研究了从无穷流图数据中学习，总结历史知识，并将其泛化到未来任务。在这个任务中，只有当前图数据可用。</li>
<li>methods: 这个论文提出了一种关注图边的隐藏关系，并设计了一种可变模型RAM-CG，包括一个用于探索图边隐藏关系的模块和一个用于考虑时间推移的masking类ifier。</li>
<li>results: 对于CitationNet、OGBN-arxiv和TWITCH dataset，RAM-CG模型比现有最佳实现提供了2.2%、6.9%和6.6%的准确率提升。<details>
<summary>Abstract</summary>
Continual graph learning (CGL) studies the problem of learning from an infinite stream of graph data, consolidating historical knowledge, and generalizing it to the future task. At once, only current graph data are available. Although some recent attempts have been made to handle this task, we still face two potential challenges: 1) most of existing works only manipulate on the intermediate graph embedding and ignore intrinsic properties of graphs. It is non-trivial to differentiate the transferred information across graphs. 2) recent attempts take a parameter-sharing policy to transfer knowledge across time steps or progressively expand new architecture given shifted graph distribution. Learning a single model could loss discriminative information for each graph task while the model expansion scheme suffers from high model complexity. In this paper, we point out that latent relations behind graph edges can be attributed as an invariant factor for the evolving graphs and the statistical information of latent relations evolves. Motivated by this, we design a relation-aware adaptive model, dubbed as RAM-CG, that consists of a relation-discovery modular to explore latent relations behind edges and a task-awareness masking classifier to accounts for the shifted. Extensive experiments show that RAM-CG provides significant 2.2%, 6.9% and 6.6% accuracy improvements over the state-of-the-art results on CitationNet, OGBN-arxiv and TWITCH dataset, respective.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们指出了图像边缘下的隐藏关系可以作为不变因素，并且这些隐藏关系的统计信息在演化过程中发展。基于这一点，我们设计了一种相关性感知的自适应模型，即RAM-CG，它包括一个用于探索隐藏关系的关系探索模块和一个用于考虑时间步骤或graph distribution的变化的任务意识Masking类ifier。经过广泛的实验，我们发现RAM-CG可以提供2.2%、6.9%和6.6%的精度提高 compared to state-of-the-art results on CitationNet、OGBN-arxiv和TWITCH dataset，分别。
</details></li>
</ul>
<hr>
<h2 id="Two-Phases-of-Scaling-Laws-for-Nearest-Neighbor-Classifiers"><a href="#Two-Phases-of-Scaling-Laws-for-Nearest-Neighbor-Classifiers" class="headerlink" title="Two Phases of Scaling Laws for Nearest Neighbor Classifiers"></a>Two Phases of Scaling Laws for Nearest Neighbor Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08247">http://arxiv.org/abs/2308.08247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pengkun Yang, Jingzhao Zhang</li>
<li>for: 本研究旨在研究 nearest neighbor 分类器的扩展法则，即 Observation 的训练数据量增加后，模型的测试性能会提高。</li>
<li>methods: 本研究使用了 nearest neighbor 分类器，并对其进行了分析和研究。</li>
<li>results: 研究发现， nearest neighbor 分类器可以在一定的数据量范围内具有 polynomial 型的扩展法则，而在其他范围内具有 exponential 型的扩展法则。这种结果反映了数据分布的复杂性对模型的泛化误差的影响。<details>
<summary>Abstract</summary>
A scaling law refers to the observation that the test performance of a model improves as the number of training data increases. A fast scaling law implies that one can solve machine learning problems by simply boosting the data and the model sizes. Yet, in many cases, the benefit of adding more data can be negligible. In this work, we study the rate of scaling laws of nearest neighbor classifiers. We show that a scaling law can have two phases: in the first phase, the generalization error depends polynomially on the data dimension and decreases fast; whereas in the second phase, the error depends exponentially on the data dimension and decreases slowly. Our analysis highlights the complexity of the data distribution in determining the generalization error. When the data distributes benignly, our result suggests that nearest neighbor classifier can achieve a generalization error that depends polynomially, instead of exponentially, on the data dimension.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese)一个尺度法则指的是模型在训练数据量增加后测试性能的改进。一个快速尺度法则意味着可以通过简单地增加数据和模型大小解决机器学习问题。然而，在许多情况下，增加更多数据的好处很小。在这个工作中，我们研究 nearest neighbor 分类器的尺度法则。我们发现一个尺度法则可以有两个阶段：在第一阶段，泛化错误与数据维度之间存在 polynomial 关系，随着数据维度增加而快速下降；而在第二阶段，错误与数据维度之间存在 exponential 关系，随着数据维度增加而慢慢下降。我们的分析表明数据分布的复杂性对泛化错误的影响。当数据分布良好时，我们的结果表明 nearest neighbor 分类器可以在数据维度上取得 polynomial 类型的泛化错误，而不是 exponential 类型。
</details></li>
</ul>
<hr>
<h2 id="The-Expressive-Power-of-Graph-Neural-Networks-A-Survey"><a href="#The-Expressive-Power-of-Graph-Neural-Networks-A-Survey" class="headerlink" title="The Expressive Power of Graph Neural Networks: A Survey"></a>The Expressive Power of Graph Neural Networks: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08235">http://arxiv.org/abs/2308.08235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bingxu Zhang, Changjun Fan, Shixuan Liu, Kuihua Huang, Xiang Zhao, Jincai Huang, Zhong Liu</li>
<li>for: 本研究旨在探讨图aelastic networks（GNNs）的表达能力限制，包括图 isomorphism recognition 和 subgraph counting等问题。</li>
<li>methods: 本研究使用了多种方法来探讨 GNNs 的表达能力，包括 graph feature enhancement、graph topology enhancement 和 GNNs 架构增强等方法。</li>
<li>results: 本研究结果显示，GNNs 的表达能力有很多限制，但可以通过不同的方法来增强表达能力，如图 feature enhancement、graph topology enhancement 和 GNNs 架构增强等方法。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) are effective machine learning models for many graph-related applications. Despite their empirical success, many research efforts focus on the theoretical limitations of GNNs, i.e., the GNNs expressive power. Early works in this domain mainly focus on studying the graph isomorphism recognition ability of GNNs, and recent works try to leverage the properties such as subgraph counting and connectivity learning to characterize the expressive power of GNNs, which are more practical and closer to real-world. However, no survey papers and open-source repositories comprehensively summarize and discuss models in this important direction. To fill the gap, we conduct a first survey for models for enhancing expressive power under different forms of definition. Concretely, the models are reviewed based on three categories, i.e., Graph feature enhancement, Graph topology enhancement, and GNNs architecture enhancement.
</details>
<details>
<summary>摘要</summary>
格点网络（GNNs）是许多图形应用中的有效机器学习模型。尽管其实验成功，但许多研究努力集中在GNNs的理论局限性上，即GNNs表达能力。早期工作主要关注图同构识别能力，而最近的工作尝试利用子图计数和连接学习来描述GNNs的表达能力，这些方法更加实际和更加接近实际应用。然而，没有任何报告和开源库全面总结和讨论这个重要方向。为了填补这个空白，我们进行了首次的survey，检讨不同定义下GNNs表达能力的模型。具体来说，模型被评估基于三类划分，即图特征增强、图结构增强和GNNs架构增强。
</details></li>
</ul>
<hr>
<h2 id="Challenges-and-Opportunities-of-Using-Transformer-Based-Multi-Task-Learning-in-NLP-Through-ML-Lifecycle-A-Survey"><a href="#Challenges-and-Opportunities-of-Using-Transformer-Based-Multi-Task-Learning-in-NLP-Through-ML-Lifecycle-A-Survey" class="headerlink" title="Challenges and Opportunities of Using Transformer-Based Multi-Task Learning in NLP Through ML Lifecycle: A Survey"></a>Challenges and Opportunities of Using Transformer-Based Multi-Task Learning in NLP Through ML Lifecycle: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08234">http://arxiv.org/abs/2308.08234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lovre Torbarina, Tin Ferkovic, Lukasz Roguski, Velimir Mihelcic, Bruno Sarlija, Zeljko Kraljevic</li>
<li>for: 本研究旨在探讨使用多任务学习（MTL）方法在自然语言处理（NLP）领域中提高效率和性能。</li>
<li>methods: 本文首先提供了转换器基于MTL方法的概述，然后讨论了在NLP领域中MTL方法在数据工程、模型开发、部署和监控阶段的挑战和机遇。</li>
<li>results: 本文系统地分析了在NLP领域中使用转换器基于MTL方法如何适应ML生命周期阶段，并提出了在MTL和 continual learning（CL）之间的连接研究的可能性，以便在实际应用中更方便地定期重新训练模型、更新模型due to distribution shifts，并添加新功能来满足实际需求。<details>
<summary>Abstract</summary>
The increasing adoption of natural language processing (NLP) models across industries has led to practitioners' need for machine learning systems to handle these models efficiently, from training to serving them in production. However, training, deploying, and updating multiple models can be complex, costly, and time-consuming, mainly when using transformer-based pre-trained language models. Multi-Task Learning (MTL) has emerged as a promising approach to improve efficiency and performance through joint training, rather than training separate models. Motivated by this, we first provide an overview of transformer-based MTL approaches in NLP. Then, we discuss the challenges and opportunities of using MTL approaches throughout typical ML lifecycle phases, specifically focusing on the challenges related to data engineering, model development, deployment, and monitoring phases. This survey focuses on transformer-based MTL architectures and, to the best of our knowledge, is novel in that it systematically analyses how transformer-based MTL in NLP fits into ML lifecycle phases. Furthermore, we motivate research on the connection between MTL and continual learning (CL), as this area remains unexplored. We believe it would be practical to have a model that can handle both MTL and CL, as this would make it easier to periodically re-train the model, update it due to distribution shifts, and add new capabilities to meet real-world requirements.
</details>
<details>
<summary>摘要</summary>
随着自然语言处理（NLP）模型在不同领域的推广，机器学习（ML）实践者需要高效地处理这些模型，从训练到生产环境中部署。然而，训练、部署和更新多个模型可能会复杂、成本高和时间费时，尤其是使用基于转换器的预训练语言模型。多任务学习（MTL）已经作为一种有 Promise的方法，以提高效率和性能，通过共同训练而不是独立训练多个模型。我们首先提供了基于转换器的 MTL 方法在 NLP 领域的概述。然后，我们讨论了在 ML 生命周期阶段中使用 MTL 方法的挑战和机遇，尤其是在数据工程、模型开发、部署和监控阶段。本文主要关注基于转换器的 MTL 架构，并且，到我们所知，是在 ML 生命周期阶段中系统地分析了基于转换器的 MTL 在 NLP 领域的应用。此外，我们还鼓励了研究基于 MTL 和连续学习（CL）的连续关系，因为这个领域还没有得到充分的探索。我们认为，一个能够同时处理 MTL 和 CL 的模型会更加实用，因为这样可以更方便地在 periodic  retrained 模型，因应分布Shift 更新模型，并添加新的功能来满足实际需求。
</details></li>
</ul>
<hr>
<h2 id="SCQPTH-an-efficient-differentiable-splitting-method-for-convex-quadratic-programming"><a href="#SCQPTH-an-efficient-differentiable-splitting-method-for-convex-quadratic-programming" class="headerlink" title="SCQPTH: an efficient differentiable splitting method for convex quadratic programming"></a>SCQPTH: an efficient differentiable splitting method for convex quadratic programming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08232">http://arxiv.org/abs/2308.08232</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew Butler</li>
<li>for: 这篇论文是用来解决凸quadratic programs（QPs）的问题的。</li>
<li>methods: 这篇论文使用了alternating direction method of multipliers（ADMM）和operator splitting方法来解决QPs。</li>
<li>results: 实验结果显示，使用SCQPTH可以提供1-10倍的计算效率提升，相比于现有的可微QP解决方法。<details>
<summary>Abstract</summary>
We present SCQPTH: a differentiable first-order splitting method for convex quadratic programs. The SCQPTH framework is based on the alternating direction method of multipliers (ADMM) and the software implementation is motivated by the state-of-the art solver OSQP: an operating splitting solver for convex quadratic programs (QPs). The SCQPTH software is made available as an open-source python package and contains many similar features including efficient reuse of matrix factorizations, infeasibility detection, automatic scaling and parameter selection. The forward pass algorithm performs operator splitting in the dimension of the original problem space and is therefore suitable for large scale QPs with $100-1000$ decision variables and thousands of constraints. Backpropagation is performed by implicit differentiation of the ADMM fixed-point mapping. Experiments demonstrate that for large scale QPs, SCQPTH can provide a $1\times - 10\times$ improvement in computational efficiency in comparison to existing differentiable QP solvers.
</details>
<details>
<summary>摘要</summary>
我们介绍SCQPTH：一种可微的首项拆分法 для凸quadratic programs。SCQPTH框架基于多方向方法（ADMM），并受到现代QP解析器OSQP的驱动。SCQPTH软件作为一个开源python套件，具有许多相似特性，包括有效的续用矩阵因数、无法构成检测、自动尺度调整和参数选择。forward pass算法在原始问题空间的维度进行操作拆分，适用于大规模QP的解释，具有100-1000个决策变数和千个约束。回传算法使用凸拓扑ADMM定点映射的隐含对偶积分。实验表明，对于大规模QP，SCQPTH可以提供1倍-10倍的计算效率提升，与现有的可微QP解析器相比。
</details></li>
</ul>
<hr>
<h2 id="Self-Deception-Reverse-Penetrating-the-Semantic-Firewall-of-Large-Language-Models"><a href="#Self-Deception-Reverse-Penetrating-the-Semantic-Firewall-of-Large-Language-Models" class="headerlink" title="Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models"></a>Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11521">http://arxiv.org/abs/2308.11521</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenhua Wang, Wei Xie, Kai Chen, Baosheng Wang, Zhiwen Gui, Enze Wang<br>for: 这篇论文 investigate了LLM囚禁问题，并提出了一种自动囚禁方法。methods: 论文提出了三种实现方法，包括使用语义防火墙、自我误导攻击和隐藏攻击。results: 实验结果显示，在两种模型上，自动囚禁方法的成功率分别为86.2%和67%，失败率分别为4.7%和2.2%。<details>
<summary>Abstract</summary>
Large language models (LLMs), such as ChatGPT, have emerged with astonishing capabilities approaching artificial general intelligence. While providing convenience for various societal needs, LLMs have also lowered the cost of generating harmful content. Consequently, LLM developers have deployed semantic-level defenses to recognize and reject prompts that may lead to inappropriate content. Unfortunately, these defenses are not foolproof, and some attackers have crafted "jailbreak" prompts that temporarily hypnotize the LLM into forgetting content defense rules and answering any improper questions. To date, there is no clear explanation of the principles behind these semantic-level attacks and defenses in both industry and academia.   This paper investigates the LLM jailbreak problem and proposes an automatic jailbreak method for the first time. We propose the concept of a semantic firewall and provide three technical implementation approaches. Inspired by the attack that penetrates traditional firewalls through reverse tunnels, we introduce a "self-deception" attack that can bypass the semantic firewall by inducing LLM to generate prompts that facilitate jailbreak. We generated a total of 2,520 attack payloads in six languages (English, Russian, French, Spanish, Chinese, and Arabic) across seven virtual scenarios, targeting the three most common types of violations: violence, hate, and pornography. The experiment was conducted on two models, namely the GPT-3.5-Turbo and GPT-4. The success rates on the two models were 86.2% and 67%, while the failure rates were 4.7% and 2.2%, respectively. This highlighted the effectiveness of the proposed attack method. All experimental code and raw data will be released as open-source to inspire future research. We believe that manipulating AI behavior through carefully crafted prompts will become an important research direction in the future.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM），如ChatGPT，已经出现了惊人的能力，仅次于人工智能。这些模型可以为社会的不同需求提供便利，但也lowered the cost of generating harmful content。因此，LLM开发者们已经部署了semantic-level防御来识别和拒绝可能导致不当内容的提示。不过，这些防御并不是不可攻击的，有些攻击者已经crafted "jailbreak"提示，使LLM暂时忘记内容防御规则，回答任何不当问题。到目前为止，在业界和学术界都没有清楚的解释这些semantic-level攻击和防御的原则。本文investigates the LLM jailbreak problem，并提出了一个自动化jailbreak方法。我们提出了semantic firewall的概念，并提供了三种技术实现方法。受到传统防火墙被攻击的启发，我们引入了一种"自我欺骗"攻击，可以绕过semantic firewall，使LLM生成提示，促使jailbreak。我们在六种语言（英文、俄语、法语、西班牙语、中文和阿拉伯语）的七个虚拟enario中，总共生成了2,520个攻击payload。实验使用了GPT-3.5-Turbo和GPT-4两个模型，成功率为86.2%和67%，失败率为4.7%和2.2%。这诉求了我们的提案攻击方法的效果。我们将所有实验代码和原始数据发布为开源，以启发未来的研究。我们相信，通过精心设计的提示，可以操纵AI的行为，将成为未来的重要研究方向。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Winograd-Convolution-for-Cost-effective-Neural-Network-Fault-Tolerance"><a href="#Exploring-Winograd-Convolution-for-Cost-effective-Neural-Network-Fault-Tolerance" class="headerlink" title="Exploring Winograd Convolution for Cost-effective Neural Network Fault Tolerance"></a>Exploring Winograd Convolution for Cost-effective Neural Network Fault Tolerance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08230">http://arxiv.org/abs/2308.08230</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinghua Xue, Cheng Liu, Bo Liu, Haitong Huang, Ying Wang, Tao Luo, Lei Zhang, Huawei Li, Xiaowei Li</li>
<li>for: 本文研究了winograd核函数在神经网络中的 fault tolerance 特性，以及其与传统 fault-tolerant 设计方法的相互作用。</li>
<li>methods: 本文首次从不同的granularity水平（模型、层、操作类型）进行了winograd convolution fault tolerance的全面评估。并 explore了使用winograd convolution的自然fault tolerance来实现cost-effective的神经网络保护。</li>
<li>results:  experiments show that winograd convolution可以在 fault-tolerant neural networks 中减少设计开销by 55.77% 平均而无损失精度，并在考虑winograd convolution的自然fault tolerance时减少计算开销by 17.24%。在应用于具有多种 faults 的情况下，使用winograd convolution和 fault-aware retraining和 constrained activation functions 组合的模型精度显著提高。<details>
<summary>Abstract</summary>
Winograd is generally utilized to optimize convolution performance and computational efficiency because of the reduced multiplication operations, but the reliability issues brought by winograd are usually overlooked. In this work, we observe the great potential of winograd convolution in improving neural network (NN) fault tolerance. Based on the observation, we evaluate winograd convolution fault tolerance comprehensively from different granularities ranging from models, layers, and operation types for the first time. Then, we explore the use of inherent fault tolerance of winograd convolution for cost-effective NN protection against soft errors. Specifically, we mainly investigate how winograd convolution can be effectively incorporated with classical fault-tolerant design approaches including triple modular redundancy (TMR), fault-aware retraining, and constrained activation functions. According to our experiments, winograd convolution can reduce the fault-tolerant design overhead by 55.77\% on average without any accuracy loss compared to standard convolution, and further reduce the computing overhead by 17.24\% when the inherent fault tolerance of winograd convolution is considered. When it is applied on fault-tolerant neural networks enhanced with fault-aware retraining and constrained activation functions, the resulting model accuracy generally shows significant improvement in presence of various faults.
</details>
<details>
<summary>摘要</summary>
winograd通常用于优化卷积性能和计算效率，因为它减少了多个乘法操作，但winograd引入的可靠性问题通常被忽略。在这项工作中，我们发现winograd卷积可以大幅提高神经网络（NN）fault tolerance。基于这一观察，我们对winograd卷积的FAULT TOLERANCE进行了全面的评估，从不同的粒度（模型、层、操作类型）进行了首次评估。然后，我们探索了使用winograd卷积的内在fault tolerance来实现cost-effectiveNN保护 against soft errors。具体来说，我们主要研究了如何effectively incorporate winograd卷积与传统的 fault-tolerant设计方法，包括TMR、 fault-aware retraining和constrained activation functions。根据我们的实验，winograd卷积可以将FAULT-TOLERANT DESIGN overhead减少55.77%，而无需损失精度，并且在考虑winograd卷积的内在 fault tolerance时，可以减少计算 overhead的17.24%。当应用于强化了FAULT-TOLERANT NEURAL NETWORKS的模型中，模型的准确率在不同的FAULT Condition下都会表现出明显的改善。
</details></li>
</ul>
<hr>
<h2 id="Inherent-Redundancy-in-Spiking-Neural-Networks"><a href="#Inherent-Redundancy-in-Spiking-Neural-Networks" class="headerlink" title="Inherent Redundancy in Spiking Neural Networks"></a>Inherent Redundancy in Spiking Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08227">http://arxiv.org/abs/2308.08227</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/biclab/asa-snn">https://github.com/biclab/asa-snn</a></li>
<li>paper_authors: Man Yao, Jiakui Hu, Guangshe Zhao, Yaoyuan Wang, Ziyang Zhang, Bo Xu, Guoqi Li</li>
<li>for: 本研究旨在探讨逻辑神经网络（SNN）中的内在重复性，以提高逻辑神经网络在准确性和能效性方面的优势。</li>
<li>methods: 本研究使用了一种名为 Advance Spatial Attention（ASA）模块，该模块可以自适应优化SNN的膜电压分布，从而精准地控制噪声脉冲特征。</li>
<li>results: 实验结果表明，提出的方法可以显著降低脉冲发生率，并且在比较于现有SNN基elines的情况下，表现出较好的性能。<details>
<summary>Abstract</summary>
Spiking Neural Networks (SNNs) are well known as a promising energy-efficient alternative to conventional artificial neural networks. Subject to the preconceived impression that SNNs are sparse firing, the analysis and optimization of inherent redundancy in SNNs have been largely overlooked, thus the potential advantages of spike-based neuromorphic computing in accuracy and energy efficiency are interfered. In this work, we pose and focus on three key questions regarding the inherent redundancy in SNNs. We argue that the redundancy is induced by the spatio-temporal invariance of SNNs, which enhances the efficiency of parameter utilization but also invites lots of noise spikes. Further, we analyze the effect of spatio-temporal invariance on the spatio-temporal dynamics and spike firing of SNNs. Then, motivated by these analyses, we propose an Advance Spatial Attention (ASA) module to harness SNNs' redundancy, which can adaptively optimize their membrane potential distribution by a pair of individual spatial attention sub-modules. In this way, noise spike features are accurately regulated. Experimental results demonstrate that the proposed method can significantly drop the spike firing with better performance than state-of-the-art SNN baselines. Our code is available in \url{https://github.com/BICLab/ASA-SNN}.
</details>
<details>
<summary>摘要</summary>
神经网络（SNN）因其能够减少能耗而被广泛认为是软件人工神经网络的有望代替。然而，由于人们对SNN的假设偏见，即SNN的快速发射是稀疏的，因此对SNN中的内在冗余进行分析和优化得到了相对少的关注。在这项工作中，我们提出了三个关键问题，即SNN中冗余的来源、冗余如何影响SNN的空间时间动力学和发射，以及如何利用SNN的冗余来提高准确率和能效性。我们提出了一种提高空间注意力（ASA）模块，该模块可以自适应优化SNN的膜电压分布，并减少噪声发射特征。实验结果表明，我们的方法可以至关重要地降低SNN的噪声发射，并在比较现有SNN基eline的情况下提高性能。我们的代码可以在<https://github.com/BICLab/ASA-SNN>中找到。
</details></li>
</ul>
<hr>
<h2 id="How-To-Overcome-Confirmation-Bias-in-Semi-Supervised-Image-Classification-By-Active-Learning"><a href="#How-To-Overcome-Confirmation-Bias-in-Semi-Supervised-Image-Classification-By-Active-Learning" class="headerlink" title="How To Overcome Confirmation Bias in Semi-Supervised Image Classification By Active Learning"></a>How To Overcome Confirmation Bias in Semi-Supervised Image Classification By Active Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08224">http://arxiv.org/abs/2308.08224</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sandra Gilhuber, Rasmus Hvingelby, Mang Ling Ada Fok, Thomas Seidl</li>
<li>for: 这篇论文是关于是否需要活动学习的研究，它发现了强大的深度半supervised方法的出现，使得有限的标签数据设置中的活动学习是否可以使用的问题提出了问题。</li>
<li>methods: 这篇论文使用了 semi-supervised learning（SSL）方法和活动学习（AL）方法进行比较，以确定在实际数据场景中是否可以结合这两种方法以获得更好的性能。</li>
<li>results: 研究发现，SSL方法和AL方法的组合可以在实际数据场景中提高性能，但是这些结果来自于已知的benchmark数据集，这可能会过度估计外部效应。此外，文献缺乏关于实际数据场景中active semi-supervised learning方法的研究，这导致了我们对这些方法在实际场景中的性能的理解有限。因此，这篇论文提出了三个常见的数据挑战： между类异质、内类异质和between类相似。这些挑战可能会影响SSL性能，但是Random sampling不能 Mitigate这些挑战，有时甚至比supervised learning更差。然而，我们发现，在实际数据场景中，AL可以超越Confirmation bias，提高SSL的性能。<details>
<summary>Abstract</summary>
Do we need active learning? The rise of strong deep semi-supervised methods raises doubt about the usability of active learning in limited labeled data settings. This is caused by results showing that combining semi-supervised learning (SSL) methods with a random selection for labeling can outperform existing active learning (AL) techniques. However, these results are obtained from experiments on well-established benchmark datasets that can overestimate the external validity. However, the literature lacks sufficient research on the performance of active semi-supervised learning methods in realistic data scenarios, leaving a notable gap in our understanding. Therefore we present three data challenges common in real-world applications: between-class imbalance, within-class imbalance, and between-class similarity. These challenges can hurt SSL performance due to confirmation bias. We conduct experiments with SSL and AL on simulated data challenges and find that random sampling does not mitigate confirmation bias and, in some cases, leads to worse performance than supervised learning. In contrast, we demonstrate that AL can overcome confirmation bias in SSL in these realistic settings. Our results provide insights into the potential of combining active and semi-supervised learning in the presence of common real-world challenges, which is a promising direction for robust methods when learning with limited labeled data in real-world applications.
</details>
<details>
<summary>摘要</summary>
是否需要活动学习？强大深度半supervised方法的出现使得有限量标注数据的使用可能性受到了质问。这是因为结果表明将半supervised学习（SSL）方法与随机选择标注结合可以超越现有的活动学习（AL）技术。然而，这些结果是基于可靠的 benchmark 数据集进行实验所获得的，这可能会过度估计外部有效性。然而， литераature 缺乏对活动半supervised学习方法在实际数据场景中的性能的研究，这种空白在我们的理解中留下了一个大的坑。因此，我们介绍了三种常见的实际数据挑战： между类差异、 Within 类差异和 между类相似性。这些挑战可能会对 SSL 性能产生负面影响，因为确认偏见。我们在 simulated 数据挑战上进行了 SSL 和 AL 实验，发现随机抽样不能 Mitigate 确认偏见，而在一些情况下，随机抽样甚至比supervised learning（SL）更差。然而，我们示出了 AL 可以在这些实际设置中超越确认偏见。我们的结果提供了对将活动和半supervised学习结合使用在实际数据场景中的可能性的深入理解，这是一个可靠的方法在有限量标注数据的实际应用中学习。
</details></li>
</ul>
<hr>
<h2 id="HyperSNN-A-new-efficient-and-robust-deep-learning-model-for-resource-constrained-control-applications"><a href="#HyperSNN-A-new-efficient-and-robust-deep-learning-model-for-resource-constrained-control-applications" class="headerlink" title="HyperSNN: A new efficient and robust deep learning model for resource constrained control applications"></a>HyperSNN: A new efficient and robust deep learning model for resource constrained control applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08222">http://arxiv.org/abs/2308.08222</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhanglu Yan, Shida Wang, Kaiwen Tang, Weng-Fai Wong</li>
<li>for: 这个论文主要针对智能家居、机器人和智能家具等领域的边缘计算技术进行研究和开发，以提高系统的能效性和可靠性。</li>
<li>methods: 该论文提出了一种新的控制任务方法，称为HyperSNN，它将神经网络和高维度计算相结合，以降低能耗并提高系统的可靠性和可能性。HyperSNN使用8位整数加法取代昂贵的32位浮点 multiply操作，从而降低了能耗，同时保持了与传统机器学习方法相当的控制精度。</li>
<li>results: 该论文通过在AI Gym benchmark上测试HyperSNN模型，发现HyperSNN可以与传统机器学习方法具有相同的控制精度，但具有9.96%到1.36%的能耗减少。此外，实验还表明HyperSNN具有更高的可靠性。因此，HyperSNN适用于交互式、移动和穿戴式设备，推动了能效性和可靠性的系统设计。同时，它开创了实际工业场景中复杂算法如模型预测控制（MPC）的实践之路。<details>
<summary>Abstract</summary>
In light of the increasing adoption of edge computing in areas such as intelligent furniture, robotics, and smart homes, this paper introduces HyperSNN, an innovative method for control tasks that uses spiking neural networks (SNNs) in combination with hyperdimensional computing. HyperSNN substitutes expensive 32-bit floating point multiplications with 8-bit integer additions, resulting in reduced energy consumption while enhancing robustness and potentially improving accuracy. Our model was tested on AI Gym benchmarks, including Cartpole, Acrobot, MountainCar, and Lunar Lander. HyperSNN achieves control accuracies that are on par with conventional machine learning methods but with only 1.36% to 9.96% of the energy expenditure. Furthermore, our experiments showed increased robustness when using HyperSNN. We believe that HyperSNN is especially suitable for interactive, mobile, and wearable devices, promoting energy-efficient and robust system design. Furthermore, it paves the way for the practical implementation of complex algorithms like model predictive control (MPC) in real-world industrial scenarios.
</details>
<details>
<summary>摘要</summary>
在智能家居、机器人和智能家庭等领域的边缘计算技术普及化的背景下，这篇论文引入了HyperSNN，一种使用脉冲神经网络（SNN）和高维ensional计算的创新方法，用于控制任务。HyperSNN将昂贵的32位浮点 multiply替换为8位整数加法，从而降低能耗而不失精度，可能提高准确率。我们的模型在 AI Gym 标准彩色环境中进行了测试，包括 Cartpole、Acrobot、MountainCar 和 Lunar Lander 等benchmark。HyperSNN在控制精度方面与传统机器学习方法相当，但只需1.36%到9.96%的能耗。此外，我们的实验表明，使用HyperSNN可以提高系统的可靠性。我们认为HyperSNN特别适合交互式、移动和穿戴式设备，推动能效的系统设计。此外，它开创了实际工业应用场景中实施复杂算法如预测控制（MPC）的实际应用之路。
</details></li>
</ul>
<hr>
<h2 id="In-situ-Fault-Diagnosis-of-Indium-Tin-Oxide-Electrodes-by-Processing-S-Parameter-Patterns"><a href="#In-situ-Fault-Diagnosis-of-Indium-Tin-Oxide-Electrodes-by-Processing-S-Parameter-Patterns" class="headerlink" title="In situ Fault Diagnosis of Indium Tin Oxide Electrodes by Processing S-Parameter Patterns"></a>In situ Fault Diagnosis of Indium Tin Oxide Electrodes by Processing S-Parameter Patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11639">http://arxiv.org/abs/2308.11639</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tae Yeob Kang, Haebom Lee, Sungho Suh</li>
<li>for: 这研究旨在开发一种可以早期检测和诊断氧化镧锆矿电极（ITO电极）的缺陷，以确保设备的性能和可靠性。</li>
<li>methods: 该研究使用了散射参数（S-parameter）信号处理技术，可以提供早期检测、高精度诊断、防噪特性和根本原因分析。</li>
<li>results: 研究发现，通过将不同频率频谱的S-parameter作为输入，可以使用深度学习（DL）算法同时分析缺陷的原因和严重程度，并在噪声水平下提高诊断性能。<details>
<summary>Abstract</summary>
In the field of optoelectronics, indium tin oxide (ITO) electrodes play a crucial role in various applications, such as displays, sensors, and solar cells. Effective fault detection and diagnosis of the ITO electrodes are essential to ensure the performance and reliability of the devices. However, traditional visual inspection is challenging with transparent ITO electrodes, and existing fault detection methods have limitations in determining the root causes of the defects, often requiring destructive evaluations. In this study, an in situ fault diagnosis method is proposed using scattering parameter (S-parameter) signal processing, offering early detection, high diagnostic accuracy, noise robustness, and root cause analysis. A comprehensive S-parameter pattern database is obtained according to defect states. Deep learning (DL) approaches, including multilayer perceptron (MLP), convolutional neural network (CNN), and transformer, are then used to simultaneously analyze the cause and severity of defects. Notably, it is demonstrated that the diagnostic performance under additive noise levels can be significantly enhanced by combining different channels of the S-parameters as input to the learning algorithms, as confirmed through the t-distributed stochastic neighbor embedding (t-SNE) dimension reduction visualization.
</details>
<details>
<summary>摘要</summary>
在光电子学领域，锌镉矿（ITO）电极在不同应用中发挥关键作用，如显示器、感测器和太阳能电池。确切检测和诊断ITO电极的缺陷非常重要，以确保设备的性能和可靠性。然而，传统的视觉检查困难于透明的ITO电极，现有的缺陷检测方法有限制，常需要破坏评估。本研究提出了即场缺陷诊断方法，使用散射参数（S-parameter）信号处理，可以早期检测、高度准确地诊断缺陷，并具有噪声抗性和根本原因分析能力。通过对缺陷状态的S-parameter模式数据库的建立，使用深度学习（DL）方法，包括多层感知网络（MLP）、卷积神经网络（CNN）和变换器，同时分析缺陷的原因和严重程度。尤其是，研究表明，将不同通道的S-parameters作为输入，可以增强对噪声水平的诊断性能，这得到了通过t-分布随机邻居embedding（t-SNE）维度减少视觉化的证明。
</details></li>
</ul>
<hr>
<h2 id="Epicure-Distilling-Sequence-Model-Predictions-into-Patterns"><a href="#Epicure-Distilling-Sequence-Model-Predictions-into-Patterns" class="headerlink" title="Epicure: Distilling Sequence Model Predictions into Patterns"></a>Epicure: Distilling Sequence Model Predictions into Patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08203">http://arxiv.org/abs/2308.08203</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miltiadis Allamanis, Earl T. Barr</li>
<li>for: 这篇论文主要是为了提高机器学习模型在预测高 entropy 序列时的准确率。</li>
<li>methods: 这篇论文提出了一种方法 called Epicure，它可以将机器学习模型的预测转换成简单的模式。Epicure 使用一个格子来表示模型预测的各种抽象模式，这些模式可以更好地捕捉模型预测的细节。</li>
<li>results: 在测试两个任务中，namely predicting a descriptive name of a function given its source code body和detecting anomalous names given a function，Epicure 能够更准确地预测名称。Compared to the best model prediction, Epicure 可以提高准确率 by 61% for a false alarm rate of 10%.<details>
<summary>Abstract</summary>
Most machine learning models predict a probability distribution over concrete outputs and struggle to accurately predict names over high entropy sequence distributions. Here, we explore finding abstract, high-precision patterns intrinsic to these predictions in order to make abstract predictions that usefully capture rare sequences. In this short paper, we present Epicure, a method that distils the predictions of a sequence model, such as the output of beam search, into simple patterns. Epicure maps a model's predictions into a lattice that represents increasingly more general patterns that subsume the concrete model predictions.   On the tasks of predicting a descriptive name of a function given the source code of its body and detecting anomalous names given a function, we show that Epicure yields accurate naming patterns that match the ground truth more often compared to just the highest probability model prediction. For a false alarm rate of 10%, Epicure predicts patterns that match 61% more ground-truth names compared to the best model prediction, making Epicure well-suited for scenarios that require high precision.
</details>
<details>
<summary>摘要</summary>
大多数机器学习模型预测结果为概率分布，困难准确预测高 entropy 序列分布中的名称。我们在这里探索找到高精度抽象模式，以便在罕见序列中准确预测名称。在这篇短文中，我们介绍 Epicure，一种方法，可以将序列模型的预测映射到一个表示增加更一般模式的格子中。在函数体代码中预测函数名和检测异常名称任务上，我们显示 Epicure 可以准确地预测名称，与真实值更常地匹配。对于 false alarm rate 为 10%，Epicure 可以预测匹配真实值的名称61%多于最佳模型预测，使其适用于需要高精度的场景。
</details></li>
</ul>
<hr>
<h2 id="DeSCo-Towards-Generalizable-and-Scalable-Deep-Subgraph-Counting"><a href="#DeSCo-Towards-Generalizable-and-Scalable-Deep-Subgraph-Counting" class="headerlink" title="DeSCo: Towards Generalizable and Scalable Deep Subgraph Counting"></a>DeSCo: Towards Generalizable and Scalable Deep Subgraph Counting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08198">http://arxiv.org/abs/2308.08198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyu Fu, Chiyue Wei, Yu Wang, Rex Ying<br>for: 这篇论文是针对大规模子граφ counting问题（subgraph counting）提出了一个专业的解决方案。大规模子граφ counting有很多实际应用，例如社交网络分析中的模式计数以及交易网络中的过滤探测。methods: 这篇论文使用了一个叫做DeSCo的神经深度子граφ counting管线，以精确地预测查询的计数和出现位置。DeSCo首先使用了一个新的 canonical partition 技术，将大型目标图分成小型邻接图。这种技术可以严重减少查询的计数 variation，并且保证不会 missed 或 double-counting。其次，DeSCo使用了一个具有表现力的子граφ-based heterogeneous graph neural network 来实现每个邻接图中的计数。最后，DeSCo使用了一个称为 gossip propagation 的传播方法，将邻接图中的计数传递到下一个邻接图中。results: 这篇论文的实验结果显示，DeSCo 可以在八个真实世界数据集上进行预测，并且与现有的神经方法相比，具有137倍的平均方差错误，同时保持了多项式时间复杂性。<details>
<summary>Abstract</summary>
Subgraph counting is the problem of counting the occurrences of a given query graph in a large target graph. Large-scale subgraph counting is useful in various domains, such as motif counting for social network analysis and loop counting for money laundering detection on transaction networks. Recently, to address the exponential runtime complexity of scalable subgraph counting, neural methods are proposed. However, existing neural counting approaches fall short in three aspects. Firstly, the counts of the same query can vary from zero to millions on different target graphs, posing a much larger challenge than most graph regression tasks. Secondly, current scalable graph neural networks have limited expressive power and fail to efficiently distinguish graphs in count prediction. Furthermore, existing neural approaches cannot predict the occurrence position of queries in the target graph.   Here we design DeSCo, a scalable neural deep subgraph counting pipeline, which aims to accurately predict the query count and occurrence position on any target graph after one-time training. Firstly, DeSCo uses a novel canonical partition and divides the large target graph into small neighborhood graphs. The technique greatly reduces the count variation while guaranteeing no missing or double-counting. Secondly, neighborhood counting uses an expressive subgraph-based heterogeneous graph neural network to accurately perform counting in each neighborhood. Finally, gossip propagation propagates neighborhood counts with learnable gates to harness the inductive biases of motif counts. DeSCo is evaluated on eight real-world datasets from various domains. It outperforms state-of-the-art neural methods with 137x improvement in the mean squared error of count prediction, while maintaining the polynomial runtime complexity.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>大规模的子图计数问题是计算给定查询图在大型目标图中出现的次数。这种问题在社会网络分析中计算模式的搜索和金融网络中检测购买交易的循环计数等领域都是有用的。然而，目前的神经方法并不能够准确地解决这个问题。主要的问题包括：首先，查询的计数可以在不同的目标图上从零到百万之间变化，这比大多数图回归任务更加具有挑战性。其次，当前可扩展的图神经网络具有有限的表达能力，无法高效地 distinguishing 图。最后，现有的神经方法无法预测查询在目标图中的出现位置。为了解决这些问题，我们设计了DeSCo，一种可扩展的神经深度子图计数管道。DeSCo通过一种新的准确的分区方法将大型目标图分解成小 neighbohood 图。这种技术可以很好地减少计数的变化，同时保证不会产生错过或重复计数。其次，neighborhood 计数使用表达能力强的不同类型图神经网络来准确地进行计数。最后，gossip 传播使用学习门户来传递 neighborhood 计数，以便利用模式计数的启发性。DeSCo在八个实际数据集上进行了评估，与当前的神经方法相比，具有137倍的平均平方误差提升，同时保持了对数时间复杂度。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Explainable-AI-to-Analyze-Researchers’-Aspect-Based-Sentiment-about-ChatGPT"><a href="#Leveraging-Explainable-AI-to-Analyze-Researchers’-Aspect-Based-Sentiment-about-ChatGPT" class="headerlink" title="Leveraging Explainable AI to Analyze Researchers’ Aspect-Based Sentiment about ChatGPT"></a>Leveraging Explainable AI to Analyze Researchers’ Aspect-Based Sentiment about ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11001">http://arxiv.org/abs/2308.11001</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shilpa Lakhanpal, Ajay Gupta, Rajeev Agrawal</li>
<li>for: 本研究旨在分析研究者对ChatGPT的看法，即使用哪些方法和得到什么结果。</li>
<li>methods: 本研究使用可解释AI来实现方面基于情感分析，以推动state of the art的提升。</li>
<li>results: 本研究提供了延伸 newer datasets上的方面基于情感分析的有价值信息，并不受文本数据的长度所限制。<details>
<summary>Abstract</summary>
The groundbreaking invention of ChatGPT has triggered enormous discussion among users across all fields and domains. Among celebration around its various advantages, questions have been raised with regards to its correctness and ethics of its use. Efforts are already underway towards capturing user sentiments around it. But it begs the question as to how the research community is analyzing ChatGPT with regards to various aspects of its usage. It is this sentiment of the researchers that we analyze in our work. Since Aspect-Based Sentiment Analysis has usually only been applied on a few datasets, it gives limited success and that too only on short text data. We propose a methodology that uses Explainable AI to facilitate such analysis on research data. Our technique presents valuable insights into extending the state of the art of Aspect-Based Sentiment Analysis on newer datasets, where such analysis is not hampered by the length of the text data.
</details>
<details>
<summary>摘要</summary>
<<SYS>> chatgpt 的创新性发明引发了各界用户的广泛讨论，包括其多种优点的欢快和使用 ethics 的问题。各方面的用户情 Sentiment 已经在进行 Capture 的工作。但是，我们需要问到研究者们如何对 chatgpt 进行多方面的分析。我们的方法使用可解释 AI 来实现这种分析，并提供了valuable insights 以推进 aspect-based sentiment analysis 的状态艺术的扩展。</SYS>>Here's the translation of the text into Traditional Chinese:<<SYS>> chatgpt 的创新性发明引发了各界用户的广泛讨论，包括其多种优点的欢快和使用 ethics 的问题。各方面的用户情感已经在进行 Capture 的工作。但是，我们需要问到研究者们如何进行 chatgpt 的多方面分析。我们的方法使用可解释 AI 来实现这种分析，并提供了 valuable insights 以推进 aspect-based sentiment analysis 的状态艺术的扩展。</SYS>>
</details></li>
</ul>
<hr>
<h2 id="Endogenous-Macrodynamics-in-Algorithmic-Recourse"><a href="#Endogenous-Macrodynamics-in-Algorithmic-Recourse" class="headerlink" title="Endogenous Macrodynamics in Algorithmic Recourse"></a>Endogenous Macrodynamics in Algorithmic Recourse</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08187">http://arxiv.org/abs/2308.08187</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pat-alt/endogenous-macrodynamics-in-algorithmic-recourse">https://github.com/pat-alt/endogenous-macrodynamics-in-algorithmic-recourse</a></li>
<li>paper_authors: Patrick Altmeyer, Giovan Angela, Aleksander Buszydlik, Karol Dobiczek, Arie van Deursen, Cynthia C. S. Liem</li>
<li>for: 本研究旨在探讨Counterfactual Explanations (CE)和Algorithmic Recourse (AR)在动态环境下的应用，包括如何处理数据和模型演变的问题。</li>
<li>methods: 本研究使用一种普适的框架来描述现有的方法ologies，并发现了一种隐藏的外部成本，即在群体水平研究救济的终结性影响。</li>
<li>results: 通过使用各种现状最佳化器和数据集，我们通过模拟实验发现了救济引入的领域和模型变化很大，可能会妨碍救济的应用。然而，我们也提出了一些缓解方法，并开发了一个快速的 simulate框架。<details>
<summary>Abstract</summary>
Existing work on Counterfactual Explanations (CE) and Algorithmic Recourse (AR) has largely focused on single individuals in a static environment: given some estimated model, the goal is to find valid counterfactuals for an individual instance that fulfill various desiderata. The ability of such counterfactuals to handle dynamics like data and model drift remains a largely unexplored research challenge. There has also been surprisingly little work on the related question of how the actual implementation of recourse by one individual may affect other individuals. Through this work, we aim to close that gap. We first show that many of the existing methodologies can be collectively described by a generalized framework. We then argue that the existing framework does not account for a hidden external cost of recourse, that only reveals itself when studying the endogenous dynamics of recourse at the group level. Through simulation experiments involving various state-of the-art counterfactual generators and several benchmark datasets, we generate large numbers of counterfactuals and study the resulting domain and model shifts. We find that the induced shifts are substantial enough to likely impede the applicability of Algorithmic Recourse in some situations. Fortunately, we find various strategies to mitigate these concerns. Our simulation framework for studying recourse dynamics is fast and opensourced.
</details>
<details>
<summary>摘要</summary>
现有的工作主要关注单个个体在静止环境下的Counterfactual Explanations（CE）和Algorithmic Recourse（AR），即给一个估计模型下找到满足多种要求的有效counterfactuals。然而，这些counterfactuals在数据和模型变化时的处理能力仍然是一个未经探索的研究挑战。此外，很少有研究关于个体实施救济后对其他个体的影响。我们通过这项工作来填补这一差距。我们首先表明了许多现有方法可以总结为一个通用框架。然后，我们 argue that现有的框架不会考虑一种隐藏的外部成本，只有在研究群体级别的救济动力学时才能发现。通过使用多种state-of-the-art counterfactual生成器和多个标准数据集，我们生成了大量的counterfactuals，并研究其导致的领域和模型变化。我们发现这些变化足够大，可能会阻碍救济的应用。幸运地，我们发现了多种缓解这些问题的策略。我们的救济动力学 simulate框架快速且开源。
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Generic-Graph-Neural-Networks-via-Architecture-Compiler-Partition-Method-Co-Design"><a href="#Accelerating-Generic-Graph-Neural-Networks-via-Architecture-Compiler-Partition-Method-Co-Design" class="headerlink" title="Accelerating Generic Graph Neural Networks via Architecture, Compiler, Partition Method Co-Design"></a>Accelerating Generic Graph Neural Networks via Architecture, Compiler, Partition Method Co-Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08174">http://arxiv.org/abs/2308.08174</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuwen Lu, Zhihui Zhang, Cong Guo, Jingwen Leng, Yangjie Zhou, Minyi Guo<br>for: This paper aims to develop a high-performance and efficient hardware acceleration framework for graph neural network (GNN) models, addressing the challenges of high bandwidth requirements and model diversity.methods: The proposed framework, called SwitchBlade, utilizes a new type of partition-level operator fusion, partition-level multi-threading, and fine-grained graph partitioning to reduce the bandwidth requirement and improve hardware utilization.results: The proposed framework achieves an average speedup of 1.85 times and energy savings of 19.03 times compared to the NVIDIA V100 GPU, and delivers performance comparable to state-of-the-art specialized accelerators.<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have shown significant accuracy improvements in a variety of graph learning domains, sparking considerable research interest. To translate these accuracy improvements into practical applications, it is essential to develop high-performance and efficient hardware acceleration for GNN models. However, designing GNN accelerators faces two fundamental challenges: the high bandwidth requirement of GNN models and the diversity of GNN models. Previous works have addressed the first challenge by using more expensive memory interfaces to achieve higher bandwidth. For the second challenge, existing works either support specific GNN models or have generic designs with poor hardware utilization.   In this work, we tackle both challenges simultaneously. First, we identify a new type of partition-level operator fusion, which we utilize to internally reduce the high bandwidth requirement of GNNs. Next, we introduce partition-level multi-threading to schedule the concurrent processing of graph partitions, utilizing different hardware resources. To further reduce the extra on-chip memory required by multi-threading, we propose fine-grained graph partitioning to generate denser graph partitions. Importantly, these three methods make no assumptions about the targeted GNN models, addressing the challenge of model variety. We implement these methods in a framework called SwitchBlade, consisting of a compiler, a graph partitioner, and a hardware accelerator. Our evaluation demonstrates that SwitchBlade achieves an average speedup of $1.85\times$ and energy savings of $19.03\times$ compared to the NVIDIA V100 GPU. Additionally, SwitchBlade delivers performance comparable to state-of-the-art specialized accelerators.
</details>
<details>
<summary>摘要</summary>
格raph神经网络（GNN）在各种图学任务中表现出了显著的准确性改进，引发了广泛的研究兴趣。为将这些准确性改进应用于实际场景，必须开发高性能和高效的硬件加速器 для GNN 模型。然而，设计 GNN 加速器面临两个基本挑战：GNN 模型的带宽要求高，以及 GNN 模型的多样性。先前的工作通过使用更昂贵的内存接口来实现更高的带宽来解决第一个挑战。对于第二个挑战，现有的工作ether支持特定的 GNN 模型或者有通用的设计，导致硬件利用率低下。在这个工作中，我们同时解决了这两个挑战。首先，我们发现了一种新的合并类型的分区级操作，我们利用这种合并来减少 GNN 模型的带宽要求。然后，我们引入分区级多线程来调度图分 partitions 的同时处理，使用不同的硬件资源。为了避免多线程增加的额外内存开销，我们提议细化的图分解。这三种方法不仅不假设目标 GNN 模型，而且还可以减少硬件内存占用。我们将这些方法集成到一个名为 SwitchBlade 的框架中，包括编译器、图分解器和硬件加速器。我们的评估表明，SwitchBlade 可以在 NVIDIA V100 GPU 上实现平均的速度提升 $1.85\times$ 和能源减少 $19.03\times$。此外，SwitchBlade 可以与当前的特化加速器相比。
</details></li>
</ul>
<hr>
<h2 id="Expressivity-of-Graph-Neural-Networks-Through-the-Lens-of-Adversarial-Robustness"><a href="#Expressivity-of-Graph-Neural-Networks-Through-the-Lens-of-Adversarial-Robustness" class="headerlink" title="Expressivity of Graph Neural Networks Through the Lens of Adversarial Robustness"></a>Expressivity of Graph Neural Networks Through the Lens of Adversarial Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08173">http://arxiv.org/abs/2308.08173</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/francesco-campi/rob-subgraphs">https://github.com/francesco-campi/rob-subgraphs</a></li>
<li>paper_authors: Francesco Campi, Lukas Gosch, Tom Wollschläger, Yan Scholten, Stephan Günnemann</li>
<li>for: This paper studies the adversarial robustness of Graph Neural Networks (GNNs) and compares their expressive power to traditional Message Passing Neural Networks (MPNNs).</li>
<li>methods: The paper uses adversarial attacks to test the ability of GNNs to count specific subgraph patterns, and extends the concept of adversarial robustness to this task.</li>
<li>results: The paper shows that more powerful GNNs fail to generalize to small perturbations to the graph’s structure and fail to count substructures on out-of-distribution graphs.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文研究了图神经网络（GNNs）的 adversarial robustness，并与传统的消息传递神经网络（MPNNs）进行比较。</li>
<li>methods: 论文使用了针对特定子图 Pattern的 adversarial 攻击，并将这种概念扩展到这个任务上。</li>
<li>results: 论文显示了更强大的 GNNs 对小 strucure 的变化和 out-of-distribution 图表示不能通过总结。<details>
<summary>Abstract</summary>
We perform the first adversarial robustness study into Graph Neural Networks (GNNs) that are provably more powerful than traditional Message Passing Neural Networks (MPNNs). In particular, we use adversarial robustness as a tool to uncover a significant gap between their theoretically possible and empirically achieved expressive power. To do so, we focus on the ability of GNNs to count specific subgraph patterns, which is an established measure of expressivity, and extend the concept of adversarial robustness to this task. Based on this, we develop efficient adversarial attacks for subgraph counting and show that more powerful GNNs fail to generalize even to small perturbations to the graph's structure. Expanding on this, we show that such architectures also fail to count substructures on out-of-distribution graphs.
</details>
<details>
<summary>摘要</summary>
我们进行了第一个对图神经网络（GNNs）的逆攻击 robustness 研究，发现它们在许多情况下比传统的消息传递神经网络（MPNNs）更具有潜在的表达能力。特别是，我们使用逆攻击 robustness 作为一种工具，揭示了 GNNs 的表达能力与理论可能的表达能力之间存在很大的差距。为此，我们专注于 GNNs 的子图计数能力，这是一个已知的表达能力指标，并将逆攻击 robustness 扩展到这个任务。 Based on this, we develop efficient adversarial attacks for subgraph counting and show that more powerful GNNs fail to generalize even to small perturbations to the graph's structure. In addition, we show that such architectures also fail to count substructures on out-of-distribution graphs.
</details></li>
</ul>
<hr>
<h2 id="AATCT-IDS-A-Benchmark-Abdominal-Adipose-Tissue-CT-Image-Dataset-for-Image-Denoising-Semantic-Segmentation-and-Radiomics-Evaluation"><a href="#AATCT-IDS-A-Benchmark-Abdominal-Adipose-Tissue-CT-Image-Dataset-for-Image-Denoising-Semantic-Segmentation-and-Radiomics-Evaluation" class="headerlink" title="AATCT-IDS: A Benchmark Abdominal Adipose Tissue CT Image Dataset for Image Denoising, Semantic Segmentation, and Radiomics Evaluation"></a>AATCT-IDS: A Benchmark Abdominal Adipose Tissue CT Image Dataset for Image Denoising, Semantic Segmentation, and Radiomics Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08172">http://arxiv.org/abs/2308.08172</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiyu Ma, Chen Li, Tianming Du, Le Zhang, Dechao Tang, Deguo Ma, Shanchuan Huang, Yan Liu, Yihao Sun, Zhihao Chen, Jin Yuan, Qianqing Nie, Marcin Grzegorzek, Hongzan Sun</li>
<li>for: 这个研究用于探讨腹部脂肪组织CT图像数据集（AATTCT-IDS）的研究潜力。</li>
<li>methods: 该研究使用了AATTCT-IDS数据集，并对其进行了不同任务的比较研究，包括图像压缩、 semantics 分割和 радиологиcal 分析。</li>
<li>results: 研究结果显示，在图像压缩任务中，使用缓和策略可以更好地降低杂噪，但是会导致图像细节的损失。在 semantics 分割任务中，BiSeNet 模型可以在短时间内 obtaint 比较好的 segmentation 结果，并具有良好的隔离小型和隔离的脂肪组织能力。在 радиологиcal 分析任务中，研究发现了脂肪分布的多dimensional 特征。<details>
<summary>Abstract</summary>
Methods: In this study, a benchmark \emph{Abdominal Adipose Tissue CT Image Dataset} (AATTCT-IDS) containing 300 subjects is prepared and published. AATTCT-IDS publics 13,732 raw CT slices, and the researchers individually annotate the subcutaneous and visceral adipose tissue regions of 3,213 of those slices that have the same slice distance to validate denoising methods, train semantic segmentation models, and study radiomics. For different tasks, this paper compares and analyzes the performance of various methods on AATTCT-IDS by combining the visualization results and evaluation data. Thus, verify the research potential of this data set in the above three types of tasks.   Results: In the comparative study of image denoising, algorithms using a smoothing strategy suppress mixed noise at the expense of image details and obtain better evaluation data. Methods such as BM3D preserve the original image structure better, although the evaluation data are slightly lower. The results show significant differences among them. In the comparative study of semantic segmentation of abdominal adipose tissue, the segmentation results of adipose tissue by each model show different structural characteristics. Among them, BiSeNet obtains segmentation results only slightly inferior to U-Net with the shortest training time and effectively separates small and isolated adipose tissue. In addition, the radiomics study based on AATTCT-IDS reveals three adipose distributions in the subject population.   Conclusion: AATTCT-IDS contains the ground truth of adipose tissue regions in abdominal CT slices. This open-source dataset can attract researchers to explore the multi-dimensional characteristics of abdominal adipose tissue and thus help physicians and patients in clinical practice. AATCT-IDS is freely published for non-commercial purpose at: \url{https://figshare.com/articles/dataset/AATTCT-IDS/23807256}.
</details>
<details>
<summary>摘要</summary>
方法：本研究使用的Benchmark数据集是《 Abdomen Adipose Tissue CT Image Dataset》（AATTCT-IDS），包含300个研究对象，并公布了13,732个Raw CT slice。研究人员对3,213个slice进行了手动标注，以验证减噪方法、训练semantic segmentation模型和研究 ради米克特性。通过组合视觉化结果和评估数据来对不同任务进行比较分析，以验证数据集的研究潜力。结果：在图像减噪比较研究中，使用缓和策略的算法可以更好地抑制杂噪，但是会增加图像细节的损失。BM3D算法可以更好地保持原始图像结构，但评估数据略为下降。结果显示了不同算法之间存在很大的差异。在 Abdomen Adipose Tissue Semantic Segmentation 比较研究中，每种模型的 segmentation 结果具有不同的结构特征。BISeNet模型可以在短时间内 obtener segmentation 结果，与 U-Net 相当，并且能够有效地分离小 isolated adipose tissue。此外，基于 AATTCT-IDS 的 ради米克特研究发现了脂肪分布的三种类型。结论：AATTCT-IDS 包含了 Abdomen CT 图像中脂肪组织区域的真实标准。这个开源数据集可以吸引研究人员通过多维度特征的探索，帮助临床医生和病人。AATTCT-IDS 采用非商业用途发布，可以免费下载，请参考：https://figshare.com/articles/dataset/AATTCT-IDS/23807256。
</details></li>
</ul>
<hr>
<h2 id="A-Quantum-Approximation-Scheme-for-k-Means"><a href="#A-Quantum-Approximation-Scheme-for-k-Means" class="headerlink" title="A Quantum Approximation Scheme for k-Means"></a>A Quantum Approximation Scheme for k-Means</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08167">http://arxiv.org/abs/2308.08167</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ragesh Jaiswal</li>
<li>for: 这个论文targets the classical $k$-means clustering problem, and proposes a quantum approximation scheme with a polylogarithmic running time.</li>
<li>methods: 这个算法使用了QRAM数据结构，并使用了一种$(1 + \varepsilon)$-approximation方法，其中$\varepsilon &gt; 0$是任意的 positivereal number.</li>
<li>results: 这个算法可以在时间复杂度为 $\tilde{O} \left( 2^{\tilde{O}(\frac{k}{\varepsilon})} \eta^2 d\right)$ 内运行，并且 WITH HIGH PROBABILITY输出一个$k$个中心点集合，其中$cost(V, C) \leq (1+\varepsilon) \cdot cost(V, C_{OPT})$，where $C_{OPT}$是最优的$k$-中心点集合，$cost(.)$是标准的$k$-means成本函数（即点到最近中心的平方距离的总和），并且$\eta$是最大距离到最小距离的比率。这是第一个具有polylogarithmic running time的量子算法，并且不需要量子线性代数子过程，运行时间独立于参数（例如condition number）。<details>
<summary>Abstract</summary>
We give a quantum approximation scheme (i.e., $(1 + \varepsilon)$-approximation for every $\varepsilon > 0$) for the classical $k$-means clustering problem in the QRAM model with a running time that has only polylogarithmic dependence on the number of data points. More specifically, given a dataset $V$ with $N$ points in $\mathbb{R}^d$ stored in QRAM data structure, our quantum algorithm runs in time $\tilde{O} \left( 2^{\tilde{O}(\frac{k}{\varepsilon})} \eta^2 d\right)$ and with high probability outputs a set $C$ of $k$ centers such that $cost(V, C) \leq (1+\varepsilon) \cdot cost(V, C_{OPT})$. Here $C_{OPT}$ denotes the optimal $k$-centers, $cost(.)$ denotes the standard $k$-means cost function (i.e., the sum of the squared distance of points to the closest center), and $\eta$ is the aspect ratio (i.e., the ratio of maximum distance to minimum distance). This is the first quantum algorithm with a polylogarithmic running time that gives a provable approximation guarantee of $(1+\varepsilon)$ for the $k$-means problem. Also, unlike previous works on unsupervised learning, our quantum algorithm does not require quantum linear algebra subroutines and has a running time independent of parameters (e.g., condition number) that appear in such procedures.
</details>
<details>
<summary>摘要</summary>
我们提供一种量子近似方案（即$(1+\varepsilon)$-近似方案，其中$\varepsilon > 0$） для классической$k$-Means分布问题在QRAM模型中，并且running时间只具有多项式幂ilogarithmic（polylogarithmic）依赖于数据点的数量。更具体地，给定一个数据集$V$包含$N$个点的归一化在QRAM数据结构中，我们的量子算法在时间 $\tilde{O} \left( 2^{\tilde{O}(\frac{k}{\varepsilon})} \eta^2 d\right)$ 内运行，并且高probability输出一组$C$的$k$个中心，使得 $cost(V, C) \leq (1+\varepsilon) \cdot cost(V, C_{OPT})$，其中$C_{OPT}$表示最优的$k$-中心，$cost(.)$表示标准的$k$-Means成本函数（即点到最近中心的平方距离之和），并且$\eta$是最大距离与最小距离的比率。这是首个具有多项式幂ilogarithmic running time的量子算法，并且不需要量子线性代数子算法，运行时间不依赖于参数（例如condition number）的量子学习算法。
</details></li>
</ul>
<hr>
<h2 id="PEvoLM-Protein-Sequence-Evolutionary-Information-Language-Model"><a href="#PEvoLM-Protein-Sequence-Evolutionary-Information-Language-Model" class="headerlink" title="PEvoLM: Protein Sequence Evolutionary Information Language Model"></a>PEvoLM: Protein Sequence Evolutionary Information Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08578">http://arxiv.org/abs/2308.08578</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/issararab/pevolm">https://github.com/issararab/pevolm</a></li>
<li>paper_authors: Issar Arab</li>
<li>for: 这个论文的目的是提出一种基于语言模型的多序列对应方法，以提高生物计算机学中的蛋白质序列分析。</li>
<li>methods: 该方法使用了一种基于语言模型的 bidirectional Long Short-Term Memory (LSTMs) 网络，并将 PSSMs 与转移学习结合，以降低模型的自由参数数量。</li>
<li>results: 该方法可以同时预测下一个氨基酸和其演化信息，并且可以通过多任务学习来学习蛋白质序列的进化信息。Here’s the simplified Chinese text:</li>
<li>for: 这个论文的目的是提出一种基于语言模型的多序列对应方法，以提高生物计算机学中的蛋白质序列分析。</li>
<li>methods: 该方法使用了一种基于语言模型的 bidirectional Long Short-Term Memory (LSTMs) 网络，并将 PSSMs 与转移学习结合，以降低模型的自由参数数量。</li>
<li>results: 该方法可以同时预测下一个氨基酸和其演化信息，并且可以通过多任务学习来学习蛋白质序列的进化信息。<details>
<summary>Abstract</summary>
With the exponential increase of the protein sequence databases over time, multiple-sequence alignment (MSA) methods, like PSI-BLAST, perform exhaustive and time-consuming database search to retrieve evolutionary information. The resulting position-specific scoring matrices (PSSMs) of such search engines represent a crucial input to many machine learning (ML) models in the field of bioinformatics and computational biology. A protein sequence is a collection of contiguous tokens or characters called amino acids (AAs). The analogy to natural language allowed us to exploit the recent advancements in the field of Natural Language Processing (NLP) and therefore transfer NLP state-of-the-art algorithms to bioinformatics. This research presents an Embedding Language Model (ELMo), converting a protein sequence to a numerical vector representation. While the original ELMo trained a 2-layer bidirectional Long Short-Term Memory (LSTMs) network following a two-path architecture, one for the forward and the second for the backward pass, by merging the idea of PSSMs with the concept of transfer-learning, this work introduces a novel bidirectional language model (bi-LM) with four times less free parameters and using rather a single path for both passes. The model was trained not only on predicting the next AA but also on the probability distribution of the next AA derived from similar, yet different sequences as summarized in a PSSM, simultaneously for multi-task learning, hence learning evolutionary information of protein sequences as well. The network architecture and the pre-trained model are made available as open source under the permissive MIT license on GitHub at https://github.com/issararab/PEvoLM.
</details>
<details>
<summary>摘要</summary>
随着蛋白序列数据库的呈指数增长，多重序列对齐（MSA）方法如PSI-BLAST在时间上进行耗时和耗力的数据库搜索，以获取进化信息。得到的位置特异分数据（PSSM）被多种机器学习（ML）模型在生物信息学和计算生物学中作为重要输入。蛋白序列是一系列连续的字符或氨基酸（AA）的集合。通过将蛋白序列与自然语言的相似性进行比较，我们可以利用自然语言处理（NLP）领域的最新进展，并将其应用到生物信息学中。本研究投入了一个Embedding Language Model（ELMo），将蛋白序列转换为数值vector表示。在原ELMo模型中，一个2层扩展LSTM网络按照两路架构，一路为前向传输，另一路为后向传输。在将PSSM的概念与传输学习混合到一起的基础上，本工作提出了一种新的双向语言模型（bi-LM），具有四倍少的自由参数，并使用单路架构进行两个方向的传输。该模型在预测下一个AA以外，同时也预测来自相似 yet different 序列的AA的概率分布，即PSSM，并在多任务学习中同时学习蛋白序列的进化信息。网络架构和预训练模型在MIT免费许可下在GitHub上提供，可以在https://github.com/issararab/PEvoLM中下载。
</details></li>
</ul>
<hr>
<h2 id="Stochastic-Controlled-Averaging-for-Federated-Learning-with-Communication-Compression"><a href="#Stochastic-Controlled-Averaging-for-Federated-Learning-with-Communication-Compression" class="headerlink" title="Stochastic Controlled Averaging for Federated Learning with Communication Compression"></a>Stochastic Controlled Averaging for Federated Learning with Communication Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08165">http://arxiv.org/abs/2308.08165</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinmeng Huang, Ping Li, Xiaoyun Li</li>
<li>for: 降低 Federated Learning（FL）的通信开销，提高FL的效率和可扩展性。</li>
<li>methods: 提出了一种更加高效&#x2F;简单的渐控方法，并基于此实现了两种压缩FL算法（SCALLION和SCAFCOM），支持不偏和偏压缩。</li>
<li>results:  experiments show that SCALLION和SCAFCOM可以与相应的全精度FL方法匹配或超越其通信和计算复杂度，并且可以在不同的数据不均性下表现良好。<details>
<summary>Abstract</summary>
Communication compression, a technique aiming to reduce the information volume to be transmitted over the air, has gained great interests in Federated Learning (FL) for the potential of alleviating its communication overhead. However, communication compression brings forth new challenges in FL due to the interplay of compression-incurred information distortion and inherent characteristics of FL such as partial participation and data heterogeneity. Despite the recent development, the performance of compressed FL approaches has not been fully exploited. The existing approaches either cannot accommodate arbitrary data heterogeneity or partial participation, or require stringent conditions on compression.   In this paper, we revisit the seminal stochastic controlled averaging method by proposing an equivalent but more efficient/simplified formulation with halved uplink communication costs. Building upon this implementation, we propose two compressed FL algorithms, SCALLION and SCAFCOM, to support unbiased and biased compression, respectively. Both the proposed methods outperform the existing compressed FL methods in terms of communication and computation complexities. Moreover, SCALLION and SCAFCOM accommodates arbitrary data heterogeneity and do not make any additional assumptions on compression errors. Experiments show that SCALLION and SCAFCOM can match the performance of corresponding full-precision FL approaches with substantially reduced uplink communication, and outperform recent compressed FL methods under the same communication budget.
</details>
<details>
<summary>摘要</summary>
通信压缩，一种目的是减少在空中传输的信息量，在联合学习（FL）中获得了广泛的关注，因为它可能减轻联合学习的通信负担。然而，通信压缩在FL中带来了新的挑战，这是因为压缩引入的信息扭曲和联合学习的自然特点，如数据不同性和部分参与。尽管有最近的发展，已有的压缩FL方法的性能尚未被完全利用。现有的方法 Either cannot accommodate arbitrary data heterogeneity or partial participation, or require stringent conditions on compression.在这篇论文中，我们重新考虑了seminal stochastic controlled averaging方法，并提出了一种更加有效/简单的表述，减少了上行通信成本的一半。基于这个实现，我们提出了两种压缩FL算法，即SCALLION和SCAFCOM，以支持不偏和偏 compression。两种方法在communication和计算复杂度方面都有更好的性能，并且可以满足任意数据不同性和不添加任何压缩错误的假设。实验表明，SCALLION和SCAFCOM可以与相应的全精度FL方法匹配性能，并且在相同的通信预算下出perform recent compressed FL methods。
</details></li>
</ul>
<hr>
<h2 id="Characteristics-of-networks-generated-by-kernel-growing-neural-gas"><a href="#Characteristics-of-networks-generated-by-kernel-growing-neural-gas" class="headerlink" title="Characteristics of networks generated by kernel growing neural gas"></a>Characteristics of networks generated by kernel growing neural gas</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08163">http://arxiv.org/abs/2308.08163</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kazuhisafujita/kernelgng">https://github.com/kazuhisafujita/kernelgng</a></li>
<li>paper_authors: Kazuhisa Fujita</li>
<li>for: 本研究旨在开发kernel GNG，一种基于GNG算法的kernel化版本，以及investigate kernel GNG所生成的网络特性。</li>
<li>methods: 本研究使用了五种kernels，包括Gaussian、Laplacian、Cauchy、inverse multiquadric和logkernels，将数据集映射到特征空间。</li>
<li>results: 研究发现，kernel GNG可以生成具有不同特性的网络，其中每种kernel生成的网络具有不同的特征。<details>
<summary>Abstract</summary>
This research aims to develop kernel GNG, a kernelized version of the growing neural gas (GNG) algorithm, and to investigate the features of the networks generated by the kernel GNG. The GNG is an unsupervised artificial neural network that can transform a dataset into an undirected graph, thereby extracting the features of the dataset as a graph. The GNG is widely used in vector quantization, clustering, and 3D graphics. Kernel methods are often used to map a dataset to feature space, with support vector machines being the most prominent application. This paper introduces the kernel GNG approach and explores the characteristics of the networks generated by kernel GNG. Five kernels, including Gaussian, Laplacian, Cauchy, inverse multiquadric, and log kernels, are used in this study.
</details>
<details>
<summary>摘要</summary>
这项研究的目标是开发kernel GNG，即基于基域神经网络（GNG）算法的基域化版本，并研究由kernel GNG生成的网络特性。GNG是一种无监督的人工神经网络，可以将数据集转换成无向图，从而提取数据集中的特征。GNG广泛应用于 вектор量化、归一化和3D图形。基域方法通常用于将数据集映射到特征空间，Support Vector Machines（SVM）是最广泛应用的应用。本文介绍了基域GNG方法，并探讨由基域GNG生成的网络特性。本研究使用的五种基域包括 Gaussian、Laplacian、Cauchy、 inverse multiquadric 和 log 基域。
</details></li>
</ul>
<hr>
<h2 id="Interpretability-Benchmark-for-Evaluating-Spatial-Misalignment-of-Prototypical-Parts-Explanations"><a href="#Interpretability-Benchmark-for-Evaluating-Spatial-Misalignment-of-Prototypical-Parts-Explanations" class="headerlink" title="Interpretability Benchmark for Evaluating Spatial Misalignment of Prototypical Parts Explanations"></a>Interpretability Benchmark for Evaluating Spatial Misalignment of Prototypical Parts Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08162">http://arxiv.org/abs/2308.08162</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikołaj Sacha, Bartosz Jura, Dawid Rymarczyk, Łukasz Struski, Jacek Tabor, Bartosz Zieliński</li>
<li>for: 该研究旨在检验和解释Prototypical Parts Networks（PPN）的准确性和可解释性问题。</li>
<li>methods: 研究人员提出了一种新的可解释性指标集，用于评估PPN模型中prototype activation region的准确性和可解释性问题。此外，他们还提出了一种misalignment compensation方法，用于解决这种问题。</li>
<li>results: 研究人员通过广泛的实验研究，证明了他们的指标集和补做方法的效果。他们发现，通过使用他们的方法，PPN模型可以更好地准确地捕捉和解释图像中的部分。<details>
<summary>Abstract</summary>
Prototypical parts-based networks are becoming increasingly popular due to their faithful self-explanations. However, their similarity maps are calculated in the penultimate network layer. Therefore, the receptive field of the prototype activation region often depends on parts of the image outside this region, which can lead to misleading interpretations. We name this undesired behavior a spatial explanation misalignment and introduce an interpretability benchmark with a set of dedicated metrics for quantifying this phenomenon. In addition, we propose a method for misalignment compensation and apply it to existing state-of-the-art models. We show the expressiveness of our benchmark and the effectiveness of the proposed compensation methodology through extensive empirical studies.
</details>
<details>
<summary>摘要</summary>
幻化网络的各部分模型在不断增长的 популяр度中，主要是因为它们的自我解释能力很强。然而，它们的相似度地图通常在半 finales层计算，这意味着prototype activation区域的受感知范围经常受到图像外部部分的影响，这可能导致不准确的解释。我们称这种情况为空间解释误差，并引入了一个特有的可度量这种现象的解释指标集。此外，我们还提出了一种补偿方法，并应用于现有的状态对照模型。我们通过广泛的实验研究证明了我们的指标和补偿方法的表达能力和有效性。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Adversarial-Robustness-of-Compressed-Deep-Learning-Models"><a href="#Benchmarking-Adversarial-Robustness-of-Compressed-Deep-Learning-Models" class="headerlink" title="Benchmarking Adversarial Robustness of Compressed Deep Learning Models"></a>Benchmarking Adversarial Robustness of Compressed Deep Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08160">http://arxiv.org/abs/2308.08160</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brijesh Vora, Kartik Patwari, Syed Mahbub Hafiz, Zubair Shafiq, Chen-Nee Chuah</li>
<li>for: 本研究旨在探讨针对基本模型的攻击输入对压缩后的模型的影响。</li>
<li>methods: 我们开发了一个多样化的攻击测试环境，并对多种常用的深度神经网络模型进行了探索。我们采用了优化的压缩策略，以保持准确性和性能。</li>
<li>results: 我们发现，即使压缩后的模型具有更好的总体性能和执行速度，它们对攻击输入的抵抗性仍然保持相对不变。这表明，模型压缩不会对针对攻击的鲁棒性产生负面影响。<details>
<summary>Abstract</summary>
The increasing size of Deep Neural Networks (DNNs) poses a pressing need for model compression, particularly when employed on resource constrained devices. Concurrently, the susceptibility of DNNs to adversarial attacks presents another significant hurdle. Despite substantial research on both model compression and adversarial robustness, their joint examination remains underexplored. Our study bridges this gap, seeking to understand the effect of adversarial inputs crafted for base models on their pruned versions. To examine this relationship, we have developed a comprehensive benchmark across diverse adversarial attacks and popular DNN models. We uniquely focus on models not previously exposed to adversarial training and apply pruning schemes optimized for accuracy and performance. Our findings reveal that while the benefits of pruning enhanced generalizability, compression, and faster inference times are preserved, adversarial robustness remains comparable to the base model. This suggests that model compression while offering its unique advantages, does not undermine adversarial robustness.
</details>
<details>
<summary>摘要</summary>
随着深度神经网络（DNN）的尺度不断增加，模型压缩成为了资源有限设备上的一 pressing需求。同时，DNN对攻击性输入的抵触也成为了一大难题。虽然关于模型压缩和攻击性稳定性的研究均有很大进步，但这两个领域之间的交叠仍然尚未得到充分研究。我们的研究尝试填补这一空白，探讨针对基本模型的攻击性输入如何影响其压缩版本。为了实现这一目标，我们在多种攻击方法和流行的DNN模型之间建立了一个完整的比较平台。我们独特地将注意力集中在没有接受过防御性训练的DNN模型上，并应用优化了准确和性能的压缩方案。我们的发现表明，即使使用压缩，模型的总体性能和攻击性稳定性仍然保持相对不变。这表明，模型压缩不会损害攻击性稳定性。
</details></li>
</ul>
<hr>
<h2 id="Deep-Generative-Imputation-Model-for-Missing-Not-At-Random-Data"><a href="#Deep-Generative-Imputation-Model-for-Missing-Not-At-Random-Data" class="headerlink" title="Deep Generative Imputation Model for Missing Not At Random Data"></a>Deep Generative Imputation Model for Missing Not At Random Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08158">http://arxiv.org/abs/2308.08158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jialei Chen, Yuanbo Xu, Pengyang Wang, Yongjian Yang</li>
<li>for: 解决Missing Not At Random (MNAR)问题，即数据损失的原因不完全 observable。</li>
<li>methods: 提出了一种基于 JOINT 分布的模型，并使用深度生成模型来处理实际世界中的损失机制，以并行地恢复不完整数据和重建损失的面具。</li>
<li>results: 对比STATE-OF-THE-ART 基eline，提出的GNR模型在RMSE方面平均提高9.9%到18.8%，并且总是在面具重建精度方面获得更好的结果，这使得恢复更加原理。<details>
<summary>Abstract</summary>
Data analysis usually suffers from the Missing Not At Random (MNAR) problem, where the cause of the value missing is not fully observed. Compared to the naive Missing Completely At Random (MCAR) problem, it is more in line with the realistic scenario whereas more complex and challenging. Existing statistical methods model the MNAR mechanism by different decomposition of the joint distribution of the complete data and the missing mask. But we empirically find that directly incorporating these statistical methods into deep generative models is sub-optimal. Specifically, it would neglect the confidence of the reconstructed mask during the MNAR imputation process, which leads to insufficient information extraction and less-guaranteed imputation quality. In this paper, we revisit the MNAR problem from a novel perspective that the complete data and missing mask are two modalities of incomplete data on an equal footing. Along with this line, we put forward a generative-model-specific joint probability decomposition method, conjunction model, to represent the distributions of two modalities in parallel and extract sufficient information from both complete data and missing mask. Taking a step further, we exploit a deep generative imputation model, namely GNR, to process the real-world missing mechanism in the latent space and concurrently impute the incomplete data and reconstruct the missing mask. The experimental results show that our GNR surpasses state-of-the-art MNAR baselines with significant margins (averagely improved from 9.9% to 18.8% in RMSE) and always gives a better mask reconstruction accuracy which makes the imputation more principle.
</details>
<details>
<summary>摘要</summary>
通常情况下，数据分析会面临缺失不具有完整性（Missing Not At Random，MNAR）问题，其中缺失的原因不是完全观察到。与完全随机缺失（Missing Completely At Random，MCAR）问题相比，MNAR问题更加真实和复杂。现有的统计方法将MNAR机制分解为不同的共同分布。但我们发现，直接将这些统计方法 интеGRATE到深度生成模型中是不佳的。特别是，它会忽略恢复mask的自信度，导致信息提取不充分和缺失补做质量不够保证。在这篇论文中，我们从一个新的视角来看待MNAR问题，即完整数据和缺失mask是两种不同的束缚数据模式。随着这一线，我们提出了一种特有的生成模型协同分布方法，即并合模型。这种方法可以在平行方式 represent complete data和缺失mask的分布，并提取完整数据和缺失mask中的足够信息。进一步，我们利用深度生成补做模型，即GNR，来处理实际中的缺失机制，并同时补做缺失数据和恢复缺失mask。实验结果显示，我们的GNR在RMSE方面与状态当前的MNAR基线之间提高了9.9%到18.8%的平均提升，并且总是提供更好的mask重建精度，这使得补做更符合原理。
</details></li>
</ul>
<hr>
<h2 id="Sarcasm-Detection-in-a-Disaster-Context"><a href="#Sarcasm-Detection-in-a-Disaster-Context" class="headerlink" title="Sarcasm Detection in a Disaster Context"></a>Sarcasm Detection in a Disaster Context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08156">http://arxiv.org/abs/2308.08156</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiberiu Sosea, Junyi Jessy Li, Cornelia Caragea</li>
<li>for: 这篇论文主要是为了研究在自然灾害时人们使用社交媒体平台发送的 sarcastic 语言，以提高自然语言理解。</li>
<li>methods: 该论文使用了预训练语言模型进行嘲讽检测，并提供了一个包含15,000个推文的 HurricaneSARC 数据集。</li>
<li>results: 研究结果显示，使用中间任务转移学习可以提高 HurricaneSARC 上的性能，最佳模型可以达到0.70的 F1 分数。<details>
<summary>Abstract</summary>
During natural disasters, people often use social media platforms such as Twitter to ask for help, to provide information about the disaster situation, or to express contempt about the unfolding event or public policies and guidelines. This contempt is in some cases expressed as sarcasm or irony. Understanding this form of speech in a disaster-centric context is essential to improving natural language understanding of disaster-related tweets. In this paper, we introduce HurricaneSARC, a dataset of 15,000 tweets annotated for intended sarcasm, and provide a comprehensive investigation of sarcasm detection using pre-trained language models. Our best model is able to obtain as much as 0.70 F1 on our dataset. We also demonstrate that the performance on HurricaneSARC can be improved by leveraging intermediate task transfer learning. We release our data and code at https://github.com/tsosea2/HurricaneSarc.
</details>
<details>
<summary>摘要</summary>
在自然灾害事件中，人们经常使用社交媒体平台如推特请求帮助、提供灾害情况信息或表达对事件或公共政策的负面情感。在这种情况下，人们可能会通过讽刺或反讽的方式表达自己的不满。在这种情况下，理解这种语言表达方式是提高自然语言理解灾害相关推特的关键。在这篇论文中，我们介绍了风暴沙射（HurricaneSARC）数据集，该数据集包含15,000个推特，每个推特都被标注为带有讽刺意图。我们提供了一项全面的讽刺检测研究，使用预训练语言模型。我们的最佳模型在我们的数据集上可以获得0.70的F1分。我们还证明了在风暴沙射数据集上的性能可以通过中间任务转移学习提高。我们将数据和代码发布在GitHub上，请参考https://github.com/tsosea2/HurricaneSarc。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Topological-Ordering-with-Conditional-Independence-Test-for-Limited-Time-Series"><a href="#Hierarchical-Topological-Ordering-with-Conditional-Independence-Test-for-Limited-Time-Series" class="headerlink" title="Hierarchical Topological Ordering with Conditional Independence Test for Limited Time Series"></a>Hierarchical Topological Ordering with Conditional Independence Test for Limited Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08148">http://arxiv.org/abs/2308.08148</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anpeng Wu, Haoxuan Li, Kun Kuang, Keli Zhang, Fei Wu</li>
<li>For: 本研究旨在使用导向acyclic graphs (DAGs) 揭示观测数据中的 causal 关系。* Methods: 本研究使用 topology-based 方法，首先学习变量的排序，然后消除冗余的边，以确保图形 remain acyclic。* Results: 研究人员提出了一种改进 topology-based 方法，通过 incorporating conditional instrumental variables as exogenous interventions，可以 Identify descendant nodes for each variable。HT-CIT 算法可以减少需要被截割的边的数量，并且在实际数据上得到了更好的性能。<details>
<summary>Abstract</summary>
Learning directed acyclic graphs (DAGs) to identify causal relations underlying observational data is crucial but also poses significant challenges. Recently, topology-based methods have emerged as a two-step approach to discovering DAGs by first learning the topological ordering of variables and then eliminating redundant edges, while ensuring that the graph remains acyclic. However, one limitation is that these methods would generate numerous spurious edges that require subsequent pruning. To overcome this limitation, in this paper, we propose an improvement to topology-based methods by introducing limited time series data, consisting of only two cross-sectional records that need not be adjacent in time and are subject to flexible timing. By incorporating conditional instrumental variables as exogenous interventions, we aim to identify descendant nodes for each variable. Following this line, we propose a hierarchical topological ordering algorithm with conditional independence test (HT-CIT), which enables the efficient learning of sparse DAGs with a smaller search space compared to other popular approaches. The HT-CIT algorithm greatly reduces the number of edges that need to be pruned. Empirical results from synthetic and real-world datasets demonstrate the superiority of the proposed HT-CIT algorithm.
</details>
<details>
<summary>摘要</summary>
To overcome this limitation, we propose an improvement to topology-based methods by incorporating limited time series data, consisting of only two cross-sectional records that do not need to be adjacent in time and are subject to flexible timing. By incorporating conditional instrumental variables as exogenous interventions, we aim to identify descendant nodes for each variable.We propose a hierarchical topological ordering algorithm with conditional independence test (HT-CIT), which enables the efficient learning of sparse DAGs with a smaller search space compared to other popular approaches. The HT-CIT algorithm greatly reduces the number of edges that need to be pruned.Empirical results from synthetic and real-world datasets demonstrate the superiority of the proposed HT-CIT algorithm.
</details></li>
</ul>
<hr>
<h2 id="Online-Control-for-Linear-Dynamics-A-Data-Driven-Approach"><a href="#Online-Control-for-Linear-Dynamics-A-Data-Driven-Approach" class="headerlink" title="Online Control for Linear Dynamics: A Data-Driven Approach"></a>Online Control for Linear Dynamics: A Data-Driven Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08138">http://arxiv.org/abs/2308.08138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zishun Liu, Yongxin Chen</li>
<li>for: 这个论文关注在线控制问题，特别是Linear Time-Invariant系统（LTI）中 unknown dynamics、bounded disturbance 和 adversarial cost。</li>
<li>methods: 我们提出了一种数据驱动的策略，以减少控制器的 regret。不同于模型基于方法，我们的算法不需要确定系统模型，而是利用单个噪音自由轨迹来计算干扰的积累，并使用我们设计的积累干扰控制器来做决策，其参数通过在线加权 descent 进行更新。</li>
<li>results: 我们证明了我们的算法的 regret 是 $\mathcal{O}(\sqrt{T})$，这意味着它的性能与模型基于方法相当。<details>
<summary>Abstract</summary>
This paper considers an online control problem over a linear time-invariant system with unknown dynamics, bounded disturbance, and adversarial cost. We propose a data-driven strategy to reduce the regret of the controller. Unlike model-based methods, our algorithm does not identify the system model, instead, it leverages a single noise-free trajectory to calculate the accumulation of disturbance and makes decisions using the accumulated disturbance action controller we design, whose parameters are updated by online gradient descent. We prove that the regret of our algorithm is $\mathcal{O}(\sqrt{T})$ under mild assumptions, suggesting that its performance is on par with model-based methods.
</details>
<details>
<summary>摘要</summary>
这篇论文考虑了一个在线控制问题，其中系统为线性时不变的，受到干扰和恶意成本的影响。我们提出了一种数据驱动的策略，以减少控制器的 regret。不同于模型基于方法，我们的算法不需要确定系统模型，而是利用一个干净的轨迹来计算干扰的积累，并使用我们设计的积累干扰控制器，其参数通过在线梯度下降进行更新。我们证明了我们的算法的 regret是 $\mathcal{O}(\sqrt{T})$，这意味着它的性能与模型基于方法相当。
</details></li>
</ul>
<hr>
<h2 id="Microstructure-Empowered-Stock-Factor-Extraction-and-Utilization"><a href="#Microstructure-Empowered-Stock-Factor-Extraction-and-Utilization" class="headerlink" title="Microstructure-Empowered Stock Factor Extraction and Utilization"></a>Microstructure-Empowered Stock Factor Extraction and Utilization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08135">http://arxiv.org/abs/2308.08135</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xianfeng Jiao, Zizhong Li, Chang Xu, Yang Liu, Weiqing Liu, Jiang Bian<br>for:This paper aims to effectively extract essential factors from order flow data for diverse downstream tasks across different granularities and scenarios.methods:The proposed framework consists of a Context Encoder and an Factor Extractor, using unsupervised learning methods to select important signals from the given context.results:The extracted factors are utilized for downstream tasks, demonstrating significant improvement for stock trend prediction and order execution tasks at the second and minute level, compared to existing tick-level approaches.Here’s the simplified Chinese text:for: 这篇论文目的是EXTRACTING essential factors from order flow data for 多种下游任务 Across different granularities and scenarios.methods: 该提议的框架包括Context Encoder和Factor Extractor，使用无监督学习方法选择Context中的重要信号。results: 提取的因素被用于下游任务，对stock trend prediction和订单执行任务 at the second and minute level exhibit significant improvement, compared to现有的tick-level Approaches.<details>
<summary>Abstract</summary>
High-frequency quantitative investment is a crucial aspect of stock investment. Notably, order flow data plays a critical role as it provides the most detailed level of information among high-frequency trading data, including comprehensive data from the order book and transaction records at the tick level. The order flow data is extremely valuable for market analysis as it equips traders with essential insights for making informed decisions. However, extracting and effectively utilizing order flow data present challenges due to the large volume of data involved and the limitations of traditional factor mining techniques, which are primarily designed for coarser-level stock data. To address these challenges, we propose a novel framework that aims to effectively extract essential factors from order flow data for diverse downstream tasks across different granularities and scenarios. Our method consists of a Context Encoder and an Factor Extractor. The Context Encoder learns an embedding for the current order flow data segment's context by considering both the expected and actual market state. In addition, the Factor Extractor uses unsupervised learning methods to select such important signals that are most distinct from the majority within the given context. The extracted factors are then utilized for downstream tasks. In empirical studies, our proposed framework efficiently handles an entire year of stock order flow data across diverse scenarios, offering a broader range of applications compared to existing tick-level approaches that are limited to only a few days of stock data. We demonstrate that our method extracts superior factors from order flow data, enabling significant improvement for stock trend prediction and order execution tasks at the second and minute level.
</details>
<details>
<summary>摘要</summary>
高频量资金投资是股票投资的重要方面。特别是订单流量数据提供了最详细的信息 amid高频度交易，包括订单书和交易记录的粒度水平。订单流量数据非常有价值 для市场分析，因为它们提供了对决策的重要见解。然而，从订单流量数据中提取和充分利用该数据具有挑战，主要是因为该数据的量太大，以及传统因数挖掘技术的局限性，这些技术主要是设计 для粗细度股票数据。为了解决这些挑战，我们提出了一个新的框架，旨在充分提取订单流量数据中的重要因素，并将其应用到不同的细节和情况下。我们的方法包括内容编码器和因数挖掘器。内容编码器会将目前订单流量数据段的内容嵌入学习到一个内容嵌入，考虑到预期和实际市场状态。此外，因数挖掘器会使用无监督学习方法来选择订单流量数据中最重要的信号，这些信号与大多数信号在该情况下最为不同。提取的因素则可以在不同的细节和情况下被重新利用。在实验研究中，我们的提案方法可以高效处理一整年的股票订单流量数据，提供更广泛的应用场景，比起现有的几天股票数据tick水平的方法。我们显示，我们的方法可以从订单流量数据中提取出超越性的因素，导致股票趋势预测和订单执行任务在第二和分钟级别上得到了重要改善。
</details></li>
</ul>
<hr>
<h2 id="Is-Self-Supervised-Pretraining-Good-for-Extrapolation-in-Molecular-Property-Prediction"><a href="#Is-Self-Supervised-Pretraining-Good-for-Extrapolation-in-Molecular-Property-Prediction" class="headerlink" title="Is Self-Supervised Pretraining Good for Extrapolation in Molecular Property Prediction?"></a>Is Self-Supervised Pretraining Good for Extrapolation in Molecular Property Prediction?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08129">http://arxiv.org/abs/2308.08129</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shun Takashige, Masatoshi Hanai, Toyotaro Suzumura, Limin Wang, Kenjiro Taura</li>
<li>for: 这个论文主要是为了研究如何使用自适应预训练技术提高材料属性预测的准确性。</li>
<li>methods: 这个论文使用了自适应预训练技术，首先在无标签数据上训练模型，然后在标签数据上进行目标任务训练。</li>
<li>results: 研究发现，自适应预训练可以提高模型对未观察属性值的预测，但不能准确地预测绝对属性值。<details>
<summary>Abstract</summary>
The prediction of material properties plays a crucial role in the development and discovery of materials in diverse applications, such as batteries, semiconductors, catalysts, and pharmaceuticals. Recently, there has been a growing interest in employing data-driven approaches by using machine learning technologies, in combination with conventional theoretical calculations. In material science, the prediction of unobserved values, commonly referred to as extrapolation, is particularly critical for property prediction as it enables researchers to gain insight into materials beyond the limits of available data. However, even with the recent advancements in powerful machine learning models, accurate extrapolation is still widely recognized as a significantly challenging problem. On the other hand, self-supervised pretraining is a machine learning technique where a model is first trained on unlabeled data using relatively simple pretext tasks before being trained on labeled data for target tasks. As self-supervised pretraining can effectively utilize material data without observed property values, it has the potential to improve the model's extrapolation ability. In this paper, we clarify how such self-supervised pretraining can enhance extrapolation performance.We propose an experimental framework for the demonstration and empirically reveal that while models were unable to accurately extrapolate absolute property values, self-supervised pretraining enables them to learn relative tendencies of unobserved property values and improve extrapolation performance.
</details>
<details>
<summary>摘要</summary>
material的性质预测在材料的开发和发现中扮演着关键性的角色，如电池、半导体、催化剂和药品等。最近，有越来越多的研究者开始使用数据驱动方法，将机器学习技术与传统的理论计算相结合。在物理学中，预测未知值（commonly referred to as extrapolation）是特别重要的，因为它允许研究者对材料进行深入的研究，并且不仅限于可用数据的范围。然而，即使最近的高功能机器学习模型也存在着准确预测的问题。自我超vised pretraining是一种机器学习技术，其中模型首先在无标签数据上通过简单的预TEXT tasks进行训练，然后在标签数据上进行目标任务的训练。由于自我超vised pretraining可以充分利用材料数据无需观察到的性质值，因此它有可能提高模型的预测能力。在这篇论文中，我们解释了如何使用自我超vised pretraining来提高预测性能。我们提出了一种实验框架，并通过实验证明，虽然模型无法准确预测绝对性质值，但是自我超vised pretraining使得它们学习了未知性质值的相对倾向，从而提高预测性能。
</details></li>
</ul>
<hr>
<h2 id="How-to-Mask-in-Error-Correction-Code-Transformer-Systematic-and-Double-Masking"><a href="#How-to-Mask-in-Error-Correction-Code-Transformer-Systematic-and-Double-Masking" class="headerlink" title="How to Mask in Error Correction Code Transformer: Systematic and Double Masking"></a>How to Mask in Error Correction Code Transformer: Systematic and Double Masking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08128">http://arxiv.org/abs/2308.08128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seong-Joon Park, Hee-Youl Kwak, Sang-Hyo Kim, Sunghwan Kim, Yongjune Kim, Jong-Seon No</li>
<li>for: 提高 Error Correction Code Transformer (ECCT) 的性能和计算复杂度。</li>
<li>methods: 引入新的掩码矩阵和修改 ECCT 的 transformer 架构，以提高 decoding 性能。</li>
<li>results: 对 ECCT 进行改进，实现了 state-of-the-art 的 decoding 性能，与传统的 decoding 算法相比，带来了显著的性能提升。<details>
<summary>Abstract</summary>
In communication and storage systems, error correction codes (ECCs) are pivotal in ensuring data reliability. As deep learning's applicability has broadened across diverse domains, there is a growing research focus on neural network-based decoders that outperform traditional decoding algorithms. Among these neural decoders, Error Correction Code Transformer (ECCT) has achieved the state-of-the-art performance, outperforming other methods by large margins. To further enhance the performance of ECCT, we propose two novel methods. First, leveraging the systematic encoding technique of ECCs, we introduce a new masking matrix for ECCT, aiming to improve the performance and reduce the computational complexity. Second, we propose a novel transformer architecture of ECCT called a double-masked ECCT. This architecture employs two different mask matrices in a parallel manner to learn more diverse features of the relationship between codeword bits in the masked self-attention blocks. Extensive simulation results show that the proposed double-masked ECCT outperforms the conventional ECCT, achieving the state-of-the-art decoding performance with significant margins.
</details>
<details>
<summary>摘要</summary>
在通信和存储系统中，错误修正码（ECC）是确保数据可靠性的关键。随着深度学习在多个领域的应用积极扩大，有一个增长的研究重点是基于神经网络的解码器，以超越传统的解码算法。其中，Error Correction Code Transformer（ECCT）已经实现了状态收敛性能，超过其他方法的大幅提高。为了进一步提高ECCT的性能，我们提出了两种新的方法。首先，利用ECC的系统编码技术，我们引入了一个新的面积矩阵，以提高性能并降低计算复杂性。其次，我们提出了一种新的ECCT架构，即双面Masked ECCT。这种架构在并行方式中使用了两个不同的面积矩阵，以学习codeword位数据之间的更多多样性的关系。经过广泛的 simulate结果表明，我们的双面Masked ECCT可以超越传统的ECCT，实现最佳解码性能，并且具有显著的提高。
</details></li>
</ul>
<hr>
<h2 id="S-Mixup-Structural-Mixup-for-Graph-Neural-Networks"><a href="#S-Mixup-Structural-Mixup-for-Graph-Neural-Networks" class="headerlink" title="S-Mixup: Structural Mixup for Graph Neural Networks"></a>S-Mixup: Structural Mixup for Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08097">http://arxiv.org/abs/2308.08097</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sukwonyun/s-mixup">https://github.com/sukwonyun/s-mixup</a></li>
<li>paper_authors: Junghurn Kim, Sukwon Yun, Chanyoung Park</li>
<li>for: 本研究主要针对图像分类任务进行应用混合技术，而研究在节点分类 tasks 仍然尚未得到充分的研究。</li>
<li>methods: 本文提出了一种新的结构混合方法（S-Mixup），利用图гра树神经网络（GNN）分类器来获得 pseudo-标签并且通过 edge gradient 来选择edge。</li>
<li>results: 通过对真实世界 benchmark 数据集进行了广泛的实验，我们证明了 S-Mixup 在节点分类任务中的效果，尤其是在不同类型的图像中。<details>
<summary>Abstract</summary>
Existing studies for applying the mixup technique on graphs mainly focus on graph classification tasks, while the research in node classification is still under-explored. In this paper, we propose a novel mixup augmentation for node classification called Structural Mixup (S-Mixup). The core idea is to take into account the structural information while mixing nodes. Specifically, S-Mixup obtains pseudo-labels for unlabeled nodes in a graph along with their prediction confidence via a Graph Neural Network (GNN) classifier. These serve as the criteria for the composition of the mixup pool for both inter and intra-class mixups. Furthermore, we utilize the edge gradient obtained from the GNN training and propose a gradient-based edge selection strategy for selecting edges to be attached to the nodes generated by the mixup. Through extensive experiments on real-world benchmark datasets, we demonstrate the effectiveness of S-Mixup evaluated on the node classification task. We observe that S-Mixup enhances the robustness and generalization performance of GNNs, especially in heterophilous situations. The source code of S-Mixup can be found at \url{https://github.com/SukwonYun/S-Mixup}
</details>
<details>
<summary>摘要</summary>
先前的研究主要集中在图像分类任务上应用mixup技术，而节点分类任务的研究仍然处于初期阶段。在这篇论文中，我们提出了一种新的节点分类mixup增强方法，称为结构mixup（S-Mixup）。核心思想是在混合节点时考虑结构信息。特别是，S-Mixup使用图神经网络（GNN）分类器获取未标注节点的 pseudo-标签以及其预测信度。这些 pseudo-标签 serve as the criteria for the composition of the mixup pool for both inter and intra-class mixups。此外，我们提出了基于GNN训练的边 Gradient的选择策略，用于选择混合时附加到节点上的边。经过广泛的实验，我们证明了S-Mixup可以增强GNN的Robustness和泛化性，特别在不同类型的情况下。S-Mixup的源代码可以在 \url{https://github.com/SukwonYun/S-Mixup} 找到。
</details></li>
</ul>
<hr>
<h2 id="Safety-Filter-Design-for-Neural-Network-Systems-via-Convex-Optimization"><a href="#Safety-Filter-Design-for-Neural-Network-Systems-via-Convex-Optimization" class="headerlink" title="Safety Filter Design for Neural Network Systems via Convex Optimization"></a>Safety Filter Design for Neural Network Systems via Convex Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08086">http://arxiv.org/abs/2308.08086</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shaoruchen/nn-system-psf">https://github.com/shaoruchen/nn-system-psf</a></li>
<li>paper_authors: Shaoru Chen, Kong Yao Chee, Nikolai Matni, M. Ani Hsieh, George J. Pappas</li>
<li>for: 这个论文目的是提出一种基于 convex 优化的安全筛选方法，以保证基于神经网络（NN）系统的控制器是安全的，面对添加型干扰。</li>
<li>methods: 该方法利用 NN 验证工具来上界化 NN 动力学，然后通过Robust linear MPC 搜索一个能 garantuee 约束满足的控制器。</li>
<li>results: 数学示例表明，该方法可以有效地保证 NN 系统的安全性，并且可以适应不同的模型误差。<details>
<summary>Abstract</summary>
With the increase in data availability, it has been widely demonstrated that neural networks (NN) can capture complex system dynamics precisely in a data-driven manner. However, the architectural complexity and nonlinearity of the NNs make it challenging to synthesize a provably safe controller. In this work, we propose a novel safety filter that relies on convex optimization to ensure safety for a NN system, subject to additive disturbances that are capable of capturing modeling errors. Our approach leverages tools from NN verification to over-approximate NN dynamics with a set of linear bounds, followed by an application of robust linear MPC to search for controllers that can guarantee robust constraint satisfaction. We demonstrate the efficacy of the proposed framework numerically on a nonlinear pendulum system.
</details>
<details>
<summary>摘要</summary>
随着数据的增加，已经广泛证明了神经网络（NN）可以准确地捕捉复杂系统动态，以数据驱动的方式。然而，神经网络的建筑复杂性和非线性使得Synthesize a provably safe controller是一项挑战。在这项工作中，我们提出了一种新的安全筛选器，该筛选器基于凸优化来保证系统的安全性，面对加itive disturbances，这些disturbances可以捕捉模型错误。我们的方法利用了神经网络验证的工具，将NN动态覆盖成一组线性上下文，然后通过Robust linear MPC来搜索一个能确保约束满足的控制器。我们通过数值实验示范了我们的框架在非线性护卷系统上的效果。
</details></li>
</ul>
<hr>
<h2 id="Rigid-Transformations-for-Stabilized-Lower-Dimensional-Space-to-Support-Subsurface-Uncertainty-Quantification-and-Interpretation"><a href="#Rigid-Transformations-for-Stabilized-Lower-Dimensional-Space-to-Support-Subsurface-Uncertainty-Quantification-and-Interpretation" class="headerlink" title="Rigid Transformations for Stabilized Lower Dimensional Space to Support Subsurface Uncertainty Quantification and Interpretation"></a>Rigid Transformations for Stabilized Lower Dimensional Space to Support Subsurface Uncertainty Quantification and Interpretation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08079">http://arxiv.org/abs/2308.08079</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ademide O. Mabadeje, Michael J. Pyrcz<br>for:This paper aims to improve the accuracy and repeatability of nonlinear dimensionality reduction (NDR) methods for subsurface datasets, which are characterized by big data challenges such as high dimensionality and complex relationships.methods:The proposed method employs rigid transformations to stabilize the Euclidean invariant representation of the data, integrates out-of-sample points (OOSP), and quantifies uncertainty using a stress ratio (SR) metric.results:The proposed method is validated using synthetic data, distance metrics, and real-world wells from the Duvernay Formation, and shows improved accuracy and repeatability compared to existing methods. The SR metric provides valuable insights into uncertainty, enabling better model adjustments and inferential analysis.<details>
<summary>Abstract</summary>
Subsurface datasets inherently possess big data characteristics such as vast volume, diverse features, and high sampling speeds, further compounded by the curse of dimensionality from various physical, engineering, and geological inputs. Among the existing dimensionality reduction (DR) methods, nonlinear dimensionality reduction (NDR) methods, especially Metric-multidimensional scaling (MDS), are preferred for subsurface datasets due to their inherent complexity. While MDS retains intrinsic data structure and quantifies uncertainty, its limitations include unstabilized unique solutions invariant to Euclidean transformations and an absence of out-of-sample points (OOSP) extension. To enhance subsurface inferential and machine learning workflows, datasets must be transformed into stable, reduced-dimension representations that accommodate OOSP.   Our solution employs rigid transformations for a stabilized Euclidean invariant representation for LDS. By computing an MDS input dissimilarity matrix, and applying rigid transformations on multiple realizations, we ensure transformation invariance and integrate OOSP. This process leverages a convex hull algorithm and incorporates loss function and normalized stress for distortion quantification. We validate our approach with synthetic data, varying distance metrics, and real-world wells from the Duvernay Formation. Results confirm our method's efficacy in achieving consistent LDS representations. Furthermore, our proposed "stress ratio" (SR) metric provides insight into uncertainty, beneficial for model adjustments and inferential analysis. Consequently, our workflow promises enhanced repeatability and comparability in NDR for subsurface energy resource engineering and associated big data workflows.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate text into Simplified Chinese<<SYS>>Subsurface datasets inherently possess big data characteristics such as vast volume, diverse features, and high sampling speeds, further compounded by the curse of dimensionality from various physical, engineering, and geological inputs. Among the existing dimensionality reduction (DR) methods, nonlinear dimensionality reduction (NDR) methods, especially Metric-multidimensional scaling (MDS), are preferred for subsurface datasets due to their inherent complexity. While MDS retains intrinsic data structure and quantifies uncertainty, its limitations include unstabilized unique solutions invariant to Euclidean transformations and an absence of out-of-sample points (OOSP) extension. To enhance subsurface inferential and machine learning workflows, datasets must be transformed into stable, reduced-dimension representations that accommodate OOSP.   Our solution employs rigid transformations for a stabilized Euclidean invariant representation for LDS. By computing an MDS input dissimilarity matrix, and applying rigid transformations on multiple realizations, we ensure transformation invariance and integrate OOSP. This process leverages a convex hull algorithm and incorporates loss function and normalized stress for distortion quantification. We validate our approach with synthetic data, varying distance metrics, and real-world wells from the Duvernay Formation. Results confirm our method's efficacy in achieving consistent LDS representations. Furthermore, our proposed "stress ratio" (SR) metric provides insight into uncertainty, beneficial for model adjustments and inferential analysis. Consequently, our workflow promises enhanced repeatability and comparability in NDR for subsurface energy resource engineering and associated big data workflows.
</details></li>
</ul>
<hr>
<h2 id="Decentralized-Graph-Neural-Network-for-Privacy-Preserving-Recommendation"><a href="#Decentralized-Graph-Neural-Network-for-Privacy-Preserving-Recommendation" class="headerlink" title="Decentralized Graph Neural Network for Privacy-Preserving Recommendation"></a>Decentralized Graph Neural Network for Privacy-Preserving Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08072">http://arxiv.org/abs/2308.08072</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaolin Zheng, Zhongyu Wang, Chaochao Chen, Jiashu Qian, Yao Yang</li>
<li>for: 提出了一种隐私保护的图学链接分析系统，解决了现有方法的通信效率低下和隐私泄露问题。</li>
<li>methods: 该方法包括三个阶段：图构建、本地梯度计算和全局梯度传递。在第一阶段，为每名用户构建了本地内项质量图和全局用户图。在第二阶段，用户偏好模型化并计算每个本地设备上的梯度。在第三阶段，实现了一种安全梯度分享机制，以保障用户私人数据的隐私。</li>
<li>results: 通过对三个公共数据集进行广泛的实验 validate了我们的框架在不同的情况下的一致性优于现有方法。<details>
<summary>Abstract</summary>
Building a graph neural network (GNN)-based recommender system without violating user privacy proves challenging. Existing methods can be divided into federated GNNs and decentralized GNNs. But both methods have undesirable effects, i.e., low communication efficiency and privacy leakage. This paper proposes DGREC, a novel decentralized GNN for privacy-preserving recommendations, where users can choose to publicize their interactions. It includes three stages, i.e., graph construction, local gradient calculation, and global gradient passing. The first stage builds a local inner-item hypergraph for each user and a global inter-user graph. The second stage models user preference and calculates gradients on each local device. The third stage designs a local differential privacy mechanism named secure gradient-sharing, which proves strong privacy-preserving of users' private data. We conduct extensive experiments on three public datasets to validate the consistent superiority of our framework.
</details>
<details>
<summary>摘要</summary>
建立一个基于图 neural network（GNN）的推荐系统，不让用户隐私泄露是一个挑战。现有的方法可以分为联邦GNN和分散GNN两种。但是这两种方法都有不良影响，即低通信效率和隐私泄露。本文提出了DGREC，一个新的分散GNN推荐系统，其中用户可以选择公开其互动。这个系统包括三个阶段：图建构、本地梯度计算和全球梯度传递。第一阶段在每个用户的本地内部项目图中建立了一个内部图，并在所有用户之间建立了一个全球跨用户图。第二阶段模型用户的喜好，并在每个本地设备上计算梯度。第三阶段设计了一个本地隐私保证机制，名为安全梯度分享，证明了用户隐私的严格保证。我们在三个公共数据集上进行了广泛的实验，以验证我们的框架的一致性和优化。
</details></li>
</ul>
<hr>
<h2 id="Freshness-or-Accuracy-Why-Not-Both-Addressing-Delayed-Feedback-via-Dynamic-Graph-Neural-Networks"><a href="#Freshness-or-Accuracy-Why-Not-Both-Addressing-Delayed-Feedback-via-Dynamic-Graph-Neural-Networks" class="headerlink" title="Freshness or Accuracy, Why Not Both? Addressing Delayed Feedback via Dynamic Graph Neural Networks"></a>Freshness or Accuracy, Why Not Both? Addressing Delayed Feedback via Dynamic Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08071">http://arxiv.org/abs/2308.08071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaolin Zheng, Zhongyu Wang, Chaochao Chen, Feng Zhu, Jiashu Qian<br>for: 这篇论文旨在解决在线商业系统中的延迟反馈问题，因为用户的延迟反馈通常会导致模型训练受到影响。methods: 本论文提出了一个名为延迟反馈模型化（DGDFEM）的方法，它包括三个阶段：准备一个数据管线、建立动态图和训练一个CVR预测模型。在模型训练过程中，我们提出了一种新的图解扩展方法 named HLGCN，它可以处理对于转换和非转换关系的处理。results: 我们进行了广泛的实验， validate the consistent superiority of our method on three industry datasets。<details>
<summary>Abstract</summary>
The delayed feedback problem is one of the most pressing challenges in predicting the conversion rate since users' conversions are always delayed in online commercial systems. Although new data are beneficial for continuous training, without complete feedback information, i.e., conversion labels, training algorithms may suffer from overwhelming fake negatives. Existing methods tend to use multitask learning or design data pipelines to solve the delayed feedback problem. However, these methods have a trade-off between data freshness and label accuracy. In this paper, we propose Delayed Feedback Modeling by Dynamic Graph Neural Network (DGDFEM). It includes three stages, i.e., preparing a data pipeline, building a dynamic graph, and training a CVR prediction model. In the model training, we propose a novel graph convolutional method named HLGCN, which leverages both high-pass and low-pass filters to deal with conversion and non-conversion relationships. The proposed method achieves both data freshness and label accuracy. We conduct extensive experiments on three industry datasets, which validate the consistent superiority of our method.
</details>
<details>
<summary>摘要</summary>
延迟反馈问题是在预测转化率时最为紧迫的挑战，因为在线商业系统中用户的转化都会延迟。新的数据对于连续训练有益，但是无法获得完整的反馈信息，即转化标签，训练算法可能会受到干扰性的假负样本的影响。现有方法通常使用多任务学习或设计数据管道来解决延迟反馈问题，但这些方法存在数据新鲜度和标签准确性之间的负担。在本文中，我们提出延迟反馈模型化方法（DGDFEM），它包括三个阶段：准备数据管道、建立动态图和训练转化率预测模型。在模型训练中，我们提出了一种新的图 convolution方法 named HLGCN，它利用了高通和低通滤波器来处理转化和非转化关系。我们的方法可以同时保证数据新鲜度和标签准确性。我们对三个行业数据集进行了广泛的实验，并证明了我们的方法的一致性优势。
</details></li>
</ul>
<hr>
<h2 id="Max-affine-regression-via-first-order-methods"><a href="#Max-affine-regression-via-first-order-methods" class="headerlink" title="Max-affine regression via first-order methods"></a>Max-affine regression via first-order methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08070">http://arxiv.org/abs/2308.08070</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seonho Kim, Kiryung Lee</li>
<li>for: 这个论文是研究max-affine模型的回归问题，该模型可以通过综合多个affine模型使用最大函数来生成一个piecewise线性模型。</li>
<li>methods: 论文使用了梯度下降（GD）和批处理梯度下降（SGD）来解决max-affine模型的回归问题，并进行了非假设性分析。</li>
<li>results: 研究发现，在随机观察到模型的情况下，GD和SGD在sub-Gaussian性和反射性下 converge linearly to a neighborhood of the ground truth，并且SGD在低样本场景中不仅更快速 convergence，还能够超过alternating minimization和GD的性能。<details>
<summary>Abstract</summary>
We consider regression of a max-affine model that produces a piecewise linear model by combining affine models via the max function. The max-affine model ubiquitously arises in applications in signal processing and statistics including multiclass classification, auction problems, and convex regression. It also generalizes phase retrieval and learning rectifier linear unit activation functions. We present a non-asymptotic convergence analysis of gradient descent (GD) and mini-batch stochastic gradient descent (SGD) for max-affine regression when the model is observed at random locations following the sub-Gaussianity and an anti-concentration with additive sub-Gaussian noise. Under these assumptions, a suitably initialized GD and SGD converge linearly to a neighborhood of the ground truth specified by the corresponding error bound. We provide numerical results that corroborate the theoretical finding. Importantly, SGD not only converges faster in run time with fewer observations than alternating minimization and GD in the noiseless scenario but also outperforms them in low-sample scenarios with noise.
</details>
<details>
<summary>摘要</summary>
我们考虑一种最大平均模型，它生成一个分割线性模型，通过最大函数将多个平均模型相互结合。这种最大平均模型广泛应用于信号处理和统计领域，包括多类分类、拍卖问题和凸回归。它还泛化phas retrieval和学习Rectifier Linear Unit激活函数。我们对梯度下降（GD）和批处理梯度下降（SGD）在最大平均 regresión中进行非假设性分析，当模型在随机位置上观察时，ASSUMING SUB-Gaussianity和反射激活函数。在这些假设下，初始化GD和SGD会在一个固定误差 bound 的附近Linearly converge。我们提供了数值结果，证明了这些理论发现。重要的是，SGD不仅在干净场景下更快的 converge 时间和观察 fewer than alternating minimization和GD，而且在噪声场景下也超越它们。
</details></li>
</ul>
<hr>
<h2 id="A-Reinforcement-Learning-Approach-for-Performance-aware-Reduction-in-Power-Consumption-of-Data-Center-Compute-Nodes"><a href="#A-Reinforcement-Learning-Approach-for-Performance-aware-Reduction-in-Power-Consumption-of-Data-Center-Compute-Nodes" class="headerlink" title="A Reinforcement Learning Approach for Performance-aware Reduction in Power Consumption of Data Center Compute Nodes"></a>A Reinforcement Learning Approach for Performance-aware Reduction in Power Consumption of Data Center Compute Nodes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08069">http://arxiv.org/abs/2308.08069</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/akhileshraj91/generalized_rl_anl">https://github.com/akhileshraj91/generalized_rl_anl</a></li>
<li>paper_authors: Akhilesh Raj, Swann Perarnau, Aniruddha Gokhale</li>
<li>for: 本研究旨在透过实现协调处理器的电力消耗和应用程序的性能，以减少云端资料中心的能源需求。</li>
<li>methods: 本研究使用了强化学习（Reinforcement Learning）来设计云端处理器的电力规则，并与argoNode资源管理软件库和Intel Running Average Power Limit（RAPL）硬件控制机制搭配使用。</li>
<li>results: 经过训练的代理人可以通过调整处理器的最大供电功率，以实现协调电力消耗和应用程序性能。在使用STREAM套件进行评估时，已经示出了一个可以找到平衡点的实时执行环境。<details>
<summary>Abstract</summary>
As Exascale computing becomes a reality, the energy needs of compute nodes in cloud data centers will continue to grow. A common approach to reducing this energy demand is to limit the power consumption of hardware components when workloads are experiencing bottlenecks elsewhere in the system. However, designing a resource controller capable of detecting and limiting power consumption on-the-fly is a complex issue and can also adversely impact application performance. In this paper, we explore the use of Reinforcement Learning (RL) to design a power capping policy on cloud compute nodes using observations on current power consumption and instantaneous application performance (heartbeats). By leveraging the Argo Node Resource Management (NRM) software stack in conjunction with the Intel Running Average Power Limit (RAPL) hardware control mechanism, we design an agent to control the maximum supplied power to processors without compromising on application performance. Employing a Proximal Policy Optimization (PPO) agent to learn an optimal policy on a mathematical model of the compute nodes, we demonstrate and evaluate using the STREAM benchmark how a trained agent running on actual hardware can take actions by balancing power consumption and application performance.
</details>
<details>
<summary>摘要</summary>
In this paper, we explore the use of Reinforcement Learning (RL) to design a power capping policy on cloud compute nodes using observations on current power consumption and instantaneous application performance (heartbeats). By leveraging the Argo Node Resource Management (NRM) software stack in conjunction with the Intel Running Average Power Limit (RAPL) hardware control mechanism, we design an agent to control the maximum supplied power to processors without compromising on application performance.Employing a Proximal Policy Optimization (PPO) agent to learn an optimal policy on a mathematical model of the compute nodes, we demonstrate and evaluate using the STREAM benchmark how a trained agent running on actual hardware can take actions by balancing power consumption and application performance.
</details></li>
</ul>
<hr>
<h2 id="The-Costly-Dilemma-Generalization-Evaluation-and-Cost-Optimal-Deployment-of-Large-Language-Models"><a href="#The-Costly-Dilemma-Generalization-Evaluation-and-Cost-Optimal-Deployment-of-Large-Language-Models" class="headerlink" title="The Costly Dilemma: Generalization, Evaluation and Cost-Optimal Deployment of Large Language Models"></a>The Costly Dilemma: Generalization, Evaluation and Cost-Optimal Deployment of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08061">http://arxiv.org/abs/2308.08061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abi Aryan, Aakash Kumar Nain, Andrew McMahon, Lucas Augusto Meyer, Harpreet Singh Sahota<br>for: 这个论文是为了解决在生产环境中部署机器学习模型时，常见的三个属性的问题。methods: 论文提出了一种框架，用于考虑这三个属性的关系，并为大语言模型的开发、部署和管理提供了新的思路。results: 论文表明，通过这种框架，可以帮助企业更好地评估大语言模型的投资，并且可以在生产环境中部署这些模型，以便更好地满足企业的需求。<details>
<summary>Abstract</summary>
When deploying machine learning models in production for any product/application, there are three properties that are commonly desired. First, the models should be generalizable, in that we can extend it to further use cases as our knowledge of the domain area develops. Second they should be evaluable, so that there are clear metrics for performance and the calculation of those metrics in production settings are feasible. Finally, the deployment should be cost-optimal as far as possible. In this paper we propose that these three objectives (i.e. generalization, evaluation and cost-optimality) can often be relatively orthogonal and that for large language models, despite their performance over conventional NLP models, enterprises need to carefully assess all the three factors before making substantial investments in this technology. We propose a framework for generalization, evaluation and cost-modeling specifically tailored to large language models, offering insights into the intricacies of development, deployment and management for these large language models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>模型应该可扩展，以便在未来我们对领域知识的发展中可以继续使用它。2. 模型应该可评估，以便有明确的性能指标和在生产环境中计算这些指标是可能的。3. 模型的部署应该是可最化的，尽可能地低效。在这篇论文中，我们提出了一个框架，用于模型泛化、评估和成本模型，专门针对大语言模型。这个框架可以为大语言模型的开发、部署和管理提供深入的理解。我们认为这三个目标通常是相互独立的，而且对于企业来说，在投入大语言模型技术之前，需要仔细评估这三个因素。</details></li>
</ol>
<hr>
<h2 id="Robust-Bayesian-Tensor-Factorization-with-Zero-Inflated-Poisson-Model-and-Consensus-Aggregation"><a href="#Robust-Bayesian-Tensor-Factorization-with-Zero-Inflated-Poisson-Model-and-Consensus-Aggregation" class="headerlink" title="Robust Bayesian Tensor Factorization with Zero-Inflated Poisson Model and Consensus Aggregation"></a>Robust Bayesian Tensor Factorization with Zero-Inflated Poisson Model and Consensus Aggregation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08060">http://arxiv.org/abs/2308.08060</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/klarman-cell-observatory/scbtf_experiments">https://github.com/klarman-cell-observatory/scbtf_experiments</a></li>
<li>paper_authors: Daniel Chafamo, Vignesh Shanmugam, Neriman Tokcan</li>
<li>for: 这个论文的目的是提出一种新的tensor因子分解方法，以处理高维计数数据中的空值偏好。</li>
<li>methods: 这个论文使用的方法包括Zero Inflated Poisson Tensor Factorization（ZIPTF）和Consensus Zero Inflated Poisson Tensor Factorization（C-ZIPTF），它们都是基于tensor因子分解的新方法，可以处理高维计数数据中的空值偏好。</li>
<li>results: 实验结果表明，ZIPTF和C-ZIPTF在各种 sintetic和实际的Single-cell RNA sequencing（scRNA-seq）数据中都能够成功地重建known和生物学意义的蛋白表达计划。特别是，当数据中存在高概率的空值时，ZIPTF可以达到2.4倍的准确率提高。此外，C-ZIPTF可以提高因子化结果的一致性和准确率。<details>
<summary>Abstract</summary>
Tensor factorizations (TF) are powerful tools for the efficient representation and analysis of multidimensional data. However, classic TF methods based on maximum likelihood estimation underperform when applied to zero-inflated count data, such as single-cell RNA sequencing (scRNA-seq) data. Additionally, the stochasticity inherent in TFs results in factors that vary across repeated runs, making interpretation and reproducibility of the results challenging. In this paper, we introduce Zero Inflated Poisson Tensor Factorization (ZIPTF), a novel approach for the factorization of high-dimensional count data with excess zeros. To address the challenge of stochasticity, we introduce Consensus Zero Inflated Poisson Tensor Factorization (C-ZIPTF), which combines ZIPTF with a consensus-based meta-analysis. We evaluate our proposed ZIPTF and C-ZIPTF on synthetic zero-inflated count data and synthetic and real scRNA-seq data. ZIPTF consistently outperforms baseline matrix and tensor factorization methods in terms of reconstruction accuracy for zero-inflated data. When the probability of excess zeros is high, ZIPTF achieves up to $2.4\times$ better accuracy. Additionally, C-ZIPTF significantly improves the consistency and accuracy of the factorization. When tested on both synthetic and real scRNA-seq data, ZIPTF and C-ZIPTF consistently recover known and biologically meaningful gene expression programs.
</details>
<details>
<summary>摘要</summary>
tensor分解（TF）是一种高效的工具，用于表示和分析多维数据。然而，经典的TF方法基于最大化可能性估计，在应用于零含量数据，如单元RNA测序（scRNA-seq）数据时，表现不佳。此外，TF的恒定性使得因素在重复运行中异常，这使得结果的解释和复制困难。在本文中，我们介绍了零含量ポイッソンtensor分解（ZIPTF），一种用于高维计数数据中的零含量因素分解的新方法。为了解决恒定性挑战，我们引入了consensus零含量ポイッソンtensor分解（C-ZIPTF），它将ZIPTF与consensus-based meta-analysis结合。我们对ZIPTF和C-ZIPTF进行了synthetic zero-inflated count data和synthetic和实际scRNA-seq数据的评估。ZIPTF在零含量数据上的重建精度比基eline矩阵和tensor分解方法高，当零含量probability高时，ZIPTF可以达到2.4倍的精度提高。此外，C-ZIPTF可以提高因素分解的一致性和精度。当测试在synthetic和实际scRNA-seq数据上时，ZIPTF和C-ZIPTF都可以一致地恢复知道的和生物学意义的蛋白表达程序。
</details></li>
</ul>
<hr>
<h2 id="Simple-online-learning-with-consistency-oracle"><a href="#Simple-online-learning-with-consistency-oracle" class="headerlink" title="Simple online learning with consistency oracle"></a>Simple online learning with consistency oracle</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08055">http://arxiv.org/abs/2308.08055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Kozachinskiy, Tomasz Steifer</li>
<li>for: 本研究旨在提出一种在模型中进行在线学习的算法，该模型允许学习算法只能通过一个一致性oracle访问类。</li>
<li>methods: 本研究使用了一种新的算法，该算法可以在类的Littlestone维度为$d$时 makest at most $O(256^d)$ mistake。我们的证明比前一个算法简单得多，只需要使用了基本的Littlestone维度属性。</li>
<li>results: 本研究的结果是，无论类的Littlestone维度如何大，我们的算法都可以在类中 makest at most $O(256^d)$ mistake。此外，我们还证明了Hasrati和Ben-David（ALT’23）的开题，即每个有 recursively enumerable representation 的类都可以有一个可计算的在线学习算法（可能是undefined on unrealizable samples）。<details>
<summary>Abstract</summary>
We consider online learning in the model where a learning algorithm can access the class only via the consistency oracle -- an oracle, that, at any moment, can give a function from the class that agrees with all examples seen so far. This model was recently considered by Assos et al. (COLT'23). It is motivated by the fact that standard methods of online learning rely on computing the Littlestone dimension of subclasses, a problem that is computationally intractable. Assos et al. gave an online learning algorithm in this model that makes at most $C^d$ mistakes on classes of Littlestone dimension $d$, for some absolute unspecified constant $C > 0$. We give a novel algorithm that makes at most $O(256^d)$ mistakes. Our proof is significantly simpler and uses only very basic properties of the Littlestone dimension. We also observe that there exists no algorithm in this model that makes at most $2^{d+1}-2$ mistakes. We also observe that our algorithm (as well as the algorithm of Assos et al.) solves an open problem by Hasrati and Ben-David (ALT'23). Namely, it demonstrates that every class of finite Littlestone dimension with recursively enumerable representation admits a computable online learner (that may be undefined on unrealizable samples).
</details>
<details>
<summary>摘要</summary>
我们考虑在模型中进行在网络学习，其中学习算法可以通过一个一致性 oracle 访问类别。这个 oracle 可以在任何时刻给出一个对应到所有已经看过的例子的函数。这个模型最近由Assos et al. (COLT'23) 考虑过。它是由于标准的在网络学习方法需要 Computing Littlestone 维度的问题是 computationally intractable 而提出的。Assos et al. 提出了一个在这个模型中的线上学习算法，它在类别的 Littlestone 维度为 $d$ 时会误导最多 $C^d$ 次。我们提出了一个新的算法，它在类别的 Littlestone 维度为 $d$ 时会误导最多 $O(256^d)$ 次。我们的证明比较简单，只需要使用一些非常基本的 Littlestone 维度的性质。我们还观察到，在这个模型中没有任何算法可以在类别的 Littlestone 维度为 $d$ 时误导最多 $2^{d+1}-2$ 次。此外，我们的算法（以及Assos et al. 的算法）解决了 Hasrati 和 Ben-David (ALT'23) 的开问题。即，它证明了所有具有 finite Littlestone 维度的类别，都存在可 computable 的线上学习者（可能是 undefined 的 samples）。
</details></li>
</ul>
<hr>
<h2 id="Natural-Evolution-Strategies-as-a-Black-Box-Estimator-for-Stochastic-Variational-Inference"><a href="#Natural-Evolution-Strategies-as-a-Black-Box-Estimator-for-Stochastic-Variational-Inference" class="headerlink" title="Natural Evolution Strategies as a Black Box Estimator for Stochastic Variational Inference"></a>Natural Evolution Strategies as a Black Box Estimator for Stochastic Variational Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08053">http://arxiv.org/abs/2308.08053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmad Ayaz Amin</li>
<li>for: 用于超越 varyational autoencoders（VAE）中的权重估计问题，以实现 Bayesian 推断在大数据集上的有效进行。</li>
<li>methods: 使用自然进化策略（Natural Evolution Strategies）提出的一种alternative estimator，不假设使用的分布类型，allowing for the creation of models that would otherwise not have been possible under the VAE framework。</li>
<li>results: 提出的estimator不受权重估计问题的限制，可以创建不同类型的模型，并且可以在VAE中实现更高的效果。<details>
<summary>Abstract</summary>
Stochastic variational inference and its derivatives in the form of variational autoencoders enjoy the ability to perform Bayesian inference on large datasets in an efficient manner. However, performing inference with a VAE requires a certain design choice (i.e. reparameterization trick) to allow unbiased and low variance gradient estimation, restricting the types of models that can be created. To overcome this challenge, an alternative estimator based on natural evolution strategies is proposed. This estimator does not make assumptions about the kind of distributions used, allowing for the creation of models that would otherwise not have been possible under the VAE framework.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese: Stochastic variational inference和其 derivatives in the form of variational autoencoders具有能够有效地进行 Bayesian inference on 大量数据的能力。然而，使用 VAE 进行推理需要特定的设计选择（即重parameterization trick），以实现无偏误和低幂度的Gradient估计，限制了可以创建的模型类型。为了解决这个挑战，一种基于自然进化策略的替代估计器被提议。这个估计器不会对分布类型进行假设，允许创建不可能在 VAE 框架下创建的模型。
</details></li>
</ul>
<hr>
<h2 id="Unbiased-Decisions-Reduce-Regret-Adversarial-Domain-Adaptation-for-the-Bank-Loan-Problem"><a href="#Unbiased-Decisions-Reduce-Regret-Adversarial-Domain-Adaptation-for-the-Bank-Loan-Problem" class="headerlink" title="Unbiased Decisions Reduce Regret: Adversarial Domain Adaptation for the Bank Loan Problem"></a>Unbiased Decisions Reduce Regret: Adversarial Domain Adaptation for the Bank Loan Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08051">http://arxiv.org/abs/2308.08051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elena Gal, Shaun Singh, Aldo Pacchiano, Ben Walker, Terry Lyons, Jakob Foerster</li>
<li>for: 本研究targets 实际世界中的二分类决策问题，即基于有限数据和实时决策的借款申请等问题。</li>
<li>methods: 本研究使用了对抗优化（AdOpt）来直接Address bias in the training set，通过对适应域适应来学习不偏且有用的表示。</li>
<li>results: AdOptsignificantly exceeds state-of-the-art performance on a set of challenging benchmark problems, and our experiments also provide initial evidence that the introduction of adversarial domain adaptation improves fairness in this setting.<details>
<summary>Abstract</summary>
In many real world settings binary classification decisions are made based on limited data in near real-time, e.g. when assessing a loan application. We focus on a class of these problems that share a common feature: the true label is only observed when a data point is assigned a positive label by the principal, e.g. we only find out whether an applicant defaults if we accepted their loan application. As a consequence, the false rejections become self-reinforcing and cause the labelled training set, that is being continuously updated by the model decisions, to accumulate bias. Prior work mitigates this effect by injecting optimism into the model, however this comes at the cost of increased false acceptance rate. We introduce adversarial optimism (AdOpt) to directly address bias in the training set using adversarial domain adaptation. The goal of AdOpt is to learn an unbiased but informative representation of past data, by reducing the distributional shift between the set of accepted data points and all data points seen thus far. AdOpt significantly exceeds state-of-the-art performance on a set of challenging benchmark problems. Our experiments also provide initial evidence that the introduction of adversarial domain adaptation improves fairness in this setting.
</details>
<details>
<summary>摘要</summary>
在许多实际世界中的设定下，二进制分类决策基于有限数据集，例如评审借款申请。我们关注一类问题，这些问题共同特点是：真实标签只有当数据点被主体分配正确标签时才能见到，例如只有当借款申请被accept时才能确定 defaults。这导致false rejects 自我加强，从而导致labels 训练集，由模型决策而不断更新的，受到偏见。先前的工作利用模型内置optimism来mitigate这种效应，但是这会导致准确接受率增加。我们引入对抗优化（AdOpt），直接通过对抗领域适应来纠正训练集的偏见。AdOpt的目标是学习不偏见但具有信息的过去数据表示，通过减少所有见过的数据点和接受数据点之间的分布差异来实现。AdOpt在一组复杂的benchmark问题上表现出了显著超过状态艺术性的表现。我们的实验还提供了初步证据，表明在这种设定下，引入对抗领域适应可以改善公平性。
</details></li>
</ul>
<hr>
<h2 id="Regret-Lower-Bounds-in-Multi-agent-Multi-armed-Bandit"><a href="#Regret-Lower-Bounds-in-Multi-agent-Multi-armed-Bandit" class="headerlink" title="Regret Lower Bounds in Multi-agent Multi-armed Bandit"></a>Regret Lower Bounds in Multi-agent Multi-armed Bandit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08046">http://arxiv.org/abs/2308.08046</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengfan Xu, Diego Klabjan</li>
<li>for: 本研究的目的是提供多臂投注机制下的证明Upper bound和Lower bound的方法，以及对这些方法的分析和比较。</li>
<li>methods: 本研究使用了多臂投注机制，并提供了一系列的算法和方法来解决这些问题。</li>
<li>results: 本研究提供了一系列的Lower bound，包括适用于各种设定下的下界，以及与之前的研究中的Upper bound之间的差异。<details>
<summary>Abstract</summary>
Multi-armed Bandit motivates methods with provable upper bounds on regret and also the counterpart lower bounds have been extensively studied in this context. Recently, Multi-agent Multi-armed Bandit has gained significant traction in various domains, where individual clients face bandit problems in a distributed manner and the objective is the overall system performance, typically measured by regret. While efficient algorithms with regret upper bounds have emerged, limited attention has been given to the corresponding regret lower bounds, except for a recent lower bound for adversarial settings, which, however, has a gap with let known upper bounds. To this end, we herein provide the first comprehensive study on regret lower bounds across different settings and establish their tightness. Specifically, when the graphs exhibit good connectivity properties and the rewards are stochastically distributed, we demonstrate a lower bound of order $O(\log T)$ for instance-dependent bounds and $\sqrt{T}$ for mean-gap independent bounds which are tight. Assuming adversarial rewards, we establish a lower bound $O(T^{\frac{2}{3})$ for connected graphs, thereby bridging the gap between the lower and upper bound in the prior work. We also show a linear regret lower bound when the graph is disconnected. While previous works have explored these settings with upper bounds, we provide a thorough study on tight lower bounds.
</details>
<details>
<summary>摘要</summary>
多臂猎手驱动方法的提高方法拥有可证明的上界 regret，同时对应的下界Bound也得到了广泛的研究。在这个 Setting 中，Recently, Multi-agent Multi-armed Bandit 在不同领域中得到了广泛应用，每个客户端面临分布式的猎手问题，系统性能通常由 regret 来衡量。虽然有效的算法出现了，但对应的 regret 下界却受到了限制的注意。为此，我们在这里提供了首次对 regret 下界的全面研究，并证明其紧致性。 Specifically, 当图表现出好连接性和奖励是随机分布的时候，我们展示了一个下界 bound 的规模为 $\order{ \log T}$ 的实例依赖性 bounds 和 $\sqrt{T}$ 的不相互独立 bounds，这些下界都是紧致的。在对抗性奖励情况下，我们提出了一个下界 bound 的规模为 $O(T^{ \frac{2}{3})$，这个 bound 可以将上下界之间的差异bridged。此外，我们还证明了连接图时的线性下界 bound。而在之前的工作中，只有对 upper bounds 进行了研究。我们对这些设定进行了全面的研究，并证明了其下界的紧致性。
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Analysis-of-the-Capabilities-of-Nature-inspired-Feature-Selection-Algorithms-in-Predicting-Student-Performance"><a href="#A-Comparative-Analysis-of-the-Capabilities-of-Nature-inspired-Feature-Selection-Algorithms-in-Predicting-Student-Performance" class="headerlink" title="A Comparative Analysis of the Capabilities of Nature-inspired Feature Selection Algorithms in Predicting Student Performance"></a>A Comparative Analysis of the Capabilities of Nature-inspired Feature Selection Algorithms in Predicting Student Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08574">http://arxiv.org/abs/2308.08574</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Trask</li>
<li>for: 预测学生表现，以便采取有效的预failure措施，帮助学生避免落后。</li>
<li>methods: 使用12种自然引导的算法，包括特征选择和传统机器学习算法，对3个数据集进行预测，包括单元操作数据、单一课程表现数据和同时攻读多门课程表现数据。</li>
<li>results: 结果表明，对于所有数据集，使用NIAs进行特征选择和传统机器学习算法进行分类，可以提高预测精度，同时减少特征集的大小。<details>
<summary>Abstract</summary>
Predicting student performance is key in leveraging effective pre-failure interventions for at-risk students. In this paper, I have analyzed the relative performance of a suite of 12 nature-inspired algorithms when used to predict student performance across 3 datasets consisting of instance-based clickstream data, intra-course single-course performance, and performance when taking multiple courses simultaneously. I found that, for all datasets, leveraging an ensemble approach using NIAs for feature selection and traditional ML algorithms for classification increased predictive accuracy while also reducing feature set size by 2/3.
</details>
<details>
<summary>摘要</summary>
预测学生表现是关键在实施有效的预failure措施的过程中，以帮助学生避免失败。在这篇论文中，我分析了12种自然引导的算法在预测学生表现方面的相对性，并使用这些算法来选择特征并使用传统的机器学习算法进行分类。我发现，无论 dataset 是哪一个，使用 Ensemble 方法和 NIAs 选择特征，并使用传统的机器学习算法进行分类，可以提高预测精度，同时减少特征集的大小，比例为2/3。
</details></li>
</ul>
<hr>
<h2 id="Classification-of-Data-Generated-by-Gaussian-Mixture-Models-Using-Deep-ReLU-Networks"><a href="#Classification-of-Data-Generated-by-Gaussian-Mixture-Models-Using-Deep-ReLU-Networks" class="headerlink" title="Classification of Data Generated by Gaussian Mixture Models Using Deep ReLU Networks"></a>Classification of Data Generated by Gaussian Mixture Models Using Deep ReLU Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08030">http://arxiv.org/abs/2308.08030</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tian-Yi Zhou, Xiaoming Huo</li>
<li>for: 这个论文研究了使用深度ReLU神经网络进行 ${\mathbb R}^d$ 上二分类问题的解决方案，不受模型参数的限制。</li>
<li>methods: 我们使用了深度ReLU神经网络，并提供了一个新的近似误差 bounds  для一般分析函数，以便进行分析。</li>
<li>results: 我们得到了不依赖于维度 $d$ 的非假 asymptotic upper bounds 和折补风险（类别错误率）的强化证明，表明深度ReLU 神经网络可以超越维度的咒语。<details>
<summary>Abstract</summary>
This paper studies the binary classification of unbounded data from ${\mathbb R}^d$ generated under Gaussian Mixture Models (GMMs) using deep ReLU neural networks. We obtain $\unicode{x2013}$ for the first time $\unicode{x2013}$ non-asymptotic upper bounds and convergence rates of the excess risk (excess misclassification error) for the classification without restrictions on model parameters. The convergence rates we derive do not depend on dimension $d$, demonstrating that deep ReLU networks can overcome the curse of dimensionality in classification. While the majority of existing generalization analysis of classification algorithms relies on a bounded domain, we consider an unbounded domain by leveraging the analyticity and fast decay of Gaussian distributions. To facilitate our analysis, we give a novel approximation error bound for general analytic functions using ReLU networks, which may be of independent interest. Gaussian distributions can be adopted nicely to model data arising in applications, e.g., speeches, images, and texts; our results provide a theoretical verification of the observed efficiency of deep neural networks in practical classification problems.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese translation:这篇论文研究了使用深度ReLU神经网络进行 $\mathbb{R}^d$ 中无穷数据的二分类问题，基于 Gaussian Mixture Models (GMMs)。我们得到了 $\unicode{x2013}$ 的首次非增长上界和抽象率，而这些上界不依赖维度 $d$。这表明深度ReLU 网络可以超越维度约束。大多数现有的泛化分析中的 Classification 算法都是基于固定维度的，而我们则考虑了无穷维度的情况，通过利用 Gaussian 分布的分布和快速衰减。为便于我们的分析，我们还提出了一个 novel 的 Approximation 错误 bound  для一般的分析函数，可能具有独立的 интерес。 Gaussian 分布可以很好地模型应用中的数据，如演讲、图像和文本等；我们的结果提供了实际问题中深度神经网络的理论验证。
</details></li>
</ul>
<hr>
<h2 id="Planning-to-Learn-A-Novel-Algorithm-for-Active-Learning-during-Model-Based-Planning"><a href="#Planning-to-Learn-A-Novel-Algorithm-for-Active-Learning-during-Model-Based-Planning" class="headerlink" title="Planning to Learn: A Novel Algorithm for Active Learning during Model-Based Planning"></a>Planning to Learn: A Novel Algorithm for Active Learning during Model-Based Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08029">http://arxiv.org/abs/2308.08029</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rowanlibr/sophisticated-learning">https://github.com/rowanlibr/sophisticated-learning</a></li>
<li>paper_authors: Rowan Hodson, Bruce Bassett, Charel van Hoof, Benjamin Rosman, Mark Solms, Jonathan P. Shock, Ryan Smith</li>
<li>for: 这 paper 的目的是比较 Active Inference 和 Bayesian reinforcement learning  schemes 在解决类似问题上的性能，以及提出一种 incorporating active learning during planning 的方法。</li>
<li>methods: 这 paper 使用了 Sophisticated Inference (SI) 算法和 Sophisticated Learning (SL) 算法，SI 使用了回归搜索来解决多步计划问题，而 SL 维护了对模型参数的信念变化的想法，以实现对未来观测的反思式学习。</li>
<li>results:  simulations 表明，SL 在一种生物学上levant的环境中表现出色，比 Bayes-adaptive RL 和 upper confidence bound algorithms 更高效，这些算法都使用了类似的原则（例如，导向探索和对未来观测的反思）来解决多步计划问题。<details>
<summary>Abstract</summary>
Active Inference is a recent framework for modeling planning under uncertainty. Empirical and theoretical work have now begun to evaluate the strengths and weaknesses of this approach and how it might be improved. A recent extension - the sophisticated inference (SI) algorithm - improves performance on multi-step planning problems through recursive decision tree search. However, little work to date has been done to compare SI to other established planning algorithms. SI was also developed with a focus on inference as opposed to learning. The present paper has two aims. First, we compare performance of SI to Bayesian reinforcement learning (RL) schemes designed to solve similar problems. Second, we present an extension of SI - sophisticated learning (SL) - that more fully incorporates active learning during planning. SL maintains beliefs about how model parameters would change under the future observations expected under each policy. This allows a form of counterfactual retrospective inference in which the agent considers what could be learned from current or past observations given different future observations. To accomplish these aims, we make use of a novel, biologically inspired environment designed to highlight the problem structure for which SL offers a unique solution. Here, an agent must continually search for available (but changing) resources in the presence of competing affordances for information gain. Our simulations show that SL outperforms all other algorithms in this context - most notably, Bayes-adaptive RL and upper confidence bound algorithms, which aim to solve multi-step planning problems using similar principles (i.e., directed exploration and counterfactual reasoning). These results provide added support for the utility of Active Inference in solving this class of biologically-relevant problems and offer added tools for testing hypotheses about human cognition.
</details>
<details>
<summary>摘要</summary>
aktive Inference 是一种最近的 планинг下 uncertainty 的框架。 empirical 和 theoretischen 工作已经开始评估这种方法的优劣和如何改进它。一种最近的扩展 - 智能推理（SI）算法 - 在多步 планинг问题上提高性能通过 recursively 决策树搜索。然而，到目前为止，尚未对 SI 与其他已知的 планинг算法进行比较。SI 也是在推理方面而不是学习方面进行发展。本文的两个目标是：首先， Comparing the performance of SI with Bayesian reinforcement learning (RL) schemes designed to solve similar problems; second, presenting an extension of SI - sophisticated learning (SL) - that more fully incorporates active learning during planning. SL maintains beliefs about how model parameters would change under the future observations expected under each policy, allowing a form of counterfactual retrospective inference in which the agent considers what could be learned from current or past observations given different future observations. To accomplish these aims, we make use of a novel, biologically inspired environment designed to highlight the problem structure for which SL offers a unique solution. In this environment, an agent must continually search for available (but changing) resources in the presence of competing affordances for information gain. Our simulations show that SL outperforms all other algorithms in this context - most notably, Bayes-adaptive RL and upper confidence bound algorithms, which aim to solve multi-step planning problems using similar principles (i.e., directed exploration and counterfactual reasoning). These results provide added support for the utility of Active Inference in solving this class of biologically-relevant problems and offer added tools for testing hypotheses about human cognition.
</details></li>
</ul>
<hr>
<h2 id="Potential-Energy-Advantage-of-Quantum-Economy"><a href="#Potential-Energy-Advantage-of-Quantum-Economy" class="headerlink" title="Potential Energy Advantage of Quantum Economy"></a>Potential Energy Advantage of Quantum Economy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08025">http://arxiv.org/abs/2308.08025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyu Liu, Hansheng Jiang, Zuo-Jun Max Shen</li>
<li>for: 本研究旨在探讨量子计算在能源效率方面的优势，并证明量子计算可以在能源消耗方面比经典计算更高效。</li>
<li>methods: 我们使用 Cournot 竞争模型，将能源消耗作为约束条件，并通过 Nash 均衡来证明量子计算机器可以在财务收益和能源效率两个方面比经典计算机器更高效。</li>
<li>results: 我们发现，量子计算在大规模计算中可以获得更高的能源效率优势，并且需要在大规模的操作范围内进行计算。基于实际物理参数，我们还证明了实现这种能源效率优势所需的规模的尺度。<details>
<summary>Abstract</summary>
Energy cost is increasingly crucial in the modern computing industry with the wide deployment of large-scale machine learning models and language models. For the firms that provide computing services, low energy consumption is important both from the perspective of their own market growth and the government's regulations. In this paper, we study the energy benefits of quantum computing vis-a-vis classical computing. Deviating from the conventional notion of quantum advantage based solely on computational complexity, we redefine advantage in an energy efficiency context. Through a Cournot competition model constrained by energy usage, we demonstrate quantum computing firms can outperform classical counterparts in both profitability and energy efficiency at Nash equilibrium. Therefore quantum computing may represent a more sustainable pathway for the computing industry. Moreover, we discover that the energy benefits of quantum computing economies are contingent on large-scale computation. Based on real physical parameters, we further illustrate the scale of operation necessary for realizing this energy efficiency advantage.
</details>
<details>
<summary>摘要</summary>
现代计算业务中能源成本日益重要，尤其是大规模机器学习模型和自然语言处理模型的广泛部署。为提供计算服务的公司而言，低能耗是重要的，不仅从市场增长的角度来看，也从政府的法规来看。在这篇论文中，我们研究了量子计算对于纳什平衡下的能源利益。我们偏离了传统的量子优势概念，定义了能效上的优势。使用偏处比率模型，我们示出了量子计算公司在纳什平衡下可以超越经典对手，在利润和能效性方面表现出优势。因此，量子计算可能代表计算业务更可持续的发展 paths。此外，我们发现了大规模计算的能源利益，并通过实际物理参数来证明实现这种能效优势的规模。
</details></li>
</ul>
<hr>
<h2 id="Active-Inverse-Learning-in-Stackelberg-Trajectory-Games"><a href="#Active-Inverse-Learning-in-Stackelberg-Trajectory-Games" class="headerlink" title="Active Inverse Learning in Stackelberg Trajectory Games"></a>Active Inverse Learning in Stackelberg Trajectory Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08017">http://arxiv.org/abs/2308.08017</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Yu, Jacob Levy, Negar Mehr, David Fridovich-Keil, Ufuk Topcu</li>
<li>for: 本文研究的目的是使用游戏理论学习来推断玩家的目标函数。</li>
<li>methods: 本文提出了一种活动式反向学习方法，使得领导者可以尝试不同的假设来描述追随者的目标函数。这种方法不同于现有方法，不是通过消耗观察到的行为来进行反向学习。而是通过活动地增加不同假设下的追随者的轨迹差异，以加速领导者的推断。</li>
<li>results: 在循环轨迹游戏中，提出的方法可以大幅提高领导者对不同假设下追随者轨迹的 Conditional Probability 的快速抽象。相比于随机输入，提出的领导者输入可以加速抽象的推断过程，几乎可以达到多个数量级的提速。<details>
<summary>Abstract</summary>
Game-theoretic inverse learning is the problem of inferring the players' objectives from their actions. We formulate an inverse learning problem in a Stackelberg game between a leader and a follower, where each player's action is the trajectory of a dynamical system. We propose an active inverse learning method for the leader to infer which hypothesis among a finite set of candidates describes the follower's objective function. Instead of using passively observed trajectories like existing methods, the proposed method actively maximizes the differences in the follower's trajectories under different hypotheses to accelerate the leader's inference. We demonstrate the proposed method in a receding-horizon repeated trajectory game. Compared with uniformly random inputs, the leader inputs provided by the proposed method accelerate the convergence of the probability of different hypotheses conditioned on the follower's trajectory by orders of magnitude.
</details>
<details>
<summary>摘要</summary>
<<SYS>>游戏理论反学习问题是从行为中推断玩家的目标。我们将游戏形式 Stackelberg 游戏中的领导和追随者作为例子，每个玩家的行为是动力系统的轨迹。我们提议一种活动反学习方法，以便领导者根据追随者的动力系统轨迹中的不同假设来推断追随者的目标函数。不同于现有方法，我们的方法不是通过被动观察到的轨迹来进行反学习，而是通过活动地增加追随者的轨迹下不同假设的差异，以加速领导者的推断。我们在回溯 horizon 重复轨迹游戏中应用了该方法，并与随机输入相比，该方法可以加速各个假设 conditioned on 追随者的轨迹上的概率的减少，减少至数量级。
</details></li>
</ul>
<hr>
<h2 id="GRINN-A-Physics-Informed-Neural-Network-for-solving-hydrodynamic-systems-in-the-presence-of-self-gravity"><a href="#GRINN-A-Physics-Informed-Neural-Network-for-solving-hydrodynamic-systems-in-the-presence-of-self-gravity" class="headerlink" title="GRINN: A Physics-Informed Neural Network for solving hydrodynamic systems in the presence of self-gravity"></a>GRINN: A Physics-Informed Neural Network for solving hydrodynamic systems in the presence of self-gravity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08010">http://arxiv.org/abs/2308.08010</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sayantan Auddy, Ramit Dey, Neal J. Turner, Shantanu Basu</li>
<li>for: 模拟三维自引力液体系统，用于解答宇宙学中许多基础问题，如 planet-forming 盘状物、星系形成、星系演化和宇宙大规模结构的发展。</li>
<li>methods: 利用物理学 informed neural network（PINN）的 универса适应能力，在 mesh-free 框架中实现解决时间依赖部分强 differential equations（PDEs）的问题。</li>
<li>results: 与 аналитиче解准相匹配，在线性 régime中与传统网格代码解准相差 1%，在非线性 régime中与传统网格代码解准相差 5%。 GRINN 计算时间与维度无关，与传统网格代码计算时间相比，在一维和二维计算中快得多，在三维计算中 slower 但是更准。<details>
<summary>Abstract</summary>
Modeling self-gravitating gas flows is essential to answering many fundamental questions in astrophysics. This spans many topics including planet-forming disks, star-forming clouds, galaxy formation, and the development of large-scale structures in the Universe. However, the nonlinear interaction between gravity and fluid dynamics offers a formidable challenge to solving the resulting time-dependent partial differential equations (PDEs) in three dimensions (3D). By leveraging the universal approximation capabilities of a neural network within a mesh-free framework, physics informed neural networks (PINNs) offer a new way of addressing this challenge. We introduce the gravity-informed neural network (GRINN), a PINN-based code, to simulate 3D self-gravitating hydrodynamic systems. Here, we specifically study gravitational instability and wave propagation in an isothermal gas. Our results match a linear analytic solution to within 1\% in the linear regime and a conventional grid code solution to within 5\% as the disturbance grows into the nonlinear regime. We find that the computation time of the GRINN does not scale with the number of dimensions. This is in contrast to the scaling of the grid-based code for the hydrodynamic and self-gravity calculations as the number of dimensions is increased. Our results show that the GRINN computation time is longer than the grid code in one- and two- dimensional calculations but is an order of magnitude lesser than the grid code in 3D with similar accuracy. Physics-informed neural networks like GRINN thus show promise for advancing our ability to model 3D astrophysical flows.
</details>
<details>
<summary>摘要</summary>
To address this challenge, we propose a novel approach based on physics-informed neural networks (PINNs), which leverages the universal approximation capabilities of neural networks to simulate 3D self-gravitating hydrodynamic systems. We introduce the gravity-informed neural network (GRINN), a PINN-based code that can accurately simulate 3D self-gravitating flows.In this study, we specifically focus on gravitational instability and wave propagation in an isothermal gas. Our results show that the GRINN code can accurately capture the linear analytic solution to within 1% in the linear regime and a conventional grid code solution to within 5% as the disturbance grows into the nonlinear regime. Moreover, we find that the computation time of the GRINN code does not scale with the number of dimensions, which is in contrast to the scaling of the grid-based code for the hydrodynamic and self-gravity calculations as the number of dimensions is increased.Our results demonstrate that the GRINN code is an order of magnitude faster than the grid code in 3D with similar accuracy, indicating that PINNs like GRINN hold great promise for advancing our ability to model 3D astrophysical flows.
</details></li>
</ul>
<hr>
<h2 id="BI-LAVA-Biocuration-with-Hierarchical-Image-Labeling-through-Active-Learning-and-Visual-Analysis"><a href="#BI-LAVA-Biocuration-with-Hierarchical-Image-Labeling-through-Active-Learning-and-Visual-Analysis" class="headerlink" title="BI-LAVA: Biocuration with Hierarchical Image Labeling through Active Learning and Visual Analysis"></a>BI-LAVA: Biocuration with Hierarchical Image Labeling through Active Learning and Visual Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08003">http://arxiv.org/abs/2308.08003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juan Trelles, Andrew Wentzel, William Berrios, G. Elisabeta Marai</li>
<li>for: 本研究目的是提高生物资料数据的品质和可用性，通过可视分析和人工学习策略，帮助模型建立者面对受限的实验数据和分类 hierarchy 的挑战。</li>
<li>methods: 本研究使用输入数据的自适应学习和可视分析策略，具有以下特点：（1）使用对应的数据分布、分类 hierarchy 和图像投影来表示数据;（2）运用活动学习和人工学习来处理受限的实验数据和标签;（3）透过可视分析来帮助模型建立者理解数据集和分类 hierarchy 的特点。</li>
<li>results: 经过评估，人机混合方法能够成功地帮助领域专家理解数据集和分类 hierarchy 的特点，并且可以提高数据的品质和可用性。<details>
<summary>Abstract</summary>
In the biomedical domain, taxonomies organize the acquisition modalities of scientific images in hierarchical structures. Such taxonomies leverage large sets of correct image labels and provide essential information about the importance of a scientific publication, which could then be used in biocuration tasks. However, the hierarchical nature of the labels, the overhead of processing images, the absence or incompleteness of labeled data, and the expertise required to label this type of data impede the creation of useful datasets for biocuration. From a multi-year collaboration with biocurators and text-mining researchers, we derive an iterative visual analytics and active learning strategy to address these challenges. We implement this strategy in a system called BI-LAVA Biocuration with Hierarchical Image Labeling through Active Learning and Visual Analysis. BI-LAVA leverages a small set of image labels, a hierarchical set of image classifiers, and active learning to help model builders deal with incomplete ground-truth labels, target a hierarchical taxonomy of image modalities, and classify a large pool of unlabeled images. BI-LAVA's front end uses custom encodings to represent data distributions, taxonomies, image projections, and neighborhoods of image thumbnails, which help model builders explore an unfamiliar image dataset and taxonomy and correct and generate labels. An evaluation with machine learning practitioners shows that our mixed human-machine approach successfully supports domain experts in understanding the characteristics of classes within the taxonomy, as well as validating and improving data quality in labeled and unlabeled collections.
</details>
<details>
<summary>摘要</summary>
在生物医学领域，taxonomies将科学图像收集modalities组织成层次结构。这些taxonomies利用大量正确的图像标签，提供了科学出版物的重要信息，可以用于生物CURATION任务。然而，层次性标签、图像处理过程的开销、标签数据缺失或不完整、以及标签这类数据的专业知识却使得创建有用的数据集变得困难。基于多年的合作与生物CURATORS和文本挖掘研究人员，我们提出了一种迭代式视觉分析和活动学习策略。我们实现了这种策略在BI-LAVA生物CURATION系统中。BI-LAVA利用一小sets of image标签、层次Set of image分类器和活动学习来帮助模型建立者处理不完整的真实标签、目标层次的图像模式和一大量的未标记图像。BI-LAVA的前端使用自定编码来表示数据分布、税onomy、图像投影和图像缩略图，帮助模型建立者探索未familiar的图像数据集和税onomy，并且更正和生成标签。我们与机器学习专家合作进行评估，表明我们的人机结合方法可以成功地支持领域专家理解税onomy中类别的特征，以及验证和提高标记和未标记集的数据质量。
</details></li>
</ul>
<hr>
<h2 id="A-physics-informed-machine-learning-model-for-reconstruction-of-dynamic-loads"><a href="#A-physics-informed-machine-learning-model-for-reconstruction-of-dynamic-loads" class="headerlink" title="A physics-informed machine learning model for reconstruction of dynamic loads"></a>A physics-informed machine learning model for reconstruction of dynamic loads</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08571">http://arxiv.org/abs/2308.08571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gledson Rodrigo Tondo, Igor Kavrakov, Guido Morgenthal</li>
<li>For: This paper aims to develop a probabilistic physics-informed machine-learning framework for reconstructing dynamic forces on long-span bridges based on measured deflections, velocities, or accelerations.* Methods: The proposed framework uses Gaussian process regression to model the relationship between the measured data and the dynamic forces, and can handle incomplete and contaminated data.* Results: The developed framework is applied to an aerodynamic analysis of the Great Belt East Bridge, and the results show good agreement between the applied and predicted dynamic load. The framework can be used for validation of design models and assumptions, as well as prognosis of responses to assist in damage detection and structural health monitoring.Here is the same information in Simplified Chinese:* For: 这篇论文目的是开发一种基于测量数据的可靠物理学整合机器学习框架，用于重 span 桥的动态力计算。* Methods: 提posed 框架使用 Gaussian 过程回归来模型测量数据和动态力之间的关系，并可以处理受损和污染的数据。* Results: 该框架在应用于大彩虹大桥的风动分析中获得了良好的吻合度，可以用于验证设计模型和假设，以及损害检测和结构健康监测。<details>
<summary>Abstract</summary>
Long-span bridges are subjected to a multitude of dynamic excitations during their lifespan. To account for their effects on the structural system, several load models are used during design to simulate the conditions the structure is likely to experience. These models are based on different simplifying assumptions and are generally guided by parameters that are stochastically identified from measurement data, making their outputs inherently uncertain. This paper presents a probabilistic physics-informed machine-learning framework based on Gaussian process regression for reconstructing dynamic forces based on measured deflections, velocities, or accelerations. The model can work with incomplete and contaminated data and offers a natural regularization approach to account for noise in the measurement system. An application of the developed framework is given by an aerodynamic analysis of the Great Belt East Bridge. The aerodynamic response is calculated numerically based on the quasi-steady model, and the underlying forces are reconstructed using sparse and noisy measurements. Results indicate a good agreement between the applied and the predicted dynamic load and can be extended to calculate global responses and the resulting internal forces. Uses of the developed framework include validation of design models and assumptions, as well as prognosis of responses to assist in damage detection and structural health monitoring.
</details>
<details>
<summary>摘要</summary>
长 Span 桥拱需要面对多种动态冲击 durante 其使用寿命。为了考虑这些影响结构系统的效应，设计时使用多种荷载模型来模拟结构可能经历的条件。这些模型基于不同的简化假设和由测量数据指导的参数，因此其输出具有内在的不确定性。这篇文章描述了一种基于 Gaussian 进程回归的概率物理学 informed 机器学习框架，用于重建动态力 based on 测量弯曲、速度或加速度。该模型可以处理部分完整和污染的数据，并提供自然的正则化方法来考虑测量系统中的噪声。应用于大吃水东桥的 aerodynamic 分析， numerically 计算 quasi-steady 模型下的 aerodynamic 响应，并使用稀缺和污染的测量数据来重建动态荷载。结果表明了 applied 和预测的动态荷载之间的良好一致，并可以扩展到计算全局响应和结构内部力。用于验证设计模型和假设，以及检测结构受损和监测结构健康状况。
</details></li>
</ul>
<hr>
<h2 id="Monte-Carlo-guided-Diffusion-for-Bayesian-linear-inverse-problems"><a href="#Monte-Carlo-guided-Diffusion-for-Bayesian-linear-inverse-problems" class="headerlink" title="Monte Carlo guided Diffusion for Bayesian linear inverse problems"></a>Monte Carlo guided Diffusion for Bayesian linear inverse problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07983">http://arxiv.org/abs/2308.07983</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriel Cardoso, Yazid Janati El Idrissi, Sylvain Le Corff, Eric Moulines</li>
<li>for: 这个论文是为了解决杂合知识的线性逆问题，包括计算摄影到医学影像等多个应用。</li>
<li>methods: 这个论文使用了分数基生成模型（SGM）来解决这些问题，特别是在填充问题中。</li>
<li>results: 该论文提出了一种基于贝叶斯框架的Feynman-Kac模型，并使用了顺序 Monte Carlo 方法解决这个问题。数据显示，该算法在处理杂合知识的逆问题时表现更好于基准值。<details>
<summary>Abstract</summary>
Ill-posed linear inverse problems that combine knowledge of the forward measurement model with prior models arise frequently in various applications, from computational photography to medical imaging. Recent research has focused on solving these problems with score-based generative models (SGMs) that produce perceptually plausible images, especially in inpainting problems. In this study, we exploit the particular structure of the prior defined in the SGM to formulate recovery in a Bayesian framework as a Feynman--Kac model adapted from the forward diffusion model used to construct score-based diffusion. To solve this Feynman--Kac problem, we propose the use of Sequential Monte Carlo methods. The proposed algorithm, MCGdiff, is shown to be theoretically grounded and we provide numerical simulations showing that it outperforms competing baselines when dealing with ill-posed inverse problems.
</details>
<details>
<summary>摘要</summary>
“对于具有前进模型知识的抽象线性逆问题，在计算摄影和医疗影像等多个应用中经常出现。现代研究强调使用得分生成模型（SGM）解决这些问题，以生成可见感合理的图像，特别是在填充问题中。本研究利用SGM中的特殊结构，将恢复问题转化为bayesian框架中的Feynman-Kac模型，并使用顺序 Monte Carlo方法解决。我们提出的算法MCGdiff理论上有基础，并在数值实验中证明其在不良定性逆问题中表现更好于竞争对手。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="An-Adaptive-Approach-for-Probabilistic-Wind-Power-Forecasting-Based-on-Meta-Learning"><a href="#An-Adaptive-Approach-for-Probabilistic-Wind-Power-Forecasting-Based-on-Meta-Learning" class="headerlink" title="An Adaptive Approach for Probabilistic Wind Power Forecasting Based on Meta-Learning"></a>An Adaptive Approach for Probabilistic Wind Power Forecasting Based on Meta-Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07980">http://arxiv.org/abs/2308.07980</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zichao Meng, Ye Guo, Hongbin Sun</li>
<li>for: 这个论文研究了一种适应性approach for probabilistic wind power forecasting (WPF), 包括Offline和Online学习过程。</li>
<li>methods: 在Offline学习阶段，使用内部和外部循环更新的meta-学习方法来训练基础预测模型，使其具有不同预测任务中的优秀适应性，如 probabilistic WPF with different lead times or locations。在Online学习阶段，基础预测模型被应用于在线预测，并与增量学习技术相结合。</li>
<li>results: 对于不同的预测任务和地点，提出了两种应用：一是针对不同的领先时间（temporal adaptation），二是针对新建的风力电站（spatial adaptation）。数据集实验结果表明，提出的方法具有优秀的适应性，与现有的方法相比。<details>
<summary>Abstract</summary>
This paper studies an adaptive approach for probabilistic wind power forecasting (WPF) including offline and online learning procedures. In the offline learning stage, a base forecast model is trained via inner and outer loop updates of meta-learning, which endows the base forecast model with excellent adaptability to different forecast tasks, i.e., probabilistic WPF with different lead times or locations. In the online learning stage, the base forecast model is applied to online forecasting combined with incremental learning techniques. On this basis, the online forecast takes full advantage of recent information and the adaptability of the base forecast model. Two applications are developed based on our proposed approach concerning forecasting with different lead times (temporal adaptation) and forecasting for newly established wind farms (spatial adaptation), respectively. Numerical tests were conducted on real-world wind power data sets. Simulation results validate the advantages in adaptivity of the proposed methods compared with existing alternatives.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MultiSChuBERT-Effective-Multimodal-Fusion-for-Scholarly-Document-Quality-Prediction"><a href="#MultiSChuBERT-Effective-Multimodal-Fusion-for-Scholarly-Document-Quality-Prediction" class="headerlink" title="MultiSChuBERT: Effective Multimodal Fusion for Scholarly Document Quality Prediction"></a>MultiSChuBERT: Effective Multimodal Fusion for Scholarly Document Quality Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07971">http://arxiv.org/abs/2308.07971</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gideon Maillette de Buy Wenniger, Thomas van Dongen, Lambert Schomaker</li>
<li>for: 本研究旨在提高学术文献质量预测 task 的性能，通过将文本信息和视觉信息结合起来进行预测。</li>
<li>methods: 本研究使用了一种多模态预测模型 MultiSChuBERT，它将文本模型基于块化全文本和计算BERT块编码（SChuBERT），与视觉模型基于 Inception V3 结合在一起。</li>
<li>results: 研究发现，将视觉信息和文本信息结合在一起可以显著改善预测结果。此外，研究还发现逐渐冰结视觉子模型的Weight可以降低过拟合现象，提高结果。最后，研究发现使用不同的文本嵌入模型可以进一步提高结果。<details>
<summary>Abstract</summary>
Automatic assessment of the quality of scholarly documents is a difficult task with high potential impact. Multimodality, in particular the addition of visual information next to text, has been shown to improve the performance on scholarly document quality prediction (SDQP) tasks. We propose the multimodal predictive model MultiSChuBERT. It combines a textual model based on chunking full paper text and aggregating computed BERT chunk-encodings (SChuBERT), with a visual model based on Inception V3.Our work contributes to the current state-of-the-art in SDQP in three ways. First, we show that the method of combining visual and textual embeddings can substantially influence the results. Second, we demonstrate that gradual-unfreezing of the weights of the visual sub-model, reduces its tendency to ovefit the data, improving results. Third, we show the retained benefit of multimodality when replacing standard BERT$_{\textrm{BASE}$ embeddings with more recent state-of-the-art text embedding models.   Using BERT$_{\textrm{BASE}$ embeddings, on the (log) number of citations prediction task with the ACL-BiblioMetry dataset, our MultiSChuBERT (text+visual) model obtains an $R^{2}$ score of 0.454 compared to 0.432 for the SChuBERT (text only) model. Similar improvements are obtained on the PeerRead accept/reject prediction task. In our experiments using SciBERT, scincl, SPECTER and SPECTER2.0 embeddings, we show that each of these tailored embeddings adds further improvements over the standard BERT$_{\textrm{BASE}$ embeddings, with the SPECTER2.0 embeddings performing best.
</details>
<details>
<summary>摘要</summary>
自动评估学术文献质量是一项复杂的任务，具有高的潜在影响力。在特定的情况下，添加视觉信息可以提高学术文献质量预测（SDQP）任务的性能。我们提出了多模态预测模型MultiSChuBERT，它将文本模型基于批处全篇文本和计算BERT批处编码（SChuBERT），与视觉模型基于Inception V3.0结合。我们的工作对学术文献质量预测领域做出了三种贡献。首先，我们发现将视觉和文本嵌入结合的方法可以具有显著的影响。其次，我们证明在归一化权重的过程中，慢慢地冰封视觉子模型的权重可以降低它的预测数据的偏向性，提高结果。最后，我们发现在使用更新的文本嵌入模型而不是标准的BERT$_{\textrm{BASE}$嵌入模型时，多模态性仍然具有保留的优势。使用BERT$_{\textrm{BASE}$嵌入模型，我们在ACL-BiblioMetry数据集上进行（ilog）引用数预测任务，MultiSChuBERT（文本+视觉）模型的$R^{2}$分数为0.454，比SChuBERT（文本只）模型的0.432高。类似的改进也在PeerRead接受/拒绝预测任务中获得。在我们使用SciBERT、scincl、SPECTER和SPECTER2.0嵌入模型时，我们发现每种适应嵌入模型都带来进一步的改进，SPECTER2.0嵌入模型表现最佳。
</details></li>
</ul>
<hr>
<h2 id="RAVEN-In-Context-Learning-with-Retrieval-Augmented-Encoder-Decoder-Language-Models"><a href="#RAVEN-In-Context-Learning-with-Retrieval-Augmented-Encoder-Decoder-Language-Models" class="headerlink" title="RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models"></a>RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07922">http://arxiv.org/abs/2308.07922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Huang, Wei Ping, Peng Xu, Mohammad Shoeybi, Kevin Chen-Chuan Chang, Bryan Catanzaro</li>
<li>for: 本研究探讨了基于搜索增强encoder-decoder语言模型的上下文学习能力。</li>
<li>methods: 我们首先对state-of-the-art ATLAS模型进行了全面分析，并发现其在上下文学习中存在一些局限性，主要是预训练和测试之间的匹配性不佳，以及上下文长度的限制。为了解决这些问题，我们提出了RAVEN模型，它将搜索增强的masked语言模型和前缀语言模型相结合。此外，我们还提出了Fusion-in-Context Learning来提高几何学习性能，使模型可以在不需要额外训练或模型修改的情况下，更好地利用上下文中的更多示例。</li>
<li>results: 通过广泛的实验，我们证明了RAVEN模型在certain情况下可以明显超越ATLAS模型，并在一些情况下与当前最先进的语言模型匹配。此外，我们还发现RAVEN模型在几何学习中的表现比ATLAS模型更好，尽管它具有更少的参数。我们的研究见证了 Retrieval-augmented encoder-decoder语言模型在上下文学习中的潜力，并鼓励了进一步的研究。<details>
<summary>Abstract</summary>
In this paper, we investigate the in-context learning ability of retrieval-augmented encoder-decoder language models. We first conduct a comprehensive analysis of the state-of-the-art ATLAS model and identify its limitations in in-context learning, primarily due to a mismatch between pretraining and testing, as well as a restricted context length. To address these issues, we propose RAVEN, a model that combines retrieval-augmented masked language modeling and prefix language modeling. We further introduce Fusion-in-Context Learning to enhance the few-shot performance by enabling the model to leverage more in-context examples without requiring additional training or model modifications. Through extensive experiments, we demonstrate that RAVEN significantly outperforms ATLAS and achieves results comparable to the most advanced language models in certain scenarios, despite having substantially fewer parameters. Our work underscores the potential of retrieval-augmented encoder-decoder language models for in-context learning and encourages further research in this direction.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了基于检索扩展encoder-decoder语言模型的Contextual Learning能力。我们首先对当前领先的ATLAS模型进行了全面分析，并发现其在Contextual Learning中存在一些限制，主要归结于预训练和测试的匹配性不足以及context长度的限制。为了解决这些问题，我们提议了RAVEN模型，它将检索扩展的MASKED语言模型和前缀语言模型相结合。我们还引入了Context-in-Fusion Learning，以提高少量示例的性能，使模型可以在不需要额外训练或模型修改的情况下，更好地利用更多的Contextual例子。通过广泛的实验，我们证明了RAVEN模型可以在某些场景下明显超过ATLAS模型，并达到与当前最先进语言模型相当的性能，即使有substantially fewer parameters。我们的工作强调了基于检索扩展encoder-decoder语言模型的Contextual Learning的潜力，并鼓励进一步的研究。
</details></li>
</ul>
<hr>
<h2 id="The-Regular-Expression-Inference-Challenge"><a href="#The-Regular-Expression-Inference-Challenge" class="headerlink" title="The Regular Expression Inference Challenge"></a>The Regular Expression Inference Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07899">http://arxiv.org/abs/2308.07899</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mojtaba Valizadeh, Philip John Gorinski, Ignacio Iacobacci, Martin Berger</li>
<li>for: 这 paper 是为了探讨 code&#x2F;language modelling 中的一个挑战问题，即 regular expression inference (REI)。</li>
<li>methods: 这 paper 使用的方法包括 program synthesis 技术和 GPU 加速。</li>
<li>results: 这 paper 提供了首个大规模的 REI 数据集，以及一些初步的机器学习基线。<details>
<summary>Abstract</summary>
We propose \emph{regular expression inference (REI)} as a challenge for code/language modelling, and the wider machine learning community. REI is a supervised machine learning (ML) and program synthesis task, and poses the problem of finding minimal regular expressions from examples: Given two finite sets of strings $P$ and $N$ and a cost function $\text{cost}(\cdot)$, the task is to generate an expression $r$ that accepts all strings in $P$ and rejects all strings in $N$, while no other such expression $r'$ exists with $\text{cost}(r')<\text{cost}(r)$.   REI has advantages as a challenge problem: (i) regular expressions are well-known, widely used, and a natural idealisation of code; (ii) REI's asymptotic worst-case complexity is well understood; (iii) REI has a small number of easy to understand parameters (e.g.~$P$ or $N$ cardinality, string lengths of examples, or the cost function); this lets us easily finetune REI-hardness; (iv) REI is an unsolved problem for deep learning based ML.   Recently, an REI solver was implemented on GPUs, using program synthesis techniques. This enabled, for the first time, fast generation of minimal expressions for complex REI instances. Building on this advance, we generate and publish the first large-scale datasets for REI, and devise and evaluate several initial heuristic and machine learning baselines.   We invite the community to participate and explore ML methods that learn to solve REI problems. We believe that progress in REI directly translates to code/language modelling.
</details>
<details>
<summary>摘要</summary>
我们提出了常规表达式推理（REI）作为代码/语言模型化挑战，以及更广泛的机器学习社区。REI是一个监督式机器学习（ML）和程式合成任务，挑战找到最小的常规表达式，使得所有$P$集合中的字串都被接受，而$N$集合中的字串都被拒绝，而且没有其他可以实现这一结果的表达式$r'$，则cost函数中的cost(r')>cost(r)。REI有以下优点作为挑战问题：（一）常规表达式很受欢迎，广泛使用，并是代码的自然化的理想化;（二）REI的极限最差情况的复杂度良好了解;（三）REI只有一小部分容易理解的参数（例如$P$或$N$的 cardinality，字串示例的长度，或者cost函数），这使我们可以轻松地调整REI的困难度;（四）REI是深度学习基于机器学习的未解决问题。最近，一个REI解决方案在GPU上被实现，使用程式合成技术。这使得，在一次性的情况下，快速生成复杂的REI问题中的最小表达式。我们为了进一步推动REI的研究，为首次发布了大规模的REI数据集，并设计了许多初步的假设和机器学习基准。我们邀请社区参与，探索机器学习方法可以解决REI问题。我们相信，REI的进步直接对代码/语言模型化有益。
</details></li>
</ul>
<hr>
<h2 id="SciRE-Solver-Efficient-Sampling-of-Diffusion-Probabilistic-Models-by-Score-integrand-Solver-with-Recursive-Derivative-Estimation"><a href="#SciRE-Solver-Efficient-Sampling-of-Diffusion-Probabilistic-Models-by-Score-integrand-Solver-with-Recursive-Derivative-Estimation" class="headerlink" title="SciRE-Solver: Efficient Sampling of Diffusion Probabilistic Models by Score-integrand Solver with Recursive Derivative Estimation"></a>SciRE-Solver: Efficient Sampling of Diffusion Probabilistic Models by Score-integrand Solver with Recursive Derivative Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07896">http://arxiv.org/abs/2308.07896</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shigui Li, Wei Chen, Delu Zeng<br>for:This paper proposes a high-efficiency sampler for Diffusion Probabilistic Models (DPMs), which are powerful generative models known for their ability to generate high-fidelity image samples.methods:The proposed method uses a score-based exact solution paradigm for the diffusion ODEs corresponding to the sampling process of DPMs, and introduces a new perspective on developing numerical algorithms for solving diffusion ODEs. The method also uses a recursive derivative estimation (RDE) method to reduce the estimation error.results:The proposed method, called SciRE-Solver, achieves state-of-the-art (SOTA) sampling performance with a limited number of score function evaluations (NFE) on both discrete-time and continuous-time DPMs. Specifically, the method achieves $3.48$ FID with $12$ NFE and $2.42$ FID with $20$ NFE for continuous-time DPMs on CIFAR10, respectively. The method also reaches SOTA values of $2.40$ FID with $100$ NFE for continuous-time DPM and of $3.15$ FID with $84$ NFE for discrete-time DPM on CIFAR-10, as well as of $2.17$ ($2.02$) FID with $18$ ($50$) NFE for discrete-time DPM on CelebA 64$\times$64.<details>
<summary>Abstract</summary>
Diffusion probabilistic models (DPMs) are a powerful class of generative models known for their ability to generate high-fidelity image samples. A major challenge in the implementation of DPMs is the slow sampling process. In this work, we bring a high-efficiency sampler for DPMs. Specifically, we propose a score-based exact solution paradigm for the diffusion ODEs corresponding to the sampling process of DPMs, which introduces a new perspective on developing numerical algorithms for solving diffusion ODEs. To achieve an efficient sampler, we propose a recursive derivative estimation (RDE) method to reduce the estimation error. With our proposed solution paradigm and RDE method, we propose the score-integrand solver with the convergence order guarantee as efficient solver (SciRE-Solver) for solving diffusion ODEs. The SciRE-Solver attains state-of-the-art (SOTA) sampling performance with a limited number of score function evaluations (NFE) on both discrete-time and continuous-time DPMs in comparison to existing training-free sampling algorithms. Such as, we achieve $3.48$ FID with $12$ NFE and $2.42$ FID with $20$ NFE for continuous-time DPMs on CIFAR10, respectively. Different from other samplers, SciRE-Solver has the promising potential to surpass the FIDs achieved in the original papers of some pre-trained models with a small NFEs. For example, we reach SOTA value of $2.40$ FID with $100$ NFE for continuous-time DPM and of $3.15$ FID with $84$ NFE for discrete-time DPM on CIFAR-10, as well as of $2.17$ ($2.02$) FID with $18$ ($50$) NFE for discrete-time DPM on CelebA 64$\times$64.
</details>
<details>
<summary>摘要</summary>
Diffusion probabilistic models (DPMs) 是一种强大的生成模型，能够生成高质量的图像样本。然而，DPMs 的实现受到批处的概率过程的缓慢问题。在这项工作中，我们提出了一种高效的抽象方法，用于解决DPMs 的批处概率过程。具体来说，我们提出了一种基于得分函数的精确解方法，用于解决相应的批处Diffusion ODEs。此外，我们还提出了一种级联Derivative估计（RDE）方法，以降低估计误差。通过我们的提出的解方法和RDE方法，我们提出了一种得分函数整合器（SciRE-Solver），用于解决Diffusion ODEs。SciRE-Solver 实现了状态前的（SOTA）抽样性能，并且只需要少量的得分函数评估（NFE）。例如，我们在 CIFAR10 上实现了 $3.48$ FID 和 $2.42$ FID 的抽样性能，它们分别需要 $12$ NFE 和 $20$ NFE。与其他抽样器不同，SciRE-Solver 具有可能超过原始模型的 FID 的潜在能力，例如，我们在 CIFAR10 上实现了 $2.40$ FID 和 $3.15$ FID，它们分别需要 $100$ NFE 和 $84$ NFE。此外，我们还在 CelebA 64$\times$64 上实现了 $2.17$ ($2.02$) FID 和 $18$ ($50$) NFE。
</details></li>
</ul>
<hr>
<h2 id="On-regularized-Radon-Nikodym-differentiation"><a href="#On-regularized-Radon-Nikodym-differentiation" class="headerlink" title="On regularized Radon-Nikodym differentiation"></a>On regularized Radon-Nikodym differentiation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07887">http://arxiv.org/abs/2308.07887</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duc Hoan Nguyen, Werner Zellinger, Sergei V. Pereverzyev</li>
<li>for: 本文解决了Radon-NikodymDerivative的估计问题，这个问题在各种应用中出现，如covariate shift适应、likelihood-ratio测试、mutual information估计和conditional probability估计中。</li>
<li>methods: 本文使用总体规范化 regularization scheme在 reproduce kernel Hilbert space中解决了上述问题，并确定了相应的正则化算法的收敛速率，这是通过考虑权重函数的平滑性和估计空间的容量来实现的。</li>
<li>results: 本文的理论结果通过数学实验证明，可以高精度地重construct Radon-NikodymDerivative在任何特定点。<details>
<summary>Abstract</summary>
We discuss the problem of estimating Radon-Nikodym derivatives. This problem appears in various applications, such as covariate shift adaptation, likelihood-ratio testing, mutual information estimation, and conditional probability estimation. To address the above problem, we employ the general regularization scheme in reproducing kernel Hilbert spaces. The convergence rate of the corresponding regularized algorithm is established by taking into account both the smoothness of the derivative and the capacity of the space in which it is estimated. This is done in terms of general source conditions and the regularized Christoffel functions. We also find that the reconstruction of Radon-Nikodym derivatives at any particular point can be done with high order of accuracy. Our theoretical results are illustrated by numerical simulations.
</details>
<details>
<summary>摘要</summary>
我们讨论类 Radon-Nikodym Derivative 的估计问题。这个问题在各种应用中出现，例如对应测验、likelihood-ratio 测试、共轨关系估计和conditional probability 估计。为了解决这个问题，我们使用通用的常数化方案在复原核函数空间中实现。我们证明了这个常数化算法的数据速度，通过考虑类 derivative 的平滑性和核函数空间中的容量。此外，我们也发现了在特定点进行类 Radon-Nikodym  Derivative 的重建可以实现高精度。我们的理论结果通过数学实验进行说明。
</details></li>
</ul>
<hr>
<h2 id="Back-to-Basics-A-Sanity-Check-on-Modern-Time-Series-Classification-Algorithms"><a href="#Back-to-Basics-A-Sanity-Check-on-Modern-Time-Series-Classification-Algorithms" class="headerlink" title="Back to Basics: A Sanity Check on Modern Time Series Classification Algorithms"></a>Back to Basics: A Sanity Check on Modern Time Series Classification Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07886">http://arxiv.org/abs/2308.07886</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mlgig/tabularmodelsfortsc">https://github.com/mlgig/tabularmodelsfortsc</a></li>
<li>paper_authors: Bhaskar Dhariyal, Thach Le Nguyen, Georgiana Ifrim</li>
<li>for: 这篇论文的目的是为了评估时序分类方法的基线性能。</li>
<li>methods: 这篇论文使用的方法包括经典机器学习算法（如拥平面、LDA、Random Forest）和ROCKET家族的分类器（如Rocket、MiniRocket、MultiRocket）。</li>
<li>results: 研究发现，使用Tabular模型可以在大约19%的单variate数据集和28%的多variate数据集上表现出色，并且在大约50%的数据集上达到了10个百分点的准确率。这些结果表明，在开发时序分类器时，需要考虑使用Tabular模型作为基线。这些模型快速、简单、可能比较容易理解和部署。<details>
<summary>Abstract</summary>
The state-of-the-art in time series classification has come a long way, from the 1NN-DTW algorithm to the ROCKET family of classifiers. However, in the current fast-paced development of new classifiers, taking a step back and performing simple baseline checks is essential. These checks are often overlooked, as researchers are focused on establishing new state-of-the-art results, developing scalable algorithms, and making models explainable. Nevertheless, there are many datasets that look like time series at first glance, but classic algorithms such as tabular methods with no time ordering may perform better on such problems. For example, for spectroscopy datasets, tabular methods tend to significantly outperform recent time series methods. In this study, we compare the performance of tabular models using classic machine learning approaches (e.g., Ridge, LDA, RandomForest) with the ROCKET family of classifiers (e.g., Rocket, MiniRocket, MultiRocket). Tabular models are simple and very efficient, while the ROCKET family of classifiers are more complex and have state-of-the-art accuracy and efficiency among recent time series classifiers. We find that tabular models outperform the ROCKET family of classifiers on approximately 19% of univariate and 28% of multivariate datasets in the UCR/UEA benchmark and achieve accuracy within 10 percentage points on about 50% of datasets. Our results suggest that it is important to consider simple tabular models as baselines when developing time series classifiers. These models are very fast, can be as effective as more complex methods and may be easier to understand and deploy.
</details>
<details>
<summary>摘要</summary>
现代时间序列分类技术已经进步很远，从1NN-DTW算法到ROCKET家族的分类器。然而，在当前的快速发展新的分类器，需要从时刻停下来，进行简单的基准检查。这些检查通常被忽略，因为研究人员对于创造新的state-of-the-art结果、开发可扩展的算法以及使模型可读性有着紧迫的需求。然而，有许多数据集看起来像时间序列，但 классические算法如表格方法无时间排序可能在这些问题上表现更好。例如，对于spectroscopy数据集，表格方法在许多情况下会表现出色。在这种研究中，我们比较了使用经典机器学习方法（如ridge、LDA、RandomForest）的表格模型与ROCKET家族的分类器（如Rocket、MiniRocket、MultiRocket）的性能。表格模型简单、很有效，而ROCKET家族的分类器则是更复杂，在最近的时间序列分类器中具有state-of-the-art的准确率和效率。我们发现，表格模型在UCRC/UEA数据集上约占19%的单variate数据集和28%的多variate数据集上表现较佳，并且在大约50%的数据集上达到了准确率在10个百分点之间。我们的结果表明，在开发时间序列分类器时，需要考虑使用简单的表格模型作为基准。这些模型很快速，可能与更复杂的方法相当有效，而且可能更容易理解和部署。
</details></li>
</ul>
<hr>
<h2 id="The-Challenge-of-Fetal-Cardiac-MRI-Reconstruction-Using-Deep-Learning"><a href="#The-Challenge-of-Fetal-Cardiac-MRI-Reconstruction-Using-Deep-Learning" class="headerlink" title="The Challenge of Fetal Cardiac MRI Reconstruction Using Deep Learning"></a>The Challenge of Fetal Cardiac MRI Reconstruction Using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07885">http://arxiv.org/abs/2308.07885</a></li>
<li>repo_url: None</li>
<li>paper_authors: Denis Prokopenko, Kerstin Hammernik, Thomas Roberts, David F A Lloyd, Daniel Rueckert, Joseph V Hajnal</li>
<li>for: 这个研究旨在提高非扫描kt-SENSE重建质量，使用深度学习方法来恢复不完整扫描的数据。</li>
<li>methods: 这个研究使用了深度学习网络来重建kt-SENSE样式所获取的数据，并对不同网络架构和训练策略进行了研究。</li>
<li>results: 研究发现，使用多架构和训练策略可以提高模型的性能，但是这些模型仍然不能准确地捕捉婴儿心脏的动态特征。<details>
<summary>Abstract</summary>
Dynamic free-breathing fetal cardiac MRI is one of the most challenging modalities, which requires high temporal and spatial resolution to depict rapid changes in a small fetal heart. The ability of deep learning methods to recover undersampled data could help to optimise the kt-SENSE acquisition strategy and improve non-gated kt-SENSE reconstruction quality. In this work, we explore supervised deep learning networks for reconstruction of kt-SENSE style acquired data using an extensive in vivo dataset. Having access to fully-sampled low-resolution multi-coil fetal cardiac MRI, we study the performance of the networks to recover fully-sampled data from undersampled data. We consider model architectures together with training strategies taking into account their application in the real clinical setup used to collect the dataset to enable networks to recover prospectively undersampled data. We explore a set of modifications to form a baseline performance evaluation for dynamic fetal cardiac MRI on real data. We systematically evaluate the models on coil-combined data to reveal the effect of the suggested changes to the architecture in the context of fetal heart properties. We show that the best-performers recover a detailed depiction of the maternal anatomy on a large scale, but the dynamic properties of the fetal heart are under-represented. Training directly on multi-coil data improves the performance of the models, allows their prospective application to undersampled data and makes them outperform CTFNet introduced for adult cardiac cine MRI. However, these models deliver similar qualitative performances recovering the maternal body very well but underestimating the dynamic properties of fetal heart. This dynamic feature of fast change of fetal heart that is highly localised suggests both more targeted training and evaluation methods might be needed for fetal heart application.
</details>
<details>
<summary>摘要</summary>
“几何受测疫苗Cardiac MRI是最具挑战性的测疫方法，需要高度的时间和空间分辨率，以呈现迅速变化的小胎心脏。深度学习方法可以复原缺少样本的数据，可以帮助优化kt-SENSE取样策略和提高非锁定kt-SENSE重建品质。在这个工作中，我们使用了对应式深度学习网络来重建kt-SENSE式取得的数据，使用了实验室中的广泛生物实验数据。由于我们有完整的低分辨率多晶粒胎心MRI数据，我们可以研究深度学习网络是否可以从缺少样本中获取完整的数据。我们考虑了网络架构和训练策略，以便在实际的临床设置中执行。我们系统地评估了这些模型，并且使用了多晶粒数据进行训练，以便在未来遇到缺少样本时进行预测。我们发现这些最佳performer可以精确地重建大规模的母体 анатоми，但是儿 heart的动力学特性受到了忽略。这些模型在训练 directly on multi-coil data 时表现更好，并且可以在缺少样本情况下进行预测。但是，这些模型对儿 heart的动力学特性做出了不充分的评估。这个儿 heart的快速变化和高度地方化的特性表明，为了对儿 heart进行评估，可能需要更加特定的训练和评估方法。”
</details></li>
</ul>
<hr>
<h2 id="A-Trustable-LSTM-Autoencoder-Network-for-Cyberbullying-Detection-on-Social-Media-Using-Synthetic-Data"><a href="#A-Trustable-LSTM-Autoencoder-Network-for-Cyberbullying-Detection-on-Social-Media-Using-Synthetic-Data" class="headerlink" title="A Trustable LSTM-Autoencoder Network for Cyberbullying Detection on Social Media Using Synthetic Data"></a>A Trustable LSTM-Autoencoder Network for Cyberbullying Detection on Social Media Using Synthetic Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09722">http://arxiv.org/abs/2308.09722</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mst Shapna Akter, Hossain Shahriar, Alfredo Cuzzocrea</li>
<li>for: 本研究旨在提出一种可靠的LSTM-Autoencoder网络，用于社交媒体上的虐待推测。</li>
<li>methods: 本研究使用了人工生成的数据集，并对英语、印地语和孟加拉语三种语言进行实验验证。使用了LSTM、BiLSTM、LSTM-Autoencoder、Word2vec、BERT和GPT-2等模型进行比较。</li>
<li>results: 研究发现，提出的LSTM-Autoencoder网络在所有数据集上表现最佳，具有95%的准确率。与前一些相关研究相比，本研究的结果具有状态的前进。<details>
<summary>Abstract</summary>
Social media cyberbullying has a detrimental effect on human life. As online social networking grows daily, the amount of hate speech also increases. Such terrible content can cause depression and actions related to suicide. This paper proposes a trustable LSTM-Autoencoder Network for cyberbullying detection on social media using synthetic data. We have demonstrated a cutting-edge method to address data availability difficulties by producing machine-translated data. However, several languages such as Hindi and Bangla still lack adequate investigations due to a lack of datasets. We carried out experimental identification of aggressive comments on Hindi, Bangla, and English datasets using the proposed model and traditional models, including Long Short-Term Memory (LSTM), Bidirectional Long Short-Term Memory (BiLSTM), LSTM-Autoencoder, Word2vec, Bidirectional Encoder Representations from Transformers (BERT), and Generative Pre-trained Transformer 2 (GPT-2) models. We employed evaluation metrics such as f1-score, accuracy, precision, and recall to assess the models performance. Our proposed model outperformed all the models on all datasets, achieving the highest accuracy of 95%. Our model achieves state-of-the-art results among all the previous works on the dataset we used in this paper.
</details>
<details>
<summary>摘要</summary>
社交媒体cyberbullying有着恶势力影响人类生活。随着在线社交网络的日常增长，讨厌言语的数量也在增加。这种凶杀内容可能导致抑郁和自杀行为。本文提议一种可靠的LSTM-Autoencoder网络来检测社交媒体上的cyberbullying，使用合成数据。我们已经实现了数据可用性问题的解决方法，生成了机器翻译数据。然而，一些语言，如希ن迪和孟加拉语，仍然缺乏足够的调查，因为数据缺乏。我们对印地语、孟加拉语和英语数据集进行了实验性的攻击性评论标识，使用我们提出的模型和传统模型，包括Long Short-Term Memory（LSTM）、Bidirectional Long Short-Term Memory（BiLSTM）、LSTM-Autoencoder、Word2vec、Bidirectional Encoder Representations from Transformers（BERT）和Generative Pre-trained Transformer 2（GPT-2）模型。我们使用了评价指标，如f1-score、准确率、精度和回归率，评估模型的表现。我们的提议模型在所有数据集上表现出色，实现了最高的准确率95%。我们的模型在所有之前的作品中获得了状态的杰出成绩。
</details></li>
</ul>
<hr>
<h2 id="Towards-Temporal-Edge-Regression-A-Case-Study-on-Agriculture-Trade-Between-Nations"><a href="#Towards-Temporal-Edge-Regression-A-Case-Study-on-Agriculture-Trade-Between-Nations" class="headerlink" title="Towards Temporal Edge Regression: A Case Study on Agriculture Trade Between Nations"></a>Towards Temporal Edge Regression: A Case Study on Agriculture Trade Between Nations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07883">http://arxiv.org/abs/2308.07883</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/scylj1/gnn_edge_regression">https://github.com/scylj1/gnn_edge_regression</a></li>
<li>paper_authors: Lekang Jiang, Caiqi Zhang, Farimah Poursafaei, Shenyang Huang</li>
<li>for: This paper is written for exploring the application of Graph Neural Networks (GNNs) to edge regression tasks in both static and dynamic settings, specifically focusing on predicting food and agriculture trade values between nations.</li>
<li>methods: The paper introduces three simple yet strong baselines and comprehensively evaluates one static and three dynamic GNN models using the UN Trade dataset.</li>
<li>results: The experimental results show that the baselines exhibit remarkably strong performance across various settings, highlighting the inadequacy of existing GNNs. Additionally, the paper finds that TGN outperforms other GNN models, suggesting TGN is a more appropriate choice for edge regression tasks. Furthermore, the proportion of negative edges in the training samples significantly affects the test performance.Here is the information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了探索图神经网络（GNNs）在边 regression 任务中的应用，特别是针对食品和农业贸易值 между国家。</li>
<li>methods: 论文引入三种简单强大的基线，并对一个静态和三个动态 GNN 模型进行了全面的评估，使用 UN 贸易 dataset。</li>
<li>results: 实验结果显示，基线在不同设置下表现出色，强调现有 GNN 的不足。此外，论文发现 TGN 在边 regression 任务中表现更出色，建议 TGN 是更适合的选择。此外，论文发现训练样本中负边的比例对测试性能产生了显著的影响。<details>
<summary>Abstract</summary>
Recently, Graph Neural Networks (GNNs) have shown promising performance in tasks on dynamic graphs such as node classification, link prediction and graph regression. However, few work has studied the temporal edge regression task which has important real-world applications. In this paper, we explore the application of GNNs to edge regression tasks in both static and dynamic settings, focusing on predicting food and agriculture trade values between nations. We introduce three simple yet strong baselines and comprehensively evaluate one static and three dynamic GNN models using the UN Trade dataset. Our experimental results reveal that the baselines exhibit remarkably strong performance across various settings, highlighting the inadequacy of existing GNNs. We also find that TGN outperforms other GNN models, suggesting TGN is a more appropriate choice for edge regression tasks. Moreover, we note that the proportion of negative edges in the training samples significantly affects the test performance. The companion source code can be found at: https://github.com/scylj1/GNN_Edge_Regression.
</details>
<details>
<summary>摘要</summary>
近期，图神经网络（GNNs）在动态图像任务中表现出色，包括节点分类、链接预测和图像 regression。然而，有少量研究探讨了时间Edge regression任务，它在实际世界中具有重要应用。在这篇论文中，我们探讨了 GNNs 在静态和动态设置下的边 regression 任务，专注于预测国家之间的食品和农业贸易价值。我们提出三种简单却强大的基elines，并且广泛评估了一个静态 GNN 模型和三个动态 GNN 模型，使用 UN Trade 数据集进行实验。我们的实验结果表明，基elines 在不同设置下具有惊人的表现，反映现有 GNNs 的不足。此外，我们发现预测样本中负边的比例对测试性能产生了重要的影响。 companion 的源代码可以在：https://github.com/scylj1/GNN_Edge_Regression 找到。
</details></li>
</ul>
<hr>
<h2 id="Synthesizing-Political-Zero-Shot-Relation-Classification-via-Codebook-Knowledge-NLI-and-ChatGPT"><a href="#Synthesizing-Political-Zero-Shot-Relation-Classification-via-Codebook-Knowledge-NLI-and-ChatGPT" class="headerlink" title="Synthesizing Political Zero-Shot Relation Classification via Codebook Knowledge, NLI, and ChatGPT"></a>Synthesizing Political Zero-Shot Relation Classification via Codebook Knowledge, NLI, and ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07876">http://arxiv.org/abs/2308.07876</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/snowood1/zero-shot-plover">https://github.com/snowood1/zero-shot-plover</a></li>
<li>paper_authors: Yibo Hu, Erick Skorupa Parolin, Latifur Khan, Patrick T. Brandt, Javier Osorio, Vito J. D’Orazio</li>
<li>for: 本研究旨在提高政治事件代码分类的效率和可扩展性，通过利用知识从已有的注释编目中提取 transferred learning 和已有专家知识。</li>
<li>methods: 本研究使用了 Zero-shot approach，包括一种基于自然语言理解 (NLI) 的新方法 named ZSP，它采用了树查询框架，将任务分解成上下文、Modalities 和类别异常级别。</li>
<li>results: 对于细化的 Rootcode 分类，ZSP 实现了40%的 F1 分数提升，与 supervised BERT 模型相比，ZSP 的性能相对稳定，可用于事件记录验证和 ontology 开发。<details>
<summary>Abstract</summary>
Recent supervised models for event coding vastly outperform pattern-matching methods. However, their reliance solely on new annotations disregards the vast knowledge within expert databases, hindering their applicability to fine-grained classification. To address these limitations, we explore zero-shot approaches for political event ontology relation classification, by leveraging knowledge from established annotation codebooks. Our study encompasses both ChatGPT and a novel natural language inference (NLI) based approach named ZSP. ZSP adopts a tree-query framework that deconstructs the task into context, modality, and class disambiguation levels. This framework improves interpretability, efficiency, and adaptability to schema changes. By conducting extensive experiments on our newly curated datasets, we pinpoint the instability issues within ChatGPT and highlight the superior performance of ZSP. ZSP achieves an impressive 40% improvement in F1 score for fine-grained Rootcode classification. ZSP demonstrates competitive performance compared to supervised BERT models, positioning it as a valuable tool for event record validation and ontology development. Our work underscores the potential of leveraging transfer learning and existing expertise to enhance the efficiency and scalability of research in the field.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Emotion-Embeddings-unicode-x2014-Learning-Stable-and-Homogeneous-Abstractions-from-Heterogeneous-Affective-Datasets"><a href="#Emotion-Embeddings-unicode-x2014-Learning-Stable-and-Homogeneous-Abstractions-from-Heterogeneous-Affective-Datasets" class="headerlink" title="Emotion Embeddings $\unicode{x2014}$ Learning Stable and Homogeneous Abstractions from Heterogeneous Affective Datasets"></a>Emotion Embeddings $\unicode{x2014}$ Learning Stable and Homogeneous Abstractions from Heterogeneous Affective Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07871">http://arxiv.org/abs/2308.07871</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sven Buechel, Udo Hahn</li>
<li>for: 这篇论文旨在提出一种统一计算模型，用于处理不同表达形式和标签类型的人类情感表达。</li>
<li>methods: 该模型使用训练程序学习共享幂等表达空间，以独立于不同自然语言、通信模式、媒体和表达标签格式，并且可以应用于不同的模型架构。</li>
<li>results: 实验结果表明，该方法可以实现数据和标签类型之间的协同性，无需增加预测质量的损害，同时提供了可重用、可解释和灵活的模型。<details>
<summary>Abstract</summary>
Human emotion is expressed in many communication modalities and media formats and so their computational study is equally diversified into natural language processing, audio signal analysis, computer vision, etc. Similarly, the large variety of representation formats used in previous research to describe emotions (polarity scales, basic emotion categories, dimensional approaches, appraisal theory, etc.) have led to an ever proliferating diversity of datasets, predictive models, and software tools for emotion analysis. Because of these two distinct types of heterogeneity, at the expressional and representational level, there is a dire need to unify previous work on increasingly diverging data and label types. This article presents such a unifying computational model. We propose a training procedure that learns a shared latent representation for emotions, so-called emotion embeddings, independent of different natural languages, communication modalities, media or representation label formats, and even disparate model architectures. Experiments on a wide range of heterogeneous affective datasets indicate that this approach yields the desired interoperability for the sake of reusability, interpretability and flexibility, without penalizing prediction quality. Code and data are archived under https://doi.org/10.5281/zenodo.7405327 .
</details>
<details>
<summary>摘要</summary>
人类情感表达在多种通信modalities和媒体格式中表现出来，因此计算研究也在自然语言处理、音频信号分析、计算机视觉等领域得到了多样化的应用。在过去的研究中，用于描述情感的不同格式（如负责度维度、基本情绪类别、维度方法、评估理论等）导致了数据集、预测模型和软件工具的总体化，从而产生了不断增长的多样化问题。为了解决这两种不同的多样性，即表达层次和表示层次的多样性，这篇文章提出了一种统一的计算模型。我们提议一种在不同的自然语言、通信modalities、媒体和表示格式之间学习共享的潜在表达（emotion embeddings）的训练方法，无论是不同的语言、模式、媒体或表示格式，都可以学习到共享的表达特征。在各种多样化的情感数据集上进行了广泛的实验，结果表明，这种方法可以实现数据集之间的可 reuse、可 interpretability和灵活性，而不会增加预测质量的损失。代码和数据可以在https://doi.org/10.5281/zenodo.7405327上找到。
</details></li>
</ul>
<hr>
<h2 id="Brain-Inspired-Computational-Intelligence-via-Predictive-Coding"><a href="#Brain-Inspired-Computational-Intelligence-via-Predictive-Coding" class="headerlink" title="Brain-Inspired Computational Intelligence via Predictive Coding"></a>Brain-Inspired Computational Intelligence via Predictive Coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07870">http://arxiv.org/abs/2308.07870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tommaso Salvatori, Ankur Mali, Christopher L. Buckley, Thomas Lukasiewicz, Rajesh P. N. Rao, Karl Friston, Alexander Ororbia</li>
<li>for: This paper aims to explore the potential of predictive coding (PC) in addressing the limitations of deep neural networks in machine learning and computational intelligence.</li>
<li>methods: The paper surveys the literature on PC and its applications in cognitive control, robotics, and variational inference, highlighting its exciting properties and potential value for the machine learning community.</li>
<li>results: The paper hopes to foreground research in PC-inspired machine learning and encourage further exploration of its potential in the future of computational intelligence.<details>
<summary>Abstract</summary>
Artificial intelligence (AI) is rapidly becoming one of the key technologies of this century. The majority of results in AI thus far have been achieved using deep neural networks trained with the error backpropagation learning algorithm. However, the ubiquitous adoption of this approach has highlighted some important limitations such as substantial computational cost, difficulty in quantifying uncertainty, lack of robustness, unreliability, and biological implausibility. It is possible that addressing these limitations may require schemes that are inspired and guided by neuroscience theories. One such theory, called predictive coding (PC), has shown promising performance in machine intelligence tasks, exhibiting exciting properties that make it potentially valuable for the machine learning community: PC can model information processing in different brain areas, can be used in cognitive control and robotics, and has a solid mathematical grounding in variational inference, offering a powerful inversion scheme for a specific class of continuous-state generative models. With the hope of foregrounding research in this direction, we survey the literature that has contributed to this perspective, highlighting the many ways that PC might play a role in the future of machine learning and computational intelligence at large.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）在这个世纪 rapid becoming 一种关键技术。大多数 AI 结果都是使用深度神经网络，通过错误反propagation 学习算法训练得到的。然而，普遍采用这种方法的限制已经吸引了人们的注意，包括计算成本很高、难以量化不确定性、缺乏可靠性和生物学可能性。可能需要解决这些限制的方法可能是基于神经科学理论的。一种这种理论是预测编码（PC），它在机器智能任务中表现出了扎实的性能，并且具有许多可能有用的特性，例如：PC 可以模型不同脑区的信息处理，可以应用于认知控制和机器人学习，并且具有强大的数学基础，可以通过变量推理来解决特定类型的连续状态生成模型。希望通过这篇文章，各位能够更好地了解 PC 在机器学习和计算智能领域的前景，并且能够更好地发挥研究。因此，我们在这篇文章中survey了一些与这种视角相关的文献，并 highlighted 它们在未来机器学习和计算智能领域的可能性。
</details></li>
</ul>
<hr>
<h2 id="Graph-Structured-Kernel-Design-for-Power-Flow-Learning-using-Gaussian-Processes"><a href="#Graph-Structured-Kernel-Design-for-Power-Flow-Learning-using-Gaussian-Processes" class="headerlink" title="Graph-Structured Kernel Design for Power Flow Learning using Gaussian Processes"></a>Graph-Structured Kernel Design for Power Flow Learning using Gaussian Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07867">http://arxiv.org/abs/2308.07867</a></li>
<li>repo_url: None</li>
<li>paper_authors: Parikshit Pareek, Deepjyoti Deka, Sidhant Misra</li>
<li>for: 这篇论文旨在开发一种基于物理学的图structured kernel，用于电流流动学习 using Gaussian Process（GP）。</li>
<li>methods: 该kernel，称为顶点度kernel（VDK），基于网络图或结构的 latent decomposition 来描述电压注入关系。而VDK的设计不需要解决优化问题，从而提高效率。此外，作者还提出了一种图reducible方法，用于获得VDK表示形式中的更少项。</li>
<li>results: 作者通过实验表明，提案的 VDK-GP 可以在中等规模500-Bus和大规模1354-Bus电力系统上实现更 than two fold 样本复杂度减少，相比整个 GP。此外，作者还提出了一种新的网络滑块活动学习算法，可以快速地适应 VDK 的学习。在测试预测中，该算法可以比Random Trial 的平均性能提高两倍，在中等规模500-Bus系统上，并在大规模1354-Bus系统上达到最佳性能的 10% 。此外，作者还证明了提案的方法在不同数据集上的uncertainty quantification应用中的性能。<details>
<summary>Abstract</summary>
This paper presents a physics-inspired graph-structured kernel designed for power flow learning using Gaussian Process (GP). The kernel, named the vertex-degree kernel (VDK), relies on latent decomposition of voltage-injection relationship based on the network graph or topology. Notably, VDK design avoids the need to solve optimization problems for kernel search. To enhance efficiency, we also explore a graph-reduction approach to obtain a VDK representation with lesser terms. Additionally, we propose a novel network-swipe active learning scheme, which intelligently selects sequential training inputs to accelerate the learning of VDK. Leveraging the additive structure of VDK, the active learning algorithm performs a block-descent type procedure on GP's predictive variance, serving as a proxy for information gain. Simulations demonstrate that the proposed VDK-GP achieves more than two fold sample complexity reduction, compared to full GP on medium scale 500-Bus and large scale 1354-Bus power systems. The network-swipe algorithm outperforms mean performance of 500 random trials on test predictions by two fold for medium-sized 500-Bus systems and best performance of 25 random trials for large-scale 1354-Bus systems by 10%. Moreover, we demonstrate that the proposed method's performance for uncertainty quantification applications with distributionally shifted testing data sets.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Impression-Aware-Recommender-Systems"><a href="#Impression-Aware-Recommender-Systems" class="headerlink" title="Impression-Aware Recommender Systems"></a>Impression-Aware Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07857">http://arxiv.org/abs/2308.07857</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fernando B. Pérez Maurera, Maurizio Ferrari Dacrema, Pablo Castells, Paolo Cremonesi<br>for: 这个论文旨在探讨基于印象（过去推荐的项目）的推荐系统，以提高推荐系统的质量。methods: 本文使用系统性文献综述方法，对推荐系统使用印象进行三个基本的研究角度：推荐器、数据集和评价方法。results: 本文对各种推荐系统使用印象进行了详细的介绍，还分析了各种数据集和评价方法。最后，本文提出了一些未解决的问题和未来研究方向，强调在文献中缺失的方面可以在未来的研究中进行深入探讨。<details>
<summary>Abstract</summary>
Novel data sources bring new opportunities to improve the quality of recommender systems. Impressions are a novel data source containing past recommendations (shown items) and traditional interactions. Researchers may use impressions to refine user preferences and overcome the current limitations in recommender systems research. The relevance and interest of impressions have increased over the years; hence, the need for a review of relevant work on this type of recommenders. We present a systematic literature review on recommender systems using impressions, focusing on three fundamental angles in research: recommenders, datasets, and evaluation methodologies. We provide three categorizations of papers describing recommenders using impressions, present each reviewed paper in detail, describe datasets with impressions, and analyze the existing evaluation methodologies. Lastly, we present open questions and future directions of interest, highlighting aspects missing in the literature that can be addressed in future works.
</details>
<details>
<summary>摘要</summary>
新的数据源带来了改善推荐系统质量的新机遇。印象是一种新的数据源，包含过去的推荐（显示的项目）和传统的交互。研究人员可以使用印象来细化用户喜好，超越当前的推荐系统研究的限制。在几年内，印象的相关性和兴趣度有所增加，因此需要对这类推荐器进行系统性的文献回顾。本文提供了对推荐系统使用印象的系统性文献回顾，重点是三个基本的研究方向：推荐器、数据集和评估方法ologies。我们提供了三种描述推荐器使用印象的论文分类，详细介绍每篇评论文章，描述含印象的数据集，并分析现有的评估方法。最后，我们提出了未解决的问题和未来方向，强调文献中缺失的方面，可以在未来的研究中进行深入探究。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/16/cs.LG_2023_08_16/" data-id="clorjzl8p00o1f1881sg1e4wm" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/16/eess.IV_2023_08_16/" class="article-date">
  <time datetime="2023-08-16T09:00:00.000Z" itemprop="datePublished">2023-08-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/16/eess.IV_2023_08_16/">eess.IV - 2023-08-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Prediction-of-post-radiotherapy-recurrence-volumes-in-head-and-neck-squamous-cell-carcinoma-using-3D-U-Net-segmentation"><a href="#Prediction-of-post-radiotherapy-recurrence-volumes-in-head-and-neck-squamous-cell-carcinoma-using-3D-U-Net-segmentation" class="headerlink" title="Prediction of post-radiotherapy recurrence volumes in head and neck squamous cell carcinoma using 3D U-Net segmentation"></a>Prediction of post-radiotherapy recurrence volumes in head and neck squamous cell carcinoma using 3D U-Net segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08396">http://arxiv.org/abs/2308.08396</a></li>
<li>repo_url: None</li>
<li>paper_authors: Denis Kutnár, Ivan R Vogelius, Katrin Elisabet Håkansson, Jens Petersen, Jeppe Friborg, Lena Specht, Mogens Bernsdorf, Anita Gothelf, Claus Kristensen, Abraham George Smith<br>For:* The paper aims to investigate the use of Convolutional Neural Networks (CNNs) to predict locoregional recurrences (LRR) in head and neck squamous cell carcinoma (HNSCC) patients based on pre-treatment imaging.Methods:* The study uses pre-treatment 18F-fluorodeoxyglucose positron emission tomography (FDG-PET)&#x2F;computed tomography (CT) scans to train a CNN to predict LRR volumes.* The dataset is divided into training, validation, and test sets.* The CNN is trained from scratch and compared to a pre-trained CNN and a SUVmax threshold approach.Results:* The SUVmax threshold method had a median volume of 4.6 cubic centimeters (cc) and included 5 out of 7 relapse origin points.* The GTV contour and best CNN segmentations included the relapse origin 6 out of 7 times with median volumes of 28 and 18 cc, respectively.* The CNN included the same or greater number of relapse volume points of origin (POs) with significantly smaller relapse volumes.I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
Locoregional recurrences (LRR) are still a frequent site of treatment failure for head and neck squamous cell carcinoma (HNSCC) patients.   Identification of high risk subvolumes based on pretreatment imaging is key to biologically targeted radiation therapy. We investigated the extent to which a Convolutional neural network (CNN) is able to predict LRR volumes based on pre-treatment 18F-fluorodeoxyglucose positron emission tomography (FDG-PET)/computed tomography (CT) scans in HNSCC patients and thus the potential to identify biological high risk volumes using CNNs.   For 37 patients who had undergone primary radiotherapy for oropharyngeal squamous cell carcinoma, five oncologists contoured the relapse volumes on recurrence CT scans. Datasets of pre-treatment FDG-PET/CT, gross tumour volume (GTV) and contoured relapse for each of the patients were randomly divided into training (n=23), validation (n=7) and test (n=7) datasets. We compared a CNN trained from scratch, a pre-trained CNN, a SUVmax threshold approach, and using the GTV directly.   The SUVmax threshold method included 5 out of the 7 relapse origin points within a volume of median 4.6 cubic centimetres (cc). Both the GTV contour and best CNN segmentations included the relapse origin 6 out of 7 times with median volumes of 28 and 18 cc respectively.   The CNN included the same or greater number of relapse volume POs, with significantly smaller relapse volumes. Our novel findings indicate that CNNs may predict LRR, yet further work on dataset development is required to attain clinically useful prediction accuracy.
</details>
<details>
<summary>摘要</summary>
“头颈癌细胞瘤（HNSCC）患者的局部再次发病（LRR）仍然是治疗失败的常见现象。我们使用深度学习模型（CNN）来预测LRR объем based on pre-treatment 18F-fluorodeoxyglucose пози트рон辐射 Tomatoes（FDG-PET）/计算机成像（CT）扫描图。我们通过对37名受到首次放疗治疗的口咽癌患者的再次发病CT扫描图进行评估，并对每个患者的预处理FDG-PET/CT、GTV和评估关键点进行分类。我们对比了从零开始训练的CNN、预训练CNN、SUVmax阈值方法和直接使用GTV。我们发现，SUVmax阈值方法包含5个再次发病起点， median 4.6立方厘米（cc）内，而GTV和最佳CNN分 segmentation都包含6个再次发病起点， median 28和18cc分别。CNN包含了同样或更多的再次发病体积点，并且再次发病体积更小。我们的新发现表明，CNN可能预测LRR，但是进一步的数据集开发是需要以获得临床有用的预测精度。”
</details></li>
</ul>
<hr>
<h2 id="DeepContrast-Deep-Tissue-Contrast-Enhancement-using-Synthetic-Data-Degradations-and-OOD-Model-Predictions"><a href="#DeepContrast-Deep-Tissue-Contrast-Enhancement-using-Synthetic-Data-Degradations-and-OOD-Model-Predictions" class="headerlink" title="DeepContrast: Deep Tissue Contrast Enhancement using Synthetic Data Degradations and OOD Model Predictions"></a>DeepContrast: Deep Tissue Contrast Enhancement using Synthetic Data Degradations and OOD Model Predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08365">http://arxiv.org/abs/2308.08365</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nuno Pimpão Martins, Yannis Kalaidzidis, Marino Zerial, Florian Jug</li>
<li>For: The paper aims to improve the quality of microscopy images by addressing the problem of image degradation, specifically blurring and contrast loss, which is a major challenge in life science research.* Methods: The authors use a deep learning approach, training a neural network to learn the inverse of the degradation function, and demonstrate that the network can be used out-of-distribution to improve the quality of less severely degraded images. They also explore the effect of iterative predictions and find that a balance between contrast improvement and retention of image details is necessary.* Results: The authors show that their method can improve the quality of microscopy images, especially in deeper regions of thick samples, and demonstrate the effectiveness of their approach through experiments using real microscopy images.<details>
<summary>Abstract</summary>
Microscopy images are crucial for life science research, allowing detailed inspection and characterization of cellular and tissue-level structures and functions. However, microscopy data are unavoidably affected by image degradations, such as noise, blur, or others. Many such degradations also contribute to a loss of image contrast, which becomes especially pronounced in deeper regions of thick samples. Today, best performing methods to increase the quality of images are based on Deep Learning approaches, which typically require ground truth (GT) data during training. Our inability to counteract blurring and contrast loss when imaging deep into samples prevents the acquisition of such clean GT data. The fact that the forward process of blurring and contrast loss deep into tissue can be modeled, allowed us to propose a new method that can circumvent the problem of unobtainable GT data. To this end, we first synthetically degraded the quality of microscopy images even further by using an approximate forward model for deep tissue image degradations. Then we trained a neural network that learned the inverse of this degradation function from our generated pairs of raw and degraded images. We demonstrated that networks trained in this way can be used out-of-distribution (OOD) to improve the quality of less severely degraded images, e.g. the raw data imaged in a microscope. Since the absolute level of degradation in such microscopy images can be stronger than the additional degradation introduced by our forward model, we also explored the effect of iterative predictions. Here, we observed that in each iteration the measured image contrast kept improving while detailed structures in the images got increasingly removed. Therefore, dependent on the desired downstream analysis, a balance between contrast improvement and retention of image details has to be found.
</details>
<details>
<summary>摘要</summary>
μ可微镜图像是生命科学研究中不可或缺的，它允许详细的检查和Characterization of cellular和组织水平结构和功能。然而，μ可微镜数据必然受到图像噪声、模糊和其他问题的影响，这些问题也会导致图像尺度的损失。今天，最佳的图像质量提高方法都基于深度学习方法，这些方法通常需要训练时的标准数据（GT数据）。我们无法对深入采样中的图像进行减震和减少尺度损失，因此无法获得这些干净的GT数据。由于前向过程中的减震和尺度损失可以被模型化，我们提出了一种新的方法，可以缺省GT数据的问题。我们首先使用一种近似的前向模型来进一步降低μ可微镜图像的质量。然后，我们使用这些生成的对应图像对进行了深度学习网络的训练，这些网络可以学习对这种降低函数的逆操作。我们展示了这种方法可以在OOD（out-of-distribution）情况下提高μ可微镜图像的质量。由于μ可微镜图像中的噪声水平可能比我们添加的降低水平更高，我们也研究了反复预测的效果。在每次预测中，我们发现图像尺度的改善随着图像细节的消失而增加。因此，根据下游分析的需求，需要找到和保留图像细节的平衡。
</details></li>
</ul>
<hr>
<h2 id="GAEI-UNet-Global-Attention-and-Elastic-Interaction-U-Net-for-Vessel-Image-Segmentation"><a href="#GAEI-UNet-Global-Attention-and-Elastic-Interaction-U-Net-for-Vessel-Image-Segmentation" class="headerlink" title="GAEI-UNet: Global Attention and Elastic Interaction U-Net for Vessel Image Segmentation"></a>GAEI-UNet: Global Attention and Elastic Interaction U-Net for Vessel Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08345">http://arxiv.org/abs/2308.08345</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruiqiang Xiao, Zhuoyue Wan</li>
<li>for: 这个研究的目的是提高静脉影像分类的精度和可靠性，以提供医疗界更加准确和可靠的诊断工具。</li>
<li>methods: 本研究提出了一个新的模型，即GAEI-UNet，它结合了全球注意力和弹性互动的技术来优化高水平的 semantic 理解，以提高小血管的精确分类。</li>
<li>results: 在DRIVE retinal vessel dataset 上进行评估，GAEI-UNet 能够在 SE 和小血管之间的连接性方面表现出色，而且不需要对计算复杂性进行增加。<details>
<summary>Abstract</summary>
Vessel image segmentation plays a pivotal role in medical diagnostics, aiding in the early detection and treatment of vascular diseases. While segmentation based on deep learning has shown promising results, effectively segmenting small structures and maintaining connectivity between them remains challenging. To address these limitations, we propose GAEI-UNet, a novel model that combines global attention and elastic interaction-based techniques. GAEI-UNet leverages global spatial and channel context information to enhance high-level semantic understanding within the U-Net architecture, enabling precise segmentation of small vessels. Additionally, we adopt an elastic interaction-based loss function to improve connectivity among these fine structures. By capturing the forces generated by misalignment between target and predicted shapes, our model effectively learns to preserve the correct topology of vessel networks. Evaluation on retinal vessel dataset -- DRIVE demonstrates the superior performance of GAEI-UNet in terms of SE and connectivity of small structures, without significantly increasing computational complexity. This research aims to advance the field of vessel image segmentation, providing more accurate and reliable diagnostic tools for the medical community. The implementation code is available on Code.
</details>
<details>
<summary>摘要</summary>
船体图像分割在医疗诊断中扮演着关键角色，帮助早期发现和治疗血管疾病。然而，基于深度学习的分割方法仍然存在小结构分割和保持结构连接的挑战。为解决这些限制，我们提出了GAEI-UNet模型，该模型结合全球注意力和弹性互动基于技术。GAEI-UNet 利用全球空间和通道信息来增强 U-Net 架构中高级别 semantics的理解，以提高小血管的精确分割。此外，我们采用了弹性互动基于损失函数，以提高小结构之间的连接。通过捕捉目标形态与预测形态之间的弹性力，我们的模型能够保持血管网络的正确拓扑结构。DRIVE retinal vessel dataset 的评估表明，GAEI-UNet 在小结构连接和精确分割方面具有显著优势，而无需显著增加计算复杂性。本研究旨在提高船体图像分割领域的准确性和可靠性，为医疗界提供更多的可靠的诊断工具。代码可在 Code.
</details></li>
</ul>
<hr>
<h2 id="Denoising-Diffusion-Probabilistic-Model-for-Retinal-Image-Generation-and-Segmentation"><a href="#Denoising-Diffusion-Probabilistic-Model-for-Retinal-Image-Generation-and-Segmentation" class="headerlink" title="Denoising Diffusion Probabilistic Model for Retinal Image Generation and Segmentation"></a>Denoising Diffusion Probabilistic Model for Retinal Image Generation and Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08339">http://arxiv.org/abs/2308.08339</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aaleka/retree">https://github.com/aaleka/retree</a></li>
<li>paper_authors: Alnur Alimanov, Md Baharul Islam</li>
<li>For: The paper aims to provide a novel dataset and method for retinal image segmentation, which can help detect and diagnose various eye, blood circulation, and brain-related diseases.* Methods: The proposed method uses a Denoising Diffusion Probabilistic Model (DDPM) to generate retinal images and vessel trees, and a two-stage DDPM to guide the generation of fundus images from given vessel trees and random distribution.* Results: The proposed dataset, called Retinal Trees (ReTree), has been evaluated quantitatively and qualitatively, and the results show that the DDPM outperformed GANs in image synthesis. The dataset and source code are available online for further research and validation.<details>
<summary>Abstract</summary>
Experts use retinal images and vessel trees to detect and diagnose various eye, blood circulation, and brain-related diseases. However, manual segmentation of retinal images is a time-consuming process that requires high expertise and is difficult due to privacy issues. Many methods have been proposed to segment images, but the need for large retinal image datasets limits the performance of these methods. Several methods synthesize deep learning models based on Generative Adversarial Networks (GAN) to generate limited sample varieties. This paper proposes a novel Denoising Diffusion Probabilistic Model (DDPM) that outperformed GANs in image synthesis. We developed a Retinal Trees (ReTree) dataset consisting of retinal images, corresponding vessel trees, and a segmentation network based on DDPM trained with images from the ReTree dataset. In the first stage, we develop a two-stage DDPM that generates vessel trees from random numbers belonging to a standard normal distribution. Later, the model is guided to generate fundus images from given vessel trees and random distribution. The proposed dataset has been evaluated quantitatively and qualitatively. Quantitative evaluation metrics include Frechet Inception Distance (FID) score, Jaccard similarity coefficient, Cohen's kappa, Matthew's Correlation Coefficient (MCC), precision, recall, F1-score, and accuracy. We trained the vessel segmentation model with synthetic data to validate our dataset's efficiency and tested it on authentic data. Our developed dataset and source code is available at https://github.com/AAleka/retree.
</details>
<details>
<summary>摘要</summary>
专家利用眼球图像和血管树来检测和诊断不同的眼部、血液和脑部疾病。然而，手动分割眼球图像是一项时间consuming和需要高度专业知识的过程，另外，隐私问题也是一个大的问题。许多方法已经被提出来分割图像，但是因为缺乏大量眼球图像数据，这些方法的性能受到限制。本文提出了一种新的吸引 diffusion 概率模型（DDPM），它在图像生成方面超越了生成 adversarial 网络（GAN）的性能。我们开发了一个名为“Retinal Trees”（ReTree）的数据集，该数据集包括眼球图像、相应的血管树和基于 DDPM 的分割网络。在第一个阶段，我们开发了一种两个阶段的 DDPM，它可以将随机数据从标准差分布中生成血管树。然后，模型被引导以使用给定的血管树和随机分布生成眼球图像。我们对该数据集进行了量化和质量上的评估。量化评估指标包括Fréchet Inception Distance（FID）分数、Jaccard相似度系数、Cohen的κ值、Matthew的相似度系数（MCC）、精度、 recall、F1-score 和准确率。我们使用合成数据来验证我们的数据集的效果，并对真实数据进行测试。我们开发的数据集和源代码可以在 <https://github.com/AAleka/retree> 上下载。
</details></li>
</ul>
<hr>
<h2 id="ECPC-IDS-A-benchmark-endometrail-cancer-PET-CT-image-dataset-for-evaluation-of-semantic-segmentation-and-detection-of-hypermetabolic-regions"><a href="#ECPC-IDS-A-benchmark-endometrail-cancer-PET-CT-image-dataset-for-evaluation-of-semantic-segmentation-and-detection-of-hypermetabolic-regions" class="headerlink" title="ECPC-IDS:A benchmark endometrail cancer PET&#x2F;CT image dataset for evaluation of semantic segmentation and detection of hypermetabolic regions"></a>ECPC-IDS:A benchmark endometrail cancer PET&#x2F;CT image dataset for evaluation of semantic segmentation and detection of hypermetabolic regions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08313">http://arxiv.org/abs/2308.08313</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dechao Tang, Xuanyi Li, Tianming Du, Deguo Ma, Zhiyu Ma, Hongzan Sun, Marcin Grzegorzek, Huiyan Jiang, Chen Li</li>
<li>for: 这个论文目的是提供一个大量多图像的 ENDOMETRIAL CANCER PET&#x2F;CT影像数据集，以便研究者利用计算机支持的诊断技术来提高诊断精度和对象ivity。</li>
<li>methods: 这个论文使用的方法包括五种经典的深度学习语义 segmentation 方法和六种深度学习对象检测方法，以证明不同方法在 ECPC-IDS 上的差异。</li>
<li>results: 这个论文通过对 ECPC-IDS 进行广泛的实验，证明了深度学习基于语义 segmentation 和对象检测方法的效果，并且这些方法可以帮助研究者开发新的计算机支持诊断技术，从而为临床医生和患者带来很大的 benefit.<details>
<summary>Abstract</summary>
Endometrial cancer is one of the most common tumors in the female reproductive system and is the third most common gynecological malignancy that causes death after ovarian and cervical cancer. Early diagnosis can significantly improve the 5-year survival rate of patients. With the development of artificial intelligence, computer-assisted diagnosis plays an increasingly important role in improving the accuracy and objectivity of diagnosis, as well as reducing the workload of doctors. However, the absence of publicly available endometrial cancer image datasets restricts the application of computer-assisted diagnostic techniques.In this paper, a publicly available Endometrial Cancer PET/CT Image Dataset for Evaluation of Semantic Segmentation and Detection of Hypermetabolic Regions (ECPC-IDS) are published. Specifically, the segmentation section includes PET and CT images, with a total of 7159 images in multiple formats. In order to prove the effectiveness of segmentation methods on ECPC-IDS, five classical deep learning semantic segmentation methods are selected to test the image segmentation task. The object detection section also includes PET and CT images, with a total of 3579 images and XML files with annotation information. Six deep learning methods are selected for experiments on the detection task.This study conduct extensive experiments using deep learning-based semantic segmentation and object detection methods to demonstrate the differences between various methods on ECPC-IDS. As far as we know, this is the first publicly available dataset of endometrial cancer with a large number of multiple images, including a large amount of information required for image and target detection. ECPC-IDS can aid researchers in exploring new algorithms to enhance computer-assisted technology, benefiting both clinical doctors and patients greatly.
</details>
<details>
<summary>摘要</summary>
结直肠癌是女性生殖系统中最常见的肿瘤，是生殖系统癌症的第三大死因，只次于卵巢癌和子宫癌。早期诊断可以提高患者5年生存率。随着人工智能的发展，计算机助诊技术在诊断准确性和 объектив性方面扮演着越来越重要的角色，同时也减轻了医生的工作负担。然而，由于缺乏公共可用的结直肠癌图像数据集，计算机助诊技术的应用受到了限制。在这篇论文中，一个公共可用的结直肠癌PET/CT图像数据集（ECPC-IDS）被发布。具体来说，分别包括PET和CT图像，总计7159张图像，多种格式。为证明ECPC-IDS上 segmentation 方法的效果，我们选择了5种经典的深度学习semantic segmentation方法进行测试图像 segmentation 任务。对象检测部分也包括PET和CT图像，总计3579张图像和XML文件中的注释信息。为了证明对ECPC-IDS的不同方法的差异，我们选择了6种深度学习方法进行实验。这个研究通过深度学习基于semantic segmentation和对象检测方法进行广泛的实验，以示ECPC-IDS上不同方法之间的差异。我们知道，ECPC-IDS是目前所有公共可用的结直肠癌图像数据集中最大的一个，包括大量的图像信息和目标检测信息。ECPC-IDS可以帮助研究人员探索新的算法，以提高计算机助诊技术，对医生和病人都是非常有利。
</details></li>
</ul>
<hr>
<h2 id="OnUVS-Online-Feature-Decoupling-Framework-for-High-Fidelity-Ultrasound-Video-Synthesis"><a href="#OnUVS-Online-Feature-Decoupling-Framework-for-High-Fidelity-Ultrasound-Video-Synthesis" class="headerlink" title="OnUVS: Online Feature Decoupling Framework for High-Fidelity Ultrasound Video Synthesis"></a>OnUVS: Online Feature Decoupling Framework for High-Fidelity Ultrasound Video Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08269">http://arxiv.org/abs/2308.08269</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Zhou, Dong Ni, Ao Chang, Xinrui Zhou, Rusi Chen, Yanlin Chen, Lian Liu, Jiamin Liang, Yuhao Huang, Tong Han, Zhe Liu, Deng-Ping Fan, Xin Yang</li>
<li>For: The paper aims to address the challenges of synthesizing high-fidelity ultrasound (US) videos for medical education and diagnosis, specifically the limited availability of specific US video cases and the need to accurately animate dynamic anatomic structures while preserving image fidelity.* Methods: The proposed method, called OnUVS, is an online feature-decoupling framework that incorporates anatomic information into keypoint learning, uses a dual-decoder to decouple content and textural features, and employs a multiple-feature discriminator to enhance the sharpness and fine details of the generated videos. Additionally, the method constrains the motion trajectories of keypoints during online learning to enhance the fluidity of the generated videos.* Results: The paper reports that OnUVS synthesizes US videos with high fidelity, as validated and demonstrated through user studies on in-house echocardiographic and pelvic floor US videos.<details>
<summary>Abstract</summary>
Ultrasound (US) imaging is indispensable in clinical practice. To diagnose certain diseases, sonographers must observe corresponding dynamic anatomic structures to gather comprehensive information. However, the limited availability of specific US video cases causes teaching difficulties in identifying corresponding diseases, which potentially impacts the detection rate of such cases. The synthesis of US videos may represent a promising solution to this issue. Nevertheless, it is challenging to accurately animate the intricate motion of dynamic anatomic structures while preserving image fidelity. To address this, we present a novel online feature-decoupling framework called OnUVS for high-fidelity US video synthesis. Our highlights can be summarized by four aspects. First, we introduced anatomic information into keypoint learning through a weakly-supervised training strategy, resulting in improved preservation of anatomical integrity and motion while minimizing the labeling burden. Second, to better preserve the integrity and textural information of US images, we implemented a dual-decoder that decouples the content and textural features in the generator. Third, we adopted a multiple-feature discriminator to extract a comprehensive range of visual cues, thereby enhancing the sharpness and fine details of the generated videos. Fourth, we constrained the motion trajectories of keypoints during online learning to enhance the fluidity of generated videos. Our validation and user studies on in-house echocardiographic and pelvic floor US videos showed that OnUVS synthesizes US videos with high fidelity.
</details>
<details>
<summary>摘要</summary>
Ultrasound (US) 影像是临床实践中不可或缺的。为了诊断某些疾病，医学技术员需要观察相应的动态解剖结构，以获取全面的信息。然而，有限的特定US视频案例的可用性会导致教学困难，从而可能影响疾病检测率。 synthesizing US videos 可能是一个有前途的解决方案。然而，复杂动态解剖结构的精准动画化是一项挑战。为解决这个问题，我们提出了一种在线特征解coupling框架called OnUVS，用于高精度US видео生成。我们的主要特点包括：First, we introduced anatomic information into keypoint learning through a weakly-supervised training strategy, resulting in improved preservation of anatomical integrity and motion while minimizing the labeling burden.Second, to better preserve the integrity and textural information of US images, we implemented a dual-decoder that decouples the content and textural features in the generator.Third, we adopted a multiple-feature discriminator to extract a comprehensive range of visual cues, thereby enhancing the sharpness and fine details of the generated videos.Fourth, we constrained the motion trajectories of keypoints during online learning to enhance the fluidity of generated videos. Our validation and user studies on in-house echocardiographic and pelvic floor US videos showed that OnUVS synthesizes US videos with high fidelity.
</details></li>
</ul>
<hr>
<h2 id="Neural-Spherical-Harmonics-for-structurally-coherent-continuous-representation-of-diffusion-MRI-signal"><a href="#Neural-Spherical-Harmonics-for-structurally-coherent-continuous-representation-of-diffusion-MRI-signal" class="headerlink" title="Neural Spherical Harmonics for structurally coherent continuous representation of diffusion MRI signal"></a>Neural Spherical Harmonics for structurally coherent continuous representation of diffusion MRI signal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08210">http://arxiv.org/abs/2308.08210</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tom Hendriks, Anna Vilanova, Maxime Chamberland</li>
<li>for: 这个论文的目的是提出一种基于幂等函数（NeSH）的扩展层次模型，用于重构扩散磁共振成像（dMRI）数据，以便利用人脑的结构协调性，并且只使用单个试验者的数据。</li>
<li>methods: 这个论文使用神经网络来Parameterize NeSH series来表示单个试验者的dMRI信号，这个信号是连续的在角度和空间频谱中。这种方法可以去除梯度图像中的噪声，并且 fibre orientation distribution functions （FOD）显示了纤维轨迹中的缓和的方向变化。</li>
<li>results: 该方法可以计算mean diffusivity、fractional anisotropy和总显示纤维density等结果，这些结果可以通过单一的模型架构和 hyperparameter 的调整来实现。此外，在angular和空间频谱上的upsampling也可以实现 reconstruction 的同等或更好的结果。<details>
<summary>Abstract</summary>
We present a novel way to model diffusion magnetic resonance imaging (dMRI) datasets, that benefits from the structural coherence of the human brain while only using data from a single subject. Current methods model the dMRI signal in individual voxels, disregarding the intervoxel coherence that is present. We use a neural network to parameterize a spherical harmonics series (NeSH) to represent the dMRI signal of a single subject from the Human Connectome Project dataset, continuous in both the angular and spatial domain. The reconstructed dMRI signal using this method shows a more structurally coherent representation of the data. Noise in gradient images is removed and the fiber orientation distribution functions show a smooth change in direction along a fiber tract. We showcase how the reconstruction can be used to calculate mean diffusivity, fractional anisotropy, and total apparent fiber density. These results can be achieved with a single model architecture, tuning only one hyperparameter. In this paper we also demonstrate how upsampling in both the angular and spatial domain yields reconstructions that are on par or better than existing methods.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法来模型扩散磁共振成像（dMRI）数据集，该方法利用人脑的结构准确性而不需要多个试验者的数据。现有方法对dMRI信号在单个 vozuel 中进行模型化，忽略了 voxel 之间的幂相关性。我们使用神经网络来参数化一个圆锥函数序列（NeSH）来表示单个试验者的 dMRI 信号，这种表示是连续的 both angular 和 spatial 领域。重建的 dMRI 信号使用这种方法显示出更加结构准确的数据。gradient 图像中的噪声被除去， fibers 方向分布函数显示出一直线性的改变方向。我们显示了如何使用这种重建来计算平均扩散率、分数扩散率和总表面积磁度。这些结果可以通过单个模型架构和一个 hyperparameter 的调整来实现。在这篇论文中，我们还证明了在 both angular 和 spatial 领域进行 upsampling 可以获得与现有方法相当或更好的重建结果。
</details></li>
</ul>
<hr>
<h2 id="Self-Reference-Deep-Adaptive-Curve-Estimation-for-Low-Light-Image-Enhancement"><a href="#Self-Reference-Deep-Adaptive-Curve-Estimation-for-Low-Light-Image-Enhancement" class="headerlink" title="Self-Reference Deep Adaptive Curve Estimation for Low-Light Image Enhancement"></a>Self-Reference Deep Adaptive Curve Estimation for Low-Light Image Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08197">http://arxiv.org/abs/2308.08197</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/john-venti/self-dace">https://github.com/john-venti/self-dace</a></li>
<li>paper_authors: Jianyu Wen, Chenhao Wu, Tong Zhang, Yixuan Yu, Piotr Swierczynski</li>
<li>for: 提高低光照图像的增强</li>
<li>methods: 提出了一种两stage的低光照图像增强方法，包括一个intuitive、轻量级、快速、不需supervision的明亮增强算法，以及一种新的损失函数，用于保持自然图像的颜色、结构和准确性。</li>
<li>results: 对多个实际世界数据集进行了详细的qualitative和量化分析，并证明了方法的超越现有状态的算法。<details>
<summary>Abstract</summary>
In this paper, we propose a 2-stage low-light image enhancement method called Self-Reference Deep Adaptive Curve Estimation (Self-DACE). In the first stage, we present an intuitive, lightweight, fast, and unsupervised luminance enhancement algorithm. The algorithm is based on a novel low-light enhancement curve that can be used to locally boost image brightness. We also propose a new loss function with a simplified physical model designed to preserve natural images' color, structure, and fidelity. We use a vanilla CNN to map each pixel through deep Adaptive Adjustment Curves (AAC) while preserving the local image structure. Secondly, we introduce the corresponding denoising scheme to remove the latent noise in the darkness. We approximately model the noise in the dark and deploy a Denoising-Net to estimate and remove the noise after the first stage. Exhaustive qualitative and quantitative analysis shows that our method outperforms existing state-of-the-art algorithms on multiple real-world datasets.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出了一种两stage的低光照图像提升方法，称为Self-Reference Deep Adaptive Curve Estimation（Self-DACE）。在第一个阶段，我们提供了一种直观、轻量级、快速、不需超级视觉的亮度增强算法。该算法基于一个新的低光照增强曲线，可以在本地增加图像亮度。我们还提出了一个新的损失函数，该函数遵循简化的物理模型，以保持自然图像的颜色、结构和准确性。我们使用一个普通的CNN来将每个像素通过深度适应曲线（AAC）进行映射，同时保持图像的本地结构。在第二个阶段，我们引入了对应的噪声除减方法，以除除在黑暗中的噪声。我们简单地模型了黑暗中的噪声，并部署了一个Denoising-Net来估计和除减噪声。经过详细的质量和量化分析，我们的方法在多个真实世界数据集上表现出excel。
</details></li>
</ul>
<hr>
<h2 id="Conditional-Perceptual-Quality-Preserving-Image-Compression"><a href="#Conditional-Perceptual-Quality-Preserving-Image-Compression" class="headerlink" title="Conditional Perceptual Quality Preserving Image Compression"></a>Conditional Perceptual Quality Preserving Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08154">http://arxiv.org/abs/2308.08154</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tongda Xu, Qian Zhang, Yanghao Li, Dailan He, Zhe Wang, Yuanyuan Wang, Hongwei Qin, Yan Wang, Jingjing Liu, Ya-Qin Zhang</li>
<li>for: 这个论文是为了扩展基于用户定义信息的 conditional perceptual quality（cPQ），以提高图像压缩的质量和效率。</li>
<li>methods: 该论文使用了一种基于 rate-distortion-perception 质量评价的优化方法，并提出了一个最佳的 conditional perceptual quality 框架，以保持高质量和高效率。</li>
<li>results: 实验结果表明，该代码可以成功保持高质量和高Semantic quality，并且可以提供底层 bound of common randomness required，解决了过去的争议是否应该在生成器中添加随机性以提高 conditional perceptual quality 压缩。Here’s the full translation of the abstract in Simplified Chinese:</li>
<li>for: 本论文是为了扩展基于用户定义信息的 conditional perceptual quality（cPQ），以提高图像压缩的质量和效率。</li>
<li>methods: 该论文使用了一种基于 rate-distortion-perception 质量评价的优化方法，并提出了一个最佳的 conditional perceptual quality 框架，以保持高质量和高效率。</li>
<li>results: 实验结果表明，该代码可以成功保持高质量和高Semantic quality，并且可以提供底层 bound of common randomness required，解决了过去的争议是否应该在生成器中添加随机性以提高 conditional perceptual quality 压缩。<details>
<summary>Abstract</summary>
We propose conditional perceptual quality, an extension of the perceptual quality defined in \citet{blau2018perception}, by conditioning it on user defined information. Specifically, we extend the original perceptual quality $d(p_{X},p_{\hat{X})$ to the conditional perceptual quality $d(p_{X|Y},p_{\hat{X}|Y})$, where $X$ is the original image, $\hat{X}$ is the reconstructed, $Y$ is side information defined by user and $d(.,.)$ is divergence. We show that conditional perceptual quality has similar theoretical properties as rate-distortion-perception trade-off \citep{blau2019rethinking}. Based on these theoretical results, we propose an optimal framework for conditional perceptual quality preserving compression. Experimental results show that our codec successfully maintains high perceptual quality and semantic quality at all bitrate. Besides, by providing a lowerbound of common randomness required, we settle the previous arguments on whether randomness should be incorporated into generator for (conditional) perceptual quality compression. The source code is provided in supplementary material.
</details>
<details>
<summary>摘要</summary>
我们提议用条件的感知质量来扩展感知质量，定义在\citet{blau2018perception}中。specifically，我们将原始的感知质量$d(p_{X},p_{\hat{X})$扩展到条件感知质量$d(p_{X|Y},p_{\hat{X}|Y})$,其中$X$是原始图像，$\hat{X}$是重建的图像，$Y$是用户定义的侧信息。我们证明了条件感知质量具有类似的理论性质，与比特率-质量-感知质量评估交易\citep{blau2019rethinking}相似。基于这些理论结果，我们提出了一个优化的条件感知质量保持压缩框架。实验结果表明，我们的编码器成功保持高度的感知质量和语义质量，无论比特率如何。此外，我们提供了一个下界的公共随机性需求，解决了以前关于是否应该在生成器中包含随机性以实现（条件）感知质量压缩的辩论。代码可以在补充材料中找到。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Overview-of-Computational-Nuclei-Segmentation-Methods-in-Digital-Pathology"><a href="#A-Comprehensive-Overview-of-Computational-Nuclei-Segmentation-Methods-in-Digital-Pathology" class="headerlink" title="A Comprehensive Overview of Computational Nuclei Segmentation Methods in Digital Pathology"></a>A Comprehensive Overview of Computational Nuclei Segmentation Methods in Digital Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08112">http://arxiv.org/abs/2308.08112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vasileios Magoulianitis, Catherine A. Alexander, C. -C. Jay Kuo</li>
<li>for: 这 paper 旨在对 cancer 诊断领域中数字 PATHOLOGY 的应用进行全面的 Review，尤其是在诊断、分期和评价方面。</li>
<li>methods: 这 paper 使用了传统的图像处理技术和 Deep Learning (DL) 模型来自动 segment nuclei，并评估了不同类型的超级vision 的影响。</li>
<li>results: 这 paper 提出了一种可靠且高效的 nuclei segmentation 方法，并讨论了不同模型和超级vision 的优劣。 futur 的研究应该强调高效可解释的模型，以便医生可以信任其输出。<details>
<summary>Abstract</summary>
In the cancer diagnosis pipeline, digital pathology plays an instrumental role in the identification, staging, and grading of malignant areas on biopsy tissue specimens. High resolution histology images are subject to high variance in appearance, sourcing either from the acquisition devices or the H\&E staining process. Nuclei segmentation is an important task, as it detects the nuclei cells over background tissue and gives rise to the topology, size, and count of nuclei which are determinant factors for cancer detection. Yet, it is a fairly time consuming task for pathologists, with reportedly high subjectivity. Computer Aided Diagnosis (CAD) tools empowered by modern Artificial Intelligence (AI) models enable the automation of nuclei segmentation. This can reduce the subjectivity in analysis and reading time. This paper provides an extensive review, beginning from earlier works use traditional image processing techniques and reaching up to modern approaches following the Deep Learning (DL) paradigm. Our review also focuses on the weak supervision aspect of the problem, motivated by the fact that annotated data is scarce. At the end, the advantages of different models and types of supervision are thoroughly discussed. Furthermore, we try to extrapolate and envision how future research lines will potentially be, so as to minimize the need for labeled data while maintaining high performance. Future methods should emphasize efficient and explainable models with a transparent underlying process so that physicians can trust their output.
</details>
<details>
<summary>摘要</summary>
在肿瘤诊断管道中，数字pathology扮演着重要的角色，用于识别、分期和评价肿瘤区域的病理样本中的细胞。高分辨率历史学图像具有高度的变化，来源于取得设备或H\&E染色过程。细胞核 segmentation 是一项重要的任务，因为它可以在背景组织中检测细胞核，并且对肿瘤检测有关键的因素。然而，这是一项时间consuming的任务，它需要许多的专业人员时间和subjectivity。计算机支持诊断 (CAD) 工具，激活了现代人工智能 (AI) 模型，可以自动完成细胞核 segmentation。这可以降低分析和阅读时间的主观性，并提高诊断的准确性。本文提供了广泛的回顾，从传统图像处理技术开始，到现代DL方法。我们的回顾还特别关注了弱监督问题的挑战，由于标注数据稀缺。文章结尾，我们详细讨论了不同模型和监督方式的优势。此外，我们尝试预测未来研究的发展趋势，以减少标注数据的需求，保持高性能。未来的方法应该强调高效和可读性的模型，并且具有透明的下面过程，以便医生可以信任其输出。
</details></li>
</ul>
<hr>
<h2 id="Snapshot-High-Dynamic-Range-Imaging-with-a-Polarization-Camera"><a href="#Snapshot-High-Dynamic-Range-Imaging-with-a-Polarization-Camera" class="headerlink" title="Snapshot High Dynamic Range Imaging with a Polarization Camera"></a>Snapshot High Dynamic Range Imaging with a Polarization Camera</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08094">http://arxiv.org/abs/2308.08094</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Intelligent-Sensing/polarization-hdr">https://github.com/Intelligent-Sensing/polarization-hdr</a></li>
<li>paper_authors: Mingyang Xie, Matthew Chan, Christopher Metzler</li>
<li>for: 将折衣相机转换为高 Dynamic Range（HDR）相机</li>
<li>methods: 使用linear polarizer并capture变化exposure的四个图像，并使用自适应和异常检测算法重构HDR图像</li>
<li>results: 实验结果表明该方法可以实现高效的HDR图像重构<details>
<summary>Abstract</summary>
High dynamic range (HDR) images are important for a range of tasks, from navigation to consumer photography. Accordingly, a host of specialized HDR sensors have been developed, the most successful of which are based on capturing variable per-pixel exposures. In essence, these methods capture an entire exposure bracket sequence at once in a single shot. This paper presents a straightforward but highly effective approach for turning an off-the-shelf polarization camera into a high-performance HDR camera. By placing a linear polarizer in front of the polarization camera, we are able to simultaneously capture four images with varied exposures, which are determined by the orientation of the polarizer. We develop an outlier-robust and self-calibrating algorithm to reconstruct an HDR image (at a single polarity) from these measurements. Finally, we demonstrate the efficacy of our approach with extensive real-world experiments.
</details>
<details>
<summary>摘要</summary>
高动态范围（HDR）图像在多种任务中具有重要性，从导航到消费型摄影。因此，一些专门的HDR传感器被开发出来，最成功的是基于每像素不同曝光的捕获方法。这篇论文提出了将普通的 polarization 摄像头转化为高性能 HDR 摄像头的简单 yet effective 方法。我们在 polarization 摄像头前面加入了线性激光 polarizer，从而同时捕获了不同曝光的四个图像，其曝光强度与 polarizer 的 orientations 相关。我们开发了一种具有耐抗异常值和自适应特性的算法，将这些测量转化为 HDR 图像（具有单一极性）。最后，我们通过实验证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Framework-for-Spleen-Volume-Estimation-from-2D-Cross-sectional-Views"><a href="#Deep-Learning-Framework-for-Spleen-Volume-Estimation-from-2D-Cross-sectional-Views" class="headerlink" title="Deep Learning Framework for Spleen Volume Estimation from 2D Cross-sectional Views"></a>Deep Learning Framework for Spleen Volume Estimation from 2D Cross-sectional Views</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08038">http://arxiv.org/abs/2308.08038</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhen Yuan, Esther Puyol-Anton, Haran Jogeesvaran, Baba Inusa, Andrew P. King</li>
<li>for: 本研究的目的是开发一种自动从2D肝脏分割图像中计算肝脏体积的方法，以便在临床实践中更加准确地评估肝病和相关的临床病情。</li>
<li>methods: 我们提出了一种基于变分自动encoder的框架，用于从单个或双个视图的2D肝脏分割图像中计算肝脏体积。我们还提出了三种体积估计方法，并对这些方法进行评估。</li>
<li>results: 我们的最佳模型在单个视图和双个视图的肝脏分割图像上达到了86.62%和92.58%的相对体积准确率，超过了现有的临床标准方法和一种相关的深度学习基于2D-3D重建的方法。<details>
<summary>Abstract</summary>
Abnormal spleen enlargement (splenomegaly) is regarded as a clinical indicator for a range of conditions, including liver disease, cancer and blood diseases. While spleen length measured from ultrasound images is a commonly used surrogate for spleen size, spleen volume remains the gold standard metric for assessing splenomegaly and the severity of related clinical conditions. Computed tomography is the main imaging modality for measuring spleen volume, but it is less accessible in areas where there is a high prevalence of splenomegaly (e.g., the Global South). Our objective was to enable automated spleen volume measurement from 2D cross-sectional segmentations, which can be obtained from ultrasound imaging. In this study, we describe a variational autoencoder-based framework to measure spleen volume from single- or dual-view 2D spleen segmentations. We propose and evaluate three volume estimation methods within this framework. We also demonstrate how 95% confidence intervals of volume estimates can be produced to make our method more clinically useful. Our best model achieved mean relative volume accuracies of 86.62% and 92.58% for single- and dual-view segmentations, respectively, surpassing the performance of the clinical standard approach of linear regression using manual measurements and a comparative deep learning-based 2D-3D reconstruction-based approach. The proposed spleen volume estimation framework can be integrated into standard clinical workflows which currently use 2D ultrasound images to measure spleen length. To the best of our knowledge, this is the first work to achieve direct 3D spleen volume estimation from 2D spleen segmentations.
</details>
<details>
<summary>摘要</summary>
非常常见的脾脓肥大（splenomegaly）被认为是许多疾病的临床指标之一，包括肝病、 cancer 和血液疾病。 Although spleen length measured from ultrasound images is commonly used as a surrogate for spleen size, spleen volume remains the gold standard metric for assessing splenomegaly and the severity of related clinical conditions.  computed tomography 是评估脾脓体积的主要成像Modal，但它在全球南部地区更普遍不可用。 Our objective was to enable automated spleen volume measurement from 2D cross-sectional segmentations, which can be obtained from ultrasound imaging. In this study, we describe a variational autoencoder-based framework to measure spleen volume from single- or dual-view 2D spleen segmentations. We propose and evaluate three volume estimation methods within this framework. We also demonstrate how 95% confidence intervals of volume estimates can be produced to make our method more clinically useful. Our best model achieved mean relative volume accuracies of 86.62% and 92.58% for single- and dual-view segmentations, respectively, surpassing the performance of the clinical standard approach of linear regression using manual measurements and a comparative deep learning-based 2D-3D reconstruction-based approach. The proposed spleen volume estimation framework can be integrated into standard clinical workflows which currently use 2D ultrasound images to measure spleen length. To the best of our knowledge, this is the first work to achieve direct 3D spleen volume estimation from 2D spleen segmentations.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/16/eess.IV_2023_08_16/" data-id="clorjzlfp015of188883s4gm8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/15/cs.SD_2023_08_15/" class="article-date">
  <time datetime="2023-08-15T15:00:00.000Z" itemprop="datePublished">2023-08-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/15/cs.SD_2023_08_15/">cs.SD - 2023-08-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Preliminary-investigation-of-the-short-term-in-situ-performance-of-an-automatic-masker-selection-system"><a href="#Preliminary-investigation-of-the-short-term-in-situ-performance-of-an-automatic-masker-selection-system" class="headerlink" title="Preliminary investigation of the short-term in situ performance of an automatic masker selection system"></a>Preliminary investigation of the short-term in situ performance of an automatic masker selection system</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07767">http://arxiv.org/abs/2308.07767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bhan Lam, Zhen-Ting Ong, Kenneth Ooi, Wen-Hui Ong, Trevor Wong, Karn N. Watcharasupat, Woon-Seng Gan</li>
<li>for: 这篇论文主要是为了提高室内声学环境的听觉舒适性而写的。</li>
<li>methods: 该论文使用了一种深度学习模型，该模型是根据大规模的主观反馈数据来选择最佳的干扰音频，以提高ISO听觉舒适性（ISO 12913-2）的。</li>
<li>results: 研究发现，使用AMSS系统可以提高室内声学环境的听觉舒适性，而且可以更好地根据听觉反馈来选择最佳的干扰音频。<details>
<summary>Abstract</summary>
Soundscape augmentation or "masking" introduces wanted sounds into the acoustic environment to improve acoustic comfort. Usually, the masker selection and playback strategies are either arbitrary or based on simple rules (e.g. -3 dBA), which may lead to sub-optimal increment or even reduction in acoustic comfort for dynamic acoustic environments. To reduce ambiguity in the selection of maskers, an automatic masker selection system (AMSS) was recently developed. The AMSS uses a deep-learning model trained on a large-scale dataset of subjective responses to maximize the derived ISO pleasantness (ISO 12913-2). Hence, this study investigates the short-term in situ performance of the AMSS implemented in a gazebo in an urban park. Firstly, the predicted ISO pleasantness from the AMSS is evaluated in comparison to the in situ subjective evaluation scores. Secondly, the effect of various masker selection schemes on the perceived affective quality and appropriateness would be evaluated. In total, each participant evaluated 6 conditions: (1) ambient environment with no maskers; (2) AMSS; (3) bird and (4) water masker from prior art; (5) random selection from same pool of maskers used to train the AMSS; and (6) selection of best-performing maskers based on the analysis of the dataset used to train the AMSS.
</details>
<details>
<summary>摘要</summary>
增强声响环境或"遮盾"技术可以提高声响舒适度。通常，遮盾选择和播放策略是随意的或基于简单的规则（例如 -3 dBA），可能会导致声响舒适度的下降或不足。为了减少遮盾选择的 ambiguity，一个自动遮盾选择系统（AMSS）已经被开发出来。AMSS使用基于大规模数据集的主观反应进行训练，以最大化 derivated ISO 舒适度（ISO 12913-2）。因此，本研究探讨了在城市公园中的废墟中进行的 AMSS 实际性表现。首先，AMSS 预测的 ISO 舒适度与现场评估分数进行比较。其次，通过不同遮盾选择方案对人们对声响质量和适应性的感知做出评估。总的来说，每名参与者评估了6个条件：（1）无遮盾的 ambient 环境；（2）AMSS；（3）鸟声和水声遮盾来自优等艺术作品；（4）随机从同一个训练数据集中选择的遮盾；（5）基于训练数据集分析选择最佳遮盾；以及（6）由参与者自己选择的最佳遮盾。
</details></li>
</ul>
<hr>
<h2 id="Improving-CTC-AED-model-with-integrated-CTC-and-auxiliary-loss-regularization"><a href="#Improving-CTC-AED-model-with-integrated-CTC-and-auxiliary-loss-regularization" class="headerlink" title="Improving CTC-AED model with integrated-CTC and auxiliary loss regularization"></a>Improving CTC-AED model with integrated-CTC and auxiliary loss regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08449">http://arxiv.org/abs/2308.08449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daobin Zhu, Xiangdong Su, Hongbin Zhang</li>
<li>for:  automatic speech recognition (ASR)</li>
<li>methods:  connectionist temporal classification (CTC) 和 attention-based encoder decoder (AED) 的 JOINT 训练</li>
<li>results:  DAL 方法在注意力重新评分中表现更好，而 PMP 方法在 CTC 预refix 搜索和扩散搜索中表现更好<details>
<summary>Abstract</summary>
Connectionist temporal classification (CTC) and attention-based encoder decoder (AED) joint training has been widely applied in automatic speech recognition (ASR). Unlike most hybrid models that separately calculate the CTC and AED losses, our proposed integrated-CTC utilizes the attention mechanism of AED to guide the output of CTC. In this paper, we employ two fusion methods, namely direct addition of logits (DAL) and preserving the maximum probability (PMP). We achieve dimensional consistency by adaptively affine transforming the attention results to match the dimensions of CTC. To accelerate model convergence and improve accuracy, we introduce auxiliary loss regularization for accelerated convergence. Experimental results demonstrate that the DAL method performs better in attention rescoring, while the PMP method excels in CTC prefix beam search and greedy search.
</details>
<details>
<summary>摘要</summary>
卷积时间分类（CTC）和注意力基于编码器解码器（AED）的共同训练在自动语音识别（ASR）中广泛应用。与大多数混合模型不同，我们提议的综合CTC使用AED的注意力机制来导引CTC输出。在这篇论文中，我们采用了两种融合方法，即直接加法的峰值（DAL）和保持最大概率（PMP）。为确保维度一致，我们采用了适应权重变换来调整注意力结果的维度与CTC匹配。为加速模型 converges 和提高准确性，我们引入了辅助损失regularization。实验结果表明，DAL方法在注意力重新评分中表现更好，而PMP方法在CTC预фикс搜索和扫描搜索中表现更好。
</details></li>
</ul>
<hr>
<h2 id="Using-Text-Injection-to-Improve-Recognition-of-Personal-Identifiers-in-Speech"><a href="#Using-Text-Injection-to-Improve-Recognition-of-Personal-Identifiers-in-Speech" class="headerlink" title="Using Text Injection to Improve Recognition of Personal Identifiers in Speech"></a>Using Text Injection to Improve Recognition of Personal Identifiers in Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07393">http://arxiv.org/abs/2308.07393</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yochai Blau, Rohan Agrawal, Lior Madmony, Gary Wang, Andrew Rosenberg, Zhehuai Chen, Zorik Gekhman, Genady Beryozkin, Parisa Haghani, Bhuvana Ramabhadran</li>
<li>for: 提高自动语音识别（ASR）系统中 especific category 的识别精度，例如人名、日期等个人信息。</li>
<li>methods: 使用 text-injection 方法，在训练数据中包含假文本替换Personally Identifiable Information（PII）类别，以提高 ASR 模型对这些类别的识别精度。</li>
<li>results: 在医疗笔记中，可以提高名称和日期的回忆率，并提高总的words error rate（WER）。对于字符串数字序列，可以提高字符错误率和句子准确率。<details>
<summary>Abstract</summary>
Accurate recognition of specific categories, such as persons' names, dates or other identifiers is critical in many Automatic Speech Recognition (ASR) applications. As these categories represent personal information, ethical use of this data including collection, transcription, training and evaluation demands special care. One way of ensuring the security and privacy of individuals is to redact or eliminate Personally Identifiable Information (PII) from collection altogether. However, this results in ASR models that tend to have lower recognition accuracy of these categories. We use text-injection to improve the recognition of PII categories by including fake textual substitutes of PII categories in the training data using a text injection method. We demonstrate substantial improvement to Recall of Names and Dates in medical notes while improving overall WER. For alphanumeric digit sequences we show improvements to Character Error Rate and Sentence Accuracy.
</details>
<details>
<summary>摘要</summary>
“准确识别特定类别，如人名、日期等标识信息，在自动语音识别（ASR）应用中是关键。这些类别代表个人信息，因此使用这些数据需要特殊的注意和保护。一种方法是不收集或消除个人可识别信息（PII），但这会导致ASR模型对这些类别的识别精度下降。我们使用文本插入法来提高PII类别的识别精度，通过在训练数据中包含假文本substitute来提高名称和日期的回忆率。我们在医疗笔记中展示了明显的提高，同时提高总的word error rate。对于字符串数字序列，我们显示了字符错误率和句子准确率的改善。”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know and I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Localization-of-DOA-trajectories-–-Beyond-the-grid"><a href="#Localization-of-DOA-trajectories-–-Beyond-the-grid" class="headerlink" title="Localization of DOA trajectories – Beyond the grid"></a>Localization of DOA trajectories – Beyond the grid</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07265">http://arxiv.org/abs/2308.07265</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruchi Pandey, Santosh Nannuru</li>
<li>for: 本研究旨在提出两种轨迹模型，以捕捉DOA动态。</li>
<li>methods: 用Sliding Frank-Wolfe（SFW）和Newtonized Orthogonal Matching Pursuit（NOMP）两种网格less算法来估计轨迹参数。</li>
<li>results: 对比传统网格方法，提议的轨迹localization算法在噪音抗干扰和计算效率方面具有改善的性能。<details>
<summary>Abstract</summary>
The direction of arrival (DOA) estimation algorithms are crucial in localizing acoustic sources. Traditional localization methods rely on block-level processing to extract the directional information from multiple measurements processed together. However, these methods assume that DOA remains constant throughout the block, which may not be true in practical scenarios. Also, the performance of localization methods is limited when the true parameters do not lie on the parameter search grid. In this paper we propose two trajectory models, namely the polynomial and bandlimited trajectory models, to capture the DOA dynamics. To estimate trajectory parameters, we adopt two gridless algorithms: i) Sliding Frank-Wolfe (SFW), which solves the Beurling LASSO problem and ii) Newtonized Orthogonal Matching Pursuit (NOMP), which improves over OMP using cyclic refinement. Furthermore, we extend our analysis to include wideband processing. The simulation results indicate that the proposed trajectory localization algorithms exhibit improved performance compared to grid-based methods in terms of resolution, robustness to noise, and computational efficiency.
</details>
<details>
<summary>摘要</summary>
Direction of arrival (DOA) 估计算法是音频源本地化的关键。传统的本地化方法依赖块级处理来提取方向信息，但这些方法假设DOA在块内保持相同，这可能不符合实际场景。此外，本地化方法的性能受限于真实参数不在搜索网格上。在本文中，我们提出了两种轨迹模型：多项式轨迹模型和带有限轨迹模型，以捕捉DOA的动态。为估计轨迹参数，我们采用了两种不含网格的算法：i）滑动法沃尔夫（SFW），解决了Beurling LASSO问题，ii）增强的正交匹配追踪（NOMP），通过循环纠正提高OMP的性能。此外，我们扩展了分析至宽频处理。实验结果表明，提议的轨迹本地化算法在比grid-based方法更高的分解能力、鲁棒性和计算效率方面表现出色。
</details></li>
</ul>
<hr>
<h2 id="Compositional-nonlinear-audio-signal-processing-with-Volterra-series"><a href="#Compositional-nonlinear-audio-signal-processing-with-Volterra-series" class="headerlink" title="Compositional nonlinear audio signal processing with Volterra series"></a>Compositional nonlinear audio signal processing with Volterra series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07229">http://arxiv.org/abs/2308.07229</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jake Araujo-Simon</li>
<li>for: 这篇论文是为了构建一种基于Volterra系列的非线性音频信号处理理论，以便更好地理解和预测非线性音频系统的行为。</li>
<li>methods: 论文使用了一种基于函数器的方法，通过对Volterra系列进行Category化，以描述非线性变换的输出如何受到输入信号的线性处理的影响。它还引入了一种抽象的折射映射，用于模型非线性音频系统的变化。</li>
<li>results: 论文得出了一些关于非线性音频系统的结论，包括：非线性音频系统的变化可以通过模型为折射映射来描述； Volterra系列和其 morphisms 组织成一个Category，可以用来模型非线性音频系统的时间变化； series composition of Volterra series 是 associative。<details>
<summary>Abstract</summary>
We develop a compositional theory of nonlinear audio signal processing based on a categorification of the Volterra series. We begin by considering what it would mean for the Volterra series to be functorial with respect to a base category whose objects are temperate distributions and whose morphisms are certain linear transformations. This leads to formulae describing how the outcomes of nonlinear transformations are affected if their input signals are first linearly processed. We then consider how nonlinear audio systems change, and introduce as a model thereof a notion of morphism of Volterra series, which we exhibit as a kind of lens map. We show how morphisms can be parameterized and used to generate indexed families of Volterra series, which are well-suited to model nonstationary or time-varying nonlinear phenomena. We then describe how Volterra series and their morphisms organize into a category, which we call Volt. We exhibit the operations of sum, product, and series composition of Volterra series as monoidal products on Volt and identify, for each in turn, its corresponding universal property. We show, in particular, that the series composition of Volterra series is associative. We then bridge between our framework and a subject at the heart of audio signal processing: time-frequency analysis. Specifically, we show that an equivalence between a certain class of second-order Volterra series and the bilinear time-frequency distributions (TFDs) can be extended to one between certain higher-order Volterra series and the so-called polynomial TFDs. We end with prospects for future work, including the incorporation of nonlinear system identification techniques and the extension of our theory to the settings of compositional graph and topological audio signal processing.
</details>
<details>
<summary>摘要</summary>
我们开发了一种基于幂阶系列的非线性音频信号处理理论，该理论是基于幂阶系列的 categorification。我们首先考虑了幂阶系列是如何作为一种函手，对底Category的对象（温度分布）和态射（certain linear transformation）进行函手性的定义。这导致了输入非线性变换后的结果如何受到输入信号的线性处理影响的公式。然后，我们考虑了非线性音频系统如何改变，并引入了一种模型，即幂阶系列的态射。我们显示了这种态射可以被视为一种类型的镜像。我们还介绍了如何使用态射来生成索引 family of Volterra series，这些家族适合模拟非站ARY或时间变化的非线性现象。最后，我们描述了幂阶系列和其态射组织成一个category，我们称之为Volt。我们展示了幂阶系列和其态射之间的操作，包括加法、乘法和序列 compose，它们都是Volt中的对应的幂阶乘法。我们还证明了序列 compose 是相关的。最后，我们将我们的框架与音频信号处理中关键的主题进行桥接，即时域分析。我们证明了一种等价关系，即某种次序 Volterra series与bilinear time-frequency distributions（TFDs）之间的等价关系，可以扩展到高阶 Volterra series和叫做多项式 TFDs。我们结束于未来工作的展望，包括非线性系统识别技术的 incorporation 和我们理论的扩展到compositional graph 和 topological audio signal processing 的设置。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/15/cs.SD_2023_08_15/" data-id="clorjzlb400v6f188dgfs46un" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_08_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/15/eess.AS_2023_08_15/" class="article-date">
  <time datetime="2023-08-15T14:00:00.000Z" itemprop="datePublished">2023-08-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/15/eess.AS_2023_08_15/">eess.AS - 2023-08-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="GIST-AiTeR-Speaker-Diarization-System-for-VoxCeleb-Speaker-Recognition-Challenge-VoxSRC-2023"><a href="#GIST-AiTeR-Speaker-Diarization-System-for-VoxCeleb-Speaker-Recognition-Challenge-VoxSRC-2023" class="headerlink" title="GIST-AiTeR Speaker Diarization System for VoxCeleb Speaker Recognition Challenge (VoxSRC) 2023"></a>GIST-AiTeR Speaker Diarization System for VoxCeleb Speaker Recognition Challenge (VoxSRC) 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07788">http://arxiv.org/abs/2308.07788</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongkeon Park, Ji Won Kim, Kang Ryeol Kim, Do Hyun Lee, Hong Kook Kim</li>
<li>for: 本文描述了GIST-AiTeR团队在VoxCeleb Speaker Recognition Challenge 2023（VoxSRC-23）Track 4的提交系统。</li>
<li>methods: 该提交系统集成了多种说话分类（SD）技术，包括ResNet293和MFA-Conformer，以及不同的段和跳长组合。</li>
<li>results: ResNet293和MFA-Conformer模型在VAL46上表现出了3.65%和3.83%的分类错误率（DER），分布 ensemble模型在VAL46上表现出了3.50%的DER，并在VoxSRC-23测试集上达到4.88%的DER。<details>
<summary>Abstract</summary>
This report describes the submission system by the GIST-AiTeR team for the VoxCeleb Speaker Recognition Challenge 2023 (VoxSRC-23) Track 4. Our submission system focuses on implementing diverse speaker diarization (SD) techniques, including ResNet293 and MFA-Conformer with different combinations of segment and hop length. Then, those models are combined into an ensemble model. The ResNet293 and MFA-Conformer models exhibited the diarization error rates (DERs) of 3.65% and 3.83% on VAL46, respectively. The submitted ensemble model provided a DER of 3.50% on VAL46, and consequently, it achieved a DER of 4.88% on the VoxSRC-23 test set.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-DKU-MSXF-Diarization-System-for-the-VoxCeleb-Speaker-Recognition-Challenge-2023"><a href="#The-DKU-MSXF-Diarization-System-for-the-VoxCeleb-Speaker-Recognition-Challenge-2023" class="headerlink" title="The DKU-MSXF Diarization System for the VoxCeleb Speaker Recognition Challenge 2023"></a>The DKU-MSXF Diarization System for the VoxCeleb Speaker Recognition Challenge 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07595">http://arxiv.org/abs/2308.07595</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ming Cheng, Weiqing Wang, Xiaoyi Qin, Yuke Lin, Ning Jiang, Guoqing Zhao, Ming Li</li>
<li>for: 这篇论文是为了描述DKU-MSXF在VoxCeleb Speaker Recognition Challenge 2023（VoxSRC-23）的识别系统提交。</li>
<li>methods: 该系统管道包括声动活跃检测、分组分割、重叠演讲检测和目标说话人活跃检测，每个过程都有3个子模型的拟合输出。</li>
<li>results: 最终通过DOVER-Lap进行拟合不同的分组和TSVAD系统，实现4.30%的识别错误率（DER），在track 4的挑战排行榜上名列第一。<details>
<summary>Abstract</summary>
This paper describes the DKU-MSXF submission to track 4 of the VoxCeleb Speaker Recognition Challenge 2023 (VoxSRC-23). Our system pipeline contains voice activity detection, clustering-based diarization, overlapped speech detection, and target-speaker voice activity detection, where each procedure has a fused output from 3 sub-models. Finally, we fuse different clustering-based and TSVAD-based diarization systems using DOVER-Lap and achieve the 4.30% diarization error rate (DER), which ranks first place on track 4 of the challenge leaderboard.
</details>
<details>
<summary>摘要</summary>
这篇论文描述DKU-MSXF在VoxCeleb Speaker Recognition Challenge 2023（VoxSRC-23）的订阅提交，我们的系统管道包括声音活动检测、集群化基于分类的分类、重叠说话检测和目标说话人声音活动检测，每个过程有3个子模型的融合输出。最后，我们将不同的集群化和TSVAD基于的分类系统融合使用DOVER-Lap，实现4.30%的分类错误率（DER），在赛事排名榜上名列第一。
</details></li>
</ul>
<hr>
<h2 id="AKVSR-Audio-Knowledge-Empowered-Visual-Speech-Recognition-by-Compressing-Audio-Knowledge-of-a-Pretrained-Model"><a href="#AKVSR-Audio-Knowledge-Empowered-Visual-Speech-Recognition-by-Compressing-Audio-Knowledge-of-a-Pretrained-Model" class="headerlink" title="AKVSR: Audio Knowledge Empowered Visual Speech Recognition by Compressing Audio Knowledge of a Pretrained Model"></a>AKVSR: Audio Knowledge Empowered Visual Speech Recognition by Compressing Audio Knowledge of a Pretrained Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07593">http://arxiv.org/abs/2308.07593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeong Hun Yeo, Minsu Kim, Jeongsoo Choi, Dae Hoe Kim, Yong Man Ro</li>
<li>for: 这个论文的目的是提出一个 Audio Knowledge empowered Visual Speech Recognition 框架 (AKVSR)，以使用音频模式补充视觉模式中的不足信息。</li>
<li>methods: 提案的 AKVSR 使用大规模预训Audio模型所编码的丰富音频知识，将音频信息储存在单簇音频内存中，并通过音频桥接模组寻找最佳对应的音频特征。</li>
<li>results: 经过广泛的实验 validate the effectiveness of the proposed method，在 widely-used datasets LRS2 和 LRS3 上 achievement new state-of-the-art performances。<details>
<summary>Abstract</summary>
Visual Speech Recognition (VSR) is the task of predicting spoken words from silent lip movements. VSR is regarded as a challenging task because of the insufficient information on lip movements. In this paper, we propose an Audio Knowledge empowered Visual Speech Recognition framework (AKVSR) to complement the insufficient speech information of visual modality by using audio modality. Different from the previous methods, the proposed AKVSR 1) utilizes rich audio knowledge encoded by a large-scale pretrained audio model, 2) saves the linguistic information of audio knowledge in compact audio memory by discarding the non-linguistic information from the audio through quantization, and 3) includes Audio Bridging Module which can find the best-matched audio features from the compact audio memory, which makes our training possible without audio inputs, once after the compact audio memory is composed. We validate the effectiveness of the proposed method through extensive experiments, and achieve new state-of-the-art performances on the widely-used datasets, LRS2 and LRS3.
</details>
<details>
<summary>摘要</summary>
Visual Speech Recognition (VSR) 是指从舌部运动中预测说话的任务。由于舌部运动的信息不充分，VSR 被视为一项具有挑战性的任务。在这篇论文中，我们提出了一个 Audio Knowledge empowered Visual Speech Recognition 框架 (AKVSR)，通过使用音频模式来补充视觉模式中的不充分信息。与前一些方法不同，我们的 AKVSR 具有以下特点：1. 使用大规模预训练的音频模型来编码丰富的音频知识。2. 通过归约来抛弃非语言信息，将音频知识储存在紧凑的音频内存中。3. 包括音频桥接模块，可以在训练时找到最佳匹配的音频特征，从而使我们的训练不需要音频输入，只需要一次性将紧凑音频内存构建。我们通过广泛的实验 validate 了我们的方法的效果，并在广泛使用的 dataset 上达到了新的状态码性表现。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/15/eess.AS_2023_08_15/" data-id="clorjzlds0111f1889n52hknq" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_08_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/15/cs.CV_2023_08_15/" class="article-date">
  <time datetime="2023-08-15T13:00:00.000Z" itemprop="datePublished">2023-08-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/15/cs.CV_2023_08_15/">cs.CV - 2023-08-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="CCD-3DR-Consistent-Conditioning-in-Diffusion-for-Single-Image-3D-Reconstruction"><a href="#CCD-3DR-Consistent-Conditioning-in-Diffusion-for-Single-Image-3D-Reconstruction" class="headerlink" title="CCD-3DR: Consistent Conditioning in Diffusion for Single-Image 3D Reconstruction"></a>CCD-3DR: Consistent Conditioning in Diffusion for Single-Image 3D Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07837">http://arxiv.org/abs/2308.07837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yan Di, Chenyangguang Zhang, Pengyuan Wang, Guangyao Zhai, Ruida Zhang, Fabian Manhardt, Benjamin Busam, Xiangyang Ji, Federico Tombari</li>
<li>for: 该文章描述了一种基于扩散模型的三维稀畴点云重建方法，用于基于单一RGB图像捕捉的对象重建。</li>
<li>methods: 该方法使用了一种新的中心扩散probabilistic模型来约束本地特征来 condtioning。在反 diffusion 过程中，抑制点云和样本点云被限制在一个子空间中，以确保点云中心保持不变。</li>
<li>results: 对于Synthetic ShapeNet-R2N2测试集，CCD-3DR超过了所有竞争者，增加了 más de 40%的性能提升。同时， authors还提供了实际应用中的Result on Pix3D数据集，以证明CCD-3DR在实际应用中的潜在性。<details>
<summary>Abstract</summary>
In this paper, we present a novel shape reconstruction method leveraging diffusion model to generate 3D sparse point cloud for the object captured in a single RGB image. Recent methods typically leverage global embedding or local projection-based features as the condition to guide the diffusion model. However, such strategies fail to consistently align the denoised point cloud with the given image, leading to unstable conditioning and inferior performance. In this paper, we present CCD-3DR, which exploits a novel centered diffusion probabilistic model for consistent local feature conditioning. We constrain the noise and sampled point cloud from the diffusion model into a subspace where the point cloud center remains unchanged during the forward diffusion process and reverse process. The stable point cloud center further serves as an anchor to align each point with its corresponding local projection-based features. Extensive experiments on synthetic benchmark ShapeNet-R2N2 demonstrate that CCD-3DR outperforms all competitors by a large margin, with over 40% improvement. We also provide results on real-world dataset Pix3D to thoroughly demonstrate the potential of CCD-3DR in real-world applications. Codes will be released soon
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种基于扩散模型的新型形态重建方法，用于从单个RGB图像中恢复3D稀畴点云。现有方法通常使用全局嵌入或本地投影基于特征作为扩散模型的指导条件，但这些策略无法一致地将净化后的点云与给定图像相对应，导致稳定性差和性能下降。在这篇论文中，我们提出了CCD-3DR，它利用一种新型的中心扩散概率模型来实现一致的本地特征控制。我们在扩散和归还过程中将噪声和采样点云压缩到一个子空间，使得点云中心保持不变。稳定的点云中心更serve as一个锚点，使每个点与其相应的本地投影基于特征进行对应。我们在Synthetic benchmark ShapeNet-R2N2上进行了广泛的实验，结果表明CCD-3DR与其他竞争对手相比，提高了40%以上。我们还对实际应用中的Pix3D数据集进行了详细的研究，以展示CCD-3DR在实际应用中的潜在能力。代码将很快发布。
</details></li>
</ul>
<hr>
<h2 id="Learning-Better-Keypoints-for-Multi-Object-6DoF-Pose-Estimation"><a href="#Learning-Better-Keypoints-for-Multi-Object-6DoF-Pose-Estimation" class="headerlink" title="Learning Better Keypoints for Multi-Object 6DoF Pose Estimation"></a>Learning Better Keypoints for Multi-Object 6DoF Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07827">http://arxiv.org/abs/2308.07827</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yangzheng Wu, Michael Greenspan</li>
<li>for: 本研究探讨了预定义关键点对pose estimation的影响，并发现可以通过训练一个图 Orientated Graph Network（KeyGNet）来选择一组分散的关键点，以提高准确率和效率。</li>
<li>methods: KeyGNet使用一种组合损失函数，包括 Wassserstein 距离和分散，来监督学习颜色和几何特征来估算关键点位置。</li>
<li>results: 实验表明，使用KeyGNet选择的关键点可以提高所有评价指标的准确率，包括所有七个测试集。特别是在Occlusion LINEMOD数据集上，KeyGNet选择的关键点可以提高ADD(S)的值 by +16.4% on PVN3D。<details>
<summary>Abstract</summary>
We investigate the impact of pre-defined keypoints for pose estimation, and found that accuracy and efficiency can be improved by training a graph network to select a set of disperse keypoints with similarly distributed votes. These votes, learned by a regression network to accumulate evidence for the keypoint locations, can be regressed more accurately compared to previous heuristic keypoint algorithms. The proposed KeyGNet, supervised by a combined loss measuring both Wassserstein distance and dispersion, learns the color and geometry features of the target objects to estimate optimal keypoint locations. Experiments demonstrate the keypoints selected by KeyGNet improved the accuracy for all evaluation metrics of all seven datasets tested, for three keypoint voting methods. The challenging Occlusion LINEMOD dataset notably improved ADD(S) by +16.4% on PVN3D, and all core BOP datasets showed an AR improvement for all objects, of between +1% and +21.5%. There was also a notable increase in performance when transitioning from single object to multiple object training using KeyGNet keypoints, essentially eliminating the SISO-MIMO gap for Occlusion LINEMOD.
</details>
<details>
<summary>摘要</summary>
我们研究了预定的关键点对pose estimation的影响，并发现了通过训练一个图гра夫网络选择一组广泛分布的票点，以提高准确性和效率。这些票点由一个回归网络学习归一化证据以提高精度，相比之前的习惯性关键点算法。我们提出的KeyGNet，以combined损失函数 measuring Wasserstein distance和分布为优化目标，学习目标对象的颜色和几何特征以估算优化关键点位置。实验表明，KeyGNet选择的关键点提高了所有评价指标的准确性，包括所有七个数据集的所有三种关键点投票方法。特别是在Occlusion LINEMOD数据集上，KeyGNet选择的关键点提高了ADD(S)的准确性 by +16.4% on PVN3D，并且所有核心BOP数据集上的所有对象都显示了AR提升，分别为+1%到+21.5%。此外，使用KeyGNet关键点进行多对象训练时，可以基本消除SISO-MIMO障碍，为Occlusion LINEMOD数据集表现出了明显的提升。
</details></li>
</ul>
<hr>
<h2 id="ImbSAM-A-Closer-Look-at-Sharpness-Aware-Minimization-in-Class-Imbalanced-Recognition"><a href="#ImbSAM-A-Closer-Look-at-Sharpness-Aware-Minimization-in-Class-Imbalanced-Recognition" class="headerlink" title="ImbSAM: A Closer Look at Sharpness-Aware Minimization in Class-Imbalanced Recognition"></a>ImbSAM: A Closer Look at Sharpness-Aware Minimization in Class-Imbalanced Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07815">http://arxiv.org/abs/2308.07815</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cool-xuan/imbalanced_sam">https://github.com/cool-xuan/imbalanced_sam</a></li>
<li>paper_authors: Yixuan Zhou, Yi Qu, Xing Xu, Hengtao Shen</li>
<li>for:  Addressing the challenge of class imbalance in recognition tasks, specifically the generalization issues that arise when tail classes have limited training data.</li>
<li>methods:  Proposes a class-aware smoothness optimization algorithm called Imbalanced-SAM (ImbSAM) that leverages class priors to restrict the generalization scope of the class-agnostic SAM, improving generalization targeting tail classes.</li>
<li>results:  Demonstrates remarkable performance improvements for tail classes and anomaly detection in two prototypical applications of class-imbalanced recognition: long-tailed classification and semi-supervised anomaly detection.<details>
<summary>Abstract</summary>
Class imbalance is a common challenge in real-world recognition tasks, where the majority of classes have few samples, also known as tail classes. We address this challenge with the perspective of generalization and empirically find that the promising Sharpness-Aware Minimization (SAM) fails to address generalization issues under the class-imbalanced setting. Through investigating this specific type of task, we identify that its generalization bottleneck primarily lies in the severe overfitting for tail classes with limited training data. To overcome this bottleneck, we leverage class priors to restrict the generalization scope of the class-agnostic SAM and propose a class-aware smoothness optimization algorithm named Imbalanced-SAM (ImbSAM). With the guidance of class priors, our ImbSAM specifically improves generalization targeting tail classes. We also verify the efficacy of ImbSAM on two prototypical applications of class-imbalanced recognition: long-tailed classification and semi-supervised anomaly detection, where our ImbSAM demonstrates remarkable performance improvements for tail classes and anomaly. Our code implementation is available at https://github.com/cool-xuan/Imbalanced_SAM.
</details>
<details>
<summary>摘要</summary>
“类别不匹配是现实世界识别任务中的常见挑战，主要是因为大多数类别具有少量样本，也称为尾类。我们通过总化和实验发现，promising Sharpness-Aware Minimization (SAM) 在类别不匹配的设定下存在总化问题。通过研究这种特定任务，我们发现其总化瓶颈主要在tail classes中严重过拟合。为了缓解这个瓶颈，我们利用类别优先顺序来限制类型不感知 SAM 的总化范围，并提出一种类别意识细化优化算法名为 Imbalanced-SAM（ImbSAM）。通过类别优先顺序的引导，我们的 ImbSAM specifically 提高了tail classes的总化表现。我们还证明 ImbSAM 在long-tailed classification和 semi-supervised anomaly detection 中表现出色，尤其是在tail classes和异常处理方面。我们的代码实现可以在 GitHub 上找到：https://github.com/cool-xuan/Imbalanced_SAM。”
</details></li>
</ul>
<hr>
<h2 id="Grasp-Transfer-based-on-Self-Aligning-Implicit-Representations-of-Local-Surfaces"><a href="#Grasp-Transfer-based-on-Self-Aligning-Implicit-Representations-of-Local-Surfaces" class="headerlink" title="Grasp Transfer based on Self-Aligning Implicit Representations of Local Surfaces"></a>Grasp Transfer based on Self-Aligning Implicit Representations of Local Surfaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07807">http://arxiv.org/abs/2308.07807</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmet Tekden, Marc Peter Deisenroth, Yasemin Bekiroglu</li>
<li>for: 本文解决了将抓取经验或示例转移到新对象的问题，该对象与先前遇到的对象共享形状特征。</li>
<li>methods: 本文使用单一专家抓取示例学习了一个基于本地表面的印象模型，并在推理时使用这个模型将抓取转移到新对象的相似表面上。</li>
<li>results: 该方法在实验中可以成功将抓取转移到未看过的对象类别，并在实验和实际场景中表现出较好的空间精度和抓取精度。<details>
<summary>Abstract</summary>
Objects we interact with and manipulate often share similar parts, such as handles, that allow us to transfer our actions flexibly due to their shared functionality. This work addresses the problem of transferring a grasp experience or a demonstration to a novel object that shares shape similarities with objects the robot has previously encountered. Existing approaches for solving this problem are typically restricted to a specific object category or a parametric shape. Our approach, however, can transfer grasps associated with implicit models of local surfaces shared across object categories. Specifically, we employ a single expert grasp demonstration to learn an implicit local surface representation model from a small dataset of object meshes. At inference time, this model is used to transfer grasps to novel objects by identifying the most geometrically similar surfaces to the one on which the expert grasp is demonstrated. Our model is trained entirely in simulation and is evaluated on simulated and real-world objects that are not seen during training. Evaluations indicate that grasp transfer to unseen object categories using this approach can be successfully performed both in simulation and real-world experiments. The simulation results also show that the proposed approach leads to better spatial precision and grasp accuracy compared to a baseline approach.
</details>
<details>
<summary>摘要</summary>
Objects we interact with and manipulate often share similar parts, such as handles, that allow us to transfer our actions flexibly due to their shared functionality. This work addresses the problem of transferring a grasp experience or a demonstration to a novel object that shares shape similarities with objects the robot has previously encountered. Existing approaches for solving this problem are typically restricted to a specific object category or a parametric shape. Our approach, however, can transfer grasps associated with implicit models of local surfaces shared across object categories. Specifically, we employ a single expert grasp demonstration to learn an implicit local surface representation model from a small dataset of object meshes. At inference time, this model is used to transfer grasps to novel objects by identifying the most geometrically similar surfaces to the one on which the expert grasp is demonstrated. Our model is trained entirely in simulation and is evaluated on simulated and real-world objects that are not seen during training. Evaluations indicate that grasp transfer to unseen object categories using this approach can be successfully performed both in simulation and real-world experiments. The simulation results also show that the proposed approach leads to better spatial precision and grasp accuracy compared to a baseline approach.Here's the text in Traditional Chinese:objects we interact with and manipulate often share similar parts, such as handles, that allow us to transfer our actions flexibly due to their shared functionality. This work addresses the problem of transferring a grasp experience or a demonstration to a novel object that shares shape similarities with objects the robot has previously encountered. Existing approaches for solving this problem are typically restricted to a specific object category or a parametric shape. Our approach, however, can transfer grasps associated with implicit models of local surfaces shared across object categories. Specifically, we employ a single expert grasp demonstration to learn an implicit local surface representation model from a small dataset of object meshes. At inference time, this model is used to transfer grasps to novel objects by identifying the most geometrically similar surfaces to the one on which the expert grasp is demonstrated. Our model is trained entirely in simulation and is evaluated on simulated and real-world objects that are not seen during training. Evaluations indicate that grasp transfer to unseen object categories using this approach can be successfully performed both in simulation and real-world experiments. The simulation results also show that the proposed approach leads to better spatial precision and grasp accuracy compared to a baseline approach.
</details></li>
</ul>
<hr>
<h2 id="Neuromorphic-Seatbelt-State-Detection-for-In-Cabin-Monitoring-with-Event-Cameras"><a href="#Neuromorphic-Seatbelt-State-Detection-for-In-Cabin-Monitoring-with-Event-Cameras" class="headerlink" title="Neuromorphic Seatbelt State Detection for In-Cabin Monitoring with Event Cameras"></a>Neuromorphic Seatbelt State Detection for In-Cabin Monitoring with Event Cameras</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07802">http://arxiv.org/abs/2308.07802</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Kielty, Cian Ryan, Mehdi Sefidgar Dilmaghani, Waseem Shariff, Joe Lemley, Peter Corcoran</li>
<li>for: 这个论文主要是为了研究如何使用事件摄像机在座位安全系统中检测安全带状态。</li>
<li>methods: 这篇论文使用了事件生成器生成的Synthetic neuromorphic frames，以及基于循环卷积神经网络的检测算法。</li>
<li>results: 论文的实验结果显示，在 binary 分类任务中，fastened&#x2F;unfastened 帧的识别精度为 0.989 和 0.944 分别在 simulated 和 real test sets 上。当问题扩展到包括快速安全带的扭矩时，分别达到了 0.964 和 0.846 的 F1 分数。<details>
<summary>Abstract</summary>
Neuromorphic vision sensors, or event cameras, differ from conventional cameras in that they do not capture images at a specified rate. Instead, they asynchronously log local brightness changes at each pixel. As a result, event cameras only record changes in a given scene, and do so with very high temporal resolution, high dynamic range, and low power requirements. Recent research has demonstrated how these characteristics make event cameras extremely practical sensors in driver monitoring systems (DMS), enabling the tracking of high-speed eye motion and blinks. This research provides a proof of concept to expand event-based DMS techniques to include seatbelt state detection. Using an event simulator, a dataset of 108,691 synthetic neuromorphic frames of car occupants was generated from a near-infrared (NIR) dataset, and split into training, validation, and test sets for a seatbelt state detection algorithm based on a recurrent convolutional neural network (CNN). In addition, a smaller set of real event data was collected and reserved for testing. In a binary classification task, the fastened/unfastened frames were identified with an F1 score of 0.989 and 0.944 on the simulated and real test sets respectively. When the problem extended to also classify the action of fastening/unfastening the seatbelt, respective F1 scores of 0.964 and 0.846 were achieved.
</details>
<details>
<summary>摘要</summary>
neuromorphic vision sensors 或事件摄像头，与传统摄像头不同，不是预先定义的帧率来捕捉图像。相反，它们在每个像素上逐渐记录当地明亮变化。这意味着事件摄像头只记录场景中的变化，并且具有非常高的时间分辨率、高动态范围和低功耗要求。最新的研究表明，这些特点使得事件摄像头成为了非常实用的护身伞系统（DMS）感知器，可以跟踪高速眼动和耶飞。这些研究提供了扩展事件基于DMS技术的证明，包括座席安全带状态检测。使用事件模拟器，一个由近红外（NIR）数据集生成的108,691个神经元模拟帧的车Occupants dataset被生成，并被分配到训练、验证和测试集中。此外，一个更小的真实事件数据集也被收集并保留用于测试。在一个二分类任务中，带fastened/unfastened帧被识别出来，F1分数分别为0.989和0.944。当问题扩展到还包括快速安装/解除座席安全带的动作时，分别获得了0.964和0.846的F1分数。
</details></li>
</ul>
<hr>
<h2 id="Handwritten-Stenography-Recognition-and-the-LION-Dataset"><a href="#Handwritten-Stenography-Recognition-and-the-LION-Dataset" class="headerlink" title="Handwritten Stenography Recognition and the LION Dataset"></a>Handwritten Stenography Recognition and the LION Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07799">http://arxiv.org/abs/2308.07799</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://zenodo.org/record/8249818">https://zenodo.org/record/8249818</a></li>
<li>paper_authors: Raphaela Heil, Malin Nauwerck</li>
<li>for: 这篇论文的目的是建立一个基准模型，用于识别手写stenography。</li>
<li>methods: 这篇论文使用了现有的文本识别模型，并应用了四种不同的编码方法，将目标序列转换成表示selected aspects of the writing system。此外，还使用了预训练方案，基于合成数据。</li>
<li>results: 基准模型在测试集上的平均字符错误率（CER）为29.81%，word error rate（WER）为55.14%。通过结合stenography-specific target sequence encodings、预训练和细化，可以大幅降低测试错误率，CER在24.5% - 26%之间，WER在44.8% - 48.2%之间。<details>
<summary>Abstract</summary>
Purpose: In this paper, we establish a baseline for handwritten stenography recognition, using the novel LION dataset, and investigate the impact of including selected aspects of stenographic theory into the recognition process. We make the LION dataset publicly available with the aim of encouraging future research in handwritten stenography recognition.   Methods: A state-of-the-art text recognition model is trained to establish a baseline. Stenographic domain knowledge is integrated by applying four different encoding methods that transform the target sequence into representations, which approximate selected aspects of the writing system. Results are further improved by integrating a pre-training scheme, based on synthetic data.   Results: The baseline model achieves an average test character error rate (CER) of 29.81% and a word error rate (WER) of 55.14%. Test error rates are reduced significantly by combining stenography-specific target sequence encodings with pre-training and fine-tuning, yielding CERs in the range of 24.5% - 26% and WERs of 44.8% - 48.2%.   Conclusion: The obtained results demonstrate the challenging nature of stenography recognition. Integrating stenography-specific knowledge, in conjunction with pre-training and fine-tuning on synthetic data, yields considerable improvements. Together with our precursor study on the subject, this is the first work to apply modern handwritten text recognition to stenography. The dataset and our code are publicly available via Zenodo.
</details>
<details>
<summary>摘要</summary>
目的：在这篇论文中，我们建立了手写stenography认识基线，使用新的LION数据集，并研究包括选择的stenographic理论方面的影响。我们将LION数据集公开提供，以促进未来的手写stenography认识研究。方法：我们使用现代文本认识模型进行基线建立。stenographic领域知识被集成，通过将目标序列转换为表示形式，以估计选择的stenographic特征。此外，我们还使用基于Synthetic数据的预训练方案，进一步改进结果。结果：基线模型在测试集上的平均字符错误率（CER）为29.81%，单词错误率（WER）为55.14%。通过将stenography特有的target序列编码与预训练和细化结合使用，可以将测试错误率显著降低到24.5% - 26%之间的CER，以及44.8% - 48.2%之间的WER。结论：我们的结果表明stenography认识是一项非常具有挑战性的任务。通过结合stenography特有的知识和预训练和细化，可以获得显著的改进。这是现代手写文本认识在stenography领域的第一篇研究，同时我们的数据集和代码也公开提供了via Zenodo。
</details></li>
</ul>
<hr>
<h2 id="DiffV2S-Diffusion-based-Video-to-Speech-Synthesis-with-Vision-guided-Speaker-Embedding"><a href="#DiffV2S-Diffusion-based-Video-to-Speech-Synthesis-with-Vision-guided-Speaker-Embedding" class="headerlink" title="DiffV2S: Diffusion-based Video-to-Speech Synthesis with Vision-guided Speaker Embedding"></a>DiffV2S: Diffusion-based Video-to-Speech Synthesis with Vision-guided Speaker Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07787">http://arxiv.org/abs/2308.07787</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/joannahong/diffv2s">https://github.com/joannahong/diffv2s</a></li>
<li>paper_authors: Jeongsoo Choi, Joanna Hong, Yong Man Ro</li>
<li>for: 这个论文的目的是提出一种基于自我超vised学习模型和提示调整技术的视觉导航 speaker embedding抽取器，以便在推理时不需要外部音频信息。</li>
<li>methods: 该论文使用了一种基于自我超vised学习模型和提示调整技术的视觉导航 speaker embedding抽取器，以及一种基于这些 speaker embedding 和视觉表示的扩散基于视频到语音Synthesis模型。</li>
<li>results: 该论文的实验结果显示，使用该视觉导航 speaker embedding抽取器和扩散基于视频到语音Synthesis模型可以保持输入视频帧中的音频细节，同时还可以在推理时不需要外部音频信息。这些方法在比较之前的视频到语音Synthesis技术中达到了state-of-the-art的性能。<details>
<summary>Abstract</summary>
Recent research has demonstrated impressive results in video-to-speech synthesis which involves reconstructing speech solely from visual input. However, previous works have struggled to accurately synthesize speech due to a lack of sufficient guidance for the model to infer the correct content with the appropriate sound. To resolve the issue, they have adopted an extra speaker embedding as a speaking style guidance from a reference auditory information. Nevertheless, it is not always possible to obtain the audio information from the corresponding video input, especially during the inference time. In this paper, we present a novel vision-guided speaker embedding extractor using a self-supervised pre-trained model and prompt tuning technique. In doing so, the rich speaker embedding information can be produced solely from input visual information, and the extra audio information is not necessary during the inference time. Using the extracted vision-guided speaker embedding representations, we further develop a diffusion-based video-to-speech synthesis model, so called DiffV2S, conditioned on those speaker embeddings and the visual representation extracted from the input video. The proposed DiffV2S not only maintains phoneme details contained in the input video frames, but also creates a highly intelligible mel-spectrogram in which the speaker identities of the multiple speakers are all preserved. Our experimental results show that DiffV2S achieves the state-of-the-art performance compared to the previous video-to-speech synthesis technique.
</details>
<details>
<summary>摘要</summary>
最新的研究表明，视频到语音合成技术已经取得了很好的结果，即从视觉输入重建说话。然而，之前的工作很难准确地合成说话，因为缺乏足够的指导，使模型很难准确地推断出正确的内容和相应的声音。为解决这个问题，他们采用了Extra speaker embedding作为引导，从参考音频信息中获得。然而，在推断时不一定可以获得相应的音频信息，特别是在推断时。在这篇论文中，我们提出了一种新的视觉引导的Speaker embedding抽取器，使用自我超visumodel和提示调整技术。在这种情况下，可以从输入视频信息中提取出富有的Speaker embedding信息，不需要外部音频信息。使用提取的视觉引导Speaker embedding表示，我们进一步发展了一种扩散基于的视频到语音合成模型，称为DiffV2S。DiffV2S Conditioned on these speaker embeddings and the visual representation extracted from the input video, the proposed DiffV2S not only maintains the phoneme details contained in the input video frames, but also creates a highly intelligible mel-spectrogram in which the speaker identities of the multiple speakers are all preserved. Our experimental results show that DiffV2S achieves the state-of-the-art performance compared to the previous video-to-speech synthesis technique.
</details></li>
</ul>
<hr>
<h2 id="Future-Video-Prediction-from-a-Single-Frame-for-Video-Anomaly-Detection"><a href="#Future-Video-Prediction-from-a-Single-Frame-for-Video-Anomaly-Detection" class="headerlink" title="Future Video Prediction from a Single Frame for Video Anomaly Detection"></a>Future Video Prediction from a Single Frame for Video Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07783">http://arxiv.org/abs/2308.07783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Baradaran, Robert Bergevin</li>
<li>for: 这篇论文的目的是提出一个新的代理任务，即从单一帧画面中预测未来的影片，以便实现影片异常检测（VAD）中的长期运动模型。</li>
<li>methods: 这篇论文使用了一种新的semi-supervised anomaly detection方法，具体来说是使用未来帧画面预测作为代理任务，并将初始和未来的原始帧替换为它们的Semantic segmentation map，以增强模型的敏感度和精度。</li>
<li>results: 实验结果显示，这篇论文的方法能够实现长期运动模型的学习，并且与现有的预测基于VAD方法相比，具有更高的效果和精度。<details>
<summary>Abstract</summary>
Video anomaly detection (VAD) is an important but challenging task in computer vision. The main challenge rises due to the rarity of training samples to model all anomaly cases. Hence, semi-supervised anomaly detection methods have gotten more attention, since they focus on modeling normals and they detect anomalies by measuring the deviations from normal patterns. Despite impressive advances of these methods in modeling normal motion and appearance, long-term motion modeling has not been effectively explored so far. Inspired by the abilities of the future frame prediction proxy-task, we introduce the task of future video prediction from a single frame, as a novel proxy-task for video anomaly detection. This proxy-task alleviates the challenges of previous methods in learning longer motion patterns. Moreover, we replace the initial and future raw frames with their corresponding semantic segmentation map, which not only makes the method aware of object class but also makes the prediction task less complex for the model. Extensive experiments on the benchmark datasets (ShanghaiTech, UCSD-Ped1, and UCSD-Ped2) show the effectiveness of the method and the superiority of its performance compared to SOTA prediction-based VAD methods.
</details>
<details>
<summary>摘要</summary>
视频异常检测（VAD）是计算机视觉中的一项重要而困难的任务。主要挑战在于缺乏异常情况下训练样本，因此半监督异常检测方法在过去几年中得到了更多的关注，这些方法通过模型常规动作和出现异常的方式进行检测。 despite impressive advances of these methods in modeling normal motion and appearance, long-term motion modeling has not been effectively explored so far. 鼓励了未来帧预测代理任务的能力，我们引入了从单一帧预测未来视频的任务，作为一种新的代理任务，以解决先前方法学习更长的动作模式的挑战。此外，我们将初始和未来的原始帧替换为它们对应的 semantic segmentation map，这不仅使得方法能够识别物体类型，还使得预测任务对模型更加简单。我们在 ShanghaiTech、UCSD-Ped1 和 UCSD-Ped2  benchmark datasets 进行了广泛的实验，并证明了方法的有效性和与State-of-the-art（SOTA）预测基于 VAD 方法的性能的superiority。
</details></li>
</ul>
<hr>
<h2 id="Learning-Image-Deraining-Transformer-Network-with-Dynamic-Dual-Self-Attention"><a href="#Learning-Image-Deraining-Transformer-Network-with-Dynamic-Dual-Self-Attention" class="headerlink" title="Learning Image Deraining Transformer Network with Dynamic Dual Self-Attention"></a>Learning Image Deraining Transformer Network with Dynamic Dual Self-Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07781">http://arxiv.org/abs/2308.07781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhentao Fan, Hongming Chen, Yufeng Li</li>
<li>for: 本研究旨在提出一种高效的单张图像雨水去除方法，使用Transformer架构和动态双层自注意（DDSA）精确地捕捉图像中的雨水信息。</li>
<li>methods: 本方法使用了动态双层自注意（DDSA）精确地选择图像中的相似性值，并结合了一种新的空间增强Feedforward网络（SEFN）来提高图像的重建质量。</li>
<li>results: 经验表明，本方法在标准数据集上达到了高质量的雨水去除效果，并且在不同的雨水环境下都能够保持高度的稳定性。<details>
<summary>Abstract</summary>
Recently, Transformer-based architecture has been introduced into single image deraining task due to its advantage in modeling non-local information. However, existing approaches tend to integrate global features based on a dense self-attention strategy since it tend to uses all similarities of the tokens between the queries and keys. In fact, this strategy leads to ignoring the most relevant information and inducing blurry effect by the irrelevant representations during the feature aggregation. To this end, this paper proposes an effective image deraining Transformer with dynamic dual self-attention (DDSA), which combines both dense and sparse attention strategies to better facilitate clear image reconstruction. Specifically, we only select the most useful similarity values based on top-k approximate calculation to achieve sparse attention. In addition, we also develop a novel spatial-enhanced feed-forward network (SEFN) to further obtain a more accurate representation for achieving high-quality derained results. Extensive experiments on benchmark datasets demonstrate the effectiveness of our proposed method.
</details>
<details>
<summary>摘要</summary>
最近，基于Transformer架构的单图雨水减除技术在单图雨水减除任务中得到应用。这是因为Transformer架构可以更好地模型非本地信息。然而，现有的方法通常会将全球特征集成到一个笔 dense self-attention策略中，这会导致忽略最重要的信息并通过不相关的表示导致图像重建的模糊效果。为了解决这个问题，本文提出了一种高效的图像雨水减除Transformer（DDSA），它结合了密集和疏缺注意策略来更好地促进清晰图像重建。具体来说，我们只选择最有用的相似性值，并通过top-k相似计算来实现疏缺注意。此外，我们还开发了一种新的空间增强Feed-Forward网络（SEFN），以更好地获得更高质量的雨水减除结果。我们在标准数据集上进行了广泛的实验，并证明了我们的提议的效果。
</details></li>
</ul>
<hr>
<h2 id="An-Interpretable-Machine-Learning-Model-with-Deep-Learning-based-Imaging-Biomarkers-for-Diagnosis-of-Alzheimer’s-Disease"><a href="#An-Interpretable-Machine-Learning-Model-with-Deep-Learning-based-Imaging-Biomarkers-for-Diagnosis-of-Alzheimer’s-Disease" class="headerlink" title="An Interpretable Machine Learning Model with Deep Learning-based Imaging Biomarkers for Diagnosis of Alzheimer’s Disease"></a>An Interpretable Machine Learning Model with Deep Learning-based Imaging Biomarkers for Diagnosis of Alzheimer’s Disease</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07778">http://arxiv.org/abs/2308.07778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjie Kang, Bo Li, Janne M. Papma, Lize C. Jiskoot, Peter Paul De Deyn, Geert Jan Biessels, Jurgen A. H. R. Claassen, Huub A. M. Middelkoop, Wiesje M. van der Flier, Inez H. G. B. Ramakers, Stefan Klein, Esther E. Bron</li>
<li>for: 本研究旨在提出一种可解释的机器学习框架，用于自动早期诊断阿尔茨heimer病（AD）。</li>
<li>methods: 本研究使用了可解释的机器学习模型（EBM），并使用深度学习来提取特征。</li>
<li>results: 研究在Alzheimer’s Disease Neuroimaging Initiative（ADNI）数据集上 achieved accuracy of 0.883和area-under-the-curve（AUC）of 0.970在AD和控制分类中。在一个外部测试集上也达到了accuracy of 0.778和AUC of 0.887在AD和主观认知下降（SCD）分类中。 compared to使用体量生物标志代替深度学习特征的EBM模型，以及一个优化的 convolutional neural network（CNN）模型。<details>
<summary>Abstract</summary>
Machine learning methods have shown large potential for the automatic early diagnosis of Alzheimer's Disease (AD). However, some machine learning methods based on imaging data have poor interpretability because it is usually unclear how they make their decisions. Explainable Boosting Machines (EBMs) are interpretable machine learning models based on the statistical framework of generalized additive modeling, but have so far only been used for tabular data. Therefore, we propose a framework that combines the strength of EBM with high-dimensional imaging data using deep learning-based feature extraction. The proposed framework is interpretable because it provides the importance of each feature. We validated the proposed framework on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset, achieving accuracy of 0.883 and area-under-the-curve (AUC) of 0.970 on AD and control classification. Furthermore, we validated the proposed framework on an external testing set, achieving accuracy of 0.778 and AUC of 0.887 on AD and subjective cognitive decline (SCD) classification. The proposed framework significantly outperformed an EBM model using volume biomarkers instead of deep learning-based features, as well as an end-to-end convolutional neural network (CNN) with optimized architecture.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Dual-path-TokenLearner-for-Remote-Photoplethysmography-based-Physiological-Measurement-with-Facial-Videos"><a href="#Dual-path-TokenLearner-for-Remote-Photoplethysmography-based-Physiological-Measurement-with-Facial-Videos" class="headerlink" title="Dual-path TokenLearner for Remote Photoplethysmography-based Physiological Measurement with Facial Videos"></a>Dual-path TokenLearner for Remote Photoplethysmography-based Physiological Measurement with Facial Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07771">http://arxiv.org/abs/2308.07771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Qian, Dan Guo, Kun Li, Xilan Tian, Meng Wang</li>
<li>for: 这个论文旨在提出一种基于 Transformer 框架的远程光谱 Plethysmography (rPPG) 测量方法，以减少干扰因素的影响，提高测量精度。</li>
<li>methods: 该方法使用了两种 TokenLearner（S-TL 和 T-TL）来捕捉不同的 facial ROI 之间的关系，以及 quasi- periodic  patrern 的推断，以减少干扰因素的影响。</li>
<li>results: 在四个 physiological measurement  benchmark 数据集上进行了广泛的实验，结果表明 Dual-TL 可以在 intra- 和 cross-dataset 测试中达到 state-of-the-art 性能，表明其在 rPPG 测量中的潜在应用前景。<details>
<summary>Abstract</summary>
Remote photoplethysmography (rPPG) based physiological measurement is an emerging yet crucial vision task, whose challenge lies in exploring accurate rPPG prediction from facial videos accompanied by noises of illumination variations, facial occlusions, head movements, \etc, in a non-contact manner. Existing mainstream CNN-based models make efforts to detect physiological signals by capturing subtle color changes in facial regions of interest (ROI) caused by heartbeats. However, such models are constrained by the limited local spatial or temporal receptive fields in the neural units. Unlike them, a native Transformer-based framework called Dual-path TokenLearner (Dual-TL) is proposed in this paper, which utilizes the concept of learnable tokens to integrate both spatial and temporal informative contexts from the global perspective of the video. Specifically, the proposed Dual-TL uses a Spatial TokenLearner (S-TL) to explore associations in different facial ROIs, which promises the rPPG prediction far away from noisy ROI disturbances. Complementarily, a Temporal TokenLearner (T-TL) is designed to infer the quasi-periodic pattern of heartbeats, which eliminates temporal disturbances such as head movements. The two TokenLearners, S-TL and T-TL, are executed in a dual-path mode. This enables the model to reduce noise disturbances for final rPPG signal prediction. Extensive experiments on four physiological measurement benchmark datasets are conducted. The Dual-TL achieves state-of-the-art performances in both intra- and cross-dataset testings, demonstrating its immense potential as a basic backbone for rPPG measurement. The source code is available at \href{https://github.com/VUT-HFUT/Dual-TL}{https://github.com/VUT-HFUT/Dual-TL}
</details>
<details>
<summary>摘要</summary>
distant photoplethysmography (rPPG) 基于视频的生物指标测量是一个emerging yet crucial vision task， whose challenge lies in accurately predicting rPPG from facial videos accompanied by illumination variations, facial occlusions, head movements, etc. in a non-contact manner. Existing mainstream CNN-based models make efforts to detect physiological signals by capturing subtle color changes in facial regions of interest (ROI) caused by heartbeats. However, such models are constrained by the limited local spatial or temporal receptive fields in the neural units. Unlike them, a native Transformer-based framework called Dual-path TokenLearner (Dual-TL) is proposed in this paper, which utilizes the concept of learnable tokens to integrate both spatial and temporal informative contexts from the global perspective of the video. Specifically, the proposed Dual-TL uses a Spatial TokenLearner (S-TL) to explore associations in different facial ROIs, which promises the rPPG prediction far away from noisy ROI disturbances. Complementarily, a Temporal TokenLearner (T-TL) is designed to infer the quasi-periodic pattern of heartbeats, which eliminates temporal disturbances such as head movements. The two TokenLearners, S-TL and T-TL, are executed in a dual-path mode. This enables the model to reduce noise disturbances for final rPPG signal prediction. Extensive experiments on four physiological measurement benchmark datasets are conducted. The Dual-TL achieves state-of-the-art performances in both intra- and cross-dataset testings, demonstrating its immense potential as a basic backbone for rPPG measurement. The source code is available at \href{https://github.com/VUT-HFUT/Dual-TL}{https://github.com/VUT-HFUT/Dual-TL}.
</details></li>
</ul>
<hr>
<h2 id="Multi-scale-Promoted-Self-adjusting-Correlation-Learning-for-Facial-Action-Unit-Detection"><a href="#Multi-scale-Promoted-Self-adjusting-Correlation-Learning-for-Facial-Action-Unit-Detection" class="headerlink" title="Multi-scale Promoted Self-adjusting Correlation Learning for Facial Action Unit Detection"></a>Multi-scale Promoted Self-adjusting Correlation Learning for Facial Action Unit Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07770">http://arxiv.org/abs/2308.07770</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuankaishen2001/Self-adjusting-AU">https://github.com/yuankaishen2001/Self-adjusting-AU</a></li>
<li>paper_authors: Xin Liu, Kaishen Yuan, Xuesong Niu, Jingang Shi, Zitong Yu, Huanjing Yue, Jingyu Yang</li>
<li>for: 这个论文旨在提出一种新的自适应AU相关学习方法（SACL），以提高人类表情识别task中AU相关性的准确性和效率。</li>
<li>methods: 本文使用了一种自适应AU相关学习方法，通过约束AU相关性的学习和更新，以及一种多层学习（MSFL）方法，将多个尺度的特征学习到一个统一的表征中。</li>
<li>results: 实验结果显示，提案的方法在广泛使用的AU识别benchmark datasets上表现出色，与现有的方法相比，只需28.7%和12.0%的parameters和FLOPs，并且具有较高的准确性和稳定性。<details>
<summary>Abstract</summary>
Facial Action Unit (AU) detection is a crucial task in affective computing and social robotics as it helps to identify emotions expressed through facial expressions. Anatomically, there are innumerable correlations between AUs, which contain rich information and are vital for AU detection. Previous methods used fixed AU correlations based on expert experience or statistical rules on specific benchmarks, but it is challenging to comprehensively reflect complex correlations between AUs via hand-crafted settings. There are alternative methods that employ a fully connected graph to learn these dependencies exhaustively. However, these approaches can result in a computational explosion and high dependency with a large dataset. To address these challenges, this paper proposes a novel self-adjusting AU-correlation learning (SACL) method with less computation for AU detection. This method adaptively learns and updates AU correlation graphs by efficiently leveraging the characteristics of different levels of AU motion and emotion representation information extracted in different stages of the network. Moreover, this paper explores the role of multi-scale learning in correlation information extraction, and design a simple yet effective multi-scale feature learning (MSFL) method to promote better performance in AU detection. By integrating AU correlation information with multi-scale features, the proposed method obtains a more robust feature representation for the final AU detection. Extensive experiments show that the proposed method outperforms the state-of-the-art methods on widely used AU detection benchmark datasets, with only 28.7\% and 12.0\% of the parameters and FLOPs of the best method, respectively. The code for this method is available at \url{https://github.com/linuxsino/Self-adjusting-AU}.
</details>
<details>
<summary>摘要</summary>
Facial Action Unit (AU) 检测是影响情感计算和社会机器人的关键任务，因为它帮助确定人脸表达中的情感。生物学上来说，AU 之间存在无数关系，这些关系含有丰富的信息，对 AU 检测至关重要。以前的方法通过专家经验或统计规则在特定基准上预先定义 AU 关系，但这些方法无法全面反映复杂的 AU 关系。其他方法使用完全连接图来学习这些依赖关系，但这些方法可能会导致计算暴涨和大量数据依赖。为解决这些挑战，本文提出了一种新的自适应AU correlation学习方法（SACL），它具有较少的计算量，但能够提高 AU 检测的性能。SACL 方法通过有效地利用不同层次AU动作特征和情感表示信息来学习和更新 AU 关系图。此外，本文还探讨了多尺度学习在相关信息提取中的作用，并设计了一种简单 yet 有效的多尺度特征学习（MSFL）方法，以提高 AU 检测的性能。通过将 AU 关系信息与多尺度特征结合，提出的方法可以获得更加稳定的特征表示，进而提高 AU 检测的准确率。广泛的实验表明，提出的方法在多个常用 AU 检测基准数据集上的性能较之前的状态艺法高，仅使用 28.7% 和 12.0% 的参数和 FLOPs。代码可以在 <https://github.com/linuxsino/Self-adjusting-AU> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Whale-Detection-Enhancement-through-Synthetic-Satellite-Images"><a href="#Whale-Detection-Enhancement-through-Synthetic-Satellite-Images" class="headerlink" title="Whale Detection Enhancement through Synthetic Satellite Images"></a>Whale Detection Enhancement through Synthetic Satellite Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07766">http://arxiv.org/abs/2308.07766</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/prgumd/seadronesim2">https://github.com/prgumd/seadronesim2</a></li>
<li>paper_authors: Akshaj Gaur, Cheng Liu, Xiaomin Lin, Nare Karapetyan, Yiannis Aloimonos</li>
<li>for: 该研究目的是开发一个名为SeaDroneSim2的测试环境和数据集，以提高鲸鱼检测和减少训练数据收集的努力。</li>
<li>methods: 该研究使用现代计算机视觉算法和人工智能技术来生成synthetic图像数据集，以便用于训练机器学习算法。</li>
<li>results: 研究发现，通过将synthetic数据集与实际数据集相结合训练，可以提高鲸鱼检测性能15%，而不需要额外的数据收集努力。<details>
<summary>Abstract</summary>
With a number of marine populations in rapid decline, collecting and analyzing data about marine populations has become increasingly important to develop effective conservation policies for a wide range of marine animals, including whales. Modern computer vision algorithms allow us to detect whales in images in a wide range of domains, further speeding up and enhancing the monitoring process. However, these algorithms heavily rely on large training datasets, which are challenging and time-consuming to collect particularly in marine or aquatic environments. Recent advances in AI however have made it possible to synthetically create datasets for training machine learning algorithms, thus enabling new solutions that were not possible before. In this work, we present a solution - SeaDroneSim2 benchmark suite, which addresses this challenge by generating aerial, and satellite synthetic image datasets to improve the detection of whales and reduce the effort required for training data collection. We show that we can achieve a 15% performance boost on whale detection compared to using the real data alone for training, by augmenting a 10% real data. We open source both the code of the simulation platform SeaDroneSim2 and the dataset generated through it.
</details>
<details>
<summary>摘要</summary>
“由于海洋生物种群在快速减少，收集和分析海洋生物数据已成为保护各种海洋动物，包括鲸鱼的重要策略。现代计算机视觉算法可以在各种领域中检测鲸鱼的图像，从而加速和提高监测过程。然而，这些算法具有大量训练数据的需求，特别是在海洋或水生环境中收集这些数据是困难和耗时的。最近的人工智能技术 however 使得可以 sintetically create datasets for training machine learning algorithms，从而开启了以前不可能的解决方案。在这项工作中，我们提出一个解决方案 - SeaDroneSim2  benchmark suite，它解决了这个挑战，通过生成航空和卫星synthetic图像数据，以提高鲸鱼检测的准确率。我们证明，通过将10%的实际数据与15%的synthetic数据混合，可以在训练中提高鲸鱼检测的性能，相比使用实际数据 alone。我们开源了 SeaDroneSim2 的代码和生成的数据集。”Note: The translation is done using Google Translate and may not be perfect. Please let me know if you need any further assistance.
</details></li>
</ul>
<hr>
<h2 id="CASPNet-Joint-Multi-Agent-Motion-Prediction"><a href="#CASPNet-Joint-Multi-Agent-Motion-Prediction" class="headerlink" title="CASPNet++: Joint Multi-Agent Motion Prediction"></a>CASPNet++: Joint Multi-Agent Motion Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07751">http://arxiv.org/abs/2308.07751</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maximilian Schäfer, Kun Zhao, Anton Kummert</li>
<li>for: 本研究旨在提高自动驾驶技术的支持，具体来说是预测道路用户未来的运动。</li>
<li>methods: 本研究使用Context-Aware Scene Prediction Network (CASPNet)的改进版本CASPNet++，通过改进景象理解和交互模型，支持场景中所有道路用户的联合预测。此外，还引入了基于实例的输出头，以提供多模态的轨迹。</li>
<li>results: 在EXTENSIVE量化和质量分析中，我们展示了CASPNet++在使用和融合多种环境输入源（如HD地图、雷达探测和激光分 segmentation）的能力。在 nuScenes 城市预测数据集上测试，CASPNet++达到了现状之最的性能。模型已经在测试车辆中部署，并在实时下运行，具有moderate的计算资源。<details>
<summary>Abstract</summary>
The prediction of road users' future motion is a critical task in supporting advanced driver-assistance systems (ADAS). It plays an even more crucial role for autonomous driving (AD) in enabling the planning and execution of safe driving maneuvers. Based on our previous work, Context-Aware Scene Prediction Network (CASPNet), an improved system, CASPNet++, is proposed. In this work, we focus on further enhancing the interaction modeling and scene understanding to support the joint prediction of all road users in a scene using spatiotemporal grids to model future occupancy. Moreover, an instance-based output head is introduced to provide multi-modal trajectories for agents of interest. In extensive quantitative and qualitative analysis, we demonstrate the scalability of CASPNet++ in utilizing and fusing diverse environmental input sources such as HD maps, Radar detection, and Lidar segmentation. Tested on the urban-focused prediction dataset nuScenes, CASPNet++ reaches state-of-the-art performance. The model has been deployed in a testing vehicle, running in real-time with moderate computational resources.
</details>
<details>
<summary>摘要</summary>
预测路用户未来运动是智能驾驶技术支持的关键任务之一，尤其是自动驾驶（AD）。在这种情况下，预测安全驾驶动作的能力变得非常重要。基于我们之前的工作，Context-Aware Scene Prediction Network（CASPNet），我们提出了改进的系统CASPNet++。在这个工作中，我们将更进一步地提高交互模型和场景理解，以支持场景中所有道路用户的未来运动预测。此外，我们还引入了基于实例的输出头，以提供多模态轨迹 для关注点。在详细的量化和质量分析中，我们示出了CASPNet++在使用和融合多种环境输入源，如高分辨环境地图、雷达探测和激光分 segmentation 的可扩展性。在 nuScenes 城市预测数据集上，CASPNet++ 达到了状态agh 的性能。模型已经在测试车辆上部署，在实时运行中具有中等计算资源。
</details></li>
</ul>
<hr>
<h2 id="ChartDETR-A-Multi-shape-Detection-Network-for-Visual-Chart-Recognition"><a href="#ChartDETR-A-Multi-shape-Detection-Network-for-Visual-Chart-Recognition" class="headerlink" title="ChartDETR: A Multi-shape Detection Network for Visual Chart Recognition"></a>ChartDETR: A Multi-shape Detection Network for Visual Chart Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07743">http://arxiv.org/abs/2308.07743</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenyuan Xue, Dapeng Chen, Baosheng Yu, Yifei Chen, Sai Zhou, Wei Peng<br>for: 这篇论文的目的是提出一种基于变换器的多形态检测器，以便自动从图表图像中识别表头和数据元素。methods: 该方法使用变换器来地址现有方法中的分组错误，通过引入查询组来预测所有数据元素形状，从而消除后期处理步骤。results: 该方法在三个 dataset 上达到了竞争性的 результа，包括在 Adobe Synthetic 上达到了 0.98 的 F1 分数，与之前最佳模型的 0.71  F1 分数相比显著提高。此外，我们也实现了一个新的状态对标结果，达到了 ExcelChart400k 上的 0.97。<details>
<summary>Abstract</summary>
Visual chart recognition systems are gaining increasing attention due to the growing demand for automatically identifying table headers and values from chart images. Current methods rely on keypoint detection to estimate data element shapes in charts but suffer from grouping errors in post-processing. To address this issue, we propose ChartDETR, a transformer-based multi-shape detector that localizes keypoints at the corners of regular shapes to reconstruct multiple data elements in a single chart image. Our method predicts all data element shapes at once by introducing query groups in set prediction, eliminating the need for further postprocessing. This property allows ChartDETR to serve as a unified framework capable of representing various chart types without altering the network architecture, effectively detecting data elements of diverse shapes. We evaluated ChartDETR on three datasets, achieving competitive results across all chart types without any additional enhancements. For example, ChartDETR achieved an F1 score of 0.98 on Adobe Synthetic, significantly outperforming the previous best model with a 0.71 F1 score. Additionally, we obtained a new state-of-the-art result of 0.97 on ExcelChart400k. The code will be made publicly available.
</details>
<details>
<summary>摘要</summary>
“图表识别系统在当前receiving increasing attention，主要是因为需要自动从图表图像中识别表头和数据值。现有方法通过关键点检测来估算图表元素的形状，但是会在后处理中出现分组错误。为解决这个问题，我们提出了 ChartDETR，一种基于transformer的多形态检测器，可以在单个图表图像中寻找多个数据元素的角点。我们的方法通过设置查询组来预测所有数据元素的形状，从而消除后处理步骤。这个特性使得 ChartDETR 可以作为一个通用的框架，无需修改网络结构，可以有效地检测各种图表类型中的数据元素。我们对 ChartDETR 进行了三个数据集的评估，实现了所有图表类型中的竞争性结果，不需要任何额外增强。例如，在 Adobe Synthetic 数据集上，ChartDETR  achieved an F1 score of 0.98，与之前的最佳模型（F1 score 0.71）有所显著超越。此外，我们在 ExcelChart400k 数据集上获得了新的州际最佳结果（F1 score 0.97）。代码将会公开发布。”
</details></li>
</ul>
<hr>
<h2 id="Identity-Consistent-Aggregation-for-Video-Object-Detection"><a href="#Identity-Consistent-Aggregation-for-Video-Object-Detection" class="headerlink" title="Identity-Consistent Aggregation for Video Object Detection"></a>Identity-Consistent Aggregation for Video Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07737">http://arxiv.org/abs/2308.07737</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bladewaltz1/clipvid">https://github.com/bladewaltz1/clipvid</a></li>
<li>paper_authors: Chaorui Deng, Da Chen, Qi Wu</li>
<li>for: 在视频对象检测（VID）中，通常利用视频中的丰富时间上下文来提高每帧中的对象表示。现有方法往往对不同对象的时间上下文进行一个汇总，而忽略其不同身份。而intuitively, 将不同对象的本地视图在不同帧中汇总可能为对象的理解提供更好的帮助。因此，在这篇论文中，我们目的是使模型能够专注于每个对象的一致性时间上下文，从而获得更全面的对象表示，并处理快速的对象出现变化，如遮挡、运动模糊等。</li>
<li>methods: 我们提出了一种名为ClipVID的VID模型，它具有特定于个体的汇总（ICA）层，可以为每个对象提取更细致的一致性时间上下文。通过设计非重复的集 Prediction 策略，我们减少了模型的重复性，使ICA层非常高效。此外，我们还设计了一个并行的剪辑clip-wise预测方案，使得整个视频clip的预测都可以在一个时间内完成。</li>
<li>results: 我们的方法在ImageNet VID数据集上实现了state-of-the-art（SOTA）性能（84.7% mAP），而且在7倍的运行速度（39.3 fps）上达到了前一代SOTA的速度。<details>
<summary>Abstract</summary>
In Video Object Detection (VID), a common practice is to leverage the rich temporal contexts from the video to enhance the object representations in each frame. Existing methods treat the temporal contexts obtained from different objects indiscriminately and ignore their different identities. While intuitively, aggregating local views of the same object in different frames may facilitate a better understanding of the object. Thus, in this paper, we aim to enable the model to focus on the identity-consistent temporal contexts of each object to obtain more comprehensive object representations and handle the rapid object appearance variations such as occlusion, motion blur, etc. However, realizing this goal on top of existing VID models faces low-efficiency problems due to their redundant region proposals and nonparallel frame-wise prediction manner. To aid this, we propose ClipVID, a VID model equipped with Identity-Consistent Aggregation (ICA) layers specifically designed for mining fine-grained and identity-consistent temporal contexts. It effectively reduces the redundancies through the set prediction strategy, making the ICA layers very efficient and further allowing us to design an architecture that makes parallel clip-wise predictions for the whole video clip. Extensive experimental results demonstrate the superiority of our method: a state-of-the-art (SOTA) performance (84.7% mAP) on the ImageNet VID dataset while running at a speed about 7x faster (39.3 fps) than previous SOTAs.
</details>
<details>
<summary>摘要</summary>
在视频对象检测（VID）中，一种常见的做法是利用视频中的丰富时间上下文来强化每帧中的对象表示。现有的方法对不同对象的时间上下文待遇一样，而忽略了它们的不同标识。而 intuitively，将不同对象的本地视图在不同帧中聚合可能会更好地理解这些对象。因此，在这篇论文中，我们目标是让模型能够关注每个对象的一致性时间上下文，以获得更全面的对象表示，并处理快速的对象出现变化，如遮挡、动态模糊等。然而，在现有的 VID 模型之上实现这个目标存在低效率的问题，主要是因为它们的重复的区域提案和非平行的帧次预测方式。为了解决这个问题，我们提议 ClipVID，一种具有一致性聚合（ICA）层的 VID 模型，专门用于挖掘细致的时间上下文。它通过设置预测策略，有效减少了重复性，使 ICA 层非常高效，并让我们能够设计一个可以并行预测整个视频剪辑的架构。我们的实验结果表明，我们的方法可以达到最新的状态（SOTA）的性能（84.7% mAP）在 ImageNet VID 数据集上，而且在运行速度方面比前一代 SOTA 快约 7 倍（39.3 fps）。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Low-Rank-Instance-Adaptation-for-Universal-Neural-Image-Compression"><a href="#Dynamic-Low-Rank-Instance-Adaptation-for-Universal-Neural-Image-Compression" class="headerlink" title="Dynamic Low-Rank Instance Adaptation for Universal Neural Image Compression"></a>Dynamic Low-Rank Instance Adaptation for Universal Neural Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07733">http://arxiv.org/abs/2308.07733</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/llvy21/duic">https://github.com/llvy21/duic</a></li>
<li>paper_authors: Yue Lv, Jinxi Xiang, Jun Zhang, Wenming Yang, Xiao Han, Wei Yang</li>
<li>For: The paper aims to address the domain gap between training and inference datasets in neural image compression, and to improve the rate-distortion performance of out-of-domain images.* Methods: The proposed method uses low-rank adaptation and a dynamic gating network to update the adaptation parameters of the client’s decoder. The low-rank constraint reduces the bit rate overhead, and the dynamic gating network decides which decoder layers should employ adaptation.* Results: The proposed method significantly mitigates the domain gap and outperforms non-adaptive and instance adaptive methods with an average BD-rate improvement of approximately $19%$ and $5%$, respectively. Ablation studies confirm the method’s universality across various image compression architectures.Here is the information in Simplified Chinese text:* 用途：纸上提出了解决神经图像压缩领域中域外图像的问题，并且提高域外图像的率-损失性能。* 方法：提议使用低级别适应和动态阀网络来更新客户端解码器的适应参数。低级别约束 redues the bit rate overhead，而动态阀网络决定了哪些解码层应用适应。* 结果：提议方法能够有效地减少域外图像的域 gap，并且超越非适应方法和实例适应方法，具体是BD-rate上下降约19%和5%。杂合研究证明了方法的通用性。<details>
<summary>Abstract</summary>
The latest advancements in neural image compression show great potential in surpassing the rate-distortion performance of conventional standard codecs. Nevertheless, there exists an indelible domain gap between the datasets utilized for training (i.e., natural images) and those utilized for inference (e.g., artistic images). Our proposal involves a low-rank adaptation approach aimed at addressing the rate-distortion drop observed in out-of-domain datasets. Specifically, we perform low-rank matrix decomposition to update certain adaptation parameters of the client's decoder. These updated parameters, along with image latents, are encoded into a bitstream and transmitted to the decoder in practical scenarios. Due to the low-rank constraint imposed on the adaptation parameters, the resulting bit rate overhead is small. Furthermore, the bit rate allocation of low-rank adaptation is \emph{non-trivial}, considering the diverse inputs require varying adaptation bitstreams. We thus introduce a dynamic gating network on top of the low-rank adaptation method, in order to decide which decoder layer should employ adaptation. The dynamic adaptation network is optimized end-to-end using rate-distortion loss. Our proposed method exhibits universality across diverse image datasets. Extensive results demonstrate that this paradigm significantly mitigates the domain gap, surpassing non-adaptive methods with an average BD-rate improvement of approximately $19\%$ across out-of-domain images. Furthermore, it outperforms the most advanced instance adaptive methods by roughly $5\%$ BD-rate. Ablation studies confirm our method's ability to universally enhance various image compression architectures.
</details>
<details>
<summary>摘要</summary>
最新的神经网络图像压缩技术显示出了超越传统标准编码器的可能性。然而，存在一个不可缓和的领域差距 между训练集（即自然图像）和推理集（例如艺术图像）。我们的提议是通过对客户端解码器的certain adaptation参数进行低级精度约束来解决Rate-Distortion Drop在异领域数据集上。特别是，我们使用低级精度约束来更新客户端解码器的adaptation参数，然后将这些参数、 along with image latents，编码到bitstream中并在实际应用场景中传输。由于低级精度约束的存在，所得到的bit rate overhead很小。此外，低级精度 adaptation的bit rate分配是非易的，需要根据异类输入的需求进行调整。我们因此引入了一个动态闭合网络，以确定哪些解码层应该使用适应。这个动态闭合网络通过练习率-损失函数来优化。我们的提议显示了对各种图像压缩架构的通用性。广泛的结果表明，我们的方法可以减少异领域图像压缩中的领域差距，相比非适应方法的平均BD-rate提高约19%。此外，它还超过了最先进的实例适应方法的BD-rate提高约5%。ablation研究证明了我们的方法可以通过不同的图像压缩架构进行加强。
</details></li>
</ul>
<hr>
<h2 id="UniTR-A-Unified-and-Efficient-Multi-Modal-Transformer-for-Bird’s-Eye-View-Representation"><a href="#UniTR-A-Unified-and-Efficient-Multi-Modal-Transformer-for-Bird’s-Eye-View-Representation" class="headerlink" title="UniTR: A Unified and Efficient Multi-Modal Transformer for Bird’s-Eye-View Representation"></a>UniTR: A Unified and Efficient Multi-Modal Transformer for Bird’s-Eye-View Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07732">http://arxiv.org/abs/2308.07732</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haiyang-w/unitr">https://github.com/haiyang-w/unitr</a></li>
<li>paper_authors: Haiyang Wang, Hao Tang, Shaoshuai Shi, Aoxue Li, Zhenguo Li, Bernt Schiele, Liwei Wang</li>
<li>For: The paper is written for developing an efficient multi-modal backbone for outdoor 3D perception, which can handle a variety of modalities with unified modeling and shared parameters.* Methods: The paper uses a modality-agnostic transformer encoder to handle view-discrepant sensor data for parallel modal-wise representation learning and automatic cross-modal interaction without additional fusion steps. It also presents a novel multi-modal integration strategy that considers semantic-abundant 2D perspective and geometry-aware 3D sparse neighborhood relations.* Results: The paper achieves a new state-of-the-art performance on the nuScenes benchmark, with +1.1 NDS higher for 3D object detection and +12.0 higher mIoU for BEV map segmentation, and lower inference latency.<details>
<summary>Abstract</summary>
Jointly processing information from multiple sensors is crucial to achieving accurate and robust perception for reliable autonomous driving systems. However, current 3D perception research follows a modality-specific paradigm, leading to additional computation overheads and inefficient collaboration between different sensor data. In this paper, we present an efficient multi-modal backbone for outdoor 3D perception named UniTR, which processes a variety of modalities with unified modeling and shared parameters. Unlike previous works, UniTR introduces a modality-agnostic transformer encoder to handle these view-discrepant sensor data for parallel modal-wise representation learning and automatic cross-modal interaction without additional fusion steps. More importantly, to make full use of these complementary sensor types, we present a novel multi-modal integration strategy by both considering semantic-abundant 2D perspective and geometry-aware 3D sparse neighborhood relations. UniTR is also a fundamentally task-agnostic backbone that naturally supports different 3D perception tasks. It sets a new state-of-the-art performance on the nuScenes benchmark, achieving +1.1 NDS higher for 3D object detection and +12.0 higher mIoU for BEV map segmentation with lower inference latency. Code will be available at https://github.com/Haiyang-W/UniTR .
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Context-Aware-Pseudo-Label-Refinement-for-Source-Free-Domain-Adaptive-Fundus-Image-Segmentation"><a href="#Context-Aware-Pseudo-Label-Refinement-for-Source-Free-Domain-Adaptive-Fundus-Image-Segmentation" class="headerlink" title="Context-Aware Pseudo-Label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation"></a>Context-Aware Pseudo-Label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07731">http://arxiv.org/abs/2308.07731</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/cpr">https://github.com/xmed-lab/cpr</a></li>
<li>paper_authors: Zheang Huai, Xinpeng Ding, Yi Li, Xiaomeng Li</li>
<li>for: 这个论文是针对源数不可用的目标端做domain adaptation问题，即将源端模型训练到目标端，但源数没有可用，因此使用源模型生成的 Pseudo-label 进行更新。</li>
<li>methods: 本论文提出了一个基于上下文关系的 Pseudo-label 精度更新方法，包括将上下文相似度学习到 Pseudo-label 更新、适用于不同类别的Pixel-level和Class-level降噪方法，以及适度调整 Pseudo-label 以补偿错误更新。</li>
<li>results: 在跨领域眼部像素数据上进行实验，结果显示本方法可以实现顶尖的结果。<details>
<summary>Abstract</summary>
In the domain adaptation problem, source data may be unavailable to the target client side due to privacy or intellectual property issues. Source-free unsupervised domain adaptation (SF-UDA) aims at adapting a model trained on the source side to align the target distribution with only the source model and unlabeled target data. The source model usually produces noisy and context-inconsistent pseudo-labels on the target domain, i.e., neighbouring regions that have a similar visual appearance are annotated with different pseudo-labels. This observation motivates us to refine pseudo-labels with context relations. Another observation is that features of the same class tend to form a cluster despite the domain gap, which implies context relations can be readily calculated from feature distances. To this end, we propose a context-aware pseudo-label refinement method for SF-UDA. Specifically, a context-similarity learning module is developed to learn context relations. Next, pseudo-label revision is designed utilizing the learned context relations. Further, we propose calibrating the revised pseudo-labels to compensate for wrong revision caused by inaccurate context relations. Additionally, we adopt a pixel-level and class-level denoising scheme to select reliable pseudo-labels for domain adaptation. Experiments on cross-domain fundus images indicate that our approach yields the state-of-the-art results. Code is available at https://github.com/xmed-lab/CPR.
</details>
<details>
<summary>摘要</summary>
在领域适应问题中，源数据可能无法提供到目标客边，因为隐私或知识产权问题。源无supervised领域适应（SF-UDA）target的目标分布，仅使用源模型和目标无标签数据进行对领域的适应。源模型通常对目标领域生成噪音和无法适应的文本标签，即邻近区域可能会被不同的文本标签。这个观察动机我们更新 pseudo-label。另外，我们发现在领域差距下，同一类型的特征通常会形成一个对应的分布，这implies context relations可以从特征距离中Calculate。为此，我们提出一个context-aware pseudo-label revision方法。具体来说，我们开发了一个context-similarity learning module，用于学习context relations。接下来，我们设计了使用学习的context relations来修订 pseudo-label。此外，我们提出了calibrate revisions的方法，以补偿因为不准确的context relations而导致的错误修订。此外，我们还采用了像素级和类别级的噪音除掉方法，以选择可靠的 pseudo-label для领域适应。实验结果显示，我们的方法在跨领域基因摄像头上获得了state-of-the-art的结果。代码可以在https://github.com/xmed-lab/CPR上获取。
</details></li>
</ul>
<hr>
<h2 id="Domain-Aware-Fine-Tuning-Enhancing-Neural-Network-Adaptability"><a href="#Domain-Aware-Fine-Tuning-Enhancing-Neural-Network-Adaptability" class="headerlink" title="Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability"></a>Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07728">http://arxiv.org/abs/2308.07728</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seokhyeon Ha, Sunbeom Jung, Jungwoo Lee<br>for: 本研究旨在提高 fine-tuning 过程中的模型性能，特别是在新目标领域中。methods: 本研究提出了一种名为 Domain-Aware Fine-Tuning (DAFT) 的新方法，它包括批量准则转换和细致调整。results: 对于多个基线方法，DAFT 方法能够明显提高模型的性能，并且在各种不同的数据集上都有显著的优势。<details>
<summary>Abstract</summary>
Fine-tuning pre-trained neural network models has become a widely adopted approach across various domains. However, it can lead to the distortion of pre-trained feature extractors that already possess strong generalization capabilities. Mitigating feature distortion during adaptation to new target domains is crucial. Recent studies have shown promising results in handling feature distortion by aligning the head layer on in-distribution datasets before performing fine-tuning. Nonetheless, a significant limitation arises from the treatment of batch normalization layers during fine-tuning, leading to suboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning (DAFT), a novel approach that incorporates batch normalization conversion and the integration of linear probing and fine-tuning. Our batch normalization conversion method effectively mitigates feature distortion by reducing modifications to the neural network during fine-tuning. Additionally, we introduce the integration of linear probing and fine-tuning to optimize the head layer with gradual adaptation of the feature extractor. By leveraging batch normalization layers and integrating linear probing and fine-tuning, our DAFT significantly mitigates feature distortion and achieves improved model performance on both in-distribution and out-of-distribution datasets. Extensive experiments demonstrate that our method outperforms other baseline methods, demonstrating its effectiveness in not only improving performance but also mitigating feature distortion.
</details>
<details>
<summary>摘要</summary>
“现代化的预训练神经网络模型已成为各个领域的广泛采用方法。然而，这可能导致预训练的特征提取器受到扭曲，这会影响模型的泛化能力。避免特征扭曲在新目标领域中进行适应是非常重要。近期的研究表明，在进行适应时对头层进行对齐可以有效地避免特征扭曲。然而，在细化过程中对批处理归一化层的处理会导致表现下降。在本文中，我们提出了适应领域域特征 fine-tuning（DAFT）方法，该方法包括批处理归一化转换和细化过程中的线性探测与细化。我们的批处理归一化转换方法可以减少在细化过程中对神经网络的修改，从而避免特征扭曲。此外，我们引入了线性探测与细化的集成，以便逐渐适应头层和特征提取器。通过利用批处理归一化层和集成线性探测与细化，我们的DAFT可以有效地避免特征扭曲，并在各种预测和非预测 datasets 上显著提高模型的性能。我们的实验结果表明，我们的方法可以超越其他基准方法，说明了它的效果不仅在提高性能，还在避免特征扭曲。”
</details></li>
</ul>
<hr>
<h2 id="Real-time-Automatic-M-mode-Echocardiography-Measurement-with-Panel-Attention-from-Local-to-Global-Pixels"><a href="#Real-time-Automatic-M-mode-Echocardiography-Measurement-with-Panel-Attention-from-Local-to-Global-Pixels" class="headerlink" title="Real-time Automatic M-mode Echocardiography Measurement with Panel Attention from Local-to-Global Pixels"></a>Real-time Automatic M-mode Echocardiography Measurement with Panel Attention from Local-to-Global Pixels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07717">http://arxiv.org/abs/2308.07717</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hanktseng131415go/ramem">https://github.com/hanktseng131415go/ramem</a></li>
<li>paper_authors: Ching-Hsun Tseng, Shao-Ju Chien, Po-Shen Wang, Shin-Jye Lee, Wei-Huan Hu, Bin Pu, Xiao-jun Zeng</li>
<li>For: 这个论文的目的是提出一种实时自动echocardiography测量方法，以解决现有的三个主要阻碍：无法建立一个自动化方案、手动标注M-mode echocardiogram是时间consuming、现有的卷积层（如ResNet）在处理大对象时效率低下。* Methods: 该论文使用了MEIS数据集（M-mode echocardiogram的实例分割数据集），提出了面掌注意力（local-to-global efficient attention）和更新后的UPANets V2，以实现大对象检测和全局接受场。* Results: 实验结果表明，RAMEM比现有的RIS脊梁（带有非本地注意力）在PASCAL 2012 SBD和人类性能测试中表现更好，并且可以在实时中进行自动化echocardiography测量。<details>
<summary>Abstract</summary>
Motion mode (M-mode) recording is an essential part of echocardiography to measure cardiac dimension and function. However, the current diagnosis cannot build an automatic scheme, as there are three fundamental obstructs: Firstly, there is no open dataset available to build the automation for ensuring constant results and bridging M-mode echocardiography with real-time instance segmentation (RIS); Secondly, the examination is involving the time-consuming manual labelling upon M-mode echocardiograms; Thirdly, as objects in echocardiograms occupy a significant portion of pixels, the limited receptive field in existing backbones (e.g., ResNet) composed from multiple convolution layers are inefficient to cover the period of a valve movement. Existing non-local attentions (NL) compromise being unable real-time with a high computation overhead or losing information from a simplified version of the non-local block. Therefore, we proposed RAMEM, a real-time automatic M-mode echocardiography measurement scheme, contributes three aspects to answer the problems: 1) provide MEIS, a dataset of M-mode echocardiograms for instance segmentation, to enable consistent results and support the development of an automatic scheme; 2) propose panel attention, local-to-global efficient attention by pixel-unshuffling, embedding with updated UPANets V2 in a RIS scheme toward big object detection with global receptive field; 3) develop and implement AMEM, an efficient algorithm of automatic M-mode echocardiography measurement enabling fast and accurate automatic labelling among diagnosis. The experimental results show that RAMEM surpasses existing RIS backbones (with non-local attention) in PASCAL 2012 SBD and human performances in real-time MEIS tested. The code of MEIS and dataset are available at https://github.com/hanktseng131415go/RAME.
</details>
<details>
<summary>摘要</summary>
幻象模式（M-mode）记录是听觉心动图像测量的重要组成部分，但现有的诊断方案无法建立自动化机制，因为存在以下三个基本障碍：首先，没有开放的数据集可用于建立自动化，以确保定制化结果并将M-mode听觉心动图像与实时实例 segmentation（RIS）相连接；其次，检查需要手动标注M-mode听觉心动图像，时间consuming；第三，因为听觉心动图像中的对象占用了大量像素，现有的卷积层（例如ResNet）的有限感知范围不能覆盖心动期间oval movement。现有的非本地注意力（NL）不能实现实时，或者 computation overhead过高，或者 lost information from a simplified version of the non-local block。因此，我们提出了RAMEM，一种实时自动M-mode听觉心动图像测量方案，它在以下三个方面做出贡献：1. 提供MEIS数据集，用于实例 segmentation，以确保定制化结果和支持自动化方案的发展。2. 提出面attenion，具有local-to-global高效注意力，通过像素排序和更新UPANets V2在RIS方案中，以实现大对象检测与全局感知范围。3. 开发和实现AMEM算法，一种高效的自动M-mode听觉心动图像测量算法，可以快速和准确地自动标注诊断过程中。实验结果表明，RAMEM在PASCAL 2012 SBD和人类性能上都超过了现有的RIS卷积层（带有非本地注意力）。MEIS数据集和代码可以在https://github.com/hanktseng131415go/RAME中获取。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Network-Initialization-for-Medical-AI-Models-Using-Large-Scale-Unlabeled-Natural-Images"><a href="#Enhancing-Network-Initialization-for-Medical-AI-Models-Using-Large-Scale-Unlabeled-Natural-Images" class="headerlink" title="Enhancing Network Initialization for Medical AI Models Using Large-Scale, Unlabeled Natural Images"></a>Enhancing Network Initialization for Medical AI Models Using Large-Scale, Unlabeled Natural Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07688">http://arxiv.org/abs/2308.07688</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soroosh Tayebi Arasteh, Leo Misera, Jakob Nikolas Kather, Daniel Truhn, Sven Nebelung</li>
<li>for: 这个研究旨在测试SSL在非医学影像领域进行预训练，以及与对非医学影像和医学影像进行预训练进行比较。</li>
<li>methods: 我们使用了视觉 трансформер，并将其预设的参数基于以下三种预训练方法：(i) SSL预训练自然影像（DINOv2），(ii) ImageNet dataset上的SL预训练，以及(iii) MIMIC-CXR dataset上的SL预训练。</li>
<li>results: 我们发现，使用这些预训练方法可以在800,000多帧颈部X-光像中诊断更多于20种不同的内部发现。SSL预训练在预训练自然影像时不仅超过ImageNet-based预训练（P&lt;0.001 for all datasets），甚至在某些情况下也超过了预训练MIMIC-CXR dataset。<details>
<summary>Abstract</summary>
Pre-training datasets, like ImageNet, have become the gold standard in medical image analysis. However, the emergence of self-supervised learning (SSL), which leverages unlabeled data to learn robust features, presents an opportunity to bypass the intensive labeling process. In this study, we explored if SSL for pre-training on non-medical images can be applied to chest radiographs and how it compares to supervised pre-training on non-medical images and on medical images. We utilized a vision transformer and initialized its weights based on (i) SSL pre-training on natural images (DINOv2), (ii) SL pre-training on natural images (ImageNet dataset), and (iii) SL pre-training on chest radiographs from the MIMIC-CXR database. We tested our approach on over 800,000 chest radiographs from six large global datasets, diagnosing more than 20 different imaging findings. Our SSL pre-training on curated images not only outperformed ImageNet-based pre-training (P<0.001 for all datasets) but, in certain cases, also exceeded SL on the MIMIC-CXR dataset. Our findings suggest that selecting the right pre-training strategy, especially with SSL, can be pivotal for improving artificial intelligence (AI)'s diagnostic accuracy in medical imaging. By demonstrating the promise of SSL in chest radiograph analysis, we underline a transformative shift towards more efficient and accurate AI models in medical imaging.
</details>
<details>
<summary>摘要</summary>
医疗图像分析领域的预训练数据集，如ImageNet，已成为黄金标准。然而，自动学习（SSL）技术，利用无标签数据来学习强健特征，现在提供了一种可能的代替方案。在本研究中，我们研究了将SSL预训练在非医学图像上应用于胸部X射线图像，以及与超参数预训练在非医学图像和医学图像上的比较。我们使用了视觉 трансформа器，并将其参数初始化为（i）SSL预训练natural images（DINOv2），（ii）SL预训练natural images（ImageNet数据集），和（iii）SL预训练在MIMIC-CXR数据库上的胸部X射线图像。我们对6个大型全球数据集中的超过800,000个胸部X射线图像进行测试，并识别了20种不同的成像发现。我们的SSL预训练在精心选择的图像上不仅超过了ImageNet基础预训练（P<0.001 for all datasets），而且在某些情况下还超过了SL在MIMIC-CXR数据库上的预训练。我们的发现表明，选择合适的预训练策略，特别是使用SSL，可以对医疗图像识别精度进行重要改进。我们的研究证明了SSL在胸部X射线图像分析中的承诺，并标识了医疗图像识别领域的一种转型变革，从而实现更高效和准确的人工智能模型。
</details></li>
</ul>
<hr>
<h2 id="A-Review-of-Adversarial-Attacks-in-Computer-Vision"><a href="#A-Review-of-Adversarial-Attacks-in-Computer-Vision" class="headerlink" title="A Review of Adversarial Attacks in Computer Vision"></a>A Review of Adversarial Attacks in Computer Vision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07673">http://arxiv.org/abs/2308.07673</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yutong Zhang, Yao Li, Yin Li, Zhichang Guo</li>
<li>for: 本研究旨在解决深度神经网络受到敌意样本攻击的问题，尤其是在自动驾驶等安全关键场景中。</li>
<li>methods: 本研究使用了黑盒设定，即攻击者只能获得模型的输入和输出，而不知道模型的参数和梯度。</li>
<li>results: 研究发现，黑盒攻击可以 Transferability 到不同的深度学习和机器学习模型，并且可以 Achievability 在实际场景中。<details>
<summary>Abstract</summary>
Deep neural networks have been widely used in various downstream tasks, especially those safety-critical scenario such as autonomous driving, but deep networks are often threatened by adversarial samples. Such adversarial attacks can be invisible to human eyes, but can lead to DNN misclassification, and often exhibits transferability between deep learning and machine learning models and real-world achievability. Adversarial attacks can be divided into white-box attacks, for which the attacker knows the parameters and gradient of the model, and black-box attacks, for the latter, the attacker can only obtain the input and output of the model. In terms of the attacker's purpose, it can be divided into targeted attacks and non-targeted attacks, which means that the attacker wants the model to misclassify the original sample into the specified class, which is more practical, while the non-targeted attack just needs to make the model misclassify the sample. The black box setting is a scenario we will encounter in practice.
</details>
<details>
<summary>摘要</summary>
深度神经网络在各种下游任务中广泛应用，特别是安全关键的情况下，如自动驾驶等，但深度网络受到反对攻击的威胁。这些反对攻击可能会在人类眼中不可见，但可能导致神经网络误分类，并且常常具有神经网络和机器学习模型之间的传播性和实际应用性。反对攻击可以分为白盒攻击和黑盒攻击两类，其中白盒攻击者知道模型的参数和梯度，黑盒攻击者只能获得输入和输出。根据攻击者的目的，反对攻击可以分为targeted攻击和非targeted攻击。targeted攻击需要模型误分类原始样本为指定的类别，更加实际；非targeted攻击只需要模型误分类样本。黑盒设定是我们在实践中会遇到的情况。
</details></li>
</ul>
<hr>
<h2 id="Inversion-by-Inversion-Exemplar-based-Sketch-to-Photo-Synthesis-via-Stochastic-Differential-Equations-without-Training"><a href="#Inversion-by-Inversion-Exemplar-based-Sketch-to-Photo-Synthesis-via-Stochastic-Differential-Equations-without-Training" class="headerlink" title="Inversion-by-Inversion: Exemplar-based Sketch-to-Photo Synthesis via Stochastic Differential Equations without Training"></a>Inversion-by-Inversion: Exemplar-based Sketch-to-Photo Synthesis via Stochastic Differential Equations without Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07665">http://arxiv.org/abs/2308.07665</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ximinng/inversion-by-inversion">https://github.com/ximinng/inversion-by-inversion</a></li>
<li>paper_authors: Ximing Xing, Chuang Wang, Haitao Zhou, Zhihao Hu, Chongxuan Li, Dong Xu, Qian Yu</li>
<li>for: 将简图转换为真实图像</li>
<li>methods: 使用“倒反”两Stage方法，包括形态倒反和全控倒反两个阶段，通过形态能函数和外观能函数来控制图像的形态和外观特征。</li>
<li>results: 实验结果表明，提议的“倒反”方法能够生成高质量的真实图像，并且可以根据不同的示例图来控制图像的颜色和 текстура特征。<details>
<summary>Abstract</summary>
Exemplar-based sketch-to-photo synthesis allows users to generate photo-realistic images based on sketches. Recently, diffusion-based methods have achieved impressive performance on image generation tasks, enabling highly-flexible control through text-driven generation or energy functions. However, generating photo-realistic images with color and texture from sketch images remains challenging for diffusion models. Sketches typically consist of only a few strokes, with most regions left blank, making it difficult for diffusion-based methods to produce photo-realistic images. In this work, we propose a two-stage method named ``Inversion-by-Inversion" for exemplar-based sketch-to-photo synthesis. This approach includes shape-enhancing inversion and full-control inversion. During the shape-enhancing inversion process, an uncolored photo is generated with the guidance of a shape-energy function. This step is essential to ensure control over the shape of the generated photo. In the full-control inversion process, we propose an appearance-energy function to control the color and texture of the final generated photo.Importantly, our Inversion-by-Inversion pipeline is training-free and can accept different types of exemplars for color and texture control. We conducted extensive experiments to evaluate our proposed method, and the results demonstrate its effectiveness.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language: zh-CN<<SYS>> exemplar-based sketch-to-photo synthesis 可以让用户生成基于绘图的 photo-realistic 图像。最近，Diffusion-based 方法在图像生成任务上 achieved 出色的表现，允许通过文本驱动生成或能量函数进行高度灵活的控制。然而，通过Diffusion 模型生成具有颜色和 texture 的 photo-realistic 图像仍然是一个挑战。绘图通常只有几个笔画，大多数区域都是质感，使得Diffusion 模型很难生成 photo-realistic 图像。在这项工作中，我们提出了一种 Two-stage 方法，名为“Inversion-by-Inversion”，用于 exemplar-based sketch-to-photo synthesis。这种方法包括 shape-enhancing inversion 和 full-control inversion。在 shape-enhancing inversion 过程中，通过一个 shape-energy 函数的引导，生成一个没有颜色的照片。这一步很重要，以确保对生成的照片的形状进行控制。在 full-control inversion 过程中，我们提出了一种 appearance-energy 函数，用于控制照片的颜色和 texture。重要的是，我们的 Inversion-by-Inversion 管道是无需训练的，可以接受不同类型的 exemplar 进行颜色和 texture 控制。我们进行了广泛的实验来评估我们的提议方法，结果显示其效果。
</details></li>
</ul>
<hr>
<h2 id="Gradient-Based-Post-Training-Quantization-Challenging-the-Status-Quo"><a href="#Gradient-Based-Post-Training-Quantization-Challenging-the-Status-Quo" class="headerlink" title="Gradient-Based Post-Training Quantization: Challenging the Status Quo"></a>Gradient-Based Post-Training Quantization: Challenging the Status Quo</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07662">http://arxiv.org/abs/2308.07662</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edouard Yvinec, Arnaud Dapogny, Kevin Bailly</li>
<li>For: The paper focuses on gradient-based post-training quantization (GPTQ) methods for efficient deployment of deep neural networks.* Methods: The paper challenges common choices in GPTQ methods and derives best practices for designing more efficient and scalable GPTQ methods, including the problem formulation and optimization process.* Results: The paper proposes a novel importance-based mixed-precision technique and shows significant performance improvements on all tested state-of-the-art GPTQ methods and networks, achieving +6.819 points on ViT for 4-bit quantization.Here’s the simplified Chinese version:</li>
<li>for: 这篇论文关注的是在训练后进行权重调整的混合精度方法，以实现深度神经网络的高效部署。</li>
<li>methods: 论文挑战了常见的GPTQ方法选择，并提出了更有效和可扩展的GPTQ方法设计方法，包括问题定义和优化过程。</li>
<li>results: 论文提出了一种新的重要性基于混合精度技术，并在所有测试的当前GPTQ方法和网络上实现了显著的性能提升，例如在ViT网络上的4位量化得到了+6.819点的提升。<details>
<summary>Abstract</summary>
Quantization has become a crucial step for the efficient deployment of deep neural networks, where floating point operations are converted to simpler fixed point operations. In its most naive form, it simply consists in a combination of scaling and rounding transformations, leading to either a limited compression rate or a significant accuracy drop. Recently, Gradient-based post-training quantization (GPTQ) methods appears to be constitute a suitable trade-off between such simple methods and more powerful, yet expensive Quantization-Aware Training (QAT) approaches, particularly when attempting to quantize LLMs, where scalability of the quantization process is of paramount importance. GPTQ essentially consists in learning the rounding operation using a small calibration set. In this work, we challenge common choices in GPTQ methods. In particular, we show that the process is, to a certain extent, robust to a number of variables (weight selection, feature augmentation, choice of calibration set). More importantly, we derive a number of best practices for designing more efficient and scalable GPTQ methods, regarding the problem formulation (loss, degrees of freedom, use of non-uniform quantization schemes) or optimization process (choice of variable and optimizer). Lastly, we propose a novel importance-based mixed-precision technique. Those guidelines lead to significant performance improvements on all the tested state-of-the-art GPTQ methods and networks (e.g. +6.819 points on ViT for 4-bit quantization), paving the way for the design of scalable, yet effective quantization methods.
</details>
<details>
<summary>摘要</summary>
量化已成为深度神经网络的有效部署步骤，将浮点运算转换为简单的固定点运算。最简单的方式是通过缩放和四舍五入变换，但这将导致压缩率有限或准确率下降。现在，使用梯度based后期量化（GPTQ）方法可以实现一个适当的平衡，特别是在尝试量化LLMs（大型语言模型）时，因为量化过程的扩展性是非常重要。GPTQ通过学习缩放操作使用小量训练集来实现。在这个工作中，我们挑战了GPTQ方法的常见选择。具体来说，我们发现这个过程在一定程度上是Robust，即选择特征、增强特征和calibration集的变量的影响相对较小。此外，我们还提出了一些设计更高效和可扩展的GPTQ方法的最佳实践，包括问题定义（损失、自由度、非对称量化方案）和优化过程（变量和优化器）中的一些变量。最后，我们提出了一种新的重要性基于混合精度技术。这些指南导致所有测试的State-of-the-art GPTQ方法和网络（如+6.819点的ViT для4位量化）获得显著性能提高，为设计可扩展、有效的量化方法铺平道路。
</details></li>
</ul>
<hr>
<h2 id="Geometry-of-the-Visual-Cortex-with-Applications-to-Image-Inpainting-and-Enhancement"><a href="#Geometry-of-the-Visual-Cortex-with-Applications-to-Image-Inpainting-and-Enhancement" class="headerlink" title="Geometry of the Visual Cortex with Applications to Image Inpainting and Enhancement"></a>Geometry of the Visual Cortex with Applications to Image Inpainting and Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07652">http://arxiv.org/abs/2308.07652</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ballerin/v1diffusion">https://github.com/ballerin/v1diffusion</a></li>
<li>paper_authors: Francesco Ballerin, Erlend Grong</li>
<li>for: 这篇论文是为了提出基于视觉核心V1的扩展矩阵群$SE(2)$的图像填充和改善算法。</li>
<li>methods: 这篇论文使用了浸泡-推拿（WaxOn-WaxOff）方法，并利用了子 riemannian 结构来定义一种新的不锈滤波器，用于图像提高。</li>
<li>results: 研究人员通过应用这种方法于血管扩大扫描中的血管增强，得到了更加锐利的结果。<details>
<summary>Abstract</summary>
Equipping the rototranslation group $SE(2)$ with a sub-Riemannian structure inspired by the visual cortex V1, we propose algorithms for image inpainting and enhancement based on hypoelliptic diffusion. We innovate on previous implementations of the methods by Citti, Sarti and Boscain et al., by proposing an alternative that prevents fading and capable of producing sharper results in a procedure that we call WaxOn-WaxOff. We also exploit the sub-Riemannian structure to define a completely new unsharp using $SE(2)$, analogous of the classical unsharp filter for 2D image processing, with applications to image enhancement. We demonstrate our method on blood vessels enhancement in retinal scans.
</details>
<details>
<summary>摘要</summary>
将$SE(2)$拓扑群受到视觉核V1的启发下的半里曼尼拓扑结构，我们提出了基于液体扩散的图像填充和改善算法。我们在之前的实现方法（Citti、Sarti和Boscain等人的方法）的基础上做出了修改，以避免模糊和生成更加锐利的结果，我们称之为“WaxOn-WaxOff”过程。我们还利用了子拓扑结构来定义一种全新的不锐化器，类似于传统的2D图像处理中的不锐化过滤器，并应用于图像增强。我们在血管扩大retinal扫描中进行了示例。
</details></li>
</ul>
<hr>
<h2 id="Prompt-Switch-Efficient-CLIP-Adaptation-for-Text-Video-Retrieval"><a href="#Prompt-Switch-Efficient-CLIP-Adaptation-for-Text-Video-Retrieval" class="headerlink" title="Prompt Switch: Efficient CLIP Adaptation for Text-Video Retrieval"></a>Prompt Switch: Efficient CLIP Adaptation for Text-Video Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07648">http://arxiv.org/abs/2308.07648</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bladewaltz1/promptswitch">https://github.com/bladewaltz1/promptswitch</a></li>
<li>paper_authors: Chaorui Deng, Qi Chen, Pengda Qin, Da Chen, Qi Wu</li>
<li>for: 本文主要研究text-video retrieval领域中的问题，即如何使用预训练的文本-图像基础模型（如CLIP）在视频领域中进行有效的学习。</li>
<li>methods: 本文提出了一种新的方法，即在CLIP图像Encoder中引入空间-时间”Prompt Cube”，以快速包含全视频 semantics在帧表示中。此外，本文还提出了一种auxiliary video captioning目标函数，以帮助学习详细的视频 semantics。</li>
<li>results: 通过使用本文提出的方法，可以在三个标准 benchmark dataset上取得状态机器的性能（MSR-VTT、MSVD、LSMDC），而且只需要使用一个简单的时间融合策略（即mean-pooling）。<details>
<summary>Abstract</summary>
In text-video retrieval, recent works have benefited from the powerful learning capabilities of pre-trained text-image foundation models (e.g., CLIP) by adapting them to the video domain. A critical problem for them is how to effectively capture the rich semantics inside the video using the image encoder of CLIP. To tackle this, state-of-the-art methods adopt complex cross-modal modeling techniques to fuse the text information into video frame representations, which, however, incurs severe efficiency issues in large-scale retrieval systems as the video representations must be recomputed online for every text query. In this paper, we discard this problematic cross-modal fusion process and aim to learn semantically-enhanced representations purely from the video, so that the video representations can be computed offline and reused for different texts. Concretely, we first introduce a spatial-temporal "Prompt Cube" into the CLIP image encoder and iteratively switch it within the encoder layers to efficiently incorporate the global video semantics into frame representations. We then propose to apply an auxiliary video captioning objective to train the frame representations, which facilitates the learning of detailed video semantics by providing fine-grained guidance in the semantic space. With a naive temporal fusion strategy (i.e., mean-pooling) on the enhanced frame representations, we obtain state-of-the-art performances on three benchmark datasets, i.e., MSR-VTT, MSVD, and LSMDC.
</details>
<details>
<summary>摘要</summary>
在文本视频检索中， latest works 受益于预训练的文本图像基础模型（如 CLIP）的强大学习能力，通过适应它们到视频频谱中来进行改进。然而，一个 kritical problem 是如何有效地在图像Encoder中捕捉视频中的丰富 semantics。 To tackle this, state-of-the-art methods 采用复杂的跨模态模型化技术来融合文本信息到视频帧表示中，这 however, incurs severe efficiency issues in large-scale retrieval systems as the video representations must be recomputed online for every text query. In this paper, we discard this problematic cross-modal fusion process and aim to learn semantically-enhanced representations purely from the video, so that the video representations can be computed offline and reused for different texts.Concretely, we first introduce a spatial-temporal "Prompt Cube" into the CLIP image encoder and iteratively switch it within the encoder layers to efficiently incorporate the global video semantics into frame representations. We then propose to apply an auxiliary video captioning objective to train the frame representations, which facilitates the learning of detailed video semantics by providing fine-grained guidance in the semantic space. With a naive temporal fusion strategy (i.e., mean-pooling) on the enhanced frame representations, we obtain state-of-the-art performances on three benchmark datasets, i.e., MSR-VTT, MSVD, and LSMDC.
</details></li>
</ul>
<hr>
<h2 id="Backpropagation-Path-Search-On-Adversarial-Transferability"><a href="#Backpropagation-Path-Search-On-Adversarial-Transferability" class="headerlink" title="Backpropagation Path Search On Adversarial Transferability"></a>Backpropagation Path Search On Adversarial Transferability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07625">http://arxiv.org/abs/2308.07625</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuoer Xu, Zhangxuan Gu, Jianping Zhang, Shiwen Cui, Changhua Meng, Weiqiang Wang</li>
<li>for: 防御深度神经网络受到攻击的隐晦攻击，需要在部署之前测试模型的可靠性。</li>
<li>methods: 使用Transfer-based攻击者制作攻击示例，并将其传递给黑盒中部署的受害者模型。为提高攻击性能，结构基本攻击者调整反propagation路径，但现有的结构基本攻击者未能探索 convolution 模块在 CNN 中的作用，并且修改反propagation 图表时使用了优化的方法。</li>
<li>results: 在各种传输设置下，我们的 backPropagation pAth Search (PAS) 可以大幅提高攻击成功率，包括正常训练和防御模型。<details>
<summary>Abstract</summary>
Deep neural networks are vulnerable to adversarial examples, dictating the imperativeness to test the model's robustness before deployment. Transfer-based attackers craft adversarial examples against surrogate models and transfer them to victim models deployed in the black-box situation. To enhance the adversarial transferability, structure-based attackers adjust the backpropagation path to avoid the attack from overfitting the surrogate model. However, existing structure-based attackers fail to explore the convolution module in CNNs and modify the backpropagation graph heuristically, leading to limited effectiveness. In this paper, we propose backPropagation pAth Search (PAS), solving the aforementioned two problems. We first propose SkipConv to adjust the backpropagation path of convolution by structural reparameterization. To overcome the drawback of heuristically designed backpropagation paths, we further construct a DAG-based search space, utilize one-step approximation for path evaluation and employ Bayesian Optimization to search for the optimal path. We conduct comprehensive experiments in a wide range of transfer settings, showing that PAS improves the attack success rate by a huge margin for both normally trained and defense models.
</details>
<details>
<summary>摘要</summary>
深度神经网络容易受到反例攻击，需要在部署之前测试模型的可靠性。转移基于攻击者通过对代理模型创建反例，并将其传递到黑盒环境中部署的受害者模型。为增强反例传递性，结构基于攻击者可以修改反例传递的背景干扰路径，以避免攻击过拟合代理模型。然而，现有的结构基于攻击者未能探索 convolution 模块在 CNN 中，并修改背景干扰路径的方法，导致有限的效果。在这篇论文中，我们提出了 backPropagation pAth Search (PAS)，解决以下两个问题。我们首先提出 SkipConv，用于调整 convolution 模块的背景干扰路径。为了超越轮循的设计方法，我们进一步建立了 DAG 型搜索空间，利用一步逼近方法来评估路径，并使用 Bayesian 优化来搜索最佳路径。我们在各种转移设置下进行了广泛的实验，结果显示，PAS 可以在各种转移设置下提高攻击成功率，并且在防御模型上也有显著改善。
</details></li>
</ul>
<hr>
<h2 id="Self-Prompting-Large-Vision-Models-for-Few-Shot-Medical-Image-Segmentation"><a href="#Self-Prompting-Large-Vision-Models-for-Few-Shot-Medical-Image-Segmentation" class="headerlink" title="Self-Prompting Large Vision Models for Few-Shot Medical Image Segmentation"></a>Self-Prompting Large Vision Models for Few-Shot Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07624">http://arxiv.org/abs/2308.07624</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/peteryyzhang/few-shot-self-prompt-sam">https://github.com/peteryyzhang/few-shot-self-prompt-sam</a></li>
<li>paper_authors: Qi Wu, Yuyao Zhang, Marawan Elbatel</li>
<li>for: 这篇论文主要应用于医疗领域的大基础模型（Segment Anything Model，SAM），以提高医疗影像分类的性能。</li>
<li>methods: 这篇论文提出了一种新的自我推问法，利用SAM的嵌入空间来推问自己，通过简单 yet有效的直线像素层级分类器。</li>
<li>results: 这篇论文在多个数据集上（比如几几个医疗影像分类 зада问）取得了竞争性的结果，较以少数影像进行微调的方法提高约15%。<details>
<summary>Abstract</summary>
Recent advancements in large foundation models have shown promising potential in the medical industry due to their flexible prompting capability. One such model, the Segment Anything Model (SAM), a prompt-driven segmentation model, has shown remarkable performance improvements, surpassing state-of-the-art approaches in medical image segmentation. However, existing methods primarily rely on tuning strategies that require extensive data or prior prompts tailored to the specific task, making it particularly challenging when only a limited number of data samples are available. In this paper, we propose a novel perspective on self-prompting in medical vision applications. Specifically, we harness the embedding space of SAM to prompt itself through a simple yet effective linear pixel-wise classifier. By preserving the encoding capabilities of the large model, the contextual information from its decoder, and leveraging its interactive promptability, we achieve competitive results on multiple datasets (i.e. improvement of more than 15% compared to fine-tuning the mask decoder using a few images).
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:最近的大基础模型在医疗领域的应用显示了扎实的投资潜力，特别是其 flexible 的提示能力。一种名为 Segment Anything Model（SAM）的提示驱动 segmentation 模型，在医疗图像 segmentation 方面显示了非凡的表现，超越了当前的状态艺术方法。然而，现有的方法主要依赖于调整策略，需要大量的数据或特定任务的先前提示，这使得只有有限数量的数据样本时 особенelly 挑战。在这篇论文中，我们提出了一种新的自我提示视角，具体来说是利用 SAM 的 embedding 空间来自我提示，通过一种简单 yet effective 的线性像素级分类器。通过保留大型模型的编码能力，保留解码器的上下文信息，以及利用其交互提示能力，我们在多个 dataset 上达到了竞争力的结果（比如 Fine-tuning mask decoder 使用几个图像时的提高 более 15%）。
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-Hypergraphs-for-Learning-Multiple-World-Interpretations"><a href="#Self-supervised-Hypergraphs-for-Learning-Multiple-World-Interpretations" class="headerlink" title="Self-supervised Hypergraphs for Learning Multiple World Interpretations"></a>Self-supervised Hypergraphs for Learning Multiple World Interpretations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07615">http://arxiv.org/abs/2308.07615</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alina Marcu, Mihai Pirvu, Dragos Costea, Emanuela Haller, Emil Slusanschi, Ahmed Nabil Belbachir, Rahul Sukthankar, Marius Leordeanu</li>
<li>for: 学习多个场景表示，使用小量标注集。</li>
<li>methods: 利用场景表示之间的关系，建立多任务超гра�。使用超гра�提高VisTransformer模型，无需额外标注数据。</li>
<li>results: 比其他多任务图模型表现出色，在不同类型的超гра�和ensemble模型下进行自我超vision学习。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
We present a method for learning multiple scene representations given a small labeled set, by exploiting the relationships between such representations in the form of a multi-task hypergraph. We also show how we can use the hypergraph to improve a powerful pretrained VisTransformer model without any additional labeled data. In our hypergraph, each node is an interpretation layer (e.g., depth or segmentation) of the scene. Within each hyperedge, one or several input nodes predict the layer at the output node. Thus, each node could be an input node in some hyperedges and an output node in others. In this way, multiple paths can reach the same node, to form ensembles from which we obtain robust pseudolabels, which allow self-supervised learning in the hypergraph. We test different ensemble models and different types of hyperedges and show superior performance to other multi-task graph models in the field. We also introduce Dronescapes, a large video dataset captured with UAVs in different complex real-world scenes, with multiple representations, suitable for multi-task learning.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法，通过利用场景表示的关系形式为多任务 гиперграフ来学习多个场景表示。我们还示出了如何使用 гиперграフ来提高一种强大预训练 VisTransformer 模型，无需任何额外的标注数据。在我们的 гиперграフ中，每个节点是一个解释层（例如深度或分割）的场景表示。在每个 гипер边上，一个或多个输入节点预测输出节点的层。因此，每个节点可以是输入节点在某些 гипер边上，并且是输出节点在其他 гипер边上。这样，多个路径可以达到同一个节点，从而形成ensemble，并使用这些ensemble来获得Robustpseudolabel，以实现自动标注学习在 гиперграフ中。我们测试了不同的ensemble模型和不同类型的 гипер边，并显示了与其他多任务图模型在领域中的超越性。我们还介绍了 Dronescapes，一个大量视频数据集，captured with UAVs在不同的复杂实际场景中，具有多种表示，适合多任务学习。
</details></li>
</ul>
<hr>
<h2 id="GAMER-MRIL-identifies-Disability-Related-Brain-Changes-in-Multiple-Sclerosis"><a href="#GAMER-MRIL-identifies-Disability-Related-Brain-Changes-in-Multiple-Sclerosis" class="headerlink" title="GAMER-MRIL identifies Disability-Related Brain Changes in Multiple Sclerosis"></a>GAMER-MRIL identifies Disability-Related Brain Changes in Multiple Sclerosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07611">http://arxiv.org/abs/2308.07611</a></li>
<li>repo_url: None</li>
<li>paper_authors: Po-Jui Lu, Benjamin Odry, Muhamed Barakovic, Matthias Weigel, Robin Sandkühler, Reza Rahmanzadeh, Xinjie Chen, Mario Ocampo-Pineda, Jens Kuhle, Ludwig Kappos, Philippe Cattin, Cristina Granziera</li>
<li>For: The paper aims to identify disability-related brain changes in multiple sclerosis (MS) patients using whole-brain quantitative MRI (qMRI) and a novel comprehensive approach called GAMER-MRIL.* Methods: The approach uses a gated-attention-based convolutional neural network (CNN) to select patch-based qMRI images that are important for a given task&#x2F;question, and incorporates a structure-aware interpretability method called Layer-wise Relevance Propagation (LRP) to identify disability-related brain regions.* Results: The approach achieved an AUC of 0.885, and the most sensitive measures related to disability were qT1 and NDI. The proposed LRP approach obtained more specifically relevant regions than other interpretability methods, including the saliency map, the integrated gradients, and the original LRP. The relevant regions included the corticospinal tract, where average qT1 and NDI significantly correlated with patients’ disability scores.Here’s the Chinese version of the three key points:* 用途：本研究旨在通过整体approach GAMER-MRIL，利用整个脑quantitative MRI (qMRI) 数据，为多发性静脉炎 (MS) 患者识别缺乏功能相关的脑区域。* 方法：该方法使用 gated-attention-based convolutional neural network (CNN) 选择 qMRI 图像中重要的 patch，并 incorporates 结构意识的 interpretability method Layer-wise Relevance Propagation (LRP) 来发现缺乏功能相关的脑区域。* 结果：该方法实现了 AUC 0.885，qT1 和 NDI 是缺乏功能相关的最敏感度量。提议的 LRP 方法在其他 interpretability methods 中获得了更加特定的相关区域，包括 corticospinal tract，其中 qT1 和 NDI 与患者缺乏功能分数相关性 ($ \rho $ &#x3D; -0.37 和 0.44)。<details>
<summary>Abstract</summary>
Objective: Identifying disability-related brain changes is important for multiple sclerosis (MS) patients. Currently, there is no clear understanding about which pathological features drive disability in single MS patients. In this work, we propose a novel comprehensive approach, GAMER-MRIL, leveraging whole-brain quantitative MRI (qMRI), convolutional neural network (CNN), and an interpretability method from classifying MS patients with severe disability to investigating relevant pathological brain changes. Methods: One-hundred-sixty-six MS patients underwent 3T MRI acquisitions. qMRI informative of microstructural brain properties was reconstructed, including quantitative T1 (qT1), myelin water fraction (MWF), and neurite density index (NDI). To fully utilize the qMRI, GAMER-MRIL extended a gated-attention-based CNN (GAMER-MRI), which was developed to select patch-based qMRI important for a given task/question, to the whole-brain image. To find out disability-related brain regions, GAMER-MRIL modified a structure-aware interpretability method, Layer-wise Relevance Propagation (LRP), to incorporate qMRI. Results: The test performance was AUC=0.885. qT1 was the most sensitive measure related to disability, followed by NDI. The proposed LRP approach obtained more specifically relevant regions than other interpretability methods, including the saliency map, the integrated gradients, and the original LRP. The relevant regions included the corticospinal tract, where average qT1 and NDI significantly correlated with patients' disability scores ($\rho$=-0.37 and 0.44). Conclusion: These results demonstrated that GAMER-MRIL can classify patients with severe disability using qMRI and subsequently identify brain regions potentially important to the integrity of the mobile function. Significance: GAMER-MRIL holds promise for developing biomarkers and increasing clinicians' trust in NN.
</details>
<details>
<summary>摘要</summary>
目标：identifying multiple sclerosis (MS) 患者中 relate to disability 的 brain changes是非常重要的。目前，没有明确的认知关于单个 MS 患者中哪些病理特征驱动残疾。在这种工作中，我们提出了一种全新的 comprehensive 方法，GAMER-MRIL，通过整个大脑量化MRI (qMRI)、卷积神经网络 (CNN) 和可解释方法来从MS患者中分类患者严重残疾。方法：一百六十六名 MS 患者通过3T MRI成像。qMRI 中提供了微结构脑 Properties 的信息，包括量化T1 (qT1)、myelin water fraction (MWF) 和 neurite density index (NDI)。为了完全利用 qMRI，GAMER-MRIL 扩展了一种闭合注意力基于CNN (GAMER-MRI)，将其应用到整个大脑图像。为了找出残疾相关的脑区，GAMER-MRIL 修改了结构意识 interpretability 方法，卷积层感知 propagation (LRP)，以包含 qMRI。结果：测试性能为 AUC=0.885。qT1 是残疾相关度最高的度量，其次是 NDI。提出的 LRP 方法在特定的脑区中获得了更多的相关区域，比其他可解释方法更加具有特点。这些相关区域包括 corticospinal tract，其中 qT1 和 NDI 与患者残疾分数相关性 (-0.37 和 0.44)。结论：这些结果表明GAMER-MRIL 可以使用 qMRI 分类患者严重残疾，并在脑区中寻找可能与 mobil 功能完整性相关的区域。意义：GAMER-MRIL 具有发展生物标志物和提高临床医生对NN的信任的潜在价值。
</details></li>
</ul>
<hr>
<h2 id="AKVSR-Audio-Knowledge-Empowered-Visual-Speech-Recognition-by-Compressing-Audio-Knowledge-of-a-Pretrained-Model"><a href="#AKVSR-Audio-Knowledge-Empowered-Visual-Speech-Recognition-by-Compressing-Audio-Knowledge-of-a-Pretrained-Model" class="headerlink" title="AKVSR: Audio Knowledge Empowered Visual Speech Recognition by Compressing Audio Knowledge of a Pretrained Model"></a>AKVSR: Audio Knowledge Empowered Visual Speech Recognition by Compressing Audio Knowledge of a Pretrained Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07593">http://arxiv.org/abs/2308.07593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeong Hun Yeo, Minsu Kim, Jeongsoo Choi, Dae Hoe Kim, Yong Man Ro</li>
<li>for: 这篇论文主要应用在无音识别中，将声音资讯与视觉资讯结合，以提高无音识别的精度。</li>
<li>methods: 提案的 Audio Knowledge empowered Visual Speech Recognition 框架（AKVSR）使用大规模预训Audio模型对声音资讯进行了丰富的编码，并将非语言信息从声音资料中排除，将语言信息储存在高精度的Audio内存中，最后通过Audio Bridging Module与视觉资讯进行匹配，以实现无需声音输入的训练。</li>
<li>results: 这篇论文透过广泛的实验证明了提案的方法的有效性，在两个广泛使用的数据集LRS2和LRS3上实现了新的顶峰性能。<details>
<summary>Abstract</summary>
Visual Speech Recognition (VSR) is the task of predicting spoken words from silent lip movements. VSR is regarded as a challenging task because of the insufficient information on lip movements. In this paper, we propose an Audio Knowledge empowered Visual Speech Recognition framework (AKVSR) to complement the insufficient speech information of visual modality by using audio modality. Different from the previous methods, the proposed AKVSR 1) utilizes rich audio knowledge encoded by a large-scale pretrained audio model, 2) saves the linguistic information of audio knowledge in compact audio memory by discarding the non-linguistic information from the audio through quantization, and 3) includes Audio Bridging Module which can find the best-matched audio features from the compact audio memory, which makes our training possible without audio inputs, once after the compact audio memory is composed. We validate the effectiveness of the proposed method through extensive experiments, and achieve new state-of-the-art performances on the widely-used datasets, LRS2 and LRS3.
</details>
<details>
<summary>摘要</summary>
visual speech recognition (VSR) 是指从舌头运动中预测说话的任务。 VSR 被视为一个具有挑战性的任务，因为舌头运动的信息不够。 在这篇论文中，我们提议了一个听音知识强化的视频语音识别框架（AKVSR），用于补充视觉模式中的不够的语音信息。 与前一些方法不同，我们的 AKVSR 具有以下特点：1. 利用大规模预训练的音频模型编码的丰富听音知识。2. 通过归约非语言信息，将音频信息储存在高效的音频内存中，以便在训练时不需要音频输入。3. 包括听音桥接模块，可以在训练时找到最佳匹配的音频特征，从而实现无需音频输入的训练。我们通过广泛的实验 validate 了我们的提议，并在常用的 datasets 上达到了新的state-of-the-art 性能。
</details></li>
</ul>
<hr>
<h2 id="Graph-Segmenter-Graph-Transformer-with-Boundary-aware-Attention-for-Semantic-Segmentation"><a href="#Graph-Segmenter-Graph-Transformer-with-Boundary-aware-Attention-for-Semantic-Segmentation" class="headerlink" title="Graph-Segmenter: Graph Transformer with Boundary-aware Attention for Semantic Segmentation"></a>Graph-Segmenter: Graph Transformer with Boundary-aware Attention for Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07592">http://arxiv.org/abs/2308.07592</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zizhang Wu, Yuanzhu Gan, Tianhao Xu, Fan Wang</li>
<li>for: 提高 semantic segmentation 的性能</li>
<li>methods: 使用 Graph Transformer 和 Boundary-aware Attention 模块</li>
<li>results: 在三个 widely used semantic segmentation  dataset 上达到 state-of-the-art 性能<details>
<summary>Abstract</summary>
The transformer-based semantic segmentation approaches, which divide the image into different regions by sliding windows and model the relation inside each window, have achieved outstanding success. However, since the relation modeling between windows was not the primary emphasis of previous work, it was not fully utilized. To address this issue, we propose a Graph-Segmenter, including a Graph Transformer and a Boundary-aware Attention module, which is an effective network for simultaneously modeling the more profound relation between windows in a global view and various pixels inside each window as a local one, and for substantial low-cost boundary adjustment. Specifically, we treat every window and pixel inside the window as nodes to construct graphs for both views and devise the Graph Transformer. The introduced boundary-aware attention module optimizes the edge information of the target objects by modeling the relationship between the pixel on the object's edge. Extensive experiments on three widely used semantic segmentation datasets (Cityscapes, ADE-20k and PASCAL Context) demonstrate that our proposed network, a Graph Transformer with Boundary-aware Attention, can achieve state-of-the-art segmentation performance.
</details>
<details>
<summary>摘要</summary>
“ transformer-based semantic segmentation 方法，通过将图像分成不同区域，使用滑块窗口来建立关系，已经取得了出色的成果。然而，由于在这些方法中模型窗口之间的关系不是主要的强调点，因此未能充分利用。为了解决这个问题，我们提出了一个名为 Graph-Segmenter 的网络，包括 Graph Transformer 和Boundary-aware Attention 模组。这个网络可以同时在全球视图中模型窗口之间的深层关系，以及每个窗口和内部每个像素之间的本地关系，并且实现了低成本的边界调整。具体来说，我们将每个窗口和内部每个像素视为节点，以建立这两个视图的图形。我们还引入了边界意识注意力模组，以便优化目标物边界上的像素关系。我们在 Cityscapes、ADE-20k 和 PASCAL Context 三个通用 semantic segmentation 数据集上进行了广泛的实验，结果显示，我们的提案的网络，Graph Transformer with Boundary-aware Attention，可以 дости得 estado-of-the-art 的 segmentation 性能。”
</details></li>
</ul>
<hr>
<h2 id="ADD-An-Automatic-Desensitization-Fisheye-Dataset-for-Autonomous-Driving"><a href="#ADD-An-Automatic-Desensitization-Fisheye-Dataset-for-Autonomous-Driving" class="headerlink" title="ADD: An Automatic Desensitization Fisheye Dataset for Autonomous Driving"></a>ADD: An Automatic Desensitization Fisheye Dataset for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07590">http://arxiv.org/abs/2308.07590</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zizhang Wu, Chenxin Yuan, Hongyang Wei, Fan Song, Tianhao Xu</li>
<li>for: 提供一个大 FoV 鱼眼相机拍摄的自动驾驶环境中数据保护的解决方案，以满足法规要求。</li>
<li>methods: 基于大 FoV 鱼眼相机的自动驾驶拍摄数据，构建了首个 Autopilot Desensitization Dataset (ADD)，并提出了一种深度学习基于图像感知的图像隐藏框架。</li>
<li>results: 在 ADD 数据集上，提出了一种高效的多任务感知网络（DesCenterNet），可以同时实现人脸和车牌检测和隐藏任务。对于图像隐藏任务，我们提出了一种新的评价标准，并进行了广泛的比较实验，证明了我们的方法的有效性和超越性。<details>
<summary>Abstract</summary>
Autonomous driving systems require many images for analyzing the surrounding environment. However, there is fewer data protection for private information among these captured images, such as pedestrian faces or vehicle license plates, which has become a significant issue. In this paper, in response to the call for data security laws and regulations and based on the advantages of large Field of View(FoV) of the fisheye camera, we build the first Autopilot Desensitization Dataset, called ADD, and formulate the first deep-learning-based image desensitization framework, to promote the study of image desensitization in autonomous driving scenarios. The compiled dataset consists of 650K images, including different face and vehicle license plate information captured by the surround-view fisheye camera. It covers various autonomous driving scenarios, including diverse facial characteristics and license plate colors. Then, we propose an efficient multitask desensitization network called DesCenterNet as a benchmark on the ADD dataset, which can perform face and vehicle license plate detection and desensitization tasks. Based on ADD, we further provide an evaluation criterion for desensitization performance, and extensive comparison experiments have verified the effectiveness and superiority of our method on image desensitization.
</details>
<details>
<summary>摘要</summary>
自动驾驶系统需要大量图像来分析周围环境。然而， captured 图像中的private信息，如行人脸或车辆识别号，却受到较少的数据保护，这成为了一个重要的问题。在这篇论文中，我们根据宽视场(FoV)大的鱼眼镜头的优点，建立了首个Autopilot Desensitization Dataset（ADD），并提出了首个深度学习基于图像抑制框架。通过ADD集成了650000张图像，包括不同的脸和车辆识别号信息， captured by surround-view fisheye camera。它覆盖了各种自动驾驶场景，包括多样化的脸容特征和车辆识别号颜色。然后，我们提出了一种高效的多任务抑制网络， called DesCenterNet，作为ADD集成的benchmark，可以同时完成脸和车辆识别号检测和抑制任务。基于ADD，我们还提供了图像抑制性评价标准，并进行了广泛的比较实验，证明了我们的方法在图像抑制方面的效果和优势。
</details></li>
</ul>
<hr>
<h2 id="Synthetic-data-generation-method-for-hybrid-image-tabular-data-using-two-generative-adversarial-networks"><a href="#Synthetic-data-generation-method-for-hybrid-image-tabular-data-using-two-generative-adversarial-networks" class="headerlink" title="Synthetic data generation method for hybrid image-tabular data using two generative adversarial networks"></a>Synthetic data generation method for hybrid image-tabular data using two generative adversarial networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07573">http://arxiv.org/abs/2308.07573</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tomohiro Kikuchi, Shouhei Hanaoka, Takahiro Nakao, Tomomi Takenaga, Yukihiro Nomura, Harushi Mori, Takeharu Yoshikawa</li>
<li>for: 这篇论文旨在提出一种生成医疗资料的方法，以便解决医疗领域中隐私问题和促进数据共享。</li>
<li>methods: 这篇论文使用了一种称为 auto-encoding GAN（αGAN）和一种称为 conditional tabular GAN（CTGAN）的生成opponent neural network（GAN）方法，以生成医疗领域中的合成医疗资料。</li>
<li>results: 这篇论文成功地实现了生成多样化的合成医疗资料，包括颈部X射像（CXR）和结构化的数据（包括人体尺寸数据和实验室测试数据），并且保持了这些数据之间的对应关系。<details>
<summary>Abstract</summary>
The generation of synthetic medical records using generative adversarial networks (GANs) has become increasingly important for addressing privacy concerns and promoting data sharing in the medical field. In this paper, we propose a novel method for generating synthetic hybrid medical records consisting of chest X-ray images (CXRs) and structured tabular data (including anthropometric data and laboratory tests) using an auto-encoding GAN ({\alpha}GAN) and a conditional tabular GAN (CTGAN). Our approach involves training a {\alpha}GAN model on a large public database (pDB) to reduce the dimensionality of CXRs. We then applied the trained encoder of the GAN model to the images in original database (oDB) to obtain the latent vectors. These latent vectors were combined with tabular data in oDB, and these joint data were used to train the CTGAN model. We successfully generated diverse synthetic records of hybrid CXR and tabular data, maintaining correspondence between them. We evaluated this synthetic database (sDB) through visual assessment, distribution of interrecord distances, and classification tasks. Our evaluation results showed that the sDB captured the features of the oDB while maintaining the correspondence between the images and tabular data. Although our approach relies on the availability of a large-scale pDB containing a substantial number of images with the same modality and imaging region as those in the oDB, this method has the potential for the public release of synthetic datasets without compromising the secondary use of data.
</details>
<details>
<summary>摘要</summary>
现代生成技术在医疗领域中得到了广泛应用，尤其是通过生成对抗网络（GAN）来解决隐私问题和促进数据共享。本文提出了一种新的方法，使用自动编码GAN（αGAN）和条件表格GAN（CTGAN）生成混合类医疗记录，包括胸部X射线图像（CXR）和结构化表格数据（包括人体测量数据和实验室测试结果）。我们的方法是使用大规模公共数据库（pDB）来减少CXR的维度，然后使用训练过的GAN模型的编码器对oDB中的图像进行编码，得到了潜在 вектор。这些潜在 вектор与表格数据进行结合，并将这些联合数据用于CTGAN模型的训练。我们成功地生成了多样化的医疗记录，保持了图像和表格数据之间的协调。我们对这个synthetic数据库（sDB）进行了视觉评估、记录间距离分布和分类任务的评估。我们的评估结果表明，sDB捕捉了oDB中的特征，同时保持了图像和表格数据之间的协调。虽然我们的方法需要一个大规模的pDB，但这种方法具有公开 синтетиче数据库的潜在优势，不需要牺牲第二次使用数据的隐私。
</details></li>
</ul>
<hr>
<h2 id="Ske2Grid-Skeleton-to-Grid-Representation-Learning-for-Action-Recognition"><a href="#Ske2Grid-Skeleton-to-Grid-Representation-Learning-for-Action-Recognition" class="headerlink" title="Ske2Grid: Skeleton-to-Grid Representation Learning for Action Recognition"></a>Ske2Grid: Skeleton-to-Grid Representation Learning for Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07571">http://arxiv.org/abs/2308.07571</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/osvai/ske2grid">https://github.com/osvai/ske2grid</a></li>
<li>paper_authors: Dongqi Cai, Yangyuxuan Kang, Anbang Yao, Yurong Chen</li>
<li>for: 本文提出了一种新的 Representation Learning 框架，用于改进人体skeleton基于动作识别。</li>
<li>methods: 本文使用了三种新的设计方法：Graph-node index transform (GIT)、Up-sampling transform (UPT) 和 Progressive learning strategy (PLS)，用于构建一个具有更高表示能力的人体skeleton网格表示。</li>
<li>results:  experiments 表明，使用本文提出的Ske2Grid方法可以在六个主流的人体skeleton基于动作识别 dataset 上达到更高的性能，而不需要额外的设计。<details>
<summary>Abstract</summary>
This paper presents Ske2Grid, a new representation learning framework for improved skeleton-based action recognition. In Ske2Grid, we define a regular convolution operation upon a novel grid representation of human skeleton, which is a compact image-like grid patch constructed and learned through three novel designs. Specifically, we propose a graph-node index transform (GIT) to construct a regular grid patch through assigning the nodes in the skeleton graph one by one to the desired grid cells. To ensure that GIT is a bijection and enrich the expressiveness of the grid representation, an up-sampling transform (UPT) is learned to interpolate the skeleton graph nodes for filling the grid patch to the full. To resolve the problem when the one-step UPT is aggressive and further exploit the representation capability of the grid patch with increasing spatial size, a progressive learning strategy (PLS) is proposed which decouples the UPT into multiple steps and aligns them to multiple paired GITs through a compact cascaded design learned progressively. We construct networks upon prevailing graph convolution networks and conduct experiments on six mainstream skeleton-based action recognition datasets. Experiments show that our Ske2Grid significantly outperforms existing GCN-based solutions under different benchmark settings, without bells and whistles. Code and models are available at https://github.com/OSVAI/Ske2Grid
</details>
<details>
<summary>摘要</summary>
First, we propose a graph-node index transform (GIT) to assign nodes in the skeleton graph to desired grid cells. This ensures that GIT is a bijection and enriches the expressiveness of the grid representation.Second, we learn an up-sampling transform (UPT) to interpolate the skeleton graph nodes for filling the grid patch to the full. This ensures that the grid representation is dense and detailed.Third, we propose a progressive learning strategy (PLS) to decouple the UPT into multiple steps and align them with multiple paired GITs through a compact cascaded design learned progressively. This further exploits the representation capability of the grid patch with increasing spatial size.We construct networks upon prevailing graph convolution networks and conduct experiments on six mainstream skeleton-based action recognition datasets. The results show that our Ske2Grid significantly outperforms existing GCN-based solutions under different benchmark settings, without bells and whistles. The code and models are available at https://github.com/OSVAI/Ske2Grid.
</details></li>
</ul>
<hr>
<h2 id="Improved-mirror-ball-projection-for-more-accurate-merging-of-multiple-camera-outputs-and-process-monitoring"><a href="#Improved-mirror-ball-projection-for-more-accurate-merging-of-multiple-camera-outputs-and-process-monitoring" class="headerlink" title="Improved mirror ball projection for more accurate merging of multiple camera outputs and process monitoring"></a>Improved mirror ball projection for more accurate merging of multiple camera outputs and process monitoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10991">http://arxiv.org/abs/2308.10991</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/FrostKiwi/Mirrorball">https://github.com/FrostKiwi/Mirrorball</a></li>
<li>paper_authors: Wladislav Artsimovich, Yoko Hirono</li>
<li>for: 用圆镜代替宽角摄像机，实现低成本的生产过程监测在危险环境中，包括高温、真空和强电磁场环境。</li>
<li>methods: 使用圆镜反射将多种摄像机类型（如彩色图像、近红外、长波长红外、 ultraviolet）集成到单一宽角输出中，并考虑不同摄像机位置和镜头使用。</li>
<li>results: 研究表明，使用圆镜反射可以减少不同摄像机位置引入的视角偏移，具体取决于镜子大小和监测目标距离。此外，本文还介绍了一种受限于投影镜球的扭曲问题的变种，并评估了过程监测via圆镜球的效果。<details>
<summary>Abstract</summary>
Using spherical mirrors in place of wide-angle cameras allows for cost-effective monitoring of manufacturing processes in hazardous environment, where a camera would normally not operate. This includes environments of high heat, vacuum and strong electromagnetic fields. Moreover, it allows the layering of multiple camera types (e.g., color image, near-infrared, long-wavelength infrared, ultraviolet) into a single wide-angle output, whilst accounting for the different camera placements and lenses used. Normally, the different camera positions introduce a parallax shift between the images, but with a spherical projection as produced by a spherical mirror, this parallax shift is reduced, depending on mirror size and distance to the monitoring target.   This paper introduces a variation of the 'mirror ball projection', that accounts for distortion produced by a perspective camera at the pole of the projection. Finally, the efficacy of process monitoring via a mirror ball is evaluated.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is also known as "简化字" or "简化字".Translation Notes:* "wide-angle camera" is translated as "广角镜头" (guǎng jiàng jīng tóu), which is a more common term in Simplified Chinese.* "spherical mirror" is translated as "球形镜" (qiu xíng jìng), which is a more precise term in Simplified Chinese.* "parallax shift" is translated as "偏移" (piān yì), which is a more common term in Simplified Chinese.* "perspective camera" is translated as "投影镜头" (pù yǐng jīng tóu), which is a more precise term in Simplified Chinese.* "mirror ball projection" is translated as "镜球投影" (jìng qiu pù yǐng), which is a more common term in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="SST-A-Simplified-Swin-Transformer-based-Model-for-Taxi-Destination-Prediction-based-on-Existing-Trajectory"><a href="#SST-A-Simplified-Swin-Transformer-based-Model-for-Taxi-Destination-Prediction-based-on-Existing-Trajectory" class="headerlink" title="SST: A Simplified Swin Transformer-based Model for Taxi Destination Prediction based on Existing Trajectory"></a>SST: A Simplified Swin Transformer-based Model for Taxi Destination Prediction based on Existing Trajectory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07555">http://arxiv.org/abs/2308.07555</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zepu Wang, Yifei Sun, Zhiyu Lei, Xincheng Zhu, Peng Sun</li>
<li>for: 预测AXI trajectory的目的地有很多减值，可以帮助智能位置基础服务。</li>
<li>methods: 将AXI trajectory转换为二维网格，使用计算机视觉技术进行预测。</li>
<li>results: 我们的实验结果表明，使用简化的Swin Transformer（SST）结构可以在实际 trajectory数据上达到更高的准确率，比state-of-the-art方法更高。<details>
<summary>Abstract</summary>
Accurately predicting the destination of taxi trajectories can have various benefits for intelligent location-based services. One potential method to accomplish this prediction is by converting the taxi trajectory into a two-dimensional grid and using computer vision techniques. While the Swin Transformer is an innovative computer vision architecture with demonstrated success in vision downstream tasks, it is not commonly used to solve real-world trajectory problems. In this paper, we propose a simplified Swin Transformer (SST) structure that does not use the shifted window idea in the traditional Swin Transformer, as trajectory data is consecutive in nature. Our comprehensive experiments, based on real trajectory data, demonstrate that SST can achieve higher accuracy compared to state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本为简化中文。</SYS>>预测出AXI Taxi trajectory的目的地可以有各种 beneficial effects for intelligent location-based services. one potential method to achieve this prediction is by converting the taxi trajectory into a two-dimensional grid and using computer vision techniques. Although the Swin Transformer is an innovative computer vision architecture with demonstrated success in vision downstream tasks, it is not commonly used to solve real-world trajectory problems. In this paper, we propose a simplified Swin Transformer (SST) structure that does not use the shifted window idea in the traditional Swin Transformer, as trajectory data is consecutive in nature. Our comprehensive experiments, based on real trajectory data, demonstrate that SST can achieve higher accuracy compared to state-of-the-art methods.Here's the word-for-word translation of the text into Simplified Chinese:<<SYS>>将给定文本转换为简化中文。</SYS>>预测AXI taxi trajectory的目的地可以有各种有益的效果 для智能位置基于服务。一个 potential method to achieve this prediction is by converting the taxi trajectory into a two-dimensional grid and using computer vision techniques. Although the Swin Transformer is an innovative computer vision architecture with demonstrated success in vision downstream tasks, it is not commonly used to solve real-world trajectory problems. In this paper, we propose a simplified Swin Transformer (SST) structure that does not use the shifted window idea in the traditional Swin Transformer, as trajectory data is consecutive in nature. Our comprehensive experiments, based on real trajectory data, demonstrate that SST can achieve higher accuracy compared to state-of-the-art methods.
</details></li>
</ul>
<hr>
<h2 id="Multi-view-3D-Face-Reconstruction-Based-on-Flame"><a href="#Multi-view-3D-Face-Reconstruction-Based-on-Flame" class="headerlink" title="Multi-view 3D Face Reconstruction Based on Flame"></a>Multi-view 3D Face Reconstruction Based on Flame</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07551">http://arxiv.org/abs/2308.07551</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenzhuo Zheng, Junhao Zhao, Xiaohong Liu, Yongyang Pan, Zhenghao Gan, Haozhe Han, Ning Liu</li>
<li>for: 本研究旨在提高面部3D重建质量，通过结合多视图训练框架和面 Parametric模型Flame，提出多视图训练和测试模型MFNet。</li>
<li>methods: 我们建立了一个无监督训练框架，并实施了多视图光流损失函数和面点损失约束，最后获得了完整的MFNet。我们还提出了多视图光流损失和可见面罩的创新实现。</li>
<li>results: 我们在AFLW和facescape数据集上测试了我们的模型，并在实际场景中拍摄了我们的脸部图像，并实现了3D面部重建的好 Result。我们的工作主要解决了将面 Parametric模型与多视图face 3D重建结合的问题，并探讨了基于Flame的多视图训练和测试框架在面部3D重建领域的贡献。<details>
<summary>Abstract</summary>
At present, face 3D reconstruction has broad application prospects in various fields, but the research on it is still in the development stage. In this paper, we hope to achieve better face 3D reconstruction quality by combining multi-view training framework with face parametric model Flame, propose a multi-view training and testing model MFNet (Multi-view Flame Network). We build a self-supervised training framework and implement constraints such as multi-view optical flow loss function and face landmark loss, and finally obtain a complete MFNet. We propose innovative implementations of multi-view optical flow loss and the covisible mask. We test our model on AFLW and facescape datasets and also take pictures of our faces to reconstruct 3D faces while simulating actual scenarios as much as possible, which achieves good results. Our work mainly addresses the problem of combining parametric models of faces with multi-view face 3D reconstruction and explores the implementation of a Flame based multi-view training and testing framework for contributing to the field of face 3D reconstruction.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:当前，人脸3D重建具有广泛应用前景，但相关研究还处于发展阶段。在这篇论文中，我们希望通过结合多视图培训框架和人脸参数模型Flame，提出一种多视图培训和测试模型MFNet（多视图Flame网络）。我们建立了一个无监督培训框架，并实施约束 Multi-view optical flow loss function和面部标记损失等，最后获得了完整的MFNet。我们提出了面部 parametric 模型和多视图 face 3D 重建的innovative实现，包括多视图 optical flow 损失和可见面罩。我们在 AFLW 和 facescape 数据集上测试了我们的模型，并在实际场景中拍摄了我们的脸部图像，并实现了三维人脸重建。我们的工作主要解决了人脸参数模型与多视图 face 3D 重建的组合问题，并探讨了基于 Flame 的多视图培训和测试框架在人脸3D重建领域的应用。
</details></li>
</ul>
<hr>
<h2 id="3DHacker-Spectrum-based-Decision-Boundary-Generation-for-Hard-label-3D-Point-Cloud-Attack"><a href="#3DHacker-Spectrum-based-Decision-Boundary-Generation-for-Hard-label-3D-Point-Cloud-Attack" class="headerlink" title="3DHacker: Spectrum-based Decision Boundary Generation for Hard-label 3D Point Cloud Attack"></a>3DHacker: Spectrum-based Decision Boundary Generation for Hard-label 3D Point Cloud Attack</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07546">http://arxiv.org/abs/2308.07546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunbo Tao, Daizong Liu, Pan Zhou, Yulai Xie, Wei Du, Wei Hu</li>
<li>for: 攻击3D点云模型的安全性在自动驾驶和机器人导航等应用中 receiving increasing attention。</li>
<li>methods: 我们提出了一种新的3D攻击方法，称为3D Hard-label Attacker（3DHacker），基于分类标签知识生成敏感amples。</li>
<li>results: 我们的3DHacker方法在具有黑盒环境的情况下，可以凭借高效率和小型做出比较出色的攻击性能，并且对攻击者的质量也有较好的控制。<details>
<summary>Abstract</summary>
With the maturity of depth sensors, the vulnerability of 3D point cloud models has received increasing attention in various applications such as autonomous driving and robot navigation. Previous 3D adversarial attackers either follow the white-box setting to iteratively update the coordinate perturbations based on gradients, or utilize the output model logits to estimate noisy gradients in the black-box setting. However, these attack methods are hard to be deployed in real-world scenarios since realistic 3D applications will not share any model details to users. Therefore, we explore a more challenging yet practical 3D attack setting, \textit{i.e.}, attacking point clouds with black-box hard labels, in which the attacker can only have access to the prediction label of the input. To tackle this setting, we propose a novel 3D attack method, termed \textbf{3D} \textbf{H}ard-label att\textbf{acker} (\textbf{3DHacker}), based on the developed decision boundary algorithm to generate adversarial samples solely with the knowledge of class labels. Specifically, to construct the class-aware model decision boundary, 3DHacker first randomly fuses two point clouds of different classes in the spectral domain to craft their intermediate sample with high imperceptibility, then projects it onto the decision boundary via binary search. To restrict the final perturbation size, 3DHacker further introduces an iterative optimization strategy to move the intermediate sample along the decision boundary for generating adversarial point clouds with smallest trivial perturbations. Extensive evaluations show that, even in the challenging hard-label setting, 3DHacker still competitively outperforms existing 3D attacks regarding the attack performance as well as adversary quality.
</details>
<details>
<summary>摘要</summary>
随着深度感知器的成熟，3D点云模型的漏洞受到了各种应用程序中的关注，如自动驾驶和机器人导航。先前的3D反击器都是采用白盒设定来逐渐更新坐标偏移量基于梯度，或者使用输出模型的логи值来估计噪声梯度在黑盒设定下。然而，这些攻击方法在实际应用场景中很难实施，因为实际的3D应用程序不会分享任何模型细节给用户。因此，我们研究一种更加具有挑战性且实用的3D攻击设定，即在黑盒硬标记下攻击点云，在这个设定下，攻击者只有访问输入的预测标签。为解决这个设定，我们提出了一种新的3D攻击方法，即3D硬标记攻击者（3DHacker），基于已发展的决策边界算法来生成反击样本，只需通过知道类别标签来生成对抗样本。具体来说，为构建类别意识模型的决策边界，3DHacker首先随机将两个不同类型的点云在spectral domain中混合为中间样本，然后将其投射到决策边界 via binary search。为限制最终的偏移量，3DHacker进一步引入了一种迭代优化策略，将中间样本在决策边界上移动，以生成最小的极小偏移量。广泛的评估表明，即使在挑战性的硬标记设定下，3DHacker仍然可以与现有3D攻击相比，在攻击性和对手质量方面具有竞争力。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Dataset-Distillation-for-Image-Text-Retrieval"><a href="#Multimodal-Dataset-Distillation-for-Image-Text-Retrieval" class="headerlink" title="Multimodal Dataset Distillation for Image-Text Retrieval"></a>Multimodal Dataset Distillation for Image-Text Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07545">http://arxiv.org/abs/2308.07545</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xindi Wu, Zhiwei Deng, Olga Russakovsky</li>
<li>for: 这篇论文的目的是扩展 dataset distillation 方法到vision-language模型的训练中，以实现从零开始训练新模型的可能性。</li>
<li>methods: 本文提出了一个基于对应汇总的多Modal dataset distillation 方法，将影像和其相应的语言描述汇总在一个对应的形式中。</li>
<li>results: 本文比较了三种核心集选择方法 (strategic subsampling of the training dataset)，并证明了对于具有挑战性的 Flickr30K 和 COCO 检索准确度测试 benchmark 的改进，将最好的核心集选择方法选择 1000 个影像-文本组合用于训练，仅能实现 5.6% 的影像-文本搜寻精度 (recall@1)，而对于我们的 dataset distillation 方法仅需要 100 个训练组合 (一个次元的数量更少)，则可以实现类似的精度。<details>
<summary>Abstract</summary>
Dataset distillation methods offer the promise of reducing a large-scale dataset down to a significantly smaller set of (potentially synthetic) training examples, which preserve sufficient information for training a new model from scratch. So far dataset distillation methods have been developed for image classification. However, with the rise in capabilities of vision-language models, and especially given the scale of datasets necessary to train these models, the time is ripe to expand dataset distillation methods beyond image classification. In this work, we take the first steps towards this goal by expanding on the idea of trajectory matching to create a distillation method for vision-language datasets. The key challenge is that vision-language datasets do not have a set of discrete classes. To overcome this, our proposed multimodal dataset distillation method jointly distill the images and their corresponding language descriptions in a contrastive formulation. Since there are no existing baselines, we compare our approach to three coreset selection methods (strategic subsampling of the training dataset), which we adapt to the vision-language setting. We demonstrate significant improvements on the challenging Flickr30K and COCO retrieval benchmark: the best coreset selection method which selects 1000 image-text pairs for training is able to achieve only 5.6% image-to-text retrieval accuracy (recall@1); in contrast, our dataset distillation approach almost doubles that with just 100 (an order of magnitude fewer) training pairs.
</details>
<details>
<summary>摘要</summary>
dataset 简化方法可以将大规模 dataset 缩小到一个较小的（可能是人工生成的）训练示例集，保留足够的信息来训练一个新模型从头开始。目前，dataset 简化方法已经被开发出来用于图像分类。然而，随着视觉语言模型的能力的提高，特别是对于训练这些模型所需的数据集的规模的增长，现在是时候扩展 dataset 简化方法到更多领域。在这项工作中，我们做出了首先的尝试，扩展了路径匹配的想法，以创建一种用于视觉语言 dataset 的简化方法。主要挑战在于视觉语言 dataset 没有固定的分类集。为了解决这个问题，我们提出了一种多Modal 的 dataset 简化方法，通过对图像和其相应的语言描述进行joint降维来实现。由于没有现有的基准，我们对这种方法进行比较，并将其与三种核心选择方法（策略性抽样）进行比较。我们在复杂的 Flickr30K 和 COCO 检索benchmark上显示出了显著的改善：最佳核心选择方法，选择 1000 个图像-文本对作为训练集，只能达到 5.6% 的图像-文本检索精度（recall@1）；相比之下，我们的 dataset 简化方法可以在 100 个训练对（一个小数量）下达到同样的精度。
</details></li>
</ul>
<hr>
<h2 id="Visual-and-Textual-Prior-Guided-Mask-Assemble-for-Few-Shot-Segmentation-and-Beyond"><a href="#Visual-and-Textual-Prior-Guided-Mask-Assemble-for-Few-Shot-Segmentation-and-Beyond" class="headerlink" title="Visual and Textual Prior Guided Mask Assemble for Few-Shot Segmentation and Beyond"></a>Visual and Textual Prior Guided Mask Assemble for Few-Shot Segmentation and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07539">http://arxiv.org/abs/2308.07539</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Shuai, Meng Fanman, Zhang Runtong, Qiu Heqian, Li Hongliang, Wu Qingbo, Xu Linfeng<br>for:* The paper is written for few-shot segmentation (FSS) tasks, specifically to enhance the generalization ability of FSS models using CLIP.methods:* The proposed method, PGMA-Net, employs a class-agnostic mask assembly process to alleviate bias towards base classes, and formulates diverse tasks into a unified manner by assembling prior through affinity.* The method includes a Prior-Guided Mask Assemble Module (PGMAM) with multiple General Assemble Units (GAUs) that consider diverse and plug-and-play interactions, and a Hierarchical Decoder with Channel-Drop Mechanism (HDCDM) to flexibly exploit assembled masks and low-level features.results:* The proposed PGMA-Net achieves new state-of-the-art results in the FSS task, with mIoU of $77.6$ on $\text{PASCAL-}5^i$ and $59.4$ on $\text{COCO-}20^i$ in 1-shot scenario.* The method can also solve bbox-level and cross-domain FSS, co-segmentation, zero-shot segmentation (ZSS) tasks, leading to an any-shot segmentation framework without extra re-training.<details>
<summary>Abstract</summary>
Few-shot segmentation (FSS) aims to segment the novel classes with a few annotated images. Due to CLIP's advantages of aligning visual and textual information, the integration of CLIP can enhance the generalization ability of FSS model. However, even with the CLIP model, the existing CLIP-based FSS methods are still subject to the biased prediction towards base classes, which is caused by the class-specific feature level interactions. To solve this issue, we propose a visual and textual Prior Guided Mask Assemble Network (PGMA-Net). It employs a class-agnostic mask assembly process to alleviate the bias, and formulates diverse tasks into a unified manner by assembling the prior through affinity. Specifically, the class-relevant textual and visual features are first transformed to class-agnostic prior in the form of probability map. Then, a Prior-Guided Mask Assemble Module (PGMAM) including multiple General Assemble Units (GAUs) is introduced. It considers diverse and plug-and-play interactions, such as visual-textual, inter- and intra-image, training-free, and high-order ones. Lastly, to ensure the class-agnostic ability, a Hierarchical Decoder with Channel-Drop Mechanism (HDCDM) is proposed to flexibly exploit the assembled masks and low-level features, without relying on any class-specific information. It achieves new state-of-the-art results in the FSS task, with mIoU of $77.6$ on $\text{PASCAL-}5^i$ and $59.4$ on $\text{COCO-}20^i$ in 1-shot scenario. Beyond this, we show that without extra re-training, the proposed PGMA-Net can solve bbox-level and cross-domain FSS, co-segmentation, zero-shot segmentation (ZSS) tasks, leading an any-shot segmentation framework.
</details>
<details>
<summary>摘要</summary>
“几shot分类（FSS）的目标是使用几个标注图像来分类新的类别。由于CLIP的优点，将CLIP与FSS模型结合可以提高模型的扩展能力。然而，即使使用CLIP模型，现有的CLIP-based FSS方法仍然受到基本类别的预测偏好，这是由于类别特定的层次交互所致。为解决这个问题，我们提出了一个可视和文本对照的 Prior Guided Mask Assemble Network (PGMA-Net)。它使用一个类别不偏的掩模过程来减少偏好，并将多种任务转换为一个统一的形式。具体来说，首先将类别相关的文本和可见特征转换为类别不偏的机会地图。然后，我们引入一个 Prior-Guided Mask Assemble Module (PGMAM)，包括多个通用组合单元 (GAUs)。它考虑了多种不同和可插入的交互，例如可见文本、间隔和内部图像、训练无须、高阶的交互。最后，为保持类别不偏的能力，我们提出了一个弹性调节的高级解码器 (HDCDM)，以灵活地利用掩模和低层特征，不需要靠类别特定的信息。它实现了新的顶尖成绩在FSS任务中，具体为PASCAL-$5^i$中的$77.6$和COCO-$20^i$中的$59.4$在1架构enario中。此外，我们显示了在无需额外重训的情况下，提案的PGMA-Net可以解决矩形范围内的FSS、共 segmentation、零shot segmentation (ZSS)任务，实现一个任何shot segmentation框架。”
</details></li>
</ul>
<hr>
<h2 id="AttMOT-Improving-Multiple-Object-Tracking-by-Introducing-Auxiliary-Pedestrian-Attributes"><a href="#AttMOT-Improving-Multiple-Object-Tracking-by-Introducing-Auxiliary-Pedestrian-Attributes" class="headerlink" title="AttMOT: Improving Multiple-Object Tracking by Introducing Auxiliary Pedestrian Attributes"></a>AttMOT: Improving Multiple-Object Tracking by Introducing Auxiliary Pedestrian Attributes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07537">http://arxiv.org/abs/2308.07537</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunhao Li, Zhen Xiao, Lin Yang, Dan Meng, Xin Zhou, Heng Fan, Libo Zhang<br>for:* The paper aims to address the gap in exploring pedestrian attributes in multi-object tracking (MOT) and propose a method to predict pedestrian attributes to support general Re-ID embedding.methods:* The proposed method AAM explores different approaches to fuse Re-ID embedding and pedestrian attributes, including attention mechanisms, to improve the performance of MOT.results:* The proposed method AAM achieves consistent improvements in MOTA, HOTA, AssA, IDs, and IDF1 scores on several representative pedestrian multi-object tracking benchmarks, including MOT17 and MOT20, when applied to state-of-the-art trackers.<details>
<summary>Abstract</summary>
Multi-object tracking (MOT) is a fundamental problem in computer vision with numerous applications, such as intelligent surveillance and automated driving. Despite the significant progress made in MOT, pedestrian attributes, such as gender, hairstyle, body shape, and clothing features, which contain rich and high-level information, have been less explored. To address this gap, we propose a simple, effective, and generic method to predict pedestrian attributes to support general Re-ID embedding. We first introduce AttMOT, a large, highly enriched synthetic dataset for pedestrian tracking, containing over 80k frames and 6 million pedestrian IDs with different time, weather conditions, and scenarios. To the best of our knowledge, AttMOT is the first MOT dataset with semantic attributes. Subsequently, we explore different approaches to fuse Re-ID embedding and pedestrian attributes, including attention mechanisms, which we hope will stimulate the development of attribute-assisted MOT. The proposed method AAM demonstrates its effectiveness and generality on several representative pedestrian multi-object tracking benchmarks, including MOT17 and MOT20, through experiments on the AttMOT dataset. When applied to state-of-the-art trackers, AAM achieves consistent improvements in MOTA, HOTA, AssA, IDs, and IDF1 scores. For instance, on MOT17, the proposed method yields a +1.1 MOTA, +1.7 HOTA, and +1.8 IDF1 improvement when used with FairMOT. To encourage further research on attribute-assisted MOT, we will release the AttMOT dataset.
</details>
<details>
<summary>摘要</summary>
多bject tracking (MOT) 是计算机视觉中的基本问题，具有许多应用，如智能监控和自动驾驶。 DESPITE 在 MOT 中做出了 significan progress， pedestrian 特征，如性别、发型、身体形态和服装特征，具有丰富和高级信息，却得到了更少的关注。为了解决这一漏洞，我们提出了一种简单、有效和通用的方法，可以预测 pedestrian 特征，以支持通用 Re-ID 嵌入。我们首先介绍 AttMOT，一个大型、高度充实的人工synthetic dataset for pedestrian tracking，包含了80k帧和6000万个 pedestrian ID，具有不同的时间、天气和场景。我们知道 AttMOT 是首个具有semantic attribute的 MOT dataset。然后，我们探索了不同的方法来融合 Re-ID 嵌入和 pedestrian 特征，包括注意力机制。我们希望这种方法能够激发attribute-assisted MOT的发展。我们提出的方法 AAM 在多个表现 pedestrian multi-object tracking benchmarks，包括 MOT17 和 MOT20，通过在 AttMOT  dataset上进行实验，实现了显著的改进。例如，在 MOT17 上，我们的方法可以提高 +1.1 MOTA、+1.7 HOTA 和 +1.8 IDF1 分数。为了鼓励 attribute-assisted MOT 的进一步研究，我们将在 AttMOT dataset上发布 AttMOT。
</details></li>
</ul>
<hr>
<h2 id="Improved-Region-Proposal-Network-for-Enhanced-Few-Shot-Object-Detection"><a href="#Improved-Region-Proposal-Network-for-Enhanced-Few-Shot-Object-Detection" class="headerlink" title="Improved Region Proposal Network for Enhanced Few-Shot Object Detection"></a>Improved Region Proposal Network for Enhanced Few-Shot Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07535">http://arxiv.org/abs/2308.07535</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zshanggu/htrpn">https://github.com/zshanggu/htrpn</a></li>
<li>paper_authors: Zeyu Shangguan, Mohammad Rostami</li>
<li>For: 这种研究是为了解决深度学习基本监督学习方法的限制，提高对象检测任务的性能。* Methods: 该研究提出了一种半监督法，通过使用无标注数据进行训练，提高几招物检测性能。具体来说，他们开发了一种层次三元分类区提案网络（HTRPN），以便检测并分类未标注的新类实例。* Results: 对COCO和PASCAL VOC基准数据集进行测试，研究结果表明，该方法可以提高几招物检测性能，并超越现有的状态平台FSOD方法。<details>
<summary>Abstract</summary>
Despite significant success of deep learning in object detection tasks, the standard training of deep neural networks requires access to a substantial quantity of annotated images across all classes. Data annotation is an arduous and time-consuming endeavor, particularly when dealing with infrequent objects. Few-shot object detection (FSOD) methods have emerged as a solution to the limitations of classic object detection approaches based on deep learning. FSOD methods demonstrate remarkable performance by achieving robust object detection using a significantly smaller amount of training data. A challenge for FSOD is that instances from novel classes that do not belong to the fixed set of training classes appear in the background and the base model may pick them up as potential objects. These objects behave similarly to label noise because they are classified as one of the training dataset classes, leading to FSOD performance degradation. We develop a semi-supervised algorithm to detect and then utilize these unlabeled novel objects as positive samples during the FSOD training stage to improve FSOD performance. Specifically, we develop a hierarchical ternary classification region proposal network (HTRPN) to localize the potential unlabeled novel objects and assign them new objectness labels to distinguish these objects from the base training dataset classes. Our improved hierarchical sampling strategy for the region proposal network (RPN) also boosts the perception ability of the object detection model for large objects. We test our approach and COCO and PASCAL VOC baselines that are commonly used in FSOD literature. Our experimental results indicate that our method is effective and outperforms the existing state-of-the-art (SOTA) FSOD methods. Our implementation is provided as a supplement to support reproducibility of the results.
</details>
<details>
<summary>摘要</summary>
尽管深度学习在对象检测任务中具有显著的成功，但标准的深度神经网络训练需要大量的标注图像，特别是处理不常见的对象。几拟对象检测（FSOD）方法已经出现，以解决深度学习对象检测方法的限制。FSOD方法可以达到使用较少的训练数据来实现稳定的对象检测性能。然而，FSOD中的一个挑战是，训练集中不存在的新类型对象可能会出现在背景中，并被基础模型认为是可能的对象。这些对象会被视为标注噪声，导致FSOD性能下降。我们提出了一种半supervised算法，用于检测并利用训练集外的未标注新对象作为Positive样本，以改进FSOD性能。具体来说，我们开发了一种嵌入式三元分类区域提案网络（HTRPN），用于定位潜在的未标注新对象，并将其分配新的对象性标签，以分开这些对象与基础训练集类别。我们还改进了RPN的层次采样策略，以提高对象检测模型对大对象的感知能力。我们测试了我们的方法，并与COCO和PASCAL VOC基线相比较。我们的实验结果表明，我们的方法是有效的，并超越了现有的状态的最佳方法（SOTA）。我们的实现提供了补充，以支持result的重复性。
</details></li>
</ul>
<hr>
<h2 id="Inverse-Lithography-Physics-informed-Deep-Neural-Level-Set-for-Mask-Optimization"><a href="#Inverse-Lithography-Physics-informed-Deep-Neural-Level-Set-for-Mask-Optimization" class="headerlink" title="Inverse Lithography Physics-informed Deep Neural Level Set for Mask Optimization"></a>Inverse Lithography Physics-informed Deep Neural Level Set for Mask Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12299">http://arxiv.org/abs/2308.12299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xing-Yu Ma, Shaogang Hao</li>
<li>for: 提高磁版印刷过程中的分辨率，提高磁版印刷过程中的印刷可靠性</li>
<li>methods: 利用深度学习（DL）方法和层设法（ILT），实现磁版优化</li>
<li>results: 相比于各种纯DL和ILT方法，ILDLS方法可以减少计算时间，提高印刷可靠性和过程窗口（PW）等效果<details>
<summary>Abstract</summary>
As the feature size of integrated circuits continues to decrease, optical proximity correction (OPC) has emerged as a crucial resolution enhancement technology for ensuring high printability in the lithography process. Recently, level set-based inverse lithography technology (ILT) has drawn considerable attention as a promising OPC solution, showcasing its powerful pattern fidelity, especially in advanced process. However, massive computational time consumption of ILT limits its applicability to mainly correcting partial layers and hotspot regions. Deep learning (DL) methods have shown great potential in accelerating ILT. However, lack of domain knowledge of inverse lithography limits the ability of DL-based algorithms in process window (PW) enhancement and etc. In this paper, we propose an inverse lithography physics-informed deep neural level set (ILDLS) approach for mask optimization. This approach utilizes level set based-ILT as a layer within the DL framework and iteratively conducts mask prediction and correction to significantly enhance printability and PW in comparison with results from pure DL and ILT. With this approach, computation time is reduced by a few orders of magnitude versus ILT. By gearing up DL with knowledge of inverse lithography physics, ILDLS provides a new and efficient mask optimization solution.
</details>
<details>
<summary>摘要</summary>
(Note: The text has been translated into Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. The translation may vary depending on the region and dialect.)
</details></li>
</ul>
<hr>
<h2 id="Confidence-Contours-Uncertainty-Aware-Annotation-for-Medical-Semantic-Segmentation"><a href="#Confidence-Contours-Uncertainty-Aware-Annotation-for-Medical-Semantic-Segmentation" class="headerlink" title="Confidence Contours: Uncertainty-Aware Annotation for Medical Semantic Segmentation"></a>Confidence Contours: Uncertainty-Aware Annotation for Medical Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07528">http://arxiv.org/abs/2308.07528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andre Ye, Quan Ze Chen, Amy Zhang</li>
<li>for: 本研究旨在提出一种新的验证图像分割模型的方法，以增强模型对不确定性的理解，从而更好地处理视觉抽象。</li>
<li>methods: 本研究提出了一种新的分割表示方法，称为信度轮廓（Confidence Contours），该方法通过高信度和低信度的轮廓来捕捉不确定性。同时，研究人员还开发了一种新的标注系统，用于收集轮廓数据。</li>
<li>results: 研究人员在Lung Image Dataset Consortium（LIDC）和一个 sintetic dataset上进行了评估，结果表明，Confidence Contours可以准确地捕捉不确定性，而且与标准的单个精度标注相比， annotator的努力不会增加太多。此外，研究人员还发现，通用的分割模型可以很好地学习Confidence Contours。最后，在5名医学专家的采访中，研究人员发现，Confidence Contour map比bayesian map更易于理解，因为它能够反映结构不确定性。<details>
<summary>Abstract</summary>
Medical image segmentation modeling is a high-stakes task where understanding of uncertainty is crucial for addressing visual ambiguity. Prior work has developed segmentation models utilizing probabilistic or generative mechanisms to infer uncertainty from labels where annotators draw a singular boundary. However, as these annotations cannot represent an individual annotator's uncertainty, models trained on them produce uncertainty maps that are difficult to interpret. We propose a novel segmentation representation, Confidence Contours, which uses high- and low-confidence ``contours'' to capture uncertainty directly, and develop a novel annotation system for collecting contours. We conduct an evaluation on the Lung Image Dataset Consortium (LIDC) and a synthetic dataset. From an annotation study with 30 participants, results show that Confidence Contours provide high representative capacity without considerably higher annotator effort. We also find that general-purpose segmentation models can learn Confidence Contours at the same performance level as standard singular annotations. Finally, from interviews with 5 medical experts, we find that Confidence Contour maps are more interpretable than Bayesian maps due to representation of structural uncertainty.
</details>
<details>
<summary>摘要</summary>
医学图像分割模型化是一项高风险任务，理解不确定性是关键来解决视觉 ambiguity。先前的工作已经开发出了使用概率或生成机制来推导不确定性从标签中的分割模型，但这些标签不能表示个体注意者的不确定性，因此模型从这些标签学习的不确定性地图很难解释。我们提出了一种新的分割表示方式，即信心轮廓，可以直接捕捉不确定性，并开发了一种新的注意者系统来收集轮廓。我们在Lung Image Dataset Consortium（LIDC）和一个 sintetic  dataset上进行了评估。从30名参与者的注意者研究中，结果表明，信心轮廓可以提供高度表示能力，而无需较大的注意者努力。此外，我们发现，通用分割模型可以学习信心轮廓，并且与标准单个标签学习模型的性能相当。最后，经验了5名医学专家的采访，发现，信心轮廓地图比bayesian地图更易于理解，因为它表示了结构不确定性。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Scalable-Epistemic-Uncertainty-Quantification-in-Organ-Segmentation"><a href="#Benchmarking-Scalable-Epistemic-Uncertainty-Quantification-in-Organ-Segmentation" class="headerlink" title="Benchmarking Scalable Epistemic Uncertainty Quantification in Organ Segmentation"></a>Benchmarking Scalable Epistemic Uncertainty Quantification in Organ Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07506">http://arxiv.org/abs/2308.07506</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jadie1/medseguq">https://github.com/jadie1/medseguq</a></li>
<li>paper_authors: Jadie Adams, Shireen Y. Elhabian</li>
<li>for: 这个论文的目的是评估多种基于深度学习的自动组织器gmentation方法中的epistemicuncertainty量化方法，以便在临床应用中提供可靠和可Robust的模型。</li>
<li>methods: 本文使用了多种epistemic uncertainty量化方法，包括Bayesian neural networks, Monte Carlo dropout, and Deep Ensembles，并进行了比较性 benchmarking 测试。</li>
<li>results: 研究发现，Deep Ensembles方法在accuracy和uncertainty calibration方面表现最佳，而Bayesian neural networks方法在out-of-distribution detection方面表现最好。本文还提供了每种方法的优缺点和未来改进的建议。<details>
<summary>Abstract</summary>
Deep learning based methods for automatic organ segmentation have shown promise in aiding diagnosis and treatment planning. However, quantifying and understanding the uncertainty associated with model predictions is crucial in critical clinical applications. While many techniques have been proposed for epistemic or model-based uncertainty estimation, it is unclear which method is preferred in the medical image analysis setting. This paper presents a comprehensive benchmarking study that evaluates epistemic uncertainty quantification methods in organ segmentation in terms of accuracy, uncertainty calibration, and scalability. We provide a comprehensive discussion of the strengths, weaknesses, and out-of-distribution detection capabilities of each method as well as recommendations for future improvements. These findings contribute to the development of reliable and robust models that yield accurate segmentations while effectively quantifying epistemic uncertainty.
</details>
<details>
<summary>摘要</summary>
深度学习基于方法可能在自动器官分割方面展示了较好的表现，但是量化和理解模型预测结果中的不确定性是重要的。虽然许多技术已经被提出用于知识型或模型基的不确定性估计，但是尚未清楚哪种方法在医学图像分析场景中更有优势。这篇论文提供了一项完整的比较研究，评估了器官分割中 epistemic 不确定性估计方法的准确性、不确定性归一化和可扩展性。我们提供了每种方法的优缺点、缺失和离群检测能力，以及未来改进的建议。这些发现有助于开发可靠和可靠的模型，以便实现准确的分割和有效地量化 epistemic 不确定性。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and widely used in other countries as well. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="ICAFusion-Iterative-Cross-Attention-Guided-Feature-Fusion-for-Multispectral-Object-Detection"><a href="#ICAFusion-Iterative-Cross-Attention-Guided-Feature-Fusion-for-Multispectral-Object-Detection" class="headerlink" title="ICAFusion: Iterative Cross-Attention Guided Feature Fusion for Multispectral Object Detection"></a>ICAFusion: Iterative Cross-Attention Guided Feature Fusion for Multispectral Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07504">http://arxiv.org/abs/2308.07504</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chanchanchan97/icafusion">https://github.com/chanchanchan97/icafusion</a></li>
<li>paper_authors: Jifeng Shen, Yifei Chen, Yue Liu, Xin Zuo, Heng Fan, Wankou Yang</li>
<li>for: 本研究旨在提高多spectral图像的特征融合，以提高多spectral对象检测的精度。</li>
<li>methods: 提出了一种基于双cross-attention transformer的新特征融合框架，通过全球特征交互模型，捕捉多modalitat中的补偿信息，提高对象特征的抑制性。</li>
<li>results: 实验结果表明，提出的方法可以在KAIST、FLIR和VEDAI数据集上实现superior表现，同时具有更快的推理速度，适用于各种实际应用场景。<details>
<summary>Abstract</summary>
Effective feature fusion of multispectral images plays a crucial role in multi-spectral object detection. Previous studies have demonstrated the effectiveness of feature fusion using convolutional neural networks, but these methods are sensitive to image misalignment due to the inherent deffciency in local-range feature interaction resulting in the performance degradation. To address this issue, a novel feature fusion framework of dual cross-attention transformers is proposed to model global feature interaction and capture complementary information across modalities simultaneously. This framework enhances the discriminability of object features through the query-guided cross-attention mechanism, leading to improved performance. However, stacking multiple transformer blocks for feature enhancement incurs a large number of parameters and high spatial complexity. To handle this, inspired by the human process of reviewing knowledge, an iterative interaction mechanism is proposed to share parameters among block-wise multimodal transformers, reducing model complexity and computation cost. The proposed method is general and effective to be integrated into different detection frameworks and used with different backbones. Experimental results on KAIST, FLIR, and VEDAI datasets show that the proposed method achieves superior performance and faster inference, making it suitable for various practical scenarios. Code will be available at https://github.com/chanchanchan97/ICAFusion.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translates into 多spectral图像的有效特征融合在多spectral对象检测中发挥关键作用。先前的研究已经证明了使用卷积神经网络进行特征融合的效iveness，但这些方法容易受到图像不对齐的影响，导致性能下降。为解决这问题，一种新的特征融合框架基于双重交叉注意力变换器是提出来，可以模型全局特征交互和同时捕捉不同模式之间的补做信息。这个框架通过尝试引导的交叉注意力机制来增强对象特征的抗混淆性，从而提高性能。然而，堆叠多个变换器块以提高特征的增强，会增加模型的参数量和空间复杂度。为解决这问题，根据人类审查知识的过程，一种循环互动机制是提出来，可以在不同模式之间共享参数，从而降低模型的参数量和计算量。提出的方法可以与不同的检测框架集成，并且可以与不同的后处器结合使用。实验结果表明，提出的方法在KAIST、FLIR和VEDAI datasets上 achieve superior performance和快速的检测，适用于各种实际应用场景。代码将在https://github.com/chanchanchan97/ICAFusion中公开。
</details></li>
</ul>
<hr>
<h2 id="SpecTracle-Wearable-Facial-Motion-Tracking-from-Unobtrusive-Peripheral-Cameras"><a href="#SpecTracle-Wearable-Facial-Motion-Tracking-from-Unobtrusive-Peripheral-Cameras" class="headerlink" title="SpecTracle: Wearable Facial Motion Tracking from Unobtrusive Peripheral Cameras"></a>SpecTracle: Wearable Facial Motion Tracking from Unobtrusive Peripheral Cameras</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07502">http://arxiv.org/abs/2308.07502</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinan Xuan, Varun Viswanath, Sunny Chu, Owen Bartolf, Jessica Echterhoff, Edward Wang</li>
<li>for: 这个论文旨在实现无障碍的虚拟现实环境中的”面对面”互动。</li>
<li>methods: 该系统使用两个宽角相机，位于幕面上，以实现面部动作跟踪。</li>
<li>results: 该系统可以在实时24帧&#x2F;秒的� mobil GPU上运行，并且可以精准地跟踪用户面部的不同部分运动。个性化协调可以提高跟踪性能42.3%。<details>
<summary>Abstract</summary>
Facial motion tracking in head-mounted displays (HMD) has the potential to enable immersive "face-to-face" interaction in a virtual environment. However, current works on facial tracking are not suitable for unobtrusive augmented reality (AR) glasses or do not have the ability to track arbitrary facial movements. In this work, we demonstrate a novel system called SpecTracle that tracks a user's facial motions using two wide-angle cameras mounted right next to the visor of a Hololens. Avoiding the usage of cameras extended in front of the face, our system greatly improves the feasibility to integrate full-face tracking into a low-profile form factor. We also demonstrate that a neural network-based model processing the wide-angle cameras can run in real-time at 24 frames per second (fps) on a mobile GPU and track independent facial movement for different parts of the face with a user-independent model. Using a short personalized calibration, the system improves its tracking performance by 42.3% compared to the user-independent model.
</details>
<details>
<summary>摘要</summary>
“头戴式显示器（HMD）中的面部运动跟踪可能启用虚拟环境中的互动。然而，当前的面部跟踪方法不适用于不干扰的增强现实（AR）镜或无法跟踪自由的面部运动。在这项工作中，我们介绍了一种名为SpecTracle的系统，它使用两个宽角相机安装在Hololens镜的两侧，以跟踪用户的面部运动。避免使用扩展到面前的相机，我们的系统可以大幅提高将全面跟踪集成到低 профиль形态中的可能性。我们还示出了一种基于神经网络的模型，通过处理宽角相机可以在实时24帧/秒（fps）的移动硬件上运行，并可以独立地跟踪不同部分的面部运动。通过短时间的个性化准备，系统可以提高跟踪性能42.3%比用户无关模型。”
</details></li>
</ul>
<hr>
<h2 id="BSED-Baseline-Shapley-Based-Explainable-Detector"><a href="#BSED-Baseline-Shapley-Based-Explainable-Detector" class="headerlink" title="BSED: Baseline Shapley-Based Explainable Detector"></a>BSED: Baseline Shapley-Based Explainable Detector</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07490">http://arxiv.org/abs/2308.07490</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michihiro Kuroki, Toshihiko Yamasaki</li>
<li>for: 这个论文的目的是提高Explainable Artificial Intelligence（XAI）在图像识别领域的可解释性，并提供一种基于基线特征的可解释的检测器（BSED），以满足可解释性axioms。</li>
<li>methods: 这个论文使用了Shapley值来扩展对象检测，并将其应用到各种检测器中，以实现可解释性。它还可以在不需要细致参数调整的情况下，对各种检测目标进行解释。</li>
<li>results: 论文的结果表明，BSED可以提供更有效的解释，并且可以在各种应用中 correction based on explanations from our method。此外，BSED的处理成本在理解的范围内，而原始的Shapley值则是计算成本过高的。<details>
<summary>Abstract</summary>
Explainable artificial intelligence (XAI) has witnessed significant advances in the field of object recognition, with saliency maps being used to highlight image features relevant to the predictions of learned models. Although these advances have made AI-based technology more interpretable to humans, several issues have come to light. Some approaches present explanations irrelevant to predictions, and cannot guarantee the validity of XAI (axioms). In this study, we propose the Baseline Shapley-based Explainable Detector (BSED), which extends the Shapley value to object detection, thereby enhancing the validity of interpretation. The Shapley value can attribute the prediction of a learned model to a baseline feature while satisfying the explainability axioms. The processing cost for the BSED is within the reasonable range, while the original Shapley value is prohibitively computationally expensive. Furthermore, BSED is a generalizable method that can be applied to various detectors in a model-agnostic manner, and interpret various detection targets without fine-grained parameter tuning. These strengths can enable the practical applicability of XAI. We present quantitative and qualitative comparisons with existing methods to demonstrate the superior performance of our method in terms of explanation validity. Moreover, we present some applications, such as correcting detection based on explanations from our method.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）的解释性（XAI）在图像识别领域已经取得了重要进步，使用焦点图来高亮图像特征，有助于人类更好地理解AI模型的预测。然而，这些进步并不能解决所有问题。一些方法提供不相关的解释，无法保证XAI的AXIoms的正确性。在本研究中，我们提出了基线Shapley值基于的解释探测器（BSED），该方法扩展了Shapley值到对象检测，从而提高了解释的正确性。Shapley值可以归因预测的learned模型到基线特征，同时满足解释AXIoms。BSED的处理成本在合理范围内，而原始Shapley值计算成本过高。此外，BSED是一种通用的方法，可以适用于不同的检测器，并且可以对各种检测目标进行不必做细致参数调整的解释。这些优点使得XAI在实际应用中得到了加强。我们对现有方法进行了量化和质量比较，以示我们的方法在解释正确性方面的超越。此外，我们还展示了一些应用，如通过我们的方法提供的解释来修正检测结果。
</details></li>
</ul>
<hr>
<h2 id="Space-Object-Identification-and-Classification-from-Hyperspectral-Material-Analysis"><a href="#Space-Object-Identification-and-Classification-from-Hyperspectral-Material-Analysis" class="headerlink" title="Space Object Identification and Classification from Hyperspectral Material Analysis"></a>Space Object Identification and Classification from Hyperspectral Material Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07481">http://arxiv.org/abs/2308.07481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Massimiliano Vasile, Lewis Walker, Andrew Campbell, Simao Marto, Paul Murray, Stephen Marshall, Vasili Savitski</li>
<li>for: 这个论文是为了提取未知宇宙对象的谱spectrum信息而设计的数据处理管道。</li>
<li>methods: 该论文使用了两种物质标识和分类技术：一种是基于机器学习，另一种是基于最小二乘匹配已知谱spectrum库。</li>
<li>results: 论文将展示一些初步的物体识别和分类结果。<details>
<summary>Abstract</summary>
This paper presents a data processing pipeline designed to extract information from the hyperspectral signature of unknown space objects. The methodology proposed in this paper determines the material composition of space objects from single pixel images. Two techniques are used for material identification and classification: one based on machine learning and the other based on a least square match with a library of known spectra. From this information, a supervised machine learning algorithm is used to classify the object into one of several categories based on the detection of materials on the object. The behaviour of the material classification methods is investigated under non-ideal circumstances, to determine the effect of weathered materials, and the behaviour when the training library is missing a material that is present in the object being observed. Finally the paper will present some preliminary results on the identification and classification of space objects.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "hyperspectral signature" is translated as "多spectral特征" (duō spectrum de tiào xiàng)* "material composition" is translated as "物质组成" (wù zhì zhōng jī)* "machine learning" is translated as "机器学习" (jī shì xué xí)* "least square match" is translated as "最小二乘匹配" (zuì xiǎo èr chuī pīng pái)* "library of known spectra" is translated as "已知spectra库" (yǐ zhī spectrum kù)* "supervised machine learning algorithm" is translated as "指导式机器学习算法" (dì dǎo xìng jī shì xué xí algoritmos)* "classify the object" is translated as "对象分类" (duì yì fāng lèi)* "weathered materials" is translated as "天然风化物" (tiān zhēn fēng huà wù)Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-MIMO-U-Net-Efficient-and-Accurate-Uncertainty-Estimation-for-Pixel-wise-Regression"><a href="#Probabilistic-MIMO-U-Net-Efficient-and-Accurate-Uncertainty-Estimation-for-Pixel-wise-Regression" class="headerlink" title="Probabilistic MIMO U-Net: Efficient and Accurate Uncertainty Estimation for Pixel-wise Regression"></a>Probabilistic MIMO U-Net: Efficient and Accurate Uncertainty Estimation for Pixel-wise Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07477">http://arxiv.org/abs/2308.07477</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/antonbaumann/mimo-unet">https://github.com/antonbaumann/mimo-unet</a></li>
<li>paper_authors: Anton Baumann, Thomas Roßberg, Michael Schmitt</li>
<li>For: 提高机器学习模型的可靠性和可读性，特别在高度重要的实际应用场景中。* Methods: 基于多输入多输出（MIMO）框架，利用深度神经网络的过参数化来实现像素级回归任务。采用U-Net架构，在单个模型中培养多个互相约束的子网络。还提出了一种同步子网络性能的新程序。* Results: 对两个正交的数据集进行了全面的评估，与现有模型相比，具有相似的准确率，更好的准确率Calibration，robust的out-of-distribution检测能力，并且具有较小的参数大小和执行时间。代码可以在github.com&#x2F;antonbaumann&#x2F;MIMO-Unet中下载。<details>
<summary>Abstract</summary>
Uncertainty estimation in machine learning is paramount for enhancing the reliability and interpretability of predictive models, especially in high-stakes real-world scenarios. Despite the availability of numerous methods, they often pose a trade-off between the quality of uncertainty estimation and computational efficiency. Addressing this challenge, we present an adaptation of the Multiple-Input Multiple-Output (MIMO) framework -- an approach exploiting the overparameterization of deep neural networks -- for pixel-wise regression tasks. Our MIMO variant expands the applicability of the approach from simple image classification to broader computer vision domains. For that purpose, we adapted the U-Net architecture to train multiple subnetworks within a single model, harnessing the overparameterization in deep neural networks. Additionally, we introduce a novel procedure for synchronizing subnetwork performance within the MIMO framework. Our comprehensive evaluations of the resulting MIMO U-Net on two orthogonal datasets demonstrate comparable accuracy to existing models, superior calibration on in-distribution data, robust out-of-distribution detection capabilities, and considerable improvements in parameter size and inference time. Code available at github.com/antonbaumann/MIMO-Unet
</details>
<details>
<summary>摘要</summary>
“机器学习中的不确定性估计是对预测模型的可靠性和解释性提高的重要因素，特别是在高度重要的实际应用中。 despite the availability of numerous methods, they often pose a trade-off between the quality of uncertainty estimation and computational efficiency.  Addressing this challenge, we present an adaptation of the Multiple-Input Multiple-Output（MIMO）framework—an approach exploiting the overparameterization of deep neural networks—for pixel-wise regression tasks. Our MIMO variant expands the applicability of the approach from simple image classification to broader computer vision domains. For that purpose, we adapted the U-Net architecture to train multiple subnetworks within a single model, harnessing the overparameterization in deep neural networks. Additionally, we introduce a novel procedure for synchronizing subnetwork performance within the MIMO framework. Our comprehensive evaluations of the resulting MIMO U-Net on two orthogonal datasets demonstrate comparable accuracy to existing models, superior calibration on in-distribution data, robust out-of-distribution detection capabilities, and considerable improvements in parameter size and inference time. Code available at github.com/antonbaumann/MIMO-Unet”Here's the breakdown of the translation:1. 机器学习 (machine learning) -> 机器学习 (machine learning)2. 不确定性估计 (uncertainty estimation) -> 不确定性估计 (uncertainty estimation)3. MIMO (Multiple-Input Multiple-Output) -> MIMO (多输入多输出)4. overparameterization -> 过参数化5. deep neural networks -> 深度神经网络6. pixel-wise regression -> 像素级别回归7. U-Net -> U-Net8. subnetworks -> 子网络9. in-distribution data -> 在分布中的数据10. out-of-distribution detection -> 外分布检测11. parameter size -> 参数大小12. inference time -> 推理时间Note that Simplified Chinese is used in the translation, which is the standard writing system used in mainland China.
</details></li>
</ul>
<hr>
<h2 id="Reducing-Training-Demands-for-3D-Gait-Recognition-with-Deep-Koopman-Operator-Constraints"><a href="#Reducing-Training-Demands-for-3D-Gait-Recognition-with-Deep-Koopman-Operator-Constraints" class="headerlink" title="Reducing Training Demands for 3D Gait Recognition with Deep Koopman Operator Constraints"></a>Reducing Training Demands for 3D Gait Recognition with Deep Koopman Operator Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07468">http://arxiv.org/abs/2308.07468</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cole Hill, Mauricio Pamplona Segundo, Sudeep Sarkar</li>
<li>for: 本研究的目的是提出一种使用深度学习技术实现人体步态识别，并使用Linear Dynamical Systems（LDS）模块和损失函数来保证时间相关性和动态可靠性。</li>
<li>methods: 本研究使用了深度神经网络来适应3D人体步态数据，并引入了LDS模块和基于Koopman算子理论的损失函数来保证模型的动态可靠性和时间相关性。</li>
<li>results: 根据USF HumanID和CASIA-B dataset的比较，本研究的LDS方法可以在训练数据的限制下实现更高的准确率，而且3D模型方法在不同的视角变化和包袋等情况下也表现更好。<details>
<summary>Abstract</summary>
Deep learning research has made many biometric recognition solution viable, but it requires vast training data to achieve real-world generalization. Unlike other biometric traits, such as face and ear, gait samples cannot be easily crawled from the web to form massive unconstrained datasets. As the human body has been extensively studied for different digital applications, one can rely on prior shape knowledge to overcome data scarcity. This work follows the recent trend of fitting a 3D deformable body model into gait videos using deep neural networks to obtain disentangled shape and pose representations for each frame. To enforce temporal consistency in the network, we introduce a new Linear Dynamical Systems (LDS) module and loss based on Koopman operator theory, which provides an unsupervised motion regularization for the periodic nature of gait, as well as a predictive capacity for extending gait sequences. We compare LDS to the traditional adversarial training approach and use the USF HumanID and CASIA-B datasets to show that LDS can obtain better accuracy with less training data. Finally, we also show that our 3D modeling approach is much better than other 3D gait approaches in overcoming viewpoint variation under normal, bag-carrying and clothing change conditions.
</details>
<details>
<summary>摘要</summary>
This study uses deep neural networks to fit a 3D deformable body model to gait videos and get separate shape and pose representations for each frame. To make sure the movements in the videos are consistent, we use a new Linear Dynamical Systems (LDS) module and loss based on Koopman operator theory. This approach provides an unsupervised motion regularization for the periodic nature of gait, as well as a way to predict how gait sequences will continue. We compare LDS to traditional adversarial training and use the USF HumanID and CASIA-B datasets to show that LDS can get better accuracy with less training data.Finally, we show that our 3D modeling approach is much better than other 3D gait approaches at handling changes in viewpoint, bag-carrying, and clothing under normal conditions.
</details></li>
</ul>
<hr>
<h2 id="There-Is-a-Digital-Art-History"><a href="#There-Is-a-Digital-Art-History" class="headerlink" title="There Is a Digital Art History"></a>There Is a Digital Art History</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07464">http://arxiv.org/abs/2308.07464</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Gracetyty/art-gallery">https://github.com/Gracetyty/art-gallery</a></li>
<li>paper_authors: Leonardo Impett, Fabian Offert</li>
<li>for: 本研究探讨了 Johanna Drucker 十年前提出的问题：“是否有数字艺术历史？”，以及在大规模变换器基础上的视觉模型的出现对数字艺术历史的影响。</li>
<li>methods: 本研究使用了两种主要方法：一是对大规模视觉模型中新编码的视艺抄影重要性进行分析，二是通过使用当代大规模视觉模型investigate基本问题从艺术史和城市规划等领域来进行技术 caso study。</li>
<li>results: 研究结果表明，大规模视觉模型在数字艺术历史方面可能会导致一个新的парадигShift，因为它们可以自动批处和抽象不同形式的视觉逻辑，并且在数字生活中已经广泛应用。同时，这些系统需要一种新的批判方法，该方法需要考虑模型和其应用之间的知识共生。<details>
<summary>Abstract</summary>
In this paper, we revisit Johanna Drucker's question, "Is there a digital art history?" -- posed exactly a decade ago -- in the light of the emergence of large-scale, transformer-based vision models. While more traditional types of neural networks have long been part of digital art history, and digital humanities projects have recently begun to use transformer models, their epistemic implications and methodological affordances have not yet been systematically analyzed. We focus our analysis on two main aspects that, together, seem to suggest a coming paradigm shift towards a "digital" art history in Drucker's sense. On the one hand, the visual-cultural repertoire newly encoded in large-scale vision models has an outsized effect on digital art history. The inclusion of significant numbers of non-photographic images allows for the extraction and automation of different forms of visual logics. Large-scale vision models have "seen" large parts of the Western visual canon mediated by Net visual culture, and they continuously solidify and concretize this canon through their already widespread application in all aspects of digital life. On the other hand, based on two technical case studies of utilizing a contemporary large-scale visual model to investigate basic questions from the fields of art history and urbanism, we suggest that such systems require a new critical methodology that takes into account the epistemic entanglement of a model and its applications. This new methodology reads its corpora through a neural model's training data, and vice versa: the visual ideologies of research datasets and training datasets become entangled.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们重新回归到 Johanna Drucker 提出的十年前的问题：“是否有数字艺术历史？”——传统神经网络已经长期出现在数字艺术史上，而最近的数字人文科学项目则开始使用转换器模型。然而，这些模型的认知途径和方法论上的影响尚未系统地分析。我们将分析两个主要方面，这两个方面共同表明一种可能的未来方向：数字艺术史。一方面，大规模感知模型中新编码的视觉文化财富对数字艺术史产生了巨大的影响。由于大量非摄影图像的包容，可以自动提取和抽象不同类型的视觉逻辑。大规模感知模型已经“看到”了西方视觉Canvas的大部分，并且不断巩固和固化这个Canvas，通过在所有数字生活中广泛应用。另一方面，基于两个实践案例，我们建议需要一种新的批判方法，该方法考虑模型和其应用之间的认知纠缠。这种新方法可以通过神经网络的训练数据和研究数据来读取 corpora，并且反之，研究数据和训练数据的视觉意识都会紧密相互纠缠。
</details></li>
</ul>
<hr>
<h2 id="U-Turn-Diffusion"><a href="#U-Turn-Diffusion" class="headerlink" title="U-Turn Diffusion"></a>U-Turn Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07421">http://arxiv.org/abs/2308.07421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamidreza Behjoo, Michael Chertkov</li>
<li>for: 这个论文探讨了基于人工智能的 diffusion 模型，用于生成合成图像。这些模型利用动态辅助时间机制，通过随机差分方程来获得分数函数。</li>
<li>methods: 该论文提出了一个效果评价标准：生成过程中快速谱 correlation 的破坏能力直接关系到生成图像质量。此外， authors 还提出了一种“U-Turn Diffusion”技术，通过将标准前向 diffusion 过程缩短，然后执行标准反向动力，最终生成一个与 i.i.d. 样本 Distribution 相似的合成图像。</li>
<li>results: 该论文通过使用不同的分析工具，如自相关分析、分数函数质量分析和高斯分布预测测试，来分析相关的时间尺度。结果表明，在优化 U-turn 时间后，生成的合成图像与实际数据样本之间的干扰距离最小化。<details>
<summary>Abstract</summary>
We present a comprehensive examination of score-based diffusion models of AI for generating synthetic images. These models hinge upon a dynamic auxiliary time mechanism driven by stochastic differential equations, wherein the score function is acquired from input images. Our investigation unveils a criterion for evaluating efficiency of the score-based diffusion models: the power of the generative process depends on the ability to de-construct fast correlations during the reverse/de-noising phase. To improve the quality of the produced synthetic images, we introduce an approach coined "U-Turn Diffusion". The U-Turn Diffusion technique starts with the standard forward diffusion process, albeit with a condensed duration compared to conventional settings. Subsequently, we execute the standard reverse dynamics, initialized with the concluding configuration from the forward process. This U-Turn Diffusion procedure, combining forward, U-turn, and reverse processes, creates a synthetic image approximating an independent and identically distributed (i.i.d.) sample from the probability distribution implicitly described via input samples. To analyze relevant time scales we employ various analytical tools, including auto-correlation analysis, weighted norm of the score-function analysis, and Kolmogorov-Smirnov Gaussianity test. The tools guide us to establishing that the Kernel Intersection Distance, a metric comparing the quality of synthetic samples with real data samples, is minimized at the optimal U-turn time.
</details>
<details>
<summary>摘要</summary>
我们提出了一项全面的检查Score-based扩散模型，用于生成 sintetic 图像。这些模型基于动态辅助时间机制驱动的随机差分方程，其中Score函数从输入图像中获得。我们的调查发现一个用于评估扩散模型效率的标准：扩散过程中快速相关性的破坏能力直接关系到生成过程的能效性。为了提高生成的 sintetic 图像质量，我们提出了“U-Turn扩散”技术。U-Turn扩散过程从标准前进Diffusion过程开始，但是压缩了传统设置中的时间。然后，我们执行标准的反动动态，初始化使用前进过程的结束配置。这种U-Turn扩散过程，包括前进、U-turn和反向过程，可以生成一个约束相同分布的 sintetic 图像，与输入样本的随机分布相对独立。为了分析相关的时间尺度，我们使用了多种分析工具，包括自相关分析、加重函数分析和高斯假设测试。这些工具引导我们确定了最佳U-turn时间，以使得扩散模型可以生成高质量的 sintetic 图像。
</details></li>
</ul>
<hr>
<h2 id="Semantify-Simplifying-the-Control-of-3D-Morphable-Models-using-CLIP"><a href="#Semantify-Simplifying-the-Control-of-3D-Morphable-Models-using-CLIP" class="headerlink" title="Semantify: Simplifying the Control of 3D Morphable Models using CLIP"></a>Semantify: Simplifying the Control of 3D Morphable Models using CLIP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07415">http://arxiv.org/abs/2308.07415</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Omergral/Semantify">https://github.com/Omergral/Semantify</a></li>
<li>paper_authors: Omer Gralnik, Guy Gafni, Ariel Shamir</li>
<li>for: 用于自动控制3D形态模型</li>
<li>methods: 使用CLIP语言视觉基础模型的 semantic 力进行自我超vised 训练</li>
<li>results: 实现了定制3D形态模型的简单 slider  интерфей스，并可以快速地适应各种3D模型的定制。Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written for the purpose of simplifying the control of 3D morphable models, specifically using self-supervised learning and the semantic power of CLIP language-vision foundation models.</li>
<li>methods: The paper proposes a method called Semantify, which utilizes the semantic power of CLIP to learn a non-linear mapping from scores across a small set of semantically meaningful and disentangled descriptors to the parametric coefficients of a given 3D morphable model. This is done without a human-in-the-loop and using training data created by randomly sampling the model’s parameters, creating various shapes, and rendering them.</li>
<li>results: The paper presents results on numerous 3D morphable models, including body shape models, face shape and expression models, and animal shapes. The results show that the proposed method defines a simple slider interface for intuitive modeling and can be used to instantly fit a 3D parametric body shape to in-the-wild images.<details>
<summary>Abstract</summary>
We present Semantify: a self-supervised method that utilizes the semantic power of CLIP language-vision foundation model to simplify the control of 3D morphable models. Given a parametric model, training data is created by randomly sampling the model's parameters, creating various shapes and rendering them. The similarity between the output images and a set of word descriptors is calculated in CLIP's latent space. Our key idea is first to choose a small set of semantically meaningful and disentangled descriptors that characterize the 3DMM, and then learn a non-linear mapping from scores across this set to the parametric coefficients of the given 3DMM. The non-linear mapping is defined by training a neural network without a human-in-the-loop. We present results on numerous 3DMMs: body shape models, face shape and expression models, as well as animal shapes. We demonstrate how our method defines a simple slider interface for intuitive modeling, and show how the mapping can be used to instantly fit a 3D parametric body shape to in-the-wild images.
</details>
<details>
<summary>摘要</summary>
我们介绍Semantify：一种自动超级方法，利用CLIP语言视觉基础模型的 semantic 力来简化3D可变模型的控制。将 parametric 模型作为input，通过随机抽样模型参数，创建不同形状并rendering 它们。然后，使用CLIP的内存空间计算模型的出力图像和一组字幕描述的相似度。我们的关键想法是首先选择一小集 semantically meaningful 和分离的描述符，描述3DMM的特征，然后学习一个非线性的 mapping 将 scores across 这个集合转换为input 模型的参数。这个 mapping 是通过人工不在从事的方式定义的，我们提出了一个 neural network 的解释。我们在 numerous 3DMM 上进行了实验，包括人体形状模型、脸形和表情模型以及动物形状。我们显示了我们的方法可以定义一个简单的滑块界面，并说明了如何将 mapping 用于快速适应实验中的内部显示。最后，我们显示了我们的方法可以将3D parametric 体形快速适应到野外图像中。
</details></li>
</ul>
<hr>
<h2 id="A-Unified-Query-based-Paradigm-for-Camouflaged-Instance-Segmentation"><a href="#A-Unified-Query-based-Paradigm-for-Camouflaged-Instance-Segmentation" class="headerlink" title="A Unified Query-based Paradigm for Camouflaged Instance Segmentation"></a>A Unified Query-based Paradigm for Camouflaged Instance Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07392">http://arxiv.org/abs/2308.07392</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dongbo811/uqformer">https://github.com/dongbo811/uqformer</a></li>
<li>paper_authors: Do Dong, Jialun Pei, Rongrong Gao, Tian-Zhu Xiang, Shuo Wang, Huan Xiong</li>
<li>for: 提高隐藏的实例分割精度</li>
<li>methods: 使用 query-based 多任务学习框架，包括设计多scales的 unified learning transformer decoder 和 composed query learning paradigm，以 capture 隐藏的对象区域和边界特征</li>
<li>results: 与 14 状态级方法进行比较，实现了隐藏实例分割的显著提高<details>
<summary>Abstract</summary>
Due to the high similarity between camouflaged instances and the background, the recently proposed camouflaged instance segmentation (CIS) faces challenges in accurate localization and instance segmentation. To this end, inspired by query-based transformers, we propose a unified query-based multi-task learning framework for camouflaged instance segmentation, termed UQFormer, which builds a set of mask queries and a set of boundary queries to learn a shared composed query representation and efficiently integrates global camouflaged object region and boundary cues, for simultaneous instance segmentation and instance boundary detection in camouflaged scenarios. Specifically, we design a composed query learning paradigm that learns a shared representation to capture object region and boundary features by the cross-attention interaction of mask queries and boundary queries in the designed multi-scale unified learning transformer decoder. Then, we present a transformer-based multi-task learning framework for simultaneous camouflaged instance segmentation and camouflaged instance boundary detection based on the learned composed query representation, which also forces the model to learn a strong instance-level query representation. Notably, our model views the instance segmentation as a query-based direct set prediction problem, without other post-processing such as non-maximal suppression. Compared with 14 state-of-the-art approaches, our UQFormer significantly improves the performance of camouflaged instance segmentation. Our code will be available at https://github.com/dongbo811/UQFormer.
</details>
<details>
<summary>摘要</summary>
due to the high similarity between camouflaged instances and the background, the recently proposed camouflaged instance segmentation (CIS) faces challenges in accurate localization and instance segmentation. to this end, inspired by query-based transformers, we propose a unified query-based multi-task learning framework for camouflaged instance segmentation, termed UQFormer, which builds a set of mask queries and a set of boundary queries to learn a shared composed query representation and efficiently integrates global camouflaged object region and boundary cues, for simultaneous instance segmentation and instance boundary detection in camouflaged scenarios. specifically, we design a composed query learning paradigm that learns a shared representation to capture object region and boundary features by the cross-attention interaction of mask queries and boundary queries in the designed multi-scale unified learning transformer decoder. then, we present a transformer-based multi-task learning framework for simultaneous camouflaged instance segmentation and camouflaged instance boundary detection based on the learned composed query representation, which also forces the model to learn a strong instance-level query representation. notably, our model views the instance segmentation as a query-based direct set prediction problem, without other post-processing such as non-maximal suppression. compared with 14 state-of-the-art approaches, our UQFormer significantly improves the performance of camouflaged instance segmentation. our code will be available at https://github.com/dongbo811/UQFormer.Here's the word-for-word translation of the text into Simplified Chinese:由于隐形实例和背景的高相似性，最近提出的隐形实例分割（CIS）面临精度地位和实例分割挑战。为此，我们取得了 query-based transformers 的灵感，并提出了一种统一的 query-based 多任务学习框架，称为 UQFormer，该框架在隐形场景中同时进行实例分割和实例边界检测。specifically，我们设计了一种组合查询学习方案，通过 маска查询和边界查询的交互跨度束对象区域和边界特征进行学习共享的查询表示。然后，我们提出了一种基于 transformer 的多任务学习框架，通过学习共享的查询表示来同时进行隐形实例分割和隐形实例边界检测。此外，我们的模型视实例分割为直接查询集prediction问题，不需要其他后处理如非最大suppression。与14种状态级方法进行比较，我们的 UQFormer 显著提高了隐形实例分割的性能。我们的代码将在 https://github.com/dongbo811/UQFormer 上发布。
</details></li>
</ul>
<hr>
<h2 id="DISBELIEVE-Distance-Between-Client-Models-is-Very-Essential-for-Effective-Local-Model-Poisoning-Attacks"><a href="#DISBELIEVE-Distance-Between-Client-Models-is-Very-Essential-for-Effective-Local-Model-Poisoning-Attacks" class="headerlink" title="DISBELIEVE: Distance Between Client Models is Very Essential for Effective Local Model Poisoning Attacks"></a>DISBELIEVE: Distance Between Client Models is Very Essential for Effective Local Model Poisoning Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07387">http://arxiv.org/abs/2308.07387</a></li>
<li>repo_url: None</li>
<li>paper_authors: Indu Joshi, Priyank Upadhya, Gaurav Kumar Nayak, Peter Schüffler, Nassir Navab</li>
<li>for: This paper focuses on the privacy issues in federated learning, specifically in the medical image analysis domain, and proposes a local model poisoning attack called DISBELIEVE to defend against robust aggregation methods.</li>
<li>methods: The proposed DISBELIEVE attack creates malicious parameters or gradients that are close to benign clients’ parameters or gradients but have a high adverse effect on the global model’s performance.</li>
<li>results: The proposed attack significantly lowers the performance of state-of-the-art robust aggregation methods for medical image analysis on three publicly available datasets, and is also effective on natural images for multi-class classification on the benchmark dataset CIFAR-10.Here’s the full Chinese translation of the paper’s abstract:for: 这篇论文关注联合学习中的隐私问题，具体是医疗图像分析领域，并提出了一种本地模型毒品攻击方法called DISBELIEVE，以防止robust集成方法。methods: DISBELIEVE攻击方法创造了假的参数或梯度，使其与正常客户端的参数或梯度很近，但是对全局模型的性能产生高度的负面影响。results: 提议的攻击方法对state-of-the-art robust集成方法在三个公开的医疗图像数据集上显示出了显著的下降性能，并且在自然图像的多类分类任务上也有严重的下降性能。<details>
<summary>Abstract</summary>
Federated learning is a promising direction to tackle the privacy issues related to sharing patients' sensitive data. Often, federated systems in the medical image analysis domain assume that the participating local clients are \textit{honest}. Several studies report mechanisms through which a set of malicious clients can be introduced that can poison the federated setup, hampering the performance of the global model. To overcome this, robust aggregation methods have been proposed that defend against those attacks. We observe that most of the state-of-the-art robust aggregation methods are heavily dependent on the distance between the parameters or gradients of malicious clients and benign clients, which makes them prone to local model poisoning attacks when the parameters or gradients of malicious and benign clients are close. Leveraging this, we introduce DISBELIEVE, a local model poisoning attack that creates malicious parameters or gradients such that their distance to benign clients' parameters or gradients is low respectively but at the same time their adverse effect on the global model's performance is high. Experiments on three publicly available medical image datasets demonstrate the efficacy of the proposed DISBELIEVE attack as it significantly lowers the performance of the state-of-the-art \textit{robust aggregation} methods for medical image analysis. Furthermore, compared to state-of-the-art local model poisoning attacks, DISBELIEVE attack is also effective on natural images where we observe a severe drop in classification performance of the global model for multi-class classification on benchmark dataset CIFAR-10.
</details>
<details>
<summary>摘要</summary>
“联邦学习”是一种解决医疗资料共享时隐私问题的可能性。在医疗影像分析领域中，联邦系统通常假设地方客户端是“正直”的。然而，一些研究发现，可以将一组黑客户端引入联邦系统，导致全球模型的性能下降。为了解决这个问题，一些防护整合方法被提出，但大多数这些方法对于黑客户端的攻击 remain vulnerable。我们引入了一种名为“DISBELIEVE”的本地模型欺骗攻击，它可以创造出黑客户端的参数或梯度，使其与正常客户端的参数或梯度之间的距离很近，但同时对全球模型的性能产生严重的影响。我们在三个公开可用的医疗影像数据集上进行实验，结果显示 DISBELIEVE 攻击可以对现有的robust aggregation方法进行严重攻击，并且与其他本地模型欺骗攻击相比，DISBELIEVE 攻击在自然图像中也有很好的效果。
</details></li>
</ul>
<hr>
<h2 id="The-Devil-in-the-Details-Simple-and-Effective-Optical-Flow-Synthetic-Data-Generation"><a href="#The-Devil-in-the-Details-Simple-and-Effective-Optical-Flow-Synthetic-Data-Generation" class="headerlink" title="The Devil in the Details: Simple and Effective Optical Flow Synthetic Data Generation"></a>The Devil in the Details: Simple and Effective Optical Flow Synthetic Data Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07378">http://arxiv.org/abs/2308.07378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kwon Byung-Ki, Kim Sung-Bin, Tae-Hyun Oh</li>
<li>for: 这 paper 是为了研究 dense optical flow 的进展而写的，特别是使用 supervised learning 方法，需要大量标注数据。</li>
<li>methods: 这 paper 使用了一种 simpler synthetic data generation method，通过组合基本操作来实现一定的真实感。 authors 还提出了一种使用 occlusion masks 的新方法，以帮助 RAFT 网络在 supervised 方法中进行更好的初始化。</li>
<li>results: 据 authors 的实验结果，使用这种新方法可以让 RAFT 网络在 MPI Sintel 和 KITTI 2015 上表现出色，超过原始 RAFT 的表现。<details>
<summary>Abstract</summary>
Recent work on dense optical flow has shown significant progress, primarily in a supervised learning manner requiring a large amount of labeled data. Due to the expensiveness of obtaining large scale real-world data, computer graphics are typically leveraged for constructing datasets. However, there is a common belief that synthetic-to-real domain gaps limit generalization to real scenes. In this paper, we show that the required characteristics in an optical flow dataset are rather simple and present a simpler synthetic data generation method that achieves a certain level of realism with compositions of elementary operations. With 2D motion-based datasets, we systematically analyze the simplest yet critical factors for generating synthetic datasets. Furthermore, we propose a novel method of utilizing occlusion masks in a supervised method and observe that suppressing gradients on occluded regions serves as a powerful initial state in the curriculum learning sense. The RAFT network initially trained on our dataset outperforms the original RAFT on the two most challenging online benchmarks, MPI Sintel and KITTI 2015.
</details>
<details>
<summary>摘要</summary>
最近的紧密光流研究已经取得了重要进步，主要是以监督学习方式进行，需要大量标注数据。由于真实世界数据的获得成本较高，因此通常会利用计算机图形进行数据构造。然而，有一种常见的信念是， sintetic-to-real 领域差限制了对真实场景的泛化。在这篇论文中，我们表明了光流数据集中所需的特征很简单，并提出了一种简单的 sintetic 数据生成方法，该方法可以在元素操作的组合下实现一定的真实感。对于 2D 运动基于的数据集，我们系统地分析了生成 sintetic 数据的最简 yet critical 因素。此外，我们提议了在监督学习方法中使用遮盲mask，并观察到在遮盲区域上抑制梯度 serve as a powerful initial state in the curriculum learning sense。RAFT 网络首先在我们的数据集上进行了训练，然后在两个最为挑战的在线抽象上出色表现，即 MPI Sintel 和 KITTI 2015。
</details></li>
</ul>
<hr>
<h2 id="Jurassic-World-Remake-Bringing-Ancient-Fossils-Back-to-Life-via-Zero-Shot-Long-Image-to-Image-Translation"><a href="#Jurassic-World-Remake-Bringing-Ancient-Fossils-Back-to-Life-via-Zero-Shot-Long-Image-to-Image-Translation" class="headerlink" title="Jurassic World Remake: Bringing Ancient Fossils Back to Life via Zero-Shot Long Image-to-Image Translation"></a>Jurassic World Remake: Bringing Ancient Fossils Back to Life via Zero-Shot Long Image-to-Image Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07316">http://arxiv.org/abs/2308.07316</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alexmartin1722/Revive-2I">https://github.com/alexmartin1722/Revive-2I</a></li>
<li>paper_authors: Alexander Martin, Haitian Zheng, Jie An, Jiebo Luo</li>
<li>for: 这个论文的目的是提出一种可以在大领域差距下进行零shot图像到图像翻译（I2I）的方法，并且能够在不同领域中进行应用。</li>
<li>methods: 这个论文使用了文本引导的潜在扩散模型来实现零shot I2I，并且提出了一个新的任务——Skull2Animal，用于翻译骨骼和生物体之间。</li>
<li>results: 研究发现，传统的I2I方法无法跨大领域差距进行翻译，而文本引导的扩散和图像编辑模型则能够准确地完成零shot I2I。此外，研究还发现，提示是跨大领域差距翻译的关键因素，因为需要将目标领域的优先知识传递给模型。<details>
<summary>Abstract</summary>
With a strong understanding of the target domain from natural language, we produce promising results in translating across large domain gaps and bringing skeletons back to life. In this work, we use text-guided latent diffusion models for zero-shot image-to-image translation (I2I) across large domain gaps (longI2I), where large amounts of new visual features and new geometry need to be generated to enter the target domain. Being able to perform translations across large domain gaps has a wide variety of real-world applications in criminology, astrology, environmental conservation, and paleontology. In this work, we introduce a new task Skull2Animal for translating between skulls and living animals. On this task, we find that unguided Generative Adversarial Networks (GANs) are not capable of translating across large domain gaps. Instead of these traditional I2I methods, we explore the use of guided diffusion and image editing models and provide a new benchmark model, Revive-2I, capable of performing zero-shot I2I via text-prompting latent diffusion models. We find that guidance is necessary for longI2I because, to bridge the large domain gap, prior knowledge about the target domain is needed. In addition, we find that prompting provides the best and most scalable information about the target domain as classifier-guided diffusion models require retraining for specific use cases and lack stronger constraints on the target domain because of the wide variety of images they are trained on.
</details>
<details>
<summary>摘要</summary>
With a strong understanding of the target domain from natural language, we produce promising results in translating across large domain gaps and bringing skeletons back to life. In this work, we use text-guided latent diffusion models for zero-shot image-to-image translation (I2I) across large domain gaps (longI2I), where large amounts of new visual features and new geometry need to be generated to enter the target domain. Being able to perform translations across large domain gaps has a wide variety of real-world applications in criminology, astrology, environmental conservation, and paleontology. In this work, we introduce a new task Skull2Animal for translating between skulls and living animals. On this task, we find that unguided Generative Adversarial Networks (GANs) are not capable of translating across large domain gaps. Instead of these traditional I2I methods, we explore the use of guided diffusion and image editing models and provide a new benchmark model, Revive-2I, capable of performing zero-shot I2I via text-prompting latent diffusion models. We find that guidance is necessary for longI2I because, to bridge the large domain gap, prior knowledge about the target domain is needed. In addition, we find that prompting provides the best and most scalable information about the target domain as classifier-guided diffusion models require retraining for specific use cases and lack stronger constraints on the target domain because of the wide variety of images they are trained on.Note: Simplified Chinese is used in mainland China and Singapore, while Traditional Chinese is used in Hong Kong, Macau, and Taiwan. The translation is written in Simplified Chinese, but the original text is in Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Dual-Associated-Encoder-for-Face-Restoration"><a href="#Dual-Associated-Encoder-for-Face-Restoration" class="headerlink" title="Dual Associated Encoder for Face Restoration"></a>Dual Associated Encoder for Face Restoration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07314">http://arxiv.org/abs/2308.07314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu-Ju Tsai, Yu-Lun Liu, Lu Qi, Kelvin C. K. Chan, Ming-Hsuan Yang</li>
<li>for: 提高低质量图像中的人脸细节 restored</li>
<li>methods: 使用双支持分支框架DAEFR，其中一支支持高质量图像（HQ）特征提取，另一支支持低质量图像（LQ）特征提取，并通过相互协同训练来促进代码预测和输出质量提高</li>
<li>results: 在 synthetic 和 real-world 数据集上，DAEFR 表现出色，可以更好地恢复人脸细节<details>
<summary>Abstract</summary>
Restoring facial details from low-quality (LQ) images has remained a challenging problem due to its ill-posedness induced by various degradations in the wild. The existing codebook prior mitigates the ill-posedness by leveraging an autoencoder and learned codebook of high-quality (HQ) features, achieving remarkable quality. However, existing approaches in this paradigm frequently depend on a single encoder pre-trained on HQ data for restoring HQ images, disregarding the domain gap between LQ and HQ images. As a result, the encoding of LQ inputs may be insufficient, resulting in suboptimal performance. To tackle this problem, we propose a novel dual-branch framework named DAEFR. Our method introduces an auxiliary LQ branch that extracts crucial information from the LQ inputs. Additionally, we incorporate association training to promote effective synergy between the two branches, enhancing code prediction and output quality. We evaluate the effectiveness of DAEFR on both synthetic and real-world datasets, demonstrating its superior performance in restoring facial details.
</details>
<details>
<summary>摘要</summary>
优化 facial details 从低质量（LQ）图像的恢复问题一直是一个挑战，因为这个问题受到野外环境中各种破坏的影响，导致非固定的问题。现有的代码库先验 Mitigates 这个问题，通过使用 autoencoder 和学习的高质量（HQ）特征 codebook，实现了 Remarkable 的质量。然而，现有的这些方法 часто依赖于单个 encoder 预训练在 HQ 数据上，忽视 LQ 和 HQ 图像之间的领域差异。这导致 LQ 输入的编码可能不够，从而导致优化性不足。为解决这个问题，我们提出了一种新的 dual-branch 框架，名为 DAEFR。我们的方法在 auxiliary LQ 分支中提取了关键信息，并将这些信息与主要 HQ 分支相关联，以便在编码和输出质量之间产生有利的共同作用。我们在 synthetic 和实际世界的数据集上评估了 DAEFR 的效果，并证明其在恢复 facial details 方面具有 Superior 的性能。
</details></li>
</ul>
<hr>
<h2 id="Group-Pose-A-Simple-Baseline-for-End-to-End-Multi-person-Pose-Estimation"><a href="#Group-Pose-A-Simple-Baseline-for-End-to-End-Multi-person-Pose-Estimation" class="headerlink" title="Group Pose: A Simple Baseline for End-to-End Multi-person Pose Estimation"></a>Group Pose: A Simple Baseline for End-to-End Multi-person Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07313">http://arxiv.org/abs/2308.07313</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/michel-liu/grouppose-paddle">https://github.com/michel-liu/grouppose-paddle</a></li>
<li>paper_authors: Huan Liu, Qiang Chen, Zichang Tan, Jiang-Jiang Liu, Jian Wang, Xiangbo Su, Xiaolong Li, Kun Yao, Junyu Han, Errui Ding, Yao Zhao, Jingdong Wang</li>
<li>For: 这种论文的目的是研究终端多人姿态估计问题，以DETR-like框架为基础，并主要发展复杂的解码器。* Methods: 这种方法使用了简单 yet effective transformerapproach，名为Group Pose。它将 $K$-keypoint pose estimation视为预测 $N\times K$ 关键点位置，每个关键点从一个关键点查询中预测，同时每个姿态被表示为一个实例查询用于得分 $N$ 姿态预测。* Results: 这种方法无需人工框架监督，在 MS COCO 和 CrowdPose 上实验表明，其表现比前一些使用复杂解码器的方法更好，甚至与使用人工框架监督的 ED-Pose 相当。可以在 $\href{<a target="_blank" rel="noopener" href="https://github.com/Michel-liu/GroupPose-Paddle%7D%7B/rm">https://github.com/Michel-liu/GroupPose-Paddle}{\rm</a> Paddle}$ 和 $\href{<a target="_blank" rel="noopener" href="https://github.com/Michel-liu/GroupPose%7D%7B/rm">https://github.com/Michel-liu/GroupPose}{\rm</a> PyTorch}$ 中找到代码。<details>
<summary>Abstract</summary>
In this paper, we study the problem of end-to-end multi-person pose estimation. State-of-the-art solutions adopt the DETR-like framework, and mainly develop the complex decoder, e.g., regarding pose estimation as keypoint box detection and combining with human detection in ED-Pose, hierarchically predicting with pose decoder and joint (keypoint) decoder in PETR. We present a simple yet effective transformer approach, named Group Pose. We simply regard $K$-keypoint pose estimation as predicting a set of $N\times K$ keypoint positions, each from a keypoint query, as well as representing each pose with an instance query for scoring $N$ pose predictions. Motivated by the intuition that the interaction, among across-instance queries of different types, is not directly helpful, we make a simple modification to decoder self-attention. We replace single self-attention over all the $N\times(K+1)$ queries with two subsequent group self-attentions: (i) $N$ within-instance self-attention, with each over $K$ keypoint queries and one instance query, and (ii) $(K+1)$ same-type across-instance self-attention, each over $N$ queries of the same type. The resulting decoder removes the interaction among across-instance type-different queries, easing the optimization and thus improving the performance. Experimental results on MS COCO and CrowdPose show that our approach without human box supervision is superior to previous methods with complex decoders, and even is slightly better than ED-Pose that uses human box supervision. $\href{https://github.com/Michel-liu/GroupPose-Paddle}{\rm Paddle}$ and $\href{https://github.com/Michel-liu/GroupPose}{\rm PyTorch}$ code are available.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了端到端多人姿态估计问题。现有的解决方案大多采用DETR-like框架，主要是开发复杂的解码器，例如将姿态估计视为关键点框检测并与人体检测结合在ED-Pose中，或者在PETR中 hierarchically 预测姿态和关键点。我们提出了一种简单 yet 有效的 transformer 方法，名为 Group Pose。我们简单地认为 $K$-关键点姿态估计是预测 $N\times K$ 关键点位置，每个从关键点查询中预测，同时每个姿态被 Represented 为一个实例查询用于得分 $N$ 姿态预测。我们受到了关键点查询之间相互交互不直接有助于的想法，因此我们对解码器自注意的进行了简单修改。我们将单个自注意所有 $N\times(K+1)$ 查询被替换为两个顺序的组自注意：（i） $N$ 内部实例自注意，每个在 $K$ 关键点查询和一个实例查询之间进行自注意，和（ii） $(K+1)$ 同类 across-instance 自注意，每个在 $N$ 查询之间进行自注意。这些修改后的解码器可以减少不同类型的关键点查询之间的交互，从而简化优化，并提高性能。我们在 COCO 和 CrowdPose 上进行了实验，发现我们的方法无需人工盒子超级视觉是与前一代方法相比提高性能，甚至与使用人工盒子超级视觉的 ED-Pose 相比略有提高。我们在 $\href{https://github.com/Michel-liu/GroupPose-Paddle}{\rm Paddle}$ 和 $\href{https://github.com/Michel-liu/GroupPose}{\rm PyTorch}$ 上提供了代码。
</details></li>
</ul>
<hr>
<h2 id="A-Unified-Masked-Autoencoder-with-Patchified-Skeletons-for-Motion-Synthesis"><a href="#A-Unified-Masked-Autoencoder-with-Patchified-Skeletons-for-Motion-Synthesis" class="headerlink" title="A Unified Masked Autoencoder with Patchified Skeletons for Motion Synthesis"></a>A Unified Masked Autoencoder with Patchified Skeletons for Motion Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07301">http://arxiv.org/abs/2308.07301</a></li>
<li>repo_url: None</li>
<li>paper_authors: Esteve Valls Mascaro, Hyemin Ahn, Dongheui Lee</li>
<li>for: 这个论文的目的是提出一种新的任务无关的人体动作生成模型，即UNIMASK-M，可以有效地解决人体动作预测和填充中间姿势等问题。</li>
<li>methods: 这个模型使用了着重体部关系的架构，以及基于ViTs的人体姿势分解方法，以利用人体动作中的空间时间关系。此外，该模型还通过不同的面罩设计来进行姿势conditioned的动作生成。</li>
<li>results: 实验结果表明，UNIMASK-M模型在Human3.6M数据集上成功预测人体动作，并在LaFAN1数据集上实现了状态之最的动作填充结果，特别是在长距离转换期。更多信息可以查看项目官方网站：<a target="_blank" rel="noopener" href="https://sites.google.com/view/estevevallsmascaro/publications/unimask-m%E3%80%82">https://sites.google.com/view/estevevallsmascaro/publications/unimask-m。</a><details>
<summary>Abstract</summary>
The synthesis of human motion has traditionally been addressed through task-dependent models that focus on specific challenges, such as predicting future motions or filling in intermediate poses conditioned on known key-poses. In this paper, we present a novel task-independent model called UNIMASK-M, which can effectively address these challenges using a unified architecture. Our model obtains comparable or better performance than the state-of-the-art in each field. Inspired by Vision Transformers (ViTs), our UNIMASK-M model decomposes a human pose into body parts to leverage the spatio-temporal relationships existing in human motion. Moreover, we reformulate various pose-conditioned motion synthesis tasks as a reconstruction problem with different masking patterns given as input. By explicitly informing our model about the masked joints, our UNIMASK-M becomes more robust to occlusions. Experimental results show that our model successfully forecasts human motion on the Human3.6M dataset. Moreover, it achieves state-of-the-art results in motion inbetweening on the LaFAN1 dataset, particularly in long transition periods. More information can be found on the project website https://sites.google.com/view/estevevallsmascaro/publications/unimask-m.
</details>
<details>
<summary>摘要</summary>
历史上人体运动的合成总是通过任务 dependent 模型来解决，这些模型通常会专注于特定的挑战，例如预测未来运动或者使用知道的关键姿势来填充中间姿势。在这篇论文中，我们提出了一种新的任务独立的模型，即 UNIMASK-M，它可以有效地解决这些挑战。我们的模型在每个领域中都可以达到或更好的性能。我们的 UNIMASK-M 模型从人体 pose 中提取了身体部分，以利用人体运动中的空间时间关系。此外，我们将各种姿势conditioned 的运动合成任务重新表述为一个重建问题，输入不同的面纱模式。通过直接告诉我们模型关于遮盖的关节的信息，我们的 UNIMASK-M 模型变得更加鲁棒，更能够抵御遮挡。实验结果表明，我们的模型在 Human3.6M 数据集上预测人体运动成功。此外，它在 LaFAN1 数据集上的动作填充任务中达到了领先的成绩，特别是在长期跨过渡期内。更多信息可以在项目网站（https://sites.google.com/view/estevevallsmascaro/publications/unimask-m）上找到。
</details></li>
</ul>
<hr>
<h2 id="Accurate-Eye-Tracking-from-Dense-3D-Surface-Reconstructions-using-Single-Shot-Deflectometry"><a href="#Accurate-Eye-Tracking-from-Dense-3D-Surface-Reconstructions-using-Single-Shot-Deflectometry" class="headerlink" title="Accurate Eye Tracking from Dense 3D Surface Reconstructions using Single-Shot Deflectometry"></a>Accurate Eye Tracking from Dense 3D Surface Reconstructions using Single-Shot Deflectometry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07298">http://arxiv.org/abs/2308.07298</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiazhang Wang, Tianfu Wang, Bingjie Xu, Oliver Cossairt, Florian Willomitzer</li>
<li>for: 提高虚拟现实设备、神经科学研究和心理学中的眼动跟踪精度和速度。</li>
<li>methods: 基于单shotphasemeasuring-deflectometry（PMD）的新方法，通过获取肤色镜面上的密集3D表面信息来提高眼动跟踪精度和速度。</li>
<li>results: 实验表明，该方法可以实现眼动跟踪精度低于0.25度，至少比现有技术高出了$&gt;3300\times$。<details>
<summary>Abstract</summary>
Eye-tracking plays a crucial role in the development of virtual reality devices, neuroscience research, and psychology. Despite its significance in numerous applications, achieving an accurate, robust, and fast eye-tracking solution remains a considerable challenge for current state-of-the-art methods. While existing reflection-based techniques (e.g., "glint tracking") are considered the most accurate, their performance is limited by their reliance on sparse 3D surface data acquired solely from the cornea surface. In this paper, we rethink the way how specular reflections can be used for eye tracking: We propose a novel method for accurate and fast evaluation of the gaze direction that exploits teachings from single-shot phase-measuring-deflectometry (PMD). In contrast to state-of-the-art reflection-based methods, our method acquires dense 3D surface information of both cornea and sclera within only one single camera frame (single-shot). Improvements in acquired reflection surface points("glints") of factors $>3300 \times$ are easily achievable. We show the feasibility of our approach with experimentally evaluated gaze errors of only $\leq 0.25^\circ$ demonstrating a significant improvement over the current state-of-the-art.
</details>
<details>
<summary>摘要</summary>
眼动跟踪在虚拟现实设备的开发、 neuroscience 研究和心理学中扮演着关键性的角色。尽管它在多个应用程序中具有重要的作用，但是实现高度准确、可靠和快速的眼动跟踪解决方案仍然是当前技术的主要挑战。现有的反射基本技术（如“光泽跟踪”）被认为是最准确的，但它们的性能受到仅仅凭借硬件表面的辐射数据的限制。在这篇论文中，我们重新思考了如何使用折射来跟踪眼动：我们提出了一种新的方法，可以准确地和快速地评估眼动方向，这种方法利用了单 shot 相位测量折射（PMD）的教程。与现有的反射基本技术不同，我们的方法可以在单个摄像头帧中获得硬件表面的密集3D数据，包括辐射表面点的增加。我们实验证明，我们的方法可以在辐射表面点的增加比例上提高了$>3300\times$，并且我们实验证明了我们的方法的可行性，误差仅为$\leq 0.25^\circ$，这表明了我们的方法与当前技术的显著提高。
</details></li>
</ul>
<hr>
<h2 id="A-Robust-Approach-Towards-Distinguishing-Natural-and-Computer-Generated-Images-using-Multi-Colorspace-fused-and-Enriched-Vision-Transformer"><a href="#A-Robust-Approach-Towards-Distinguishing-Natural-and-Computer-Generated-Images-using-Multi-Colorspace-fused-and-Enriched-Vision-Transformer" class="headerlink" title="A Robust Approach Towards Distinguishing Natural and Computer Generated Images using Multi-Colorspace fused and Enriched Vision Transformer"></a>A Robust Approach Towards Distinguishing Natural and Computer Generated Images using Multi-Colorspace fused and Enriched Vision Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07279">http://arxiv.org/abs/2308.07279</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/manjaryp/mce-vit">https://github.com/manjaryp/mce-vit</a></li>
<li>paper_authors: Manjary P Gangan, Anoop Kadan, Lajish V L</li>
<li>for: 能够分辨 natura 和计算机生成的图像</li>
<li>methods: 使用两个视Transformers进行拟合，一个在 RGB 色域，另一个在 YCbCr 色域，并将两个拟合结果进行拟合</li>
<li>results: 提高了对计算机生成图像和 GAN 生成图像的分辨率，以及提高了对压缩、噪音等后处理图像的Robustness和普适性<details>
<summary>Abstract</summary>
The works in literature classifying natural and computer generated images are mostly designed as binary tasks either considering natural images versus computer graphics images only or natural images versus GAN generated images only, but not natural images versus both classes of the generated images. Also, even though this forensic classification task of distinguishing natural and computer generated images gets the support of the new convolutional neural networks and transformer based architectures that can give remarkable classification accuracies, they are seen to fail over the images that have undergone some post-processing operations usually performed to deceive the forensic algorithms, such as JPEG compression, gaussian noise, etc. This work proposes a robust approach towards distinguishing natural and computer generated images including both, computer graphics and GAN generated images using a fusion of two vision transformers where each of the transformer networks operates in different color spaces, one in RGB and the other in YCbCr color space. The proposed approach achieves high performance gain when compared to a set of baselines, and also achieves higher robustness and generalizability than the baselines. The features of the proposed model when visualized are seen to obtain higher separability for the classes than the input image features and the baseline features. This work also studies the attention map visualizations of the networks of the fused model and observes that the proposed methodology can capture more image information relevant to the forensic task of classifying natural and generated images.
</details>
<details>
<summary>摘要</summary>
文学类别自然和计算机生成的图像工作大都设计为二分类任务， Either considering natural images versus computer graphics images only or natural images versus GAN generated images only，但不是natural images versus both classes of generated images。 尽管这种审查类别任务可以通过新的 convolutional neural networks 和 transformer 基础架构得到惊人的分类精度，但它们在图像经过一些预处理操作后，如 JPEG 压缩、 Gaussian noise 等，会失败。 这个工作提议一种可靠的方法，用于分类自然和计算机生成的图像，包括计算机图形和 GAN 生成的图像，使用两个视transformer 网络，其中一个在 RGB 色空间中运行，另一个在 YCbCr 色空间中运行。 提议的方法在比较基eline 的情况下， achieve 高性能增加，同时也 achieve 更高的可靠性和普遍性。 图像特征视觉化时，可以看到提议的模型对类别之间的分离性更高，than the input image features 和基eline 的特征。 此外，研究提议的模型网络的注意力地图时，发现该方法可以更好地捕捉与审查任务相关的图像信息。
</details></li>
</ul>
<hr>
<h2 id="Diving-with-Penguins-Detecting-Penguins-and-their-Prey-in-Animal-borne-Underwater-Videos-via-Deep-Learning"><a href="#Diving-with-Penguins-Detecting-Penguins-and-their-Prey-in-Animal-borne-Underwater-Videos-via-Deep-Learning" class="headerlink" title="Diving with Penguins: Detecting Penguins and their Prey in Animal-borne Underwater Videos via Deep Learning"></a>Diving with Penguins: Detecting Penguins and their Prey in Animal-borne Underwater Videos via Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07267">http://arxiv.org/abs/2308.07267</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kejia Zhang, Mingyu Yang, Stephen D. J. Lang, Alistair M. McInnes, Richard B. Sherley, Tilo Burghardt</li>
<li>for: 这个论文的目的是提供一个可靠的海水下enguin检测器，以及一个鱼类检测器，并对enguin的捕食行为进行自动识别。</li>
<li>methods: 这个论文使用了现代生物学logging技术，并使用了深度学习系统来检测enguin和鱼类。</li>
<li>results: 这个论文提供了一个高度可靠的海水下enguin检测器，并对enguin的捕食行为进行了自动识别。但是，进一步的工作是必要的以使这种技术在实际场景中有用。<details>
<summary>Abstract</summary>
African penguins (Spheniscus demersus) are an endangered species. Little is known regarding their underwater hunting strategies and associated predation success rates, yet this is essential for guiding conservation. Modern bio-logging technology has the potential to provide valuable insights, but manually analysing large amounts of data from animal-borne video recorders (AVRs) is time-consuming. In this paper, we publish an animal-borne underwater video dataset of penguins and introduce a ready-to-deploy deep learning system capable of robustly detecting penguins (mAP50@98.0%) and also instances of fish (mAP50@73.3%). We note that the detectors benefit explicitly from air-bubble learning to improve accuracy. Extending this detector towards a dual-stream behaviour recognition network, we also provide the first results for identifying predation behaviour in penguin underwater videos. Whilst results are promising, further work is required for useful applicability of predation behaviour detection in field scenarios. In summary, we provide a highly reliable underwater penguin detector, a fish detector, and a valuable first attempt towards an automated visual detection of complex behaviours in a marine predator. We publish the networks, the DivingWithPenguins video dataset, annotations, splits, and weights for full reproducibility and immediate usability by practitioners.
</details>
<details>
<summary>摘要</summary>
非洲伯劳鸟（Spheniscus demersus）是一种濒临灭绝的物种。关于它们在水下猎食策略和相关的捕食成功率的知识很少，但这些信息对保护非常重要。现代生物 logging技术有potential提供有价值的洞察，但是手动分析动物携带视频记录器（AVR）上的大量数据非常时间consuming。在这篇论文中，我们发布了一个动物携带的水下视频数据集和一个准备就绪的深度学习系统，能够准确地检测伯劳鸟（mAP50@98.0%）和鱼雷（mAP50@73.3%）。我们发现检测器受到空气泡学习的帮助，以提高准确性。通过扩展这个检测器，我们还提供了第一次在水下伯劳鸟视频中自动识别捕食行为的结果。虽然结果有前途，但更多的工作是需要在实际场景中使用捕食行为检测。总之，我们提供了一个非常可靠的水下伯劳鸟检测器、鱼雷检测器和水下伯劳鸟视频中自动识别复杂行为的第一次尝试。我们发布了网络、DivingWithPenguins视频数据集、注释、分割和 weights，以便实现全 reproduceability和 immediate usability by practitioners。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Real-time-Smoke-Filtration-with-3D-LiDAR-for-Search-and-Rescue-with-Autonomous-Heterogeneous-Robotic-Systems"><a href="#Efficient-Real-time-Smoke-Filtration-with-3D-LiDAR-for-Search-and-Rescue-with-Autonomous-Heterogeneous-Robotic-Systems" class="headerlink" title="Efficient Real-time Smoke Filtration with 3D LiDAR for Search and Rescue with Autonomous Heterogeneous Robotic Systems"></a>Efficient Real-time Smoke Filtration with 3D LiDAR for Search and Rescue with Autonomous Heterogeneous Robotic Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07264">http://arxiv.org/abs/2308.07264</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Kyuroson, Anton Koval, George Nikolakopoulos</li>
<li>for: 提高机器人在具有烟尘的潜地环境中的自主导航和定位精度。</li>
<li>methods: 提出了一种模块化agnostic滤除管道，利用照度和空间信息进行烟尘排除，以提高点云检测的精度。</li>
<li>results: 对多个前沿探索任务进行了实验研究，并提供了对比其他方法的计算影响和安全自主导航的价值观。<details>
<summary>Abstract</summary>
Search and Rescue (SAR) missions in harsh and unstructured Sub-Terranean (Sub-T) environments in the presence of aerosol particles have recently become the main focus in the field of robotics. Aerosol particles such as smoke and dust directly affect the performance of any mobile robotic platform due to their reliance on their onboard perception systems for autonomous navigation and localization in Global Navigation Satellite System (GNSS)-denied environments. Although obstacle avoidance and object detection algorithms are robust to the presence of noise to some degree, their performance directly relies on the quality of captured data by onboard sensors such as Light Detection And Ranging (LiDAR) and camera. Thus, this paper proposes a novel modular agnostic filtration pipeline based on intensity and spatial information such as local point density for removal of detected smoke particles from Point Cloud (PCL) prior to its utilization for collision detection. Furthermore, the efficacy of the proposed framework in the presence of smoke during multiple frontier exploration missions is investigated while the experimental results are presented to facilitate comparison with other methodologies and their computational impact. This provides valuable insight to the research community for better utilization of filtration schemes based on available computation resources while considering the safe autonomous navigation of mobile robots.
</details>
<details>
<summary>摘要</summary>
寻找和救援（SAR）任务在恶劣和无结构的地壳环境中变得越来越重要，特别是在Global Navigation Satellite System（GNSS）被排除的环境中。由于移动 робот平台的自主导航和地点化依赖于其 бордов的感知系统，因此尘埃和烟雾直接影响移动 робот的性能。虽然障碍物避免和物体探测算法有一定的鲁棒性，但它们的性能直接取决于捕获到的数据质量，例如雷达和摄像头的数据。因此，这篇论文提出了一种新的模块不可识别的筛选管道，基于照度和空间信息，如地点密度，以去除从点云（PCL）中探测到的烟雾。此外，这篇论文还 investigate了在多个前沿探索任务中，提议的框架在烟雾存在下的效果，并对结果进行实验，以便与其他方法ologies和计算影响进行比较。这为研究者提供了有价值的反馈，以便更好地利用筛选方案，同时考虑移动 robot的自主导航安全性。
</details></li>
</ul>
<hr>
<h2 id="Large-kernel-Attention-for-Efficient-and-Robust-Brain-Lesion-Segmentation"><a href="#Large-kernel-Attention-for-Efficient-and-Robust-Brain-Lesion-Segmentation" class="headerlink" title="Large-kernel Attention for Efficient and Robust Brain Lesion Segmentation"></a>Large-kernel Attention for Efficient and Robust Brain Lesion Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07251">http://arxiv.org/abs/2308.07251</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liamchalcroft/mdunet">https://github.com/liamchalcroft/mdunet</a></li>
<li>paper_authors: Liam Chalcroft, Ruben Lourenço Pereira, Mikael Brudfors, Andrew S. Kayser, Mark D’Esposito, Cathy J. Price, Ioannis Pappas, John Ashburner</li>
<li>for: 这个论文主要用于提出一种基于 transformer 块的 U-Net 架构，用于三维脑损块分割。</li>
<li>methods: 该模型使用了一种混合 convolutional 和 transformer 块的 variant，用于模elling 长距离交互。</li>
<li>results: 研究表明，该模型在三维脑损块分割任务中提供了最佳的折衔点，即性能与当前状态体系相当，且参数效率与 CNN 相当，同时具有转化不变性的良好假设。<details>
<summary>Abstract</summary>
Vision transformers are effective deep learning models for vision tasks, including medical image segmentation. However, they lack efficiency and translational invariance, unlike convolutional neural networks (CNNs). To model long-range interactions in 3D brain lesion segmentation, we propose an all-convolutional transformer block variant of the U-Net architecture. We demonstrate that our model provides the greatest compromise in three factors: performance competitive with the state-of-the-art; parameter efficiency of a CNN; and the favourable inductive biases of a transformer. Our public implementation is available at https://github.com/liamchalcroft/MDUNet .
</details>
<details>
<summary>摘要</summary>
视transformer是深度学习模型，用于视觉任务，包括医学影像分割。然而，它缺乏效率和翻译不变性，与卷积神经网络（CNN）不同。为了模型3D脑损害分割中的长距离交互，我们提议一种alleviation transformer块变体的U-Net架构。我们示示了我们的模型提供了三个因素的最佳妥协：与状态之artefact的性能竞争; 参数效率与CNN相同; 以及转移器的有利 inductive bias。我们的公共实现可以在https://github.com/liamchalcroft/MDUNet上找到。
</details></li>
</ul>
<hr>
<h2 id="AAFACE-Attribute-aware-Attentional-Network-for-Face-Recognition"><a href="#AAFACE-Attribute-aware-Attentional-Network-for-Face-Recognition" class="headerlink" title="AAFACE: Attribute-aware Attentional Network for Face Recognition"></a>AAFACE: Attribute-aware Attentional Network for Face Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07243">http://arxiv.org/abs/2308.07243</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niloufar Alipour Talemi, Hossein Kashiani, Sahar Rahimi Malakshan, Mohammad Saeed Ebrahimi Saadabadi, Nima Najafzadeh, Mohammad Akyash, Nasser M. Nasrabadi</li>
<li>for: 这个论文是为了提出一种新的多分支神经网络，该网络同时进行软生物ometrics（SB）预测和人脸识别（FR）两个任务。</li>
<li>methods: 该网络使用SB特征来增强FR表示的推断能力。具体来说，我们提出了一个属性意识的集成（AAI）模块，该模块通过对FR与SB特征图进行Weighted集成来实现。AAI模块不仅具有完全上下文意识，还可以学习输入特征之间复杂的关系。</li>
<li>results: 我们的提出的网络在比较于现状的SB预测和FR方法上表现出了superiority。<details>
<summary>Abstract</summary>
In this paper, we present a new multi-branch neural network that simultaneously performs soft biometric (SB) prediction as an auxiliary modality and face recognition (FR) as the main task. Our proposed network named AAFace utilizes SB attributes to enhance the discriminative ability of FR representation. To achieve this goal, we propose an attribute-aware attentional integration (AAI) module to perform weighted integration of FR with SB feature maps. Our proposed AAI module is not only fully context-aware but also capable of learning complex relationships between input features by means of the sequential multi-scale channel and spatial sub-modules. Experimental results verify the superiority of our proposed network compared with the state-of-the-art (SoTA) SB prediction and FR methods.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出了一种新的多分支神经网络，该网络同时进行软生物特征（SB）预测作为辅助特征和人脸识别（FR）作为主要任务。我们提出的AAFace网络利用SB特征来增强FR表示的分类能力。为此，我们提出了一种属性意识权重整合（AAI）模块，以进行FR与SB特征地图的Weighted整合。我们的AAI模块不仅具有完整的上下文意识，还能够学习输入特征之间的复杂关系，通过纵向多尺度通道和空间子模块。实验结果证明了我们提出的网络的优越性，与当前最佳状态（SoTA）SB预测和FR方法相比。
</details></li>
</ul>
<hr>
<h2 id="UniWorld-Autonomous-Driving-Pre-training-via-World-Models"><a href="#UniWorld-Autonomous-Driving-Pre-training-via-World-Models" class="headerlink" title="UniWorld: Autonomous Driving Pre-training via World Models"></a>UniWorld: Autonomous Driving Pre-training via World Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07234">http://arxiv.org/abs/2308.07234</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chaytonmin/uniworld">https://github.com/chaytonmin/uniworld</a></li>
<li>paper_authors: Chen Min, Dawei Zhao, Liang Xiao, Yiming Nie, Bin Dai</li>
<li>for: This paper is written for those interested in developing world models for robots, specifically for autonomous driving.</li>
<li>methods: The paper proposes a unified pre-training framework called UniWorld, which uses a spatial-temporal world model to perceive the surroundings and predict the future behavior of other participants. The framework is based on Alberto Elfes’ pioneering work in 1989 and uses a label-free pre-training process to build a foundational model.</li>
<li>results: The proposed method demonstrates promising results in key tasks such as motion prediction, multi-camera 3D object detection, and surrounding semantic scene completion. Compared to monocular pre-training methods on the nuScenes dataset, UniWorld shows a significant improvement of about 1.5% in IoU for motion prediction, 2.0% in mAP and 2.0% in NDS for multi-camera 3D object detection, as well as a 3% increase in mIoU for surrounding semantic scene completion. Additionally, the method achieves a 25% reduction in 3D training annotation costs, offering significant practical value for real-world autonomous driving.<details>
<summary>Abstract</summary>
In this paper, we draw inspiration from Alberto Elfes' pioneering work in 1989, where he introduced the concept of the occupancy grid as World Models for robots. We imbue the robot with a spatial-temporal world model, termed UniWorld, to perceive its surroundings and predict the future behavior of other participants. UniWorld involves initially predicting 4D geometric occupancy as the World Models for foundational stage and subsequently fine-tuning on downstream tasks. UniWorld can estimate missing information concerning the world state and predict plausible future states of the world. Besides, UniWorld's pre-training process is label-free, enabling the utilization of massive amounts of image-LiDAR pairs to build a Foundational Model.The proposed unified pre-training framework demonstrates promising results in key tasks such as motion prediction, multi-camera 3D object detection, and surrounding semantic scene completion. When compared to monocular pre-training methods on the nuScenes dataset, UniWorld shows a significant improvement of about 1.5% in IoU for motion prediction, 2.0% in mAP and 2.0% in NDS for multi-camera 3D object detection, as well as a 3% increase in mIoU for surrounding semantic scene completion. By adopting our unified pre-training method, a 25% reduction in 3D training annotation costs can be achieved, offering significant practical value for the implementation of real-world autonomous driving. Codes are publicly available at https://github.com/chaytonmin/UniWorld.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们启发自阿尔伯托·艾尔法斯在1989年的开创性工作，其中提出了机器人世界模型的概念。我们为机器人提供了一个空间-时间世界模型，称之为UniWorld，以便理解它所处的环境和预测其他参与者的未来行为。UniWorld包括先预测4D几何占据的世界模型为基础阶段，然后细化到下游任务。UniWorld可以 estimte missing world state information和预测可能的未来世界状态。此外，UniWorld的预训练过程无需标签，可以使用巨量的图像-LiDAR对组建基础模型。提出的统一预训练框架在关键任务中表现出了可观的成果，比如运动预测、多摄像头3D物体检测和周围 semanticscene完成。与单摄像头预训练方法在nuScenes dataset上进行比较，UniWorld在运动预测、3D物体检测和semanticscene完成任务中显示出了约1.5%的 IoU提升、2.0%的 mAP提升和2.0%的 NDS提升。通过采用我们的统一预训练方法，可以降低3D训练注释成本的25%，提供了实际应用自动驾驶的重要实践价值。代码可以在https://github.com/chaytonmin/UniWorld中找到。
</details></li>
</ul>
<hr>
<h2 id="RestoreFormer-Towards-Real-World-Blind-Face-Restoration-from-Undegraded-Key-Value-Pairs"><a href="#RestoreFormer-Towards-Real-World-Blind-Face-Restoration-from-Undegraded-Key-Value-Pairs" class="headerlink" title="RestoreFormer++: Towards Real-World Blind Face Restoration from Undegraded Key-Value Pairs"></a>RestoreFormer++: Towards Real-World Blind Face Restoration from Undegraded Key-Value Pairs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07228">http://arxiv.org/abs/2308.07228</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhouxia Wang, Jiawei Zhang, Tianshui Chen, Wenping Wang, Ping Luo</li>
<li>for: 这个论文的目标是提高高质量的人脸图像从不知道的干扰中进行恢复。</li>
<li>methods: 该论文提出了RestoreFormer++,一种新的人脸图像恢复算法，它在一个手动注意力机制上模型了人脸图像的上下文信息，并在另一个扩展降低模型上帮助生成更加真实的降低图像，以增强对真实场景的适应性。</li>
<li>results: 对比当前算法，RestoreFormer++有多个优势，包括更高的真实性和质量，以及更好的适应性和泛化能力。<details>
<summary>Abstract</summary>
Blind face restoration aims at recovering high-quality face images from those with unknown degradations. Current algorithms mainly introduce priors to complement high-quality details and achieve impressive progress. However, most of these algorithms ignore abundant contextual information in the face and its interplay with the priors, leading to sub-optimal performance. Moreover, they pay less attention to the gap between the synthetic and real-world scenarios, limiting the robustness and generalization to real-world applications. In this work, we propose RestoreFormer++, which on the one hand introduces fully-spatial attention mechanisms to model the contextual information and the interplay with the priors, and on the other hand, explores an extending degrading model to help generate more realistic degraded face images to alleviate the synthetic-to-real-world gap. Compared with current algorithms, RestoreFormer++ has several crucial benefits. First, instead of using a multi-head self-attention mechanism like the traditional visual transformer, we introduce multi-head cross-attention over multi-scale features to fully explore spatial interactions between corrupted information and high-quality priors. In this way, it can facilitate RestoreFormer++ to restore face images with higher realness and fidelity. Second, in contrast to the recognition-oriented dictionary, we learn a reconstruction-oriented dictionary as priors, which contains more diverse high-quality facial details and better accords with the restoration target. Third, we introduce an extending degrading model that contains more realistic degraded scenarios for training data synthesizing, and thus helps to enhance the robustness and generalization of our RestoreFormer++ model. Extensive experiments show that RestoreFormer++ outperforms state-of-the-art algorithms on both synthetic and real-world datasets.
</details>
<details>
<summary>摘要</summary>
目标是从不知名的降低中恢复高质量的面孔图像。现有算法主要通过引入约束来补充高质量的细节，实现了很好的进步。然而，大多数这些算法忽略面孔中的丰富上下文信息和它们之间的互动，导致优化性不佳。另外，它们对实际世界应用场景的差异不够关注，限制了其robustness和泛化性。在这种情况下，我们提出了RestoreFormer++，它在一个方面引入了完全的空间注意力机制，以模型面孔中的上下文信息和约束之间的互动；另一方面，它探索了一种扩展降低模型，以帮助生成更真实的降低面孔图像，从而缓解实际世界和synthetic世界之间的差异。与现有算法相比，RestoreFormer++有几个重要优点。首先，不同于传统的视觉转换器，我们引入了多头跨度的cross-attention机制，以全面探索降低信息和高质量约束之间的空间互动，从而使RestoreFormer++能够更好地恢复面孔图像。第二，我们不是通过认知 oriented的字典来学习约束，而是通过恢复 oriented的字典来学习约束，这种字典包含更多的多样化的高质量 facial detail，更好地符合恢复目标。第三，我们引入了一种扩展降低模型，该模型包含更真实的降低场景，从而帮助提高RestoreFormer++模型的Robustness和泛化性。广泛的实验表明，RestoreFormer++在synthetic和实际世界数据上都能够超越现有的算法。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/15/cs.CV_2023_08_15/" data-id="clorjzl6800gsf188cp6k5op7" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_08_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/15/cs.AI_2023_08_15/" class="article-date">
  <time datetime="2023-08-15T12:00:00.000Z" itemprop="datePublished">2023-08-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/15/cs.AI_2023_08_15/">cs.AI - 2023-08-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="REFORMS-Reporting-Standards-for-Machine-Learning-Based-Science"><a href="#REFORMS-Reporting-Standards-for-Machine-Learning-Based-Science" class="headerlink" title="REFORMS: Reporting Standards for Machine Learning Based Science"></a>REFORMS: Reporting Standards for Machine Learning Based Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07832">http://arxiv.org/abs/2308.07832</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sayash Kapoor, Emily Cantrell, Kenny Peng, Thanh Hien Pham, Christopher A. Bail, Odd Erik Gundersen, Jake M. Hofman, Jessica Hullman, Michael A. Lones, Momin M. Malik, Priyanka Nanayakkara, Russell A. Poldrack, Inioluwa Deborah Raji, Michael Roberts, Matthew J. Salganik, Marta Serra-Garcia, Brandon M. Stewart, Gilles Vandewiele, Arvind Narayanan</li>
<li>for: 这篇论文的目的是提供机器学习（ML）基于科学研究的清晰报告标准。</li>
<li>methods: 这篇论文使用了一份名为REFORMS（Reporting Standards For Machine Learning Based Science）的检查列表，该列表包含32个问题和一对拥有的指南。REFORMS是基于19名研究者来自计算机科学、数据科学、数学、社会科学和医学等领域的共识而开发的。</li>
<li>results: 这篇论文提供了一个资源 для研究者在设计和实施研究时使用，以及为评审人员在审查论文时使用，以确保透明度和可重复性。<details>
<summary>Abstract</summary>
Machine learning (ML) methods are proliferating in scientific research. However, the adoption of these methods has been accompanied by failures of validity, reproducibility, and generalizability. These failures can hinder scientific progress, lead to false consensus around invalid claims, and undermine the credibility of ML-based science. ML methods are often applied and fail in similar ways across disciplines. Motivated by this observation, our goal is to provide clear reporting standards for ML-based science. Drawing from an extensive review of past literature, we present the REFORMS checklist ($\textbf{Re}$porting Standards $\textbf{For}$ $\textbf{M}$achine Learning Based $\textbf{S}$cience). It consists of 32 questions and a paired set of guidelines. REFORMS was developed based on a consensus of 19 researchers across computer science, data science, mathematics, social sciences, and biomedical sciences. REFORMS can serve as a resource for researchers when designing and implementing a study, for referees when reviewing papers, and for journals when enforcing standards for transparency and reproducibility.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Tightest-Admissible-Shortest-Path"><a href="#Tightest-Admissible-Shortest-Path" class="headerlink" title="Tightest Admissible Shortest Path"></a>Tightest Admissible Shortest Path</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08453">http://arxiv.org/abs/2308.08453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eyal Weiss, Ariel Felner, Gal A. Kaminka</li>
<li>for: 解决Weighted Directed Graphs中的短est path问题，考虑edge-weight计算时间和不确定性的影响。</li>
<li>methods: 基于 generalized framework的提议，引入紧跟最优路径问题（TASP），解决在 bounded uncertainty 下的短est path问题，通过质量保证来提供解决方案。</li>
<li>results: 提出了一种完整的算法，并提供了解决方案的质量保证，验证结果表明该方法的有效性。<details>
<summary>Abstract</summary>
The shortest path problem in graphs is fundamental to AI. Nearly all variants of the problem and relevant algorithms that solve them ignore edge-weight computation time and its common relation to weight uncertainty. This implies that taking these factors into consideration can potentially lead to a performance boost in relevant applications. Recently, a generalized framework for weighted directed graphs was suggested, where edge-weight can be computed (estimated) multiple times, at increasing accuracy and run-time expense. We build on this framework to introduce the problem of finding the tightest admissible shortest path (TASP); a path with the tightest suboptimality bound on the optimal cost. This is a generalization of the shortest path problem to bounded uncertainty, where edge-weight uncertainty can be traded for computational cost. We present a complete algorithm for solving TASP, with guarantees on solution quality. Empirical evaluation supports the effectiveness of this approach.
</details>
<details>
<summary>摘要</summary>
<SYS>将文本翻译成简化字符串。</SYS>图形中的最短路径问题是人工智能的基础问题之一。大多数变体的问题和解决方案忽略了边Weight计算时间和Weight不确定性之间的通常关系。这意味着考虑这些因素可能会导致应用中的性能提升。最近，一种总结框架 для权重有向图被建议，其中边Weight可以在不同的精度和计算成本下重复计算。我们在这个框架之上引入了找到最紧张的可接受路径（TASP）问题，这是一种对于不确定性 bounded 的扩展，可以通过计算成本来贸易边Weight uncertainty。我们提出了一个完整的解决TASP问题的算法，并提供了解决方案质量的保证。实验证明了这种方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Identify-Critical-States-for-Reinforcement-Learning-from-Videos"><a href="#Learning-to-Identify-Critical-States-for-Reinforcement-Learning-from-Videos" class="headerlink" title="Learning to Identify Critical States for Reinforcement Learning from Videos"></a>Learning to Identify Critical States for Reinforcement Learning from Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07795">http://arxiv.org/abs/2308.07795</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai-initiative-kaust/videorlcs">https://github.com/ai-initiative-kaust/videorlcs</a></li>
<li>paper_authors: Haozhe Liu, Mingchen Zhuge, Bing Li, Yuhui Wang, Francesco Faccio, Bernard Ghanem, Jürgen Schmidhuber</li>
<li>for: 本研究的目的是利用视频数据提取深度强化学习中的有用策略信息，不需要明确的动作信息。</li>
<li>methods: 该方法使用视频编码的集集数据，通过深度学习预测回报，然后使用面积基于的敏感分析提取重要的关键状态。</li>
<li>results: 广泛的实验显示，该方法可以理解和改进代理行为。代码和生成的数据集可以在 GitHub 上找到。<details>
<summary>Abstract</summary>
Recent work on deep reinforcement learning (DRL) has pointed out that algorithmic information about good policies can be extracted from offline data which lack explicit information about executed actions. For example, videos of humans or robots may convey a lot of implicit information about rewarding action sequences, but a DRL machine that wants to profit from watching such videos must first learn by itself to identify and recognize relevant states/actions/rewards. Without relying on ground-truth annotations, our new method called Deep State Identifier learns to predict returns from episodes encoded as videos. Then it uses a kind of mask-based sensitivity analysis to extract/identify important critical states. Extensive experiments showcase our method's potential for understanding and improving agent behavior. The source code and the generated datasets are available at https://github.com/AI-Initiative-KAUST/VideoRLCS.
</details>
<details>
<summary>摘要</summary>
近期深度强化学习（DRL）的研究表明，可以从没有明确行动信息的线上数据中提取良好策略的算法信息。例如，人类或机器人视频可以传递大量的隐式信息关于奖励行动序列，但一个DRL机器人想要从这些视频中获益，首先必须自己学习 identificifying和识别相关的状态/行动/奖励。无需基于真实标注，我们的新方法called Deep State Identifier可以预测episode编码为视频中的返回。然后使用一种mask-based敏感分析来提取/识别重要的关键状态。广泛的实验表明了我们方法的可能性 для理解和改进代理行为。源代码和生成的数据集可以在https://github.com/AI-Initiative-KAUST/VideoRLCS中下载。
</details></li>
</ul>
<hr>
<h2 id="Implementing-Quantum-Generative-Adversarial-Network-qGAN-and-QCBM-in-Finance"><a href="#Implementing-Quantum-Generative-Adversarial-Network-qGAN-and-QCBM-in-Finance" class="headerlink" title="Implementing Quantum Generative Adversarial Network (qGAN) and QCBM in Finance"></a>Implementing Quantum Generative Adversarial Network (qGAN) and QCBM in Finance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08448">http://arxiv.org/abs/2308.08448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Santanu Ganguly</li>
<li>for: 本研究探讨了应用量子机器学习（QML）在金融领域的新热点研究领域，包括股票价格预测、资产风险管理和评估等。</li>
<li>methods: 本研究使用了真实的金融数据集和模拟环境，对量子机器学习（QML）模型进行比较，包括qGAN（量子生成对抗网络）和QCBM（量子环境生成机器）等模型。</li>
<li>results: 研究发现，量子机器学习（QML）在金融领域可以提供未来的量子优势，特别是在股票价格预测和资产风险管理等领域。<details>
<summary>Abstract</summary>
Quantum machine learning (QML) is a cross-disciplinary subject made up of two of the most exciting research areas: quantum computing and classical machine learning (ML), with ML and artificial intelligence (AI) being projected as the first fields that will be impacted by the rise of quantum machines. Quantum computers are being used today in drug discovery, material & molecular modelling and finance. In this work, we discuss some upcoming active new research areas in application of quantum machine learning (QML) in finance. We discuss certain QML models that has become areas of active interest in the financial world for various applications. We use real world financial dataset and compare models such as qGAN (quantum generative adversarial networks) and QCBM (quantum circuit Born machine) among others, using simulated environments. For the qGAN, we define quantum circuits for discriminators and generators and show promises of future quantum advantage via QML in finance.
</details>
<details>
<summary>摘要</summary>
量子机器学习（QML）是两个最有前途的研究领域之间的跨学科领域：量子计算和经典机器学习（ML）， Machine learning和人工智能（AI）被predict为第一个受到量子机器的影响的领域。量子计算机在今天的药物发现、物质和分子模型以及金融领域中使用。在这项工作中，我们讨论了在金融领域中应用量子机器学习（QML）的未来活跃研究领域。我们讨论了一些在金融世界中受到关注的QML模型，并使用实际的金融数据进行比较。我们使用 simulated environments 来评估 qGAN 和 QCBM 等模型，并显示了未来量子优势的推荐。Note: Please note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Informed-Named-Entity-Recognition-Decoding-for-Generative-Language-Models"><a href="#Informed-Named-Entity-Recognition-Decoding-for-Generative-Language-Models" class="headerlink" title="Informed Named Entity Recognition Decoding for Generative Language Models"></a>Informed Named Entity Recognition Decoding for Generative Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07791">http://arxiv.org/abs/2308.07791</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tobias Deußer, Lars Hillebrand, Christian Bauckhage, Rafet Sifa</li>
<li>for: 这个论文主要是为了提高命名实体识别（Named Entity Recognition，NER）的性能。</li>
<li>methods: 这篇论文提出了一种简单 yet effective的方法，即 Informed Named Entity Recognition Decoding（iNERD），它将命名实体识别视为一种生成过程，利用了最新的生成模型的语言理解能力，并采用了了一种有知识的解码方案，以便更好地处理有限的信息抽取任务。</li>
<li>results: 论文在使用五种生成语言模型，测试在八个命名实体识别 datasets 上，得到了很出色的结果，特别是在未知实体类型集合的环境下，这说明了该方法的适应性。<details>
<summary>Abstract</summary>
Ever-larger language models with ever-increasing capabilities are by now well-established text processing tools. Alas, information extraction tasks such as named entity recognition are still largely unaffected by this progress as they are primarily based on the previous generation of encoder-only transformer models. Here, we propose a simple yet effective approach, Informed Named Entity Recognition Decoding (iNERD), which treats named entity recognition as a generative process. It leverages the language understanding capabilities of recent generative models in a future-proof manner and employs an informed decoding scheme incorporating the restricted nature of information extraction into open-ended text generation, improving performance and eliminating any risk of hallucinations. We coarse-tune our model on a merged named entity corpus to strengthen its performance, evaluate five generative language models on eight named entity recognition datasets, and achieve remarkable results, especially in an environment with an unknown entity class set, demonstrating the adaptability of the approach.
</details>
<details>
<summary>摘要</summary>
现代语言模型在功能上不断提高，成为文本处理工具的标准配置。可是，信息提取任务，如命名实体识别，仍然受到这些进步的影响很少，因为它们主要基于上一代encoder-only transformer模型。在这里，我们提出了一种简单 yet有效的方法，命名实体识别生成（iNERD），它将命名实体识别视为生成过程。它利用最新的生成模型对语言理解能力的提高，并采用了有知识的编码方案，将开放式文本生成和信息提取的限制纳入考虑，从而提高性能并消除所有的幻觉。我们在合并的命名实体 корпу斯上粗略调整我们的模型，以强化其表现，并评估了五种生成语言模型在八个命名实体识别 datasets 上的表现，取得了非常出色的结果，特别是在未知实体类集的环境中，这表明了方法的适应性。
</details></li>
</ul>
<hr>
<h2 id="Do-We-Fully-Understand-Students’-Knowledge-States-Identifying-and-Mitigating-Answer-Bias-in-Knowledge-Tracing"><a href="#Do-We-Fully-Understand-Students’-Knowledge-States-Identifying-and-Mitigating-Answer-Bias-in-Knowledge-Tracing" class="headerlink" title="Do We Fully Understand Students’ Knowledge States? Identifying and Mitigating Answer Bias in Knowledge Tracing"></a>Do We Fully Understand Students’ Knowledge States? Identifying and Mitigating Answer Bias in Knowledge Tracing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07779">http://arxiv.org/abs/2308.07779</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lucky7-code/core">https://github.com/lucky7-code/core</a></li>
<li>paper_authors: Chaoran Cui, Hebo Ma, Chen Zhang, Chunyun Zhang, Yumo Yao, Meng Chen, Yuling Ma</li>
<li>for: 这 paper 的目的是解决知识追踪 (KT) 中存在的答案偏见问题，以便更好地理解学生们的知识状态。</li>
<li>methods: 这 paper 使用了 causality 理论来解决 KT 中的答案偏见问题，并提出了一种 COunterfactual REasoning (CORE) 框架来减少答案偏见的影响。</li>
<li>results: 这 paper 的实验结果表明，CORE 框架可以减少 KT 中答案偏见的影响，并且可以与现有的多种 KT 模型结合使用。<details>
<summary>Abstract</summary>
Knowledge tracing (KT) aims to monitor students' evolving knowledge states through their learning interactions with concept-related questions, and can be indirectly evaluated by predicting how students will perform on future questions. In this paper, we observe that there is a common phenomenon of answer bias, i.e., a highly unbalanced distribution of correct and incorrect answers for each question. Existing models tend to memorize the answer bias as a shortcut for achieving high prediction performance in KT, thereby failing to fully understand students' knowledge states. To address this issue, we approach the KT task from a causality perspective. A causal graph of KT is first established, from which we identify that the impact of answer bias lies in the direct causal effect of questions on students' responses. A novel COunterfactual REasoning (CORE) framework for KT is further proposed, which separately captures the total causal effect and direct causal effect during training, and mitigates answer bias by subtracting the latter from the former in testing. The CORE framework is applicable to various existing KT models, and we implement it based on the prevailing DKT, DKVMN, and AKT models, respectively. Extensive experiments on three benchmark datasets demonstrate the effectiveness of CORE in making the debiased inference for KT.
</details>
<details>
<summary>摘要</summary>
知识跟踪（KT）目的是监测学生在学习过程中知识状态的变化，通过问题相关的问题来评估学生的知识水平，并可以通过预测未来问题的回答来间接评估。在这篇论文中，我们发现了一种常见的答案偏见现象，即每个问题的答案准确率和错误率存在极大的偏见。现有的模型通常会借助答案偏见作为短cut来实现高度预测性能，从而忽略了学生的知识状态。为解决这问题，我们从 causality 角度对 KT 进行了研究。首先，我们从 KT 问题中构建了一个 causal 图，并发现了答案偏见对学生回答的直接 causal 效应。基于这个 causal 图，我们提出了一种新的 COunterfactual REasoning（CORE）框架，它在训练时分别捕捉总 causal 效应和直接 causal 效应，并在测试时对答案偏见进行补做，以确保debias 的推理。CORE 框架可以应用于多种现有 KT 模型，我们在 DKT、DKVMN 和 AKT 模型上实现了它。我们在三个 benchmark 数据集上进行了广泛的实验，并证明了 CORE 在 KT 中的有效性。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-generative-modelling-for-autonomous-robots"><a href="#Hierarchical-generative-modelling-for-autonomous-robots" class="headerlink" title="Hierarchical generative modelling for autonomous robots"></a>Hierarchical generative modelling for autonomous robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07775">http://arxiv.org/abs/2308.07775</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Yuan, Noor Sajid, Karl Friston, Zhibin Li</li>
<li>for: 这个论文旨在研究人类在与环境交互时如何生成复杂全身运动，以便在自主机器人操作中实现高效的目标完成。</li>
<li>methods: 这篇论文使用了层次生成模型，包括多级规划和自动控制，来模拟人类动作控制的深度时间架构。</li>
<li>results: 通过数字和物理实验，这篇论文证明了使用人类动作控制算法可以实现自主机器人完成复杂任务，例如抓取和运输箱子、穿过门户、踢足球等，并在身体损伤和地面不平的情况下保持稳定性。<details>
<summary>Abstract</summary>
Humans can produce complex whole-body motions when interacting with their surroundings, by planning, executing and combining individual limb movements. We investigated this fundamental aspect of motor control in the setting of autonomous robotic operations. We approach this problem by hierarchical generative modelling equipped with multi-level planning-for autonomous task completion-that mimics the deep temporal architecture of human motor control. Here, temporal depth refers to the nested time scales at which successive levels of a forward or generative model unfold, for example, delivering an object requires a global plan to contextualise the fast coordination of multiple local movements of limbs. This separation of temporal scales also motivates robotics and control. Specifically, to achieve versatile sensorimotor control, it is advantageous to hierarchically structure the planning and low-level motor control of individual limbs. We use numerical and physical simulation to conduct experiments and to establish the efficacy of this formulation. Using a hierarchical generative model, we show how a humanoid robot can autonomously complete a complex task that necessitates a holistic use of locomotion, manipulation, and grasping. Specifically, we demonstrate the ability of a humanoid robot that can retrieve and transport a box, open and walk through a door to reach the destination, approach and kick a football, while showing robust performance in presence of body damage and ground irregularities. Our findings demonstrated the effectiveness of using human-inspired motor control algorithms, and our method provides a viable hierarchical architecture for the autonomous completion of challenging goal-directed tasks.
</details>
<details>
<summary>摘要</summary>
人类可以生成复杂全身运动when interacting with其 surroundings，通过规划、执行和组合各个肢体运动。我们在自主 роботизирован操作的设置下调查了这一基本的 дви作控制问题。我们采用层次生成模型，带有多级规划，以模仿人类 дви作控制的深度时间建筑。在这里，时间深度指的是成功级别模型在不同时间层次上进行的嵌套执行，例如，为了交付物品，需要一个全局规划，以Contextualize the rapid coordination of multiple local limb movements。这种时间层次分离也驱动了机器人和控制。特别是，为了实现多元感知motor控制，是在层次结构的规划和低级motor控制中进行分离。我们使用数字和物理 simulate experiments to verify the effectiveness of this approach.使用层次生成模型，我们展示了一个人工智能机器人可以自主完成一个复杂任务，需要整体使用 locomotion、抓取和抓取。例如，我们示出了一个人工智能机器人可以拾取和运送一个盒子，通过门way，然后踢过一个足球，并在存在身体损伤和地面不平的情况下表现稳定。我们的发现表明了使用人类 inspirational motor control算法的有效性，而我们的方法提供了一个可靠的层次建筑，用于自主完成具有挑战性的目标导向任务。
</details></li>
</ul>
<hr>
<h2 id="A-Graph-Encoder-Decoder-Network-for-Unsupervised-Anomaly-Detection"><a href="#A-Graph-Encoder-Decoder-Network-for-Unsupervised-Anomaly-Detection" class="headerlink" title="A Graph Encoder-Decoder Network for Unsupervised Anomaly Detection"></a>A Graph Encoder-Decoder Network for Unsupervised Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07774">http://arxiv.org/abs/2308.07774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahsa Mesgaran, A. Ben Hamza</li>
<li>for: 检测图像中异常节点</li>
<li>methods: 使用无监督图像编码器-解码器模型，学习异常分数函数对节点进行排序，并使用本地性受限的线性编码方法来找到异常分数矩阵</li>
<li>results: 在六个基准数据集上使用多种评价指标进行实验，结果显示该方法在异常检测方面具有优异性，比之前的方法更高效和可靠。<details>
<summary>Abstract</summary>
A key component of many graph neural networks (GNNs) is the pooling operation, which seeks to reduce the size of a graph while preserving important structural information. However, most existing graph pooling strategies rely on an assignment matrix obtained by employing a GNN layer, which is characterized by trainable parameters, often leading to significant computational complexity and a lack of interpretability in the pooling process. In this paper, we propose an unsupervised graph encoder-decoder model to detect abnormal nodes from graphs by learning an anomaly scoring function to rank nodes based on their degree of abnormality. In the encoding stage, we design a novel pooling mechanism, named LCPool, which leverages locality-constrained linear coding for feature encoding to find a cluster assignment matrix by solving a least-squares optimization problem with a locality regularization term. By enforcing locality constraints during the coding process, LCPool is designed to be free from learnable parameters, capable of efficiently handling large graphs, and can effectively generate a coarser graph representation while retaining the most significant structural characteristics of the graph. In the decoding stage, we propose an unpooling operation, called LCUnpool, to reconstruct both the structure and nodal features of the original graph. We conduct empirical evaluations of our method on six benchmark datasets using several evaluation metrics, and the results demonstrate its superiority over state-of-the-art anomaly detection approaches.
</details>
<details>
<summary>摘要</summary>
Many graph neural networks (GNNs) 的关键组件是聚合操作，该操作的目标是将图的大小减小，保留重要的结构信息。然而，大多数现有的图聚合策略都是基于使用 GNN 层获得的分配矩阵，这些矩阵通常具有可学习参数，导致计算复杂性很高并且解释性差。在这篇论文中，我们提出了一种无监督的图编码器-解码器模型，用于从图中检测异常节点。在编码阶段，我们设计了一种新的聚合机制，名为 LCPool，它利用了本地化的线性编码来找到一个归一化矩阵，通过解决一个最小二乘优化问题来实现。通过在编码过程中强制实施本地化约束，LCPool 设计为无学习参数，能够高效处理大图，并能够生成一个粗略的图表示，保留原图的最重要的结构特征。在解码阶段，我们提出了一种解聚机制，名为 LCUnpool，用于重建原始图的结构和节点特征。我们对六个标准数据集进行了实验评估，并通过多个评价指标证明了我们的方法的优越性。
</details></li>
</ul>
<hr>
<h2 id="MOLE-MOdular-Learning-FramEwork-via-Mutual-Information-Maximization"><a href="#MOLE-MOdular-Learning-FramEwork-via-Mutual-Information-Maximization" class="headerlink" title="MOLE: MOdular Learning FramEwork via Mutual Information Maximization"></a>MOLE: MOdular Learning FramEwork via Mutual Information Maximization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07772">http://arxiv.org/abs/2308.07772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianchao Li, Yulong Pei</li>
<li>for: 这个论文旨在介绍一种异步本地学习框架，即Modular Learning Framework (MOLE)，用于神经网络。</li>
<li>methods: 这个框架通过层次归一化神经网络，定义每个模块的训练目标为相互信息增大，然后逐次训练每个模块以增大相互信息。</li>
<li>results: 实验表明，MOLE可以解决不同类型的数据，包括向量、网格和图数据。此外，MOLE还可以解决图数据上的节点级和图级任务。因此，MOLE已经在实验上证明是对不同类型数据的通用解决方案。<details>
<summary>Abstract</summary>
This paper is to introduce an asynchronous and local learning framework for neural networks, named Modular Learning Framework (MOLE). This framework modularizes neural networks by layers, defines the training objective via mutual information for each module, and sequentially trains each module by mutual information maximization. MOLE makes the training become local optimization with gradient-isolated across modules, and this scheme is more biologically plausible than BP. We run experiments on vector-, grid- and graph-type data. In particular, this framework is capable of solving both graph- and node-level tasks for graph-type data. Therefore, MOLE has been experimentally proven to be universally applicable to different types of data.
</details>
<details>
<summary>摘要</summary>
这份论文旨在介绍一种异步本地学习框架，名为模块学习框架（MOLE）。这个框架将神经网络归一化为层，通过互信息定义每个模块的训练目标，并逐渐训练每个模块以互信息最大化。MOLE使得训练变成了本地优化，梯度归一化在模块之间，这种方法更加生物学可靠性高于bp。我们在向量-, 网格-和图型数据上进行了实验，并证明MOLE可以解决图型数据上的图级和节点级任务。因此，MOLE已经实验证明对不同类型的数据都是通用的。
</details></li>
</ul>
<hr>
<h2 id="NeFL-Nested-Federated-Learning-for-Heterogeneous-Clients"><a href="#NeFL-Nested-Federated-Learning-for-Heterogeneous-Clients" class="headerlink" title="NeFL: Nested Federated Learning for Heterogeneous Clients"></a>NeFL: Nested Federated Learning for Heterogeneous Clients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07761">http://arxiv.org/abs/2308.07761</a></li>
<li>repo_url: None</li>
<li>paper_authors: Honggu Kang, Seohyeon Cha, Jinwoo Shin, Jongmyeong Lee, Joonhyuk Kang</li>
<li>For: The paper is written for discussing the issue of slow or incapable clients in federated learning (FL) and proposing a new framework called nested federated learning (NeFL) to address this issue.* Methods: The paper uses a generalized framework that efficiently divides a model into submodels using both depthwise and widthwise scaling, and interprets models as solving ordinary differential equations (ODEs) with adaptive step sizes.* Results: The paper demonstrates that NeFL leads to significant gains, especially for the worst-case submodel, and aligns with recent studies in FL. Specifically, the paper shows an improvement of 8.33 on CIFAR-10.<details>
<summary>Abstract</summary>
Federated learning (FL) is a promising approach in distributed learning keeping privacy. However, during the training pipeline of FL, slow or incapable clients (i.e., stragglers) slow down the total training time and degrade performance. System heterogeneity, including heterogeneous computing and network bandwidth, has been addressed to mitigate the impact of stragglers. Previous studies split models to tackle the issue, but with less degree-of-freedom in terms of model architecture. We propose nested federated learning (NeFL), a generalized framework that efficiently divides a model into submodels using both depthwise and widthwise scaling. NeFL is implemented by interpreting models as solving ordinary differential equations (ODEs) with adaptive step sizes. To address the inconsistency that arises when training multiple submodels with different architecture, we decouple a few parameters. NeFL enables resource-constrained clients to effectively join the FL pipeline and the model to be trained with a larger amount of data. Through a series of experiments, we demonstrate that NeFL leads to significant gains, especially for the worst-case submodel (e.g., 8.33 improvement on CIFAR-10). Furthermore, we demonstrate NeFL aligns with recent studies in FL.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种有前途的方法，它可以保持隐私性而在分布式学习中进行训练。然而，在 FL 的训练管道中，慢速或无力的客户端（即延迟者）会降低总训练时间和性能。系统不同性，包括不同的计算和网络带宽，已经得到了 Mitigate 的注意。先前的研究把模型分成了解决这个问题，但是它们具有较少的度量自由度，即模型体系结构。我们提出了嵌入式联邦学习（NeFL），一种总体化的框架，它可以高效地将模型分成子模型使用深度和宽度的扩展。NeFL 通过解释模型为解决常微分方程（ODEs）的解释，并使用适应步长来实现。为了解决多个子模型不同体系结构时出现的不一致性，我们划分了一些参数。NeFL 让资源受限的客户端可以有效地参与 FL 管道，并让模型在更大的数据量上进行训练。通过一系列实验，我们表明了 NeFL 带来了显著的改善，特别是最差的子模型（例如， CIFAR-10 上的 8.33 提高）。此外，我们还证明了 NeFL 与最近的 FL 研究相一致。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Embedding-Size-Search-with-Minimum-Regret-for-Streaming-Recommender-System"><a href="#Dynamic-Embedding-Size-Search-with-Minimum-Regret-for-Streaming-Recommender-System" class="headerlink" title="Dynamic Embedding Size Search with Minimum Regret for Streaming Recommender System"></a>Dynamic Embedding Size Search with Minimum Regret for Streaming Recommender System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07760">http://arxiv.org/abs/2308.07760</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hebowei2000/DESS">https://github.com/hebowei2000/DESS</a></li>
<li>paper_authors: Bowei He, Xu He, Renrui Zhang, Yingxue Zhang, Ruiming Tang, Chen Ma</li>
<li>for: 寻找适合不断增长的用户和项目的流动推荐系统，以适应 dynamically changing environments。</li>
<li>methods: 模型更新过程中采用了流动模型更新策略，并将 embedding layer 的大小设置为动态变量，以提高推荐性能和减少内存成本。</li>
<li>results: 对两个推荐任务中的四个公共数据集进行了实验，证明了我们的方法可以在流动环境下提供更好的推荐性能，同时具有更低的内存成本和更高的时间效率。<details>
<summary>Abstract</summary>
With the continuous increase of users and items, conventional recommender systems trained on static datasets can hardly adapt to changing environments. The high-throughput data requires the model to be updated in a timely manner for capturing the user interest dynamics, which leads to the emergence of streaming recommender systems. Due to the prevalence of deep learning-based recommender systems, the embedding layer is widely adopted to represent the characteristics of users, items, and other features in low-dimensional vectors. However, it has been proved that setting an identical and static embedding size is sub-optimal in terms of recommendation performance and memory cost, especially for streaming recommendations. To tackle this problem, we first rethink the streaming model update process and model the dynamic embedding size search as a bandit problem. Then, we analyze and quantify the factors that influence the optimal embedding sizes from the statistics perspective. Based on this, we propose the \textbf{D}ynamic \textbf{E}mbedding \textbf{S}ize \textbf{S}earch (\textbf{DESS}) method to minimize the embedding size selection regret on both user and item sides in a non-stationary manner. Theoretically, we obtain a sublinear regret upper bound superior to previous methods. Empirical results across two recommendation tasks on four public datasets also demonstrate that our approach can achieve better streaming recommendation performance with lower memory cost and higher time efficiency.
</details>
<details>
<summary>摘要</summary>
随着用户和项目的增加，传统的推荐系统通常采用静态数据集训练，但这些系统在变化的环境中难以适应。高 Throughput 数据需要模型在有效时间内进行更新，以捕捉用户兴趣动态，这导致了流处理推荐系统的出现。由于深度学习基本推荐系统的普遍性，嵌入层广泛采用低维度向量表示用户、项目和其他特征的特征。但是，已经证明将嵌入层大小设置为静态和共同的是优化推荐性和内存成本的不佳选择，特别是在流处理推荐中。为解决这个问题，我们首先重新思考流处理模型更新过程，并将动态嵌入大小搜索视为一个bandit问题。然后，我们分析和量化影响优化嵌入大小的因素，并基于这些因素提出了\textbf{D}ynamic \textbf{E}mbedding \textbf{S}ize \textbf{S}earch (\textbf{DESS})方法，以最小化嵌入大小选择 regret 在用户和项目两个方面。理论上，我们获得了superior于之前方法的下线 regret upper bound。实验结果在四个公共数据集上的两个推荐任务中也表明，我们的方法可以在不同的环境下实现更好的流处理推荐性，同时具有较低的内存成本和更高的时间效率。
</details></li>
</ul>
<hr>
<h2 id="Forward-Backward-Reasoning-in-Large-Language-Models-for-Verification"><a href="#Forward-Backward-Reasoning-in-Large-Language-Models-for-Verification" class="headerlink" title="Forward-Backward Reasoning in Large Language Models for Verification"></a>Forward-Backward Reasoning in Large Language Models for Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07758">http://arxiv.org/abs/2308.07758</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weisen Jiang, Han Shi, Longhui Yu, Zhengying Liu, Yu Zhang, Zhenguo Li, James T. Kwok</li>
<li>for: 本研究旨在提高开端问题 answering 的能力，提出了一种基于 Self-Consistency 和 backwards reasoning 的方法。</li>
<li>methods: 本方法使用了 Self-Consistency  sampling 一些可能的 reasoning chains，并使用了 backwards reasoning 来验证 candidate answers。</li>
<li>results: 实验结果表明，FOBAR 可以在六个数据集和三个 LLMS 上达到开端问题 answering 的state-of-the-art性能。<details>
<summary>Abstract</summary>
Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \textit{a simple template}, i.e., "\textit{\textbf{If we know the answer of the above question is \{a candidate answer\}, what is the value of unknown variable ${\bf x}$?}" Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three LLMs. Experimental results demonstrate that FOBAR achieves state-of-the-art performance on various reasoning benchmarks.
</details>
<details>
<summary>摘要</summary>
Chain-of-Though (CoT) 提示法在不同的理解任务中表现出色。自Consistency \citep{wang2023selfconsistency} 提议采样多种不同的理解链，以便通过不同的答案来选择最佳答案。在这篇论文中，我们提出了一种使用反向理解的新方法，用于验证候选答案。我们将问题中的一个token用{\bf x}来mask，然后问LLM predict这个masked token，当提供了一个简单的模板，即 "\textit{\textbf{如果我们知道上面的问题的答案是\{一个候选答案\},则unknown变量{\bf x}的值是什么？}"。Intuitively，LLM是预计能够成功预测masked token，如果提供的候选答案是正确的。我们还提出了FOBAR来组合前向和反向理解来估计候选答案的概率。我们在六个数据集和三个LLM上进行了广泛的实验，实验结果表明，FOBAR在多种理解 bencmarks 上达到了当前最佳性能。
</details></li>
</ul>
<hr>
<h2 id="Dancing-Avatar-Pose-and-Text-Guided-Human-Motion-Videos-Synthesis-with-Image-Diffusion-Model"><a href="#Dancing-Avatar-Pose-and-Text-Guided-Human-Motion-Videos-Synthesis-with-Image-Diffusion-Model" class="headerlink" title="Dancing Avatar: Pose and Text-Guided Human Motion Videos Synthesis with Image Diffusion Model"></a>Dancing Avatar: Pose and Text-Guided Human Motion Videos Synthesis with Image Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07749">http://arxiv.org/abs/2308.07749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bosheng Qin, Wentao Ye, Qifan Yu, Siliang Tang, Yueting Zhuang</li>
<li>for: 生成高质量人物动画视频，用于应用于游戏、电影等领域。</li>
<li>methods: 使用预训练的T2I扩散模型，通过权重学习模型来生成每帧视频，并使用文本引导和人物姿势来控制人物的动作。</li>
<li>results: 与现有state-of-the-art方法相比，Dancing Avatar可以生成高质量的人物动画视频，保持人物和背景的一致性，同时具有更高的时间协调性。<details>
<summary>Abstract</summary>
The rising demand for creating lifelike avatars in the digital realm has led to an increased need for generating high-quality human videos guided by textual descriptions and poses. We propose Dancing Avatar, designed to fabricate human motion videos driven by poses and textual cues. Our approach employs a pretrained T2I diffusion model to generate each video frame in an autoregressive fashion. The crux of innovation lies in our adept utilization of the T2I diffusion model for producing video frames successively while preserving contextual relevance. We surmount the hurdles posed by maintaining human character and clothing consistency across varying poses, along with upholding the background's continuity amidst diverse human movements. To ensure consistent human appearances across the entire video, we devise an intra-frame alignment module. This module assimilates text-guided synthesized human character knowledge into the pretrained T2I diffusion model, synergizing insights from ChatGPT. For preserving background continuity, we put forth a background alignment pipeline, amalgamating insights from segment anything and image inpainting techniques. Furthermore, we propose an inter-frame alignment module that draws inspiration from an auto-regressive pipeline to augment temporal consistency between adjacent frames, where the preceding frame guides the synthesis process of the current frame. Comparisons with state-of-the-art methods demonstrate that Dancing Avatar exhibits the capacity to generate human videos with markedly superior quality, both in terms of human and background fidelity, as well as temporal coherence compared to existing state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
“因应数字世界中创造生命如实的人物需求的增加，我们提出了舞动人物（Dancing Avatar），一个基于文本描述和姿势驱动的人工动画生成器。我们的方法使用预训T2I散射模型来生成每帧影像，透过autoregressive的方式实现每帧影像的生成。我们的创新在于，通过将文本描述和姿势知识融合到预训T2I散射模型中，以确保人物和背景的一致性。为保持人物的一致性，我们提出了一个内部对焦模块，将文本描述驱动的人物知识融合到预训T2I散射模型中。此外，我们还提出了一个间隔对焦模块，将预训T2I散射模型与另一个自动推理管线结合，以增强动画中人物的一致性。 Comparisons with state-of-the-art methods show that Dancing Avatar can generate high-quality human videos with superior fidelity, both in terms of human and background, as well as temporal coherence compared to existing state-of-the-art approaches.”
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Sparsity-in-Automotive-Radar-Object-Detection-Networks"><a href="#Exploiting-Sparsity-in-Automotive-Radar-Object-Detection-Networks" class="headerlink" title="Exploiting Sparsity in Automotive Radar Object Detection Networks"></a>Exploiting Sparsity in Automotive Radar Object Detection Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07748">http://arxiv.org/abs/2308.07748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marius Lippke, Maurice Quach, Sascha Braun, Daniel Köhler, Michael Ulrich, Bastian Bischoff, Wei Yap Tan</li>
<li>for: 本研究旨在提高自动驾驶系统中的环境识别精度，以确保系统的安全和可靠运行。</li>
<li>methods: 本文使用 sparse convolutional object detection networks，这种网络结合了高效的网格式检测和低计算资源。 authors 还提出了适应 радиар特有挑战的 sparse kernel point pillars (SKPP) 和 dual voxel point convolutions (DVPC)，以解决网格渲染和稀疏基础架构的问题。</li>
<li>results: 在 nuScenes 上测试的 SKPP-DPVCN 架构，与基eline 相比提高了4.19%，并且与之前的状态分析提高了5.89%的Car AP4.0。此外，SKPP-DPVCN 还将平均扩散错误 (ASE) 降低了21.41%。<details>
<summary>Abstract</summary>
Having precise perception of the environment is crucial for ensuring the secure and reliable functioning of autonomous driving systems. Radar object detection networks are one fundamental part of such systems. CNN-based object detectors showed good performance in this context, but they require large compute resources. This paper investigates sparse convolutional object detection networks, which combine powerful grid-based detection with low compute resources. We investigate radar specific challenges and propose sparse kernel point pillars (SKPP) and dual voxel point convolutions (DVPC) as remedies for the grid rendering and sparse backbone architectures. We evaluate our SKPP-DPVCN architecture on nuScenes, which outperforms the baseline by 5.89% and the previous state of the art by 4.19% in Car AP4.0. Moreover, SKPP-DPVCN reduces the average scale error (ASE) by 21.41% over the baseline.
</details>
<details>
<summary>摘要</summary>
“精确的环境认知是自动驾驶系统的安全和可靠运行所必备的。这篇论文探讨了具有强大的格子基础的对象探测网络，它们可以在自动驾驶系统中提供高性能，但是它们需要大量的计算资源。本文提出了稀疑几何点柱（SKPP）和双对称点核心（DVPC）来解决格式化和稀疑网络架构的挑战。我们评估了基于SKPP-DVPC的SKPP-DPVCN架构在nuScenes上的表现，该架构比基准点出5.89%的提升和前一个状态的实验出4.19%的提升。此外，SKPP-DPVCN还 redues了平均规模错误（ASE）的值比基准点下降21.41%。”
</details></li>
</ul>
<hr>
<h2 id="Formally-Sharp-DAgger-for-MCTS-Lower-Latency-Monte-Carlo-Tree-Search-using-Data-Aggregation-with-Formal-Methods"><a href="#Formally-Sharp-DAgger-for-MCTS-Lower-Latency-Monte-Carlo-Tree-Search-using-Data-Aggregation-with-Formal-Methods" class="headerlink" title="Formally-Sharp DAgger for MCTS: Lower-Latency Monte Carlo Tree Search using Data Aggregation with Formal Methods"></a>Formally-Sharp DAgger for MCTS: Lower-Latency Monte Carlo Tree Search using Data Aggregation with Formal Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07738">http://arxiv.org/abs/2308.07738</a></li>
<li>repo_url: None</li>
<li>paper_authors: Debraj Chakraborty, Damien Busatto-Gaston, Jean-François Raskin, Guillermo A. Pérez</li>
<li>for: 这 paper 的目的是提出一种高效的组合 formal methods、Monte Carlo Tree Search (MCTS) 和 deep learning 来生成大 Markov Decision processes (MDPs) 中的高质量递减时间策略。</li>
<li>methods: 这 paper 使用 model-checking 技术来引导 MCTS 算法，生成 MDP 中的高质量决策样本，并用这些样本来训练一个模仿策略。这个模仿策略可以用作在线 MCTS 搜索的导向，或者作为最低延迟时间的策略。</li>
<li>results: 这 paper 使用 statistical model checking 来检测需要更多样本的情况，并将更多样本集中在 MDP 中的不同配置下，以便训练模仿策略。并在 Frozen Lake 和 Pac-Man 环境中进行了实验，证明了该方法的有效性。<details>
<summary>Abstract</summary>
We study how to efficiently combine formal methods, Monte Carlo Tree Search (MCTS), and deep learning in order to produce high-quality receding horizon policies in large Markov Decision processes (MDPs). In particular, we use model-checking techniques to guide the MCTS algorithm in order to generate offline samples of high-quality decisions on a representative set of states of the MDP. Those samples can then be used to train a neural network that imitates the policy used to generate them. This neural network can either be used as a guide on a lower-latency MCTS online search, or alternatively be used as a full-fledged policy when minimal latency is required. We use statistical model checking to detect when additional samples are needed and to focus those additional samples on configurations where the learnt neural network policy differs from the (computationally-expensive) offline policy. We illustrate the use of our method on MDPs that model the Frozen Lake and Pac-Man environments -- two popular benchmarks to evaluate reinforcement-learning algorithms.
</details>
<details>
<summary>摘要</summary>
我们研究如何有效地结合正式方法、Monte Carlo Tree Search（MCTS）和深度学习，以生成高质量的回溯时间政策在大Markov决策过程（MDP）中。特别是，我们使用模型检查技术来引导MCTS算法，以生成 offline 样本高质量决策在 MDP 的表示集中。这些样本可以用来训练一个模仿政策的神经网络，这个神经网络可以在更低的延迟下在线搜索中作为引导，或者作为尽可能快的全功能政策。我们使用统计模型检查来检测需要更多的样本，并将这些样本集中在计算机严重的 offline 政策与学习的神经网络政策之间的差异。我们在 Frozen Lake 和 Pac-Man 环境中使用我们的方法进行示例。
</details></li>
</ul>
<hr>
<h2 id="Flashpoints-Signal-Hidden-Inherent-Instabilities-in-Land-Use-Planning"><a href="#Flashpoints-Signal-Hidden-Inherent-Instabilities-in-Land-Use-Planning" class="headerlink" title="Flashpoints Signal Hidden Inherent Instabilities in Land-Use Planning"></a>Flashpoints Signal Hidden Inherent Instabilities in Land-Use Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07714">http://arxiv.org/abs/2308.07714</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hazhir Aliahmadi, Maeve Beckett, Sam Connolly, Dongmei Chen, Greg van Anders</li>
<li>For: The paper aims to improve the objectivity and transparency of land-use decision-making processes by using optimization-based planning approaches, such as Multi-Objective Land Allocation (MOLA).* Methods: The paper uses quantitative methods to evaluate planning priorities and generate a series of unstable “flashpoints” where small changes in planning priorities lead to large-scale changes in land use.* Results: The paper shows that quantitative methods can reduce the combinatorially large space of possible land-use patterns to a small, characteristic set that can engage stakeholders to arrive at more efficient and just outcomes. Additionally, the paper identifies “gray areas” in land-use type that arise due to instabilities in the planning process.<details>
<summary>Abstract</summary>
Land-use decision-making processes have a long history of producing globally pervasive systemic equity and sustainability concerns. Quantitative, optimization-based planning approaches, e.g. Multi-Objective Land Allocation (MOLA), seemingly open the possibility to improve objectivity and transparency by explicitly evaluating planning priorities by the type, amount, and location of land uses. Here, we show that optimization-based planning approaches with generic planning criteria generate a series of unstable "flashpoints" whereby tiny changes in planning priorities produce large-scale changes in the amount of land use by type. We give quantitative arguments that the flashpoints we uncover in MOLA models are examples of a more general family of instabilities that occur whenever planning accounts for factors that coordinate use on- and between-sites, regardless of whether these planning factors are formulated explicitly or implicitly. We show that instabilities lead to regions of ambiguity in land-use type that we term "gray areas". By directly mapping gray areas between flashpoints, we show that quantitative methods retain utility by reducing combinatorially large spaces of possible land-use patterns to a small, characteristic set that can engage stakeholders to arrive at more efficient and just outcomes.
</details>
<details>
<summary>摘要</summary>
农用决策过程具有历史悠久的生产全球性平等和可持续发展问题。量化优化规划方法，例如多目标农用分配（MOLA），似乎可以提高 объекivity和透明度，由明确规划优先级来评估农用类型、量和位置。在这里，我们表明了量化规划方法中的“闪点”现象，即小 Change in 规划优先级可能导致大规模的农用类型占用量变化。我们提供了量化的证明，表明这些闪点在MOLA模型中是一种更通用的不稳定性现象，无论规划因素是否明确或暗示地表达。我们还显示了这些不稳定性导致农用类型之间的“灰色区”，即不同规划优先级下的农用类型占用量的变化范围。通过直接映射灰色区，我们表明了量化方法仍然保留了实用性，可以将可能的农用模式空间减少到一个小、特征集，以便更有效和公正的决策结果。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Transfer-Learning-in-Medical-Image-Segmentation-using-Vision-Language-Models"><a href="#Exploring-Transfer-Learning-in-Medical-Image-Segmentation-using-Vision-Language-Models" class="headerlink" title="Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models"></a>Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07706">http://arxiv.org/abs/2308.07706</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kanchan Poudel, Manish Dhakal, Prasiddha Bhandari, Rabin Adhikari, Safal Thapaliya, Bishesh Khanal</li>
<li>for: 医疗图像分割是医疗领域中重要的应用之一，但是将文本指导integrated到图像分割模型中仍然是一个有限的进展。</li>
<li>methods: 我们提议使用多Modal vision-language模型来捕捉图像描述和图像的semantic信息，以便进行多种医疗图像的分割。</li>
<li>results: 我们的研究发现，将open-domain图像的视觉语言模型直接应用于医疗图像分割 tasks是不可靠的，但是通过微调可以提高其性能。我们在11个医疗dataset上使用4种VLMs和9种提示来评估其零基eline和微调性能。<details>
<summary>Abstract</summary>
Medical Image Segmentation is crucial in various clinical applications within the medical domain. While state-of-the-art segmentation models have proven effective, integrating textual guidance to enhance visual features for this task remains an area with limited progress. Existing segmentation models that utilize textual guidance are primarily trained on open-domain images, raising concerns about their direct applicability in the medical domain without manual intervention or fine-tuning.   To address these challenges, we propose using multimodal vision-language models for capturing semantic information from image descriptions and images, enabling the segmentation of diverse medical images. This study comprehensively evaluates existing vision language models across multiple datasets to assess their transferability from the open domain to the medical field. Furthermore, we introduce variations of image descriptions for previously unseen images in the dataset, revealing notable variations in model performance based on the generated prompts.   Our findings highlight the distribution shift between the open-domain images and the medical domain and show that the segmentation models trained on open-domain images are not directly transferrable to the medical field. But their performance can be increased by finetuning them in the medical datasets. We report the zero-shot and finetuned segmentation performance of 4 Vision Language Models (VLMs) on 11 medical datasets using 9 types of prompts derived from 14 attributes.
</details>
<details>
<summary>摘要</summary>
医疗图像分割是医疗领域中不同临床应用中的关键。虽然当前的分割模型有效，但将文本指导 integrate into 图像特征以提高分割效果是一个有限的进展。现有的分割模型，它们主要是在开放领域图像上训练的，这引发了对其直接适用性在医疗领域的担忧。为了解决这些挑战，我们提议使用多模态视语言模型，以便从图像描述和图像中提取 semantic information，以便分割多种医疗图像。本研究对多个数据集进行了全面的评估，以评估现有的视语言模型在医疗领域是否可以进行转移。此外，我们还引入了图像描述中的变化，并评估模型的性能差异。我们的发现表明，开放领域图像和医疗领域之间存在分布差异，而且训练在开放领域图像上的模型不能直接应用于医疗领域。但是，通过训练这些模型在医疗数据集上，可以提高其性能。我们在11个医疗数据集上使用4种视语言模型进行零基础和训练性能测试，使用9种Prompt derived from 14个特征。
</details></li>
</ul>
<hr>
<h2 id="DiffGuard-Semantic-Mismatch-Guided-Out-of-Distribution-Detection-using-Pre-trained-Diffusion-Models"><a href="#DiffGuard-Semantic-Mismatch-Guided-Out-of-Distribution-Detection-using-Pre-trained-Diffusion-Models" class="headerlink" title="DiffGuard: Semantic Mismatch-Guided Out-of-Distribution Detection using Pre-trained Diffusion Models"></a>DiffGuard: Semantic Mismatch-Guided Out-of-Distribution Detection using Pre-trained Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07687">http://arxiv.org/abs/2308.07687</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cure-lab/diffguard">https://github.com/cure-lab/diffguard</a></li>
<li>paper_authors: Ruiyuan Gao, Chenchen Zhao, Lanqing Hong, Qiang Xu</li>
<li>for: 这个研究旨在提出一种基于 semantic mismatch 的 Out-of-Distribution (OOD) 检测方法，并使用 pre-trained diffusion models 来实现。</li>
<li>methods: 本研究使用了 conditional Generative Adversarial Network (cGAN) 来增加 semantic mismatch 在图像空间中，并使用 pre-trained diffusion models 来直接进行 semantic mismatch-guided OOD 检测。</li>
<li>results: 实验结果显示 DiffGuard 能够在 Cifar-10 和 ImageNet 上达到州际级的 OOD 检测效果，并且可以与现有的 OOD 检测技术结合以 дости持续获得最佳 OOD 检测结果。<details>
<summary>Abstract</summary>
Given a classifier, the inherent property of semantic Out-of-Distribution (OOD) samples is that their contents differ from all legal classes in terms of semantics, namely semantic mismatch. There is a recent work that directly applies it to OOD detection, which employs a conditional Generative Adversarial Network (cGAN) to enlarge semantic mismatch in the image space. While achieving remarkable OOD detection performance on small datasets, it is not applicable to ImageNet-scale datasets due to the difficulty in training cGANs with both input images and labels as conditions. As diffusion models are much easier to train and amenable to various conditions compared to cGANs, in this work, we propose to directly use pre-trained diffusion models for semantic mismatch-guided OOD detection, named DiffGuard. Specifically, given an OOD input image and the predicted label from the classifier, we try to enlarge the semantic difference between the reconstructed OOD image under these conditions and the original input image. We also present several test-time techniques to further strengthen such differences. Experimental results show that DiffGuard is effective on both Cifar-10 and hard cases of the large-scale ImageNet, and it can be easily combined with existing OOD detection techniques to achieve state-of-the-art OOD detection results.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese)给定一个分类器，外围样本的内在特性是其 contenuto 与所有合法类型的 semantics 不同，即 semantics mismatch。有一项最近的工作直接应用于 OOD 检测，使用 conditional Generative Adversarial Network (cGAN) 来增大图像空间中的 semantic mismatch。虽然在小 dataset 上达到了惊人的 OOD 检测性能，但是在 ImageNet  scale 上 dataset 上不可能进行训练 cGAN 因为 condition 的困难。由于 diffusion models 训练更加容易，并且可以适应多种 condition，因此在这里我们提议直接使用预训练的 diffusion models 进行 semantics mismatch 导向的 OOD 检测，名为 DiffGuard。Specifically，给定一个 OOD 输入图像和分类器预测的标签，我们尝试通过增大这些 condition 下重建 OOD 图像的 semantic difference 和原始输入图像之间的差异来增大 semantic mismatch。我们还提供了多种测试时技术来进一步强化这些差异。实验结果表明，DiffGuard 效果良好于 Cifar-10 和 ImageNet 中的困难情况，并且可以与现有 OOD 检测技术相结合以 достиieving state-of-the-art OOD 检测结果。
</details></li>
</ul>
<hr>
<h2 id="Boosting-Multi-modal-Model-Performance-with-Adaptive-Gradient-Modulation"><a href="#Boosting-Multi-modal-Model-Performance-with-Adaptive-Gradient-Modulation" class="headerlink" title="Boosting Multi-modal Model Performance with Adaptive Gradient Modulation"></a>Boosting Multi-modal Model Performance with Adaptive Gradient Modulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07686">http://arxiv.org/abs/2308.07686</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lihong2303/agm_iccv2023">https://github.com/lihong2303/agm_iccv2023</a></li>
<li>paper_authors: Hong Li, Xingyu Li, Pengbo Hu, Yinuo Lei, Chunxiao Li, Yi Zhou</li>
<li>for: 提高多模态学习模型的性能，解决现有的模态竞争问题。</li>
<li>methods: 提出了一种适应性Gradient Modulation方法，可以提高多模态模型的表现，并且可以应用于不同的融合策略。</li>
<li>results: 经验表明，我们的方法可以超越现有的模ulation方法，并且通过引入一种新的竞争强度度量，得到了对模态竞争的量化理解。<details>
<summary>Abstract</summary>
While the field of multi-modal learning keeps growing fast, the deficiency of the standard joint training paradigm has become clear through recent studies. They attribute the sub-optimal performance of the jointly trained model to the modality competition phenomenon. Existing works attempt to improve the jointly trained model by modulating the training process. Despite their effectiveness, those methods can only apply to late fusion models. More importantly, the mechanism of the modality competition remains unexplored. In this paper, we first propose an adaptive gradient modulation method that can boost the performance of multi-modal models with various fusion strategies. Extensive experiments show that our method surpasses all existing modulation methods. Furthermore, to have a quantitative understanding of the modality competition and the mechanism behind the effectiveness of our modulation method, we introduce a novel metric to measure the competition strength. This metric is built on the mono-modal concept, a function that is designed to represent the competition-less state of a modality. Through systematic investigation, our results confirm the intuition that the modulation encourages the model to rely on the more informative modality. In addition, we find that the jointly trained model typically has a preferred modality on which the competition is weaker than other modalities. However, this preferred modality need not dominate others. Our code will be available at https://github.com/lihong2303/AGM_ICCV2023.
</details>
<details>
<summary>摘要</summary>
而 field of multi-modal learning 的发展速度不断增长，标准的联合训练方法的缺点也日益明显。这些研究表明，联合训练模型的性能下降归结于modal competition现象。现有的方法可以通过修改训练过程来改善联合训练模型，但这些方法只适用于late fusion模型。更重要的是，modal competition的机制还没有得到解释。在这篇论文中，我们首先提出一种适应性的梯度修正方法，可以提高不同拟合策略的多Modal模型性能。广泛的实验表明，我们的方法超过了所有现有的修正方法。此外，为了有一个准确的理解modal competition的机制，我们引入了一种新的竞争力度量，它基于单模态概念，这是一种用于表示没有竞争的状态的函数。通过系统性的调查，我们的结果证明了我们的修正方法能够鼓励模型依赖于更有用的感知模式。此外，我们发现，联合训练模型通常有一个具有较弱竞争力的首选模式，但这并不意味着这个模式会完全控制其他模式。我们的代码将在https://github.com/lihong2303/AGM_ICCV2023上发布。
</details></li>
</ul>
<hr>
<h2 id="EQ-Net-Elastic-Quantization-Neural-Networks"><a href="#EQ-Net-Elastic-Quantization-Neural-Networks" class="headerlink" title="EQ-Net: Elastic Quantization Neural Networks"></a>EQ-Net: Elastic Quantization Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07650">http://arxiv.org/abs/2308.07650</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xuke225/eq-net">https://github.com/xuke225/eq-net</a></li>
<li>paper_authors: Ke Xu, Lei Han, Ye Tian, Shangshang Yang, Xingyi Zhang</li>
<li>for: 该 paper 目的是提出一种一键网络量化 regime， named Elastic Quantization Neural Networks (EQ-Net)，用于训练一个可重用的量化超网。</li>
<li>methods: 该 paper 使用了一种弹性量化空间 (包括弹性比特宽、粒子大小和对称) 适应不同的主流量化形式。其次，提出了 Weight Distribution Regularization Loss (WDR-Loss) 和 Group Progressive Guidance Loss (GPG-Loss) 两种损失函数来减少量化空间中 weights 和输出 logits 的分布不一致。最后，使用了遗传算法和提出的 Conditional Quantization-Aware Accuracy Predictor (CQAP) 作为估计器快速搜索混合精度量化神经网络在超网中。</li>
<li>results: 广泛的实验表明，我们的 EQ-Net 与其静态对应物以及当前最佳稳定量化方法几乎相当或更好。代码可以在 \href{<a target="_blank" rel="noopener" href="https://github.com/xuke225/EQ-Net.git%7D%7Bhttps://github.com/xuke225/EQ-Net%7D">https://github.com/xuke225/EQ-Net.git}{https://github.com/xuke225/EQ-Net}</a> 上获得。<details>
<summary>Abstract</summary>
Current model quantization methods have shown their promising capability in reducing storage space and computation complexity. However, due to the diversity of quantization forms supported by different hardware, one limitation of existing solutions is that usually require repeated optimization for different scenarios. How to construct a model with flexible quantization forms has been less studied. In this paper, we explore a one-shot network quantization regime, named Elastic Quantization Neural Networks (EQ-Net), which aims to train a robust weight-sharing quantization supernet. First of all, we propose an elastic quantization space (including elastic bit-width, granularity, and symmetry) to adapt to various mainstream quantitative forms. Secondly, we propose the Weight Distribution Regularization Loss (WDR-Loss) and Group Progressive Guidance Loss (GPG-Loss) to bridge the inconsistency of the distribution for weights and output logits in the elastic quantization space gap. Lastly, we incorporate genetic algorithms and the proposed Conditional Quantization-Aware Accuracy Predictor (CQAP) as an estimator to quickly search mixed-precision quantized neural networks in supernet. Extensive experiments demonstrate that our EQ-Net is close to or even better than its static counterparts as well as state-of-the-art robust bit-width methods. Code can be available at \href{https://github.com/xuke225/EQ-Net.git}{https://github.com/xuke225/EQ-Net}.
</details>
<details>
<summary>摘要</summary>
当前的模型量化方法已经表现出了减少存储空间和计算复杂度的承诺。然而，由于不同硬件支持的量化形式的多样性，现有的解决方案通常需要重复优化不同的场景。在这篇论文中，我们探索了一种一键网络量化方式，名为弹性量化神经网络（EQ-Net），旨在训练一个可以共享量化超网。首先，我们提出了弹性量化空间（包括弹性位数、粒度和对称），以适应不同主流量化形式。其次，我们提出了Weight Distribution Regularization Loss（WDR-Loss）和Group Progressive Guidance Loss（GPG-Loss）来bridging弹性量化空间中 weights和输出logits的分布不一致性。最后，我们将遗传算法和提出的Conditional Quantization-Aware Accuracy Predictor（CQAP）作为估计器，快速查找混合精度量化神经网络在超网中。广泛的实验证明了我们的EQ-Net与其静态对手以及State-of-the-art Robust Bit-Width Methods相当或甚至更好。代码可以在 \href{https://github.com/xuke225/EQ-Net.git}{https://github.com/xuke225/EQ-Net} 中获取。
</details></li>
</ul>
<hr>
<h2 id="Ternary-Singular-Value-Decomposition-as-a-Better-Parameterized-Form-in-Linear-Mapping"><a href="#Ternary-Singular-Value-Decomposition-as-a-Better-Parameterized-Form-in-Linear-Mapping" class="headerlink" title="Ternary Singular Value Decomposition as a Better Parameterized Form in Linear Mapping"></a>Ternary Singular Value Decomposition as a Better Parameterized Form in Linear Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07641">http://arxiv.org/abs/2308.07641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boyu Chen, Hanxuan Chen, Jiao He, Fengyu Sun, Shangling Jui</li>
<li>For: 本文提出了一种简单 yet novel的参数化线性映射方法，以实现杰出的网络压缩性能。* Methods: 该方法基于pseudo SVD（Ternary SVD，TSVD），与标准SVD不同的是，TSVD限制$U$和$V$矩阵为ternary矩阵（${\pm 1, 0}$）。这意味着在计算$U(\cdot)$和$V(\cdot)$时，只需要进行加法运算。* Results: 在各种网络和任务中，TSVD可以实现现状顶峰的网络压缩性能，包括当前基线模型如ConvNext、Swim、BERT以及大型语言模型如OPT。<details>
<summary>Abstract</summary>
We present a simple yet novel parameterized form of linear mapping to achieves remarkable network compression performance: a pseudo SVD called Ternary SVD (TSVD).   Unlike vanilla SVD, TSVD limits the $U$ and $V$ matrices in SVD to ternary matrices form in $\{\pm 1, 0\}$. This means that instead of using the expensive multiplication instructions, TSVD only requires addition instructions when computing $U(\cdot)$ and $V(\cdot)$.   We provide direct and training transition algorithms for TSVD like Post Training Quantization and Quantization Aware Training respectively. Additionally, we analyze the convergence of the direct transition algorithms in theory.   In experiments, we demonstrate that TSVD can achieve state-of-the-art network compression performance in various types of networks and tasks, including current baseline models such as ConvNext, Swim, BERT, and large language model like OPT.
</details>
<details>
<summary>摘要</summary>
我们提出了一种简单 yet novel的参数化线性映射方法，可以夺得惊人的网络压缩性能：一种 pseudo SVD called Ternary SVD (TSVD)。 不同于普通的 SVD，TSVD 限制 $U$ 和 $V$ 矩阵在 SVD 中到了三元矩阵形式 ($\{\pm 1, 0\}$)。这意味着在计算 $U(\cdot)$ 和 $V(\cdot)$ 时，TSVD 只需要使用加法指令，而不需要昂贵的乘法指令。我们提供了直接转移算法和训练转移算法 для TSVD，如 Post Training Quantization 和 Quantization Aware Training 等。此外，我们也 theoretically 分析了直接转移算法的整合性。在实验中，我们证明了 TSVD 可以在不同类型的网络和任务上夺得当今基线模型如 ConvNext、Swim、BERT 和大语言模型 OPT 的状态级网络压缩性能。
</details></li>
</ul>
<hr>
<h2 id="LLM-Mini-CEX-Automatic-Evaluation-of-Large-Language-Model-for-Diagnostic-Conversation"><a href="#LLM-Mini-CEX-Automatic-Evaluation-of-Large-Language-Model-for-Diagnostic-Conversation" class="headerlink" title="LLM-Mini-CEX: Automatic Evaluation of Large Language Model for Diagnostic Conversation"></a>LLM-Mini-CEX: Automatic Evaluation of Large Language Model for Diagnostic Conversation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07635">http://arxiv.org/abs/2308.07635</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoming Shi, Jie Xu, Jinru Ding, Jiali Pang, Sichen Liu, Shuqing Luo, Xingwei Peng, Lu Lu, Haihong Yang, Mingtao Hu, Tong Ruan, Shaoting Zhang</li>
<li>for: 这研究旨在提供一个统一的评估标准，以评估医疗语言模型（LLM）的诊断能力。</li>
<li>methods: 该研究首先建立了一个特有的评估标准，称为LLM特有的Mini-CEX，以评估医疗LLM的诊断能力。此外，研究者还开发了一个patient simulator，用于自动与LLM进行对话，并使用ChatGPT来自动评估诊断对话的质量。</li>
<li>results: 实验结果表明，LLM特有的Mini-CEX是一个有效和必需的评估标准，可以评估医疗LLM的诊断对话质量。此外，ChatGPT也可以自动评估诊断对话的人文特质，并提供可重复和自动比较不同LLM的能力。<details>
<summary>Abstract</summary>
There is an increasing interest in developing LLMs for medical diagnosis to improve diagnosis efficiency. Despite their alluring technological potential, there is no unified and comprehensive evaluation criterion, leading to the inability to evaluate the quality and potential risks of medical LLMs, further hindering the application of LLMs in medical treatment scenarios. Besides, current evaluations heavily rely on labor-intensive interactions with LLMs to obtain diagnostic dialogues and human evaluation on the quality of diagnosis dialogue. To tackle the lack of unified and comprehensive evaluation criterion, we first initially establish an evaluation criterion, termed LLM-specific Mini-CEX to assess the diagnostic capabilities of LLMs effectively, based on original Mini-CEX. To address the labor-intensive interaction problem, we develop a patient simulator to engage in automatic conversations with LLMs, and utilize ChatGPT for evaluating diagnosis dialogues automatically. Experimental results show that the LLM-specific Mini-CEX is adequate and necessary to evaluate medical diagnosis dialogue. Besides, ChatGPT can replace manual evaluation on the metrics of humanistic qualities and provides reproducible and automated comparisons between different LLMs.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>随着医疗推荐系统的发展，有越来越多的研究者关注开发医疗推荐系统，以提高诊断效率。然而，这些系统的评价标准尚未统一，导致诊断系统的质量和风险难以评估，从而限制了医疗推荐系统的应用场景。此外，当前的评价方法仍然依赖于人工干预，通过与医疗推荐系统进行劳动密集的对话来获取诊断对话，以及人工评估诊断对话的质量。为了解决统一评价标准的缺失，我们首先建立了一个特定于医疗推荐系统的评价标准，称为LLM特定的Mini-CEX，以评估医疗推荐系统的诊断能力。为了解决人工干预的问题，我们开发了一个模拟病人的模拟器，可以自动与医疗推荐系统进行对话，并使用ChatGPT来自动评估诊断对话的质量。实验结果表明，LLM特定的Mini-CEX是有效和必要的评价医疗诊断对话的标准，而ChatGPT可以替代人工评估，并提供可重复和自动化的对比。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Model-Compression-for-Large-Language-Models"><a href="#A-Survey-on-Model-Compression-for-Large-Language-Models" class="headerlink" title="A Survey on Model Compression for Large Language Models"></a>A Survey on Model Compression for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07633">http://arxiv.org/abs/2308.07633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xunyu Zhu, Jian Li, Yong Liu, Can Ma, Weiping Wang</li>
<li>for: 本文旨在概述大自然语言处理任务中的语言模型压缩技术，尤其是针对资源有限的环境下进行实用部署。</li>
<li>methods: 本文介绍了各种压缩方法，包括量化、剪辑、知识传承和更多的技术，并讲述了每种方法的最新发展和创新应用。</li>
<li>results: 本文提供了评估压缩后模型效果的方法和指标，并探讨了这些方法在实际应用中的实用性。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have revolutionized natural language processing tasks with remarkable success. However, their formidable size and computational demands present significant challenges for practical deployment, especially in resource-constrained environments. As these challenges become increasingly pertinent, the field of model compression has emerged as a pivotal research area to alleviate these limitations. This paper presents a comprehensive survey that navigates the landscape of model compression techniques tailored specifically for LLMs. Addressing the imperative need for efficient deployment, we delve into various methodologies, encompassing quantization, pruning, knowledge distillation, and more. Within each of these techniques, we highlight recent advancements and innovative approaches that contribute to the evolving landscape of LLM research. Furthermore, we explore benchmarking strategies and evaluation metrics that are essential for assessing the effectiveness of compressed LLMs. By providing insights into the latest developments and practical implications, this survey serves as an invaluable resource for both researchers and practitioners. As LLMs continue to evolve, this survey aims to facilitate enhanced efficiency and real-world applicability, establishing a foundation for future advancements in the field.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Vision-based-Semantic-Communications-for-Metaverse-Services-A-Contest-Theoretic-Approach"><a href="#Vision-based-Semantic-Communications-for-Metaverse-Services-A-Contest-Theoretic-Approach" class="headerlink" title="Vision-based Semantic Communications for Metaverse Services: A Contest Theoretic Approach"></a>Vision-based Semantic Communications for Metaverse Services: A Contest Theoretic Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07618">http://arxiv.org/abs/2308.07618</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangyuan Liu, Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong, Boon Hee Soong</li>
<li>for: 提供一个对Metaverse中人员与服务提供商之间的对话和资源分配的Semantic Communication框架，以提高用户在虚拟世界中的体验。</li>
<li>methods: 使用Contest Theory来模型用户和服务提供商之间的互动，并根据每个用户的需求进行资源分配。使用Semantic Communication技术将数据量降至51字节，从而减少网络资源的消耗。使用深度Q学网来优化优先级，以最大化性能和资源分配效率。</li>
<li>results: 比较传统平均分配方法，透过优化优先级，将下调损失率降至66.076%。提供一个为Metaverse中人员与服务提供商之间的资源分配解决方案，以提高用户在虚拟世界中的体验。<details>
<summary>Abstract</summary>
The popularity of Metaverse as an entertainment, social, and work platform has led to a great need for seamless avatar integration in the virtual world. In Metaverse, avatars must be updated and rendered to reflect users' behaviour. Achieving real-time synchronization between the virtual bilocation and the user is complex, placing high demands on the Metaverse Service Provider (MSP)'s rendering resource allocation scheme. To tackle this issue, we propose a semantic communication framework that leverages contest theory to model the interactions between users and MSPs and determine optimal resource allocation for each user. To reduce the consumption of network resources in wireless transmission, we use the semantic communication technique to reduce the amount of data to be transmitted. Under our simulation settings, the encoded semantic data only contains 51 bytes of skeleton coordinates instead of the image size of 8.243 megabytes. Moreover, we implement Deep Q-Network to optimize reward settings for maximum performance and efficient resource allocation. With the optimal reward setting, users are incentivized to select their respective suitable uploading frequency, reducing down-sampling loss due to rendering resource constraints by 66.076\% compared with the traditional average distribution method. The framework provides a novel solution to resource allocation for avatar association in VR environments, ensuring a smooth and immersive experience for all users.
</details>
<details>
<summary>摘要</summary>
“Metaverse的受欢迎程度使得虚拟世界中的人物集成变得非常重要。在Metaverse中，人物需要实时更新和渲染，以反映用户的行为。实现实时同步是复杂的，对Metaverse服务提供商（MSP）的渲染资源分配方案带来高要求。为解决这个问题，我们提议一种基于 semantics 的通信框架，利用对用户和 MSP 之间的交互进行模型化，并确定每个用户的优化资源分配策略。使用semantic通信技术可以减少无线传输中的网络资源消耗，并且我们使用 Deep Q-Network 优化奖励设置，以实现最佳性和有效的资源分配。根据优化奖励设置，用户可以选择适合自己的上传频率，从而减少由渲染资源限制引起的下采样损失，比传统均值分布方法减少了66.076%。该框架为虚拟世界中人物协调资源分配提供了一种新的解决方案，以保证所有用户都能获得平滑和充满感的体验。”
</details></li>
</ul>
<hr>
<h2 id="ERA-Enhanced-Relaxed-A-algorithm-for-Solving-the-Shortest-Path-Problem-in-Regular-Grid-Maps"><a href="#ERA-Enhanced-Relaxed-A-algorithm-for-Solving-the-Shortest-Path-Problem-in-Regular-Grid-Maps" class="headerlink" title="ERA*: Enhanced Relaxed A* algorithm for Solving the Shortest Path Problem in Regular Grid Maps"></a>ERA*: Enhanced Relaxed A* algorithm for Solving the Shortest Path Problem in Regular Grid Maps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10988">http://arxiv.org/abs/2308.10988</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adel Ammar</li>
<li>for: 解决点到点最短路径问题在静态Regular 8- neighbbor connectivity（G8）格网中。</li>
<li>methods: 使用一种新的算法，可以看作是 Hadlock 算法的普遍化，并且与 relaxed $A^*$（$RA^*）算法相等于，但具有不同的计算策略，基于定义lookup矩阵。</li>
<li>results: 通过对不同类型和大小的格图（1290个运行在43个地图上）进行实验，证明该算法比 $RA^*$ 快2.25倍，比原始 $A^*$ 快17倍，具有更好的内存利用率，不需要存储 G Score 矩阵。<details>
<summary>Abstract</summary>
This paper introduces a novel algorithm for solving the point-to-point shortest path problem in a static regular 8-neighbor connectivity (G8) grid. This algorithm can be seen as a generalization of Hadlock algorithm to G8 grids, and is shown to be theoretically equivalent to the relaxed $A^*$ ($RA^*$) algorithm in terms of the provided solution's path length, but with substantial time and memory savings, due to a completely different computation strategy, based on defining a set of lookup matrices. Through an experimental study on grid maps of various types and sizes (1290 runs on 43 maps), it is proven to be 2.25 times faster than $RA^*$ and 17 times faster than the original $A^*$, in average. Moreover, it is more memory-efficient, since it does not need to store a G score matrix.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了一种新的算法，用于解决在静态正方形8邻连接（G8）网格上的点到点最短路径问题。这种算法可以看作是 Hadlock 算法的总线式扩展，并且与 $RA^*$ 算法在解提供的路径长度方面是等价的，但具有不同的计算策略，基于定义一组查找表。通过对不同类型和大小的网格图（1290 个运行在 43 个图）进行实验研究，这种算法被证明为 $RA^*$ 的 2.25 倍快，并且比原始 $A^*$ 快了 17 倍，平均而言。此外，它还更加具有内存效率，因为它不需要存储 G 分数矩阵。
</details></li>
</ul>
<hr>
<h2 id="SGDiff-A-Style-Guided-Diffusion-Model-for-Fashion-Synthesis"><a href="#SGDiff-A-Style-Guided-Diffusion-Model-for-Fashion-Synthesis" class="headerlink" title="SGDiff: A Style Guided Diffusion Model for Fashion Synthesis"></a>SGDiff: A Style Guided Diffusion Model for Fashion Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07605">http://arxiv.org/abs/2308.07605</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/taited/sgdiff">https://github.com/taited/sgdiff</a></li>
<li>paper_authors: Zhengwentai Sun, Yanghong Zhou, Honghong He, P. Y. Mok</li>
<li>For: 本研究旨在开发一种新的样式指导扩散模型（SGDiff），以解决现有图像生成模型存在的一些缺陷。* Methods: 该模型结合图像modal和预训练的文本到图像扩散模型，以实现创新的时尚图像生成。它利用补充性的样式指导，降低训练成本，并在文本输入只能控制生成的样式方面解决了一些问题。* Results: 本研究新引入了SG-Fashion数据集，该数据集专门用于时尚图像生成应用，具有高分辨率图像和广泛的衣物类别。通过了全面的缺失学习研究，我们证明了提议的模型可以生成符合类别、产品特性和风格的时尚图像。<details>
<summary>Abstract</summary>
This paper reports on the development of \textbf{a novel style guided diffusion model (SGDiff)} which overcomes certain weaknesses inherent in existing models for image synthesis. The proposed SGDiff combines image modality with a pretrained text-to-image diffusion model to facilitate creative fashion image synthesis. It addresses the limitations of text-to-image diffusion models by incorporating supplementary style guidance, substantially reducing training costs, and overcoming the difficulties of controlling synthesized styles with text-only inputs. This paper also introduces a new dataset -- SG-Fashion, specifically designed for fashion image synthesis applications, offering high-resolution images and an extensive range of garment categories. By means of comprehensive ablation study, we examine the application of classifier-free guidance to a variety of conditions and validate the effectiveness of the proposed model for generating fashion images of the desired categories, product attributes, and styles. The contributions of this paper include a novel classifier-free guidance method for multi-modal feature fusion, a comprehensive dataset for fashion image synthesis application, a thorough investigation on conditioned text-to-image synthesis, and valuable insights for future research in the text-to-image synthesis domain. The code and dataset are available at: \url{https://github.com/taited/SGDiff}.
</details>
<details>
<summary>摘要</summary>
The paper also introduces a new dataset called SG-Fashion, which is specifically designed for fashion image synthesis and includes high-resolution images and a wide range of garment categories. The authors conduct a comprehensive ablation study to examine the effectiveness of the proposed method in various conditions and demonstrate its ability to generate fashion images with the desired categories, attributes, and styles.The main contributions of this paper include a novel classifier-free guidance method for multi-modal feature fusion, a comprehensive dataset for fashion image synthesis, a thorough investigation of conditioned text-to-image synthesis, and valuable insights for future research in the text-to-image synthesis domain. The code and dataset are available online at: <https://github.com/taited/SGDiff>.
</details></li>
</ul>
<hr>
<h2 id="Generating-Personas-for-Games-with-Multimodal-Adversarial-Imitation-Learning"><a href="#Generating-Personas-for-Games-with-Multimodal-Adversarial-Imitation-Learning" class="headerlink" title="Generating Personas for Games with Multimodal Adversarial Imitation Learning"></a>Generating Personas for Games with Multimodal Adversarial Imitation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07598">http://arxiv.org/abs/2308.07598</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Ahlberg, Alessandro Sestini, Konrad Tollmar, Linus Gisslén</li>
<li>for: 这篇论文目标是生成多个人工智能机器人可以模仿人类游戏玩家的多种玩法。</li>
<li>methods: 该论文提出了一种基于多模式生成对抗学习（MultiGAIL）的新方法，使用辅助输入参数来学习不同的人工智能玩家模式，并使用多个批评器作为奖励模型。</li>
<li>results: 实验结果表明，该方法在两个环境中（连续和离散动作空间）都有效，可以生成多个不同的人工智能玩家模式。<details>
<summary>Abstract</summary>
Reinforcement learning has been widely successful in producing agents capable of playing games at a human level. However, this requires complex reward engineering, and the agent's resulting policy is often unpredictable. Going beyond reinforcement learning is necessary to model a wide range of human playstyles, which can be difficult to represent with a reward function. This paper presents a novel imitation learning approach to generate multiple persona policies for playtesting. Multimodal Generative Adversarial Imitation Learning (MultiGAIL) uses an auxiliary input parameter to learn distinct personas using a single-agent model. MultiGAIL is based on generative adversarial imitation learning and uses multiple discriminators as reward models, inferring the environment reward by comparing the agent and distinct expert policies. The reward from each discriminator is weighted according to the auxiliary input. Our experimental analysis demonstrates the effectiveness of our technique in two environments with continuous and discrete action spaces.
</details>
<details>
<summary>摘要</summary>
现在的人工智能技术中，强化学习已经广泛应用于生成人类水平的游戏机器人。然而，这需要复杂的奖励工程，并且机器人的结果策略可能很难预测。为了模型人类多种游戏风格，超过强化学习是必要的，但这可以很难以表示为奖励函数。本文提出了一种新的模仿学习方法，可以生成多个人格策略用于游戏测试。我们称之为多modal生成对抗学习（MultiGAIL）。MultiGAIL使用了一个辅助输入参数，通过单个机器人模型来学习不同的人格。我们使用多个判据器作为奖励模型，通过比较机器人和各个专家策略来推断环境奖励。每个判据器的奖励得分被Weighted According to辅助输入。我们的实验分析表明，我们的技术在 kontinuous 和 discrete 动作空间中的两个环境中具有效果。
</details></li>
</ul>
<hr>
<h2 id="AutoLTS-Automating-Cycling-Stress-Assessment-via-Contrastive-Learning-and-Spatial-Post-processing"><a href="#AutoLTS-Automating-Cycling-Stress-Assessment-via-Contrastive-Learning-and-Spatial-Post-processing" class="headerlink" title="AutoLTS: Automating Cycling Stress Assessment via Contrastive Learning and Spatial Post-processing"></a>AutoLTS: Automating Cycling Stress Assessment via Contrastive Learning and Spatial Post-processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07580">http://arxiv.org/abs/2308.07580</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bo Lin, Shoshanna Saxe, Timothy C. Y. Chan</li>
<li>for: 这个论文是为了提供一种快速、精准和大规模的自行车压力评估方法，以便在城市道路网中规划自行车设施和路线建议。</li>
<li>methods: 这个论文使用了深度学习框架，利用街景图像来支持快速、精准和大规模的自行车压力评估。具体来说，这个框架包括一种对比学习方法，利用自行车压力标签的顺序关系，以及一种后处理技术，以保证预测结果的空间稳定性。</li>
<li>results: 在使用了39,153条道路段的 datasets 上，我们的结果表明，我们的深度学习框架可以快速、精准地进行自行车压力评估，并且可以使用街景图像来评估自行车压力，即使没有高质量的道路几何和机动车数据。<details>
<summary>Abstract</summary>
Cycling stress assessment, which quantifies cyclists' perceived stress imposed by the built environment and motor traffics, increasingly informs cycling infrastructure planning and cycling route recommendation. However, currently calculating cycling stress is slow and data-intensive, which hinders its broader application. In this paper, We propose a deep learning framework to support accurate, fast, and large-scale cycling stress assessments for urban road networks based on street-view images. Our framework features i) a contrastive learning approach that leverages the ordinal relationship among cycling stress labels, and ii) a post-processing technique that enforces spatial smoothness into our predictions. On a dataset of 39,153 road segments collected in Toronto, Canada, our results demonstrate the effectiveness of our deep learning framework and the value of using image data for cycling stress assessment in the absence of high-quality road geometry and motor traffic data.
</details>
<details>
<summary>摘要</summary>
《单车压力评估》，它衡量单车者对建筑环境和机动车流的感知压力，逐渐成为单车基础设施规划和单车路径建议的重要指标。但目前计算单车压力却是慢且资料密集的，这限制了它的广泛应用。在这篇论文中，我们提出了一个深度学习框架，用于支持精确、快速、大规模的单车压力评估，基于街景影像。我们的框架包括：①一种对比学习方法，利用单车压力标签之间的顺序关系，并②一种后处理技术，对我们的预测进行空间稳定化。在加拿大多伦多的39,153条道路段的数据集上，我们的结果显示了我们的深度学习框架的有效性，以及使用影像数据进行单车压力评估在缺乏高品质道路几何和机动车流数据的情况下的价值。
</details></li>
</ul>
<hr>
<h2 id="IoT-Data-Trust-Evaluation-via-Machine-Learning"><a href="#IoT-Data-Trust-Evaluation-via-Machine-Learning" class="headerlink" title="IoT Data Trust Evaluation via Machine Learning"></a>IoT Data Trust Evaluation via Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11638">http://arxiv.org/abs/2308.11638</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Timothy Tadj, Reza Arablouei, Volkan Dedeoglu</li>
<li>for: 评估互联网器件（IoT）数据的信任性。</li>
<li>methods: 使用随机散步填充（RWI）方法生成具有不可靠性的数据，并从感知器件数据中提取有效地捕捉自适应性和它们与邻居传感器数据的相关性的新特征。</li>
<li>results: 通过对多种基于机器学习（ML）的 IoT 数据信任评估方法进行广泛的实验，发现常见的 ML 基于方法表现不佳，这可以归因于不可靠的假设，即归一化提供可靠的标签 для数据信任。同时，通过使用RWI生成的数据和提取的特征，ML模型在未看到的数据上进行了好的普适性和超越性。此外， semi-supervised ML 方法，只需要约 10% 的数据标注，可以提供竞争力强的表现，并且在实际应用中更加具有实用性。<details>
<summary>Abstract</summary>
Various approaches based on supervised or unsupervised machine learning (ML) have been proposed for evaluating IoT data trust. However, assessing their real-world efficacy is hard mainly due to the lack of related publicly-available datasets that can be used for benchmarking. Since obtaining such datasets is challenging, we propose a data synthesis method, called random walk infilling (RWI), to augment IoT time-series datasets by synthesizing untrustworthy data from existing trustworthy data. Thus, RWI enables us to create labeled datasets that can be used to develop and validate ML models for IoT data trust evaluation. We also extract new features from IoT time-series sensor data that effectively capture its auto-correlation as well as its cross-correlation with the data of the neighboring (peer) sensors. These features can be used to learn ML models for recognizing the trustworthiness of IoT sensor data. Equipped with our synthesized ground-truth-labeled datasets and informative correlation-based feature, we conduct extensive experiments to critically examine various approaches to evaluating IoT data trust via ML. The results reveal that commonly used ML-based approaches to IoT data trust evaluation, which rely on unsupervised cluster analysis to assign trust labels to unlabeled data, perform poorly. This poor performance can be attributed to the underlying unsubstantiated assumption that clustering provides reliable labels for data trust, a premise that is found to be untenable. The results also show that the ML models learned from datasets augmented via RWI while using the proposed features generalize well to unseen data and outperform existing related approaches. Moreover, we observe that a semi-supervised ML approach that requires only about 10% of the data labeled offers competitive performance while being practically more appealing compared to the fully-supervised approaches.
</details>
<details>
<summary>摘要</summary>
各种基于监督或无监督机器学习（ML）方法已经提出来评估互联网器件（IoT）数据的信任性。然而，在实际世界中评估这些方法的效果很难，主要因为缺乏相关的公共可用数据集，可以用于比较。为了解决这个问题，我们提议一种数据生成方法，即随机游走填充（RWI），用于增强IoT时序数据集。这种方法可以生成可信worthy数据，并将其与现有的可信worthy数据相结合，以生成可靠的标注数据集。这些标注数据集可以用于开发和验证ML模型，以评估IoT数据的信任性。此外，我们还提取了IoT时序感知器数据中有效地捕捉自身的自相关性以及与邻居感知器数据的相关性。这些特征可以用于学习ML模型，以识别IoT感知器数据的信任性。利用我们生成的标注数据集和有用的相关特征，我们进行了广泛的实验，用以检验不同的IoT数据信任评估方法。结果显示，通常用于IoT数据信任评估的ML基于方法，即基于归一化分析进行无监督标注，表现不佳。这种不佳表现可以归因于下面的前提，即归一化分析提供了可靠的标注数据，这是一个不可靠的假设。另外，我们发现，使用RWI生成的数据集和相关特征来学习ML模型，可以在未看过数据时达到比较好的表现，并且与现有相关方法相比，具有更好的普适性。此外，我们还发现，一种半监督的ML方法，只需要标注约10%的数据，可以达到相对较高的表现，而且在实际应用中更加实际。
</details></li>
</ul>
<hr>
<h2 id="Story-Visualization-by-Online-Text-Augmentation-with-Context-Memory"><a href="#Story-Visualization-by-Online-Text-Augmentation-with-Context-Memory" class="headerlink" title="Story Visualization by Online Text Augmentation with Context Memory"></a>Story Visualization by Online Text Augmentation with Context Memory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07575">http://arxiv.org/abs/2308.07575</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yonseivnl/cmota">https://github.com/yonseivnl/cmota</a></li>
<li>paper_authors: Daechul Ahn, Daneul Kim, Gwangmo Song, Seung Hwan Kim, Honglak Lee, Dongyeop Kang, Jonghyun Choi</li>
<li>for: 提高 Story Visualization  task 的效果，使得模型能够更好地从文本描述中提取视觉细节并在多句文本中保持上下文。</li>
<li>methods: 提出了一种基于 Bi-directional Transformer 框架的新内存体系结构，并在训练过程中使用在线文本增强来生成多个 Pseudo-descriptions 作为补做性的超级vision 。</li>
<li>results: 在 Pororo-SV 和 Flintstones-SV 两个常用的 Story Visualization 测试集上，提出的方法significantly 超过了现有的状态天地，包括 FID、character F1、frame accuracy、BLEU-2&#x2F;3 和 R-precision 等多个维度的metric ，同时 computation complexity 相对较低。<details>
<summary>Abstract</summary>
Story visualization (SV) is a challenging text-to-image generation task for the difficulty of not only rendering visual details from the text descriptions but also encoding a long-term context across multiple sentences. While prior efforts mostly focus on generating a semantically relevant image for each sentence, encoding a context spread across the given paragraph to generate contextually convincing images (e.g., with a correct character or with a proper background of the scene) remains a challenge. To this end, we propose a novel memory architecture for the Bi-directional Transformer framework with an online text augmentation that generates multiple pseudo-descriptions as supplementary supervision during training for better generalization to the language variation at inference. In extensive experiments on the two popular SV benchmarks, i.e., the Pororo-SV and Flintstones-SV, the proposed method significantly outperforms the state of the arts in various metrics including FID, character F1, frame accuracy, BLEU-2/3, and R-precision with similar or less computational complexity.
</details>
<details>
<summary>摘要</summary>
Story visualization (SV) 是一个具有挑战性的文本到图像生成任务，因为不仅需要从文本描述中提取视觉细节，还需要编码长期上下文 Across multiple sentences。 而且，在给出的段落中生成语言上下文感的图像（例如，正确的人物或场景背景）仍然是一个挑战。为此，我们提议一种新的储存架构，用于 Bi-directional Transformer 框架中的在线文本增强，在训练期间生成多个 pseudo-descriptions 作为补充性超级视图，以提高语言变化的泛化性。在两个流行的 SV 测试基准上，即 Pororo-SV 和 Flintstones-SV，我们的方法在多个纪录metric中显著超越了现状的术语，包括 FID、人物 F1、帧精度、BLEU-2/3 和 R-精度，同时具有相似或更低的计算复杂度。
</details></li>
</ul>
<hr>
<h2 id="Action-Class-Relation-Detection-and-Classification-Across-Multiple-Video-Datasets"><a href="#Action-Class-Relation-Detection-and-Classification-Across-Multiple-Video-Datasets" class="headerlink" title="Action Class Relation Detection and Classification Across Multiple Video Datasets"></a>Action Class Relation Detection and Classification Across Multiple Video Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07558">http://arxiv.org/abs/2308.07558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuya Yoshikawa, Yutaro Shigeto, Masashi Shimbo, Akikazu Takeuchi</li>
<li>for: 提高视频人体动作识别的数据集 augmenteion</li>
<li>methods: 使用语言和视觉信息关联类别进行类别关系探测和分类</li>
<li>results: 使用预训练的最新神经网络模型对文本和视频进行预测，可以获得高度预测性能，并且文本标签预测性能高于视频预测，可以将多模态融合以提高预测性能。<details>
<summary>Abstract</summary>
The Meta Video Dataset (MetaVD) provides annotated relations between action classes in major datasets for human action recognition in videos. Although these annotated relations enable dataset augmentation, it is only applicable to those covered by MetaVD. For an external dataset to enjoy the same benefit, the relations between its action classes and those in MetaVD need to be determined. To address this issue, we consider two new machine learning tasks: action class relation detection and classification. We propose a unified model to predict relations between action classes, using language and visual information associated with classes. Experimental results show that (i) pre-trained recent neural network models for texts and videos contribute to high predictive performance, (ii) the relation prediction based on action label texts is more accurate than based on videos, and (iii) a blending approach that combines predictions by both modalities can further improve the predictive performance in some cases.
</details>
<details>
<summary>摘要</summary>
meta 视频集（MetaVD）提供了动作类别之间的注解关系，这些注解关系可以用于视频人体动作识别的数据集进行数据增强。然而，这些注解关系只适用于MetaVD覆盖的数据集。为了解决这个问题，我们考虑了两个新的机器学习任务：动作类别关系检测和分类。我们提议一种统一的模型，可以预测动作类别之间的关系，使用类别相关的语言和视觉信息。实验结果表明：（i）使用最近的预训练神经网络模型对文本和视频进行预测可以获得高度的预测性能，（ii）基于动作标签文本的关系预测比基于视频的预测更准确，（iii）将两种模态的预测结果融合使用可以在一些情况下提高预测性能。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Learning-RL-Augmented-Cold-Start-Frequency-Reduction-in-Serverless-Computing"><a href="#Reinforcement-Learning-RL-Augmented-Cold-Start-Frequency-Reduction-in-Serverless-Computing" class="headerlink" title="Reinforcement Learning (RL) Augmented Cold Start Frequency Reduction in Serverless Computing"></a>Reinforcement Learning (RL) Augmented Cold Start Frequency Reduction in Serverless Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07541">http://arxiv.org/abs/2308.07541</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddharth Agarwal, Maria A. Rodriguez, Rajkumar Buyya</li>
<li>for: 本研究旨在减少 Function-as-a-Service（FaaS）平台上的冷启动频率，通过使用人工智能学习算法来预先启动函数。</li>
<li>methods: 本研究使用了Q学习算法，考虑了函数CPU使用率、现有函数实例和响应失败率等指标，以实现在预期需求的基础上进行前置启动函数。</li>
<li>results: 对比 Kubeless 默认策略和函数保持活动策略，RL 算法能够提高吞吐量达到 8.81%，降低计算负担和资源浪费达到 55% 和 37%，这直接归结于减少冷启动。<details>
<summary>Abstract</summary>
Function-as-a-Service is a cloud computing paradigm offering an event-driven execution model to applications. It features serverless attributes by eliminating resource management responsibilities from developers and offers transparent and on-demand scalability of applications. Typical serverless applications have stringent response time and scalability requirements and therefore rely on deployed services to provide quick and fault-tolerant feedback to clients. However, the FaaS paradigm suffers from cold starts as there is a non-negligible delay associated with on-demand function initialization. This work focuses on reducing the frequency of cold starts on the platform by using Reinforcement Learning. Our approach uses Q-learning and considers metrics such as function CPU utilization, existing function instances, and response failure rate to proactively initialize functions in advance based on the expected demand. The proposed solution was implemented on Kubeless and was evaluated using a normalised real-world function demand trace with matrix multiplication as the workload. The results demonstrate a favourable performance of the RL-based agent when compared to Kubeless' default policy and function keep-alive policy by improving throughput by up to 8.81% and reducing computation load and resource wastage by up to 55% and 37%, respectively, which is a direct outcome of reduced cold starts.
</details>
<details>
<summary>摘要</summary>
Function-as-a-Service 是一种云计算 paradigm，它提供了事件驱动的执行模型，让应用程序在无需管理资源的情况下进行执行。它具有无Serverless特性，从开发者那里消除了资源管理责任，同时提供了透明的升级和缩放应用程序的功能。通常的Serverless应用程序具有严格的响应时间和可扩展性要求，因此它们通常依赖于部署的服务来提供快速的和可靠的反馈给客户端。然而，FaaS paradigm 受到冷启动的限制，即在需求时启动函数时存在非致命的延迟。这种工作将focuses on reducing the frequency of cold starts on the platform by using Reinforcement Learning。我们的方法使用Q-learning，考虑了函数 CPU 利用率、现有函数实例和响应失败率，以进行预先 initialize 函数基于预计的需求。我们的解决方案在 Kubeless 上实现，并通过使用一个 нормализован的实际函数需求轨迹进行评估。结果表明，我们的RL-based 代理比 Kubeless 的默认策略和函数保持活动策略提高了吞吐量，同时降低了计算负担和资源浪费，减少了冷启动的频率，从而提高了系统的性能。
</details></li>
</ul>
<hr>
<h2 id="Domain-Adaptation-via-Minimax-Entropy-for-Real-Bogus-Classification-of-Astronomical-Alerts"><a href="#Domain-Adaptation-via-Minimax-Entropy-for-Real-Bogus-Classification-of-Astronomical-Alerts" class="headerlink" title="Domain Adaptation via Minimax Entropy for Real&#x2F;Bogus Classification of Astronomical Alerts"></a>Domain Adaptation via Minimax Entropy for Real&#x2F;Bogus Classification of Astronomical Alerts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07538">http://arxiv.org/abs/2308.07538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillermo Cabrera-Vives, César Bolivar, Francisco Förster, Alejandra M. Muñoz Arancibia, Manuel Pérez-Carrasco, Esteban Reyes</li>
<li>for: 这个论文是为了研究域域适应（Domain Adaptation）在天文物理数据分析中的应用，以提高天文物理数据分类的准确率。</li>
<li>methods: 该论文使用了四个不同的数据集：HiTS、DES、ATLAS和ZTF，并研究了这些数据集之间的域shift。它使用了一种简单的深度学习分类模型，并通过微调和半supervised深度域适应（MME）来改进模型。</li>
<li>results: 研究发现，只要在目标数据集上有一个或 fewer 的标注项目，就可以使得基本模型得到显著提高。此外，MME模型不会对源数据集的性能产生负面影响。<details>
<summary>Abstract</summary>
Time domain astronomy is advancing towards the analysis of multiple massive datasets in real time, prompting the development of multi-stream machine learning models. In this work, we study Domain Adaptation (DA) for real/bogus classification of astronomical alerts using four different datasets: HiTS, DES, ATLAS, and ZTF. We study the domain shift between these datasets, and improve a naive deep learning classification model by using a fine tuning approach and semi-supervised deep DA via Minimax Entropy (MME). We compare the balanced accuracy of these models for different source-target scenarios. We find that both the fine tuning and MME models improve significantly the base model with as few as one labeled item per class coming from the target dataset, but that the MME does not compromise its performance on the source dataset.
</details>
<details>
<summary>摘要</summary>
时域天文学在实时处理多个大规模数据集方面取得了进展，导致多流机器学习模型的开发。在这个工作中，我们研究天文知讯报警的域 adapted（DA）技术，用于实时/假报警分类。我们使用四个不同的数据集进行研究：HiTS、DES、ATLAS和ZTF。我们研究这些数据集之间的域转换，并通过微调和半supervised深度DA来提高基本模型的性能。我们对不同的源目标场景进行比较，发现两种方法都可以大幅提高基本模型的性能，但MME方法不会优化源数据集的性能。
</details></li>
</ul>
<hr>
<h2 id="KMF-Knowledge-Aware-Multi-Faceted-Representation-Learning-for-Zero-Shot-Node-Classification"><a href="#KMF-Knowledge-Aware-Multi-Faceted-Representation-Learning-for-Zero-Shot-Node-Classification" class="headerlink" title="KMF: Knowledge-Aware Multi-Faceted Representation Learning for Zero-Shot Node Classification"></a>KMF: Knowledge-Aware Multi-Faceted Representation Learning for Zero-Shot Node Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08563">http://arxiv.org/abs/2308.08563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Likang Wu, Junji Jiang, Hongke Zhao, Hao Wang, Defu Lian, Mengdi Zhang, Enhong Chen</li>
<li>for:  Zero-Shot Node Classification (ZNC) task in graph data analysis, to predict nodes from unseen classes.</li>
<li>methods:  Knowledge-Aware Multi-Faceted (KMF) framework that enhances label semantics via extracted KG-based topics, and reconstructs node content to a topic-level representation.</li>
<li>results:  Extensive experiments on several public graph datasets, with an application of zero-shot cross-domain recommendation, demonstrating the effectiveness and generalization of KMF compared to state-of-the-art baselines.Here is the same information in Simplified Chinese:</li>
<li>for:  Zero-Shot Node Classification (ZNC) 任务在图数据分析中，预测从训练过程中未经见过的节点。</li>
<li>methods:  Knowledge-Aware Multi-Faceted (KMF) 框架，通过提取的知识图(KG)来增强标签 semantics，并将节点内容重建到一个话题级别表示。</li>
<li>results: 在多个公共图据集上进行了广泛的实验，并设计了跨领域零shot推荐应用，比较了状态的基elines。<details>
<summary>Abstract</summary>
Recently, Zero-Shot Node Classification (ZNC) has been an emerging and crucial task in graph data analysis. This task aims to predict nodes from unseen classes which are unobserved in the training process. Existing work mainly utilizes Graph Neural Networks (GNNs) to associate features' prototypes and labels' semantics thus enabling knowledge transfer from seen to unseen classes. However, the multi-faceted semantic orientation in the feature-semantic alignment has been neglected by previous work, i.e. the content of a node usually covers diverse topics that are relevant to the semantics of multiple labels. It's necessary to separate and judge the semantic factors that tremendously affect the cognitive ability to improve the generality of models. To this end, we propose a Knowledge-Aware Multi-Faceted framework (KMF) that enhances the richness of label semantics via the extracted KG (Knowledge Graph)-based topics. And then the content of each node is reconstructed to a topic-level representation that offers multi-faceted and fine-grained semantic relevancy to different labels. Due to the particularity of the graph's instance (i.e., node) representation, a novel geometric constraint is developed to alleviate the problem of prototype drift caused by node information aggregation. Finally, we conduct extensive experiments on several public graph datasets and design an application of zero-shot cross-domain recommendation. The quantitative results demonstrate both the effectiveness and generalization of KMF with the comparison of state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
近期，零shot节点分类（ZNC）在图数据分析中变得越来越重要。这个任务的目标是从训练过程中未见过的类型中预测节点。现有的工作主要利用图神经网络（GNNs）将特征抽象和标签含义相关联，从而实现知识传递从见到未见类型。然而，先前的工作忽略了多面性 semantic orientation的问题，即节点的内容通常涉及多个相关的标签 semantics。为了提高模型的通用性，我们提出了知识注意力多面性框架（KMF），该框架通过提取的知识图（KG）基于主题来增强标签 semantics的 ricness。然后，每个节点的内容被重建为主题级别的表示，以提供多面性和细化的semantic relevancy。由于图的实例（即节点）表示的特殊性，我们开发了一种新的 геометрических约束，以解决由节点信息汇集引起的prototype drift问题。最后，我们进行了多个公共图据集的广泛实验，并设计了零shot跨领域推荐应用。实验结果表明，KMF具有与状态 искусственныйbaseline的效果和通用性。
</details></li>
</ul>
<hr>
<h2 id="Nonlinearity-Feedback-and-Uniform-Consistency-in-Causal-Structural-Learning"><a href="#Nonlinearity-Feedback-and-Uniform-Consistency-in-Causal-Structural-Learning" class="headerlink" title="Nonlinearity, Feedback and Uniform Consistency in Causal Structural Learning"></a>Nonlinearity, Feedback and Uniform Consistency in Causal Structural Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07520">http://arxiv.org/abs/2308.07520</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuyan Wang</li>
<li>for: 这个论文的目的是找到自动搜索方法，以便从观察数据中学习 causal structure。</li>
<li>methods: 这个论文使用的方法包括提出一种弱 faithfulness 定义，以及一种修改后的 causal discovery 算法，以relaxing Various simplification assumptions，使其适用于更广泛的 causal mechanism 和统计现象。</li>
<li>results: 这个论文的结果表明，使用修改后的 causal discovery 算法，可以在不同的 distributive 下学习 causal structure，并且可以找到 latent variables 的 causal connections。<details>
<summary>Abstract</summary>
The goal of Causal Discovery is to find automated search methods for learning causal structures from observational data. In some cases all variables of the interested causal mechanism are measured, and the task is to predict the effects one measured variable has on another. In contrast, sometimes the variables of primary interest are not directly observable but instead inferred from their manifestations in the data. These are referred to as latent variables. One commonly known example is the psychological construct of intelligence, which cannot directly measured so researchers try to assess through various indicators such as IQ tests. In this case, casual discovery algorithms can uncover underlying patterns and structures to reveal the causal connections between the latent variables and between the latent and observed variables. This thesis focuses on two questions in causal discovery: providing an alternative definition of k-Triangle Faithfulness that (i) is weaker than strong faithfulness when applied to the Gaussian family of distributions, (ii) can be applied to non-Gaussian families of distributions, and (iii) under the assumption that the modified version of Strong Faithfulness holds, can be used to show the uniform consistency of a modified causal discovery algorithm; relaxing the sufficiency assumption to learn causal structures with latent variables. Given the importance of inferring cause-and-effect relationships for understanding and forecasting complex systems, the work in this thesis of relaxing various simplification assumptions is expected to extend the causal discovery method to be applicable in a wider range with diversified causal mechanism and statistical phenomena.
</details>
<details>
<summary>摘要</summary>
目标是找到自动搜寻方法，以学习 causal 结构从观察数据中。在某些情况下，所有变量的 interested  causal mechanism 都被测量，需要预测一个测量到的变量对另一个变量的效应。相反，有时变量的首选变量不直接可观察，而是从数据中推导出来的。这些变量被称为 latent 变量。一个常见的例子是心理学中的智商，无法直接测量，因此研究人员会通过不同的指标，如 IQ 测试，来评估。在这种情况下， causal discovery 算法可以揭示下面的 causal 连接和 latent 变量与观察变量之间的连接。本论文关注两个问题在 causal discovery：提供一个 alternative 定义，可以用于 Gaussian 家族的分布，并且可以应用于非 Gaussian 分布家族，以及在 modified 版本的 Strong Faithfulness 假设下，可以用来证明一种修改后的 causal discovery 算法的均匀一致性。减少 sufficiency 假设，以学习包含 latent 变量的 causal 结构。由于推导 causal 关系的重要性，以上工作的扩展 causal discovery 方法的应用范围，预计将能够扩展到更多的 causal 机制和统计现象。
</details></li>
</ul>
<hr>
<h2 id="Boosting-Semi-Supervised-Learning-by-bridging-high-and-low-confidence-predictions"><a href="#Boosting-Semi-Supervised-Learning-by-bridging-high-and-low-confidence-predictions" class="headerlink" title="Boosting Semi-Supervised Learning by bridging high and low-confidence predictions"></a>Boosting Semi-Supervised Learning by bridging high and low-confidence predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07509">http://arxiv.org/abs/2308.07509</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khanh-Binh Nguyen, Joon-Sung Yang</li>
<li>for: 本研究旨在解决 Pseudo-labeling 方法中的三大问题，提高 semi-supervised learning 的性能和泛化能力。</li>
<li>methods: 本研究提出了一种新的 ReFixMatch 方法，通过全面利用无标示数据来提高模型的泛化能力和性能。</li>
<li>results: 对于 ImageNet 图像集，ReFixMatch 方法实现了 41.05% 的 top-1 准确率，超过 FixMatch 和当前状态的方法。<details>
<summary>Abstract</summary>
Pseudo-labeling is a crucial technique in semi-supervised learning (SSL), where artificial labels are generated for unlabeled data by a trained model, allowing for the simultaneous training of labeled and unlabeled data in a supervised setting. However, several studies have identified three main issues with pseudo-labeling-based approaches. Firstly, these methods heavily rely on predictions from the trained model, which may not always be accurate, leading to a confirmation bias problem. Secondly, the trained model may be overfitted to easy-to-learn examples, ignoring hard-to-learn ones, resulting in the \textit{"Matthew effect"} where the already strong become stronger and the weak weaker. Thirdly, most of the low-confidence predictions of unlabeled data are discarded due to the use of a high threshold, leading to an underutilization of unlabeled data during training. To address these issues, we propose a new method called ReFixMatch, which aims to utilize all of the unlabeled data during training, thus improving the generalizability of the model and performance on SSL benchmarks. Notably, ReFixMatch achieves 41.05\% top-1 accuracy with 100k labeled examples on ImageNet, outperforming the baseline FixMatch and current state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
假标注是SSL中的一种重要技术，其中一个训练过的模型将生成对未标注数据的人工标注，允许同时在指导下进行 Label 和无标注数据的同时训练。然而，一些研究发现了假标注基于方法的三大问题。首先，这些方法强调训练过的模型的预测结果，可能并不准确，导致确认偏见问题。其次，训练过的模型可能会过拟合易学习的示例，忽略困难学习的示例，从而导致"马太效应"，即已经强的变得更强，弱的变得更弱。最后，大多数无标注数据的低信度预测被抛弃，因为使用高reshold，导致在训练中未充分利用无标注数据。为了解决这些问题，我们提出了一种新的方法 called ReFixMatch，它计划在训练中利用所有的无标注数据，从而提高模型的一般化性和SSL Benchmark中的性能。各种方法的实验结果表明，ReFixMatch可以与 FixMatch 和当前状态的方法相比，在 ImageNet 上达到41.05% 的 top-1 准确率，使用 100k 标注示例。
</details></li>
</ul>
<hr>
<h2 id="Detecting-The-Corruption-Of-Online-Questionnaires-By-Artificial-Intelligence"><a href="#Detecting-The-Corruption-Of-Online-Questionnaires-By-Artificial-Intelligence" class="headerlink" title="Detecting The Corruption Of Online Questionnaires By Artificial Intelligence"></a>Detecting The Corruption Of Online Questionnaires By Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07499">http://arxiv.org/abs/2308.07499</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin Lebrun, Sharon Temtsin, Andrew Vonasch, Christoph Bartneck</li>
<li>for: 这个研究是为了检测在在线问卷中使用人工智能生成的文本是否可以被识别出来。</li>
<li>methods: 这个研究使用了人类和自动AI检测系统来检测文本的作者性。</li>
<li>results: 人类参与者的识别率为76%，但是这还不够保证数据质量。自动AI检测系统完全无用。如果AI变得太普遍，那么检测伪造提交的成本将超过在线问卷的 beneficial。这问题只能由群组平台系统上解决。<details>
<summary>Abstract</summary>
Online questionnaires that use crowd-sourcing platforms to recruit participants have become commonplace, due to their ease of use and low costs. Artificial Intelligence (AI) based Large Language Models (LLM) have made it easy for bad actors to automatically fill in online forms, including generating meaningful text for open-ended tasks. These technological advances threaten the data quality for studies that use online questionnaires. This study tested if text generated by an AI for the purpose of an online study can be detected by both humans and automatic AI detection systems. While humans were able to correctly identify authorship of text above chance level (76 percent accuracy), their performance was still below what would be required to ensure satisfactory data quality. Researchers currently have to rely on the disinterest of bad actors to successfully use open-ended responses as a useful tool for ensuring data quality. Automatic AI detection systems are currently completely unusable. If AIs become too prevalent in submitting responses then the costs associated with detecting fraudulent submissions will outweigh the benefits of online questionnaires. Individual attention checks will no longer be a sufficient tool to ensure good data quality. This problem can only be systematically addressed by crowd-sourcing platforms. They cannot rely on automatic AI detection systems and it is unclear how they can ensure data quality for their paying clients.
</details>
<details>
<summary>摘要</summary>
在线问卷使用人群投票平台Recruit participants变得普遍，因为它们的使用容易和成本低。人工智能（AI）基于大语言模型（LLM）使得坏actor可以自动填充在线表单，包括生成有意义的文本 для开放式任务。这些技术进步威胁在线问卷中的数据质量。这个研究测试了一I生成的文本是否可以由人类和自动AI检测系统检测出来。人类参与者的准确率为76%，但是这还不够保证数据质量的满意度。研究人员目前需要坏actor的无自身利益来成功使用开放式回答。自动AI检测系统目前不可用。如果AI变得太普遍，则提交假答案的成本将超过在线问卷的利益。人类注意性检查不再是一个有效的数据质量保证工具。这个问题只能通过人群投票平台系统地解决。它们不能依靠自动AI检测系统，而且不清楚如何保证付出客户的数据质量。
</details></li>
</ul>
<hr>
<h2 id="DREAMWALKER-Mental-Planning-for-Continuous-Vision-Language-Navigation"><a href="#DREAMWALKER-Mental-Planning-for-Continuous-Vision-Language-Navigation" class="headerlink" title="DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation"></a>DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07498">http://arxiv.org/abs/2308.07498</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HanqingWangAI/Dreamwalker">https://github.com/HanqingWangAI/Dreamwalker</a></li>
<li>paper_authors: Hanqing Wang, Wei Liang, Luc Van Gool, Wenguan Wang<br>for:* DREAMWALKER is a world model-based VLN-CE agent that leverages mental experiments to plan and make strategic decisions in a freely traversable environment.methods:* The world model is built to summarize the visual, topological, and dynamic properties of the environment into a discrete, structured, and compact representation.* DREAMWALKER simulates and evaluates possible plans entirely in the internal abstract world before executing costly actions.results:* Extensive experiments and ablation studies on VLN-CE dataset confirm the effectiveness of the proposed approach and outline fruitful directions for future work.Here’s the Chinese translation:for:* DREAMWALKER 是一个基于世界模型的 VLN-CE 代理，通过MENTAL EXPERIMENTS 进行规划和策略决策在一个自由可行的环境中。methods:* 世界模型将环境的视觉、拓扑和动态特性总结为一个简化、结构化和压缩的表示。* DREAMWALKER 在内部抽象世界中 simulate 和评估可能的计划，以避免在真实世界中的浪费。results:* 对 VLN-CE 数据集的广泛实验和减少研究表明，提议的方法有效，并提供了未来工作的有价值导向。<details>
<summary>Abstract</summary>
VLN-CE is a recently released embodied task, where AI agents need to navigate a freely traversable environment to reach a distant target location, given language instructions. It poses great challenges due to the huge space of possible strategies. Driven by the belief that the ability to anticipate the consequences of future actions is crucial for the emergence of intelligent and interpretable planning behavior, we propose DREAMWALKER -- a world model based VLN-CE agent. The world model is built to summarize the visual, topological, and dynamic properties of the complicated continuous environment into a discrete, structured, and compact representation. DREAMWALKER can simulate and evaluate possible plans entirely in such internal abstract world, before executing costly actions. As opposed to existing model-free VLN-CE agents simply making greedy decisions in the real world, which easily results in shortsighted behaviors, DREAMWALKER is able to make strategic planning through large amounts of ``mental experiments.'' Moreover, the imagined future scenarios reflect our agent's intention, making its decision-making process more transparent. Extensive experiments and ablation studies on VLN-CE dataset confirm the effectiveness of the proposed approach and outline fruitful directions for future work.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ST-MLP-A-Cascaded-Spatio-Temporal-Linear-Framework-with-Channel-Independence-Strategy-for-Traffic-Forecasting"><a href="#ST-MLP-A-Cascaded-Spatio-Temporal-Linear-Framework-with-Channel-Independence-Strategy-for-Traffic-Forecasting" class="headerlink" title="ST-MLP: A Cascaded Spatio-Temporal Linear Framework with Channel-Independence Strategy for Traffic Forecasting"></a>ST-MLP: A Cascaded Spatio-Temporal Linear Framework with Channel-Independence Strategy for Traffic Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07496">http://arxiv.org/abs/2308.07496</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zepu Wang, Yuqi Nie, Peng Sun, Nam H. Nguyen, John Mulvey, H. Vincent Poor</li>
<li>for: 预测交通流量管理在智能交通系统（ITS）中的优化</li>
<li>methods: 使用简单的多层感知机制（MLP）模组和线性层，同时考虑时间资讯、空间资讯和预先定义的路径结构</li>
<li>results: 与现有的STGNNs和其他模型相比，ST-MLP表现出更高的准确性和computational efficiency<details>
<summary>Abstract</summary>
The criticality of prompt and precise traffic forecasting in optimizing traffic flow management in Intelligent Transportation Systems (ITS) has drawn substantial scholarly focus. Spatio-Temporal Graph Neural Networks (STGNNs) have been lauded for their adaptability to road graph structures. Yet, current research on STGNNs architectures often prioritizes complex designs, leading to elevated computational burdens with only minor enhancements in accuracy. To address this issue, we propose ST-MLP, a concise spatio-temporal model solely based on cascaded Multi-Layer Perceptron (MLP) modules and linear layers. Specifically, we incorporate temporal information, spatial information and predefined graph structure with a successful implementation of the channel-independence strategy - an effective technique in time series forecasting. Empirical results demonstrate that ST-MLP outperforms state-of-the-art STGNNs and other models in terms of accuracy and computational efficiency. Our finding encourages further exploration of more concise and effective neural network architectures in the field of traffic forecasting.
</details>
<details>
<summary>摘要</summary>
“智能交通系统（ITS）中的实时流量预测 Criticality has drawn substantial scholarly focus, and Spatio-Temporal Graph Neural Networks (STGNNs) have been praised for their adaptability to road graph structures. However, current research on STGNNs architectures often prioritizes complex designs, leading to elevated computational burdens with only minor enhancements in accuracy. To address this issue, we propose ST-MLP, a concise spatio-temporal model based solely on cascaded Multi-Layer Perceptron (MLP) modules and linear layers. Specifically, we incorporate temporal information, spatial information, and predefined graph structure with a successful implementation of the channel-independence strategy - an effective technique in time series forecasting. Empirical results demonstrate that ST-MLP outperforms state-of-the-art STGNNs and other models in terms of accuracy and computational efficiency. Our finding encourages further exploration of more concise and effective neural network architectures in the field of traffic forecasting.”Here's a word-for-word translation of the text into Simplified Chinese:“智能交通系统（ITS）中的实时流量预测 Criticality 吸引了大量的学术关注，而 Spatio-Temporal Graph Neural Networks (STGNNs) 被赞誉为路径 граosph 结构的适应性。然而，目前 STGNNs 架构设计中经常优先过往复杂的设计，导致计算成本增加，仅对准确性有小量改善。为解决这个问题，我们提出 ST-MLP，一个简洁的 spatio-temporal 模型，基于弹性 Multi-Layer Perceptron (MLP) 模组和线性层。具体而言，我们将时间信息、空间信息和预设的路径结构融合在一起，并成功地实现了通道独立策略 - 一种有效的时间序列预测技术。实验结果显示，ST-MLP 在准确性和计算效率方面都超过了现有的 STGNNs 和其他模型。我们的发现鼓励我们继续探索更简洁和有效的神经网络架构在交通预测领域。”
</details></li>
</ul>
<hr>
<h2 id="Omega-Regular-Reward-Machines"><a href="#Omega-Regular-Reward-Machines" class="headerlink" title="Omega-Regular Reward Machines"></a>Omega-Regular Reward Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07469">http://arxiv.org/abs/2308.07469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ernst Moritz Hahn, Mateo Perez, Sven Schewe, Fabio Somenzi, Ashutosh Trivedi, Dominik Wojtczak</li>
<li>for: 这篇论文旨在探讨在强化学习中设计合适的奖励机制是如何实现更高效的行为学习。</li>
<li>methods: 这篇论文使用了奖励机器和ωRegular语言来表达非马歇维奖励，以满足更复杂的学习目标。</li>
<li>results: 研究人员通过设计了一种基于模型自由RL算法的ε-优化策略来评估奖励机器的效果，并通过实验证明了该方法的可行性和有效性。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) is a powerful approach for training agents to perform tasks, but designing an appropriate reward mechanism is critical to its success. However, in many cases, the complexity of the learning objectives goes beyond the capabilities of the Markovian assumption, necessitating a more sophisticated reward mechanism. Reward machines and omega-regular languages are two formalisms used to express non-Markovian rewards for quantitative and qualitative objectives, respectively. This paper introduces omega-regular reward machines, which integrate reward machines with omega-regular languages to enable an expressive and effective reward mechanism for RL. We present a model-free RL algorithm to compute epsilon-optimal strategies against omega-egular reward machines and evaluate the effectiveness of the proposed algorithm through experiments.
</details>
<details>
<summary>摘要</summary>
利用强化学习（RL）训练代理人完成任务，但设计合适的奖励机制是关键。然而，在许多情况下，学习目标的复杂性超出了Markov预测的能力，需要更加复杂的奖励机制。奖励机器和ωRegular语言是两种用于表达非Markov奖励的形式，分别用于量化和质量目标。本文提出了ωRegular奖励机器，可以将奖励机器与ωRegular语言集成，以实现RL中的表达力和有效性。我们提出了一种无模型RL算法，可以计算ε优策略对ωRegular奖励机器，并通过实验评估提案的效果。
</details></li>
</ul>
<hr>
<h2 id="Playing-with-Words-Comparing-the-Vocabulary-and-Lexical-Richness-of-ChatGPT-and-Humans"><a href="#Playing-with-Words-Comparing-the-Vocabulary-and-Lexical-Richness-of-ChatGPT-and-Humans" class="headerlink" title="Playing with Words: Comparing the Vocabulary and Lexical Richness of ChatGPT and Humans"></a>Playing with Words: Comparing the Vocabulary and Lexical Richness of ChatGPT and Humans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07462">http://arxiv.org/abs/2308.07462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pedro Reviriego, Javier Conde, Elena Merino-Gómez, Gonzalo Martínez, José Alberto Hernández</li>
<li>for: This study aims to compare the vocabulary and lexical richness of ChatGPT and human responses when performing the same tasks.</li>
<li>methods: The study uses two datasets containing answers to different types of questions answered by ChatGPT and humans, and analyzes the number of distinct words and lexical richness of each.</li>
<li>results: Preliminary results show that ChatGPT tends to use fewer distinct words and lower lexical richness than humans.Here’s the same information in Simplified Chinese:</li>
<li>for: 这项研究目的是对 chatGPT 和人类回答相同任务的 vocabulary 和语言丰富度进行比较。</li>
<li>methods: 研究使用两个数据集，每个数据集包含不同类型的问题，chatGPT 和人类回答的答案，并对每个数据集进行 vocabulary 和语言丰富度的分析。</li>
<li>results: 初步结果显示，chatGPT 使用的单词数量和语言丰富度比人类低。<details>
<summary>Abstract</summary>
The introduction of Artificial Intelligence (AI) generative language models such as GPT (Generative Pre-trained Transformer) and tools such as ChatGPT has triggered a revolution that can transform how text is generated. This has many implications, for example, as AI-generated text becomes a significant fraction of the text in many disciplines, would this have an effect on the language capabilities of readers and also on the training of newer AI tools? Would it affect the evolution of languages? Focusing on one specific aspect of the language: words; will the use of tools such as ChatGPT increase or reduce the vocabulary used or the lexical richness (understood as the number of different words used in a written or oral production) when writing a given text? This has implications for words, as those not included in AI-generated content will tend to be less and less popular and may eventually be lost. In this work, we perform an initial comparison of the vocabulary and lexical richness of ChatGPT and humans when performing the same tasks. In more detail, two datasets containing the answers to different types of questions answered by ChatGPT and humans are used, and the analysis shows that ChatGPT tends to use fewer distinct words and lower lexical richness than humans. These results are very preliminary and additional datasets and ChatGPT configurations have to be evaluated to extract more general conclusions. Therefore, further research is needed to understand how the use of ChatGPT and more broadly generative AI tools will affect the vocabulary and lexical richness in different types of text and languages.
</details>
<details>
<summary>摘要</summary>
人工智能语言生成模型如GPT（生成预训练变换器）和ChatGPT等工具的出现已经引发了一场革命，这将对文本生成方式产生深远的影响。这有很多意义，例如，如果人工智能生成的文本在多个领域中占据了一定的比例，会对读者的语言能力和 newer AI工具的训练产生影响吗？会对语言演化产生影响？对于语言中的一个方面来说，使用工具如ChatGPT会增加或减少在写作文本时使用的词汇数量和语言 ricness（理解为在书面或口语中使用的不同词汇数量）？这有关词汇的影响，那些不包括在人工智能生成内容中将变得更加少用，并最终可能产生失传。在这项工作中，我们对ChatGPT和人类的答案集进行了初步比较，结果显示ChatGPT使用的词汇数量和语言 ricness较低。这些结果是非常初步的，需要更多的数据和ChatGPT配置来提取更广泛的结论。因此，进一步的研究是必要的，以了解人工智能生成工具在不同类型的文本和语言中的词汇和语言 ricness的影响。
</details></li>
</ul>
<hr>
<h2 id="Inductive-Knowledge-Graph-Completion-with-GNNs-and-Rules-An-Analysis"><a href="#Inductive-Knowledge-Graph-Completion-with-GNNs-and-Rules-An-Analysis" class="headerlink" title="Inductive Knowledge Graph Completion with GNNs and Rules: An Analysis"></a>Inductive Knowledge Graph Completion with GNNs and Rules: An Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07942">http://arxiv.org/abs/2308.07942</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anilakash/indkgc">https://github.com/anilakash/indkgc</a></li>
<li>paper_authors: Akash Anil, Víctor Gutiérrez-Basulto, Yazmín Ibañéz-García, Steven Schockaert</li>
<li>for: 这个论文的目的是研究 inductive knowledge graph completion 任务，即从训练图像上学习推理规则，并将其应用于独立的测试图像上进行预测。</li>
<li>methods: 这个论文使用了规则基于的方法，但是在实践中，这些方法表现非常差。作者认为这是因为两个因素：（i）不可能的实体没有被评估，（ii）只考虑最有用的路径来确定链接预测答案的信任程度。作者们为了解决这些问题，研究了一些变种方法，包括一些专门 Addressing 这些问题。</li>
<li>results: 研究发现，这些变种方法可以几乎与 NBFNet 相当的性能，而且它们只使用了 NBFNet 使用的一小部分证据。此外，作者们还发现，一个further variant，即考虑整个知识图，可以一直高于 NBFNet 的性能。<details>
<summary>Abstract</summary>
The task of inductive knowledge graph completion requires models to learn inference patterns from a training graph, which can then be used to make predictions on a disjoint test graph. Rule-based methods seem like a natural fit for this task, but in practice they significantly underperform state-of-the-art methods based on Graph Neural Networks (GNNs), such as NBFNet. We hypothesise that the underperformance of rule-based methods is due to two factors: (i) implausible entities are not ranked at all and (ii) only the most informative path is taken into account when determining the confidence in a given link prediction answer. To analyse the impact of these factors, we study a number of variants of a rule-based approach, which are specifically aimed at addressing the aforementioned issues. We find that the resulting models can achieve a performance which is close to that of NBFNet. Crucially, the considered variants only use a small fraction of the evidence that NBFNet relies on, which means that they largely keep the interpretability advantage of rule-based methods. Moreover, we show that a further variant, which does look at the full KG, consistently outperforms NBFNet.
</details>
<details>
<summary>摘要</summary>
任务是 inductive 知识图完成需要模型学习从训练图中学习推理模式，然后用于测试图上预测。 规则式方法看起来很自然地适合这个任务，但在实践中它们实际上显著地下表现。我们认为这是因为两个因素：（i）不可能的实体没有被排序，（ii）只考虑测试图中最有用的路径来确定链接预测答案的信任度。为了分析这些因素的影响，我们研究了一些 variants 的规则式方法，它们专门解决这些问题。我们发现这些模型可以达到与 NBFNet 相似的性能，而且它们只使用了 NBFNet 使用的一小部分证据，这意味着它们保持了解释性的优势。此外，我们还显示了一种 further 的变体，它会在全知识图上进行预测，并一直表现出perform better 于 NBFNet。
</details></li>
</ul>
<hr>
<h2 id="Artificial-Intelligence-for-Smart-Transportation"><a href="#Artificial-Intelligence-for-Smart-Transportation" class="headerlink" title="Artificial Intelligence for Smart Transportation"></a>Artificial Intelligence for Smart Transportation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07457">http://arxiv.org/abs/2308.07457</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/SarmisthaDutta/application-of-artificial-Intelligence-for-future-sustainable-smart-city">https://github.com/SarmisthaDutta/application-of-artificial-Intelligence-for-future-sustainable-smart-city</a></li>
<li>paper_authors: Michael Wilbur, Amutheezan Sivagnanam, Afiya Ayman, Samitha Samaranayeke, Abhishek Dubey, Aron Laszka</li>
<li>for: 提高公共交通系统的效率和使用率，以满足社会发展和人类价值创造的需求。</li>
<li>methods: 利用人工智能技术，提供数据驱动的智能交通系统，包括数据收集、人工智能决策支持和计算机科学问题解决方案。</li>
<li>results: 通过对交通系统的数据分析和人工智能技术应用，提高交通系统的效率和使用率，为社会发展和人类价值创造提供可能性。<details>
<summary>Abstract</summary>
There are more than 7,000 public transit agencies in the U.S. (and many more private agencies), and together, they are responsible for serving 60 billion passenger miles each year. A well-functioning transit system fosters the growth and expansion of businesses, distributes social and economic benefits, and links the capabilities of community members, thereby enhancing what they can accomplish as a society. Since affordable public transit services are the backbones of many communities, this work investigates ways in which Artificial Intelligence (AI) can improve efficiency and increase utilization from the perspective of transit agencies. This book chapter discusses the primary requirements, objectives, and challenges related to the design of AI-driven smart transportation systems. We focus on three major topics. First, we discuss data sources and data. Second, we provide an overview of how AI can aid decision-making with a focus on transportation. Lastly, we discuss computational problems in the transportation domain and AI approaches to these problems.
</details>
<details>
<summary>摘要</summary>
美国有超过7,000个公共交通机构，以及许多私人机构，每年共运送600亿公里的乘客。一个健全的公共交通系统会促进企业增长和扩张、分配社会和经济的利益，并将社区成员的能力相互连接，从而提高社会的可能性。由于公共交通服务是许多社区的基础设施，这项工作研究了使用人工智能（AI）提高公共交通系统的效率和使用率。本章讨论了智能交通系统设计的主要需求、目标和挑战。我们主要讨论以下三个主题：第一，讨论数据来源和数据；第二，介绍transportation领域中AI助于决策的方法；第三，讨论交通领域的计算问题和AI应用于这些问题。
</details></li>
</ul>
<hr>
<h2 id="GRU-D-Weibull-A-Novel-Real-Time-Individualized-Endpoint-Prediction"><a href="#GRU-D-Weibull-A-Novel-Real-Time-Individualized-Endpoint-Prediction" class="headerlink" title="GRU-D-Weibull: A Novel Real-Time Individualized Endpoint Prediction"></a>GRU-D-Weibull: A Novel Real-Time Individualized Endpoint Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07452">http://arxiv.org/abs/2308.07452</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoyang Ruan, Liwei Wang, Charat Thongprayoon, Wisit Cheungpasitporn, Hongfang Liu</li>
<li>for: 这份研究的目的是发展一个新的方法，即GRU-D-Weibull，用于预测个人水平的终点和时间至终点。这种方法可以实现实时个人化终点预测和人口水平的风险管理。</li>
<li>methods: 这份研究使用了一个新的方法，即GRU-D-Weibull，它结合了闸道运算和衰减（GRU-D）来模型Weibull分布。这种方法可以实现实时个人化终点预测和人口水平的风险管理。</li>
<li>results: 这份研究发现，GRU-D-Weibull方法在终点预测中表现出色，C-指数约为0.7，并在4.3年的追踪期间持续提高到约0.77。这与随机生存树的表现相似。GRU-D-Weibull方法在L1损失上显示出了优秀的表现，与其他方法相比，具有较低的损失值。此外，GRU-D-Weibull方法还能够对终点预测进行时间轴上的精确调整。<details>
<summary>Abstract</summary>
Accurate prediction models for individual-level endpoints and time-to-endpoints are crucial in clinical practice. In this study, we propose a novel approach, GRU-D-Weibull, which combines gated recurrent units with decay (GRU-D) to model the Weibull distribution. Our method enables real-time individualized endpoint prediction and population-level risk management. Using a cohort of 6,879 patients with stage 4 chronic kidney disease (CKD4), we evaluated the performance of GRU-D-Weibull in endpoint prediction. The C-index of GRU-D-Weibull was ~0.7 at the index date and increased to ~0.77 after 4.3 years of follow-up, similar to random survival forest. Our approach achieved an absolute L1-loss of ~1.1 years (SD 0.95) at the CKD4 index date and a minimum of ~0.45 years (SD0.3) at 4 years of follow-up, outperforming competing methods significantly. GRU-D-Weibull consistently constrained the predicted survival probability at the time of an event within a smaller and more fixed range compared to other models throughout the follow-up period. We observed significant correlations between the error in point estimates and missing proportions of input features at the index date (correlations from ~0.1 to ~0.3), which diminished within 1 year as more data became available. By post-training recalibration, we successfully aligned the predicted and observed survival probabilities across multiple prediction horizons at different time points during follow-up. Our findings demonstrate the considerable potential of GRU-D-Weibull as the next-generation architecture for endpoint risk management, capable of generating various endpoint estimates for real-time monitoring using clinical data.
</details>
<details>
<summary>摘要</summary>
准确的预测模型对个体级别终点和时间至终点是临床实践中非常重要。在这项研究中，我们提出了一种新的方法，即GRU-D-Weibull，它将闭合杂列单元（GRU-D）与减速分布（Weibull distribution）结合在一起。我们的方法可以实现实时个体化终点预测和人口级别风险管理。使用6,879名CKD4阶段4慢性肾病患者的 cohort，我们评估了GRU-D-Weibull在终点预测方面的性能。GRU-D-Weibull的C-指数在指定日期为 aproximadamente 0.7，而在4.3年后跟踪中，它提高至 aproximadamente 0.77，与随机生存森林类似。我们的方法实现了终点预测的L1损失约为1.1年（SD 0.95）在CKD4指定日期，并在4年后跟踪中达到了约0.45年（SD0.3）的最小值，与其他方法相比显著性能更高。GRU-D-Weibull通常在终点预测过程中压缩预测生存概率的范围，与其他模型在跟踪期间保持相对更小和更固定的范围相比，表现更稳定。我们发现在指定日期的输入特征批量缺失率和预测点估值的误差之间存在 statistically significant 的相关性（从 approximately 0.1到 approximately 0.3），这种相关性随着时间的推移而减少。通过后期重新训练，我们成功地将预测和观察到的生存概率相互对应，并在不同的跟踪时间点 durante el seguimiento。我们的发现表明GRU-D-Weibull可能是下一代结构，它可以使用临床数据生成多种终点估计，用于实时监测终点风险。
</details></li>
</ul>
<hr>
<h2 id="Open-set-Face-Recognition-using-Ensembles-trained-on-Clustered-Data"><a href="#Open-set-Face-Recognition-using-Ensembles-trained-on-Clustered-Data" class="headerlink" title="Open-set Face Recognition using Ensembles trained on Clustered Data"></a>Open-set Face Recognition using Ensembles trained on Clustered Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07445">http://arxiv.org/abs/2308.07445</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rafael Henrique Vareto, William Robson Schwartz</li>
<li>for: 该论文目的是开发一种可扩展的开放集面Recognition方法，能够处理大量未知人脸。</li>
<li>methods: 该方法使用 clustering 和多个二进制学习算法，对查询人脸样本进行分类，并使用 ensemble 提高预测性能。</li>
<li>results: 实验结果表明，该方法可以在大量人脸库中实现竞争力强的表现，即使面临大量未知人脸。Here’s the English version for reference:</li>
<li>for: The purpose of this paper is to develop a scalable open-set face recognition approach that can handle large numbers of unfamiliar faces.</li>
<li>methods: The method uses clustering and an ensemble of binary learning algorithms to classify query face samples and retrieve their correct identity from a gallery of hundreds or thousands of subjects.</li>
<li>results: Experimental results show that the approach can achieve competitive performance even when targeting scalability, handling large numbers of unfamiliar faces.<details>
<summary>Abstract</summary>
Open-set face recognition describes a scenario where unknown subjects, unseen during the training stage, appear on test time. Not only it requires methods that accurately identify individuals of interest, but also demands approaches that effectively deal with unfamiliar faces. This work details a scalable open-set face identification approach to galleries composed of hundreds and thousands of subjects. It is composed of clustering and an ensemble of binary learning algorithms that estimates when query face samples belong to the face gallery and then retrieves their correct identity. The approach selects the most suitable gallery subjects and uses the ensemble to improve prediction performance. We carry out experiments on well-known LFW and YTF benchmarks. Results show that competitive performance can be achieved even when targeting scalability.
</details>
<details>
<summary>摘要</summary>
开放集 face recognition 描述一种enario，在训练阶段未经见过的不明人脸出现在测试阶段。不仅需要准确地识别关注人脸，而且也需要采用有效地处理未知脸的方法。这篇文章介绍了一种可扩展的开放集 face 识别方法，可以对包括百万多个主题的人脸库进行识别。该方法包括分集和一个 ensemble of 二分学习算法，以便在测试阶段确定查询脸样本是否属于人脸库，并且使用ensemble提高预测性能。我们在well-known LFW和YTF标准准样本上进行了实验，结果表明，即使targeting可扩展性，也可以实现竞争性的表现。
</details></li>
</ul>
<hr>
<h2 id="The-Performance-of-Transferability-Metrics-does-not-Translate-to-Medical-Tasks"><a href="#The-Performance-of-Transferability-Metrics-does-not-Translate-to-Medical-Tasks" class="headerlink" title="The Performance of Transferability Metrics does not Translate to Medical Tasks"></a>The Performance of Transferability Metrics does not Translate to Medical Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07444">http://arxiv.org/abs/2308.07444</a></li>
<li>repo_url: None</li>
<li>paper_authors: Levy Chaves, Alceu Bissoto, Eduardo Valle, Sandra Avila</li>
<li>for: 本研究旨在评估七种传输可能性分数在医疗图像分析中的表现，以便更好地选择适合目标数据集的深度学习架构。</li>
<li>methods: 本研究使用了七种传输可能性分数，包括三种基于特征之间的相似度的方法，以及四种基于特征之间的相似度和特征之间的相似度的权重平均值的方法。</li>
<li>results: 本研究在三个医疗图像分析应用中进行了广泛的评估，并发现了 Transferability 分数在医疗图像分析中的表现不一定可靠和一致，需要进一步的研究。<details>
<summary>Abstract</summary>
Transfer learning boosts the performance of medical image analysis by enabling deep learning (DL) on small datasets through the knowledge acquired from large ones. As the number of DL architectures explodes, exhaustively attempting all candidates becomes unfeasible, motivating cheaper alternatives for choosing them. Transferability scoring methods emerge as an enticing solution, allowing to efficiently calculate a score that correlates with the architecture accuracy on any target dataset. However, since transferability scores have not been evaluated on medical datasets, their use in this context remains uncertain, preventing them from benefiting practitioners. We fill that gap in this work, thoroughly evaluating seven transferability scores in three medical applications, including out-of-distribution scenarios. Despite promising results in general-purpose datasets, our results show that no transferability score can reliably and consistently estimate target performance in medical contexts, inviting further work in that direction.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>基于知识传递的深度学习（DL）在医疗图像分析中提高了性能，但由于DL模型的数量爆炸式增长，探索所有候选者成为不可能的，因此需要更加经济的选择方法。基于传输性的分数评估方法在这个领域出现，允许效率地计算任何目标数据集上模型准确率的相关分数。然而，由于这些传输性分数在医疗图像 datasets 上的应用仍然不明确，因此这种方法在实践中的应用尚未得到推广。我们在这个工作中填补了这个空白，对七种传输性分数在三个医疗应用中的性能进行了全面评估。虽然在通用数据集上显示了承诺的结果，但我们的结果表明，在医疗上下文中，没有一个可靠和一致地估计目标性能的传输性分数。这对未来的研究提出了挑战。
</details></li>
</ul>
<hr>
<h2 id="Physics-Informed-Deep-Learning-to-Reduce-the-Bias-in-Joint-Prediction-of-Nitrogen-Oxides"><a href="#Physics-Informed-Deep-Learning-to-Reduce-the-Bias-in-Joint-Prediction-of-Nitrogen-Oxides" class="headerlink" title="Physics-Informed Deep Learning to Reduce the Bias in Joint Prediction of Nitrogen Oxides"></a>Physics-Informed Deep Learning to Reduce the Bias in Joint Prediction of Nitrogen Oxides</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07441">http://arxiv.org/abs/2308.07441</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lianfa Li, Roxana Khalili, Frederick Lurmann, Nathan Pavlovic, Jun Wu, Yan Xu, Yisi Liu, Karl O’Sharkey, Beate Ritz, Luke Oman, Meredith Franklin, Theresa Bastain, Shohreh F. Farzan, Carrie Breton, Rima Habre</li>
<li>for: 这个论文旨在提高空气质量预测的准确性和可靠性，尤其是对氧气杂合物（NOx）的预测。</li>
<li>methods: 这篇论文使用了机器学习（ML）方法和化学运输模型（CTM）的知识来开发一个physics-informed deep learning框架，用于预测NO2和NOx的分布。这个框架具有减少预测偏差的能力，并能够提供明确的uncertainty估计。</li>
<li>results: 这篇论文的结果表明，使用这个physics-informed deep learning框架可以减少ML模型的预测偏差，并且能够更好地预测NO2和NOx的分布。这个框架还能够提供明确的uncertainty估计，这有助于更好地理解空气质量的变化和风险。<details>
<summary>Abstract</summary>
Atmospheric nitrogen oxides (NOx) primarily from fuel combustion have recognized acute and chronic health and environmental effects. Machine learning (ML) methods have significantly enhanced our capacity to predict NOx concentrations at ground-level with high spatiotemporal resolution but may suffer from high estimation bias since they lack physical and chemical knowledge about air pollution dynamics. Chemical transport models (CTMs) leverage this knowledge; however, accurate predictions of ground-level concentrations typically necessitate extensive post-calibration. Here, we present a physics-informed deep learning framework that encodes advection-diffusion mechanisms and fluid dynamics constraints to jointly predict NO2 and NOx and reduce ML model bias by 21-42%. Our approach captures fine-scale transport of NO2 and NOx, generates robust spatial extrapolation, and provides explicit uncertainty estimation. The framework fuses knowledge-driven physicochemical principles of CTMs with the predictive power of ML for air quality exposure, health, and policy applications. Our approach offers significant improvements over purely data-driven ML methods and has unprecedented bias reduction in joint NO2 and NOx prediction.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Interaction-Aware-Personalized-Vehicle-Trajectory-Prediction-Using-Temporal-Graph-Neural-Networks"><a href="#Interaction-Aware-Personalized-Vehicle-Trajectory-Prediction-Using-Temporal-Graph-Neural-Networks" class="headerlink" title="Interaction-Aware Personalized Vehicle Trajectory Prediction Using Temporal Graph Neural Networks"></a>Interaction-Aware Personalized Vehicle Trajectory Prediction Using Temporal Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07439">http://arxiv.org/abs/2308.07439</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amr Abdelraouf, Rohit Gupta, Kyungtae Han</li>
<li>for: 提高先进驾驶辅助系统和自动驾驶车辆的预测性能</li>
<li>methods: 使用时间图 convolutional neural networks (GCN) 和长期快速传递Memory (LSTM) 模型当地交通数据</li>
<li>results: 比较先进的预测性能，尤其是在较长的预测时间范围内，并且在不同的驾驶者环境下显示出较高的预测精度。<details>
<summary>Abstract</summary>
Accurate prediction of vehicle trajectories is vital for advanced driver assistance systems and autonomous vehicles. Existing methods mainly rely on generic trajectory predictions derived from large datasets, overlooking the personalized driving patterns of individual drivers. To address this gap, we propose an approach for interaction-aware personalized vehicle trajectory prediction that incorporates temporal graph neural networks. Our method utilizes Graph Convolution Networks (GCN) and Long Short-Term Memory (LSTM) to model the spatio-temporal interactions between target vehicles and their surrounding traffic. To personalize the predictions, we establish a pipeline that leverages transfer learning: the model is initially pre-trained on a large-scale trajectory dataset and then fine-tuned for each driver using their specific driving data. We employ human-in-the-loop simulation to collect personalized naturalistic driving trajectories and corresponding surrounding vehicle trajectories. Experimental results demonstrate the superior performance of our personalized GCN-LSTM model, particularly for longer prediction horizons, compared to its generic counterpart. Moreover, the personalized model outperforms individual models created without pre-training, emphasizing the significance of pre-training on a large dataset to avoid overfitting. By incorporating personalization, our approach enhances trajectory prediction accuracy.
</details>
<details>
<summary>摘要</summary>
<<SYS>>预测车辆轨迹的准确性是智能驾驶系统和自动驾驶车辆的关键。现有方法主要依靠大规模的数据集来 derivate 通用的轨迹预测，忽略了个人驾驶模式的特点。为了解决这个空白，我们提出了一种基于互动的个性化车辆轨迹预测方法，该方法利用图像卷积神经网络（GCN）和长短期记忆（LSTM）来模型目标车辆和它们周围的交通之间的空间时间互动。为了个性化预测，我们建立了一个管道，该管道通过转移学习来使用大规模轨迹数据进行初始化，然后通过每个驾驶员的特定驾驶数据进行微调。我们使用人类在Loop的 simulate 来收集个性化自然驾驶轨迹和相应的周围车辆轨迹。实验结果表明我们的个性化GCN-LSTM模型在较长的预测时间范围内表现更出色，特别是比其通用对应模型更出色。此外，个性化模型也比没有预训练的模型更好，这说明了大规模数据集的预训练可以避免过拟合。通过个性化，我们的方法可以提高轨迹预测精度。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Similarity-Loss-for-Neural-Source-Code-Summarization"><a href="#Semantic-Similarity-Loss-for-Neural-Source-Code-Summarization" class="headerlink" title="Semantic Similarity Loss for Neural Source Code Summarization"></a>Semantic Similarity Loss for Neural Source Code Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07429">http://arxiv.org/abs/2308.07429</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/apcl-research/funcom-useloss">https://github.com/apcl-research/funcom-useloss</a></li>
<li>paper_authors: Chia-Yi Su, Collin McMillan</li>
<li>for: 本研究旨在提出一种改进的损失函数，用于自动生成源代码描述。</li>
<li>methods: 本研究使用一种semantic similarity metric来计算损失，并与传统的一个字一个字损失函数相结合。</li>
<li>results: 对多种基eline进行评估，得到了大多数情况下的改进。<details>
<summary>Abstract</summary>
This paper presents an improved loss function for neural source code summarization. Code summarization is the task of writing natural language descriptions of source code. Neural code summarization refers to automated techniques for generating these descriptions using neural networks. Almost all current approaches involve neural networks as either standalone models or as part of a pretrained large language models e.g., GPT, Codex, LLaMA. Yet almost all also use a categorical cross-entropy (CCE) loss function for network optimization. Two problems with CCE are that 1) it computes loss over each word prediction one-at-a-time, rather than evaluating a whole sentence, and 2) it requires a perfect prediction, leaving no room for partial credit for synonyms. We propose and evaluate a loss function to alleviate this problem. In essence, we propose to use a semantic similarity metric to calculate loss over the whole output sentence prediction per training batch, rather than just loss for each word. We also propose to combine our loss with traditional CCE for each word, which streamlines the training process compared to baselines. We evaluate our approach over several baselines and report an improvement in the vast majority of conditions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>It computes loss for each word prediction individually, rather than evaluating the entire sentence.2. It requires perfect predictions, leaving no room for partial credit for synonyms.To address these issues, the proposed loss function uses a semantic similarity metric to calculate loss over the entire output sentence prediction for each training batch, rather than just for each word. Additionally, the proposed loss function combines with traditional CCE for each word, streamlining the training process compared to baselines. The approach is evaluated over several baselines and shows an improvement in the majority of conditions.In simplified Chinese, the text can be translated as:这篇论文提出了一种改进的损失函数，用于神经源代码概要。神经代码概要是自动生成源代码的自然语言描述的任务。目前大多数方法都使用神经网络作为独立模型或大语言模型的组件，如GPT、Codex、LLaMA。然而，这些模型的优化都使用分类交叉熵损失函数（CCE），它有两个问题：1. 它计算每个单词预测的损失，而不是整个句子。2. 它需要精准预测，不允许任何减少偏差。我们提议使用semantic相似度度量来计算损失，而不是单独计算每个单词的损失。此外，我们还提议将我们的损失函数与传统的CCE相结合，以便在训练过程中对每个单词进行损失计算，而不是直接使用CCE。我们对多个基准进行评估，并发现大多数情况下有改进。</details></li>
</ol>
<hr>
<h2 id="UniBrain-Unify-Image-Reconstruction-and-Captioning-All-in-One-Diffusion-Model-from-Human-Brain-Activity"><a href="#UniBrain-Unify-Image-Reconstruction-and-Captioning-All-in-One-Diffusion-Model-from-Human-Brain-Activity" class="headerlink" title="UniBrain: Unify Image Reconstruction and Captioning All in One Diffusion Model from Human Brain Activity"></a>UniBrain: Unify Image Reconstruction and Captioning All in One Diffusion Model from Human Brain Activity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07428">http://arxiv.org/abs/2308.07428</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijian Mai, Zhijun Zhang</li>
<li>for: 这篇论文旨在探讨使用人脑活动诱发的视觉刺激来重建图像和文本描述，以更好地理解人脑和视觉系统之间的连接。</li>
<li>methods: 该论文提出了一种名为UniBrain的一种普适傅 diffusion模型，通过将fMRI voxels转换为图像和文本的latent空间，并通过基于CLIP的fMRI图像和文本条件来导向反向傅 diffusion过程，生成真实的caption和图像。</li>
<li>results: UniBrain在图像重建和描述方面与现有方法相比，表现出较高的qualitative和quantitative性能，并在Natural Scenes Dataset（NSD）数据集上首次实现了图像描述结果。此外，简除试验和功能ROI分析还表明UniBrain的优势和视觉脑 decode的全面意义。<details>
<summary>Abstract</summary>
Image reconstruction and captioning from brain activity evoked by visual stimuli allow researchers to further understand the connection between the human brain and the visual perception system. While deep generative models have recently been employed in this field, reconstructing realistic captions and images with both low-level details and high semantic fidelity is still a challenging problem. In this work, we propose UniBrain: Unify Image Reconstruction and Captioning All in One Diffusion Model from Human Brain Activity. For the first time, we unify image reconstruction and captioning from visual-evoked functional magnetic resonance imaging (fMRI) through a latent diffusion model termed Versatile Diffusion. Specifically, we transform fMRI voxels into text and image latent for low-level information and guide the backward diffusion process through fMRI-based image and text conditions derived from CLIP to generate realistic captions and images. UniBrain outperforms current methods both qualitatively and quantitatively in terms of image reconstruction and reports image captioning results for the first time on the Natural Scenes Dataset (NSD) dataset. Moreover, the ablation experiments and functional region-of-interest (ROI) analysis further exhibit the superiority of UniBrain and provide comprehensive insight for visual-evoked brain decoding.
</details>
<details>
<summary>摘要</summary>
Image重建和描述从脑动活动诱发的视觉系统的连接，使研究人员更深入了解人脑和视觉系统之间的关系。而最近，深度生成模型在这一领域得到了广泛应用。但是，重现真实的描述和图像，同时具有低级别细节和高semantic faithfulness仍然是一个挑战。在这种情况下，我们提出了UniBrain：基于人脑活动的一种普适的扩展模型，用于 reunifying image reconstruction和 captioning。UniBrain通过将fMRI voxels转换为文本和图像的latent空间，并通过基于CLIP的fMRI图像和文本条件，驱动回传diffusion过程，生成真实的描述和图像。UniBrain在质量和量上比现有方法出色，并在自然场景数据集（NSD）上首次报告图像描述结果。此外，我们还进行了层次 ROI分析和简要实验，以更深入地探索视觉诱发的脑决码。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Intersection-of-Large-Language-Models-and-Agent-Based-Modeling-via-Prompt-Engineering"><a href="#Exploring-the-Intersection-of-Large-Language-Models-and-Agent-Based-Modeling-via-Prompt-Engineering" class="headerlink" title="Exploring the Intersection of Large Language Models and Agent-Based Modeling via Prompt Engineering"></a>Exploring the Intersection of Large Language Models and Agent-Based Modeling via Prompt Engineering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07411">http://arxiv.org/abs/2308.07411</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ejunprung/llm-agents">https://github.com/ejunprung/llm-agents</a></li>
<li>paper_authors: Edward Junprung</li>
<li>for: 这个研究旨在使用大语言模型（LLM）来实现人类行为的准确模拟，以探索人类行为在复杂社会系统中的行为和互动方式。</li>
<li>methods: 研究使用了提示工程（inspired by Park et al. (2023)），通过设计了两个人类行为的假设 scenario：一个是两个代理的谈判，另一个是六个代理的谋杀推理游戏。</li>
<li>results: 研究发现，使用LLM可以创造出非常真实的人类行为，包括在谈判和推理游戏中的互动和决策。这些结果表明，LLM可以成为模拟人类行为的有效工具。<details>
<summary>Abstract</summary>
The final frontier for simulation is the accurate representation of complex, real-world social systems. While agent-based modeling (ABM) seeks to study the behavior and interactions of agents within a larger system, it is unable to faithfully capture the full complexity of human-driven behavior. Large language models (LLMs), like ChatGPT, have emerged as a potential solution to this bottleneck by enabling researchers to explore human-driven interactions in previously unimaginable ways. Our research investigates simulations of human interactions using LLMs. Through prompt engineering, inspired by Park et al. (2023), we present two simulations of believable proxies of human behavior: a two-agent negotiation and a six-agent murder mystery game.
</details>
<details>
<summary>摘要</summary>
最终的前ier для模拟是准确地表现复杂的现实世界社会系统。而代理人模型（ABM）则尝试研究代理人在更大的系统中的行为和互动，但它无法准确地捕捉人类驱动的行为的全部复杂性。大语言模型（LLM），如ChatGPT，在这个瓶颈上出现了作为解决方案，让研究人员能够在前所未有的方式探索人类驱动的互动。我们的研究探讨了使用LLM进行人类互动的模拟。通过提示工程，取得自 Park et al. (2023)，我们展示了两个人类行为的准确代理：一个两个代理的谈判和一个六个代理的谋杀推理游戏。
</details></li>
</ul>
<hr>
<h2 id="PARIS-Part-level-Reconstruction-and-Motion-Analysis-for-Articulated-Objects"><a href="#PARIS-Part-level-Reconstruction-and-Motion-Analysis-for-Articulated-Objects" class="headerlink" title="PARIS: Part-level Reconstruction and Motion Analysis for Articulated Objects"></a>PARIS: Part-level Reconstruction and Motion Analysis for Articulated Objects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07391">http://arxiv.org/abs/2308.07391</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/3dlg-hcvc/paris">https://github.com/3dlg-hcvc/paris</a></li>
<li>paper_authors: Jiayi Liu, Ali Mahdavi-Amiri, Manolis Savva</li>
<li>for:  simultaneous part-level reconstruction and motion parameter estimation for articulated objects</li>
<li>methods: self-supervised, end-to-end architecture with implicit shape and appearance models, optimizes motion parameters jointly without 3D supervision or semantic annotation</li>
<li>results: generalizes better across object categories, outperforms baselines and prior work, improves reconstruction with a Chamfer-L1 distance reduction of 3.94 (45.2%) for objects and 26.79 (84.5%) for parts, achieves 5% error rate for motion estimation across 10 object categories.<details>
<summary>Abstract</summary>
We address the task of simultaneous part-level reconstruction and motion parameter estimation for articulated objects. Given two sets of multi-view images of an object in two static articulation states, we decouple the movable part from the static part and reconstruct shape and appearance while predicting the motion parameters. To tackle this problem, we present PARIS: a self-supervised, end-to-end architecture that learns part-level implicit shape and appearance models and optimizes motion parameters jointly without any 3D supervision, motion, or semantic annotation. Our experiments show that our method generalizes better across object categories, and outperforms baselines and prior work that are given 3D point clouds as input. Our approach improves reconstruction relative to state-of-the-art baselines with a Chamfer-L1 distance reduction of 3.94 (45.2%) for objects and 26.79 (84.5%) for parts, and achieves 5% error rate for motion estimation across 10 object categories.   Video summary at: https://youtu.be/tDSrROPCgUc
</details>
<details>
<summary>摘要</summary>
我们考虑了同时进行部件重建和运动参数估计的骨架对象问题。给出两个多视图图像集合，我们将可动部分与静止部分分离，重建形状和外观，同时预测运动参数。为解决这个问题，我们提出了PARIS：一种自主、端到端架构，不需要3D超视觉、运动或semantic注解，可以同时学习部件级别的隐式形状和外观模型，并同步优化运动参数。我们的实验表明，我们的方法可以更好地泛化到不同的对象类别，并超越基elines和先前的方法，它们输入3D点云作为输入。我们的方法可以将 Chamfer-L1 距离减少3.94（45.2%） для对象和26.79（84.5%） для部件，并实现了10种对象类别中的运动估计错误率为5%。Video summary: <https://youtu.be/tDSrROPCgUc>
</details></li>
</ul>
<hr>
<h2 id="LLM-Self-Defense-By-Self-Examination-LLMs-Know-They-Are-Being-Tricked"><a href="#LLM-Self-Defense-By-Self-Examination-LLMs-Know-They-Are-Being-Tricked" class="headerlink" title="LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked"></a>LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07308">http://arxiv.org/abs/2308.07308</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alec Helbling, Mansi Phute, Matthew Hull, Duen Horng Chau</li>
<li>for: 防止语言模型生成危险内容（如犯罪指南）</li>
<li>methods: 使用大语言模型自身的过滤机制来防止生成危险内容</li>
<li>results: even if a model is not fine-tuned to be aligned with human values, it is possible to stop it from presenting harmful content to users by validating the content using a language model.<details>
<summary>Abstract</summary>
Large language models (LLMs) have skyrocketed in popularity in recent years due to their ability to generate high-quality text in response to human prompting. However, these models have been shown to have the potential to generate harmful content in response to user prompting (e.g., giving users instructions on how to commit crimes). There has been a focus in the literature on mitigating these risks, through methods like aligning models with human values through reinforcement learning. However, it has been shown that even aligned language models are susceptible to adversarial attacks that bypass their restrictions on generating harmful text. We propose a simple approach to defending against these attacks by having a large language model filter its own responses. Our current results show that even if a model is not fine-tuned to be aligned with human values, it is possible to stop it from presenting harmful content to users by validating the content using a language model.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在最近几年内 Popularity 急剧增长，这主要归功于它们可以根据人类提示生成高质量的文本。然而，这些模型也被证明可能生成有害内容（如提供用户提示以commit犯罪）。在文献中，有许多研究探讨如何 Mitigate 这些风险，如通过 reinforcement learning 将模型与人类价值观念进行对齐。然而，研究表明，即使模型已经对齐，也可能受到恶意攻击，这些攻击可以让模型生成有害内容。我们提议一种简单的方法，即让大型语言模型自己过滤其回快。我们当前的结果表明，即使模型没有 fine-tune 对齐人类价值观念，也可以通过语言模型验证来防止它生成有害内容给用户。
</details></li>
</ul>
<hr>
<h2 id="Extend-Wave-Function-Collapse-to-Large-Scale-Content-Generation"><a href="#Extend-Wave-Function-Collapse-to-Large-Scale-Content-Generation" class="headerlink" title="Extend Wave Function Collapse to Large-Scale Content Generation"></a>Extend Wave Function Collapse to Large-Scale Content Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07307">http://arxiv.org/abs/2308.07307</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhe Nie, Shaoming Zheng, Zhan Zhuang, Xuan Song</li>
<li>for: 解决 Wave Function Collapse (WFC) 算法在大规模内容生成中的时间复杂性和约束矛盾问题。</li>
<li>methods: 提出 Nested WFC (N-WFC) 算法框架，采用完整和部分完整块集准备策略，可以避免矛盾和回tracking问题，并且可以生成 deterministic 和 periodic 的无限内容。</li>
<li>results: 验证 N-WFC 算法的可行性和适用性，并且通过 weight-brush 系统和游戏设计方法，证明其适用于游戏设计。<details>
<summary>Abstract</summary>
Wave Function Collapse (WFC) is a widely used tile-based algorithm in procedural content generation, including textures, objects, and scenes. However, the current WFC algorithm and related research lack the ability to generate commercialized large-scale or infinite content due to constraint conflict and time complexity costs. This paper proposes a Nested WFC (N-WFC) algorithm framework to reduce time complexity. To avoid conflict and backtracking problems, we offer a complete and sub-complete tileset preparation strategy, which requires only a small number of tiles to generate aperiodic and deterministic infinite content. We also introduce the weight-brush system that combines N-WFC and sub-complete tileset, proving its suitability for game design. Our contribution addresses WFC's challenge in massive content generation and provides a theoretical basis for implementing concrete games.
</details>
<details>
<summary>摘要</summary>
wave function collapse (WFC) 是一种广泛使用的瓷砖式算法 в procedural content generation 中，包括文本、物体和场景等。然而，当前 WFC 算法和相关研究缺乏可商业化的大规模或无限内容生成能力，这是因为约束冲突和时间复杂度成本的问题。本文提出了一种嵌套 WFC（N-WFC）算法框架，以降低时间复杂度。为了避免冲突和回溯问题，我们提供了完整的和半完整的瓷砖集准备策略，只需一小数量的瓷砖可以生成 periodic 和 deterministic 无限内容。我们还介绍了 weight-brush 系统，该系统结合 N-WFC 和半完整瓷砖集，证明其适用于游戏设计。我们的贡献解决了 WFC 在大规模内容生成中的挑战，并提供了实现具体游戏的理论基础。
</details></li>
</ul>
<hr>
<h2 id="Neural-Authorship-Attribution-Stylometric-Analysis-on-Large-Language-Models"><a href="#Neural-Authorship-Attribution-Stylometric-Analysis-on-Large-Language-Models" class="headerlink" title="Neural Authorship Attribution: Stylometric Analysis on Large Language Models"></a>Neural Authorship Attribution: Stylometric Analysis on Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07305">http://arxiv.org/abs/2308.07305</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tharindu Kumarage, Huan Liu</li>
<li>for: 这篇研究旨在探讨人工智能生成文本的伪造问题，并寻找一种可靠的方法来追溯这些文本的来源。</li>
<li>methods: 研究使用了现有的语言模型（LLMs），包括GPT-4、PaLM和Llama，并对这些模型进行了分析和比较。</li>
<li>results: 研究发现了不同的商业和开源模型之间的区别，并发现了这些模型在不同的语言方面的差异。这些结果可以帮助未来的伪造探测和对抗人工智能生成的伪造文本。<details>
<summary>Abstract</summary>
Large language models (LLMs) such as GPT-4, PaLM, and Llama have significantly propelled the generation of AI-crafted text. With rising concerns about their potential misuse, there is a pressing need for AI-generated-text forensics. Neural authorship attribution is a forensic effort, seeking to trace AI-generated text back to its originating LLM. The LLM landscape can be divided into two primary categories: proprietary and open-source. In this work, we delve into these emerging categories of LLMs, focusing on the nuances of neural authorship attribution. To enrich our understanding, we carry out an empirical analysis of LLM writing signatures, highlighting the contrasts between proprietary and open-source models, and scrutinizing variations within each group. By integrating stylometric features across lexical, syntactic, and structural aspects of language, we explore their potential to yield interpretable results and augment pre-trained language model-based classifiers utilized in neural authorship attribution. Our findings, based on a range of state-of-the-art LLMs, provide empirical insights into neural authorship attribution, paving the way for future investigations aimed at mitigating the threats posed by AI-generated misinformation.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）如GPT-4、PaLM和Llama已经有效地推动人工生成的文本生成。随着AI生成文本的可能性的滥用，有一个急需AI生成文本科学的发展。人工作文本识别是一种审查AI生成文本的努力，寻求跟踪AI生成文本的来源LLM。LLM领域可以分为两个主要类别： propriety 和 open-source。在这项工作中，我们深入探究这些新兴类别的LLM，强调在 neural authorship attribution 方面的特点。通过对 LLM 的写作特征进行实质分析，包括 lexical、syntactic 和 structural 方面的语言特征，我们探讨了这些特征是否可以生成可读取的结果，并可以增强基于预训练语言模型的扩展语言模型来进行分类。我们的发现，基于一系列现代LLM，提供了实质性的启示，帮助我们更好地理解 neural authorship attribution，并为未来防止AI生成的误导而做出了重要贡献。
</details></li>
</ul>
<hr>
<h2 id="Why-Not-Explaining-Missing-Entailments-with-Evee-Technical-Report"><a href="#Why-Not-Explaining-Missing-Entailments-with-Evee-Technical-Report" class="headerlink" title="Why Not? Explaining Missing Entailments with Evee (Technical Report)"></a>Why Not? Explaining Missing Entailments with Evee (Technical Report)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07294">http://arxiv.org/abs/2308.07294</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Alrabbaa, Stefan Borgwardt, Tom Friese, Patrick Koopmann, Mikhail Kotlov</li>
<li>for: 本研究旨在帮助ontology用户更好地理解逻辑推论 derivations。</li>
<li>methods: 本研究使用了描述逻辑理解器和Protégé中的插件，以及新的abduction和 counterexample技术来解释缺失的结论。</li>
<li>results: 本研究提出了一种新的 $\rm E{\scriptsize VEE}$ 插件，可以帮助用户更好地理解ontology中缺失的结论。<details>
<summary>Abstract</summary>
Understanding logical entailments derived by a description logic reasoner is not always straight-forward for ontology users. For this reason, various methods for explaining entailments using justifications and proofs have been developed and implemented as plug-ins for the ontology editor Prot\'eg\'e. However, when the user expects a missing consequence to hold, it is equally important to explain why it does not follow from the ontology. In this paper, we describe a new version of $\rm E{\scriptsize VEE}$, a Prot\'eg\'e plugin that now also provides explanations for missing consequences, via existing and new techniques based on abduction and counterexamples.
</details>
<details>
<summary>摘要</summary>
理解推理推论结论由描述逻辑理解者不一定是直观的，对ontology用户而言。为此，各种用于说明推论使用证明和证据的方法已经开发和实现为Protégé编辑器插件。然而，当用户期望缺失的结论不存在时，也是非常重要的解释为什么不从ontology中推论出来。在这篇论文中，我们描述了一种新版本的 $\rm E{\scriptsize VEE}$，一个Protégé插件，现在还提供缺失结论的解释，通过现有和新的技术基于推理和反例。
</details></li>
</ul>
<hr>
<h2 id="Cross-Attribute-Matrix-Factorization-Model-with-Shared-User-Embedding"><a href="#Cross-Attribute-Matrix-Factorization-Model-with-Shared-User-Embedding" class="headerlink" title="Cross-Attribute Matrix Factorization Model with Shared User Embedding"></a>Cross-Attribute Matrix Factorization Model with Shared User Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07284">http://arxiv.org/abs/2308.07284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wen Liang, Zeng Fan, Youzhi Liang, Jianguo Jia</li>
<li>for: 这个研究旨在应用深度学习技术来解决推荐系统中的寒冷开始问题，并且考虑用户和项目的特征对推荐系统的影响。</li>
<li>methods: 本研究使用的方法是内容匹配网络（Neural Matrix Factorization，NeuMF），并且将用户和项目的特征考虑进行扩展。</li>
<li>results: 实验结果显示，我们的跨特征网络匹配网络（Cross-Attribute Matrix Factorization，CAMF）模型在MovieLens和Pinterest dataset上具有较高的性能，特别是在资料集稀疏性较高的情况下。<details>
<summary>Abstract</summary>
Over the past few years, deep learning has firmly established its prowess across various domains, including computer vision, speech recognition, and natural language processing. Motivated by its outstanding success, researchers have been directing their efforts towards applying deep learning techniques to recommender systems. Neural collaborative filtering (NCF) and Neural Matrix Factorization (NeuMF) refreshes the traditional inner product in matrix factorization with a neural architecture capable of learning complex and data-driven functions. While these models effectively capture user-item interactions, they overlook the specific attributes of both users and items. This can lead to robustness issues, especially for items and users that belong to the "long tail". Such challenges are commonly recognized in recommender systems as a part of the cold-start problem. A direct and intuitive approach to address this issue is by leveraging the features and attributes of the items and users themselves. In this paper, we introduce a refined NeuMF model that considers not only the interaction between users and items, but also acrossing associated attributes. Moreover, our proposed architecture features a shared user embedding, seamlessly integrating with user embeddings to imporve the robustness and effectively address the cold-start problem. Rigorous experiments on both the Movielens and Pinterest datasets demonstrate the superiority of our Cross-Attribute Matrix Factorization model, particularly in scenarios characterized by higher dataset sparsity.
</details>
<details>
<summary>摘要</summary>
在过去几年中，深度学习在不同领域，如计算机视觉、语音识别和自然语言处理等领域，都有着杰出的成绩。驱动于其出色的成绩，研究人员开始将深度学习技术应用于推荐系统。基于神经网络的共同积分（NCF）和神经矩阵分解（NeuMF）等模型，将传统的内积分换成神经网络架构，能够学习复杂的数据驱动函数。然而，这些模型可能会忽视用户和项目的特定属性，这可能会导致稳定性问题，尤其是对于“长尾”用户和项目。这种问题在推荐系统中广泛存在，通常被称为冷启动问题。在本文中，我们提出了一种改进的NeuMF模型，该模型不仅考虑用户和项目之间的交互，还考虑用户和项目之间的关联属性。此外，我们的提议的架构还包括共享用户嵌入，可以融合用户嵌入，从而提高系统的稳定性和效果地解决冷启动问题。我们在MovieLens和Pinterest数据集上进行了严格的实验，并证明我们的横向积分模型在数据集稀缺的情况下表现出优异性。
</details></li>
</ul>
<hr>
<h2 id="Autonomous-Point-Cloud-Segmentation-for-Power-Lines-Inspection-in-Smart-Grid"><a href="#Autonomous-Point-Cloud-Segmentation-for-Power-Lines-Inspection-in-Smart-Grid" class="headerlink" title="Autonomous Point Cloud Segmentation for Power Lines Inspection in Smart Grid"></a>Autonomous Point Cloud Segmentation for Power Lines Inspection in Smart Grid</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07283">http://arxiv.org/abs/2308.07283</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Kyuroson, Anton Koval, George Nikolakopoulos</li>
<li>for: 本研究旨在提出一种无监督机器学习（ML）框架，用于从 LiDAR 数据中探测、提取和分析高压和低压电缆的特征，以及相关的绿色区域。</li>
<li>methods: 提议的方法包括Initially eliminating ground points based on statistical analysis, denoising and transforming the remaining candidate points using Principle Component Analysis (PCA) and Kd-tree, and then segmenting power lines using a two-stage DBSCAN clustering.</li>
<li>results: 实验结果表明，提议的框架可以效率地探测电缆和进行相关的风险分析。<details>
<summary>Abstract</summary>
LiDAR is currently one of the most utilized sensors to effectively monitor the status of power lines and facilitate the inspection of remote power distribution networks and related infrastructures. To ensure the safe operation of the smart grid, various remote data acquisition strategies, such as Airborne Laser Scanning (ALS), Mobile Laser Scanning (MLS), and Terrestrial Laser Scanning (TSL) have been leveraged to allow continuous monitoring of regional power networks, which are typically surrounded by dense vegetation. In this article, an unsupervised Machine Learning (ML) framework is proposed, to detect, extract and analyze the characteristics of power lines of both high and low voltage, as well as the surrounding vegetation in a Power Line Corridor (PLC) solely from LiDAR data. Initially, the proposed approach eliminates the ground points from higher elevation points based on statistical analysis that applies density criteria and histogram thresholding. After denoising and transforming of the remaining candidate points by applying Principle Component Analysis (PCA) and Kd-tree, power line segmentation is achieved by utilizing a two-stage DBSCAN clustering to identify each power line individually. Finally, all high elevation points in the PLC are identified based on their distance to the newly segmented power lines. Conducted experiments illustrate that the proposed framework is an agnostic method that can efficiently detect the power lines and perform PLC-based hazard analysis.
</details>
<details>
<summary>摘要</summary>
利达（LiDAR）现在是智能电网运行的一个最广泛使用的感知器，用于监测电力线路的状况和远程电力分布网络和相关基础设施的检查。为保证智能电网的安全运行，远程数据获取策略，如空中激光扫描（ALS）、移动激光扫描（MLS）和地面激光扫描（TSL）已经被利用，以实现区域电网的连续监测，这些区域通常被密集的植被环绕着。在这篇文章中，一种无监测机器学习（ML）框架被提议，用于从LiDAR数据中检测、提取和分析高压和低压电力线路的特征，以及周围的植被。首先，提议的方法从高空点的高程点中排除地面点，基于统计分析，应用密度标准和对 histogram 进行阈值设置。接着，通过应用原理Components分析（PCA）和 Kd-tree 转换，对剩下的候选点进行减噪和变换。然后，通过使用两个阶段 DBSCAN 聚合，对每个电力线 individually 进行分类，以实现电力线的分 Segmentation。最后，通过对新分类的高空点进行距离计算，全部在 PLC 中的高空点被标识出来。实验表明，提议的方法是一种无关的方法，可以有效地检测电力线和在 PLC 中进行风险分析。
</details></li>
</ul>
<hr>
<h2 id="Data-Efficient-Energy-Aware-Participant-Selection-for-UAV-Enabled-Federated-Learning"><a href="#Data-Efficient-Energy-Aware-Participant-Selection-for-UAV-Enabled-Federated-Learning" class="headerlink" title="Data-Efficient Energy-Aware Participant Selection for UAV-Enabled Federated Learning"></a>Data-Efficient Energy-Aware Participant Selection for UAV-Enabled Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07273">http://arxiv.org/abs/2308.07273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youssra Cheriguene, Wael Jaafar, Chaker Abdelaziz Kerrache, Halim Yanikomeroglu, Fatima Zohra Bousbaa, Nasreddine Lagraa</li>
<li>for: 提高Edge Federated Learning（FL）模型的准确性，并且考虑UAV的能源消耗和通信质量等约束。</li>
<li>methods: 提出了一种新的UAV参与者选择策略，即基于地区数据的结构相似度指数平均分数和能源消耗配置的数据高效能观测选择策略（DEEPS）。</li>
<li>results: 通过实验，提出的选择策略比废 randomly选择策略更高的Edge FL模型准确性、训练时间和UAV能源消耗。<details>
<summary>Abstract</summary>
Unmanned aerial vehicle (UAV)-enabled edge federated learning (FL) has sparked a rise in research interest as a result of the massive and heterogeneous data collected by UAVs, as well as the privacy concerns related to UAV data transmissions to edge servers. However, due to the redundancy of UAV collected data, e.g., imaging data, and non-rigorous FL participant selection, the convergence time of the FL learning process and bias of the FL model may increase. Consequently, we investigate in this paper the problem of selecting UAV participants for edge FL, aiming to improve the FL model's accuracy, under UAV constraints of energy consumption, communication quality, and local datasets' heterogeneity. We propose a novel UAV participant selection scheme, called data-efficient energy-aware participant selection strategy (DEEPS), which consists of selecting the best FL participant in each sub-region based on the structural similarity index measure (SSIM) average score of its local dataset and its power consumption profile. Through experiments, we demonstrate that the proposed selection scheme is superior to the benchmark random selection method, in terms of model accuracy, training time, and UAV energy consumption.
</details>
<details>
<summary>摘要</summary>
“无人机（UAV）启用边缘联合学习（FL）已经引起了研究兴趣，由于UAV收集的数据量庞大和多样化，以及UAV数据传输到边缘服务器的隐私问题。然而，由于UAV收集的数据重复，如图像数据，以及不严谨的FL参与者选择，FL学习过程的收敛时间和模型偏见可能增加。因此，我们在本纸中 investigate 选择UAV参与者 для边缘FL，以提高FL模型的准确性，在UAV的能量消耗、通信质量和本地数据的多样性等因素限制下。我们提出了一种新的UAV参与者选择策略，称为数据高效能觉 participant selection strategy（DEEPS），该策略基于每个子区域中的本地数据和能量消耗profile中的结构相似度平均分数来选择最佳的FL参与者。通过实验，我们示出了提案的选择策略与参照方法（随机选择）相比，在模型准确性、训练时间和UAV能量消耗方面具有显著优势。”Note: The translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="EasyEdit-An-Easy-to-use-Knowledge-Editing-Framework-for-Large-Language-Models"><a href="#EasyEdit-An-Easy-to-use-Knowledge-Editing-Framework-for-Large-Language-Models" class="headerlink" title="EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models"></a>EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07269">http://arxiv.org/abs/2308.07269</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zjunlp/easyedit">https://github.com/zjunlp/easyedit</a></li>
<li>paper_authors: Peng Wang, Ningyu Zhang, Xin Xie, Yunzhi Yao, Bozhong Tian, Mengru Wang, Zekun Xi, Siyuan Cheng, Kangwei Liu, Guozhou Zheng, Huajun Chen</li>
<li>for: 提高LLMs的知识更新和修正能力，以提高其可靠性和通用性。</li>
<li>methods: 支持多种现代知识编辑方法，可以轻松应用于多种well-known LLMs。</li>
<li>results: 在LlaMA-2上进行了知识编辑实验，表明知识编辑比传统精度uning更高效和更通用。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy issues, which means they are unaware of unseen events or generate text with incorrect facts owing to the outdated/noisy data. To this end, many knowledge editing approaches for LLMs have emerged -- aiming to subtly inject/edit updated knowledge or adjust undesired behavior while minimizing the impact on unrelated inputs. Nevertheless, due to significant differences among various knowledge editing methods and the variations in task setups, there is no standard implementation framework available for the community, which hinders practitioners to apply knowledge editing to applications. To address these issues, we propose EasyEdit, an easy-to-use knowledge editing framework for LLMs. It supports various cutting-edge knowledge editing approaches and can be readily apply to many well-known LLMs such as T5, GPT-J, LlaMA, etc. Empirically, we report the knowledge editing results on LlaMA-2 with EasyEdit, demonstrating that knowledge editing surpasses traditional fine-tuning in terms of reliability and generalization. We have released the source code on GitHub at https://github.com/zjunlp/EasyEdit, along with Google Colab tutorials and comprehensive documentation for beginners to get started. Besides, we present an online system for real-time knowledge editing, and a demo video at http://knowlm.zjukg.cn/easyedit.mp4.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）通常会受到知识剖除或误差问题的影响，这意味着它们不知道未经见过的事件或生成文本中含有错误的信息，这由于模型使用的数据过时或噪音有关。为解决这些问题，许多知识编辑方法 для LLM  emerged  --  hoping to subtly inject 或编辑更新的知识或调整不符合预期的行为，同时尽量减少对无关输入的影响。然而，由于不同的知识编辑方法之间存在差异，以及任务设置的变化，当前没有一个标准的实现框架可供社区使用，这限制了实践者在应用知识编辑方法时的能力。为解决这些问题，我们提出了 EasyEdit，一个易于使用的知识编辑框架 для LLM。它支持多种前沿知识编辑方法，并可以 readily 应用于多个知名的 LLM such as T5, GPT-J, LlaMA, etc. 我们在 LlaMA-2 上进行了知识编辑试验，并证明知识编辑超过了传统的精细调整在可靠性和泛化方面的表现。我们在 GitHub 上公布了源代码，并提供了 Google Colab 教程和详细的文档，以便初学者快速入门。此外，我们还提供了在线实时知识编辑系统和 demo 视频，请参考 http://knowlm.zjukg.cn/easyedit.mp4.
</details></li>
</ul>
<hr>
<h2 id="Can-we-Agree-On-the-Rashomon-Effect-and-the-Reliability-of-Post-Hoc-Explainable-AI"><a href="#Can-we-Agree-On-the-Rashomon-Effect-and-the-Reliability-of-Post-Hoc-Explainable-AI" class="headerlink" title="Can we Agree? On the Rashōmon Effect and the Reliability of Post-Hoc Explainable AI"></a>Can we Agree? On the Rashōmon Effect and the Reliability of Post-Hoc Explainable AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07247">http://arxiv.org/abs/2308.07247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Clement Poiret, Antoine Grigis, Justin Thomas, Marion Noulhiane</li>
<li>for: 这个研究检查了Rashomon效应对机器学习模型中的知识抽取所带来的挑战。</li>
<li>methods: 这个研究使用了SHAP方法对Rashomon集中的模型进行解释。</li>
<li>results: 实验结果显示，随着样本大小增加，解释的一致性逐渐提高，但在少于128个样本的情况下，解释具有高度的变化性，因此不可靠地抽取知识。然而，在更多的数据下，模型之间的一致性提高，allowing for consensus。 bagging ensemble often had higher agreement。这些结果为我们提供了足够的数据来信任解释的指南。<details>
<summary>Abstract</summary>
The Rash\=omon effect poses challenges for deriving reliable knowledge from machine learning models. This study examined the influence of sample size on explanations from models in a Rash\=omon set using SHAP. Experiments on 5 public datasets showed that explanations gradually converged as the sample size increased. Explanations from <128 samples exhibited high variability, limiting reliable knowledge extraction. However, agreement between models improved with more data, allowing for consensus. Bagging ensembles often had higher agreement. The results provide guidance on sufficient data to trust explanations. Variability at low samples suggests that conclusions may be unreliable without validation. Further work is needed with more model types, data domains, and explanation methods. Testing convergence in neural networks and with model-specific explanation methods would be impactful. The approaches explored here point towards principled techniques for eliciting knowledge from ambiguous models.
</details>
<details>
<summary>摘要</summary>
瑞索蒙效应对机器学习模型中提取可靠知识带来挑战。这项研究检查了样本大小对模型解释的影响，使用SHAP进行了5个公共数据集的实验。结果显示，随着样本大小增加，解释逐渐协调，但从128个样本开始，解释具有高度的变化， limiting reliable knowledge extraction。然而，随着更多的数据，模型之间的一致性提高，allowing for consensus。 Bagging ensemble often had higher agreement。结果提供了足够数据来信任解释的指导，变化在低样本数量 suggets that conclusion may be unreliable without validation。进一步的工作需要更多的模型类型，数据领域和解释方法。测试 converges in neural networks和模型特定的解释方法会对其具有深远的影响。研究方法可以带来原则性的技术，从ambiguous models中提取知识。
</details></li>
</ul>
<hr>
<h2 id="Context-Aware-Planning-and-Environment-Aware-Memory-for-Instruction-Following-Embodied-Agents"><a href="#Context-Aware-Planning-and-Environment-Aware-Memory-for-Instruction-Following-Embodied-Agents" class="headerlink" title="Context-Aware Planning and Environment-Aware Memory for Instruction Following Embodied Agents"></a>Context-Aware Planning and Environment-Aware Memory for Instruction Following Embodied Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07241">http://arxiv.org/abs/2308.07241</a></li>
<li>repo_url: None</li>
<li>paper_authors: Byeonghwi Kim, Jinyeon Kim, Yuyeong Kim, Cheolhong Min, Jonghyun Choi</li>
<li>for: 提高家庭任务完成需要规划一系列的行动，考虑到 previous 动作的后果。</li>
<li>methods: 我们提出了 Context-Aware Planning and Environment-Aware Memory (CAPEAM)，它将Semantic context (例如适合交互的物品)和改变的空间安排和交互对象的状态 (例如交互对象的移动位置)包含在一系列动作中，以推断后续动作。</li>
<li>results: 我们经验表明，搭载CAPEAM的机器人在多个指标中达到了最新的状态前的表现，包括在不同环境中完成交互指令的任务，差异为大致 (+10.70%在未看到环境中)。<details>
<summary>Abstract</summary>
Accomplishing household tasks requires to plan step-by-step actions considering the consequences of previous actions. However, the state-of-the-art embodied agents often make mistakes in navigating the environment and interacting with proper objects due to imperfect learning by imitating experts or algorithmic planners without such knowledge. To improve both visual navigation and object interaction, we propose to consider the consequence of taken actions by CAPEAM (Context-Aware Planning and Environment-Aware Memory) that incorporates semantic context (e.g., appropriate objects to interact with) in a sequence of actions, and the changed spatial arrangement and states of interacted objects (e.g., location that the object has been moved to) in inferring the subsequent actions. We empirically show that the agent with the proposed CAPEAM achieves state-of-the-art performance in various metrics using a challenging interactive instruction following benchmark in both seen and unseen environments by large margins (up to +10.70% in unseen env.).
</details>
<details>
<summary>摘要</summary>
完成家务需要规划每一步行动，考虑先前行动的后果。然而，现状的凉身agent经常在环境中导航和与合适的物体交互时出错，因为它们通过专家学习或算法规划而学习的知识不够完善。为了提高视觉导航和物体交互，我们提议考虑行动的后果，通过Context-Aware Planning and Environment-Aware Memory（CAPEAM）来 incorporate semantic context（例如，与物体交互时适用的对象）在一系列动作中，以及交互对象的改变的空间布局和状态（例如，交互对象的移动位置）。我们实验表明，携带我们提议的CAPEAM的代理人在多种 metrics 中表现出STATE-OF-THE-ART的表现，包括在seen和unseen环境中的挑战性交互指令遵循测试中，差异达 +10.70%。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/15/cs.AI_2023_08_15/" data-id="clorjzl1s002ff1885mi7eu43" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_08_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/15/cs.CL_2023_08_15/" class="article-date">
  <time datetime="2023-08-15T11:00:00.000Z" itemprop="datePublished">2023-08-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/15/cs.CL_2023_08_15/">cs.CL - 2023-08-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="DS4DH-at-SMM4H-2023-Zero-Shot-Adverse-Drug-Events-Normalization-using-Sentence-Transformers-and-Reciprocal-Rank-Fusion"><a href="#DS4DH-at-SMM4H-2023-Zero-Shot-Adverse-Drug-Events-Normalization-using-Sentence-Transformers-and-Reciprocal-Rank-Fusion" class="headerlink" title="DS4DH at #SMM4H 2023: Zero-Shot Adverse Drug Events Normalization using Sentence Transformers and Reciprocal-Rank Fusion"></a>DS4DH at #SMM4H 2023: Zero-Shot Adverse Drug Events Normalization using Sentence Transformers and Reciprocal-Rank Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12877">http://arxiv.org/abs/2308.12877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anthony Yazdani, Hossein Rouhizadeh, David Vicente Alvarez, Douglas Teodoro</li>
<li>for: 本研究是为了评估一种基于BERT fine-tuning和sentence transformers的社交媒体文本挖掘系统，用于正常化恶性药物事件提到Medical Dictionary for Regulatory Activities（MDRA）词汇。</li>
<li>methods: 本研究采用了两stage方法，首先使用BERT fine-tuning进行实体识别，然后使用sentence transformers和reciprocal-rank fusion进行零 shot正常化。</li>
<li>results: 本研究的结果显示，这种方法在MDRA词汇正常化中得到了44.9%的精度、40.5%的准确率和42.6%的F1分数，超过了共享任务5中的中值性能提高10%，并且在所有参与者中显示出最高性能。这些结果证明了该方法的有效性和在社交媒体文本挖掘领域的应用潜力。<details>
<summary>Abstract</summary>
This paper outlines the performance evaluation of a system for adverse drug event normalization, developed by the Data Science for Digital Health group for the Social Media Mining for Health Applications 2023 shared task 5. Shared task 5 targeted the normalization of adverse drug event mentions in Twitter to standard concepts from the Medical Dictionary for Regulatory Activities terminology. Our system hinges on a two-stage approach: BERT fine-tuning for entity recognition, followed by zero-shot normalization using sentence transformers and reciprocal-rank fusion. The approach yielded a precision of 44.9%, recall of 40.5%, and an F1-score of 42.6%. It outperformed the median performance in shared task 5 by 10% and demonstrated the highest performance among all participants. These results substantiate the effectiveness of our approach and its potential application for adverse drug event normalization in the realm of social media text mining.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:这篇论文介绍了一种基于BERT微调和sentence transformers的社交媒体文本挖掘系统，用于正常化投诉病药事件。该系统采用了两 stageapproach：首先微调BERT进行实体识别，然后使用sentence transformers和reciprocal-rank fusions进行零批normalization。该approach实现了44.9%的精度、40.5%的准确率和42.6%的F1分数，比共享任务5中的中值性能提高10%，并达到了所有参与者中最高的性能。这些结果证明了该approach的有效性，并适用于社交媒体文本挖掘中的投诉病药事件正常化。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Visually-Rich-Document-Understanding-via-Layout-Structure-Modeling"><a href="#Enhancing-Visually-Rich-Document-Understanding-via-Layout-Structure-Modeling" class="headerlink" title="Enhancing Visually-Rich Document Understanding via Layout Structure Modeling"></a>Enhancing Visually-Rich Document Understanding via Layout Structure Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07777">http://arxiv.org/abs/2308.07777</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiwei Li, Zuchao Li, Xiantao Cai, Bo Du, Hai Zhao</li>
<li>for: 这 paper 的目的是提高文档理解的精度，特别是利用文档结构图模型文档的布局结构知识。</li>
<li>methods: 该 paper 提出了一种名为 GraphLayoutLM 的新型文档理解模型，该模型利用文档结构图模型文档的布局结构知识，并使用图重新排序算法和布局意识多头自注意力层来学习文档布局知识。</li>
<li>results: 该 paper 在多个 benchmark 上达到了最佳成绩，包括 FUNSD、XFUND 和 CORD 等 datasets，并且通过对模型组件的缺省研究，表明了每个组件的贡献。<details>
<summary>Abstract</summary>
In recent years, the use of multi-modal pre-trained Transformers has led to significant advancements in visually-rich document understanding. However, existing models have mainly focused on features such as text and vision while neglecting the importance of layout relationship between text nodes. In this paper, we propose GraphLayoutLM, a novel document understanding model that leverages the modeling of layout structure graph to inject document layout knowledge into the model. GraphLayoutLM utilizes a graph reordering algorithm to adjust the text sequence based on the graph structure. Additionally, our model uses a layout-aware multi-head self-attention layer to learn document layout knowledge. The proposed model enables the understanding of the spatial arrangement of text elements, improving document comprehension. We evaluate our model on various benchmarks, including FUNSD, XFUND and CORD, and achieve state-of-the-art results among these datasets. Our experimental results demonstrate that our proposed method provides a significant improvement over existing approaches and showcases the importance of incorporating layout information into document understanding models. We also conduct an ablation study to investigate the contribution of each component of our model. The results show that both the graph reordering algorithm and the layout-aware multi-head self-attention layer play a crucial role in achieving the best performance.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:在最近几年，基于多modal预训练的Transformers模型在文本 ricoh 理解方面带来了显著的进步。然而，现有的模型主要集中在文本和视觉特征之间，忽略了文档布局关系的重要性。在本文中，我们提出了 GraphLayoutLM 模型，它利用文档布局结构图来注入文档布局知识到模型中。GraphLayoutLM 模型使用图重新排序算法来根据图结构调整文本序列。此外，我们的模型还使用了布局意识多头自注意层来学习文档布局知识。该模型可以理解文本元素的空间排序，从而提高文档理解能力。我们在不同的benchmark上评估了我们的模型，包括FUNSD、XFUND和CORD等，并在这些数据集中达到了状态之最好的结果。我们的实验结果表明，我们提出的方法具有显著的改进，并证明了在文档理解模型中包含布局信息的重要性。我们还进行了一个ablation研究，以 Investigate each component of our model的贡献。结果显示，图重新排序算法和布局意识多头自注意层均对获得最佳性能做出了重要贡献。
</details></li>
</ul>
<hr>
<h2 id="SPM-Structured-Pretraining-and-Matching-Architectures-for-Relevance-Modeling-in-Meituan-Search"><a href="#SPM-Structured-Pretraining-and-Matching-Architectures-for-Relevance-Modeling-in-Meituan-Search" class="headerlink" title="SPM: Structured Pretraining and Matching Architectures for Relevance Modeling in Meituan Search"></a>SPM: Structured Pretraining and Matching Architectures for Relevance Modeling in Meituan Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07711">http://arxiv.org/abs/2308.07711</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wen Zan, Yaopeng Han, Xiaotian Jiang, Yao Xiao, Yang Yang, Dayao Chen, Sheng Chen</li>
<li>for: 提高生活服务平台上搜索结果的相关性，以提高用户体验。</li>
<li>methods: 提出了一种两stage预训练和匹配架构，使用了查询和文档多个字段作为输入，并使用了有效的信息压缩方法来处理长文档。</li>
<li>results: 经过大规模的实验和在线A&#x2F;B测试，表明提出的架构有效提高了搜索结果的相关性，已经在Meituan上线部署一年多。<details>
<summary>Abstract</summary>
In e-commerce search, relevance between query and documents is an essential requirement for satisfying user experience. Different from traditional e-commerce platforms that offer products, users search on life service platforms such as Meituan mainly for product providers, which usually have abundant structured information, e.g. name, address, category, thousands of products. Modeling search relevance with these rich structured contents is challenging due to the following issues: (1) there is language distribution discrepancy among different fields of structured document, making it difficult to directly adopt off-the-shelf pretrained language model based methods like BERT. (2) different fields usually have different importance and their length vary greatly, making it difficult to extract document information helpful for relevance matching.   To tackle these issues, in this paper we propose a novel two-stage pretraining and matching architecture for relevance matching with rich structured documents. At pretraining stage, we propose an effective pretraining method that employs both query and multiple fields of document as inputs, including an effective information compression method for lengthy fields. At relevance matching stage, a novel matching method is proposed by leveraging domain knowledge in search query to generate more effective document representations for relevance scoring. Extensive offline experiments and online A/B tests on millions of users verify that the proposed architectures effectively improve the performance of relevance modeling. The model has already been deployed online, serving the search traffic of Meituan for over a year.
</details>
<details>
<summary>摘要</summary>
在电商搜索中，搜索结果的相关性是用户体验的关键要求。与传统电商平台不同，用户在生活服务平台such as Meituan上查询主要是为了找到供应商，这些供应商通常有很多结构化信息，例如名称、地址、类别、千种产品。使用这些丰富的结构化内容进行搜索相关性模型化是有挑战的，因为：（1）不同的结构化文档字段存在语言分布差异，使得直接采用市场上已有预训练语言模型的方法如BERT不太可能。（2）不同的字段通常有不同的重要性和长度，使得提取文档信息有帮助于相关性匹配的部分很困难。为解决这些问题，本文提出了一种新的两Stage预训练和匹配架构，用于与结构化文档进行相关性模型化。预训练阶段，我们提出了一种有效的预训练方法，该方法使用查询和多个文档字段作为输入，并使用有效的信息压缩方法来处理长字段。匹配阶段，我们提出了一种基于搜索查询领域知识的新匹配方法，该方法可以更有效地生成文档表示，用于相关性分数。广泛的Offline实验和在线A/B测试表明，提出的架构有效地提高了相关性模型的性能。该模型已经在Meituan上线服务了一年多。
</details></li>
</ul>
<hr>
<h2 id="Better-Zero-Shot-Reasoning-with-Role-Play-Prompting"><a href="#Better-Zero-Shot-Reasoning-with-Role-Play-Prompting" class="headerlink" title="Better Zero-Shot Reasoning with Role-Play Prompting"></a>Better Zero-Shot Reasoning with Role-Play Prompting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07702">http://arxiv.org/abs/2308.07702</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HLT-NLP/Role-Play-Prompting">https://github.com/HLT-NLP/Role-Play-Prompting</a></li>
<li>paper_authors: Aobo Kong, Shiwan Zhao, Hao Chen, Qicheng Li, Yong Qin, Ruiqi Sun, Xin Zhou</li>
<li>for: 这种研究旨在探讨LLMs中的角色扮演如何影响其理解能力。</li>
<li>methods: 研究使用了一种策略性的角色扮演提示方法，在零基eline设定下测试了12种不同的理解准则，包括代数、常识理解、 симвоlic理解等。</li>
<li>results: 研究结果表明，使用角色扮演提示可以在大多数数据集上超越标准的零基eline方法，其中AQuA的准确率由53.5%提高到63.8%，Last Letter的准确率由23.8%提高到84.2%。这表明角色扮演提示可以提高LLMs的上下文理解和链条思维能力。<details>
<summary>Abstract</summary>
Modern large language models (LLMs), such as ChatGPT, exhibit a remarkable capacity for role-playing, enabling them to embody not only human characters but also non-human entities like a Linux terminal. This versatility allows them to simulate complex human-like interactions and behaviors within various contexts, as well as to emulate specific objects or systems. While these capabilities have enhanced user engagement and introduced novel modes of interaction, the influence of role-playing on LLMs' reasoning abilities remains underexplored. In this study, we introduce a strategically designed role-play prompting methodology and assess its performance under the zero-shot setting across twelve diverse reasoning benchmarks, encompassing arithmetic, commonsense reasoning, symbolic reasoning, and more. Leveraging models such as ChatGPT and Llama 2, our empirical results illustrate that role-play prompting consistently surpasses the standard zero-shot approach across most datasets. Notably, accuracy on AQuA rises from 53.5% to 63.8%, and on Last Letter from 23.8% to 84.2%. Beyond enhancing contextual understanding, we posit that role-play prompting serves as an implicit Chain-of-Thought (CoT) trigger, thereby improving the quality of reasoning. By comparing our approach with the Zero-Shot-CoT technique, which prompts the model to "think step by step", we further demonstrate that role-play prompting can generate a more effective CoT. This highlights its potential to augment the reasoning capabilities of LLMs.
</details>
<details>
<summary>摘要</summary>
现代大型语言模型（LLM），如ChatGPT，显示出了很强的角色扮演能力，可以不仅扮演人类角色，还可以模拟非人类Entity，如Linux终端。这种多样性使得它们能够模拟人类间的复杂互动和行为，以及模拟特定的对象或系统。虽然这些能力提高了用户参与度和引入了新的交互方式，但LLM的理解能力下的影响仍未得到足够的探索。在这项研究中，我们提出了一种策略性的角色扮演提问方法，并评估其在零基础设定下的性能。通过使用ChatGPT和Llama 2这两种模型，我们的实验结果表明，角色扮演提问在大多数数据集上都能够超越标准的零基础设定。特别是，AQuA的准确率由53.5%提高到63.8%，Last Letter的准确率由23.8%提高到84.2%。除了提高上下文理解，我们认为角色扮演提问可以作为隐藏链条（Chain-of-Thought，CoT）触发器，因此改善LLM的理解质量。通过与零基础CoT技术进行比较，我们进一步证明了角色扮演提问可以生成更有效的CoT。这说明它可以增强LLM的理解能力。
</details></li>
</ul>
<hr>
<h2 id="Attention-Is-Not-All-You-Need-Anymore"><a href="#Attention-Is-Not-All-You-Need-Anymore" class="headerlink" title="Attention Is Not All You Need Anymore"></a>Attention Is Not All You Need Anymore</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07661">http://arxiv.org/abs/2308.07661</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rprokap/pset-9">https://github.com/rprokap/pset-9</a></li>
<li>paper_authors: Zhe Chen</li>
<li>for: 本文提出了一种用于减少Transformer架构中自注意机制的计算和内存复杂性的drop-in替换方案，以提高Transformer的性能。</li>
<li>methods: 本文提出的Extractor可以作为Transformer的自注意机制替换，并且可以减少计算和内存复杂性。</li>
<li>results: 实验结果表明，使用Extractor可以提高Transformer的性能，并且它的计算路径更短，可以更快速地完成计算。<details>
<summary>Abstract</summary>
In recent years, the popular Transformer architecture has achieved great success in many application areas, including natural language processing and computer vision. Many existing works aim to reduce the computational and memory complexity of the self-attention mechanism in the Transformer by trading off performance. However, performance is key for the continuing success of the Transformer. In this paper, a drop-in replacement for the self-attention mechanism in the Transformer, called the Extractor, is proposed. Experimental results show that replacing the self-attention mechanism with the Extractor improves the performance of the Transformer. Furthermore, the proposed Extractor has the potential to run faster than the self-attention since it has a much shorter critical path of computation. Additionally, the sequence prediction problem in the context of text generation is formulated using variable-length discrete-time Markov chains, and the Transformer is reviewed based on our understanding.
</details>
<details>
<summary>摘要</summary>
近年来，受欢迎的Transformer架构在许多应用领域取得了很大成功，包括自然语言处理和计算机视觉。许多现有的工作尝试通过减少Transformer中的自注意机制的计算和内存复杂性，但是性能是Transformer继续成功的关键。在这篇论文中，一种可替换Transformer中的自注意机制，称为Extractor，被提议。实验结果表明，将自注意机制替换为Extractor可以提高Transformer的性能。此外，提议的Extractor可能比自注意机制更快速，因为它有许多短的计算路径。此外，在文本生成中的序列预测问题被形式化为变量长 discrete-time Markov链，并根据我们的理解对Transformer进行了评估。
</details></li>
</ul>
<hr>
<h2 id="SEER-Super-Optimization-Explorer-for-HLS-using-E-graph-Rewriting-with-MLIR"><a href="#SEER-Super-Optimization-Explorer-for-HLS-using-E-graph-Rewriting-with-MLIR" class="headerlink" title="SEER: Super-Optimization Explorer for HLS using E-graph Rewriting with MLIR"></a>SEER: Super-Optimization Explorer for HLS using E-graph Rewriting with MLIR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07654">http://arxiv.org/abs/2308.07654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianyi Cheng, Samuel Coward, Lorenzo Chelini, Rafael Barbalho, Theo Drane</li>
<li>for:  This paper aims to improve the performance of hardware designs produced by high-level synthesis (HLS) tools by automatically rewriting software programs into efficient HLS code.</li>
<li>methods:  The proposed method, called SEER, uses an e-graph data structure to efficiently explore equivalent implementations of a program at scale, and orchestrates existing software compiler passes and hardware synthesis optimizers.</li>
<li>results:  The paper shows that SEER achieves up to 38x the performance within 1.4x the area of the original program, and outperforms manually optimized designs produced by hardware experts in an Intel-provided case study.<details>
<summary>Abstract</summary>
High-level synthesis (HLS) is a process that automatically translates a software program in a high-level language into a low-level hardware description. However, the hardware designs produced by HLS tools still suffer from a significant performance gap compared to manual implementations. This is because the input HLS programs must still be written using hardware design principles.   Existing techniques either leave the program source unchanged or perform a fixed sequence of source transformation passes, potentially missing opportunities to find the optimal design. We propose a super-optimization approach for HLS that automatically rewrites an arbitrary software program into efficient HLS code that can be used to generate an optimized hardware design. We developed a toolflow named SEER, based on the e-graph data structure, to efficiently explore equivalent implementations of a program at scale. SEER provides an extensible framework, orchestrating existing software compiler passes and hardware synthesis optimizers.   Our work is the first attempt to exploit e-graph rewriting for large software compiler frameworks, such as MLIR. Across a set of open-source benchmarks, we show that SEER achieves up to 38x the performance within 1.4x the area of the original program. Via an Intel-provided case study, SEER demonstrates the potential to outperform manually optimized designs produced by hardware experts.
</details>
<details>
<summary>摘要</summary>
高级合成（HLS）是一个过程，它自动将高级语言程序转换为低级硬件描述。然而，由HLS工具生成的硬件设计仍然受到性能差距的影响，这是因为输入HLS程序仍需遵循硬件设计原则。现有的技术可能会留下程序源代码不变，或者执行固定的源代码转换步骤，可能会错过优化的机会。我们提出了一种超优化方法，它可以自动将任何软件程序转换为高效的HLS代码，可以生成优化的硬件设计。我们开发了一个名为SEER的工具流，基于e-graph数据结构，以高效地探索相当的实现方式。SEER提供了可扩展的框架，可以启用现有的软件编译器过程和硬件合成优化器。我们的工作是首次利用e-graph重写来大规模的软件编译框架，如MLIR。对一组开源 benchmark 进行测试，我们发现SEER可以达到38倍的性能，在1.4倍的面积内。通过Intel提供的案例研究，SEER还能够超越由硬件专家手动优化的设计。
</details></li>
</ul>
<hr>
<h2 id="Steering-Language-Generation-Harnessing-Contrastive-Expert-Guidance-and-Negative-Prompting-for-Coherent-and-Diverse-Synthetic-Data-Generation"><a href="#Steering-Language-Generation-Harnessing-Contrastive-Expert-Guidance-and-Negative-Prompting-for-Coherent-and-Diverse-Synthetic-Data-Generation" class="headerlink" title="Steering Language Generation: Harnessing Contrastive Expert Guidance and Negative Prompting for Coherent and Diverse Synthetic Data Generation"></a>Steering Language Generation: Harnessing Contrastive Expert Guidance and Negative Prompting for Coherent and Diverse Synthetic Data Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07645">http://arxiv.org/abs/2308.07645</a></li>
<li>repo_url: None</li>
<li>paper_authors: Charles O’Neill, Yuan-Sen Ting, Ioana Ciuca, Jack Miller, Thang Bui</li>
<li>for: 提高大语言模型生成的数据质量和多样性，以便下游模型训练和实际数据利用。</li>
<li>methods: 引入对比专家指导，以确保领域遵循性，并使用现有真实数据和 sintetic 示例作为负例准入，以保证多样性和 authenticty。</li>
<li>results: 比前一些生成数据技术提高表现，在三个不同任务中（假设生成、恶意和非恶意评论生成、常识理解任务生成） Displaying better balance between data diversity and coherence。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) hold immense potential to generate synthetic data of high quality and utility, which has numerous applications from downstream model training to practical data utilisation. However, contemporary models, despite their impressive capacities, consistently struggle to produce both coherent and diverse data. To address the coherency issue, we introduce contrastive expert guidance, where the difference between the logit distributions of fine-tuned and base language models is emphasised to ensure domain adherence. In order to ensure diversity, we utilise existing real and synthetic examples as negative prompts to the model. We deem this dual-pronged approach to logit reshaping as STEER: Semantic Text Enhancement via Embedding Repositioning. STEER operates at inference-time and systematically guides the LLMs to strike a balance between adherence to the data distribution (ensuring semantic fidelity) and deviation from prior synthetic examples or existing real datasets (ensuring diversity and authenticity). This delicate balancing act is achieved by dynamically moving towards or away from chosen representations in the latent space. STEER demonstrates improved performance over previous synthetic data generation techniques, exhibiting better balance between data diversity and coherency across three distinct tasks: hypothesis generation, toxic and non-toxic comment generation, and commonsense reasoning task generation. We demonstrate how STEER allows for fine-tuned control over the diversity-coherency trade-off via its hyperparameters, highlighting its versatility.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="LogPrompt-Prompt-Engineering-Towards-Zero-Shot-and-Interpretable-Log-Analysis"><a href="#LogPrompt-Prompt-Engineering-Towards-Zero-Shot-and-Interpretable-Log-Analysis" class="headerlink" title="LogPrompt: Prompt Engineering Towards Zero-Shot and Interpretable Log Analysis"></a>LogPrompt: Prompt Engineering Towards Zero-Shot and Interpretable Log Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07610">http://arxiv.org/abs/2308.07610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yilun Liu, Shimin Tao, Weibin Meng, Jingyu Wang, Wenbing Ma, Yanqing Zhao, Yuhang Chen, Hao Yang, Yanfei Jiang, Xun Chen</li>
<li>for: 本文提出了一种新的零shot和可解释的系统事件分析方法，以提高系统维护和工程生命周期中的可靠性和抗抗锋性。</li>
<li>methods: 本文使用大型自然语言模型（LLM）进行零shot系统事件分析任务，并采用了一系列高级提示策略，以提高LLM的性能。</li>
<li>results: 实验结果显示， LogPrompt 在九个公开的评估数据集上，在两个任务上表现出色，比既有方法（使用千余个日志）高于50%。此外， LogPrompt 的可解释性得到了专业人员的高度评估（4.42&#x2F;5）。<details>
<summary>Abstract</summary>
Automated log analysis is crucial in modern software-intensive systems for ensuring reliability and resilience throughout software maintenance and engineering life cycles. Existing methods perform tasks such as log parsing and log anomaly detection by providing a single prediction value without interpretation. However, given the increasing volume of system events, the limited interpretability of analysis results hinders analysts' trust and their ability to take appropriate actions. Moreover, these methods require substantial in-domain training data, and their performance declines sharply (by up to 62.5%) in online scenarios involving unseen logs from new domains, a common occurrence due to rapid software updates. In this paper, we propose LogPrompt, a novel zero-shot and interpretable log analysis approach. LogPrompt employs large language models (LLMs) to perform zero-shot log analysis tasks via a suite of advanced prompt strategies tailored for log tasks, which enhances LLMs' performance by up to 107.5% compared with simple prompts. Experiments on nine publicly available evaluation datasets across two tasks demonstrate that LogPrompt, despite using no training data, outperforms existing approaches trained on thousands of logs by up to around 50%. We also conduct a human evaluation of LogPrompt's interpretability, with six practitioners possessing over 10 years of experience, who highly rated the generated content in terms of usefulness and readability (averagely 4.42/5). LogPrompt also exhibits remarkable compatibility with open-source and smaller-scale LLMs, making it flexible for practical deployment.
</details>
<details>
<summary>摘要</summary>
现代软件强调系统中，自动化日志分析是关键要素，以确保软件稳定性和恢复能力在维护和工程生命周期中。现有方法可以完成日志分析任务，如日志分析和异常日志检测，但是这些方法通常只提供单个预测值而不具备解释。由于系统事件的增加，以及分析结果的有限可读性，分析人员对结果的信任和他们对结果的应用能力受到限制。此外，这些方法通常需要大量域内训练数据，并且在在线enario中（新领域的日志）发现的日志上进行分析时，其性能会下降（最多下降62.5%）。在这篇论文中，我们提出了一种新的零shot和可解释的日志分析方法——LogPrompt。LogPrompt使用大型自然语言模型（LLMs）来实现零shot日志分析任务，通过一组适用于日志任务的高级提示策略，提高LLMs的性能（最多提高107.5%）。我们在九个公共可用的评估数据集上进行了九个任务的实验，并证明了LogPrompt，即使没有使用任何训练数据，可以与已经训练 thousands of logs 的现有方法相比，在两个任务上提高性能（最多提高50%）。我们还进行了人类评估LogPrompt的可解释性，六位具有超过10年经验的实践者对生成的内容进行了评估，并评估结果表明，生成的内容在有用性和可读性方面得分4.42/5。此外，LogPrompt还表现出了remarkable的兼容性，可以与开源和较小规模的LLMs进行实际应用。
</details></li>
</ul>
<hr>
<h2 id="VBD-MT-Chinese-Vietnamese-Translation-Systems-for-VLSP-2022"><a href="#VBD-MT-Chinese-Vietnamese-Translation-Systems-for-VLSP-2022" class="headerlink" title="VBD-MT Chinese-Vietnamese Translation Systems for VLSP 2022"></a>VBD-MT Chinese-Vietnamese Translation Systems for VLSP 2022</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07601">http://arxiv.org/abs/2308.07601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hai Long Trieu, Song Kiet Bui, Tan Minh Tran, Van Khanh Tran, Hai An Nguyen</li>
<li>for: 本研究参加了2022年VLSP机器翻译共同任务。</li>
<li>methods: 我们基于神经网络模型的Transformer模型，使用了强大的多语言干扰预测模型mBART进行建构。我们还应用了一种采样方法来进行反向翻译，以利用大规模的可用单语言数据。此外，我们还应用了一些提高翻译质量的方法，包括拟合和后处理。</li>
<li>results: 我们在公共测试集上 achievement 38.9 BLEU在中越翻译和38.0 BLEU在越中翻译 tasks，这些成绩超过了一些强大的基eline。<details>
<summary>Abstract</summary>
We present our systems participated in the VLSP 2022 machine translation shared task. In the shared task this year, we participated in both translation tasks, i.e., Chinese-Vietnamese and Vietnamese-Chinese translations. We build our systems based on the neural-based Transformer model with the powerful multilingual denoising pre-trained model mBART. The systems are enhanced by a sampling method for backtranslation, which leverage large scale available monolingual data. Additionally, several other methods are applied to improve the translation quality including ensembling and postprocessing. We achieve 38.9 BLEU on ChineseVietnamese and 38.0 BLEU on VietnameseChinese on the public test sets, which outperform several strong baselines.
</details>
<details>
<summary>摘要</summary>
我们在VLSP 2022机器翻译共同任务中提交了我们的系统。本年度共同任务中，我们参与了中越文和越文中翻译两个任务。我们基于神经网络模型的Transformer模型，并使用大规模可用的单语言数据进行采样方法进行增强。此外，我们还应用了多种方法来提高翻译质量，包括集成和后处理。在公共测试集上，我们取得了38.9的BLEU指标在中越文翻译和38.0的BLEU指标在越文中翻译，这些成绩超过了一些强大的基线。
</details></li>
</ul>
<hr>
<h2 id="A-User-Centered-Evaluation-of-Spanish-Text-Simplification"><a href="#A-User-Centered-Evaluation-of-Spanish-Text-Simplification" class="headerlink" title="A User-Centered Evaluation of Spanish Text Simplification"></a>A User-Centered Evaluation of Spanish Text Simplification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07556">http://arxiv.org/abs/2308.07556</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrian de Wynter, Anthony Hevia, Si-Qing Chen</li>
<li>for: 这个论文的目的是评估西班牙语文本简化（TS）系统的生产性，通过复杂句子和复杂词语识别两个 corpora 进行评估。</li>
<li>methods: 这个论文使用了神经网络来比较西班牙语特有的阅读性分数，并显示神经网络在预测用户TS首选项上一直表现出色。作者们还发现，多语言模型在同一任务上下降性能，但所有模型往往围绕幻数统计特征，如句子长度，进行围绕。</li>
<li>results: 作者们发现，神经网络在同一任务上一直表现出色，而且可以准确预测用户TS首选项。同时，作者们发现多语言模型在同一任务上下降性能，并且发现所有模型往往围绕幻数统计特征，如句子长度，进行围绕。<details>
<summary>Abstract</summary>
We present an evaluation of text simplification (TS) in Spanish for a production system, by means of two corpora focused in both complex-sentence and complex-word identification. We compare the most prevalent Spanish-specific readability scores with neural networks, and show that the latter are consistently better at predicting user preferences regarding TS. As part of our analysis, we find that multilingual models underperform against equivalent Spanish-only models on the same task, yet all models focus too often on spurious statistical features, such as sentence length. We release the corpora in our evaluation to the broader community with the hopes of pushing forward the state-of-the-art in Spanish natural language processing.
</details>
<details>
<summary>摘要</summary>
我们对西班牙语文本简化（TS）的评估进行了一种生产系统的研究，通过两个聚合了复杂句子和复杂词的字句 corpus 进行了评估。我们将西班牙语特有的阅读性分数与神经网络进行比较，并发现后者在预测用户对TS的偏好时表现更好。在我们的分析中，我们发现了多语言模型在同一任务上下降表现，然而所有模型都太过注重干扰性的统计特征，如句子长度。我们将我们的评估 corpora 公开发布给广泛的社区，希望能够推动西班牙自然语言处理领域的前沿。
</details></li>
</ul>
<hr>
<h2 id="Improving-CTC-AED-model-with-integrated-CTC-and-auxiliary-loss-regularization"><a href="#Improving-CTC-AED-model-with-integrated-CTC-and-auxiliary-loss-regularization" class="headerlink" title="Improving CTC-AED model with integrated-CTC and auxiliary loss regularization"></a>Improving CTC-AED model with integrated-CTC and auxiliary loss regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08449">http://arxiv.org/abs/2308.08449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daobin Zhu, Xiangdong Su, Hongbin Zhang</li>
<li>for:  automatic speech recognition (ASR)</li>
<li>methods:  Connectionist temporal classification (CTC) and attention-based encoder decoder (AED) joint training, with two fusion methods (DAL and PMP) and auxiliary loss regularization</li>
<li>results:  Experimental results show that DAL method performs better in attention rescoring, while PMP method excels in CTC prefix beam search and greedy search.<details>
<summary>Abstract</summary>
Connectionist temporal classification (CTC) and attention-based encoder decoder (AED) joint training has been widely applied in automatic speech recognition (ASR). Unlike most hybrid models that separately calculate the CTC and AED losses, our proposed integrated-CTC utilizes the attention mechanism of AED to guide the output of CTC. In this paper, we employ two fusion methods, namely direct addition of logits (DAL) and preserving the maximum probability (PMP). We achieve dimensional consistency by adaptively affine transforming the attention results to match the dimensions of CTC. To accelerate model convergence and improve accuracy, we introduce auxiliary loss regularization for accelerated convergence. Experimental results demonstrate that the DAL method performs better in attention rescoring, while the PMP method excels in CTC prefix beam search and greedy search.
</details>
<details>
<summary>摘要</summary>
Connectionist temporal classification (CTC) 和 attention-based encoder decoder (AED) 的共同训练已经广泛应用在自动语音识别（ASR）中。与大多数混合模型不同，我们提出的集成-CTC 使用 AED 的注意力机制来导引 CTC 的输出。在这篇论文中，我们采用了两种合并方法，namely direct addition of logits (DAL) 和 preserving the maximum probability (PMP)。我们通过适应性折射变换来保持维度的一致性，以适应 CTC 的维度。为了加速模型的启动和提高准确性，我们引入了辅助损失补偿。实验结果表明，DAL 方法在注意力重新评分中表现更好，而 PMP 方法在 CTC 前缀搜索和扩散搜索中表现更好。
</details></li>
</ul>
<hr>
<h2 id="CALYPSO-LLMs-as-Dungeon-Masters’-Assistants"><a href="#CALYPSO-LLMs-as-Dungeon-Masters’-Assistants" class="headerlink" title="CALYPSO: LLMs as Dungeon Masters’ Assistants"></a>CALYPSO: LLMs as Dungeon Masters’ Assistants</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07540">http://arxiv.org/abs/2308.07540</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/northern-lights-province/calypso-aiide-artifact">https://github.com/northern-lights-province/calypso-aiide-artifact</a></li>
<li>paper_authors: Andrew Zhu, Lara J. Martin, Andrew Head, Chris Callison-Burch</li>
<li>for: 这篇论文的目的是探讨用大自然语言模型（LLM）在桌面角色扮演游戏（D&amp;D）中的应用场景，以及这些技术在桌面游戏中的可能性。</li>
<li>methods: 该论文使用了大自然语言模型（GPT-3和ChatGPT）来生成合理的自然语言文本，并通过与游戏导ilder（DM）进行形成评估，以确定LLM在D&amp;D中的应用场景。</li>
<li>results: 研究发现，当给DM们提供LLM-力Point的支持时，他们表示可以 direktly present高品质的自然语言文本给玩家，以及低品质的想法，以便继续保持创作主义。这种方法可以帮助DMs在游戏中提供更多的创新和灵感，而无需干扰他们的创作过程。<details>
<summary>Abstract</summary>
The role of a Dungeon Master, or DM, in the game Dungeons & Dragons is to perform multiple tasks simultaneously. The DM must digest information about the game setting and monsters, synthesize scenes to present to other players, and respond to the players' interactions with the scene. Doing all of these tasks while maintaining consistency within the narrative and story world is no small feat of human cognition, making the task tiring and unapproachable to new players. Large language models (LLMs) like GPT-3 and ChatGPT have shown remarkable abilities to generate coherent natural language text. In this paper, we conduct a formative evaluation with DMs to establish the use cases of LLMs in D&D and tabletop gaming generally. We introduce CALYPSO, a system of LLM-powered interfaces that support DMs with information and inspiration specific to their own scenario. CALYPSO distills game context into bite-sized prose and helps brainstorm ideas without distracting the DM from the game. When given access to CALYPSO, DMs reported that it generated high-fidelity text suitable for direct presentation to players, and low-fidelity ideas that the DM could develop further while maintaining their creative agency. We see CALYPSO as exemplifying a paradigm of AI-augmented tools that provide synchronous creative assistance within established game worlds, and tabletop gaming more broadly.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Finding-Stakeholder-Material-Information-from-10-K-Reports-using-Fine-Tuned-BERT-and-LSTM-Models"><a href="#Finding-Stakeholder-Material-Information-from-10-K-Reports-using-Fine-Tuned-BERT-and-LSTM-Models" class="headerlink" title="Finding Stakeholder-Material Information from 10-K Reports using Fine-Tuned BERT and LSTM Models"></a>Finding Stakeholder-Material Information from 10-K Reports using Fine-Tuned BERT and LSTM Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07522">http://arxiv.org/abs/2308.07522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Victor Zitian Chen</li>
<li>For: The paper aims to identify stakeholder-material information in annual 10-K reports to help companies and investors efficiently extract material information.* Methods: The authors fine-tuned BERT models and RNN models with LSTM layers to identify stakeholder-material information, using business expert-labeled training data.* Results: The best model achieved an accuracy of 0.904 and an F1 score of 0.899 in test data, significantly outperforming the baseline model.<details>
<summary>Abstract</summary>
All public companies are required by federal securities law to disclose their business and financial activities in their annual 10-K reports. Each report typically spans hundreds of pages, making it difficult for human readers to identify and extract the material information efficiently. To solve the problem, I have fine-tuned BERT models and RNN models with LSTM layers to identify stakeholder-material information, defined as statements that carry information about a company's influence on its stakeholders, including customers, employees, investors, and the community and natural environment. The existing practice uses keyword search to identify such information, which is my baseline model. Using business expert-labeled training data of nearly 6,000 sentences from 62 10-K reports published in 2022, the best model has achieved an accuracy of 0.904 and an F1 score of 0.899 in test data, significantly above the baseline model's 0.781 and 0.749 respectively. Furthermore, the same work was replicated on more granular taxonomies, based on which four distinct groups of stakeholders (i.e., customers, investors, employees, and the community and natural environment) are tested separately. Similarly, fined-tuned BERT models outperformed LSTM and the baseline. The implications for industry application and ideas for future extensions are discussed.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)所有公开公司都需要根据联邦证券法披露其业务和财务活动在每年的10-K报告中。每份报告通常包含数百页的内容，使得人类读者很难快速 identificar和提取重要信息。为解决这个问题，我已经细化BERT模型和RNN模型的LSTM层来标识利益相关者材料信息，其定义为公司对利益相关者（包括客户、员工、投资者和社区和自然环境）的影响信息。现行做法使用关键词搜索来标识这类信息，这是我的基线模型。使用2022年62份10-K报告中的商业专家标注训练数据（约6,000句），最佳模型在测试数据中达到了0.904的准确率和0.899的F1得分，与基线模型的0.781和0.749分别显著上升。此外，同样的工作也在更细化的分类中进行了重复，基于这四个不同的利益相关者组（即客户、投资者、员工和社区和自然环境）进行了分开测试。同样，细化BERT模型也超过了LSTM和基线模型。关于业务应用和未来扩展的想法都是讨论的。
</details></li>
</ul>
<hr>
<h2 id="Data-Race-Detection-Using-Large-Language-Models"><a href="#Data-Race-Detection-Using-Large-Language-Models" class="headerlink" title="Data Race Detection Using Large Language Models"></a>Data Race Detection Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07505">http://arxiv.org/abs/2308.07505</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Le Chen, Xianzhong Ding, Murali Emani, Tristan Vanderbruggen, Pei-hung Lin, Chuanhua Liao</li>
<li>for: 本研究旨在探讨一种基于大语言模型（LLM）的数据竞争检测方法，以代替手动创建资源投入庞大的工具。</li>
<li>methods: 本研究使用了提示工程和精度调整技术，创建了专门的DRB-ML数据集，并使用了代表性的LLM和开源LLM进行评估。</li>
<li>results: 研究显示，LLM可以成为数据竞争检测的可能性，但是它们还无法与传统数据竞争检测工具相比提供详细的变量对 causing 数据竞争的信息。<details>
<summary>Abstract</summary>
Large language models (LLMs) are demonstrating significant promise as an alternate strategy to facilitate analyses and optimizations of high-performance computing programs, circumventing the need for resource-intensive manual tool creation. In this paper, we explore a novel LLM-based data race detection approach combining prompting engineering and fine-tuning techniques. We create a dedicated dataset named DRB-ML, which is derived from DataRaceBench, with fine-grain labels showing the presence of data race pairs and their associated variables, line numbers, and read/write information. DRB-ML is then used to evaluate representative LLMs and fine-tune open-source ones. Our experiment shows that LLMs can be a viable approach to data race detection. However, they still cannot compete with traditional data race detection tools when we need detailed information about variable pairs causing data races.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SOTASTREAM-A-Streaming-Approach-to-Machine-Translation-Training"><a href="#SOTASTREAM-A-Streaming-Approach-to-Machine-Translation-Training" class="headerlink" title="SOTASTREAM: A Streaming Approach to Machine Translation Training"></a>SOTASTREAM: A Streaming Approach to Machine Translation Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07489">http://arxiv.org/abs/2308.07489</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/marian-nmt/sotastream">https://github.com/marian-nmt/sotastream</a></li>
<li>paper_authors: Matt Post, Thamme Gowda, Roman Grundkiewicz, Huda Khayrallah, Rohit Jain, Marcin Junczys-Dowmunt</li>
<li>for: 这 paper aims to address the limitations of traditional data preparation methods for machine translation toolkits, which can be time-consuming, expensive, and cumbersome.</li>
<li>methods: The proposed approach separates the generation of data from its consumption, allowing for on-the-fly modifications and eliminating the need for a separate pre-processing step.</li>
<li>results: The proposed approach reduces training time, adds flexibility, reduces experiment management complexity, and reduces disk space without affecting the accuracy of the trained models.Here’s the simplified Chinese text:</li>
<li>for: 这 paper 的目的是解决传统机器翻译工具集的数据准备方法的限制，这些方法可能会占用很多时间、成本和复杂度。</li>
<li>methods: 提议的方法是将数据生成和数据消耗分离开来，这样允许在实际使用过程中进行实时修改，并完全消除预处理步骤。</li>
<li>results: 提议的方法可以减少训练时间、添加灵活性、减少实验管理复杂度和减少磁盘空间，而不影响训练出来的模型的准确性。<details>
<summary>Abstract</summary>
Many machine translation toolkits make use of a data preparation step wherein raw data is transformed into a tensor format that can be used directly by the trainer. This preparation step is increasingly at odds with modern research and development practices because this process produces a static, unchangeable version of the training data, making common training-time needs difficult (e.g., subword sampling), time-consuming (preprocessing with large data can take days), expensive (e.g., disk space), and cumbersome (managing experiment combinatorics). We propose an alternative approach that separates the generation of data from the consumption of that data. In this approach, there is no separate pre-processing step; data generation produces an infinite stream of permutations of the raw training data, which the trainer tensorizes and batches as it is consumed. Additionally, this data stream can be manipulated by a set of user-definable operators that provide on-the-fly modifications, such as data normalization, augmentation or filtering. We release an open-source toolkit, SOTASTREAM, that implements this approach: https://github.com/marian-nmt/sotastream. We show that it cuts training time, adds flexibility, reduces experiment management complexity, and reduces disk space, all without affecting the accuracy of the trained models.
</details>
<details>
<summary>摘要</summary>
许多机器翻译工具包括一个数据准备步骤，将原始数据转换成可直接用于训练的张量格式。这个过程在现代研发实践中变得越来越不合适，因为这会生成一个静态、不可变的版本的训练数据，使得一些常见的训练时间需求（如字符抽样）变得困难、时间consuming（处理大量数据可以花费多天）、昂贵（如磁盘空间）和困难（实验组合管理）。我们提出一种新的方法，即将数据生成与数据消耗分离开来。在这种方法中，没有单独的预处理步骤；数据生成生成了无限长的Permutation序列，这些 permutation被训练者张量化并批处理，直到它们被消耗。此外，这个数据流可以通过用户定义的操作符进行实时修改，例如数据Normalization、扩展或筛选。我们发布了一个开源工具kit，SOTASTREAM，实现了这种方法：https://github.com/marian-nmt/sotastream。我们表明，它可以减少训练时间，添加灵活性，降低实验管理复杂度，并降低磁盘空间，而无需影响训练出来的模型准确性。
</details></li>
</ul>
<hr>
<h2 id="O-1-Self-training-with-Oracle-and-1-best-Hypothesis"><a href="#O-1-Self-training-with-Oracle-and-1-best-Hypothesis" class="headerlink" title="O-1: Self-training with Oracle and 1-best Hypothesis"></a>O-1: Self-training with Oracle and 1-best Hypothesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07486">http://arxiv.org/abs/2308.07486</a></li>
<li>repo_url: None</li>
<li>paper_authors: Murali Karthick Baskar, Andrew Rosenberg, Bhuvana Ramabhadran, Kartik Audhkhasi</li>
<li>for: 提高Speech Recognition训练的准确率和评估 metrics</li>
<li>methods: 使用O-1自适应目标函数，可以处理both超级vised和无级vised数据，并且可以减少训练偏见</li>
<li>results: O-1对SpeechStew数据集和一个大规模的内部数据集进行评估，与EMBR相比，O-1可以将实际和oracle表现之间的差距减少80%，并且在不同的SpeechStew数据集上实现13%-25%的相对改善，对EMBR训练的内部数据集也可以减少12%的差距。总的来说，O-1可以提高WER的准确率9%。<details>
<summary>Abstract</summary>
We introduce O-1, a new self-training objective to reduce training bias and unify training and evaluation metrics for speech recognition. O-1 is a faster variant of Expected Minimum Bayes Risk (EMBR), that boosts the oracle hypothesis and can accommodate both supervised and unsupervised data. We demonstrate the effectiveness of our approach in terms of recognition on publicly available SpeechStew datasets and a large-scale, in-house data set. On Speechstew, the O-1 objective closes the gap between the actual and oracle performance by 80\% relative compared to EMBR which bridges the gap by 43\% relative. O-1 achieves 13\% to 25\% relative improvement over EMBR on the various datasets that SpeechStew comprises of, and a 12\% relative gap reduction with respect to the oracle WER over EMBR training on the in-house dataset. Overall, O-1 results in a 9\% relative improvement in WER over EMBR, thereby speaking to the scalability of the proposed objective for large-scale datasets.
</details>
<details>
<summary>摘要</summary>
我们介绍O-1，一个新的自我训练目标，用于降低训练偏见和统一训练和评估指标 для语音识别。O-1是EMBR的快速版本，可以提高oracle假设，并可以处理both监控和无监控数据。我们透过使用O-1目标，在公开ailable的SpeechStew数据集和一个大规模的内部数据集上进行评估。在SpeechStew上，O-1目标可以关闭实际和oracle性能之间的差距 by 80% relative compared to EMBR，而EMBR则可以关闭差距 by 43% relative。O-1在不同的SpeechStew数据集上的表现亮眼，比EMBR高13%到25% relative，并且与oracle WER之间的差距降低12% relative。总的来说，O-1对EMBR的WER进行了9%的相对改善，证明了O-1目标的扩展性。
</details></li>
</ul>
<hr>
<h2 id="Development-and-Evaluation-of-Three-Chatbots-for-Postpartum-Mood-and-Anxiety-Disorders"><a href="#Development-and-Evaluation-of-Three-Chatbots-for-Postpartum-Mood-and-Anxiety-Disorders" class="headerlink" title="Development and Evaluation of Three Chatbots for Postpartum Mood and Anxiety Disorders"></a>Development and Evaluation of Three Chatbots for Postpartum Mood and Anxiety Disorders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07407">http://arxiv.org/abs/2308.07407</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuewen Yao, Miriam Mikhelson, S. Craig Watkins, Eunsol Choi, Edison Thomaz, Kaya de Barbaro</li>
<li>for: 本研究的目的是开发一些聊天机器人，以提供新生婴期护理者所需的情感支持。</li>
<li>methods: 我们使用了规则引导的和生成模型，以提供上下文特定的同情支持。</li>
<li>results: 我们的规则引导模型表现最佳，其输出与真实参考数据几乎相同，同时含有最高水平的同情。人工用户对规则引导聊天机器人表示喜欢，因为它的回答具有上下文特定和人类化的特点。生成模型也能生成同情的回答，但由于训练数据的限制，它的回答经常具有含糊不清的问题。<details>
<summary>Abstract</summary>
In collaboration with Postpartum Support International (PSI), a non-profit organization dedicated to supporting caregivers with postpartum mood and anxiety disorders, we developed three chatbots to provide context-specific empathetic support to postpartum caregivers, leveraging both rule-based and generative models. We present and evaluate the performance of our chatbots using both machine-based metrics and human-based questionnaires. Overall, our rule-based model achieves the best performance, with outputs that are close to ground truth reference and contain the highest levels of empathy. Human users prefer the rule-based chatbot over the generative chatbot for its context-specific and human-like replies. Our generative chatbot also produced empathetic responses and was described by human users as engaging. However, limitations in the training dataset often result in confusing or nonsensical responses. We conclude by discussing practical benefits of rule-based vs. generative models for supporting individuals with mental health challenges. In light of the recent surge of ChatGPT and BARD, we also discuss the possibilities and pitfalls of large language models for digital mental healthcare.
</details>
<details>
<summary>摘要</summary>
合作 Postpartum Support International (PSI) 非营利组织，我们开发了三个聊天机器人，以提供适应性强的同理支持给孕后照顾者，利用规则基本和生成模型。我们对聊天机器人的表现进行评估，使用机器人和人类Questionnaire。总的来说，我们的规则基本模型实现了最好的表现，输出与真实参照接近，同时具有最高水平的同理。人类用户对规则基本聊天机器人的喜欢度最高，因为它的回答具有Context-specific和人类化的特点。我们的生成模型也生成了同理的回答，但是训练数据的限制导致它们的回答有时会很混乱或无意义。我们 conclude 规则基本模型和生成模型在支持人们 mental health 挑战时的实际效用，以及 ChatGPT 和 BARD 等大语言模型在数字 mental healthcare 中的可能性和风险。
</details></li>
</ul>
<hr>
<h2 id="Text-Injection-for-Capitalization-and-Turn-Taking-Prediction-in-Speech-Models"><a href="#Text-Injection-for-Capitalization-and-Turn-Taking-Prediction-in-Speech-Models" class="headerlink" title="Text Injection for Capitalization and Turn-Taking Prediction in Speech Models"></a>Text Injection for Capitalization and Turn-Taking Prediction in Speech Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07395">http://arxiv.org/abs/2308.07395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaan Bijwadia, Shuo-yiin Chang, Weiran Wang, Zhong Meng, Hao Zhang, Tara N. Sainath</li>
<li>for: 提高 auxiliary 任务表现（非ASR任务）</li>
<li>methods: 使用文本注入（JEIT）训练 ASR 模型，并在两个 auxiliary 任务上进行训练</li>
<li>results: 文本注入方法可以提高长尾数据的首字母排序性能，并提高转接推断精度<details>
<summary>Abstract</summary>
Text injection for automatic speech recognition (ASR), wherein unpaired text-only data is used to supplement paired audio-text data, has shown promising improvements for word error rate. This study examines the use of text injection for auxiliary tasks, which are the non-ASR tasks often performed by an E2E model. In this work, we use joint end-to-end and internal language model training (JEIT) as our text injection algorithm to train an ASR model which performs two auxiliary tasks. The first is capitalization, which is a de-normalization task. The second is turn-taking prediction, which attempts to identify whether a user has completed their conversation turn in a digital assistant interaction. We show results demonstrating that our text injection method boosts capitalization performance for long-tail data, and improves turn-taking detection recall.
</details>
<details>
<summary>摘要</summary>
文本注入技术可以用于自动语音识别（ASR），其中使用无对应的文本数据来补充带有音频数据的对应数据，有效地降低了单词错误率。本研究探讨了文本注入技术在辅助任务中的应用，这些任务通常是END-TO-END模型完成的非ASR任务。在这个工作中，我们使用了结合端到端和内部语言模型训练（JEIT）作为我们的文本注入算法，用于训练一个ASR模型，该模型完成了两个辅助任务。第一个是字母大小 normalization 任务，第二个是判断用户是否已经完成了在数字助手交互中的对话转移。我们的实验结果表明，我们的文本注入方法可以提高长尾数据中的字母大小正确率，并提高了对话转移检测的准确率。
</details></li>
</ul>
<hr>
<h2 id="Using-Text-Injection-to-Improve-Recognition-of-Personal-Identifiers-in-Speech"><a href="#Using-Text-Injection-to-Improve-Recognition-of-Personal-Identifiers-in-Speech" class="headerlink" title="Using Text Injection to Improve Recognition of Personal Identifiers in Speech"></a>Using Text Injection to Improve Recognition of Personal Identifiers in Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07393">http://arxiv.org/abs/2308.07393</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yochai Blau, Rohan Agrawal, Lior Madmony, Gary Wang, Andrew Rosenberg, Zhehuai Chen, Zorik Gekhman, Genady Beryozkin, Parisa Haghani, Bhuvana Ramabhadran</li>
<li>for: 提高自动语音识别（ASR）系统中个人特定信息（PII）的识别率。</li>
<li>methods: 使用文本插入法将假文本替换PII类别，以提高训练数据中PII类别的识别率。</li>
<li>results: 在医疗记录中提高了名称和日期的回忆率，同时提高了总的word error rate（WER）。对数字序列也显示了改善Character Error Rate和句子准确率。<details>
<summary>Abstract</summary>
Accurate recognition of specific categories, such as persons' names, dates or other identifiers is critical in many Automatic Speech Recognition (ASR) applications. As these categories represent personal information, ethical use of this data including collection, transcription, training and evaluation demands special care. One way of ensuring the security and privacy of individuals is to redact or eliminate Personally Identifiable Information (PII) from collection altogether. However, this results in ASR models that tend to have lower recognition accuracy of these categories. We use text-injection to improve the recognition of PII categories by including fake textual substitutes of PII categories in the training data using a text injection method. We demonstrate substantial improvement to Recall of Names and Dates in medical notes while improving overall WER. For alphanumeric digit sequences we show improvements to Character Error Rate and Sentence Accuracy.
</details>
<details>
<summary>摘要</summary>
准确地识别特定类别，如人名、日期等标识信息，在自动语音识别（ASR）应用中是非常重要的。这些类别代表个人信息，因此对这些数据的采集、译写、训练和评估需要特殊的注意。一种方法是完全不收集人类标识信息（PII），但这会导致ASR模型对这些类别的识别精度下降。我们使用文本插入法来提高PII类别的识别精度，通过在训练数据中插入假文本substitute来实现。我们在医疗笔记中示出了大幅提高名称和日期的回忆率，同时提高总的word Error Rate。对于字符串数字序列，我们示出了字符错误率和句子准确率的改善。
</details></li>
</ul>
<hr>
<h2 id="Platypus-Quick-Cheap-and-Powerful-Refinement-of-LLMs"><a href="#Platypus-Quick-Cheap-and-Powerful-Refinement-of-LLMs" class="headerlink" title="Platypus: Quick, Cheap, and Powerful Refinement of LLMs"></a>Platypus: Quick, Cheap, and Powerful Refinement of LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07317">http://arxiv.org/abs/2308.07317</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/arielnlee/Platypus">https://github.com/arielnlee/Platypus</a></li>
<li>paper_authors: Ariel N. Lee, Cole J. Hunter, Nataniel Ruiz</li>
<li>for: 这个论文是为了描述一个名为Platypus的家族 Large Language Models (LLMs)，它们在 HuggingFace 的开放 LLM Leaderboard 上达到了最高的表现并现在位于第一名。</li>
<li>methods: 这个论文使用了一个名为 Open-Platypus 的精心准备和合并 LoRA 模块，以保留预训练 LLMs 的强大优先知识，同时将特定领域知识带到表面。</li>
<li>results:  Platypus 家族在量化 LLM 度量上表现出色，在模型尺寸上占据了全球 Open LLM leaderboard 的排名，而使用的 fine-tuning 数据和总计算量只是其他 state-of-the-art fine-tuned LLMs 所需的一小部分。例如，一个 13B Platypus 模型可以在单个 A100 GPU 上使用 25k 问题进行 5 小时的训练。这是 Open-Platypus 数据集的质量的证明，并开启了更多改进的可能性。<details>
<summary>Abstract</summary>
We present $\textbf{Platypus}$, a family of fine-tuned and merged Large Language Models (LLMs) that achieves the strongest performance and currently stands at first place in HuggingFace's Open LLM Leaderboard as of the release date of this work. In this work we describe (1) our curated dataset $\textbf{Open-Platypus}$, that is a subset of other open datasets and which $\textit{we release to the public}$ (2) our process of fine-tuning and merging LoRA modules in order to conserve the strong prior of pretrained LLMs, while bringing specific domain knowledge to the surface (3) our efforts in checking for test data leaks and contamination in the training data, which can inform future research. Specifically, the Platypus family achieves strong performance in quantitative LLM metrics across model sizes, topping the global Open LLM leaderboard while using just a fraction of the fine-tuning data and overall compute that are required for other state-of-the-art fine-tuned LLMs. In particular, a 13B Platypus model can be trained on $\textit{a single}$ A100 GPU using 25k questions in 5 hours. This is a testament of the quality of our Open-Platypus dataset, and opens opportunities for more improvements in the field. Project page: https://platypus-llm.github.io
</details>
<details>
<summary>摘要</summary>
我们现在提出了$\textbf{ Platypus}$家族，这是一些精心调整和合并的大语言模型（LLMs），它在HuggingFace的开源LLM排名榜上 currently stands at first place as of the release date of this work. 在这个工作中，我们描述了我们的手动抽象 dataset $\textbf{Open-Platypus}$，这是其他开放数据集的一个子集，并且 $\textit{我们向公众发布了这些数据}$。我们的过程包括了精心调整和合并LoRA模块，以保留预训练LLMs的强大优先知识，同时将特定领域知识带到表面。我们还尽可能地检查测试数据泄露和训练数据污染，以便未来的研究。特别是，Platypus家族在量化LLM指标中表现出色，在模型尺寸上占据全球开源LLM排名榜首位，而使用的是比其他 state-of-the-art 精心调整LLMs的一部分的精心调整数据和总计算资源。例如，一个13B Platypus模型可以在 $\textit{单个}$ A100 GPU 上使用 25k 问题，在 5 小时内训练完成。这是一个证明我们 Open-Platypus 数据集的质量，并开创了更多的改进机会。项目页面：https://platypus-llm.github.io
</details></li>
</ul>
<hr>
<h2 id="The-Devil-is-in-the-Errors-Leveraging-Large-Language-Models-for-Fine-grained-Machine-Translation-Evaluation"><a href="#The-Devil-is-in-the-Errors-Leveraging-Large-Language-Models-for-Fine-grained-Machine-Translation-Evaluation" class="headerlink" title="The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation"></a>The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07286">http://arxiv.org/abs/2308.07286</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André F. T. Martins, Graham Neubig, Ankush Garg, Jonathan H. Clark, Markus Freitag, Orhan Firat</li>
<li>for: 这篇论文是为了提供一种自动评估机器翻译（MT）系统的方法，以便在MT系统的快速迭代发展中进行评估。</li>
<li>methods: 这篇论文使用了大语言模型（LLM）的理解和在场景学习能力，并让它们标注翻译中的错误。</li>
<li>results: 研究发现，使用AutoMQM技术可以提高MT系统的性能，特别是使用更大的模型时。此外，AutoMQM还提供了解释性的错误块，与人工标注相Alignment。<details>
<summary>Abstract</summary>
Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iterative development of MT systems. While considerable progress has been made on estimating a single scalar quality score, current metrics lack the informativeness of more detailed schemes that annotate individual errors, such as Multidimensional Quality Metrics (MQM). In this paper, we help fill this gap by proposing AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particularly large gains for larger models) while providing interpretability through error spans that align with human annotations.
</details>
<details>
<summary>摘要</summary>
自动评估机器翻译（MT）是翻译系统的快速迭代发展的重要工具。虽然已经取得了较大的进步，但当前的度量仍然缺乏详细的错误标注，如多维质量指标（MQM）。在这篇文章中，我们帮助填补这个空白，并提出了AutoMQM技术，它利用大型自然语言模型（LLM）的理解和上下文学习能力，并让它们标注和分类翻译中的错误。我们首先通过对最近的LLM，如PaLM和PaLM-2，进行简单的分数预测提问，并研究了标注数据的影响。然后，我们评估了AutoMQM技术，并发现它在比只是提问分数时提高性能（尤其是大型模型），并提供了解释性的错误排序。
</details></li>
</ul>
<hr>
<h2 id="Comparison-between-parameter-efficient-techniques-and-full-fine-tuning-A-case-study-on-multilingual-news-article-classification"><a href="#Comparison-between-parameter-efficient-techniques-and-full-fine-tuning-A-case-study-on-multilingual-news-article-classification" class="headerlink" title="Comparison between parameter-efficient techniques and full fine-tuning: A case study on multilingual news article classification"></a>Comparison between parameter-efficient techniques and full fine-tuning: A case study on multilingual news article classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07282">http://arxiv.org/abs/2308.07282</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olesya Razuvayevskaya, Ben Wu, Joao A. Leite, Freddy Heppell, Ivan Srba, Carolina Scarton, Kalina Bontcheva, Xingyi Song</li>
<li>for: 本研究旨在investigating parameter-efficient fine-tuning techniques的影响于多语言文本分类任务（类型、框架和说服技巧检测），包括不同的输入长度、预测类数和分类难度。</li>
<li>methods: 本研究使用了Adaptors和LoRA技术来实现parameter-efficient fine-tuning，并进行了对不同训练场景（训练原始多语言数据、翻译成英语和英语只数据）和不同语言的深入分析。</li>
<li>results: 研究发现，在多语言文本分类任务中，Adaptors和LoRA技术可以减少训练时间和计算成本，并且在某些情况下可以提高性能。<details>
<summary>Abstract</summary>
Adapters and Low-Rank Adaptation (LoRA) are parameter-efficient fine-tuning techniques designed to make the training of language models more efficient. Previous results demonstrated that these methods can even improve performance on some classification tasks. This paper complements the existing research by investigating how these techniques influence the classification performance and computation costs compared to full fine-tuning when applied to multilingual text classification tasks (genre, framing, and persuasion techniques detection; with different input lengths, number of predicted classes and classification difficulty), some of which have limited training data. In addition, we conduct in-depth analyses of their efficacy across different training scenarios (training on the original multilingual data; on the translations into English; and on a subset of English-only data) and different languages. Our findings provide valuable insights into the applicability of the parameter-efficient fine-tuning techniques, particularly to complex multilingual and multilabel classification tasks.
</details>
<details>
<summary>摘要</summary>
这篇文章进一步探讨了微调和低阶化适应（LoRA）技术的影响，它们是用于对语言模型进行更有效的训练。过往的研究显示这些技术可以提高一些分类任务的性能。本文在多种多元文本分类任务（文类、几何、说服等）中进行了广泛的实验，包括有限的训练数据。此外，我们还进行了不同训练enario（训练原始多种语言数据；训练英文翻译；和使用英文数据subset）和不同语言的深入分析。我们的发现将有价值的帮助在复杂的多种语言和多类分类任务中应用这些参数有效的微调技术。
</details></li>
</ul>
<hr>
<h2 id="Dialogue-for-Prompting-a-Policy-Gradient-Based-Discrete-Prompt-Optimization-for-Few-shot-Learning"><a href="#Dialogue-for-Prompting-a-Policy-Gradient-Based-Discrete-Prompt-Optimization-for-Few-shot-Learning" class="headerlink" title="Dialogue for Prompting: a Policy-Gradient-Based Discrete Prompt Optimization for Few-shot Learning"></a>Dialogue for Prompting: a Policy-Gradient-Based Discrete Prompt Optimization for Few-shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07272">http://arxiv.org/abs/2308.07272</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengzhengxu Li, Xiaoming Liu, Yichen Wang, Duyi Li, Yu Lan, Chao Shen</li>
<li>for: 提高几何学NLU任务中的表现，减少专家知识和人工干预。</li>
<li>methods: 对PLMs进行对话分析，设计可读性检测 metric，使用RL框架和政策网络进行优化。</li>
<li>results: 在四个开源数据集上，DP_2O方法在几何学NLU任务中的表现高于SOTA方法1.52%，并且具有良好的通用性、Robustness和普适性。<details>
<summary>Abstract</summary>
Prompt-based pre-trained language models (PLMs) paradigm have succeeded substantially in few-shot natural language processing (NLP) tasks. However, prior discrete prompt optimization methods require expert knowledge to design the base prompt set and identify high-quality prompts, which is costly, inefficient, and subjective. Meanwhile, existing continuous prompt optimization methods improve the performance by learning the ideal prompts through the gradient information of PLMs, whose high computational cost, and low readability and generalizability are often concerning. To address the research gap, we propose a Dialogue-comprised Policy-gradient-based Discrete Prompt Optimization ($DP_2O$) method. We first design a multi-round dialogue alignment strategy for readability prompt set generation based on GPT-4. Furthermore, we propose an efficient prompt screening metric to identify high-quality prompts with linear complexity. Finally, we construct a reinforcement learning (RL) framework based on policy gradients to match the prompts to inputs optimally. By training a policy network with only 0.67% of the PLM parameter size on the tasks in the few-shot setting, $DP_2O$ outperforms the state-of-the-art (SOTA) method by 1.52% in accuracy on average on four open-source datasets. Moreover, subsequent experiments also demonstrate that $DP_2O$ has good universality, robustness, and generalization ability.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/15/cs.CL_2023_08_15/" data-id="clorjzl3w009pf188dffo2p37" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/15/cs.LG_2023_08_15/" class="article-date">
  <time datetime="2023-08-15T10:00:00.000Z" itemprop="datePublished">2023-08-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/15/cs.LG_2023_08_15/">cs.LG - 2023-08-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Dyadic-Reinforcement-Learning"><a href="#Dyadic-Reinforcement-Learning" class="headerlink" title="Dyadic Reinforcement Learning"></a>Dyadic Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07843">http://arxiv.org/abs/2308.07843</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/statisticalreinforcementlearninglab/roadmap2.0testbed">https://github.com/statisticalreinforcementlearninglab/roadmap2.0testbed</a></li>
<li>paper_authors: Shuangning Li, Lluis Salvat Niell, Sung Won Choi, Inbal Nahum-Shani, Guy Shani, Susan Murphy</li>
<li>for: 该论文旨在提高健康结果，通过在日常生活中提供便捷的干预方法。</li>
<li>methods: 该论文提出了一种基于在线演进学习算法，以个性化干预发送方式，基于Contextual因素和target人和他们的护理伴侣之前的反应。</li>
<li>results: 通过在模拟场景和实际数据集上进行实验研究，提出了一种 bayesian和层次的dyadic RL算法，并证明了其可预测性。<details>
<summary>Abstract</summary>
Mobile health aims to enhance health outcomes by delivering interventions to individuals as they go about their daily life. The involvement of care partners and social support networks often proves crucial in helping individuals managing burdensome medical conditions. This presents opportunities in mobile health to design interventions that target the dyadic relationship -- the relationship between a target person and their care partner -- with the aim of enhancing social support. In this paper, we develop dyadic RL, an online reinforcement learning algorithm designed to personalize intervention delivery based on contextual factors and past responses of a target person and their care partner. Here, multiple sets of interventions impact the dyad across multiple time intervals. The developed dyadic RL is Bayesian and hierarchical. We formally introduce the problem setup, develop dyadic RL and establish a regret bound. We demonstrate dyadic RL's empirical performance through simulation studies on both toy scenarios and on a realistic test bed constructed from data collected in a mobile health study.
</details>
<details>
<summary>摘要</summary>
Mobile health aimed to enhance health outcomes by delivering interventions to individuals as they go about their daily life. The involvement of care partners and social support networks often proved crucial in helping individuals manage burdensome medical conditions. This presented opportunities in mobile health to design interventions that targeted the dyadic relationship - the relationship between a target person and their care partner - with the aim of enhancing social support. In this paper, we developed dyadic RL, an online reinforcement learning algorithm designed to personalize intervention delivery based on contextual factors and past responses of a target person and their care partner. Here, multiple sets of interventions impacted the dyad across multiple time intervals. The developed dyadic RL was Bayesian and hierarchical. We formally introduced the problem setup, developed dyadic RL, and established a regret bound. We demonstrated dyadic RL's empirical performance through simulation studies on both toy scenarios and on a realistic test bed constructed from data collected in a mobile health study.
</details></li>
</ul>
<hr>
<h2 id="Simple-and-Efficient-Partial-Graph-Adversarial-Attack-A-New-Perspective"><a href="#Simple-and-Efficient-Partial-Graph-Adversarial-Attack-A-New-Perspective" class="headerlink" title="Simple and Efficient Partial Graph Adversarial Attack: A New Perspective"></a>Simple and Efficient Partial Graph Adversarial Attack: A New Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07834">http://arxiv.org/abs/2308.07834</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pasalab/pga">https://github.com/pasalab/pga</a></li>
<li>paper_authors: Guanghui Zhu, Mengyu Chen, Chunfeng Yuan, Yihua Huang</li>
<li>for: 提高图 neural network 的 robustness和安全性，针对图中所有节点的全球攻击方法。</li>
<li>methods: 提出一种全新的partial graph attack（PGA）方法，选择易于攻击的节点作为攻击目标，并提出了一种层次目标选择策略、一种成本效果较高的锚点选择策略和一种迭代循环增强的迭代式攻击方法。</li>
<li>results: 对比其他现有的图全球攻击方法，PGA可以实现显著提高攻击效果和攻击效率。<details>
<summary>Abstract</summary>
As the study of graph neural networks becomes more intensive and comprehensive, their robustness and security have received great research interest. The existing global attack methods treat all nodes in the graph as their attack targets. Although existing methods have achieved excellent results, there is still considerable space for improvement. The key problem is that the current approaches rigidly follow the definition of global attacks. They ignore an important issue, i.e., different nodes have different robustness and are not equally resilient to attacks. From a global attacker's view, we should arrange the attack budget wisely, rather than wasting them on highly robust nodes. To this end, we propose a totally new method named partial graph attack (PGA), which selects the vulnerable nodes as attack targets. First, to select the vulnerable items, we propose a hierarchical target selection policy, which allows attackers to only focus on easy-to-attack nodes. Then, we propose a cost-effective anchor-picking policy to pick the most promising anchors for adding or removing edges, and a more aggressive iterative greedy-based attack method to perform more efficient attacks. Extensive experimental results demonstrate that PGA can achieve significant improvements in both attack effect and attack efficiency compared to other existing graph global attack methods.
</details>
<details>
<summary>摘要</summary>
Our approach consists of three key components:1. Hierarchical target selection policy: This policy allows attackers to focus on easy-to-attack nodes, reducing the overall cost of the attack.2. Cost-effective anchor-picking policy: This policy selects the most promising anchors for adding or removing edges, maximizing the attack effect while minimizing the cost.3. Iterative greedy-based attack method: This method performs more efficient attacks by iteratively adding or removing edges based on the selected anchors.Our extensive experimental results show that PGA achieves significant improvements in both attack effect and attack efficiency compared to other existing graph global attack methods.
</details></li>
</ul>
<hr>
<h2 id="REFORMS-Reporting-Standards-for-Machine-Learning-Based-Science"><a href="#REFORMS-Reporting-Standards-for-Machine-Learning-Based-Science" class="headerlink" title="REFORMS: Reporting Standards for Machine Learning Based Science"></a>REFORMS: Reporting Standards for Machine Learning Based Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07832">http://arxiv.org/abs/2308.07832</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sayash Kapoor, Emily Cantrell, Kenny Peng, Thanh Hien Pham, Christopher A. Bail, Odd Erik Gundersen, Jake M. Hofman, Jessica Hullman, Michael A. Lones, Momin M. Malik, Priyanka Nanayakkara, Russell A. Poldrack, Inioluwa Deborah Raji, Michael Roberts, Matthew J. Salganik, Marta Serra-Garcia, Brandon M. Stewart, Gilles Vandewiele, Arvind Narayanan</li>
<li>For: The paper aims to provide clear reporting standards for machine learning (ML) based science to address the issues of validity, reproducibility, and generalizability in scientific research.* Methods: The paper presents the REFORMS checklist, a set of 32 questions and guidelines developed based on a consensus of 19 researchers across various disciplines.* Results: The REFORMS checklist can serve as a resource for researchers, referees, and journals to ensure transparency and reproducibility in ML-based scientific research.Here is the information in Simplified Chinese text:* For: 这篇论文目标是提供机器学习（ML）基于科学研究的清晰报告标准，以解决科学研究中有效性、可重现性和普适性的问题。* Methods: 论文提出了REFORMS检查表（Reporting Standards For Machine Learning Based Science），这是基于19位研究者来自计算机科学、数据科学、数学、社会科学和生物医学等领域的共识，包括32个问题和对应的指南。* Results: REFORMS检查表可以为研究者、审稿人和杂志编辑提供一个资源，以确保机器学习基于科学研究的透明度和可重现性。<details>
<summary>Abstract</summary>
Machine learning (ML) methods are proliferating in scientific research. However, the adoption of these methods has been accompanied by failures of validity, reproducibility, and generalizability. These failures can hinder scientific progress, lead to false consensus around invalid claims, and undermine the credibility of ML-based science. ML methods are often applied and fail in similar ways across disciplines. Motivated by this observation, our goal is to provide clear reporting standards for ML-based science. Drawing from an extensive review of past literature, we present the REFORMS checklist ($\textbf{Re}$porting Standards $\textbf{For}$ $\textbf{M}$achine Learning Based $\textbf{S}$cience). It consists of 32 questions and a paired set of guidelines. REFORMS was developed based on a consensus of 19 researchers across computer science, data science, mathematics, social sciences, and biomedical sciences. REFORMS can serve as a resource for researchers when designing and implementing a study, for referees when reviewing papers, and for journals when enforcing standards for transparency and reproducibility.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）方法在科学研究中广泛应用，但是其应用也伴随着有效性、可重复性和普遍性的失败。这些失败可能会阻碍科学进步，导致无效的宣称得到共识，并且可能会下降机器学习基于科学的威信。机器学习方法经常在不同领域应用并失败，这使我们意识到了需要提供明确的报告标准。基于过去的文献检索，我们提出了REFORMS检查列表（Reporting Standards For Machine Learning Based Science）。它包括32个问题和一对拥有相同目标的指南。REFORMS是由19名来自计算机科学、数据科学、数学、社会科学和生物医学科学的研究人员共识而成，它可以作为研究人员设计和实施研究时的参考，同时也可以用于审稿人们审核文章，以及杂志 enforcing 透明度和可重复性的标准。
</details></li>
</ul>
<hr>
<h2 id="CMISR-Circular-Medical-Image-Super-Resolution"><a href="#CMISR-Circular-Medical-Image-Super-Resolution" class="headerlink" title="CMISR: Circular Medical Image Super-Resolution"></a>CMISR: Circular Medical Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08567">http://arxiv.org/abs/2308.08567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Honggui Li, Maria Trocan, Dimitri Galayko, Mohamad Sawan</li>
<li>for: 提高医疗影像超分辨率（MISR）的性能，提出一种基于循环反馈的医疗影像超分辨率框架（CMISR）。</li>
<li>methods: 使用循环反馈机制，分为本地反馈和全局反馈两类，实现了关键点稳定性和稳定性。</li>
<li>results: CMISR在三种缩放因子和三个开源医疗影像dataset上的实验结果表明，其在重建性能方面胜过传统MISR，特别适用于医疗影像中具有强的边缘或激烈对比。<details>
<summary>Abstract</summary>
Classical methods of medical image super-resolution (MISR) utilize open-loop architecture with implicit under-resolution (UR) unit and explicit super-resolution (SR) unit. The UR unit can always be given, assumed, or estimated, while the SR unit is elaborately designed according to various SR algorithms. The closed-loop feedback mechanism is widely employed in current MISR approaches and can efficiently improve their performance. The feedback mechanism may be divided into two categories: local and global feedback. Therefore, this paper proposes a global feedback-based closed-cycle framework, circular MISR (CMISR), with unambiguous UR and SR elements. Mathematical model and closed-loop equation of CMISR are built. Mathematical proof with Taylor-series approximation indicates that CMISR has zero recovery error in steady-state. In addition, CMISR holds plug-and-play characteristic which can be established on any existing MISR algorithms. Five CMISR algorithms are respectively proposed based on the state-of-the-art open-loop MISR algorithms. Experimental results with three scale factors and on three open medical image datasets show that CMISR is superior to MISR in reconstruction performance and is particularly suited to medical images with strong edges or intense contrast.
</details>
<details>
<summary>摘要</summary>
传统的医疗影像超分辨 (MISR) 方法使用开放式架构，其中隐式下解 (UR) 单元和显式超分辨 (SR) 单元是分开的。UR单元可以被给定、 Assuming 或估算，而SR单元则根据不同的SR算法进行精心设计。现有的MISR方法广泛采用了关闭着反馈机制，可以有效提高其性能。反馈机制可以分为两类：本地反馈和全球反馈。因此，本文提出了一种基于全球反馈的循环式框架，即循环MISR (CMISR)，其中UR和SR元素具有明确的定义。我们建立了CMISR的数学模型和关闭着方程，并通过泰勒级数拟合得出了CMISR在稳态状态下的零回归误差。此外，CMISR具有插件和玩儿特性，可以在任何现有的MISR算法基础上实现。我们根据现有的开放式MISR算法，分别提出了5种CMISR算法。实验结果表明，CMISR在重建性能方面高于MISR，特别适用于医疗影像中具有强的边缘或激烈的对比。
</details></li>
</ul>
<hr>
<h2 id="Cerberus-A-Deep-Learning-Hybrid-Model-for-Lithium-Ion-Battery-Aging-Estimation-and-Prediction-Based-on-Relaxation-Voltage-Curves"><a href="#Cerberus-A-Deep-Learning-Hybrid-Model-for-Lithium-Ion-Battery-Aging-Estimation-and-Prediction-Based-on-Relaxation-Voltage-Curves" class="headerlink" title="Cerberus: A Deep Learning Hybrid Model for Lithium-Ion Battery Aging Estimation and Prediction Based on Relaxation Voltage Curves"></a>Cerberus: A Deep Learning Hybrid Model for Lithium-Ion Battery Aging Estimation and Prediction Based on Relaxation Voltage Curves</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07824">http://arxiv.org/abs/2308.07824</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Xiang, Bo Jiang, Haifeng Dai</li>
<li>For: The paper aims to estimate and predict the capacity aging of lithium-ion batteries using a hybrid model based on deep learning, which can accurately forecast the future capacity of the batteries.* Methods: The model uses historical capacity decay data and extracts salient features from charge and discharge relaxation processes to estimate the present capacity and predict future capacity.* Results: The model achieves a mean absolute percentage error (MAPE) of 0.29% under a charging condition of 0.25C, demonstrating its effectiveness in estimating and predicting capacity aging using real-world relaxation processes and historical capacity records within battery management systems (BMS).<details>
<summary>Abstract</summary>
The degradation process of lithium-ion batteries is intricately linked to their entire lifecycle as power sources and energy storage devices, encompassing aspects such as performance delivery and cycling utilization. Consequently, the accurate and expedient estimation or prediction of the aging state of lithium-ion batteries has garnered extensive attention. Nonetheless, prevailing research predominantly concentrates on either aging estimation or prediction, neglecting the dynamic fusion of both facets. This paper proposes a hybrid model for capacity aging estimation and prediction based on deep learning, wherein salient features highly pertinent to aging are extracted from charge and discharge relaxation processes. By amalgamating historical capacity decay data, the model dynamically furnishes estimations of the present capacity and forecasts of future capacity for lithium-ion batteries. Our approach is validated against a novel dataset involving charge and discharge cycles at varying rates. Specifically, under a charging condition of 0.25C, a mean absolute percentage error (MAPE) of 0.29% is achieved. This outcome underscores the model's adeptness in harnessing relaxation processes commonly encountered in the real world and synergizing with historical capacity records within battery management systems (BMS), thereby affording estimations and prognostications of capacity decline with heightened precision.
</details>
<details>
<summary>摘要</summary>
锂离子电池的衰退过程与其整个生命周期深度相关，涵盖性能提供和能量存储等方面。因此，正确和快速地估计或预测锂离子电池的衰退状况备受广泛关注。然而，现有研究主要集中在 either 衰退估计或预测，忽视了这两个方面的动态融合。本文提出了一种基于深度学习的锂离子电池容量衰退估计和预测模型，其中抽象出了具有衰退相关性的充电和充放电过程特征。通过结合历史容量衰退数据，模型在实时提供了当前容量的估计和未来容量的预测，并且在充电条件下0.25C下达到了0.29%的平均绝对百分比误差（MAPE）。这一结果表明模型能够充分利用实际世界中常见的充电和充放电过程，同时与锂离子电池管理系统（BMS）中的历史容量记录相结合，从而为容量衰退的估计和预测提供了更高精度。
</details></li>
</ul>
<hr>
<h2 id="Deep-reinforcement-learning-for-process-design-Review-and-perspective"><a href="#Deep-reinforcement-learning-for-process-design-Review-and-perspective" class="headerlink" title="Deep reinforcement learning for process design: Review and perspective"></a>Deep reinforcement learning for process design: Review and perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07822">http://arxiv.org/abs/2308.07822</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Qinghe Gao, Artur M. Schweidtmann</li>
<li>for: 本研究旨在探讨人工智能技术如何加速化学工业中的可再生能源和原料供应转型。</li>
<li>methods: 本研究使用深度强化学习，一种机器学习技术，来解决化学工程中复杂的决策问题，并且探讨了这些技术在过程设计中的应用前景。</li>
<li>results: 本研究对现有的深度强化学习在过程设计中的应用进行了抽象和评估，并探讨了未来这些技术在化学工程中的发展前景。<details>
<summary>Abstract</summary>
The transformation towards renewable energy and feedstock supply in the chemical industry requires new conceptual process design approaches. Recently, breakthroughs in artificial intelligence offer opportunities to accelerate this transition. Specifically, deep reinforcement learning, a subclass of machine learning, has shown the potential to solve complex decision-making problems and aid sustainable process design. We survey state-of-the-art research in reinforcement learning for process design through three major elements: (i) information representation, (ii) agent architecture, and (iii) environment and reward. Moreover, we discuss perspectives on underlying challenges and promising future works to unfold the full potential of reinforcement learning for process design in chemical engineering.
</details>
<details>
<summary>摘要</summary>
“对于化学工业中的可再生能源和原料供应转型，需要新的概念过程设计方法。现在，人工智能技术的突破发展带来了加速这个转型的机遇。特别是深度强化学习，一种机器学习的 subclass，在解决复杂决策问题和推动可持续过程设计方面表现出了潜力。我们通过三个主要元素：（i）信息表示，（ii）代理架构，以及（iii）环境和奖励，总结了现代研究的深度强化学习在过程设计方面的状况。此外，我们还讨论了下一步的挑战和未来研究的前景，以探索深度强化学习在化学工程中的潜力。”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Quantifying-the-Cost-of-Learning-in-Queueing-Systems"><a href="#Quantifying-the-Cost-of-Learning-in-Queueing-Systems" class="headerlink" title="Quantifying the Cost of Learning in Queueing Systems"></a>Quantifying the Cost of Learning in Queueing Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07817">http://arxiv.org/abs/2308.07817</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Freund, Thodoris Lykouris, Wentao Weng</li>
<li>For:  This paper is written for researchers and practitioners interested in queueing systems and their optimal control, particularly in the context of parameter uncertainty.* Methods: The paper proposes a new metric called the Cost of Learning in Queueing (CLQ) to quantify the maximum increase in time-averaged queue length caused by parameter uncertainty. The authors also propose a unified analysis framework that bridges Lyapunov and bandit analysis to establish the results.* Results: The paper characterizes the CLQ of a single-queue multi-server system and extends the results to multi-queue multi-server systems and networks of queues. The authors show that the CLQ is a useful metric for evaluating the performance of queueing systems in the presence of parameter uncertainty.<details>
<summary>Abstract</summary>
Queueing systems are widely applicable stochastic models with use cases in communication networks, healthcare, service systems, etc. Although their optimal control has been extensively studied, most existing approaches assume perfect knowledge of system parameters. Of course, this assumption rarely holds in practice where there is parameter uncertainty, thus motivating a recent line of work on bandit learning for queueing systems. This nascent stream of research focuses on the asymptotic performance of the proposed algorithms.   In this paper, we argue that an asymptotic metric, which focuses on late-stage performance, is insufficient to capture the intrinsic statistical complexity of learning in queueing systems which typically occurs in the early stage. Instead, we propose the Cost of Learning in Queueing (CLQ), a new metric that quantifies the maximum increase in time-averaged queue length caused by parameter uncertainty. We characterize the CLQ of a single-queue multi-server system, and then extend these results to multi-queue multi-server systems and networks of queues. In establishing our results, we propose a unified analysis framework for CLQ that bridges Lyapunov and bandit analysis, which could be of independent interest.
</details>
<details>
<summary>摘要</summary>
queueing 系统是广泛应用的随机模型，有用cases在通信网络、医疗、服务系统等。虽然其优化控制得到了广泛的研究，但大多数现有方法假设系统参数具有完美的知识。然而，这种假设在实践中rarely holds，因此引起了一种Recent Line of Work on Bandit Learning for Queueing Systems。这个流行的研究方向主要关注 asymptotic performance of the proposed algorithms。在这篇论文中，我们 argue that an asymptotic metric, which focuses on late-stage performance, is insufficient to capture the intrinsic statistical complexity of learning in queueing systems, which typically occurs in the early stage。 Instead, we propose the Cost of Learning in Queueing (CLQ), a new metric that quantifies the maximum increase in time-averaged queue length caused by parameter uncertainty。 We characterize the CLQ of a single-queue multi-server system, and then extend these results to multi-queue multi-server systems and networks of queues。在证明我们的结果时，我们提出了一个统一的分析框架 для CLQ，该框架可以将 Lyapunov 和 bandit 分析相结合，这可能会对独立的研究有益。
</details></li>
</ul>
<hr>
<h2 id="Fairness-and-Privacy-in-Federated-Learning-and-Their-Implications-in-Healthcare"><a href="#Fairness-and-Privacy-in-Federated-Learning-and-Their-Implications-in-Healthcare" class="headerlink" title="Fairness and Privacy in Federated Learning and Their Implications in Healthcare"></a>Fairness and Privacy in Federated Learning and Their Implications in Healthcare</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07805">http://arxiv.org/abs/2308.07805</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/UVA-MLSys/DS7406">https://github.com/UVA-MLSys/DS7406</a></li>
<li>paper_authors: Navya Annapareddy, Jade Preston, Judy Fox<br>for: This paper aims to provide an overview of the typical lifecycle of fair federated learning in research and an updated taxonomy to account for the current state of fairness in implementations, with a focus on the healthcare domain.methods: The paper uses a decentralized approach to training machine learning models, called federated learning, to address data security, privacy, and vulnerability considerations.results: The paper highlights the challenges of implementing fairness in federated learning, including node data not being independent and identically distributed (iid), clients requiring high levels of communication overhead between peers, and the heterogeneity of different clients within a network with respect to dataset bias and size. The paper also provides added insight into the implications and challenges of implementing and supporting fairness in federated learning in the healthcare domain.<details>
<summary>Abstract</summary>
Currently, many contexts exist where distributed learning is difficult or otherwise constrained by security and communication limitations. One common domain where this is a consideration is in Healthcare where data is often governed by data-use-ordinances like HIPAA. On the other hand, larger sample sizes and shared data models are necessary to allow models to better generalize on account of the potential for more variability and balancing underrepresented classes. Federated learning is a type of distributed learning model that allows data to be trained in a decentralized manner. This, in turn, addresses data security, privacy, and vulnerability considerations as data itself is not shared across a given learning network nodes. Three main challenges to federated learning include node data is not independent and identically distributed (iid), clients requiring high levels of communication overhead between peers, and there is the heterogeneity of different clients within a network with respect to dataset bias and size. As the field has grown, the notion of fairness in federated learning has also been introduced through novel implementations. Fairness approaches differ from the standard form of federated learning and also have distinct challenges and considerations for the healthcare domain. This paper endeavors to outline the typical lifecycle of fair federated learning in research as well as provide an updated taxonomy to account for the current state of fairness in implementations. Lastly, this paper provides added insight into the implications and challenges of implementing and supporting fairness in federated learning in the healthcare domain.
</details>
<details>
<summary>摘要</summary>
当前，有许多情况存在分布式学习是困难或受到安全和通信限制的情况。一个常见的领域是医疗领域，数据经常受到数据使用规定如HIPAA的限制。然而，更大的样本大小和共享数据模型是必要的，以使模型更好地泛化，因为可能存在更多的变化和平衡不足表示的类别。分布式学习是一种分布式学习模型，它使得数据在分布式学习网络中被训练，并解决了数据安全、隐私和抵触问题，因为数据本身不被分布式学习网络中的节点共享。主要挑战包括节点数据不是独立和同分布（iid）、客户需要高度的同域通信开销和客户网络中的数据偏好和大小不均。随着领域的发展，对分布式学习的公平性也被引入，并通过新的实现方式。公平性方法与标准的分布式学习不同，也有特殊的挑战和医疗领域中的考虑。本文尝试将研究中的公平分布式学习的典型生命周期和更新的分类表示出来，并提供了对当前公平性实现的更多的深入视角。最后，本文还提供了在实施和支持公平分布式学习在医疗领域中的挑战和问题。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Noise-Covariance-Estimation-under-Colored-Noise-using-Dynamic-Expectation-Maximization"><a href="#Adaptive-Noise-Covariance-Estimation-under-Colored-Noise-using-Dynamic-Expectation-Maximization" class="headerlink" title="Adaptive Noise Covariance Estimation under Colored Noise using Dynamic Expectation Maximization"></a>Adaptive Noise Covariance Estimation under Colored Noise using Dynamic Expectation Maximization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07797">http://arxiv.org/abs/2308.07797</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ajitham123/DEM_NCM">https://github.com/ajitham123/DEM_NCM</a></li>
<li>paper_authors: Ajith Anil Meera, Pablo Lanillos</li>
<li>for: 这篇论文是为了提出一个新的脑心理静电组织（Brain-Inspired Algorithm），用于精确地估计动态系统中的噪音协调矩阵（Noise Covariance Matrix，NCM）。</li>
<li>methods: 这个算法extend了Dynamic Expectation Maximization（DEM）算法，以在线上估计噪音协调矩阵和状态估计，并且可以适应彩色噪音（colored noise）的情况。</li>
<li>results: 透过Randomized numerical simulations，我们展示了我们的估计方法在彩色噪音下比基准方法（Variational Bayes）更好，并且在高彩色噪音情况下也能够实现更好的结果。<details>
<summary>Abstract</summary>
The accurate estimation of the noise covariance matrix (NCM) in a dynamic system is critical for state estimation and control, as it has a major influence in their optimality. Although a large number of NCM estimation methods have been developed, most of them assume the noises to be white. However, in many real-world applications, the noises are colored (e.g., they exhibit temporal autocorrelations), resulting in suboptimal solutions. Here, we introduce a novel brain-inspired algorithm that accurately and adaptively estimates the NCM for dynamic systems subjected to colored noise. Particularly, we extend the Dynamic Expectation Maximization algorithm to perform both online noise covariance and state estimation by optimizing the free energy objective. We mathematically prove that our NCM estimator converges to the global optimum of this free energy objective. Using randomized numerical simulations, we show that our estimator outperforms nine baseline methods with minimal noise covariance estimation error under colored noise conditions. Notably, we show that our method outperforms the best baseline (Variational Bayes) in joint noise and state estimation for high colored noise. We foresee that the accuracy and the adaptive nature of our estimator make it suitable for online estimation in real-world applications.
</details>
<details>
<summary>摘要</summary>
预测动态系统中噪声 covariance 矩阵（NCM）的准确性是控制和状态估计中关键的，因为它对系统的优化产生了很大的影响。虽然有很多 NCMEstimation 方法已经开发，但大多数它们假设噪声是白噪声（即噪声无相关性）。然而，在实际应用中，噪声通常是染色的（即噪声展现了时间自相关性），从而导致估计结果不佳。在这篇文章中，我们介绍了一种基于脑神经元的新算法，可以准确地适应 colored noise 的动态系统 NCM 估计。我们在 Dynamic Expectation Maximization 算法的基础上扩展了该算法，以在线进行噪声 covariance 和状态估计，并通过优化自由能对象来实现。我们数学证明了我们的 NCMEstimation 算法 converge 到 global optimum 的自由能对象上。使用随机数字 simulations，我们示出了我们的估计算法在噪声 Conditions 下比基eline方法（Variational Bayes）的噪声 covariance 估计误差较低。特别是，我们示出了我们的方法在高染色噪声 Conditions 下与 Variational Bayes 的联合噪声和状态估计表现较好。我们认为我们的方法的准确性和适应性使其适用于实际应用中的在线估计。
</details></li>
</ul>
<hr>
<h2 id="Implementing-Quantum-Generative-Adversarial-Network-qGAN-and-QCBM-in-Finance"><a href="#Implementing-Quantum-Generative-Adversarial-Network-qGAN-and-QCBM-in-Finance" class="headerlink" title="Implementing Quantum Generative Adversarial Network (qGAN) and QCBM in Finance"></a>Implementing Quantum Generative Adversarial Network (qGAN) and QCBM in Finance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08448">http://arxiv.org/abs/2308.08448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Santanu Ganguly</li>
<li>for: 这个论文探讨了应用量子机器学习（QML）在金融领域的未来研究方向，以及在金融世界中各种应用的QML模型。</li>
<li>methods: 本文使用实际的金融数据和模拟环境，比较了不同QML模型的性能，包括qGAN和QCBM等。</li>
<li>results: 研究显示，QML在金融领域可能具有未来的优势，并且qGAN模型在某些情况下表现出了明显的优势。<details>
<summary>Abstract</summary>
Quantum machine learning (QML) is a cross-disciplinary subject made up of two of the most exciting research areas: quantum computing and classical machine learning (ML), with ML and artificial intelligence (AI) being projected as the first fields that will be impacted by the rise of quantum machines. Quantum computers are being used today in drug discovery, material & molecular modelling and finance. In this work, we discuss some upcoming active new research areas in application of quantum machine learning (QML) in finance. We discuss certain QML models that has become areas of active interest in the financial world for various applications. We use real world financial dataset and compare models such as qGAN (quantum generative adversarial networks) and QCBM (quantum circuit Born machine) among others, using simulated environments. For the qGAN, we define quantum circuits for discriminators and generators and show promises of future quantum advantage via QML in finance.
</details>
<details>
<summary>摘要</summary>
量子机器学习（QML）是一个跨学科领域，包括量子计算和经典机器学习（ML），被认为是未来量子机器的发展将首先影响的两个领域之一。现在，量子计算机已经在药物发现、物质和分子模拟以及金融领域中使用。在这个工作中，我们讨论了在金融领域应用量子机器学习（QML）的一些新 aktive研究领域。我们讨论了一些在金融界引起了广泛关注的QML模型，如quantum generative adversarial networks（qGAN）和Quantum Circuit Born Machine（QCBM）等，并使用实际世界金融数据进行比较。对于qGAN，我们定义了量子电路 для批分类器和生成器，并显示了未来量子优势的承诺。
</details></li>
</ul>
<hr>
<h2 id="Informed-Named-Entity-Recognition-Decoding-for-Generative-Language-Models"><a href="#Informed-Named-Entity-Recognition-Decoding-for-Generative-Language-Models" class="headerlink" title="Informed Named Entity Recognition Decoding for Generative Language Models"></a>Informed Named Entity Recognition Decoding for Generative Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07791">http://arxiv.org/abs/2308.07791</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tobias Deußer, Lars Hillebrand, Christian Bauckhage, Rafet Sifa</li>
<li>for: 本研究旨在提出一种简单 yet effective的方法，即 Informed Named Entity Recognition Decoding (iNERD)，用于循环的 named entity recognition 任务。</li>
<li>methods: 该方法利用当前的生成模型来进行语言理解，并采用一种有知识的 decoding 方式，将信息提取 tasks 与开放式文本生成结合，从而提高性能并消除所有的幻觉。</li>
<li>results: 通过在 eight 个 named entity recognition 数据集上评估五种生成语言模型，研究发现该方法在未知实体类集合环境下表现出优异的适应能力，特别是在不知道实体类集合时，表现更加出色。<details>
<summary>Abstract</summary>
Ever-larger language models with ever-increasing capabilities are by now well-established text processing tools. Alas, information extraction tasks such as named entity recognition are still largely unaffected by this progress as they are primarily based on the previous generation of encoder-only transformer models. Here, we propose a simple yet effective approach, Informed Named Entity Recognition Decoding (iNERD), which treats named entity recognition as a generative process. It leverages the language understanding capabilities of recent generative models in a future-proof manner and employs an informed decoding scheme incorporating the restricted nature of information extraction into open-ended text generation, improving performance and eliminating any risk of hallucinations. We coarse-tune our model on a merged named entity corpus to strengthen its performance, evaluate five generative language models on eight named entity recognition datasets, and achieve remarkable results, especially in an environment with an unknown entity class set, demonstrating the adaptability of the approach.
</details>
<details>
<summary>摘要</summary>
现代语言模型不断增长，功能也不断提高。可是，信息提取任务，如名实Recognition，仍然受到这些进步的影响很少，因为它们基本上是基于上一代encoder-only transformer模型。在这里，我们提出了一种简单 yet effective的方法，即 Informed Named Entity Recognition Decoding（iNERD）。它将名实Recognition视为生成过程，利用当前的生成模型对语言理解能力，并采用了了知情 decode 策略，将开放式文本生成和信息提取相结合，提高性能，并完全消除任何幻觉的风险。我们在合并的名实Corpus上粗略调整我们的模型，使其在八个名实Recognition 数据集上表现出色，特别是在未知类型集的环境中，表现出了适应性。
</details></li>
</ul>
<hr>
<h2 id="DiffV2S-Diffusion-based-Video-to-Speech-Synthesis-with-Vision-guided-Speaker-Embedding"><a href="#DiffV2S-Diffusion-based-Video-to-Speech-Synthesis-with-Vision-guided-Speaker-Embedding" class="headerlink" title="DiffV2S: Diffusion-based Video-to-Speech Synthesis with Vision-guided Speaker Embedding"></a>DiffV2S: Diffusion-based Video-to-Speech Synthesis with Vision-guided Speaker Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07787">http://arxiv.org/abs/2308.07787</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/joannahong/diffv2s">https://github.com/joannahong/diffv2s</a></li>
<li>paper_authors: Jeongsoo Choi, Joanna Hong, Yong Man Ro</li>
<li>for: 这个研究的目的是提高视频到语音合成的精度和可理解性，使得可以从视频输入中恰当地重建出高质量的语音。</li>
<li>methods: 这个研究使用了一种新的视觉导向说话嵌入表示器，它使用了一个自我超vised预训练模型和提问调整技术来提取嵌入。此外，它还使用了一种扩散基于的视频到语音合成模型，称为DiffV2S，其 conditioned于提取的嵌入和输入视频帧中的视觉表示。</li>
<li>results: 这个研究的实验结果表明，DiffV2S可以保持输入视频帧中的音素细节，同时创造一个高度可理解的mel-spectrogram，其中每个说话者的身份都被保留。相比之下，DiffV2S的表现比之前的视频到语音合成技术更高。<details>
<summary>Abstract</summary>
Recent research has demonstrated impressive results in video-to-speech synthesis which involves reconstructing speech solely from visual input. However, previous works have struggled to accurately synthesize speech due to a lack of sufficient guidance for the model to infer the correct content with the appropriate sound. To resolve the issue, they have adopted an extra speaker embedding as a speaking style guidance from a reference auditory information. Nevertheless, it is not always possible to obtain the audio information from the corresponding video input, especially during the inference time. In this paper, we present a novel vision-guided speaker embedding extractor using a self-supervised pre-trained model and prompt tuning technique. In doing so, the rich speaker embedding information can be produced solely from input visual information, and the extra audio information is not necessary during the inference time. Using the extracted vision-guided speaker embedding representations, we further develop a diffusion-based video-to-speech synthesis model, so called DiffV2S, conditioned on those speaker embeddings and the visual representation extracted from the input video. The proposed DiffV2S not only maintains phoneme details contained in the input video frames, but also creates a highly intelligible mel-spectrogram in which the speaker identities of the multiple speakers are all preserved. Our experimental results show that DiffV2S achieves the state-of-the-art performance compared to the previous video-to-speech synthesis technique.
</details>
<details>
<summary>摘要</summary>
近期研究已经展示了视频到语音合成的出色成果，即通过视觉输入重建语音。然而，之前的研究往往因缺乏足够的指导，使模型很难准确地推理出正确的内容和合适的声音。为解决这问题，他们采用了外部的 speaker embedding 作为引导，从参考听力信息中提取出speaker embedding。然而，在推理时不一定能获取相应的音频信息，特别是在推理时。在这篇论文中，我们提出了一种新的视频引导的 speaker embedding EXTRACTOR，使用自我超vised pre-trained模型和提示调整技术。通过这种方法，我们可以从输入视频信息中提取出丰富的 speaker embedding信息，而不需要外部的音频信息。使用提取的视频引导 speaker embedding表示，我们进一步开发了一种扩散基于的视频到语音合成模型，称为DiffV2S。DiffV2S 模型通过 Conditioned on those speaker embeddings and the visual representation extracted from the input video, the proposed DiffV2S not only maintains phoneme details contained in the input video frames, but also creates a highly intelligible mel-spectrogram in which the speaker identities of the multiple speakers are all preserved. Our experimental results show that DiffV2S achieves the state-of-the-art performance compared to the previous video-to-speech synthesis technique.
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-generative-modelling-for-autonomous-robots"><a href="#Hierarchical-generative-modelling-for-autonomous-robots" class="headerlink" title="Hierarchical generative modelling for autonomous robots"></a>Hierarchical generative modelling for autonomous robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07775">http://arxiv.org/abs/2308.07775</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Yuan, Noor Sajid, Karl Friston, Zhibin Li</li>
<li>for:  investigate the fundamental aspect of motor control in autonomous robotic operations, and develop a hierarchical generative model to achieve versatile sensorimotor control.</li>
<li>methods:  use hierarchical generative modeling, multi-level planning, and numerical&#x2F;physical simulation to achieve autonomous completion of complex tasks.</li>
<li>results:  demonstrate the effectiveness of using human-inspired motor control algorithms, and show the ability of a humanoid robot to retrieve, transport, open, walk through a door, approach, and kick a football, while showing robust performance in presence of body damage and ground irregularities.Here’s the summary in Traditional Chinese:</li>
<li>for: 研究自主机械操作中的动作控制基础，并开发一个嵌入式生成模型以实现多标的感知动作控制。</li>
<li>methods: 使用嵌入式生成模型、多层规划和数据&#x2F;物理模拟来完成自主任务。</li>
<li>results: 显示人类动作控制算法的效果，并展示一个人型机器人能够自主完成复杂任务，例如抓取、运输、开启、通过门、与足球进行踢动作，并在身体损坏和地面不平的情况下保持Robust性。<details>
<summary>Abstract</summary>
Humans can produce complex whole-body motions when interacting with their surroundings, by planning, executing and combining individual limb movements. We investigated this fundamental aspect of motor control in the setting of autonomous robotic operations. We approach this problem by hierarchical generative modelling equipped with multi-level planning-for autonomous task completion-that mimics the deep temporal architecture of human motor control. Here, temporal depth refers to the nested time scales at which successive levels of a forward or generative model unfold, for example, delivering an object requires a global plan to contextualise the fast coordination of multiple local movements of limbs. This separation of temporal scales also motivates robotics and control. Specifically, to achieve versatile sensorimotor control, it is advantageous to hierarchically structure the planning and low-level motor control of individual limbs. We use numerical and physical simulation to conduct experiments and to establish the efficacy of this formulation. Using a hierarchical generative model, we show how a humanoid robot can autonomously complete a complex task that necessitates a holistic use of locomotion, manipulation, and grasping. Specifically, we demonstrate the ability of a humanoid robot that can retrieve and transport a box, open and walk through a door to reach the destination, approach and kick a football, while showing robust performance in presence of body damage and ground irregularities. Our findings demonstrated the effectiveness of using human-inspired motor control algorithms, and our method provides a viable hierarchical architecture for the autonomous completion of challenging goal-directed tasks.
</details>
<details>
<summary>摘要</summary>
人类可以生成复杂全身运动when interacting with他们的环境，通过规划、执行和组合个体肢体运动。我们在自主 робоック操作中调查了这一基本问题。我们采用层次生成模型，带有多级规划，以模仿人类motor控制的深度 temporal architecture。在这里， temporal depth 指的是成功层次模型 unfold 的不同时间尺度，例如，为了交付物品，需要一个全局规划，以Contextualize 多个快速协调的肢体运动。这种层次分离也驱动了机器人和控制。具体来说，以实现多样化的感知动作控制，是通过层次结构的规划和低级动作控制来实现的。我们通过数字和物理模拟进行实验，并证明了这种形式的有效性。使用层次生成模型，我们展示了一个人型机器人可以自主完成一个复杂任务，需要整体的运动、抓取、搬运和踢球等多种功能。Specifically，我们示出了一个人型机器人可以拾取和运输一个箱子，通过门打开和走进去到目的地，并且在踢球时表现出了Robust performance 的特点。我们的发现表明了使用人类 inspirited motor control算法的有效性，而我们的方法提供了一种可靠的层次建筑，以便自主完成具有挑战性的目标指导任务。
</details></li>
</ul>
<hr>
<h2 id="A-Graph-Encoder-Decoder-Network-for-Unsupervised-Anomaly-Detection"><a href="#A-Graph-Encoder-Decoder-Network-for-Unsupervised-Anomaly-Detection" class="headerlink" title="A Graph Encoder-Decoder Network for Unsupervised Anomaly Detection"></a>A Graph Encoder-Decoder Network for Unsupervised Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07774">http://arxiv.org/abs/2308.07774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahsa Mesgaran, A. Ben Hamza</li>
<li>for: 检测图граFC中异常节点</li>
<li>methods: 使用自适应图形编码器-解码器模型，学习异常分数函数，将节点排序根据其异常程度。编码阶段使用新型的LCPool方法，通过本地化约束的线性编码来生成团 assignment matrix，解决了传统方法中学习参数的问题，提高了效率和可解释性。解码阶段使用LCUnpool方法重construct原始图гра的结构和节点特征。</li>
<li>results: 在六个基准数据集上进行了实验评估，结果表明该方法在比较状态前的异常检测方法中表现出色，超过了现有方法。<details>
<summary>Abstract</summary>
A key component of many graph neural networks (GNNs) is the pooling operation, which seeks to reduce the size of a graph while preserving important structural information. However, most existing graph pooling strategies rely on an assignment matrix obtained by employing a GNN layer, which is characterized by trainable parameters, often leading to significant computational complexity and a lack of interpretability in the pooling process. In this paper, we propose an unsupervised graph encoder-decoder model to detect abnormal nodes from graphs by learning an anomaly scoring function to rank nodes based on their degree of abnormality. In the encoding stage, we design a novel pooling mechanism, named LCPool, which leverages locality-constrained linear coding for feature encoding to find a cluster assignment matrix by solving a least-squares optimization problem with a locality regularization term. By enforcing locality constraints during the coding process, LCPool is designed to be free from learnable parameters, capable of efficiently handling large graphs, and can effectively generate a coarser graph representation while retaining the most significant structural characteristics of the graph. In the decoding stage, we propose an unpooling operation, called LCUnpool, to reconstruct both the structure and nodal features of the original graph. We conduct empirical evaluations of our method on six benchmark datasets using several evaluation metrics, and the results demonstrate its superiority over state-of-the-art anomaly detection approaches.
</details>
<details>
<summary>摘要</summary>
graph neural networks (GNNs) 的一个重要 компонент是聚合操作，它想要将 graphs 的大小减少，保留重要的结构信息。然而，大多数现有的 graph 聚合策略依赖一个由 GNN 层所得到的对称矩阵，这个矩阵通常具有可读的参数，往往导致计算复杂和模型解释不足。在这篇文章中，我们提出了一个无supervised graph encoder-decoder模型，用于侦测 graphs 中的异常点。在编码阶段，我们设计了一个名为 LCPool 的新的聚合机制，通过本地性受限的线性编码来找到一个对称矩阵，并通过解决一个最小二乘问题来找到一个最佳的对称矩阵。由于在编码过程中强制 enforcing 本地性限制，LCPool 可以免除学习参数，可以高效地处理大型 graphs，并且可以将原始图的主要结构特征传递到更粗糙的表示中。在解码阶段，我们提出了一个名为 LCUnpool 的解码操作，用于重建原始图的结构和节点特征。我们在六个 benchmark dataset 上进行了实验评估，结果显示我们的方法在与现有的侦测方法比较之下表现出色。
</details></li>
</ul>
<hr>
<h2 id="MOLE-MOdular-Learning-FramEwork-via-Mutual-Information-Maximization"><a href="#MOLE-MOdular-Learning-FramEwork-via-Mutual-Information-Maximization" class="headerlink" title="MOLE: MOdular Learning FramEwork via Mutual Information Maximization"></a>MOLE: MOdular Learning FramEwork via Mutual Information Maximization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07772">http://arxiv.org/abs/2308.07772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianchao Li, Yulong Pei</li>
<li>for: 这个论文是为了介绍一种异步和本地学习框架，即Module Learning Framework（MOLE）。</li>
<li>methods: 这个框架将神经网络归一化为层，通过矩阵乘法来定义训练目标，并逐渐训练每个模块以达到最大化矩阵乘法的目标。</li>
<li>results: 实验表明，MOLE可以在向量-, 网格-和图形数据上进行高效的训练，并且可以解决图形数据上的节点级和图级任务。因此，MOLE已经在不同类型的数据上得到了实验证明。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
This paper is to introduce an asynchronous and local learning framework for neural networks, named Modular Learning Framework (MOLE). This framework modularizes neural networks by layers, defines the training objective via mutual information for each module, and sequentially trains each module by mutual information maximization. MOLE makes the training become local optimization with gradient-isolated across modules, and this scheme is more biologically plausible than BP. We run experiments on vector-, grid- and graph-type data. In particular, this framework is capable of solving both graph- and node-level tasks for graph-type data. Therefore, MOLE has been experimentally proven to be universally applicable to different types of data.
</details>
<details>
<summary>摘要</summary>
这篇论文旨在介绍一种异步和本地学习框架 для神经网络，名为模块学习框架（MOLE）。这个框架将神经网络归类为层，通过每个模块的互信息定义训练目标，并逐渐训练每个模块以互信息最大化。MOLE使得训练变成了本地优化，梯度在模块之间隔离，这种方式更加生物学可能性高于BP。我们在矢量-, 网格-和图形数据上进行了实验，并证明MOLE可以解决图形数据中的图级和节点级任务。因此，MOLE在不同类型的数据上都有广泛的应用前景。
</details></li>
</ul>
<hr>
<h2 id="NeFL-Nested-Federated-Learning-for-Heterogeneous-Clients"><a href="#NeFL-Nested-Federated-Learning-for-Heterogeneous-Clients" class="headerlink" title="NeFL: Nested Federated Learning for Heterogeneous Clients"></a>NeFL: Nested Federated Learning for Heterogeneous Clients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07761">http://arxiv.org/abs/2308.07761</a></li>
<li>repo_url: None</li>
<li>paper_authors: Honggu Kang, Seohyeon Cha, Jinwoo Shin, Jongmyeong Lee, Joonhyuk Kang</li>
<li>for: 这个研究旨在解决联合学习（Federated Learning，FL）训练过程中缓态或无法进行训练的客户端（即慢车）对整个训练时间的影响，以及实现训练模型的更好可扩展性。</li>
<li>methods: 本研究提出了一个称为嵌套联合学习（NeFL）的架构，它可以将模型分解为多个子模型，并使用深度和宽度的扩展来实现。NeFL还使用了解析方程（ODEs）来调整步长大小，以便在不同的客户端上进行训练。</li>
<li>results: 透过一系列实验，本研究表明NeFL可以实现训练模型的更好可扩展性，特别是在最差的子模型（例如CIFAR-10上的8.33提升）。此外，NeFL与最近的FL研究相互协调。<details>
<summary>Abstract</summary>
Federated learning (FL) is a promising approach in distributed learning keeping privacy. However, during the training pipeline of FL, slow or incapable clients (i.e., stragglers) slow down the total training time and degrade performance. System heterogeneity, including heterogeneous computing and network bandwidth, has been addressed to mitigate the impact of stragglers. Previous studies split models to tackle the issue, but with less degree-of-freedom in terms of model architecture. We propose nested federated learning (NeFL), a generalized framework that efficiently divides a model into submodels using both depthwise and widthwise scaling. NeFL is implemented by interpreting models as solving ordinary differential equations (ODEs) with adaptive step sizes. To address the inconsistency that arises when training multiple submodels with different architecture, we decouple a few parameters. NeFL enables resource-constrained clients to effectively join the FL pipeline and the model to be trained with a larger amount of data. Through a series of experiments, we demonstrate that NeFL leads to significant gains, especially for the worst-case submodel (e.g., 8.33 improvement on CIFAR-10). Furthermore, we demonstrate NeFL aligns with recent studies in FL.
</details>
<details>
<summary>摘要</summary>
Federated learning (FL) 是一种有前途的方法，可以保持隐私性在分布式学习中。然而，在 FL 训练管线中，慢速或无法进行训练的客户端（即废物）会导致总训练时间增加和性能下降。系统多样性，包括不同的计算和网络带宽，已经被解决以减少废物的影响。先前的研究把模型分成了两部分来解决问题，但是这些方法具有较少的度量自由度，对于模型架构而言。我们提出了嵌套 federated learning（NeFL），一个通用的框架，可以快速地将模型分成子模型，使用深度和宽度的扩展。NeFL 通过将模型视为解决 ordinary differential equations（ODEs）的解，并使用适应步长来实现。为了解决不同架构下训练多个子模型时出现的不一致，我们将一些参数分离。NeFL 允许资源有限的客户端能够有效地加入 FL 管线，并让模型在更多数据上进行训练。通过一系列实验，我们展示了 NeFL 对 CIFAR-10 等数据集的进步，特别是最差的子模型（例如，8.33 倍进步）。此外，我们还证明 NeFL 与最近的 FL 研究相关。
</details></li>
</ul>
<hr>
<h2 id="Forward-Backward-Reasoning-in-Large-Language-Models-for-Verification"><a href="#Forward-Backward-Reasoning-in-Large-Language-Models-for-Verification" class="headerlink" title="Forward-Backward Reasoning in Large Language Models for Verification"></a>Forward-Backward Reasoning in Large Language Models for Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07758">http://arxiv.org/abs/2308.07758</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weisen Jiang, Han Shi, Longhui Yu, Zhengying Liu, Yu Zhang, Zhenguo Li, James T. Kwok</li>
<li>for: 提高理解任务中的推理能力</li>
<li>methods: 使用反向推理和前向推理的组合方法</li>
<li>results: 实验结果表明，FOBAR方法在多个数据集和三种LLM中表现出状元水平的推理能力<details>
<summary>Abstract</summary>
Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \textit{a simple template}, i.e., "\textit{\textbf{If we know the answer of the above question is \{a candidate answer\}, what is the value of unknown variable ${\bf x}$?}" Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three LLMs. Experimental results demonstrate that FOBAR achieves state-of-the-art performance on various reasoning benchmarks.
</details>
<details>
<summary>摘要</summary>
链式思维（CoT）提示法在多种理解任务中表现出色。自康凝 \citep{wang2023selfconsistency}提出了采样多个理解链，以便通过不同的答案而产生多个可能性。在这篇论文中，我们提出了一种使用反向思维的新方法，用于验证候选答案。我们将问题中的一个token用${\bf x}$进行遮盖，然后询问LLM predict该遮盖的token，当提供一个简单的模板，即 "\textit{\textbf{如果我们知道上面的问题的答案是 \{一个候选答案\}, то值Unknown变量${\bf x}$是什么？}"。 intuitionally，LLM可以成功预测遮盖的token，如果提供的候选答案是正确的。我们还提出了FOBAR，用于将前向和反向思维相结合，以估算候选答案的概率。我们在六个数据集和三个LLM上进行了广泛的实验，实验结果表明，FOBAR在多种理解 bencmarks 上达到了领先的性能。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Sparsity-in-Automotive-Radar-Object-Detection-Networks"><a href="#Exploiting-Sparsity-in-Automotive-Radar-Object-Detection-Networks" class="headerlink" title="Exploiting Sparsity in Automotive Radar Object Detection Networks"></a>Exploiting Sparsity in Automotive Radar Object Detection Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07748">http://arxiv.org/abs/2308.07748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marius Lippke, Maurice Quach, Sascha Braun, Daniel Köhler, Michael Ulrich, Bastian Bischoff, Wei Yap Tan</li>
<li>for: 这 paper 的目的是提出一种基于 sparse convolutional neural network 的对象检测方法，用于解决自动驾驶系统中的环境感知问题。</li>
<li>methods: 该 paper 使用了 grid-based detection 和 sparse backbone 架构，并提出了 sparse kernel point pillars (SKPP) 和 dual voxel point convolutions (DVPC) 等技术来解决 радиар特有的挑战。</li>
<li>results: 该 paper 在 nuScenes 数据集上进行了评测，并证明了 SKPP-DPVCN 架构可以比基线和前一个状态的对象检测方法提高 Car AP4.0 的性能，并降低了平均缩放错误 (ASE) 值。<details>
<summary>Abstract</summary>
Having precise perception of the environment is crucial for ensuring the secure and reliable functioning of autonomous driving systems. Radar object detection networks are one fundamental part of such systems. CNN-based object detectors showed good performance in this context, but they require large compute resources. This paper investigates sparse convolutional object detection networks, which combine powerful grid-based detection with low compute resources. We investigate radar specific challenges and propose sparse kernel point pillars (SKPP) and dual voxel point convolutions (DVPC) as remedies for the grid rendering and sparse backbone architectures. We evaluate our SKPP-DPVCN architecture on nuScenes, which outperforms the baseline by 5.89% and the previous state of the art by 4.19% in Car AP4.0. Moreover, SKPP-DPVCN reduces the average scale error (ASE) by 21.41% over the baseline.
</details>
<details>
<summary>摘要</summary>
“精准感知环境是自动驾驶系统的关键，以确保其安全和可靠运行。雷达对象检测网络是这种系统的基本组件之一。使用CNN的对象检测器显示了良好的性能，但它们需要大量的计算资源。这篇论文研究了稀疏 convolutional 对象检测网络，它们将强大的格子基础与低计算资源相结合。我们研究了雷达特有挑战，并提出了 sparse kernel point pillars（SKPP）和 dual voxel point convolutions（DVPC）来解决grid rendering和稀疏脊梁架构的问题。我们评估了我们的 SKPP-DPVCN 架构在 nuScenes 上，其与基准值相比提高了4.19%，并且与前一个状态的艺术提高了5.89%。此外，SKPP-DPVCN 还下降了平均扩散误差（ASE）的21.41%。”
</details></li>
</ul>
<hr>
<h2 id="Real-Robot-Challenge-2022-Learning-Dexterous-Manipulation-from-Offline-Data-in-the-Real-World"><a href="#Real-Robot-Challenge-2022-Learning-Dexterous-Manipulation-from-Offline-Data-in-the-Real-World" class="headerlink" title="Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World"></a>Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07741">http://arxiv.org/abs/2308.07741</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nico Gürtler, Felix Widmaier, Cansu Sancaktar, Sebastian Blaes, Pavel Kolev, Stefan Bauer, Manuel Wüthrich, Markus Wulfmeier, Martin Riedmiller, Arthur Allshire, Qiang Wang, Robert McCarthy, Hangyeol Kim, Jongchan Baek Pohang, Wookyong Kwon, Shanliang Qian, Yasunori Toshimitsu, Mike Yan Michelis, Amirhossein Kazemipour, Arman Raayatsanati, Hehui Zheng, Barnabasa Gavin Cangan, Bernhard Schölkopf, Georg Martius</li>
<li>for: 本研究的目的是bridge reinforcement learning（RL）和机器人共同体，让参与者通过实际操作real robot来验证RL算法的性能。</li>
<li>methods: 本研究使用了现有的real robot dataset，并提供了丰富的软件文档和初始化阶段，使参与者可以轻松地在real robot上进行学习和评估。</li>
<li>results: 研究发现，winning teams使用的方法可以在real robot上实现高效的dexterous manipulation任务，并且比预期的state-of-the-art offline RL算法更高效。<details>
<summary>Abstract</summary>
Experimentation on real robots is demanding in terms of time and costs. For this reason, a large part of the reinforcement learning (RL) community uses simulators to develop and benchmark algorithms. However, insights gained in simulation do not necessarily translate to real robots, in particular for tasks involving complex interactions with the environment. The Real Robot Challenge 2022 therefore served as a bridge between the RL and robotics communities by allowing participants to experiment remotely with a real robot - as easily as in simulation.   In the last years, offline reinforcement learning has matured into a promising paradigm for learning from pre-collected datasets, alleviating the reliance on expensive online interactions. We therefore asked the participants to learn two dexterous manipulation tasks involving pushing, grasping, and in-hand orientation from provided real-robot datasets. An extensive software documentation and an initial stage based on a simulation of the real set-up made the competition particularly accessible. By giving each team plenty of access budget to evaluate their offline-learned policies on a cluster of seven identical real TriFinger platforms, we organized an exciting competition for machine learners and roboticists alike.   In this work we state the rules of the competition, present the methods used by the winning teams and compare their results with a benchmark of state-of-the-art offline RL algorithms on the challenge datasets.
</details>
<details>
<summary>摘要</summary>
实验在真正机器人上具有时间和成本的限制，因此许多学习强化（RL）社区使用模拟器来开发和比较算法。然而，在实际环境中的交互性较复杂时，在模拟器上获得的 Insight 可能不准确。为了 bridging 这两个社区，我们在2022年的真机器人挑战中让参与者通过远程控制真机器人来进行实验，与在模拟器上进行实验一样简单。在过去几年中，离线学习强化学习（offline RL）已经成熟为一种有前途的学习方法，可以避免在临时便捷的在线交互中花费高昂的成本。因此，我们要求参与者通过学习提供的真机器人数据集来完成两项灵活的机械操作任务，包括推动、抓取和手中 orienting。为了使参与者更加方便地参与到竞赛中，我们提供了广泛的软件文档和一个基于真实设置的初始阶段。为了让每个团队有足够的访问预算来评估他们在一群七个相同的真机器人平台上的离线学习策略，我们组织了一场吸引了机器人学家和学习机器人之间的精彩竞赛。在这篇文章中，我们介绍了竞赛规则，表明赢家们使用的方法，并与当前的离线RL算法在挑战数据集上的比较。
</details></li>
</ul>
<hr>
<h2 id="Domain-Aware-Fine-Tuning-Enhancing-Neural-Network-Adaptability"><a href="#Domain-Aware-Fine-Tuning-Enhancing-Neural-Network-Adaptability" class="headerlink" title="Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability"></a>Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07728">http://arxiv.org/abs/2308.07728</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seokhyeon Ha, Sunbeom Jung, Jungwoo Lee</li>
<li>For: 本研究旨在提出一种新的方法，以便在不同目标领域进行精度的适应和性能优化。* Methods: 本研究使用了域名映射和精度评估来缓解特征扭曲问题，并通过Linear Probing和精度调整来优化头层。* Results: 对比基eline方法，本研究的方法在域外数据上显示出较高的性能，并且可以减少特征扭曲。<details>
<summary>Abstract</summary>
Fine-tuning pre-trained neural network models has become a widely adopted approach across various domains. However, it can lead to the distortion of pre-trained feature extractors that already possess strong generalization capabilities. Mitigating feature distortion during adaptation to new target domains is crucial. Recent studies have shown promising results in handling feature distortion by aligning the head layer on in-distribution datasets before performing fine-tuning. Nonetheless, a significant limitation arises from the treatment of batch normalization layers during fine-tuning, leading to suboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning (DAFT), a novel approach that incorporates batch normalization conversion and the integration of linear probing and fine-tuning. Our batch normalization conversion method effectively mitigates feature distortion by reducing modifications to the neural network during fine-tuning. Additionally, we introduce the integration of linear probing and fine-tuning to optimize the head layer with gradual adaptation of the feature extractor. By leveraging batch normalization layers and integrating linear probing and fine-tuning, our DAFT significantly mitigates feature distortion and achieves improved model performance on both in-distribution and out-of-distribution datasets. Extensive experiments demonstrate that our method outperforms other baseline methods, demonstrating its effectiveness in not only improving performance but also mitigating feature distortion.
</details>
<details>
<summary>摘要</summary>
“已成为各领域的普遍采用方法，微型网络组件的精致调整已成为一个广泛应用的方法。然而，这可能会导致原有具备强化泛化能力的预训网络组件的扭曲。缓和预训网络组件的扭曲是非常重要的。 recent studies have shown promising results in handling feature distortion by aligning the head layer on in-distribution datasets before performing fine-tuning. However, a significant limitation arises from the treatment of batch normalization layers during fine-tuning, leading to suboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning (DAFT), a novel approach that incorporates batch normalization conversion and the integration of linear probing and fine-tuning. Our batch normalization conversion method effectively mitigates feature distortion by reducing modifications to the neural network during fine-tuning. Additionally, we introduce the integration of linear probing and fine-tuning to optimize the head layer with gradual adaptation of the feature extractor. By leveraging batch normalization layers and integrating linear probing and fine-tuning, our DAFT significantly mitigates feature distortion and achieves improved model performance on both in-distribution and out-of-distribution datasets. Extensive experiments demonstrate that our method outperforms other baseline methods, demonstrating its effectiveness in not only improving performance but also mitigating feature distortion.”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know and I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Fast-Machine-Unlearning-Without-Retraining-Through-Selective-Synaptic-Dampening"><a href="#Fast-Machine-Unlearning-Without-Retraining-Through-Selective-Synaptic-Dampening" class="headerlink" title="Fast Machine Unlearning Without Retraining Through Selective Synaptic Dampening"></a>Fast Machine Unlearning Without Retraining Through Selective Synaptic Dampening</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07707">http://arxiv.org/abs/2308.07707</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/if-loops/selective-synaptic-dampening">https://github.com/if-loops/selective-synaptic-dampening</a></li>
<li>paper_authors: Jack Foster, Stefan Schoepf, Alexandra Brintrup</li>
<li>for: 本研究旨在解决机器学习模型忘记Specific information的挑战，以遵守数据隐私法规和 removing harmful, manipulated, or outdated information。</li>
<li>methods: 本研究提出了一种名为Selective Synaptic Dampening（SSD）的两步，Post hoc，无需重新训练的方法，它快速、高效，不需要长期存储训练数据。</li>
<li>results: 对比 existed unlearning 方法，SSD 的性能与重新训练方法相当，这表明了无需重新训练的后置式忘记方法的可行性。<details>
<summary>Abstract</summary>
Machine unlearning, the ability for a machine learning model to forget, is becoming increasingly important to comply with data privacy regulations, as well as to remove harmful, manipulated, or outdated information. The key challenge lies in forgetting specific information while protecting model performance on the remaining data. While current state-of-the-art methods perform well, they typically require some level of retraining over the retained data, in order to protect or restore model performance. This adds computational overhead and mandates that the training data remain available and accessible, which may not be feasible. In contrast, other methods employ a retrain-free paradigm, however, these approaches are prohibitively computationally expensive and do not perform on par with their retrain-based counterparts. We present Selective Synaptic Dampening (SSD), a novel two-step, post hoc, retrain-free approach to machine unlearning which is fast, performant, and does not require long-term storage of the training data. First, SSD uses the Fisher information matrix of the training and forgetting data to select parameters that are disproportionately important to the forget set. Second, SSD induces forgetting by dampening these parameters proportional to their relative importance to the forget set with respect to the wider training data. We evaluate our method against several existing unlearning methods in a range of experiments using ResNet18 and Vision Transformer. Results show that the performance of SSD is competitive with retrain-based post hoc methods, demonstrating the viability of retrain-free post hoc unlearning approaches.
</details>
<details>
<summary>摘要</summary>
机器学习模型的忘记能力，也就是机器学习模型的“忘记”，在符合数据隐私法规以及移除有害、操纵或过时信息方面变得越来越重要。然而，现有的状态 искусственный智能技术通常需要一定的重新训练，以保护或恢复模型在保留的数据上的性能。这会增加计算开销，并且需要训练数据保持可用和可访问，这可能不是可行的。相比之下，其他方法采用一种不需要重新训练的方法，但这些方法的计算成本过高，并且性能不如重新训练的方法。我们提出了一种新的两步、Post Hoc、无需重新训练的机器学习忘记方法：选择性神经元减弱（SSD）。首先，SSD使用训练和忘记数据的 Fisher 信息矩阵来选择对忘记集数据的重要参数。然后，SSD 通过对这些参数进行减弱，使其与忘记集数据相对更重要的参数相比，来实现忘记。我们在使用 ResNet18 和 Vision Transformer 进行了一系列实验，结果表明 SSD 的性能与重新训练后的Post Hoc方法相当竞争，这说明了无需重新训练的忘记方法的可行性。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Transfer-Learning-in-Medical-Image-Segmentation-using-Vision-Language-Models"><a href="#Exploring-Transfer-Learning-in-Medical-Image-Segmentation-using-Vision-Language-Models" class="headerlink" title="Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models"></a>Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07706">http://arxiv.org/abs/2308.07706</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kanchan Poudel, Manish Dhakal, Prasiddha Bhandari, Rabin Adhikari, Safal Thapaliya, Bishesh Khanal</li>
<li>for: 本研究旨在提高医疗领域图像分割 task 的效果，通过文本引导来增强视觉特征。</li>
<li>methods: 本研究使用多Modal vision-language模型，包括图像描述和图像特征，以捕捉图像描述中的semantic信息，提高医疗领域图像分割 task 的效果。</li>
<li>results: 研究发现，existings vision language模型在多个 dataset 上的传输性不高，需要进行手动调整或 fine-tuning 以适应医疗领域。而通过生成不同的图像描述来训练模型，可以提高模型的性能。<details>
<summary>Abstract</summary>
Medical Image Segmentation is crucial in various clinical applications within the medical domain. While state-of-the-art segmentation models have proven effective, integrating textual guidance to enhance visual features for this task remains an area with limited progress. Existing segmentation models that utilize textual guidance are primarily trained on open-domain images, raising concerns about their direct applicability in the medical domain without manual intervention or fine-tuning.   To address these challenges, we propose using multimodal vision-language models for capturing semantic information from image descriptions and images, enabling the segmentation of diverse medical images. This study comprehensively evaluates existing vision language models across multiple datasets to assess their transferability from the open domain to the medical field. Furthermore, we introduce variations of image descriptions for previously unseen images in the dataset, revealing notable variations in model performance based on the generated prompts.   Our findings highlight the distribution shift between the open-domain images and the medical domain and show that the segmentation models trained on open-domain images are not directly transferrable to the medical field. But their performance can be increased by finetuning them in the medical datasets. We report the zero-shot and finetuned segmentation performance of 4 Vision Language Models (VLMs) on 11 medical datasets using 9 types of prompts derived from 14 attributes.
</details>
<details>
<summary>摘要</summary>
医疗图像分割是医疗领域内的重要应用之一。虽然现有的分割模型已经证明有效，但是将文本指导integrated到图像特征中以提高分割性能仍然是一个有限的领域。现有的分割模型主要是在开放领域图像上训练，这引发了对其直接适用性在医疗领域的担忧。为了解决这些挑战，我们提议使用多Modal vision-language模型来捕捉图像描述和图像中的semantic信息，以便分割多种医疗图像。本研究对多个数据集进行了广泛的评估，以评估现有的视力语言模型在医疗领域的转移性。此外，我们还引入了未经见过的图像描述，并证明了基于生成的提示的模型性能的很大变化。我们的发现表明了开放领域图像和医疗领域之间的分布差异，并证明了训练在开放领域图像上的分割模型不能直接应用于医疗领域。但是，通过finetuning，可以提高这些模型在医疗数据集上的性能。我们对4种视力语言模型在11个医疗数据集上进行了零容量和finetuning的分割性能测试，使用9种基于14个特征的提示。
</details></li>
</ul>
<hr>
<h2 id="Parametric-entropy-based-Cluster-Centriod-Initialization-for-k-means-clustering-of-various-Image-datasets"><a href="#Parametric-entropy-based-Cluster-Centriod-Initialization-for-k-means-clustering-of-various-Image-datasets" class="headerlink" title="Parametric entropy based Cluster Centriod Initialization for k-means clustering of various Image datasets"></a>Parametric entropy based Cluster Centriod Initialization for k-means clustering of various Image datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07705">http://arxiv.org/abs/2308.07705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Faheem Hussayn, Shahid M Shah</li>
<li>for: 这篇论文的目的是提出一种基于参数 entropy 的 k-means 初始化方法，以提高 k-means 算法在图像数据上的表现。</li>
<li>methods: 该论文使用了多种参数 entropy 来初始化 k-means 算法的中心点，并对不同的图像 dataset 进行了测试。</li>
<li>results: 研究发现，不同的 dataset 使用不同的参数 entropy 可以提供更好的结果，而且提议的方法可以提高 k-means 算法在图像数据上的表现。 例如，在 Satellite、Toys、Fruits、Cars 等 dataset 上，使用 Taneja entropy、Kapur entropy、Aczel Daroczy entropy 和 Sharma Mittal entropy 等参数 entropy 可以提供更好的结果。<details>
<summary>Abstract</summary>
One of the most employed yet simple algorithm for cluster analysis is the k-means algorithm. k-means has successfully witnessed its use in artificial intelligence, market segmentation, fraud detection, data mining, psychology, etc., only to name a few. The k-means algorithm, however, does not always yield the best quality results. Its performance heavily depends upon the number of clusters supplied and the proper initialization of the cluster centroids or seeds. In this paper, we conduct an analysis of the performance of k-means on image data by employing parametric entropies in an entropy based centroid initialization method and propose the best fitting entropy measures for general image datasets. We use several entropies like Taneja entropy, Kapur entropy, Aczel Daroczy entropy, Sharma Mittal entropy. We observe that for different datasets, different entropies provide better results than the conventional methods. We have applied our proposed algorithm on these datasets: Satellite, Toys, Fruits, Cars, Brain MRI, Covid X-Ray.
</details>
<details>
<summary>摘要</summary>
一种非常常用但简单的聚类分析算法是k-means算法。k-means算法在人工智能、市场 segmentation、诈骗探测、数据挖掘、心理学等领域都有广泛的应用，只是名些。然而，k-means算法并不总是能够提供最佳的结果。其性能很大程度上取决于提供的聚类数量和聚类中心点或种子的初始化。在这篇论文中，我们通过使用参数 entropy 来初始化聚类中心点，并提出了适用于普通图像数据集的最佳 entropy 度量。我们使用了多种 entropy，如Taneja entropy、Kapur entropy、Aczel Daroczy entropy和Sharma Mittal entropy。我们发现，不同的数据集，不同的 entropy 度量会提供更好的结果。我们在这些数据集上应用了我们的提议的算法：卫星、玩具、水果、汽车、脑Magnetic Resonance Imaging（MRI）、Covid X-Ray。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Network-Initialization-for-Medical-AI-Models-Using-Large-Scale-Unlabeled-Natural-Images"><a href="#Enhancing-Network-Initialization-for-Medical-AI-Models-Using-Large-Scale-Unlabeled-Natural-Images" class="headerlink" title="Enhancing Network Initialization for Medical AI Models Using Large-Scale, Unlabeled Natural Images"></a>Enhancing Network Initialization for Medical AI Models Using Large-Scale, Unlabeled Natural Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07688">http://arxiv.org/abs/2308.07688</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soroosh Tayebi Arasteh, Leo Misera, Jakob Nikolas Kather, Daniel Truhn, Sven Nebelung</li>
<li>for: 这个研究的目的是探索可以使用非医学影像进行自主学习预训（SSL），以提高医学影像分析中的人工智能（AI）精度。</li>
<li>methods: 我们使用了一个视觉转化器，并将其初始化为（i）SSL预训自然影像（DINOv2）、（ii）SL预训自然影像（ImageNet dataset）和（iii）SL预训颈部X线成像（MIMIC-CXR dataset）。</li>
<li>results: 我们在6个大型全球颈部X线成像数据集上进行了过80万张颈部X线成像的测试，并识别了20多种不同的医学影像找到结果。我们的SSL预训策略不仅在所有数据集上比ImageNet预训（P&lt;0.001）表现更好，甚至在某些情况下还超过了SL在MIMIC-CXR数据集上的表现。<details>
<summary>Abstract</summary>
Pre-training datasets, like ImageNet, have become the gold standard in medical image analysis. However, the emergence of self-supervised learning (SSL), which leverages unlabeled data to learn robust features, presents an opportunity to bypass the intensive labeling process. In this study, we explored if SSL for pre-training on non-medical images can be applied to chest radiographs and how it compares to supervised pre-training on non-medical images and on medical images. We utilized a vision transformer and initialized its weights based on (i) SSL pre-training on natural images (DINOv2), (ii) SL pre-training on natural images (ImageNet dataset), and (iii) SL pre-training on chest radiographs from the MIMIC-CXR database. We tested our approach on over 800,000 chest radiographs from six large global datasets, diagnosing more than 20 different imaging findings. Our SSL pre-training on curated images not only outperformed ImageNet-based pre-training (P<0.001 for all datasets) but, in certain cases, also exceeded SL on the MIMIC-CXR dataset. Our findings suggest that selecting the right pre-training strategy, especially with SSL, can be pivotal for improving artificial intelligence (AI)'s diagnostic accuracy in medical imaging. By demonstrating the promise of SSL in chest radiograph analysis, we underline a transformative shift towards more efficient and accurate AI models in medical imaging.
</details>
<details>
<summary>摘要</summary>
预训 datasets，如 ImageNet，已成为医学影像分析的标准。然而，自动学习（SSL）技术的出现，可以使用无标签数据学习强大的特征，可能以替代复杂的标签过程。在这项研究中，我们研究了是否可以将SSL预训短图用于骨胸影像，并与其他两种预训方法进行比较。我们使用了一种视觉转换器，并将其参数初始化为（i）SSL预训自然图像（DINOv2），（ii）SL预训自然图像（ImageNet dataset），和（iii）SL预训骨胸影像（MIMIC-CXR数据库）。我们对超过800,000个骨胸影像进行测试，识别了 более20种不同的医学影像发现。我们的SSL预训策略不仅在所有数据集上超过ImageNet预训策略（P<0.001），而且在某些情况下， même exceeded SL在MIMIC-CXR数据库上。我们的发现表明，选择合适的预训策略，特别是使用SSL，可以对医学影像识别精度进行改进。通过证明SSL在骨胸影像分析中的推荐，我们强调了医学影像识别模型的更有效和精度的转型。
</details></li>
</ul>
<hr>
<h2 id="DiffGuard-Semantic-Mismatch-Guided-Out-of-Distribution-Detection-using-Pre-trained-Diffusion-Models"><a href="#DiffGuard-Semantic-Mismatch-Guided-Out-of-Distribution-Detection-using-Pre-trained-Diffusion-Models" class="headerlink" title="DiffGuard: Semantic Mismatch-Guided Out-of-Distribution Detection using Pre-trained Diffusion Models"></a>DiffGuard: Semantic Mismatch-Guided Out-of-Distribution Detection using Pre-trained Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07687">http://arxiv.org/abs/2308.07687</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cure-lab/diffguard">https://github.com/cure-lab/diffguard</a></li>
<li>paper_authors: Ruiyuan Gao, Chenchen Zhao, Lanqing Hong, Qiang Xu</li>
<li>for: 本研究的目的是提出一种基于扩展模型的Semantic Out-of-Distribution（OOD）检测方法，以提高图像分类器的OOD检测性能。</li>
<li>methods: 该方法使用了 conditional Generative Adversarial Network（cGAN）来增大图像空间中的semantic mismatch，并且使用pre-trained diffusion models来实现Semantic mismatch-guided OOD detection。</li>
<li>results: 实验结果表明，DiffGuard可以在Cifar-10和ImageNet上达到州-of-the-art的OOD检测性能，并且可以与现有的OOD检测技术相结合以获得更高的检测性能。<details>
<summary>Abstract</summary>
Given a classifier, the inherent property of semantic Out-of-Distribution (OOD) samples is that their contents differ from all legal classes in terms of semantics, namely semantic mismatch. There is a recent work that directly applies it to OOD detection, which employs a conditional Generative Adversarial Network (cGAN) to enlarge semantic mismatch in the image space. While achieving remarkable OOD detection performance on small datasets, it is not applicable to ImageNet-scale datasets due to the difficulty in training cGANs with both input images and labels as conditions. As diffusion models are much easier to train and amenable to various conditions compared to cGANs, in this work, we propose to directly use pre-trained diffusion models for semantic mismatch-guided OOD detection, named DiffGuard. Specifically, given an OOD input image and the predicted label from the classifier, we try to enlarge the semantic difference between the reconstructed OOD image under these conditions and the original input image. We also present several test-time techniques to further strengthen such differences. Experimental results show that DiffGuard is effective on both Cifar-10 and hard cases of the large-scale ImageNet, and it can be easily combined with existing OOD detection techniques to achieve state-of-the-art OOD detection results.
</details>
<details>
<summary>摘要</summary>
（简化中文）给定一个分类器，则它的Out-of-Distribution（OOD）样本的内在特性是与所有法定类型的内容不同，即semantic mismatch。有一项最近的工作直接应用它到OOD检测中，使用conditional Generative Adversarial Network（cGAN）来扩大图像空间中的semantic mismatch。尽管在小 dataset上达到了惊人的OOD检测性能，但是在ImageNet scale dataset上不可能因为cGAN的训练是不可能的。因为diffusion模型比cGAN更容易训练，在这项工作中，我们直接使用预训练的diffusion模型来实现semantic mismatch-guided OOD检测，名为DiffGuard。具体来说，给定一个OOD输入图像和分类器预测的标签，我们尝试通过在这些条件下重建OOD图像，并与原始输入图像进行比较，以扩大semantic difference。我们还提供了多种测试时技术来进一步强化这种差异。实验结果表明，DiffGuard是效果好的，在Cifar-10和ImageNet中的困难情况下都能够达到state-of-the-art的OOD检测结果。
</details></li>
</ul>
<hr>
<h2 id="Portfolio-Selection-via-Topological-Data-Analysis"><a href="#Portfolio-Selection-via-Topological-Data-Analysis" class="headerlink" title="Portfolio Selection via Topological Data Analysis"></a>Portfolio Selection via Topological Data Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07944">http://arxiv.org/abs/2308.07944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Petr Sokerin, Kristian Kuznetsov, Elizaveta Makhneva, Alexey Zaytsev</li>
<li>for: 投资决策中的资产组合管理是一项重要的任务，但传统方法往往无法实现合理的性能。</li>
<li>methods: 本文提出了一种两阶段的投资资产组合建立方法，首先生成时间序列表示，然后进行划分。该方法利用了 topological data analysis（TDA）特征来生成表示，从而揭示时间序列数据中的Topological结构。</li>
<li>results: 实验结果显示，我们提出的方法在不同时间帧下具有superior性能，与其他方法相比，这种性能的稳定性和可靠性得到了证明。这些结果表明TDA可以作为一种强大的工具来选择资产组合。<details>
<summary>Abstract</summary>
Portfolio management is an essential part of investment decision-making. However, traditional methods often fail to deliver reasonable performance. This problem stems from the inability of these methods to account for the unique characteristics of multivariate time series data from stock markets. We present a two-stage method for constructing an investment portfolio of common stocks. The method involves the generation of time series representations followed by their subsequent clustering. Our approach utilizes features based on Topological Data Analysis (TDA) for the generation of representations, allowing us to elucidate the topological structure within the data. Experimental results show that our proposed system outperforms other methods. This superior performance is consistent over different time frames, suggesting the viability of TDA as a powerful tool for portfolio selection.
</details>
<details>
<summary>摘要</summary>
资产管理是投资决策的重要组成部分，但传统方法通常无法提供合理的性能。这个问题源于这些方法无法考虑股票市场多元时间序列数据的特殊特征。我们提出了一种两阶段方法，用于建立公司股票投资组合。该方法包括生成时间序列表示，然后进行归一化。我们的方法利用基于拓扑数据分析（TDA）的特征来生成表示，从而揭示时间序列数据中的拓扑结构。实验结果显示，我们的提议系统在不同时间框架下具有优秀的性能，这表明TDA可以成为资产选择中的强大工具。
</details></li>
</ul>
<hr>
<h2 id="Gradient-Based-Post-Training-Quantization-Challenging-the-Status-Quo"><a href="#Gradient-Based-Post-Training-Quantization-Challenging-the-Status-Quo" class="headerlink" title="Gradient-Based Post-Training Quantization: Challenging the Status Quo"></a>Gradient-Based Post-Training Quantization: Challenging the Status Quo</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07662">http://arxiv.org/abs/2308.07662</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edouard Yvinec, Arnaud Dapogny, Kevin Bailly</li>
<li>for: 这篇论文的目的是提出一种新的量化方法，以提高量化深度神经网络的效率和可扩展性。</li>
<li>methods: 这篇论文使用了Gradient-based post-training quantization（GPTQ）方法，并且挑战了常用的GPTQ方法设计。具体来说，这篇论文提出了一些最佳实践方法，例如调整参数选择、增强特征变数、选择参考集等，以提高量化的效率和可扩展性。</li>
<li>results: 这篇论文的实验结果显示，这些最佳实践方法可以实现 significiant 的性能改进（例如，在ViT模型上，使用4位量化可以提高6.819点的表现），这些结果显示了这篇论文的量化方法的可行性和有效性。<details>
<summary>Abstract</summary>
Quantization has become a crucial step for the efficient deployment of deep neural networks, where floating point operations are converted to simpler fixed point operations. In its most naive form, it simply consists in a combination of scaling and rounding transformations, leading to either a limited compression rate or a significant accuracy drop. Recently, Gradient-based post-training quantization (GPTQ) methods appears to be constitute a suitable trade-off between such simple methods and more powerful, yet expensive Quantization-Aware Training (QAT) approaches, particularly when attempting to quantize LLMs, where scalability of the quantization process is of paramount importance. GPTQ essentially consists in learning the rounding operation using a small calibration set. In this work, we challenge common choices in GPTQ methods. In particular, we show that the process is, to a certain extent, robust to a number of variables (weight selection, feature augmentation, choice of calibration set). More importantly, we derive a number of best practices for designing more efficient and scalable GPTQ methods, regarding the problem formulation (loss, degrees of freedom, use of non-uniform quantization schemes) or optimization process (choice of variable and optimizer). Lastly, we propose a novel importance-based mixed-precision technique. Those guidelines lead to significant performance improvements on all the tested state-of-the-art GPTQ methods and networks (e.g. +6.819 points on ViT for 4-bit quantization), paving the way for the design of scalable, yet effective quantization methods.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:量化已成为深度神经网络的高效部署的关键步骤，将浮点操作转换为更简单的固定点操作。在最简单的形式下，它只是通过缩放和圆拟操作的组合来实现压缩率的受限或减少准确率。在最近，Gradient-based post-training quantization（GPTQ）方法变得更加重要，它们在尝试量化LLMs时， scalability of the quantization process是关键。GPTQ主要是通过学习圆拟操作来实现，使用一个小量化集。在这项工作中，我们挑战了GPTQ方法的常见选择。具体来说，我们发现该过程在一些变量（weight选择、特征增强、选择量化集）的影响下具有一定的抗预测性。此外，我们还提出了一些优化GPTQ方法的最佳实践，包括问题表示（损失、自由度、非均匀量化方案）和优化过程（变量和优化器选择）。最后，我们提出了一种重要性基于混合精度技术。这些指南导致了所有测试的现有GPTQ方法和网络（例如，+6.819点在ViT中 для 4比特量化）获得了显著性能提高，开启了可扩展、有效的量化方法的设计。
</details></li>
</ul>
<hr>
<h2 id="Attention-Is-Not-All-You-Need-Anymore"><a href="#Attention-Is-Not-All-You-Need-Anymore" class="headerlink" title="Attention Is Not All You Need Anymore"></a>Attention Is Not All You Need Anymore</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07661">http://arxiv.org/abs/2308.07661</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rprokap/pset-9">https://github.com/rprokap/pset-9</a></li>
<li>paper_authors: Zhe Chen</li>
<li>for: 提高 transformer 性能</li>
<li>methods: 提出一种drop-in replacement self-attention mechanism，称为Extractor</li>
<li>results: 实验结果表明，将 self-attention mechanism  replaced with Extractor 可以提高 transformer 性能，并且可以更快than self-attention mechanism。<details>
<summary>Abstract</summary>
In recent years, the popular Transformer architecture has achieved great success in many application areas, including natural language processing and computer vision. Many existing works aim to reduce the computational and memory complexity of the self-attention mechanism in the Transformer by trading off performance. However, performance is key for the continuing success of the Transformer. In this paper, a drop-in replacement for the self-attention mechanism in the Transformer, called the Extractor, is proposed. Experimental results show that replacing the self-attention mechanism with the Extractor improves the performance of the Transformer. Furthermore, the proposed Extractor has the potential to run faster than the self-attention since it has a much shorter critical path of computation. Additionally, the sequence prediction problem in the context of text generation is formulated using variable-length discrete-time Markov chains, and the Transformer is reviewed based on our understanding.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="From-Commit-Message-Generation-to-History-Aware-Commit-Message-Completion"><a href="#From-Commit-Message-Generation-to-History-Aware-Commit-Message-Completion" class="headerlink" title="From Commit Message Generation to History-Aware Commit Message Completion"></a>From Commit Message Generation to History-Aware Commit Message Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07655">http://arxiv.org/abs/2308.07655</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jetbrains-research/commit_message_generation">https://github.com/jetbrains-research/commit_message_generation</a></li>
<li>paper_authors: Aleksandra Eliseeva, Yaroslav Sokolov, Egor Bogomolov, Yaroslav Golubev, Danny Dig, Timofey Bryksin</li>
<li>for: 提高 commits 的质量和个性化程度，使开发者更容易跟踪变更和协作。</li>
<li>methods: 利用 previous commit history 作为额外 контекст，通过 completion 和 generation 两种方法来生成高质量 commits。</li>
<li>results: 结果显示，在某些情况下，使用 completion 方法可以达到更高的质量和个性化程度，而使用历史信息可以提高 CMG 模型在生成任务中的表现。<details>
<summary>Abstract</summary>
Commit messages are crucial to software development, allowing developers to track changes and collaborate effectively. Despite their utility, most commit messages lack important information since writing high-quality commit messages is tedious and time-consuming. The active research on commit message generation (CMG) has not yet led to wide adoption in practice. We argue that if we could shift the focus from commit message generation to commit message completion and use previous commit history as additional context, we could significantly improve the quality and the personal nature of the resulting commit messages.   In this paper, we propose and evaluate both of these novel ideas. Since the existing datasets lack historical data, we collect and share a novel dataset called CommitChronicle, containing 10.7M commits across 20 programming languages. We use this dataset to evaluate the completion setting and the usefulness of the historical context for state-of-the-art CMG models and GPT-3.5-turbo. Our results show that in some contexts, commit message completion shows better results than generation, and that while in general GPT-3.5-turbo performs worse, it shows potential for long and detailed messages. As for the history, the results show that historical information improves the performance of CMG models in the generation task, and the performance of GPT-3.5-turbo in both generation and completion.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>软件开发中的提交消息非常重要，它允许开发者跟踪更改并协作有效。尽管它们的重要性，但大多数提交消息缺乏重要信息，因为写好提交消息是时间consuming和繁琐的。有活跃的研究在提交消息生成（CMG）领域，但尚未得到广泛的实践应用。我们认为，如果我们可以将注重点从提交消息生成转移到提交消息完成，并使用之前的提交历史作为更多的上下文，我们可以大幅提高提交消息质量和个性化度。  在这篇论文中，我们提出并评估了两个新的想法。由于现有的数据集缺乏历史数据，我们收集和分享了一个新的数据集called CommitChronicle，包含20种编程语言的10.7万个提交。我们使用这个数据集来评估完成设定和使用历史上下文来评估当前CMG模型和GPT-3.5-turbo的表现。我们的结果表明，在某些情况下，提交消息完成比生成更好，而且GPT-3.5-turbo在详细的消息中表现较差，但在某些情况下具有潜在的潜力。对于历史信息，我们的结果表明，历史信息可以提高CMG模型在生成任务中的表现，并且GPT-3.5-turbo在生成和完成任务中的表现。
</details></li>
</ul>
<hr>
<h2 id="Ternary-Singular-Value-Decomposition-as-a-Better-Parameterized-Form-in-Linear-Mapping"><a href="#Ternary-Singular-Value-Decomposition-as-a-Better-Parameterized-Form-in-Linear-Mapping" class="headerlink" title="Ternary Singular Value Decomposition as a Better Parameterized Form in Linear Mapping"></a>Ternary Singular Value Decomposition as a Better Parameterized Form in Linear Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07641">http://arxiv.org/abs/2308.07641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boyu Chen, Hanxuan Chen, Jiao He, Fengyu Sun, Shangling Jui</li>
<li>for: 这个论文的目的是提出一种简单 yet novel的线性映射方法来实现优秀的网络压缩性能。</li>
<li>methods: 这个论文使用的方法是一种叫做ternary SVD（TSVD）的 pseudo SVD，其限制了 $U$ 和 $V$ 矩阵在 SVD 中的形式为 ${\pm 1, 0}$ 的三元矩阵。这意味着在计算 $U(\cdot)$ 和 $V(\cdot)$ 时只需要使用加法操作。</li>
<li>results: 实验结果表明，TSVD 可以在不同类型的网络和任务中实现当今基eline模型如 ConvNext、Swim、BERT 和大型语言模型 OPT 的状态级压缩性能。<details>
<summary>Abstract</summary>
We present a simple yet novel parameterized form of linear mapping to achieves remarkable network compression performance: a pseudo SVD called Ternary SVD (TSVD).   Unlike vanilla SVD, TSVD limits the $U$ and $V$ matrices in SVD to ternary matrices form in $\{\pm 1, 0\}$. This means that instead of using the expensive multiplication instructions, TSVD only requires addition instructions when computing $U(\cdot)$ and $V(\cdot)$.   We provide direct and training transition algorithms for TSVD like Post Training Quantization and Quantization Aware Training respectively. Additionally, we analyze the convergence of the direct transition algorithms in theory.   In experiments, we demonstrate that TSVD can achieve state-of-the-art network compression performance in various types of networks and tasks, including current baseline models such as ConvNext, Swim, BERT, and large language model like OPT.
</details>
<details>
<summary>摘要</summary>
我们提出了一种简单 yet novel的线性映射参数化方法，可以实现出色的网络压缩性能：一种叫做ternary SVD（TSVD）的 Pseudo SVD。 unlike vanilla SVD, TSVD限制了 $U$ 和 $V$ 矩阵在 SV 中仅能是三元矩阵（\{\pm 1, 0\}）。这意味着在计算 $U(\cdot)$ 和 $V(\cdot)$ 时，TSVD 只需要使用加法指令，而不需要使用昂贵的乘法指令。我们提供了直接迁移算法和训练迁移算法，如Post Training Quantization 和 Quantization Aware Training 等。此外，我们还对直接迁移算法的整体性进行了理论分析。在实验中，我们表明了 TSVD 可以在不同类型的网络和任务上实现state-of-the-art的压缩性能，包括当前基eline模型 ConvNext、Swim、BERT 以及大型语言模型 OPT。
</details></li>
</ul>
<hr>
<h2 id="Backpropagation-Path-Search-On-Adversarial-Transferability"><a href="#Backpropagation-Path-Search-On-Adversarial-Transferability" class="headerlink" title="Backpropagation Path Search On Adversarial Transferability"></a>Backpropagation Path Search On Adversarial Transferability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07625">http://arxiv.org/abs/2308.07625</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuoer Xu, Zhangxuan Gu, Jianping Zhang, Shiwen Cui, Changhua Meng, Weiqiang Wang</li>
<li>for: 防御深度神经网络受到敌意例之攻击，需要在部署前测试模型的可靠性。</li>
<li>methods: 基于传输的攻击者使用拷贝模型构建敌意例，然后将其传输到黑盒环境中部署的受害者模型。为了增强攻击性能，结构基于的攻击者修改了反propagation路径，但现有的结构基于的攻击者忽略了 convolution 模块，并使用伪函数来修改反propagation图。</li>
<li>results: 我们提出了 backPropagation pAth Search (PAS)，解决了上述两个问题。我们首先提出了 SkipConv，用于调整 convolution 模块的反propagation路径。以免攻击路径过拟合 surrogate 模型，我们还构建了 DAG 基于搜索空间，使用一步靠近法评估路径，并使用 bayesian 优化来搜索最佳路径。我们在各种传输设置下进行了广泛的实验，显示 PAS 可以大幅提高攻击成功率，包括常训练的模型和防御模型。<details>
<summary>Abstract</summary>
Deep neural networks are vulnerable to adversarial examples, dictating the imperativeness to test the model's robustness before deployment. Transfer-based attackers craft adversarial examples against surrogate models and transfer them to victim models deployed in the black-box situation. To enhance the adversarial transferability, structure-based attackers adjust the backpropagation path to avoid the attack from overfitting the surrogate model. However, existing structure-based attackers fail to explore the convolution module in CNNs and modify the backpropagation graph heuristically, leading to limited effectiveness. In this paper, we propose backPropagation pAth Search (PAS), solving the aforementioned two problems. We first propose SkipConv to adjust the backpropagation path of convolution by structural reparameterization. To overcome the drawback of heuristically designed backpropagation paths, we further construct a DAG-based search space, utilize one-step approximation for path evaluation and employ Bayesian Optimization to search for the optimal path. We conduct comprehensive experiments in a wide range of transfer settings, showing that PAS improves the attack success rate by a huge margin for both normally trained and defense models.
</details>
<details>
<summary>摘要</summary>
深度神经网络容易受到敌意例际的攻击，因此在部署之前测试模型的可靠性是非常重要的。转移基于攻击者通过附加模型制造敌意例并将其传递到部署在黑盒子情况下的受害模型。为增强敌意例的可传递性，结构基于攻击者可以修改归并征求的路径，以避免攻击过拟合附加模型。然而，现有的结构基于攻击者未能探索CNN中的卷积模块，并修改归并图表使用了规则性的方法，导致效果有限。在这篇论文中，我们提出了backPropagation pAth Search（PAS），解决以下两个问题。我们首先提出了SkipConv，用于调整卷积后的归并路径。为了超越规则性设计的归并路径的缺点，我们进一步构建了DAG基本搜索空间，使用一步逼近方法评估归并路径，并使用bayesian优化来搜索最佳路径。我们在各种转移设置下进行了广泛的实验，结果显示，PAS可以大幅提高攻击成功率，包括常训练的模型和防御模型。
</details></li>
</ul>
<hr>
<h2 id="A-Multilayer-Perceptron-based-Fast-Sunlight-Assessment-for-the-Conceptual-Design-of-Residential-Neighborhoods-under-Chinese-Policy"><a href="#A-Multilayer-Perceptron-based-Fast-Sunlight-Assessment-for-the-Conceptual-Design-of-Residential-Neighborhoods-under-Chinese-Policy" class="headerlink" title="A Multilayer Perceptron-based Fast Sunlight Assessment for the Conceptual Design of Residential Neighborhoods under Chinese Policy"></a>A Multilayer Perceptron-based Fast Sunlight Assessment for the Conceptual Design of Residential Neighborhoods under Chinese Policy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07616">http://arxiv.org/abs/2308.07616</a></li>
<li>repo_url: None</li>
<li>paper_authors: Can Jiang, Xiong Liang, Yu-Cheng Zhou, Yong Tian, Shengli Xu, Jia-Rui Lin, Zhiliang Ma, Shiji Yang, Hao Zhou</li>
<li>for: 本研究旨在应用深度学习技术来加速建筑设计阶段的日照时数 simulations，以减少计算时间和提高设计效率。</li>
<li>methods: 本研究提出了一个多层感知器（Multilayer Perceptron，MLP）基本的一阶预测方法，可以快速地预测建筑物的日照时数。方法首先将建筑物分解为多个立方体形状的部分，然后运用一个一阶预测模型来预测每个部分的日照时数。</li>
<li>results: 经过三个 numeral experiments，包括水平层和倾斜分析、模拟运算和优化，结果显示，本方法可以将计算时间降低到1&#x2F;84<del>1&#x2F;50，并保持96.5%</del>98%的准确性。此外，基于提案的模型，也开发了一个实用的住宅区布局规划插件 для Rhino 7&#x2F;Grasshopper。<details>
<summary>Abstract</summary>
In Chinese building codes, it is required that residential buildings receive a minimum number of hours of natural, direct sunlight on a specified winter day, which represents the worst sunlight condition in a year. This requirement is a prerequisite for obtaining a building permit during the conceptual design of a residential project. Thus, officially sanctioned software is usually used to assess the sunlight performance of buildings. These software programs predict sunlight hours based on repeated shading calculations, which is time-consuming. This paper proposed a multilayer perceptron-based method, a one-stage prediction approach, which outputs a shading time interval caused by the inputted cuboid-form building. The sunlight hours of a site can be obtained by calculating the union of the sunlight time intervals (complement of shading time interval) of all the buildings. Three numerical experiments, i.e., horizontal level and slope analysis, and simulation-based optimization are carried out; the results show that the method reduces the computation time to 1/84~1/50 with 96.5%~98% accuracies. A residential neighborhood layout planning plug-in for Rhino 7/Grasshopper is also developed based on the proposed model. This paper indicates that deep learning techniques can be adopted to accelerate sunlight hour simulations at the conceptual design phase.
</details>
<details>
<summary>摘要</summary>
中国建筑标准要求住宅建筑在指定的冬季日子上得到最少的自然、直接日光时间，这是为了获得建筑许可证的必要条件。因此，官方批准的软件通常用于评估建筑的日光性能。这些软件计算出日光时间基于重复的遮挡计算，这是时间消耗大。本文提出了基于多层感知器的方法，一种一stage预测方法，它输出一个输入的立方体形建筑物遮挡时间间隔。通过计算所有建筑物的遮挡时间间隔的并集（补做遮挡时间间隔），可以获得建筑地点的日光时间。三个数学实验，即水平层和坡度分析，以及基于仿真优化的模拟，都表明了该方法可以将计算时间减少到1/84~1/50，并保持96.5%~98%的准确性。此外，基于提议的模型也开发了一个基于Rhino 7/Grasshopper的住宅街区规划插件。本文表明，深度学习技术可以在概念设计阶段加速日光时间的估算。
</details></li>
</ul>
<hr>
<h2 id="Searching-for-Novel-Chemistry-in-Exoplanetary-Atmospheres-using-Machine-Learning-for-Anomaly-Detection"><a href="#Searching-for-Novel-Chemistry-in-Exoplanetary-Atmospheres-using-Machine-Learning-for-Anomaly-Detection" class="headerlink" title="Searching for Novel Chemistry in Exoplanetary Atmospheres using Machine Learning for Anomaly Detection"></a>Searching for Novel Chemistry in Exoplanetary Atmospheres using Machine Learning for Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07604">http://arxiv.org/abs/2308.07604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roy T. Forestano, Konstantin T. Matchev, Katia Matcheva, Eyup B. Unlu</li>
<li>for: 本研究旨在开发新的快速高效的机器学习方法，用于检测望远镜观测数据中异常的行星，以找到具有不同化学成分的行星和可能的生物标志物。</li>
<li>methods: 本研究使用了两种流行的异常检测方法：本地异常因子和一类支持向量机器学习。</li>
<li>results: 研究成功地应用了这两种方法于大量的人工数据库中，并通过ROC曲线评估和比较了两种方法的性能。<details>
<summary>Abstract</summary>
The next generation of telescopes will yield a substantial increase in the availability of high-resolution spectroscopic data for thousands of exoplanets. The sheer volume of data and number of planets to be analyzed greatly motivate the development of new, fast and efficient methods for flagging interesting planets for reobservation and detailed analysis. We advocate the application of machine learning (ML) techniques for anomaly (novelty) detection to exoplanet transit spectra, with the goal of identifying planets with unusual chemical composition and even searching for unknown biosignatures. We successfully demonstrate the feasibility of two popular anomaly detection methods (Local Outlier Factor and One Class Support Vector Machine) on a large public database of synthetic spectra. We consider several test cases, each with different levels of instrumental noise. In each case, we use ROC curves to quantify and compare the performance of the two ML techniques.
</details>
<details>
<summary>摘要</summary>
下一代望远镜将提供大量高分辨率光谱数据，用于千个外围星球的分析。数据量和星球数量的增加，大大推动了新的快速高效的方法的开发，用于标注有趣的星球进行重新观测和详细分析。我们提议通过机器学习（ML）技术进行外围星球谱spectra中异常（新型）检测，以找到不寻常的化学组成和甚至搜索未知生物标志。我们成功地在大规模公共数据库中使用synthetic spectra进行了两种流行的异常检测方法的实验（Local Outlier Factor和One Class Support Vector Machine），并在不同的实rumental noise水平下进行了多个测试case。在每个测试case中，我们使用ROC曲线来评估和比较两种ML技术的性能。
</details></li>
</ul>
<hr>
<h2 id="Generating-Personas-for-Games-with-Multimodal-Adversarial-Imitation-Learning"><a href="#Generating-Personas-for-Games-with-Multimodal-Adversarial-Imitation-Learning" class="headerlink" title="Generating Personas for Games with Multimodal Adversarial Imitation Learning"></a>Generating Personas for Games with Multimodal Adversarial Imitation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07598">http://arxiv.org/abs/2308.07598</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Ahlberg, Alessandro Sestini, Konrad Tollmar, Linus Gisslén</li>
<li>for: 本研究旨在开发一种可以生成多个个性化策略的协同学习方法，以便模拟人类游戏玩家的多种玩法。</li>
<li>methods: 本研究使用了多模块生成 adversarial imitation learning（MultiGAIL）方法，通过在单机器模型中学习多个专家策略，并使用多个评估器来学习环境奖励。</li>
<li>results: 实验结果表明，MultiGAIL方法可以在连续和离散动作空间中的两个环境中生成多个个性化策略，并且在这些环境中表现出色。<details>
<summary>Abstract</summary>
Reinforcement learning has been widely successful in producing agents capable of playing games at a human level. However, this requires complex reward engineering, and the agent's resulting policy is often unpredictable. Going beyond reinforcement learning is necessary to model a wide range of human playstyles, which can be difficult to represent with a reward function. This paper presents a novel imitation learning approach to generate multiple persona policies for playtesting. Multimodal Generative Adversarial Imitation Learning (MultiGAIL) uses an auxiliary input parameter to learn distinct personas using a single-agent model. MultiGAIL is based on generative adversarial imitation learning and uses multiple discriminators as reward models, inferring the environment reward by comparing the agent and distinct expert policies. The reward from each discriminator is weighted according to the auxiliary input. Our experimental analysis demonstrates the effectiveness of our technique in two environments with continuous and discrete action spaces.
</details>
<details>
<summary>摘要</summary>
� Reinforcement learning 已经成功地将机器人训练到人类水准。但是，这需要复杂的奖励工程，并且机器人的结果策略可能是随机的。为了模型人类玩家的广泛风格，超出奖励学习是必要的。这篇论文介绍了一种具有多个人格的循环学习方法，即多模倍GAIL（MultiGAIL）。MultiGAIL使用辅助输入参数来学习不同的人格，使用多个批评者作为奖励模型，推算环境奖励通过比较机器人和具体专家策略。奖励从每个批评者被权重根据辅助输入。我们的实验分析表明我们的技术在维度和整数动作空间的两个环境中具有效果。
</details></li>
</ul>
<hr>
<h2 id="High-Probability-Risk-Bounds-via-Sequential-Predictors"><a href="#High-Probability-Risk-Bounds-via-Sequential-Predictors" class="headerlink" title="High-Probability Risk Bounds via Sequential Predictors"></a>High-Probability Risk Bounds via Sequential Predictors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07588">http://arxiv.org/abs/2308.07588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dirk van der Hoeven, Nikita Zhivotovskiy, Nicolò Cesa-Bianchi</li>
<li>for: 这 paper written for what?	+ 这 paper 目的是提供一种在线学习方法，可以在 minimal assumptions 下提供sequential regret bounds和in-expectation risk bounds。</li>
<li>methods: 这 paper 使用哪些方法?	+ 这 paper 使用 online learning methods，包括 general online learning algorithms 和 second-order correction to the loss function。</li>
<li>results: 这 paper 得到了哪些结果?	+ 这 paper 得到了nearly optimal high-probability risk bounds for several classical statistical estimation problems，such as discrete distribution estimation, linear regression, logistic regression, and conditional density estimation。<details>
<summary>Abstract</summary>
Online learning methods yield sequential regret bounds under minimal assumptions and provide in-expectation risk bounds for statistical learning. However, despite the apparent advantage of online guarantees over their statistical counterparts, recent findings indicate that in many important cases, regret bounds may not guarantee tight high-probability risk bounds in the statistical setting. In this work we show that online to batch conversions applied to general online learning algorithms can bypass this limitation. Via a general second-order correction to the loss function defining the regret, we obtain nearly optimal high-probability risk bounds for several classical statistical estimation problems, such as discrete distribution estimation, linear regression, logistic regression, and conditional density estimation. Our analysis relies on the fact that many online learning algorithms are improper, as they are not restricted to use predictors from a given reference class. The improper nature of our estimators enables significant improvements in the dependencies on various problem parameters. Finally, we discuss some computational advantages of our sequential algorithms over their existing batch counterparts.
</details>
<details>
<summary>摘要</summary>
在线学习方法提供序列 regret bound nder minimal 假设，并提供预期 риск bound  для统计学学习。然而， despite the apparent advantage of online guarantees over their statistical counterparts, recent findings indicate that in many important cases, regret bounds may not guarantee tight high-probability risk bounds in the statistical setting. In this work, we show that online to batch conversions applied to general online learning algorithms can bypass this limitation. Via a general second-order correction to the loss function defining the regret, we obtain nearly optimal high-probability risk bounds for several classical statistical estimation problems, such as discrete distribution estimation, linear regression, logistic regression, and conditional density estimation. Our analysis relies on the fact that many online learning algorithms are improper, as they are not restricted to use predictors from a given reference class. The improper nature of our estimators enables significant improvements in the dependencies on various problem parameters. Finally, we discuss some computational advantages of our sequential algorithms over their existing batch counterparts.
</details></li>
</ul>
<hr>
<h2 id="Temporal-Interest-Network-for-Click-Through-Rate-Prediction"><a href="#Temporal-Interest-Network-for-Click-Through-Rate-Prediction" class="headerlink" title="Temporal Interest Network for Click-Through Rate Prediction"></a>Temporal Interest Network for Click-Through Rate Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08487">http://arxiv.org/abs/2308.08487</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shenweichen/DSIN">https://github.com/shenweichen/DSIN</a></li>
<li>paper_authors: Haolin Zhou, Junwei Pan, Xinyi Zhou, Xihua Chen, Jie Jiang, Xiaofeng Gao, Guihai Chen</li>
<li>for: 预测点击率 (CTR) 的预测，研究者发现了用户行为历史记录的四元相关性（行为语义、目标语义、行为时间和目标时间）对性能的影响。</li>
<li>methods: 研究者使用了各种用户行为方法，包括 Semantic Embedding 和 Temporal Encoding，以及 Target-Aware Attention 和 Target-Aware Representation。</li>
<li>results: 研究者发现，现有方法无法学习这种四元相关性，而他们提出的 Temporal Interest Network (TIN) 可以有效地捕捉这种相关性，并在 Amazon 和 Alibaba 数据集上进行了广泛的评估，并与最佳基eline相比，TIN 表现出了0.43% 和 0.29% 的提升。<details>
<summary>Abstract</summary>
The history of user behaviors constitutes one of the most significant characteristics in predicting the click-through rate (CTR), owing to their strong semantic and temporal correlation with the target item. While the literature has individually examined each of these correlations, research has yet to analyze them in combination, that is, the quadruple correlation of (behavior semantics, target semantics, behavior temporal, and target temporal). The effect of this correlation on performance and the extent to which existing methods learn it remain unknown. To address this gap, we empirically measure the quadruple correlation and observe intuitive yet robust quadruple patterns. We measure the learned correlation of several representative user behavior methods, but to our surprise, none of them learn such a pattern, especially the temporal one.   In this paper, we propose the Temporal Interest Network (TIN) to capture the quadruple semantic and temporal correlation between behaviors and the target. We achieve this by incorporating target-aware temporal encoding, in addition to semantic embedding, to represent behaviors and the target. Furthermore, we deploy target-aware attention, along with target-aware representation, to explicitly conduct the 4-way interaction. We performed comprehensive evaluations on the Amazon and Alibaba datasets. Our proposed TIN outperforms the best-performing baselines by 0.43\% and 0.29\% on two datasets, respectively. Comprehensive analysis and visualization show that TIN is indeed capable of learning the quadruple correlation effectively, while all existing methods fail to do so. We provide our implementation of TIN in Tensorflow.
</details>
<details>
<summary>摘要</summary>
历史用户行为特征是预测点击率(CTR)的一个最重要的特征，因为它们具有强的 semantics和时间相关性。 Although literature has individually examined each of these correlations, research has yet to analyze them in combination, that is, the quadruple correlation of (behavior semantics, target semantics, behavior temporal, and target temporal). The effect of this correlation on performance and the extent to which existing methods learn it remain unknown. To address this gap, we empirically measure the quadruple correlation and observe intuitive yet robust quadruple patterns. We measure the learned correlation of several representative user behavior methods, but to our surprise, none of them learn such a pattern, especially the temporal one.In this paper, we propose the Temporal Interest Network (TIN) to capture the quadruple semantic and temporal correlation between behaviors and the target. We achieve this by incorporating target-aware temporal encoding, in addition to semantic embedding, to represent behaviors and the target. Furthermore, we deploy target-aware attention, along with target-aware representation, to explicitly conduct the 4-way interaction. We performed comprehensive evaluations on the Amazon and Alibaba datasets. Our proposed TIN outperforms the best-performing baselines by 0.43% and 0.29% on two datasets, respectively. Comprehensive analysis and visualization show that TIN is indeed capable of learning the quadruple correlation effectively, while all existing methods fail to do so. We provide our implementation of TIN in Tensorflow.
</details></li>
</ul>
<hr>
<h2 id="IoT-Data-Trust-Evaluation-via-Machine-Learning"><a href="#IoT-Data-Trust-Evaluation-via-Machine-Learning" class="headerlink" title="IoT Data Trust Evaluation via Machine Learning"></a>IoT Data Trust Evaluation via Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11638">http://arxiv.org/abs/2308.11638</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Timothy Tadj, Reza Arablouei, Volkan Dedeoglu<br>for:This paper aims to address the lack of publicly-available datasets for evaluating the trustworthiness of IoT data by proposing a data synthesis method called random walk infilling (RWI) to augment existing trustworthy datasets with untrustworthy data.methods:The proposed method uses RWI to generate untrustworthy data from existing trustworthy datasets, and extracts new features from IoT time-series sensor data that capture its auto-correlation and cross-correlation with neighboring sensors. These features are used to learn ML models for recognizing the trustworthiness of IoT sensor data.results:The proposed method outperforms existing ML-based approaches to IoT data trust evaluation, and a semi-supervised ML approach that requires only about 10% of the data labeled offers competitive performance while being more practical. The results also show that the ML models learned from datasets augmented via RWI generalize well to unseen data.<details>
<summary>Abstract</summary>
Various approaches based on supervised or unsupervised machine learning (ML) have been proposed for evaluating IoT data trust. However, assessing their real-world efficacy is hard mainly due to the lack of related publicly-available datasets that can be used for benchmarking. Since obtaining such datasets is challenging, we propose a data synthesis method, called random walk infilling (RWI), to augment IoT time-series datasets by synthesizing untrustworthy data from existing trustworthy data. Thus, RWI enables us to create labeled datasets that can be used to develop and validate ML models for IoT data trust evaluation. We also extract new features from IoT time-series sensor data that effectively capture its auto-correlation as well as its cross-correlation with the data of the neighboring (peer) sensors. These features can be used to learn ML models for recognizing the trustworthiness of IoT sensor data. Equipped with our synthesized ground-truth-labeled datasets and informative correlation-based feature, we conduct extensive experiments to critically examine various approaches to evaluating IoT data trust via ML. The results reveal that commonly used ML-based approaches to IoT data trust evaluation, which rely on unsupervised cluster analysis to assign trust labels to unlabeled data, perform poorly. This poor performance can be attributed to the underlying unsubstantiated assumption that clustering provides reliable labels for data trust, a premise that is found to be untenable. The results also show that the ML models learned from datasets augmented via RWI while using the proposed features generalize well to unseen data and outperform existing related approaches. Moreover, we observe that a semi-supervised ML approach that requires only about 10% of the data labeled offers competitive performance while being practically more appealing compared to the fully-supervised approaches.
</details>
<details>
<summary>摘要</summary>
各种基于指导或无指导机器学习（ML）的方法已经为评估互联网器件数据（IoT）的可信度提出了多种方法。然而，评估它们在实际世界中的有效性很难，主要因为缺乏相关的公共可用数据集，用于比较。由于获取这些数据集困难，我们提议一种数据生成方法，即随机扩散填充（RWI），以增强IoT时间序数据集。通过将可信数据 Synthesize into不可信数据，我们可以创建可用于开发和验证ML模型的标注数据集。我们还提取了IoT时间序感知器数据中有效地捕捉自动相关性以及与邻近（邻居）感知器数据的相关性。这些特征可以用来学习识别IoT感知器数据的可信度。配备我们生成的标注数据集和有用的相关特征，我们进行了广泛的实验，critically examine了多种基于ML的IoT数据可信度评估方法。结果显示，常用的ML基于方法，通过不supervised cluster analysis将无标签数据分类为可信数据，表现不佳。这些结果可以归结于这些方法下的一个不实际的假设，即分类提供可靠的数据可信度标签。结果还表明，使用RWI生成的数据集和我们提出的特征来学习ML模型，在未看到数据时generalize well，并且超过现有相关方法。此外，我们发现 semi-supervised ML方法，只需要约10%的数据标注，可以提供竞争力强的性能，而且在实际应用中更加吸引人。
</details></li>
</ul>
<hr>
<h2 id="Story-Visualization-by-Online-Text-Augmentation-with-Context-Memory"><a href="#Story-Visualization-by-Online-Text-Augmentation-with-Context-Memory" class="headerlink" title="Story Visualization by Online Text Augmentation with Context Memory"></a>Story Visualization by Online Text Augmentation with Context Memory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07575">http://arxiv.org/abs/2308.07575</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yonseivnl/cmota">https://github.com/yonseivnl/cmota</a></li>
<li>paper_authors: Daechul Ahn, Daneul Kim, Gwangmo Song, Seung Hwan Kim, Honglak Lee, Dongyeop Kang, Jonghyun Choi</li>
<li>for: 提高文本描述到图像生成 task 中的语言多样性抗锋性。</li>
<li>methods: 提出了一种基于 Bi-directional Transformer 框架的内存架构，并在训练时使用在线文本增强来生成多个 pseudo-descriptions 作为补做性超级vision 的权威指导。</li>
<li>results: 在 Pororo-SV 和 Flintstones-SV 两个受欢迎的 SV  benchmark 上，提出的方法与现状相比，在多个纪录中表现出优于其他方法，包括 FID、人物 F1、帧精度、 BLEU-2&#x2F;3 和 R-precision 等指标。<details>
<summary>Abstract</summary>
Story visualization (SV) is a challenging text-to-image generation task for the difficulty of not only rendering visual details from the text descriptions but also encoding a long-term context across multiple sentences. While prior efforts mostly focus on generating a semantically relevant image for each sentence, encoding a context spread across the given paragraph to generate contextually convincing images (e.g., with a correct character or with a proper background of the scene) remains a challenge. To this end, we propose a novel memory architecture for the Bi-directional Transformer framework with an online text augmentation that generates multiple pseudo-descriptions as supplementary supervision during training for better generalization to the language variation at inference. In extensive experiments on the two popular SV benchmarks, i.e., the Pororo-SV and Flintstones-SV, the proposed method significantly outperforms the state of the arts in various metrics including FID, character F1, frame accuracy, BLEU-2/3, and R-precision with similar or less computational complexity.
</details>
<details>
<summary>摘要</summary>
story visualization (SV) 是一个具有挑战性的文本到图像生成任务，因为不仅需要从文本描述中提取视觉细节，还需要在多句话中编码长期上下文。而现有的尝试主要是为每句文本生成Semantically relevant的图像，但是在保持场景背景和人物性别正确的情况下，在整个段落上编码上下文并生成情节感地投入的图像仍然是一个挑战。为此，我们提议一种新的内存架构，用于Bi-directional Transformer框架的在线文本增强，在训练过程中生成多个假描述作为补做性的超vision，以提高语言变化的适应性。在两个流行的 SV 标准测试集上，即Pororo-SV 和 Flintstones-SV，我们的方法与现有的状态arius signicantly outperform，包括 FID、character F1、frame accuracy、BLEU-2/3 和 R-precision 等多个维度的指标，同时具有相似或更少的计算复杂度。
</details></li>
</ul>
<hr>
<h2 id="Synthetic-data-generation-method-for-hybrid-image-tabular-data-using-two-generative-adversarial-networks"><a href="#Synthetic-data-generation-method-for-hybrid-image-tabular-data-using-two-generative-adversarial-networks" class="headerlink" title="Synthetic data generation method for hybrid image-tabular data using two generative adversarial networks"></a>Synthetic data generation method for hybrid image-tabular data using two generative adversarial networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07573">http://arxiv.org/abs/2308.07573</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tomohiro Kikuchi, Shouhei Hanaoka, Takahiro Nakao, Tomomi Takenaga, Yukihiro Nomura, Harushi Mori, Takeharu Yoshikawa</li>
<li>for: 提供一种生成静脉呼吸图像和结构化表格数据的新方法，以解决医疗隐私和数据共享问题。</li>
<li>methods: 使用自适应GAN和条件表格GAN模型，将大型公共数据库（pDB）中的静脉呼吸图像维度减少，并将图像编码器与原始数据库（oDB）中的图像进行对应。</li>
<li>results: 成功生成了多样化的合成医疗记录，保持了图像和表格数据之间的协调性，并通过视觉评估、分布分析和分类任务进行评估。<details>
<summary>Abstract</summary>
The generation of synthetic medical records using generative adversarial networks (GANs) has become increasingly important for addressing privacy concerns and promoting data sharing in the medical field. In this paper, we propose a novel method for generating synthetic hybrid medical records consisting of chest X-ray images (CXRs) and structured tabular data (including anthropometric data and laboratory tests) using an auto-encoding GAN ({\alpha}GAN) and a conditional tabular GAN (CTGAN). Our approach involves training a {\alpha}GAN model on a large public database (pDB) to reduce the dimensionality of CXRs. We then applied the trained encoder of the GAN model to the images in original database (oDB) to obtain the latent vectors. These latent vectors were combined with tabular data in oDB, and these joint data were used to train the CTGAN model. We successfully generated diverse synthetic records of hybrid CXR and tabular data, maintaining correspondence between them. We evaluated this synthetic database (sDB) through visual assessment, distribution of interrecord distances, and classification tasks. Our evaluation results showed that the sDB captured the features of the oDB while maintaining the correspondence between the images and tabular data. Although our approach relies on the availability of a large-scale pDB containing a substantial number of images with the same modality and imaging region as those in the oDB, this method has the potential for the public release of synthetic datasets without compromising the secondary use of data.
</details>
<details>
<summary>摘要</summary>
现代生成技术在医疗领域得到了广泛应用，特别是使用生成对抗网络（GANs）生成合成医疗记录，以解决隐私问题和促进数据共享。在这篇论文中，我们提出了一种新的方法，使用自动编码GAN（αGAN）和条件表格GAN（CTGAN）生成 hybrid 的胸部X射线图像（CXR）和结构化表格数据（包括人体测量数据和实验室测试）。我们的方法包括在大规模公共数据库（pDB）中训练αGAN模型，以减少CXR的维度。然后，我们将训练过的GAN模型的编码器应用到oDB中的图像上，以获取秘密 вектор。这些秘密 вектор与表格数据在oDB中进行结合，并将这些联合数据用于训练CTGAN模型。我们成功地生成了多样性的合成记录，保持了图像和表格数据之间的协调。我们通过视觉评估、记录间距离分布和分类任务进行评估。我们的评估结果表明，sDB捕捉了oDB中的特征，同时保持了图像和表格数据之间的协调。虽然我们的方法需要大规模的pDB，但这种方法具有公共发布合成数据库的潜在优势，不会损害数据的次要使用。
</details></li>
</ul>
<hr>
<h2 id="Ske2Grid-Skeleton-to-Grid-Representation-Learning-for-Action-Recognition"><a href="#Ske2Grid-Skeleton-to-Grid-Representation-Learning-for-Action-Recognition" class="headerlink" title="Ske2Grid: Skeleton-to-Grid Representation Learning for Action Recognition"></a>Ske2Grid: Skeleton-to-Grid Representation Learning for Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07571">http://arxiv.org/abs/2308.07571</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/osvai/ske2grid">https://github.com/osvai/ske2grid</a></li>
<li>paper_authors: Dongqi Cai, Yangyuxuan Kang, Anbang Yao, Yurong Chen</li>
<li>for: 这篇论文提出了一种基于骨架的动作识别表征学习框架，即Ske2Grid，以提高骨架基рован动作识别的准确率。</li>
<li>methods: 该框架使用了三种新的设计方法：图节点指定变换（GIT）、上升变换（UPT）和进步学习策略（PLS）。GIT用于构建一个固定大小的网格图像块，而UPT用于填充网格图像块中的节点。PLS用于解决一步UPT的严格性问题，并且可以逐步提高表征能力。</li>
<li>results: 实验表明，Ske2Grid在六个主流骨架基рован动作识别数据集上表现出色，与现有GCN基于解决方案相比，无需添加其他特殊设计。代码和模型可以在<a target="_blank" rel="noopener" href="https://github.com/OSVAI/Ske2Grid%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/OSVAI/Ske2Grid上下载。</a><details>
<summary>Abstract</summary>
This paper presents Ske2Grid, a new representation learning framework for improved skeleton-based action recognition. In Ske2Grid, we define a regular convolution operation upon a novel grid representation of human skeleton, which is a compact image-like grid patch constructed and learned through three novel designs. Specifically, we propose a graph-node index transform (GIT) to construct a regular grid patch through assigning the nodes in the skeleton graph one by one to the desired grid cells. To ensure that GIT is a bijection and enrich the expressiveness of the grid representation, an up-sampling transform (UPT) is learned to interpolate the skeleton graph nodes for filling the grid patch to the full. To resolve the problem when the one-step UPT is aggressive and further exploit the representation capability of the grid patch with increasing spatial size, a progressive learning strategy (PLS) is proposed which decouples the UPT into multiple steps and aligns them to multiple paired GITs through a compact cascaded design learned progressively. We construct networks upon prevailing graph convolution networks and conduct experiments on six mainstream skeleton-based action recognition datasets. Experiments show that our Ske2Grid significantly outperforms existing GCN-based solutions under different benchmark settings, without bells and whistles. Code and models are available at https://github.com/OSVAI/Ske2Grid
</details>
<details>
<summary>摘要</summary>
First, we propose a graph-node index transform (GIT) to construct a regular grid patch by assigning the nodes in the skeleton graph one by one to the desired grid cells. To ensure that GIT is a bijection and enrich the expressiveness of the grid representation, we also learn an up-sampling transform (UPT) to interpolate the skeleton graph nodes for filling the grid patch to the full.To further exploit the representation capability of the grid patch with increasing spatial size, we propose a progressive learning strategy (PLS) that decouples the UPT into multiple steps and aligns them to multiple paired GITs through a compact cascaded design learned progressively.We construct networks upon prevailing graph convolution networks and conduct experiments on six mainstream skeleton-based action recognition datasets. The results show that our Ske2Grid significantly outperforms existing GCN-based solutions under different benchmark settings, without any additional modifications or tricks. The code and models are available at https://github.com/OSVAI/Ske2Grid.
</details></li>
</ul>
<hr>
<h2 id="Semi-Supervised-Learning-with-Multiple-Imputations-on-Non-Random-Missing-Labels"><a href="#Semi-Supervised-Learning-with-Multiple-Imputations-on-Non-Random-Missing-Labels" class="headerlink" title="Semi-Supervised Learning with Multiple Imputations on Non-Random Missing Labels"></a>Semi-Supervised Learning with Multiple Imputations on Non-Random Missing Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07562">http://arxiv.org/abs/2308.07562</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jason Lu, Michael Ma, Huaze Xu, Zixi Xu</li>
<li>for: 这个论文主要针对 semi-supervised learning (SSL) 中的三个主要问题：missing at random (MAR)、missing completely at random (MCAR) 和 missing not at random (MNAR)。</li>
<li>methods: 这篇论文提出了两种新的方法，用于combine multiple imputation models，以提高准确性和减少偏见。第一种方法是使用多个插值模型，创建信任区间，并应用一个阈值来忽略低信任 pseudo-labels。第二种方法是our new method，SSL with De-biased Imputations (SSL-DI)，通过过滤不准确的数据，找到一个准确可靠的子集，然后将这个子集插值到另一个 SSL 模型中，以减少偏见。</li>
<li>results: 该论文的实验结果表明，提出的方法可以在 MCAR 和 MNAR  Situations 中效果地减少偏见，并在类别准确率方面与现有方法相比，表现出较高的性能。<details>
<summary>Abstract</summary>
Semi-Supervised Learning (SSL) is implemented when algorithms are trained on both labeled and unlabeled data. This is a very common application of ML as it is unrealistic to obtain a fully labeled dataset. Researchers have tackled three main issues: missing at random (MAR), missing completely at random (MCAR), and missing not at random (MNAR). The MNAR problem is the most challenging of the three as one cannot safely assume that all class distributions are equal. Existing methods, including Class-Aware Imputation (CAI) and Class-Aware Propensity (CAP), mostly overlook the non-randomness in the unlabeled data. This paper proposes two new methods of combining multiple imputation models to achieve higher accuracy and less bias. 1) We use multiple imputation models, create confidence intervals, and apply a threshold to ignore pseudo-labels with low confidence. 2) Our new method, SSL with De-biased Imputations (SSL-DI), aims to reduce bias by filtering out inaccurate data and finding a subset that is accurate and reliable. This subset of the larger dataset could be imputed into another SSL model, which will be less biased. The proposed models have been shown to be effective in both MCAR and MNAR situations, and experimental results show that our methodology outperforms existing methods in terms of classification accuracy and reducing bias.
</details>
<details>
<summary>摘要</summary>
《半supervised学习（SSL）实现方法》在实际应用中，通常不可能获得完全标注数据集，因此SSL成为了非常常见的应用。研究人员面临着三个主要问题：Random missing（MAR）、完全随机缺失（MCAR）和非随机缺失（MNAR）。MNAR问题是三个问题中最为困难，因为不可能安全地假设所有类别分布相同。现有的方法，包括Class-Aware Imputation（CAI）和Class-Aware Propensity（CAP），几乎忽略了不Random的未标注数据。本文提出了两种新的方法，用于 combinig多个替补模型，以达到更高的准确率和更少的偏见。1. 我们使用多个替补模型，创建信任区间，并应用一个阈值，以忽略低信任 Pseudo-labels。2. 我们的新方法，SSL with De-biased Imputations（SSL-DI），旨在减少偏见，通过筛选不准确的数据，并找到一个准确可靠的子集，并将这个子集用于另一个SSL模型，以减少偏见。我们的方法在MCAR和MNAR情况下都有显著的优势，实验结果表明，我们的方法在分类精度和减少偏见方面都超过了现有方法。
</details></li>
</ul>
<hr>
<h2 id="A-User-Centered-Evaluation-of-Spanish-Text-Simplification"><a href="#A-User-Centered-Evaluation-of-Spanish-Text-Simplification" class="headerlink" title="A User-Centered Evaluation of Spanish Text Simplification"></a>A User-Centered Evaluation of Spanish Text Simplification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07556">http://arxiv.org/abs/2308.07556</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrian de Wynter, Anthony Hevia, Si-Qing Chen</li>
<li>for: 评估西班牙文简化（TS）生产系统中的表现，通过复杂句子和复杂词语识别两个字库。</li>
<li>methods: 使用神经网络对西班牙语TS进行评估，并与最常见的西班牙语特有可读性分数进行比较，发现神经网络在预测用户TS偏好时表现更好。</li>
<li>results: 发现多语言模型在同一任务中下降表现，而所有模型往往围绕毫不重要的统计特征（如句子长度）集中焦点。<details>
<summary>Abstract</summary>
We present an evaluation of text simplification (TS) in Spanish for a production system, by means of two corpora focused in both complex-sentence and complex-word identification. We compare the most prevalent Spanish-specific readability scores with neural networks, and show that the latter are consistently better at predicting user preferences regarding TS. As part of our analysis, we find that multilingual models underperform against equivalent Spanish-only models on the same task, yet all models focus too often on spurious statistical features, such as sentence length. We release the corpora in our evaluation to the broader community with the hopes of pushing forward the state-of-the-art in Spanish natural language processing.
</details>
<details>
<summary>摘要</summary>
我们对西班牙文简化文本（TS）进行了评估，使用了两个文本库，专注于复杂句子和复杂单词识别。我们比较了最常见的西班牙语特有的可读性分数，以及神经网络，并发现后者在预测用户对TS的偏好时表现更好。在我们的分析中，我们发现了许多多语言模型在同一任务上表现较差，但所有模型很多时候强调无关紧要的统计特征，如句子长度。我们将我们的评估 corpora 发布给广泛的社区，以促进西班牙自然语言处理领域的进步。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-the-Antidote-Improved-Pointwise-Certifications-against-Poisoning-Attacks"><a href="#Enhancing-the-Antidote-Improved-Pointwise-Certifications-against-Poisoning-Attacks" class="headerlink" title="Enhancing the Antidote: Improved Pointwise Certifications against Poisoning Attacks"></a>Enhancing the Antidote: Improved Pointwise Certifications against Poisoning Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07553">http://arxiv.org/abs/2308.07553</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shijie Liu, Andrew C. Cullen, Paul Montague, Sarah M. Erfani, Benjamin I. P. Rubinstein</li>
<li>for: 防止毒素攻击影响模型行为</li>
<li>methods: 使用渐进隐私和随机采样 Gaussian 机制确保测试实例对 finite 数量毒素样例的不变性</li>
<li>results: 提供更大于先前证明的攻击Robustness 保证<details>
<summary>Abstract</summary>
Poisoning attacks can disproportionately influence model behaviour by making small changes to the training corpus. While defences against specific poisoning attacks do exist, they in general do not provide any guarantees, leaving them potentially countered by novel attacks. In contrast, by examining worst-case behaviours Certified Defences make it possible to provide guarantees of the robustness of a sample against adversarial attacks modifying a finite number of training samples, known as pointwise certification. We achieve this by exploiting both Differential Privacy and the Sampled Gaussian Mechanism to ensure the invariance of prediction for each testing instance against finite numbers of poisoned examples. In doing so, our model provides guarantees of adversarial robustness that are more than twice as large as those provided by prior certifications.
</details>
<details>
<summary>摘要</summary>
毒素攻击可以夹击模型行为，通过小量修改训练集来让模型表现出巨大的影响。虽然有针对特定毒素攻击的防御方法，但这些防御方法通常不提供任何保证，因此可能会被新的攻击所打砸。相比之下，通过检查最坏情况的证明 Certified Defences，我们可以提供对测试实例的预测结果进行保证，并 garantuee 测试实例对于有限多个毒素样本的修改后的预测结果的一致性。通过利用差分隐私和随机 Gaussian 机制，我们的模型可以保证对于每个测试实例，对于有限多个毒素样本的修改后的预测结果的一致性。这使得我们的模型可以提供更大于之前证明的防御 robustness。
</details></li>
</ul>
<hr>
<h2 id="Domain-Adaptation-via-Minimax-Entropy-for-Real-Bogus-Classification-of-Astronomical-Alerts"><a href="#Domain-Adaptation-via-Minimax-Entropy-for-Real-Bogus-Classification-of-Astronomical-Alerts" class="headerlink" title="Domain Adaptation via Minimax Entropy for Real&#x2F;Bogus Classification of Astronomical Alerts"></a>Domain Adaptation via Minimax Entropy for Real&#x2F;Bogus Classification of Astronomical Alerts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07538">http://arxiv.org/abs/2308.07538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillermo Cabrera-Vives, César Bolivar, Francisco Förster, Alejandra M. Muñoz Arancibia, Manuel Pérez-Carrasco, Esteban Reyes</li>
<li>for: 这个论文旨在研究域域适应（Domain Adaptation）技术，用于实时分析大量天文数据。</li>
<li>methods: 该论文使用了四个不同的数据集（HiTS、DES、ATLAS和ZTF），研究这些数据集之间的域Shift，并通过微调和半supervised深度适应来提高一个简单的深度学习分类模型。</li>
<li>results: 研究发现，微调和MME模型可以在目标数据集上提高基本模型的准确率，但MME模型不会对源数据集产生影响。只需要一些来自目标数据集的标注项（一个或 fewer），微调和MME模型就可以显著提高分类精度。<details>
<summary>Abstract</summary>
Time domain astronomy is advancing towards the analysis of multiple massive datasets in real time, prompting the development of multi-stream machine learning models. In this work, we study Domain Adaptation (DA) for real/bogus classification of astronomical alerts using four different datasets: HiTS, DES, ATLAS, and ZTF. We study the domain shift between these datasets, and improve a naive deep learning classification model by using a fine tuning approach and semi-supervised deep DA via Minimax Entropy (MME). We compare the balanced accuracy of these models for different source-target scenarios. We find that both the fine tuning and MME models improve significantly the base model with as few as one labeled item per class coming from the target dataset, but that the MME does not compromise its performance on the source dataset.
</details>
<details>
<summary>摘要</summary>
时域天文学在实时处理大量数据方面升级，导致多流机器学习模型的发展。在这项工作中，我们研究天文学警报真假分类中的领域适应（DA），使用四个不同的数据集：HiTS、DES、ATLAS和ZTF。我们研究这些数据集之间的领域差异，并通过微调和半supervised深度DA via Minimax Entropy（MME）提高了基本模型的性能。我们对不同的源目标场景进行比较，发现两者都可以大幅提高基本模型，但MME不会在目标集中妥协性能。
</details></li>
</ul>
<hr>
<h2 id="KMF-Knowledge-Aware-Multi-Faceted-Representation-Learning-for-Zero-Shot-Node-Classification"><a href="#KMF-Knowledge-Aware-Multi-Faceted-Representation-Learning-for-Zero-Shot-Node-Classification" class="headerlink" title="KMF: Knowledge-Aware Multi-Faceted Representation Learning for Zero-Shot Node Classification"></a>KMF: Knowledge-Aware Multi-Faceted Representation Learning for Zero-Shot Node Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08563">http://arxiv.org/abs/2308.08563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Likang Wu, Junji Jiang, Hongke Zhao, Hao Wang, Defu Lian, Mengdi Zhang, Enhong Chen</li>
<li>for:  zero-shot node classification (ZNC) task in graph data analysis, to predict nodes from unseen classes</li>
<li>methods:  Knowledge-Aware Multi-Faceted (KMF) framework that enhances label semantics via extracted KG-based topics, and reconstructs node content to a topic-level representation</li>
<li>results:  extensive experiments on several public graph datasets, demonstrating effectiveness and generalization of KMF compared to state-of-the-art baselines, and an application of zero-shot cross-domain recommendation.<details>
<summary>Abstract</summary>
Recently, Zero-Shot Node Classification (ZNC) has been an emerging and crucial task in graph data analysis. This task aims to predict nodes from unseen classes which are unobserved in the training process. Existing work mainly utilizes Graph Neural Networks (GNNs) to associate features' prototypes and labels' semantics thus enabling knowledge transfer from seen to unseen classes. However, the multi-faceted semantic orientation in the feature-semantic alignment has been neglected by previous work, i.e. the content of a node usually covers diverse topics that are relevant to the semantics of multiple labels. It's necessary to separate and judge the semantic factors that tremendously affect the cognitive ability to improve the generality of models. To this end, we propose a Knowledge-Aware Multi-Faceted framework (KMF) that enhances the richness of label semantics via the extracted KG (Knowledge Graph)-based topics. And then the content of each node is reconstructed to a topic-level representation that offers multi-faceted and fine-grained semantic relevancy to different labels. Due to the particularity of the graph's instance (i.e., node) representation, a novel geometric constraint is developed to alleviate the problem of prototype drift caused by node information aggregation. Finally, we conduct extensive experiments on several public graph datasets and design an application of zero-shot cross-domain recommendation. The quantitative results demonstrate both the effectiveness and generalization of KMF with the comparison of state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
近期，零批节点分类（ZNC）已成为图数据分析中emerging和重要的任务。这个任务的目标是从训练过程中未见过的类型中预测节点。现有的工作主要利用图神经网络（GNNs）来协调特征的抽象和标签的 semantics，从而实现知识传递从见到未见类型。然而，先前的工作忽略了多元的semantic orientation在特征-semanticAlignment中，即节点的内容通常涵盖多个标签的 semantics，这些标签相关的多个话题。为了提高模型的通用性，我们提出了一种知识具有Multi-Faceted框架（KMF），该框架可以提高标签的 semantics richness via 提取的知识图（KG）基于话题。然后，每个节点的内容被重建到话题级别的表示，这种表示提供了多个标签的多元和细化的semantic relevancy。由于图的实例（即节点）表示的特殊性，我们开发了一种 novel geometric constraint来缓解由节点信息汇集所引起的 prototype drift问题。最后，我们在多个公共图据集上进行了广泛的实验，并设计了零shot Cross-Domain recommender。量化结果表明，KMF的效果和通用性在比较现有的基准下都显著。
</details></li>
</ul>
<hr>
<h2 id="Projection-Free-Methods-for-Stochastic-Simple-Bilevel-Optimization-with-Convex-Lower-level-Problem"><a href="#Projection-Free-Methods-for-Stochastic-Simple-Bilevel-Optimization-with-Convex-Lower-level-Problem" class="headerlink" title="Projection-Free Methods for Stochastic Simple Bilevel Optimization with Convex Lower-level Problem"></a>Projection-Free Methods for Stochastic Simple Bilevel Optimization with Convex Lower-level Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07536">http://arxiv.org/abs/2308.07536</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jincheng Cao, Ruichen Jiang, Nazanin Abolfazli, Erfan Yazdandoost Hamedani, Aryan Mokhtari</li>
<li>For: 该论文研究了一类随机二级优化问题，即随机简单二级优化问题，其中我们寻找一个最优的解，满足一个随机对象函数的最小化。* Methods: 我们提出了一种新的随机二级优化方法，利用随机割辑来近似下一级问题的解，然后通过条件梯度更新和减少误差来控制随机导数引起的错误。* Results: 我们证明了，对于凹函数上的上一级问题，我们的方法需要 $\tilde{\mathcal{O}(\max{1&#x2F;\epsilon_f^{2},1&#x2F;\epsilon_g^{2}})$ 随机票 query，以获得一个 $\epsilon_f$-优化的上一级解和 $\epsilon_g$-优化的下一级解。这个证明超越了之前的最佳记录 $\mathcal{O}(\max{1&#x2F;\epsilon_f^{4},1&#x2F;\epsilon_g^{4}})$. 更进一步，对于非凹函数上的上一级问题，我们的方法需要最多 $\tilde{\mathcal{O}(\max{1&#x2F;\epsilon_f^{3},1&#x2F;\epsilon_g^{3}})$ 随机票 query，以找到一个 $(\epsilon_f, \epsilon_g)$-静点。在finite-sum设定下，我们证明了我们的方法需要 $\tilde{\mathcal{O}(\sqrt{n}&#x2F;\epsilon)$ 和 $\tilde{\mathcal{O}(\sqrt{n}&#x2F;\epsilon^{2})$ 随机票 query，它们取决于对象函数是 convex 还是 non-convex。<details>
<summary>Abstract</summary>
In this paper, we study a class of stochastic bilevel optimization problems, also known as stochastic simple bilevel optimization, where we minimize a smooth stochastic objective function over the optimal solution set of another stochastic convex optimization problem. We introduce novel stochastic bilevel optimization methods that locally approximate the solution set of the lower-level problem via a stochastic cutting plane, and then run a conditional gradient update with variance reduction techniques to control the error induced by using stochastic gradients. For the case that the upper-level function is convex, our method requires $\tilde{\mathcal{O}(\max\{1/\epsilon_f^{2},1/\epsilon_g^{2}\}) $ stochastic oracle queries to obtain a solution that is $\epsilon_f$-optimal for the upper-level and $\epsilon_g$-optimal for the lower-level. This guarantee improves the previous best-known complexity of $\mathcal{O}(\max\{1/\epsilon_f^{4},1/\epsilon_g^{4}\})$. Moreover, for the case that the upper-level function is non-convex, our method requires at most $\tilde{\mathcal{O}(\max\{1/\epsilon_f^{3},1/\epsilon_g^{3}\}) $ stochastic oracle queries to find an $(\epsilon_f, \epsilon_g)$-stationary point. In the finite-sum setting, we show that the number of stochastic oracle calls required by our method are $\tilde{\mathcal{O}(\sqrt{n}/\epsilon)$ and $\tilde{\mathcal{O}(\sqrt{n}/\epsilon^{2})$ for the convex and non-convex settings, respectively, where $\epsilon=\min \{\epsilon_f,\epsilon_g\}$.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究一类随机二重优化问题，即随机简单二重优化问题，其中我们寻找最优解集的最小值。我们提出了一种新的随机二重优化方法，使用随机割辑来近似下一级问题的解集，然后运行一个 conditional gradient update 的方法来控制由随机导数引起的误差。对于凸Upper-level函数的情况，我们的方法需要 $\tilde{\mathcal{O}(\max\{1/\epsilon_f^{2},1/\epsilon_g^{2}\}) $ 随机oracle查询来获得一个 $\epsilon_f$-优化的上一级解和 $\epsilon_g$-优化的下一级解。这个 garantía提高了之前的最佳 Complexity 的 $\mathcal{O}(\max\{1/\epsilon_f^{4},1/\epsilon_g^{4}\})$.而对于非凸 Upper-level函数的情况，我们的方法需要最多 $\tilde{\mathcal{O}(\max\{1/\epsilon_f^{3},1/\epsilon_g^{3}\}) $ 随机oracle查询来找到一个$(\epsilon_f, \epsilon_g)$-静点。在finite-sum设定下，我们证明了我们的方法需要 $\tilde{\mathcal{O}(\sqrt{n}/\epsilon)$ 和 $\tilde{\mathcal{O}(\sqrt{n}/\epsilon^{2})$ 随机oracle查询，其中 $\epsilon = \min \{\epsilon_f, \epsilon_g\}$.
</details></li>
</ul>
<hr>
<h2 id="Inverse-Lithography-Physics-informed-Deep-Neural-Level-Set-for-Mask-Optimization"><a href="#Inverse-Lithography-Physics-informed-Deep-Neural-Level-Set-for-Mask-Optimization" class="headerlink" title="Inverse Lithography Physics-informed Deep Neural Level Set for Mask Optimization"></a>Inverse Lithography Physics-informed Deep Neural Level Set for Mask Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12299">http://arxiv.org/abs/2308.12299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xing-Yu Ma, Shaogang Hao</li>
<li>for: 这篇论文主要是为了提出一种基于深度学习的 inverse lithography physics-informed deep neural level set（ILDLS）方法，用于mask优化。</li>
<li>methods: 这篇论文使用了深度学习（DL）方法，并将 inverse lithography physics  incorporated into DL 框架中。具体来说，这篇论文使用了 level set 基于的 inverse lithography technology（ILT）作为层，并在这个层中进行了mask预测和修正。</li>
<li>results: 相比于纯度DL和ILT，ILDLS可以减少计算时间的数个数量级，同时提高了印刷可能性和过程窗口（PW）。这篇论文的结果表明，ILDLS可以提供一种高效的mask优化解决方案。<details>
<summary>Abstract</summary>
As the feature size of integrated circuits continues to decrease, optical proximity correction (OPC) has emerged as a crucial resolution enhancement technology for ensuring high printability in the lithography process. Recently, level set-based inverse lithography technology (ILT) has drawn considerable attention as a promising OPC solution, showcasing its powerful pattern fidelity, especially in advanced process. However, massive computational time consumption of ILT limits its applicability to mainly correcting partial layers and hotspot regions. Deep learning (DL) methods have shown great potential in accelerating ILT. However, lack of domain knowledge of inverse lithography limits the ability of DL-based algorithms in process window (PW) enhancement and etc. In this paper, we propose an inverse lithography physics-informed deep neural level set (ILDLS) approach for mask optimization. This approach utilizes level set based-ILT as a layer within the DL framework and iteratively conducts mask prediction and correction to significantly enhance printability and PW in comparison with results from pure DL and ILT. With this approach, computation time is reduced by a few orders of magnitude versus ILT. By gearing up DL with knowledge of inverse lithography physics, ILDLS provides a new and efficient mask optimization solution.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose an inverse lithography physics-informed deep neural level set (ILDLS) approach for mask optimization. This approach utilizes level set-based ILT as a layer within the DL framework and iteratively conducts mask prediction and correction to significantly enhance printability and PW in comparison with results from pure DL and ILT. With this approach, computation time is reduced by a few orders of magnitude versus ILT. By combining DL with knowledge of inverse lithography physics, ILDLS provides a new and efficient mask optimization solution.
</details></li>
</ul>
<hr>
<h2 id="FeatGeNN-Improving-Model-Performance-for-Tabular-Data-with-Correlation-based-Feature-Extraction"><a href="#FeatGeNN-Improving-Model-Performance-for-Tabular-Data-with-Correlation-based-Feature-Extraction" class="headerlink" title="FeatGeNN: Improving Model Performance for Tabular Data with Correlation-based Feature Extraction"></a>FeatGeNN: Improving Model Performance for Tabular Data with Correlation-based Feature Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07527">http://arxiv.org/abs/2308.07527</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sammuel Ramos Silva, Rodrigo Silva</li>
<li>for: 提高机器学习模型性能和统计分析中获取更多信息</li>
<li>methods: 使用 corr 函数作为池化函数，从数据矩阵中提取和创建新特征</li>
<li>results: 在多个标准测试集上比较 FeatGeNN 与现有 AutoFE 方法，显示 FeatGeNN 可以提高模型性能。 corr 函数可以作为 tabular 数据中 pooling 函数的有力的替代方案。<details>
<summary>Abstract</summary>
Automated Feature Engineering (AutoFE) has become an important task for any machine learning project, as it can help improve model performance and gain more information for statistical analysis. However, most current approaches for AutoFE rely on manual feature creation or use methods that can generate a large number of features, which can be computationally intensive and lead to overfitting. To address these challenges, we propose a novel convolutional method called FeatGeNN that extracts and creates new features using correlation as a pooling function. Unlike traditional pooling functions like max-pooling, correlation-based pooling considers the linear relationship between the features in the data matrix, making it more suitable for tabular data. We evaluate our method on various benchmark datasets and demonstrate that FeatGeNN outperforms existing AutoFE approaches regarding model performance. Our results suggest that correlation-based pooling can be a promising alternative to max-pooling for AutoFE in tabular data applications.
</details>
<details>
<summary>摘要</summary>
自动Feature工程（AutoFE）已成为机器学习项目中重要的任务，因为它可以提高模型性能并提供更多的统计分析信息。然而，现有的AutoFE方法大多依赖于手动创建特征或使用生成大量特征的方法，这可能会占用大量计算资源并导致过拟合。为解决这些挑战，我们提出了一种新的卷积方法 called FeatGeNN，它通过对数据矩阵中特征之间的相关性进行抽象，从而生成新的特征。与传统的最大值抽取函数不同，相关性基于的抽取函数更适合逻辑数据。我们在多个 benchmark 数据集上评估了我们的方法，并证明了 FeatGeNN 在AutoFE中超过了现有的方法。我们的结果表明，相关性基于的抽取函数可以成为 tabular 数据应用中 AutoFE 中的一个有前途的代替方案。
</details></li>
</ul>
<hr>
<h2 id="Potential-of-Deep-Operator-Networks-in-Digital-Twin-enabling-Technology-for-Nuclear-System"><a href="#Potential-of-Deep-Operator-Networks-in-Digital-Twin-enabling-Technology-for-Nuclear-System" class="headerlink" title="Potential of Deep Operator Networks in Digital Twin-enabling Technology for Nuclear System"></a>Potential of Deep Operator Networks in Digital Twin-enabling Technology for Nuclear System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07523">http://arxiv.org/abs/2308.07523</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kazuma Kobayashi, Syed Bahauddin Alam</li>
<li>for: 这个研究旨在提出一种可靠且高精度的数据零层级模型（DeepONet），用于数位双（DT）系统中的核工程应用。</li>
<li>methods: 这个研究使用DeepONet方法，将函数作为输入数据，从训练数据中构建了算子G。</li>
<li>results: DeepONet方法在解决困难的粒子运输问题上展现了杰出的预测精度，比传统机器学习方法更高。<details>
<summary>Abstract</summary>
This research introduces the Deep Operator Network (DeepONet) as a robust surrogate modeling method within the context of digital twin (DT) systems for nuclear engineering. With the increasing importance of nuclear energy as a carbon-neutral solution, adopting DT technology has become crucial to enhancing operational efficiencies, safety, and predictive capabilities in nuclear engineering applications. DeepONet exhibits remarkable prediction accuracy, outperforming traditional ML methods. Through extensive benchmarking and evaluation, this study showcases the scalability and computational efficiency of DeepONet in solving a challenging particle transport problem. By taking functions as input data and constructing the operator $G$ from training data, DeepONet can handle diverse and complex scenarios effectively. However, the application of DeepONet also reveals challenges related to optimal sensor placement and model evaluation, critical aspects of real-world implementation. Addressing these challenges will further enhance the method's practicality and reliability. Overall, DeepONet presents a promising and transformative tool for nuclear engineering research and applications. Its accurate prediction and computational efficiency capabilities can revolutionize DT systems, advancing nuclear engineering research. This study marks an important step towards harnessing the power of surrogate modeling techniques in critical engineering domains.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:这项研究介绍了深度操作网络（DeepONet）作为核动力工程中数字双（DT）系统中的稳定和准确的模拟方法。随着核能作为碳中和解方案的重要性增加，采用DT技术已经成为核动力工程应用中提高操作效率、安全性和预测能力的关键。DeepONet在多个核心问题上表现出了惊人的预测精度，超越传统机器学习方法。通过广泛的 benchmarking 和评估，这项研究展示了 DeepONet 在解决复杂的粒子传输问题时的扩展性和计算效率。通过将函数作为输入数据，从训练数据中构建操作符 $G$，DeepONet 可以有效地处理多样化和复杂的场景。然而， DeepONet 的应用也暴露了优化传感器布局和模型评估的挑战，这些挑战在实际应用中是关键的。解决这些挑战将进一步提高 DeepONet 的实用性和可靠性。总的来说，DeepONet 提供了核动力工程研究和应用中的一个可靠和转型的工具。它的准确预测和计算效率能力可以对 DT 系统进行革命性的改进，推动核动力工程研究。这项研究标志着使用表达式模拟技术在关键工程领域的应用的重要一步。
</details></li>
</ul>
<hr>
<h2 id="Nonlinearity-Feedback-and-Uniform-Consistency-in-Causal-Structural-Learning"><a href="#Nonlinearity-Feedback-and-Uniform-Consistency-in-Causal-Structural-Learning" class="headerlink" title="Nonlinearity, Feedback and Uniform Consistency in Causal Structural Learning"></a>Nonlinearity, Feedback and Uniform Consistency in Causal Structural Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07520">http://arxiv.org/abs/2308.07520</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuyan Wang</li>
<li>for: 这个论文的目的是找到自动搜索方法，用于从观察数据中学习 causal structure。</li>
<li>methods: 这个论文使用的方法包括 modifying Strong Faithfulness 和 relaxing sufficiency assumption，以扩展 causal discovery 方法的应用范围。</li>
<li>results: 这个论文的研究结果表明，通过 relaxing 简化假设，可以扩展 causal discovery 方法的应用范围，并且可以学习 causal structure with latent variables。<details>
<summary>Abstract</summary>
The goal of Causal Discovery is to find automated search methods for learning causal structures from observational data. In some cases all variables of the interested causal mechanism are measured, and the task is to predict the effects one measured variable has on another. In contrast, sometimes the variables of primary interest are not directly observable but instead inferred from their manifestations in the data. These are referred to as latent variables. One commonly known example is the psychological construct of intelligence, which cannot directly measured so researchers try to assess through various indicators such as IQ tests. In this case, casual discovery algorithms can uncover underlying patterns and structures to reveal the causal connections between the latent variables and between the latent and observed variables. This thesis focuses on two questions in causal discovery: providing an alternative definition of k-Triangle Faithfulness that (i) is weaker than strong faithfulness when applied to the Gaussian family of distributions, (ii) can be applied to non-Gaussian families of distributions, and (iii) under the assumption that the modified version of Strong Faithfulness holds, can be used to show the uniform consistency of a modified causal discovery algorithm; relaxing the sufficiency assumption to learn causal structures with latent variables. Given the importance of inferring cause-and-effect relationships for understanding and forecasting complex systems, the work in this thesis of relaxing various simplification assumptions is expected to extend the causal discovery method to be applicable in a wider range with diversified causal mechanism and statistical phenomena.
</details>
<details>
<summary>摘要</summary>
目的是找到自动搜寻方法，以了解观察资料中的 causal 结构。在某些情况下，所有兴趣的 causal 机制中的所有变量都是观察到的，而任务是预测一个观察到的变量对另一个变量的影响。相比之下，有时候兴趣的变量不是直接观察到的，而是从资料中的现象推断出来的。这些被称为潜在变量。例如，心理学中的智商不能直接观察，因此研究人员会尝试透过不同的指标，如智商测验，来评估。在这种情况下， causal 发现算法可以探测到底层结构和联系，以显示 causal 连接between latent 变量和观察到的变量。本论文关注两个问题：提供一个弱 faithfulness 定义，其中（i）在 Gaussian 家族中的分布下是弱 faithfulness 的，（ii）可以应用到非 Gaussian 家族的分布，以及（iii）在 modified 稳定假设下，可以用来证明一个修改版的 causal 发现算法的均匀一致性。将适当的假设放宽，以学习包含潜在变量的 causal 结构。由于推断 causal 关系的重要性，这项工作预期能够将 causal 发现方法扩展到更广泛的应用，包括多元的 causal 机制和 statistically 现象。
</details></li>
</ul>
<hr>
<h2 id="Distilling-Knowledge-from-Resource-Management-Algorithms-to-Neural-Networks-A-Unified-Training-Assistance-Approach"><a href="#Distilling-Knowledge-from-Resource-Management-Algorithms-to-Neural-Networks-A-Unified-Training-Assistance-Approach" class="headerlink" title="Distilling Knowledge from Resource Management Algorithms to Neural Networks: A Unified Training Assistance Approach"></a>Distilling Knowledge from Resource Management Algorithms to Neural Networks: A Unified Training Assistance Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07511">http://arxiv.org/abs/2308.07511</a></li>
<li>repo_url: None</li>
<li>paper_authors: Longfei Ma, Nan Cheng, Xiucheng Wang, Zhisheng Yin, Haibo Zhou, Wei Quan</li>
<li>for: 提高多用户通信系统中信号干扰比例（SINR）优化的精度和速度。</li>
<li>methods: 使用知识储存（KD）技术和人工神经网络（NN）结合 traditional SINR 优化方法，以提高无监督和强化学习方法的性能和速度。</li>
<li>results: 在模拟结果中，提出的AD方法比传统学习方法更高效，并且能够解决无监督学习和强化学习中常见的问题，如获取优质解决方案和避免过度适应。<details>
<summary>Abstract</summary>
As a fundamental problem, numerous methods are dedicated to the optimization of signal-to-interference-plus-noise ratio (SINR), in a multi-user setting. Although traditional model-based optimization methods achieve strong performance, the high complexity raises the research of neural network (NN) based approaches to trade-off the performance and complexity. To fully leverage the high performance of traditional model-based methods and the low complexity of the NN-based method, a knowledge distillation (KD) based algorithm distillation (AD) method is proposed in this paper to improve the performance and convergence speed of the NN-based method, where traditional SINR optimization methods are employed as ``teachers" to assist the training of NNs, which are ``students", thus enhancing the performance of unsupervised and reinforcement learning techniques. This approach aims to alleviate common issues encountered in each of these training paradigms, including the infeasibility of obtaining optimal solutions as labels and overfitting in supervised learning, ensuring higher convergence performance in unsupervised learning, and improving training efficiency in reinforcement learning. Simulation results demonstrate the enhanced performance of the proposed AD-based methods compared to traditional learning methods. Remarkably, this research paves the way for the integration of traditional optimization insights and emerging NN techniques in wireless communication system optimization.
</details>
<details>
<summary>摘要</summary>
Traditional model-based optimization methods have been widely used to optimize signal-to-interference-plus-noise ratio (SINR) in multi-user settings, but these methods are often complex and have high computational complexity. To address this issue, this paper proposes a knowledge distillation (KD) based algorithm distillation (AD) method that combines the strengths of traditional model-based methods and neural network (NN) based approaches. The proposed method uses traditional SINR optimization methods as "teachers" to assist the training of NNs, which are "students", to improve the performance and convergence speed of the NN-based method. This approach can alleviate common issues encountered in each of these training paradigms, such as the infeasibility of obtaining optimal solutions as labels and overfitting in supervised learning, ensuring higher convergence performance in unsupervised learning, and improving training efficiency in reinforcement learning. Simulation results show that the proposed AD-based methods outperform traditional learning methods, paving the way for the integration of traditional optimization insights and emerging NN techniques in wireless communication system optimization.Here's the word-for-word translation of the text into Simplified Chinese:多种方法都是为了优化信号干扰比例（SINR）的多用户设置中的问题。虽然传统的模型基于优化方法具有强大的表现，但它们的计算复杂性高。为了解决这个问题，这篇文章提出了知识储备（KD）基于算法储备（AD）方法。该方法使用传统的SINR优化方法作为“教师”，以帮助训练神经网络（NN），作为“学生”，从而提高NN的性能和速度。这种方法可以解决每个训练模式中的常见问题，如获得优化解为标签的不可能性和超参数过敏，确保更高的整合性表现，并提高循环学习的训练效率。实验结果表明，提议的AD-based方法比传统的学习方法更高效。这些研究开创了传统优化思想和新兴神经网络技术在无线通信系统优化中的集成。
</details></li>
</ul>
<hr>
<h2 id="Data-Race-Detection-Using-Large-Language-Models"><a href="#Data-Race-Detection-Using-Large-Language-Models" class="headerlink" title="Data Race Detection Using Large Language Models"></a>Data Race Detection Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07505">http://arxiv.org/abs/2308.07505</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Le Chen, Xianzhong Ding, Murali Emani, Tristan Vanderbruggen, Pei-hung Lin, Chuanhua Liao</li>
<li>for: 本文旨在探讨使用大语言模型（LLMs）来检测数据竞争问题，以代替手动创建资源密集的工具。</li>
<li>methods: 本文提出了一种基于提示工程和精度调整技术的新的数据竞争检测方法，使用了特制的DRB-ML数据集，并对代表性的LLMs和开源LLMs进行了评估和精度调整。</li>
<li>results: 实验表明，LLMs可以成为数据竞争检测的可能的方法，但是它们仍无法与传统的数据竞争检测工具相比提供详细的变量对 causing数据竞争的信息。<details>
<summary>Abstract</summary>
Large language models (LLMs) are demonstrating significant promise as an alternate strategy to facilitate analyses and optimizations of high-performance computing programs, circumventing the need for resource-intensive manual tool creation. In this paper, we explore a novel LLM-based data race detection approach combining prompting engineering and fine-tuning techniques. We create a dedicated dataset named DRB-ML, which is derived from DataRaceBench, with fine-grain labels showing the presence of data race pairs and their associated variables, line numbers, and read/write information. DRB-ML is then used to evaluate representative LLMs and fine-tune open-source ones. Our experiment shows that LLMs can be a viable approach to data race detection. However, they still cannot compete with traditional data race detection tools when we need detailed information about variable pairs causing data races.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）正在展示出很大的承诺，用作高性能计算程序分析和优化的代替策略，减少手动工具的创建成本。在这篇论文中，我们探索了一种基于提示工程和精细调整技术的新型LLM数据竞争检测方法。我们创建了一个专门的数据集名为DRB-ML，它是基于DataRaceBench的数据集，并具有精细的标签，显示数据竞争对的存在、相关变量、行号、读写信息等。然后，我们使用DRB-ML评估了一些代表性的LLM，并对开源LLM进行了精细调整。我们的实验表明，LLM可以成为数据竞争检测的有效方法，但是它们仍无法与传统的数据竞争检测工具相比，提供详细的变量对 causing 数据竞争的信息。
</details></li>
</ul>
<hr>
<h2 id="ST-MLP-A-Cascaded-Spatio-Temporal-Linear-Framework-with-Channel-Independence-Strategy-for-Traffic-Forecasting"><a href="#ST-MLP-A-Cascaded-Spatio-Temporal-Linear-Framework-with-Channel-Independence-Strategy-for-Traffic-Forecasting" class="headerlink" title="ST-MLP: A Cascaded Spatio-Temporal Linear Framework with Channel-Independence Strategy for Traffic Forecasting"></a>ST-MLP: A Cascaded Spatio-Temporal Linear Framework with Channel-Independence Strategy for Traffic Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07496">http://arxiv.org/abs/2308.07496</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zepu Wang, Yuqi Nie, Peng Sun, Nam H. Nguyen, John Mulvey, H. Vincent Poor</li>
<li>For: 本研究旨在提高智能交通系统中的流量管理优化，通过提出一种简洁准确的交通预测模型。* Methods: 本文提出了一种基于多层感知器（MLP）模块和线性层的简洁空间时间图 neural network（STGNN）模型，利用时间信息、空间信息和预定的图结构，并实现了通道独立策略。* Results: 实验结果显示，ST-MLP模型在准确率和计算效率两个方面都有较高的表现，比其他模型和现有的STGNNs架构更高。<details>
<summary>Abstract</summary>
The criticality of prompt and precise traffic forecasting in optimizing traffic flow management in Intelligent Transportation Systems (ITS) has drawn substantial scholarly focus. Spatio-Temporal Graph Neural Networks (STGNNs) have been lauded for their adaptability to road graph structures. Yet, current research on STGNNs architectures often prioritizes complex designs, leading to elevated computational burdens with only minor enhancements in accuracy. To address this issue, we propose ST-MLP, a concise spatio-temporal model solely based on cascaded Multi-Layer Perceptron (MLP) modules and linear layers. Specifically, we incorporate temporal information, spatial information and predefined graph structure with a successful implementation of the channel-independence strategy - an effective technique in time series forecasting. Empirical results demonstrate that ST-MLP outperforms state-of-the-art STGNNs and other models in terms of accuracy and computational efficiency. Our finding encourages further exploration of more concise and effective neural network architectures in the field of traffic forecasting.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "prompt and precise" is translated as "快速和准确" (kuài sù and zhèng jí) to emphasize the importance of timeliness and accuracy in traffic forecasting.* "Spatio-Temporal Graph Neural Networks" is translated as "空间时间图 neural network" (kōng jiān shí jiān tiě xiào) to emphasize the graph structure and the combination of spatial and temporal information.* " Multi-Layer Perceptron" is translated as "多层感知器" (duō cèng kàn shì qì) to emphasize the hierarchical structure of the model.* "channel-independence strategy" is translated as "通道独立策略" (tōng dào dāo lì bàng yì) to emphasize the technique's ability to improve the model's performance without relying on specific channel information.
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Tracking-of-a-Single-Rigid-Body-Character-in-Various-Environments"><a href="#Adaptive-Tracking-of-a-Single-Rigid-Body-Character-in-Various-Environments" class="headerlink" title="Adaptive Tracking of a Single-Rigid-Body Character in Various Environments"></a>Adaptive Tracking of a Single-Rigid-Body Character in Various Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07491">http://arxiv.org/abs/2308.07491</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taesoo Kwon, Taehong Gu, Jaewon Ahn, Yoonsang Lee</li>
<li>for:  This paper proposes a deep reinforcement learning method for simulating full-body human motions in various scenarios, with the goal of adapting to unobserved environmental changes and controller transitions without requiring additional learning.</li>
<li>methods:  The proposed method uses the centroidal dynamics model (CDM) to express the full-body character as a single rigid body (SRB) and trains a policy to track a reference motion using deep reinforcement learning. The SRB simulation is formulated as a quadratic programming (QP) problem, and the policy outputs an action that allows the SRB character to follow the reference motion.</li>
<li>results:  The proposed method is demonstrated to be sample-efficient and able to cope with environments that have not been experienced during learning, such as running on uneven terrain or pushing a box, and transitions between learned policies, without any additional learning. The policy can be efficiently trained within 30 minutes on an ultraportable laptop.Here is the simplified Chinese version of the three key points:</li>
<li>for: 这篇论文提出了一种基于深度学习的人体全身动作模拟方法，以适应不同的环境和控制器转换，无需进行额外的学习。</li>
<li>methods: 该方法使用中心动力学模型（CDM）表示全身人体为单一静体（SRB），并通过深度强化学习训练一个策略来跟踪参照动作。SRB模拟被形式化为quadratic programming（QP）问题，策略输出一个动作，使SRB人体按照参照动作进行动作。</li>
<li>results: 该方法能够快速地在ultraportable笔记本电脑上进行高效地训练，并在不同的环境中进行适应，如跑在不平的地面上或推Push一个箱子，以及控制器转换。<details>
<summary>Abstract</summary>
Since the introduction of DeepMimic [Peng et al. 2018], subsequent research has focused on expanding the repertoire of simulated motions across various scenarios. In this study, we propose an alternative approach for this goal, a deep reinforcement learning method based on the simulation of a single-rigid-body character. Using the centroidal dynamics model (CDM) to express the full-body character as a single rigid body (SRB) and training a policy to track a reference motion, we can obtain a policy that is capable of adapting to various unobserved environmental changes and controller transitions without requiring any additional learning. Due to the reduced dimension of state and action space, the learning process is sample-efficient. The final full-body motion is kinematically generated in a physically plausible way, based on the state of the simulated SRB character. The SRB simulation is formulated as a quadratic programming (QP) problem, and the policy outputs an action that allows the SRB character to follow the reference motion. We demonstrate that our policy, efficiently trained within 30 minutes on an ultraportable laptop, has the ability to cope with environments that have not been experienced during learning, such as running on uneven terrain or pushing a box, and transitions between learned policies, without any additional learning.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese: desde la introducción de DeepMimic [Peng et al. 2018], la investigación subsiguiente se ha centrado en expandir el repertorio de movimientos simulados en diversas escenarios. En este estudio, proponemos un enfoque alternativo para lograr esto, un método de aprendizaje por refuerzo profundo basado en la simulación de un cuerpo rígido único (SRB). Usando el modelo de dinámica centroidal (CDM) para expresar el cuerpo rígido completo como un SRB y entrenar una política para seguir una referencia de movimiento, podemos obtener una política que es capaz de adaptarse a cambios ambientales no observados y transiciones de controlador sin necesidad de aprendizaje adicional. Debido a la reducción de la dimensión del espacio de estado y acción, el proceso de aprendizaje es eficiente en muestras. El movimiento final es generado de manera plausible físicamente, basado en el estado de la simulación del SRB. La simulación del SRB se formulates como un problema de programación cuadrática (QP), y la política devuelve una acción que permite al SRB seguir la referencia de movimiento. Demostramos que nuestra política, entrenada eficientemente dentro de 30 minutos en un portátil ultra, tiene la capacidad de adaptarse a entornos que no se han experimentado durante el aprendizaje, como correr sobre terreno irregular o empujar una caja, y transiciones entre políticas aprendidas, sin aprendizaje adicional.
</details></li>
</ul>
<hr>
<h2 id="O-1-Self-training-with-Oracle-and-1-best-Hypothesis"><a href="#O-1-Self-training-with-Oracle-and-1-best-Hypothesis" class="headerlink" title="O-1: Self-training with Oracle and 1-best Hypothesis"></a>O-1: Self-training with Oracle and 1-best Hypothesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07486">http://arxiv.org/abs/2308.07486</a></li>
<li>repo_url: None</li>
<li>paper_authors: Murali Karthick Baskar, Andrew Rosenberg, Bhuvana Ramabhadran, Kartik Audhkhasi</li>
<li>for: 本研究旨在提出一种新的自教程目标函数O-1，以减少训练偏见和统一训练和评估 метри。</li>
<li>methods: O-1是EMBR的快速变体，可以使用 Both supervised和Unsupervised数据，并且可以提高 oracle假设。</li>
<li>results: 对于SpeechStew数据集和一个大规模的内部数据集，O-1对识别效果有13%-25%的相对提升，与EMBR相比，O-1在不同的SpeechStew数据集上提高了80%的相对幅度，并在内部数据集上与oracle WER之间减少了12%的差距。总的来说，O-1在大规模数据集上实现了9%的相对提升。<details>
<summary>Abstract</summary>
We introduce O-1, a new self-training objective to reduce training bias and unify training and evaluation metrics for speech recognition. O-1 is a faster variant of Expected Minimum Bayes Risk (EMBR), that boosts the oracle hypothesis and can accommodate both supervised and unsupervised data. We demonstrate the effectiveness of our approach in terms of recognition on publicly available SpeechStew datasets and a large-scale, in-house data set. On Speechstew, the O-1 objective closes the gap between the actual and oracle performance by 80\% relative compared to EMBR which bridges the gap by 43\% relative. O-1 achieves 13\% to 25\% relative improvement over EMBR on the various datasets that SpeechStew comprises of, and a 12\% relative gap reduction with respect to the oracle WER over EMBR training on the in-house dataset. Overall, O-1 results in a 9\% relative improvement in WER over EMBR, thereby speaking to the scalability of the proposed objective for large-scale datasets.
</details>
<details>
<summary>摘要</summary>
我们引入O-1，一个新的自我训练目标，以减少训练偏见和统一训练和评估度量 для语音识别。O-1是EMBR的更快版本，可以提高oracle假设和处理bothsupervised和无监督数据。我们透过使用O-1目标，在公开ailable的SpeechStew数据集和大规模内部数据集上显示了效果。在SpeechStew上，O-1目标可以与oracle性能相对比较，将实际性能与oracle性能之间的差距降低了80%相对数据。在SpeechStew的不同数据集上，O-1目标可以与EMBR目标相比，实现13%至25%的相对改善，并且对于内部数据集的oracle WER进行了12%的相对降低。总之，O-1目标对EMBR目标的9%相对改善，说明了这个目标的扩展性。
</details></li>
</ul>
<hr>
<h2 id="OCDaf-Ordered-Causal-Discovery-with-Autoregressive-Flows"><a href="#OCDaf-Ordered-Causal-Discovery-with-Autoregressive-Flows" class="headerlink" title="OCDaf: Ordered Causal Discovery with Autoregressive Flows"></a>OCDaf: Ordered Causal Discovery with Autoregressive Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07480">http://arxiv.org/abs/2308.07480</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vahidzee/ocdaf">https://github.com/vahidzee/ocdaf</a></li>
<li>paper_authors: Hamidreza Kamkari, Vahid Zehtab, Vahid Balazadeh, Rahul G. Krishnan</li>
<li>for: 学习 causal graphs 从观察数据中学习 causal graphs</li>
<li>methods: 使用 order-based 方法，通过 continuous search 算法找到 causal structures</li>
<li>results: 在 Sachs 和 SynTReN benchmark 上达到 state-of-the-art 性能，并在多种 parametic 和 nonparametric synthetic datasets 中验证了identifiability theory 的正确性。<details>
<summary>Abstract</summary>
We propose OCDaf, a novel order-based method for learning causal graphs from observational data. We establish the identifiability of causal graphs within multivariate heteroscedastic noise models, a generalization of additive noise models that allow for non-constant noise variances. Drawing upon the structural similarities between these models and affine autoregressive normalizing flows, we introduce a continuous search algorithm to find causal structures. Our experiments demonstrate state-of-the-art performance across the Sachs and SynTReN benchmarks in Structural Hamming Distance (SHD) and Structural Intervention Distance (SID). Furthermore, we validate our identifiability theory across various parametric and nonparametric synthetic datasets and showcase superior performance compared to existing baselines.
</details>
<details>
<summary>摘要</summary>
我们提出了OCDaf，一种基于顺序的方法，用于从观察数据中学习 causal 图。我们证明了 causal 图在多变量非常性噪声模型中可以uniquely 特征标识，这是常量噪声模型的推广。基于这些模型和 afine autoregressive normalizing flows 的结构相似性，我们引入了连续搜索算法来找到 causal 结构。我们的实验表明在 Sachs 和 SynTReN benchmark 上的状态当前性表现，以及在 Structural Hamming Distance (SHD) 和 Structural Intervention Distance (SID) 中的优秀表现。此外，我们还验证了我们的可 identificability 理论，并在不同的参数和非参数 synthetic 数据上显示了superior的表现，与现有的基准值相比。
</details></li>
</ul>
<hr>
<h2 id="Symphony-Optimized-Model-Serving-using-Centralized-Orchestration"><a href="#Symphony-Optimized-Model-Serving-using-Centralized-Orchestration" class="headerlink" title="Symphony: Optimized Model Serving using Centralized Orchestration"></a>Symphony: Optimized Model Serving using Centralized Orchestration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07470">http://arxiv.org/abs/2308.07470</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lequn Chen, Weixin Deng, Anirudh Canumalla, Yu Xin, Matthai Philipose, Arvind Krishnamurthy</li>
<li>for: 提高深度神经网络（DNN）模型推理的加速率和服务级别目标（SLO）。</li>
<li>methods: 使用中央化调度系统，可以扩展到百万个请求每秒，并将万个GPU进行协调。使用非工作保存的调度算法，可以实现高批处理效率，同时也可以启用灵活自适应扩展。</li>
<li>results: 通过广泛的实验表明，Symphony系统比前一代系统高效性可以达到4.7倍。<details>
<summary>Abstract</summary>
The orchestration of deep neural network (DNN) model inference on GPU clusters presents two significant challenges: achieving high accelerator efficiency given the batching properties of model inference while meeting latency service level objectives (SLOs), and adapting to workload changes both in terms of short-term fluctuations and long-term resource allocation. To address these challenges, we propose Symphony, a centralized scheduling system that can scale to millions of requests per second and coordinate tens of thousands of GPUs. Our system utilizes a non-work-conserving scheduling algorithm capable of achieving high batch efficiency while also enabling robust autoscaling. Additionally, we developed an epoch-scale algorithm that allocates models to sub-clusters based on the compute and memory needs of the models. Through extensive experiments, we demonstrate that Symphony outperforms prior systems by up to 4.7x higher goodput.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）模型推理在GPU集群中的协调表现两大挑战：实现批处理性质下的加速器效率，同时满足响应时间服务水平目标（SLO）。为解决这些挑战，我们提议了Symphony，一个可扩展到百万个请求每秒的中央调度系统，可以协调数万个GPU。我们的系统使用非工作保持式调度算法，可以实现高批处理效率，同时也允许自动扩缩。此外，我们还开发了一种时间尺度算法，将模型分配到子集群基于计算和内存需求。通过广泛的实验，我们证明Symphony比先前系统高效性更高，最高可以达4.7倍。
</details></li>
</ul>
<hr>
<h2 id="Omega-Regular-Reward-Machines"><a href="#Omega-Regular-Reward-Machines" class="headerlink" title="Omega-Regular Reward Machines"></a>Omega-Regular Reward Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07469">http://arxiv.org/abs/2308.07469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ernst Moritz Hahn, Mateo Perez, Sven Schewe, Fabio Somenzi, Ashutosh Trivedi, Dominik Wojtczak</li>
<li>for: 这篇论文旨在探讨如何透过奖励机制来训练智能代理人（Agent）来完成任务，但是设计合适的奖励机制是训练成功的关键。</li>
<li>methods: 这篇论文使用了奖励机制的两种形式：奖励机器和欧姆regular语言，以表达不同类型的学习目标。</li>
<li>results: 论文提出了奖励机器和欧姆regular语言的 интеграción，以实现更加表达力和有效的奖励机制，并提出了一种基于模型自由学习算法的ε-优化策略来对奖励机器进行计算。通过实验，证明了提出的方法的有效性。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) is a powerful approach for training agents to perform tasks, but designing an appropriate reward mechanism is critical to its success. However, in many cases, the complexity of the learning objectives goes beyond the capabilities of the Markovian assumption, necessitating a more sophisticated reward mechanism. Reward machines and omega-regular languages are two formalisms used to express non-Markovian rewards for quantitative and qualitative objectives, respectively. This paper introduces omega-regular reward machines, which integrate reward machines with omega-regular languages to enable an expressive and effective reward mechanism for RL. We present a model-free RL algorithm to compute epsilon-optimal strategies against omega-egular reward machines and evaluate the effectiveness of the proposed algorithm through experiments.
</details>
<details>
<summary>摘要</summary>
《强化学习（RL）是一种强大的方法，用于训练代理人员执行任务，但设计合适的奖励机制是RL的成功关键。然而，在许多情况下，学习目标的复杂性超出了Markov预测的能力，需要更加复杂的奖励机制。奖励机器和ωRegular语言是两种用于表达非Markov奖励的形式主义，分别用于量化和质量目标。本文提出了ωRegular奖励机器，它将奖励机器与ωRegular语言集成，以实现RL中的表达和效果的奖励机制。我们提出了一种无模型RL算法，用于计算ε优策略对ωRegular奖励机器，并通过实验评估提出的算法的效果。》Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="There-Is-a-Digital-Art-History"><a href="#There-Is-a-Digital-Art-History" class="headerlink" title="There Is a Digital Art History"></a>There Is a Digital Art History</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07464">http://arxiv.org/abs/2308.07464</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Gracetyty/art-gallery">https://github.com/Gracetyty/art-gallery</a></li>
<li>paper_authors: Leonardo Impett, Fabian Offert</li>
<li>for: 本文重新评问 Johanna Drucker 十年前的问题：是否有一种数字艺术历史？在大规模变换器基础模型的出现下，传统类型的神经网络已经成为数字艺术历史的一部分，但是这些模型的知识价值和方法价值尚未得到系统的分析。</li>
<li>methods: 本文主要分析了两个方面：一是大规模视模型中新编码的视觉文化，对数字艺术历史有很大的影响；二是使用当今大规模视模型研究艺术史和城市规划等领域的技术案例，提出了一种新的批判方法，该方法考虑模型和其应用之间的知识杂糅。</li>
<li>results: 本文的结果表明，大规模视模型在艺术史和城市规划等领域的应用需要一种新的批判方法，该方法可以通过读取研究数据集和训练数据集的视觉意识来批判模型和其应用的知识杂糅。<details>
<summary>Abstract</summary>
In this paper, we revisit Johanna Drucker's question, "Is there a digital art history?" -- posed exactly a decade ago -- in the light of the emergence of large-scale, transformer-based vision models. While more traditional types of neural networks have long been part of digital art history, and digital humanities projects have recently begun to use transformer models, their epistemic implications and methodological affordances have not yet been systematically analyzed. We focus our analysis on two main aspects that, together, seem to suggest a coming paradigm shift towards a "digital" art history in Drucker's sense. On the one hand, the visual-cultural repertoire newly encoded in large-scale vision models has an outsized effect on digital art history. The inclusion of significant numbers of non-photographic images allows for the extraction and automation of different forms of visual logics. Large-scale vision models have "seen" large parts of the Western visual canon mediated by Net visual culture, and they continuously solidify and concretize this canon through their already widespread application in all aspects of digital life. On the other hand, based on two technical case studies of utilizing a contemporary large-scale visual model to investigate basic questions from the fields of art history and urbanism, we suggest that such systems require a new critical methodology that takes into account the epistemic entanglement of a model and its applications. This new methodology reads its corpora through a neural model's training data, and vice versa: the visual ideologies of research datasets and training datasets become entangled.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们重新探讨了 Johanna Drucker 提出的问题：“是否有数字艺术历史？” ——  exactly a decade ago —— 在大规模变换器基础模型的出现下。 although more traditional types of neural networks have long been part of digital art history, and digital humanities projects have recently begun to use transformer models, their epistemic implications and methodological affordances have not yet been systematically analyzed. We focus our analysis on two main aspects that, together, seem to suggest a coming paradigm shift towards a "digital" art history in Drucker's sense. On the one hand, the visual-cultural repertoire newly encoded in large-scale vision models has an outsized effect on digital art history. The inclusion of significant numbers of non-photographic images allows for the extraction and automation of different forms of visual logics. Large-scale vision models have "seen" large parts of the Western visual canon mediated by Net visual culture, and they continuously solidify and concretize this canon through their already widespread application in all aspects of digital life. On the other hand, based on two technical case studies of utilizing a contemporary large-scale visual model to investigate basic questions from the fields of art history and urbanism, we suggest that such systems require a new critical methodology that takes into account the epistemic entanglement of a model and its applications. This new methodology reads its corpora through a neural model's training data, and vice versa: the visual ideologies of research datasets and training datasets become entangled.Here's a word-for-word translation of the text into Simplified Chinese:在这篇论文中，我们重新探讨了 Johanna Drucker 提出的问题：“是否有数字艺术历史？” —— exact 10 年前 —— 在大规模变换器基础模型的出现下。 although more traditional types of neural networks have long been part of digital art history, and digital humanities projects have recently begun to use transformer models, their epistemic implications and methodological affordances have not yet been systematically analyzed. We focus our analysis on two main aspects that, together, seem to suggest a coming paradigm shift towards a "digital" art history in Drucker's sense. On the one hand, the visual-cultural repertoire newly encoded in large-scale vision models has an outsized effect on digital art history. The inclusion of significant numbers of non-photographic images allows for the extraction and automation of different forms of visual logics. Large-scale vision models have "seen" large parts of the Western visual canon mediated by Net visual culture, and they continuously solidify and concretize this canon through their already widespread application in all aspects of digital life. On the other hand, based on two technical case studies of utilizing a contemporary large-scale visual model to investigate basic questions from the fields of art history and urbanism, we suggest that such systems require a new critical methodology that takes into account the epistemic entanglement of a model and its applications. This new methodology reads its corpora through a neural model's training data, and vice versa: the visual ideologies of research datasets and training datasets become entangled.
</details></li>
</ul>
<hr>
<h2 id="Inductive-Knowledge-Graph-Completion-with-GNNs-and-Rules-An-Analysis"><a href="#Inductive-Knowledge-Graph-Completion-with-GNNs-and-Rules-An-Analysis" class="headerlink" title="Inductive Knowledge Graph Completion with GNNs and Rules: An Analysis"></a>Inductive Knowledge Graph Completion with GNNs and Rules: An Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07942">http://arxiv.org/abs/2308.07942</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anilakash/indkgc">https://github.com/anilakash/indkgc</a></li>
<li>paper_authors: Akash Anil, Víctor Gutiérrez-Basulto, Yazmín Ibañéz-García, Steven Schockaert</li>
<li>for: 本研究旨在解释 inductive knowledge graph completion 任务中，模型如何学习推理规则，并用于预测测试图库中的链接。</li>
<li>methods: 本研究使用了规则基于的方法，并研究了一些变种来解决具体的问题。</li>
<li>results: 研究发现，变种方法可以减少不可靠的实体的影响，并且可以保持 interpretability 优势。而且，一种变种方法，可以不断地利用整个知识图，并且一直高于 NBFNet 的性能。<details>
<summary>Abstract</summary>
The task of inductive knowledge graph completion requires models to learn inference patterns from a training graph, which can then be used to make predictions on a disjoint test graph. Rule-based methods seem like a natural fit for this task, but in practice they significantly underperform state-of-the-art methods based on Graph Neural Networks (GNNs), such as NBFNet. We hypothesise that the underperformance of rule-based methods is due to two factors: (i) implausible entities are not ranked at all and (ii) only the most informative path is taken into account when determining the confidence in a given link prediction answer. To analyse the impact of these factors, we study a number of variants of a rule-based approach, which are specifically aimed at addressing the aforementioned issues. We find that the resulting models can achieve a performance which is close to that of NBFNet. Crucially, the considered variants only use a small fraction of the evidence that NBFNet relies on, which means that they largely keep the interpretability advantage of rule-based methods. Moreover, we show that a further variant, which does look at the full KG, consistently outperforms NBFNet.
</details>
<details>
<summary>摘要</summary>
任务是完成 inductive 知识图结构需要模型学习从训练图中推导出推理模式，以便在测试图上进行预测。 规则基本方法似乎适合这个任务，但在实践中它们在比基于图神经网络（GNNS）的方法下表现出现下降。我们认为这是因为两个因素：（i）不可能的实体没有被排序，（ii）只考虑最有用的路径来确定链接预测答案的信任度。为了分析这些因素的影响，我们研究了一些 variants of rule-based approach，它们专门解决这些问题。我们发现这些模型可以达到与 NBFNet 相似的性能，而且它们只使用了一小部分的证据，这意味着它们保留了规则基本方法的解释性优势。此外，我们还证明了一个 variant，它查看了整个知识图，可以一直高于 NBFNet 的性能。
</details></li>
</ul>
<hr>
<h2 id="GRU-D-Weibull-A-Novel-Real-Time-Individualized-Endpoint-Prediction"><a href="#GRU-D-Weibull-A-Novel-Real-Time-Individualized-Endpoint-Prediction" class="headerlink" title="GRU-D-Weibull: A Novel Real-Time Individualized Endpoint Prediction"></a>GRU-D-Weibull: A Novel Real-Time Individualized Endpoint Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07452">http://arxiv.org/abs/2308.07452</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoyang Ruan, Liwei Wang, Charat Thongprayoon, Wisit Cheungpasitporn, Hongfang Liu</li>
<li>for: 这个研究的目的是提出一种新的方法，GRU-D-Weibull，用于模型Weibull分布，以实现个人化终点预测和人口水平风险管理。</li>
<li>methods: 这个方法使用了门控Recurrent Unit（GRU）和衰减（D）来模型Weibull分布，并实现了实时个人化终点预测和人口水平风险管理。</li>
<li>results: 使用了6879名CKD4阶段4患者的 cohort，我们评估了GRU-D-Weibull在终点预测中的表现。GRU-D-Weibull在终点预测中的C-指数在指定日期为<del>0.7，而后续4.3年的跟踪中为</del>0.77，与随机生存树相当。我们的方法实现了终点预测中的绝对L1损失为<del>1.1年（SD 0.95），并在4年的跟踪中达到最低值为</del>0.45年（SD0.3），与其他方法相比显著出众。GRU-D-Weibull consistently constrained the predicted survival probability within a smaller and more fixed range compared to other models throughout the follow-up period.<details>
<summary>Abstract</summary>
Accurate prediction models for individual-level endpoints and time-to-endpoints are crucial in clinical practice. In this study, we propose a novel approach, GRU-D-Weibull, which combines gated recurrent units with decay (GRU-D) to model the Weibull distribution. Our method enables real-time individualized endpoint prediction and population-level risk management. Using a cohort of 6,879 patients with stage 4 chronic kidney disease (CKD4), we evaluated the performance of GRU-D-Weibull in endpoint prediction. The C-index of GRU-D-Weibull was ~0.7 at the index date and increased to ~0.77 after 4.3 years of follow-up, similar to random survival forest. Our approach achieved an absolute L1-loss of ~1.1 years (SD 0.95) at the CKD4 index date and a minimum of ~0.45 years (SD0.3) at 4 years of follow-up, outperforming competing methods significantly. GRU-D-Weibull consistently constrained the predicted survival probability at the time of an event within a smaller and more fixed range compared to other models throughout the follow-up period. We observed significant correlations between the error in point estimates and missing proportions of input features at the index date (correlations from ~0.1 to ~0.3), which diminished within 1 year as more data became available. By post-training recalibration, we successfully aligned the predicted and observed survival probabilities across multiple prediction horizons at different time points during follow-up. Our findings demonstrate the considerable potential of GRU-D-Weibull as the next-generation architecture for endpoint risk management, capable of generating various endpoint estimates for real-time monitoring using clinical data.
</details>
<details>
<summary>摘要</summary>
准确的预测模型对个体级终点和时间到终点是临床实践中非常重要。在本研究中，我们提出了一种新的方法，即GRU-D-Weibull，它将闭包隐藏单元（GRU-D）与减少（Decay）结合以模型Weibull分布。我们的方法可以实现实时个体化终点预测和人口级风险管理。使用6,879名CKD4阶段4慢性肾病患者的 cohort，我们评估了GRU-D-Weibull在终点预测中的性能。GRU-D-Weibull的C指数在指定日期为 approximately 0.7，并在4.3年后跟踪 period 后提高到approximately 0.77，与随机生存森林相似。我们的方法在终点预测中实现了约1.1年的绝对L1损失（SD 0.95），并在4年后的跟踪期间保持在约0.45年（SD0.3）以上，与其他方法相比显著超越。GRU-D-Weibull在跟踪期间一直压制了预测生存概率的误差，并在不同的跟踪时间点保持在更小和固定的范围内表现出色。我们发现在指定日期的输入特征损失率和缺失比例之间存在显著相关性（相关性从approximately 0.1到approximately 0.3），这种相关性随着更多数据的获得而逐渐减少。通过后期重新训练，我们成功地将预测和观测生存概率Alignment在不同的预测时间点和跟踪时间点。我们的发现表明GRU-D-Weibull可以作为下一代结构，用于实时终点风险管理，可以通过临床数据生成多个终点预测。
</details></li>
</ul>
<hr>
<h2 id="Open-set-Face-Recognition-using-Ensembles-trained-on-Clustered-Data"><a href="#Open-set-Face-Recognition-using-Ensembles-trained-on-Clustered-Data" class="headerlink" title="Open-set Face Recognition using Ensembles trained on Clustered Data"></a>Open-set Face Recognition using Ensembles trained on Clustered Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07445">http://arxiv.org/abs/2308.07445</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rafael Henrique Vareto, William Robson Schwartz</li>
<li>for: 开放集面识别场景下，Unknown人物会在测试阶段出现，需要精准识别个人并有效地处理不熟悉的面孔。这篇论文描述了一种可扩展的开放集面识别方法，用于千计多个人的 галерее。</li>
<li>methods: 方法包括聚类和一个ensemble of binary learning algorithms，用于确定查询面孔样本是否属于face gallery，并且正确地确定它们的身份。</li>
<li>results: 实验表明，即使targeting scalability，也可以达到竞争性的性能。<details>
<summary>Abstract</summary>
Open-set face recognition describes a scenario where unknown subjects, unseen during the training stage, appear on test time. Not only it requires methods that accurately identify individuals of interest, but also demands approaches that effectively deal with unfamiliar faces. This work details a scalable open-set face identification approach to galleries composed of hundreds and thousands of subjects. It is composed of clustering and an ensemble of binary learning algorithms that estimates when query face samples belong to the face gallery and then retrieves their correct identity. The approach selects the most suitable gallery subjects and uses the ensemble to improve prediction performance. We carry out experiments on well-known LFW and YTF benchmarks. Results show that competitive performance can be achieved even when targeting scalability.
</details>
<details>
<summary>摘要</summary>
开放集 face recognition 描述一种情况，在训练阶段未看到的未知人脸在测试阶段出现。不仅需要准确地识别关心人脸，还需要采用有效地处理不熟悉的人脸方法。这篇文章介绍了一种可扩展的开放集face标识方法，用于数百或千个主题的图库。它包括 clustering 和一个 ensemble of binary learning algorithms，可以确定测试人脸样本是否属于图库，并且可以 accurately retrieve their correct identity。该方法选择了最适合的图库主题，并使用 ensemble 进行改进预测性能。我们在well-known LFW 和 YTF  benchmark上进行了实验，结果显示，可以达到竞争性的性能，即使targeting scalability。
</details></li>
</ul>
<hr>
<h2 id="Physics-Informed-Deep-Learning-to-Reduce-the-Bias-in-Joint-Prediction-of-Nitrogen-Oxides"><a href="#Physics-Informed-Deep-Learning-to-Reduce-the-Bias-in-Joint-Prediction-of-Nitrogen-Oxides" class="headerlink" title="Physics-Informed Deep Learning to Reduce the Bias in Joint Prediction of Nitrogen Oxides"></a>Physics-Informed Deep Learning to Reduce the Bias in Joint Prediction of Nitrogen Oxides</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07441">http://arxiv.org/abs/2308.07441</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lianfa Li, Roxana Khalili, Frederick Lurmann, Nathan Pavlovic, Jun Wu, Yan Xu, Yisi Liu, Karl O’Sharkey, Beate Ritz, Luke Oman, Meredith Franklin, Theresa Bastain, Shohreh F. Farzan, Carrie Breton, Rima Habre</li>
<li>for: 这个论文主要是为了提高地面氮氧化物（NOx）的预测，以便更好地了解它们对健康和环境的影响。</li>
<li>methods: 这篇论文使用机器学习（ML）方法，但是这些方法缺乏物理和化学知识，因此可能会产生高度估计偏差。作者们提出了一种physics-informed deep learning框架，该框架可以编码扩散-扩散机制和流体动力约束，以提高NO2和NOx预测的准确性。</li>
<li>results: 作者们发现，该框架可以减少ML模型的估计偏差，并且可以提供精确的空气质量推算和明确的不确定性评估。此外，该框架还可以捕捉NO2和NOx的细致传输，并提供了可靠的空间抽象。<details>
<summary>Abstract</summary>
Atmospheric nitrogen oxides (NOx) primarily from fuel combustion have recognized acute and chronic health and environmental effects. Machine learning (ML) methods have significantly enhanced our capacity to predict NOx concentrations at ground-level with high spatiotemporal resolution but may suffer from high estimation bias since they lack physical and chemical knowledge about air pollution dynamics. Chemical transport models (CTMs) leverage this knowledge; however, accurate predictions of ground-level concentrations typically necessitate extensive post-calibration. Here, we present a physics-informed deep learning framework that encodes advection-diffusion mechanisms and fluid dynamics constraints to jointly predict NO2 and NOx and reduce ML model bias by 21-42%. Our approach captures fine-scale transport of NO2 and NOx, generates robust spatial extrapolation, and provides explicit uncertainty estimation. The framework fuses knowledge-driven physicochemical principles of CTMs with the predictive power of ML for air quality exposure, health, and policy applications. Our approach offers significant improvements over purely data-driven ML methods and has unprecedented bias reduction in joint NO2 and NOx prediction.
</details>
<details>
<summary>摘要</summary>
燃烧产生的大气氮氧化物（NOx）已被认定为有急性和长期健康和环境影响。机器学习（ML）方法已经大幅提高我们预测地面NOx浓度的能力，但这些方法可能会受到高估偏差因为没有物理和化学知识关于空气污染动力学。化学运输模型（CTM）利用这些知识，但精确预测地面浓度通常需要广泛的后调整。在这里，我们介绍一个具有物理知识的深度学习框架，该框架编码了扩散运输机制和流体动力学约束，以预测NO2和NOx的 JOINT 预测和减少ML模型偏差21-42%。我们的方法能够捕捉精确的NO2和NOx的细胞运输，生成坚固的空间推导，并提供明确的 uncertainty 估计。这个框架融合了物理化学知识驱动的CTMs和ML的预测力，实现了空气质量露地暴露、健康和政策应用中的融合。我们的方法具有与对纯数据驱动ML方法的比较，无 precedent 的偏差减少在 JOINT NO2和NOx 预测中。
</details></li>
</ul>
<hr>
<h2 id="Interaction-Aware-Personalized-Vehicle-Trajectory-Prediction-Using-Temporal-Graph-Neural-Networks"><a href="#Interaction-Aware-Personalized-Vehicle-Trajectory-Prediction-Using-Temporal-Graph-Neural-Networks" class="headerlink" title="Interaction-Aware Personalized Vehicle Trajectory Prediction Using Temporal Graph Neural Networks"></a>Interaction-Aware Personalized Vehicle Trajectory Prediction Using Temporal Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07439">http://arxiv.org/abs/2308.07439</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amr Abdelraouf, Rohit Gupta, Kyungtae Han</li>
<li>for: 预测汽车轨迹的精度是自动驾驶系统和高级驾驶助手系统中的关键。现有方法主要依靠大规模的数据集来生成通用的轨迹预测，忽略了每位司机的个性驾驶模式。</li>
<li>methods: 我们提出了一种基于交互aware的个性化轨迹预测方法，该方法利用temporal graph neural networks（GCN）和Long Short-Term Memory（LSTM）模型了target vehicles和它们周围的交通之间的空间-时间交互。为了个性化预测，我们设立了一个管道，该管道通过转移学习来使模型在大规模轨迹数据集上进行初始化预training，然后在每位司机的具体驾驶数据上进行细化调整。</li>
<li>results: 我们的个性化GCN-LSTM模型在较长的预测时间范围内表现出优于其通用版本，并且与没有预training的个体模型相比，具有更高的预测精度。此外，我们的个性化模型还能够避免过拟合现象，强调了大规模数据集的预training对个性化预测的重要性。通过个性化，我们的方法提高了轨迹预测精度。<details>
<summary>Abstract</summary>
Accurate prediction of vehicle trajectories is vital for advanced driver assistance systems and autonomous vehicles. Existing methods mainly rely on generic trajectory predictions derived from large datasets, overlooking the personalized driving patterns of individual drivers. To address this gap, we propose an approach for interaction-aware personalized vehicle trajectory prediction that incorporates temporal graph neural networks. Our method utilizes Graph Convolution Networks (GCN) and Long Short-Term Memory (LSTM) to model the spatio-temporal interactions between target vehicles and their surrounding traffic. To personalize the predictions, we establish a pipeline that leverages transfer learning: the model is initially pre-trained on a large-scale trajectory dataset and then fine-tuned for each driver using their specific driving data. We employ human-in-the-loop simulation to collect personalized naturalistic driving trajectories and corresponding surrounding vehicle trajectories. Experimental results demonstrate the superior performance of our personalized GCN-LSTM model, particularly for longer prediction horizons, compared to its generic counterpart. Moreover, the personalized model outperforms individual models created without pre-training, emphasizing the significance of pre-training on a large dataset to avoid overfitting. By incorporating personalization, our approach enhances trajectory prediction accuracy.
</details>
<details>
<summary>摘要</summary>
提高汽车轨迹预测精度是当前驱动助手系统和自动驱动技术的关键。现有方法主要依靠大规模数据集上的通用轨迹预测，忽略了个人驾驶模式的特殊性。为了解决这个差距，我们提出了一种依赖于互动的个性化汽车轨迹预测方法，该方法利用图гра拓扑神经网络（GCN）和长短期记忆（LSTM）模型了目标汽车和它们周围交通的空间时间互动。为了个性化预测，我们建立了一个管道，其中模型首先在大规模轨迹数据集上进行预训练，然后对每名司机进行细化调整，使用具有特定驾驶模式的自适应驾驶数据。我们使用人工在车Loop模拟器收集个性化自然驾驶轨迹和相应的围绕汽车轨迹。实验结果表明我们的个性化GCN-LSTM模型在较长预测时间 horizons 上表现出色，特别是比其通用对应模型更高。此外，个性化模型也比不进行预训练的模型（无预训练）表现更好，这说明了预训练大数据集可以避免过拟合。通过包含个性化，我们的方法可以提高轨迹预测精度。
</details></li>
</ul>
<hr>
<h2 id="A-Hybrid-Deep-Spatio-Temporal-Attention-Based-Model-for-Parkinson’s-Disease-Diagnosis-Using-Resting-State-EEG-Signals"><a href="#A-Hybrid-Deep-Spatio-Temporal-Attention-Based-Model-for-Parkinson’s-Disease-Diagnosis-Using-Resting-State-EEG-Signals" class="headerlink" title="A Hybrid Deep Spatio-Temporal Attention-Based Model for Parkinson’s Disease Diagnosis Using Resting State EEG Signals"></a>A Hybrid Deep Spatio-Temporal Attention-Based Model for Parkinson’s Disease Diagnosis Using Resting State EEG Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07436">http://arxiv.org/abs/2308.07436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niloufar Delfan, Mohammadreza Shahsavari, Sadiq Hussain, Robertas Damaševičius, U. Rajendra Acharya</li>
<li>for: 这个研究的目的是为了开发一个自动化的 Parkinson’s disease 诊断模型，使用休息状态 EEG 信号。</li>
<li>methods: 这个模型使用了一种混合模型，包括卷积神经网络 (CNN)、双向关键缓冲网络 (Bi-GRU) 和注意机制。</li>
<li>results: 研究结果显示，提案的模型可以高度准确地诊断 Parkinson’s disease，并且在不同测试数据上也能够获得高性能。此外，模型还能够对部分输入信息的损失具有耐性。<details>
<summary>Abstract</summary>
Parkinson's disease (PD), a severe and progressive neurological illness, affects millions of individuals worldwide. For effective treatment and management of PD, an accurate and early diagnosis is crucial. This study presents a deep learning-based model for the diagnosis of PD using resting state electroencephalogram (EEG) signal. The objective of the study is to develop an automated model that can extract complex hidden nonlinear features from EEG and demonstrate its generalizability on unseen data. The model is designed using a hybrid model, consists of convolutional neural network (CNN), bidirectional gated recurrent unit (Bi-GRU), and attention mechanism. The proposed method is evaluated on three public datasets (Uc San Diego Dataset, PRED-CT, and University of Iowa (UI) dataset), with one dataset used for training and the other two for evaluation. The results show that the proposed model can accurately diagnose PD with high performance on both the training and hold-out datasets. The model also performs well even when some part of the input information is missing. The results of this work have significant implications for patient treatment and for ongoing investigations into the early detection of Parkinson's disease. The suggested model holds promise as a non-invasive and reliable technique for PD early detection utilizing resting state EEG.
</details>
<details>
<summary>摘要</summary>
Parkinson's disease（PD）是一种严重和进行的神经疾病，影响全球数百万人。为了有效地治疗和管理PD，准确早期诊断是非常重要。本研究提出了基于深度学习的PD诊断模型，使用休息态电энцефаogram（EEG）信号。研究的目标是开发一个自动化的模型，可以从EEG中提取复杂隐藏的非线性特征，并在未看过数据上进行普适性评估。模型采用混合模型，包括卷积神经网络（CNN）、双向闭包Recurrent Unit（Bi-GRU）和注意机制。本研究在三个公共数据集（UC San Diego数据集、PRED-CT数据集和University of Iowa（UI）数据集）进行评估，其中一个数据集用于训练，另外两个数据集用于评估。结果表明，提议的模型可以准确地诊断PD，并且在训练和剩下数据集上都有高性能。此外，模型还能够在一部分输入信息缺失时保持良好的性能。本研究结果对患者治疗和PD早期检测的研究有着重要意义。建议的模型具有非侵入性和可靠性，可能成为PD早期诊断的非侵入性和可靠的技术。
</details></li>
</ul>
<hr>
<h2 id="Addressing-Distribution-Shift-in-RTB-Markets-via-Exponential-Tilting"><a href="#Addressing-Distribution-Shift-in-RTB-Markets-via-Exponential-Tilting" class="headerlink" title="Addressing Distribution Shift in RTB Markets via Exponential Tilting"></a>Addressing Distribution Shift in RTB Markets via Exponential Tilting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07424">http://arxiv.org/abs/2308.07424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minji Kim, Seong Jin Lee, Bumsik Kim</li>
<li>for: This paper aims to address the issue of distribution shift in machine learning models, specifically in the context of Real-Time Bidding (RTB) market models.</li>
<li>methods: The proposed method is called Exponential Tilt Reweighting Alignment (ExTRA), which uses importance weights to minimize the KL divergence between the weighted source and target datasets. The ExTRA method can operate using labeled source data and unlabeled target data.</li>
<li>results: The paper evaluates the effectiveness of the ExTRA method through simulated real-world data, demonstrating its ability to address distribution shift and improve the performance of machine learning models.<details>
<summary>Abstract</summary>
Distribution shift in machine learning models can be a primary cause of performance degradation. This paper delves into the characteristics of these shifts, primarily motivated by Real-Time Bidding (RTB) market models. We emphasize the challenges posed by class imbalance and sample selection bias, both potent instigators of distribution shifts. This paper introduces the Exponential Tilt Reweighting Alignment (ExTRA) algorithm, as proposed by Marty et al. (2023), to address distribution shifts in data. The ExTRA method is designed to determine the importance weights on the source data, aiming to minimize the KL divergence between the weighted source and target datasets. A notable advantage of this method is its ability to operate using labeled source data and unlabeled target data. Through simulated real-world data, we investigate the nature of distribution shift and evaluate the applicacy of the proposed model.
</details>
<details>
<summary>摘要</summary>
Distribution shift in machine learning models can be a primary cause of performance degradation. This paper explores the characteristics of these shifts, primarily motivated by Real-Time Bidding (RTB) market models. We highlight the challenges posed by class imbalance and sample selection bias, both powerful instigators of distribution shifts. This paper introduces the Exponential Tilt Reweighting Alignment (ExTRA) algorithm, as proposed by Marty et al. (2023), to address distribution shifts in data. The ExTRA method determines the importance weights on the source data to minimize the KL divergence between the weighted source and target datasets. A notable advantage of this method is its ability to operate using labeled source data and unlabeled target data. Through simulated real-world data, we investigate the nature of distribution shift and evaluate the applicability of the proposed model.Here's the word-for-word translation:分布shift在机器学习模型中可以是表现下降的主要原因。这篇论文探讨分布shift的特点，主要受Real-Time Bidding（RTB）市场模型的激发。我们强调分布shift的挑战，包括分类偏度和样本选择偏见，这两者都是分布shift的强力引起者。这篇论文介绍Marty等人（2023）提出的扩展tilt重要性补做算法（ExTRA），用于Addressing分布shift in data。ExTRA方法通过确定源数据中的重要性Weight来减少源数据和目标数据之间的KL偏度。这种方法的一个优点是它可以使用标注的源数据和无标注的目标数据进行操作。通过实际的世界数据 simulate，我们investigate分布shift的本质和提出方法的适用性。
</details></li>
</ul>
<hr>
<h2 id="U-Turn-Diffusion"><a href="#U-Turn-Diffusion" class="headerlink" title="U-Turn Diffusion"></a>U-Turn Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07421">http://arxiv.org/abs/2308.07421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamidreza Behjoo, Michael Chertkov<br>for:这种Diffusion模型是用于生成人工图像的。methods:这些模型基于动态助手时间机制，其中Score函数来自输入图像。results:我们的研究发现了评估Diffusion模型效果的标准：生成过程中快速谱相关性的能力直接影响生成图像质量。此外，我们还提出了“U-Turn扩散”技术，该技术通过组合前向、U-turn和后向过程，生成一个接近独立同分布（i.i.d）样本。<details>
<summary>Abstract</summary>
We present a comprehensive examination of score-based diffusion models of AI for generating synthetic images. These models hinge upon a dynamic auxiliary time mechanism driven by stochastic differential equations, wherein the score function is acquired from input images. Our investigation unveils a criterion for evaluating efficiency of the score-based diffusion models: the power of the generative process depends on the ability to de-construct fast correlations during the reverse/de-noising phase. To improve the quality of the produced synthetic images, we introduce an approach coined "U-Turn Diffusion". The U-Turn Diffusion technique starts with the standard forward diffusion process, albeit with a condensed duration compared to conventional settings. Subsequently, we execute the standard reverse dynamics, initialized with the concluding configuration from the forward process. This U-Turn Diffusion procedure, combining forward, U-turn, and reverse processes, creates a synthetic image approximating an independent and identically distributed (i.i.d.) sample from the probability distribution implicitly described via input samples. To analyze relevant time scales we employ various analytical tools, including auto-correlation analysis, weighted norm of the score-function analysis, and Kolmogorov-Smirnov Gaussianity test. The tools guide us to establishing that the Kernel Intersection Distance, a metric comparing the quality of synthetic samples with real data samples, is minimized at the optimal U-turn time.
</details>
<details>
<summary>摘要</summary>
我们对基于分数的扩散模型的人工智能生成 sintetic 图像进行了全面的检验。这些模型基于动态辅助时间机制驱动的随机 diffeq 方程，其中分数函数从输入图像中获取。我们的调查发现一个评估分数基于扩散模型的效率的标准：生成过程中快速相关的破坏速率决定了扩散模型的能效性。为了提高生成的 sintetic 图像质量，我们提出了“U-Turn 扩散”技术。U-Turn 扩散技术从标准的前向扩散过程开始，但是与传统设置相比，它具有缩短的时间长度。然后，我们执行标准的反向动力学，初始化为前向过程的结束配置。这种 U-Turn 扩散过程，结合前向、U-turn 和反向过程，创造了一个约束为独立同分布（i.i.d.）随机变量的 sintetic 图像。为了分析相关的时间尺度，我们使用了多种分析工具，包括自相关分析、分数函数的Weighted  нор 分析和kolmogorov-smirnov  Gaussianity 测试。这些工具引导我们确定了最佳 U-turn 时间，使得权重 norm 分数函数的质量最佳。
</details></li>
</ul>
<hr>
<h2 id="Locally-Adaptive-and-Differentiable-Regression"><a href="#Locally-Adaptive-and-Differentiable-Regression" class="headerlink" title="Locally Adaptive and Differentiable Regression"></a>Locally Adaptive and Differentiable Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07418">http://arxiv.org/abs/2308.07418</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingxuan Han, Varun Shankar, Jeff M Phillips, Chenglong Ye</li>
<li>for: 提出了一种基于本地学习模型的全球连续可导模型框架，以寻求处理数据中存在不同密度或函数值规模的问题。</li>
<li>methods: 使用权重加权平均方法将本地学习模型在相应的地方进行连续拟合，以实现全球连续可导模型。</li>
<li>results: 在推理中，该模型可以更快地达到统计准确性，并在各种实际应用中提供了改进的表现。<details>
<summary>Abstract</summary>
Over-parameterized models like deep nets and random forests have become very popular in machine learning. However, the natural goals of continuity and differentiability, common in regression models, are now often ignored in modern overparametrized, locally-adaptive models. We propose a general framework to construct a global continuous and differentiable model based on a weighted average of locally learned models in corresponding local regions. This model is competitive in dealing with data with different densities or scales of function values in different local regions. We demonstrate that when we mix kernel ridge and polynomial regression terms in the local models, and stitch them together continuously, we achieve faster statistical convergence in theory and improved performance in various practical settings.
</details>
<details>
<summary>摘要</summary>
现代机器学习中，过参数化模型如深度网络和随机森林已经非常流行。然而，传统机器学习模型中的稳定性和导数性目标，通常被现代过参数化、地方适应型模型所忽略。我们提出了一种通用框架，用于基于本地学习模型的权重平均构建全球连续和导数可达的模型。这种模型能够在不同的地方域上处理数据的不同浓度或函数值的比例。我们示出，当混合核ridge和多项式回归项在本地模型中，并将其们连续叠加时，我们可以在理论上更快地达到统计征 converge，并在各种实际场景中提高表现。
</details></li>
</ul>
<hr>
<h2 id="Text-Injection-for-Capitalization-and-Turn-Taking-Prediction-in-Speech-Models"><a href="#Text-Injection-for-Capitalization-and-Turn-Taking-Prediction-in-Speech-Models" class="headerlink" title="Text Injection for Capitalization and Turn-Taking Prediction in Speech Models"></a>Text Injection for Capitalization and Turn-Taking Prediction in Speech Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07395">http://arxiv.org/abs/2308.07395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaan Bijwadia, Shuo-yiin Chang, Weiran Wang, Zhong Meng, Hao Zhang, Tara N. Sainath</li>
<li>for: 提高auxiliary任务性能（非ASR任务）</li>
<li>methods: 使用文本注入法（JEIT）对ASR模型进行训练，并同时完成两个auxiliary任务</li>
<li>results: 提高了长尾数据的首字母排序性能，以及提高了对话转接检测的受检率<details>
<summary>Abstract</summary>
Text injection for automatic speech recognition (ASR), wherein unpaired text-only data is used to supplement paired audio-text data, has shown promising improvements for word error rate. This study examines the use of text injection for auxiliary tasks, which are the non-ASR tasks often performed by an E2E model. In this work, we use joint end-to-end and internal language model training (JEIT) as our text injection algorithm to train an ASR model which performs two auxiliary tasks. The first is capitalization, which is a de-normalization task. The second is turn-taking prediction, which attempts to identify whether a user has completed their conversation turn in a digital assistant interaction. We show results demonstrating that our text injection method boosts capitalization performance for long-tail data, and improves turn-taking detection recall.
</details>
<details>
<summary>摘要</summary>
文本注入技术用于自动语音识别（ASR），其中不带文本数据用于补充带有音频文本数据的训练，已经显示出了词错率的明显改善。本研究探讨了文本注入的应用于 auxiliary task，这些任务通常由一个端到端模型完成。在这个工作中，我们使用联合端到端和内部语言模型训练算法（JEIT）来训练一个 ASR 模型，该模型同时完成了两个 auxiliary task。第一个是字母大小Normalization 任务，第二个是对话交换预测任务，它们是在数字助手交互中确定用户是否已经完成了对话转换。我们的文本注入方法可以提高长尾数据的字母大小正确率，并提高对话交换检测精度。
</details></li>
</ul>
<hr>
<h2 id="DISBELIEVE-Distance-Between-Client-Models-is-Very-Essential-for-Effective-Local-Model-Poisoning-Attacks"><a href="#DISBELIEVE-Distance-Between-Client-Models-is-Very-Essential-for-Effective-Local-Model-Poisoning-Attacks" class="headerlink" title="DISBELIEVE: Distance Between Client Models is Very Essential for Effective Local Model Poisoning Attacks"></a>DISBELIEVE: Distance Between Client Models is Very Essential for Effective Local Model Poisoning Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07387">http://arxiv.org/abs/2308.07387</a></li>
<li>repo_url: None</li>
<li>paper_authors: Indu Joshi, Priyank Upadhya, Gaurav Kumar Nayak, Peter Schüffler, Nassir Navab</li>
<li>for: 该研究旨在探讨 Federated Learning 如何解决医疗数据隐私问题，并研究如何防止恶意客户端攻击 Federated 系统。</li>
<li>methods: 该研究提出了一种新的本地模型欺骗攻击（DISBELIEVE），该攻击可以在 Robust Aggregation 方法下降低本地模型的性能，从而影响全局模型的性能。</li>
<li>results: 实验结果表明，DISBELIEVE 攻击可以在三个公共可用的医疗图像集上显著降低 Robust Aggregation 方法的性能，并且在自然图像集上也有较好的效果。<details>
<summary>Abstract</summary>
Federated learning is a promising direction to tackle the privacy issues related to sharing patients' sensitive data. Often, federated systems in the medical image analysis domain assume that the participating local clients are \textit{honest}. Several studies report mechanisms through which a set of malicious clients can be introduced that can poison the federated setup, hampering the performance of the global model. To overcome this, robust aggregation methods have been proposed that defend against those attacks. We observe that most of the state-of-the-art robust aggregation methods are heavily dependent on the distance between the parameters or gradients of malicious clients and benign clients, which makes them prone to local model poisoning attacks when the parameters or gradients of malicious and benign clients are close. Leveraging this, we introduce DISBELIEVE, a local model poisoning attack that creates malicious parameters or gradients such that their distance to benign clients' parameters or gradients is low respectively but at the same time their adverse effect on the global model's performance is high. Experiments on three publicly available medical image datasets demonstrate the efficacy of the proposed DISBELIEVE attack as it significantly lowers the performance of the state-of-the-art \textit{robust aggregation} methods for medical image analysis. Furthermore, compared to state-of-the-art local model poisoning attacks, DISBELIEVE attack is also effective on natural images where we observe a severe drop in classification performance of the global model for multi-class classification on benchmark dataset CIFAR-10.
</details>
<details>
<summary>摘要</summary>
Federated learning 是一个有前途的方向，以解决分享患者敏感数据时的隐私问题。在医疗影像分析领域，联邦系统经常假设参与的本地客户端是诚实的。然而，一些研究表明，可以引入一组恶意客户端，使联邦设置受损，global模型性能下降。为解决这个问题，一些robust汇集方法被提出，以防止这些攻击。我们发现，大多数当前的state-of-the-art robust汇集方法都是依赖本地客户端和良好客户端之间的距离，这使得它们容易受到本地模型毒 poisoning攻击，当本地客户端和良好客户端之间的参数或梯度距离很近时。基于这一点，我们介绍了DISBELIEVE，一种本地模型毒 poisoning攻击，可以创造出谎言的参数或梯度，使其与良好客户端之间的距离很近，但同时对全局模型性能产生严重的影响。我们在三个公共可用的医疗影像数据集上进行了实验，并证明了我们提出的DISBELIEVE攻击的有效性，可以在医疗影像分析领域对当前的robust汇集方法进行重要的攻击。此外，我们还发现，DISBELIEVE攻击也能够在自然图像领域中效果，在CIFAR-10benchmark数据集上，全局模型的多类分类性能下降很严重。
</details></li>
</ul>
<hr>
<h2 id="DiffSED-Sound-Event-Detection-with-Denoising-Diffusion"><a href="#DiffSED-Sound-Event-Detection-with-Denoising-Diffusion" class="headerlink" title="DiffSED: Sound Event Detection with Denoising Diffusion"></a>DiffSED: Sound Event Detection with Denoising Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07293">http://arxiv.org/abs/2308.07293</a></li>
<li>repo_url: None</li>
<li>paper_authors: Swapnil Bhosale, Sauradip Nag, Diptesh Kanojia, Jiankang Deng, Xiatian Zhu</li>
<li>for: 这个论文的目标是提出一种基于生成学习的声音时间边界检测方法，以提高声音事件检测的精度和效率。</li>
<li>methods: 该方法使用一种基于扩散过程的梯度下降算法，通过在适应性Transformer核心网络框架中对含杂的提议进行修复，以将含杂的提议转换为高质量的事件时间边界。</li>
<li>results: 对于Urban-SED和EPIC-Sounds数据集，该方法在训练和测试中均显示出优于现有方法的性能，具有40%以上的更快的训练速度。<details>
<summary>Abstract</summary>
Sound Event Detection (SED) aims to predict the temporal boundaries of all the events of interest and their class labels, given an unconstrained audio sample. Taking either the splitand-classify (i.e., frame-level) strategy or the more principled event-level modeling approach, all existing methods consider the SED problem from the discriminative learning perspective. In this work, we reformulate the SED problem by taking a generative learning perspective. Specifically, we aim to generate sound temporal boundaries from noisy proposals in a denoising diffusion process, conditioned on a target audio sample. During training, our model learns to reverse the noising process by converting noisy latent queries to the groundtruth versions in the elegant Transformer decoder framework. Doing so enables the model generate accurate event boundaries from even noisy queries during inference. Extensive experiments on the Urban-SED and EPIC-Sounds datasets demonstrate that our model significantly outperforms existing alternatives, with 40+% faster convergence in training.
</details>
<details>
<summary>摘要</summary>
声音事件检测（SED）目标是预测各个事件的时间边界和类别标签，给一个未Constrained的音频样本。现有的所有方法都是从推理学教学角度来解决SED问题，包括分割和分类（即帧级）策略或更为原理性的事件级模型。在这项工作中，我们重新定义了SED问题，通过一种生成学学习角度来解决。具体来说，我们目标是通过降噪过程中的批量梯度下降来生成各个事件的时间边界，使用搅拌Transformer推理框架。在训练中，我们的模型学习了将噪音提取过程反转，将噪音潜在提取到真实的样本。这使得我们的模型能够在推理中生成准确的事件边界，即使噪音提取过程不准确。我们在Urban-SED和EPIC-Sounds数据集上进行了广泛的实验，结果表明，我们的模型与现有的方法相比，在训练时间上提高了40%以上。
</details></li>
</ul>
<hr>
<h2 id="The-Devil-is-in-the-Errors-Leveraging-Large-Language-Models-for-Fine-grained-Machine-Translation-Evaluation"><a href="#The-Devil-is-in-the-Errors-Leveraging-Large-Language-Models-for-Fine-grained-Machine-Translation-Evaluation" class="headerlink" title="The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation"></a>The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07286">http://arxiv.org/abs/2308.07286</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André F. T. Martins, Graham Neubig, Ankush Garg, Jonathan H. Clark, Markus Freitag, Orhan Firat</li>
<li>for: 提高机器翻译系统的质量</li>
<li>methods: 利用大语言模型的推理和在场景学习能力，询问模型 identificar和分类翻译错误</li>
<li>results: 与只提示分数prompting比较，AutoMQM可以提高模型的性能，特别是大型模型，并提供可读性通过错误块与人工标注相对应<details>
<summary>Abstract</summary>
Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iterative development of MT systems. While considerable progress has been made on estimating a single scalar quality score, current metrics lack the informativeness of more detailed schemes that annotate individual errors, such as Multidimensional Quality Metrics (MQM). In this paper, we help fill this gap by proposing AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particularly large gains for larger models) while providing interpretability through error spans that align with human annotations.
</details>
<details>
<summary>摘要</summary>
自动评估机器翻译（MT）是一种关键的工具，帮助快速迭代MT系统的发展。虽然已经做出了很大的进步，但当前的 метрикиlack the informativeness of more detailed schemes that annotate individual errors, such as Multidimensional Quality Metrics (MQM). 在这篇论文中，我们帮助填补这一漏洞，提出了AutoMQM，一种提示技术，利用大型自然语言模型（LLMs）的思维和Context learning能力，让它们标识和分类翻译中的错误。我们开始通过对最近的LLMs，如PaLM和PaLM-2，进行简单的分数预测提示，然后研究了abeled data的影响通过 Context learning和 fine-tuning。最后，我们评估了AutoMQM与PaLM-2模型，发现它可以提高性能，特别是对于更大的模型，而且提供了可读性通过错误跨度与人类标注相对应。
</details></li>
</ul>
<hr>
<h2 id="Cross-Attribute-Matrix-Factorization-Model-with-Shared-User-Embedding"><a href="#Cross-Attribute-Matrix-Factorization-Model-with-Shared-User-Embedding" class="headerlink" title="Cross-Attribute Matrix Factorization Model with Shared User Embedding"></a>Cross-Attribute Matrix Factorization Model with Shared User Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07284">http://arxiv.org/abs/2308.07284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wen Liang, Zeng Fan, Youzhi Liang, Jianguo Jia</li>
<li>for: 提高推荐系统的准确率和稳定性，特别是对于“长尾”用户和 Item。</li>
<li>methods: 使用神经网络抽象来捕捉用户-项目交互，同时考虑用户和项目的特性和属性，以解决冷启始问题。</li>
<li>results: 对于 MovieLens 和 Pinterest 数据集，我们的 Cross-Attribute Matrix Factorization 模型在 sparse 数据场景下显示出优于常见方法的性能。<details>
<summary>Abstract</summary>
Over the past few years, deep learning has firmly established its prowess across various domains, including computer vision, speech recognition, and natural language processing. Motivated by its outstanding success, researchers have been directing their efforts towards applying deep learning techniques to recommender systems. Neural collaborative filtering (NCF) and Neural Matrix Factorization (NeuMF) refreshes the traditional inner product in matrix factorization with a neural architecture capable of learning complex and data-driven functions. While these models effectively capture user-item interactions, they overlook the specific attributes of both users and items. This can lead to robustness issues, especially for items and users that belong to the "long tail". Such challenges are commonly recognized in recommender systems as a part of the cold-start problem. A direct and intuitive approach to address this issue is by leveraging the features and attributes of the items and users themselves. In this paper, we introduce a refined NeuMF model that considers not only the interaction between users and items, but also acrossing associated attributes. Moreover, our proposed architecture features a shared user embedding, seamlessly integrating with user embeddings to imporve the robustness and effectively address the cold-start problem. Rigorous experiments on both the Movielens and Pinterest datasets demonstrate the superiority of our Cross-Attribute Matrix Factorization model, particularly in scenarios characterized by higher dataset sparsity.
</details>
<details>
<summary>摘要</summary>
过去几年，深度学习在不同领域，包括计算机视觉、语音识别和自然语言处理等领域，展示了它的卓越。驱动于其成功，研究人员开始尝试应用深度学习技术到推荐系统中。尽管Neural Collaborative Filtering（NCF）和Neural Matrix Factorization（NeuMF）模型能够有效地捕捉用户-项目交互，但它们忽略了用户和项目的特定属性。这可能导致 robustness 问题，特别是对于数据中的"长尾"用户和项目。这种问题在推荐系统中通常被称为冷启始问题。为解决这个问题，我们在这篇论文中引入了一种改进的NeuMF模型，该模型不仅考虑用户和项目之间的交互，还考虑用户和项目之间的属性相互关系。此外，我们的提议的体系还具有共享用户嵌入，可以融合用户嵌入，从而提高系统的稳定性和效果地解决冷启始问题。我们在MovieLens和Pinterest数据集上进行了严格的实验，结果显示我们的 Cross-Attribute Matrix Factorization模型在数据集稀疏程度较高的情况下表现出色。
</details></li>
</ul>
<hr>
<h2 id="Data-Efficient-Energy-Aware-Participant-Selection-for-UAV-Enabled-Federated-Learning"><a href="#Data-Efficient-Energy-Aware-Participant-Selection-for-UAV-Enabled-Federated-Learning" class="headerlink" title="Data-Efficient Energy-Aware Participant Selection for UAV-Enabled Federated Learning"></a>Data-Efficient Energy-Aware Participant Selection for UAV-Enabled Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07273">http://arxiv.org/abs/2308.07273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youssra Cheriguene, Wael Jaafar, Chaker Abdelaziz Kerrache, Halim Yanikomeroglu, Fatima Zohra Bousbaa, Nasreddine Lagraa</li>
<li>for: 本研究旨在提高边缘 federated learning（FL）模型的准确性，通过选择合适的无人机参与者，并且考虑无人机的能源消耗、通信质量和本地数据的不同性。</li>
<li>methods: 本研究提出了一种新的无人机参与者选择策略，即基于数据效率和能源占用率的能源意识参与者选择策略（DEEPS），该策略通过选择每个子区域中最佳的FL参与者，基于本地数据的结构相似度指数平均分数和能源占用资料来实现。</li>
<li>results: 通过实验，本研究表明，对于边缘FL，使用DEEPS策略可以提高模型准确性、减少训练时间和无人机的能源消耗，相比于随机选择策略。<details>
<summary>Abstract</summary>
Unmanned aerial vehicle (UAV)-enabled edge federated learning (FL) has sparked a rise in research interest as a result of the massive and heterogeneous data collected by UAVs, as well as the privacy concerns related to UAV data transmissions to edge servers. However, due to the redundancy of UAV collected data, e.g., imaging data, and non-rigorous FL participant selection, the convergence time of the FL learning process and bias of the FL model may increase. Consequently, we investigate in this paper the problem of selecting UAV participants for edge FL, aiming to improve the FL model's accuracy, under UAV constraints of energy consumption, communication quality, and local datasets' heterogeneity. We propose a novel UAV participant selection scheme, called data-efficient energy-aware participant selection strategy (DEEPS), which consists of selecting the best FL participant in each sub-region based on the structural similarity index measure (SSIM) average score of its local dataset and its power consumption profile. Through experiments, we demonstrate that the proposed selection scheme is superior to the benchmark random selection method, in terms of model accuracy, training time, and UAV energy consumption.
</details>
<details>
<summary>摘要</summary>
“无人航空器（UAV）启动的边缘联合学习（FL）已经引起了研究者们的探索，因为UAV所收集的数据量巨大且多样，同时也存在资料传输到边缘服务器的隐私问题。然而，由于UAV收集的数据存在重复性，例如影像数据，以及不充分的FL参与者选择，FL学习过程的参数调整和模型偏好可能会增加。因此，本文研究UAV参与者选择的问题，以提高FL模型的准确性，并且遵循UAV的能源消耗、通信质量和本地数据的多样性限制。我们提出了一个 novel UAV参与者选择策略，called 数据效率能源注意的参与者选择策略（DEEPS），它是根据每个子区域中的本地数据和能源消耗观察所得到的结构相似度平均分数（SSIM）的平均分数，选择每个子区域中最佳的 FL 参与者。经过实验，我们发现，提案的选择策略与参考随机选择方法相比，在于模型准确性、训练时间和UAV能源消耗方面均有优势。”
</details></li>
</ul>
<hr>
<h2 id="Dialogue-for-Prompting-a-Policy-Gradient-Based-Discrete-Prompt-Optimization-for-Few-shot-Learning"><a href="#Dialogue-for-Prompting-a-Policy-Gradient-Based-Discrete-Prompt-Optimization-for-Few-shot-Learning" class="headerlink" title="Dialogue for Prompting: a Policy-Gradient-Based Discrete Prompt Optimization for Few-shot Learning"></a>Dialogue for Prompting: a Policy-Gradient-Based Discrete Prompt Optimization for Few-shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07272">http://arxiv.org/abs/2308.07272</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengzhengxu Li, Xiaoming Liu, Yichen Wang, Duyi Li, Yu Lan, Chao Shen</li>
<li>For: 提高几何语言处理（NLP）任务中的几何学习效果，以及解决现有的精度优化方法的问题。* Methods: 使用对话Alignment策略生成可读性提示集，并提出高效的提示筛选指标来选择高质量提示。然后，通过policy梯度学习算法来匹配提示和输入。* Results: 在四个开源数据集上，DP_2O方法在几何学习设定下的准确率高于当前最佳方法的1.52%，并且在不同的任务和数据集上都有good的通用性、稳定性和泛化能力。<details>
<summary>Abstract</summary>
Prompt-based pre-trained language models (PLMs) paradigm have succeeded substantially in few-shot natural language processing (NLP) tasks. However, prior discrete prompt optimization methods require expert knowledge to design the base prompt set and identify high-quality prompts, which is costly, inefficient, and subjective. Meanwhile, existing continuous prompt optimization methods improve the performance by learning the ideal prompts through the gradient information of PLMs, whose high computational cost, and low readability and generalizability are often concerning. To address the research gap, we propose a Dialogue-comprised Policy-gradient-based Discrete Prompt Optimization ($DP_2O$) method. We first design a multi-round dialogue alignment strategy for readability prompt set generation based on GPT-4. Furthermore, we propose an efficient prompt screening metric to identify high-quality prompts with linear complexity. Finally, we construct a reinforcement learning (RL) framework based on policy gradients to match the prompts to inputs optimally. By training a policy network with only 0.67% of the PLM parameter size on the tasks in the few-shot setting, $DP_2O$ outperforms the state-of-the-art (SOTA) method by 1.52% in accuracy on average on four open-source datasets. Moreover, subsequent experiments also demonstrate that $DP_2O$ has good universality, robustness, and generalization ability.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="EasyEdit-An-Easy-to-use-Knowledge-Editing-Framework-for-Large-Language-Models"><a href="#EasyEdit-An-Easy-to-use-Knowledge-Editing-Framework-for-Large-Language-Models" class="headerlink" title="EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models"></a>EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07269">http://arxiv.org/abs/2308.07269</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zjunlp/easyedit">https://github.com/zjunlp/easyedit</a></li>
<li>paper_authors: Peng Wang, Ningyu Zhang, Xin Xie, Yunzhi Yao, Bozhong Tian, Mengru Wang, Zekun Xi, Siyuan Cheng, Kangwei Liu, Guozhou Zheng, Huajun Chen</li>
<li>for: 这个论文的目的是提出一个轻松使用的知识编辑框架，以便在大语言模型（LLMs）上应用多种 cutting-edge 知识编辑方法。</li>
<li>methods: 该论文使用了多种知识编辑方法，包括粘贴、替换、剪辑等，以及一些自动生成的方法。</li>
<li>results: 该论文的实验结果表明，使用知识编辑方法可以超过传统的精度调整，并且具有更好的一致性和普适性。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy issues, which means they are unaware of unseen events or generate text with incorrect facts owing to the outdated/noisy data. To this end, many knowledge editing approaches for LLMs have emerged -- aiming to subtly inject/edit updated knowledge or adjust undesired behavior while minimizing the impact on unrelated inputs. Nevertheless, due to significant differences among various knowledge editing methods and the variations in task setups, there is no standard implementation framework available for the community, which hinders practitioners to apply knowledge editing to applications. To address these issues, we propose EasyEdit, an easy-to-use knowledge editing framework for LLMs. It supports various cutting-edge knowledge editing approaches and can be readily apply to many well-known LLMs such as T5, GPT-J, LlaMA, etc. Empirically, we report the knowledge editing results on LlaMA-2 with EasyEdit, demonstrating that knowledge editing surpasses traditional fine-tuning in terms of reliability and generalization. We have released the source code on GitHub at https://github.com/zjunlp/EasyEdit, along with Google Colab tutorials and comprehensive documentation for beginners to get started. Besides, we present an online system for real-time knowledge editing, and a demo video at http://knowlm.zjukg.cn/easyedit.mp4.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）通常会受到知识割裂或错误问题的影响，这意味着它们不知道未看过的事件或生成文本中含有错误的 факти due to outdated/noisy data。为了解决这个问题，许多知识编辑方法 для LLM 已经出现 -- 目的是通过微妙地将更新的知识或不适合的行为进行调整，以最小化对无关输入的影响。然而，由于不同的知识编辑方法和任务设置的差异，现在没有一个标准的实现框架可以供社区使用，这限制了实践者将知识编辑应用于应用程序。为了解决这些问题，我们提出了 EasyEdit，一个易于使用的知识编辑框架 для LLM。它支持许多最新的知识编辑方法，并可以轻松地应用到许多已知的 LLM，如 T5、GPT-J、LlaMA 等。我们在 LlaMA-2 上进行了知识编辑实验，结果显示，知识编辑超过了传统精细调整的可靠性和应用性。我们在 GitHub 上发布了源代码，并提供 Google Colab 教学和详细的文档，以便初学者开始。此外，我们还提供了线上系统 для实时知识编辑，以及一个网页demo video，请参考 http://knowlm.zjukg.cn/easyedit.mp4。
</details></li>
</ul>
<hr>
<h2 id="LCE-An-Augmented-Combination-of-Bagging-and-Boosting-in-Python"><a href="#LCE-An-Augmented-Combination-of-Bagging-and-Boosting-in-Python" class="headerlink" title="LCE: An Augmented Combination of Bagging and Boosting in Python"></a>LCE: An Augmented Combination of Bagging and Boosting in Python</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07250">http://arxiv.org/abs/2308.07250</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/localcascadeensemble/lce">https://github.com/localcascadeensemble/lce</a></li>
<li>paper_authors: Kevin Fauvel, Élisa Fromont, Véronique Masson, Philippe Faverdin, Alexandre Termier</li>
<li>for: 本研究开发了一个高性能、可扩展、易用的Python包lcensemble，用于对 классификация和回归问题进行通用任务。</li>
<li>methods: 本研究使用了Local Cascade Ensemble（LCE）机器学习方法，它将Random Forest和XGBoost两种现状顶峰方法融合，以实现更好的泛化预测器。</li>
<li>results: lcensemble可以与scikit-learn集成，并且可以与scikit-learn的管道和模型选择工具互动。它在处理大规模数据时表现出了高性能。<details>
<summary>Abstract</summary>
lcensemble is a high-performing, scalable and user-friendly Python package for the general tasks of classification and regression. The package implements Local Cascade Ensemble (LCE), a machine learning method that further enhances the prediction performance of the current state-of-the-art methods Random Forest and XGBoost. LCE combines their strengths and adopts a complementary diversification approach to obtain a better generalizing predictor. The package is compatible with scikit-learn, therefore it can interact with scikit-learn pipelines and model selection tools. It is distributed under the Apache 2.0 license, and its source code is available at https://github.com/LocalCascadeEnsemble/LCE.
</details>
<details>
<summary>摘要</summary>
LCensemble 是一个高性能、可扩展、易用的 Python 包，用于执行分类和回归的通用任务。该包实现了本地随机森林 ensemble（LCE）机器学习方法，该方法可以进一步提高当前状态的艺术法和 XGBoost 方法的预测性能。LCE 结合了它们的优点，采用了补做的多样化方法，从而获得一个更好的总体预测器。该包与 scikit-learn 兼容，因此可以与 scikit-learn 管道和模型选择工具进行交互。它根据 Apache 2.0 license 分发，源代码可以在 https://github.com/LocalCascadeEnsemble/LCE 上 obtener。
</details></li>
</ul>
<hr>
<h2 id="Can-we-Agree-On-the-Rashomon-Effect-and-the-Reliability-of-Post-Hoc-Explainable-AI"><a href="#Can-we-Agree-On-the-Rashomon-Effect-and-the-Reliability-of-Post-Hoc-Explainable-AI" class="headerlink" title="Can we Agree? On the Rashōmon Effect and the Reliability of Post-Hoc Explainable AI"></a>Can we Agree? On the Rashōmon Effect and the Reliability of Post-Hoc Explainable AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07247">http://arxiv.org/abs/2308.07247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Clement Poiret, Antoine Grigis, Justin Thomas, Marion Noulhiane</li>
<li>for: 这项研究探讨了使用SHAP在Rashomon集中获得可靠知识的挑战。</li>
<li>methods: 研究使用5个公共数据集进行实验，发现采样大小的增加可以提高模型的解释的一致性。但在少量采样下（&lt;128个样本），解释具有高度的变化性，因此不可靠地抽取知识。然而，随着更多的数据，模型之间的一致性提高，允许达成共识。bagging ensemble通常具有更高的一致性。</li>
<li>results: 研究结果表明，要在少量采样下（&lt;128个样本）进行验证，以确保结论的可靠性。此外，对于不同的模型类型、数据领域和解释方法，进一步的研究是必要的。测试神经网络和特定解释方法的收敛性也是有价值的。本研究的方法指向了可靠地从模糊模型中提取知识的原则方法。<details>
<summary>Abstract</summary>
The Rash\=omon effect poses challenges for deriving reliable knowledge from machine learning models. This study examined the influence of sample size on explanations from models in a Rash\=omon set using SHAP. Experiments on 5 public datasets showed that explanations gradually converged as the sample size increased. Explanations from <128 samples exhibited high variability, limiting reliable knowledge extraction. However, agreement between models improved with more data, allowing for consensus. Bagging ensembles often had higher agreement. The results provide guidance on sufficient data to trust explanations. Variability at low samples suggests that conclusions may be unreliable without validation. Further work is needed with more model types, data domains, and explanation methods. Testing convergence in neural networks and with model-specific explanation methods would be impactful. The approaches explored here point towards principled techniques for eliciting knowledge from ambiguous models.
</details>
<details>
<summary>摘要</summary>
“落差omon效应对机器学习模型知识抽取带来挑战。这项研究研究了模型在Rashomon集中的解释如何受样本大小影响。使用SHAP进行实验，发现随着样本大小增加，解释的一致性逐渐提高。但是，从128个样本开始，解释呈现高度的变化， limiting 可靠知识抽取。然而，通过更多的数据，模型之间的一致性提高，allowing for consensus。 bagging  ensemble 通常具有更高的一致性。结果为我们提供了足够数据来信任解释的指南。低样本数时的变化表明，不进行验证，得出的结论可能不可靠。未来的工作应该进一步探索更多的模型类型、数据领域和解释方法。测试神经网络和特定模型解释方法的受样本大小影响也是有价值的。研究进行的方法指向了有良好原则的模型解释技术。”
</details></li>
</ul>
<hr>
<h2 id="A-Unifying-Generator-Loss-Function-for-Generative-Adversarial-Networks"><a href="#A-Unifying-Generator-Loss-Function-for-Generative-Adversarial-Networks" class="headerlink" title="A Unifying Generator Loss Function for Generative Adversarial Networks"></a>A Unifying Generator Loss Function for Generative Adversarial Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07233">http://arxiv.org/abs/2308.07233</a></li>
<li>repo_url: None</li>
<li>paper_authors: Justin Veiner, Fady Alajaji, Bahman Gharesifard</li>
<li>for: 这个论文主要关注的是 dual-objective generative adversarial network (GAN) 的 $\alpha$-parametrized generator loss function，用于替代原始 GAN 系统中的 classical discriminator loss function。</li>
<li>methods: 这个论文提出了一种基于 symmetric class probability estimation 类型的 generator loss function，称为 $\mathcal{L}_\alpha$，并使用这个loss function来定义 $\mathcal{L}_\alpha$-GAN 系统。</li>
<li>results: 研究人员通过分析 generator 的优化问题，发现 generator 的优化问题可以表示为一个 Jensen-$f_\alpha$- divergence 的最小化问题，其中 $f_\alpha$ 是一个 convex 函数，具体表示为 loss function $\mathcal{L}_\alpha$。此外，这个 $\mathcal{L}_\alpha$-GAN 问题还可以恢复一些在文献中提出的 GAN 问题，包括 VanillaGAN、LSGAN、L$k$GAN 和 $({\alpha_D},{\alpha_G})$-GAN 中的 $\alpha_D&#x3D;1$。最后，在 MNIST、CIFAR-10 和 Stacked MNIST 三个数据集上进行了实验，以证明不同的例子的 $\mathcal{L}_\alpha$-GAN 系统的性能。<details>
<summary>Abstract</summary>
A unifying $\alpha$-parametrized generator loss function is introduced for a dual-objective generative adversarial network (GAN), which uses a canonical (or classical) discriminator loss function such as the one in the original GAN (VanillaGAN) system. The generator loss function is based on a symmetric class probability estimation type function, $\mathcal{L}_\alpha$, and the resulting GAN system is termed $\mathcal{L}_\alpha$-GAN. Under an optimal discriminator, it is shown that the generator's optimization problem consists of minimizing a Jensen-$f_\alpha$-divergence, a natural generalization of the Jensen-Shannon divergence, where $f_\alpha$ is a convex function expressed in terms of the loss function $\mathcal{L}_\alpha$. It is also demonstrated that this $\mathcal{L}_\alpha$-GAN problem recovers as special cases a number of GAN problems in the literature, including VanillaGAN, Least Squares GAN (LSGAN), Least $k$th order GAN (L$k$GAN) and the recently introduced $(\alpha_D,\alpha_G)$-GAN with $\alpha_D=1$. Finally, experimental results are conducted on three datasets, MNIST, CIFAR-10, and Stacked MNIST to illustrate the performance of various examples of the $\mathcal{L}_\alpha$-GAN system.
</details>
<details>
<summary>摘要</summary>
文本中引入了一种对称$\alpha$-参数化生成器损失函数，用于一个双目标生成对抗网络（GAN）系统。生成器损失函数基于一种对称的класси型概率估计函数，$\mathcal{L}_\alpha$，并将系统称为$\mathcal{L}_\alpha$-GAN。在理想的权衡器下， generator的优化问题可以表示为最小化一种Jensen-$f_\alpha$-分配，这是自然推广Jensen-Shannon分配的一种自然推广，其中$f_\alpha$是一个对称的束缚函数，它与损失函数$\mathcal{L}_\alpha$有关。此外，这个$\mathcal{L}_\alpha$-GAN问题还能够恢复一些文献中的GAN问题，包括VanillaGAN、LSGAN、L$k$GAN和$(\alpha_D,\alpha_G)$-GAN中的$\alpha_D=1$。最后，对三个数据集（MNIST、CIFAR-10和Stacked MNIST）进行了实验，以示出不同的$\mathcal{L}_\alpha$-GAN系统的性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/15/cs.LG_2023_08_15/" data-id="clorjzl8q00o3f1885q65am0w" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/15/eess.IV_2023_08_15/" class="article-date">
  <time datetime="2023-08-15T09:00:00.000Z" itemprop="datePublished">2023-08-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/15/eess.IV_2023_08_15/">eess.IV - 2023-08-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Targeted-Multispectral-Filter-Array-Design-for-Endoscopic-Cancer-Detection-in-the-Gastrointestinal-Tract"><a href="#Targeted-Multispectral-Filter-Array-Design-for-Endoscopic-Cancer-Detection-in-the-Gastrointestinal-Tract" class="headerlink" title="Targeted Multispectral Filter Array Design for Endoscopic Cancer Detection in the Gastrointestinal Tract"></a>Targeted Multispectral Filter Array Design for Endoscopic Cancer Detection in the Gastrointestinal Tract</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07947">http://arxiv.org/abs/2308.07947</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michaela Taylor-Williams, Ran Tao, Travis W Sawyer, Dale J Waterhouse, Jonghee Yoon, Sarah E Bohndiek</li>
<li>for: 检测肠胃肉体中疾病的颜色差异，以提高疾病检测的准确性。</li>
<li>methods: 使用自定义多spectral filter arrays (MSFAs)，并使用开源工具箱Opti-MSFA进行优化设计。</li>
<li>results: 结果显示，MSFA设计具有高分类精度，表明将在未来实施在检查器硬件中可能有助于提高肠胃肉体检测的早期检测。<details>
<summary>Abstract</summary>
Colour differences between healthy and diseased tissue in the gastrointestinal tract are detected visually by clinicians during white light endoscopy (WLE); however, the earliest signs of disease are often just a slightly different shade of pink compared to healthy tissue. Here, we propose to target alternative colours for imaging to improve contrast using custom multispectral filter arrays (MSFAs) that could be deployed in an endoscopic chip-on-tip configuration. Using an open-source toolbox, Opti-MSFA, we examined the optimal design of MSFAs for early cancer detection in the gastrointestinal tract. The toolbox was first extended to use additional classification models (k-Nearest Neighbour, Support Vector Machine, and Spectral Angle Mapper). Using input spectral data from published clinical trials examining the oesophagus and colon, we optimised the design of MSFAs with 3 to 9 different bands. We examined the variation of the spectral and spatial classification accuracy as a function of number of bands. The MSFA designs have high classification accuracies, suggesting that future implementation in endoscopy hardware could potentially enable improved early detection of disease in the gastrointestinal tract during routine screening and surveillance. Optimal MSFA configurations can achieve similar classification accuracies as the full spectral data in an implementation that could be realised in far simpler hardware. The reduced number of spectral bands could enable future deployment of multispectral imaging in an endoscopic chip-on-tip configuration.
</details>
<details>
<summary>摘要</summary>
医生在白光endooscopy（WLE）中可以通过视觉检测肠道内健康和疾病组织的颜色差异。然而，疾病的早期征象通常只是健康组织的微妙变化。我们提议使用自定义多spectral filter array（MSFA）来提高对比度。我们使用了一个开源工具箱，Opti-MSFA，来调查最佳MSFA的设计。我们首先扩展了工具箱，使其支持更多的分类模型（k-最近邻居、支持向量机和spectral angle mapper）。使用来自已发布临床试验的迷你镜诊断数据，我们优化了MSFA的3到9个频谱带的设计。我们分析了频谱和空间分类精度的变化与频谱带数的关系。我们发现，最佳MSFA配置具有高分类精度， suggesting that future implementation in endoscopy hardware could potentially enable improved early detection of disease in the gastrointestinal tract during routine screening and surveillance。最佳MSFA配置可以实现与全spectral数据相同的分类精度，但具有更少的频谱带数，这可能使得未来在endooscopy中实现多spectral imaging的方式更加简单。
</details></li>
</ul>
<hr>
<h2 id="DSFNet-Convolutional-Encoder-Decoder-Architecture-Combined-Dual-GCN-and-Stand-alone-Self-attention-by-Fast-Normalized-Fusion-for-Polyps-Segmentation"><a href="#DSFNet-Convolutional-Encoder-Decoder-Architecture-Combined-Dual-GCN-and-Stand-alone-Self-attention-by-Fast-Normalized-Fusion-for-Polyps-Segmentation" class="headerlink" title="DSFNet: Convolutional Encoder-Decoder Architecture Combined Dual-GCN and Stand-alone Self-attention by Fast Normalized Fusion for Polyps Segmentation"></a>DSFNet: Convolutional Encoder-Decoder Architecture Combined Dual-GCN and Stand-alone Self-attention by Fast Normalized Fusion for Polyps Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07946">http://arxiv.org/abs/2308.07946</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juntong Fan, Tieyong Zeng, Dayang Wang<br>for:This paper aims to address the challenging task of polyp segmentation in colonoscopy images using a novel U-shaped network called DSFNet.methods:The proposed DSFNet combines the advantages of Dual-GCN and self-attention mechanisms, including a feature enhancement block module and a stand-alone self-attention module, as well as a Fast Normalized Fusion method for efficient feature fusion.results:The proposed model surpasses other state-of-the-art models in terms of Dice, MAE, and IoU on two public datasets (Endoscene and Kvasir-SEG), and ablation studies verify the efficacy and effectiveness of each module. The proposed model has great clinical significance for polyp segmentation in colonoscopy images.Here is the Chinese version of the three key points:for:本研究使用一种新型的U型网络，即DSFNet，解决了医学内视镜中肠道肿瘤分 segmentation的挑战性问题。methods:提议的DSFNet结合了 dual-GCN和自注意机制的优点，包括特征增强块模块和独立自注意模块，以及一种高效的快Normalized Fusion方法。results:实验结果表明，提议的模型在两个公共数据集（Endoscene和Kvasir-SEG）上比其他状态对比模型在多个指标（Dice、MAE和IoU）上表现出优异，并进行了ablation研究来验证每个模块的有效性和效iveness。结果表明，提议的模型在医学中具有大的临床意义。<details>
<summary>Abstract</summary>
In the past few decades, deep learning technology has been widely used in medical image segmentation and has made significant breakthroughs in the fields of liver and liver tumor segmentation, brain and brain tumor segmentation, video disc segmentation, heart image segmentation, and so on. However, the segmentation of polyps is still a challenging task since the surface of the polyps is flat and the color is very similar to that of surrounding tissues. Thus, It leads to the problems of the unclear boundary between polyps and surrounding mucosa, local overexposure, and bright spot reflection. To counter this problem, this paper presents a novel U-shaped network, namely DSFNet, which effectively combines the advantages of Dual-GCN and self-attention mechanisms. First, we introduce a feature enhancement block module based on Dual-GCN module as an attention mechanism to enhance the feature extraction of local spatial and structural information with fine granularity. Second, the stand-alone self-attention module is designed to enhance the integration ability of the decoding stage model to global information. Finally, the Fast Normalized Fusion method with trainable weights is used to efficiently fuse the corresponding three feature graphs in encoding, bottleneck, and decoding blocks, thus promoting information transmission and reducing the semantic gap between encoder and decoder. Our model is tested on two public datasets including Endoscene and Kvasir-SEG and compared with other state-of-the-art models. Experimental results show that the proposed model surpasses other competitors in many indicators, such as Dice, MAE, and IoU. In the meantime, ablation studies are also conducted to verify the efficacy and effectiveness of each module. Qualitative and quantitative analysis indicates that the proposed model has great clinical significance.
</details>
<details>
<summary>摘要</summary>
在过去几十年中，深度学习技术在医学影像分割领域得到广泛应用，并在肝脏和肝癌分割、脑和脑癌分割、视频碟分割、心脏影像分割等领域取得了显著突破。然而，肿瘤分割仍然是一项挑战性的任务，因为肿瘤表面平滑，颜色与周围组织相似，导致边界不清晰、局部过曝光和光泽反射等问题。为解决这些问题，本文提出了一种新的U型网络，即DSFNet，该网络效果地结合了DUAL-GCN模块和自注意机制。首先，我们引入了基于DUAL-GCN模块的特征增强块模块，以增强本地空间和结构信息的特征提取。其次，我们设计了独立的自注意模块，以提高解码阶段模型的全局信息集成能力。最后，我们使用可学习权重的快速 нормализа化融合方法，以有效地融合编码、瓶颈和解码块中的三个特征图，从而提高信息传递和减少编码器和解码器之间的semantic gap。我们的模型在Endoscene和Kvasir-SEG两个公共数据集上进行测试，与其他当前顶尖模型进行比较。实验结果表明，我们的模型在多个指标上超过其他竞争对手，包括 dice、MAE和IoU等指标。同时，我们还进行了ablation研究，以验证每个模块的有效性和效果。 qualitative和quantitative分析表明，我们的模型在临床上具有很大的价值。
</details></li>
</ul>
<hr>
<h2 id="An-Interpretable-Machine-Learning-Model-with-Deep-Learning-based-Imaging-Biomarkers-for-Diagnosis-of-Alzheimer’s-Disease"><a href="#An-Interpretable-Machine-Learning-Model-with-Deep-Learning-based-Imaging-Biomarkers-for-Diagnosis-of-Alzheimer’s-Disease" class="headerlink" title="An Interpretable Machine Learning Model with Deep Learning-based Imaging Biomarkers for Diagnosis of Alzheimer’s Disease"></a>An Interpretable Machine Learning Model with Deep Learning-based Imaging Biomarkers for Diagnosis of Alzheimer’s Disease</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07778">http://arxiv.org/abs/2308.07778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjie Kang, Bo Li, Janne M. Papma, Lize C. Jiskoot, Peter Paul De Deyn, Geert Jan Biessels, Jurgen A. H. R. Claassen, Huub A. M. Middelkoop, Wiesje M. van der Flier, Inez H. G. B. Ramakers, Stefan Klein, Esther E. Bron</li>
<li>for: 预测阿尔ц海默病（AD）的早期诊断。</li>
<li>methods: combines Explainable Boosting Machines (EBM) with deep learning-based feature extraction，提供了每个特征的重要性。</li>
<li>results: 在Alzheimer’s Disease Neuroimaging Initiative（ADNI）数据集上 achieved accuracy of 0.883和area-under-the-curve（AUC）of 0.970 on AD和control分类，并在一个外部测试集上 achieved accuracy of 0.778和AUC of 0.887 on AD和主观认知下降（SCD）分类。<details>
<summary>Abstract</summary>
Machine learning methods have shown large potential for the automatic early diagnosis of Alzheimer's Disease (AD). However, some machine learning methods based on imaging data have poor interpretability because it is usually unclear how they make their decisions. Explainable Boosting Machines (EBMs) are interpretable machine learning models based on the statistical framework of generalized additive modeling, but have so far only been used for tabular data. Therefore, we propose a framework that combines the strength of EBM with high-dimensional imaging data using deep learning-based feature extraction. The proposed framework is interpretable because it provides the importance of each feature. We validated the proposed framework on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset, achieving accuracy of 0.883 and area-under-the-curve (AUC) of 0.970 on AD and control classification. Furthermore, we validated the proposed framework on an external testing set, achieving accuracy of 0.778 and AUC of 0.887 on AD and subjective cognitive decline (SCD) classification. The proposed framework significantly outperformed an EBM model using volume biomarkers instead of deep learning-based features, as well as an end-to-end convolutional neural network (CNN) with optimized architecture.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Dynamic-Low-Rank-Instance-Adaptation-for-Universal-Neural-Image-Compression"><a href="#Dynamic-Low-Rank-Instance-Adaptation-for-Universal-Neural-Image-Compression" class="headerlink" title="Dynamic Low-Rank Instance Adaptation for Universal Neural Image Compression"></a>Dynamic Low-Rank Instance Adaptation for Universal Neural Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07733">http://arxiv.org/abs/2308.07733</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/llvy21/duic">https://github.com/llvy21/duic</a></li>
<li>paper_authors: Yue Lv, Jinxi Xiang, Jun Zhang, Wenming Yang, Xiao Han, Wei Yang</li>
<li>for: 本研究旨在提高 neural image compression 的环境不同时的表现，即addressing the domain gap between training datasets (natural images) and inference datasets (e.g., artistic images).</li>
<li>methods: 我们提出了一种基于low-rank adaptation的方法，包括在客户端decoder中进行低级别矩阵分解，并将更新了适应参数传输到客户端。此外，我们还引入了一种动态阀网，以确定需要适应的层。</li>
<li>results: 我们的方法可以 universal across diverse image datasets，并且在out-of-domain图像上表现出较好的表现，与非适应方法相比，平均BD-rate提高约19%。此外，我们的方法还可以在不同的图像压缩架构上进行改进。<details>
<summary>Abstract</summary>
The latest advancements in neural image compression show great potential in surpassing the rate-distortion performance of conventional standard codecs. Nevertheless, there exists an indelible domain gap between the datasets utilized for training (i.e., natural images) and those utilized for inference (e.g., artistic images). Our proposal involves a low-rank adaptation approach aimed at addressing the rate-distortion drop observed in out-of-domain datasets. Specifically, we perform low-rank matrix decomposition to update certain adaptation parameters of the client's decoder. These updated parameters, along with image latents, are encoded into a bitstream and transmitted to the decoder in practical scenarios. Due to the low-rank constraint imposed on the adaptation parameters, the resulting bit rate overhead is small. Furthermore, the bit rate allocation of low-rank adaptation is \emph{non-trivial}, considering the diverse inputs require varying adaptation bitstreams. We thus introduce a dynamic gating network on top of the low-rank adaptation method, in order to decide which decoder layer should employ adaptation. The dynamic adaptation network is optimized end-to-end using rate-distortion loss. Our proposed method exhibits universality across diverse image datasets. Extensive results demonstrate that this paradigm significantly mitigates the domain gap, surpassing non-adaptive methods with an average BD-rate improvement of approximately $19\%$ across out-of-domain images. Furthermore, it outperforms the most advanced instance adaptive methods by roughly $5\%$ BD-rate. Ablation studies confirm our method's ability to universally enhance various image compression architectures.
</details>
<details>
<summary>摘要</summary>
最新的神经网络图像压缩技术具有可能超越传统标准编解码器的环境-质量表现的潜在力量。然而，存在一个不可缺少的领域差距（domain gap），即训练集（natural images）和推理集（e.g., artistic images）之间的差异。我们的提议是一种基于低级数的适应方法，用于Addressing the rate-distortion drop observed in out-of-domain datasets。具体来说，我们通过低级数矩阵分解更新客户端的解码器某些适应参数。这些更新后的参数，加上图像latent，被编码到一个bit流中并在实际应用场景中传输。由于低级数约束对适应参数的影响，bit流扩展的负担小。此外，低级数适应的bit流分配是非易的，需要根据各种输入的多样性进行变动的适应。我们因此引入了一种基于低级数适应的动态阻止网络，以确定哪些解码层应该使用适应。这个动态适应网络通过练习环境-质量损失来优化。我们的提议在多种图像压缩架构上 universality，并且在多种图像数据集上进行了广泛的测试。结果显示，这种方法可以有效 mitigate the domain gap，与非适应方法相比，平均BD-rate提高约19%，而与最先进的实例适应方法相比，BD-rate提高约5%。剖析研究证明了我们的方法可以通过universal enhancement来改善多种图像压缩架构。
</details></li>
</ul>
<hr>
<h2 id="A-deep-deformable-residual-learning-network-for-SAR-images-segmentation"><a href="#A-deep-deformable-residual-learning-network-for-SAR-images-segmentation" class="headerlink" title="A deep deformable residual learning network for SAR images segmentation"></a>A deep deformable residual learning network for SAR images segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07627">http://arxiv.org/abs/2308.07627</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenwei Wang, Jifang Pei, Yulin Huang, Jianyu Yang</li>
<li>for: 这篇论文是为了提出一个基于深度学习网络的新方法 для SAR 对象分类，以提高 SAR 对象分类的精度和速度。</li>
<li>methods: 本文使用的方法包括对于 SAR 图像进行深度学习网络的建立，并将对象分类问题转化为一个对于图像进行分类的问题。另外，本文还使用了扭转卷网络和复原学习块，以提高网络的准确性和稳定性。</li>
<li>results: 根据 MSTAR 资料集的实验结果显示，提出的深度对象分类网络能够实现高精度和高速度的 SAR 对象分类，并且比传统方法更加精确和可靠。<details>
<summary>Abstract</summary>
Reliable automatic target segmentation in Synthetic Aperture Radar (SAR) imagery has played an important role in the SAR fields. Different from the traditional methods, Spectral Residual (SR) and CFAR detector, with the recent adavance in machine learning theory, there has emerged a novel method for SAR target segmentation, based on the deep learning networks. In this paper, we proposed a deep deformable residual learning network for target segmentation that attempts to preserve the precise contour of the target. For this, the deformable convolutional layers and residual learning block are applied, which could extract and preserve the geometric information of the targets as much as possible. Based on the Moving and Stationary Target Acquisition and Recognition (MSTAR) data set, experimental results have shown the superiority of the proposed network for the precise targets segmentation.
</details>
<details>
<summary>摘要</summary>
<<SYS>启用简化中文</SYS>静电Synthetic Aperture Radar（SAR）图像中的自动目标分割可靠性在SAR领域中发挥了重要作用。与传统方法不同，我们提出了基于深度学习网络的新方法，即吸引强度差（SR）和CFAR探测器。在这篇论文中，我们提出了一种深度变形剩余学习网络，用于目标分割，以保留目标精确的轮廓。为了实现这一目标，我们使用了变形卷积层和剩余学习块，以提取和保留目标的几何信息。基于Moving and Stationary Target Acquisition and Recognition（MSTAR）数据集，我们的实验结果表明，提出的网络可以准确地分割精确目标。
</details></li>
</ul>
<hr>
<h2 id="GAMER-MRIL-identifies-Disability-Related-Brain-Changes-in-Multiple-Sclerosis"><a href="#GAMER-MRIL-identifies-Disability-Related-Brain-Changes-in-Multiple-Sclerosis" class="headerlink" title="GAMER-MRIL identifies Disability-Related Brain Changes in Multiple Sclerosis"></a>GAMER-MRIL identifies Disability-Related Brain Changes in Multiple Sclerosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07611">http://arxiv.org/abs/2308.07611</a></li>
<li>repo_url: None</li>
<li>paper_authors: Po-Jui Lu, Benjamin Odry, Muhamed Barakovic, Matthias Weigel, Robin Sandkühler, Reza Rahmanzadeh, Xinjie Chen, Mario Ocampo-Pineda, Jens Kuhle, Ludwig Kappos, Philippe Cattin, Cristina Granziera</li>
<li>for: 这个研究的目的是为了确定多创性疾病（MS）患者的残障相关的脑部变化。</li>
<li>methods: 这个研究使用了全脑量化MRI（qMRI）、神经网络（CNN）和可解释方法，将MS患者分为严重残障和不严重残障两组。</li>
<li>results: 研究获得了0.885的测试效果，qT1是残障相关最敏感的测量指标，其次是neurite density index（NDI）。这个研究还发现了残障相关的脑部区域，包括 corticospinal tract，这些区域与患者的残障分数有 statistically significant 的相关性（ρ&#x3D;-0.37和0.44）。<details>
<summary>Abstract</summary>
Objective: Identifying disability-related brain changes is important for multiple sclerosis (MS) patients. Currently, there is no clear understanding about which pathological features drive disability in single MS patients. In this work, we propose a novel comprehensive approach, GAMER-MRIL, leveraging whole-brain quantitative MRI (qMRI), convolutional neural network (CNN), and an interpretability method from classifying MS patients with severe disability to investigating relevant pathological brain changes. Methods: One-hundred-sixty-six MS patients underwent 3T MRI acquisitions. qMRI informative of microstructural brain properties was reconstructed, including quantitative T1 (qT1), myelin water fraction (MWF), and neurite density index (NDI). To fully utilize the qMRI, GAMER-MRIL extended a gated-attention-based CNN (GAMER-MRI), which was developed to select patch-based qMRI important for a given task/question, to the whole-brain image. To find out disability-related brain regions, GAMER-MRIL modified a structure-aware interpretability method, Layer-wise Relevance Propagation (LRP), to incorporate qMRI. Results: The test performance was AUC=0.885. qT1 was the most sensitive measure related to disability, followed by NDI. The proposed LRP approach obtained more specifically relevant regions than other interpretability methods, including the saliency map, the integrated gradients, and the original LRP. The relevant regions included the corticospinal tract, where average qT1 and NDI significantly correlated with patients' disability scores ($\rho$=-0.37 and 0.44). Conclusion: These results demonstrated that GAMER-MRIL can classify patients with severe disability using qMRI and subsequently identify brain regions potentially important to the integrity of the mobile function. Significance: GAMER-MRIL holds promise for developing biomarkers and increasing clinicians' trust in NN.
</details>
<details>
<summary>摘要</summary>
目标：为多发性硬化病（MS）患者 Identify 负面功能相关的脑变化。现在，没有明确的认知，哪些生理学特征驱动单个MS患者的残疾。在这种工作中，我们提议了一种全新的全脑量化MRI（qMRI）、卷积神经网络（CNN）和可解释方法，从分类MS患者严重残疾到研究相关的脑变化。方法：一百六十六名MS患者通过3T MRI成像。重要的质量MRI（qMRI）信息，包括质量T1（qT1）、脑白质含量（MWF）和神经纤维数（NDI），都被重构。为了全面利用qMRI，我们扩展了基于闭合注意力的CNN（GAMER-MRI），并将其应用到整个脑图像。为了找出残疾相关的脑区域，我们修改了层次相关传播（LRP）方法，以包括qMRI。结果：测试性能为AUC=0.885。qT1是残疾相关度最高的指标，其次是NDI。我们的LRP方法在特定任务/问题中更有特点地找出了残疾相关的脑区域，比如脑束束络（ corticospinal tract），其中qT1和NDI的平均值与患者残疾分数（ρ=-0.37和0.44）有 statistically significant 相关性。结论：这些结果表明，GAMER-MRIL可以通过qMRI和CNN来分类患者严重残疾，并且可以特定残疾相关的脑区域。这些结果表明GAMER-MRIL具有开发生物标志物和增加临床医生对NN的信任的潜力。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Scalable-Epistemic-Uncertainty-Quantification-in-Organ-Segmentation"><a href="#Benchmarking-Scalable-Epistemic-Uncertainty-Quantification-in-Organ-Segmentation" class="headerlink" title="Benchmarking Scalable Epistemic Uncertainty Quantification in Organ Segmentation"></a>Benchmarking Scalable Epistemic Uncertainty Quantification in Organ Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07506">http://arxiv.org/abs/2308.07506</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jadie1/medseguq">https://github.com/jadie1/medseguq</a></li>
<li>paper_authors: Jadie Adams, Shireen Y. Elhabian</li>
<li>for:  aid diagnosis and treatment planning</li>
<li>methods:  epistemic uncertainty quantification methods in organ segmentation</li>
<li>results:  comprehensive benchmarking study to evaluate the accuracy, uncertainty calibration, and scalability of different methods<details>
<summary>Abstract</summary>
Deep learning based methods for automatic organ segmentation have shown promise in aiding diagnosis and treatment planning. However, quantifying and understanding the uncertainty associated with model predictions is crucial in critical clinical applications. While many techniques have been proposed for epistemic or model-based uncertainty estimation, it is unclear which method is preferred in the medical image analysis setting. This paper presents a comprehensive benchmarking study that evaluates epistemic uncertainty quantification methods in organ segmentation in terms of accuracy, uncertainty calibration, and scalability. We provide a comprehensive discussion of the strengths, weaknesses, and out-of-distribution detection capabilities of each method as well as recommendations for future improvements. These findings contribute to the development of reliable and robust models that yield accurate segmentations while effectively quantifying epistemic uncertainty.
</details>
<details>
<summary>摘要</summary>
深度学习基于方法为自动器官 segmentation 表现出了许多批处的可能性，帮助诊断和治疗规划。然而，量化和理解模型预测结果中的uncertainty是在重要的临床应用中关键。虽然许多技术被提出用于知识 Based uncertainty estimation，但是没有一种方法在医学图像分析中被强调。这篇论文提供了一项全面的比较研究，评估了器官 segmentation 中epistemic uncertainty quantification 方法的准确性、uncertainty calibration和可扩展性。我们提供了每种方法的优缺点、out-of-distribution检测能力和未来改进的建议。这些发现有助于开发可靠和可靠的模型，以获得准确的 segmentation 结果，同时有效地量化epistemic uncertainty。
</details></li>
</ul>
<hr>
<h2 id="Brain-Tumor-Detection-Based-on-a-Novel-and-High-Quality-Prediction-of-the-Tumor-Pixel-Distributions"><a href="#Brain-Tumor-Detection-Based-on-a-Novel-and-High-Quality-Prediction-of-the-Tumor-Pixel-Distributions" class="headerlink" title="Brain Tumor Detection Based on a Novel and High-Quality Prediction of the Tumor Pixel Distributions"></a>Brain Tumor Detection Based on a Novel and High-Quality Prediction of the Tumor Pixel Distributions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07495">http://arxiv.org/abs/2308.07495</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanming Sun, Chunyan Wang<br>for:* The paper proposes a system for detecting brain tumors in 3D MRI brain scans of Flair modality.methods:* The system uses a 2D histogram presentation to comprehend the gray-level distribution and pixel-location distribution of a 3D object.* It exploits the left-right asymmetry of a brain structure to establish particular 2D histograms, which are then modulated to attenuate irrelevant elements.* The system predicts the tumor pixel distribution in 3 steps, on the axial, coronal, and sagittal slice series, respectively.results:* The system delivers very good tumor detection results, comparable to those of state-of-the-art CNN systems with mono-modality inputs.* The system achieves this at an extremely low computation cost and without the need for training.Here is the answer in Simplified Chinese text:for:* 这篇论文提出了一种用于检测大脑肿瘤的系统，该系统使用3D MRI脑部扫描的FLAIR模式。methods:* 该系统使用2D histogram展示来理解肿瘤区域的灰度分布和像素位置分布。* 它利用脑结构的左右偏好来建立特定的2D histogram，并对其进行减小。* 系统在axial、coronal和sagittal slice series上预测肿瘤像素分布，并在每个步骤中使用预测结果来identify&#x2F;remove肿瘤自由的 slice。results:* 系统实现了非常好的肿瘤检测结果，与单模态输入的state-of-the-art CNN系统相当。* 系统在计算成本非常低的情况下实现了这一结果，而无需训练。<details>
<summary>Abstract</summary>
In this paper, we propose a system to detect brain tumor in 3D MRI brain scans of Flair modality. It performs 2 functions: (a) predicting gray-level and locational distributions of the pixels in the tumor regions and (b) generating tumor mask in pixel-wise precision. To facilitate 3D data analysis and processing, we introduced a 2D histogram presentation that comprehends the gray-level distribution and pixel-location distribution of a 3D object. In the proposed system, particular 2D histograms, in which tumor-related feature data get concentrated, are established by exploiting the left-right asymmetry of a brain structure. A modulation function is generated from the input data of each patient case and applied to the 2D histograms to attenuate the element irrelevant to the tumor regions. The prediction of the tumor pixel distribution is done in 3 steps, on the axial, coronal and sagittal slice series, respectively. In each step, the prediction result helps to identify/remove tumor-free slices, increasing the tumor information density in the remaining data to be applied to the next step. After the 3-step removal, the 3D input is reduced to a minimum bounding box of the tumor region. It is used to finalize the prediction and then transformed into a 3D tumor mask, by means of gray level thresholding and low-pass-based morphological operations. The final prediction result is used to determine the critical threshold. The proposed system has been tested extensively with the data of more than one thousand patient cases in the datasets of BraTS 2018~21. The test results demonstrate that the predicted 2D histograms have a high degree of similarity with the true ones. The system delivers also very good tumor detection results, comparable to those of state-of-the-art CNN systems with mono-modality inputs, which is achieved at an extremely low computation cost and no need for training.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种系统，用于检测脑肿瘤在3D MRI脑内部分模式下的检测。它执行了两个功能：（a）预测肿瘤区域中像素的灰度和位坐标分布，（b）生成精准的肿瘤面积。为了便于3D数据分析和处理，我们引入了2D分布图表示法，该法涵盖了肿瘤区域中像素的灰度分布和位坐标分布。在我们提出的系统中，特定的2D分布，在其中肿瘤相关特征数据受集中，被建立了。然后，通过对输入数据的每个患者案例中的偏置函数进行应用，以减少不相关于肿瘤区域的元素。肿瘤像素分布预测在 axial、coronal 和 sagittal 三个方向上进行了3步骤逐步进行，每步骤结果帮助确定/移除肿瘤不存在的剖面，从而提高肿瘤信息的浓度在剩下的数据中，并应用到下一步。经过3步 removals，输入3D数据被减少到最小 bounding box 的肿瘤区域。它被用于最终预测，并使用灰度阈值和低通过的杂谱操作来转换为3D肿瘤面积。测试结果表明，预测的2D分布与实际分布有高度的相似性。系统还提供了非常好的肿瘤检测结果，与STATE-OF-THE-ART CNN系统的单模式输入相比，并且在极低的计算成本下达到了这一点，无需训练。
</details></li>
</ul>
<hr>
<h2 id="Space-Object-Identification-and-Classification-from-Hyperspectral-Material-Analysis"><a href="#Space-Object-Identification-and-Classification-from-Hyperspectral-Material-Analysis" class="headerlink" title="Space Object Identification and Classification from Hyperspectral Material Analysis"></a>Space Object Identification and Classification from Hyperspectral Material Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07481">http://arxiv.org/abs/2308.07481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Massimiliano Vasile, Lewis Walker, Andrew Campbell, Simao Marto, Paul Murray, Stephen Marshall, Vasili Savitski</li>
<li>for: 本研究旨在提取不知名空间物体的干涉特征信息，并使用这些信息来确定物体的物理组成。</li>
<li>methods: 本研究使用了两种材料标识和分类技术：一种基于机器学习，另一种基于最小二乘匹配known спектры库。通过这些信息，一种监督式机器学习算法用于将物体分类为不同类别，根据检测到物体上的材料。</li>
<li>results: 研究结果表明，当材料库缺失一种物质时，材料分类方法的行为会受到影响。此外，研究还发现在不理想的天气条件下，材料分类方法的行为也会受到影响。最终，文章将展示一些初步的空间物体识别和分类结果。<details>
<summary>Abstract</summary>
This paper presents a data processing pipeline designed to extract information from the hyperspectral signature of unknown space objects. The methodology proposed in this paper determines the material composition of space objects from single pixel images. Two techniques are used for material identification and classification: one based on machine learning and the other based on a least square match with a library of known spectra. From this information, a supervised machine learning algorithm is used to classify the object into one of several categories based on the detection of materials on the object. The behaviour of the material classification methods is investigated under non-ideal circumstances, to determine the effect of weathered materials, and the behaviour when the training library is missing a material that is present in the object being observed. Finally the paper will present some preliminary results on the identification and classification of space objects.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "hyperspectral signature" is translated as "多спектル特征" (duō yán jīng)* "material composition" is translated as "物质组成" (wù zhì zhōng)* "machine learning" is translated as "机器学习" (jī shì xué xí)* "least square match" is translated as "最小二乘匹配" (zuì xiǎo èr chuī pīng pái)* "training library" is translated as "训练库" (xùn xí kù)* "weathered materials" is translated as "气候变化的材料" (qì hòu biàn gē de zhì lǐ)
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-MIMO-U-Net-Efficient-and-Accurate-Uncertainty-Estimation-for-Pixel-wise-Regression"><a href="#Probabilistic-MIMO-U-Net-Efficient-and-Accurate-Uncertainty-Estimation-for-Pixel-wise-Regression" class="headerlink" title="Probabilistic MIMO U-Net: Efficient and Accurate Uncertainty Estimation for Pixel-wise Regression"></a>Probabilistic MIMO U-Net: Efficient and Accurate Uncertainty Estimation for Pixel-wise Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07477">http://arxiv.org/abs/2308.07477</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/antonbaumann/mimo-unet">https://github.com/antonbaumann/mimo-unet</a></li>
<li>paper_authors: Anton Baumann, Thomas Roßberg, Michael Schmitt</li>
<li>for: 这个论文的目的是提高机器学习模型的可靠性和可读性，特别是在高度关键的实际应用场景中。</li>
<li>methods: 这篇论文使用了多输入多输出（MIMO）框架，利用深度神经网络的过参数化来进行像素级回归任务。它还引入了一种同步多个子网络性能的新程序。</li>
<li>results: 对两个正交的数据集进行了全面的评估，显示MIMO U-Net模型具有与现有模型相当的准确率，更好的折衔在正常数据上，robust的对于异常数据检测能力，并且具有较小的参数大小和更快的推理时间。代码可以在github.com&#x2F;antonbaumann&#x2F;MIMO-Unet中找到。<details>
<summary>Abstract</summary>
Uncertainty estimation in machine learning is paramount for enhancing the reliability and interpretability of predictive models, especially in high-stakes real-world scenarios. Despite the availability of numerous methods, they often pose a trade-off between the quality of uncertainty estimation and computational efficiency. Addressing this challenge, we present an adaptation of the Multiple-Input Multiple-Output (MIMO) framework -- an approach exploiting the overparameterization of deep neural networks -- for pixel-wise regression tasks. Our MIMO variant expands the applicability of the approach from simple image classification to broader computer vision domains. For that purpose, we adapted the U-Net architecture to train multiple subnetworks within a single model, harnessing the overparameterization in deep neural networks. Additionally, we introduce a novel procedure for synchronizing subnetwork performance within the MIMO framework. Our comprehensive evaluations of the resulting MIMO U-Net on two orthogonal datasets demonstrate comparable accuracy to existing models, superior calibration on in-distribution data, robust out-of-distribution detection capabilities, and considerable improvements in parameter size and inference time. Code available at github.com/antonbaumann/MIMO-Unet
</details>
<details>
<summary>摘要</summary>
Machine learning 中的不确定性估计是对预测模型的可靠性和可解释性提高的重要环节,特别是在高度的实际应用场景中。Despite the numerous methods available, they often involve a trade-off between the quality of uncertainty estimation and computational efficiency. To address this challenge, we present an adaptation of the Multiple-Input Multiple-Output (MIMO) framework for pixel-wise regression tasks. Our MIMO variant expands the applicability of the approach from simple image classification to broader computer vision domains. To achieve this, we adapted the U-Net architecture to train multiple subnetworks within a single model, leveraging the overparameterization in deep neural networks. Additionally, we propose a novel procedure for synchronizing subnetwork performance within the MIMO framework. Our comprehensive evaluations of the resulting MIMO U-Net on two orthogonal datasets demonstrate comparable accuracy to existing models, superior calibration on in-distribution data, robust out-of-distribution detection capabilities, and significant improvements in parameter size and inference time. 相关代码可以在github.com/antonbaumann/MIMO-Unet 上找到。
</details></li>
</ul>
<hr>
<h2 id="Large-kernel-Attention-for-Efficient-and-Robust-Brain-Lesion-Segmentation"><a href="#Large-kernel-Attention-for-Efficient-and-Robust-Brain-Lesion-Segmentation" class="headerlink" title="Large-kernel Attention for Efficient and Robust Brain Lesion Segmentation"></a>Large-kernel Attention for Efficient and Robust Brain Lesion Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07251">http://arxiv.org/abs/2308.07251</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liamchalcroft/mdunet">https://github.com/liamchalcroft/mdunet</a></li>
<li>paper_authors: Liam Chalcroft, Ruben Lourenço Pereira, Mikael Brudfors, Andrew S. Kayser, Mark D’Esposito, Cathy J. Price, Ioannis Pappas, John Ashburner</li>
<li>for: 这个论文是为了解决医疗影像分类 задачі中的深度学习模型效果不高、缺乏调节不变性的问题。</li>
<li>methods: 这个论文提出了一种基于全条件对称化Transformer块的U-Net架构，以模型3D脑膜疾病分类中的长距离互动。</li>
<li>results: 这个模型能够提供最大的折衔点，即性能与现有State-of-the-art相当，并且具有调节不变性和对称性的优点。<details>
<summary>Abstract</summary>
Vision transformers are effective deep learning models for vision tasks, including medical image segmentation. However, they lack efficiency and translational invariance, unlike convolutional neural networks (CNNs). To model long-range interactions in 3D brain lesion segmentation, we propose an all-convolutional transformer block variant of the U-Net architecture. We demonstrate that our model provides the greatest compromise in three factors: performance competitive with the state-of-the-art; parameter efficiency of a CNN; and the favourable inductive biases of a transformer. Our public implementation is available at https://github.com/liamchalcroft/MDUNet .
</details>
<details>
<summary>摘要</summary>
“vision transformer”是深度学习模型，用于视觉任务，包括医疗图像分割。然而，它缺乏效率和翻译不变性，与卷积神经网络（CNN）不同。为了在3D脑损害分割中模型长距离交互，我们提议一种具有U-Net架构的所有卷积转换器块变体。我们示示了我们的模型在三个因素中均提供了最大的妥协：与现状前景竞争性的性能; 参数效率与CNN相同; 以及转换器的有利假设。我们的公共实现可以在https://github.com/liamchalcroft/MDUNet上找到。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/15/eess.IV_2023_08_15/" data-id="clorjzlfo015mf1886fyldk4u" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/58/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/57/">57</a><a class="page-number" href="/page/58/">58</a><span class="page-number current">59</span><a class="page-number" href="/page/60/">60</a><a class="page-number" href="/page/61/">61</a><span class="space">&hellip;</span><a class="page-number" href="/page/89/">89</a><a class="extend next" rel="next" href="/page/60/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">60</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">118</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">58</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
