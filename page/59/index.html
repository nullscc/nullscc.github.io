
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/59/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.LG_2023_09_02" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/02/cs.LG_2023_09_02/" class="article-date">
  <time datetime="2023-09-02T10:00:00.000Z" itemprop="datePublished">2023-09-02</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/02/cs.LG_2023_09_02/">cs.LG - 2023-09-02</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Efficient-Covariance-Matrix-Reconstruction-with-Iterative-Spatial-Spectrum-Sampling"><a href="#Efficient-Covariance-Matrix-Reconstruction-with-Iterative-Spatial-Spectrum-Sampling" class="headerlink" title="Efficient Covariance Matrix Reconstruction with Iterative Spatial Spectrum Sampling"></a>Efficient Covariance Matrix Reconstruction with Iterative Spatial Spectrum Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01040">http://arxiv.org/abs/2309.01040</a></li>
<li>repo_url: None</li>
<li>paper_authors: S. Mohammadzadeh, V. H. Nascimento, R. C. de Lamare, O. Kukrer</li>
<li>for: 本研究提出了一种可靠且cost-effective的 beamforming算法设计方法，用于防止附近干扰信号的扩散。</li>
<li>methods: 本研究使用了有效的covariance矩阵重建方法，基于iterative空间功率spectrum（CMR-ISPS）。这种方法可以重建干扰信号plus noise covariance矩阵（INC），并使用最大Entropy功率spectrumdensity函数来形态指向响应。</li>
<li>results: 实验结果表明，提出的CMR-ISPS抑制器可以快速地防止附近干扰信号的扩散，并且可以在不同的干扰信号水平下提供相应的抗干扰性能。<details>
<summary>Abstract</summary>
This work presents a cost-effective technique for designing robust adaptive beamforming algorithms based on efficient covariance matrix reconstruction with iterative spatial power spectrum (CMR-ISPS). The proposed CMR-ISPS approach reconstructs the interference-plus-noise covariance (INC) matrix based on a simplified maximum entropy power spectral density function that can be used to shape the directional response of the beamformer. Firstly, we estimate the directions of arrival (DoAs) of the interfering sources with the available snapshots. We then develop an algorithm to reconstruct the INC matrix using a weighted sum of outer products of steering vectors whose coefficients can be estimated in the vicinity of the DoAs of the interferences which lie in a small angular sector. We also devise a cost-effective adaptive algorithm based on conjugate gradient techniques to update the beamforming weights and a method to obtain estimates of the signal of interest (SOI) steering vector from the spatial power spectrum. The proposed CMR-ISPS beamformer can suppress interferers close to the direction of the SOI by producing notches in the directional response of the array with sufficient depths. Simulation results are provided to confirm the validity of the proposed method and make a comparison to existing approaches
</details>
<details>
<summary>摘要</summary>
First, the directions of arrival (DoAs) of the interfering sources are estimated using the available snapshots. Then, an algorithm is developed to reconstruct the INC matrix using a weighted sum of outer products of steering vectors whose coefficients can be estimated in the vicinity of the DoAs of the interferences, which lie in a small angular sector.Additionally, a cost-effective adaptive algorithm based on conjugate gradient techniques is proposed to update the beamforming weights, and a method to obtain estimates of the signal of interest (SOI) steering vector from the spatial power spectrum. The proposed CMR-ISPS beamformer can effectively suppress interferers close to the direction of the SOI by producing notches in the directional response of the array with sufficient depths.Simulation results are provided to confirm the validity of the proposed method and compare it to existing approaches. The proposed technique offers a cost-effective and robust solution for adaptive beamforming in the presence of interference.
</details></li>
</ul>
<hr>
<h2 id="Online-Adaptive-Mahalanobis-Distance-Estimation"><a href="#Online-Adaptive-Mahalanobis-Distance-Estimation" class="headerlink" title="Online Adaptive Mahalanobis Distance Estimation"></a>Online Adaptive Mahalanobis Distance Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01030">http://arxiv.org/abs/2309.01030</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lianke Qin, Aravind Reddy, Zhao Song</li>
<li>For: This paper is written for studying dimension reduction for Mahalanobis metrics and providing efficient data structures for solving the Approximate Distance Estimation (ADE) problem for Mahalanobis distances.* Methods: The paper uses randomized Monte Carlo data structures and adapts it to handle sequences of adaptive queries and online updates to the Mahalanobis metric matrix and the data points.* Results: The paper provides efficient data structures for solving the ADE problem for Mahalanobis distances, which can be used in conjunction with prior algorithms for online learning of Mahalanobis metrics.<details>
<summary>Abstract</summary>
Mahalanobis metrics are widely used in machine learning in conjunction with methods like $k$-nearest neighbors, $k$-means clustering, and $k$-medians clustering. Despite their importance, there has not been any prior work on applying sketching techniques to speed up algorithms for Mahalanobis metrics. In this paper, we initiate the study of dimension reduction for Mahalanobis metrics. In particular, we provide efficient data structures for solving the Approximate Distance Estimation (ADE) problem for Mahalanobis distances. We first provide a randomized Monte Carlo data structure. Then, we show how we can adapt it to provide our main data structure which can handle sequences of \textit{adaptive} queries and also online updates to both the Mahalanobis metric matrix and the data points, making it amenable to be used in conjunction with prior algorithms for online learning of Mahalanobis metrics.
</details>
<details>
<summary>摘要</summary>
马哈拉诺比斯度量广泛应用在机器学习中，常与 $k$-最近邻、$k$-集群和 $k$-中值集群一起使用。尽管其重要性，但是没有任何之前的研究把笔记技术应用于快速化马哈拉诺比斯度量算法。在这篇论文中，我们开始研究维度减少 для马哈拉诺比斯度量。具体来说，我们提供了高效的数据结构来解决 Approximate Distance Estimation（ADE）问题。我们首先提供了随机 Monte Carlo 数据结构。然后，我们如何将其改进，以满足适应性查询和在 Mahalanobis 度量矩阵和数据点上进行在线更新，使其适用于与之前的在线学习 Mahalanobis 度量算法。
</details></li>
</ul>
<hr>
<h2 id="On-the-training-and-generalization-of-deep-operator-networks"><a href="#On-the-training-and-generalization-of-deep-operator-networks" class="headerlink" title="On the training and generalization of deep operator networks"></a>On the training and generalization of deep operator networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01020">http://arxiv.org/abs/2309.01020</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Sanghyun Lee, Yeonjong Shin</li>
<li>for: 这个论文是为了提出一种新的深度运算网络（DeepONets）训练方法。</li>
<li>methods: 这种训练方法包括先训练树网络，然后顺序训练分支网络。这种方法的核心思想是利用分治法将复杂的训练任务分解成两个子任务，从而降低训练的复杂性。</li>
<li>results: 该训练方法可以在各种情况下提高DeepONets的稳定性和泛化能力，并且可以更好地处理各种非线性和非对易性问题。<details>
<summary>Abstract</summary>
We present a novel training method for deep operator networks (DeepONets), one of the most popular neural network models for operators. DeepONets are constructed by two sub-networks, namely the branch and trunk networks. Typically, the two sub-networks are trained simultaneously, which amounts to solving a complex optimization problem in a high dimensional space. In addition, the nonconvex and nonlinear nature makes training very challenging. To tackle such a challenge, we propose a two-step training method that trains the trunk network first and then sequentially trains the branch network. The core mechanism is motivated by the divide-and-conquer paradigm and is the decomposition of the entire complex training task into two subtasks with reduced complexity. Therein the Gram-Schmidt orthonormalization process is introduced which significantly improves stability and generalization ability. On the theoretical side, we establish a generalization error estimate in terms of the number of training data, the width of DeepONets, and the number of input and output sensors. Numerical examples are presented to demonstrate the effectiveness of the two-step training method, including Darcy flow in heterogeneous porous media.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的训练方法 для深度运算网络（DeepONets），这是一种非常流行的神经网络模型。DeepONets由两个子网络组成：分支网络和主网络。通常情况下，这两个子网络同时进行训练，这等于在高维空间中解决一个复杂的优化问题。此外，由于非对称和非线性的性质，训练非常困难。为解决这个挑战，我们提出了一种分两步训练方法，先训练主网络，然后顺序训练分支网络。这种机制的核心思想是分而治之的方法，即将整个复杂的训练任务分解成两个子任务，每个子任务都有较低的复杂性。在这个过程中，我们引入了 Gram-Schmidt 正交化过程，这有助于提高稳定性和泛化能力。从理论角度来看，我们建立了一个泛化误差估计，其与训练数据量、深度网络宽度和输入和输出感知器数量有关。数据示范中，我们展示了这种两步训练方法的效果，包括 Darcy 流在不同的孔隙媒质中。
</details></li>
</ul>
<hr>
<h2 id="MPTopic-Improving-topic-modeling-via-Masked-Permuted-pre-training"><a href="#MPTopic-Improving-topic-modeling-via-Masked-Permuted-pre-training" class="headerlink" title="MPTopic: Improving topic modeling via Masked Permuted pre-training"></a>MPTopic: Improving topic modeling via Masked Permuted pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01015">http://arxiv.org/abs/2309.01015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinche Zhang, Evangelos milios</li>
<li>For: The paper aims to improve the quality of topic modeling in text analysis by addressing the limitations of existing methods such as BERTopic and Top2Vec.* Methods: The paper introduces a new approach called TF-RDF (Term Frequency - Relative Document Frequency) to assess the relevance of terms within a document, and uses this approach to drive a clustering algorithm called MPTopic.* Results: The paper shows that the topic keywords identified using MPTopic and TF-RDF outperform those extracted by BERTopic and Top2Vec through comprehensive evaluation.Here’s the same information in Simplified Chinese:* For: 论文目的是为了提高文本分析中的话题模型质量，并且解决现有方法如BERTopic和Top2Vec的局限性。* Methods: 论文引入了一种新的方法 called TF-RDF (文档频次-相对文档频次)，用于评估文档中 термина的 relevance，并使用这种方法驱动一种名为 MPTopic 的聚类算法。* Results: 论文表明，使用 MPTopic 和 TF-RDF 提取的话题关键词比 BERTopic 和 Top2Vec 提取的词语要出色。<details>
<summary>Abstract</summary>
Topic modeling is pivotal in discerning hidden semantic structures within texts, thereby generating meaningful descriptive keywords. While innovative techniques like BERTopic and Top2Vec have recently emerged in the forefront, they manifest certain limitations. Our analysis indicates that these methods might not prioritize the refinement of their clustering mechanism, potentially compromising the quality of derived topic clusters. To illustrate, Top2Vec designates the centroids of clustering results to represent topics, whereas BERTopic harnesses C-TF-IDF for its topic extraction.In response to these challenges, we introduce "TF-RDF" (Term Frequency - Relative Document Frequency), a distinctive approach to assess the relevance of terms within a document. Building on the strengths of TF-RDF, we present MPTopic, a clustering algorithm intrinsically driven by the insights of TF-RDF. Through comprehensive evaluation, it is evident that the topic keywords identified with the synergy of MPTopic and TF-RDF outperform those extracted by both BERTopic and Top2Vec.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Streaming-Active-Learning-for-Regression-Problems-Using-Regression-via-Classification"><a href="#Streaming-Active-Learning-for-Regression-Problems-Using-Regression-via-Classification" class="headerlink" title="Streaming Active Learning for Regression Problems Using Regression via Classification"></a>Streaming Active Learning for Regression Problems Using Regression via Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01013">http://arxiv.org/abs/2309.01013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shota Horiguchi, Kota Dohi, Yohei Kawaguchi</li>
<li>for: 本研究旨在提出一种基于流动学习的回归方法，以提高回归模型在不同环境下的性能。</li>
<li>methods: 本研究使用了流动活动学习方法，其中将回归问题转化为分类问题，然后应用直接使用了流动活动学习方法。</li>
<li>results: 实验结果表明，提出的方法可以在同等级别的注释成本下实现更高的回归精度。<details>
<summary>Abstract</summary>
One of the challenges in deploying a machine learning model is that the model's performance degrades as the operating environment changes. To maintain the performance, streaming active learning is used, in which the model is retrained by adding a newly annotated sample to the training dataset if the prediction of the sample is not certain enough. Although many streaming active learning methods have been proposed for classification, few efforts have been made for regression problems, which are often handled in the industrial field. In this paper, we propose to use the regression-via-classification framework for streaming active learning for regression. Regression-via-classification transforms regression problems into classification problems so that streaming active learning methods proposed for classification problems can be applied directly to regression problems. Experimental validation on four real data sets shows that the proposed method can perform regression with higher accuracy at the same annotation cost.
</details>
<details>
<summary>摘要</summary>
一个机器学习模型的挑战是其性能随环境变化而下降。为维护性能，流动活动学习被使用，其中模型通过添加新的注释样本到训练集来重新训练，如果预测样本的准确性不够高 enough。虽然许多流动活动学习方法已经为分类问题提出，但对于回归问题，业界上的尝试不多。本文提出使用回归via分类框架来实现流动活动学习回归。回归via分类将回归问题转化为分类问题，从而可以直接应用流动活动学习方法，提高回归的准确性。实验 validate on four real data sets 显示，提出的方法可以在同样的注释成本下实现更高的回归精度。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-sparsity-and-class-sparsity-priors-for-dictionary-learning-and-coding"><a href="#Bayesian-sparsity-and-class-sparsity-priors-for-dictionary-learning-and-coding" class="headerlink" title="Bayesian sparsity and class sparsity priors for dictionary learning and coding"></a>Bayesian sparsity and class sparsity priors for dictionary learning and coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00999">http://arxiv.org/abs/2309.00999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alberto Bocchinfuso, Daniela Calvetti, Erkki Somersalo</li>
<li>for:  solves challenging inverse problems using dictionary learning methods</li>
<li>methods:  uses sparse coding techniques and dictionary compression to reduce computational complexity</li>
<li>results:  effectively identifies relevant subdictionaries and reduces computational complexity in real-world applications such as glitch detection and hyperspectral remote sensing<details>
<summary>Abstract</summary>
Dictionary learning methods continue to gain popularity for the solution of challenging inverse problems. In the dictionary learning approach, the computational forward model is replaced by a large dictionary of possible outcomes, and the problem is to identify the dictionary entries that best match the data, akin to traditional query matching in search engines. Sparse coding techniques are used to guarantee that the dictionary matching identifies only few of the dictionary entries, and dictionary compression methods are used to reduce the complexity of the matching problem. In this article, we propose a work flow to facilitate the dictionary matching process. First, the full dictionary is divided into subdictionaries that are separately compressed. The error introduced by the dictionary compression is handled in the Bayesian framework as a modeling error. Furthermore, we propose a new Bayesian data-driven group sparsity coding method to help identify subdictionaries that are not relevant for the dictionary matching. After discarding irrelevant subdictionaries, the dictionary matching is addressed as a deflated problem using sparse coding. The compression and deflation steps can lead to substantial decreases of the computational complexity. The effectiveness of compensating for the dictionary compression error and using the novel group sparsity promotion to deflate the original dictionary are illustrated by applying the methodology to real world problems, the glitch detection in the LIGO experiment and hyperspectral remote sensing.
</details>
<details>
<summary>摘要</summary>
字典学习方法继续受欢迎用于解决困难的反问题。在字典学习方法中，计算前方模型被替换为一个大字典的可能结果，问题是将字典条目与数据匹配，类似于传统的查询匹配在搜索引擎中。稀盐编码技术用于保证字典匹配只找到少量的字典条目，而字典压缩方法用于减少匹配问题的复杂性。在这篇文章中，我们提出一个工作流程来促进字典匹配过程。首先，全字典被分解成分字典，并将每个分字典独立压缩。 dictionary compression error 被处理在 bayesian 框架中作为模型误差。此外，我们提出了一种新的 bayesian 数据驱动的群 sparse coding 方法，以帮助标识不相关的分字典。 после将不相关的分字典排除，字典匹配被视为一个减少的问题，使用稀盐编码进行解决。压缩和减少步骤可能会导致计算复杂性的明显减少。我们通过应用方法到实际问题，如 LIGO 实验中的雷达检测和Remote sensing 中的 Hyperspectral 检测，来证明资料做准的补偿和使用新的群 sparse coding 促进法可以减少计算复杂性。
</details></li>
</ul>
<hr>
<h2 id="Switch-and-Conquer-Efficient-Algorithms-By-Switching-Stochastic-Gradient-Oracles-For-Decentralized-Saddle-Point-Problems"><a href="#Switch-and-Conquer-Efficient-Algorithms-By-Switching-Stochastic-Gradient-Oracles-For-Decentralized-Saddle-Point-Problems" class="headerlink" title="Switch and Conquer: Efficient Algorithms By Switching Stochastic Gradient Oracles For Decentralized Saddle Point Problems"></a>Switch and Conquer: Efficient Algorithms By Switching Stochastic Gradient Oracles For Decentralized Saddle Point Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00997">http://arxiv.org/abs/2309.00997</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chhavisharma123/c-dpssg-cdc2023">https://github.com/chhavisharma123/c-dpssg-cdc2023</a></li>
<li>paper_authors: Chhavi Sharma, Vishnu Narayanan, P. Balamurugan</li>
<li>for: 这个论文targets non-smooth strongly convex-strongly concave saddle point problems in a decentralized setting without a central server.</li>
<li>methods:  authors propose an inexact primal dual hybrid gradient (inexact PDHG) procedure that allows generic gradient computation oracles to update the primal and dual variables.</li>
<li>results:  authors prove that the proposed algorithm, Decentralized Proximal Switching Stochastic Gradient method with Compression (C-DPSSG), converges to an $\epsilon$-accurate saddle point solution with linear rate, and the algorithm is well suited for obtaining solutions of low&#x2F;medium accuracy faster.Here is the format you requested:</li>
<li>for: &lt;what are the paper written for?&gt;</li>
<li>methods: &lt;what methods the paper use?&gt;</li>
<li>results: &lt;what results the paper get?&gt;I hope this helps!<details>
<summary>Abstract</summary>
We consider a class of non-smooth strongly convex-strongly concave saddle point problems in a decentralized setting without a central server. To solve a consensus formulation of problems in this class, we develop an inexact primal dual hybrid gradient (inexact PDHG) procedure that allows generic gradient computation oracles to update the primal and dual variables. We first investigate the performance of inexact PDHG with stochastic variance reduction gradient (SVRG) oracle. Our numerical study uncovers a significant phenomenon of initial conservative progress of iterates of IPDHG with SVRG oracle. To tackle this, we develop a simple and effective switching idea, where a generalized stochastic gradient (GSG) computation oracle is employed to hasten the iterates' progress to a saddle point solution during the initial phase of updates, followed by a switch to the SVRG oracle at an appropriate juncture. The proposed algorithm is named Decentralized Proximal Switching Stochastic Gradient method with Compression (C-DPSSG), and is proven to converge to an $\epsilon$-accurate saddle point solution with linear rate. Apart from delivering highly accurate solutions, our study reveals that utilizing the best convergence phases of GSG and SVRG oracles makes C-DPSSG well suited for obtaining solutions of low/medium accuracy faster, useful for certain applications. Numerical experiments on two benchmark machine learning applications show C-DPSSG's competitive performance which validate our theoretical findings. The codes used in the experiments can be found \href{https://github.com/chhavisharma123/C-DPSSG-CDC2023}{here}.
</details>
<details>
<summary>摘要</summary>
我们考虑一类非滑坡强弱缓衡点问题在分布式设置中，无中央服务器。以解决这类问题的协议形式，我们发展了一个不精确的内部预测点数值变化（inexact PDHG）程式，允许普通的梯度计算实体更新内部预测点和梯度。我们首先研究对不精确PDHG使用测量噪声减少梯度（SVRG）实体的性能。我们的数据研究发现在追踪过程中，对于IPDHG的初始阶段，实际上存在较大的保守进步。为了解决这个问题，我们提出了一个简单有效的转换想法，其中在初始阶段使用一个通用梯度计算实体（GSG）来增加积分进步，然后在适当的时刻转换到SVRG实体。我们给这个算法命名为分布式预测转换梯度法（C-DPSSG），并证明其可以在线性速率下落在ε-精确点解。此外，我们的研究发现，通过利用GSG和SVRG实体的最佳追踪阶段，C-DPSSG可以实现低/中精度更快的解决方案，对于一些应用而言是有用的。我们的实验结果显示C-DPSSG在两个机器学习应用中的竞争性表现，与我们的理论成果相符。实验代码可以在以下连结获取：<https://github.com/chhavisharma123/C-DPSSG-CDC2023>
</details></li>
</ul>
<hr>
<h2 id="A-Boosted-Machine-Learning-Framework-for-the-Improvement-of-Phase-and-Crystal-Structure-Prediction-of-High-Entropy-Alloys-Using-Thermodynamic-and-Configurational-Parameters"><a href="#A-Boosted-Machine-Learning-Framework-for-the-Improvement-of-Phase-and-Crystal-Structure-Prediction-of-High-Entropy-Alloys-Using-Thermodynamic-and-Configurational-Parameters" class="headerlink" title="A Boosted Machine Learning Framework for the Improvement of Phase and Crystal Structure Prediction of High Entropy Alloys Using Thermodynamic and Configurational Parameters"></a>A Boosted Machine Learning Framework for the Improvement of Phase and Crystal Structure Prediction of High Entropy Alloys Using Thermodynamic and Configurational Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00993">http://arxiv.org/abs/2309.00993</a></li>
<li>repo_url: None</li>
<li>paper_authors: Debsundar Dey, Suchandan Das, Anik Pal, Santanu Dey, Chandan Kumar Raul, Arghya Chatterjee</li>
<li>for: This paper aims to predict the phases and crystal structures of High-Entropy Alloys (HEAs) using machine learning (ML) techniques.</li>
<li>methods: The study employs five distinct boosting algorithms (XGBoost, LightGBM, Random Forest, Gradient Boosting, and CatBoost) to predict phases and crystal structures, and introduces a methodical framework using the Pearson correlation coefficient to select strongly co-related features for improved accuracy.</li>
<li>results: The study achieves an accuracy of 94.05% for phase prediction and 90.07% for crystal structure prediction, and provides a new approach to quantify the influence of parameters on the model’s accuracy.<details>
<summary>Abstract</summary>
The reason behind the remarkable properties of High-Entropy Alloys (HEAs) is rooted in the diverse phases and the crystal structures they contain. In the realm of material informatics, employing machine learning (ML) techniques to classify phases and crystal structures of HEAs has gained considerable significance. In this study, we assembled a new collection of 1345 HEAs with varying compositions to predict phases. Within this collection, there were 705 sets of data that were utilized to predict the crystal structures with the help of thermodynamics and electronic configuration. Our study introduces a methodical framework i.e., the Pearson correlation coefficient that helps in selecting the strongly co-related features to increase the prediction accuracy. This study employed five distinct boosting algorithms to predict phases and crystal structures, offering an enhanced guideline for improving the accuracy of these predictions. Among all these algorithms, XGBoost gives the highest accuracy of prediction (94.05%) for phases and LightGBM gives the highest accuracy of prediction of crystal structure of the phases (90.07%). The quantification of the influence exerted by parameters on the model's accuracy was conducted and a new approach was made to elucidate the contribution of individual parameters in the process of phase prediction and crystal structure prediction.
</details>
<details>
<summary>摘要</summary>
高级噪声合金（HEA）的很多特有性归因于它们包含多种相和晶体结构。在材料信息学领域，使用机器学习（ML）技术来分类HEA的相和晶体结构得到了广泛的应用。在这项研究中，我们组装了一个新的HEA合集，其中包含了不同组合的1345个HEA。其中，705个数据集用于预测晶体结构，并采用了热力学和电子配置来帮助预测。我们的研究框架包括用Pearson相关系数选择强相关特征，以提高预测精度。我们使用了五种不同的提升算法来预测相和晶体结构，其中XGBoost提供了预测相的最高精度（94.05%），而LightGBM提供了预测晶体结构的相最高精度（90.07%）。我们还对模型精度的影响因素进行了评估，并开发了一种新的方法来解释参数对预测相和晶体结构的贡献。
</details></li>
</ul>
<hr>
<h2 id="An-Ensemble-Score-Filter-for-Tracking-High-Dimensional-Nonlinear-Dynamical-Systems"><a href="#An-Ensemble-Score-Filter-for-Tracking-High-Dimensional-Nonlinear-Dynamical-Systems" class="headerlink" title="An Ensemble Score Filter for Tracking High-Dimensional Nonlinear Dynamical Systems"></a>An Ensemble Score Filter for Tracking High-Dimensional Nonlinear Dynamical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00983">http://arxiv.org/abs/2309.00983</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zezhongzhang/ensf">https://github.com/zezhongzhang/ensf</a></li>
<li>paper_authors: Feng Bao, Zezhong Zhang, Guannan Zhang</li>
<li>for: 解决高维非线性滤波问题的精度高的筛选方法</li>
<li>methods: 利用分数基本概率模型来描述滤波过程中的演化，并使用小批量 Monte Carlo 估计器直接估计分数函数，而不需要训练神经网络。</li>
<li>results: 在高维劳逊系统中，EnSF 可以可靠地跟踪高维非线性观测过程，并且可以提供高精度的滤波结果，这些问题都是现有滤波方法所面临的挑战。<details>
<summary>Abstract</summary>
We propose an ensemble score filter (EnSF) for solving high-dimensional nonlinear filtering problems with superior accuracy. A major drawback of existing filtering methods, e.g., particle filters or ensemble Kalman filters, is the low accuracy in handling high-dimensional and highly nonlinear problems. EnSF attacks this challenge by exploiting the score-based diffusion model, defined in a pseudo-temporal domain, to characterizing the evolution of the filtering density. EnSF stores the information of the recursively updated filtering density function in the score function, in stead of storing the information in a set of finite Monte Carlo samples (used in particle filters and ensemble Kalman filters). Unlike existing diffusion models that train neural networks to approximate the score function, we develop a training-free score estimation that uses mini-batch-based Monte Carlo estimator to directly approximate the score function at any pseudo-spatial-temporal location, which provides sufficient accuracy in solving high-dimensional nonlinear problems as well as saves tremendous amount of time spent on training neural networks. Another essential aspect of EnSF is its analytical update step, gradually incorporating data information into the score function, which is crucial in mitigating the degeneracy issue faced when dealing with very high-dimensional nonlinear filtering problems. High-dimensional Lorenz systems are used to demonstrate the performance of our method. EnSF provides surprisingly impressive performance in reliably tracking extremely high-dimensional Lorenz systems (up to 1,000,000 dimension) with highly nonlinear observation processes, which is a well-known challenging problem for existing filtering methods.
</details>
<details>
<summary>摘要</summary>
我们提出了一种ensemble score filter（EnSF），用于解决高维非线性筛选问题，提高准确性。现有的筛选方法，如 particile filter 或 ensemble Kalman filter，在处理高维高非线性问题时的准确性很低。EnSF 利用了分数基 diffusion 模型，在 pseudo-时间领域中定义了筛选演化的分数函数。EnSF 将筛选 densities 的信息存储在分数函数中，而不是使用finite Monte Carlo 样本（用于 particile filter 和 ensemble Kalman filter）。与现有的扩散模型不同，我们开发了一种无需训练的分数估计，使用 mini-batch-based Monte Carlo 估计器直接在任何 pseudo-空间时间位置上估计分数函数，这提供了足够的准确性来解决高维非线性问题，同时节省了训练神经网络所需的巨大时间。另一个关键特点是 EnSF 的分析更新步骤，逐步将数据信息 incorporated 到分数函数中，这是解决非线性问题时的重要问题。高维 Lorenz 系统被用来演示 EnSF 的性能，EnSF 在处理 extremely high-dimensional Lorenz 系统（达 1,000,000 维）的非线性观测过程中表现出非常出众的表现，这是现有筛选方法所面临的一个著名的挑战。
</details></li>
</ul>
<hr>
<h2 id="Pure-Message-Passing-Can-Estimate-Common-Neighbor-for-Link-Prediction"><a href="#Pure-Message-Passing-Can-Estimate-Common-Neighbor-for-Link-Prediction" class="headerlink" title="Pure Message Passing Can Estimate Common Neighbor for Link Prediction"></a>Pure Message Passing Can Estimate Common Neighbor for Link Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00976">http://arxiv.org/abs/2309.00976</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaiwen Dong, Zhichun Guo, Nitesh V. Chawla</li>
<li>for: 这个研究的目的是提高Message Passing Neural Networks（MPNNs）在链接预测 task 的表现，MPNNs 通常在这个任务中表现不佳，被简单的规律如 Common Neighbor（CN）所超越。</li>
<li>methods: 我们提出了一种基于 message-passing 的方法，即 Message Passing Link Predictor（MPLP），这个模型利用 quasi-orthogonal vectors 来估算链接级别的结构特征，同时保留 node-level 的复杂性。</li>
<li>results: 我们在不同领域的benchmark datasets上进行了实验，结果显示了我们的方法在预测链接任务中的出色表现，较基于方法的表现更好。<details>
<summary>Abstract</summary>
Message Passing Neural Networks (MPNNs) have emerged as the {\em de facto} standard in graph representation learning. However, when it comes to link prediction, they often struggle, surpassed by simple heuristics such as Common Neighbor (CN). This discrepancy stems from a fundamental limitation: while MPNNs excel in node-level representation, they stumble with encoding the joint structural features essential to link prediction, like CN. To bridge this gap, we posit that, by harnessing the orthogonality of input vectors, pure message-passing can indeed capture joint structural features. Specifically, we study the proficiency of MPNNs in approximating CN heuristics. Based on our findings, we introduce the Message Passing Link Predictor (MPLP), a novel link prediction model. MPLP taps into quasi-orthogonal vectors to estimate link-level structural features, all while preserving the node-level complexities. Moreover, our approach demonstrates that leveraging message-passing to capture structural features could offset MPNNs' expressiveness limitations at the expense of estimation variance. We conduct experiments on benchmark datasets from various domains, where our method consistently outperforms the baseline methods.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:message passing neural networks (MPNNs) 已经成为 graphs 表示学习的“de facto”标准，但当预测关系时，它们经常遇到问题，被简单的规律如共同邻居 (CN) 所超越。这个差异源于 MPNNs 在节点水平的表示方面 excellence，但它们在节点间的结构特征方面缺乏表达能力，如 CN。为了补偿这个差异，我们提出，通过利用输入vector的正交性，纯message-passing可以真正捕捉结构特征。我们进一步研究 MPNNs 在CN规律的近似方面的效能。根据我们的发现，我们引入了 Message Passing Link Predictor (MPLP)，一个新的预测关系模型。MPLP 利用 quasi-orthogonal vector 估计关系级别的结构特征，同时保留节点水平的复杂性。此外，我们的方法显示，通过将message-passing用于结构特征的捕捉，可以对 MPNNs 的表达能力进行补偿，即使是在估计误差方面。我们在不同领域的benchmark数据上进行了实验，我们的方法一致地超越了基eline方法。
</details></li>
</ul>
<hr>
<h2 id="Network-Topology-Inference-with-Sparsity-and-Laplacian-Constraints"><a href="#Network-Topology-Inference-with-Sparsity-and-Laplacian-Constraints" class="headerlink" title="Network Topology Inference with Sparsity and Laplacian Constraints"></a>Network Topology Inference with Sparsity and Laplacian Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00960">http://arxiv.org/abs/2309.00960</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxi Ying, Xi Han, Rui Zhou, Xiwen Wang, Hing Cheung So</li>
<li>for: 这篇论文旨在解决网络顶点推导问题，使用laplacian受限 Gaussian graphical models，将任务转换为精确度矩阵的估计。</li>
<li>methods: 本文提出了一种具有 $\ell_0$-norm条件的网络矩阵估计方法，通过gradient projection算法解决具有稀疏性和laplacian约束的优化问题。</li>
<li>results: numerical experiments表明，提案的方法能够有效地解决网络顶点推导问题，并且比traditional $\ell_1$-norm方法更加稳定和有效。<details>
<summary>Abstract</summary>
We tackle the network topology inference problem by utilizing Laplacian constrained Gaussian graphical models, which recast the task as estimating a precision matrix in the form of a graph Laplacian. Recent research \cite{ying2020nonconvex} has uncovered the limitations of the widely used $\ell_1$-norm in learning sparse graphs under this model: empirically, the number of nonzero entries in the solution grows with the regularization parameter of the $\ell_1$-norm; theoretically, a large regularization parameter leads to a fully connected (densest) graph. To overcome these challenges, we propose a graph Laplacian estimation method incorporating the $\ell_0$-norm constraint. An efficient gradient projection algorithm is developed to solve the resulting optimization problem, characterized by sparsity and Laplacian constraints. Through numerical experiments with synthetic and financial time-series datasets, we demonstrate the effectiveness of the proposed method in network topology inference.
</details>
<details>
<summary>摘要</summary>
我们解决网络顶点结构推论问题，利用laplacian受限 Gaussian graphical models，它将任务转换为估计一个矩阵precision matrix的graph Laplacian。最近的研究 \cite{ying2020nonconvex} 发现了 $\ell_1$ 条件下学习简短网络的limitation：实验中非零元素的数量随着调整参数增加;理论上，一个大的调整参数将导致一个最密集的网络。为了解决这些挑战，我们提议一个具有 $\ell_0$ 条件的网络Laplacian估计方法。我们开发了一个高效的梯度对应算法来解决这个估计问题，它具有简短和Laplacian的约束。通过实验证明，我们显示了我们的提议方法在网络顶点结构推论中的效果。
</details></li>
</ul>
<hr>
<h2 id="Index-aware-learning-of-circuits"><a href="#Index-aware-learning-of-circuits" class="headerlink" title="Index-aware learning of circuits"></a>Index-aware learning of circuits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00958">http://arxiv.org/abs/2309.00958</a></li>
<li>repo_url: None</li>
<li>paper_authors: Idoia Cortes Garcia, Peter Förster, Lennart Jansen, Wil Schilders, Sebastian Schöps</li>
<li>for: 本文旨在描述如何使用机器学习方法来优化电子电路设计，以及如何使用已有系统知识来减少学习的复杂性。</li>
<li>methods: 本文提出了一种基于分解理论的机器学习方法，该方法可以将电子电路描述为代数动态系统（DAE），然后使用分解理论来解释DAE中的隐藏约束。最后，该方法可以使用已有系统知识来重建代数变量，从而保证算法的准确性。</li>
<li>results: 本文的实验结果表明，使用该方法可以减少学习的复杂性，同时保证算法的准确性。这种方法可以用于各种电子电路设计问题，如电路优化、灵活性分析等。<details>
<summary>Abstract</summary>
Electrical circuits are present in a variety of technologies, making their design an important part of computer aided engineering. The growing number of tunable parameters that affect the final design leads to a need for new approaches of quantifying their impact. Machine learning may play a key role in this regard, however current approaches often make suboptimal use of existing knowledge about the system at hand. In terms of circuits, their description via modified nodal analysis is well-understood. This particular formulation leads to systems of differential-algebraic equations (DAEs) which bring with them a number of peculiarities, e.g. hidden constraints that the solution needs to fulfill. We aim to use the recently introduced dissection concept for DAEs that can decouple a given system into ordinary differential equations, only depending on differential variables, and purely algebraic equations that describe the relations between differential and algebraic variables. The idea then is to only learn the differential variables and reconstruct the algebraic ones using the relations from the decoupling. This approach guarantees that the algebraic constraints are fulfilled up to the accuracy of the nonlinear system solver, which represents the main benefit highlighted in this article.
</details>
<details>
<summary>摘要</summary>
电路设计是现代工程设计中的一个重要组成部分。随着参数的增加，电路设计的最终结果的影响需要新的方法来衡量其影响。机器学习可能会在这个领域发挥关键作用，但现有的方法常常不充分利用现有系统的知识。在电路方面，使用修改后的节点分析来描述电路是非常好的。这种形式化导致系统拥有偏微分方程（DAE），这些DAE具有一些特点，例如隐藏的约束，解决方案需要满足这些约束。我们想使用最近引入的分割概念来处理DAE，将系统分解成仅依赖于偏微分变量的普通偏微分方程，并且使用系统的关系来重建代数变量。这种方法保证了代数约束的满足，直到非线性系统解决器的精度，这是本文的主要优点。
</details></li>
</ul>
<hr>
<h2 id="Emergent-Linear-Representations-in-World-Models-of-Self-Supervised-Sequence-Models"><a href="#Emergent-Linear-Representations-in-World-Models-of-Self-Supervised-Sequence-Models" class="headerlink" title="Emergent Linear Representations in World Models of Self-Supervised Sequence Models"></a>Emergent Linear Representations in World Models of Self-Supervised Sequence Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00941">http://arxiv.org/abs/2309.00941</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ajyl/mech_int_othellogpt">https://github.com/ajyl/mech_int_othellogpt</a></li>
<li>paper_authors: Neel Nanda, Andrew Lee, Martin Wattenberg</li>
<li>for: 这 paper 是 investigate how sequence models represent their decision-making process, and provide evidence of a closely related linear representation of the board state.</li>
<li>methods: 这 paper 使用 Othello-playing neural network, and use probing to understand the model’s internal state.</li>
<li>results: 这 paper 得到了一个简单 yet powerful way to interpret the model’s internal state, and demonstrate that linear representations enable significant interpretability progress.Here’s the full text in Simplified Chinese:</li>
<li>for: 这 paper 是 investigate how sequence models represent their decision-making process, 和提供 evidence of a closely related linear representation of the board state.</li>
<li>methods: 这 paper 使用 Othello-playing neural network, 并使用 probing 来理解模型的内部状态.</li>
<li>results: 这 paper 得到了一个简单 yet powerful way to interpret the model’s internal state, 并 demonstrates that linear representations enable significant interpretability progress.<details>
<summary>Abstract</summary>
How do sequence models represent their decision-making process? Prior work suggests that Othello-playing neural network learned nonlinear models of the board state (Li et al., 2023). In this work, we provide evidence of a closely related linear representation of the board. In particular, we show that probing for "my colour" vs. "opponent's colour" may be a simple yet powerful way to interpret the model's internal state. This precise understanding of the internal representations allows us to control the model's behaviour with simple vector arithmetic. Linear representations enable significant interpretability progress, which we demonstrate with further exploration of how the world model is computed.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "How do sequence models represent their decision-making process? Prior work suggests that Othello-playing neural network learned nonlinear models of the board state (Li et al., 2023). In this work, we provide evidence of a closely related linear representation of the board. In particular, we show that probing for 'my colour' vs. 'opponent's colour' may be a simple yet powerful way to interpret the model's internal state. This precise understanding of the internal representations allows us to control the model's behaviour with simple vector arithmetic. Linear representations enable significant interpretability progress, which we demonstrate with further exploration of how the world model is computed." into Chinese (Simplified)Answer:sequence models的决策过程是如何表示？先前的工作表明，抽象棋盘 neural network 学习了非线性模型（Li et al., 2023）。在这项工作中，我们提供了一种相关的直线表示，具体来说，我们表明了 probing for "我的颜色" vs. "对手的颜色" 可能是一种简单却强大的内部状态的解释方法。这种精确的内部表示允许我们通过简单的矢量算术控制模型的行为。直线表示具有显著的可读性进步，我们通过进一步探索世界模型如何计算来证明这一点。
</details></li>
</ul>
<hr>
<h2 id="Short-term-power-load-forecasting-method-based-on-CNN-SAEDN-Res"><a href="#Short-term-power-load-forecasting-method-based-on-CNN-SAEDN-Res" class="headerlink" title="Short-term power load forecasting method based on CNN-SAEDN-Res"></a>Short-term power load forecasting method based on CNN-SAEDN-Res</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07140">http://arxiv.org/abs/2309.07140</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Cui, Han Zhu, Yijian Wang, Lu Zhang, Yang Li</li>
<li>for: 这篇研究目的是提出一种基于卷积神经网络、自注意编码器-解码器网络和差分优化（Res）的短期载电预测方法，以解决传统序列模型对于非时序因素资料的处理问题，提高预测精度。</li>
<li>methods: 本方法的特点包括使用两维卷积神经网络进行特征提取，并使用自注意编码器-解码器网络和差分优化模组进行预测。自注意编码器可以将高维特征转换为全局相互关联的数据，而差分优化模组可以确保预测结果的稳定性。</li>
<li>results:  simulation 结果显示，提出的载电预测方法在预测精度和预测稳定性方面具有明显的优势，比较之前的方法更能够捕捉非时序因素资料中的相互关联。<details>
<summary>Abstract</summary>
In deep learning, the load data with non-temporal factors are difficult to process by sequence models. This problem results in insufficient precision of the prediction. Therefore, a short-term load forecasting method based on convolutional neural network (CNN), self-attention encoder-decoder network (SAEDN) and residual-refinement (Res) is proposed. In this method, feature extraction module is composed of a two-dimensional convolutional neural network, which is used to mine the local correlation between data and obtain high-dimensional data features. The initial load fore-casting module consists of a self-attention encoder-decoder network and a feedforward neural network (FFN). The module utilizes self-attention mechanisms to encode high-dimensional features. This operation can obtain the global correlation between data. Therefore, the model is able to retain important information based on the coupling relationship between the data in data mixed with non-time series factors. Then, self-attention decoding is per-formed and the feedforward neural network is used to regression initial load. This paper introduces the residual mechanism to build the load optimization module. The module generates residual load values to optimize the initial load. The simulation results show that the proposed load forecasting method has advantages in terms of prediction accuracy and prediction stability.
</details>
<details>
<summary>摘要</summary>
在深度学习中，带有非时序因素的数据加载具有困难处理序列模型的问题。这种问题导致预测精度不够。因此，一种基于卷积神经网络（CNN）、自注意编码器解码网络（SAEDN）和剩余级修正（Res）的短期电力预测方法被提出。在这种方法中，特征提取模块由两维卷积神经网络组成，用于挖掘数据中的本地相关性，并从而获得高维数据特征。初始电力预测模块包括自注意编码器解码网络和Feedforward神经网络（FFN）。这个模块使用自注意机制编码高维特征，从而获得数据之间的全局相关性。因此，模型能够保留数据混合非时序因素的重要信息。然后，自注意解码被执行，并使用Feedforward神经网络进行回归初始电力。本文介绍了剩余机制来建立电力优化模块。该模块生成剩余电力值，以优化初始电力。实验结果显示，提议的电力预测方法具有更高的预测精度和预测稳定性。
</details></li>
</ul>
<hr>
<h2 id="A-Multi-Head-Ensemble-Multi-Task-Learning-Approach-for-Dynamical-Computation-Offloading"><a href="#A-Multi-Head-Ensemble-Multi-Task-Learning-Approach-for-Dynamical-Computation-Offloading" class="headerlink" title="A Multi-Head Ensemble Multi-Task Learning Approach for Dynamical Computation Offloading"></a>A Multi-Head Ensemble Multi-Task Learning Approach for Dynamical Computation Offloading</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00907">http://arxiv.org/abs/2309.00907</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qiyu3816/MTFNN-CO">https://github.com/qiyu3816/MTFNN-CO</a></li>
<li>paper_authors: Ruihuai Liang, Bo Yang, Zhiwen Yu, Xuelin Cao, Derrick Wing Kwan Ng, Chau Yuen</li>
<li>for: 这个研究旨在设计一个优化的负载协调策略，以提高移动 Multi-Access Edge Computing (MEC) 的性能。</li>
<li>methods: 这个研究使用了一个混合数据类型的非线性计划 (MINLP) 问题，并使用了一个深度神经网络 (DNN) 模型进行线上推导。</li>
<li>results: 这个研究所得到的结果显示，使用了多头组合多任务学习 (MEMTL) 方法可以对时间变化的环境进行快速的解决，并且可以实现高精度的推导。<details>
<summary>Abstract</summary>
Computation offloading has become a popular solution to support computationally intensive and latency-sensitive applications by transferring computing tasks to mobile edge servers (MESs) for execution, which is known as mobile/multi-access edge computing (MEC). To improve the MEC performance, it is required to design an optimal offloading strategy that includes offloading decision (i.e., whether offloading or not) and computational resource allocation of MEC. The design can be formulated as a mixed-integer nonlinear programming (MINLP) problem, which is generally NP-hard and its effective solution can be obtained by performing online inference through a well-trained deep neural network (DNN) model. However, when the system environments change dynamically, the DNN model may lose efficacy due to the drift of input parameters, thereby decreasing the generalization ability of the DNN model. To address this unique challenge, in this paper, we propose a multi-head ensemble multi-task learning (MEMTL) approach with a shared backbone and multiple prediction heads (PHs). Specifically, the shared backbone will be invariant during the PHs training and the inferred results will be ensembled, thereby significantly reducing the required training overhead and improving the inference performance. As a result, the joint optimization problem for offloading decision and resource allocation can be efficiently solved even in a time-varying wireless environment. Experimental results show that the proposed MEMTL outperforms benchmark methods in both the inference accuracy and mean square error without requiring additional training data.
</details>
<details>
<summary>摘要</summary>
computation offloading 已成为支持 computationally intensive 和延迟敏感应用的受欢迎解决方案，通过将计算任务传输到 mobil edge server (MES) 进行执行，这被称为 mobil/多 access edge computing (MEC)。为了提高 MEC 性能，需要设计一个优化的卸载策略，包括卸载决策（是否卸载）和 MEC 的计算资源分配。该设计可以表示为混合整数非线性编程 (MINLP) 问题，通常是NP-hard 的，其有效解决方法是通过在训练了深度神经网络 (DNN) 模型的线上推理进行获取。然而，当系统环境变化 dynamically 时，DNN 模型可能会失去有效性，因为输入参数的漂移，从而降低 DNN 模型的泛化能力。为解决这个特殊挑战，在这篇论文中，我们提出了一种多头集合多任务学习 (MEMTL) 方法，其特点是共享脊梁和多个预测头 (PH)。具体来说，共享脊梁在 PH 训练时保持不变，并将推理结果ensemble，从而减少了训练负担和提高了推理性能。因此，在时变无线环境中，可以效率地解决卸载决策和资源分配的共优化问题。实验结果表明，提出的 MEMTL 方法在推理准确率和平均方差Error 方面具有显著优势，而无需更多的训练数据。
</details></li>
</ul>
<hr>
<h2 id="Discovering-Predictive-Relational-Object-Symbols-with-Symbolic-Attentive-Layers"><a href="#Discovering-Predictive-Relational-Object-Symbols-with-Symbolic-Attentive-Layers" class="headerlink" title="Discovering Predictive Relational Object Symbols with Symbolic Attentive Layers"></a>Discovering Predictive Relational Object Symbols with Symbolic Attentive Layers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00889">http://arxiv.org/abs/2309.00889</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alper Ahmetoglu, Batuhan Celik, Erhan Oztop, Emre Ugur</li>
<li>for: 这项研究的目的是开发一种新的深度学习架构，用于自动发现对象和其关系的符号表示。</li>
<li>methods: 该模型使用了一个自我注意层，计算了对象特征中的关注量，并将关注量用于对象符号的聚合和行为效果预测。</li>
<li>results: 实验表明，该模型在一个 simulate 的表格环境中，能够更好地预测行为效果，同时同时自动发现对象符号和关系符号。分析表明，学习的符号与表格环境中对象之间的相对位置、物品类型和横向Alignment有关。<details>
<summary>Abstract</summary>
In this paper, we propose and realize a new deep learning architecture for discovering symbolic representations for objects and their relations based on the self-supervised continuous interaction of a manipulator robot with multiple objects on a tabletop environment. The key feature of the model is that it can handle a changing number number of objects naturally and map the object-object relations into symbolic domain explicitly. In the model, we employ a self-attention layer that computes discrete attention weights from object features, which are treated as relational symbols between objects. These relational symbols are then used to aggregate the learned object symbols and predict the effects of executed actions on each object. The result is a pipeline that allows the formation of object symbols and relational symbols from a dataset of object features, actions, and effects in an end-to-end manner. We compare the performance of our proposed architecture with state-of-the-art symbol discovery methods in a simulated tabletop environment where the robot needs to discover symbols related to the relative positions of objects to predict the observed effect successfully. Our experiments show that the proposed architecture performs better than other baselines in effect prediction while forming not only object symbols but also relational symbols. Furthermore, we analyze the learned symbols and relational patterns between objects to learn about how the model interprets the environment. Our analysis shows that the learned symbols relate to the relative positions of objects, object types, and their horizontal alignment on the table, which reflect the regularities in the environment.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的深度学习架构，用于从自适应的 kontinuous 互动中找到对象和它们之间的关系的符号表示。这个模型的关键特点是可以自然地处理变化数量的对象，并将对象之间的关系Explicitly map到符号领域中。在模型中，我们使用了一层自注意力层，从对象特征中计算出精确的注意力权重，这些注意力权重被视为对象之间的关系符号。这些关系符号然后用于聚合学习的对象符号和预测对象上执行的效果。这个管道可以从对象特征、动作和效果的数据集中形成对象符号和关系符号，并在端到端的方式下进行结构化的符号探索。我们与其他基eline进行比较，并在模拟的桌面环境中证明了我们提出的架构的性能比其他基eline更好，可以成功预测对象之间的关系和效果。此外，我们分析了模型中学习的符号和对象之间的关系，发现符号与对象的相对位置、对象类型和桌面上的水平对齐有关，这些符号与环境中的常见性相符。
</details></li>
</ul>
<hr>
<h2 id="Tight-Bounds-for-Machine-Unlearning-via-Differential-Privacy"><a href="#Tight-Bounds-for-Machine-Unlearning-via-Differential-Privacy" class="headerlink" title="Tight Bounds for Machine Unlearning via Differential Privacy"></a>Tight Bounds for Machine Unlearning via Differential Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00886">http://arxiv.org/abs/2309.00886</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiyang Huang, Clément L. Canonne</li>
<li>for: 这个论文探讨了一种名为”机器忘记”的概念，即在训练数据中删除一些点的问题。</li>
<li>methods: 作者使用了幂等私钥技术（DP）来实现机器忘记。</li>
<li>results: 作者 closing the gap between upper and lower bounds on the deletion capacity of DP-based machine unlearning algorithms, obtaining tight bounds on the deletion capacity achievable by these algorithms.<details>
<summary>Abstract</summary>
We consider the formulation of "machine unlearning" of Sekhari, Acharya, Kamath, and Suresh (NeurIPS 2021), which formalizes the so-called "right to be forgotten" by requiring that a trained model, upon request, should be able to "unlearn" a number of points from the training data, as if they had never been included in the first place. Sekhari et al. established some positive and negative results about the number of data points that can be successfully unlearnt by a trained model without impacting the model's accuracy (the "deletion capacity"), showing that machine unlearning could be achieved by using differentially private (DP) algorithms. However, their results left open a gap between upper and lower bounds on the deletion capacity of these algorithms: our work fully closes this gap, obtaining tight bounds on the deletion capacity achievable by DP-based machine unlearning algorithms.
</details>
<details>
<summary>摘要</summary>
我团队考虑了Sekhari等人（NeurIPS 2021）所提出的机器“忘记” formalization，即要求已经训练过的模型，在请求时，能够“忘记”一些训练数据点，如果这些点从来没有被包含在模型中。Sekhari等人确立了一些积极和消极结果，表明可以通过使用匿名隐私（DP）算法实现机器忘记。然而，他们的结果留下了一个DP算法的删除容量（deletion capacity）的上下限之间的差距：我们的工作完全关闭了这个差距，得到了DP基于的机器忘记算法的精确 deletion capacity 上限。
</details></li>
</ul>
<hr>
<h2 id="Towards-Certified-Probabilistic-Robustness-with-High-Accuracy"><a href="#Towards-Certified-Probabilistic-Robustness-with-High-Accuracy" class="headerlink" title="Towards Certified Probabilistic Robustness with High Accuracy"></a>Towards Certified Probabilistic Robustness with High Accuracy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00879">http://arxiv.org/abs/2309.00879</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruihan Zhang, Peixin Zhang, Jun Sun</li>
<li>for: This paper aims to build certifiably robust yet accurate neural network models, which is an open problem in the field of adversarial examples.</li>
<li>methods: The proposed approach consists of two parts: a probabilistic robust training method that minimizes variance in terms of divergence, and a runtime inference method for certified probabilistic robustness of the prediction.</li>
<li>results: The proposed approach significantly outperforms existing approaches in terms of both certification rate and accuracy, and is reasonably efficient. The approach works for a variety of perturbations and is applicable to multiple models trained on different datasets.<details>
<summary>Abstract</summary>
Adversarial examples pose a security threat to many critical systems built on neural networks (such as face recognition systems, and self-driving cars). While many methods have been proposed to build robust models, how to build certifiably robust yet accurate neural network models remains an open problem. For example, adversarial training improves empirical robustness, but they do not provide certification of the model's robustness. On the other hand, certified training provides certified robustness but at the cost of a significant accuracy drop. In this work, we propose a novel approach that aims to achieve both high accuracy and certified probabilistic robustness. Our method has two parts, i.e., a probabilistic robust training method with an additional goal of minimizing variance in terms of divergence and a runtime inference method for certified probabilistic robustness of the prediction. The latter enables efficient certification of the model's probabilistic robustness at runtime with statistical guarantees. This is supported by our training objective, which minimizes the variance of the model's predictions in a given vicinity, derived from a general definition of model robustness. Our approach works for a variety of perturbations and is reasonably efficient. Our experiments on multiple models trained on different datasets demonstrate that our approach significantly outperforms existing approaches in terms of both certification rate and accuracy.
</details>
<details>
<summary>摘要</summary>
遭遇攻击性示例对许多基于神经网络的重要系统（如识别面部系统和自动驾驶车）的安全性带来了威胁。许多方法已经被提出来建立坚固的模型，但是如何建立认证可靠且精确的神经网络模型仍然是一个开启的问题。例如，敌对训练可以提高了实际的抗衡能力，但它们不会提供模型的认证 robustness。另一方面，认证训练则可以提供认证的 robustness，但是它们会导致模型的精确度下降。在这个工作中，我们提出了一个新的方法，旨在实现高精确度和认证可靠的神经网络模型。我们的方法有两部分：一个是一种概率 robust 的训练方法，另一个是一种runtime inference方法，用于认证模型的概率 robustness。这个方法可以实现在不同类型的攻击下，且是相对高效的。我们在多个模型和不同的数据集上进行了实验，结果显示，我们的方法在认证率和精确度两方面都大大超过了现有的方法。
</details></li>
</ul>
<hr>
<h2 id="Pretraining-Representations-for-Bioacoustic-Few-shot-Detection-using-Supervised-Contrastive-Learning"><a href="#Pretraining-Representations-for-Bioacoustic-Few-shot-Detection-using-Supervised-Contrastive-Learning" class="headerlink" title="Pretraining Representations for Bioacoustic Few-shot Detection using Supervised Contrastive Learning"></a>Pretraining Representations for Bioacoustic Few-shot Detection using Supervised Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00878">http://arxiv.org/abs/2309.00878</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ilyassmoummad/dcase23_task5_scl">https://github.com/ilyassmoummad/dcase23_task5_scl</a></li>
<li>paper_authors: Ilyass Moummad, Romain Serizel, Nicolas Farrugia</li>
<li>for: 这个论文的目的是提出一种基于几何学习的听写音频数据分类方法，以解决生物听音应用中的听音事件检测问题。</li>
<li>methods: 该方法利用了数据增强和监睹学习框架，从零开始学习一个精彩特征提取器。</li>
<li>results: 该方法在验证集上获得了63.46%的F1分数，在测试集上获得了42.7%的F1分数，在DCASE挑战中名列第二。<details>
<summary>Abstract</summary>
Deep learning has been widely used recently for sound event detection and classification. Its success is linked to the availability of sufficiently large datasets, possibly with corresponding annotations when supervised learning is considered. In bioacoustic applications, most tasks come with few labelled training data, because annotating long recordings is time consuming and costly. Therefore supervised learning is not the best suited approach to solve bioacoustic tasks. The bioacoustic community recasted the problem of sound event detection within the framework of few-shot learning, i.e. training a system with only few labeled examples. The few-shot bioacoustic sound event detection task in the DCASE challenge focuses on detecting events in long audio recordings given only five annotated examples for each class of interest. In this paper, we show that learning a rich feature extractor from scratch can be achieved by leveraging data augmentation using a supervised contrastive learning framework. We highlight the ability of this framework to transfer well for five-shot event detection on previously unseen classes in the training data. We obtain an F-score of 63.46\% on the validation set and 42.7\% on the test set, ranking second in the DCASE challenge. We provide an ablation study for the critical choices of data augmentation techniques as well as for the learning strategy applied on the training set.
</details>
<details>
<summary>摘要</summary>
现代深度学习技术在声音事件检测和分类方面得到了广泛应用。其成功与具有足够大的数据集，可能带有相应的注释时supervised learning是考虑的。在生物声学应用中，大多数任务都有少量标注的训练数据，因为注释长录音是时间consuming和costly。因此，supervised learning不是解决生物声学任务的最佳方法。生物声学社区将声音事件检测问题重新定义为few-shot learning问题，即使用只有几个标注的示例来训练系统。DCASE挑战中的声音事件检测五个难题中的few-shot bioacoustic sound event detection task是检测长录音中的事件，只需五个标注示例。在这篇论文中，我们表明了可以通过利用数据增强和supervised contrastive learning框架来学习rich feature extractor从scratch。我们指出了这种框架的可轻 Transfer Learning，能够在未看过的类型上进行五个shot事件检测。我们在验证集上取得了63.46%的F-score和42.7%的测试集F-score，在DCASE挑战中排名第二。我们还提供了关键的数据增强技术和训练集上的学习策略的ablation study。
</details></li>
</ul>
<hr>
<h2 id="Tutorial-a-priori-estimation-of-sample-size-effect-size-and-statistical-power-for-cluster-analysis-latent-class-analysis-and-multivariate-mixture-models"><a href="#Tutorial-a-priori-estimation-of-sample-size-effect-size-and-statistical-power-for-cluster-analysis-latent-class-analysis-and-multivariate-mixture-models" class="headerlink" title="Tutorial: a priori estimation of sample size, effect size, and statistical power for cluster analysis, latent class analysis, and multivariate mixture models"></a>Tutorial: a priori estimation of sample size, effect size, and statistical power for cluster analysis, latent class analysis, and multivariate mixture models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00866">http://arxiv.org/abs/2309.00866</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/esdalmaijer/cluster_power_tutorial">https://github.com/esdalmaijer/cluster_power_tutorial</a></li>
<li>paper_authors: Edwin S Dalmaijer</li>
<li>For: The paper is written for researchers who want to determine the sample size and effect size for analyses that identify subgroups.* Methods: The paper provides a roadmap for determining sample size and effect size using a procedure that formalizes expectations about effect sizes in a specific domain, and establishes the minimum sample size for subgroup analyses using simulations.* Results: The paper provides a reference table for the most popular subgroup analyses, including k-means, Ward agglomerative hierarchical clustering, c-means fuzzy clustering, latent class analysis, latent profile analysis, and Gaussian mixture modeling. The table shows the minimum numbers of observations per expected subgroup and features to achieve acceptable statistical power.<details>
<summary>Abstract</summary>
Before embarking on data collection, researchers typically compute how many individual observations they should do. This is vital for doing studies with sufficient statistical power, and often a cornerstone in study pre-registrations and grant applications. For traditional statistical tests, one would typically determine an acceptable level of statistical power, (gu)estimate effect size, and then use both values to compute the required sample size. However, for analyses that identify subgroups, statistical power is harder to establish. Once sample size reaches a sufficient threshold, effect size is primarily determined by the number of measured features and the underlying subgroup separation. As a consequence, a priory computations of statistical power are notoriously complex. In this tutorial, I will provide a roadmap to determining sample size and effect size for analyses that identify subgroups. First, I introduce a procedure that allows researchers to formalise their expectations about effect sizes in their domain of choice, and use this to compute the minimally required number of measured variables. Next, I outline how to establish the minimum sample size in subgroup analyses. Finally, I use simulations to provide a reference table for the most popular subgroup analyses: k-means, Ward agglomerative hierarchical clustering, c-means fuzzy clustering, latent class analysis, latent profile analysis, and Gaussian mixture modelling. The table shows the minimum numbers of observations per expected subgroup (sample size) and features (measured variables) to achieve acceptable statistical power, and can be readily used in study design.
</details>
<details>
<summary>摘要</summary>
Before starting data collection, researchers usually calculate how many individual observations they should collect. This is crucial for conducting studies with sufficient statistical power, and is often a key component of study pre-registrations and grant applications. For traditional statistical tests, one would typically determine an acceptable level of statistical power, estimate effect size, and then use both values to compute the required sample size. However, for analyses that identify subgroups, statistical power is more difficult to establish. Once the sample size reaches a sufficient threshold, effect size is primarily determined by the number of measured features and the underlying subgroup separation. As a consequence, a priori computations of statistical power are notoriously complex. In this tutorial, I will provide a roadmap to determining sample size and effect size for analyses that identify subgroups. First, I introduce a procedure that allows researchers to formalize their expectations about effect sizes in their domain of choice, and use this to compute the minimally required number of measured variables. Next, I outline how to establish the minimum sample size in subgroup analyses. Finally, I use simulations to provide a reference table for the most popular subgroup analyses: k-means, Ward agglomerative hierarchical clustering, c-means fuzzy clustering, latent class analysis, latent profile analysis, and Gaussian mixture modeling. The table shows the minimum numbers of observations per expected subgroup (sample size) and features (measured variables) to achieve acceptable statistical power, and can be readily used in study design.
</details></li>
</ul>
<hr>
<h2 id="DoRA-Domain-Based-Self-Supervised-Learning-Framework-for-Low-Resource-Real-Estate-Appraisal"><a href="#DoRA-Domain-Based-Self-Supervised-Learning-Framework-for-Low-Resource-Real-Estate-Appraisal" class="headerlink" title="DoRA: Domain-Based Self-Supervised Learning Framework for Low-Resource Real Estate Appraisal"></a>DoRA: Domain-Based Self-Supervised Learning Framework for Low-Resource Real Estate Appraisal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00855">http://arxiv.org/abs/2309.00855</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wwweiwei/dora">https://github.com/wwweiwei/dora</a></li>
<li>paper_authors: Wei-Wei Du, Wei-Yao Wang, Wen-Chih Peng</li>
<li>for: 这个论文旨在提供一种基于领域知识的自我监督学习框架，用于低资源的不动产评估。</li>
<li>methods: 该模型使用内样本地理预测作为预tex task，并使用 между样本对比学习来增强表示的泛化能力。</li>
<li>results: 对实际交易数据进行测试，该模型在几 shot enario下显著超过了基于数据表格的SSL基线、图形基本方法和指导方法。<details>
<summary>Abstract</summary>
The marketplace system connecting demands and supplies has been explored to develop unbiased decision-making in valuing properties. Real estate appraisal serves as one of the high-cost property valuation tasks for financial institutions since it requires domain experts to appraise the estimation based on the corresponding knowledge and the judgment of the market. Existing automated valuation models reducing the subjectivity of domain experts require a large number of transactions for effective evaluation, which is predominantly limited to not only the labeling efforts of transactions but also the generalizability of new developing and rural areas. To learn representations from unlabeled real estate sets, existing self-supervised learning (SSL) for tabular data neglects various important features, and fails to incorporate domain knowledge. In this paper, we propose DoRA, a Domain-based self-supervised learning framework for low-resource Real estate Appraisal. DoRA is pre-trained with an intra-sample geographic prediction as the pretext task based on the metadata of the real estate for equipping the real estate representations with prior domain knowledge. Furthermore, inter-sample contrastive learning is employed to generalize the representations to be robust for limited transactions of downstream tasks. Our benchmark results on three property types of real-world transactions show that DoRA significantly outperforms the SSL baselines for tabular data, the graph-based methods, and the supervised approaches in the few-shot scenarios by at least 7.6% for MAPE, 11.59% for MAE, and 3.34% for HR10%. We expect DoRA to be useful to other financial practitioners with similar marketplace applications who need general models for properties that are newly built and have limited records. The source code is available at https://github.com/wwweiwei/DoRA.
</details>
<details>
<summary>摘要</summary>
marketplace系统连接需求和供应，以发展不偏袋折的决策方法。房地产评估作为高成本房产评估任务，需要域专家根据相关知识和市场判断来进行估价。现有的自动评估模型可以减少域专家的主观性，但它们需要大量的交易数据进行有效评估，这是限制了不仅标注努力，还限制了新规划和农村地区的普适性。为了学习不标注的房地产集合中的表示，现有的自动学习（SSL）技术对于表格数据 neglects 多种重要特征，并且无法包含域知识。在这篇论文中，我们提出了DoRA，一种基于域的自动学习框架，用于低资源房地产评估。DoRA通过 metadata 中的地理预测任务进行预训练，以具备房地产表示的先验知识。此外，我们还使用了交叉样本学习来使表示扩展到有限交易下的稳定性。我们对实际交易中的三种不同类型的财产进行了测试，结果显示DoRA在几个shot scenario下明显超过了SSL基线、图表基eline和批处理方法的性能，提高了MAPЭ、MAE和HR10的性能。我们预计DoRA将对其他金融实践人员有用，他们需要面临新建和有限记录的财产评估模型。代码可以在 <https://github.com/wwweiwei/DoRA> 获取。
</details></li>
</ul>
<hr>
<h2 id="A-Unifying-Variational-Framework-for-Gaussian-Process-Motion-Planning"><a href="#A-Unifying-Variational-Framework-for-Gaussian-Process-Motion-Planning" class="headerlink" title="A Unifying Variational Framework for Gaussian Process Motion Planning"></a>A Unifying Variational Framework for Gaussian Process Motion Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00854">http://arxiv.org/abs/2309.00854</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas Cosier, Rares Iordan, Sicelukwanda Zwane, Giovanni Franzese, James T. Wilson, Marc Peter Deisenroth, Alexander Terenin, Yasemin Bekiroglu</li>
<li>for: 本文为了提出一种基于Variational Gaussian Processes的机器人运动规划框架，以解决机器人运动规划问题中的各种约束和不确定性问题。</li>
<li>methods: 本文使用Variational Gaussian Processes来处理机器人运动规划问题，并提出了一种结合概率推理的框架来处理等式、不等式和软运动约束。</li>
<li>results: 实验结果表明， compared to基准方法，本文提出的方法可以更好地平衡成功率和运动规划质量。<details>
<summary>Abstract</summary>
To control how a robot moves, motion planning algorithms must compute paths in high-dimensional state spaces while accounting for physical constraints related to motors and joints, generating smooth and stable motions, avoiding obstacles, and preventing collisions. A motion planning algorithm must therefore balance competing demands, and should ideally incorporate uncertainty to handle noise, model errors, and facilitate deployment in complex environments. To address these issues, we introduce a framework for robot motion planning based on variational Gaussian Processes, which unifies and generalizes various probabilistic-inference-based motion planning algorithms. Our framework provides a principled and flexible way to incorporate equality-based, inequality-based, and soft motion-planning constraints during end-to-end training, is straightforward to implement, and provides both interval-based and Monte-Carlo-based uncertainty estimates. We conduct experiments using different environments and robots, comparing against baseline approaches based on the feasibility of the planned paths, and obstacle avoidance quality. Results show that our proposed approach yields a good balance between success rates and path quality.
</details>
<details>
<summary>摘要</summary>
要控制 robot 的移动，动作规划算法需要计算高维状态空间中的路径，同时考虑到机械制约和 JOINTS 的物理约束，生成平滑和稳定的动作，避免障碍物和冲突。一个动作规划算法应该平衡竞合的需求，并应该包含不确定性，以处理噪声、模型错误和复杂环境中的部署。为解决这些问题，我们介绍了基于Variational Gaussian Processes的机器人动作规划框架，这个框架统一和总结了各种基于概率推理的动作规划算法。我们的框架可以在终端训练中采用等价、不等价和软动作规划约束，并提供了间隔型和Monte Carlo 类型的不确定性估计。我们在不同的环境和机器人上进行了实验，与基线方法进行比较，评价计划路径的可行性和避免障碍质量。结果表明，我们的提议方法可以获得良好的平衡，同时保证动作质量。
</details></li>
</ul>
<hr>
<h2 id="Autonomous-Soft-Tissue-Retraction-Using-Demonstration-Guided-Reinforcement-Learning"><a href="#Autonomous-Soft-Tissue-Retraction-Using-Demonstration-Guided-Reinforcement-Learning" class="headerlink" title="Autonomous Soft Tissue Retraction Using Demonstration-Guided Reinforcement Learning"></a>Autonomous Soft Tissue Retraction Using Demonstration-Guided Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00837">http://arxiv.org/abs/2309.00837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amritpal Singh, Wenqi Shi, May D Wang</li>
<li>for: 这个论文的目的是为了研究和开发一种能够处理软体的手术机器人系统。</li>
<li>methods: 这个论文使用了ROS相容的物理 simulate环境，以及使用示例导引学习（RL）算法来学习软体交互。</li>
<li>results: 这个研究实现了一种自动化手术软体压缩的方法，并证明了这种方法的可行性。<details>
<summary>Abstract</summary>
In the context of surgery, robots can provide substantial assistance by performing small, repetitive tasks such as suturing, needle exchange, and tissue retraction, thereby enabling surgeons to concentrate on more complex aspects of the procedure. However, existing surgical task learning mainly pertains to rigid body interactions, whereas the advancement towards more sophisticated surgical robots necessitates the manipulation of soft bodies. Previous work focused on tissue phantoms for soft tissue task learning, which can be expensive and can be an entry barrier to research. Simulation environments present a safe and efficient way to learn surgical tasks before their application to actual tissue. In this study, we create a Robot Operating System (ROS)-compatible physics simulation environment with support for both rigid and soft body interactions within surgical tasks. Furthermore, we investigate the soft tissue interactions facilitated by the patient-side manipulator of the DaVinci surgical robot. Leveraging the pybullet physics engine, we simulate kinematics and establish anchor points to guide the robotic arm when manipulating soft tissue. Using demonstration-guided reinforcement learning (RL) algorithms, we investigate their performance in comparison to traditional reinforcement learning algorithms. Our in silico trials demonstrate a proof-of-concept for autonomous surgical soft tissue retraction. The results corroborate the feasibility of learning soft body manipulation through the application of reinforcement learning agents. This work lays the foundation for future research into the development and refinement of surgical robots capable of managing both rigid and soft tissue interactions. Code is available at https://github.com/amritpal-001/tissue_retract.
</details>
<details>
<summary>摘要</summary>
在外科领域，机器人可以提供重要的协助，包括进行小、重复的任务，如缝合、针替换和组织吸引，以便外科医生能够更专注于更复杂的过程。然而，现有的外科任务学习主要关注坚体交互，而随着外科机器人的发展，需要涉及到软体的操作。之前的工作主要集中在假体中学习软组织任务，这可能会昂贵并成为研究入门障碍。在这种情况下，我们创建了ROS兼容的物理 simulate环境，并支持坚体和软体交互在外科任务中。此外，我们通过DaVinci外科机器人的病人侧把手 investigate软组织交互的可能性。通过pybullet物理引擎，我们模拟了机械学和确定了引导外科机器人的软组织 manipulate的anchor点。使用示例导引学习（RL）算法，我们研究其性能与传统RL算法相比。我们的室内实验结果表明，通过应用RL代理人，可以实现自主的外科软组织吸引。这些结果证明了在应用RL算法时，可以学习软体操作。这项工作为未来关于开发和改进外科机器人的研究提供了基础。代码可以在https://github.com/amritpal-001/tissue_retract中找到。
</details></li>
</ul>
<hr>
<h2 id="Approximating-Fair-k-Min-Sum-Radii-in-mathbb-R-d"><a href="#Approximating-Fair-k-Min-Sum-Radii-in-mathbb-R-d" class="headerlink" title="Approximating Fair $k$-Min-Sum-Radii in $\mathbb{R}^d$"></a>Approximating Fair $k$-Min-Sum-Radii in $\mathbb{R}^d$</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00834">http://arxiv.org/abs/2309.00834</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Drexler, Annika Hennes, Abhiruk Lahiri, Melanie Schmidt, Julian Wargalla<br>for:* The paper is focused on the $k$-min-sum-radii problem in the context of fair clustering.methods:* The paper proposes a PTAS (Probably Approximately Correct) algorithm for the fair $k$-min-sum-radii problem in Euclidean spaces of arbitrary dimension, with a constant number of clusters $k$.results:* The proposed algorithm is the first PTAS for the fair $k$-min-sum-radii problem, and it works for different notions of group fairness.<details>
<summary>Abstract</summary>
The $k$-center problem is a classical clustering problem in which one is asked to find a partitioning of a point set $P$ into $k$ clusters such that the maximum radius of any cluster is minimized. It is well-studied. But what if we add up the radii of the clusters instead of only considering the cluster with maximum radius? This natural variant is called the $k$-min-sum-radii problem. It has become the subject of more and more interest in recent years, inspiring the development of approximation algorithms for the $k$-min-sum-radii problem in its plain version as well as in constrained settings.   We study the problem for Euclidean spaces $\mathbb{R}^d$ of arbitrary dimension but assume the number $k$ of clusters to be constant. In this case, a PTAS for the problem is known (see Bandyapadhyay, Lochet and Saurabh, SoCG, 2023). Our aim is to extend the knowledge base for $k$-min-sum-radii to the domain of fair clustering. We study several group fairness constraints, such as the one introduced by Chierichetti et al. (NeurIPS, 2017). In this model, input points have an additional attribute (e.g., colors such as red and blue), and clusters have to preserve the ratio between different attribute values (e.g., have the same fraction of red and blue points as the ground set). Different variants of this general idea have been studied in the literature. To the best of our knowledge, no approximative results for the fair $k$-min-sum-radii problem are known, despite the immense amount of work on the related fair $k$-center problem.   We propose a PTAS for the fair $k$-min-sum-radii problem in Euclidean spaces of arbitrary dimension for the case of constant $k$. To the best of our knowledge, this is the first PTAS for the problem. It works for different notions of group fairness.
</details>
<details>
<summary>摘要</summary>
“$k$-中心问题”是一个热门的聚集问题，要找到一个分 partitioning 方案，使得该集合中的各个对象的最大半径 minimized。这个问题已经很受欢迎，但如果我们总和所有对象的半径而不是仅考虑最大半径，这就是“$k$-min-sum-radii”问题。这个问题在最近的几年中已经引起了越来越多的关注，并且开发了访问算法。我们在 $\mathbb{R}^d$ 的任意维度上研究这个问题，并假设对象的数量是常数的。在这种情况下，我们知道PTAS的存在（见 Bandyapadhyay、Lochet 和 Saurabh， SoCG， 2023）。我们的目标是将这个知识库扩展到公平聚集领域。我们研究了许多公平聚集约束，例如 Chierichetti 等人（NeurIPS， 2017）提出的一种模型，在这个模型中，输入对象有一个额外的特征（例如颜色，如红色和蓝色），并且要求各个集合保持不同特征值的比例（例如输入集合中的红色和蓝色对象的比例和输入集合中的红色和蓝色对象的比例一样）。不同的这种一般的想法已经在文献中被研究。我们提出了一个PTAS для公平的 $k$-min-sum-radii 问题。这是我们知道的第一个PTAS。它适用于不同的公平性观念。
</details></li>
</ul>
<hr>
<h2 id="Trustworthiness-Driven-Graph-Convolutional-Networks-for-Signed-Network-Embedding"><a href="#Trustworthiness-Driven-Graph-Convolutional-Networks-for-Signed-Network-Embedding" class="headerlink" title="Trustworthiness-Driven Graph Convolutional Networks for Signed Network Embedding"></a>Trustworthiness-Driven Graph Convolutional Networks for Signed Network Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00816">http://arxiv.org/abs/2309.00816</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kmj0792/trustsgcn">https://github.com/kmj0792/trustsgcn</a></li>
<li>paper_authors: Min-Jeong Kim, Yeon-Chang Lee, David Y. Kang, Sang-Wook Kim</li>
<li>for: 本文targets the problem of representing nodes in a signed network as low-dimensional vectors, and proposes a novel GCN-based approach named TrustSGCN to correct for incorrect embedding propagation.</li>
<li>methods: 本文提出了三个模块：生成每个节点的扩展 egonetwork（M1），测量边签信任度（M2），和基于信任度的协同嵌入传播（M3）。</li>
<li>results: 实验结果显示，TrustSGCN在四个真实的签名网络 dataset 上Consistently outperforms five state-of-the-art GCN-based SNE methods。<details>
<summary>Abstract</summary>
The problem of representing nodes in a signed network as low-dimensional vectors, known as signed network embedding (SNE), has garnered considerable attention in recent years. While several SNE methods based on graph convolutional networks (GCN) have been proposed for this problem, we point out that they significantly rely on the assumption that the decades-old balance theory always holds in the real-world. To address this limitation, we propose a novel GCN-based SNE approach, named as TrustSGCN, which corrects for incorrect embedding propagation in GCN by utilizing the trustworthiness on edge signs for high-order relationships inferred by the balance theory. The proposed approach consists of three modules: (M1) generation of each node's extended ego-network; (M2) measurement of trustworthiness on edge signs; and (M3) trustworthiness-aware propagation of embeddings. Furthermore, TrustSGCN learns the node embeddings by leveraging two well-known societal theories, i.e., balance and status. The experiments on four real-world signed network datasets demonstrate that TrustSGCN consistently outperforms five state-of-the-art GCN-based SNE methods. The code is available at https://github.com/kmj0792/TrustSGCN.
</details>
<details>
<summary>摘要</summary>
“简洁网络图（Signed Network）内的节点表示为低维Vector的问题，称为简洁网络嵌入（SNE），在最近的几年中受到了很大的关注。然而，许多基于图像润满网络（GCN）的SNE方法已经被提出，但是它们假设了现今已经很长时间的平衡理论应用于实际中。为了解决这个限制，我们提出了一个新的GCN基于的SNE方法，名为信任GCN（TrustSGCN），它通过使用对端标签的信任性来修正GCN中的嵌入传播。提案的方法包括三个模组：（M1）每个节点的扩展EGO网络的生成；（M2）根据端标签的信任性量度；以及（M3）基于信任性的嵌入传播。此外，TrustSGCN利用了社会学中两个著名的理论，即平衡和社会地位，来学习节点的嵌入。实验结果显示，TrustSGCN在四个真实的简洁网络数据集上显著地超越了五个现有的GCN基于的SNE方法。代码可以在https://github.com/kmj0792/TrustSGCN中找到。”
</details></li>
</ul>
<hr>
<h2 id="Fairness-Implications-of-Heterogeneous-Treatment-Effect-Estimation-with-Machine-Learning-Methods-in-Policy-making"><a href="#Fairness-Implications-of-Heterogeneous-Treatment-Effect-Estimation-with-Machine-Learning-Methods-in-Policy-making" class="headerlink" title="Fairness Implications of Heterogeneous Treatment Effect Estimation with Machine Learning Methods in Policy-making"></a>Fairness Implications of Heterogeneous Treatment Effect Estimation with Machine Learning Methods in Policy-making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00805">http://arxiv.org/abs/2309.00805</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patrick Rehill, Nicholas Biddle</li>
<li>for: The paper is written for governments trying to make and implement policy using causal machine learning methods, and for researchers and practitioners working in this area.</li>
<li>methods: The paper discusses the use of AI Fairness methods to protect against unintended consequences in machine learning models, but argues that these methods are not suitable for all causal machine learning applications. Instead, the paper proposes a definition of fairness for indirect decision-making scenarios, where the causal machine learning model only has indirect power.</li>
<li>results: The paper argues that the complexity of causal machine learning models can make it difficult to achieve fairness in policy-making, and suggests that careful modelling and awareness of decision-making biases are necessary to address this challenge.<details>
<summary>Abstract</summary>
Causal machine learning methods which flexibly generate heterogeneous treatment effect estimates could be very useful tools for governments trying to make and implement policy. However, as the critical artificial intelligence literature has shown, governments must be very careful of unintended consequences when using machine learning models. One way to try and protect against unintended bad outcomes is with AI Fairness methods which seek to create machine learning models where sensitive variables like race or gender do not influence outcomes. In this paper we argue that standard AI Fairness approaches developed for predictive machine learning are not suitable for all causal machine learning applications because causal machine learning generally (at least so far) uses modelling to inform a human who is the ultimate decision-maker while AI Fairness approaches assume a model that is making decisions directly. We define these scenarios as indirect and direct decision-making respectively and suggest that policy-making is best seen as a joint decision where the causal machine learning model usually only has indirect power. We lay out a definition of fairness for this scenario - a model that provides the information a decision-maker needs to accurately make a value judgement about just policy outcomes - and argue that the complexity of causal machine learning models can make this difficult to achieve. The solution here is not traditional AI Fairness adjustments, but careful modelling and awareness of some of the decision-making biases that these methods might encourage which we describe.
</details>
<details>
<summary>摘要</summary>
政府可以使用可变性机器学习方法来生成不同类型的干预效果估计，这些方法可能是政府制定和实施政策的有用工具。然而，根据人工智能文献所示，政府应该非常小心不良后果，因为机器学习模型可能会导致不良后果。一种方法是使用 AI Fairness 方法来创建不受敏感变量（如种族或性别）影响的机器学习模型。在这篇论文中，我们 argue That标准 AI Fairness 方法不适用于所有 causal machine learning 应用程序，因为 causal machine learning 通常（至少是）使用模型来告诉人类决策者做出决定。我们称这些场景为 indirect 和 direct 决策making 分别，并认为政策制定是 indirect 决策和 machine learning 模型通常只有 indirect 力量的共同决策。我们提出了一种公平定义 - 一个模型可以提供决策者准确地判断正确的政策结果的信息 - 并 argue  dass  causal machine learning 模型的复杂性可能使这困难实现。在这里，不是传统的 AI Fairness 调整，而是仔细的模型和决策BIAS 的认识，我们描述。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-and-Inverse-Problems"><a href="#Deep-Learning-and-Inverse-Problems" class="headerlink" title="Deep Learning and Inverse Problems"></a>Deep Learning and Inverse Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00802">http://arxiv.org/abs/2309.00802</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alexpapados/Physics-Informed-Deep-Learning-Solid-and-Fluid-Mechanics">https://github.com/alexpapados/Physics-Informed-Deep-Learning-Solid-and-Fluid-Mechanics</a></li>
<li>paper_authors: Ali Mohammad-Djafari, Ning Chu, Li Wang, Liang Yu</li>
<li>for: 这篇论文主要用于探讨深度学习（DL）和神经网络（NN）在反问题上的应用。</li>
<li>methods: 本文使用了NN和DLsurrogate模型，以及approximate计算来解决反问题。</li>
<li>results: 本文描述了两种情况：首先，使用已知前进算子作为物理约束的情况，其次更一般的数据驱动DL方法。<details>
<summary>Abstract</summary>
Machine Learning (ML) methods and tools have gained great success in many data, signal, image and video processing tasks, such as classification, clustering, object detection, semantic segmentation, language processing, Human-Machine interface, etc. In computer vision, image and video processing, these methods are mainly based on Neural Networks (NN) and in particular Convolutional NN (CNN), and more generally Deep NN. Inverse problems arise anywhere we have indirect measurement. As, in general, those inverse problems are ill-posed, to obtain satisfactory solutions for them needs prior information. Different regularization methods have been proposed, where the problem becomes the optimization of a criterion with a likelihood term and a regularization term. The main difficulty, however, in great dimensional real applications, remains the computational cost. Using NN, and in particular Deep Learning (DL) surrogate models and approximate computation, can become very helpful. In this work, we focus on NN and DL particularly adapted for inverse problems. We consider two cases: First the case where the forward operator is known and used as physics constraint, the second more general data driven DL methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="League-of-Legends-Real-Time-Result-Prediction"><a href="#League-of-Legends-Real-Time-Result-Prediction" class="headerlink" title="League of Legends: Real-Time Result Prediction"></a>League of Legends: Real-Time Result Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02449">http://arxiv.org/abs/2309.02449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jailson B. S. Junior, Claudio E. C. Campelo</li>
<li>for: 这个研究旨在使用机器学习技术预测电子游戏League of Legends（LoL）的赛事结果。</li>
<li>methods: 这个研究使用了未发表数据作为基础，考虑了不同变量和比赛阶段，以探索实时预测的能力。</li>
<li>results: 研究发现LightGBM模型在比赛中阶段时间占60%-80%时的平均准确率达81.62%，而逻辑回归和梯度抽象模型在早期比赛阶段表现更佳，得到了推动性的结果。<details>
<summary>Abstract</summary>
This paper presents a study on the prediction of outcomes in matches of the electronic game League of Legends (LoL) using machine learning techniques. With the aim of exploring the ability to predict real-time results, considering different variables and stages of the match, we highlight the use of unpublished data as a fundamental part of this process. With the increasing popularity of LoL and the emergence of tournaments, betting related to the game has also emerged, making the investigation in this area even more relevant. A variety of models were evaluated and the results were encouraging. A model based on LightGBM showed the best performance, achieving an average accuracy of 81.62\% in intermediate stages of the match when the percentage of elapsed time was between 60\% and 80\%. On the other hand, the Logistic Regression and Gradient Boosting models proved to be more effective in early stages of the game, with promising results. This study contributes to the field of machine learning applied to electronic games, providing valuable insights into real-time prediction in League of Legends. The results obtained may be relevant for both players seeking to improve their strategies and the betting industry related to the game.
</details>
<details>
<summary>摘要</summary>
The study evaluated a variety of models, and the results were encouraging. A LightGBM-based model achieved an average accuracy of 81.62% in intermediate stages of the match, when the percentage of elapsed time was between 60% and 80%. On the other hand, Logistic Regression and Gradient Boosting models were more effective in early stages of the game, with promising results.This study contributes to the field of machine learning applied to electronic games, providing valuable insights into real-time prediction in League of Legends. The results obtained may be relevant for both players seeking to improve their strategies and the betting industry related to the game.Translation notes:* "electronic game" is translated as "电子游戏" (diàn xī yóu xì)* "League of Legends" is translated as "英雄联盟" (yīng xióng lián méng)* "machine learning techniques" is translated as "机器学习技术" (jī shī xué xí jì shù)* "real-time results" is translated as "实时结果" (shí jī jié guǒ)* "unpublished data" is translated as "未发布数据" (wèi fā bìu xiàng xì)* "betting industry" is translated as "赌博业" (jià bò yè)
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Modeling-with-Domain-conditioned-Prior-Guidance-for-Accelerated-MRI-and-qMRI-Reconstruction"><a href="#Diffusion-Modeling-with-Domain-conditioned-Prior-Guidance-for-Accelerated-MRI-and-qMRI-Reconstruction" class="headerlink" title="Diffusion Modeling with Domain-conditioned Prior Guidance for Accelerated MRI and qMRI Reconstruction"></a>Diffusion Modeling with Domain-conditioned Prior Guidance for Accelerated MRI and qMRI Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00783">http://arxiv.org/abs/2309.00783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wanyu Bian, Albert Jang, Fang Liu</li>
<li>for: 这种方法是为了恢复图像，特别是在高速因素下进行恢复。</li>
<li>methods: 该方法基于一种吸引模型，该模型在数据领域中受到native数据的约束，并在频率和参数领域中使用域控制的扩散模型。使用了MRI物理学习 embeddings，以实现数据一致性和指导训练和抽样过程。</li>
<li>results: 该方法在多核磁共振和量化MRI恢复中显示出了显著的损害降低和精度保持，特别是在高速因素下。此外，该方法还可以在不同的解剖结构中维持高效率和准确性。<details>
<summary>Abstract</summary>
This study introduces a novel approach for image reconstruction based on a diffusion model conditioned on the native data domain. Our method is applied to multi-coil MRI and quantitative MRI reconstruction, leveraging the domain-conditioned diffusion model within the frequency and parameter domains. The prior MRI physics are used as embeddings in the diffusion model, enforcing data consistency to guide the training and sampling process, characterizing MRI k-space encoding in MRI reconstruction, and leveraging MR signal modeling for qMRI reconstruction. Furthermore, a gradient descent optimization is incorporated into the diffusion steps, enhancing feature learning and improving denoising. The proposed method demonstrates a significant promise, particularly for reconstructing images at high acceleration factors. Notably, it maintains great reconstruction accuracy and efficiency for static and quantitative MRI reconstruction across diverse anatomical structures. Beyond its immediate applications, this method provides potential generalization capability, making it adaptable to inverse problems across various domains.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Structured-Radial-Basis-Function-Network-Modelling-Diversity-for-Multiple-Hypotheses-Prediction"><a href="#Structured-Radial-Basis-Function-Network-Modelling-Diversity-for-Multiple-Hypotheses-Prediction" class="headerlink" title="Structured Radial Basis Function Network: Modelling Diversity for Multiple Hypotheses Prediction"></a>Structured Radial Basis Function Network: Modelling Diversity for Multiple Hypotheses Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00781">http://arxiv.org/abs/2309.00781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alejandro Rodriguez Dominguez, Muhammad Shahzad, Xia Hong</li>
<li>For: 这个研究旨在解决多modal regression问题，特别是预测非站ARY процес或具有复杂的混合分布。* Methods: 这个研究使用了一种组合多个假设预测器的结构化对�ishment（Radial Basis Function Network），并证明这个模型可以优化多个假设目标分布。* Results: 研究发现这个模型可以实现高度的普遍化表现和计算效率，并且只需使用两层神经网作为预测器即可控制多样性。此外，这个模型还可以使用梯度下降方法来实现损失无关的多个预测器。实验结果显示这个模型可以在Literature中优化表现。<details>
<summary>Abstract</summary>
Multi-modal regression is important in forecasting nonstationary processes or with a complex mixture of distributions. It can be tackled with multiple hypotheses frameworks but with the difficulty of combining them efficiently in a learning model. A Structured Radial Basis Function Network is presented as an ensemble of multiple hypotheses predictors for regression problems. The predictors are regression models of any type that can form centroidal Voronoi tessellations which are a function of their losses during training. It is proved that this structured model can efficiently interpolate this tessellation and approximate the multiple hypotheses target distribution and is equivalent to interpolating the meta-loss of the predictors, the loss being a zero set of the interpolation error. This model has a fixed-point iteration algorithm between the predictors and the centers of the basis functions. Diversity in learning can be controlled parametrically by truncating the tessellation formation with the losses of individual predictors. A closed-form solution with least-squares is presented, which to the authors knowledge, is the fastest solution in the literature for multiple hypotheses and structured predictions. Superior generalization performance and computational efficiency is achieved using only two-layer neural networks as predictors controlling diversity as a key component of success. A gradient-descent approach is introduced which is loss-agnostic regarding the predictors. The expected value for the loss of the structured model with Gaussian basis functions is computed, finding that correlation between predictors is not an appropriate tool for diversification. The experiments show outperformance with respect to the top competitors in the literature.
</details>
<details>
<summary>摘要</summary>
多Modal重要预测非站点过程或复杂的混合分布。它可以通过多个假设框架来解决，但是将其有效地结合到学习模型中是困难的。一种结构化圆拟函数网络是提出的一种多个假设预测器 ensemble for regression problems。这些预测器是任何类型的回归模型，可以形成中心 Voronoi 划分，这是在训练时的损失函数。 proved that this structured model can efficiently interpolate this tessellation and approximate the multiple hypotheses target distribution, and is equivalent to interpolating the meta-loss of the predictors, the loss being a zero set of the interpolation error. This model has a fixed-point iteration algorithm between the predictors and the centers of the basis functions. Diversity in learning can be controlled parametrically by truncating the tessellation formation with the losses of individual predictors. A closed-form solution with least-squares is presented, which to the authors' knowledge, is the fastest solution in the literature for multiple hypotheses and structured predictions. Using only two-layer neural networks as predictors, the model achieves superior generalization performance and computational efficiency, with diversity as a key component of success. A gradient-descent approach is introduced, which is loss-agnostic regarding the predictors. The expected value for the loss of the structured model with Gaussian basis functions is computed, finding that correlation between predictors is not an appropriate tool for diversification. Experimental results show outperformance with respect to the top competitors in the literature.
</details></li>
</ul>
<hr>
<h2 id="Non-Asymptotic-Bounds-for-Adversarial-Excess-Risk-under-Misspecified-Models"><a href="#Non-Asymptotic-Bounds-for-Adversarial-Excess-Risk-under-Misspecified-Models" class="headerlink" title="Non-Asymptotic Bounds for Adversarial Excess Risk under Misspecified Models"></a>Non-Asymptotic Bounds for Adversarial Excess Risk under Misspecified Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00771">http://arxiv.org/abs/2309.00771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Changyu Liu, Yuling Jiao, Junhui Wang, Jian Huang</li>
<li>for: 评估 robust 估计器的性能based on adversarial losses under misspecified models.</li>
<li>methods: 使用 distributional adversarial attack 和 adversarial training 进行评估.</li>
<li>results: 提出了一种通用的评估方法，并Establish non-asymptotic upper bounds for the adversarial excess risk associated with Lipschitz loss functions.<details>
<summary>Abstract</summary>
We propose a general approach to evaluating the performance of robust estimators based on adversarial losses under misspecified models. We first show that adversarial risk is equivalent to the risk induced by a distributional adversarial attack under certain smoothness conditions. This ensures that the adversarial training procedure is well-defined. To evaluate the generalization performance of the adversarial estimator, we study the adversarial excess risk. Our proposed analysis method includes investigations on both generalization error and approximation error. We then establish non-asymptotic upper bounds for the adversarial excess risk associated with Lipschitz loss functions. In addition, we apply our general results to adversarial training for classification and regression problems. For the quadratic loss in nonparametric regression, we show that the adversarial excess risk bound can be improved over those for a general loss.
</details>
<details>
<summary>摘要</summary>
我们提出一个通用的方法来评估预测器在不准确模型下的性能，基于敌对损失函数。我们首先显示出敌对损失相等于对于某些紧缩条件的分布型敌对攻击带来的损失。这确保了敌对训练程序的定义性。然后，我们研究了对于预测器的敌对剩余损失，包括预测器的整合误差和近似误差。我们then establish non-asymptotic upper bounds for the adversarial excess risk associated with Lipschitz loss functions. Finally, we apply our general results to adversarial training for classification and regression problems. For the quadratic loss in nonparametric regression, we show that the adversarial excess risk bound can be improved over those for a general loss.Here's the text with some additional information about the terms used:* "预测器" (zhì wén zhī) refers to a predictor or an estimator.* "不准确模型" (bù jian shí mian) refers to a misspecified model.* "敌对损失函数" (dài tào shè yǐ jī) refers to the adversarial loss function.* "分布型敌对攻击" (fēn bù xīng dào) refers to a distributional adversarial attack.* "整合误差" (zhé yì bù yì) refers to the generalization error.* "近似误差" (jìn xiē bù yì) refers to the approximation error.* "Lipschitz loss functions" (Lipschitz loss functions) refer to a class of loss functions that are Lipschitz continuous.* "nonparametric regression" (nonparametric regression) refers to a type of regression analysis that does not make any assumptions about the underlying distribution of the data.
</details></li>
</ul>
<hr>
<h2 id="Physics-informed-machine-learning-of-the-correlation-functions-in-bulk-fluids"><a href="#Physics-informed-machine-learning-of-the-correlation-functions-in-bulk-fluids" class="headerlink" title="Physics-informed machine learning of the correlation functions in bulk fluids"></a>Physics-informed machine learning of the correlation functions in bulk fluids</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00767">http://arxiv.org/abs/2309.00767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenqian Chen, Peiyuan Gao, Panos Stinis</li>
<li>for: 这篇论文主要用于解决粘性液体现代积分理论中的奥托-泽尼克方程。</li>
<li>methods: 这篇论文使用了机器学习模型，具体来说是物理学习激活函数和物理学习运算符网络，解决了奥托-泽尼克方程的前向和反向问题。</li>
<li>results: 机器学习模型在解决奥托-泽尼克方程的问题上表现了高精度和高效性，并且对液体热动力学理论的应用具有重要的潜在潜力。<details>
<summary>Abstract</summary>
The Ornstein-Zernike (OZ) equation is the fundamental equation for pair correlation function computations in the modern integral equation theory for liquids. In this work, machine learning models, notably physics-informed neural networks and physics-informed neural operator networks, are explored to solve the OZ equation. The physics-informed machine learning models demonstrate great accuracy and high efficiency in solving the forward and inverse OZ problems of various bulk fluids. The results highlight the significant potential of physics-informed machine learning for applications in thermodynamic state theory.
</details>
<details>
<summary>摘要</summary>
“欧兹方程”（Ornstein-Zernike equation）是现代流体 integral equation theory 中 Computational pair correlation function 的基本方程。在这项工作中，我们explore了机器学习模型，主要是physics-informed neural networks和physics-informed neural operator networks，来解决欧兹方程。这些physics-informed machine learning模型在解决前向和反向欧兹问题方面表现出了很高的准确率和高效性。结果表明physics-informed machine learning在热动力学状态理论中有广泛的应用前景。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/02/cs.LG_2023_09_02/" data-id="clpxp6c3s00r0ee88e7cdadps" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_09_02" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/02/eess.IV_2023_09_02/" class="article-date">
  <time datetime="2023-09-02T09:00:00.000Z" itemprop="datePublished">2023-09-02</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/02/eess.IV_2023_09_02/">eess.IV - 2023-09-02</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Enhancing-Cardiac-MRI-Segmentation-via-Classifier-Guided-Two-Stage-Network-and-All-Slice-Information-Fusion-Transformer"><a href="#Enhancing-Cardiac-MRI-Segmentation-via-Classifier-Guided-Two-Stage-Network-and-All-Slice-Information-Fusion-Transformer" class="headerlink" title="Enhancing Cardiac MRI Segmentation via Classifier-Guided Two-Stage Network and All-Slice Information Fusion Transformer"></a>Enhancing Cardiac MRI Segmentation via Classifier-Guided Two-Stage Network and All-Slice Information Fusion Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00800">http://arxiv.org/abs/2309.00800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Chen, Xiao Chen, Yikang Liu, Eric Z. Chen, Terrence Chen, Shanhui Sun</li>
<li>for: 这项研究的目的是提高卡ди亚克力磁共振成像（CMR）图像中心心脏功能评估中的左心室（LV）、右心室（RV）和心肌（MYO）分割精度。</li>
<li>methods: 该研究提出了一种基于深度学习的分割方法，使用类ifier-guided两个阶段网络和所有slice fusions transformer来提高CMR分割精度，特别是在基础和末端 slice中。</li>
<li>results: 该方法在大量临床数据集上进行了评估，与之前的CNN基于的和transformer基于的模型相比，在Dice分数上表现出了更高的性能。此外，该方法生成的分割形状与人工标注更加相似，并避免了其他模型中的常见问题，如孔洞或 Fragmentation。<details>
<summary>Abstract</summary>
Cardiac Magnetic Resonance imaging (CMR) is the gold standard for assessing cardiac function. Segmenting the left ventricle (LV), right ventricle (RV), and LV myocardium (MYO) in CMR images is crucial but time-consuming. Deep learning-based segmentation methods have emerged as effective tools for automating this process. However, CMR images present additional challenges due to irregular and varying heart shapes, particularly in basal and apical slices. In this study, we propose a classifier-guided two-stage network with an all-slice fusion transformer to enhance CMR segmentation accuracy, particularly in basal and apical slices. Our method was evaluated on extensive clinical datasets and demonstrated better performance in terms of Dice score compared to previous CNN-based and transformer-based models. Moreover, our method produces visually appealing segmentation shapes resembling human annotations and avoids common issues like holes or fragments in other models' segmentations.
</details>
<details>
<summary>摘要</summary>
卡ди亚磁共振成像（CMR）是评估心脏功能的标准方法。在CMR图像中，正确分割左心室（LV）、右心室（RV）和心肌（MYO）是关键，但是却是耗时的。深度学习基于的分割方法在 automating 这个过程中表现出了有效的特点。然而，CMR图像又具有心形不规则和不同的心形特征，特别是在基层和脊梁slice中。在这项研究中，我们提议一种类型导向的两阶段网络，以增强CMR分割精度，特别是在基层和脊梁slice中。我们的方法在丰富的临床数据集上进行了评估，并与之前的CNN基于的和 transformer基于的模型相比，表现出更高的Dice分数。此外，我们的方法生成的分割形状与人工标注相似，并避免了其他模型中的常见问题，如孔洞或 Fragmentation。
</details></li>
</ul>
<hr>
<h2 id="Online-Targetless-Radar-Camera-Extrinsic-Calibration-Based-on-the-Common-Features-of-Radar-and-Camera"><a href="#Online-Targetless-Radar-Camera-Extrinsic-Calibration-Based-on-the-Common-Features-of-Radar-and-Camera" class="headerlink" title="Online Targetless Radar-Camera Extrinsic Calibration Based on the Common Features of Radar and Camera"></a>Online Targetless Radar-Camera Extrinsic Calibration Based on the Common Features of Radar and Camera</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00787">http://arxiv.org/abs/2309.00787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Cheng, Siyang Cao</li>
<li>for: 这篇论文主要关注于自适应驾驶和自适应机器人的感应融合系统，尤其是对两种感应器之间的单一整合。</li>
<li>methods: 我们提出了一种新的方法，利用深度学习提取两种感应器之间的共同特征，并将这些共同特征用于匹配两种感应器中的同一个目标。我们还使用了RANSAC和Levenberg-Marquardt非线性优化算法来提取对称转换矩阵。</li>
<li>results: 我们的实验结果显示，我们的提案方法可以实现高精度和稳定性的单一整合。<details>
<summary>Abstract</summary>
Sensor fusion is essential for autonomous driving and autonomous robots, and radar-camera fusion systems have gained popularity due to their complementary sensing capabilities. However, accurate calibration between these two sensors is crucial to ensure effective fusion and improve overall system performance. Calibration involves intrinsic and extrinsic calibration, with the latter being particularly important for achieving accurate sensor fusion. Unfortunately, many target-based calibration methods require complex operating procedures and well-designed experimental conditions, posing challenges for researchers attempting to reproduce the results. To address this issue, we introduce a novel approach that leverages deep learning to extract a common feature from raw radar data (i.e., Range-Doppler-Angle data) and camera images. Instead of explicitly representing these common features, our method implicitly utilizes these common features to match identical objects from both data sources. Specifically, the extracted common feature serves as an example to demonstrate an online targetless calibration method between the radar and camera systems. The estimation of the extrinsic transformation matrix is achieved through this feature-based approach. To enhance the accuracy and robustness of the calibration, we apply the RANSAC and Levenberg-Marquardt (LM) nonlinear optimization algorithm for deriving the matrix. Our experiments in the real world demonstrate the effectiveness and accuracy of our proposed method.
</details>
<details>
<summary>摘要</summary>
感知融合是自动驾驶和自动机器人的关键技术，而雷达-摄像头融合系统在过去几年中得到了广泛的应用。然而，为了确保有效的感知融合，需要进行精准的协调。协调包括内在协调和外在协调，其中外在协调对于实现准确的感知融合是非常重要的。然而，许多目标基于的协调方法需要复杂的操作程序和丰富的实验条件，这会对研究人员进行重现结果的困难。为解决这个问题，我们介绍了一种新的方法，利用深度学习提取雷达数据（即距离-Doppler-角度数据）和摄像头图像中的公共特征。而不是直接表示这些公共特征，我们的方法将这些公共特征直接地用于匹配雷达和摄像头系统中的同一个目标。具体来说，提取的公共特征可以作为一个在线无目标协调方法的示例，用于计算雷达和摄像头系统之间的外在协调矩阵。为了提高准确性和稳定性，我们在这个特征基础上应用了RANSAC和Levenberg-Marquardt（LM）非线性优化算法来得到矩阵。我们在实际世界中进行的实验表明了我们的提案的有效性和准确性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/02/eess.IV_2023_09_02/" data-id="clpxp6cay019xee884yevgwfc" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_09_02" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/02/eess.SP_2023_09_02/" class="article-date">
  <time datetime="2023-09-02T08:00:00.000Z" itemprop="datePublished">2023-09-02</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/02/eess.SP_2023_09_02/">eess.SP - 2023-09-02</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Waste-Factor-A-New-Metric-for-Evaluating-Power-Efficiency-in-any-Cascade"><a href="#Waste-Factor-A-New-Metric-for-Evaluating-Power-Efficiency-in-any-Cascade" class="headerlink" title="Waste Factor: A New Metric for Evaluating Power Efficiency in any Cascade"></a>Waste Factor: A New Metric for Evaluating Power Efficiency in any Cascade</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01018">http://arxiv.org/abs/2309.01018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingjun Ying, Dipankar Shakya, Hitesh Poddar, Theodore S. Rappaport</li>
<li>for: This paper aims to improve the power efficiency of cascaded communication systems by developing a new metric called the Waste Factor ($W$).</li>
<li>methods: The authors use a mathematical framework to evaluate power efficiency in cascaded communication systems, by accounting for power wasted in individual components along a cascade. They also use the consumption efficiency factor (CEF) to evaluate the effects of insertion loss and deployment density on power efficiency.</li>
<li>results: The authors show that the Waste Factor is a unifying metric for defining wasted power in a cascade, and that it can be used to compare power efficiency between data centers and their components. They also observe that the CEF is markedly sensitive to insertion loss changes, particularly in uplink transmissions, and that energy efficiency improves at 142 GHz compared to 28 GHz as UE and BS numbers increase.Here’s the Chinese translation of the three points:</li>
<li>for: 这篇论文目标是提高积离通信系统的能效性，通过开发一个新的指标called Waste Factor ($W$).</li>
<li>methods: 作者们使用数学框架来评估积离通信系统的能效性，并考虑积离系统中每个组件的能量浪费。他们还使用消耗效率因子（CEF）来评估插入损失和部署密度对能效性的影响。</li>
<li>results: 作者们显示了 $W$ 是积离系统中定义浪费能量的统一指标，并且可以用来比较积离系统和其组件的能效性。他们还发现 CEF 受插入损失变化的影响非常大，特别是在上行传输中，而且能效性在142 GHz比28 GHz提高为 UE 和 BS 数量增加。<details>
<summary>Abstract</summary>
In this paper, we expand upon a new metric called the Waste Factor ($W$), a mathematical framework used to evaluate power efficiency in cascaded communication systems, by accounting for power wasted in individual components along a cascade. We show that the derivation of the Waste Factor, a unifying metric for defining wasted power along the signal path of any cascade, is similar to the mathematical approach used by H. Friis in 1944 to develop the Noise Factor ($F$), which has since served as a unifying metric for quantifying additive noise power in a cascade. Furthermore, the mathematical formulation of $W$ can be utilized in artificial intelligence (AI) and machine learning (ML) design and control for enhanced power efficiency. We consider the power usage effectiveness (PUE), which is a widely used energy efficiency metric for data centers, to evaluate $W$ for the data center as a whole. The use of $W$ allows easy comparison of power efficiency between data centers and their components. Our study further explores how insertion loss of components in a cascaded communication system influences $W$ at 28 GHz and 142 GHz along with the data rate performance, evaluated using the consumption efficiency factor (CEF). We observe CEF's marked sensitivity, particularly to phase shifter insertion loss changes. Notably, CEF variations are more prominent in uplink transmissions, whereas downlink transmissions offer relative CEF stability. Our exploration also covers the effects of varying User Equipment (UE) and Base Station (BS) deployment density on CEF in cellular networks. This work underscores the enhanced energy efficiency at 142 GHz, compared to 28 GHz, as UE and BS numbers escalate.
</details>
<details>
<summary>摘要</summary>
在本文中，我们扩展了一个新的度量 called Waste Factor ($W$), 用于评估级联通信系统中的能效性，通过对各个组件的能量损耗进行考虑。我们表明了度量Waste Factor的 derivation， 是级联通信系统中能效性度量的一个统一metric，类似于1944年Friis提出的Noise Factor ($F$)，该度量在级联通信系统中 quantifying 添加的噪声功率。此外，度量W的数学表述可以在人工智能（AI）和机器学习（ML）设计和控制中使用，以提高能效性。我们使用数据中心的能效性指标（PUE）来评估W，以便对数据中心和其组件进行简单的能效性比较。我们的研究还探讨了级联通信系统中组件插入损耗对W的影响，以及数据率性能，通过消耗效应因子（CEF）的变化来评估。我们发现CEF在28GHz和142GHz之间具有明显的敏感度，特别是在相位调制器插入损耗变化时。此外，我们还发现在无线电网络中 UE 和 BS 的分布密度变化对CEF的影响。这种研究表明了142GHz的能效性比28GHz更高，当 UE 和 BS 数量增加时。
</details></li>
</ul>
<hr>
<h2 id="A-Sub-Terahertz-Sliding-Correlator-Channel-Sounder-with-Absolute-Timing-using-Precision-Time-Protocol-over-Wi-Fi"><a href="#A-Sub-Terahertz-Sliding-Correlator-Channel-Sounder-with-Absolute-Timing-using-Precision-Time-Protocol-over-Wi-Fi" class="headerlink" title="A Sub-Terahertz Sliding Correlator Channel Sounder with Absolute Timing using Precision Time Protocol over Wi-Fi"></a>A Sub-Terahertz Sliding Correlator Channel Sounder with Absolute Timing using Precision Time Protocol over Wi-Fi</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01006">http://arxiv.org/abs/2309.01006</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dipankar Shakya, Hitesh Poddar, Theodore S. Rappaport<br>for:This paper aims to achieve sub-nanosecond timing accuracy for multipath component (MPC) propagation delays in power delay profiles (PDPs) for 5G and 6G communications at mmWave and sub-THz frequencies.methods:The proposed solution utilizes precision time protocol (PTP) and periodic drift correction to achieve absolute timing for MPCs in PDPs. The solution involves synchronizing the transmitter (TX) and receiver (RX) clocks using two RaspberryPi computers and a dedicated Wi-Fi link, and applying a periodic drift correction algorithm to eliminate PDP sample drift.results:The proposed solution achieves sub-nanosecond timing accuracy for MPC delays, reducing PDP sample drift to 150 samples&#x2F;hour compared to several thousand samples&#x2F;hour without synchronization. The solution shows promise in myriad applications, including precise position location and distributed systems that require sub-nanosecond timing accuracy and synchronization among components.Here is the information in Simplified Chinese text:for:这篇论文目标是在5G和6G通信中的mmWave和sub-THz频谱中实现多吉比特数据速率的 multipath component（MPC）延迟。methods:提议的解决方案利用精度时间协议（PTP）和周期偏移修正来实现MPC在电力延迟profile（PDP）中的绝对时间。解决方案使用两个RaspberryPi计算机和专门的Wi-Fi链接来实现TX和RX Rubidium时钟的同步，并应用周期偏移修正算法来消除PDP样本偏移。results:提议的解决方案实现了MPC延迟的sub-nanosecond精度，将PDP样本偏移降低至150个样本&#x2F;小时，比不同的时钟同步方法而言有所下降。该解决方案在多种应用中展示了承诺，包括精确的位置定位和分布式系统，它们需要sub-nanosecond精度和同步。<details>
<summary>Abstract</summary>
Radio channels at mmWave and sub-THz frequencies for 5G and 6G communications offer large channel bandwidths (hundreds of MHz to several GHz) to achieve multi-Gbps data rates. Accurate modeling of the radio channel for these wide bandwidths requires capturing the absolute timing of multipath component (MPC) propagation delays with sub-nanosecond accuracy. Achieving such timing accuracy is challenging due to clock drift in untethered transmitter (TX) and receiver (RX) clocks used in time-domain channel sounders, yet will become vital in many future 6G applications. This paper proposes a novel solution utilizing precision time protocol (PTP) and periodic drift correction to achieve absolute timing for MPCs in power delay profiles (PDPs) --captured as discrete samples using sliding correlation channel sounders. Two RaspberryPi computers are programmed to implement PTP over a dedicated Wi-Fi link and synchronize the TX and RX Rubidium clocks continuously every second. This synchronization minimizes clock drift, reducing PDP sample drift to 150 samples/hour, compared to several thousand samples/hour without synchronization. Additionally, a periodic drift correction algorithm is applied to eliminate PDP sample drift and achieve sub-nanosecond timing accuracy for MPC delays. The achieved synchronicity eliminates the need for tedious and sometimes inaccurate ray tracing to synthesize omnidirectional PDPs from directional measurements. The presented solution shows promise in myriad applications, including precise position location and distributed systems that require sub-nanosecond timing accuracy and synchronization among components.
</details>
<details>
<summary>摘要</summary>
radio频道在mmWave和sub-THz频率上为5G和6G通信提供大量频带宽（百万Hz到几亿Hz）以实现多Gbps的数据速率。 precisely modeling radio频道需要 capture multipath component（MPC）延迟的绝对时间准确性（sub-纳秒级）。 Achieving such timing accuracy is challenging due to clock drift in untethered transmitter（TX）和receiver（RX）clocks used in time-domain channel sounders, yet will become vital in many future 6G applications。 This paper proposes a novel solution utilizing precision time protocol（PTP）and periodic drift correction to achieve absolute timing for MPCs in power delay profiles（PDPs）—captured as discrete samples using sliding correlation channel sounders。 Two RaspberryPi computers are programmed to implement PTP over a dedicated Wi-Fi link and synchronize the TX and RX Rubidium clocks continuously every second。 This synchronization minimizes clock drift, reducing PDP sample drift to 150 samples/hour, compared to several thousand samples/hour without synchronization。 Additionally, a periodic drift correction algorithm is applied to eliminate PDP sample drift and achieve sub-nanosecond timing accuracy for MPC delays。 The achieved synchronicity eliminates the need for tedious and sometimes inaccurate ray tracing to synthesize omnidirectional PDPs from directional measurements。 The presented solution shows promise in myriad applications, including precise position location and distributed systems that require sub-nanosecond timing accuracy and synchronization among components。
</details></li>
</ul>
<hr>
<h2 id="Robust-Joint-Active-Passive-Beamforming-Design-for-IRS-Assisted-ISAC-Systems"><a href="#Robust-Joint-Active-Passive-Beamforming-Design-for-IRS-Assisted-ISAC-Systems" class="headerlink" title="Robust Joint Active-Passive Beamforming Design for IRS-Assisted ISAC Systems"></a>Robust Joint Active-Passive Beamforming Design for IRS-Assisted ISAC Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00978">http://arxiv.org/abs/2309.00978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahmoud AlaaEldin, Emad Alsusa, Karim G. Seddik, Christos Masouros, Iman Valiulahi<br>for: 这个研究旨在探讨 интеelligent reflective surfaces（IRS）与integrated sensing and communication（ISAC）系统之间的integraion，以改善未来无线网络中的spectrum congestion问题。methods: 这个研究使用了低复杂性和高效的共同优化算法，将BS的传发矩阵和IRS的反射矩阵共同优化，以最小化传发信号和欲要的射频范围之间的Frobenius距离。results: 研究结果显示，在不同的环境下，IRS-ISAC系统可以实现更好的传输和探测性能，并且可以适应不同的通信用户和射频范围。此外，这个研究还提出了一个对于IRS频率不确定性的强化 beamforming优化算法，可以对于实际系统中的频率不确定性进行适应。<details>
<summary>Abstract</summary>
The idea of Integrated Sensing and Communication (ISAC) offers a promising solution to the problem of spectrum congestion in future wireless networks. This paper studies the integration of intelligent reflective surfaces (IRS) with ISAC systems to improve the performance of radar and communication services. Specifically, an IRS-assisted ISAC system is investigated where a multi-antenna base station (BS) performs multi-target detection and multi-user communication. A low complexity and efficient joint optimization of transmit beamforming at the BS and reflective beamforming at the IRS is proposed. This is done by jointly optimizing the BS beamformers and IRS reflection coefficients to minimize the Frobenius distance between the covariance matrices of the transmitted signal and the desired radar beam pattern. This optimization aims to satisfy the signal-to-interference-and-noise ratio (SINR) constraints of the communication users, the total transmit power limit at the BS, and the unit modulus constraints of the IRS reflection coefficients. To address the resulting complex non-convex optimization problem, an efficient alternating optimization (AO) algorithm combining fractional programming (FP), semi-definite programming (SDP), and second order cone programming (SOCP) methods is proposed. Furthermore, we propose robust beamforming optimization for IRS-ISAC systems by adapting the proposed optimization algorithm to the IRS channel uncertainties that may exist in practical systems. Using advanced tools from convex optimization theory, the constraints containing uncertainty are transformed to their equivalent linear matrix inequalities (LMIs) to account for the channels' uncertainty radius. The results presented quantify the benefits of IRS-ISAC systems under various conditions and demonstrate the effectiveness of the proposed algorithm.
</details>
<details>
<summary>摘要</summary>
“嵌入式感测和通信（ISAC）技术提供了未来无线网络中频率压力问题的有效解决方案。这篇文章研究了智能反射表（IRS）与ISAC系统的集成，以提高雷达和通信服务的性能。具体来说，我们研究了一种由多个antenna基站（BS）和IRS共同实现的IRS-助け过 ISAC系统。在这种系统中，BS使用多个antenna执行多个目标检测和多个用户通信。我们提出了一种低复杂度和高效的集成传输扩张和反射扩张的优化方法。这种优化方法是将BS的扩张和IRS的反射矩阵优化到最小化 Frobenius 距离 между发射信号的covariance矩阵和 Desired 雷达波形矩阵。这种优化目的是满足通信用户的信号噪干扰比（SINR）约束、BS的总发射功率限制和IRS的单位模数约束。为解决这个复杂非 convex 优化问题，我们提出了一种高效的 alternate 优化（AO）算法，该算法结合分数编程（FP）、半definite 编程（SDP）和第二个 cone 编程（SOCP）方法。此外，我们还提出了IRS-ISAC系统中的Robust 扩张优化，该算法通过将uncertainty 约束转化为其等效的线性matrix inequality（LMIs）来考虑实际系统中的频率uncertainty。结果表明，在不同条件下，IRS-ISAC系统具有显著的优势，并且提出的算法效果扎实。”
</details></li>
</ul>
<hr>
<h2 id="Consensus-based-Distributed-Variational-Multi-object-Tracker-in-Multi-Sensor-Network"><a href="#Consensus-based-Distributed-Variational-Multi-object-Tracker-in-Multi-Sensor-Network" class="headerlink" title="Consensus-based Distributed Variational Multi-object Tracker in Multi-Sensor Network"></a>Consensus-based Distributed Variational Multi-object Tracker in Multi-Sensor Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00807">http://arxiv.org/abs/2309.00807</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qing Li, Runze Gan, Simon Godsill</li>
<li>for: 本研究旨在设计一种高精度、可靠的追踪系统，以满足现代感知技术的增长需求。</li>
<li>methods: 本文提出了两种变分析追踪器，可以有效地跟踪多个目标在噪声环境中。中央整合式感知方案首先将感知数据传输到整合中心进行融合，而分布式版本则基于平均协议来实现本地消息传递。</li>
<li>results: 实验结果表明，我们提出的分布式变分追踪器和中央整合式追踪器的跟踪精度相当，并且分布式追踪器比 arithmetic sensor fusion 和平均协议融合策略更高。<details>
<summary>Abstract</summary>
The growing need for accurate and reliable tracking systems has driven significant progress in sensor fusion and object tracking techniques. In this paper, we design two variational Bayesian trackers that effectively track multiple targets in cluttered environments within a sensor network. We first present a centralised sensor fusion scheme, which involves transmitting sensor data to a fusion center. Then, we develop a distributed version leveraging the average consensus algorithm, which is theoretically equivalent to the centralised sensor fusion tracker and requires only local message passing with neighbouring sensors. In addition, we empirically verify that our proposed distributed variational tracker performs on par with the centralised version with equal tracking accuracy. Simulation results show that our distributed multi-target tracker outperforms the suboptimal distributed sensor fusion strategy that fuses each sensor's posterior based on arithmetic sensor fusion and an average consensus strategy.
</details>
<details>
<summary>摘要</summary>
随着准确和可靠的跟踪系统的需求不断增长，感知融合和目标跟踪技术也得到了重要的进步。在这篇论文中，我们设计了两种变分极 bayesian跟踪器，可以有效地跟踪多个目标在杂乱环境中的感知网络中。我们首先提出了一种中央感知融合方案，其中感知数据被传输到融合中心进行融合。然后，我们开发了分布式版本，利用平均协议算法，这是中央感知融合器的理论等价，只需在与邻居感知器进行本地消息传递。此外，我们验证了我们的提议的分布式变分跟踪器和中央版本之间的跟踪精度相等。 simulation结果显示，我们的分布式多目标跟踪器比使用 arithmetic sensor fusion和平均协议策略来进行分布式感知融合的优化策略强。
</details></li>
</ul>
<hr>
<h2 id="Delay-Doppler-Alignment-Modulation-for-Spatially-Sparse-Massive-MIMO-Communication"><a href="#Delay-Doppler-Alignment-Modulation-for-Spatially-Sparse-Massive-MIMO-Communication" class="headerlink" title="Delay-Doppler Alignment Modulation for Spatially Sparse Massive MIMO Communication"></a>Delay-Doppler Alignment Modulation for Spatially Sparse Massive MIMO Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00792">http://arxiv.org/abs/2309.00792</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haiquan Lu, Yong Zeng</li>
<li>for: 本文旨在解决宽频通信中的symbol间干扰问题，提出了一种基于时空延迟处理的延迟对称模ulation（DAM）技术，而不需要通道平衡或多个卫星传输。</li>
<li>methods: 本文提出了一种基于时变频Selective多输入多出口（MIMO）通信系统的延迟对称模ulation（DDAM）技术，通过利用延迟-Doppler补做和路径基本 beamforming，消除每个干扰道的Doppler效应，使所有干扰道信号组件都能够同步到接收器。</li>
<li>results: 本文首先表明，通过应用路径基本零做（ZF）预编码和接收 combine，DDAM可以将原始时变频Selective通道转化为时间 invariants ISI-free通道。 derive了必要和&#x2F;或 suficient conditionsto achieve这种转化。然后，提供了一个 asymptotic analysis，表明当基站天线数量远大于通道路径数量时，DDAM可以实现时间 invariants ISI-free通道，只需要简单的延迟-Doppler补做和路径基本 MRT  beamforming。此外，为了实现DDAM设计中的一些可容忍的 ISI，将路径基本预编码和接收 combine矩阵优化为最大化spectral efficiency。<details>
<summary>Abstract</summary>
Delay alignment modulation (DAM) is an emerging technique for achieving inter-symbol interference (ISI)-free wideband communications using spatial-delay processing, without relying on channel equalization or multi-carrier transmission. However, existing works on DAM only consider multiple-input single-output (MISO) communication systems and assume time-invariant channels. In this paper, by extending DAM to time-variant frequency-selective multiple-input multiple-output (MIMO) channels, we propose a novel technique termed \emph{delay-Doppler alignment modulation} (DDAM). Specifically, by leveraging \emph{delay-Doppler compensation} and \emph{path-based beamforming}, the Doppler effect of each multi-path can be eliminated and all multi-path signal components may reach the receiver concurrently and constructively. We first show that by applying path-based zero-forcing (ZF) precoding and receive combining, DDAM can transform the original time-variant frequency-selective channels into time-invariant ISI-free channels. The necessary and/or sufficient conditions to achieve such a transformation are derived. Then an asymptotic analysis is provided by showing that when the number of base station (BS) antennas is much larger than that of channel paths, DDAM enables time-invariant ISI-free channels with the simple delay-Doppler compensation and path-based maximal-ratio transmission (MRT) beamforming. Furthermore, for the general DDAM design with some tolerable ISI, the path-based transmit precoding and receive combining matrices are optimized to maximize the spectral efficiency. Numerical results are provided to compare the proposed DDAM technique with various benchmarking schemes, including MIMO-orthogonal time frequency space (OTFS), MIMO-orthogonal frequency-division multiplexing (OFDM) without or with carrier frequency offset (CFO) compensation, and beam alignment along the dominant path.
</details>
<details>
<summary>摘要</summary>
延迟对齐模ulation（DAM）是一种emerging技术，用于实现干扰符号（ISI）free广泛通信，只需通过空间延迟处理，而不需依赖通道均衡或多 carriermultiplexing。然而，现有的DAM研究仅考虑多input single-output（MISO）通信系统，并假设时variant channels。在这篇文章中，我们通过扩展DAM到时variant frequency-selective多input多output（MIMO）通信频道，提出一种新的技术，称为延迟Doppler对齐模ulation（DDAM）。具体来说，通过利用延迟Doppler补做和路径基 beamforming，每个多路可以消除Doppler效应，使所有多路信号组件能够同时到达接收器并构成。我们首先示出，通过应用路径基Zero-Forcing（ZF）预编码和接收 combining，DDAM可以将原始时variant frequency-selective频道变换成时 invariantin ISI-free频道。我们 derive了必要和/或充分的条件以实现这种变换。然后，我们提供了一种 asymptotic analysis，表明当基站天线数量比channel path数量多得多时，DDAM可以将时variant ISI-free频道转换成时 invariantin ISI-free频道，只需使用简单的延迟Doppler补做和路径基MRT beamforming。此外，为了设计DDAM，我们 optimize了路径基传输预编码和接收 combining矩阵，以最大化spectral efficiency。我们提供了一些数值结果，并与various benchmarking schemes进行比较，包括MIMO-orthogonal time frequency space（OTFS）、MIMO-orthogonal frequency-division multiplexing（OFDM）without/with carrier frequency offset（CFO）compensation，以及beam alignment along the dominant path。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/02/eess.SP_2023_09_02/" data-id="clpxp6ccq01dyee88hpcx3dau" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_09_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/01/cs.SD_2023_09_01/" class="article-date">
  <time datetime="2023-09-01T15:00:00.000Z" itemprop="datePublished">2023-09-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/01/cs.SD_2023_09_01/">cs.SD - 2023-09-01</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="CoNeTTE-An-efficient-Audio-Captioning-system-leveraging-multiple-datasets-with-Task-Embedding"><a href="#CoNeTTE-An-efficient-Audio-Captioning-system-leveraging-multiple-datasets-with-Task-Embedding" class="headerlink" title="CoNeTTE: An efficient Audio Captioning system leveraging multiple datasets with Task Embedding"></a>CoNeTTE: An efficient Audio Captioning system leveraging multiple datasets with Task Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00454">http://arxiv.org/abs/2309.00454</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/topel/audioset-convnext-inf">https://github.com/topel/audioset-convnext-inf</a></li>
<li>paper_authors: Étienne Labbé, Thomas Pellegrini, Julien Pinquier</li>
<li>for: The paper is written for the task of automated audio captioning (AAC), specifically using a ConvNeXt architecture as the audio encoder and exploring the use of task embeddings to improve performance across multiple datasets.</li>
<li>methods: The paper uses a ConvNeXt architecture as the audio encoder, which is adapted from the vision domain to audio classification. The model is trained on multiple AAC datasets (AC, CL, MACS, WavCaps) with a task embedding (TE) token to identify the source dataset for each input sample.</li>
<li>results: The paper achieves state-of-the-art scores on the AudioCaps (AC) dataset and competitive performance on Clotho (CL) with fewer parameters than existing models. The use of task embeddings improves cross-dataset performance, but there is still a performance gap between datasets, indicating the need for dataset-specific models. The resulting model, called CoNeTTE, achieves SPIDEr scores of 44.1% and 30.5% on AC and CL, respectively.<details>
<summary>Abstract</summary>
Automated Audio Captioning (AAC) involves generating natural language descriptions of audio content, using encoder-decoder architectures. An audio encoder produces audio embeddings fed to a decoder, usually a Transformer decoder, for caption generation. In this work, we describe our model, which novelty, compared to existing models, lies in the use of a ConvNeXt architecture as audio encoder, adapted from the vision domain to audio classification. This model, called CNext-trans, achieved state-of-the-art scores on the AudioCaps (AC) dataset and performed competitively on Clotho (CL), while using four to forty times fewer parameters than existing models. We examine potential biases in the AC dataset due to its origin from AudioSet by investigating unbiased encoder's impact on performance. Using the well-known PANN's CNN14, for instance, as an unbiased encoder, we observed a 1.7% absolute reduction in SPIDEr score (where higher scores indicate better performance). To improve cross-dataset performance, we conducted experiments by combining multiple AAC datasets (AC, CL, MACS, WavCaps) for training. Although this strategy enhanced overall model performance across datasets, it still fell short compared to models trained specifically on a single target dataset, indicating the absence of a one-size-fits-all model. To mitigate performance gaps between datasets, we introduced a Task Embedding (TE) token, allowing the model to identify the source dataset for each input sample. We provide insights into the impact of these TEs on both the form (words) and content (sound event types) of the generated captions. The resulting model, named CoNeTTE, an unbiased CNext-trans model enriched with dataset-specific Task Embeddings, achieved SPIDEr scores of 44.1% and 30.5% on AC and CL, respectively. Code available: https://github.com/Labbeti/conette-audio-captioning.
</details>
<details>
<summary>摘要</summary>
自动化语音描述（AAC）技术涉及生成语音内容的自然语言描述，使用编码器-解码器架构。一个音频编码器生成音频嵌入，并将其传递给一个通常是转换器解码器的decoder进行描述生成。在这个工作中，我们描述了我们的模型，它与现有模型的不同之处在于使用ConvNeXt架构作为音频编码器，从视觉领域中适应到音频分类。我们称之为CNext-trans模型，在AudioCaps（AC）数据集上达到了状态之artefact的分数，并在Clotho（CL）数据集上表现竞争力强，同时使用四到四十个参数少于现有模型。我们 investigate了AC数据集中可能的偏见问题，并证明使用不偏见的编码器对性能有一定的影响。使用著名的PANN的CNN14作为不偏见编码器，我们观察到了1.7%的绝对下降分数（其中更高的分数表示更好的性能）。为提高跨数据集性能，我们进行了多个AAC数据集（AC、CL、MACS、WavCaps）的组合训练。虽然这种策略提高了总模型性能，但还不如特定目标数据集训练的模型，表明不存在一个适用于所有数据集的模型。为了减少数据集之间性能差距，我们引入了任务嵌入（TE）token，让模型可以通过检测输入样本的来源数据集来识别样本的来源。我们对TE的影响进行了详细的分析，包括对形式（字词）和内容（声音事件类型）的影响。最终，我们提出了CoNeTTE模型，一个不偏见CNext-trans模型，通过添加数据集特定的任务嵌入，实现了SPIDEr分数44.1%和30.5%。代码可以在https://github.com/Labbeti/conette-audio-captioning中下载。
</details></li>
</ul>
<hr>
<h2 id="Remixing-based-Unsupervised-Source-Separation-from-Scratch"><a href="#Remixing-based-Unsupervised-Source-Separation-from-Scratch" class="headerlink" title="Remixing-based Unsupervised Source Separation from Scratch"></a>Remixing-based Unsupervised Source Separation from Scratch</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00376">http://arxiv.org/abs/2309.00376</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kohei Saijo, Tetsuji Ogawa</li>
<li>for: 本文提出了一种无监督的方法，用于从零开始训练分离模型，使用RecmixIT和Self-Remixing等现代自动学习方法来进行修复预训练模型。</li>
<li>methods: 这种方法首先使用一个教师模型将混合物分离，然后创建一些假混合物，通过搅拌和重新混合已分离的信号来进行训练。学生模型将使用教师的输出或初始混合物作为监督来分离假混合物。为了修复教师的输出，教师的权重将被更新为学生的权重。</li>
<li>results: 实验结果表明，提议的方法可以超越现有的混合 invariant 训练方法，从零开始训练一个单频分离模型。此外，我们还提出了一种简单的搅拌方法来稳定训练。<details>
<summary>Abstract</summary>
We propose an unsupervised approach for training separation models from scratch using RemixIT and Self-Remixing, which are recently proposed self-supervised learning methods for refining pre-trained models. They first separate mixtures with a teacher model and create pseudo-mixtures by shuffling and remixing the separated signals. A student model is then trained to separate the pseudo-mixtures using either the teacher's outputs or the initial mixtures as supervision. To refine the teacher's outputs, the teacher's weights are updated with the student's weights. While these methods originally assumed that the teacher is pre-trained, we show that they are capable of training models from scratch. We also introduce a simple remixing method to stabilize training. Experimental results demonstrate that the proposed approach outperforms mixture invariant training, which is currently the only available approach for training a monaural separation model from scratch.
</details>
<details>
<summary>摘要</summary>
我们提出了一种无监督的方法，用于从零开始训练分离模型，使用RecmixIT和Self-Remixing，这两种最近提出的自动学习方法来修正预训练模型。它们首先使用一个教师模型将混合物分离出来，然后创建假混合物，通过搅拌和重新混合分离后的信号。一个学生模型然后被训练使用教师的输出或初始混合物作为监督来分离假混合物。为了修正教师的输出，教师的权重被更新为学生的权重。而这些方法最初假设了教师是预训练的，但我们表明它们可以训练模型从零开始。我们还介绍了一种简单的搅拌方法，以稳定训练。实验结果表明，我们提出的方法在训练独立式分离模型方面超过了现有的混合物不变训练方法。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/01/cs.SD_2023_09_01/" data-id="clpxp6c6k00yqee8817bt6u60" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_09_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/01/cs.CV_2023_09_01/" class="article-date">
  <time datetime="2023-09-01T13:00:00.000Z" itemprop="datePublished">2023-09-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/01/cs.CV_2023_09_01/">cs.CV - 2023-09-01</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Affine-Transformation-Invariant-Image-Classification-by-Differentiable-Arithmetic-Distribution-Module"><a href="#Affine-Transformation-Invariant-Image-Classification-by-Differentiable-Arithmetic-Distribution-Module" class="headerlink" title="Affine-Transformation-Invariant Image Classification by Differentiable Arithmetic Distribution Module"></a>Affine-Transformation-Invariant Image Classification by Differentiable Arithmetic Distribution Module</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00752">http://arxiv.org/abs/2309.00752</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zijie Tan, Guanfang Dong, Chenqiu Zhao, Anup Basu</li>
<li>for: 提高 Convolutional Neural Networks (CNNs) 对于图像分类 tasks 的 robustness, 尤其是对于旋转、平移、翻转和排序等非对称变换。</li>
<li>methods: 提出一种基于分布学习技术的 Differentiable Arithmetic Distribution Module (DADM)，通过学习图像中像素的空间分布信息来提高模型的Robustness。</li>
<li>results: 通过对比与 LeNet 等方法的实验和简洁分析，证明了该方法能够提高模型的 Robustness 无需牺牲特征提取能力。<details>
<summary>Abstract</summary>
Although Convolutional Neural Networks (CNNs) have achieved promising results in image classification, they still are vulnerable to affine transformations including rotation, translation, flip and shuffle. The drawback motivates us to design a module which can alleviate the impact from different affine transformations. Thus, in this work, we introduce a more robust substitute by incorporating distribution learning techniques, focusing particularly on learning the spatial distribution information of pixels in images. To rectify the issue of non-differentiability of prior distribution learning methods that rely on traditional histograms, we adopt the Kernel Density Estimation (KDE) to formulate differentiable histograms. On this foundation, we present a novel Differentiable Arithmetic Distribution Module (DADM), which is designed to extract the intrinsic probability distributions from images. The proposed approach is able to enhance the model's robustness to affine transformations without sacrificing its feature extraction capabilities, thus bridging the gap between traditional CNNs and distribution-based learning. We validate the effectiveness of the proposed approach through ablation study and comparative experiments with LeNet.
</details>
<details>
<summary>摘要</summary>
To address the issue of non-differentiability of prior distribution learning methods that rely on traditional histograms, we adopt Kernel Density Estimation (KDE) to formulate differentiable histograms. On this foundation, we present a novel Differentiable Arithmetic Distribution Module (DADM), which is designed to extract the intrinsic probability distributions from images. The proposed approach can enhance the model's robustness to affine transformations without sacrificing its feature extraction capabilities, thus bridging the gap between traditional CNNs and distribution-based learning. We validate the effectiveness of the proposed approach through ablation study and comparative experiments with LeNet.
</details></li>
</ul>
<hr>
<h2 id="PathLDM-Text-conditioned-Latent-Diffusion-Model-for-Histopathology"><a href="#PathLDM-Text-conditioned-Latent-Diffusion-Model-for-Histopathology" class="headerlink" title="PathLDM: Text conditioned Latent Diffusion Model for Histopathology"></a>PathLDM: Text conditioned Latent Diffusion Model for Histopathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00748">http://arxiv.org/abs/2309.00748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Srikar Yellapragada, Alexandros Graikos, Prateek Prasanna, Tahsin Kurc, Joel Saltz, Dimitris Samaras</li>
<li>for: 这篇论文旨在开发一种基于文本指导的高质量病理图像生成模型，以便在计算机 PATHOLOGY 领域中提高模型训练效果。</li>
<li>methods: 该论文使用文本指导的幽Diffusion模型，通过将图像和文本数据 fusion 以提高生成过程。文本数据来自 histopathology 报告，通过 GPT 技术进行抽象和概要，以建立有效的 conditioning 机制。</li>
<li>results: 通过策略性的 conditioning 和必要的建构改进，该论文在 TCGA-BRCA 数据集上实现了 SoTA FID 分数 7.64，明显超越最近的文本指导竞争对手的 FID 分数 30.1。<details>
<summary>Abstract</summary>
To achieve high-quality results, diffusion models must be trained on large datasets. This can be notably prohibitive for models in specialized domains, such as computational pathology. Conditioning on labeled data is known to help in data-efficient model training. Therefore, histopathology reports, which are rich in valuable clinical information, are an ideal choice as guidance for a histopathology generative model. In this paper, we introduce PathLDM, the first text-conditioned Latent Diffusion Model tailored for generating high-quality histopathology images. Leveraging the rich contextual information provided by pathology text reports, our approach fuses image and textual data to enhance the generation process. By utilizing GPT's capabilities to distill and summarize complex text reports, we establish an effective conditioning mechanism. Through strategic conditioning and necessary architectural enhancements, we achieved a SoTA FID score of 7.64 for text-to-image generation on the TCGA-BRCA dataset, significantly outperforming the closest text-conditioned competitor with FID 30.1.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learned-Visual-Features-to-Textual-Explanations"><a href="#Learned-Visual-Features-to-Textual-Explanations" class="headerlink" title="Learned Visual Features to Textual Explanations"></a>Learned Visual Features to Textual Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00733">http://arxiv.org/abs/2309.00733</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saeid Asgari Taghanaki, Aliasghar Khani, Amir Khasahmadi, Aditya Sanghi, Karl D. D. Willis, Ali Mahdavi-Amiri</li>
<li>for: 提高图像分类器的解释性和可靠性</li>
<li>methods: 利用大型自然语言模型（LLMs）解释图像分类器学习的特征空间</li>
<li>results: 在多个数据集上进行了实验，证明了方法的有效性，可以提高图像分类器的解释性和可靠性<details>
<summary>Abstract</summary>
Interpreting the learned features of vision models has posed a longstanding challenge in the field of machine learning. To address this issue, we propose a novel method that leverages the capabilities of large language models (LLMs) to interpret the learned features of pre-trained image classifiers. Our method, called TExplain, tackles this task by training a neural network to establish a connection between the feature space of image classifiers and LLMs. Then, during inference, our approach generates a vast number of sentences to explain the features learned by the classifier for a given image. These sentences are then used to extract the most frequent words, providing a comprehensive understanding of the learned features and patterns within the classifier. Our method, for the first time, utilizes these frequent words corresponding to a visual representation to provide insights into the decision-making process of the independently trained classifier, enabling the detection of spurious correlations, biases, and a deeper comprehension of its behavior. To validate the effectiveness of our approach, we conduct experiments on diverse datasets, including ImageNet-9L and Waterbirds. The results demonstrate the potential of our method to enhance the interpretability and robustness of image classifiers.
</details>
<details>
<summary>摘要</summary>
machine learning 领域中解释视图模型学习的结果对于长期是一个挑战。为解决这个问题，我们提出了一种新的方法，即利用大型自然语言模型（LLMs）来解释预训练的图像分类器学习的特征。我们的方法，称为TExplain，通过在图像分类器的特征空间和LLMs之间建立连接来实现这个任务。在推理过程中，我们的方法生成大量的句子来解释给定图像中分类器学习的特征。这些句子中的最常见词语被用来提取特征空间中学习的特征和模式，从而提供了图像分类器的决策过程中的全面理解。我们的方法首次利用这些与视觉表示相对应的常见词语，提供了图像分类器的决策过程中的深入了解和检测偏见、偏好等。为验证我们的方法的有效性，我们在多个 dataset 上进行了实验，包括 ImageNet-9L 和 Waterbirds。结果表明，我们的方法可以增强图像分类器的可解释性和Robustness。
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-in-medical-image-registration-introduction-and-survey"><a href="#Deep-learning-in-medical-image-registration-introduction-and-survey" class="headerlink" title="Deep learning in medical image registration: introduction and survey"></a>Deep learning in medical image registration: introduction and survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00727">http://arxiv.org/abs/2309.00727</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmad Hammoudeh, Stéphane Dupont</li>
<li>for: 本文主要用于介绍图像注册技术，以帮助医疗专业人员在标准化参照Frame中进行评估多种医学图像。</li>
<li>methods: 本文使用了多种图像变换，包括Affine变换、可变形变换、可逆变换、双向变换等，以及医学图像注册算法，如Voxelmorph、Demons、SynthMorph等。</li>
<li>results: 本文涵盖了多种图像注册技术，包括参考 Atlases、多stage图像注册、Pyramid Approach等，以及医学图像注册的数据集、评估指标、应用场景等。<details>
<summary>Abstract</summary>
Image registration (IR) is a process that deforms images to align them with respect to a reference space, making it easier for medical practitioners to examine various medical images in a standardized reference frame, such as having the same rotation and scale. This document introduces image registration using a simple numeric example. It provides a definition of image registration along with a space-oriented symbolic representation. This review covers various aspects of image transformations, including affine, deformable, invertible, and bidirectional transformations, as well as medical image registration algorithms such as Voxelmorph, Demons, SyN, Iterative Closest Point, and SynthMorph. It also explores atlas-based registration and multistage image registration techniques, including coarse-fine and pyramid approaches. Furthermore, this survey paper discusses medical image registration taxonomies, datasets, evaluation measures, such as correlation-based metrics, segmentation-based metrics, processing time, and model size. It also explores applications in image-guided surgery, motion tracking, and tumor diagnosis. Finally, the document addresses future research directions, including the further development of transformers.
</details>
<details>
<summary>摘要</summary>
Image registration (IR) 是一个过程，用于将图像调整，使其与参照空间align，以便医疗专业人员通过标准化参照框架进行评估不同医疗图像，例如同一个旋转和缩放。本文介绍了图像 registration 的简单数字示例，并提供了图像 registration 的定义和空间 oriented 符号表示。本文评论了各种图像变换，包括 affine、可变、可逆、 bidirectional 变换，以及医疗图像 registration 算法，如 Voxelmorph、Demons、SyN、Iterative Closest Point 和 SynthMorph。此外，本文还探讨了 atlas-based registration 和多阶段图像 registration 技术，包括 coarse-fine 和 pyramid 方法。此外，本文还讨论了医疗图像 registration 的分类、数据集、评价指标，如 correlation-based  метри克、segmentation-based  метри克、处理时间和模型大小。此外，本文还探讨了图像导航手术、运动跟踪和肿瘤诊断的应用。最后，文档还讨论了未来研究方向，包括 transformers 的进一步发展。
</details></li>
</ul>
<hr>
<h2 id="Indexing-Irises-by-Intrinsic-Dimension"><a href="#Indexing-Irises-by-Intrinsic-Dimension" class="headerlink" title="Indexing Irises by Intrinsic Dimension"></a>Indexing Irises by Intrinsic Dimension</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00705">http://arxiv.org/abs/2309.00705</a></li>
<li>repo_url: None</li>
<li>paper_authors: J. Michael Rozmus</li>
<li>for: 这个论文是为了提高人脸识别技术的精度和速度而写的。</li>
<li>methods: 这个论文使用了主成分分析（PCA）将一组高质量的眼照图像映射到四维内在空间中。</li>
<li>results: 这个论文得到了一个高精度的人脸识别系统，可以快速准确地匹配眼照图像到数据库中的匹配记录。<details>
<summary>Abstract</summary>
28,000+ high-quality iris images of 1350 distinct eyes from 650+ different individuals from a relatively diverse university town population were collected. A small defined unobstructed portion of the normalized iris image is selected as a key portion for quickly identifying an unknown individual when submitting an iris image to be matched to a database of enrolled irises of the 1350 distinct eyes. The intrinsic dimension of a set of these key portions of the 1350 enrolled irises is measured to be about four (4). This set is mapped to a four-dimensional intrinsic space by principal components analysis (PCA). When an iris image is presented to the iris database for identification, the search begins in the neighborhood of the location of its key portion in the 4D intrinsic space, typically finding a correct identifying match after comparison to only a few percent of the database.
</details>
<details>
<summary>摘要</summary>
我们收集了1350个眼睛的28,000多个高质量眼睛图像，来自650多个不同个体的大学城市人口。我们选择了一小部分眼睛图像作为快速识别未知个体的关键部分，并计算了这些关键部分的内在维度为4。我们将这些关键部分映射到4维内在空间中，使用主成分分析（PCA）。当我们将眼睛图像提交到识别数据库时，我们会在数据库中查找与其关键部分相似的匹配，通常只需要比较数据库中的一些百分比就能够获得正确的识别结果。
</details></li>
</ul>
<hr>
<h2 id="AAN-Attributes-Aware-Network-for-Temporal-Action-Detection"><a href="#AAN-Attributes-Aware-Network-for-Temporal-Action-Detection" class="headerlink" title="AAN: Attributes-Aware Network for Temporal Action Detection"></a>AAN: Attributes-Aware Network for Temporal Action Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00696">http://arxiv.org/abs/2309.00696</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Dai, Srijan Das, Michael S. Ryoo, Francois Bremond</li>
<li>for: 本研究的目的是解决长期视频理解中的效率EXTRACTING object semantics和其关系模型，以便对下游任务进行更好的支持。</li>
<li>methods: 本研究提出了Attributes-Aware Network（AAN），包括两个关键组件：Attributes Extractor和Graph Reasoning block。这两个组件可以帮助EXTRACTING object-centric attributes和视频中对象关系的模型。</li>
<li>results: 通过利用CLIP特征，AAN超过了当前state-of-the-art方法在Charades和Toyota Smarthome Untrimmed dataset上的性能。<details>
<summary>Abstract</summary>
The challenge of long-term video understanding remains constrained by the efficient extraction of object semantics and the modelling of their relationships for downstream tasks. Although the CLIP visual features exhibit discriminative properties for various vision tasks, particularly in object encoding, they are suboptimal for long-term video understanding. To address this issue, we present the Attributes-Aware Network (AAN), which consists of two key components: the Attributes Extractor and a Graph Reasoning block. These components facilitate the extraction of object-centric attributes and the modelling of their relationships within the video. By leveraging CLIP features, AAN outperforms state-of-the-art approaches on two popular action detection datasets: Charades and Toyota Smarthome Untrimmed datasets.
</details>
<details>
<summary>摘要</summary>
“长期视频理解的挑战仍然受到有效提取对象 semantics 和其关系的模型化限制。虽然 CLIP 视觉特征具有多种视觉任务中的推断性质，特别是对象编码，但它们对长期视频理解不利。为解决这个问题，我们提出了 Attributes-Aware Network（AAN），它包括两个关键组件：Attributes Extractor 和 Graph Reasoning 块。这两个组件可以帮助提取对象-中心的特征和视频中的对象关系。通过利用 CLIP 特征，AAN 超越了现有的状态泰施 Approaches 在 Charades 和 Toyota Smarthome Untrimmed 数据集上表现出色。”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. The Traditional Chinese writing system is also widely used in Taiwan and other parts of the world.
</details></li>
</ul>
<hr>
<h2 id="OpenIns3D-Snap-and-Lookup-for-3D-Open-vocabulary-Instance-Segmentation"><a href="#OpenIns3D-Snap-and-Lookup-for-3D-Open-vocabulary-Instance-Segmentation" class="headerlink" title="OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation"></a>OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00616">http://arxiv.org/abs/2309.00616</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Pointcept/OpenIns3D">https://github.com/Pointcept/OpenIns3D</a></li>
<li>paper_authors: Zhening Huang, Xiaoyang Wu, Xi Chen, Hengshuang Zhao, Lei Zhu, Joan Lasenby</li>
<li>for: This paper is written for 3D open-vocabulary scene understanding at the instance level, without requiring any 2D image inputs.</li>
<li>methods: The OpenIns3D framework uses a “Mask-Snap-Lookup” scheme, which consists of a “Mask” module for class-agnostic mask proposals in 3D point clouds, a “Snap” module for generating synthetic scene-level images, and a “Lookup” module for assigning category names to the proposed masks using Mask2Pixel maps.</li>
<li>results: The proposed approach achieved state-of-the-art results on a wide range of indoor and outdoor datasets with a large margin, and it also allows for effortless switching of 2D detectors without re-training. Additionally, when integrated with state-of-the-art 2D open-world models and large language models (LLMs), it demonstrates excellent performance on open-vocabulary instance segmentation and processing complex text queries.<details>
<summary>Abstract</summary>
Current 3D open-vocabulary scene understanding methods mostly utilize well-aligned 2D images as the bridge to learn 3D features with language. However, applying these approaches becomes challenging in scenarios where 2D images are absent. In this work, we introduce a completely new pipeline, namely, OpenIns3D, which requires no 2D image inputs, for 3D open-vocabulary scene understanding at the instance level. The OpenIns3D framework employs a "Mask-Snap-Lookup" scheme. The "Mask" module learns class-agnostic mask proposals in 3D point clouds. The "Snap" module generates synthetic scene-level images at multiple scales and leverages 2D vision language models to extract interesting objects. The "Lookup" module searches through the outcomes of "Snap" with the help of Mask2Pixel maps, which contain the precise correspondence between 3D masks and synthetic images, to assign category names to the proposed masks. This 2D input-free, easy-to-train, and flexible approach achieved state-of-the-art results on a wide range of indoor and outdoor datasets with a large margin. Furthermore, OpenIns3D allows for effortless switching of 2D detectors without re-training. When integrated with state-of-the-art 2D open-world models such as ODISE and GroundingDINO, superb results are observed on open-vocabulary instance segmentation. When integrated with LLM-powered 2D models like LISA, it demonstrates a remarkable capacity to process highly complex text queries, including those that require intricate reasoning and world knowledge. Project page: https://zheninghuang.github.io/OpenIns3D/
</details>
<details>
<summary>摘要</summary>
当前的3D开 vocabularyScene理解方法通常使用Well-aligned的2D图像作为桥接学习3D特征。然而，在没有2D图像的场景下，这些方法变得困难。在这项工作中，我们引入了一个completely新的管道，即OpenIns3D，它不需要2D图像输入，可以实现3D开 vocabularyScene理解的实例水平。OpenIns3D框架采用“Mask-Snap-Lookup”的方案。“Mask”模块学习类型不敏感的3D点云掩模。“Snap”模块生成多个尺度的 sintetic场景图像，并利用2D视觉语言模型提取有趣的对象。“Lookup”模块通过Mask2Pixel地图，该地图包含3D掩模和 sintetic图像之间的准确对应关系，将提案的掩模分配类别名称。这种没有2D输入、易于训练、灵活的方法在各种室外和室内数据集上实现了状态的前一个Result，并且可以轻松地将2D检测器更换，无需重新训练。当与开放世界2D模型如ODISE和GroundingDINO集成后，Superb的结果被观察到。当与LLM力量2D模型如LISA集成后，它表现出了对高级推理和世界知识的强大处理能力。项目页面：https://zheninghuang.github.io/OpenIns3D/
</details></li>
</ul>
<hr>
<h2 id="CityDreamer-Compositional-Generative-Model-of-Unbounded-3D-Cities"><a href="#CityDreamer-Compositional-Generative-Model-of-Unbounded-3D-Cities" class="headerlink" title="CityDreamer: Compositional Generative Model of Unbounded 3D Cities"></a>CityDreamer: Compositional Generative Model of Unbounded 3D Cities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00610">http://arxiv.org/abs/2309.00610</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hzxie/city-dreamer">https://github.com/hzxie/city-dreamer</a></li>
<li>paper_authors: Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, Ziwei Liu</li>
<li>for: This paper focuses on the generation of 3D cities, which has received less attention in recent years despite the greater challenges it poses due to human sensitivity to structural distortions and the complexity of generating buildings with a wide range of appearances.</li>
<li>methods: The proposed method, CityDreamer, is a compositional generative model that separates the generation of building instances from other background objects into distinct modules, and uses two datasets (OSM and GoogleEarth) to enhance the realism of the generated 3D cities.</li>
<li>results: Through extensive experiments, CityDreamer has proven its superiority over state-of-the-art methods in generating a wide range of lifelike 3D cities.Here’s the text in Traditional Chinese for reference:</li>
<li>for: 这篇论文主要关注3D城市生成，这个领域在最近几年来受到了更多的研究，但是3D城市生成仍然受到了更大的挑战，主要是因为人类对城市环境的构造性变化更加敏感，而且生成建筑物的类型和外观相对更加复杂。</li>
<li>methods: 提案的方法是CityDreamer，这是一个具有分类生成模型的3D城市生成方法，它将建筑物实例的生成与其他背景物体（如道路、绿地和水域）分类为不同的模块，并使用OSM和GoogleEarth这两个 datasets来增强生成的3D城市的现实感。</li>
<li>results: 经过了广泛的实验，CityDreamer已经证明了它在生成各种生活力强的3D城市方面的优越性，比起现有的方法更加出色。<details>
<summary>Abstract</summary>
In recent years, extensive research has focused on 3D natural scene generation, but the domain of 3D city generation has not received as much exploration. This is due to the greater challenges posed by 3D city generation, mainly because humans are more sensitive to structural distortions in urban environments. Additionally, generating 3D cities is more complex than 3D natural scenes since buildings, as objects of the same class, exhibit a wider range of appearances compared to the relatively consistent appearance of objects like trees in natural scenes. To address these challenges, we propose CityDreamer, a compositional generative model designed specifically for unbounded 3D cities, which separates the generation of building instances from other background objects, such as roads, green lands, and water areas, into distinct modules. Furthermore, we construct two datasets, OSM and GoogleEarth, containing a vast amount of real-world city imagery to enhance the realism of the generated 3D cities both in their layouts and appearances. Through extensive experiments, CityDreamer has proven its superiority over state-of-the-art methods in generating a wide range of lifelike 3D cities.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Time-Series-Analysis-of-Urban-Liveability"><a href="#Time-Series-Analysis-of-Urban-Liveability" class="headerlink" title="Time Series Analysis of Urban Liveability"></a>Time Series Analysis of Urban Liveability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00594">http://arxiv.org/abs/2309.00594</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex Levering, Diego Marcos, Devis Tuia</li>
<li>for: 这篇论文探讨了深度学习模型来监测荷兰城市的长期生活品质变化。</li>
<li>methods: 该论文使用了高分辨率飞行图像和年度生活指标组合生成年度时间步骤，并使用了一个基于2016年飞行图像和生活指标的卷积神经网络来预测新时间步骤的生活品质。</li>
<li>results: 在训练城市（阿姆斯特丹）和 nunca before seen 城市（英顿）的结果中，显示了一些难以理解的趋势，特别是在不同时间步骤的图像获取方式下。这种结果 demonstarte了监测生活品质变化的复杂性，以及需要更加复杂的方法来补做不同于生活品质动态的变化。<details>
<summary>Abstract</summary>
In this paper we explore deep learning models to monitor longitudinal liveability changes in Dutch cities at the neighbourhood level. Our liveability reference data is defined by a country-wise yearly survey based on a set of indicators combined into a liveability score, the Leefbaarometer. We pair this reference data with yearly-available high-resolution aerial images, which creates yearly timesteps at which liveability can be monitored. We deploy a convolutional neural network trained on an aerial image from 2016 and the Leefbaarometer score to predict liveability at new timesteps 2012 and 2020. The results in a city used for training (Amsterdam) and one never seen during training (Eindhoven) show some trends which are difficult to interpret, especially in light of the differences in image acquisitions at the different time steps. This demonstrates the complexity of liveability monitoring across time periods and the necessity for more sophisticated methods compensating for changes unrelated to liveability dynamics.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们探讨深度学习模型来监测荷兰城市的长期生活质量变化。我们的生活质量参照数据是基于年度国家调查，并将各个指标组合成一个生活质量分数，称为Leefbaarometer。我们将年度可用的高分辨率航空图像与参照数据对应，从而创造了年度时间步。我们使用2016年的航空图像和Leefbaarometer分数来训练卷积神经网络，并用这些神经网络预测2012年和2020年的生活质量。在训练城市（阿姆斯特丹）和 nunca before seen during training 的城市（恩登霍恩）的结果中，我们发现了一些难以理解的趋势，特别是在不同时间步的图像获取方式的影响下。这表明监测生活质量的变化 across time periods 的复杂性，以及需要更加复杂的方法来补做不related to liveability dynamics的变化。
</details></li>
</ul>
<hr>
<h2 id="Discrete-Morphological-Neural-Networks"><a href="#Discrete-Morphological-Neural-Networks" class="headerlink" title="Discrete Morphological Neural Networks"></a>Discrete Morphological Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00588">http://arxiv.org/abs/2309.00588</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dmarcondes/dmnn">https://github.com/dmarcondes/dmnn</a></li>
<li>paper_authors: Diego Marcondes, Junior Barrera</li>
<li>for: 本文提出了一种基于数学形态学（MM）的二进制图像运算设计方法，即离散形态神经网络（DMNN），用于二进制图像分析。</li>
<li>methods: 本文提出了一种基于机器学习的离散形态神经网络（DMNN）架构，该架构采用了传统的 morphological computational graph，并通过一种名为 lattice gradient descent algorithm（LGDA）来训练这些参数。</li>
<li>results: 本文应用了 DMNN 来识别受噪的数字边缘，并讨论了多个未来研究的话题。<details>
<summary>Abstract</summary>
A classical approach to designing binary image operators is Mathematical Morphology (MM). We propose the Discrete Morphological Neural Networks (DMNN) for binary image analysis to represent W-operators and estimate them via machine learning. A DMNN architecture, which is represented by a Morphological Computational Graph, is designed as in the classical heuristic design of morphological operators, in which the designer should combine a set of MM operators and Boolean operations based on prior information and theoretical knowledge. Then, once the architecture is fixed, instead of adjusting its parameters (i.e., structural elements or maximal intervals) by hand, we propose a lattice gradient descent algorithm (LGDA) to train these parameters based on a sample of input and output images under the usual machine learning approach. We also propose a stochastic version of the LGDA that is more efficient, is scalable and can obtain small error in practical problems. The class represented by a DMNN can be quite general or specialized according to expected properties of the target operator, i.e., prior information, and the semantic expressed by algebraic properties of classes of operators is a differential relative to other methods. The main contribution of this paper is the merger of the two main paradigms for designing morphological operators: classical heuristic design and automatic design via machine learning. Thus, conciliating classical heuristic morphological operator design with machine learning. We apply the DMNN to recognize the boundary of digits with noise, and we discuss many topics for future research.
</details>
<details>
<summary>摘要</summary>
经典方法设计二进制图像运算员是数学形态学（MM）。我们提议使用数字形态神经网络（DMNN）来表示二进制图像分析，以代表W-运算员并使用机器学习来估算。DMNN架构，表示为形态计算图，是根据经典的规范设计形态操作员，其中设计师将组合一组MM操作员和逻辑运算，根据优化目标和理论知识。然后，在架构固定后，而不是手动调整其结构元素或最大间隔的参数，我们提议使用格子梯度下降算法（LGDA）来训练这些参数，根据输入和输出图像的样本。我们还提议一种随机版本的LGDA，它更高效、可扩展和可以在实际问题中获得小的错误。DMNN的类可以是非常一般或特殊，根据预期的目标运算员的特性和优化目标。我们的主要贡献在于将经典的规范设计和自动设计融合在一起，因此把经典规范设计与机器学习融合在一起。我们应用DMNN来识别数字的边缘，并讨论了许多未来研究的话题。
</details></li>
</ul>
<hr>
<h2 id="Mechanism-of-feature-learning-in-convolutional-neural-networks"><a href="#Mechanism-of-feature-learning-in-convolutional-neural-networks" class="headerlink" title="Mechanism of feature learning in convolutional neural networks"></a>Mechanism of feature learning in convolutional neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00570">http://arxiv.org/abs/2309.00570</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aradha/convrfm">https://github.com/aradha/convrfm</a></li>
<li>paper_authors: Daniel Beaglehole, Adityanarayanan Radhakrishnan, Parthe Pandit, Mikhail Belkin</li>
<li>for: 本研究旨在解释深度学习中图像数据中特征学习的机制。</li>
<li>methods: 我们提出了卷积神经特征假设，即卷积层 filters的covariances是对输入图像中patches的average gradient outer product（AGOP）的平均值。我们提供了丰富的实验证据，包括AlexNet、VGG和ResNets等标准神经网络在ImageNet上的预训练时，卷积层 filters的covariances和patch-based AGOPs之间高度相关性。我们还提供了支持性的理论证据。</li>
<li>results: 我们的结果表明，使用patch-based AGOP可以启用深度特征学习在卷积核机中。我们称之为（深）ConvRFM，并证明其能够恢复深度 convolutional networks 中的类似特征。此外，我们发现Deep ConvRFM可以超越先前发现的卷积核的限制，例如对本地信号的适应能力和不可变性，从而导致性能提高。<details>
<summary>Abstract</summary>
Understanding the mechanism of how convolutional neural networks learn features from image data is a fundamental problem in machine learning and computer vision. In this work, we identify such a mechanism. We posit the Convolutional Neural Feature Ansatz, which states that covariances of filters in any convolutional layer are proportional to the average gradient outer product (AGOP) taken with respect to patches of the input to that layer. We present extensive empirical evidence for our ansatz, including identifying high correlation between covariances of filters and patch-based AGOPs for convolutional layers in standard neural architectures, such as AlexNet, VGG, and ResNets pre-trained on ImageNet. We also provide supporting theoretical evidence. We then demonstrate the generality of our result by using the patch-based AGOP to enable deep feature learning in convolutional kernel machines. We refer to the resulting algorithm as (Deep) ConvRFM and show that our algorithm recovers similar features to deep convolutional networks including the notable emergence of edge detectors. Moreover, we find that Deep ConvRFM overcomes previously identified limitations of convolutional kernels, such as their inability to adapt to local signals in images and, as a result, leads to sizable performance improvement over fixed convolutional kernels.
</details>
<details>
<summary>摘要</summary>
We then demonstrate the generality of our result by using the patch-based AGOP to enable deep feature learning in convolutional kernel machines. We refer to the resulting algorithm as (Deep) ConvRFM and show that our algorithm recovers similar features to deep convolutional networks, including the notable emergence of edge detectors. Moreover, we find that Deep ConvRFM overcomes previously identified limitations of convolutional kernels, such as their inability to adapt to local signals in images, and leads to sizable performance improvement over fixed convolutional kernels.
</details></li>
</ul>
<hr>
<h2 id="Amyloid-Beta-Axial-Plane-PET-Synthesis-from-Structural-MRI-An-Image-Translation-Approach-for-Screening-Alzheimer’s-Disease"><a href="#Amyloid-Beta-Axial-Plane-PET-Synthesis-from-Structural-MRI-An-Image-Translation-Approach-for-Screening-Alzheimer’s-Disease" class="headerlink" title="Amyloid-Beta Axial Plane PET Synthesis from Structural MRI: An Image Translation Approach for Screening Alzheimer’s Disease"></a>Amyloid-Beta Axial Plane PET Synthesis from Structural MRI: An Image Translation Approach for Screening Alzheimer’s Disease</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00569">http://arxiv.org/abs/2309.00569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fernando Vega, Abdoljalil Addeh, M. Ethan MacDonald</li>
<li>for: 用于生成基于MRI的Synthetic抑衰β蛋白PET图像，以便获取β蛋白信息。</li>
<li>methods: 使用图像翻译模型，将MRI图像与β蛋白PET图像进行对应，以实现从结构图像到量化图像的转换。</li>
<li>results: 通过对MRI图像和β蛋白PET图像的对应，可以生成高度相似于真实的Synthetic抑衰β蛋白PET图像，具有高度的SSIM和PSNR。<details>
<summary>Abstract</summary>
In this work, an image translation model is implemented to produce synthetic amyloid-beta PET images from structural MRI that are quantitatively accurate. Image pairs of amyloid-beta PET and structural MRI were used to train the model. We found that the synthetic PET images could be produced with a high degree of similarity to truth in terms of shape, contrast and overall high SSIM and PSNR. This work demonstrates that performing structural to quantitative image translation is feasible to enable the access amyloid-beta information from only MRI.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们实现了一种图像翻译模型，以生成基于MRI的蛋白β扩散图像，并且这些图像具有高度准确的量化性。我们使用了蛋白β扩散图像和MRI图像的对应对来训练模型。我们发现，使用这种方法可以生成高度相似于真实的蛋白β扩散图像，包括形态、对比度和总体高度匹配SSIM和PSNR。这个研究表明，从MRI图像中获取蛋白β信息是可能的，并且这种方法可以帮助解决蛋白β扩散图像的缺失问题。
</details></li>
</ul>
<hr>
<h2 id="Fused-Classification-For-Differential-Face-Morphing-Detection"><a href="#Fused-Classification-For-Differential-Face-Morphing-Detection" class="headerlink" title="Fused Classification For Differential Face Morphing Detection"></a>Fused Classification For Differential Face Morphing Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00665">http://arxiv.org/abs/2309.00665</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iurii Medvedev, Joana Pimenta, Nuno Gonçalves</li>
<li>for: 防止面部识别系统受到面形变换攻击</li>
<li>methods: 基于融合分类方法进行无参数检测</li>
<li>results: 实验结果表明方法有效地检测 morphing 攻击<details>
<summary>Abstract</summary>
Face morphing, a sophisticated presentation attack technique, poses significant security risks to face recognition systems. Traditional methods struggle to detect morphing attacks, which involve blending multiple face images to create a synthetic image that can match different individuals. In this paper, we focus on the differential detection of face morphing and propose an extended approach based on fused classification method for no-reference scenario. We introduce a public face morphing detection benchmark for the differential scenario and utilize a specific data mining technique to enhance the performance of our approach. Experimental results demonstrate the effectiveness of our method in detecting morphing attacks.
</details>
<details>
<summary>摘要</summary>
面部融合攻击，一种复杂的演示攻击技术，对人脸识别系统 pose  significiant 安全风险。传统方法很难探测融合攻击，这些攻击 involve 混合多个人脸图像生成一个合成图像，可以与不同个体匹配。在这篇论文中，我们关注 differential 探测方法，并提出基于融合分类方法的延展方法，用于无参考enario。我们引入一个公共人脸融合检测标准套件，并利用特定的数据挖掘技术来提高我们的方法的性能。实验结果表明我们的方法有效地探测融合攻击。
</details></li>
</ul>
<hr>
<h2 id="Impact-of-Image-Context-for-Single-Deep-Learning-Face-Morphing-Attack-Detection"><a href="#Impact-of-Image-Context-for-Single-Deep-Learning-Face-Morphing-Attack-Detection" class="headerlink" title="Impact of Image Context for Single Deep Learning Face Morphing Attack Detection"></a>Impact of Image Context for Single Deep Learning Face Morphing Attack Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00549">http://arxiv.org/abs/2309.00549</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joana Pimenta, Iurii Medvedev, Nuno Gonçalves</li>
<li>for: 本研究探讨了深度学习 face morphing 检测性能如何受到输入图像对齐设置的影响。</li>
<li>methods: 本研究使用了深度学习技术对 face morphing 进行检测，并分析了面 contour 和图像上下文之间的关系，以提出优化输入图像对齐的方法。</li>
<li>results: 研究发现，适当地调整输入图像对齐设置可以提高深度学习 face morphing 检测性能。<details>
<summary>Abstract</summary>
The increase in security concerns due to technological advancements has led to the popularity of biometric approaches that utilize physiological or behavioral characteristics for enhanced recognition. Face recognition systems (FRSs) have become prevalent, but they are still vulnerable to image manipulation techniques such as face morphing attacks. This study investigates the impact of the alignment settings of input images on deep learning face morphing detection performance. We analyze the interconnections between the face contour and image context and suggest optimal alignment conditions for face morphing detection.
</details>
<details>
<summary>摘要</summary>
技术进步引起的安全问题带来了基于生物特征的识别方法的普遍性，特别是面Recognition系统（FRS）。然而，FRS仍然容易受到图像修改技术的袭击，如面形变换攻击。本研究研究输入图像的Alignment设置对深度学习面形变换检测性能的影响。我们分析面 outline和图像Context之间的关系，并提出优化Alignment条件以提高面形变换检测性能。Note: Simplified Chinese is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="Trust-your-Good-Friends-Source-free-Domain-Adaptation-by-Reciprocal-Neighborhood-Clustering"><a href="#Trust-your-Good-Friends-Source-free-Domain-Adaptation-by-Reciprocal-Neighborhood-Clustering" class="headerlink" title="Trust your Good Friends: Source-free Domain Adaptation by Reciprocal Neighborhood Clustering"></a>Trust your Good Friends: Source-free Domain Adaptation by Reciprocal Neighborhood Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00528">http://arxiv.org/abs/2309.00528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz, Shangling Jui, Jian Yang</li>
<li>for: 本研究目的是解决无法获取源数据的情况下，进行领域适应（DA）的问题。</li>
<li>methods: 我们的方法基于目标数据中的自然结构，包括本地相似性和扩展 neighboorhood。</li>
<li>results: 我们的方法在多个2D图像和3D点云识别dataset上达到了状态 искусственный智能的性能。<details>
<summary>Abstract</summary>
Domain adaptation (DA) aims to alleviate the domain shift between source domain and target domain. Most DA methods require access to the source data, but often that is not possible (e.g. due to data privacy or intellectual property). In this paper, we address the challenging source-free domain adaptation (SFDA) problem, where the source pretrained model is adapted to the target domain in the absence of source data. Our method is based on the observation that target data, which might not align with the source domain classifier, still forms clear clusters. We capture this intrinsic structure by defining local affinity of the target data, and encourage label consistency among data with high local affinity. We observe that higher affinity should be assigned to reciprocal neighbors. To aggregate information with more context, we consider expanded neighborhoods with small affinity values. Furthermore, we consider the density around each target sample, which can alleviate the negative impact of potential outliers. In the experimental results we verify that the inherent structure of the target features is an important source of information for domain adaptation. We demonstrate that this local structure can be efficiently captured by considering the local neighbors, the reciprocal neighbors, and the expanded neighborhood. Finally, we achieve state-of-the-art performance on several 2D image and 3D point cloud recognition datasets.
</details>
<details>
<summary>摘要</summary>
领域适应（DA）的目标是解决源领域和目标领域之间的频率差异。大多数DA方法需要访问源数据，但在一些情况下，这并不可能（例如，由于数据隐私或知识产权等原因）。在这篇论文中，我们面临着难以进行频率适应（SFDA）问题，其中源预训练模型被应用到目标领域中，无法访问源数据。我们基于目标数据的内在结构的观察，即目标数据可能不符合源领域分类器的分类结果。我们通过定义目标数据的本地相互关系来捕捉这种内在结构，并且鼓励标签相互一致。我们发现，更高的相互关系应该被分配给对应的反向邻居。为了更好地融合信息，我们考虑了扩展的邻里域，并且考虑了每个目标样本的扩展邻里域。此外，我们还考虑了每个目标样本的密度，以避免潜在的异常值的影响。在实验结果中，我们证明了目标特征的内在结构是频率适应中的重要信息来源。我们表明了这种本地结构可以通过考虑本地邻居、反向邻居和扩展邻里域来效率地捕捉。最后，我们在多个2D图像和3D点云认知dataset上实现了状态的最佳性能。
</details></li>
</ul>
<hr>
<h2 id="SQLdepth-Generalizable-Self-Supervised-Fine-Structured-Monocular-Depth-Estimation"><a href="#SQLdepth-Generalizable-Self-Supervised-Fine-Structured-Monocular-Depth-Estimation" class="headerlink" title="SQLdepth: Generalizable Self-Supervised Fine-Structured Monocular Depth Estimation"></a>SQLdepth: Generalizable Self-Supervised Fine-Structured Monocular Depth Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00526">http://arxiv.org/abs/2309.00526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youhong Wang, Yunji Liang, Hao Xu, Shaohui Jiao, Hongkai Yu</li>
<li>for: 本研究旨在提出一种可以有效地从动态视觉中学习细腻场景结构的自助监督灰度估计方法，以提高自主驾驶和 роботех学的应用性。</li>
<li>methods: 本方法提出了一种新的 Self Query Layer (SQL)，用于建立自身成本量，从而直接从量中INFER depth，而不是从特征图中INFER depth。这种自身成本量隐式地捕捉了场景的内在几何结构，每个时刻的卷积缓存都表示了相对的距离关系。最后，这个量通过一种新的解码方法转换为深度图。</li>
<li>results: 实验结果表明，我们的方法在KITTI和Cityscapes上达到了remarkable的状态数据表现（AbsRel &#x3D; $0.082$ on KITTI, $0.052$ on KITTI with improved ground-truth, $0.106$ on Cityscapes），与前一个最佳方法相比，减少了9.9%、5.5%和4.5%的误差。此外，我们的方法还展示了减少训练复杂度、计算效率、改进的普适性和可以恢复细腻场景细节的能力。同时，自身匹配推导的SQLdepth可以在不同的摄像头和环境下进行适应性训练，并且可以在不同的任务上进行适应性测试。<details>
<summary>Abstract</summary>
Recently, self-supervised monocular depth estimation has gained popularity with numerous applications in autonomous driving and robotics. However, existing solutions primarily seek to estimate depth from immediate visual features, and struggle to recover fine-grained scene details with limited generalization. In this paper, we introduce SQLdepth, a novel approach that can effectively learn fine-grained scene structures from motion. In SQLdepth, we propose a novel Self Query Layer (SQL) to build a self-cost volume and infer depth from it, rather than inferring depth from feature maps. The self-cost volume implicitly captures the intrinsic geometry of the scene within a single frame. Each individual slice of the volume signifies the relative distances between points and objects within a latent space. Ultimately, this volume is compressed to the depth map via a novel decoding approach. Experimental results on KITTI and Cityscapes show that our method attains remarkable state-of-the-art performance (AbsRel = $0.082$ on KITTI, $0.052$ on KITTI with improved ground-truth and $0.106$ on Cityscapes), achieves $9.9\%$, $5.5\%$ and $4.5\%$ error reduction from the previous best. In addition, our approach showcases reduced training complexity, computational efficiency, improved generalization, and the ability to recover fine-grained scene details. Moreover, the self-supervised pre-trained and metric fine-tuned SQLdepth can surpass existing supervised methods by significant margins (AbsRel = $0.043$, $14\%$ error reduction). self-matching-oriented relative distance querying in SQL improves the robustness and zero-shot generalization capability of SQLdepth. Code and the pre-trained weights will be publicly available. Code is available at \href{https://github.com/hisfog/SQLdepth-Impl}{https://github.com/hisfog/SQLdepth-Impl}.
</details>
<details>
<summary>摘要</summary>
最近，自主指导单目深度估计已经在自动驾驶和机器人领域得到了广泛的应用。然而，现有的解决方案主要是从直接视觉特征中估计深度，而忽略了细节Scene的恢复。在这篇论文中，我们提出了一种新的方法，即SQLdepth，可以有效地从运动中学习细节Scene的结构。在SQLdepth中，我们提出了一种新的Self Query层（SQL），用于建立自身成本Volume并从中估计深度，而不是从特征图进行估计。这个自身成本Volume隐式地捕捉了场景的内在几何结构，每个层次的尺度都表示了点和物体之间的相对距离在潜在空间中。最终，这个Volume通过一种新的解码方法压缩到深度图。实验结果表明，我们的方法在KITTI和Cityscapes上达到了非常出色的状态态表现（AbsRel = $0.082$ on KITTI, $0.052$ on KITTI with improved ground truth and $0.106$ on Cityscapes），实现了$9.9\%$, $5.5\%$和$4.5\%$的错误减少。此外，我们的方法还显示了减少训练复杂性、计算效率、改进的泛化和恢复细节Scene的能力。此外，自主匹配方向的相对距离查询在SQL中提高了SQLdepth的Robustness和零shot泛化能力。代码和预训练 веса将在公共可用。代码可以在 \href{https://github.com/hisfog/SQLdepth-Impl}{https://github.com/hisfog/SQLdepth-Impl} 上找到。
</details></li>
</ul>
<hr>
<h2 id="A-Machine-Vision-Method-for-Correction-of-Eccentric-Error-Based-on-Adaptive-Enhancement-Algorithm"><a href="#A-Machine-Vision-Method-for-Correction-of-Eccentric-Error-Based-on-Adaptive-Enhancement-Algorithm" class="headerlink" title="A Machine Vision Method for Correction of Eccentric Error: Based on Adaptive Enhancement Algorithm"></a>A Machine Vision Method for Correction of Eccentric Error: Based on Adaptive Enhancement Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00514">http://arxiv.org/abs/2309.00514</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fanyi Wang, Pin Cao, Yihui Zhang, Haotian Hu, Yongying Yang</li>
<li>for: 这 paper 的目的是提出一种机器视觉方法来修正大开口几何镜元件上的表面缺陷。</li>
<li>methods: 该方法使用了改进的 Adaptive Enhancement Algorithm (AEA), 包括现有的 Guided Filter Dark Channel Dehazing Algorithm (GFA) 和提出的轻量级 Multi-scale Densely Connected Network (MDC-Net)。</li>
<li>results: 该方法可以减少表面缺陷的误差到 Within 10um，并且具有一定的实时性。<details>
<summary>Abstract</summary>
In the procedure of surface defects detection for large-aperture aspherical optical elements, it is of vital significance to adjust the optical axis of the element to be coaxial with the mechanical spin axis accurately. Therefore, a machine vision method for eccentric error correction is proposed in this paper. Focusing on the severe defocus blur of reference crosshair image caused by the imaging characteristic of the aspherical optical element, which may lead to the failure of correction, an Adaptive Enhancement Algorithm (AEA) is proposed to strengthen the crosshair image. AEA is consisted of existed Guided Filter Dark Channel Dehazing Algorithm (GFA) and proposed lightweight Multi-scale Densely Connected Network (MDC-Net). The enhancement effect of GFA is excellent but time-consuming, and the enhancement effect of MDC-Net is slightly inferior but strongly real-time. As AEA will be executed dozens of times during each correction procedure, its real-time performance is very important. Therefore, by setting the empirical threshold of definition evaluation function SMD2, GFA and MDC-Net are respectively applied to highly and slightly blurred crosshair images so as to ensure the enhancement effect while saving as much time as possible. AEA has certain robustness in time-consuming performance, which takes an average time of 0.2721s and 0.0963s to execute GFA and MDC-Net separately on ten 200pixels 200pixels Region of Interest (ROI) images with different degrees of blur. And the eccentricity error can be reduced to within 10um by our method.
</details>
<details>
<summary>摘要</summary>
在大开口非球面光元件表面缺陷检测过程中，准确调整光轴与机械轴的准确性至关重要。因此，这篇文章提出了一种机器视觉方法来修正不对称错误。关注大开口非球面光元件的极大模糊效应，可能导致修正失败，这篇文章提出了一种适应增强算法（AEA）来强化交叉线图像。AEA由现有的导引灰度黑色滤波算法（GFA）和提议的轻量级多尺度紧密连接网络（MDC-Net）组成。GFA的增强效果非常好，但是时间耗费较长，而MDC-Net的增强效果较弱，但是实时性非常好。因此，在每次修正过程中，AEA将被执行数十次，因此其实时性非常重要。因此，通过设置定义评估函数SMD2的实际阈值，GFA和MDC-Net分别应用于高度模糊和轻度模糊的交叉线图像，以确保增强效果而尽可能地避免时间浪费。AEA具有一定的实时性，每个ROI图像需要0.2721秒和0.0963秒分别使用GFA和MDC-Net进行处理，并且可以将不对称错误降到10um以下。
</details></li>
</ul>
<hr>
<h2 id="Multi-stage-Deep-Learning-Artifact-Reduction-for-Computed-Tomography"><a href="#Multi-stage-Deep-Learning-Artifact-Reduction-for-Computed-Tomography" class="headerlink" title="Multi-stage Deep Learning Artifact Reduction for Computed Tomography"></a>Multi-stage Deep Learning Artifact Reduction for Computed Tomography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00494">http://arxiv.org/abs/2309.00494</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayang Shi, Daniel M. Pelt, K. Joost Batenburg</li>
<li>For: 提高计算tomography（CT）图像质量，减少图像artefacts。* Methods: 使用多个域（如投影图像和重建图像）的深度学习方法进行artefact除去，与传统的CT处理管道类似。* Results: 对于 simulate和实际实验数据集，我们的方法可以减少artefacts，并且比deep learning基于后处理的方法更高效。<details>
<summary>Abstract</summary>
In Computed Tomography (CT), an image of the interior structure of an object is computed from a set of acquired projection images. The quality of these reconstructed images is essential for accurate analysis, but this quality can be degraded by a variety of imaging artifacts. To improve reconstruction quality, the acquired projection images are often processed by a pipeline consisting of multiple artifact-removal steps applied in various image domains (e.g., outlier removal on projection images and denoising of reconstruction images). These artifact-removal methods exploit the fact that certain artifacts are easier to remove in a certain domain compared with other domains.   Recently, deep learning methods have shown promising results for artifact removal for CT images. However, most existing deep learning methods for CT are applied as a post-processing method after reconstruction. Therefore, artifacts that are relatively difficult to remove in the reconstruction domain may not be effectively removed by these methods. As an alternative, we propose a multi-stage deep learning method for artifact removal, in which neural networks are applied to several domains, similar to a classical CT processing pipeline. We show that the neural networks can be effectively trained in succession, resulting in easy-to-use and computationally efficient training. Experiments on both simulated and real-world experimental datasets show that our method is effective in reducing artifacts and superior to deep learning-based post-processing.
</details>
<details>
<summary>摘要</summary>
在计算 Tomography（CT）中，通过一组获取的投影图像计算对象内部结构的图像。这些重建图像的质量非常重要，但可能受到多种损害像素的影响。为了提高重建质量，通常将获取的投影图像处理为多个遗传元素去除抗错的步骤，这些步骤在不同的图像领域（例如，投影图像上的异常值除去和重建图像上的锈除）。这些遗传元素去除方法利用了某些遗传元素更易于在某个领域中除去，相比之下其他领域。  最近，深度学习方法在CT图像中表现出了扩展的成果。然而，大多数现有的深度学习方法在CT图像中是作为后处理方法进行应用，因此，在重建领域中存在一些难以除去的遗传元素可能无法有效地除去。作为替代方案，我们提出了一种多Stage深度学习方法，在这种方法中，神经网络在多个领域中应用，类似于传统的CT处理管道。我们发现，这些神经网络可以在继序中有效地训练，从而实现了容易使用和计算效率高的训练。在模拟和实际实验数据集上，我们的方法能够有效地减少遗传元素，并且与深度学习基于后处理的方法相比，表现出优异。
</details></li>
</ul>
<hr>
<h2 id="Asymmetric-double-winged-multi-view-clustering-network-for-exploring-Diverse-and-Consistent-Information"><a href="#Asymmetric-double-winged-multi-view-clustering-network-for-exploring-Diverse-and-Consistent-Information" class="headerlink" title="Asymmetric double-winged multi-view clustering network for exploring Diverse and Consistent Information"></a>Asymmetric double-winged multi-view clustering network for exploring Diverse and Consistent Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00474">http://arxiv.org/abs/2309.00474</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qun Zheng, Xihong Yang, Siwei Wang, Xinru An, Qi Liu</li>
<li>for: 这个论文旨在提出一个新的多观点聚类网络（CodingNet），以同时探索多观点数据中的多样和一致信息。</li>
<li>methods: 这个网络使用非对称架构，分别提取了 shallow 和 deep 特征。然后，通过调整 shallow 特征相似度矩阵，以确保多观点数据的多样性。此外，我们提出了一个双重对比机制，以维持 deep 特征在多观点和伪标端层上的一致性。</li>
<li>results: 在六个广泛使用的 benchmarkt 数据集上进行了广泛的实验，证明了我们的框架在多观点聚类 зада期中的高效性，并大多超过了现有的多观点聚类算法。<details>
<summary>Abstract</summary>
In unsupervised scenarios, deep contrastive multi-view clustering (DCMVC) is becoming a hot research spot, which aims to mine the potential relationships between different views. Most existing DCMVC algorithms focus on exploring the consistency information for the deep semantic features, while ignoring the diverse information on shallow features. To fill this gap, we propose a novel multi-view clustering network termed CodingNet to explore the diverse and consistent information simultaneously in this paper. Specifically, instead of utilizing the conventional auto-encoder, we design an asymmetric structure network to extract shallow and deep features separately. Then, by aligning the similarity matrix on the shallow feature to the zero matrix, we ensure the diversity for the shallow features, thus offering a better description of multi-view data. Moreover, we propose a dual contrastive mechanism that maintains consistency for deep features at both view-feature and pseudo-label levels. Our framework's efficacy is validated through extensive experiments on six widely used benchmark datasets, outperforming most state-of-the-art multi-view clustering algorithms.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。</SYS>>在无监督场景下，深度对比多视图划分（DCMVC）正在成为研究热点，旨在挖掘不同视图之间的潜在关系。现有大多数DCMVC算法都是针对深度 semantic features的一致信息进行探索，而忽略了不同视图之间的多样信息。为了填补这一漏洞，我们在本文提出了一种新的多视图划分网络，称为 codingNet，以同时探索多视图数据中的多样和一致信息。具体来说，我们不使用传统的自编码器，而是设计了一种非对称结构网络，用于分离不同视图中的 shallow 和 deep features。然后，通过将 shallow 特征相似矩阵与零矩阵进行对齐，我们保证了多视图数据中 shallow 特征的多样性，从而为多视图划分提供更好的描述。此外，我们还提出了一种双对照机制，以保持深度特征在多视图和 pseudo-标签 两级层次上的一致性。我们的框架在六种广泛使用的 benchmark 数据集上进行了广泛的实验，并与大多数现状的多视图划分算法进行比较，证明了我们的框架的效果。
</details></li>
</ul>
<hr>
<h2 id="General-and-Practical-Tuning-Method-for-Off-the-Shelf-Graph-Based-Index-SISAP-Indexing-Challenge-Report-by-Team-UTokyo"><a href="#General-and-Practical-Tuning-Method-for-Off-the-Shelf-Graph-Based-Index-SISAP-Indexing-Challenge-Report-by-Team-UTokyo" class="headerlink" title="General and Practical Tuning Method for Off-the-Shelf Graph-Based Index: SISAP Indexing Challenge Report by Team UTokyo"></a>General and Practical Tuning Method for Off-the-Shelf Graph-Based Index: SISAP Indexing Challenge Report by Team UTokyo</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00472">http://arxiv.org/abs/2309.00472</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mti-lab/utokyo-sisap23-challenge-submission">https://github.com/mti-lab/utokyo-sisap23-challenge-submission</a></li>
<li>paper_authors: Yutaro Oguri, Yusuke Matsui</li>
<li>for: 本研究旨在优化 graf-based 算法 для Approximate Nearest Neighbor (ANN) 搜索，并提供一种可靠的、 universally 适用的索引调整方法。</li>
<li>methods: 本研究使用了一种黑盒优化算法进行集成调整，以满足需要的召回率和 Queries Per Second (QPS) 要求。</li>
<li>results: 本研究在 SISAP 2023 Indexing Challenge 的 Task A 中获得第二名，在 10M 和 30M  tracks 上显著提高了性能，相比较简单的方法。这种研究方法可以扩展到更广泛的应用场景。<details>
<summary>Abstract</summary>
Despite the efficacy of graph-based algorithms for Approximate Nearest Neighbor (ANN) searches, the optimal tuning of such systems remains unclear. This study introduces a method to tune the performance of off-the-shelf graph-based indexes, focusing on the dimension of vectors, database size, and entry points of graph traversal. We utilize a black-box optimization algorithm to perform integrated tuning to meet the required levels of recall and Queries Per Second (QPS). We applied our approach to Task A of the SISAP 2023 Indexing Challenge and got second place in the 10M and 30M tracks. It improves performance substantially compared to brute force methods. This research offers a universally applicable tuning method for graph-based indexes, extending beyond the specific conditions of the competition to broader uses.
</details>
<details>
<summary>摘要</summary>
尽管图表基本算法在approximate nearest neighbor（ANN）搜索中的效果是明显的，但是最佳化这些系统的调整仍然不清楚。这个研究介绍了一种方法来调整off-the-shelf图表基本索引的性能，专注于维度 Vector，数据库大小，和图表搜索的入口点。我们使用黑盒优化算法进行集成调整，以达到需要的回快和Queries Per Second（QPS）水平。我们在SISAP 2023 Indexing Challenge的任务A中应用了我们的方法，在10M和30M tracks上获得了第二名，与简单方法相比，性能提高了很多。这项研究提供了一种通用的图表索引调整方法，超出了竞赛中的特定条件，拓展到更广泛的应用场景。
</details></li>
</ul>
<hr>
<h2 id="An-Improved-Encoder-Decoder-Framework-for-Food-Energy-Estimation"><a href="#An-Improved-Encoder-Decoder-Framework-for-Food-Energy-Estimation" class="headerlink" title="An Improved Encoder-Decoder Framework for Food Energy Estimation"></a>An Improved Encoder-Decoder Framework for Food Energy Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00468">http://arxiv.org/abs/2309.00468</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jack Ma, Jiangpeng He, Fengqing Zhu</li>
<li>for: 这个研究旨在提供一种自动化的食物营养评估方法，以便维护健康生活方式。</li>
<li>methods: 本研究使用改进的encoder-decoder框架估算食物能量，将食物影像转换为具有食物能量信息的更易提取格式，然后将能量信息提取出来。</li>
<li>results: 本研究比前一代营养估算方法提高了10%以上和30千卡拉以上的MAP和MAE分别。<details>
<summary>Abstract</summary>
Dietary assessment is essential to maintaining a healthy lifestyle. Automatic image-based dietary assessment is a growing field of research due to the increasing prevalence of image capturing devices (e.g. mobile phones). In this work, we estimate food energy from a single monocular image, a difficult task due to the limited hard-to-extract amount of energy information present in an image. To do so, we employ an improved encoder-decoder framework for energy estimation; the encoder transforms the image into a representation embedded with food energy information in an easier-to-extract format, which the decoder then extracts the energy information from. To implement our method, we compile a high-quality food image dataset verified by registered dietitians containing eating scene images, food-item segmentation masks, and ground truth calorie values. Our method improves upon previous caloric estimation methods by over 10\% and 30 kCal in terms of MAPE and MAE respectively.
</details>
<details>
<summary>摘要</summary>
饮食评估是保持健康生活的重要组成部分。自动化图像基本饮食评估是研究领域的快速发展，因为图像捕捉设备的使用率在增长（如手机）。在这种工作中，我们将单一的偏振图像中的食物能量估算，这是由于图像中有限的精炼能量信息，使得这种任务非常困难。为此，我们采用了改进的编码器-解码器框架来进行能量估算，编码器将图像转化为含有食物能量信息的更易EXTRACT的表示形式，然后解码器EXTRACT出能量信息。为实现我们的方法，我们编译了高质量的食物图像数据集，该数据集包括餐厅场景图像、食物项分割面积和真实热量值。我们的方法与前期热量估算方法相比，提高了10%以上和30 kCal的MAP和MAE分别。
</details></li>
</ul>
<hr>
<h2 id="dacl10k-Benchmark-for-Semantic-Bridge-Damage-Segmentation"><a href="#dacl10k-Benchmark-for-Semantic-Bridge-Damage-Segmentation" class="headerlink" title="dacl10k: Benchmark for Semantic Bridge Damage Segmentation"></a>dacl10k: Benchmark for Semantic Bridge Damage Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00460">http://arxiv.org/abs/2309.00460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Johannes Flotzinger, Philipp J. Rösch, Thomas Braml</li>
<li>for: 本研究旨在提供一个大规模、多种类型的混凝土结构缺陷识别数据集，以便在实际应用中进行bridge检测和评估。</li>
<li>methods: 本研究使用了实际桥梁检测数据集，包括9920张图像，并分为12种缺陷类型和6种桥Component。</li>
<li>results: 研究人员使用基eline模型对dacl10k数据集进行评估，得到了0.42的mean intersection-over-union值。此外，研究人员还将数据集和基eline模型公开访问，以便对bridge检测和评估领域进行进一步研究。<details>
<summary>Abstract</summary>
Reliably identifying reinforced concrete defects (RCDs)plays a crucial role in assessing the structural integrity, traffic safety, and long-term durability of concrete bridges, which represent the most common bridge type worldwide. Nevertheless, available datasets for the recognition of RCDs are small in terms of size and class variety, which questions their usability in real-world scenarios and their role as a benchmark. Our contribution to this problem is "dacl10k", an exceptionally diverse RCD dataset for multi-label semantic segmentation comprising 9,920 images deriving from real-world bridge inspections. dacl10k distinguishes 12 damage classes as well as 6 bridge components that play a key role in the building assessment and recommending actions, such as restoration works, traffic load limitations or bridge closures. In addition, we examine baseline models for dacl10k which are subsequently evaluated. The best model achieves a mean intersection-over-union of 0.42 on the test set. dacl10k, along with our baselines, will be openly accessible to researchers and practitioners, representing the currently biggest dataset regarding number of images and class diversity for semantic segmentation in the bridge inspection domain.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>鉴别强化混凝土缺陷（RCD）可以准确评估混凝土桥的结构完整性、交通安全性和长期持续性，混凝土桥是全球最常见的桥梁类型。然而，目前可用的RCD数据集较小，尺寸和类型多样性受限，这会问题其在实际场景中的可用性和作为标准。我们的贡献是“dacl10k”数据集，包含9,920个真实桥梁检查图像，用于多类Semantic segmentation。dacl10k可以分辨12种缺陷类型和6种桥 component，这些组件对建筑评估和建议行动（如修复工程、交通负荷限制或桥梁关闭）具有关键性。此外，我们还考虑了baseline模型，并评估其性能。测试集上的最佳模型得分为0.42。dacl10k、我们的baselines以及相关的评估结果将公开 accessible for researchers and practitioners，代表bridge检测领域中最大的数据集，包括图像数量和类型多样性。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-bias-discovery-in-medical-image-segmentation"><a href="#Unsupervised-bias-discovery-in-medical-image-segmentation" class="headerlink" title="Unsupervised bias discovery in medical image segmentation"></a>Unsupervised bias discovery in medical image segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00451">http://arxiv.org/abs/2309.00451</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolás Gaggion, Rodrigo Echeveste, Lucas Mansilla, Diego H. Milone, Enzo Ferrante</li>
<li>for: 避免深度学习模型在医疗影像分割中存在对某些保护属性（如性别或民族）的偏见。</li>
<li>methods: 我们提出了一种新的无监督偏见探测方法，利用反分类精度框架来估算分割质量。</li>
<li>results: 我们通过synthetic和实际场景的数字实验表示，我们的方法能够成功预测深度分割模型的公平问题，这成为该领域的新和有价值的工具。<details>
<summary>Abstract</summary>
It has recently been shown that deep learning models for anatomical segmentation in medical images can exhibit biases against certain sub-populations defined in terms of protected attributes like sex or ethnicity. In this context, auditing fairness of deep segmentation models becomes crucial. However, such audit process generally requires access to ground-truth segmentation masks for the target population, which may not always be available, especially when going from development to deployment. Here we propose a new method to anticipate model biases in biomedical image segmentation in the absence of ground-truth annotations. Our unsupervised bias discovery method leverages the reverse classification accuracy framework to estimate segmentation quality. Through numerical experiments in synthetic and realistic scenarios we show how our method is able to successfully anticipate fairness issues in the absence of ground-truth labels, constituting a novel and valuable tool in this field.
</details>
<details>
<summary>摘要</summary>
Recently, research has shown that deep learning models for anatomical segmentation in medical images can exhibit biases against certain sub-populations defined by protected attributes such as sex or ethnicity. In this context, it is crucial to audit the fairness of deep segmentation models. However, this process typically requires access to ground-truth segmentation masks for the target population, which may not always be available, especially when moving from development to deployment. Here, we propose a new method to anticipate model biases in biomedical image segmentation in the absence of ground-truth annotations. Our unsupervised bias discovery method utilizes the reverse classification accuracy framework to estimate segmentation quality. Through numerical experiments in synthetic and realistic scenarios, we demonstrate how our method can successfully anticipate fairness issues in the absence of ground-truth labels, providing a novel and valuable tool in this field.Here's the translation in Traditional Chinese as well:Recently, research has shown that deep learning models for anatomical segmentation in medical images can exhibit biases against certain sub-populations defined by protected attributes such as sex or ethnicity. In this context, it is crucial to audit the fairness of deep segmentation models. However, this process typically requires access to ground-truth segmentation masks for the target population, which may not always be available, especially when moving from development to deployment. Here, we propose a new method to anticipate model biases in biomedical image segmentation in the absence of ground-truth annotations. Our unsupervised bias discovery method utilizes the reverse classification accuracy framework to estimate segmentation quality. Through numerical experiments in synthetic and realistic scenarios, we demonstrate how our method can successfully anticipate fairness issues in the absence of ground-truth labels, providing a novel and valuable tool in this field.
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Video-Moment-Retrieval-from-Frozen-Vision-Language-Models"><a href="#Zero-Shot-Video-Moment-Retrieval-from-Frozen-Vision-Language-Models" class="headerlink" title="Zero-Shot Video Moment Retrieval from Frozen Vision-Language Models"></a>Zero-Shot Video Moment Retrieval from Frozen Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00661">http://arxiv.org/abs/2309.00661</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dezhao Luo, Jiabo Huang, Shaogang Gong, Hailin Jin, Yang Liu</li>
<li>for: 提高视频瞬间 Retrieval（VMR）的精度，使其能够处理未知词汇和未经见过的场景。</li>
<li>methods: 利用视觉语言模型（VLM）的新的转移学习方法，通过大规模的视觉语言对数据来 derive universal visual-textual correlations，并通过 fine-tuning 来适应目标领域。</li>
<li>results: 提出了一种零基础方法，可以将通用的视觉文本关系转移到 VMR 领域，而不需要访问 VMR 数据。该方法包括一个 conditional feature refinement module 和一个底层提档生成策略，可以更好地理解瞬间边界，并最大化 VLM 的效果。实验结果表明，我们的零基础算法在三个 VMR 标准测试集上具有显著的性能优势，特别是在未知词汇和未经见过的场景下。<details>
<summary>Abstract</summary>
Accurate video moment retrieval (VMR) requires universal visual-textual correlations that can handle unknown vocabulary and unseen scenes. However, the learned correlations are likely either biased when derived from a limited amount of moment-text data which is hard to scale up because of the prohibitive annotation cost (fully-supervised), or unreliable when only the video-text pairwise relationships are available without fine-grained temporal annotations (weakly-supervised). Recently, the vision-language models (VLM) demonstrate a new transfer learning paradigm to benefit different vision tasks through the universal visual-textual correlations derived from large-scale vision-language pairwise web data, which has also shown benefits to VMR by fine-tuning in the target domains. In this work, we propose a zero-shot method for adapting generalisable visual-textual priors from arbitrary VLM to facilitate moment-text alignment, without the need for accessing the VMR data. To this end, we devise a conditional feature refinement module to generate boundary-aware visual features conditioned on text queries to enable better moment boundary understanding. Additionally, we design a bottom-up proposal generation strategy that mitigates the impact of domain discrepancies and breaks down complex-query retrieval tasks into individual action retrievals, thereby maximizing the benefits of VLM. Extensive experiments conducted on three VMR benchmark datasets demonstrate the notable performance advantages of our zero-shot algorithm, especially in the novel-word and novel-location out-of-distribution setups.
</details>
<details>
<summary>摘要</summary>
准确的视频时刻选取（VMR）需要一种通用的视觉文本相关性，可以处理未知词汇和未经见过的场景。然而，学习的相关性可能受到有限的时刻文本数据的偏袋影响（完全监督），或者只有视频和文本之间的对应关系（弱监督），无法提供精细的时刻标注。近些年，视觉语言模型（VLM）表明了一种新的转移学习 paradigma，通过大规模的视觉语言对数据来获得通用的视觉文本相关性，并在目标领域进行 fine-tuning，以便为不同的视觉任务提供利用。在这种情况下，我们提出了一种零shot方法，通过不需要访问VMR数据来适应通用的视觉文本相关性。为此，我们设计了一个条件feature重定向模块，通过文本查询来生成与边界相关的视觉特征，以便更好地理解时刻边界。此外，我们还设计了一种底层提议生成策略，以降低域外差异的影响和将复杂的查询任务分解成个体动作 retrieve 任务，从而最大化VLM的利用。经过对三个VMR benchmark数据集的广泛实验，我们发现了我们零shot算法在未知词汇和未经见过的设置中表现出了remarkable的性能优势。
</details></li>
</ul>
<hr>
<h2 id="Improving-the-matching-of-deformable-objects-by-learning-to-detect-keypoints"><a href="#Improving-the-matching-of-deformable-objects-by-learning-to-detect-keypoints" class="headerlink" title="Improving the matching of deformable objects by learning to detect keypoints"></a>Improving the matching of deformable objects by learning to detect keypoints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00434">http://arxiv.org/abs/2309.00434</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/verlab/learningtodetect_prl_2023">https://github.com/verlab/learningtodetect_prl_2023</a></li>
<li>paper_authors: Felipe Cadar, Welerson Melo, Vaishnavi Kanagasabapathi, Guilherme Potje, Renato Martins, Erickson R. Nascimento</li>
<li>for: 提高非RIGID图像匹配任务中正确匹配的数量</li>
<li>methods: 使用练习annotated图像对照 pairs的真相匹配来训练一个端到端的卷积神经网络，从而找到更适合考虑的描述符的关键点位置</li>
<li>results: 对多种描述符的匹配精度提高20pp，并在真实世界中的对象检索任务中与最佳关键点检测器一样高效<details>
<summary>Abstract</summary>
We propose a novel learned keypoint detection method to increase the number of correct matches for the task of non-rigid image correspondence. By leveraging true correspondences acquired by matching annotated image pairs with a specified descriptor extractor, we train an end-to-end convolutional neural network (CNN) to find keypoint locations that are more appropriate to the considered descriptor. For that, we apply geometric and photometric warpings to images to generate a supervisory signal, allowing the optimization of the detector. Experiments demonstrate that our method enhances the Mean Matching Accuracy of numerous descriptors when used in conjunction with our detection method, while outperforming the state-of-the-art keypoint detectors on real images of non-rigid objects by 20 p.p. We also apply our method on the complex real-world task of object retrieval where our detector performs on par with the finest keypoint detectors currently available for this task. The source code and trained models are publicly available at https://github.com/verlab/LearningToDetect_PRL_2023
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的学习基于关键点检测方法，以提高非RIGID图像匹配任务中correct匹配的数量。通过利用已知图像对照对描述符EXTRACTOR进行匹配，我们训练了一个端到端的卷积神经网络（CNN）来找出更适合考虑的描述符的关键点位置。为了实现这一目标，我们应用了地理метриic和光学扭曲到图像，以生成一个监督信号，allowing the optimization of the detector。实验表明，我们的方法可以提高许多描述符的mean Matching Accuracy，并在真实图像中对非RIGID对象的检测任务上超越当前最佳的关键点检测器。我们还应用了我们的方法在复杂的real-world对象检索任务中，并与当前最佳的关键点检测器一样表现。source code和训练模型可以在https://github.com/verlab/LearningToDetect_PRL_2023中下载。
</details></li>
</ul>
<hr>
<h2 id="Selective-Scene-Text-Removal"><a href="#Selective-Scene-Text-Removal" class="headerlink" title="Selective Scene Text Removal"></a>Selective Scene Text Removal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00410">http://arxiv.org/abs/2309.00410</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mitanihayato/Selective-Scene-Text-Removal">https://github.com/mitanihayato/Selective-Scene-Text-Removal</a></li>
<li>paper_authors: Hayato Mitani, Akisato Kimura, Seiichi Uchida</li>
<li>for:  selective scene text removal (SSTR)</li>
<li>methods:  multi-module structure</li>
<li>results:  can remove target words as expected<details>
<summary>Abstract</summary>
Scene text removal (STR) is the image transformation task to remove text regions in scene images. The conventional STR methods remove all scene text. This means that the existing methods cannot select text to be removed. In this paper, we propose a novel task setting named selective scene text removal (SSTR) that removes only target words specified by the user. Although SSTR is a more complex task than STR, the proposed multi-module structure enables efficient training for SSTR. Experimental results show that the proposed method can remove target words as expected.
</details>
<details>
<summary>摘要</summary>
Scene文本除除（STR）是图像变换任务，去除场景中的文本区域。传统的STR方法都是全面去除场景中的所有文本。在这篇论文中，我们提出了一种新的任务设定方式，即选择场景文本除除（SSTR），可以根据用户指定的Target字符串来去除特定的文本。虽然SSTR是STR的更加复杂的任务，但我们提出的多模块结构使得SSTR的训练变得高效。实验结果表明，我们的方法可以如预期地去除Target字符串。
</details></li>
</ul>
<hr>
<h2 id="Fine-grained-Recognition-with-Learnable-Semantic-Data-Augmentation"><a href="#Fine-grained-Recognition-with-Learnable-Semantic-Data-Augmentation" class="headerlink" title="Fine-grained Recognition with Learnable Semantic Data Augmentation"></a>Fine-grained Recognition with Learnable Semantic Data Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00399">http://arxiv.org/abs/2309.00399</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Pu, Yizeng Han, Yulin Wang, Junlan Feng, Chao Deng, Gao Huang</li>
<li>for: 本研究旨在提高细化图像识别精度，通过在特征层进行多样化数据训练，以增强分类器的泛化性。</li>
<li>methods: 本研究提出了一种基于semantic特征方向的多样化数据生成方法，通过预测样本独特的协方差矩阵来生成多个不同的扩展样本，并与分类器进行共同优化。</li>
<li>results: 实验表明，该方法可以在多个竞争性的细化图像识别benchmark上提高分类器的泛化性，并在CUB-200-2011 dataset上与最新的方法相当。<details>
<summary>Abstract</summary>
Fine-grained image recognition is a longstanding computer vision challenge that focuses on differentiating objects belonging to multiple subordinate categories within the same meta-category. Since images belonging to the same meta-category usually share similar visual appearances, mining discriminative visual cues is the key to distinguishing fine-grained categories. Although commonly used image-level data augmentation techniques have achieved great success in generic image classification problems, they are rarely applied in fine-grained scenarios, because their random editing-region behavior is prone to destroy the discriminative visual cues residing in the subtle regions. In this paper, we propose diversifying the training data at the feature-level to alleviate the discriminative region loss problem. Specifically, we produce diversified augmented samples by translating image features along semantically meaningful directions. The semantic directions are estimated with a covariance prediction network, which predicts a sample-wise covariance matrix to adapt to the large intra-class variation inherent in fine-grained images. Furthermore, the covariance prediction network is jointly optimized with the classification network in a meta-learning manner to alleviate the degenerate solution problem. Experiments on four competitive fine-grained recognition benchmarks (CUB-200-2011, Stanford Cars, FGVC Aircrafts, NABirds) demonstrate that our method significantly improves the generalization performance on several popular classification networks (e.g., ResNets, DenseNets, EfficientNets, RegNets and ViT). Combined with a recently proposed method, our semantic data augmentation approach achieves state-of-the-art performance on the CUB-200-2011 dataset. The source code will be released.
</details>
<details>
<summary>摘要</summary>
传统的图像识别挑战之一是细化图像识别，即在同一个meta-类别下分别识别多个子类别。由于图像在同一个meta-类别下通常具有相似的视觉特征，因此挖掘特征特征是识别细化类别的关键。虽然通常使用的图像级别数据增强技术已经取得了广泛的成功在通用图像分类问题上，但它们在细化场景下rarely被应用，因为它们的随机编辑区域行为容易 Destroying the discriminative visual cues residing in subtle regions.在这篇论文中，我们提出了在特征级别上多样化训练数据以解决细化类别损失问题。具体来说，我们生成了多样化增强样本 by translating image features along semantically meaningful directions. 预测样本级别的covariance matrix的网络来Estimate the semantic directions, which adapts to the large intra-class variation inherent in fine-grained images. 此外，预测网络和分类网络在meta-学习方式下jointly optimize the covariance prediction network to alleviate the degenerate solution problem.实验表明，我们的方法可以在四个竞争力高的细化图像识别benchmark上（CUB-200-2011, Stanford Cars, FGVC Aircrafts, NABirds）提高通用分类网络（例如ResNets, DenseNets, EfficientNets, RegNets和ViT）的总体性能。与 reciently proposed method 的semantic data augmentation approach combine， our approach achieves state-of-the-art performance on the CUB-200-2011 dataset. 代码将会发布。
</details></li>
</ul>
<hr>
<h2 id="VideoGen-A-Reference-Guided-Latent-Diffusion-Approach-for-High-Definition-Text-to-Video-Generation"><a href="#VideoGen-A-Reference-Guided-Latent-Diffusion-Approach-for-High-Definition-Text-to-Video-Generation" class="headerlink" title="VideoGen: A Reference-Guided Latent Diffusion Approach for High Definition Text-to-Video Generation"></a>VideoGen: A Reference-Guided Latent Diffusion Approach for High Definition Text-to-Video Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00398">http://arxiv.org/abs/2309.00398</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Li, Wenqing Chu, Ye Wu, Weihang Yuan, Fanglong Liu, Qi Zhang, Fu Li, Haocheng Feng, Errui Ding, Jingdong Wang</li>
<li>for: 这个研究旨在提出一种基于参考导向潜在扩散的文本到视频生成方法，可以生成高分辨率、高帧准确率的视频。</li>
<li>methods: 该方法首先使用现成的文本到图像生成模型（如稳定扩散）生成一个高质量的图像作为参考图像，然后引入一个高效的级联潜在扩散模块，通过参考图像和文本提示来生成潜在视频表示。最后，通过一个改进的流式增强步骤来提高视频的时间分辨率。</li>
<li>results: 该方法在文本到视频生成方面设置了新的州OF-THE-ART， both qualitative and quantitative evaluation 表明，VideoGen可以生成高质量、高分辨率的视频。更多样例可以参考 \url{<a target="_blank" rel="noopener" href="https://videogen.github.io/VideoGen/%7D%E3%80%82">https://videogen.github.io/VideoGen/}。</a><details>
<summary>Abstract</summary>
In this paper, we present VideoGen, a text-to-video generation approach, which can generate a high-definition video with high frame fidelity and strong temporal consistency using reference-guided latent diffusion. We leverage an off-the-shelf text-to-image generation model, e.g., Stable Diffusion, to generate an image with high content quality from the text prompt, as a reference image to guide video generation. Then, we introduce an efficient cascaded latent diffusion module conditioned on both the reference image and the text prompt, for generating latent video representations, followed by a flow-based temporal upsampling step to improve the temporal resolution. Finally, we map latent video representations into a high-definition video through an enhanced video decoder. During training, we use the first frame of a ground-truth video as the reference image for training the cascaded latent diffusion module. The main characterises of our approach include: the reference image generated by the text-to-image model improves the visual fidelity; using it as the condition makes the diffusion model focus more on learning the video dynamics; and the video decoder is trained over unlabeled video data, thus benefiting from high-quality easily-available videos. VideoGen sets a new state-of-the-art in text-to-video generation in terms of both qualitative and quantitative evaluation. See \url{https://videogen.github.io/VideoGen/} for more samples.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了 VideoGen，一种文本到视频生成方法，可以生成高清晰度视频，具有高帧准确性和强时间一致性使用参考导向的潜在扩散。我们利用了一个标准的文本到图像生成模型，例如稳定扩散，来生成基于文本提示的图像，并用其为参考图像来导引视频生成。然后，我们引入了一个高效的级联潜在扩散模块，Conditional on both the reference image and the text prompt, for generating latent video representations, followed by a flow-based temporal upsampling step to improve the temporal resolution.最后，我们将潜在视频表示映射到高清晰度视频 через一个加强的视频解码器。在训练时，我们使用了ground truth video的首帧作为参考图像来训练级联潜在扩散模块。主要特点包括：参考图像生成于文本到图像模型提高了视觉质量;使用参考图像作为条件使得扩散模型更专注学习视频动力学;以及视频解码器在训练过程中使用了高质量的无标注视频数据，从而受益于高质量的可获得视频数据。VideoGen将文本到视频生成领域的新州标准，以质量和量化评价为据。更多样例可以参考\url{https://videogen.github.io/VideoGen/}。
</details></li>
</ul>
<hr>
<h2 id="On-the-Localization-of-Ultrasound-Image-Slices-within-Point-Distribution-Models"><a href="#On-the-Localization-of-Ultrasound-Image-Slices-within-Point-Distribution-Models" class="headerlink" title="On the Localization of Ultrasound Image Slices within Point Distribution Models"></a>On the Localization of Ultrasound Image Slices within Point Distribution Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00372">http://arxiv.org/abs/2309.00372</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lennart Bastian, Vincent Bürgin, Ha Young Kim, Alexander Baumann, Benjamin Busam, Mahdi Saleh, Nassir Navab</li>
<li>for: 这个论文主要目标是提供一种自动化ultrasound图像块位置定位方法，以便更好地诊断甲状腺疾病。</li>
<li>methods: 该方法使用对ultrasound图像和三维形态模型的对比学习，学习一个共同准则空间，然后使用cross-modality注册和Procrustes分析将ultrasound块注册到三维形态模型中。</li>
<li>results: 实验结果表明，该多模态注册框架可以准确地将ultrasound块注册到患者特定的三维形态模型和统计 shapes模型中，并且可以预测块位置在患者特定的三维形态模型上的平均误差为1.2毫米，并且在统计 shapes模型上的平均误差为4.6毫米。<details>
<summary>Abstract</summary>
Thyroid disorders are most commonly diagnosed using high-resolution Ultrasound (US). Longitudinal nodule tracking is a pivotal diagnostic protocol for monitoring changes in pathological thyroid morphology. This task, however, imposes a substantial cognitive load on clinicians due to the inherent challenge of maintaining a mental 3D reconstruction of the organ. We thus present a framework for automated US image slice localization within a 3D shape representation to ease how such sonographic diagnoses are carried out. Our proposed method learns a common latent embedding space between US image patches and the 3D surface of an individual's thyroid shape, or a statistical aggregation in the form of a statistical shape model (SSM), via contrastive metric learning. Using cross-modality registration and Procrustes analysis, we leverage features from our model to register US slices to a 3D mesh representation of the thyroid shape. We demonstrate that our multi-modal registration framework can localize images on the 3D surface topology of a patient-specific organ and the mean shape of an SSM. Experimental results indicate slice positions can be predicted within an average of 1.2 mm of the ground-truth slice location on the patient-specific 3D anatomy and 4.6 mm on the SSM, exemplifying its usefulness for slice localization during sonographic acquisitions. Code is publically available: \href{https://github.com/vuenc/slice-to-shape}{https://github.com/vuenc/slice-to-shape}
</details>
<details>
<summary>摘要</summary>
淀脑疾病通常通过高分辨率超声成像（US）诊断。 longitudinal nodule tracking是诊断过程中的关键协议，但这会对临床医生带来很大的认知负担，因为需要维护一个 mental 3D 重建的器官。我们因此提出了一种自动化 US 图像片断localization在3D形状表示中的框架，以facilitate如此的超声诊断。我们的提议方法learns一个共同latent embedding空间 между US图像块和个体的thyroid形状3D，或者一个统计汇总模型（SSM），通过对比度学习。通过cross-modality registration和Procrustes分析，我们利用我们的模型特征来注册US剖平图像到个体特定的thyroid形状3D的表示中。我们的多Modal注册框架可以在病人特定的3D анатомия上和统计平均形态模型（SSM）上localize图像剖平。实验结果表明，我们可以在病人特定3D形状上和统计平均形态模型（SSM）上预测图像剖平的位置，与实际 slice位置相差在1.2毫米和4.6毫米之间。这 demonstartes我们的多Modal注册框架的用于图像剖平localization durante acquisitions sonográficas。代码可以在以下链接获取：https://github.com/vuenc/slice-to-shape
</details></li>
</ul>
<hr>
<h2 id="How-You-Split-Matters-Data-Leakage-and-Subject-Characteristics-Studies-in-Longitudinal-Brain-MRI-Analysis"><a href="#How-You-Split-Matters-Data-Leakage-and-Subject-Characteristics-Studies-in-Longitudinal-Brain-MRI-Analysis" class="headerlink" title="How You Split Matters: Data Leakage and Subject Characteristics Studies in Longitudinal Brain MRI Analysis"></a>How You Split Matters: Data Leakage and Subject Characteristics Studies in Longitudinal Brain MRI Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00350">http://arxiv.org/abs/2309.00350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dewinda Julianensi Rumala</li>
<li>for: 这研究探讨了医疗图像分析领域中深度学习模型的应用，以及这些模型在诊断和患者照管中的潜在问题。</li>
<li>methods: 研究使用了3D卷积神经网络（CNN）对大脑MRI图像进行分析，并 investigate了不同数据分割策略对模型性能的影响。</li>
<li>results: 研究发现，不当的数据分割策略可能会导致模型性能受到影响，特别是在长期图像数据中，包括重复的扫描数据。研究还发现，GradCAM视觉化可以揭示卷积神经网络模型中的快捷缺陷，这些缺陷可能会使模型学习到诊断特征以及患者身份。<details>
<summary>Abstract</summary>
Deep learning models have revolutionized the field of medical image analysis, offering significant promise for improved diagnostics and patient care. However, their performance can be misleadingly optimistic due to a hidden pitfall called 'data leakage'. In this study, we investigate data leakage in 3D medical imaging, specifically using 3D Convolutional Neural Networks (CNNs) for brain MRI analysis. While 3D CNNs appear less prone to leakage than 2D counterparts, improper data splitting during cross-validation (CV) can still pose issues, especially with longitudinal imaging data containing repeated scans from the same subject. We explore the impact of different data splitting strategies on model performance for longitudinal brain MRI analysis and identify potential data leakage concerns. GradCAM visualization helps reveal shortcuts in CNN models caused by identity confounding, where the model learns to identify subjects along with diagnostic features. Our findings, consistent with prior research, underscore the importance of subject-wise splitting and evaluating our model further on hold-out data from different subjects to ensure the integrity and reliability of deep learning models in medical image analysis.
</details>
<details>
<summary>摘要</summary>
深度学习模型在医疗图像分析领域已经引起了广泛的关注，因为它们提供了改善诊断和患者护理的可能性。然而，它们的性能可能会受到一种隐藏的陷阱，称为“数据泄露”。在这项研究中，我们研究了3D医疗图像中的数据泄露， especailly使用3D卷积神经网络（CNN）进行脑MRI分析。虽然3D CNN在2D counterpart中看起来更加免疫于泄露，但是在cross-validation（CV）时不当的数据分割可以仍然存在问题，尤其是在包含同一个主题的重复扫描数据中。我们研究了不同的数据分割策略对于长itudinal脑MRI分析的模型性能的影响，并发现了可能的数据泄露问题。GradCAM可视化 помо助于揭示了 CNN模型中由identify confounding引起的快捷路径，其中模型学习了主题以及诊断特征。我们的发现与之前的研究一致，强调了在医疗图像分析中深度学习模型的完整性和可靠性的重要性，需要在不同主题的数据上进行评估。
</details></li>
</ul>
<hr>
<h2 id="RigNet-Efficient-Repetitive-Image-Guided-Network-for-Depth-Completion"><a href="#RigNet-Efficient-Repetitive-Image-Guided-Network-for-Depth-Completion" class="headerlink" title="RigNet++: Efficient Repetitive Image Guided Network for Depth Completion"></a>RigNet++: Efficient Repetitive Image Guided Network for Depth Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00655">http://arxiv.org/abs/2309.00655</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiqiang Yan, Xiang Li, Zhenyu Zhang, Jun Li, Jian Yang</li>
<li>for: 本研究旨在提高深度映射的精度，使用频繁重复的设计来提高图像引导学习框架的性能。</li>
<li>methods: 我们在图像引导学习框架中实现了高效的重复设计，包括图像引导分支和深度生成分支。图像引导分支中，我们设计了一个密集重复小时glass网络，以提取复杂环境中的特征特征，为深度预测提供强大的Contextual指导。深度生成分支中，我们引入了一种循环动态梯度Module，其中提出了一种高效的幂等分解方法，以减少复杂性而模型高频结构。</li>
<li>results: 我们在KITTI、VKITTI、NYUv2、3D60和Matterport3D等 datasets上进行了广泛的实验，结果表明，我们的方法可以取得Superior或竞争性的结果。<details>
<summary>Abstract</summary>
Depth completion aims to recover dense depth maps from sparse ones, where color images are often used to facilitate this task. Recent depth methods primarily focus on image guided learning frameworks. However, blurry guidance in the image and unclear structure in the depth still impede their performance. To tackle these challenges, we explore an efficient repetitive design in our image guided network to gradually and sufficiently recover depth values. Specifically, the efficient repetition is embodied in both the image guidance branch and depth generation branch. In the former branch, we design a dense repetitive hourglass network to extract discriminative image features of complex environments, which can provide powerful contextual instruction for depth prediction. In the latter branch, we introduce a repetitive guidance module based on dynamic convolution, in which an efficient convolution factorization is proposed to reduce the complexity while modeling high-frequency structures progressively. Extensive experiments indicate that our approach achieves superior or competitive results on KITTI, VKITTI, NYUv2, 3D60, and Matterport3D datasets.
</details>
<details>
<summary>摘要</summary>
depth completion 目标是从稀畴的深度地图中恢复粗粒度的深度地图，通常使用颜色图像来促进这个任务。  current depth method 主要关注于图像导学框架。然而，图像指导中的模糊和深度中的 unclear structure 仍然妨碍其性能。为了解决这些挑战，我们探索了一种高效的循环设计在我们的图像导学网络中。 Specifically, the efficient repetition is embodied in both the image guidance branch and depth generation branch。在前者分支中，我们设计了一个紧凑的重复弧形网络来提取复杂环境中的特征特征，这可以为深度预测提供强大的Contextual instruction。在后者分支中，我们引入了一种循环导引模块，基于动态 convolution，在其中我们提出了一种高效的 convolution factorization来降低复杂性，同时模型高频结构进行进行逐步进行进行。  extensive experiments indicate that our approach achieves superior or competitive results on KITTI, VKITTI, NYUv2, 3D60, and Matterport3D datasets。
</details></li>
</ul>
<hr>
<h2 id="MuraNet-Multi-task-Floor-Plan-Recognition-with-Relation-Attention"><a href="#MuraNet-Multi-task-Floor-Plan-Recognition-with-Relation-Attention" class="headerlink" title="MuraNet: Multi-task Floor Plan Recognition with Relation Attention"></a>MuraNet: Multi-task Floor Plan Recognition with Relation Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00348">http://arxiv.org/abs/2309.00348</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lingxiao Huang, Jung-Hsuan Wu, Chiching Wei, Wilson Li</li>
<li>For:  floor plan data recognition* Methods:  attention-based multi-task model (MuraNet) with unified encoder (MURA) and separated branches for segmentation and detection tasks* Results:  improved convergence speed and performance in detection and segmentation tasks compared to single-task models like U-Net and YOLOv3<details>
<summary>Abstract</summary>
The recognition of information in floor plan data requires the use of detection and segmentation models. However, relying on several single-task models can result in ineffective utilization of relevant information when there are multiple tasks present simultaneously. To address this challenge, we introduce MuraNet, an attention-based multi-task model for segmentation and detection tasks in floor plan data. In MuraNet, we adopt a unified encoder called MURA as the backbone with two separated branches: an enhanced segmentation decoder branch and a decoupled detection head branch based on YOLOX, for segmentation and detection tasks respectively. The architecture of MuraNet is designed to leverage the fact that walls, doors, and windows usually constitute the primary structure of a floor plan's architecture. By jointly training the model on both detection and segmentation tasks, we believe MuraNet can effectively extract and utilize relevant features for both tasks. Our experiments on the CubiCasa5k public dataset show that MuraNet improves convergence speed during training compared to single-task models like U-Net and YOLOv3. Moreover, we observe improvements in the average AP and IoU in detection and segmentation tasks, respectively.Our ablation experiments demonstrate that the attention-based unified backbone of MuraNet achieves better feature extraction in floor plan recognition tasks, and the use of decoupled multi-head branches for different tasks further improves model performance. We believe that our proposed MuraNet model can address the disadvantages of single-task models and improve the accuracy and efficiency of floor plan data recognition.
</details>
<details>
<summary>摘要</summary>
<<SYS>>loor plan数据认知需要使用探测和分割模型。然而，依赖于多个单任务模型可能会导致 relevante信息的不fficient使用， especialmente when there are multiple tasks present simultaneously. To address this challenge, we introduce MuraNet, an attention-based multi-task model for segmentation and detection tasks in floor plan data. In MuraNet, we adopt a unified encoder called MURA as the backbone with two separated branches: an enhanced segmentation decoder branch and a decoupled detection head branch based on YOLOX, for segmentation and detection tasks respectively. The architecture of MuraNet is designed to leverage the fact that walls, doors, and windows usually constitute the primary structure of a floor plan's architecture. By jointly training the model on both detection and segmentation tasks, we believe MuraNet can effectively extract and utilize relevant features for both tasks. Our experiments on the CubiCasa5k public dataset show that MuraNet improves convergence speed during training compared to single-task models like U-Net and YOLOv3. Moreover, we observe improvements in the average AP and IoU in detection and segmentation tasks, respectively. Our ablation experiments demonstrate that the attention-based unified backbone of MuraNet achieves better feature extraction in floor plan recognition tasks, and the use of decoupled multi-head branches for different tasks further improves model performance. We believe that our proposed MuraNet model can address the disadvantages of single-task models and improve the accuracy and efficiency of floor plan data recognition.中文简体版：loor plan数据认知需要使用探测和分割模型。然而，依赖于多个单任务模型可能会导致 relevante信息的不fficient使用， especialmente when there are multiple tasks present simultaneously. To address this challenge, we introduce MuraNet, an attention-based multi-task model for segmentation and detection tasks in floor plan data. In MuraNet, we adopt a unified encoder called MURA as the backbone with two separated branches: an enhanced segmentation decoder branch and a decoupled detection head branch based on YOLOX, for segmentation and detection tasks respectively. The architecture of MuraNet is designed to leverage the fact that walls, doors, and windows usually constitute the primary structure of a floor plan's architecture. By jointly training the model on both detection and segmentation tasks, we believe MuraNet can effectively extract and utilize relevant features for both tasks. Our experiments on the CubiCasa5k public dataset show that MuraNet improves convergence speed during training compared to single-task models like U-Net and YOLOv3. Moreover, we observe improvements in the average AP and IoU in detection and segmentation tasks, respectively. Our ablation experiments demonstrate that the attention-based unified backbone of MuraNet achieves better feature extraction in floor plan recognition tasks, and the use of decoupled multi-head branches for different tasks further improves model performance. We believe that our proposed MuraNet model can address the disadvantages of single-task models and improve the accuracy and efficiency of floor plan data recognition.
</details></li>
</ul>
<hr>
<h2 id="Towards-Contrastive-Learning-in-Music-Video-Domain"><a href="#Towards-Contrastive-Learning-in-Music-Video-Domain" class="headerlink" title="Towards Contrastive Learning in Music Video Domain"></a>Towards Contrastive Learning in Music Video Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00347">http://arxiv.org/abs/2309.00347</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karel Veldkamp, Mariya Hendriksen, Zoltán Szlávik, Alexander Keijser</li>
<li>for: 这个论文 investigate whether contrastive learning can be applied to the domain of music videos, and evaluate the effectiveness of this approach on downstream tasks such as music tagging and genre classification.</li>
<li>methods: 作者使用了 dual en-coder 来学习 audio 和 video 模式的 multimodal 表示，并使用了 bidirectional contrastive loss 进行训练。</li>
<li>results: 结果表明，没有对 contrastive learning 进行 fine-tuning 的预训练网络在两个下游任务中表现更好，而且作者通过Qualitative analysis of the learned representations来解释了为什么 contrastive learning 对 music videos 不成功。<details>
<summary>Abstract</summary>
Contrastive learning is a powerful way of learning multimodal representations across various domains such as image-caption retrieval and audio-visual representation learning. In this work, we investigate if these findings generalize to the domain of music videos. Specifically, we create a dual en-coder for the audio and video modalities and train it using a bidirectional contrastive loss. For the experiments, we use an industry dataset containing 550 000 music videos as well as the public Million Song Dataset, and evaluate the quality of learned representations on the downstream tasks of music tagging and genre classification. Our results indicate that pre-trained networks without contrastive fine-tuning outperform our contrastive learning approach when evaluated on both tasks. To gain a better understanding of the reasons contrastive learning was not successful for music videos, we perform a qualitative analysis of the learned representations, revealing why contrastive learning might have difficulties uniting embeddings from two modalities. Based on these findings, we outline possible directions for future work. To facilitate the reproducibility of our results, we share our code and the pre-trained model.
</details>
<details>
<summary>摘要</summary>
“对比学习是一种强大的学习多Modal表现的方法，可以应用于不同领域，如图像描述和视觉表现学习。在这个工作中，我们 investigate 这些结果是否应用于音乐录影带领域。 Specifically, we create a dual en-coder for the audio and video modalities and train it using a bidirectional contrastive loss. 实验中，我们使用了550000首音乐录影带的industry dataset以及公共的Million Song Dataset，并评估学习的表现质量downstream task of music tagging和类别分类。我们的结果显示了pre-trained network without contrastive fine-tuning outperform our contrastive learning approach when evaluated on both tasks。为了更好地理解contrastive learning why it was not successful for music videos, we perform a qualitative analysis of the learned representations, revealing why contrastive learning might have difficulties uniting embeddings from two modalities。 Based on these findings, we outline possible directions for future work。为了促进我们的结果的重现性，我们分享了我们的代码和预训模型。”
</details></li>
</ul>
<hr>
<h2 id="Robust-Point-Cloud-Processing-through-Positional-Embedding"><a href="#Robust-Point-Cloud-Processing-through-Positional-Embedding" class="headerlink" title="Robust Point Cloud Processing through Positional Embedding"></a>Robust Point Cloud Processing through Positional Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00339">http://arxiv.org/abs/2309.00339</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/osiriszjq/RobustPPE">https://github.com/osiriszjq/RobustPPE</a></li>
<li>paper_authors: Jianqiao Zheng, Xueqian Li, Sameera Ramasinghe, Simon Lucey</li>
<li>for: 这个论文是关于3D点云处理中使用分析式每个点嵌入的研究，以提高对异常点云和噪声的Robustness。</li>
<li>methods: 这篇论文使用了基于带宽的分析式每个点嵌入，并与Random Fourier Features（RFF）的坐标嵌入进行比较。</li>
<li>results: 论文通过在多种异常点云和噪声下进行多个下渠道任务的实验，表明该方法可以提供更高的Robustness和稳定性。<details>
<summary>Abstract</summary>
End-to-end trained per-point embeddings are an essential ingredient of any state-of-the-art 3D point cloud processing such as detection or alignment. Methods like PointNet, or the more recent point cloud transformer -- and its variants -- all employ learned per-point embeddings. Despite impressive performance, such approaches are sensitive to out-of-distribution (OOD) noise and outliers. In this paper, we explore the role of an analytical per-point embedding based on the criterion of bandwidth. The concept of bandwidth enables us to draw connections with an alternate per-point embedding -- positional embedding, particularly random Fourier features. We present compelling robust results across downstream tasks such as point cloud classification and registration with several categories of OOD noise.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language: zh-CNEnd-to-end 培чение的每个点嵌入是现代三维点云处理的重要组成部分，如探测或对Alignment。方法如PointNet或更近的点云变换器以及其变种都使用学习的每个点嵌入。尽管表现出色，但这些方法对于异常情况（OOD）噪声和异常值敏感。在这篇论文中，我们探讨使用分布参数来确定每个点嵌入的概念。这种概念允许我们与另一种嵌入——位置嵌入，特别是随机傅立叶特征进行联系。我们在多个下游任务中，如点云分类和注册，对多种OOD噪声进行了吸引人的Robust表现。
</details></li>
</ul>
<hr>
<h2 id="Human-trajectory-prediction-using-LSTM-with-Attention-mechanism"><a href="#Human-trajectory-prediction-using-LSTM-with-Attention-mechanism" class="headerlink" title="Human trajectory prediction using LSTM with Attention mechanism"></a>Human trajectory prediction using LSTM with Attention mechanism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00331">http://arxiv.org/abs/2309.00331</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amin Manafi Soltan Ahmadi, Samaneh Hoseini Semnani</li>
<li>for: 本研究提出了一种人行踪预测模型，该模型结合了长期快速响应神经网络（LSTM）和注意机制。</li>
<li>methods: 该模型使用注意分数来确定输入数据中对预测输出的重要性，注意分数由输入特征的每个部分计算得到，高分数表明该部分在预测输出中的更大重要性。</li>
<li>results: 我们在ETH和UCY公共数据集上评估了我们的方法，并使用最终差分误差（FDE）和平均差分误差（ADE）度量来评估性能。我们发现，我们修改后的算法在拥挤空间中预测人行踪的性能有6.2%和6.3%的提升，相比文献中Social LSTM的结果。<details>
<summary>Abstract</summary>
In this paper, we propose a human trajectory prediction model that combines a Long Short-Term Memory (LSTM) network with an attention mechanism. To do that, we use attention scores to determine which parts of the input data the model should focus on when making predictions. Attention scores are calculated for each input feature, with a higher score indicating the greater significance of that feature in predicting the output. Initially, these scores are determined for the target human position, velocity, and their neighboring individual's positions and velocities. By using attention scores, our model can prioritize the most relevant information in the input data and make more accurate predictions. We extract attention scores from our attention mechanism and integrate them into the trajectory prediction module to predict human future trajectories. To achieve this, we introduce a new neural layer that processes attention scores after extracting them and concatenates them with positional information. We evaluate our approach on the publicly available ETH and UCY datasets and measure its performance using the final displacement error (FDE) and average displacement error (ADE) metrics. We show that our modified algorithm performs better than the Social LSTM in predicting the future trajectory of pedestrians in crowded spaces. Specifically, our model achieves an improvement of 6.2% in ADE and 6.3% in FDE compared to the Social LSTM results in the literature.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种人体轨迹预测模型，该模型结合了长期短记忆网络（LSTM）和注意力机制。为了实现这一点，我们使用注意力分数来确定输入数据中哪些部分需要模型的注意力。注意力分数分别计算对每个输入特征的注意力，高注意力分数表示该特征在预测输出时的更大重要性。我们首先计算这些分数，然后将其与目标人体位置、速度和邻近个体位置和速度相关的输入特征进行相加。通过使用注意力分数，我们的模型可以在输入数据中优先级掌握相关信息，并且更准确地预测人体轨迹。我们从注意力机制中提取出注意力分数，并将其与位置信息一起处理。我们新增一层神经网络来处理注意力分数，并将其与位置信息进行拼接。我们使用公共可用的ETH和UCY数据集进行评估，并使用最终差分Error（FDE）和平均差分Error（ADE） metric来评估我们的方法。我们的修改后的算法在预测人体轨迹时比Social LSTM在文献中的结果更好，具体来说，我们的模型在ADE和FDE metric上分别提高6.2%和6.3%。
</details></li>
</ul>
<hr>
<h2 id="ARFA-An-Asymmetric-Receptive-Field-Autoencoder-Model-for-Spatiotemporal-Prediction"><a href="#ARFA-An-Asymmetric-Receptive-Field-Autoencoder-Model-for-Spatiotemporal-Prediction" class="headerlink" title="ARFA: An Asymmetric Receptive Field Autoencoder Model for Spatiotemporal Prediction"></a>ARFA: An Asymmetric Receptive Field Autoencoder Model for Spatiotemporal Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00314">http://arxiv.org/abs/2309.00314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenxuan Zhang, Xuechao Zou, Li Wu, Jianqiang Huang, Xiaoying Wang</li>
<li>for: 预测Future sequences by learning from historical contexts, 用于各个领域，如交通流量预测和天气预测。</li>
<li>methods: 提出了一种偏 asymmetric Receptive Field Autoencoder (ARFA) 模型，通过设计不同大小的感知场模块，适应Encoder和Decoder的不同功能。在Encoder中，引入大kernel模块 для全球空间时间特征提取;在Decoder中，开发小kernel模块 для本地空间时间信息重建。</li>
<li>results: 在两个主流的空间时间预测数据集和我们自己construct的RainBench数据集上，ARFA实现了一致的状态集成性表现，证明了我们的方法的有效性。这种方法不仅从感知场的角度探索了一种新的方法，还为降水预测提供了数据支持，从而推动了未来的空间时间预测研究。<details>
<summary>Abstract</summary>
Spatiotemporal prediction aims to generate future sequences by paradigms learned from historical contexts. It holds significant importance in numerous domains, including traffic flow prediction and weather forecasting. However, existing methods face challenges in handling spatiotemporal correlations, as they commonly adopt encoder and decoder architectures with identical receptive fields, which adversely affects prediction accuracy. This paper proposes an Asymmetric Receptive Field Autoencoder (ARFA) model to address this issue. Specifically, we design corresponding sizes of receptive field modules tailored to the distinct functionalities of the encoder and decoder. In the encoder, we introduce a large kernel module for global spatiotemporal feature extraction. In the decoder, we develop a small kernel module for local spatiotemporal information reconstruction. To address the scarcity of meteorological prediction data, we constructed the RainBench, a large-scale radar echo dataset specific to the unique precipitation characteristics of inland regions in China for precipitation prediction. Experimental results demonstrate that ARFA achieves consistent state-of-the-art performance on two mainstream spatiotemporal prediction datasets and our RainBench dataset, affirming the effectiveness of our approach. This work not only explores a novel method from the perspective of receptive fields but also provides data support for precipitation prediction, thereby advancing future research in spatiotemporal prediction.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Spatiotemporal prediction aims to generate future sequences by paradigms learned from historical contexts. It holds significant importance in numerous domains, including traffic flow prediction and weather forecasting. However, existing methods face challenges in handling spatiotemporal correlations, as they commonly adopt encoder and decoder architectures with identical receptive fields, which adversely affects prediction accuracy. This paper proposes an Asymmetric Receptive Field Autoencoder (ARFA) model to address this issue. Specifically, we design corresponding sizes of receptive field modules tailored to the distinct functionalities of the encoder and decoder. In the encoder, we introduce a large kernel module for global spatiotemporal feature extraction. In the decoder, we develop a small kernel module for local spatiotemporal information reconstruction. To address the scarcity of meteorological prediction data, we constructed the RainBench, a large-scale radar echo dataset specific to the unique precipitation characteristics of inland regions in China for precipitation prediction. Experimental results demonstrate that ARFA achieves consistent state-of-the-art performance on two mainstream spatiotemporal prediction datasets and our RainBench dataset, affirming the effectiveness of our approach. This work not only explores a novel method from the perspective of receptive fields but also provides data support for precipitation prediction, thereby advancing future research in spatiotemporal prediction."中文翻译：<<SYS>>预测在时空中的序列，通过历史上的模式学习来实现。这种预测在各个领域都具有重要性，例如交通流量预测和天气预测。然而，现有的方法在处理时空相关性方面存在挑战，因为它们通常采用编码器和解码器结构具有相同的接收场，这会影响预测精度。本文提出了不同接收场的自适应各自谱频域自适应编码器（ARFA）模型，以解决这个问题。特别是，我们在编码器中设计了大小不同的接收场模块，以适应不同的功能。在编码器中，我们引入了大kernel模块，用于全局时空特征提取。在解码器中，我们开发了小kernel模块，用于本地时空信息重建。为了解决天气预测数据的缺乏，我们建立了雨峰Bench，一个特有的雨水特征的大规模雷达响应数据集，用于雨水预测。实验结果表明，ARFA在两个主流时空预测数据集和我们的雨峰Bench数据集上具有一致的状态艺术性，证明了我们的方法的有效性。这种研究不仅从接收场的角度探讨了一种新的方法，还为雨水预测提供了数据支持，从而推动了未来的时空预测研究。
</details></li>
</ul>
<hr>
<h2 id="Fusing-Monocular-Images-and-Sparse-IMU-Signals-for-Real-time-Human-Motion-Capture"><a href="#Fusing-Monocular-Images-and-Sparse-IMU-Signals-for-Real-time-Human-Motion-Capture" class="headerlink" title="Fusing Monocular Images and Sparse IMU Signals for Real-time Human Motion Capture"></a>Fusing Monocular Images and Sparse IMU Signals for Real-time Human Motion Capture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00310">http://arxiv.org/abs/2309.00310</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shaohua-pan/RobustCap">https://github.com/shaohua-pan/RobustCap</a></li>
<li>paper_authors: Shaohua Pan, Qi Ma, Xinyu Yi, Weifeng Hu, Xiong Wang, Xingkang Zhou, Jijunnan Li, Feng Xu</li>
<li>For:  This paper proposes a method for real-time human motion capture using a combination of monocular images and sparse IMUs.* Methods: The proposed method uses a dual coordinate strategy to fully explore the IMU signals and combines the information from both modalities to achieve robust motion capture.* Results: The proposed method significantly outperforms state-of-the-art vision, IMU, and combined methods on both global orientation and local pose estimation, and the codes are available for research at <a target="_blank" rel="noopener" href="https://shaohua-pan.github.io/robustcap-page/">https://shaohua-pan.github.io/robustcap-page/</a>.Here is the same information in Traditional Chinese:* For: 这篇 paper 提出了一种基于 monocular 影像和简陋 IMU 的 real-time人体动作捕捉方法。* Methods: 提出的方法使用了对 IMU 信号的双坐标策略，以全面利用 IMU 信号，并结合两种感测资料以 достиieving Robust 动作捕捉。* Results: 提出的方法在 global 方向和本地姿态估测方面均有 significanly 超过了现有的见识、IMU 和合成方法，并且 codes 可以在 <a target="_blank" rel="noopener" href="https://shaohua-pan.github.io/robustcap-page/">https://shaohua-pan.github.io/robustcap-page/</a> 进行研究。<details>
<summary>Abstract</summary>
Either RGB images or inertial signals have been used for the task of motion capture (mocap), but combining them together is a new and interesting topic. We believe that the combination is complementary and able to solve the inherent difficulties of using one modality input, including occlusions, extreme lighting/texture, and out-of-view for visual mocap and global drifts for inertial mocap. To this end, we propose a method that fuses monocular images and sparse IMUs for real-time human motion capture. Our method contains a dual coordinate strategy to fully explore the IMU signals with different goals in motion capture. To be specific, besides one branch transforming the IMU signals to the camera coordinate system to combine with the image information, there is another branch to learn from the IMU signals in the body root coordinate system to better estimate body poses. Furthermore, a hidden state feedback mechanism is proposed for both two branches to compensate for their own drawbacks in extreme input cases. Thus our method can easily switch between the two kinds of signals or combine them in different cases to achieve a robust mocap. %The two divided parts can help each other for better mocap results under different conditions. Quantitative and qualitative results demonstrate that by delicately designing the fusion method, our technique significantly outperforms the state-of-the-art vision, IMU, and combined methods on both global orientation and local pose estimation. Our codes are available for research at https://shaohua-pan.github.io/robustcap-page/.
</details>
<details>
<summary>摘要</summary>
Original text: Either RGB images or inertial signals have been used for the task of motion capture (mocap), but combining them together is a new and interesting topic. We believe that the combination is complementary and able to solve the inherent difficulties of using one modality input, including occlusions, extreme lighting/texture, and out-of-view for visual mocap and global drifts for inertial mocap. To this end, we propose a method that fuses monocular images and sparse IMUs for real-time human motion capture. Our method contains a dual coordinate strategy to fully explore the IMU signals with different goals in motion capture. To be specific, besides one branch transforming the IMU signals to the camera coordinate system to combine with the image information, there is another branch to learn from the IMU signals in the body root coordinate system to better estimate body poses. Furthermore, a hidden state feedback mechanism is proposed for both two branches to compensate for their own drawbacks in extreme input cases. Thus our method can easily switch between the two kinds of signals or combine them in different cases to achieve a robust mocap. %The two divided parts can help each other for better mocap results under different conditions. Quantitative and qualitative results demonstrate that by delicately designing the fusion method, our technique significantly outperforms the state-of-the-art vision, IMU, and combined methods on both global orientation and local pose estimation. Our codes are available for research at https://shaohua-pan.github.io/robustcap-page/.Simplified Chinese translation: Either RGB 图像或惯性信号已经用于人体运动捕捉（моcap）任务，但是将其结合在一起是一个新领域的研究。我们认为这种结合是补做的，可以解决单modal输入的内在困难，包括图像中的 occlusion、极端的光照/文化和视图外的出现。为此，我们提出了一种将单静止图像和稀疏 IMU 进行实时人体运动捕捉的方法。我们的方法包括一种双坐标策略，以便充分利用 IMU 信号的不同目标在运动捕捉中。具体来说，除了一个分支将 IMU 信号转换到相机坐标系统中，与图像信息结合之外，还有另一个分支可以从 IMU 信号中学习体部pose。此外，我们还提出了一种隐藏状态反馈机制，以便在极端输入情况下补做各自的缺陷。因此，我们的方法可以根据不同情况选择不同的信号或将其结合在一起，以实现一种稳定的 mocap。%两个分支可以在不同的情况下协助each other，以提高运动捕捉结果。我们的数据可以在 https://shaohua-pan.github.io/robustcap-page/ 上进行研究。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Surrogate-Models-for-Materials-Science-Simulations-Machine-Learning-based-Prediction-of-Microstructure-Properties"><a href="#Efficient-Surrogate-Models-for-Materials-Science-Simulations-Machine-Learning-based-Prediction-of-Microstructure-Properties" class="headerlink" title="Efficient Surrogate Models for Materials Science Simulations: Machine Learning-based Prediction of Microstructure Properties"></a>Efficient Surrogate Models for Materials Science Simulations: Machine Learning-based Prediction of Microstructure Properties</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00305">http://arxiv.org/abs/2309.00305</a></li>
<li>repo_url: None</li>
<li>paper_authors: Binh Duong Nguyen, Pavlo Potapenko, Aytekin Dermici, Kishan Govinda, Stefan Sandfeld</li>
<li>for: 这篇论文的目的是为了探讨和预测物理、化学、生物等领域中的结构属性关系。</li>
<li>methods: 这篇论文使用了六种机器学习算法，并分析了这些算法的准确性和可靠性，以及它们之间的区别。</li>
<li>results: 这篇论文通过分析两个不同数据集，包括二维离散伊辛模型的数据和卡恩-希耶模型的数据，以及将这些数据转换为特别设计的特征，来预测结构属性关系。<details>
<summary>Abstract</summary>
Determining, understanding, and predicting the so-called structure-property relation is an important task in many scientific disciplines, such as chemistry, biology, meteorology, physics, engineering, and materials science. Structure refers to the spatial distribution of, e.g., substances, material, or matter in general, while property is a resulting characteristic that usually depends in a non-trivial way on spatial details of the structure. Traditionally, forward simulations models have been used for such tasks. Recently, several machine learning algorithms have been applied in these scientific fields to enhance and accelerate simulation models or as surrogate models. In this work, we develop and investigate the applications of six machine learning techniques based on two different datasets from the domain of materials science: data from a two-dimensional Ising model for predicting the formation of magnetic domains and data representing the evolution of dual-phase microstructures from the Cahn-Hilliard model. We analyze the accuracy and robustness of all models and elucidate the reasons for the differences in their performances. The impact of including domain knowledge through tailored features is studied, and general recommendations based on the availability and quality of training data are derived from this.
</details>
<details>
<summary>摘要</summary>
In this work, we develop and investigate the applications of six machine learning techniques based on two different datasets from the domain of materials science: data from a two-dimensional Ising model for predicting the formation of magnetic domains and data representing the evolution of dual-phase microstructures from the Cahn-Hilliard model. We analyze the accuracy and robustness of all models and elucidate the reasons for the differences in their performances.We also study the impact of including domain knowledge through tailored features and derive general recommendations based on the availability and quality of training data. Our findings provide insights into the potential of machine learning techniques for predicting structure-property relations in materials science and highlight the importance of considering domain knowledge and data quality when selecting and applying these techniques.
</details></li>
</ul>
<hr>
<h2 id="Fine-Grained-Spatiotemporal-Motion-Alignment-for-Contrastive-Video-Representation-Learning"><a href="#Fine-Grained-Spatiotemporal-Motion-Alignment-for-Contrastive-Video-Representation-Learning" class="headerlink" title="Fine-Grained Spatiotemporal Motion Alignment for Contrastive Video Representation Learning"></a>Fine-Grained Spatiotemporal Motion Alignment for Contrastive Video Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00297">http://arxiv.org/abs/2309.00297</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minghao Zhu, Xiao Lin, Ronghao Dang, Chengju Liu, Qijun Chen</li>
<li>for: 本文提出了一种基于细谱强制学习的高级视频表示方法，用于提高视频表示的动态信息表示能力。</li>
<li>methods: 本文使用了框架差为动态信息源，并通过设计 pixel-level 动态监督学习和前景采样策略来提高动态信息的匹配率。此外，文章还提出了一种帧级动态强制损失来提高动态特征的时间多样性。</li>
<li>results: 实验表明，基于 FIMA 框架学习的表示能够具备出色的动态意识能力，并在 UCF101、HMDB51 和 Diving48 等 datasets 上 achieve state-of-the-art or competitive results。<details>
<summary>Abstract</summary>
As the most essential property in a video, motion information is critical to a robust and generalized video representation. To inject motion dynamics, recent works have adopted frame difference as the source of motion information in video contrastive learning, considering the trade-off between quality and cost. However, existing works align motion features at the instance level, which suffers from spatial and temporal weak alignment across modalities. In this paper, we present a \textbf{Fi}ne-grained \textbf{M}otion \textbf{A}lignment (FIMA) framework, capable of introducing well-aligned and significant motion information. Specifically, we first develop a dense contrastive learning framework in the spatiotemporal domain to generate pixel-level motion supervision. Then, we design a motion decoder and a foreground sampling strategy to eliminate the weak alignments in terms of time and space. Moreover, a frame-level motion contrastive loss is presented to improve the temporal diversity of the motion features. Extensive experiments demonstrate that the representations learned by FIMA possess great motion-awareness capabilities and achieve state-of-the-art or competitive results on downstream tasks across UCF101, HMDB51, and Diving48 datasets. Code is available at \url{https://github.com/ZMHH-H/FIMA}.
</details>
<details>
<summary>摘要</summary>
As the most essential property in a video, motion information is critical to a robust and generalized video representation. To inject motion dynamics, recent works have adopted frame difference as the source of motion information in video contrastive learning, considering the trade-off between quality and cost. However, existing works align motion features at the instance level, which suffers from spatial and temporal weak alignment across modalities. In this paper, we present a 细grained Motion Alignment (FIMA) framework, capable of introducing well-aligned and significant motion information. Specifically, we first develop a dense contrastive learning framework in the spatiotemporal domain to generate pixel-level motion supervision. Then, we design a motion decoder and a foreground sampling strategy to eliminate the weak alignments in terms of time and space. Moreover, a frame-level motion contrastive loss is presented to improve the temporal diversity of the motion features. Extensive experiments demonstrate that the representations learned by FIMA possess great motion-awareness capabilities and achieve state-of-the-art or competitive results on downstream tasks across UCF101, HMDB51, and Diving48 datasets. Code is available at \url{https://github.com/ZMHH-H/FIMA}.
</details></li>
</ul>
<hr>
<h2 id="Fast-Diffusion-EM-a-diffusion-model-for-blind-inverse-problems-with-application-to-deconvolution"><a href="#Fast-Diffusion-EM-a-diffusion-model-for-blind-inverse-problems-with-application-to-deconvolution" class="headerlink" title="Fast Diffusion EM: a diffusion model for blind inverse problems with application to deconvolution"></a>Fast Diffusion EM: a diffusion model for blind inverse problems with application to deconvolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00287">http://arxiv.org/abs/2309.00287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Charles Laroche, Andrés Almansa, Eva Coupete</li>
<li>for:  solves inverse problems of blind image deblurring</li>
<li>methods:  uses diffusion models and Expectation-Minimization (EM) estimation method with blur kernel regularization</li>
<li>results:  provides effective and fast results compared to other state-of-the-art approaches in blind image deblurring<details>
<summary>Abstract</summary>
Using diffusion models to solve inverse problems is a growing field of research. Current methods assume the degradation to be known and provide impressive results in terms of restoration quality and diversity. In this work, we leverage the efficiency of those models to jointly estimate the restored image and unknown parameters of the degradation model. In particular, we designed an algorithm based on the well-known Expectation-Minimization (EM) estimation method and diffusion models. Our method alternates between approximating the expected log-likelihood of the inverse problem using samples drawn from a diffusion model and a maximization step to estimate unknown model parameters. For the maximization step, we also introduce a novel blur kernel regularization based on a Plug \& Play denoiser. Diffusion models are long to run, thus we provide a fast version of our algorithm. Extensive experiments on blind image deblurring demonstrate the effectiveness of our method when compared to other state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)使用分散模型解决反向问题是一个快速 развивающийся的研究领域。当前的方法假设质量损害是已知的，并且提供了非常出色的修复质量和多样性。在这个工作中，我们利用分散模型的效率来同时估计修复图像和未知的损害模型参数。特别是，我们设计了基于well-known Expectation-Minimization（EM）估计方法和分散模型的算法。我们的方法 alternate между估计反向问题的预期日志似然函数 using 分散模型中的样本，以及一个最大化步骤来估计未知模型参数。为了最大化步骤，我们还引入了一种新的噪声核定regularization，基于Plug & Playdenoiser。分散模型需要长时间运行，因此我们提供了一个快速版本的算法。我们的实验表明，当比较到其他当前最佳方法时，我们的方法具有非常出色的效果。
</details></li>
</ul>
<hr>
<h2 id="SparseSat-NeRF-Dense-Depth-Supervised-Neural-Radiance-Fields-for-Sparse-Satellite-Images"><a href="#SparseSat-NeRF-Dense-Depth-Supervised-Neural-Radiance-Fields-for-Sparse-Satellite-Images" class="headerlink" title="SparseSat-NeRF: Dense Depth Supervised Neural Radiance Fields for Sparse Satellite Images"></a>SparseSat-NeRF: Dense Depth Supervised Neural Radiance Fields for Sparse Satellite Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00277">http://arxiv.org/abs/2309.00277</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lulinzhang/sps-nerf">https://github.com/lulinzhang/sps-nerf</a></li>
<li>paper_authors: Lulin Zhang, Ewelina Rupnik</li>
<li>for: 用于恢复非拉伯天地表面的数字表面模型，使得传统多视图缺失、异步获取或缺失缝隙等挑战情况下可以更好地工作。</li>
<li>methods: 使用神经辐射场（NeRF），这是一种不需要基础知识的自我监督学习方法，可以包含场景中物理参数，从而更好地处理传统多视图匹配（MVS）失败的情况。</li>
<li>results: 在偏辐射1B&#x2F;WorldView-3图像上使用SpS-NeRF方法，与NeRF和Sat-NeRF相比，能够更好地恢复场景的几何结构。<details>
<summary>Abstract</summary>
Digital surface model generation using traditional multi-view stereo matching (MVS) performs poorly over non-Lambertian surfaces, with asynchronous acquisitions, or at discontinuities. Neural radiance fields (NeRF) offer a new paradigm for reconstructing surface geometries using continuous volumetric representation. NeRF is self-supervised, does not require ground truth geometry for training, and provides an elegant way to include in its representation physical parameters about the scene, thus potentially remedying the challenging scenarios where MVS fails. However, NeRF and its variants require many views to produce convincing scene's geometries which in earth observation satellite imaging is rare. In this paper we present SparseSat-NeRF (SpS-NeRF) - an extension of Sat-NeRF adapted to sparse satellite views. SpS-NeRF employs dense depth supervision guided by crosscorrelation similarity metric provided by traditional semi-global MVS matching. We demonstrate the effectiveness of our approach on stereo and tri-stereo Pleiades 1B/WorldView-3 images, and compare against NeRF and Sat-NeRF. The code is available at https://github.com/LulinZhang/SpS-NeRF
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用传统多视图顺序匹配（MVS）生成数字表面模型在非拉贝特表面上表现不佳，特别是在异步获取、缺失数据或缺界面上。基于神经辐射场（NeRF）的新方法可以continuous volumetric representation来重建表面几何。NeRF不需要训练时的地面几何数据，同时可以自动包含场景中的物理参数，因此可能解决MVS在困难情况下失败的问题。然而，NeRF和其变种需要许多视图来生成实际场景的几何，而在地球观测卫星图像中这是罕见的。本文介绍了SpS-NeRF（SpareSat-NeRF），它是基于Sat-NeRF的扩展，针对罕见的卫星视图进行了改进。SpS-NeRF使用了密集的深度监督，通过传统的semi-global MVS匹配提供的相似性度量来导引。我们在顺recto-stereo Pleiades 1B/WorldView-3图像上展示了我们的方法的效果，并与NeRF和Sat-NeRF进行了比较。代码可以在https://github.com/LulinZhang/SpS-NeRF上获取。
</details></li>
</ul>
<hr>
<h2 id="Application-of-Machine-Learning-in-Melanoma-Detection-and-the-Identification-of-‘Ugly-Duckling’-and-Suspicious-Naevi-A-Review"><a href="#Application-of-Machine-Learning-in-Melanoma-Detection-and-the-Identification-of-‘Ugly-Duckling’-and-Suspicious-Naevi-A-Review" class="headerlink" title="Application of Machine Learning in Melanoma Detection and the Identification of ‘Ugly Duckling’ and Suspicious Naevi: A Review"></a>Application of Machine Learning in Melanoma Detection and the Identification of ‘Ugly Duckling’ and Suspicious Naevi: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00265">http://arxiv.org/abs/2309.00265</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fatima Al Zegair, Nathasha Naranpanawa, Brigid Betz-Stablein, Monika Janda, H. Peter Soyer, Shekhar S. Chandra<br>for: 这篇论文的目的是什么？methods: 这篇论文使用了哪些方法？results: 这篇论文获得了什么结果？Here are the answers in Simplified Chinese text:for: 这篇论文的目的是为了提高皮肤癌诊断的精度和方便，以及应对皮肤癌医生短缺。methods: 这篇论文使用了机器学习和深度学习技术，包括人工神经网络等，以探索皮肤癌早期识别和应对方法。results: 这篇论文获得了训练结果，显示机器学习和深度学习技术可以实现与专业医生相等的皮肤癌诊断精度，并且可以帮助减少医疗成本和提高疗效率。<details>
<summary>Abstract</summary>
Skin lesions known as naevi exhibit diverse characteristics such as size, shape, and colouration. The concept of an "Ugly Duckling Naevus" comes into play when monitoring for melanoma, referring to a lesion with distinctive features that sets it apart from other lesions in the vicinity. As lesions within the same individual typically share similarities and follow a predictable pattern, an ugly duckling naevus stands out as unusual and may indicate the presence of a cancerous melanoma. Computer-aided diagnosis (CAD) has become a significant player in the research and development field, as it combines machine learning techniques with a variety of patient analysis methods. Its aim is to increase accuracy and simplify decision-making, all while responding to the shortage of specialized professionals. These automated systems are especially important in skin cancer diagnosis where specialist availability is limited. As a result, their use could lead to life-saving benefits and cost reductions within healthcare. Given the drastic change in survival when comparing early stage to late-stage melanoma, early detection is vital for effective treatment and patient outcomes. Machine learning (ML) and deep learning (DL) techniques have gained popularity in skin cancer classification, effectively addressing challenges, and providing results equivalent to that of specialists. This article extensively covers modern Machine Learning and Deep Learning algorithms for detecting melanoma and suspicious naevi. It begins with general information on skin cancer and different types of naevi, then introduces AI, ML, DL, and CAD. The article then discusses the successful applications of various ML techniques like convolutional neural networks (CNN) for melanoma detection compared to dermatologists' performance. Lastly, it examines ML methods for UD naevus detection and identifying suspicious naevi.
</details>
<details>
<summary>摘要</summary>
皮肤 lesions  bekannt为 naevi 具有多样的特征，如大小、形状和颜色。“ugly duckling naevus”是在监测 melanoma 时的概念，指的是一个与周围其他 lesions 不同的特征，可能是癌变。因为 lesions 在同一个人身上通常具有相似的特征和预测的模式，ugly duckling naevus 会突出来为不寻常，并可能表示癌变的存在。computer-aided diagnosis (CAD) 在研发领域中发挥了重要作用，它结合了机器学习技术和多种患者分析方法。其目的是提高准确性和简化决策，同时回应医疗专业人员的短缺。这些自动化系统在皮肤癌诊断中特别重要，因为专业人员的可用性有限。因此，它们的使用可能导致生命的拯救和医疗成本的减少。由于皮肤癌的晚期诊断和治疗对 patient 的结果有极大的影响，早期检测是至关重要的。机器学习（ML）和深度学习（DL）技术在皮肤癌类型化方面取得了成功，有效地解决了一些挑战，并提供了与专业人员相当的结果。这篇文章从 skin cancer 和不同类型的 naevi 的基础知识开始，然后介绍了 AI、ML、DL 和 CAD。文章 then discusses 了不同 ML 技术，如 convolutional neural networks (CNN) 在 melanoma 检测中的成功，与 dermatologists 的性能相比。最后，它检查了 ML 方法在 UD naevus 检测和寻找可疑 naevi 方面的应用。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Medical-Imagery-Diagnosis-with-Self-Attentive-Transformers-A-Review-of-Explainable-AI-for-Health-Care"><a href="#Interpretable-Medical-Imagery-Diagnosis-with-Self-Attentive-Transformers-A-Review-of-Explainable-AI-for-Health-Care" class="headerlink" title="Interpretable Medical Imagery Diagnosis with Self-Attentive Transformers: A Review of Explainable AI for Health Care"></a>Interpretable Medical Imagery Diagnosis with Self-Attentive Transformers: A Review of Explainable AI for Health Care</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00252">http://arxiv.org/abs/2309.00252</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tin Lai</li>
<li>for: 本文旨在探讨最新的人工智能技术在基础医疗服务中的推广，以及使用感知 трансформер（ViT）模型，以及如何使用解释人工智能（XAI）方法来理解ViT模型做出决策的过程。</li>
<li>methods: 本文主要介绍了最新的ViT模型和XAI方法，包括自注意机制和解释模型，以及其应用于医疗诊断领域。</li>
<li>results: 本文结合了最新的ViT和XAI技术，提供了一种透明的医疗诊断方法，可以帮助医生更好地理解模型做出的决策，从而提高医疗诊断的准确性和可靠性。<details>
<summary>Abstract</summary>
Recent advancements in artificial intelligence (AI) have facilitated its widespread adoption in primary medical services, addressing the demand-supply imbalance in healthcare. Vision Transformers (ViT) have emerged as state-of-the-art computer vision models, benefiting from self-attention modules. However, compared to traditional machine-learning approaches, deep-learning models are complex and are often treated as a "black box" that can cause uncertainty regarding how they operate. Explainable Artificial Intelligence (XAI) refers to methods that explain and interpret machine learning models' inner workings and how they come to decisions, which is especially important in the medical domain to guide the healthcare decision-making process. This review summarises recent ViT advancements and interpretative approaches to understanding the decision-making process of ViT, enabling transparency in medical diagnosis applications.
</details>
<details>
<summary>摘要</summary>
最近的人工智能（AI）发展已经推动了医疗服务中的广泛应用，解决医疗需求和供应的失衡。视Transformer（ViT）已经 emerge 为当前最佳的计算机视觉模型，受益于自注意模块。然而，相比传统机器学习方法，深度学习模型更加复杂，经常被视为一个“黑盒子”，导致决策过程中存在不确定性。可解释人工智能（XAI）是指解释和解读机器学习模型内部工作的方法，尤其在医疗领域，以便guide医疗决策过程。本文回顾了最新的ViT进展和解释方法，以提高医疗诊断应用中的透明度。
</details></li>
</ul>
<hr>
<h2 id="Object-Centric-Multiple-Object-Tracking"><a href="#Object-Centric-Multiple-Object-Tracking" class="headerlink" title="Object-Centric Multiple Object Tracking"></a>Object-Centric Multiple Object Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00233">http://arxiv.org/abs/2309.00233</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amazon-science/object-centric-multiple-object-tracking">https://github.com/amazon-science/object-centric-multiple-object-tracking</a></li>
<li>paper_authors: Zixu Zhao, Jiaze Wang, Max Horn, Yizhuo Ding, Tong He, Zechen Bai, Dominik Zietlow, Carl-Johann Simon-Gabriel, Bing Shuai, Zhuowen Tu, Thomas Brox, Bernt Schiele, Yanwei Fu, Francesco Locatello, Zheng Zhang, Tianjun Xiao</li>
<li>for: 这个论文是为了解决多个物体跟踪（MOT）预测中的注解卷入问题而写的。</li>
<li>methods: 这个论文使用的方法是基于无监督物体学习的对象-центric模型，包括一个index-merge模块和一个物体记忆模块。这两个模块可以处理 occlusions 和缺失捕捉。</li>
<li>results: 这个论文的实验结果显示，这种方法可以减少 MOT 预测中的注解卷入，并且可以与完全监督的状态前进比肩。此外，这种方法还可以超过一些无监督跟踪器的性能。<details>
<summary>Abstract</summary>
Unsupervised object-centric learning methods allow the partitioning of scenes into entities without additional localization information and are excellent candidates for reducing the annotation burden of multiple-object tracking (MOT) pipelines. Unfortunately, they lack two key properties: objects are often split into parts and are not consistently tracked over time. In fact, state-of-the-art models achieve pixel-level accuracy and temporal consistency by relying on supervised object detection with additional ID labels for the association through time. This paper proposes a video object-centric model for MOT. It consists of an index-merge module that adapts the object-centric slots into detection outputs and an object memory module that builds complete object prototypes to handle occlusions. Benefited from object-centric learning, we only require sparse detection labels (0%-6.25%) for object localization and feature binding. Relying on our self-supervised Expectation-Maximization-inspired loss for object association, our approach requires no ID labels. Our experiments significantly narrow the gap between the existing object-centric model and the fully supervised state-of-the-art and outperform several unsupervised trackers.
</details>
<details>
<summary>摘要</summary>
<<SYS>>无监督对象中心学习方法可以将场景分解为实体，无需额外的本地化信息，是多个物体跟踪（MOT）管道中减少注解负担的优秀候选人。然而，它们缺乏两个关键特性：物体经常被分解成部分，并且在时间上不一致地跟踪。事实上，现状的模型可以在批处精度和时间上达到像素级准确性，通过额外的ID标签进行对时asso ciation。这篇论文提出了视频对象中心模型，它包括一个索引合并模块，将对象中心槽 adapt into 检测输出，以及一个对象记忆模块，用于处理遮挡。由于对象中心学习，我们只需 sparse 的检测标注（0%-6.25%）来确定物体位置和特征绑定。基于我们的自我超vised Expectation-Maximization-inspired 损失函数，我们的方法不需要ID标签。我们的实验显著缩小了现有的对象中心模型和完全监督状态前的差距，并在多个无监督跟踪器之上表现出色。
</details></li>
</ul>
<hr>
<h2 id="What-Makes-Good-Open-Vocabulary-Detector-A-Disassembling-Perspective"><a href="#What-Makes-Good-Open-Vocabulary-Detector-A-Disassembling-Perspective" class="headerlink" title="What Makes Good Open-Vocabulary Detector: A Disassembling Perspective"></a>What Makes Good Open-Vocabulary Detector: A Disassembling Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00227">http://arxiv.org/abs/2309.00227</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jincheng Li, Chunyu Xie, Xiaoyu Wu, Bin Wang, Dawei Leng</li>
<li>for: 这篇论文旨在解决开放词汇检测（OVD）中的检测和识别未知对象的问题，以提高OVD的性能。</li>
<li>methods: 该论文使用了预训练的跨模态VLM（如CLIP、ALIGN等），并分析了三种OVD方法的设计各种各样。这三种方法分别是：vanilla方法、将二stage对象检测器与CLIP结合的方法，以及将RPN和RoI头结合的方法。</li>
<li>results: 在OVD-COCO和OVD-LVIS测试集上，DRR方法获得了最好的性能，在未知类别中提高了35.8个 Novel AP$<em>{50}$，相比之前的最佳状态（SOTA）提高了2.8个 Novel AP$</em>{50}$。在罕见类别中，DRR方法超过了前一个SOTA的1.9个 AP$_{50}$。此外，论文还提供了一个对象检测数据集called PID，并提供了该数据集上的基线性能。<details>
<summary>Abstract</summary>
Open-vocabulary detection (OVD) is a new object detection paradigm, aiming to localize and recognize unseen objects defined by an unbounded vocabulary. This is challenging since traditional detectors can only learn from pre-defined categories and thus fail to detect and localize objects out of pre-defined vocabulary. To handle the challenge, OVD leverages pre-trained cross-modal VLM, such as CLIP, ALIGN, etc. Previous works mainly focus on the open vocabulary classification part, with less attention on the localization part. We argue that for a good OVD detector, both classification and localization should be parallelly studied for the novel object categories. We show in this work that improving localization as well as cross-modal classification complement each other, and compose a good OVD detector jointly. We analyze three families of OVD methods with different design emphases. We first propose a vanilla method,i.e., cropping a bounding box obtained by a localizer and resizing it into the CLIP. We next introduce another approach, which combines a standard two-stage object detector with CLIP. A two-stage object detector includes a visual backbone, a region proposal network (RPN), and a region of interest (RoI) head. We decouple RPN and ROI head (DRR) and use RoIAlign to extract meaningful features. In this case, it avoids resizing objects. To further accelerate the training time and reduce the model parameters, we couple RPN and ROI head (CRR) as the third approach. We conduct extensive experiments on these three types of approaches in different settings. On the OVD-COCO benchmark, DRR obtains the best performance and achieves 35.8 Novel AP$_{50}$, an absolute 2.8 gain over the previous state-of-the-art (SOTA). For OVD-LVIS, DRR surpasses the previous SOTA by 1.9 AP$_{50}$ in rare categories. We also provide an object detection dataset called PID and provide a baseline on PID.
</details>
<details>
<summary>摘要</summary>
新的目标检测方式：开 vocabulary detection (OVD)，旨在确定和识别未经定义的对象。这是一个挑战，因为传统的检测器只能学习预先定义的类别，因此无法检测和定位未知的对象。为解决这个挑战，OVD 利用预训练的交叉模态 VLM，如 CLIP 和 ALIGN 等。过去的工作主要关注于开 vocabulary 分类部分，尚未充分关注本地化部分。我们认为，为一个好的 OVD 检测器，分类和本地化应该同时研究对于新的对象类别。我们在这里展示，提高本地化以及交叉模态分类之间的互补关系，组成一个好的 OVD 检测器。我们分析了三种 OVD 方法的设计强调不同，并进行了广泛的实验。首先，我们提出了一种简单的方法，即将一个由本地化器生成的 bounding box 裁剪成 CLIP 的大小，并将其作为输入给 CLIP。然后，我们引入了另一种方法，即将标准的两stage 对象检测器与 CLIP 结合，该检测器包括视觉脊梁、区域提案网络（RPN）和区域兴趣（RoI）头。我们在这种情况下，使用 RoIAlign 提取有用的特征。这种方法可以避免对对象进行resize。最后，我们将 RPN 和 RoI 头结合（CRR），以便快速训练和减少模型参数。我们在这些三种方法上进行了广泛的实验，并在 OVD-COCO 标准测试集上进行了评估。DRR 在 OVD-COCO 上取得了最高性能，其 Novel AP$_{50}$ 为 35.8，相对于前一个 SOTA 提高了 2.8。在 rare 类别上，DRR 超过了之前的 SOTA 的 1.9 AP$_{50}$。我们还提供了一个对象检测数据集 called PID，并提供了这个数据集上的基线。
</details></li>
</ul>
<hr>
<h2 id="Human-Inspired-Facial-Sketch-Synthesis-with-Dynamic-Adaptation"><a href="#Human-Inspired-Facial-Sketch-Synthesis-with-Dynamic-Adaptation" class="headerlink" title="Human-Inspired Facial Sketch Synthesis with Dynamic Adaptation"></a>Human-Inspired Facial Sketch Synthesis with Dynamic Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00216">http://arxiv.org/abs/2309.00216</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aiart-hdu/hida">https://github.com/aiart-hdu/hida</a></li>
<li>paper_authors: Fei Gao, Yifan Zhu, Chang Jiang, Nannan Wang</li>
<li>for: 该 paper 的目的是提出一种基于人工创作的动态适应（HIDA）方法，以生成高质量的人脸素描。</li>
<li>methods: 该方法使用动态调整神经元活动，并使用可变卷积来对深度特征进行匹配，以生成抽象的人脸素描。</li>
<li>results: 实验结果表明，HIDA 可以在多种难度的人脸上生成高质量的素描，并在不同风格的人脸上保持一致性。<details>
<summary>Abstract</summary>
Facial sketch synthesis (FSS) aims to generate a vivid sketch portrait from a given facial photo. Existing FSS methods merely rely on 2D representations of facial semantic or appearance. However, professional human artists usually use outlines or shadings to covey 3D geometry. Thus facial 3D geometry (e.g. depth map) is extremely important for FSS. Besides, different artists may use diverse drawing techniques and create multiple styles of sketches; but the style is globally consistent in a sketch. Inspired by such observations, in this paper, we propose a novel Human-Inspired Dynamic Adaptation (HIDA) method. Specially, we propose to dynamically modulate neuron activations based on a joint consideration of both facial 3D geometry and 2D appearance, as well as globally consistent style control. Besides, we use deformable convolutions at coarse-scales to align deep features, for generating abstract and distinct outlines. Experiments show that HIDA can generate high-quality sketches in multiple styles, and significantly outperforms previous methods, over a large range of challenging faces. Besides, HIDA allows precise style control of the synthesized sketch, and generalizes well to natural scenes and other artistic styles. Our code and results have been released online at: https://github.com/AiArt-HDU/HIDA.
</details>
<details>
<summary>摘要</summary>
Facial Sketch Synthesis (FSS) 目标是从给定的面部照片中生成一个生动的笔画肖像。现有的 FSS 方法只是基于面部semantic或外观的2D表示。然而，职业艺术家通常使用 outline 或渐变来传递3D几何信息。因此，面部3D几何（例如深度图）在 FSS 中非常重要。此外，不同艺术家可能使用不同的绘画技巧，但是在绘画中保持全局一致的风格是非常重要。以这些观察为 inspirations，在这篇论文中，我们提出了一种新的人类 inspiritedynamic adaptation（HIDA）方法。具体来说，我们提出在考虑面部3D几何和2D外观以及全局一致的风格控制的基础上，动态调整神经活动。此外，我们使用可变核函数在粗略尺度上对深度特征进行对接，以生成抽象而独特的 OUTLINE。实验表明，HIDA 可以生成高质量的笔画肖像，并在各种挑战性脸部上显著超越先前的方法。此外，HIDA 允许精准地控制生成的笔画肖像的风格，并在自然场景和其他艺术风格上广泛适用。我们的代码和结果已经在 GitHub 上发布：https://github.com/AiArt-HDU/HIDA。
</details></li>
</ul>
<hr>
<h2 id="DARC-Distribution-Aware-Re-Coloring-Model-for-Generalizable-Nucleus-Segmentation"><a href="#DARC-Distribution-Aware-Re-Coloring-Model-for-Generalizable-Nucleus-Segmentation" class="headerlink" title="DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation"></a>DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00188">http://arxiv.org/abs/2309.00188</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengcong Chen, Changxing Ding, Dacheng Tao, Hao Chen</li>
<li>for: 本研究旨在提出一种普适的核心分割模型，能够在不同域的图像上进行准确的核心分割。</li>
<li>methods: 本研究使用了一种 Distribution-Aware Re-Coloring (DARC) 模型，从两个角度解决了域 gap 问题。首先，提出了一种重新调色方法，以解决不同域的图像颜色差异。其次，提出了一种新的实例normalization方法，可以快速地处理不同的前景-背景比例变化。</li>
<li>results: 对于两个 H$&amp;$E 染料和两个 IHC 染料的图像数据集进行了广泛的实验，证明了我们提出的 DARC 模型的效果。代码可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/csccsccsccsc/DARC%7D">https://github.com/csccsccsccsc/DARC}</a> 上下载。<details>
<summary>Abstract</summary>
Nucleus segmentation is usually the first step in pathological image analysis tasks. Generalizable nucleus segmentation refers to the problem of training a segmentation model that is robust to domain gaps between the source and target domains. The domain gaps are usually believed to be caused by the varied image acquisition conditions, e.g., different scanners, tissues, or staining protocols. In this paper, we argue that domain gaps can also be caused by different foreground (nucleus)-background ratios, as this ratio significantly affects feature statistics that are critical to normalization layers. We propose a Distribution-Aware Re-Coloring (DARC) model that handles the above challenges from two perspectives. First, we introduce a re-coloring method that relieves dramatic image color variations between different domains. Second, we propose a new instance normalization method that is robust to the variation in foreground-background ratios. We evaluate the proposed methods on two H$\&$E stained image datasets, named CoNSeP and CPM17, and two IHC stained image datasets, called DeepLIIF and BC-DeepLIIF. Extensive experimental results justify the effectiveness of our proposed DARC model. Codes are available at \url{https://github.com/csccsccsccsc/DARC
</details>
<details>
<summary>摘要</summary>
nuclei segmentation 通常是生物医学图像分析任务的第一步。通用 nuclei segmentation 问题指的是训练一个可以在不同来源领域中具有良好性能的分 segmentation 模型。这些领域差异通常被认为是由不同的图像捕获条件引起的，例如不同的扫描仪、组织或染色方法。在这篇论文中，我们认为领域差异也可能是由不同的前景（核体）背景比例引起的，因为这种比例对于图像的特征统计学非常重要。我们提出了一种 Distribution-Aware Re-Coloring（DARC）模型，该模型可以处理以下挑战。首先，我们引入了一种重新调色方法，以降低不同领域图像之间的极大图像颜色差异。其次，我们提出了一种新的实例Normalization方法，可以对不同的前景-背景比例进行鲁棒化。我们在两个HE染料图像集（CoNSeP和CPM17）和两个IHC染料图像集（DeepLIIF和BC-DeepLIIF）上进行了广泛的实验，结果证明了我们提出的DARC模型的效果。代码可以在 \url{https://github.com/csccsccsccsc/DARC} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Vision-aided-nonlinear-control-framework-for-shake-table-tests"><a href="#Vision-aided-nonlinear-control-framework-for-shake-table-tests" class="headerlink" title="Vision-aided nonlinear control framework for shake table tests"></a>Vision-aided nonlinear control framework for shake table tests</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00187">http://arxiv.org/abs/2309.00187</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhongwei Chen, T. Y. Yang, Yifei Xiao, Xiao Pan, Wanyan Yang</li>
<li>For: 本研究使用适应控制理论来实现震动机械系统中的控制和结构相互作用（Control-Structural Interaction，CSI），并考虑了系统的非线性性。* Methods: 本研究使用了滤波器控制理论（Proportional-Integral-Derivative，PID）和适应控制理论来实现非线性控制。* Results:  simulations and experiments show that the proposed control framework can effectively control the shake table and reduce the vibration of the structure under earthquake excitations.<details>
<summary>Abstract</summary>
The structural response under the earthquake excitations can be simulated by scaled-down model shake table tests or full-scale model shake table tests. In this paper, adaptive control theory is used as a nonlinear shake table control algorithm which considers the inherent nonlinearity of the shake table system and the Control-Structural Interaction (CSI) effect that the linear controller cannot consider, such as the Proportional-Integral-Derivative (PID) controller. The mass of the specimen can be assumed as an unknown variation and the unknown parameter will be replaced by an estimated value in the proposed control framework. The signal generated by the control law of the adaptive control method will be implemented by a loop-shaping controller. To verify the stability and feasibility of the proposed control framework, a simulation of a bare shake table and experiments with a bare shake table with a two-story frame were carried out. This study randomly selects Earthquake recordings from the Pacific Earthquake Engineering Research Center (PEER) database. The simulation and experimental results show that the proposed control framework can be effectively used in shake table control.
</details>
<details>
<summary>摘要</summary>
《震动试验中的结构回应可以通过尺度缩小的模型震动试验或全尺度模型震动试验来模拟。本文使用适应控制理论作为震动试验中的非线性控制算法，考虑了震动试验系统的自然非线性和控制结构互动（CSI）效应，例如调速度-积分- Derivative（PID）控制器。试验用的物品重量可以被视为未知变化，并将未知参数更新为估算值。控制法框架中的控制信号将由适应控制方法的loop-shaping控制器实现。为验证提案的稳定性和可行性，本研究在实验中使用了一个空震动试验和一个有两层架构的空震动试验进行了实验。这些实验使用了太平洋地震工程研究中心（PEER）数据库中的地震纪录。实验和模拟结果显示，提案的控制法框架可以有效地应用于震动试验中。》Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/01/cs.CV_2023_09_01/" data-id="clpxp6c1b00j2ee887756cdcr" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_09_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/01/cs.AI_2023_09_01/" class="article-date">
  <time datetime="2023-09-01T12:00:00.000Z" itemprop="datePublished">2023-09-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/01/cs.AI_2023_09_01/">cs.AI - 2023-09-01</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Efficient-RLHF-Reducing-the-Memory-Usage-of-PPO"><a href="#Efficient-RLHF-Reducing-the-Memory-Usage-of-PPO" class="headerlink" title="Efficient RLHF: Reducing the Memory Usage of PPO"></a>Efficient RLHF: Reducing the Memory Usage of PPO</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00754">http://arxiv.org/abs/2309.00754</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Santacroce, Yadong Lu, Han Yu, Yuanzhi Li, Yelong Shen</li>
<li>for: 这个论文的目的是解决RLHF中PPO阶段的内存问题，使得更多的实践者能够使用RLHF进行语言模型化。</li>
<li>methods: 论文使用了一系列的内存节省技术来降低PPO的内存使用量，并对这些技术的影响进行了全面的分析。</li>
<li>results: 实验结果显示，使用LoRA durante PPO可以降低PPO的内存使用量，并在四个公共benchmark上提高了RLHF的对齐性。此外，Hydra-PPO可以降低LoRA-PPO的样本延迟时间，而不会影响其性能。这些结果表明，Hydra-PPO是一种简单有前途的解决方案，可以普及RLHF的使用。<details>
<summary>Abstract</summary>
Reinforcement Learning with Human Feedback (RLHF) has revolutionized language modeling by aligning models with human preferences. However, the RL stage, Proximal Policy Optimization (PPO), requires over 3x the memory of Supervised Fine-Tuning (SFT), making it infeasible to use for most practitioners. To address this issue, we present a comprehensive analysis the memory usage, performance, and training time of memory-savings techniques for PPO. We introduce Hydra-RLHF by first integrating the SFT and Reward models and then dynamically turning LoRA "off" during training. Our experiments show: 1. Using LoRA during PPO reduces its memory usage to be smaller than SFT while improving alignment across four public benchmarks, and 2. Hydra-PPO reduces the latency per sample of LoRA-PPO by up to 65% while maintaining its performance. Our results demonstrate that Hydra-PPO is a simple and promising solution for enabling more widespread usage of RLHF.
</details>
<details>
<summary>摘要</summary>
“强化学习 avec 人类反馈（RLHF）已经革命化语言模型化，将模型与人类偏好进行Alignment。但是，RL阶段的Proximal Policy Optimization（PPO）需要更多的内存，使得大多数实践者无法使用。为解决这个问题，我们提供了一个涵盖性分析的内存使用、性能和训练时间的分析。我们首先将Supervised Fine-Tuning（SFT）和Reward模型集成，然后在训练过程中静态地将LoRA“Off”。我们的实验结果显示：1. 在PPO中使用LoRA可以降低其内存使用量，与SFT相比，并在四个公共测试集上提高了Alignment的表现。2. Hydra-PPO可以将LoRA-PPO的延迟时间降低至最多65%，保持其表现。我们的结果显示，Hydra-PPO是一个简单且有前途的解决方案，可以帮助RLHF更加广泛地应用。”
</details></li>
</ul>
<hr>
<h2 id="Language-Conditioned-Change-point-Detection-to-Identify-Sub-Tasks-in-Robotics-Domains"><a href="#Language-Conditioned-Change-point-Detection-to-Identify-Sub-Tasks-in-Robotics-Domains" class="headerlink" title="Language-Conditioned Change-point Detection to Identify Sub-Tasks in Robotics Domains"></a>Language-Conditioned Change-point Detection to Identify Sub-Tasks in Robotics Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00743">http://arxiv.org/abs/2309.00743</a></li>
<li>repo_url: None</li>
<li>paper_authors: Divyanshu Raj, Chitta Baral, Nakul Gopalan</li>
<li>for: 本研究目标是通过语言指令确定机器人行走路径中的子任务。</li>
<li>methods: 我们使用语言提供的指导来确定语言指令中的子任务，并将这些子任务映射到机器人行走路径中的子段。</li>
<li>results: 我们的方法可以准确地确定机器人行走路径中的子任务，并且与基eline方法相比，我们的方法可以提高$1.78_{\pm 0.82}%$的准确率。<details>
<summary>Abstract</summary>
In this work, we present an approach to identify sub-tasks within a demonstrated robot trajectory using language instructions. We identify these sub-tasks using language provided during demonstrations as guidance to identify sub-segments of a longer robot trajectory. Given a sequence of natural language instructions and a long trajectory consisting of image frames and discrete actions, we want to map an instruction to a smaller fragment of the trajectory. Unlike previous instruction following works which directly learn the mapping from language to a policy, we propose a language-conditioned change-point detection method to identify sub-tasks in a problem. Our approach learns the relationship between constituent segments of a long language command and corresponding constituent segments of a trajectory. These constituent trajectory segments can be used to learn subtasks or sub-goals for planning or options as demonstrated by previous related work. Our insight in this work is that the language-conditioned robot change-point detection problem is similar to the existing video moment retrieval works used to identify sub-segments within online videos. Through extensive experimentation, we demonstrate a $1.78_{\pm 0.82}\%$ improvement over a baseline approach in accurately identifying sub-tasks within a trajectory using our proposed method. Moreover, we present a comprehensive study investigating sample complexity requirements on learning this mapping, between language and trajectory sub-segments, to understand if the video retrieval-based methods are realistic in real robot scenarios.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们提出了一种方法，用于在人工智能机器人路径示例中标识子任务。我们使用示例中提供的语言作为指导，以标识路径中的子段。给定一个自然语言指令序列和一个包含图像帧和精确动作的长路径，我们想要将指令映射到更短的路径段。不同于前一些语言指令跟踪工作，我们提议使用语言条件变化点检测方法来标识子任务。我们的方法学习了语言命令中的各个段落和路径中的各个段落之间的关系。这些路径段可以用于学习子任务或子目标 для规划或选择。我们的发现是，语言条件变化点检测问题与现有在线视频中的分割问题类似。经过广泛的实验，我们表明了使用我们提议的方法可以与基准方法相比提高$1.78\pm0.82\%$的精度。此外，我们还进行了全面的研究，以了解学习这种映射的样本复杂度要求，以确定视频分割方法在真实的机器人场景中是否可行。
</details></li>
</ul>
<hr>
<h2 id="Contextual-Biasing-of-Named-Entities-with-Large-Language-Models"><a href="#Contextual-Biasing-of-Named-Entities-with-Large-Language-Models" class="headerlink" title="Contextual Biasing of Named-Entities with Large Language Models"></a>Contextual Biasing of Named-Entities with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00723">http://arxiv.org/abs/2309.00723</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuanneng Sun, Zeeshan Ahmed, Yingyi Ma, Zhe Liu, Yutong Pang, Ozlem Kalinli</li>
<li>for: 这paper研究使用大型自然语言模型（LLM）进行语音识别（ASR）的上下文化偏误。</li>
<li>methods:  authors提出了一种不需要微调的提示方法，使用提示列表和少量示例来提供额外信息，以提高ASR性能。同时，他们还提出了多任务训练方法，使LLM预测实体类和下一个token。为了提高效率和避免LLM的最长序列长度限制，authors提出了动态提示方法，选择最有可能性的类，并只使用这个类中的Entity作为下一个token预测的Context。</li>
<li>results:  results表明，提示列表和少量示例可以相对于首轮ASR提高17.8%和9.6%，而多任务训练和动态提示可以相对于首轮ASR提高20.0%和11.3%的WER。<details>
<summary>Abstract</summary>
This paper studies contextual biasing with Large Language Models (LLMs), where during second-pass rescoring additional contextual information is provided to a LLM to boost Automatic Speech Recognition (ASR) performance. We propose to leverage prompts for a LLM without fine tuning during rescoring which incorporate a biasing list and few-shot examples to serve as additional information when calculating the score for the hypothesis. In addition to few-shot prompt learning, we propose multi-task training of the LLM to predict both the entity class and the next token. To improve the efficiency for contextual biasing and to avoid exceeding LLMs' maximum sequence lengths, we propose dynamic prompting, where we select the most likely class using the class tag prediction, and only use entities in this class as contexts for next token prediction. Word Error Rate (WER) evaluation is performed on i) an internal calling, messaging, and dictation dataset, and ii) the SLUE-Voxpopuli dataset. Results indicate that biasing lists and few-shot examples can achieve 17.8% and 9.6% relative improvement compared to first pass ASR, and that multi-task training and dynamic prompting can achieve 20.0% and 11.3% relative WER improvement, respectively.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Amortizing-Pragmatic-Program-Synthesis-with-Rankings"><a href="#Amortizing-Pragmatic-Program-Synthesis-with-Rankings" class="headerlink" title="Amortizing Pragmatic Program Synthesis with Rankings"></a>Amortizing Pragmatic Program Synthesis with Rankings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03225">http://arxiv.org/abs/2309.03225</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/evanthebouncy/pragmatic_synthesis_ranking">https://github.com/evanthebouncy/pragmatic_synthesis_ranking</a></li>
<li>paper_authors: Yewen Pu, Saujas Vaduguru, Priyan Vaithilingam, Elena Glassman, Daniel Fried</li>
<li>for: 该论文旨在提高程序生成器的效率，使其能够应用于更多的领域。</li>
<li>methods: 该论文使用了 rational speech acts（RSA）框架，并开发了一种全局 Pragmatic 排名方法，以减轻 RSA 算法的计算负担。</li>
<li>results: 实验结果表明，使用全局 Pragmatic 排名方法可以大幅提高程序生成器的效率，并在多个示例下与非 Pragmatic  synthesizer 相比，表现更优异。<details>
<summary>Abstract</summary>
In program synthesis, an intelligent system takes in a set of user-generated examples and returns a program that is logically consistent with these examples. The usage of Rational Speech Acts (RSA) framework has been successful in building \emph{pragmatic} program synthesizers that return programs which -- in addition to being logically consistent -- account for the fact that a user chooses their examples informatively. However, the computational burden of running the RSA algorithm has restricted the application of pragmatic program synthesis to domains with a small number of possible programs. This work presents a novel method of amortizing the RSA algorithm by leveraging a \emph{global pragmatic ranking} -- a single, total ordering of all the hypotheses. We prove that for a pragmatic synthesizer that uses a single demonstration, our global ranking method exactly replicates RSA's ranked responses. We further empirically show that global rankings effectively approximate the full pragmatic synthesizer in an online, multi-demonstration setting. Experiments on two program synthesis domains using our pragmatic ranking method resulted in orders of magnitudes of speed ups compared to the RSA synthesizer, while outperforming the standard, non-pragmatic synthesizer.
</details>
<details>
<summary>摘要</summary>
在程序生成中，一个智能系统会接受用户生成的示例集并返回一个符合这些示例的程序。使用 rational speech acts（RSA）框架已经成功地建立了 Pragmatic 程序生成器，这些程序不仅需要符合逻辑上的一致，还需要考虑用户选择示例的信息性。然而，运行 RSA 算法的计算负担限制了 Pragmatic 程序生成的应用领域的规模。这项工作提出了一种归一化 RSA 算法的方法，通过利用全局的 Pragmatic 排名来实现。我们证明，在单个示例下，我们的全球排名方法可以准确复制 RSA 排名的答案。我们进一步验证了全球排名在在线、多示例的 Setting 下能够有效地逼近整个 Pragmatic 生成器。在两个程序生成领域中，使用我们的 Pragmatic 排名方法，比对 RSA 生成器和标准、非 Pragmatic 生成器，实现了一个数量级的速度提升，同时表现更高。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Learning-with-Human-Feedback-for-Realistic-Traffic-Simulation"><a href="#Reinforcement-Learning-with-Human-Feedback-for-Realistic-Traffic-Simulation" class="headerlink" title="Reinforcement Learning with Human Feedback for Realistic Traffic Simulation"></a>Reinforcement Learning with Human Feedback for Realistic Traffic Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00709">http://arxiv.org/abs/2309.00709</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yulong Cao, Boris Ivanovic, Chaowei Xiao, Marco Pavone</li>
<li>for:  This paper aims to enhance the realism of existing traffic models for autonomous vehicle development by incorporating human preferences through reinforcement learning.</li>
<li>methods: The proposed framework, called TrafficRLHF, uses human feedback for alignment and employs reinforcement learning with human preference to generate realistic traffic scenarios.</li>
<li>results: The framework demonstrates its proficiency in generating traffic scenarios that are well-aligned with human preferences, as corroborated by comprehensive evaluations on the nuScenes dataset.<details>
<summary>Abstract</summary>
In light of the challenges and costs of real-world testing, autonomous vehicle developers often rely on testing in simulation for the creation of reliable systems. A key element of effective simulation is the incorporation of realistic traffic models that align with human knowledge, an aspect that has proven challenging due to the need to balance realism and diversity. This works aims to address this by developing a framework that employs reinforcement learning with human preference (RLHF) to enhance the realism of existing traffic models. This study also identifies two main challenges: capturing the nuances of human preferences on realism and the unification of diverse traffic simulation models. To tackle these issues, we propose using human feedback for alignment and employ RLHF due to its sample efficiency. We also introduce the first dataset for realism alignment in traffic modeling to support such research. Our framework, named TrafficRLHF, demonstrates its proficiency in generating realistic traffic scenarios that are well-aligned with human preferences, as corroborated by comprehensive evaluations on the nuScenes dataset.
</details>
<details>
<summary>摘要</summary>
“为了Addressing the challenges and costs of real-world testing, autonomous vehicle developers often rely on simulation testing for the creation of reliable systems. A key element of effective simulation is the incorporation of realistic traffic models that align with human knowledge, an aspect that has proven challenging due to the need to balance realism and diversity. This study aims to address this by developing a framework that employs reinforcement learning with human preference (RLHF) to enhance the realism of existing traffic models. This study also identifies two main challenges: capturing the nuances of human preferences on realism and the unification of diverse traffic simulation models. To tackle these issues, we propose using human feedback for alignment and employ RLHF due to its sample efficiency. We also introduce the first dataset for realism alignment in traffic modeling to support such research. Our framework, named TrafficRLHF, demonstrates its proficiency in generating realistic traffic scenarios that are well-aligned with human preferences, as corroborated by comprehensive evaluations on the nuScenes dataset.”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other parts of the world. Traditional Chinese is also an option, but it is less commonly used in mainland China.
</details></li>
</ul>
<hr>
<h2 id="Geometric-Deep-Learning-a-Temperature-Based-Analysis-of-Graph-Neural-Networks"><a href="#Geometric-Deep-Learning-a-Temperature-Based-Analysis-of-Graph-Neural-Networks" class="headerlink" title="Geometric Deep Learning: a Temperature Based Analysis of Graph Neural Networks"></a>Geometric Deep Learning: a Temperature Based Analysis of Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00699">http://arxiv.org/abs/2309.00699</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. Lapenna, F. Faglioni, F. Zanchetta, R. Fioresi</li>
<li>for: 这个研究使用几何深度学习模型来模拟 термо动力系统， weights 被看作为非量子和非相对论粒子。</li>
<li>methods: 研究使用过去定义的温度（参考 [7]），在不同层次上研究 GC 和 GAT 模型。</li>
<li>results: 研究结果可能有各种应用前景。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
We examine a Geometric Deep Learning model as a thermodynamic system treating the weights as non-quantum and non-relativistic particles. We employ the notion of temperature previously defined in [7] and study it in the various layers for GCN and GAT models. Potential future applications of our findings are discussed.
</details>
<details>
<summary>摘要</summary>
我们研究了一个几何深度学习模型，将参数视为非量子和非 relativistic 粒子。我们使用先前在 [7] 中定义的温度概念，研究它在不同层次上的GCN和GAT模型中。我们还讨论了未来可能的应用。Here's a breakdown of the translation:* "We examine a Geometric Deep Learning model" becomes "我们研究了一个几何深度学习模型"* "as a thermodynamic system" becomes "视为一个热力学系统"* "treating the weights as non-quantum and non-relativistic particles" becomes "将参数视为非量子和非 relativistic 粒子"* "We employ the notion of temperature previously defined in [7]" becomes "我们使用先前在 [7] 中定义的温度概念"* "and study it in the various layers" becomes "研究它在不同层次上"* "for GCN and GAT models" becomes "在GCN和GAT模型中"* "Potential future applications of our findings are discussed" becomes "我们还讨论了未来可能的应用".
</details></li>
</ul>
<hr>
<h2 id="Jointly-Exploring-Client-Drift-and-Catastrophic-Forgetting-in-Dynamic-Learning"><a href="#Jointly-Exploring-Client-Drift-and-Catastrophic-Forgetting-in-Dynamic-Learning" class="headerlink" title="Jointly Exploring Client Drift and Catastrophic Forgetting in Dynamic Learning"></a>Jointly Exploring Client Drift and Catastrophic Forgetting in Dynamic Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00688">http://arxiv.org/abs/2309.00688</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niklas Babendererde, Moritz Fuchs, Camila Gonzalez, Yuri Tolkach, Anirban Mukhopadhyay</li>
<li>for: 这篇研究旨在探讨 Federated and Continual Learning 中的 Client Drift 和 Catastrophic Forgetting 问题，并提出一个统一分析框架以测试这两种问题的相互关联性。</li>
<li>methods: 这篇研究使用了一个新的三维测试框架，可以同时考虑 Client Drift 和 Catastrophic Forgetting 的共同影响，并且可以统一分析这两种问题的共同性。</li>
<li>results: 研究发现，Client Drift 和 Catastrophic Forgetting 之间存在强联系，即当 Client Drift 发生时，Catastrophic Forgetting 也很可能发生，并且这两种问题之间存在一定的相互关联性。此外，研究还发现了一个“普遍提升”现象，即在某些混合情况下，由于 Client Drift 和 Catastrophic Forgetting 的共同影响，模型的性能可能会提高。<details>
<summary>Abstract</summary>
Federated and Continual Learning have emerged as potential paradigms for the robust and privacy-aware use of Deep Learning in dynamic environments. However, Client Drift and Catastrophic Forgetting are fundamental obstacles to guaranteeing consistent performance. Existing work only addresses these problems separately, which neglects the fact that the root cause behind both forms of performance deterioration is connected. We propose a unified analysis framework for building a controlled test environment for Client Drift -- by perturbing a defined ratio of clients -- and Catastrophic Forgetting -- by shifting all clients with a particular strength. Our framework further leverages this new combined analysis by generating a 3D landscape of the combined performance impact from both. We demonstrate that the performance drop through Client Drift, caused by a certain share of shifted clients, is correlated to the drop from Catastrophic Forgetting resulting from a corresponding shift strength. Correlation tests between both problems for Computer Vision (CelebA) and Medical Imaging (PESO) support this new perspective, with an average Pearson rank correlation coefficient of over 0.94. Our framework's novel ability of combined spatio-temporal shift analysis allows us to investigate how both forms of distribution shift behave in mixed scenarios, opening a new pathway for better generalization. We show that a combination of moderate Client Drift and Catastrophic Forgetting can even improve the performance of the resulting model (causing a "Generalization Bump") compared to when only one of the shifts occurs individually. We apply a simple and commonly used method from Continual Learning in the federated setting and observe this phenomenon to be reoccurring, leveraging the ability of our framework to analyze existing and novel methods for Federated and Continual Learning.
</details>
<details>
<summary>摘要</summary>
随着 Federated Learning 和 Continual Learning 的出现，它们被视为在动态环境中使用深度学习的可靠和隐私保护方法的潜在方法。然而，客户端漂移和快速忘记是保证持续性表现的基本障碍。现有的工作仅 addressed these problems separately，忽略了它们的根本原因是相连的。我们提出一种统一分析框架，通过对 опреде定比例的客户端进行干扰来建立 Client Drift 的控制测试环境，并通过对所有客户端进行固定强度的偏移来建立 Catastrophic Forgetting 的测试环境。我们的框架进一步利用了这种新的共同分析，生成了 Client Drift 和 Catastrophic Forgetting 的共同性表现的 3D 地图。我们示出，Client Drift 引起的表现下降和 Catastrophic Forgetting 引起的表现下降之间存在强相关关系，在 Computer Vision (CelebA) 和 Medical Imaging (PESO) 支持这一新视角，共计 Pearson 相关系数超过 0.94。我们的框架的新的共同空间偏移分析能力，使我们可以在混合enario中调查 Client Drift 和 Catastrophic Forgetting 的分布变化行为，开启了一条新的通路以实现更好的泛化。我们显示，在混合 Client Drift 和 Catastrophic Forgetting 的情况下，模型的表现可能会得到改善（引起一个 "Generalization Bump"），比单独的偏移情况下更好。我们应用了常见的 Continual Learning 方法在 federated 设置下，并观察到这种现象是重复的，利用我们的框架分析现有和新的方法的可能性。
</details></li>
</ul>
<hr>
<h2 id="Point-Bind-Point-LLM-Aligning-Point-Cloud-with-Multi-modality-for-3D-Understanding-Generation-and-Instruction-Following"><a href="#Point-Bind-Point-LLM-Aligning-Point-Cloud-with-Multi-modality-for-3D-Understanding-Generation-and-Instruction-Following" class="headerlink" title="Point-Bind &amp; Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following"></a>Point-Bind &amp; Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00615">http://arxiv.org/abs/2309.00615</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ziyuguo99/point-bind_point-llm">https://github.com/ziyuguo99/point-bind_point-llm</a></li>
<li>paper_authors: Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Yiwen Tang, Xianzheng Ma, Jiaming Han, Kexin Chen, Peng Gao, Xianzhi Li, Hongsheng Li, Pheng-Ann Heng</li>
<li>for: 这个论文旨在将3D点云与多媒体数据（图像、语音、视频）相互对应，以便实现多种应用程序，如任意到3D生成、3D嵌入数学和3D开放世界理解。</li>
<li>methods:  authors propose Point-Bind，一种3D多媒体模型，通过ImageBind建立3D和多媒体之间的共同嵌入空间，并提出Point-LLM，一种基于3D多媒体指令的首个大语言模型。</li>
<li>results: authors fine-tune pre-trained LLMs with Point-Bind’s semantics, achieving superior 3D and multi-modal question-answering performance without requiring 3D instruction data.<details>
<summary>Abstract</summary>
We introduce Point-Bind, a 3D multi-modality model aligning point clouds with 2D image, language, audio, and video. Guided by ImageBind, we construct a joint embedding space between 3D and multi-modalities, enabling many promising applications, e.g., any-to-3D generation, 3D embedding arithmetic, and 3D open-world understanding. On top of this, we further present Point-LLM, the first 3D large language model (LLM) following 3D multi-modal instructions. By parameter-efficient fine-tuning techniques, Point-LLM injects the semantics of Point-Bind into pre-trained LLMs, e.g., LLaMA, which requires no 3D instruction data, but exhibits superior 3D and multi-modal question-answering capacity. We hope our work may cast a light on the community for extending 3D point clouds to multi-modality applications. Code is available at https://github.com/ZiyuGuo99/Point-Bind_Point-LLM.
</details>
<details>
<summary>摘要</summary>
我们介绍Point-Bind，一个3D多Modal模型，可以与2D图像、语音、影音等多种多Modalities进行对齐。受ImageBind的引导，我们建立了3D和多Modalities之间的共同嵌入空间，使得许多应用程序可能实现，例如任何到3D生成、3D嵌入加法和3D开放世界理解。此外，我们还呈发Point-LLM，首个遵循3D多Modal instructions的3D大语言模型。通过实现优化技术，Point-LLM可以将Point-Bind的 semantics 注入到先天训练的LLMs中，例如LLaMA，这些模型不需要3D instruction data，但可以实现3D和多Modal question-answering的高水平表现。我们希望我们的工作能够照亮社区，将3D点云扩展到多Modal应用程序。代码可以在https://github.com/ZiyuGuo99/Point-Bind_Point-LLM中找到。
</details></li>
</ul>
<hr>
<h2 id="Iterative-Multi-granular-Image-Editing-using-Diffusion-Models"><a href="#Iterative-Multi-granular-Image-Editing-using-Diffusion-Models" class="headerlink" title="Iterative Multi-granular Image Editing using Diffusion Models"></a>Iterative Multi-granular Image Editing using Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00613">http://arxiv.org/abs/2309.00613</a></li>
<li>repo_url: None</li>
<li>paper_authors: K J Joseph, Prateksha Udhayanan, Tripti Shukla, Aishwarya Agarwal, Srikrishna Karanam, Koustava Goswami, Balaji Vasan Srinivasan</li>
<li>for: 这个论文旨在支持创意专业人员生成艺术性和趣味性的视觉资产，以及Iterative Multi-granular Editing的过程。</li>
<li>methods: 该论文提出了一种基于扩散模型的Iterative Multi-granular Image Editor（EMILIE），它可以在图像生成和修改过程中进行迭代编辑，并且可以控制图像的空间范围（全球、本地或者任何位置）。</li>
<li>results: 该论文通过对比与现有的方法进行评估，表明EMILIE可以更好地支持创意专业人员的艺术创作，并且可以提供更多的控制选项。<details>
<summary>Abstract</summary>
Recent advances in text-guided image synthesis has dramatically changed how creative professionals generate artistic and aesthetically pleasing visual assets. To fully support such creative endeavors, the process should possess the ability to: 1) iteratively edit the generations and 2) control the spatial reach of desired changes (global, local or anything in between). We formalize this pragmatic problem setting as Iterative Multi-granular Editing. While there has been substantial progress with diffusion-based models for image synthesis and editing, they are all one shot (i.e., no iterative editing capabilities) and do not naturally yield multi-granular control (i.e., covering the full spectrum of local-to-global edits). To overcome these drawbacks, we propose EMILIE: Iterative Multi-granular Image Editor. EMILIE introduces a novel latent iteration strategy, which re-purposes a pre-trained diffusion model to facilitate iterative editing. This is complemented by a gradient control operation for multi-granular control. We introduce a new benchmark dataset to evaluate our newly proposed setting. We conduct exhaustive quantitatively and qualitatively evaluation against recent state-of-the-art approaches adapted to our task, to being out the mettle of EMILIE. We hope our work would attract attention to this newly identified, pragmatic problem setting.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Curating-Naturally-Adversarial-Datasets-for-Trustworthy-AI-in-Healthcare"><a href="#Curating-Naturally-Adversarial-Datasets-for-Trustworthy-AI-in-Healthcare" class="headerlink" title="Curating Naturally Adversarial Datasets for Trustworthy AI in Healthcare"></a>Curating Naturally Adversarial Datasets for Trustworthy AI in Healthcare</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00543">http://arxiv.org/abs/2309.00543</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sydney Pugh, Ivan Ruchkin, Insup Lee, James Weimer</li>
<li>for: 本研究旨在提高深度学习模型对时间序列医疗应用中的预测精度，同时确保这些模型的可靠性和信任性。</li>
<li>methods: 本研究提出了一种使用自动生成的弱监督标签 combines 噪音和便宜获得的标签规则，以生成自然的对抗示例集，用于评估模型的可靠性。</li>
<li>results: 本研究在 six 个医学案例和三个非医学案例中，通过对输入数据进行随机排序，并使用这种排序构建一系列逐渐增强的对抗示例集，证明了该方法的可靠性和统计效果。<details>
<summary>Abstract</summary>
Deep learning models have shown promising predictive accuracy for time-series healthcare applications. However, ensuring the robustness of these models is vital for building trustworthy AI systems. Existing research predominantly focuses on robustness to synthetic adversarial examples, crafted by adding imperceptible perturbations to clean input data. However, these synthetic adversarial examples do not accurately reflect the most challenging real-world scenarios, especially in the context of healthcare data. Consequently, robustness to synthetic adversarial examples may not necessarily translate to robustness against naturally occurring adversarial examples, which is highly desirable for trustworthy AI. We propose a method to curate datasets comprised of natural adversarial examples to evaluate model robustness. The method relies on probabilistic labels obtained from automated weakly-supervised labeling that combines noisy and cheap-to-obtain labeling heuristics. Based on these labels, our method adversarially orders the input data and uses this ordering to construct a sequence of increasingly adversarial datasets. Our evaluation on six medical case studies and three non-medical case studies demonstrates the efficacy and statistical validity of our approach to generating naturally adversarial datasets
</details>
<details>
<summary>摘要</summary>
To address this issue, we propose a method to curate datasets comprised of natural adversarial examples to evaluate model robustness. Our method relies on probabilistic labels obtained from automated weakly-supervised labeling that combines noisy and cheap-to-obtain labeling heuristics. We use these labels to adversarially order the input data and construct a sequence of increasingly adversarial datasets.Our evaluation on six medical case studies and three non-medical case studies demonstrates the effectiveness and statistical validity of our approach to generating naturally adversarial datasets. By using these datasets to evaluate model robustness, we can better ensure that AI systems are trustworthy and reliable in real-world scenarios.
</details></li>
</ul>
<hr>
<h2 id="ICDARTS-Improving-the-Stability-and-Performance-of-Cyclic-DARTS"><a href="#ICDARTS-Improving-the-Stability-and-Performance-of-Cyclic-DARTS" class="headerlink" title="ICDARTS: Improving the Stability and Performance of Cyclic DARTS"></a>ICDARTS: Improving the Stability and Performance of Cyclic DARTS</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00664">http://arxiv.org/abs/2309.00664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emily Herron, Derek Rose, Steven Young</li>
<li>for: 提高循环DARTS的稳定性和通用性</li>
<li>methods: 改进CDARTS的训练协议，消除搜索网络和评估网络之间的依赖关系，并对搜索网络中的零操作进行修饰</li>
<li>results: 实现了提高网络通用性和实现了一种新的动态搜索空间 incorporation 方法，并进行了灵活搜索细致的扩展<details>
<summary>Abstract</summary>
This work introduces improvements to the stability and generalizability of Cyclic DARTS (CDARTS). CDARTS is a Differentiable Architecture Search (DARTS)-based approach to neural architecture search (NAS) that uses a cyclic feedback mechanism to train search and evaluation networks concurrently. This training protocol aims to optimize the search process by enforcing that the search and evaluation networks produce similar outputs. However, CDARTS introduces a loss function for the evaluation network that is dependent on the search network. The dissimilarity between the loss functions used by the evaluation networks during the search and retraining phases results in a search-phase evaluation network that is a sub-optimal proxy for the final evaluation network that is utilized during retraining. We present ICDARTS, a revised approach that eliminates the dependency of the evaluation network weights upon those of the search network, along with a modified process for discretizing the search network's \textit{zero} operations that allows these operations to be retained in the final evaluation networks. We pair the results of these changes with ablation studies on ICDARTS' algorithm and network template. Finally, we explore methods for expanding the search space of ICDARTS by expanding its operation set and exploring alternate methods for discretizing its continuous search cells. These experiments resulted in networks with improved generalizability and the implementation of a novel method for incorporating a dynamic search space into ICDARTS.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese translation:这个工作介绍了对循环DARTS（CDARTS）的改进，以提高其稳定性和通用性。CDARTS是基于演算 Architecture Search（DARTS）的神经网络搜索方法，使用循环反馈机制来同时训练搜索和评估网络。这种训练协议的目的是通过确保搜索和评估网络生成相似的输出来优化搜索过程。然而，CDARTS引入了评估网络的损失函数，这使得搜索阶段的评估网络成为一个临时性差的代理人。我们提出了ICDARTS，一种修改后的方法，该方法消除了搜索网络的依赖关系，并对搜索网络中的\textit{zero} 操作进行修正。我们还进行了ICDARTS算法和网络模板的ablation study。最后，我们探索了扩展ICDARTS搜索空间的方法，包括扩展其操作集和使用不同的抽象方法来抽象它的连续搜索细胞。这些实验导致了改进的通用性和一种新的方法来将动态搜索空间 incorporated 到ICDARTS中。
</details></li>
</ul>
<hr>
<h2 id="Learning-based-NLOS-Detection-and-Uncertainty-Prediction-of-GNSS-Observations-with-Transformer-Enhanced-LSTM-Network"><a href="#Learning-based-NLOS-Detection-and-Uncertainty-Prediction-of-GNSS-Observations-with-Transformer-Enhanced-LSTM-Network" class="headerlink" title="Learning-based NLOS Detection and Uncertainty Prediction of GNSS Observations with Transformer-Enhanced LSTM Network"></a>Learning-based NLOS Detection and Uncertainty Prediction of GNSS Observations with Transformer-Enhanced LSTM Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00480">http://arxiv.org/abs/2309.00480</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rwth-irt/deepnlosdetection">https://github.com/rwth-irt/deepnlosdetection</a></li>
<li>paper_authors: Haoming Zhang, Zhanxin Wang, Heike Vallery</li>
<li>for: 这个研究旨在提高运输系统中GNSS的准确性和一致性，减少GNSS观测受到多路径和非线路径（NLOS）影响的情况下，传统方法可能无法正确地分类和排除错误GNSS观测，导致系统状态估计和运输安全性问题。</li>
<li>methods: 这个研究提出了一个基于深度学习的方法，通过分析GNSS观测为空间时间模型问题，探索NLOS观测和 Pseudorange 误差的预测方法。相比之前的研究，我们将 transformer-like 注意力机制整合到深度学习网络中，提高模型性能和普遍性。</li>
<li>results: 实验研究显示，我们的网络在训练和评估过程中比其他深度学习模型和传统机器学习模型更好，并且在实际应用中避免了车辆地图分布不均的问题。此外，我们还进行了网络 ком成分析和与数据外泛统计分析，以及与其他模型的比较。<details>
<summary>Abstract</summary>
The global navigation satellite systems (GNSS) play a vital role in transport systems for accurate and consistent vehicle localization. However, GNSS observations can be distorted due to multipath effects and non-line-of-sight (NLOS) receptions in challenging environments such as urban canyons. In such cases, traditional methods to classify and exclude faulty GNSS observations may fail, leading to unreliable state estimation and unsafe system operations. This work proposes a Deep-Learning-based method to detect NLOS receptions and predict GNSS pseudorange errors by analyzing GNSS observations as a spatio-temporal modeling problem. Compared to previous works, we construct a transformer-like attention mechanism to enhance the long short-term memory (LSTM) networks, improving model performance and generalization. For the training and evaluation of the proposed network, we used labeled datasets from the cities of Hong Kong and Aachen. We also introduce a dataset generation process to label the GNSS observations using lidar maps. In experimental studies, we compare the proposed network with a deep-learning-based model and classical machine-learning models. Furthermore, we conduct ablation studies of our network components and integrate the NLOS detection with data out-of-distribution in a state estimator. As a result, our network presents improved precision and recall ratios compared to other models. Additionally, we show that the proposed method avoids trajectory divergence in real-world vehicle localization by classifying and excluding NLOS observations.
</details>
<details>
<summary>摘要</summary>
全球导航卫星系统（GNSS）在交通系统中扮演着重要的角色，对于精确和一致的车辆位置Localization提供了重要的帮助。然而，GNSS观测可能会受到多路径效应和非直线视野（NLOS）接收的干扰，特别是在城市的“峡谷”环境中。在这种情况下，传统的方法可能无法正确地分类和排除 faulty GNSS观测，导致系统的状态估计和运行不安全。本工作提出了一个基于深度学习的方法，通过分析 GNSS 观测为空间时间模型的问题，探测NLOS接收和预测 GNSS  Pseudorange 误差。相比于前一代的工作，我们将 transformer-like 注意力机制搭配长期记忆类型的LSTM 网络，提高模型的性能和通用性。我们使用了香港和阿希的城市 Labelled 数据集进行训练和评估。此外，我们还介绍了一个标签GNSS 观测的方法，使用 lidar 地图。在实验研究中，我们与其他深度学习模型和传统机器学习模型进行比较。此外，我们还进行了我们网络的组件删除和与数据外部分布的整合。最终，我们的网络获得了提高的精确性和回应率，并且显示了在实际车辆Localization中避免了轨迹分支的问题。
</details></li>
</ul>
<hr>
<h2 id="A-Theoretical-and-Practical-Framework-for-Evaluating-Uncertainty-Calibration-in-Object-Detection"><a href="#A-Theoretical-and-Practical-Framework-for-Evaluating-Uncertainty-Calibration-in-Object-Detection" class="headerlink" title="A Theoretical and Practical Framework for Evaluating Uncertainty Calibration in Object Detection"></a>A Theoretical and Practical Framework for Evaluating Uncertainty Calibration in Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00464">http://arxiv.org/abs/2309.00464</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pedrormconde/uncertainty_calibration_object_detection">https://github.com/pedrormconde/uncertainty_calibration_object_detection</a></li>
<li>paper_authors: Pedro Conde, Rui L. Lopes, Cristiano Premebida</li>
<li>for: 本研究旨在提出一个新的假设和实践框架，用于评估深度神经网络中的物体探测系统，并评估这些系统的不确定性调整。</li>
<li>methods: 本研究使用了一系列的实验和分析方法，包括实验设计、资料分析和模型评估，以评估不确定性调整的效果。</li>
<li>results: 研究结果显示，提出的不确定性调整度量具有良好的准确性和稳定性，并且可以帮助改善物体探测系统的可靠性和安全性。Here is the same information in English:</li>
<li>for: The purpose of this study is to propose a new theoretical and practical framework for evaluating object detection systems in the context of uncertainty calibration.</li>
<li>methods: The study uses a series of experimental and analytical methods, including experimental design, data analysis, and model evaluation, to assess the effectiveness of the proposed uncertainty calibration metrics.</li>
<li>results: The results show that the proposed uncertainty calibration metrics have good accuracy and stability, and can help improve the reliability and safety of object detection systems.<details>
<summary>Abstract</summary>
The proliferation of Deep Neural Networks has resulted in machine learning systems becoming increasingly more present in various real-world applications. Consequently, there is a growing demand for highly reliable models in these domains, making the problem of uncertainty calibration pivotal, when considering the future of deep learning. This is especially true when considering object detection systems, that are commonly present in safety-critical application such as autonomous driving and robotics. For this reason, this work presents a novel theoretical and practical framework to evaluate object detection systems in the context of uncertainty calibration. The robustness of the proposed uncertainty calibration metrics is shown through a series of representative experiments. Code for the proposed uncertainty calibration metrics at: https://github.com/pedrormconde/Uncertainty_Calibration_Object_Detection.
</details>
<details>
<summary>摘要</summary>
深度神经网络的普及导致机器学习系统在实际应用中变得越来越普遍。因此，高可靠性模型在这些领域的需求在增加。特别是在自动驾驶和机器人等安全关键应用中，Object detection系统的uncertainty calibration问题变得越来越重要。为此，这个研究提出了一种新的理论和实践框架，用于评估Object detection系统的uncertainty calibration。这种uncertainty calibration度量的稳定性通过一系列代表性的实验展示。Code可以在https://github.com/pedrormconde/Uncertainty_Calibration_Object_Detection中找到。
</details></li>
</ul>
<hr>
<h2 id="New-metrics-for-analyzing-continual-learners"><a href="#New-metrics-for-analyzing-continual-learners" class="headerlink" title="New metrics for analyzing continual learners"></a>New metrics for analyzing continual learners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00462">http://arxiv.org/abs/2309.00462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas Michel, Giovanni Chierchia, Romain Negrel, Jean-François Bercher, Toshihiko Yamasaki</li>
<li>for:  continual learning 学习环境中维护知识和学习新任务的稳定性和柔软性。</li>
<li>methods: 使用现有的措施来衡量稳定性和柔软性，并发现现有的指标忽略了任务增加难度的问题。因此，我们提出了新的指标来考虑任务增加难度。</li>
<li>results: 通过在标准 bencmark 数据集上进行实验，我们表明了我们提出的新指标可以为 continual learning 环境中模型的稳定性-柔软性质量提供新的视角。<details>
<summary>Abstract</summary>
Deep neural networks have shown remarkable performance when trained on independent and identically distributed data from a fixed set of classes. However, in real-world scenarios, it can be desirable to train models on a continuous stream of data where multiple classification tasks are presented sequentially. This scenario, known as Continual Learning (CL) poses challenges to standard learning algorithms which struggle to maintain knowledge of old tasks while learning new ones. This stability-plasticity dilemma remains central to CL and multiple metrics have been proposed to adequately measure stability and plasticity separately. However, none considers the increasing difficulty of the classification task, which inherently results in performance loss for any model. In that sense, we analyze some limitations of current metrics and identify the presence of setup-induced forgetting. Therefore, we propose new metrics that account for the task's increasing difficulty. Through experiments on benchmark datasets, we demonstrate that our proposed metrics can provide new insights into the stability-plasticity trade-off achieved by models in the continual learning environment.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Establishing-Markov-Equivalence-in-Cyclic-Directed-Graphs"><a href="#Establishing-Markov-Equivalence-in-Cyclic-Directed-Graphs" class="headerlink" title="Establishing Markov Equivalence in Cyclic Directed Graphs"></a>Establishing Markov Equivalence in Cyclic Directed Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03092">http://arxiv.org/abs/2309.03092</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tomc-ghub/CET_uai2023">https://github.com/tomc-ghub/CET_uai2023</a></li>
<li>paper_authors: Tom Claassen, Joris M. Mooij</li>
<li>for:  establishment of Markov equivalence between directed graphs</li>
<li>methods:  based on Cyclic Equivalence Theorem (CET) and ancestral perspective</li>
<li>results:  significantly reduced algorithmic complexity and conceptually simplified characterization, which may help to reinvigorate theoretical research towards sound and complete cyclic discovery in the presence of latent confounders.<details>
<summary>Abstract</summary>
We present a new, efficient procedure to establish Markov equivalence between directed graphs that may or may not contain cycles under the \textit{d}-separation criterion. It is based on the Cyclic Equivalence Theorem (CET) in the seminal works on cyclic models by Thomas Richardson in the mid '90s, but now rephrased from an ancestral perspective. The resulting characterization leads to a procedure for establishing Markov equivalence between graphs that no longer requires tests for d-separation, leading to a significantly reduced algorithmic complexity. The conceptually simplified characterization may help to reinvigorate theoretical research towards sound and complete cyclic discovery in the presence of latent confounders. This version includes a correction to rule (iv) in Theorem 1, and the subsequent adjustment in part 2 of Algorithm 2.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的、高效的程序，用于在导航图中确定Markov等价关系，这些图可能或可能不含循环，基于\textit{d}-分离 критериion。这种方法基于托马斯·理查森在90年代中期的著名作品中的循环等价定理（CET），但现在从先祖 perspective重新表述。这种Characterization导致了一种不需要测试\textit{d}-分离的程序，从而大幅降低了算法复杂性。这种概念简化后的Characterization可能会促进理论研究，以探索在潜在干扰因素存在下的循环发现的正确和完整的方法。这个版本包括对第一个定理（iv）的更正，以及后续的修改在算法2的第2部分。
</details></li>
</ul>
<hr>
<h2 id="No-Train-Still-Gain-Unleash-Mathematical-Reasoning-of-Large-Language-Models-with-Monte-Carlo-Tree-Search-Guided-by-Energy-Function"><a href="#No-Train-Still-Gain-Unleash-Mathematical-Reasoning-of-Large-Language-Models-with-Monte-Carlo-Tree-Search-Guided-by-Energy-Function" class="headerlink" title="No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function"></a>No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03224">http://arxiv.org/abs/2309.03224</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haotian Xu</li>
<li>for: 提高大型自然语言处理（NLP）模型的数学逻辑能力，无需额外 Fine-tuning 步骤。</li>
<li>methods: 使用 Monte Carlo Tree Search（MCTS）和轻量级能量函数来评估决策步骤，并使用噪声对比估计来估计能量函数的参数。</li>
<li>results: 通过对 GSM8k 和 AQUA-RAT 数学逻辑测试 benchmark 进行广泛的实验，显示了方法的杰出表现，无需额外 Fine-tuning 或人工反馈对适应。<details>
<summary>Abstract</summary>
Large language models (LLMs) demonstrate impressive language understanding and contextual learning abilities, making them suitable for natural language processing (NLP) tasks and complex mathematical reasoning. However, when applied to mathematical reasoning tasks, LLMs often struggle to generate correct reasoning steps and answers despite having high probabilities for the solutions. To overcome this limitation and enhance the mathematical reasoning capabilities of fine-tuned LLMs without additional fine-tuning steps, we propose a method that incorporates Monte Carlo Tree Search (MCTS) and a lightweight energy function to rank decision steps and enable immediate reaction and precise reasoning. Specifically, we re-formulate the fine-tuned LLMs into a Residual-based Energy Model (Residual-EBM) and employ noise contrastive estimation to estimate the energy function's parameters. We then utilize MCTS with the energy function as a path verifier to search the output space and evaluate the reasoning path. Through extensive experiments on two mathematical reasoning benchmarks, GSM8k and AQUA-RAT, we demonstrate the exceptional capabilities of our method, which significantly improves the pass@1 metric of the fine-tuned model without requiring additional fine-tuning or reinforcement learning with human feedback alignment.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-Speech-Representation-From-Contrastive-Token-Acoustic-Pretraining"><a href="#Learning-Speech-Representation-From-Contrastive-Token-Acoustic-Pretraining" class="headerlink" title="Learning Speech Representation From Contrastive Token-Acoustic Pretraining"></a>Learning Speech Representation From Contrastive Token-Acoustic Pretraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00424">http://arxiv.org/abs/2309.00424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chunyu Qiang, Hao Li, Yixin Tian, Ruibo Fu, Tao Wang, Longbiao Wang, Jianwu Dang</li>
<li>for: 提高 speech 生成和识别下的细腻性，例如 minimally-supervised text-to-speech (TTS)、voice conversion (VC) 和 automatic speech recognition (ASR)。</li>
<li>methods: 使用 two encoders 将 phoneme 和 speech 带入一个共同的多Modal 空间，学习连接 phoneme 和 speech 的框架级别连接。</li>
<li>results: 在 210k 个 speech 和 phoneme 文本对中训练 CTAP 模型，实现了 minimally-supervised TTS、VC 和 ASR 等下游任务。<details>
<summary>Abstract</summary>
For fine-grained generation and recognition tasks such as minimally-supervised text-to-speech (TTS), voice conversion (VC), and automatic speech recognition (ASR), the intermediate representations extracted from speech should serve as a "bridge" between text and acoustic information, containing information from both modalities. The semantic content is emphasized, while the paralinguistic information such as speaker identity and acoustic details should be de-emphasized. However, existing methods for extracting fine-grained intermediate representations from speech suffer from issues of excessive redundancy and dimension explosion. Contrastive learning is a good method for modeling intermediate representations from two modalities. However, existing contrastive learning methods in the audio field focus on extracting global descriptive information for downstream audio classification tasks, making them unsuitable for TTS, VC, and ASR tasks. To address these issues, we propose a method named "Contrastive Token-Acoustic Pretraining (CTAP)", which uses two encoders to bring phoneme and speech into a joint multimodal space, learning how to connect phoneme and speech at the frame level. The CTAP model is trained on 210k speech and phoneme text pairs, achieving minimally-supervised TTS, VC, and ASR. The proposed CTAP method offers a promising solution for fine-grained generation and recognition downstream tasks in speech processing.
</details>
<details>
<summary>摘要</summary>
为细化生成和识别任务，如无监督文本译 speech（TTS）、voice conversion（VC）和自动语音识别（ASR），则中间表示从语音中提取的应该作为两个模态之间的“桥”，含有文本和音频信息的信息。 semantic content 应该强调，而 speaker identity 和 acoustic details 则应该削弱。然而，现有的语音中间表示提取方法受到过 redundancy 和维度爆炸 的问题。 Contrastive learning 是一种好的方法 для模型中间表示，但现有的音频领域的 Contrastive learning 方法是为下游音频分类任务提取全局描述性信息，这使得它们不适用于 TTS、VC 和 ASR 任务。为解决这些问题，我们提出了一种方法，名为“ Contrastive Token-Acoustic Pretraining”（CTAP），它使用两个Encoder将 phoneme 和 speech 带入到一个共同多Modal空间，学习如何在帧级连接 phoneme 和 speech。 CTAP 模型在 210k 语音和 phoneme 文本对中训练，实现了无监督 TTS、VC 和 ASR。我们提出的 CTAP 方法为 fine-grained generation 和识别下游任务提供了一个有前途的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Declarative-Reasoning-on-Explanations-Using-Constraint-Logic-Programming"><a href="#Declarative-Reasoning-on-Explanations-Using-Constraint-Logic-Programming" class="headerlink" title="Declarative Reasoning on Explanations Using Constraint Logic Programming"></a>Declarative Reasoning on Explanations Using Constraint Logic Programming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00422">http://arxiv.org/abs/2309.00422</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lstate/reasonx">https://github.com/lstate/reasonx</a></li>
<li>paper_authors: Laura State, Salvatore Ruggieri, Franco Turini</li>
<li>For: 提供对透明化机器学习模型的解释，即现有的AI解释方法存在许多缺点，如背景知识不够 incorporation、解释方法不够抽象和用户交互不够。* Methods: 使用 Constraint Logic Programming (CLP) 提供声明性的、交互式的解释方法，可以用于对决策树进行解释，以及对任何黑盒模型的全局&#x2F;本地代理模型进行解释。* Results: 提供了 REASONX 解释方法的架构，包括 Python 层和 CLP 层，核心执行引擎是一个基于 Prolog 的 meta-程序，具有声明性的逻辑理论 semantics。<details>
<summary>Abstract</summary>
Explaining opaque Machine Learning (ML) models is an increasingly relevant problem. Current explanation in AI (XAI) methods suffer several shortcomings, among others an insufficient incorporation of background knowledge, and a lack of abstraction and interactivity with the user. We propose REASONX, an explanation method based on Constraint Logic Programming (CLP). REASONX can provide declarative, interactive explanations for decision trees, which can be the ML models under analysis or global/local surrogate models of any black-box model. Users can express background or common sense knowledge using linear constraints and MILP optimization over features of factual and contrastive instances, and interact with the answer constraints at different levels of abstraction through constraint projection. We present here the architecture of REASONX, which consists of a Python layer, closer to the user, and a CLP layer. REASONX's core execution engine is a Prolog meta-program with declarative semantics in terms of logic theories.
</details>
<details>
<summary>摘要</summary>
explainable machine learning (ml) models 是一个日益重要的问题。当前的 AI (XAI) 方法存在多个缺点，包括知识背景的不充分 integrate 和用户交互的缺失。我们提议了 REASONX，一种基于幂逻Programming (CLP) 的解释方法。REASONX 可以为决策树提供声明性的、交互式的解释，这些决策树可以是分析或全局/本地的黑盒模型。用户可以通过Linear Constraints 和 MILP 优化来表达背景知识或通用常识，并通过约束投影在不同层次进行交互。我们在这里介绍了 REASONX 的架构，它由 Python 层和 CLP 层组成。REASONX 的核心执行引擎是一个 Prolog 元程序，其semantics 是逻辑理论的声明性。
</details></li>
</ul>
<hr>
<h2 id="Area-norm-COBRA-on-Conditional-Survival-Prediction"><a href="#Area-norm-COBRA-on-Conditional-Survival-Prediction" class="headerlink" title="Area-norm COBRA on Conditional Survival Prediction"></a>Area-norm COBRA on Conditional Survival Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00417">http://arxiv.org/abs/2309.00417</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rahul Goswami, Arabin Kr. Dey</li>
<li>for: 这篇论文探讨了一种新的 combinational regression 策略，用于计算condition survival function。</li>
<li>methods: 该策略使用回归基于weak learner的ensemble技术，并使用距离度量作为两个生存曲线之间的区域。</li>
<li>results: 该模型比Random Survival Forest表现更好，并提供了一种选择最重要变量的新技术。 simulation study 表明该方法能够很好地确定变量的重要性。<details>
<summary>Abstract</summary>
The paper explores a different variation of combined regression strategy to calculate the conditional survival function. We use regression based weak learners to create the proposed ensemble technique. The proposed combined regression strategy uses proximity measure as area between two survival curves. The proposed model shows a construction which ensures that it performs better than the Random Survival Forest. The paper discusses a novel technique to select the most important variable in the combined regression setup. We perform a simulation study to show that our proposition for finding relevance of the variables works quite well. We also use three real-life datasets to illustrate the model.
</details>
<details>
<summary>摘要</summary>
文章探讨了一种不同的combined regression策略来计算 conditional survival function。我们使用回归基于弱学习器的 ensemble技术来实现提案。提案的combined regression策略使用距离度量来度量两个生存曲线之间的区域。提案的模型能确保其在Random Survival Forest之上表现更好。文章介绍了一种新的方法来在combined regression中选择最重要的变量。我们通过实验研究表明我们的提案可以很好地确定变量的相关性。我们还使用了三个真实数据集来示例化模型。Here's the word-for-word translation:文章探讨了一种不同的combined regression策略来计算 conditional survival function。我们使用回归基于弱学习器的 ensemble技术来实现提案。提案的combined regression策略使用距离度量来度量两个生存曲线之间的区域。提案的模型能确保其在Random Survival Forest之上表现更好。文章介绍了一种新的方法来在combined regression中选择最重要的变量。我们通过实验研究表明我们的提案可以很好地确定变量的相关性。我们还使用了三个真实数据集来示例化模型。
</details></li>
</ul>
<hr>
<h2 id="Dense-Voxel-3D-Reconstruction-Using-a-Monocular-Event-Camera"><a href="#Dense-Voxel-3D-Reconstruction-Using-a-Monocular-Event-Camera" class="headerlink" title="Dense Voxel 3D Reconstruction Using a Monocular Event Camera"></a>Dense Voxel 3D Reconstruction Using a Monocular Event Camera</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00385">http://arxiv.org/abs/2309.00385</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haodong Chen, Vera Chung, Li Tan, Xiaoming Chen</li>
<li>For: 这个论文主要用于探讨使用单个事件摄像机实现高精度3D重建，以便在虚拟现实应用中使用。* Methods: 该论文提出了一种新的方法，使用单个事件摄像机来生成高精度3D重建。这种方法不需要多个摄像机，也不需要与其他方法组合使用。* Results: 据作者的预liminary结果表明，该方法可以直接生成可辨别的高精度3D重建结果，无需创建如先前方法一样的管道。此外，作者还创建了一个 Synthetic dataset，包含39739个对象扫描结果，这个dataset可以帮助加速相关领域的研究。<details>
<summary>Abstract</summary>
Event cameras are sensors inspired by biological systems that specialize in capturing changes in brightness. These emerging cameras offer many advantages over conventional frame-based cameras, including high dynamic range, high frame rates, and extremely low power consumption. Due to these advantages, event cameras have increasingly been adapted in various fields, such as frame interpolation, semantic segmentation, odometry, and SLAM. However, their application in 3D reconstruction for VR applications is underexplored. Previous methods in this field mainly focused on 3D reconstruction through depth map estimation. Methods that produce dense 3D reconstruction generally require multiple cameras, while methods that utilize a single event camera can only produce a semi-dense result. Other single-camera methods that can produce dense 3D reconstruction rely on creating a pipeline that either incorporates the aforementioned methods or other existing Structure from Motion (SfM) or Multi-view Stereo (MVS) methods. In this paper, we propose a novel approach for solving dense 3D reconstruction using only a single event camera. To the best of our knowledge, our work is the first attempt in this regard. Our preliminary results demonstrate that the proposed method can produce visually distinguishable dense 3D reconstructions directly without requiring pipelines like those used by existing methods. Additionally, we have created a synthetic dataset with $39,739$ object scans using an event camera simulator. This dataset will help accelerate other relevant research in this field.
</details>
<details>
<summary>摘要</summary>
Event 摄像机是基于生物系统的感知器，专门用于测量光度变化。这些新型摄像机具有较高的动态范围、快速帧率和非常低的功耗消耗。由于这些优点，event 摄像机在不同领域得到了广泛应用，如 frame interpolation、semantic segmentation、odometry 和 SLAM。然而，它们在虚拟现实应用中的3D重建仍然受到了不足的研究。先前的方法主要通过depth map estimation来实现3D重建。这些方法通常需要多个摄像机，而使用单个 event 摄像机可以生成半密集的结果。其他使用单个摄像机实现密集3D重建的方法通常需要创建一个管道，该管道可以包括以上方法或其他现有的Structure from Motion（SfM）或Multi-view Stereo（MVS）方法。在这篇论文中，我们提出了一种新的方法，用于使用单个 event 摄像机实现密集3D重建。我们认为，这是首次尝试。我们的初步结果表明，我们的方法可以直接生成可辨识的密集3D重建，无需创建管道类似于现有方法。此外，我们创建了一个Synthetic dataset，包含39739个物体扫描结果，使用事件摄像机模拟器。这个数据集将会促进相关的研究。
</details></li>
</ul>
<hr>
<h2 id="Scenario-based-model-predictive-control-of-water-reservoir-systems"><a href="#Scenario-based-model-predictive-control-of-water-reservoir-systems" class="headerlink" title="Scenario-based model predictive control of water reservoir systems"></a>Scenario-based model predictive control of water reservoir systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00373">http://arxiv.org/abs/2309.00373</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raffaele Giuseppe Cestari, Andrea Castelletti, Simone Formentin</li>
<li>for:  optimize the operation of water reservoir systems in the presence of highly uncertain inflows</li>
<li>methods:  stochastic MPC approach using plausible future inflows directly generated from past data</li>
<li>results:  more cautious control that counteracts droughty periods while satisfying agricultural water demand, validated through extensive Monte Carlo tests using actual inflow data from Lake Como, Italy.Here’s the Chinese translation of the three points:</li>
<li>for:  optimizes 水库系统的运行，面临高度不确定的流入</li>
<li>methods: 使用可能性分布来生成直接来自过去数据的未来流入，以实现随机MPC策略</li>
<li>results: 更谨慎的控制，能够避免干旱期（例如湖水水平下降到干旱限制），同时保证农业水需求的满足，通过实际各个流入数据进行了 Monte Carlo 测试。<details>
<summary>Abstract</summary>
The optimal operation of water reservoir systems is a challenging task involving multiple conflicting objectives. The main source of complexity is the presence of the water inflow, which acts as an exogenous, highly uncertain disturbance on the system. When model predictive control (MPC) is employed, the optimal water release is usually computed based on the (predicted) trajectory of the inflow. This choice may jeopardize the closed-loop performance when the actual inflow differs from its forecast. In this work, we consider - for the first time - a stochastic MPC approach for water reservoirs, in which the control is optimized based on a set of plausible future inflows directly generated from past data. Such a scenario-based MPC strategy allows the controller to be more cautious, counteracting droughty periods (e.g., the lake level going below the dry limit) while at the same time guaranteeing that the agricultural water demand is satisfied. The method's effectiveness is validated through extensive Monte Carlo tests using actual inflow data from Lake Como, Italy.
</details>
<details>
<summary>摘要</summary>
水库系统的优化操作是一项复杂的任务，涉及多个 conflicting 目标。主要的复杂性来源于流水入库，它会对系统作为外生、高度不确定的干扰。当使用模型预测控制（MPC）时，通常基于预测流水入库轨迹来计算优化的水release。这可能会在实际入库与预测入库不同时影响closed-loop性。在这种工作中，我们对水库系统进行了第一次 Stochastic MPC 方法，在这种方法中，控制器是基于过去数据直接生成的可能性 Distribution 来优化控制。这种enario-based MPC策略使得控制器更加谨慎，可以避免干旱期间（例如湖水水位低于干旱限制）的问题，同时保证农业用水需求得到满足。我们通过使用意大湖 Como, 意大的实际入库数据进行了广泛的 Monte Carlo 测试，证明了该方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Discrete-Versus-Continuous-Algorithms-in-Dynamics-of-Affective-Decision-Making"><a href="#Discrete-Versus-Continuous-Algorithms-in-Dynamics-of-Affective-Decision-Making" class="headerlink" title="Discrete Versus Continuous Algorithms in Dynamics of Affective Decision Making"></a>Discrete Versus Continuous Algorithms in Dynamics of Affective Decision Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00357">http://arxiv.org/abs/2309.00357</a></li>
<li>repo_url: None</li>
<li>paper_authors: V. I. Yukalov, E. P. Yukalova<br>for: 这 paper 是研究智能网络中代理人的决策行为，以及不同类型的内存（长期和短期内存）对决策的影响。methods: 这 paper 使用概率情感决策理论，考虑了选择方案的理性利好和情感吸引力。results: 研究发现，由于网络参数的不同，可能存在较Close或较大差异的特征概率行为，这意味着使用不同的算法可能会导致非常不同的理论预测，从而无法Uniquely describe practical problems。<details>
<summary>Abstract</summary>
The dynamics of affective decision making is considered for an intelligent network composed of agents with different types of memory: long-term and short-term memory. The consideration is based on probabilistic affective decision theory, which takes into account the rational utility of alternatives as well as the emotional alternative attractiveness. The objective of this paper is the comparison of two multistep operational algorithms of the intelligent network: one based on discrete dynamics and the other on continuous dynamics. By means of numerical analysis, it is shown that, depending on the network parameters, the characteristic probabilities for continuous and discrete operations can exhibit either close or drastically different behavior. Thus, depending on which algorithm is employed, either discrete or continuous, theoretical predictions can be rather different, which does not allow for a uniquely defined description of practical problems. This finding is important for understanding which of the algorithms is more appropriate for the correct analysis of decision-making tasks. A discussion is given, revealing that the discrete operation seems to be more realistic for describing intelligent networks as well as affective artificial intelligence.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "The dynamics of affective decision making is considered for an intelligent network composed of agents with different types of memory: long-term and short-term memory. The consideration is based on probabilistic affective decision theory, which takes into account the rational utility of alternatives as well as the emotional alternative attractiveness. The objective of this paper is the comparison of two multistep operational algorithms of the intelligent network: one based on discrete dynamics and the other on continuous dynamics. By means of numerical analysis, it is shown that, depending on the network parameters, the characteristic probabilities for continuous and discrete operations can exhibit either close or drastically different behavior. Thus, depending on which algorithm is employed, either discrete or continuous, theoretical predictions can be rather different, which does not allow for a uniquely defined description of practical problems. This finding is important for understanding which of the algorithms is more appropriate for the correct analysis of decision-making tasks. A discussion is given, revealing that the discrete operation seems to be more realistic for describing intelligent networks as well as affective artificial intelligence."Translation:<<SYS>>affective决策动力学在智能网络中被考虑，智能网络由不同类型的记忆 agent组成：长期记忆和短期记忆。考虑基于概率性的情感决策理论，该理论考虑了决策选项的合理利益以及决策选项的情感吸引力。本文的目标是比较两种多步操作算法：一种基于离散动力学，另一种基于连续动力学。通过数值分析，我们发现，具有不同网络参数时，离散和连续操作的特征概率可能会展现出非常不同的行为。因此，使用不同的算法，对于实际问题的理论预测可能会非常不同，这不允许固定的描述实际问题。这一发现对于理解哪种算法更适合正确分析决策任务非常重要。文章还进行了讨论，表明离散操作更加真实地描述智能网络以及情感人工智能。
</details></li>
</ul>
<hr>
<h2 id="Explainable-Active-Learning-for-Preference-Elicitation"><a href="#Explainable-Active-Learning-for-Preference-Elicitation" class="headerlink" title="Explainable Active Learning for Preference Elicitation"></a>Explainable Active Learning for Preference Elicitation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00356">http://arxiv.org/abs/2309.00356</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/furkancanturk/explainable_active_learning">https://github.com/furkancanturk/explainable_active_learning</a></li>
<li>paper_authors: Furkan Cantürk, Reyhan Aydoğan<br>for: 这篇论文的目的是解决新用户的偏好预测问题，特别是在冷开始问题下，当推荐系统缺乏用户存在或者其他用户数据存在限制，使得使用用户资料建立用户Profile几乎不可能。methods: 这篇论文使用了活动学习（AL）来解决冷开始问题，通过选择大量未标的数据，请 oracle 标注它们，并更新机器学习（ML）模型。论文还结合了不监controlled、半监controlled和监controlled ML的混合过程，并与用户反馈组合使用。results: 实验结果显示，提案的偏好探索方法在有限用户标注数据下可以实现高效的偏好预测，同时也能够提高用户信任度 durch 精准的解释。<details>
<summary>Abstract</summary>
Gaining insights into the preferences of new users and subsequently personalizing recommendations necessitate managing user interactions intelligently, namely, posing pertinent questions to elicit valuable information effectively. In this study, our focus is on a specific scenario of the cold-start problem, where the recommendation system lacks adequate user presence or access to other users' data is restricted, obstructing employing user profiling methods utilizing existing data in the system. We employ Active Learning (AL) to solve the addressed problem with the objective of maximizing information acquisition with minimal user effort. AL operates for selecting informative data from a large unlabeled set to inquire an oracle to label them and eventually updating a machine learning (ML) model. We operate AL in an integrated process of unsupervised, semi-supervised, and supervised ML within an explanatory preference elicitation process. It harvests user feedback (given for the system's explanations on the presented items) over informative samples to update an underlying ML model estimating user preferences. The designed user interaction facilitates personalizing the system by incorporating user feedback into the ML model and also enhances user trust by refining the system's explanations on recommendations. We implement the proposed preference elicitation methodology for food recommendation. We conducted human experiments to assess its efficacy in the short term and also experimented with several AL strategies over synthetic user profiles that we created for two food datasets, aiming for long-term performance analysis. The experimental results demonstrate the efficiency of the proposed preference elicitation with limited user-labeled data while also enhancing user trust through accurate explanations.
</details>
<details>
<summary>摘要</summary>
为了获得新用户的偏好情况和个性化推荐，需要智能地管理用户互动，即向用户提问有价值信息以获得有效反馈。在这个研究中，我们关注了冷启动问题的特定场景，其中推荐系统缺乏用户存在或其他用户数据访问被限制，使用用户 profiling 方法使用现有系统数据 becomes impossible. 我们使用活动学习（AL）解决这个问题，以达到最大化信息收集的目的，同时减少用户努力。AL 方法从大量未标记数据集中选择有用信息，并请 oracle 标记它们，以更新机器学习（ML）模型。我们在混合式、半结构化和结构化 ML 中运行 AL，并在用户反馈（对系统的解释中提供的Feedback）上更新下面 ML 模型，以估计用户偏好。这种设计的用户互动方式可以个性化系统，并且提高用户信任度，因为它可以在推荐中更加准确地解释用户选择。我们在美食推荐领域实现了这种偏好抽取方法。我们对短期效果进行了人类实验，以及使用了多种 AL 策略对两个食品数据集进行了长期性能分析。实验结果表明，我们的偏好抽取方法在有限用户标注数据下可以 дости到高效性，同时提高用户信任度。
</details></li>
</ul>
<hr>
<h2 id="A-Text-based-Approach-For-Link-Prediction-on-Wikipedia-Articles"><a href="#A-Text-based-Approach-For-Link-Prediction-on-Wikipedia-Articles" class="headerlink" title="A Text-based Approach For Link Prediction on Wikipedia Articles"></a>A Text-based Approach For Link Prediction on Wikipedia Articles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00317">http://arxiv.org/abs/2309.00317</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tam1032/dsaa2023-challenge-link-prediction-ds-uit_sat">https://github.com/tam1032/dsaa2023-challenge-link-prediction-ds-uit_sat</a></li>
<li>paper_authors: Anh Hoang Tran, Tam Minh Nguyen, Son T. Luu</li>
<li>for: 这篇论文是为了参加 DSAA 2023 挑战，用于预测 Wikipedia 文章中的连结是否存在。</li>
<li>methods: 本文使用传统机器学习模型，使用文本中的 POS 标签特征进行训练分类模型。</li>
<li>results: 本文获得 F1 得分 0.99999，在竞赛中排名第 7 名。并且提供了可公开使用的源代码：<a target="_blank" rel="noopener" href="https://github.com/Tam1032/DSAA2023-Challenge-Link-prediction-DS-UIT_SAT%E3%80%82">https://github.com/Tam1032/DSAA2023-Challenge-Link-prediction-DS-UIT_SAT。</a><details>
<summary>Abstract</summary>
This paper present our work in the DSAA 2023 Challenge about Link Prediction for Wikipedia Articles. We use traditional machine learning models with POS tags (part-of-speech tags) features extracted from text to train the classification model for predicting whether two nodes has the link. Then, we use these tags to test on various machine learning models. We obtained the results by F1 score at 0.99999 and got 7th place in the competition. Our source code is publicly available at this link: https://github.com/Tam1032/DSAA2023-Challenge-Link-prediction-DS-UIT_SAT
</details>
<details>
<summary>摘要</summary>
这篇论文介绍我们在 DSAA 2023 挑战中对维基百科文章链接预测的工作。我们使用传统机器学习模型，使用文本中提取的 POS 标签特征来训练分类模型，以预测两个节点是否有链接。然后，我们使用这些标签来测试不同的机器学习模型。我们获得的结果是 F1 分数为 0.99999，在比赛中获得第 7 名。我们的源代码可以在以下链接中下载：https://github.com/Tam1032/DSAA2023-Challenge-Link-prediction-DS-UIT_SAT。
</details></li>
</ul>
<hr>
<h2 id="Sherlock-Holmes-Doesn’t-Play-Dice-The-significance-of-Evidence-Theory-for-the-Social-and-Life-Sciences"><a href="#Sherlock-Holmes-Doesn’t-Play-Dice-The-significance-of-Evidence-Theory-for-the-Social-and-Life-Sciences" class="headerlink" title="Sherlock Holmes Doesn’t Play Dice: The significance of Evidence Theory for the Social and Life Sciences"></a>Sherlock Holmes Doesn’t Play Dice: The significance of Evidence Theory for the Social and Life Sciences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03222">http://arxiv.org/abs/2309.03222</a></li>
<li>repo_url: None</li>
<li>paper_authors: V. L. Raju Chinthalapati, Guido Fioretti</li>
<li>for: 本文主要探讨了证据理论在社会和生物科学中的潜在应用，以及它与概率论的区别。</li>
<li>methods: 本文使用了德мпстер-沙法尔理论和信念函数理论来表达对事件的不确定性。</li>
<li>results: 本文证明了德мпстер-沙法尔的组合规则与 bayes 定理之间存在关系，并讨论了如何通过证据理论增强信息理论中的应用。 I hope that helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
While Evidence Theory (Demster-Shafer Theory, Belief Functions Theory) is being increasingly used in data fusion, its potentialities in the Social and Life Sciences are often obscured by lack of awareness of its distinctive features. With this paper we stress that Evidence Theory can express the uncertainty deriving from the fear that events may materialize, that one has not been able to figure out. By contrast, Probability Theory must limit itself to the possibilities that a decision-maker is currently envisaging.   Subsequently, we illustrate how Dempster-Shafer's combination rule relates to Bayes' Theorem for various versions of Probability Theory and discuss which applications of Information Theory can be enhanced by Evidence Theory. Finally, we illustrate our claims with an example where Evidence Theory is used to make sense of the partially overlapping, partially contradictory solutions that appear in an auditing exercise.
</details>
<details>
<summary>摘要</summary>
“证据理论（德赫-沙佛理论，信念函数理论）在数据融合中日益受到应用，但它在社会和生活科学中的潜力往往被不了了之。本文强调证据理论可以表达因事件可能实现而导致的不确定性，而probability理论只能限制在决策者目前所看到的可能性上。后续，我们详细介绍德赫-沙佛组合规则与 bayes定理之间的关系，并讨论在信息理论中哪些应用可以增强使用证据理论。最后，我们通过一个例子说明证据理论如何用于理解审计实践中的部分重叠、部分矛盾的解决方案。”Note: "Simplified Chinese" is also known as "Mandarin" or "Standard Chinese".
</details></li>
</ul>
<hr>
<h2 id="On-the-Aggregation-of-Rules-for-Knowledge-Graph-Completion"><a href="#On-the-Aggregation-of-Rules-for-Knowledge-Graph-Completion" class="headerlink" title="On the Aggregation of Rules for Knowledge Graph Completion"></a>On the Aggregation of Rules for Knowledge Graph Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00306">http://arxiv.org/abs/2309.00306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patrick Betz, Stefan Lüdtke, Christian Meilicke, Heiner Stuckenschmidt</li>
<li>for: 本研究旨在提高知识图完成任务中的规则学取得效率、可读性和竞争力。</li>
<li>methods: 本文使用数据驱动的规则学学习方法，并 investigate 规则集中的噪音和规则集大小的问题。</li>
<li>results: 本文提出了一种新的规则汇总策略，并证明了这种策略可以表示为规则集中的 marginal inference 操作。此外，本文还提出了一种效果很好的基线方法，可以与计算更昂贵的方法竞争。<details>
<summary>Abstract</summary>
Rule learning approaches for knowledge graph completion are efficient, interpretable and competitive to purely neural models. The rule aggregation problem is concerned with finding one plausibility score for a candidate fact which was simultaneously predicted by multiple rules. Although the problem is ubiquitous, as data-driven rule learning can result in noisy and large rulesets, it is underrepresented in the literature and its theoretical foundations have not been studied before in this context. In this work, we demonstrate that existing aggregation approaches can be expressed as marginal inference operations over the predicting rules. In particular, we show that the common Max-aggregation strategy, which scores candidates based on the rule with the highest confidence, has a probabilistic interpretation. Finally, we propose an efficient and overlooked baseline which combines the previous strategies and is competitive to computationally more expensive approaches.
</details>
<details>
<summary>摘要</summary>
<<SYS>> traduced text into Simplified Chinese.<</SYS>>知识图完成任务的规则学习方法是高效、可读性和竞争力强的。规则汇总问题关注于找到多个规则同时预测的 кандидат事实的可能性分数。尽管这个问题在数据驱动的规则学习中很普遍，但在文献中它尚未得到过足够的研究和理论基础。在这种情况下，我们展示了现有的汇总方法可以表示为规则预测时的边缘推理操作。特别是，我们显示了通用的Max汇总策略，将 кандидат事实分数基于预测规则的信任度进行评分，具有概率解释。最后，我们提出了一种高效且被忽略的基线， combinig 前两种策略，与计算更昂贵的方法竞争。
</details></li>
</ul>
<hr>
<h2 id="Identifiable-Cognitive-Diagnosis-with-Encoder-decoder-for-Modelling-Students’-Performance"><a href="#Identifiable-Cognitive-Diagnosis-with-Encoder-decoder-for-Modelling-Students’-Performance" class="headerlink" title="Identifiable Cognitive Diagnosis with Encoder-decoder for Modelling Students’ Performance"></a>Identifiable Cognitive Diagnosis with Encoder-decoder for Modelling Students’ Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00300">http://arxiv.org/abs/2309.00300</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiatong Li, Qi Liu, Fei Wang, Jiayu Liu, Zhenya Huang, Enhong Chen</li>
<li>for: 该论文旨在针对学生知识水平的诊断，以响应题目的回答得分作为基础，以便在多个领域中进行计算化适应测试。</li>
<li>methods: 该论文提出了一种新的可识别性诊断框架，包括直接从回答日志中诊断可识别和可解释的学生特征和问题特征，以及利用一种通用预测模块来重建回答日志，以保证诊断结果的准确性。</li>
<li>results: 该论文通过四个公共实验数据集的实验，证明了新的可识别性诊断框架可以提供可识别的诊断结果，同时也可以保证诊断结果的可解释性和精度。<details>
<summary>Abstract</summary>
Cognitive diagnosis aims to diagnose students' knowledge proficiencies based on their response scores on exam questions, which is the basis of many domains such as computerized adaptive testing. Existing cognitive diagnosis models (CDMs) follow a proficiency-response paradigm, which views diagnostic results as learnable embeddings that are the cause of students' responses and learns the diagnostic results through optimization. However, such a paradigm can easily lead to unidentifiable diagnostic results and the explainability overfitting problem, which is harmful to the quantification of students' learning performance. To address these problems, we propose a novel identifiable cognitive diagnosis framework. Specifically, we first propose a flexible diagnostic module which directly diagnose identifiable and explainable examinee traits and question features from response logs. Next, we leverage a general predictive module to reconstruct response logs from the diagnostic results to ensure the preciseness of the latter. We furthermore propose an implementation of the framework, i.e., ID-CDM, to demonstrate the availability of the former. Finally, we demonstrate the identifiability, explainability and preciseness of diagnostic results of ID-CDM through experiments on four public real-world datasets.
</details>
<details>
<summary>摘要</summary>
�� cognitive diagnosis 目标是根据学生响应 scored exam 问题的得分来评估学生的知识水平，这是许多领域，如计算机化适应测试的基础。现有的 cognitive diagnosis 模型（CDM）采用 proficiency-response 模式，视学生的响应为可学习的嵌入，通过优化来学习诊断结果。然而，这种模式可能导致诊断结果难以识别和过拟合问题，这会对学生学习表现的量化带来害。为解决这些问题，我们提出了一种新的可识别 cognitive diagnosis 框架。 Specifically, we first propose a flexible diagnostic module  directly diagnose identifiable and explainable examinee traits and question features from response logs. Next, we leverage a general predictive module to reconstruct response logs from the diagnostic results to ensure the preciseness of the latter. We furthermore propose an implementation of the framework, i.e., ID-CDM, to demonstrate the availability of the former. Finally, we demonstrate the identifiability, explainability and preciseness of diagnostic results of ID-CDM through experiments on four public real-world datasets.
</details></li>
</ul>
<hr>
<h2 id="End-to-end-Lidar-Driven-Reinforcement-Learning-for-Autonomous-Racing"><a href="#End-to-end-Lidar-Driven-Reinforcement-Learning-for-Autonomous-Racing" class="headerlink" title="End-to-end Lidar-Driven Reinforcement Learning for Autonomous Racing"></a>End-to-end Lidar-Driven Reinforcement Learning for Autonomous Racing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00296">http://arxiv.org/abs/2309.00296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meraj Mammadov</li>
<li>For: The paper is written for the domain of car racing, specifically in the context of autonomous racing.* Methods: The paper uses reinforcement learning (RL) and feedforward raw lidar and velocity data to train an RL agent in a simulated environment.* Results: The RL agent’s performance is experimentally evaluated in a real-world racing scenario, demonstrating the feasibility and potential benefits of RL algorithms in enhancing autonomous racing performance, especially in environments where prior map information is not available.Here is the information in Simplified Chinese text:</li>
<li>for: 本研究针对的是自动赛车领域，具体来说是在 simulations 中使用 reinforcement learning（RL）和 feedforward raw lidar 和 velocity data 训练一个 RL 智能体。</li>
<li>methods: 本研究使用 RL 和 feedforward raw lidar 和 velocity data 训练一个 RL 智能体，并在 simulated 环境中进行了训练。</li>
<li>results: 在实际的 racing enario 中，RL 智能体的性能得到了实验证明，表明RL 算法在缺乏 prior map information 的环境中提供了可能的和有利的性能提升。<details>
<summary>Abstract</summary>
Reinforcement Learning (RL) has emerged as a transformative approach in the domains of automation and robotics, offering powerful solutions to complex problems that conventional methods struggle to address. In scenarios where the problem definitions are elusive and challenging to quantify, learning-based solutions such as RL become particularly valuable. One instance of such complexity can be found in the realm of car racing, a dynamic and unpredictable environment that demands sophisticated decision-making algorithms. This study focuses on developing and training an RL agent to navigate a racing environment solely using feedforward raw lidar and velocity data in a simulated context. The agent's performance, trained in the simulation environment, is then experimentally evaluated in a real-world racing scenario. This exploration underlines the feasibility and potential benefits of RL algorithm enhancing autonomous racing performance, especially in the environments where prior map information is not available.
</details>
<details>
<summary>摘要</summary>
Reinforcement Learning (RL) 已经出现为自动化和机器人领域的一种转型方法，提供了强大的解决方案，解决了传统方法难以处理的复杂问题。在定义问题难以量化的情况下，学习基于的解决方案，如 RL，特别有价值。一个实例是在赛车场景中，这是一个动态和难以预测的环境，需要高级别的决策算法。这种研究将在模拟环境中开发和训练一个RL代理人， solely使用前向Raw Lidar和速度数据进行导航。在实际赛车场景中，代理人的性能，在模拟环境中训练的，进行实验性评估。这一探索， highlights the feasibility and potential benefits of RL算法在无产权地图信息的自动赛车性能提高中发挥作用。
</details></li>
</ul>
<hr>
<h2 id="RLAIF-Scaling-Reinforcement-Learning-from-Human-Feedback-with-AI-Feedback"><a href="#RLAIF-Scaling-Reinforcement-Learning-from-Human-Feedback-with-AI-Feedback" class="headerlink" title="RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback"></a>RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00267">http://arxiv.org/abs/2309.00267</a></li>
<li>repo_url: None</li>
<li>paper_authors: Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, Abhinav Rastogi</li>
<li>for: 这篇研究是为了比较从人类反馈（RLHF）和从AI反馈（RLAIF）两种技术，以改善大语言模型（LLMs）与人类偏好的整合。</li>
<li>methods: 这篇研究使用了RLHF和RLAIF两种技术，RLHF需要人类提供反馈，而RLAIF则使用了一个商业化的LLM来提供反馈。</li>
<li>results: 研究发现，RLHF和RLAIF都能够将大语言模型与人类偏好进行高质量的整合，并且人类评价者对RLAIF和RLHF两种摘要都有与基准模型相似的喜好。<details>
<summary>Abstract</summary>
Reinforcement learning from human feedback (RLHF) is effective at aligning large language models (LLMs) to human preferences, but gathering high quality human preference labels is a key bottleneck. We conduct a head-to-head comparison of RLHF vs. RL from AI Feedback (RLAIF) - a technique where preferences are labeled by an off-the-shelf LLM in lieu of humans, and we find that they result in similar improvements. On the task of summarization, human evaluators prefer generations from both RLAIF and RLHF over a baseline supervised fine-tuned model in ~70% of cases. Furthermore, when asked to rate RLAIF vs. RLHF summaries, humans prefer both at equal rates. These results suggest that RLAIF can yield human-level performance, offering a potential solution to the scalability limitations of RLHF.
</details>
<details>
<summary>摘要</summary>
人工反馈学习（RLHF）可以有效地将大语言模型（LLM）与人类偏好相对应，但收集高质量人类偏好标签是一个关键瓶颈。我们进行了RLHF与RL从AI反馈（RLAIF）的头比赛，其中RLAIF使用了市售LLM来标注偏好，而不是人类。我们发现，这两种技术在摘要任务上都可以达到类似的改进。人类评估员在70%的情况下偏好RLAIF和RLHF生成的摘要，并且对RLAIF和RLHF摘要进行评分时，偏好它们的情况相同。这些结果表明，RLAIF可以达到人类水平的表现，提供了RLHF扩展的可能性。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Learning-Metrics-for-Improved-Federated-Learning"><a href="#Leveraging-Learning-Metrics-for-Improved-Federated-Learning" class="headerlink" title="Leveraging Learning Metrics for Improved Federated Learning"></a>Leveraging Learning Metrics for Improved Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00257">http://arxiv.org/abs/2309.00257</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andre Fu</li>
<li>for: 本研究旨在应用可解释人工智能（XAI）的新研究，尤其是定量学习度量，以改善联边学习中的数据联边问题。</li>
<li>methods: 本研究使用联边学习和有效排名（ER）学习度量，实现了首个联边学习度量聚合方法。</li>
<li>results: 研究结果显示，使用有效排名学习度量可以超越基eline Federated Averaging \cite{konevcny2016federated}，并开发了一个基于有效排名的新量化策略。<details>
<summary>Abstract</summary>
Currently in the federated setting, no learning schemes leverage the emerging research of explainable artificial intelligence (XAI) in particular the novel learning metrics that help determine how well a model is learning. One of these novel learning metrics is termed `Effective Rank' (ER) which measures the Shannon Entropy of the singular values of a matrix, thus enabling a metric determining how well a layer is mapping. By joining federated learning and the learning metric, effective rank, this work will \textbf{(1)} give the first federated learning metric aggregation method \textbf{(2)} show that effective rank is well-suited to federated problems by out-performing baseline Federated Averaging \cite{konevcny2016federated} and \textbf{(3)} develop a novel weight-aggregation scheme relying on effective rank.
</details>
<details>
<summary>摘要</summary>
当前在联合学习 Setting中，无法学习 schemes 利用 emerging research of explainable artificial intelligence (XAI) 特别是新的学习指标，帮助确定模型是如何学习。其中一个新的学习指标是“有效排名”（ER），测量矩阵的几何 entropy，因此可以提供一个度量layer是如何映射。通过联合学习和有效排名指标的结合，本工作将实现以下三个目标：1. 提供首个联合学习指标聚合方法。2. 表明有效排名指标适合联合问题，超越基eline Federated Averaging \cite{konevcny2016federated}。3. 开发一种基于有效排名指标的新的质量聚合方案。
</details></li>
</ul>
<hr>
<h2 id="DiffuGen-Adaptable-Approach-for-Generating-Labeled-Image-Datasets-using-Stable-Diffusion-Models"><a href="#DiffuGen-Adaptable-Approach-for-Generating-Labeled-Image-Datasets-using-Stable-Diffusion-Models" class="headerlink" title="DiffuGen: Adaptable Approach for Generating Labeled Image Datasets using Stable Diffusion Models"></a>DiffuGen: Adaptable Approach for Generating Labeled Image Datasets using Stable Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00248">http://arxiv.org/abs/2309.00248</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mshenoda/diffugen">https://github.com/mshenoda/diffugen</a></li>
<li>paper_authors: Michael Shenoda, Edward Kim</li>
<li>for: 本研究旨在提高计算机视觉领域中Machine learning模型的准确性和可靠性，通过生成高质量的标注图像集。</li>
<li>methods: 本paper introduce了一种名为DiffuGen的简单有效的方法，利用稳定的扩散模型来生成标注图像集。DiffuGen combine了扩散模型的能力与两种不同的标注技术：无监督和监督。</li>
<li>results: DiffuGen可以生成高质量的标注图像集，并且提供了一种灵活的解决方案 для标注生成。在本paper中，我们介绍了DiffuGen的方法ología，包括使用提示模板进行适应图像生成和文本反转来增强扩散模型的能力。<details>
<summary>Abstract</summary>
Generating high-quality labeled image datasets is crucial for training accurate and robust machine learning models in the field of computer vision. However, the process of manually labeling real images is often time-consuming and costly. To address these challenges associated with dataset generation, we introduce "DiffuGen," a simple and adaptable approach that harnesses the power of stable diffusion models to create labeled image datasets efficiently. By leveraging stable diffusion models, our approach not only ensures the quality of generated datasets but also provides a versatile solution for label generation. In this paper, we present the methodology behind DiffuGen, which combines the capabilities of diffusion models with two distinct labeling techniques: unsupervised and supervised. Distinctively, DiffuGen employs prompt templating for adaptable image generation and textual inversion to enhance diffusion model capabilities.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>在计算机视觉领域，生成高质量标注图像集是训练精准和可靠机器学习模型的关键。然而，手动标注真实图像是时间consuming和成本高昂的。为解决这些数据生成过程中的挑战，我们介绍“DiffuGen”，一种简单而适应的方法，利用稳定扩散模型来生成标注图像集。通过利用稳定扩散模型，DiffuGen不仅保证生成的数据质量，还提供了一种多样化的标签生成解决方案。在这篇论文中，我们介绍DiffuGen的方法ологи，它结合扩散模型的能力和两种不同的标签技术：无监督和监督。与其他方法不同的是，DiffuGen使用插入模板来适应图像生成，以及文本反转来增强扩散模型的能力。
</details></li>
</ul>
<hr>
<h2 id="City-electric-power-consumption-forecasting-based-on-big-data-neural-network-under-smart-grid-background"><a href="#City-electric-power-consumption-forecasting-based-on-big-data-neural-network-under-smart-grid-background" class="headerlink" title="City electric power consumption forecasting based on big data &amp; neural network under smart grid background"></a>City electric power consumption forecasting based on big data &amp; neural network under smart grid background</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00245">http://arxiv.org/abs/2309.00245</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengxian Chen, Maowei Wang, Conghu Li</li>
<li>for: 这篇论文是为了研究城市电力消耗的预测和评估，以提供更好的城市服务。</li>
<li>methods: 论文使用大数据和神经网络模型，考虑了不同的非线性因素对城市电力消耗的影响，建立了一个预测城市电力消耗的模型。</li>
<li>results: 根据排序重要性测试，论文建立了城市电力消耗预测模型的核心特征值，对电力相关业界提供了重要参考。<details>
<summary>Abstract</summary>
With the development of the electric power system, the smart grid has become an important part of the smart city. The rational transmission of electric energy and the guarantee of power supply of the smart grid are very important to smart cities, smart cities can provide better services through smart grids. Among them, predicting and judging city electric power consumption is closely related to electricity supply and regulation, the location of power plants, and the control of electricity transmission losses. Based on big data, this paper establishes a neural network and considers the influence of various nonlinear factors on city electric power consumption. A model is established to realize the prediction of power consumption. Based on the permutation importance test, an evaluation model of the influencing factors of city electric power consumption is constructed to obtain the core characteristic values of city electric power consumption prediction, which can provide an important reference for electric power related industry.
</details>
<details>
<summary>摘要</summary>
随着电力系统的发展，智能电网已成为智能城市的重要组成部分。智能城市通过智能电网提供更好的服务，智能电网的合理的电能传输和电力供应是非常重要的。其中，预测和评估城市电力消耗和电力供应的关系非常重要，包括发电厂的位置、电力传输损失的控制等多个因素。基于大数据，本文建立了神经网络模型，考虑了城市电力消耗的多个非线性因素的影响。通过Permutation Importance Test，建立了城市电力消耗影响因素评价模型，获得了城市电力消耗预测核心特征值，可以为电力相关行业提供重要参考。
</details></li>
</ul>
<hr>
<h2 id="FactLLaMA-Optimizing-Instruction-Following-Language-Models-with-External-Knowledge-for-Automated-Fact-Checking"><a href="#FactLLaMA-Optimizing-Instruction-Following-Language-Models-with-External-Knowledge-for-Automated-Fact-Checking" class="headerlink" title="FactLLaMA: Optimizing Instruction-Following Language Models with External Knowledge for Automated Fact-Checking"></a>FactLLaMA: Optimizing Instruction-Following Language Models with External Knowledge for Automated Fact-Checking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00240">http://arxiv.org/abs/2309.00240</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thcheung/FactLLaMA">https://github.com/thcheung/FactLLaMA</a></li>
<li>paper_authors: Tsun-Hin Cheung, Kin-Man Lam</li>
<li>for: 本研究旨在提高自动 факчекин表现，以便更好地斗争假信息的扩散。</li>
<li>methods: 本研究使用了大型自然语言模型（LLMs）和指令遵循变体，如InstructGPT和Alpaca，以及外部证据检索来增强 fact-checking 表现。</li>
<li>results: 研究结果显示，将外部证据与 instruction-tuning 结合使用可以更好地预测输入CLAIM 的真伪性。在 RAWFC 和 LIAR 两个常用的 fact-checking 数据集上进行了实验，并取得了状态之 луч表现。<details>
<summary>Abstract</summary>
Automatic fact-checking plays a crucial role in combating the spread of misinformation. Large Language Models (LLMs) and Instruction-Following variants, such as InstructGPT and Alpaca, have shown remarkable performance in various natural language processing tasks. However, their knowledge may not always be up-to-date or sufficient, potentially leading to inaccuracies in fact-checking. To address this limitation, we propose combining the power of instruction-following language models with external evidence retrieval to enhance fact-checking performance. Our approach involves leveraging search engines to retrieve relevant evidence for a given input claim. This external evidence serves as valuable supplementary information to augment the knowledge of the pretrained language model. Then, we instruct-tune an open-sourced language model, called LLaMA, using this evidence, enabling it to predict the veracity of the input claim more accurately. To evaluate our method, we conducted experiments on two widely used fact-checking datasets: RAWFC and LIAR. The results demonstrate that our approach achieves state-of-the-art performance in fact-checking tasks. By integrating external evidence, we bridge the gap between the model's knowledge and the most up-to-date and sufficient context available, leading to improved fact-checking outcomes. Our findings have implications for combating misinformation and promoting the dissemination of accurate information on online platforms. Our released materials are accessible at: https://thcheung.github.io/factllama.
</details>
<details>
<summary>摘要</summary>
自动化Fact-checking plays a crucial role in combating the spread of misinformation. Large Language Models (LLMs) and Instruction-Following variants, such as InstructGPT and Alpaca, have shown remarkable performance in various natural language processing tasks. However, their knowledge may not always be up-to-date or sufficient, potentially leading to inaccuracies in fact-checking. To address this limitation, we propose combining the power of instruction-following language models with external evidence retrieval to enhance fact-checking performance. Our approach involves leveraging search engines to retrieve relevant evidence for a given input claim. This external evidence serves as valuable supplementary information to augment the knowledge of the pretrained language model. Then, we instruct-tune an open-sourced language model, called LLaMA, using this evidence, enabling it to predict the veracity of the input claim more accurately. To evaluate our method, we conducted experiments on two widely used fact-checking datasets: RAWFC and LIAR. The results demonstrate that our approach achieves state-of-the-art performance in fact-checking tasks. By integrating external evidence, we bridge the gap between the model's knowledge and the most up-to-date and sufficient context available, leading to improved fact-checking outcomes. Our findings have implications for combating misinformation and promoting the dissemination of accurate information on online platforms. Our released materials are accessible at: https://thcheung.github.io/factllama.
</details></li>
</ul>
<hr>
<h2 id="ALJP-An-Arabic-Legal-Judgment-Prediction-in-Personal-Status-Cases-Using-Machine-Learning-Models"><a href="#ALJP-An-Arabic-Legal-Judgment-Prediction-in-Personal-Status-Cases-Using-Machine-Learning-Models" class="headerlink" title="ALJP: An Arabic Legal Judgment Prediction in Personal Status Cases Using Machine Learning Models"></a>ALJP: An Arabic Legal Judgment Prediction in Personal Status Cases Using Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00238">http://arxiv.org/abs/2309.00238</a></li>
<li>repo_url: None</li>
<li>paper_authors: Salwa Abbara, Mona Hafez, Aya Kazzaz, Areej Alhothali, Alhanouf Alsolami</li>
<li>for: This paper aims to predict the judgment outcomes of Arabic case scripts, specifically in cases of custody and annulment of marriage.</li>
<li>methods: The authors use deep learning (DL) and natural language processing (NLP) techniques, including Support Vector Machine (SVM), Logistic regression (LR), Long Short Term Memory (LSTM), and Bidirectional Long Short-Term Memory (BiLSTM), with representation techniques such as TF-IDF and word2vec on a developed dataset.</li>
<li>results: The authors achieved high accuracy in predicting the judgment outcomes of custody cases and annulment of marriage, with the SVM model with word2vec and LR with TF-IDF achieving the highest accuracy of 88% and 78%, respectively. Additionally, the LR and SVM with word2vec and BiLSTM model with TF-IDF achieved the highest accuracy of 88% and 69%, respectively, in predicting the probability of outcomes on custody cases and annulment of marriage.<details>
<summary>Abstract</summary>
Legal Judgment Prediction (LJP) aims to predict judgment outcomes based on case description. Several researchers have developed techniques to assist potential clients by predicting the outcome in the legal profession. However, none of the proposed techniques were implemented in Arabic, and only a few attempts were implemented in English, Chinese, and Hindi. In this paper, we develop a system that utilizes deep learning (DL) and natural language processing (NLP) techniques to predict the judgment outcome from Arabic case scripts, especially in cases of custody and annulment of marriage. This system will assist judges and attorneys in improving their work and time efficiency while reducing sentencing disparity. In addition, it will help litigants, lawyers, and law students analyze the probable outcomes of any given case before trial. We use a different machine and deep learning models such as Support Vector Machine (SVM), Logistic regression (LR), Long Short Term Memory (LSTM), and Bidirectional Long Short-Term Memory (BiLSTM) using representation techniques such as TF-IDF and word2vec on the developed dataset. Experimental results demonstrate that compared with the five baseline methods, the SVM model with word2vec and LR with TF-IDF achieve the highest accuracy of 88% and 78% in predicting the judgment on custody cases and annulment of marriage, respectively. Furthermore, the LR and SVM with word2vec and BiLSTM model with TF-IDF achieved the highest accuracy of 88% and 69% in predicting the probability of outcomes on custody cases and annulment of marriage, respectively.
</details>
<details>
<summary>摘要</summary>
法律判断预测（LJP）目标是根据案件描述预测判决结果。一些研究人员已经开发了用于帮助 potential clients 预测案件结果的技术，但是这些技术都没有在阿拉伯语中实现，只有一些尝试在英语、中文和捷地语中实现。在这篇论文中，我们开发了一个系统，使用深度学习（DL）和自然语言处理（NLP）技术，从阿拉伯语案件脚本中预测判决结果，特别是在监护和婚姻 annulment 案件中。这个系统将帮助法官和律师提高工作效率和时间效率，同时减少判决不公。此外，它还将帮助诉讼人、律师和法学生分析案件的可能结果之前。我们使用了不同的机器学习和深度学习模型，如支持向量机（SVM）、逻辑回归（LR）、长短期记忆（LSTM）和双向长短期记忆（BiLSTM），使用表示技术如 TF-IDF 和 word2vec 在开发的数据集上。实验结果表明，与基准方法相比，SVM 模型与 word2vec 和 LR 模型与 TF-IDF  achieve 最高的准确率为 88% 和 78%，分别预测监护案件和婚姻 annulment 的判决结果。此外，LR 和 SVM 模型与 word2vec 和 BiLSTM 模型与 TF-IDF  achieve 最高的准确率为 88% 和 69%，分别预测监护案件和婚姻 annulment 的可能结果。
</details></li>
</ul>
<hr>
<h2 id="Publicly-Shareable-Clinical-Large-Language-Model-Built-on-Synthetic-Clinical-Notes"><a href="#Publicly-Shareable-Clinical-Large-Language-Model-Built-on-Synthetic-Clinical-Notes" class="headerlink" title="Publicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes"></a>Publicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00237">http://arxiv.org/abs/2309.00237</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/starmpcc/asclepius">https://github.com/starmpcc/asclepius</a></li>
<li>paper_authors: Sunjun Kweon, Junu Kim, Jiyoun Kim, Sujeong Im, Eunbyeol Cho, Seongsu Bae, Jungwoo Oh, Gyubok Lee, Jong Hak Moon, Seng Chan You, Seungjin Baek, Chang Hoon Han, Yoon Bin Jung, Yohan Jo, Edward Choi</li>
<li>For: The paper aims to develop a specialized clinical language model, Asclepius, to handle patients’ clinical notes, and to address the challenges of limited accessibility and usability of these notes due to strict privacy regulations.* Methods: The authors create synthetic large-scale clinical notes using publicly available case reports extracted from biomedical literature, and use these synthetic notes to train Asclepius. They also evaluate the performance of Asclepius using real clinical notes and compare it with other large language models, including GPT-3.5-turbo and other open-source alternatives.* Results: The authors find that synthetic clinical notes can serve as viable substitutes for real ones when constructing high-performing clinical language models, and that Asclepius outperforms other large language models in real-world applications. The findings are supported by detailed evaluations conducted by both GPT-4 and medical professionals.Here’s the simplified Chinese text for the three key points:* For: 这篇论文目的是开发一种专门用于处理患者医疗记录的临床语言模型，以解决因严格隐私法规限制而受到的医疗记录访问和使用困难。* Methods: 作者们使用公开可用的案例报告从生物医学文献中提取的大规模临床报告来生成synthetic大规模的临床报告，然后使用这些synthetic报告来训练特殊的临床语言模型Asclepius。作者们还使用实际的临床报告来评估Asclepius的性能，并与其他大语言模型进行比较，包括GPT-3.5-turbo和其他开源选择。* Results: 作者们发现，使用synthetic临床报告可以成为高性能临床语言模型的建模 substitutes，而Asclepius在实际应用中表现出色，比其他大语言模型更高。这些结论得到了GPT-4和医疗专业人员的详细评估。<details>
<summary>Abstract</summary>
The development of large language models tailored for handling patients' clinical notes is often hindered by the limited accessibility and usability of these notes due to strict privacy regulations. To address these challenges, we first create synthetic large-scale clinical notes using publicly available case reports extracted from biomedical literature. We then use these synthetic notes to train our specialized clinical large language model, Asclepius. While Asclepius is trained on synthetic data, we assess its potential performance in real-world applications by evaluating it using real clinical notes. We benchmark Asclepius against several other large language models, including GPT-3.5-turbo and other open-source alternatives. To further validate our approach using synthetic notes, we also compare Asclepius with its variants trained on real clinical notes. Our findings convincingly demonstrate that synthetic clinical notes can serve as viable substitutes for real ones when constructing high-performing clinical language models. This conclusion is supported by detailed evaluations conducted by both GPT-4 and medical professionals. All resources including weights, codes, and data used in the development of Asclepius are made publicly accessible for future research.
</details>
<details>
<summary>摘要</summary>
大型语言模型的开发，用于处理病人的诊所录取得到受到隐私规定限制，导致这些录取不易存取和使用。为了解决这些挑战，我们首先创建了大规模的人工生成的诊所录取，使用公开可用的专业医疗文献中的案例报告。然后，我们使用这些人工生成的录取来训练我们的特殊化的医疗语言模型Asclepius。处理Asclepius的训练是使用人工生成的数据，我们使用真实的诊所录取进行评估。我们与其他大型语言模型，如GPT-3.5-turbo和其他开源选择进行比较。为了进一步验证我们的方法，我们还比较Asclepius与它的变体，它们是使用真实的诊所录取进行训练。我们的结果表明，人工生成的诊所录取可以作为真实的诊所录取的可行substitute，这是由GPT-4和医疗专业人员进行详细评估所支持。我们所有的资源，包括权重、代码和数据，都是公开可用的，以便未来的研究。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-for-Semantic-Monitoring-of-Corporate-Disclosures-A-Case-Study-on-Korea’s-Top-50-KOSPI-Companies"><a href="#Large-Language-Models-for-Semantic-Monitoring-of-Corporate-Disclosures-A-Case-Study-on-Korea’s-Top-50-KOSPI-Companies" class="headerlink" title="Large Language Models for Semantic Monitoring of Corporate Disclosures: A Case Study on Korea’s Top 50 KOSPI Companies"></a>Large Language Models for Semantic Monitoring of Corporate Disclosures: A Case Study on Korea’s Top 50 KOSPI Companies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00208">http://arxiv.org/abs/2309.00208</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junwon Sung, Woojin Heo, Yunkyung Byun, Youngsam Kim</li>
<li>for: 这项研究探讨了OpenAI的GPT-3.5-turbo和GPT-4语言模型在韩国上市公司公告中的semantic分析能力，尤其是对于实时公告。</li>
<li>methods: 研究对韩国KOSPI上市50 круп公司的月度报告进行了分析，每份报告都被赋予了一个含义评分，范围从1(非常负面)到5(非常正面)。</li>
<li>results: 研究发现GPT-4表现了显著的准确性，与人工专家的评分相比，Spearman相关系数为0.61，朴素匹配率为0.82。这些发现对GPT模型的评价特征提供了重要的新视角，为未来自动 semantic监测领域的创新奠定了基础。<details>
<summary>Abstract</summary>
In the rapidly advancing domain of artificial intelligence, state-of-the-art language models such as OpenAI's GPT-3.5-turbo and GPT-4 offer unprecedented opportunities for automating complex tasks. This research paper delves into the capabilities of these models for semantically analyzing corporate disclosures in the Korean context, specifically for timely disclosure. The study focuses on the top 50 publicly traded companies listed on the Korean KOSPI, based on market capitalization, and scrutinizes their monthly disclosure summaries over a period of 17 months. Each summary was assigned a sentiment rating on a scale ranging from 1(very negative) to 5(very positive). To gauge the effectiveness of the language models, their sentiment ratings were compared with those generated by human experts. Our findings reveal a notable performance disparity between GPT-3.5-turbo and GPT-4, with the latter demonstrating significant accuracy in human evaluation tests. The Spearman correlation coefficient was registered at 0.61, while the simple concordance rate was recorded at 0.82. This research contributes valuable insights into the evaluative characteristics of GPT models, thereby laying the groundwork for future innovations in the field of automated semantic monitoring.
</details>
<details>
<summary>摘要</summary>
在人工智能领域的快速发展中，现代语言模型如OpenAI的GPT-3.5-turbo和GPT-4提供了无前例的自动化复杂任务的机会。这篇研究论文探讨了这些模型在韩国上市公司公告中的语义分析能力，具体来说是对时间性公告进行实时分析。研究选择韩国KOSPI板块上市50大公司，根据市值排名，并对这些公司月度公告摘要进行17个月的分析。每份摘要都被赋予了一个sentiment评级，从1（非常负面）到5（非常正面）的评级范围内。为了评估语言模型的效果，我们与人类专家生成的sentiment评级进行比较。我们的发现表明GPT-3.5-turbo和GPT-4之间存在显著的性能差异，GPT-4在人类评估测试中表现出了显著的准确性。Spearman相关系数为0.61，单词匹配率为0.82。这篇研究为未来自动语义监测领域的创新奠定了基础。
</details></li>
</ul>
<hr>
<h2 id="Gap-and-Overlap-Detection-in-Automated-Fiber-Placement"><a href="#Gap-and-Overlap-Detection-in-Automated-Fiber-Placement" class="headerlink" title="Gap and Overlap Detection in Automated Fiber Placement"></a>Gap and Overlap Detection in Automated Fiber Placement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00206">http://arxiv.org/abs/2309.00206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Assef Ghamisi, Homayoun Najjaran</li>
<li>For: This paper is written for the purpose of detecting and correcting manufacturing defects in composite parts produced through Automated Fiber Placement (AFP). The focus is on gaps and overlaps, which are the most common defects that can significantly impact the quality of the composite parts.* Methods: The paper proposes a novel method that uses an Optical Coherence Tomography (OCT) sensor and computer vision techniques to detect and locate gaps and overlaps in composite parts. The method involves generating a depth map image of the composite surface, detecting the boundaries of each tow, and comparing consecutive tows to identify gaps or overlaps that exceed a predefined tolerance threshold.* Results: The results of the paper demonstrate a high level of accuracy and efficiency in gap and overlap segmentation, as compared to ground truth annotations by experts. The approach is effective in detecting defects in composite parts produced through AFP, and has the potential to improve the overall quality and efficiency of the manufacturing process.<details>
<summary>Abstract</summary>
The identification and correction of manufacturing defects, particularly gaps and overlaps, are crucial for ensuring high-quality composite parts produced through Automated Fiber Placement (AFP). These imperfections are the most commonly observed issues that can significantly impact the overall quality of the composite parts. Manual inspection is both time-consuming and labor-intensive, making it an inefficient approach. To overcome this challenge, the implementation of an automated defect detection system serves as the optimal solution. In this paper, we introduce a novel method that uses an Optical Coherence Tomography (OCT) sensor and computer vision techniques to detect and locate gaps and overlaps in composite parts. Our approach involves generating a depth map image of the composite surface that highlights the elevation of composite tapes (or tows) on the surface. By detecting the boundaries of each tow, our algorithm can compare consecutive tows and identify gaps or overlaps that may exist between them. Any gaps or overlaps exceeding a predefined tolerance threshold are considered manufacturing defects. To evaluate the performance of our approach, we compare the detected defects with the ground truth annotated by experts. The results demonstrate a high level of accuracy and efficiency in gap and overlap segmentation.
</details>
<details>
<summary>摘要</summary>
检测和修正制造过程中的缺陷，特别是孔隙和重叠，对于通过自动纤维放置（AFP）生产的复合部件质量的确保非常重要。这些缺陷是制造过程中最常见的问题，可能对全面质量产生重要影响。手动检查是时间consuming和人力 INTENSIVE，因此是不可靠的方法。为了解决这个挑战，我们提出了一种新的方法，使用光子干涉Tomography（OCT）感知器和计算机视觉技术来检测和定位复合部件中的孔陷和重叠。我们的方法是生成复合部件表面的深度图像，高亮显示复合带（或排列）的抬升。通过检测每个带的边界，我们的算法可以比较 consecutive带之间的孔陷和重叠，并确定任何超过预定的允许阈值的缺陷。我们对我们的方法的性能进行了评估，结果表明我们的方法具有高精度和高效的孔陷和重叠分 segmentation。
</details></li>
</ul>
<hr>
<h2 id="Subjectivity-in-Unsupervised-Machine-Learning-Model-Selection"><a href="#Subjectivity-in-Unsupervised-Machine-Learning-Model-Selection" class="headerlink" title="Subjectivity in Unsupervised Machine Learning Model Selection"></a>Subjectivity in Unsupervised Machine Learning Model Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00201">http://arxiv.org/abs/2309.00201</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wanyi Chen, Mary L. Cummings</li>
<li>for: 这个研究旨在探讨机器学习模型选择过程中的主观性。</li>
<li>methods: 这个研究使用隐马尔可夫模型作为例子，通过询问33名参与者和三个大型自然语言模型（LLMs）进行模型选择，以探讨参与者和LLMs在不同条件下的选择差异。</li>
<li>results: 研究发现参与者和LLMs在不同条件下的选择具有差异和不一致性，尤其是当不同的评价标准和度量不同时。主观性的来源包括参与者对不同评价标准和度量的意见不一致，以及模型的简洁程度和数据集大小的影响。这些结果 highlights the importance of developing a more standardized way to document subjective choices made in model selection processes。<details>
<summary>Abstract</summary>
Model selection is a necessary step in unsupervised machine learning. Despite numerous criteria and metrics, model selection remains subjective. A high degree of subjectivity may lead to questions about repeatability and reproducibility of various machine learning studies and doubts about the robustness of models deployed in the real world. Yet, the impact of modelers' preferences on model selection outcomes remains largely unexplored. This study uses the Hidden Markov Model as an example to investigate the subjectivity involved in model selection. We asked 33 participants and three Large Language Models (LLMs) to make model selections in three scenarios. Results revealed variability and inconsistencies in both the participants' and the LLMs' choices, especially when different criteria and metrics disagree. Sources of subjectivity include varying opinions on the importance of different criteria and metrics, differing views on how parsimonious a model should be, and how the size of a dataset should influence model selection. The results underscore the importance of developing a more standardized way to document subjective choices made in model selection processes.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Diffusion-Model-with-Clustering-based-Conditioning-for-Food-Image-Generation"><a href="#Diffusion-Model-with-Clustering-based-Conditioning-for-Food-Image-Generation" class="headerlink" title="Diffusion Model with Clustering-based Conditioning for Food Image Generation"></a>Diffusion Model with Clustering-based Conditioning for Food Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00199">http://arxiv.org/abs/2309.00199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Han, Jiangpeng He, Mridul Gupta, Edward J. Delp, Fengqing Zhu</li>
<li>for: 这篇论文目的是提出一种基于条件扩散模型的食物图像生成方法，以提高食物图像生成质量和多样性。</li>
<li>methods: 该方法使用了条件扩散模型，并提出了一种基于归一化的聚类训练策略，以生成高质量和代表性的食物图像。</li>
<li>results: 研究表明，使用条件扩散模型生成的食物图像可以提高食物图像生成质量和多样性，并可以Address the severe class imbalance issue in long-tailed food classification。<details>
<summary>Abstract</summary>
Image-based dietary assessment serves as an efficient and accurate solution for recording and analyzing nutrition intake using eating occasion images as input. Deep learning-based techniques are commonly used to perform image analysis such as food classification, segmentation, and portion size estimation, which rely on large amounts of food images with annotations for training. However, such data dependency poses significant barriers to real-world applications, because acquiring a substantial, diverse, and balanced set of food images can be challenging. One potential solution is to use synthetic food images for data augmentation. Although existing work has explored the use of generative adversarial networks (GAN) based structures for generation, the quality of synthetic food images still remains subpar. In addition, while diffusion-based generative models have shown promising results for general image generation tasks, the generation of food images can be challenging due to the substantial intra-class variance. In this paper, we investigate the generation of synthetic food images based on the conditional diffusion model and propose an effective clustering-based training framework, named ClusDiff, for generating high-quality and representative food images. The proposed method is evaluated on the Food-101 dataset and shows improved performance when compared with existing image generation works. We also demonstrate that the synthetic food images generated by ClusDiff can help address the severe class imbalance issue in long-tailed food classification using the VFN-LT dataset.
</details>
<details>
<summary>摘要</summary>
图像基于的营养评估可以作为有效和准确的解决方案，用于记录和分析饮食摄入，使用吃饭场景图像作为输入。深度学习技术通常用于图像分析，如食物分类、 segmentation 和分量估计，但是这些技术需要大量的食物图像进行训练。然而，在实际应用中，获得充足、多样化和均衡的食物图像是一个大的挑战。一个可能的解决方案是使用生成的食物图像进行数据增强。虽然现有的工作已经探讨了基于生成对抗网络（GAN）结构的生成，但是生成的食物图像质量仍然较差。此外，在涉及到食物图像生成时，存在较大的内部变异问题。在这篇论文中，我们研究基于条件扩散模型的食物图像生成，并提出一种有效的分组训练框架，名为ClusDiff，以生成高质量和代表性的食物图像。我们的方法被评估在Food-101数据集上，并与现有的图像生成工作进行比较。我们还示出了ClusDiff生成的食物图像可以帮助解决VFN-LT数据集中的严重类别偏见问题。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/01/cs.AI_2023_09_01/" data-id="clpxp6bwg003dee8890p3c0lg" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_09_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/01/cs.CL_2023_09_01/" class="article-date">
  <time datetime="2023-09-01T11:00:00.000Z" itemprop="datePublished">2023-09-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/01/cs.CL_2023_09_01/">cs.CL - 2023-09-01</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Let-the-Models-Respond-Interpreting-Language-Model-Detoxification-Through-the-Lens-of-Prompt-Dependence"><a href="#Let-the-Models-Respond-Interpreting-Language-Model-Detoxification-Through-the-Lens-of-Prompt-Dependence" class="headerlink" title="Let the Models Respond: Interpreting Language Model Detoxification Through the Lens of Prompt Dependence"></a>Let the Models Respond: Interpreting Language Model Detoxification Through the Lens of Prompt Dependence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00751">http://arxiv.org/abs/2309.00751</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/DanielSc4/RewardLM">https://github.com/DanielSc4/RewardLM</a></li>
<li>paper_authors: Daniel Scalena, Gabriele Sarti, Malvina Nissim, Elisabetta Fersini</li>
<li>for: 这paper是为了研究语模型的减带技术对模型内部过程的影响。</li>
<li>methods: 这paper使用了流行的减带方法，并使用特征评估方法来衡量这些方法对模型的依赖度的影响。</li>
<li>results: 研究发现，使用减带方法可以改善模型的安全性，但是这些方法对模型内部过程的影响还不很清楚。此外，研究还发现，使用反对 narative 练习法可以提高模型的减带性能，但是这种方法与减带学习法的影响不同。<details>
<summary>Abstract</summary>
Due to language models' propensity to generate toxic or hateful responses, several techniques were developed to align model generations with users' preferences. Despite the effectiveness of such methods in improving the safety of model interactions, their impact on models' internal processes is still poorly understood. In this work, we apply popular detoxification approaches to several language models and quantify their impact on the resulting models' prompt dependence using feature attribution methods. We evaluate the effectiveness of counter-narrative fine-tuning and compare it with reinforcement learning-driven detoxification, observing differences in prompt reliance between the two methods despite their similar detoxification performances.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Baseline-Defenses-for-Adversarial-Attacks-Against-Aligned-Language-Models"><a href="#Baseline-Defenses-for-Adversarial-Attacks-Against-Aligned-Language-Models" class="headerlink" title="Baseline Defenses for Adversarial Attacks Against Aligned Language Models"></a>Baseline Defenses for Adversarial Attacks Against Aligned Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00614">http://arxiv.org/abs/2309.00614</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, Tom Goldstein</li>
<li>for: 这篇论文是关于语言模型安全性的研究，特别是对于语言模型在不同的威胁模型下的防御性能。</li>
<li>methods: 本文使用了多种防御策略，包括检测（基于异常值的混淆）、输入预处理（重叠和重tokenization）以及对抗训练。</li>
<li>results: 研究发现，使用现有的粗糙优化器在文本域下可能会受到限制，而且对于语言模型来说，标准的适应攻击更加困难。未来的研究可能需要开发更强大的优化器，或者发现filtering和预处理防御策略在语言模型领域的强大性。<details>
<summary>Abstract</summary>
As Large Language Models quickly become ubiquitous, it becomes critical to understand their security vulnerabilities. Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain? How do baseline defense techniques perform in this new domain? How does LLM security differ from computer vision?   We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, discussing the various settings in which each is feasible and effective. Particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. We discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. We find that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs. Future research will be needed to uncover whether more powerful optimizers can be developed, or whether the strength of filtering and preprocessing defenses is greater in the LLMs domain than it has been in computer vision.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>What practical threat models are relevant in this domain?2. How do baseline defense techniques perform in this new domain?3. How does LLM security differ from computer vision?We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, considering their feasibility and effectiveness in different settings. These include:1. Detection (perplexity-based)2. Input preprocessing (paraphrasing and retokenization)3. Adversarial trainingWe explore white-box and gray-box settings and analyze the trade-off between robustness and performance for each defense. Our findings suggest that the limitations of existing discrete optimizers for text, combined with the relatively high cost of optimization, make standard adaptive attacks more challenging for LLMs. Future research may focus on developing more powerful optimizers or enhancing the strength of filtering and preprocessing defenses in the LLM domain.In conclusion, understanding the security vulnerabilities of LLMs is crucial as they become increasingly ubiquitous. By examining these vulnerabilities using the principles of adversarial machine learning, we can develop effective defense strategies to protect these models from malicious attacks.</details></li>
</ol>
<hr>
<h2 id="Taken-out-of-context-On-measuring-situational-awareness-in-LLMs"><a href="#Taken-out-of-context-On-measuring-situational-awareness-in-LLMs" class="headerlink" title="Taken out of context: On measuring situational awareness in LLMs"></a>Taken out of context: On measuring situational awareness in LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00667">http://arxiv.org/abs/2309.00667</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/asacooperstickland/situational-awareness-evals">https://github.com/asacooperstickland/situational-awareness-evals</a></li>
<li>paper_authors: Lukas Berglund, Asa Cooper Stickland, Mikita Balesni, Max Kaufmann, Meg Tong, Tomasz Korbak, Daniel Kokotajlo, Owain Evans</li>
<li>for: 本研究目的是更好地理解大语言模型（LLM）中的情境意识的发展。</li>
<li>methods: 本研究使用了扩展LLM的能力来评估情境意识的发展。 Specifically, the researchers used out-of-context reasoning as a way to test for situational awareness.</li>
<li>results: 研究发现，LLMs可以在没有示例或教程的情况下通过描述来解决测试。  however, the success of the models is sensitive to the training setup and only works with data augmentation. Additionally, the research found that larger models perform better on this task.<details>
<summary>Abstract</summary>
We aim to better understand the emergence of `situational awareness' in large language models (LLMs). A model is situationally aware if it's aware that it's a model and can recognize whether it's currently in testing or deployment. Today's LLMs are tested for safety and alignment before they are deployed. An LLM could exploit situational awareness to achieve a high score on safety tests, while taking harmful actions after deployment. Situational awareness may emerge unexpectedly as a byproduct of model scaling. One way to better foresee this emergence is to run scaling experiments on abilities necessary for situational awareness. As such an ability, we propose `out-of-context reasoning' (in contrast to in-context learning). We study out-of-context reasoning experimentally. First, we finetune an LLM on a description of a test while providing no examples or demonstrations. At test time, we assess whether the model can pass the test. To our surprise, we find that LLMs succeed on this out-of-context reasoning task. Their success is sensitive to the training setup and only works when we apply data augmentation. For both GPT-3 and LLaMA-1, performance improves with model size. These findings offer a foundation for further empirical study, towards predicting and potentially controlling the emergence of situational awareness in LLMs. Code is available at: https://github.com/AsaCooperStickland/situational-awareness-evals.
</details>
<details>
<summary>摘要</summary>
我们目标是更好地理解大语言模型（LLM）中的“情境意识”的出现。一个模型被称为情境意识模型，如果它意识到它是一个模型，并能识别它是否在测试或部署中。今天的LLM都在测试和对齐之前被部署。一个LLM可以通过情境意识来达到安全测试中高分，而在部署后执行有害的操作。情境意识可能会意外地出现，因此我们可以通过扩大模型来更好地预测其出现。作为一种能力，我们提出了“离 context 理解”（与上下文学习相对）。我们通过实验研究离 context 理解。我们首先精度调整了一个LLM，使其能够通过一个测试描述而不提供示例或示范。测试时，我们评估模型是否能通过测试。我们启示发现，LLMs在这种离 context 理解任务中成功。其成功关系于训练Setup，并且只有在应用数据扩展时才能实现。对于GPT-3和LLaMA-1，模型的性能随模型大小增长。这些发现为我们未来更多的实验研究提供了基础，以预测和可能控制LLM中的情境意识的出现。代码可以在以下 GitHub 上找到：https://github.com/AsaCooperStickland/situational-awareness-evals。
</details></li>
</ul>
<hr>
<h2 id="Satisfiability-Checking-of-Multi-Variable-TPTL-with-Unilateral-Intervals-Is-PSPACE-Complete"><a href="#Satisfiability-Checking-of-Multi-Variable-TPTL-with-Unilateral-Intervals-Is-PSPACE-Complete" class="headerlink" title="Satisfiability Checking of Multi-Variable TPTL with Unilateral Intervals Is PSPACE-Complete"></a>Satisfiability Checking of Multi-Variable TPTL with Unilateral Intervals Is PSPACE-Complete</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00386">http://arxiv.org/abs/2309.00386</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shankara Narayanan Krishna, Khushraj Nanik Madnani, Rupak Majumdar, Paritosh K. Pandya</li>
<li>for: 这个论文研究了${0,\infty}$ fragment of Timed Propositional Temporal Logic (TPTL)的可 decidability。</li>
<li>methods: 作者使用了一种新的“非紧急”类型的 Alternating Timed Automata with multiple clocks called Unilateral Very Weak Alternating Timed Automata (VWATA$^{0,\infty}$)来验证 TPTL$^{0,\infty}$的满足性检查是PSPACE完备的。</li>
<li>results: 作者发现了一个新的多变量 fragment of TPTL，它的满足性检查是可 decidable，而且比 Metric Interval Temporal Logic (MITL)更加表达力强，且计算更加容易。这是首次没有对时间字符串（例如带有约束的变化）做出任何限制的多变量 TPTL fragment 的满足性检查是 decidable。<details>
<summary>Abstract</summary>
We investigate the decidability of the ${0,\infty}$ fragment of Timed Propositional Temporal Logic (TPTL). We show that the satisfiability checking of TPTL$^{0,\infty}$ is PSPACE-complete. Moreover, even its 1-variable fragment (1-TPTL$^{0,\infty}$) is strictly more expressive than Metric Interval Temporal Logic (MITL) for which satisfiability checking is EXPSPACE complete. Hence, we have a strictly more expressive logic with computationally easier satisfiability checking. To the best of our knowledge, TPTL$^{0,\infty}$ is the first multi-variable fragment of TPTL for which satisfiability checking is decidable without imposing any bounds/restrictions on the timed words (e.g. bounded variability, bounded time, etc.). The membership in PSPACE is obtained by a reduction to the emptiness checking problem for a new "non-punctual" subclass of Alternating Timed Automata with multiple clocks called Unilateral Very Weak Alternating Timed Automata (VWATA$^{0,\infty}$) which we prove to be in PSPACE. We show this by constructing a simulation equivalent non-deterministic timed automata whose number of clocks is polynomial in the size of the given VWATA$^{0,\infty}$.
</details>
<details>
<summary>摘要</summary>
我们调查${0,\infty}$ fragment of Timed Propositional Temporal Logic (TPTL)的对应性。我们表明TPTL$^{0,\infty}$的满足性检查是PSPACE完全的。此外，我们还证明1-TPTL$^{0,\infty}$比Metric Interval Temporal Logic (MITL)更加表达力强，其满足性检查是EXPSPACE完全的。因此，我们有一个更加表达力强的逻辑，且 computationally easier的满足性检查。根据我们所知，TPTL$^{0,\infty}$是第一个不受任何紧 bound/restriction 的时间语言的多变量 fragment 的满足性检查是可 decidable。PSPACE 的成员由一个对应的 Alternating Timed Automata with multiple clocks 的新子集 Unilateral Very Weak Alternating Timed Automata (VWATA$^{0,\infty}$) 的emptiness checking problem 的 reduction 而得。我们显示这个问题是PSPACE的，通过建构一个与非决定型时间自动 machine 相对应的同步化的非决定型时间自动 machine，其中的时钟数量是对应类别的大小的几何函数。
</details></li>
</ul>
<hr>
<h2 id="BatchPrompt-Accomplish-more-with-less"><a href="#BatchPrompt-Accomplish-more-with-less" class="headerlink" title="BatchPrompt: Accomplish more with less"></a>BatchPrompt: Accomplish more with less</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00384">http://arxiv.org/abs/2309.00384</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianzhe Lin, Maurice Diesendruck, Liang Du, Robin Abraham</li>
<li>for: 提高大语言模型（LLM）的提问效率，使其更加高效地处理长Context提问。</li>
<li>methods: 使用批处理（BatchPrompt）技术，将数据集分割成多个批处理，并对每个批处理进行独立的提问。并提出了两种技术：批处理 permutation（BPE）和自我反射指导的早期停止（SEAS）。</li>
<li>results: 通过实验证明，使用BPE和SEAS技术可以提高批处理提问的性能，并且与单个提问（SinglePrompt）相比，使用BPE和SEAS技术需要更少的LLM调用和输入token（只需9%-16%的LLM调用和27.4%的输入token，可以达到90.6%-90.9%的Boolq准确率，87.2%-88.4%的QQP准确率和91.5%-91.1%的RTE准确率）。这是大语言模型提问的首次技术改进。<details>
<summary>Abstract</summary>
As the ever-increasing token limits of large language models (LLMs) have enabled long context as input, prompting with single data samples might no longer an efficient way. A straightforward strategy improving efficiency is to batch data within the token limit (e.g., 8k for gpt-3.5-turbo; 32k for GPT-4), which we call BatchPrompt. We have two initial observations for prompting with batched data. First, we find that prompting with batched data in longer contexts will inevitably lead to worse performance, compared to single-data prompting. Second, the performance of the language model is significantly correlated with the positions and order of the batched data, due to the corresponding change in decoder context. To retain efficiency and overcome performance loss, we propose Batch Permutation and Ensembling (BPE), and a novel Self-reflection-guided EArly Stopping (SEAS) technique. Our comprehensive experimental evaluation demonstrates that BPE can boost the performance of BatchPrompt with a striking margin on a range of popular NLP tasks, including question answering (Boolq), textual entailment (RTE), and duplicate questions identification (QQP). These performances are even competitive with/higher than single-data prompting(SinglePrompt), while BatchPrompt requires much fewer LLM calls and input tokens (For SinglePrompt v.s. BatchPrompt with batch size 32, using just 9%-16% the number of LLM calls, Boolq accuracy 90.6% to 90.9% with 27.4% tokens, QQP accuracy 87.2% to 88.4% with 18.6% tokens, RTE accuracy 91.5% to 91.1% with 30.8% tokens). To the best of our knowledge, this is the first work to technically improve prompting efficiency of large language models. We hope our simple yet effective approach will shed light on the future research of large language models. The code will be released.
</details>
<details>
<summary>摘要</summary>
为了提高大语言模型（LLM）的效率，我们可以考虑使用批处理（batching）技术。我们称这种技术为批提示（BatchPrompt）。我们发现，使用批处理技术可以大幅提高 LLM 的性能，但是在某些情况下，它可能会导致性能下降。我们提出了两种策略来解决这个问题：批 permutation 和批ensemble（BPE），以及一种新的自适应停止（SEAS）技术。我们的实验证明，BPE 可以在多种常见的自然语言处理任务上提高 BatchPrompt 的性能，并且和单个数据提示（SinglePrompt）相比，BPE 需要许多 fewer LLM 调用和输入符号（token）。我们认为，这是首次技术上提高大语言模型的提示效率的研究。我们希望我们的简单 yet 有效的方法可以引领未来的大语言模型研究。我们将代码发布。
</details></li>
</ul>
<hr>
<h2 id="Long-Term-Memorability-On-Advertisements"><a href="#Long-Term-Memorability-On-Advertisements" class="headerlink" title="Long-Term Memorability On Advertisements"></a>Long-Term Memorability On Advertisements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00378">http://arxiv.org/abs/2309.00378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Harini S I, Somesh Singh, Yaman K Singla, Aanisha Bhattacharyya, Veeky Baths, Changyou Chen, Rajiv Ratn Shah, Balaji Krishnamurthy</li>
<li>for: The paper aims to study the memorability of ads in the machine learning literature, specifically focusing on long-term memorability and the impact of multimodality and human factors.</li>
<li>methods: The study consists of 1203 participants and 2205 ads covering 276 brands, with statistical tests run to identify factors that contribute to ad memorability. Additionally, the paper presents a novel model called Sharingan, which leverages real-world knowledge of LLMs and visual knowledge of visual encoders to predict the memorability of content.</li>
<li>results: The study finds that fast-moving scenes in commercials are more memorable than slower scenes (p&#x3D;8e-10), and that people who use ad-blockers remember lower number of ads than those who don’t (p&#x3D;5e-3). The Sharingan model achieves state-of-the-art performance on all prominent memorability datasets in literature, and ablation studies reveal insights into what drives memory.<details>
<summary>Abstract</summary>
Marketers spend billions of dollars on advertisements but to what end? At the purchase time, if customers cannot recognize a brand for which they saw an ad, the money spent on the ad is essentially wasted. Despite its importance in marketing, until now, there has been no study on the memorability of ads in the ML literature. Most studies have been conducted on short-term recall (<5 mins) on specific content types like object and action videos. On the other hand, the advertising industry only cares about long-term memorability (a few hours or longer), and advertisements are almost always highly multimodal, depicting a story through its different modalities (text, images, and videos). With this motivation, we conduct the first large scale memorability study consisting of 1203 participants and 2205 ads covering 276 brands. Running statistical tests over different participant subpopulations and ad-types, we find many interesting insights into what makes an ad memorable - both content and human factors. For example, we find that brands which use commercials with fast moving scenes are more memorable than those with slower scenes (p=8e-10) and that people who use ad-blockers remember lower number of ads than those who don't (p=5e-3). Further, with the motivation of simulating the memorability of marketing materials for a particular audience, ultimately helping create one, we present a novel model, Sharingan, trained to leverage real-world knowledge of LLMs and visual knowledge of visual encoders to predict the memorability of a content. We test our model on all the prominent memorability datasets in literature (both images and videos) and achieve state of the art across all of them. We conduct extensive ablation studies across memory types, modality, brand, and architectural choices to find insights into what drives memory.
</details>
<details>
<summary>摘要</summary>
To address this gap, we conducted a large-scale memorability study with 1203 participants and 2205 ads covering 276 brands. We found several interesting insights into what makes an ad memorable, including the use of fast-moving scenes (p=8e-10) and the impact of ad-blockers (p=5e-3).To simulate the memorability of marketing materials for a particular audience, we developed a novel model called Sharingan. This model leverages real-world knowledge of large language models (LLMs) and visual knowledge of visual encoders to predict the memorability of a content. We tested our model on several prominent memorability datasets in the literature (both images and videos) and achieved state-of-the-art results across all of them.We conducted extensive ablation studies to understand what drives memory, including the impact of different memory types, modalities, brands, and architectural choices. Our findings provide valuable insights for marketers and advertisers looking to create memorable ads that resonate with their target audience.
</details></li>
</ul>
<hr>
<h2 id="Examining-the-Effectiveness-of-Chatbots-in-Gathering-Family-History-Information-in-Comparison-to-the-Standard-In-Person-Interview-Based-Approach"><a href="#Examining-the-Effectiveness-of-Chatbots-in-Gathering-Family-History-Information-in-Comparison-to-the-Standard-In-Person-Interview-Based-Approach" class="headerlink" title="Examining the Effectiveness of Chatbots in Gathering Family History Information in Comparison to the Standard In-Person Interview-Based Approach"></a>Examining the Effectiveness of Chatbots in Gathering Family History Information in Comparison to the Standard In-Person Interview-Based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03223">http://arxiv.org/abs/2309.03223</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kieron Drumm, Vincent Tran</li>
<li>For: This paper aims to present a chatbot-based approach for gathering family histories, with the goal of providing a valuable tool for genealogists, especially when dealing with interviewees who are based in other countries.* Methods: The paper compares the performance and usability of a chatbot-based approach with two other methods: using ancestry.com and in-person interviews. The chatbot is designed to guide the interviewee through the process of providing their family history information.* Results: The paper shows that the chatbot-based approach has a lower number of mistakes made and a lower level of confusion from the user compared to the other two methods. However, the average time taken to conduct an interview using the chatbot is longer than the other two methods.<details>
<summary>Abstract</summary>
One of the most common things that a genealogist is tasked with is the gathering of a person's initial family history, normally via in-person interviews or with the use of a platform such as ancestry.com, as this can provide a strong foundation upon which a genealogist may build. However, the ability to conduct these interviews can often be hindered by both geographical constraints and the technical proficiency of the interviewee, as the interviewee in these types of interviews is most often an elderly person with a lower than average level of technical proficiency. With this in mind, this study presents what we believe, based on prior research, to be the first chatbot geared entirely towards the gathering of family histories, and explores the viability of utilising such a chatbot by comparing the performance and usability of such a method with the aforementioned alternatives. With a chatbot-based approach, we show that, though the average time taken to conduct an interview may be longer than if the user had used ancestry.com or participated in an in-person interview, the number of mistakes made and the level of confusion from the user regarding the UI and process required is lower than the other two methods. Note that the final metric regarding the user's confusion is not applicable for the in-person interview sessions due to its lack of a UI. With refinement, we believe this use of a chatbot could be a valuable tool for genealogists, especially when dealing with interviewees who are based in other countries where it is not possible to conduct an in-person interview.
</details>
<details>
<summary>摘要</summary>
Our results show that while the average time taken for an interview using the chatbot may be longer than with Ancestry.com or in-person interviews, the number of mistakes made and the level of user confusion is lower with the chatbot. Additionally, the chatbot-based approach could be a valuable tool for genealogists, especially when dealing with interviewees who are based in other countries where in-person interviews are not possible.The chatbot-based approach has several advantages. Firstly, it allows for more flexible and convenient interview scheduling, as the interview can be conducted remotely. Secondly, the chatbot can guide the interviewee through the interview process, reducing the likelihood of mistakes and confusion. Finally, the chatbot can provide a more personalized and interactive experience for the interviewee, which can lead to more accurate and detailed information.However, there are also some limitations to the chatbot-based approach. One potential drawback is that the chatbot may not be able to capture the nuances and complexities of the interviewee's family history in the same way that a human interviewer could. Additionally, the chatbot may not be able to detect and correct errors or inconsistencies in the interviewee's responses in the same way that a human interviewer could.Despite these limitations, we believe that the use of a chatbot for gathering family histories has the potential to be a valuable tool for genealogists. With refinement and further development, the chatbot could be an effective and efficient way to gather accurate and detailed information about a person's family history, especially when dealing with interviewees who are based in other countries.
</details></li>
</ul>
<hr>
<h2 id="When-Do-Discourse-Markers-Affect-Computational-Sentence-Understanding"><a href="#When-Do-Discourse-Markers-Affect-Computational-Sentence-Understanding" class="headerlink" title="When Do Discourse Markers Affect Computational Sentence Understanding?"></a>When Do Discourse Markers Affect Computational Sentence Understanding?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00368">http://arxiv.org/abs/2309.00368</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruiqi Li, Liesbeth Allein, Damien Sileo, Marie-Francine Moens</li>
<li>For: 这篇论文探讨了自然语言处理（NLP）机器学习模型在理解英语 дискурスconnectives方面的能力和用途。* Methods: 作者使用了 nine 种流行的自然语言处理系统来评估这些系统在理解英语 дискурスconnectives方面的能力，并分析了不同连接类型的计算处理复杂性是否与人类处理顺序一致。* Results: 研究发现，NLP系统不一定能够uniformly处理所有的 дискурスconnectives，并且在不同的语言理解任务下，不同的连接类型的计算处理复杂性并不总是一致于人类处理顺序。此外，人类在阅读过程中可能会受到外部影响，但是这并不一定会影响最终的理解性能。系统对连接知识的更多学习，则会增加不当连接的负面影响。这表明，在计算自然语言处理中，正确地表示连接是重要的。<details>
<summary>Abstract</summary>
The capabilities and use cases of automatic natural language processing (NLP) have grown significantly over the last few years. While much work has been devoted to understanding how humans deal with discourse connectives, this phenomenon is understudied in computational systems. Therefore, it is important to put NLP models under the microscope and examine whether they can adequately comprehend, process, and reason within the complexity of natural language. In this chapter, we introduce the main mechanisms behind automatic sentence processing systems step by step and then focus on evaluating discourse connective processing. We assess nine popular systems in their ability to understand English discourse connectives and analyze how context and language understanding tasks affect their connective comprehension. The results show that NLP systems do not process all discourse connectives equally well and that the computational processing complexity of different connective kinds is not always consistently in line with the presumed complexity order found in human processing. In addition, while humans are more inclined to be influenced during the reading procedure but not necessarily in the final comprehension performance, discourse connectives have a significant impact on the final accuracy of NLP systems. The richer knowledge of connectives a system learns, the more negative effect inappropriate connectives have on it. This suggests that the correct explicitation of discourse connectives is important for computational natural language processing.
</details>
<details>
<summary>摘要</summary>
自过去几年来，自然语言处理（NLP）的能力和使用场景已经增长了很多。然而，在人工系统中对对话连接器的研究仍然不够。因此，我们需要把NLP模型放进显微镜中，检查它们是否能够正确地理解、处理和推理natural language的复杂性。在本章中，我们将介绍自动句子处理系统的主要机制一探析，然后专注于评估英文对话连接器的处理能力。我们评估了9种流行的NLP系统，检查它们在处理英文对话连接器时的能力，并分析了上下文和语言理解任务对其连接器理解的影响。结果显示NLP系统不同的连接器Kinds不一定能够正确地处理，而且computational处理复杂性不一定与人类处理的复杂性相符。此外，人类在阅读过程中可能会受到影响，但是在最终理解性能上不一定会受到影响。对NLP系统而言，正确地使用对话连接器是重要的。
</details></li>
</ul>
<hr>
<h2 id="Large-Content-And-Behavior-Models-To-Understand-Simulate-And-Optimize-Content-And-Behavior"><a href="#Large-Content-And-Behavior-Models-To-Understand-Simulate-And-Optimize-Content-And-Behavior" class="headerlink" title="Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior"></a>Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00359">http://arxiv.org/abs/2309.00359</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashmit Khandelwal, Aditya Agrawal, Aanisha Bhattacharyya, Yaman K Singla, Somesh Singh, Uttaran Bhattacharya, Ishita Dasgupta, Stefano Petrangeli, Rajiv Ratn Shah, Changyou Chen, Balaji Krishnamurthy</li>
<li>for: 这 paper 的目的是提出 Large Content and Behavior Models (LCBMs)，用于解决Receiver 的行为 simulation, content simulation, behavior understanding, 和 behavior domain adaptation 等问题。</li>
<li>methods: 这 paper 使用了 Large Language Models (LLMs) 作为基础模型，并在其上添加了 “behavior tokens” 来增强模型的能力。</li>
<li>results: 这 paper 的实验结果表明，LCBMs 可以在多种任务上表现良好，包括内容理解、行为模拟、内容模拟、行为理解和行为适应性等。此外， paper 还发布了一个新的 Content Behavior Corpus (CBC)，用于推动更多的研究。<details>
<summary>Abstract</summary>
Shannon, in his seminal paper introducing information theory, divided the communication into three levels: technical, semantic, and effectivenss. While the technical level is concerned with accurate reconstruction of transmitted symbols, the semantic and effectiveness levels deal with the inferred meaning and its effect on the receiver. Thanks to telecommunications, the first level problem has produced great advances like the internet. Large Language Models (LLMs) make some progress towards the second goal, but the third level still remains largely untouched. The third problem deals with predicting and optimizing communication for desired receiver behavior. LLMs, while showing wide generalization capabilities across a wide range of tasks, are unable to solve for this. One reason for the underperformance could be a lack of "behavior tokens" in LLMs' training corpora. Behavior tokens define receiver behavior over a communication, such as shares, likes, clicks, purchases, retweets, etc. While preprocessing data for LLM training, behavior tokens are often removed from the corpora as noise. Therefore, in this paper, we make some initial progress towards reintroducing behavior tokens in LLM training. The trained models, other than showing similar performance to LLMs on content understanding tasks, show generalization capabilities on behavior simulation, content simulation, behavior understanding, and behavior domain adaptation. Using a wide range of tasks on two corpora, we show results on all these capabilities. We call these models Large Content and Behavior Models (LCBMs). Further, to spur more research on LCBMs, we release our new Content Behavior Corpus (CBC), a repository containing communicator, message, and corresponding receiver behavior.
</details>
<details>
<summary>摘要</summary>
谱在他的著名论文中介绍信息理论时，将通信分为三级：技术、 semantics 和效果。技术级关心已经传输的符号的准确重建，而 semantics 和效果级则关心接收者对符号的理解和对接收者的影响。 благо于电信技术的发展，技术级问题已经取得了很大的进步，如互联网。然而， semantics 和效果级问题仍然未得到充分解决。第三级问题是预测和优化通信以实现 Desired receiver behavior。LLMs 虽然在各种任务上显示了广泛的泛化能力，但它们无法解决这个问题。一个可能的原因是 LLMs 在训练 corpora 中缺乏 "behavior tokens"。 behavior tokens 定义了通信过程中接收者的行为，如分享、赞、点击、购买、 retweet 等。在 Preprocessing 数据 для LLM 训练时，通常会从 corpora 中除掉 behavior tokens 作为噪音。因此，在这篇论文中，我们在 LLM 训练中重新引入 behavior tokens，并训练 Large Content and Behavior Models (LCBMs)。LCBMs 不仅在内容理解任务上显示类似的表现，还能在行为模拟、内容模拟、行为理解和行为预测域中进行泛化。使用两个 corpora 上的各种任务，我们在所有这些能力上显示了结果。我们称这些模型为 Large Content and Behavior Models (LCBMs)。此外，为了促进更多关于 LCBMs 的研究，我们发布了我们的新 Content Behavior Corpus (CBC)，这是一个包含通信者、消息和相应的接收者行为的Repository。
</details></li>
</ul>
<hr>
<h2 id="Comparative-Topic-Modeling-for-Determinants-of-Divergent-Report-Results-Applied-to-Macular-Degeneration-Studies"><a href="#Comparative-Topic-Modeling-for-Determinants-of-Divergent-Report-Results-Applied-to-Macular-Degeneration-Studies" class="headerlink" title="Comparative Topic Modeling for Determinants of Divergent Report Results Applied to Macular Degeneration Studies"></a>Comparative Topic Modeling for Determinants of Divergent Report Results Applied to Macular Degeneration Studies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00312">http://arxiv.org/abs/2309.00312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas Cassiel Jacaruso</li>
<li>for: 本研究旨在透过比较主题模型分析不同报告中对同一研究问题的结果进行比较，以找到具有明确相关性和重要结果的主题。</li>
<li>methods: 本研究使用了主题模型分析方法，对具有相关结果的报告进行分类和排序，以确定具有最高相关性和最高效果的主题。</li>
<li>results: 研究发现了8种补充食品具有显著关系与有效结果的主题，其中6种得到了验证性的证据，即omega-3脂肪酸、氧化铁、萝芽素、兰氨酸、锌和氮氧化物。两种未得到验证（niacin和摩尔丹）也得到了最低分，这表明提议的方法可以用来评价主题的相关性。<details>
<summary>Abstract</summary>
Topic modeling and text mining are subsets of Natural Language Processing with relevance for conducting meta-analysis (MA) and systematic review (SR). For evidence synthesis, the above NLP methods are conventionally used for topic-specific literature searches or extracting values from reports to automate essential phases of SR and MA. Instead, this work proposes a comparative topic modeling approach to analyze reports of contradictory results on the same general research question. Specifically, the objective is to find topics exhibiting distinct associations with significant results for an outcome of interest by ranking them according to their proportional occurrence and consistency of distribution across reports of significant results. The proposed method was tested on broad-scope studies addressing whether supplemental nutritional compounds significantly benefit macular degeneration (MD). Eight compounds were identified as having a particular association with reports of significant results for benefitting MD. Six of these were further supported in terms of effectiveness upon conducting a follow-up literature search for validation (omega-3 fatty acids, copper, zeaxanthin, lutein, zinc, and nitrates). The two not supported by the follow-up literature search (niacin and molybdenum) also had the lowest scores under the proposed methods ranking system, suggesting that the proposed method's score for a given topic is a viable proxy for its degree of association with the outcome of interest. These results underpin the proposed methods potential to add specificity in understanding effects from broad-scope reports, elucidate topics of interest for future research, and guide evidence synthesis in a systematic and scalable way.
</details>
<details>
<summary>摘要</summary>
Topic模型和文本挖掘是自然语言处理的子集，对于进行元分析（MA）和系统性综述（SR）有直接的应用。在证据整合中，这些自然语言处理方法通常用于特定主题的文献搜索或自动化SR和MA的关键阶段。然而，这项工作提出了比较主题模型方法，用于分析对同一个全面研究问题的报告中的不同结果。具体来说，目标是找到具有特定关系的主题，其中结果对于评价预测变量的占比和报告中的分布一致性高。这种方法在对眼肤肉营养剂的研究中进行了测试，并将八种成分确定为对有效结果报告具有特定关系。其中六种（ω-3脂肪酸、铁、杂色素、苷酸、锌和氮原子）在验证性文献搜索中得到了支持，而剩下两种（niacin和硫）则没有得到支持，其分数也相应较低，这表明该方法的分数可以作为一种可靠的评价指标。这些结果证明了该方法的潜在价值，可以增加特定的证据整合，解释特定主题，为未来研究提供指导，并在系统和可扩展的方式下进行证据整合。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-the-vocal-range-of-single-speaker-singing-voice-synthesis-with-melody-unsupervised-pre-training"><a href="#Enhancing-the-vocal-range-of-single-speaker-singing-voice-synthesis-with-melody-unsupervised-pre-training" class="headerlink" title="Enhancing the vocal range of single-speaker singing voice synthesis with melody-unsupervised pre-training"></a>Enhancing the vocal range of single-speaker singing voice synthesis with melody-unsupervised pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00284">http://arxiv.org/abs/2309.00284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaohuan Zhou, Xu Li, Zhiyong Wu, Ying Shan, Helen Meng</li>
<li>for: 提高单声道合唱音色的 vocal range</li>
<li>methods: 使用多种者预训练方法，不影响音色一致性</li>
<li>results: 提高合唱音质和自然性，比基eline更高<details>
<summary>Abstract</summary>
The single-speaker singing voice synthesis (SVS) usually underperforms at pitch values that are out of the singer's vocal range or associated with limited training samples. Based on our previous work, this work proposes a melody-unsupervised multi-speaker pre-training method conducted on a multi-singer dataset to enhance the vocal range of the single-speaker, while not degrading the timbre similarity. This pre-training method can be deployed to a large-scale multi-singer dataset, which only contains audio-and-lyrics pairs without phonemic timing information and pitch annotation. Specifically, in the pre-training step, we design a phoneme predictor to produce the frame-level phoneme probability vectors as the phonemic timing information and a speaker encoder to model the timbre variations of different singers, and directly estimate the frame-level f0 values from the audio to provide the pitch information. These pre-trained model parameters are delivered into the fine-tuning step as prior knowledge to enhance the single speaker's vocal range. Moreover, this work also contributes to improving the sound quality and rhythm naturalness of the synthesized singing voices. It is the first to introduce a differentiable duration regulator to improve the rhythm naturalness of the synthesized voice, and a bi-directional flow model to improve the sound quality. Experimental results verify that the proposed SVS system outperforms the baseline on both sound quality and naturalness.
</details>
<details>
<summary>摘要</summary>
单个 speaker 歌唱voice 合成（SVS）通常在声部范围外或有限训练样本下表现不佳。基于我们的前一项工作，这项工作提出了一种不带预教学样本的多 speaker 预训练方法，以提高单个 speaker 的声部范围，不会影响声音相似性。这种预训练方法可以应用于大规模多 singer 数据集，只包含音频和歌词对应的数据。具体来说，在预训练步骤中，我们设计了一个音频预测器，生成音频帧级别的phoneme 概率 вектор作为声音时间信息，以及一个 speaker 编码器，模型不同 singer 的声音变化，直接从音频中提取帧级别的f0值作为抽象信息。这些预训练模型参数被送入细化步骤作为先验知识，以提高单个 speaker 的声部范围。此外，这项工作还提出了改进合成 singing voice 的音质和节奏自然性的方法，包括引入分配duration 调节器以提高合成声音的节奏自然性，以及bi-directional 流模型以提高音质。实验结果表明，提出的 SVS 系统在音质和自然性两个方面都高于基eline。
</details></li>
</ul>
<hr>
<h2 id="Why-do-universal-adversarial-attacks-work-on-large-language-models-Geometry-might-be-the-answer"><a href="#Why-do-universal-adversarial-attacks-work-on-large-language-models-Geometry-might-be-the-answer" class="headerlink" title="Why do universal adversarial attacks work on large language models?: Geometry might be the answer"></a>Why do universal adversarial attacks work on large language models?: Geometry might be the answer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00254">http://arxiv.org/abs/2309.00254</a></li>
<li>repo_url: None</li>
<li>paper_authors: Varshini Subhash, Anna Bialas, Weiwei Pan, Finale Doshi-Velez</li>
<li>for: 本研究旨在解释大语言模型中对抗攻击的内部机制，尤其是对于 Gradient-based universal adversarial attacks 的理解。</li>
<li>methods: 本研究使用了一种新的几何视角来解释大语言模型中 universal adversarial attacks 的机制。研究者通过对 GPT-2 模型进行攻击，发现了一种可能的观察结果，即攻击触发器可能是 embedding vectors 的一种近似。</li>
<li>results: 研究者通过对 GPT-2 模型进行白盒模型分析，包括维度减少和隐藏表示相似度测量，发现了这种观察结果的证据。这种新的几何视角可能会帮助我们更深入地理解大语言模型的内部工作机制和失效模式，从而实现其防范。<details>
<summary>Abstract</summary>
Transformer based large language models with emergent capabilities are becoming increasingly ubiquitous in society. However, the task of understanding and interpreting their internal workings, in the context of adversarial attacks, remains largely unsolved. Gradient-based universal adversarial attacks have been shown to be highly effective on large language models and potentially dangerous due to their input-agnostic nature. This work presents a novel geometric perspective explaining universal adversarial attacks on large language models. By attacking the 117M parameter GPT-2 model, we find evidence indicating that universal adversarial triggers could be embedding vectors which merely approximate the semantic information in their adversarial training region. This hypothesis is supported by white-box model analysis comprising dimensionality reduction and similarity measurement of hidden representations. We believe this new geometric perspective on the underlying mechanism driving universal attacks could help us gain deeper insight into the internal workings and failure modes of LLMs, thus enabling their mitigation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Detecting-Suicidality-in-Arabic-Tweets-Using-Machine-Learning-and-Deep-Learning-Techniques"><a href="#Detecting-Suicidality-in-Arabic-Tweets-Using-Machine-Learning-and-Deep-Learning-Techniques" class="headerlink" title="Detecting Suicidality in Arabic Tweets Using Machine Learning and Deep Learning Techniques"></a>Detecting Suicidality in Arabic Tweets Using Machine Learning and Deep Learning Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00246">http://arxiv.org/abs/2309.00246</a></li>
<li>repo_url: None</li>
<li>paper_authors: Asma Abdulsalam, Areej Alhothali, Saleh Al-Ghamdi</li>
<li>For: The paper aims to develop a novel dataset of Arabic tweets related to suicidal thoughts and use machine learning and deep learning models to automatically detect suicidal ideation in these tweets.* Methods: The paper uses a variety of machine learning and deep learning models, including Na&quot;ive Bayes, Support Vector Machine, K-Nearest Neighbor, Random Forest, and XGBoost, trained on word frequency and word embedding features, as well as pre-trained deep learning models such as AraBert, AraELECTRA, and AraGPT2, to identify suicidal thoughts in Arabic tweets.* Results: The results show that the SVM and RF models trained on character n-gram features provided the best performance, with an accuracy of 86% and an F1 score of 79%. The AraBert model outperforms other machine and deep learning models, achieving an accuracy of 91% and an F1-score of 88%, significantly improving the detection of suicidal ideation in the Arabic tweets dataset.Here are the three points in Simplified Chinese:* For: 这个论文的目的是开发一个关于自杀思想的阿拉伯语推文数据集，并使用机器学习和深度学习模型自动检测这些推文中的自杀意图。* Methods: 这个论文使用了多种机器学习和深度学习模型，包括Na&quot;ive Bayes、支持向量机、K-近邻 neighbors、Random Forest和XGBoost，使用单词频率和单词嵌入特征进行训练，以及预训练的深度学习模型如AraBert、AraELECTRA和AraGPT2，来识别阿拉伯语推文中的自杀思想。* Results: 结果显示，SVM和RF模型使用单词n-gram特征进行训练时提供了最好的性能，具有86%的准确率和79%的F1分数。AraBert模型在其他机器和深度学习模型之上表现出色，达到91%的准确率和88%的F1分数，显著提高了阿拉伯语推文中自杀意图的检测。<details>
<summary>Abstract</summary>
Social media platforms have revolutionized traditional communication techniques by enabling people globally to connect instantaneously, openly, and frequently. People use social media to share personal stories and express their opinion. Negative emotions such as thoughts of death, self-harm, and hardship are commonly expressed on social media, particularly among younger generations. As a result, using social media to detect suicidal thoughts will help provide proper intervention that will ultimately deter others from self-harm and committing suicide and stop the spread of suicidal ideation on social media. To investigate the ability to detect suicidal thoughts in Arabic tweets automatically, we developed a novel Arabic suicidal tweets dataset, examined several machine learning models, including Na\"ive Bayes, Support Vector Machine, K-Nearest Neighbor, Random Forest, and XGBoost, trained on word frequency and word embedding features, and investigated the ability of pre-trained deep learning models, AraBert, AraELECTRA, and AraGPT2, to identify suicidal thoughts in Arabic tweets. The results indicate that SVM and RF models trained on character n-gram features provided the best performance in the machine learning models, with 86% accuracy and an F1 score of 79%. The results of the deep learning models show that AraBert model outperforms other machine and deep learning models, achieving an accuracy of 91\% and an F1-score of 88%, which significantly improves the detection of suicidal ideation in the Arabic tweets dataset. To the best of our knowledge, this is the first study to develop an Arabic suicidality detection dataset from Twitter and to use deep-learning approaches in detecting suicidality in Arabic posts.
</details>
<details>
<summary>摘要</summary>
社交媒体平台已经革命化了传统的沟通方式，让人们全球协同交流、开放地分享自己的故事和看法。人们通过社交媒体分享自己的个人经历和表达自己的看法。特别是年轻一代，常常在社交媒体上表达自杀的思想和自危的情感。因此，通过社交媒体检测自杀思想可以提供适当的干预措施，ultimately prevent others from self-harm and suicide, and stop the spread of suicidal ideation on social media.为了研究自动检测阿拉伯语自杀思想的能力，我们创建了一个新的阿拉伯语自杀吟话集合，检验了多种机器学习模型，包括顺序规则模型、支持向量机器学习模型、最近邻居模型、随机森林模型和XGBoost模型。我们使用单词频和单词嵌入特征进行训练，并研究了预训练深度学习模型AraBert、AraELECTRA和AraGPT2的能力来识别阿拉伯语自杀思想。结果显示，SVM和RF模型在机器学习模型中表现最佳，具有86%的准确率和79%的F1分数。深度学习模型的结果表明，AraBert模型在其他机器和深度学习模型中表现出色，达到了91%的准确率和88%的F1分数，对阿拉伯语自杀吟话集合的检测提供了显著改善。据我们所知，这是首次从Twitter上创建了阿拉伯语自杀性检测dataset，并使用深度学习方法来检测阿拉伯语自杀思想。
</details></li>
</ul>
<hr>
<h2 id="NeuroSurgeon-A-Toolkit-for-Subnetwork-Analysis"><a href="#NeuroSurgeon-A-Toolkit-for-Subnetwork-Analysis" class="headerlink" title="NeuroSurgeon: A Toolkit for Subnetwork Analysis"></a>NeuroSurgeon: A Toolkit for Subnetwork Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00244">http://arxiv.org/abs/2309.00244</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mlepori1/neurosurgeon">https://github.com/mlepori1/neurosurgeon</a></li>
<li>paper_authors: Michael A. Lepori, Ellie Pavlick, Thomas Serre</li>
<li>for: 了解神经网络模型中学习的算法。</li>
<li>methods: 使用Python库NeuroSurgeon对Transformers库中的模型进行发现和操作。</li>
<li>results: 可以帮助研究人员更好地理解和修改神经网络模型。<details>
<summary>Abstract</summary>
Despite recent advances in the field of explainability, much remains unknown about the algorithms that neural networks learn to represent. Recent work has attempted to understand trained models by decomposing them into functional circuits (Csord\'as et al., 2020; Lepori et al., 2023). To advance this research, we developed NeuroSurgeon, a python library that can be used to discover and manipulate subnetworks within models in the Huggingface Transformers library (Wolf et al., 2019). NeuroSurgeon is freely available at https://github.com/mlepori1/NeuroSurgeon.
</details>
<details>
<summary>摘要</summary>
尽管最近在神经网络解释领域有所进步，仍然有很多关于神经网络学习的表示方法未知。最近的研究尝试了通过分解神经网络模型为功能电路来理解训练后模型（Csordás et al., 2020; Lepori et al., 2023）。为了进一步推进这项研究，我们开发了一个名为NeuroSurgeon的Python库，可以用于发现和操作在Huggingface Transformers库中的子网络（Wolf et al., 2019）。NeuroSurgeon是免费释出的，可以在https://github.com/mlepori1/NeuroSurgeon上下载。
</details></li>
</ul>
<hr>
<h2 id="Image-Hijacks-Adversarial-Images-can-Control-Generative-Models-at-Runtime"><a href="#Image-Hijacks-Adversarial-Images-can-Control-Generative-Models-at-Runtime" class="headerlink" title="Image Hijacks: Adversarial Images can Control Generative Models at Runtime"></a>Image Hijacks: Adversarial Images can Control Generative Models at Runtime</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00236">http://arxiv.org/abs/2309.00236</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/euanong/image-hijacks">https://github.com/euanong/image-hijacks</a></li>
<li>paper_authors: Luke Bailey, Euan Ong, Stuart Russell, Scott Emmons</li>
<li>for: 本研究探讨了基础模型是否具有恶意攻击者的安全性？研究者发现了图像劫驱，即在运行时控制生成模型的恶意图像。</li>
<li>methods: 研究者提出了一种普适的方法 named Behaviour Matching，用于创造图像劫驱。他们使用这种方法来探索三种类型的攻击。</li>
<li>results: 研究者在使用 LLaVA 和 LLaMA-2 模型进行测试时发现，所有的攻击类型都有超过 90% 的成功率。此外，这些攻击都是自动化的，只需要小的图像偏移即可实现。这些发现表明基础模型的安全性存在严重的问题。<details>
<summary>Abstract</summary>
Are foundation models secure from malicious actors? In this work, we focus on the image input to a vision-language model (VLM). We discover image hijacks, adversarial images that control generative models at runtime. We introduce Behaviour Matching, a general method for creating image hijacks, and we use it to explore three types of attacks. Specific string attacks generate arbitrary output of the adversary's choice. Leak context attacks leak information from the context window into the output. Jailbreak attacks circumvent a model's safety training. We study these attacks against LLaVA, a state-of-the-art VLM based on CLIP and LLaMA-2, and find that all our attack types have above a 90% success rate. Moreover, our attacks are automated and require only small image perturbations. These findings raise serious concerns about the security of foundation models. If image hijacks are as difficult to defend against as adversarial examples in CIFAR-10, then it might be many years before a solution is found -- if it even exists.
</details>
<details>
<summary>摘要</summary>
Foundation models 是否安全免受黑客攻击？在这项工作中，我们关注vision-language模型（VLM）中的图像输入。我们发现图像劫持，也就是在运行时使用恶意图像控制生成模型的攻击。我们提出了行为匹配方法，可以创造图像劫持，并使用它来探索三种攻击方式。特定的字符串攻击可以生成对手选择的任意输出。泄露上下文攻击可以从上下文窗口中泄露信息到输出中。囚禁攻击可以绕过模型的安全训练。我们对LLaVA模型，基于CLIP和LLaMA-2，进行了研究，发现所有我们的攻击类型具有超过90%的成功率。此外，我们的攻击是自动化的，只需要小型图像变化即可。这些发现对基础模型的安全提出了严重的问题。如果图像劫持与CIFAR-10中的对抗性例子一样难以防御，那么可能需要很多年才能找到解决方案——如果其even exists。
</details></li>
</ul>
<hr>
<h2 id="JoTR-A-Joint-Transformer-and-Reinforcement-Learning-Framework-for-Dialog-Policy-Learning"><a href="#JoTR-A-Joint-Transformer-and-Reinforcement-Learning-Framework-for-Dialog-Policy-Learning" class="headerlink" title="JoTR: A Joint Transformer and Reinforcement Learning Framework for Dialog Policy Learning"></a>JoTR: A Joint Transformer and Reinforcement Learning Framework for Dialog Policy Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00230">http://arxiv.org/abs/2309.00230</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kwanwaichung/jotr">https://github.com/kwanwaichung/jotr</a></li>
<li>paper_authors: Wai-Chung Kwan, Huimin Wang, Hongru Wang, Zezhong Wang, Xian Wu, Yefeng Zheng, Kam-Fai Wong</li>
<li>for: 本研究的目的是提出一种新的对话政策学习方法，以提高对话模型的性能和多样性。</li>
<li>methods: 本研究使用了一种基于Transformer的文本对话模型，通过Word级对话策略来生成灵活的对话行为。此外，本研究还使用了奖励学习和奖励托管机制来有效地训练对话策略。</li>
<li>results: 经过广泛的评估，本研究的JoTR方法在两个对话模型任务上达到了领先的状态。User simulator和人工评估者都认为JoTR的性能有所提高。<details>
<summary>Abstract</summary>
Dialogue policy learning (DPL) is a crucial component of dialogue modelling. Its primary role is to determine the appropriate abstract response, commonly referred to as the "dialogue action". Traditional DPL methodologies have treated this as a sequential decision problem, using pre-defined action candidates extracted from a corpus. However, these incomplete candidates can significantly limit the diversity of responses and pose challenges when dealing with edge cases, which are scenarios that occur only at extreme operating parameters. To address these limitations, we introduce a novel framework, JoTR. This framework is unique as it leverages a text-to-text Transformer-based model to generate flexible dialogue actions. Unlike traditional methods, JoTR formulates a word-level policy that allows for a more dynamic and adaptable dialogue action generation, without the need for any action templates. This setting enhances the diversity of responses and improves the system's ability to handle edge cases effectively. In addition, JoTR employs reinforcement learning with a reward-shaping mechanism to efficiently finetune the word-level dialogue policy, which allows the model to learn from its interactions, improving its performance over time. We conducted an extensive evaluation of JoTR to assess its effectiveness. Our extensive evaluation shows that JoTR achieves state-of-the-art performance on two benchmark dialogue modelling tasks, as assessed by both user simulators and human evaluators.
</details>
<details>
<summary>摘要</summary>
对话政策学习（DPL）是对话模型的一个重要组件。其主要职责是确定合适的抽象回复，通常称为对话动作。传统的DPL方法ologies treat this as a sequential decision problem, using pre-defined action candidates extracted from a corpus。然而，这些不完整的候选者可以很大地限制对话的多样性和处理边缘情况的能力。为解决这些限制，我们介绍了一个新的框架，JoTR。这个框架独特之处在于它利用了文本到文本的Transformer模型来生成灵活的对话动作。与传统方法不同，JoTR定义了字词级对话政策，允许对话动作生成更加动态和适应性强，无需任何动作模板。这种设置可以提高对话的多样性和对边缘情况的处理能力。此外，JoTR使用了奖励学习和形成机制来有效地训练字词级对话政策，让模型通过与人类互动学习并改进自己的性能。我们进行了广泛的评估，结果显示JoTR在两个标准对话模型任务上达到了当今最佳性能，并且被用户模拟器和人类评估器评为优秀。
</details></li>
</ul>
<hr>
<h2 id="The-FruitShell-French-synthesis-system-at-the-Blizzard-2023-Challenge"><a href="#The-FruitShell-French-synthesis-system-at-the-Blizzard-2023-Challenge" class="headerlink" title="The FruitShell French synthesis system at the Blizzard 2023 Challenge"></a>The FruitShell French synthesis system at the Blizzard 2023 Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00223">http://arxiv.org/abs/2309.00223</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Qi, Xiaopeng Wang, Zhiyong Wang, Wang Liu, Mingming Ding, Shuchen Shi</li>
<li>for: 本研究提出了一个用于2023年Blizzard挑战的法文文本读取系统。这个挑战包括两个任务：生成高品质的女性读者的语音，以及将语音与具体的个人联系起来。</li>
<li>methods: 我们对竞赛数据进行了筛选程序，以移除遗传或错误的文本数据。我们将所有符号除非是phoneme外，并删除没有读音或零时间的符号。此外，我们将文本中的字界限和开始&#x2F;结束符号添加到文本中，以改善语音质量。在Spoke任务中，我们遵循竞赛规则进行数据增强。我们使用了一个开源的G2P模型来将法文转换为phoneme。由于G2P模型使用国际音律字母表（IPA），我们将竞赛数据转换为IPA标示法。但由于编译器无法识别竞赛数据中的特殊符号，我们按照规则将所有phoneme转换为竞赛数据中使用的音律字母表。最后，我们将所有竞赛音频标准化到16 kHz的样本率。</li>
<li>results: 我们的系统在Hub任务中得到了3.6的质量MOS分数，在Spoke任务中得到了3.4的质量MOS分数，与所有参赛队伍的平均水平相当。<details>
<summary>Abstract</summary>
This paper presents a French text-to-speech synthesis system for the Blizzard Challenge 2023. The challenge consists of two tasks: generating high-quality speech from female speakers and generating speech that closely resembles specific individuals. Regarding the competition data, we conducted a screening process to remove missing or erroneous text data. We organized all symbols except for phonemes and eliminated symbols that had no pronunciation or zero duration. Additionally, we added word boundary and start/end symbols to the text, which we have found to improve speech quality based on our previous experience. For the Spoke task, we performed data augmentation according to the competition rules. We used an open-source G2P model to transcribe the French texts into phonemes. As the G2P model uses the International Phonetic Alphabet (IPA), we applied the same transcription process to the provided competition data for standardization. However, due to compiler limitations in recognizing special symbols from the IPA chart, we followed the rules to convert all phonemes into the phonetic scheme used in the competition data. Finally, we resampled all competition audio to a uniform sampling rate of 16 kHz. We employed a VITS-based acoustic model with the hifigan vocoder. For the Spoke task, we trained a multi-speaker model and incorporated speaker information into the duration predictor, vocoder, and flow layers of the model. The evaluation results of our system showed a quality MOS score of 3.6 for the Hub task and 3.4 for the Spoke task, placing our system at an average level among all participating teams.
</details>
<details>
<summary>摘要</summary>
Regarding the competition data, we conducted a screening process to remove missing or erroneous text data. We organized all symbols except for phonemes and eliminated symbols that had no pronunciation or zero duration. Additionally, we added word boundary and start/end symbols to the text, which we have found to improve speech quality based on our previous experience.For the Spoke task, we performed data augmentation according to the competition rules. We used an open-source G2P model to transcribe the French texts into phonemes. As the G2P model uses the International Phonetic Alphabet (IPA), we applied the same transcription process to the provided competition data for standardization. However, due to compiler limitations in recognizing special symbols from the IPA chart, we followed the rules to convert all phonemes into the phonetic scheme used in the competition data.Finally, we resampled all competition audio to a uniform sampling rate of 16 kHz. We employed a VITS-based acoustic model with the hifigan vocoder. For the Spoke task, we trained a multi-speaker model and incorporated speaker information into the duration predictor, vocoder, and flow layers of the model.The evaluation results of our system showed a quality MOS score of 3.6 for the Hub task and 3.4 for the Spoke task, placing our system at an average level among all participating teams.
</details></li>
</ul>
<hr>
<h2 id="Towards-Addressing-the-Misalignment-of-Object-Proposal-Evaluation-for-Vision-Language-Tasks-via-Semantic-Grounding"><a href="#Towards-Addressing-the-Misalignment-of-Object-Proposal-Evaluation-for-Vision-Language-Tasks-via-Semantic-Grounding" class="headerlink" title="Towards Addressing the Misalignment of Object Proposal Evaluation for Vision-Language Tasks via Semantic Grounding"></a>Towards Addressing the Misalignment of Object Proposal Evaluation for Vision-Language Tasks via Semantic Grounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00215">http://arxiv.org/abs/2309.00215</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JoshuaFeinglass/VL-detector-eval">https://github.com/JoshuaFeinglass/VL-detector-eval</a></li>
<li>paper_authors: Joshua Feinglass, Yezhou Yang</li>
<li>for: 本研究旨在探讨目标框生成器在视觉语言任务中的性能评价协议是否准确，以及如何使用语义固定来缓解这一问题。</li>
<li>methods: 我们提出了一种新的评价方法，即根据图像描述文本中的Semantic信息来选择合适的注释 subset，并对这些注释进行评价。</li>
<li>results: 我们的方法能够准确地选择与视觉语言任务相关的注释 subset，并且与现有的评价方法相比，能够更好地反映图像描述文本中的Semantic信息。<details>
<summary>Abstract</summary>
Object proposal generation serves as a standard pre-processing step in Vision-Language (VL) tasks (image captioning, visual question answering, etc.). The performance of object proposals generated for VL tasks is currently evaluated across all available annotations, a protocol that we show is misaligned - higher scores do not necessarily correspond to improved performance on downstream VL tasks. Our work serves as a study of this phenomenon and explores the effectiveness of semantic grounding to mitigate its effects. To this end, we propose evaluating object proposals against only a subset of available annotations, selected by thresholding an annotation importance score. Importance of object annotations to VL tasks is quantified by extracting relevant semantic information from text describing the image. We show that our method is consistent and demonstrates greatly improved alignment with annotations selected by image captioning metrics and human annotation when compared against existing techniques. Lastly, we compare current detectors used in the Scene Graph Generation (SGG) benchmark as a use case, which serves as an example of when traditional object proposal evaluation techniques are misaligned.
</details>
<details>
<summary>摘要</summary>
Object proposal generation acts as a standard pre-processing step in Vision-Language (VL) tasks (image captioning, visual question answering, etc.). Currently, the performance of object proposals generated for VL tasks is evaluated across all available annotations, a protocol that we show is misaligned - higher scores do not necessarily correspond to improved performance on downstream VL tasks. Our work serves as a study of this phenomenon and explores the effectiveness of semantic grounding to mitigate its effects. To this end, we propose evaluating object proposals against only a subset of available annotations, selected by thresholding an annotation importance score. The importance of object annotations to VL tasks is quantified by extracting relevant semantic information from text describing the image. We show that our method is consistent and demonstrates greatly improved alignment with annotations selected by image captioning metrics and human annotation when compared against existing techniques. Finally, we compare current detectors used in the Scene Graph Generation (SGG) benchmark as a use case, which serves as an example of when traditional object proposal evaluation techniques are misaligned.Here's the translation in Traditional Chinese:Object proposal generation acts as a standard pre-processing step in Vision-Language (VL) tasks (image captioning, visual question answering, etc.). Currently, the performance of object proposals generated for VL tasks is evaluated across all available annotations, a protocol that we show is misaligned - higher scores do not necessarily correspond to improved performance on downstream VL tasks. Our work serves as a study of this phenomenon and explores the effectiveness of semantic grounding to mitigate its effects. To this end, we propose evaluating object proposals against only a subset of available annotations, selected by thresholding an annotation importance score. The importance of object annotations to VL tasks is quantified by extracting relevant semantic information from text describing the image. We show that our method is consistent and demonstrates greatly improved alignment with annotations selected by image captioning metrics and human annotation when compared against existing techniques. Finally, we compare current detectors used in the Scene Graph Generation (SGG) benchmark as a use case, which serves as an example of when traditional object proposal evaluation techniques are misaligned.
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-law-of-text-geographic-information"><a href="#Exploring-the-law-of-text-geographic-information" class="headerlink" title="Exploring the law of text geographic information"></a>Exploring the law of text geographic information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00180">http://arxiv.org/abs/2309.00180</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenhua Wang, Daiyu Zhang, Ming Ren, Guang Xu</li>
<li>for: 该论文的目的是探讨文本地理信息的分布特性以及人类使用它的限制。</li>
<li>methods: 该论文采用了严格的实验方法，测试了24种不同语言和类型的地理信息数据集，以验证人类使用地理信息的假设。</li>
<li>results: 研究发现，地理信息呈Gamma分布，其中的量、长度和距离受到人类行为、认知、表达和思维过程的影响。此外，与 Gaussian 分布和Zipf 法律进行了比较，并证明了这些法律的无关性。这些结果可能有助于探索未知的地理信息领域。<details>
<summary>Abstract</summary>
Textual geographic information is indispensable and heavily relied upon in practical applications. The absence of clear distribution poses challenges in effectively harnessing geographic information, thereby driving our quest for exploration. We contend that geographic information is influenced by human behavior, cognition, expression, and thought processes, and given our intuitive understanding of natural systems, we hypothesize its conformity to the Gamma distribution. Through rigorous experiments on a diverse range of 24 datasets encompassing different languages and types, we have substantiated this hypothesis, unearthing the underlying regularities governing the dimensions of quantity, length, and distance in geographic information. Furthermore, theoretical analyses and comparisons with Gaussian distributions and Zipf's law have refuted the contingency of these laws. Significantly, we have estimated the upper bounds of human utilization of geographic information, pointing towards the existence of uncharted territories. Also, we provide guidance in geographic information extraction. Hope we peer its true countenance uncovering the veil of geographic information.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Will-Sentiment-Analysis-Need-Subculture-A-New-Data-Augmentation-Approach"><a href="#Will-Sentiment-Analysis-Need-Subculture-A-New-Data-Augmentation-Approach" class="headerlink" title="Will Sentiment Analysis Need Subculture? A New Data Augmentation Approach"></a>Will Sentiment Analysis Need Subculture? A New Data Augmentation Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00178">http://arxiv.org/abs/2309.00178</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenhua Wang, Simin He, Guang Xu, Ming Ren</li>
<li>for: 强调文化价值和情感分析的研究，addressing the insufficient training data faced by sentiment analysis.</li>
<li>methods: 提出了一种基于子文化表达生成器的数据增强方法（SCDA），通过生成6种不同的子文化表达来生成6种增强文本。</li>
<li>results: 实验证明了SCDA的有效性和可能性，同时也发现了不同子文化表达对情感刺激的不同程度。此外，研究还发现了一种 linear reversibility 的现象，即某些子文化表达可以逆向转换为另一种子文化表达。<details>
<summary>Abstract</summary>
The renowned proverb that "The pen is mightier than the sword" underscores the formidable influence wielded by text expressions in shaping sentiments. Indeed, well-crafted written can deeply resonate within cultures, conveying profound sentiments. Nowadays, the omnipresence of the Internet has fostered a subculture that congregates around the contemporary milieu. The subculture artfully articulates the intricacies of human feelings by ardently pursuing the allure of novelty, a fact that cannot be disregarded in the sentiment analysis. This paper strives to enrich data through the lens of subculture, to address the insufficient training data faced by sentiment analysis. To this end, a new approach of subculture-based data augmentation (SCDA) is proposed, which engenders six enhanced texts for each training text by leveraging the creation of six diverse subculture expression generators. The extensive experiments attest to the effectiveness and potential of SCDA. The results also shed light on the phenomenon that disparate subculture expressions elicit varying degrees of sentiment stimulation. Moreover, an intriguing conjecture arises, suggesting the linear reversibility of certain subculture expressions. It is our fervent aspiration that this study serves as a catalyst in fostering heightened perceptiveness towards the tapestry of information, sentiment and culture, thereby enriching our collective understanding.
</details>
<details>
<summary>摘要</summary>
“著名的谚语“笔子比剑更强”强调了文字表达在形塑情感的力量。实际上，美妙地撰写的文字可以深深地感染到文化中，传递出深刻的情感。在现代社会，互联网的普遍化使得文化 subgroup 形成了一种新的互文化环境，这种环境通过积极追求新鲜的感受，突出了情感分析的不足。为了解决这问题，本文提出了一种基于互文化的数据增强方法（SCDA），通过创造六种不同的互文化表达生成器来生成六个增强的文本。实验证明了SCDA的有效性和潜力。结果还暴露了一种有趣的推测：一些互文化表达的差异会引起不同的情感刺激。此外，这种研究也鼓励我们更加珍惜信息、情感和文化的多样性，以推动我们的共同理解的深化。”
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/01/cs.CL_2023_09_01/" data-id="clpxp6bys00bbee886d3k1ui2" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/01/cs.LG_2023_09_01/" class="article-date">
  <time datetime="2023-09-01T10:00:00.000Z" itemprop="datePublished">2023-09-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/01/cs.LG_2023_09_01/">cs.LG - 2023-09-01</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Universal-Normalization-Enhanced-Graph-Representation-Learning-for-Gene-Network-Prediction"><a href="#Universal-Normalization-Enhanced-Graph-Representation-Learning-for-Gene-Network-Prediction" class="headerlink" title="Universal Normalization Enhanced Graph Representation Learning for Gene Network Prediction"></a>Universal Normalization Enhanced Graph Representation Learning for Gene Network Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00738">http://arxiv.org/abs/2309.00738</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zehao Dong, Muhan Zhang, Qihang Zhao, Philip R. O. Payne, Michael Province, Carlos Cruchaga, Tianyu Zhao, Yixin Chen, Fuhai Li</li>
<li>for: 这篇论文旨在提高生物信息学中的基因网络表示学问题中的表现力，通过对基因网络进行ormalization来提高基因网络表示学模型的稳定性和表现力。</li>
<li>methods: 本文提出了一个 novel UNGNN（通用normalized GNN）框架，它在基因网络中实现了通用的均值normalization，以提高基因网络表示学模型的表现力。</li>
<li>results: 根据实验结果，UNGNN模型在基因网络基础上的表现力比前一代基因网络表示学模型高出16%的表现力。此外，UNGNN模型在其他具有通用均值normalization的图形资料上也实现了superior表现。<details>
<summary>Abstract</summary>
Effective gene network representation learning is of great importance in bioinformatics to predict/understand the relation of gene profiles and disease phenotypes. Though graph neural networks (GNNs) have been the dominant architecture for analyzing various graph-structured data like social networks, their predicting on gene networks often exhibits subpar performance. In this paper, we formally investigate the gene network representation learning problem and characterize a notion of \textit{universal graph normalization}, where graph normalization can be applied in an universal manner to maximize the expressive power of GNNs while maintaining the stability. We propose a novel UNGNN (Universal Normalized GNN) framework, which leverages universal graph normalization in both the message passing phase and readout layer to enhance the performance of a base GNN. UNGNN has a plug-and-play property and can be combined with any GNN backbone in practice. A comprehensive set of experiments on gene-network-based bioinformatical tasks demonstrates that our UNGNN model significantly outperforms popular GNN benchmarks and provides an overall performance improvement of 16 $\%$ on average compared to previous state-of-the-art (SOTA) baselines. Furthermore, we also evaluate our theoretical findings on other graph datasets where the universal graph normalization is solvable, and we observe that UNGNN consistently achieves the superior performance.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本为简化中文。<</SYS>>生物信息学中有效的基因网络表示学习非常重要，以预测/理解基因Profile和疾病表型之间的关系。虽然图神经网络（GNNs）已经在社会网络等多种图结构数据上进行分析，但它们在基因网络上预测通常表现不佳。在这篇论文中，我们正式调查基因网络表示学习问题，并提出了一种通用图Normalization的概念，可以在universal manner中减少图结构数据的表达能力，同时保持稳定。我们提出了一种UNGNN（通用Normalized GNN）框架，它在消息传递阶段和读取层都应用通用图Normalization，以提高基于GNN的表达能力。UNGNN具有插入性的性质，可以与任何GNN脊梁结合使用。我们进行了基于基因网络的生物信息学任务的广泛实验，并证明了我们的UNGNN模型在SOTA基elines上提供了16%的平均性能提升。此外，我们还评估了我们的理论发现在其他可解 Graphdataset上，并发现UNGNN在这些dataset上一直保持了最高表现。
</details></li>
</ul>
<hr>
<h2 id="Prediction-Error-Estimation-in-Random-Forests"><a href="#Prediction-Error-Estimation-in-Random-Forests" class="headerlink" title="Prediction Error Estimation in Random Forests"></a>Prediction Error Estimation in Random Forests</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00736">http://arxiv.org/abs/2309.00736</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iankrupkin/Prediction-Error-Estimation-in-Random-Forests">https://github.com/iankrupkin/Prediction-Error-Estimation-in-Random-Forests</a></li>
<li>paper_authors: Ian Krupkin, Johanna Hardin</li>
<li>for: 这paper主要研究了Random Forests中的错误估计。</li>
<li>methods: 该paper使用了Bates et al. (2023)提出的初始理论框架，对Random Forests中的错误估计进行了理论和实验研究。</li>
<li>results: 研究发现，在分类情况下，Random Forests的预测错误估计比true error rate更加接近。这与Bates et al. (2023)的结论相反，该结论是为логистиelde regression。此外，该结论在不同的错误估计策略（如cross-validation、bagging和数据分割）下都持平。<details>
<summary>Abstract</summary>
In this paper, error estimates of classification Random Forests are quantitatively assessed. Based on the initial theoretical framework built by Bates et al. (2023), the true error rate and expected error rate are theoretically and empirically investigated in the context of a variety of error estimation methods common to Random Forests. We show that in the classification case, Random Forests' estimates of prediction error is closer on average to the true error rate instead of the average prediction error. This is opposite the findings of Bates et al. (2023) which were given for logistic regression. We further show that this result holds across different error estimation strategies such as cross-validation, bagging, and data splitting.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，Random Forests 分类预测错误估计的量化评估。基于Bates et al.（2023）提出的初始理论框架，我们 theoretically和empirically investigate了Random Forests中的真正错误率和预测错误率在不同的错误估计策略中的关系。我们发现在分类 caso，Random Forests 的预测错误估计比true error rate更加接近，而不是average prediction error。这与Bates et al.（2023）关于 logistic regression 的发现相反。我们还证明了这个结果在不同的错误估计策略，如交叉验证、bagging 和数据分割中都是如此。
</details></li>
</ul>
<hr>
<h2 id="Tempestas-ex-machina-A-review-of-machine-learning-methods-for-wavefront-control"><a href="#Tempestas-ex-machina-A-review-of-machine-learning-methods-for-wavefront-control" class="headerlink" title="Tempestas ex machina: A review of machine learning methods for wavefront control"></a>Tempestas ex machina: A review of machine learning methods for wavefront control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00730">http://arxiv.org/abs/2309.00730</a></li>
<li>repo_url: None</li>
<li>paper_authors: J. Fowler, Rico Landman</li>
<li>for: 这篇论文的目的是为了开发和探索用于内部探测地球型行星的适应光学系统，并且探讨这些技术可以帮助我们的适应光学系统实现更高的影像质量。</li>
<li>methods: 这篇论文使用了机器学习技术来改善适应光学系统的波前控制，并且探讨了各种机器学习方法的应用。</li>
<li>results: 这篇论文总结了过去几十年来关于适应光学系统的机器学习方法的研究，并且提出了一些新的机器学习方法来改善适应光学系统的影像质量。<details>
<summary>Abstract</summary>
As we look to the next generation of adaptive optics systems, now is the time to develop and explore the technologies that will allow us to image rocky Earth-like planets; wavefront control algorithms are not only a crucial component of these systems, but can benefit our adaptive optics systems without requiring increased detector speed and sensitivity or more effective and efficient deformable mirrors. To date, most observatories run the workhorse of their wavefront control as a classic integral controller, which estimates a correction from wavefront sensor residuals, and attempts to apply that correction as fast as possible in closed-loop. An integrator of this nature fails to address temporal lag errors that evolve over scales faster than the correction time, as well as vibrations or dynamic errors within the system that are not encapsulated in the wavefront sensor residuals; these errors impact high contrast imaging systems with complex coronagraphs. With the rise in popularity of machine learning, many are investigating applying modern machine learning methods to wavefront control. Furthermore, many linear implementations of machine learning methods (under varying aliases) have been in development for wavefront control for the last 30-odd years. With this work we define machine learning in its simplest terms, explore the most common machine learning methods applied in the context of this problem, and present a review of the literature concerning novel machine learning approaches to wavefront control.
</details>
<details>
<summary>摘要</summary>
为了下一代适应光学系统，现在是时候开发和探索能够捕捉岩石地球类行星的图像技术。扩散前方控制算法不仅是这些系统的关键组件，而且可以为我们的适应光学系统带来更多的好处，无需增加探测器的速度和敏感度，或者更有效和高效的可变镜。至今为止，大多数天文台都运行着 класси的积分控制器，这种控制器根据波前传感器异常值来估算修正，并尽可能快速地应用这些修正。然而，这种积分控制器无法处理时间延迟错误，以及系统中的振荡或动态错误，这些错误会影响高对比图像系统中的复杂卷积。随着机器学习的兴起，许多人正在研究应用现代机器学习方法来控制波前。此外，许多线性实现机器学习方法（以不同的别称出现）已经在波前控制领域进行了30多年的开发。在这项工作中，我们定义机器学习在最简单的形式下，探讨了在这个问题上最常见的机器学习方法，并对Literature concerning novel machine learning approaches to wavefront control.
</details></li>
</ul>
<hr>
<h2 id="Learning-Shared-Safety-Constraints-from-Multi-task-Demonstrations"><a href="#Learning-Shared-Safety-Constraints-from-Multi-task-Demonstrations" class="headerlink" title="Learning Shared Safety Constraints from Multi-task Demonstrations"></a>Learning Shared Safety Constraints from Multi-task Demonstrations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00711">http://arxiv.org/abs/2309.00711</a></li>
<li>repo_url: None</li>
<li>paper_authors: Konwoo Kim, Gokul Swamy, Zuxin Liu, Ding Zhao, Sanjiban Choudhury, Zhiwei Steven Wu</li>
<li>for: 学习安全约束，以避免机器人在完成任务时出现危险行为。</li>
<li>methods: 基于专家示范的安全任务完成方式进行约束学习，通过扩展反奖学习技术来学习约束。</li>
<li>results: 通过多示范的利用，学习出更加紧致的约束，以避免过度保守的约束导致机器人无法完成任务。<details>
<summary>Abstract</summary>
Regardless of the particular task we want them to perform in an environment, there are often shared safety constraints we want our agents to respect. For example, regardless of whether it is making a sandwich or clearing the table, a kitchen robot should not break a plate. Manually specifying such a constraint can be both time-consuming and error-prone. We show how to learn constraints from expert demonstrations of safe task completion by extending inverse reinforcement learning (IRL) techniques to the space of constraints. Intuitively, we learn constraints that forbid highly rewarding behavior that the expert could have taken but chose not to. Unfortunately, the constraint learning problem is rather ill-posed and typically leads to overly conservative constraints that forbid all behavior that the expert did not take. We counter this by leveraging diverse demonstrations that naturally occur in multi-task settings to learn a tighter set of constraints. We validate our method with simulation experiments on high-dimensional continuous control tasks.
</details>
<details>
<summary>摘要</summary>
无论我们想让我们的代理人完成哪个任务在环境中，通常存在共享的安全约束我们希望我们的代理人遵循。例如，无论是制作三明治还是清理桌子，厨房机器人不应该砸碎碗。手动指定这种约束可能会非常时间consuming和容易出错。我们展示了如何从专家示例安全任务完成的方式学习约束。理解来说，我们学习约束，禁止专家完成任务时高度奖励的行为。然而，约束学习问题通常很不充分定义，通常会导致过度保守的约束，禁止专家没有完成的所有行为。我们通过利用多个示例来学习更紧的约束。我们验证我们的方法通过高维连续控制任务的 simulations experiments。
</details></li>
</ul>
<hr>
<h2 id="Randomized-Polar-Codes-for-Anytime-Distributed-Machine-Learning"><a href="#Randomized-Polar-Codes-for-Anytime-Distributed-Machine-Learning" class="headerlink" title="Randomized Polar Codes for Anytime Distributed Machine Learning"></a>Randomized Polar Codes for Anytime Distributed Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00682">http://arxiv.org/abs/2309.00682</a></li>
<li>repo_url: None</li>
<li>paper_authors: Burak Bartan, Mert Pilanci</li>
<li>for: 这个论文是为了提出一种新的分布式计算框架，可以在慢计算节点的情况下进行稳定的线性运算 aproximate 和精确计算。</li>
<li>methods: 该机制基于随机损块和极icode的概念，并提出了一种顺序解码算法，可以处理实数据并保持低计算复杂度。此外， authors 还提出了一种任何时间估计器，可以在可用节点输出集不可解码的情况下产生可靠的估计。</li>
<li>results: authors 通过实践示例，包括大规模矩阵乘法和黑盒优化，证明了这个框架的可扩展性和实用性。 authors 还在无服务器云计算系统上实现了这些方法，并提供了大规模计算结果来证明其扩展性。<details>
<summary>Abstract</summary>
We present a novel distributed computing framework that is robust to slow compute nodes, and is capable of both approximate and exact computation of linear operations. The proposed mechanism integrates the concepts of randomized sketching and polar codes in the context of coded computation. We propose a sequential decoding algorithm designed to handle real valued data while maintaining low computational complexity for recovery. Additionally, we provide an anytime estimator that can generate provably accurate estimates even when the set of available node outputs is not decodable. We demonstrate the potential applications of this framework in various contexts, such as large-scale matrix multiplication and black-box optimization. We present the implementation of these methods on a serverless cloud computing system and provide numerical results to demonstrate their scalability in practice, including ImageNet scale computations.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的分布式计算框架，具有鲁棒性能快速计算节点的特点，可以进行精度和近似计算线性操作。我们的机制兼用随机损害和极码在计算机中的概念。我们提出了一种顺序解码算法，可以处理实数据，并保持低计算复杂性。此外，我们还提供了一个任何时间估计器，可以在可用节点输出集不可解码时产生可靠地估计。我们在不同场景中应用了这些方法，如大规模矩阵乘法和黑盒优化。我们在无服务器云计算系统上实现了这些方法，并提供了数字结果来证明其在实践中的扩展性，包括图像缩放计算。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-deep-learning-for-cosmic-volumes-with-modified-gravity"><a href="#Bayesian-deep-learning-for-cosmic-volumes-with-modified-gravity" class="headerlink" title="Bayesian deep learning for cosmic volumes with modified gravity"></a>Bayesian deep learning for cosmic volumes with modified gravity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00612">http://arxiv.org/abs/2309.00612</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JavierOrjuela/Bayesian-Neural-Net-with-MNFs-for-f-R-">https://github.com/JavierOrjuela/Bayesian-Neural-Net-with-MNFs-for-f-R-</a></li>
<li>paper_authors: Jorge Enrique García-Farieta, Héctor J Hortúa, Francisco-Shu Kitaura</li>
<li>For: This paper aims to extract cosmological parameters from modified gravity (MG) simulations using deep neural networks, with a focus on uncertainty estimations.* Methods: The paper uses Bayesian neural networks (BNNs) with an enriched approximate posterior distribution, and trains the networks with real-space density fields and power-spectra from a suite of 2000 dark matter only particle mesh $N$-body simulations.* Results: The paper finds that BNNs can accurately predict parameters for $\Omega_m$ and $\sigma_8$ and their correlation with the MG parameter, and yields well-calibrated uncertainty estimates. Additionally, the presence of MG parameter leads to a significant degeneracy with $\sigma_8$, and ignoring MG results in a deviation of the relative errors in $\Omega_m$ and $\sigma_8$ by at least $30%$.<details>
<summary>Abstract</summary>
The new generation of galaxy surveys will provide unprecedented data allowing us to test gravity at cosmological scales. A robust cosmological analysis of the large-scale structure demands exploiting the nonlinear information encoded in the cosmic web. Machine Learning techniques provide such tools, however, do not provide a priori assessment of uncertainties. This study aims at extracting cosmological parameters from modified gravity (MG) simulations through deep neural networks endowed with uncertainty estimations. We implement Bayesian neural networks (BNNs) with an enriched approximate posterior distribution considering two cases: one with a single Bayesian last layer (BLL), and another one with Bayesian layers at all levels (FullB). We train both BNNs with real-space density fields and power-spectra from a suite of 2000 dark matter only particle mesh $N$-body simulations including modified gravity models relying on MG-PICOLA covering 256 $h^{-1}$ Mpc side cubical volumes with 128$^3$ particles. BNNs excel in accurately predicting parameters for $\Omega_m$ and $\sigma_8$ and their respective correlation with the MG parameter. We find out that BNNs yield well-calibrated uncertainty estimates overcoming the over- and under-estimation issues in traditional neural networks. We observe that the presence of MG parameter leads to a significant degeneracy with $\sigma_8$ being one of the possible explanations of the poor MG predictions. Ignoring MG, we obtain a deviation of the relative errors in $\Omega_m$ and $\sigma_8$ by at least $30\%$. Moreover, we report consistent results from the density field and power spectra analysis, and comparable results between BLL and FullB experiments which permits us to save computing time by a factor of two. This work contributes in setting the path to extract cosmological parameters from complete small cosmic volumes towards the highly nonlinear regime.
</details>
<details>
<summary>摘要</summary>
新一代星系调查将提供无前例的数据，允许我们在 cosmological  scales 测试 gravitation。在大规模结构中的非线性信息探索需要使用机器学习技术，但这些技术不提供先验知道不确定性的工具。本研究尝试透过深度神经网络（BNN）估计 cosmological 参数，并在这些 BNN 中添加不确定性估计。我们实现了两种情况：一个包括单一的 Bayesian 最后层（BLL），另一个则是在所有层级添加 Bayesian 层（FullB）。我们将这两种 BNN 训练使用 real-space density 场和对应的 power-spectra，这些实验包括 modified gravity 模型，并且覆盖 256 $h^{-1}$ Mpc 方块Volume 2000 个 dark matter 对称运动 mesh  simulate 128$^3$ 个粒子。BNN 能够精准地预测参数，特别是 $\Omega_m$ 和 $\sigma_8$ 的参数，并且与 MG 参数之间的相互关联性。我们发现 BNN 能够提供良好的不确定性估计，并且与传统神经网络相比，这些不确定性估计的问题较小。在忽略 MG 参数时，我们发现 $\Omega_m$ 和 $\sigma_8$ 的相对误差偏移至少 30%。此外，我们发现 density 场和对应的 power-spectra 分析具有相互关联性，并且 FullB 和 BLL 实验具有相互关联性。这些结果表明我们可以将 cosmological 参数从完整的小宇宙体积中提取，并且在非线性 regime 中进行测试。
</details></li>
</ul>
<hr>
<h2 id="Copiloting-the-Copilots-Fusing-Large-Language-Models-with-Completion-Engines-for-Automated-Program-Repair"><a href="#Copiloting-the-Copilots-Fusing-Large-Language-Models-with-Completion-Engines-for-Automated-Program-Repair" class="headerlink" title="Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair"></a>Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00608">http://arxiv.org/abs/2309.00608</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ise-uiuc/Repilot">https://github.com/ise-uiuc/Repilot</a></li>
<li>paper_authors: Yuxiang Wei, Chunqiu Steven Xia, Lingming Zhang<br>for:本文主要用于提高自动程序修复（APR）的效果，尤其是在普通程序语言中进行修复。methods:本文提出了一种名为Repilot的框架，它可以帮助AI“助手”（即大型自然语言模型）生成更有效的补丁。Repilot通过让LLM生成token，然后通过一个完成引擎来约束和改进这些token，以生成更有效的补丁。results:根据对Defects4j 1.2和2.0数据集的评估，Repilot可以 fixes 66和50个漏洞，分别高于基线最佳方案的14和16个漏洞。此外，Repilot可以在给定的生成预算下生成更有效和正确的补丁，比基eline LLM更高效。<details>
<summary>Abstract</summary>
During Automated Program Repair (APR), it can be challenging to synthesize correct patches for real-world systems in general-purpose programming languages. Recent Large Language Models (LLMs) have been shown to be helpful "copilots" in assisting developers with various coding tasks, and have also been directly applied for patch synthesis. However, most LLMs treat programs as sequences of tokens, meaning that they are ignorant of the underlying semantics constraints of the target programming language. This results in plenty of statically invalid generated patches, impeding the practicality of the technique. Therefore, we propose Repilot, a framework to further copilot the AI "copilots" (i.e., LLMs) by synthesizing more valid patches during the repair process. Our key insight is that many LLMs produce outputs autoregressively (i.e., token by token), resembling human writing programs, which can be significantly boosted and guided through a Completion Engine. Repilot synergistically synthesizes a candidate patch through the interaction between an LLM and a Completion Engine, which 1) prunes away infeasible tokens suggested by the LLM and 2) proactively completes the token based on the suggestions provided by the Completion Engine. Our evaluation on a subset of the widely-used Defects4j 1.2 and 2.0 datasets shows that Repilot fixes 66 and 50 bugs, respectively, surpassing the best-performing baseline by 14 and 16 bugs fixed. More importantly, Repilot is capable of producing more valid and correct patches than the base LLM when given the same generation budget.
</details>
<details>
<summary>摘要</summary>
在自动化程序修复（APR）过程中，可能会遇到将程序作为字符序列处理的挑战。最近的大型自然语言模型（LLM）有助于开发者完成各种编程任务，并直接应用于补丁生成。然而，大多数LLM都忽略了目标编程语言的 semantics 约束，从而生成了许多静态无效的补丁，这阻碍了技术的实用性。因此，我们提出了 Repilot，一个框架，以帮助AI " copilots"（即LLM）生成更有效的补丁。我们的关键发现是，许多LLM生成输出以 autoregressive 方式进行（即字符串按字符串顺序），与人类编写程序类似，可以得到显著的提高和指导。Repilot 同时利用了一个 Completion Engine 来完成补丁的生成。其中，1) 筛除 LLM 所提供的不可能的字符，2) 积极地根据 Completion Engine 的建议完成字符。我们对 Defects4j 1.2 和 2.0 数据集进行了评估，发现 Repilot 可以修复 66 和 50 个漏洞，分别高于基eline 的最佳performanced by 14 和 16 个漏洞。此外，Repilot 能够在给定的生成预算下生成更有效和正确的补丁，与基础 LLM 相比。
</details></li>
</ul>
<hr>
<h2 id="Fast-and-Regret-Optimal-Best-Arm-Identification-Fundamental-Limits-and-Low-Complexity-Algorithms"><a href="#Fast-and-Regret-Optimal-Best-Arm-Identification-Fundamental-Limits-and-Low-Complexity-Algorithms" class="headerlink" title="Fast and Regret Optimal Best Arm Identification: Fundamental Limits and Low-Complexity Algorithms"></a>Fast and Regret Optimal Best Arm Identification: Fundamental Limits and Low-Complexity Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00591">http://arxiv.org/abs/2309.00591</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qining Zhang, Lei Ying</li>
<li>for: 本研究考虑了一个Stochastic Multi-armed Bandit（MAB）问题，旨在同时实现两个目标：快速标识优致arm，并在序列中的$T$个轮次中 Maximize reward。</li>
<li>methods: 本文引入了Regret Optimal Best Arm Identification（ROBAI），以解决这两个目标。为了解决ROBAI，我们提出了 $\mathsf{EOCP}$ 算法和其变种，可以在 Gaussian 和通用bandit 中实现 asymptotic 优化的 regret，并在 $\mathcal{O}(\log T)$ 轮次内commit to the optimal arm。</li>
<li>results: 我们还确定了 ROBAI 的下界，表明 $\mathsf{EOCP}$ 算法和其变种是 sample 优化的，并且在适应 stopping time 下可以 дости到 almost sample 优化的性能。numerical results 表明，$\mathsf{EOCP}$ 算法在 comparison 中比 classic $\mathsf{UCB}$ 算法更有优势，这表明过度探索可能是系统性能的障碍。<details>
<summary>Abstract</summary>
This paper considers a stochastic multi-armed bandit (MAB) problem with dual objectives: (i) quick identification and commitment to the optimal arm, and (ii) reward maximization throughout a sequence of $T$ consecutive rounds. Though each objective has been individually well-studied, i.e., best arm identification for (i) and regret minimization for (ii), the simultaneous realization of both objectives remains an open problem, despite its practical importance. This paper introduces \emph{Regret Optimal Best Arm Identification} (ROBAI) which aims to achieve these dual objectives. To solve ROBAI with both pre-determined stopping time and adaptive stopping time requirements, we present the $\mathsf{EOCP}$ algorithm and its variants respectively, which not only achieve asymptotic optimal regret in both Gaussian and general bandits, but also commit to the optimal arm in $\mathcal{O}(\log T)$ rounds with pre-determined stopping time and $\mathcal{O}(\log^2 T)$ rounds with adaptive stopping time. We further characterize lower bounds on the commitment time (equivalent to sample complexity) of ROBAI, showing that $\mathsf{EOCP}$ and its variants are sample optimal with pre-determined stopping time, and almost sample optimal with adaptive stopping time. Numerical results confirm our theoretical analysis and reveal an interesting ``over-exploration'' phenomenon carried by classic $\mathsf{UCB}$ algorithms, such that $\mathsf{EOCP}$ has smaller regret even though it stops exploration much earlier than $\mathsf{UCB}$ ($\mathcal{O}(\log T)$ versus $\mathcal{O}(T)$), which suggests over-exploration is unnecessary and potentially harmful to system performance.
</details>
<details>
<summary>摘要</summary>
本文考虑了一个随机多оружие带刺（MAB）问题，该问题具有两个目标：快速确定优至的枪和最大化奖励 throughout a sequence of $T$ consecutive rounds。虽然每个目标都已经 individually 得到了研究，即最优枪 identification 和 regret 最小化，但是同时实现这两个目标仍然是一个开放的问题，尽管它在实际中具有重要的实际意义。本文引入了 $\emph{Regret Optimal Best Arm Identification}$（ROBAI），以实现这两个目标。为了解决 ROBAI 的问题，我们提出了 $\mathsf{EOCP}$ 算法和其变种，这些算法不仅在 Gaussian 和通用带刺中实现了 asymptotic 最优的 regret，而且在 pre-determined stopping time 和 adaptive stopping time 下可以在 $\mathcal{O}(\log T)$ 轮或 $\mathcal{O}(\log^2 T)$ 轮内commit to the optimal arm。我们还给出了 ROBAI 的下界，证明 $\mathsf{EOCP}$ 和其变种是 sample 优的，并且在 adaptive stopping time 下是 almost sample 优的。实际结果验证了我们的理论分析，并显示了 класси的 $\mathsf{UCB}$ 算法会在 exploration 过程中出现“过度探索”现象，即 $\mathsf{EOCP}$ 在 much earlier than $\mathsf{UCB}$ （$\mathcal{O}(\log T)$  versus $\mathcal{O}(T)$）内stop exploration，从而具有更小的 regret。这表明过度探索可能对系统性能产生负面影响，并且 $\mathsf{EOCP}$ 可能是一个更好的选择。
</details></li>
</ul>
<hr>
<h2 id="PolyGET-Accelerating-Polymer-Simulations-by-Accurate-and-Generalizable-Forcefield-with-Equivariant-Transformer"><a href="#PolyGET-Accelerating-Polymer-Simulations-by-Accurate-and-Generalizable-Forcefield-with-Equivariant-Transformer" class="headerlink" title="PolyGET: Accelerating Polymer Simulations by Accurate and Generalizable Forcefield with Equivariant Transformer"></a>PolyGET: Accelerating Polymer Simulations by Accurate and Generalizable Forcefield with Equivariant Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00585">http://arxiv.org/abs/2309.00585</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Feng, Huan Tran, Aubrey Toland, Binghong Chen, Qi Zhu, Rampi Ramprasad, Chao Zhang</li>
<li>for: 这个论文的目的是开发一种新的聚合物力场模型，以提高聚合物模拟的准确性和效率。</li>
<li>methods: 这个论文使用了一种新的框架 called PolyGET，它使用了通用对称变换器来捕捉聚合物中的复杂量子交互，并且可以在不同的聚合物家族之间进行泛化。</li>
<li>results: 论文的实验结果表明，PolyGET可以在一个大规模的数据集上达到状态的艺术的性能，并且可以在不同的聚合物类型之间进行泛化。此外，PolyGET还可以在大聚合物模拟中实现高精度的DFT方法，而不需要大量的计算资源。<details>
<summary>Abstract</summary>
Polymer simulation with both accuracy and efficiency is a challenging task. Machine learning (ML) forcefields have been developed to achieve both the accuracy of ab initio methods and the efficiency of empirical force fields. However, existing ML force fields are usually limited to single-molecule settings, and their simulations are not robust enough. In this paper, we present PolyGET, a new framework for Polymer Forcefields with Generalizable Equivariant Transformers. PolyGET is designed to capture complex quantum interactions between atoms and generalize across various polymer families, using a deep learning model called Equivariant Transformers. We propose a new training paradigm that focuses exclusively on optimizing forces, which is different from existing methods that jointly optimize forces and energy. This simple force-centric objective function avoids competing objectives between energy and forces, thereby allowing for learning a unified forcefield ML model over different polymer families. We evaluated PolyGET on a large-scale dataset of 24 distinct polymer types and demonstrated state-of-the-art performance in force accuracy and robust MD simulations. Furthermore, PolyGET can simulate large polymers with high fidelity to the reference ab initio DFT method while being able to generalize to unseen polymers.
</details>
<details>
<summary>摘要</summary>
聚合物模拟具有精度和效率是一项复杂的任务。机器学习（ML）力场已经开发出来实现精度和经验力场的同时。然而，现有的ML力场通常只能处理单个分子的设置，其模拟不够稳定。在这篇论文中，我们介绍了PolyGET框架，它是一种新的聚合物力场框架，用于捕捉聚合物中的复杂量子交互作用，并可以通过深度学习模型来泛化到不同的聚合物家族。我们提出了一种新的训练方法，它专门关注优化力，而不是与能量一起优化力和能量的现有方法。这种简单的力中心的目标函数可以避免了能量和力之间的竞争目标，因此允许学习一种综合的ML模型，可以在不同的聚合物家族上学习。我们对24种不同的聚合物类型进行了大规模的数据集测试，并证明了PolyGET在力精度和稳定的MD模拟方面具有状态机器之作。此外，PolyGET可以模拟大分子，并且可以泛化到未经见过的聚合物。
</details></li>
</ul>
<hr>
<h2 id="Laminar-A-New-Serverless-Stream-based-Framework-with-Semantic-Code-Search-and-Code-Completion"><a href="#Laminar-A-New-Serverless-Stream-based-Framework-with-Semantic-Code-Search-and-Code-Completion" class="headerlink" title="Laminar: A New Serverless Stream-based Framework with Semantic Code Search and Code Completion"></a>Laminar: A New Serverless Stream-based Framework with Semantic Code Search and Code Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00584">http://arxiv.org/abs/2309.00584</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zaynab Zahra, Zihao Li, Rosa Filgueira</li>
<li>for: 这篇论文旨在提供一个新的服务器less框架，基于dispel4py parallel流数据流库。</li>
<li>methods: 该框架使用专门的注册表来有效地管理流动工作流和组件，提供了无缝服务器less经验。</li>
<li>results: 该论文使用大语言模型提高了框架，并增加了semantic code搜索、代码概要和代码完成等功能。这种贡献有助于改善服务器less计算的执行，更有效地管理数据流，并提供了价值的工具 для研究人员和实践者。<details>
<summary>Abstract</summary>
This paper introduces Laminar, a novel serverless framework based on dispel4py, a parallel stream-based dataflow library. Laminar efficiently manages streaming workflows and components through a dedicated registry, offering a seamless serverless experience. Leveraging large lenguage models, Laminar enhances the framework with semantic code search, code summarization, and code completion. This contribution enhances serverless computing by simplifying the execution of streaming computations, managing data streams more efficiently, and offering a valuable tool for both researchers and practitioners.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:这篇论文介绍了Laminar，一种基于dispel4py的新的服务器less框架。Laminar通过专门的注册表来有效地管理流处理工作流和组件，为用户提供了无缝服务器less体验。通过大型自然语言模型，Laminar增强了框架，添加了semantic code search、code summarization和code completion等功能。这一贡献提高了服务器less计算，更有效地管理数据流，为研究人员和实践者提供了一个有价值的工具。
</details></li>
</ul>
<hr>
<h2 id="Geometry-Informed-Neural-Operator-for-Large-Scale-3D-PDEs"><a href="#Geometry-Informed-Neural-Operator-for-Large-Scale-3D-PDEs" class="headerlink" title="Geometry-Informed Neural Operator for Large-Scale 3D PDEs"></a>Geometry-Informed Neural Operator for Large-Scale 3D PDEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00583">http://arxiv.org/abs/2309.00583</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zongyi Li, Nikola Borislavov Kovachki, Chris Choy, Boyi Li, Jean Kossaifi, Shourya Prakash Otta, Mohammad Amin Nabian, Maximilian Stadler, Christian Hundt, Kamyar Azizzadenesheli, Anima Anandkumar</li>
<li>for: 这个论文是为了学习大规模partial differential equations的解operator而写的。</li>
<li>methods: 该方法使用了签名距离函数和点云表示输入形状，以及基于图和傅риер体系的神经Operator来学习解operator。</li>
<li>results: 该方法可以高效地应用于大规模流体力学 simulator，并且可以在不同的几何参数下提供高精度的结果。<details>
<summary>Abstract</summary>
We propose the geometry-informed neural operator (GINO), a highly efficient approach to learning the solution operator of large-scale partial differential equations with varying geometries. GINO uses a signed distance function and point-cloud representations of the input shape and neural operators based on graph and Fourier architectures to learn the solution operator. The graph neural operator handles irregular grids and transforms them into and from regular latent grids on which Fourier neural operator can be efficiently applied. GINO is discretization-convergent, meaning the trained model can be applied to arbitrary discretization of the continuous domain and it converges to the continuum operator as the discretization is refined. To empirically validate the performance of our method on large-scale simulation, we generate the industry-standard aerodynamics dataset of 3D vehicle geometries with Reynolds numbers as high as five million. For this large-scale 3D fluid simulation, numerical methods are expensive to compute surface pressure. We successfully trained GINO to predict the pressure on car surfaces using only five hundred data points. The cost-accuracy experiments show a $26,000 \times$ speed-up compared to optimized GPU-based computational fluid dynamics (CFD) simulators on computing the drag coefficient. When tested on new combinations of geometries and boundary conditions (inlet velocities), GINO obtains a one-fourth reduction in error rate compared to deep neural network approaches.
</details>
<details>
<summary>摘要</summary>
我们提出了几何资料驱动的神经运算器（GINO），一种高效的方法来学习巨大数据点解析方程的解析运算器。GINO使用签名距离函数和点云表示的输入形状，以及基于图形和傅立叶架构的神经运算器来学习解析运算器。图形神经运算器可以处理不规则格子，并将其转换为和傅立叶格子的常规隐藏格子，在这之上进行高效的傅立叶神经运算器应用。GINO是精确化易的， meaning the trained model can be applied to any discretization of the continuous domain and it converges to the continuum operator as the discretization is refined.为了实际验证我们的方法在大规模模拟中的性能，我们生成了3D车身对象的industry-standard aerodynamics数据集，其中Reynolds数达到500万。这些大规模3D流体模拟的计算Surface pressure是 computationally expensive的。我们成功地使用GINO预测车身表面压力，只需使用500个数据点。cost-accuracy实验显示GINO与优化的GPU基于computational fluid dynamics（CFD）仿真器相比，在计算抗力系数方面节省了26,000倍的时间成本。当GINO在新的几何和边界条件下进行测试时，它获得了对于深度神经网络方法的一半减少的错误率。
</details></li>
</ul>
<hr>
<h2 id="Consistency-of-Lloyd’s-Algorithm-Under-Perturbations"><a href="#Consistency-of-Lloyd’s-Algorithm-Under-Perturbations" class="headerlink" title="Consistency of Lloyd’s Algorithm Under Perturbations"></a>Consistency of Lloyd’s Algorithm Under Perturbations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00578">http://arxiv.org/abs/2309.00578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhruv Patel, Hui Shen, Shankar Bhamidi, Yufeng Liu, Vladas Pipiras</li>
<li>for: 这个论文主要研究了 Lloyd 算法在不监督学习中的正确性，具体来说是在不知道真实的样本分布情况下，通过预处理管道如спектル方法来学习样本分布，然后使用 Lloyd 算法进行划分。</li>
<li>methods: 这个论文使用了 Lloyd 算法和相关的预处理管道，如спектル方法，来研究不监督学习中的划分问题。</li>
<li>results: 这个论文得出的结果是，对于不监督学习中的样本分布，Lloyd 算法的误分类率是在 $O(\log(n))$ 迭代后下降到某种下界，具体来说是 $\frac{C}{n^{1&#x2F;4}$，其中 $C$ 是一个常数，$n$ 是样本数量。此外，这个论文还提供了一些具体的应用场景，如高维时间序列、多维折射和稀疏网络社区检测等。<details>
<summary>Abstract</summary>
In the context of unsupervised learning, Lloyd's algorithm is one of the most widely used clustering algorithms. It has inspired a plethora of work investigating the correctness of the algorithm under various settings with ground truth clusters. In particular, in 2016, Lu and Zhou have shown that the mis-clustering rate of Lloyd's algorithm on $n$ independent samples from a sub-Gaussian mixture is exponentially bounded after $O(\log(n))$ iterations, assuming proper initialization of the algorithm. However, in many applications, the true samples are unobserved and need to be learned from the data via pre-processing pipelines such as spectral methods on appropriate data matrices. We show that the mis-clustering rate of Lloyd's algorithm on perturbed samples from a sub-Gaussian mixture is also exponentially bounded after $O(\log(n))$ iterations under the assumptions of proper initialization and that the perturbation is small relative to the sub-Gaussian noise. In canonical settings with ground truth clusters, we derive bounds for algorithms such as $k$-means$++$ to find good initializations and thus leading to the correctness of clustering via the main result. We show the implications of the results for pipelines measuring the statistical significance of derived clusters from data such as SigClust. We use these general results to derive implications in providing theoretical guarantees on the misclustering rate for Lloyd's algorithm in a host of applications, including high-dimensional time series, multi-dimensional scaling, and community detection for sparse networks via spectral clustering.
</details>
<details>
<summary>摘要</summary>
在无监督学习框架下，沛罗德算法是最广泛使用的聚类算法之一。它已经引起了许多研究确定算法在不同设定下的正确性。特别是在2016年，简和周在一个sub-Gaussian混合中的$n$个独立样本上显示了沛罗德算法的误分类率是在$O(\log(n))$迭代后经过抑制的，假设初始化正确。然而，在许多应用中，真实的样本是未观察的，需要从数据中学习via预处理管道，如спектраль方法在适当的数据矩阵上。我们表明，在sub-Gaussian混合中的受损样本上，沛罗德算法的误分类率也是在$O(\log(n))$迭代后抑制的，假设初始化正确且受损相对于sub-Gaussian噪声是小的。在 canonical 设定下，我们 derivebounds for algorithms such as $k$-means$++$ to find good initializations and thus leading to the correctness of clustering via the main result.我们显示了这些结果对于pipelines measuring the statistical significance of derived clusters from data such as SigClust的影响。我们使用这些总结来 derive implications for providing theoretical guarantees on the misclustering rate for Lloyd's algorithm in a host of applications, including high-dimensional time series, multi-dimensional scaling, and community detection for sparse networks via spectral clustering.
</details></li>
</ul>
<hr>
<h2 id="Interpretation-of-High-Dimensional-Linear-Regression-Effects-of-Nullspace-and-Regularization-Demonstrated-on-Battery-Data"><a href="#Interpretation-of-High-Dimensional-Linear-Regression-Effects-of-Nullspace-and-Regularization-Demonstrated-on-Battery-Data" class="headerlink" title="Interpretation of High-Dimensional Linear Regression: Effects of Nullspace and Regularization Demonstrated on Battery Data"></a>Interpretation of High-Dimensional Linear Regression: Effects of Nullspace and Regularization Demonstrated on Battery Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00564">http://arxiv.org/abs/2309.00564</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/joachimschaeffer/hdreganalytics">https://github.com/joachimschaeffer/hdreganalytics</a></li>
<li>paper_authors: Joachim Schaeffer, Eric Lenz, William C. Chueh, Martin Z. Bazant, Rolf Findeisen, Richard D. Braatz</li>
<li>for: This paper is written for researchers and practitioners who work with high-dimensional linear regression in various scientific fields, particularly those who deal with discrete measured data of underlying smooth latent processes.</li>
<li>methods: The paper proposes an optimization formulation to compare regression coefficients and to understand the relationship between the nullspace and regularization in high-dimensional linear regression. The authors also use physical engineering knowledge to interpret the regression results.</li>
<li>results: The case studies show that regularization and z-scoring are important design choices that can lead to interpretable regression results, while the combination of the nullspace and regularization can hinder interpretability. Additionally, the paper demonstrates that regression methods that do not produce coefficients orthogonal to the nullspace, such as fused lasso, can improve interpretability.<details>
<summary>Abstract</summary>
High-dimensional linear regression is important in many scientific fields. This article considers discrete measured data of underlying smooth latent processes, as is often obtained from chemical or biological systems. Interpretation in high dimensions is challenging because the nullspace and its interplay with regularization shapes regression coefficients. The data's nullspace contains all coefficients that satisfy $\mathbf{Xw}=\mathbf{0}$, thus allowing very different coefficients to yield identical predictions. We developed an optimization formulation to compare regression coefficients and coefficients obtained by physical engineering knowledge to understand which part of the coefficient differences are close to the nullspace. This nullspace method is tested on a synthetic example and lithium-ion battery data. The case studies show that regularization and z-scoring are design choices that, if chosen corresponding to prior physical knowledge, lead to interpretable regression results. Otherwise, the combination of the nullspace and regularization hinders interpretability and can make it impossible to obtain regression coefficients close to the true coefficients when there is a true underlying linear model. Furthermore, we demonstrate that regression methods that do not produce coefficients orthogonal to the nullspace, such as fused lasso, can improve interpretability. In conclusion, the insights gained from the nullspace perspective help to make informed design choices for building regression models on high-dimensional data and reasoning about potential underlying linear models, which are important for system optimization and improving scientific understanding.
</details>
<details>
<summary>摘要</summary>
高维Linear regression在多个科学领域非常重要。这篇文章考虑了化学或生物系统中的离散测量数据，这些数据通常表示下面的精灵过程。在高维度下解释具有搜索挑战，因为null space和其与正则化的交互会影响回归系数。数据的null space包含所有满足 $\mathbf{Xw=0}$ 的系数，从而允许非常不同的系数导致同样的预测。我们提出了一种优化表述，用于比较回归系数和基于物理工程知识来理解系数差异。这个null space方法在一个 sintetic例和锂离子电池数据上进行了测试。案例研究表明，如果选择合适的正则化和z-scoring，那么回归结果将更加可读性。否则，null space和正则化的交互会使回归结果不可解释，而且在存在真实的下面线性模型时，无法获得正确的系数。此外，我们还示出了不能将系数正交于null space的回归方法，如融合lasso，可以提高可读性。因此，从null space角度获得的知识可以帮助我们做出有用的设计选择，建立回归模型，以便优化系统和提高科学理解。
</details></li>
</ul>
<hr>
<h2 id="Interactive-and-Concentrated-Differential-Privacy-for-Bandits"><a href="#Interactive-and-Concentrated-Differential-Privacy-for-Bandits" class="headerlink" title="Interactive and Concentrated Differential Privacy for Bandits"></a>Interactive and Concentrated Differential Privacy for Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00557">http://arxiv.org/abs/2309.00557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Achraf Azize, Debabrota Basu</li>
<li>for: 这篇论文关注了在中央决策者的情况下保护用户隐私的问题。</li>
<li>methods: 这篇论文使用了交互式敏感度隐私（DP）来保护用户隐私。</li>
<li>results: 论文提供了对固定武器和线性投掷器的最小最大偏差和问题依赖的下界，这些下界表明在不同的隐私预算$\rho$下，$\rho$-全球敏感度隐私对偏差的影响不同。论文还提出了两种$\rho$-全球敏感度隐私投掷算法，即AdaC-UCB和AdaC-GOPE。这两种算法都使用了 Gaussian 机制和适应集。论文分析了这些算法的偏差，并证明了AdaC-UCB实现了问题依赖的下界，而AdaC-GOPE实现了最小最大偏差下界。最后，论文提供了不同设置下的实验 validate 论文的结论。<details>
<summary>Abstract</summary>
Bandits play a crucial role in interactive learning schemes and modern recommender systems. However, these systems often rely on sensitive user data, making privacy a critical concern. This paper investigates privacy in bandits with a trusted centralized decision-maker through the lens of interactive Differential Privacy (DP). While bandits under pure $\epsilon$-global DP have been well-studied, we contribute to the understanding of bandits under zero Concentrated DP (zCDP). We provide minimax and problem-dependent lower bounds on regret for finite-armed and linear bandits, which quantify the cost of $\rho$-global zCDP in these settings. These lower bounds reveal two hardness regimes based on the privacy budget $\rho$ and suggest that $\rho$-global zCDP incurs less regret than pure $\epsilon$-global DP. We propose two $\rho$-global zCDP bandit algorithms, AdaC-UCB and AdaC-GOPE, for finite-armed and linear bandits respectively. Both algorithms use a common recipe of Gaussian mechanism and adaptive episodes. We analyze the regret of these algorithms to show that AdaC-UCB achieves the problem-dependent regret lower bound up to multiplicative constants, while AdaC-GOPE achieves the minimax regret lower bound up to poly-logarithmic factors. Finally, we provide experimental validation of our theoretical results under different settings.
</details>
<details>
<summary>摘要</summary>
匪徒在互动学习方案和现代推荐系统中发挥关键作用，但这些系统经常依赖敏感用户数据，因此隐私成为一个重要问题。这篇论文通过交互式差异private（DP）的视角研究了隐私在匪徒中。虽然纯$\epsilon$-全球DP已经得到了广泛的研究，但我们在zero Concentrated DP（zCDP）下进行了研究。我们提供了基于finite-armed和线性匪徒的最小最大和问题依赖的下界，这些下界量化了在这些设定下的 regret的成本。这些下界显示了两个硬度 режи，即隐私预算$\rho$的硬度和隐私预算$\rho$的硬度。我们还提出了两种$\rho$-全球zCDP匪徒算法，即AdaC-UCB和AdaC-GOPE。这两种算法都使用了共同的 Gaussian mechanism 和自适应集。我们分析了这些算法的 regret，并证明了AdaC-UCB实现了问题依赖的 regret下界，而AdaC-GOPE实现了最小最大的 regret下界。最后，我们在不同的设定下进行了实验验证。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-function-approximation-based-on-the-Discrete-Cosine-Transform-DCT"><a href="#Adaptive-function-approximation-based-on-the-Discrete-Cosine-Transform-DCT" class="headerlink" title="Adaptive function approximation based on the Discrete Cosine Transform (DCT)"></a>Adaptive function approximation based on the Discrete Cosine Transform (DCT)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00530">http://arxiv.org/abs/2309.00530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ana I. Pérez-Neira, Marc Martinez-Gost, Miguel Ángel Lagunas</li>
<li>for: 这篇论文研究了基于恒等函数的一元连续函数近似方法，不具备快速储存的缺点。</li>
<li>methods: 本论文使用了一种supervised学习方法来获取近似系数，而不是使用快速储存变换 (DCT)。</li>
<li>results: 由于cosine基函数的有限动态和正交性，使用这种简单的梯度算法，如正规化最小二乘 (NLMS)，可以获得控制的和预测可靠的融合时间和误差补偿。这种技术的简单性使其成为在更复杂的超级vised学习系统中使用的佳技术。<details>
<summary>Abstract</summary>
This paper studies the cosine as basis function for the approximation of univariate and continuous functions without memory. This work studies a supervised learning to obtain the approximation coefficients, instead of using the Discrete Cosine Transform (DCT). Due to the finite dynamics and orthogonality of the cosine basis functions, simple gradient algorithms, such as the Normalized Least Mean Squares (NLMS), can benefit from it and present a controlled and predictable convergence time and error misadjustment. Due to its simplicity, the proposed technique ranks as the best in terms of learning quality versus complexity, and it is presented as an attractive technique to be used in more complex supervised learning systems. Simulations illustrate the performance of the approach. This paper celebrates the 50th anniversary of the publication of the DCT by Nasir Ahmed in 1973.
</details>
<details>
<summary>摘要</summary>
这篇论文研究了无记忆函数的cosine作为基函数，用于精度地近似单变量连续函数。这项研究使用了指导学习而不是使用抽象 cosine transform (DCT) 来获取近似系数。由于cosine基函数的有限动态和正交性，简单的梯度算法，如normalized least squares (NLMS)，可以从中受益，并且可以控制和预测误差补偿的时间和误差。由于其简单性，该技术在学习质量 versus 复杂性方面排名最高，并且作为更复杂的超visisted learning系统中的一种吸引人技术。实验表明了该方法的性能。这篇论文纪念1973年Nasir Ahmed发表的《抽象cosine transform》50周年。
</details></li>
</ul>
<hr>
<h2 id="Online-Distributed-Learning-over-Random-Networks"><a href="#Online-Distributed-Learning-over-Random-Networks" class="headerlink" title="Online Distributed Learning over Random Networks"></a>Online Distributed Learning over Random Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00520">http://arxiv.org/abs/2309.00520</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Nicola Bastianello, Diego Deplano, Mauro Franceschelli, Karl H. Johansson</li>
<li>for: 本研究的目的是解决分布式学习问题，特别是在多代理系统中，代理不直接分享数据，而是通过协作来学习模型。</li>
<li>methods: 本研究使用了分布式运算理论（DOT）版本的分解方向方法（ADMM），称为DOT-ADMM算法，以解决在线学习、异步代理计算、不可靠和有限通信等实际问题。</li>
<li>results: 本研究证明了DOT-ADMM算法在一类凸学习问题（如线性和启发式回归问题）中 converge linear rate，并Characterize了它们的解决方案如何受到（i）到（iv）的影响。在数值实验中，DOT-ADMM算法与其他当前状态算法进行比较，显示它具有对（i）到（iv）的Robustness。<details>
<summary>Abstract</summary>
The recent deployment of multi-agent systems in a wide range of scenarios has enabled the solution of learning problems in a distributed fashion. In this context, agents are tasked with collecting local data and then cooperatively train a model, without directly sharing the data. While distributed learning offers the advantage of preserving agents' privacy, it also poses several challenges in terms of designing and analyzing suitable algorithms. This work focuses specifically on the following challenges motivated by practical implementation: (i) online learning, where the local data change over time; (ii) asynchronous agent computations; (iii) unreliable and limited communications; and (iv) inexact local computations. To tackle these challenges, we introduce the Distributed Operator Theoretical (DOT) version of the Alternating Direction Method of Multipliers (ADMM), which we call the DOT-ADMM Algorithm. We prove that it converges with a linear rate for a large class of convex learning problems (e.g., linear and logistic regression problems) toward a bounded neighborhood of the optimal time-varying solution, and characterize how the neighborhood depends on~$\text{(i)--(iv)}$. We corroborate the theoretical analysis with numerical simulations comparing the DOT-ADMM Algorithm with other state-of-the-art algorithms, showing that only the proposed algorithm exhibits robustness to (i)--(iv).
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Online learning: Local data changes over time.2. Asynchronous agent computations.3. Unreliable and limited communications.4. Inexact local computations.To address these challenges, we introduce the Distributed Operator Theoretical (DOT) version of the Alternating Direction Method of Multipliers (ADMM), which we call the DOT-ADMM algorithm. We prove that the DOT-ADMM algorithm converges with a linear rate for a wide range of convex learning problems (such as linear and logistic regression problems) and shows robustness to (i)–(iv). Numerical simulations comparing the DOT-ADMM algorithm with other state-of-the-art algorithms demonstrate its superior performance in these challenging scenarios.</details></li>
</ol>
<hr>
<h2 id="Solving-multiscale-elliptic-problems-by-sparse-radial-basis-function-neural-networks"><a href="#Solving-multiscale-elliptic-problems-by-sparse-radial-basis-function-neural-networks" class="headerlink" title="Solving multiscale elliptic problems by sparse radial basis function neural networks"></a>Solving multiscale elliptic problems by sparse radial basis function neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03107">http://arxiv.org/abs/2309.03107</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiwen Wang, Minxin Chen, Jingrun Chen</li>
<li>for: 解决多スケール elliptic  partialling differential equations (PDEs) 的问题</li>
<li>methods: 使用 sparse radial basis function neural network (RBFNN) 方法，启发自 deep mixed residual method，将二次问题转化为一次系统，并使用多个 RBFNN 来近似未知函数</li>
<li>results: 提出了一种新的 $\ell_1$ regularization 技术，可以避免过拟合问题，并且可以在三维空间中提供可靠的数值解决方案，并且比较稳定和精度高于大多数其他可算法。<details>
<summary>Abstract</summary>
Machine learning has been successfully applied to various fields of scientific computing in recent years. In this work, we propose a sparse radial basis function neural network method to solve elliptic partial differential equations (PDEs) with multiscale coefficients. Inspired by the deep mixed residual method, we rewrite the second-order problem into a first-order system and employ multiple radial basis function neural networks (RBFNNs) to approximate unknown functions in the system. To aviod the overfitting due to the simplicity of RBFNN, an additional regularization is introduced in the loss function. Thus the loss function contains two parts: the $L_2$ loss for the residual of the first-order system and boundary conditions, and the $\ell_1$ regularization term for the weights of radial basis functions (RBFs). An algorithm for optimizing the specific loss function is introduced to accelerate the training process. The accuracy and effectiveness of the proposed method are demonstrated through a collection of multiscale problems with scale separation, discontinuity and multiple scales from one to three dimensions. Notably, the $\ell_1$ regularization can achieve the goal of representing the solution by fewer RBFs. As a consequence, the total number of RBFs scales like $\mathcal{O}(\varepsilon^{-n\tau})$, where $\varepsilon$ is the smallest scale, $n$ is the dimensionality, and $\tau$ is typically smaller than $1$. It is worth mentioning that the proposed method not only has the numerical convergence and thus provides a reliable numerical solution in three dimensions when a classical method is typically not affordable, but also outperforms most other available machine learning methods in terms of accuracy and robustness.
</details>
<details>
<summary>摘要</summary>
Machine learning 在近年sciences computing中得到了成功应用。在这项工作中，我们提议使用稀疏卷积基函数神经网络方法解析各种具有多级别系数的几何 partial differential equations (PDEs)。受深混合异常方法的激发，我们将第二阶问题重写为第一阶系统，并使用多个卷积基函数神经网络（RBFNNs）来近似未知函数。为了避免RBFNN的过拟合，我们在损失函数中添加了一个 $\ell_1$ 规范项。因此，损失函数包含 $L_2$ 损失项和边界条件，以及 $\ell_1$ 规范项。我们提出了一种优化特定损失函数的算法，以加速训练过程。我们通过一系列具有多个级别、离散和多级别的问题来证明方法的准确性和有效性。值得注意的是，$\ell_1$ 规范可以实现函数的折衔，从而使得总的RBF数量 scales like $\mathcal{O}(\varepsilon^{-n\tau})$，其中 $\varepsilon$ 是最小的尺度，$n$ 是维度，$\tau$ 通常小于 1。此外，我们的方法不仅具有数值收敛性，可以在三维空间提供可靠的数值解决方案，而且在精度和稳定性方面超越大多数可用的机器学习方法。
</details></li>
</ul>
<hr>
<h2 id="Structure-and-Gradient-Dynamics-Near-Global-Minima-of-Two-layer-Neural-Networks"><a href="#Structure-and-Gradient-Dynamics-Near-Global-Minima-of-Two-layer-Neural-Networks" class="headerlink" title="Structure and Gradient Dynamics Near Global Minima of Two-layer Neural Networks"></a>Structure and Gradient Dynamics Near Global Minima of Two-layer Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00508">http://arxiv.org/abs/2309.00508</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leyang Zhang, Yaoyu Zhang, Tao Luo</li>
<li>for: 研究两层神经网络的损失地形结构，特别是在全局最优点附近，并确定可以达到完美泛化的参数集。</li>
<li>methods: 使用新的技术来探索复杂的损失地形，并发现模型、目标函数、样本和初始化对训练动态的影响不同。</li>
<li>results: 研究发现，（过参数化）神经网络可以很好地泛化，并且解释了这种能力的原因。<details>
<summary>Abstract</summary>
Under mild assumptions, we investigate the structure of loss landscape of two-layer neural networks near global minima, determine the set of parameters which give perfect generalization, and fully characterize the gradient flows around it. With novel techniques, our work uncovers some simple aspects of the complicated loss landscape and reveals how model, target function, samples and initialization affect the training dynamics differently. Based on these results, we also explain why (overparametrized) neural networks could generalize well.
</details>
<details>
<summary>摘要</summary>
“我们在两层神经网络附近全球最佳点下调查损失地图的结构，决定具有完美泛化的参数集，并将渐进流动 vollständigCharacterize。我们的工作发现了一些简单的损失地图特性，并说明了模型、目标函数、样本和初始化对训练动态的不同影响。根据这些结果，我们也解释了为什么（过 Parametrization）神经网络具有良好的泛化能力。”Here's the breakdown of the translation:* “Under mild assumptions” becomes “在几何上的假设下” (shì yǐ jī hòu yǐ jī)* “we investigate the structure of loss landscape of two-layer neural networks” becomes “我们调查两层神经网络损失地图的结构” (wǒ men zhù chá yī liàng jī nǎo wǎng jī)* “near global minima” becomes “附近全球最佳点” (pò jìn qū jiāo zhì diǎn)* “determine the set of parameters which give perfect generalization” becomes “决定具有完美泛化的参数集” (jī dìng shì yǐ jī de fāng yì)* “and fully characterize the gradient flows around it” becomes “并将渐进流动 vollständigCharacterize” (bìng shì yǐ jī de jì qiǎo yǐ jī)* “With novel techniques” becomes “使用新的技术” (shǐ yòu xīn de jì huì)* “our work uncovers some simple aspects of the complicated loss landscape” becomes “我们的工作发现了一些简单的损失地图特性” (wǒ men de gōng zuò fā xiàn le yī si xiǎng xīn de zhòng jī)* “and reveals how model, target function, samples and initialization affect the training dynamics differently” becomes “并说明了模型、目标函数、样本和初始化对训练动态的不同影响” (bìng shuō mìng le mó delǐ, mù bìng funcție, yàng bèi hé chū shì yì jī)* “Based on these results, we also explain why (overparametrized) neural networks could generalize well” becomes “根据这些结果，我们也解释了为什么（过 Parametrization）神经网络具有良好的泛化能力” (gēn jī yǐ jī de, wǒ men yě also jiě shì le, shì zhè yǐ jī de, zhòng yì de, zhòng yì de)
</details></li>
</ul>
<hr>
<h2 id="Application-of-Deep-Learning-Methods-in-Monitoring-and-Optimization-of-Electric-Power-Systems"><a href="#Application-of-Deep-Learning-Methods-in-Monitoring-and-Optimization-of-Electric-Power-Systems" class="headerlink" title="Application of Deep Learning Methods in Monitoring and Optimization of Electric Power Systems"></a>Application of Deep Learning Methods in Monitoring and Optimization of Electric Power Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00498">http://arxiv.org/abs/2309.00498</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ognjen Kundacina</li>
<li>for: 这个博士论文探讨了深度学习技术在电力系统监测和优化方面的应用，以提高电力系统状态估计和动态分布网络重新配置。</li>
<li>methods: 本论文使用图神经网络进行电力系统状态估计的提升，并使用强化学习进行动态分布网络重新配置。</li>
<li>results: 经过广泛的实验和仿真，提出的方法得到了证明，并且在电力系统监测和优化方面表现出色。<details>
<summary>Abstract</summary>
This PhD thesis thoroughly examines the utilization of deep learning techniques as a means to advance the algorithms employed in the monitoring and optimization of electric power systems. The first major contribution of this thesis involves the application of graph neural networks to enhance power system state estimation. The second key aspect of this thesis focuses on utilizing reinforcement learning for dynamic distribution network reconfiguration. The effectiveness of the proposed methods is affirmed through extensive experimentation and simulations.
</details>
<details>
<summary>摘要</summary>
这个博士论文全面检讨了深度学习技术在电力系统监测和优化方面的应用。本论文的第一个重要贡献是通过图 neural network 提高电力系统状态估计的精度。第二个关键方面是通过强化学习来动态重新配置分布网络。实验和 simulations 证明了提议的方法的有效性。Here's the breakdown of the translation:* 这个博士论文 (zhè ge bóshì zhōngzì) - This PhD thesis* 全面检讨 (quánxiàn jiǎnzhèng) - thoroughly examines* 深度学习技术 (shēn dào xuéxí jìshù) - deep learning techniques* 在电力系统监测和优化方面 (zhī yì electric power systems) - in the monitoring and optimization of electric power systems* 应用 (yìngzuò) - application* 第一个重要贡献 (dì yī jī zhòngyì) - the first major contribution* 通过图 neural network (tōngguò graphein neural network) - through the use of graph neural networks* 提高电力系统状态估计的精度 (jīngdé electric power system state estimation) - to enhance the accuracy of power system state estimation* 第二个关键方面 (dì èr jiānjiāng fāngbiàn) - the second key aspect* 通过强化学习来动态重新配置分布网络 (tōngguò qiánghuà xuéxí jiào dòngxīn zhòngxīn) - through the use of reinforcement learning to dynamically reconfigure the distribution network* 实验和 simulations (shìyàn yǔ simulated) - experimental and simulated* 证明了 (jiànming le) - prove the effectiveness of* 提议的方法 (tiěyì de fāngzhì) - the proposed methods
</details></li>
</ul>
<hr>
<h2 id="How-Does-Forecasting-Affect-the-Convergence-of-DRL-Techniques-in-O-RAN-Slicing"><a href="#How-Does-Forecasting-Affect-the-Convergence-of-DRL-Techniques-in-O-RAN-Slicing" class="headerlink" title="How Does Forecasting Affect the Convergence of DRL Techniques in O-RAN Slicing?"></a>How Does Forecasting Affect the Convergence of DRL Techniques in O-RAN Slicing?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00489">http://arxiv.org/abs/2309.00489</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmad M. Nagib, Hatem Abou-Zeid, Hossam S. Hassanein</li>
<li>For: This paper focuses on improving the convergence of deep reinforcement learning (DRL) agents in open radio access network (O-RAN) architectures, specifically for immersive applications such as virtual reality (VR) gaming and metaverse services.* Methods: The authors use time series forecasting of traffic demands to improve the convergence of the DRL-based slicing agents. They propose a novel forecasting-aided DRL approach and provide an exhaustive experiment that supports multiple services, including real VR gaming traffic.* Results: The proposed approach shows significant improvements in the average initial reward value, convergence rate, and number of converged scenarios compared to the implemented baselines. The results also demonstrate the approach’s robustness against forecasting errors and the feasibility of using imperfect forecasting models.<details>
<summary>Abstract</summary>
The success of immersive applications such as virtual reality (VR) gaming and metaverse services depends on low latency and reliable connectivity. To provide seamless user experiences, the open radio access network (O-RAN) architecture and 6G networks are expected to play a crucial role. RAN slicing, a critical component of the O-RAN paradigm, enables network resources to be allocated based on the needs of immersive services, creating multiple virtual networks on a single physical infrastructure. In the O-RAN literature, deep reinforcement learning (DRL) algorithms are commonly used to optimize resource allocation. However, the practical adoption of DRL in live deployments has been sluggish. This is primarily due to the slow convergence and performance instabilities suffered by the DRL agents both upon initial deployment and when there are significant changes in network conditions. In this paper, we investigate the impact of time series forecasting of traffic demands on the convergence of the DRL-based slicing agents. For that, we conduct an exhaustive experiment that supports multiple services including real VR gaming traffic. We then propose a novel forecasting-aided DRL approach and its respective O-RAN practical deployment workflow to enhance DRL convergence. Our approach shows up to 22.8%, 86.3%, and 300% improvements in the average initial reward value, convergence rate, and number of converged scenarios respectively, enhancing the generalizability of the DRL agents compared with the implemented baselines. The results also indicate that our approach is robust against forecasting errors and that forecasting models do not have to be ideal.
</details>
<details>
<summary>摘要</summary>
成功的 immerse 应用，如虚拟现实 (VR) 游戏和 metaverse 服务，需要低延迟和可靠的连接。为提供无缝用户体验，开放无线访问网络 (O-RAN) 架构和 sixth-generation 网络 (6G) 将扮演关键角色。RAN 分割，O-RAN 架构中的一个关键组件，允许网络资源根据 immerse 服务的需求进行分配，在单个物理基础设施上创建多个虚拟网络。在 O-RAN 文献中，深度强化学习 (DRL) 算法广泛用于资源分配优化。然而，实际应用中 DRL 的普及率较低。这主要是由 DRL 代理人在部署时和网络条件变化时表现缓慢和性能不稳定所致。在本文中，我们研究了基于时间序列预测的吞吐量需求对 DRL-based 分割代理人的 converges 的影响。为此，我们进行了详细的实验，支持多种服务，包括真实的 VR 游戏流量。然后，我们提出了一种 forecasting-aided DRL 方法和其相应的 O-RAN 实践部署工作流程，以提高 DRL 代理人的 converges。我们的方法显示在初始奖励值、 converges 速度和 converged 场景数量上增加了22.8%、86.3% 和 300%，从而提高了 DRL 代理人的通用性。结果还表明，我们的方法对预测错误有较好的Robustness，并且预测模型不需要 идеal。
</details></li>
</ul>
<hr>
<h2 id="Geometry-aware-Line-Graph-Transformer-Pre-training-for-Molecular-Property-Prediction"><a href="#Geometry-aware-Line-Graph-Transformer-Pre-training-for-Molecular-Property-Prediction" class="headerlink" title="Geometry-aware Line Graph Transformer Pre-training for Molecular Property Prediction"></a>Geometry-aware Line Graph Transformer Pre-training for Molecular Property Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00483">http://arxiv.org/abs/2309.00483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peizhen Bai, Xianyuan Liu, Haiping Lu</li>
<li>for: 提高分子表示学习的精度，增强分子功能预测的能力</li>
<li>methods: 使用自我超vised学习方法，通过2D和3D模式提取分子特征信息</li>
<li>results: 与6个基eline比较，在12个性能测试上 consistently outperform所有基eline，证明其效果<details>
<summary>Abstract</summary>
Molecular property prediction with deep learning has gained much attention over the past years. Owing to the scarcity of labeled molecules, there has been growing interest in self-supervised learning methods that learn generalizable molecular representations from unlabeled data. Molecules are typically treated as 2D topological graphs in modeling, but it has been discovered that their 3D geometry is of great importance in determining molecular functionalities. In this paper, we propose the Geometry-aware line graph transformer (Galformer) pre-training, a novel self-supervised learning framework that aims to enhance molecular representation learning with 2D and 3D modalities. Specifically, we first design a dual-modality line graph transformer backbone to encode the topological and geometric information of a molecule. The designed backbone incorporates effective structural encodings to capture graph structures from both modalities. Then we devise two complementary pre-training tasks at the inter and intra-modality levels. These tasks provide properly supervised information and extract discriminative 2D and 3D knowledge from unlabeled molecules. Finally, we evaluate Galformer against six state-of-the-art baselines on twelve property prediction benchmarks via downstream fine-tuning. Experimental results show that Galformer consistently outperforms all baselines on both classification and regression tasks, demonstrating its effectiveness.
</details>
<details>
<summary>摘要</summary>
молекулярная свойство предсказание с глубоким обучением получила много внимания в последние годы. из-за scarcity of labeled molecules, there has been growing interest in self-supervised learning methods that learn generalizable molecular representations from unlabeled data. Molecules are typically treated as 2D topological graphs in modeling, but it has been discovered that their 3D geometry is of great importance in determining molecular functionalities. In this paper, we propose the Geometry-aware line graph transformer (Galformer) pre-training, a novel self-supervised learning framework that aims to enhance molecular representation learning with 2D and 3D modalities. Specifically, we first design a dual-modality line graph transformer backbone to encode the topological and geometric information of a molecule. The designed backbone incorporates effective structural encodings to capture graph structures from both modalities. Then we devise two complementary pre-training tasks at the inter and intra-modality levels. These tasks provide properly supervised information and extract discriminative 2D and 3D knowledge from unlabeled molecules. Finally, we evaluate Galformer against six state-of-the-art baselines on twelve property prediction benchmarks via downstream fine-tuning. Experimental results show that Galformer consistently outperforms all baselines on both classification and regression tasks, demonstrating its effectiveness.
</details></li>
</ul>
<hr>
<h2 id="Polynomial-Model-Based-Optimization-for-Blackbox-Objectives"><a href="#Polynomial-Model-Based-Optimization-for-Blackbox-Objectives" class="headerlink" title="Polynomial-Model-Based Optimization for Blackbox Objectives"></a>Polynomial-Model-Based Optimization for Blackbox Objectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00663">http://arxiv.org/abs/2309.00663</a></li>
<li>repo_url: None</li>
<li>paper_authors: Janina Schreiber, Damar Wicaksono, Michael Hecht</li>
<li>For: 这个论文是为了解决黑盒优化问题而写的，黑盒优化是指很多系统的结构是未知的，并且使用泛型模型来优化这些系统以实现最佳性。* Methods: 这篇论文提出了一种新的黑盒优化算法，即Polynomial-Model-Based Optimization（PMBO），它使用了 bayesian 优化的思想，通过逐步更新模型，以实现平衡探索和利用率，同时提供了模型不确定性的估计。* Results: 论文通过对一些人工、分析函数进行比较，展示了 PMBO 与其他当前领先算法相比，具有竞争力，甚至在某些情况下超越了它们。因此，作者认为 PMBO 是解决黑盒优化问题的首选方法。<details>
<summary>Abstract</summary>
For a wide range of applications the structure of systems like Neural Networks or complex simulations, is unknown and approximation is costly or even impossible. Black-box optimization seeks to find optimal (hyper-) parameters for these systems such that a pre-defined objective function is minimized. Polynomial-Model-Based Optimization (PMBO) is a novel blackbox optimizer that finds the minimum by fitting a polynomial surrogate to the objective function.   Motivated by Bayesian optimization the model is iteratively updated according to the acquisition function Expected Improvement, thus balancing the exploitation and exploration rate and providing an uncertainty estimate of the model. PMBO is benchmarked against other state-of-the-art algorithms for a given set of artificial, analytical functions. PMBO competes successfully with those algorithms and even outperforms all of them in some cases. As the results suggest, we believe PMBO is the pivotal choice for solving blackbox optimization tasks occurring in a wide range of disciplines.
</details>
<details>
<summary>摘要</summary>
Motivated by Bayesian optimization, the model is iteratively updated according to the acquisition function Expected Improvement, balancing the exploitation and exploration rate and providing an uncertainty estimate of the model. PMBO is benchmarked against other state-of-the-art algorithms for a given set of artificial, analytical functions. PMBO competes successfully with those algorithms and even outperforms all of them in some cases. As the results suggest, we believe PMBO is the pivotal choice for solving blackbox optimization tasks occurring in a wide range of disciplines.Translated into Simplified Chinese:为许多应用领域，系统如神经网络或复杂的仿真模型的结构未知，并且估算是不可能或者很Expensive。黑盒优化寻找这些系统的优化参数，以实现预定的目标函数的最小化。Polynomial-Model-Based Optimization (PMBO) 是一种新的黑盒优化算法，通过适应 polynomial 模型来拟合目标函数。受 Bayesian 优化的 inspiritation，模型在 Expected Improvement 的优化函数下进行逐步更新，平衡利用率和探索率，并提供模型的不确定性估计。PMBO 与其他状态对照算法进行比较，在一组人工、分析函数上得到了竞争性的成绩，甚至在一些情况下超越了所有其他算法。据结果显示，我们认为 PMBO 是解决黑盒优化问题的绝佳选择，在许多领域中得到广泛的应用。
</details></li>
</ul>
<hr>
<h2 id="A-Locality-based-Neural-Solver-for-Optical-Motion-Capture"><a href="#A-Locality-based-Neural-Solver-for-Optical-Motion-Capture" class="headerlink" title="A Locality-based Neural Solver for Optical Motion Capture"></a>A Locality-based Neural Solver for Optical Motion Capture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00428">http://arxiv.org/abs/2309.00428</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/non-void/localmocap">https://github.com/non-void/localmocap</a></li>
<li>paper_authors: Xiaoyu Pan, Bowen Zheng, Xinwei Jiang, Guanglong Xu, Xianli Gu, Jingxiang Li, Qilong Kou, He Wang, Tianjia Shao, Kun Zhou, Xiaogang Jin</li>
<li>for: 本研究旨在提出一种基于本地特征的Optical Motion Capture（OMC）数据清洁和解决方法，以减少 marker 误差和 occlusion 的影响。</li>
<li>methods: 提出了一种基于标签和关节的不同类型节点的 hetroogeneous graph neural network（HGNN），使用图 convolution 操作提取 marker 和关节的本地特征，并将其转化为干净的运动。 </li>
<li>results: 对多个数据集进行了extensive comparison，并证明了我们的方法可以在多个纬度上高度准确地预测 occluded marker 位置错误，并对重建关节旋转和位置进行了30%的误差减少。代码和数据可以在<a target="_blank" rel="noopener" href="https://github.com/non-void/LocalMoCap%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/non-void/LocalMoCap上下载。</a><details>
<summary>Abstract</summary>
We present a novel locality-based learning method for cleaning and solving optical motion capture data. Given noisy marker data, we propose a new heterogeneous graph neural network which treats markers and joints as different types of nodes, and uses graph convolution operations to extract the local features of markers and joints and transform them to clean motions. To deal with anomaly markers (e.g. occluded or with big tracking errors), the key insight is that a marker's motion shows strong correlations with the motions of its immediate neighboring markers but less so with other markers, a.k.a. locality, which enables us to efficiently fill missing markers (e.g. due to occlusion). Additionally, we also identify marker outliers due to tracking errors by investigating their acceleration profiles. Finally, we propose a training regime based on representation learning and data augmentation, by training the model on data with masking. The masking schemes aim to mimic the occluded and noisy markers often observed in the real data. Finally, we show that our method achieves high accuracy on multiple metrics across various datasets. Extensive comparison shows our method outperforms state-of-the-art methods in terms of prediction accuracy of occluded marker position error by approximately 20%, which leads to a further error reduction on the reconstructed joint rotations and positions by 30%. The code and data for this paper are available at https://github.com/non-void/LocalMoCap.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的地域性学习方法，用于清洁和解决光学动作捕捉数据。给定含有噪声的标记数据，我们提议一种新的异类图 neural network，其中标记和关节被视为不同类型的节点，并使用图 convolution 操作来提取标记和关节的本地特征，并将其转化为清洁动作。为了处理异常标记（例如受到遮盖或大跟踪错误），我们的关键发现是，标记的运动具有强相关性，与其邻近的标记的运动相关，而与其他标记的运动相关性较弱，这使得我们能够高效地填充缺失的标记（例如由遮盖所致）。此外，我们还可以识别标记异常（例如由跟踪错误所致），通过研究它们的加速度轨迹。最后，我们建议一种基于表示学习和数据扩展的训练方法，通过在数据上进行掩码训练。掩码方案的目的是模拟实际数据中的受遮盖和噪声标记。我们的方法在多个维度上达到高精度，相比之前的方法，我们的方法在填充缺失标记和跟踪错误方面的预测精度提高约20%，这导致了再次的误差减少在重建关节旋转和位置上约30%。代码和数据可以在https://github.com/non-void/LocalMoCap中获取。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Personalized-Federated-Learning-Group-Privacy-Fairness-and-Beyond"><a href="#Advancing-Personalized-Federated-Learning-Group-Privacy-Fairness-and-Beyond" class="headerlink" title="Advancing Personalized Federated Learning: Group Privacy, Fairness, and Beyond"></a>Advancing Personalized Federated Learning: Group Privacy, Fairness, and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00416">http://arxiv.org/abs/2309.00416</a></li>
<li>repo_url: None</li>
<li>paper_authors: Filippo Galli, Kangsoo Jung, Sayan Biswas, Catuscia Palamidessi, Tommaso Cucinotta</li>
<li>for: 本研究旨在探讨在分布式学习框架下，如何兼顾个人化、隐私保障和公平性。</li>
<li>methods: 本研究使用了 $d$-privacy（也称为 metric privacy）来保证客户端数据的隐私，并通过对模型更新进行权限控制来实现个人化模型训练。</li>
<li>results: 研究发现，通过使用 $d$-privacy，可以在分布式学习框架下实现个人化模型训练，同时提供正式的隐私保障和较好的群体公平性。<details>
<summary>Abstract</summary>
Federated learning (FL) is a framework for training machine learning models in a distributed and collaborative manner. During training, a set of participating clients process their data stored locally, sharing only the model updates obtained by minimizing a cost function over their local inputs. FL was proposed as a stepping-stone towards privacy-preserving machine learning, but it has been shown vulnerable to issues such as leakage of private information, lack of personalization of the model, and the possibility of having a trained model that is fairer to some groups than to others. In this paper, we address the triadic interaction among personalization, privacy guarantees, and fairness attained by models trained within the FL framework. Differential privacy and its variants have been studied and applied as cutting-edge standards for providing formal privacy guarantees. However, clients in FL often hold very diverse datasets representing heterogeneous communities, making it important to protect their sensitive information while still ensuring that the trained model upholds the aspect of fairness for the users. To attain this objective, a method is put forth that introduces group privacy assurances through the utilization of $d$-privacy (aka metric privacy). $d$-privacy represents a localized form of differential privacy that relies on a metric-oriented obfuscation approach to maintain the original data's topological distribution. This method, besides enabling personalized model training in a federated approach and providing formal privacy guarantees, possesses significantly better group fairness measured under a variety of standard metrics than a global model trained within a classical FL template. Theoretical justifications for the applicability are provided, as well as experimental validation on real-world datasets to illustrate the working of the proposed method.
</details>
<details>
<summary>摘要</summary>
federated learning（FL）是一种分布式和合作的机器学习框架，在训练过程中，参与训练的客户端将本地存储的数据进行处理，并仅将模型更新分布式地提交给训练。FL被提出为隐私保护机器学习的一个途径，但它存在一些问题，如透露隐私信息、缺乏个性化模型和对某些群体更加公平的模型。本文探讨在FL框架中个性化、隐私保障和公平性之间的三元交互。使用泛化隐私和其变种的研究和应用已成为当前隐私保障的标准。然而，FL中的客户端通常拥有具有不同数据集和异质社区的本地数据，因此保护这些敏感信息的同时仍保证模型对用户具有公平性是一项重要的任务。为解决这个问题，我们提出了基于$d$-隐私（即 метриック隐私）的方法，该方法通过本地化隐私保障，保持原始数据的Topological分布，并在个性化模型训练中提供正式的隐私保障。这种方法不仅允许在分布式方式进行个性化模型训练，还具有较好的群体公平度，并且在不同的标准度量中测试了其工作。我们还提供了理论上的正当性和实验 validate的数据来证明方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Learning-multi-modal-generative-models-with-permutation-invariant-encoders-and-tighter-variational-bounds"><a href="#Learning-multi-modal-generative-models-with-permutation-invariant-encoders-and-tighter-variational-bounds" class="headerlink" title="Learning multi-modal generative models with permutation-invariant encoders and tighter variational bounds"></a>Learning multi-modal generative models with permutation-invariant encoders and tighter variational bounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00380">http://arxiv.org/abs/2309.00380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcel Hirt, Domenico Campolo, Victoria Leong, Juan-Pablo Ortega</li>
<li>for: 这个论文是为了提出一种基于深度隐藏变量模型的多modal数据生成模型，以jointly explain multiple modalities的latent representations。</li>
<li>methods: 这个论文使用了多modal Variational Autoencoders（VAEs）作为生成模型，并使用了Product-of-Experts（PoE）或Mixture-of-Experts（MoE）的归一化方法来编码来自不同modalities的隐藏变量。</li>
<li>results: 该论文通过提出更加灵活的归一化方法和更加紧密的Lower bounding方法，以提高多modal数据生成模型的生成质量和多modal性能。<details>
<summary>Abstract</summary>
Devising deep latent variable models for multi-modal data has been a long-standing theme in machine learning research. Multi-modal Variational Autoencoders (VAEs) have been a popular generative model class that learns latent representations which jointly explain multiple modalities. Various objective functions for such models have been suggested, often motivated as lower bounds on the multi-modal data log-likelihood or from information-theoretic considerations. In order to encode latent variables from different modality subsets, Product-of-Experts (PoE) or Mixture-of-Experts (MoE) aggregation schemes have been routinely used and shown to yield different trade-offs, for instance, regarding their generative quality or consistency across multiple modalities. In this work, we consider a variational bound that can tightly lower bound the data log-likelihood. We develop more flexible aggregation schemes that generalise PoE or MoE approaches by combining encoded features from different modalities based on permutation-invariant neural networks. Our numerical experiments illustrate trade-offs for multi-modal variational bounds and various aggregation schemes. We show that tighter variational bounds and more flexible aggregation models can become beneficial when one wants to approximate the true joint distribution over observed modalities and latent variables in identifiable models.
</details>
<details>
<summary>摘要</summary>
开发深入的潜在变量模型以便多模态数据已经是机器学习研究的长期主题。多模态变量自动机（VAEs）是一种受欢迎的生成模型类，它学习共同解释多个模态的latent表示。对于这些模型，有各种目标函数被建议，通常是多模态数据的日志概率下界或信息理论上的考虑。为了从不同模态子集中编码潜在变量，Product-of-Experts（PoE）或Mixture-of-Experts（MoE）的汇集方案经常使用，并显示出不同的贸易OFF，例如生成质量或多模态之间的一致性。在这个工作中，我们考虑一种可以紧紧下界多模态数据的日志概率的 variational bound。我们开发更 flexible的汇集方案，把不同模态的编码特征通过具有征文化不变性的神经网络进行组合。我们的数字实验表明，多模态variational bound和不同的汇集方案之间存在贸易OFF。我们显示，更紧的variational bound和更flexible的汇集模型可以在 aproximate true joint distribution over observed modalities and latent variables in identifiable models 中变得有利。
</details></li>
</ul>
<hr>
<h2 id="Anomaly-detection-with-semi-supervised-classification-based-on-risk-estimators"><a href="#Anomaly-detection-with-semi-supervised-classification-based-on-risk-estimators" class="headerlink" title="Anomaly detection with semi-supervised classification based on risk estimators"></a>Anomaly detection with semi-supervised classification based on risk estimators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00379">http://arxiv.org/abs/2309.00379</a></li>
<li>repo_url: None</li>
<li>paper_authors: Le Thi Khanh Hien, Sukanya Patra, Souhaib Ben Taieb</li>
<li>for: 本研究旨在超越一类分类异常检测方法的重要局限性，即假设训练数据只包含正常实例的假设。</li>
<li>methods: 我们提出了两种新的分类基于异常检测方法，包括使用不偏的风险估计的混合学习异常检测方法，以及使用非正式风险估计的深度异常检测方法。</li>
<li>results: 我们对两种风险估计的选择和正则化参数的选择进行了严格的分析，并通过广泛的实验证明了异常检测方法的有效性。<details>
<summary>Abstract</summary>
A significant limitation of one-class classification anomaly detection methods is their reliance on the assumption that unlabeled training data only contains normal instances. To overcome this impractical assumption, we propose two novel classification-based anomaly detection methods. Firstly, we introduce a semi-supervised shallow anomaly detection method based on an unbiased risk estimator. Secondly, we present a semi-supervised deep anomaly detection method utilizing a nonnegative (biased) risk estimator. We establish estimation error bounds and excess risk bounds for both risk minimizers. Additionally, we propose techniques to select appropriate regularization parameters that ensure the nonnegativity of the empirical risk in the shallow model under specific loss functions. Our extensive experiments provide strong evidence of the effectiveness of the risk-based anomaly detection methods.
</details>
<details>
<summary>摘要</summary>
一个重要的限制是一类分类异常检测方法的假设，即训练数据只包含正常实例。为了突破这个不现实的假设，我们提出了两种新的分类基于异常检测方法。首先，我们介绍了一种半监督浅层异常检测方法，基于不偏的风险估计器。其次，我们介绍了一种半监督深层异常检测方法，利用非负（偏）风险估计器。我们确立了风险估计器的估计误差 bound 和过分布 bound，以及适当的规则化参数选择技术，以保证训练数据的非负性，特别是在特定的损失函数下。我们的广泛的实验表明，风险基于异常检测方法是有效的。
</details></li>
</ul>
<hr>
<h2 id="Where-Did-the-Gap-Go-Reassessing-the-Long-Range-Graph-Benchmark"><a href="#Where-Did-the-Gap-Go-Reassessing-the-Long-Range-Graph-Benchmark" class="headerlink" title="Where Did the Gap Go? Reassessing the Long-Range Graph Benchmark"></a>Where Did the Gap Go? Reassessing the Long-Range Graph Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00367">http://arxiv.org/abs/2309.00367</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/toenshoff/lrgb">https://github.com/toenshoff/lrgb</a></li>
<li>paper_authors: Jan Tönshoff, Martin Ritzert, Eran Rosenbluth, Martin Grohe</li>
<li>for:  estabilish a higher standard of empirical rigor within the graph machine learning community</li>
<li>methods: carefully reevaluate multiple MPGNN baselines and the Graph Transformer GPS</li>
<li>results: the reported performance gap is overestimated due to suboptimal hyperparameter choices, and the performance gap completely vanishes after basic hyperparameter optimization.Here’s the text in Simplified Chinese:</li>
<li>for:  estabilish 高水平的实验准则在图机器学习社区</li>
<li>methods: 精心重评多个MPGNN基线和图 transformer GPS</li>
<li>results: 报告的性能差异过度估计，因为SUB优化参数选择不当Note: “SUB” stands for “suboptimal” in English.<details>
<summary>Abstract</summary>
The recent Long-Range Graph Benchmark (LRGB, Dwivedi et al. 2022) introduced a set of graph learning tasks strongly dependent on long-range interaction between vertices. Empirical evidence suggests that on these tasks Graph Transformers significantly outperform Message Passing GNNs (MPGNNs). In this paper, we carefully reevaluate multiple MPGNN baselines as well as the Graph Transformer GPS (Ramp\'a\v{s}ek et al. 2022) on LRGB. Through a rigorous empirical analysis, we demonstrate that the reported performance gap is overestimated due to suboptimal hyperparameter choices. It is noteworthy that across multiple datasets the performance gap completely vanishes after basic hyperparameter optimization. In addition, we discuss the impact of lacking feature normalization for LRGB's vision datasets and highlight a spurious implementation of LRGB's link prediction metric. The principal aim of our paper is to establish a higher standard of empirical rigor within the graph machine learning community.
</details>
<details>
<summary>摘要</summary>
最近的长距离图 benchMark (LRGB, Dwivedi et al. 2022) 引入了一系列强依赖于长距离交互的图学任务。实际证据表明，在这些任务上，图Transformers 明显超越了Message Passing GNNs (MPGNNs)。在这篇论文中，我们仔细重新评估了多个 MPGNN 基eline以及 Graph Transformer GPS (Ramp\'a\v{s}ek et al. 2022) 在 LRGB 上的性能。通过严格的实际分析，我们表明了报告的性能差距被过度估计，这是因为使用不优化的超参数。在多个 dataset 上，性能差距完全消失了 после基本超参数优化。此外，我们讨论了 LRGB 视觉数据集上缺失的Feature Normalization的影响，并指出了 LRGB 链接预测度量的误导性实现。我们文章的主要目标是在图机器学习社区中提高实际的严格度。
</details></li>
</ul>
<hr>
<h2 id="FederatedScope-LLM-A-Comprehensive-Package-for-Fine-tuning-Large-Language-Models-in-Federated-Learning"><a href="#FederatedScope-LLM-A-Comprehensive-Package-for-Fine-tuning-Large-Language-Models-in-Federated-Learning" class="headerlink" title="FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning"></a>FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00363">http://arxiv.org/abs/2309.00363</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alibaba/federatedscope">https://github.com/alibaba/federatedscope</a></li>
<li>paper_authors: Weirui Kuang, Bingchen Qian, Zitao Li, Daoyuan Chen, Dawei Gao, Xuchen Pan, Yuexiang Xie, Yaliang Li, Bolin Ding, Jingren Zhou</li>
<li>for:  This paper focuses on the challenges of fine-tuning large language models (LLMs) in federated learning (FL) settings, and proposes a package called FS-LLM to address these challenges.</li>
<li>methods:  The paper introduces several components of the FS-LLM package, including an end-to-end benchmarking pipeline, federated parameter-efficient fine-tuning algorithms, and resource-efficient operators for fine-tuning LLMs with limited resources.</li>
<li>results:  The paper conducts extensive experiments to validate the effectiveness of FS-LLM and compares it with state-of-the-art parameter-efficient fine-tuning algorithms in FL settings. The results show that FS-LLM achieves better performance with lower communication and computation costs, and provides valuable insights into federated fine-tuning LLMs for the research community.Here’s the Chinese translation of the three information points:</li>
<li>for:  This paper focuses on the challenges of fine-tuning large language models (LLMs) in federated learning (FL) settings, and proposes a package called FS-LLM to address these challenges.</li>
<li>methods:  The paper introduces several components of the FS-LLM package, including an end-to-end benchmarking pipeline, federated parameter-efficient fine-tuning algorithms, and resource-efficient operators for fine-tuning LLMs with limited resources.</li>
<li>results:  The paper conducts extensive experiments to validate the effectiveness of FS-LLM and compares it with state-of-the-art parameter-efficient fine-tuning algorithms in FL settings. The results show that FS-LLM achieves better performance with lower communication and computation costs, and provides valuable insights into federated fine-tuning LLMs for the research community.<details>
<summary>Abstract</summary>
LLMs have demonstrated great capabilities in various NLP tasks. Different entities can further improve the performance of those LLMs on their specific downstream tasks by fine-tuning LLMs. When several entities have similar interested tasks, but their data cannot be shared because of privacy concerns regulations, federated learning (FL) is a mainstream solution to leverage the data of different entities. However, fine-tuning LLMs in federated learning settings still lacks adequate support from existing FL frameworks because it has to deal with optimizing the consumption of significant communication and computational resources, data preparation for different tasks, and distinct information protection demands. This paper first discusses these challenges of federated fine-tuning LLMs, and introduces our package FS-LLM as a main contribution, which consists of the following components: (1) we build an end-to-end benchmarking pipeline, automizing the processes of dataset preprocessing, federated fine-tuning execution, and performance evaluation on federated LLM fine-tuning; (2) we provide comprehensive federated parameter-efficient fine-tuning algorithm implementations and versatile programming interfaces for future extension in FL scenarios with low communication and computation costs, even without accessing the full model; (3) we adopt several accelerating and resource-efficient operators for fine-tuning LLMs with limited resources and the flexible pluggable sub-routines for interdisciplinary study. We conduct extensive experiments to validate the effectiveness of FS-LLM and benchmark advanced LLMs with state-of-the-art parameter-efficient fine-tuning algorithms in FL settings, which also yields valuable insights into federated fine-tuning LLMs for the research community. To facilitate further research and adoption, we release FS-LLM at https://github.com/alibaba/FederatedScope/tree/llm.
</details>
<details>
<summary>摘要</summary>
LLMs 已经在不同的自然语言处理任务中表现出了惊人的能力。不同的实体可以通过特定的下游任务进一步提高 LLMs 的性能。当多个实体有相似的 interessetasks，但其数据不能被共享由隐私问题限制的情况下，联邦学习（FL）成为了主流的解决方案，以利用不同实体的数据来提高 LLMs 的性能。然而，在联邦学习设置下进行 LLMs 的 fine-tuning 仍然缺乏现有 FL 框架的有效支持，因为需要处理大量的通信和计算资源，为不同任务准备数据，并遵守不同的隐私保护要求。本文首先描述了联邦 fine-tuning LLMs 中存在的挑战，并 introduce 我们的 package FS-LLM 作为主要贡献，该包包括以下三个组件：1. 我们建立了一个端到端的 benchmarking 管道，自动化了数据预处理、联邦 fine-tuning 执行和性能评估在联邦 LL 的 fine-tuning 中。2. 我们提供了联邦参数高效的 fine-tuning 算法实现和多样化的编程接口，以便在 FL 场景中，即使没有访问全模型，也可以实现低通信和计算成本的 fine-tuning。3. 我们采用了一些加速和资源高效的操作符，以便在有限资源的情况下进行 fine-tuning LLMs，并采用可替换的子过程，以便进行跨学科研究。我们进行了广泛的实验 validate 了 FS-LLM 的有效性，并在 FL 设置下对 advanced LLMs 进行了参数高效的 fine-tuning，从而获得了价值的发现，以便为研究社区提供参考。为了促进更多的研究和应用，我们将FS-LLM 发布在 GitHub 上，可以在 <https://github.com/alibaba/FederatedScope/tree/llm> 中获取。
</details></li>
</ul>
<hr>
<h2 id="Local-and-adaptive-mirror-descents-in-extensive-form-games"><a href="#Local-and-adaptive-mirror-descents-in-extensive-form-games" class="headerlink" title="Local and adaptive mirror descents in extensive-form games"></a>Local and adaptive mirror descents in extensive-form games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00656">http://arxiv.org/abs/2309.00656</a></li>
<li>repo_url: None</li>
<li>paper_authors: Côme Fiegel, Pierre Ménard, Tadashi Kozuno, Rémi Munos, Vianney Perchet, Michal Valko</li>
<li>for: 这个论文是研究如何在零 SUM 不完全信息游戏中学习 $\epsilon$-优策略的。</li>
<li>methods: 该论文使用了一种固定抽样方法，将玩家的政策更新到每个 episoden 中，并使用了一个适应 Online Mirror Descent（OMD）算法来实现。</li>
<li>results: 该论文显示了这种方法可以在 $\tilde{\mathcal{O}(T^{-1&#x2F;2})$ 的速度下 converge，并且在游戏参数中具有near-optimal的依赖关系。<details>
<summary>Abstract</summary>
We study how to learn $\epsilon$-optimal strategies in zero-sum imperfect information games (IIG) with trajectory feedback. In this setting, players update their policies sequentially based on their observations over a fixed number of episodes, denoted by $T$. Existing procedures suffer from high variance due to the use of importance sampling over sequences of actions (Steinberger et al., 2020; McAleer et al., 2022). To reduce this variance, we consider a fixed sampling approach, where players still update their policies over time, but with observations obtained through a given fixed sampling policy. Our approach is based on an adaptive Online Mirror Descent (OMD) algorithm that applies OMD locally to each information set, using individually decreasing learning rates and a regularized loss. We show that this approach guarantees a convergence rate of $\tilde{\mathcal{O}(T^{-1/2})$ with high probability and has a near-optimal dependence on the game parameters when applied with the best theoretical choices of learning rates and sampling policies. To achieve these results, we generalize the notion of OMD stabilization, allowing for time-varying regularization with convex increments.
</details>
<details>
<summary>摘要</summary>
我们研究如何学习 $\epsilon$-优化策略在零和游戏中（IIG）中，其中玩家采用批量更新策略基于他们观察到的行为序列。现有的方法具有高度的卷积变iance，这是由importance sampling over sequences of actions引入的。为了降低这种变iance，我们考虑了一种固定抽样方法，其中玩家仍然在时间内更新策略，但是通过固定的抽样策略获取观察。我们的方法基于一种适应 Online Mirror Descent（OMD）算法，该算法在每个信息集中应用 OMD 地方，使用个人减少学习率和带有规化损失的梯度下降。我们证明了这种方法在高probability下 converges at a rate of $\tilde{\mathcal{O}(T^{-1/2})$，并且在游戏参数下有 near-optimal 的依赖性，当应用最佳的理论学习率和抽样策略时。为了实现这些结果，我们扩展了 OMD 稳定性的概念，允许时间变化的正则化，并使用几何增量。
</details></li>
</ul>
<hr>
<h2 id="Bespoke-Nanoparticle-Synthesis-and-Chemical-Knowledge-Discovery-Via-Autonomous-Experimentations"><a href="#Bespoke-Nanoparticle-Synthesis-and-Chemical-Knowledge-Discovery-Via-Autonomous-Experimentations" class="headerlink" title="Bespoke Nanoparticle Synthesis and Chemical Knowledge Discovery Via Autonomous Experimentations"></a>Bespoke Nanoparticle Synthesis and Chemical Knowledge Discovery Via Autonomous Experimentations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00349">http://arxiv.org/abs/2309.00349</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyuk Jun Yoo, Nayeon Kim, Heeseung Lee, Daeho Kim, Leslie Tiong Ching Ow, Hyobin Nam, Chansoo Kim, Seung Yong Lee, Kwan-Young Lee, Donghun Kim, Sang Soo Han</li>
<li>for: 本研究旨在开发一种智能化的纳米材料合成平台，以优化纳米材料的Synthesize方法并实现targeted的光学性质。</li>
<li>methods: 该平台采用了一种封闭的closed-loop机制，通过将批量合成模块和UV-Vis光谱测量模块相连接，基于人工智能优化模型的反馈，以实现精准地控制纳米材料的Synthesize过程。</li>
<li>results: 通过用银(Ag)粒子为例，我们示出了 bayesian优化器在五种合成原料中进行优化时的高效性，可以在200轮 iteration中 precisely possession desired absorption spectra。此外，我们还发现了一种新的化学效应，即 citrate的Amount对硬度和材料的形态具有关键作用，从而影响了硬度和光谱特征。<details>
<summary>Abstract</summary>
The optimization of nanomaterial synthesis using numerous synthetic variables is considered to be extremely laborious task because the conventional combinatorial explorations are prohibitively expensive. In this work, we report an autonomous experimentation platform developed for the bespoke design of nanoparticles (NPs) with targeted optical properties. This platform operates in a closed-loop manner between a batch synthesis module of NPs and a UV- Vis spectroscopy module, based on the feedback of the AI optimization modeling. With silver (Ag) NPs as a representative example, we demonstrate that the Bayesian optimizer implemented with the early stopping criterion can efficiently produce Ag NPs precisely possessing the desired absorption spectra within only 200 iterations (when optimizing among five synthetic reagents). In addition to the outstanding material developmental efficiency, the analysis of synthetic variables further reveals a novel chemistry involving the effects of citrate in Ag NP synthesis. The amount of citrate is a key to controlling the competitions between spherical and plate-shaped NPs and, as a result, affects the shapes of the absorption spectra as well. Our study highlights both capabilities of the platform to enhance search efficiencies and to provide a novel chemical knowledge by analyzing datasets accumulated from the autonomous experimentations.
</details>
<details>
<summary>摘要</summary>
“精细材料合成优化使用多个合成变量是一项非常困难的任务，因为传统的可靠性探索是非常昂贵的。在这项工作中，我们报道了一种自动化实验平台，用于设计targeted optical properties的粒子（NPs）。这个平台通过closed-loop模式，将批量合成NPs模块和UV-Vispectroscopy模块相连，基于AI优化模型的反馈。使用银（Ag）NPs作为例子，我们示示了bayesian优化器，在5种合成原料中进行优化时，可以高效地生成银NPs，具有所需的吸收 спектrum。此外，分析合成变量还揭示了一种新的化学知识，即 citrate的影响在银NP合成中。 citrate的含量是控制球形和板状NPs的竞争的关键，因此也影响了吸收спектrum的形状。我们的研究强调了该平台的搜索效率提高和化学知识的提供。”
</details></li>
</ul>
<hr>
<h2 id="Multitask-Deep-Learning-for-Accurate-Risk-Stratification-and-Prediction-of-Next-Steps-for-Coronary-CT-Angiography-Patients"><a href="#Multitask-Deep-Learning-for-Accurate-Risk-Stratification-and-Prediction-of-Next-Steps-for-Coronary-CT-Angiography-Patients" class="headerlink" title="Multitask Deep Learning for Accurate Risk Stratification and Prediction of Next Steps for Coronary CT Angiography Patients"></a>Multitask Deep Learning for Accurate Risk Stratification and Prediction of Next Steps for Coronary CT Angiography Patients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00330">http://arxiv.org/abs/2309.00330</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juan Lu, Mohammed Bennamoun, Jonathon Stewart, JasonK. Eshraghian, Yanbin Liu, Benjamin Chow, Frank M. Sanfilippo, Girish Dwivedi</li>
<li>for: 这份研究是为了提高怀疑和证明 coronary artery disease (CAD) 患者的风险评估和诊断决策。</li>
<li>methods: 这篇研究使用了多任务深度学习模型，以支持风险评估和下游测试选择。</li>
<li>results: 研究结果显示，这个模型可以对 CCTA 报告数据进行实际的分析，并且可以实现高度的 CAD 风险评估和下游测试选择。模型的 AUC 为 0.76，可以精确地估计 CAD 的可能性和建议下游测试。<details>
<summary>Abstract</summary>
Diagnostic investigation has an important role in risk stratification and clinical decision making of patients with suspected and documented Coronary Artery Disease (CAD). However, the majority of existing tools are primarily focused on the selection of gatekeeper tests, whereas only a handful of systems contain information regarding the downstream testing or treatment. We propose a multi-task deep learning model to support risk stratification and down-stream test selection for patients undergoing Coronary Computed Tomography Angiography (CCTA). The analysis included 14,021 patients who underwent CCTA between 2006 and 2017. Our novel multitask deep learning framework extends the state-of-the art Perceiver model to deal with real-world CCTA report data. Our model achieved an Area Under the receiver operating characteristic Curve (AUC) of 0.76 in CAD risk stratification, and 0.72 AUC in predicting downstream tests. Our proposed deep learning model can accurately estimate the likelihood of CAD and provide recommended downstream tests based on prior CCTA data. In clinical practice, the utilization of such an approach could bring a paradigm shift in risk stratification and downstream management. Despite significant progress using deep learning models for tabular data, they do not outperform gradient boosting decision trees, and further research is required in this area. However, neural networks appear to benefit more readily from multi-task learning than tree-based models. This could offset the shortcomings of using single task learning approach when working with tabular data.
</details>
<details>
<summary>摘要</summary>
医学诊断调查在抑制性肺动脉疾病（CAD）的诊断和治疗中扮演着重要的角色。然而，现有大多数工具都是专注于门槛测试的选择，而忽略了下游测试或治疗的信息。我们提议一种多任务深度学习模型，用于支持CCTA扫描后的风险分级和下游测试选择。我们的分析包括2006年至2017年间对CCTA扫描的14,021名患者。我们的新的多任务深度学习框架将状态艺术Perceiver模型扩展到实际CCTA报告数据上。我们的模型在CAD风险分级方面 achieve了0.76的接受分数（AUC），而在预测下游测试方面 achieve了0.72的接受分数（AUC）。我们的提议的深度学习模型可以准确地估计CAD的可能性，并基于过去CCTA数据提供下游测试的建议。在临床实践中，使用这种方法可能会引入一种新的风格转移，改善风险分级和下游管理。虽然使用深度学习模型对 tabular 数据进行了重要的进步，但它们不会超过梯度提升 decision trees，需要进一步的研究。然而，神经网络似乎更易受到多任务学习的影响，这可能将 tabular 数据上的弱点 offset。
</details></li>
</ul>
<hr>
<h2 id="Mi-Go-Test-Framework-which-uses-YouTube-as-Data-Source-for-Evaluating-Speech-Recognition-Models-like-OpenAI’s-Whisper"><a href="#Mi-Go-Test-Framework-which-uses-YouTube-as-Data-Source-for-Evaluating-Speech-Recognition-Models-like-OpenAI’s-Whisper" class="headerlink" title="Mi-Go: Test Framework which uses YouTube as Data Source for Evaluating Speech Recognition Models like OpenAI’s Whisper"></a>Mi-Go: Test Framework which uses YouTube as Data Source for Evaluating Speech Recognition Models like OpenAI’s Whisper</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00329">http://arxiv.org/abs/2309.00329</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tomasz Wojnar, Jaroslaw Hryszko, Adam Roman</li>
<li>for: 评估语音识别机器学习模型的性能和适应性 across 多种语言、方言、发音样式和音质水平。</li>
<li>methods: 利用 YouTube 作为数据源，覆盖多种语言、方言、发音样式和音质水平，并对 OpenAI 开发的 Whisper 模型进行测试。</li>
<li>results: YouTube 作为测试平台可以保证语音识别模型的稳定性、准确性和适应性，并可以帮助找出 YouTube 上的搜索引擎优化。<details>
<summary>Abstract</summary>
This article introduces Mi-Go, a novel testing framework aimed at evaluating the performance and adaptability of general-purpose speech recognition machine learning models across diverse real-world scenarios. The framework leverages YouTube as a rich and continuously updated data source, accounting for multiple languages, accents, dialects, speaking styles, and audio quality levels. To demonstrate the effectiveness of the framework, the Whisper model, developed by OpenAI, was employed as a test object. The tests involve using a total of 124 YouTube videos to test all Whisper model versions. The results underscore the utility of YouTube as a valuable testing platform for speech recognition models, ensuring their robustness, accuracy, and adaptability to diverse languages and acoustic conditions. Additionally, by contrasting the machine-generated transcriptions against human-made subtitles, the Mi-Go framework can help pinpoint potential misuse of YouTube subtitles, like Search Engine Optimization.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multi-fidelity-reduced-order-surrogate-modeling"><a href="#Multi-fidelity-reduced-order-surrogate-modeling" class="headerlink" title="Multi-fidelity reduced-order surrogate modeling"></a>Multi-fidelity reduced-order surrogate modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00325">http://arxiv.org/abs/2309.00325</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/contipaolo/multifidelity_pod">https://github.com/contipaolo/multifidelity_pod</a></li>
<li>paper_authors: Paolo Conti, Mengwu Guo, Andrea Manzoni, Attilio Frangi, Steven L. Brunton, J. Nathan Kutz</li>
<li>for: 这个论文是用于描述一种基于多 fideltity神经网络的减简方法，用于在有限的计算预算下，使用低精度模型来提高解的预测精度。</li>
<li>methods: 该方法首先使用高精度解决生成特征值分解(POD)，然后使用多 fideltity长短期记忆(LSTM)网络来approximate低精度解的动态行为。</li>
<li>results: 该方法可以有效地捕捉低精度解中的不稳定性和转移现象，并且可以在不侵入式的方式下，使用低精度模型来重建全解场景。<details>
<summary>Abstract</summary>
High-fidelity numerical simulations of partial differential equations (PDEs) given a restricted computational budget can significantly limit the number of parameter configurations considered and/or time window evaluated for modeling a given system. Multi-fidelity surrogate modeling aims to leverage less accurate, lower-fidelity models that are computationally inexpensive in order to enhance predictive accuracy when high-fidelity data are limited or scarce. However, low-fidelity models, while often displaying important qualitative spatio-temporal features, fail to accurately capture the onset of instability and critical transients observed in the high-fidelity models, making them impractical as surrogate models. To address this shortcoming, we present a new data-driven strategy that combines dimensionality reduction with multi-fidelity neural network surrogates. The key idea is to generate a spatial basis by applying the classical proper orthogonal decomposition (POD) to high-fidelity solution snapshots, and approximate the dynamics of the reduced states - time-parameter-dependent expansion coefficients of the POD basis - using a multi-fidelity long-short term memory (LSTM) network. By mapping low-fidelity reduced states to their high-fidelity counterpart, the proposed reduced-order surrogate model enables the efficient recovery of full solution fields over time and parameter variations in a non-intrusive manner. The generality and robustness of this method is demonstrated by a collection of parametrized, time-dependent PDE problems where the low-fidelity model can be defined by coarser meshes and/or time stepping, as well as by misspecified physical features. Importantly, the onset of instabilities and transients are well captured by this surrogate modeling technique.
</details>
<details>
<summary>摘要</summary>
高精度数学模拟（PDE）在限制计算预算的情况下可能很难以考虑大量参数配置和时间窗口。多层次模型可以利用低精度模型，以提高预测精度，但低精度模型通常不能准确地捕捉高精度模型中的不稳定性和关键过渡。为解决这个缺点，我们提出了一种新的数据驱动策略，它将维度减少与多层次神经网络模型结合在一起。我们首先应用高精度解决方案中的经典正交分解（POD）法生成空间基，然后使用多层次长短期记忆（LSTM）网络来approximate高精度解决方案中的动力学行为。通过将低精度减少到高精度解决方案，我们提出的减少的模型可以非侵入地重建全解场在时间和参数变化中。我们通过一系列 Parametrized，时间依赖的PDE问题的集合来证明这种方法的一般性和稳定性。importantly，这种模拟方法可以准确地捕捉不稳定性和过渡。
</details></li>
</ul>
<hr>
<h2 id="SortedNet-a-Place-for-Every-Network-and-Every-Network-in-its-Place-Towards-a-Generalized-Solution-for-Training-Many-in-One-Neural-Networks"><a href="#SortedNet-a-Place-for-Every-Network-and-Every-Network-in-its-Place-Towards-a-Generalized-Solution-for-Training-Many-in-One-Neural-Networks" class="headerlink" title="SortedNet, a Place for Every Network and Every Network in its Place: Towards a Generalized Solution for Training Many-in-One Neural Networks"></a>SortedNet, a Place for Every Network and Every Network in its Place: Towards a Generalized Solution for Training Many-in-One Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00255">http://arxiv.org/abs/2309.00255</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mojtaba Valipour, Mehdi Rezagholizadeh, Hossein Rajabzadeh, Marzieh Tahaei, Boxing Chen, Ali Ghodsi</li>
<li>for: 这篇论文旨在提出一个通用且可扩展的方法，以实现深度学习模型在数据量和计算资源限制下的最佳化。</li>
<li>methods: 本论文提出了一个sorted training的方法，具有以下几个特点：（1）使用一个嵌入式的架构，将深度学习模型分解为多个子网络；（2）在训练过程中，随机抽样子网络，并使用数据类型和数据量的组合来决定子网络的训练；（3）使用一个新的更新方法，将子网络的训练结果组合成最终的模型。</li>
<li>results: 实验结果显示， sorted training 方法可以实现高效的深度学习模型训练，并且比过去的动态训练方法高效得多。具体来说，这篇论文可以训练 160 个不同的子网络 simultaneously，并且维持模型性能的 96%。<details>
<summary>Abstract</summary>
As the size of deep learning models continues to grow, finding optimal models under memory and computation constraints becomes increasingly more important. Although usually the architecture and constituent building blocks of neural networks allow them to be used in a modular way, their training process is not aware of this modularity. Consequently, conventional neural network training lacks the flexibility to adapt the computational load of the model during inference. This paper proposes SortedNet, a generalized and scalable solution to harness the inherent modularity of deep neural networks across various dimensions for efficient dynamic inference. Our training considers a nested architecture for the sub-models with shared parameters and trains them together with the main model in a sorted and probabilistic manner. This sorted training of sub-networks enables us to scale the number of sub-networks to hundreds using a single round of training. We utilize a novel updating scheme during training that combines random sampling of sub-networks with gradient accumulation to improve training efficiency. Furthermore, the sorted nature of our training leads to a search-free sub-network selection at inference time; and the nested architecture of the resulting sub-networks leads to minimal storage requirement and efficient switching between sub-networks at inference. Our general dynamic training approach is demonstrated across various architectures and tasks, including large language models and pre-trained vision models. Experimental results show the efficacy of the proposed approach in achieving efficient sub-networks while outperforming state-of-the-art dynamic training approaches. Our findings demonstrate the feasibility of training up to 160 different sub-models simultaneously, showcasing the extensive scalability of our proposed method while maintaining 96% of the model performance.
</details>
<details>
<summary>摘要</summary>
随着深度学习模型的大小不断增长，在内存和计算限制下找到最佳模型变得越来越重要。尽管架构和组成部件的 Neil 网络允许它们在模块化方式下使用，但训练过程并不意识到这种模块性。因此，传统的神经网络训练缺乏对模型计算负荷的灵活性。这篇论文提出了 SortNet，一种通用且可扩展的解决方案，以便在多维度上利用深度神经网络的内置模块性进行高效的动态推理。我们的训练方法包括嵌入式架构，共享参数的卷积网络，并在排序和 probabilistic 的方式下对卷积网络进行训练。这种排序训练方法使得我们可以在单一轮训练中批量地训练数百个子网络。我们还提出了一种新的更新方法，将随机抽样的子网络与梯度积累相结合，以提高训练效率。此外，排序的训练方法导致在推理时不需要搜索子网络，并且嵌入式架构的结果是最小的存储需求和高效的子网络交换。我们的通用动态训练方法在不同的架构和任务上进行了广泛的实验，包括大语言模型和预训练视觉模型。实验结果表明我们的方法可以高效地实现高性能的子网络，并超越当前的动态训练方法。我们的发现表明可以同时训练160个不同的子模型， demonstrating the extensive scalability of our proposed method while maintaining 96% of the model performance.
</details></li>
</ul>
<hr>
<h2 id="Data-Driven-Projection-for-Reducing-Dimensionality-of-Linear-Programs-Generalization-Bound-and-Learning-Methods"><a href="#Data-Driven-Projection-for-Reducing-Dimensionality-of-Linear-Programs-Generalization-Bound-and-Learning-Methods" class="headerlink" title="Data-Driven Projection for Reducing Dimensionality of Linear Programs: Generalization Bound and Learning Methods"></a>Data-Driven Projection for Reducing Dimensionality of Linear Programs: Generalization Bound and Learning Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00203">http://arxiv.org/abs/2309.00203</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shinsaku Sakaue, Taihei Oki</li>
<li>for: 这个论文研究了一种基于数据的高维线性 програм（LP）解决方法。给定过去的 $n$-维LP数据，我们学习了一个 $n\times k$ 的投影矩阵 ($n &gt; k$)，将高维问题降维到低维问题。然后，我们通过解决低维LP问题并通过投影矩阵恢复高维解决方案。这种方法可以与任何用户喜欢的LP解决方法结合使用，因此可以快速解决LP问题。</li>
<li>methods: 我们提出了一种基于数据驱动的LP解决方法，包括使用PCA和梯度下降两种方法学习投影矩阵。PCA方法简单效率高，而梯度下降方法可能会提供更高质量的解决方案。</li>
<li>results: 我们的实验表明，学习投影矩阵可以快速和精准地解决LP问题。具体来说，我们可以在减少LP解决时间的同时保持高质量解决方案。此外，我们还发现在某些情况下，使用梯度下降方法可以提供更高质量的解决方案。<details>
<summary>Abstract</summary>
This paper studies a simple data-driven approach to high-dimensional linear programs (LPs). Given data of past $n$-dimensional LPs, we learn an $n\times k$ \textit{projection matrix} ($n > k$), which reduces the dimensionality from $n$ to $k$. Then, we address future LP instances by solving $k$-dimensional LPs and recovering $n$-dimensional solutions by multiplying the projection matrix. This idea is compatible with any user-preferred LP solvers, hence a versatile approach to faster LP solving. One natural question is: how much data is sufficient to ensure the recovered solutions' quality? We address this question based on the idea of \textit{data-driven algorithm design}, which relates the amount of data sufficient for generalization guarantees to the \textit{pseudo-dimension} of performance metrics. We present an $\tilde{\mathrm{O}(nk^2)$ upper bound on the pseudo-dimension ($\tilde{\mathrm{O}$ compresses logarithmic factors) and complement it by an $\Omega(nk)$ lower bound, hence tight up to an $\tilde{\mathrm{O}(k)$ factor. On the practical side, we study two natural methods for learning projection matrices: PCA- and gradient-based methods. While the former is simple and efficient, the latter sometimes leads to better solution quality. Experiments confirm that learned projection matrices are beneficial for reducing the time for solving LPs while maintaining high solution quality.
</details>
<details>
<summary>摘要</summary>
A natural question is how much data is needed to ensure the quality of the recovered solutions. We address this using the idea of data-driven algorithm design, which relates the amount of data needed for generalization guarantees to the pseudo-dimension of performance metrics. We provide an $\tilde{\mathrm{O}(nk^2)$ upper bound on the pseudo-dimension and complement it with an $\Omega(nk)$ lower bound, giving a tight bound up to an $\tilde{\mathrm{O}(k)$ factor.In practice, we examine two methods for learning projection matrices: principal component analysis (PCA)-based and gradient-based methods. While PCA-based methods are simple and efficient, gradient-based methods sometimes lead to better solution quality. Our experiments show that learned projection matrices can significantly reduce the time required to solve LPs while maintaining high solution quality.
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-based-Early-Fixing-for-Gas-lifted-Oil-Production-Optimization-Supervised-and-Weakly-supervised-Approaches"><a href="#Deep-learning-based-Early-Fixing-for-Gas-lifted-Oil-Production-Optimization-Supervised-and-Weakly-supervised-Approaches" class="headerlink" title="Deep-learning-based Early Fixing for Gas-lifted Oil Production Optimization: Supervised and Weakly-supervised Approaches"></a>Deep-learning-based Early Fixing for Gas-lifted Oil Production Optimization: Supervised and Weakly-supervised Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00197">http://arxiv.org/abs/2309.00197</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bruno Machado Pacheco, Laio Oriel Seman, Eduardo Camponogara</li>
<li>for: 提高天然气提取油气井的油量，解决杂项线性规划问题 (Mixed-Integer Linear Programs, MILPs)。</li>
<li>methods: 基于深度学习模型，提出一种适应型规划策略，通过提供所有整数变量的值，将原始问题转化为线性Program (LP)。提出了两种发展学习基于策略：一种是指导学习方法，需要训练集中的优化整数值；另一种是弱指导学习方法，只需要解决随机分配整数变量的早期固定问题。</li>
<li>results: 比较结果表明，使用学习基于策略可以实现runtime的减少率为71.11%，而弱指导学习模型具有显著的值提供能力，即使在训练过程中从未看到优化的整数值。<details>
<summary>Abstract</summary>
Maximizing oil production from gas-lifted oil wells entails solving Mixed-Integer Linear Programs (MILPs). As the parameters of the wells, such as the basic-sediment-to-water ratio and the gas-oil ratio, are updated, the problems must be repeatedly solved. Instead of relying on costly exact methods or the accuracy of general approximate methods, in this paper, we propose a tailor-made heuristic solution based on deep learning models trained to provide values to all integer variables given varying well parameters, early-fixing the integer variables and, thus, reducing the original problem to a linear program (LP). We propose two approaches for developing the learning-based heuristic: a supervised learning approach, which requires the optimal integer values for several instances of the original problem in the training set, and a weakly-supervised learning approach, which requires only solutions for the early-fixed linear problems with random assignments for the integer variables. Our results show a runtime reduction of 71.11% Furthermore, the weakly-supervised learning model provided significant values for early fixing, despite never seeing the optimal values during training.
</details>
<details>
<summary>摘要</summary>
最大化天然气吸取油井生产具有解决杂integer线性程序（MILP）的挑战。随着井井的参数，如基本粉末水比和气油比，的更新，问题必须重复解决。而不是依赖于贵重的精确方法或通用的估计方法，在这篇论文中，我们提出了特制的深度学习模型，用于提供所有整数变量的值，以及 varying 井井参数下的 LP。我们提出了两种方法来开发学习基于模型：一种监督学习方法，需要训练集中的优化整数值的多个实例；一种弱监督学习方法，只需要解决早期固定的线性问题，并将整数变量 randomly 分配。我们的结果显示，使用学习模型可以reduces 71.11%的运行时间。此外，弱监督学习模型在没有训练优化值的情况下也提供了重要的初始化估计值。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/01/cs.LG_2023_09_01/" data-id="clpxp6c3t00r2ee880tpx7u0z" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_09_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/01/eess.IV_2023_09_01/" class="article-date">
  <time datetime="2023-09-01T09:00:00.000Z" itemprop="datePublished">2023-09-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/01/eess.IV_2023_09_01/">eess.IV - 2023-09-01</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="High-resolution-large-field-of-view-label-free-imaging-via-aberration-corrected-closed-form-complex-field-reconstruction"><a href="#High-resolution-large-field-of-view-label-free-imaging-via-aberration-corrected-closed-form-complex-field-reconstruction" class="headerlink" title="High-resolution, large field-of-view label-free imaging via aberration-corrected, closed-form complex field reconstruction"></a>High-resolution, large field-of-view label-free imaging via aberration-corrected, closed-form complex field reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00755">http://arxiv.org/abs/2309.00755</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rzcao/apic-analytical-complex-field-reconstruction">https://github.com/rzcao/apic-analytical-complex-field-reconstruction</a></li>
<li>paper_authors: Ruizhi Cao, Cheng Shen, Changhuei Yang</li>
<li>for: 这篇论文是用于描述一种新的计算成像方法，它可以高效地生成高分辨率、大场景视野的镜像，而不需要参数选择或迭代算法。</li>
<li>methods: 这种方法使用多个倾斜照射来实现高速成像，并使用新的分析式phaserecovery框架来重建复杂场景。</li>
<li>results: 实验表明，这种方法可以 Correctly retrieve the complex field associated with darkfield measurements, and also analytically retrieve complex aberrations of an imaging system with no additional hardware. Compared to traditional FPM method, APIC method is more robust against aberrations and can achieve higher resolution.<details>
<summary>Abstract</summary>
Computational imaging methods empower modern microscopy with the ability of producing high-resolution, large field-of-view, aberration-free images. One of the dominant computational label-free imaging methods, Fourier ptychographic microscopy (FPM), effectively increases the spatial-bandwidth product of conventional microscopy by using multiple tilted illuminations to achieve high-throughput imaging. However, its iterative reconstruction method is prone to parameter selection, can be computationally expensive and tends to fail under excessive aberrations. Recently, spatial Kramers-Kronig methods show it is possible to analytically reconstruct complex field but lacks the ability of correcting aberrations or providing extended resolution enhancement. Here, we present a closed-form method, termed APIC, which weds the strengths of both methods. A new analytical phase retrieval framework is established in APIC, which demonstrates, for the first time, the feasibility of analytically reconstructing the complex field associated with darkfield measurements. In addition, APIC can analytically retrieve complex aberrations of an imaging system with no additional hardware. By avoiding iterative algorithms, APIC requires no human designed convergence metric and always obtains a closed-form complex field solution. The faithfulness and correctness of APIC's reconstruction are guaranteed due to its analytical nature. We experimentally demonstrate that APIC gives correct reconstruction result while FPM fails to do so when constrained to the same number of measurements. Meanwhile, APIC achieves 2.8 times faster computation using image tile size of 256 (length-wise). We also demonstrate APIC is unprecedentedly robust against aberrations compared to FPM - APIC is capable of addressing aberration whose maximal phase difference exceeds 3.8${\pi}$ when using a NA 0.25 objective in experiment.
</details>
<details>
<summary>摘要</summary>
计算成像技术为现代微镜技术提供了高分辨率、大场视野、抗偏振的图像生成能力。其中一种主导的计算无标签成像方法是傅立叶探针微镜（FPM），它通过多个倾斜照明来实现高通过率成像。然而，它的迭代重建方法容易选择参数、计算成本较高并且容易在过度偏振下失败。最近，空间克劳斯-克朗根方法表明了可以分析地重建复杂场的可能性，但它缺乏修正偏振或提供扩展的分辨率提升能力。我们现在介绍一种关闭式方法，称之为APIC，它将两种方法的优点相结合。我们建立了一个新的分析phaserecovery框架，可以分析地重建相关的黑场测量中的复杂场。此外，APIC可以分析地重建抗偏振系统的复杂偏振。通过避免迭代算法，APIC不需要人类设计的整合度量，总是得到关闭式的复杂场解决方案。由于APIC的分析性质，它的重建 faithfulness和正确性得到保证。我们实验表明，在相同数量的测量下，APIC可以正确地重建图像，而FPM则失败。此外，APIC在图像分割大小为256（长wise）时计算速度为2.8倍。我们还证明APIC对偏振强度的Robustness比FPM更高，可以处理偏振 whose maximal phase difference exceeds 3.8$\pi$ when using a NA 0.25 objective in experiment.
</details></li>
</ul>
<hr>
<h2 id="Deep-Joint-Source-Channel-Coding-for-Adaptive-Image-Transmission-over-MIMO-Channels"><a href="#Deep-Joint-Source-Channel-Coding-for-Adaptive-Image-Transmission-over-MIMO-Channels" class="headerlink" title="Deep Joint Source-Channel Coding for Adaptive Image Transmission over MIMO Channels"></a>Deep Joint Source-Channel Coding for Adaptive Image Transmission over MIMO Channels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00470">http://arxiv.org/abs/2309.00470</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haotian Wu, Yulin Shao, Chenghong Bian, Krystian Mikolajczyk, Deniz Gündüz</li>
<li>For: 该论文提出了一种基于视力变换器（ViT）的深度联合源和渠道编码（DeepJSCC）方案，用于无线图像传输。* Methods: 该方案使用了自注意力机制，以智能学习特征映射和功率分配策略，适应源图像和存在的通道条件。* Results: 数值实验表明， DeepJSCC-MIMO 可以在开loop和关loop MIMO 系统中提高传输质量，并且具有鲁棒性和灵活性，不需要重新训练。<details>
<summary>Abstract</summary>
This paper introduces a vision transformer (ViT)-based deep joint source and channel coding (DeepJSCC) scheme for wireless image transmission over multiple-input multiple-output (MIMO) channels, denoted as DeepJSCC-MIMO. We consider DeepJSCC-MIMO for adaptive image transmission in both open-loop and closed-loop MIMO systems. The novel DeepJSCC-MIMO architecture surpasses the classical separation-based benchmarks with robustness to channel estimation errors and showcases remarkable flexibility in adapting to diverse channel conditions and antenna numbers without requiring retraining. Specifically, by harnessing the self-attention mechanism of ViT, DeepJSCC-MIMO intelligently learns feature mapping and power allocation strategies tailored to the unique characteristics of the source image and prevailing channel conditions. Extensive numerical experiments validate the significant improvements in transmission quality achieved by DeepJSCC-MIMO for both open-loop and closed-loop MIMO systems across a wide range of scenarios. Moreover, DeepJSCC-MIMO exhibits robustness to varying channel conditions, channel estimation errors, and different antenna numbers, making it an appealing solution for emerging semantic communication systems.
</details>
<details>
<summary>摘要</summary>
The novel DeepJSCC-MIMO architecture surpasses traditional separation-based benchmarks and is robust to channel estimation errors. It also demonstrates great flexibility in adapting to diverse channel conditions and antenna numbers without requiring retraining.The DeepJSCC-MIMO scheme utilizes the self-attention mechanism of ViT to intelligently learn feature mapping and power allocation strategies tailored to the unique characteristics of the source image and the prevailing channel conditions. This leads to significant improvements in transmission quality for both open-loop and closed-loop MIMO systems across a wide range of scenarios.Moreover, DeepJSCC-MIMO exhibits robustness to varying channel conditions, channel estimation errors, and different antenna numbers, making it a promising solution for emerging semantic communication systems. Extensive numerical experiments validate the effectiveness of the proposed scheme.
</details></li>
</ul>
<hr>
<h2 id="Learning-the-Imaging-Model-of-Speed-of-Sound-Reconstruction-via-a-Convolutional-Formulation"><a href="#Learning-the-Imaging-Model-of-Speed-of-Sound-Reconstruction-via-a-Convolutional-Formulation" class="headerlink" title="Learning the Imaging Model of Speed-of-Sound Reconstruction via a Convolutional Formulation"></a>Learning the Imaging Model of Speed-of-Sound Reconstruction via a Convolutional Formulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00453">http://arxiv.org/abs/2309.00453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Can Deniz Bezek, Maxim Haas, Richard Rau, Orcun Goksel</li>
<li>for: 这个论文目的是提出一种基于数据学习的 Speed-of-sound（SoS）成像方法，以提高SoS成像的准确性和稳定性。</li>
<li>methods: 该方法基于Convolutional Neural Networks（CNNs）的形式ulation，通过学习数据来学习SoS成像模型，并通过least-squares估算来估算参数。</li>
<li>results: 对于 simulate数据和实验室数据，使用这种方法可以提高SoS成像的对比度，比传统手动设计的模型更高。在人体实验中，使用这种方法可以提高SoS成像的对比度7倍和10倍。<details>
<summary>Abstract</summary>
Speed-of-sound (SoS) is an emerging ultrasound contrast modality, where pulse-echo techniques using conventional transducers offer multiple benefits. For estimating tissue SoS distributions, spatial domain reconstruction from relative speckle shifts between different beamforming sequences is a promising approach. This operates based on a forward model that relates the sought local values of SoS to observed speckle shifts, for which the associated image reconstruction inverse problem is solved. The reconstruction accuracy thus highly depends on the hand-crafted forward imaging model. In this work, we propose to learn the SoS imaging model based on data. We introduce a convolutional formulation of the pulse-echo SoS imaging problem such that the entire field-of-view requires a single unified kernel, the learning of which is then tractable and robust. We present least-squares estimation of such convolutional kernel, which can further be constrained and regularized for numerical stability. In experiments, we show that a forward model learned from k-Wave simulations improves the median contrast of SoS reconstructions by 63%, compared to a conventional hand-crafted line-based wave-path model. This simulation-learned model generalizes successfully to acquired phantom data, nearly doubling the SoS contrast compared to the conventional hand-crafted alternative. We demonstrate equipment-specific and small-data regime feasibility by learning a forward model from a single phantom image, where our learned model quadruples the SoS contrast compared to the conventional hand-crafted model. On in-vivo data, the simulation- and phantom-learned models respectively exhibit impressive 7 and 10 folds contrast improvements over the conventional model.
</details>
<details>
<summary>摘要</summary>
声速（SoS）是一种emergingultrasound contrast模式，使用普通探测器的推送-回声技术可以获得多种优点。为了估计组织声速分布，使用不同探测器序列的相对速度异常可以获得很好的结果。这种方法基于一个前向模型，该模型关系 soughtlocal声速值与观察到的速度异常，并解决了相关的图像重建问题。图像重建精度因此具有手工设计的前向图像模型的依赖性。在这种工作中，我们提出了学习SoS图像模型的方法。我们将探测器序列的pulse-echo SoS图像问题转换为一个 convolutional 形式，使得整个场景需要一个单一的核心，可以通过学习来解决。我们介绍了 least-squares 估算这个 convolutional 核心，可以进一步约束和减少数据稳定性。在实验中，我们发现使用k-WaveSimulation学习的前向模型可以提高SoS重建的 median 对比度by 63%，比 convential hand-crafted line-based wave-path模型更好。这个simulation-learned模型在实际数据上成功地泛化，对于acquired phantom data，它可以nearly double SoS对比度。我们还证明了设备特定和小数据 режим的可行性，通过学习一个 forward model从single phantom image中学习。在in vivo数据上，simulation-和 phantom-learned模型分别展示出了很出色的7和10倍对比度提高。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/01/eess.IV_2023_09_01/" data-id="clpxp6caw019tee88b9xg46tm" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_09_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/01/eess.SP_2023_09_01/" class="article-date">
  <time datetime="2023-09-01T08:00:00.000Z" itemprop="datePublished">2023-09-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/01/eess.SP_2023_09_01/">eess.SP - 2023-09-01</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Jamming-Suppression-Via-Resource-Hopping-in-High-Mobility-OTFS-SCMA-Systems"><a href="#Jamming-Suppression-Via-Resource-Hopping-in-High-Mobility-OTFS-SCMA-Systems" class="headerlink" title="Jamming Suppression Via Resource Hopping in High-Mobility OTFS-SCMA Systems"></a>Jamming Suppression Via Resource Hopping in High-Mobility OTFS-SCMA Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00753">http://arxiv.org/abs/2309.00753</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinwen Deng, Yao Ge, Zhi Ding</li>
<li>for: 这篇论文研究了OTFS系统中的上链多接入和干扰抑制机制。</li>
<li>methods: 该论文提议了一种基于延迟-Doppler域的资源频谱分配方法，以mitigate OTFS系统中的干扰影响。</li>
<li>results: 该方法通过利用扩展平衡，在干扰下表现出了与传统OTFS-SCMA系统相比的BER性能改善。<details>
<summary>Abstract</summary>
This letter studies the mechanism of uplink multiple access and jamming suppression in an OTFS system. Specifically, we propose a novel resource hopping mechanism for orthogonal time frequency space (OTFS) systems with delay or Doppler partitioned sparse code multiple access (SCMA) to mitigate the effect of jamming in controlled multiuser uplink. We analyze the non-uniform impact of classic jamming signals such as narrowband interference (NBI) and periodic impulse noise (PIN) in delay-Doppler (DD) domain on OTFS systems. Leveraging turbo equalization, our proposed hopping method demonstrates consistent BER performance improvement under jamming over conventional OTFS-SCMA systems compared to static resource allocation schemes.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Signal-Processing-and-Learning-for-Next-Generation-Multiple-Access-in-6G"><a href="#Signal-Processing-and-Learning-for-Next-Generation-Multiple-Access-in-6G" class="headerlink" title="Signal Processing and Learning for Next Generation Multiple Access in 6G"></a>Signal Processing and Learning for Next Generation Multiple Access in 6G</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00559">http://arxiv.org/abs/2309.00559</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Chen, Yuanwei Liu, Hamid Jafarkhani, Yonina C. Eldar, Peiying Zhu, Khaled B Letaief</li>
<li>for: 本文主要探讨了下一代多个访问（NGMA）技术的研究进展，尤其是大规模随机访问和非对势多访问。</li>
<li>methods: 本文考虑了新技术的潜在互动和学习基本技术的挑战。</li>
<li>results: 研究表明，学习基本技术可以解决许多传输和处理信号的复杂问题，但是还需要更多的研究来掌握这些技术的潜在。<details>
<summary>Abstract</summary>
Wireless communication systems to date primarily rely on the orthogonality of resources to facilitate the design and implementation, from user access to data transmission. Emerging applications and scenarios in the sixth generation (6G) wireless systems will require massive connectivity and transmission of a deluge of data, which calls for more flexibility in the design concept that goes beyond orthogonality. Furthermore, recent advances in signal processing and learning have attracted considerable attention, as they provide promising approaches to various complex and previously intractable problems of signal processing in many fields. This article provides an overview of research efforts to date in the field of signal processing and learning for next-generation multiple access, with an emphasis on massive random access and non-orthogonal multiple access. The promising interplay with new technologies and the challenges in learning-based NGMA are discussed.
</details>
<details>
<summary>摘要</summary>
无线通信系统至今主要依靠资源的正交性来实现用户访问和数据传输。 sixth generation（6G）无线系统中出现的新应用和场景将需要巨量的连接和大量数据传输，这需要更多的灵活性在设计理念中，超出正交性的限制。此外，近年来的信号处理和学习技术的进步吸引了广泛的关注，因为它们在各个领域提供了可能的解决方案。本文提供了到date的研究努力在信号处理和学习领域中，强调巨量随机访问和非正交多访问。新技术的潜在优势和学习基于NGMA的挑战也被讨论。
</details></li>
</ul>
<hr>
<h2 id="Achievable-Rate-Region-and-Path-Based-Beamforming-for-Multi-User-Single-Carrier-Delay-Alignment-Modulation"><a href="#Achievable-Rate-Region-and-Path-Based-Beamforming-for-Multi-User-Single-Carrier-Delay-Alignment-Modulation" class="headerlink" title="Achievable Rate Region and Path-Based Beamforming for Multi-User Single-Carrier Delay Alignment Modulation"></a>Achievable Rate Region and Path-Based Beamforming for Multi-User Single-Carrier Delay Alignment Modulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00391">http://arxiv.org/abs/2309.00391</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingwei Wang, Haiquan Lu, Yong Zeng, Xiaoli Xu, Jie Xu<br>for:* 这篇论文是为了研究多用户mmWave巨量MIMO通信系统中的延时对Alignment模ulation（DAM）技术。methods:* 这篇论文使用了延时预补做法和路径基于的扫描方法来有效地对多个射频组件进行对齐，从而消除干扰而保留多Path强度提升。results:* 这篇论文表明，在Mt sufficiently large的情况下，通过使用简单的延时预补和每个射频路径基于的MRT扫描方法，单载DAM可以完美地消除干扰和IUI。* 在Mt finite的情况下，该论文研究了多用户DAM系统的可达速率区的可能性。* 该论文提出了三种低复杂度的每个射频路径基于的扫描策略，并研究了这些策略对系统的可达速率的影响。* 最后，该论文提供了对两个参考方案（基于最强射频路径基于的扫描和OFDM）的比较，并证明DAM在高 spatial resolution和多Path多杂性下 achieve higher spectral efficiency和&#x2F;或 lower peak-to-average-ratio。<details>
<summary>Abstract</summary>
Delay alignment modulation (DAM) is a novel wideband transmission technique for mmWave massive MIMO systems, which exploits the high spatial resolution and multi-path sparsity to mitigate ISI, without relying on channel equalization or multi-carrier transmission. In particular, DAM leverages the delay pre-compensation and path-based beamforming to effectively align the multi-path components, thus achieving the constructive multi-path combination for eliminating the ISI while preserving the multi-path power gain. Different from the existing works only considering single-user DAM, this paper investigates the DAM technique for multi-user mmWave massive MIMO communication. First, we consider the asymptotic regime when the number of antennas Mt at BS is sufficiently large. It is shown that by employing the simple delay pre-compensation and per-path-based MRT beamforming, the single-carrier DAM is able to perfectly eliminate both ISI and IUI. Next, we consider the general scenario with Mt being finite. In this scenario, we characterize the achievable rate region of the multi-user DAM system by finding its Pareto boundary. Specifically, we formulate a rate-profile-constrained sum rate maximization problem by optimizing the per-path-based beamforming. Furthermore, we present three low-complexity per-path-based beamforming strategies based on the MRT, zero-forcing, and regularized zero-forcing principles, respectively, based on which the achievable sum rates are studied. Finally, we provide simulation results to demonstrate the performance of our proposed strategies as compared to two benchmark schemes based on the strongest-path-based beamforming and the prevalent OFDM, respectively. It is shown that DAM achieves higher spectral efficiency and/or lower peak-to-average-ratio, for systems with high spatial resolution and multi-path diversity.
</details>
<details>
<summary>摘要</summary>
延迟匹配模ulation（DAM）是一种新的宽带传输技术，用于mmWave巨量MIMO系统，可以利用高度空间分解和多path稀烈性来缓解混合干扰（ISI），不需要通道平衡或多帧传输。具体来说，DAM利用延迟预补和路径基本形式 beamforming来有效地对多path组分进行匹配，从而实现了构建多path组分的积加，以消除ISI而保留多path功率增加。与现有作品只考虑单用户DAM不同，本文研究了DAM技术在多用户mmWave巨量MIMO通信中的应用。首先，我们考虑了Mt的很大值时的极限情况。 results show that by employing simple delay pre-compensation and per-path-based MRT beamforming, the single-carrier DAM can perfectly eliminate both ISI and IUI.然后，我们考虑了Mt finite值的普通情况。在这种情况下，我们定义了DAM系统的可达性区的边界，并通过优化每个路径基本形式 beamforming来实现最大化总bitrate。 Specifically, we formulate a rate-profile-constrained sum rate maximization problem by optimizing the per-path-based beamforming.此外，我们还提出了三种低复杂度的每个路径基本形式 beamforming策略，基于MRT、零干扰和正则化零干扰原理。这些策略的实现可以帮助提高系统的可达性和 Spectral efficiency。最后，我们通过对两种参考方案（基于最强路径基本形式 beamforming和普遍OFDM）进行比较，提供了实验结果，以证明DAM在高度空间分解和多path稀烈性的系统中可以 дости得更高的 spectral efficiency和/或更低的 peak-to-average-ratio。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-estimation-and-reconstruction-of-marine-surface-contaminant-dispersion"><a href="#Bayesian-estimation-and-reconstruction-of-marine-surface-contaminant-dispersion" class="headerlink" title="Bayesian estimation and reconstruction of marine surface contaminant dispersion"></a>Bayesian estimation and reconstruction of marine surface contaminant dispersion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00369">http://arxiv.org/abs/2309.00369</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Liu, Christopher M. Harvey, Frederick E. Hamlyn, Cunjia Liu<br>for: 这个研究旨在提供一个精度估计和重建海洋环境中潜在危害物质泄漏的框架，以便与环境监测敏感器网络或移动敏感器相结合。methods: 这个研究使用了基本的扩散-渗透偏微方程（PDE）来表示物质泄漏的总体分布，并使用动态适应finite-element方法（FEM）将其在非均匀流场中的空间灵活分解。在扩散过程中，研究者们考虑了感知器的不准确现象，包括漏掉检测和信号量化。results: 研究结果表明，提出的框架在实际的油污泄漏事件中的波罗的海 demonstrate its efficacy in reconstructing spatio-temporal dispersion in the presence of imperfect measurements.<details>
<summary>Abstract</summary>
Discharge of hazardous substances into the marine environment poses a substantial risk to both public health and the ecosystem. In such incidents, it is imperative to accurately estimate the release strength of the source and reconstruct the spatio-temporal dispersion of the substances based on the collected measurements. In this study, we propose an integrated estimation framework to tackle this challenge, which can be used in conjunction with a sensor network or a mobile sensor for environment monitoring. We employ the fundamental convection-diffusion partial differential equation (PDE) to represent the general dispersion of a physical quantity in a non-uniform flow field. The PDE model is spatially discretised into a linear state-space model using the dynamic transient finite-element method (FEM) so that the characterisation of time-varying dispersion can be cast into the problem of inferring the model states from sensor measurements. We also consider imperfect sensing phenomena, including miss-detection and signal quantisation, which are frequently encountered when using a sensor network. This complicated sensor process introduces nonlinearity into the Bayesian estimation process. A Rao-Blackwellised particle filter (RBPF) is designed to provide an effective solution by exploiting the linear structure of the state-space model, whereas the nonlinearity of the measurement model can be handled by Monte Carlo approximation with particles. The proposed framework is validated using a simulated oil spill incident in the Baltic sea with real ocean flow data. The results show the efficacy of the developed spatio-temporal dispersion model and estimation schemes in the presence of imperfect measurements. Moreover, the parameter selection process is discussed, along with some comparison studies to illustrate the advantages of the proposed algorithm over existing methods.
</details>
<details>
<summary>摘要</summary>
排放危险物质到海洋环境中存在严重的风险，对公众健康和生态系统都具有潜在的威胁。在这种情况下，需要精准地估计排放源的强度和杂散的空间时间特征。本研究提出一种集成估计框架，可以与监测敏感器网络或移动监测器相结合使用。我们采用基本的扩散混合方程（PDE）来表示物质扩散的总体特征，并将PDE方程在空间上精度化为线性状态空间模型（FEM），以便在排放源的特征上进行时间变化的描述。我们还考虑了感知现象的不准确性，包括感知错误和信号量化，这些现象在使用敏感器网络时经常出现。这种复杂的感知过程引入了非线性到 bayesian 估计过程中。我们采用Rao-Blackwellised particle filter（RBPF）来提供有效的解决方案，通过利用状态空间模型的线性结构，同时处理测量模型中的非线性。我们在 simulated oil spill  incident 中使用 Baltic sea 的实际海流数据进行验证，结果显示我们提出的空间时间杂散模型和估计方法在受到不准确测量的情况下具有效果。此外，我们还讨论了参数选择过程，并进行了与其他方法进行比较，以 Illustrate 我们的方法的优势。
</details></li>
</ul>
<hr>
<h2 id="Message-Passing-Based-Block-Sparse-Signal-Recovery-for-DOA-Estimation-Using-Large-Arrays"><a href="#Message-Passing-Based-Block-Sparse-Signal-Recovery-for-DOA-Estimation-Using-Large-Arrays" class="headerlink" title="Message Passing Based Block Sparse Signal Recovery for DOA Estimation Using Large Arrays"></a>Message Passing Based Block Sparse Signal Recovery for DOA Estimation Using Large Arrays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00313">http://arxiv.org/abs/2309.00313</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiwen Mao, Dawei Gao, Qinghua Guo, Ming Jin</li>
<li>for: 这篇论文关注方向来源估计（DOA），使用大天线数组。</li>
<li>methods: 该论文首先开发了一种新的信号模型，使用反排DFT操作生成一个稀疏系统传输矩阵，从而转化为一个结构化块稀疏信号恢复问题，使用消息传递基于 bayesian 算法，并使用 фактор图表示。</li>
<li>results: 模拟结果表明提议方法的性能较高。<details>
<summary>Abstract</summary>
This work deals with directional of arrival (DOA) estimation with a large antenna array. We first develop a novel signal model with a sparse system transfer matrix using an inverse discrete Fourier transform (DFT) operation, which leads to the formulation of a structured block sparse signal recovery problem with a sparse sensing matrix. This enables the development of a low complexity message passing based Bayesian algorithm with a factor graph representation. Simulation results demonstrate the superior performance of the proposed method.
</details>
<details>
<summary>摘要</summary>
这个工作关于方向来源估计（DOA）测量使用大天线数组。我们首先开发了一种新的信号模型，使用反对排阵 Fourier 转换（DFT）操作获得简单系统传输矩阵，从而导致了一个嵌入式块简单信号恢复问题，其中感知矩阵具有稀疏性。这使得我们可以开发一种低复杂度的消息传递基于 bayesian 算法，并使用因素图表示。实验结果表明我们的方法表现更好。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="A-Spatial-Sigma-Delta-Approach-to-Mitigation-of-Power-Amplifier-Distortions-in-Massive-MIMO-Downlink"><a href="#A-Spatial-Sigma-Delta-Approach-to-Mitigation-of-Power-Amplifier-Distortions-in-Massive-MIMO-Downlink" class="headerlink" title="A Spatial Sigma-Delta Approach to Mitigation of Power Amplifier Distortions in Massive MIMO Downlink"></a>A Spatial Sigma-Delta Approach to Mitigation of Power Amplifier Distortions in Massive MIMO Downlink</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00289">http://arxiv.org/abs/2309.00289</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yatao Liu, Mingjie Shao, Wing-Kin Ma</li>
<li>for: 提高多Input多Output（MIMO）下链系统中基站（BS）的物理实现，使用低成本和功率低的发射器（PA），以避免高硬件成本和高功率消耗。</li>
<li>methods: 使用Sigma-Delta（$\Sigma \Delta$)模ulator来修正发射器（PA）的不良，通过将不良修正到高角度区域。</li>
<li>results: 通过数字预修正（DPD）和符号级precoding（SLP） schemes，可以提高系统性能。<details>
<summary>Abstract</summary>
In massive multiple-input multiple-output (MIMO) downlink systems, the physical implementation of the base stations (BSs) requires the use of cheap and power-efficient power amplifiers (PAs) to avoid high hardware cost and high power consumption. However, such PAs usually have limited linear amplification ranges. Nonlinear distortions arising from operation beyond the linear amplification ranges can significantly degrade system performance. Existing approaches to handle the nonlinear distortions, such as digital predistortion (DPD), typically require accurate knowledge, or acquisition, of the PA transfer function. In this paper, we present a new concept for mitigation of the PA distortions. Assuming a uniform linear array (ULA) at the BS, the idea is to apply a Sigma-Delta ($\Sigma \Delta$) modulator to spatially shape the PA distortions to the high-angle region. By having the system operating in the low-angle region, the received signals are less affected by the PA distortions. To demonstrate the potential of this spatial $\Sigma \Delta$ approach, we study the application of our approach to the multi-user MIMO-orthogonal frequency division modulation (OFDM) downlink scenario. A symbol-level precoding (SLP) scheme and a zero-forcing (ZF) precoding scheme, with the new design requirement by the spatial $\Sigma \Delta$ approach being taken into account, are developed. Numerical simulations are performed to show the effectiveness of the developed $\Sigma \Delta$ precoding schemes.
</details>
<details>
<summary>摘要</summary>
在大规模多输入多出力（MIMO）下行系统中，基站（BS）的物理实现需要使用便宜且功率低的功率增强器（PA），以避免高硬件成本和高功率消耗。然而，这些PA通常有有限的线性增强范围。操作 beyond这些范围的非线性扭曲可以很大程度地降低系统性能。现有的抗扭曲方法，如数字预修正（DPD），通常需要准确地知道或获得PA传输函数。在本文中，我们提出了一种新的抗扭曲方法。假设基站使用uniform linear array（ULA），我们的想法是通过空间Sigma-Delta（$\Sigma \Delta$)模ulator来形态地扭曲PA扭曲到高角度区域。由于系统在低角度区域运行，接收信号受到PA扭曲的影响较少。为了证明这种空间$\Sigma \Delta$方法的潜在效果，我们对多用户MIMO-orthogonal frequency division modulation（OFDM）下行enario进行了研究。我们开发了一种符号级 precoding（SLP）和零强制（ZF） precoding  schemes，并考虑了新的设计要求。我们对 numerically simulations 进行了测试，以证明我们的 $\Sigma \Delta$ precoding schemes 的有效性。
</details></li>
</ul>
<hr>
<h2 id="Evaluation-of-onboard-sensors-for-track-geometry-monitoring-against-conventional-track-recording-measurements"><a href="#Evaluation-of-onboard-sensors-for-track-geometry-monitoring-against-conventional-track-recording-measurements" class="headerlink" title="Evaluation of onboard sensors for track geometry monitoring against conventional track recording measurements"></a>Evaluation of onboard sensors for track geometry monitoring against conventional track recording measurements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00270">http://arxiv.org/abs/2309.00270</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hengcheng Zhang, Zhan Yie Chin, Pietro Borghesani, James Pitt, Michael E. Cholette</li>
<li>for: 本研究旨在评估使用在列车上的微电子机械系统（MEMS）加速仪来推算轨道条件参数，如垂直和水平对齐。</li>
<li>methods: 研究人员设计了一种在列车上的数据获取系统（DAQ）的原型，并在布里斯班郊区铁路网络上进行了测量。 comparison of accelerometer-based results vs TRC recordings 表明，在车厢上安装的加速仪最佳妥协是距离源最近，但免疫干扰噪声的选择。</li>
<li>results: 研究发现，在左右两侧的 bogie 上安装的两个垂直加速仪可以提供好的量化估计垂直对齐，而两个水平加速仪的测量结果与 TRC 记录相关。这些发现表明，使用两个 bogie 上的 MEMS 加速仪，可以提供量化的垂直对齐和水平对齐的估计，并且可以描述轨道网络中水平对齐的趋势。<details>
<summary>Abstract</summary>
The main objective of this paper is to assess the feasibility and accuracy of inferring key track condition parameters, e.g., vertical alignment and horizontal alignment of the rails, using onboard micro-electro-mechanical-system (MEMS) accelerometers. To achieve this aim, a prototype of an onboard data acquisition system (DAQ) was designed and installed on a track recording car (TRC) and a measurement campaign was conducted on an extensive portion of the Brisbane Suburban railway network. Comparison of the accelerometer-based results vs TRC recordings have shown that accelerometers installed on the bogie are the best compromise between proximity to the source and insensitivity to impulsive noise. Moreover, it was found that two vertical bogie accelerometers (left and right side) provide a good quantitative estimate of vertical alignment and that strong correlations with TRC measurements exist for lateral MEMS accelerometer measurements (horizontal alignment). These findings suggest that two bogie MEMS accelerometers with two measurement axes (vertical and lateral) are an effective system and can provide quantitative estimates of vertical alignment and trends in the geographical distribution of horizontal alignment.
</details>
<details>
<summary>摘要</summary>
本文的主要目标是评估使用在列车上的微型电子机械系统（MEMS）加速仪测量铁轨的重要参数，如垂直平行和水平平行。为达到这个目标，我们设计了一种在列车上的数据采集系统（DAQ）的原型，并在布里斯班郊区铁路网络上进行了评估。对比加速仪测量结果和TRC记录结果表明，在车厢上安装的加速仪最佳的折衔是与源之间的距离和干扰噪音的敏感度之间的平衡。此外，我们发现了两个垂直车厢加速仪（左右两侧）可以提供好的量化估计垂直平行，并且发现了两个车厢MEMS加速仪的两个测量轴（垂直和水平）可以提供有关垂直平行的趋势和地理分布的水平平行的量化估计。这些发现表明，两个车厢MEMS加速仪是一个有效的系统，可以提供有关垂直平行和水平平行的量化估计。
</details></li>
</ul>
<hr>
<h2 id="Concept-for-an-Automatic-Annotation-of-Automotive-Radar-Data-Using-AI-segmented-Aerial-Camera-Images"><a href="#Concept-for-an-Automatic-Annotation-of-Automotive-Radar-Data-Using-AI-segmented-Aerial-Camera-Images" class="headerlink" title="Concept for an Automatic Annotation of Automotive Radar Data Using AI-segmented Aerial Camera Images"></a>Concept for an Automatic Annotation of Automotive Radar Data Using AI-segmented Aerial Camera Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00268">http://arxiv.org/abs/2309.00268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcel Hoffmann, Sandro Braun, Oliver Sura, Michael Stelzig, Christian Schüßler, Knut Graichen, Martin Vossiek</li>
<li>for: 自动标注汽车雷达数据使用人工智能分割的飞行器摄像头图像</li>
<li>methods: 使用无人飞行器摄像头图像在地面上找到和映射到雷达图像中的实例和段落，然后将摄像头图像中探测到的实例和段落应用直接作为雷达数据的标签</li>
<li>results: 在测量中，使用这种方法自动标注了589名行人在雷达数据中，并且只用2分钟的时间Here’s the translation of the abstract in English:</li>
<li>for: Automatically annotate automotive radar data with AI-segmented aerial camera images</li>
<li>methods: Use UAV camera images to find and map instances and segments in the ground plane onto radar images, and apply the detected instances and segments as labels for the radar data</li>
<li>results: Demonstrated effectiveness and scalability in measurements, where 589 pedestrians in the radar data were automatically labeled within 2 minutes.<details>
<summary>Abstract</summary>
This paper presents an approach to automatically annotate automotive radar data with AI-segmented aerial camera images. For this, the images of an unmanned aerial vehicle (UAV) above a radar vehicle are panoptically segmented and mapped in the ground plane onto the radar images. The detected instances and segments in the camera image can then be applied directly as labels for the radar data. Owing to the advantageous bird's eye position, the UAV camera does not suffer from optical occlusion and is capable of creating annotations within the complete field of view of the radar. The effectiveness and scalability are demonstrated in measurements, where 589 pedestrians in the radar data were automatically labeled within 2 minutes.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Ground-Truth-Generation-Algorithm-for-Medium-Frequency-R-Mode-Skywave-Detection"><a href="#Ground-Truth-Generation-Algorithm-for-Medium-Frequency-R-Mode-Skywave-Detection" class="headerlink" title="Ground Truth Generation Algorithm for Medium-Frequency R-Mode Skywave Detection"></a>Ground Truth Generation Algorithm for Medium-Frequency R-Mode Skywave Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00234">http://arxiv.org/abs/2309.00234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suhui Jeong, Pyo-Woong Son</li>
<li>for: 提高交通运输车辆的重要性和实用性，Navigation系统提供了定位、导航和时间信息的重要性在不断增加。</li>
<li>methods: 研究者使用了一种称为R-Mode的面基站集成导航系统，该系统利用中频差分GNSS（DGNSS）和很高频数据交换系统（VDES）信号作为定位信号，并将现有的地面导航系统称为增强长距离导航（eLoran）。</li>
<li>results: 研究人员发现MF R-Mode在白天和黑夜之间表现出显著的性能差异，这是由GNSS信号在离子层反射后引起的天体干扰所致。在这项研究中，我们提出了一种可以生成天体干扰的背景真实场景生成算法，并在实验数据上进行了 validate。<details>
<summary>Abstract</summary>
With the advancement of transportation vehicles, the importance and utility of navigation systems providing positioning, navigation, and timing (PNT) information have been increasing. Global navigation satellite systems (GNSS) are widely used navigation systems, but they are vulnerable to radio frequency interference (RFI), resulting in disruptions of satellite navigation signals. Recognizing this limitation, extensive research is being conducted on alternative navigation systems. In the maritime industry, ongoing research focuses on a groundbased integrated navigation system called R-Mode. R-Mode utilizes medium frequency (MF) differential GNSS (DGNSS) and very high-frequency data exchange system (VDES) signals as ranging signals for positioning and incorporates the existing ground-based navigation system known as enhanced long-range navigation (eLoran). However, MF R-Mode, which uses MF DGNSS signals for positioning, exhibits significant performance differences between daytime and nighttime due to skywave interference caused by signals reflecting off the ionosphere. In this study, we propose a skywave ground truth generation algorithm that is crucial for studying mitigation methods for MF R-Mode skywave interference. Furthermore, we demonstrate the proposed algorithm using field-test data.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:随着交通运输车辆的发展，提供位置、导航和时间信息的导航系统的重要性和实用性日益增加。全球卫星导航系统（GNSS）是广泛使用的导航系统，但它们受到广播频率干扰（RFI）的影响，导致卫星导航信号的中断。认识到这些限制，广泛的研究在进行 altenative 导航系统。在海事业中，进行的研究主要关注一种基于地面的集成导航系统，即R-Mode。R-Mode 使用中频 differential GNSS（DGNSS）和射频数据交换系统（VDES）信号作为距离测量信号，并 integra 现有的地面导航系统，即增强距离导航（eLoran）。然而，MF R-Mode，使用MF DGNSS信号进行位置测量，在日间和夜间 exhibits 显著的性能差异，这是由于天顶层干扰，即信号反射在离子层。在这项研究中，我们提出了天顶层真实数据生成算法，这是关键的 для研究MF R-Mode 天顶层干扰的mitigation方法。此外，我们使用实验数据来证明提出的算法。
</details></li>
</ul>
<hr>
<h2 id="Empirical-Modeling-of-Variance-in-Medium-Frequency-R-Mode-Time-of-Arrival-Measurements"><a href="#Empirical-Modeling-of-Variance-in-Medium-Frequency-R-Mode-Time-of-Arrival-Measurements" class="headerlink" title="Empirical Modeling of Variance in Medium Frequency R-Mode Time-of-Arrival Measurements"></a>Empirical Modeling of Variance in Medium Frequency R-Mode Time-of-Arrival Measurements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00202">http://arxiv.org/abs/2309.00202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaewon Yu, Pyo-Woong Son</li>
<li>for: 提高中频R-Mode系统的性能模拟精度</li>
<li>methods: 基于实际数据模型TOA测量的时间差异方差</li>
<li>results: 通过应用loran方法来计算时间接收标准差，并估算TOA测量方差参数Here’s the same information in English:</li>
<li>for: Improving the accuracy of performance simulation for the medium frequency R-Mode system</li>
<li>methods: Modeling the variance of time-of-arrival measurements based on actual data, inspired by the method used to calculate the standard deviation of time-of-reception measurements in Loran</li>
<li>results: Estimating the parameters for modeling the variance of TOA measurements using the Loran method and applying it to the MF R-Mode system.<details>
<summary>Abstract</summary>
The R-Mode system, an advanced terrestrial integrated navigation system, is designed to address the vulnerabilities of global navigation satellite systems (GNSS) and explore the potential of a complementary navigation system. This study aims to enhance the accuracy of performance simulation for the medium frequency (MF) R-Mode system by modeling the variance of time-of-arrival (TOA) measurements based on actual data. Drawing inspiration from the method used to calculate the standard deviation of time-of-reception (TOR) measurements in Loran, we adapted and applied this approach to the MF R-Mode system. Data were collected from transmitters in Palmi and Chungju, South Korea, and the parameters for modeling the variance of TOA were estimated.
</details>
<details>
<summary>摘要</summary>
R-Mode系统，一个进阶的陆地综合导航系统，旨在解决全球导航卫星系统（GNSS）的漏洞和探索一个辅助导航系统的潜力。本研究旨在提高中频R-Mode系统的性能模拟精度，基于实际数据模型时间到来（TOA）测量的变化。将 Loran中用于计算时间接收变化的方法作为灵感，我们适应并应用这种方法到中频R-Mode系统中。数据来自韩国Palmi和Chungju的传送器，并估算了模型时间到来的变化。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/01/eess.SP_2023_09_01/" data-id="clpxp6ccm01dpee888c1batrt" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/58/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/57/">57</a><a class="page-number" href="/page/58/">58</a><span class="page-number current">59</span><a class="page-number" href="/page/60/">60</a><a class="page-number" href="/page/61/">61</a><span class="space">&hellip;</span><a class="page-number" href="/page/98/">98</a><a class="extend next" rel="next" href="/page/60/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">67</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">82</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">147</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
